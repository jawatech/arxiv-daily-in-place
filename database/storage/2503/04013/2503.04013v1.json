{"2503.04013": {"publish_time": "2025-03-06", "title": "Benchmarking Large Language Models on Multiple Tasks in Bioinformatics NLP with Prompting", "paper_summary": "Large language models (LLMs) have become important tools in solving\nbiological problems, offering improvements in accuracy and adaptability over\nconventional methods. Several benchmarks have been proposed to evaluate the\nperformance of these LLMs. However, current benchmarks can hardly evaluate the\nperformance of these models across diverse tasks effectively. In this paper, we\nintroduce a comprehensive prompting-based benchmarking framework, termed\nBio-benchmark, which includes 30 key bioinformatics tasks covering areas such\nas proteins, RNA, drugs, electronic health records, and traditional Chinese\nmedicine. Using this benchmark, we evaluate six mainstream LLMs, including\nGPT-4o and Llama-3.1-70b, etc., using 0-shot and few-shot Chain-of-Thought\n(CoT) settings without fine-tuning to reveal their intrinsic capabilities. To\nimprove the efficiency of our evaluations, we demonstrate BioFinder, a new tool\nfor extracting answers from LLM responses, which increases extraction accuracy\nby round 30% compared to existing methods. Our benchmark results show the\nbiological tasks suitable for current LLMs and identify specific areas\nrequiring enhancement. Furthermore, we propose targeted prompt engineering\nstrategies for optimizing LLM performance in these contexts. Based on these\nfindings, we provide recommendations for the development of more robust LLMs\ntailored for various biological applications. This work offers a comprehensive\nevaluation framework and robust tools to support the application of LLMs in\nbioinformatics.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u89e3\u6c7a\u751f\u7269\u554f\u984c\u7684\u91cd\u8981\u5de5\u5177\uff0c\u5728\u6e96\u78ba\u6027\u548c\u9069\u61c9\u6027\u65b9\u9762\u6bd4\u50b3\u7d71\u65b9\u6cd5\u6709\u6240\u63d0\u5347\u3002\u76ee\u524d\u5df2\u63d0\u51fa\u591a\u9805\u57fa\u6e96\u4f86\u8a55\u4f30\u9019\u4e9b LLM \u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u57fa\u6e96\u5f88\u96e3\u6709\u6548\u5730\u8a55\u4f30\u9019\u4e9b\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u57fa\u65bc\u63d0\u793a\u7684\u7d9c\u5408\u57fa\u6e96\u6e2c\u8a66\u6846\u67b6\uff0c\u7a31\u70ba Bio-benchmark\uff0c\u5b83\u5305\u542b 30 \u9805\u95dc\u9375\u7684\u751f\u7269\u8cc7\u8a0a\u5b78\u4efb\u52d9\uff0c\u6db5\u84cb\u86cb\u767d\u8cea\u3001RNA\u3001\u85e5\u7269\u3001\u96fb\u5b50\u5065\u5eb7\u8a18\u9304\u548c\u4e2d\u91ab\u85e5\u7b49\u9818\u57df\u3002\u4f7f\u7528\u6b64\u57fa\u6e96\uff0c\u6211\u5011\u5728\u7121\u9700\u5fae\u8abf\u7684\u60c5\u6cc1\u4e0b\uff0c\u4f7f\u7528\u96f6\u6a23\u672c\u548c\u5c11\u6a23\u672c\u601d\u7dad\u93c8 (CoT) \u8a2d\u5b9a\uff0c\u8a55\u4f30\u4e86\u516d\u7a2e\u4e3b\u6d41 LLM\uff0c\u5305\u62ec GPT-4o \u548c Llama-3.1-70b \u7b49\uff0c\u4ee5\u63ed\u793a\u5b83\u5011\u7684\u5167\u5728\u80fd\u529b\u3002\u70ba\u4e86\u63d0\u9ad8\u8a55\u4f30\u6548\u7387\uff0c\u6211\u5011\u5c55\u793a\u4e86 BioFinder\uff0c\u9019\u662f\u4e00\u6b3e\u7528\u65bc\u5f9e LLM \u56de\u61c9\u4e2d\u63d0\u53d6\u7b54\u6848\u7684\u65b0\u5de5\u5177\uff0c\u8207\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5176\u63d0\u53d6\u6e96\u78ba\u5ea6\u63d0\u9ad8\u4e86\u7d04 30%\u3002\u6211\u5011\u7684\u57fa\u6e96\u6e2c\u8a66\u7d50\u679c\u986f\u793a\u4e86\u9069\u7528\u65bc\u76ee\u524d LLM \u7684\u751f\u7269\u4efb\u52d9\uff0c\u4e26\u78ba\u5b9a\u4e86\u9700\u8981\u6539\u9032\u7684\u7279\u5b9a\u9818\u57df\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u6709\u91dd\u5c0d\u6027\u7684\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff0c\u7528\u65bc\u5728\u9019\u4e9b\u60c5\u6cc1\u4e0b\u512a\u5316 LLM \u6548\u80fd\u3002\u57fa\u65bc\u9019\u4e9b\u767c\u73fe\uff0c\u6211\u5011\u70ba\u958b\u767c\u9069\u7528\u65bc\u5404\u7a2e\u751f\u7269\u61c9\u7528\u7a0b\u5f0f\u3001\u66f4\u5f37\u5927\u7684 LLM \u63d0\u4f9b\u4e86\u5efa\u8b70\u3002\u9019\u9805\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u500b\u5168\u9762\u7684\u8a55\u4f30\u6846\u67b6\u548c\u5f37\u5927\u7684\u5de5\u5177\uff0c\u4ee5\u652f\u63f4 LLM \u5728\u751f\u7269\u8cc7\u8a0a\u5b78\u4e2d\u7684\u61c9\u7528\u3002\n", "author": "Jiyue Jiang et.al.", "authors": "Jiyue Jiang, Pengan Chen, Jiuming Wang, Dongchen He, Ziqin Wei, Liang Hong, Licheng Zong, Sheng Wang, Qinze Yu, Zixian Ma, Yanyu Chen, Yimin Fan, Xiangyu Shi, Jiawei Sun, Chuan Wu, Yu Li", "id": "2503.04013v1", "paper_url": "http://arxiv.org/abs/2503.04013v1", "repo": "null"}}