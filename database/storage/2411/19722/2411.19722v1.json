{"2411.19722": {"publish_time": "2024-11-29", "title": "JetFormer: An Autoregressive Generative Model of Raw Images and Text", "paper_summary": "Removing modeling constraints and unifying architectures across domains has\nbeen a key driver of the recent progress in training large multimodal models.\nHowever, most of these models still rely on many separately trained components\nsuch as modality-specific encoders and decoders. In this work, we further\nstreamline joint generative modeling of images and text. We propose an\nautoregressive decoder-only transformer - JetFormer - which is trained to\ndirectly maximize the likelihood of raw data, without relying on any separately\npretrained components, and can understand and generate both text and images.\nSpecifically, we leverage a normalizing flow model to obtain a soft-token image\nrepresentation that is jointly trained with an autoregressive multimodal\ntransformer. The normalizing flow model serves as both an image encoder for\nperception tasks and an image decoder for image generation tasks during\ninference. JetFormer achieves text-to-image generation quality competitive with\nrecent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained\nimage autoencoders, which are trained with a complex mixture of losses,\nincluding perceptual ones. At the same time, JetFormer demonstrates robust\nimage understanding capabilities. To the best of our knowledge, JetFormer is\nthe first model that is capable of generating high-fidelity images and\nproducing strong log-likelihood bounds.", "paper_summary_zh": "\u79fb\u9664\u6a21\u578b\u9650\u5236\u548c\u7d71\u4e00\u8de8\u9818\u57df\u67b6\u69cb\u4e00\u76f4\u662f\u6700\u8fd1\u8a13\u7df4\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\u9032\u5c55\u7684\u95dc\u9375\u9a45\u52d5\u529b\u3002\n\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u5927\u591a\u4ecd\u4f9d\u8cf4\u8a31\u591a\u500b\u5225\u8a13\u7df4\u7684\u7d44\u6210\u90e8\u5206\uff0c\u4f8b\u5982\u7279\u5b9a\u65bc\u6a21\u614b\u7684\u7de8\u78bc\u5668\u548c\u89e3\u78bc\u5668\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9032\u4e00\u6b65\u7c21\u5316\u4e86\u5f71\u50cf\u548c\u6587\u5b57\u7684\u806f\u5408\u751f\u6210\u5f0f\u5efa\u6a21\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u81ea\u8ff4\u6b78\u50c5\u89e3\u78bc\u5668\u8f49\u63db\u5668 - JetFormer - \u8a13\u7df4\u76f4\u63a5\u6700\u5927\u5316\u539f\u59cb\u8cc7\u6599\u7684\u53ef\u80fd\u6027\uff0c\u800c\u7121\u9700\u4f9d\u8cf4\u4efb\u4f55\u500b\u5225\u9810\u8a13\u7df4\u7d44\u6210\u90e8\u5206\uff0c\u4e26\u4e14\u53ef\u4ee5\u7406\u89e3\u4e26\u751f\u6210\u6587\u5b57\u548c\u5f71\u50cf\u3002\n\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5229\u7528\u6b63\u898f\u5316\u6d41\u6a21\u578b\u4f86\u53d6\u5f97\u8207\u81ea\u8ff4\u6b78\u591a\u6a21\u614b\u8f49\u63db\u5668\u806f\u5408\u8a13\u7df4\u7684\u8edf\u4ee3\u5e63\u5f71\u50cf\u8868\u793a\u3002\u6b63\u898f\u5316\u6d41\u6a21\u578b\u7528\u4f5c\u611f\u77e5\u4efb\u52d9\u7684\u5f71\u50cf\u7de8\u78bc\u5668\u548c\u63a8\u8ad6\u671f\u9593\u5f71\u50cf\u751f\u6210\u4efb\u52d9\u7684\u5f71\u50cf\u89e3\u78bc\u5668\u3002JetFormer \u9054\u5230\u8207\u6700\u8fd1\u57fa\u65bc VQ-VAE \u548c VAE \u7684\u57fa\u6e96\u76f8\u7576\u7684\u6587\u5b57\u5230\u5f71\u50cf\u751f\u6210\u54c1\u8cea\u3002\u9019\u4e9b\u57fa\u6e96\u4f9d\u8cf4\u65bc\u9810\u8a13\u7df4\u7684\u5f71\u50cf\u81ea\u52d5\u7de8\u78bc\u5668\uff0c\u9019\u4e9b\u7de8\u78bc\u5668\u4f7f\u7528\u8907\u96dc\u7684\u640d\u5931\u6df7\u5408\u8a13\u7df4\uff0c\u5305\u62ec\u611f\u77e5\u640d\u5931\u3002\u540c\u6642\uff0cJetFormer \u5c55\u793a\u4e86\u5f37\u5927\u7684\u5f71\u50cf\u7406\u89e3\u80fd\u529b\u3002\u64da\u6211\u5011\u6240\u77e5\uff0cJetFormer \u662f\u7b2c\u4e00\u500b\u80fd\u5920\u751f\u6210\u9ad8\u4fdd\u771f\u5f71\u50cf\u4e26\u7522\u751f\u5f37\u52c1\u5c0d\u6578\u4f3c\u7136\u754c\u9650\u7684\u6a21\u578b\u3002", "author": "Michael Tschannen et.al.", "authors": "Michael Tschannen, Andr\u00e9 Susano Pinto, Alexander Kolesnikov", "id": "2411.19722v1", "paper_url": "http://arxiv.org/abs/2411.19722v1", "repo": "null"}}