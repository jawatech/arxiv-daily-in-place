{"2411.10914": {"publish_time": "2024-11-16", "title": "BPO: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment", "paper_summary": "Reinforcement Learning with Human Feedback (RLHF) is the key to the success\nof large language models (LLMs) in recent years. In this work, we first\nintroduce the concepts of knowledge breadth and knowledge depth, which measure\nthe comprehensiveness and depth of an LLM or knowledge source respectively. We\nreveal that the imbalance in the number of prompts and responses can lead to a\npotential disparity in breadth and depth learning within alignment tuning\ndatasets by showing that even a simple uniform method for balancing the number\nof instructions and responses can lead to significant improvements. Building on\nthis, we further propose Balanced Preference Optimization (BPO), designed to\ndynamically augment the knowledge depth of each sample. BPO is motivated by the\nobservation that the usefulness of knowledge varies across samples,\nnecessitating tailored learning of knowledge depth. To achieve this, we\nintroduce gradient-based clustering, estimating the knowledge informativeness\nand usefulness of each augmented sample based on the model's optimization\ndirection. Our experimental results across various benchmarks demonstrate that\nBPO outperforms other baseline methods in alignment tuning while maintaining\ntraining efficiency. Furthermore, we conduct a detailed analysis of each\ncomponent of BPO, providing guidelines for future research in preference data\noptimization.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6210\u529f\u767c\u5c55\u7684\u95dc\u9375\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u9996\u5148\u4ecb\u7d39\u77e5\u8b58\u5ee3\u5ea6\u548c\u77e5\u8b58\u6df1\u5ea6\u7684\u6982\u5ff5\uff0c\u5206\u5225\u7528\u65bc\u8861\u91cf LLM \u6216\u77e5\u8b58\u4f86\u6e90\u7684\u5168\u9762\u6027\u548c\u6df1\u5ea6\u3002\u6211\u5011\u63ed\u9732\u63d0\u793a\u548c\u56de\u61c9\u6578\u91cf\u7684\u4e0d\u5e73\u8861\u53ef\u80fd\u5c0e\u81f4\u5c0d\u9f4a\u8abf\u6574\u8cc7\u6599\u96c6\u4e2d\u5ee3\u5ea6\u548c\u6df1\u5ea6\u5b78\u7fd2\u7684\u6f5b\u5728\u5dee\u7570\uff0c\u4e26\u8aaa\u660e\u5373\u4f7f\u662f\u5e73\u8861\u6307\u4ee4\u548c\u56de\u61c9\u6578\u91cf\u7684\u4e00\u7a2e\u7c21\u55ae\u7d71\u4e00\u65b9\u6cd5\uff0c\u4e5f\u80fd\u5e36\u4f86\u986f\u8457\u7684\u6539\u5584\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u5e73\u8861\u504f\u597d\u6700\u4f73\u5316 (BPO)\uff0c\u65e8\u5728\u52d5\u614b\u589e\u52a0\u6bcf\u500b\u7bc4\u4f8b\u7684\u77e5\u8b58\u6df1\u5ea6\u3002BPO \u7684\u9748\u611f\u4f86\u81ea\u65bc\u4e00\u500b\u89c0\u5bdf\uff1a\u77e5\u8b58\u7684\u6548\u7528\u6703\u56e0\u7bc4\u4f8b\u800c\u7570\uff0c\u56e0\u6b64\u9700\u8981\u5ba2\u88fd\u5316\u5b78\u7fd2\u77e5\u8b58\u6df1\u5ea6\u3002\u70ba\u4e86\u9054\u6210\u9019\u500b\u76ee\u6a19\uff0c\u6211\u5011\u5f15\u5165\u57fa\u65bc\u68af\u5ea6\u7684\u5206\u7fa4\uff0c\u6839\u64da\u6a21\u578b\u7684\u6700\u4f73\u5316\u65b9\u5411\u4f86\u4f30\u8a08\u6bcf\u500b\u5df2\u589e\u52a0\u7bc4\u4f8b\u7684\u77e5\u8b58\u8cc7\u8a0a\u91cf\u548c\u6548\u7528\u3002\u6211\u5011\u5728\u5404\u7a2e\u57fa\u6e96\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cBPO \u5728\u5c0d\u9f4a\u8abf\u6574\u4e2d\u512a\u65bc\u5176\u4ed6\u57fa\u6e96\u65b9\u6cd5\uff0c\u540c\u6642\u7dad\u6301\u8a13\u7df4\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c0d BPO \u7684\u6bcf\u500b\u7d44\u6210\u90e8\u5206\u9032\u884c\u8a73\u7d30\u5206\u6790\uff0c\u70ba\u504f\u597d\u8cc7\u6599\u6700\u4f73\u5316\u7684\u672a\u4f86\u7814\u7a76\u63d0\u4f9b\u6307\u5c0e\u65b9\u91dd\u3002", "author": "Sizhe Wang et.al.", "authors": "Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, Xin Zhang, Tianlong Chen", "id": "2411.10914v1", "paper_url": "http://arxiv.org/abs/2411.10914v1", "repo": "null"}}