{"2412.07730": {"publish_time": "2024-12-10", "title": "STIV: Scalable Text and Image Conditioned Video Generation", "paper_summary": "The field of video generation has made remarkable advancements, yet there\nremains a pressing need for a clear, systematic recipe that can guide the\ndevelopment of robust and scalable models. In this work, we present a\ncomprehensive study that systematically explores the interplay of model\narchitectures, training recipes, and data curation strategies, culminating in a\nsimple and scalable text-image-conditioned video generation method, named STIV.\nOur framework integrates image condition into a Diffusion Transformer (DiT)\nthrough frame replacement, while incorporating text conditioning via a joint\nimage-text conditional classifier-free guidance. This design enables STIV to\nperform both text-to-video (T2V) and text-image-to-video (TI2V) tasks\nsimultaneously. Additionally, STIV can be easily extended to various\napplications, such as video prediction, frame interpolation, multi-view\ngeneration, and long video generation, etc. With comprehensive ablation studies\non T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple\ndesign. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V,\nsurpassing both leading open and closed-source models like CogVideoX-5B, Pika,\nKling, and Gen-3. The same-sized model also achieves a state-of-the-art result\nof 90.1 on VBench I2V task at 512 resolution. By providing a transparent and\nextensible recipe for building cutting-edge video generation models, we aim to\nempower future research and accelerate progress toward more versatile and\nreliable video generation solutions.", "paper_summary_zh": "\u5f71\u7247\u751f\u6210\u9818\u57df\u5df2\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4f46\u4ecd\u8feb\u5207\u9700\u8981\u4e00\u500b\u6e05\u6670\u4e14\u7cfb\u7d71\u5316\u7684\u914d\u65b9\uff0c\u4ee5\u5f15\u5c0e\u7a69\u5065\u4e14\u53ef\u64f4\u5145\u6a21\u578b\u7684\u958b\u767c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u9805\u5168\u9762\u6027\u7684\u7814\u7a76\uff0c\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u6a21\u578b\u67b6\u69cb\u3001\u8a13\u7df4\u914d\u65b9\u548c\u8cc7\u6599\u7b56\u5c55\u7b56\u7565\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u6700\u7d42\u5f62\u6210\u4e00\u7a2e\u7c21\u55ae\u4e14\u53ef\u64f4\u5145\u7684\u6587\u5b57\u5f71\u50cf\u689d\u4ef6\u5f71\u7247\u751f\u6210\u65b9\u6cd5\uff0c\u7a31\u70ba STIV\u3002\u6211\u5011\u7684\u67b6\u69cb\u900f\u904e\u756b\u9762\u66ff\u63db\u5c07\u5f71\u50cf\u689d\u4ef6\u6574\u5408\u5230\u64f4\u6563\u8f49\u63db\u5668 (DiT) \u4e2d\uff0c\u540c\u6642\u900f\u904e\u806f\u5408\u5f71\u50cf\u6587\u5b57\u689d\u4ef6\u5206\u985e\u5668\u81ea\u7531\u5f15\u5c0e\u4f86\u7d0d\u5165\u6587\u5b57\u689d\u4ef6\u3002\u6b64\u8a2d\u8a08\u8b93 STIV \u80fd\u5920\u540c\u6642\u57f7\u884c\u6587\u5b57\u8f49\u5f71\u7247 (T2V) \u548c\u6587\u5b57\u5f71\u50cf\u8f49\u5f71\u7247 (TI2V) \u4efb\u52d9\u3002\u6b64\u5916\uff0cSTIV \u53ef\u4ee5\u8f15\u9b06\u5ef6\u4f38\u81f3\u5404\u7a2e\u61c9\u7528\uff0c\u4f8b\u5982\u5f71\u7247\u9810\u6e2c\u3001\u756b\u9762\u5167\u63d2\u3001\u591a\u8996\u5716\u751f\u6210\u548c\u9577\u5f71\u7247\u751f\u6210\u7b49\u3002\u900f\u904e\u5c0d T2I\u3001T2V \u548c TI2V \u9032\u884c\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\uff0cSTIV \u5c55\u73fe\u51fa\u5f37\u5927\u7684\u6548\u80fd\uff0c\u5118\u7ba1\u5176\u8a2d\u8a08\u7c21\u55ae\u3002\u4e00\u500b\u89e3\u6790\u5ea6\u70ba 512 \u7684 8.7B \u6a21\u578b\u5728 VBench T2V \u4e0a\u9054\u5230 83.1\uff0c\u8d85\u8d8a\u4e86 CogVideoX-5B\u3001Pika\u3001Kling \u548c Gen-3 \u7b49\u9818\u5148\u7684\u958b\u653e\u548c\u9589\u6e90\u6a21\u578b\u3002\u76f8\u540c\u5927\u5c0f\u7684\u6a21\u578b\u5728\u89e3\u6790\u5ea6\u70ba 512 \u7684 VBench I2V \u4efb\u52d9\u4e0a\u4e5f\u9054\u5230\u4e86 90.1 \u7684\u6700\u65b0\u6280\u8853\u6210\u679c\u3002\u900f\u904e\u63d0\u4f9b\u4e00\u500b\u900f\u660e\u4e14\u53ef\u64f4\u5145\u7684\u914d\u65b9\u4f86\u5efa\u69cb\u5c16\u7aef\u7684\u5f71\u7247\u751f\u6210\u6a21\u578b\uff0c\u6211\u5011\u65e8\u5728\u8ce6\u80fd\u672a\u4f86\u7684\u7814\u7a76\uff0c\u4e26\u52a0\u901f\u671d\u5411\u66f4\u901a\u7528\u4e14\u53ef\u9760\u7684\u5f71\u7247\u751f\u6210\u89e3\u6c7a\u65b9\u6848\u9081\u9032\u3002", "author": "Zongyu Lin et.al.", "authors": "Zongyu Lin, Wei Liu, Chen Chen, Jiasen Lu, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Zhengfeng Lai, Liangchen Song, Bowen Zhang, Cha Chen, Yiran Fei, Yifan Jiang, Lezhi Li, Yizhou Sun, Kai-Wei Chang, Yinfei Yang", "id": "2412.07730v1", "paper_url": "http://arxiv.org/abs/2412.07730v1", "repo": "null"}}