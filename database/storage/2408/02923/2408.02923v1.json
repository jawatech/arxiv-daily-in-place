{"2408.02923": {"publish_time": "2024-08-06", "title": "Intermediate direct preference optimization", "paper_summary": "We propose the intermediate direct preference optimization (DPO) method to\ncalculate the DPO loss at selected intermediate layers as an auxiliary loss for\nfinetuning large language models (LLMs). The conventional DPO method fine-tunes\na supervised fine-tuning (SFT) model by calculating the DPO loss using logits\nfrom the final layer. In our intermediate DPO approach, DPO losses are\ncalculated using the logits from K-selected intermediate layers and averaged to\nobtain the intermediate DPO loss. For training the intermediate DPO model, the\nfinal loss is obtained by calculating the weighted sum of the DPO and\nintermediate DPO losses. During inference, the intermediate DPO model decodes\nusing the final layer logits similarly to the conventional DPO model. In\nexperiments using the ultrafeedback dataset, the performance of the\nintermediate DPO model was evaluated using GPT-4. As a result, the intermediate\nDPO model trained using the intermediate DPO loss calculated at the 22nd layer\nof a 32-layer SFT model achieved win rates of 52.5% and 67.5% against the\nconventional DPO and SFT models, respectively, demonstrating the effectiveness\nof the proposed method. Furthermore, we report the relationships among the\nposition of the selected intermediate layers, the number of layers, and\nperformance.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e2d\u9593\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u65b9\u6cd5\uff0c\u4ee5\u5728\u9078\u5b9a\u7684\u4e2d\u9593\u5c64\u8a08\u7b97 DPO \u640d\u5931\uff0c\u4f5c\u70ba\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8f14\u52a9\u640d\u5931\u3002\u50b3\u7d71\u7684 DPO \u65b9\u6cd5\u901a\u904e\u4f7f\u7528\u4f86\u81ea\u6700\u7d42\u5c64\u7684 logit \u8a08\u7b97 DPO \u640d\u5931\uff0c\u5c0d\u76e3\u7763\u5fae\u8abf (SFT) \u6a21\u578b\u9032\u884c\u5fae\u8abf\u3002\u5728\u6211\u5011\u7684\u4e2d\u9593 DPO \u65b9\u6cd5\u4e2d\uff0cDPO \u640d\u5931\u4f7f\u7528\u4f86\u81ea K \u500b\u9078\u5b9a\u4e2d\u9593\u5c64\u7684 logit \u8a08\u7b97\uff0c\u4e26\u53d6\u5e73\u5747\u503c\u4ee5\u7372\u5f97\u4e2d\u9593 DPO \u640d\u5931\u3002\u70ba\u4e86\u8a13\u7df4\u4e2d\u9593 DPO \u6a21\u578b\uff0c\u6700\u7d42\u640d\u5931\u662f\u901a\u904e\u8a08\u7b97 DPO \u548c\u4e2d\u9593 DPO \u640d\u5931\u7684\u52a0\u6b0a\u548c\u4f86\u7372\u5f97\u7684\u3002\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\uff0c\u4e2d\u9593 DPO \u6a21\u578b\u4f7f\u7528\u6700\u7d42\u5c64 logit \u89e3\u78bc\uff0c\u985e\u4f3c\u65bc\u50b3\u7d71\u7684 DPO \u6a21\u578b\u3002\u5728\u4f7f\u7528 ultrafeedback \u8cc7\u6599\u96c6\u7684\u5be6\u9a57\u4e2d\uff0c\u4f7f\u7528 GPT-4 \u8a55\u4f30\u4e86\u4e2d\u9593 DPO \u6a21\u578b\u7684\u6548\u80fd\u3002\u7d50\u679c\uff0c\u4f7f\u7528\u5728 32 \u5c64 SFT \u6a21\u578b\u7684\u7b2c 22 \u5c64\u8a08\u7b97\u7684\u4e2d\u9593 DPO \u640d\u5931\u8a13\u7df4\u7684\u4e2d\u9593 DPO \u6a21\u578b\uff0c\u5206\u5225\u5c0d\u50b3\u7d71 DPO \u548c SFT \u6a21\u578b\u9054\u5230\u4e86 52.5% \u548c 67.5% \u7684\u7372\u52dd\u7387\uff0c\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u5831\u544a\u4e86\u9078\u5b9a\u7684\u4e2d\u9593\u5c64\u7684\u4f4d\u7f6e\u3001\u5c64\u6578\u548c\u6548\u80fd\u4e4b\u9593\u7684\u95dc\u4fc2\u3002", "author": "Atsushi Kojima et.al.", "authors": "Atsushi Kojima", "id": "2408.02923v1", "paper_url": "http://arxiv.org/abs/2408.02923v1", "repo": "null"}}