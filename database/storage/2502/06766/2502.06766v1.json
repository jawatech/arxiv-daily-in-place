{"2502.06766": {"publish_time": "2025-02-10", "title": "Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs", "paper_summary": "There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common long context\nbenchmarks (LM-Eval, AlpacaEval, and RULER).", "paper_summary_zh": "\u96a8\u8457\u5c0d\u8a13\u7df4\u597d\u7684\u8f49\u63db\u5668\u6a21\u578b\u57f7\u884c\u6578\u5341\u842c\u500b\u8f38\u5165\u4ee3\u78bc\u7684\u9700\u6c42\u65e5\u76ca\u589e\u9577\uff0c\u6975\u7aef\u898f\u6a21\u7684\u63a8\u8ad6\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\uff0c\u9019\u963b\u7919\u4e86\u8f49\u63db\u5668\u5728\u5546\u54c1\uff08\u5373\u975e\u8cc7\u6599\u4e2d\u5fc3\u898f\u6a21\uff09\u786c\u9ad4\u4e0a\u7684\u9577\u6587\u8108\u61c9\u7528\u3002\u70ba\u4e86\u89e3\u6c7a\u8207\u5728\u9577\u6587\u8108\u4e0a\u57f7\u884c\u57fa\u65bc\u81ea\u6ce8\u610f\u529b\u8f49\u63db\u5668\u8a9e\u8a00\u6a21\u578b\u76f8\u95dc\u7684\u63a8\u8ad6\u6642\u9593\u6210\u672c\uff0c\u4e26\u4f7f\u5176\u80fd\u5920\u5728\u5ee3\u6cdb\u53ef\u7528\u7684\u786c\u9ad4\u4e0a\u63a1\u7528\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u53ef\u8abf\u6574\u6a5f\u5236\uff0c\u8a72\u6a5f\u5236\u901a\u904e\u4f7f\u7528\u9802\u90e8 k \u9078\u64c7\u6a5f\u5236\u50c5\u95dc\u6ce8\u6bcf\u500b\u751f\u6210\u6b65\u9a5f\u4e2d\u6700\u91cd\u8981\u7684\u4ee3\u78bc\uff0c\u5f9e\u800c\u964d\u4f4e\u4e86\u524d\u5411\u50b3\u905e\u7684\u6210\u672c\u3002\u6211\u5011\u901a\u904e\u5728\u9ad8\u9054 1M \u500b\u4ee3\u78bc\u7684\u4e0a\u4e0b\u6587\u8996\u7a97\u4e0a\u57f7\u884c\u63a8\u8ad6\uff0c\u540c\u6642\u4f7f\u7528\u5927\u7d04 16GB \u7684 GPU RAM\uff0c\u5c55\u793a\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5e36\u4f86\u7684\u6548\u7387\u63d0\u5347\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6a21\u578b\u80fd\u5920\u8655\u7406\u7531\u6e1b\u5c11\u7684\u9375\u548c\u503c\u6578\u91cf\u5f15\u8d77\u7684\u7a00\u758f\u6027\u3002\u901a\u904e\u95dc\u6ce8\u4e0d\u5230 2% \u7684\u8f38\u5165\u4ee3\u78bc\uff0c\u6211\u5011\u5728\u5e38\u898b\u7684\u9577\u6587\u8108\u57fa\u6e96\uff08LM-Eval\u3001AlpacaEval \u548c RULER\uff09\u4e0a\u5be6\u73fe\u4e86\u8d85\u904e 95% \u7684\u6a21\u578b\u6548\u80fd\u3002", "author": "Ryan Synk et.al.", "authors": "Ryan Synk, Monte Hoover, John Kirchenbauer, Neel Jain, Alex Stein, Manli Shu, Josue Melendez Sanchez, Ramani Duraiswami, Tom Goldstein", "id": "2502.06766v1", "paper_url": "http://arxiv.org/abs/2502.06766v1", "repo": "null"}}