{"2409.10955": {"publish_time": "2024-09-17", "title": "Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style", "paper_summary": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by\nincorporating external information into the response generation process.\nHowever, how context-faithful LLMs are and what factors influence LLMs'\ncontext-faithfulness remain largely unexplored. In this study, we investigate\nthe impact of memory strength and evidence presentation on LLMs' receptiveness\nto external evidence. We introduce a method to quantify the memory strength of\nLLMs by measuring the divergence in LLMs' responses to different paraphrases of\nthe same question, which is not considered by previous works. We also generate\nevidence in various styles to evaluate the effects of evidence in different\nstyles. Two datasets are used for evaluation: Natural Questions (NQ) with\npopular questions and popQA featuring long-tail questions. Our results show\nthat for questions with high memory strength, LLMs are more likely to rely on\ninternal memory, particularly for larger LLMs such as GPT-4. On the other hand,\npresenting paraphrased evidence significantly increases LLMs' receptiveness\ncompared to simple repetition or adding details.", "paper_summary_zh": "\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u900f\u904e\u5c07\u5916\u90e8\u8cc7\u8a0a\u7d0d\u5165\u56de\u61c9\u751f\u6210\u7a0b\u5e8f\u4e2d\uff0c\u6539\u5584\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\n\u7136\u800c\uff0cLLM \u7684\u4e0a\u4e0b\u6587\u5fe0\u5be6\u5ea6\u5982\u4f55\u4ee5\u53ca\u54ea\u4e9b\u56e0\u7d20\u6703\u5f71\u97ff LLM \u7684\u4e0a\u4e0b\u6587\u5fe0\u5be6\u5ea6\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u5728\u9019\u500b\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u8a18\u61b6\u5f37\u5ea6\u548c\u8b49\u64da\u5448\u73fe\u5c0d LLM \u5c0d\u5916\u90e8\u8b49\u64da\u7684\u63a5\u53d7\u7a0b\u5ea6\u7684\u5f71\u97ff\u3002\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u65b9\u6cd5\uff0c\u900f\u904e\u6e2c\u91cf LLM \u5c0d\u540c\u4e00\u500b\u554f\u984c\u7684\u4e0d\u540c\u540c\u7fa9\u8a5e\u6539\u5beb\u7684\u56de\u61c9\u7684\u5206\u6b67\uff0c\u4f86\u91cf\u5316 LLM \u7684\u8a18\u61b6\u5f37\u5ea6\uff0c\u9019\u662f\u5148\u524d\u7814\u7a76\u672a\u8003\u616e\u7684\u3002\u6211\u5011\u4e5f\u751f\u6210\u4e0d\u540c\u98a8\u683c\u7684\u8b49\u64da\uff0c\u4ee5\u8a55\u4f30\u4e0d\u540c\u98a8\u683c\u8b49\u64da\u7684\u6548\u679c\u3002\u6211\u5011\u4f7f\u7528\u5169\u500b\u8cc7\u6599\u96c6\u9032\u884c\u8a55\u4f30\uff1a\u5305\u542b\u71b1\u9580\u554f\u984c\u7684\u81ea\u7136\u554f\u984c (NQ) \u548c\u5305\u542b\u9577\u5c3e\u554f\u984c\u7684 popQA\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u5c0d\u65bc\u8a18\u61b6\u5f37\u5ea6\u9ad8\u7684\u554f\u984c\uff0cLLM \u66f4\u53ef\u80fd\u4f9d\u8cf4\u5167\u90e8\u8a18\u61b6\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u8f03\u5927\u7684 LLM\uff0c\u4f8b\u5982 GPT-4\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u8207\u55ae\u7d14\u7684\u91cd\u8907\u6216\u589e\u52a0\u7d30\u7bc0\u76f8\u6bd4\uff0c\u5448\u73fe\u540c\u7fa9\u8a5e\u6539\u5beb\u7684\u8b49\u64da\u6703\u986f\u8457\u589e\u52a0 LLM \u7684\u63a5\u53d7\u5ea6\u3002", "author": "Yuepei Li et.al.", "authors": "Yuepei Li, Kang Zhou, Qiao Qiao, Bach Nguyen, Qing Wang, Qi Li", "id": "2409.10955v1", "paper_url": "http://arxiv.org/abs/2409.10955v1", "repo": "null"}}