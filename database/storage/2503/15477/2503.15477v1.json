{"2503.15477": {"publish_time": "2025-03-19", "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective", "paper_summary": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.", "paper_summary_zh": "<paragraph>\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u7684\u6210\u529f\u8207\u5426\uff0c\u95dc\u9375\u53d6\u6c7a\u65bc\u734e\u52f5\u6a21\u578b\u7684\u54c1\u8cea\u3002\u96d6\u7136\u9019\u500b\u54c1\u8cea\u4e3b\u8981\u900f\u904e\u6e96\u78ba\u6027\u4f86\u8a55\u4f30\uff0c\u4f46\u6e96\u78ba\u6027\u662f\u5426\u80fd\u5b8c\u5168\u6355\u6349\u5230\u4e00\u500b\u6709\u6548\u7684\u734e\u52f5\u6a21\u578b\u4f5c\u70ba\u300c\u8001\u5e2b\u300d\u7684\u7279\u8cea\uff0c\u4ecd\u4e0d\u6e05\u695a\u3002\u6211\u5011\u5f9e\u6700\u4f73\u5316\u7684\u89d2\u5ea6\u4f86\u63a2\u8a0e\u9019\u500b\u554f\u984c\u3002\u9996\u5148\uff0c\u6211\u5011\u8b49\u660e\uff0c\u7121\u8ad6\u734e\u52f5\u6a21\u578b\u6709\u591a\u6e96\u78ba\uff0c\u5982\u679c\u5b83\u5f15\u8d77\u7684\u734e\u52f5\u8b8a\u7570\u6578\u5f88\u4f4e\uff0c\u90a3\u9ebc RLHF \u76ee\u6a19\u5c31\u6703\u9047\u5230\u5e73\u5766\u7684\u640d\u5931\u5730\u5f62\uff08flat landscape\uff09\u3002\u56e0\u6b64\uff0c\u5373\u4f7f\u662f\u5b8c\u5168\u6e96\u78ba\u7684\u734e\u52f5\u6a21\u578b\uff0c\u4e5f\u53ef\u80fd\u5c0e\u81f4\u6975\u5176\u7de9\u6162\u7684\u6700\u4f73\u5316\uff0c\u5176\u8868\u73fe\u4e0d\u5982\u5f15\u8d77\u8f03\u9ad8\u734e\u52f5\u8b8a\u7570\u6578\u7684\u8f03\u4e0d\u6e96\u78ba\u6a21\u578b\u3002\u6211\u5011\u9084\u767c\u73fe\uff0c\u4e00\u500b\u5c0d\u67d0\u500b\u8a9e\u8a00\u6a21\u578b\u6709\u6548\u7684\u734e\u52f5\u6a21\u578b\uff0c\u53ef\u80fd\u6703\u5c0d\u53e6\u4e00\u500b\u8a9e\u8a00\u6a21\u578b\u5f15\u8d77\u4f4e\u734e\u52f5\u8b8a\u7570\u6578\uff0c\u5f9e\u800c\u5c0e\u81f4\u5e73\u5766\u7684\u76ee\u6a19\u640d\u5931\u5730\u5f62\u3002\u9019\u4e9b\u7d50\u679c\u78ba\u7acb\u4e86\u50c5\u6839\u64da\u6e96\u78ba\u6027\u6216\u7368\u7acb\u65bc\u5176\u6307\u5c0e\u7684\u8a9e\u8a00\u6a21\u578b\u4f86\u8a55\u4f30\u734e\u52f5\u6a21\u578b\u7684\u6839\u672c\u9650\u5236\u3002\u4f7f\u7528\u9ad8\u9054 80 \u5104\u53c3\u6578\u6a21\u578b\u7684\u5be6\u9a57\u8b49\u5be6\u4e86\u6211\u5011\u7684\u7406\u8ad6\uff0c\u8b49\u660e\u4e86\u734e\u52f5\u8b8a\u7570\u6578\u3001\u6e96\u78ba\u6027\u548c\u734e\u52f5\u6700\u5927\u5316\u7387\u4e4b\u9593\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u5f37\u8abf\uff0c\u9664\u4e86\u6e96\u78ba\u6027\u4e4b\u5916\uff0c\u734e\u52f5\u6a21\u578b\u9084\u9700\u8981\u5f15\u767c\u8db3\u5920\u7684\u8b8a\u7570\u6578\u624d\u80fd\u5be6\u73fe\u6709\u6548\u7684\u6700\u4f73\u5316\u3002</paragraph>\n", "author": "Noam Razin et.al.", "authors": "Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason D. Lee, Sanjeev Arora", "id": "2503.15477v1", "paper_url": "http://arxiv.org/abs/2503.15477v1", "repo": "null"}}