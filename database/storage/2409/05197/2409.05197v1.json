{"2409.05197": {"publish_time": "2024-09-08", "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "paper_summary": "State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.", "paper_summary_zh": "\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u88ab\u8ba4\u4e3a\u5177\u6709\u8d8a\u6765\u8d8a\u591a\u7684\u4e0d\u540c\u80fd\u529b\uff0c\u4ece\u9605\u8bfb\u7406\u89e3\u3001\u9ad8\u7ea7\u6570\u5b66\u548c\u63a8\u7406\u6280\u80fd\u5230\u62e5\u6709\u79d1\u5b66\u77e5\u8bc6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u5b83\u4eec\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff1a\u8bc6\u522b\u548c\u6574\u5408\u6765\u81ea\u591a\u4e2a\u6587\u672c\u6765\u6e90\u7684\u4fe1\u606f\u7684\u80fd\u529b\u3002\n\u9274\u4e8e\u73b0\u6709\u7684\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u4e2d\u5b58\u5728\u7b80\u5316\u63d0\u793a\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u63d0\u793a\u5141\u8bb8\u6a21\u578b\u7ed5\u8fc7\u63a8\u7406\u8981\u6c42\uff0c\u6211\u4eec\u7740\u624b\u8c03\u67e5 LLM \u662f\u5426\u5bb9\u6613\u5229\u7528\u6b64\u7c7b\u7b80\u5316\u63d0\u793a\u3002\u6211\u4eec\u53d1\u73b0\u8bc1\u636e\u8868\u660e\u5b83\u4eec\u786e\u5b9e\u7ed5\u8fc7\u4e86\u6267\u884c\u591a\u8df3\u63a8\u7406\u7684\u8981\u6c42\uff0c\u4f46\u5b83\u4eec\u8fd9\u6837\u505a\u7684\u65b9\u5f0f\u6bd4\u9488\u5bf9\u7ecf\u8fc7\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (PLM) \u524d\u8f88\u6240\u62a5\u544a\u7684\u66f4\u52a0\u5fae\u5999\u3002\u53d7\u6b64\u53d1\u73b0\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u8df3\u63a8\u7406\u57fa\u51c6\uff0c\u901a\u8fc7\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u63a8\u7406\u94fe\uff0c\u6700\u7ec8\u5f97\u51fa\u9519\u8bef\u7684\u7b54\u6848\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u4e2a\u5f00\u653e\u4e14\u4e13\u6709\u7684\u6700\u5148\u8fdb\u7684 LLM\uff0c\u5e76\u53d1\u73b0\u5b83\u4eec\u6267\u884c\u591a\u8df3\u63a8\u7406\u7684\u6027\u80fd\u53d7\u5230\u5f71\u54cd\uff0c\u5f53\u63d0\u4f9b\u5982\u6b64\u770b\u4f3c\u5408\u7406\u7684\u66ff\u4ee3\u65b9\u6848\u65f6\uff0cF1 \u5206\u6570\u6700\u591a\u4f1a\u76f8\u5bf9\u4e0b\u964d 45%\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u66f4\u6df1\u5165\u7684\u5206\u6790\uff0c\u5e76\u627e\u5230\u4e86\u8bc1\u636e\u8868\u660e\uff0c\u867d\u7136 LLM \u503e\u5411\u4e8e\u5ffd\u7565\u5177\u6709\u8bef\u5bfc\u6027\u7684\u8bcd\u6c47\u63d0\u793a\uff0c\u4f46\u5177\u6709\u8bef\u5bfc\u6027\u7684\u63a8\u7406\u8def\u5f84\u786e\u5b9e\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002", "author": "Neeladri Bhuiya et.al.", "authors": "Neeladri Bhuiya, Viktor Schlegel, Stefan Winkler", "id": "2409.05197v1", "paper_url": "http://arxiv.org/abs/2409.05197v1", "repo": "null"}}