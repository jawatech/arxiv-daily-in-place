{"2501.04436": {"publish_time": "2025-01-08", "title": "Federated Fine-Tuning of LLMs: Framework Comparison and Research Directions", "paper_summary": "Federated learning (FL) provides a privacy-preserving solution for\nfine-tuning pre-trained large language models (LLMs) using distributed private\ndatasets, enabling task-specific adaptation while preserving data privacy.\nHowever, fine-tuning the extensive parameters in LLMs is particularly\nchallenging in resource-constrained federated scenarios due to the significant\ncommunication and computational costs. To gain a deeper understanding of how\nthese challenges can be addressed, this article conducts a comparative analysis\nthree advanced federated LLM (FedLLM) frameworks that integrate knowledge\ndistillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs,\nwhere clients upload model parameters or gradients to enable straightforward\nand effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient\nknowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into\ntwo parts, with one part executed on the client and the other one on the\nserver, to balance the computational load. Each framework is evaluated based on\nkey performance metrics, including model accuracy, communication overhead, and\nclient-side computational load, offering insights into their effectiveness for\nvarious federated fine-tuning scenarios. Through this analysis, we identify\nframework-specific optimization opportunities to enhance the efficiency of\nFedLLMs and discuss broader research directions, highlighting open\nopportunities to better adapt FedLLMs for real-world applications. A use case\nis presented to demonstrate the performance comparison of these three\nframeworks under varying configurations and settings.", "paper_summary_zh": "<paragraph>\u806f\u5408\u5b78\u7fd2 (FL) \u63d0\u4f9b\u4e86\u4e00\u500b\u96b1\u79c1\u4fdd\u8b77\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u4f7f\u7528\u5206\u5e03\u5f0f\u79c1\u6709\u8cc7\u6599\u96c6\u5fae\u8abf\u9810\u5148\u8a13\u7df4\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u540c\u6642\u5728\u4fdd\u8b77\u8cc7\u6599\u96b1\u79c1\u7684\u540c\u6642\u5be6\u73fe\u7279\u5b9a\u4efb\u52d9\u7684\u9069\u61c9\u3002\n\u7136\u800c\uff0c\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u806f\u5408\u5834\u666f\u4e2d\u5fae\u8abf LLM \u4e2d\u7684\u5ee3\u6cdb\u53c3\u6578\u7279\u5225\u5177\u6709\u6311\u6230\u6027\uff0c\u9019\u662f\u56e0\u70ba\u5b83\u6703\u5e36\u4f86\u986f\u8457\u7684\u901a\u8a0a\u548c\u8a08\u7b97\u6210\u672c\u3002\u70ba\u4e86\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u5982\u4f55\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u672c\u6587\u5c0d\u4e09\u500b\u5148\u9032\u7684\u806f\u5408 LLM (FedLLM) \u6846\u67b6\u9032\u884c\u4e86\u6bd4\u8f03\u5206\u6790\uff0c\u9019\u4e9b\u6846\u67b6\u96c6\u6210\u4e86\u77e5\u8b58\u84b8\u993e (KD) \u548c\u5206\u5272\u5b78\u7fd2 (SL) \u4ee5\u6e1b\u8f15\u9019\u4e9b\u554f\u984c\uff1a1) FedLLM\uff0c\u5ba2\u6236\u7aef\u4e0a\u50b3\u6a21\u578b\u53c3\u6578\u6216\u68af\u5ea6\u4ee5\u5be6\u73fe\u76f4\u63a5\u800c\u6709\u6548\u7684\u5fae\u8abf\uff1b2) KD-FedLLM\uff0c\u5b83\u5229\u7528 KD \u901a\u904e\u908f\u8f2f\u503c\u9032\u884c\u6709\u6548\u7684\u77e5\u8b58\u5171\u4eab\uff1b3) Split-FedLLM\uff0c\u5b83\u5c07 LLM \u5206\u6210\u5169\u90e8\u5206\uff0c\u4e00\u90e8\u5206\u5728\u5ba2\u6236\u7aef\u57f7\u884c\uff0c\u53e6\u4e00\u90e8\u5206\u5728\u4f3a\u670d\u5668\u4e0a\u57f7\u884c\uff0c\u4ee5\u5e73\u8861\u8a08\u7b97\u8ca0\u8f09\u3002\u6bcf\u500b\u6846\u67b6\u90fd\u6839\u64da\u95dc\u9375\u6548\u80fd\u6307\u6a19\u9032\u884c\u8a55\u4f30\uff0c\u5305\u62ec\u6a21\u578b\u6e96\u78ba\u5ea6\u3001\u901a\u8a0a\u958b\u92b7\u548c\u5ba2\u6236\u7aef\u8a08\u7b97\u8ca0\u8f09\uff0c\u5f9e\u800c\u6df1\u5165\u4e86\u89e3\u5b83\u5011\u5728\u5404\u7a2e\u806f\u5408\u5fae\u8abf\u5834\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u901a\u904e\u6b64\u5206\u6790\uff0c\u6211\u5011\u78ba\u5b9a\u4e86\u7279\u5b9a\u65bc\u6846\u67b6\u7684\u6700\u4f73\u5316\u6a5f\u6703\uff0c\u4ee5\u63d0\u9ad8 FedLLM \u7684\u6548\u7387\uff0c\u4e26\u8a0e\u8ad6\u4e86\u66f4\u5ee3\u6cdb\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5f37\u8abf\u4e86\u70ba\u5be6\u969b\u61c9\u7528\u66f4\u597d\u5730\u9069\u61c9 FedLLM \u7684\u516c\u958b\u6a5f\u6703\u3002\u63d0\u4f9b\u4e86\u4e00\u500b\u7528\u4f8b\u4f86\u8aaa\u660e\u9019\u4e09\u500b\u6846\u67b6\u5728\u4e0d\u540c\u914d\u7f6e\u548c\u8a2d\u5b9a\u4e0b\u7684\u6548\u80fd\u6bd4\u8f03\u3002</paragraph>", "author": "Na Yan et.al.", "authors": "Na Yan, Yang Su, Yansha Deng, Robert Schober", "id": "2501.04436v1", "paper_url": "http://arxiv.org/abs/2501.04436v1", "repo": "null"}}