{"2405.02754": {"publish_time": "2024-05-04", "title": "Implicit Safe Set Algorithm for Provably Safe Reinforcement Learning", "paper_summary": "Deep reinforcement learning (DRL) has demonstrated remarkable performance in\nmany continuous control tasks. However, a significant obstacle to the\nreal-world application of DRL is the lack of safety guarantees. Although DRL\nagents can satisfy system safety in expectation through reward shaping,\ndesigning agents to consistently meet hard constraints (e.g., safety\nspecifications) at every time step remains a formidable challenge. In contrast,\nexisting work in the field of safe control provides guarantees on persistent\nsatisfaction of hard safety constraints. However, these methods require\nexplicit analytical system dynamics models to synthesize safe control, which\nare typically inaccessible in DRL settings. In this paper, we present a\nmodel-free safe control algorithm, the implicit safe set algorithm, for\nsynthesizing safeguards for DRL agents that ensure provable safety throughout\ntraining. The proposed algorithm synthesizes a safety index (barrier\ncertificate) and a subsequent safe control law solely by querying a black-box\ndynamic function (e.g., a digital twin simulator). Moreover, we theoretically\nprove that the implicit safe set algorithm guarantees finite time convergence\nto the safe set and forward invariance for both continuous-time and\ndiscrete-time systems. We validate the proposed algorithm on the\nstate-of-the-art Safety Gym benchmark, where it achieves zero safety violations\nwhile gaining $95\\% \\pm 9\\%$ cumulative reward compared to state-of-the-art\nsafe DRL methods. Furthermore, the resulting algorithm scales well to\nhigh-dimensional systems with parallel computing.", "paper_summary_zh": "", "author": "Weiye Zhao et.al.", "authors": "Weiye Zhao,Tairan He,Feihan Li,Changliu Liu", "id": "2405.02754v1", "paper_url": "http://arxiv.org/abs/2405.02754v1", "repo": "null"}}