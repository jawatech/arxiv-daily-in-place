{"2407.14212": {"publish_time": "2024-07-19", "title": "Braille-to-Speech Generator: Audio Generation Based on Joint Fine-Tuning of CLIP and Fastspeech2", "paper_summary": "An increasing number of Chinese people are troubled by different degrees of\nvisual impairment, which has made the modal conversion between a single image\nor video frame in the visual field and the audio expressing the same\ninformation a research hotspot. Deep learning technologies such as OCR+Vocoder\nand Im2Wav enable English audio synthesis or image-to-sound matching in a\nself-supervised manner. However, the audio data used for training is limited\nand English is not universal for visually impaired people with different\neducational levels. Therefore, for the sake of solving the problems of data\nvolume and language applicability to improve the reading efficiency of visually\nimpaired people, a set of image-to-speech framework CLIP-KNN-Fastspeech2 based\non the Chinese context was constructed. The framework integrates multiple basic\nmodels and adopts the strategy of independent pre-training and joint\nfine-tuning. First, the Chinese CLIP and Fastspeech2 text-to-speech models were\npre-trained on two public datasets, MUGE and Baker, respectively, and their\nconvergence was verified. Subsequently, joint fine-tuning was performed using a\nself-built Braille image dataset. Experimental results on multiple public\ndatasets such as VGGSound, Flickr8k, ImageHear, and the self-built Braille\ndataset BIT-DP show that the model has improved objective indicators such as\nBLEU4,FAD(Fr\\'echet Audio Distance), WER(Word Error Ratio), and even inference\nspeed. This verifies that the constructed model still has the ability to\nsynthesize high-quality speech under limited data, and also proves the\neffectiveness of the joint training strategy that integrates multiple basic\nmodels.", "paper_summary_zh": "<paragraph>\u8d8a\u4f86\u8d8a\u591a\u7684\u4e2d\u570b\u4eba\u98fd\u53d7\u4e0d\u540c\u7a0b\u5ea6\u7684\u8996\u529b\u969c\u7919\u56f0\u64fe\uff0c\u9019\u4f7f\u5f97\u8996\u89ba\u5834\u666f\u4e2d\u7684\u55ae\u5e45\u5716\u50cf\u6216\u5f71\u7247\u5e40\u8207\u8868\u9054\u76f8\u540c\u8cc7\u8a0a\u7684\u97f3\u8a0a\u4e4b\u9593\u7684\u6a21\u614b\u8f49\u63db\u6210\u70ba\u7814\u7a76\u71b1\u9ede\u3002OCR+Vocoder \u548c Im2Wav \u7b49\u6df1\u5ea6\u5b78\u7fd2\u6280\u8853\u4ee5\u81ea\u76e3\u7763\u7684\u65b9\u5f0f\u5be6\u73fe\u82f1\u6587\u97f3\u8a0a\u5408\u6210\u6216\u5f71\u50cf\u8f49\u8072\u97f3\u7684\u5339\u914d\u3002\u7136\u800c\uff0c\u7528\u65bc\u8a13\u7df4\u7684\u97f3\u8a0a\u8cc7\u6599\u6709\u9650\uff0c\u4e14\u82f1\u6587\u5c0d\u65bc\u4e0d\u540c\u6559\u80b2\u7a0b\u5ea6\u7684\u8996\u969c\u4eba\u58eb\u800c\u8a00\u4e26\u975e\u901a\u7528\u3002\u56e0\u6b64\uff0c\u70ba\u4e86\u89e3\u6c7a\u8cc7\u6599\u91cf\u548c\u8a9e\u8a00\u9069\u7528\u6027\u554f\u984c\u4ee5\u63d0\u5347\u8996\u969c\u4eba\u58eb\u7684\u95b1\u8b80\u6548\u7387\uff0c\u69cb\u5efa\u4e86\u4e00\u5957\u57fa\u65bc\u4e2d\u6587\u8a9e\u5883\u7684\u5f71\u50cf\u8f49\u8a9e\u97f3\u6846\u67b6 CLIP-KNN-Fastspeech2\u3002\u8a72\u6846\u67b6\u6574\u5408\u591a\u500b\u57fa\u790e\u6a21\u578b\uff0c\u63a1\u7528\u7368\u7acb\u9810\u8a13\u7df4\u548c\u806f\u5408\u5fae\u8abf\u7684\u7b56\u7565\u3002\u9996\u5148\uff0c\u5206\u5225\u5728\u5169\u500b\u516c\u958b\u8cc7\u6599\u96c6 MUGE \u548c Baker \u4e0a\u9810\u8a13\u7df4\u4e2d\u6587 CLIP \u548c Fastspeech2 \u6587\u672c\u8f49\u8a9e\u97f3\u6a21\u578b\uff0c\u4e26\u9a57\u8b49\u5176\u6536\u6582\u6027\u3002\u96a8\u5f8c\uff0c\u4f7f\u7528\u81ea\u5efa\u7684\u9ede\u5b57\u5f71\u50cf\u8cc7\u6599\u96c6\u9032\u884c\u806f\u5408\u5fae\u8abf\u3002\u5728 VGGSound\u3001Flickr8k\u3001ImageHear \u7b49\u591a\u500b\u516c\u958b\u8cc7\u6599\u96c6\u548c\u81ea\u5efa\u9ede\u5b57\u8cc7\u6599\u96c6 BIT-DP \u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8a72\u6a21\u578b\u5728 BLEU4\u3001FAD\uff08Fr\u00e9chet Audio Distance\uff09\u3001WER\uff08Word Error Ratio\uff09 \u7b49\u5ba2\u89c0\u6307\u6a19\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u751a\u81f3\u63a8\u7406\u901f\u5ea6\u4e5f\u6709\u6240\u63d0\u5347\u3002\u9019\u9a57\u8b49\u4e86\u6240\u69cb\u5efa\u7684\u6a21\u578b\u5728\u8cc7\u6599\u91cf\u53d7\u9650\u7684\u60c5\u6cc1\u4e0b\u4ecd\u5177\u5099\u5408\u6210\u9ad8\u54c1\u8cea\u8a9e\u97f3\u7684\u80fd\u529b\uff0c\u4e5f\u8b49\u660e\u4e86\u6574\u5408\u591a\u500b\u57fa\u790e\u6a21\u578b\u7684\u806f\u5408\u8a13\u7df4\u7b56\u7565\u7684\u6709\u6548\u6027\u3002</paragraph>", "author": "Chun Xu et.al.", "authors": "Chun Xu, En-Wei Sun", "id": "2407.14212v1", "paper_url": "http://arxiv.org/abs/2407.14212v1", "repo": "null"}}