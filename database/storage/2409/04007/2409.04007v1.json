{"2409.04007": {"publish_time": "2024-09-06", "title": "Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition", "paper_summary": "Speech emotion recognition (SER) classifies human emotions in speech with a\ncomputer model. Recently, performance in SER has steadily increased as deep\nlearning techniques have adapted. However, unlike many domains that use speech\ndata, data for training in the SER model is insufficient. This causes\noverfitting of training of the neural network, resulting in performance\ndegradation. In fact, successful emotion recognition requires an effective\npreprocessing method and a model structure that efficiently uses the number of\nweight parameters. In this study, we propose using eight dataset versions with\ndifferent frequency-time resolutions to search for an effective emotional\nspeech preprocessing method. We propose a 6-layer convolutional neural network\n(CNN) model with efficient channel attention (ECA) to pursue an efficient model\nstructure. In particular, the well-positioned ECA blocks can improve channel\nfeature representation with only a few parameters. With the interactive\nemotional dyadic motion capture (IEMOCAP) dataset, increasing the frequency\nresolution in preprocessing emotional speech can improve emotion recognition\nperformance. Also, ECA after the deep convolution layer can effectively\nincrease channel feature representation. Consequently, the best result (79.37UA\n79.68WA) can be obtained, exceeding the performance of previous SER models.\nFurthermore, to compensate for the lack of emotional speech data, we experiment\nwith multiple preprocessing data methods that augment trainable data\npreprocessed with all different settings from one sample. In the experiment, we\ncan achieve the highest result (80.28UA 80.46WA).", "paper_summary_zh": "\u8a9e\u97f3\u60c5\u7dd2\u8fa8\u8b58 (SER) \u5229\u7528\u96fb\u8166\u6a21\u578b\u5c0d\u4eba\u985e\u5728\u8a9e\u97f3\u4e2d\u7684\u60c5\u7dd2\u9032\u884c\u5206\u985e\u3002\u6700\u8fd1\uff0c\u96a8\u8457\u6df1\u5ea6\u5b78\u7fd2\u6280\u8853\u7684\u61c9\u7528\uff0cSER \u7684\u8868\u73fe\u6301\u7e8c\u63d0\u5347\u3002\u7136\u800c\uff0c\u8207\u8a31\u591a\u4f7f\u7528\u8a9e\u97f3\u8cc7\u6599\u7684\u9818\u57df\u4e0d\u540c\uff0c\u8a13\u7df4 SER \u6a21\u578b\u7684\u8cc7\u6599\u4e0d\u8db3\u3002\u9019\u6703\u9020\u6210\u795e\u7d93\u7db2\u8def\u8a13\u7df4\u904e\u5ea6\u64ec\u5408\uff0c\u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\u3002\u4e8b\u5be6\u4e0a\uff0c\u6210\u529f\u7684\u8a9e\u97f3\u8fa8\u8b58\u9700\u8981\u4e00\u500b\u6709\u6548\u7684\u524d\u8655\u7406\u65b9\u6cd5\u548c\u4e00\u500b\u6709\u6548\u5229\u7528\u6b0a\u91cd\u53c3\u6578\u6578\u91cf\u4e4b\u6a21\u578b\u7d50\u69cb\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4f7f\u7528\u516b\u500b\u5177\u6709\u4e0d\u540c\u983b\u7387\u6642\u9593\u89e3\u6790\u5ea6\u7684\u8cc7\u6599\u96c6\u7248\u672c\u4f86\u641c\u5c0b\u4e00\u500b\u6709\u6548\u7684\u60c5\u7dd2\u5316\u8a9e\u97f3\u524d\u8655\u7406\u65b9\u6cd5\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5177\u6709\u9ad8\u6548\u901a\u9053\u6ce8\u610f\u529b (ECA) \u7684 6 \u5c64\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN) \u6a21\u578b\uff0c\u4ee5\u8ffd\u6c42\u4e00\u500b\u9ad8\u6548\u7684\u6a21\u578b\u7d50\u69cb\u3002\u7279\u5225\u662f\uff0c\u4f4d\u7f6e\u826f\u597d\u7684 ECA \u5340\u584a\u50c5\u4f7f\u7528\u5c11\u6578\u53c3\u6578\u5c31\u80fd\u6539\u5584\u901a\u9053\u7279\u5fb5\u8868\u793a\u3002\u900f\u904e\u4e92\u52d5\u5f0f\u60c5\u7dd2\u5316\u4e8c\u5143\u52d5\u4f5c\u6355\u6349 (IEMOCAP) \u8cc7\u6599\u96c6\uff0c\u63d0\u5347\u524d\u8655\u7406\u60c5\u7dd2\u5316\u8a9e\u97f3\u4e2d\u7684\u983b\u7387\u89e3\u6790\u5ea6\u53ef\u4ee5\u6539\u5584\u60c5\u7dd2\u8fa8\u8b58\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6df1\u5ea6\u5377\u7a4d\u5c64\u4e4b\u5f8c\u7684 ECA \u53ef\u4ee5\u6709\u6548\u63d0\u5347\u901a\u9053\u7279\u5fb5\u8868\u793a\u3002\u56e0\u6b64\uff0c\u53ef\u4ee5\u7372\u5f97\u6700\u4f73\u7d50\u679c (79.37UA 79.68WA)\uff0c\u8d85\u8d8a\u5148\u524d\u7684 SER \u6a21\u578b\u6548\u80fd\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u89e3\u6c7a\u60c5\u7dd2\u5316\u8a9e\u97f3\u8cc7\u6599\u4e0d\u8db3\u7684\u554f\u984c\uff0c\u6211\u5011\u5617\u8a66\u4f7f\u7528\u591a\u91cd\u524d\u8655\u7406\u8cc7\u6599\u65b9\u6cd5\uff0c\u9019\u4e9b\u65b9\u6cd5\u6703\u64f4\u589e\u4f86\u81ea\u4e00\u500b\u7bc4\u4f8b\u7684\u6240\u6709\u4e0d\u540c\u8a2d\u5b9a\u4e2d\u7d93\u904e\u524d\u8655\u7406\u7684\u53ef\u8a13\u7df4\u8cc7\u6599\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u53ef\u4ee5\u7372\u5f97\u6700\u9ad8\u7d50\u679c (80.28UA 80.46WA)\u3002", "author": "Byunggun Kim et.al.", "authors": "Byunggun Kim, Younghun Kwon", "id": "2409.04007v1", "paper_url": "http://arxiv.org/abs/2409.04007v1", "repo": "null"}}