{"2502.09854": {"publish_time": "2025-02-14", "title": "Efficient Multitask Learning in Small Language Models Through Upside-Down Reinforcement Learning", "paper_summary": "In this work, we demonstrate that small language models (SLMs), specifically\na 100M parameter GPT-2 model, can achieve competitive performance in multitask\nprompt generation tasks while requiring only a fraction of the computational\nresources needed by large language models (LLMs). Through a novel combination\nof upside-down reinforcement learning and synthetic data distillation from a\npowerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5%\nof state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite\nbeing up to 80 times smaller, making it highly suitable for\nresource-constrained and real-time applications. This study highlights the\npotential of SLMs as efficient multitask learners in multimodal settings,\nproviding a promising alternative to LLMs for scalable, low-latency\ndeployments.", "paper_summary_zh": "\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86\u5c0f\u8a9e\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u7279\u5225\u662f 100M \u53c3\u6578\u7684 GPT-2 \u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u591a\u4efb\u52d9\u63d0\u793a\u751f\u6210\u4efb\u52d9\u4e2d\u5be6\u73fe\u5177\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\uff0c\u540c\u6642\u53ea\u9700\u8981\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u6240\u9700\u8a08\u7b97\u8cc7\u6e90\u7684\u4e00\u5c0f\u90e8\u5206\u3002\u900f\u904e\u65b0\u7a4e\u7684\u985b\u5012\u5f0f\u5f37\u5316\u5b78\u7fd2\u548c\u4f86\u81ea\u5f37\u5927 LLM Llama-3 \u7684\u5408\u6210\u8cc7\u6599\u8403\u53d6\u7684\u7d44\u5408\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b SLM\uff0c\u5176\u76f8\u95dc\u6027\u5206\u6578\u5728 5% \u4ee5\u5167\uff0c\u5305\u62ec Llama-3\u3001Qwen2 \u548c Mistral\uff0c\u5118\u7ba1\u5b83\u5c0f\u4e86 80 \u500d\uff0c\u4f7f\u5176\u975e\u5e38\u9069\u5408\u8cc7\u6e90\u53d7\u9650\u548c\u5373\u6642\u61c9\u7528\u7a0b\u5f0f\u3002\u9019\u9805\u7814\u7a76\u7a81\u51fa\u4e86 SLM \u4f5c\u70ba\u591a\u6a21\u614b\u8a2d\u5b9a\u4e2d\u9ad8\u6548\u7684\u591a\u4efb\u52d9\u5b78\u7fd2\u8005\u7684\u6f5b\u529b\uff0c\u70ba\u53ef\u64f4\u5145\u3001\u4f4e\u5ef6\u9072\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684 LLM \u66ff\u4ee3\u65b9\u6848\u3002", "author": "Yu-Chen Lin et.al.", "authors": "Yu-Chen Lin, Sanat Sharma, Hari Manikandan, Jayant Kumar, Tracy Holloway King, Jing Zheng", "id": "2502.09854v1", "paper_url": "http://arxiv.org/abs/2502.09854v1", "repo": "null"}}