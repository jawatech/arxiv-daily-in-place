{"2408.14023": {"publish_time": "2024-08-26", "title": "Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks for Short and Long Videos", "paper_summary": "Multi-modal large language models (MLLMs) have demonstrated considerable\npotential across various downstream tasks that require cross-domain knowledge.\nMLLMs capable of processing videos, known as Video-MLLMs, have attracted broad\ninterest in video-language understanding. However, videos, especially long\nvideos, contain more visual tokens than images, making them difficult for LLMs\nto process. Existing works either downsample visual features or extend the LLM\ncontext size, risking the loss of high-resolution information or slowing down\ninference speed. To address these limitations, we apply cross-attention layers\nin the intermediate projector between the visual encoder and the large language\nmodel (LLM). As the naive cross-attention mechanism is insensitive to temporal\norder, we further introduce causal cross-attention masks (CCAMs) within the\ncross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a\nstraightforward two-stage fashion: feature alignment and visual instruction\ntuning. We develop several Video-CCAM models based on LLMs of different sizes\n(4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows\noutstanding performance from short videos to long ones. Among standard video\nbenchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding\nperformances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA,\nMSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos,\nVideo-CCAM models can be directly adapted to long video understanding and still\nachieve exceptional scores despite being trained solely with images and\n16-frame videos. Using 96 frames (6$\\times$ the training number of frames),\nVideo-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among\nall open-source Video-MLLMs, respectively. The code is publicly available in\n\\url{https://github.com/QQ-MM/Video-CCAM}.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLM) \u5df2\u5728\u9700\u8981\u8de8\u9886\u57df\u77e5\u8bc6\u7684\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5de8\u5927\u7684\u6f5c\u529b\u3002\u80fd\u591f\u5904\u7406\u89c6\u9891\u7684 MLLM\uff08\u79f0\u4e3a Video-MLLM\uff09\u5f15\u8d77\u4e86\u4eba\u4eec\u5bf9\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u7684\u5e7f\u6cdb\u5174\u8da3\u3002\u4f46\u662f\uff0c\u89c6\u9891\uff08\u5c24\u5176\u662f\u957f\u89c6\u9891\uff09\u5305\u542b\u6bd4\u56fe\u50cf\u66f4\u591a\u7684\u89c6\u89c9\u6807\u8bb0\uff0c\u8fd9\u4f7f\u5f97 LLM \u96be\u4ee5\u5904\u7406\u3002\u73b0\u6709\u7684\u5de5\u4f5c\u8981\u4e48\u5bf9\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u964d\u91c7\u6837\uff0c\u8981\u4e48\u6269\u5c55 LLM \u4e0a\u4e0b\u6587\u5927\u5c0f\uff0c\u5192\u7740\u4e22\u5931\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\u6216\u964d\u4f4e\u63a8\u7406\u901f\u5ea6\u7684\u98ce\u9669\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5728\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u4e4b\u95f4\u7684\u4e2d\u95f4\u6295\u5f71\u4eea\u4e2d\u5e94\u7528\u4ea4\u53c9\u6ce8\u610f\u5c42\u3002\u7531\u4e8e\u6734\u7d20\u7684\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u5bf9\u65f6\u95f4\u987a\u5e8f\u4e0d\u654f\u611f\uff0c\u6211\u4eec\u5728\u4ea4\u53c9\u6ce8\u610f\u5c42\u4e2d\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u56e0\u679c\u4ea4\u53c9\u6ce8\u610f\u63a9\u7801 (CCAM)\u3002\u8fd9\u4e2a\u540d\u4e3a Video-CCAM \u7684 Video-MLLM \u4ee5\u7b80\u5355\u7684\u4e24\u9636\u6bb5\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff1a\u7279\u5f81\u5bf9\u9f50\u548c\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\u3002\u6211\u4eec\u57fa\u4e8e\u4e0d\u540c\u5927\u5c0f\u7684 LLM\uff084B\u30019B \u548c 14B\uff09\u5f00\u53d1\u4e86\u591a\u4e2a Video-CCAM \u6a21\u578b\u3002\u4e8b\u5b9e\u8bc1\u660e\uff0cVideo-CCAM \u662f\u4e00\u6b3e\u5f3a\u5927\u7684 Video-MLLM\uff0c\u4ece\u77ed\u89c6\u9891\u5230\u957f\u89c6\u9891\u90fd\u8868\u73b0\u51fa\u8272\u3002\u5728 MVBench \u548c VideoChatGPT-QA \u7b49\u6807\u51c6\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideo-CCAM \u8868\u73b0\u51fa\u8272\uff08\u5728 MVBench \u548c TGIF-QA \u4e2d\u6392\u540d\u7b2c 1/2/3 \u4f4d\uff0c\u5728 MSVD-QA\u3001MSRVTT-QA \u548c ActivityNet-QA \u4e2d\u6392\u540d\u7b2c 2/3/4 \u4f4d\uff09\u3002\u5728\u5305\u542b\u957f\u89c6\u9891\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideo-CCAM \u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u9002\u5e94\u957f\u89c6\u9891\u7406\u89e3\uff0c\u5c3d\u7ba1\u4ec5\u4f7f\u7528\u56fe\u50cf\u548c 16 \u5e27\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u4ecd\u80fd\u83b7\u5f97\u51fa\u8272\u7684\u5206\u6570\u3002\u4f7f\u7528 96 \u5e27\uff08\u8bad\u7ec3\u5e27\u6570\u7684 6 \u500d\uff09\uff0cVideo-CCAM \u6a21\u578b\u5728\u6240\u6709\u5f00\u6e90 Video-MLLM \u4e2d\u5206\u522b\u5728 VideoVista \u4e2d\u6392\u540d\u7b2c 1/2/3 \u4f4d\uff0c\u5728 MLVU \u4e2d\u6392\u540d\u7b2c 1/2/4 \u4f4d\u3002\u8be5\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/QQ-MM/Video-CCAM} \u4e2d\u516c\u5f00\u83b7\u5f97\u3002", "author": "Jiajun Fei et.al.", "authors": "Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, Hui Wang", "id": "2408.14023v1", "paper_url": "http://arxiv.org/abs/2408.14023v1", "repo": "https://github.com/qq-mm/video-ccam"}}