{"2412.08737": {"publish_time": "2024-12-11", "title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions", "paper_summary": "Multimodal large language models (MLLMs) have made rapid progress in recent\nyears, yet continue to struggle with low-level visual perception (LLVP) --\nparticularly the ability to accurately describe the geometric details of an\nimage. This capability is crucial for applications in areas such as robotics,\nmedical image analysis, and manufacturing. In this paper, we first introduce\nGeoperception, a benchmark designed to evaluate an MLLM's ability to accurately\ntranscribe 2D geometric information from an image. Using this benchmark, we\ndemonstrate the limitations of leading MLLMs, and then conduct a comprehensive\nempirical study to explore strategies for improving their performance on\ngeometric tasks. Our findings highlight the benefits of certain model\narchitectures, training techniques, and data strategies, including the use of\nhigh-fidelity synthetic data and multi-stage training with a data curriculum.\nNotably, we find that a data curriculum enables models to learn challenging\ngeometry understanding tasks which they fail to learn from scratch. Leveraging\nthese insights, we develop Euclid, a family of models specifically optimized\nfor strong low-level geometric perception. Although purely trained on synthetic\nmultimodal data, Euclid shows strong generalization ability to novel geometry\nshapes. For instance, Euclid outperforms the best closed-source model,\nGemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and\n10.65% on average across all tasks.", "paper_summary_zh": "\u8fd1\u5e7e\u5e74\uff0c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u8fc5\u901f\u9032\u5c55\uff0c\u4f46\u4ecd\u6301\u7e8c\u8207\u4f4e\u968e\u8996\u89ba\u611f\u77e5 (LLVP) \u596e\u6230\uff0c\u5c24\u5176\u662f\u6e96\u78ba\u63cf\u8ff0\u5f71\u50cf\u5e7e\u4f55\u7d30\u7bc0\u7684\u80fd\u529b\u3002\u6b64\u529f\u80fd\u5c0d\u65bc\u6a5f\u5668\u4eba\u3001\u91ab\u5b78\u5f71\u50cf\u5206\u6790\u548c\u88fd\u9020\u7b49\u9818\u57df\u7684\u61c9\u7528\u81f3\u95dc\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u5148\u4ecb\u7d39 Geoperception\uff0c\u4e00\u500b\u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30 MLLM \u5f9e\u5f71\u50cf\u6e96\u78ba\u8f49\u9304 2D \u5e7e\u4f55\u8cc7\u8a0a\u7684\u80fd\u529b\u3002\u4f7f\u7528\u6b64\u57fa\u6e96\uff0c\u6211\u5011\u5c55\u793a\u4e86\u9818\u5148 MLLM \u7684\u9650\u5236\uff0c\u7136\u5f8c\u9032\u884c\u5168\u9762\u7684\u5be6\u8b49\u7814\u7a76\uff0c\u63a2\u8a0e\u6539\u5584\u5176\u5728\u5e7e\u4f55\u4efb\u52d9\u4e0a\u8868\u73fe\u7684\u7b56\u7565\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u7a81\u51fa\u4e86\u7279\u5b9a\u6a21\u578b\u67b6\u69cb\u3001\u8a13\u7df4\u6280\u8853\u548c\u8cc7\u6599\u7b56\u7565\u7684\u512a\u9ede\uff0c\u5305\u62ec\u4f7f\u7528\u9ad8\u4fdd\u771f\u5408\u6210\u8cc7\u6599\u548c\u5177\u6709\u8cc7\u6599\u8ab2\u7a0b\u7684\u591a\u968e\u6bb5\u8a13\u7df4\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe\u8cc7\u6599\u8ab2\u7a0b\u80fd\u8b93\u6a21\u578b\u5b78\u7fd2\u4ed6\u5011\u7121\u6cd5\u5f9e\u982d\u958b\u59cb\u5b78\u7fd2\u7684\u5177\u6709\u6311\u6230\u6027\u7684\u5e7e\u4f55\u7406\u89e3\u4efb\u52d9\u3002\u5229\u7528\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u958b\u767c\u4e86 Euclid\uff0c\u4e00\u500b\u5c08\u9580\u91dd\u5c0d\u5f37\u4f4e\u968e\u5e7e\u4f55\u611f\u77e5\u800c\u6700\u4f73\u5316\u7684\u6a21\u578b\u5bb6\u65cf\u3002\u5118\u7ba1\u7d14\u7cb9\u5728\u5408\u6210\u591a\u6a21\u614b\u8cc7\u6599\u4e0a\u8a13\u7df4\uff0c\u4f46 Euclid \u5c0d\u65b0\u5e7e\u4f55\u5f62\u72c0\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f8b\u5982\uff0cEuclid \u5728\u67d0\u4e9b Geoperception \u57fa\u6e96\u4efb\u52d9\u4e0a\u6bd4\u6700\u4f73\u9589\u6e90\u6a21\u578b Gemini-1.5-Pro \u9ad8\u51fa 58.56%\uff0c\u5728\u6240\u6709\u4efb\u52d9\u4e0a\u7684\u5e73\u5747\u8868\u73fe\u9ad8\u51fa 10.65%\u3002", "author": "Jiarui Zhang et.al.", "authors": "Jiarui Zhang, Ollie Liu, Tianyu Yu, Jinyi Hu, Willie Neiswanger", "id": "2412.08737v1", "paper_url": "http://arxiv.org/abs/2412.08737v1", "repo": "null"}}