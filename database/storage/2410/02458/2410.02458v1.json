{"2410.02458": {"publish_time": "2024-10-03", "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation", "paper_summary": "Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u5176\u5728\u6587\u672c\u6578\u64da\u4e2d\u7684\u591a\u529f\u80fd\u6027\u800c\u805e\u540d\uff0c\n\u6b63\u8d8a\u4f86\u8d8a\u591a\u5730\u88ab\u63a2\u7d22\u5176\u589e\u5f37\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u7684\u6f5b\u529b\uff0c\u9019\u5c0d\u65bc\u6e96\u78ba\u7684\u8a3a\u65b7\u5f71\u50cf\u81f3\u95dc\u91cd\u8981\u3002\u9019\u9805\u7814\u7a76\n\u63a2\u8a0e\u4e86\u901a\u904e\u6574\u5408\u9810\u5148\u8a13\u7df4\u7684 LLM transformer \u584a\u4f86\u589e\u5f37\u7528\u65bc\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u7684 Vision Transformers (ViT)\u3002\u6211\u5011\u7684\u505a\u6cd5\u662f\uff0c\n\u5c07\u51cd\u7d50\u7684 LLM transformer \u584a\u6574\u5408\u5230\u57fa\u65bc ViT \u7684\u6a21\u578b\u7684\u7de8\u78bc\u5668\u4e2d\uff0c\u5f9e\u800c\u5927\u5e45\u63d0\u5347\u4e86\u5404\u7a2e\u91ab\u5b78\u5f71\u50cf\u6a21\u5f0f\u7684\u5206\u5272\u6548\u80fd\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u6df7\u5408\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\n\u7d50\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5fb5\u5b78\u7fd2\uff0c\u4e26\u4f7f\u7528\u591a\u5c3a\u5ea6\u878d\u5408\u584a\u4f86\u5f59\u7e3d\u4e0d\u540c\u5c3a\u5ea6\u7684\u7279\u5fb5\u3002\u589e\u5f37\u5f8c\u7684\u6a21\u578b\u986f\u793a\u51fa\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u5305\u62ec Dice \u8a55\u5206\u5e73\u5747\u5f9e\n0.74 \u63d0\u5347\u81f3 0.79\uff0c\u4ee5\u53ca\u6e96\u78ba\u5ea6\u3001\u7cbe\u78ba\u5ea6\u548c Jaccard \u6307\u6578\u7684\u63d0\u5347\u3002\n\u9019\u4e9b\u7d50\u679c\u8b49\u660e\u4e86\u57fa\u65bc LLM \u7684 transformer \u5728\u512a\u5316\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7a81\u986f\u4e86\u5176\u5927\u5e45\u63d0\u5347\u6a21\u578b\u6e96\u78ba\u5ea6\u548c\u7a69\u5065\u6027\u7684\u6f5b\u529b\u3002\n\u539f\u59cb\u78bc\u548c\u6211\u5011\u7684\u5be6\u4f5c\u53ef\u65bc\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1ahttps://bit.ly/3zf2CVs", "author": "Gurucharan Marthi Krishna Kumar et.al.", "authors": "Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel", "id": "2410.02458v1", "paper_url": "http://arxiv.org/abs/2410.02458v1", "repo": "null"}}