{"2412.19509": {"publish_time": "2024-12-27", "title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models", "paper_summary": "Vision-Language Models (VLMs) have enabled a variety of real-world\napplications. The large parameter size of VLMs brings large memory and\ncomputation overhead which poses significant challenges for deployment.\nPost-Training Quantization (PTQ) is an effective technique to reduce the memory\nand computation overhead. Existing PTQ methods mainly focus on large language\nmodels (LLMs), without considering the differences across other modalities. In\nthis paper, we discover that there is a significant difference in sensitivity\nbetween language and vision tokens in large VLMs. Therefore, treating tokens\nfrom different modalities equally, as in existing PTQ methods, may\nover-emphasize the insensitive modalities, leading to significant accuracy\nloss. To deal with the above issue, we propose a simple yet effective method,\nModality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ\nincorporates the different sensitivities across modalities during the\ncalibration process to minimize the reconstruction loss for better quantization\nparameters. Extensive experiments show that MBQ can significantly improve task\naccuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B\nVLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel\nthat fuses the dequantization and GEMV operators, achieving a 1.4x speedup on\nLLaVA-onevision-7B on the RTX 4090. The code is available at\nhttps://github.com/thu-nics/MBQ.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5df2\u555f\u7528\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u61c9\u7528\u7a0b\u5f0f\u3002VLM \u7684\u5927\u578b\u53c3\u6578\u5927\u5c0f\u5e36\u4f86\u5de8\u5927\u7684\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u8ca0\u64d4\uff0c\u5c0d\u90e8\u7f72\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u8a13\u7df4\u5f8c\u91cf\u5316 (PTQ) \u662f\u4e00\u7a2e\u6709\u6548\u7684\u6280\u8853\uff0c\u53ef\u6e1b\u5c11\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u8ca0\u64d4\u3002\u73fe\u6709\u7684 PTQ \u65b9\u6cd5\u4e3b\u8981\u95dc\u6ce8\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u800c\u4e0d\u8003\u616e\u5176\u4ed6\u6a21\u5f0f\u7684\u5dee\u7570\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u5927\u578b VLM \u4e2d\u7684\u8a9e\u8a00\u548c\u8996\u89ba\u4ee3\u78bc\u4e4b\u9593\u7684\u654f\u611f\u5ea6\u5b58\u5728\u986f\u8457\u5dee\u7570\u3002\u56e0\u6b64\uff0c\u5982\u73fe\u6709 PTQ \u65b9\u6cd5\u4e00\u6a23\uff0c\u5e73\u7b49\u5c0d\u5f85\u4e0d\u540c\u6a21\u5f0f\u7684\u4ee3\u78bc\u53ef\u80fd\u6703\u904e\u5ea6\u5f37\u8abf\u4e0d\u654f\u611f\u7684\u6a21\u5f0f\uff0c\u5c0e\u81f4\u6e96\u78ba\u5ea6\u5927\u5e45\u4e0b\u964d\u3002\u70ba\u4e86\u8655\u7406\u4e0a\u8ff0\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7c21\u55ae\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5373\u5927\u578b VLM \u7684\u6a21\u5f0f\u5e73\u8861\u91cf\u5316 (MBQ)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cMBQ \u5728\u6821\u6e96\u904e\u7a0b\u4e2d\u7d0d\u5165\u8de8\u6a21\u5f0f\u7684\u4e0d\u540c\u654f\u611f\u5ea6\uff0c\u4ee5\u6700\u5c0f\u5316\u91cd\u5efa\u640d\u5931\uff0c\u4ee5\u7372\u5f97\u66f4\u597d\u7684\u91cf\u5316\u53c3\u6578\u3002\u5927\u91cf\u5be6\u9a57\u8868\u660e\uff0c\u8207 SOTA \u57fa\u6e96\u76f8\u6bd4\uff0cMBQ \u53ef\u4ee5\u986f\u8457\u63d0\u9ad8\u4efb\u52d9\u6e96\u78ba\u5ea6\uff0c\u5c0d\u65bc 7B \u5230 70B VLM\uff0c\u5728 W3 \u548c W4A8 \u91cf\u5316\u4e0b\u5206\u5225\u63d0\u9ad8\u4e86 4.4% \u548c 11.6%\u3002\u6b64\u5916\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u4e00\u500b W3 GPU \u6838\u5fc3\uff0c\u5b83\u878d\u5408\u4e86\u53bb\u91cf\u5316\u548c GEMV \u7b97\u5b50\uff0c\u5728 RTX 4090 \u4e0a\u5c0d LLaVA-onevision-7B \u5b9e\u73b0\u4e86 1.4 \u500d\u7684\u52a0\u901f\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/thu-nics/MBQ \u53d6\u5f97\u3002", "author": "Shiyao Li et.al.", "authors": "Shiyao Li, Yingchun Hu, Xuefei Ning, Xihui Liu, Ke Hong, Xiaotao Jia, Xiuhong Li, Yaqi Yan, Pei Ran, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang", "id": "2412.19509v1", "paper_url": "http://arxiv.org/abs/2412.19509v1", "repo": "https://github.com/thu-nics/mbq"}}