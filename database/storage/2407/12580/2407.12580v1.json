{"2407.12580": {"publish_time": "2024-07-17", "title": "E5-V: Universal Embeddings with Multimodal Large Language Models", "paper_summary": "Multimodal large language models (MLLMs) have shown promising advancements in\ngeneral visual and language understanding. However, the representation of\nmultimodal information using MLLMs remains largely unexplored. In this work, we\nintroduce a new framework, E5-V, designed to adapt MLLMs for achieving\nuniversal multimodal embeddings. Our findings highlight the significant\npotential of MLLMs in representing multimodal inputs compared to previous\napproaches. By leveraging MLLMs with prompts, E5-V effectively bridges the\nmodality gap between different types of inputs, demonstrating strong\nperformance in multimodal embeddings even without fine-tuning. We propose a\nsingle modality training approach for E5-V, where the model is trained\nexclusively on text pairs. This method demonstrates significant improvements\nover traditional multimodal training on image-text pairs, while reducing\ntraining costs by approximately 95%. Additionally, this approach eliminates the\nneed for costly multimodal training data collection. Extensive experiments\nacross four types of tasks demonstrate the effectiveness of E5-V. As a\nuniversal multimodal model, E5-V not only achieves but often surpasses\nstate-of-the-art performance in each task, despite being trained on a single\nmodality.", "paper_summary_zh": "\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5728\u4e00\u822c\u8996\u89ba\u548c\u8a9e\u8a00\u7406\u89e3\u65b9\u9762\u5c55\u73fe\u51fa\u6709\u524d\u9014\u7684\u9032\u5c55\u3002\u7136\u800c\uff0c\u4f7f\u7528 MLLM \u8868\u793a\u591a\u6a21\u614b\u8cc7\u8a0a\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u65b0\u7684\u6846\u67b6 E5-V\uff0c\u65e8\u5728\u8abf\u6574 MLLM \u4ee5\u5be6\u73fe\u901a\u7528\u591a\u6a21\u614b\u5d4c\u5165\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u7a81\u986f\u4e86 MLLM \u5728\u8868\u793a\u591a\u6a21\u614b\u8f38\u5165\u65b9\u9762\u7684\u5de8\u5927\u6f5b\u529b\uff0c\u8207\u5148\u524d\u7684\u505a\u6cd5\u76f8\u6bd4\u3002\u900f\u904e\u5229\u7528\u63d0\u793a\u7684 MLLM\uff0cE5-V \u6709\u6548\u5730\u5f4c\u5408\u4e86\u4e0d\u540c\u985e\u578b\u8f38\u5165\u4e4b\u9593\u7684\u6a21\u614b\u5dee\u8ddd\uff0c\u5373\u4f7f\u6c92\u6709\u5fae\u8abf\uff0c\u4e5f\u80fd\u5728\u591a\u6a21\u614b\u5d4c\u5165\u4e2d\u5c55\u73fe\u5f37\u5927\u7684\u6548\u80fd\u3002\u6211\u5011\u70ba E5-V \u63d0\u51fa\u55ae\u4e00\u6a21\u614b\u8a13\u7df4\u65b9\u6cd5\uff0c\u5176\u4e2d\u6a21\u578b\u50c5\u5728\u6587\u5b57\u5c0d\u4e0a\u8a13\u7df4\u3002\u9019\u7a2e\u65b9\u6cd5\u8b49\u660e\u4e86\u8207\u50b3\u7d71\u7684\u591a\u6a21\u614b\u5f71\u50cf\u6587\u5b57\u5c0d\u8a13\u7df4\u76f8\u6bd4\u6709\u986f\u8457\u7684\u9032\u6b65\uff0c\u540c\u6642\u5c07\u8a13\u7df4\u6210\u672c\u964d\u4f4e\u4e86\u7d04 95%\u3002\u6b64\u5916\uff0c\u9019\u7a2e\u65b9\u6cd5\u6d88\u9664\u4e86\u5c0d\u6602\u8cb4\u7684\u591a\u6a21\u614b\u8a13\u7df4\u8cc7\u6599\u6536\u96c6\u7684\u9700\u6c42\u3002\u8de8\u56db\u7a2e\u985e\u578b\u7684\u4efb\u52d9\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 E5-V \u7684\u6709\u6548\u6027\u3002\u4f5c\u70ba\u4e00\u500b\u901a\u7528\u7684\u591a\u6a21\u614b\u6a21\u578b\uff0cE5-V \u4e0d\u50c5\u5728\u6bcf\u500b\u4efb\u52d9\u4e2d\u90fd\u9054\u5230\uff0c\u800c\u4e14\u5e38\u5e38\u8d85\u8d8a\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u5118\u7ba1\u50c5\u5728\u55ae\u4e00\u6a21\u614b\u4e0a\u8a13\u7df4\u3002", "author": "Ting Jiang et.al.", "authors": "Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang", "id": "2407.12580v1", "paper_url": "http://arxiv.org/abs/2407.12580v1", "repo": "https://github.com/kongds/e5-v"}}