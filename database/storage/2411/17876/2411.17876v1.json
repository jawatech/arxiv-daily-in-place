{"2411.17876": {"publish_time": "2024-11-26", "title": "Leveraging Large Language Models and Topic Modeling for Toxicity Classification", "paper_summary": "Content moderation and toxicity classification represent critical tasks with\nsignificant social implications. However, studies have shown that major\nclassification models exhibit tendencies to magnify or reduce biases and\npotentially overlook or disadvantage certain marginalized groups within their\nclassification processes. Researchers suggest that the positionality of\nannotators influences the gold standard labels in which the models learned from\npropagate annotators' bias. To further investigate the impact of annotator\npositionality, we delve into fine-tuning BERTweet and HateBERT on the dataset\nwhile using topic-modeling strategies for content moderation. The results\nindicate that fine-tuning the models on specific topics results in a notable\nimprovement in the F1 score of the models when compared to the predictions\ngenerated by other prominent classification models such as GPT-4,\nPerspectiveAPI, and RewireAPI. These findings further reveal that the\nstate-of-the-art large language models exhibit significant limitations in\naccurately detecting and interpreting text toxicity contrasted with earlier\nmethodologies. Code is available at\nhttps://github.com/aheldis/Toxicity-Classification.git.", "paper_summary_zh": "\u5167\u5bb9\u5be9\u6838\u548c\u6bd2\u6027\u5206\u985e\u4ee3\u8868\u8457\u5177\u6709\u91cd\u5927\u793e\u6703\u610f\u6db5\u7684\u95dc\u9375\u4efb\u52d9\u3002\u7136\u800c\uff0c\u7814\u7a76\u986f\u793a\uff0c\u4e3b\u8981\u7684\u5206\u985e\u6a21\u578b\u5c55\u73fe\u51fa\u653e\u5927\u6216\u7e2e\u5c0f\u504f\u898b\u7684\u50be\u5411\uff0c\u4e26\u53ef\u80fd\u5728\u5206\u985e\u904e\u7a0b\u4e2d\u5ffd\u7565\u6216\u4e0d\u5229\u65bc\u67d0\u4e9b\u908a\u7de3\u5316\u7fa4\u9ad4\u3002\u7814\u7a76\u4eba\u54e1\u5efa\u8b70\uff0c\u8a3b\u89e3\u8005\u7684\u4f4d\u7f6e\u6027\u6703\u5f71\u97ff\u6a21\u578b\u5f9e\u4e2d\u5b78\u7fd2\u7684\u9ec3\u91d1\u6a19\u6e96\u6a19\u7c64\uff0c\u9032\u800c\u50b3\u64ad\u8a3b\u89e3\u8005\u7684\u504f\u898b\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u63a2\u8a0e\u8a3b\u89e3\u8005\u4f4d\u7f6e\u6027\u7684\u5f71\u97ff\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u5fae\u8abf BERTweet \u548c HateBERT \u5728\u8cc7\u6599\u96c6\u4e0a\u7684\u8868\u73fe\uff0c\u540c\u6642\u4f7f\u7528\u4e3b\u984c\u5efa\u6a21\u7b56\u7565\u9032\u884c\u5167\u5bb9\u5be9\u6838\u3002\u7d50\u679c\u986f\u793a\uff0c\u91dd\u5c0d\u7279\u5b9a\u4e3b\u984c\u5fae\u8abf\u6a21\u578b\u6703\u8b93\u6a21\u578b\u7684 F1 \u5206\u6578\u986f\u8457\u63d0\u5347\uff0c\u76f8\u8f03\u65bc GPT-4\u3001PerspectiveAPI \u548c RewireAPI \u7b49\u5176\u4ed6\u5091\u51fa\u5206\u985e\u6a21\u578b\u6240\u7522\u751f\u7684\u9810\u6e2c\u3002\u9019\u4e9b\u767c\u73fe\u9032\u4e00\u6b65\u63ed\u793a\uff0c\u6700\u5148\u9032\u7684\u5927\u8a9e\u8a00\u6a21\u578b\u5728\u6e96\u78ba\u5075\u6e2c\u548c\u8a6e\u91cb\u6587\u5b57\u6bd2\u6027\u65b9\u9762\u5c55\u73fe\u51fa\u986f\u8457\u7684\u9650\u5236\uff0c\u8207\u65e9\u671f\u7684\u65b9\u6cd5\u8ad6\u5f62\u6210\u5c0d\u6bd4\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/aheldis/Toxicity-Classification.git \u53d6\u5f97\u3002", "author": "Haniyeh Ehsani Oskouie et.al.", "authors": "Haniyeh Ehsani Oskouie, Christina Chance, Claire Huang, Margaret Capetz, Elizabeth Eyeson, Majid Sarrafzadeh", "id": "2411.17876v1", "paper_url": "http://arxiv.org/abs/2411.17876v1", "repo": "https://github.com/aheldis/toxicity-classification"}}