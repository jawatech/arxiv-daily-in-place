{"2407.17412": {"publish_time": "2024-07-24", "title": "(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent HyperNetwork", "paper_summary": "Large-scale neural networks have demonstrated remarkable performance in\ndifferent domains like vision and language processing, although at the cost of\nmassive computation resources. As illustrated by compression literature,\nstructural model pruning is a prominent algorithm to encourage model\nefficiency, thanks to its acceleration-friendly sparsity patterns. One of the\nkey questions of structural pruning is how to estimate the channel\nsignificance. In parallel, work on data-centric AI has shown that\nprompting-based techniques enable impressive generalization of large language\nmodels across diverse downstream tasks. In this paper, we investigate a\ncharming possibility - \\textit{leveraging visual prompts to capture the channel\nimportance and derive high-quality structural sparsity}. To this end, we\npropose a novel algorithmic framework, namely \\texttt{PASS}. It is a tailored\nhyper-network to take both visual prompts and network weight statistics as\ninput, and output layer-wise channel sparsity in a recurrent manner. Such\ndesigns consider the intrinsic channel dependency between layers. Comprehensive\nexperiments across multiple network architectures and six datasets demonstrate\nthe superiority of \\texttt{PASS} in locating good structural sparsity. For\nexample, at the same FLOPs level, \\texttt{PASS} subnetworks achieve $1\\%\\sim\n3\\%$ better accuracy on Food101 dataset; or with a similar performance of\n$80\\%$ accuracy, \\texttt{PASS} subnetworks obtain $0.35\\times$ more speedup\nthan the baselines.", "paper_summary_zh": "\u5927\u578b\u795e\u7ecf\u7db2\u8def\u5728\u4e0d\u540c\u7684\u9818\u57df\uff0c\u4f8b\u5982\u8996\u89ba\u548c\u8a9e\u8a00\u8655\u7406\u4e2d\uff0c\u5c55\u73fe\u4e86\u975e\u51e1\u7684\u6548\u80fd\uff0c\u5118\u7ba1\u662f\u4ee5\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\u70ba\u4ee3\u50f9\u3002\u6b63\u5982\u58d3\u7e2e\u6587\u737b\u6240\u8aaa\u660e\u7684\uff0c\u7d50\u69cb\u6a21\u578b\u526a\u679d\u662f\u4e00\u7a2e\u7a81\u51fa\u7684\u6f14\u7b97\u6cd5\uff0c\u7528\u65bc\u63d0\u5347\u6a21\u578b\u6548\u7387\uff0c\u9019\u8981\u6b78\u529f\u65bc\u5176\u6709\u5229\u65bc\u52a0\u901f\u7684\u7a00\u758f\u6a21\u5f0f\u3002\u7d50\u69cb\u526a\u679d\u7684\u95dc\u9375\u554f\u984c\u4e4b\u4e00\u662f\u5982\u4f55\u4f30\u8a08\u901a\u9053\u91cd\u8981\u6027\u3002\u8207\u6b64\u540c\u6642\uff0c\u8cc7\u6599\u4e2d\u5fc3 AI \u7684\u7814\u7a76\u8868\u660e\uff0c\u57fa\u65bc\u63d0\u793a\u7684\u6280\u8853\u80fd\u5920\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u4e2d\u9032\u884c\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6cdb\u5316\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4e00\u500b\u8ff7\u4eba\u7684\u53ef\u80fd\u6027\u2014\u2014\u5229\u7528\u8996\u89ba\u63d0\u793a\u4f86\u64f7\u53d6\u901a\u9053\u91cd\u8981\u6027\uff0c\u4e26\u63a8\u5c0e\u51fa\u9ad8\u54c1\u8cea\u7684\u7d50\u69cb\u7a00\u758f\u6027\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u6f14\u7b97\u6cd5\u67b6\u69cb\uff0c\u5373 \\texttt{PASS}\u3002\u5b83\u662f\u4e00\u500b\u91cf\u8eab\u6253\u9020\u7684\u8d85\u7db2\u8def\uff0c\u7528\u65bc\u5c07\u8996\u89ba\u63d0\u793a\u548c\u7db2\u8def\u6b0a\u91cd\u7d71\u8a08\u8cc7\u6599\u4f5c\u70ba\u8f38\u5165\uff0c\u4e26\u4ee5\u905e\u8ff4\u65b9\u5f0f\u8f38\u51fa\u9010\u5c64\u901a\u9053\u7a00\u758f\u6027\u3002\u6b64\u985e\u8a2d\u8a08\u8003\u616e\u4e86\u5c64\u4e4b\u9593\u7684\u5167\u5728\u901a\u9053\u4f9d\u8cf4\u6027\u3002\u8de8\u591a\u500b\u7db2\u8def\u67b6\u69cb\u548c\u516d\u500b\u8cc7\u6599\u96c6\u7684\u7d9c\u5408\u5be6\u9a57\u8b49\u660e\u4e86 \\texttt{PASS} \u5728\u5b9a\u4f4d\u826f\u597d\u7d50\u69cb\u7a00\u758f\u6027\u65b9\u9762\u7684\u512a\u8d8a\u6027\u3002\u4f8b\u5982\uff0c\u5728\u76f8\u540c\u7684 FLOP \u5c64\u7d1a\uff0c\\texttt{PASS} \u5b50\u7db2\u8def\u5728 Food101 \u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u4e86 $1\\%\\sim 3\\%$ \u7684\u66f4\u4f73\u6e96\u78ba\u5ea6\uff1b\u6216\u8005\u5728\u5177\u6709\u76f8\u4f3c\u7684 $80\\%$ \u6e96\u78ba\u5ea6\u6548\u80fd\u4e0b\uff0c\\texttt{PASS} \u5b50\u7db2\u8def\u6bd4\u57fa\u6e96\u5feb\u4e86 $0.35\\times$ \u500d\u3002", "author": "Tianjin Huang et.al.", "authors": "Tianjin Huang, Fang Meng, Li Shen, Fan Liu, Yulong Pei, Mykola Pechenizkiy, Shiwei Liu, Tianlong Chen", "id": "2407.17412v1", "paper_url": "http://arxiv.org/abs/2407.17412v1", "repo": "null"}}