{"2407.10949": {"publish_time": "2024-07-15", "title": "Representing Rule-based Chatbots with Transformers", "paper_summary": "Transformer-based chatbots can conduct fluent, natural-sounding\nconversations, but we have limited understanding of the mechanisms underlying\ntheir behavior. Prior work has taken a bottom-up approach to understanding\nTransformers by constructing Transformers for various synthetic and formal\nlanguage tasks, such as regular expressions and Dyck languages. However, it is\nnot obvious how to extend this approach to understand more naturalistic\nconversational agents. In this work, we take a step in this direction by\nconstructing a Transformer that implements the ELIZA program, a classic,\nrule-based chatbot. ELIZA illustrates some of the distinctive challenges of the\nconversational setting, including both local pattern matching and long-term\ndialog state tracking. We build on constructions from prior work -- in\nparticular, for simulating finite-state automata -- showing how simpler\nconstructions can be composed and extended to give rise to more sophisticated\nbehavior. Next, we train Transformers on a dataset of synthetically generated\nELIZA conversations and investigate the mechanisms the models learn. Our\nanalysis illustrates the kinds of mechanisms these models tend to prefer -- for\nexample, models favor an induction head mechanism over a more precise, position\nbased copying mechanism; and using intermediate generations to simulate\nrecurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing\nan explicit connection between neural chatbots and interpretable, symbolic\nmechanisms, our results offer a new setting for mechanistic analysis of\nconversational agents.", "paper_summary_zh": "\u57fa\u65bc Transformer \u7684\u804a\u5929\u6a5f\u5668\u4eba\u53ef\u4ee5\u9032\u884c\u6d41\u66a2\u3001\u807d\u8d77\u4f86\u5f88\u81ea\u7136\u7684\u5c0d\u8a71\uff0c\u4f46\u6211\u5011\u5c0d\u65bc\u5176\u884c\u70ba\u80cc\u5f8c\u6a5f\u5236\u7684\u4e86\u89e3\u6709\u9650\u3002\u5148\u524d\u7684\u5de5\u4f5c\u5df2\u63a1\u53d6\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u5f0f\u4f86\u4e86\u89e3 Transformer\uff0c\u65b9\u6cd5\u662f\u70ba\u5404\u7a2e\u5408\u6210\u548c\u5f62\u5f0f\u8a9e\u8a00\u4efb\u52d9\uff08\u4f8b\u5982\u6b63\u898f\u8868\u9054\u5f0f\u548c Dyck \u8a9e\u8a00\uff09\u5efa\u69cb Transformer\u3002\u7136\u800c\uff0c\u5982\u4f55\u5ef6\u4f38\u6b64\u65b9\u6cd5\u4f86\u4e86\u89e3\u66f4\u81ea\u7136\u7684\u5c0d\u8a71\u4ee3\u7406\u7a0b\u5f0f\u4e26\u4e0d\u660e\u986f\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5efa\u69cb\u4e00\u500b\u5be6\u4f5c ELIZA \u7a0b\u5f0f\uff08\u4e00\u7a2e\u7d93\u5178\u7684\u57fa\u65bc\u898f\u5247\u7684\u804a\u5929\u6a5f\u5668\u4eba\uff09\u7684 Transformer\uff0c\u671d\u9019\u500b\u65b9\u5411\u9081\u51fa\u4e86\u4e00\u6b65\u3002ELIZA \u8aaa\u660e\u4e86\u5c0d\u8a71\u8a2d\u5b9a\u4e2d\u7684\u4e00\u4e9b\u7368\u7279\u6311\u6230\uff0c\u5305\u62ec\u5c40\u90e8\u6a21\u5f0f\u6bd4\u5c0d\u548c\u9577\u671f\u5c0d\u8a71\u72c0\u614b\u8ffd\u8e64\u3002\u6211\u5011\u5efa\u7acb\u5728\u5148\u524d\u5de5\u4f5c\u7684\u5efa\u69cb\u4e4b\u4e0a\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u6a21\u64ec\u6709\u9650\u72c0\u614b\u81ea\u52d5\u6a5f\uff0c\u5c55\u793a\u5982\u4f55\u5c07\u8f03\u7c21\u55ae\u7684\u5efa\u69cb\u7d44\u5408\u4e26\u5ef6\u4f38\uff0c\u4ee5\u7522\u751f\u66f4\u7cbe\u5bc6\u7684\u884c\u70ba\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u5728\u4e00\u500b\u5408\u6210\u7522\u751f\u7684 ELIZA \u5c0d\u8a71\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4 Transformer\uff0c\u4e26\u63a2\u8a0e\u6a21\u578b\u5b78\u7fd2\u7684\u6a5f\u5236\u3002\u6211\u5011\u7684\u5206\u6790\u8aaa\u660e\u4e86\u9019\u4e9b\u6a21\u578b\u50be\u5411\u65bc\u504f\u597d\u7684\u6a5f\u5236\u985e\u578b\uff0c\u4f8b\u5982\uff0c\u6a21\u578b\u504f\u597d\u6b78\u7d0d\u982d\u6a5f\u5236\uff0c\u800c\u975e\u66f4\u7cbe\u78ba\u7684\u57fa\u65bc\u4f4d\u7f6e\u7684\u8907\u88fd\u6a5f\u5236\uff1b\u4ee5\u53ca\u4f7f\u7528\u4e2d\u9593\u751f\u6210\u4f86\u6a21\u64ec\u905e\u8ff4\u8cc7\u6599\u7d50\u69cb\uff0c\u4f8b\u5982 ELIZA \u7684\u8a18\u61b6\u6a5f\u5236\u3002\u7e3d\u7684\u4f86\u8aaa\uff0c\u900f\u904e\u5728\u795e\u7d93\u804a\u5929\u6a5f\u5668\u4eba\u8207\u53ef\u89e3\u91cb\u7684\u7b26\u865f\u6a5f\u5236\u4e4b\u9593\u5efa\u7acb\u660e\u78ba\u7684\u9023\u7d50\uff0c\u6211\u5011\u7684\u7d50\u679c\u70ba\u5c0d\u8a71\u4ee3\u7406\u7a0b\u5f0f\u7684\u6a5f\u5236\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u8a2d\u5b9a\u3002", "author": "Dan Friedman et.al.", "authors": "Dan Friedman, Abhishek Panigrahi, Danqi Chen", "id": "2407.10949v1", "paper_url": "http://arxiv.org/abs/2407.10949v1", "repo": "https://github.com/princeton-nlp/eliza-transformer"}}