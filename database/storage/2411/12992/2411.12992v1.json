{"2411.12992": {"publish_time": "2024-11-20", "title": "MemoryFormer: Minimize Transformer Computation by Removing Fully-Connected Layers", "paper_summary": "In order to reduce the computational complexity of large language models,\ngreat efforts have been made to to improve the efficiency of transformer models\nsuch as linear attention and flash-attention. However, the model size and\ncorresponding computational complexity are constantly scaled up in pursuit of\nhigher performance. In this work, we present MemoryFormer, a novel transformer\narchitecture which significantly reduces the computational complexity (FLOPs)\nfrom a new perspective. We eliminate nearly all the computations of the\ntransformer model except for the necessary computation required by the\nmulti-head attention operation. This is made possible by utilizing an\nalternative method for feature transformation to replace the linear projection\nof fully-connected layers. Specifically, we first construct a group of\nin-memory lookup tables that store a large amount of discrete vectors to\nreplace the weight matrix used in linear projection. We then use a hash\nalgorithm to retrieve a correlated subset of vectors dynamically based on the\ninput embedding. The retrieved vectors combined together will form the output\nembedding, which provides an estimation of the result of matrix multiplication\noperation in a fully-connected layer. Compared to conducting matrix\nmultiplication, retrieving data blocks from memory is a much cheaper operation\nwhich requires little computations. We train MemoryFormer from scratch and\nconduct extensive experiments on various benchmarks to demonstrate the\neffectiveness of the proposed model.", "paper_summary_zh": "\u4e3a\u4e86\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\n\u5728\u63d0\u9ad8 Transformer \u6a21\u578b\u7684\u6548\u7387\u65b9\u9762\uff0c\u4f8b\u5982\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u95ea\u5b58\u6ce8\u610f\u529b\uff0c\u5df2\u7ecf\u505a\u51fa\u4e86\u5de8\u5927\u7684\u52aa\u529b\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u8ffd\u6c42\u66f4\u9ad8\u7684\u6027\u80fd\uff0c\u6a21\u578b\u5927\u5c0f\u548c\u76f8\u5e94\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0d\u65ad\u6269\u5927\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MemoryFormer\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684 Transformer \u67b6\u6784\uff0c\u5b83\u4ece\u4e00\u4e2a\u65b0\u7684\u89d2\u5ea6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6 (FLOP)\u3002\u6211\u4eec\u6d88\u9664\u4e86 Transformer \u6a21\u578b\u51e0\u4e4e\u6240\u6709\u8ba1\u7b97\uff0c\u9664\u4e86\u591a\u5934\u6ce8\u610f\u529b\u64cd\u4f5c\u6240\u9700\u7684\u5fc5\u8981\u8ba1\u7b97\u3002\u8fd9\u662f\u901a\u8fc7\u5229\u7528\u4e00\u79cd\u66ff\u4ee3\u7684\u7279\u5f81\u8f6c\u6362\u65b9\u6cd5\u6765\u66ff\u6362\u5168\u8fde\u63a5\u5c42\u7684\u7ebf\u6027\u6295\u5f71\u800c\u5b9e\u73b0\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e00\u7ec4\u5185\u5b58\u4e2d\u67e5\u627e\u8868\uff0c\u5b58\u50a8\u5927\u91cf\u7684\u79bb\u6563\u5411\u91cf\u6765\u66ff\u6362\u7ebf\u6027\u6295\u5f71\u4e2d\u4f7f\u7528\u7684\u6743\u91cd\u77e9\u9635\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u54c8\u5e0c\u7b97\u6cd5\u6839\u636e\u8f93\u5165\u5d4c\u5165\u52a8\u6001\u5730\u68c0\u7d22\u4e00\u4e2a\u76f8\u5173\u7684\u5411\u91cf\u5b50\u96c6\u3002\u68c0\u7d22\u5230\u7684\u5411\u91cf\u7ec4\u5408\u5728\u4e00\u8d77\u5c06\u5f62\u6210\u8f93\u51fa\u5d4c\u5165\uff0c\u5b83\u63d0\u4f9b\u4e86\u5168\u8fde\u63a5\u5c42\u4e2d\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\u7ed3\u679c\u7684\u4f30\u8ba1\u3002\u4e0e\u6267\u884c\u77e9\u9635\u4e58\u6cd5\u76f8\u6bd4\uff0c\u4ece\u5185\u5b58\u4e2d\u68c0\u7d22\u6570\u636e\u5757\u662f\u4e00\u4e2a\u5f00\u9500\u5c0f\u5f97\u591a\u7684\u64cd\u4f5c\uff0c\u51e0\u4e4e\u4e0d\u9700\u8981\u8ba1\u7b97\u3002\u6211\u4eec\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3 MemoryFormer\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc1\u660e\u6240\u63d0\u51fa\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "author": "Ning Ding et.al.", "authors": "Ning Ding, Yehui Tang, Haochen Qin, Zhenli Zhou, Chao Xu, Lin Li, Kai Han, Heng Liao, Yunhe Wang", "id": "2411.12992v1", "paper_url": "http://arxiv.org/abs/2411.12992v1", "repo": "null"}}