{"2410.03249": {"publish_time": "2024-10-04", "title": "How much can we forget about Data Contamination?", "paper_summary": "The leakage of benchmark data into the training data has emerged as a\nsignificant challenge for evaluating the capabilities of large language models\n(LLMs). In this work, we use experimental evidence and theoretical estimates to\nchallenge the common assumption that small-scale contamination renders\nbenchmark evaluations invalid. First, we experimentally quantify the magnitude\nof benchmark overfitting based on scaling along three dimensions: The number of\nmodel parameters (up to 1.6B), the number of times an example is seen (up to\n144), and the number of training tokens (up to 40B). We find that if model and\ndata follow the Chinchilla scaling laws, minor contamination indeed leads to\noverfitting. At the same time, even 144 times of contamination can be forgotten\nif the training data is scaled beyond five times Chinchilla, a regime\ncharacteristic of many modern LLMs. We then derive a simple theory of example\nforgetting via cumulative weight decay. It allows us to bound the number of\ngradient steps required to forget past data for any training run where we know\nthe hyperparameters of AdamW. This indicates that many LLMs, including Llama 3,\nhave forgotten the data seen at the beginning of training. Experimentally, we\ndemonstrate that forgetting occurs faster than what is predicted by our bounds.\nTaken together, our results suggest that moderate amounts of contamination can\nbe forgotten at the end of realistically scaled training runs.", "paper_summary_zh": "\u57fa\u6e96\u8cc7\u6599\u5916\u6d29\u5230\u8a13\u7df4\u8cc7\u6599\u4e2d\uff0c\u5df2\u6210\u70ba\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u529b\u7684\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u5be6\u9a57\u8b49\u64da\u548c\u7406\u8ad6\u4f30\u8a08\uff0c\u4f86\u6311\u6230\u5c0f\u898f\u6a21\u6c61\u67d3\u6703\u8b93\u57fa\u6e96\u8a55\u4f30\u5931\u6548\u7684\u666e\u904d\u5047\u8a2d\u3002\u9996\u5148\uff0c\u6211\u5011\u6839\u64da\u4e09\u500b\u9762\u5411\u7684\u898f\u6a21\u5316\uff0c\u5be6\u9a57\u91cf\u5316\u57fa\u6e96\u904e\u5ea6\u64ec\u5408\u7684\u5e45\u5ea6\uff1a\u6a21\u578b\u53c3\u6578\u6578\u91cf (\u6700\u9ad8 1.6B)\u3001\u7bc4\u4f8b\u88ab\u770b\u898b\u7684\u6b21\u6578 (\u6700\u9ad8 144) \u548c\u8a13\u7df4\u6b0a\u6756\u6578\u91cf (\u6700\u9ad8 40B)\u3002\u6211\u5011\u767c\u73fe\uff0c\u5982\u679c\u6a21\u578b\u548c\u8cc7\u6599\u9075\u5faa Chinchilla \u7684\u898f\u6a21\u5316\u6cd5\u5247\uff0c\u8f15\u5fae\u7684\u6c61\u67d3\u78ba\u5be6\u6703\u5c0e\u81f4\u904e\u5ea6\u64ec\u5408\u3002\u540c\u6642\uff0c\u5982\u679c\u8a13\u7df4\u8cc7\u6599\u898f\u6a21\u5316\u8d85\u904e Chinchilla \u7684\u4e94\u500d\uff0c\u5373\u4f7f 144 \u500d\u7684\u6c61\u67d3\u4e5f\u80fd\u88ab\u907a\u5fd8\uff0c\u800c\u9019\u6b63\u662f\u8a31\u591a\u73fe\u4ee3 LLM \u7684\u7279\u5fb5\u3002\u7136\u5f8c\uff0c\u6211\u5011\u900f\u904e\u7d2f\u7a4d\u6b0a\u91cd\u8870\u6e1b\uff0c\u63a8\u5c0e\u51fa\u4e00\u500b\u7c21\u55ae\u7684\u7bc4\u4f8b\u907a\u5fd8\u7406\u8ad6\u3002\u9019\u8b93\u6211\u5011\u80fd\u5920\u9650\u5236\u5fd8\u8a18\u904e\u53bb\u8cc7\u6599\u6240\u9700\u7684\u68af\u5ea6\u6b65\u9a5f\u6578\u91cf\uff0c\u9069\u7528\u65bc\u6211\u5011\u5df2\u77e5 AdamW \u8d85\u53c3\u6578\u7684\u4efb\u4f55\u8a13\u7df4\u57f7\u884c\u3002\u9019\u8868\u793a\u8a31\u591a LLM\uff0c\u5305\u62ec Llama 3\uff0c\u90fd\u5df2\u5fd8\u8a18\u5728\u8a13\u7df4\u958b\u59cb\u6642\u770b\u5230\u7684\u8cc7\u6599\u3002\u900f\u904e\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u907a\u5fd8\u7684\u901f\u5ea6\u6bd4\u6211\u5011\u8a2d\u5b9a\u7684\u754c\u9650\u9810\u6e2c\u5f97\u66f4\u5feb\u3002\u7d9c\u5408\u6211\u5011\u7684\u7d50\u679c\uff0c\u5efa\u8b70\u5728\u5be6\u969b\u898f\u6a21\u7684\u8a13\u7df4\u57f7\u884c\u7d50\u675f\u6642\uff0c\u53ef\u4ee5\u907a\u5fd8\u9069\u5ea6\u7684\u6c61\u67d3\u91cf\u3002", "author": "Sebastian Bordt et.al.", "authors": "Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg", "id": "2410.03249v1", "paper_url": "http://arxiv.org/abs/2410.03249v1", "repo": "null"}}