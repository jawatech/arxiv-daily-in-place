{"2405.17025": {"publish_time": "2024-05-27", "title": "SWAT: Scalable and Efficient Window Attention-based Transformers Acceleration on FPGAs", "paper_summary": "Efficiently supporting long context length is crucial for Transformer models.\nThe quadratic complexity of the self-attention computation plagues traditional\nTransformers. Sliding window-based static sparse attention mitigates the\nproblem by limiting the attention scope of the input tokens, reducing the\ntheoretical complexity from quadratic to linear. Although the sparsity induced\nby window attention is highly structured, it does not align perfectly with the\nmicroarchitecture of the conventional accelerators, leading to suboptimal\nimplementation. In response, we propose a dataflow-aware FPGA-based accelerator\ndesign, SWAT, that efficiently leverages the sparsity to achieve scalable\nperformance for long input. The proposed microarchitecture is based on a design\nthat maximizes data reuse by using a combination of row-wise dataflow, kernel\nfusion optimization, and an input-stationary design considering the distributed\nmemory and computation resources of FPGA. Consequently, it achieves up to\n22$\\times$ and 5.7$\\times$ improvement in latency and energy efficiency\ncompared to the baseline FPGA-based accelerator and 15$\\times$ energy\nefficiency compared to GPU-based solution.", "paper_summary_zh": "\u6709\u6548\u652f\u63f4\u9577\u80cc\u666f\u9577\u5ea6\u5c0d Transformer \u6a21\u578b\u81f3\u95dc\u91cd\u8981\u3002\n\u81ea\u6ce8\u610f\u529b\u904b\u7b97\u7684\u4e8c\u6b21\u8907\u96dc\u5ea6\u56f0\u64fe\u8457\u50b3\u7d71 Transformer\u3002\n\u57fa\u65bc\u6ed1\u52d5\u8996\u7a97\u7684\u975c\u614b\u7a00\u758f\u6ce8\u610f\u529b\u900f\u904e\u9650\u5236\u8f38\u5165\u6a19\u8a18\u7684\u6ce8\u610f\u529b\u7bc4\u570d\u4f86\u6e1b\u8f15\u554f\u984c\uff0c\u5c07\u7406\u8ad6\u8907\u96dc\u5ea6\u5f9e\u4e8c\u6b21\u964d\u4f4e\u5230\u7dda\u6027\u3002\n\u96d6\u7136\u8996\u7a97\u6ce8\u610f\u529b\u6240\u5f15\u767c\u7684\u7a00\u758f\u6027\u9ad8\u5ea6\u7d50\u69cb\u5316\uff0c\u4f46\u5b83\u8207\u50b3\u7d71\u52a0\u901f\u5668\u7684\u5fae\u67b6\u69cb\u4e26\u4e0d\u5b8c\u5168\u4e00\u81f4\uff0c\u5c0e\u81f4\u6b21\u4f73\u5be6\u4f5c\u3002\n\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u8cc7\u6599\u6d41\u7a0b\u611f\u77e5\u7684\u57fa\u65bc FPGA \u7684\u52a0\u901f\u5668\u8a2d\u8a08 SWAT\uff0c\u5b83\u6709\u6548\u5229\u7528\u7a00\u758f\u6027\uff0c\u4ee5\u9054\u6210\u9577\u8f38\u5165\u7684\u53ef\u64f4\u5145\u6548\u80fd\u3002\n\u6240\u63d0\u51fa\u7684\u5fae\u67b6\u69cb\u57fa\u65bc\u4e00\u7a2e\u8a2d\u8a08\uff0c\u5b83\u900f\u904e\u7d50\u5408\u9010\u884c\u8cc7\u6599\u6d41\u7a0b\u3001\u6838\u5fc3\u878d\u5408\u6700\u4f73\u5316\u4ee5\u53ca\u8003\u91cf FPGA \u7684\u5206\u6563\u5f0f\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u8cc7\u6e90\u7684\u8f38\u5165\u975c\u6b62\u8a2d\u8a08\u4f86\u6700\u5927\u5316\u8cc7\u6599\u91cd\u8907\u4f7f\u7528\u3002\n\u56e0\u6b64\uff0c\u8207\u57fa\u7dda\u7684\u57fa\u65bc FPGA \u7684\u52a0\u901f\u5668\u76f8\u6bd4\uff0c\u5b83\u5728\u5ef6\u9072\u548c\u80fd\u6e90\u6548\u7387\u65b9\u9762\u5206\u5225\u9054\u5230\u4e86 22 \u500d\u548c 5.7 \u500d\u7684\u63d0\u5347\uff0c\u8207\u57fa\u65bc GPU \u7684\u89e3\u6c7a\u65b9\u6848\u76f8\u6bd4\uff0c\u80fd\u6e90\u6548\u7387\u63d0\u5347\u4e86 15 \u500d\u3002", "author": "Zhenyu Bai et.al.", "authors": "Zhenyu Bai, Pranav Dangi, Huize Li, Tulika Mitra", "id": "2405.17025v1", "paper_url": "http://arxiv.org/abs/2405.17025v1", "repo": "null"}}