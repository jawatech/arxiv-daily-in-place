{"2412.08283": {"publish_time": "2024-12-11", "title": "A Preliminary Analysis of Automatic Word and Syllable Prominence Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings", "paper_summary": "Automatic detection of prominence at the word and syllable-levels is critical\nfor building computer-assisted language learning systems. It has been shown\nthat prosody embeddings learned by the current state-of-the-art (SOTA)\ntext-to-speech (TTS) systems could generate word- and syllable-level prominence\nin the synthesized speech as natural as in native speech. To understand the\neffectiveness of prosody embeddings from TTS for prominence detection under\nnonnative context, a comparative analysis is conducted on the embeddings\nextracted from native and non-native speech considering the prominence-related\nembeddings: duration, energy, and pitch from a SOTA TTS named FastSpeech2.\nThese embeddings are extracted under two conditions considering: 1) only text,\n2) both speech and text. For the first condition, the embeddings are extracted\ndirectly from the TTS inference mode, whereas for the second condition, we\npropose to extract from the TTS under training mode. Experiments are conducted\non native speech corpus: Tatoeba, and non-native speech corpus: ISLE. For\nexperimentation, word-level prominence locations are manually annotated for\nboth corpora. The highest relative improvement on word \\& syllable-level\nprominence detection accuracies with the TTS embeddings are found to be 13.7% &\n5.9% and 16.2% & 6.9% compared to those with the heuristic-based features and\nself-supervised Wav2Vec-2.0 representations, respectively.", "paper_summary_zh": "\u81ea\u52d5\u5075\u6e2c\u55ae\u5b57\u548c\u97f3\u7bc0\u5c64\u7d1a\u7684\u91cd\u97f3\u5c0d\u65bc\u5efa\u69cb\u96fb\u8166\u8f14\u52a9\u8a9e\u8a00\u5b78\u7fd2\u7cfb\u7d71\u81f3\u95dc\u91cd\u8981\u3002\u76ee\u524d\u6700\u5148\u9032 (SOTA) \u7684\u6587\u5b57\u8f49\u8a9e\u97f3 (TTS) \u7cfb\u7d71\u6240\u5b78\u7fd2\u5230\u7684\u97fb\u5f8b\u5d4c\u5165\uff0c\u5df2\u88ab\u8b49\u5be6\u53ef\u4ee5\u5408\u6210\u51fa\u8207\u6bcd\u8a9e\u4eba\u58eb\u8aaa\u8a71\u4e00\u6a23\u81ea\u7136\u7684\u55ae\u5b57\u548c\u97f3\u7bc0\u5c64\u7d1a\u91cd\u97f3\u3002\u70ba\u4e86\u4e86\u89e3 TTS \u97fb\u5f8b\u5d4c\u5165\u5728\u975e\u6bcd\u8a9e\u8108\u7d61\u4e2d\u7528\u65bc\u91cd\u97f3\u5075\u6e2c\u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5c0d\u5f9e\u6bcd\u8a9e\u548c\u975e\u6bcd\u8a9e\u6f14\u8aaa\u4e2d\u8403\u53d6\u51fa\u7684\u5d4c\u5165\u9032\u884c\u6bd4\u8f03\u5206\u6790\uff0c\u8003\u91cf SOTA TTS\uff08\u7a31\u70ba FastSpeech2\uff09\u4e2d\u8207\u91cd\u97f3\u76f8\u95dc\u7684\u5d4c\u5165\uff1a\u6642\u9577\u3001\u80fd\u91cf\u548c\u97f3\u9ad8\u3002\u9019\u4e9b\u5d4c\u5165\u662f\u5728\u5169\u500b\u689d\u4ef6\u4e0b\u8403\u53d6\u7684\uff1a1) \u53ea\u6709\u6587\u5b57\uff0c2) \u8a9e\u97f3\u548c\u6587\u5b57\u7686\u6709\u3002\u5c0d\u65bc\u7b2c\u4e00\u500b\u689d\u4ef6\uff0c\u5d4c\u5165\u662f\u76f4\u63a5\u5f9e TTS \u63a8\u8ad6\u6a21\u5f0f\u4e2d\u8403\u53d6\uff0c\u800c\u5c0d\u65bc\u7b2c\u4e8c\u500b\u689d\u4ef6\uff0c\u6211\u5011\u5efa\u8b70\u5f9e\u8a13\u7df4\u6a21\u5f0f\u4e2d\u7684 TTS \u4e2d\u8403\u53d6\u3002\u5be6\u9a57\u662f\u5728\u6bcd\u8a9e\u8a9e\u6599\u5eab\uff1aTatoeba \u548c\u975e\u6bcd\u8a9e\u8a9e\u6599\u5eab\uff1aISLE \u4e0a\u9032\u884c\u3002\u70ba\u4e86\u5be6\u9a57\uff0c\u5169\u500b\u8a9e\u6599\u5eab\u7684\u55ae\u5b57\u5c64\u7d1a\u91cd\u97f3\u4f4d\u7f6e\u90fd\u7d93\u904e\u624b\u52d5\u6a19\u8a3b\u3002\u767c\u73fe\u4f7f\u7528 TTS \u5d4c\u5165\u5728\u55ae\u5b57\u548c\u97f3\u7bc0\u5c64\u7d1a\u91cd\u97f3\u5075\u6e2c\u6e96\u78ba\u7387\u4e0a\u6700\u9ad8\u7684\u76f8\u5c0d\u6539\u5584\u70ba 13.7% \u548c 5.9% \u4ee5\u53ca 16.2% \u548c 6.9%\uff0c\u5206\u5225\u8207\u57fa\u65bc\u555f\u767c\u5f0f\u7279\u5fb5\u548c\u81ea\u76e3\u7763 Wav2Vec-2.0 \u8868\u5fb5\u76f8\u6bd4\u3002", "author": "Anindita Mondal et.al.", "authors": "Anindita Mondal, Rangavajjala Sankara Bharadwaj, Jhansi Mallela, Anil Kumar Vuppala, Chiranjeevi Yarra", "id": "2412.08283v1", "paper_url": "http://arxiv.org/abs/2412.08283v1", "repo": "null"}}