{"2405.06522": {"publish_time": "2024-05-10", "title": "Heterogeneous Graph Neural Networks with Loss-decrease-aware Curriculum Learning", "paper_summary": "In recent years, heterogeneous graph neural networks (HGNNs) have achieved\nexcellent performance in handling heterogeneous information networks (HINs).\nCurriculum learning is a machine learning strategy where training examples are\npresented to a model in a structured order, starting with easy examples and\ngradually increasing difficulty, aiming to improve learning efficiency and\ngeneralization. To better exploit the rich information in HINs, previous\nmethods have started to explore the use of curriculum learning strategy to\ntrain HGNNs. Specifically, these works utilize the absolute value of the loss\nat each training epoch to evaluate the learning difficulty of each training\nsample. However, the relative loss, rather than the absolute value of loss,\nreveals the learning difficulty. Therefore, we propose a novel\nloss-decrease-aware training schedule (LDTS). LDTS uses the trend of loss\ndecrease between each training epoch to better evaluating the difficulty of\ntraining samples, thereby enhancing the curriculum learning of HGNNs for\ndownstream tasks. Additionally, we propose a sampling strategy to alleviate\ntraining imbalance issues. Our method further demonstrate the efficacy of\ncurriculum learning in enhancing HGNNs capabilities. We call our method\nLoss-decrease-aware Heterogeneous Graph Neural Networks (LDHGNN). The code is\npublic at https://github.com/wangyili00/LDHGNN.", "paper_summary_zh": "\u8fd1\u5e74\u6765\uff0c\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNN\uff09\u5728\u5904\u7406\u5f02\u6784\u4fe1\u606f\u7f51\u7edc\uff08HIN\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002\u8bfe\u7a0b\u5b66\u4e60\u662f\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u7b56\u7565\uff0c\u5176\u4e2d\u8bad\u7ec3\u793a\u4f8b\u4ee5\u7ed3\u6784\u5316\u987a\u5e8f\u5448\u73b0\u7ed9\u6a21\u578b\uff0c\u4ece\u7b80\u5355\u7684\u793a\u4f8b\u5f00\u59cb\uff0c\u9010\u6e10\u589e\u52a0\u96be\u5ea6\uff0c\u65e8\u5728\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u7528 HIN \u4e2d\u4e30\u5bcc\u7684\u7684\u4fe1\u606f\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u5df2\u7ecf\u5f00\u59cb\u63a2\u7d22\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6765\u8bad\u7ec3 HGNN\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8fd9\u4e9b\u5de5\u4f5c\u5229\u7528\u6bcf\u4e2a\u8bad\u7ec3 epoch \u7684\u635f\u5931\u7684\u7edd\u5bf9\u503c\u6765\u8bc4\u4f30\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u7684\u5b66\u4e60\u96be\u5ea6\u3002\u7136\u800c\uff0c\u76f8\u5bf9\u635f\u5931\u800c\u4e0d\u662f\u635f\u5931\u7684\u7edd\u5bf9\u503c\u63ed\u793a\u4e86\u5b66\u4e60\u96be\u5ea6\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u635f\u5931\u51cf\u5c11\u611f\u77e5\u8bad\u7ec3\u8ba1\u5212\uff08LDTS\uff09\u3002LDTS \u4f7f\u7528\u6bcf\u4e2a\u8bad\u7ec3 epoch \u4e4b\u95f4\u7684\u635f\u5931\u51cf\u5c11\u8d8b\u52bf\u6765\u66f4\u597d\u5730\u8bc4\u4f30\u8bad\u7ec3\u6837\u672c\u7684\u96be\u5ea6\uff0c\u4ece\u800c\u589e\u5f3a HGNN \u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u8bfe\u7a0b\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u6837\u7b56\u7565\u6765\u7f13\u89e3\u8bad\u7ec3\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u8bfe\u7a0b\u5b66\u4e60\u5728\u589e\u5f3a HGNN \u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u79f0\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u635f\u5931\u51cf\u5c11\u611f\u77e5\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08LDHGNN\uff09\u3002\u4ee3\u7801\u5728 https://github.com/wangyili00/LDHGNN \u516c\u5f00\u3002", "author": "Yili Wang et.al.", "authors": "Yili Wang", "id": "2405.06522v1", "paper_url": "http://arxiv.org/abs/2405.06522v1", "repo": "https://github.com/wangyili00/ldhgnn"}}