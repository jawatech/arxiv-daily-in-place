{"2405.21040": {"publish_time": "2024-05-31", "title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement", "paper_summary": "Reinforcement Learning from Human Feedback (RLHF) has been commonly used to\nalign the behaviors of Large Language Models (LLMs) with human preferences.\nRecently, a popular alternative is Direct Policy Optimization (DPO), which\nreplaces an LLM-based reward model with the policy itself, thus obviating the\nneed for extra memory and training time to learn the reward model. However, DPO\ndoes not consider the relative qualities of the positive and negative\nresponses, and can lead to sub-optimal training outcomes. To alleviate this\nproblem, we investigate the use of intrinsic knowledge within the on-the-fly\nfine-tuning LLM to obtain relative qualities and help to refine the loss\nfunction. Specifically, we leverage the knowledge of the LLM to design a\nrefinement function to estimate the quality of both the positive and negative\nresponses. We show that the constructed refinement function can help\nself-refine the loss function under mild assumptions. The refinement function\nis integrated into DPO and its variant Identity Policy Optimization (IPO).\nExperiments across various evaluators indicate that they can improve the\nperformance of the fine-tuned models over DPO and IPO.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5e38\u7528\u65bc\u8abf\u6574\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u884c\u70ba\u4ee5\u7b26\u5408\u4eba\u985e\u504f\u597d\u3002\u6700\u8fd1\uff0c\u4e00\u7a2e\u6d41\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u662f\u76f4\u63a5\u7b56\u7565\u6700\u4f73\u5316 (DPO)\uff0c\u5b83\u4f7f\u7528\u7b56\u7565\u672c\u8eab\u53d6\u4ee3\u57fa\u65bc LLM \u7684\u734e\u52f5\u6a21\u578b\uff0c\u5f9e\u800c\u907f\u514d\u4e86\u5b78\u7fd2\u734e\u52f5\u6a21\u578b\u6240\u9700\u7684\u984d\u5916\u8a18\u61b6\u9ad4\u548c\u8a13\u7df4\u6642\u9593\u3002\u7136\u800c\uff0cDPO \u6c92\u6709\u8003\u616e\u6b63\u9762\u548c\u8ca0\u9762\u56de\u61c9\u7684\u76f8\u5c0d\u54c1\u8cea\uff0c\u4e26\u4e14\u53ef\u80fd\u5c0e\u81f4\u6b21\u4f73\u7684\u8a13\u7df4\u7d50\u679c\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63a2\u8a0e\u5728\u5373\u6642\u5fae\u8abf LLM \u4e2d\u4f7f\u7528\u5167\u5728\u77e5\u8b58\u4ee5\u53d6\u5f97\u76f8\u5c0d\u54c1\u8cea\u4e26\u5e6b\u52a9\u6539\u5584\u640d\u5931\u51fd\u6578\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5229\u7528 LLM \u7684\u77e5\u8b58\u4f86\u8a2d\u8a08\u4e00\u500b\u6539\u5584\u51fd\u6578\uff0c\u4ee5\u4f30\u8a08\u6b63\u9762\u548c\u8ca0\u9762\u56de\u61c9\u7684\u54c1\u8cea\u3002\u6211\u5011\u8868\u660e\uff0c\u6240\u5efa\u69cb\u7684\u6539\u5584\u51fd\u6578\u53ef\u4ee5\u5728\u6eab\u548c\u7684\u5047\u8a2d\u4e0b\u5e6b\u52a9\u81ea\u6211\u6539\u5584\u640d\u5931\u51fd\u6578\u3002\u6539\u5584\u51fd\u6578\u6574\u5408\u5230 DPO \u53ca\u5176\u8b8a\u9ad4\u8eab\u5206\u7b56\u7565\u6700\u4f73\u5316 (IPO) \u4e2d\u3002\u5728\u5404\u7a2e\u8a55\u4f30\u8005\u4e2d\u7684\u5be6\u9a57\u8868\u660e\uff0c\u5b83\u5011\u53ef\u4ee5\u6539\u5584\u5fae\u8abf\u6a21\u578b\u5728 DPO \u548c IPO \u4e0a\u7684\u6548\u80fd\u3002", "author": "Runsheng Yu et.al.", "authors": "Runsheng Yu, Yong Wang, Xiaoqi Jiao, Youzhi Zhang, James T. Kwok", "id": "2405.21040v1", "paper_url": "http://arxiv.org/abs/2405.21040v1", "repo": "null"}}