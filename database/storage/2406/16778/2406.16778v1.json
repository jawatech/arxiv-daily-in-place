{"2406.16778": {"publish_time": "2024-06-24", "title": "Finding Transformer Circuits with Edge Pruning", "paper_summary": "The path to interpreting a language model often proceeds via analysis of\ncircuits -- sparse computational subgraphs of the model that capture specific\naspects of its behavior. Recent work has automated the task of discovering\ncircuits. Yet, these methods have practical limitations, as they rely either on\ninefficient search algorithms or inaccurate approximations. In this paper, we\nframe automated circuit discovery as an optimization problem and propose *Edge\nPruning* as an effective and scalable solution. Edge Pruning leverages\ngradient-based pruning techniques, but instead of removing neurons or\ncomponents, it prunes the \\emph{edges} between components. Our method finds\ncircuits in GPT-2 that use less than half the number of edges compared to\ncircuits found by previous methods while being equally faithful to the full\nmodel predictions on standard circuit-finding tasks. Edge Pruning is efficient\neven with as many as 100K examples, outperforming previous methods in speed and\nproducing substantially better circuits. It also perfectly recovers the\nground-truth circuits in two models compiled with Tracr. Thanks to its\nefficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale\nthat prior methods operate on. We use this setting for a case study comparing\nthe mechanisms behind instruction prompting and in-context learning. We find\ntwo circuits with more than 99.96% sparsity that match the performance of the\nfull model and reveal that the mechanisms in the two settings overlap\nsubstantially. Our case study shows that Edge Pruning is a practical and\nscalable tool for interpretability and sheds light on behaviors that only\nemerge in large models.", "paper_summary_zh": "<paragraph>\u8a6e\u91cb\u8a9e\u8a00\u6a21\u578b\u7684\u8def\u5f91\u901a\u5e38\u6703\u900f\u904e\u96fb\u8def\u5206\u6790\u9032\u884c\uff0c\u96fb\u8def\u662f\u6a21\u578b\u7684\u7a00\u758f\u8a08\u7b97\u5b50\u5716\uff0c\u7528\u4f86\u64f7\u53d6\u5176\u884c\u70ba\u7684\u7279\u5b9a\u9762\u5411\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u81ea\u52d5\u5316\u96fb\u8def\u767c\u73fe\u7684\u4efb\u52d9\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u6709\u5be6\u969b\u9650\u5236\uff0c\u56e0\u70ba\u5b83\u5011\u4f9d\u8cf4\u4f4e\u6548\u7387\u7684\u641c\u5c0b\u6f14\u7b97\u6cd5\u6216\u4e0d\u6e96\u78ba\u7684\u8fd1\u4f3c\u503c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07\u81ea\u52d5\u5316\u96fb\u8def\u767c\u73fe\u5efa\u69cb\u70ba\u4e00\u500b\u6700\u4f73\u5316\u554f\u984c\uff0c\u4e26\u63d0\u51fa *\u908a\u7de3\u4fee\u526a* \u4f5c\u70ba\u4e00\u500b\u6709\u6548\u4e14\u53ef\u64f4\u5145\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u908a\u7de3\u4fee\u526a\u5229\u7528\u57fa\u65bc\u68af\u5ea6\u7684\u4fee\u526a\u6280\u8853\uff0c\u4f46\u5b83\u4e26\u975e\u79fb\u9664\u795e\u7d93\u5143\u6216\u5143\u4ef6\uff0c\u800c\u662f\u4fee\u526a\u5143\u4ef6\u4e4b\u9593\u7684 *\u908a\u7de3*\u3002\u6211\u5011\u7684\u6a21\u578b\u5728 GPT-2 \u4e2d\u627e\u5230\u96fb\u8def\uff0c\u8207\u5148\u524d\u65b9\u6cd5\u627e\u5230\u7684\u96fb\u8def\u76f8\u6bd4\uff0c\u4f7f\u7528\u7684\u908a\u7de3\u6578\u91cf\u5c11\u65bc\u4e00\u534a\uff0c\u540c\u6642\u5c0d\u6a19\u6e96\u96fb\u8def\u5c0b\u627e\u4efb\u52d9\u7684\u5b8c\u6574\u6a21\u578b\u9810\u6e2c\u540c\u6a23\u5fe0\u5be6\u3002\u5373\u4f7f\u7bc4\u4f8b\u591a\u9054 100K\uff0c\u908a\u7de3\u4fee\u526a\u4ecd\u7136\u6709\u6548\u7387\uff0c\u5728\u901f\u5ea6\u4e0a\u512a\u65bc\u5148\u524d\u7684\u6a21\u578b\uff0c\u4e26\u7522\u751f\u5927\u5e45\u66f4\u597d\u7684\u96fb\u8def\u3002\u5b83\u4e5f\u80fd\u5b8c\u7f8e\u9084\u539f Tracr \u7de8\u8b6f\u7684\u5169\u500b\u6a21\u578b\u4e2d\u7684\u771f\u5be6\u96fb\u8def\u3002\u7531\u65bc\u5176\u6548\u7387\uff0c\u6211\u5011\u5c07\u908a\u7de3\u4fee\u526a\u64f4\u5145\u5230 CodeLlama-13B\uff0c\u4e00\u500b\u898f\u6a21\u6bd4\u5148\u524d\u65b9\u6cd5\u64cd\u4f5c\u7684\u6a21\u578b\u5927 100 \u500d\u4ee5\u4e0a\u7684\u6a21\u578b\u3002\u6211\u5011\u4f7f\u7528\u9019\u500b\u8a2d\u5b9a\u9032\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u6bd4\u8f03\u6307\u4ee4\u63d0\u793a\u548c\u60c5\u5883\u5b78\u7fd2\u80cc\u5f8c\u7684\u6a5f\u5236\u3002\u6211\u5011\u627e\u5230\u5169\u500b\u7a00\u758f\u5ea6\u8d85\u904e 99.96% \u7684\u96fb\u8def\uff0c\u5b83\u5011\u8207\u5b8c\u6574\u6a21\u578b\u7684\u6548\u80fd\u76f8\u7b26\uff0c\u4e26\u63ed\u793a\u9019\u5169\u500b\u8a2d\u5b9a\u4e2d\u7684\u6a5f\u5236\u6709\u5927\u5e45\u91cd\u758a\u3002\u6211\u5011\u7684\u6848\u4f8b\u7814\u7a76\u986f\u793a\uff0c\u908a\u7de3\u4fee\u526a\u662f\u4e00\u500b\u5be6\u7528\u4e14\u53ef\u64f4\u5145\u7684\u8a6e\u91cb\u5de5\u5177\uff0c\u4e26\u95e1\u660e\u50c5\u51fa\u73fe\u5728\u5927\u578b\u6a21\u578b\u4e2d\u7684\u884c\u70ba\u3002</paragraph>", "author": "Adithya Bhaskar et.al.", "authors": "Adithya Bhaskar, Alexander Wettig, Dan Friedman, Danqi Chen", "id": "2406.16778v1", "paper_url": "http://arxiv.org/abs/2406.16778v1", "repo": "https://github.com/princeton-nlp/edge-pruning"}}