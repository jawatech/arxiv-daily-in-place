{"2405.18874": {"publish_time": "2024-05-29", "title": "Are queries and keys always relevant? A case study on Transformer wave functions", "paper_summary": "The dot product attention mechanism, originally designed for natural language\nprocessing (NLP) tasks, is a cornerstone of modern Transformers. It adeptly\ncaptures semantic relationships between word pairs in sentences by computing a\nsimilarity overlap between queries and keys. In this work, we explore the\nsuitability of Transformers, focusing on their attention mechanisms, in the\nspecific domain of the parametrization of variational wave functions to\napproximate ground states of quantum many-body spin Hamiltonians. Specifically,\nwe perform numerical simulations on the two-dimensional $J_1$-$J_2$ Heisenberg\nmodel, a common benchmark in the field of quantum-many body systems on lattice.\nBy comparing the performance of standard attention mechanisms with a simplified\nversion that excludes queries and keys, relying solely on positions, we achieve\ncompetitive results while reducing computational cost and parameter usage.\nFurthermore, through the analysis of the attention maps generated by standard\nattention mechanisms, we show that the attention weights become effectively\ninput-independent at the end of the optimization. We support the numerical\nresults with analytical calculations, providing physical insights of why\nqueries and keys should be, in principle, omitted from the attention mechanism\nwhen studying large systems. Interestingly, the same arguments can be extended\nto the NLP domain, in the limit of long input sentences.", "paper_summary_zh": "\u9ede\u7a4d\u6ce8\u610f\u6a5f\u5236\u6700\u521d\u662f\u70ba\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u800c\u8a2d\u8a08\u7684\uff0c\u662f\u73fe\u4ee3 Transformer \u7684\u57fa\u77f3\u3002\u5b83\u900f\u904e\u8a08\u7b97\u67e5\u8a62\u548c\u9375\u4e4b\u9593\u7684\u76f8\u4f3c\u6027\u91cd\u758a\uff0c\u5de7\u5999\u5730\u6355\u6349\u53e5\u5b50\u4e2d\u5b57\u8a5e\u5c0d\u4e4b\u9593\u7684\u8a9e\u7fa9\u95dc\u4fc2\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86 Transformer \u7684\u9069\u7528\u6027\uff0c\u91cd\u9ede\u653e\u5728\u5b83\u5011\u5728\u8b8a\u5206\u6ce2\u51fd\u6578\u53c3\u6578\u5316\u7684\u7279\u5b9a\u9818\u57df\u4e2d\u7684\u6ce8\u610f\u6a5f\u5236\uff0c\u4ee5\u903c\u8fd1\u91cf\u5b50\u591a\u9ad4\u81ea\u65cb\u54c8\u5bc6\u9813\u91cf\u7684\u57fa\u614b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c0d\u4e8c\u7dad $J_1$-$J_2$ \u6d77\u68ee\u5821\u6a21\u578b\u9032\u884c\u6578\u503c\u6a21\u64ec\uff0c\u9019\u662f\u6676\u683c\u4e0a\u91cf\u5b50\u591a\u9ad4\u7cfb\u7d71\u9818\u57df\u4e2d\u7684\u5e38\u898b\u57fa\u6e96\u3002\u900f\u904e\u6bd4\u8f03\u6a19\u6e96\u6ce8\u610f\u6a5f\u5236\u8207\u6392\u9664\u67e5\u8a62\u548c\u9375\u7684\u7c21\u5316\u7248\u672c\u7684\u6548\u80fd\uff0c\u50c5\u4f9d\u8cf4\u65bc\u4f4d\u7f6e\uff0c\u6211\u5011\u5728\u964d\u4f4e\u904b\u7b97\u6210\u672c\u548c\u53c3\u6578\u4f7f\u7528\u91cf\u7684\u540c\u6642\uff0c\u9054\u5230\u4e86\u7af6\u722d\u529b\u7684\u7d50\u679c\u3002\u6b64\u5916\uff0c\u900f\u904e\u5206\u6790\u6a19\u6e96\u6ce8\u610f\u6a5f\u5236\u751f\u6210\u7684\u6ce8\u610f\u5716\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6ce8\u610f\u6b0a\u91cd\u5728\u6700\u4f73\u5316\u7d50\u675f\u6642\u6709\u6548\u5730\u8b8a\u5f97\u8207\u8f38\u5165\u7121\u95dc\u3002\u6211\u5011\u4ee5\u5206\u6790\u8a08\u7b97\u652f\u6301\u6578\u503c\u7d50\u679c\uff0c\u63d0\u4f9b\u7269\u7406\u898b\u89e3\uff0c\u8aaa\u660e\u70ba\u4ec0\u9ebc\u5728\u7814\u7a76\u5927\u578b\u7cfb\u7d71\u6642\uff0c\u539f\u5247\u4e0a\u61c9\u8a72\u5f9e\u6ce8\u610f\u6a5f\u5236\u4e2d\u7701\u7565\u67e5\u8a62\u548c\u9375\u3002\u6709\u8da3\u7684\u662f\uff0c\u76f8\u540c\u7684\u8ad6\u9ede\u53ef\u4ee5\u64f4\u5c55\u5230 NLP \u9818\u57df\uff0c\u5728\u9577\u8f38\u5165\u53e5\u5b50\u7684\u9650\u5236\u4e0b\u3002", "author": "Riccardo Rende et.al.", "authors": "Riccardo Rende, Luciano Loris Viteritti", "id": "2405.18874v1", "paper_url": "http://arxiv.org/abs/2405.18874v1", "repo": "null"}}