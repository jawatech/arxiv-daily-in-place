{"2408.10600": {"publish_time": "2024-08-20", "title": "Breast tumor classification based on self-supervised contrastive learning from ultrasound videos", "paper_summary": "Background: Breast ultrasound is prominently used in diagnosing breast\ntumors. At present, many automatic systems based on deep learning have been\ndeveloped to help radiologists in diagnosis. However, training such systems\nremains challenging because they are usually data-hungry and demand amounts of\nlabeled data, which need professional knowledge and are expensive. Methods: We\nadopted a triplet network and a self-supervised contrastive learning technique\nto learn representations from unlabeled breast ultrasound video clips. We\nfurther designed a new hard triplet loss to to learn representations that\nparticularly discriminate positive and negative image pairs that are hard to\nrecognize. We also constructed a pretraining dataset from breast ultrasound\nvideos (1,360 videos from 200 patients), which includes an anchor sample\ndataset with 11,805 images, a positive sample dataset with 188,880 images, and\na negative sample dataset dynamically generated from video clips. Further, we\nconstructed a finetuning dataset, including 400 images from 66 patients. We\ntransferred the pretrained network to a downstream benign/malignant\nclassification task and compared the performance with other state-of-the-art\nmodels, including three models pretrained on ImageNet and a previous\ncontrastive learning model retrained on our datasets. Results and conclusion:\nExperiments revealed that our model achieved an area under the receiver\noperating characteristic curve (AUC) of 0.952, which is significantly higher\nthan the others. Further, we assessed the dependence of our pretrained model on\nthe number of labeled data and revealed that <100 samples were required to\nachieve an AUC of 0.901. The proposed framework greatly reduces the demand for\nlabeled data and holds potential for use in automatic breast ultrasound image\ndiagnosis.", "paper_summary_zh": "<paragraph>\u80cc\u666f\uff1a\u4e73\u623f\u8d85\u97f3\u6ce2\u5728\u8bca\u65ad\u4e73\u623f\u816b\u7624\u4e2d\u4f54\u6709\u91cd\u8981\u5730\u4f4d\u3002\u76ee\u524d\uff0c\u8a31\u591a\u57fa\u65bc\u6df1\u5ea6\u5b78\u7fd2\u7684\u81ea\u52d5\u5316\u7cfb\u7d71\u5df2\u958b\u767c\u51fa\u4f86\uff0c\u4ee5\u5354\u52a9\u653e\u5c04\u79d1\u91ab\u5e2b\u8a3a\u65b7\u3002\u7136\u800c\uff0c\u8a13\u7df4\u6b64\u985e\u7cfb\u7d71\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\uff0c\u56e0\u70ba\u5b83\u5011\u901a\u5e38\u9700\u8981\u5927\u91cf\u8cc7\u6599\uff0c\u4e14\u9700\u8981\u5927\u91cf\u7684\u6a19\u7c64\u8cc7\u6599\uff0c\u9019\u9700\u8981\u5c08\u696d\u77e5\u8b58\u4e14\u6602\u8cb4\u3002\u65b9\u6cd5\uff1a\u6211\u5011\u63a1\u7528\u4e09\u5143\u7d44\u7db2\u8def\u548c\u81ea\u6211\u76e3\u7763\u5c0d\u6bd4\u5b78\u7fd2\u6280\u8853\uff0c\u5f9e\u672a\u6a19\u7c64\u7684\u4e73\u623f\u8d85\u97f3\u6ce2\u5f71\u7247\u7247\u6bb5\u4e2d\u5b78\u7fd2\u8868\u5fb5\u3002\u6211\u5011\u9032\u4e00\u6b65\u8a2d\u8a08\u4e86\u4e00\u500b\u65b0\u7684\u56f0\u96e3\u4e09\u5143\u7d44\u640d\u5931\uff0c\u4ee5\u5b78\u7fd2\u7279\u5225\u5340\u5206\u96e3\u4ee5\u8fa8\u8b58\u7684\u6b63\u8ca0\u5f71\u50cf\u5c0d\u7684\u8868\u5fb5\u3002\u6211\u5011\u9084\u5f9e\u4e73\u623f\u8d85\u97f3\u6ce2\u5f71\u7247\u4e2d\u5efa\u69cb\u4e86\u4e00\u500b\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\uff08\u4f86\u81ea 200 \u4f4d\u60a3\u8005\u7684 1,360 \u500b\u5f71\u7247\uff09\uff0c\u5176\u4e2d\u5305\u62ec\u5305\u542b 11,805 \u5f35\u5f71\u50cf\u7684\u9328\u5b9a\u6a23\u672c\u8cc7\u6599\u96c6\u3001\u5305\u542b 188,880 \u5f35\u5f71\u50cf\u7684\u6b63\u6a23\u672c\u8cc7\u6599\u96c6\uff0c\u4ee5\u53ca\u5f9e\u5f71\u7247\u7247\u6bb5\u4e2d\u52d5\u614b\u751f\u6210\u7684\u8ca0\u6a23\u672c\u8cc7\u6599\u96c6\u3002\u6b64\u5916\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u5fae\u8abf\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4f86\u81ea 66 \u4f4d\u60a3\u8005\u7684 400 \u5f35\u5f71\u50cf\u3002\u6211\u5011\u5c07\u9810\u8a13\u7df4\u7db2\u8def\u8f49\u79fb\u5230\u4e0b\u6e38\u826f\u6027/\u60e1\u6027\u5206\u985e\u4efb\u52d9\uff0c\u4e26\u5c07\u5176\u6548\u80fd\u8207\u5176\u4ed6\u6700\u5148\u9032\u7684\u6a21\u578b\u9032\u884c\u6bd4\u8f03\uff0c\u5305\u62ec\u4e09\u500b\u5728 ImageNet \u4e0a\u9810\u8a13\u7df4\u7684\u6a21\u578b\u548c\u4e00\u500b\u5728\u6211\u5011\u7684\u8cc7\u6599\u96c6\u4e0a\u91cd\u65b0\u8a13\u7df4\u7684\u5c0d\u6bd4\u5b78\u7fd2\u6a21\u578b\u3002\u7d50\u679c\u8207\u7d50\u8ad6\uff1a\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u53d7\u8a66\u8005\u5de5\u4f5c\u7279\u6027\u66f2\u7dda (AUC) \u4e0b\u65b9\u9054\u5230\u4e86 0.952 \u7684\u9762\u7a4d\uff0c\u986f\u8457\u9ad8\u65bc\u5176\u4ed6\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u6211\u5011\u9810\u8a13\u7df4\u6a21\u578b\u5c0d\u6a19\u7c64\u8cc7\u6599\u6578\u91cf\u4f9d\u8cf4\u6027\u7684\uff0c\u4e26\u767c\u73fe\u53ea\u9700\u4e0d\u5230 100 \u500b\u6a23\u672c\u5373\u53ef\u9054\u5230 0.901 \u7684 AUC\u3002\u6240\u63d0\u51fa\u7684\u67b6\u69cb\u5927\u5e45\u6e1b\u5c11\u4e86\u5c0d\u6a19\u7c64\u8cc7\u6599\u7684\u9700\u6c42\uff0c\u4e26\u5177\u6709\u7528\u65bc\u81ea\u52d5\u4e73\u623f\u8d85\u97f3\u6ce2\u5f71\u50cf\u8a3a\u65b7\u7684\u6f5b\u529b\u3002</paragraph>", "author": "Yunxin Tang et.al.", "authors": "Yunxin Tang, Siyuan Tang, Jian Zhang, Hao Chen", "id": "2408.10600v1", "paper_url": "http://arxiv.org/abs/2408.10600v1", "repo": "null"}}