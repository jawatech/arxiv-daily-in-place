{"2411.02317": {"publish_time": "2024-11-04", "title": "Defining and Evaluating Physical Safety for Large Language Models", "paper_summary": "Large Language Models (LLMs) are increasingly used to control robotic systems\nsuch as drones, but their risks of causing physical threats and harm in\nreal-world applications remain unexplored. Our study addresses the critical gap\nin evaluating LLM physical safety by developing a comprehensive benchmark for\ndrone control. We classify the physical safety risks of drones into four\ncategories: (1) human-targeted threats, (2) object-targeted threats, (3)\ninfrastructure attacks, and (4) regulatory violations. Our evaluation of\nmainstream LLMs reveals an undesirable trade-off between utility and safety,\nwith models that excel in code generation often performing poorly in crucial\nsafety aspects. Furthermore, while incorporating advanced prompt engineering\ntechniques such as In-Context Learning and Chain-of-Thought can improve safety,\nthese methods still struggle to identify unintentional attacks. In addition,\nlarger models demonstrate better safety capabilities, particularly in refusing\ndangerous commands. Our findings and benchmark can facilitate the design and\nevaluation of physical safety for LLMs. The project page is available at\nhuggingface.co/spaces/TrustSafeAI/LLM-physical-safety.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6108\u4f86\u6108\u5e38\u88ab\u7528\u65bc\u63a7\u5236\u6a5f\u5668\u4eba\u7cfb\u7d71\uff0c\u4f8b\u5982\u7121\u4eba\u6a5f\uff0c\u4f46\u5b83\u5011\u5728\u73fe\u5be6\u4e16\u754c\u61c9\u7528\u4e2d\u9020\u6210\u7269\u7406\u5a01\u8105\u548c\u50b7\u5bb3\u7684\u98a8\u96aa\u4ecd\u672a\u88ab\u63a2\u8a0e\u3002\u6211\u5011\u7684\u7814\u7a76\u900f\u904e\u958b\u767c\u7121\u4eba\u6a5f\u63a7\u5236\u7684\u7d9c\u5408\u57fa\u6e96\uff0c\u4f86\u89e3\u6c7a\u8a55\u4f30 LLM \u7269\u7406\u5b89\u5168\u6027\u7684\u95dc\u9375\u5dee\u8ddd\u3002\u6211\u5011\u5c07\u7121\u4eba\u6a5f\u7684\u7269\u7406\u5b89\u5168\u98a8\u96aa\u5206\u985e\u70ba\u56db\u985e\uff1a(1) \u4ee5\u4eba\u985e\u70ba\u76ee\u6a19\u7684\u5a01\u8105\u3001(2) \u4ee5\u7269\u9ad4\u70ba\u76ee\u6a19\u7684\u5a01\u8105\u3001(3) \u57fa\u790e\u8a2d\u65bd\u653b\u64ca\uff0c\u4ee5\u53ca (4) \u6cd5\u898f\u9055\u898f\u3002\u6211\u5011\u5c0d\u4e3b\u6d41 LLM \u7684\u8a55\u4f30\u63ed\u9732\u4e86\u5be6\u7528\u6027\u548c\u5b89\u5168\u6027\u4e4b\u9593\u4ee4\u4eba\u907a\u61be\u7684\u6b0a\u8861\uff0c\u5728\u7a0b\u5f0f\u78bc\u7522\u751f\u65b9\u9762\u8868\u73fe\u51fa\u8272\u7684\u6a21\u578b\u901a\u5e38\u5728\u95dc\u9375\u7684\u5b89\u5168\u65b9\u9762\u8868\u73fe\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u96d6\u7136\u7d50\u5408\u4e86\u9032\u968e\u63d0\u793a\u5de5\u7a0b\u6280\u8853\uff08\u4f8b\u5982\u60c5\u5883\u5b78\u7fd2\u548c\u601d\u7dad\u93c8\uff09\uff0c\u53ef\u4ee5\u6539\u5584\u5b89\u5168\u6027\uff0c\u4f46\u9019\u4e9b\u65b9\u6cd5\u4ecd\u7136\u96e3\u4ee5\u8b58\u5225\u7121\u610f\u7684\u653b\u64ca\u3002\u6b64\u5916\uff0c\u8f03\u5927\u7684\u6a21\u578b\u5c55\u73fe\u51fa\u66f4\u597d\u7684\u5b89\u5168\u6027\uff0c\u7279\u5225\u662f\u5728\u62d2\u7d55\u5371\u96aa\u7684\u6307\u4ee4\u65b9\u9762\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u548c\u57fa\u6e96\u53ef\u4ee5\u4fc3\u9032 LLM \u7269\u7406\u5b89\u5168\u6027\u7684\u8a2d\u8a08\u548c\u8a55\u4f30\u3002\u5c08\u6848\u9801\u9762\u53ef\u65bc huggingface.co/spaces/TrustSafeAI/LLM-physical-safety \u53d6\u5f97\u3002", "author": "Yung-Chen Tang et.al.", "authors": "Yung-Chen Tang, Pin-Yu Chen, Tsung-Yi Ho", "id": "2411.02317v1", "paper_url": "http://arxiv.org/abs/2411.02317v1", "repo": "null"}}