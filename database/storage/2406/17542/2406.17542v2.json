{"2406.17542": {"publish_time": "2024-06-25", "title": "CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained Models using Greedy Coordinate Descent", "paper_summary": "Large language models (LLMs) have recently demonstrated remarkable\nperformance across diverse language tasks. But their deployment is often\nconstrained by their substantial computational and storage requirements.\nQuantization has emerged as a key technique for addressing this challenge,\nenabling the compression of large models with minimal impact on performance.\nThe recent GPTQ algorithm, a post-training quantization (PTQ) method, has\nproven highly effective for compressing LLMs, sparking a wave of research that\nleverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the\nPTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ\nwith improved performance. CDQuant uses coordinate descent to minimize the\nlayer-wise reconstruction loss to achieve high-quality quantized weights. Our\nalgorithm is easy to implement and scales efficiently to models with hundreds\nof billions of parameters. Through extensive evaluation on the PaLM2 model\nfamily, we demonstrate that CDQuant consistently outperforms GPTQ across\ndiverse model sizes and quantization levels. In particular, for INT2\nquantization of PaLM2-Otter, CDQuant achieves a 10% reduction in perplexity\ncompared to GPTQ.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8fd1\u671f\u5728\u5404\u7a2e\u8a9e\u8a00\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u986f\u8457\u7684\u6548\u80fd\u3002\u4f46\u5176\u90e8\u7f72\u901a\u5e38\u53d7\u5230\u5176\u9f90\u5927\u7684\u904b\u7b97\u548c\u5132\u5b58\u9700\u6c42\u6240\u9650\u5236\u3002\u91cf\u5316\u5df2\u6210\u70ba\u89e3\u6c7a\u6b64\u6311\u6230\u7684\u4e00\u9805\u95dc\u9375\u6280\u8853\uff0c\u80fd\u58d3\u7e2e\u5927\u578b\u6a21\u578b\uff0c\u4e14\u5c0d\u6548\u80fd\u7684\u5f71\u97ff\u6975\u5c0f\u3002\u6700\u8fd1\u7684 GPTQ \u6f14\u7b97\u6cd5\uff0c\u4e00\u7a2e\u8a13\u7df4\u5f8c\u91cf\u5316 (PTQ) \u65b9\u6cd5\uff0c\u5df2\u88ab\u8b49\u660e\u5c0d\u65bc\u58d3\u7e2e LLM \u975e\u5e38\u6709\u6548\uff0c\u5f15\u767c\u4e86\u4e00\u6ce2\u4ee5 GPTQ \u70ba\u6838\u5fc3\u5143\u4ef6\u7684\u7814\u7a76\u6d6a\u6f6e\u3002\u9451\u65bc GPTQ \u5728 PTQ \u9818\u57df\u4e2d\u7684\u95dc\u9375\u89d2\u8272\uff0c\u6211\u5011\u5f15\u5165\u4e86 CDQuant\uff0c\u4e00\u7a2e\u7c21\u55ae\u4e14\u53ef\u64f4\u5145\u7684 GPTQ \u66ff\u4ee3\u65b9\u6848\uff0c\u4e14\u6548\u80fd\u6709\u6240\u63d0\u5347\u3002CDQuant \u4f7f\u7528\u5ea7\u6a19\u4e0b\u964d\u4f86\u6700\u5c0f\u5316\u5c64\u7d1a\u91cd\u5efa\u640d\u5931\uff0c\u4ee5\u9054\u6210\u9ad8\u54c1\u8cea\u7684\u91cf\u5316\u6b0a\u91cd\u3002\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u6613\u65bc\u5be6\u4f5c\uff0c\u4e14\u53ef\u6709\u6548\u64f4\u5145\u81f3\u5177\u6709\u6578\u5343\u5104\u500b\u53c3\u6578\u7684\u6a21\u578b\u3002\u900f\u904e\u5c0d PaLM2 \u6a21\u578b\u7cfb\u5217\u9032\u884c\u5ee3\u6cdb\u8a55\u4f30\uff0c\u6211\u5011\u8b49\u660e CDQuant \u5728\u5404\u7a2e\u6a21\u578b\u5927\u5c0f\u548c\u91cf\u5316\u5c64\u7d1a\u4e2d\u59cb\u7d42\u512a\u65bc GPTQ\u3002\u7279\u5225\u662f\uff0c\u5c0d\u65bc PaLM2-Otter \u7684 INT2 \u91cf\u5316\uff0cCDQuant \u8207 GPTQ \u76f8\u6bd4\uff0c\u56f0\u60d1\u5ea6\u964d\u4f4e\u4e86 10%\u3002", "author": "Pranav Ajit Nair et.al.", "authors": "Pranav Ajit Nair, Arun Sai Suggala", "id": "2406.17542v2", "paper_url": "http://arxiv.org/abs/2406.17542v2", "repo": "null"}}