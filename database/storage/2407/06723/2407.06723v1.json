{"2407.06723": {"publish_time": "2024-07-09", "title": "Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions", "paper_summary": "Humans describe complex scenes with compositionality, using simple text\ndescriptions enriched with links and relationships. While vision-language\nresearch has aimed to develop models with compositional understanding\ncapabilities, this is not reflected yet in existing datasets which, for the\nmost part, still use plain text to describe images. In this work, we propose a\nnew annotation strategy, graph-based captioning (GBC) that describes an image\nusing a labelled graph structure, with nodes of various types. The nodes in GBC\nare created using, in a first stage, object detection and dense captioning\ntools nested recursively to uncover and describe entity nodes, further linked\ntogether in a second stage by highlighting, using new types of nodes,\ncompositions and relations among entities. Since all GBC nodes hold plain text\ndescriptions, GBC retains the flexibility found in natural language, but can\nalso encode hierarchical information in its edges. We demonstrate that GBC can\nbe produced automatically, using off-the-shelf multimodal LLMs and\nopen-vocabulary detection models, by building a new dataset, GBC10M, gathering\nGBC annotations for about 10M images of the CC12M dataset. We use GBC10M to\nshowcase the wealth of node captions uncovered by GBC, as measured with CLIP\ntraining. We show that using GBC nodes' annotations -- notably those stored in\ncomposition and relation nodes -- results in significant performance boost on\ndownstream models when compared to other dataset formats. To further explore\nthe opportunities provided by GBC, we also propose a new attention mechanism\nthat can leverage the entire GBC graph, with encouraging experimental results\nthat show the extra benefits of incorporating the graph structure. Our datasets\nare released at \\url{https://huggingface.co/graph-based-captions}.", "paper_summary_zh": "<paragraph>\u4eba\u985e\u4f7f\u7528\u7c21\u55ae\u7684\u6587\u5b57\u63cf\u8ff0\uff0c\u8c50\u5bcc\u7684\u9023\u7d50\u548c\u95dc\u4fc2\uff0c\u4f86\u63cf\u8ff0\u8907\u96dc\u7684\u5834\u666f\u3002\u96d6\u7136\u8996\u89ba\u8a9e\u8a00\u7684\u7814\u7a76\u65e8\u5728\u958b\u767c\u5177\u6709\u7d44\u5408\u7406\u89e3\u80fd\u529b\u7684\u6a21\u578b\uff0c\u4f46\u73fe\u6709\u7684\u6578\u64da\u96c6\u5c1a\u672a\u53cd\u6620\u9019\u4e00\u9ede\uff0c\u9019\u4e9b\u6578\u64da\u96c6\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u4f7f\u7528\u7d14\u6587\u672c\u4f86\u63cf\u8ff0\u5716\u50cf\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u8a3b\u91cb\u7b56\u7565\uff0c\u57fa\u65bc\u5716\u8868\u7684\u6a19\u984c (GBC)\uff0c\u5b83\u4f7f\u7528\u6a19\u7c64\u5716\u8868\u7d50\u69cb\u4f86\u63cf\u8ff0\u5716\u50cf\uff0c\u5176\u4e2d\u5305\u542b\u5404\u7a2e\u985e\u578b\u7684\u7bc0\u9ede\u3002GBC \u4e2d\u7684\u7bc0\u9ede\u662f\u4f7f\u7528\u7269\u9ad4\u6aa2\u6e2c\u548c\u5bc6\u96c6\u6a19\u984c\u5de5\u5177\u5728\u7b2c\u4e00\u968e\u6bb5\u5275\u5efa\u7684\uff0c\u4ee5\u905e\u8ff4\u5d4c\u5957\u7684\u65b9\u5f0f\u767c\u73fe\u548c\u63cf\u8ff0\u5be6\u9ad4\u7bc0\u9ede\uff0c\u4e26\u5728\u7b2c\u4e8c\u968e\u6bb5\u4f7f\u7528\u65b0\u985e\u578b\u7684\u7bc0\u9ede\u7a81\u51fa\u986f\u793a\uff0c\u5f9e\u800c\u5c07\u5b83\u5011\u9032\u4e00\u6b65\u9023\u7d50\u5728\u4e00\u8d77\uff0c\u5be6\u9ad4\u4e4b\u9593\u7684\u7d44\u5408\u548c\u95dc\u4fc2\u3002\u7531\u65bc\u6240\u6709 GBC \u7bc0\u9ede\u90fd\u5305\u542b\u7d14\u6587\u672c\u63cf\u8ff0\uff0c\u56e0\u6b64 GBC \u4fdd\u7559\u4e86\u81ea\u7136\u8a9e\u8a00\u4e2d\u7684\u9748\u6d3b\u6027\uff0c\u4f46\u4e5f\u53ef\u4ee5\u5728\u5176\u908a\u7de3\u7de8\u78bc\u5206\u5c64\u4fe1\u606f\u3002\u6211\u5011\u8b49\u660e\u4e86 GBC \u53ef\u4ee5\u4f7f\u7528\u73fe\u6210\u7684\u591a\u6a21\u614b LLM \u548c\u958b\u653e\u8a5e\u5f59\u6aa2\u6e2c\u6a21\u578b\u81ea\u52d5\u751f\u6210\uff0c\u901a\u904e\u69cb\u5efa\u4e00\u500b\u65b0\u7684\u6578\u64da\u96c6 GBC10M\uff0c\u6536\u96c6\u4e86\u5927\u7d04 10M CC12M \u6578\u64da\u96c6\u5716\u50cf\u7684 GBC \u8a3b\u91cb\u3002\u6211\u5011\u4f7f\u7528 GBC10M \u4f86\u5c55\u793a GBC \u767c\u73fe\u7684\u8c50\u5bcc\u7bc0\u9ede\u6a19\u984c\uff0c\u4e26\u4f7f\u7528 CLIP \u8a13\u7df4\u9032\u884c\u6e2c\u91cf\u3002\u6211\u5011\u8868\u660e\uff0c\u8207\u5176\u4ed6\u6578\u64da\u96c6\u683c\u5f0f\u76f8\u6bd4\uff0c\u4f7f\u7528 GBC \u7bc0\u9ede\u7684\u8a3b\u91cb\u2014\u2014\u7279\u5225\u662f\u5b58\u5132\u5728\u7d44\u5408\u548c\u95dc\u4fc2\u7bc0\u9ede\u4e2d\u7684\u8a3b\u91cb\u2014\u2014\u6703\u986f\u8457\u63d0\u5347\u4e0b\u6e38\u6a21\u578b\u7684\u6027\u80fd\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u63a2\u7d22 GBC \u63d0\u4f9b\u7684\u6a5f\u6703\uff0c\u6211\u5011\u9084\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u6ce8\u610f\u6a5f\u5236\uff0c\u5b83\u53ef\u4ee5\u5229\u7528\u6574\u500b GBC \u5716\u8868\uff0c\u4e26\u901a\u904e\u9f13\u52f5\u6027\u7684\u5be6\u9a57\u7d50\u679c\u5c55\u793a\u4e86\u7d50\u5408\u5716\u8868\u7d50\u69cb\u7684\u984d\u5916\u597d\u8655\u3002\u6211\u5011\u7684\u6578\u64da\u96c6\u767c\u5e03\u5728 \\url{https://huggingface.co/graph-based-captions}\u3002</paragraph>", "author": "Yu-Guan Hsieh et.al.", "authors": "Yu-Guan Hsieh, Cheng-Yu Hsieh, Shih-Ying Yeh, Louis B\u00e9thune, Hadi Pour Ansari, Pavan Kumar Anasosalu Vasu, Chun-Liang Li, Ranjay Krishna, Oncel Tuzel, Marco Cuturi", "id": "2407.06723v1", "paper_url": "http://arxiv.org/abs/2407.06723v1", "repo": "null"}}