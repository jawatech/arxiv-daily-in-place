{"2407.19594": {"publish_time": "2024-07-28", "title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge", "paper_summary": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many\ndomains. While improving these models traditionally relies on costly human\ndata, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs\ncan improve by judging their own responses instead of relying on human\nlabelers. However, existing methods have primarily focused on improving model\nresponses rather than judgment capabilities, resulting in rapid saturation\nduring iterative training. To address this issue, we introduce a novel\nMeta-Rewarding step to the self-improvement process, where the model judges its\nown judgements and uses that feedback to refine its judgment skills.\nSurprisingly, this unsupervised approach improves the model's ability to judge\n{\\em and} follow instructions, as demonstrated by a win rate improvement of\nLlama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on\nArena-Hard. These results strongly suggest the potential for self-improving\nmodels without human supervision.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a31\u591a\u9818\u57df\u4e2d\u8fc5\u901f\u8d85\u8d8a\u4eba\u985e\u7684\u77e5\u8b58\u3002\u96d6\u7136\u6539\u5584\u9019\u4e9b\u6a21\u578b\u50b3\u7d71\u4e0a\u4f9d\u8cf4\u65bc\u6602\u8cb4\u7684\u4eba\u985e\u6578\u64da\uff0c\u4f46\u6700\u8fd1\u7684\u81ea\u6211\u734e\u52f5\u6a5f\u5236 (Yuan \u7b49\u4eba\uff0c2024 \u5e74) \u8868\u660e LLM \u53ef\u4ee5\u901a\u904e\u5224\u65b7\u81ea\u5df1\u7684\u53cd\u61c9\u800c\u4e0d\u662f\u4f9d\u8cf4\u65bc\u4eba\u985e\u6a19\u7c64\u54e1\u4f86\u9032\u884c\u6539\u9032\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u65bc\u6539\u9032\u6a21\u578b\u97ff\u61c9\uff0c\u800c\u4e0d\u662f\u5224\u65b7\u80fd\u529b\uff0c\u5f9e\u800c\u5c0e\u81f4\u5728\u53cd\u8986\u8a13\u7df4\u671f\u9593\u5feb\u901f\u98fd\u548c\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5728\u81ea\u6211\u6539\u9032\u904e\u7a0b\u4e2d\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u5143\u734e\u52f5\u6b65\u9a5f\uff0c\u6a21\u578b\u5728\u5176\u4e2d\u5224\u65b7\u81ea\u5df1\u7684\u5224\u65b7\uff0c\u4e26\u4f7f\u7528\u8a72\u56de\u994b\u4f86\u6539\u5584\u5176\u5224\u65b7\u6280\u80fd\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u9019\u7a2e\u7121\u76e3\u7763\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u5224\u65b7\u7684\u80fd\u529b{\\em \u548c}\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u6b63\u5982 Llama-3-8B-Instruct \u5728 AlpacaEval 2 \u4e0a\u7684\u7372\u52dd\u7387\u5f9e 22.9% \u63d0\u9ad8\u5230 39.4%\uff0c\u5728 Arena-Hard \u4e0a\u5f9e 20.6% \u63d0\u9ad8\u5230 29.1% \u6240\u8b49\u660e\u7684\u90a3\u6a23\u3002\u9019\u4e9b\u7d50\u679c\u5f37\u70c8\u8868\u660e\u4e86\u5728\u6c92\u6709\u4eba\u985e\u76e3\u7763\u7684\u60c5\u6cc1\u4e0b\u81ea\u6211\u6539\u9032\u6a21\u578b\u7684\u6f5b\u529b\u3002", "author": "Tianhao Wu et.al.", "authors": "Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar", "id": "2407.19594v2", "paper_url": "http://arxiv.org/abs/2407.19594v2", "repo": "null"}}