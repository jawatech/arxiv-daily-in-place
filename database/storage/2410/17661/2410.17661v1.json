{"2410.17661": {"publish_time": "2024-10-23", "title": "PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context", "paper_summary": "Following their success in natural language processing (NLP), there has been\na shift towards transformer models in computer vision. While transformers\nperform well and offer promising multi-tasking performance, due to their high\ncompute requirements, many resource-constrained applications still rely on\nconvolutional or hybrid models that combine the benefits of convolution and\nattention layers and achieve the best results in the sub 100M parameter range.\nSimultaneously, task adaptation techniques that allow for the use of one shared\ntransformer backbone for multiple downstream tasks, resulting in great storage\nsavings at negligible cost in performance, have not yet been adopted for hybrid\ntransformers. In this work, we investigate how to achieve the best\ntask-adaptation performance and introduce PETAH: Parameter Efficient Task\nAdaptation for Hybrid Transformers. We further combine PETAH adaptation with\npruning to achieve highly performant and storage friendly models for\nmulti-tasking. In our extensive evaluation on classification and other vision\ntasks, we demonstrate that our PETAH-adapted hybrid models outperform\nestablished task-adaptation techniques for ViTs while requiring fewer\nparameters and being more efficient on mobile hardware.", "paper_summary_zh": "\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u9818\u57df\u53d6\u5f97\u6210\u529f\u5f8c\uff0c\u96fb\u8166\u8996\u89ba\u9818\u57df\u958b\u59cb\u8f49\u5411\u4f7f\u7528 Transformer \u6a21\u578b\u3002\u96d6\u7136 Transformer \u7684\u8868\u73fe\u826f\u597d\uff0c\u4e14\u63d0\u4f9b\u4ee4\u4eba\u671f\u5f85\u7684\u591a\u5de5\u8655\u7406\u6548\u80fd\uff0c\u4f46\u7531\u65bc\u5176\u904b\u7b97\u9700\u6c42\u9ad8\uff0c\u8a31\u591a\u8cc7\u6e90\u53d7\u9650\u7684\u61c9\u7528\u7a0b\u5f0f\u4ecd\u4ef0\u8cf4\u5377\u7a4d\u6216\u6df7\u5408\u6a21\u578b\uff0c\u9019\u7a2e\u6a21\u578b\u7d50\u5408\u4e86\u5377\u7a4d\u548c\u6ce8\u610f\u529b\u5c64\u7684\u512a\u9ede\uff0c\u4e26\u5728\u4f4e\u65bc 1 \u5104\u500b\u53c3\u6578\u7684\u7bc4\u570d\u5167\u9054\u5230\u6700\u4f73\u7d50\u679c\u3002\u540c\u6642\uff0c\u5141\u8a31\u5c07\u4e00\u500b\u5171\u7528\u7684 Transformer \u4e3b\u5e79\u7528\u65bc\u591a\u500b\u4e0b\u6e38\u4efb\u52d9\u7684\u4efb\u52d9\u9069\u61c9\u6280\u8853\uff0c\u53ef\u5927\u5e45\u7bc0\u7701\u5132\u5b58\u7a7a\u9593\uff0c\u4e14\u6548\u80fd\u640d\u5931\u6975\u5c0f\uff0c\u4f46\u5c1a\u672a\u7528\u65bc\u6df7\u5408 Transformer\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u5982\u4f55\u9054\u6210\u6700\u4f73\u4efb\u52d9\u9069\u61c9\u6548\u80fd\uff0c\u4e26\u4ecb\u7d39 PETAH\uff1a\u6df7\u5408 Transformer \u7684\u53c3\u6578\u9ad8\u6548\u4efb\u52d9\u9069\u61c9\u3002\u6211\u5011\u9032\u4e00\u6b65\u7d50\u5408 PETAH \u9069\u61c9\u548c\u526a\u679d\uff0c\u4ee5\u9054\u6210\u9ad8\u6027\u80fd\u4e14\u5132\u5b58\u7a7a\u9593\u53cb\u5584\u7684\u591a\u5de5\u8655\u7406\u6a21\u578b\u3002\u5728\u6211\u5011\u5c0d\u5206\u985e\u548c\u5176\u4ed6\u8996\u89ba\u4efb\u52d9\u9032\u884c\u7684\u5ee3\u6cdb\u8a55\u4f30\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684 PETAH \u9069\u61c9\u6df7\u5408\u6a21\u578b\u512a\u65bc ViT \u7684\u65e2\u5b9a\u4efb\u52d9\u9069\u61c9\u6280\u8853\uff0c\u540c\u6642\u6240\u9700\u53c3\u6578\u66f4\u5c11\uff0c\u4e14\u5728\u884c\u52d5\u88dd\u7f6e\u786c\u9ad4\u4e0a\u7684\u6548\u7387\u66f4\u9ad8\u3002", "author": "Maximilian Augustin et.al.", "authors": "Maximilian Augustin, Syed Shakib Sarwar, Mostafa Elhoushi, Sai Qian Zhang, Yuecheng Li, Barbara De Salvo", "id": "2410.17661v1", "paper_url": "http://arxiv.org/abs/2410.17661v1", "repo": "null"}}