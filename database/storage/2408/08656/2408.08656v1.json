{"2408.08656": {"publish_time": "2024-08-16", "title": "LLMs Are Biased Towards Output Formats! Systematically Evaluating and Mitigating Output Format Bias of LLMs", "paper_summary": "We present the first systematic evaluation examining format bias in\nperformance of large language models (LLMs). Our approach distinguishes between\ntwo categories of an evaluation metric under format constraints to reliably and\naccurately assess performance: one measures performance when format constraints\nare adhered to, while the other evaluates performance regardless of constraint\nadherence. We then define a metric for measuring the format bias of LLMs and\nestablish effective strategies to reduce it. Subsequently, we present our\nempirical format bias evaluation spanning four commonly used categories --\nmultiple-choice question-answer, wrapping, list, and mapping -- covering 15\nwidely-used formats. Our evaluation on eight generation tasks uncovers\nsignificant format bias across state-of-the-art LLMs. We further discover that\nimproving the format-instruction following capabilities of LLMs across formats\npotentially reduces format bias. Based on our evaluation findings, we study\nprompting and fine-tuning with synthesized format data techniques to mitigate\nformat bias. Our methods successfully reduce the variance in ChatGPT's\nperformance among wrapping formats from 235.33 to 0.71 (%$^2$).", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u7b2c\u4e00\u500b\u7cfb\u7d71\u6027\u8a55\u4f30\uff0c\u7528\u65bc\u6aa2\u9a57\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6548\u80fd\u4e2d\u7684\u683c\u5f0f\u504f\u5dee\u3002\u6211\u5011\u7684\u505a\u6cd5\u5340\u5206\u4e86\u5728\u683c\u5f0f\u9650\u5236\u4e0b\uff0c\u8a55\u4f30\u6307\u6a19\u7684\u5169\u500b\u985e\u5225\uff0c\u4ee5\u53ef\u9760\u4e14\u6e96\u78ba\u5730\u8a55\u4f30\u6548\u80fd\uff1a\u4e00\u500b\u662f\u6e2c\u91cf\u5728\u9075\u5b88\u683c\u5f0f\u9650\u5236\u4e0b\u7684\u6548\u80fd\uff0c\u800c\u53e6\u4e00\u500b\u5247\u8a55\u4f30\u4e0d\u8ad6\u662f\u5426\u9075\u5b88\u9650\u5236\u7684\u6548\u80fd\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5b9a\u7fa9\u4e00\u500b\u6307\u6a19\uff0c\u7528\u65bc\u6e2c\u91cf LLM \u7684\u683c\u5f0f\u504f\u5dee\uff0c\u4e26\u5efa\u7acb\u6709\u6548\u7684\u7b56\u7565\u4f86\u6e1b\u5c11\u5b83\u3002\u96a8\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u6211\u5011\u7684\u7d93\u9a57\u683c\u5f0f\u504f\u5dee\u8a55\u4f30\uff0c\u6db5\u84cb\u56db\u500b\u5e38\u7528\u7684\u985e\u5225\u2014\u2014\u591a\u91cd\u9078\u64c7\u554f\u7b54\u3001\u63db\u884c\u3001\u6e05\u55ae\u548c\u5c0d\u61c9\u2014\u2014\u6db5\u84cb 15 \u7a2e\u5ee3\u6cdb\u4f7f\u7528\u7684\u683c\u5f0f\u3002\u6211\u5011\u5c0d\u516b\u500b\u7522\u751f\u4efb\u52d9\u7684\u8a55\u4f30\u63ed\u793a\u4e86\u6700\u5148\u9032\u7684 LLM \u4e2d\u7684\u986f\u8457\u683c\u5f0f\u504f\u5dee\u3002\u6211\u5011\u9032\u4e00\u6b65\u767c\u73fe\uff0c\u6539\u5584 LLM \u5728\u6240\u6709\u683c\u5f0f\u4e2d\u9075\u5faa\u683c\u5f0f\u8aaa\u660e\u7684\u80fd\u529b\uff0c\u53ef\u80fd\u6703\u6e1b\u5c11\u683c\u5f0f\u504f\u5dee\u3002\u6839\u64da\u6211\u5011\u7684\u8a55\u4f30\u7d50\u679c\uff0c\u6211\u5011\u7814\u7a76\u63d0\u793a\u548c\u5fae\u8abf\uff0c\u4e26\u4f7f\u7528\u5408\u6210\u7684\u683c\u5f0f\u6578\u64da\u6280\u8853\u4f86\u6e1b\u8f15\u683c\u5f0f\u504f\u5dee\u3002\u6211\u5011\u7684\u9019\u4e9b\u65b9\u6cd5\u6210\u529f\u5730\u5c07 ChatGPT \u5728\u63db\u884c\u683c\u5f0f\u4e2d\u7684\u6548\u80fd\u5dee\u7570\u5f9e 235.33 \u964d\u4f4e\u5230 0.71 (%$^2$)\u3002", "author": "Do Xuan Long et.al.", "authors": "Do Xuan Long, Hai Nguyen Ngoc, Tiviatis Sim, Hieu Dao, Shafiq Joty, Kenji Kawaguchi, Nancy F. Chen, Min-Yen Kan", "id": "2408.08656v1", "paper_url": "http://arxiv.org/abs/2408.08656v1", "repo": "null"}}