{"2405.20654": {"publish_time": "2024-05-31", "title": "Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "paper_summary": "Effective passage retrieval and reranking methods have been widely utilized\nto identify suitable candidates in open-domain question answering tasks, recent\nstudies have resorted to LLMs for reranking the retrieved passages by the\nlog-likelihood of the question conditioned on each passage. Although these\nmethods have demonstrated promising results, the performance is notably\nsensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs\ncan be computationally intensive and time-consuming. Furthermore, this approach\nlimits the leverage of question-passage relevance pairs and passage-specific\nknowledge to enhance the ranking capabilities of LLMs. In this paper, we\npropose passage-specific prompt tuning for reranking in open-domain question\nanswering (PSPT): a parameter-efficient method that fine-tunes learnable\npassage-specific soft prompts, incorporating passage-specific knowledge from a\nlimited set of question-passage relevance pairs. The method involves ranking\nretrieved passages based on the log-likelihood of the model generating the\nquestion conditioned on each passage and the learned soft prompt. We conducted\nextensive experiments utilizing the Llama-2-chat-7B model across three publicly\navailable open-domain question answering datasets and the results demonstrate\nthe effectiveness of the proposed approach.", "paper_summary_zh": "\u6709\u6548\u7684\u6bb5\u843d\u64f7\u53d6\u548c\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u5df2\u88ab\u5ee3\u6cdb\u7528\u65bc\u8b58\u5225\u958b\u653e\u9818\u57df\u554f\u984c\u56de\u7b54\u4efb\u52d9\u4e2d\u7684\u5408\u9069\u5019\u9078\u6bb5\u843d\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u8a34\u8af8 LLMs \u900f\u904e\u6bcf\u6bb5\u843d\u689d\u4ef6\u4e0b\u7684\u554f\u984c\u5c0d\u6578\u4f3c\u7136\u503c\u5c0d\u64f7\u53d6\u7684\u6bb5\u843d\u9032\u884c\u91cd\u65b0\u6392\u5e8f\u3002\u5118\u7ba1\u9019\u4e9b\u65b9\u6cd5\u5df2\u5c55\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7d50\u679c\uff0c\u4f46\u6548\u80fd\u986f\u8457\u5730\u53d7\u5230\u4eba\u5de5\u64b0\u5beb\u63d0\u793a\uff08\u6216\u786c\u63d0\u793a\uff09\u7684\u5f71\u97ff\uff0c\u4e14\u5fae\u8abf LLM \u53ef\u80fd\u5728\u8a08\u7b97\u4e0a\u5f88\u5bc6\u96c6\u4e14\u8017\u6642\u3002\u6b64\u5916\uff0c\u6b64\u65b9\u6cd5\u9650\u5236\u4e86\u554f\u984c\u6bb5\u843d\u76f8\u95dc\u6027\u5c0d\u548c\u6bb5\u843d\u7279\u5b9a\u77e5\u8b58\u7684\u69d3\u687f\u4f5c\u7528\uff0c\u4ee5\u589e\u5f37 LLM \u7684\u6392\u5e8f\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u958b\u653e\u9818\u57df\u554f\u984c\u56de\u7b54\u4e2d\u7684\u6bb5\u843d\u7279\u5b9a\u63d0\u793a\u8abf\u6574\u4ee5\u9032\u884c\u91cd\u65b0\u6392\u5e8f\uff08PSPT\uff09\uff1a\u4e00\u7a2e\u53c3\u6578\u6709\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u5fae\u8abf\u53ef\u5b78\u7fd2\u7684\u6bb5\u843d\u7279\u5b9a\u8edf\u63d0\u793a\uff0c\u4e26\u5f9e\u4e00\u7d44\u6709\u9650\u7684\u554f\u984c\u6bb5\u843d\u76f8\u95dc\u6027\u5c0d\u4e2d\u7d0d\u5165\u6bb5\u843d\u7279\u5b9a\u77e5\u8b58\u3002\u6b64\u65b9\u6cd5\u6d89\u53ca\u6839\u64da\u6a21\u578b\u5728\u6bcf\u6bb5\u843d\u689d\u4ef6\u4e0b\u7522\u751f\u554f\u984c\u7684\u5c0d\u6578\u4f3c\u7136\u503c\u548c\u5b78\u7fd2\u7684\u8edf\u63d0\u793a\u5c0d\u64f7\u53d6\u7684\u6bb5\u843d\u9032\u884c\u6392\u5e8f\u3002\u6211\u5011\u5229\u7528 Llama-2-chat-7B \u6a21\u578b\u5c0d\u4e09\u500b\u516c\u958b\u53ef\u7528\u7684\u958b\u653e\u9818\u57df\u554f\u984c\u56de\u7b54\u8cc7\u6599\u96c6\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u7d50\u679c\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "author": "Xuyang Wu et.al.", "authors": "Xuyang Wu, Zhiyuan Peng, Sravanthi Rajanala, Hsin-Tai Wu, Yi Fang", "id": "2405.20654v1", "paper_url": "http://arxiv.org/abs/2405.20654v1", "repo": "null"}}