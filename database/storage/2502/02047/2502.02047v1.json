{"2502.02047": {"publish_time": "2025-02-04", "title": "AmaSQuAD: A Benchmark for Amharic Extractive Question Answering", "paper_summary": "This research presents a novel framework for translating extractive\nquestion-answering datasets into low-resource languages, as demonstrated by the\ncreation of the AmaSQuAD dataset, a translation of SQuAD 2.0 into Amharic. The\nmethodology addresses challenges related to misalignment between translated\nquestions and answers, as well as the presence of multiple answer instances in\nthe translated context. For this purpose, we used cosine similarity utilizing\nembeddings from a fine-tuned BERT-based model for Amharic and Longest Common\nSubsequence (LCS). Additionally, we fine-tune the XLM-R model on the AmaSQuAD\nsynthetic dataset for Amharic Question-Answering. The results show an\nimprovement in baseline performance, with the fine-tuned model achieving an\nincrease in the F1 score from 36.55% to 44.41% and 50.01% to 57.5% on the\nAmaSQuAD development dataset. Moreover, the model demonstrates improvement on\nthe human-curated AmQA dataset, increasing the F1 score from 67.80% to 68.80%\nand the exact match score from 52.50% to 52.66%.The AmaSQuAD dataset is\npublicly available Datasets", "paper_summary_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u7528\u65bc\u5c07\u8403\u53d6\u5f0f\u554f\u7b54\u8cc7\u6599\u96c6\u7ffb\u8b6f\u6210\u4f4e\u8cc7\u6e90\u8a9e\u8a00\uff0c\u6b63\u5982 AmaSQuAD \u8cc7\u6599\u96c6\u7684\u5efa\u7acb\u6240\u5c55\u793a\u7684\u90a3\u6a23\uff0c\u5b83\u662f\u5c07 SQuAD 2.0 \u7ffb\u8b6f\u6210\u963f\u59c6\u54c8\u62c9\u8a9e\u3002\u8a72\u65b9\u6cd5\u89e3\u6c7a\u4e86\u7ffb\u8b6f\u5f8c\u7684\u554f\u7b54\u4e4b\u9593\u932f\u4f4d\u4ee5\u53ca\u7ffb\u8b6f\u5f8c\u7684\u8a9e\u5883\u4e2d\u5b58\u5728\u591a\u500b\u7b54\u6848\u5be6\u4f8b\u7b49\u76f8\u95dc\u6311\u6230\u3002\u70ba\u6b64\uff0c\u6211\u5011\u4f7f\u7528\u4e86\u9918\u5f26\u76f8\u4f3c\u5ea6\uff0c\u5229\u7528\u4e86\u91dd\u5c0d\u963f\u59c6\u54c8\u62c9\u8a9e\u548c\u6700\u9577\u516c\u5171\u5b50\u5e8f\u5217 (LCS) \u7684\u7d93\u904e\u5fae\u8abf\u7684 BERT \u6a21\u578b\u4e2d\u7684\u5d4c\u5165\u3002\u6b64\u5916\uff0c\u6211\u5011\u91dd\u5c0d\u963f\u59c6\u54c8\u62c9\u8a9e\u554f\u7b54\u7684 AmaSQuAD \u5408\u6210\u8cc7\u6599\u96c6\u5fae\u8abf\u4e86 XLM-R \u6a21\u578b\u3002\u7d50\u679c\u8868\u660e\u57fa\u7dda\u6548\u80fd\u6709\u6240\u63d0\u5347\uff0c\u7d93\u904e\u5fae\u8abf\u7684\u6a21\u578b\u5728 AmaSQuAD \u958b\u767c\u8cc7\u6599\u96c6\u4e0a\u7684 F1 \u5206\u6578\u5f9e 36.55% \u63d0\u5347\u5230 44.41%\uff0c\u5f9e 50.01% \u63d0\u5347\u5230 57.5%\u3002\u6b64\u5916\uff0c\u8a72\u6a21\u578b\u5728\u4eba\u5de5\u6574\u7406\u7684 AmQA \u8cc7\u6599\u96c6\u4e0a\u4e5f\u8868\u73fe\u51fa\u63d0\u5347\uff0c\u5c07 F1 \u5206\u6578\u5f9e 67.80% \u63d0\u5347\u5230 68.80%\uff0c\u5c07\u5b8c\u5168\u5339\u914d\u5206\u6578\u5f9e 52.50% \u63d0\u5347\u5230 52.66%\u3002AmaSQuAD \u8cc7\u6599\u96c6\u662f\u516c\u958b\u7684\u8cc7\u6599\u96c6", "author": "Nebiyou Daniel Hailemariam et.al.", "authors": "Nebiyou Daniel Hailemariam, Blessed Guda, Tsegazeab Tefferi", "id": "2502.02047v1", "paper_url": "http://arxiv.org/abs/2502.02047v1", "repo": "null"}}