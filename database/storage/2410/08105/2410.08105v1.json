{"2410.08105": {"publish_time": "2024-10-10", "title": "What Makes Large Language Models Reason in (Multi-Turn) Code Generation?", "paper_summary": "Prompting techniques such as chain-of-thought have established themselves as\na popular vehicle for improving the outputs of large language models (LLMs).\nFor code generation, however, their exact mechanics and efficacy are\nunder-explored. We thus investigate the effects of a wide range of prompting\nstrategies with a focus on automatic re-prompting over multiple turns and\ncomputational requirements. After systematically decomposing reasoning,\ninstruction, and execution feedback prompts, we conduct an extensive grid\nsearch on the competitive programming benchmarks CodeContests and TACO for\nmultiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o).\nOur study reveals strategies that consistently improve performance across all\nmodels with small and large sampling budgets. We then show how finetuning with\nsuch an optimal configuration allows models to internalize the induced\nreasoning process and obtain improvements in performance and scalability for\nmulti-turn code generation.", "paper_summary_zh": "\u63d0\u793a\u6280\u8853\uff0c\u4f8b\u5982\u601d\u60f3\u93c8\uff0c\u5df2\u78ba\u7acb\u70ba\u6539\u5584\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8f38\u51fa\u7684\u71b1\u9580\u8f09\u9ad4\u3002\n\u7136\u800c\uff0c\u5c0d\u65bc\u7a0b\u5f0f\u78bc\u7522\u751f\uff0c\u5b83\u5011\u78ba\u5207\u7684\u6a5f\u5236\u548c\u6548\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7814\u7a76\u4e86\u5404\u7a2e\u63d0\u793a\u7b56\u7565\u7684\u5f71\u97ff\uff0c\u91cd\u9ede\u95dc\u6ce8\u591a\u8f2a\u81ea\u52d5\u91cd\u65b0\u63d0\u793a\u548c\u904b\u7b97\u9700\u6c42\u3002\u5728\u7cfb\u7d71\u5206\u89e3\u63a8\u7406\u3001\u6307\u4ee4\u548c\u57f7\u884c\u56de\u994b\u63d0\u793a\u5f8c\uff0c\u6211\u5011\u5c0d\u7af6\u722d\u6027\u7a0b\u5f0f\u8a2d\u8a08\u57fa\u6e96 CodeContests \u548c TACO \u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u7db2\u683c\u641c\u5c0b\uff0c\u4ee5\u6db5\u84cb\u591a\u500b LLM \u5bb6\u65cf\u548c\u898f\u6a21\uff08Llama 3.0 \u548c 3.1\u30018B\u300170B\u3001405B \u548c GPT-4o\uff09\u3002\n\u6211\u5011\u7684\u7814\u7a76\u63ed\u793a\u4e86\u5728\u6240\u6709\u6a21\u578b\u4e2d\u6301\u7e8c\u6539\u5584\u6548\u80fd\u7684\u7b56\u7565\uff0c\u9019\u4e9b\u6a21\u578b\u5177\u6709\u5c0f\u548c\u5927\u62bd\u6a23\u9810\u7b97\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u6b64\u985e\u6700\u4f73\u914d\u7f6e\u9032\u884c\u5fae\u8abf\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u5167\u5316\u8a98\u5c0e\u63a8\u7406\u904e\u7a0b\uff0c\u4e26\u7372\u5f97\u591a\u8f2a\u7a0b\u5f0f\u78bc\u7522\u751f\u7684\u6548\u80fd\u548c\u53ef\u64f4\u5145\u6027\u6539\u5584\u3002", "author": "Kunhao Zheng et.al.", "authors": "Kunhao Zheng, Juliette Decugis, Jonas Gehring, Taco Cohen, Benjamin Negrevergne, Gabriel Synnaeve", "id": "2410.08105v1", "paper_url": "http://arxiv.org/abs/2410.08105v1", "repo": "null"}}