{"2501.06937": {"publish_time": "2025-01-12", "title": "An Empirical Study of Deep Reinforcement Learning in Continuing Tasks", "paper_summary": "In reinforcement learning (RL), continuing tasks refer to tasks where the\nagent-environment interaction is ongoing and can not be broken down into\nepisodes. These tasks are suitable when environment resets are unavailable,\nagent-controlled, or predefined but where all rewards-including those beyond\nresets-are critical. These scenarios frequently occur in real-world\napplications and can not be modeled by episodic tasks. While modern deep RL\nalgorithms have been extensively studied and well understood in episodic tasks,\ntheir behavior in continuing tasks remains underexplored. To address this gap,\nwe provide an empirical study of several well-known deep RL algorithms using a\nsuite of continuing task testbeds based on Mujoco and Atari environments,\nhighlighting several key insights concerning continuing tasks. Using these\ntestbeds, we also investigate the effectiveness of a method for improving\ntemporal-difference-based RL algorithms in continuing tasks by centering\nrewards, as introduced by Naik et al. (2024). While their work primarily\nfocused on this method in conjunction with Q-learning, our results extend their\nfindings by demonstrating that this method is effective across a broader range\nof algorithms, scales to larger tasks, and outperforms two other\nreward-centering approaches.", "paper_summary_zh": "\u5728\u5f37\u5316\u5b78\u7fd2 (RL) \u4e2d\uff0c\u6301\u7e8c\u4efb\u52d9\u662f\u6307\u4ee3\u7406\u74b0\u5883\u4e92\u52d5\u6b63\u5728\u9032\u884c\u4e14\u7121\u6cd5\u5206\u89e3\u70ba\u5404\u500b\u4e8b\u4ef6\u7684\u4efb\u52d9\u3002\u7576\u74b0\u5883\u91cd\u7f6e\u4e0d\u53ef\u7528\u3001\u53d7\u4ee3\u7406\u63a7\u5236\u6216\u9810\u5148\u5b9a\u7fa9\u4f46\u6240\u6709\u734e\u52f5\uff08\u5305\u62ec\u91cd\u7f6e\u5f8c\u7684\u734e\u52f5\uff09\u90fd\u5f88\u91cd\u8981\u6642\uff0c\u9019\u4e9b\u4efb\u52d9\u5f88\u5408\u9069\u3002\u9019\u4e9b\u5834\u666f\u7d93\u5e38\u767c\u751f\u5728\u771f\u5be6\u4e16\u754c\u7684\u61c9\u7528\u4e2d\uff0c\u4e14\u7121\u6cd5\u900f\u904e\u4e8b\u4ef6\u4efb\u52d9\u5efa\u6a21\u3002\u5118\u7ba1\u73fe\u4ee3\u6df1\u5ea6 RL \u6f14\u7b97\u6cd5\u5df2\u7d93\u5728\u4e8b\u4ef6\u4efb\u52d9\u4e2d\u5ee3\u6cdb\u7814\u7a76\u4e14\u5ee3\u70ba\u4e86\u89e3\uff0c\u5b83\u5011\u5728\u6301\u7e8c\u4efb\u52d9\u4e2d\u7684\u884c\u70ba\u4ecd\u672a\u5145\u5206\u63a2\u8a0e\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u9805\u4f7f\u7528\u57fa\u65bc Mujoco \u548c Atari \u74b0\u5883\u7684\u4e00\u7d44\u6301\u7e8c\u4efb\u52d9\u6e2c\u8a66\u5e73\u53f0\u7684\u5e7e\u7a2e\u773e\u6240\u5468\u77e5\u7684\u6df1\u5ea6 RL \u6f14\u7b97\u6cd5\u7684\u5be6\u8b49\u7814\u7a76\uff0c\u91cd\u9ede\u8aaa\u660e\u4e86\u5e7e\u500b\u95dc\u65bc\u6301\u7e8c\u4efb\u52d9\u7684\u91cd\u8981\u898b\u89e3\u3002\u4f7f\u7528\u9019\u4e9b\u6e2c\u8a66\u5e73\u53f0\uff0c\u6211\u5011\u9084\u7814\u7a76\u4e86\u4e00\u7a2e\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8a72\u65b9\u6cd5\u900f\u904e\u96c6\u4e2d\u734e\u52f5\u4f86\u6539\u5584\u6301\u7e8c\u4efb\u52d9\u4e2d\u7684\u57fa\u65bc\u6642\u9593\u5dee\u5206\u7684 RL \u6f14\u7b97\u6cd5\uff0c\u6b63\u5982 Naik \u7b49\u4eba (2024) \u6240\u4ecb\u7d39\u7684\u90a3\u6a23\u3002\u5118\u7ba1\u4ed6\u5011\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u65bc\u7d50\u5408 Q \u5b78\u7fd2\u7684\u9019\u7a2e\u65b9\u6cd5\uff0c\u4f46\u6211\u5011\u7684\u7d50\u679c\u900f\u904e\u8b49\u660e\u9019\u7a2e\u65b9\u6cd5\u5728\u66f4\u5ee3\u6cdb\u7684\u6f14\u7b97\u6cd5\u4e2d\u6709\u6548\u3001\u64f4\u5c55\u5230\u66f4\u5927\u7684\u4efb\u52d9\uff0c\u4e14\u512a\u65bc\u5176\u4ed6\u5169\u7a2e\u734e\u52f5\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u4f86\u64f4\u5c55\u4ed6\u5011\u7684\u767c\u73fe\u3002", "author": "Yi Wan et.al.", "authors": "Yi Wan, Dmytro Korenkevych, Zheqing Zhu", "id": "2501.06937v1", "paper_url": "http://arxiv.org/abs/2501.06937v1", "repo": "https://github.com/facebookresearch/deeprl-continuing-tasks"}}