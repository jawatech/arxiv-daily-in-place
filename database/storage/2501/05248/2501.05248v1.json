{"2501.05248": {"publish_time": "2025-01-09", "title": "Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning", "paper_summary": "Large Language Models (LLMs) have demonstrated their exceptional performance\nin various complex code generation tasks. However, their broader adoption is\nlimited by significant computational demands and high resource requirements,\nparticularly memory and processing power. To mitigate such requirements, model\npruning techniques are used to create more compact models with significantly\nfewer parameters. However, current approaches do not focus on the efficient\nextraction of programming-language-specific sub-models. In this work, we\nexplore the idea of efficiently deriving coding-specific sub-models through\nunstructured pruning (i.e., Wanda). We investigate the impact of different\ndomain-specific calibration datasets on pruning outcomes across three distinct\ndomains and extend our analysis to extracting four language-specific\nsub-models: Python, Java, C++, and JavaScript. We are the first to efficiently\nextract programming-language-specific sub-models using appropriate calibration\ndatasets while maintaining acceptable accuracy w.r.t. full models. We are also\nthe first to provide analytical evidence that domain-specific tasks activate\ndistinct regions within LLMs, supporting the creation of specialized sub-models\nthrough unstructured pruning. We believe that this work has significant\npotential to enhance LLM accessibility for coding by reducing computational\nrequirements to enable local execution on consumer-grade hardware, and\nsupporting faster inference times critical for real-time development feedback.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5176\u5728\u5404\u7a2e\u8907\u96dc\u7a0b\u5f0f\u78bc\u7522\u751f\u4efb\u52d9\u4e2d\u7684\u5353\u8d8a\u6548\u80fd\u3002\u7136\u800c\uff0c\u5176\u66f4\u5ee3\u6cdb\u7684\u63a1\u7528\u53d7\u5230\u986f\u8457\u7684\u904b\u7b97\u9700\u6c42\u548c\u9ad8\u8cc7\u6e90\u9700\u6c42\u7684\u9650\u5236\uff0c\u7279\u5225\u662f\u8a18\u61b6\u9ad4\u548c\u8655\u7406\u80fd\u529b\u3002\u70ba\u4e86\u6e1b\u8f15\u6b64\u985e\u9700\u6c42\uff0c\u6a21\u578b\u526a\u679d\u6280\u8853\u7528\u65bc\u5efa\u7acb\u66f4\u7cbe\u7c21\u7684\u6a21\u578b\uff0c\u5176\u53c3\u6578\u986f\u8457\u6e1b\u5c11\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u505a\u6cd5\u4e26\u672a\u5c08\u6ce8\u65bc\u7a0b\u5f0f\u8a9e\u8a00\u7279\u5b9a\u5b50\u6a21\u578b\u7684\u6709\u6548\u63d0\u53d6\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u900f\u904e\u975e\u7d50\u69cb\u5316\u526a\u679d\uff08\u5373 Wanda\uff09\u6709\u6548\u884d\u751f\u7279\u5b9a\u65bc\u7de8\u78bc\u7684\u5b50\u6a21\u578b\u7684\u60f3\u6cd5\u3002\u6211\u5011\u7814\u7a76\u4e86\u4e0d\u540c\u7279\u5b9a\u65bc\u9818\u57df\u7684\u6821\u6b63\u8cc7\u6599\u96c6\u5c0d\u4e09\u500b\u4e0d\u540c\u9818\u57df\u7684\u526a\u679d\u7d50\u679c\u7684\u5f71\u97ff\uff0c\u4e26\u5c07\u6211\u5011\u7684\u5206\u6790\u5ef6\u4f38\u81f3\u63d0\u53d6\u56db\u500b\u7279\u5b9a\u65bc\u8a9e\u8a00\u7684\u5b50\u6a21\u578b\uff1aPython\u3001Java\u3001C++ \u548c JavaScript\u3002\u6211\u5011\u662f\u7b2c\u4e00\u500b\u4f7f\u7528\u9069\u7576\u7684\u6821\u6b63\u8cc7\u6599\u96c6\u6709\u6548\u63d0\u53d6\u7279\u5b9a\u65bc\u7a0b\u5f0f\u8a9e\u8a00\u7684\u5b50\u6a21\u578b\uff0c\u540c\u6642\u7dad\u6301\u53ef\u63a5\u53d7\u7684\u6e96\u78ba\u5ea6\uff0c\u76f8\u5c0d\u65bc\u5b8c\u6574\u7684\u6a21\u578b\u3002\u6211\u5011\u4e5f\u662f\u7b2c\u4e00\u500b\u63d0\u4f9b\u5206\u6790\u8b49\u64da\uff0c\u8b49\u660e\u7279\u5b9a\u65bc\u9818\u57df\u7684\u4efb\u52d9\u6703\u5728 LLM \u5167\u555f\u52d5\u4e0d\u540c\u7684\u5340\u57df\uff0c\u652f\u63f4\u900f\u904e\u975e\u7d50\u69cb\u5316\u526a\u679d\u5efa\u7acb\u5c08\u9580\u7684\u5b50\u6a21\u578b\u3002\u6211\u5011\u76f8\u4fe1\uff0c\u9019\u9805\u5de5\u4f5c\u5177\u6709\u986f\u8457\u7684\u6f5b\u529b\uff0c\u53ef\u900f\u904e\u964d\u4f4e\u904b\u7b97\u9700\u6c42\u4ee5\u5728\u6d88\u8cbb\u7d1a\u786c\u9ad4\u4e0a\u9032\u884c\u672c\u5730\u57f7\u884c\uff0c\u5f9e\u800c\u589e\u5f37 LLM \u5728\u7de8\u78bc\u65b9\u9762\u7684\u53ef\u53ca\u6027\uff0c\u4e26\u652f\u63f4\u5c0d\u65bc\u5373\u6642\u958b\u767c\u56de\u994b\u81f3\u95dc\u91cd\u8981\u7684\u66f4\u5feb\u901f\u7684\u63a8\u8ad6\u6642\u9593\u3002", "author": "Laura Puccioni et.al.", "authors": "Laura Puccioni, Alireza Farshin, Mariano Scazzariello, Changjie Wang, Marco Chiesa, Dejan Kostic", "id": "2501.05248v1", "paper_url": "http://arxiv.org/abs/2501.05248v1", "repo": "null"}}