{"2501.03200": {"publish_time": "2025-01-06", "title": "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input", "paper_summary": "We introduce FACTS Grounding, an online leaderboard and associated benchmark\nthat evaluates language models' ability to generate text that is factually\naccurate with respect to given context in the user prompt. In our benchmark,\neach prompt includes a user request and a full document, with a maximum length\nof 32k tokens, requiring long-form responses. The long-form responses are\nrequired to be fully grounded in the provided context document while fulfilling\nthe user request. Models are evaluated using automated judge models in two\nphases: (1) responses are disqualified if they do not fulfill the user request;\n(2) they are judged as accurate if the response is fully grounded in the\nprovided document. The automated judge models were comprehensively evaluated\nagainst a held-out test-set to pick the best prompt template, and the final\nfactuality score is an aggregate of multiple judge models to mitigate\nevaluation bias. The FACTS Grounding leaderboard will be actively maintained\nover time, and contains both public and private splits to allow for external\nparticipation while guarding the integrity of the leaderboard. It can be found\nat https://www.kaggle.com/facts-leaderboard.", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86 FACTS Grounding\uff0c\u9019\u662f\u4e00\u500b\u7dda\u4e0a\u6392\u884c\u699c\u548c\u76f8\u95dc\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30\u8a9e\u8a00\u6a21\u578b\u6839\u64da\u4f7f\u7528\u8005\u63d0\u793a\u4e2d\u63d0\u4f9b\u7684\u80cc\u666f\uff0c\u7522\u751f\u4e8b\u5be6\u4e0a\u6e96\u78ba\u7684\u6587\u672c\u7684\u80fd\u529b\u3002\u5728\u6211\u5011\u7684\u57fa\u6e96\u4e2d\uff0c\u6bcf\u500b\u63d0\u793a\u90fd\u5305\u542b\u4e00\u500b\u4f7f\u7528\u8005\u8acb\u6c42\u548c\u4e00\u500b\u5b8c\u6574\u6587\u4ef6\uff0c\u6700\u5927\u9577\u5ea6\u70ba 32k \u500b\u7b26\u865f\uff0c\u9700\u8981\u9577\u7bc7\u56de\u61c9\u3002\u9577\u7bc7\u56de\u61c9\u5fc5\u9808\u5b8c\u5168\u5efa\u7acb\u5728\u63d0\u4f9b\u7684\u80cc\u666f\u6587\u4ef6\u4e2d\uff0c\u540c\u6642\u6eff\u8db3\u4f7f\u7528\u8005\u7684\u8acb\u6c42\u3002\u6a21\u578b\u4f7f\u7528\u81ea\u52d5\u8a55\u5206\u6a21\u578b\u5728\u5169\u500b\u968e\u6bb5\u9032\u884c\u8a55\u4f30\uff1a(1) \u5982\u679c\u56de\u61c9\u4e0d\u6eff\u8db3\u4f7f\u7528\u8005\u7684\u8acb\u6c42\uff0c\u5247\u53d6\u6d88\u8cc7\u683c\uff1b(2) \u5982\u679c\u56de\u61c9\u5b8c\u5168\u5efa\u7acb\u5728\u63d0\u4f9b\u7684\u6587\u4ef6\u4e2d\uff0c\u5247\u5224\u65b7\u70ba\u6e96\u78ba\u3002\u81ea\u52d5\u8a55\u5206\u6a21\u578b\u6839\u64da\u4fdd\u7559\u7684\u6e2c\u8a66\u96c6\u9032\u884c\u5168\u9762\u8a55\u4f30\uff0c\u4ee5\u9078\u64c7\u6700\u4f73\u63d0\u793a\u7bc4\u672c\uff0c\u6700\u7d42\u7684\u4e8b\u5be6\u5206\u6578\u662f\u591a\u500b\u8a55\u5206\u6a21\u578b\u7684\u7e3d\u548c\uff0c\u4ee5\u6e1b\u8f15\u8a55\u4f30\u504f\u5dee\u3002FACTS Grounding \u6392\u884c\u699c\u5c07\u96a8\u8457\u6642\u9593\u7a4d\u6975\u7dad\u8b77\uff0c\u4e26\u5305\u542b\u516c\u958b\u548c\u79c1\u4eba\u62c6\u5206\uff0c\u4ee5\u5141\u8a31\u5916\u90e8\u53c3\u8207\uff0c\u540c\u6642\u4fdd\u8b77\u6392\u884c\u699c\u7684\u5b8c\u6574\u6027\u3002\u5b83\u53ef\u4ee5\u5728 https://www.kaggle.com/facts-leaderboard \u627e\u5230\u3002", "author": "Alon Jacovi et.al.", "authors": "Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, Sasha Goldshtein, Dipanjan Das", "id": "2501.03200v1", "paper_url": "http://arxiv.org/abs/2501.03200v1", "repo": "null"}}