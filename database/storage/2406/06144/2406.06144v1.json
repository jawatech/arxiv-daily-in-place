{"2406.06144": {"publish_time": "2024-06-10", "title": "Language Models Resist Alignment", "paper_summary": "Large language models (LLMs) may exhibit undesirable behaviors. Recent\nefforts have focused on aligning these models to prevent harmful generation.\nDespite these efforts, studies have shown that even a well-conducted alignment\nprocess can be easily circumvented, whether intentionally or accidentally. Do\nalignment fine-tuning have robust effects on models, or are merely superficial?\nIn this work, we answer this question through both theoretical and empirical\nmeans. Empirically, we demonstrate the elasticity of post-alignment models,\ni.e., the tendency to revert to the behavior distribution formed during the\npre-training phase upon further fine-tuning. Using compression theory, we\nformally derive that such fine-tuning process \\textit{disproportionately}\nundermines alignment compared to pre-training, potentially by orders of\nmagnitude. We conduct experimental validations to confirm the presence of\nelasticity across models of varying types and sizes. Specifically, we find that\nmodel performance declines rapidly before reverting to the pre-training\ndistribution, after which the rate of decline drops significantly. We further\nreveal that elasticity positively correlates with increased model size and the\nexpansion of pre-training data. Our discovery signifies the importance of\ntaming the inherent elasticity of LLMs, thereby overcoming the resistance of\nLLMs to alignment finetuning.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u80fd\u6703\u8868\u73fe\u51fa\u4e0d\u826f\u884c\u70ba\u3002\u6700\u8fd1\u7684\u52aa\u529b\u96c6\u4e2d\u65bc\u8abf\u6574\u9019\u4e9b\u6a21\u578b\u4ee5\u9632\u6b62\u6709\u5bb3\u751f\u6210\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u52aa\u529b\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u9032\u884c\u826f\u597d\u7684\u8abf\u6574\u904e\u7a0b\uff0c\u4e5f\u53ef\u80fd\u5728\u6709\u610f\u6216\u7121\u610f\u9593\u8f15\u6613\u5730\u88ab\u8ff4\u907f\u3002\u8abf\u6574\u5fae\u8abf\u5c0d\u6a21\u578b\u6709\u5f37\u5927\u7684\u5f71\u97ff\uff0c\u9084\u662f\u50c5\u50c5\u662f\u8868\u9762\u4e0a\u7684\uff1f\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u901a\u904e\u7406\u8ad6\u548c\u7d93\u9a57\u65b9\u6cd5\u56de\u7b54\u9019\u500b\u554f\u984c\u3002\u7d93\u9a57\u4e0a\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5f8c\u8abf\u6574\u6a21\u578b\u7684\u5f48\u6027\uff0c\u5373\u5728\u9032\u4e00\u6b65\u5fae\u8abf\u6642\u6062\u5fa9\u5230\u9810\u8a13\u7df4\u968e\u6bb5\u5f62\u6210\u7684\u884c\u70ba\u5206\u4f48\u7684\u8da8\u52e2\u3002\u4f7f\u7528\u58d3\u7e2e\u7406\u8ad6\uff0c\u6211\u5011\u6b63\u5f0f\u63a8\u5c0e\u51fa\u9019\u7a2e\u5fae\u8abf\u904e\u7a0b\u8207\u9810\u8a13\u7df4\u76f8\u6bd4\uff0c\u5c0d\u9f4a\u65b9\u5f0f\u7684\u5f71\u97ff\u300c\u4e0d\u6210\u6bd4\u4f8b\u300d\uff0c\u53ef\u80fd\u9054\u5230\u6578\u91cf\u7d1a\u3002\u6211\u5011\u9032\u884c\u5be6\u9a57\u9a57\u8b49\u4ee5\u78ba\u8a8d\u5404\u7a2e\u985e\u578b\u548c\u898f\u6a21\u7684\u6a21\u578b\u4e2d\u5f48\u6027\u7684\u5b58\u5728\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u767c\u73fe\u6a21\u578b\u6027\u80fd\u5728\u6062\u5fa9\u5230\u9810\u8a13\u7df4\u5206\u4f48\u4e4b\u524d\u8fc5\u901f\u4e0b\u964d\uff0c\u4e4b\u5f8c\u4e0b\u964d\u7387\u986f\u8457\u4e0b\u964d\u3002\u6211\u5011\u9032\u4e00\u6b65\u63ed\u793a\u5f48\u6027\u8207\u6a21\u578b\u5c3a\u5bf8\u7684\u589e\u52a0\u548c\u9810\u8a13\u7df4\u6578\u64da\u7684\u64f4\u5c55\u5448\u6b63\u76f8\u95dc\u3002\u6211\u5011\u7684\u767c\u73fe\u8868\u660e\u99b4\u670d LLM \u56fa\u6709\u7684\u5f48\u6027\u975e\u5e38\u91cd\u8981\uff0c\u5f9e\u800c\u514b\u670d\u4e86 LLM \u5c0d\u8abf\u6574\u5fae\u8abf\u7684\u62b5\u6297\u529b\u3002", "author": "Jiaming Ji et.al.", "authors": "Jiaming Ji, Kaile Wang, Tianyi Qiu, Boyuan Chen, Jiayi Zhou, Changye Li, Hantao Lou, Yaodong Yang", "id": "2406.06144v1", "paper_url": "http://arxiv.org/abs/2406.06144v1", "repo": "null"}}