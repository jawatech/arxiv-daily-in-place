{"2409.12136": {"publish_time": "2024-09-18", "title": "GRIN: GRadient-INformed MoE", "paper_summary": "Mixture-of-Experts (MoE) models scale more effectively than dense models due\nto sparse computation through expert routing, selectively activating only a\nsmall subset of expert modules. However, sparse computation challenges\ntraditional training practices, as discrete expert routing hinders standard\nbackpropagation and thus gradient-based optimization, which are the cornerstone\nof deep learning. To better pursue the scaling power of MoE, we introduce GRIN\n(GRadient-INformed MoE training), which incorporates sparse gradient estimation\nfor expert routing and configures model parallelism to avoid token dropping.\nApplying GRIN to autoregressive language modeling, we develop a top-2\n16$\\times$3.8B MoE model. Our model, with only 6.6B activated parameters,\noutperforms a 7B dense model and matches the performance of a 14B dense model\ntrained on the same data. Extensive evaluations across diverse tasks\ndemonstrate the potential of GRIN to significantly enhance MoE efficacy,\nachieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.", "paper_summary_zh": "\u6df7\u5408\u4e13\u5bb6 (MoE) \u6a21\u578b\u6bd4\u5bc6\u96c6\u6a21\u578b\u66f4\u6709\u6548\u5730\u6269\u5c55\uff0c\u8fd9\u662f\u56e0\u4e3a\u901a\u8fc7\u4e13\u5bb6\u8def\u7531\u8fdb\u884c\u7a00\u758f\u8ba1\u7b97\uff0c\u6709\u9009\u62e9\u5730\u4ec5\u6fc0\u6d3b\u4e00\u5c0f\u90e8\u5206\u4e13\u5bb6\u6a21\u5757\u3002\u7136\u800c\uff0c\u7a00\u758f\u8ba1\u7b97\u5bf9\u4f20\u7edf\u8bad\u7ec3\u5b9e\u8df5\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u56e0\u4e3a\u79bb\u6563\u4e13\u5bb6\u8def\u7531\u963b\u788d\u4e86\u6807\u51c6\u53cd\u5411\u4f20\u64ad\uff0c\u4ece\u800c\u963b\u788d\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\uff0c\u800c\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u662f\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u77f3\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u8ffd\u6c42 MoE \u7684\u6269\u5c55\u80fd\u529b\uff0c\u6211\u4eec\u5f15\u5165\u4e86 GRIN\uff08\u68af\u5ea6\u4fe1\u606f MoE \u8bad\u7ec3\uff09\uff0c\u5b83\u7ed3\u5408\u4e86\u4e13\u5bb6\u8def\u7531\u7684\u7a00\u758f\u68af\u5ea6\u4f30\u8ba1\uff0c\u5e76\u914d\u7f6e\u6a21\u578b\u5e76\u884c\u6027\u4ee5\u907f\u514d\u4ee4\u724c\u4e22\u5931\u3002\u5c06 GRIN \u5e94\u7528\u4e8e\u81ea\u56de\u5f52\u8bed\u8a00\u5efa\u6a21\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a top-2 16\u00d73.8B MoE \u6a21\u578b\u3002\u6211\u4eec\u7684\u6a21\u578b\u4ec5\u6fc0\u6d3b\u4e86 6.6B \u4e2a\u53c2\u6570\uff0c\u4f18\u4e8e 7B \u5bc6\u96c6\u6a21\u578b\uff0c\u5e76\u4e14\u4e0e\u5728\u76f8\u540c\u6570\u636e\u4e0a\u8bad\u7ec3\u7684 14B \u5bc6\u96c6\u6a21\u578b\u7684\u6027\u80fd\u76f8\u5339\u914d\u3002\u8de8\u4e0d\u540c\u4efb\u52a1\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u660e\u4e86 GRIN \u5728\u663e\u8457\u63d0\u9ad8 MoE \u6548\u80fd\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5728 MMLU \u4e0a\u8fbe\u5230 79.4\uff0c\u5728 HellaSwag \u4e0a\u8fbe\u5230 83.7\uff0c\u5728 HumanEval \u4e0a\u8fbe\u5230 74.4\uff0c\u5728 MATH \u4e0a\u8fbe\u5230 58.9\u3002", "author": "Liyuan Liu et.al.", "authors": "Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, Weizhu Chen", "id": "2409.12136v1", "paper_url": "http://arxiv.org/abs/2409.12136v1", "repo": "null"}}