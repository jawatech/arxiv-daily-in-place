{"2406.10710": {"publish_time": "2024-06-15", "title": "SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task", "paper_summary": "Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG)\ndatabases presents a promising avenue for enhancing LLMs' efficacy and\nmitigating their \"hallucinations\". Given that most KGs reside in graph\ndatabases accessible solely through specialized query languages (e.g., Cypher),\nthere exists a critical need to bridge the divide between LLMs and KG databases\nby automating the translation of natural language into Cypher queries (commonly\ntermed the \"Text2Cypher\" task). Prior efforts tried to bolster LLMs'\nproficiency in Cypher generation through Supervised Fine-Tuning. However, these\nexplorations are hindered by the lack of annotated datasets of Query-Cypher\npairs, resulting from the labor-intensive and domain-specific nature of\nannotating such datasets. In this study, we propose SyntheT2C, a methodology\nfor constructing a synthetic Query-Cypher pair dataset, comprising two distinct\npipelines: (1) LLM-based prompting and (2) template-filling. SyntheT2C\nfacilitates the generation of extensive Query-Cypher pairs with values sampled\nfrom an underlying Neo4j graph database. Subsequently, SyntheT2C is applied to\ntwo medical databases, culminating in the creation of a synthetic dataset,\nMedT2C. Comprehensive experiments demonstrate that the MedT2C dataset\neffectively enhances the performance of backbone LLMs on the Text2Cypher task.\nBoth the SyntheT2C codebase and the MedT2C dataset will be released soon.", "paper_summary_zh": "<paragraph>\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u73fe\u6709\u7684\u77e5\u8b58\u5716\u8b5c (KG) \u8cc7\u6599\u5eab\u6574\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u63d0\u5347 LLM \u6548\u80fd\u4e26\u6e1b\u8f15\u5176\u300c\u5e7b\u89ba\u300d\u7684\u9014\u5f91\u3002\u7531\u65bc\u5927\u591a\u6578 KG \u90fd\u5b58\u5728\u65bc\u50c5\u80fd\u900f\u904e\u5c08\u7528\u67e5\u8a62\u8a9e\u8a00\uff08\u4f8b\u5982 Cypher\uff09\u5b58\u53d6\u7684\u5716\u5f62\u8cc7\u6599\u5eab\u4e2d\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u81ea\u52d5\u5316\u5c07\u81ea\u7136\u8a9e\u8a00\u8f49\u63db\u70ba Cypher \u67e5\u8a62\uff0c\u4ee5\u5f4c\u5408 LLM \u8207 KG \u8cc7\u6599\u5eab\u4e4b\u9593\u7684\u9d3b\u6e9d\uff08\u901a\u5e38\u7a31\u70ba\u300cText2Cypher\u300d\u4efb\u52d9\uff09\u3002\u5148\u524d\u7684\u52aa\u529b\u5617\u8a66\u900f\u904e\u76e3\u7763\u5fae\u8abf\u4f86\u63d0\u5347 LLM \u5728 Cypher \u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u63a2\u7d22\u53d7\u5230\u7f3a\u4e4f\u67e5\u8a62-Cypher \u914d\u5c0d\u7684\u8a3b\u89e3\u8cc7\u6599\u96c6\u7684\u963b\u7919\uff0c\u9019\u662f\u56e0\u70ba\u6b64\u985e\u8cc7\u6599\u96c6\u7684\u8a3b\u89e3\u9700\u8981\u5927\u91cf\u4eba\u529b\u4e14\u5177\u6709\u7279\u5b9a\u9818\u57df\u7684\u6027\u8cea\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 SyntheT2C\uff0c\u9019\u662f\u4e00\u7a2e\u7528\u65bc\u5efa\u69cb\u5408\u6210\u67e5\u8a62-Cypher \u914d\u5c0d\u8cc7\u6599\u96c6\u7684\u65b9\u6cd5\uff0c\u5305\u542b\u5169\u500b\u4e0d\u540c\u7684\u7ba1\u9053\uff1a(1) \u57fa\u65bc LLM \u7684\u63d0\u793a\u548c (2) \u7bc4\u672c\u586b\u5beb\u3002SyntheT2C \u4fc3\u9032\u4e86\u5927\u91cf\u67e5\u8a62-Cypher \u914d\u5c0d\u7684\u7522\u751f\uff0c\u5176\u503c\u53d6\u6a23\u81ea\u57fa\u790e\u7684 Neo4j \u5716\u5f62\u8cc7\u6599\u5eab\u3002\u96a8\u5f8c\uff0c\u5c07 SyntheT2C \u61c9\u7528\u65bc\u5169\u500b\u91ab\u7642\u8cc7\u6599\u5eab\uff0c\u6700\u7d42\u5efa\u7acb\u4e86\u4e00\u500b\u5408\u6210\u8cc7\u6599\u96c6 MedT2C\u3002\u5168\u9762\u7684\u5be6\u9a57\u8b49\u660e\uff0cMedT2C \u8cc7\u6599\u96c6\u6709\u6548\u63d0\u5347\u4e86\u4e3b\u5e79 LLM \u5728 Text2Cypher \u4efb\u52d9\u4e0a\u7684\u6548\u80fd\u3002SyntheT2C \u7a0b\u5f0f\u78bc\u5eab\u548c MedT2C \u8cc7\u6599\u96c6\u90fd\u5c07\u5f88\u5feb\u91cb\u51fa\u3002</paragraph>", "author": "Ziije Zhong et.al.", "authors": "Ziije Zhong, Linqing Zhong, Zhaoze Sun, Qingyun Jin, Zengchang Qin, Xiaofan Zhang", "id": "2406.10710v1", "paper_url": "http://arxiv.org/abs/2406.10710v1", "repo": "null"}}