{"2412.07446": {"publish_time": "2024-12-10", "title": "Causal World Representation in the GPT Model", "paper_summary": "Are generative pre-trained transformer (GPT) models only trained to predict\nthe next token, or do they implicitly learn a world model from which a sequence\nis generated one token at a time? We examine this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that\nGPT-models, at inference time, can be utilized for zero-shot causal structure\nlearning for in-distribution sequences. Empirical evaluation is conducted in a\ncontrolled synthetic environment using the setup and rules of the Othello board\ngame. A GPT, pre-trained on real-world games played with the intention of\nwinning, is tested on synthetic data that only adheres to the game rules. We\nfind that the GPT model tends to generate next moves that adhere to the game\nrules for sequences for which the attention mechanism encodes a causal\nstructure with high confidence. In general, in cases for which the GPT model\ngenerates moves that do not adhere to the game rules, it also fails to capture\nany causal structure.", "paper_summary_zh": "\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u8f6c\u6362\u5668 (GPT) \u6a21\u578b\u662f\u5426\u4ec5\u63a5\u53d7\u8fc7\u9884\u6d4b\u4e0b\u4e00\u4e2a\u6807\u8bb0\u7684\u8bad\u7ec3\uff0c\u8fd8\u662f\u5b83\u4eec\u9690\u5f0f\u5730\u5b66\u4e60\u4e86\u4e00\u4e2a\u4e16\u754c\u6a21\u578b\uff0c\u4ece\u4e2d\u4e00\u6b21\u751f\u6210\u4e00\u4e2a\u6807\u8bb0\u7684\u5e8f\u5217\uff1f\u6211\u4eec\u901a\u8fc7\u63a8\u5bfc GPT \u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u56e0\u679c\u89e3\u91ca\u6765\u68c0\u9a8c\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u7531\u6b64\u89e3\u91ca\u4ea7\u751f\u7684\u56e0\u679c\u4e16\u754c\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa GPT \u6a21\u578b\u5728\u63a8\u7406\u65f6\u53ef\u7528\u4e8e\u5206\u5e03\u5185\u5e8f\u5217\u7684\u96f6\u6837\u672c\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u3002\u4f7f\u7528\u5965\u8d5b\u7f57\u68cb\u76d8\u6e38\u620f\u7684\u8bbe\u7f6e\u548c\u89c4\u5219\uff0c\u5728\u53d7\u63a7\u7684\u5408\u6210\u73af\u5883\u4e2d\u8fdb\u884c\u7ecf\u9a8c\u8bc4\u4f30\u3002\u5bf9\u4e00\u4e2a\u7ecf\u8fc7\u9884\u8bad\u7ec3\u7684 GPT\uff08\u4f7f\u7528\u8d62\u5f97\u6e38\u620f\u7684\u610f\u56fe\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u6e38\u620f\uff09\u5728\u4ec5\u9075\u5faa\u6e38\u620f\u89c4\u5219\u7684\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5bf9\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u7f16\u7801\u56e0\u679c\u7ed3\u6784\u7684\u5e8f\u5217\uff0cGPT \u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u9075\u5b88\u6e38\u620f\u89c4\u5219\u7684\u4e0b\u4e00\u6b65\u79fb\u52a8\u3002\u4e00\u822c\u6765\u8bf4\uff0c\u5728 GPT \u6a21\u578b\u751f\u6210\u4e0d\u9075\u5b88\u6e38\u620f\u89c4\u5219\u7684\u79fb\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u4e5f\u65e0\u6cd5\u6355\u6349\u5230\u4efb\u4f55\u56e0\u679c\u7ed3\u6784\u3002", "author": "Raanan Y. Rohekar et.al.", "authors": "Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Vasudev Lal", "id": "2412.07446v1", "paper_url": "http://arxiv.org/abs/2412.07446v1", "repo": "null"}}