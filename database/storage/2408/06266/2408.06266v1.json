{"2408.06266": {"publish_time": "2024-08-12", "title": "Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment", "paper_summary": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u901a\u5e38\u4f7f\u7528\u5c0d\u6bd4\u5f0f\u6bd4\u5c0d\u76ee\u6a19\u548c\u504f\u597d\u914d\u5c0d\u8cc7\u6599\u96c6\u4f86\u5c0d\u9f4a\u3002\u6a21\u578b\u3001\u914d\u5c0d\u8cc7\u6599\u548c\u76ee\u6a19\u4e4b\u9593\u7684\u4e92\u52d5\u4f7f\u5c0d\u9f4a\u6210\u70ba\u4e00\u500b\u8907\u96dc\u7684\u7a0b\u5e8f\uff0c\u6709\u6642\u6703\u7522\u751f\u6b21\u4f73\u7d50\u679c\u3002\u6211\u5011\u7814\u7a76\u4e86\u9019\u4e00\u9ede\uff0c\u767c\u73fe (i) \u7576\u57fa\u790e\u56de\u61c9\u5177\u6709\u5c0d\u6bd4\u6027\u6642\uff0c\u504f\u597d\u8cc7\u6599\u6703\u63d0\u4f9b\u66f4\u597d\u7684\u5b78\u7fd2\u8a0a\u865f\uff0c\u4ee5\u53ca (ii) \u5c0d\u9f4a\u76ee\u6a19\u6703\u5728\u8a13\u7df4\u671f\u9593\u5c0d\u6a21\u578b\u6709\u66f4\u591a\u63a7\u5236\u6642\u5e36\u4f86\u66f4\u597d\u7684\u6548\u80fd\u3002\u6839\u64da\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86 AI \u4fee\u8a02\u5c0d\u6bd4\u5b78\u7fd2 (CLAIR)\uff0c\u9019\u662f\u4e00\u7a2e\u8cc7\u6599\u5efa\u7acb\u65b9\u6cd5\uff0c\u53ef\u7522\u751f\u66f4\u591a\u5c0d\u6bd4\u504f\u597d\u914d\u5c0d\uff0c\u4ee5\u53ca\u9328\u5b9a\u504f\u597d\u6700\u4f73\u5316 (APO)\uff0c\u9019\u662f\u4e00\u7a2e\u53ef\u63a7\u4e14\u66f4\u7a69\u5b9a\u7684\u5c0d\u9f4a\u76ee\u6a19\u3002\u6211\u5011\u4f7f\u7528\u5404\u7a2e\u53ef\u6bd4\u8f03\u7684\u8cc7\u6599\u96c6\u548c\u5c0d\u9f4a\u76ee\u6a19\u4f86\u5c0d\u9f4a Llama-3-8B-Instruct\uff0c\u4e26\u6e2c\u91cf\u8207\u4eba\u985e\u5224\u65b7\u9ad8\u5ea6\u76f8\u95dc\u7684 MixEval-Hard \u5206\u6578\u3002\u5728\u6240\u6709\u8cc7\u6599\u96c6\u4e2d\uff0cCLAIR \u504f\u597d\u5e36\u4f86\u6700\u5f37\u7684\u6548\u80fd\uff0c\u800c APO \u5247\u59cb\u7d42\u512a\u65bc\u53ef\u63a7\u6027\u8f03\u4f4e\u7684\u76ee\u6a19\u3002\u6211\u5011\u5728 32K CLAIR \u504f\u597d\u4e0a\u8a13\u7df4\u7684\u6700\u4f73\u6a21\u578b\uff0c\u4f7f\u7528 APO\uff0c\u5c07 Llama-3-8B-Instruct \u63d0\u5347\u4e86 7.65%\uff0c\u5c07\u8207 GPT4-turbo \u7684\u5dee\u8ddd\u7e2e\u5c0f\u4e86 45%\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/ContextualAI/CLAIR_and_APO \u53d6\u5f97\u3002", "author": "Karel D'Oosterlinck et.al.", "authors": "Karel D'Oosterlinck, Winnie Xu, Chris Develder, Thomas Demeester, Amanpreet Singh, Christopher Potts, Douwe Kiela, Shikib Mehri", "id": "2408.06266v1", "paper_url": "http://arxiv.org/abs/2408.06266v1", "repo": "null"}}