{"2409.03115": {"publish_time": "2024-09-04", "title": "Probing self-attention in self-supervised speech models for cross-linguistic differences", "paper_summary": "Speech models have gained traction thanks to increase in accuracy from novel\ntransformer architectures. While this impressive increase in performance across\nautomatic speech recognition (ASR) benchmarks is noteworthy, there is still\nmuch that is unknown about the use of attention mechanisms for speech-related\ntasks. For example, while it is assumed that these models are learning\nlanguage-independent (i.e., universal) speech representations, there has not\nyet been an in-depth exploration of what it would mean for the models to be\nlanguage-independent. In the current paper, we explore this question within the\nrealm of self-attention mechanisms of one small self-supervised speech\ntransformer model (TERA). We find that even with a small model, the attention\nheads learned are diverse ranging from almost entirely diagonal to almost\nentirely global regardless of the training language. We highlight some notable\ndifferences in attention patterns between Turkish and English and demonstrate\nthat the models do learn important phonological information during pretraining.\nWe also present a head ablation study which shows that models across languages\nprimarily rely on diagonal heads to classify phonemes.", "paper_summary_zh": "\u8a9e\u97f3\u6a21\u578b\u7531\u65bc\u65b0\u7a4e\u7684\u8f49\u63db\u5668\u67b6\u69cb\u7684\u6e96\u78ba\u6027\u63d0\u5347\u800c\u7372\u5f97\u95dc\u6ce8\u3002\u96d6\u7136\u5728\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u57fa\u6e96\u4e2d\u6027\u80fd\u6709\u986f\u8457\u63d0\u5347\u503c\u5f97\u6ce8\u610f\uff0c\u4f46\u5c0d\u65bc\u4f7f\u7528\u6ce8\u610f\u529b\u6a5f\u5236\u9032\u884c\u8207\u8a9e\u97f3\u76f8\u95dc\u7684\u4efb\u52d9\uff0c\u4ecd\u6709\u8a31\u591a\u672a\u77e5\u4e4b\u8655\u3002\u4f8b\u5982\uff0c\u96d6\u7136\u5047\u8a2d\u9019\u4e9b\u6a21\u578b\u6b63\u5728\u5b78\u7fd2\u8207\u8a9e\u8a00\u7121\u95dc\uff08\u5373\u901a\u7528\uff09\u7684\u8a9e\u97f3\u8868\u793a\uff0c\u4f46\u5c0d\u65bc\u6a21\u578b\u8207\u8a9e\u8a00\u7121\u95dc\u7684\u610f\u7fa9\u5c1a\u672a\u6df1\u5165\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5728\u4e00\u500b\u5c0f\u578b\u81ea\u6211\u76e3\u7763\u8a9e\u97f3\u8f49\u63db\u5668\u6a21\u578b (TERA) \u7684\u81ea\u6211\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u9818\u57df\u4e2d\u63a2\u8a0e\u9019\u500b\u554f\u984c\u3002\u6211\u5011\u767c\u73fe\u5373\u4f7f\u4f7f\u7528\u5c0f\u578b\u6a21\u578b\uff0c\u5b78\u7fd2\u5230\u7684\u6ce8\u610f\u529b\u982d\u4e5f\u662f\u591a\u6a23\u5316\u7684\uff0c\u5f9e\u5e7e\u4e4e\u5b8c\u5168\u5c0d\u89d2\u7dda\u5230\u5e7e\u4e4e\u5b8c\u5168\u5168\u5c40\uff0c\u8207\u8a13\u7df4\u8a9e\u8a00\u7121\u95dc\u3002\u6211\u5011\u91cd\u9ede\u8aaa\u660e\u571f\u8033\u5176\u8a9e\u548c\u82f1\u8a9e\u4e4b\u9593\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u4e00\u4e9b\u986f\u8457\u5dee\u7570\uff0c\u4e26\u8b49\u660e\u6a21\u578b\u5728\u9810\u8a13\u7df4\u671f\u9593\u78ba\u5be6\u5b78\u7fd2\u91cd\u8981\u7684\u97f3\u97fb\u8cc7\u8a0a\u3002\u6211\u5011\u9084\u63d0\u51fa\u4e00\u500b\u982d\u90e8\u6d88\u878d\u7814\u7a76\uff0c\u986f\u793a\u8de8\u8a9e\u8a00\u7684\u6a21\u578b\u4e3b\u8981\u4f9d\u8cf4\u5c0d\u89d2\u7dda\u982d\u90e8\u4f86\u5206\u985e\u97f3\u7d20\u3002", "author": "Sai Gopinath et.al.", "authors": "Sai Gopinath, Joselyn Rodriguez", "id": "2409.03115v1", "paper_url": "http://arxiv.org/abs/2409.03115v1", "repo": "null"}}