
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-01-23**|**CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation**|Guofeng Cui et.al.|[2501.13927v1](http://arxiv.org/abs/2501.13927v1)|null|
|**2025-01-23**|**Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step**|Ziyu Guo et.al.|[2501.13926v1](http://arxiv.org/abs/2501.13926v1)|[link](https://github.com/ziyuguo99/image-generation-cot)|
|**2025-01-23**|**Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization**|Hao Dong et.al.|[2501.13924v1](http://arxiv.org/abs/2501.13924v1)|[link](https://github.com/donghao51/aeo)|
|**2025-01-23**|**The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama with Vision-Aware and Function-Calling Capabilities**|Chan-Jan Hsu et.al.|[2501.13921v1](http://arxiv.org/abs/2501.13921v1)|null|
|**2025-01-23**|**IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models**|Jiayi Lei et.al.|[2501.13920v1](http://arxiv.org/abs/2501.13920v1)|null|
|**2025-01-23**|**Temporal Preference Optimization for Long-Form Video Understanding**|Rui Li et.al.|[2501.13919v1](http://arxiv.org/abs/2501.13919v1)|null|
|**2025-01-23**|**Improving Video Generation with Human Feedback**|Jie Liu et.al.|[2501.13918v1](http://arxiv.org/abs/2501.13918v1)|null|
|**2025-01-23**|**Analysis of Indic Language Capabilities in LLMs**|Aatman Vaidya et.al.|[2501.13912v1](http://arxiv.org/abs/2501.13912v1)|null|
|**2025-01-23**|**PointOBB-v3: Expanding Performance Boundaries of Single Point-Supervised Oriented Object Detection**|Peiyuan Zhang et.al.|[2501.13898v1](http://arxiv.org/abs/2501.13898v1)|null|
|**2025-01-23**|**GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous Exploration**|Yue Fan et.al.|[2501.13896v1](http://arxiv.org/abs/2501.13896v1)|null|
|**2025-01-23**|**Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning**|Zuyao You et.al.|[2501.13893v1](http://arxiv.org/abs/2501.13893v1)|[link](https://github.com/geshang777/pix2cap)|
|**2025-01-23**|**Exploring Finetuned Audio-LLM on Heart Murmur Features**|Adrian Florea et.al.|[2501.13884v1](http://arxiv.org/abs/2501.13884v1)|null|
|**2025-01-23**|**A RAG-Based Institutional Assistant**|Gustavo Kuratomi et.al.|[2501.13880v1](http://arxiv.org/abs/2501.13880v1)|null|
|**2025-01-23**|**Where Do You Go? Pedestrian Trajectory Prediction using Scene Features**|Mohammad Ali Rezaei et.al.|[2501.13848v1](http://arxiv.org/abs/2501.13848v1)|null|
|**2025-01-23**|**Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages**|Farhana Shahid et.al.|[2501.13836v1](http://arxiv.org/abs/2501.13836v1)|null|
|**2025-01-23**|**On the Reasoning Capacity of AI Models and How to Quantify It**|Santosh Kumar Radha et.al.|[2501.13833v1](http://arxiv.org/abs/2501.13833v1)|null|
|**2025-01-23**|**Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing**|Hao Zhang et.al.|[2501.13831v1](http://arxiv.org/abs/2501.13831v1)|null|
|**2025-01-23**|**A space-decoupling framework for optimization on bounded-rank matrices with orthogonally invariant constraints**|Yan Yang et.al.|[2501.13830v1](http://arxiv.org/abs/2501.13830v1)|null|
|**2025-01-23**|**Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos**|Kairui Hu et.al.|[2501.13826v1](http://arxiv.org/abs/2501.13826v1)|null|
|**2025-01-23**|**Hallucinations Can Improve Large Language Models in Drug Discovery**|Shuzhou Yuan et.al.|[2501.13824v1](http://arxiv.org/abs/2501.13824v1)|null|
|**2025-01-23**|**Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data**|Frederik Pahde et.al.|[2501.13818v1](http://arxiv.org/abs/2501.13818v1)|[link](https://github.com/frederikpahde/medical-ai-safety)|
|**2025-01-23**|**Learning to Help in Multi-Class Settings**|Yu Wu et.al.|[2501.13810v1](http://arxiv.org/abs/2501.13810v1)|null|
|**2025-01-23**|**Parameter-Efficient Fine-Tuning for Foundation Models**|Dan Zhang et.al.|[2501.13787v1](http://arxiv.org/abs/2501.13787v1)|[link](https://github.com/thudm/awesome-parameter-efficient-fine-tuning-for-foundation-models)|
|**2025-01-23**|**Not Every AI Problem is a Data Problem: We Should Be Intentional About Data Scaling**|Tanya Rodchenko et.al.|[2501.13779v1](http://arxiv.org/abs/2501.13779v1)|null|
|**2025-01-23**|**Explainable XR: Understanding User Behaviors of XR Environments using LLM-assisted Analytics Framework**|Yoonsang Kim et.al.|[2501.13778v1](http://arxiv.org/abs/2501.13778v1)|[link](https://github.com/yoonsang0910/explainablexr)|
|**2025-01-23**|**Do Large Language Models Truly Understand Geometric Structures?**|Xiaofeng Wang et.al.|[2501.13773v1](http://arxiv.org/abs/2501.13773v1)|null|
|**2025-01-23**|**Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak**|Erjia Xiao et.al.|[2501.13772v1](http://arxiv.org/abs/2501.13772v1)|null|
|**2025-01-23**|**UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level Mathematical Reasoning with Large Language Models**|Xin Xu et.al.|[2501.13766v1](http://arxiv.org/abs/2501.13766v1)|null|
|**2025-01-23**|**2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings**|Yumeng Wang et.al.|[2501.13758v1](http://arxiv.org/abs/2501.13758v1)|null|
|**2025-01-23**|**Solving the long-tailed distribution problem by exploiting the synergies and balance of different techniques**|Ziheng Wang et.al.|[2501.13756v1](http://arxiv.org/abs/2501.13756v1)|null|
|**2025-01-23**|**EICopilot: Search and Explore Enterprise Information over Large-scale Knowledge Graphs with LLM-driven Agents**|Yuhui Yun et.al.|[2501.13746v1](http://arxiv.org/abs/2501.13746v1)|null|
|**2025-01-23**|**A Study of the Plausibility of Attention between RNN Encoders in Natural Language Inference**|Duc Hau Nguyen et.al.|[2501.13735v1](http://arxiv.org/abs/2501.13735v1)|null|
|**2025-01-23**|**Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational Tasks**|Chang Gong et.al.|[2501.13731v1](http://arxiv.org/abs/2501.13731v1)|null|
|**2025-01-23**|**RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation**|Shi-Qi Yan et.al.|[2501.13726v1](http://arxiv.org/abs/2501.13726v1)|null|
|**2025-01-23**|**You Only Crash Once v2: Perceptually Consistent Strong Features for One-Stage Domain Adaptive Detection of Space Terrain**|Timothy Chase Jr et.al.|[2501.13725v1](http://arxiv.org/abs/2501.13725v1)|null|
|**2025-01-23**|**Musical ethnocentrism in Large Language Models**|Anna Kruspe et.al.|[2501.13720v1](http://arxiv.org/abs/2501.13720v1)|null|
|**2025-01-23**|**Skin Disease Detection and Classification of Actinic Keratosis and Psoriasis Utilizing Deep Transfer Learning**|Fahud Ahmmed et.al.|[2501.13713v1](http://arxiv.org/abs/2501.13713v1)|null|
|**2025-01-23**|**Formally Verified Neurosymbolic Trajectory Learning via Tensor-based Linear Temporal Logic on Finite Traces**|Mark Chevallier et.al.|[2501.13712v1](http://arxiv.org/abs/2501.13712v1)|null|
|**2025-01-23**|**YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised Re-ID**|IÃ±aki Erregue et.al.|[2501.13710v1](http://arxiv.org/abs/2501.13710v1)|null|
|**2025-01-23**|**EventVL: Understand Event Streams via Multimodal Large Language Model**|Pengteng Li et.al.|[2501.13707v1](http://arxiv.org/abs/2501.13707v1)|null|
|**2025-01-23**|**DI-BENCH: Benchmarking Large Language Models on Dependency Inference with Testable Repositories at Scale**|Linghao Zhang et.al.|[2501.13699v1](http://arxiv.org/abs/2501.13699v1)|null|
|**2025-01-23**|**Training-Free Consistency Pipeline for Fashion Repose**|Potito Aghilar et.al.|[2501.13692v1](http://arxiv.org/abs/2501.13692v1)|null|
|**2025-01-23**|**Question Answering on Patient Medical Records with Private Fine-Tuned LLMs**|Sara Kothari et.al.|[2501.13687v1](http://arxiv.org/abs/2501.13687v1)|null|
|**2025-01-23**|**Unlearning Clients, Features and Samples in Vertical Federated Learning**|Ayush K. Varshney et.al.|[2501.13683v1](http://arxiv.org/abs/2501.13683v1)|null|
|**2025-01-23**|**Certified Robustness Under Bounded Levenshtein Distance**|Elias Abad Rocamora et.al.|[2501.13676v1](http://arxiv.org/abs/2501.13676v1)|null|
|**2025-01-23**|**How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization**|Shezheng Song et.al.|[2501.13669v1](http://arxiv.org/abs/2501.13669v1)|null|
|**2025-01-23**|**LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models**|Yizheng Sun et.al.|[2501.13652v1](http://arxiv.org/abs/2501.13652v1)|null|
|**2025-01-23**|**Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models**|Zhenghao Lin et.al.|[2501.13629v1](http://arxiv.org/abs/2501.13629v1)|null|
|**2025-01-23**|**Coarse-to-Fine Process Reward Modeling for Enhanced Mathematical Reasoning**|Yulan Hu et.al.|[2501.13622v1](http://arxiv.org/abs/2501.13622v1)|null|
|**2025-01-23**|**Cognitive Paradigms for Evaluating VLMs on Visual Reasoning Task**|Mohit Vaishnav et.al.|[2501.13620v1](http://arxiv.org/abs/2501.13620v1)|null|
|**2025-01-23**|**Efficient Synaptic Delay Implementation in Digital Event-Driven AI Accelerators**|Roy Meijer et.al.|[2501.13610v1](http://arxiv.org/abs/2501.13610v1)|null|
|**2025-01-23**|**Domain-Specific Machine Translation to Translate Medicine Brochures in English to Sorani Kurdish**|Mariam Shamal et.al.|[2501.13609v1](http://arxiv.org/abs/2501.13609v1)|null|
|**2025-01-23**|**Text-to-SQL based on Large Language Models and Database Keyword Search**|Eduardo R. Nascimento et.al.|[2501.13594v1](http://arxiv.org/abs/2501.13594v1)|null|
|**2025-01-23**|**Contrastive Representation Learning Helps Cross-institutional Knowledge Transfer: A Study in Pediatric Ventilation Management**|Yuxuan et.al.|[2501.13587v1](http://arxiv.org/abs/2501.13587v1)|null|
|**2025-01-23**|**Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced Optimization**|Lei Huang et.al.|[2501.13573v1](http://arxiv.org/abs/2501.13573v1)|null|
|**2025-01-23**|**K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor**|Jeonghun Cho et.al.|[2501.13567v1](http://arxiv.org/abs/2501.13567v1)|null|
|**2025-01-23**|**Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving**|Lu Wang et.al.|[2501.13563v1](http://arxiv.org/abs/2501.13563v1)|null|
|**2025-01-23**|**One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt**|Tao Liu et.al.|[2501.13554v1](http://arxiv.org/abs/2501.13554v1)|[link](https://github.com/byliutao/1prompt1story)|
|**2025-01-23**|**Explainable AI-aided Feature Selection and Model Reduction for DRL-based V2X Resource Allocation**|Nasir Khan et.al.|[2501.13552v1](http://arxiv.org/abs/2501.13552v1)|null|
|**2025-01-23**|**LLMs Can Plan Only If We Tell Them**|Bilgehan Sel et.al.|[2501.13545v1](http://arxiv.org/abs/2501.13545v1)|null|
|**2025-01-23**|**ReasVQA: Advancing VideoQA with Imperfect Reasoning Process**|Jianxin Liang et.al.|[2501.13536v1](http://arxiv.org/abs/2501.13536v1)|null|
|**2025-01-23**|**Towards a Theory of AI Personhood**|Francis Rhys Ward et.al.|[2501.13533v1](http://arxiv.org/abs/2501.13533v1)|null|
|**2025-01-23**|**DQ-Data2vec: Decoupling Quantization for Multilingual Speech Recognition**|Qijie Shao et.al.|[2501.13497v1](http://arxiv.org/abs/2501.13497v1)|null|
|**2025-01-23**|**GCAD: Anomaly Detection in Multivariate Time Series from the Perspective of Granger Causality**|Zehao Liu et.al.|[2501.13493v1](http://arxiv.org/abs/2501.13493v1)|null|
|**2025-01-23**|**RECALL: Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles**|Munachiso Nwadike et.al.|[2501.13491v1](http://arxiv.org/abs/2501.13491v1)|null|
|**2025-01-23**|**MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods**|Zukang Xu et.al.|[2501.13484v1](http://arxiv.org/abs/2501.13484v1)|[link](https://github.com/mambaquant/mambaquant)|
|**2025-01-23**|**Adaptive Testing for LLM-Based Applications: A Diversity-based Approach**|Juyeon Yoon et.al.|[2501.13480v1](http://arxiv.org/abs/2501.13480v1)|null|
|**2025-01-23**|**Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility**|Rishabh Agrawal et.al.|[2501.13479v1](http://arxiv.org/abs/2501.13479v1)|null|
|**2025-01-23**|**Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge**|Haomiao Xiong et.al.|[2501.13468v1](http://arxiv.org/abs/2501.13468v1)|[link](https://github.com/hmxiong/streamchat)|
|**2025-01-23**|**Multi-Level Attention and Contrastive Learning for Enhanced Text Classification with an Optimized Transformer**|Jia Gao et.al.|[2501.13467v1](http://arxiv.org/abs/2501.13467v1)|null|
|**2025-01-23**|**Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks**|Ruijia Liu et.al.|[2501.13457v1](http://arxiv.org/abs/2501.13457v1)|null|
|**2025-01-23**|**KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks**|Taoran Fang et.al.|[2501.13456v1](http://arxiv.org/abs/2501.13456v1)|[link](https://github.com/luckytiger123/kaa)|
|**2025-01-23**|**One-cycle Structured Pruning with Stability Driven Structure Search**|Deepak Ghimire et.al.|[2501.13439v1](http://arxiv.org/abs/2501.13439v1)|null|
|**2025-01-23**|**Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models**|Bo Gao et.al.|[2501.13428v1](http://arxiv.org/abs/2501.13428v1)|null|
|**2025-01-23**|**A Survey of Code-switched Arabic NLP: Progress, Challenges, and Future Directions**|Injy Hamed et.al.|[2501.13419v1](http://arxiv.org/abs/2501.13419v1)|null|
|**2025-01-23**|**Rethinking the Sample Relations for Few-Shot Classification**|Guowei Yin et.al.|[2501.13418v1](http://arxiv.org/abs/2501.13418v1)|null|
|**2025-01-23**|**M3PT: A Transformer for Multimodal, Multi-Party Social Signal Prediction with Person-aware Blockwise Attention**|Yiming Tang et.al.|[2501.13416v1](http://arxiv.org/abs/2501.13416v1)|[link](https://github.com/abraranwar/masked-social-signals)|
|**2025-01-23**|**Load and Renewable Energy Forecasting Using Deep Learning for Grid Stability**|Kamal Sarkar et.al.|[2501.13412v1](http://arxiv.org/abs/2501.13412v1)|null|
|**2025-01-23**|**YOLOv8 to YOLO11: A Comprehensive Architecture In-depth Comparative Review**|Priyanto Hidayatullah et.al.|[2501.13400v1](http://arxiv.org/abs/2501.13400v1)|null|
|**2025-01-23**|**ExLM: Rethinking the Impact of $\texttt{[MASK]}$ Tokens in Masked Language Models**|Kangjie Zheng et.al.|[2501.13397v1](http://arxiv.org/abs/2501.13397v1)|null|
|**2025-01-23**|**Can Large Language Models Understand Preferences in Personalized Recommendation?**|Zhaoxuan Tan et.al.|[2501.13391v1](http://arxiv.org/abs/2501.13391v1)|null|
|**2025-01-23**|**Do as We Do, Not as You Think: the Conformity of Large Language Models**|Zhiyuan Weng et.al.|[2501.13381v1](http://arxiv.org/abs/2501.13381v1)|[link](https://github.com/zhiyuan-weng/benchform)|
|**2025-01-23**|**Generative Data Augmentation Challenge: Zero-Shot Speech Synthesis for Personalized Speech Enhancement**|Jae-Sung Bae et.al.|[2501.13372v1](http://arxiv.org/abs/2501.13372v1)|null|
|**2025-01-23**|**A review on development of eco-friendly filters in Nepal for use in cigarettes and masks and Air Pollution Analysis with Machine Learning and SHAP Interpretability**|Bishwash Paneru et.al.|[2501.13369v1](http://arxiv.org/abs/2501.13369v1)|null|
|**2025-01-23**|**Enhanced Extractor-Selector Framework and Symmetrization Weighted Binary Cross-Entropy for Edge Detections**|Hao Shu et.al.|[2501.13365v1](http://arxiv.org/abs/2501.13365v1)|null|
|**2025-01-23**|**One Fits All: General Mobility Trajectory Modeling via Masked Conditional Diffusion**|Qingyue Long et.al.|[2501.13347v1](http://arxiv.org/abs/2501.13347v1)|null|
|**2025-01-23**|**Full-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation**|Rong Shan et.al.|[2501.13344v1](http://arxiv.org/abs/2501.13344v1)|null|
|**2025-01-23**|**AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback**|Joshua Park et.al.|[2501.13333v1](http://arxiv.org/abs/2501.13333v1)|[link](https://github.com/joshprk/agentrec)|
|**2025-01-23**|**Sparse identification of nonlinear dynamics and Koopman operators with Shallow Recurrent Decoder Networks**|Mars Liyao Gao et.al.|[2501.13329v1](http://arxiv.org/abs/2501.13329v1)|null|
|**2025-01-23**|**Investigation of the Privacy Concerns in AI Systems for Young Digital Citizens: A Comparative Stakeholder Analysis**|Molly Campbell et.al.|[2501.13321v1](http://arxiv.org/abs/2501.13321v1)|null|
|**2025-01-23**|**Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers**|Akshit Achara et.al.|[2501.13302v1](http://arxiv.org/abs/2501.13302v1)|null|
|**2025-01-23**|**Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents**|Shrinidhi Kumbhar et.al.|[2501.13299v1](http://arxiv.org/abs/2501.13299v1)|null|
|**2025-01-23**|**RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question Answering**|Yang Bai et.al.|[2501.13297v1](http://arxiv.org/abs/2501.13297v1)|[link](https://github.com/tonyby/ramqa)|
|**2025-01-23**|**Parallel Belief Contraction via Order Aggregation**|Jake Chandler et.al.|[2501.13295v1](http://arxiv.org/abs/2501.13295v1)|null|
|**2025-01-23**|**Toyteller: AI-powered Visual Storytelling Through Toy-Playing with Character Symbols**|John Joon Young Chung et.al.|[2501.13284v1](http://arxiv.org/abs/2501.13284v1)|null|
|**2025-01-23**|**Experience with GitHub Copilot for Developer Productivity at Zoominfo**|Gal Bakal et.al.|[2501.13282v1](http://arxiv.org/abs/2501.13282v1)|null|
|**2025-01-22**|**RAG-Reward: Optimizing RAG with Reward Modeling and RLHF**|Hanning Zhang et.al.|[2501.13264v1](http://arxiv.org/abs/2501.13264v1)|null|
|**2025-01-22**|**Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions**|Yan Ru Pei et.al.|[2501.13230v1](http://arxiv.org/abs/2501.13230v1)|[link](https://github.com/Brainchip-Inc/Centaurus)|
|**2025-01-22**|**Learning in Log-Domain: Subthreshold Analog AI Accelerator Based on Stochastic Gradient Descent**|Momen K Tageldeen et.al.|[2501.13181v1](http://arxiv.org/abs/2501.13181v1)|null|
|**2025-01-22**|**QuFeX: Quantum feature extraction module for hybrid quantum-classical deep neural networks**|Naman Jain et.al.|[2501.13165v1](http://arxiv.org/abs/2501.13165v1)|null|

#### Abstracts
##### **CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation**
2501.13927v1 by Guofeng Cui, Pichao Wang, Yang Liu, Zemian Ke, Zhu Liu, Vimal Bhat

Large language models (LLMs) have shown great potential in natural language
processing tasks, but their application to machine translation (MT) remains
challenging due to pretraining on English-centric data and the complexity of
reinforcement learning from human feedback (RLHF). Direct Preference
Optimization (DPO) has emerged as a simpler and more efficient alternative, but
its performance depends heavily on the quality of preference data. To address
this, we propose Confidence-Reward driven Preference Optimization (CRPO), a
novel method that combines reward scores with model confidence to improve data
selection for fine-tuning. CRPO selects challenging sentence pairs where the
model is uncertain or underperforms, leading to more effective learning. While
primarily designed for LLMs, CRPO also generalizes to encoder-decoder models
like NLLB, demonstrating its versatility. Empirical results show that CRPO
outperforms existing methods such as RS-DPO, RSO and MBR score in both
translation accuracy and data efficiency.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨èªç¶èªè¨èçä»»åä¸­å±ç¾åºæ¥µå¤§çæ½åï¼ä½ç±æ¼é è¨ç·´æä»¥è±èªçºä¸­å¿è³æï¼ä»¥åå¾äººé¡åé¥ä¸­é²è¡å¼·åå­¸ç¿çè¤éæ§ï¼å¶å¨æ©å¨ç¿»è­¯ (MT) ä¸­çæç¨ä»ç¶å·æææ°æ§ãç´æ¥åå¥½æä½³å (DPO) å·²æçºä¸ç¨®æ´ç°¡å®ä¸æ´ææççæ¿ä»£æ¹æ¡ï¼ä½å¶æè½é«åº¦ä¾è³´æ¼åå¥½è³æçåè³ªãçºäºè§£æ±ºæ­¤åé¡ï¼æåæåºä»¥ä¿¡å¿çåµçºé©ååçåå¥½æä½³å (CRPO)ï¼éæ¯ä¸ç¨®çµåçåµåæ¸èæ¨¡åä¿¡å¿çæ°æ¹æ³ï¼ä»¥æ¹åå¾®èª¿çè³æé¸åãCRPO é¸ææ¨¡åä¸ç¢ºå®æè¡¨ç¾ä¸ä½³çå·ææ°æ§å¥å­å°ï¼é²èå¸¶ä¾æ´ææçå­¸ç¿ãåç®¡ CRPO ä¸»è¦è¨­è¨ç¨æ¼ LLMï¼ä½å®ä¹é©ç¨æ¼ç·¨ç¢¼å¨-è§£ç¢¼å¨æ¨¡åï¼ä¾å¦ NLLBï¼è­æäºå¶å¤åè½æ§ãå¯¦è­çµæé¡¯ç¤ºï¼CRPO å¨ç¿»è­¯æºç¢ºåº¦åè³ææçæ¹é¢ååªæ¼ç¾ææ¹æ³ï¼ä¾å¦ RS-DPOãRSO å MBR åæ¸ã

##### **Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step**
2501.13926v1 by Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, Pheng-Ann Heng

Chain-of-Thought (CoT) reasoning has been extensively explored in large
models to tackle complex understanding tasks. However, it still remains an open
question whether such strategies can be applied to verifying and reinforcing
image generation scenarios. In this paper, we provide the first comprehensive
investigation of the potential of CoT reasoning to enhance autoregressive image
generation. We focus on three techniques: scaling test-time computation for
verification, aligning model preferences with Direct Preference Optimization
(DPO), and integrating these techniques for complementary effects. Our results
demonstrate that these approaches can be effectively adapted and combined to
significantly improve image generation performance. Furthermore, given the
pivotal role of reward models in our findings, we propose the Potential
Assessment Reward Model (PARM) and PARM++, specialized for autoregressive image
generation. PARM adaptively assesses each generation step through a potential
assessment approach, merging the strengths of existing reward models, and
PARM++ further introduces a reflection mechanism to self-correct the generated
unsatisfactory image. Using our investigated reasoning strategies, we enhance a
baseline model, Show-o, to achieve superior results, with a significant +24%
improvement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We
hope our study provides unique insights and paves a new path for integrating
CoT reasoning with autoregressive image generation. Code and models are
released at https://github.com/ZiyuGuo99/Image-Generation-CoT

æè¦ï¼<paragraph>éå¼æè (CoT) æ¨çå·²è¢«å»£æ³å°æ¢ç´¢æ¼å¤§åæ¨¡åä¸­ï¼ä»¥è§£æ±ºè¤éççè§£ä»»åãç¶èï¼æ­¤é¡ç­ç¥æ¯å¦è½æç¨æ¼é©è­åå¼·åå½±åçæå ´æ¯ï¼ä»æ¯ä¸åéæ¾æ§çåé¡ãå¨æ¬æä¸­ï¼æåæä¾äºç¬¬ä¸åéæ¼ CoT æ¨çæ½åç¨æ¼å¢å¼·èªè¿´æ­¸å½±åçæçå¨é¢èª¿æ¥ãæåå°æ³¨æ¼ä¸ç¨®æè¡ï¼æ´å±æ¸¬è©¦æééç®ä»¥é²è¡é©è­ãå°æ¨¡ååå¥½èç´æ¥åå¥½æä½³å (DPO) å°é½ï¼ä»¥åæ´åéäºæè¡ä»¥ç¢çäºè£ææãæåççµæè­æï¼éäºæ¹æ³å¯ä»¥ææå°é©æä¸¦çµåï¼ä»¥é¡¯èæ¹åå½±åçææè½ãæ­¤å¤ï¼éæ¼çåµæ¨¡åå¨æåçç¼ç¾ä¸­æ®æ¼èééµè§è²ï¼æåæåºäºæ½åè©ä¼°çåµæ¨¡å (PARM) å PARM++ï¼å°éç¨æ¼èªè¿´æ­¸å½±åçæãPARM ééæ½åè©ä¼°æ¹æ³èªé©æå°è©ä¼°æ¯åçææ­¥é©ï¼åä½µç¾æçåµæ¨¡åçåªé»ï¼è PARM++ é²ä¸æ­¥å¼å¥åå°æ©å¶ä¾èªæä¿®æ­£çæçä»¤äººä¸æ»¿æçå½±åãä½¿ç¨æåèª¿æ¥çæ¨çç­ç¥ï¼æåå¢å¼·äºä¸ååºæºæ¨¡å Show-oï¼ä»¥åå¾åªç°ççµæï¼å¨ GenEval åºæºä¸é¡¯èæå +24%ï¼è¶è¶ Stable Diffusion 3 +15%ãæåå¸ææåçç ç©¶æä¾ç¨ç¹çè¦è§£ï¼ä¸¦çºå° CoT æ¨çèèªè¿´æ­¸å½±åçææ´åéé¢ä¸æ¢æ°éå¾ãç¨å¼ç¢¼åæ¨¡åå·²æ¼ https://github.com/ZiyuGuo99/Image-Generation-CoT ç¼å¸</paragraph>

##### **Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization**
2501.13924v1 by Hao Dong, Eleni Chatzi, Olga Fink

Test-time adaptation (TTA) has demonstrated significant potential in
addressing distribution shifts between training and testing data. Open-set
test-time adaptation (OSTTA) aims to adapt a source pre-trained model online to
an unlabeled target domain that contains unknown classes. This task becomes
more challenging when multiple modalities are involved. Existing methods have
primarily focused on unimodal OSTTA, often filtering out low-confidence samples
without addressing the complexities of multimodal data. In this work, we
present Adaptive Entropy-aware Optimization (AEO), a novel framework
specifically designed to tackle Multimodal Open-set Test-time Adaptation
(MM-OSTTA) for the first time. Our analysis shows that the entropy difference
between known and unknown samples in the target domain strongly correlates with
MM-OSTTA performance. To leverage this, we propose two key components:
Unknown-aware Adaptive Entropy Optimization (UAE) and Adaptive Modality
Prediction Discrepancy Optimization (AMP). These components enhance the ability
of model to distinguish unknown class samples during online adaptation by
amplifying the entropy difference between known and unknown samples. To
thoroughly evaluate our proposed methods in the MM-OSTTA setting, we establish
a new benchmark derived from existing datasets. This benchmark includes two
downstream tasks and incorporates five modalities. Extensive experiments across
various domain shift situations demonstrate the efficacy and versatility of the
AEO framework. Additionally, we highlight the strong performance of AEO in
long-term and continual MM-OSTTA settings, both of which are challenging and
highly relevant to real-world applications. Our source code is available at
https://github.com/donghao51/AEO.

æè¦ï¼<paragraph>æ¸¬è©¦æéé©æ (TTA) å·²å±ç¾åºå¨è§£æ±ºè¨ç·´è³æèæ¸¬è©¦è³æä¹éçåéè½ç§»ä¸å·æé¡¯èæ½åãéæ¾éæ¸¬è©¦æéé©æ (OSTTA) æ¨å¨å°ä¾æºé åè¨ç·´çæ¨¡åç·ä¸é©æå°åå«æªç¥é¡å¥çæªæ¨è¨ç®æ¨ç¶²åãç¶æ¶åå¤ç¨®æ¨¡å¼æï¼éé ä»»åå°è®å¾æ´å·ææ°æ§ãç¾ææ¹æ³ä¸»è¦éæ³¨æ¼å®æ¨¡æ OSTTAï¼éå¸¸æéæ¿¾æä½ä¿¡å¿æ¨£æ¬ï¼èä¸æè§£æ±ºå¤æ¨¡æè³æçè¤éæ§ãå¨éé å·¥ä½ä¸­ï¼æåæåºèªé©æçµæç¥æä½³å (AEO)ï¼éæ¯ä¸åæ°ç©çæ¶æ§ï¼å°éè¨­è¨ä¾é¦æ¬¡èçå¤æ¨¡æéæ¾éæ¸¬è©¦æéé©æ (MM-OSTTA)ãæåçåæé¡¯ç¤ºï¼ç®æ¨ç¶²åä¸­å·²ç¥åæªç¥æ¨£æ¬ä¹éççµå·®ç°è MM-OSTTA æè½å¯åç¸éãçºäºå©ç¨éä¸é»ï¼æåæåºäºå©åééµçµæé¨åï¼æªç¥æç¥èªé©æçµæä½³å (UAE) åèªé©ææ¨¡å¼é æ¸¬å·®ç°æä½³å (AMP)ãéäºçµæé¨åå¢å¼·äºæ¨¡åå¨ç·ä¸é©ææéååæªç¥é¡å¥æ¨£æ¬çè½åï¼æ¹æ³æ¯æ´å¤§å·²ç¥åæªç¥æ¨£æ¬ä¹éççµå·®ç°ãçºäºå¨ MM-OSTTA è¨­å®ä¸­å¾¹åºè©ä¼°æåæåºçæ¹æ³ï¼æåå»ºç«äºä¸åæºèªç¾æè³æéçæ°åºæºãæ­¤åºæºåå«å©åä¸æ¸¸ä»»åä¸¦ç´å¥äºäºç¨®æ¨¡å¼ãå¨åç¨®é åè½ç§»ææ³ä¸çå»£æ³å¯¦é©è­æäº AEO æ¶æ§çæè½åå¤åè½æ§ãæ­¤å¤ï¼æåå¼·èª¿äº AEO å¨é·æåæçºç MM-OSTTA è¨­å®ä¸­çå¼·åæè½ï¼éå©èé½å·æææ°æ§ï¼ä¸¦ä¸èå¯¦éæç¨é«åº¦ç¸éãæåçåå§ç¢¼å¯å¨ https://github.com/donghao51/AEO åå¾ã</paragraph>

##### **The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama with Vision-Aware and Function-Calling Capabilities**
2501.13921v1 by Chan-Jan Hsu, Chia-Sheng Liu, Meng-Hsi Chen, Muxi Chen, Po-Chun Hsu, Yi-Chang Chen, Da-Shan Shiu

Breeze 2 is a suite of advanced multi-modal language models, available in 3B
and 8B parameter configurations, specifically designed to enhance Traditional
Chinese language representation. Building upon the Llama 3, Breeze 2 continues
pretraining on an extensive corpus to enhance the linguistic and cultural
heritage of Traditional Chinese. It incorporates vision-aware capabilities
through a visual encoder and a bridge module, and supports function-calling via
prompt templates and post-training on function-calling data. The effectiveness
of Breeze 2 is benchmarked across various tasks, including Taiwan general
knowledge, instruction-following, long context, function calling, and vision
understanding. Furthermore, we showcase the capabilities of the its 3B model in
a mobile application. We are publicly releasing all Breeze 2 models under the
Llama 3 Community License.

æè¦ï¼Breeze 2 æ¯ä¸å¥é²éçå¤æ¨¡æèªè¨æ¨¡åï¼æä¾ 3B å 8B åæ¸éç½®ï¼å°éè¨­è¨ç¨æ¼å¢å¼·ç¹é«ä¸­æèªè¨è¡¨ç¤ºãBreeze 2 å»ºç«å¨ Llama 3 çåºç¤ä¸ï¼æçºå¨å»£æ³çèªæåº«ä¸é²è¡é è¨ç·´ï¼ä»¥å¢å¼·ç¹é«ä¸­æçèªè¨åæåéºç¢ãå®ééè¦è¦ºç·¨ç¢¼å¨åæ©æ¥æ¨¡çµæ´åäºè¦è¦ºæç¥è½åï¼ä¸¦ééæç¤ºç¯æ¬ååè½å¼å«è³æçå¾çºè¨ç·´æ¯æ´åè½å¼å«ãBreeze 2 çæææ§å·²éå°åç¨®ä»»åé²è¡åºæºæ¸¬è©¦ï¼åæ¬å°ç£ä¸è¬ç¥è­ãéµå¾ªæç¤ºãé·ç¯èªå¢ãåè½å¼å«åè¦è¦ºçè§£ãæ­¤å¤ï¼æåå¨è¡åæç¨ç¨å¼ä¸­å±ç¤ºå¶ 3B æ¨¡åçåè½ãæåå¨ Llama 3 ç¤¾ç¾¤ææ¬ä¸å¬éç¼å¸ææ Breeze 2 æ¨¡åã

##### **IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models**
2501.13920v1 by Jiayi Lei, Renrui Zhang, Xiangfei Hu, Weifeng Lin, Zhen Li, Wenjian Sun, Ruoyi Du, Le Zhuo, Zhongyu Li, Xinyue Li, Shitian Zhao, Ziyu Guo, Yiting Lu, Peng Gao, Hongsheng Li

With the rapid development of diffusion models, text-to-image(T2I) models
have made significant progress, showcasing impressive abilities in prompt
following and image generation. Recently launched models such as FLUX.1 and
Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have
demonstrated exceptional performance across various complex tasks, raising
questions about whether T2I models are moving towards general-purpose
applicability. Beyond traditional image generation, these models exhibit
capabilities across a range of fields, including controllable generation, image
editing, video, audio, 3D, and motion generation, as well as computer vision
tasks like semantic segmentation and depth estimation. However, current
evaluation frameworks are insufficient to comprehensively assess these models'
performance across expanding domains. To thoroughly evaluate these models, we
developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0,
Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided
into five key domains: structured output generation, realism, and physical
consistency, specific domain generation, challenging scenario generation, and
multi-style creation tasks. This comprehensive assessment highlights each
model's strengths and limitations, particularly the outstanding performance of
FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring
the expanding applications and potential of T2I models as foundational AI
tools. This study provides valuable insights into the current state and future
trajectory of T2I models as they evolve towards general-purpose usability.
Evaluation scripts will be released at https://github.com/jylei16/Imagine-e.

æè¦ï¼<paragraph>é¨èæ´æ£æ¨¡åçå¿«éç¼å±ï¼æå­è½åå (T2I) æ¨¡åå·²åå¾é¡¯èé²å±ï¼å¨æç¤ºè¿½è¹¤åå½±åçææ¹é¢å±ç¾ä»¤äººå°è±¡æ·±å»çè½åãæè¿æ¨åºçæ¨¡åï¼å¦ FLUX.1 å Ideogram2.0ï¼ä»¥å Dall-E3 å Stable Diffusion 3 ç­å¶ä»æ¨¡åï¼å·²å¨åç¨®è¤éä»»åä¸­å±ç¾åºè²çæè½ï¼å¼ç¼ T2I æ¨¡åæ¯å¦æ­£æåéç¨é©ç¨æ§éé²ççåãé¤äºå³çµ±çå½±åçæå¤ï¼éäºæ¨¡åå¨å¯æ§çæãå½±åç·¨è¼¯ãå½±çãé³è¨ã3D ååä½çæï¼ä»¥åèªæåå²åæ·±åº¦ä¼°è¨ç­é»è¦è¦è¦ºä»»åä¸­å±ç¾åºè·¨é åçè½åãç¶èï¼ç®åçè©ä¼°æ¶æ§ä¸è¶³ä»¥å¨é¢è©ä¼°éäºæ¨¡åå¨æ´å±é åä¸­çæè½ãçºäºå¾¹åºè©ä¼°éäºæ¨¡åï¼æåéç¼äº IMAGINE-Eï¼ä¸¦æ¸¬è©¦äºå­åååºçæ¨¡åï¼FLUX.1ãIdeogram2.0ãMidjourneyãDall-E3ãStable Diffusion 3 å Jimengãæåçè©ä¼°åçºäºåééµé åï¼çµæ§åè¼¸åºçæãçå¯¦æåç©çä¸è´æ§ãç¹å®é åçæãå·ææ°æ§çå ´æ¯çæåå¤æ¨£å¼åµä½ä»»åãæ­¤ç¶åè©ä¼°çªé¡¯äºæ¯åæ¨¡åçåªå¢åéå¶ï¼ç¹å¥æ¯ FLUX.1 å Ideogram2.0 å¨çµæ§ååç¹å®é åä»»åä¸­çåºè²æè½ï¼å¼·èª¿äº T2I æ¨¡åä½çºåºç¤ AI å·¥å·çæ´å±æç¨åæ½åãæ¬ç ç©¶æä¾äºå° T2I æ¨¡åç¶åçæåæªä¾è»è·¡çå¯¶è²´è¦è§£ï¼å çºå®åæåéç¨å¯ç¨æ§æ¼é²ãè©ä¼°è³æ¬å°å¨ https://github.com/jylei16/Imagine-e ç¼å¸ã</paragraph>

##### **Temporal Preference Optimization for Long-Form Video Understanding**
2501.13919v1 by Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy

Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.

æè¦ï¼åç®¡å¨è¦è¨å¤§åå¤æ¨¡ææ¨¡åï¼video-LMMsï¼ä¸­åå¾é¡¯èé²å±ï¼ä½å¨é·ç¯å½±çä¸­å¯¦ç¾ææçæéåºç¤ä»æ¯ç¾ææ¨¡åçææ°ãçºäºè§£æ±ºæ­¤éå¶ï¼æåæåºæéåå¥½æä½³åï¼TPOï¼ï¼éæ¯ä¸åæ°ç©çå¾è¨ç·´æ¶æ§ï¼æ¨å¨ééåå¥½å­¸ç¿å¢å¼· video-LMMs çæéåºç¤è½åãTPO æ¡ç¨èªè¨ç·´æ¹æ³ï¼ä½¿æ¨¡åè½å¤ ééå©ç¨å©åç²åº¦å±¤ç´çç²¾é¸åå¥½è³æéä¾åååºç¤è¯å¥½çæéåæèè¼ä¸æºç¢ºçæéåæï¼å±é¨æéåºç¤ï¼å°æ³¨æ¼ç¹å®å½±ççæ®µï¼ä»¥åå¨é¢æéåºç¤ï¼æ·åæ´åå½±çåºåä¸­å»¶ä¼¸çæéä¾è³´æ§ãééæä½³åéäºåå¥½è³æéï¼TPO å¤§å¹å¢å¼·æéçè§£ï¼åææ¸å°å°æåè¨»è§£è³æçä¾è³´ãå¨ä¸åé·ç¯å½±ççè§£åºæºæ¸¬è©¦ï¼LongVideoBenchãMLVU å Video-MMEï¼ä¸é²è¡çå¤§éå¯¦é©è­æäº TPO å¨å©åæåé²ç video-LMMs ä¸­çæææ§ãå¼å¾æ³¨æçæ¯ï¼LLaVA-Video-TPO å¨ Video-MME åºæºæ¸¬è©¦ä¸­ç¢ºç«äºèªå·±ä½çºé åç 7B æ¨¡åï¼çªé¡¯äº TPO ä½çºå¯æ´åä¸ææè§£æ±ºæ¹æ¡çæ½åï¼å¯ä¿é²é·ç¯å½±ççè§£ä¸­çæéæ¨çãå°æ¡é é¢ï¼https://ruili33.github.io/tpo_websiteã

##### **Improving Video Generation with Human Feedback**
2501.13918v1 by Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang, Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, Wanli Ouyang

Video generation has achieved significant advances through rectified flow
techniques, but issues like unsmooth motion and misalignment between videos and
prompts persist. In this work, we develop a systematic pipeline that harnesses
human feedback to mitigate these problems and refine the video generation
model. Specifically, we begin by constructing a large-scale human preference
dataset focused on modern video generation models, incorporating pairwise
annotations across multi-dimensions. We then introduce VideoReward, a
multi-dimensional video reward model, and examine how annotations and various
design choices impact its rewarding efficacy. From a unified reinforcement
learning perspective aimed at maximizing reward with KL regularization, we
introduce three alignment algorithms for flow-based models by extending those
from diffusion models. These include two training-time strategies: direct
preference optimization for flow (Flow-DPO) and reward weighted regression for
flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies
reward guidance directly to noisy videos. Experimental results indicate that
VideoReward significantly outperforms existing reward models, and Flow-DPO
demonstrates superior performance compared to both Flow-RWR and standard
supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom
weights to multiple objectives during inference, meeting personalized video
quality needs. Project page: https://gongyeliu.github.io/videoalign.

æè¦ï¼å½±ççæééä¿®æ­£æµæè¡å·²ç²å¾éå¤§é²å±ï¼ä½å½±çèæç¤ºä¹éçä¸æµæ¢åæåæªå°é½ç­åé¡ä»ç¶å­å¨ãå¨æ¬æä¸­ï¼æåéç¼äºä¸åç³»çµ±åç®¡éï¼å©ç¨äººé¡åé¥ä¾æ¸è¼éäºåé¡ä¸¦æ¹åå½±ççææ¨¡åãå·é«ä¾èªªï¼æåé¦åå»ºç«ä¸åå¤§åäººé¡åå¥½è³æéï¼å°æ³¨æ¼ç¾ä»£å½±ççææ¨¡åï¼ä¸¦çµåè·¨å¤ç¶­åº¦çæå°è¨»è§£ãæ¥èæåä»ç´¹ VideoRewardï¼ä¸åå¤ç¶­å½±ççåµæ¨¡åï¼ä¸¦æ¢è¨è¨»è§£ååç¨®è¨­è¨é¸æå¦ä½å½±é¿å¶çåµæè½ãå¾çµ±ä¸çå¼·åå­¸ç¿è§é»ï¼æ¨å¨æå¤§åå·æ KL æ­£è¦åççåµï¼æåééæ´åæ´æ£æ¨¡åçæ¼ç®æ³ï¼çºåºæ¼æµçæ¨¡åå¼å¥äºä¸ç¨®å°é½æ¼ç®æ³ãéäºæ¼ç®æ³åæ¬å©åè¨ç·´æéç­ç¥ï¼æµçç´æ¥åå¥½æä½³å (Flow-DPO) åæµççåµå æ¬è¿´æ­¸ (Flow-RWR)ï¼ä»¥åä¸åæ¨è«æéæè¡ Flow-NRGï¼å®å°çåµå¼å°ç´æ¥æç¨æ¼æéè¨çå½±çãå¯¦é©çµæé¡¯ç¤ºï¼VideoReward æé¡¯åªæ¼ç¾æççåµæ¨¡åï¼è Flow-DPO è Flow-RWR åæ¨æºç£ç£å¾®èª¿æ¹æ³ç¸æ¯ï¼è¡¨ç¾åºåªç°çæè½ãæ­¤å¤ï¼Flow-NRG è®ä½¿ç¨èå¨æ¨è«æéçºå¤åç®æ¨åéèªè¨æ¬éï¼æ»¿è¶³åäººåçå½±çåè³ªéæ±ãå°æ¡é é¢ï¼https://gongyeliu.github.io/videoalignã

##### **Analysis of Indic Language Capabilities in LLMs**
2501.13912v1 by Aatman Vaidya, Tarunima Prabhakar, Denny George, Swair Shah

This report evaluates the performance of text-in text-out Large Language
Models (LLMs) to understand and generate Indic languages. This evaluation is
used to identify and prioritize Indic languages suited for inclusion in safety
benchmarks. We conduct this study by reviewing existing evaluation studies and
datasets; and a set of twenty-eight LLMs that support Indic languages. We
analyze the LLMs on the basis of the training data, license for model and data,
type of access and model developers. We also compare Indic language performance
across evaluation datasets and find that significant performance disparities in
performance across Indic languages. Hindi is the most widely represented
language in models. While model performance roughly correlates with number of
speakers for the top five languages, the assessment after that varies.

æè¦ï¼éä»½å ±åè©ä¼°ææ¬è¼¸å¥ææ¬è¼¸åºçå¤§åèªè¨æ¨¡å (LLM) äºè§£åç¢çå°åº¦èªè¨çæè½ãæ­¤è©ä¼°ç¨æ¼æ¾åºä¸¦åªåèçé©åç´å¥å®å¨åºæºçå°åº¦èªè¨ãæåééæª¢é±ç¾æçè©ä¼°ç ç©¶åè³æéä¾é²è¡éé ç ç©¶ï¼ä»¥åæ¯æ´å°åº¦èªè¨çäºåå«å LLMãæåæ ¹æè¨ç·´è³æãæ¨¡ååè³æçææ¬ãå­åé¡ååæ¨¡åéç¼èä¾åæ LLMãæåä¹æ¯è¼äºä¸åè©ä¼°è³æéçå°åº¦èªè¨æè½ï¼ä¸¦ç¼ç¾å°åº¦èªè¨çæè½æé¡¯èçå·®ç°ãå°å°èªæ¯æ¨¡åä¸­æå»£æ³ä»£è¡¨çèªè¨ãåç®¡æ¨¡åæè½å¤§è´èåäºåèªè¨çä½¿ç¨èäººæ¸ç¸éï¼ä½æ­¤å¾çè©ä¼°åææä¸åã

##### **PointOBB-v3: Expanding Performance Boundaries of Single Point-Supervised Oriented Object Detection**
2501.13898v1 by Peiyuan Zhang, Junwei Luo, Xue Yang, Yi Yu, Qingyun Li, Yue Zhou, Xiaosong Jia, Xudong Lu, Jingdong Chen, Xiang Li, Junchi Yan, Yansheng Li

With the growing demand for oriented object detection (OOD), recent studies
on point-supervised OOD have attracted significant interest. In this paper, we
propose PointOBB-v3, a stronger single point-supervised OOD framework. Compared
to existing methods, it generates pseudo rotated boxes without additional
priors and incorporates support for the end-to-end paradigm. PointOBB-v3
functions by integrating three unique image views: the original view, a resized
view, and a rotated/flipped (rot/flp) view. Based on the views, a scale
augmentation module and an angle acquisition module are constructed. In the
first module, a Scale-Sensitive Consistency (SSC) loss and a Scale-Sensitive
Feature Fusion (SSFF) module are introduced to improve the model's ability to
estimate object scale. To achieve precise angle predictions, the second module
employs symmetry-based self-supervised learning. Additionally, we introduce an
end-to-end version that eliminates the pseudo-label generation process by
integrating a detector branch and introduces an Instance-Aware Weighting (IAW)
strategy to focus on high-quality predictions. We conducted extensive
experiments on the DIOR-R, DOTA-v1.0/v1.5/v2.0, FAIR1M, STAR, and RSAR
datasets. Across all these datasets, our method achieves an average improvement
in accuracy of 3.56% in comparison to previous state-of-the-art methods. The
code will be available at https://github.com/ZpyWHU/PointOBB-v3.

æè¦ï¼é¨èé¢åç®æ¨åµæ¸¬ (OOD) éæ±çå¢é·ï¼æè¿å°é»ç£ç£ OOD çç ç©¶å¼èµ·äºæ¥µå¤§çèè¶£ãå¨æ¬æä¸­ï¼æåæåºäº PointOBB-v3ï¼ä¸åæ´å¼·å¤§çå®é»ç£ç£ OOD æ¡æ¶ãèç¾ææ¹æ³ç¸æ¯ï¼å®å¨æ²æé¡å¤åé©ç¥è­çææ³ä¸çæäºå½æè½æ¡ï¼ä¸¦çµåäºå°ç«¯å°ç«¯ç¯ä¾çæ¯æãPointOBB-v3 çåè½æ¯ééæ´åä¸åç¨ç¹çå½±åæª¢è¦ï¼åå§æª¢è¦ãç¸®æ¾æª¢è¦åæè½/ç¿»è½ (rot/flp) æª¢è¦ãæ ¹æéäºæª¢è¦ï¼æ§å»ºäºä¸åæ¯ä¾æ´åæ¨¡çµåä¸åè§åº¦æ·åæ¨¡çµãå¨ç¬¬ä¸åæ¨¡çµä¸­ï¼å¼å¥äºæ¯ä¾ææä¸è´æ§ (SSC) æå¤±åæ¯ä¾ææç¹å¾µèå (SSFF) æ¨¡çµï¼ä»¥æé«æ¨¡åä¼°è¨ç©ä»¶æ¯ä¾çè½åãçºäºå¯¦ç¾ç²¾ç¢ºçè§åº¦é æ¸¬ï¼ç¬¬äºåæ¨¡çµæ¡ç¨äºåºæ¼å°ç¨±çèªç£ç£å­¸ç¿ãæ­¤å¤ï¼æåå¼å¥äºç«¯å°ç«¯çæ¬ï¼ééæ´ååµæ¸¬å¨åæ¯ä¸¦å¼å¥å¯¦ä¾æç¥å æ¬ (IAW) ç­ç¥ä¾æ¶é¤å½æ¨ç±¤çæéç¨ï¼ä»¥å°æ³¨æ¼é«åè³ªé æ¸¬ãæåå° DIOR-RãDOTA-v1.0/v1.5/v2.0ãFAIR1MãSTAR å RSAR è³æéé²è¡äºå»£æ³çå¯¦é©ãå¨ææéäºè³æéä¸­ï¼èååçæåé²æ¹æ³ç¸æ¯ï¼æåçæ¨¡åå¨æºç¢ºåº¦æ¹é¢å¹³åæåäº 3.56%ãç¨å¼ç¢¼å°å¯å¨ https://github.com/ZpyWHU/PointOBB-v3 åå¾ã

##### **GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous Exploration**
2501.13896v1 by Yue Fan, Handong Zhao, Ruiyi Zhang, Yu Shen, Xin Eric Wang, Gang Wu

Graphical User Interface (GUI) action grounding is a critical step in GUI
automation that maps language instructions to actionable elements on GUI
screens. Most recent works of GUI action grounding leverage large GUI datasets
to fine-tune MLLMs. However, the fine-tuning data always covers limited GUI
environments, and we find the performance of the resulting model deteriorates
in novel environments. We argue that the GUI grounding models should be further
aligned to the novel environments to reveal their full potential, when the
inference is known to involve novel environments, i.e., environments not used
during the previous fine-tuning. To realize this, we first propose GUI-Bee, an
MLLM-based autonomous agent, to collect high-quality, environment-specific data
through exploration and then continuously fine-tune GUI grounding models with
the collected data. Our agent leverages a novel Q-value-Incentive In-Context
Reinforcement Learning (Q-ICRL) method to optimize exploration efficiency and
data quality. Additionally, we introduce NovelScreenSpot, a benchmark for
testing how well the data can help align GUI action grounding models to novel
environments and demonstrate the effectiveness of data collected by GUI-Bee in
the experiments. Furthermore, we conduct an ablation study to validate the
Q-ICRL method in enhancing the efficiency of GUI-Bee. Project page:
https://gui-bee.github.io

æè¦ï¼åå½¢ä½¿ç¨èä»é¢ (GUI) åä½åºç¤æ¯ GUI èªååçééµæ­¥é©ï¼å®å°èªè¨æä»¤å°æå° GUI ç«é¢ä¸çå¯æä½åç´ ãæè¿ GUI åä½åºç¤çå¤§é¨åç ç©¶å©ç¨å¤§å GUI è³æéä¾å¾®èª¿ MLLMãç¶èï¼å¾®èª¿è³æç¸½æ¯æ¶µèæéç GUI ç°å¢ï¼èæåç¼ç¾ç¢ççæ¨¡åæè½æå¨æ°çç°å¢ä¸­æ¡åãæåä¸»å¼µ GUI åºç¤æ¨¡åæè©²é²ä¸æ­¥èæ°ç°å¢å°é½ï¼ä»¥ç¼æ®å¶å¨é¨æ½åï¼ç¶å·²ç¥æ¨è«æ¶åæ°ç°å¢æï¼ä¹å°±æ¯å¨ååçå¾®èª¿ä¸­æªä½¿ç¨çç°å¢ãçºäºå¯¦ç¾éä¸é»ï¼æåé¦åæåº GUI-Beeï¼ä¸ååºæ¼ MLLM çèªä¸»ä»£çï¼ééæ¢ç´¢æ¶éé«åè³ªãç¹å®æ¼ç°å¢çè³æï¼ç¶å¾æçºå¾®èª¿ GUI åºç¤æ¨¡åï¼ä¸¦ä½¿ç¨æ¶éå°çè³æãæåçä»£çå©ç¨æ°ç©ç Q å¼èªå æå¢å§å¼·åå­¸ç¿ (Q-ICRL) æ¹æ³ä¾æä½³åæ¢ç´¢æçåè³æåè³ªãæ­¤å¤ï¼æåå¼å¥äº NovelScreenSpotï¼ä¸åç¨æ¼æ¸¬è©¦è³æå¨ä½ç¨®ç¨åº¦ä¸å¯ä»¥åå©å° GUI åä½åºç¤æ¨¡åèæ°ç°å¢å°é½çåºæºï¼ä¸¦å¨å¯¦é©ä¸­å±ç¤º GUI-Bee ææ¶éçè³æçæææ§ãæ­¤å¤ï¼æåé²è¡æ¶èç ç©¶ï¼ä»¥é©è­ Q-ICRL æ¹æ³å¨æå GUI-Bee æçæ¹é¢çä½ç¨ãå°æ¡é é¢ï¼https://gui-bee.github.io

##### **Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning**
2501.13893v1 by Zuyao You, Junke Wang, Lingyu Kong, Bo He, Zuxuan Wu

We present Pix2Cap-COCO, the first panoptic pixel-level caption dataset
designed to advance fine-grained visual understanding. To achieve this, we
carefully design an automated annotation pipeline that prompts GPT-4V to
generate pixel-aligned, instance-specific captions for individual objects
within images, enabling models to learn more granular relationships between
objects and their contexts. This approach results in 167,254 detailed captions,
with an average of 22.94 words per caption. Building on Pix2Cap-COCO, we
introduce a novel task, panoptic segmentation-captioning, which challenges
models to recognize instances in an image and provide detailed descriptions for
each simultaneously. To benchmark this task, we design a robust baseline based
on X-Decoder. The experimental results demonstrate that Pix2Cap-COCO is a
particularly challenging dataset, as it requires models to excel in both
fine-grained visual understanding and detailed language generation.
Furthermore, we leverage Pix2Cap-COCO for Supervised Fine-Tuning (SFT) on large
multimodal models (LMMs) to enhance their performance. For example, training
with Pix2Cap-COCO significantly improves the performance of GPT4RoI, yielding
gains in CIDEr +1.4%, ROUGE +0.4%, and SPICE +0.5% on Visual Genome dataset,
and strengthens its region understanding ability on the ViP-BENCH, with an
overall improvement of +5.1%, including notable increases in recognition
accuracy +11.2% and language generation quality +22.2%.

æè¦ï¼<paragraph>æåæåº Pix2Cap-COCOï¼éæ¯ç¬¬ä¸åå¨æ¯åç´ ç´æ¨é¡è³æéï¼æ¨å¨æ¨é²ç²¾ç´°çè¦è¦ºçè§£ãçºæ­¤ï¼æåä»ç´°è¨­è¨äºä¸åèªååæ¨è¨»ç®¡éï¼æç¤º GPT-4V çºå½±åä¸­çåå¥ç©ä»¶ç¢çåç´ å°é½çç¹å®å¯¦ä¾æ¨é¡ï¼ä½¿æ¨¡åè½å¤ å­¸ç¿ç©ä»¶åå¶èçµ¡ä¹éæ´ç´°ç·»çéä¿ãæ­¤æ¹æ³ç¢çäº 167,254 åè©³ç´°æ¨é¡ï¼æ¯åæ¨é¡å¹³åæ 22.94 åå­ãå¨ Pix2Cap-COCO çåºç¤ä¸ï¼æåå¼å¥äºä¸é æ°ä»»åï¼å¨æ¯åå²æ¨é¡ï¼å®ææ°æ¨¡åè­å¥å½±åä¸­çå¯¦ä¾ï¼ä¸¦åæçºæ¯åå¯¦ä¾æä¾è©³ç´°æè¿°ãçºäºå°æ­¤ä»»åé²è¡åºæºæ¸¬è©¦ï¼æåè¨­è¨äºä¸ååºæ¼ X-Decoder çç©©å¥åºæºãå¯¦é©çµæè¡¨æï¼Pix2Cap-COCO æ¯ä¸åç¹å¥å·æææ°æ§çè³æéï¼å çºå®è¦æ±æ¨¡åå¨ç²¾ç´°çè¦è¦ºçè§£åè©³ç´°çèªè¨çææ¹é¢é½è¡¨ç¾åºè²ãæ­¤å¤ï¼æåå©ç¨ Pix2Cap-COCO å°å¤§åå¤æ¨¡ææ¨¡å (LMM) é²è¡ç£ç£å¾®èª¿ (SFT)ï¼ä»¥å¢å¼·å¶æè½ãä¾å¦ï¼ä½¿ç¨ Pix2Cap-COCO é²è¡è¨ç·´å¯é¡¯èæå GPT4RoI çæè½ï¼å¨ Visual Genome è³æéä¸ç²å¾ CIDEr +1.4%ãROUGE +0.4% å SPICE +0.5% çæåï¼ä¸¦å¨ ViP-BENCH ä¸å¢å¼·å¶ååçè§£è½åï¼æ´é«æå +5.1%ï¼åæ¬è­å¥æºç¢ºåº¦ +11.2% åèªè¨çæåè³ª +22.2% çé¡¯èæåã</paragraph>

##### **Exploring Finetuned Audio-LLM on Heart Murmur Features**
2501.13884v1 by Adrian Florea, Xilin Jiang, Nima Mesgarani, Xiaofan Jiang

Large language models (LLMs) for audio have excelled in recognizing and
analyzing human speech, music, and environmental sounds. However, their
potential for understanding other types of sounds, particularly biomedical
sounds, remains largely underexplored despite significant scientific interest.
In this study, we focus on diagnosing cardiovascular diseases using
phonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)
paradigms are restricted to heart murmur classification (healthy vs unhealthy)
and do not predict other acoustic features of the murmur such as timing,
grading, harshness, pitch, and quality, which are important in helping
physicians diagnose the underlying heart conditions. We propose to finetune an
audio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)
dataset and evaluate its performance in classifying 11 expert-labeled murmur
features. Additionally, we aim to achieve more noise-robust and generalizable
system by exploring a preprocessing segmentation algorithm using an audio
representation model, SSAMBA. Our results indicate that the LLM-based model
outperforms state-of-the-art methods in 8 of the 11 features and performs
comparably in the remaining 3. Moreover, the LLM successfully classifies
long-tail murmur features with limited training data, a task that all previous
methods have failed to classify. These findings underscore the potential of
audio LLMs as assistants to human cardiologists in enhancing heart disease
diagnosis.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨è­å¥ååæäººé¡èªè¨ãé³æ¨åç°å¢é³æ¹é¢è¡¨ç¾åºè²ãç¶èï¼åç®¡ç§å­¸çå°æ­¤ææ¿åçèè¶£ï¼ä½å®åå¨çè§£å¶ä»é¡åçè²é³ï¼ç¹å¥æ¯çç©é«å­¸è²é³æ¹é¢çæ½åå¨å¾å¤§ç¨åº¦ä¸ä»æªå¾å°ååæ¢ç´¢ãå¨æ¬ç ç©¶ä¸­ï¼æåå°æ³¨æ¼ä½¿ç¨å¿é³åï¼å³å¿é³ï¼è¨ºæ·å¿è¡ç®¡ç¾çãç¾æçå¤§å¤æ¸æ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) å¸ç¯åéæ¼å¿éé³åé¡ï¼å¥åº·èä¸å¥åº·ï¼ï¼ä¸¦ä¸ä¸æé æ¸¬éé³çå¶ä»è²å­¸ç¹å¾µï¼ä¾å¦æåºãåç´ãç²ç³åº¦ãé³é«ååè³ªï¼éäºç¹å¾µå°æ¼å¹«å©é«çè¨ºæ·æ½å¨çå¿èç¾çéå¸¸éè¦ãæåå»ºè­°å°é³è¨ LLM Qwen2-Audio é²è¡å¾®èª¿ï¼ä½¿ç¨ PhysioNet CirCor DigiScope å¿é³å (PCG) è³æéï¼ä¸¦è©ä¼°å¶å¨åé¡ 11 åå°å®¶æ¨è¨éé³ç¹å¾µæ¹é¢çæè½ãæ­¤å¤ï¼æåæ¨å¨ééæ¢ç´¢ä½¿ç¨é³è¨è¡¨ç¤ºæ¨¡å SSAMBA çé èçåå²æ¼ç®æ³ï¼ä¾å¯¦ç¾æ´å¼·å¤§çæåªæ§åå¯æ¦åçç³»çµ±ãæåççµæè¡¨æï¼åºæ¼ LLM çæ¨¡åå¨ 11 åç¹å¾µä¸­ç 8 åç¹å¾µä¸­åªæ¼æåé²çæ¹æ³ï¼èå¨å¶é¤ 3 åç¹å¾µä¸­è¡¨ç¾ç¸ç¶ãæ­¤å¤ï¼LLM æåå°å°å·ææéè¨ç·´è³æçé·å°¾éé³ç¹å¾µé²è¡åé¡ï¼éæ¯ä»¥åæææ¹æ³é½ç¡æ³åé¡çä¸é ä»»åãéäºç¼ç¾å¼·èª¿äºé³è¨ LLM ä½çºäººé¡å¿èçå°å®¶çå©æå¨å¢å¼·å¿èçè¨ºæ·æ¹é¢çæ½åã

##### **A RAG-Based Institutional Assistant**
2501.13880v1 by Gustavo Kuratomi, Paulo Pirozelli, Fabio G. Cozman, Sarajane M. Peres

Although large language models (LLMs) demonstrate strong text generation
capabilities, they struggle in scenarios requiring access to structured
knowledge bases or specific documents, limiting their effectiveness in
knowledge-intensive tasks. To address this limitation, retrieval-augmented
generation (RAG) models have been developed, enabling generative models to
incorporate relevant document fragments into their inputs. In this paper, we
design and evaluate a RAG-based virtual assistant specifically tailored for the
University of S\~ao Paulo. Our system architecture comprises two key modules: a
retriever and a generative model. We experiment with different types of models
for both components, adjusting hyperparameters such as chunk size and the
number of retrieved documents. Our optimal retriever model achieves a Top-5
accuracy of 30%, while our most effective generative model scores 22.04\%
against ground truth answers. Notably, when the correct document chunks are
supplied to the LLMs, accuracy significantly improves to 54.02%, an increase of
over 30 percentage points. Conversely, without contextual input, performance
declines to 13.68%. These findings highlight the critical role of database
access in enhancing LLM performance. They also reveal the limitations of
current semantic search methods in accurately identifying relevant documents
and underscore the ongoing challenges LLMs face in generating precise
responses.

æè¦ï¼åç®¡å¤§åèªè¨æ¨¡å (LLM) å±ç¾åºå¼·å¤§çæå­çæè½åï¼ä½å®åå¨éè¦å­åçµæ§åç¥è­åº«æç¹å®æä»¶çææ³ä¸æéå°å°é£ï¼ééå¶äºå®åå¨ç¥è­å¯éåä»»åä¸­çæè½ãçºäºè§£æ±ºéåéå¶ï¼å·²ç¶éç¼åºæª¢ç´¢å¢å¼·çæ (RAG) æ¨¡åï¼è®çææ¨¡åè½å¤ å°ç¸éçæä»¶çæ®µç´å¥å¶è¼¸å¥ä¸­ãå¨æ¬æä¸­ï¼æåè¨­è¨ä¸¦è©ä¼°ä¸åç¹å¥éå°èä¿ç¾å¤§å­¸éèº«æé çåºæ¼ RAG çèæ¬å©çãæåçç³»çµ±æ¶æ§åå«å©åééµæ¨¡çµï¼ä¸åæª¢ç´¢å¨åä¸åçææ¨¡åãæåå°éå©ååä»¶çä¸åé¡åæ¨¡åé²è¡å¯¦é©ï¼èª¿æ´è¶åæ¸ï¼ä¾å¦åå¡å¤§å°åæª¢ç´¢æä»¶æ¸ãæåæä½³çæª¢ç´¢å¨æ¨¡åéå° 30% çå 5 åæºç¢ºçï¼èæåæææççææ¨¡åéå°çå¯¦ç­æ¡å¾åçº 22.04%ãå¼å¾æ³¨æçæ¯ï¼ç¶æ­£ç¢ºçæä»¶åå¡æä¾çµ¦ LLM æï¼æºç¢ºçé¡¯èæåè³ 54.02%ï¼æåè¶é 30 åç¾åé»ãç¸åå°ï¼å¨æ²æä¸ä¸æè¼¸å¥çææ³ä¸ï¼æè½ä¸éè³ 13.68%ãéäºç¼ç¾çªé¡¯äºè³æåº«å­åå¨æå LLM æè½ä¸­æ®æ¼ééµè§è²ãå®åä¹æ­ç¤ºäºç¶åèªææå°æ¹æ³å¨æºç¢ºè­å¥ç¸éæä»¶æ¹é¢çéå¶ï¼ä¸¦å¼·èª¿ LLM å¨ç¢çç²¾ç¢ºåææé¢è¨çæçºææ°ã

##### **Where Do You Go? Pedestrian Trajectory Prediction using Scene Features**
2501.13848v1 by Mohammad Ali Rezaei, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi

Accurate prediction of pedestrian trajectories is crucial for enhancing the
safety of autonomous vehicles and reducing traffic fatalities involving
pedestrians. While numerous studies have focused on modeling interactions among
pedestrians to forecast their movements, the influence of environmental factors
and scene-object placements has been comparatively underexplored. In this
paper, we present a novel trajectory prediction model that integrates both
pedestrian interactions and environmental context to improve prediction
accuracy. Our approach captures spatial and temporal interactions among
pedestrians within a sparse graph framework. To account for pedestrian-scene
interactions, we employ advanced image enhancement and semantic segmentation
techniques to extract detailed scene features. These scene and interaction
features are then fused through a cross-attention mechanism, enabling the model
to prioritize relevant environmental factors that influence pedestrian
movements. Finally, a temporal convolutional network processes the fused
features to predict future pedestrian trajectories. Experimental results
demonstrate that our method significantly outperforms existing state-of-the-art
approaches, achieving ADE and FDE values of 0.252 and 0.372 meters,
respectively, underscoring the importance of incorporating both social
interactions and environmental context in pedestrian trajectory prediction.

æè¦ï¼è¡äººè»è·¡çç²¾æºé æ¸¬å°æ¼æåèªåé§é§è»è¼çå®å¨æ§åæ¸å°è¡äººäº¤éäºæè³ééè¦ãéç¶è¨±å¤ç ç©¶å°æ³¨æ¼å»ºæ¨¡è¡äººä¹éçäºåä»¥é æ¸¬å¶ç§»åï¼ä½ç°å¢å ç´ åå ´æ¯ç©ä»¶æºæ¾çå½±é¿ç¸å°æªè¢«ååæ¢è¨ãå¨æ¬æä¸­ï¼æåæåºä¸ååµæ°çè»è·¡é æ¸¬æ¨¡åï¼æ´åè¡äººäºååç°å¢èæ¯ä»¥æåé æ¸¬æºç¢ºåº¦ãæåçåæ³å¨ç¨çåå½¢æ¶æ§ä¸­æ·åè¡äººä¹éçç©ºéåæéäºåãçºäºèéè¡äººèå ´æ¯çäºåï¼æåæ¡ç¨åé²çå½±åå¢å¼·åèªæåå²æè¡ä¾èåè©³ç´°çå ´æ¯ç¹å¾µãç¶å¾ééäº¤åæ³¨ææ©å¶èåéäºå ´æ¯åäºåç¹å¾µï¼ä½¿æ¨¡åè½å¤ åªåèæ®å½±é¿è¡äººç§»åçç¸å³ç°å¢å ç´ ãæå¾ï¼æåºå·ç©ç¶²è·¯èçèåçç¹å¾µä»¥é æ¸¬æªä¾çè¡äººè»è·¡ãå¯¦é©çµæè­æï¼æåçæ¨¡åæé¡¯åªæ¼ç¾æçæåé²æ¹æ³ï¼åå¥éå° ADE å FDE å¼çº 0.252 å 0.372 å¬å°ºï¼å¼·èª¿äºå¨è¡äººè»è·¡é æ¸¬ä¸­ç´å¥ç¤¾æäºååç°å¢èæ¯çéè¦æ§ã

##### **Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages**
2501.13836v1 by Farhana Shahid, Mona Elswah, Aditya Vashistha

Most social media users come from non-English speaking countries in the
Global South. Despite the widespread prevalence of harmful content in these
regions, current moderation systems repeatedly struggle in low-resource
languages spoken there. In this work, we examine the challenges AI researchers
and practitioners face when building moderation tools for low-resource
languages. We conducted semi-structured interviews with 22 AI researchers and
practitioners specializing in automatic detection of harmful content in four
diverse low-resource languages from the Global South. These are: Tamil from
South Asia, Swahili from East Africa, Maghrebi Arabic from North Africa, and
Quechua from South America. Our findings reveal that social media companies'
restrictions on researchers' access to data exacerbate the historical
marginalization of these languages, which have long lacked datasets for
studying online harms. Moreover, common preprocessing techniques and language
models, predominantly designed for data-rich English, fail to account for the
linguistic complexity of low-resource languages. This leads to critical errors
when moderating content in Tamil, Swahili, Arabic, and Quechua, which are
morphologically richer than English. Based on our findings, we establish that
the precarities in current moderation pipelines are rooted in deep systemic
inequities and continue to reinforce historical power imbalances. We conclude
by discussing multi-stakeholder approaches to improve moderation for
low-resource languages.

æè¦ï¼å¤§å¤æ¸ç¤¾ç¾¤åªé«ä½¿ç¨èä¾èªå¨çåæ¹çéè±èªç³»åå®¶ãåç®¡éäºå°åæ®éå­å¨æå®³å§å®¹ï¼ä½ç®åçå¯©æ ¸ç³»çµ±ä»åè¦å¨ç¶å°æä½¿ç¨çä½è³æºèªè¨ä¸­é¢è¨ææ°ãå¨éé ç ç©¶ä¸­ï¼æåæ¢è¨äºäººå·¥æºæ§ç ç©¶äººå¡åå¾æ¥­äººå¡å¨çºä½è³æºèªè¨å»ºæ§å¯©æ ¸å·¥å·ææé¢è¨çææ°ãæåå° 22 ä½äººå·¥æºæ§ç ç©¶äººå¡åå¾æ¥­äººå¡é²è¡äºåçµæ§å¼è¨ªè«ï¼éäºäººå¡å°ç²¾æ¼èªååµæ¸¬ä¾èªå¨çåæ¹åç¨®ä¸åä½è³æºèªè¨ä¸­çæå®³å§å®¹ãéäºèªè¨åå¥çºï¼ä¾èªåäºçæ³°ç±³ç¾èªãä¾èªæ±éçæ¯ç¦å¸éèªãä¾èªåéçé¦¬æ ¼éå¸é¿æä¼¯èªåä¾èªåç¾æ´²çèä¸äºèªãæåçç ç©¶çµæé¡¯ç¤ºï¼ç¤¾ç¾¤åªé«å¬å¸å°ç ç©¶äººå¡åå¾è³æçéå¶å åäºéäºèªè¨å¨æ­·å²ä¸çéç·£åï¼éäºèªè¨é·æä»¥ä¾ç¼ºä¹ç¨æ¼ç ç©¶ç¶²è·¯å±å®³çè³æéãæ­¤å¤ï¼å¸¸è¦çé èçæè¡åèªè¨æ¨¡åä¸»è¦è¨­è¨ç¨æ¼è³æè±å¯çè±èªï¼ç¡æ³èéä½è³æºèªè¨çèªè¨è¤éæ§ãéå°è´å¨å¯©æ ¸æ³°ç±³ç¾èªãæ¯ç¦å¸éèªãé¿æä¼¯èªåèä¸äºèªçå§å®¹æåºç¾å´éçé¯èª¤ï¼éäºèªè¨å¨å½¢æä¸æ¯è±èªæ´è±å¯ãæ ¹ææåçç ç©¶çµæï¼æåç¢ºç«äºç®åå¯©æ ¸æµç¨ä¸­çä¸ç©©å®æ§æ ¹æºæ¼æ·±å±¤çç³»çµ±æ§ä¸å¹³ç­ï¼ä¸¦æçºå¼·åæ­·å²ä¸çæ¬åå¤±è¡¡ãæåæå¾è¨è«äºæ¹åä½è³æºèªè¨å¯©æ ¸çå¤æ¹å©çç¸éèæ¹æ³ã

##### **On the Reasoning Capacity of AI Models and How to Quantify It**
2501.13833v1 by Santosh Kumar Radha, Oktay Goktas

Recent advances in Large Language Models (LLMs) have intensified the debate
surrounding the fundamental nature of their reasoning capabilities. While
achieving high performance on benchmarks such as GPQA and MMLU, these models
exhibit limitations in more complex reasoning tasks, highlighting the need for
more rigorous evaluation methodologies. We propose a novel phenomenological
approach that goes beyond traditional accuracy metrics to probe the underlying
mechanisms of model behavior, establishing a framework that could broadly
impact how we analyze and understand AI systems. Using positional bias in
multiple-choice reasoning tasks as a case study, we demonstrate how systematic
perturbations can reveal fundamental aspects of model decision-making. To
analyze these behaviors, we develop two complementary phenomenological models:
a Probabilistic Mixture Model (PMM) that decomposes model responses into
reasoning, memorization, and guessing components and an Information-Theoretic
Consistency (ITC) analysis that quantifies the relationship between model
confidence and strategy selection. Through controlled experiments on reasoning
benchmarks, we show that true reasoning remains challenging for current models,
with apparent success often relying on sophisticated combinations of
memorization and pattern matching rather than genuine logical deduction. More
fundamentally, we demonstrate that accuracy alone often overstates a model's
reasoning abilities, as model behavior can be characterized through underlying
mechanisms in the phase space of cognitive strategies, revealing how models
dynamically balance different approaches when responding to queries. This
framework enables quantitative criteria for real-world deployments, allowing
applications to specify reliability thresholds based on strategy distributions
rather than aggregate performance metrics.

æè¦ï¼<paragraph>å¤§åèªè¨æ¨¡å (LLM) çææ°é²å±å åäºåç¹å¶æ¨çè½åçæ ¹æ¬æ§è³ªçç­è«ãåç®¡å¨ GPQA å MMLU ç­åºæºæ¸¬è©¦ä¸­åå¾äºå¾é«çæ§è½ï¼ä½éäºæ¨¡åå¨æ´è¤éçæ¨çä»»åä¸­è¡¨ç¾åºä¾·éæ§ï¼å¸é¡¯äºå°æ´å´è¬¹è©ä¼°æ¹æ³çéæ±ãæåæåºäºä¸ç¨®æ°ç©çç¾è±¡å­¸æ¹æ³ï¼å®è¶è¶äºå³çµ±çæºç¢ºåº¦ææ¨ï¼ä»¥æ¢è¨æ¨¡åè¡çºçåºå±¤æ©å¶ï¼å»ºç«ä¸åæ¡æ¶ï¼å¯ä»¥å»£æ³å°å½±é¿æååæåçè§£ AI ç³»çµ±çæ¹å¼ãä½¿ç¨å¤é¸æ¨çä»»åä¸­çä½ç½®åèª¤ä½çºæ¡ä¾ç ç©¶ï¼æåå±ç¤ºäºç³»çµ±æ§æ¾åå¦ä½æ­ç¤ºæ¨¡åæ±ºç­å¶å®çåºæ¬æ¹é¢ãçºäºåæéäºè¡çºï¼æåéç¼äºå©åäºè£çç¾è±¡å­¸æ¨¡åï¼ä¸åæ¦çæ··åæ¨¡å (PMM)ï¼å®å°æ¨¡ååæåè§£çºæ¨çãè¨æ¶åçæ¸¬çµæé¨åï¼ä»¥åä¸åä¿¡æ¯è«ä¸è´æ§ (ITC) åæï¼å®éåäºæ¨¡åä¿¡å¿åç­ç¥é¸æä¹éçéä¿ãééå¨æ¨çåºæºæ¸¬è©¦ä¸é²è¡åæ§å¯¦é©ï¼æåè¡¨æçæ­£çæ¨çå°æ¼ç¶åæ¨¡åä»ç¶å·æææ°æ§ï¼æé¡¯çæåéå¸¸ä¾è³´æ¼è¨æ¶åæ¨¡å¼å¹éçè¤éçµåï¼èä¸æ¯çæ­£çéè¼¯æ¨è«ãæ´æ ¹æ¬çæ¯ï¼æåè­æäºæºç¢ºæ§æ¬èº«éå¸¸æèªå¤§æ¨¡åçæ¨çè½åï¼å çºæ¨¡åè¡çºå¯ä»¥ééèªç¥ç­ç¥ç¸ç©ºéä¸­çåºå±¤æ©å¶ä¾è¡¨å¾µï¼æ­ç¤ºäºæ¨¡åå¨é¿ææ¥è©¢æå¦ä½åæå¹³è¡¡ä¸åçæ¹æ³ãæ­¤æ¡æ¶çºå¯¦éé¨ç½²æä¾äºå®éæ¨æºï¼åè¨±æç¨ç¨åºæ ¹æç­ç¥åä½èä¸æ¯ç¸½é«æ§è½ææ¨æå®å¯é æ§é¾å¼ã</paragraph>

##### **Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing**
2501.13831v1 by Hao Zhang, Felix Stahlberg, Shankar Kumar

Large Language Models (LLMs) excel at rewriting tasks such as text style
transfer and grammatical error correction. While there is considerable overlap
between the inputs and outputs in these tasks, the decoding cost still
increases with output length, regardless of the amount of overlap. By
leveraging the overlap between the input and the output, Kaneko and Okazaki
(2023) proposed model-agnostic edit span representations to compress the
rewrites to save computation. They reported an output length reduction rate of
nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,
we propose alternative edit phrase representations inspired by phrase-based
statistical machine translation. We systematically compare our phrasal
representations with their span representations. We apply the LLM rewriting
model to the task of Automatic Speech Recognition (ASR) post editing and show
that our target-phrase-only edit representation has the best
efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes
50-60% of the WER gap between the edit span model and the full rewrite model
while losing only 10-20% of the length reduction rate of the edit span model.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨éå¯«ä»»åä¸­è¡¨ç¾åºè²ï¼ä¾å¦æå­é¢¨æ ¼è½æåèªæ³é¯èª¤æ´æ­£ãéç¶å¨éäºä»»åä¸­è¼¸å¥åè¼¸åºä¹éæç¸ç¶å¤§çéçï¼ä½è§£ç¢¼ææ¬ä»æé¨èè¼¸åºé·åº¦å¢å ï¼èèéçéç¡éãééå©ç¨è¼¸å¥åè¼¸åºä¹éçéçï¼Kaneko å Okazaki (2023) æåºèæ¨¡åç¡éçç·¨è¼¯ç¯åè¡¨ç¤ºæ³ï¼ä»¥å£ç¸®éå¯«å§å®¹ä»¥ç¯çéç®ãä»åå ±åèªªï¼å¨åé éå¯«ä»»åä¸­ï¼è¼¸åºé·åº¦æ¸å°çæ¥è¿ 80%ï¼ä¸å°æºç¢ºåº¦çå½±é¿å¾å°ãå¨æ¬æä¸­ï¼æåæåºååºæ¼ç­èªççµ±è¨æ©å¨ç¿»è­¯åç¼çæ¿ä»£ç·¨è¼¯ç­èªè¡¨ç¤ºæ³ãæåç³»çµ±æ§å°å°æåçç­èªè¡¨ç¤ºæ³èä»åçç¯åè¡¨ç¤ºæ³é²è¡æ¯è¼ãæåå° LLM éå¯«æ¨¡åæç¨æ¼èªåèªé³è­å¥ (ASR) å¾ç·¨è¼¯ä»»åï¼ä¸¦å±ç¤ºæåçåç®æ¨ç­èªç·¨è¼¯è¡¨ç¤ºæ³å·ææä½³çæçæºç¢ºæ§æ¬è¡¡ãå¨ LibriSpeech æ¸¬è©¦éä¸­ï¼æåçæ¨¡åç¸®å°äºç·¨è¼¯ç¯åæ¨¡ååå®æ´éå¯«æ¨¡åä¹é 50-60% ç WER å·®è·ï¼åæåæå¤±äºç·¨è¼¯ç¯åæ¨¡å 10-20% çé·åº¦æ¸å°çã

##### **A space-decoupling framework for optimization on bounded-rank matrices with orthogonally invariant constraints**
2501.13830v1 by Yan Yang, Bin Gao, Ya-xiang Yuan

Imposing additional constraints on low-rank optimization has garnered growing
interest. However, the geometry of coupled constraints hampers the
well-developed low-rank structure and makes the problem intricate. To this end,
we propose a space-decoupling framework for optimization on bounded-rank
matrices with orthogonally invariant constraints. The ``space-decoupling" is
reflected in several ways. We show that the tangent cone of coupled constraints
is the intersection of tangent cones of each constraint. Moreover, we decouple
the intertwined bounded-rank and orthogonally invariant constraints into two
spaces, leading to optimization on a smooth manifold. Implementing Riemannian
algorithms on this manifold is painless as long as the geometry of additional
constraints is known. In addition, we unveil the equivalence between the
reformulated problem and the original problem. Numerical experiments on
real-world applications -- spherical data fitting, graph similarity measuring,
low-rank SDP, model reduction of Markov processes, reinforcement learning, and
deep learning -- validate the superiority of the proposed framework.

æè¦ï¼æ½å é¡å¤çç´æå¨ä½ç§©æä½³åå·²ç¶å¼èµ·è¶ä¾è¶å¤çèè¶£ãç¶èï¼è¦åç´æçå¹¾ä½çµæ§é»ç¤äºç¼å±è¯å¥½çä½ç§©çµæ§ï¼ä½¿å¾åé¡è®å¾è¤éãçºæ­¤ï¼æåæåºäºä¸åç¨æ¼æä½³åæ­£äº¤ä¸è®ç´æçéçç§©ç©é£çç©ºéè§£è¦æ¡æ¶ã``ç©ºéè§£è¦''åæ å¨å¹¾åæ¹é¢ãæåè¡¨æè¦åç´æçåç·éæ¯æ¯åç´æçåç·éçäº¤éãæ­¤å¤ï¼æåå°äº¤ç¹çéçç§©åæ­£äº¤ä¸è®ç´æè§£è¦æå©åç©ºéï¼å¾èå°è´å¨å¹³æ»æµå½¢ä¸é²è¡æä½³åãåªè¦å·²ç¥é¡å¤ç´æçå¹¾ä½çµæ§ï¼å¨éåæµå½¢ä¸å¯¦ä½é»æ¼æ¼ç®æ³å°±æå¾è¼é¬ãæ­¤å¤ï¼æåæ­ç¤ºäºéæ°è¡¨è¿°çåé¡ååå§åé¡ä¹éçç­å¹æ§ãå¨ç¾å¯¦ä¸çæç¨ä¸­çæ¸å¼å¯¦é©ââçå½¢è³ææ¬åãåå½¢ç¸ä¼¼æ§æ¸¬éãä½ç§© SDPãé¦¬å¯å¤«éç¨çæ¨¡åç°¡åãå¼·åå­¸ç¿åæ·±åº¦å­¸ç¿ââé©è­äºææåºçæ¡æ¶çåªè¶æ§ã

##### **Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos**
2501.13826v1 by Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, Ziwei Liu

Humans acquire knowledge through three cognitive stages: perceiving
information, comprehending knowledge, and adapting knowledge to solve novel
problems. Videos serve as an effective medium for this learning process,
facilitating a progression through these cognitive stages. However, existing
video benchmarks fail to systematically evaluate the knowledge acquisition
capabilities in Large Multimodal Models (LMMs). To address this gap, we
introduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed to
assess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMU
features a curated collection of 300 expert-level videos and 900
human-annotated questions across six disciplines, evaluating knowledge
acquisition through stage-aligned question-answer pairs: Perception,
Comprehension, and Adaptation. A proposed knowledge gain metric,
{\Delta}knowledge, quantifies improvement in performance after video viewing.
Evaluation of LMMs reveals a steep decline in performance as cognitive demands
increase and highlights a significant gap between human and model knowledge
acquisition, underscoring the need for methods to enhance LMMs' capability to
learn and adapt from videos.

æè¦ï¼äººé¡ééä¸åèªç¥éæ®µä¾ç²åç¥è­ï¼æç¥è³è¨ãçè§£ç¥è­ï¼ä»¥åé©æç¥è­ä¾è§£æ±ºæ°åé¡ãå½±çä½çºæ­¤å­¸ç¿æ­·ç¨çææåªä»ï¼ä¿é²äºéäºèªç¥éæ®µçé²å±ãç¶èï¼ç¾æçå½±çåºæºæªè½ç³»çµ±æ§å°è©ä¼°å¤§åå¤æ¨¡ææ¨¡å (LMM) ä¸­çç¥è­ç²åè½åãçºäºè§£æ±ºéåå·®è·ï¼æåå¼å¥äº Video-MMMUï¼ä¸åå¤æ¨¡æãå¤é åçåºæºï¼æ¨å¨è©ä¼° LMM å¾å½±çä¸­ç²ååå©ç¨ç¥è­çè½åãVideo-MMMU å·æç¶éç­å±ç 300 åå°å®¶ç´å½±çå 900 åè·¨å­åé åçäººå·¥æ¨è¨»åé¡ï¼ééèéæ®µå°é½çåé¡åç­å°ä¾è©ä¼°ç¥è­ç²åï¼æç¥ãçè§£åé©æãä¸åæåºçç¥è­ç²åææ¨ {\Delta}knowledgeï¼éåäºè§çå½±çå¾æè½çæåãå° LMM çè©ä¼°é¡¯ç¤ºï¼é¨èèªç¥éæ±çå¢å ï¼æè½å¤§å¹ä¸éï¼ä¸¦çªé¡¯äºäººé¡åæ¨¡åç¥è­ç²åä¹éçé¡¯èå·®è·ï¼å¼·èª¿äºå¢å¼· LMM å¾å½±çä¸­å­¸ç¿åé©æçè½åçæ¹æ³çå¿è¦æ§ã

##### **Hallucinations Can Improve Large Language Models in Drug Discovery**
2501.13824v1 by Shuzhou Yuan, Michael FÃ¤rber

Concerns about hallucinations in Large Language Models (LLMs) have been
raised by researchers, yet their potential in areas where creativity is vital,
such as drug discovery, merits exploration. In this paper, we come up with the
hypothesis that hallucinations can improve LLMs in drug discovery. To verify
this hypothesis, we use LLMs to describe the SMILES string of molecules in
natural language and then incorporate these descriptions as part of the prompt
to address specific tasks in drug discovery. Evaluated on seven LLMs and five
classification tasks, our findings confirm the hypothesis: LLMs can achieve
better performance with text containing hallucinations. Notably, Llama-3.1-8B
achieves an 18.35% gain in ROC-AUC compared to the baseline without
hallucination. Furthermore, hallucinations generated by GPT-4o provide the most
consistent improvements across models. Additionally, we conduct empirical
analyses and a case study to investigate key factors affecting performance and
the underlying reasons. Our research sheds light on the potential use of
hallucinations for LLMs and offers new perspectives for future research
leveraging LLMs in drug discovery.

æè¦ï¼å¤§åè¯­è¨æ¨¡å (LLM) ä¸­çå¹»è§é®é¢å·²è¢«ç ç©¶äººåæåºï¼ä½å¶å¨åé åè³å³éè¦çé¢åï¼ä¾å¦è¯ç©åç°ï¼ä¸­çæ½åå¼å¾æ¢ç´¢ãå¨æ¬æä¸­ï¼æä»¬æåºäºä¸ä¸ªåè®¾ï¼å³å¹»è§å¯ä»¥æ¹å LLM å¨è¯ç©åç°ä¸­çè¡¨ç°ãä¸ºäºéªè¯è¿ä¸ªåè®¾ï¼æä»¬ä½¿ç¨ LLM ç¨èªç¶è¯­è¨æè¿°åå­ç SMILES å­ç¬¦ä¸²ï¼ç¶åå°è¿äºæè¿°ä½ä¸ºæç¤ºçä¸é¨åï¼ä»¥è§£å³è¯ç©åç°ä¸­çç¹å®ä»»å¡ãå¨ä¸ä¸ª LLM åäºä¸ªåç±»ä»»å¡ä¸çè¯ä¼°ä¸­ï¼æä»¬çåç°è¯å®äºè¿ä¸åè®¾ï¼LLM å¯ä»¥éè¿åå«å¹»è§çææ¬è·å¾æ´å¥½çæ§è½ãå¼å¾æ³¨æçæ¯ï¼ä¸æ²¡æå¹»è§çåºçº¿ç¸æ¯ï¼Llama-3.1-8B å¨ ROC-AUC ä¸­è·å¾äº 18.35% çæ¶çãæ­¤å¤ï¼GPT-4o çæçå¹»è§å¨æææ¨¡åä¸­æä¾äºæä¸è´çæ¹è¿ãæ­¤å¤ï¼æä»¬è¿è¡äºå®è¯åæåæ¡ä¾ç ç©¶ï¼ä»¥è°æ¥å½±åæ§è½çå³é®å ç´ åæ½å¨åå ãæä»¬çç ç©¶æ­ç¤ºäºå¹»è§å¨ LLM ä¸­çæ½å¨ç¨éï¼å¹¶ä¸ºæªæ¥å©ç¨ LLM è¿è¡è¯ç©åç°çç ç©¶æä¾äºæ°çè§è§ã

##### **Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data**
2501.13818v1 by Frederik Pahde, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek

Deep neural networks are increasingly employed in high-stakes medical
applications, despite their tendency for shortcut learning in the presence of
spurious correlations, which can have potentially fatal consequences in
practice. Detecting and mitigating shortcut behavior is a challenging task that
often requires significant labeling efforts from domain experts. To alleviate
this problem, we introduce a semi-automated framework for the identification of
spurious behavior from both data and model perspective by leveraging insights
from eXplainable Artificial Intelligence (XAI). This allows the retrieval of
spurious data points and the detection of model circuits that encode the
associated prediction rules. Moreover, we demonstrate how these shortcut
encodings can be used for XAI-based sample- and pixel-level data annotation,
providing valuable information for bias mitigation methods to unlearn the
undesired shortcut behavior. We show the applicability of our framework using
four medical datasets across two modalities, featuring controlled and
real-world spurious correlations caused by data artifacts. We successfully
identify and mitigate these biases in VGG16, ResNet50, and contemporary Vision
Transformer models, ultimately increasing their robustness and applicability
for real-world medical tasks.

æè¦ï¼æ·±åº¦ç¥ç»ç½ç»è¶æ¥è¶å¤å°ç¨äºé«é£é©å»çåºç¨ä¸­ï¼å°½ç®¡å®ä»¬å¨å­å¨èåç¸å³æ§çæåµä¸å¾åäºæ·å¾å­¦ä¹ ï¼è¿å¨å®è·µä¸­å¯è½äº§çè´å½çåæãæ£æµåç¼è§£æ·å¾è¡ä¸ºæ¯ä¸é¡¹è°å·¨çä»»å¡ï¼éå¸¸éè¦é¢åä¸å®¶çå¤§éæ è®°å·¥ä½ãä¸ºäºç¼è§£è¿ä¸ªé®é¢ï¼æä»¬å¼å¥äºä¸ä¸ªåèªå¨æ¡æ¶ï¼ç¨äºä»æ°æ®åæ¨¡åçè§åº¦è¯å«èåè¡ä¸ºï¼æ¹æ³æ¯å©ç¨å¯è§£éäººå·¥æºè½ (XAI) çè§è§£ãè¿åè®¸æ£ç´¢èåæ°æ®ç¹å¹¶æ£æµå¯¹å³èé¢æµè§åè¿è¡ç¼ç çæ¨¡åçµè·¯ãæ­¤å¤ï¼æä»¬æ¼ç¤ºäºå¦ä½ä½¿ç¨è¿äºæ·å¾ç¼ç è¿è¡åºäº XAI çæ ·æ¬ååç´ çº§æ°æ®æ³¨éï¼ä¸ºåå·®ç¼è§£æ¹æ³æä¾æä»·å¼çä¿¡æ¯ï¼ä»¥æ¶é¤ä¸éè¦çæ·å¾è¡ä¸ºãæä»¬ä½¿ç¨è·¨è¶ä¸¤ç§æ¹å¼çåä¸ªå»å­¦æ°æ®éå±ç¤ºäºæä»¬æ¡æ¶çéç¨æ§ï¼è¿äºæ°æ®éå·æç±æ°æ®ä¼ªåå¼èµ·çåæ§åçå®ä¸çèåç¸å³æ§ãæä»¬æåå°è¯å«å¹¶åè½»äº VGG16ãResNet50 åå½ä»£ Vision Transformer æ¨¡åä¸­çè¿äºåå·®ï¼æç»æé«äºå®ä»¬çé²æ£æ§åå¨çå®ä¸çå»çä»»å¡ä¸­çéç¨æ§ã

##### **Learning to Help in Multi-Class Settings**
2501.13810v1 by Yu Wu, Yansong Li, Zeyu Dong, Nitya Sathyavageeswaran, Anand D. Sarwate

Deploying complex machine learning models on resource-constrained devices is
challenging due to limited computational power, memory, and model
retrainability. To address these limitations, a hybrid system can be
established by augmenting the local model with a server-side model, where
samples are selectively deferred by a rejector and then sent to the server for
processing. The hybrid system enables efficient use of computational resources
while minimizing the overhead associated with server usage. The recently
proposed Learning to Help (L2H) model trains a server model given a fixed local
(client) model, differing from the Learning to Defer (L2D) framework, which
trains the client for a fixed (expert) server. In both L2D and L2H, the
training includes learning a rejector at the client to determine when to query
the server. In this work, we extend the L2H model from binary to multi-class
classification problems and demonstrate its applicability in a number of
different scenarios of practical interest in which access to the server may be
limited by cost, availability, or policy. We derive a stage-switching surrogate
loss function that is differentiable, convex, and consistent with the Bayes
rule corresponding to the 0-1 loss for the L2H model. Experiments show that our
proposed methods offer an efficient and practical solution for multi-class
classification in resource-constrained environments.

æè¦ï¼<paragraph>å¨è³æºåéçè£ç½®ä¸é¨ç½²è¤éçæ©å¨å­¸ç¿æ¨¡åï¼ç±æ¼éç®è½åãè¨æ¶é«åæ¨¡ååè¨ç·´è½åæéï¼å æ­¤å·æææ°æ§ãçºäºè§£æ±ºéäºéå¶ï¼å¯ä»¥ééæ´åæ¬å°æ¨¡åæ­éä¼ºæå¨ç«¯æ¨¡åä¾å»ºç«ä¸åæ··åç³»çµ±ï¼å¶ä¸­æ¨£æ¬ç±æçµå¨é¸ææ§å°éå»¶ï¼ç¶å¾å³éè³ä¼ºæå¨é²è¡èçãæ··åç³»çµ±è½ææå©ç¨éç®è³æºï¼åæå°èä¼ºæå¨ä½¿ç¨ç¸éçéé·éå°æä½ãæè¿æåºçå­¸ç¿åå© (L2H) æ¨¡åæè¨ç·´ä¸åä¼ºæå¨æ¨¡åï¼çµ¦å®ä¸ååºå®çæ¬å° (ç¨æ¶ç«¯) æ¨¡åï¼éèå­¸ç¿éå»¶ (L2D) æ¶æ§ä¸åï¼å¾èæè¨ç·´ç¨æ¶ç«¯ä»¥æ­éä¸ååºå®ç (å°å®¶) ä¼ºæå¨ãå¨ L2D å L2H ä¸­ï¼è¨ç·´åå«å¨ç¨æ¶ç«¯å­¸ç¿ä¸åæçµå¨ï¼ä»¥æ±ºå®ä½ææ¥è©¢ä¼ºæå¨ãå¨éé å·¥ä½ä¸­ï¼æåå° L2H æ¨¡åå¾äºååé¡åé¡æ´åå°å¤é¡å¥åé¡åé¡ï¼ä¸¦å±ç¤ºå¶å¨è¨±å¤ä¸åçå¯¦éæç¨æå¢ä¸­çé©ç¨æ§ï¼å¶ä¸­å°ä¼ºæå¨çå­åå¯è½åå°ææ¬ãå¯ç¨æ§ææ¿ç­çéå¶ãæåæ¨å°åºä¸åå¯å¾®åãå¸ä¸è L2H æ¨¡åç 0-1 æå¤±ç¸ç¬¦çè²æ°å®çåæ®µåæä»£çæå¤±å½æ¸ãå¯¦é©é¡¯ç¤ºï¼æåæåºçæ¹æ³æä¾äºä¸åå¨è³æºåéçç°å¢ä¸­é²è¡å¤é¡å¥åé¡çææä¸å¯¦ç¨çè§£æ±ºæ¹æ¡ã</paragraph>

##### **Parameter-Efficient Fine-Tuning for Foundation Models**
2501.13787v1 by Dan Zhang, Tao Feng, Lilong Xue, Yuandong Wang, Yuxiao Dong, Jie Tang

This survey delves into the realm of Parameter-Efficient Fine-Tuning (PEFT)
within the context of Foundation Models (FMs). PEFT, a cost-effective
fine-tuning technique, minimizes parameters and computational complexity while
striving for optimal downstream task performance. FMs, like ChatGPT, DALL-E,
and LLaVA specialize in language understanding, generative tasks, and
multimodal tasks, trained on diverse datasets spanning text, images, and
videos. The diversity of FMs guides various adaptation strategies for PEFT.
Therefore, this survey aims to provide a comprehensive overview of PEFT
techniques applied to diverse FMs and address critical gaps in understanding
the techniques, trends, and applications. We start by providing a detailed
development of FMs and PEFT. Subsequently, we systematically review the key
categories and core mechanisms of PEFT across diverse FMs to offer a
comprehensive understanding of trends. We also explore the most recent
applications across various FMs to demonstrate the versatility of PEFT,
shedding light on the integration of systematic PEFT methods with a range of
FMs. Furthermore, we identify potential research and development directions for
improving PEFTs in the future. This survey provides a valuable resource for
both newcomers and experts seeking to understand and use the power of PEFT
across FMs. All reviewed papers are listed at
\url{https://github.com/THUDM/Awesome-Parameter-Efficient-Fine-Tuning-for-Foundation-Models}.

æè¦ï¼éé èª¿æ¥æ·±å¥æ¢è¨åºç¤æ¨¡å (FM) ä¸­åæ¸ææå¾®èª¿ (PEFT) çé åãPEFT æ¯ä¸ç¨®å·ææ¬æççå¾®èª¿æè¡ï¼å¯æå°ååæ¸åè¨ç®è¤éåº¦ï¼åæåªåå¯¦ç¾æä½³çä¸æ¸¸ä»»åæè½ãå ChatGPTãDALL-E å LLaVA ç­ FM å°ç²¾æ¼èªè¨çè§£ãçæä»»ååå¤æ¨¡æä»»åï¼ä¸¦éå°æ¶µèæå­ãå½±ååå½±ççåç¨®è³æéé²è¡è¨ç·´ãFM çå¤æ¨£æ§å¼å°äº PEFT çåç¨®é©æç­ç¥ãå æ­¤ï¼æ¬èª¿æ¥æ¨å¨å°æç¨æ¼åç¨® FM ç PEFT æè¡æä¾å¨é¢çæ¦è§ï¼ä¸¦è§£æ±ºå¨çè§£æè¡ãè¶¨å¢åæç¨æ¹é¢çééµå·®è·ãæåå¾æä¾ FM å PEFT çè©³ç´°éç¼éå§ãé¨å¾ï¼æåç³»çµ±æ§å°æª¢è¦åç¨® FM ä¸­ PEFT çä¸»è¦é¡å¥åæ ¸å¿æ©å¶ï¼ä»¥æä¾å°è¶¨å¢çå¨é¢çè§£ãæåéæ¢è¨äºåç¨® FM ä¸­ææ°çæç¨ï¼ä»¥å±ç¤º PEFT çå¤åè½æ§ï¼é¡æç³»çµ±æ§ PEFT æ¹æ³èåç¨® FM æ´åçåªé»ãæ­¤å¤ï¼æåæ¾åºæ½å¨çç ç©¶åéç¼æ¹åï¼ä»¥æ¹åæªä¾ PEFTãæ¬èª¿æ¥çºå°æ±äºè§£åä½¿ç¨ FM ä¸­ PEFT å¨åçæ°æåå°å®¶æä¾äºå¯¶è²´çè³æºãæææª¢è¦éçè«æé½åå¨
\url{https://github.com/THUDM/Awesome-Parameter-Efficient-Fine-Tuning-for-Foundation-Models}ã

##### **Not Every AI Problem is a Data Problem: We Should Be Intentional About Data Scaling**
2501.13779v1 by Tanya Rodchenko, Natasha Noy, Nino Scherrer, Jennifer Prendki

While Large Language Models require more and more data to train and scale,
rather than looking for any data to acquire, we should consider what types of
tasks are more likely to benefit from data scaling. We should be intentional in
our data acquisition. We argue that the topology of data itself informs which
tasks to prioritize in data scaling, and shapes the development of the next
generation of compute paradigms for tasks where data scaling is inefficient, or
even insufficient.

æè¦ï¼åç®¡å¤§åèªè¨æ¨¡åéè¦è¶ä¾è¶å¤çè³æä¾è¨ç·´åæ´å±ï¼
èå¶å°æ¾ä»»ä½è³æä¾åå¾ï¼æåæè©²èæ®åªç¨®é¡åç
ä»»åæ´æå¯è½å¾è³ææ´å±ä¸­åçãæåæè©²å¨
æåçè³æåå¾ä¸­æ¯ææçãæåèªçºè³ææ¬èº«çææ²çµæ§æåç¥
åªäºä»»åå¨è³ææ´å±ä¸­åªåèçï¼ä¸¦å¡é ä¸ä¸
ä»£è¨ç®ç¯ä¾çç¼å±ï¼å çºå¨è³ææ´å±æçä½ä¸çä»»åä¸­ï¼
çè³æ¯ä¸è¶³å¤ çã

##### **Explainable XR: Understanding User Behaviors of XR Environments using LLM-assisted Analytics Framework**
2501.13778v1 by Yoonsang Kim, Zainab Aamir, Mithilesh Singh, Saeed Boorboor, Klaus Mueller, Arie E. Kaufman

We present Explainable XR, an end-to-end framework for analyzing user
behavior in diverse eXtended Reality (XR) environments by leveraging Large
Language Models (LLMs) for data interpretation assistance. Existing XR user
analytics frameworks face challenges in handling cross-virtuality - AR, VR, MR
- transitions, multi-user collaborative application scenarios, and the
complexity of multimodal data. Explainable XR addresses these challenges by
providing a virtuality-agnostic solution for the collection, analysis, and
visualization of immersive sessions. We propose three main components in our
framework: (1) A novel user data recording schema, called User Action
Descriptor (UAD), that can capture the users' multimodal actions, along with
their intents and the contexts; (2) a platform-agnostic XR session recorder,
and (3) a visual analytics interface that offers LLM-assisted insights tailored
to the analysts' perspectives, facilitating the exploration and analysis of the
recorded XR session data. We demonstrate the versatility of Explainable XR by
demonstrating five use-case scenarios, in both individual and collaborative XR
applications across virtualities. Our technical evaluation and user studies
show that Explainable XR provides a highly usable analytics solution for
understanding user actions and delivering multifaceted, actionable insights
into user behaviors in immersive environments.

æè¦ï¼<paragraph>æåæåº Explainable XRï¼éæ¯ä¸åç«¯å°ç«¯æ¶æ§ï¼ç¨æ¼åæä½¿ç¨èå¨åç¨®æ´å¢å¯¦å¢ (XR) ç°å¢ä¸­çè¡çºï¼æ¹æ³æ¯å©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾åå©è³æè§£è®ãç¾æç XR ä½¿ç¨èåææ¶æ§å¨èçè·¨èæ¬åææéå°ææ°ï¼ä¾å¦ ARãVRãMR è½æãå¤ä½¿ç¨èåä½æç¨ç¨å¼å ´æ¯ï¼ä»¥åå¤æ¨¡æè³æçè¤éæ§ãExplainable XR ééæä¾ä¸åèèæ¬åç¡éçè§£æ±ºæ¹æ¡ä¾æ¶éãåæåè¦è¦ºåæ²æµ¸å¼æè©±ï¼é²èè§£æ±ºéäºææ°ãæåå¨æ¶æ§ä¸­æåºä¸åä¸»è¦åä»¶ï¼(1) ä¸ç¨®ç¨±çºä½¿ç¨èåä½æè¿°ç¬¦ (UAD) çåµæ°ä½¿ç¨èè³æè¨éæ¶æ§ï¼å¯ä»¥æ·åä½¿ç¨èçå¤æ¨¡æåä½ï¼ä»¥åä»åçæååèçµ¡ï¼(2) ä¸åèå¹³å°ç¡éç XR æè©±è¨éå¨ï¼ä»¥å (3) ä¸åè¦è¦ºåæä»é¢ï¼æä¾éå°åæå¸«è§é»éèº«æé ç LLM åå©è¦è§£ï¼åå©æ¢ç´¢ååæè¨éç XR æè©±è³æãæåééå±ç¤ºäºç¨®ä½¿ç¨æ¡ä¾æå¢ï¼å¨è·¨èæ¬åçåäººååä½å¼ XR æç¨ç¨å¼ä¸­ï¼ä¾å±ç¤º Explainable XR çå¤åè½æ§ãæåçæè¡è©ä¼°åä½¿ç¨èç ç©¶é¡¯ç¤ºï¼Explainable XR æä¾äºä¸åé«åº¦å¯ç¨çåæè§£æ±ºæ¹æ¡ï¼ç¨æ¼äºè§£ä½¿ç¨èåä½ï¼ä¸¦æä¾å¤é¢åä¸å¯æä½çè¦è§£ï¼ä»¥äºè§£æ²æµ¸å¼ç°å¢ä¸­çä½¿ç¨èè¡çºã</paragraph>

##### **Do Large Language Models Truly Understand Geometric Structures?**
2501.13773v1 by Xiaofeng Wang, Yiming Wang, Wenhong Zhu, Rui Wang

Geometric ability is a significant challenge for large language models (LLMs)
due to the need for advanced spatial comprehension and abstract thinking.
Existing datasets primarily evaluate LLMs on their final answers, but they
cannot truly measure their true understanding of geometric structures, as LLMs
can arrive at correct answers by coincidence. To fill this gap, we introduce
the GeomRel dataset, designed to evaluate LLMs' understanding of geometric
structures by isolating the core step of geometric relationship identification
in problem-solving. Using this benchmark, we conduct thorough evaluations of
diverse LLMs and identify key limitations in understanding geometric
structures. We further propose the Geometry Chain-of-Thought (GeoCoT) method,
which enhances LLMs' ability to identify geometric relationships, resulting in
significant performance improvements.

æè¦ï¼å¹¾ä½è½åå°æ¼å¤§åèªè¨æ¨¡å (LLM) ä¾èªªæ¯ä¸é éå¤§çææ°ï¼å çºéè¦é²éçç©ºéçè§£åæ½è±¡æèãç¾æçè³æéä¸»è¦æ ¹æ LLM çæçµç­æ¡ä¾è©ä¼°å®åï¼ä½å®åç¡æ³çæ­£è¡¡é LLM å°å¹¾ä½çµæ§ççæ­£çè§£ï¼å çº LLM å¯ä»¥ééå·§åå¾åºæ­£ç¢ºç­æ¡ãçºäºå¡«è£éåç¼ºå£ï¼æåå¼å¥äº GeomRel è³æéï¼æ¨å¨ééå¨åé¡è§£æ±ºä¸­åé¢å¹¾ä½éä¿è­å¥çæ ¸å¿æ­¥é©ä¾è©ä¼° LLM å°å¹¾ä½çµæ§ççè§£ãä½¿ç¨æ­¤åºæºï¼æåå°åç¨® LLM é²è¡äºå¾¹åºçè©ä¼°ï¼ä¸¦æ¾åºçè§£å¹¾ä½çµæ§çä¸»è¦éå¶ãæåé²ä¸æ­¥æåºäºå¹¾ä½æèé (GeoCoT) æ¹æ³ï¼å¢å¼·äº LLM è­å¥å¹¾ä½éä¿çè½åï¼å¾èé¡¯èæåäºæè½ã

##### **Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak**
2501.13772v1 by Erjia Xiao, Hao Cheng, Jing Shao, Jinhao Duan, Kaidi Xu, Le Yang, Jindong Gu, Renjing Xu

Large Language Models (LLMs) demonstrate remarkable zero-shot performance
across various natural language processing tasks. The integration of multimodal
encoders extends their capabilities, enabling the development of Multimodal
Large Language Models that process vision, audio, and text. However, these
capabilities also raise significant security concerns, as these models can be
manipulated to generate harmful or inappropriate content through jailbreak.
While extensive research explores the impact of modality-specific input edits
on text-based LLMs and Large Vision-Language Models in jailbreak, the effects
of audio-specific edits on Large Audio-Language Models (LALMs) remain
underexplored. Hence, this paper addresses this gap by investigating how
audio-specific edits influence LALMs inference regarding jailbreak. We
introduce the Audio Editing Toolbox (AET), which enables audio-modality edits
such as tone adjustment, word emphasis, and noise injection, and the Edited
Audio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also
conduct extensive evaluations of state-of-the-art LALMs to assess their
robustness under different audio edits. This work lays the groundwork for
future explorations on audio-modality interactions in LALMs security.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨åç¨®èªç¶èªè¨èçä»»åä¸­å±ç¾äºéå¡çé¶æ¬¡å­¸ç¿è¡¨ç¾ãå¤æ¨¡æç·¨ç¢¼å¨çæ´åæ´å±äºå®åçè½åï¼ä¿æäºèçè¦è¦ºãé³è¨åæå­çå¤æ¨¡æå¤§åèªè¨æ¨¡åçéç¼ãç¶èï¼éäºè½åä¹å¼ç¼äºéå¤§çå®å¨æ§çæ®ï¼å çºéäºæ¨¡åå¯ä»¥ééè¶çæç¸±ä¾ç¢çæå®³æä¸é©ç¶çå§å®¹ãéç¶å»£æ³çç ç©¶æ¢ç´¢äºç¹å®æ¼æ¨¡æçè¼¸å¥ç·¨è¼¯å°è¶çä¸­çåºæ¼æå­ç LLM åå¤§åè¦è¦ºèªè¨æ¨¡åçå½±é¿ï¼ä½ç¹å®æ¼é³è¨çç·¨è¼¯å°å¤§åé³è¨èªè¨æ¨¡å (LALM) çå½±é¿ä»æªååæ¢è¨ãå æ­¤ï¼æ¬æééæ¢è¨ç¹å®æ¼é³è¨çç·¨è¼¯å¦ä½å½±é¿ LALM å°è¶ççæ¨è«ä¾è§£æ±ºéåå·®è·ãæåå¼å¥äºé³è¨ç·¨è¼¯å·¥å·ç®± (AET)ï¼å®æ¯æ´é³è¨æ¨¡æç·¨è¼¯ï¼ä¾å¦é³èª¿èª¿æ´ãå­è©å¼·èª¿åéè¨æ³¨å¥ï¼ä»¥åç·¨è¼¯éçé³è¨è³æé (EAD)ï¼ä¸åå¨é¢çé³è¨è¶çåºæºãæåä¹å°æåé²ç LALM é²è¡å»£æ³çè©ä¼°ï¼ä»¥è©ä¼°å®åå¨ä¸åé³è¨ç·¨è¼¯ä¸çç©©å¥æ§ãéé å·¥ä½çºæªä¾æ¢ç´¢ LALM å®å¨æ§ä¸­é³è¨æ¨¡æäºåå¥ å®äºåºç¤ã

##### **UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level Mathematical Reasoning with Large Language Models**
2501.13766v1 by Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, Can Yang

Large Language Models (LLMs) have made significant strides in mathematical
reasoning, underscoring the need for a comprehensive and fair evaluation of
their capabilities. However, existing benchmarks often fall short, either
lacking extensive coverage of undergraduate-level mathematical problems or
probably suffering from test-set contamination. To address these issues, we
introduce UGMathBench, a diverse and dynamic benchmark specifically designed
for evaluating undergraduate-level mathematical reasoning with LLMs.
UGMathBench comprises 5,062 problems across 16 subjects and 111 topics,
featuring 10 distinct answer types. Each problem includes three randomized
versions, with additional versions planned for release as leading open-source
LLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:
effective accuracy (EAcc), which measures the percentage of correctly solved
problems across all three versions, and reasoning gap ($\Delta$), which
assesses reasoning robustness by calculating the difference between the average
accuracy across all versions and EAcc. Our extensive evaluation of 23 leading
LLMs reveals that the highest EAcc achieved is 56.3\% by OpenAI-o1-mini, with
large $\Delta$ values observed across different models. This highlights the
need for future research aimed at developing "large reasoning models" with high
EAcc and $\Delta = 0$. We anticipate that the release of UGMathBench, along
with its detailed evaluation codes, will serve as a valuable resource to
advance the development of LLMs in solving mathematical problems.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨æ¸å­¸æ¨çæ¹é¢åå¾äºéå¤§é²å±ï¼éå¸é¡¯äºå°å¶è½åé²è¡å¨é¢ä¸å¬å¹³è©ä¼°çéæ±ãç¶èï¼ç¾æçåºæºå¾å¾ä¸å¤ ï¼è¦ä¹ç¼ºä¹å°å¤§å­¸ç¨åº¦æ¸å­¸åé¡çå»£æ³æ¶µèï¼è¦ä¹å¯è½é­åæ¸¬è©¦éæ±¡æãçºäºè§£æ±ºéäºåé¡ï¼æåå¼å¥äº UGMathBenchï¼éæ¯ä¸åå°éè¨­è¨ç¨æ¼è©ä¼° LLM çå¤§å­¸ç¨åº¦æ¸å­¸æ¨ççå¤æ¨£ååæåºæºãUGMathBench åå« 16 åç§ç®å 111 åä¸»é¡ç 5,062 ååé¡ï¼å·æ 10 ç¨®ä¸åçç­æ¡é¡åãæ¯ååé¡åæ¬ä¸åé¨æ©çæ¬ï¼ä¸¦è¨åé¨èé åçéæº LLM å¨ UGMathBench ä¸­é£½åèç¼å¸å¶ä»çæ¬ãæ­¤å¤ï¼æåæåºäºå©åééµææ¨ï¼æææºç¢ºç (EAcc)ï¼å®è¡¡éå¨ææä¸åçæ¬ä¸­æ­£ç¢ºè§£æ±ºåé¡çç¾åæ¯ï¼ä»¥åæ¨çå·®è·ï¼$\Delta$ï¼ï¼å®ééè¨ç®ææçæ¬ä¸­çå¹³åæºç¢ºçå EAcc ä¹éçå·®ç°ä¾è©ä¼°æ¨çç©©å¥æ§ãæåå° 23 åé åç LLM çå»£æ³è©ä¼°è¡¨æï¼OpenAI-o1-mini éå°çæé« EAcc çº 56.3%ï¼å¨ä¸åçæ¨¡åä¸­è§å¯å°è¼å¤§ç $\Delta$ å¼ãéå¸é¡¯äºæªä¾ç ç©¶çå¿è¦æ§ï¼æ¨å¨éç¼å·æé« EAcc å $\Delta = 0$ çãå¤§åæ¨çæ¨¡åããæåé è¨ UGMathBench çç¼å¸åå¶è©³ç´°çè©ä¼°ä»£ç¢¼å°æçºæ¨é² LLM å¨è§£æ±ºæ¸å­¸åé¡æ¹é¢çç¼å±çå¯¶è²´è³æºã

##### **2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings**
2501.13758v1 by Yumeng Wang, Ziran Zhou, Junjin Wang

Effective sentence embeddings that capture semantic nuances and generalize
well across diverse contexts are crucial for natural language processing tasks.
We address this challenge by applying SimCSE (Simple Contrastive Learning of
Sentence Embeddings) using contrastive learning to fine-tune the minBERT model
for sentiment analysis, semantic textual similarity (STS), and paraphrase
detection. Our contributions include experimenting with three different dropout
techniques, namely standard dropout, curriculum dropout, and adaptive dropout,
to tackle overfitting, proposing a novel 2-Tier SimCSE Fine-tuning Model that
combines both unsupervised and supervised SimCSE on STS task, and exploring
transfer learning potential for Paraphrase and SST tasks. Our findings
demonstrate the effectiveness of SimCSE, with the 2-Tier model achieving
superior performance on the STS task, with an average test score of 0.742
across all three downstream tasks. The results of error analysis reveals
challenges in handling complex sentiments and reliance on lexical overlap for
paraphrase detection, highlighting areas for future research. The ablation
study revealed that removing Adaptive Dropout in the Single-Task Unsupervised
SimCSE Model led to improved performance on the STS task, indicating
overfitting due to added parameters. Transfer learning from SimCSE models on
Paraphrase and SST tasks did not enhance performance, suggesting limited
transferability of knowledge from the STS task.

æè¦ï¼ææçå¥å­åµå¥å¯ä»¥ææè¯­ä¹ç»å¾®å·®å«ï¼å¹¶å¨ä¸åçè¯­å¢ä¸­å¾å¥½å°æ¦æ¬ï¼è¿å¯¹èªç¶è¯­è¨å¤çä»»å¡è³å³éè¦ã
æä»¬éè¿åºç¨ SimCSEï¼å¥å­åµå¥çç®åå¯¹æ¯å­¦ä¹ ï¼æ¥è§£å³è¿ä¸ææï¼ä½¿ç¨å¯¹æ¯å­¦ä¹ å¯¹ minBERT æ¨¡åè¿è¡å¾®è°ï¼ä»¥è¿è¡ææåæãè¯­ä¹ææ¬ç¸ä¼¼æ§ (STS) åéä¹æ£æµãæä»¬çè´¡ç®åæ¬å°è¯ä½¿ç¨ä¸ç§ä¸åç dropout ææ¯ï¼å³æ å dropoutãè¯¾ç¨ dropout åèªéåº dropoutï¼ä»¥è§£å³è¿åº¦æåé®é¢ï¼æåºäºä¸ç§æ°é¢ç 2 å± SimCSE å¾®è°æ¨¡åï¼è¯¥æ¨¡åç»åäº STS ä»»å¡ä¸­çæ çç£åæçç£ SimCSEï¼å¹¶æ¢ç´¢äºéä¹å SST ä»»å¡çè¿ç§»å­¦ä¹ æ½åãæä»¬çç ç©¶ç»æè¯æäº SimCSE çæææ§ï¼2 å±æ¨¡åå¨ STS ä»»å¡ä¸åå¾äºåè¶çæ§è½ï¼å¨ææä¸ä¸ªä¸æ¸¸ä»»å¡ä¸­çå¹³åæµè¯å¾åä¸º 0.742ãéè¯¯åæçç»ææ­ç¤ºäºå¨å¤çå¤æææåä¾èµè¯æ³éå è¿è¡éä¹æ£æµæ¹é¢çææï¼çªåºäºæªæ¥ç ç©¶çé¢åãæ¶èç ç©¶è¡¨æï¼å¨åä»»å¡æ çç£ SimCSE æ¨¡åä¸­ç§»é¤èªéåº Dropout ä¼æé« STS ä»»å¡çæ§è½ï¼è¿è¡¨æç±äºæ·»å äºåæ°èå¯¼è´è¿åº¦æåãä» SimCSE æ¨¡åå°éä¹å SST ä»»å¡çè¿ç§»å­¦ä¹ å¹¶æ²¡ææé«æ§è½ï¼è¿è¡¨æä» STS ä»»å¡ä¸­è·åç¥è¯çå¯è½¬ç§»æ§æéã

##### **Solving the long-tailed distribution problem by exploiting the synergies and balance of different techniques**
2501.13756v1 by Ziheng Wang, Toni Lassila, Sharib Ali

In real-world data, long-tailed data distribution is common, making it
challenging for models trained on empirical risk minimisation to learn and
classify tail classes effectively. While many studies have sought to improve
long tail recognition by altering the data distribution in the feature space
and adjusting model decision boundaries, research on the synergy and corrective
approach among various methods is limited. Our study delves into three
long-tail recognition techniques: Supervised Contrastive Learning (SCL),
Rare-Class Sample Generator (RSG), and Label-Distribution-Aware Margin Loss
(LDAM). SCL enhances intra-class clusters based on feature similarity and
promotes clear inter-class separability but tends to favour dominant classes
only. When RSG is integrated into the model, we observed that the intra-class
features further cluster towards the class centre, which demonstrates a
synergistic effect together with SCL's principle of enhancing intra-class
clustering. RSG generates new tail features and compensates for the tail
feature space squeezed by SCL. Similarly, LDAM is known to introduce a larger
margin specifically for tail classes; we demonstrate that LDAM further bolsters
the model's performance on tail classes when combined with the more explicit
decision boundaries achieved by SCL and RSG. Furthermore, SCL can compensate
for the dominant class accuracy sacrificed by RSG and LDAM. Our research
emphasises the synergy and balance among the three techniques, with each
amplifying the strengths of the others and mitigating their shortcomings. Our
experiment on long-tailed distribution datasets, using an end-to-end
architecture, yields competitive results by enhancing tail class accuracy
without compromising dominant class performance, achieving a balanced
improvement across all classes.

æè¦ï¼å¨çå¯¦ä¸ççè³æä¸­ï¼é·å°¾è³æåä½å¾å¸¸è¦ï¼éä½¿å¾å¨ç¶é©é¢¨éªæå°åè¨ç·´çæ¨¡åé£ä»¥ææå°å­¸ç¿ååé¡å°¾é¨é¡å¥ãåç®¡è¨±å¤ç ç©¶è©¦åééæ¹è®ç¹å¾µç©ºéä¸­çè³æåä½åèª¿æ´æ¨¡åæ±ºç­éçä¾æ¹åé·å°¾è­å¥ï¼ä½å°åç¨®æ¹æ³ä¹éçååææåç³¾æ­£æ¹æ³çç ç©¶å»å¾æéãæåçç ç©¶æ·±å¥æ¢è¨äºä¸ç¨®é·å°¾è­å¥æè¡ï¼ç£ç£å°æ¯å­¸ç¿ (SCL)ãç¨æé¡å¥æ¨£æ¬çæå¨ (RSG) åæ¨ç±¤åä½æç¥éçæå¤± (LDAM)ãSCL åºæ¼ç¹å¾µç¸ä¼¼æ§å¢å¼·é¡å§ç°ï¼ä¸¦ä¿é²æ¸æ°çé¡éå¯åé¢æ§ï¼ä½å¾åæ¼åªåå¥½ä¸»å°é¡å¥ãç¶ RSG éæå°æ¨¡åä¸­æï¼æåè§å¯å°é¡å§ç¹å¾µé²ä¸æ­¥åé¡ä¸­å¿èéï¼éè SCL å¢å¼·é¡å§èé¡çåçä¸èµ·å±ç¤ºäºååææãRSG çæäºæ°çå°¾é¨ç¹å¾µï¼ä¸¦å½è£äºè¢« SCL å£ç¸®çå°¾é¨ç¹å¾µç©ºéãé¡ä¼¼å°ï¼å·²ç¥ LDAM å°éçºå°¾é¨é¡å¥å¼å¥äºæ´å¤§çéçï¼æåè­æï¼LDAM å¨è SCL å RSG å¯¦ç¾çæ´æç¢ºçæ±ºç­éçç¸çµåæï¼é²ä¸æ­¥æåäºæ¨¡åå¨å°¾é¨é¡å¥ä¸çæè½ãæ­¤å¤ï¼SCL å¯ä»¥å½è£ RSG å LDAM ç§ç²çä¸»å°é¡å¥æºç¢ºæ§ãæåçç ç©¶å¼·èª¿äºéä¸ç¨®æè¡ä¹éçååææåå¹³è¡¡ï¼æ¯ç¨®æè¡é½æ¾å¤§äºå¶ä»æè¡çåªå¢ä¸¦æ¸è¼äºå®åçç¼ºé»ãæåä½¿ç¨ç«¯å°ç«¯æ¶æ§å°é·å°¾åä½è³æéé²è¡çå¯¦é©ï¼ééæé«å°¾é¨é¡å¥æºç¢ºæ§èä¸æå®³ä¸»å°é¡å¥æè½ï¼å¨ææé¡å¥ä¸­å¯¦ç¾äºå¹³è¡¡çæ¹é²ï¼å¾èç¢çäºæç«¶ç­åççµæã

##### **EICopilot: Search and Explore Enterprise Information over Large-scale Knowledge Graphs with LLM-driven Agents**
2501.13746v1 by Yuhui Yun, Huilong Ye, Xinru Li, Ruojia Li, Jingfeng Deng, Li Li, Haoyi Xiong

The paper introduces EICopilot, an novel agent-based solution enhancing
search and exploration of enterprise registration data within extensive online
knowledge graphs like those detailing legal entities, registered capital, and
major shareholders. Traditional methods necessitate text-based queries and
manual subgraph explorations, often resulting in time-consuming processes.
EICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this
landscape by utilizing Large Language Models (LLMs) to interpret natural
language queries. This solution automatically generates and executes Gremlin
scripts, providing efficient summaries of complex enterprise relationships.
Distinct feature a data pre-processing pipeline that compiles and annotates
representative queries into a vector database of examples for In-context
learning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought
with ICL to enhance Gremlin script generation for knowledge graph search and
exploration, and a novel query masking strategy that improves intent
recognition for heightened script accuracy. Empirical evaluations demonstrate
the superior performance of EICopilot, including speed and accuracy, over
baseline methods, with the \emph{Full Mask} variant achieving a syntax error
rate reduction to as low as 10.00% and an execution correctness of up to
82.14%. These components collectively contribute to superior querying
capabilities and summarization of intricate datasets, positioning EICopilot as
a groundbreaking tool in the exploration and exploitation of large-scale
knowledge graphs for enterprise information search.

æè¦ï¼æ¬æä»ç´¹äº EICopilotï¼éæ¯ä¸ç¨®åºæ¼ä»£ççæ°åè§£æ±ºæ¹æ¡ï¼å¯å¢å¼·å¨å»£æ³çç·ä¸ç¥è­åè­ä¸­æå°åæ¢ç´¢ä¼æ¥­è¨»åè³æï¼ä¾å¦è©³ç´°èªªææ³å¾å¯¦é«ãè¨»åè³æ¬åä¸»è¦è¡æ±çè³æãå³çµ±æ¹æ³éè¦åºæ¼æå­çæ¥è©¢åæåå­åæ¢ç´¢ï¼éå¸¸æå°è´èæçæµç¨ãEICopilot é¨ç½²çºç¾åº¦ä¼æ¥­æå°çèå¤©æ©å¨äººï¼ééå©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾è©®éèªç¶èªè¨æ¥è©¢ï¼é²èæ¹åéé æè¡ãæ­¤è§£æ±ºæ¹æ¡æèªåç¢çä¸¦å·è¡ Gremlin è³æ¬ï¼æä¾è¤éä¼æ¥­éä¿çæææè¦ãå¶ç¨ç¹åè½çºè³æåèçç®¡ç·ï¼å¯å°å·ä»£è¡¨æ§çæ¥è©¢ç·¨è­¯ä¸¦è¨»è§£å°ç¯ä¾çåéè³æåº«ä¸­ï¼ä»¥é²è¡èçµ¡ä¸­å­¸ç¿ (ICL)ï¼éæ¯ä¸åçµåäºæèéè ICL çç¶åæ¨çç®¡ç·ï¼ç¨æ¼å¢å¼· Gremlin è³æ¬ç¢çï¼ä»¥é²è¡ç¥è­åè­æå°åæ¢ç´¢ï¼ä»¥åä¸ç¨®æ°ç©çæ¥è©¢é®ç½©ç­ç¥ï¼å¯æ¹åæåè¾¨è­ï¼é²èæé«è³æ¬æºç¢ºåº¦ãå¯¦è­è©ä¼°é¡¯ç¤ºï¼EICopilot çæè½åªæ¼åºç·æ¹æ³ï¼åæ¬éåº¦åæºç¢ºåº¦ï¼å¶ä¸­ãå®æ´é®ç½©ãè®é«å°èªæ³é¯èª¤çéä½è³ä½æ¼ 10.00%ï¼å·è¡æ­£ç¢ºçé«é 82.14%ãéäºåä»¶å±åä¿æäºåªç°çæ¥è©¢åè½åè¤éè³æéçæè¦ï¼å° EICopilot å®ä½çºæ¢ç´¢åå©ç¨å¤§è¦æ¨¡ç¥è­åè­é²è¡ä¼æ¥­è³è¨æå°çåµæ°å·¥å·ã

##### **A Study of the Plausibility of Attention between RNN Encoders in Natural Language Inference**
2501.13735v1 by Duc Hau Nguyen, Duc Hau Nguyen, Pascale SÃ©billot

Attention maps in neural models for NLP are appealing to explain the decision
made by a model, hopefully emphasizing words that justify the decision. While
many empirical studies hint that attention maps can provide such justification
from the analysis of sound examples, only a few assess the plausibility of
explanations based on attention maps, i.e., the usefulness of attention maps
for humans to understand the decision. These studies furthermore focus on text
classification. In this paper, we report on a preliminary assessment of
attention maps in a sentence comparison task, namely natural language
inference. We compare the cross-attention weights between two RNN encoders with
human-based and heuristic-based annotations on the eSNLI corpus. We show that
the heuristic reasonably correlates with human annotations and can thus
facilitate evaluation of plausible explanations in sentence comparison tasks.
Raw attention weights however remain only loosely related to a plausible
explanation.

æè¦ï¼ç¥ç¶ç¶²è·¯èªè¨æ¨¡åä¸­çæ³¨æååå¯ä»¥ç¨ä¾è§£éæ¨¡åååºçæ±ºç­ï¼å¸æå¼·èª¿ç¨ä¾è­ææ±ºç­çè©å½ãéç¶è¨±å¤å¯¦è­ç ç©¶æç¤ºæ³¨æååå¯ä»¥æä¾éç¨®è­æï¼ä½åªæå°æ¸ç ç©¶è©ä¼°åºæ¼æ³¨æååçè§£éçåçæ§ï¼ä¹å°±æ¯èªªï¼æ³¨æååå°äººé¡çè§£æ±ºç­çæç¨ãéäºç ç©¶é²ä¸æ­¥éæ³¨æ¼æå­åé¡ãå¨æ¬æä¸­ï¼æåå ±åäºå¨å¥å­æ¯è¼ä»»åï¼å³èªç¶èªè¨æ¨è«ï¼ä¸­å°æ³¨æååçåæ­¥è©ä¼°ãæåå°å©å RNN ç·¨ç¢¼å¨ä¹éçäº¤åæ³¨æåæ¬éèäººé¡æ¨è¨»å eSNLI èªæåº«ä¸çåç¼å¼æ¨è¨»é²è¡æ¯è¼ãæåè¡¨æåç¼å¼èäººé¡æ¨è¨»åçç¸éï¼å æ­¤å¯ä»¥ä¿é²å¥å­æ¯è¼ä»»åä¸­åçè§£éçè©ä¼°ãç¶èï¼åå§æ³¨æåæ¬éä»ç¶åªèåççè§£éé¬æ£ç¸éã

##### **Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational Tasks**
2501.13731v1 by Chang Gong, Wanrui Bian, Zhijie Zhang, Weiguo Zheng

Graph computational tasks are inherently challenging and often demand the
development of advanced algorithms for effective solutions. With the emergence
of large language models (LLMs), researchers have begun investigating their
potential to address these tasks. However, existing approaches are constrained
by LLMs' limited capability to comprehend complex graph structures and their
high inference costs, rendering them impractical for handling large-scale
graphs. Inspired by human approaches to graph problems, we introduce a novel
framework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for Graph
Computational Tasks), which consists of three key steps: problem understanding,
prompt design, and code generation. In this framework, LLMs are tasked with
understanding the problem and extracting relevant information to generate
correct code. The responsibility for analyzing the graph structure and
executing the code is delegated to the interpreter. We inject task-related
pseudocodes into the prompts to further assist the LLMs in generating efficient
code. We also employ cost-effective trial-and-error techniques to ensure that
the LLM-generated code executes correctly. Unlike other methods that require
invoking LLMs for each individual test case, PIE only calls the LLM during the
code generation phase, allowing the generated code to be reused and
significantly reducing inference costs. Extensive experiments demonstrate that
PIE outperforms existing baselines in terms of both accuracy and computational
efficiency.

æè¦ï¼åè¡¨è¨ç®ä»»åæ¬è³ªä¸å·æææ°æ§ï¼èä¸éå¸¸éè¦éç¼åé²çæ¼ç®æ³æè½ææè§£æ±ºãé¨èå¤§åèªè¨æ¨¡å (LLM) çåºç¾ï¼ç ç©¶äººå¡å·²éå§æ¢è¨å¶è§£æ±ºéäºä»»åçå¯è½æ§ãç¶èï¼ç¾ææ¹æ³åå° LLM çè§£è¤éåå½¢çµæ§çè½åæéä»¥åå¶é«æ¨çææ¬çéå¶ï¼éä½¿å¾å®åä¸åå¯¦éå°èçå¤§è¦æ¨¡åå½¢ãåå°äººé¡è§£æ±ºåå½¢åé¡çæ¹æ³åç¼ï¼æåå¼å¥äº PIEï¼å½ä»£ç¢¼æ³¨å¥å¢å¼· LLM åå½¢è¨ç®ä»»åæ¨çï¼éåæ°æ¡æ¶ï¼å®åå«ä¸åééµæ­¥é©ï¼åé¡çè§£ãæç¤ºè¨­è¨åä»£ç¢¼çæãå¨æ­¤æ¡æ¶ä¸­ï¼LLM çä»»åæ¯çè§£åé¡ä¸¦æ·åç¸éè³è¨ä»¥ç¢çæ­£ç¢ºçä»£ç¢¼ãåæåå½¢çµæ§åå·è¡ä»£ç¢¼çè²¬ä»»å§æ´¾çµ¦è§£éå¨ãæåå°èä»»åç¸éçå½ä»£ç¢¼æ³¨å¥æç¤ºä¸­ï¼ä»¥é²ä¸æ­¥åå© LLM ç¢çææçä»£ç¢¼ãæåéæ¡ç¨å·æææ¬æççè©¦é¯æè¡ï¼ä»¥ç¢ºä¿ LLM çæçä»£ç¢¼æ­£ç¢ºå·è¡ãèéè¦çºæ¯ååå¥æ¸¬è©¦æ¡ä¾å¼å« LLM çå¶ä»æ¹æ³ä¸åï¼PIE åå¨ä»£ç¢¼ç¢çéæ®µå¼å« LLMï¼åè¨±éè¤ä½¿ç¨ç¢ççä»£ç¢¼ä¸¦å¤§å¹éä½æ¨çææ¬ãå¤§éçå¯¦é©è­æï¼PIE å¨æºç¢ºæ§åè¨ç®æçæ¹é¢é½åªæ¼ç¾æçåºæºã

##### **RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation**
2501.13726v1 by Shi-Qi Yan, Zhen-Hua Ling

While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing
external knowledge, its generation process heavily depends on the quality and
accuracy of the retrieved context. Large language models (LLMs) struggle to
evaluate the correctness of non-parametric knowledge retrieved externally when
it differs from internal memorization, leading to knowledge conflicts during
response generation. To this end, we introduce the Retrieval Preference
Optimization (RPO), a lightweight and effective alignment method to adaptively
leverage multi-source knowledge based on retrieval relevance. An implicit
representation of retrieval relevance is derived and incorporated into the
reward model to integrate retrieval evaluation and response generation into a
single model, solving the problem that previous methods necessitate the
additional procedure to assess the retrieval quality. Notably, RPO is the only
RAG-dedicated alignment approach that quantifies the awareness of retrieval
relevance in training, overcoming mathematical obstacles. Experiments on four
datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any
extra component, exhibiting its robust generalization.

æè¦ï¼æª¢ç´¢å¢å¼·çæï¼RAGï¼éç¶å¨å©ç¨å¤é¨ç¥è­æ¹é¢è¡¨ç¾åºåæ¯ï¼ä½å¶çæéç¨å´éä¾è³´æ¼æª¢ç´¢èªå¢çåè³ªåæºç¢ºæ§ãå¤§åèªè¨æ¨¡åï¼LLMï¼å¨è©ä¼°å¤é¨æª¢ç´¢çéåæ¸ç¥è­çæ­£ç¢ºæ§ææéå°å°é£ï¼ç¹å¥æ¯å¨å¶èå§é¨è¨æ¶ä¸åæï¼éæå¨åæçææéå°è´ç¥è­è¡çªãçºæ­¤ï¼æåå¼å¥äºæª¢ç´¢åå¥½æä½³åï¼RPOï¼ï¼éæ¯ä¸ç¨®è¼éä¸ææçæ¯å°æ¹æ³ï¼å¯æ ¹ææª¢ç´¢ç¸éæ§èªé©æå°å©ç¨å¤ä¾æºç¥è­ãæåè¡çäºä¸åæª¢ç´¢ç¸éæ§çé±å«è¡¨å¾µï¼ä¸¦å°å¶ç´å¥çåµæ¨¡åï¼ä»¥å°æª¢ç´¢è©ä¼°ååæçææ´åå°ä¸åæ¨¡åä¸­ï¼è§£æ±ºäºååæ¹æ³éè¦é¡å¤ç¨åºä¾è©ä¼°æª¢ç´¢åè³ªçåé¡ãå¼å¾æ³¨æçæ¯ï¼RPO æ¯å¯ä¸éåè¨ç·´ä¸­æª¢ç´¢ç¸éæ§èªç¥ç RAG å°ç¨æ¯å°æ¹æ³ï¼åæäºæ¸å­¸éç¤ãå¨ååè³æéä¸çå¯¦é©è¡¨æï¼RPO å¨æºç¢ºæ§ä¸æ¯ RAG é«åº 4-10%ï¼ä¸ç¡ä»»ä½é¡å¤åä»¶ï¼å±ç¾åºå¶ç©©å¥çæ³åæ§ã

##### **You Only Crash Once v2: Perceptually Consistent Strong Features for One-Stage Domain Adaptive Detection of Space Terrain**
2501.13725v1 by Timothy Chase Jr, Christopher Wilson, Karthik Dantu

The in-situ detection of planetary, lunar, and small-body surface terrain is
crucial for autonomous spacecraft applications, where learning-based computer
vision methods are increasingly employed to enable intelligence without prior
information or human intervention. However, many of these methods remain
computationally expensive for spacecraft processors and prevent real-time
operation. Training of such algorithms is additionally complex due to the
scarcity of labeled data and reliance on supervised learning approaches.
Unsupervised Domain Adaptation (UDA) offers a promising solution by
facilitating model training with disparate data sources such as simulations or
synthetic scenes, although UDA is difficult to apply to celestial environments
where challenging feature spaces are paramount. To alleviate such issues, You
Only Crash Once (YOCOv1) has studied the integration of Visual Similarity-based
Alignment (VSA) into lightweight one-stage object detection architectures to
improve space terrain UDA. Although proven effective, the approach faces
notable limitations, including performance degradations in multi-class and
high-altitude scenarios. Building upon the foundation of YOCOv1, we propose
novel additions to the VSA scheme that enhance terrain detection capabilities
under UDA, and our approach is evaluated across both simulated and real-world
data. Our second YOCO rendition, YOCOv2, is capable of achieving
state-of-the-art UDA performance on surface terrain detection, where we
showcase improvements upwards of 31% compared with YOCOv1 and terrestrial
state-of-the-art. We demonstrate the practical utility of YOCOv2 with
spacecraft flight hardware performance benchmarking and qualitative evaluation
of NASA mission data.

æè¦ï¼è¡æãæçåå°å¤©é«è¡¨é¢å°å½¢çåä½æ¢æ¸¬å°æ¼èªä¸»å¤ªç©ºè¹æç¨è³ééè¦ï¼å¶ä¸­åºæ¼å­¸ç¿çé»è¦è¦è¦ºæ¹æ³æ­£æ¥çå»£æ³å°ç¨æ¼å¨æ²æåé©è³è¨æäººå·¥å¹²é çææ³ä¸å¯¦ç¾æºæ§ãç¶èï¼å¶ä¸­è¨±å¤æ¹æ³å°æ¼å¤ªç©ºè¹èçå¨èè¨ä»ç¶è¨ç®ææ¬é«æï¼ä¸ç¡æ³é²è¡å³ææä½ãç±æ¼æ¨è¨è³æçç¨ç¼ºæ§åå°ç£ç£å¼å­¸ç¿æ¹æ³çä¾è³´æ§ï¼æ­¤é¡æ¼ç®æ³çè¨ç·´ä¹æ´å è¤éãéç£ç£åé©æ (UDA) æä¾äºä¸åæåæ¯çè§£æ±ºæ¹æ¡ï¼å®ä¿é²äºä½¿ç¨ä¸åè³æä¾æºï¼ä¾å¦æ¨¡æ¬æåæå ´æ¯ï¼é²è¡æ¨¡åè¨ç·´ï¼åç®¡ UDA é£ä»¥æç¨æ¼ä»¥å·æææ°æ§çç¹å¾µç©ºéçºä¸»çæé«ç°å¢ãçºäºç·©è§£æ­¤é¡åé¡ï¼æ¨åªå´©æ½°ä¸æ¬¡ (YOCOv1) ç ç©¶äºå°åºæ¼è¦è¦ºç¸ä¼¼æ§çå°é½ (VSA) æ´åå°è¼éç´ä¸éæ®µç©ä»¶åµæ¸¬æ¶æ§ä¸­ï¼ä»¥æ¹åå¤ªç©ºå°å½¢ UDAãåç®¡è¢«è­æææï¼ä½è©²æ¹æ³é¢è¨é¡¯èçéå¶ï¼åæ¬å¨å¤é¡åé«æµ·æå ´æ¯ä¸­çæè½ä¸éãå¨ YOCOv1 çåºç¤ä¸ï¼æåæåºå° VSA æ¹æ¡çæ°å¢åè½ï¼ä»¥å¢å¼· UDA ä¸çå°å½¢åµæ¸¬è½åï¼ä¸¦ä¸æåçåæ³å·²å¨æ¨¡æ¬åçå¯¦ä¸çè³æä¸­é²è¡è©ä¼°ãæåçç¬¬äºå YOCO çæ¬ YOCOv2 è½å¤ å¨è¡¨é¢å°å½¢åµæ¸¬ä¸å¯¦ç¾æåé²ç UDA æè½ï¼è YOCOv1 åé¸å°æåé²æè¡ç¸æ¯ï¼æåå±ç¤ºäºé«é 31% çé²æ­¥ãæåééå¤ªç©ºè¹é£è¡ç¡¬é«æè½åºæºæ¸¬è©¦å NASA ä»»åè³æçå®æ§è©ä¼°ï¼å±ç¤ºäº YOCOv2 çå¯¦ç¨æ§ã

##### **Musical ethnocentrism in Large Language Models**
2501.13720v1 by Anna Kruspe

Large Language Models (LLMs) reflect the biases in their training data and,
by extension, those of the people who created this training data. Detecting,
analyzing, and mitigating such biases is becoming a focus of research. One type
of bias that has been understudied so far are geocultural biases. Those can be
caused by an imbalance in the representation of different geographic regions
and cultures in the training data, but also by value judgments contained
therein. In this paper, we make a first step towards analyzing musical biases
in LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the
first, we prompt LLMs to provide lists of the "Top 100" musical contributors of
various categories and analyze their countries of origin. In the second
experiment, we ask the LLMs to numerically rate various aspects of the musical
cultures of different countries. Our results indicate a strong preference of
the LLMs for Western music cultures in both experiments.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) åæ å¶è¨ç·´è³æä¸­çåè¦ï¼é²ä¸æ­¥èè¨ï¼ä¹åæ äºå»ºç«éäºè¨ç·´è³æçäººçåè¦ãåµæ¸¬ãåæåæ¸è¼æ­¤é¡åè¦å·²æçºç ç©¶éé»ãå°ç®åçºæ­¢ï¼ä¸ç¨®ç ç©¶ä¸è¶³çåè¦é¡åæ¯å°çæååè¦ãéäºåè¦å¯è½æ¯ç±æ¼è¨ç·´è³æä¸­ä¸åå°çåååæåçä»£è¡¨æ§ä¸å¹³è¡¡ï¼ä¹å¯è½æ¯ç±æ¼å¶ä¸­åå«çå¹å¼å¤æ·æé æçãå¨æ¬æä¸­ï¼æåéåºäºåæ LLM ä¸­é³æ¨åè¦çç¬¬ä¸æ­¥ï¼ç¹å¥æ¯ ChatGPT å Mixtralãæåé²è¡äºå©åå¯¦é©ãå¨ç¬¬ä¸åå¯¦é©ä¸­ï¼æåæç¤º LLM æä¾åç¨®é¡å¥çãå 100 åãé³æ¨è²¢ç»èæ¸å®ï¼ä¸¦åæå¶åç±åãå¨ç¬¬äºåå¯¦é©ä¸­ï¼æåè¦æ± LLM å°ä¸ååå®¶çé³æ¨æåçååæ¹é¢é²è¡æ¸å­è©åãæåççµæè¡¨æï¼å¨å©åå¯¦é©ä¸­ï¼LLM é½å¼·çåå¥½è¥¿æ¹é³æ¨æåã

##### **Skin Disease Detection and Classification of Actinic Keratosis and Psoriasis Utilizing Deep Transfer Learning**
2501.13713v1 by Fahud Ahmmed, Md. Zaheer Raihan, Kamnur Nahar, D. M. Asadujjaman, Md. Mahfujur Rahman, Abdullah Tamim

Skin diseases can arise from infections, allergies, genetic factors,
autoimmune disorders, hormonal imbalances, or environmental triggers such as
sun damage and pollution. Some skin diseases, such as Actinic Keratosis and
Psoriasis, can be fatal if not treated in time. Early identification is
crucial, but the diagnostic methods for these conditions are often expensive
and not widely accessible. In this study, we propose a novel and efficient
method for diagnosing skin diseases using deep learning techniques. This
approach employs a modified VGG16 Convolutional Neural Network (CNN) model. The
model includes several convolutional layers and utilizes ImageNet weights with
modified top layers. The top layer is updated with fully connected layers and a
final softmax activation layer to classify skin diseases. The dataset used,
titled "Skin Disease Dataset," is publicly available. While the VGG16
architecture does not include data augmentation by default, preprocessing
techniques such as rotation, shifting, and zooming were applied to augment the
data prior to model training. The proposed methodology achieved 90.67% accuracy
using the modified VGG16 model, demonstrating its reliability in classifying
skin diseases. The promising results highlight the potential of this approach
for real-world applications.

æè¦ï¼ç®èç¾çå¯è½æºæ¼ææãéæãéºå³å ç´ ã
èªé«åç«ç¾çãè·ç¾èå¤±è¡¡æç°å¢èªå ï¼ä¾å¦
é½åå·å®³åæ±¡æãæäºç®èç¾çï¼ä¾å¦ååæ§è§åçå
çç®ç¬ï¼å¦ææ²æåææ²»çï¼å¯è½æè´å½ãæ©æç¼ç¾è³ééè¦ï¼ä½éäºç¾ççè¨ºæ·æ¹æ³éå¸¸å¾æè²´ï¼èä¸ç¡æ³å»£æ³ä½¿ç¨ãå¨æ¬ç ç©¶ä¸­ï¼æåæåºäºä¸ç¨®æ°ç©ä¸ææçæ¹æ³ï¼ä½¿ç¨æ·±åº¦å­¸ç¿æè¡è¨ºæ·ç®èç¾çãéç¨®
æ¹æ³æ¡ç¨äºä¿®æ¹å¾ç VGG16 å·ç©ç¥ç¶ç¶²è·¯ (CNN) æ¨¡åãè©²
æ¨¡ååå«å¤åå·ç©å±¤ï¼ä¸¦å©ç¨å·æä¿®æ¹å¾çé å±¤ç ImageNet æ¬éãé å±¤ä½¿ç¨å¨é£æ¥å±¤åæçµ softmax æ¿æ´»å±¤é²è¡æ´æ°ï¼ä»¥å°ç®èç¾çé²è¡åé¡ãæä½¿ç¨çåçºãç®èç¾çæ¸æéãçæ¸æéå¬éå¯ç¨ãéç¶ VGG16
æ¶æ§é è¨­ä¸åå«æ¸ææ´åï¼ä½æè½ãå¹³ç§»åç¸®æ¾ç­é èç
æè¡å·²æç¨æ¼å¨æ¨¡åè¨ç·´ä¹åæ´åæ¸æãææåºçæ¹æ³ä½¿ç¨ä¿®æ¹å¾ç VGG16 æ¨¡åéå°äº 90.67% çæºç¢ºåº¦ï¼è­æäºå¶å¨å°ç®èç¾çé²è¡åé¡æ¹é¢çå¯é æ§ãéäºæå¸æççµæçªé¡¯äºéç¨®æ¹æ³å¨å¯¦éæç¨ä¸­çæ½åã

##### **Formally Verified Neurosymbolic Trajectory Learning via Tensor-based Linear Temporal Logic on Finite Traces**
2501.13712v1 by Mark Chevallier, Filip Smola, Richard Schmoetten, Jacques D. Fleuriot

We present a novel formalisation of tensor semantics for linear temporal
logic on finite traces (LTLf), with formal proofs of correctness carried out in
the theorem prover Isabelle/HOL. We demonstrate that this formalisation can be
integrated into a neurosymbolic learning process by defining and verifying a
differentiable loss function for the LTLf constraints, and automatically
generating an implementation that integrates with PyTorch. We show that, by
using this loss, the process learns to satisfy pre-specified logical
constraints. Our approach offers a fully rigorous framework for constrained
training, eliminating many of the inherent risks of ad-hoc, manual
implementations of logical aspects directly in an "unsafe" programming language
such as Python, while retaining efficiency in implementation.

æè¦ï¼æåæåºæåºéè¼¯ä¸å¼µéèªæçå½¢å¼åæ°ç©æ¹æ³ï¼éå°æéè»è·¡ï¼LTLfï¼ï¼ä¸¦å¨å®çè­æå¨ Isabelle/HOL ä¸­å·è¡æ­£ç¢ºæ§å½¢å¼è­æãæåå±ç¤ºæ­¤å½¢å¼åæ¹æ³å¯æ´åè³ç¥ç¶ç¬¦èå­¸ç¿æµç¨ï¼æ¹æ³æ¯å®ç¾©ä¸¦é©è­ LTLf éå¶çå¾®åæå¤±å½æ¸ï¼ä¸¦èªåç¢çè PyTorch æ´åçå¯¦ä½ãæåé¡¯ç¤ºï¼ééä½¿ç¨æ­¤æå¤±ï¼æ­¤æµç¨å­¸ç¿æ»¿è¶³é åæå®çéè¼¯éå¶ãæåçåæ³æä¾åéè¨ç·´çå´æ ¼æ¶æ§ï¼æ¶é¤è¨±å¤ç´æ¥å¨ãä¸å®å¨ãçç¨å¼èªè¨ï¼ä¾å¦ Pythonï¼ä¸­è¨æãæåå¯¦ä½éè¼¯é¢åçåºæé¢¨éªï¼åæä¿çå¯¦ä½æçã

##### **YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised Re-ID**
2501.13710v1 by IÃ±aki Erregue, Kamal Nasrollahi, Sergio Escalera

We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)
solution that combines real-time object detection with self-supervised
Re-Identification (Re-ID). By incorporating a dedicated Re-ID branch into
YOLO11s, our model performs Joint Detection and Embedding (JDE), generating
appearance features for each detection. The Re-ID branch is trained in a fully
self-supervised setting while simultaneously training for detection,
eliminating the need for costly identity-labeled datasets. The triplet loss,
with hard positive and semi-hard negative mining strategies, is used for
learning discriminative embeddings. Data association is enhanced with a custom
tracking implementation that successfully integrates motion, appearance, and
location cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20
benchmarks, surpassing existing JDE methods in terms of FPS and using up to ten
times fewer parameters. Thus, making our method a highly attractive solution
for real-world applications.

æè¦ï¼æåå¼å¥äº YOLO11-JDEï¼éæ¯ä¸ç¨®å¿«éä¸æºç¢ºçå¤ç®æ¨è¿½è¹¤ (MOT) è§£å³æ¹æ¡ï¼å®çµåäºå³æç©ä»¶åµæ¸¬èèªæç£ç£çéæ°è¾¨è­ (Re-ID)ãééå°å°ç¨ç Re-ID åæ¯æ´åå° YOLO11sï¼æåçæ¨¡åå·è¡è¯ååµæ¸¬ååµå¥ (JDE)ï¼çºæ¯ååµæ¸¬ç¢çå¤è§ç¹å¾µãRe-ID åæ¯å¨å®å¨èªæç£ç£çè¨­å®ä¸­è¨ç·´ï¼åæè¨ç·´åµæ¸¬ï¼æ¶é¤äºå°æè²´èº«åæ¨ç±¤è³æéçéæ±ãä¸éæå¤±ï¼æ¡ç¨ç¡¬æ­£ä¾ååç¡¬è² ä¾ææç­ç¥ï¼ç¨æ¼å­¸ç¿å¤å¥å¼åµå¥ãè³æéè¯ééèªè¨è¿½è¹¤å¯¦ä½å¢å¼·ï¼è©²å¯¦ä½æåæ´åäºåä½ãå¤è§åä½ç½®ç·ç´¢ãYOLO11-JDE å¨ MOT17 å MOT20 åºæºä¸åå¾äºå·æç«¶ç­åççµæï¼å¨ FPS æ¹é¢è¶è¶äºç¾æç JDE æ¹æ³ï¼ä¸¦ä¸ä½¿ç¨çåæ¸æ¸å°äºå¤éååãå æ­¤ï¼ä½¿æåçæ¨¡åæçºç¾å¯¦ä¸çæç¨ä¸­æ¥µå·å¸å¼åçè§£æ±ºæ¹æ¡ã

##### **EventVL: Understand Event Streams via Multimodal Large Language Model**
2501.13707v1 by Pengteng Li, Yunfan Lu, Pinghao Song, Wuyang Li, Huizai Yao, Hui Xiong

The event-based Vision-Language Model (VLM) recently has made good progress
for practical vision tasks. However, most of these works just utilize CLIP for
focusing on traditional perception tasks, which obstruct model understanding
explicitly the sufficient semantics and context from event streams. To address
the deficiency, we propose EventVL, the first generative event-based MLLM
(Multimodal Large Language Model) framework for explicit semantic
understanding. Specifically, to bridge the data gap for connecting different
modalities semantics, we first annotate a large event-image/video-text dataset,
containing almost 1.4 million high-quality pairs of data, which enables
effective learning across various scenes, e.g., drive scene or human motion.
After that, we design Event Spatiotemporal Representation to fully explore the
comprehensive information by diversely aggregating and segmenting the event
stream. To further promote a compact semantic space, Dynamic Semantic Alignment
is introduced to improve and complete sparse semantic spaces of events.
Extensive experiments show that our EventVL can significantly surpass existing
MLLM baselines in event captioning and scene description generation tasks. We
hope our research could contribute to the development of the event vision
community.

æè¦ï¼<paragraph>åºæ¼äºä»¶çè¦è¦ºèªè¨æ¨¡å (VLM) æè¿å¨å¯¦åè¦è¦ºä»»åä¸åå¾è¯å¥½çé²å±ãç¶èï¼éäºä½åå¤§å¤åå©ç¨ CLIP å°æ³¨æ¼å³çµ±çæç¥ä»»åï¼éé»ç¤äºæ¨¡åæç¢ºçè§£äºä»¶ä¸²æµä¸­çè¶³å¤ èªæåèæ¯ãçºäºè§£æ±ºéåç¼ºé·ï¼æåæåºäº EventVLï¼éæ¯ç¬¬ä¸åç¨æ¼æç¢ºèªæçè§£ççæå¼åºæ¼äºä»¶ç MLLMï¼å¤æ¨¡æå¤§åèªè¨æ¨¡åï¼æ¶æ§ãå·é«ä¾èªªï¼çºäºå½åé£æ¥ä¸åæ¨¡æèªæçè³æå·®è·ï¼æåé¦åæ¨è¨»ä¸åå¤§åäºä»¶å½±å/å½±çæå­è³æéï¼å¶ä¸­åå«å°è¿ 140 è¬å°é«åè³ªè³æï¼éä½¿å¾è·¨åç¨®å ´æ¯ï¼ä¾å¦é§é§å ´æ¯æäººé¡åä½ï¼çææå­¸ç¿æçºå¯è½ãå¨é£ä¹å¾ï¼æåè¨­è¨äºäºä»¶æç©ºè¡¨ç¤ºï¼èç±å¤æ¨£åå°å½ç¸½ååå²äºä»¶ä¸²æµï¼ä¾ååæ¢ç´¢ç¶åè³è¨ãçºäºé²ä¸æ­¥ä¿é²ä¸åç·æ¹çèªæç©ºéï¼å¼å¥äºåæèªæå°é½ï¼ä»¥æ¹ååå®æäºä»¶çç¨çèªæç©ºéãå»£æ³çå¯¦é©é¡¯ç¤ºï¼æåç EventVL è½å¨äºä»¶å­å¹åå ´æ¯æè¿°ç¢çä»»åä¸­ï¼é¡¯èè¶è¶ç¾æç MLLM åºæºãæåå¸ææåçç ç©¶è½çºäºä»¶è¦è¦ºç¤¾ç¾¤çç¼å±ååºè²¢ç»ã</paragraph>

##### **DI-BENCH: Benchmarking Large Language Models on Dependency Inference with Testable Repositories at Scale**
2501.13699v1 by Linghao Zhang, Junhao Wang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Jiaheng Wen, Chengxing Xie, Maoquan Wang, Yufan Huang, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, Qi Zhang

Large Language Models have advanced automated software development, however,
it remains a challenge to correctly infer dependencies, namely, identifying the
internal components and external packages required for a repository to
successfully run. Existing studies highlight that dependency-related issues
cause over 40\% of observed runtime errors on the generated repository. To
address this, we introduce DI-BENCH, a large-scale benchmark and evaluation
framework specifically designed to assess LLMs' capability on dependency
inference. The benchmark features 581 repositories with testing environments
across Python, C#, Rust, and JavaScript. Extensive experiments with textual and
execution-based metrics reveal that the current best-performing model achieves
only a 42.9% execution pass rate, indicating significant room for improvement.
DI-BENCH establishes a new viewpoint for evaluating LLM performance on
repositories, paving the way for more robust end-to-end software synthesis.

æè¦ï¼å¤§åèªè¨æ¨¡åå·²æ¨åèªååè»é«éç¼ï¼ç¶èï¼æ­£ç¢ºæ¨æ·ä¾è³´éä¿ï¼ä¹å°±æ¯è­å¥å²å­åº«æåå·è¡æéçå§é¨åä»¶åå¤é¨å¥ä»¶ï¼ä»ç¶æ¯ä¸é ææ°ãç¾æç ç©¶å¼·èª¿èä¾è³´éä¿ç¸éçåé¡å°è´å¨çæçå²å­åº«ä¸­è§å¯å°çå·è¡æéé¯èª¤è¶é 40%ãçºäºè§£æ±ºæ­¤åé¡ï¼æåå¼å¥äº DI-BENCHï¼ä¸åå°éè¨­è¨ä¾è©ä¼° LLM å¨ä¾è³´éä¿æ¨æ·è½åä¸çå¤§ååºæºæ¸¬è©¦åè©ä¼°æ¶æ§ãæ­¤åºæºæ¸¬è©¦åå« 581 åå²å­åº«ï¼æ¸¬è©¦ç°å¢æ¶µè PythonãC#ãRust å JavaScriptãä½¿ç¨æå­ååºæ¼å·è¡çææ¨é²è¡çå»£æ³å¯¦é©é¡¯ç¤ºï¼ç®åæè½æä½³çæ¨¡ååéå° 42.9% çå·è¡ééçï¼è¡¨ç¤ºæé¡¯èçæ¹é²ç©ºéãDI-BENCH çºè©ä¼° LLM å¨å²å­åº«ä¸çæè½å»ºç«äºä¸åæ°çè§é»ï¼çºæ´å¼·å¤§çç«¯å°ç«¯è»é«åæéªè·¯ã

##### **Training-Free Consistency Pipeline for Fashion Repose**
2501.13692v1 by Potito Aghilar, Vito Walter Anelli, Michelantonio Trizio, Tommaso Di Noia

Recent advancements in diffusion models have significantly broadened the
possibilities for editing images of real-world objects. However, performing
non-rigid transformations, such as changing the pose of objects or image-based
conditioning, remains challenging. Maintaining object identity during these
edits is difficult, and current methods often fall short of the precision
needed for industrial applications, where consistency is critical.
Additionally, fine-tuning diffusion models requires custom training data, which
is not always accessible in real-world scenarios. This work introduces
FashionRepose, a training-free pipeline for non-rigid pose editing specifically
designed for the fashion industry. The approach integrates off-the-shelf models
to adjust poses of long-sleeve garments, maintaining identity and branding
attributes. FashionRepose uses a zero-shot approach to perform these edits in
near real-time, eliminating the need for specialized training. consistent image
editing. The solution holds potential for applications in the fashion industry
and other fields demanding identity preservation in image editing.

æè¦ï¼æè¿å¨æ´æ£æ¨¡åæ¹é¢çé²å±é¡¯èæå¯¬äºç·¨è¼¯çå¯¦ä¸çç©é«ååçå¯è½æ§ãç¶èï¼å·è¡éåæ§è½æï¼ä¾å¦æ´æ¹ç©é«çå§¿å¢æåºæ¼ååçæ¢ä»¶ï¼ä»ç¶å·æææ°æ§ãå¨éäºç·¨è¼¯éç¨ä¸­ç¶­è­·ç©é«èº«åå¾å°é£ï¼èä¸ç®åçæè¡éå¸¸ç¡æ³éå°ç¢æ¥­æç¨æéçç²¾ç¢ºåº¦ï¼èä¸è´æ§å¨ç¢æ¥­æç¨ä¸­è³ééè¦ãæ­¤å¤ï¼å¾®èª¿æ´æ£æ¨¡åéè¦èªè¨è¨ç·´è³æï¼éå¨çå¯¦ä¸ççå ´æ¯ä¸­ä¸¦ä¸ç¸½æ¯å¯åå¾çãéé å·¥ä½ä»ç´¹äº FashionReposeï¼ä¸ç¨®å°éçºæå°ç¢æ¥­è¨­è¨çéåæ§å§¿å¢ç·¨è¼¯åè¨ç·´ç®¡éãéç¨®æ¹æ³æ´åäºç¾æçæ¨¡åä¾èª¿æ´é·è¢æè£çå§¿å¢ï¼åæç¶­æèº«åååçå±¬æ§ãFashionRepose ä½¿ç¨é¶æ¬¡å­¸ç¿æ¹æ³ä¾å·è¡éäºç·¨è¼¯ï¼å¹¾ä¹å¯ä»¥å³æå®æï¼ç¡éé²è¡å°æ¥­è¨ç·´ãä¸è´çååç·¨è¼¯ãæ­¤è§£æ±ºæ¹æ¡å¨æå°ç¢æ¥­åå¶ä»éè¦å¨ååç·¨è¼¯ä¸­ä¿çèº«åçé åå·ææç¨æ½åã

##### **Question Answering on Patient Medical Records with Private Fine-Tuned LLMs**
2501.13687v1 by Sara Kothari, Ayush Gupta

Healthcare systems continuously generate vast amounts of electronic health
records (EHRs), commonly stored in the Fast Healthcare Interoperability
Resources (FHIR) standard. Despite the wealth of information in these records,
their complexity and volume make it difficult for users to retrieve and
interpret crucial health insights. Recent advances in Large Language Models
(LLMs) offer a solution, enabling semantic question answering (QA) over medical
data, allowing users to interact with their health records more effectively.
However, ensuring privacy and compliance requires edge and private deployments
of LLMs.
  This paper proposes a novel approach to semantic QA over EHRs by first
identifying the most relevant FHIR resources for a user query (Task1) and
subsequently answering the query based on these resources (Task2). We explore
the performance of privately hosted, fine-tuned LLMs, evaluating them against
benchmark models such as GPT-4 and GPT-4o. Our results demonstrate that
fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by
0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we
examine advanced aspects of LLM usage, including sequential fine-tuning, model
self-evaluation (narcissistic evaluation), and the impact of training data size
on performance. The models and datasets are available here:
https://huggingface.co/genloop

æè¦ï¼é«çä¿å¥ç³»çµ±æçºç¢çå¤§éçé»å­å¥åº·ç´é (EHR)ï¼éå¸¸å²å­å¨å¿«éé«çäºéæ§è³æº (FHIR) æ¨æºä¸­ãåç®¡éäºç´éä¸­åå«è±å¯çè³è¨ï¼ä½å¶è¤éæ§åé¾å¤§æ¸éè®ä½¿ç¨èé£ä»¥æ·ååè©®ééè¦çå¥åº·è¦è§£ãå¤§åèªè¨æ¨¡å (LLM) çææ°é²å±æä¾äºè§£æ±ºæ¹æ¡ï¼è½å°é«çè³æé²è¡èªç¾©åç­ (QA)ï¼è®ä½¿ç¨èè½æ´ææå°èå¶å¥åº·ç´éäºåãç¶èï¼ç¢ºä¿é±ç§åç¸å®¹æ§éè¦ LLM çéç·£åç§äººé¨ç½²ãæ¬ææåºäºèªç¾©åç­çæ°æ¹æ³ï¼åæ¾åºèä½¿ç¨èæ¥è©¢æç¸éç FHIR è³æº (ä»»å 1)ï¼ç¶å¾æ ¹æéäºè³æºåç­æ¥è©¢ (ä»»å 2)ãæåæ¢è¨äºç§äººä¸»æ©ãå¾®èª¿ LLM çæè½ï¼ä¸¦æ ¹æ GPT-4 å GPT-4o ç­åºæºæ¨¡åè©ä¼°å®åãæåççµæé¡¯ç¤ºï¼å¾®èª¿ LLM çå¤§å°éç¶å° 250 åï¼ä½å¨ä»»å 1 ç F1 åæ¸ä¸åªæ¼ GPT-4 ç³»åæ¨¡å 0.55%ï¼å¨ä»»å 2 ç Meteor ä»»åä¸­åªæ¼ 42%ãæ­¤å¤ï¼æåæ¢è¨äº LLM ä½¿ç¨çé«éé¢åï¼åæ¬å¾ªåºå¾®èª¿ãæ¨¡åèªæè©ä¼°ï¼èªæå¼è©ä¼°ï¼åè¨ç·´è³æå¤§å°å°æè½çå½±é¿ãæ¨¡ååè³æéå¨æ­¤èæä¾ï¼https://huggingface.co/genloop

##### **Unlearning Clients, Features and Samples in Vertical Federated Learning**
2501.13683v1 by Ayush K. Varshney, Konstantinos Vandikas, VicenÃ§ Torra

Federated Learning (FL) has emerged as a prominent distributed learning
paradigm. Within the scope of privacy preservation, information privacy
regulations such as GDPR entitle users to request the removal (or unlearning)
of their contribution from a service that is hosting the model. For this
purpose, a server hosting an ML model must be able to unlearn certain
information in cases such as copyright infringement or security issues that can
make the model vulnerable or impact the performance of a service based on that
model. While most unlearning approaches in FL focus on Horizontal FL (HFL),
where clients share the feature space and the global model, Vertical FL (VFL)
has received less attention from the research community. VFL involves clients
(passive parties) sharing the sample space among them while not having access
to the labels. In this paper, we explore unlearning in VFL from three
perspectives: unlearning clients, unlearning features, and unlearning samples.
To unlearn clients and features we introduce VFU-KD which is based on knowledge
distillation (KD) while to unlearn samples, VFU-GA is introduced which is based
on gradient ascent. To provide evidence of approximate unlearning, we utilize
Membership Inference Attack (MIA) to audit the effectiveness of our unlearning
approach. Our experiments across six tabular datasets and two image datasets
demonstrate that VFU-KD and VFU-GA achieve performance comparable to or better
than both retraining from scratch and the benchmark R2S method in many cases,
with improvements of $(0-2\%)$. In the remaining cases, utility scores remain
comparable, with a modest utility loss ranging from $1-5\%$. Unlike existing
methods, VFU-KD and VFU-GA require no communication between active and passive
parties during unlearning. However, they do require the active party to store
the previously communicated embeddings.

æè¦ï¼<paragraph>èé¦å­¦ä¹  (FL) å·²æä¸ºä¸ç§çªåºçåå¸å¼å­¦ä¹ èä¾ãå¨éç§ä¿æ¤èå´åï¼ä¿¡æ¯éç§æ³è§ï¼å¦ GDPRï¼èµäºç¨æ·è¦æ±ä»æç®¡æ¨¡åçæå¡ä¸­ç§»é¤ï¼æåæ¶å­¦ä¹ ï¼å¶è´¡ç®çæå©ãä¸ºæ­¤ï¼æç®¡ ML æ¨¡åçæå¡å¨å¿é¡»è½å¤å¨æäºæåµä¸åæ¶å­¦ä¹ ç¹å®ä¿¡æ¯ï¼ä¾å¦çæä¾µææå®å¨é®é¢ï¼è¿äºé®é¢å¯è½ä½¿æ¨¡åå®¹æåå°æ»å»æå½±ååºäºè¯¥æ¨¡åçæå¡çæ§è½ãè½ç¶ FL ä¸­çå¤§å¤æ°åæ¶å­¦ä¹ æ¹æ³é½éä¸­å¨æ°´å¹³ FL (HFL) ä¸ï¼å¶ä¸­å®¢æ·ç«¯å±äº«ç¹å¾ç©ºé´åå¨å±æ¨¡åï¼ä½åç´ FL (VFL) å´è¾å°åå°ç ç©¶ççå³æ³¨ãVFL æ¶åå®¢æ·ç«¯ï¼è¢«å¨æ¹ï¼å¨ä»ä»¬ä¹é´å±äº«æ ·æ¬ç©ºé´ï¼åæ¶æ æ³è®¿é®æ ç­¾ãå¨æ¬æä¸­ï¼æä»¬ä»ä¸ä¸ªè§åº¦æ¢ç´¢ VFL ä¸­çåæ¶å­¦ä¹ ï¼åæ¶å­¦ä¹ å®¢æ·ç«¯ãåæ¶å­¦ä¹ ç¹å¾ååæ¶å­¦ä¹ æ ·æ¬ãä¸ºäºåæ¶å­¦ä¹ å®¢æ·ç«¯åç¹å¾ï¼æä»¬å¼å¥äºåºäºç¥è¯è¸é¦ (KD) ç VFU-KDï¼èä¸ºäºåæ¶å­¦ä¹ æ ·æ¬ï¼æä»¬å¼å¥äºåºäºæ¢¯åº¦ä¸åç VFU-GAãä¸ºäºæä¾è¿ä¼¼åæ¶å­¦ä¹ çè¯æ®ï¼æä»¬å©ç¨æåèµæ ¼æ¨çæ»å» (MIA) æ¥å®¡è®¡æä»¬åæ¶å­¦ä¹ æ¹æ³çæææ§ãæä»¬å¨å­ä¸ªè¡¨æ ¼æ°æ®éåä¸¤ä¸ªå¾åæ°æ®éä¸çå®éªè¡¨æï¼å¨è®¸å¤æåµä¸ï¼VFU-KD å VFU-GA å®ç°çæ§è½ä¸ä»å¤´å¼å§éæ°è®­ç»ååºå R2S æ¹æ³ç¸å½ææ´å¥½ï¼æ¹è¿äº $(0-2\%)ãå¨å¶ä½æåµä¸ï¼æç¨å¾åä»ç¶ç¸å½ï¼æç¨æå¤±éä¸­ï¼èå´ä» $1-5\%$ãä¸ç°ææ¹æ³ä¸åï¼VFU-KD å VFU-GA å¨åæ¶å­¦ä¹ æé´ä¸éè¦ä¸»å¨æ¹åè¢«å¨æ¹ä¹é´çéä¿¡ãä½æ¯ï¼å®ä»¬ç¡®å®è¦æ±ä¸»å¨æ¹å­å¨ä»¥åä¼ è¾¾çåµå¥ã</paragraph>

##### **Certified Robustness Under Bounded Levenshtein Distance**
2501.13676v1 by Elias Abad Rocamora, Grigorios G. Chrysos, Volkan Cevher

Text classifiers suffer from small perturbations, that if chosen
adversarially, can dramatically change the output of the model. Verification
methods can provide robustness certificates against such adversarial
perturbations, by computing a sound lower bound on the robust accuracy.
Nevertheless, existing verification methods incur in prohibitive costs and
cannot practically handle Levenshtein distance constraints. We propose the
first method for computing the Lipschitz constant of convolutional classifiers
with respect to the Levenshtein distance. We use these Lipschitz constant
estimates for training 1-Lipschitz classifiers. This enables computing the
certified radius of a classifier in a single forward pass. Our method, LipsLev,
is able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and
$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude
faster than existing approaches. We believe our work can open the door to more
efficient verification in the text domain.

æè¦ï¼æå­åé¡å¨æåå°å°æ¾åçå½±é¿ï¼å¦æé¸æå°ææ§æ¾åï¼å¯è½æå¤§å¹æ¹è®æ¨¡åçè¼¸åºãé©è­æ¹æ³å¯ä»¥ééè¨ç®ç©©å¥ç²¾ç¢ºåº¦çä¸éï¼æä¾éå°æ­¤é¡å°ææ§æ¾åçç©©å¥æ§è­æãåç®¡å¦æ­¤ï¼ç¾æçé©è­æ¹æ³æç¢çé«æçææ¬ï¼ä¸å¯¦éä¸ç¡æ³èç Levenshtein è·é¢éå¶ãæåæåºç¬¬ä¸åç¨æ¼è¨ç®å·ç©åé¡å¨å° Levenshtein è·é¢ç Lipschitz å¸¸æ¸çæ¹æ³ãæåä½¿ç¨éäº Lipschitz å¸¸æ¸ä¼°è¨å¼ä¾è¨ç·´ 1-Lipschitz åé¡å¨ãéè½å¤ å¨å®æ¬¡ååå³éä¸­è¨ç®åé¡å¨çèªè­åå¾ãæåç LipsLev æ¹æ³è½å¤ å¨ AG-News è³æéä¸­åå¥å¨è·é¢ 1 å 2 ç²å¾ $38.80$% å $13.93$% çé©è­ç²¾ç¢ºåº¦ï¼åææ¯ç¾ææ¹æ³å¿« 4 åæ¸éç´ãæåç¸ä¿¡æåçç ç©¶ææå¯ä»¥çºæå­é åä¸­æ´ææçé©è­æéå¤§éã

##### **How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization**
2501.13669v1 by Shezheng Song, Hao Xu, Jun Ma, Shasha Li, Long Peng, Qian Wan, Xiaodong Liu, Jie Yu

Large Language Models (LLMs) exhibit strong general-purpose language
capabilities. However, fine-tuning these models on domain-specific tasks often
leads to catastrophic forgetting, where the model overwrites or loses essential
knowledge acquired during pretraining. This phenomenon significantly limits the
broader applicability of LLMs. To address this challenge, we propose a novel
approach to compute the element-wise importance of model parameters crucial for
preserving general knowledge during fine-tuning. Our method utilizes a
dual-objective optimization strategy: (1) regularization loss to retain the
parameter crucial for general knowledge; (2) cross-entropy loss to adapt to
domain-specific tasks. Additionally, we introduce layer-wise coefficients to
account for the varying contributions of different layers, dynamically
balancing the dual-objective optimization. Extensive experiments on scientific,
medical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our
approach mitigates catastrophic forgetting while enhancing model adaptability.
Compared to previous methods, our solution is approximately 20 times faster and
requires only 10%-15% of the storage, highlighting the practical efficiency.
The code will be released.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å±ç¾å¼·å¤§çéç¨èªè¨è½åãç¶èï¼éå°ç¹å®é åä»»åå¾®èª¿éäºæ¨¡åæï¼å¸¸å¸¸æå°è´ç½é£æ§éºå¿ï¼æ¨¡åæè¦å¯«æéºå¤±é è¨ç·´æéç¿å¾çåºæ¬ç¥è­ãéç¨®ç¾è±¡å¤§å¹éå¶äº LLM çå»£æ³é©ç¨æ§ãçºäºæå°éé ææ°ï¼æåæåºäºä¸ç¨®æ°ç©æ¹æ³ï¼ç¨æ¼è¨ç®æ¨¡ååæ¸çåç´ ç´éè¦æ§ï¼éäºåæ¸å°æ¼å¨å¾®èª¿æéä¿çä¸è¬ç¥è­è³ééè¦ãæåçåæ³æ¡ç¨éç®æ¨åªåç­ç¥ï¼(1) æ­£ååæå¤±ï¼ç¨æ¼ä¿çå°ä¸è¬ç¥è­è³ééè¦çåæ¸ï¼(2) äº¤åçµæå¤±ï¼ç¨æ¼é©æç¹å®é åçä»»åãæ­¤å¤ï¼æåå¼å¥äºå±¤ç´ä¿æ¸ï¼ç¨æ¼èéä¸åå±¤çè®ç°è²¢ç»ï¼ä¸¦åæå¹³è¡¡éç®æ¨åªåãä½¿ç¨ GPT-J å LLaMA-3 å¨ç§å­¸ãé«çåç©çä»»åä¸é²è¡çå»£æ³å¯¦é©è­æï¼æåçåæ³æ¸è¼äºç½é£æ§éºå¿ï¼åæå¢å¼·äºæ¨¡åé©ææ§ãèä¹åçåæ³ç¸æ¯ï¼æåçè§£æ±ºæ¹æ¡éåº¦å¿«äºç´ 20 åï¼èä¸åªéè¦ 10%-15% çå²å­ç©ºéï¼çªé¡¯äºå¶å¯¦ç¨çæçãç¨å¼ç¢¼å°æéåºã

##### **LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models**
2501.13652v1 by Yizheng Sun, Yanze Xin, Hao Li, Jingyuan Sun, Chenghua Lin, Riza Batista-Navarro

Multi-modal Large Language Models (MLLMs) have achieved remarkable success by
integrating visual and textual modalities. However, they incur significant
computational overhead due to the large number of vision tokens processed,
limiting their practicality in resource-constrained environments. We introduce
Language-Guided Vision Token Pruning (LVPruning) for MLLMs, an effective yet
simple method that significantly reduces the computational burden while
preserving model performance. LVPruning employs cross-attention modules to
compute the importance of vision tokens based on their interaction with
language tokens, determining which to prune. Importantly, LVPruning can be
integrated without modifying the original MLLM parameters, which makes
LVPruning simple to apply or remove. Our experiments show that LVPruning can
effectively reduce up to 90% of vision tokens by the middle layer of LLaVA-1.5,
resulting in a 62.1% decrease in inference Tera Floating-Point Operations Per
Second (TFLOPs), with an average performance loss of just 0.45% across nine
multi-modal benchmarks.

æè¦ï¼å¤æ¨¡æå¤§åè¯­è¨æ¨¡å (MLLM) éè¿æ´åè§è§åææ¬æ¨¡æï¼å·²åå¾æ¾èçæåãç¶èï¼ç±äºå¤çå¤§éè§è§æ è®°ï¼å®ä»¬ä¼äº§çå¤§éçè®¡ç®å¼éï¼éå¶äºå®ä»¬å¨èµæºåéç¯å¢ä¸­çå®ç¨æ§ãæä»¬éå¯¹ MLLM å¼å¥äºè¯­è¨å¼å¯¼è§è§æ è®°ä¿®åª (LVPruning)ï¼è¿æ¯ä¸ç§ææä¸ç®åçæ¹æ³ï¼å¯ä»¥å¨ä¿çæ¨¡åæ§è½çåæ¶å¤§å¹åå°è®¡ç®è´æãLVPruning ä½¿ç¨äº¤åæ³¨æåæ¨¡åï¼æ ¹æ®è§è§æ è®°ä¸è¯­è¨æ è®°çäº¤äºä½ç¨è®¡ç®è§è§æ è®°çéè¦æ§ï¼ä»¥ç¡®å®è¦ä¿®åªåªäºæ è®°ãéè¦çæ¯ï¼LVPruning å¯ä»¥å¨ä¸ä¿®æ¹åå§ MLLM åæ°çæåµä¸è¿è¡æ´åï¼è¿ä½¿å¾ LVPruning æäºåºç¨æç§»é¤ãæä»¬çå®éªè¡¨æï¼LVPruning å¯ä»¥ææå°å° LLaVA-1.5 ä¸­å±å¤è¾¾ 90% çè§è§æ è®°åå°ï¼ä»èä½¿æ¯ç§æ¨çæµ®ç¹è¿ç® (TFLOPs) åå° 62.1%ï¼å¨ä¹ä¸ªå¤æ¨¡æåºåæµè¯ä¸­ï¼å¹³åæ§è½æå¤±ä»ä¸º 0.45%ã

##### **Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models**
2501.13629v1 by Zhenghao Lin, Zihao Tang, Xiao Liu, Yeyun Gong, Yi Cheng, Qi Chen, Hang Li, Ying Xin, Ziyue Yang, Kailai Yang, Yu Yan, Xiao Liang, Shuai Lu, Yiming Huang, Zheheng Luo, Lei Qu, Xuan Feng, Yaoxiang Wang, Yuqing Xia, Feiyang Chen, Yuting Jiang, Yasen Hu, Hao Ni, Binyang Li, Guoshuai Zhao, Jui-Hao Chiang, Zhongxin Guo, Chen Lin, Kun Kuang, Wenjie Li, Yelong Shen, Jian Jiao, Peng Cheng, Mao Yang

We introduce Sigma, an efficient large language model specialized for the
system domain, empowered by a novel architecture including DiffQKV attention,
and pre-trained on our meticulously collected system domain data. DiffQKV
attention significantly enhances the inference efficiency of Sigma by
optimizing the Query (Q), Key (K), and Value (V) components in the attention
mechanism differentially, based on their varying impacts on the model
performance and efficiency indicators. Specifically, we (1) conduct extensive
experiments that demonstrate the model's varying sensitivity to the compression
of K and V components, leading to the development of differentially compressed
KV, and (2) propose augmented Q to expand the Q head dimension, which enhances
the model's representation capacity with minimal impacts on the inference
speed. Rigorous theoretical and empirical analyses reveal that DiffQKV
attention significantly enhances efficiency, achieving up to a 33.36%
improvement in inference speed over the conventional grouped-query attention
(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various
sources, including 19.5B system domain data that we carefully collect and 1T
tokens of synthesized and rewritten data. In general domains, Sigma achieves
comparable performance to other state-of-arts models. In the system domain, we
introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates
remarkable performance across all tasks, significantly outperforming GPT-4 with
an absolute improvement up to 52.5%.

æè¦ï¼<paragraph>æåæ¨åº Sigmaï¼éæ¯ä¸åå°ééå°ç³»çµ±é åçé«æå¤§åèªè¨æ¨¡åï¼å®æ¡ç¨åµæ°çæ¶æ§ï¼åæ¬ DiffQKV æ³¨æåï¼ä¸¦åºæ¼æåç²¾å¿æ¶éçç³»çµ±é åè³æé²è¡é è¨ç·´ãDiffQKV æ³¨æåéééå°æ¥è©¢ (Q)ãéé° (K) åå¼ (V) åä»¶é²è¡å·®ç°åæä½³åï¼é¡¯èæå Sigma çæ¨è«æçï¼éæ¯æ ¹æå®åå°æ¨¡åæè½åæçææ¨çå½±é¿èå®çãå·é«ä¾èªªï¼æå (1) é²è¡å»£æ³çå¯¦é©ï¼è­ææ¨¡åå° K å V åä»¶çå£ç¸®å·æä¸åçææåº¦ï¼é²èéç¼åºå·®ç°åå£ç¸®ç KVï¼ä»¥å (2) æåºæ´å¢ Q ä»¥æ´å± Q é ­é¨ç¶­åº¦ï¼éè½æåæ¨¡åçè¡¨ç¤ºè½åï¼åæå°æ¨è«éåº¦çå½±é¿æå°ãå´è¬¹ççè«åå¯¦è­åæé¡¯ç¤ºï¼DiffQKV æ³¨æåé¡¯èæåæçï¼å¨é·èªå¢å ´æ¯ä¸­ï¼æ¨è«éåº¦æ¯å³çµ±çåçµæ¥è©¢æ³¨æå (GQA) æåå¤é 33.36%ãæåä½¿ç¨ä¾èªåç¨®ä¾æºç 6T åç¬¦èå° Sigma é²è¡é è¨ç·´ï¼åæ¬æåç²¾å¿æ¶éç 19.5B åç³»çµ±é åè³æï¼ä»¥å 1T ååæåéå¯«çè³æãå¨ä¸è¬é åï¼Sigma éå°èå¶ä»æåé²æ¨¡åç¸ç¶çæè½ãå¨ç³»çµ±é åï¼æåæ¨åºç¬¬ä¸åå¨é¢çåºæº AIMiciusï¼å¶ä¸­ Sigma å¨ææä»»åä¸­é½å±ç¾åºåè¶çæè½ï¼æé¡¯åªæ¼ GPT-4ï¼çµå°æåå¹åº¦é«é 52.5%ã</paragraph>

##### **Coarse-to-Fine Process Reward Modeling for Enhanced Mathematical Reasoning**
2501.13622v1 by Yulan Hu, Sheng Ouyang, Yong Liu

Process reward model (PRM) is critical for mathematical reasoning tasks to
assign rewards for each intermediate steps. The PRM requires constructing
process-wise supervision data for training, which rely on chain-of-thought
(CoT) or tree-based methods to construct the reasoning steps, however, the
individual reasoning steps may be redundant or containing nuanced errors that
difficult to detect. We attribute these to the issue of the overlook of
granularity division during process data collection. In this paper, we propose
a coarse-to-fine framework to tackle this issue. Specifically, while gathering
the process supervision data, we collect the coarse reasoning steps by merging
adjacent steps according to preset merging granularity, then we sequentially
reduce the merging granularity to collect fine-grained reasoning steps. For
each synthesized new step, we relabel according to the label of last step.
During training, we also traverse the collected training corpus in a
coarse-to-fine manner. We conduct extensive experiments on popular mathematical
reasoning datasets across diverse loss criterions, the proposed framework can
consistently boost the reasoning performance.

æè¦ï¼èççåµæ¨¡å (PRM) å°æ¸å­¸æ¨çä»»åè³ééè¦ï¼ä»¥åéæ¯åä¸­éæ­¥é©ççåµãPRM éè¦æ§å»ºç¨æ¼è¨ç·´çéç¨ç£ç£è³æï¼éä¾è³´æ¼ææ³é (CoT) æåºæ¼æ¨¹çæ¹æ³ä¾æ§å»ºæ¨çæ­¥é©ï¼ç¶èï¼åå¥çæ¨çæ­¥é©å¯è½åé¤æåå«é£ä»¥æª¢æ¸¬çç´°å¾®é¯èª¤ãæåå°éäºæ­¸å æ¼éç¨è³ææ¶ééç¨ä¸­ç²åº¦ååçå¿½è¦åé¡ãå¨æ¬æä¸­ï¼æåæåºäºä¸åå¾ç²å°ç´°çæ¡æ¶ä¾è§£æ±ºéååé¡ãå·é«ä¾èªªï¼å¨æ¶ééç¨ç£ç£è³ææï¼æåééæ ¹æé è¨­åä½µç²åº¦åä½µç¸é°æ­¥é©ä¾æ¶éç²ç¥çæ¨çæ­¥é©ï¼ç¶å¾æåä¾æ¬¡éä½åä½µç²åº¦ä»¥æ¶éç´°ç²åº¦çæ¨çæ­¥é©ãå°æ¼æ¯ååæçæ°æ­¥é©ï¼æåæ ¹ææå¾ä¸æ­¥çæ¨ç±¤éæ°æ¨è¨ãå¨è¨ç·´æéï¼æåä¹ä»¥å¾ç²å°ç´°çæ¹å¼éæ­·æ¶éçè¨ç·´èªæåº«ãæåå°åç¨®æµè¡çæ¸å­¸æ¨çè³æéé²è¡äºå»£æ³çå¯¦é©ï¼è·¨è¶äºä¸åçæå¤±æºåï¼ææåºçæ¡æ¶å¯ä»¥æçºæåæ¨çæ§è½ã

##### **Cognitive Paradigms for Evaluating VLMs on Visual Reasoning Task**
2501.13620v1 by Mohit Vaishnav, Tanel Tammet

Evaluating the reasoning capabilities of Vision-Language Models (VLMs) in
complex visual tasks provides valuable insights into their potential and
limitations. In this work, we assess the performance of VLMs on the challenging
Bongard Openworld Problems benchmark, which involves reasoning over natural
images. We propose and evaluate three human-inspired paradigms: holistic
analysis (global context processing), deductive rule learning (explicit rule
derivation and application), and componential analysis (structured
decomposition of images into components). Our results demonstrate that
state-of-the-art models, including GPT-4o and Gemini, not only surpass human
benchmarks but also excel in structured reasoning tasks, with componential
analysis proving especially effective. However, ablation studies reveal key
challenges, such as handling synthetic images, making fine-grained
distinctions, and interpreting nuanced contextual information. These insights
underscore the need for further advancements in model robustness and
generalization, while highlighting the transformative potential of structured
reasoning approaches in enhancing VLM capabilities.

æè¦ï¼è©ä¼°è¦è¦ºèªè¨æ¨¡å (VLM) å¨è¤éè¦è¦ºä»»åä¸­çæ¨çè½åï¼è½æä¾æå¹å¼çè¦è§£ï¼äºè§£å¶æ½ååéå¶ãå¨éé å·¥ä½ä¸­ï¼æåè©ä¼° VLM å¨å·æææ°æ§ç Bongard Openworld Problems åºæºä¸çæè½ï¼å¶ä¸­æ¶åå°èªç¶å½±åçæ¨çãæåæåºä¸¦è©ä¼°ä¸ååäººé¡åç¼çç¯ä¾ï¼æ´é«åæï¼å¨å±èçµ¡èçï¼ãæ¼ç¹¹è¦åå­¸ç¿ï¼æç¢ºè¦åæ¨å°åæç¨ï¼ï¼ä»¥åçµæåæï¼å°å½±åçµæ§ååè§£çºçµæé¨åï¼ãæåççµæè¡¨æï¼åæ¬ GPT-4o å Gemini å¨å§çææ°æ¨¡åä¸åè¶è¶äºäººé¡åºæºï¼èä¸å¨çµæ§åæ¨çä»»åä¸­è¡¨ç¾åºè²ï¼å¶ä¸­çµæåæç¹å¥ææãç¶èï¼æ¶èç ç©¶æ­ç¤ºäºééµææ°ï¼ä¾å¦èçåæå½±åãååºç´°å¾®ååä»¥åè©®éç´°å¾®çèçµ¡è³è¨ãéäºè¦è§£å¼·èª¿äºé²ä¸æ­¥æåæ¨¡åç©©å¥æ§åæ³åçå¿è¦æ§ï¼åæå¼·èª¿äºçµæ§åæ¨çæ¹æ³å¨å¢å¼· VLM è½åæ¹é¢çè½åæ½åã

##### **Efficient Synaptic Delay Implementation in Digital Event-Driven AI Accelerators**
2501.13610v1 by Roy Meijer, Paul Detterer, Amirreza Yousefzadeh, Alberto Patino-Saucedo, Guanghzi Tang, Kanishkan Vadivel, Yinfu Xu, Manil-Dev Gomony, Federico Corradi, Bernabe Linares-Barranco, Manolis Sifalakis

Synaptic delay parameterization of neural network models have remained
largely unexplored but recent literature has been showing promising results,
suggesting the delay parameterized models are simpler, smaller, sparser, and
thus more energy efficient than similar performing (e.g. task accuracy)
non-delay parameterized ones. We introduce Shared Circular Delay Queue (SCDQ),
a novel hardware structure for supporting synaptic delays on digital
neuromorphic accelerators. Our analysis and hardware results show that it
scales better in terms of memory, than current commonly used approaches, and is
more amortizable to algorithm-hardware co-optimizations, where in fact, memory
scaling is modulated by model sparsity and not merely network size. Next to
memory we also report performance on latency area and energy per inference.

æè¦ï¼ç¥ç¶ç¶²è·¯æ¨¡åççªè§¸å»¶é²åæ¸åå¨å¾å¤§ç¨åº¦ä¸ä»æªè¢«æ¢ç´¢ï¼ä½æè¿çæç»é¡¯ç¤ºäºæå¸æççµæï¼è¡¨æå»¶é²åæ¸åæ¨¡åæ´ç°¡å®ãæ´å°ãæ´ç¨çï¼å æ­¤æ¯å·è¡é¡ä¼¼ï¼ä¾å¦ä»»åæºç¢ºåº¦ï¼çéå»¶é²åæ¸åæ¨¡åæ´ç¯è½ãæåå¼å¥äºå±äº«å¾ªç°å»¶é²ä½å (SCDQ)ï¼éæ¯ä¸ç¨®ç¨æ¼æ¯ææ¸ä½ç¥ç¶å½¢æå éå¨ä¸çªè§¸å»¶é²çæ°ç©ç¡¬é«çµæ§ãæåçåæåç¡¬é«çµæè¡¨æï¼èç¶åå¸¸ç¨çæ¹æ³ç¸æ¯ï¼å®å¨è¨æ¶é«æ¹é¢å·ææ´å¥½çæ´åæ§ï¼ä¸¦ä¸æ´é©åæ¼ç®æ³ç¡¬é«ååæä½³åï¼å¶ä¸­è¨æ¶é«æ´åæ§å¯¦éä¸æ¯ç±æ¨¡åç¨çæ§èª¿ç¯çï¼èä¸ä»ä»æ¯ç¶²è·¯è¦æ¨¡ãé¤äºè¨æ¶é«ä¹å¤ï¼æåéå ±åäºæ¯åæ¨è«çå»¶é²é¢ç©åè½éæè½ã

##### **Domain-Specific Machine Translation to Translate Medicine Brochures in English to Sorani Kurdish**
2501.13609v1 by Mariam Shamal, Hossein Hassani

Access to Kurdish medicine brochures is limited, depriving Kurdish-speaking
communities of critical health information. To address this problem, we
developed a specialized Machine Translation (MT) model to translate English
medicine brochures into Sorani Kurdish using a parallel corpus of 22,940
aligned sentence pairs from 319 brochures, sourced from two pharmaceutical
companies in the Kurdistan Region of Iraq (KRI). We trained a Statistical
Machine Translation (SMT) model using the Moses toolkit, conducting seven
experiments that resulted in BLEU scores ranging from 22.65 to 48.93. We
translated three new brochures to improve the evaluation process and
encountered unknown words. We addressed unknown words through post-processing
with a medical dictionary, resulting in BLEU scores of 56.87, 31.05, and 40.01.
Human evaluation by native Kurdish-speaking pharmacists, physicians, and
medicine users showed that 50% of professionals found the translations
consistent, while 83.3% rated them accurate. Among users, 66.7% considered the
translations clear and felt confident using the medications.

æè¦ï¼åº«å¾·èªé«çæåçåå¾ç®¡éæéï¼åå¥ªäºåº«å¾·èªç¤¾ç¾¤ç²å¾ééµé«çè³è¨çæ¬å©ãçºäºè§£æ±ºéååé¡ï¼æåéç¼äºä¸åå°éæ©å¨ç¿»è­¯ (MT) æ¨¡åï¼ä½¿ç¨ä¾èªä¼æååº«å¾·æ¯å¦å°å (KRI) å©å®¶è£½è¥å¬å¸ç 319 æ¬æåã22,940 å°æ ¡æºå¥å­çå¹³è¡èªæåº«ï¼å°è±æé«çæåç¿»è­¯æç´¢æå°¼åº«å¾·èªãæåä½¿ç¨ Moses å·¥å·åè¨ç·´äºä¸åçµ±è¨æ©å¨ç¿»è­¯ (SMT) æ¨¡åï¼é²è¡äºä¸æ¬¡å¯¦é©ï¼å¾å°ç BLEU åæ¸ä»æ¼ 22.65 å° 48.93 ä¹éãæåç¿»è­¯äºä¸æ¬æ°çå°åå­ä»¥æ¹åè©ä¼°ç¨åºï¼ä¸¦éå°äºæªç¥çå®å­ãæåééä½¿ç¨é«å­¸è¾­å¸é²è¡å¾èçä¾èçæªç¥çå®å­ï¼å¾å°ç BLEU åæ¸çº 56.87ã31.05 å 40.01ãç±åº«å¾·èªæ¯èªçè¥åå¸«ãé«å¸«åé«çä½¿ç¨èé²è¡çäººå·¥è©ä¼°é¡¯ç¤ºï¼50% çå°æ¥­äººå£«èªçºç¿»è­¯å§å®¹ä¸è´ï¼è 83.3% çäººè©ä¼°å®åæºç¢ºãå¨ä½¿ç¨èä¸­ï¼66.7% çäººèªçºç¿»è­¯å§å®¹æ¸æ¥ï¼ä¸¦æä¿¡å¿ä½¿ç¨éäºè¥ç©ã

##### **Text-to-SQL based on Large Language Models and Database Keyword Search**
2501.13594v1 by Eduardo R. Nascimento, Caio Viktor S. Avila, Yenier T. Izquierdo, Grettel M. GarcÃ­a, Lucas FeijÃ³ L. Andrade, Michelle S. P. Facina, Melissa Lemos, Marco A. Casanova

Text-to-SQL prompt strategies based on Large Language Models (LLMs) achieve
remarkable performance on well-known benchmarks. However, when applied to
real-world databases, their performance is significantly less than for these
benchmarks, especially for Natural Language (NL) questions requiring complex
filters and joins to be processed. This paper then proposes a strategy to
compile NL questions into SQL queries that incorporates a dynamic few-shot
examples strategy and leverages the services provided by a database keyword
search (KwS) platform. The paper details how the precision and recall of the
schema-linking process are improved with the help of the examples provided and
the keyword-matching service that the KwS platform offers. Then, it shows how
the KwS platform can be used to synthesize a view that captures the joins
required to process an input NL question and thereby simplify the SQL query
compilation step. The paper includes experiments with a real-world relational
database to assess the performance of the proposed strategy. The experiments
suggest that the strategy achieves an accuracy on the real-world relational
database that surpasses state-of-the-art approaches. The paper concludes by
discussing the results obtained.

æè¦ï¼åºæ¼å¤§åèªè¨æ¨¡å (LLM) çæå­è½ SQL æç¤ºç­ç¥å¨ç¾æå¨ç¥çåºæºä¸åå¾é¡¯èçæè½ãç¶èï¼ç¶æç¨æ¼çå¯¦ä¸ççè³æåº«æï¼å¶æè½é¡¯èä½æ¼éäºåºæºï¼ç¹å¥æ¯å°æ¼éè¦èçè¤éç¯©é¸å¨åè¯çµçèªç¶èªè¨ (NL) åé¡ãæ¬ææåºä¸åç­ç¥ï¼å° NL åé¡ç·¨è­¯æ SQL æ¥è©¢ï¼å¶ä¸­åå«åæå°æ¬¡ç¯ä¾ç­ç¥ï¼ä¸¦å©ç¨è³æåº«ééµå­æå° (KwS) å¹³å°æä¾çæåãæ¬æè©³ç´°èªªæå¦ä½å©ç¨æä¾çç¯ä¾å KwS å¹³å°æä¾çééµå­æ¯å°æåï¼ä¾æ¹åæ¨¡å¼é£çµéç¨çç²¾æºåº¦åå¬åçãç¶å¾ï¼å®å±ç¤ºå¦ä½ä½¿ç¨ KwS å¹³å°ä¾åæä¸åæª¢è¦ï¼ä»¥æ·åèçè¼¸å¥ NL åé¡æéçè¯çµï¼å¾èç°¡å SQL æ¥è©¢ç·¨è­¯æ­¥é©ãæ¬æåå«ä½¿ç¨çå¯¦ä¸çéä¿è³æåº«çå¯¦é©ï¼ä»¥è©ä¼°ææåºç­ç¥çæè½ãå¯¦é©è¡¨æï¼è©²ç­ç¥å¨çå¯¦ä¸çéä¿è³æåº«ä¸éå°çæºç¢ºåº¦è¶è¶äºæåé²çæ¹æ³ãæ¬ææå¾è¨è«æç²å¾ççµæã

##### **Contrastive Representation Learning Helps Cross-institutional Knowledge Transfer: A Study in Pediatric Ventilation Management**
2501.13587v1 by Yuxuan, Liu, Jinpei Han, Padmanabhan Ramnarayan, A. Aldo Faisal

Clinical machine learning deployment across institutions faces significant
challenges when patient populations and clinical practices differ
substantially. We present a systematic framework for cross-institutional
knowledge transfer in clinical time series, demonstrated through pediatric
ventilation management between a general pediatric intensive care unit (PICU)
and a cardiac-focused unit. Using contrastive predictive coding (CPC) for
representation learning, we investigate how different data regimes and
fine-tuning strategies affect knowledge transfer across institutional
boundaries. Our results show that while direct model transfer performs poorly,
CPC with appropriate fine-tuning enables effective knowledge sharing between
institutions, with benefits particularly evident in limited data scenarios.
Analysis of transfer patterns reveals an important asymmetry: temporal
progression patterns transfer more readily than point-of-care decisions,
suggesting practical pathways for cross-institutional deployment. Through a
systematic evaluation of fine-tuning approaches and transfer patterns, our work
provides insights for developing more generalizable clinical decision support
systems while enabling smaller specialized units to leverage knowledge from
larger centers.

æè¦ï¼è¨åºæ©å¨å­¸ç¿é¨ç½²å¨æ©æ§éé¢è¨éå¤§ææ°ï¼ç¶æ£èæç¾¤åè¨åºå¯¦åæé¡¯èå·®ç°æãæåæåºä¸åç¨æ¼è¨åºæéåºåçè·¨æ©æ§ç¥è­è½ç§»çç³»çµ±åæ¶æ§ï¼ééä¸è¬å°åå è­·çæ¿ (PICU) åå¿èå°ç§çæ¿ä¹éçåç§å¼å¸å¨ç®¡çå ä»¥è­æãä½¿ç¨å°æ¯é æ¸¬ç·¨ç¢¼ (CPC) é²è¡è¡¨å¾µå­¸ç¿ï¼æåæ¢è¨ä¸åçè³æå¶åº¦åå¾®èª¿ç­ç¥å¦ä½å½±é¿è·¨æ©æ§éççç¥è­è½ç§»ãæåççµæé¡¯ç¤ºï¼åç®¡ç´æ¥æ¨¡åè½ç§»å·è¡ä¸ä½³ï¼ä½ä½¿ç¨é©ç¶å¾®èª¿ç CPC è½å¤ å¨æ©æ§éé²è¡ææçç¥è­åäº«ï¼å¶å¥½èå¨æéè³ææå¢ä¸­ç¹å¥æé¡¯ãè½ç§»æ¨¡å¼åææ­é²äºä¸åéè¦çä¸å°ç¨±æ§ï¼æéé²ç¨æ¨¡å¼æ¯ç§è­·é»æ±ºç­æ´å®¹æè½ç§»ï¼éè¡¨ç¤ºè·¨æ©æ§é¨ç½²çå¯¦åéå¾ãééå¾®èª¿æ¹æ³åè½ç§»æ¨¡å¼çç³»çµ±æ§è©ä¼°ï¼æåçç ç©¶æä¾è¦è§£ï¼ç¨æ¼éç¼æ´å·æ¦æ¬æ§çè¨åºæ±ºç­æ¯æ´ç³»çµ±ï¼åæè®è¼å°çå°ç§å®ä½è½å¤ å©ç¨ä¾èªè¼å¤§ä¸­å¿çç¥è­ã

##### **Improving Contextual Faithfulness of Large Language Models via Retrieval Heads-Induced Optimization**
2501.13573v1 by Lei Huang, Xiaocheng Feng, Weitao Ma, Yuchun Fan, Xiachong Feng, Yangfan Ye, Weihong Zhong, Yuxuan Gu, Baoxin Wang, Dayong Wu, Guoping Hu, Bing Qin

Ensuring contextual faithfulness in retrieval-augmented large language models
(LLMs) is crucial for building trustworthy information-seeking systems,
particularly in long-form question-answering (LFQA) scenarios. In this work, we
identify a salient correlation between LFQA faithfulness and retrieval heads, a
set of attention heads responsible for retrieving contextual information.
Leveraging this insight, we propose RHIO, a framework designed to teach LLMs to
explicitly discriminate between faithful and unfaithful generations. RHIO first
augments unfaithful samples that simulate realistic model-intrinsic errors by
selectively masking retrieval heads. Then, these samples are incorporated into
joint training, enabling the model to distinguish unfaithful outputs from
faithful ones conditioned on control tokens. Furthermore, these control tokens
are leveraged to self-induce contrastive outputs, amplifying their difference
through contrastive decoding. Additionally, to facilitate the evaluation of
contextual faithfulness, we also introduce GroundBench, a comprehensive
benchmark compiled from five existing LFQA datasets. Extensive experimental
results on GroundBench demonstrate that RHIO significantly improves
faithfulness, even outperforming GPT-4o.

æè¦ï¼ç¢ºä¿å¨æª¢ç´¢å¢å¼·åå¤§åèªè¨æ¨¡å (LLM) ä¸­çä¸ä¸æä¿çåº¦ï¼å°æ¼å»ºç«å¼å¾ä¿¡è³´çè³è¨æå°ç³»çµ±è³ééè¦ï¼ç¹å¥æ¯å¨é·ç¯åç­ (LFQA) å ´æ¯ä¸­ãå¨éé å·¥ä½ä¸­ï¼æåç¼ç¾ LFQA ä¿çåº¦èæª¢ç´¢é ­ä¹éå­å¨é¡¯èç¸éæ§ï¼æª¢ç´¢é ­æ¯ä¸çµè² è²¬æª¢ç´¢ä¸ä¸æè³è¨çæ³¨æåé ­ãå©ç¨éé è¦è§£ï¼æåæåºäº RHIOï¼ä¸åæ¨å¨æå° LLM æç¢ºååä¿çåä¸ä¿çççæãRHIO é¦åééé¸ææ§é®è½æª¢ç´¢é ­ä¾æ´åæ¨¡æ¬å¯¦éæ¨¡åå§å¨é¯èª¤çä¸ä¿çæ¨£æ¬ãç¶å¾ï¼å°éäºæ¨£æ¬ç´å¥è¯åè¨ç·´ä¸­ï¼ä½¿æ¨¡åè½å¤ æ ¹ææ§å¶ä»£ç¢¼ååä¸ä¿ççè¼¸åºåä¿ççè¼¸åºãæ­¤å¤ï¼éäºæ§å¶ä»£ç¢¼è¢«ç¨æ¼èªæèªå°å°æ¯è¼¸åºï¼ééå°æ¯è§£ç¢¼æ¾å¤§å®åçå·®ç°ãæ­¤å¤ï¼çºäºä¿é²å°ä¸ä¸æä¿çåº¦çè©ä¼°ï¼æåéå¼å¥äº GroundBenchï¼ä¸åç±äºåç¾æ LFQA è³æéç·¨è­¯èæçç¶ååºæºãå¨ GroundBench ä¸çå»£æ³å¯¦é©çµæè¡¨æï¼RHIO å¤§å¹æåäºä¿çåº¦ï¼çè³åªæ¼ GPT-4oã

##### **K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor**
2501.13567v1 by Jeonghun Cho, Gary Geunbae Lee

Retrieval-augmented question answering (QA) integrates external information,
and thereby increases the QA accuracy of reader models that lack domain
knowledge. However, documents retrieved for closed domains require high
expertise, so the reader model may have difficulty fully comprehending the
text. Moreover, the retrieved documents contain thousands of tokens, some
unrelated to the question. As a result, the documents include some inaccurate
information, which could lead the reader model to mistrust the passages and
could result in hallucinations. To solve these problems, we propose K-COMP
(Knowledge-injected compressor) which provides the knowledge required to answer
correctly. The compressor automatically generates the requisite prior knowledge
to facilitate the answering process prior to the compression of retrieved
passages. Subsequently, the passages are compressed autoregressively, with the
generated knowledge being integrated into the compression process. This process
ensures alignment between the question intent and the compressed context. By
augmenting this prior knowledge and concise context, the reader models are
guided toward relevant answers and trust the context.

æè¦ï¼æª¢ç´¢å¢å¼·ååç­ï¼QAï¼æ´åå¤é¨è³è¨ï¼å¾èæé«ç¼ºä¹é åç¥è­çè®åå¨æ¨¡åç QA ç²¾ç¢ºåº¦ãç¶èï¼éå°å°éé åæª¢ç´¢çææªéè¦å¾é«çå°æ¥­ç¥è­ï¼å æ­¤è®åå¨æ¨¡åå¯è½æé£ä»¥å®å¨çè§£æå­ãæ­¤å¤ï¼æª¢ç´¢å°çææªåå«æ¸ååä»£ç¢¼ï¼å¶ä¸­ä¸äºèåé¡ç¡éãå æ­¤ï¼ææªåå«ä¸äºä¸æºç¢ºçè³è¨ï¼éå¯è½æå°è´è®åå¨æ¨¡åä¸ä¿¡ä»»æ®µè½ï¼ä¸¦å¯è½å°è´å¹»è¦ºãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäº K-COMPï¼ç¥è­æ³¨å¥å£ç¸®å¨ï¼ï¼å®æä¾äºæ­£ç¢ºåç­æéç¥è­ãå£ç¸®å¨æèªåç¢çå¿è¦çååç¥è­ï¼ä»¥ä¾¿å¨å£ç¸®æª¢ç´¢å°çæ®µè½ä¹ååå©åç­ç¨åºãé¨å¾ï¼éäºæ®µè½æèªååæ­¸å£ç¸®ï¼ä¸¦å°ç¢ççç¥è­æ´åå°å£ç¸®ç¨åºä¸­ãæ­¤ç¨åºç¢ºä¿åé¡æåèå£ç¸®å§å®¹ä¹éçä¸è´æ§ãééæ´åæ­¤ååç¥è­åç°¡æ½çå§å®¹ï¼è®åå¨æ¨¡åæè¢«å¼å°è³ç¸éç­æ¡ï¼ä¸¦ä¿¡ä»»å§å®¹ã

##### **Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving**
2501.13563v1 by Lu Wang, Tianyuan Zhang, Yang Qu, Siyuan Liang, Yuwei Chen, Aishan Liu, Xianglong Liu, Dacheng Tao

Vision-language models (VLMs) have significantly advanced autonomous driving
(AD) by enhancing reasoning capabilities; however, these models remain highly
susceptible to adversarial attacks. While existing research has explored
white-box attacks to some extent, the more practical and challenging black-box
scenarios remain largely underexplored due to their inherent difficulty. In
this paper, we take the first step toward designing black-box adversarial
attacks specifically targeting VLMs in AD. We identify two key challenges for
achieving effective black-box attacks in this context: the effectiveness across
driving reasoning chains in AD systems and the dynamic nature of driving
scenarios. To address this, we propose Cascading Adversarial Disruption (CAD).
It first introduces Decision Chain Disruption, which targets low-level
reasoning breakdown by generating and injecting deceptive semantics, ensuring
the perturbations remain effective across the entire decision-making chain.
Building on this, we present Risky Scene Induction, which addresses dynamic
adaptation by leveraging a surrogate VLM to understand and construct high-level
risky scenarios that are likely to result in critical errors in the current
driving contexts. Extensive experiments conducted on multiple AD VLMs and
benchmarks demonstrate that CAD achieves state-of-the-art attack effectiveness,
significantly outperforming existing methods (+13.43% on average). Moreover, we
validate its practical applicability through real-world attacks on AD vehicles
powered by VLMs, where the route completion rate drops by 61.11% and the
vehicle crashes directly into the obstacle vehicle with adversarial patches.
Finally, we release CADA dataset, comprising 18,808 adversarial
visual-question-answer pairs, to facilitate further evaluation and research in
this critical domain. Our codes and dataset will be available after paper's
acceptance.

æè¦ï¼<paragraph>è¦è¦ºèªè¨æ¨¡å (VLM) ééå¢å¼·æ¨çè½åï¼é¡¯èå°æåäºèªåé§é§ (AD) æè¡ï¼ç¶èï¼éäºæ¨¡åä»ç¶æ¥µæåå°å°ææ§æ»æãéç¶ç¾æç ç©¶å·²å¨ä¸å®ç¨åº¦ä¸æ¢è¨äºç½çæ»æï¼ä½æ´å¯¦ç¨ä¸æ´å·ææ°æ§çé»çæå¢ç±æ¼å¶å§å¨çå°é£æ§ï¼å¨å¾å¤§ç¨åº¦ä¸ä»æªå¾å°ååæ¢è¨ãå¨æ¬æä¸­ï¼æåéåºäºè¨­è¨å°ééå° AD ä¸­ VLM çé»çå°ææ§æ»æçç¬¬ä¸æ­¥ãæåç¢ºå®äºå¨éç¨®ææ³ä¸å¯¦ç¾ææé»çæ»æçå©åééµææ°ï¼AD ç³»çµ±ä¸­é§é§æ¨çéçæææ§ä»¥åé§é§å ´æ¯çåæç¹æ§ãçºäºè§£æ±ºéååé¡ï¼æåæåºäºä¸²è¯å°æå¹²æ¾ (CAD)ãå®é¦åå¼å¥äºæ±ºç­éå¹²æ¾ï¼ééçæåæ³¨å¥æ¬ºé¨æ§èªç¾©ä¾éå°ä½å±¤ç´æ¨çæéï¼ç¢ºä¿æ¾åå¨æ´åæ±ºç­éä¸­ä¿ææææ§ãå¨æ­¤åºç¤ä¸ï¼æåæåºäºé¢¨éªå ´æ¯èªå°ï¼ééå©ç¨ä»£ç VLM ä¾çè§£åæ§å»ºé«å±¤ç´é¢¨éªå ´æ¯ï¼ä»¥æå°åæé©æï¼éäºå ´æ¯å¾å¯è½å°è´ç¶åé§é§ç°å¢ä¸­çå´éé¯èª¤ãå¨å¤å AD VLM ååºæºä¸é²è¡çå»£æ³å¯¦é©è¡¨æï¼CAD éå°äºæåé²çæ»æææï¼é¡¯èåªæ¼ç¾ææ¹æ³ï¼å¹³åæé«äº +13.43%ï¼ãæ­¤å¤ï¼æåééå°ç± VLM é©åç AD è»è¼é²è¡å¯¦éæ»æä¾é©è­å¶å¯¦ç¨æ§ï¼å¶ä¸­è·¯ç·å®æçä¸éäº 61.11%ï¼ä¸¦ä¸è»è¼ç´æ¥æå°äºè²¼æå°ææ§è£ä¸çéç¤ç©è»è¼ãæå¾ï¼æåç¼å¸äº CADA æ¸æéï¼å¶ä¸­åå« 18,808 åå°ææ§è¦è¦ºåé¡ç­æ¡å°ï¼ä»¥ä¿é²å¨éåééµé åçé²ä¸æ­¥è©ä¼°åç ç©¶ãæåçä»£ç¢¼åæ¸æéå°å¨è«æè¢«æ¥åå¾æä¾ã</paragraph>

##### **One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt**
2501.13554v1 by Tao Liu, Kai Wang, Senmao Li, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang, Ming-Ming Cheng

Text-to-image generation models can create high-quality images from input
prompts. However, they struggle to support the consistent generation of
identity-preserving requirements for storytelling. Existing approaches to this
problem typically require extensive training in large datasets or additional
modifications to the original model architectures. This limits their
applicability across different domains and diverse diffusion model
configurations. In this paper, we first observe the inherent capability of
language models, coined context consistency, to comprehend identity through
context with a single prompt. Drawing inspiration from the inherent context
consistency, we propose a novel training-free method for consistent
text-to-image (T2I) generation, termed "One-Prompt-One-Story" (1Prompt1Story).
Our approach 1Prompt1Story concatenates all prompts into a single input for T2I
diffusion models, initially preserving character identities. We then refine the
generation process using two novel techniques: Singular-Value Reweighting and
Identity-Preserving Cross-Attention, ensuring better alignment with the input
description for each frame. In our experiments, we compare our method against
various existing consistent T2I generation approaches to demonstrate its
effectiveness through quantitative metrics and qualitative assessments. Code is
available at https://github.com/byliutao/1Prompt1Story.

æè¦ï¼æå­è½ååçææ¨¡åå¯ä»¥æ ¹æè¼¸å¥æç¤ºå»ºç«é«åè³ªå½±åãç¶èï¼å®åé£ä»¥æ¯æ´æçºç¢çèªªæäºæéçç¶­æèº«åè¦æ±ãç¾æçè§£æ±ºæ¹æ³éå¸¸éè¦å¤§éè³æéçè¨ç·´æå°åå§æ¨¡åæ¶æ§é²è¡é¡å¤ä¿®æ¹ãééå¶äºå®åå¨ä¸åé ååå¤ç¨®æ´æ£æ¨¡åçµæä¸­çé©ç¨æ§ãå¨æ¬æä¸­ï¼æåé¦åè§å¯å°èªè¨æ¨¡åçå§å¨è½åï¼å³æè¬çèçµ¡ä¸è´æ§ï¼å¯ééå®ä¸æç¤ºééèçµ¡çè§£èº«åãå¾èçµ¡ä¸è´æ§ä¸­æ±²åéæï¼æåæåºäºä¸ç¨®æ°ç©çç¡è¨ç·´æ¹æ³ï¼ç¨æ¼æçºçæå­è½åå (T2I) çæï¼ç¨±çºãä¸æç¤ºä¸æäºã(1Prompt1Story)ãæåç 1Prompt1Story æ¹æ³å°æææç¤ºä¸²æ¥æ T2I æ´æ£æ¨¡åçå®ä¸è¼¸å¥ï¼æåä¿çè§è²èº«åãç¶å¾ï¼æåä½¿ç¨å©ç¨®æ°æè¡èª¿æ´çæç¨åºï¼å¥ç°å¼éæ°å æ¬åç¶­æèº«åçäº¤åæ³¨æï¼ç¢ºä¿èæ¯åç«é¢çè¼¸å¥æè¿°ææ´å¥½çå°é½ãå¨æåçå¯¦é©ä¸­ï¼æåå°æåçæ¨¡åèåç¨®ç¾æçæçº T2I çææ¹æ³é²è¡æ¯è¼ï¼ä»¥éééåææ¨åå®æ§è©ä¼°è­æå¶æææ§ãç¨å¼ç¢¼å¯æ¼ https://github.com/byliutao/1Prompt1Story åå¾ã

##### **Explainable AI-aided Feature Selection and Model Reduction for DRL-based V2X Resource Allocation**
2501.13552v1 by Nasir Khan, Asmaa Abdallah, Abdulkadir Celik, Ahmed M. Eltawil, Sinem Coleri

Artificial intelligence (AI) is expected to significantly enhance radio
resource management (RRM) in sixth-generation (6G) networks. However, the lack
of explainability in complex deep learning (DL) models poses a challenge for
practical implementation. This paper proposes a novel explainable AI (XAI)-
based framework for feature selection and model complexity reduction in a
model-agnostic manner. Applied to a multi-agent deep reinforcement learning
(MADRL) setting, our approach addresses the joint sub-band assignment and power
allocation problem in cellular vehicle-to-everything (V2X) communications. We
propose a novel two-stage systematic explainability framework leveraging
feature relevance-oriented XAI to simplify the DRL agents. While the former
stage generates a state feature importance ranking of the trained models using
Shapley additive explanations (SHAP)-based importance scores, the latter stage
exploits these importance-based rankings to simplify the state space of the
agents by removing the least important features from the model input.
Simulation results demonstrate that the XAI-assisted methodology achieves 97%
of the original MADRL sum-rate performance while reducing optimal state
features by 28%, average training time by 11%, and trainable weight parameters
by 46% in a network with eight vehicular pairs.

æè¦ï¼äººå·¥æºè½ (AI) é è¨å°å¤§å¹æåç¬¬å­ä»£ (6G) ç¶²è·¯ä¸­çç¡ç·è³æºç®¡ç (RRM)ãç¶èï¼è¤éæ·±åº¦å­¸ç¿ (DL) æ¨¡åç¼ºä¹å¯è§£éæ§ï¼å°å¯¦éæç¨æ§æææ°ãæ¬ææåºä¸åæ°ç©çå¯è§£é AI (XAI) åºç¤æ¶æ§ï¼ä»¥æ¨¡åä¸å¯ç¥çæ¹å¼é²è¡ç¹å¾µé¸æåæ¨¡åè¤éåº¦éä½ãæç¨æ¼å¤æºè½é«æ·±åº¦å¼·åå­¸ç¿ (MADRL) è¨­å®ï¼æåçåæ³èçèå·¢è»å°è¬ç© (V2X) éè¨ä¸­çè¯åå­é »æ®µåéååçéç½®åé¡ãæåæåºä¸åæ°ç©çå©éæ®µç³»çµ±å¯è§£éæ§æ¶æ§ï¼å©ç¨é¢åç¹å¾µç¸éæ§ç XAI ä¾ç°¡å DRL æºè½é«ãéç¶åä¸éæ®µä½¿ç¨ Shapley å æ§è§£é (SHAP) çºåºç¤çéè¦æ§åæ¸ï¼ç¢çè¨ç·´æ¨¡åççæç¹å¾µéè¦æ§æåï¼ä½å¾ä¸éæ®µå©ç¨éäºåºæ¼éè¦æ§çæåï¼ééç§»é¤æ¨¡åè¼¸å¥ä¸­æéè¦çç¹å¾µä¾ç°¡åæºè½é«ççæç©ºéãæ¨¡æ¬çµæé¡¯ç¤ºï¼XAI åå©çæ¹æ³å¨å«åè»è¼å°çç¶²è·¯ä¸­ï¼éå°åå§ MADRL ç¸½åçæè½ç 97%ï¼åæå°æä½³çæç¹å¾µæ¸å° 28%ãå¹³åè¨ç·´æéæ¸å° 11%ï¼ä»¥åå¯è¨ç·´æ¬éåæ¸æ¸å° 46%ã

##### **LLMs Can Plan Only If We Tell Them**
2501.13545v1 by Bilgehan Sel, Ruoxi Jia, Ming Jin

Large language models (LLMs) have demonstrated significant capabilities in
natural language processing and reasoning, yet their effectiveness in
autonomous planning has been under debate. While existing studies have utilized
LLMs with external feedback mechanisms or in controlled environments for
planning, these approaches often involve substantial computational and
development resources due to the requirement for careful design and iterative
backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to
match human performance on standard planning benchmarks, such as the
Blocksworld, without additional support. This paper investigates whether LLMs
can independently generate long-horizon plans that rival human baselines. Our
novel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help
achieve state-of-the-art results in planning benchmarks out-competing prior
methods and human baselines all autonomously.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨èªç¶èªè¨èçåæ¨çæ¹é¢å±ç¤ºäºé¡¯èçè½åï¼ä½å®åå¨èªä¸»è¦åä¸­çæææ§ä¸ç´å­å¨ç­è­°ãåç®¡ç¾æç ç©¶å·²å° LLM èå¤é¨åé¥æ©å¶çµåä½¿ç¨ï¼æå¨åæ§ç°å¢ä¸­é²è¡è¦åï¼ä½ç±æ¼éè¦ä»ç´°è¨­è¨ååè¦æç¤ºï¼éäºæ¹æ³éå¸¸æ¶åå¤§éçè¨ç®åéç¼è³æºãæ­¤å¤ï¼å³ä½¿æ¯æåé²ç LLMï¼ä¾å¦ GPT-4ï¼å¨æ²æé¡å¤æ¯æ´çææ³ä¸ï¼ä¹å¾é£å¨æ¨æºè¦ååºæºï¼ä¾å¦ Blocksworldï¼ä¸éå°äººé¡çè¡¨ç¾ãæ¬ææ¢è¨ LLM æ¯å¦è½ç¨ç«çæèäººé¡åºæºç¸åª²ç¾çé·é è¨ç«ãæåå°ææ³æ¼ç®æ³ (AoT) çåµæ°å¼·åï¼æåç¨±ä¹çº AoT+ï¼æå©æ¼å¨è¦ååºæºä¸­åå¾æåé²çææï¼å¨å®å¨èªä¸»çææ³ä¸åéååçåç¨®æ¹æ³åäººé¡åºæºã

##### **ReasVQA: Advancing VideoQA with Imperfect Reasoning Process**
2501.13536v1 by Jianxin Liang, Xiaojun Meng, Huishuai Zhang, Yueqian Wang, Jiansheng Wei, Dongyan Zhao

Video Question Answering (VideoQA) is a challenging task that requires
understanding complex visual and temporal relationships within videos to answer
questions accurately. In this work, we introduce \textbf{ReasVQA}
(Reasoning-enhanced Video Question Answering), a novel approach that leverages
reasoning processes generated by Multimodal Large Language Models (MLLMs) to
improve the performance of VideoQA models. Our approach consists of three
phases: reasoning generation, reasoning refinement, and learning from
reasoning. First, we generate detailed reasoning processes using additional
MLLMs, and second refine them via a filtering step to ensure data quality.
Finally, we use the reasoning data, which might be in an imperfect form, to
guide the VideoQA model via multi-task learning, on how to interpret and answer
questions based on a given video. We evaluate ReasVQA on three popular
benchmarks, and our results establish new state-of-the-art performance with
significant improvements of +2.9 on NExT-QA, +7.3 on STAR, and +5.9 on
IntentQA. Our findings demonstrate the supervising benefits of integrating
reasoning processes into VideoQA. Further studies validate each component of
our method, also with different backbones and MLLMs, and again highlight the
advantages of this simple but effective method. We offer a new perspective on
enhancing VideoQA performance by utilizing advanced reasoning techniques,
setting a new benchmark in this research field.

æè¦ï¼å½±çåç­ (VideoQA) æ¯ä¸é å·æææ°æ§çä»»åï¼éè¦äºè§£å½±çä¸­è¤éçè¦è¦ºåæééä¿æè½æºç¢ºåç­åé¡ãå¨æ­¤ç ç©¶ä¸­ï¼æåä»ç´¹äº \textbf{ReasVQA}ï¼æ¨çå¢å¼·å½±çåç­ï¼ï¼éæ¯ä¸ç¨®æ°ç©çæ¹æ³ï¼å©ç¨å¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM) çæçæ¨çç¨åºä¾æå VideoQA æ¨¡åçæè½ãæåçåæ³åå«ä¸åéæ®µï¼æ¨ççæãæ¨çç²¾çåå¾æ¨çä¸­å­¸ç¿ãé¦åï¼æåä½¿ç¨é¡å¤ç MLLM çæè©³ç´°çæ¨çç¨åºï¼ç¶å¾éééæ¿¾æ­¥é©ç²¾çå®åä»¥ç¢ºä¿è³æåè³ªãæå¾ï¼æåä½¿ç¨æ¨çè³æï¼å¯è½æ¯ä¸å®ç¾çå½¢å¼ï¼ééå¤ä»»åå­¸ç¿ä¾å¼å° VideoQA æ¨¡åï¼äºè§£å¦ä½æ ¹æçµ¦å®çå½±çè©®éååç­åé¡ãæåå¨ä¸åç±éåºæºä¸è©ä¼° ReasVQAï¼æåççµæå»ºç«äºæ°çæåé²æè½ï¼å¨ NExT-QA ä¸é¡¯èæå +2.9ãå¨ STAR ä¸æå +7.3ï¼ä»¥åå¨ IntentQA ä¸æå +5.9ãæåçç¼ç¾è­æäºå°æ¨çç¨åºæ´åå° VideoQA ä¸­çç£ç£æçãé²ä¸æ­¥çç ç©¶é©è­äºæåæ¹æ³çæ¯åçµæé¨åï¼ä¹é©è­äºä¸åçä¸»å¹¹å MLLMï¼ä¸¦åæ¬¡å¼·èª¿äºéç¨®ç°¡å®ä½æææ¹æ³çåªé»ãæåæä¾äºå©ç¨åé²æ¨çæè¡ä¾æå VideoQA æè½çæ°è§é»ï¼çºéåç ç©¶é åè¨­å®äºæ°çåºæºã

##### **Towards a Theory of AI Personhood**
2501.13533v1 by Francis Rhys Ward

I am a person and so are you. Philosophically we sometimes grant personhood
to non-human animals, and entities such as sovereign states or corporations can
legally be considered persons. But when, if ever, should we ascribe personhood
to AI systems? In this paper, we outline necessary conditions for AI
personhood, focusing on agency, theory-of-mind, and self-awareness. We discuss
evidence from the machine learning literature regarding the extent to which
contemporary AI systems, such as language models, satisfy these conditions,
finding the evidence surprisingly inconclusive.
  If AI systems can be considered persons, then typical framings of AI
alignment may be incomplete. Whereas agency has been discussed at length in the
literature, other aspects of personhood have been relatively neglected. AI
agents are often assumed to pursue fixed goals, but AI persons may be
self-aware enough to reflect on their aims, values, and positions in the world
and thereby induce their goals to change. We highlight open research directions
to advance the understanding of AI personhood and its relevance to alignment.
Finally, we reflect on the ethical considerations surrounding the treatment of
AI systems. If AI systems are persons, then seeking control and alignment may
be ethically untenable.

æè¦ï¼ææ¯äººï¼ä½ ä¹æ¯ãä»å²å­¦çè§åº¦æ¥è¯´ï¼æä»¬ææ¶ä¼èµäºéäººç±»å¨ç©ä»¥äººæ ¼ï¼èä¸»æå½å®¶æå¬å¸ç­å®ä½å¨æ³å¾ä¸å¯ä»¥è¢«è§ä¸ºäººæ ¼ãä½æ¯ï¼æä»¬ä»ä¹æ¶ååºè¯¥å°äººæ ¼å½äºäººå·¥æºè½ç³»ç»ï¼å¨æ¬æä¸­ï¼æä»¬æ¦è¿°äºäººå·¥æºè½äººæ ¼çå¿è¦æ¡ä»¶ï¼éç¹å³æ³¨è½å¨æ§ãå¿æºçè®ºåèªææè¯ãæä»¬è®¨è®ºäºæºå¨å­¦ä¹ æç®ä¸­å³äºå½ä»£äººå·¥æºè½ç³»ç»ï¼å¦è¯­è¨æ¨¡åï¼æ»¡è¶³è¿äºæ¡ä»¶çç¨åº¦çè¯æ®ï¼åç°è¯æ®ä»¤äººæè®¶å°æ²¡æå®è®ºã
å¦æäººå·¥æºè½ç³»ç»å¯ä»¥è¢«è§ä¸ºäººæ ¼ï¼é£ä¹äººå·¥æºè½å¯¹é½çå¸åæ¡æ¶å¯è½æ¯ä¸å®æ´çãè½ç¶è½å¨æ§å¨æç®ä¸­å·²è¢«å¹¿æ³è®¨è®ºï¼ä½äººæ ¼çå¶ä»æ¹é¢å´ç¸å¯¹è¢«å¿½è§ãäººä»¬éå¸¸è®¤ä¸ºäººå·¥æºè½ä»£çä¼è¿½æ±æ¢å®çç®æ ï¼ä½äººå·¥æºè½äººæ ¼å¯è½å·æè¶³å¤çèªææè¯æ¥åæå¶ç®æ ãä»·å¼è§åå¨ä¸çä¸­çä½ç½®ï¼å¹¶ç±æ­¤è¯±å¯¼å¶ç®æ åçæ¹åãæä»¬å¼ºè°äºå¼æ¾çç ç©¶æ¹åï¼ä»¥ä¿è¿å¯¹äººå·¥æºè½äººæ ¼åå¶ä¸å¯¹é½ç¸å³æ§ççè§£ã
æåï¼æä»¬åæäºå´ç»äººå·¥æºè½ç³»ç»å¯¹å¾çä¼¦çèéãå¦æäººå·¥æºè½ç³»ç»æ¯äººæ ¼ï¼é£ä¹å¯»æ±æ§å¶åå¯¹é½å¨ä¼¦çä¸å¯è½æ¯ç«ä¸ä½èçã

##### **DQ-Data2vec: Decoupling Quantization for Multilingual Speech Recognition**
2501.13497v1 by Qijie Shao, Linhao Dong, Kun Wei, Sining Sun, Lei Xie

Data2vec is a self-supervised learning (SSL) approach that employs a
teacher-student architecture for contextual representation learning via masked
prediction, demonstrating remarkable performance in monolingual ASR. Previous
studies have revealed that data2vec's shallow layers capture speaker and
language information, middle layers encode phoneme and word features, while
deep layers are responsible for reconstruction. Language and phoneme features
are crucial for multilingual ASR. However, data2vec's masked representation
generation relies on multi-layer averaging, inevitably coupling these features.
To address this limitation, we propose a decoupling quantization based data2vec
(DQ-Data2vec) for multilingual ASR, which includes a data2vec backbone and two
improved online K-means quantizers. Our core idea is using the K-means
quantizer with specified cluster numbers to decouple language and phoneme
information for masked prediction. Specifically, in the language quantization,
considering that the number of languages is significantly different from other
irrelevant features (e.g., speakers), we assign the cluster number to match the
number of languages, explicitly decoupling shallow layers' language-related
information from irrelevant features. This strategy is also applied to
decoupling middle layers' phoneme and word features. In a self-supervised
scenario, experiments on the CommonVoice dataset demonstrate that DQ-Data2vec
achieves a relative reduction of 9.51% in phoneme error rate (PER) and 11.58%
in word error rate (WER) compared to data2vec and UniData2vec. Moreover, in a
weakly-supervised scenario incorporating language labels and high-resource
language text labels, the relative reduction is 18.09% and 1.55%, respectively.

æè¦ï¼Data2vec æ¯ä¸ç¨®èªç£ç£å­¸ç¿ (SSL) æ¹æ³ï¼å®æ¡ç¨æå¸«-å­¸çæ¶æ§é²è¡æå¢è¡¨å¾µå­¸ç¿ï¼ééé®ç½©é æ¸¬ä¾è¡¨ç¾ï¼å¨å®èªé³ ASR ä¸­è¡¨ç¾åè¶ãååç ç©¶å·²æ­ç¤ºï¼data2vec çæ·ºå±¤ææ·åèªªè©±èåèªè¨è³è¨ï¼ä¸­éå±¤æç·¨ç¢¼é³ç´ åå®å­ç¹å¾µï¼èæ·±å±¤åè² è²¬éå»ºãèªè¨åé³ç´ ç¹å¾µå°æ¼å¤èªé³ ASR è³ééè¦ãç¶èï¼data2vec çé®ç½©è¡¨å¾µç¢çä¾è³´æ¼å¤å±¤å¹³åï¼ä¸å¯é¿åå°æçµåéäºç¹å¾µãçºäºè§£æ±ºéåéå¶ï¼æåéå°å¤èªé³ ASR æåºäºä¸åå»è¦éååºæ¼ data2vec (DQ-Data2vec)ï¼å¶ä¸­åå«ä¸å data2vec ä¸»å¹¹åå©åæ¹é²çç·ä¸ K å¹³åéåå¨ãæåçæ ¸å¿æ¦å¿µæ¯ä½¿ç¨å·ææå®å¢éæ¸éç K å¹³åéåå¨ï¼çºé®ç½©é æ¸¬å»è¦èªè¨åé³ç´ è³è¨ãç¹å¥æ¯å¨èªè¨éåä¸­ï¼èæ®å°èªè¨æ¸éèå¶ä»ç¡éç¹å¾µï¼ä¾å¦èªªè©±èï¼æé¡¯èä¸åï¼æåå°å¢éæ¸éæå®çºèèªè¨æ¸éç¸ç¬¦ï¼æç¢ºå°å°æ·ºå±¤èèªè¨ç¸éçè³è¨èç¡éç¹å¾µå»è¦ãæ­¤ç­ç¥ä¹æç¨æ¼å»è¦ä¸­éå±¤çé³ç´ åå®å­ç¹å¾µãå¨èªç£ç£å ´æ¯ä¸­ï¼CommonVoice è³æéçå¯¦é©è­æï¼è data2vec å UniData2vec ç¸æ¯ï¼DQ-Data2vec å¨é³ç´ é¯èª¤ç (PER) ä¸­éå° 9.51% çç¸å°æ¸å°ï¼å¨å®å­é¯èª¤ç (WER) ä¸­éå° 11.58% çç¸å°æ¸å°ãæ­¤å¤ï¼å¨çµåèªè¨æ¨ç±¤åé«è³æºèªè¨æå­æ¨ç±¤çå¼±ç£ç£å ´æ¯ä¸­ï¼ç¸å°æ¸å°åå¥çº 18.09% å 1.55%ã

##### **GCAD: Anomaly Detection in Multivariate Time Series from the Perspective of Granger Causality**
2501.13493v1 by Zehao Liu, Mengzhou Gao, Pengfei Jiao

Multivariate time series anomaly detection has numerous real-world
applications and is being extensively studied. Modeling pairwise correlations
between variables is crucial. Existing methods employ learnable graph
structures and graph neural networks to explicitly model the spatial
dependencies between variables. However, these methods are primarily based on
prediction or reconstruction tasks, which can only learn similarity
relationships between sequence embeddings and lack interpretability in how
graph structures affect time series evolution. In this paper, we designed a
framework that models spatial dependencies using interpretable causal
relationships and detects anomalies through changes in causal patterns.
Specifically, we propose a method to dynamically discover Granger causality
using gradients in nonlinear deep predictors and employ a simple sparsification
strategy to obtain a Granger causality graph, detecting anomalies from a causal
perspective. Experiments on real-world datasets demonstrate that the proposed
model achieves more accurate anomaly detection compared to baseline methods.

æè¦ï¼å¤åæéåºåç°å¸¸åµæ¸¬æè¨±å¤å¯¦éæç¨ï¼ä¸å»£æ³åå°ç ç©¶ãå°è®æ¸ä¹éæå°ç¸éæ§çå»ºæ¨¡è³ééè¦ãç¾ææ¹æ³æ¡ç¨å¯å­¸ç¿åå½¢çµæ§ååå½¢ç¥ç¶ç¶²è·¯ä¾æç¢ºå»ºæ¨¡è®æ¸ä¹éçç©ºéä¾è³´æ§ãç¶èï¼éäºæ¹æ³ä¸»è¦åºæ¼é æ¸¬æéå»ºä»»åï¼åªè½å­¸ç¿åºååµå¥ä¹éçç¸ä¼¼æ§éä¿ï¼ä¸ç¼ºä¹åå½¢çµæ§å¦ä½å½±é¿æéåºåæ¼åçå¯è§£éæ§ãå¨æ¬æä¸­ï¼æåè¨­è¨äºä¸åæ¡æ¶ï¼ä½¿ç¨å¯è§£éå æéä¿ä¾å»ºæ¨¡ç©ºéä¾è³´æ§ï¼ä¸¦ééå ææ¨¡å¼çè®åä¾åµæ¸¬ç°å¸¸ãå·é«ä¾èªªï¼æåæåºä¸åæ¹æ³ï¼ä½¿ç¨éç·æ§æ·±åº¦é æ¸¬å¨ä¸­çæ¢¯åº¦ä¾åæç¼ç¾ Granger å æéä¿ï¼ä¸¦æ¡ç¨ä¸åç°¡å®çç¨çåç­ç¥ä¾ç²å Granger å æéä¿åï¼å¾å æè§åº¦åµæ¸¬ç°å¸¸ãå¨çå¯¦ä¸çè³æéä¸çå¯¦é©è­æï¼èåºæºæ¹æ³ç¸æ¯ï¼ææåºçæ¨¡åå¯éææ´æºç¢ºçç°å¸¸åµæ¸¬ã

##### **RECALL: Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles**
2501.13491v1 by Munachiso Nwadike, Zangir Iklassov, Toluwani Aremu, Tatsuya Hiraoka, Velibor Bojkovic, Benjamin Heinzerling, Hilal Alqaubeh, Martin TakÃ¡Ä, Kentaro Inui

We introduce the concept of the self-referencing causal cycle (abbreviated
RECALL) - a mechanism that enables large language models (LLMs) to bypass the
limitations of unidirectional causality, which underlies a phenomenon known as
the reversal curse. When an LLM is prompted with sequential data, it often
fails to recall preceding context. For example, when we ask an LLM to recall
the line preceding "O say does that star-spangled banner yet wave" in the U.S.
National Anthem, it often fails to correctly return "Gave proof through the
night that our flag was still there" - this is due to the reversal curse. It
occurs because language models such as ChatGPT and Llama generate text based on
preceding tokens, requiring facts to be learned and reproduced in a consistent
token order. While the reversal curse is often viewed as a limitation, we offer
evidence of an alternative view: it is not always an obstacle in practice. We
find that RECALL is driven by what we designate as cycle tokens - sequences
that connect different parts of the training data, enabling recall of preceding
tokens from succeeding ones. Through rigorous probabilistic formalization and
controlled experiments, we demonstrate how the cycles they induce influence a
model's ability to reproduce information. To facilitate reproducibility, we
provide our code and experimental details at
https://anonymous.4open.science/r/remember-B0B8/.

æè¦ï¼æåå¼å¥äºèªåç§å æå¾ªç° (ç°¡ç¨± RECALL) çæ¦å¿µï¼éæ¯ä¸ç¨®æ©å¶ï¼å¯è®å¤§åèªè¨æ¨¡å (LLM) ç¹éå®åå æéä¿çéå¶ï¼èå®åå æéä¿æ¯å°è´éè½è©åç¾è±¡çåºç¤ãç¶ LLM åå°é åºè³ææç¤ºæï¼å®ç¶å¸¸ç¡æ³åæ¶åé¢çå§å®¹ãä¾å¦ï¼ç¶æåè¦æ± LLM åæ¶ç¾ååæ­ä¸­ãO say does that star-spangled banner yet waveãåä¸å¥æï¼å®ç¶å¸¸ç¡æ³æ­£ç¢ºå°åç­ãGave proof through the night that our flag was still thereãââéæ¯å çºéè½è©åãéæç¼çï¼æ¯å çºå ChatGPT å Llama éæ¨£çèªè¨æ¨¡åææ ¹æåé¢çè©å½ç¢çæå­ï¼éè¦ä»¥ä¸è´çè©å½é åºå­¸ç¿åè¤è£½äºå¯¦ãéç¶éè½è©åéå¸¸è¢«è¦çºä¸ç¨®éå¶ï¼ä½æåæä¾äºå¦ä¸ç¨®è§é»çè­æï¼å¨å¯¦åä¸ï¼å®ä¸¦éç¸½æ¯éç¤ãæåç¼ç¾ RECALL æ¯ç±æåæå®çå¾ªç°è©å½æ¨åçï¼å¾ªç°è©å½æ¯å°è¨ç·´è³æçä¸åé¨åé£æ¥èµ·ä¾çåºåï¼å¯å¾å¾çºè©å½ä¸­åæ¶åé¢çè©å½ãééå´è¬¹çæ©çå½¢å¼åååæ§å¯¦é©ï¼æåå±ç¤ºäºå®åå¼ç¼çå¾ªç°å¦ä½å½±é¿æ¨¡åè¤è£½è³è¨çè½åãçºäºä¿é²éç¾æ§ï¼æåå¨ https://anonymous.4open.science/r/remember-B0B8/ æä¾æåçç¨å¼ç¢¼åå¯¦é©è©³ç´°è³è¨ã

##### **MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods**
2501.13484v1 by Zukang Xu, Yuxuan Yue, Xing Hu, Zhihang Yuan, Zixu Jiang, Zhixuan Chen, Jiangyong Yu, Chen Xu, Sifan Zhou, Dawei Yang

Mamba is an efficient sequence model that rivals Transformers and
demonstrates significant potential as a foundational architecture for various
tasks. Quantization is commonly used in neural networks to reduce model size
and computational latency. However, applying quantization to Mamba remains
underexplored, and existing quantization methods, which have been effective for
CNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot
suffers a 21% accuracy drop on Vim-T$^\dagger$ even under W8A8). We have
pioneered the exploration of this issue and identified several key challenges.
First, significant outliers are present in gate projections, output
projections, and matrix multiplications. Second, Mamba's unique parallel scan
further amplifies these outliers, leading to uneven and heavy-tailed data
distributions. Third, even with the application of the Hadamard transform, the
variance across channels in weights and activations still remains inconsistent.
To these ends, we propose MambaQuant, a post-training quantization (PTQ)
framework consisting of: 1) Karhunen-Loeve Transformation (KLT) enhanced
rotation, rendering the rotation matrix adaptable to diverse channel
distributions. 2) Smooth-Fused rotation, which equalizes channel variances and
can merge additional parameters into model weights. Experiments show that
MambaQuant can quantize both weights and activations into 8-bit with less than
1% accuracy loss for Mamba-based vision and language tasks. To the best of our
knowledge, MambaQuant is the first comprehensive PTQ design for the Mamba
family, paving the way for further advancements in its application.

æè¦ï¼Mamba æ¯ä¸ç¨®é«æçåºåæ¨¡åï¼å¯è Transformers ç¸åª²ç¾ï¼ä¸¦å±ç¤ºåºä½çºåç¨®ä»»ååºç¤æ¶æ§çå·¨å¤§æ½åãéåéå¸¸ç¨æ¼ç¥ç¶ç¶²è·¯ï¼ä»¥æ¸å°æ¨¡åå¤§å°åè¨ç®å»¶é²ãç¶èï¼å°éåæç¨æ¼ Mamba ä»æªå¾å°ååæ¢ç´¢ï¼èç¾æçéåæ¹æ³ï¼å° CNN å Transformer æ¨¡åææï¼ä¼¼ä¹ä¸é©å Mamba æ¨¡åï¼ä¾å¦ï¼å³ä½¿å¨ W8A8 ä¸ï¼Quarot å¨ Vim-T$^\dagger$ ä¸çæºç¢ºåº¦ä¹æä¸é 21%ï¼ãæåçåæ¢ç´¢äºéååé¡ï¼ä¸¦æ¾åºå¹¾åééµææ°ãé¦åï¼ééæå½±ãè¼¸åºæå½±åç©é£ä¹æ³ä¸­å­å¨é¡¯èç°å¸¸å¼ãå¶æ¬¡ï¼Mamba ç¨ç¹çä¸¦è¡ææé²ä¸æ­¥æ¾å¤§äºéäºç°å¸¸å¼ï¼å°è´æ¸æåä½ä¸åä¸å°¾é¨è¼éãç¬¬ä¸ï¼å³ä½¿æç¨ Hadamard è®æï¼æ¬éåæ¿æ´»ä¸­çéééå·®ç°ä»ç¶ä¸ä¸è´ãçºæ­¤ï¼æåæåºäº MambaQuantï¼ä¸åç±ä»¥ä¸é¨åçµæçè¨ç·´å¾éå (PTQ) æ¡æ¶ï¼1) å¡å«-ç¾å¤«è®æ (KLT) å¢å¼·æè½ï¼ä½¿æè½ç©é£é©æä¸åçééåä½ã2) å¹³æ»èåæè½ï¼å®ä½¿ééæ¹å·®ç¸ç­ï¼ä¸¦ä¸å¯ä»¥å°é¡å¤åæ¸åä½µå°æ¨¡åæ¬éä¸­ãå¯¦é©è¡¨æï¼MambaQuant å¯ä»¥å°æ¬éåæ¿æ´»éåçº 8 ä½ï¼èåºæ¼ Mamba çè¦è¦ºåèªè¨ä»»åçæºç¢ºåº¦æå¤±å°æ¼ 1%ãææåæç¥ï¼MambaQuant æ¯ Mamba å®¶æçç¬¬ä¸åç¶åæ§ PTQ è¨­è¨ï¼çºå¶æç¨é²ä¸æ­¥ç¼å±éªå¹³äºéè·¯ã

##### **Adaptive Testing for LLM-Based Applications: A Diversity-based Approach**
2501.13480v1 by Juyeon Yoon, Robert Feldt, Shin Yoo

The recent surge of building software systems powered by Large Language
Models (LLMs) has led to the development of various testing frameworks,
primarily focused on treating prompt templates as the unit of testing. Despite
the significant costs associated with test input execution and output
assessment, the curation of optimized test suites is yet overlooked in these
tools, which calls for tailored test selection or prioritization strategies. In
this paper, we show that diversity-based testing techniques, such as Adaptive
Random Testing (ART) with appropriate string distance metrics, can be
effectively applied to the testing of prompt templates. Our proposed adaptive
testing approach adjusts the conventional ART process to this context by
selecting new test inputs based on scores derived from existing test suite and
their labelling results. Our results, obtained using various implementations
that explore several string-based distances, confirm that our approach enables
the discovery of failures with reduced testing budgets and promotes the
generation of more varied outputs.

æè¦ï¼æè¿ç±å¤§åèªè¨æ¨¡å (LLM) é©åçå»ºç½®è»é«ç³»çµ±æ¿å¢ï¼å°è´åç¨®æ¸¬è©¦æ¶æ§çç¼å±ï¼ä¸»è¦å°æ³¨æ¼å°æç¤ºç¯æ¬è¦çºæ¸¬è©¦å®åãåç®¡èæ¸¬è©¦è¼¸å¥å·è¡åè¼¸åºè©ä¼°ç¸éçææ¬å¾é«ï¼ä½å¨éäºå·¥å·ä¸­åªåæ¸¬è©¦çµçç­åå»è¢«å¿½ç¥äºï¼ééè¦éèº«æé çæ¸¬è©¦é¸åæåªåé åºç­ç¥ãå¨æ¬æä¸­ï¼æåå±ç¤ºäºåºæ¼å¤æ¨£æ§çæ¸¬è©¦æè¡ï¼ä¾å¦æ¡ç¨é©ç¶å­ä¸²è·é¢éåº¦çèªé©æé¨æ©æ¸¬è©¦ (ART)ï¼å¯ä»¥æææç¨æ¼æç¤ºç¯æ¬çæ¸¬è©¦ãæåæåºçèªé©ææ¸¬è©¦æ¹æ³æ ¹æå¾ç¾ææ¸¬è©¦çµåå¶æ¨ç±¤çµæè¡ççåæ¸ï¼èª¿æ´å³çµ±ç ART ç¨åºä»¥ç¬¦åæ­¤æå¢ãæåççµææ¯ä½¿ç¨æ¢ç´¢å¤ç¨®åºæ¼å­ä¸²çè·é¢çåç¨®å¯¦ä½æç²å¾ï¼è­å¯¦äºæåçæ¹æ³è½å¤ ä»¥è¼ä½çæ¸¬è©¦é ç®æ¾åºå¤±æï¼ä¸¦ä¿é²ç¢çæ´å¤æ¨£åçè¼¸åºã

##### **Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility**
2501.13479v1 by Rishabh Agrawal

Few-shot learning (FSL) enables machine learning models to generalize
effectively with minimal labeled data, making it crucial for data-scarce
domains such as healthcare, robotics, and natural language processing. Despite
its potential, FSL faces challenges including sensitivity to initialization,
difficulty in adapting to diverse domains, and vulnerability to noisy datasets.
To address these issues, this paper introduces Adaptive Few-Shot Learning
(AFSL), a framework that integrates advancements in meta-learning, domain
alignment, noise resilience, and multi-modal integration. AFSL consists of four
key modules: a Dynamic Stability Module for performance consistency, a
Contextual Domain Alignment Module for domain adaptation, a Noise-Adaptive
Resilience Module for handling noisy data, and a Multi-Modal Fusion Module for
integrating diverse modalities. This work also explores strategies such as
task-aware data augmentation, semi-supervised learning, and explainable AI
techniques to enhance the applicability and robustness of FSL. AFSL provides
scalable, reliable, and impactful solutions for real-world, high-stakes
domains.

æè¦ï¼å°æ¨£æ¬å­¸ç¿ (FSL) è½è®æ©å¨å­¸ç¿æ¨¡åå¨æ¨è¨è³ææ¥µå°çææ³ä¸ææå°æ¨å»£ï¼éå°æ¼é«çä¿å¥ãæ©å¨äººåèªç¶èªè¨èçç­è³æç¨å°çé åè³ééè¦ãåç®¡ææ½åï¼FSL é¢è¨çææ°åæ¬å°åå§åçæææ§ãé£ä»¥é©æä¸åçé åä»¥åå®¹æåå°éè¨è³æéçå½±é¿ãçºäºè§£æ±ºéäºåé¡ï¼æ¬æä»ç´¹äºèªé©æå°æ¨£æ¬å­¸ç¿ (AFSL)ï¼éæ¯ä¸åæ´åäºåå­¸ç¿ãé åå°é½ãæéè¨åå¤æ¨¡ææ´åçæ¡æ¶ãAFSL åå«ååééµæ¨¡çµï¼ç¨æ¼æè½ä¸è´æ§çåæç©©å®æ¨¡çµãç¨æ¼é åé©æçèçµ¡é åå°é½æ¨¡çµãç¨æ¼èçéè¨è³æçæéè¨èªé©ææ¨¡çµï¼ä»¥åç¨æ¼æ´åä¸åæ¨¡æçå¤æ¨¡æèåæ¨¡çµãéé å·¥ä½éæ¢è¨äºä»»åæç¥è³ææ´åãåç£ç£å¼å­¸ç¿åå¯è§£é AI æè¡ç­ç­ç¥ï¼ä»¥å¢å¼· FSL çé©ç¨æ§åç©©å¥æ§ãAFSL çºç¾å¯¦ä¸ççé«é¢¨éªé åæä¾äºå¯æ´åãå¯é ä¸æå½±é¿åçè§£æ±ºæ¹æ¡ã

##### **Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge**
2501.13468v1 by Haomiao Xiong, Zongxin Yang, Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Jiawen Zhu, Huchuan Lu

Recent advances in Large Language Models (LLMs) have enabled the development
of Video-LLMs, advancing multimodal learning by bridging video data with
language tasks. However, current video understanding models struggle with
processing long video sequences, supporting multi-turn dialogues, and adapting
to real-world dynamic scenarios. To address these issues, we propose
StreamChat, a training-free framework for streaming video reasoning and
conversational interaction. $\StreamChat$ leverages a novel hierarchical memory
system to efficiently process and compress video features over extended
sequences, enabling real-time, multi-turn dialogue. Our framework incorporates
a parallel system scheduling strategy that enhances processing speed and
reduces latency, ensuring robust performance in real-world applications.
Furthermore, we introduce StreamBench, a versatile benchmark that evaluates
streaming video understanding across diverse media types and interactive
scenarios, including multi-turn interactions and complex reasoning tasks.
Extensive evaluations on StreamBench and other public benchmarks demonstrate
that StreamChat significantly outperforms existing state-of-the-art models in
terms of accuracy and response times, confirming its effectiveness for
streaming video understanding. Code is available at StreamChat:
https://github.com/hmxiong/StreamChat.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çææ°é²å±ä¿æäºå½±ç LLM çéç¼ï¼ééå°å½±çè³æèèªè¨ä»»åé£çµï¼æ¨é²å¤æ¨¡æå­¸ç¿ãç¶èï¼ç®åçå½±ççè§£æ¨¡åå¨èçé·å½±çåºåãæ¯æ´å¤è¼ªå°è©±ï¼ä»¥åé©æçå¯¦ä¸ççåæå ´æ¯æ¹é¢ä»æå°é£ãçºäºè§£æ±ºéäºåé¡ï¼æåæåº StreamChatï¼ä¸åç¡éè¨ç·´çä¸²æµå½±çæ¨çåå°è©±äºåæ¶æ§ã$\StreamChat$ æ¡ç¨åµæ°çéå±¤å¼è¨æ¶é«ç³»çµ±ï¼ææèçåå£ç¸®é·åºåçå½±çç¹å¾µï¼å¯¦ç¾å³æãå¤è¼ªå°è©±ãæåçæ¶æ§çµåäºå¹³è¡ç³»çµ±æç¨ç­ç¥ï¼å¢å¼·èçéåº¦ä¸¦éä½å»¶é²ï¼ç¢ºä¿å¨çå¯¦ä¸çæç¨ä¸­çå¼·å¥æè½ãæ­¤å¤ï¼æåå¼å¥äº StreamBenchï¼ä¸åå¤åè½åºæºï¼ç¨æ¼è©ä¼°ä¸²æµå½±ççè§£å¨ä¸ååªé«é¡ååäºåå ´æ¯ä¸­çè¡¨ç¾ï¼åæ¬å¤è¼ªäºååè¤éçæ¨çä»»åãå¨ StreamBench åå¶ä»å¬éåºæºä¸çå»£æ³è©ä¼°è­æï¼StreamChat å¨æºç¢ºæ§ååææéæ¹é¢é¡¯èåªæ¼ç¾æçæåé²æ¨¡åï¼è­å¯¦äºå¶å¨ä¸²æµå½±ççè§£æ¹é¢çæè½ãç¨å¼ç¢¼å¯å¨ StreamChat åå¾ï¼
https://github.com/hmxiong/StreamChatã

##### **Multi-Level Attention and Contrastive Learning for Enhanced Text Classification with an Optimized Transformer**
2501.13467v1 by Jia Gao, Guiran Liu, Binrong Zhu, Shicheng Zhou, Hongye Zheng, Xiaoxuan Liao

This paper studies a text classification algorithm based on an improved
Transformer to improve the performance and efficiency of the model in text
classification tasks. Aiming at the shortcomings of the traditional Transformer
model in capturing deep semantic relationships and optimizing computational
complexity, this paper introduces a multi-level attention mechanism and a
contrastive learning strategy. The multi-level attention mechanism effectively
models the global semantics and local features in the text by combining global
attention with local attention; the contrastive learning strategy enhances the
model's ability to distinguish between different categories by constructing
positive and negative sample pairs while improving the classification effect.
In addition, in order to improve the training and inference efficiency of the
model on large-scale text data, this paper designs a lightweight module to
optimize the feature transformation process and reduce the computational cost.
Experimental results on the dataset show that the improved Transformer model
outperforms the comparative models such as BiLSTM, CNN, standard Transformer,
and BERT in terms of classification accuracy, F1 score, and recall rate,
showing stronger semantic representation ability and generalization
performance. The method proposed in this paper provides a new idea for
algorithm optimization in the field of text classification and has good
application potential and practical value. Future work will focus on studying
the performance of this model in multi-category imbalanced datasets and
cross-domain tasks and explore the integration wi

æè¦ï¼æ¬æç ç©¶äºä¸ç§åºäºæ¹è¿ Transformer çææ¬åç±»ç®æ³ï¼ä»¥æé«æ¨¡åå¨ææ¬åç±»ä»»å¡ä¸­çæ§è½åæçãéå¯¹ä¼ ç» Transformer æ¨¡åå¨æææ·±åº¦è¯­ä¹å³ç³»åä¼åè®¡ç®å¤æåº¦æ¹é¢çä¸è¶³ï¼æ¬æå¼å¥äºä¸ç§å¤çº§æ³¨æåæºå¶åä¸ç§å¯¹æ¯å­¦ä¹ ç­ç¥ãå¤çº§æ³¨æåæºå¶éè¿å°å¨å±æ³¨æåä¸å±é¨æ³¨æåç¸ç»åï¼ææå°å¯¹ææ¬ä¸­çå¨å±è¯­ä¹åå±é¨ç¹å¾è¿è¡å»ºæ¨¡ï¼å¯¹æ¯å­¦ä¹ ç­ç¥éè¿æå»ºæ­£è´æ ·æ¬å¯¹ï¼å¨æé«åç±»ææçåæ¶å¢å¼ºäºæ¨¡ååºåä¸åç±»å«çè½åãæ­¤å¤ï¼ä¸ºäºæé«æ¨¡åå¨å¤§è§æ¨¡ææ¬æ°æ®ä¸çè®­ç»åæ¨çæçï¼æ¬æè®¾è®¡äºä¸ä¸ªè½»éçº§æ¨¡åï¼ä»¥ä¼åç¹å¾è½¬æ¢è¿ç¨å¹¶éä½è®¡ç®ææ¬ãå¨æ°æ®éä¸çå®éªç»æè¡¨æï¼æ¹è¿åç Transformer æ¨¡åå¨åç±»åç¡®çãF1 å¾ååå¬åçæ¹é¢ä¼äº BiLSTMãCNNãæ å Transformer å BERT ç­å¯¹æ¯æ¨¡åï¼è¡¨ç°åºæ´å¼ºçè¯­ä¹è¡¨ç¤ºè½ååæ³åæ§è½ãæ¬ææåºçæ¹æ³ä¸ºææ¬åç±»é¢åä¸­çç®æ³ä¼åæä¾äºæ°æè·¯ï¼å·æè¯å¥½çåºç¨æ½ååå®ç¨ä»·å¼ãæªæ¥çå·¥ä½å°éç¹ç ç©¶è¯¥æ¨¡åå¨å¤ç±»å«ä¸å¹³è¡¡æ°æ®éåè·¨åä»»å¡ä¸­çæ§è½ï¼å¹¶æ¢ç´¢ä¸

##### **Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks**
2501.13457v1 by Ruijia Liu, Ancheng Hou, Xiao Yu, Xiang Yin

Signal Temporal Logic (STL) is a powerful specification language for
describing complex temporal behaviors of continuous signals, making it
well-suited for high-level robotic task descriptions. However, generating
executable plans for STL tasks is challenging, as it requires consideration of
the coupling between the task specification and the system dynamics. Existing
approaches either follow a model-based setting that explicitly requires
knowledge of the system dynamics or adopt a task-oriented data-driven approach
to learn plans for specific tasks. In this work, we investigate the problem of
generating executable STL plans for systems whose dynamics are unknown a
priori. We propose a new planning framework that uses only task-agnostic data
during the offline training stage, enabling zero-shot generalization to new STL
tasks. Our framework is hierarchical, involving: (i) decomposing the STL task
into a set of progress and time constraints, (ii) searching for time-aware
waypoints guided by task-agnostic data, and (iii) generating trajectories using
a pre-trained safe diffusion model. Simulation results demonstrate the
effectiveness of our method indeed in achieving zero-shot generalization to
various STL tasks.

æè¦ï¼è¨èæåºéè¼¯ (STL) æ¯ä¸ç¨®å¼·å¤§çè¦æ ¼èªè¨ï¼ç¨æ¼æè¿°é£çºè¨èçè¤éæåºè¡çºï¼ä½¿å¶éå¸¸é©åç¨æ¼é«éæ©å¨äººä»»åæè¿°ãç¶èï¼çº STL ä»»åç¢çå¯å·è¡è¨ç«å·æææ°æ§ï¼å çºå®éè¦èæ®ä»»åè¦æ ¼åç³»çµ±åæä¹éçè¦åãç¾æçæ¹æ³éµå¾ªåºæ¼æ¨¡åçè¨­å®ï¼æç¢ºéè¦ç³»çµ±åæçç¥è­ï¼ææ¡ç¨ä»¥ä»»åçºå°åçè³æé©åæ¹æ³ä¾å­¸ç¿ç¹å®ä»»åçè¨ç«ãå¨éé å·¥ä½ä¸­ï¼æåæ¢è¨äºçºåææªç¥çç³»çµ±ç¢çå¯å·è¡ STL è¨ç«çåé¡ãæåæåºä¸åæ°çè¦åæ¶æ§ï¼å¨é¢ç·è¨ç·´éæ®µåä½¿ç¨èä»»åç¡éçè³æï¼å¯¦ç¾å°æ° STL ä»»åçé¶æ¬¡å­¸ç¿æ³åãæåçæ¶æ§æ¯éå±¤å¼çï¼æ¶åï¼(i) å° STL ä»»ååè§£çºä¸çµé²åº¦åæéç´æï¼(ii) æå°ç±èä»»åç¡éçè³æå¼å°çæéæç¥è·¯å¾é»ï¼ä»¥å (iii) ä½¿ç¨é åè¨ç·´çå®å¨æ´æ£æ¨¡åç¢çè»è·¡ãæ¨¡æ¬çµæè­æäºæåçæ¹æ³å¨å¯¦ç¾å°åç¨® STL ä»»åçé¶æ¬¡å­¸ç¿æ³åæ¹é¢çç¢ºææã

##### **KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks**
2501.13456v1 by Taoran Fang, Tianhong Gao, Chunping Wang, Yihao Shang, Wei Chow, Lei Chen, Yang Yang

Graph neural networks (GNNs) with attention mechanisms, often referred to as
attentive GNNs, have emerged as a prominent paradigm in advanced GNN models in
recent years. However, our understanding of the critical process of scoring
neighbor nodes remains limited, leading to the underperformance of many
existing attentive GNNs. In this paper, we unify the scoring functions of
current attentive GNNs and propose Kolmogorov-Arnold Attention (KAA), which
integrates the Kolmogorov-Arnold Network (KAN) architecture into the scoring
process. KAA enhances the performance of scoring functions across the board and
can be applied to nearly all existing attentive GNNs. To compare the expressive
power of KAA with other scoring functions, we introduce Maximum Ranking
Distance (MRD) to quantitatively estimate their upper bounds in ranking errors
for node importance. Our analysis reveals that, under limited parameters and
constraints on width and depth, both linear transformation-based and MLP-based
scoring functions exhibit finite expressive power. In contrast, our proposed
KAA, even with a single-layer KAN parameterized by zero-order B-spline
functions, demonstrates nearly infinite expressive power. Extensive experiments
on both node-level and graph-level tasks using various backbone models show
that KAA-enhanced scoring functions consistently outperform their original
counterparts, achieving performance improvements of over 20% in some cases.

æè¦ï¼è¿å¹´ä¾ï¼å¸¶ææ³¨æåæ©å¶çåå½¢ç¥ç¶ç¶²è·¯ (GNN)ï¼éå¸¸ç¨±çºæ³¨æå GNNï¼å·²æçºåé² GNN æ¨¡åä¸­çé¡¯èç¯ä¾ãç¶èï¼æåå°è©åé°è¿ç¯é»çééµéç¨ççè§£ä»ç¶æéï¼å°è´è¨±å¤ç¾æçæ³¨æå GNN æè½ä¸ä½³ãå¨æ¬æä¸­ï¼æåçµ±ä¸äºç¾ææ³¨æå GNN çè©åå½æ¸ï¼ä¸¦æåºäº Kolmogorov-Arnold æ³¨æå (KAA)ï¼å®å° Kolmogorov-Arnold ç¶²è·¯ (KAN) æ¶æ§æ´åå°è©åéç¨ä¸­ãKAA å¨é¢æåäºè©åå½æ¸çæè½ï¼ä¸¦ä¸å¯ä»¥æç¨æ¼å¹¾ä¹ææç¾æçæ³¨æå GNNãçºäºæ¯è¼ KAA èå¶ä»è©åå½æ¸çè¡¨éè½åï¼æåå¼å¥äºæå¤§æåè·é¢ (MRD) ä¾å®éä¼°è¨å¶å¨ç¯é»éè¦æ§æåèª¤å·®ä¸­çä¸éãæåçåæè¡¨æï¼å¨æéçåæ¸åå¯¬åº¦åæ·±åº¦éå¶ä¸ï¼åºæ¼ç·æ§è½æååºæ¼ MLP çè©åå½æ¸é½è¡¨ç¾åºæéçè¡¨éè½åãç¸åï¼æåæåºç KAAï¼å³ä½¿ä½¿ç¨é¶é B æ¨£æ¢å½æ¸åæ¸åçå®å±¤ KANï¼ä¹è¡¨ç¾åºè¿ä¹ç¡éçè¡¨éè½åãå¨ä½¿ç¨åç¨®éª¨å¹¹æ¨¡åçç¯é»å±¤ç´ååå½¢å±¤ç´ä»»åä¸é²è¡çå»£æ³å¯¦é©è¡¨æï¼KAA å¢å¼·çè©åå½æ¸å§çµåªæ¼å¶åå§å°æå½æ¸ï¼å¨æäºææ³ä¸æè½æåè¶é 20%ã

##### **One-cycle Structured Pruning with Stability Driven Structure Search**
2501.13439v1 by Deepak Ghimire, Dayoung Kil, Seonghwan Jeong, Jaesik Park, Seong-heum Kim

Existing structured pruning typically involves multi-stage training
procedures that often demand heavy computation. Pruning at initialization,
which aims to address this limitation, reduces training costs but struggles
with performance. To address these challenges, we propose an efficient
framework for one-cycle structured pruning without compromising model
performance. In this approach, we integrate pre-training, pruning, and
fine-tuning into a single training cycle, referred to as the `one cycle
approach'. The core idea is to search for the optimal sub-network during the
early stages of network training, guided by norm-based group saliency criteria
and structured sparsity regularization. We introduce a novel pruning indicator
that determines the stable pruning epoch by assessing the similarity between
evolving pruning sub-networks across consecutive training epochs. Also, group
sparsity regularization helps to accelerate the pruning process and results in
speeding up the entire process. Extensive experiments on datasets, including
CIFAR-10/100, and ImageNet, using VGGNet, ResNet, MobileNet, and ViT
architectures, demonstrate that our method achieves state-of-the-art accuracy
while being one of the most efficient pruning frameworks in terms of training
time. The source code will be made publicly available.

æè¦ï¼ç¾æççµæ§ååªæéå¸¸æ¶åå¤éæ®µè¨ç·´ç¨åºï¼éäºç¨åºéå¸¸éè¦å¤§éçéç®ãåå§åæé²è¡åªæï¼æ¨å¨è§£æ±ºæ­¤éå¶ï¼å¯æ¸å°è¨ç·´ææ¬ï¼ä½æå½±é¿æè½ãçºäºæå°éäºææ°ï¼æåæåºäºä¸åææççæ¡æ¶ï¼ç¨æ¼é²è¡ä¸å¾ªç°çµæ§ååªæï¼èä¸ææå®³æ¨¡åæè½ãå¨æ­¤æ¹æ³ä¸­ï¼æåå°é è¨ç·´ãåªæåå¾®èª¿æ´åå°ä¸åè¨ç·´é±æä¸­ï¼ç¨±çºãä¸å¾ªç°æ¹æ³ããæ ¸å¿æ¦å¿µæ¯å¨ç¶²è·¯è¨ç·´çæ©æéæ®µæå°æä½³å­ç¶²è·¯ï¼ä¸¦ä»¥åºæ¼æ¨æºçç¾¤çµé¡¯èæ§æºååçµæ§åç¨çæ­£è¦åä½çºæå°ãæåå¼å¥äºä¸åæ°ç©çåªæææ¨ï¼ééè©ä¼°é£çºè¨ç·´é±æä¸­ä¸æ·æ¼åçåªæå­ç¶²è·¯ä¹éçç¸ä¼¼æ§ï¼ä¾ç¢ºå®ç©©å®çåªæé±æãæ­¤å¤ï¼ç¾¤çµç¨çæ­£è¦åæå©æ¼å éåªææµç¨ï¼ä¸¦å å¿«æ´åæµç¨ãå¨è³æéï¼åæ¬ä½¿ç¨ VGGNetãResNetãMobileNet å ViT æ¶æ§ç CIFAR-10/100 å ImageNetï¼ä¸é²è¡çå»£æ³å¯¦é©è­æï¼æåçæ¨¡åå¨è¨ç·´æéæ¹é¢æ¯æææççåªææ¡æ¶ä¹ä¸ï¼åæéå°æåé²çæºç¢ºåº¦ãåå§ç¨å¼ç¢¼å°å¬éæä¾ã

##### **Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models**
2501.13428v1 by Bo Gao, Michael W. Spratling

Large language models have achieved remarkable success in recent years,
primarily due to the implementation of self-attention mechanisms. However,
traditional Softmax attention suffers from numerical instability and reduced
performance as the length of inference tokens increases. This paper addresses
these issues by decomposing the Softmax operation into a non-linear
transformation and the $l_1$-norm. We identify the latter as essential for
maintaining model performance. By replacing the non-linear transformation with
the Softplus activation function and introducing a dynamic length scale factor
for different token lengths based on invariance entropy, we create a novel
attention mechanism with performance better than conventional Softmax attention
across various inference lengths. To further improve the length extrapolation
ability of the proposed attention mechanism, we introduce a re-weighting
mechanism that amplifies significant attention weights while diminishing weaker
ones, enabling the model to concentrate more effectively on relevant tokens.
When combined with our proposed attention mechanism, this approach demonstrates
significant promise in managing longer sequences, maintaining nearly constant
validation loss even at 16$\times$ the training token length while ensuring
numerical stability. Our code is available at:
https://github.com/iminfine/freeatten.

æè¦ï¼è¿å¹´ä¾ï¼å¤§åèªè¨æ¨¡åå·²åå¾é¡¯èæåï¼
éä¸»è¦æ¯å çºå¯¦ä½äºèªææ³¨ææ©å¶ãç¶èï¼
å³çµ±ç Softmax æ³¨æåæé¨èæ¨è«ä»£ç¢¼é·åº¦çå¢å èç¢çæ¸å¼ä¸ç©©å®æ§ï¼ä¸¦éä½æè½ãéç¯è«æééå° Softmax éç®åè§£æéç·æ§è½æå $l_1$-ç¯æ¸ä¾è§£æ±ºéäºåé¡ãæåå°å¾èè¦çºç¶­ææ¨¡åæè½çå¿è¦æ¢ä»¶ãééå°éç·æ§è½ææ¿ææ Softplus ååå½æ¸ï¼ä¸¦æ ¹æä¸è®æ§çµçºä¸åçä»£ç¢¼é·åº¦å°å¥åæé·åº¦æ¯ä¾å å­ï¼æååµé åºä¸åæ°ç©çæ³¨æåæ©å¶ï¼å¶æè½åªæ¼å³çµ±ç Softmax æ³¨æåï¼é©ç¨æ¼åç¨®æ¨è«é·åº¦ãçºäºé²ä¸æ­¥æåææåºçæ³¨æåæ©å¶çé·åº¦å¤æ¨è½åï¼æåå°å¥ä¸åéæ°å æ¬æ©å¶ï¼ä»¥æ¾å¤§éè¦çæ³¨æåæ¬éï¼åæç¸®å°è¼å¼±çæ¬éï¼è®æ¨¡åè½æ´ææå°éä¸­æ¼ç¸éä»£ç¢¼ãç¶èæåææåºçæ³¨æåæ©å¶çµåæï¼æ­¤æ¹æ³å¨ç®¡çè¼é·åºåæ¹é¢å±ç¾é¡¯èçæ½åï¼å³ä½¿å¨è¨ç·´ä»£ç¢¼é·åº¦ç 16 åä¸ï¼ä¹è½ç¶­æè¿ä¹æå®çé©è­æå¤±ï¼åæç¢ºä¿æ¸å¼ç©©å®æ§ãæåçç¨å¼ç¢¼å¯å¨ä»¥ä¸ç¶²ååå¾ï¼
https://github.com/iminfine/freeattenã

##### **A Survey of Code-switched Arabic NLP: Progress, Challenges, and Future Directions**
2501.13419v1 by Injy Hamed, Caroline Sabty, Slim Abdennadher, Ngoc Thang Vu, Thamar Solorio, Nizar Habash

Language in the Arab world presents a complex diglossic and multilingual
setting, involving the use of Modern Standard Arabic, various dialects and
sub-dialects, as well as multiple European languages. This diverse linguistic
landscape has given rise to code-switching, both within Arabic varieties and
between Arabic and foreign languages. The widespread occurrence of
code-switching across the region makes it vital to address these linguistic
needs when developing language technologies. In this paper, we provide a review
of the current literature in the field of code-switched Arabic NLP, offering a
broad perspective on ongoing efforts, challenges, research gaps, and
recommendations for future research directions.

æè¦ï¼é¿æä¼¯ä¸ççèªè¨åç¾åºè¤éçéèªåå¤èª
ç°å¢ï¼æ¶åç¾ä»£æ¨æºé¿æä¼¯èªãåç¨®æ¹è¨å
æ¬¡æ¹è¨ï¼ä»¥åå¤ç¨®æ­æ´²èªè¨ãéç¨®å¤æ¨£çèªè¨
ç°å¢é æäºä»£ç¢¼è½æï¼ç¡è«æ¯å¨é¿æä¼¯èªè®é«ä¸­éæ¯
å¨é¿æä¼¯èªåå¤èªä¹éãä»£ç¢¼è½æå¨è©²å°åçå»£æ³ç¼ç
ä½¿å¾å¨éç¼èªè¨æè¡ææ»¿è¶³éäºèªè¨éæ±è³ééè¦ãå¨æ¬æä¸­ï¼æåæä¾äº
ä»£ç¢¼è½æé¿æä¼¯èªèªç¶èªè¨èçé åçç¶åæç»ç¶è¿°ï¼æä¾äº
å°æ­£å¨é²è¡çåªåãææ°ãç ç©¶å·®è·çå»£æ³è§é»ï¼ä»¥å
å°æªä¾ç ç©¶æ¹åçå»ºè­°ã

##### **Rethinking the Sample Relations for Few-Shot Classification**
2501.13418v1 by Guowei Yin, Sheng Huang, Luwen Huangfu, Yi Zhang, Xiaohong Zhang

Feature quality is paramount for classification performance, particularly in
few-shot scenarios. Contrastive learning, a widely adopted technique for
enhancing feature quality, leverages sample relations to extract intrinsic
features that capture semantic information and has achieved remarkable success
in Few-Shot Learning (FSL). Nevertheless, current few-shot contrastive learning
approaches often overlook the semantic similarity discrepancies at different
granularities when employing the same modeling approach for different sample
relations, which limits the potential of few-shot contrastive learning. In this
paper, we introduce a straightforward yet effective contrastive learning
approach, Multi-Grained Relation Contrastive Learning (MGRCL), as a
pre-training feature learning model to boost few-shot learning by meticulously
modeling sample relations at different granularities. MGRCL categorizes sample
relations into three types: intra-sample relation of the same sample under
different transformations, intra-class relation of homogenous samples, and
inter-class relation of inhomogeneous samples. In MGRCL, we design
Transformation Consistency Learning (TCL) to ensure the rigorous semantic
consistency of a sample under different transformations by aligning predictions
of input pairs. Furthermore, to preserve discriminative information, we employ
Class Contrastive Learning (CCL) to ensure that a sample is always closer to
its homogenous samples than its inhomogeneous ones, as homogenous samples share
similar semantic content while inhomogeneous samples have different semantic
content. Our method is assessed across four popular FSL benchmarks, showing
that such a simple pre-training feature learning method surpasses a majority of
leading FSL methods. Moreover, our method can be incorporated into other FSL
methods as the pre-trained model and help them obtain significant performance
gains.

æè¦ï¼ç¹å¾µåè³ªå°æ¼åé¡æè½è³ééè¦ï¼ç¹å¥æ¯å¨å°æ¨£æ¬æå¢ä¸­ãå°æ¯å­¸ç¿æ¯ä¸ç¨®å»£æ³æ¡ç¨çæè¡ï¼ç¨æ¼æåç¹å¾µåè³ªï¼å®å©ç¨æ¨£æ¬éä¿ä¾èåå§å¨ç¹å¾µï¼ä»¥æ·åèªæè³è¨ï¼ä¸¦å¨å°æ¨£æ¬å­¸ç¿ (FSL) ä¸­ç²å¾é¡¯èçæåãåç®¡å¦æ­¤ï¼ç®åçå°æ¨£æ¬å°æ¯å­¸ç¿æ¹æ³å¨å°ä¸åæ¨£æ¬éä¿æ¡ç¨ç¸åçå»ºæ¨¡æ¹æ³æï¼éå¸¸æå¿½ç¥ä¸åç²åº¦ä¸çèªæç¸ä¼¼æ§å·®ç°ï¼ééå¶äºå°æ¨£æ¬å°æ¯å­¸ç¿çæ½åãå¨æ¬æä¸­ï¼æåä»ç´¹ä¸ç¨®ç°¡å®ä½ææçå°æ¯å­¸ç¿æ¹æ³ï¼å³å¤ç²åº¦éä¿å°æ¯å­¸ç¿ (MGRCL)ï¼ä½çºé è¨ç·´ç¹å¾µå­¸ç¿æ¨¡åï¼ééå¨ä¸åç²åº¦ä¸ä»ç´°å»ºæ¨¡æ¨£æ¬éä¿ä¾æåå°æ¨£æ¬å­¸ç¿ãMGRCL å°æ¨£æ¬éä¿åé¡çºä¸ç¨®é¡åï¼åä¸æ¨£æ¬å¨ä¸åè½æä¸çæ¨£æ¬å§éä¿ãåé¡æ¨£æ¬çé¡å§éä¿ï¼ä»¥åä¸åé¡æ¨£æ¬çé¡ééä¿ãå¨ MGRCL ä¸­ï¼æåè¨­è¨äºè½æä¸è´æ§å­¸ç¿ (TCL)ï¼ééæ¯å°è¼¸å¥å°çé æ¸¬ï¼ä¾ç¢ºä¿æ¨£æ¬å¨ä¸åè½æä¸çå´è¬¹èªæä¸è´æ§ãæ­¤å¤ï¼çºäºä¿çåè¾¨æ§è³è¨ï¼æåæ¡ç¨é¡å°æ¯å­¸ç¿ (CCL)ï¼ä»¥ç¢ºä¿æ¨£æ¬å§çµæ¯å¶ä¸åé¡æ¨£æ¬æ´æ¥è¿å¶åé¡æ¨£æ¬ï¼å çºåé¡æ¨£æ¬å·æç¸ä¼¼çèªæå§å®¹ï¼èä¸åé¡æ¨£æ¬å·æä¸åçèªæå§å®¹ãæåçæ¨¡åå¨ååæµè¡ç FSL åºæºä¸é²è¡è©ä¼°ï¼çµæé¡¯ç¤ºéç¨®ç°¡å®çé è¨ç·´ç¹å¾µå­¸ç¿æ¹æ³åªæ¼å¤§å¤æ¸é åç FSL æ¹æ³ãæ­¤å¤ï¼æåçæ¨¡åå¯ä»¥ä½çºé è¨ç·´æ¨¡åæ´åå°å¶ä» FSL æ¹æ³ä¸­ï¼ä¸¦å¹«å©å®åç²å¾é¡¯èçæè½æåã

##### **M3PT: A Transformer for Multimodal, Multi-Party Social Signal Prediction with Person-aware Blockwise Attention**
2501.13416v1 by Yiming Tang, Abrar Anwar, Jesse Thomason

Understanding social signals in multi-party conversations is important for
human-robot interaction and artificial social intelligence. Multi-party
interactions include social signals like body pose, head pose, speech, and
context-specific activities like acquiring and taking bites of food when
dining. Incorporating all the multimodal signals in a multi-party interaction
is difficult, and past work tends to build task-specific models for predicting
social signals. In this work, we address the challenge of predicting multimodal
social signals in multi-party settings in a single model. We introduce M3PT, a
causal transformer architecture with modality and temporal blockwise attention
masking which allows for the simultaneous processing of multiple social cues
across multiple participants and their temporal interactions. This approach
better captures social dynamics over time by considering longer horizons of
social signals between individuals. We train and evaluate our unified model on
the Human-Human Commensality Dataset (HHCD), and demonstrate that using
multiple modalities improves bite timing and speaking status prediction. Source
code: https://github.com/AbrarAnwar/masked-social-signals/

æè¦ï¼äºè§£å¤æ¹å¯¹è¯ä¸­çç¤¾äº¤ä¿¡å·å¯¹äºäººæºäº¤äºåäººå·¥æºè½è³å³éè¦ãå¤æ¹äºå¨åæ¬èº«ä½å§¿å¿ãå¤´é¨å§¿å¿ãè¨è¯­åç¹å®ç¯å¢æ´»å¨ï¼ä¾å¦å¨ç¨é¤æ¶è·ååé£ç¨é£ç©ï¼ç­ç¤¾äº¤ä¿¡å·ãå¨å¤æ¹äºå¨ä¸­æ´åææå¤æ¨¡æä¿¡å·å¾å°é¾ï¼è¿å»çå·¥ä½å¾åäºæå»ºç¹å®ä»»å¡çæ¨¡åæ¥é¢æµç¤¾äº¤ä¿¡å·ãå¨è¿é¡¹å·¥ä½ä¸­ï¼æä»¬è§£å³äºå¨åä¸ªæ¨¡åä¸­é¢æµå¤æ¹ç¯å¢ä¸­çå¤æ¨¡æç¤¾äº¤ä¿¡å·çææãæä»¬å¼å¥äº M3PTï¼ä¸ç§å æè½¬æ¢å¨æ¶æï¼å·ææ¨¡æåæ¶é´åç¶æ³¨ææ©ç ï¼åè®¸åæ¶å¤çå¤ä¸ªåä¸èåå¶æ¶é´äº¤äºä¸­çå¤ä¸ªç¤¾äº¤çº¿ç´¢ãè¿ç§æ¹æ³éè¿èèä¸ªäººä¹é´æ´é¿æ¶é´èå´çç¤¾äº¤ä¿¡å·ï¼å¯ä»¥æ´å¥½å°ææç¤¾äº¤å¨æãæä»¬å¨äººéå±é¤æ°æ®é (HHCD) ä¸è®­ç»åè¯ä¼°æä»¬çç»ä¸æ¨¡åï¼å¹¶è¯æä½¿ç¨å¤ç§æ¨¡æå¯ä»¥æ¹åå¬åæ¶é´åè¯´è¯ç¶æé¢æµãæºä»£ç ï¼https://github.com/AbrarAnwar/masked-social-signals/

##### **Load and Renewable Energy Forecasting Using Deep Learning for Grid Stability**
2501.13412v1 by Kamal Sarkar

As the energy landscape changes quickly, grid operators face several
challenges, especially when integrating renewable energy sources with the grid.
The most important challenge is to balance supply and demand because the solar
and wind energy are highly unpredictable. When dealing with such uncertainty,
trustworthy short-term load and renewable energy forecasting can help stabilize
the grid, maximize energy storage, and guarantee the effective use of renewable
resources. Physical models and statistical techniques were the previous
approaches employed for this kind of forecasting tasks. In forecasting
renewable energy, machine learning and deep learning techniques have recently
demonstrated encouraging results. More specifically, the deep learning
techniques like CNN and LSTM and the conventional machine learning techniques
like regression that are mostly utilized for load and renewable energy
forecasting tasks. In this article, we will focus mainly on CNN and LSTM-based
forecasting methods.

æè¦ï¼é¨èè½æºçåå¿«éè®åï¼é»ç¶²çéåé¢è¨å¤é ææ°ï¼ç¹å¥æ¯å¨å°åçè½æºæ´åå°é»ç¶²æã
æéè¦çææ°å¨æ¼å¹³è¡¡ä¾éï¼å çºå¤ªé½è½åé¢¨è½é«åº¦ä¸å¯é æ¸¬ãå¨æå°éç¨®ä¸ç¢ºå®æ§æï¼å¯ä¿¡è³´çç­æè² è¼ååçè½æºé æ¸¬æå©æ¼ç©©å®é»ç¶²ãæå¤§åå²è½ï¼ä¸¦ç¢ºä¿åçè½æºçææå©ç¨ãç©çæ¨¡ååçµ±è¨æè¡æ¯ä»¥åç¨æ¼æ­¤é¡é æ¸¬ä»»åçæ¹æ³ãå¨é æ¸¬åçè½æºæ¹é¢ï¼æ©å¨å­¸ç¿åæ·±åº¦å­¸ç¿æè¡æè¿å·²å±ç¾ä»¤äººé¼èçææãæ´å·é«å°èªªï¼æ·±åº¦å­¸ç¿æè¡ï¼å¦ CNN å LSTMï¼åå³çµ±æ©å¨å­¸ç¿æè¡ï¼å¦åæ­¸ï¼ï¼éäºæè¡ä¸»è¦ç¨æ¼è² è¼ååçè½æºé æ¸¬ä»»åãå¨æ¬æä¸­ï¼æåå°ä¸»è¦éæ³¨åºæ¼ CNN å LSTM çé æ¸¬æ¹æ³ã

##### **YOLOv8 to YOLO11: A Comprehensive Architecture In-depth Comparative Review**
2501.13400v1 by Priyanto Hidayatullah, Nurjannah Syakrani, Muhammad Rizqi Sholahuddin, Trisna Gelar, Refdinal Tubagus

In the field of deep learning-based computer vision, YOLO is revolutionary.
With respect to deep learning models, YOLO is also the one that is evolving the
most rapidly. Unfortunately, not every YOLO model possesses scholarly
publications. Moreover, there exists a YOLO model that lacks a publicly
accessible official architectural diagram. Naturally, this engenders
challenges, such as complicating the understanding of how the model operates in
practice. Furthermore, the review articles that are presently available do not
delve into the specifics of each model. The objective of this study is to
present a comprehensive and in-depth architecture comparison of the four most
recent YOLO models, specifically YOLOv8 through YOLO11, thereby enabling
readers to quickly grasp not only how each model functions, but also the
distinctions between them. To analyze each YOLO version's architecture, we
meticulously examined the relevant academic papers, documentation, and
scrutinized the source code. The analysis reveals that while each version of
YOLO has improvements in architecture and feature extraction, certain blocks
remain unchanged. The lack of scholarly publications and official diagrams
presents challenges for understanding the model's functionality and future
enhancement. Future developers are encouraged to provide these resources.

æè¦ï¼å¨æ·±åº¦å­¸ç¿çºåºç¤çé»è¦è¦è¦ºé åï¼YOLO æ¯é©å½æ§çã
å¨æ·±åº¦å­¸ç¿æ¨¡åä¸­ï¼YOLO ä¹æ¯æ¼é²æå¿«éçãéºæ¾çæ¯ï¼ä¸¦éæ¯å YOLO æ¨¡åé½æå­¸è¡è«æãæ­¤å¤ï¼éæä¸å YOLO æ¨¡åç¼ºä¹å¬éçå®æ¹æ¶æ§åãéèªç¶æç¢çææ°ï¼ä¾å¦ä½¿çè§£æ¨¡åå¨å¯¦åä¸­å¦ä½éä½è®å¾è¤éãæ­¤å¤ï¼ç®åå¯åå¾çè©è«æç« ä¸¦æªæ·±å¥æ¢è¨æ¯åæ¨¡åçå·é«ç´°ç¯ãæ¬ç ç©¶çç®æ¨æ¯éå°æè¿çåå YOLO æ¨¡åï¼ç¹å¥æ¯ YOLOv8 å° YOLO11ï¼æåºä¸åå¨é¢çæ·±å¥æ¶æ§æ¯è¼ï¼é²èè®è®èè½å¤ å¿«éææ¡æ¯åæ¨¡åçåè½ï¼ä»¥åå®åä¹éçå·®ç°ãçºäºåææ¯å YOLO çæ¬çæ¶æ§ï¼æåä»ç´°å¯©æ¥ç¸éçå­¸è¡è«æãæä»¶ï¼ä¸¦ä»ç´°æª¢è¦åå§ç¢¼ãåæé¡¯ç¤ºï¼åç®¡æ¯åçæ¬ç YOLO é½å¨æ¶æ§åç¹å¾µèåä¸ææ¹é²ï¼ä½æäºåå¡ä»ç¶ä¸è®ãç¼ºä¹å­¸è¡è«æåå®æ¹åè¡¨æå°çè§£æ¨¡åçåè½åæªä¾çå¼·åé æææ°ãæåé¼åµæªä¾çéç¼äººå¡æä¾éäºè³æºã

##### **ExLM: Rethinking the Impact of $\texttt{[MASK]}$ Tokens in Masked Language Models**
2501.13397v1 by Kangjie Zheng, Junwei Yang, Siyue Liang, Bin Feng, Zequn Liu, Wei Ju, Zhiping Xiao, Ming Zhang

Masked Language Models (MLMs) have achieved remarkable success in many
self-supervised representation learning tasks. MLMs are trained by randomly
replacing some tokens in the input sentences with $\texttt{[MASK]}$ tokens and
predicting the original tokens based on the remaining context. This paper
explores the impact of $\texttt{[MASK]}$ tokens on MLMs. Analytical studies
show that masking tokens can introduce the corrupted semantics problem, wherein
the corrupted context may convey multiple, ambiguous meanings. This problem is
also a key factor affecting the performance of MLMs on downstream tasks. Based
on these findings, we propose a novel enhanced-context MLM, ExLM. Our approach
expands $\texttt{[MASK]}$ tokens in the input context and models the
dependencies between these expanded states. This expansion increases context
capacity and enables the model to capture richer semantic information,
effectively mitigating the corrupted semantics problem during pre-training.
Experimental results demonstrate that ExLM achieves significant performance
improvements in both text modeling and SMILES modeling tasks. Further analysis
confirms that ExLM enhances semantic representations through context
enhancement, and effectively reduces the multimodality problem commonly
observed in MLMs.

æè¦ï¼èé¢èªè¨æ¨¡åï¼MLMï¼å¨è¨±å¤èªæç£ç£è¡¨å¾µå­¸ç¿ä»»åä¸­åå¾é¡¯èæåãMLM çè¨ç·´æ¹å¼æ¯é¨æ©ç¨ [MASK] æ¨è¨åä»£è¼¸å¥å¥å­ä¸­çä¸äºæ¨è¨ï¼ä¸¦æ ¹æå¶é¤çå§å®¹é æ¸¬åå§æ¨è¨ãæ¬ææ¢è¨äº [MASK] æ¨è¨å° MLM çå½±é¿ãåæç ç©¶é¡¯ç¤ºï¼é®è½æ¨è¨å¯è½æå¼å¥èªææå£åé¡ï¼å¶ä¸­æå£çå§å®¹å¯è½æå³éå¤ç¨®å«ç³çæç¾©ãéååé¡ä¹æ¯å½±é¿ MLM å¨ä¸æ¸¸ä»»åä¸­è¡¨ç¾çä¸åééµå ç´ ãæ ¹æéäºç¼ç¾ï¼æåæåºäºä¸ç¨®æ°çå¢å¼·å§å®¹ MLMï¼ExLMãæåçåæ³æ´å±äºè¼¸å¥å§å®¹ä¸­ç [MASK] æ¨è¨ï¼ä¸¦å°éäºæ´å±çæä¹éçä¾è³´éä¿é²è¡å»ºæ¨¡ãéç¨®æ´å±å¢å äºå§å®¹å®¹éï¼ä¸¦ä½¿æ¨¡åè½å¤ æ·åæ´è±å¯çèªç¾©è³è¨ï¼æææ¸è¼é è¨ç·´æéçèªææå£åé¡ãå¯¦é©çµæè­æï¼ExLM å¨ææ¬å»ºæ¨¡å SMILES å»ºæ¨¡ä»»åä¸­é½åå¾äºé¡¯èçæè½æåãé²ä¸æ­¥çåæè­å¯¦ï¼ExLM ééå§å®¹å¢å¼·ä¾å¢å¼·èªç¾©è¡¨å¾µï¼ä¸¦æææ¸å°äº MLM ä¸­å¸¸è¦çå¤æ¨¡æåé¡ã

##### **Can Large Language Models Understand Preferences in Personalized Recommendation?**
2501.13391v1 by Zhaoxuan Tan, Zinan Zeng, Qingkai Zeng, Zhenyu Wu, Zheyuan Liu, Fengran Mo, Meng Jiang

Large Language Models (LLMs) excel in various tasks, including personalized
recommendations. Existing evaluation methods often focus on rating prediction,
relying on regression errors between actual and predicted ratings. However,
user rating bias and item quality, two influential factors behind rating
scores, can obscure personal preferences in user-item pair data. To address
this, we introduce PerRecBench, disassociating the evaluation from these two
factors and assessing recommendation techniques on capturing the personal
preferences in a grouped ranking manner. We find that the LLM-based
recommendation techniques that are generally good at rating prediction fail to
identify users' favored and disfavored items when the user rating bias and item
quality are eliminated by grouping users. With PerRecBench and 19 LLMs, we find
that while larger models generally outperform smaller ones, they still struggle
with personalized recommendation. Our findings reveal the superiority of
pairwise and listwise ranking approaches over pointwise ranking, PerRecBench's
low correlation with traditional regression metrics, the importance of user
profiles, and the role of pretraining data distributions. We further explore
three supervised fine-tuning strategies, finding that merging weights from
single-format training is promising but improving LLMs' understanding of user
preferences remains an open research problem. Code and data are available at
https://github.com/TamSiuhin/PerRecBench

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨åç¨®ä»»åä¸­è¡¨ç¾åºè²ï¼åæ¬åäººåæ¨è¦ãç¾æçè©ä¼°æ¹æ³éå¸¸å°æ³¨æ¼è©åé æ¸¬ï¼ä¾è³´æ¼å¯¦éè©ååé æ¸¬è©åä¹éçåæ­¸èª¤å·®ãç¶èï¼ä½¿ç¨èè©ååå·®åé ç®åè³ªéå©åå½±é¿è©åèå¾çå ç´ ï¼å¯è½ææ¨¡ç³ä½¿ç¨è-é ç®éå°è³æä¸­çåäººåå¥½ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº PerRecBenchï¼å°è©ä¼°èéå©åå ç´ åé¢ï¼ä¸¦è©ä¼°æ¨è¦æè¡ä»¥ç¾¤çµæåæ¹å¼ææåäººåå¥½ãæåç¼ç¾ï¼åºæ¼ LLM çæ¨è¦æè¡éå¸¸å¾æé·è©åé æ¸¬ï¼ä½å¨ç¾¤çµä½¿ç¨èæç¡æ³è­å¥ä½¿ç¨èçåå¥½åä¸åå¥½çé ç®ãä½¿ç¨ PerRecBench å 19 å LLMï¼æåç¼ç¾éç¶è¼å¤§çæ¨¡åéå¸¸åªæ¼è¼å°çæ¨¡åï¼ä½å®åå¨åäººåæ¨è¦æ¹é¢ä»ç¶æå°é£ãæåçç ç©¶çµææ­ç¤ºäºæå°åæ¸å®æåæ¹æ³åªæ¼éé»æåãPerRecBench èå³çµ±åæ­¸ææ¨çä½ç¸éæ§ãä½¿ç¨èåäººè³æçéè¦æ§ä»¥åé è¨ç·´è³æåå¸çè§è²ãæåé²ä¸æ­¥æ¢è¨äºä¸ç¨®ç£ç£å¾®èª¿ç­ç¥ï¼ç¼ç¾åä½µå®ä¸æ ¼å¼è¨ç·´çæ¬éæ¯æå¸æçï¼ä½æ¹å LLM å°ä½¿ç¨èåå¥½ççè§£ä»ç¶æ¯ä¸åéæ¾çç ç©¶åé¡ãç¨å¼ç¢¼åè³æå¯å¨ https://github.com/TamSiuhin/PerRecBench åå¾

##### **Do as We Do, Not as You Think: the Conformity of Large Language Models**
2501.13381v1 by Zhiyuan Weng, Guikun Chen, Wenguan Wang

Recent advancements in large language models (LLMs) revolutionize the field
of intelligent agents, enabling collaborative multi-agent systems capable of
tackling complex problems across various domains. However, the potential of
conformity within these systems, analogous to phenomena like conformity bias
and groupthink in human group dynamics, remains largely unexplored, raising
concerns about their collective problem-solving capabilities and possible
ethical implications. This paper presents a comprehensive study on conformity
in LLM-driven multi-agent systems, focusing on three aspects: the existence of
conformity, the factors influencing conformity, and potential mitigation
strategies. In particular, we introduce BenchForm, a new conformity-oriented
benchmark, featuring reasoning-intensive tasks and five distinct interaction
protocols designed to probe LLMs' behavior in collaborative scenarios. Several
representative LLMs are evaluated on BenchForm, using metrics such as
conformity rate and independence rate to quantify conformity's impact. Our
analysis delves into factors influencing conformity, including interaction time
and majority size, and examines how the subject agent rationalizes its
conforming behavior. Furthermore, we explore two strategies to mitigate
conformity effects, i.e., developing enhanced personas and implementing a
reflection mechanism. Several interesting findings regarding LLMs' conformity
are derived from empirical results and case studies. We hope that these
insights can pave the way for more robust and ethically-aligned collaborative
AI systems. Our benchmark and code are available at BenchForm.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çææ°é²å±å¾¹åºé©æ°äºæºæ§ä»£çé åï¼ä¿æäºåä½å¼å¤éä»£çç³»çµ±ï¼è½å¤ è§£æ±ºè·¨è¶ååé åçè¤éåé¡ãç¶èï¼éäºç³»çµ±å§é¨ä¸è´æ§çæ½åï¼é¡ä¼¼æ¼äººé¡ç¾¤é«åæä¸­çä¸è´æ§åèª¤ååé«è¿·æç­ç¾è±¡ï¼ä»æªè¢«å»£æ³æ¢è¨ï¼å¼ç¼äºå°å¶éé«åé¡è§£æ±ºè½ååæ½å¨éå¾·å½±é¿çææãæ¬æéå° LLM é©åçå¤éä»£çç³»çµ±ä¸­çä¸è´æ§é²è¡å¨é¢ç ç©¶ï¼éé»éæ³¨ä¸åé¢åï¼ä¸è´æ§çå­å¨ãå½±é¿ä¸è´æ§çå ç´ ä»¥åæ½å¨çç·©è§£ç­ç¥ãç¹å¥æ¯ï¼æåå¼å¥äº BenchFormï¼ä¸åæ°çä»¥ä¸è´æ§çºå°åçåºæºï¼å·åæ¨çå¯éåä»»ååäºç¨®ä¸åçäºååå®ï¼æ¨å¨æ¢è¨ LLM å¨åä½å ´æ¯ä¸­çè¡çºãä½¿ç¨ä¸è´æ§æ¯çåç¨ç«æ§æ¯çç­ææ¨å° BenchForm ä¸çå¹¾åå·ä»£è¡¨æ§ç LLM é²è¡è©ä¼°ï¼ä»¥éåä¸è´æ§çå½±é¿ãæåçåææ·±å¥æ¢è¨äºå½±é¿ä¸è´æ§çå ç´ ï¼åæ¬äºåæéåå¤æ¸è¦æ¨¡ï¼ä¸¦æ¢è¨äºä¸»é«ä»£çå¦ä½åçåå¶ä¸è´æ§è¡çºãæ­¤å¤ï¼æåæ¢è¨äºæ¸è¼ä¸è´æ§å½±é¿çå©ç¨®ç­ç¥ï¼å³éç¼å¢å¼·çè§è²åå¯¦æ½åææ©å¶ãå¾å¯¦è­çµæåæ¡ä¾ç ç©¶ä¸­å¾åºäºå¹¾åéæ¼ LLM ä¸è´æ§çæè¶£ç¼ç¾ãæåå¸æéäºè¦è§£è½çºæ´å¼·å¤§ä¸ç¬¦åéå¾·çåä½å¼ AI ç³»çµ±éªè·¯ãæåçåºæºåç¨å¼ç¢¼å¯å¨ BenchForm åå¾ã

##### **Generative Data Augmentation Challenge: Zero-Shot Speech Synthesis for Personalized Speech Enhancement**
2501.13372v1 by Jae-Sung Bae, Anastasia Kuznetsova, Dinesh Manocha, John Hershey, Trausti Kristjansson, Minje Kim

This paper presents a new challenge that calls for zero-shot text-to-speech
(TTS) systems to augment speech data for the downstream task, personalized
speech enhancement (PSE), as part of the Generative Data Augmentation workshop
at ICASSP 2025. Collecting high-quality personalized data is challenging due to
privacy concerns and technical difficulties in recording audio from the test
scene. To address these issues, synthetic data generation using generative
models has gained significant attention. In this challenge, participants are
tasked first with building zero-shot TTS systems to augment personalized data.
Subsequently, PSE systems are asked to be trained with this augmented
personalized dataset. Through this challenge, we aim to investigate how the
quality of augmented data generated by zero-shot TTS models affects PSE model
performance. We also provide baseline experiments using open-source zero-shot
TTS models to encourage participation and benchmark advancements. Our baseline
code implementation and checkpoints are available online.

æè¦ï¼éç¯è«ææåºäºæ°çææ°ï¼å¼ç±²é¶æ¬¡å­¸ç¿æå­è½èªé³ (TTS) ç³»çµ±æ´å¢ä¸æ¸¸ä»»åçèªé³è³æï¼ä¹å°±æ¯åäººåèªé³å¢å¼· (PSE)ï¼ä½çº ICASSP 2025 çæè³ææ´åå·¥ä½åçä¸é¨åãç±æ¼é±ç§åé¡åéè£½æ¸¬è©¦å ´æ¯é³è¨çæè¡é£åº¦ï¼æ¶éé«åè³ªçåäººåè³ææ¯ä¸é ææ°ãçºäºè§£æ±ºéäºåé¡ï¼ä½¿ç¨çææ¨¡åé²è¡åæè³æçæååéæ³¨ãå¨éåææ°ä¸­ï¼åèèé¦åè¢«è¦æ±å»ºæ§é¶æ¬¡å­¸ç¿ TTS ç³»çµ±ä»¥æ´å¢åäººåè³æãé¨å¾ï¼è¦æ± PSE ç³»çµ±ä½¿ç¨éåæ´å¢çåäººåè³æéé²è¡è¨ç·´ãéééåææ°ï¼æåæ¨å¨æ¢è¨é¶æ¬¡å­¸ç¿ TTS æ¨¡åçæçæ´å¢è³æåè³ªå¦ä½å½±é¿ PSE æ¨¡åçæè½ãæåä¹æä¾ä½¿ç¨éæ¾åå§ç¢¼é¶æ¬¡å­¸ç¿ TTS æ¨¡åçåºæºå¯¦é©ï¼ä»¥é¼åµåèååºæºé²å±ãæåçåºæºç¨å¼ç¢¼å¯¦ä½åæª¢æ¥é»å¯æ¼ç·ä¸åå¾ã

##### **A review on development of eco-friendly filters in Nepal for use in cigarettes and masks and Air Pollution Analysis with Machine Learning and SHAP Interpretability**
2501.13369v1 by Bishwash Paneru, Biplov Paneru, Tanka Mukhiya, Khem Narayan Poudyal

In Nepal, air pollution is a serious public health concern, especially in
cities like Kathmandu where particulate matter (PM2.5 and PM10) has a major
influence on respiratory health and air quality. The Air Quality Index (AQI) is
predicted in this work using a Random Forest Regressor, and the model's
predictions are interpreted using SHAP (SHapley Additive exPlanations)
analysis. With the lowest Testing RMSE (0.23) and flawless R2 scores (1.00),
CatBoost performs better than other models, demonstrating its greater accuracy
and generalization which is cross validated using a nested cross validation
approach. NowCast Concentration and Raw Concentration are the most important
elements influencing AQI values, according to SHAP research, which shows that
the machine learning results are highly accurate. Their significance as major
contributors to air pollution is highlighted by the fact that high values of
these characteristics significantly raise the AQI. This study investigates the
Hydrogen-Alpha (HA) biodegradable filter as a novel way to reduce the related
health hazards. With removal efficiency of more than 98% for PM2.5 and 99.24%
for PM10, the HA filter offers exceptional defense against dangerous airborne
particles. These devices, which are biodegradable face masks and cigarette
filters, address the environmental issues associated with traditional filters'
non-biodegradable trash while also lowering exposure to air contaminants.

æè¦ï¼å¨å°¼æ³ç¾ï¼ç©ºæ°£æ±¡ææ¯ä¸åå´éçå¬å±è¡çåé¡ï¼ç¹å¥æ¯å¨å å¾·æ»¿é½ç­åå¸ï¼é£è£¡çæ¸æµ®å¾®ç²ï¼PM2.5 å PM10ï¼å°å¼å¸ç³»çµ±å¥åº·åç©ºæ°£åè³ªæéå¤§å½±é¿ãéé å·¥ä½ä½¿ç¨é¨æ©æ£®æåæ­¸å¨é æ¸¬ç©ºæ°£åè³ªææ¸ (AQI)ï¼ä¸¦ä½¿ç¨ SHAPï¼SHapley å æ³è§£éï¼åæä¾è§£éæ¨¡åçé æ¸¬ãCatBoost çæ¸¬è©¦ RMSE æä½ï¼0.23ï¼ï¼R2 åæ¸å®ç¾ï¼1.00ï¼ï¼è¡¨ç¾åªæ¼å¶ä»æ¨¡åï¼è­æå¶å·ææ´é«çæºç¢ºæ§åæ³åæ§ï¼ä¸¦ä½¿ç¨åµå¥äº¤åé©è­æ¹æ³é²è¡äº¤åé©è­ãæ ¹æ SHAP ç ç©¶ï¼ç¾å¨æ¿åº¦ååå§æ¿åº¦æ¯å½±é¿ AQI å¼æéè¦çåç´ ï¼éè¡¨ææ©å¨å­¸ç¿çµæéå¸¸æºç¢ºãå®åä½çºç©ºæ°£æ±¡æçä¸»è¦è²¢ç»èçéè¦æ§å¨æ¼ï¼éäºç¹å¾µçé«å¼æé¡¯èæé« AQIãæ¬ç ç©¶æ¢è¨äºæ°«-Î±ï¼HAï¼å¯çç©éè§£éæ¿¾å¨ä½çºæ¸å°ç¸éå¥åº·å±å®³çä¸ç¨®æ°æ¹æ³ãHA éæ¿¾å¨å° PM2.5 çå»é¤æçè¶é 98%ï¼å° PM10 çå»é¤æçè¶é 99.24%ï¼å¯æä¾é²ç¯å±éªç©ºæ°£æ¸æµ®å¾®ç²çåºè²é²è­·ãéäºå¯çç©éè§£å£ç½©åé¦è¸éæ¿¾å¨çè£ç½®è§£æ±ºäºå³çµ±éæ¿¾å¨ä¸å¯çç©éè§£åå¾ç¸éçç°å¢åé¡ï¼åæä¹éä½äºæ¥è§¸ç©ºæ°£æ±¡æç©çé¢¨éªã

##### **Enhanced Extractor-Selector Framework and Symmetrization Weighted Binary Cross-Entropy for Edge Detections**
2501.13365v1 by Hao Shu

Recent advancements have demonstrated the effectiveness of the
extractor-selector (E-S) framework in edge detection (ED) tasks, which achieves
state-of-the-art (SOTA) performance in both quantitative metrics and perceptual
quality. However, this method still falls short of fully exploiting the
potential of feature extractors, as selectors only operate on highly compressed
feature maps that lack diversity and suffer from substantial information loss.
Additionally, while union training can improve perceptual quality, the highest
evaluation scores are typically obtained without it, creating a trade-off
between quantitative accuracy and perceptual fidelity. To address these
limitations, we propose an enhanced E-S architecture, which utilizes richer,
less-loss feature representations and incorporates auxiliary features during
the selection process, thereby improving the effectiveness of the feature
selection mechanism. Additionally, we introduce a novel loss function, the
Symmetrization Weight Binary Cross-Entropy (SWBCE), which simultaneously
emphasizes both the recall of edge pixels and the suppression of erroneous edge
predictions, thereby enhancing the predictions both in the perceptual quality
and the prediction accuracy. The effectiveness and superiority of our
approaches over baseline models, the standard E-S framework, and the standard
Weight Binary Cross-Entropy (WBCE) loss function are demonstrated by extensive
experiments. For example, our enhanced E-S architecture trained with SWBCE loss
function achieves average improvements of 8.25$\%$, 8.01$\%$, and 33.25$\%$ in
ODS, OIS, and AP, measured on BIPED2 compared with the baseline models,
significantly outperforming the standard E-S method. The results set new
benchmarks for ED tasks, and highlight the potential of the methods in beyond.

æè¦ï¼<paragraph>æè¿çè¿æ­¥å±ç¤ºäºæåå¨-éæ©å¨ (E-S) æ¡æ¶å¨è¾¹ç¼æ£æµ (ED) ä»»å¡ä¸­çæææ§ï¼å¨å®éææ åæç¥è´¨éæ¹é¢é½åå¾äºæåè¿ (SOTA) çæ§è½ãç¶èï¼è¿ç§æ¹æ³ä»ç¶æ²¡æååå©ç¨ç¹å¾æåå¨çæ½åï¼å ä¸ºéæ©å¨åªä½ç¨äºé«åº¦åç¼©çç¹å¾å¾ï¼è¿äºç¹å¾å¾ç¼ºä¹å¤æ ·æ§å¹¶é­åå¤§éä¿¡æ¯ä¸¢å¤±ãæ­¤å¤ï¼è½ç¶èåè®­ç»å¯ä»¥æé«æç¥è´¨éï¼ä½éå¸¸å¨æ²¡æèåè®­ç»çæåµä¸è·å¾æé«è¯ä¼°åæ°ï¼ä»èå¨å®éåç¡®æ§åæç¥ä¿çåº¦ä¹é´å½¢ææè¡¡ãä¸ºäºè§£å³è¿äºéå¶ï¼æä»¬æåºäºä¸ç§å¢å¼ºç E-S æ¶æï¼å®å©ç¨æ´ä¸°å¯ãæ´å°æå¤±çç¹å¾è¡¨ç¤ºï¼å¹¶å¨éæ©è¿ç¨ä¸­çº³å¥è¾å©ç¹å¾ï¼ä»èæé«äºç¹å¾éæ©æºå¶çæææ§ãæ­¤å¤ï¼æä»¬å¼å¥äºä¸ç§æ°é¢çæå¤±å½æ°ï¼å³å¯¹ç§°å æäºåäº¤åçµ (SWBCE)ï¼å®åæ¶å¼ºè°äºè¾¹ç¼åç´ çå¬ååéè¯¯è¾¹ç¼é¢æµçæå¶ï¼ä»èå¨æç¥è´¨éåé¢æµåç¡®æ§æ¹é¢é½å¢å¼ºäºé¢æµãéè¿å¹¿æ³çå®éªï¼è¯æäºæä»¬æ¹æ³ç¸å¯¹äºåºçº¿æ¨¡åãæ å E-S æ¡æ¶åæ åå æäºåäº¤åçµ (WBCE) æå¤±å½æ°çæææ§åä¼è¶æ§ãä¾å¦ï¼æä»¬ä½¿ç¨ SWBCE æå¤±å½æ°è®­ç»çå¢å¼ºå E-S æ¶æå¨ ODSãOIS å AP ä¸­å®ç°äº 8.25%ã8.01% å 33.25% çå¹³åæ¹è¿ï¼ä¸åºçº¿æ¨¡åç¸æ¯ï¼å¨ BIPED2 ä¸æµéï¼ææ¾ä¼äºæ å E-S æ¹æ³ãç»æä¸º ED ä»»å¡è®¾å®äºæ°çåºåï¼å¹¶çªåºäºè¿äºæ¹æ³å¨å¶ä»é¢åçæ½åã</paragraph>

##### **One Fits All: General Mobility Trajectory Modeling via Masked Conditional Diffusion**
2501.13347v1 by Qingyue Long, Can Rong, Huandong Wang, Yong Li

Trajectory data play a crucial role in many applications, ranging from
network optimization to urban planning. Existing studies on trajectory data are
task-specific, and their applicability is limited to the specific tasks on
which they have been trained, such as generation, recovery, or prediction.
However, the potential of a unified model has not yet been fully explored in
trajectory modeling. Although various trajectory tasks differ in inputs,
outputs, objectives, and conditions, they share common mobility patterns. Based
on these common patterns, we can construct a general framework that enables a
single model to address different tasks. However, building a trajectory
task-general framework faces two critical challenges: 1) the diversity in the
formats of different tasks and 2) the complexity of the conditions imposed on
different tasks. In this work, we propose a general trajectory modeling
framework via masked conditional diffusion (named GenMove). Specifically, we
utilize mask conditions to unify diverse formats. To adapt to complex
conditions associated with different tasks, we utilize historical trajectory
data to obtain contextual trajectory embeddings, which include rich contexts
such as spatiotemporal characteristics and user preferences. Integrating the
contextual trajectory embedding into diffusion models through a classifier-free
guidance approach allows the model to flexibly adjust its outputs based on
different conditions. Extensive experiments on mainstream tasks demonstrate
that our model significantly outperforms state-of-the-art baselines, with the
highest performance improvement exceeding 13% in generation tasks.

æè¦ï¼è»è·¡è³æå¨è¨±å¤æç¨ä¸­æ®æ¼ééµè§è²ï¼å¾ç¶²è·¯æä½³åå°é½å¸è¦åçç¶ãç¾æçè»è·¡è³æç ç©¶çºç¹å®ä»»åèè¨­è¨ï¼å¶é©ç¨æ§åéæ¼å¶åéè¨ç·´çç¹å®ä»»åï¼ä¾å¦ç¢çãæ¢å¾©æé æ¸¬ãç¶èï¼çµ±ä¸æ¨¡åçæ½åå°æªå¨è»è·¡å»ºæ¨¡ä¸­å¾å°ååæ¢ç´¢ãåç®¡åç¨®è»è·¡ä»»åå¨è¼¸å¥ãè¼¸åºãç®æ¨åæ¢ä»¶ä¸ææä¸åï¼ä½å®åå±äº«éç¨çç§»åæ¨¡å¼ãåºæ¼éäºå±åæ¨¡å¼ï¼æåå¯ä»¥å»ºæ§ä¸åéç¨æ¡æ¶ï¼è®å®ä¸æ¨¡åè½å¤ èçä¸åçä»»åãç¶èï¼å»ºç«ä¸åè»è·¡ä»»åéç¨æ¡æ¶é¢è¨å©åééµææ°ï¼1ï¼ä¸åä»»åæ ¼å¼çå¤æ¨£æ§ï¼ä»¥å 2ï¼å°ä¸åä»»åæ½å çæ¢ä»¶çè¤éæ§ãå¨éé å·¥ä½ä¸­ï¼æåééé®ç½©æ¢ä»¶æ´æ£ï¼ç¨±çº GenMoveï¼æåºä¸åéç¨çè»è·¡å»ºæ¨¡æ¡æ¶ãå·é«ä¾èªªï¼æåå©ç¨é®ç½©æ¢ä»¶ä¾çµ±ä¸å¤æ¨£åçæ ¼å¼ãçºäºé©æèä¸åä»»åç¸éçè¤éæ¢ä»¶ï¼æåå©ç¨æ­·å²è»è·¡è³æä¾ç²åèçµ¡è»è·¡åµå¥ï¼å¶ä¸­åå«è±å¯çèçµ¡ï¼ä¾å¦æç©ºç¹å¾µåä½¿ç¨èåå¥½ãééåé¡å¨èªç±å¼å°æ¹æ³å°èçµ¡è»è·¡åµå¥æ´åå°æ´æ£æ¨¡åä¸­ï¼è®æ¨¡åè½å¤ æ ¹æä¸åçæ¢ä»¶éæ´»èª¿æ´å¶è¼¸åºãå¨ä¸»æµä»»åä¸çå»£æ³å¯¦é©è­æï¼æåçæ¨¡åé¡¯èåªæ¼æåé²çåºæºï¼å¨ç¢çä»»åä¸­çæé«æè½æåè¶é 13%ã

##### **Full-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation**
2501.13344v1 by Rong Shan, Jiachen Zhu, Jianghao Lin, Chenxu Zhu, Bo Chen, Ruiming Tang, Yong Yu, Weinan Zhang

In this paper, we address the lifelong sequential behavior incomprehension
problem in large language models (LLMs) for recommendation, where LLMs struggle
to extract useful information from long user behavior sequences, even within
their context limits. To tackle this, we propose ReLLaX (Retrieval-enhanced
Large Language models Plus), a framework offering optimization across data,
prompt, and parameter levels. At the data level, we introduce Semantic User
Behavior Retrieval (SUBR) to reduce sequence heterogeneity, making it easier
for LLMs to extract key information. For prompt-level enhancement, we employ
Soft Prompt Augmentation (SPA) to inject collaborative knowledge, aligning item
representations with recommendation tasks and improving LLMs's exploration of
item relationships. Finally, at the parameter level, we propose Component
Fully-interactive LoRA (CFLoRA), which enhances LoRA's expressiveness by
enabling interactions between its components, allowing better capture of
sequential information. Moreover, we present new perspectives to compare
current LoRA-based LLM4Rec methods, i.e. from both a composite and a decomposed
view. We theoretically demonstrate that the ways they employ LoRA for
recommendation are degraded versions of our CFLoRA, with different constraints
on atom component interactions. Extensive experiments on three public datasets
demonstrate ReLLaX's superiority over existing baselines and its ability to
mitigate lifelong sequential behavior incomprehension effectively.

æè¦ï¼<paragraph>å¨æ¬æä¸­ï¼æåæ¢è¨äºæ¨è¦ä¸­å¤§åèªè¨æ¨¡å (LLM) ä¸­ççµçé åºè¡çºé£ä»¥çè§£çåé¡ï¼å¶ä¸­ LLM é£ä»¥å¾é·ç¨æ¶è¡çºåºåä¸­æåæç¨çè³è¨ï¼å³ä½¿å¨å®åçå§å®¹éå¶å§ä¹æ¯å¦æ­¤ãçºäºè§£æ±ºéååé¡ï¼æåæåºäº ReLLaXï¼æª¢ç´¢å¢å¼·åå¤§åèªè¨æ¨¡å Plusï¼ï¼ä¸åæä¾è·¨è³æãæç¤ºååæ¸å±¤ç´æä½³åçæ¶æ§ãå¨è³æå±¤ç´ï¼æåå¼å¥äºèªç¾©ä½¿ç¨èè¡çºæª¢ç´¢ (SUBR) ä»¥æ¸å°åºåç°è³ªæ§ï¼è® LLM æ´å®¹ææåééµè³è¨ãå°æ¼æç¤ºå±¤ç´çå¢å¼·ï¼æåæ¡ç¨äºè»æç¤ºæ´å (SPA) ä¾æ³¨å¥åä½ç¥è­ï¼å°é ç®è¡¨ç¤ºèæ¨è¦ä»»åå°é½ï¼ä¸¦æ¹å LLM å°é ç®éä¿çæ¢ç´¢ãæå¾ï¼å¨åæ¸å±¤ç´ï¼æåæåºäºçµä»¶å®å¨äºå LoRA (CFLoRA)ï¼å®ééåç¨å¶çµä»¶ä¹éçäºåä¾å¢å¼· LoRA çè¡¨éåï¼å¾èå¯ä»¥æ´å¥½å°æ·åé åºè³è¨ãæ­¤å¤ï¼æåæåºäºæ°çè§é»ä¾æ¯è¼ç®åçåºæ¼ LoRA ç LLM4Rec æ¹æ³ï¼å³å¾è¤åååè§£çè§é»ä¾çãæåå¨çè«ä¸è­æäºå®åä½¿ç¨ LoRA é²è¡æ¨è¦çæ¹å¼æ¯æå CFLoRA çç°¡åçæ¬ï¼å°åå­çµä»¶äºåæä¸åçéå¶ãå¨ä¸åå¬éè³æéä¸é²è¡çå¤§éå¯¦é©è­æäº ReLLaX åªæ¼ç¾æçåºæºï¼ä¸¦ä¸è½å¤ ææå°ç·©è§£çµççé åºè¡çºé£ä»¥çè§£çåé¡ã</paragraph>

##### **AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback**
2501.13333v1 by Joshua Park, Yongfeng Zhang

Multi-agent systems must decide which agent is the most appropriate for a
given task. We propose a novel architecture for recommending which LLM agent
out of many should perform a task given a natural language prompt by extending
the Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a
top-1 accuracy of 92.2% with each classification taking less than 300
milliseconds. In contrast to traditional classification methods, our
architecture is computationally cheap, adaptive to new classes, interpretable,
and controllable with arbitrary metrics through reinforcement learning. By
encoding natural language prompts into sentence embeddings, our model captures
the semantic content relevant to recommending an agent. The distance between
sentence embeddings that belong to the same agent is then minimized through
fine-tuning and aligned to human values through reinforcement learning from
human feedback. This allows the classification of natural language prompts
based on their nearest neighbors by measuring the cosine similarity between
embeddings. This work is made possible through the generation of a synthetic
dataset for agent recommendation, which we have open-sourced to the public
along with the code for AgentRec recommendation system at
https://github.com/joshprk/agentrec.

æè¦ï¼å¤æºè½é«ç³»çµ±å¿é æ±ºå®åªåæºè½é«æé©åå·è¡æé ä»»åãæåæåºäºä¸ç¨®æ°çæ¶æ§ï¼ç¨æ¼æ¨è¦å¨çµ¦å®èªç¶èªè¨æç¤ºçææ³ä¸ï¼ç¾å¤ LLM æºè½é«ä¸­çåªä¸åæè©²å·è¡ä»»åï¼æ¹æ³æ¯æ´å± Sentence-BERT (SBERT) ç·¨ç¢¼å¨æ¨¡åãå¨æ¸¬è©¦æ¸æä¸ï¼æåè½å¤ å¯¦ç¾ 92.2% çå 1 åæºç¢ºçï¼æ¯æ¬¡åé¡è±è²»ä¸å° 300 æ¯«ç§ãèå³çµ±åé¡æ¹æ³ç¸æ¯ï¼æåçæ¶æ§å¨è¨ç®ä¸ææ¬ä½å»ï¼é©ææ°é¡å¥ï¼å¯è§£éï¼ä¸¦ä¸å¯ééå¼·åå­¸ç¿ä½¿ç¨ä»»æææ¨é²è¡æ§å¶ãééå°èªç¶èªè¨æç¤ºç·¨ç¢¼çºå¥å­åµå¥ï¼æåçæ¨¡åæç²äºèæ¨è¦æºè½é«ç¸éçèªç¾©å§å®¹ãç¶å¾ï¼ééå¾®èª¿åééäººé¡åé¥çå¼·åå­¸ç¿èäººé¡å¹å¼è§ä¿æä¸è´ï¼æå°åå±¬æ¼åä¸åæºè½é«çå¥å­åµå¥ä¹éçè·é¢ãéåè¨±åºæ¼å®åæè¿çé°å±å°èªç¶èªè¨æç¤ºé²è¡åé¡ï¼æ¹æ³æ¯æ¸¬éåµå¥ä¹éçé¤å¼¦ç¸ä¼¼åº¦ãéé å·¥ä½æ¯ééçæä¸åç¨æ¼æºè½é«æ¨è¦çåææ¸æéèå¾ä»¥å¯¦ç¾çï¼æåå·²å°å¶è AgentRec æ¨è¦ç³»çµ±çä»£ç¢¼ä¸èµ·å¬éï¼ç¶²åçº https://github.com/joshprk/agentrecã

##### **Sparse identification of nonlinear dynamics and Koopman operators with Shallow Recurrent Decoder Networks**
2501.13329v1 by Mars Liyao Gao, Jan P. Williams, J. Nathan Kutz

Spatiotemporal modeling of real-world data poses a challenging problem due to
inherent high dimensionality, measurement noise, and expensive data collection
procedures. In this paper, we present Sparse Identification of Nonlinear
Dynamics with SHallow REcurrent Decoder networks (SINDy-SHRED), a method to
jointly solve the sensing and model identification problems with simple
implementation, efficient computation, and robust performance. SINDy-SHRED uses
Gated Recurrent Units (GRUs) to model the temporal sequence of sensor
measurements along with a shallow decoder network to reconstruct the full
spatiotemporal field from the latent state space using only a few available
sensors. Our proposed algorithm introduces a SINDy-based regularization;
beginning with an arbitrary latent state space, the dynamics of the latent
space progressively converges to a SINDy-class functional, provided the
projection remains within the set. In restricting SINDy to a linear model, the
architecture produces a Koopman-SHRED model which enforces a linear latent
space dynamics. We conduct a systematic experimental study including synthetic
PDE data, real-world sensor measurements for sea surface temperature, and
direct video data. With no explicit encoder, SINDy-SHRED and Koopman-SHRED
enable efficient training with minimal hyperparameter tuning and laptop-level
computing; further, it demonstrates robust generalization in a variety of
applications with minimal to no hyperparameter adjustments. Finally, the
interpretable SINDy and Koopman models of latent state dynamics enables
accurate long-term video predictions, achieving state-of-the-art performance
and outperforming all baseline methods considered, including Convolutional
LSTM, PredRNN, ResNet, and SimVP.

æè¦ï¼<paragraph>æç©ºæ¨¡ååçå¯¦ä¸çè³æç±æ¼åºæçé«ç¶­åº¦ãéæ¸¬éè¨åæè²´çè³ææ¶éç¨åºèæ§æä¸åå·æææ°æ§çåé¡ãå¨æ¬æä¸­ï¼æåæåºéç·æ§ååå­¸çç¨çè¾¨è­èæ·ºå±¤éè¿´è§£ç¢¼å¨ç¶²è·¯ (SINDy-SHRED)ï¼ä¸ç¨®æ¹æ³ä»¥ç°¡å®çå¯¦ä½ãææççéç®åç©©å¥çæè½ä¾å±åè§£æ±ºææ¸¬åæ¨¡åè¾¨è­åé¡ãSINDy-SHRED ä½¿ç¨éæ§éè¿´å®å (GRU) ä¾å°ææ¸¬å¨éæ¸¬çæéåºåé²è¡å»ºæ¨¡ï¼ä¸¦æ­éä¸åæ·ºå±¤è§£ç¢¼å¨ç¶²è·¯ä¾ä½¿ç¨åæçå¹¾åå¯ç¨ææ¸¬å¨å¾æ½å¨çæç©ºééå»ºå®æ´çæç©ºå ´åãæåæåºçæ¼ç®æ³å¼å¥äºä¸ååºæ¼ SINDy çæ­£ååï¼å¾ä¸åä»»ææ½å¨çæç©ºééå§ï¼æ½å¨ç©ºéçåææéæ¼¸æ¶æå°ä¸å SINDy é¡åçå½æ¸ï¼åªè¦æå½±ä»ç¶å¨éåä¸­ãå¨å° SINDy éå¶çºç·æ§æ¨¡åæï¼æ­¤æ¶æ§æç¢çä¸å Koopman-SHRED æ¨¡åï¼å®å¼·å¶å·è¡ç·æ§æ½å¨ç©ºéåæãæåé²è¡äºä¸é ç³»çµ±æ§çå¯¦é©ç ç©¶ï¼åæ¬åæåå¾®åæ¹ç¨å¼è³æãæµ·é¢æº«åº¦çå¯¦ä¸çææ¸¬å¨éæ¸¬ï¼ä»¥åç´æ¥å½±çè³æãSINDy-SHRED å Koopman-SHRED æ²ææç¢ºçç·¨ç¢¼å¨ï¼è½å¤ ä»¥æå°çè¶åæ¸èª¿æ´åç­è¨åé»è¦ç­ç´çéç®é²è¡ææççè¨ç·´ï¼æ­¤å¤ï¼å®å¨åç¨®æç¨ä¸­å±ç¾åºç©©å¥çæ³åè½åï¼å¹¾ä¹ä¸éè¦èª¿æ´è¶åæ¸ãæå¾ï¼æ½å¨çæåæçå¯è§£é SINDy å Koopman æ¨¡åè½å¤ é²è¡æºç¢ºçé·æå½±çé æ¸¬ï¼éå°æåé²çæè½ï¼ä¸¦ä¸åªæ¼ææèéçåºæºæ¹æ³ï¼åæ¬å·ç© LSTMãPredRNNãResNet å SimVPã</paragraph>

##### **Investigation of the Privacy Concerns in AI Systems for Young Digital Citizens: A Comparative Stakeholder Analysis**
2501.13321v1 by Molly Campbell, Ankur Barthwal, Sandhya Joshi, Austin Shouli, Ajay Kumar Shrestha

The integration of Artificial Intelligence (AI) systems into technologies
used by young digital citizens raises significant privacy concerns. This study
investigates these concerns through a comparative analysis of stakeholder
perspectives. A total of 252 participants were surveyed, with the analysis
focusing on 110 valid responses from parents/educators and 100 from AI
professionals after data cleaning. Quantitative methods, including descriptive
statistics and Partial Least Squares Structural Equation Modeling, examined
five validated constructs: Data Ownership and Control, Parental Data Sharing,
Perceived Risks and Benefits, Transparency and Trust, and Education and
Awareness. Results showed Education and Awareness significantly influenced data
ownership and risk assessment, while Data Ownership and Control strongly
impacted Transparency and Trust. Transparency and Trust, along with Perceived
Risks and Benefits, showed minimal influence on Parental Data Sharing,
suggesting other factors may play a larger role. The study underscores the need
for user-centric privacy controls, tailored transparency strategies, and
targeted educational initiatives. Incorporating diverse stakeholder
perspectives offers actionable insights into ethical AI design and governance,
balancing innovation with robust privacy protections to foster trust in a
digital age.

æè¦ï¼äººå·¥æºæ§ (AI) ç³»çµ±æ´åå°å¹´è¼æ¸ä½å¬æ°æä½¿ç¨çæè¡ä¸­ï¼å¼ç¼äºéå¤§çé±ç§åé¡ãæ¬ç ç©¶ééæ¯è¼åæå©å®³éä¿äººçè§é»ä¾æ¢è¨éäºåé¡ãç¸½å±èª¿æ¥äº 252 ä½åèèï¼åæéé»å¨æ¼è³ææ¸çå¾ï¼ä¾èªç¶æ¯/æè²å·¥ä½èç 110 ä»½ææåæï¼ä»¥åä¾èª AI å°æ¥­äººå£«ç 100 ä»½åæãå®éæ¹æ³ï¼åæ¬æè¿°æ§çµ±è¨ååæå°å¹³æ¹æ³çµæ§æ¹ç¨æ¨¡åï¼æª¢é©äºäºåå·²é©è­çæ§å¿µï¼è³ææææ¬åæ§å¶ãç¶æ¯è³æå±äº«ãèªç¥é¢¨éªåå©çãéæåº¦åä¿¡ä»»ï¼ä»¥åæè²åæè­ãçµæé¡¯ç¤ºï¼æè²åæè­é¡¯èå½±é¿è³ææææ¬åé¢¨éªè©ä¼°ï¼èè³ææææ¬åæ§å¶åå¼·çå½±é¿éæåº¦åä¿¡ä»»ãéæåº¦åä¿¡ä»»ï¼ä»¥åèªç¥é¢¨éªåå©çï¼å°ç¶æ¯è³æå±äº«çå½±é¿å¾å°ï¼éè¡¨ç¤ºå¶ä»å ç´ å¯è½ç¼æ®æ´å¤§çä½ç¨ãæ¬ç ç©¶å¼·èª¿äºä»¥ä½¿ç¨èçºä¸­å¿çé±ç§æ§å¶ãéèº«æé çéæåº¦ç­ç¥ï¼ä»¥åæéå°æ§çæè²è¨ç«çå¿è¦æ§ãç´å¥ä¸åçå©å®³éä¿äººè§é»ï¼æä¾äºå¯è¡çè¦è§£ï¼ç¨æ¼éå¾· AI è¨­è¨åæ²»çï¼å¨åµæ°èå¼·å¤§çé±ç§ä¿è­·ä¹éåå¾å¹³è¡¡ï¼ä»¥å»ºç«æ¸ä½æä»£çä¿¡ä»»ã

##### **Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers**
2501.13302v1 by Akshit Achara, Anshuman Chhabra

AI Safety Moderation (ASM) classifiers are designed to moderate content on
social media platforms and to serve as guardrails that prevent Large Language
Models (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential
for disparate impact, it is crucial to ensure that these classifiers: (1) do
not unfairly classify content belonging to users from minority groups as unsafe
compared to those from majority groups and (2) that their behavior remains
robust and consistent across similar inputs. In this work, we thus examine the
fairness and robustness of four widely-used, closed-source ASM classifiers:
OpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)
API, and Clarifai API. We assess fairness using metrics such as demographic
parity and conditional statistical parity, comparing their performance against
ASM models and a fair-only baseline. Additionally, we analyze robustness by
testing the classifiers' sensitivity to small and natural input perturbations.
Our findings reveal potential fairness and robustness gaps, highlighting the
need to mitigate these issues in future versions of these models.

æè¦ï¼AI å®å¨å¯©æ ¸ (ASM) åé¡å¨æ¨å¨å¯©æ ¸ç¤¾ç¾¤åªé«å¹³å°ä¸çå§å®¹ï¼ä¸¦ä½çºè­·æ¬ï¼é²æ­¢å¤§åèªè¨æ¨¡å (LLM) å¨ä¸å®å¨çè¼¸å¥ä¸é²è¡å¾®èª¿ãç±æ¼å®åå·æç¢çä¸åå½±é¿çæ½åï¼å æ­¤ç¢ºä¿éäºåé¡å¨è³ééè¦ï¼(1) èå¤æ¸ç¾¤é«ç¸æ¯ï¼ä¸æä¸å¬å¹³å°å°å°æ¸ç¾¤é«ä½¿ç¨èçå§å®¹åé¡çºä¸å®å¨ï¼(2) å®åçè¡çºå¨é¡ä¼¼çè¼¸å¥ä¸­ä¿æç©©å¥ä¸ä¸è´ãå æ­¤ï¼å¨éé å·¥ä½ä¸­ï¼æåæª¢æ¥äºååå»£æ³ä½¿ç¨çéæº ASM åé¡å¨çå¬å¹³æ§åç©©å¥æ§ï¼OpenAI Moderation APIãPerspective APIãGoogle Cloud Natural Language (GCNL) API å Clarifai APIãæåä½¿ç¨äººå£çµ±è¨åè³ªæ§åæ¢ä»¶çµ±è¨åè³ªæ§ç­ææ¨è©ä¼°å¬å¹³æ§ï¼ä¸¦å°å¶æè½è ASM æ¨¡åååå¬å¹³çåºæºé²è¡æ¯è¼ãæ­¤å¤ï¼æåééæ¸¬è©¦åé¡å¨å°å°åä¸èªç¶çè¼¸å¥æ¾åçæææ§ä¾åæç©©å¥æ§ãæåçç ç©¶çµææ­ç¤ºäºæ½å¨çå¬å¹³æ§åç©©å¥æ§å·®è·ï¼çªé¡¯äºå¨éäºæ¨¡åçæªä¾çæ¬ä¸­æ¸è¼éäºåé¡çå¿è¦æ§ã

##### **Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents**
2501.13299v1 by Shrinidhi Kumbhar, Venkatesh Mishra, Kevin Coutinho, Divij Handa, Ashif Iquebal, Chitta Baral

Materials discovery and design are essential for advancing technology across
various industries by enabling the development of application-specific
materials. Recent research has leveraged Large Language Models (LLMs) to
accelerate this process. We explore the potential of LLMs to generate viable
hypotheses that, once validated, can expedite materials discovery.
Collaborating with materials science experts, we curated a novel dataset from
recent journal publications, featuring real-world goals, constraints, and
methods for designing real-world applications. Using this dataset, we test
LLM-based agents that generate hypotheses for achieving given goals under
specific constraints. To assess the relevance and quality of these hypotheses,
we propose a novel scalable evaluation metric that emulates the process a
materials scientist would use to evaluate a hypothesis critically. Our curated
dataset, proposed method, and evaluation framework aim to advance future
research in accelerating materials discovery and design with LLMs.

æè¦ï¼ææç¼ç¾åè¨­è¨å°æ¼æ¨é²åè¡åæ¥­çæè¡è³ééè¦ï¼å çºå®è½ä¿é²ç¹å®æç¨ææçéç¼ãæè¿çç ç©¶å©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾å éæ­¤éç¨ãæåæ¢ç´¢ LLM ç¢çå¯è¡åè¨­çæ½åï¼ä¸æ¦é©è­ï¼å°±è½å éææç¼ç¾ãæåèææç§å­¸å°å®¶åä½ï¼å¾æè¿çæååºçç©ä¸­ç­åäºä¸åæ°ç©çæ¸æéï¼å¶ä¸­åå«è¨­è¨å¯¦éæç¨ç¨å¼çå¯¦éç®æ¨ãéå¶åæ¹æ³ãä½¿ç¨æ­¤æ¸æéï¼æåæ¸¬è©¦äº LLM åºç¤ä»£çï¼éäºä»£çæç¢çåè¨­ï¼ä»¥å¨ç¹å®éå¶ä¸å¯¦ç¾æ¢å®ç®æ¨ãçºäºè©ä¼°éäºåè¨­çç¸éæ§ååè³ªï¼æåæåºäºä¸ç¨®æ°ç©çå¯æ´åè©ä¼°ææ¨ï¼æ¨¡æ¬ææç§å­¸å®¶ç¨ä¾æ¹å¤æ§è©ä¼°åè¨­çéç¨ãæåç­åçæ¸æéãæåºçæ¹æ³åè©ä¼°æ¶æ§æ¨å¨æ¨é²æªä¾å¨ä½¿ç¨ LLM å éææç¼ç¾åè¨­è¨æ¹é¢çç ç©¶ã

##### **RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question Answering**
2501.13297v1 by Yang Bai, Christan Earl Grant, Daisy Zhe Wang

Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text
and images, has gained significant attention in information retrieval (IR) and
natural language processing (NLP). Traditional ranking methods rely on small
encoder-based language models, which are incompatible with modern decoder-based
generative large language models (LLMs) that have advanced various NLP tasks.
To bridge this gap, we propose RAMQA, a unified framework combining
learning-to-rank methods with generative permutation-enhanced ranking
techniques. We first train a pointwise multi-modal ranker using LLaVA as the
backbone. Then, we apply instruction tuning to train a LLaMA model for
re-ranking the top-k documents using an innovative autoregressive multi-task
learning approach. Our generative ranking model generates re-ranked document
IDs and specific answers from document candidates in various permutations.
Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant
improvements over strong baselines, highlighting the effectiveness of our
approach. Code and data are available at: https://github.com/TonyBY/RAMQA

æè¦ï¼å¤æ¨¡ææ£ç´¢å¢å¼ºåé®ç­ (MRAQA) éæææ¬åå¾åï¼å¨ä¿¡æ¯æ£ç´¢ (IR) åèªç¶è¯­è¨å¤ç (NLP) ä¸­å¤åå³æ³¨ãä¼ ç»æåæ¹æ³ä¾èµäºå°åç¼ç å¨è¯­è¨æ¨¡åï¼è¿ä¸åè¿çåç§ NLP ä»»å¡çç°ä»£è§£ç å¨è¯­è¨æ¨¡å (LLM) ä¸å¼å®¹ãä¸ºäºå¼¥åè¿ä¸å·®è·ï¼æä»¬æåºäº RAMQAï¼ä¸ä¸ªå°å­¦ä¹ æåºæ¹æ³ä¸çæç½®æ¢å¢å¼ºæåºææ¯ç¸ç»åçç»ä¸æ¡æ¶ãæä»¬é¦åä½¿ç¨ LLaVA ä½ä¸ºä¸»å¹²è®­ç»ä¸ä¸ªç¹å¼å¤æ¨¡ææåºå¨ãç¶åï¼æä»¬åºç¨æä»¤å¾®è°æ¥è®­ç»ä¸ä¸ª LLaMA æ¨¡åï¼ä½¿ç¨åæ°çèªåå½å¤ä»»å¡å­¦ä¹ æ¹æ³å¯¹å k ä¸ªææ¡£è¿è¡éæ°æåºãæä»¬ççæå¼æåºæ¨¡åä»ææ¡£åéèä¸­çæéæ°æåºçææ¡£ ID åç¹å®ç­æ¡ï¼æåæ¹å¼å¤ç§å¤æ ·ãå¨ä¸¤ä¸ª MRAQA åºå WebQA å MultiModalQA ä¸çå®éªè¡¨æï¼ä¸å¼ºå¤§çåºåç¸æ¯ææ¾èçæ¹è¿ï¼çªåºäºæä»¬æ¹æ³çæææ§ãä»£ç åæ°æ®å¯å¨ä»¥ä¸ç½åè·å¾ï¼https://github.com/TonyBY/RAMQA

##### **Parallel Belief Contraction via Order Aggregation**
2501.13295v1 by Jake Chandler, Richard Booth

The standard ``serial'' (aka ``singleton'') model of belief contraction
models the manner in which an agent's corpus of beliefs responds to the removal
of a single item of information. One salient extension of this model introduces
the idea of ``parallel'' (aka ``package'' or ``multiple'') change, in which an
entire set of items of information are simultaneously removed. Existing
research on the latter has largely focussed on single-step parallel
contraction: understanding the behaviour of beliefs after a single parallel
contraction. It has also focussed on generalisations to the parallel case of
serial contraction operations whose characteristic properties are extremely
weak. Here we consider how to extend serial contraction operations that obey
stronger properties. Potentially more importantly, we also consider the
iterated case: the behaviour of beliefs after a sequence of parallel
contractions. We propose a general method for extending serial iterated belief
change operators to handle parallel change based on an n-ary generalisation of
Booth & Chandler's TeamQueue binary order aggregators.

æè¦ï¼æ¨æºçãåºåãï¼åç¨±ãå®ä¾ãï¼ä¿¡å¿µæ¶ç¸®æ¨¡åæ¨¡æ¬ä»£çä¿¡å¿µèªæåº«å°å®ä¸è³è¨é ç®ç§»é¤çåææ¹å¼ãæ­¤æ¨¡åçä¸åé¡¯èå»¶ä¼¸å¼å¥äºãå¹³è¡ãï¼åç¨±ãå°åãæãå¤éãï¼è®åçæ¦å¿µï¼å¶ä¸­åæç§»é¤ä¸æ´çµè³è¨é ç®ãå¾çºçç ç©¶ä¸»è¦éä¸­å¨å®æ­¥å¹³è¡æ¶ç¸®ï¼äºè§£ä¿¡å¿µå¨å®æ¬¡å¹³è¡æ¶ç¸®å¾çè¡çºãå®ä¹éä¸­æ¼åºåæ¶ç¸®éç®çå¹³è¡ææ³çæ¦æ¬ï¼å¶ç¹å¾µå±¬æ§æ¥µçºèå¼±ãå¨æ­¤ï¼æåèæ®å¦ä½æ´åæå¾æ´å¼·å±¬æ§çåºåæ¶ç¸®éç®ãæ½å¨æ´éè¦çæ¯ï¼æåä¹èæ®äºåè¦çææ³ï¼ä¿¡å¿µå¨å¹³è¡æ¶ç¸®åºåå¾çè¡çºãæåæåºäºä¸ç¨®éç¨æ¹æ³ï¼ç¨æ¼æ´ååºååè¦ä¿¡å¿µè®æ´éç®å­ï¼ä»¥èçåºæ¼ Booth & Chandler ç TeamQueue äºåé åºå½ç¸½å¨ç n åæ¦æ¬çå¹³è¡è®æ´ã

##### **Toyteller: AI-powered Visual Storytelling Through Toy-Playing with Character Symbols**
2501.13284v1 by John Joon Young Chung, Melissa Roemmele, Max Kreminski

We introduce Toyteller, an AI-powered storytelling system where users
generate a mix of story text and visuals by directly manipulating character
symbols like they are toy-playing. Anthropomorphized symbol motions can convey
rich and nuanced social interactions; Toyteller leverages these motions (1) to
let users steer story text generation and (2) as a visual output format that
accompanies story text. We enabled motion-steered text generation and
text-steered motion generation by mapping motions and text onto a shared
semantic space so that large language models and motion generation models can
use it as a translational layer. Technical evaluations showed that Toyteller
outperforms a competitive baseline, GPT-4o. Our user study identified that
toy-playing helps express intentions difficult to verbalize. However, only
motions could not express all user intentions, suggesting combining it with
other modalities like language. We discuss the design space of toy-playing
interactions and implications for technical HCI research on human-AI
interaction.

æè¦ï¼æåæ¨åº Toytellerï¼éæ¯ä¸åç± AI é©åçæäºæè¿°ç³»çµ±ï¼ä½¿ç¨èå¯ééç´æ¥æä½è§è²ç¬¦èï¼å°±åç©ç©å·ä¸æ¨£ï¼ä¾ç¢çæäºæå­åè¦è¦ºææççµåãæ¬äººåçç¬¦èåä½å¯ä»¥å³éè±å¯ä¸ç´°å¾®çç¤¾äº¤äºåï¼Toyteller å©ç¨éäºåä½ (1) è®ä½¿ç¨èå¼å°æäºæå­ç¢çï¼ä»¥å (2) ä½çºä¼´é¨æäºæå­çè¦è¦ºè¼¸åºæ ¼å¼ãæåééå°åä½åæå­å°æå°ä¸åå±ç¨çèªç¾©ç©ºéï¼ä¾åç¨åä½å¼å°çæå­ç¢çåæå­å¼å°çåä½ç¢çï¼ä»¥ä¾¿å¤§åèªè¨æ¨¡åååä½ç¢çæ¨¡åå¯ä»¥ä½¿ç¨å®ä½çºè½è­¯å±¤ãæè¡è©ä¼°é¡¯ç¤º Toyteller åªæ¼ç«¶ç­åºæº GPT-4oãæåçä½¿ç¨èç ç©¶ç¼ç¾ï¼ç©ç©å·æå©æ¼è¡¨éé£ä»¥ç¨è¨èªè¡¨éçæåãç¶èï¼åé åä½ç¡æ³è¡¨éææä½¿ç¨èçæåï¼éè¡¨ç¤ºéè¦å°å¶èèªè¨ç­å¶ä»æ¹å¼çµåãæåè¨è«äºç©ç©å·äºåçè¨­è¨ç©ºéï¼ä»¥åå°äººé¡è AI äºåæè¡ HCI ç ç©¶çå½±é¿ã

##### **Experience with GitHub Copilot for Developer Productivity at Zoominfo**
2501.13282v1 by Gal Bakal, Ali Dasdan, Yaniv Katz, Michael Kaufman, Guy Levin

This paper presents a comprehensive evaluation of GitHub Copilot's deployment
and impact on developer productivity at Zoominfo, a leading Go-To-Market (GTM)
Intelligence Platform. We describe our systematic four-phase approach to
evaluating and deploying GitHub Copilot across our engineering organization,
involving over 400 developers. Our analysis combines both quantitative metrics,
focusing on acceptance rates of suggestions given by GitHub Copilot and
qualitative feedback given by developers through developer satisfaction
surveys. The results show an average acceptance rate of 33% for suggestions and
20% for lines of code, with high developer satisfaction scores of 72%. We also
discuss language-specific performance variations, limitations, and lessons
learned from this medium-scale enterprise deployment. Our findings contribute
to the growing body of knowledge about AI-assisted software development in
enterprise settings.

æè¦ï¼æ¬æå¨é¢è©ä¼°äº GitHub Copilot çé¨ç½²ææ³ï¼ä»¥åå¶å° Zoominfo éç¼äººå¡çç¢åçå½±é¿ï¼Zoominfo æ¯é åçå¸å ´æ¨å»£ (GTM) æå ±å¹³å°ãæåèªªæäºç³»çµ±æ§çåéæ®µæ¹æ³ï¼ä»¥è©ä¼°åé¨ç½² GitHub Copilot å°æåçå·¥ç¨çµç¹ï¼å¶ä¸­æ¶å 400 å¤ä½éç¼äººå¡ãæåçåæçµåäºéåææ¨ï¼éé»éæ³¨ GitHub Copilot çµ¦åºçå»ºè­°çæ¥åçï¼ä»¥åéç¼äººå¡éééç¼äººå¡æ»¿æåº¦èª¿æ¥çµ¦åºçå®æ§åé¥ãçµæé¡¯ç¤ºï¼å»ºè­°çå¹³åæ¥åççº 33%ï¼ç¨å¼ç¢¼è¡çæ¥åççº 20%ï¼éç¼äººå¡æ»¿æåº¦å¾åé«é 72%ãæåä¹è¨è«äºç¹å®èªè¨çæè½å·®ç°ãéå¶ï¼ä»¥åå¾éåä¸­åä¼æ¥­é¨ç½²ä¸­å­¸å°çæè¨ãæåçç¼ç¾æå©æ¼æ´åä¼æ¥­è¨­å®ä¸­ AI è¼å©è»é«éç¼çç¥è­é«ç³»ã

##### **RAG-Reward: Optimizing RAG with Reward Modeling and RLHF**
2501.13264v1 by Hanning Zhang, Juntong Song, Juno Zhu, Yuanhao Wu, Tong Zhang, Cheng Niu

Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs)
with relevant and up-to-date knowledge, improving their ability to answer
knowledge-intensive questions. It has been shown to enhance both generation
quality and trustworthiness. While numerous works have focused on improving
retrieval, generation, and evaluation, the role of reward models in
reinforcement learning for optimizing RAG and establishing automated
benchmarking pipelines remains underexplored. In this paper, we introduce
\textbf{RAG-Reward}, a dataset designed to enable \textit{hallucination-free,
comprehensive, reliable, and efficient RAG}. We define four key metrics for
assessing generation quality and develop an automated annotation pipeline that
leverages multiple LLMs to generate outputs across diverse RAG scenarios.
GPT-4o is used to evaluate and construct preference data. Using
\textbf{RAG-Reward}, we train reward models and apply reinforcement learning
with human feedback (RLHF) to improve LLMs' effectiveness in RAG. Experimental
results show that our reward model achieves state-of-the-art performance on a
held-out test set, demonstrating both the effectiveness of our approach and the
quality of our dataset. Furthermore, the improved generation quality of the
trained policy model highlights the feasibility of using RLHF to enhance RAG
pipelines.

æè¦ï¼<paragraph>æ·åå¢å¼ºçæï¼RAGï¼å¼ºåå¤§åè¯­è¨æ¨¡åï¼LLMï¼
å·å¤ç¸å³ä¸ææ°çç¥è¯ï¼æåå¶åç­ç¥è¯å¯éåé®é¢çè½èãå®å·²è¢«è¯æè½åæ¶å¼ºåçæ
è´¨éåå¯ä¿¡åº¦ãè½ç¶è®¸å¤ç ç©¶ä¸æ³¨äºæ¹è¿æ·åãçæåè¯ä¼°ï¼ä½å¥å±æ¨¡åå¨
å¼ºåå­¦ä¹ ä¸­æ®æ¼çè§è²ï¼ä»¥ä¼å RAG åå»ºç«èªå¨ååºåç®¡éä»æªåå°ååæ¢ç´¢ãå¨æ¬æä¸­ï¼æä»¬ä»ç»
\textbf{RAG-Reward}ï¼ä¸ä¸ªæ°æ®éæ¨å¨å®ç°\textit{æ å¹»è§ãå¨é¢ãå¯é ä¸é«æç RAG}ãæä»¬å®ä¹äºåä¸ªå³é®ææ æ¥
è¯ä¼°çæè´¨éï¼å¹¶å¼åäºä¸ä¸ªèªå¨åæ³¨éç®¡éï¼å©ç¨å¤ä¸ª LLM å¨ä¸åç RAG åºæ¯ä¸­çæè¾åºã
GPT-4o ç¨äºè¯ä¼°åæå»ºåå¥½æ°æ®ãä½¿ç¨
\textbf{RAG-Reward}ï¼æä»¬è®­ç»å¥å±æ¨¡åå¹¶åºç¨å¼ºåå­¦ä¹ 
ç»åäººç±»åé¦ï¼RLHFï¼æ¥æå LLM å¨ RAG ä¸­çæææ§ãå®éª
ç»ææ¾ç¤ºæä»¬çå¥å±æ¨¡åå¨çå­æµè¯éä¸å®ç°äºæåè¿çæ§è½ï¼è¯æäºæä»¬æ¹æ³çæææ§ä»¥å
æä»¬æ°æ®éçè´¨éãæ­¤å¤ï¼è®­ç»å¥½çç­ç¥æ¨¡åççæè´¨éæåçªæ¾äºä½¿ç¨ RLHF å¢å¼º RAG
ç®¡éçå¯è¡æ§ã</paragraph>

##### **Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions**
2501.13230v1 by Yan Ru Pei

We introduce Centaurus, a class of networks composed of generalized
state-space model (SSM) blocks, where the SSM operations can be treated as
tensor contractions during training. The optimal order of tensor contractions
can then be systematically determined for every SSM block to maximize training
efficiency. This allows more flexibility in designing SSM blocks beyond the
depthwise-separable configuration commonly implemented. The new design choices
will take inspiration from classical convolutional blocks including group
convolutions, full convolutions, and bottleneck blocks. We architect the
Centaurus network with a mixture of these blocks, to balance between network
size and performance, as well as memory and computational efficiency during
both training and inference. We show that this heterogeneous network design
outperforms its homogeneous counterparts in raw audio processing tasks
including keyword spotting, speech denoising, and automatic speech recognition
(ASR). For ASR, Centaurus is the first network with competitive performance
that can be made fully state-space based, without using any nonlinear
recurrence (LSTMs), explicit convolutions (CNNs), or (surrogate) attention
mechanism. Source code is available at github.com/Brainchip-Inc/Centaurus

æè¦ï¼æåä»ç´¹ Centaurusï¼ä¸ç¨®ç±å»£ç¾©çæç©ºéæ¨¡å (SSM) åå¡çµæçç¶²è·¯é¡å¥ï¼å¶ä¸­ SSM ä½æ¥­å¯å¨è¨ç·´æéè¦çºå¼µéæ¶ç¸®ãç¶å¾å¯ä»¥ç³»çµ±æ§å°çºæ¯å SSM åå¡ç¢ºå®å¼µéæ¶ç¸®çæä½³é åºï¼ä»¥æå¤§åè¨ç·´æçãéåè¨±å¨è¨­è¨ SSM åå¡æææ´å¤§çå½æ§ï¼è¶è¶äºå¸¸è¦å¯¦ä½çæ·±åº¦å¯åé¢çµæãæ°çè¨­è¨é¸æå°å¾ç¶å¸æ²ç©åå¡ä¸­æ±²åéæï¼åæ¬ç¾¤çµæ²ç©ãå®æ´æ²ç©åç¶é ¸åå¡ãæåä½¿ç¨éäºåå¡çæ··åæ¶æ§ä¾è¨­è¨ Centaurus ç¶²è·¯ï¼å¨è¨ç·´åæ¨è«æéï¼å¨ç¶²è·¯å¤§å°åæè½ï¼ä»¥åè¨æ¶é«åéç®æçä¹éåå¾å¹³è¡¡ãæåå±ç¤ºäºéç¨®ç°è³ªç¶²è·¯è¨­è¨å¨åå§é³è¨èçä»»åä¸­åªæ¼å¶åè³ªå°æé ï¼åæ¬ééµå­åµæ¸¬ãèªé³å»éè¨åèªåèªé³è¾¨è­ (ASR)ãå°æ¼ ASRï¼Centaurus æ¯ç¬¬ä¸åå·æç«¶ç­æè½çç¶²è·¯ï¼å¯ä»¥å®å¨åºæ¼çæç©ºéï¼èç¡éä½¿ç¨ä»»ä½éç·æ§éè¿´ (LSTM)ãæç¢ºæ²ç© (CNN) æï¼æ¿ä»£ï¼æ³¨æåæ©å¶ãåå§ç¨å¼ç¢¼å¯å¨ github.com/Brainchip-Inc/Centaurus åå¾

##### **Learning in Log-Domain: Subthreshold Analog AI Accelerator Based on Stochastic Gradient Descent**
2501.13181v1 by Momen K Tageldeen, Yacine Belgaid, Vivek Mohan, Zhou Wang, Emmanuel M Drakakis

The rapid proliferation of AI models, coupled with growing demand for edge
deployment, necessitates the development of AI hardware that is both
high-performance and energy-efficient. In this paper, we propose a novel analog
accelerator architecture designed for AI/ML training workloads using stochastic
gradient descent with L2 regularization (SGDr). The architecture leverages
log-domain circuits in subthreshold MOS and incorporates volatile memory. We
establish a mathematical framework for solving SGDr in the continuous time
domain and detail the mapping of SGDr learning equations to log-domain
circuits. By operating in the analog domain and utilizing weak inversion, the
proposed design achieves significant reductions in transistor area and power
consumption compared to digital implementations. Experimental results
demonstrate that the architecture closely approximates ideal behavior, with a
mean square error below 0.87% and precision as low as 8 bits. Furthermore, the
architecture supports a wide range of hyperparameters. This work paves the way
for energy-efficient analog AI hardware with on-chip training capabilities.

æè¦ï¼é¨è AI æ¨¡åå¿«éæ´æ£ï¼å ä¸å°éç·£é¨ç½²çéæ±æ¥çå¢å ï¼éä½¿å¾éè¦éç¼æ¢é«æè½åç¯è½ç AI ç¡¬é«ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°çé¡æ¯å éå¨æ¶æ§ï¼å°çºä½¿ç¨é¨æ©æ¢¯åº¦ä¸éæ³æ­é L2 æ­£è¦å (SGDr) ç AI/ML è¨ç·´å·¥ä½è² è¼èè¨­è¨ãæ­¤æ¶æ§å©ç¨äºé¾å¼ MOS ä¸­çå°æ¸åé»è·¯ï¼ä¸¦æ´åäºæ®ç¼æ§è¨æ¶é«ãæåå»ºç«äºä¸åæ¸å­¸æ¡æ¶ï¼ç¨æ¼å¨é£çºæéåä¸­æ±è§£ SGDrï¼ä¸¦è©³ç´°èªªæå° SGDr å­¸ç¿æ¹ç¨å¼å°æå°å°æ¸åé»è·¯çéç¨ãééå¨é¡æ¯åä¸­éä½ä¸¦å©ç¨å¼±åè½ï¼èæ¸ä½å¯¦ä½ç¸æ¯ï¼ææåºçè¨­è¨å¨é»æ¶é«é¢ç©ååèæ¹é¢é½å¤§å¹éä½ãå¯¦é©çµæé¡¯ç¤ºï¼æ­¤æ¶æ§éå¸¸æ¥è¿çæ³è¡çºï¼åæ¹èª¤å·®ä½æ¼ 0.87%ï¼ä¸ç²¾æºåº¦ä½è³ 8 ä½åãæ­¤å¤ï¼æ­¤æ¶æ§æ¯æ´åç¨®è¶åæ¸ãéé å·¥ä½çºå·åæ¶çä¸è¨ç·´åè½çç¯è½é¡æ¯ AI ç¡¬é«éªè·¯ã

##### **QuFeX: Quantum feature extraction module for hybrid quantum-classical deep neural networks**
2501.13165v1 by Naman Jain, Amir Kalev

We introduce Quantum Feature Extraction (QuFeX), a novel quantum machine
learning module. The proposed module enables feature extraction in a
reduced-dimensional space, significantly decreasing the number of parallel
evaluations required in typical quantum convolutional neural network
architectures. Its design allows seamless integration into deep classical
neural networks, making it particularly suitable for hybrid quantum-classical
models. As an application of QuFeX, we propose Qu-Net -- a hybrid architecture
which integrates QuFeX at the bottleneck of a U-Net architecture. The latter is
widely used for image segmentation tasks such as medical imaging and autonomous
driving. Our numerical analysis indicates that the Qu-Net can achieve superior
segmentation performance compared to a U-Net baseline. These results highlight
the potential of QuFeX to enhance deep neural networks by leveraging hybrid
computational paradigms, providing a path towards a robust framework for
real-world applications requiring precise feature extraction.

æè¦ï¼æåå¼å¥äºéå­ç¹å¾µèå (QuFeX)ï¼éæ¯ä¸ååµæ°çéå­æ©å¨å­¸ç¿æ¨¡çµãææåºçæ¨¡çµå¯ä»¥å¨éç¶­ç©ºéä¸­é²è¡ç¹å¾µèåï¼å¤§å¹æ¸å°å¸åéå­å·ç©ç¥ç¶ç¶²è·¯æ¶æ§ä¸­æéçä¸¦è¡è©ä¼°æ¸éãå¶è¨­è¨åè¨±ç¡ç¸«æ´åå°æ·±åº¦å¤å¸ç¥ç¶ç¶²è·¯ä¸­ï¼ä½¿å¶ç¹å¥é©åæ¼æ··åéå­å¤å¸æ¨¡åãä½çº QuFeX çæç¨ï¼æåæåºäº Qu-Netï¼éæ¯ä¸ç¨®æ··åæ¶æ§ï¼å®å¨ U-Net æ¶æ§çç¶é ¸èæ´åäº QuFeXãå¾èå»£æ³ç¨æ¼å½±ååå²ä»»åï¼ä¾å¦é«å­¸å½±ååèªåé§é§ãæåçæ¸å¼åæè¡¨æï¼è U-Net åºæºç¸æ¯ï¼Qu-Net å¯ä»¥å¯¦ç¾åªç°çåå²æè½ãéäºçµæçªé¡¯äº QuFeX ééå©ç¨æ··åéç®ç¯ä¾ä¾å¢å¼·æ·±åº¦ç¥ç¶ç¶²è·¯çæ½åï¼çºéè¦ç²¾ç¢ºç¹å¾µèåççå¯¦ä¸çæç¨ç¨å¼æä¾äºä¸åéåç©©å¥æ¶æ§çéå¾ã

