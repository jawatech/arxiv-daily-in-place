{"2408.02279": {"publish_time": "2024-08-05", "title": "DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for Long Time-Series Forecasting", "paper_summary": "Long-term time series forecasting (LTSF) has been widely applied in finance,\ntraffic prediction, and other domains. Recently, patch-based transformers have\nemerged as a promising approach, segmenting data into sub-level patches that\nserve as input tokens. However, existing methods mostly rely on predetermined\npatch lengths, necessitating expert knowledge and posing challenges in\ncapturing diverse characteristics across various scales. Moreover, time series\ndata exhibit diverse variations and fluctuations across different temporal\nscales, which traditional approaches struggle to model effectively. In this\npaper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm\nto capture diverse receptive fields and sparse patterns of time series data. In\norder to build hierarchical receptive fields, we develop a multi-scale\nTransformer model, coupled with multi-scale sequence extraction, capable of\ncapturing multi-resolution features. Additionally, we introduce a group-aware\nrotary position encoding technique to enhance intra- and inter-group position\nawareness among representations across different temporal scales. Our proposed\nmodel, named DRFormer, is evaluated on various real-world datasets, and\nexperimental results demonstrate its superiority compared to existing methods.\nOur code is available at: https://github.com/ruixindingECNU/DRFormer.", "paper_summary_zh": "\u9577\u671f\u6642\u9593\u5e8f\u5217\u9810\u6e2c (LTSF) \u5df2\u5ee3\u6cdb\u61c9\u7528\u65bc\u91d1\u878d\u3001\u4ea4\u901a\u9810\u6e2c\u548c\u5176\u4ed6\u9818\u57df\u3002\u6700\u8fd1\uff0c\u57fa\u65bc patch \u7684 Transformer \u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u5c07\u8cc7\u6599\u5206\u5272\u6210\u5b50\u5c64\u7d1a patch\uff0c\u4f5c\u70ba\u8f38\u5165 token\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8cf4\u65bc\u9810\u5148\u78ba\u5b9a\u7684 patch \u9577\u5ea6\uff0c\u9700\u8981\u5c08\u5bb6\u77e5\u8b58\uff0c\u4e26\u5728\u64f7\u53d6\u5404\u7a2e\u898f\u6a21\u7684\u4e0d\u540c\u7279\u5fb5\u6642\u69cb\u6210\u6311\u6230\u3002\u6b64\u5916\uff0c\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u5728\u4e0d\u540c\u7684\u6642\u9593\u5c3a\u5ea6\u4e0a\u8868\u73fe\u51fa\u4e0d\u540c\u7684\u8b8a\u5316\u548c\u6ce2\u52d5\uff0c\u50b3\u7d71\u65b9\u6cd5\u96e3\u4ee5\u6709\u6548\u5efa\u6a21\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5177\u6709\u52d5\u614b\u7a00\u758f\u5b78\u7fd2\u6f14\u7b97\u6cd5\u7684\u52d5\u614b token\uff0c\u4ee5\u64f7\u53d6\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u7684\u4e0d\u540c\u611f\u53d7\u91ce\u548c\u7a00\u758f\u6a21\u5f0f\u3002\u70ba\u4e86\u5efa\u7acb\u5206\u5c64\u611f\u53d7\u91ce\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u591a\u5c3a\u5ea6 Transformer \u6a21\u578b\uff0c\u7d50\u5408\u591a\u5c3a\u5ea6\u5e8f\u5217\u63d0\u53d6\uff0c\u80fd\u5920\u64f7\u53d6\u591a\u89e3\u6790\u5ea6\u7279\u5fb5\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u7fa4\u7d44\u611f\u77e5\u65cb\u8f49\u4f4d\u7f6e\u7de8\u78bc\u6280\u8853\uff0c\u4ee5\u589e\u5f37\u4e0d\u540c\u6642\u9593\u5c3a\u5ea6\u4e0a\u8868\u793a\u4e4b\u9593\u7684\u7d44\u5167\u548c\u7d44\u9593\u4f4d\u7f6e\u611f\u77e5\u3002\u6211\u5011\u63d0\u51fa\u7684\u6a21\u578b\uff0c\u7a31\u70ba DRFormer\uff0c\u5728\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u5b83\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/ruixindingECNU/DRFormer \u7372\u5f97\u3002", "author": "Ruixin Ding et.al.", "authors": "Ruixin Ding, Yuqi Chen, Yu-Ting Lan, Wei Zhang", "id": "2408.02279v1", "paper_url": "http://arxiv.org/abs/2408.02279v1", "repo": "null"}}