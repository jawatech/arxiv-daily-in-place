{"2411.17607": {"publish_time": "2024-11-26", "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data", "paper_summary": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower sampling rates (e.g. 12.5Hz), while still\nmaintaining speech reconstruction quality. Starting from a pre-trained language\nmodel and scaling our pre-training to 1 trillion tokens (with 600B synthetic\ninterleaved speech-text data), we achieve state-of-the-art performance in\nspeech language modeling and spoken question answering, improving performance\non spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We\nfurther demonstrate that by fine-tuning the pre-trained model with speech\ndialogue data, we can develop an end-to-end spoken chatbot that achieves\ncompetitive performance comparable to existing baselines in both conversational\nabilities and speech quality, even operating exclusively in the speech domain.", "paper_summary_zh": "\u8a9e\u97f3\u8a9e\u8a00\u6a21\u578b (SpeechLM) \u63a5\u53d7\u8a9e\u97f3\u8f38\u5165\u4e26\u7522\u751f\u8a9e\u97f3\u8f38\u51fa\uff0c\u8207\u57fa\u65bc\u6587\u5b57\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u76f8\u6bd4\uff0c\u5141\u8a31\u66f4\u81ea\u7136\u7684\u96fb\u8166\u4e92\u52d5\u3002\u958b\u767c SpeechLM \u7684\u50b3\u7d71\u65b9\u6cd5\u53d7\u5230\u7121\u76e3\u7763\u8a9e\u97f3\u8cc7\u6599\u548c\u5e73\u884c\u8a9e\u97f3\u6587\u5b57\u8cc7\u6599\u6709\u9650\u7684\u53ef\u7528\u6027\u6240\u9650\u5236\uff0c\u9019\u4e9b\u8cc7\u6599\u9060\u5c11\u65bc\u6587\u5b57\u9810\u8a13\u7df4\u8cc7\u6599\uff0c\u5f9e\u800c\u9650\u5236\u4e86\u5b83\u5011\u4f5c\u70ba LLM \u7684\u53ef\u64f4\u5145\u6027\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\u4f86\u64f4\u5145\u8a9e\u97f3\u6587\u5b57\u9810\u8a13\u7df4\uff0c\u65b9\u6cd5\u662f\u5229\u7528\u5f9e\u6587\u5b57\u8a9e\u6599\u5eab\u4e2d\u884d\u751f\u7684\u3001\u5927\u898f\u6a21\u7684\u5408\u6210\u4ea4\u932f\u8cc7\u6599\uff0c\u5f9e\u800c\u6d88\u9664\u4e86\u5c0d\u5e73\u884c\u8a9e\u97f3\u6587\u5b57\u8cc7\u6599\u96c6\u7684\u9700\u6c42\u3002\u6211\u5011\u7684\u6a21\u578b\u900f\u904e\u5f9e\u73fe\u6709\u7684\u6587\u5b57\u8a9e\u6599\u5eab\u4e2d\u62bd\u6a23\u6587\u5b57\u8de8\u5ea6\uff0c\u4e26\u4f7f\u7528\u6587\u5b57\u8f49\u63db\u6a19\u8a18\u6a21\u578b\u5408\u6210\u5c0d\u61c9\u7684\u8a9e\u97f3\u8de8\u5ea6\uff0c\u6709\u6548\u5730\u5efa\u69cb\u8a9e\u97f3\u6587\u5b57\u4ea4\u932f\u8cc7\u6599\uff0c\u800c\u7121\u9700\u7522\u751f\u5be6\u969b\u8a9e\u97f3\u3002\u6211\u5011\u9084\u900f\u904e\u5728\u7de8\u78bc\u5668\u4e2d\u52a0\u5165\u5411\u91cf\u91cf\u5316\u74f6\u9838\uff0c\u63a1\u7528\u4e86\u6e90\u81ea\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u6a21\u578b\u7684\u76e3\u7763\u5f0f\u8a9e\u97f3\u6a19\u8a18\u5668\u3002\u9019\u7a2e\u76e3\u7763\u5f0f\u8a13\u7df4\u65b9\u6cd5\u7522\u751f\u4e86\u96e2\u6563\u7684\u8a9e\u97f3\u6a19\u8a18\uff0c\u5373\u4f7f\u5728\u8f03\u4f4e\u7684\u53d6\u6a23\u7387\uff08\u4f8b\u5982 12.5Hz\uff09\u4e0b\uff0c\u4e5f\u80fd\u5f37\u6709\u529b\u5730\u4fdd\u7559\u8a9e\u610f\uff0c\u540c\u6642\u9084\u80fd\u7dad\u6301\u8a9e\u97f3\u91cd\u5efa\u54c1\u8cea\u3002\u5f9e\u9810\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\u958b\u59cb\uff0c\u4e26\u5c07\u6211\u5011\u7684\u9810\u8a13\u7df4\u64f4\u5145\u5230 1 \u5146\u500b\u6a19\u8a18\uff08\u4f7f\u7528 600B \u5408\u6210\u4ea4\u932f\u8a9e\u97f3\u6587\u5b57\u8cc7\u6599\uff09\uff0c\u6211\u5011\u5728\u8a9e\u97f3\u8a9e\u8a00\u5efa\u6a21\u548c\u53e3\u8aaa\u554f\u984c\u56de\u7b54\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u5c07\u53e3\u8aaa\u554f\u984c\u4efb\u52d9\u7684\u6548\u80fd\u5f9e\u5148\u524d\u7684 SOTA 13%\uff08Moshi\uff09\u63d0\u5347\u81f3 31%\u3002\u6211\u5011\u9032\u4e00\u6b65\u8b49\u660e\uff0c\u900f\u904e\u4f7f\u7528\u8a9e\u97f3\u5c0d\u8a71\u8cc7\u6599\u5fae\u8abf\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u6211\u5011\u53ef\u4ee5\u958b\u767c\u4e00\u500b\u7aef\u5230\u7aef\u7684\u53e3\u8aaa\u804a\u5929\u6a5f\u5668\u4eba\uff0c\u5728\u5c0d\u8a71\u80fd\u529b\u548c\u8a9e\u97f3\u54c1\u8cea\u65b9\u9762\u90fd\u80fd\u9054\u5230\u8207\u73fe\u6709\u57fa\u6e96\u76f8\u7576\u7684\u7af6\u722d\u529b\uff0c\u751a\u81f3\u5b8c\u5168\u5728\u8a9e\u97f3\u9818\u57df\u4e2d\u904b\u4f5c\u3002", "author": "Aohan Zeng et.al.", "authors": "Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang", "id": "2411.17607v1", "paper_url": "http://arxiv.org/abs/2411.17607v1", "repo": "null"}}