{"2410.21083": {"publish_time": "2024-10-28", "title": "Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring", "paper_summary": "Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5b89\u5168\u6027\u662f\u4e00\u500b\u95dc\u9375\u554f\u984c\uff0c\u8a31\u591a\u7814\u7a76\u63a1\u7528\u7d05\u968a\u6e2c\u8a66\u4f86\u589e\u5f37\u6a21\u578b\u5b89\u5168\u6027\u3002\u5176\u4e2d\uff0c\u8d8a\u7344\u65b9\u6cd5\u900f\u904e\u88fd\u4f5c\u60e1\u610f\u7684\u63d0\u793a\u4f86\u63a2\u7d22\u6f5b\u5728\u7684\u6f0f\u6d1e\uff0c\u9019\u4e9b\u63d0\u793a\u6703\u8a98\u767c\u8207\u5b89\u5168\u5c0d\u9f4a\u76f8\u53cd\u7684\u6a21\u578b\u8f38\u51fa\u3002\u73fe\u6709\u7684\u9ed1\u76d2\u8d8a\u7344\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u65bc\u6a21\u578b\u56de\u994b\uff0c\u5728\u653b\u64ca\u641c\u5c0b\u904e\u7a0b\u4e2d\u91cd\u8907\u63d0\u4ea4\u5e36\u6709\u53ef\u6aa2\u6e2c\u60e1\u610f\u6307\u4ee4\u7684\u67e5\u8a62\u3002\u5118\u7ba1\u9019\u4e9b\u65b9\u6cd5\u6709\u6548\uff0c\u4f46\u653b\u64ca\u53ef\u80fd\u6703\u5728\u641c\u5c0b\u904e\u7a0b\u4e2d\u88ab\u5167\u5bb9\u5be9\u67e5\u54e1\u6514\u622a\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u6539\u9032\u7684\u8f49\u79fb\u653b\u64ca\u65b9\u6cd5\uff0c\u901a\u904e\u900f\u904e\u826f\u6027\u8cc7\u6599\u8403\u53d6\u4f86\u5728\u672c\u5730\u8a13\u7df4\u76ee\u6a19\u9ed1\u76d2\u6a21\u578b\u7684\u93e1\u50cf\u6a21\u578b\uff0c\u4f86\u5f15\u5c0e\u60e1\u610f\u63d0\u793a\u7684\u5efa\u69cb\u3002\u9019\u7a2e\u65b9\u6cd5\u63d0\u4f9b\u4e86\u589e\u5f37\u7684\u96b1\u853d\u6027\uff0c\u56e0\u70ba\u5b83\u4e0d\u6d89\u53ca\u5728\u641c\u5c0b\u968e\u6bb5\u5411\u76ee\u6a19\u6a21\u578b\u63d0\u4ea4\u53ef\u8b58\u5225\u7684\u60e1\u610f\u6307\u4ee4\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728 AdvBench \u7684\u4e00\u500b\u5b50\u96c6\u4e2d\u5c0d GPT-3.5 Turbo \u653b\u64ca\u6210\u529f\u7387\u6700\u9ad8\u9054\u5230 92%\uff0c\u6216\u5e73\u8861\u503c\u70ba 80%\uff0c\u5e73\u5747\u6bcf\u500b\u7bc4\u4f8b\u6709 1.5 \u500b\u53ef\u6aa2\u6e2c\u7684\u8d8a\u7344\u67e5\u8a62\u3002\u9019\u4e9b\u7d50\u679c\u5f37\u8abf\u4e86\u5c0d\u66f4\u5f37\u5927\u7684\u9632\u79a6\u6a5f\u5236\u6709\u5fc5\u8981\u6027\u7684\u9700\u6c42\u3002", "author": "Honglin Mu et.al.", "authors": "Honglin Mu, Han He, Yuxin Zhou, Yunlong Feng, Yang Xu, Libo Qin, Xiaoming Shi, Zeming Liu, Xudong Han, Qi Shi, Qingfu Zhu, Wanxiang Che", "id": "2410.21083v1", "paper_url": "http://arxiv.org/abs/2410.21083v1", "repo": "null"}}