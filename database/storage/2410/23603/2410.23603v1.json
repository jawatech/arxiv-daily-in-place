{"2410.23603": {"publish_time": "2024-10-31", "title": "Using Multimodal Deep Neural Networks to Disentangle Language from Visual Aesthetics", "paper_summary": "When we experience a visual stimulus as beautiful, how much of that\nexperience derives from perceptual computations we cannot describe versus\nconceptual knowledge we can readily translate into natural language?\nDisentangling perception from language in visually-evoked affective and\naesthetic experiences through behavioral paradigms or neuroimaging is often\nempirically intractable. Here, we circumnavigate this challenge by using linear\ndecoding over the learned representations of unimodal vision, unimodal\nlanguage, and multimodal (language-aligned) deep neural network (DNN) models to\npredict human beauty ratings of naturalistic images. We show that unimodal\nvision models (e.g. SimCLR) account for the vast majority of explainable\nvariance in these ratings. Language-aligned vision models (e.g. SLIP) yield\nsmall gains relative to unimodal vision. Unimodal language models (e.g. GPT2)\nconditioned on visual embeddings to generate captions (via CLIPCap) yield no\nfurther gains. Caption embeddings alone yield less accurate predictions than\nimage and caption embeddings combined (concatenated). Taken together, these\nresults suggest that whatever words we may eventually find to describe our\nexperience of beauty, the ineffable computations of feedforward perception may\nprovide sufficient foundation for that experience.", "paper_summary_zh": "\u7576\u6211\u5011\u9ad4\u9a57\u5230\u8996\u89ba\u523a\u6fc0\u6642\uff0c\u5176\u4e2d\u6709\u591a\u5c11\u662f\u6e90\u81ea\u65bc\u6211\u5011\u7121\u6cd5\u63cf\u8ff0\u7684\u77e5\u89ba\u8a08\u7b97\uff0c\u76f8\u8f03\u65bc\u6211\u5011\u53ef\u4ee5\u8f15\u6613\u7ffb\u8b6f\u6210\u81ea\u7136\u8a9e\u8a00\u7684\u6982\u5ff5\u77e5\u8b58\uff1f\u900f\u904e\u884c\u70ba\u7bc4\u4f8b\u6216\u795e\u7d93\u5f71\u50cf\u5b78\uff0c\u8981\u5c07\u8996\u89ba\u8a98\u767c\u7684\u60c5\u611f\u548c\u7f8e\u5b78\u9ad4\u9a57\u4e2d\u7684\u77e5\u89ba\u8207\u8a9e\u8a00\u5340\u5206\u958b\u4f86\uff0c\u901a\u5e38\u5728\u7d93\u9a57\u4e0a\u96e3\u4ee5\u6349\u6478\u3002\u5728\u6b64\uff0c\u6211\u5011\u900f\u904e\u5c0d\u55ae\u6a21\u614b\u8996\u89ba\u3001\u55ae\u6a21\u614b\u8a9e\u8a00\u548c\u591a\u6a21\u614b\uff08\u8a9e\u8a00\u5c0d\u9f4a\uff09\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\uff08DNN\uff09\u6a21\u578b\u7684\u5b78\u7fd2\u8868\u5fb5\u9032\u884c\u7dda\u6027\u89e3\u78bc\uff0c\u4f86\u9810\u6e2c\u4eba\u985e\u5c0d\u81ea\u7136\u5f71\u50cf\u7684\u7f8e\u611f\u8a55\u5206\uff0c\u5f9e\u800c\u898f\u907f\u4e86\u9019\u500b\u6311\u6230\u3002\u6211\u5011\u8868\u660e\uff0c\u55ae\u6a21\u614b\u8996\u89ba\u6a21\u578b\uff08\u4f8b\u5982 SimCLR\uff09\u89e3\u91cb\u4e86\u9019\u4e9b\u8a55\u5206\u4e2d\u7d55\u5927\u90e8\u5206\u7684\u53ef\u89e3\u91cb\u8b8a\u7570\u3002\u8207\u55ae\u6a21\u614b\u8996\u89ba\u76f8\u6bd4\uff0c\u8a9e\u8a00\u5c0d\u9f4a\u7684\u8996\u89ba\u6a21\u578b\uff08\u4f8b\u5982 SLIP\uff09\u7522\u751f\u4e86\u8f03\u5c0f\u7684\u589e\u76ca\u3002\u4ee5\u8996\u89ba\u5d4c\u5165\u70ba\u689d\u4ef6\u7522\u751f\u5b57\u5e55\u7684\u55ae\u6a21\u614b\u8a9e\u8a00\u6a21\u578b\uff08\u4f8b\u5982 GPT2\uff09\uff08\u900f\u904e CLIPCap\uff09\u6c92\u6709\u7522\u751f\u9032\u4e00\u6b65\u7684\u589e\u76ca\u3002\u55ae\u7368\u7684\u5b57\u5e55\u5d4c\u5165\u7522\u751f\u7684\u9810\u6e2c\u6e96\u78ba\u5ea6\u4f4e\u65bc\u5f71\u50cf\u548c\u5b57\u5e55\u5d4c\u5165\u7684\u7d44\u5408\uff08\u4e32\u63a5\uff09\u3002\u7d9c\u5408\u8d77\u4f86\uff0c\u9019\u4e9b\u7d50\u679c\u8868\u660e\uff0c\u7121\u8ad6\u6211\u5011\u6700\u7d42\u627e\u5230\u4ec0\u9ebc\u8a5e\u4f86\u63cf\u8ff0\u6211\u5011\u7684\u5be9\u7f8e\u9ad4\u9a57\uff0c\u524d\u994b\u611f\u77e5\u7684\u96e3\u4ee5\u8a00\u55bb\u7684\u8a08\u7b97\u53ef\u80fd\u70ba\u9019\u7a2e\u9ad4\u9a57\u63d0\u4f9b\u4e86\u5145\u5206\u7684\u57fa\u790e\u3002", "author": "Colin Conwell et.al.", "authors": "Colin Conwell, Christopher Hamblin, Chelsea Boccagno, David Mayo, Jesse Cummings, Leyla Isik, Andrei Barbu", "id": "2410.23603v1", "paper_url": "http://arxiv.org/abs/2410.23603v1", "repo": "null"}}