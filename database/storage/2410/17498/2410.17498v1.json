{"2410.17498": {"publish_time": "2024-10-23", "title": "Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks", "paper_summary": "Large Language Models (LLMs) have demonstrated impressive abilities in symbol\nprocessing through in-context learning (ICL). This success flies in the face of\ndecades of predictions that artificial neural networks cannot master abstract\nsymbol manipulation. We seek to understand the mechanisms that can enable\nrobust symbol processing in transformer networks, illuminating both the\nunanticipated success, and the significant limitations, of transformers in\nsymbol processing. Borrowing insights from symbolic AI on the power of\nProduction System architectures, we develop a high-level language, PSL, that\nallows us to write symbolic programs to do complex, abstract symbol processing,\nand create compilers that precisely implement PSL programs in transformer\nnetworks which are, by construction, 100% mechanistically interpretable. We\ndemonstrate that PSL is Turing Universal, so the work can inform the\nunderstanding of transformer ICL in general. The type of transformer\narchitecture that we compile from PSL programs suggests a number of paths for\nenhancing transformers' capabilities at symbol processing. (Note: The first\nsection of the paper gives an extended synopsis of the entire paper.)", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u900f\u904e\u60c5\u5883\u5b78\u7fd2 (ICL) \u5c55\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7b26\u865f\u8655\u7406\u80fd\u529b\u3002\u9019\u9805\u6210\u529f\u8207\u6578\u5341\u5e74\u4f86\u9810\u6e2c\u4eba\u5de5\u795e\u7d93\u7db2\u8def\u7121\u6cd5\u638c\u63e1\u62bd\u8c61\u7b26\u865f\u64cd\u4f5c\u7684\u8aaa\u6cd5\u80cc\u9053\u800c\u99b3\u3002\u6211\u5011\u8a66\u5716\u4e86\u89e3\u80fd\u5920\u8b93Transformer\u7db2\u8def\u9032\u884c\u7a69\u5065\u7b26\u865f\u8655\u7406\u7684\u6a5f\u5236\uff0c\u540c\u6642\u8aaa\u660eTransformer\u5728\u7b26\u865f\u8655\u7406\u65b9\u9762\u610f\u6599\u4e4b\u5916\u7684\u6210\u529f\u548c\u986f\u8457\u9650\u5236\u3002\u6211\u5011\u5f9e\u7b26\u865f AI \u501f\u7528\u751f\u7522\u7cfb\u7d71\u67b6\u69cb\u7684\u529b\u91cf\uff0c\u958b\u767c\u51fa\u4e00\u7a2e\u9ad8\u7d1a\u8a9e\u8a00 PSL\uff0c\u5b83\u8b93\u6211\u5011\u80fd\u5920\u64b0\u5beb\u7b26\u865f\u7a0b\u5f0f\u4f86\u9032\u884c\u8907\u96dc\u7684\u62bd\u8c61\u7b26\u865f\u8655\u7406\uff0c\u4e26\u5efa\u7acb\u7cbe\u78ba\u5be6\u4f5cTransformer\u7db2\u8def\u4e2d PSL \u7a0b\u5f0f\u7684\u7de8\u8b6f\u5668\uff0c\u9019\u4e9b\u7de8\u8b6f\u5668\u5728\u5efa\u69cb\u4e0a 100% \u53ef\u4ee5\u6a5f\u68b0\u5f0f\u5730\u89e3\u91cb\u3002\u6211\u5011\u8b49\u660e\u4e86 PSL \u662f\u5716\u9748\u901a\u7528\u7684\uff0c\u56e0\u6b64\u9019\u9805\u5de5\u4f5c\u53ef\u4ee5\u8aaa\u660e\u5c0dTransformer ICL \u7684\u4e00\u822c\u7406\u89e3\u3002\u6211\u5011\u5f9e PSL \u7a0b\u5f0f\u7de8\u8b6f\u800c\u4f86\u7684Transformer\u67b6\u69cb\u985e\u578b\uff0c\u5efa\u8b70\u4e86\u4e00\u4e9b\u589e\u5f37Transformer\u7b26\u865f\u8655\u7406\u529f\u80fd\u7684\u8def\u5f91\u3002\uff08\u8a3b\uff1a\u672c\u6587\u7684\u7b2c\u4e00\u90e8\u5206\u63d0\u4f9b\u4e86\u6574\u7bc7\u8ad6\u6587\u7684\u5ef6\u4f38\u6458\u8981\u3002\uff09", "author": "Paul Smolensky et.al.", "authors": "Paul Smolensky, Roland Fernandez, Zhenghao Herbert Zhou, Mattia Opper, Jianfeng Gao", "id": "2410.17498v1", "paper_url": "http://arxiv.org/abs/2410.17498v1", "repo": "null"}}