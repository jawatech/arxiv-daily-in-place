{"2411.14121": {"publish_time": "2024-11-21", "title": "Learning from \"Silly\" Questions Improves Large Language Models, But Only Slightly", "paper_summary": "Constructing high-quality Supervised Fine-Tuning (SFT) datasets is critical\nfor the training of large language models (LLMs). Recent studies have shown\nthat using data from a specific source, Ruozhiba, a Chinese website where users\nask \"silly\" questions to better understand certain topics, can lead to better\nfine-tuning performance. This paper aims to explore some hidden factors: the\npotential interpretations of its success and a large-scale evaluation of the\nperformance. First, we leverage GPT-4 to analyze the successful cases of\nRuozhiba questions from the perspective of education, psychology, and cognitive\nscience, deriving a set of explanatory rules. Then, we construct fine-tuning\ndatasets by applying these rules to the MMLU training set. Surprisingly, our\nresults indicate that rules can significantly improve model performance in\ncertain tasks, while potentially diminishing performance on others. For\nexample, SFT data generated following the \"Counterintuitive Thinking\" rule can\nachieve approximately a 5% improvement on the \"Global Facts\" task, whereas the\n\"Blurring the Conceptual Boundaries\" rule leads to a performance drop of 6.14%\non the \"Econometrics\" task. In addition, for specific tasks, different rules\ntend to have a consistent impact on model performance. This suggests that the\ndifferences between the extracted rules are not as significant, and the\neffectiveness of the rules is relatively consistent across tasks. Our research\nhighlights the importance of considering task diversity and rule applicability\nwhen constructing SFT datasets to achieve more comprehensive performance\nimprovements.", "paper_summary_zh": "<paragraph>\u5efa\u69cb\u9ad8\u54c1\u8cea\u7684\u76e3\u7763\u5f0f\u5fae\u8abf (SFT) \u8cc7\u6599\u96c6\u5c0d\u65bc\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u81f3\u95dc\u91cd\u8981\u3002\u6700\u8fd1\u7684\u7814\u7a76\u986f\u793a\uff0c\u4f7f\u7528\u4f86\u81ea\u7279\u5b9a\u4f86\u6e90\u7684\u8cc7\u6599\uff0c\u4f8b\u5982 Ruozhiba\uff0c\u4e00\u500b\u4f7f\u7528\u8005\u63d0\u51fa\u300c\u611a\u8822\u300d\u554f\u984c\u4ee5\u66f4\u6df1\u5165\u4e86\u89e3\u7279\u5b9a\u4e3b\u984c\u7684\u4e2d\u6587\u7db2\u7ad9\uff0c\u53ef\u4ee5\u63d0\u5347\u5fae\u8abf\u7684\u6548\u80fd\u3002\u672c\u6587\u65e8\u5728\u63a2\u8a0e\u4e00\u4e9b\u96b1\u85cf\u7684\u56e0\u7d20\uff1a\u6210\u529f\u80cc\u5f8c\u7684\u6f5b\u5728\u8a6e\u91cb\u4ee5\u53ca\u6548\u80fd\u7684\u5927\u898f\u6a21\u8a55\u4f30\u3002\u9996\u5148\uff0c\u6211\u5011\u5229\u7528 GPT-4 \u5f9e\u6559\u80b2\u3001\u5fc3\u7406\u5b78\u548c\u8a8d\u77e5\u79d1\u5b78\u7684\u89d2\u5ea6\u5206\u6790 Ruozhiba \u554f\u984c\u7684\u6210\u529f\u6848\u4f8b\uff0c\u63a8\u5c0e\u51fa\u4e00\u7d44\u89e3\u91cb\u6027\u898f\u5247\u3002\u63a5\u8457\uff0c\u6211\u5011\u5c07\u9019\u4e9b\u898f\u5247\u5957\u7528\u81f3 MMLU \u8a13\u7df4\u96c6\uff0c\u5efa\u69cb\u5fae\u8abf\u8cc7\u6599\u96c6\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\u898f\u5247\u53ef\u4ee5\u986f\u8457\u63d0\u5347\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52d9\u4e2d\u7684\u6548\u80fd\uff0c\u4f46\u53ef\u80fd\u6703\u964d\u4f4e\u5176\u4ed6\u4efb\u52d9\u7684\u6548\u80fd\u3002\u4f8b\u5982\uff0c\u9075\u5faa\u300c\u53cd\u76f4\u89ba\u601d\u8003\u300d\u898f\u5247\u7522\u751f\u7684 SFT \u8cc7\u6599\uff0c\u5728\u300c\u5168\u7403\u4e8b\u5be6\u300d\u4efb\u52d9\u4e2d\u53ef\u4ee5\u63d0\u5347\u7d04 5%\uff0c\u800c\u300c\u6a21\u7cca\u6982\u5ff5\u754c\u7dda\u300d\u898f\u5247\u5247\u5c0e\u81f4\u300c\u8a08\u91cf\u7d93\u6fdf\u5b78\u300d\u4efb\u52d9\u7684\u6548\u80fd\u4e0b\u964d 6.14%\u3002\u6b64\u5916\uff0c\u5c0d\u65bc\u7279\u5b9a\u4efb\u52d9\uff0c\u4e0d\u540c\u7684\u898f\u5247\u5f80\u5f80\u5c0d\u6a21\u578b\u6548\u80fd\u7522\u751f\u4e00\u81f4\u7684\u5f71\u97ff\u3002\u9019\u8868\u793a\u63d0\u53d6\u51fa\u7684\u898f\u5247\u4e4b\u9593\u7684\u5dee\u7570\u4e26\u4e0d\u660e\u986f\uff0c\u800c\u4e14\u898f\u5247\u7684\u6709\u6548\u6027\u5728\u5404\u9805\u4efb\u52d9\u4e2d\u76f8\u5c0d\u4e00\u81f4\u3002\u6211\u5011\u7684\u7814\u7a76\u5f37\u8abf\u5728\u5efa\u69cb SFT \u8cc7\u6599\u96c6\u6642\uff0c\u8003\u91cf\u4efb\u52d9\u7684\u591a\u6a23\u6027\u548c\u898f\u5247\u7684\u9069\u7528\u6027\u975e\u5e38\u91cd\u8981\uff0c\u624d\u80fd\u9054\u6210\u66f4\u5168\u9762\u7684\u6548\u80fd\u63d0\u5347\u3002</paragraph>", "author": "Tingyuan Zhu et.al.", "authors": "Tingyuan Zhu, Shudong Liu, Yidong Wang, Derek F. Wong, Han Yu, Takahiro Shinozaki, Jindong Wang", "id": "2411.14121v1", "paper_url": "http://arxiv.org/abs/2411.14121v1", "repo": "null"}}