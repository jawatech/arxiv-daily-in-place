{"2405.11655": {"publish_time": "2024-05-19", "title": "Track Anything Rapter(TAR)", "paper_summary": "Object tracking is a fundamental task in computer vision with broad practical\napplications across various domains, including traffic monitoring, robotics,\nand autonomous vehicle tracking. In this project, we aim to develop a\nsophisticated aerial vehicle system known as Track Anything Raptor (TAR),\ndesigned to detect, segment, and track objects of interest based on\nuser-provided multimodal queries, such as text, images, and clicks. TAR\nutilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate\nthe relative pose of the queried object. The tracking problem is approached as\na Visual Servoing task, enabling the UAV to consistently focus on the object\nthrough advanced motion planning and control algorithms. We showcase how the\nintegration of these foundational models with a custom high-level control\nalgorithm results in a highly stable and precise tracking system deployed on a\ncustom-built PX4 Autopilot-enabled Voxl2 M500 drone. To validate the tracking\nalgorithm's performance, we compare it against Vicon-based ground truth.\nAdditionally, we evaluate the reliability of the foundational models in aiding\ntracking in scenarios involving occlusions. Finally, we test and validate the\nmodel's ability to work seamlessly with multiple modalities, such as click,\nbounding box, and image templates.", "paper_summary_zh": "\u7269\u9ad4\u8ffd\u8e64\u662f\u96fb\u8166\u8996\u89ba\u4e2d\u7684\u4e00\u9805\u57fa\u672c\u4efb\u52d9\uff0c\u5728\u5305\u62ec\u4ea4\u901a\u76e3\u63a7\u3001\u6a5f\u5668\u4eba\u548c\u81ea\u52d5\u8eca\u8f1b\u8ffd\u8e64\u7b49\u591a\u7a2e\u9818\u57df\u90fd\u6709\u5ee3\u6cdb\u7684\u5be6\u969b\u61c9\u7528\u3002\u5728\u9019\u500b\u5c08\u6848\u4e2d\uff0c\u6211\u5011\u7684\u76ee\u6a19\u662f\u958b\u767c\u4e00\u500b\u7a31\u70ba Track Anything Raptor (TAR) \u7684\u5148\u9032\u7a7a\u4e2d\u8f09\u5177\u7cfb\u7d71\uff0c\u65e8\u5728\u6839\u64da\u4f7f\u7528\u8005\u63d0\u4f9b\u7684\u591a\u6a21\u614b\u67e5\u8a62\uff08\u4f8b\u5982\u6587\u5b57\u3001\u5f71\u50cf\u548c\u9ede\u64ca\uff09\u4f86\u5075\u6e2c\u3001\u5206\u5272\u548c\u8ffd\u8e64\u611f\u8208\u8da3\u7684\u7269\u9ad4\u3002TAR \u5229\u7528 DINO\u3001CLIP \u548c SAM \u7b49\u5148\u9032\u7684\u9810\u5148\u8a13\u7df4\u6a21\u578b\u4f86\u4f30\u8a08\u67e5\u8a62\u7269\u9ad4\u7684\u76f8\u5c0d\u4f4d\u59ff\u3002\u8ffd\u8e64\u554f\u984c\u88ab\u8996\u70ba\u4e00\u500b\u8996\u89ba\u4f3a\u670d\u4efb\u52d9\uff0c\u8b93\u7121\u4eba\u6a5f\u80fd\u5920\u900f\u904e\u5148\u9032\u7684\u904b\u52d5\u898f\u5283\u548c\u63a7\u5236\u6f14\u7b97\u6cd5\u6301\u7e8c\u5c0d\u7126\u5728\u7269\u9ad4\u4e0a\u3002\u6211\u5011\u5c55\u793a\u4e86\u9019\u4e9b\u57fa\u790e\u6a21\u578b\u8207\u81ea\u8a02\u9ad8\u968e\u63a7\u5236\u6f14\u7b97\u6cd5\u6574\u5408\u5f8c\uff0c\u5982\u4f55\u7522\u751f\u4e00\u500b\u90e8\u7f72\u5728\u81ea\u8a02 PX4 Autopilot \u652f\u63f4\u7684 Voxl2 M500 \u7121\u4eba\u6a5f\u4e0a\u7684\u9ad8\u5ea6\u7a69\u5b9a\u4e14\u7cbe\u78ba\u7684\u8ffd\u8e64\u7cfb\u7d71\u3002\u70ba\u4e86\u9a57\u8b49\u8ffd\u8e64\u6f14\u7b97\u6cd5\u7684\u6548\u80fd\uff0c\u6211\u5011\u5c07\u5176\u8207\u57fa\u65bc Vicon \u7684\u5730\u9762\u5be6\u6cc1\u9032\u884c\u6bd4\u8f03\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u57fa\u790e\u6a21\u578b\u5728\u5354\u52a9\u8ffd\u8e64\u906e\u64cb\u60c5\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002\u6700\u5f8c\uff0c\u6211\u5011\u6e2c\u8a66\u4e26\u9a57\u8b49\u4e86\u6a21\u578b\u8207\u591a\u7a2e\u6a21\u614b\uff08\u4f8b\u5982\u9ede\u64ca\u3001\u908a\u754c\u6846\u548c\u5f71\u50cf\u7bc4\u672c\uff09\u7121\u7e2b\u904b\u4f5c\u7684\u80fd\u529b\u3002", "author": "Tharun V. Puthanveettil et.al.", "authors": "Tharun V. Puthanveettil, Fnu Obaid ur Rahman", "id": "2405.11655v1", "paper_url": "http://arxiv.org/abs/2405.11655v1", "repo": "null"}}