{"2412.03248": {"publish_time": "2024-12-04", "title": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning", "paper_summary": "Large language models (LLMs) have enabled the creation of multi-modal LLMs\nthat exhibit strong comprehension of visual data such as images and videos.\nHowever, these models usually rely on extensive visual tokens from visual\nencoders, leading to high computational demands, which limits their\napplicability in resource-constrained environments and for long-context tasks.\nIn this work, we propose a training-free adaptive inference method for\nmulti-modal LLMs that can accommodate a broad range of efficiency requirements\nwith a minimum performance drop. Our method consists of a) iterative token\nmerging based on embedding similarity before LLMs, and b) progressive token\npruning within LLM layers based on multi-modal importance. With a minimalist\ndesign, our method can be applied to both video and image LLMs. Extensive\nexperiments on diverse video and image benchmarks demonstrate that, our method\nsubstantially reduces computation load (e.g., a $\\textbf{7-fold}$ reduction in\nFLOPs) while preserving the performance of video and image LLMs. Further, under\na similar computational cost, our method outperforms the state-of-the-art\nmethods in long video understanding (e.g., $\\textbf{+4.6}$ on MLVU).\nAdditionally, our in-depth analysis provides insights into token redundancy and\nLLM layer behaviors, offering guidance for future research in designing\nefficient multi-modal LLMs. Our code will be available at\nhttps://github.com/LaVi-Lab/AIM.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u80fd\u5efa\u7acb\u591a\u6a21\u614b LLM\uff0c\u80fd\u5c0d\u8996\u89ba\u8cc7\u6599\uff08\u4f8b\u5982\u5f71\u50cf\u548c\u5f71\u7247\uff09\u5c55\u73fe\u5f37\u5927\u7684\u7406\u89e3\u529b\u3002\n\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u901a\u5e38\u4ef0\u8cf4\u8996\u89ba\u7de8\u78bc\u5668\u7684\u5ee3\u6cdb\u8996\u89ba\u7b26\u865f\uff0c\u5c0e\u81f4\u9ad8\u904b\u7b97\u9700\u6c42\uff0c\u9650\u5236\u5176\u5728\u8cc7\u6e90\u53d7\u9650\u74b0\u5883\u548c\u9577\u8108\u7d61\u4efb\u52d9\u4e2d\u7684\u61c9\u7528\u3002\n\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u70ba\u591a\u6a21\u614b LLM \u63d0\u51fa\u4e00\u500b\u514d\u8a13\u7df4\u7684\u81ea\u9069\u61c9\u63a8\u8ad6\u65b9\u6cd5\uff0c\u80fd\u4ee5\u6700\u5c0f\u7684\u6548\u80fd\u4e0b\u964d\u4f86\u9069\u61c9\u5ee3\u6cdb\u7684\u6548\u7387\u9700\u6c42\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u62ec a) \u5728 LLM \u4e4b\u524d\u6839\u64da\u5d4c\u5165\u76f8\u4f3c\u6027\u9032\u884c\u53cd\u8986\u7b26\u865f\u5408\u4f75\uff0c\u4ee5\u53ca b) \u6839\u64da\u591a\u6a21\u614b\u91cd\u8981\u6027\u5728 LLM \u5c64\u4e2d\u9032\u884c\u6f38\u9032\u7b26\u865f\u4fee\u526a\u3002\u6211\u5011\u7684\u505a\u6cd5\u63a1\u7528\u6975\u7c21\u8a2d\u8a08\uff0c\u53ef\u61c9\u7528\u65bc\u5f71\u7247\u548c\u5f71\u50cf LLM\u3002\u5728\u5404\u7a2e\u5f71\u7247\u548c\u5f71\u50cf\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5927\u5e45\u964d\u4f4e\u904b\u7b97\u8ca0\u8f09\uff08\u4f8b\u5982\uff0cFLOP \u6e1b\u5c11\u4e86 $\\textbf{7 \u500d}$\uff09\uff0c\u540c\u6642\u4fdd\u7559\u5f71\u7247\u548c\u5f71\u50cf LLM \u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u5728\u985e\u4f3c\u7684\u904b\u7b97\u6210\u672c\u4e0b\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u9577\u5f71\u7247\u7406\u89e3\u65b9\u9762\u512a\u65bc\u73fe\u6709\u6280\u8853\uff08\u4f8b\u5982\uff0c\u5728 MLVU \u4e0a $\\textbf{+4.6}$\uff09\u3002\n\u6b64\u5916\uff0c\u6211\u5011\u7684\u6df1\u5165\u5206\u6790\u63d0\u4f9b\u4e86\u7b26\u865f\u5197\u9918\u548c LLM \u5c64\u884c\u70ba\u7684\u898b\u89e3\uff0c\u70ba\u672a\u4f86\u8a2d\u8a08\u9ad8\u6548\u591a\u6a21\u614b LLM \u7684\u7814\u7a76\u63d0\u4f9b\u6307\u5c0e\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5c07\u65bc https://github.com/LaVi-Lab/AIM \u4e0a\u63d0\u4f9b\u3002</paragraph>", "author": "Yiwu Zhong et.al.", "authors": "Yiwu Zhong, Zhuoming Liu, Yin Li, Liwei Wang", "id": "2412.03248v1", "paper_url": "http://arxiv.org/abs/2412.03248v1", "repo": "https://github.com/lavi-lab/aim"}}