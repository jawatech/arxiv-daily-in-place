{"2407.10920": {"publish_time": "2024-07-15", "title": "Benchmarking Vision Language Models for Cultural Understanding", "paper_summary": "Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.", "paper_summary_zh": "\u57fa\u790e\u6a21\u578b\u548c\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4\u986f\u8457\u9032\u6b65\uff0c\n\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u80fd\u5920\u5c0d\u8996\u89ba\u548c\u8a9e\u8a00\u8cc7\u6599\u9032\u884c\u591a\u6a21\u614b\u8655\u7406\u3002\u4f46\u662f\uff0c\u5b83\u5011\u7684\u6548\u80fd\u901a\u5e38\u5728\u4e00\u822c\u5834\u666f\u7406\u89e3\uff08\u8fa8\u8b58\u7269\u4ef6\u3001\u5c6c\u6027\u548c\u52d5\u4f5c\uff09\u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u800c\u4e0d\u662f\u6587\u5316\u7406\u89e3\u3002\u672c\u7814\u7a76\u63a8\u51fa CulturalVQA\uff0c\u9019\u662f\u4e00\u500b\u8996\u89ba\u554f\u7b54\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30 VLM \u7684\u5730\u7406\u6587\u5316\u7406\u89e3\u3002\u6211\u5011\u7b56\u5283\u4e86\u4e00\u500b\u5305\u542b 2,378 \u500b\u5f71\u50cf\u554f\u984c\u914d\u5c0d\u7684\u8cc7\u6599\u96c6\uff0c\u6bcf\u500b\u554f\u984c\u6709 1-5 \u500b\u7b54\u6848\uff0c\u4ee3\u8868 5 \u5927\u6d32 11 \u500b\u570b\u5bb6\u7684\u6587\u5316\u3002\u9019\u4e9b\u554f\u984c\u63a2\u8a0e\u5c0d\u6587\u5316\u4e0d\u540c\u9762\u5411\u7684\u7406\u89e3\uff0c\u4f8b\u5982\u670d\u88dd\u3001\u98df\u7269\u3001\u98f2\u6599\u3001\u5100\u5f0f\u548c\u50b3\u7d71\u3002\u5728 CulturalVQA \u4e0a\u5c0d VLM\uff08\u5305\u62ec GPT-4V \u548c Gemini\uff09\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u63ed\u793a\u4e86\u5b83\u5011\u5728\u4e0d\u540c\u5730\u5340\u6587\u5316\u7406\u89e3\u7a0b\u5ea6\u4e0a\u7684\u5dee\u7570\uff0c\u5317\u7f8e\u6709\u5f88\u5f37\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\uff0c\u800c\u975e\u6d32\u7684\u6548\u80fd\u5247\u986f\u8457\u8f03\u4f4e\u3002\u6211\u5011\u4e5f\u89c0\u5bdf\u5230\u5b83\u5011\u5728\u4e0d\u540c\u6587\u5316\u9762\u5411\u7684\u6548\u80fd\u5dee\u7570\uff0c\u670d\u88dd\u3001\u5100\u5f0f\u548c\u50b3\u7d71\u7684\u6548\u80fd\u9ad8\u65bc\u98df\u7269\u548c\u98f2\u6599\u3002\u9019\u4e9b\u5dee\u7570\u6709\u52a9\u65bc\u6211\u5011\u627e\u51fa VLM \u7f3a\u4e4f\u6587\u5316\u7406\u89e3\u7684\u5730\u65b9\uff0c\u4e26\u8b49\u660e CulturalVQA \u4f5c\u70ba\u8a55\u4f30 VLM \u5728\u7406\u89e3\u4e0d\u540c\u6587\u5316\u65b9\u9762\u9032\u5c55\u7684\u5168\u9762\u8a55\u4f30\u96c6\u7684\u6f5b\u529b\u3002", "author": "Shravan Nayak et.al.", "authors": "Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Sta\u0144czak, Aishwarya Agrawal", "id": "2407.10920v1", "paper_url": "http://arxiv.org/abs/2407.10920v1", "repo": "null"}}