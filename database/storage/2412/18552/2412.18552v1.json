{"2412.18552": {"publish_time": "2024-12-24", "title": "Distilling Fine-grained Sentiment Understanding from Large Language Models", "paper_summary": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at\n\\url{https://github.com/HITSZ-HLT/FSA-Distillation}.", "paper_summary_zh": "\u7d30\u7c92\u5ea6\u60c5\u7dd2\u5206\u6790 (FSA) \u65e8\u5728\u5f9e\u5927\u91cf\u7684\u610f\u898b\u6587\u672c\u4e2d\u63d0\u53d6\u548c\u7e3d\u7d50\u4f7f\u7528\u8005\u7684\u610f\u898b\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5177\u6709\u5353\u8d8a\u7684\u60c5\u7dd2\u7406\u89e3\u80fd\u529b\u3002\u7136\u800c\uff0c\u76f4\u63a5\u90e8\u7f72 LLM \u9032\u884c FSA \u61c9\u7528\u6703\u7522\u751f\u9ad8\u6602\u7684\u63a8\u8ad6\u6210\u672c\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63a2\u8a0e\u5c07 LLM \u4e2d\u7684\u7d30\u7c92\u5ea6\u60c5\u7dd2\u7406\u89e3\u84b8\u993e\u5230\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM) \u4e2d\u3002\u6211\u5011\u63d0\u793a LLM \u6aa2\u67e5\u548c\u8a6e\u91cb\u7d66\u5b9a\u8a55\u8ad6\u7684\u60c5\u7dd2\uff0c\u7136\u5f8c\u5229\u7528\u7522\u751f\u7684\u5167\u5bb9\u9810\u8a13\u7df4 SLM\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u5168\u9762\u7684 FSA \u57fa\u6e96\u4f86\u8a55\u4f30 SLM \u548c LLM\u3002\u5728\u9019\u500b\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u63ed\u793a\uff1a(1) \u84b8\u993e\u986f\u8457\u63d0\u5347 SLM \u5728 FSA \u4efb\u52d9\u4e2d\u7684\u8868\u73fe\uff0c\u5728 $F_1$-score \u4e2d\u63d0\u5347 6.00%\uff0c\u800c\u4e14\u84b8\u993e\u6a21\u578b\u50c5\u4f7f\u7528 220M \u500b\u53c3\u6578\u5c31\u80fd\u8d85\u8d8a Llama-2-7b\uff1b(2) \u84b8\u993e\u8ce6\u4e88 SLM \u512a\u7570\u7684\u96f6\u6b21\u5b78\u7fd2\u60c5\u7dd2\u5206\u985e\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u5920\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u5176\u6559\u5e2b\u6a21\u578b\u3002\u9019\u4e9b\u7d50\u679c\u8868\u660e\uff0c\u5f9e LLM \u9032\u884c\u84b8\u993e\u662f FSA \u4e00\u500b\u6975\u5177\u524d\u666f\u7684\u65b9\u5411\u3002\u6211\u5011\u5c07\u5728\n\\url{https://github.com/HITSZ-HLT/FSA-Distillation} \u767c\u5e03\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u3001\u8cc7\u6599\u548c\u9810\u8a13\u7df4\u6a21\u578b\u6b0a\u91cd\u3002", "author": "Yice Zhang et.al.", "authors": "Yice Zhang, Guangyu Xie, Hongling Xu, Kaiheng Hou, Jianzhu Bao, Qianlong Wang, Shiwei Chen, Ruifeng Xu", "id": "2412.18552v1", "paper_url": "http://arxiv.org/abs/2412.18552v1", "repo": "https://github.com/hitsz-hlt/fsa-distillation"}}