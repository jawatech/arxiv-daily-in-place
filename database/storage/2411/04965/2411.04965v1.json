{"2411.04965": {"publish_time": "2024-11-07", "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs", "paper_summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.", "paper_summary_zh": "\u6700\u8fd1\u5c0d 1 \u4f4d\u5143\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u7814\u7a76\uff0c\u4f8b\u5982 BitNet b1.58\uff0c\u63d0\u51fa\u4e86\u5728\u7dad\u6301\u5176\u6548\u80fd\u7684\u540c\u6642\u6e1b\u5c11 LLM \u63a8\u8ad6\u6210\u672c\u7684\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 BitNet a4.8\uff0c\u70ba 1 \u4f4d\u5143 LLM \u555f\u7528 4 \u4f4d\u5143\u5143\u6fc0\u6d3b\u3002BitNet a4.8 \u63a1\u7528\u6df7\u5408\u91cf\u5316\u8207\u7a00\u758f\u5316\u7b56\u7565\u4f86\u6e1b\u8f15\u7570\u5e38\u901a\u9053\u5f15\u5165\u7684\u91cf\u5316\u8aa4\u5dee\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4f7f\u7528 4 \u4f4d\u5143\u5143\u6fc0\u6d3b\u4f5c\u70ba\u6ce8\u610f\u529b\u548c\u524d\u994b\u7db2\u8def\u5c64\u7684\u8f38\u5165\uff0c\u540c\u6642\u7a00\u758f\u5316\u4e2d\u9593\u72c0\u614b\u4e26\u9032\u884c 8 \u4f4d\u5143\u5143\u91cf\u5316\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cBitNet a4.8 \u9054\u5230\u8207 BitNet b1.58 \u76f8\u7576\u7684\u6548\u80fd\uff0c\u4e14\u8a13\u7df4\u6210\u672c\u76f8\u7576\uff0c\u540c\u6642\u5728\u555f\u7528 4 \u4f4d\u5143\u5143 (INT4/FP4) \u6838\u5fc3\u6642\u63a8\u8ad6\u901f\u5ea6\u66f4\u5feb\u3002\u6b64\u5916\uff0cBitNet a4.8 \u50c5\u555f\u7528 55% \u7684\u53c3\u6578\u4e26\u652f\u63f4 3 \u4f4d\u5143\u5143 KV \u5feb\u53d6\uff0c\u9032\u4e00\u6b65\u63d0\u5347\u5927\u898f\u6a21 LLM \u90e8\u7f72\u548c\u63a8\u8ad6\u7684\u6548\u7387\u3002", "author": "Hongyu Wang et.al.", "authors": "Hongyu Wang, Shuming Ma, Furu Wei", "id": "2411.04965v1", "paper_url": "http://arxiv.org/abs/2411.04965v1", "repo": "null"}}