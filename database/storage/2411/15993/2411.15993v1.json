{"2411.15993": {"publish_time": "2024-11-24", "title": "Investigating Factuality in Long-Form Text Generation: The Roles of Self-Known and Self-Unknown", "paper_summary": "Large language models (LLMs) have demonstrated strong capabilities in text\nunderstanding and generation. However, they often lack factuality, producing a\nmixture of true and false information, especially in long-form generation. In\nthis work, we investigates the factuality of long-form text generation across\nvarious large language models (LLMs), including GPT-4, Gemini-1.5-Pro,\nClaude-3-Opus, Llama-3-70B, and Mistral. Our analysis reveals that factuality\nscores tend to decline in later sentences of the generated text, accompanied by\na rise in the number of unsupported claims. Furthermore, we explore the\neffectiveness of different evaluation settings to assess whether LLMs can\naccurately judge the correctness of their own outputs: Self-Known (the\npercentage of supported atomic claims, decomposed from LLM outputs, that the\ncorresponding LLMs judge as correct) and Self-Unknown (the percentage of\nunsupported atomic claims that the corresponding LLMs judge as incorrect). The\nresults indicate that even advanced models like GPT-4 and Gemini-1.5-Pro fail\nto achieve perfect Self-Known scores, while their Self-Unknown scores remain\nnotably above zero, reflecting ongoing uncertainty in their self-assessments.\nMoreover, we find a correlation between higher Self-Known scores and improved\nfactuality, while higher Self-Unknown scores are associated with lower\nfactuality. Interestingly, even without significant changes in the models'\nself-judgment (Self-Known and Self-Unknown), the number of unsupported claims\ncan increases, likely as an artifact of long-form generation. These findings\nshow the limitations of current LLMs in long-form generation, and provide\nvaluable insights for improving factuality in long-form text generation.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u6587\u5b57\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u5011\u5f80\u5f80\u7f3a\u4e4f\u4e8b\u5be6\u6027\uff0c\u7522\u751f\u771f\u5047\u8a0a\u606f\u6df7\u96dc\u7684\u5167\u5bb9\uff0c\u7279\u5225\u662f\u5728\u9577\u7bc7\u751f\u6210\u4e2d\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5404\u7a2e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9577\u7bc7\u6587\u5b57\u751f\u6210\u7684\u771f\u5be6\u6027\uff0c\u5305\u62ec GPT-4\u3001Gemini-1.5-Pro\u3001Claude-3-Opus\u3001Llama-3-70B \u548c Mistral\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u751f\u6210\u6587\u5b57\u7684\u5f8c\u7e8c\u53e5\u5b50\u4e2d\u771f\u5be6\u6027\u5206\u6578\u5f80\u5f80\u4e0b\u964d\uff0c\u540c\u6642\u7f3a\u4e4f\u4f9d\u64da\u7684\u4e3b\u5f35\u6578\u91cf\u589e\u52a0\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4e0d\u540c\u8a55\u4f30\u8a2d\u5b9a\u7684\u6709\u6548\u6027\uff0c\u4ee5\u8a55\u4f30 LLM \u662f\u5426\u80fd\u6e96\u78ba\u5224\u65b7\u5176\u81ea\u8eab\u8f38\u51fa\u7684\u6b63\u78ba\u6027\uff1a\u81ea\u77e5\uff08LLM \u8f38\u51fa\u4e2d\u5206\u89e3\u51fa\u7684\u5df2\u652f\u6301\u539f\u5b50\u4e3b\u5f35\u7684\u767e\u5206\u6bd4\uff0c\u5c0d\u61c9\u7684 LLM \u5224\u65b7\u70ba\u6b63\u78ba\uff09\u548c\u81ea\u4e0d\u77e5\uff08LLM \u8f38\u51fa\u4e2d\u5206\u89e3\u51fa\u7684\u672a\u652f\u6301\u539f\u5b50\u4e3b\u5f35\u7684\u767e\u5206\u6bd4\uff0c\u5c0d\u61c9\u7684 LLM \u5224\u65b7\u70ba\u4e0d\u6b63\u78ba\uff09\u3002\u7d50\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f GPT-4 \u548c Gemini-1.5-Pro \u7b49\u9032\u968e\u6a21\u578b\u4e5f\u7121\u6cd5\u9054\u5230\u5b8c\u7f8e\u7684\u81ea\u77e5\u5206\u6578\uff0c\u800c\u5176\u81ea\u4e0d\u77e5\u5206\u6578\u4ecd\u986f\u8457\u9ad8\u65bc\u96f6\uff0c\u53cd\u6620\u51fa\u5176\u81ea\u6211\u8a55\u4f30\u4e2d\u6301\u7e8c\u5b58\u5728\u7684\u4e0d\u78ba\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u81ea\u77e5\u5206\u6578\u8f03\u9ad8\u8207\u771f\u5be6\u6027\u63d0\u5347\u4e4b\u9593\u5b58\u5728\u95dc\u806f\uff0c\u800c\u81ea\u4e0d\u77e5\u5206\u6578\u8f03\u9ad8\u5247\u8207\u771f\u5be6\u6027\u964d\u4f4e\u76f8\u95dc\u3002\u6709\u8da3\u7684\u662f\uff0c\u5373\u4f7f\u6a21\u578b\u7684\u81ea\u5224\u65b7\uff08\u81ea\u77e5\u548c\u81ea\u4e0d\u77e5\uff09\u6c92\u6709\u986f\u8457\u8b8a\u5316\uff0c\u672a\u652f\u6301\u4e3b\u5f35\u7684\u6578\u91cf\u4ecd\u53ef\u80fd\u589e\u52a0\uff0c\u9019\u53ef\u80fd\u662f\u9577\u7bc7\u751f\u6210\u7684\u7522\u7269\u3002\u9019\u4e9b\u767c\u73fe\u986f\u793a\u4e86\u7576\u524d LLM \u5728\u9577\u7bc7\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e26\u70ba\u6539\u5584\u9577\u7bc7\u6587\u5b57\u751f\u6210\u7684\u771f\u5be6\u6027\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\u3002", "author": "Lifu Tu et.al.", "authors": "Lifu Tu, Rui Meng, Shafiq Joty, Yingbo Zhou, Semih Yavuz", "id": "2411.15993v1", "paper_url": "http://arxiv.org/abs/2411.15993v1", "repo": "null"}}