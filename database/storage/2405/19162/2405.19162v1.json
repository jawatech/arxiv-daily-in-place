{"2405.19162": {"publish_time": "2024-05-29", "title": "Does learning the right latent variables necessarily improve in-context learning?", "paper_summary": "Large autoregressive models like Transformers can solve tasks through\nin-context learning (ICL) without learning new weights, suggesting avenues for\nefficiently solving new tasks. For many tasks, e.g., linear regression, the\ndata factorizes: examples are independent given a task latent that generates\nthe data, e.g., linear coefficients. While an optimal predictor leverages this\nfactorization by inferring task latents, it is unclear if Transformers\nimplicitly do so or if they instead exploit heuristics and statistical\nshortcuts enabled by attention layers. Both scenarios have inspired active\nongoing work. In this paper, we systematically investigate the effect of\nexplicitly inferring task latents. We minimally modify the Transformer\narchitecture with a bottleneck designed to prevent shortcuts in favor of more\nstructured solutions, and then compare performance against standard\nTransformers across various ICL tasks. Contrary to intuition and some recent\nworks, we find little discernible difference between the two; biasing towards\ntask-relevant latent variables does not lead to better out-of-distribution\nperformance, in general. Curiously, we find that while the bottleneck\neffectively learns to extract latent task variables from context, downstream\nprocessing struggles to utilize them for robust prediction. Our study\nhighlights the intrinsic limitations of Transformers in achieving structured\nICL solutions that generalize, and shows that while inferring the right latents\naids interpretability, it is not sufficient to alleviate this problem.", "paper_summary_zh": "\u5927\u578b\u81ea\u8ff4\u6b78\u6a21\u578b\uff0c\u4f8b\u5982 Transformer\uff0c\u53ef\u4ee5\u900f\u904e\u60c5\u5883\u5b78\u7fd2 (ICL) \u89e3\u6c7a\u4efb\u52d9\uff0c\u800c\u7121\u9700\u5b78\u7fd2\u65b0\u7684\u6b0a\u91cd\uff0c\u9019\u8868\u793a\u6709\u9014\u5f91\u53ef\u6709\u6548\u7387\u5730\u89e3\u6c7a\u65b0\u4efb\u52d9\u3002\u5c0d\u65bc\u8a31\u591a\u4efb\u52d9\uff0c\u4f8b\u5982\u7dda\u6027\u8ff4\u6b78\uff0c\u8cc7\u6599\u6703\u56e0\u5f0f\u5206\u89e3\uff1a\u7bc4\u4f8b\u6703\u5728\u7522\u751f\u8cc7\u6599\u7684\u4efb\u52d9\u6f5b\u5728\u8b8a\u6578\uff08\u4f8b\u5982\u7dda\u6027\u4fc2\u6578\uff09\u4e0b\u7368\u7acb\u3002\u96d6\u7136\u6700\u4f73\u9810\u6e2c\u5668\u6703\u900f\u904e\u63a8\u8ad6\u4efb\u52d9\u6f5b\u5728\u8b8a\u6578\u4f86\u5229\u7528\u6b64\u56e0\u5f0f\u5206\u89e3\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a Transformer \u662f\u5426\u6703\u96b1\u542b\u5730\u9019\u6a23\u505a\uff0c\u6216\u8005\u5b83\u5011\u662f\u5426\u6703\u5229\u7528\u6ce8\u610f\u529b\u5c64\u6240\u555f\u7528\u7684\u555f\u767c\u6cd5\u548c\u7d71\u8a08\u6377\u5f91\u3002\u9019\u5169\u7a2e\u60c5\u6cc1\u90fd\u6fc0\u767c\u4e86\u7a4d\u6975\u7684\u6301\u7e8c\u7814\u7a76\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u4e86\u660e\u78ba\u63a8\u8ad6\u4efb\u52d9\u6f5b\u5728\u8b8a\u6578\u7684\u6548\u679c\u3002\u6211\u5011\u4f7f\u7528\u74f6\u9838\u5c0d Transformer \u67b6\u69cb\u9032\u884c\u6700\u5c0f\u7684\u4fee\u6539\uff0c\u76ee\u7684\u662f\u9632\u6b62\u6377\u5f91\uff0c\u4e26\u652f\u6301\u66f4\u7d50\u69cb\u5316\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7136\u5f8c\u6bd4\u8f03\u6a19\u6e96 Transformer \u5728\u5404\u7a2e ICL \u4efb\u52d9\u4e2d\u7684\u6548\u80fd\u3002\u8207\u76f4\u89ba\u548c\u4e00\u4e9b\u8fd1\u671f\u7814\u7a76\u76f8\u53cd\uff0c\u6211\u5011\u767c\u73fe\u5169\u8005\u4e4b\u9593\u5e7e\u4e4e\u6c92\u6709\u660e\u986f\u5dee\u7570\uff1b\u4e00\u822c\u800c\u8a00\uff0c\u504f\u5411\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u6f5b\u5728\u8b8a\u6578\u4e26\u4e0d\u6703\u5c0e\u81f4\u66f4\u597d\u7684\u5206\u4f48\u5916\u6548\u80fd\u3002\u5947\u602a\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe\u96d6\u7136\u74f6\u9838\u6709\u6548\u5730\u5b78\u6703\u5f9e\u60c5\u5883\u4e2d\u63d0\u53d6\u6f5b\u5728\u4efb\u52d9\u8b8a\u6578\uff0c\u4f46\u4e0b\u6e38\u8655\u7406\u537b\u96e3\u4ee5\u5229\u7528\u5b83\u5011\u9032\u884c\u7a69\u5065\u7684\u9810\u6e2c\u3002\u6211\u5011\u7684\u7814\u7a76\u5f37\u8abf\u4e86 Transformer \u5728\u9054\u6210\u7d50\u69cb\u5316 ICL \u89e3\u6c7a\u65b9\u6848\uff08\u53ef\u9032\u884c\u6982\u62ec\uff09\u65b9\u9762\u7684\u5167\u5728\u9650\u5236\uff0c\u4e26\u986f\u793a\u96d6\u7136\u63a8\u8ad6\u6b63\u78ba\u7684\u6f5b\u5728\u8b8a\u6578\u6709\u52a9\u65bc\u53ef\u89e3\u91cb\u6027\uff0c\u4f46\u4e0d\u8db3\u4ee5\u7de9\u89e3\u6b64\u554f\u984c\u3002", "author": "Sarthak Mittal et.al.", "authors": "Sarthak Mittal, Eric Elmoznino, Leo Gagnon, Sangnie Bhardwaj, Dhanya Sridhar, Guillaume Lajoie", "id": "2405.19162v1", "paper_url": "http://arxiv.org/abs/2405.19162v1", "repo": "https://github.com/ericelmoznino/explicit_implicit_icl"}}