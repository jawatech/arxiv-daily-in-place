{"2410.13722": {"publish_time": "2024-10-17", "title": "Persistent Pre-Training Poisoning of LLMs", "paper_summary": "Large language models are pre-trained on uncurated text datasets consisting\nof trillions of tokens scraped from the Web. Prior work has shown that: (1)\nweb-scraped pre-training datasets can be practically poisoned by malicious\nactors; and (2) adversaries can compromise language models after poisoning\nfine-tuning datasets. Our work evaluates for the first time whether language\nmodels can also be compromised during pre-training, with a focus on the\npersistence of pre-training attacks after models are fine-tuned as helpful and\nharmless chatbots (i.e., after SFT and DPO). We pre-train a series of LLMs from\nscratch to measure the impact of a potential poisoning adversary under four\ndifferent attack objectives (denial-of-service, belief manipulation,\njailbreaking, and prompt stealing), and across a wide range of model sizes\n(from 600M to 7B). Our main result is that poisoning only 0.1% of a model's\npre-training dataset is sufficient for three out of four attacks to measurably\npersist through post-training. Moreover, simple attacks like denial-of-service\npersist through post-training with a poisoning rate of only 0.001%.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u672a\u7d93\u6574\u7406\u7684\u6587\u672c\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u9810\u8a13\u7df4\uff0c\u9019\u4e9b\u8cc7\u6599\u96c6\u5305\u542b\u5f9e\u7db2\u8def\u64f7\u53d6\u7684\u6578\u5146\u500b\u4ee3\u78bc\u3002\u5148\u524d\u7684\u7814\u7a76\u986f\u793a\uff1a(1) \u7db2\u8def\u64f7\u53d6\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u53ef\u80fd\u6703\u906d\u5230\u60e1\u610f\u884c\u70ba\u8005\u6295\u6bd2\uff1b(2) \u653b\u64ca\u8005\u53ef\u4ee5\u5728\u6295\u6bd2\u5fae\u8abf\u8cc7\u6599\u96c6\u5f8c\u5371\u5bb3\u8a9e\u8a00\u6a21\u578b\u3002\u6211\u5011\u7684\u7814\u7a76\u9996\u6b21\u8a55\u4f30\u8a9e\u8a00\u6a21\u578b\u662f\u5426\u4e5f\u53ef\u4ee5\u5728\u9810\u8a13\u7df4\u671f\u9593\u53d7\u5230\u5371\u5bb3\uff0c\u91cd\u9ede\u5728\u65bc\u5728\u6a21\u578b\u5fae\u8abf\u70ba\u6709\u7528\u7684\u7121\u5bb3\u804a\u5929\u6a5f\u5668\u4eba\uff08\u5373 SFT \u548c DPO \u4e4b\u5f8c\uff09\u9810\u8a13\u7df4\u653b\u64ca\u7684\u6301\u7e8c\u6027\u3002\u6211\u5011\u5f9e\u982d\u958b\u59cb\u9810\u8a13\u7df4\u4e00\u7cfb\u5217 LLM\uff0c\u4ee5\u8861\u91cf\u6f5b\u5728\u6295\u6bd2\u653b\u64ca\u8005\u5728\u56db\u500b\u4e0d\u540c\u7684\u653b\u64ca\u76ee\u6a19\uff08\u62d2\u7d55\u670d\u52d9\u3001\u4fe1\u5ff5\u64cd\u7e31\u3001\u8d8a\u7344\u548c\u63d0\u793a\u7aca\u53d6\uff09\u548c\u5404\u7a2e\u6a21\u578b\u5927\u5c0f\uff08\u5f9e 600M \u5230 7B\uff09\u4e0b\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u662f\uff0c\u6295\u6bd2\u4e00\u500b\u6a21\u578b\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u7684 0.1% \u5c31\u8db3\u4ee5\u8b93\u56db\u5206\u4e4b\u4e09\u7684\u653b\u64ca\u5728\u8a13\u7df4\u5f8c\u6301\u7e8c\u5b58\u5728\u3002\u6b64\u5916\uff0c\u50cf\u62d2\u7d55\u670d\u52d9\u4e4b\u985e\u7684\u7c21\u55ae\u653b\u64ca\u5728\u6295\u6bd2\u7387\u50c5\u70ba 0.001% \u7684\u60c5\u6cc1\u4e0b\u4e5f\u6703\u6301\u7e8c\u5b58\u5728\u3002", "author": "Yiming Zhang et.al.", "authors": "Yiming Zhang, Javier Rando, Ivan Evtimov, Jianfeng Chi, Eric Michael Smith, Nicholas Carlini, Florian Tram\u00e8r, Daphne Ippolito", "id": "2410.13722v1", "paper_url": "http://arxiv.org/abs/2410.13722v1", "repo": "null"}}