{"2409.08381": {"publish_time": "2024-09-12", "title": "Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations", "paper_summary": "Vision-language models (VLMs) like CLIP have been adapted for Multi-Label\nRecognition (MLR) with partial annotations by leveraging prompt-learning, where\npositive and negative prompts are learned for each class to associate their\nembeddings with class presence or absence in the shared vision-text feature\nspace. While this approach improves MLR performance by relying on VLM priors,\nwe hypothesize that learning negative prompts may be suboptimal, as the\ndatasets used to train VLMs lack image-caption pairs explicitly focusing on\nclass absence. To analyze the impact of positive and negative prompt learning\non MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is\nlearned with VLM guidance while the other is replaced by an embedding vector\nlearned directly in the shared feature space without relying on the text\nencoder. Through empirical analysis, we observe that negative prompts degrade\nMLR performance, and learning only positive prompts, combined with learned\nnegative embeddings (PositiveCoOp), outperforms dual prompt learning\napproaches. Moreover, we quantify the performance benefits that prompt-learning\noffers over a simple vision-features-only baseline, observing that the baseline\ndisplays strong performance comparable to dual prompt learning approach\n(DualCoOp), when the proportion of missing labels is low, while requiring half\nthe training compute and 16 times fewer parameters", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\uff0c\u5982 CLIP\uff0c\u5df2\u900f\u904e\u63d0\u793a\u5b78\u7fd2\u9032\u884c\u591a\u6a19\u7c64\u8fa8\u8b58 (MLR)\uff0c\u5176\u4e2d\u91dd\u5c0d\u6bcf\u500b\u985e\u5225\u5b78\u7fd2\u6b63\u5411\u548c\u8ca0\u5411\u63d0\u793a\uff0c\u4ee5\u5c07\u5176\u5d4c\u5165\u8207\u985e\u5225\u5728\u5171\u4eab\u8996\u89ba\u6587\u672c\u7279\u5fb5\u7a7a\u9593\u4e2d\u662f\u5426\u5b58\u5728\u95dc\u806f\u3002\u96d6\u7136\u9019\u7a2e\u65b9\u6cd5\u900f\u904e\u4f9d\u8cf4 VLM \u5148\u9a57\u4f86\u63d0\u5347 MLR \u6548\u80fd\uff0c\u4f46\u6211\u5011\u5047\u8a2d\u5b78\u7fd2\u8ca0\u5411\u63d0\u793a\u53ef\u80fd\u662f\u6b21\u4f73\u7684\uff0c\u56e0\u70ba\u7528\u65bc\u8a13\u7df4 VLM \u7684\u8cc7\u6599\u96c6\u7f3a\u4e4f\u660e\u78ba\u8457\u91cd\u65bc\u985e\u5225\u4e0d\u5b58\u5728\u7684\u5f71\u50cf\u6a19\u984c\u914d\u5c0d\u3002\u70ba\u4e86\u5206\u6790\u6b63\u5411\u548c\u8ca0\u5411\u63d0\u793a\u5b78\u7fd2\u5c0d MLR \u7684\u5f71\u97ff\uff0c\u6211\u5011\u5f15\u5165\u4e86 PositiveCoOp \u548c NegativeCoOp\uff0c\u5176\u4e2d\u53ea\u4f7f\u7528 VLM \u6307\u5c0e\u5b78\u7fd2\u4e00\u500b\u63d0\u793a\uff0c\u800c\u53e6\u4e00\u500b\u63d0\u793a\u5247\u7531\u5728\u5171\u4eab\u7279\u5fb5\u7a7a\u9593\u4e2d\u76f4\u63a5\u5b78\u7fd2\u7684\u5d4c\u5165\u5411\u91cf\u53d6\u4ee3\uff0c\u800c\u4e0d\u4f9d\u8cf4\u6587\u5b57\u7de8\u78bc\u5668\u3002\u900f\u904e\u5be6\u8b49\u5206\u6790\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u8ca0\u5411\u63d0\u793a\u6703\u964d\u4f4e MLR \u6548\u80fd\uff0c\u800c\u4e14\u50c5\u5b78\u7fd2\u6b63\u5411\u63d0\u793a\uff0c\u4e26\u7d50\u5408\u5b78\u7fd2\u7684\u8ca0\u5411\u5d4c\u5165 (PositiveCoOp)\uff0c\u5176\u6548\u80fd\u512a\u65bc\u96d9\u91cd\u63d0\u793a\u5b78\u7fd2\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u5011\u91cf\u5316\u4e86\u63d0\u793a\u5b78\u7fd2\u76f8\u8f03\u65bc\u50c5\u9650\u8996\u89ba\u7279\u5fb5\u7684\u7c21\u55ae\u57fa\u6e96\u6240\u63d0\u4f9b\u7684\u6548\u80fd\u512a\u52e2\uff0c\u89c0\u5bdf\u5230\u7576\u907a\u5931\u6a19\u7c64\u7684\u6bd4\u4f8b\u8f03\u4f4e\u6642\uff0c\u57fa\u6e96\u6703\u986f\u793a\u51fa\u8207\u96d9\u91cd\u63d0\u793a\u5b78\u7fd2\u65b9\u6cd5 (DualCoOp) \u76f8\u7576\u7684\u5f37\u5927\u6548\u80fd\uff0c\u540c\u6642\u9700\u8981\u4e00\u534a\u7684\u8a13\u7df4\u904b\u7b97\u548c\u5c11 16 \u500d\u7684\u53c3\u6578", "author": "Samyak Rawlekar et.al.", "authors": "Samyak Rawlekar, Shubhang Bhatnagar, Narendra Ahuja", "id": "2409.08381v1", "paper_url": "http://arxiv.org/abs/2409.08381v1", "repo": "null"}}