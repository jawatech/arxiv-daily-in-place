{"2406.19934": {"publish_time": "2024-06-28", "title": "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "paper_summary": "We explore multi-step reasoning in vision-language models (VLMs). The problem\nis challenging, as reasoning data consisting of multiple steps of visual and\nlanguage processing are barely available. To overcome the challenge, we first\nintroduce a least-to-most visual reasoning paradigm, which interleaves steps of\ndecomposing a question into sub-questions and invoking external tools for\nresolving sub-questions. Based on the paradigm, we further propose a novel data\nsynthesis approach that can automatically create questions and multi-step\nreasoning paths for an image in a bottom-up manner. Our approach divides the\ncomplex synthesis task into a few simple sub-tasks, and (almost entirely)\nrelies on open-sourced models to accomplish the sub-tasks. Therefore, the\nentire synthesis process is reproducible and cost-efficient, and the\nsynthesized data is quality guaranteed. With the approach, we construct $50$k\nvisual reasoning examples. Then, we develop a visual reasoner through\nsupervised fine-tuning, which is capable of generally enhancing the reasoning\nabilities of a wide range of existing VLMs in a plug-and-play fashion.\nExtensive experiments indicate that the visual reasoner can consistently and\nsignificantly improve four VLMs on four VQA benchmarks. Our code and dataset\nare available at https://github.com/steven-ccq/VisualReasoner.", "paper_summary_zh": "\u6211\u5011\u5728\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4e2d\u63a2\u7d22\u591a\u6b65\u9a5f\u63a8\u7406\u3002\u9019\u500b\u554f\u984c\u5177\u6709\u6311\u6230\u6027\uff0c\u56e0\u70ba\u5305\u542b\u591a\u500b\u8996\u89ba\u548c\u8a9e\u8a00\u8655\u7406\u6b65\u9a5f\u7684\u63a8\u7406\u6578\u64da\u5e7e\u4e4e\u4e0d\u53ef\u7528\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u9996\u5148\u5f15\u5165\u4e00\u500b\u7531\u5c11\u5230\u591a\u7684\u8996\u89ba\u63a8\u7406\u7bc4\u4f8b\uff0c\u5b83\u4ea4\u7e54\u4e86\u5c07\u554f\u984c\u5206\u89e3\u6210\u5b50\u554f\u984c\u7684\u6b65\u9a5f\uff0c\u4e26\u8abf\u7528\u5916\u90e8\u5de5\u5177\u4f86\u89e3\u6c7a\u5b50\u554f\u984c\u3002\u57fa\u65bc\u9019\u500b\u7bc4\u4f8b\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u6578\u64da\u5408\u6210\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u81ea\u52d5\u70ba\u5716\u50cf\u5275\u5efa\u554f\u984c\u548c\u591a\u6b65\u9a5f\u63a8\u7406\u8def\u5f91\uff0c\u63a1\u7528\u7531\u4e0b\u800c\u4e0a\u7684\u65b9\u5f0f\u3002\u6211\u5011\u7684\u505a\u6cd5\u5c07\u8907\u96dc\u7684\u5408\u6210\u4efb\u52d9\u5283\u5206\u70ba\u5e7e\u500b\u7c21\u55ae\u7684\u5b50\u4efb\u52d9\uff0c\u4e26\u4e14\uff08\u5e7e\u4e4e\u5b8c\u5168\uff09\u4f9d\u8cf4\u65bc\u958b\u6e90\u6a21\u578b\u4f86\u5b8c\u6210\u5b50\u4efb\u52d9\u3002\u56e0\u6b64\uff0c\u6574\u500b\u5408\u6210\u904e\u7a0b\u662f\u53ef\u8907\u88fd\u4e14\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\uff0c\u4e26\u4e14\u5408\u6210\u6578\u64da\u7684\u54c1\u8cea\u6709\u4fdd\u8b49\u3002\u4f7f\u7528\u9019\u7a2e\u65b9\u6cd5\uff0c\u6211\u5011\u69cb\u5efa\u4e86 $50$k \u8996\u89ba\u63a8\u7406\u7bc4\u4f8b\u3002\u7136\u5f8c\uff0c\u6211\u5011\u900f\u904e\u76e3\u7763\u5fae\u8abf\u958b\u767c\u4e86\u4e00\u500b\u8996\u89ba\u63a8\u7406\u5668\uff0c\u5b83\u80fd\u5920\u4ee5\u5373\u63d2\u5373\u7528\u7684\u65b9\u5f0f\u666e\u904d\u589e\u5f37\u5404\u7a2e\u73fe\u6709 VLM \u7684\u63a8\u7406\u80fd\u529b\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8868\u660e\uff0c\u8996\u89ba\u63a8\u7406\u5668\u53ef\u4ee5\u6301\u7e8c\u4e14\u986f\u8457\u5730\u6539\u5584\u56db\u500b VQA \u57fa\u6e96\u4e0a\u7684\u56db\u500b VLM\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6578\u64da\u96c6\u53ef\u5728 https://github.com/steven-ccq/VisualReasoner \u53d6\u5f97\u3002", "author": "Chuanqi Cheng et.al.", "authors": "Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan", "id": "2406.19934v1", "paper_url": "http://arxiv.org/abs/2406.19934v1", "repo": "null"}}