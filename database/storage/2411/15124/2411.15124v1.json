{"2411.15124": {"publish_time": "2024-11-22", "title": "T\u00dcLU 3: Pushing Frontiers in Open Language Model Post-Training", "paper_summary": "Language model post-training is applied to refine behaviors and unlock new\nskills across a wide range of recent language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying training\ndata and recipes for post-training are simultaneously the most important pieces\nof the puzzle and the portion with the least transparency. To bridge this gap,\nwe introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained\nmodels, alongside its data, code, and training recipes, serving as a\ncomprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds\non Llama 3.1 base models, achieves results surpassing the instruct versions of\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\nClaude 3.5-Haiku. The training algorithms for our models include supervised\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we\nintroduce a multi-task evaluation scheme for post-training recipes with\ndevelopment and unseen evaluations, standard benchmark implementations, and\nsubstantial decontamination of existing open datasets on said benchmarks. We\nconclude with analysis and discussion of training methods that did not reliably\nimprove performance.\n  In addition to the T\\\"ULU 3 model weights and demo, we release the complete\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed report for reproducing and further adapting the T\\\"ULU\n3 approach to more domains.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u5f8c\u8a13\u7df4\u7528\u65bc\u6539\u5584\u884c\u70ba\u4e26\u89e3\u9396\u5404\u7a2e\u6700\u65b0\u8a9e\u8a00\u6a21\u578b\u7684\u65b0\u6280\u80fd\uff0c\u4f46\u61c9\u7528\u9019\u4e9b\u6280\u8853\u7684\u516c\u958b\u914d\u65b9\u843d\u5f8c\u65bc\u5c08\u6709\u914d\u65b9\u3002\u57fa\u790e\u8a13\u7df4\u6578\u64da\u548c\u5f8c\u8a13\u7df4\u914d\u65b9\u540c\u6642\u662f\u62fc\u5716\u4e2d\u6700\u91cd\u8981\u7684\u4e00\u90e8\u5206\uff0c\u4e5f\u662f\u900f\u660e\u5ea6\u6700\u4f4e\u7684\u90e8\u5206\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 T\\\"ULU 3\uff0c\u9019\u662f\u4e00\u500b\u5b8c\u5168\u958b\u653e\u7684\u6700\u65b0\u5f8c\u8a13\u7df4\u6a21\u578b\u7cfb\u5217\uff0c\u9023\u540c\u5176\u6578\u64da\u3001\u4ee3\u78bc\u548c\u8a13\u7df4\u914d\u65b9\uff0c\u4f5c\u70ba\u73fe\u4ee3\u5f8c\u8a13\u7df4\u6280\u8853\u7684\u7d9c\u5408\u6307\u5357\u3002T\\\"ULU 3 \u5efa\u69cb\u5728 Llama 3.1 \u57fa\u790e\u6a21\u578b\u4e0a\uff0c\u5176\u6210\u679c\u8d85\u8d8a\u4e86 Llama 3.1\u3001Qwen 2.5\u3001Mistral\uff0c\u751a\u81f3 GPT-4o-mini \u548c Claude 3.5-Haiku \u7b49\u5c01\u9589\u6a21\u578b\u7684\u6307\u4ee4\u7248\u672c\u3002\u6211\u5011\u6a21\u578b\u7684\u8a13\u7df4\u6f14\u7b97\u6cd5\u5305\u62ec\u76e3\u7763\u5fae\u8abf (SFT)\u3001\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\uff0c\u4ee5\u53ca\u6211\u5011\u7a31\u4e4b\u70ba\u53ef\u9a57\u8b49\u734e\u52f5\u5f37\u5316\u5b78\u7fd2 (RLVR) \u7684\u65b0\u65b9\u6cd5\u3002\u6709\u4e86 T\\\"ULU 3\uff0c\u6211\u5011\u5f15\u5165\u4e86\u591a\u4efb\u52d9\u8a55\u4f30\u65b9\u6848\uff0c\u7528\u65bc\u5f8c\u8a13\u7df4\u914d\u65b9\uff0c\u5176\u4e2d\u5305\u542b\u958b\u767c\u548c\u672a\u898b\u8a55\u4f30\u3001\u6a19\u6e96\u57fa\u6e96\u5be6\u4f5c\uff0c\u4ee5\u53ca\u5c0d\u6240\u8ff0\u57fa\u6e96\u4e0a\u73fe\u6709\u958b\u653e\u5f0f\u6578\u64da\u96c6\u7684\u5927\u91cf\u53bb\u6c59\u3002\u6211\u5011\u4ee5\u5c0d\u8a13\u7df4\u65b9\u6cd5\u7684\u5206\u6790\u548c\u8a0e\u8ad6\u4f5c\u70ba\u7d50\u8ad6\uff0c\u9019\u4e9b\u65b9\u6cd5\u4e26\u672a\u53ef\u9760\u5730\u6539\u5584\u6548\u80fd\u3002\u9664\u4e86 T\\\"ULU 3 \u6a21\u578b\u6b0a\u91cd\u548c\u793a\u7bc4\u4e4b\u5916\uff0c\u6211\u5011\u9084\u767c\u5e03\u4e86\u5b8c\u6574\u7684\u914d\u65b9\uff0c\u5305\u62ec\u7528\u65bc\u5404\u7a2e\u6838\u5fc3\u6280\u80fd\u7684\u6578\u64da\u96c6\u3001\u7528\u65bc\u6578\u64da\u6574\u7406\u548c\u8a55\u4f30\u7684\u5f37\u5927\u5de5\u5177\u5305\u3001\u8a13\u7df4\u4ee3\u78bc\u548c\u57fa\u790e\u67b6\u69cb\uff0c\u6700\u91cd\u8981\u7684\u662f\uff0c\u4e00\u4efd\u7528\u65bc\u91cd\u73fe\u548c\u9032\u4e00\u6b65\u8abf\u6574 T\\\"ULU 3 \u65b9\u6cd5\u4ee5\u9069\u61c9\u66f4\u591a\u9818\u57df\u7684\u8a73\u7d30\u5831\u544a\u3002", "author": "Nathan Lambert et.al.", "authors": "Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, Hannaneh Hajishirzi", "id": "2411.15124v1", "paper_url": "http://arxiv.org/abs/2411.15124v1", "repo": "null"}}