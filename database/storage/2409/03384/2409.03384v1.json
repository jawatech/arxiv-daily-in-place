{"2409.03384": {"publish_time": "2024-09-05", "title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison", "paper_summary": "Large Language Models (LLMs) have emerged as powerful tools for natural\nlanguage processing tasks, revolutionizing the field with their ability to\nunderstand and generate human-like text. In this paper, we present a\ncomprehensive survey of the several research efforts that have been presented\nfor the acceleration of transformer networks for Large Language Models using\nhardware accelerators.\n  The survey presents the frameworks that have been proposed and then performs\na qualitative and quantitative comparison regarding the technology, the\nprocessing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy\nefficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each\nframework. The main challenge in comparison is that every proposed scheme is\nimplemented on a different process technology making hard a fair comparison.\nThe main contribution of this paper is that we extrapolate the results of the\nperformance and the energy efficiency on the same technology to make a fair\ncomparison; one theoretical and one more practical. We implement part of the\nLLMs on several FPGA chips to extrapolate the results to the same process\ntechnology and then we make a fair comparison of the performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u7684\u5f37\u5927\u5de5\u5177\uff0c\u5b83\u5011\u80fd\u5920\u7406\u89e3\u548c\u751f\u6210\u985e\u4f3c\u4eba\u985e\u7684\u6587\u5b57\uff0c\u56e0\u800c\u5fb9\u5e95\u6539\u8b8a\u4e86\u9019\u500b\u9818\u57df\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c0d\u4f7f\u7528\u786c\u9ad4\u52a0\u901f\u5668\u4f86\u52a0\u901f\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684Transformer\u7db2\u8def\u6240\u63d0\u51fa\u7684\u591a\u9805\u7814\u7a76\u5de5\u4f5c\u9032\u884c\u4e86\u5168\u9762\u7684\u8abf\u67e5\u3002\n\u8abf\u67e5\u4ecb\u7d39\u4e86\u5df2\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u7136\u5f8c\u5c0d\u6280\u8853\u3001\u8655\u7406\u5e73\u53f0 (FPGA\u3001ASIC\u3001In-Memory\u3001GPU)\u3001\u52a0\u901f\u3001\u80fd\u6548\u3001\u6548\u80fd (GOP) \u548c\u80fd\u6548 (GOP/W) \u9032\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f03\u3002\u6bd4\u8f03\u4e2d\u7684\u4e3b\u8981\u6311\u6230\u5728\u65bc\uff0c\u6bcf\u9805\u63d0\u51fa\u7684\u65b9\u6848\u90fd\u662f\u5728\u4e0d\u540c\u7684\u88fd\u7a0b\u6280\u8853\u4e0a\u5be6\u4f5c\uff0c\u9019\u4f7f\u5f97\u516c\u5e73\u6bd4\u8f03\u8b8a\u5f97\u56f0\u96e3\u3002\u672c\u6587\u7684\u4e3b\u8981\u8ca2\u737b\u5728\u65bc\uff0c\u6211\u5011\u5c07\u6548\u80fd\u548c\u80fd\u6548\u7684\u7d50\u679c\u5916\u63a8\u5230\u76f8\u540c\u7684\u6280\u8853\u4e0a\uff0c\u4ee5\u9032\u884c\u516c\u5e73\u7684\u6bd4\u8f03\uff1b\u4e00\u7a2e\u662f\u7406\u8ad6\u4e0a\u7684\uff0c\u4e00\u7a2e\u662f\u66f4\u5be6\u969b\u7684\u3002\u6211\u5011\u5728\u591a\u500b FPGA \u6676\u7247\u4e0a\u5be6\u4f5c\u90e8\u5206 LLM\uff0c\u4ee5\u5c07\u7d50\u679c\u5916\u63a8\u5230\u76f8\u540c\u7684\u88fd\u7a0b\u6280\u8853\uff0c\u7136\u5f8c\u516c\u5e73\u5730\u6bd4\u8f03\u6548\u80fd\u3002", "author": "Nikoletta Koilia et.al.", "authors": "Nikoletta Koilia, Christoforos Kachris", "id": "2409.03384v1", "paper_url": "http://arxiv.org/abs/2409.03384v1", "repo": "null"}}