{"2412.05210": {"publish_time": "2024-12-06", "title": "Evaluating and Aligning CodeLLMs on Human Preference", "paper_summary": "Code large language models (codeLLMs) have made significant strides in code\ngeneration. Most previous code-related benchmarks, which consist of various\nprogramming exercises along with the corresponding test cases, are used as a\ncommon measure to evaluate the performance and capabilities of code LLMs.\nHowever, the current code LLMs focus on synthesizing the correct code snippet,\nignoring the alignment with human preferences, where the query should be\nsampled from the practical application scenarios and the model-generated\nresponses should satisfy the human preference. To bridge the gap between the\nmodel-generated response and human preference, we present a rigorous\nhuman-curated benchmark CodeArena to emulate the complexity and diversity of\nreal-world coding tasks, where 397 high-quality samples spanning 40 categories\nand 44 programming languages, carefully curated from user queries. Further, we\npropose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B\ntokens) by scaling instructions from the website to verify the effectiveness of\nthe large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder\ntotally trained on synthetic instruction data can achieve top-tier performance\nof open-source code LLMs. The results find performance differences between\nexecution-based benchmarks and CodeArena. Our systematic experiments of\nCodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code\nLLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring\nthe importance of the human preference\nalignment.\\footnote{\\url{https://codearenaeval.github.io/ }}", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08codeLLM\uff09\u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u986f\u8457\u9032\u5c55\u3002\u5927\u591a\u6578\u5148\u524d\u7684\u7a0b\u5f0f\u78bc\u76f8\u95dc\u57fa\u6e96\uff0c\u5305\u542b\u5404\u7a2e\u7a0b\u5f0f\u8a2d\u8a08\u7df4\u7fd2\u4ee5\u53ca\u5c0d\u61c9\u7684\u6e2c\u8a66\u6848\u4f8b\uff0c\u7528\u4f5c\u8a55\u4f30 code LLM \u6548\u80fd\u548c\u529f\u80fd\u7684\u5e38\u898b\u6e2c\u91cf\u6a19\u6e96\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684 code LLM \u5c08\u6ce8\u65bc\u5408\u6210\u6b63\u78ba\u7684\u7a0b\u5f0f\u78bc\u7247\u6bb5\uff0c\u5ffd\u7565\u8207\u4eba\u985e\u504f\u597d\u7684\u5c0d\u9f4a\uff0c\u5176\u4e2d\u67e5\u8a62\u61c9\u5f9e\u5be6\u969b\u61c9\u7528\u60c5\u5883\u4e2d\u62bd\u6a23\uff0c\u800c\u6a21\u578b\u7522\u751f\u7684\u56de\u61c9\u61c9\u6eff\u8db3\u4eba\u985e\u504f\u597d\u3002\u70ba\u4e86\u5f4c\u5408\u6a21\u578b\u7522\u751f\u7684\u56de\u61c9\u8207\u4eba\u985e\u504f\u597d\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u56b4\u8b39\u7684\u4eba\u5de5\u7b56\u5c55\u57fa\u6e96 CodeArena\uff0c\u4ee5\u6a21\u64ec\u771f\u5be6\u4e16\u754c\u7a0b\u5f0f\u8a2d\u8a08\u4efb\u52d9\u7684\u8907\u96dc\u6027\u548c\u591a\u6a23\u6027\uff0c\u5176\u4e2d 397 \u500b\u9ad8\u54c1\u8cea\u7bc4\u4f8b\u6db5\u84cb 40 \u500b\u985e\u5225\u548c 44 \u7a2e\u7a0b\u5f0f\u8a9e\u8a00\uff0c\u5f9e\u4f7f\u7528\u8005\u67e5\u8a62\u4e2d\u4ed4\u7d30\u7b56\u5c55\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u591a\u6a23\u5316\u7684\u5408\u6210\u6307\u4ee4\u8a9e\u6599\u5eab SynCode-Instruct\uff08\u5c07\u8fd1 20B \u500b\u4ee3\u5e63\uff09\uff0c\u900f\u904e\u64f4\u5145\u7db2\u7ad9\u4e0a\u7684\u6307\u4ee4\u4f86\u9a57\u8b49\u5927\u898f\u6a21\u5408\u6210\u6307\u4ee4\u5fae\u8abf\u7684\u6709\u6548\u6027\uff0c\u5176\u4e2d Qwen2.5-SynCoder \u5b8c\u5168\u8a13\u7df4\u65bc\u5408\u6210\u6307\u4ee4\u8cc7\u6599\uff0c\u53ef\u4ee5\u9054\u5230\u958b\u653e\u539f\u59cb\u78bc code LLM \u7684\u9802\u7d1a\u6548\u80fd\u3002\u7d50\u679c\u767c\u73fe\u57fa\u65bc\u57f7\u884c\u7684\u57fa\u6e96\u548c CodeArena \u4e4b\u9593\u7684\u6548\u80fd\u5dee\u7570\u3002\u6211\u5011\u5728 40 \u591a\u500b LLM \u4e0a\u9032\u884c CodeArena \u7684\u7cfb\u7d71\u6027\u5be6\u9a57\uff0c\u63ed\u793a\u4e86\u958b\u653e SOTA code LLM\uff08\u4f8b\u5982 Qwen2.5-Coder\uff09\u548c\u5c08\u6709 LLM\uff08\u4f8b\u5982 OpenAI o1\uff09\u4e4b\u9593\u986f\u8457\u7684\u6548\u80fd\u5dee\u8ddd\uff0c\u5f37\u8abf\u4e86\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u7684\u91cd\u8981\u6027\u3002\\footnote{\\url{https://codearenaeval.github.io/ }}</paragraph>", "author": "Jian Yang et.al.", "authors": "Jian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, Junyang Lin", "id": "2412.05210v1", "paper_url": "http://arxiv.org/abs/2412.05210v1", "repo": "null"}}