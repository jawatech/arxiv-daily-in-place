{"2406.18192": {"publish_time": "2024-06-26", "title": "Methodology of Adapting Large English Language Models for Specific Cultural Contexts", "paper_summary": "The rapid growth of large language models(LLMs) has emerged as a prominent\ntrend in the field of artificial intelligence. However, current\nstate-of-the-art LLMs are predominantly based on English. They encounter\nlimitations when directly applied to tasks in specific cultural domains, due to\ndeficiencies in domain-specific knowledge and misunderstandings caused by\ndifferences in cultural values. To address this challenge, our paper proposes a\nrapid adaptation method for large models in specific cultural contexts, which\nleverages instruction-tuning based on specific cultural knowledge and safety\nvalues data. Taking Chinese as the specific cultural context and utilizing the\nLLaMA3-8B as the experimental English LLM, the evaluation results demonstrate\nthat the adapted LLM significantly enhances its capabilities in domain-specific\nknowledge and adaptability to safety values, while maintaining its original\nexpertise advantages.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u767c\u5c55\u5df2\u6210\u70ba\u4eba\u5de5\u667a\u6167\u9818\u57df\u7684\u4e00\u80a1\u986f\u8457\u8da8\u52e2\u3002\u7136\u800c\uff0c\u76ee\u524d\u6700\u5148\u9032\u7684 LLM \u4e3b\u8981\u57fa\u65bc\u82f1\u6587\u3002\u7531\u65bc\u5728\u7279\u5b9a\u6587\u5316\u9818\u57df\u7684\u77e5\u8b58\u4e0d\u8db3\uff0c\u4ee5\u53ca\u6587\u5316\u50f9\u503c\u89c0\u5dee\u7570\u6240\u9020\u6210\u7684\u8aa4\u89e3\uff0c\u76f4\u63a5\u61c9\u7528\u65bc\u7279\u5b9a\u6587\u5316\u9818\u57df\u7684\u4efb\u52d9\u6642\u6703\u9047\u5230\u9650\u5236\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u6211\u5011\u7684\u8ad6\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u91dd\u5c0d\u7279\u5b9a\u6587\u5316\u80cc\u666f\u7684\u5927\u578b\u6a21\u578b\u5feb\u901f\u9069\u61c9\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u5229\u7528\u57fa\u65bc\u7279\u5b9a\u6587\u5316\u77e5\u8b58\u548c\u5b89\u5168\u50f9\u503c\u89c0\u6578\u64da\u7684\u6307\u4ee4\u5fae\u8abf\u3002\u4ee5\u4e2d\u6587\u70ba\u7279\u5b9a\u6587\u5316\u80cc\u666f\uff0c\u4e26\u5229\u7528 LLaMA3-8B \u4f5c\u70ba\u5be6\u9a57\u6027\u7684\u82f1\u6587 LLM\uff0c\u8a55\u4f30\u7d50\u679c\u8868\u660e\uff0c\u9069\u61c9\u5f8c\u7684 LLM \u5728\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\u548c\u5c0d\u5b89\u5168\u50f9\u503c\u89c0\u7684\u9069\u61c9\u6027\u65b9\u9762\u986f\u8457\u63d0\u5347\u4e86\u5176\u80fd\u529b\uff0c\u540c\u6642\u4fdd\u6301\u4e86\u5176\u539f\u6709\u7684\u5c08\u696d\u512a\u52e2\u3002", "author": "Wenjing Zhang et.al.", "authors": "Wenjing Zhang, Siqi Xiao, Xuejiao Lei, Ning Wang, Huazheng Zhang, Meijuan An, Bikun Yang, Zhaoxiang Liu, Kai Wang, Shiguo Lian", "id": "2406.18192v2", "paper_url": "http://arxiv.org/abs/2406.18192v2", "repo": "null"}}