{"2504.13180": {"publish_time": "2025-04-17", "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding", "paper_summary": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.", "paper_summary_zh": "<paragraph>\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u662f\u96fb\u8166\u8996\u89ba\u7814\u7a76\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\uff0c\u7136\u800c\u8a31\u591a\u9ad8\u6548\u80fd\u6a21\u578b\u4ecd\u7136\u662f\u9589\u6e90\u7684\uff0c\u906e\u853d\u4e86\u5b83\u5011\u7684\u6578\u64da\u3001\u8a2d\u8a08\u548c\u8a13\u7df4\u65b9\u6cd5\u3002\u7814\u7a76\u793e\u7fa4\u7684\u61c9\u5c0d\u65b9\u5f0f\u662f\u4f7f\u7528\u5f9e\u9ed1\u76d2\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u77e5\u8b58\u4f86\u6a19\u8a18\u8a13\u7df4\u6578\u64da\uff0c\u5f9e\u800c\u5728\u57fa\u6e96\u6e2c\u8a66\u4e2d\u53d6\u5f97\u4e86\u4e0d\u932f\u7684\u6210\u7e3e\uff0c\u4f46\u537b\u72a7\u7272\u4e86\u53ef\u8861\u91cf\u7684\u79d1\u5b78\u9032\u5c55\u3002\u7136\u800c\uff0c\u5982\u679c\u4e0d\u77e5\u9053\u6559\u5e2b\u6a21\u578b\u7684\u7d30\u7bc0\u53ca\u5176\u6578\u64da\u4f86\u6e90\uff0c\u5c31\u5f88\u96e3\u8861\u91cf\u79d1\u5b78\u9032\u5c55\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86\u5728\u5b8c\u5168\u958b\u653e\u548c\u53ef\u5fa9\u73fe\u7684\u6846\u67b6\u4e0b\u69cb\u5efa\u611f\u77e5\u8a9e\u8a00\u6a21\u578b (PLM)\uff0c\u4ee5\u4fc3\u9032\u5716\u50cf\u548c\u8996\u983b\u7406\u89e3\u65b9\u9762\u7684\u900f\u660e\u7814\u7a76\u3002\u6211\u5011\u5206\u6790\u4e86\u4e0d\u4f7f\u7528\u5c08\u6709\u6a21\u578b\u84b8\u993e\u7684\u6a19\u6e96\u8a13\u7df4\u6d41\u7a0b\uff0c\u4e26\u63a2\u7d22\u4e86\u5927\u898f\u6a21\u5408\u6210\u6578\u64da\uff0c\u4ee5\u8b58\u5225\u95dc\u9375\u7684\u6578\u64da\u7f3a\u53e3\uff0c\u5c24\u5176\u662f\u5728\u8a73\u7d30\u7684\u8996\u983b\u7406\u89e3\u65b9\u9762\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u4e9b\u7f3a\u53e3\uff0c\u6211\u5011\u767c\u5e03\u4e86 280 \u842c\u500b\u4eba\u5de5\u6a19\u8a18\u7684\u7d30\u7c92\u5ea6\u8996\u983b\u554f\u7b54\u5c0d\u548c\u6642\u7a7a\u5b9a\u4f4d\u7684\u8996\u983b\u5b57\u5e55\u5be6\u4f8b\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u63a8\u51fa\u4e86 PLM-VideoBench\uff0c\u9019\u662f\u4e00\u5957\u7528\u65bc\u8a55\u4f30\u5177\u6709\u6311\u6230\u6027\u7684\u8996\u983b\u7406\u89e3\u4efb\u52d9\u7684\u5de5\u5177\uff0c\u91cd\u9ede\u95dc\u6ce8\u5c0d\u8996\u983b\u7684\u300c\u4ec0\u9ebc\u300d\u3001\u300c\u54ea\u88e1\u300d\u3001\u300c\u4f55\u6642\u300d\u548c\u300c\u5982\u4f55\u300d\u9032\u884c\u63a8\u7406\u7684\u80fd\u529b\u3002\u6211\u5011\u901a\u904e\u63d0\u4f9b\u6578\u64da\u3001\u8a13\u7df4\u65b9\u6cd5\u3001\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\uff0c\u4f7f\u6211\u5011\u7684\u5de5\u4f5c\u5b8c\u5168\u53ef\u5fa9\u73fe\u3002</paragraph>\n", "author": "Jang Hyun Cho et.al.", "authors": "Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, Miguel Martin, Huiyu Wang, Hanoona Rasheed, Peize Sun, Po-Yao Huang, Daniel Bolya, Nikhila Ravi, Shashank Jain, Tammy Stark, Shane Moon, Babak Damavandi, Vivian Lee, Andrew Westbury, Salman Khan, Philipp Kr\u00e4henb\u00fchl, Piotr Doll\u00e1r, Lorenzo Torresani, Kristen Grauman, Christoph Feichtenhofer", "id": "2504.13180v1", "paper_url": "http://arxiv.org/abs/2504.13180v1", "repo": "null"}}