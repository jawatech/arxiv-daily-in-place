{"2408.00539": {"publish_time": "2024-08-01", "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs", "paper_summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.", "paper_summary_zh": "\u591a\u8f2a\u5c0d\u8a71\u662f\u4eba\u985e\u8207\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e4b\u9593\u7684\u4e00\u7a2e\u95dc\u9375\u4e92\u52d5\u65b9\u6cd5\uff0c\u7531\u65bc\u5c0d\u8a71\u6703\u6301\u7e8c\u591a\u8f2a\uff0c\u56e0\u6b64\u7dad\u6301 LLM \u7684\u9ad8\u751f\u6210\u54c1\u8cea\u548c\u4f4e\u5ef6\u9072\u662f\u4e00\u9805\u6311\u6230\u3002\u4e3b\u6d41 LLM \u53ef\u4ee5\u6839\u64da\u906e\u7f69\u7b56\u7565\u5206\u70ba\u5169\u985e\uff1a\u56e0\u679c LLM \u548c\u524d\u7db4 LLM\u3002\u591a\u9805\u7814\u7a76\u5df2\u8b49\u660e\uff0c\u524d\u7db4 LLM \u5728\u9ad8\u5ea6\u4f9d\u8cf4\u6b77\u53f2\u8108\u7d61\u7684\u5834\u666f\u4e2d\u5f80\u5f80\u512a\u65bc\u56e0\u679c LLM\uff0c\u4f8b\u5982\u591a\u8f2a\u5c0d\u8a71\u6216\u60c5\u5883\u5b78\u7fd2\uff0c\u9019\u8981\u6b78\u529f\u65bc\u5b83\u5011\u5c0d\u524d\u7db4\u5e8f\u5217\u7684\u96d9\u5411\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u524d\u7db4 LLM \u5728\u591a\u8f2a\u5c0d\u8a71\u8cc7\u6599\u96c6\u4e2d\u5b58\u5728\u56fa\u6709\u4f4e\u6548\u7387\u7684\u8a13\u7df4\u554f\u984c\u3002\u6b64\u5916\uff0c\u524d\u7db4 LLM \u7684\u6ce8\u610f\u529b\u6a5f\u5236\u4f7f\u5176\u7121\u6cd5\u5728\u5c0d\u8a71\u56de\u5408\u4e2d\u91cd\u8907\u4f7f\u7528\u9375\u503c\u5feb\u53d6 (KV \u5feb\u53d6) \u4f86\u964d\u4f4e\u751f\u6210\u5ef6\u9072\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba\u9593\u6b47\u534a\u5de5\u4f5c\u906e\u7f69 (ISM) \u7684\u65b0\u906e\u7f69\u65b9\u6848\u4f86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c0d\u5c0d\u8a71\u8a18\u9304\u4e2d\u7684\u67e5\u8a62\u548c\u7b54\u6848\u5957\u7528\u4ea4\u66ff\u7684\u96d9\u5411\u548c\u55ae\u5411\u6ce8\u610f\u529b\u3002\u900f\u904e\u9019\u7a2e\u65b9\u5f0f\uff0cISM \u80fd\u5920\u540c\u6642\u7dad\u6301\u524d\u7db4 LLM \u7684\u9ad8\u54c1\u8cea\u548c\u56e0\u679c LLM \u7684\u4f4e\u751f\u6210\u5ef6\u9072\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8aaa\u660e\u6211\u5011\u7684 ISM \u7372\u5f97\u4e86\u986f\u8457\u7684\u6548\u80fd\u3002", "author": "Mingcong Lu et.al.", "authors": "Mingcong Lu, Jiangcai Zhu, Wang Hao, Zheng Li, Shusheng Zhang, Kailai Shao, Chao Chen, Nan Li, Feng Wang, Xin Lu", "id": "2408.00539v1", "paper_url": "http://arxiv.org/abs/2408.00539v1", "repo": "null"}}