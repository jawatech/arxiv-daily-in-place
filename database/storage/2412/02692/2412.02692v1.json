{"2412.02692": {"publish_time": "2024-12-03", "title": "Taming Scalable Visual Tokenizer for Autoregressive Image Generation", "paper_summary": "Existing vector quantization (VQ) methods struggle with scalability, largely\nattributed to the instability of the codebook that undergoes partial updates\nduring training. The codebook is prone to collapse as utilization decreases,\ndue to the progressively widening distribution gap between non-activated codes\nand visual features. To solve the problem, we propose Index Backpropagation\nQuantization (IBQ), a new VQ method for the joint optimization of all codebook\nembeddings and the visual encoder. Applying a straight-through estimator on the\none-hot categorical distribution between the encoded feature and codebook, all\ncodes are differentiable and maintain a consistent latent space with the visual\nencoder. IBQ enables scalable training of visual tokenizers and, for the first\ntime, achieves a large-scale codebook ($2^{18}$) with high dimension ($256$)\nand high utilization. Experiments on the standard ImageNet benchmark\ndemonstrate the scalability and superiority of IBQ, achieving competitive\nresults on both reconstruction ($1.00$ rFID) and autoregressive visual\ngeneration ($2.05$ gFID). The code and models are available at\nhttps://github.com/TencentARC/SEED-Voken.", "paper_summary_zh": "\u73fe\u6709\u7684\u5411\u91cf\u91cf\u5316 (VQ) \u65b9\u6cd5\u96e3\u4ee5\u64f4\u5145\uff0c\u9019\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u6b78\u56e0\u65bc\u5728\u8a13\u7df4\u671f\u9593\u9032\u884c\u90e8\u5206\u66f4\u65b0\u7684\u78bc\u672c\u7684\u4e0d\u7a69\u5b9a\u6027\u3002\u7531\u65bc\u672a\u6fc0\u6d3b\u78bc\u548c\u8996\u89ba\u7279\u5fb5\u4e4b\u9593\u7684\u5206\u5e03\u5dee\u8ddd\u9010\u6f38\u64f4\u5927\uff0c\u56e0\u6b64\u78bc\u672c\u5bb9\u6613\u96a8\u8457\u5229\u7528\u7387\u7684\u964d\u4f4e\u800c\u5d29\u6f70\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7d22\u5f15\u53cd\u5411\u50b3\u64ad\u91cf\u5316 (IBQ)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684 VQ \u65b9\u6cd5\uff0c\u7528\u65bc\u6240\u6709\u78bc\u672c\u5d4c\u5165\u548c\u8996\u89ba\u7de8\u78bc\u5668\u7684\u806f\u5408\u6700\u4f73\u5316\u3002\u5728\u7de8\u78bc\u7279\u5fb5\u548c\u78bc\u672c\u4e4b\u9593\u7684\u4e00\u71b1\u985e\u5225\u5206\u4f48\u4e0a\u61c9\u7528\u76f4\u901a\u4f30\u8a08\u5668\uff0c\u6240\u6709\u78bc\u90fd\u662f\u53ef\u5fae\u5206\u7684\uff0c\u4e26\u8207\u8996\u89ba\u7de8\u78bc\u5668\u4fdd\u6301\u4e00\u81f4\u7684\u6f5b\u5728\u7a7a\u9593\u3002IBQ \u80fd\u5920\u64f4\u5145\u8996\u89ba\u6a19\u8a18\u5316\u7684\u8a13\u7df4\uff0c\u4e26\u4e14\u9996\u6b21\u5be6\u73fe\u4e86\u5177\u6709\u9ad8\u7dad\u5ea6 ($256$) \u548c\u9ad8\u5229\u7528\u7387\u7684\u5927\u898f\u6a21\u78bc\u672c ($2^{18}$)\u3002\u5728\u6a19\u6e96 ImageNet \u57fa\u6e96\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\u4e86 IBQ \u7684\u53ef\u64f4\u5145\u6027\u548c\u512a\u8d8a\u6027\uff0c\u5728\u91cd\u5efa ($1.00$ rFID) \u548c\u81ea\u8ff4\u6b78\u8996\u89ba\u751f\u6210 ($2.05$ gFID) \u4e0a\u90fd\u53d6\u5f97\u4e86\u6709\u7af6\u722d\u529b\u7684\u7d50\u679c\u3002\u4ee3\u78bc\u548c\u6a21\u578b\u53ef\u5728 https://github.com/TencentARC/SEED-Voken \u7372\u5f97\u3002", "author": "Fengyuan Shi et.al.", "authors": "Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, Limin Wang", "id": "2412.02692v1", "paper_url": "http://arxiv.org/abs/2412.02692v1", "repo": "null"}}