{"2410.03226": {"publish_time": "2024-10-04", "title": "Frame-Voyager: Learning to Query Frames for Video Large Language Models", "paper_summary": "Video Large Language Models (Video-LLMs) have made remarkable progress in\nvideo understanding tasks. However, they are constrained by the maximum length\nof input tokens, making it impractical to input entire videos. Existing frame\nselection approaches, such as uniform frame sampling and text-frame retrieval,\nfail to account for the information density variations in the videos or the\ncomplex instructions in the tasks, leading to sub-optimal performance. In this\npaper, we propose Frame-Voyager that learns to query informative frame\ncombinations, based on the given textual queries in the task. To train\nFrame-Voyager, we introduce a new data collection and labeling pipeline, by\nranking frame combinations using a pre-trained Video-LLM. Given a video of M\nframes, we traverse its T-frame combinations, feed them into a Video-LLM, and\nrank them based on Video-LLM's prediction losses. Using this ranking as\nsupervision, we train Frame-Voyager to query the frame combinations with lower\nlosses. In experiments, we evaluate Frame-Voyager on four Video Question\nAnswering benchmarks by plugging it into two different Video-LLMs. The\nexperimental results demonstrate that Frame-Voyager achieves impressive results\nin all settings, highlighting its potential as a plug-and-play solution for\nVideo-LLMs.", "paper_summary_zh": "\u5f71\u7247\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08Video-LLM\uff09\u5728\u5f71\u7247\u7406\u89e3\u4efb\u52d9\u4e2d\u53d6\u5f97\u986f\u8457\u9032\u5c55\u3002\u7136\u800c\uff0c\u5b83\u5011\u53d7\u5230\u8f38\u5165\u4ee4\u724c\u6700\u5927\u9577\u5ea6\u7684\u9650\u5236\uff0c\u4f7f\u5f97\u8f38\u5165\u6574\u500b\u5f71\u7247\u8b8a\u5f97\u4e0d\u5207\u5be6\u969b\u3002\u73fe\u6709\u7684\u5f71\u683c\u9078\u64c7\u65b9\u6cd5\uff0c\u4f8b\u5982\u5747\u52fb\u5f71\u683c\u53d6\u6a23\u548c\u6587\u5b57\u5f71\u683c\u6aa2\u7d22\uff0c\u7121\u6cd5\u8003\u91cf\u5f71\u7247\u4e2d\u7684\u8cc7\u8a0a\u5bc6\u5ea6\u8b8a\u5316\u6216\u4efb\u52d9\u4e2d\u7684\u8907\u96dc\u6307\u4ee4\uff0c\u5c0e\u81f4\u6b21\u4f73\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa Frame-Voyager\uff0c\u5b83\u6703\u6839\u64da\u4efb\u52d9\u4e2d\u7d66\u5b9a\u7684\u6587\u5b57\u67e5\u8a62\uff0c\u5b78\u7fd2\u67e5\u8a62\u6709\u8cc7\u8a0a\u6027\u7684\u5f71\u683c\u7d44\u5408\u3002\u70ba\u4e86\u8a13\u7df4 Frame-Voyager\uff0c\u6211\u5011\u900f\u904e\u4f7f\u7528\u9810\u5148\u8a13\u7df4\u7684 Video-LLM \u5c0d\u5f71\u683c\u7d44\u5408\u9032\u884c\u6392\u540d\uff0c\u5f15\u9032\u65b0\u7684\u8cc7\u6599\u6536\u96c6\u548c\u6a19\u8a18\u7ba1\u9053\u3002\u7d66\u5b9a M \u500b\u5f71\u683c\u7684\u5f71\u7247\uff0c\u6211\u5011\u6703\u904d\u6b77\u5176 T \u500b\u5f71\u683c\u7d44\u5408\uff0c\u5c07\u5b83\u5011\u8f38\u5165 Video-LLM\uff0c\u4e26\u6839\u64da Video-LLM \u7684\u9810\u6e2c\u640d\u5931\u5c0d\u5b83\u5011\u9032\u884c\u6392\u540d\u3002\u4f7f\u7528\u6b64\u6392\u540d\u4f5c\u70ba\u76e3\u7763\uff0c\u6211\u5011\u8a13\u7df4 Frame-Voyager \u67e5\u8a62\u640d\u5931\u8f03\u4f4e\u7684\u5f71\u683c\u7d44\u5408\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5c07 Frame-Voyager \u63d2\u5165\u5169\u500b\u4e0d\u540c\u7684 Video-LLM\uff0c\u5c0d\u5176\u5728\u56db\u500b\u5f71\u7247\u554f\u7b54\u57fa\u6e96\u9032\u884c\u8a55\u4f30\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0cFrame-Voyager \u5728\u6240\u6709\u8a2d\u5b9a\u4e2d\u5747\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7d50\u679c\uff0c\u7a81\u986f\u5176\u4f5c\u70ba Video-LLM \u5373\u63d2\u5373\u7528\u89e3\u6c7a\u65b9\u6848\u7684\u6f5b\u529b\u3002", "author": "Sicheng Yu et.al.", "authors": "Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xioalei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, Qianru Sun", "id": "2410.03226v1", "paper_url": "http://arxiv.org/abs/2410.03226v1", "repo": "null"}}