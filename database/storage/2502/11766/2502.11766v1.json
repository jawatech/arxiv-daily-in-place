{"2502.11766": {"publish_time": "2025-02-17", "title": "Warmup-Distill: Bridge the Distribution Mismatch between Teacher and Student before Knowledge Distillation", "paper_summary": "The widespread deployment of Large Language Models (LLMs) is hindered by the\nhigh computational demands, making knowledge distillation (KD) crucial for\ndeveloping compact smaller ones. However, the conventional KD methods endure\nthe distribution mismatch issue between the teacher and student models, leading\nto the poor performance of distillation. For instance, the widely-used KL-based\nmethods suffer the mode-averaging and mode-collapsing problems, since the\nmismatched probabitliy distribution between both models. Previous studies\nmainly optimize this issue via different distance calculations towards the\ndistribution of both models. Unfortunately, the distribution mismatch issue\nstill exists in the early stage of the distillation. Hence, to reduce the\nimpact of distribution mismatch, we propose a simple yet efficient method,\nnamed Warmup-Distill, which aligns the distillation of the student to that of\nthe teacher in advance of distillation. Specifically, we first detect the\ndistribution of the student model in practical scenarios with its internal\nknowledge, and then modify the knowledge with low probability via the teacher\nas the checker. Consequently, Warmup-Distill aligns the internal student's\nknowledge to that of the teacher, which expands the distribution of the student\nwith the teacher's, and assists the student model to learn better in the\nsubsequent distillation. Experiments on the seven benchmarks demonstrate that\nWarmup-Distill could provide a warmup student more suitable for distillation,\nwhich outperforms the vanilla student by as least +0.4 averaged score among all\nbenchmarks. Noteably, with the assistance of Warmup-Distill, the distillation\non the math task could yield a further improvement, at most +1.9% accuracy.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5ee3\u6cdb\u90e8\u7f72\u53d7\u5230\u9ad8\u904b\u7b97\u9700\u6c42\u7684\u963b\u7919\uff0c\u9019\u4f7f\u5f97\u77e5\u8b58\u84b8\u993e (KD) \u5c0d\u65bc\u958b\u767c\u7dca\u6e4a\u578b\u7684\u5c0f\u578b\u6a21\u578b\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u50b3\u7d71\u7684 KD \u65b9\u6cd5\u5fcd\u53d7\u4e86\u6559\u5e2b\u548c\u5b78\u751f\u6a21\u578b\u4e4b\u9593\u7684\u5206\u5e03\u4e0d\u5339\u914d\u554f\u984c\uff0c\u5c0e\u81f4\u84b8\u993e\u6548\u679c\u4e0d\u4f73\u3002\u4f8b\u5982\uff0c\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u65bc KL \u7684\u65b9\u6cd5\u6703\u51fa\u73fe\u6a21\u5f0f\u5e73\u5747\u548c\u6a21\u5f0f\u5d29\u6f70\u554f\u984c\uff0c\u56e0\u70ba\u5169\u500b\u6a21\u578b\u4e4b\u9593\u7684\u6a5f\u7387\u5206\u4f48\u4e0d\u5339\u914d\u3002\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u900f\u904e\u4e0d\u540c\u7684\u8ddd\u96e2\u8a08\u7b97\u4f86\u6700\u4f73\u5316\u9019\u500b\u554f\u984c\uff0c\u4ee5\u671d\u5411\u5169\u500b\u6a21\u578b\u7684\u5206\u5e03\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u5206\u5e03\u4e0d\u5339\u914d\u7684\u554f\u984c\u4ecd\u7136\u5b58\u5728\u65bc\u84b8\u993e\u7684\u65e9\u671f\u968e\u6bb5\u3002\u56e0\u6b64\uff0c\u70ba\u4e86\u6e1b\u5c11\u5206\u5e03\u4e0d\u5339\u914d\u7684\u5f71\u97ff\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7a31\u70ba Warmup-Distill\uff0c\u5b83\u5728\u84b8\u993e\u4e4b\u524d\u5c07\u5b78\u751f\u7684\u84b8\u993e\u8207\u6559\u5e2b\u7684\u84b8\u993e\u5c0d\u9f4a\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528\u5176\u5167\u90e8\u77e5\u8b58\u5728\u5be6\u969b\u5834\u666f\u4e2d\u6aa2\u6e2c\u5b78\u751f\u7684\u5206\u5e03\uff0c\u7136\u5f8c\u900f\u904e\u6559\u5e2b\u4f5c\u70ba\u6aa2\u67e5\u54e1\u4fee\u6539\u4f4e\u6a5f\u7387\u7684\u77e5\u8b58\u3002\u56e0\u6b64\uff0cWarmup-Distill \u5c07\u5b78\u751f\u7684\u5167\u90e8\u77e5\u8b58\u8207\u6559\u5e2b\u7684\u77e5\u8b58\u5c0d\u9f4a\uff0c\u9019\u6703\u5c07\u5b78\u751f\u7684\u5206\u5e03\u64f4\u5c55\u5230\u6559\u5e2b\u7684\u5206\u5e03\uff0c\u4e26\u5354\u52a9\u5b78\u751f\u6a21\u578b\u5728\u5f8c\u7e8c\u7684\u84b8\u993e\u4e2d\u5b78\u7fd2\u5f97\u66f4\u597d\u3002\u5728\u4e03\u500b\u57fa\u6e96\u6e2c\u8a66\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0cWarmup-Distill \u53ef\u4ee5\u63d0\u4f9b\u66f4\u9069\u5408\u84b8\u993e\u7684\u71b1\u8eab\u5b78\u751f\uff0c\u5728\u6240\u6709\u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0c\u5176\u8868\u73fe\u512a\u65bc\u9999\u8349\u5b78\u751f\u81f3\u5c11 +0.4 \u7684\u5e73\u5747\u5206\u6578\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728 Warmup-Distill \u7684\u5354\u52a9\u4e0b\uff0c\u6578\u5b78\u4efb\u52d9\u4e0a\u7684\u84b8\u993e\u53ef\u4ee5\u9032\u4e00\u6b65\u63d0\u5347\uff0c\u6700\u591a\u53ef\u63d0\u5347 +1.9% \u7684\u6e96\u78ba\u5ea6\u3002", "author": "Zengkui Sun et.al.", "authors": "Zengkui Sun, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou", "id": "2502.11766v1", "paper_url": "http://arxiv.org/abs/2502.11766v1", "repo": "null"}}