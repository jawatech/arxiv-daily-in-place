{"2412.06538": {"publish_time": "2024-12-09", "title": "Understanding Factual Recall in Transformers via Associative Memories", "paper_summary": "Large language models have demonstrated an impressive ability to perform\nfactual recall. Prior work has found that transformers trained on factual\nrecall tasks can store information at a rate proportional to their parameter\ncount. In our work, we show that shallow transformers can use a combination of\nassociative memories to obtain such near optimal storage capacity. We begin by\nproving that the storage capacities of both linear and MLP associative memories\nscale linearly with parameter count. We next introduce a synthetic factual\nrecall task, and prove that a transformer with a single layer of self-attention\nfollowed by an MLP can obtain 100% accuracy on the task whenever either the\ntotal number of self-attention parameters or MLP parameters scales (up to log\nfactors) linearly with the number of facts. In particular, the transformer can\ntrade off between using the value matrices or the MLP as an associative memory\nto store the dataset of facts. We complement these expressivity results with an\nanalysis of the gradient flow trajectory of a simplified linear attention model\ntrained on our factual recall task, where we show that the model exhibits\nsequential learning behavior.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5df2\u8b49\u660e\u5176\u5177\u5099\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u57f7\u884c\u4e8b\u5be6\u56de\u61b6\u7684\u80fd\u529b\u3002\u5148\u524d\u7684\u7814\u7a76\u767c\u73fe\uff0c\u91dd\u5c0d\u4e8b\u5be6\u56de\u61b6\u4efb\u52d9\u8a13\u7df4\u7684Transformer\u53ef\u4ee5\u4ee5\u8207\u5176\u53c3\u6578\u8a08\u6578\u6210\u6b63\u6bd4\u7684\u901f\u5ea6\u5132\u5b58\u8cc7\u8a0a\u3002\u5728\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6dfa\u5c64Transformer\u53ef\u4ee5\u4f7f\u7528\u806f\u60f3\u8a18\u61b6\u9ad4\u7684\u7d44\u5408\u4f86\u7372\u5f97\u9019\u7a2e\u63a5\u8fd1\u6700\u4f73\u7684\u5132\u5b58\u5bb9\u91cf\u3002\u6211\u5011\u9996\u5148\u8b49\u660e\u4e86\u7dda\u6027\u806f\u60f3\u8a18\u61b6\u9ad4\u548c MLP \u806f\u60f3\u8a18\u61b6\u9ad4\u7684\u5132\u5b58\u5bb9\u91cf\u8207\u53c3\u6578\u8a08\u6578\u6210\u6b63\u6bd4\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u5408\u6210\u4e8b\u5be6\u56de\u61b6\u4efb\u52d9\uff0c\u4e26\u8b49\u660e\u4e86\u4e00\u500b\u5177\u6709\u55ae\u5c64\u81ea\u6ce8\u610f\u529b\uff0c\u5f8c\u63a5 MLP \u7684Transformer\u53ef\u4ee5\u5728\u4efb\u52d9\u4e2d\u7372\u5f97 100% \u7684\u6e96\u78ba\u5ea6\uff0c\u53ea\u8981\u81ea\u6ce8\u610f\u529b\u53c3\u6578\u6216 MLP \u53c3\u6578\u7684\u7e3d\u6578\uff08\u6700\u591a\u70ba log \u56e0\u5b50\uff09\u8207\u4e8b\u5be6\u6578\u91cf\u6210\u7dda\u6027\u6bd4\u4f8b\u3002\u7279\u5225\u662f\uff0cTransformer\u53ef\u4ee5\u5728\u4f7f\u7528\u503c\u77e9\u9663\u6216 MLP \u4f5c\u70ba\u806f\u60f3\u8a18\u61b6\u9ad4\u4f86\u5132\u5b58\u4e8b\u5be6\u8cc7\u6599\u96c6\u4e4b\u9593\u9032\u884c\u6b0a\u8861\u3002\u6211\u5011\u7528\u5c0d\u5728\u6211\u5011\u7684\u73fe\u5be6\u56de\u61b6\u4efb\u52d9\u4e2d\u8a13\u7df4\u7684\u7c21\u5316\u7dda\u6027\u6ce8\u610f\u529b\u6a21\u578b\u7684\u68af\u5ea6\u6d41\u8ecc\u8de1\u7684\u5206\u6790\u4f86\u88dc\u5145\u9019\u4e9b\u8868\u73fe\u529b\u7d50\u679c\uff0c\u5176\u4e2d\u6211\u5011\u5c55\u793a\u4e86\u8a72\u6a21\u578b\u8868\u73fe\u51fa\u9806\u5e8f\u5b78\u7fd2\u884c\u70ba\u3002", "author": "Eshaan Nichani et.al.", "authors": "Eshaan Nichani, Jason D. Lee, Alberto Bietti", "id": "2412.06538v1", "paper_url": "http://arxiv.org/abs/2412.06538v1", "repo": "null"}}