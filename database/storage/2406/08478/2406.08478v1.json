{"2406.08478": {"publish_time": "2024-06-12", "title": "What If We Recaption Billions of Web Images with LLaMA-3?", "paper_summary": "Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate\nthat semantically aligning and enriching textual descriptions of these pairs\ncan significantly enhance model training across various vision-language tasks,\nparticularly text-to-image generation. However, large-scale investigations in\nthis area remain predominantly closed-source. Our paper aims to bridge this\ncommunity effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a\nGPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a\nLLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images\nfrom the DataComp-1B dataset. Our empirical results confirm that this enhanced\ndataset, Recap-DataComp-1B, offers substantial benefits in training advanced\nvision-language models. For discriminative models like CLIP, we observe\nenhanced zero-shot performance in cross-modal retrieval tasks. For generative\nmodels like text-to-image Diffusion Transformers, the generated images exhibit\na significant improvement in alignment with users' text instructions,\nespecially in following complex queries. Our project page is\nhttps://www.haqtu.me/Recap-Datacomp-1B/", "paper_summary_zh": "\u7db2\u8def\u722c\u53d6\u7684\u5f71\u50cf\u6587\u5b57\u914d\u5c0d\u672c\u8eab\u5c31\u5b58\u5728\u96dc\u8a0a\u3002\u5148\u524d\u7684\u7814\u7a76\u8b49\u660e\uff0c\u5c0d\u9019\u4e9b\u914d\u5c0d\u7684\u6587\u5b57\u63cf\u8ff0\u9032\u884c\u8a9e\u610f\u5c0d\u9f4a\u548c\u8c50\u5bcc\u5316\uff0c\u53ef\u4ee5\u5927\u5e45\u589e\u5f37\u5404\u7a2e\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u7684\u6a21\u578b\u8a13\u7df4\uff0c\u5c24\u5176\u662f\u6587\u5b57\u8f49\u5f71\u50cf\u751f\u6210\u3002\u7136\u800c\uff0c\u9019\u65b9\u9762\u7684\u898f\u6a21\u5316\u8abf\u67e5\u4ecd\u4ee5\u9589\u6e90\u70ba\u4e3b\u3002\u6211\u5011\u7684\u8ad6\u6587\u65e8\u5728\u5f4c\u5408\u793e\u7fa4\u7684\u52aa\u529b\uff0c\u5229\u7528\u529f\u80fd\u5f37\u5927\u4e14\\textit{\u958b\u653e\u539f\u59cb\u78bc}\u7684 LLaMA-3\uff0c\u4e00\u500b GPT-4 \u7b49\u7d1a\u7684 LLM\u3002\u6211\u5011\u7684\u91cd\u65b0\u6a19\u984c\u8655\u7406\u6d41\u7a0b\u5f88\u7c21\u55ae\uff1a\u9996\u5148\uff0c\u6211\u5011\u5fae\u8abf\u4e00\u500b\u7531 LLaMA-3-8B \u9a45\u52d5\u7684 LLaVA-1.5\uff0c\u7136\u5f8c\u4f7f\u7528\u5b83\u91cd\u65b0\u6a19\u984c\u4f86\u81ea DataComp-1B \u8cc7\u6599\u96c6\u7684 13 \u5104\u5f35\u5f71\u50cf\u3002\u6211\u5011\u7684\u7d93\u9a57\u7d50\u679c\u8b49\u5be6\uff0c\u9019\u500b\u589e\u5f37\u7684\u8cc7\u6599\u96c6 Recap-DataComp-1B\uff0c\u5728\u8a13\u7df4\u9032\u968e\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u65b9\u9762\u63d0\u4f9b\u4e86\u5927\u91cf\u7684\u512a\u9ede\u3002\u5c0d\u65bc\u50cf CLIP \u9019\u6a23\u7684\u5224\u5225\u6a21\u578b\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u5728\u8de8\u6a21\u614b\u6aa2\u7d22\u4efb\u52d9\u4e2d\u589e\u5f37\u7684\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\u3002\u5c0d\u65bc\u50cf\u6587\u5b57\u8f49\u5f71\u50cf Diffusion Transformers \u9019\u6a23\u7684\u751f\u6210\u6a21\u578b\uff0c\u751f\u6210\u7684\u5f71\u50cf\u5728\u8207\u4f7f\u7528\u8005\u6587\u5b57\u6307\u4ee4\u5c0d\u9f4a\u65b9\u9762\u6709\u986f\u8457\u7684\u6539\u5584\uff0c\u7279\u5225\u662f\u5728\u9075\u5faa\u8907\u96dc\u67e5\u8a62\u6642\u3002\u6211\u5011\u7684\u5c08\u6848\u9801\u9762\u662f https://www.haqtu.me/Recap-Datacomp-1B/", "author": "Xianhang Li et.al.", "authors": "Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, Cihang Xie", "id": "2406.08478v1", "paper_url": "http://arxiv.org/abs/2406.08478v1", "repo": "null"}}