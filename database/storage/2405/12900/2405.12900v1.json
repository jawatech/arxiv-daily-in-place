{"2405.12900": {"publish_time": "2024-05-21", "title": "Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents", "paper_summary": "Recent advancements in open-domain dialogue systems have been propelled by\nthe emergence of high-quality large language models (LLMs) and various\neffective training methodologies. Nevertheless, the presence of toxicity within\nthese models presents a significant challenge that can potentially diminish the\nuser experience. In this study, we introduce an innovative training algorithm,\nan improvement upon direct preference optimization (DPO), called adversarial\nDPO (ADPO). The ADPO algorithm is designed to train models to assign higher\nprobability distributions to preferred responses and lower distributions to\nunsafe responses, which are self-generated using the toxic control token. We\ndemonstrate that ADPO enhances the model's resilience against harmful\nconversations while minimizing performance degradation. Furthermore, we\nillustrate that ADPO offers a more stable training procedure compared to the\ntraditional DPO. To the best of our knowledge, this is the first adaptation of\nthe DPO algorithm that directly incorporates harmful data into the generative\nmodel, thereby reducing the need to artificially create safe dialogue data.", "paper_summary_zh": "\u958b\u653e\u9818\u57df\u5c0d\u8a71\u7cfb\u7d71\u7684\u6700\u65b0\u9032\u5c55\u662f\u7531\u9ad8\u54c1\u8cea\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u5404\u7a2e\u6709\u6548\u8a13\u7df4\u65b9\u6cd5\u7684\u51fa\u73fe\u6240\u63a8\u52d5\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6bd2\u6027\u662f\u4e00\u500b\u91cd\u5927\u6311\u6230\uff0c\u53ef\u80fd\u6703\u964d\u4f4e\u4f7f\u7528\u8005\u9ad4\u9a57\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u8a13\u7df4\u6f14\u7b97\u6cd5\uff0c\u4e00\u7a2e\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\u7684\u6539\u9032\uff0c\u7a31\u70ba\u5c0d\u6297\u6027 DPO (ADPO)\u3002ADPO \u6f14\u7b97\u6cd5\u65e8\u5728\u8a13\u7df4\u6a21\u578b\u5c07\u8f03\u9ad8\u7684\u6a5f\u7387\u5206\u4f48\u5206\u914d\u7d66\u504f\u597d\u7684\u56de\u61c9\uff0c\u4e26\u5c07\u8f03\u4f4e\u7684\u6a5f\u7387\u5206\u4f48\u5206\u914d\u7d66\u4e0d\u5b89\u5168\u7684\u56de\u61c9\uff0c\u9019\u4e9b\u56de\u61c9\u662f\u4f7f\u7528\u6709\u6bd2\u63a7\u5236\u4ee3\u78bc\u81ea\u751f\u3002\u6211\u5011\u8b49\u660e ADPO \u63d0\u5347\u4e86\u6a21\u578b\u5c0d\u6709\u5bb3\u5c0d\u8a71\u7684\u5fa9\u539f\u529b\uff0c\u540c\u6642\u5c07\u6548\u80fd\u4e0b\u964d\u964d\u5230\u6700\u4f4e\u3002\u6b64\u5916\uff0c\u6211\u5011\u8aaa\u660e ADPO \u8207\u50b3\u7d71 DPO \u76f8\u6bd4\u63d0\u4f9b\u4e86\u66f4\u7a69\u5b9a\u7684\u8a13\u7df4\u7a0b\u5e8f\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u9019\u662f DPO \u6f14\u7b97\u6cd5\u9996\u6b21\u5c07\u6709\u5bb3\u8cc7\u6599\u76f4\u63a5\u7d0d\u5165\u751f\u6210\u6a21\u578b\uff0c\u5f9e\u800c\u6e1b\u5c11\u4e86\u4eba\u5de5\u5efa\u7acb\u5b89\u5168\u5c0d\u8a71\u8cc7\u6599\u7684\u9700\u6c42\u3002", "author": "San Kim et.al.", "authors": "San Kim, Gary Geunbae Lee", "id": "2405.12900v1", "paper_url": "http://arxiv.org/abs/2405.12900v1", "repo": "null"}}