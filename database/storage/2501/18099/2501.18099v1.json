{"2501.18099": {"publish_time": "2025-01-30", "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge", "paper_summary": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to\ncapture the step-bystep reasoning process that underlies the final evaluation\nof a response. However, due to the lack of human annotated CoTs for evaluation,\nthe required components and structure of effective reasoning traces remain\nunderstudied. Consequently, previous approaches often (1) constrain reasoning\ntraces to hand-designed components, such as a list of criteria, reference\nanswers, or verification questions and (2) structure them such that planning is\nintertwined with the reasoning for evaluation. In this work, we propose\nEvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge\nthat first generates an unconstrained evaluation plan, followed by its\nexecution, and then the final judgment. In a self-training loop, EvalPlanner\niteratively optimizes over synthetically constructed evaluation plans and\nexecutions, leading to better final verdicts. Our method achieves a new\nstate-of-the-art performance for generative reward models on RewardBench (with\na score of 93.9), despite being trained on fewer amount of, and synthetically\ngenerated, preference pairs. Additional experiments on other benchmarks like\nRM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both\nplanning and reasoning for building robust LLM-as-a-Judge reasoning models.", "paper_summary_zh": "LLM-as-a-Judge \u6a21\u578b\u6703\u7522\u751f\u601d\u60f3\u93c8 (CoT) \u5e8f\u5217\uff0c\u65e8\u5728\u6355\u6349\u6700\u7d42\u8a55\u4f30\u56de\u61c9\u80cc\u5f8c\u7684\u9010\u6b65\u63a8\u7406\u904e\u7a0b\u3002\u7136\u800c\uff0c\u7531\u65bc\u7f3a\u4e4f\u7528\u65bc\u8a55\u4f30\u7684\u4eba\u985e\u8a3b\u89e3 CoT\uff0c\u6709\u6548\u63a8\u7406\u8ecc\u8de1\u6240\u9700\u7684\u7d44\u6210\u90e8\u5206\u548c\u7d50\u69cb\u4ecd\u7136\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u5148\u524d\u7684\u505a\u6cd5\u901a\u5e38 (1) \u5c07\u63a8\u7406\u8ecc\u8de1\u9650\u5236\u70ba\u624b\u5de5\u8a2d\u8a08\u7684\u7d44\u6210\u90e8\u5206\uff0c\u4f8b\u5982\u6a19\u6e96\u6e05\u55ae\u3001\u53c3\u8003\u7b54\u6848\u6216\u9a57\u8b49\u554f\u984c\uff0c\u4e26 (2) \u5c0d\u5176\u9032\u884c\u7d50\u69cb\u5316\uff0c\u4ee5\u4fbf\u898f\u5283\u8207\u8a55\u4f30\u63a8\u7406\u4ea4\u7e54\u5728\u4e00\u8d77\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 EvalPlanner\uff0c\u9019\u662f\u4e00\u7a2e\u504f\u597d\u6700\u4f73\u5316\u6f14\u7b97\u6cd5\uff0c\u9069\u7528\u65bc Thinking-LLM-as-a-Judge\uff0c\u5b83\u9996\u5148\u7522\u751f\u4e00\u500b\u4e0d\u53d7\u7d04\u675f\u7684\u8a55\u4f30\u8a08\u756b\uff0c\u63a5\u8457\u57f7\u884c\u5b83\uff0c\u7136\u5f8c\u505a\u51fa\u6700\u7d42\u5224\u65b7\u3002\u5728\u81ea\u8a13\u7df4\u8ff4\u5708\u4e2d\uff0cEvalPlanner \u53cd\u8986\u6700\u4f73\u5316\u5408\u6210\u5efa\u69cb\u7684\u8a55\u4f30\u8a08\u756b\u548c\u57f7\u884c\uff0c\u5f9e\u800c\u5f97\u51fa\u66f4\u597d\u7684\u6700\u7d42\u5224\u6c7a\u3002\u5118\u7ba1\u6211\u5011\u7684\u8a13\u7df4\u65b9\u6cd5\u4f7f\u7528\u8f03\u5c11\u4e14\u5408\u6210\u7522\u751f\u7684\u504f\u597d\u914d\u5c0d\uff0c\u4f46\u6211\u5011\u7684\u6a21\u578b\u5728 RewardBench \u4e0a\u91dd\u5c0d\u751f\u6210\u5f0f\u734e\u52f5\u6a21\u578b\u9054\u5230\u4e86\u65b0\u7684\u6700\u4f73\u6548\u80fd\uff08\u5f97\u5206\u70ba 93.9\uff09\u3002\u5728\u5176\u4ed6\u57fa\u6e96\u6e2c\u8a66\uff08\u4f8b\u5982 RM-Bench\u3001JudgeBench \u548c FollowBenchEval\uff09\u4e0a\u7684\u5176\u4ed6\u5be6\u9a57\u9032\u4e00\u6b65\u7a81\u986f\u4e86\u898f\u5283\u548c\u63a8\u7406\u5c0d\u65bc\u5efa\u7acb\u5f37\u5927\u7684 LLM-as-a-Judge \u63a8\u7406\u6a21\u578b\u7684\u6548\u7528\u3002", "author": "Swarnadeep Saha et.al.", "authors": "Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang", "id": "2501.18099v1", "paper_url": "http://arxiv.org/abs/2501.18099v1", "repo": "null"}}