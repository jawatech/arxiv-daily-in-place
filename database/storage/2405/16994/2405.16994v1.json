{"2405.16994": {"publish_time": "2024-05-27", "title": "Vision-and-Language Navigation Generative Pretrained Transformer", "paper_summary": "In the Vision-and-Language Navigation (VLN) field, agents are tasked with\nnavigating real-world scenes guided by linguistic instructions. Enabling the\nagent to adhere to instructions throughout the process of navigation represents\na significant challenge within the domain of VLN. To address this challenge,\ncommon approaches often rely on encoders to explicitly record past locations\nand actions, increasing model complexity and resource consumption. Our\nproposal, the Vision-and-Language Navigation Generative Pretrained Transformer\n(VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory\nsequence dependencies, bypassing the need for historical encoding modules. This\nmethod allows for direct historical information access through trajectory\nsequence, enhancing efficiency. Furthermore, our model separates the training\nprocess into offline pre-training with imitation learning and online\nfine-tuning with reinforcement learning. This distinction allows for more\nfocused training objectives and improved performance. Performance assessments\non the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art\nencoder-based models.", "paper_summary_zh": "\u5728\u8996\u89ba\u8207\u8a9e\u8a00\u5c0e\u822a (VLN) \u9818\u57df\u4e2d\uff0c\u4ee3\u7406\u88ab\u8ce6\u4e88\u4efb\u52d9\uff0c\u5728\u8a9e\u8a00\u6307\u4ee4\u7684\u5f15\u5c0e\u4e0b\u5c0e\u822a\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u3002\u8b93\u4ee3\u7406\u5728\u6574\u500b\u5c0e\u822a\u904e\u7a0b\u4e2d\u9075\u5b88\u6307\u4ee4\uff0c\u4ee3\u8868\u4e86 VLN \u9818\u57df\u4e2d\u7684\u91cd\u5927\u6311\u6230\u3002\u70ba\u4e86\u61c9\u5c0d\u6b64\u6311\u6230\uff0c\u5e38\u898b\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u7de8\u78bc\u5668\u4f86\u660e\u78ba\u8a18\u9304\u904e\u53bb\u7684\u4f4d\u7f6e\u548c\u52d5\u4f5c\uff0c\u589e\u52a0\u4e86\u6a21\u578b\u7684\u8907\u96dc\u6027\u548c\u8cc7\u6e90\u6d88\u8017\u3002\u6211\u5011\u7684\u63d0\u6848\uff0c\u5373\u8996\u89ba\u8207\u8a9e\u8a00\u5c0e\u822a\u751f\u6210\u5f0f\u9810\u8a13\u7df4Transformer (VLN-GPT)\uff0c\u63a1\u7528Transformer\u89e3\u78bc\u5668\u6a21\u578b (GPT2) \u4f86\u5efa\u6a21\u8ecc\u8de1\u5e8f\u5217\u4f9d\u8cf4\u6027\uff0c\u7e5e\u904e\u4e86\u5c0d\u6b77\u53f2\u7de8\u78bc\u6a21\u7d44\u7684\u9700\u6c42\u3002\u9019\u7a2e\u65b9\u6cd5\u5141\u8a31\u901a\u904e\u8ecc\u8de1\u5e8f\u5217\u76f4\u63a5\u5b58\u53d6\u6b77\u53f2\u8cc7\u8a0a\uff0c\u5f9e\u800c\u63d0\u9ad8\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6a21\u578b\u5c07\u8a13\u7df4\u904e\u7a0b\u5206\u70ba\u96e2\u7dda\u9810\u8a13\u7df4\uff08\u900f\u904e\u6a21\u4eff\u5b78\u7fd2\uff09\u548c\u7dda\u4e0a\u5fae\u8abf\uff08\u900f\u904e\u5f37\u5316\u5b78\u7fd2\uff09\u3002\u6b64\u5340\u5225\u5141\u8a31\u66f4\u5c08\u6ce8\u7684\u8a13\u7df4\u76ee\u6a19\u548c\u6539\u9032\u7684\u6548\u80fd\u3002\u5728 VLN \u8cc7\u6599\u96c6\u4e0a\u7684\u6548\u80fd\u8a55\u4f30\u986f\u793a\uff0cVLN-GPT \u8d85\u8d8a\u4e86\u8907\u96dc\u7684\u6700\u65b0\u7de8\u78bc\u5668\u6a21\u578b\u3002", "author": "Wen Hanlin et.al.", "authors": "Wen Hanlin", "id": "2405.16994v1", "paper_url": "http://arxiv.org/abs/2405.16994v1", "repo": "null"}}