{"2411.07688": {"publish_time": "2024-11-12", "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG", "paper_summary": "Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000\n$\\times$ 100,000 pixels or more) poses a significant challenge for current\nRemote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize\nthe UHR image to standard input image size, the extensive spatial and\ncontextual information that UHR images contain will be neglected. Otherwise,\nthe original size of these images often exceeds the token limits of standard\nRSMLLMs, making it difficult to process the entire image and capture long-range\ndependencies to answer the query based on the abundant visual context. In this\npaper, we introduce ImageRAG for RS, a training-free framework to address the\ncomplexities of analyzing UHR remote sensing imagery. By transforming UHR\nremote sensing image analysis task to image's long context selection task, we\ndesign an innovative image contextual retrieval mechanism based on the\nRetrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's\ncore innovation lies in its ability to selectively retrieve and focus on the\nmost relevant portions of the UHR image as visual contexts that pertain to a\ngiven query. Fast path and slow path are proposed in this framework to handle\nthis task efficiently and effectively. ImageRAG allows RSMLLMs to manage\nextensive context and spatial information from UHR RSI, ensuring the analysis\nis both accurate and efficient.", "paper_summary_zh": "\u8d85\u9ad8\u5206\u8fa8\u7387 (UHR) \u9065\u611f\u5f71\u50cf (RSI)\uff08\u4f8b\u5982 100,000\n$\\times$ 100,000 \u50cf\u7d20\u6216\u66f4\u591a\uff09\u5bf9\u5f53\u524d\u7684\u9065\u611f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (RSMLLM) \u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u5982\u679c\u9009\u62e9\u5c06 UHR \u5f71\u50cf\u8c03\u6574\u4e3a\u6807\u51c6\u8f93\u5165\u5f71\u50cf\u5927\u5c0f\uff0c\u5219 UHR \u5f71\u50cf\u6240\u5305\u542b\u7684\u5e7f\u6cdb\u7a7a\u95f4\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5c06\u88ab\u5ffd\u7565\u3002\u5426\u5219\uff0c\u8fd9\u4e9b\u5f71\u50cf\u7684\u539f\u59cb\u5927\u5c0f\u901a\u5e38\u4f1a\u8d85\u51fa\u6807\u51c6 RSMLLM \u7684\u6807\u8bb0\u9650\u5236\uff0c\u4ece\u800c\u96be\u4ee5\u5904\u7406\u6574\u4e2a\u5f71\u50cf\u5e76\u6355\u6349\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u6839\u636e\u4e30\u5bcc\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u6765\u56de\u7b54\u67e5\u8be2\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u7528\u4e8e\u9065\u611f\u7684 ImageRAG\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u6790 UHR \u9065\u611f\u5f71\u50cf\u7684\u590d\u6742\u6027\u3002\u901a\u8fc7\u5c06 UHR \u9065\u611f\u5f71\u50cf\u5206\u6790\u4efb\u52a1\u8f6c\u6362\u4e3a\u5f71\u50cf\u7684\u957f\u4e0a\u4e0b\u6587\u9009\u62e9\u4efb\u52a1\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u6280\u672f\u7684\u521b\u65b0\u5f71\u50cf\u4e0a\u4e0b\u6587\u68c0\u7d22\u673a\u5236\uff0c\u79f0\u4e3a ImageRAG\u3002ImageRAG \u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5b83\u80fd\u591f\u9009\u62e9\u6027\u5730\u68c0\u7d22\u548c\u5173\u6ce8 UHR \u5f71\u50cf\u4e2d\u4e0e\u7ed9\u5b9a\u67e5\u8be2\u76f8\u5173\u7684\u6700\u76f8\u5173\u90e8\u5206\u4f5c\u4e3a\u89c6\u89c9\u4e0a\u4e0b\u6587\u3002\u5728\u6b64\u6846\u67b6\u4e2d\u63d0\u51fa\u4e86\u5feb\u901f\u8def\u5f84\u548c\u6162\u901f\u8def\u5f84\u6765\u9ad8\u6548\u6709\u6548\u5730\u5904\u7406\u6b64\u4efb\u52a1\u3002ImageRAG \u5141\u8bb8 RSMLLM \u7ba1\u7406\u6765\u81ea UHR RSI \u7684\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u786e\u4fdd\u5206\u6790\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\u3002", "author": "Zilun Zhang et.al.", "authors": "Zilun Zhang, Haozhan Shen, Tiancheng Zhao, Yuhao Wang, Bin Chen, Yuxiang Cai, Yongheng Shang, Jianwei Yin", "id": "2411.07688v1", "paper_url": "http://arxiv.org/abs/2411.07688v1", "repo": "null"}}