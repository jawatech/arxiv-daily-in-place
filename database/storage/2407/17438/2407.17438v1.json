{"2407.17438": {"publish_time": "2024-07-24", "title": "HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation", "paper_summary": "Human image animation involves generating videos from a character photo,\nallowing user control and unlocking potential for video and movie production.\nWhile recent approaches yield impressive results using high-quality training\ndata, the inaccessibility of these datasets hampers fair and transparent\nbenchmarking. Moreover, these approaches prioritize 2D human motion and\noverlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation.To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of copyright-free real-world\nvideos from the internet. Through a carefully designed rule-based filtering\nstrategy, we ensure the inclusion of high-quality videos, resulting in a\ncollection of 20K human-centric videos in 1080P resolution. Human and camera\nmotion annotation is accomplished using a 2D pose estimator and a SLAM-based\nmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets\nto augment existing available 3D assets. Notably, we introduce a rule-based\ncamera trajectory generation method, enabling the synthetic pipeline to\nincorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish\na baseline model named CamAnimate, short for Camera-controllable Human\nAnimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be\npublicly available at \\url{https://github.com/zhenzhiwang/HumanVid/}.", "paper_summary_zh": "\u4eba\u985e\u5f71\u50cf\u52d5\u756b\u6d89\u53ca\u5f9e\u89d2\u8272\u7167\u7247\u7522\u751f\u5f71\u7247\uff0c\n\u8b93\u4f7f\u7528\u8005\u53ef\u4ee5\u63a7\u5236\u4e26\u89e3\u9396\u5f71\u7247\u548c\u96fb\u5f71\u88fd\u4f5c\u7684\u6f5b\u529b\u3002\n\u96d6\u7136\u6700\u8fd1\u7684\u65b9\u6cd5\u4f7f\u7528\u9ad8\u54c1\u8cea\u8a13\u7df4\u8cc7\u6599\u7522\u751f\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7d50\u679c\uff0c\n\u4f46\u9019\u4e9b\u8cc7\u6599\u96c6\u96e3\u4ee5\u53d6\u5f97\uff0c\u963b\u7919\u4e86\u516c\u5e73\u548c\u900f\u660e\u7684\u57fa\u6e96\u6e2c\u8a66\u3002\n\u6b64\u5916\uff0c\u9019\u4e9b\u65b9\u6cd5\u512a\u5148\u8003\u616e 2D \u4eba\u985e\u52d5\u4f5c\uff0c\n\u800c\u5ffd\u7565\u4e86\u5f71\u7247\u4e2d\u76f8\u6a5f\u52d5\u4f5c\u7684\u91cd\u8981\u6027\uff0c\u5c0e\u81f4\u63a7\u5236\u53d7\u9650\u4e14\u5f71\u7247\u7522\u751f\u4e0d\u7a69\u5b9a\u3002\n\u70ba\u4e86\u91d0\u6e05\u8a13\u7df4\u8cc7\u6599\uff0c\u6211\u5011\u63d0\u51fa HumanVid\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u91dd\u5c0d\u4eba\u985e\u5f71\u50cf\u52d5\u756b\u91cf\u8eab\u6253\u9020\u7684\u5927\u898f\u6a21\u9ad8\u54c1\u8cea\u8cc7\u6599\u96c6\uff0c\n\u7d50\u5408\u4e86\u7cbe\u5fc3\u88fd\u4f5c\u7684\u771f\u5be6\u4e16\u754c\u548c\u5408\u6210\u8cc7\u6599\u3002\n\u5c0d\u65bc\u771f\u5be6\u4e16\u754c\u8cc7\u6599\uff0c\u6211\u5011\u5f9e\u7db2\u8def\u4e0a\u7de8\u5236\u4e86\u5927\u91cf\u514d\u7248\u6b0a\u7684\u771f\u5be6\u4e16\u754c\u5f71\u7247\u3002\n\u900f\u904e\u7cbe\u5fc3\u8a2d\u8a08\u7684\u57fa\u65bc\u898f\u5247\u7684\u904e\u6ffe\u7b56\u7565\uff0c\u6211\u5011\u78ba\u4fdd\u7d0d\u5165\u9ad8\u54c1\u8cea\u5f71\u7247\uff0c\n\u7522\u751f\u4e86 1080P \u89e3\u6790\u5ea6\u7684 20K \u4ee5\u4eba\u985e\u70ba\u4e2d\u5fc3\u7684\u5f71\u7247\u96c6\u3002\n\u4eba\u985e\u548c\u76f8\u6a5f\u52d5\u4f5c\u8a3b\u89e3\u662f\u4f7f\u7528 2D \u59ff\u52e2\u4f30\u8a08\u5668\u548c\u57fa\u65bc SLAM \u7684\u65b9\u6cd5\u5b8c\u6210\u7684\u3002\n\u5c0d\u65bc\u5408\u6210\u8cc7\u6599\uff0c\u6211\u5011\u6536\u96c6\u4e86 2,300 \u500b\u514d\u7248\u6b0a\u7684 3D \u982d\u50cf\u8cc7\u7522\u4f86\u64f4\u5145\u73fe\u6709\u7684 3D \u8cc7\u7522\u3002\n\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u5f15\u5165\u4e86\u57fa\u65bc\u898f\u5247\u7684\u76f8\u6a5f\u8ecc\u8de1\u7522\u751f\u65b9\u6cd5\uff0c\n\u4f7f\u5408\u6210\u7ba1\u7dda\u80fd\u5920\u7d0d\u5165\u591a\u6a23\u4e14\u7cbe\u78ba\u7684\u76f8\u6a5f\u52d5\u4f5c\u8a3b\u89e3\uff0c\u9019\u5728\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u4e2d\u5f88\u5c11\u898b\u3002\n\u70ba\u4e86\u9a57\u8b49 HumanVid \u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u4e00\u500b\u540d\u70ba CamAnimate \u7684\u57fa\u6e96\u6a21\u578b\uff0c\u7c21\u7a31 Camera-controllable Human Animation\uff0c\n\u5c07\u4eba\u985e\u548c\u76f8\u6a5f\u52d5\u4f5c\u90fd\u8996\u70ba\u689d\u4ef6\u3002\n\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86\u5728\u6211\u5011\u7684 HumanVid \u4e0a\u9032\u884c\u9019\u7a2e\u7c21\u55ae\u7684\u57fa\u6e96\u8a13\u7df4\uff0c\n\u5728\u63a7\u5236\u4eba\u985e\u59ff\u52e2\u548c\u76f8\u6a5f\u52d5\u4f5c\u65b9\u9762\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u6a39\u7acb\u4e86\u65b0\u7684\u57fa\u6e96\u3002\n\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u5c07\u5728 \\url{https://github.com/zhenzhiwang/HumanVid/} \u516c\u958b\u3002", "author": "Zhenzhi Wang et.al.", "authors": "Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, Dahua Lin", "id": "2407.17438v1", "paper_url": "http://arxiv.org/abs/2407.17438v1", "repo": "https://github.com/zhenzhiwang/humanvid"}}