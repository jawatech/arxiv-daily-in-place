{"2502.06733": {"publish_time": "2025-02-10", "title": "Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining", "paper_summary": "Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.", "paper_summary_zh": "<paragraph>\u5728\u9f90\u5927\u4e14\u7570\u8cea\u7684\u8cc7\u6599\u96c6\u4e0a\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\n\u5c0d\u65bc\u5728\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u4e2d\u9054\u6210\u6700\u5148\u9032\u7684\u6548\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u8a13\u7df4\u7bc4\u4f8b\u5c07\u6240\u6709\u7bc4\u4f8b\u8996\u70ba\u540c\u7b49\u91cd\u8981\uff0c\n\u5ffd\u7565\u4e86\u500b\u5225\u7bc4\u4f8b\u5728\u6574\u500b\u8a13\u7df4\u904e\u7a0b\u4e2d\u7684\u91cd\u8981\u6027\u6216\u76f8\u95dc\u6027\u3002\u73fe\u6709\u7684\u91cd\u65b0\u52a0\u6b0a\u7b56\u7565\u4e3b\u8981\u95dc\u6ce8\n\u7fa4\u7d44\u5c64\u7d1a\u8cc7\u6599\u7684\u91cd\u8981\u6027\uff0c\u7121\u6cd5\u5229\u7528\u7d30\u7dfb\u7684\u500b\u9ad4\u5c64\u7d1a\u8cc7\u8a0a\uff0c\u800c\u4e14\u7121\u6cd5\u5728\u8a13\u7df4\u9032\u884c\u6642\u52d5\u614b\u8abf\u6574\u500b\u5225\u7bc4\u4f8b\u7684\u91cd\u8981\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u7528\u65bc\u52d5\u614b\u3001\u500b\u9ad4\u5c64\u7d1a\u8cc7\u6599\u91cd\u65b0\u52a0\u6b0a\u7684\u65b0\u6f14\u7b97\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8 LLM \u9810\u8a13\u7df4\u7684\u6548\u7387\u548c\u6548\u80fd\u3002\u6211\u5011\u7684\n\u65b9\u6cd5\u6839\u64da\u7dda\u4e0a\u65b9\u5f0f\u7684\u640d\u5931\u503c\u8abf\u6574\u6bcf\u500b\u8a13\u7df4\u7bc4\u4f8b\u7684\u6b0a\u91cd\uff0c\u8b93\u6a21\u578b\u53ef\u4ee5\u5728\u76ee\u524d\u7684\u8a13\u7df4\u968e\u6bb5\u52d5\u614b\u5c08\u6ce8\u65bc\u66f4\u591a\u8cc7\u8a0a\u8c50\u5bcc\u6216\u91cd\u8981\u7684\u7bc4\u4f8b\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u7684\u67b6\u69cb\u8b93\u6211\u5011\u53ef\u4ee5\u7cfb\u7d71\u6027\u5730\u8a2d\u8a08\u91cd\u65b0\u52a0\u6b0a\u7b56\u7565\uff0c\u964d\u4f4e\u512a\u5148\u9806\u5e8f\u8f03\u4f4e\u7684\u5197\u9918\u6216\u975e\u8cc7\u8a0a\u8c50\u5bcc\u8cc7\u6599\uff0c\u6211\u5011\u767c\u73fe\u9019\u7a2e\u65b9\u6cd5\u901a\u5e38\u6700\u6709\u6548\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u65b0\u7684\u7406\u8ad6\u67b6\u69cb\u4f86\u5206\u6790\u57fa\u65bc\u640d\u5931\u7684\u91cd\u65b0\u52a0\u6b0a\u5c0d\u57fa\u65bc\u68af\u5ea6\u7684\u6700\u4f73\u5316\u7684\u6536\u6582\u7684\u5f71\u97ff\uff0c\u9996\u6b21\u6b63\u5f0f\u63cf\u8ff0\u4e86\u9019\u4e9b\u7b56\u7565\u5982\u4f55\u5f71\u97ff\u6536\u6582\u754c\u7dda\u3002\u6211\u5011\u900f\u904e\u5404\u7a2e\u4efb\u52d9\u9a57\u8b49\u4e86\u6211\u5011\u7684\u505a\u6cd5\uff0c\u5f9e\u9810\u8a13\u7df4 7B \u548c 1.4B \u53c3\u6578\u7684 LLM \u5230\u5c0f\u898f\u6a21\u8a9e\u8a00\u6a21\u578b\u548c\u7dda\u6027\u56de\u6b78\u554f\u984c\uff0c\u8b49\u660e\u4e86\u6211\u5011\u7684\u57fa\u65bc\u640d\u5931\u7684\u91cd\u65b0\u52a0\u6b0a\u65b9\u6cd5\u53ef\u4ee5\u52a0\u901f\u6536\u6582\u4e26\u5927\u5e45\u6539\u5584\u6548\u80fd\u3002</paragraph>", "author": "Daouda Sow et.al.", "authors": "Daouda Sow, Herbert Woisetschl\u00e4ger, Saikiran Bulusu, Shiqiang Wang, Hans-Arno Jacobsen, Yingbin Liang", "id": "2502.06733v1", "paper_url": "http://arxiv.org/abs/2502.06733v1", "repo": "null"}}