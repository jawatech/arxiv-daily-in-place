{"2502.13964": {"publish_time": "2025-02-19", "title": "A Training-Free Framework for Precise Mobile Manipulation of Small Everyday Objects", "paper_summary": "Many everyday mobile manipulation tasks require precise interaction with\nsmall objects, such as grasping a knob to open a cabinet or pressing a light\nswitch. In this paper, we develop Servoing with Vision Models (SVM), a\nclosed-loop training-free framework that enables a mobile manipulator to tackle\nsuch precise tasks involving the manipulation of small objects. SVM employs an\nRGB-D wrist camera and uses visual servoing for control. Our novelty lies in\nthe use of state-of-the-art vision models to reliably compute 3D targets from\nthe wrist image for diverse tasks and under occlusion due to the end-effector.\nTo mitigate occlusion artifacts, we employ vision models to out-paint the\nend-effector thereby significantly enhancing target localization. We\ndemonstrate that aided by out-painting methods, open-vocabulary object\ndetectors can serve as a drop-in module to identify semantic targets (e.g.\nknobs) and point tracking methods can reliably track interaction sites\nindicated by user clicks. This training-free method obtains an 85% zero-shot\nsuccess rate on manipulating unseen objects in novel environments in the real\nworld, outperforming an open-loop control method and an imitation learning\nbaseline trained on 1000+ demonstrations by an absolute success rate of 50%.", "paper_summary_zh": "\u8a31\u591a\u65e5\u5e38\u884c\u52d5\u64cd\u4f5c\u4efb\u52d9\u9700\u8981\u8207\u5c0f\u7269\u4ef6\u7cbe\u78ba\u4e92\u52d5\uff0c\u4f8b\u5982\u63e1\u4f4f\u65cb\u9215\u6253\u958b\u6ac3\u5b50\u6216\u6309\u4e0b\u71c8\u958b\u95dc\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u958b\u767c\u4e86\u5177\u5099\u8996\u89ba\u6a21\u578b\u7684\u4f3a\u670d\u63a7\u5236 (SVM)\uff0c\u9019\u662f\u4e00\u500b\u9589\u8ff4\u8def\u514d\u8a13\u7df4\u67b6\u69cb\uff0c\u53ef\u8b93\u884c\u52d5\u64cd\u4f5c\u5668\u8655\u7406\u6b64\u985e\u7cbe\u78ba\u4efb\u52d9\uff0c\u5305\u62ec\u64cd\u4f5c\u5c0f\u7269\u4ef6\u3002SVM \u63a1\u7528 RGB-D \u624b\u8155\u76f8\u6a5f\uff0c\u4e26\u4f7f\u7528\u8996\u89ba\u4f3a\u670d\u9032\u884c\u63a7\u5236\u3002\u6211\u5011\u7684\u5275\u65b0\u4e4b\u8655\u5728\u65bc\u4f7f\u7528\u6700\u5148\u9032\u7684\u8996\u89ba\u6a21\u578b\uff0c\u5f9e\u624b\u8155\u5f71\u50cf\u4e2d\u53ef\u9760\u5730\u8a08\u7b97\u51fa 3D \u76ee\u6a19\uff0c\u4ee5\u61c9\u4ed8\u5404\u7a2e\u4efb\u52d9\uff0c\u4e26\u5728\u672b\u7aef\u57f7\u884c\u5668\u9020\u6210\u906e\u64cb\u7684\u60c5\u6cc1\u4e0b\u9032\u884c\u3002\u70ba\u4e86\u6e1b\u8f15\u906e\u64cb\u507d\u5f71\uff0c\u6211\u5011\u63a1\u7528\u8996\u89ba\u6a21\u578b\u4f86\u5c0d\u672b\u7aef\u57f7\u884c\u5668\u9032\u884c\u5916\u7e6a\uff0c\u5f9e\u800c\u986f\u8457\u589e\u5f37\u76ee\u6a19\u5b9a\u4f4d\u3002\u6211\u5011\u8b49\u660e\uff0c\u5728\u8f14\u52a9\u5916\u7e6a\u65b9\u6cd5\u4e0b\uff0c\u958b\u653e\u5f0f\u8a5e\u5f59\u7269\u4ef6\u5075\u6e2c\u5668\u53ef\u7528\u4f5c\u63d2\u5165\u5f0f\u6a21\u7d44\uff0c\u4ee5\u8b58\u5225\u8a9e\u7fa9\u76ee\u6a19 (\u4f8b\u5982\u65cb\u9215)\uff0c\u800c\u9ede\u8ffd\u8e64\u65b9\u6cd5\u53ef\u4ee5\u53ef\u9760\u5730\u8ffd\u8e64\u4f7f\u7528\u8005\u9ede\u64ca\u6307\u793a\u7684\u4e92\u52d5\u4f4d\u7f6e\u3002\u9019\u7a2e\u514d\u8a13\u7df4\u65b9\u6cd5\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\uff0c\u5c0d\u65b0\u74b0\u5883\u4e2d\u7684\u672a\u898b\u7269\u4ef6\u9032\u884c\u64cd\u4f5c\uff0c\u7372\u5f97 85% \u7684\u96f6\u6b21\u5b78\u7fd2\u6210\u529f\u7387\uff0c\u512a\u65bc\u958b\u653e\u8ff4\u8def\u63a7\u5236\u65b9\u6cd5\u548c\u57fa\u65bc 1000 \u591a\u6b21\u793a\u7bc4\u9032\u884c\u6a21\u4eff\u5b78\u7fd2\u7684\u57fa\u6e96\uff0c\u5f8c\u8005\u7684\u7d55\u5c0d\u6210\u529f\u7387\u70ba 50%\u3002", "author": "Arjun Gupta et.al.", "authors": "Arjun Gupta, Rishik Sathua, Saurabh Gupta", "id": "2502.13964v1", "paper_url": "http://arxiv.org/abs/2502.13964v1", "repo": "null"}}