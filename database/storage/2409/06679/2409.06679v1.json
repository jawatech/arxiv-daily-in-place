{"2409.06679": {"publish_time": "2024-09-10", "title": "E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning", "paper_summary": "In the realm of Large Language Models (LLMs), the ability to process long\ncontexts is increasingly crucial for tasks such as multi-round dialogues, code\ngeneration, and document summarization. This paper addresses the challenges of\nenhancing the long-context performance, reducing computational complexity, and\nleveraging pretrained models collectively termed the \"impossible triangle.\" We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. The method involves splitting long\ncontexts into chunks, compressing each into embedding vectors via a pretrained\ntext encoder, and utilizing an adapter to align these representations with a\ndecoder-only LLM. Two training objectives, focusing on reconstruction of the\nencoder output and long-context instruction fine-tuning, are employed to\nfacilitate the understanding of soft prompts by the LLM. Experimental results\ndemonstrate that E2LLM achieves superior performance in long-context scenarios\nwhile balancing efficiency, performance, and compatibility with pretrained\nmodels. Our framework thus represents a significant advancement in the field,\ncontributing to effective long-text modeling.", "paper_summary_zh": "\u5728\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u9818\u57df\u4e2d\uff0c\u8655\u7406\u9577\u8a9e\u5883\u7684\u80fd\u8010\u5c0d\u65bc\u591a\u8f2a\u5c0d\u8a71\u3001\u7a0b\u5f0f\u78bc\u7522\u751f\u548c\u6587\u4ef6\u6458\u8981\u7b49\u4efb\u52d9\u8d8a\u4f86\u8d8a\u91cd\u8981\u3002\u672c\u6587\u63a2\u8a0e\u4e86\u589e\u5f37\u9577\u8a9e\u5883\u6548\u80fd\u3001\u964d\u4f4e\u8a08\u7b97\u8907\u96dc\u5ea6\u548c\u904b\u7528\u9810\u8a13\u7df4\u6a21\u578b\u7684\u6311\u6230\uff0c\u9019\u4e9b\u6311\u6230\u7d71\u7a31\u70ba\u300c\u4e0d\u53ef\u80fd\u4e09\u89d2\u300d\u3002\u6211\u5011\u5f15\u5165\u4e86 E2LLM\uff08\u7de8\u78bc\u5668\u5ef6\u4f38\u5927\u8a9e\u8a00\u6a21\u578b\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u6c7a\u9019\u500b\u6096\u8ad6\u3002\u6b64\u65b9\u6cd5\u5305\u62ec\u5c07\u9577\u8a9e\u5883\u5206\u5272\u6210\u5340\u584a\uff0c\u900f\u904e\u9810\u8a13\u7df4\u6587\u672c\u7de8\u78bc\u5668\u5c07\u6bcf\u500b\u5340\u584a\u58d3\u7e2e\u6210\u5d4c\u5165\u5411\u91cf\uff0c\u4e26\u4f7f\u7528\u9069\u914d\u5668\u5c07\u9019\u4e9b\u8868\u793a\u8207\u50c5\u89e3\u78bc\u5668 LLM \u5c0d\u9f4a\u3002\u5169\u500b\u8a13\u7df4\u76ee\u6a19\uff0c\u5c08\u6ce8\u65bc\u91cd\u5efa\u7de8\u78bc\u5668\u8f38\u51fa\u548c\u9577\u8a9e\u5883\u6307\u4ee4\u5fae\u8abf\uff0c\u7528\u65bc\u4fc3\u9032 LLM \u5c0d\u8edf\u63d0\u793a\u7684\u7406\u89e3\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cE2LLM \u5728\u9577\u8a9e\u5883\u5834\u666f\u4e2d\u5be6\u73fe\u4e86\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u540c\u6642\u5e73\u8861\u4e86\u6548\u7387\u3001\u6548\u80fd\u548c\u8207\u9810\u8a13\u7df4\u6a21\u578b\u7684\u76f8\u5bb9\u6027\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7684\u67b6\u69cb\u4ee3\u8868\u4e86\u8a72\u9818\u57df\u7684\u91cd\u5927\u9032\u5c55\uff0c\u6709\u52a9\u65bc\u6709\u6548\u7684\u9577\u6587\u672c\u5efa\u6a21\u3002", "author": "Zihan Liao et.al.", "authors": "Zihan Liao, Jun Wang, Hang Yu, Lingxiao Wei, Jianguo Li, Jun Wang, Wei Zhang", "id": "2409.06679v1", "paper_url": "http://arxiv.org/abs/2409.06679v1", "repo": "null"}}