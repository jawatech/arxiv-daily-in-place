{"2410.01744": {"publish_time": "2024-10-02", "title": "LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks", "paper_summary": "Text-rich images, where text serves as the central visual element guiding the\noverall understanding, are prevalent in real-world applications, such as\npresentation slides, scanned documents, and webpage snapshots. Tasks involving\nmultiple text-rich images are especially challenging, as they require not only\nunderstanding the content of individual images but reasoning about\ninter-relationships and logical flows across multiple visual inputs. Despite\nthe importance of these scenarios, current multimodal large language models\n(MLLMs) struggle to handle such tasks due to two key challenges: (1) the\nscarcity of high-quality instruction tuning datasets for text-rich multi-image\nscenarios, and (2) the difficulty in balancing image resolution with visual\nfeature sequence length. To address these challenges, we propose \\OurMethod, a\nMLLM designed specifically for handling vision-language tasks involving\nmultiple text-rich images. First, we curated about one million high-quality\nmultimodal instruction-tuning data, tailored to text-rich, multi-image\nscenarios. Second, we developed an adaptive high-resolution multi-image\nencoding module to dynamically optimize the allocation of visual sequence\nlength based on the original aspect ratios and resolutions of the input images.\nExperiments across a wide range of benchmarks demonstrate our model's superior\ncapabilities in text-rich, multi-image evaluations and competitive performance\nin general domain evaluations.", "paper_summary_zh": "\u6587\u5b57\u8c50\u5bcc\u7684\u5f71\u50cf\uff0c\u5176\u4e2d\u6587\u5b57\u4f5c\u70ba\u5f15\u5c0e\u6574\u9ad4\u7406\u89e3\u7684\u4e2d\u5fc3\u8996\u89ba\u5143\u7d20\uff0c\u5728\u771f\u5be6\u4e16\u754c\u7684\u61c9\u7528\u4e2d\u5f88\u5e38\u898b\uff0c\u4f8b\u5982\u7c21\u5831\u6295\u5f71\u7247\u3001\u6383\u63cf\u6587\u4ef6\u548c\u7db2\u9801\u622a\u5716\u3002\u6d89\u53ca\u591a\u500b\u6587\u5b57\u8c50\u5bcc\u5f71\u50cf\u7684\u4efb\u52d9\u7279\u5225\u5177\u6709\u6311\u6230\u6027\uff0c\u56e0\u70ba\u5b83\u5011\u4e0d\u50c5\u9700\u8981\u7406\u89e3\u500b\u5225\u5f71\u50cf\u7684\u5167\u5bb9\uff0c\u9084\u8981\u63a8\u7406\u8de8\u591a\u500b\u8996\u89ba\u8f38\u5165\u7684\u76f8\u4e92\u95dc\u4fc2\u548c\u908f\u8f2f\u6d41\u7a0b\u3002\u5118\u7ba1\u9019\u4e9b\u60c5\u5883\u5f88\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7684\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7531\u65bc\u5169\u500b\u4e3b\u8981\u6311\u6230\u800c\u96e3\u4ee5\u8655\u7406\u6b64\u985e\u4efb\u52d9\uff1a(1) \u91dd\u5c0d\u6587\u5b57\u8c50\u5bcc\u7684\u591a\u5f71\u50cf\u60c5\u5883\u7684\u9ad8\u54c1\u8cea\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u96c6\u7684\u7a00\u7f3a\u6027\uff0c\u4ee5\u53ca (2) \u5e73\u8861\u5f71\u50cf\u89e3\u6790\u5ea6\u8207\u8996\u89ba\u7279\u5fb5\u5e8f\u5217\u9577\u5ea6\u7684\u96e3\u5ea6\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 \\OurMethod\uff0c\u4e00\u7a2e\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u8655\u7406\u6d89\u53ca\u591a\u500b\u6587\u5b57\u8c50\u5bcc\u5f71\u50cf\u7684\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u7684 MLLM\u3002\u9996\u5148\uff0c\u6211\u5011\u7b56\u5283\u4e86\u7d04\u4e00\u767e\u842c\u7b46\u9ad8\u54c1\u8cea\u7684\u591a\u6a21\u614b\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\uff0c\u91dd\u5c0d\u6587\u5b57\u8c50\u5bcc\u7684\u591a\u5f71\u50cf\u60c5\u5883\u91cf\u8eab\u6253\u9020\u3002\u5176\u6b21\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u9069\u61c9\u6027\u9ad8\u89e3\u6790\u5ea6\u591a\u5f71\u50cf\u7de8\u78bc\u6a21\u7d44\uff0c\u4ee5\u6839\u64da\u8f38\u5165\u5f71\u50cf\u7684\u539f\u59cb\u9577\u5bec\u6bd4\u548c\u89e3\u6790\u5ea6\u52d5\u614b\u6700\u4f73\u5316\u8996\u89ba\u5e8f\u5217\u9577\u5ea6\u7684\u914d\u7f6e\u3002\u5728\u5ee3\u6cdb\u7684\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u7684\u6a21\u578b\u5728\u6587\u5b57\u8c50\u5bcc\u7684\u591a\u5f71\u50cf\u8a55\u4f30\u4e2d\u5177\u6709\u512a\u7570\u7684\u80fd\u529b\uff0c\u4e26\u4e14\u5728\u4e00\u822c\u9818\u57df\u8a55\u4f30\u4e2d\u5177\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002", "author": "Mengzhao Jia et.al.", "authors": "Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Meng Jiang, Dong Yu", "id": "2410.01744v1", "paper_url": "http://arxiv.org/abs/2410.01744v1", "repo": "null"}}