{"2501.17654": {"publish_time": "2025-01-29", "title": "Exploring Vision Language Models for Multimodal and Multilingual Stance Detection", "paper_summary": "Social media's global reach amplifies the spread of information, highlighting\nthe need for robust Natural Language Processing tasks like stance detection\nacross languages and modalities. Prior research predominantly focuses on\ntext-only inputs, leaving multimodal scenarios, such as those involving both\nimages and text, relatively underexplored. Meanwhile, the prevalence of\nmultimodal posts has increased significantly in recent years. Although\nstate-of-the-art Vision-Language Models (VLMs) show promise, their performance\non multimodal and multilingual stance detection tasks remains largely\nunexamined. This paper evaluates state-of-the-art VLMs on a newly extended\ndataset covering seven languages and multimodal inputs, investigating their use\nof visual cues, language-specific performance, and cross-modality interactions.\nOur results show that VLMs generally rely more on text than images for stance\ndetection and this trend persists across languages. Additionally, VLMs rely\nsignificantly more on text contained within the images than other visual\ncontent. Regarding multilinguality, the models studied tend to generate\nconsistent predictions across languages whether they are explicitly\nmultilingual or not, although there are outliers that are incongruous with\nmacro F1, language support, and model size.", "paper_summary_zh": "\u793e\u7fa4\u5a92\u9ad4\u7684\u5168\u7403\u5f71\u97ff\u529b\u64f4\u5927\u4e86\u8cc7\u8a0a\u7684\u50b3\u64ad\uff0c\u7a81\u986f\u4e86\u5c0d\u5065\u5168\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u7684\u9700\u6c42\uff0c\u4f8b\u5982\u8de8\u8a9e\u8a00\u548c\u6a21\u614b\u7684\u7acb\u5834\u5075\u6e2c\u3002\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7d14\u6587\u5b57\u8f38\u5165\u4e0a\uff0c\u800c\u5c07\u6d89\u53ca\u5f71\u50cf\u548c\u6587\u5b57\u7684\u591a\u6a21\u614b\u5834\u666f\u7559\u4f5c\u76f8\u5c0d\u672a\u7d93\u63a2\u7d22\u7684\u9818\u57df\u3002\u540c\u6642\uff0c\u8fd1\u5e74\u4f86\u591a\u6a21\u614b\u8cbc\u6587\u7684\u76db\u884c\u5df2\u5927\u5e45\u589e\u52a0\u3002\u5118\u7ba1\u6700\u5148\u9032\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u986f\u793a\u51fa\u524d\u666f\uff0c\u4f46\u5b83\u5011\u5728\u591a\u6a21\u614b\u548c\u591a\u8a9e\u8a00\u7acb\u5834\u5075\u6e2c\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u4ecd\u672a\u7d93\u904e\u5ee3\u6cdb\u6aa2\u9a57\u3002\u672c\u6587\u8a55\u4f30\u4e86\u6700\u5148\u9032\u7684 VLM\uff0c\u63a1\u7528\u4e00\u500b\u65b0\u64f4\u5145\u7684\u8cc7\u6599\u96c6\uff0c\u6db5\u84cb\u4e03\u7a2e\u8a9e\u8a00\u548c\u591a\u6a21\u614b\u8f38\u5165\uff0c\u63a2\u8a0e\u5b83\u5011\u5c0d\u8996\u89ba\u7dda\u7d22\u7684\u4f7f\u7528\u3001\u7279\u5b9a\u8a9e\u8a00\u7684\u8868\u73fe\u4ee5\u53ca\u8de8\u6a21\u614b\u4e92\u52d5\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0cVLM \u4e00\u822c\u66f4\u4f9d\u8cf4\u6587\u5b57\u800c\u975e\u5f71\u50cf\u9032\u884c\u7acb\u5834\u5075\u6e2c\uff0c\u800c\u4e14\u9019\u7a2e\u8da8\u52e2\u5728\u5404\u8a9e\u8a00\u9593\u6301\u7e8c\u5b58\u5728\u3002\u6b64\u5916\uff0cVLM \u66f4\u986f\u8457\u5730\u4f9d\u8cf4\u5f71\u50cf\u4e2d\u5305\u542b\u7684\u6587\u5b57\uff0c\u800c\u975e\u5176\u4ed6\u8996\u89ba\u5167\u5bb9\u3002\u95dc\u65bc\u591a\u8a9e\u8a00\u6027\uff0c\u6240\u7814\u7a76\u7684\u6a21\u578b\u50be\u5411\u65bc\u5728\u5404\u8a9e\u8a00\u9593\u7522\u751f\u4e00\u81f4\u7684\u9810\u6e2c\uff0c\u7121\u8ad6\u5b83\u5011\u662f\u5426\u660e\u78ba\u5730\u652f\u63f4\u591a\u8a9e\u8a00\uff0c\u5118\u7ba1\u6709\u4e00\u4e9b\u7570\u5e38\u503c\u8207\u5de8\u89c0 F1\u3001\u8a9e\u8a00\u652f\u63f4\u548c\u6a21\u578b\u5927\u5c0f\u4e0d\u4e00\u81f4\u3002", "author": "Jake Vasilakes et.al.", "authors": "Jake Vasilakes, Carolina Scarton, Zhixue Zhao", "id": "2501.17654v1", "paper_url": "http://arxiv.org/abs/2501.17654v1", "repo": "null"}}