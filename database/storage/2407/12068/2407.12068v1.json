{"2407.12068": {"publish_time": "2024-07-16", "title": "Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing tasks. Recently, several LLMs-based\npipelines have been developed to enhance learning on graphs with text\nattributes, showcasing promising performance. However, graphs are well-known to\nbe susceptible to adversarial attacks and it remains unclear whether LLMs\nexhibit robustness in learning on graphs. To address this gap, our work aims to\nexplore the potential of LLMs in the context of adversarial attacks on graphs.\nSpecifically, we investigate the robustness against graph structural and\ntextual perturbations in terms of two dimensions: LLMs-as-Enhancers and\nLLMs-as-Predictors. Through extensive experiments, we find that, compared to\nshallow models, both LLMs-as-Enhancers and LLMs-as-Predictors offer superior\nrobustness against structural and textual attacks.Based on these findings, we\ncarried out additional analyses to investigate the underlying causes.\nFurthermore, we have made our benchmark library openly available to facilitate\nquick and fair evaluations, and to encourage ongoing innovative research in\nthis field.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u4e86\u5353\u8d8a\u7684\u6548\u80fd\u3002\u6700\u8fd1\uff0c\u5df2\u7d93\u958b\u767c\u4e86\u591a\u500b\u57fa\u65bc LLM \u7684\u7ba1\u9053\uff0c\u4ee5\u589e\u5f37\u5716\u5f62\u4e0a\u5177\u6709\u6587\u5b57\u5c6c\u6027\u7684\u5b78\u7fd2\uff0c\u5c55\u793a\u51fa\u6709\u524d\u666f\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u773e\u6240\u5468\u77e5\uff0c\u5716\u5f62\u5bb9\u6613\u53d7\u5230\u5c0d\u6297\u6027\u653b\u64ca\uff0c\u800c LLM \u5728\u5716\u5f62\u5b78\u7fd2\u4e2d\u662f\u5426\u8868\u73fe\u51fa\u7a69\u5065\u6027\u4ecd\u4e0d\u6e05\u695a\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u7684\u7814\u7a76\u65e8\u5728\u63a2\u7d22 LLM \u5728\u5716\u5f62\u5c0d\u6297\u653b\u64ca\u4e2d\u7684\u6f5b\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7814\u7a76\u4e86\u5728 LLM \u4f5c\u70ba\u589e\u5f37\u5668\u548c LLM \u4f5c\u70ba\u9810\u6e2c\u5668\u7684\u5169\u500b\u9762\u5411\u4e2d\uff0c\u91dd\u5c0d\u5716\u5f62\u7d50\u69cb\u548c\u6587\u5b57\u64fe\u52d5\u7684\u7a69\u5065\u6027\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u767c\u73fe\uff0c\u8207\u6dfa\u5c64\u6a21\u578b\u76f8\u6bd4\uff0c\u4f5c\u70ba\u589e\u5f37\u5668\u7684 LLM \u548c\u4f5c\u70ba\u9810\u6e2c\u5668\u7684 LLM \u90fd\u5c0d\u7d50\u69cb\u548c\u6587\u5b57\u653b\u64ca\u63d0\u4f9b\u4e86\u512a\u7570\u7684\u7a69\u5065\u6027\u3002\u57fa\u65bc\u9019\u4e9b\u767c\u73fe\uff0c\u6211\u5011\u9032\u884c\u4e86\u984d\u5916\u7684\u5206\u6790\u4f86\u63a2\u8a0e\u5176\u6839\u672c\u539f\u56e0\u3002\u6b64\u5916\uff0c\u6211\u5011\u5df2\u7d93\u516c\u958b\u4e86\u6211\u5011\u7684\u57fa\u6e96\u5eab\uff0c\u4ee5\u5229\u65bc\u5feb\u901f\u4e14\u516c\u5e73\u7684\u8a55\u4f30\uff0c\u4e26\u9f13\u52f5\u5728\u9019\u500b\u9818\u57df\u9032\u884c\u6301\u7e8c\u7684\u5275\u65b0\u7814\u7a76\u3002", "author": "Kai Guo et.al.", "authors": "Kai Guo, Zewen Liu, Zhikai Chen, Hongzhi Wen, Wei Jin, Jiliang Tang, Yi Chang", "id": "2407.12068v1", "paper_url": "http://arxiv.org/abs/2407.12068v1", "repo": "null"}}