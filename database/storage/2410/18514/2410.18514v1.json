{"2410.18514": {"publish_time": "2024-10-24", "title": "Scaling up Masked Diffusion Models on Text", "paper_summary": "Masked diffusion models (MDMs) have shown promise in language modeling, yet\ntheir scalability and effectiveness in core language tasks, such as text\ngeneration and language understanding, remain underexplored. This paper\nestablishes the first scaling law for MDMs, demonstrating a scaling rate\ncomparable to autoregressive models (ARMs) and a relatively small compute gap.\nMotivated by their scalability, we train a family of MDMs with up to 1.1\nbillion (B) parameters to systematically evaluate their performance against\nARMs of comparable or larger sizes. Fully leveraging the probabilistic\nformulation of MDMs, we propose a simple yet effective \\emph{unsupervised\nclassifier-free guidance} that effectively exploits large-scale unpaired data,\nboosting performance for conditional inference. In language understanding, a\n1.1B MDM shows competitive results, outperforming the larger 1.5B GPT-2 model\non four out of eight zero-shot benchmarks. In text generation, MDMs provide a\nflexible trade-off compared to ARMs utilizing KV-cache: MDMs match the\nperformance of ARMs while being 1.4 times faster, or achieve higher quality\nthan ARMs at a higher computational cost. Moreover, MDMs address challenging\ntasks for ARMs by effectively handling bidirectional reasoning and adapting to\ntemporal shifts in data. Notably, a 1.1B MDM breaks the \\emph{reverse curse}\nencountered by much larger ARMs with significantly more data and computation,\nsuch as Llama-2 (13B) and GPT-3 (175B). Our code is available at\n\\url{https://github.com/ML-GSAI/SMDM}.", "paper_summary_zh": "<paragraph>\u8499\u9762\u64f4\u6563\u6a21\u578b\uff08MDM\uff09\u5728\u8a9e\u8a00\u6a21\u578b\u4e2d\u5c55\u73fe\u51fa\u524d\u666f\uff0c\u4f46\u5176\u5728\u6838\u5fc3\u8a9e\u8a00\u4efb\u52d9\uff08\u4f8b\u5982\u6587\u5b57\u751f\u6210\u548c\u8a9e\u8a00\u7406\u89e3\uff09\u4e2d\u7684\u53ef\u64f4\u5145\u6027\u548c\u6709\u6548\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u672c\u6587\u5efa\u7acb\u4e86 MDMS \u7684\u7b2c\u4e00\u500b\u64f4\u5145\u5b9a\u5f8b\uff0c\u8b49\u660e\u4e86\u8207\u81ea\u8ff4\u6b78\u6a21\u578b\uff08ARM\uff09\u76f8\u7576\u7684\u64f4\u5145\u7387\u548c\u76f8\u5c0d\u8f03\u5c0f\u7684\u904b\u7b97\u5dee\u8ddd\u3002\u5728\u53ef\u64f4\u5145\u6027\u7684\u6fc0\u52f5\u4e0b\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b\u64c1\u6709\u9ad8\u9054 11 \u5104\uff08B\uff09\u53c3\u6578\u7684 MDM \u65cf\uff0c\u4ee5\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u5176\u76f8\u7b49\u6216\u66f4\u5927\u898f\u6a21 ARM \u7684\u6548\u80fd\u3002\u5145\u5206\u5229\u7528 MDM \u7684\u6a5f\u7387\u516c\u5f0f\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u300c\u7121\u76e3\u7763\u5206\u985e\u5668\u514d\u8cbb\u6307\u5c0e\u300d\uff0c\u6709\u6548\u5730\u5229\u7528\u4e86\u5927\u898f\u6a21\u672a\u914d\u5c0d\u8cc7\u6599\uff0c\u63d0\u5347\u4e86\u689d\u4ef6\u5f0f\u63a8\u8ad6\u7684\u6548\u80fd\u3002\u5728\u8a9e\u8a00\u7406\u89e3\u65b9\u9762\uff0c1.1B MDM \u986f\u793a\u51fa\u6709\u7af6\u722d\u529b\u7684\u7d50\u679c\uff0c\u5728\u516b\u500b\u96f6\u6b21\u5b78\u7fd2\u57fa\u6e96\u4e2d\u7684\u56db\u500b\u4e2d\u8868\u73fe\u512a\u65bc\u8f03\u5927\u7684 1.5B GPT-2 \u6a21\u578b\u3002\u5728\u6587\u5b57\u751f\u6210\u65b9\u9762\uff0c\u8207\u4f7f\u7528 KV \u5feb\u53d6\u7684 ARM \u76f8\u6bd4\uff0cMDM \u63d0\u4f9b\u4e86\u4e00\u500b\u9748\u6d3b\u7684\u6b0a\u8861\uff1aMDM \u5728\u901f\u5ea6\u5feb 1.4 \u500d\u7684\u60c5\u6cc1\u4e0b\u8207 ARM \u7684\u6548\u80fd\u76f8\u5339\u914d\uff0c\u6216\u4ee5\u8f03\u9ad8\u7684\u904b\u7b97\u6210\u672c\u5be6\u73fe\u6bd4 ARM \u66f4\u9ad8\u7684\u54c1\u8cea\u3002\u6b64\u5916\uff0cMDM \u900f\u904e\u6709\u6548\u8655\u7406\u96d9\u5411\u63a8\u7406\u548c\u9069\u61c9\u8cc7\u6599\u4e2d\u7684\u6642\u9593\u8b8a\u5316\uff0c\u89e3\u6c7a\u4e86 ARM \u7684\u6311\u6230\u6027\u4efb\u52d9\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c1.1B MDM \u6253\u7834\u4e86\u898f\u6a21\u5927\u5f97\u591a\u7684 ARM \u6240\u906d\u9047\u7684\u300c\u53cd\u5411\u8a5b\u5492\u300d\uff0c\u5f8c\u8005\u64c1\u6709\u66f4\u591a\u8cc7\u6599\u548c\u904b\u7b97\uff0c\u4f8b\u5982 Llama-2\uff0813B\uff09\u548c GPT-3\uff08175B\uff09\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728\\url{https://github.com/ML-GSAI/SMDM}\u53d6\u5f97\u3002</paragraph>", "author": "Shen Nie et.al.", "authors": "Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, Chongxuan Li", "id": "2410.18514v1", "paper_url": "http://arxiv.org/abs/2410.18514v1", "repo": "https://github.com/ml-gsai/smdm"}}