{"2405.17202": {"publish_time": "2024-05-27", "title": "Efficient multi-prompt evaluation of LLMs", "paper_summary": "Most popular benchmarks for comparing LLMs rely on a limited set of prompt\ntemplates, which may not fully capture the LLMs' abilities and can affect the\nreproducibility of results on leaderboards. Many recent works empirically\nverify prompt sensitivity and advocate for changes in LLM evaluation. In this\npaper, we consider the problem of estimating the performance distribution\nacross many prompt variants instead of finding a single prompt to evaluate\nwith. We introduce PromptEval, a method for estimating performance across a\nlarge set of prompts borrowing strength across prompts and examples to produce\naccurate estimates under practical evaluation budgets. The resulting\ndistribution can be used to obtain performance quantiles to construct various\nrobust performance metrics (e.g., top 95% quantile or median). We prove that\nPromptEval consistently estimates the performance distribution and demonstrate\nits efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench\nHard, and LMentry. For example, PromptEval can accurately estimate performance\nquantiles across 100 prompt templates on MMLU with a budget equivalent to two\nsingle-prompt evaluations. Our code and data can be found at\nhttps://github.com/felipemaiapolo/prompt-eval.", "paper_summary_zh": "\u5927\u591a\u6578\u7528\u65bc\u6bd4\u8f03 LLM \u7684\u71b1\u9580\u57fa\u6e96\u90fd\u4f9d\u8cf4\u65bc\u4e00\u7d44\u6709\u9650\u7684\u63d0\u793a\u7bc4\u672c\uff0c\u9019\u4e9b\u7bc4\u672c\u53ef\u80fd\u7121\u6cd5\u5b8c\u5168\u638c\u63e1 LLM \u7684\u80fd\u529b\uff0c\u4e26\u4e14\u6703\u5f71\u97ff\u6392\u884c\u699c\u4e0a\u7d50\u679c\u7684\u53ef\u8907\u88fd\u6027\u3002\u8a31\u591a\u8fd1\u671f\u7814\u7a76\u7d93\u9a57\u6027\u5730\u9a57\u8b49\u4e86\u63d0\u793a\u654f\u611f\u6027\uff0c\u4e26\u5021\u5c0e\u6539\u8b8a LLM \u8a55\u4f30\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8003\u616e\u4e86\u4f30\u8a08\u8a31\u591a\u63d0\u793a\u8b8a\u9ad4\u4e2d\u7684\u6548\u80fd\u5206\u4f48\u7684\u554f\u984c\uff0c\u800c\u4e0d\u662f\u627e\u5230\u4e00\u500b\u63d0\u793a\u4f86\u8a55\u4f30\u3002\u6211\u5011\u5f15\u5165\u4e86 PromptEval\uff0c\u9019\u662f\u4e00\u7a2e\u4f30\u8a08\u5927\u91cf\u63d0\u793a\u6548\u80fd\u7684\u65b9\u6cd5\uff0c\u5b83\u501f\u7528\u63d0\u793a\u548c\u7bc4\u4f8b\u7684\u529b\u91cf\uff0c\u5728\u5be6\u969b\u8a55\u4f30\u9810\u7b97\u4e0b\u7522\u751f\u6e96\u78ba\u7684\u4f30\u8a08\u3002\u7522\u751f\u7684\u5206\u4f48\u53ef\u7528\u65bc\u7372\u53d6\u6548\u80fd\u5206\u4f4d\u6578\uff0c\u4ee5\u5efa\u69cb\u5404\u7a2e\u7a69\u5065\u6548\u80fd\u6307\u6a19\uff08\u4f8b\u5982\uff0c\u524d 95% \u5206\u4f4d\u6578\u6216\u4e2d\u4f4d\u6578\uff09\u3002\u6211\u5011\u8b49\u660e PromptEval \u4e00\u81f4\u5730\u4f30\u8a08\u6548\u80fd\u5206\u4f48\uff0c\u4e26\u5728\u4e09\u500b\u8457\u540d\u7684 LLM \u57fa\u6e96\u4e0a\u5be6\u8b49\u5c55\u793a\u5176\u6548\u529b\uff1aMMLU\u3001BIG-bench Hard \u548c LMentry\u3002\u4f8b\u5982\uff0cPromptEval \u53ef\u4ee5\u6e96\u78ba\u4f30\u8a08 MMLU \u4e0a 100 \u500b\u63d0\u793a\u7bc4\u672c\u7684\u6548\u80fd\u5206\u4f4d\u6578\uff0c\u5176\u9810\u7b97\u7b49\u65bc\u5169\u500b\u55ae\u4e00\u63d0\u793a\u8a55\u4f30\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u4ee5\u5728 https://github.com/felipemaiapolo/prompt-eval \u627e\u5230\u3002", "author": "Felipe Maia Polo et.al.", "authors": "Felipe Maia Polo, Ronald Xu, Lucas Weber, M\u00edrian Silva, Onkar Bhardwaj, Leshem Choshen, Allysson Flavio Melo de Oliveira, Yuekai Sun, Mikhail Yurochkin", "id": "2405.17202v1", "paper_url": "http://arxiv.org/abs/2405.17202v1", "repo": "null"}}