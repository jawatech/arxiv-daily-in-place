{"2406.08334": {"publish_time": "2024-06-12", "title": "ProTrain: Efficient LLM Training via Memory-Aware Techniques", "paper_summary": "It is extremely memory-hungry to train Large Language Models (LLM). To solve\nthis problem, existing work exploits the combination of CPU and GPU for the\ntraining process, such as ZeRO-Offload. Such a technique largely democratizes\nbillion-scale model training, making it possible to train with few consumer\ngraphics cards. However, based on our observation, existing frameworks often\nprovide coarse-grained memory management and require experienced experts in\nconfiguration tuning, leading to suboptimal hardware utilization and\nperformance. This paper proposes ProTrain, a novel training system that\nintelligently balances memory usage and performance by coordinating memory,\ncomputation, and IO. ProTrain achieves adaptive memory management through\nChunk-Based Model State Management and Block-Wise Activation Management, guided\nby a Memory-Aware Runtime Profiler without user intervention. ProTrain does not\nchange the training algorithm and thus does not compromise accuracy.\nExperiments show that ProTrain improves training throughput by 1.43$\\times$ to\n2.71$\\times$ compared to the SOTA training systems.", "paper_summary_zh": "\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6975\u5ea6\u8017\u8cbb\u8a18\u61b6\u9ad4\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u73fe\u6709\u5de5\u4f5c\u5229\u7528 CPU \u548c GPU \u7684\u7d44\u5408\u9032\u884c\u8a13\u7df4\u904e\u7a0b\uff0c\u4f8b\u5982 ZeRO-Offload\u3002\u9019\u7a2e\u6280\u8853\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u6c11\u4e3b\u5316\u4e86\u5341\u5104\u898f\u6a21\u7684\u6a21\u578b\u8a13\u7df4\uff0c\u8b93\u4f7f\u7528\u5c11\u6578\u6d88\u8cbb\u8005\u7d1a\u986f\u793a\u5361\u9032\u884c\u8a13\u7df4\u6210\u70ba\u53ef\u80fd\u3002\u7136\u800c\uff0c\u6839\u64da\u6211\u5011\u7684\u89c0\u5bdf\uff0c\u73fe\u6709\u6846\u67b6\u901a\u5e38\u63d0\u4f9b\u7c97\u7565\u7684\u8a18\u61b6\u9ad4\u7ba1\u7406\uff0c\u4e26\u9700\u8981\u7d93\u9a57\u8c50\u5bcc\u7684\u5c08\u5bb6\u9032\u884c\u914d\u7f6e\u8abf\u6574\uff0c\u5c0e\u81f4\u786c\u9ad4\u5229\u7528\u7387\u548c\u6548\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa ProTrain\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u8a13\u7df4\u7cfb\u7d71\uff0c\u900f\u904e\u5354\u8abf\u8a18\u61b6\u9ad4\u3001\u904b\u7b97\u548c IO\uff0c\u5728\u8a18\u61b6\u9ad4\u4f7f\u7528\u548c\u6548\u80fd\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002ProTrain \u900f\u904e\u5340\u584a\u5316\u6a21\u578b\u72c0\u614b\u7ba1\u7406\u548c\u5340\u584a\u5316\u6fc0\u6d3b\u7ba1\u7406\uff0c\u5728\u7121\u9700\u4f7f\u7528\u8005\u4ecb\u5165\u7684\u60c5\u6cc1\u4e0b\uff0c\u900f\u904e\u8a18\u61b6\u9ad4\u611f\u77e5\u57f7\u884c\u6642\u671f\u5256\u6790\u5668\uff0c\u9054\u6210\u9069\u61c9\u6027\u8a18\u61b6\u9ad4\u7ba1\u7406\u3002ProTrain \u6c92\u6709\u6539\u8b8a\u8a13\u7df4\u6f14\u7b97\u6cd5\uff0c\u56e0\u6b64\u4e0d\u6703\u5f71\u97ff\u6e96\u78ba\u6027\u3002\u5be6\u9a57\u986f\u793a\uff0c\u8207 SOTA \u8a13\u7df4\u7cfb\u7d71\u76f8\u6bd4\uff0cProTrain \u5c07\u8a13\u7df4\u8655\u7406\u91cf\u63d0\u5347\u4e86 1.43 \u500d\u81f3 2.71 \u500d\u3002", "author": "Hanmei Yang et.al.", "authors": "Hanmei Yang, Jin Zhou, Yao Fu, Xiaoqun Wang, Ramine Roane, Hui Guan, Tongping Liu", "id": "2406.08334v1", "paper_url": "http://arxiv.org/abs/2406.08334v1", "repo": "null"}}