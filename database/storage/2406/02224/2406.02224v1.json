{"2406.02224": {"publish_time": "2024-06-04", "title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models", "paper_summary": "Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios,\nheterogeneous, homogeneous, and one-to-one, we evaluate the effectiveness of\nFedMKT using various public LLMs and SLMs on a range of NLP text generation\ntasks. Empirical results demonstrate significant performance improvements in\nclients' SLMs with the aid of the LLM. Furthermore, the LLM optimized by FedMKT\nachieves a performance comparable to that achieved through direct fine-tuning\nbased on clients' data, highlighting the effectiveness and adaptability of\nFedMKT.", "paper_summary_zh": "\u8fd1\u671f\u5728\u8054\u90a6\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u7814\u7a76\u4e3b\u8981\u4e13\u6ce8\u4e8e\u8ba9\u5ba2\u6237\u7aef\u534f\u4f5c\u5fae\u8c03\u5176\u672c\u5730\u90e8\u7f72\u7684\u540c\u8d28 LLM\uff0c\u6216\u5c06\u57fa\u4e8e\u670d\u52a1\u5668\u7684 LLM \u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u4e0b\u6e38\u5ba2\u6237\u7aef\u7684\u5c0f\u8bed\u8a00\u6a21\u578b (SLM)\u3002\u7136\u800c\uff0c\u5728\u670d\u52a1\u5668\u7684 LLM \u548c\u5ba2\u6237\u7aef\u7684 SLM \u7684\u540c\u65f6\u76f8\u4e92\u589e\u5f3a\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 FedMKT\uff0c\u4e00\u4e2a\u9488\u5bf9\u5927\u578b\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u8054\u90a6\u4e92\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\u3002\u6b64\u6846\u67b6\u65e8\u5728\u81ea\u9002\u5e94\u5730\u5c06\u77e5\u8bc6\u4ece\u670d\u52a1\u5668\u7684 LLM \u8f6c\u79fb\u5230\u5ba2\u6237\u7aef\u7684 SLM\uff0c\u540c\u65f6\u7528\u5ba2\u6237\u7aef\u72ec\u7279\u7684\u9886\u57df\u89c1\u89e3\u4e30\u5bcc LLM\u3002\u6211\u4eec\u5229\u7528\u6700\u5c0f\u7f16\u8f91\u8ddd\u79bb (MinED) \u6765\u4fc3\u8fdb\u6807\u8bb0\u5bf9\u9f50\uff0c\u7136\u540e\u5728\u5ba2\u6237\u7aef SLM \u548c\u670d\u52a1\u5668\u7aef LLM \u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\u6027\u4e92\u77e5\u8bc6\u8fc1\u79fb\uff0c\u65e8\u5728\u5171\u540c\u589e\u5f3a\u5176\u6027\u80fd\u3002\u901a\u8fc7\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u573a\u666f\uff08\u5f02\u6784\u3001\u540c\u6784\u548c\u4e00\u5bf9\u4e00\uff09\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u4f7f\u7528\u5404\u79cd\u516c\u5171 LLM \u548c SLM \u8bc4\u4f30\u4e86 FedMKT \u5728\u4e00\u7cfb\u5217 NLP \u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5728 LLM \u7684\u5e2e\u52a9\u4e0b\uff0c\u5ba2\u6237\u7aef\u7684 SLM \u6027\u80fd\u6709\u4e86\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\uff0c\u901a\u8fc7 FedMKT \u4f18\u5316\u7684 LLM \u5b9e\u73b0\u7684\u6027\u80fd\u4e0e\u57fa\u4e8e\u5ba2\u6237\u7aef\u6570\u636e\u8fdb\u884c\u76f4\u63a5\u5fae\u8c03\u6240\u5b9e\u73b0\u7684\u6027\u80fd\u76f8\u5f53\uff0c\u7a81\u51fa\u4e86 FedMKT \u7684\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "author": "Tao Fan et.al.", "authors": "Tao Fan, Guoqiang Ma, Yan Kang, Hanlin Gu, Lixin Fan, Qiang Yang", "id": "2406.02224v1", "paper_url": "http://arxiv.org/abs/2406.02224v1", "repo": "null"}}