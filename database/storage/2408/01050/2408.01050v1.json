{"2408.01050": {"publish_time": "2024-08-02", "title": "The Impact of Hyperparameters on Large Language Model Inference Performance: An Evaluation of vLLM and HuggingFace Pipelines", "paper_summary": "The recent surge of open-source large language models (LLMs) enables\ndevelopers to create AI-based solutions while maintaining control over aspects\nsuch as privacy and compliance, thereby providing governance and ownership of\nthe model deployment process. To utilize these LLMs, inference engines are\nneeded. These engines load the model's weights onto available resources, such\nas GPUs, and process queries to generate responses. The speed of inference, or\nperformance, of the LLM, is critical for real-time applications, as it computes\nmillions or billions of floating point operations per inference. Recently,\nadvanced inference engines such as vLLM have emerged, incorporating novel\nmechanisms such as efficient memory management to achieve state-of-the-art\nperformance. In this paper, we analyze the performance, particularly the\nthroughput (tokens generated per unit of time), of 20 LLMs using two inference\nlibraries: vLLM and HuggingFace's pipelines. We investigate how various\nhyperparameters, which developers must configure, influence inference\nperformance. Our results reveal that throughput landscapes are irregular, with\ndistinct peaks, highlighting the importance of hyperparameter optimization to\nachieve maximum performance. We also show that applying hyperparameter\noptimization when upgrading or downgrading the GPU model used for inference can\nimprove throughput from HuggingFace pipelines by an average of 9.16% and 13.7%,\nrespectively.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u958b\u653e\u539f\u59cb\u78bc\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6fc0\u589e\uff0c\u8b93\u958b\u767c\u4eba\u54e1\u80fd\u5920\u5728\u7dad\u8b77\u96b1\u79c1\u548c\u5408\u898f\u7b49\u65b9\u9762\u4fdd\u6301\u63a7\u5236\u7684\u60c5\u6cc1\u4e0b\u5efa\u7acb\u57fa\u65bc AI \u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u5f9e\u800c\u63d0\u4f9b\u6a21\u578b\u90e8\u7f72\u6d41\u7a0b\u7684\u6cbb\u7406\u548c\u6240\u6709\u6b0a\u3002\u70ba\u4e86\u4f7f\u7528\u9019\u4e9b LLM\uff0c\u9700\u8981\u63a8\u7406\u5f15\u64ce\u3002\u9019\u4e9b\u5f15\u64ce\u5c07\u6a21\u578b\u7684\u6b0a\u91cd\u8f09\u5165\u5230\u53ef\u7528\u8cc7\u6e90\uff08\u4f8b\u5982 GPU\uff09\u4e0a\uff0c\u4e26\u8655\u7406\u67e5\u8a62\u4ee5\u7522\u751f\u56de\u61c9\u3002LLM \u7684\u63a8\u7406\u901f\u5ea6\u6216\u6548\u80fd\u5c0d\u65bc\u5373\u6642\u61c9\u7528\u7a0b\u5f0f\u81f3\u95dc\u91cd\u8981\uff0c\u56e0\u70ba\u5b83\u6bcf\u9032\u884c\u4e00\u6b21\u63a8\u7406\u5c31\u6703\u8a08\u7b97\u6578\u767e\u842c\u6216\u6578\u5341\u5104\u500b\u6d6e\u9ede\u904b\u7b97\u3002\u6700\u8fd1\uff0c\u51fa\u73fe\u4e86 vLLM \u7b49\u5148\u9032\u7684\u63a8\u7406\u5f15\u64ce\uff0c\u5b83\u7d50\u5408\u4e86\u9ad8\u6548\u8a18\u61b6\u9ad4\u7ba1\u7406\u7b49\u65b0\u6a5f\u5236\u4f86\u5be6\u73fe\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5206\u6790\u4e86\u6548\u80fd\uff0c\u7279\u5225\u662f\u4f7f\u7528\u5169\u500b\u63a8\u7406\u5eab\uff08vLLM \u548c HuggingFace \u7684\u7ba1\u9053\uff09\u7684 20 \u500b LLM \u7684\u541e\u5410\u91cf\uff08\u6bcf\u55ae\u4f4d\u6642\u9593\u7522\u751f\u7684\u4ee3\u5e63\uff09\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u958b\u767c\u4eba\u54e1\u5fc5\u9808\u914d\u7f6e\u7684\u5404\u7a2e\u8d85\u53c3\u6578\u5982\u4f55\u5f71\u97ff\u63a8\u7406\u6548\u80fd\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u541e\u5410\u91cf\u72c0\u6cc1\u662f\u4e0d\u898f\u5247\u7684\uff0c\u6709\u660e\u986f\u7684\u9ad8\u5cf0\uff0c\u9019\u7a81\u986f\u4e86\u8d85\u53c3\u6578\u6700\u4f73\u5316\u5c0d\u65bc\u9054\u6210\u6700\u5927\u6548\u80fd\u7684\u91cd\u8981\u6027\u3002\u6211\u5011\u9084\u5c55\u793a\u4e86\u5728\u5347\u7d1a\u6216\u964d\u7d1a\u7528\u65bc\u63a8\u7406\u7684 GPU \u6a21\u578b\u6642\u61c9\u7528\u8d85\u53c3\u6578\u6700\u4f73\u5316\u53ef\u4ee5\u5206\u5225\u5c07 HuggingFace \u7ba1\u9053\u7684\u541e\u5410\u91cf\u5e73\u5747\u63d0\u5347 9.16% \u548c 13.7%\u3002</paragraph>", "author": "Matias Martinez et.al.", "authors": "Matias Martinez", "id": "2408.01050v1", "paper_url": "http://arxiv.org/abs/2408.01050v1", "repo": "null"}}