{"2502.13595": {"publish_time": "2025-02-19", "title": "MMTEB: Massive Multilingual Text Embedding Benchmark", "paper_summary": "Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost.", "paper_summary_zh": "\u6587\u672c\u5d4c\u5165\u901a\u5e38\u5728\u53d7\u8bed\u8a00\u3001\u9886\u57df\u548c\u4efb\u52a1\u591a\u6837\u6027\u9650\u5236\u7684\u4e00\u7ec4\u6709\u9650\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u5e76\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6587\u672c\u5d4c\u5165\u57fa\u51c6\uff08MMTEB\uff09\u2014\u2014MTEB \u7684\u4e00\u9879\u5927\u89c4\u6a21\u3001\u793e\u533a\u9a71\u52a8\u7684\u6269\u5c55\uff0c\u6db5\u76d6\u4e86 250 \u591a\u79cd\u8bed\u8a00\u4e2d\u7684 500 \u591a\u9879\u8d28\u91cf\u53d7\u63a7\u8bc4\u4f30\u4efb\u52a1\u3002MMTEB \u5305\u542b\u4e86\u4e00\u7ec4\u5177\u6709\u6311\u6218\u6027\u7684\u65b0\u4efb\u52a1\uff0c\u4f8b\u5982\u6307\u4ee4\u9075\u5faa\u3001\u957f\u6587\u6863\u68c0\u7d22\u548c\u4ee3\u7801\u68c0\u7d22\uff0c\u4ee3\u8868\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u8bc4\u4f30\u4efb\u52a1\u96c6\u5408\u3002\u4f7f\u7528\u6b64\u96c6\u5408\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u51e0\u4e2a\u9ad8\u5ea6\u591a\u8bed\u8a00\u7684\u57fa\u51c6\uff0c\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u57fa\u51c6\u6765\u8bc4\u4f30\u4e00\u7ec4\u6709\u4ee3\u8868\u6027\u7684\u6a21\u578b\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u867d\u7136\u5177\u6709\u6570\u5341\u4ebf\u4e2a\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u5bf9\u67d0\u4e9b\u8bed\u8a00\u5b50\u96c6\u548c\u4efb\u52a1\u7c7b\u522b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u6027\u80fd\u6700\u4f73\u7684\u516c\u5f00\u53ef\u7528\u6a21\u578b\u662f\u53ea\u6709 5.6 \u4ebf\u4e2a\u53c2\u6570\u7684\u591a\u8bed\u8a00 e5 \u5927\u578b\u6307\u4ee4\u6a21\u578b\u3002\u4e3a\u4e86\u4fbf\u4e8e\u8bbf\u95ee\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u95f4\u76f8\u5173\u6027\u7684\u65b0\u964d\u91c7\u6837\u65b9\u6cd5\uff0c\u786e\u4fdd\u591a\u6837\u5316\u7684\u9009\u62e9\u540c\u65f6\u4fdd\u7559\u76f8\u5bf9\u6a21\u578b\u6392\u540d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u56f0\u96be\u7684\u8d1f\u6837\u672c\u8fdb\u884c\u91c7\u6837\u6765\u4f18\u5316\u68c0\u7d22\u7b49\u4efb\u52a1\uff0c\u521b\u5efa\u66f4\u5c0f\u4f46\u6709\u6548\u7684\u62c6\u5206\u3002\u8fd9\u4e9b\u4f18\u5316\u4f7f\u6211\u4eec\u80fd\u591f\u5f15\u5165\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u7684\u57fa\u51c6\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u65b0\u5f15\u5165\u7684\u96f6\u6837\u672c\u82f1\u8bed\u57fa\u51c6\u4fdd\u6301\u4e86\u4e0e\u5168\u89c4\u6a21\u7248\u672c\u76f8\u4f3c\u7684\u6392\u540d\u987a\u5e8f\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u53ea\u662f\u5176\u4e00\u5c0f\u90e8\u5206\u3002", "author": "Kenneth Enevoldsen et.al.", "authors": "Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, M\u00e1rton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemi\u0144ski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystr\u00f8m, Roman Solomatin, \u00d6mer \u00c7a\u011fatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafa\u0142 Po\u015bwiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Bj\u00f6rn Pl\u00fcster, Jan Philipp Harries, Lo\u00efc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek \u0160uppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael G\u00fcnther, Mengzhou Xia, Weijia Shi, Xing Han L\u00f9, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff", "id": "2502.13595v1", "paper_url": "http://arxiv.org/abs/2502.13595v1", "repo": "null"}}