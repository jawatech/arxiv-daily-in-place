{"2406.06777": {"publish_time": "2024-06-10", "title": "MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension", "paper_summary": "Recently, Large Language Models (LLMs) with their strong task-handling\ncapabilities have shown remarkable advancements across a spectrum of fields,\nmoving beyond natural language understanding. However, their proficiency within\nthe chemistry domain remains restricted, especially in solving professional\nmolecule-related tasks. This challenge is attributed to their inherent\nlimitations in comprehending molecules using only common textual\nrepresentations, i.e., SMILES strings. In this study, we seek to enhance the\nability of LLMs to comprehend molecules by designing and equipping them with a\nmulti-modal external module, namely MolX. In particular, instead of directly\nusing a SMILES string to represent a molecule, we utilize specific encoders to\nextract fine-grained features from both SMILES string and 2D molecular graph\nrepresentations for feeding into an LLM. Moreover, a human-defined molecular\nfingerprint is incorporated to leverage its embedded domain knowledge. Then, to\nestablish an alignment between MolX and the LLM's textual input space, the\nwhole model in which the LLM is frozen, is pre-trained with a versatile\nstrategy including a diverse set of tasks. Extensive experimental evaluations\ndemonstrate that our proposed method only introduces a small number of\ntrainable parameters while outperforming baselines on various downstream\nmolecule-related tasks ranging from molecule-to-text translation to\nretrosynthesis, with and without fine-tuning the LLM.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u5177\u6709\u5f3a\u5927\u4efb\u52a1\u5904\u7406\u80fd\u529b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u8d85\u8d8a\u4e86\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u5316\u5b66\u9886\u57df\u7684\u719f\u7ec3\u5ea6\u4ecd\u7136\u53d7\u5230\u9650\u5236\uff0c\u5c24\u5176\u662f\u5728\u89e3\u51b3\u4e0e\u4e13\u4e1a\u5206\u5b50\u76f8\u5173\u7684\u4efb\u52a1\u65f6\u3002\u8fd9\u4e00\u6311\u6218\u5f52\u56e0\u4e8e\u5b83\u4eec\u5728\u4ec5\u4f7f\u7528\u5e38\u89c1\u6587\u672c\u8868\u793a\uff08\u5373 SMILES \u5b57\u7b26\u4e32\uff09\u7406\u89e3\u5206\u5b50\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\u6027\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5bfb\u6c42\u901a\u8fc7\u8bbe\u8ba1\u548c\u4e3a LLM \u914d\u5907\u591a\u6a21\u6001\u5916\u90e8\u6a21\u5757 MolX \u6765\u589e\u5f3a\u5176\u7406\u89e3\u5206\u5b50\u7684\u80fd\u529b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5229\u7528\u7279\u5b9a\u7f16\u7801\u5668\u4ece SMILES \u5b57\u7b26\u4e32\u548c 2D \u5206\u5b50\u56fe\u8868\u793a\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u7528\u4e8e\u8f93\u5165 LLM\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u4f7f\u7528 SMILES \u5b57\u7b26\u4e32\u6765\u8868\u793a\u5206\u5b50\u3002\u6b64\u5916\uff0c\u8fd8\u7eb3\u5165\u4e86\u4eba\u7c7b\u5b9a\u4e49\u7684\u5206\u5b50\u6307\u7eb9\uff0c\u4ee5\u5229\u7528\u5176\u5d4c\u5165\u7684\u9886\u57df\u77e5\u8bc6\u3002\u7136\u540e\uff0c\u4e3a\u4e86\u5728 MolX \u548c LLM \u7684\u6587\u672c\u8f93\u5165\u7a7a\u95f4\u4e4b\u95f4\u5efa\u7acb\u5bf9\u9f50\uff0c\u5728 LLM \u88ab\u51bb\u7ed3\u7684\u6574\u4e2a\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u5305\u62ec\u5404\u79cd\u4efb\u52a1\u5728\u5185\u7684\u4e00\u79cd\u901a\u7528\u7b56\u7565\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4ec5\u5f15\u5165\u4e86\u5c11\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u540c\u65f6\u5728\u5404\u79cd\u4e0b\u6e38\u5206\u5b50\u76f8\u5173\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5305\u62ec\u4ece\u5206\u5b50\u5230\u6587\u672c\u7684\u7ffb\u8bd1\u5230\u9006\u5408\u6210\uff0c\u65e0\u8bba\u662f\u5426\u5bf9 LLM \u8fdb\u884c\u5fae\u8c03\u3002", "author": "Khiem Le et.al.", "authors": "Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V. Chawla", "id": "2406.06777v1", "paper_url": "http://arxiv.org/abs/2406.06777v1", "repo": "null"}}