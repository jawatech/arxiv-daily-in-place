{"2405.14868": {"publish_time": "2024-05-23", "title": "Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis", "paper_summary": "Accurate reconstruction of complex dynamic scenes from just a single\nviewpoint continues to be a challenging task in computer vision. Current\ndynamic novel view synthesis methods typically require videos from many\ndifferent camera viewpoints, necessitating careful recording setups, and\nsignificantly restricting their utility in the wild as well as in terms of\nembodied AI applications. In this paper, we propose $\\textbf{GCD}$, a\ncontrollable monocular dynamic view synthesis pipeline that leverages\nlarge-scale diffusion priors to, given a video of any scene, generate a\nsynchronous video from any other chosen perspective, conditioned on a set of\nrelative camera pose parameters. Our model does not require depth as input, and\ndoes not explicitly model 3D scene geometry, instead performing end-to-end\nvideo-to-video translation in order to achieve its goal efficiently. Despite\nbeing trained on synthetic multi-view video data only, zero-shot real-world\ngeneralization experiments show promising results in multiple domains,\nincluding robotics, object permanence, and driving environments. We believe our\nframework can potentially unlock powerful applications in rich dynamic scene\nunderstanding, perception for robotics, and interactive 3D video viewing\nexperiences for virtual reality.", "paper_summary_zh": "\u50c5\u5f9e\u55ae\u4e00\u8996\u9ede\u6e96\u78ba\u91cd\u5efa\u8907\u96dc\u7684\u52d5\u614b\u5834\u666f\uff0c\u5728\u96fb\u8166\u8996\u89ba\u4e2d\u6301\u7e8c\u6210\u70ba\u4e00\u9805\u8271\u96e3\u7684\u4efb\u52d9\u3002\u76ee\u524d\u7684\u52d5\u614b\u65b0\u8996\u5716\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4f86\u81ea\u8a31\u591a\u4e0d\u540c\u76f8\u6a5f\u8996\u9ede\u7684\u5f71\u7247\uff0c\u9019\u9700\u8981\u4ed4\u7d30\u7684\u9304\u88fd\u8a2d\u5b9a\uff0c\u4e26\u986f\u8457\u9650\u5236\u5b83\u5011\u5728\u91ce\u5916\u4ee5\u53ca\u5177\u8eab AI \u61c9\u7528\u65b9\u9762\u7684\u6548\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa $\\textbf{GCD}$\uff0c\u4e00\u7a2e\u53ef\u63a7\u7684\u55ae\u773c\u52d5\u614b\u8996\u5716\u5408\u6210\u7ba1\u9053\uff0c\u5b83\u5229\u7528\u5927\u898f\u6a21\u64f4\u6563\u5148\u9a57\uff0c\u5728\u7d66\u5b9a\u4efb\u4f55\u5834\u666f\u5f71\u7247\u7684\u60c5\u6cc1\u4e0b\uff0c\u6839\u64da\u4e00\u7d44\u76f8\u5c0d\u76f8\u6a5f\u59ff\u52e2\u53c3\u6578\u751f\u6210\u4f86\u81ea\u4efb\u4f55\u5176\u4ed6\u9078\u64c7\u8996\u89d2\u7684\u540c\u6b65\u5f71\u7247\u3002\u6211\u5011\u7684\u6a21\u578b\u4e0d\u9700\u8981\u6df1\u5ea6\u4f5c\u70ba\u8f38\u5165\uff0c\u4e5f\u4e0d\u660e\u78ba\u5efa\u6a21 3D \u5834\u666f\u5e7e\u4f55\uff0c\u800c\u662f\u57f7\u884c\u7aef\u5c0d\u7aef\u7684\u5f71\u7247\u5230\u5f71\u7247\u8f49\u63db\uff0c\u4ee5\u6709\u6548\u9054\u6210\u5176\u76ee\u6a19\u3002\u5118\u7ba1\u50c5\u5728\u5408\u6210\u591a\u8996\u5716\u5f71\u7247\u8cc7\u6599\u4e0a\u8a13\u7df4\uff0c\u4f46\u96f6\u6b21\u5b78\u7fd2\u771f\u5be6\u4e16\u754c\u6982\u5316\u5be6\u9a57\u5728\u591a\u500b\u9818\u57df\u986f\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7d50\u679c\uff0c\u5305\u62ec\u6a5f\u5668\u4eba\u6280\u8853\u3001\u7269\u9ad4\u6046\u5e38\u6027\u548c\u99d5\u99db\u74b0\u5883\u3002\u6211\u5011\u76f8\u4fe1\u6211\u5011\u7684\u67b6\u69cb\u6709\u53ef\u80fd\u5728\u8c50\u5bcc\u7684\u52d5\u614b\u5834\u666f\u7406\u89e3\u3001\u6a5f\u5668\u4eba\u611f\u77e5\u4ee5\u53ca\u865b\u64ec\u5be6\u5883\u4e92\u52d5\u5f0f 3D \u5f71\u7247\u89c0\u770b\u9ad4\u9a57\u4e2d\u958b\u555f\u5f37\u5927\u7684\u61c9\u7528\u7a0b\u5f0f\u3002", "author": "Basile Van Hoorick et.al.", "authors": "Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, Carl Vondrick", "id": "2405.14868v1", "paper_url": "http://arxiv.org/abs/2405.14868v1", "repo": "null"}}