{"2406.08116": {"publish_time": "2024-06-12", "title": "Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling", "paper_summary": "Retrieval-augmented language models (RALMs) have recently shown great\npotential in mitigating the limitations of implicit knowledge in LLMs, such as\nuntimely updating of the latest expertise and unreliable retention of long-tail\nknowledge. However, since the external knowledge base, as well as the\nretriever, can not guarantee reliability, potentially leading to the knowledge\nretrieved not being helpful or even misleading for LLM generation. In this\npaper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust\nand pluggable knowledge rewriter inherently optimized for LLM generation.\nSpecifically, we introduce the novel concept of \"supportiveness\"--which\nrepresents how effectively a knowledge piece facilitates downstream tasks--by\nconsidering the perplexity impact of augmented knowledge on the response text\nof a white-box LLM. Based on knowledge supportiveness, we first design a\ntraining data curation strategy for our rewriter model, effectively identifying\nand filtering out poor or irrelevant rewrites (e.g., with low supportiveness\nscores) to improve data efficacy. We then introduce the direct preference\noptimization (DPO) algorithm to align the generated rewrites to optimal\nsupportiveness, guiding the rewriter model to summarize augmented content that\nbetter improves the final response. Comprehensive evaluations across six\npopular knowledge-intensive tasks and four LLMs have demonstrated the\neffectiveness and superiority of SKR. With only 7B parameters, SKR has shown\nbetter knowledge rewriting capability over GPT-4, the current state-of-the-art\ngeneral-purpose LLM.", "paper_summary_zh": "<paragraph>\u64f7\u53d6\u589e\u5f37\u8a9e\u8a00\u6a21\u578b (RALM) \u8fd1\u671f\u5df2\u5c55\u73fe\u51fa\u6975\u4f73\u6f5b\u529b\uff0c\u53ef\u6e1b\u8f15\u96b1\u542b\u77e5\u8b58\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u9650\u5236\uff0c\u4f8b\u5982\u6700\u65b0\u5c08\u696d\u77e5\u8b58\u7684\u66f4\u65b0\u4e0d\u5373\u6642\uff0c\u4ee5\u53ca\u9577\u5c3e\u77e5\u8b58\u7684\u4fdd\u7559\u4e0d\u53ef\u9760\u3002\u7136\u800c\uff0c\u7531\u65bc\u5916\u90e8\u77e5\u8b58\u5eab\u548c\u64f7\u53d6\u5668\u7121\u6cd5\u4fdd\u8b49\u53ef\u9760\u6027\uff0c\u53ef\u80fd\u6703\u5c0e\u81f4\u64f7\u53d6\u7684\u77e5\u8b58\u5c0d LLM \u751f\u6210\u6c92\u6709\u5e6b\u52a9\uff0c\u751a\u81f3\u6703\u7522\u751f\u8aa4\u5c0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u57fa\u65bc\u652f\u63f4\u5ea6\u7684\u77e5\u8b58\u6539\u5beb (SKR)\uff0c\u9019\u662f\u4e00\u500b\u5f37\u5065\u4e14\u53ef\u63d2\u5165\u7684\u77e5\u8b58\u6539\u5beb\u5668\uff0c\u672c\u8cea\u4e0a\u7d93\u904e\u6700\u4f73\u5316\u4ee5\u9032\u884c LLM \u751f\u6210\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f15\u5165\u4e86\u300c\u652f\u63f4\u5ea6\u300d\u7684\u65b0\u7a4e\u6982\u5ff5\uff0c\u5b83\u8868\u793a\u77e5\u8b58\u7247\u6bb5\u4fc3\u9032\u4e0b\u6e38\u4efb\u52d9\u7684\u6210\u6548\uff0c\u65b9\u6cd5\u662f\u8003\u91cf\u589e\u5f37\u77e5\u8b58\u5c0d\u767d\u76d2 LLM \u56de\u61c9\u6587\u5b57\u7684\u56f0\u60d1\u5ea6\u5f71\u97ff\u3002\u6839\u64da\u77e5\u8b58\u652f\u63f4\u5ea6\uff0c\u6211\u5011\u9996\u5148\u70ba\u6211\u5011\u7684\u6539\u5beb\u5668\u6a21\u578b\u8a2d\u8a08\u4e00\u500b\u8a13\u7df4\u8cc7\u6599\u7b56\u5c55\u7b56\u7565\uff0c\u6709\u6548\u5730\u627e\u51fa\u4e26\u904e\u6ffe\u6389\u4e0d\u826f\u6216\u4e0d\u76f8\u95dc\u7684\u6539\u5beb\uff08\u4f8b\u5982\uff0c\u652f\u63f4\u5ea6\u8a55\u5206\u4f4e\uff09\uff0c\u4ee5\u63d0\u5347\u8cc7\u6599\u6548\u80fd\u3002\u63a5\u8457\uff0c\u6211\u5011\u5f15\u5165\u4e86\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u6f14\u7b97\u6cd5\uff0c\u4ee5\u5c07\u7522\u751f\u7684\u6539\u5beb\u8207\u6700\u4f73\u652f\u63f4\u5ea6\u5c0d\u9f4a\uff0c\u5f15\u5c0e\u6539\u5beb\u5668\u6a21\u578b\u6458\u8981\u589e\u5f37\u7684\u5167\u5bb9\uff0c\u9032\u800c\u6539\u5584\u6700\u7d42\u7684\u56de\u61c9\u3002\u5728\u516d\u9805\u5ee3\u6cdb\u4f7f\u7528\u7684\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\u548c\u56db\u500b LLM \u4e2d\u9032\u884c\u7684\u5168\u9762\u8a55\u4f30\uff0c\u5df2\u8b49\u660e SKR \u7684\u6548\u80fd\u548c\u512a\u8d8a\u6027\u3002SKR \u50c5\u6709 7B \u500b\u53c3\u6578\uff0c\u5df2\u5c55\u73fe\u51fa\u6bd4 GPT-4 \u66f4\u4f73\u7684\u77e5\u8b58\u6539\u5beb\u80fd\u529b\uff0cGPT-4 \u662f\u76ee\u524d\u6700\u5148\u9032\u7684\u901a\u7528 LLM\u3002</paragraph>", "author": "Zile Qiao et.al.", "authors": "Zile Qiao, Wei Ye, Yong Jiang, Tong Mo, Pengjun Xie, Weiping Li, Fei Huang, Shikun Zhang", "id": "2406.08116v1", "paper_url": "http://arxiv.org/abs/2406.08116v1", "repo": "null"}}