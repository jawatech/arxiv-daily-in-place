{"2410.10479": {"publish_time": "2024-10-14", "title": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs", "paper_summary": "The rapid advancement of large language models (LLMs) has accelerated their\napplication in reasoning, with strategic reasoning drawing increasing\nattention. To evaluate LLMs' strategic reasoning capabilities, game theory,\nwith its concise structure, has become a preferred approach. However, current\nresearch focuses on a limited selection of games, resulting in low coverage.\nClassic game scenarios risk data leakage, and existing benchmarks often lack\nextensibility, making them inadequate for evaluating state-of-the-art models.\nTo address these challenges, we propose TMGBench, a benchmark with\ncomprehensive game type coverage, novel scenarios, and flexible organization.\nSpecifically, we incorporate all 144 game types summarized by the\nRobinson-Goforth topology of 2x2 games, constructed as classic games. We also\nemploy synthetic data generation to create diverse, higher-quality scenarios\nthrough topic guidance and human inspection, referred to as story-based games.\nLastly, we provide a sustainable framework for increasingly powerful LLMs by\ntreating these games as atomic units and organizing them into more complex\nforms via sequential, parallel, and nested structures. Our comprehensive\nevaluation of mainstream LLMs covers tests on rational reasoning, robustness,\nTheory-of-Mind (ToM), and reasoning in complex forms. Results reveal flaws in\naccuracy, consistency, and varying mastery of ToM. Additionally, o1-mini,\nOpenAI's latest reasoning model, achieved accuracy rates of 66.6%, 60.0%, and\n70.0% on sequential, parallel, and nested games, highlighting TMGBench's\nchallenges.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u9032\u5c55\u52a0\u901f\u4e86\u5b83\u5011\u5728\u63a8\u7406\u4e2d\u7684\u61c9\u7528\uff0c\u5176\u4e2d\u7b56\u7565\u63a8\u7406\u5f15\u8d77\u4e86\u8d8a\u4f86\u8d8a\u591a\u7684\u95dc\u6ce8\u3002\u70ba\u4e86\u8a55\u4f30 LLM \u7684\u7b56\u7565\u63a8\u7406\u80fd\u529b\uff0c\u535a\u5f08\u8ad6\u4ee5\u5176\u7c21\u6f54\u7684\u7d50\u69cb\u6210\u70ba\u4e86\u4e00\u7a2e\u9996\u9078\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u7814\u7a76\u96c6\u4e2d\u5728\u6709\u9650\u7684\u904a\u6232\u9078\u64c7\u4e0a\uff0c\u5c0e\u81f4\u8986\u84cb\u7387\u4f4e\u3002\u7d93\u5178\u904a\u6232\u5834\u666f\u6709\u6578\u64da\u6d29\u9732\u7684\u98a8\u96aa\uff0c\u73fe\u6709\u7684\u57fa\u6e96\u901a\u5e38\u7f3a\u4e4f\u53ef\u64f4\u5c55\u6027\uff0c\u9019\u4f7f\u5f97\u5b83\u5011\u4e0d\u8db3\u4ee5\u8a55\u4f30\u6700\u5148\u9032\u7684\u6a21\u578b\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 TMGBench\uff0c\u9019\u662f\u4e00\u500b\u57fa\u6e96\uff0c\u5177\u6709\u5168\u9762\u7684\u904a\u6232\u985e\u578b\u8986\u84cb\u7bc4\u570d\u3001\u65b0\u7a4e\u7684\u5834\u666f\u548c\u9748\u6d3b\u7684\u7d44\u7e54\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7d0d\u5165\u4e86 Robinson-Goforth \u62d3\u64b2\u4e2d\u7e3d\u7d50\u7684\u6240\u6709 144 \u7a2e\u904a\u6232\u985e\u578b\uff0c\u69cb\u5efa\u70ba\u7d93\u5178\u904a\u6232\u3002\u6211\u5011\u9084\u4f7f\u7528\u5408\u6210\u6578\u64da\u751f\u6210\u4f86\u901a\u904e\u4e3b\u984c\u6307\u5c0e\u548c\u4eba\u5de5\u6aa2\u67e5\u5275\u5efa\u591a\u6a23\u5316\u3001\u66f4\u9ad8\u8cea\u91cf\u7684\u5834\u666f\uff0c\u7a31\u70ba\u57fa\u65bc\u6545\u4e8b\u7684\u904a\u6232\u3002\u6700\u5f8c\uff0c\u6211\u5011\u901a\u904e\u5c07\u9019\u4e9b\u904a\u6232\u8996\u70ba\u539f\u5b50\u55ae\u5143\u4e26\u901a\u904e\u9806\u5e8f\u3001\u4e26\u884c\u548c\u5d4c\u5957\u7d50\u69cb\u5c07\u5b83\u5011\u7d44\u7e54\u6210\u66f4\u8907\u96dc\u7684\u5f62\u5f0f\uff0c\u70ba\u529f\u80fd\u8d8a\u4f86\u8d8a\u5f37\u5927\u7684 LLM \u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u6301\u7e8c\u7684\u6846\u67b6\u3002\u6211\u5011\u5c0d\u4e3b\u6d41 LLM \u7684\u7d9c\u5408\u8a55\u4f30\u6db5\u84cb\u4e86\u5c0d\u7406\u6027\u63a8\u7406\u3001\u9b6f\u68d2\u6027\u3001\u5fc3\u667a\u7406\u8ad6 (ToM) \u548c\u8907\u96dc\u5f62\u5f0f\u4e2d\u7684\u63a8\u7406\u7684\u6e2c\u8a66\u3002\u7d50\u679c\u63ed\u793a\u4e86\u6e96\u78ba\u6027\u3001\u4e00\u81f4\u6027\u548c\u5c0d ToM \u638c\u63e1\u7a0b\u5ea6\u4e0d\u540c\u7684\u7f3a\u9677\u3002\u6b64\u5916\uff0cOpenAI \u6700\u65b0\u7684\u63a8\u7406\u6a21\u578b o1-mini \u5728\u9806\u5e8f\u3001\u4e26\u884c\u548c\u5d4c\u5957\u904a\u6232\u4e2d\u5206\u5225\u9054\u5230\u4e86 66.6%\u300160.0% \u548c 70.0% \u7684\u6e96\u78ba\u7387\uff0c\u7a81\u986f\u4e86 TMGBench \u7684\u6311\u6230\u3002", "author": "Haochuan Wang et.al.", "authors": "Haochuan Wang, Xiachong Feng, Lei Li, Zhanyue Qin, Dianbo Sui, Lingpeng Kong", "id": "2410.10479v1", "paper_url": "http://arxiv.org/abs/2410.10479v1", "repo": "null"}}