{"2405.14597": {"publish_time": "2024-05-23", "title": "Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs", "paper_summary": "We introduce Integer Scale, a novel post-training quantization scheme for\nlarge language models that effectively resolves the inference bottleneck in\ncurrent fine-grained quantization approaches while maintaining similar\naccuracies. Integer Scale is a free lunch as it requires no extra calibration\nor fine-tuning which will otherwise incur additional costs. It can be used\nplug-and-play for most fine-grained quantization methods. Its integration\nresults in at most 1.85x end-to-end speed boost over the original counterpart\nwith comparable accuracy. Additionally, due to the orchestration of the\nproposed Integer Scale and fine-grained quantization, we resolved the\nquantization difficulty for Mixtral-8x7B and LLaMA-3 models with negligible\nperformance degradation, and it comes with an end-to-end speed boost of 2.13x,\nand 2.31x compared with their FP16 versions respectively.", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86\u6574\u6578\u91cf\u5316\uff0c\u9019\u662f\u4e00\u7a2e\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u5275\u65b0\u8a13\u7df4\u5f8c\u91cf\u5316\u65b9\u6848\uff0c\u53ef\u6709\u6548\u89e3\u6c7a\u7576\u524d\u7d30\u7c92\u5ea6\u91cf\u5316\u65b9\u6cd5\u4e2d\u7684\u63a8\u7406\u74f6\u9838\uff0c\u540c\u6642\u4fdd\u6301\u985e\u4f3c\u7684\u6e96\u78ba\u5ea6\u3002\u6574\u6578\u91cf\u5316\u662f\u4e00\u7a2e\u514d\u8cbb\u7684\u5348\u9910\uff0c\u56e0\u70ba\u5b83\u4e0d\u9700\u8981\u984d\u5916\u7684\u6821\u6e96\u6216\u5fae\u8abf\uff0c\u5426\u5247\u6703\u7522\u751f\u984d\u5916\u7684\u6210\u672c\u3002\u5b83\u53ef\u4ee5\u7528\u65bc\u5927\u591a\u6578\u7d30\u7c92\u5ea6\u91cf\u5316\u65b9\u6cd5\u7684\u5373\u63d2\u5373\u7528\u3002\u5b83\u7684\u6574\u5408\u5c0e\u81f4\u7aef\u5230\u7aef\u901f\u5ea6\u63d0\u5347\u81f3\u591a 1.85 \u500d\uff0c\u8207\u539f\u59cb\u5c0d\u61c9\u9805\u76f8\u6bd4\uff0c\u5177\u6709\u53ef\u6bd4\u7684\u6e96\u78ba\u5ea6\u3002\u6b64\u5916\uff0c\u7531\u65bc\u6240\u63d0\u51fa\u7684\u6574\u6578\u91cf\u5316\u548c\u7d30\u7c92\u5ea6\u91cf\u5316\u7684\u5354\u8abf\uff0c\u6211\u5011\u4ee5\u53ef\u5ffd\u7565\u7684\u6027\u80fd\u4e0b\u964d\u89e3\u6c7a\u4e86 Mixtral-8x7B \u548c LLaMA-3 \u6a21\u578b\u7684\u91cf\u5316\u96e3\u5ea6\uff0c\u4e26\u4e14\u8207\u5b83\u5011\u7684 FP16 \u7248\u672c\u76f8\u6bd4\uff0c\u5206\u5225\u5177\u6709 2.13 \u500d\u548c 2.31 \u500d\u7684\u7aef\u5230\u7aef\u901f\u5ea6\u63d0\u5347\u3002", "author": "Qingyuan Li et.al.", "authors": "Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Yifan Lu, Yerui Sun, Lin Ma, Yuchen Xie", "id": "2405.14597v1", "paper_url": "http://arxiv.org/abs/2405.14597v1", "repo": "null"}}