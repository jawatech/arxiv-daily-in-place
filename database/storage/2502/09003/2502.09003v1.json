{"2502.09003": {"publish_time": "2025-02-13", "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models", "paper_summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.", "paper_summary_zh": "\u76e3\u7763\u5f0f\u5fae\u8abf\u662f\u5c07\u9810\u8a13\u7df4\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9069\u61c9\u81f3\u4e0b\u6e38\u4efb\u52d9\u7684\u6a19\u6e96\u65b9\u6cd5\u3002\u91cf\u5316\u6700\u8fd1\u5df2\u88ab\u7814\u7a76\u4f5c\u70ba\u4e00\u7a2e\u8a13\u7df4\u5f8c\u6280\u8853\uff0c\u7528\u65bc\u9ad8\u6548\u90e8\u7f72 LLM\u3002\u70ba\u4e86\u7372\u5f97\u91cf\u5316\u7684\u5fae\u8abf LLM\uff0c\u50b3\u7d71\u7ba1\u9053\u6703\u5148\u5fae\u8abf\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u7136\u5f8c\u518d\u9032\u884c\u8a13\u7df4\u5f8c\u91cf\u5316\u3002\u9019\u901a\u5e38\u6703\u7522\u751f\u6b21\u4f73\u6548\u80fd\uff0c\u56e0\u70ba\u5b83\u7121\u6cd5\u5229\u7528\u5fae\u8abf\u548c\u91cf\u5316\u4e4b\u9593\u7684\u5354\u540c\u6548\u61c9\u3002\u70ba\u4e86\u6709\u6548\u5be6\u73fe LLM \u4e2d\u6b0a\u91cd\u3001\u6fc0\u6d3b\u548c KV \u5feb\u53d6\u7684\u4f4e\u4f4d\u5143\u91cf\u5316\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba\u65cb\u8f49\u76f4\u901a\u4f30\u8a08\u5668 (RoSTE) \u7684\u6f14\u7b97\u6cd5\uff0c\u5b83\u7d50\u5408\u4e86\u91cf\u5316\u611f\u77e5\u76e3\u7763\u5f0f\u5fae\u8abf (QA-SFT) \u548c\u4e00\u7a2e\u81ea\u9069\u61c9\u65cb\u8f49\u7b56\u7565\uff0c\u8a72\u7b56\u7565\u6703\u8b58\u5225\u6709\u6548\u7684\u65cb\u8f49\u7d44\u614b\u4ee5\u6e1b\u5c11\u6fc0\u6d3b\u7570\u5e38\u503c\u3002\u6211\u5011\u900f\u904e\u5206\u6790 RoSTE \u5728\u61c9\u7528\u65bc\u904e\u5ea6\u53c3\u6578\u5316\u6700\u5c0f\u5e73\u65b9\u91cf\u5316\u8a13\u7df4\u554f\u984c\u6642\u7684\u9810\u6e2c\u8aa4\u5dee\uff0c\u63d0\u4f9b\u4e86\u95dc\u65bc RoSTE \u7684\u7406\u8ad6\u898b\u89e3\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u9810\u6e2c\u8aa4\u5dee\u8207\u6536\u6582\u6b0a\u91cd\u7684\u91cf\u5316\u8aa4\u5dee\u6210\u6b63\u6bd4\uff0c\u800c\u9019\u53ef\u900f\u904e\u6700\u4f73\u5316\u7684\u65cb\u8f49\u7d44\u614b\u6709\u6548\u5730\u7ba1\u7406\u3002\u5728\u4e0d\u540c\u5927\u5c0f\u7684 Pythia \u548c Llama \u6a21\u578b\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\u4e86 RoSTE \u7684\u6709\u6548\u6027\u3002\u8207\u73fe\u6709\u7684\u8a13\u7df4\u5f8c SFT \u91cf\u5316\u57fa\u6e96\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u5404\u7a2e\u4efb\u52d9\u548c\u4e0d\u540c\u7684 LLM \u67b6\u69cb\u4e2d\u6301\u7e8c\u7372\u5f97\u512a\u7570\u7684\u6548\u80fd\u3002", "author": "Quan Wei et.al.", "authors": "Quan Wei, Chung-Yiu Yau, Hoi-To Wai, Yang, Zhao, Dongyeop Kang, Youngsuk Park, Mingyi Hong", "id": "2502.09003v1", "paper_url": "http://arxiv.org/abs/2502.09003v1", "repo": "null"}}