{"2501.03895": {"publish_time": "2025-01-07", "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token", "paper_summary": "The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.", "paper_summary_zh": "\u96a8\u8457 GPT-4o \u7b49\u5373\u6642\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u7684\u51fa\u73fe\uff0c\u5c0d\u65bc\u9ad8\u6548\u80fd LMM \u7684\u8208\u8da3\u4e5f\u5927\u5e45\u63d0\u5347\u3002LMM \u67b6\u69cb\u901a\u5e38\u6703\u5c07\u8996\u89ba\u8f38\u5165\u7de8\u78bc\u6210\u8996\u89ba\u7b26\u865f\uff08\u9023\u7e8c\u8868\u793a\uff09\uff0c\u4e26\u5c07\u5176\u8207\u6587\u5b57\u8aaa\u660e\u6574\u5408\u5230\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8108\u7d61\u4e2d\uff0c\u5176\u4e2d\u5927\u898f\u6a21\u53c3\u6578\u548c\u5927\u91cf\u7684\u8108\u7d61\u7b26\u865f\uff08\u4e3b\u8981\u662f\u8996\u89ba\u7b26\u865f\uff09\u6703\u9020\u6210\u5927\u91cf\u7684\u904b\u7b97\u8ca0\u64d4\u3002\u5148\u524d\u5c0d\u65bc\u9ad8\u6548\u80fd LMM \u7684\u52aa\u529b\u7e3d\u662f\u5c08\u6ce8\u65bc\u7528\u8f03\u5c0f\u7684\u6a21\u578b\u53d6\u4ee3 LLM \u4e3b\u5e79\uff0c\u800c\u5ffd\u7565\u4e86\u7b26\u865f\u6578\u91cf\u9019\u500b\u95dc\u9375\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07\u4ecb\u7d39 LLaVA-Mini\uff0c\u9019\u662f\u4e00\u500b\u7b26\u865f\u6578\u91cf\u6700\u5c11\u7684 LMM\u3002\u70ba\u4e86\u5728\u4fdd\u7559\u8996\u89ba\u8cc7\u8a0a\u7684\u540c\u6642\uff0c\u9054\u6210\u8996\u89ba\u7b26\u865f\u7684\u9ad8\u58d3\u7e2e\u7387\uff0c\u6211\u5011\u9996\u5148\u5206\u6790 LMM \u5982\u4f55\u7406\u89e3\u8996\u89ba\u7b26\u865f\uff0c\u4e26\u767c\u73fe\u5927\u90e8\u5206\u7684\u8996\u89ba\u7b26\u865f\u53ea\u5728 LLM \u4e3b\u5e79\u7684\u65e9\u671f\u5c64\u4e2d\u626e\u6f14\u95dc\u9375\u89d2\u8272\uff0c\u5728\u9019\u4e9b\u5c64\u4e2d\uff0c\u5b83\u5011\u4e3b\u8981\u5c07\u8996\u89ba\u8cc7\u8a0a\u878d\u5408\u5230\u6587\u5b57\u7b26\u865f\u4e2d\u3002\u57fa\u65bc\u9019\u500b\u767c\u73fe\uff0cLLaVA-Mini \u5f15\u5165\u4e86\u6a21\u614b\u9810\u878d\u5408\uff0c\u4ee5\u9810\u5148\u5c07\u8996\u89ba\u8cc7\u8a0a\u878d\u5408\u5230\u6587\u5b57\u7b26\u865f\u4e2d\uff0c\u5f9e\u800c\u4fc3\u9032\u5c07\u8f38\u5165 LLM \u4e3b\u5e79\u7684\u8996\u89ba\u7b26\u865f\u6975\u5ea6\u58d3\u7e2e\u6210\u4e00\u500b\u7b26\u865f\u3002LLaVA-Mini \u662f\u4e00\u500b\u7d71\u4e00\u7684\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\uff0c\u80fd\u5920\u4ee5\u4e00\u7a2e\u9ad8\u6548\u80fd\u7684\u65b9\u5f0f\u7406\u89e3\u5f71\u50cf\u3001\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u548c\u5f71\u7247\u3002\u5728 11 \u500b\u57fa\u65bc\u5f71\u50cf\u7684\u57fa\u6e96\u548c 7 \u500b\u57fa\u65bc\u5f71\u7247\u7684\u57fa\u6e96\u7684\u5be6\u9a57\u4e2d\uff0c\u8b49\u660e LLaVA-Mini \u7684\u6548\u80fd\u512a\u65bc LLaVA-v1.5\uff0c\u50c5\u4f7f\u7528 1 \u500b\u8996\u89ba\u7b26\u865f\uff0c\u800c\u975e 576 \u500b\u3002\u6548\u7387\u5206\u6790\u986f\u793a\uff0cLLaVA-Mini \u53ef\u4ee5\u6e1b\u5c11 77% \u7684 FLOP\uff0c\u5728 40 \u6beb\u79d2\u5167\u63d0\u4f9b\u4f4e\u5ef6\u9072\u7684\u56de\u61c9\uff0c\u4e26\u5728\u914d\u5099 24GB \u8a18\u61b6\u9ad4\u7684 GPU \u786c\u9ad4\u4e0a\u8655\u7406\u8d85\u904e 10,000 \u5e40\u7684\u5f71\u7247\u3002", "author": "Shaolei Zhang et.al.", "authors": "Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng", "id": "2501.03895v1", "paper_url": "http://arxiv.org/abs/2501.03895v1", "repo": "https://github.com/ictnlp/llava-mini"}}