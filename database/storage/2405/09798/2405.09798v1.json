{"2405.09798": {"publish_time": "2024-05-16", "title": "Many-Shot In-Context Learning in Multimodal Foundation Models", "paper_summary": "Large language models are well-known to be effective at few-shot in-context\nlearning (ICL). Recent advancements in multimodal foundation models have\nenabled unprecedentedly long context windows, presenting an opportunity to\nexplore their capability to perform ICL with many more demonstrating examples.\nIn this work, we evaluate the performance of multimodal foundation models\nscaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro\nacross 10 datasets spanning multiple domains (natural imagery, medical imagery,\nremote sensing, and molecular imagery) and tasks (multi-class, multi-label, and\nfine-grained classification). We observe that many-shot ICL, including up to\nalmost 2,000 multimodal demonstrating examples, leads to substantial\nimprovements compared to few-shot (<100 examples) ICL across all of the\ndatasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly\nup to the maximum number of tested examples on many datasets. Given the high\ninference costs associated with the long prompts required for many-shot ICL, we\nalso explore the impact of batching multiple queries in a single API call. We\nshow that batching up to 50 queries can lead to performance improvements under\nzero-shot and many-shot ICL, with substantial gains in the zero-shot setting on\nmultiple datasets, while drastically reducing per-query cost and latency.\nFinally, we measure ICL data efficiency of the models, or the rate at which the\nmodels learn from more demonstrating examples. We find that while GPT-4o and\nGemini 1.5 Pro achieve similar zero-shot performance across the datasets,\nGemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most\ndatasets. Our results suggest that many-shot ICL could enable users to\nefficiently adapt multimodal foundation models to new applications and domains.\nOur codebase is publicly available at\nhttps://github.com/stanfordmlgroup/ManyICL .", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4ee5\u5728\u5c11\u91cf\u7bc4\u4f8b\u7684\u8a9e\u5883\u5b78\u7fd2 (ICL) \u4e2d\u6709\u6548\u8457\u7a31\u3002\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u7684\u6700\u65b0\u9032\u5c55\u4f7f\u5f97\u524d\u6240\u672a\u6709\u7684\u9577\u8a9e\u5883\u7a97\u53e3\u6210\u70ba\u53ef\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u6a5f\u6703\u4f86\u63a2\u7d22\u5b83\u5011\u5728\u66f4\u591a\u793a\u7bc4\u7bc4\u4f8b\u4e0b\u57f7\u884c ICL \u7684\u80fd\u529b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u5f9e\u5c11\u91cf\u7bc4\u4f8b\u5230\u5927\u91cf\u7bc4\u4f8b ICL \u7684\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u5728\u8de8\u8d8a\u591a\u500b\u9818\u57df\uff08\u81ea\u7136\u5f71\u50cf\u3001\u91ab\u5b78\u5f71\u50cf\u3001\u9059\u6e2c\u548c\u5206\u5b50\u5f71\u50cf\uff09\u548c\u4efb\u52d9\uff08\u591a\u985e\u5225\u3001\u591a\u6a19\u7c64\u548c\u7d30\u7c92\u5ea6\u5206\u985e\uff09\u7684 10 \u500b\u8cc7\u6599\u96c6\u4e0a\u5c0d GPT-4o \u548c Gemini 1.5 Pro \u9032\u884c\u4e86\u57fa\u6e96\u6e2c\u8a66\u3002\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u5927\u91cf\u7bc4\u4f8b ICL\uff0c\u5305\u62ec\u591a\u9054\u8fd1 2,000 \u500b\u591a\u6a21\u614b\u793a\u7bc4\u7bc4\u4f8b\uff0c\u8207\u6240\u6709\u8cc7\u6599\u96c6\u4e2d\u7684\u5c11\u91cf\u7bc4\u4f8b (<100 \u500b\u7bc4\u4f8b) ICL \u76f8\u6bd4\uff0c\u5e36\u4f86\u4e86\u986f\u8457\u7684\u6539\u9032\u3002\u6b64\u5916\uff0cGemini 1.5 Pro \u7684\u6548\u80fd\u6301\u7e8c\u5728\u8a31\u591a\u8cc7\u6599\u96c6\u4e0a\u96a8\u8457\u6e2c\u8a66\u7bc4\u4f8b\u7684\u6700\u5927\u6578\u91cf\u5c0d\u6578\u7dda\u6027\u5730\u63d0\u5347\u3002\u9451\u65bc\u5927\u91cf\u7bc4\u4f8b ICL \u6240\u9700\u7684\u9577\u63d0\u793a\u76f8\u95dc\u7684\u9ad8\u63a8\u8ad6\u6210\u672c\uff0c\u6211\u5011\u4e5f\u63a2\u8a0e\u4e86\u5728\u55ae\u4e00 API \u547c\u53eb\u4e2d\u6279\u6b21\u8655\u7406\u591a\u500b\u67e5\u8a62\u7684\u5f71\u97ff\u3002\u6211\u5011\u5c55\u793a\u4e86\u6279\u6b21\u8655\u7406\u591a\u9054 50 \u500b\u67e5\u8a62\u53ef\u4ee5\u5728\u96f6\u7bc4\u4f8b\u548c\u5927\u91cf\u7bc4\u4f8b ICL \u4e0b\u5c0e\u81f4\u6548\u80fd\u63d0\u5347\uff0c\u5728\u591a\u500b\u8cc7\u6599\u96c6\u4e0a\u7684\u96f6\u7bc4\u4f8b\u8a2d\u5b9a\u4e2d\u7372\u5f97\u986f\u8457\u7684\u6536\u76ca\uff0c\u540c\u6642\u5927\u5e45\u964d\u4f4e\u6bcf\u500b\u67e5\u8a62\u7684\u6210\u672c\u548c\u5ef6\u9072\u3002\u6700\u5f8c\uff0c\u6211\u5011\u6e2c\u91cf\u4e86\u6a21\u578b\u7684 ICL \u8cc7\u6599\u6548\u7387\uff0c\u6216\u6a21\u578b\u5f9e\u66f4\u591a\u793a\u7bc4\u7bc4\u4f8b\u4e2d\u5b78\u7fd2\u7684\u901f\u7387\u3002\u6211\u5011\u767c\u73fe\uff0c\u96d6\u7136 GPT-4o \u548c Gemini 1.5 Pro \u5728\u5404\u500b\u8cc7\u6599\u96c6\u4e0a\u90fd\u9054\u5230\u4e86\u985e\u4f3c\u7684\u96f6\u7bc4\u4f8b\u6548\u80fd\uff0c\u4f46 Gemini 1.5 Pro \u5728\u5927\u591a\u6578\u8cc7\u6599\u96c6\u4e0a\u5c55\u73fe\u51fa\u6bd4 GPT-4o \u66f4\u9ad8\u7684 ICL \u8cc7\u6599\u6548\u7387\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u5927\u91cf\u7bc4\u4f8b ICL \u53ef\u4ee5\u8b93\u4f7f\u7528\u8005\u6709\u6548\u5730\u5c07\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u8abf\u6574\u5230\u65b0\u7684\u61c9\u7528\u7a0b\u5f0f\u548c\u9818\u57df\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5eab\u5df2\u516c\u958b\u65bc https://github.com/stanfordmlgroup/ManyICL\u3002</paragraph>", "author": "Yixing Jiang et.al.", "authors": "Yixing Jiang, Jeremy Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan H. Chen, Andrew Y. Ng", "id": "2405.09798v1", "paper_url": "http://arxiv.org/abs/2405.09798v1", "repo": "https://github.com/stanfordmlgroup/ManyICL"}}