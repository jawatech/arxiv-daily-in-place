{"2412.06106": {"publish_time": "2024-12-08", "title": "Enhanced Computationally Efficient Long LoRA Inspired Perceiver Architectures for Auto-Regressive Language Modeling", "paper_summary": "The Transformer architecture has revolutionized the Natural Language\nProcessing field and is the backbone of Large Language Models (LLMs). The\nTransformer uses the attention mechanism that computes the pair-wise similarity\nbetween its input tokens to produce latent vectors that are able to understand\nthe semantic meaning of the input text. One of the challenges in the\nTransformer architecture is the quadratic complexity of the attention mechanism\nthat prohibits the efficient processing of long sequence lengths. While many\nrecent research works have attempted to provide a reduction from $O(n^2)$ time\ncomplexity of attention to semi-linear complexity, it remains an unsolved\nproblem in the sense of maintaining a high performance when such complexity is\nreduced. One of the important works in this respect is the Perceiver class of\narchitectures that have demonstrated excellent performance while reducing the\ncomputation complexity. In this paper, we use the PerceiverAR that was proposed\nfor Auto-Regressive modeling as a baseline, and provide three different\narchitectural enhancements to it with varying computation overhead tradeoffs.\nInspired by the recently proposed efficient attention computation approach of\nLong-LoRA, we then present an equally efficient Perceiver-based architecture\n(termed as Long LoRA Pereceiver - LLP) that can be used as the base\narchitecture in LLMs instead of just a fine-tuning add-on. Our results on\ndifferent benchmarks indicate impressive improvements compared to recent\nTransformer based models.", "paper_summary_zh": "Transformer \u67b6\u69cb\u5fb9\u5e95\u6539\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u9818\u57df\uff0c\u4e26\u4e14\u662f\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u652f\u67f1\u3002Transformer \u4f7f\u7528\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u8a08\u7b97\u5176\u8f38\u5165\u4ee3\u5e63\u4e4b\u9593\u6210\u5c0d\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u7522\u751f\u80fd\u5920\u7406\u89e3\u8f38\u5165\u6587\u672c\u8a9e\u7fa9\u542b\u7fa9\u7684\u6f5b\u5728\u5411\u91cf\u3002Transformer \u67b6\u69cb\u9762\u81e8\u7684\u6311\u6230\u4e4b\u4e00\u662f\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u4e8c\u6b21\u8907\u96dc\u5ea6\uff0c\u9019\u6703\u963b\u7919\u5c0d\u9577\u5e8f\u5217\u9577\u5ea6\u7684\u6709\u6548\u8655\u7406\u3002\u96d6\u7136\u8a31\u591a\u6700\u8fd1\u7684\u7814\u7a76\u5de5\u4f5c\u90fd\u5617\u8a66\u5c07\u6ce8\u610f\u529b\u5f9e O(n^2) \u6642\u9593\u8907\u96dc\u5ea6\u964d\u4f4e\u5230\u534a\u7dda\u6027\u8907\u96dc\u5ea6\uff0c\u4f46\u5b83\u5728\u964d\u4f4e\u9019\u7a2e\u8907\u96dc\u5ea6\u6642\u4ecd\u7121\u6cd5\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u9019\u4ecd\u7136\u662f\u4e00\u500b\u672a\u89e3\u6c7a\u7684\u554f\u984c\u3002\u9019\u65b9\u9762\u7684\u91cd\u8981\u5de5\u4f5c\u4e4b\u4e00\u662f\u611f\u77e5\u5668\u985e\u67b6\u69cb\uff0c\u5b83\u5728\u964d\u4f4e\u8a08\u7b97\u8907\u96dc\u5ea6\u7684\u540c\u6642\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u70ba\u81ea\u8ff4\u6b78\u5efa\u6a21\u63d0\u51fa\u7684\u611f\u77e5\u5668 AR \u4f5c\u70ba\u57fa\u7dda\uff0c\u4e26\u91dd\u5c0d\u5b83\u63d0\u4f9b\u4e86\u4e09\u7a2e\u4e0d\u540c\u7684\u67b6\u69cb\u589e\u5f37\uff0c\u9019\u4e9b\u589e\u5f37\u5177\u6709\u4e0d\u540c\u7684\u8a08\u7b97\u958b\u92b7\u6b0a\u8861\u3002\u53d7\u6700\u8fd1\u63d0\u51fa\u7684 Long-LoRA \u9ad8\u6548\u6ce8\u610f\u529b\u8a08\u7b97\u65b9\u6cd5\u7684\u555f\u767c\uff0c\u6211\u5011\u96a8\u5f8c\u63d0\u51fa\u4e86\u4e00\u500b\u540c\u6a23\u9ad8\u6548\u7684\u57fa\u65bc\u611f\u77e5\u5668\u7684\u67b6\u69cb\uff08\u7a31\u70ba Long LoRA \u611f\u77e5\u5668 - LLP\uff09\uff0c\u5b83\u53ef\u4ee5\u7528\u4f5c LLM \u4e2d\u7684\u57fa\u672c\u67b6\u69cb\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e00\u500b\u5fae\u8abf\u9644\u52a0\u5143\u4ef6\u3002\u6211\u5011\u5728\u4e0d\u540c\u57fa\u6e96\u6e2c\u8a66\u4e0a\u7684\u7d50\u679c\u8868\u660e\uff0c\u8207\u6700\u8fd1\u57fa\u65bc Transformer \u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u6709\u4e86\u986f\u8457\u7684\u6539\u9032\u3002", "author": "Kaleel Mahmood et.al.", "authors": "Kaleel Mahmood, Shaoyi Huang", "id": "2412.06106v1", "paper_url": "http://arxiv.org/abs/2412.06106v1", "repo": "null"}}