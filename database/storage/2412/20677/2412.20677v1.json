{"2412.20677": {"publish_time": "2024-12-30", "title": "Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA", "paper_summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5df2\u986f\u793a\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u554f\u984c\u4e2d\u8868\u73fe\u826f\u597d\u3002\u7136\u800c\uff0c\u96a8\u8457\u6a21\u578b\u5927\u5c0f\u548c\u8f38\u5165\u5e8f\u5217\u9577\u5ea6\u7684\u589e\u52a0\uff0cKV \u5feb\u53d6\u7684\u5feb\u901f\u589e\u52a0\u986f\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u901f\u5ea6\u3002\u56e0\u6b64\uff0cGQA \u6a21\u578b\u4f5c\u70ba MHA \u6a21\u578b\u7684\u66ff\u4ee3\u54c1\uff0c\u5df2\u88ab\u5ee3\u6cdb\u5f15\u5165 LLM\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u5c07 MHA \u6a21\u578b\u526a\u679d\u6210\u5177\u6709\u4efb\u4f55\u9375\u503c\u982d\u58d3\u7e2e\u7387\u7684 GQA \u6a21\u578b\u3002\u6211\u5011\u7684\u6a21\u578b\u57fa\u65bc $\\mathit{L_0}$ \u63a9\u78bc\uff0c\u4ee5\u9010\u6f38\u79fb\u9664\u591a\u9918\u7684\u53c3\u6578\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728\u4e0d\u6539\u8b8a\u6a21\u578b\u7684\u60c5\u6cc1\u4e0b\u5c07\u6b63\u4ea4\u8b8a\u63db\u61c9\u7528\u65bc\u6ce8\u610f\u529b\u982d\uff0c\u4ee5\u5728\u526a\u679d\u8a13\u7df4\u524d\u589e\u52a0\u6ce8\u610f\u529b\u982d\u4e4b\u9593\u7684\u76f8\u4f3c\u5ea6\uff0c\u4ee5\u9032\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u8207\u65cb\u8f49\u4f4d\u7f6e\u5d4c\u5165 (RoPE) \u76f8\u5bb9\uff0c\u9019\u8868\u793a\u8a13\u7df4\u5f8c\u7684\u6a21\u578b\u53ef\u4ee5\u5b8c\u5168\u9069\u61c9\u4e3b\u6d41\u6a19\u6e96 GQA \u6846\u67b6\u3002\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u7b56\u7565\u53ef\u4ee5\u58d3\u7e2e LLaMA2-7B \u6a21\u578b\u4e2d\u9ad8\u9054 87.5% \u7684\u9375\u503c\u982d\uff0c\u800c\u4e0d\u6703\u9020\u6210\u904e\u591a\u7684\u6548\u80fd\u4e0b\u964d\uff0c\u53ea\u9700\u900f\u904e\u76e3\u7763\u5fae\u8abf\u5373\u53ef\u9054\u6210\u3002", "author": "Qingyun Jin et.al.", "authors": "Qingyun Jin, Xiaohui Song, Feng Zhou, Zengchang Qin", "id": "2412.20677v1", "paper_url": "http://arxiv.org/abs/2412.20677v1", "repo": "null"}}