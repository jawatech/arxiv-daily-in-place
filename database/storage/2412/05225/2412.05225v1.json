{"2412.05225": {"publish_time": "2024-12-06", "title": "BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits", "paper_summary": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements make deployment on devices with constrained resources\nextremely difficult. Among various efficiency considerations, model\nbinarization and Early Exit (EE) are common effective solutions. However,\nbinarization may lead to performance loss due to reduced precision affecting\ngradient estimation and parameter updates. Besides, the present early-exit\nmechanisms are still in the nascent stages of research. To ameliorate these\nissues, we propose Binarized Early Exit Transformer (BEExformer), the\nfirst-ever selective learning transformer architecture to combine early exit\nwith binarization for textual inference. It improves the binarization process\nthrough a differentiable second-order approximation to the impulse function.\nThis enables gradient computation concerning both the sign as well as the\nmagnitude of the weights. In contrast to absolute threshold-based EE, the\nproposed EE mechanism hinges on fractional reduction in entropy among\nintermediate transformer blocks with soft-routing loss estimation. While\nbinarization results in 18.44 times reduction in model size, early exit reduces\nthe FLOPs during inference by 54.85% and even improves accuracy by 5.98%\nthrough resolving the \"overthinking\" problem inherent in deep networks.\nMoreover, the proposed BEExformer simplifies training by not requiring\nknowledge distillation from a full-precision LLM. Extensive evaluation on the\nGLUE dataset and comparison with the SOTA works showcase its pareto-optimal\nperformance-efficiency trade-off.", "paper_summary_zh": "<paragraph>\u57fa\u65bcTransformer\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u61c9\u7528\u4e0a\u90fd\u80fd\u9054\u5230\u5c16\u7aef\u7684\u7d50\u679c\u3002\u7136\u800c\uff0c\u5b83\u5011\u9f90\u5927\u7684\u898f\u6a21\u548c\u8655\u7406\u9700\u6c42\u8b93\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u88dd\u7f6e\u4e0a\u90e8\u7f72\u8b8a\u5f97\u6975\u5176\u56f0\u96e3\u3002\u5728\u5404\u7a2e\u6548\u7387\u8003\u91cf\u4e2d\uff0c\u6a21\u578b\u4e8c\u5143\u5316\u548c\u65e9\u671f\u9000\u51fa (EE) \u662f\u5e38\u898b\u7684\u6709\u6548\u65b9\u6848\u3002\u7136\u800c\uff0c\u4e8c\u5143\u5316\u53ef\u80fd\u6703\u5c0e\u81f4\u6548\u80fd\u640d\u5931\uff0c\u56e0\u70ba\u964d\u4f4e\u7684\u7cbe\u5ea6\u6703\u5f71\u97ff\u68af\u5ea6\u4f30\u8a08\u548c\u53c3\u6578\u66f4\u65b0\u3002\u6b64\u5916\uff0c\u73fe\u6709\u7684\u65e9\u671f\u9000\u51fa\u6a5f\u5236\u4ecd\u8655\u65bc\u7814\u7a76\u7684\u840c\u82bd\u968e\u6bb5\u3002\u70ba\u4e86\u6539\u5584\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e8c\u5143\u5316\u65e9\u671f\u9000\u51faTransformer (BEExformer)\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u7d50\u5408\u65e9\u671f\u9000\u51fa\u8207\u4e8c\u5143\u5316\u7684\u9078\u64c7\u6027\u5b78\u7fd2Transformer\u67b6\u69cb\uff0c\u7528\u65bc\u6587\u5b57\u63a8\u8ad6\u3002\u5b83\u900f\u904e\u5c0d\u8108\u885d\u51fd\u6578\u9032\u884c\u53ef\u5fae\u5206\u7684\u4e8c\u968e\u8fd1\u4f3c\u4f86\u6539\u5584\u4e8c\u5143\u5316\u7a0b\u5e8f\u3002\u9019\u4f7f\u5f97\u68af\u5ea6\u8a08\u7b97\u8207\u6b0a\u91cd\u7684\u7b26\u865f\u548c\u5927\u5c0f\u90fd\u6709\u95dc\u3002\u8207\u57fa\u65bc\u7d55\u5c0d\u95be\u503c\u7684 EE \u76f8\u6bd4\uff0c\u63d0\u8b70\u7684 EE \u6a5f\u5236\u53d6\u6c7a\u65bc\u4e2d\u9593Transformer\u5340\u584a\u4e4b\u9593\u71b5\u7684\u5206\u6578\u6e1b\u5c11\uff0c\u4e26\u5177\u6709\u8edf\u8def\u7531\u640d\u5931\u4f30\u8a08\u3002\u96d6\u7136\u4e8c\u5143\u5316\u5c0e\u81f4\u6a21\u578b\u5927\u5c0f\u6e1b\u5c11\u4e86 18.44 \u500d\uff0c\u4f46\u65e9\u671f\u9000\u51fa\u5c07\u63a8\u8ad6\u671f\u9593\u7684 FLOP \u6e1b\u5c11\u4e86 54.85%\uff0c\u751a\u81f3\u900f\u904e\u89e3\u6c7a\u6df1\u5ea6\u7db2\u8def\u4e2d\u56fa\u6709\u7684\u300c\u904e\u5ea6\u601d\u8003\u300d\u554f\u984c\u800c\u5c07\u6e96\u78ba\u5ea6\u63d0\u5347\u4e86 5.98%\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684 BEExformer \u7c21\u5316\u4e86\u8a13\u7df4\uff0c\u56e0\u70ba\u4e0d\u9700\u8981\u5f9e\u5168\u7cbe\u5ea6 LLM \u9032\u884c\u77e5\u8b58\u84b8\u993e\u3002\u5728 GLUE \u8cc7\u6599\u96c6\u4e0a\u7684\u5ee3\u6cdb\u8a55\u4f30\u4ee5\u53ca\u8207 SOTA \u4f5c\u54c1\u7684\u6bd4\u8f03\u5c55\u793a\u4e86\u5176\u5e15\u7d2f\u6258\u6700\u512a\u7684\u6548\u80fd\u6548\u7387\u6b0a\u8861\u3002</paragraph>", "author": "Wazib Ansar et.al.", "authors": "Wazib Ansar, Saptarsi Goswami, Amlan Chakrabarti", "id": "2412.05225v1", "paper_url": "http://arxiv.org/abs/2412.05225v1", "repo": "null"}}