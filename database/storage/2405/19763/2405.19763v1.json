{"2405.19763": {"publish_time": "2024-05-30", "title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding", "paper_summary": "Recent strides in large language models (LLMs) have yielded remarkable\nperformance, leveraging reinforcement learning from human feedback (RLHF) to\nsignificantly enhance generation and alignment capabilities. However, RLHF\nencounters numerous challenges, including the objective mismatch issue, leading\nto suboptimal performance in Natural Language Understanding (NLU) tasks. To\naddress this limitation, we propose a novel Reinforcement Learning framework\nenhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs\nin NLU tasks. By incorporating label-sensitive pairs into reinforcement\nlearning, our method aims to adeptly capture nuanced label-sensitive semantic\nfeatures during RL, thereby enhancing natural language understanding.\nExperiments conducted on five diverse foundation models across eight tasks\nshowcase promising results. In comparison to Supervised Fine-tuning models\n(SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared\nwith RLHF models, the improvement averages at 0.69%. These results reveal the\neffectiveness of our method for LLMs in NLU tasks. Code and data available at:\nhttps://github.com/MagiaSN/ACL2024_RLLR.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u7522\u751f\u4e86\u986f\u8457\u7684\u6027\u80fd\uff0c\u5229\u7528\u4eba\u985e\u53cd\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF) \u4f86\u986f\u8457\u589e\u5f37\u751f\u6210\u548c\u5c0d\u9f4a\u80fd\u529b\u3002\u7136\u800c\uff0cRLHF \u6703\u9047\u5230\u8a31\u591a\u6311\u6230\uff0c\u5305\u62ec\u76ee\u6a19\u4e0d\u5339\u914d\u554f\u984c\uff0c\u5c0e\u81f4\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u4efb\u52d9\u7684\u6027\u80fd\u4e0d\u4f73\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u5f37\u5316\u5b78\u7fd2\u6846\u67b6\uff0c\u589e\u5f37\u4e86\u6a19\u7c64\u654f\u611f\u734e\u52f5 (RLLR)\uff0c\u4ee5\u589e\u5f37 LLM \u5728 NLU \u4efb\u52d9\u4e2d\u7684\u6027\u80fd\u3002\u901a\u904e\u5c07\u6a19\u7c64\u654f\u611f\u5c0d\u6574\u5408\u5230\u5f37\u5316\u5b78\u7fd2\u4e2d\uff0c\u6211\u5011\u7684\u76ee\u6a19\u662f\u5728 RL \u671f\u9593\u5de7\u5999\u5730\u6355\u6349\u7d30\u5fae\u7684\u6a19\u7c64\u654f\u611f\u8a9e\u7fa9\u7279\u5fb5\uff0c\u5f9e\u800c\u589e\u5f37\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u3002\u5728\u516b\u9805\u4efb\u52d9\u4e2d\u5c0d\u4e94\u500b\u4e0d\u540c\u7684\u57fa\u790e\u6a21\u578b\u9032\u884c\u7684\u5be6\u9a57\u5c55\u793a\u4e86\u6709\u5e0c\u671b\u7684\u7d50\u679c\u3002\u8207\u76e3\u7763\u5fae\u8abf\u6a21\u578b (SFT) \u76f8\u6bd4\uff0cRLLR \u5c55\u793a\u4e86\u5e73\u5747\u6027\u80fd\u63d0\u5347 1.54%\u3002\u8207 RLHF \u6a21\u578b\u76f8\u6bd4\uff0c\u5e73\u5747\u63d0\u5347 0.69%\u3002\u9019\u4e9b\u7d50\u679c\u63ed\u793a\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5c0d LLM \u5728 NLU \u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\u3002\u4ee3\u78bc\u548c\u6578\u64da\u53ef\u65bc\u4ee5\u4e0b\u7db2\u5740\u7372\u5f97\uff1ahttps://github.com/MagiaSN/ACL2024_RLLR\u3002", "author": "Kuo Liao et.al.", "authors": "Kuo Liao, Shuang Li, Meng Zhao, Liqun Liu, Mengge Xue, Zhenyu Hu, Honglin Han, Chengguo Yin", "id": "2405.19763v1", "paper_url": "http://arxiv.org/abs/2405.19763v1", "repo": "https://github.com/magiasn/acl2024_rllr"}}