{"2406.17761": {"publish_time": "2024-06-25", "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages", "paper_summary": "Large language models (LLMs) are commonly used for long-form question\nanswering, which requires them to generate paragraph-length answers to complex\nquestions. While long-form QA has been well-studied in English via many\ndifferent datasets and evaluation metrics, this research has not been extended\nto cover most other languages. To bridge this gap, we introduce CaLMQA, a\ncollection of 2.6K complex questions spanning 23 languages, including\nunder-resourced, rarely-studied languages such as Fijian and Kirundi. Our\ndataset includes both naturally-occurring questions collected from community\nweb forums as well as questions written by native speakers, whom we hire for\nthis purpose. Our process yields diverse, complex questions that reflect\ncultural topics (e.g. traditions, laws, news) and the language usage of native\nspeakers. We conduct automatic evaluation across a suite of open- and\nclosed-source models using our novel metric CaLMScore, which detects incorrect\nlanguage and token repetitions in answers, and observe that the quality of\nLLM-generated answers degrades significantly for some low-resource languages.\nWe perform human evaluation on a subset of models and see that model\nperformance is significantly worse for culturally specific questions than for\nculturally agnostic questions. Our findings highlight the need for further\nresearch in LLM multilingual capabilities and non-English LFQA evaluation.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5e38\u7528\u65bc\u9577\u7bc7\u554f\u7b54\uff0c\u9019\u9700\u8981\u5b83\u5011\u91dd\u5c0d\u8907\u96dc\u554f\u984c\u7522\u751f\u6bb5\u843d\u9577\u5ea6\u7684\u7b54\u6848\u3002\u96d6\u7136\u9577\u7bc7\u554f\u7b54\u5df2\u900f\u904e\u8a31\u591a\u4e0d\u540c\u7684\u8cc7\u6599\u96c6\u548c\u8a55\u4f30\u6307\u6a19\u5728\u82f1\u6587\u4e2d\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u4f46\u6b64\u7814\u7a76\u5c1a\u672a\u64f4\u5c55\u5230\u6db5\u84cb\u5927\u591a\u6578\u5176\u4ed6\u8a9e\u8a00\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 CaLMQA\uff0c\u9019\u662f\u4e00\u500b\u5305\u542b 2.6K \u500b\u8907\u96dc\u554f\u984c\u7684\u96c6\u5408\uff0c\u6db5\u84cb 23 \u7a2e\u8a9e\u8a00\uff0c\u5305\u62ec\u6590\u6fdf\u8a9e\u548c\u57fa\u9686\u8fea\u8a9e\u7b49\u8cc7\u6e90\u4e0d\u8db3\u3001\u9bae\u5c11\u7814\u7a76\u7684\u8a9e\u8a00\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u5305\u542b\u5f9e\u793e\u7fa4\u7db2\u8def\u8ad6\u58c7\u6536\u96c6\u5230\u7684\u81ea\u7136\u767c\u751f\u554f\u984c\uff0c\u4ee5\u53ca\u6211\u5011\u70ba\u6b64\u76ee\u7684\u8058\u8acb\u6bcd\u8a9e\u4eba\u58eb\u64b0\u5beb\u7684\u554f\u984c\u3002\u6211\u5011\u7684\u6d41\u7a0b\u7522\u751f\u4e86\u591a\u5143\u3001\u8907\u96dc\u7684\u554f\u984c\uff0c\u53cd\u6620\u4e86\u6587\u5316\u4e3b\u984c\uff08\u4f8b\u5982\u50b3\u7d71\u3001\u6cd5\u5f8b\u3001\u65b0\u805e\uff09\u548c\u6bcd\u8a9e\u4eba\u58eb\u7684\u8a9e\u8a00\u4f7f\u7528\u3002\u6211\u5011\u4f7f\u7528\u6211\u5011\u7684\u65b0\u7a4e\u6307\u6a19 CaLMScore \u5c0d\u4e00\u7cfb\u5217\u958b\u653e\u548c\u9589\u6e90\u6a21\u578b\u9032\u884c\u81ea\u52d5\u8a55\u4f30\uff0c\u8a72\u6307\u6a19\u6703\u5075\u6e2c\u7b54\u6848\u4e2d\u7684\u4e0d\u6b63\u78ba\u8a9e\u8a00\u548c\u91cd\u8907\u8a18\u865f\uff0c\u4e26\u89c0\u5bdf\u5230 LLM \u751f\u6210\u7684\u7b54\u6848\u54c1\u8cea\u5c0d\u65bc\u67d0\u4e9b\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u6703\u986f\u8457\u4e0b\u964d\u3002\u6211\u5011\u5c0d\u6a21\u578b\u7684\u5b50\u96c6\u9032\u884c\u4eba\u5de5\u8a55\u4f30\uff0c\u4e26\u767c\u73fe\u91dd\u5c0d\u7279\u5b9a\u6587\u5316\u554f\u984c\u7684\u6a21\u578b\u6548\u80fd\u986f\u8457\u4f4e\u65bc\u91dd\u5c0d\u6587\u5316\u4e0d\u53ef\u77e5\u554f\u984c\u7684\u6a21\u578b\u6548\u80fd\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u7a81\u986f\u4e86\u9032\u4e00\u6b65\u7814\u7a76 LLM \u591a\u8a9e\u8a00\u80fd\u529b\u548c\u975e\u82f1\u6587 LFQA \u8a55\u4f30\u7684\u5fc5\u8981\u6027\u3002", "author": "Shane Arora et.al.", "authors": "Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi", "id": "2406.17761v1", "paper_url": "http://arxiv.org/abs/2406.17761v1", "repo": "https://github.com/2015aroras/calmqa"}}