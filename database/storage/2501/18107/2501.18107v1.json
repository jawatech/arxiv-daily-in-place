{"2501.18107": {"publish_time": "2025-01-30", "title": "Scaling Inference-Efficient Language Models", "paper_summary": "Scaling laws are powerful tools to predict the performance of large language\nmodels. However, current scaling laws fall short of accounting for inference\ncosts. In this work, we first show that model architecture affects inference\nlatency, where models of the same size can have up to 3.5x difference in\nlatency. To tackle this challenge, we modify the Chinchilla scaling laws to\nco-optimize the model parameter count, the number of training tokens, and the\nmodel architecture. Due to the reason that models of similar training loss\nexhibit gaps in downstream evaluation, we also propose a novel method to train\ninference-efficient models based on the revised scaling laws. We perform\nextensive empirical studies to fit and evaluate our inference-aware scaling\nlaws. We vary model parameters from 80M to 1B, training tokens from 1.6B to\n30B, and model shapes, training a total of 63 models. Guided by our\ninference-efficient scaling law and model selection method, we release the\nMorph-1B model, which improves inference latency by 1.8x while maintaining\naccuracy on downstream tasks compared to open-source models, pushing the Pareto\nfrontier of accuracy-latency tradeoff.", "paper_summary_zh": "\u898f\u6a21\u5316\u6cd5\u5247\u662f\u7528\u4f86\u9810\u6e2c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u6548\u80fd\u7684\u5f37\u5927\u5de5\u5177\u3002\u7136\u800c\uff0c\u7576\u524d\u7684\u898f\u6a21\u5316\u6cd5\u5247\u4e26\u672a\u8003\u616e\u63a8\u8ad6\u6210\u672c\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u9996\u5148\u5c55\u793a\u6a21\u578b\u67b6\u69cb\u6703\u5f71\u97ff\u63a8\u8ad6\u5ef6\u9072\uff0c\u5176\u4e2d\u76f8\u540c\u5927\u5c0f\u7684\u6a21\u578b\u5728\u5ef6\u9072\u4e0a\u53ef\u80fd\u6703\u6709\u9ad8\u9054 3.5 \u500d\u7684\u5dee\u7570\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u4fee\u6539\u4e86 Chinchilla \u898f\u6a21\u5316\u6cd5\u5247\uff0c\u4ee5\u5171\u540c\u6700\u4f73\u5316\u6a21\u578b\u53c3\u6578\u8a08\u6578\u3001\u8a13\u7df4\u4ee4\u724c\u6578\u548c\u6a21\u578b\u67b6\u69cb\u3002\u7531\u65bc\u8a13\u7df4\u640d\u5931\u985e\u4f3c\u7684\u6a21\u578b\u5728\u4e0b\u6e38\u8a55\u4f30\u4e2d\u8868\u73fe\u51fa\u5dee\u8ddd\uff0c\u6211\u5011\u9084\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc\u4fee\u6539\u5f8c\u7684\u898f\u6a21\u5316\u6cd5\u5247\u8a13\u7df4\u63a8\u8ad6\u6548\u7387\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u5011\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u8b49\u7814\u7a76\uff0c\u4ee5\u64ec\u5408\u548c\u8a55\u4f30\u6211\u5011\u63a8\u8ad6\u611f\u77e5\u7684\u898f\u6a21\u5316\u6cd5\u5247\u3002\u6211\u5011\u5c07\u6a21\u578b\u53c3\u6578\u5f9e 80M \u8b8a\u66f4\u5230 1B\uff0c\u8a13\u7df4\u4ee4\u724c\u5f9e 1.6B \u8b8a\u66f4\u5230 30B\uff0c\u4ee5\u53ca\u6a21\u578b\u5f62\u72c0\uff0c\u7e3d\u5171\u8a13\u7df4\u4e86 63 \u500b\u6a21\u578b\u3002\u5728\u6211\u5011\u7684\u63a8\u8ad6\u6548\u7387\u898f\u6a21\u5316\u6cd5\u5247\u548c\u6a21\u578b\u9078\u64c7\u65b9\u6cd5\u7684\u6307\u5c0e\u4e0b\uff0c\u6211\u5011\u767c\u5e03\u4e86 Morph-1B \u6a21\u578b\uff0c\u8207\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u76f8\u6bd4\uff0c\u5b83\u5c07\u63a8\u8ad6\u5ef6\u9072\u964d\u4f4e\u4e86 1.8 \u500d\uff0c\u540c\u6642\u7dad\u6301\u4e86\u4e0b\u6e38\u4efb\u52d9\u7684\u6e96\u78ba\u6027\uff0c\u63a8\u52d5\u4e86\u6e96\u78ba\u5ea6-\u5ef6\u9072\u6b0a\u8861\u7684\u5e15\u7d2f\u6258\u524d\u7de3\u3002", "author": "Song Bian et.al.", "authors": "Song Bian, Minghao Yan, Shivaram Venkataraman", "id": "2501.18107v1", "paper_url": "http://arxiv.org/abs/2501.18107v1", "repo": "null"}}