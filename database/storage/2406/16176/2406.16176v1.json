{"2406.16176": {"publish_time": "2024-06-23", "title": "GraphEval2000: Benchmarking and Improving Large Language Models on Graph Datasets", "paper_summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to reason about graph-structured data. To address\nthis gap, we introduce GraphEval2000, the first comprehensive graph dataset,\ncomprising 40 graph data structure problems along with 2000 test cases.\nAdditionally, we introduce an evaluation framework based on GraphEval2000,\ndesigned to assess the graph reasoning abilities of LLMs through coding\nchallenges. Our dataset categorizes test cases into four primary and four\nsub-categories, ensuring a comprehensive evaluation. We evaluate eight popular\nLLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of\ndirected graphs compared to undirected ones. While private LLMs consistently\noutperform open-source models, the performance gap is narrowing. Furthermore,\nto improve the usability of our evaluation framework, we propose Structured\nSymbolic Decomposition (SSD), an instruction-based method designed to enhance\nLLM performance on GraphEval2000. Results show that SSD improves the\nperformance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an\nincrease of 11.11\\%, 33.37\\%, and 33.37\\%, respectively.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u53d6\u5f97\u4e86\u986f\u8457\u7684\u6210\u529f\uff0c\u5728\u8655\u7406\u548c\u7406\u89e3\u6587\u672c\u6578\u64da\u65b9\u9762\u8868\u73fe\u51fa\u986f\u8457\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u767c\u73fe LLM \u5728\u63a8\u7406\u5716\u5f62\u7d50\u69cb\u6578\u64da\u7684\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 GraphEval2000\uff0c\u7b2c\u4e00\u500b\u5168\u9762\u7684\u5716\u5f62\u6578\u64da\u96c6\uff0c\u5305\u542b 40 \u500b\u5716\u5f62\u6578\u64da\u7d50\u69cb\u554f\u984c\u4ee5\u53ca 2000 \u500b\u6e2c\u8a66\u7528\u4f8b\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86\u57fa\u65bc GraphEval2000 \u7684\u8a55\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u901a\u904e\u7de8\u78bc\u6311\u6230\u8a55\u4f30 LLM \u7684\u5716\u5f62\u63a8\u7406\u80fd\u529b\u3002\u6211\u5011\u7684\u6578\u64da\u96c6\u5c07\u6e2c\u8a66\u7528\u4f8b\u5206\u70ba\u56db\u500b\u4e3b\u8981\u985e\u5225\u548c\u56db\u500b\u5b50\u985e\u5225\uff0c\u78ba\u4fdd\u9032\u884c\u5168\u9762\u7684\u8a55\u4f30\u3002\u6211\u5011\u5728 GraphEval2000 \u4e0a\u8a55\u4f30\u4e86\u516b\u500b\u6d41\u884c\u7684 LLM\uff0c\u7d50\u679c\u8868\u660e\uff0c\u8207\u7121\u5411\u5716\u76f8\u6bd4\uff0cLLM \u5c0d\u6709\u5411\u5716\u7684\u7406\u89e3\u66f4\u597d\u3002\u96d6\u7136\u79c1\u6709 LLM \u6301\u7e8c\u512a\u65bc\u958b\u6e90\u6a21\u578b\uff0c\u4f46\u6027\u80fd\u5dee\u8ddd\u6b63\u5728\u7e2e\u5c0f\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u63d0\u9ad8\u6211\u5011\u8a55\u4f30\u6846\u67b6\u7684\u53ef\u7528\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7d50\u69cb\u5316\u7b26\u865f\u5206\u89e3 (SSD)\uff0c\u4e00\u7a2e\u57fa\u65bc\u6307\u4ee4\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u589e\u5f37 LLM \u5728 GraphEval2000 \u4e0a\u7684\u6027\u80fd\u3002\u7d50\u679c\u8868\u660e\uff0cSSD \u5206\u5225\u63d0\u9ad8\u4e86 GPT-3.5\u3001GPT-4 \u548c GPT-4o \u5728\u8907\u96dc\u5716\u5f62\u554f\u984c\u4e0a\u7684\u6027\u80fd\uff0c\u5206\u5225\u589e\u52a0\u4e86 11.11%\u300133.37% \u548c 33.37%\u3002", "author": "Qiming Wu et.al.", "authors": "Qiming Wu, Zichen Chen, Will Corcoran, Misha Sra, Ambuj K. Singh", "id": "2406.16176v1", "paper_url": "http://arxiv.org/abs/2406.16176v1", "repo": "null"}}