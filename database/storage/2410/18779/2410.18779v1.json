{"2410.18779": {"publish_time": "2024-10-24", "title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs", "paper_summary": "A primary challenge in large language model (LLM) development is their\nonerous pre-training cost. Typically, such pre-training involves optimizing a\nself-supervised objective (such as next-token prediction) over a large corpus.\nThis paper explores a promising paradigm to improve LLM pre-training efficiency\nand quality by suitably leveraging a small language model (SLM). In particular,\nthis paradigm relies on an SLM to both (1) provide soft labels as additional\ntraining supervision, and (2) select a small subset of valuable (\"informative\"\nand \"hard\") training examples. Put together, this enables an effective transfer\nof the SLM's predictive distribution to the LLM, while prioritizing specific\nregions of the training data distribution. Empirically, this leads to reduced\nLLM training time compared to standard training, while improving the overall\nquality. Theoretically, we develop a statistical framework to systematically\nstudy the utility of SLMs in enabling efficient training of high-quality LLMs.\nIn particular, our framework characterizes how the SLM's seemingly low-quality\nsupervision can enhance the training of a much more capable LLM. Furthermore,\nit also highlights the need for an adaptive utilization of such supervision, by\nstriking a balance between the bias and variance introduced by the SLM-provided\nsoft labels. We corroborate our theoretical framework by improving the\npre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B\nparameters on the Pile dataset.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u958b\u767c\u7684\u4e00\u9805\u4e3b\u8981\u6311\u6230\u662f\u5176\u7e41\u91cd\u7684\u9810\u8a13\u7df4\u6210\u672c\u3002\u901a\u5e38\uff0c\u9019\u7a2e\u9810\u8a13\u7df4\u6d89\u53ca\u5728\u5927\u578b\u8a9e\u6599\u5eab\u4e0a\u6700\u4f73\u5316\u81ea\u76e3\u7763\u76ee\u6a19\uff08\u4f8b\u5982\u4e0b\u4e00\u500b\u7b26\u865f\u9810\u6e2c\uff09\u3002\u672c\u6587\u63a2\u8a0e\u4e86\u4e00\u7a2e\u6709\u524d\u9014\u7684\u7bc4\u4f8b\uff0c\u85c9\u7531\u9069\u7576\u5730\u5229\u7528\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM) \u4f86\u63d0\u5347 LLM \u9810\u8a13\u7df4\u6548\u7387\u548c\u54c1\u8cea\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6b64\u7bc4\u4f8b\u4f9d\u8cf4 SLM \u4f86 (1) \u63d0\u4f9b\u8edf\u6a19\u7c64\u4f5c\u70ba\u984d\u5916\u7684\u8a13\u7df4\u76e3\u7763\uff0c\u4ee5\u53ca (2) \u9078\u64c7\u4e00\u5c0f\u90e8\u5206\u6709\u50f9\u503c\u7684\uff08\u300c\u6709\u8cc7\u8a0a\u300d\u4e14\u300c\u56f0\u96e3\u300d\uff09\u8a13\u7df4\u7bc4\u4f8b\u3002\u7d9c\u5408\u4f86\u8aaa\uff0c\u9019\u80fd\u8b93 SLM \u7684\u9810\u6e2c\u5206\u914d\u6709\u6548\u8f49\u79fb\u5230 LLM\uff0c\u540c\u6642\u512a\u5148\u8655\u7406\u8a13\u7df4\u8cc7\u6599\u5206\u914d\u7684\u7279\u5b9a\u5340\u57df\u3002\u6839\u64da\u7d93\u9a57\uff0c\u8207\u6a19\u6e96\u8a13\u7df4\u76f8\u6bd4\uff0c\u9019\u6703\u7e2e\u77ed LLM \u8a13\u7df4\u6642\u9593\uff0c\u540c\u6642\u63d0\u5347\u6574\u9ad4\u54c1\u8cea\u3002\u5728\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u767c\u5c55\u4e86\u4e00\u500b\u7d71\u8a08\u67b6\u69cb\uff0c\u4ee5\u7cfb\u7d71\u6027\u5730\u7814\u7a76 SLM \u5728\u80fd\u6709\u6548\u8a13\u7df4\u9ad8\u54c1\u8cea LLM \u4e2d\u7684\u6548\u7528\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u67b6\u69cb\u63cf\u8ff0\u4e86 SLM \u770b\u4f3c\u4f4e\u54c1\u8cea\u7684\u76e3\u7763\u5982\u4f55\u80fd\u589e\u5f37\u8a13\u7df4\u4e00\u500b\u66f4\u5f37\u5927\u7684 LLM\u3002\u6b64\u5916\uff0c\u5b83\u4e5f\u5f37\u8abf\u4e86\u9069\u61c9\u6027\u5229\u7528\u6b64\u985e\u76e3\u7763\u7684\u5fc5\u8981\u6027\uff0c\u85c9\u7531\u53d6\u5f97 SLM \u63d0\u4f9b\u7684\u8edf\u6a19\u7c64\u6240\u5f15\u9032\u7684\u504f\u5dee\u548c\u8b8a\u7570\u4e4b\u9593\u7684\u5e73\u8861\u3002\u6211\u5011\u900f\u904e\u5229\u7528 Pile \u8cc7\u6599\u96c6\u4e0a\u4e00\u500b\u5177\u6709 1.5B \u53c3\u6578\u7684\u8f03\u5c0f\u578b LM\uff0c\u6539\u5584\u4e00\u500b\u5177\u6709 2.8B \u53c3\u6578\u7684 LLM \u7684\u9810\u8a13\u7df4\uff0c\u4f86\u8b49\u5be6\u6211\u5011\u7684\u7406\u8ad6\u67b6\u69cb\u3002", "author": "Ankit Singh Rawat et.al.", "authors": "Ankit Singh Rawat, Veeranjaneyulu Sadhanala, Afshin Rostamizadeh, Ayan Chakrabarti, Wittawat Jitkrittum, Vladimir Feinberg, Seungyeon Kim, Hrayr Harutyunyan, Nikunj Saunshi, Zachary Nado, Rakesh Shivanna, Sashank J. Reddi, Aditya Krishna Menon, Rohan Anil, Sanjiv Kumar", "id": "2410.18779v1", "paper_url": "http://arxiv.org/abs/2410.18779v1", "repo": "null"}}