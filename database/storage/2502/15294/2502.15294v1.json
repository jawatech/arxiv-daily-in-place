{"2502.15294": {"publish_time": "2025-02-21", "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference", "paper_summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u4e0d\u65b7\u589e\u52a0\u7684\u4e0a\u4e0b\u6587\u8996\u7a97\u5927\u5c0f\u5df2\u63d0\u5347\u5176\u8655\u7406\u8907\u96dc\u3001\u9577\u6587\u672c\u4efb\u52d9\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u96a8\u8457\u5c0d\u8a71\u56de\u5408\u7684\u6301\u7e8c\u9032\u884c\uff0c\u9700\u8981\u5728 GPU \u8a18\u61b6\u9ad4\u4e2d\u5132\u5b58\u5927\u91cf\u7684 KV \u5feb\u53d6\uff0c\u9019\u6703\u986f\u8457\u5f71\u97ff\u6a21\u578b\u670d\u52d9\u7cfb\u7d71\u7684\u6548\u7387\uff0c\u751a\u81f3\u53ef\u7528\u6027\u3002\u672c\u6587\u5206\u6790\u4e86\u4f86\u81ea\u771f\u5be6\u4f7f\u7528\u8005\u7684\u5c0d\u8a71\u8cc7\u6599\uff0c\u4e26\u767c\u73fe LLM \u63a8\u8ad6\u5448\u73fe\u5206\u6c34\u5dba\u5c64\uff0c\u5728\u6b64\u4e4b\u5f8c\uff0c\u56de\u5408\u5c64\u7d1a\u6ce8\u610f\u529b\u7684\u5206\u5e03\u986f\u793a\u51fa\u986f\u8457\u7684\u76f8\u4f3c\u6027\u3002\u6211\u5011\u63d0\u51fa\u56de\u5408\u6ce8\u610f\u529b\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u56de\u5408\u5c64\u7d1a\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u5b83\u53ea\u6703\u56de\u6eaf\u4e26\u8a08\u7b97\u6700\u76f8\u95dc\u56de\u5408\u7684 KV \u5feb\u53d6\u3002\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u4e0d\u5f71\u97ff\u6a21\u578b\u6548\u80fd\u7684\u60c5\u6cc1\u4e0b\uff0c\u7bc0\u7701\u4e86 55% \u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002", "author": "Yaohua Tang et.al.", "authors": "Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, Zhi Chen", "id": "2502.15294v1", "paper_url": "http://arxiv.org/abs/2502.15294v1", "repo": "null"}}