{"2405.02512": {"publish_time": "2024-05-03", "title": "Spatio-Temporal SwinMAE: A Swin Transformer based Multiscale Representation Learner for Temporal Satellite Imagery", "paper_summary": "Currently, the foundation models represented by large language models have\nmade dramatic progress and are used in a very wide range of domains including\n2D and 3D vision. As one of the important application domains of foundation\nmodels, earth observation has attracted attention and various approaches have\nbeen developed. When considering earth observation as a single image capture,\nearth observation imagery can be processed as an image with three or more\nchannels, and when it comes with multiple image captures of different\ntimestamps at one location, the temporal observation can be considered as a set\nof continuous image resembling video frames or medical SCAN slices. This paper\npresents Spatio-Temporal SwinMAE (ST-SwinMAE), an architecture which\nparticularly focuses on representation learning for spatio-temporal image\nprocessing. Specifically, it uses a hierarchical Masked Auto-encoder (MAE) with\nVideo Swin Transformer blocks. With the architecture, we present a pretrained\nmodel named Degas 100M as a geospatial foundation model. Also, we propose an\napproach for transfer learning with Degas 100M, which both pretrained encoder\nand decoder of MAE are utilized with skip connections added between them to\nachieve multi-scale information communication, forms an architecture named\nSpatio-Temporal SwinUNet (ST-SwinUNet). Our approach shows significant\nimprovements of performance over existing state-of-the-art of foundation\nmodels. Specifically, for transfer learning of the land cover downstream task\non the PhilEO Bench dataset, it shows 10.4\\% higher accuracy compared with\nother geospatial foundation models on average.", "paper_summary_zh": "", "author": "Yohei Nakayama et.al.", "authors": "Yohei Nakayama,Jiawei Su", "id": "2405.02512v1", "paper_url": "http://arxiv.org/abs/2405.02512v1", "repo": "null"}}