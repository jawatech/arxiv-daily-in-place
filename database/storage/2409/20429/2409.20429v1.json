{"2409.20429": {"publish_time": "2024-09-30", "title": "HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding", "paper_summary": "Large Vision-Language Models (LVLMs) have shown remarkable performance on\nmany visual-language tasks. However, these models still suffer from multimodal\nhallucination, which means the generation of objects or content that violates\nthe images. Many existing work detects hallucination by directly judging\nwhether an object exists in an image, overlooking the association between the\nobject and semantics. To address this issue, we propose Hierarchical Feedback\nLearning with Vision-enhanced Penalty Decoding (HELPD). This framework\nincorporates hallucination feedback at both object and sentence semantic\nlevels. Remarkably, even with a marginal degree of training, this approach can\nalleviate over 15% of hallucination. Simultaneously, HELPD penalizes the output\nlogits according to the image attention window to avoid being overly affected\nby generated text. HELPD can be seamlessly integrated with any LVLMs. Our\nexperiments demonstrate that the proposed framework yields favorable results\nacross multiple hallucination benchmarks. It effectively mitigates\nhallucination for different LVLMs and concurrently improves their text\ngeneration quality.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5728\u8a31\u591a\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u4ecd\u7136\u5b58\u5728\u591a\u6a21\u614b\u5e7b\u89ba\uff0c\u9019\u8868\u793a\u7522\u751f\u7684\u7269\u4ef6\u6216\u5167\u5bb9\u9055\u53cd\u4e86\u5f71\u50cf\u3002\u8a31\u591a\u73fe\u6709\u5de5\u4f5c\u900f\u904e\u76f4\u63a5\u5224\u65b7\u7269\u4ef6\u662f\u5426\u5b58\u5728\u65bc\u5f71\u50cf\u4e2d\u4f86\u5075\u6e2c\u5e7b\u89ba\uff0c\u5ffd\u7565\u4e86\u7269\u4ef6\u8207\u8a9e\u7fa9\u4e4b\u9593\u7684\u95dc\u806f\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5177\u6709\u8996\u89ba\u589e\u5f37\u61f2\u7f70\u89e3\u78bc\u7684\u968e\u5c64\u5f0f\u56de\u994b\u5b78\u7fd2 (HELPD)\u3002\u6b64\u67b6\u69cb\u5728\u7269\u4ef6\u548c\u53e5\u5b50\u8a9e\u7fa9\u5c64\u7d1a\u4e2d\u90fd\u7d0d\u5165\u4e86\u5e7b\u89ba\u56de\u994b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5373\u4f7f\u8a13\u7df4\u7a0b\u5ea6\u5f88\u4f4e\uff0c\u9019\u7a2e\u65b9\u6cd5\u4e5f\u80fd\u6e1b\u8f15\u8d85\u904e 15% \u7684\u5e7b\u89ba\u3002\u540c\u6642\uff0cHELPD \u6703\u6839\u64da\u5f71\u50cf\u6ce8\u610f\u529b\u8996\u7a97\u61f2\u7f70\u8f38\u51fa logit\uff0c\u4ee5\u907f\u514d\u53d7\u5230\u751f\u6210\u6587\u5b57\u904e\u5ea6\u5f71\u97ff\u3002HELPD \u53ef\u4ee5\u8207\u4efb\u4f55 LVLMs \u7121\u7e2b\u6574\u5408\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u69cb\u5728\u591a\u500b\u5e7b\u89ba\u57fa\u6e96\u4e2d\u7522\u751f\u4e86\u826f\u597d\u7684\u7d50\u679c\u3002\u5b83\u6709\u6548\u5730\u6e1b\u8f15\u4e86\u4e0d\u540c LVLMs \u7684\u5e7b\u89ba\uff0c\u4e26\u540c\u6642\u63d0\u5347\u5176\u6587\u5b57\u751f\u6210\u54c1\u8cea\u3002", "author": "Fan Yuan et.al.", "authors": "Fan Yuan, Chi Qin, Xiaogang Xu, Piji Li", "id": "2409.20429v1", "paper_url": "http://arxiv.org/abs/2409.20429v1", "repo": "null"}}