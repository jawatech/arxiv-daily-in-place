{"2412.11699": {"publish_time": "2024-12-16", "title": "CoinMath: Harnessing the Power of Coding Instruction for Math LLMs", "paper_summary": "Large Language Models (LLMs) have shown strong performance in solving\nmathematical problems, with code-based solutions proving particularly\neffective. However, the best practice to leverage coding instruction data to\nenhance mathematical reasoning remains underexplored. This study investigates\nthree key questions: (1) How do different coding styles of mathematical\ncode-based rationales impact LLMs' learning performance? (2) Can general-domain\ncoding instructions improve performance? (3) How does integrating textual\nrationales with code-based ones during training enhance mathematical reasoning\nabilities? Our findings reveal that code-based rationales with concise\ncomments, descriptive naming, and hardcoded solutions are beneficial, while\nimprovements from general-domain coding instructions and textual rationales are\nrelatively minor. Based on these insights, we propose CoinMath, a learning\nstrategy designed to enhance mathematical reasoning by diversifying the coding\nstyles of code-based rationales. CoinMath generates a variety of code-based\nrationales incorporating concise comments, descriptive naming conventions, and\nhardcoded solutions. Experimental results demonstrate that CoinMath\nsignificantly outperforms its baseline model, MAmmoTH, one of the SOTA math\nLLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u89e3\u6c7a\u6578\u5b78\u554f\u984c\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u5176\u4e2d\u57fa\u65bc\u7a0b\u5f0f\u78bc\u7684\u89e3\u6c7a\u65b9\u6848\u7279\u5225\u6709\u6548\u3002\u7136\u800c\uff0c\u5229\u7528\u7de8\u78bc\u6307\u4ee4\u8cc7\u6599\u4f86\u589e\u5f37\u6578\u5b78\u63a8\u7406\u7684\u6700\u4f73\u5be6\u52d9\u4ecd\u6709\u5f85\u63a2\u8a0e\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u4e09\u500b\u95dc\u9375\u554f\u984c\uff1a(1) \u6578\u5b78\u7a0b\u5f0f\u78bc\u5f0f\u4f9d\u64da\u7684\u4e0d\u540c\u7de8\u78bc\u98a8\u683c\u5982\u4f55\u5f71\u97ff LLM \u7684\u5b78\u7fd2\u8868\u73fe\uff1f(2) \u4e00\u822c\u9818\u57df\u7684\u7de8\u78bc\u6307\u4ee4\u662f\u5426\u53ef\u4ee5\u63d0\u5347\u8868\u73fe\uff1f(3) \u5728\u8a13\u7df4\u671f\u9593\u5c07\u6587\u5b57\u4f9d\u64da\u8207\u57fa\u65bc\u7a0b\u5f0f\u78bc\u7684\u4f9d\u64da\u6574\u5408\u5728\u4e00\u8d77\uff0c\u5982\u4f55\u589e\u5f37\u6578\u5b78\u63a8\u7406\u80fd\u529b\uff1f\u6211\u5011\u7684\u767c\u73fe\u986f\u793a\uff0c\u5177\u6709\u7c21\u6f54\u8a3b\u89e3\u3001\u63cf\u8ff0\u6027\u547d\u540d\u548c\u786c\u7de8\u78bc\u89e3\u6c7a\u65b9\u6848\u7684\u57fa\u65bc\u7a0b\u5f0f\u78bc\u7684\u4f9d\u64da\u662f\u6709\u76ca\u7684\uff0c\u800c\u4e00\u822c\u9818\u57df\u7de8\u78bc\u6307\u4ee4\u548c\u6587\u5b57\u4f9d\u64da\u7684\u6539\u9032\u76f8\u5c0d\u8f03\u5c0f\u3002\u6839\u64da\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86 CoinMath\uff0c\u9019\u662f\u4e00\u7a2e\u5b78\u7fd2\u7b56\u7565\uff0c\u65e8\u5728\u900f\u904e\u5206\u6563\u57fa\u65bc\u7a0b\u5f0f\u78bc\u7684\u4f9d\u64da\u7684\u7de8\u78bc\u98a8\u683c\u4f86\u589e\u5f37\u6578\u5b78\u63a8\u7406\u3002CoinMath \u7522\u751f\u5404\u7a2e\u57fa\u65bc\u7a0b\u5f0f\u78bc\u7684\u4f9d\u64da\uff0c\u5305\u62ec\u7c21\u6f54\u7684\u8a3b\u89e3\u3001\u63cf\u8ff0\u6027\u547d\u540d\u6163\u4f8b\u548c\u786c\u7de8\u78bc\u89e3\u6c7a\u65b9\u6848\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cCoinMath \u660e\u986f\u512a\u65bc\u5176\u57fa\u6e96\u6a21\u578b MAmmoTH\uff0c\u9019\u662f SOTA \u6578\u5b78 LLM \u4e4b\u4e00\u3002", "author": "Chengwei Wei et.al.", "authors": "Chengwei Wei, Bin Wang, Jung-jae Kim, Guimei Liu, Nancy F. Chen", "id": "2412.11699v1", "paper_url": "http://arxiv.org/abs/2412.11699v1", "repo": "null"}}