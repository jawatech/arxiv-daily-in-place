{"2502.07776": {"publish_time": "2025-02-11", "title": "Auditing Prompt Caching in Language Model APIs", "paper_summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u63d0\u793a\u5feb\u53d6\u6703\u5c0e\u81f4\u8cc7\u6599\u76f8\u95dc\u7684\u6642\u5e8f\u8b8a\u5316\uff1a\u5feb\u53d6\u63d0\u793a\u7684\u8655\u7406\u901f\u5ea6\u6bd4\u975e\u5feb\u53d6\u63d0\u793a\u5feb\u3002\n\u9019\u4e9b\u6642\u5e8f\u5dee\u7570\u6703\u9020\u6210\u5074\u983b\u6642\u5e8f\u653b\u64ca\u7684\u98a8\u96aa\u3002\u4f8b\u5982\uff0c\u5982\u679c\u5feb\u53d6\u5728\u4f7f\u7528\u8005\u4e4b\u9593\u5171\u7528\uff0c\u653b\u64ca\u8005\u53ef\u4ee5\u5f9e\u5feb\u901f\u7684 API \u56de\u61c9\u6642\u9593\u4e2d\u627e\u51fa\u5feb\u53d6\u63d0\u793a\uff0c\u4ee5\u5f97\u77e5\u5176\u4ed6\u4f7f\u7528\u8005\u7684\u63d0\u793a\u8cc7\u8a0a\u3002\u7531\u65bc\u63d0\u793a\u5feb\u53d6\u53ef\u80fd\u6703\u5c0e\u81f4\u96b1\u79c1\u5916\u6d29\uff0c\u56e0\u6b64 API \u63d0\u4f9b\u8005\u7684\u5feb\u53d6\u653f\u7b56\u5fc5\u9808\u4fdd\u6301\u900f\u660e\u5ea6\u3002\u70ba\u6b64\uff0c\u6211\u5011\u958b\u767c\u4e26\u57f7\u884c\u7d71\u8a08\u7a3d\u6838\uff0c\u4ee5\u5075\u6e2c\u5be6\u969b LLM API \u63d0\u4f9b\u8005\u4e2d\u7684\u63d0\u793a\u5feb\u53d6\u3002\u6211\u5011\u5728\u4e03\u500b API \u63d0\u4f9b\u8005\u4e2d\u5075\u6e2c\u5230\u4f7f\u7528\u8005\u4e4b\u9593\u7684\u5168\u7403\u5feb\u53d6\u5171\u7528\uff0c\u5305\u62ec OpenAI\uff0c\u9019\u6703\u5c0e\u81f4\u4f7f\u7528\u8005\u63d0\u793a\u7684\u96b1\u79c1\u5916\u6d29\u3002\u63d0\u793a\u5feb\u53d6\u9020\u6210\u7684\u6642\u5e8f\u8b8a\u5316\u4e5f\u6703\u5c0e\u81f4\u6a21\u578b\u67b6\u69cb\u8cc7\u8a0a\u5916\u6d29\u3002\u4e5f\u5c31\u662f\u8aaa\uff0c\u6211\u5011\u767c\u73fe\u8b49\u64da\u986f\u793a OpenAI \u7684\u5d4c\u5165\u6a21\u578b\u662f\u53ea\u542b\u89e3\u78bc\u5668\u7684 Transformer\uff0c\u9019\u5728\u4e4b\u524d\u4e26\u672a\u516c\u958b\u3002", "author": "Chenchen Gu et.al.", "authors": "Chenchen Gu, Xiang Lisa Li, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto", "id": "2502.07776v1", "paper_url": "http://arxiv.org/abs/2502.07776v1", "repo": "null"}}