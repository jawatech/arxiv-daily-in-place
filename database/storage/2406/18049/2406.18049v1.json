{"2406.18049": {"publish_time": "2024-06-26", "title": "Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources", "paper_summary": "Adverse event (AE) extraction following COVID-19 vaccines from text data is\ncrucial for monitoring and analyzing the safety profiles of immunizations.\nTraditional deep learning models are adept at learning intricate feature\nrepresentations and dependencies in sequential data, but often require\nextensive labeled data. In contrast, large language models (LLMs) excel in\nunderstanding contextual information, but exhibit unstable performance on named\nentity recognition tasks, possibly due to their broad but unspecific training.\nThis study aims to evaluate the effectiveness of LLMs and traditional deep\nlearning models in AE extraction, and to assess the impact of ensembling these\nmodels on performance. In this study, we utilized reports and posts from the\nVAERS (n=621), Twitter (n=9,133), and Reddit (n=131) as our corpora. Our goal\nwas to extract three types of entities: \"vaccine\", \"shot\", and \"ae\". We\nexplored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5,\nGPT-4, and Llama-2, as well as traditional deep learning models like RNN and\nBioBERT. To enhance performance, we created ensembles of the three models with\nthe best performance. For evaluation, we used strict and relaxed F1 scores to\nevaluate the performance for each entity type, and micro-average F1 was used to\nassess the overall performance. The ensemble model achieved the highest\nperformance in \"vaccine\", \"shot\", and \"ae\" with strict F1-scores of 0.878,\n0.930, and 0.925, respectively, along with a micro-average score of 0.903. In\nconclusion, this study demonstrates the effectiveness and robustness of\nensembling fine-tuned traditional deep learning models and LLMs, for extracting\nAE-related information. This study contributes to the advancement of biomedical\nnatural language processing, providing valuable insights into improving AE\nextraction from text data for pharmacovigilance and public health surveillance.", "paper_summary_zh": "\u5f9e\u6587\u672c\u8cc7\u6599\u4e2d\u64f7\u53d6 COVID-19 \u75ab\u82d7\u7684\u4e0d\u826f\u4e8b\u4ef6 (AE) \u5c0d\u65bc\u76e3\u63a7\u548c\u5206\u6790\u514d\u75ab\u7684\u5b89\u5168\u6027\u975e\u5e38\u91cd\u8981\u3002\u50b3\u7d71\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u64c5\u9577\u5b78\u7fd2\u5e8f\u5217\u8cc7\u6599\u4e2d\u7684\u8907\u96dc\u7279\u5fb5\u8868\u793a\u548c\u4f9d\u8cf4\u95dc\u4fc2\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u6a19\u7c64\u8cc7\u6599\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u64c5\u9577\u7406\u89e3\u4e0a\u4e0b\u6587\u8cc7\u8a0a\uff0c\u4f46\u5728\u547d\u540d\u5be6\u9ad4\u8b58\u5225\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u4e0d\u7a69\u5b9a\uff0c\u9019\u53ef\u80fd\u662f\u56e0\u70ba\u5b83\u5011\u7684\u8a13\u7df4\u7bc4\u570d\u5ee3\u6cdb\u4f46\u7f3a\u4e4f\u91dd\u5c0d\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u8a55\u4f30 LLM \u548c\u50b3\u7d71\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u5728 AE \u64f7\u53d6\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e26\u8a55\u4f30\u5c07\u9019\u4e9b\u6a21\u578b\u7d44\u6210\u7684\u5f71\u97ff\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5229\u7528 VAERS (n=621)\u3001Twitter (n=9,133) \u548c Reddit (n=131) \u7684\u5831\u544a\u548c\u6587\u7ae0\u4f5c\u70ba\u8a9e\u6599\u5eab\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\u64f7\u53d6\u4e09\u7a2e\u985e\u578b\u7684\u5be6\u9ad4\uff1a\u300c\u75ab\u82d7\u300d\u3001\u300c\u6ce8\u5c04\u300d\u548c\u300c\u4e0d\u826f\u4e8b\u4ef6\u300d\u3002\u6211\u5011\u63a2\u7d22\u4e26\u5fae\u8abf\u4e86\u591a\u500b LLM\uff0c\u5305\u62ec GPT-2\u3001GPT-3.5\u3001GPT-4 \u548c Llama-2\uff0c\u4ee5\u53ca\u50b3\u7d71\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\uff0c\u4f8b\u5982 RNN \u548c BioBERT\uff08GPT-4 \u9664\u5916\uff09\u3002\u70ba\u4e86\u63d0\u5347\u6548\u80fd\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u8868\u73fe\u6700\u4f73\u7684\u4e09\u500b\u6a21\u578b\u7684\u96c6\u5408\u3002\u5728\u8a55\u4f30\u65b9\u9762\uff0c\u6211\u5011\u4f7f\u7528\u56b4\u683c\u548c\u653e\u5bec\u7684 F1 \u5206\u6578\u4f86\u8a55\u4f30\u6bcf\u500b\u5be6\u9ad4\u985e\u578b\u7684\u6548\u80fd\uff0c\u4e26\u4f7f\u7528\u5fae\u5e73\u5747 F1 \u4f86\u8a55\u4f30\u6574\u9ad4\u6548\u80fd\u3002\u7d44\u5408\u6a21\u578b\u5728\u300c\u75ab\u82d7\u300d\u3001\u300c\u6ce8\u5c04\u300d\u548c\u300c\u4e0d\u826f\u4e8b\u4ef6\u300d\u4e2d\u5206\u5225\u4ee5 0.878\u30010.930 \u548c 0.925 \u7684\u56b4\u683c F1 \u5206\u6578\u7372\u5f97\u6700\u9ad8\u6548\u80fd\uff0c\u5fae\u5e73\u5747\u5206\u6578\u70ba 0.903\u3002\u7d50\u8ad6\u662f\uff0c\u672c\u7814\u7a76\u8b49\u660e\u4e86\u5fae\u8abf\u5f8c\u7684\u50b3\u7d71\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u548c LLM \u7684\u96c6\u5408\u5728\u64f7\u53d6\u8207 AE \u76f8\u95dc\u7684\u8cc7\u8a0a\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u7a69\u5065\u6027\u3002\u672c\u7814\u7a76\u6709\u52a9\u65bc\u4fc3\u9032\u751f\u7269\u91ab\u5b78\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u4e26\u63d0\u4f9b\u6709\u50f9\u503c\u7684\u898b\u89e3\uff0c\u4ee5\u6539\u5584\u85e5\u7269\u8b66\u6212\u548c\u516c\u5171\u885b\u751f\u76e3\u6e2c\u4e2d\u5f9e\u6587\u672c\u8cc7\u6599\u4e2d\u64f7\u53d6 AE \u7684\u65b9\u5f0f\u3002", "author": "Yiming Li et.al.", "authors": "Yiming Li, Deepthi Viswaroopan, William He, Jianfu Li, Xu Zuo, Hua Xu, Cui Tao", "id": "2406.18049v1", "paper_url": "http://arxiv.org/abs/2406.18049v1", "repo": "null"}}