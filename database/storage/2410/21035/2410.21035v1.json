{"2410.21035": {"publish_time": "2024-10-28", "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time", "paper_summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.", "paper_summary_zh": "\u81ea\u56de\u5f52 (AR) \u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u8bc1\u660e\u5728\u4f17\u591a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002\u7136\u800c\uff0cAR \u5efa\u6a21\u8303\u5f0f\u5b58\u5728\u4e00\u5b9a\u7684\u5c40\u9650\u6027\uff1b\u4f8b\u5982\uff0c\u5f53\u4ee3\u81ea\u56de\u5f52 LLM \u88ab\u8bad\u7ec3\u4e00\u6b21\u751f\u6210\u4e00\u4e2a\u6807\u8bb0\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u660e\u663e\u7684\u5ef6\u8fdf\u3002\u6700\u8fd1\u7684\u8fdb\u5c55\u8868\u660e\uff0c\u641c\u7d22\u548c\u91cd\u590d\u91c7\u6837\u53ef\u4ee5\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5229\u7528\u66f4\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u6765\u63d0\u9ad8\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\uff08\u4f8b\u5982\u5b9a\u7406\u8bc1\u660e\u3001\u4ee3\u7801\u751f\u6210\u548c\u5bf9\u9f50\uff09\u4e2d\u7684\u6027\u80fd\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u540c\u65f6\u751f\u6210\u81f3\u5c11 32 \u4e2a\u6807\u8bb0\uff0c\u540c\u65f6\u5728\u6587\u672c\u8d28\u91cf\u548c LAMBADA \u81ea\u7136\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u4e0a\u8d85\u8fc7\u4e86 AR \u6a21\u578b\u7684\u6027\u80fd\u3002\u8fd9\u4e00\u6210\u679c\u662f\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u7684\uff0c\u8be5\u65b9\u6cd5\u5c06\u63a8\u7406\u6b65\u9aa4\u7684\u6570\u91cf\u51cf\u5c11\u4e86 32-64 \u500d\u3002\u5b9e\u9645\u4e0a\uff0c\u5373\u4f7f\u4e0d\u4f7f\u7528\u7f13\u5b58\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4e5f\u53ef\u4ee5\u4ee5\u6bd4\u91c7\u7528 KV \u7f13\u5b58\u7684 AR \u6a21\u578b\u5feb 8 \u500d\u7684\u901f\u5ea6\u751f\u6210\u6807\u8bb0\uff0c\u5e76\u4e14\u6211\u4eec\u9884\u8ba1\u968f\u7740\u7f13\u5b58\u7684\u52a0\u5165\uff0c\u6027\u80fd\u5c06\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u5177\u6709\u9ad8\u8fbe 8.6 \u4ebf\u4e2a\u53c2\u6570\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "author": "Justin Deschenaux et.al.", "authors": "Justin Deschenaux, Caglar Gulcehre", "id": "2410.21035v1", "paper_url": "http://arxiv.org/abs/2410.21035v1", "repo": "null"}}