{"2408.04278": {"publish_time": "2024-08-08", "title": "LaDiMo: Layer-wise Distillation Inspired MoEfier", "paper_summary": "The advent of large language models has revolutionized natural language\nprocessing, but their increasing complexity has led to substantial training\ncosts, resource demands, and environmental impacts. In response, sparse\nMixture-of-Experts (MoE) models have emerged as a promising alternative to\ndense models. Since training MoE models from scratch can be prohibitively\nexpensive, recent studies have explored leveraging knowledge from pre-trained\nnon-MoE models. However, existing approaches have limitations, such as\nrequiring significant hardware resources and data. We propose a novel\nalgorithm, LaDiMo, which efficiently converts a Transformer-based non-MoE model\ninto a MoE model with minimal additional training cost. LaDiMo consists of two\nstages: layer-wise expert construction and routing policy decision. By\nharnessing the concept of Knowledge Distillation, we compress the model and\nrapidly recover its performance. Furthermore, we develop an adaptive router\nthat optimizes inference efficiency by profiling the distribution of routing\nweights and determining a layer-wise policy that balances accuracy and latency.\nWe demonstrate the effectiveness of our method by converting the LLaMA2-7B\nmodel to a MoE model using only 100K tokens, reducing activated parameters by\nover 20% while keeping accuracy. Our approach offers a flexible and efficient\nsolution for building and deploying MoE models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u51fa\u73fe\u5fb9\u5e95\u6539\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u4f46\u5b83\u5011\u65e5\u76ca\u589e\u9577\u7684\u8907\u96dc\u6027\u5c0e\u81f4\u4e86\u5927\u91cf\u7684\u8a13\u7df4\u6210\u672c\u3001\u8cc7\u6e90\u9700\u6c42\u548c\u74b0\u5883\u5f71\u97ff\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u7a00\u758f\u6df7\u5408\u5c08\u5bb6 (MoE) \u6a21\u578b\u5df2\u6210\u70ba\u7a20\u5bc6\u6a21\u578b\u7684\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7531\u65bc\u5f9e\u982d\u958b\u59cb\u8a13\u7df4 MoE \u6a21\u578b\u53ef\u80fd\u6703\u975e\u5e38\u6602\u8cb4\uff0c\u56e0\u6b64\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u5229\u7528\u9810\u8a13\u7df4\u975e MoE \u6a21\u578b\u7684\u77e5\u8b58\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u9700\u8981\u5927\u91cf\u7684\u786c\u9ad4\u8cc7\u6e90\u548c\u8cc7\u6599\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u6f14\u7b97\u6cd5 LaDiMo\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u5c07\u57fa\u65bc Transformer \u7684\u975e MoE \u6a21\u578b\u8f49\u63db\u70ba MoE \u6a21\u578b\uff0c\u800c\u984d\u5916\u7684\u8a13\u7df4\u6210\u672c\u6975\u4f4e\u3002LaDiMo \u5305\u542b\u5169\u500b\u968e\u6bb5\uff1a\u9010\u5c64\u5c08\u5bb6\u69cb\u9020\u548c\u8def\u7531\u7b56\u7565\u6c7a\u7b56\u3002\u901a\u904e\u5229\u7528\u77e5\u8b58\u84b8\u993e\u7684\u6982\u5ff5\uff0c\u6211\u5011\u58d3\u7e2e\u6a21\u578b\u4e26\u5feb\u901f\u6062\u5fa9\u5176\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u81ea\u9069\u61c9\u8def\u7531\u5668\uff0c\u5b83\u901a\u904e\u5206\u6790\u8def\u7531\u6b0a\u91cd\u7684\u5206\u4f48\u4e26\u78ba\u5b9a\u5e73\u8861\u6e96\u78ba\u6027\u548c\u5ef6\u9072\u7684\u9010\u5c64\u7b56\u7565\uff0c\u4f86\u6700\u4f73\u5316\u63a8\u7406\u6548\u7387\u3002\u6211\u5011\u901a\u904e\u4f7f\u7528\u50c5 100K \u500b\u7b26\u865f\u5c07 LLaMA2-7B \u6a21\u578b\u8f49\u63db\u70ba MoE \u6a21\u578b\u4f86\u8b49\u660e\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u4fdd\u6301\u6e96\u78ba\u6027\u7684\u540c\u6642\uff0c\u5c07\u5df2\u555f\u7528\u7684\u53c3\u6578\u6e1b\u5c11\u4e86 20% \u4ee5\u4e0a\u3002\u6211\u5011\u7684\u505a\u6cd5\u70ba\u5efa\u69cb\u548c\u90e8\u7f72 MoE \u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u500b\u9748\u6d3b\u4e14\u6709\u6548\u7387\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Sungyoon Kim et.al.", "authors": "Sungyoon Kim, Youngjun Kim, Kihyo Moon, Minsung Jang", "id": "2408.04278v1", "paper_url": "http://arxiv.org/abs/2408.04278v1", "repo": "null"}}