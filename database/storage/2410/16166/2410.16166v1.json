{"2410.16166": {"publish_time": "2024-10-21", "title": "Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM Pretraining", "paper_summary": "Multimodal large language models (MLLMs) have made significant strides by\nintegrating visual and textual modalities. A critical factor in training MLLMs\nis the quality of image-text pairs within multimodal pretraining datasets.\nHowever, $\\textit {de facto}$ filter-based data quality enhancement paradigms\noften discard a substantial portion of high-quality image data due to\ninadequate semantic alignment between images and texts, leading to\ninefficiencies in data utilization and scalability. In this paper, we propose\nthe Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically\nassesses and enhances the quality of image-text pairs. AITQE employs a text\nrewriting mechanism for low-quality pairs and incorporates a negative sample\nlearning strategy to improve evaluative capabilities by integrating\ndeliberately selected low-quality samples during training. Unlike prior\napproaches that significantly alter text distributions, our method minimally\nadjusts text to preserve data volume while enhancing quality. Experimental\nresults demonstrate that AITQE surpasses existing methods on various benchmark,\neffectively leveraging raw data and scaling efficiently with increasing data\nvolumes. We hope our work will inspire future works. The code and model are\navailable at: https://github.com/hanhuang22/AITQE.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u900f\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u8bad\u7ec3 MLLM \u7684\u5173\u952e\u56e0\u7d20\u662f\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u6587\u672c\u5bf9\u7684\u8d28\u91cf\u3002\u7136\u800c\uff0c\u4e8b\u5b9e\u4e0a\u57fa\u4e8e\u8fc7\u6ee4\u5668\u7684\u8d44\u6599\u54c1\u8d28\u63d0\u5347\u8303\u4f8b\uff0c\u5e38\u5e38\u4f1a\u56e0\u4e3a\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u7684\u8bed\u610f\u5bf9\u9f50\u4e0d\u8db3\uff0c\u800c\u4e22\u5f03\u5927\u91cf\u9ad8\u54c1\u8d28\u7684\u56fe\u50cf\u8d44\u6599\uff0c\u5bfc\u81f4\u8d44\u6599\u4f7f\u7528\u548c\u53ef\u6269\u5c55\u6027\u7684\u4f4e\u6548\u7387\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u81ea\u9002\u5e94\u56fe\u50cf\u6587\u672c\u54c1\u8d28\u5f3a\u5316\u5668 (AITQE)\uff0c\u8fd9\u662f\u4e00\u4e2a\u52a8\u6001\u8bc4\u4f30\u548c\u63d0\u5347\u56fe\u50cf\u6587\u672c\u5bf9\u54c1\u8d28\u7684\u6a21\u578b\u3002AITQE \u5bf9\u4f4e\u54c1\u8d28\u7684\u5bf9\u4f7f\u7528\u6587\u672c\u6539\u5199\u673a\u5236\uff0c\u5e76\u7ed3\u5408\u8d1f\u6837\u672c\u5b66\u4e60\u7b56\u7565\uff0c\u900f\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u6574\u5408\u523b\u610f\u9009\u53d6\u7684\u4f4e\u54c1\u8d28\u6837\u672c\uff0c\u6765\u63d0\u5347\u8bc4\u4f30\u80fd\u529b\u3002\u4e0e\u5927\u5e45\u5ea6\u6539\u53d8\u6587\u672c\u5206\u5e03\u7684\u5148\u524d\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6700\u5c0f\u5e45\u5ea6\u5730\u8c03\u6574\u6587\u672c\uff0c\u4ee5\u5728\u63d0\u5347\u54c1\u8d28\u7684\u540c\u65f6\u4fdd\u7559\u8d44\u6599\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAITQE \u5728\u5404\u79cd\u57fa\u51c6\u4e0a\u90fd\u8d85\u8d8a\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u5229\u7528\u539f\u59cb\u8d44\u6599\uff0c\u5e76\u968f\u7740\u8d44\u6599\u91cf\u7684\u589e\u52a0\u800c\u6709\u6548\u6269\u5c55\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u5de5\u4f5c\u80fd\u542f\u53d1\u672a\u6765\u7684\u7814\u7a76\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u53d6\u5f97\uff1ahttps://github.com/hanhuang22/AITQE\u3002", "author": "Han Huang et.al.", "authors": "Han Huang, Yuqi Huo, Zijia Zhao, Haoyu Lu, Shu Wu, Bingning Wang, Qiang Liu, Weipeng Chen, Liang Wang", "id": "2410.16166v1", "paper_url": "http://arxiv.org/abs/2410.16166v1", "repo": "https://github.com/hanhuang22/aitqe"}}