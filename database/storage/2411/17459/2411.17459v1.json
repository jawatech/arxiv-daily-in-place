{"2411.17459": {"publish_time": "2024-11-26", "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model", "paper_summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.", "paper_summary_zh": "\u5f71\u7247\u53d8\u5f02\u81ea\u52a8\u7f16\u7801\u5668 (VAE) \u5c06\u5f71\u7247\u7f16\u7801\u6210\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u6210\u4e3a\u5927\u591a\u6570\u6f5c\u5728\u5f71\u7247\u6269\u6563\u6a21\u578b (LVDM) \u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u4ee5\u964d\u4f4e\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u3002\u7136\u800c\uff0c\u968f\u7740\u751f\u6210\u5f71\u7247\u7684\u5206\u8fa8\u7387\u548c\u957f\u5ea6\u589e\u52a0\uff0c\u5f71\u7247 VAE \u7684\u7f16\u7801\u6210\u672c\u6210\u4e3a\u8bad\u7ec3 LVDM \u7684\u9650\u5236\u6027\u74f6\u9888\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570 LVDM \u91c7\u7528\u7684\u533a\u5757\u5f0f\u63a8\u7406\u65b9\u6cd5\u5728\u5904\u7406\u957f\u65f6\u95f4\u5f71\u7247\u65f6\u53ef\u80fd\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u7684\u4e0d\u8fde\u7eed\u6027\u3002\u89e3\u51b3\u8ba1\u7b97\u74f6\u9888\u7684\u5173\u952e\u5728\u4e8e\u5c06\u5f71\u7247\u5206\u89e3\u6210\u4e0d\u540c\u7684\u7ec4\u4ef6\uff0c\u5e76\u6709\u6548\u7f16\u7801\u5173\u952e\u4fe1\u606f\u3002\u5c0f\u6ce2\u53d8\u6362\u53ef\u4ee5\u5c06\u5f71\u7247\u5206\u89e3\u6210\u591a\u4e2a\u9891\u57df\u7ec4\u4ef6\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u6548\u7387\uff0c\u56e0\u6b64\u6211\u4eec\u63d0\u51fa\u4e86\u5c0f\u6ce2\u6d41 VAE (WF-VAE)\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u591a\u7ea7\u5c0f\u6ce2\u53d8\u6362\u4fc3\u8fdb\u4f4e\u9891\u80fd\u91cf\u6d41\u5165\u6f5c\u5728\u8868\u793a\u7684\u81ea\u52a8\u7f16\u7801\u5668\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u79f0\u4e3a\u56e0\u679c\u7f13\u5b58\u7684\u65b9\u6cd5\uff0c\u5b83\u5728\u533a\u5757\u5f0f\u63a8\u7406\u671f\u95f4\u4fdd\u6301\u6f5c\u5728\u7a7a\u95f4\u7684\u5b8c\u6574\u6027\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u5f71\u7247 VAE \u76f8\u6bd4\uff0cWF-VAE \u5728 PSNR \u548c LPIPS \u6307\u6807\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86 2 \u500d\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u548c 4 \u500d\u66f4\u4f4e\u7684\u5185\u5b58\u6d88\u8017\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/PKU-YuanGroup/WF-VAE \u83b7\u5f97\u3002", "author": "Zongjian Li et.al.", "authors": "Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, Li Yuan", "id": "2411.17459v1", "paper_url": "http://arxiv.org/abs/2411.17459v1", "repo": "https://github.com/pku-yuangroup/wf-vae"}}