{"2408.03735": {"publish_time": "2024-08-07", "title": "Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation", "paper_summary": "This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.", "paper_summary_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u9879\u7814\u7a76\uff0c\u63a2\u8ba8\u53c2\u6570\u91cf\u5316\u5728\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u7f13\u89e3\u5728\u89c6\u89c9\u8bed\u8a00\u6307\u4ee4\u8c03\u6574\u671f\u95f4\u9047\u5230\u7684\u91cd\u5927\u8d44\u6e90\u9650\u5236\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u9884\u70ed\u91cf\u5316\u611f\u77e5\u91cf\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u79f0\u4e3a QSLAW\u3002\u6b64\u65b9\u6cd5\u57fa\u4e8e\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a(1) \u5b66\u4e60\u91cf\u5316 LLM \u6743\u91cd\u7684\u7ec4\u7ea7\u6bd4\u4f8b\u56e0\u5b50\uff0c\u4ee5\u51cf\u8f7b\u7531\u6fc0\u6d3b\u503c\u5f02\u5e38\u503c\u5f15\u8d77\u7684\u91cf\u5316\u8bef\u5dee\uff0c\u5e76\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6307\u4ee4\u8c03\u6574\uff1b(2) \u5b9e\u65bd\u591a\u6a21\u6001\u9884\u70ed\uff0c\u9010\u6e10\u96c6\u6210\u8bed\u8a00\u548c\u591a\u6a21\u6001\u8bad\u7ec3\u6837\u672c\uff0c\u4ece\u800c\u9632\u6b62\u91cf\u5316\u6a21\u578b\u8fc7\u5ea6\u62df\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u540c\u65f6\u786e\u4fdd\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7a33\u5b9a\u9002\u5e94\u4e0b\u6e38\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u7531 QSLAW \u91cf\u5316\u7684\u6a21\u578b\u6027\u80fd\u4e0e\u5b83\u4eec\u7684\u5b8c\u5168\u7cbe\u5ea6\u5bf9\u5e94\u6a21\u578b\u76f8\u5f53\uff0c\u751a\u81f3\u8d85\u8fc7\u5b83\u4eec\uff0c\u540c\u65f6\u5c06 VL \u8c03\u6574\u65f6\u95f4\u548c GPU \u6d88\u8017\u51cf\u5c11\u4e86 1.4 \u500d\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u53d1\u5e03\u5728 https://github.com/xjjxmu/QSLAW\u3002", "author": "Jingjing Xie et.al.", "authors": "Jingjing Xie, Yuxin Zhang, Mingbao Lin, Liujuan Cao, Rongrong Ji", "id": "2408.03735v1", "paper_url": "http://arxiv.org/abs/2408.03735v1", "repo": "https://github.com/xjjxmu/qslaw"}}