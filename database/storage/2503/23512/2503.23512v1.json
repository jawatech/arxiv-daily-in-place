{"2503.23512": {"publish_time": "2025-03-30", "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives", "paper_summary": "Large Language Models (LLMs) excel at generating creative narratives but\nstruggle with long-term coherence and emotional consistency in complex stories.\nTo address this, we propose SCORE (Story Coherence and Retrieval Enhancement),\na framework integrating three components: 1) Dynamic State Tracking (monitoring\nobjects/characters via symbolic logic), 2) Context-Aware Summarization\n(hierarchical episode summaries for temporal progression), and 3) Hybrid\nRetrieval (combining TF-IDF keyword relevance with cosine similarity-based\nsemantic embeddings). The system employs a temporally-aligned\nRetrieval-Augmented Generation (RAG) pipeline to validate contextual\nconsistency. Evaluations show SCORE achieves 23.6% higher coherence (NCI-2.0\nbenchmark), 89.7% emotional consistency (EASM metric), and 41.8% fewer\nhallucinations versus baseline GPT models. Its modular design supports\nincremental knowledge graph construction for persistent story memory and\nmulti-LLM backend compatibility, offering an explainable solution for\nindustrial-scale narrative systems requiring long-term consistency.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u64c5\u9577\u751f\u6210\u5275\u610f\u6558\u4e8b\uff0c\u4f46\u5728\u8907\u96dc\u6545\u4e8b\u4e2d\u96e3\u4ee5\u4fdd\u6301\u9577\u671f\u9023\u8cab\u6027\u548c\u60c5\u611f\u4e00\u81f4\u6027\u3002\u70ba\u4e86\u61c9\u5c0d\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 SCORE\uff08\u6545\u4e8b\u9023\u8cab\u6027\u548c\u6aa2\u7d22\u589e\u5f37\uff09\uff0c\u9019\u662f\u4e00\u500b\u6574\u5408\u4e86\u4e09\u500b\u7d44\u4ef6\u7684\u6846\u67b6\uff1a1) \u52d5\u614b\u72c0\u614b\u8ddf\u8e2a\uff08\u901a\u904e\u7b26\u865f\u908f\u8f2f\u76e3\u63a7\u5c0d\u8c61/\u89d2\u8272\uff09\uff0c2) \u4e0a\u4e0b\u6587\u611f\u77e5\u6458\u8981\uff08\u7528\u65bc\u6642\u9593\u9032\u5c55\u7684\u5206\u5c64\u60c5\u7bc0\u6458\u8981\uff09\uff0c\u4ee5\u53ca 3) \u6df7\u5408\u6aa2\u7d22\uff08\u7d50\u5408 TF-IDF \u95dc\u9375\u5b57\u76f8\u95dc\u6027\u548c\u57fa\u65bc\u9918\u5f26\u76f8\u4f3c\u5ea6\u7684\u8a9e\u7fa9\u5d4c\u5165\uff09\u3002\u8a72\u7cfb\u7d71\u63a1\u7528\u6642\u9593\u5c0d\u9f4a\u7684\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u7ba1\u9053\u4f86\u9a57\u8b49\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u3002\u8a55\u4f30\u986f\u793a\uff0c\u8207\u57fa\u6e96 GPT \u6a21\u578b\u76f8\u6bd4\uff0cSCORE \u7684\u9023\u8cab\u6027\u63d0\u9ad8\u4e86 23.6%\uff08NCI-2.0\u57fa\u6e96\uff09\uff0c\u60c5\u611f\u4e00\u81f4\u6027\u63d0\u9ad8\u4e86 89.7%\uff08EASM \u6307\u6a19\uff09\uff0c\u932f\u8aa4\u8a0a\u606f\u6e1b\u5c11\u4e86 41.8%\u3002\u5176\u6a21\u584a\u5316\u8a2d\u8a08\u652f\u6301\u589e\u91cf\u77e5\u8b58\u5716\u8b5c\u69cb\u5efa\uff0c\u4ee5\u5be6\u73fe\u6301\u4e45\u7684\u6545\u4e8b\u8a18\u61b6\u548c\u591a LLM \u5f8c\u7aef\u517c\u5bb9\u6027\uff0c\u70ba\u9700\u8981\u9577\u671f\u4e00\u81f4\u6027\u7684\u5de5\u696d\u7d1a\u6558\u4e8b\u7cfb\u7d71\u63d0\u4f9b\u53ef\u89e3\u91cb\u7684\u89e3\u6c7a\u65b9\u6848\u3002</paragraph>\n", "author": "Qiang Yi et.al.", "authors": "Qiang Yi, Yangfan He, Jianhui Wang, Xinyuan Song, Shiyao Qian, Miao Zhang, Li Sun, Tianyu Shi", "id": "2503.23512v1", "paper_url": "http://arxiv.org/abs/2503.23512v1", "repo": "null"}}