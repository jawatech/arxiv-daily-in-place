{"2411.10928": {"publish_time": "2024-11-17", "title": "Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning", "paper_summary": "Multimodal Large Language Model (MLLM) have demonstrated strong\ngeneralization capabilities across diverse distributions and tasks, largely due\nto extensive pre-training datasets. Fine-tuning MLLM has become a common\npractice to improve performance on specific downstream tasks. However, during\nfine-tuning, MLLM often faces the risk of forgetting knowledge acquired during\npre-training, which can result in a decline in generalization abilities. To\nbalance the trade-off between generalization and specialization, we propose\nmeasuring the parameter importance for both pre-trained and fine-tuning\ndistributions, based on frozen pre-trained weight magnitude and accumulated\nfine-tuning gradient values. We further apply an importance-aware weight\nallocation strategy, selectively updating relatively important parameters for\ndownstream tasks. We conduct empirical evaluations on both image captioning and\nvisual question-answering tasks using various MLLM architectures. The\ncomprehensive experimental analysis demonstrates the effectiveness of the\nproposed solution, highlighting the efficiency of the crucial modules in\nenhancing downstream specialization performance while mitigating generalization\ndegradation in MLLM Fine-Tuning.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5728\u5404\u79cd\u5206\u5e03\u548c\u4efb\u52a1\u4e2d\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5f52\u529f\u4e8e\u5e7f\u6cdb\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u5fae\u8c03 MLLM \u5df2\u6210\u4e3a\u63d0\u9ad8\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5e38\u89c1\u505a\u6cd5\u3002\u7136\u800c\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0cMLLM \u7ecf\u5e38\u9762\u4e34\u5fd8\u8bb0\u9884\u8bad\u7ec3\u671f\u95f4\u83b7\u5f97\u7684\u77e5\u8bc6\u7684\u98ce\u9669\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002\u4e3a\u4e86\u5e73\u8861\u6cdb\u5316\u548c\u4e13\u4e1a\u5316\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u6211\u4eec\u5efa\u8bae\u6d4b\u91cf\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u5206\u5e03\u7684\u53c2\u6570\u91cd\u8981\u6027\uff0c\u57fa\u4e8e\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u5e45\u5ea6\u548c\u7d2f\u79ef\u7684\u5fae\u8c03\u68af\u5ea6\u503c\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5e94\u7528\u4e86\u91cd\u8981\u6027\u611f\u77e5\u6743\u91cd\u5206\u914d\u7b56\u7565\uff0c\u6709\u9009\u62e9\u5730\u66f4\u65b0\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u76f8\u5bf9\u91cd\u8981\u53c2\u6570\u3002\u6211\u4eec\u4f7f\u7528\u5404\u79cd MLLM \u67b6\u6784\u5bf9\u56fe\u50cf\u5b57\u5e55\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u8fdb\u884c\u4e86\u7ecf\u9a8c\u8bc4\u4f30\u3002\u5168\u9762\u7684\u5b9e\u9a8c\u5206\u6790\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u5f3a\u8c03\u4e86\u5173\u952e\u6a21\u5757\u5728\u63d0\u9ad8\u4e0b\u6e38\u4e13\u4e1a\u5316\u6027\u80fd\u7684\u540c\u65f6\u51cf\u8f7b MLLM \u5fae\u8c03\u4e2d\u6cdb\u5316\u9000\u5316\u7684\u6548\u7387\u3002", "author": "Wenke Huang et.al.", "authors": "Wenke Huang, Jian Liang, Zekun Shi, Didi Zhu, Guancheng Wan, He Li, Bo Du, Dacheng Tao, Mang Ye", "id": "2411.10928v1", "paper_url": "http://arxiv.org/abs/2411.10928v1", "repo": "null"}}