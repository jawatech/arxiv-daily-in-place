{"2406.15193": {"publish_time": "2024-06-21", "title": "Reward Steering with Evolutionary Heuristics for Decoding-time Alignment", "paper_summary": "The widespread applicability and increasing omnipresence of LLMs have\ninstigated a need to align LLM responses to user and stakeholder preferences.\nMany preference optimization approaches have been proposed that fine-tune LLM\nparameters to achieve good alignment. However, such parameter tuning is known\nto interfere with model performance on many tasks. Moreover, keeping up with\nshifting user preferences is tricky in such a situation. Decoding-time\nalignment with reward model guidance solves these issues at the cost of\nincreased inference time. However, most of such methods fail to strike the\nright balance between exploration and exploitation of reward -- often due to\nthe conflated formulation of these two aspects - to give well-aligned\nresponses. To remedy this we decouple these two aspects and implement them in\nan evolutionary fashion: exploration is enforced by decoding from mutated\ninstructions and exploitation is represented as the periodic replacement of\npoorly-rewarded generations with well-rewarded ones. Empirical evidences\nindicate that this strategy outperforms many preference optimization and\ndecode-time alignment approaches on two widely accepted alignment benchmarks\nAlpacaEval 2 and MT-Bench. Our implementation will be available at:\nhttps://darwin-alignment.github.io.", "paper_summary_zh": "\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c LLM \u65e5\u76ca\u666e\u53ca\u7684\u65e0\u6240\u4e0d\u5728\u6027\u5f15\u53d1\u4e86\u5c06 LLM \u54cd\u5e94\u4e0e\u7528\u6237\u548c\u5229\u76ca\u76f8\u5173\u8005\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u7684\u9700\u6c42\u3002\u5df2\u7ecf\u63d0\u51fa\u4e86\u8bb8\u591a\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u5bf9 LLM \u53c2\u6570\u8fdb\u884c\u5fae\u8c03\u4ee5\u5b9e\u73b0\u826f\u597d\u7684\u5bf9\u9f50\u3002\u7136\u800c\uff0c\u4f17\u6240\u5468\u77e5\uff0c\u8fd9\u79cd\u53c2\u6570\u8c03\u6574\u4f1a\u5e72\u6270\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8ddf\u4e0a\u4e0d\u65ad\u53d8\u5316\u7684\u7528\u6237\u504f\u597d\u662f\u4e00\u4ef6\u68d8\u624b\u7684\u4e8b\u60c5\u3002\u4f7f\u7528\u5956\u52b1\u6a21\u578b\u6307\u5bfc\u8fdb\u884c\u89e3\u7801\u65f6\u5bf9\u9f50\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u4ee3\u4ef7\u662f\u589e\u52a0\u4e86\u63a8\u7406\u65f6\u95f4\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u6b64\u7c7b\u65b9\u6cd5\u672a\u80fd\u5f88\u597d\u5730\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u5956\u52b1\u2014\u2014\u901a\u5e38\u662f\u7531\u4e8e\u8fd9\u4e24\u4e2a\u65b9\u9762\u7684\u6df7\u6dc6\u8868\u8ff0\u2014\u2014\u4ee5\u7ed9\u51fa\u5f88\u597d\u5730\u5bf9\u9f50\u7684\u54cd\u5e94\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5c06\u8fd9\u4e24\u4e2a\u65b9\u9762\u89e3\u8026\uff0c\u5e76\u4ee5\u8fdb\u5316\u65b9\u5f0f\u5b9e\u73b0\u5b83\u4eec\uff1a\u901a\u8fc7\u4ece\u7a81\u53d8\u6307\u4ee4\u4e2d\u89e3\u7801\u6765\u5f3a\u5236\u6267\u884c\u63a2\u7d22\uff0c\u5e76\u8868\u793a\u4e3a\u5b9a\u671f\u7528\u5956\u52b1\u8f83\u597d\u7684\u4e16\u4ee3\u66ff\u6362\u5956\u52b1\u8f83\u5dee\u7684\u4e16\u4ee3\u3002\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\uff0c\u8be5\u7b56\u7565\u5728\u4e24\u4e2a\u5e7f\u6cdb\u63a5\u53d7\u7684\u5bf9\u9f50\u57fa\u51c6 AlpacaEval 2 \u548c MT-Bench \u4e0a\u4f18\u4e8e\u8bb8\u591a\u504f\u597d\u4f18\u5316\u548c\u89e3\u7801\u65f6\u5bf9\u9f50\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u5b9e\u73b0\u5c06\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u63d0\u4f9b\uff1ahttps://darwin-alignment.github.io\u3002", "author": "Chia-Yu Hung et.al.", "authors": "Chia-Yu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria", "id": "2406.15193v1", "paper_url": "http://arxiv.org/abs/2406.15193v1", "repo": "null"}}