{"2412.08635": {"publish_time": "2024-12-11", "title": "Multimodal Latent Language Modeling with Next-Token Diffusion", "paper_summary": "Multimodal generative models require a unified approach to handle both\ndiscrete data (e.g., text and code) and continuous data (e.g., image, audio,\nvideo). In this work, we propose Latent Language Modeling (LatentLM), which\nseamlessly integrates continuous and discrete data using causal Transformers.\nSpecifically, we employ a variational autoencoder (VAE) to represent continuous\ndata as latent vectors and introduce next-token diffusion for autoregressive\ngeneration of these vectors. Additionally, we develop $\\sigma$-VAE to address\nthe challenges of variance collapse, which is crucial for autoregressive\nmodeling. Extensive experiments demonstrate the effectiveness of LatentLM\nacross various modalities. In image generation, LatentLM surpasses Diffusion\nTransformers in both performance and scalability. When integrated into\nmultimodal large language models, LatentLM provides a general-purpose interface\nthat unifies multimodal generation and understanding. Experimental results show\nthat LatentLM achieves favorable performance compared to Transfusion and vector\nquantized models in the setting of scaling up training tokens. In\ntext-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2\nmodel in speaker similarity and robustness, while requiring 10x fewer decoding\nsteps. The results establish LatentLM as a highly effective and scalable\napproach to advance large multimodal models.", "paper_summary_zh": "\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u5904\u7406\u79bb\u6563\u6570\u636e\uff08\u4f8b\u5982\u6587\u672c\u548c\u4ee3\u7801\uff09\u548c\u8fde\u7eed\u6570\u636e\uff08\u4f8b\u5982\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\uff09\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6f5c\u5728\u8bed\u8a00\u6a21\u578b\uff08LatentLM\uff09\uff0c\u5b83\u4f7f\u7528\u56e0\u679c Transformer \u65e0\u7f1d\u5730\u96c6\u6210\u4e86\u8fde\u7eed\u6570\u636e\u548c\u79bb\u6563\u6570\u636e\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668 (VAE) \u5c06\u8fde\u7eed\u6570\u636e\u8868\u793a\u4e3a\u6f5c\u5728\u5411\u91cf\uff0c\u5e76\u5f15\u5165\u4e0b\u4e00\u4e2a\u6807\u8bb0\u6269\u6563\u6765\u81ea\u52a8\u56de\u5f52\u751f\u6210\u8fd9\u4e9b\u5411\u91cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 \u03c3-VAE \u6765\u89e3\u51b3\u65b9\u5dee\u574d\u7f29\u7684\u6311\u6218\uff0c\u8fd9\u5bf9\u81ea\u56de\u5f52\u5efa\u6a21\u81f3\u5173\u91cd\u8981\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e LatentLM \u5728\u5404\u79cd\u6a21\u6001\u4e2d\u90fd\u5f88\u6709\u6548\u3002\u5728\u56fe\u50cf\u751f\u6210\u4e2d\uff0cLatentLM \u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u90fd\u8d85\u8d8a\u4e86\u6269\u6563 Transformer\u3002\u5f53\u96c6\u6210\u5230\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u65f6\uff0cLatentLM \u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u63a5\u53e3\uff0c\u5b83\u7edf\u4e00\u4e86\u591a\u6a21\u6001\u751f\u6210\u548c\u7406\u89e3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6269\u5c55\u8bad\u7ec3\u6807\u8bb0\u7684\u8bbe\u7f6e\u4e2d\uff0c\u4e0e Transfusion \u548c\u77e2\u91cf\u91cf\u5316\u6a21\u578b\u76f8\u6bd4\uff0cLatentLM \u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002\u5728\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\u4e2d\uff0cLatentLM \u5728\u8bf4\u8bdd\u8005\u76f8\u4f3c\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 VALL-E 2 \u6a21\u578b\uff0c\u540c\u65f6\u9700\u8981\u7684\u89e3\u7801\u6b65\u9aa4\u51cf\u5c11\u4e86 10 \u500d\u3002\u7ed3\u679c\u8868\u660e LatentLM \u662f\u4e00\u79cd\u975e\u5e38\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63a8\u8fdb\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u3002", "author": "Yutao Sun et.al.", "authors": "Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, Furu Wei", "id": "2412.08635v1", "paper_url": "http://arxiv.org/abs/2412.08635v1", "repo": "null"}}