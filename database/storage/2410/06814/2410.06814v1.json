{"2410.06814": {"publish_time": "2024-10-09", "title": "Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning", "paper_summary": "Over-parameterized models are typically vulnerable to membership inference\nattacks, which aim to determine whether a specific sample is included in the\ntraining of a given model. Previous Weight regularizations (e.g., L1\nregularization) typically impose uniform penalties on all parameters, leading\nto a suboptimal tradeoff between model utility and privacy. In this work, we\nfirst show that only a small fraction of parameters substantially impact the\nprivacy risk. In light of this, we propose Privacy-aware Sparsity Tuning\n(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties\nto different parameters. Our key idea behind PAST is to promote sparsity in\nparameters that significantly contribute to privacy leakage. In particular, we\nconstruct the adaptive weight for each parameter based on its privacy\nsensitivity, i.e., the gradient of the loss gap with respect to the parameter.\nUsing PAST, the network shrinks the loss gap between members and non-members,\nleading to strong resistance to privacy attacks. Extensive experiments\ndemonstrate the superiority of PAST, achieving a state-of-the-art balance in\nthe privacy-utility trade-off.", "paper_summary_zh": "\u904e\u5ea6\u53c3\u6578\u5316\u7684\u6a21\u578b\u901a\u5e38\u5bb9\u6613\u53d7\u5230\u6210\u54e1\u63a8\u8ad6\u653b\u64ca\uff0c\u5176\u76ee\u7684\u662f\u78ba\u5b9a\u7279\u5b9a\u7bc4\u4f8b\u662f\u5426\u5305\u542b\u5728\u7d66\u5b9a\u6a21\u578b\u7684\u8a13\u7df4\u4e2d\u3002\u5148\u524d\u7684\u6b0a\u91cd\u6b63\u5247\u5316\uff08\u4f8b\u5982 L1 \u6b63\u5247\u5316\uff09\u901a\u5e38\u5c0d\u6240\u6709\u53c3\u6578\u65bd\u52a0\u5747\u52fb\u7684\u61f2\u7f70\uff0c\u5c0e\u81f4\u6a21\u578b\u5be6\u7528\u6027\u8207\u96b1\u79c1\u4e4b\u9593\u7684\u6b0a\u8861\u53d6\u6368\u4e0d\u4f73\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u5148\u5c55\u793a\u53ea\u6709\u5c11\u90e8\u5206\u53c3\u6578\u6703\u5927\u5e45\u5f71\u97ff\u96b1\u79c1\u98a8\u96aa\u3002\u6709\u9451\u65bc\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u5177\u6709\u96b1\u79c1\u610f\u8b58\u7684\u7a00\u758f\u8abf\u6574 (PAST)\uff0c\u4e00\u7a2e\u900f\u904e\u5c0d\u4e0d\u540c\u53c3\u6578\u63a1\u7528\u81ea\u9069\u61c9\u61f2\u7f70\u4f86\u5c0d L1 \u6b63\u5247\u5316\u9032\u884c\u7c21\u55ae\u4fee\u6b63\u7684\u65b9\u6cd5\u3002PAST \u80cc\u5f8c\u7684\u4e3b\u8981\u6982\u5ff5\u662f\u4fc3\u9032\u5c0d\u5c0e\u81f4\u96b1\u79c1\u5916\u6d29\u6709\u91cd\u5927\u5f71\u97ff\u7684\u53c3\u6578\u4e2d\u7684\u7a00\u758f\u6027\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u6839\u64da\u6bcf\u500b\u53c3\u6578\u7684\u96b1\u79c1\u654f\u611f\u5ea6\uff08\u5373\u640d\u5931\u5dee\u8ddd\u76f8\u5c0d\u65bc\u53c3\u6578\u7684\u68af\u5ea6\uff09\u4f86\u5efa\u69cb\u81ea\u9069\u61c9\u6b0a\u91cd\u3002\u4f7f\u7528 PAST\uff0c\u7db2\u8def\u6703\u7e2e\u5c0f\u6210\u54e1\u548c\u975e\u6210\u54e1\u4e4b\u9593\u7684\u640d\u5931\u5dee\u8ddd\uff0c\u5f9e\u800c\u5c0d\u96b1\u79c1\u653b\u64ca\u7522\u751f\u5f37\u5927\u7684\u62b5\u6297\u529b\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86 PAST \u7684\u512a\u8d8a\u6027\uff0c\u5728\u96b1\u79c1\u5be6\u7528\u6027\u6b0a\u8861\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u5e73\u8861\u3002", "author": "Qiang Hu et.al.", "authors": "Qiang Hu, Hengxiang Zhang, Hongxin Wei", "id": "2410.06814v1", "paper_url": "http://arxiv.org/abs/2410.06814v1", "repo": "null"}}