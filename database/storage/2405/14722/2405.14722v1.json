{"2405.14722": {"publish_time": "2024-05-23", "title": "CAPE: Context-Adaptive Positional Encoding for Length Extrapolation", "paper_summary": "Positional encoding plays a crucial role in transformers, significantly\nimpacting model performance and length generalization. Prior research has\nintroduced absolute positional encoding (APE) and relative positional encoding\n(RPE) to distinguish token positions in given sequences. However, both APE and\nRPE remain fixed after model training regardless of input data, limiting their\nadaptability and flexibility. Hence, we expect that the desired positional\nencoding should be context-adaptive and can be dynamically adjusted with the\ngiven attention. In this paper, we propose a Context-Adaptive Positional\nEncoding (CAPE) method, which dynamically and semantically adjusts based on\ninput context and learned fixed priors. Experimental validation on real-world\ndatasets (Arxiv, Books3, and CHE) demonstrates that CAPE enhances model\nperformances in terms of trained length and length generalization, where the\nimprovements are statistically significant. The model visualization suggests\nthat our model can keep both local and anti-local information. Finally, we\nsuccessfully train the model on sequence length 128 and achieve better\nperformance at evaluation sequence length 8192, compared with other static\npositional encoding methods, revealing the benefit of the adaptive positional\nencoding method.", "paper_summary_zh": "\u4f4d\u7f6e\u7de8\u78bc\u5728 Transformer \u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\uff0c\u5b83\u6703\u986f\u8457\u5f71\u97ff\u6a21\u578b\u6548\u80fd\u548c\u9577\u5ea6\u6cdb\u5316\u3002\u5148\u524d\u7684\u7814\u7a76\u5f15\u5165\u4e86\u7d55\u5c0d\u4f4d\u7f6e\u7de8\u78bc (APE) \u548c\u76f8\u5c0d\u4f4d\u7f6e\u7de8\u78bc (RPE) \u4f86\u5340\u5206\u7d66\u5b9a\u5e8f\u5217\u4e2d\u7684\u4ee3\u5e63\u4f4d\u7f6e\u3002\u7136\u800c\uff0cAPE \u548c RPE \u5728\u6a21\u578b\u8a13\u7df4\u5f8c\u4ecd\u7136\u56fa\u5b9a\u4e0d\u8b8a\uff0c\u8207\u8f38\u5165\u8cc7\u6599\u7121\u95dc\uff0c\u9019\u9650\u5236\u4e86\u5b83\u5011\u7684\u9069\u61c9\u6027\u548c\u9748\u6d3b\u6027\u3002\u56e0\u6b64\uff0c\u6211\u5011\u9810\u671f\u6240\u9700\u7684\u5b9a\u4f4d\u7de8\u78bc\u61c9\u5177\u5099\u9069\u61c9\u8a9e\u5883\u7684\u7279\u6027\uff0c\u4e26\u53ef\u6839\u64da\u7d66\u5b9a\u7684\u6ce8\u610f\u529b\u9032\u884c\u52d5\u614b\u8abf\u6574\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u8a9e\u5883\u9069\u61c9\u4f4d\u7f6e\u7de8\u78bc (CAPE) \u65b9\u6cd5\uff0c\u5b83\u6703\u6839\u64da\u8f38\u5165\u8a9e\u5883\u548c\u5b78\u7fd2\u5230\u7684\u56fa\u5b9a\u5148\u9a57\u9032\u884c\u52d5\u614b\u4e14\u8a9e\u7fa9\u5316\u7684\u8abf\u6574\u3002\u5728\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6 (Arxiv\u3001Books3 \u548c CHE) \u4e0a\u9032\u884c\u7684\u5be6\u9a57\u9a57\u8b49\u8868\u660e\uff0cCAPE \u5728\u8a13\u7df4\u9577\u5ea6\u548c\u9577\u5ea6\u6cdb\u5316\u65b9\u9762\u589e\u5f37\u4e86\u6a21\u578b\u6548\u80fd\uff0c\u5176\u4e2d\u6539\u9032\u5177\u6709\u7d71\u8a08\u986f\u8457\u6027\u3002\u6a21\u578b\u8996\u89ba\u5316\u986f\u793a\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u4fdd\u7559\u5c40\u90e8\u548c\u53cd\u5c40\u90e8\u8cc7\u8a0a\u3002\u6700\u5f8c\uff0c\u6211\u5011\u6210\u529f\u5730\u8a13\u7df4\u4e86\u5e8f\u5217\u9577\u5ea6\u70ba 128 \u7684\u6a21\u578b\uff0c\u4e26\u5728\u8a55\u4f30\u5e8f\u5217\u9577\u5ea6\u70ba 8192 \u6642\u7372\u5f97\u4e86\u6bd4\u5176\u4ed6\u975c\u614b\u4f4d\u7f6e\u7de8\u78bc\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u80fd\uff0c\u9019\u63ed\u793a\u4e86\u81ea\u9069\u61c9\u4f4d\u7f6e\u7de8\u78bc\u65b9\u6cd5\u7684\u512a\u9ede\u3002", "author": "Chuanyang Zheng et.al.", "authors": "Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li", "id": "2405.14722v1", "paper_url": "http://arxiv.org/abs/2405.14722v1", "repo": "https://github.com/chuanyang-zheng/cape"}}