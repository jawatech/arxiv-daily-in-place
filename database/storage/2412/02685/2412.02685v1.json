{"2412.02685": {"publish_time": "2024-12-03", "title": "T-REG: Preference Optimization with Token-Level Reward Regularization", "paper_summary": "Reinforcement learning from human feedback (RLHF) has been crucial in\naligning large language models (LLMs) with human values. Traditionally, RLHF\ninvolves generating responses to a query and using a reward model to assign a\nreward to the entire response. However, this approach faces challenges due to\nits reliance on a single, sparse reward, which makes it challenging for the\nmodel to identify which parts of the sequence contribute most significantly to\nthe final reward. Recent methods have attempted to address this limitation by\nintroducing token-level rewards. However, these methods often rely on either a\ntrained credit assignment model or AI annotators, raising concerns about the\nquality and reliability of the rewards. In this paper, we propose token-level\nreward regularization (T-REG), a novel approach that leverages both\nsequence-level and token-level rewards for preference optimization. Harnessing\nthe self-refinement capabilities of LLMs, our method uses contrastive prompting\nto enable LLMs to self-generate token-level rewards. These self-generated\nrewards then act as reward regularization, guiding the model to more\neffectively distribute sequence-level rewards across tokens. This facilitates\nbetter token-level credit assignment and enhances alignment performance.\nExperiments on the instruction following benchmarks, including Alpaca Eval 2\nand Arena-Hard, show that our method consistently outperforms baseline methods\nby up to 3.8% and 4.4%, respectively. We will release the code and models at\nhttps://github.com/wzhouad/T-REG.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5728\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u50f9\u503c\u89c0\u4fdd\u6301\u4e00\u81f4\u65b9\u9762\u81f3\u95dc\u91cd\u8981\u3002\u50b3\u7d71\u4e0a\uff0cRLHF \u6d89\u53ca\u5c0d\u67e5\u8a62\u7522\u751f\u56de\u61c9\uff0c\u4e26\u4f7f\u7528\u734e\u52f5\u6a21\u578b\u5c0d\u6574\u500b\u56de\u61c9\u5206\u914d\u734e\u52f5\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u7531\u65bc\u4f9d\u8cf4\u65bc\u55ae\u4e00\u7684\u3001\u7a00\u758f\u7684\u734e\u52f5\u800c\u9762\u81e8\u6311\u6230\uff0c\u9019\u4f7f\u5f97\u6a21\u578b\u96e3\u4ee5\u8b58\u5225\u5e8f\u5217\u7684\u54ea\u4e9b\u90e8\u5206\u5c0d\u6700\u7d42\u734e\u52f5\u7684\u8ca2\u737b\u6700\u5927\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u5df2\u5617\u8a66\u901a\u904e\u5f15\u5165\u4ee4\u724c\u7d1a\u5225\u734e\u52f5\u4f86\u89e3\u6c7a\u6b64\u9650\u5236\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u65bc\u8a13\u7df4\u6709\u7d20\u7684\u4fe1\u7528\u5206\u914d\u6a21\u578b\u6216 AI \u6ce8\u89e3\u5668\uff0c\u5f9e\u800c\u5f15\u767c\u4e86\u5c0d\u734e\u52f5\u7684\u8cea\u91cf\u548c\u53ef\u9760\u6027\u7684\u64d4\u6182\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4ee4\u724c\u7d1a\u5225\u734e\u52f5\u6b63\u5247\u5316 (T-REG)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5e8f\u5217\u7d1a\u5225\u548c\u4ee4\u724c\u7d1a\u5225\u734e\u52f5\u9032\u884c\u504f\u597d\u512a\u5316\u3002\u5229\u7528 LLM \u7684\u81ea\u6211\u512a\u5316\u80fd\u529b\uff0c\u6211\u5011\u7684\u6a21\u578b\u4f7f\u7528\u5c0d\u6bd4\u63d0\u793a\u4f86\u4f7f LLM \u80fd\u5920\u81ea\u6211\u7522\u751f\u4ee4\u724c\u7d1a\u5225\u734e\u52f5\u3002\u9019\u4e9b\u81ea\u6211\u7522\u751f\u7684\u734e\u52f5\u96a8\u5f8c\u5145\u7576\u734e\u52f5\u6b63\u5247\u5316\uff0c\u6307\u5c0e\u6a21\u578b\u66f4\u6709\u6548\u5730\u5c07\u5e8f\u5217\u7d1a\u5225\u734e\u52f5\u5206\u914d\u5230\u4ee4\u724c\u4e2d\u3002\u9019\u4fc3\u9032\u4e86\u66f4\u597d\u7684\u4ee4\u724c\u7d1a\u5225\u4fe1\u7528\u5206\u914d\uff0c\u4e26\u589e\u5f37\u4e86\u5c0d\u9f4a\u6027\u80fd\u3002\u5728\u6307\u4ee4\u9075\u5faa\u57fa\u6e96\u6e2c\u8a66\uff08\u5305\u62ec Alpaca Eval 2 \u548c Arena-Hard\uff09\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u59cb\u7d42\u512a\u65bc\u57fa\u6e96\u6a21\u578b\uff0c\u5206\u5225\u9ad8\u9054 3.8% \u548c 4.4%\u3002\u6211\u5011\u5c07\u5728 https://github.com/wzhouad/T-REG \u4e0a\u767c\u5e03\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u3002", "author": "Wenxuan Zhou et.al.", "authors": "Wenxuan Zhou, Shujian Zhang, Lingxiao Zhao, Tao Meng", "id": "2412.02685v1", "paper_url": "http://arxiv.org/abs/2412.02685v1", "repo": "null"}}