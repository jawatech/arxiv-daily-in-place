{"2412.09282": {"publish_time": "2024-12-12", "title": "CRVQ: Channel-relaxed Vector Quantization for Extreme Compression of LLMs", "paper_summary": "Powerful large language models (LLMs) are increasingly expected to be\ndeployed with lower computational costs, enabling their capabilities on\nresource-constrained devices. Post-training quantization (PTQ) has emerged as a\nstar approach to achieve this ambition, with best methods compressing weights\nto less than 2 bit on average. In this paper, we propose Channel-Relaxed Vector\nQuantization (CRVQ), a novel technique that significantly improves the\nperformance of PTQ baselines at the cost of only minimal additional bits. This\nstate-of-the-art extreme compression method achieves its results through two\nkey innovations: (1) carefully selecting and reordering a very small subset of\ncritical weight channels, and (2) leveraging multiple codebooks to relax the\nconstraint of critical channels. With our method, we demonstrate a 38.9%\nimprovement over the current strongest sub-2-bit PTQ baseline, enabling nearer\nlossless 1-bit compression. Furthermore, our approach offers flexible\ncustomization of quantization bit-width and performance, providing a wider\nrange of deployment options for diverse hardware platforms.", "paper_summary_zh": "\u5f37\u5927\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6108\u4f86\u6108\u6709\u671b\u4ee5\u8f03\u4f4e\u7684\u904b\u7b97\u6210\u672c\u90e8\u7f72\uff0c\u8b93\u5b83\u5011\u80fd\u5920\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u88dd\u7f6e\u4e0a\u767c\u63ee\u529f\u80fd\u3002\u8a13\u7df4\u5f8c\u91cf\u5316 (PTQ) \u5df2\u6210\u70ba\u5be6\u73fe\u6b64\u76ee\u6a19\u7684\u660e\u661f\u65b9\u6cd5\uff0c\u5176\u4e2d\u6700\u4f73\u65b9\u6cd5\u5c07\u6b0a\u91cd\u5e73\u5747\u58d3\u7e2e\u5230\u4e0d\u5230 2 \u4f4d\u5143\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u901a\u9053\u653e\u9b06\u5411\u91cf\u91cf\u5316 (CRVQ)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u6280\u8853\uff0c\u50c5\u4ee5\u6975\u5c11\u7684\u984d\u5916\u4f4d\u5143\u70ba\u4ee3\u50f9\uff0c\u5c31\u80fd\u986f\u8457\u63d0\u5347 PTQ \u57fa\u6e96\u7684\u6548\u80fd\u3002\u6b64\u6700\u5148\u9032\u7684\u6975\u7aef\u58d3\u7e2e\u65b9\u6cd5\u900f\u904e\u5169\u9805\u95dc\u9375\u5275\u65b0\u4f86\u9054\u6210\u5176\u6210\u679c\uff1a(1) \u4ed4\u7d30\u9078\u53d6\u4e26\u91cd\u65b0\u6392\u5e8f\u6975\u5c11\u6578\u7684\u95dc\u9375\u6b0a\u91cd\u901a\u9053\uff0c\u4ee5\u53ca (2) \u63a1\u7528\u591a\u500b\u78bc\u672c\u653e\u5bec\u95dc\u9375\u901a\u9053\u7684\u7d04\u675f\u3002\u900f\u904e\u6211\u5011\u7684\u65b9\u6cd5\uff0c\u6211\u5011\u5c55\u793a\u51fa\u6bd4\u76ee\u524d\u6700\u5f37\u7684\u6b21 2 \u4f4d\u5143 PTQ \u57fa\u6e96\u9ad8\u51fa 38.9% \u7684\u63d0\u5347\uff0c\u5be6\u73fe\u66f4\u63a5\u8fd1\u7121\u5931\u771f\u7684 1 \u4f4d\u5143\u58d3\u7e2e\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u505a\u6cd5\u63d0\u4f9b\u91cf\u5316\u4f4d\u5143\u5bec\u5ea6\u548c\u6548\u80fd\u7684\u5f48\u6027\u81ea\u8a02\uff0c\u70ba\u5404\u7a2e\u786c\u9ad4\u5e73\u53f0\u63d0\u4f9b\u66f4\u5ee3\u6cdb\u7684\u90e8\u7f72\u9078\u9805\u3002", "author": "Yuzhuang Xu et.al.", "authors": "Yuzhuang Xu, Shiyu Ji, Qingfu Zhu, Wanxiang Che", "id": "2412.09282v1", "paper_url": "http://arxiv.org/abs/2412.09282v1", "repo": "null"}}