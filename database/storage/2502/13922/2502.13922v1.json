{"2502.13922": {"publish_time": "2025-02-19", "title": "LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, \\ourMethod-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u900f\u904e\u9810\u8a13\u7df4\u548c\u6bd4\u5c0d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5353\u8d8a\u7684\u77ed\u8108\u7d61 LLM \u53ef\u80fd\u6703\u5728\u9577\u8108\u7d61\u5834\u666f\u4e2d\u8868\u73fe\u4e0d\u4f73\uff0c\u56e0\u70ba\u9577\u8108\u7d61\u6bd4\u5c0d\u4e0d\u8db3\u3002\u6b64\u6bd4\u5c0d\u7a0b\u5e8f\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\uff0c\u539f\u56e0\u662f\u91dd\u5c0d\u5ef6\u4f38\u8108\u7d61\u9032\u884c\u4eba\u5de5\u8a3b\u89e3\u4e0d\u5207\u5be6\u969b\uff0c\u4e14\u96e3\u4ee5\u5e73\u8861\u77ed\u8108\u7d61\u548c\u9577\u8108\u7d61\u7684\u6548\u80fd\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86 LongPO\uff0c\u5b83\u80fd\u8b93\u77ed\u8108\u7d61 LLM \u81ea\u6211\u6f14\u5316\uff0c\u900f\u904e\u5167\u90e8\u8f49\u79fb\u77ed\u8108\u7d61\u80fd\u529b\uff0c\u5728\u9577\u8108\u7d61\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002LongPO \u5229\u7528 LLM \u5f9e\u81ea\u6211\u7522\u751f\u7684\u77ed\u5230\u9577\u504f\u597d\u8cc7\u6599\u4e2d\u5b78\u7fd2\uff0c\u5305\u542b\u70ba\u5177\u6709\u9577\u8108\u7d61\u8f38\u5165\u7684\u76f8\u540c\u6307\u4ee4\u7522\u751f\u7684\u914d\u5c0d\u56de\u61c9\uff0c\u4ee5\u53ca\u5b83\u5011\u58d3\u7e2e\u7684\u77ed\u8108\u7d61\u5c0d\u61c9\u9805\u3002\u6b64\u504f\u597d\u63ed\u793a\u4e86 LLM \u5728\u77ed\u8108\u7d61\u6bd4\u5c0d\u671f\u9593\u57f9\u990a\u7684\u80fd\u529b\u548c\u6f5b\u529b\uff0c\u9019\u4e9b\u80fd\u529b\u548c\u6f5b\u529b\u53ef\u80fd\u6703\u5728\u6bd4\u5c0d\u4e0d\u8db3\u7684\u9577\u8108\u7d61\u5834\u666f\u4e2d\u6e1b\u5f31\u3002\u6b64\u5916\uff0cLongPO \u7d50\u5408\u4e86\u77ed\u5230\u9577\u7684 KL \u7d04\u675f\uff0c\u4ee5\u6e1b\u8f15\u9577\u8108\u7d61\u6bd4\u5c0d\u671f\u9593\u7684\u77ed\u8108\u7d61\u6548\u80fd\u4e0b\u964d\u3002\u7576\u61c9\u7528\u65bc Mistral-7B-Instruct-v0.2\uff0c\u5f9e 128K \u5230 512K \u7684\u8108\u7d61\u9577\u5ea6\u6642\uff0cLongPO \u5b8c\u5168\u4fdd\u7559\u4e86\u77ed\u8108\u7d61\u6548\u80fd\uff0c\u4e26\u4e14\u5728\u9577\u8108\u7d61\u548c\u77ed\u8108\u7d61\u4efb\u52d9\u4e2d\u90fd\u5927\u5e45\u512a\u65bc\u6a38\u7d20\u7684 SFT \u548c DPO\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u7d93\u904e\u6211\u5011\u65b9\u6cd5\u8a13\u7df4\u7684\u6a21\u578b\u53ef\u4ee5\u5728\u9577\u8108\u7d61\u57fa\u6e96\u4e0a\u53d6\u5f97\u8207\u5353\u8d8a LLM\uff08\u4f8b\u5982\uff0cGPT-4-128K\uff09\u76f8\u7576\u751a\u81f3\u8d85\u8d8a\u7684\u7d50\u679c\uff0c\u800c\u5353\u8d8a LLM \u6d89\u53ca\u5ee3\u6cdb\u7684\u9577\u8108\u7d61\u8a3b\u89e3\u548c\u66f4\u5927\u7684\u53c3\u6578\u898f\u6a21\u3002", "author": "Guanzheng Chen et.al.", "authors": "Guanzheng Chen, Xin Li, Michael Qizhe Shieh, Lidong Bing", "id": "2502.13922v1", "paper_url": "http://arxiv.org/abs/2502.13922v1", "repo": "https://github.com/DAMO-NLP-SG/LongPO"}}