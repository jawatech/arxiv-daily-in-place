{"2412.15978": {"publish_time": "2024-12-20", "title": "BabyHGRN: Exploring RNNs for Sample-Efficient Training of Language Models", "paper_summary": "This paper explores the potential of recurrent neural networks (RNNs) and\nother subquadratic architectures as competitive alternatives to\ntransformer-based models in low-resource language modeling scenarios. We\nutilize HGRN2 (Qin et al., 2024), a recently proposed RNN-based architecture,\nand comparatively evaluate its effectiveness against transformer-based\nbaselines and other subquadratic architectures (LSTM, xLSTM, Mamba). Our\nexperimental results show that BABYHGRN, our HGRN2 language model, outperforms\ntransformer-based models in both the 10M and 100M word tracks of the challenge,\nas measured by their performance on the BLiMP, EWoK, GLUE and BEAR benchmarks.\nFurther, we show the positive impact of knowledge distillation. Our findings\nchallenge the prevailing focus on transformer architectures and indicate the\nviability of RNN-based models, particularly in resource-constrained\nenvironments.", "paper_summary_zh": "\u9019\u7bc7\u8ad6\u6587\u63a2\u8a0e\u4e86\u905e\u8ff4\u795e\u7d93\u7db2\u8def (RNN) \u548c\u5176\u4ed6\u6b21\u4e8c\u6b21\u67b6\u69cb\u4f5c\u70ba\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u5efa\u6a21\u5834\u666f\u4e2d\u57fa\u65bcTransformer\u7684\u6a21\u578b\u7684\u7af6\u722d\u66ff\u4ee3\u65b9\u6848\u7684\u6f5b\u529b\u3002\u6211\u5011\u5229\u7528 HGRN2 (Qin et al., 2024)\uff0c\u4e00\u7a2e\u6700\u8fd1\u63d0\u51fa\u7684\u57fa\u65bc RNN \u7684\u67b6\u69cb\uff0c\u4e26\u6bd4\u8f03\u8a55\u4f30\u5176\u5c0d\u57fa\u65bcTransformer\u7684\u57fa\u7dda\u548c\u5176\u4ed6\u6b21\u4e8c\u6b21\u67b6\u69cb (LSTM\u3001xLSTM\u3001Mamba) \u7684\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684 HGRN2 \u8a9e\u8a00\u6a21\u578b BABYHGRN \u5728\u6311\u6230\u7684 10M \u548c 100M \u5b57\u8a5e\u8ecc\u9053\u4e2d\u90fd\u512a\u65bc\u57fa\u65bcTransformer\u7684\u6a21\u578b\uff0c\u5982\u5176\u5728 BLiMP\u3001EWoK\u3001GLUE \u548c BEAR \u57fa\u6e96\u4e0a\u7684\u6548\u80fd\u6240\u6e2c\u91cf\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u77e5\u8b58\u8403\u53d6\u7684\u6b63\u9762\u5f71\u97ff\u3002\u6211\u5011\u7684\u767c\u73fe\u6311\u6230\u4e86\u5c0dTransformer\u67b6\u69cb\u7684\u666e\u904d\u95dc\u6ce8\uff0c\u4e26\u8868\u660e\u57fa\u65bc RNN \u7684\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u7279\u5225\u662f\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u3002", "author": "Patrick Haller et.al.", "authors": "Patrick Haller, Jonas Golde, Alan Akbik", "id": "2412.15978v1", "paper_url": "http://arxiv.org/abs/2412.15978v1", "repo": "null"}}