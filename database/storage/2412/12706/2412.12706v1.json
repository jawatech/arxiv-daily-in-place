{"2412.12706": {"publish_time": "2024-12-17", "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision Trade-off in KV Cache Compression", "paper_summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8655\u7406\u8d8a\u4f86\u8d8a\u5927\u7684\u5167\u5bb9\u8996\u7a97\uff0cKV \u5feb\u53d6\u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u5df2\u6210\u70ba\u63a8\u8ad6\u671f\u9593\u7684\u95dc\u9375\u74f6\u9838\u3002\u4e3b\u6d41\u7684 KV \u58d3\u7e2e\u65b9\u6cd5\uff0c\u5305\u62ec KV \u526a\u679d\u548c KV \u91cf\u5316\uff0c\u4e3b\u8981\u95dc\u6ce8\u65bc\u7b26\u865f\u6216\u7cbe\u78ba\u5ea6\u7dad\u5ea6\uff0c\u5f88\u5c11\u63a2\u8a0e\u5b83\u5011\u7d44\u5408\u7684\u6548\u7387\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5168\u9762\u63a2\u8a0e KV \u5feb\u53d6\u58d3\u7e2e\u4e2d\u7684\u7b26\u865f\u7cbe\u78ba\u5ea6\u6b0a\u8861\u3002\u5be6\u9a57\u8b49\u660e\uff0c\u5728 KV \u5feb\u53d6\u4e2d\u5132\u5b58\u66f4\u591a\u5177\u6709\u8f03\u4f4e\u7cbe\u78ba\u5ea6\uff08\u5373\u91cf\u5316\u526a\u679d\uff09\u7684\u7b26\u865f\uff0c\u53ef\u4ee5\u986f\u8457\u63d0\u5347 LLM \u7684\u9577\u5167\u5bb9\u6548\u80fd\u3002\u6b64\u5916\uff0c\u5f9e\u4e00\u7cfb\u5217\u95dc\u9375\u9762\u5411\u6df1\u5165\u5206\u6790\u7b26\u865f\u7cbe\u78ba\u5ea6\u6b0a\u8861\uff0c\u986f\u793a\u91cf\u5316\u526a\u679d\u5728\u6aa2\u7d22\u76f8\u95dc\u4efb\u52d9\u4e2d\u7372\u5f97\u986f\u8457\u7684\u6539\u5584\uff0c\u4e26\u4e14\u5728\u4e0d\u540c\u7684\u8f38\u5165\u9577\u5ea6\u4e2d\u59cb\u7d42\u8868\u73fe\u826f\u597d\u3002\u6b64\u5916\uff0c\u91cf\u5316\u526a\u679d\u5728\u4e0d\u540c\u7684 KV \u526a\u679d\u65b9\u6cd5\u3001\u91cf\u5316\u7b56\u7565\u548c\u6a21\u578b\u898f\u6a21\u4e2d\u5c55\u73fe\u51fa\u986f\u8457\u7684\u7a69\u5b9a\u6027\u3002\u9019\u4e9b\u767c\u73fe\u70ba KV \u5feb\u53d6\u58d3\u7e2e\u4e2d\u7684\u7b26\u865f\u7cbe\u78ba\u5ea6\u6b0a\u8861\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\u3002\u6211\u5011\u8a08\u756b\u5728\u4e0d\u4e45\u7684\u5c07\u4f86\u767c\u5e03\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u3002", "author": "Jiebin Zhang et.al.", "authors": "Jiebin Zhang, Dawei Zhu, Yifan Song, Wenhao Wu, Chuqiao Kuang, Xiaoguang Li, Lifeng Shang, Qun Liu, Sujian Li", "id": "2412.12706v1", "paper_url": "http://arxiv.org/abs/2412.12706v1", "repo": "null"}}