{"2411.16158": {"publish_time": "2024-11-25", "title": "MixPE: Quantization and Hardware Co-design for Efficient LLM Inference", "paper_summary": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess as model sizes continue to grow, yet their deployment remains\nchallenging due to significant computational and memory demands. Quantization\nhas emerged as a promising solution, and state-of-the-art quantization\nalgorithms for LLMs introduce the need for mixed-precision matrix\nmultiplication (mpGEMM), where lower-precision weights are multiplied with\nhigher-precision activations. Despite its benefits, current hardware\naccelerators such as GPUs and TPUs lack native support for efficient mpGEMM,\nleading to inefficient dequantization operations in the main sequential loop.\nTo address this limitation, we introduce MixPE, a specialized mixed-precision\nprocessing element designed for efficient low-bit quantization in LLM\ninference. MixPE leverages two key innovations to minimize dequantization\noverhead and unlock the full potential of low-bit quantization. First,\nrecognizing that scale and zero point are shared within each quantization\ngroup, we propose performing dequantization after per-group mpGEMM,\nsignificantly reducing dequantization overhead. Second, instead of relying on\nconventional multipliers, MixPE utilizes efficient shift\\&add operations for\nmultiplication, optimizing both computation and energy efficiency. Our\nexperimental results demonstrate that MixPE surpasses the state-of-the-art\nquantization accelerators by $2.6\\times$ speedup and $1.4\\times$ energy\nreduction.", "paper_summary_zh": "<paragraph>\u96a8\u8457\u6a21\u578b\u898f\u6a21\u6301\u7e8c\u64f4\u5927\uff0c\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u53d6\u5f97\u986f\u8457\u6210\u529f\uff0c\u4f46\u7531\u65bc\u5176\u9f90\u5927\u7684\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u90e8\u7f72\u4ecd\u662f\u4e00\u9805\u6311\u6230\u3002\u91cf\u5316\u5df2\u6210\u70ba\u4e00\u7a2e\u5f88\u6709\u524d\u666f\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u800c LLM \u7684\u6700\u5148\u9032\u91cf\u5316\u6f14\u7b97\u6cd5\u5f15\u5165\u4e86\u6df7\u5408\u7cbe\u5ea6\u77e9\u9663\u4e58\u6cd5 (mpGEMM) \u7684\u9700\u6c42\uff0c\u5176\u4e2d\u8f03\u4f4e\u7cbe\u5ea6\u7684\u6b0a\u91cd\u6703\u8207\u8f03\u9ad8\u7cbe\u5ea6\u7684\u6fc0\u6d3b\u76f8\u4e58\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u512a\u9ede\uff0c\u4f46\u76ee\u524d\u7684\u786c\u9ad4\u52a0\u901f\u5668\uff08\u4f8b\u5982 GPU \u548c TPU\uff09\u7f3a\u4e4f\u5c0d\u9ad8\u6548 mpGEMM \u7684\u539f\u751f\u652f\u63f4\uff0c\u5c0e\u81f4\u4e3b\u5e8f\u5411\u8ff4\u5708\u4e2d\u51fa\u73fe\u4f4e\u6548\u7387\u7684\u53bb\u91cf\u5316\u4f5c\u696d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 MixPE\uff0c\u9019\u662f\u4e00\u7a2e\u5c08\u9580\u7684\u6df7\u5408\u7cbe\u5ea6\u8655\u7406\u5143\u4ef6\uff0c\u8a2d\u8a08\u7528\u65bc\u5728 LLM \u63a8\u8ad6\u4e2d\u57f7\u884c\u9ad8\u6548\u7684\u4f4e\u4f4d\u5143\u91cf\u5316\u3002MixPE \u5229\u7528\u5169\u9805\u95dc\u9375\u5275\u65b0\u4f86\u6700\u5c0f\u5316\u53bb\u91cf\u5316\u958b\u92b7\uff0c\u4e26\u767c\u63ee\u4f4e\u4f4d\u5143\u91cf\u5316\u7684\u5168\u90e8\u6f5b\u529b\u3002\u9996\u5148\uff0c\u6211\u5011\u8a8d\u77e5\u5230\u7e2e\u653e\u548c\u96f6\u9ede\u5728\u6bcf\u500b\u91cf\u5316\u7fa4\u7d44\u4e2d\u90fd\u662f\u5171\u7528\u7684\uff0c\u56e0\u6b64\u6211\u5011\u5efa\u8b70\u5728\u6bcf\u500b\u7fa4\u7d44\u7684 mpGEMM \u4e4b\u5f8c\u57f7\u884c\u53bb\u91cf\u5316\uff0c\u5927\u5e45\u6e1b\u5c11\u53bb\u91cf\u5316\u958b\u92b7\u3002\u5176\u6b21\uff0cMixPE \u4e0d\u4f9d\u8cf4\u50b3\u7d71\u7684\u4e58\u6cd5\u5668\uff0c\u800c\u662f\u5229\u7528\u9ad8\u6548\u7684\u4f4d\u79fb\u548c\u52a0\u6cd5\u904b\u7b97\u9032\u884c\u4e58\u6cd5\uff0c\u540c\u6642\u6700\u4f73\u5316\u904b\u7b97\u548c\u80fd\u6e90\u6548\u7387\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0cMixPE \u5728\u52a0\u901f\u65b9\u9762\u6bd4\u6700\u5148\u9032\u7684\u91cf\u5316\u52a0\u901f\u5668\u5feb\u4e86 2.6 \u500d\uff0c\u5728\u80fd\u6e90\u6d88\u8017\u65b9\u9762\u6e1b\u5c11\u4e86 1.4 \u500d\u3002</paragraph>", "author": "Yu Zhang et.al.", "authors": "Yu Zhang, Mingzi Wang, Lancheng Zou, Wulong Liu, Hui-Ling Zhen, Mingxuan Yuan, Bei Yu", "id": "2411.16158v1", "paper_url": "http://arxiv.org/abs/2411.16158v1", "repo": "null"}}