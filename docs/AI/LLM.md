
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-06**|**Pose Priors from Language Models**|Sanjay Subramanian et.al.|[2405.03689v1](http://arxiv.org/abs/2405.03689v1)|null|
|**2024-05-06**|**Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames**|Keith Burghardt et.al.|[2405.03688v1](http://arxiv.org/abs/2405.03688v1)|null|
|**2024-05-06**|**Language-Image Models with 3D Understanding**|Jang Hyun Cho et.al.|[2405.03685v1](http://arxiv.org/abs/2405.03685v1)|null|
|**2024-05-06**|**Towards A Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis**|Clayton Cohn et.al.|[2405.03677v1](http://arxiv.org/abs/2405.03677v1)|null|
|**2024-05-06**|**MemoryMamba: Memory-Augmented State Space Model for Defect Recognition**|Qianning Wang et.al.|[2405.03673v1](http://arxiv.org/abs/2405.03673v1)|null|
|**2024-05-06**|**ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection**|Arpit Bahety et.al.|[2405.03666v1](http://arxiv.org/abs/2405.03666v1)|null|
|**2024-05-06**|**Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent**|Shang Shang et.al.|[2405.03654v2](http://arxiv.org/abs/2405.03654v2)|null|
|**2024-05-06**|**When LLMs Meet Cybersecurity: A Systematic Literature Review**|Jie Zhang et.al.|[2405.03644v1](http://arxiv.org/abs/2405.03644v1)|null|
|**2024-05-06**|**Detecting Android Malware: From Neural Embeddings to Hands-On Validation with BERTroid**|Meryam Chaieb et.al.|[2405.03620v1](http://arxiv.org/abs/2405.03620v1)|null|
|**2024-05-06**|**A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama**|Vlad-Andrei Cursaru et.al.|[2405.03616v1](http://arxiv.org/abs/2405.03616v1)|null|
|**2024-05-06**|**GREEN: Generative Radiology Report Evaluation and Error Notation**|Sophie Ostmeier et.al.|[2405.03595v1](http://arxiv.org/abs/2405.03595v1)|null|
|**2024-05-06**|**Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment**|Abhinav Agarwalla et.al.|[2405.03594v1](http://arxiv.org/abs/2405.03594v1)|null|
|**2024-05-06**|**Deep Space Separable Distillation for Lightweight Acoustic Scene Classification**|ShuQi Ye et.al.|[2405.03567v1](http://arxiv.org/abs/2405.03567v1)|null|
|**2024-05-06**|**AlphaMath Almost Zero: process Supervision without process**|Guoxin Chen et.al.|[2405.03553v1](http://arxiv.org/abs/2405.03553v1)|null|
|**2024-05-06**|**MAmmoTH2: Scaling Instructions from the Web**|Xiang Yue et.al.|[2405.03548v1](http://arxiv.org/abs/2405.03548v1)|null|
|**2024-05-06**|**Position Paper: Leveraging Foundational Models for Black-Box Optimization: Benefits, Challenges, and Future Directions**|Xingyou Song et.al.|[2405.03547v1](http://arxiv.org/abs/2405.03547v1)|null|
|**2024-05-06**|**Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation**|Jesher Joshua M et.al.|[2405.03537v1](http://arxiv.org/abs/2405.03537v1)|null|
|**2024-05-06**|**A Rate-Distortion-Classification Approach for Lossy Image Compression**|Yuefeng Zhang et.al.|[2405.03500v1](http://arxiv.org/abs/2405.03500v1)|null|
|**2024-05-06**|**A Lightweight Neural Architecture Search Model for Medical Image Classification**|Lunchen Xie et.al.|[2405.03462v1](http://arxiv.org/abs/2405.03462v1)|null|
|**2024-05-06**|**Large Language Models (LLMs) as Agents for Augmented Democracy**|Jairo Gudiño-Rosero et.al.|[2405.03452v2](http://arxiv.org/abs/2405.03452v2)|null|
|**2024-05-06**|**Robotic Constrained Imitation Learning for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery**|Kento Kawaharazuka et.al.|[2405.03440v1](http://arxiv.org/abs/2405.03440v1)|null|
|**2024-05-06**|**A method for quantifying the generalization capabilities of generative models for solving Ising models**|Qunlong Ma et.al.|[2405.03435v1](http://arxiv.org/abs/2405.03435v1)|null|
|**2024-05-06**|**ReCycle: Fast and Efficient Long Time Series Forecasting with Residual Cyclic Transformers**|Arvid Weyrauch et.al.|[2405.03429v1](http://arxiv.org/abs/2405.03429v1)|[link](https://github.com/helmholtz-ai-energy/recycle)|
|**2024-05-06**|**Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large Language Models**|Emre Onal et.al.|[2405.03425v1](http://arxiv.org/abs/2405.03425v1)|null|
|**2024-05-06**|**Automated Computation of Therapies Using Failure Mode and Effects Analysis in the Medical Domain**|Malte Luttermann et.al.|[2405.03406v1](http://arxiv.org/abs/2405.03406v1)|null|
|**2024-05-06**|**E2GNN: Efficient Graph Neural Network Ensembles for Semi-Supervised Classification**|Xin Zhang et.al.|[2405.03401v1](http://arxiv.org/abs/2405.03401v1)|null|
|**2024-05-06**|**Don't Waste Your Time: Early Stopping Cross-Validation**|Edward Bergman et.al.|[2405.03389v1](http://arxiv.org/abs/2405.03389v1)|null|
|**2024-05-06**|**The high dimensional psychological profile and cultural bias of ChatGPT**|Hang Yuan et.al.|[2405.03387v1](http://arxiv.org/abs/2405.03387v1)|null|
|**2024-05-06**|**Snake Learning: A Communication- and Computation-Efficient Distributed Learning Framework for 6G**|Xiaoxue Yu et.al.|[2405.03372v1](http://arxiv.org/abs/2405.03372v1)|null|
|**2024-05-06**|**Explainable Fake News Detection With Large Language Model via Defense Among Competing Wisdom**|Bo Wang et.al.|[2405.03371v1](http://arxiv.org/abs/2405.03371v1)|null|
|**2024-05-06**|**MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models in the Context of the Pediatric Hypertension Guideline**|Mohamed Yaseen Jabarulla et.al.|[2405.03359v1](http://arxiv.org/abs/2405.03359v1)|[link](https://github.com/yaseen28/meddoc-bot)|
|**2024-05-06**|**Enhancing Q-Learning with Large Language Model Heuristics**|Xiefeng Wu et.al.|[2405.03341v1](http://arxiv.org/abs/2405.03341v1)|null|
|**2024-05-06**|**Functional Equivalence with NARS**|Robert Johansson et.al.|[2405.03340v1](http://arxiv.org/abs/2405.03340v1)|null|
|**2024-05-06**|**Enhancing Spatiotemporal Disease Progression Models via Latent Diffusion and Prior Knowledge**|Lemuel Puglisi et.al.|[2405.03328v1](http://arxiv.org/abs/2405.03328v1)|null|
|**2024-05-06**|**Artificial Intelligence in the Autonomous Navigation of Endovascular Interventions: A Systematic Review**|Harry Robertshaw et.al.|[2405.03305v1](http://arxiv.org/abs/2405.03305v1)|null|
|**2024-05-06**|**Coefficient Decomposition for Spectral Graph Convolution**|Feng Huang et.al.|[2405.03296v1](http://arxiv.org/abs/2405.03296v1)|null|
|**2024-05-06**|**Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity**|Yizhuo Lu et.al.|[2405.03280v1](http://arxiv.org/abs/2405.03280v1)|null|
|**2024-05-06**|**Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning**|Qizhou Chen et.al.|[2405.03279v1](http://arxiv.org/abs/2405.03279v1)|null|
|**2024-05-06**|**Exploring the Frontiers of Softmax: Provable Optimization, Applications in Diffusion Model, and Beyond**|Jiuxiang Gu et.al.|[2405.03251v1](http://arxiv.org/abs/2405.03251v1)|null|
|**2024-05-06**|**Communication-Efficient Federated Learning with Adaptive Compression under Dynamic Bandwidth**|Ying Zhuansun et.al.|[2405.03248v1](http://arxiv.org/abs/2405.03248v1)|null|
|**2024-05-06**|**Deep Learning for Detecting and Early Predicting Chronic Obstructive Pulmonary Disease from Spirogram Time Series: A UK Biobank Study**|Shuhao Mei et.al.|[2405.03239v1](http://arxiv.org/abs/2405.03239v1)|null|
|**2024-05-06**|**A Philosophical Introduction to Language Models - Part II: The Way Forward**|Raphaël Millière et.al.|[2405.03207v1](http://arxiv.org/abs/2405.03207v1)|null|
|**2024-05-06**|**Vietnamese AI Generated Text Detection**|Quang-Dan Tran et.al.|[2405.03206v1](http://arxiv.org/abs/2405.03206v1)|null|
|**2024-05-06**|**Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions**|Ruizhe Li et.al.|[2405.03205v1](http://arxiv.org/abs/2405.03205v1)|[link](https://github.com/ruizheliuoa/anchored_bias_gpt2)|
|**2024-05-06**|**QuadraNet V2: Efficient and Sustainable Training of High-Order Neural Networks with Quadratic Adaptation**|Chenhui Xu et.al.|[2405.03192v1](http://arxiv.org/abs/2405.03192v1)|null|
|**2024-05-06**|**Oracle-Checker Scheme for Evaluating a Generative Large Language Model**|Yueling Jenny Zeng et.al.|[2405.03170v1](http://arxiv.org/abs/2405.03170v1)|null|
|**2024-05-06**|**The Role of Predictive Uncertainty and Diversity in Embodied AI and Robot Learning**|Ransalu Senanayake et.al.|[2405.03164v1](http://arxiv.org/abs/2405.03164v1)|null|
|**2024-05-06**|**Advancing Multimodal Medical Capabilities of Gemini**|Lin Yang et.al.|[2405.03162v1](http://arxiv.org/abs/2405.03162v1)|null|
|**2024-05-06**|**Exploring the Potential of the Large Language Models (LLMs) in Identifying Misleading News Headlines**|Md Main Uddin Rony et.al.|[2405.03153v1](http://arxiv.org/abs/2405.03153v1)|null|
|**2024-05-06**|**Time Series Stock Price Forecasting Based on Genetic Algorithm (GA)-Long Short-Term Memory Network (LSTM) Optimization**|Xinye Sha et.al.|[2405.03151v1](http://arxiv.org/abs/2405.03151v1)|null|
|**2024-05-06**|**Quantifying the Capabilities of LLMs across Scale and Precision**|Sher Badshah et.al.|[2405.03146v1](http://arxiv.org/abs/2405.03146v1)|null|
|**2024-05-06**|**Automatic Ultrasound Curve Angle Measurement via Affinity Clustering for Adolescent Idiopathic Scoliosis Evaluation**|Yihao Zhou et.al.|[2405.03141v2](http://arxiv.org/abs/2405.03141v2)|null|
|**2024-05-06**|**CRAFT: Extracting and Tuning Cultural Instructions from the Wild**|Bin Wang et.al.|[2405.03138v1](http://arxiv.org/abs/2405.03138v1)|null|
|**2024-05-06**|**Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training**|Zexuan Zhong et.al.|[2405.03133v1](http://arxiv.org/abs/2405.03133v1)|null|
|**2024-05-06**|**WDMoE: Wireless Distributed Large Language Models with Mixture of Experts**|Nan Xue et.al.|[2405.03131v1](http://arxiv.org/abs/2405.03131v1)|null|
|**2024-05-06**|**AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding**|Tao Liu et.al.|[2405.03121v1](http://arxiv.org/abs/2405.03121v1)|[link](https://github.com/x-lance/anitalker)|
|**2024-05-06**|**An Active Inference Agent for Simulating Human Translation Processes in a Hierarchical Architecture: Integrating the Task Segment Framework and the HOF taxonomy**|Michael Carl et.al.|[2405.03111v1](http://arxiv.org/abs/2405.03111v1)|null|
|**2024-05-06**|**FairMonitor: A Dual-framework for Detecting Stereotypes and Biases in Large Language Models**|Yanhong Bai et.al.|[2405.03098v1](http://arxiv.org/abs/2405.03098v1)|null|
|**2024-05-06**|**To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models**|George-Octavian Barbulescu et.al.|[2405.03097v1](http://arxiv.org/abs/2405.03097v1)|null|
|**2024-05-06**|**Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation**|Kaize Shi et.al.|[2405.03085v1](http://arxiv.org/abs/2405.03085v1)|null|
|**2024-05-05**|**On Probabilistic and Causal Reasoning with Summation Operators**|Duligur Ibeling et.al.|[2405.03069v1](http://arxiv.org/abs/2405.03069v1)|null|
|**2024-05-05**|**AC-MAMBASEG: An adaptive convolution and Mamba-based architecture for enhanced skin lesion segmentation**|Viet-Thanh Nguyen et.al.|[2405.03011v1](http://arxiv.org/abs/2405.03011v1)|null|
|**2024-05-05**|**High Order Reasoning for Time Critical Recommendation in Evidence-based Medicine**|Manjiang Yu et.al.|[2405.03010v1](http://arxiv.org/abs/2405.03010v1)|null|
|**2024-05-05**|**Explainable Malware Detection with Tailored Logic Explained Networks**|Peter Anthony et.al.|[2405.03009v1](http://arxiv.org/abs/2405.03009v1)|null|
|**2024-05-05**|**On the performativity of SDG classifications in large bibliometric databases**|Matteo Ottaviani et.al.|[2405.03007v1](http://arxiv.org/abs/2405.03007v1)|null|
|**2024-05-05**|**Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints**|Siow Meng Low et.al.|[2405.03005v1](http://arxiv.org/abs/2405.03005v1)|null|
|**2024-05-05**|**Exploring prompts to elicit memorization in masked language model-based named entity recognition**|Yuxi Xia et.al.|[2405.03004v1](http://arxiv.org/abs/2405.03004v1)|null|
|**2024-05-05**|**Parameter-Efficient Fine-Tuning with Discrete Fourier Transform**|Ziqi Gao et.al.|[2405.03003v1](http://arxiv.org/abs/2405.03003v1)|null|
|**2024-05-05**|**MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning**|Wenqi Shi et.al.|[2405.03000v1](http://arxiv.org/abs/2405.03000v1)|null|
|**2024-05-05**|**RepAugment: Input-Agnostic Representation-Level Augmentation for Respiratory Sound Classification**|June-Woo Kim et.al.|[2405.02996v1](http://arxiv.org/abs/2405.02996v1)|null|
|**2024-05-05**|**Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability to Mark Short Answer Questions in K-12 Education**|Owen Henkel et.al.|[2405.02985v1](http://arxiv.org/abs/2405.02985v1)|null|
|**2024-05-05**|**E-TSL: A Continuous Educational Turkish Sign Language Dataset with Baseline Methods**|Şükrü Öztürk et.al.|[2405.02984v1](http://arxiv.org/abs/2405.02984v1)|null|
|**2024-05-05**|**Multi-Agent RL-Based Industrial AIGC Service Offloading over Wireless Edge Networks**|Siyuan Li et.al.|[2405.02972v1](http://arxiv.org/abs/2405.02972v1)|null|
|**2024-05-05**|**Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents**|Junkai Li et.al.|[2405.02957v1](http://arxiv.org/abs/2405.02957v1)|null|
|**2024-05-05**|**Unraveling the Dominance of Large Language Models Over Transformer Models for Bangla Natural Language Inference: A Comprehensive Study**|Fatema Tuj Johora Faria et.al.|[2405.02937v2](http://arxiv.org/abs/2405.02937v2)|[link](https://github.com/fatemafaria142/Large-Language-Models-Over-Transformer-Models-for-Bangla-NLI)|
|**2024-05-05**|**On the tractability of SHAP explanations under Markovian distributions**|Reda Marzouk et.al.|[2405.02936v1](http://arxiv.org/abs/2405.02936v1)|null|
|**2024-05-05**|**Relay Decoding: Concatenating Large Language Models for Machine Translation**|Chengpeng Fu et.al.|[2405.02933v1](http://arxiv.org/abs/2405.02933v1)|null|
|**2024-05-05**|**Unified Dynamic Scanpath Predictors Outperform Individually Trained Neural Models**|Fares Abawi et.al.|[2405.02929v2](http://arxiv.org/abs/2405.02929v2)|null|
|**2024-05-05**|**A Two-Stage Prediction-Aware Contrastive Learning Framework for Multi-Intent NLU**|Guanhua Chen et.al.|[2405.02925v1](http://arxiv.org/abs/2405.02925v1)|null|
|**2024-05-05**|**Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models**|Tobias Groot et.al.|[2405.02917v1](http://arxiv.org/abs/2405.02917v1)|null|
|**2024-05-05**|**Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English**|Aekansh Kathunia et.al.|[2405.02887v1](http://arxiv.org/abs/2405.02887v1)|null|
|**2024-05-05**|**Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models**|Yang Liu et.al.|[2405.02861v1](http://arxiv.org/abs/2405.02861v1)|[link](https://github.com/jacklanda/lexbench)|
|**2024-05-05**|**Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation**|Jinyu Cai et.al.|[2405.02858v1](http://arxiv.org/abs/2405.02858v1)|null|
|**2024-05-05**|**Modelling Opaque Bilateral Market Dynamics in Financial Trading: Insights from a Multi-Agent Simulation Study**|Alicia Vidler et.al.|[2405.02849v1](http://arxiv.org/abs/2405.02849v1)|null|
|**2024-05-05**|**Responsible AI: Portraits with Intelligent Bibliometrics**|Yi Zhang et.al.|[2405.02846v1](http://arxiv.org/abs/2405.02846v1)|null|
|**2024-05-05**|**Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction**|Changan Chen et.al.|[2405.02821v1](http://arxiv.org/abs/2405.02821v1)|null|
|**2024-05-05**|**HuixiangDou-CR: Coreference Resolution in Group Chats**|Huanjun Kong et.al.|[2405.02817v1](http://arxiv.org/abs/2405.02817v1)|[link](https://github.com/internlm/huixiangdou)|
|**2024-05-05**|**Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization**|Hamed Zamani et.al.|[2405.02816v1](http://arxiv.org/abs/2405.02816v1)|null|
|**2024-05-05**|**Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**|Zhusi Zhong et.al.|[2405.02815v1](http://arxiv.org/abs/2405.02815v1)|null|
|**2024-05-05**|**NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli**|Xu Wang et.al.|[2405.02814v1](http://arxiv.org/abs/2405.02814v1)|[link](https://github.com/wangxu0820/negativeprompt)|
|**2024-05-05**|**Kinematic analysis of structural mechanics based on convolutional neural network**|Leye Zhang et.al.|[2405.02807v1](http://arxiv.org/abs/2405.02807v1)|[link](https://github.com/zhangleye/Kinematic-Analysis)|
|**2024-05-05**|**Mozart's Touch: A Lightweight Multi-modal Music Generation Framework Based on Pre-Trained Large Models**|Tianze Xu et.al.|[2405.02801v2](http://arxiv.org/abs/2405.02801v2)|[link](https://github.com/wangtoonaive/mozartstouch)|
|**2024-05-05**|**ImageInWords: Unlocking Hyper-Detailed Image Descriptions**|Roopal Garg et.al.|[2405.02793v1](http://arxiv.org/abs/2405.02793v1)|[link](https://github.com/google/imageinwords)|
|**2024-05-05**|**Efficient Text-driven Motion Generation via Latent Consistency Training**|Mengxian Hu et.al.|[2405.02791v1](http://arxiv.org/abs/2405.02791v1)|null|
|**2024-05-05**|**Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs**|Feiyang Kang et.al.|[2405.02774v1](http://arxiv.org/abs/2405.02774v1)|null|
|**2024-05-04**|**Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities for Lifelong Learning**|Fahad Sarfraz et.al.|[2405.02766v1](http://arxiv.org/abs/2405.02766v1)|null|
|**2024-05-04**|**Detecting Edited Knowledge in Language Models**|Paul Youssef et.al.|[2405.02765v1](http://arxiv.org/abs/2405.02765v1)|null|
|**2024-05-04**|**Assessing Adversarial Robustness of Large Language Models: An Empirical Study**|Zeyu Yang et.al.|[2405.02764v1](http://arxiv.org/abs/2405.02764v1)|null|
|**2024-05-04**|**Implicit Safe Set Algorithm for Provably Safe Reinforcement Learning**|Weiye Zhao et.al.|[2405.02754v1](http://arxiv.org/abs/2405.02754v1)|null|
|**2024-05-04**|**Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding**|Zheng Zhao et.al.|[2405.02750v1](http://arxiv.org/abs/2405.02750v1)|[link](https://github.com/amazon-science/contextualunderstanding-contrastivedecoding)|

#### Abstracts
##### **Pose Priors from Language Models**
2405.03689v1 by Sanjay Subramanian,Evonne Ng,Lea Müller,Dan Klein,Shiry Ginosar,Trevor Darrell

We present a zero-shot pose optimization method that enforces accurate
physical contact constraints when estimating the 3D pose of humans. Our central
insight is that since language is often used to describe physical interaction,
large pretrained text-based models can act as priors on pose estimation.
  We can thus leverage this insight to improve pose estimation by converting
natural language descriptors, generated by a large multimodal model (LMM), into
tractable losses to constrain the 3D pose optimization. Despite its simplicity,
our method produces surprisingly compelling pose reconstructions of people in
close contact, correctly capturing the semantics of the social and physical
interactions. We demonstrate that our method rivals more complex
state-of-the-art approaches that require expensive human annotation of contact
points and training specialized models. Moreover, unlike previous approaches,
our method provides a unified framework for resolving self-contact and
person-to-person contact.

摘要：

##### **Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames**
2405.03688v1 by Keith Burghardt,Kai Chen,Kristina Lerman

Adversarial information operations can destabilize societies by undermining
fair elections, manipulating public opinions on policies, and promoting scams.
Despite their widespread occurrence and potential impacts, our understanding of
influence campaigns is limited by manual analysis of messages and subjective
interpretation of their observable behavior. In this paper, we explore whether
these limitations can be mitigated with large language models (LLMs), using
GPT-3.5 as a case-study for coordinated campaign annotation. We first use
GPT-3.5 to scrutinize 126 identified information operations spanning over a
decade. We utilize a number of metrics to quantify the close (if imperfect)
agreement between LLM and ground truth descriptions. We next extract
coordinated campaigns from two large multilingual datasets from X (formerly
Twitter) that respectively discuss the 2022 French election and 2023 Balikaran
Philippine-U.S. military exercise in 2023. For each coordinated campaign, we
use GPT-3.5 to analyze posts related to a specific concern and extract goals,
tactics, and narrative frames, both before and after critical events (such as
the date of an election). While the GPT-3.5 sometimes disagrees with subjective
interpretation, its ability to summarize and interpret demonstrates LLMs'
potential to extract higher-order indicators from text to provide a more
complete picture of the information campaigns compared to previous methods.

摘要：

##### **Language-Image Models with 3D Understanding**
2405.03685v1 by Jang Hyun Cho,Boris Ivanovic,Yulong Cao,Edward Schmerling,Yue Wang,Xinshuo Weng,Boyi Li,Yurong You,Philipp Krähenbühl,Yan Wang,Marco Pavone

Multi-modal large language models (MLLMs) have shown incredible capabilities
in a variety of 2D vision and language tasks. We extend MLLMs' perceptual
capabilities to ground and reason about images in 3-dimensional space. To that
end, we first develop a large-scale pre-training dataset for 2D and 3D called
LV3D by combining multiple existing 2D and 3D recognition datasets under a
common task formulation: as multi-turn question-answering. Next, we introduce a
new MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data
scaling makes a strong 3D perception capability without 3D specific
architectural design or training objective. Cube-LLM exhibits intriguing
properties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting
to improve 3D understanding from 2D context information. (2) Cube-LLM can
follow complex and diverse instructions and adapt to versatile input and output
formats. (3) Cube-LLM can be visually prompted such as 2D box or a set of
candidate 3D boxes from specialists. Our experiments on outdoor benchmarks
demonstrate that Cube-LLM significantly outperforms existing baselines by 21.3
points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7
points on the DriveLM dataset for complex reasoning about driving scenarios,
respectively. Cube-LLM also shows competitive results in general MLLM
benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well
as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for
complex reasoning. Our project is available at
https://janghyuncho.github.io/Cube-LLM.

摘要：

##### **Towards A Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis**
2405.03677v1 by Clayton Cohn,Caitlin Snyder,Justin Montenegro,Gautam Biswas

LLMs have demonstrated proficiency in contextualizing their outputs using
human input, often matching or beating human-level performance on a variety of
tasks. However, LLMs have not yet been used to characterize synergistic
learning in students' collaborative discourse. In this exploratory work, we
take a first step towards adopting a human-in-the-loop prompt engineering
approach with GPT-4-Turbo to summarize and categorize students' synergistic
learning during collaborative discourse. Our preliminary findings suggest
GPT-4-Turbo may be able to characterize students' synergistic learning in a
manner comparable to humans and that our approach warrants further
investigation.

摘要：

##### **MemoryMamba: Memory-Augmented State Space Model for Defect Recognition**
2405.03673v1 by Qianning Wang,He Hu,Yucheng Zhou

As automation advances in manufacturing, the demand for precise and
sophisticated defect detection technologies grows. Existing vision models for
defect recognition methods are insufficient for handling the complexities and
variations of defects in contemporary manufacturing settings. These models
especially struggle in scenarios involving limited or imbalanced defect data.
In this work, we introduce MemoryMamba, a novel memory-augmented state space
model (SSM), designed to overcome the limitations of existing defect
recognition models. MemoryMamba integrates the state space model with the
memory augmentation mechanism, enabling the system to maintain and retrieve
essential defect-specific information in training. Its architecture is designed
to capture dependencies and intricate defect characteristics, which are crucial
for effective defect detection. In the experiments, MemoryMamba was evaluated
across four industrial datasets with diverse defect types and complexities. The
model consistently outperformed other methods, demonstrating its capability to
adapt to various defect recognition scenarios.

摘要：

##### **ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection**
2405.03666v1 by Arpit Bahety,Priyanka Mandikal,Ben Abbatematteo,Roberto Martín-Martín

Bimanual manipulation is a longstanding challenge in robotics due to the
large number of degrees of freedom and the strict spatial and temporal
synchronization required to generate meaningful behavior. Humans learn bimanual
manipulation skills by watching other humans and by refining their abilities
through play. In this work, we aim to enable robots to learn bimanual
manipulation behaviors from human video demonstrations and fine-tune them
through interaction. Inspired by seminal work in psychology and biomechanics,
we propose modeling the interaction between two hands as a serial kinematic
linkage -- as a screw motion, in particular, that we use to define a new action
space for bimanual manipulation: screw actions. We introduce ScrewMimic, a
framework that leverages this novel action representation to facilitate
learning from human demonstration and self-supervised policy fine-tuning. Our
experiments demonstrate that ScrewMimic is able to learn several complex
bimanual behaviors from a single human video demonstration, and that it
outperforms baselines that interpret demonstrations and fine-tune directly in
the original space of motion of both arms. For more information and video
results, https://robin-lab.cs.utexas.edu/ScrewMimic/

摘要：

##### **Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent**
2405.03654v2 by Shang Shang,Xinqiang Zhao,Zhongjiang Yao,Yepeng Yao,Liya Su,Zijing Fan,Xiaodan Zhang,Zhengwei Jiang

To demonstrate and address the underlying maliciousness, we propose a
theoretical hypothesis and analytical approach, and introduce a new black-box
jailbreak attack methodology named IntentObfuscator, exploiting this identified
flaw by obfuscating the true intentions behind user prompts.This approach
compels LLMs to inadvertently generate restricted content, bypassing their
built-in content security measures. We detail two implementations under this
framework: "Obscure Intention" and "Create Ambiguity", which manipulate query
complexity and ambiguity to evade malicious intent detection effectively. We
empirically validate the effectiveness of the IntentObfuscator method across
several models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving
an average jailbreak success rate of 69.21\%. Notably, our tests on
ChatGPT-3.5, which claims 100 million weekly active users, achieved a
remarkable success rate of 83.65\%. We also extend our validation to diverse
types of sensitive content like graphic violence, racism, sexism, political
sensitivity, cybersecurity threats, and criminal skills, further proving the
substantial impact of our findings on enhancing 'Red Team' strategies against
LLM content security frameworks.

摘要：

##### **When LLMs Meet Cybersecurity: A Systematic Literature Review**
2405.03644v1 by Jie Zhang,Haoyu Bu,Hui Wen,Yu Chen,Lun Li,Hongsong Zhu

The rapid advancements in large language models (LLMs) have opened new
avenues across various fields, including cybersecurity, which faces an
ever-evolving threat landscape and need for innovative technologies. Despite
initial explorations into the application of LLMs in cybersecurity, there is a
lack of a comprehensive overview of this research area. This paper bridge this
gap by providing a systematic literature review, encompassing an analysis of
over 180 works, spanning across 25 LLMs and more than 10 downstream scenarios.
Our comprehensive overview addresses three critical research questions: the
construction of cybersecurity-oriented LLMs, LLMs' applications in various
cybersecurity tasks, and the existing challenges and further research in this
area. This study aims to shed light on the extensive potential of LLMs in
enhancing cybersecurity practices, and serve as a valuable resource for
applying LLMs in this doamin. We also maintain and regularly updated list of
practical guides on LLMs for cybersecurity at
https://github.com/tmylla/Awesome-LLM4Cybersecurity.

摘要：

##### **Detecting Android Malware: From Neural Embeddings to Hands-On Validation with BERTroid**
2405.03620v1 by Meryam Chaieb,Mostafa Anouar Ghorab,Mohamed Aymen Saied

As cyber threats and malware attacks increasingly alarm both individuals and
businesses, the urgency for proactive malware countermeasures intensifies. This
has driven a rising interest in automated machine learning solutions.
Transformers, a cutting-edge category of attention-based deep learning methods,
have demonstrated remarkable success. In this paper, we present BERTroid, an
innovative malware detection model built on the BERT architecture. Overall,
BERTroid emerged as a promising solution for combating Android malware. Its
ability to outperform state-of-the-art solutions demonstrates its potential as
a proactive defense mechanism against malicious software attacks. Additionally,
we evaluate BERTroid on multiple datasets to assess its performance across
diverse scenarios. In the dynamic landscape of cybersecurity, our approach has
demonstrated promising resilience against the rapid evolution of malware on
Android systems. While the machine learning model captures broad patterns, we
emphasize the role of manual validation for deeper comprehension and insight
into these behaviors. This human intervention is critical for discerning
intricate and context-specific behaviors, thereby validating and reinforcing
the model's findings.

摘要：

##### **A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama**
2405.03616v1 by Vlad-Andrei Cursaru,Laura Duits,Joel Milligan,Damla Ural,Berta Rodriguez Sanchez,Vincenzo Stoico,Ivano Malavolta

Context. Nowadays, 83% of software developers use Large Language Models
(LLMs) to generate code. LLMs recently became essential to increase the
productivity of software developers and decrease the time and cost of software
development. Developers ranging from novices to experts use LLM tools not only
to detect and patch bugs, but also to integrate generated code into their
software. However, as of today there is no objective assessment of the energy
efficiency of the source code generated by LLM tools. Released in August 2023,
Code Llama is one of the most recent LLM tools.
  Goal. In this paper, we present an empirical study that assesses the energy
efficiency of Code Llama with respect to human-written source code.
  Method. We design an experiment involving three human-written benchmarks
implemented in C++, JavaScript, and Python. We ask Code Llama to generate the
code of the benchmarks using different prompts and temperatures. Therefore, we
execute both implementations and profile their energy efficiency.
  Results. Our study shows that the energy efficiency of code generated by Code
Llama is heavily-dependent on the chosen programming language and the specific
code problem at hand. Also, human implementations tend to be more energy
efficient overall, with generated JavaScript code outperforming its human
counterpart. Moreover, explicitly asking Code Llama to generate
energy-efficient code results in an equal or worse energy efficiency, as well
as using different temperatures seems not to affect the energy efficiency of
generated code.
  Conclusions. According to our results, code generated using Code Llama does
not guarantee energy efficiency, even when prompted to do so. Therefore,
software developers should evaluate the energy efficiency of generated code
before integrating it into the software system under development.

摘要：

##### **GREEN: Generative Radiology Report Evaluation and Error Notation**
2405.03595v1 by Sophie Ostmeier,Justin Xu,Zhihong Chen,Maya Varma,Louis Blankemeier,Christian Bluethgen,Arne Edward Michalson,Michael Moseley,Curtis Langlotz,Akshay S Chaudhari,Jean-Benoit Delbrouck

Evaluating radiology reports is a challenging problem as factual correctness
is extremely important due to the need for accurate medical communication about
medical images. Existing automatic evaluation metrics either suffer from
failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited
in their interpretability (e.g., F1CheXpert and F1RadGraph). In this paper, we
introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a
radiology report generation metric that leverages the natural language
understanding of language models to identify and explain clinically significant
errors in candidate reports, both quantitatively and qualitatively. Compared to
current metrics, GREEN offers: 1) a score aligned with expert preferences, 2)
human interpretable explanations of clinically significant errors, enabling
feedback loops with end-users, and 3) a lightweight open-source method that
reaches the performance of commercial counterparts. We validate our GREEN
metric by comparing it to GPT-4, as well as to error counts of 6 experts and
preferences of 2 experts. Our method demonstrates not only higher correlation
with expert error counts, but simultaneously higher alignment with expert
preferences when compared to previous approaches."

摘要：

##### **Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment**
2405.03594v1 by Abhinav Agarwalla,Abhay Gupta,Alexandre Marques,Shubhra Pandit,Michael Goin,Eldar Kurtic,Kevin Leong,Tuan Nguyen,Mahmoud Salem,Dan Alistarh,Sean Lie,Mark Kurtz

Large language models (LLMs) have revolutionized Natural Language Processing
(NLP), but their size creates computational bottlenecks. We introduce a novel
approach to create accurate, sparse foundational versions of performant LLMs
that achieve full accuracy recovery for fine-tuning tasks at up to 70%
sparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT
one-shot pruning method and sparse pretraining of those models on a subset of
the SlimPajama dataset mixed with a Python subset of The Stack dataset. We
exhibit training acceleration due to sparsity on Cerebras CS-3 chips that
closely matches theoretical scaling. In addition, we establish inference
acceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine
and 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are
realized via sparsity alone, thus enabling further gains through additional use
of quantization. Specifically, we show a total speedup on CPUs for
sparse-quantized LLaMA models of up to 8.6x. We demonstrate these results
across diverse, challenging tasks, including chat, instruction following, code
generation, arithmetic reasoning, and summarization to prove their generality.
This work paves the way for rapidly creating smaller and faster LLMs without
sacrificing accuracy.

摘要：

##### **Deep Space Separable Distillation for Lightweight Acoustic Scene Classification**
2405.03567v1 by ShuQi Ye,Yuan Tian

Acoustic scene classification (ASC) is highly important in the real world.
Recently, deep learning-based methods have been widely employed for acoustic
scene classification. However, these methods are currently not lightweight
enough as well as their performance is not satisfactory. To solve these
problems, we propose a deep space separable distillation network. Firstly, the
network performs high-low frequency decomposition on the log-mel spectrogram,
significantly reducing computational complexity while maintaining model
performance. Secondly, we specially design three lightweight operators for ASC,
including Separable Convolution (SC), Orthonormal Separable Convolution (OSC),
and Separable Partial Convolution (SPC). These operators exhibit highly
efficient feature extraction capabilities in acoustic scene classification
tasks. The experimental results demonstrate that the proposed method achieves a
performance gain of 9.8% compared to the currently popular deep learning
methods, while also having smaller parameter count and computational
complexity.

摘要：

##### **AlphaMath Almost Zero: process Supervision without process**
2405.03553v1 by Guoxin Chen,Minpeng Liao,Chengxi Li,Kai Fan

Recent advancements in large language models (LLMs) have substantially
enhanced their mathematical reasoning abilities. However, these models still
struggle with complex problems that require multiple reasoning steps,
frequently leading to logical or numerical errors. While numerical mistakes can
largely be addressed by integrating a code interpreter, identifying logical
errors within intermediate steps is more challenging. Moreover, manually
annotating these steps for training is not only expensive but also demands
specialized expertise. In this study, we introduce an innovative approach that
eliminates the need for manual annotation by leveraging the Monte Carlo Tree
Search (MCTS) framework to generate both the process supervision and evaluation
signals automatically. Essentially, when a LLM is well pre-trained, only the
mathematical questions and their final answers are required to generate our
training data, without requiring the solutions. We proceed to train a
step-level value model designed to improve the LLM's inference process in
mathematical domains. Our experiments indicate that using automatically
generated solutions by LLMs enhanced with MCTS significantly improves the
model's proficiency in dealing with intricate mathematical reasoning tasks.

摘要：

##### **MAmmoTH2: Scaling Instructions from the Web**
2405.03548v1 by Xiang Yue,Tuney Zheng,Ge Zhang,Wenhu Chen

Instruction tuning improves the reasoning abilities of large language models
(LLMs), with data quality and scalability being the crucial factors. Most
instruction tuning data come from human crowd-sourcing or GPT-4 distillation.
We propose a paradigm to efficiently harvest 10 million naturally existing
instruction data from the pre-training web corpus to enhance LLM reasoning. Our
approach involves (1) recalling relevant documents, (2) extracting
instruction-response pairs, and (3) refining the extracted pairs using
open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2
models, which significantly boost performance on reasoning benchmarks. Notably,
MAmmoTH2-7B's (Mistral) performance increases from 11% to 34% on MATH and from
36% to 67% on GSM8K without training on any in-domain data. Further training
MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving
state-of-the-art performance on several reasoning and chatbot benchmarks. Our
work demonstrates how to harvest large-scale, high-quality instruction data
without costly human annotation or GPT-4 distillation, providing a new paradigm
for building better instruction tuning data.

摘要：

##### **Position Paper: Leveraging Foundational Models for Black-Box Optimization: Benefits, Challenges, and Future Directions**
2405.03547v1 by Xingyou Song,Yingtao Tian,Robert Tjarko Lange,Chansoo Lee,Yujin Tang,Yutian Chen

Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave
of innovation in the machine learning research domain, resulting in substantial
impact across diverse fields such as reinforcement learning, robotics, and
computer vision. Their incorporation has been rapid and transformative, marking
a significant paradigm shift in the field of machine learning research.
  However, the field of experimental design, grounded on black-box
optimization, has been much less affected by such a paradigm shift, even though
integrating LLMs with optimization presents a unique landscape ripe for
exploration. In this position paper, we frame the field of black-box
optimization around sequence-based foundation models and organize their
relationship with previous literature. We discuss the most promising ways
foundational language models can revolutionize optimization, which include
harnessing the vast wealth of information encapsulated in free-form text to
enrich task comprehension, utilizing highly flexible sequence models such as
Transformers to engineer superior optimization strategies, and enhancing
performance prediction over previously unseen search spaces.

摘要：

##### **Exploring the Efficacy of Federated-Continual Learning Nodes with Attention-Based Classifier for Robust Web Phishing Detection: An Empirical Investigation**
2405.03537v1 by Jesher Joshua M,Adhithya R,Sree Dananjay S,M Revathi

Web phishing poses a dynamic threat, requiring detection systems to quickly
adapt to the latest tactics. Traditional approaches of accumulating data and
periodically retraining models are outpaced. We propose a novel paradigm
combining federated learning and continual learning, enabling distributed nodes
to continually update models on streams of new phishing data, without
accumulating data. These locally adapted models are then aggregated at a
central server via federated learning. To enhance detection, we introduce a
custom attention-based classifier model with residual connections, tailored for
web phishing, leveraging attention mechanisms to capture intricate phishing
patterns. We evaluate our hybrid learning paradigm across continual learning
strategies (cumulative, replay, MIR, LwF) and model architectures through an
empirical investigation. Our main contributions are: (1) a new hybrid
federated-continual learning paradigm for robust web phishing detection, and
(2) a novel attention + residual connections based model explicitly designed
for this task, attaining 0.93 accuracy, 0.90 precision, 0.96 recall and 0.93
f1-score with the LwF strategy, outperforming traditional approaches in
detecting emerging phishing threats while retaining past knowledge.

摘要：

##### **A Rate-Distortion-Classification Approach for Lossy Image Compression**
2405.03500v1 by Yuefeng Zhang

In lossy image compression, the objective is to achieve minimal signal
distortion while compressing images to a specified bit rate. The increasing
demand for visual analysis applications, particularly in classification tasks,
has emphasized the significance of considering semantic distortion in
compressed images. To bridge the gap between image compression and visual
analysis, we propose a Rate-Distortion-Classification (RDC) model for lossy
image compression, offering a unified framework to optimize the trade-off
between rate, distortion, and classification accuracy. The RDC model is
extensively analyzed both statistically on a multi-distribution source and
experimentally on the widely used MNIST dataset. The findings reveal that the
RDC model exhibits desirable properties, including monotonic non-increasing and
convex functions, under certain conditions. This work provides insights into
the development of human-machine friendly compression methods and Video Coding
for Machine (VCM) approaches, paving the way for end-to-end image compression
techniques in real-world applications.

摘要：

##### **A Lightweight Neural Architecture Search Model for Medical Image Classification**
2405.03462v1 by Lunchen Xie,Eugenio Lomurno,Matteo Gambella,Danilo Ardagna,Manuel Roveri,Matteo Matteucci,Qingjiang Shi

Accurate classification of medical images is essential for modern
diagnostics. Deep learning advancements led clinicians to increasingly use
sophisticated models to make faster and more accurate decisions, sometimes
replacing human judgment. However, model development is costly and repetitive.
Neural Architecture Search (NAS) provides solutions by automating the design of
deep learning architectures. This paper presents ZO-DARTS+, a differentiable
NAS algorithm that improves search efficiency through a novel method of
generating sparse probabilities by bi-level optimization. Experiments on five
public medical datasets show that ZO-DARTS+ matches the accuracy of
state-of-the-art solutions while reducing search times by up to three times.

摘要：

##### **Large Language Models (LLMs) as Agents for Augmented Democracy**
2405.03452v2 by Jairo Gudiño-Rosero,Umberto Grandi,César A. Hidalgo

We explore the capabilities of an augmented democracy system built on
off-the-shelf LLMs fine-tuned on data summarizing individual preferences across
67 policy proposals collected during the 2022 Brazilian presidential elections.
We use a train-test cross-validation setup to estimate the accuracy with which
the LLMs predict both: a subject's individual political choices and the
aggregate preferences of the full sample of participants. At the individual
level, the accuracy of the out of sample predictions lie in the range 69%-76%
and are significantly better at predicting the preferences of liberal and
college educated participants. At the population level, we aggregate
preferences using an adaptation of the Borda score and compare the ranking of
policy proposals obtained from a probabilistic sample of participants and from
data augmented using LLMs. We find that the augmented data predicts the
preferences of the full population of participants better than probabilistic
samples alone when these represent less than 30% to 40% of the total
population. These results indicate that LLMs are potentially useful for the
construction of systems of augmented democracy.

摘要：

##### **Robotic Constrained Imitation Learning for the Peg Transfer Task in Fundamentals of Laparoscopic Surgery**
2405.03440v1 by Kento Kawaharazuka,Kei Okada,Masayuki Inaba

In this study, we present an implementation strategy for a robot that
performs peg transfer tasks in Fundamentals of Laparoscopic Surgery (FLS) via
imitation learning, aimed at the development of an autonomous robot for
laparoscopic surgery. Robotic laparoscopic surgery presents two main
challenges: (1) the need to manipulate forceps using ports established on the
body surface as fulcrums, and (2) difficulty in perceiving depth information
when working with a monocular camera that displays its images on a monitor.
Especially, regarding issue (2), most prior research has assumed the
availability of depth images or models of a target to be operated on.
Therefore, in this study, we achieve more accurate imitation learning with only
monocular images by extracting motion constraints from one exemplary motion of
skilled operators, collecting data based on these constraints, and conducting
imitation learning based on the collected data. We implemented an overall
system using two Franka Emika Panda Robot Arms and validated its effectiveness.

摘要：

##### **A method for quantifying the generalization capabilities of generative models for solving Ising models**
2405.03435v1 by Qunlong Ma,Zhi Ma,Ming Gao

For Ising models with complex energy landscapes, whether the ground state can
be found by neural networks depends heavily on the Hamming distance between the
training datasets and the ground state. Despite the fact that various recently
proposed generative models have shown good performance in solving Ising models,
there is no adequate discussion on how to quantify their generalization
capabilities. Here we design a Hamming distance regularizer in the framework of
a class of generative models, variational autoregressive networks (VAN), to
quantify the generalization capabilities of various network architectures
combined with VAN. The regularizer can control the size of the overlaps between
the ground state and the training datasets generated by networks, which,
together with the success rates of finding the ground state, form a
quantitative metric to quantify their generalization capabilities. We conduct
numerical experiments on several prototypical network architectures combined
with VAN, including feed-forward neural networks, recurrent neural networks,
and graph neural networks, to quantify their generalization capabilities when
solving Ising models. Moreover, considering the fact that the quantification of
the generalization capabilities of networks on small-scale problems can be used
to predict their relative performance on large-scale problems, our method is of
great significance for assisting in the Neural Architecture Search field of
searching for the optimal network architectures when solving large-scale Ising
models.

摘要：

##### **ReCycle: Fast and Efficient Long Time Series Forecasting with Residual Cyclic Transformers**
2405.03429v1 by Arvid Weyrauch,Thomas Steens,Oskar Taubert,Benedikt Hanke,Aslan Eqbal,Ewa Götz,Achim Streit,Markus Götz,Charlotte Debus

Transformers have recently gained prominence in long time series forecasting
by elevating accuracies in a variety of use cases. Regrettably, in the race for
better predictive performance the overhead of model architectures has grown
onerous, leading to models with computational demand infeasible for most
practical applications. To bridge the gap between high method complexity and
realistic computational resources, we introduce the Residual Cyclic
Transformer, ReCycle. ReCycle utilizes primary cycle compression to address the
computational complexity of the attention mechanism in long time series. By
learning residuals from refined smoothing average techniques, ReCycle surpasses
state-of-the-art accuracy in a variety of application use cases. The reliable
and explainable fallback behavior ensured by simple, yet robust, smoothing
average techniques additionally lowers the barrier for user acceptance. At the
same time, our approach reduces the run time and energy consumption by more
than an order of magnitude, making both training and inference feasible on
low-performance, low-power and edge computing devices. Code is available at
https://github.com/Helmholtz-AI-Energy/ReCycle

摘要：

##### **Gaussian Stochastic Weight Averaging for Bayesian Low-Rank Adaptation of Large Language Models**
2405.03425v1 by Emre Onal,Klemens Flöge,Emma Caldwell,Arsen Sheverdin,Vincent Fortuin

Fine-tuned Large Language Models (LLMs) often suffer from overconfidence and
poor calibration, particularly when fine-tuned on small datasets. To address
these challenges, we propose a simple combination of Low-Rank Adaptation (LoRA)
with Gaussian Stochastic Weight Averaging (SWAG), facilitating approximate
Bayesian inference in LLMs. Through extensive testing across several Natural
Language Processing (NLP) benchmarks, we demonstrate that our straightforward
and computationally efficient approach improves model generalization and
calibration. We further show that our method exhibits greater robustness
against distribution shift, as reflected in its performance on
out-of-distribution tasks.

摘要：

##### **Automated Computation of Therapies Using Failure Mode and Effects Analysis in the Medical Domain**
2405.03406v1 by Malte Luttermann,Edgar Baake,Juljan Bouchagiar,Benjamin Gebel,Philipp Grüning,Dilini Manikwadura,Franziska Schollemann,Elisa Teifke,Philipp Rostalski,Ralf Möller

Failure mode and effects analysis (FMEA) is a systematic approach to identify
and analyse potential failures and their effects in a system or process. The
FMEA approach, however, requires domain experts to manually analyse the FMEA
model to derive risk-reducing actions that should be applied. In this paper, we
provide a formal framework to allow for automatic planning and acting in FMEA
models. More specifically, we cast the FMEA model into a Markov decision
process which can then be solved by existing solvers. We show that the FMEA
approach can not only be used to support medical experts during the modelling
process but also to automatically derive optimal therapies for the treatment of
patients.

摘要：

##### **E2GNN: Efficient Graph Neural Network Ensembles for Semi-Supervised Classification**
2405.03401v1 by Xin Zhang,Daochen Zha,Qiaoyu Tan

This work studies ensemble learning for graph neural networks (GNNs) under
the popular semi-supervised setting. Ensemble learning has shown superiority in
improving the accuracy and robustness of traditional machine learning by
combining the outputs of multiple weak learners. However, adopting a similar
idea to integrate different GNN models is challenging because of two reasons.
First, GNN is notorious for its poor inference ability, so naively assembling
multiple GNN models would deteriorate the inference efficiency. Second, when
GNN models are trained with few labeled nodes, their performance are limited.
In this case, the vanilla ensemble approach, e.g., majority vote, may be
sub-optimal since most base models, i.e., GNNs, may make the wrong predictions.
To this end, in this paper, we propose an efficient ensemble learner--E2GNN to
assemble multiple GNNs in a learnable way by leveraging both labeled and
unlabeled nodes. Specifically, we first pre-train different GNN models on a
given data scenario according to the labeled nodes. Next, instead of directly
combing their outputs for label inference, we train a simple multi-layer
perceptron--MLP model to mimic their predictions on both labeled and unlabeled
nodes. Then the unified MLP model is deployed to infer labels for unlabeled or
new nodes. Since the predictions of unlabeled nodes from different GNN models
may be incorrect, we develop a reinforced discriminator to effectively filter
out those wrongly predicted nodes to boost the performance of MLP. By doing
this, we suggest a principled approach to tackle the inference issues of GNN
ensembles and maintain the merit of ensemble learning: improved performance.
Comprehensive experiments over both transductive and inductive settings, across
different GNN backbones and 8 benchmark datasets, demonstrate the superiority
of E2GNN.

摘要：

##### **Don't Waste Your Time: Early Stopping Cross-Validation**
2405.03389v1 by Edward Bergman,Lennart Purucker,Frank Hutter

State-of-the-art automated machine learning systems for tabular data often
employ cross-validation; ensuring that measured performances generalize to
unseen data, or that subsequent ensembling does not overfit. However, using
k-fold cross-validation instead of holdout validation drastically increases the
computational cost of validating a single configuration. While ensuring better
generalization and, by extension, better performance, the additional cost is
often prohibitive for effective model selection within a time budget. We aim to
make model selection with cross-validation more effective. Therefore, we study
early stopping the process of cross-validation during model selection. We
investigate the impact of early stopping on random search for two algorithms,
MLP and random forest, across 36 classification datasets. We further analyze
the impact of the number of folds by considering 3-, 5-, and 10-folds. In
addition, we investigate the impact of early stopping with Bayesian
optimization instead of random search and also repeated cross-validation. Our
exploratory study shows that even a simple-to-understand and easy-to-implement
method consistently allows model selection to converge faster; in ~94% of all
datasets, on average by ~214%. Moreover, stopping cross-validation enables
model selection to explore the search space more exhaustively by considering
+167% configurations on average within one hour, while also obtaining better
overall performance.

摘要：

##### **The high dimensional psychological profile and cultural bias of ChatGPT**
2405.03387v1 by Hang Yuan,Zhongyue Che,Shao Li,Yue Zhang,Xiaomeng Hu,Siyang Luo

Given the rapid advancement of large-scale language models, artificial
intelligence (AI) models, like ChatGPT, are playing an increasingly prominent
role in human society. However, to ensure that artificial intelligence models
benefit human society, we must first fully understand the similarities and
differences between the human-like characteristics exhibited by artificial
intelligence models and real humans, as well as the cultural stereotypes and
biases that artificial intelligence models may exhibit in the process of
interacting with humans. This study first measured ChatGPT in 84 dimensions of
psychological characteristics, revealing differences between ChatGPT and human
norms in most dimensions as well as in high-dimensional psychological
representations. Additionally, through the measurement of ChatGPT in 13
dimensions of cultural values, it was revealed that ChatGPT's cultural value
patterns are dissimilar to those of various countries/regions worldwide.
Finally, an analysis of ChatGPT's performance in eight decision-making tasks
involving interactions with humans from different countries/regions revealed
that ChatGPT exhibits clear cultural stereotypes in most decision-making tasks
and shows significant cultural bias in third-party punishment and ultimatum
games. The findings indicate that, compared to humans, ChatGPT exhibits a
distinct psychological profile and cultural value orientation, and it also
shows cultural biases and stereotypes in interpersonal decision-making. Future
research endeavors should emphasize enhanced technical oversight and augmented
transparency in the database and algorithmic training procedures to foster more
efficient cross-cultural communication and mitigate social disparities.

摘要：

##### **Snake Learning: A Communication- and Computation-Efficient Distributed Learning Framework for 6G**
2405.03372v1 by Xiaoxue Yu,Xingfu Yi,Rongpeng Li,Fei Wang,Chenghui Peng,Zhifeng Zhao,Honggang Zhang

In the evolution towards 6G, integrating Artificial Intelligence (AI) with
advanced network infrastructure emerges as a pivotal strategy for enhancing
network intelligence and resource utilization. Existing distributed learning
frameworks like Federated Learning and Split Learning often struggle with
significant challenges in dynamic network environments including high
synchronization demands, costly communication overheads, severe computing
resource consumption, and data heterogeneity across network nodes. These
obstacles hinder the applications of ubiquitous computing capabilities of 6G
networks, especially in light of the trend of escalating model parameters and
training data volumes. To address these challenges effectively, this paper
introduces "Snake Learning", a cost-effective distributed learning framework.
Specifically, Snake Learning respects the heterogeneity of inter-node computing
capability and local data distribution in 6G networks, and sequentially trains
the designated part of model layers on individual nodes. This layer-by-layer
serpentine update mechanism contributes to significantly reducing the
requirements for storage, memory and communication during the model training
phase, and demonstrates superior adaptability and efficiency for both Computer
Vision (CV) training and Large Language Model (LLM) fine-tuning tasks across
homogeneous and heterogeneous data distributions.

摘要：

##### **Explainable Fake News Detection With Large Language Model via Defense Among Competing Wisdom**
2405.03371v1 by Bo Wang,Jing Ma,Hongzhan Lin,Zhiwei Yang,Ruichao Yang,Yuan Tian,Yi Chang

Most fake news detection methods learn latent feature representations based
on neural networks, which makes them black boxes to classify a piece of news
without giving any justification. Existing explainable systems generate
veracity justifications from investigative journalism, which suffer from
debunking delayed and low efficiency. Recent studies simply assume that the
justification is equivalent to the majority opinions expressed in the wisdom of
crowds. However, the opinions typically contain some inaccurate or biased
information since the wisdom of crowds is uncensored. To detect fake news from
a sea of diverse, crowded and even competing narratives, in this paper, we
propose a novel defense-based explainable fake news detection framework.
Specifically, we first propose an evidence extraction module to split the
wisdom of crowds into two competing parties and respectively detect salient
evidences. To gain concise insights from evidences, we then design a
prompt-based module that utilizes a large language model to generate
justifications by inferring reasons towards two possible veracities. Finally,
we propose a defense-based inference module to determine veracity via modeling
the defense among these justifications. Extensive experiments conducted on two
real-world benchmarks demonstrate that our proposed method outperforms
state-of-the-art baselines in terms of fake news detection and provides
high-quality justifications.

摘要：

##### **MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models in the Context of the Pediatric Hypertension Guideline**
2405.03359v1 by Mohamed Yaseen Jabarulla,Steffen Oeltze-Jafra,Philipp Beerbaum,Theodor Uden

This research focuses on evaluating the non-commercial open-source large
language models (LLMs) Meditron, MedAlpaca, Mistral, and Llama-2 for their
efficacy in interpreting medical guidelines saved in PDF format. As a specific
test scenario, we applied these models to the guidelines for hypertension in
children and adolescents provided by the European Society of Cardiology (ESC).
Leveraging Streamlit, a Python library, we developed a user-friendly medical
document chatbot tool (MedDoc-Bot). This tool enables authorized users to
upload PDF files and pose questions, generating interpretive responses from
four locally stored LLMs. A pediatric expert provides a benchmark for
evaluation by formulating questions and responses extracted from the ESC
guidelines. The expert rates the model-generated responses based on their
fidelity and relevance. Additionally, we evaluated the METEOR and chrF metric
scores to assess the similarity of model responses to reference answers. Our
study found that Llama-2 and Mistral performed well in metrics evaluation.
However, Llama-2 was slower when dealing with text and tabular data. In our
human evaluation, we observed that responses created by Mistral, Meditron, and
Llama-2 exhibited reasonable fidelity and relevance. This study provides
valuable insights into the strengths and limitations of LLMs for future
developments in medical document interpretation. Open-Source Code:
https://github.com/yaseen28/MedDoc-Bot

摘要：

##### **Enhancing Q-Learning with Large Language Model Heuristics**
2405.03341v1 by Xiefeng Wu

Q-learning excels in learning from feedback within sequential decision-making
tasks but requires extensive sampling for significant improvements. Although
reward shaping is a powerful technique for enhancing learning efficiency, it
can introduce biases that affect agent performance. Furthermore,
potential-based reward shaping is constrained as it does not allow for reward
modifications based on actions or terminal states, potentially limiting its
effectiveness in complex environments. Additionally, large language models
(LLMs) can achieve zero-shot learning, but this is generally limited to simpler
tasks. They also exhibit low inference speeds and occasionally produce
hallucinations. To address these issues, we propose \textbf{LLM-guided
Q-learning} that employs LLMs as heuristic to aid in learning the Q-function
for reinforcement learning. It combines the advantages of both technologies
without introducing performance bias. Our theoretical analysis demonstrates
that the LLM heuristic provides action-level guidance. Additionally, our
architecture has the capability to convert the impact of hallucinations into
exploration costs. Moreover, the converged Q function corresponds to the MDP
optimal Q function. Experiment results demonstrated that our algorithm enables
agents to avoid ineffective exploration, enhances sampling efficiency, and is
well-suited for complex control tasks.

摘要：

##### **Functional Equivalence with NARS**
2405.03340v1 by Robert Johansson,Patrick Hammer,Tony Lofthouse

This study explores the concept of functional equivalence within the
framework of the Non-Axiomatic Reasoning System (NARS), specifically through
OpenNARS for Applications (ONA). Functional equivalence allows organisms to
categorize and respond to varied stimuli based on their utility rather than
perceptual similarity, thus enhancing cognitive efficiency and adaptability. In
this study, ONA was modified to allow the derivation of functional equivalence.
This paper provides practical examples of the capability of ONA to apply
learned knowledge across different functional situations, demonstrating its
utility in complex problem-solving and decision-making. An extended example is
included, where training of ONA aimed to learn basic human-like language
abilities, using a systematic procedure in relating spoken words, objects and
written words. The research carried out as part of this study extends the
understanding of functional equivalence in AGI systems, and argues for its
necessity for level of flexibility in learning and adapting necessary for
human-level AGI.

摘要：

##### **Enhancing Spatiotemporal Disease Progression Models via Latent Diffusion and Prior Knowledge**
2405.03328v1 by Lemuel Puglisi,Daniel C. Alexander,Daniele Ravì

In this work, we introduce Brain Latent Progression (BrLP), a novel
spatiotemporal disease progression model based on latent diffusion. BrLP is
designed to predict the evolution of diseases at the individual level on 3D
brain MRIs. Existing deep generative models developed for this task are
primarily data-driven and face challenges in learning disease progressions.
BrLP addresses these challenges by incorporating prior knowledge from disease
models to enhance the accuracy of predictions. To implement this, we propose to
integrate an auxiliary model that infers volumetric changes in various brain
regions. Additionally, we introduce Latent Average Stabilization (LAS), a novel
technique to improve spatiotemporal consistency of the predicted progression.
BrLP is trained and evaluated on a large dataset comprising 11,730 T1-weighted
brain MRIs from 2,805 subjects, collected from three publicly available,
longitudinal Alzheimer's Disease (AD) studies. In our experiments, we compare
the MRI scans generated by BrLP with the actual follow-up MRIs available from
the subjects, in both cross-sectional and longitudinal settings. BrLP
demonstrates significant improvements over existing methods, with an increase
of 22% in volumetric accuracy across AD-related brain regions and 43% in image
similarity to the ground-truth scans. The ability of BrLP to generate
conditioned 3D scans at the subject level, along with the novelty of
integrating prior knowledge to enhance accuracy, represents a significant
advancement in disease progression modeling, opening new avenues for precision
medicine. The code of BrLP is available at the following link:
https://github.com/LemuelPuglisi/BrLP.

摘要：

##### **Artificial Intelligence in the Autonomous Navigation of Endovascular Interventions: A Systematic Review**
2405.03305v1 by Harry Robertshaw,Lennart Karstensen,Benjamin Jackson,Hadi Sadati,Kawal Rhode,Sebastien Ourselin,Alejandro Granados,Thomas C Booth

Purpose: Autonomous navigation of devices in endovascular interventions can
decrease operation times, improve decision-making during surgery, and reduce
operator radiation exposure while increasing access to treatment. This
systematic review explores recent literature to assess the impact, challenges,
and opportunities artificial intelligence (AI) has for the autonomous
endovascular intervention navigation.
  Methods: PubMed and IEEEXplore databases were queried. Eligibility criteria
included studies investigating the use of AI in enabling the autonomous
navigation of catheters/guidewires in endovascular interventions. Following
PRISMA, articles were assessed using QUADAS-2. PROSPERO: CRD42023392259.
  Results: Among 462 studies, fourteen met inclusion criteria. Reinforcement
learning (9/14, 64%) and learning from demonstration (7/14, 50%) were used as
data-driven models for autonomous navigation. Studies predominantly utilised
physical phantoms (10/14, 71%) and in silico (4/14, 29%) models. Experiments
within or around the blood vessels of the heart were reported by the majority
of studies (10/14, 71%), while simple non-anatomical vessel platforms were used
in three studies (3/14, 21%), and the porcine liver venous system in one study.
We observed that risk of bias and poor generalisability were present across
studies. No procedures were performed on patients in any of the studies
reviewed. Studies lacked patient selection criteria, reference standards, and
reproducibility, resulting in low clinical evidence levels.
  Conclusions: AI's potential in autonomous endovascular navigation is
promising, but in an experimental proof-of-concept stage, with a technology
readiness level of 3. We highlight that reference standards with
well-identified performance metrics are crucial to allow for comparisons of
data-driven algorithms proposed in the years to come.

摘要：

##### **Coefficient Decomposition for Spectral Graph Convolution**
2405.03296v1 by Feng Huang,Wen Zhang

Spectral graph convolutional network (SGCN) is a kind of graph neural
networks (GNN) based on graph signal filters, and has shown compelling
expressivity for modeling graph-structured data. Most SGCNs adopt polynomial
filters and learn the coefficients from the training data. Many of them focus
on which polynomial basis leads to optimal expressive power and models'
architecture is little discussed. In this paper, we propose a general form in
terms of spectral graph convolution, where the coefficients of polynomial basis
are stored in a third-order tensor. Then, we show that the convolution block in
existing SGCNs can be derived by performing a certain coefficient decomposition
operation on the coefficient tensor. Based on the generalized view, we develop
novel spectral graph convolutions CoDeSGC-CP and -Tucker by tensor
decomposition CP and Tucker on the coefficient tensor. Extensive experimental
results demonstrate that the proposed convolutions achieve favorable
performance improvements.

摘要：

##### **Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity**
2405.03280v1 by Yizhuo Lu,Changde Du,Chong Wang,Xuanliu Zhu,Liuyun Jiang,Huiguang He

Reconstructing human dynamic vision from brain activity is a challenging task
with great scientific significance. The difficulty stems from two primary
issues: (1) vision-processing mechanisms in the brain are highly intricate and
not fully revealed, making it challenging to directly learn a mapping between
fMRI and video; (2) the temporal resolution of fMRI is significantly lower than
that of natural videos. To overcome these issues, this paper propose a
two-stage model named Mind-Animator, which achieves state-of-the-art
performance on three public datasets. Specifically, during the fMRI-to-feature
stage, we decouple semantic, structural, and motion features from fMRI through
fMRI-vision-language tri-modal contrastive learning and sparse causal
attention. In the feature-to-video stage, these features are merged to videos
by an inflated Stable Diffusion. We substantiate that the reconstructed video
dynamics are indeed derived from fMRI, rather than hallucinations of the
generative model, through permutation tests. Additionally, the visualization of
voxel-wise and ROI-wise importance maps confirms the neurobiological
interpretability of our model.

摘要：

##### **Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning**
2405.03279v1 by Qizhou Chen,Taolin Zhang,Dongyang Li,Longtao Huang,Hui Xue,Chengyu Wang,Xiaofeng He

Model editing aims to correct outdated or erroneous knowledge in large
language models (LLMs) without the need for costly retraining. Lifelong model
editing is the most challenging task that caters to the continuous editing
requirements of LLMs. Prior works primarily focus on single or batch editing;
nevertheless, these methods fall short in lifelong editing scenarios due to
catastrophic knowledge forgetting and the degradation of model performance.
Although retrieval-based methods alleviate these issues, they are impeded by
slow and cumbersome processes of integrating the retrieved knowledge into the
model. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous
Prompt lEarning method, to boost editing efficacy and inference efficiency in
lifelong learning. RECIPE first converts knowledge statements into short and
informative continuous prompts, prefixed to the LLM's input query embedding, to
efficiently refine the response grounded on the knowledge. It further
integrates the Knowledge Sentinel (KS) that acts as an intermediary to
calculate a dynamic threshold, determining whether the retrieval repository
contains relevant knowledge. Our retriever and prompt encoder are jointly
trained to achieve editing properties, i.e., reliability, generality, and
locality. In our experiments, RECIPE is assessed extensively across multiple
LLMs and editing datasets, where it achieves superior editing performance.
RECIPE also demonstrates its capability to maintain the overall performance of
LLMs alongside showcasing fast editing and inference speed.

摘要：

##### **Exploring the Frontiers of Softmax: Provable Optimization, Applications in Diffusion Model, and Beyond**
2405.03251v1 by Jiuxiang Gu,Chenyang Li,Yingyu Liang,Zhenmei Shi,Zhao Song

The softmax activation function plays a crucial role in the success of large
language models (LLMs), particularly in the self-attention mechanism of the
widely adopted Transformer architecture. However, the underlying learning
dynamics that contribute to the effectiveness of softmax remain largely
unexplored. As a step towards better understanding, this paper provides a
theoretical study of the optimization and generalization properties of
two-layer softmax neural networks, providing theoretical insights into their
superior performance as other activation functions, such as ReLU and
exponential. Leveraging the Neural Tangent Kernel (NTK) framework, our analysis
reveals that the normalization effect of the softmax function leads to a good
perturbation property of the induced NTK matrix, resulting in a good convex
region of the loss landscape. Consequently, softmax neural networks can learn
the target function in the over-parametrization regime. To demonstrate the
broad applicability of our theoretical findings, we apply them to the task of
learning score estimation functions in diffusion models, a promising approach
for generative modeling. Our analysis shows that gradient-based algorithms can
learn the score function with a provable accuracy. Our work provides a deeper
understanding of the effectiveness of softmax neural networks and their
potential in various domains, paving the way for further advancements in
natural language processing and beyond.

摘要：

##### **Communication-Efficient Federated Learning with Adaptive Compression under Dynamic Bandwidth**
2405.03248v1 by Ying Zhuansun,Dandan Li,Xiaohong Huang,Caijun Sun

Federated learning can train models without directly providing local data to
the server. However, the frequent updating of the local model brings the
problem of large communication overhead. Recently, scholars have achieved the
communication efficiency of federated learning mainly by model compression. But
they ignore two problems: 1) network state of each client changes dynamically;
2) network state among clients is not the same. The clients with poor bandwidth
update local model slowly, which leads to low efficiency. To address this
challenge, we propose a communication-efficient federated learning algorithm
with adaptive compression under dynamic bandwidth (called AdapComFL).
Concretely, each client performs bandwidth awareness and bandwidth prediction.
Then, each client adaptively compresses its local model via the improved sketch
mechanism based on his predicted bandwidth. Further, the server aggregates
sketched models with different sizes received. To verify the effectiveness of
the proposed method, the experiments are based on real bandwidth data which are
collected from the network topology we build, and benchmark datasets which are
obtained from open repositories. We show the performance of AdapComFL
algorithm, and compare it with existing algorithms. The experimental results
show that our AdapComFL achieves more efficient communication as well as
competitive accuracy compared to existing algorithms.

摘要：

##### **Deep Learning for Detecting and Early Predicting Chronic Obstructive Pulmonary Disease from Spirogram Time Series: A UK Biobank Study**
2405.03239v1 by Shuhao Mei,Yuxi Zhou,Jiahao Xu,Yuxuan Wan,Shan Cao,Qinghao Zhao,Shijia Geng,Junqing Xie,Shenda Hong

Chronic Obstructive Pulmonary Disease (COPD) is a chronic inflammatory lung
condition that causes airflow obstruction. The existing methods can only detect
patients who already have COPD based on obvious features shown in the spirogram
(In this article, the spirogram specifically involves measuring Volume-Flow
curve time series). Early prediction of COPD risk is vital for monitoring COPD
disease progression, slowing it down, or even preventing its onset. However,
these methods fail to early predict an individual's probability of COPD in the
future based on subtle features in the spirogram. To address this gap, for the
first time, we propose DeepSpiro, a method based on deep learning for early
prediction of future COPD risk. DeepSpiro consists of four parts. First, we
construct Volume-Flow curves guided by Time-Volume instability smoothing
(SpiroSmoother) to enhance the stability of the original Volume-Flow curves
precisely. Second, we extract critical features from the evolution of
varied-length key patches (SpiroEncoder) to capture the key temporal evolution
from original high-dimensional dynamic sequences to a unified low-dimensional
temporal representation. Third, we explain the model based on temporal
attention and heterogeneous feature fusion (SpiroExplainer), which integrates
information from heterogeneous data such as spirogram and demographic
information. Fourth, we predict the risk of COPD based on the evolution of key
patch concavity (SpiroPredictor), enabling accurate prediction of the risk of
disease in high-risk patients who are not yet diagnosed, for up to 1, 2, 3, 4,
5 years, and beyond. We conduct experiments on the UK Biobank dataset. Results
show that DeepSpiro achieves an AUC value of 0.8328 in the task of detecting
COPD. In early prediction tasks, high-risk and low-risk groups show significant
differences in the future, with a p-value of <0.001.

摘要：

##### **A Philosophical Introduction to Language Models - Part II: The Way Forward**
2405.03207v1 by Raphaël Millière,Cameron Buckner

In this paper, the second of two companion pieces, we explore novel
philosophical questions raised by recent progress in large language models
(LLMs) that go beyond the classical debates covered in the first part. We focus
particularly on issues related to interpretability, examining evidence from
causal intervention methods about the nature of LLMs' internal representations
and computations. We also discuss the implications of multimodal and modular
extensions of LLMs, recent debates about whether such systems may meet minimal
criteria for consciousness, and concerns about secrecy and reproducibility in
LLM research. Finally, we discuss whether LLM-like systems may be relevant to
modeling aspects of human cognition, if their architectural characteristics and
learning scenario are adequately constrained.

摘要：

##### **Vietnamese AI Generated Text Detection**
2405.03206v1 by Quang-Dan Tran,Van-Quan Nguyen,Quang-Huy Pham,K. B. Thang Nguyen,Trong-Hop Do

In recent years, Large Language Models (LLMs) have become integrated into our
daily lives, serving as invaluable assistants in completing tasks. Widely
embraced by users, the abuse of LLMs is inevitable, particularly in using them
to generate text content for various purposes, leading to difficulties in
distinguishing between text generated by LLMs and that written by humans. In
this study, we present a dataset named ViDetect, comprising 6.800 samples of
Vietnamese essay, with 3.400 samples authored by humans and the remainder
generated by LLMs, serving the purpose of detecting text generated by AI. We
conducted evaluations using state-of-the-art methods, including ViT5, BartPho,
PhoBERT, mDeberta V3, and mBERT. These results contribute not only to the
growing body of research on detecting text generated by AI but also demonstrate
the adaptability and effectiveness of different methods in the Vietnamese
language context. This research lays the foundation for future advancements in
AI-generated text detection and provides valuable insights for researchers in
the field of natural language processing.

摘要：

##### **Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions**
2405.03205v1 by Ruizhe Li,Yanjun Gao

Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have
demonstrated considerable success across diverse tasks, including
multiple-choice questions (MCQs). However, these models exhibit a positional
bias, particularly an even worse anchored bias in the GPT-2 family, where they
consistently favour the first choice 'A' in MCQs during inference. This
anchored bias challenges the integrity of GPT-2's decision-making process, as
it skews performance based on the position rather than the content of the
choices in MCQs. In this study, we utilise the mechanistic interpretability
approach to identify the internal modules within GPT-2 models responsible for
this bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention
heads, using the "logit lens" method to trace and modify the specific value
vectors that contribute to the bias. By updating these vectors within MLP and
recalibrating attention patterns to neutralise the preference for the first
choice 'A', we effectively mitigate the anchored bias. Our interventions not
only correct the bias but also improve the overall MCQ prediction accuracy for
the GPT-2 family across various datasets. This work represents the first
comprehensive mechanistic analysis of anchored bias in MCQs within the GPT-2
models, introducing targeted, minimal-intervention strategies that
significantly enhance GPT2 model robustness and accuracy in MCQs. Our code is
available at https://github.com/ruizheliUOA/Anchored_Bias_GPT2.

摘要：

##### **QuadraNet V2: Efficient and Sustainable Training of High-Order Neural Networks with Quadratic Adaptation**
2405.03192v1 by Chenhui Xu,Xinyao Wang,Fuxun Yu,JInjun Xiong,Xiang Chen

Machine learning is evolving towards high-order models that necessitate
pre-training on extensive datasets, a process associated with significant
overheads. Traditional models, despite having pre-trained weights, are becoming
obsolete due to architectural differences that obstruct the effective transfer
and initialization of these weights. To address these challenges, we introduce
a novel framework, QuadraNet V2, which leverages quadratic neural networks to
create efficient and sustainable high-order learning models. Our method
initializes the primary term of the quadratic neuron using a standard neural
network, while the quadratic term is employed to adaptively enhance the
learning of data non-linearity or shifts. This integration of pre-trained
primary terms with quadratic terms, which possess advanced modeling
capabilities, significantly augments the information characterization capacity
of the high-order network. By utilizing existing pre-trained weights, QuadraNet
V2 reduces the required GPU hours for training by 90\% to 98.4\% compared to
training from scratch, demonstrating both efficiency and effectiveness.

摘要：

##### **Oracle-Checker Scheme for Evaluating a Generative Large Language Model**
2405.03170v1 by Yueling Jenny Zeng,Li-C. Wang,Thomas Ibbetson

This work presents a novel approach called oracle-checker scheme for
evaluating the answer given by a generative large language model (LLM). Two
types of checkers are presented. The first type of checker follows the idea of
property testing. The second type of checker follows the idea of program
checking. Their applications are demonstrated in two separate contexts, entity
extraction and paraphrase decision, respectively.

摘要：

##### **The Role of Predictive Uncertainty and Diversity in Embodied AI and Robot Learning**
2405.03164v1 by Ransalu Senanayake

Uncertainty has long been a critical area of study in robotics, particularly
when robots are equipped with analytical models. As we move towards the
widespread use of deep neural networks in robots, which have demonstrated
remarkable performance in research settings, understanding the nuances of
uncertainty becomes crucial for their real-world deployment. This guide offers
an overview of the importance of uncertainty and provides methods to quantify
and evaluate it from an applications perspective.

摘要：

##### **Advancing Multimodal Medical Capabilities of Gemini**
2405.03162v1 by Lin Yang,Shawn Xu,Andrew Sellergren,Timo Kohlberger,Yuchen Zhou,Ira Ktena,Atilla Kiraly,Faruk Ahmed,Farhad Hormozdiari,Tiam Jaroensri,Eric Wang,Ellery Wulczyn,Fayaz Jamil,Theo Guidroz,Chuck Lau,Siyuan Qiao,Yun Liu,Akshay Goel,Kendall Park,Arnav Agharwal,Nick George,Yang Wang,Ryutaro Tanno,David G. T. Barrett,Wei-Hung Weng,S. Sara Mahdavi,Khaled Saab,Tao Tu,Sreenivasa Raju Kalidindi,Mozziyar Etemadi,Jorge Cuadros,Gregory Sorensen,Yossi Matias,Katherine Chou,Greg Corrado,Joelle Barral,Shravya Shetty,David Fleet,S. M. Ali Eslami,Daniel Tse,Shruthi Prabhakara,Cory McLean,Dave Steiner,Rory Pilgrim,Christopher Kelly,Shekoofeh Azizi,Daniel Golden

Many clinical tasks require an understanding of specialized data, such as
medical images and genomics, which is not typically found in general-purpose
large multimodal models. Building upon Gemini's multimodal models, we develop
several models within the new Med-Gemini family that inherit core capabilities
of Gemini and are optimized for medical use via fine-tuning with 2D and 3D
radiology, histopathology, ophthalmology, dermatology and genomic data.
Med-Gemini-2D sets a new standard for AI-based chest X-ray (CXR) report
generation based on expert evaluation, exceeding previous best results across
two separate datasets by an absolute margin of 1% and 12%, where 57% and 96% of
AI reports on normal cases, and 43% and 65% on abnormal cases, are evaluated as
"equivalent or better" than the original radiologists' reports. We demonstrate
the first ever large multimodal model-based report generation for 3D computed
tomography (CT) volumes using Med-Gemini-3D, with 53% of AI reports considered
clinically acceptable, although additional research is needed to meet expert
radiologist reporting quality. Beyond report generation, Med-Gemini-2D
surpasses the previous best performance in CXR visual question answering (VQA)
and performs well in CXR classification and radiology VQA, exceeding SoTA or
baselines on 17 of 20 tasks. In histopathology, ophthalmology, and dermatology
image classification, Med-Gemini-2D surpasses baselines across 18 out of 20
tasks and approaches task-specific model performance. Beyond imaging,
Med-Gemini-Polygenic outperforms the standard linear polygenic risk score-based
approach for disease risk prediction and generalizes to genetically correlated
diseases for which it has never been trained. Although further development and
evaluation are necessary in the safety-critical medical domain, our results
highlight the potential of Med-Gemini across a wide range of medical tasks.

摘要：

##### **Exploring the Potential of the Large Language Models (LLMs) in Identifying Misleading News Headlines**
2405.03153v1 by Md Main Uddin Rony,Md Mahfuzul Haque,Mohammad Ali,Ahmed Shatil Alam,Naeemul Hassan

In the digital age, the prevalence of misleading news headlines poses a
significant challenge to information integrity, necessitating robust detection
mechanisms. This study explores the efficacy of Large Language Models (LLMs) in
identifying misleading versus non-misleading news headlines. Utilizing a
dataset of 60 articles, sourced from both reputable and questionable outlets
across health, science & tech, and business domains, we employ three LLMs-
ChatGPT-3.5, ChatGPT-4, and Gemini-for classification. Our analysis reveals
significant variance in model performance, with ChatGPT-4 demonstrating
superior accuracy, especially in cases with unanimous annotator agreement on
misleading headlines. The study emphasizes the importance of human-centered
evaluation in developing LLMs that can navigate the complexities of
misinformation detection, aligning technical proficiency with nuanced human
judgment. Our findings contribute to the discourse on AI ethics, emphasizing
the need for models that are not only technically advanced but also ethically
aligned and sensitive to the subtleties of human interpretation.

摘要：

##### **Time Series Stock Price Forecasting Based on Genetic Algorithm (GA)-Long Short-Term Memory Network (LSTM) Optimization**
2405.03151v1 by Xinye Sha

In this paper, a time series algorithm based on Genetic Algorithm (GA) and
Long Short-Term Memory Network (LSTM) optimization is used to forecast stock
prices effectively, taking into account the trend of the big data era. The data
are first analyzed by descriptive statistics, and then the model is built and
trained and tested on the dataset. After optimization and adjustment, the mean
absolute error (MAE) of the model gradually decreases from 0.11 to 0.01 and
tends to be stable, indicating that the model prediction effect is gradually
close to the real value. The results on the test set show that the time series
algorithm optimized based on Genetic Algorithm (GA)-Long Short-Term Memory
Network (LSTM) is able to accurately predict the stock prices, and is highly
consistent with the actual price trends and values, with strong generalization
ability. The MAE on the test set is 2.41, the MSE is 9.84, the RMSE is 3.13,
and the R2 is 0.87. This research result not only provides a novel stock price
prediction method, but also provides a useful reference for financial market
analysis using computer technology and big data.

摘要：

##### **Quantifying the Capabilities of LLMs across Scale and Precision**
2405.03146v1 by Sher Badshah,Hassan Sajjad

Scale is often attributed as one of the factors that cause an increase in the
performance of LLMs, resulting in models with billion and trillion parameters.
One of the limitations of such large models is the high computational
requirements that limit their usage, deployment, and debugging in
resource-constrained scenarios. Two commonly used alternatives to bypass these
limitations are to use the smaller versions of LLMs (e.g. Llama 7B instead of
Llama 70B) and lower the memory requirements by using quantization. While these
approaches effectively address the limitation of resources, their impact on
model performance needs thorough examination. In this study, we perform a
comprehensive evaluation to investigate the effect of model scale and
quantization on the performance. We experiment with two major families of
open-source instruct models ranging from 7 billion to 70 billion parameters.
Our extensive zero-shot experiments across various tasks including natural
language understanding, reasoning, misinformation detection, and hallucination
reveal that larger models generally outperform their smaller counterparts,
suggesting that scale remains an important factor in enhancing performance. We
found that larger models show exceptional resilience to precision reduction and
can maintain high accuracy even at 4-bit quantization for numerous tasks and
they serve as a better solution than using smaller models at high precision
under similar memory requirements.

摘要：

##### **Automatic Ultrasound Curve Angle Measurement via Affinity Clustering for Adolescent Idiopathic Scoliosis Evaluation**
2405.03141v2 by Yihao Zhou,Timothy Tin-Yan Lee,Kelly Ka-Lee Lai,Chonglin Wu,Hin Ting Lau,De Yang,Chui-Yi Chan,Winnie Chiu-Wing Chu,Jack Chun-Yiu Cheng,Tsz-Ping Lam,Yong-Ping Zheng

The current clinical gold standard for evaluating adolescent idiopathic
scoliosis (AIS) is X-ray radiography, using Cobb angle measurement. However,
the frequent monitoring of the AIS progression using X-rays poses a challenge
due to the cumulative radiation exposure. Although 3D ultrasound has been
validated as a reliable and radiation-free alternative for scoliosis
assessment, the process of measuring spinal curvature is still carried out
manually. Consequently, there is a considerable demand for a fully automatic
system that can locate bony landmarks and perform angle measurements. To this
end, we introduce an estimation model for automatic ultrasound curve angle
(UCA) measurement. The model employs a dual-branch network to detect candidate
landmarks and perform vertebra segmentation on ultrasound coronal images. An
affinity clustering strategy is utilized within the vertebral segmentation area
to illustrate the affinity relationship between candidate landmarks.
Subsequently, we can efficiently perform line delineation from a clustered
affinity map for UCA measurement. As our method is specifically designed for
UCA calculation, this method outperforms other state-of-the-art methods for
landmark and line detection tasks. The high correlation between the automatic
UCA and Cobb angle (R$^2$=0.858) suggests that our proposed method can
potentially replace manual UCA measurement in ultrasound scoliosis assessment.

摘要：

##### **CRAFT: Extracting and Tuning Cultural Instructions from the Wild**
2405.03138v1 by Bin Wang,Geyu Lin,Zhengyuan Liu,Chengwei Wei,Nancy F. Chen

Large language models (LLMs) have rapidly evolved as the foundation of
various natural language processing (NLP) applications. Despite their wide use
cases, their understanding of culturally-related concepts and reasoning remains
limited. Meantime, there is a significant need to enhance these models'
cultural reasoning capabilities, especially concerning underrepresented
regions. This paper introduces a novel pipeline for extracting high-quality,
culturally-related instruction tuning datasets from vast unstructured corpora.
We utilize a self-instruction generation pipeline to identify cultural concepts
and trigger instruction. By integrating with a general-purpose instruction
tuning dataset, our model demonstrates enhanced capabilities in recognizing and
understanding regional cultural nuances, thereby enhancing its reasoning
capabilities. We conduct experiments across three regions: Singapore, the
Philippines, and the United States, achieving performance improvement of up to
6%. Our research opens new avenues for extracting cultural instruction tuning
sets directly from unstructured data, setting a precedent for future
innovations in the field.

摘要：

##### **Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training**
2405.03133v1 by Zexuan Zhong,Mengzhou Xia,Danqi Chen,Mike Lewis

Mixture-of-experts (MoE) models facilitate efficient scaling; however,
training the router network introduces the challenge of optimizing a
non-differentiable, discrete objective. Recently, a fully-differentiable MoE
architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges
experts in the parameter space; nevertheless, its effectiveness was only
demonstrated in downstream fine-tuning on classification tasks. In this paper,
we present Lory, the first approach that scales such architectures to
autoregressive language model pre-training. Lory introduces two key techniques:
(1) a causal segment routing strategy that achieves high efficiency for expert
merging operations while preserving the autoregressive nature of language
models; (2) a similarity-based data batching method that encourages expert
specialization by grouping similar documents in training instances. We
pre-train a series of Lory models on 150B tokens from scratch, with up to 32
experts and 30B (1.5B active) parameters. Experimental results show significant
performance gains over parameter-matched dense models on both perplexity
(+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level
routing, Lory models achieve competitive performance compared to
state-of-the-art MoE models with token-level routing. We further demonstrate
that the trained experts in Lory capture domain-level specialization without
supervision. Our work highlights the potential of fully-differentiable MoE
architectures for language model pre-training and advocates future research in
this area.

摘要：

##### **WDMoE: Wireless Distributed Large Language Models with Mixture of Experts**
2405.03131v1 by Nan Xue,Yaping Sun,Zhiyong Chen,Meixia Tao,Xiaodong Xu,Liang Qian,Shuguang Cui,Ping Zhang

Large Language Models (LLMs) have achieved significant success in various
natural language processing tasks, but how wireless communications can support
LLMs has not been extensively studied. In this paper, we propose a wireless
distributed LLMs paradigm based on Mixture of Experts (MoE), named WDMoE,
deploying LLMs collaboratively across edge servers of base station (BS) and
mobile devices in the wireless communications system. Specifically, we
decompose the MoE layer in LLMs by deploying the gating network and the
preceding neural network layer at BS, while distributing the expert networks
across the devices. This arrangement leverages the parallel capabilities of
expert networks on distributed devices. Moreover, to overcome the instability
of wireless communications, we design an expert selection policy by taking into
account both the performance of the model and the end-to-end latency, which
includes both transmission delay and inference delay. Evaluations conducted
across various LLMs and multiple datasets demonstrate that WDMoE not only
outperforms existing models, such as Llama 2 with 70 billion parameters, but
also significantly reduces end-to-end latency.

摘要：

##### **AniTalker: Animate Vivid and Diverse Talking Faces through Identity-Decoupled Facial Motion Encoding**
2405.03121v1 by Tao Liu,Feilong Chen,Shuai Fan,Chenpeng Du,Qi Chen,Xie Chen,Kai Yu

The paper introduces AniTalker, an innovative framework designed to generate
lifelike talking faces from a single portrait. Unlike existing models that
primarily focus on verbal cues such as lip synchronization and fail to capture
the complex dynamics of facial expressions and nonverbal cues, AniTalker
employs a universal motion representation. This innovative representation
effectively captures a wide range of facial dynamics, including subtle
expressions and head movements. AniTalker enhances motion depiction through two
self-supervised learning strategies: the first involves reconstructing target
video frames from source frames within the same identity to learn subtle motion
representations, and the second develops an identity encoder using metric
learning while actively minimizing mutual information between the identity and
motion encoders. This approach ensures that the motion representation is
dynamic and devoid of identity-specific details, significantly reducing the
need for labeled data. Additionally, the integration of a diffusion model with
a variance adapter allows for the generation of diverse and controllable facial
animations. This method not only demonstrates AniTalker's capability to create
detailed and realistic facial movements but also underscores its potential in
crafting dynamic avatars for real-world applications. Synthetic results can be
viewed at https://github.com/X-LANCE/AniTalker.

摘要：

##### **An Active Inference Agent for Simulating Human Translation Processes in a Hierarchical Architecture: Integrating the Task Segment Framework and the HOF taxonomy**
2405.03111v1 by Michael Carl

In this paper, we propose modelling human translation production as a
hierarchy of three embedded translation processes. The proposed architecture
replicates the temporal dynamics of keystroke production across sensorimotor,
cognitive, and phenomenal layers. Utilizing data from the CRITT TPR-DB, the
Task Segment Framework, and the HOF taxonomy, we demonstrate the temporal
breakdown of the typing flow on distinct timelines within these three layers.

摘要：

##### **FairMonitor: A Dual-framework for Detecting Stereotypes and Biases in Large Language Models**
2405.03098v1 by Yanhong Bai,Jiabao Zhao,Jinxin Shi,Zhentao Xie,Xingjiao Wu,Liang He

Detecting stereotypes and biases in Large Language Models (LLMs) is crucial
for enhancing fairness and reducing adverse impacts on individuals or groups
when these models are applied. Traditional methods, which rely on embedding
spaces or are based on probability metrics, fall short in revealing the nuanced
and implicit biases present in various contexts. To address this challenge, we
propose the FairMonitor framework and adopt a static-dynamic detection method
for a comprehensive evaluation of stereotypes and biases in LLMs. The static
component consists of a direct inquiry test, an implicit association test, and
an unknown situation test, including 10,262 open-ended questions with 9
sensitive factors and 26 educational scenarios. And it is effective for
evaluating both explicit and implicit biases. Moreover, we utilize the
multi-agent system to construst the dynamic scenarios for detecting subtle
biases in more complex and realistic setting. This component detects the biases
based on the interaction behaviors of LLMs across 600 varied educational
scenarios. The experimental results show that the cooperation of static and
dynamic methods can detect more stereotypes and biased in LLMs.

摘要：

##### **To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models**
2405.03097v1 by George-Octavian Barbulescu,Peter Triantafillou

LLMs have been found to memorize training textual sequences and regurgitate
verbatim said sequences during text generation time. This fact is known to be
the cause of privacy and related (e.g., copyright) problems. Unlearning in LLMs
then takes the form of devising new algorithms that will properly deal with
these side-effects of memorized data, while not hurting the model's utility. We
offer a fresh perspective towards this goal, namely, that each textual sequence
to be forgotten should be treated differently when being unlearned based on its
degree of memorization within the LLM. We contribute a new metric for measuring
unlearning quality, an adversarial attack showing that SOTA algorithms lacking
this perspective fail for privacy, and two new unlearning methods based on
Gradient Ascent and Task Arithmetic, respectively. A comprehensive performance
evaluation across an extensive suite of NLP tasks then mapped the solution
space, identifying the best solutions under different scales in model
capacities and forget set sizes and quantified the gains of the new approaches.

摘要：

##### **Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation**
2405.03085v1 by Kaize Shi,Xueyao Sun,Qing Li,Guandong Xu

Large Language Models (LLMs) have made significant strides in information
acquisition. However, their overreliance on potentially flawed parametric
knowledge leads to hallucinations and inaccuracies, particularly when handling
long-tail, domain-specific queries. Retrieval Augmented Generation (RAG)
addresses this limitation by incorporating external, non-parametric knowledge.
Nevertheless, the retrieved long-context documents often contain noisy,
irrelevant information alongside vital knowledge, negatively diluting LLMs'
attention. Inspired by the supportive role of essential concepts in
individuals' reading comprehension, we propose a novel concept-based RAG
framework with the Abstract Meaning Representation (AMR)-based concept
distillation algorithm. The proposed algorithm compresses the cluttered raw
retrieved documents into a compact set of crucial concepts distilled from the
informative nodes of AMR by referring to reliable linguistic features. The
concepts explicitly constrain LLMs to focus solely on vital information in the
inference process. We conduct extensive experiments on open-domain
question-answering datasets to empirically evaluate the proposed method's
effectiveness. The results indicate that the concept-based RAG framework
outperforms other baseline methods, particularly as the number of supporting
documents increases, while also exhibiting robustness across various backbone
LLMs. This emphasizes the distilled concepts are informative for augmenting the
RAG process by filtering out interference information. To the best of our
knowledge, this is the first work introducing AMR to enhance the RAG,
presenting a potential solution to augment inference performance with
semantic-based context compression.

摘要：

##### **On Probabilistic and Causal Reasoning with Summation Operators**
2405.03069v1 by Duligur Ibeling,Thomas F. Icard,Milan Mossé

Ibeling et al. (2023). axiomatize increasingly expressive languages of
causation and probability, and Mosse et al. (2024) show that reasoning
(specifically the satisfiability problem) in each causal language is as
difficult, from a computational complexity perspective, as reasoning in its
merely probabilistic or "correlational" counterpart. Introducing a summation
operator to capture common devices that appear in applications -- such as the
$do$-calculus of Pearl (2009) for causal inference, which makes ample use of
marginalization -- van der Zander et al. (2023) partially extend these earlier
complexity results to causal and probabilistic languages with marginalization.
We complete this extension, fully characterizing the complexity of
probabilistic and causal reasoning with summation, demonstrating that these
again remain equally difficult. Surprisingly, allowing free variables for
random variable values results in a system that is undecidable, so long as the
ranges of these random variables are unrestricted. We finally axiomatize these
languages featuring marginalization (or more generally summation), resolving
open questions posed by Ibeling et al. (2023).

摘要：

##### **AC-MAMBASEG: An adaptive convolution and Mamba-based architecture for enhanced skin lesion segmentation**
2405.03011v1 by Viet-Thanh Nguyen,Van-Truong Pham,Thi-Thao Tran

Skin lesion segmentation is a critical task in computer-aided diagnosis
systems for dermatological diseases. Accurate segmentation of skin lesions from
medical images is essential for early detection, diagnosis, and treatment
planning. In this paper, we propose a new model for skin lesion segmentation
namely AC-MambaSeg, an enhanced model that has the hybrid CNN-Mamba backbone,
and integrates advanced components such as Convolutional Block Attention Module
(CBAM), Attention Gate, and Selective Kernel Bottleneck. AC-MambaSeg leverages
the Vision Mamba framework for efficient feature extraction, while CBAM and
Selective Kernel Bottleneck enhance its ability to focus on informative regions
and suppress background noise. We evaluate the performance of AC-MambaSeg on
diverse datasets of skin lesion images including ISIC-2018 and PH2; then
compare it against existing segmentation methods. Our model shows promising
potential for improving computer-aided diagnosis systems and facilitating early
detection and treatment of dermatological diseases. Our source code will be
made available at: https://github.com/vietthanh2710/AC-MambaSeg.

摘要：

##### **High Order Reasoning for Time Critical Recommendation in Evidence-based Medicine**
2405.03010v1 by Manjiang Yu,Xue Li

In time-critical decisions, human decision-makers can interact with
AI-enabled situation-aware software to evaluate many imminent and possible
scenarios, retrieve billions of facts, and estimate different outcomes based on
trillions of parameters in a fraction of a second. In high-order reasoning,
"what-if" questions can be used to challenge the assumptions or pre-conditions
of the reasoning, "why-not" questions can be used to challenge on the method
applied in the reasoning, "so-what" questions can be used to challenge the
purpose of the decision, and "how-about" questions can be used to challenge the
applicability of the method. When above high-order reasoning questions are
applied to assist human decision-making, it can help humans to make
time-critical decisions and avoid false-negative or false-positive types of
errors. In this paper, we present a model of high-order reasoning to offer
recommendations in evidence-based medicine in a time-critical fashion for the
applications in ICU. The Large Language Model (LLM) is used in our system. The
experiments demonstrated the LLM exhibited optimal performance in the "What-if"
scenario, achieving a similarity of 88.52% with the treatment plans of human
doctors. In the "Why-not" scenario, the best-performing model tended to opt for
alternative treatment plans in 70% of cases for patients who died after being
discharged from the ICU. In the "So-what" scenario, the optimal model provided
a detailed analysis of the motivation and significance of treatment plans for
ICU patients, with its reasoning achieving a similarity of 55.6% with actual
diagnostic information. In the "How-about" scenario, the top-performing LLM
demonstrated a content similarity of 66.5% in designing treatment plans
transferring for similar diseases. Meanwhile, LLMs managed to predict the life
status of patients after their discharge from the ICU with an accuracy of 70%.

摘要：

##### **Explainable Malware Detection with Tailored Logic Explained Networks**
2405.03009v1 by Peter Anthony,Francesco Giannini,Michelangelo Diligenti,Martin Homola,Marco Gori,Stefan Balogh,Jan Mojzis

Malware detection is a constant challenge in cybersecurity due to the rapid
development of new attack techniques. Traditional signature-based approaches
struggle to keep pace with the sheer volume of malware samples. Machine
learning offers a promising solution, but faces issues of generalization to
unseen samples and a lack of explanation for the instances identified as
malware. However, human-understandable explanations are especially important in
security-critical fields, where understanding model decisions is crucial for
trust and legal compliance. While deep learning models excel at malware
detection, their black-box nature hinders explainability. Conversely,
interpretable models often fall short in performance. To bridge this gap in
this application domain, we propose the use of Logic Explained Networks (LENs),
which are a recently proposed class of interpretable neural networks providing
explanations in the form of First-Order Logic (FOL) rules. This paper extends
the application of LENs to the complex domain of malware detection,
specifically using the large-scale EMBER dataset. In the experimental results
we show that LENs achieve robustness that exceeds traditional interpretable
methods and that are rivaling black-box models. Moreover, we introduce a
tailored version of LENs that is shown to generate logic explanations with
higher fidelity with respect to the model's predictions.

摘要：

##### **On the performativity of SDG classifications in large bibliometric databases**
2405.03007v1 by Matteo Ottaviani,Stephan Stahlschmidt

Large bibliometric databases, such as Web of Science, Scopus, and OpenAlex,
facilitate bibliometric analyses, but are performative, affecting the
visibility of scientific outputs and the impact measurement of participating
entities. Recently, these databases have taken up the UN's Sustainable
Development Goals (SDGs) in their respective classifications, which have been
criticised for their diverging nature. This work proposes using the feature of
large language models (LLMs) to learn about the "data bias" injected by diverse
SDG classifications into bibliometric data by exploring five SDGs. We build a
LLM that is fine-tuned in parallel by the diverse SDG classifications inscribed
into the databases' SDG classifications. Our results show high sensitivity in
model architecture, classified publications, fine-tuning process, and natural
language generation. The wide arbitrariness at different levels raises concerns
about using LLM in research practice.

摘要：

##### **Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints**
2405.03005v1 by Siow Meng Low,Akshat Kumar

In safe Reinforcement Learning (RL), safety cost is typically defined as a
function dependent on the immediate state and actions. In practice, safety
constraints can often be non-Markovian due to the insufficient fidelity of
state representation, and safety cost may not be known. We therefore address a
general setting where safety labels (e.g., safe or unsafe) are associated with
state-action trajectories. Our key contributions are: first, we design a safety
model that specifically performs credit assignment to assess contributions of
partial state-action trajectories on safety. This safety model is trained using
a labeled safety dataset. Second, using RL-as-inference strategy we derive an
effective algorithm for optimizing a safe policy using the learned safety
model. Finally, we devise a method to dynamically adapt the tradeoff
coefficient between reward maximization and safety compliance. We rewrite the
constrained optimization problem into its dual problem and derive a
gradient-based method to dynamically adjust the tradeoff coefficient during
training. Our empirical results demonstrate that this approach is highly
scalable and able to satisfy sophisticated non-Markovian safety constraints.

摘要：

##### **Exploring prompts to elicit memorization in masked language model-based named entity recognition**
2405.03004v1 by Yuxi Xia,Anastasiia Sedova,Pedro Henrique Luz de Araujo,Vasiliki Kougia,Lisa Nußbaumer,Benjamin Roth

Training data memorization in language models impacts model capability
(generalization) and safety (privacy risk). This paper focuses on analyzing
prompts' impact on detecting the memorization of 6 masked language model-based
named entity recognition models. Specifically, we employ a diverse set of 400
automatically generated prompts, and a pairwise dataset where each pair
consists of one person's name from the training set and another name out of the
set. A prompt completed with a person's name serves as input for getting the
model's confidence in predicting this name. Finally, the prompt performance of
detecting model memorization is quantified by the percentage of name pairs for
which the model has higher confidence for the name from the training set. We
show that the performance of different prompts varies by as much as 16
percentage points on the same model, and prompt engineering further increases
the gap. Moreover, our experiments demonstrate that prompt performance is
model-dependent but does generalize across different name sets. A comprehensive
analysis indicates how prompt performance is influenced by prompt properties,
contained tokens, and the model's self-attention weights on the prompt.

摘要：

##### **Parameter-Efficient Fine-Tuning with Discrete Fourier Transform**
2405.03003v1 by Ziqi Gao,Qichao Wang,Aochuan Chen,Zijing Liu,Bingzhe Wu,Liang Chen,Jia Li

Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning
foundation models. It effectively reduces the number of trainable parameters by
incorporating low-rank matrices $A$ and $B$ to represent the weight change,
i.e., $\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when
handling extensive customization adaptations or larger base models. In this
work, we aim to further compress trainable parameters by enjoying the powerful
expressiveness of the Fourier transform. Specifically, we introduce FourierFT,
which treats $\Delta W$ as a matrix in the spatial domain and learns only a
small fraction of its spectral coefficients. With the trained spectral
coefficients, we implement the inverse discrete Fourier transform to recover
$\Delta W$. Empirically, our FourierFT method shows comparable or better
performance with fewer parameters than LoRA on various tasks, including natural
language understanding, natural language generation, instruction tuning, and
image classification. For example, when performing instruction tuning on the
LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable
parameters, compared to LoRA's 33.5M. Our code is released at
\url{https://github.com/Chaos96/fourierft}.

摘要：

##### **MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning**
2405.03000v1 by Wenqi Shi,Ran Xu,Yuchen Zhuang,Yue Yu,Hang Wu,Carl Yang,May D. Wang

Despite their improved capabilities in generation and reasoning, adapting
large language models (LLMs) to the biomedical domain remains challenging due
to their immense size and corporate privacy. In this work, we propose
MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards
biomedical applications. Instead of fine-tuning the entire LLM, MedAdapter
effectively adapts the original model by fine-tuning only a small BERT-sized
adapter to rank candidate solutions generated by LLMs. Experiments demonstrate
that MedAdapter effectively adapts both white-box and black-box LLMs in
biomedical reasoning, achieving average performance improvements of 25.48% and
11.31%, respectively, without requiring extensive computational resources or
sharing data with third parties. MedAdapter also yields superior performance
when combined with train-time adaptation, highlighting a flexible and
complementary solution to existing adaptation methods. Faced with the
challenges of balancing model performance, computational resources, and data
privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective,
and transparent solution for adapting LLMs to the biomedical domain.

摘要：

##### **RepAugment: Input-Agnostic Representation-Level Augmentation for Respiratory Sound Classification**
2405.02996v1 by June-Woo Kim,Miika Toikkanen,Sangmin Bae,Minseok Kim,Ho-Young Jung

Recent advancements in AI have democratized its deployment as a healthcare
assistant. While pretrained models from large-scale visual and audio datasets
have demonstrably generalized to this task, surprisingly, no studies have
explored pretrained speech models, which, as human-originated sounds,
intuitively would share closer resemblance to lung sounds. This paper explores
the efficacy of pretrained speech models for respiratory sound classification.
We find that there is a characterization gap between speech and lung sound
samples, and to bridge this gap, data augmentation is essential. However, the
most widely used augmentation technique for audio and speech, SpecAugment,
requires 2-dimensional spectrogram format and cannot be applied to models
pretrained on speech waveforms. To address this, we propose RepAugment, an
input-agnostic representation-level augmentation technique that outperforms
SpecAugment, but is also suitable for respiratory sound classification with
waveform pretrained models. Experimental results show that our approach
outperforms the SpecAugment, demonstrating a substantial improvement in the
accuracy of minority disease classes, reaching up to 7.14%.

摘要：

##### **Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability to Mark Short Answer Questions in K-12 Education**
2405.02985v1 by Owen Henkel,Adam Boxer,Libby Hills,Bill Roberts

This paper presents reports on a series of experiments with a novel dataset
evaluating how well Large Language Models (LLMs) can mark (i.e. grade) open
text responses to short answer questions, Specifically, we explore how well
different combinations of GPT version and prompt engineering strategies
performed at marking real student answers to short answer across different
domain areas (Science and History) and grade-levels (spanning ages 5-16) using
a new, never-used-before dataset from Carousel, a quizzing platform. We found
that GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and,
importantly, very close to human-level performance (0.75). This research builds
on prior findings that GPT-4 could reliably score short answer reading
comprehension questions at a performance-level very close to that of expert
human raters. The proximity to human-level performance, across a variety of
subjects and grade levels suggests that LLMs could be a valuable tool for
supporting low-stakes formative assessment tasks in K-12 education and has
important implications for real-world education delivery.

摘要：

##### **E-TSL: A Continuous Educational Turkish Sign Language Dataset with Baseline Methods**
2405.02984v1 by Şükrü Öztürk,Hacer Yalim Keles

This study introduces the continuous Educational Turkish Sign Language
(E-TSL) dataset, collected from online Turkish language lessons for 5th, 6th,
and 8th grades. The dataset comprises 1,410 videos totaling nearly 24 hours and
includes performances from 11 signers. Turkish, an agglutinative language,
poses unique challenges for sign language translation, particularly with a
vocabulary where 64% are singleton words and 85% are rare words, appearing less
than five times. We developed two baseline models to address these challenges:
the Pose to Text Transformer (P2T-T) and the Graph Neural Network based
Transformer (GNN-T) models. The GNN-T model achieved 19.13% BLEU-1 score and
3.28% BLEU-4 score, presenting a significant challenge compared to existing
benchmarks. The P2T-T model, while demonstrating slightly lower performance in
BLEU scores, achieved a higher ROUGE-L score of 22.09%. Additionally, we
benchmarked our model using the well-known PHOENIX-Weather 2014T dataset to
validate our approach.

摘要：

##### **Multi-Agent RL-Based Industrial AIGC Service Offloading over Wireless Edge Networks**
2405.02972v1 by Siyuan Li,Xi Lin,Hansong Xu,Kun Hua,Xiaomin Jin,Gaolei Li,Jianhua Li

Currently, the generative model has garnered considerable attention due to
its application in addressing the challenge of scarcity of abnormal samples in
the industrial Internet of Things (IoT). However, challenges persist regarding
the edge deployment of generative models and the optimization of joint edge
AI-generated content (AIGC) tasks. In this paper, we focus on the edge
optimization of AIGC task execution and propose GMEL, a generative model-driven
industrial AIGC collaborative edge learning framework. This framework aims to
facilitate efficient few-shot learning by leveraging realistic sample synthesis
and edge-based optimization capabilities. First, a multi-task AIGC
computational offloading model is presented to ensure the efficient execution
of heterogeneous AIGC tasks on edge servers. Then, we propose an
attention-enhanced multi-agent reinforcement learning (AMARL) algorithm aimed
at refining offloading policies within the IoT system, thereby supporting
generative model-driven edge learning. Finally, our experimental results
demonstrate the effectiveness of the proposed algorithm in optimizing the total
system latency of the edge-based AIGC task completion.

摘要：

##### **Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents**
2405.02957v1 by Junkai Li,Siyu Wang,Meng Zhang,Weitao Li,Yunghwei Lai,Xinhui Kang,Weizhi Ma,Yang Liu

In this paper, we introduce a simulacrum of hospital called Agent Hospital
that simulates the entire process of treating illness. All patients, nurses,
and doctors are autonomous agents powered by large language models (LLMs). Our
central goal is to enable a doctor agent to learn how to treat illness within
the simulacrum. To do so, we propose a method called MedAgent-Zero. As the
simulacrum can simulate disease onset and progression based on knowledge bases
and LLMs, doctor agents can keep accumulating experience from both successful
and unsuccessful cases. Simulation experiments show that the treatment
performance of doctor agents consistently improves on various tasks. More
interestingly, the knowledge the doctor agents have acquired in Agent Hospital
is applicable to real-world medicare benchmarks. After treating around ten
thousand patients (real-world doctors may take over two years), the evolved
doctor agent achieves a state-of-the-art accuracy of 93.06% on a subset of the
MedQA dataset that covers major respiratory diseases. This work paves the way
for advancing the applications of LLM-powered agent techniques in medical
scenarios.

摘要：

##### **Unraveling the Dominance of Large Language Models Over Transformer Models for Bangla Natural Language Inference: A Comprehensive Study**
2405.02937v2 by Fatema Tuj Johora Faria,Mukaffi Bin Moin,Asif Iftekher Fahim,Pronay Debnath,Faisal Muhammad Shah

Natural Language Inference (NLI) is a cornerstone of Natural Language
Processing (NLP), providing insights into the entailment relationships between
text pairings. It is a critical component of Natural Language Understanding
(NLU), demonstrating the ability to extract information from spoken or written
interactions. NLI is mainly concerned with determining the entailment
relationship between two statements, known as the premise and hypothesis. When
the premise logically implies the hypothesis, the pair is labeled "entailment".
If the hypothesis contradicts the premise, the pair receives the
"contradiction" label. When there is insufficient evidence to establish a
connection, the pair is described as "neutral". Despite the success of Large
Language Models (LLMs) in various tasks, their effectiveness in NLI remains
constrained by issues like low-resource domain accuracy, model overconfidence,
and difficulty in capturing human judgment disagreements. This study addresses
the underexplored area of evaluating LLMs in low-resourced languages such as
Bengali. Through a comprehensive evaluation, we assess the performance of
prominent LLMs and state-of-the-art (SOTA) models in Bengali NLP tasks,
focusing on natural language inference. Utilizing the XNLI dataset, we conduct
zero-shot and few-shot evaluations, comparing LLMs like GPT-3.5 Turbo and
Gemini 1.5 Pro with models such as BanglaBERT, Bangla BERT Base, DistilBERT,
mBERT, and sahajBERT. Our findings reveal that while LLMs can achieve
comparable or superior performance to fine-tuned SOTA models in few-shot
scenarios, further research is necessary to enhance our understanding of LLMs
in languages with modest resources like Bengali. This study underscores the
importance of continued efforts in exploring LLM capabilities across diverse
linguistic contexts.

摘要：

##### **On the tractability of SHAP explanations under Markovian distributions**
2405.02936v1 by Reda Marzouk,Colin de La Higuera

Thanks to its solid theoretical foundation, the SHAP framework is arguably
one the most widely utilized frameworks for local explainability of ML models.
Despite its popularity, its exact computation is known to be very challenging,
proven to be NP-Hard in various configurations. Recent works have unveiled
positive complexity results regarding the computation of the SHAP score for
specific model families, encompassing decision trees, random forests, and some
classes of boolean circuits. Yet, all these positive results hinge on the
assumption of feature independence, often simplistic in real-world scenarios.
In this article, we investigate the computational complexity of the SHAP score
by relaxing this assumption and introducing a Markovian perspective. We show
that, under the Markovian assumption, computing the SHAP score for the class of
Weighted automata, Disjoint DNFs and Decision Trees can be performed in
polynomial time, offering a first positive complexity result for the problem of
SHAP score computation that transcends the limitations of the feature
independence assumption.

摘要：

##### **Relay Decoding: Concatenating Large Language Models for Machine Translation**
2405.02933v1 by Chengpeng Fu,Xiaocheng Feng,Yichong Huang,Wenshuai Huo,Baohang Li,Hui Wang,Bin Qin,Ting Liu

Leveraging large language models for machine translation has demonstrated
promising results. However, it does require the large language models to
possess the capability of handling both the source and target languages in
machine translation. When it is challenging to find large models that support
the desired languages, resorting to continuous learning methods becomes a
costly endeavor. To mitigate these expenses, we propose an innovative approach
called RD (Relay Decoding), which entails concatenating two distinct large
models that individually support the source and target languages. By
incorporating a simple mapping layer to facilitate the connection between these
two models and utilizing a limited amount of parallel data for training, we
successfully achieve superior results in the machine translation task.
Experimental results conducted on the Multi30k and WikiMatrix datasets validate
the effectiveness of our proposed method.

摘要：

##### **Unified Dynamic Scanpath Predictors Outperform Individually Trained Neural Models**
2405.02929v2 by Fares Abawi,Di Fu,Stefan Wermter

Previous research on scanpath prediction has mainly focused on group models,
disregarding the fact that the scanpaths and attentional behaviors of
individuals are diverse. The disregard of these differences is especially
detrimental to social human-robot interaction, whereby robots commonly emulate
human gaze based on heuristics or predefined patterns. However, human gaze
patterns are heterogeneous and varying behaviors can significantly affect the
outcomes of such human-robot interactions. To fill this gap, we developed a
deep learning-based social cue integration model for saliency prediction to
instead predict scanpaths in videos. Our model learned scanpaths by recursively
integrating fixation history and social cues through a gating mechanism and
sequential attention. We evaluated our approach on gaze datasets of dynamic
social scenes, observed under the free-viewing condition. The introduction of
fixation history into our models makes it possible to train a single unified
model rather than the resource-intensive approach of training individual models
for each set of scanpaths. We observed that the late neural integration
approach surpasses early fusion when training models on a large dataset, in
comparison to a smaller dataset with a similar distribution. Results also
indicate that a single unified model, trained on all the observers' scanpaths,
performs on par or better than individually trained models. We hypothesize that
this outcome is a result of the group saliency representations instilling
universal attention in the model, while the supervisory signal and fixation
history guide it to learn personalized attentional behaviors, providing the
unified model a benefit over individual models due to its implicit
representation of universal attention.

摘要：

##### **A Two-Stage Prediction-Aware Contrastive Learning Framework for Multi-Intent NLU**
2405.02925v1 by Guanhua Chen,Yutong Yao,Derek F. Wong,Lidia S. Chao

Multi-intent natural language understanding (NLU) presents a formidable
challenge due to the model confusion arising from multiple intents within a
single utterance. While previous works train the model contrastively to
increase the margin between different multi-intent labels, they are less suited
to the nuances of multi-intent NLU. They ignore the rich information between
the shared intents, which is beneficial to constructing a better embedding
space, especially in low-data scenarios. We introduce a two-stage
Prediction-Aware Contrastive Learning (PACL) framework for multi-intent NLU to
harness this valuable knowledge. Our approach capitalizes on shared intent
information by integrating word-level pre-training and prediction-aware
contrastive fine-tuning. We construct a pre-training dataset using a word-level
data augmentation strategy. Subsequently, our framework dynamically assigns
roles to instances during contrastive fine-tuning while introducing a
prediction-aware contrastive loss to maximize the impact of contrastive
learning. We present experimental results and empirical analysis conducted on
three widely used datasets, demonstrating that our method surpasses the
performance of three prominent baselines on both low-data and full-data
scenarios.

摘要：

##### **Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models**
2405.02917v1 by Tobias Groot,Matias Valdenegro-Toro

Language and Vision-Language Models (LLMs/VLMs) have revolutionized the field
of AI by their ability to generate human-like text and understand images, but
ensuring their reliability is crucial. This paper aims to evaluate the ability
of LLMs (GPT4, GPT-3.5, LLaMA2, and PaLM 2) and VLMs (GPT4V and Gemini Pro
Vision) to estimate their verbalized uncertainty via prompting. We propose the
new Japanese Uncertain Scenes (JUS) dataset, aimed at testing VLM capabilities
via difficult queries and object counting, and the Net Calibration Error (NCE)
to measure direction of miscalibration. Results show that both LLMs and VLMs
have a high calibration error and are overconfident most of the time,
indicating a poor capability for uncertainty estimation. Additionally we
develop prompts for regression tasks, and we show that VLMs have poor
calibration when producing mean/standard deviation and 95% confidence
intervals.

摘要：

##### **Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English**
2405.02887v1 by Aekansh Kathunia,Mohammad Kaif,Nalin Arora,N Narotam

People communicate in more than 7,000 languages around the world, with around
780 languages spoken in India alone. Despite this linguistic diversity,
research on Sentiment Analysis has predominantly focused on English text data,
resulting in a disproportionate availability of sentiment resources for
English. This paper examines the performance of transformer models in Sentiment
Analysis tasks across multilingual datasets and text that has undergone machine
translation. By comparing the effectiveness of these models in different
linguistic contexts, we gain insights into their performance variations and
potential implications for sentiment analysis across diverse languages. We also
discuss the shortcomings and potential for future work towards the end.

摘要：

##### **Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models**
2405.02861v1 by Yang Liu,Melissa Xiaohui Qin,Hongming Li,Chao Huang

We introduce LexBench, a comprehensive evaluation suite enabled to test
language models (LMs) on ten semantic phrase processing tasks. Unlike prior
studies, it is the first work to propose a framework from the comparative
perspective to model the general semantic phrase (i.e., lexical collocation)
and three fine-grained semantic phrases, including idiomatic expression, noun
compound, and verbal construction. Thanks to \ourbenchmark, we assess the
performance of 15 LMs across model architectures and parameter scales in
classification, extraction, and interpretation tasks. Through the experiments,
we first validate the scaling law and find that, as expected, large models
excel better than the smaller ones in most tasks. Second, we investigate
further through the scaling semantic relation categorization and find that
few-shot LMs still lag behind vanilla fine-tuned models in the task. Third,
through human evaluation, we find that the performance of strong models is
comparable to the human level regarding semantic phrase processing. Our
benchmarking findings can serve future research aiming to improve the generic
capability of LMs on semantic phrase comprehension. Our source code and data
are available at https://github.com/jacklanda/LexBench

摘要：

##### **Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation**
2405.02858v1 by Jinyu Cai,Jialong Li,Mingyue Zhang,Munan Li,Chen-Shu Wang,Kenji Tei

Social media platforms such as Twitter, Reddit, and Sina Weibo play a crucial
role in global communication but often encounter strict regulations in
geopolitically sensitive regions. This situation has prompted users to
ingeniously modify their way of communicating, frequently resorting to coded
language in these regulated social media environments. This shift in
communication is not merely a strategy to counteract regulation, but a vivid
manifestation of language evolution, demonstrating how language naturally
evolves under societal and technological pressures. Studying the evolution of
language in regulated social media contexts is of significant importance for
ensuring freedom of speech, optimizing content moderation, and advancing
linguistic research. This paper proposes a multi-agent simulation framework
using Large Language Models (LLMs) to explore the evolution of user language in
regulated social media environments. The framework employs LLM-driven agents:
supervisory agent who enforce dialogue supervision and participant agents who
evolve their language strategies while engaging in conversation, simulating the
evolution of communication styles under strict regulations aimed at evading
social media regulation. The study evaluates the framework's effectiveness
through a range of scenarios from abstract scenarios to real-world situations.
Key findings indicate that LLMs are capable of simulating nuanced language
dynamics and interactions in constrained settings, showing improvement in both
evading supervision and information accuracy as evolution progresses.
Furthermore, it was found that LLM agents adopt different strategies for
different scenarios.

摘要：

##### **Modelling Opaque Bilateral Market Dynamics in Financial Trading: Insights from a Multi-Agent Simulation Study**
2405.02849v1 by Alicia Vidler,Toby Walsh

Exploring complex adaptive financial trading environments through multi-agent
based simulation methods presents an innovative approach within the realm of
quantitative finance. Despite the dominance of multi-agent reinforcement
learning approaches in financial markets with observable data, there exists a
set of systematically significant financial markets that pose challenges due to
their partial or obscured data availability. We, therefore, devise a
multi-agent simulation approach employing small-scale meta-heuristic methods.
This approach aims to represent the opaque bilateral market for Australian
government bond trading, capturing the bilateral nature of bank-to-bank
trading, also referred to as "over-the-counter" (OTC) trading, and commonly
occurring between "market makers". The uniqueness of the bilateral market,
characterized by negotiated transactions and a limited number of agents, yields
valuable insights for agent-based modelling and quantitative finance. The
inherent rigidity of this market structure, which is at odds with the global
proliferation of multilateral platforms and the decentralization of finance,
underscores the unique insights offered by our agent-based model. We explore
the implications of market rigidity on market structure and consider the
element of stability, in market design. This extends the ongoing discourse on
complex financial trading environments, providing an enhanced understanding of
their dynamics and implications.

摘要：

##### **Responsible AI: Portraits with Intelligent Bibliometrics**
2405.02846v1 by Yi Zhang,Mengjia Wu,Guangquan Zhang,Jie Lu

Shifting the focus from principles to practical implementation, responsible
artificial intelligence (AI) has garnered considerable attention across
academia, industry, and society at large. Despite being in its nascent stages,
this emerging field grapples with nebulous concepts and intricate knowledge
frameworks. By analyzing three prevailing concepts - explainable AI,
trustworthy AI, and ethical AI, this study defined responsible AI and
identified its core principles. Methodologically, this study successfully
demonstrated the implementation of leveraging AI's capabilities into
bibliometrics for enhanced knowledge discovery and the cross-validation of
experimentally examined models with domain insights. Empirically, this study
investigated 17,799 research articles contributed by the AI community since
2015. This involves recognizing key technological players and their
relationships, unveiling the topical landscape and hierarchy of responsible AI,
charting its evolution, and elucidating the interplay between the
responsibility principles and primary AI techniques. An analysis of a core
cohort comprising 380 articles from multiple disciplines captures the most
recent advancements in responsible AI. As one of the pioneering bibliometric
studies dedicated to exploring responsible AI, this study will provide
comprehensive macro-level insights, enhancing the understanding of responsible
AI while furnishing valuable knowledge support for AI regulation and governance
initiatives.

摘要：

##### **Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction**
2405.02821v1 by Changan Chen,Jordi Ramos,Anshul Tomar,Kristen Grauman

Sim2real transfer has received increasing attention lately due to the success
of learning robotic tasks in simulation end-to-end. While there has been a lot
of progress in transferring vision-based navigation policies, the existing
sim2real strategy for audio-visual navigation performs data augmentation
empirically without measuring the acoustic gap. The sound differs from light in
that it spans across much wider frequencies and thus requires a different
solution for sim2real. We propose the first treatment of sim2real for
audio-visual navigation by disentangling it into acoustic field prediction
(AFP) and waypoint navigation. We first validate our design choice in the
SoundSpaces simulator and show improvement on the Continuous AudioGoal
navigation benchmark. We then collect real-world data to measure the spectral
difference between the simulation and the real world by training AFP models
that only take a specific frequency subband as input. We further propose a
frequency-adaptive strategy that intelligently selects the best frequency band
for prediction based on both the measured spectral difference and the energy
distribution of the received audio, which improves the performance on the real
data. Lastly, we build a real robot platform and show that the transferred
policy can successfully navigate to sounding objects. This work demonstrates
the potential of building intelligent agents that can see, hear, and act
entirely from simulation, and transferring them to the real world.

摘要：

##### **HuixiangDou-CR: Coreference Resolution in Group Chats**
2405.02817v1 by Huanjun Kong

How to eliminate pronominal reference in group chats? In this work, we have
preprocessed 58k authentic chat data and manually annotated 2.3k questions. The
reliability of this annotation was confirmed by the scaling law. After this, we
conducted fine-tuning on Qwen models, ranging from 0.5B to 32B parameters. The
optimal version improved 29.07 in F1 score. This confirms the viability of
fine-tuning Large Language Model (LLM) for downstream Natural Language
Processing (NLP) tasks. Our contributions are: 1) Created Supervised
Fine-Tuning (SFT) training data in alpaca format, along with a set of Low-Rank
Adaptation (LoRA) weights, and 2) Developed a method for acquiring high-quality
data leveraging scaling law principle. The script, raw data with alpaca format
and experiments track are open-sourced on Github
https://github.com/InternLM/HuixiangDou/tree/main/web/tools, HuggingFace
https://huggingface.co/tpoisonooo and WandB
https://wandb.ai/tpoisonooo/huixiangdou-cr/table?nw=nwusertpoisonooo . The
privacy of the data involved has been authorized by users.

摘要：

##### **Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization**
2405.02816v1 by Hamed Zamani,Michael Bendersky

This paper introduces Stochastic RAG--a novel approach for end-to-end
optimization of retrieval-augmented generation (RAG) models that relaxes the
simplifying assumptions of marginalization and document independence, made in
most prior work. Stochastic RAG casts the retrieval process in RAG as a
stochastic sampling without replacement process. Through this formulation, we
employ straight-through Gumbel-top-k that provides a differentiable
approximation for sampling without replacement and enables effective end-to-end
optimization for RAG. We conduct extensive experiments on seven diverse
datasets on a wide range of tasks, from open-domain question answering to fact
verification to slot-filling for relation extraction and to dialogue systems.
By applying this optimization method to a recent and effective RAG model, we
advance state-of-the-art results on six out of seven datasets.

摘要：

##### **Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**
2405.02815v1 by Zhusi Zhong,Jie Li,Zhuoqi Ma,Scott Collins,Harrison Bai,Paul Zhang,Terrance Healey,Xinbo Gao,Michael K. Atalay,Zhicheng Jiao

The COVID-19 pandemic has strained global public health, necessitating
accurate diagnosis and intervention to control disease spread and reduce
mortality rates. This paper introduces an interpretable deep survival
prediction model designed specifically for improved understanding and trust in
COVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale
pretrained image encoder, Risk-specific Grad-CAM, and anatomical region
detection techniques, our approach produces regional interpretable outcomes
that effectively capture essential disease features while focusing on rare but
critical abnormal regions. Our model's predictive results provide enhanced
clarity and transparency through risk area localization, enabling clinicians to
make informed decisions regarding COVID-19 diagnosis with better understanding
of prognostic insights. We evaluate the proposed method on a multi-center
survival dataset and demonstrate its effectiveness via quantitative and
qualitative assessments, achieving superior C-indexes (0.764 and 0.727) and
time-dependent AUCs (0.799 and 0.691). These results suggest that our
explainable deep survival prediction model surpasses traditional survival
analysis methods in risk prediction, improving interpretability for clinical
decision making and enhancing AI system trustworthiness.

摘要：

##### **NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli**
2405.02814v1 by Xu Wang,Cheng Li,Yi Chang,Jindong Wang,Yuan Wu

Large Language Models (LLMs) have become integral to a wide spectrum of
applications, ranging from traditional computing tasks to advanced artificial
intelligence (AI) applications. This widespread adoption has spurred extensive
research into LLMs across various disciplines, including the social sciences.
Notably, studies have revealed that LLMs possess emotional intelligence, which
can be further developed through positive emotional stimuli. This discovery
raises an intriguing question: can negative emotions similarly influence LLMs,
potentially enhancing their performance? In response to this question, we
introduce NegativePrompt, a novel approach underpinned by psychological
principles, involving ten specifically designed negative emotional stimuli. We
embark on rigorous experimental evaluations of five LLMs including
Flan-T5-Large, Vicuna, Llama 2, ChatGPT, and GPT-4, across a set of 45 tasks.
The results are revealing: NegativePrompt markedly enhances the performance of
LLMs, evidenced by relative improvements of 12.89% in Instruction Induction
tasks and 46.25% in BIG-Bench tasks. Moreover, we conduct attention
visualization experiments to decipher the underlying mechanisms of
NegativePrompt's influence. Our research contributes significantly to the
understanding of LLMs and emotion interaction, demonstrating the practical
efficacy of NegativePrompt as an emotion-driven method and offering novel
insights for the enhancement of LLMs in real-world applications. The code is
available at https://github.com/wangxu0820/NegativePrompt.

摘要：

##### **Kinematic analysis of structural mechanics based on convolutional neural network**
2405.02807v1 by Leye Zhang,Xiangxiang Tian,Hongjun Zhang

Attempt to use convolutional neural network to achieve kinematic analysis of
plane bar structure. Through 3dsMax animation software and OpenCV module,
self-build image dataset of geometrically stable system and geometrically
unstable system. we construct and train convolutional neural network model
based on the TensorFlow and Keras deep learning platform framework. The model
achieves 100% accuracy on the training set, validation set, and test set. The
accuracy on the additional test set is 93.7%, indicating that convolutional
neural network can learn and master the relevant knowledge of kinematic
analysis of structural mechanics. In the future, the generalization ability of
the model can be improved through the diversity of dataset, which has the
potential to surpass human experts for complex structures. Convolutional neural
network has certain practical value in the field of kinematic analysis of
structural mechanics. Using visualization technology, we reveal how
convolutional neural network learns and recognizes structural features. Using
pre-trained VGG16 model for feature extraction and fine-tuning, we found that
the generalization ability is inferior to the self-built model.

摘要：

##### **Mozart's Touch: A Lightweight Multi-modal Music Generation Framework Based on Pre-Trained Large Models**
2405.02801v2 by Tianze Xu,Jiajun Li,Xuesong Chen,Xinrui Yao,Shuchang Liu

In recent years, AI-Generated Content (AIGC) has witnessed rapid
advancements, facilitating the generation of music, images, and other forms of
artistic expression across various industries. However, researches on general
multi-modal music generation model remain scarce. To fill this gap, we propose
a multi-modal music generation framework Mozart's Touch. It could generate
aligned music with the cross-modality inputs, such as images, videos and text.
Mozart's Touch is composed of three main components: Multi-modal Captioning
Module, Large Language Model (LLM) Understanding & Bridging Module, and Music
Generation Module. Unlike traditional approaches, Mozart's Touch requires no
training or fine-tuning pre-trained models, offering efficiency and
transparency through clear, interpretable prompts. We also introduce
"LLM-Bridge" method to resolve the heterogeneous representation problems
between descriptive texts of different modalities. We conduct a series of
objective and subjective evaluations on the proposed model, and results
indicate that our model surpasses the performance of current state-of-the-art
models. Our codes and examples is availble at:
https://github.com/WangTooNaive/MozartsTouch

摘要：

##### **ImageInWords: Unlocking Hyper-Detailed Image Descriptions**
2405.02793v1 by Roopal Garg,Andrea Burns,Burcu Karagol Ayan,Yonatan Bitton,Ceslee Montgomery,Yasumasa Onoe,Andrew Bunner,Ranjay Krishna,Jason Baldridge,Radu Soricut

Despite the longstanding adage "an image is worth a thousand words," creating
accurate and hyper-detailed image descriptions for training Vision-Language
models remains challenging. Current datasets typically have web-scraped
descriptions that are short, low-granularity, and often contain details
unrelated to the visual content. As a result, models trained on such data
generate descriptions replete with missing information, visual inconsistencies,
and hallucinations. To address these issues, we introduce ImageInWords (IIW), a
carefully designed human-in-the-loop annotation framework for curating
hyper-detailed image descriptions and a new dataset resulting from this
process. We validate the framework through evaluations focused on the quality
of the dataset and its utility for fine-tuning with considerations for
readability, comprehensiveness, specificity, hallucinations, and
human-likeness. Our dataset significantly improves across these dimensions
compared to recently released datasets (+66%) and GPT-4V outputs (+48%).
Furthermore, models fine-tuned with IIW data excel by +31% against prior work
along the same human evaluation dimensions. Given our fine-tuned models, we
also evaluate text-to-image generation and vision-language reasoning. Our
model's descriptions can generate images closest to the original, as judged by
both automated and human metrics. We also find our model produces more
compositionally rich descriptions, outperforming the best baseline by up to 6%
on ARO, SVO-Probes, and Winoground datasets.

摘要：

##### **Efficient Text-driven Motion Generation via Latent Consistency Training**
2405.02791v1 by Mengxian Hu,Minghao Zhu,Xun Zhou,Qingqing Yan,Shu Li,Chengju Liu,Qijun Chen

Motion diffusion models have recently proven successful for text-driven human
motion generation. Despite their excellent generation performance, they are
challenging to infer in real time due to the multi-step sampling mechanism that
involves tens or hundreds of repeat function evaluation iterations. To this
end, we investigate a motion latent consistency Training (MLCT) for motion
generation to alleviate the computation and time consumption during iteration
inference. It applies diffusion pipelines to low-dimensional motion latent
spaces to mitigate the computational burden of each function evaluation.
Explaining the diffusion process with probabilistic flow ordinary differential
equation (PF-ODE) theory, the MLCT allows extremely few steps infer between the
prior distribution to the motion latent representation distribution via
maintaining consistency of the outputs over the trajectory of PF-ODE.
Especially, we introduce a quantization constraint to optimize motion latent
representations that are bounded, regular, and well-reconstructed compared to
traditional variational constraints. Furthermore, we propose a conditional
PF-ODE trajectory simulation method, which improves the conditional generation
performance with minimal additional training costs. Extensive experiments on
two human motion generation benchmarks show that the proposed model achieves
state-of-the-art performance with less than 10\% time cost.

摘要：

##### **Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs**
2405.02774v1 by Feiyang Kang,Hoang Anh Just,Yifan Sun,Himanshu Jahagirdar,Yuanzhi Zhang,Rongxing Du,Anit Kumar Sahu,Ruoxi Jia

This work focuses on leveraging and selecting from vast, unlabeled, open data
to pre-fine-tune a pre-trained language model. The goal is to minimize the need
for costly domain-specific data for subsequent fine-tuning while achieving
desired performance levels. While many data selection algorithms have been
designed for small-scale applications, rendering them unsuitable for our
context, some emerging methods do cater to language data scales. However, they
often prioritize data that aligns with the target distribution. While this
strategy may be effective when training a model from scratch, it can yield
limited results when the model has already been pre-trained on a different
distribution. Differing from prior work, our key idea is to select data that
nudges the pre-training distribution closer to the target distribution. We show
the optimality of this approach for fine-tuning tasks under certain conditions.
We demonstrate the efficacy of our methodology across a diverse array of tasks
(NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently
surpasses other selection methods. Moreover, our proposed method is
significantly faster than existing techniques, scaling to millions of samples
within a single GPU hour. Our code is open-sourced (Code repository:
https://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers
significant potential for enhancing performance across diverse tasks, its
associated costs often limit its widespread adoption; with this work, we hope
to lay the groundwork for cost-effective fine-tuning, making its benefits more
accessible.

摘要：

##### **Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities for Lifelong Learning**
2405.02766v1 by Fahad Sarfraz,Bahram Zonooz,Elahe Arani

While humans excel at continual learning (CL), deep neural networks (DNNs)
exhibit catastrophic forgetting. A salient feature of the brain that allows
effective CL is that it utilizes multiple modalities for learning and
inference, which is underexplored in DNNs. Therefore, we study the role and
interactions of multiple modalities in mitigating forgetting and introduce a
benchmark for multimodal continual learning. Our findings demonstrate that
leveraging multiple views and complementary information from multiple
modalities enables the model to learn more accurate and robust representations.
This makes the model less vulnerable to modality-specific regularities and
considerably mitigates forgetting. Furthermore, we observe that individual
modalities exhibit varying degrees of robustness to distribution shift.
Finally, we propose a method for integrating and aligning the information from
different modalities by utilizing the relational structural similarities
between the data points in each modality. Our method sets a strong baseline
that enables both single- and multimodal inference. Our study provides a
promising case for further exploring the role of multiple modalities in
enabling CL and provides a standard benchmark for future research.

摘要：

##### **Detecting Edited Knowledge in Language Models**
2405.02765v1 by Paul Youssef,Zhixue Zhao,Jörg Schlötterer,Christin Seifert

Knowledge editing techniques (KEs) can update language models' obsolete or
inaccurate knowledge learned from pre-training. However, KE also faces
potential malicious applications, e.g. inserting misinformation and toxic
content. Moreover, in the context of responsible AI, it is instructive for
end-users to know whether a generated output is driven by edited knowledge or
first-hand knowledge from pre-training. To this end, we study detecting edited
knowledge in language models by introducing a novel task: given an edited model
and a specific piece of knowledge the model generates, our objective is to
classify the knowledge as either "non-edited" (based on the pre-training), or
``edited'' (based on subsequent editing). We initiate the task with two
state-of-the-art KEs, two language models, and two datasets. We further propose
a simple classifier, RepReg, a logistic regression model that takes hidden
state representations as input features. Our results reveal that RepReg
establishes a strong baseline, achieving a peak accuracy of 99.81%, and 97.79%
in out-of-domain settings. Second, RepReg achieves near-optimal performance
with a limited training set (200 training samples), and it maintains its
performance even in out-of-domain settings. Last, we find it more challenging
to separate edited and non-edited knowledge when they contain the same subject
or object.

摘要：

##### **Assessing Adversarial Robustness of Large Language Models: An Empirical Study**
2405.02764v1 by Zeyu Yang,Zhao Meng,Xiaochen Zheng,Roger Wattenhofer

Large Language Models (LLMs) have revolutionized natural language processing,
but their robustness against adversarial attacks remains a critical concern. We
presents a novel white-box style attack approach that exposes vulnerabilities
in leading open-source LLMs, including Llama, OPT, and T5. We assess the impact
of model size, structure, and fine-tuning strategies on their resistance to
adversarial perturbations. Our comprehensive evaluation across five diverse
text classification tasks establishes a new benchmark for LLM robustness. The
findings of this study have far-reaching implications for the reliable
deployment of LLMs in real-world applications and contribute to the advancement
of trustworthy AI systems.

摘要：

##### **Implicit Safe Set Algorithm for Provably Safe Reinforcement Learning**
2405.02754v1 by Weiye Zhao,Tairan He,Feihan Li,Changliu Liu

Deep reinforcement learning (DRL) has demonstrated remarkable performance in
many continuous control tasks. However, a significant obstacle to the
real-world application of DRL is the lack of safety guarantees. Although DRL
agents can satisfy system safety in expectation through reward shaping,
designing agents to consistently meet hard constraints (e.g., safety
specifications) at every time step remains a formidable challenge. In contrast,
existing work in the field of safe control provides guarantees on persistent
satisfaction of hard safety constraints. However, these methods require
explicit analytical system dynamics models to synthesize safe control, which
are typically inaccessible in DRL settings. In this paper, we present a
model-free safe control algorithm, the implicit safe set algorithm, for
synthesizing safeguards for DRL agents that ensure provable safety throughout
training. The proposed algorithm synthesizes a safety index (barrier
certificate) and a subsequent safe control law solely by querying a black-box
dynamic function (e.g., a digital twin simulator). Moreover, we theoretically
prove that the implicit safe set algorithm guarantees finite time convergence
to the safe set and forward invariance for both continuous-time and
discrete-time systems. We validate the proposed algorithm on the
state-of-the-art Safety Gym benchmark, where it achieves zero safety violations
while gaining $95\% \pm 9\%$ cumulative reward compared to state-of-the-art
safe DRL methods. Furthermore, the resulting algorithm scales well to
high-dimensional systems with parallel computing.

摘要：

##### **Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding**
2405.02750v1 by Zheng Zhao,Emilio Monti,Jens Lehmann,Haytham Assem

Large language models (LLMs) tend to inadequately integrate input context
during text generation, relying excessively on encoded prior knowledge in model
parameters, potentially resulting in generated text with factual
inconsistencies or contextually unfaithful content. LLMs utilize two primary
knowledge sources: 1) prior (parametric) knowledge from pretraining, and 2)
contextual (non-parametric) knowledge from input prompts. The study addresses
the open question of how LLMs effectively balance these knowledge sources
during the generation process, specifically in the context of open-domain
question answering. To address this issue, we introduce a novel approach
integrating contrastive decoding with adversarial irrelevant passages as
negative samples to enhance robust context grounding during generation.
Notably, our method operates at inference time without requiring further
training. We conduct comprehensive experiments to demonstrate its applicability
and effectiveness, providing empirical evidence showcasing its superiority over
existing methodologies. Our code is publicly available at:
https://github.com/amazon-science/ContextualUnderstanding-ContrastiveDecoding.

摘要：

