{"2406.18450": {"publish_time": "2024-06-26", "title": "Preference Elicitation for Offline Reinforcement Learning", "paper_summary": "Applying reinforcement learning (RL) to real-world problems is often made\nchallenging by the inability to interact with the environment and the\ndifficulty of designing reward functions. Offline RL addresses the first\nchallenge by considering access to an offline dataset of environment\ninteractions labeled by the reward function. In contrast, Preference-based RL\ndoes not assume access to the reward function and learns it from preferences,\nbut typically requires an online interaction with the environment. We bridge\nthe gap between these frameworks by exploring efficient methods for acquiring\npreference feedback in a fully offline setup. We propose Sim-OPRL, an offline\npreference-based reinforcement learning algorithm, which leverages a learned\nenvironment model to elicit preference feedback on simulated rollouts. Drawing\non insights from both the offline RL and the preference-based RL literature,\nour algorithm employs a pessimistic approach for out-of-distribution data, and\nan optimistic approach for acquiring informative preferences about the optimal\npolicy. We provide theoretical guarantees regarding the sample complexity of\nour approach, dependent on how well the offline data covers the optimal policy.\nFinally, we demonstrate the empirical performance of Sim-OPRL in different\nenvironments.", "paper_summary_zh": "\u5c07\u5f37\u5316\u5b78\u7fd2 (RL) \u904b\u7528\u65bc\u5be6\u969b\u554f\u984c\u6642\uff0c\u7d93\u5e38\u6703\u9047\u5230\u7121\u6cd5\u8207\u74b0\u5883\u4e92\u52d5\uff0c\u4ee5\u53ca\u8a2d\u8a08\u734e\u52f5\u51fd\u6578\u7684\u56f0\u96e3\uff0c\u800c\u9019\u6703\u9020\u6210\u6311\u6230\u3002\u96e2\u7dda RL \u900f\u904e\u8003\u616e\u5b58\u53d6\u6a19\u8a18\u6709\u734e\u52f5\u51fd\u6578\u7684\u74b0\u5883\u4e92\u52d5\u96e2\u7dda\u8cc7\u6599\u96c6\u4f86\u89e3\u6c7a\u7b2c\u4e00\u500b\u6311\u6230\u3002\u76f8\u5c0d\u5730\uff0c\u57fa\u65bc\u504f\u597d\u7684 RL \u6c92\u6709\u5047\u8a2d\u53ef\u4ee5\u5b58\u53d6\u734e\u52f5\u51fd\u6578\uff0c\u800c\u662f\u5f9e\u504f\u597d\u4e2d\u5b78\u7fd2\u734e\u52f5\u51fd\u6578\uff0c\u4f46\u901a\u5e38\u9700\u8981\u8207\u74b0\u5883\u7dda\u4e0a\u4e92\u52d5\u3002\u6211\u5011\u900f\u904e\u63a2\u7d22\u5728\u5b8c\u5168\u96e2\u7dda\u8a2d\u5b9a\u4e2d\u53d6\u5f97\u504f\u597d\u56de\u994b\u7684\u6709\u6548\u65b9\u6cd5\u4f86\u5f4c\u88dc\u9019\u4e9b\u6846\u67b6\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u6211\u5011\u63d0\u51fa Sim-OPRL\uff0c\u4e00\u7a2e\u96e2\u7dda\u57fa\u65bc\u504f\u597d\u7684\u5f37\u5316\u5b78\u7fd2\u6f14\u7b97\u6cd5\uff0c\u5b83\u5229\u7528\u5df2\u5b78\u7fd2\u7684\u74b0\u5883\u6a21\u578b\u5728\u6a21\u64ec\u7684\u6efe\u52d5\u4e2d\u5f15\u767c\u504f\u597d\u56de\u994b\u3002\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u6839\u64da\u96e2\u7dda RL \u548c\u57fa\u65bc\u504f\u597d\u7684 RL \u6587\u737b\u4e2d\u7684\u898b\u89e3\uff0c\u63a1\u7528\u60b2\u89c0\u7684\u65b9\u5f0f\u8655\u7406\u5206\u4f48\u5916\u8cc7\u6599\uff0c\u4e26\u63a1\u7528\u6a02\u89c0\u7684\u65b9\u5f0f\u53d6\u5f97\u95dc\u65bc\u6700\u4f73\u653f\u7b56\u7684\u8cc7\u8a0a\u6027\u504f\u597d\u3002\u6211\u5011\u63d0\u4f9b\u95dc\u65bc\u6211\u5011\u65b9\u6cd5\u7684\u7bc4\u4f8b\u8907\u96dc\u6027\u7684\u7406\u8ad6\u4fdd\u8b49\uff0c\u9019\u53d6\u6c7a\u65bc\u96e2\u7dda\u8cc7\u6599\u6db5\u84cb\u6700\u4f73\u653f\u7b56\u7684\u7a0b\u5ea6\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728\u4e0d\u540c\u7684\u74b0\u5883\u4e2d\u5c55\u793a Sim-OPRL \u7684\u7d93\u9a57\u6548\u80fd\u3002", "author": "Aliz\u00e9e Pace et.al.", "authors": "Aliz\u00e9e Pace, Bernhard Sch\u00f6lkopf, Gunnar R\u00e4tsch, Giorgia Ramponi", "id": "2406.18450v1", "paper_url": "http://arxiv.org/abs/2406.18450v1", "repo": "null"}}