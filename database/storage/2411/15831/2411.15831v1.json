{"2411.15831": {"publish_time": "2024-11-24", "title": "Efficient and Private: Memorisation under differentially private parameter-efficient fine-tuning in language models", "paper_summary": "Fine-tuning large language models (LLMs) for specific tasks introduces\nprivacy risks, as models may inadvertently memorise and leak sensitive training\ndata. While Differential Privacy (DP) offers a solution to mitigate these\nrisks, it introduces significant computational and performance trade-offs,\nparticularly with standard fine-tuning approaches. Previous work has primarily\nfocused on full-parameter updates, which are computationally intensive and may\nnot fully leverage DPs potential in large models. In this work, we address\nthese shortcomings by investigating Parameter-Efficient Fine-Tuning (PEFT)\nmethods under DP constraints. We show that PEFT methods achieve comparable\nperformance to standard fine-tuning while requiring fewer parameters and\nsignificantly reducing privacy leakage. Furthermore, we incorporate a data\npoisoning experiment involving intentional mislabelling to assess model\nmemorisation and directly measure privacy risks. Our findings indicate that\nPEFT methods not only provide a promising alternative but also serve as a\ncomplementary approach for privacy-preserving, resource-efficient fine-tuning\nof LLMs.", "paper_summary_zh": "\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u61c9\u5c0d\u7279\u5b9a\u4efb\u52d9\u6642\u6703\u5f15\u767c\u96b1\u79c1\u98a8\u96aa\uff0c\u56e0\u70ba\u6a21\u578b\u53ef\u80fd\u6703\u7121\u610f\u9593\u8a18\u61b6\u4e26\u6d29\u9732\u654f\u611f\u7684\u8a13\u7df4\u8cc7\u6599\u3002\u96d6\u7136\u5dee\u5206\u96b1\u79c1 (DP) \u63d0\u4f9b\u4e86\u89e3\u6c7a\u9019\u4e9b\u98a8\u96aa\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u6703\u9020\u6210\u986f\u8457\u7684\u904b\u7b97\u548c\u6548\u80fd\u53d6\u6368\uff0c\u7279\u5225\u662f\u63a1\u7528\u6a19\u6e96\u5fae\u8abf\u65b9\u6cd5\u6642\u3002\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u65bc\u5168\u53c3\u6578\u66f4\u65b0\uff0c\u9019\u5728\u904b\u7b97\u4e0a\u5f88\u5bc6\u96c6\uff0c\u800c\u4e14\u53ef\u80fd\u7121\u6cd5\u5145\u5206\u767c\u63ee\u5927\u578b\u6a21\u578b\u4e2d DP \u7684\u6f5b\u529b\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u900f\u904e\u7814\u7a76 DP \u7d04\u675f\u4e0b\u7684\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\u4f86\u89e3\u6c7a\u9019\u4e9b\u7f3a\u9ede\u3002\u6211\u5011\u986f\u793a PEFT \u65b9\u6cd5\u53ef\u9054\u6210\u8207\u6a19\u6e96\u5fae\u8abf\u76f8\u7576\u7684\u6548\u80fd\uff0c\u540c\u6642\u6240\u9700\u53c3\u6578\u8f03\u5c11\uff0c\u4e26\u5927\u5e45\u964d\u4f4e\u96b1\u79c1\u5916\u6d29\u3002\u6b64\u5916\uff0c\u6211\u5011\u7d0d\u5165\u4e00\u9805\u8cc7\u6599\u4e2d\u6bd2\u5be6\u9a57\uff0c\u5176\u4e2d\u6d89\u53ca\u6545\u610f\u932f\u8aa4\u6a19\u7c64\uff0c\u4ee5\u8a55\u4f30\u6a21\u578b\u8a18\u61b6\u4e26\u76f4\u63a5\u8861\u91cf\u96b1\u79c1\u98a8\u96aa\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0cPEFT \u65b9\u6cd5\u4e0d\u50c5\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e5f\u4f5c\u70ba\u4e00\u7a2e\u88dc\u5145\u65b9\u6cd5\uff0c\u7528\u65bc LLM \u7684\u96b1\u79c1\u4fdd\u8b77\u3001\u8cc7\u6e90\u6709\u6548\u5fae\u8abf\u3002", "author": "Olivia Ma et.al.", "authors": "Olivia Ma, Jonathan Passerat-Palmbach, Dmitrii Usynin", "id": "2411.15831v1", "paper_url": "http://arxiv.org/abs/2411.15831v1", "repo": "null"}}