{"2409.11233": {"publish_time": "2024-09-17", "title": "Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models", "paper_summary": "Large language models (LLMs) offer powerful capabilities but incur\nsubstantial computational costs, driving the need for efficient compression\ntechniques. This study evaluates the impact of popular compression methods -\nMagnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on\nthe trade-offs between model size reduction, downstream task performance, and\nthe role of calibration data. Our findings reveal that while SparseGPT and\nWanda preserve perplexity even at 50% sparsity, they suffer significant\ndegradation on downstream tasks, highlighting the inadequacy of perplexity as\nthe sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)\nDivergence as a more comprehensive metric that captures nuanced changes in\nmodel behavior post-compression. We further demonstrate that task-specific\ncalibration data significantly enhances the downstream performance of\ncompressed models compared to general calibration data. This research\nunderscores the necessity for diverse evaluation metrics and careful\ncalibration data selection to fully understand the complexities of LLM\ncompression and its implications for practical applications.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u63d0\u4f9b\u5f37\u5927\u7684\u529f\u80fd\uff0c\u4f46\u6703\u7522\u751f\u5927\u91cf\u7684\u8a08\u7b97\u6210\u672c\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u58d3\u7e2e\u6280\u8853\u3002\u672c\u7814\u7a76\u8a55\u4f30\u4e86\u6d41\u884c\u7684\u58d3\u7e2e\u65b9\u6cd5\u5c0d LLaMA-2-7B \u6a21\u578b\u7684\u5f71\u97ff\uff0c\u5305\u62ec\u5e45\u5ea6\u526a\u679d\u3001SparseGPT \u548c Wanda\uff0c\u91cd\u9ede\u5728\u65bc\u6a21\u578b\u5927\u5c0f\u7e2e\u6e1b\u3001\u4e0b\u6e38\u4efb\u52d9\u6548\u80fd\u548c\u6821\u6b63\u8cc7\u6599\u7684\u89d2\u8272\u4e4b\u9593\u7684\u6b0a\u8861\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u5118\u7ba1 SparseGPT \u548c Wanda \u5373\u4f7f\u5728 50% \u7684\u7a00\u758f\u5ea6\u4e0b\u4e5f\u80fd\u7dad\u6301\u56f0\u60d1\u5ea6\uff0c\u4f46\u5b83\u5011\u5728\u4e0b\u6e38\u4efb\u52d9\u4e2d\u6703\u986f\u8457\u60e1\u5316\uff0c\u9019\u7a81\u986f\u4e86\u56f0\u60d1\u5ea6\u4f5c\u70ba\u552f\u4e00\u8a55\u4f30\u6307\u6a19\u7684\u4e0d\u8db3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 Jensen-Shannon (JS) \u8ddd\u96e2\u4f5c\u70ba\u4e00\u500b\u66f4\u5168\u9762\u7684\u6307\u6a19\uff0c\u5b83\u80fd\u6355\u6349\u6a21\u578b\u884c\u70ba\u5728\u58d3\u7e2e\u5f8c\u767c\u751f\u7684\u7d30\u5fae\u8b8a\u5316\u3002\u6211\u5011\u9032\u4e00\u6b65\u8b49\u660e\uff0c\u8207\u4e00\u822c\u6821\u6b63\u8cc7\u6599\u76f8\u6bd4\uff0c\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u6821\u6b63\u8cc7\u6599\u53ef\u4ee5\u986f\u8457\u63d0\u5347\u58d3\u7e2e\u6a21\u578b\u7684\u4e0b\u6e38\u6548\u80fd\u3002\u9019\u9805\u7814\u7a76\u5f37\u8abf\u4e86\u4f7f\u7528\u591a\u5143\u8a55\u4f30\u6307\u6a19\u548c\u4ed4\u7d30\u9078\u64c7\u6821\u6b63\u8cc7\u6599\u7684\u5fc5\u8981\u6027\uff0c\u624d\u80fd\u5145\u5206\u4e86\u89e3 LLM \u58d3\u7e2e\u7684\u8907\u96dc\u6027\u53ca\u5176\u5c0d\u5be6\u969b\u61c9\u7528\u7a0b\u5f0f\u9020\u6210\u7684\u5f71\u97ff\u3002", "author": "Bishwash Khanal et.al.", "authors": "Bishwash Khanal, Jeffery M. Capone", "id": "2409.11233v1", "paper_url": "http://arxiv.org/abs/2409.11233v1", "repo": "null"}}