{"2501.04694": {"publish_time": "2025-01-08", "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation", "paper_summary": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method.", "paper_summary_zh": "\u6709\u6548\u7684\u6307\u4ee4\u8abf\u6574\u5c0d\u65bc\u6700\u4f73\u5316\u7a0b\u5f0f\u78bc LLM \u81f3\u95dc\u91cd\u8981\uff0c\u53ef\u5c07\u6a21\u578b\u884c\u70ba\u8207\u4f7f\u7528\u8005\u9810\u671f\u4fdd\u6301\u4e00\u81f4\uff0c\u4e26\u63d0\u5347\u6a21\u578b\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u65b9\u6cd5\u5927\u591a\u8457\u91cd\u65bc\u7a0b\u5f0f\u78bc\u7247\u6bb5\uff0c\u50c5\u9650\u65bc\u7279\u5b9a\u529f\u80fd\u548c\u50f5\u5316\u7684\u7d50\u69cb\uff0c\u9650\u5236\u4e86\u5408\u6210\u8cc7\u6599\u7684\u8907\u96dc\u6027\u548c\u591a\u6a23\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u65b0\u7a4e\u7684\u57fa\u65bc\u7279\u5fb5\u6a39\u7684\u5408\u6210\u67b6\u69cb\uff0c\u9748\u611f\u4f86\u81ea\u62bd\u8c61\u8a9e\u6cd5\u6a39 (AST)\u3002\u8207\u64f7\u53d6\u7a0b\u5f0f\u78bc\u8a9e\u6cd5\u7d50\u69cb\u7684 AST \u4e0d\u540c\uff0c\u6211\u5011\u7684\u67b6\u69cb\u6703\u5c0d\u7a0b\u5f0f\u78bc\u5143\u7d20\u4e4b\u9593\u7684\u8a9e\u610f\u95dc\u4fc2\u5efa\u6a21\uff0c\u80fd\u5920\u7522\u751f\u66f4\u7d30\u7dfb\u4e14\u591a\u6a23\u5316\u7684\u8cc7\u6599\u3002\u7279\u5fb5\u6a39\u662f\u7531\u539f\u59cb\u8cc7\u6599\u5efa\u69cb\u800c\u6210\uff0c\u4e26\u53cd\u8986\u7cbe\u7149\u4ee5\u589e\u52a0\u63d0\u53d6\u7279\u5fb5\u7684\u6578\u91cf\u548c\u591a\u6a23\u6027\u3002\u6b64\u7a0b\u5e8f\u80fd\u8b58\u5225\u7a0b\u5f0f\u78bc\u4e2d\u66f4\u8907\u96dc\u7684\u6a21\u5f0f\u548c\u95dc\u4fc2\u3002\u900f\u904e\u4ee5\u53d7\u63a7\u6df1\u5ea6\u548c\u5ee3\u5ea6\u53d6\u6a23\u5b50\u6a39\uff0c\u6211\u5011\u7684\u67b6\u69cb\u5141\u8a31\u7cbe\u78ba\u8abf\u6574\u7522\u751f\u7a0b\u5f0f\u78bc\u7684\u8907\u96dc\u5ea6\uff0c\u652f\u63f4\u5f9e\u7c21\u55ae\u51fd\u5f0f\u5c64\u7d1a\u64cd\u4f5c\u5230\u8907\u96dc\u591a\u6a94\u6848\u5834\u666f\u7684\u5404\u7a2e\u4efb\u52d9\u3002\u6211\u5011\u5fae\u8abf\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u790e\u6a21\u578b\u4ee5\u5efa\u7acb EpiCoder \u7cfb\u5217\uff0c\u5728\u51fd\u5f0f\u548c\u6a94\u6848\u5c64\u7d1a\u4e0a\u65bc\u591a\u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u9054\u6210\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5be6\u8b49\u8b49\u64da\u986f\u793a\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u5408\u6210\u9ad8\u5ea6\u8907\u96dc\u7684\u5132\u5b58\u5eab\u5c64\u7d1a\u7a0b\u5f0f\u78bc\u8cc7\u6599\u65b9\u9762\u5177\u6709\u986f\u8457\u7684\u6f5b\u529b\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u900f\u904e\u8edf\u9ad4\u5de5\u7a0b\u539f\u5247\u548c LLM \u4f5c\u70ba\u8a55\u5224\u65b9\u6cd5\uff0c\u56b4\u8b39\u5730\u8a55\u4f30\u8cc7\u6599\u7684\u8907\u96dc\u6027\u548c\u591a\u6a23\u6027\uff0c\u95e1\u660e\u6b64\u65b9\u6cd5\u7684\u512a\u9ede\u3002", "author": "Yaoxiang Wang et.al.", "authors": "Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, Jinsong Su, Qi Chen, Scarlett Li", "id": "2501.04694v1", "paper_url": "http://arxiv.org/abs/2501.04694v1", "repo": "null"}}