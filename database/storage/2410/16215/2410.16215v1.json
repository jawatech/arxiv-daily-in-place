{"2410.16215": {"publish_time": "2024-10-21", "title": "Pre-training Distillation for Large Language Models: A Design Space Exploration", "paper_summary": "Knowledge distillation (KD) aims to transfer knowledge from a large teacher\nmodel to a smaller student model. Previous work applying KD in the field of\nlarge language models (LLMs) typically focused on the post-training phase,\nwhere the student LLM learns directly from instructions and corresponding\nresponses generated by the teacher model. In this paper, we extend KD to the\npre-training phase of LLMs, named pre-training distillation (PD). We first\nconduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a\n1.9B parameter student LLM, validating the effectiveness of PD. Considering the\nkey impact factors of distillation, we systematically explore the design space\nof pre-training distillation across four aspects: logits processing, loss\nselection, scaling law, and offline or online logits. We conduct extensive\nexperiments to explore the design space of pre-training distillation and find\nbetter configurations and interesting conclusions, such as larger student LLMs\ngenerally benefiting more from pre-training distillation, while a larger\nteacher LLM does not necessarily guarantee better results. We hope our\nexploration of the design space will inform future practices in pre-training\ndistillation.", "paper_summary_zh": "\u77e5\u8b58\u84b8\u993e (KD) \u7684\u76ee\u6a19\u662f\u5c07\u77e5\u8b58\u5f9e\u5927\u578b\u6559\u5e2b\u6a21\u578b\u50b3\u8f38\u5230\u5c0f\u578b\u5b78\u751f\u6a21\u578b\u3002\u5148\u524d\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9818\u57df\u4e2d\u61c9\u7528 KD \u7684\u5de5\u4f5c\u901a\u5e38\u5c08\u6ce8\u65bc\u5f8c\u8a13\u7df4\u968e\u6bb5\uff0c\u5176\u4e2d\u5b78\u751f LLM \u76f4\u63a5\u5f9e\u6559\u5e2b\u6a21\u578b\u7522\u751f\u7684\u8aaa\u660e\u548c\u5c0d\u61c9\u56de\u61c9\u4e2d\u5b78\u7fd2\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07 KD \u64f4\u5c55\u5230 LLM \u7684\u9810\u8a13\u7df4\u968e\u6bb5\uff0c\u7a31\u70ba\u9810\u8a13\u7df4\u84b8\u993e (PD)\u3002\u6211\u5011\u9996\u5148\u4f7f\u7528 GLM-4-9B \u4f5c\u70ba\u6559\u5e2b LLM \u9032\u884c\u521d\u6b65\u5be6\u9a57\uff0c\u4ee5\u84b8\u993e 1.9B \u53c3\u6578\u5b78\u751f LLM\uff0c\u9a57\u8b49 PD \u7684\u6709\u6548\u6027\u3002\u8003\u616e\u5230\u84b8\u993e\u7684\u4e3b\u8981\u5f71\u97ff\u56e0\u7d20\uff0c\u6211\u5011\u7cfb\u7d71\u5730\u63a2\u7d22\u4e86\u9810\u8a13\u7df4\u84b8\u993e\u7684\u8a2d\u8a08\u7a7a\u9593\uff0c\u6db5\u84cb\u56db\u500b\u65b9\u9762\uff1a\u5c0d\u6578\u8655\u7406\u3001\u640d\u5931\u9078\u64c7\u3001\u6bd4\u4f8b\u5b9a\u5f8b\u548c\u96e2\u7dda\u6216\u7dda\u4e0a\u5c0d\u6578\u3002\u6211\u5011\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4ee5\u63a2\u7d22\u9810\u8a13\u7df4\u84b8\u993e\u7684\u8a2d\u8a08\u7a7a\u9593\uff0c\u4e26\u627e\u51fa\u66f4\u597d\u7684\u914d\u7f6e\u548c\u6709\u8da3\u7684\u7d50\u8ad6\uff0c\u4f8b\u5982\uff0c\u8f03\u5927\u7684\u5b78\u751f LLM \u901a\u5e38\u5f9e\u9810\u8a13\u7df4\u84b8\u993e\u4e2d\u7372\u76ca\u66f4\u591a\uff0c\u800c\u8f03\u5927\u7684\u6559\u5e2b LLM \u4e26\u4e0d\u4e00\u5b9a\u80fd\u4fdd\u8b49\u66f4\u597d\u7684\u7d50\u679c\u3002\u6211\u5011\u5e0c\u671b\u6211\u5011\u5c0d\u8a2d\u8a08\u7a7a\u9593\u7684\u63a2\u7d22\u5c07\u70ba\u9810\u8a13\u7df4\u84b8\u993e\u4e2d\u7684\u672a\u4f86\u5be6\u52d9\u63d0\u4f9b\u8cc7\u8a0a\u3002", "author": "Hao Peng et.al.", "authors": "Hao Peng, Xin Lv, Yushi Bai, Zijun Yao, Jiajie Zhang, Lei Hou, Juanzi Li", "id": "2410.16215v1", "paper_url": "http://arxiv.org/abs/2410.16215v1", "repo": "null"}}