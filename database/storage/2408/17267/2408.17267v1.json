{"2408.17267": {"publish_time": "2024-08-30", "title": "UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-View Urban Scenarios", "paper_summary": "Recent evaluations of Large Multimodal Models (LMMs) have explored their\ncapabilities in various domains, with only few benchmarks specifically focusing\non urban environments. Moreover, existing urban benchmarks have been limited to\nevaluating LMMs with basic region-level urban tasks under singular views,\nleading to incomplete evaluations of LMMs' abilities in urban environments. To\naddress these issues, we present UrBench, a comprehensive benchmark designed\nfor evaluating LMMs in complex multi-view urban scenarios. UrBench contains\n11.6K meticulously curated questions at both region-level and role-level that\ncover 4 task dimensions: Geo-Localization, Scene Reasoning, Scene\nUnderstanding, and Object Understanding, totaling 14 task types. In\nconstructing UrBench, we utilize data from existing datasets and additionally\ncollect data from 11 cities, creating new annotations using a cross-view\ndetection-matching method. With these images and annotations, we then integrate\nLMM-based, rule-based, and human-based methods to construct large-scale\nhigh-quality questions. Our evaluations on 21 LMMs show that current LMMs\nstruggle in the urban environments in several aspects. Even the best performing\nGPT-4o lags behind humans in most tasks, ranging from simple tasks such as\ncounting to complex tasks such as orientation, localization and object\nattribute recognition, with an average performance gap of 17.4%. Our benchmark\nalso reveals that LMMs exhibit inconsistent behaviors with different urban\nviews, especially with respect to understanding cross-view relations. UrBench\ndatasets and benchmark results will be publicly available at\nhttps://opendatalab.github.io/UrBench/.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u5c0d\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u7684\u8a55\u4f30\u5df2\u63a2\u8a0e\u5176\u5728\u5404\u7a2e\u9818\u57df\u7684\u80fd\u529b\uff0c\u53ea\u6709\u5c11\u6578\u57fa\u6e96\u7279\u5225\u5c08\u6ce8\u65bc\u57ce\u5e02\u74b0\u5883\u3002\u6b64\u5916\uff0c\u73fe\u6709\u7684\u57ce\u5e02\u57fa\u6e96\u50c5\u9650\u65bc\u5728\u55ae\u4e00\u8996\u5716\u4e0b\u4f7f\u7528\u57fa\u672c\u7684\u5340\u57df\u7d1a\u57ce\u5e02\u4efb\u52d9\u4f86\u8a55\u4f30 LMM\uff0c\u5c0e\u81f4\u5c0d LMM \u5728\u57ce\u5e02\u74b0\u5883\u4e2d\u7684\u80fd\u529b\u9032\u884c\u4e0d\u5b8c\u5168\u7684\u8a55\u4f30\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 UrBench\uff0c\u9019\u662f\u4e00\u500b\u5168\u9762\u7684\u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30\u8907\u96dc\u7684\u591a\u8996\u5716\u57ce\u5e02\u5834\u666f\u4e2d\u7684 LMM\u3002UrBench \u5305\u542b 11.6K \u500b\u7cbe\u5fc3\u7b56\u5283\u7684\u554f\u984c\uff0c\u65e2\u6709\u5340\u57df\u7d1a\u5225\uff0c\u4e5f\u6709\u89d2\u8272\u7d1a\u5225\uff0c\u6db5\u84cb 4 \u500b\u4efb\u52d9\u7dad\u5ea6\uff1a\u5730\u7406\u5b9a\u4f4d\u3001\u5834\u666f\u63a8\u7406\u3001\u5834\u666f\u7406\u89e3\u548c\u5c0d\u8c61\u7406\u89e3\uff0c\u7e3d\u5171 14 \u500b\u4efb\u52d9\u985e\u578b\u3002\u5728\u69cb\u5efa UrBench \u6642\uff0c\u6211\u5011\u5229\u7528\u73fe\u6709\u6578\u64da\u96c6\u4e2d\u7684\u6578\u64da\uff0c\u4e26\u984d\u5916\u5f9e 11 \u500b\u57ce\u5e02\u6536\u96c6\u6578\u64da\uff0c\u4f7f\u7528\u8de8\u8996\u5716\u6aa2\u6e2c\u5339\u914d\u65b9\u6cd5\u5275\u5efa\u65b0\u7684\u8a3b\u89e3\u3002\u6709\u4e86\u9019\u4e9b\u5716\u50cf\u548c\u8a3b\u89e3\uff0c\u6211\u5011\u6574\u5408\u4e86\u57fa\u65bc LMM\u3001\u57fa\u65bc\u898f\u5247\u548c\u57fa\u65bc\u4eba\u985e\u7684\u65b9\u6cd5\u4f86\u69cb\u5efa\u5927\u898f\u6a21\u7684\u9ad8\u8cea\u91cf\u554f\u984c\u3002\u6211\u5011\u5c0d 21 \u500b LMM \u7684\u8a55\u4f30\u8868\u660e\uff0c\u7576\u524d\u7684 LMM \u5728\u5e7e\u500b\u65b9\u9762\u5728\u57ce\u5e02\u74b0\u5883\u4e2d\u6399\u624e\u3002\u5373\u4f7f\u6027\u80fd\u6700\u597d\u7684 GPT-4o \u5728\u5927\u591a\u6578\u4efb\u52d9\u4e2d\u4e5f\u843d\u5f8c\u65bc\u4eba\u985e\uff0c\u5f9e\u7c21\u55ae\u4efb\u52d9\uff08\u4f8b\u5982\u8a08\u6578\uff09\u5230\u8907\u96dc\u4efb\u52d9\uff08\u4f8b\u5982\u65b9\u5411\u3001\u5b9a\u4f4d\u548c\u5c0d\u8c61\u5c6c\u6027\u8b58\u5225\uff09\uff0c\u5e73\u5747\u6027\u80fd\u5dee\u8ddd\u70ba 17.4%\u3002\u6211\u5011\u7684\u57fa\u6e96\u9084\u8868\u660e\uff0cLMM \u5c0d\u4e0d\u540c\u7684\u57ce\u5e02\u8996\u5716\u8868\u73fe\u51fa\u4e0d\u4e00\u81f4\u7684\u884c\u70ba\uff0c\u5c24\u5176\u662f\u5728\u7406\u89e3\u8de8\u8996\u5716\u95dc\u4fc2\u65b9\u9762\u3002UrBench \u6578\u64da\u96c6\u548c\u57fa\u6e96\u7d50\u679c\u5c07\u5728 https://opendatalab.github.io/UrBench/ \u516c\u958b\u3002</paragraph>", "author": "Baichuan Zhou et.al.", "authors": "Baichuan Zhou, Haote Yang, Dairong Chen, Junyan Ye, Tianyi Bai, Jinhua Yu, Songyang Zhang, Dahua Lin, Conghui He, Weijia Li", "id": "2408.17267v1", "paper_url": "http://arxiv.org/abs/2408.17267v1", "repo": "null"}}