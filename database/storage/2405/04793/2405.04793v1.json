{"2405.04793": {"publish_time": "2024-05-08", "title": "Zero-shot LLM-guided Counterfactual Generation for Text", "paper_summary": "Counterfactual examples are frequently used for model development and\nevaluation in many natural language processing (NLP) tasks. Although methods\nfor automated counterfactual generation have been explored, such methods depend\non models such as pre-trained language models that are then fine-tuned on\nauxiliary, often task-specific datasets. Collecting and annotating such\ndatasets for counterfactual generation is labor intensive and therefore,\ninfeasible in practice. Therefore, in this work, we focus on a novel problem\nsetting: \\textit{zero-shot counterfactual generation}. To this end, we propose\na structured way to utilize large language models (LLMs) as general purpose\ncounterfactual example generators. We hypothesize that the\ninstruction-following and textual understanding capabilities of recent LLMs can\nbe effectively leveraged for generating high quality counterfactuals in a\nzero-shot manner, without requiring any training or fine-tuning. Through\ncomprehensive experiments on various downstream tasks in natural language\nprocessing (NLP), we demonstrate the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models.", "paper_summary_zh": "\u53cd\u4e8b\u5be6\u7bc4\u4f8b\u7d93\u5e38\u61c9\u7528\u65bc\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u4e2d\u7684\u6a21\u578b\u958b\u767c\u548c\u8a55\u4f30\u3002\u5118\u7ba1\u5df2\u63a2\u7d22\u7528\u65bc\u81ea\u52d5\u5316\u53cd\u4e8b\u5be6\u7522\u751f\u4e4b\u65b9\u6cd5\uff0c\u4f46\u6b64\u985e\u65b9\u6cd5\u4ef0\u8cf4\u65bc\u6a21\u578b\uff0c\u4f8b\u5982\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u7136\u5f8c\u5728\u8f14\u52a9\u6027\u8cc7\u6599\u96c6\uff08\u901a\u5e38\u70ba\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u8cc7\u6599\u96c6\uff09\u4e0a\u9032\u884c\u5fae\u8abf\u3002\u6536\u96c6\u548c\u6a19\u8a3b\u6b64\u985e\u8cc7\u6599\u96c6\u4ee5\u9032\u884c\u53cd\u4e8b\u5be6\u7522\u751f\u9700\u8981\u5927\u91cf\u4eba\u529b\uff0c\u56e0\u6b64\u5728\u5be6\u52d9\u4e0a\u4e0d\u53ef\u884c\u3002\u56e0\u6b64\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u4e00\u500b\u65b0\u7a4e\u7684\u554f\u984c\u8a2d\u5b9a\uff1a\\textit{\u96f6\u6b21\u5b78\u7fd2\u53cd\u4e8b\u5be6\u7522\u751f}\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7d50\u69cb\u5316\u7684\u65b9\u5f0f\uff0c\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f5c\u70ba\u901a\u7528\u53cd\u4e8b\u5be6\u7bc4\u4f8b\u7522\u751f\u5668\u3002\u6211\u5011\u5047\u8a2d\uff0c\u6700\u8fd1\u7684 LLM \u7684\u6307\u4ee4\u9075\u5faa\u548c\u6587\u672c\u7406\u89e3\u80fd\u529b\u53ef\u4ee5\u6709\u6548\u5730\u7528\u65bc\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u53cd\u4e8b\u5be6\uff0c\u4ee5\u96f6\u6b21\u5b78\u7fd2\u7684\u65b9\u5f0f\u9032\u884c\uff0c\u7121\u9700\u4efb\u4f55\u8a13\u7df4\u6216\u5fae\u8abf\u3002\u900f\u904e\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u9032\u884c\u5168\u9762\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86 LLM \u4f5c\u70ba\u96f6\u6b21\u5b78\u7fd2\u53cd\u4e8b\u5be6\u7522\u751f\u5668\u5728\u8a55\u4f30\u548c\u89e3\u91cb\u9ed1\u76d2 NLP \u6a21\u578b\u4e2d\u7684\u6548\u80fd\u3002", "author": "Amrita Bhattacharjee et.al.", "authors": "Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu", "id": "2405.04793v1", "paper_url": "http://arxiv.org/abs/2405.04793v1", "repo": "null"}}