{"2501.03700": {"publish_time": "2025-01-07", "title": "AuxDepthNet: Real-Time Monocular 3D Object Detection with Depth-Sensitive Features", "paper_summary": "Monocular 3D object detection is a challenging task in autonomous systems due\nto the lack of explicit depth information in single-view images. Existing\nmethods often depend on external depth estimators or expensive sensors, which\nincrease computational complexity and hinder real-time performance. To overcome\nthese limitations, we propose AuxDepthNet, an efficient framework for real-time\nmonocular 3D object detection that eliminates the reliance on external depth\nmaps or pre-trained depth models. AuxDepthNet introduces two key components:\nthe Auxiliary Depth Feature (ADF) module, which implicitly learns\ndepth-sensitive features to improve spatial reasoning and computational\nefficiency, and the Depth Position Mapping (DPM) module, which embeds depth\npositional information directly into the detection process to enable accurate\nobject localization and 3D bounding box regression. Leveraging the DepthFusion\nTransformer architecture, AuxDepthNet globally integrates visual and\ndepth-sensitive features through depth-guided interactions, ensuring robust and\nefficient detection. Extensive experiments on the KITTI dataset show that\nAuxDepthNet achieves state-of-the-art performance, with $\\text{AP}_{3D}$ scores\nof 24.72\\% (Easy), 18.63\\% (Moderate), and 15.31\\% (Hard), and\n$\\text{AP}_{\\text{BEV}}$ scores of 34.11\\% (Easy), 25.18\\% (Moderate), and\n21.90\\% (Hard) at an IoU threshold of 0.7.", "paper_summary_zh": "\u55ae\u76ee 3D \u7269\u4ef6\u5075\u6e2c\u7531\u65bc\u55ae\u8996\u5716\u5f71\u50cf\u4e2d\u7f3a\u4e4f\u660e\u78ba\u7684\u6df1\u5ea6\u8cc7\u8a0a\uff0c\u56e0\u6b64\u5728\u81ea\u99d5\u7cfb\u7d71\u4e2d\u662f\u4e00\u9805\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\u3002\u73fe\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u5916\u90e8\u6df1\u5ea6\u4f30\u6e2c\u5668\u6216\u6602\u8cb4\u7684\u611f\u6e2c\u5668\uff0c\u9019\u6703\u589e\u52a0\u904b\u7b97\u8907\u96dc\u5ea6\u4e26\u963b\u7919\u5373\u6642\u6548\u80fd\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa AuxDepthNet\uff0c\u4e00\u500b\u7528\u65bc\u5373\u6642\u55ae\u76ee 3D \u7269\u4ef6\u5075\u6e2c\u7684\u9ad8\u6548\u67b6\u69cb\uff0c\u5b83\u6d88\u9664\u4e86\u5c0d\u5916\u90e8\u6df1\u5ea6\u5716\u6216\u9810\u5148\u8a13\u7df4\u6df1\u5ea6\u6a21\u578b\u7684\u4f9d\u8cf4\u3002AuxDepthNet \u5c0e\u5165\u5169\u500b\u95dc\u9375\u5143\u4ef6\uff1a\u8f14\u52a9\u6df1\u5ea6\u7279\u5fb5 (ADF) \u6a21\u7d44\uff0c\u5b83\u96b1\u5f0f\u5b78\u7fd2\u6df1\u5ea6\u654f\u611f\u7279\u5fb5\u4ee5\u6539\u5584\u7a7a\u9593\u63a8\u7406\u548c\u904b\u7b97\u6548\u7387\uff0c\u4ee5\u53ca\u6df1\u5ea6\u4f4d\u7f6e\u5c0d\u61c9 (DPM) \u6a21\u7d44\uff0c\u5b83\u5c07\u6df1\u5ea6\u4f4d\u7f6e\u8cc7\u8a0a\u76f4\u63a5\u5d4c\u5165\u5075\u6e2c\u6d41\u7a0b\u4e2d\uff0c\u4ee5\u5be6\u73fe\u6e96\u78ba\u7684\u7269\u4ef6\u5b9a\u4f4d\u548c 3D \u908a\u754c\u6846\u56de\u6b78\u3002\u900f\u904e\u5229\u7528 DepthFusion Transformer \u67b6\u69cb\uff0cAuxDepthNet \u900f\u904e\u6df1\u5ea6\u5f15\u5c0e\u4e92\u52d5\uff0c\u5c07\u8996\u89ba\u548c\u6df1\u5ea6\u654f\u611f\u7279\u5fb5\u5728\u5168\u7403\u6574\u5408\uff0c\u78ba\u4fdd\u7a69\u5065\u4e14\u9ad8\u6548\u7684\u5075\u6e2c\u3002\u5728 KITTI \u8cc7\u6599\u96c6\u4e0a\u7684\u5927\u91cf\u5be6\u9a57\u986f\u793a\uff0cAuxDepthNet \u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u5728 IoU \u95be\u503c\u70ba 0.7 \u6642\uff0c$\\text{AP}_{3D}$ \u5206\u6578\u5206\u5225\u70ba 24.72%\uff08\u5bb9\u6613\uff09\u300118.63%\uff08\u4e2d\u7b49\uff09\u548c 15.31%\uff08\u56f0\u96e3\uff09\uff0c\u800c $\\text{AP}_{\\text{BEV}}$ \u5206\u6578\u5206\u5225\u70ba 34.11%\uff08\u5bb9\u6613\uff09\u300125.18%\uff08\u4e2d\u7b49\uff09\u548c 21.90%\uff08\u56f0\u96e3\uff09\u3002", "author": "Ruochen Zhang et.al.", "authors": "Ruochen Zhang, Hyeung-Sik Choi, Dongwook Jung, Phan Huy Nam Anh, Sang-Ki Jeong, Zihao Zhu", "id": "2501.03700v1", "paper_url": "http://arxiv.org/abs/2501.03700v1", "repo": "null"}}