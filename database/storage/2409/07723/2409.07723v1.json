{"2409.07723": {"publish_time": "2024-09-12", "title": "Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy", "paper_summary": "Depth estimation is a cornerstone of 3D reconstruction and plays a vital role\nin minimally invasive endoscopic surgeries. However, most current depth\nestimation networks rely on traditional convolutional neural networks, which\nare limited in their ability to capture global information. Foundation models\noffer a promising avenue for enhancing depth estimation, but those currently\navailable are primarily trained on natural images, leading to suboptimal\nperformance when applied to endoscopic images. In this work, we introduce a\nnovel fine-tuning strategy for the Depth Anything Model and integrate it with\nan intrinsic-based unsupervised monocular depth estimation framework. Our\napproach includes a low-rank adaptation technique based on random vectors,\nwhich improves the model's adaptability to different scales. Additionally, we\npropose a residual block built on depthwise separable convolution to compensate\nfor the transformer's limited ability to capture high-frequency details, such\nas edges and textures. Our experimental results on the SCARED dataset show that\nour method achieves state-of-the-art performance while minimizing the number of\ntrainable parameters. Applying this method in minimally invasive endoscopic\nsurgery could significantly enhance both the precision and safety of these\nprocedures.", "paper_summary_zh": "\u6df1\u5ea6\u4f30\u8a08\u662f 3D \u91cd\u5efa\u7684\u57fa\u77f3\uff0c\u5728\u5fae\u5275\u5167\u8996\u93e1\u624b\u8853\u4e2d\u626e\u6f14\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u7136\u800c\uff0c\u76ee\u524d\u5927\u591a\u6578\u6df1\u5ea6\u4f30\u8a08\u7db2\u8def\u4f9d\u8cf4\u50b3\u7d71\u7684\u5377\u7a4d\u795e\u7d93\u7db2\u8def\uff0c\u5176\u6355\u6349\u5168\u5c40\u8cc7\u8a0a\u7684\u80fd\u529b\u6709\u9650\u3002\u57fa\u790e\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u500b\u589e\u5f37\u6df1\u5ea6\u4f30\u8a08\u7684\u6709\u5e0c\u671b\u9014\u5f91\uff0c\u4f46\u76ee\u524d\u53ef\u7528\u7684\u57fa\u790e\u6a21\u578b\u4e3b\u8981\u5728\u81ea\u7136\u5f71\u50cf\u4e0a\u8a13\u7df4\uff0c\u5c0e\u81f4\u61c9\u7528\u65bc\u5167\u8996\u93e1\u5f71\u50cf\u6642\u6548\u80fd\u4e0d\u4f73\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u70ba Depth Anything Model \u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u5fae\u8abf\u7b56\u7565\uff0c\u4e26\u5c07\u5176\u8207\u57fa\u65bc\u5167\u5728\u7684\u7121\u76e3\u7763\u55ae\u773c\u6df1\u5ea6\u4f30\u8a08\u6846\u67b6\u6574\u5408\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u62ec\u4e00\u7a2e\u57fa\u65bc\u96a8\u6a5f\u5411\u91cf\u7684\u4f4e\u79e9\u9069\u61c9\u6280\u8853\uff0c\u5b83\u6539\u5584\u4e86\u6a21\u578b\u5c0d\u4e0d\u540c\u5c3a\u5ea6\u7684\u9069\u61c9\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5efa\u7acb\u5728\u6df1\u5ea6\u53ef\u5206\u96e2\u5377\u7a4d\u4e0a\u7684\u6b98\u5dee\u5340\u584a\uff0c\u4ee5\u5f4c\u88dcTransformer\u6355\u6349\u9ad8\u983b\u7d30\u7bc0\uff08\u4f8b\u5982\u908a\u7de3\u548c\u7d0b\u7406\uff09\u7684\u80fd\u529b\u6709\u9650\u3002\u6211\u5011\u5728 SCARED \u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u6700\u5c0f\u5316\u53ef\u8a13\u7df4\u53c3\u6578\u6578\u91cf\u7684\u60c5\u6cc1\u4e0b\uff0c\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u5728\u5fae\u5275\u5167\u8996\u93e1\u624b\u8853\u4e2d\u61c9\u7528\u6b64\u65b9\u6cd5\u53ef\u4ee5\u986f\u8457\u63d0\u9ad8\u9019\u4e9b\u7a0b\u5e8f\u7684\u7cbe\u78ba\u5ea6\u548c\u5b89\u5168\u6027\u3002", "author": "Bojian Li et.al.", "authors": "Bojian Li, Bo Liu, Jinghua Yue, Fugen Zhou", "id": "2409.07723v1", "paper_url": "http://arxiv.org/abs/2409.07723v1", "repo": "null"}}