{"2409.17834": {"publish_time": "2024-09-26", "title": "PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent Representation MOdification", "paper_summary": "Due to their substantial sizes, large language models (LLMs) are typically\ndeployed within a single-backbone multi-tenant framework. In this setup, a\nsingle instance of an LLM backbone must cater to multiple users or tasks\nthrough the application of various parameter-efficient fine-tuning (PEFT)\nmodels. Despite the availability of numerous effective PEFT techniques such as\nLoRA, there remains a need for a PEFT approach that achieves both high\nefficiency during inference and competitive performance on downstream tasks. In\nthis research, we introduce a new and straightforward PEFT methodology named\n\\underline{P}rompt D\\underline{E}pen\\underline{D}ent \\underline{R}epresentation\nM\\underline{O}dification (PEDRO). The proposed method involves integrating a\nlightweight vector generator into each Transformer layer, which generates\nvectors contingent upon the input prompts. These vectors then modify the hidden\nrepresentations created by the LLM through a dot product operation, thereby\ninfluencing the semantic output and generated content of the model. Extensive\nexperimentation across a variety of tasks indicates that: (a) PEDRO surpasses\nrecent PEFT benchmarks when using a similar number of tunable parameters. (b)\nUnder the single-backbone multi-tenant deployment model, PEDRO exhibits\nsuperior efficiency compared to LoRA, indicating significant industrial\npotential.", "paper_summary_zh": "\u7531\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u898f\u6a21\u9f90\u5927\uff0c\u56e0\u6b64\u901a\u5e38\u6703\u90e8\u7f72\u5728\u55ae\u4e00\u4e3b\u5e79\u591a\u79df\u6236\u67b6\u69cb\u4e2d\u3002\u5728\u9019\u7a2e\u67b6\u69cb\u4e2d\uff0cLLM \u4e3b\u5e79\u7684\u55ae\u4e00\u57f7\u884c\u500b\u9ad4\u5fc5\u9808\u900f\u904e\u61c9\u7528\u5404\u7a2e\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u6a21\u578b\u4f86\u6eff\u8db3\u591a\u500b\u4f7f\u7528\u8005\u6216\u4efb\u52d9\u3002\u5118\u7ba1\u6709\u8a31\u591a\u6709\u6548\u7684 PEFT \u6280\u8853\u53ef\u7528\uff0c\u4f8b\u5982 LoRA\uff0c\u4f46\u4ecd\u9700\u8981\u4e00\u7a2e PEFT \u65b9\u6cd5\uff0c\u65e2\u80fd\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u5be6\u73fe\u9ad8\u6548\u7387\uff0c\u53c8\u80fd\u5728\u4e0b\u6e38\u4efb\u52d9\u4e2d\u5c55\u73fe\u7af6\u722d\u529b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u3001\u76f4\u63a5\u7684 PEFT \u65b9\u6cd5\uff0c\u7a31\u70ba\u63d0\u793a\u4f9d\u8cf4\u8868\u793a\u4fee\u6539 (PEDRO)\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6d89\u53ca\u5c07\u8f15\u91cf\u7d1a\u5411\u91cf\u7522\u751f\u5668\u6574\u5408\u5230\u6bcf\u500b Transformer \u5c64\u4e2d\uff0c\u8a72\u7522\u751f\u5668\u6703\u6839\u64da\u8f38\u5165\u63d0\u793a\u7522\u751f\u5411\u91cf\u3002\u7136\u5f8c\uff0c\u9019\u4e9b\u5411\u91cf\u900f\u904e\u9ede\u7a4d\u904b\u7b97\u4fee\u6539 LLM \u6240\u5efa\u7acb\u7684\u96b1\u85cf\u8868\u793a\uff0c\u5f9e\u800c\u5f71\u97ff\u6a21\u578b\u7684\u8a9e\u7fa9\u8f38\u51fa\u548c\u7522\u751f\u7684\u5167\u5bb9\u3002\u900f\u904e\u5404\u7a2e\u4efb\u52d9\u7684\u5927\u91cf\u5be6\u9a57\uff0c\u7d50\u679c\u986f\u793a\uff1a(a) PEDRO \u5728\u4f7f\u7528\u985e\u4f3c\u6578\u91cf\u53ef\u8abf\u6574\u53c3\u6578\u6642\uff0c\u8d85\u8d8a\u4e86\u6700\u8fd1\u7684 PEFT \u57fa\u6e96\u3002(b) \u5728\u55ae\u4e00\u4e3b\u5e79\u591a\u79df\u6236\u90e8\u7f72\u6a21\u5f0f\u4e0b\uff0cPEDRO \u5c55\u73fe\u51fa\u512a\u65bc LoRA \u7684\u6548\u7387\uff0c\u986f\u793a\u51fa\u986f\u8457\u7684\u7522\u696d\u6f5b\u529b\u3002", "author": "Tianfang Xie et.al.", "authors": "Tianfang Xie, Tianjing Li, Wei Zhu, Wei Han, Yi Zhao", "id": "2409.17834v1", "paper_url": "http://arxiv.org/abs/2409.17834v1", "repo": "null"}}