{"2407.15362": {"publish_time": "2024-07-22", "title": "A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model", "paper_summary": "Remarkable strides in computational pathology have been made in the\ntask-agnostic foundation model that advances the performance of a wide array of\ndownstream clinical tasks. Despite the promising performance, there are still\nseveral challenges. First, prior works have resorted to either vision-only or\nvision-captions data, disregarding invaluable pathology reports and gene\nexpression profiles which respectively offer distinct knowledge for versatile\nclinical applications. Second, the current progress in pathology FMs\npredominantly concentrates on the patch level, where the restricted context of\npatch-level pretraining fails to capture whole-slide patterns. Here we curated\nthe largest multimodal dataset consisting of H\\&E diagnostic whole slide images\nand their associated pathology reports and RNA-Seq data, resulting in 26,169\nslide-level modality pairs from 10,275 patients across 32 cancer types. To\nleverage these data for CPath, we propose a novel whole-slide pretraining\nparadigm which injects multimodal knowledge at the whole-slide context into the\npathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed\nparadigm revolutionizes the workflow of pretraining for CPath, which enables\nthe pathology FM to acquire the whole-slide context. To our knowledge, this is\nthe first attempt to incorporate multimodal knowledge at the slide level for\nenhancing pathology FMs, expanding the modelling context from unimodal to\nmultimodal knowledge and from patch-level to slide-level. To systematically\nevaluate the capabilities of mSTAR, extensive experiments including slide-level\nunimodal and multimodal applications, are conducted across 7 diverse types of\ntasks on 43 subtasks, resulting in the largest spectrum of downstream tasks.\nThe average performance in various slide-level applications consistently\ndemonstrates significant performance enhancements for mSTAR compared to SOTA\nFMs.", "paper_summary_zh": "<paragraph>\u5728\u8a08\u7b97\u75c5\u7406\u5b78\u4e2d\uff0c\u4efb\u52d9\u4e0d\u53ef\u77e5\u57fa\u790e\u6a21\u578b\u5df2\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u53ef\u63d0\u5347\u5ee3\u6cdb\u4e0b\u6e38\u81e8\u5e8a\u4efb\u52d9\u7684\u6548\u80fd\u3002\u5118\u7ba1\u6548\u80fd\u4ee4\u4eba\u6eff\u610f\uff0c\u4f46\u4ecd\u6709\u5e7e\u500b\u6311\u6230\u3002\u9996\u5148\uff0c\u5148\u524d\u7684\u7814\u7a76\u50c5\u63a1\u7528\u50c5\u9650\u5f71\u50cf\u6216\u5f71\u50cf\u6a19\u984c\u8cc7\u6599\uff0c\u5ffd\u7565\u4e86\u5bf6\u8cb4\u7684\u75c5\u7406\u5831\u544a\u548c\u57fa\u56e0\u8868\u73fe\u7279\u5fb5\uff0c\u800c\u9019\u4e9b\u7279\u5fb5\u5206\u5225\u70ba\u591a\u529f\u80fd\u81e8\u5e8a\u61c9\u7528\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u77e5\u8b58\u3002\u5176\u6b21\uff0c\u75c5\u7406 FM \u7684\u7576\u524d\u9032\u5ea6\u4e3b\u8981\u96c6\u4e2d\u5728\u5340\u584a\u5c64\u7d1a\uff0c\u5340\u584a\u5c64\u7d1a\u9810\u8a13\u7df4\u7684\u53d7\u9650\u80cc\u666f\u7121\u6cd5\u64f7\u53d6\u5168\u5207\u7247\u6a21\u5f0f\u3002\u5728\u6b64\uff0c\u6211\u5011\u7b56\u5283\u4e86\u6700\u5927\u7684\u591a\u6a21\u614b\u8cc7\u6599\u96c6\uff0c\u5305\u542b H&E \u8a3a\u65b7\u5168\u5207\u7247\u5f71\u50cf\u53ca\u5176\u76f8\u95dc\u75c5\u7406\u5831\u544a\u548c RNA-Seq \u8cc7\u6599\uff0c\u5171\u7522\u751f\u4f86\u81ea 32 \u7a2e\u764c\u75c7\u985e\u578b\u7684 10,275 \u540d\u60a3\u8005\u7684 26,169 \u500b\u5207\u7247\u5c64\u7d1a\u6a21\u614b\u914d\u5c0d\u3002\u70ba\u4e86\u5c07\u9019\u4e9b\u8cc7\u6599\u7528\u65bc CPath\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u5168\u5207\u7247\u9810\u8a13\u7df4\u7bc4\u4f8b\uff0c\u5c07\u5168\u5207\u7247\u80cc\u666f\u7684\u591a\u6a21\u614b\u77e5\u8b58\u6ce8\u5165\u75c5\u7406 FM\uff0c\u7a31\u70ba\u591a\u6a21\u614b\u81ea\u6559\u9810\u8a13\u7df4 (mSTAR)\u3002\u6240\u63d0\u51fa\u7684\u7bc4\u4f8b\u5fb9\u5e95\u6539\u8b8a\u4e86 CPath \u7684\u9810\u8a13\u7df4\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u75c5\u7406 FM \u80fd\u5920\u7372\u53d6\u5168\u5207\u7247\u80cc\u666f\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u9019\u662f\u9996\u6b21\u5617\u8a66\u5728\u5207\u7247\u5c64\u7d1a\u7d0d\u5165\u591a\u6a21\u614b\u77e5\u8b58\u4ee5\u589e\u5f37\u75c5\u7406 FM\uff0c\u5c07\u5efa\u6a21\u80cc\u666f\u5f9e\u55ae\u4e00\u6a21\u614b\u77e5\u8b58\u64f4\u5c55\u5230\u591a\u6a21\u614b\u77e5\u8b58\uff0c\u4ee5\u53ca\u5f9e\u5340\u584a\u5c64\u7d1a\u64f4\u5c55\u5230\u5207\u7247\u5c64\u7d1a\u3002\u70ba\u4e86\u7cfb\u7d71\u6027\u5730\u8a55\u4f30 mSTAR \u7684\u529f\u80fd\uff0c\u6211\u5011\u5728 43 \u500b\u5b50\u4efb\u52d9\u4e2d\u5c0d 7 \u7a2e\u4e0d\u540c\u985e\u578b\u7684\u4efb\u52d9\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u5305\u62ec\u5207\u7247\u5c64\u7d1a\u7684\u55ae\u4e00\u6a21\u614b\u548c\u591a\u6a21\u614b\u61c9\u7528\uff0c\u7522\u751f\u4e86\u6700\u5927\u7684\u4e0b\u6e38\u4efb\u52d9\u7bc4\u570d\u3002\u5728\u5404\u7a2e\u5207\u7247\u5c64\u7d1a\u61c9\u7528\u4e2d\uff0c\u5e73\u5747\u6548\u80fd\u6301\u7e8c\u986f\u793a mSTAR \u8207 SOTA FM \u76f8\u6bd4\u6709\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\u3002</paragraph>", "author": "Yingxue Xu et.al.", "authors": "Yingxue Xu, Yihui Wang, Fengtao Zhou, Jiabo Ma, Shu Yang, Huangjing Lin, Xin Wang, Jiguang Wang, Li Liang, Anjia Han, Ronald Cheong Kin Chan, Hao Chen", "id": "2407.15362v1", "paper_url": "http://arxiv.org/abs/2407.15362v1", "repo": "null"}}