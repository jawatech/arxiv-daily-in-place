{"2409.11689": {"publish_time": "2024-09-18", "title": "GUNet: A Graph Convolutional Network United Diffusion Model for Stable and Diversity Pose Generation", "paper_summary": "Pose skeleton images are an important reference in pose-controllable image\ngeneration. In order to enrich the source of skeleton images, recent works have\ninvestigated the generation of pose skeletons based on natural language. These\nmethods are based on GANs. However, it remains challenging to perform diverse,\nstructurally correct and aesthetically pleasing human pose skeleton generation\nwith various textual inputs. To address this problem, we propose a framework\nwith GUNet as the main model, PoseDiffusion. It is the first generative\nframework based on a diffusion model and also contains a series of variants\nfine-tuned based on a stable diffusion model. PoseDiffusion demonstrates\nseveral desired properties that outperform existing methods. 1) Correct\nSkeletons. GUNet, a denoising model of PoseDiffusion, is designed to\nincorporate graphical convolutional neural networks. It is able to learn the\nspatial relationships of the human skeleton by introducing skeletal information\nduring the training process. 2) Diversity. We decouple the key points of the\nskeleton and characterise them separately, and use cross-attention to introduce\ntextual conditions. Experimental results show that PoseDiffusion outperforms\nexisting SoTA algorithms in terms of stability and diversity of text-driven\npose skeleton generation. Qualitative analyses further demonstrate its\nsuperiority for controllable generation in Stable Diffusion.", "paper_summary_zh": "\u59ff\u52e2\u9aa8\u67b6\u5716\u50cf\u662f\u59ff\u52e2\u53ef\u63a7\u5716\u50cf\u751f\u6210\u4e2d\u91cd\u8981\u7684\u53c3\u8003\u3002\u70ba\u4e86\u8c50\u5bcc\u9aa8\u67b6\u5716\u50cf\u7684\u4f86\u6e90\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8abf\u67e5\u4e86\u57fa\u65bc\u81ea\u7136\u8a9e\u8a00\u7684\u59ff\u52e2\u9aa8\u67b6\u751f\u6210\u3002\u9019\u4e9b\u65b9\u6cd5\u57fa\u65bc GAN\u3002\u7136\u800c\uff0c\u8981\u57f7\u884c\u591a\u6a23\u5316\u3001\u7d50\u69cb\u6b63\u78ba\u4e14\u7f8e\u89c0\u7684\u4eba\u9ad4\u59ff\u52e2\u9aa8\u67b6\u751f\u6210\uff0c\u4e26\u5177\u6709\u5404\u7a2e\u6587\u672c\u8f38\u5165\uff0c\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4ee5 GUNet \u70ba\u4e3b\u8981\u6a21\u578b\u7684\u6846\u67b6\uff0cPoseDiffusion\u3002\u5b83\u662f\u57fa\u65bc\u64f4\u6563\u6a21\u578b\u7684\u7b2c\u4e00\u500b\u751f\u6210\u6846\u67b6\uff0c\u9084\u5305\u542b\u4e00\u7cfb\u5217\u57fa\u65bc\u7a69\u5b9a\u64f4\u6563\u6a21\u578b\u9032\u884c\u5fae\u8abf\u7684\u8b8a\u9ad4\u3002PoseDiffusion \u5c55\u793a\u4e86\u591a\u9805\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u7684\u7406\u60f3\u5c6c\u6027\u30021) \u6b63\u78ba\u7684\u9aa8\u67b6\u3002PoseDiffusion \u7684\u53bb\u566a\u6a21\u578b GUNet \u88ab\u8a2d\u8a08\u70ba\u7d50\u5408\u5716\u5f62\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u3002\u5b83\u80fd\u5920\u900f\u904e\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u5f15\u5165\u9aa8\u67b6\u8cc7\u8a0a\u4f86\u5b78\u7fd2\u4eba\u9ad4\u9aa8\u67b6\u7684\u7a7a\u9593\u95dc\u4fc2\u30022) \u591a\u6a23\u6027\u3002\u6211\u5011\u89e3\u8026\u9aa8\u67b6\u7684\u95dc\u9375\u9ede\u4e26\u5206\u5225\u5c0d\u5176\u9032\u884c\u8868\u5fb5\uff0c\u4e26\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u4f86\u5f15\u5165\u6587\u672c\u689d\u4ef6\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cPoseDiffusion \u5728\u6587\u672c\u9a45\u52d5\u59ff\u52e2\u9aa8\u67b6\u751f\u6210\u7684\u7a69\u5b9a\u6027\u548c\u591a\u6a23\u6027\u65b9\u9762\u512a\u65bc\u73fe\u6709\u7684 SoTA \u6f14\u7b97\u6cd5\u3002\u5b9a\u6027\u5206\u6790\u9032\u4e00\u6b65\u8b49\u660e\u4e86\u5b83\u5728 Stable Diffusion \u4e2d\u53ef\u63a7\u751f\u6210\u7684\u512a\u8d8a\u6027\u3002", "author": "Shuowen Liang et.al.", "authors": "Shuowen Liang, Sisi Li, Qingyun Wang, Cen Zhang, Kaiquan Zhu, Tian Yang", "id": "2409.11689v1", "paper_url": "http://arxiv.org/abs/2409.11689v1", "repo": "null"}}