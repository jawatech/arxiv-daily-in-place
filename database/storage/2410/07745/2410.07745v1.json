{"2410.07745": {"publish_time": "2024-10-10", "title": "StepTool: A Step-grained Reinforcement Learning Framework for Tool Learning in LLMs", "paper_summary": "Despite having powerful reasoning and inference capabilities, Large Language\nModels (LLMs) still need external tools to acquire real-time information\nretrieval or domain-specific expertise to solve complex tasks, which is\nreferred to as tool learning. Existing tool learning methods primarily rely on\ntuning with expert trajectories, focusing on token-sequence learning from a\nlinguistic perspective. However, there are several challenges: 1) imitating\nstatic trajectories limits their ability to generalize to new tasks. 2) even\nexpert trajectories can be suboptimal, and better solution paths may exist. In\nthis work, we introduce StepTool, a novel step-grained reinforcement learning\nframework to improve tool learning in LLMs. It consists of two components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on tool invocation success and its contribution to the task, and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\nproviding a robust solution for complex task environments. Codes are available\nat https://github.com/yuyq18/StepTool.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u64c1\u6709\u5f37\u5927\u7684\u63a8\u7406\u548c\u63a8\u8ad6\u80fd\u529b\uff0c\u4f46\u4ecd\u9700\u8981\u5916\u90e8\u5de5\u5177\u4f86\u7372\u53d6\u5373\u6642\u8cc7\u8a0a\u6aa2\u7d22\u6216\u7279\u5b9a\u9818\u57df\u7684\u5c08\u696d\u77e5\u8b58\uff0c\u4ee5\u89e3\u6c7a\u8907\u96dc\u4efb\u52d9\uff0c\u9019\u88ab\u7a31\u70ba\u5de5\u5177\u5b78\u7fd2\u3002\u73fe\u6709\u7684\u5de5\u5177\u5b78\u7fd2\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8cf4\u65bc\u5c08\u5bb6\u8ecc\u8de1\u7684\u8abf\u6574\uff0c\u5c08\u6ce8\u65bc\u5f9e\u8a9e\u8a00\u5b78\u7684\u89d2\u5ea6\u9032\u884c\u4ee4\u724c\u5e8f\u5217\u5b78\u7fd2\u3002\u7136\u800c\uff0c\u6709\u5e7e\u500b\u6311\u6230\uff1a1) \u6a21\u4eff\u975c\u614b\u8ecc\u8de1\u9650\u5236\u4e86\u5b83\u5011\u6cdb\u5316\u5230\u65b0\u4efb\u52d9\u7684\u80fd\u529b\u30022) \u5373\u4f7f\u662f\u5c08\u5bb6\u8ecc\u8de1\u4e5f\u53ef\u80fd\u4e0d\u662f\u6700\u4f73\u7684\uff0c\u4e26\u4e14\u53ef\u80fd\u5b58\u5728\u66f4\u597d\u7684\u89e3\u6c7a\u65b9\u6848\u8def\u5f91\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 StepTool\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6b65\u9a5f\u5316\u5f37\u5316\u5b78\u7fd2\u6846\u67b6\uff0c\u7528\u65bc\u6539\u9032 LLM \u4e2d\u7684\u5de5\u5177\u5b78\u7fd2\u3002\u5b83\u5305\u542b\u5169\u500b\u7d44\u6210\u90e8\u5206\uff1a\u6b65\u9a5f\u5316\u734e\u52f5\u8abf\u6574\uff0c\u5b83\u6839\u64da\u5de5\u5177\u8abf\u7528\u6210\u529f\u53ca\u5176\u5c0d\u4efb\u52d9\u7684\u8ca2\u737b\u5728\u6bcf\u6b21\u5de5\u5177\u4e92\u52d5\u6642\u5206\u914d\u734e\u52f5\uff0c\u4ee5\u53ca\u6b65\u9a5f\u5316\u6700\u4f73\u5316\uff0c\u5b83\u4f7f\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4ee5\u591a\u6b65\u9a5f\u7684\u65b9\u5f0f\u6700\u4f73\u5316\u6a21\u578b\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cStepTool \u5728\u57fa\u65bc\u5de5\u5177\u7684\u591a\u6b65\u9a5f\u4efb\u52d9\u4e2d\u986f\u8457\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u70ba\u8907\u96dc\u4efb\u52d9\u74b0\u5883\u63d0\u4f9b\u4e86\u4e00\u500b\u7a69\u5065\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/yuyq18/StepTool \u7372\u5f97\u3002", "author": "Yuanqing Yu et.al.", "authors": "Yuanqing Yu, Zhefan Wang, Weizhi Ma, Zhicheng Guo, Jingtao Zhan, Shuai Wang, Chuhan Wu, Zhiqiang Guo, Min Zhang", "id": "2410.07745v1", "paper_url": "http://arxiv.org/abs/2410.07745v1", "repo": "https://github.com/yuyq18/steptool"}}