{"2407.11282": {"publish_time": "2024-07-15", "title": "Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models", "paper_summary": "Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u88ab\u7528\u65bc\u5404\u7a2e\u9ad8\u98a8\u96aa\u9818\u57df\uff0c\n\u5176\u4e2d\u5176\u8f38\u51fa\u7684\u53ef\u9760\u6027\u81f3\u95dc\u91cd\u8981\u3002\u4e00\u7a2e\u5e38\u7528\u7684\u65b9\u6cd5\u4f86\n\u8a55\u4f30 LLM \u56de\u61c9\u7684\u53ef\u9760\u6027\u662f\u4e0d\u78ba\u5b9a\u6027\u4f30\u8a08\uff0c\u5b83\n\u8861\u91cf\u5176\u7b54\u6848\u6b63\u78ba\u7684\u53ef\u80fd\u6027\u3002\u96d6\u7136\u8a31\u591a\u7814\u7a76\u5c08\u6ce8\u65bc\n\u63d0\u9ad8 LLM \u4e0d\u78ba\u5b9a\u6027\u4f30\u8a08\u7684\u6e96\u78ba\u6027\uff0c\u4f46\u6211\u5011\u7684\u7814\u7a76\n\u8abf\u67e5\u4e86\u4e0d\u78ba\u5b9a\u6027\u4f30\u8a08\u7684\u8106\u5f31\u6027\u4e26\u63a2\u8a0e\u4e86\u6f5b\u5728\u7684\n\u653b\u64ca\u3002\u6211\u5011\u8b49\u660e\u653b\u64ca\u8005\u53ef\u4ee5\u5728 LLM \u4e2d\u5d4c\u5165\u5f8c\u9580\uff0c\u5f8c\u9580\n\u5728\u8f38\u5165\u4e2d\u88ab\u7279\u5b9a\u89f8\u767c\u5668\u6fc0\u6d3b\u6642\uff0c\u6703\u64cd\u7e31\u6a21\u578b\u7684\n\u4e0d\u78ba\u5b9a\u6027\uff0c\u800c\u4e0d\u6703\u5f71\u97ff\u6700\u7d42\u8f38\u51fa\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u63d0\u51fa\u7684\n\u5f8c\u9580\u653b\u64ca\u65b9\u6cd5\u53ef\u4ee5\u6539\u8b8a LLM \u7684\u8f38\u51fa\u6982\u7387\u5206\u4f48\uff0c\n\u5c0e\u81f4\u6982\u7387\u5206\u4f48\u6536\u6582\u5230\u653b\u64ca\u8005\u9810\u5b9a\u7fa9\u7684\n\u5206\u4f48\uff0c\u540c\u6642\u78ba\u4fdd top-1 \u9810\u6e2c\u4fdd\u6301\u4e0d\u8b8a\u3002\u6211\u5011\u7684\n\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u9019\u7a2e\u653b\u64ca\u6709\u6548\u5730\u7834\u58de\u4e86\n\u6a21\u578b\u5728\u591a\u9078\u984c\u4e2d\u7684\u81ea\u6211\u8a55\u4f30\u53ef\u9760\u6027\u3002\u4f8b\u5982\uff0c\n\u6211\u5011\u5728\u56db\u500b\u6a21\u578b\u4e2d\u63a1\u7528\u4e86\u4e09\u7a2e\u4e0d\u540c\u7684\u89f8\u767c\n\u7b56\u7565\uff0c\u9054\u5230\u4e86 100% \u7684\u653b\u64ca\u6210\u529f\u7387 (ASR)\u3002\u6b64\u5916\uff0c\u6211\u5011\u7814\u7a76\u4e86\u9019\u7a2e\u64cd\u7e31\n\u662f\u5426\u9069\u7528\u65bc\u4e0d\u540c\u7684\u63d0\u793a\u548c\u9818\u57df\u3002\u9019\u9805\u5de5\u4f5c\u7a81\u51fa\u4e86\n\u5c0d LLM \u53ef\u9760\u6027\u7684\u91cd\u5927\u5a01\u8105\uff0c\u4e26\u5f37\u8abf\u4e86\u5c0d\u6b64\u985e\u653b\u64ca\u9032\u884c\u672a\u4f86\u9632\u79a6\u7684\u5fc5\u8981\u6027\u3002\u4ee3\u78bc\u53ef\u5728\nhttps://github.com/qcznlp/uncertainty_attack \u7372\u5f97\u3002", "author": "Qingcheng Zeng et.al.", "authors": "Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang", "id": "2407.11282v1", "paper_url": "http://arxiv.org/abs/2407.11282v1", "repo": "https://github.com/qcznlp/uncertainty_attack"}}