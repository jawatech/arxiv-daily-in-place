{"2405.17991": {"publish_time": "2024-05-28", "title": "VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections", "paper_summary": "Large language models (LLMs) have recently emerged as powerful tools for\ntackling many language-processing tasks. Despite their success, training and\nfine-tuning these models is still far too computationally and memory intensive.\nIn this paper, we identify and characterise the important components needed for\neffective model convergence using gradient descent. In doing so we find that\nthe intermediate activations used to implement backpropagation can be\nexcessively compressed without incurring any degradation in performance. This\nresult leads us to a cheap and memory-efficient algorithm for both fine-tuning\nand pre-training LLMs. The proposed algorithm simply divides the tokens up into\nsmaller sub-tokens before projecting them onto a fixed 1-dimensional subspace\nduring the forward pass. These features are then coarsely reconstructed during\nthe backward pass to implement the update rules. We confirm the effectiveness\nof our algorithm as being complimentary to many state-of-the-art PEFT methods\non the VTAB-1k fine-tuning benchmark. Furthermore, we outperform QLoRA for\nfine-tuning LLaMA and show competitive performance against other\nmemory-efficient pre-training methods on the large-scale C4 dataset.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6700\u8fd1\u5df2\u6210\u70ba\u8655\u7406\u8a31\u591a\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u7684\u5f37\u5927\u5de5\u5177\u3002\u5118\u7ba1\u5b83\u5011\u5f88\u6210\u529f\uff0c\u4f46\u8a13\u7df4\u548c\u5fae\u8abf\u9019\u4e9b\u6a21\u578b\u5728\u8a08\u7b97\u548c\u8a18\u61b6\u9ad4\u4e0a\u4ecd\u904e\u65bc\u5bc6\u96c6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8b58\u5225\u4e26\u63cf\u8ff0\u4e86\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u9032\u884c\u6709\u6548\u6a21\u578b\u6536\u6582\u6240\u9700\u7684\u91cd\u8981\u7d44\u6210\u90e8\u5206\u3002\u5728\u9019\u6a23\u505a\u7684\u904e\u7a0b\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u7528\u65bc\u5be6\u4f5c\u53cd\u5411\u50b3\u64ad\u7684\u4e2d\u9593\u6d3b\u5316\u53ef\u4ee5\u904e\u5ea6\u58d3\u7e2e\uff0c\u800c\u4e0d\u6703\u9020\u6210\u6548\u80fd\u4e0b\u964d\u3002\u9019\u500b\u7d50\u679c\u8b93\u6211\u5011\u627e\u5230\u4e00\u500b\u4fbf\u5b9c\u4e14\u8a18\u61b6\u9ad4\u6548\u7387\u9ad8\u7684\u6f14\u7b97\u6cd5\uff0c\u53ef\u7528\u65bc\u5fae\u8abf\u548c LLM \u9810\u8a13\u7df4\u3002\u6240\u63d0\u51fa\u7684\u6f14\u7b97\u6cd5\u53ea\u662f\u5728\u6b63\u5411\u50b3\u905e\u671f\u9593\u5c07\u7b26\u865f\u5206\u6210\u8f03\u5c0f\u7684\u5b50\u7b26\u865f\uff0c\u7136\u5f8c\u5c07\u5b83\u5011\u6295\u5f71\u5230\u56fa\u5b9a\u7684 1 \u7dad\u5b50\u7a7a\u9593\u3002\u7136\u5f8c\u5728\u53cd\u5411\u50b3\u905e\u671f\u9593\u7c97\u7565\u91cd\u5efa\u9019\u4e9b\u7279\u5fb5\u4ee5\u5be6\u4f5c\u66f4\u65b0\u898f\u5247\u3002\u6211\u5011\u78ba\u8a8d\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u56e0\u70ba\u5b83\u53ef\u4ee5\u88dc\u5145 VTAB-1k \u5fae\u8abf\u57fa\u6e96\u4e0a\u7684\u8a31\u591a\u6700\u65b0 PEFT \u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728\u5fae\u8abf LLaMA \u6642\u512a\u65bc QLoRA\uff0c\u4e26\u5728\u5927\u578b C4 \u8cc7\u6599\u96c6\u4e0a\u5c55\u73fe\u51fa\u8207\u5176\u4ed6\u8a18\u61b6\u9ad4\u6548\u7387\u9ad8\u7684\u9810\u8a13\u7df4\u65b9\u6cd5\u7af6\u722d\u7684\u6548\u80fd\u3002", "author": "Roy Miles et.al.", "authors": "Roy Miles, Pradyumna Reddy, Ismail Elezi, Jiankang Deng", "id": "2405.17991v1", "paper_url": "http://arxiv.org/abs/2405.17991v1", "repo": "null"}}