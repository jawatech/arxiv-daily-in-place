{"2409.10164": {"publish_time": "2024-09-16", "title": "Quantile Regression for Distributional Reward Models in RLHF", "paper_summary": "Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5df2\u6210\u70ba\u900f\u904e\u4f7f\u7528\u734e\u52f5\u6a21\u578b\uff0c\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u7684\u4e00\u7a2e\u95dc\u9375\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u50b3\u7d71\u7684\u734e\u52f5\u6a21\u578b\u901a\u5e38\u6703\u7522\u751f\u9ede\u4f30\u8a08\uff0c\u9019\u904e\u5ea6\u7c21\u5316\u4e86\u4eba\u985e\u50f9\u503c\u89c0\u548c\u504f\u597d\u7684\u591a\u6a23\u6027\u548c\u8907\u96dc\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u5206\u4f4d\u6578\u734e\u52f5\u6a21\u578b (QRM)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u734e\u52f5\u5efa\u6a21\u65b9\u6cd5\uff0c\u5b83\u6703\u5b78\u7fd2\u734e\u52f5\u5206\u4f48\uff0c\u800c\u4e0d\u662f\u55ae\u4e00\u6a19\u91cf\u503c\u3002\u6211\u5011\u7684\u6a21\u578b\u4f7f\u7528\u5206\u4f4d\u6578\u56de\u6b78\u4f86\u4f30\u8a08\u504f\u597d\u7684\u5b8c\u6574\u591a\u6a21\u614b\u5206\u4f48\uff0c\u63d0\u4f9b\u66f4\u5f37\u5927\u4e14\u7d30\u7dfb\u7684\u504f\u597d\u8868\u793a\u3002\u9019\u7a2e\u5206\u4f48\u5f0f\u65b9\u6cd5\u53ef\u4ee5\u66f4\u597d\u5730\u6355\u6349\u4eba\u985e\u50f9\u503c\u89c0\u7684\u591a\u6a23\u6027\uff0c\u89e3\u6c7a\u6a19\u7c64\u96dc\u8a0a\uff0c\u4e26\u900f\u904e\u5c07\u5176\u5efa\u6a21\u70ba\u5206\u4f48\u4e2d\u7684\u4e0d\u540c\u6a21\u5f0f\u4f86\u5bb9\u7d0d\u76f8\u4e92\u885d\u7a81\u7684\u504f\u597d\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cQRM \u5728 RewardBench \u4e0a\u512a\u65bc\u53ef\u6bd4\u8f03\u7684\u50b3\u7d71\u9ede\u4f30\u8a08\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e\u4e86\u5206\u4f48\u5f0f\u4f30\u8a08\u6240\u63d0\u4f9b\u7684\u984d\u5916\u8cc7\u8a0a\u53ef\u7528\u65bc\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\uff0c\u4f8b\u5982\u98a8\u96aa\u611f\u77e5\u5f37\u5316\u5b78\u7fd2\uff0c\u9019\u6703\u7522\u751f\u66f4\u5c11\u6975\u7aef\u8ca0\u9762\u56de\u61c9\u7684 LLM \u653f\u7b56\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u5df2\u5728 https://github.com/Nicolinho/QRM \u767c\u5e03\u3002", "author": "Nicolai Dorka et.al.", "authors": "Nicolai Dorka", "id": "2409.10164v1", "paper_url": "http://arxiv.org/abs/2409.10164v1", "repo": "null"}}