{"2405.06058": {"publish_time": "2024-05-09", "title": "Large Language Models Show Human-like Social Desirability Biases in Survey Responses", "paper_summary": "As Large Language Models (LLMs) become widely used to model and simulate\nhuman behavior, understanding their biases becomes critical. We developed an\nexperimental framework using Big Five personality surveys and uncovered a\npreviously undetected social desirability bias in a wide range of LLMs. By\nsystematically varying the number of questions LLMs were exposed to, we\ndemonstrate their ability to infer when they are being evaluated. When\npersonality evaluation is inferred, LLMs skew their scores towards the\ndesirable ends of trait dimensions (i.e., increased extraversion, decreased\nneuroticism, etc). This bias exists in all tested models, including GPT-4/3.5,\nClaude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent\nmodels, with GPT-4's survey responses changing by 1.20 (human) standard\ndeviations and Llama 3's by 0.98 standard deviations-very large effects. This\nbias is robust to randomization of question order and paraphrasing.\nReverse-coding all the questions decreases bias levels but does not eliminate\nthem, suggesting that this effect cannot be attributed to acquiescence bias.\nOur findings reveal an emergent social desirability bias and suggest\nconstraints on profiling LLMs with psychometric tests and on using LLMs as\nproxies for human participants.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u88ab\u5ee3\u6cdb\u7528\u65bc\u6a21\u64ec\u548c\u6a21\u64ec\n\u4eba\u985e\u884c\u70ba\uff0c\u4e86\u89e3\u5176\u504f\u898b\u8b8a\u5f97\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\n\u4f7f\u7528\u5927\u4e94\u4eba\u683c\u6e2c\u9a57\u7684\u5be6\u9a57\u6846\u67b6\uff0c\u4e26\u767c\u73fe\u4e86\u5ee3\u6cdb\u7684 LLM \u4e2d\u4ee5\u524d\u672a\u6aa2\u6e2c\u5230\u7684\u793e\u6703\u671f\u671b\u504f\u5dee\u3002\u901a\u904e\n\u7cfb\u7d71\u5730\u6539\u8b8a LLM \u63a5\u89f8\u5230\u7684\u554f\u984c\u6578\u91cf\uff0c\u6211\u5011\n\u8b49\u660e\u4e86\u4ed6\u5011\u63a8\u65b7\u81ea\u5df1\u4f55\u6642\u88ab\u8a55\u4f30\u7684\u80fd\u529b\u3002\u7576\n\u4eba\u683c\u8a55\u4f30\u88ab\u63a8\u65b7\u51fa\u4f86\u6642\uff0cLLM \u6703\u5c07\u5176\u5f97\u5206\u504f\u5411\u7279\u8cea\u7dad\u5ea6\u7684\u7406\u60f3\u7d42\u9ede\uff08\u5373\uff0c\u5916\u5411\u6027\u589e\u52a0\uff0c\u795e\u7d93\u8cea\u6e1b\u5c11\n\u7b49\uff09\u3002\u9019\u7a2e\u504f\u5dee\u5b58\u5728\u65bc\u6240\u6709\u6e2c\u8a66\u6a21\u578b\u4e2d\uff0c\u5305\u62ec GPT-4/3.5\u3001\nClaude 3\u3001Llama 3 \u548c PaLM-2\u3002\u504f\u5dee\u6c34\u5e73\u4f3c\u4e4e\u5728\u66f4\u65b0\u7684\n\u6a21\u578b\u4e2d\u6709\u6240\u589e\u52a0\uff0cGPT-4 \u7684\u8abf\u67e5\u56de\u61c9\u6539\u8b8a\u4e86 1.20\uff08\u4eba\u985e\uff09\u6a19\u6e96\n\u504f\u5dee\u548c Llama 3 \u7684 0.98 \u6a19\u6e96\u504f\u5dee - \u975e\u5e38\u5927\u7684\u5f71\u97ff\u3002\u9019\u7a2e\n\u504f\u5dee\u5c0d\u554f\u984c\u9806\u5e8f\u548c\u540c\u7fa9\u8a5e\u7684\u96a8\u6a5f\u5316\u5177\u6709\u9b6f\u68d2\u6027\u3002\n\u5c0d\u6240\u6709\u554f\u984c\u9032\u884c\u53cd\u5411\u7de8\u78bc\u6703\u964d\u4f4e\u504f\u5dee\u6c34\u5e73\uff0c\u4f46\u4e26\u4e0d\u80fd\u6d88\u9664\n\u5b83\u5011\uff0c\u8868\u660e\u9019\u7a2e\u6548\u61c9\u4e0d\u80fd\u6b78\u56e0\u65bc\u9806\u5f9e\u504f\u5dee\u3002\n\u6211\u5011\u7684\u767c\u73fe\u63ed\u793a\u4e86\u4e00\u500b\u65b0\u51fa\u73fe\u7684\u793e\u6703\u671f\u671b\u504f\u5dee\uff0c\u4e26\u6697\u793a\n\u5c0d\u4f7f\u7528\u5fc3\u7406\u6e2c\u9a57\u5c0d LLM \u9032\u884c\u5206\u6790\u548c\u4f7f\u7528 LLM \u4f5c\u70ba\n\u4eba\u985e\u53c3\u8207\u8005\u7684\u4ee3\u7406\u5b58\u5728\u9650\u5236\u3002", "author": "Aadesh Salecha et.al.", "authors": "Aadesh Salecha, Molly E. Ireland, Shashanka Subrahmanya, Jo\u00e3o Sedoc, Lyle H. Ungar, Johannes C. Eichstaedt", "id": "2405.06058v1", "paper_url": "http://arxiv.org/abs/2405.06058v1", "repo": "null"}}