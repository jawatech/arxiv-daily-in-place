{"2407.20584": {"publish_time": "2024-07-30", "title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse Training", "paper_summary": "Transformer-based Large Language Models (LLMs) have demonstrated remarkable\nsuccess across various challenging tasks. However, the deployment of LLMs is\nhindered by their substantial parameter count and memory consumption. Recently,\nnumerous studies have attempted to compress LLMs by pruning them using\ntraining-free methods. However, these pruned models often experience\nsignificant performance degradation on complex tasks. To address this issue, we\npropose a novel training pipeline for semi-structured sparse models, named\nAdaptive Sparse Trainer (AST). By distilling the knowledge stored in its dense\ncounterpart, we prevent the sparse model from overfitting and ensure a stable\ntraining process. Moreover, AST allows the model to adaptively select better\nlottery tickets (e.g., masks) during training. Additionally, we discovered that\nadding extra well-initialized parameters can further enhance model performance\nwith only a small increase in memory footprint. Our method significantly\nnarrows the performance gap between dense and sparse models while maintaining\nlimited computational cost. Furthermore, when combined with existing\nquantization methods, AST can compress language models by up to 16x compared to\ndense FP32 precision models with minimal performance loss. AST outperforms\nprevious state-of-the-art methods by reducing the zero-shot accuracy gap\nbetween dense and semi-structured sparse models to 1.12% across multiple\nzero-shot tasks on Llama2-7B, using less than 0.4% of the pretraining tokens.", "paper_summary_zh": "<paragraph>\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u5177\u6311\u6230\u6027\u7684\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u6210\u529f\u3002\u7136\u800c\uff0cLLM \u7684\u90e8\u7f72\u53d7\u5230\u5176\u9f90\u5927\u7684\u53c3\u6578\u6578\u91cf\u548c\u8a18\u61b6\u9ad4\u6d88\u8017\u7684\u963b\u7919\u3002\u6700\u8fd1\uff0c\u8a31\u591a\u7814\u7a76\u5617\u8a66\u900f\u904e\u4f7f\u7528\u7121\u9700\u8a13\u7df4\u7684\u65b9\u6cd5\u526a\u679d\u4f86\u58d3\u7e2e LLM\u3002\u7136\u800c\uff0c\u9019\u4e9b\u526a\u679d\u6a21\u578b\u5728\u8907\u96dc\u4efb\u52d9\u4e2d\u7d93\u5e38\u6703\u906d\u9047\u986f\u8457\u7684\u6548\u80fd\u4e0b\u964d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba\u81ea\u9069\u61c9\u7a00\u758f\u8a13\u7df4\u5668 (AST) \u7684\u534a\u7d50\u69cb\u7a00\u758f\u6a21\u578b\u7684\u65b0\u8a13\u7df4\u7ba1\u9053\u3002\u900f\u904e\u8403\u53d6\u5132\u5b58\u5728\u5176\u7a20\u5bc6\u5c0d\u61c9\u6a21\u578b\u4e2d\u7684\u77e5\u8b58\uff0c\u6211\u5011\u53ef\u4ee5\u9632\u6b62\u7a00\u758f\u6a21\u578b\u904e\u5ea6\u64ec\u5408\u4e26\u78ba\u4fdd\u7a69\u5b9a\u7684\u8a13\u7df4\u904e\u7a0b\u3002\u6b64\u5916\uff0cAST \u5141\u8a31\u6a21\u578b\u5728\u8a13\u7df4\u671f\u9593\u81ea\u9069\u61c9\u5730\u9078\u64c7\u66f4\u597d\u7684\u6a02\u900f\u5f69\u5238\uff08\u4f8b\u5982\u906e\u7f69\uff09\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u52a0\u5165\u984d\u5916\u7684\u826f\u597d\u521d\u59cb\u5316\u53c3\u6578\u53ef\u4ee5\u5728\u50c5\u5c0f\u5e45\u589e\u52a0\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u7684\u72c0\u6cc1\u4e0b\u9032\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6548\u80fd\u3002\u6211\u5011\u7684\u6a21\u578b\u5927\u5e45\u7e2e\u5c0f\u4e86\u7a20\u5bc6\u6a21\u578b\u548c\u7a00\u758f\u6a21\u578b\u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\uff0c\u540c\u6642\u7dad\u6301\u6709\u9650\u7684\u904b\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0cAST \u7d50\u5408\u73fe\u6709\u7684\u91cf\u5316\u65b9\u6cd5\u5f8c\uff0c\u8207\u7a20\u5bc6 FP32 \u7cbe\u5ea6\u6a21\u578b\u76f8\u6bd4\uff0c\u53ef\u4ee5\u5c07\u8a9e\u8a00\u6a21\u578b\u58d3\u7e2e\u81f3 16 \u500d\uff0c\u6548\u80fd\u640d\u5931\u537b\u6975\u5c0f\u3002AST \u7684\u8868\u73fe\u512a\u65bc\u5148\u524d\u7684\u6700\u5148\u9032\u65b9\u6cd5\uff0c\u5c07 Llama2-7B \u4e0a\u591a\u500b\u96f6\u6b21\u5b78\u7fd2\u4efb\u52d9\u4e2d\u7a20\u5bc6\u6a21\u578b\u548c\u534a\u7d50\u69cb\u7a00\u758f\u6a21\u578b\u4e4b\u9593\u7684\u96f6\u6b21\u5b78\u7fd2\u6e96\u78ba\u5ea6\u5dee\u8ddd\u7e2e\u5c0f\u81f3 1.12%\uff0c\u4f7f\u7528\u7684\u9810\u8a13\u7df4\u4ee3\u5e63\u4e0d\u5230 0.4%\u3002</paragraph>", "author": "Weiyu Huang et.al.", "authors": "Weiyu Huang, Guohao Jian, Yuezhou Hu, Jun Zhu, Jianfei Chen", "id": "2407.20584v1", "paper_url": "http://arxiv.org/abs/2407.20584v1", "repo": "null"}}