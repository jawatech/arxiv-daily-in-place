{"2408.04693": {"publish_time": "2024-08-08", "title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning", "paper_summary": "Due to the cost-prohibitive nature of training Large Language Models (LLMs),\nfine-tuning has emerged as an attractive alternative for specializing LLMs for\nspecific tasks using limited compute resources in a cost-effective manner. In\nthis paper, we characterize sparse Mixture of Experts (MoE) based LLM\nfine-tuning to understand their accuracy and runtime performance on a single\nGPU. Our evaluation provides unique insights into the training efficacy of\nsparse and dense versions of MoE models, as well as their runtime\ncharacteristics, including maximum batch size, execution time breakdown,\nend-to-end throughput, GPU hardware utilization, and load distribution. Our\nstudy identifies the optimization of the MoE layer as crucial for further\nimproving the performance of LLM fine-tuning. Using our profiling results, we\nalso develop and validate an analytical model to estimate the cost of LLM\nfine-tuning on the cloud. This model, based on parameters of the model and GPU\narchitecture, estimates LLM throughput and the cost of training, aiding\npractitioners in industry and academia to budget the cost of fine-tuning a\nspecific model.", "paper_summary_zh": "\u7531\u65bc\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6210\u672c\u904e\u9ad8\uff0c\u5fae\u8abf\u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u4ee5\u5229\u7528\u6709\u9650\u7684\u904b\u7b97\u8cc7\u6e90\u4ee5\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u65b9\u5f0f\u5c07 LLM \u5c08\u9580\u5316\u65bc\u7279\u5b9a\u4efb\u52d9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u7a00\u758f\u5c08\u5bb6\u6df7\u5408 (MoE) \u57fa\u65bc LLM \u7684\u5fae\u8abf\uff0c\u4ee5\u4e86\u89e3\u5b83\u5011\u5728\u55ae\u4e00 GPU \u4e0a\u7684\u6e96\u78ba\u5ea6\u548c\u57f7\u884c\u6642\u9593\u6548\u80fd\u3002\u6211\u5011\u7684\u8a55\u4f30\u63d0\u4f9b\u4e86\u5c0d\u7a00\u758f\u548c\u5bc6\u96c6\u7248\u672c\u7684 MoE \u6a21\u578b\u8a13\u7df4\u6548\u80fd\u7684\u7368\u7279\u898b\u89e3\uff0c\u4ee5\u53ca\u5b83\u5011\u7684\u57f7\u884c\u6642\u9593\u7279\u6027\uff0c\u5305\u62ec\u6700\u5927\u6279\u6b21\u5927\u5c0f\u3001\u57f7\u884c\u6642\u9593\u7d30\u76ee\u3001\u7aef\u5230\u7aef\u541e\u5410\u91cf\u3001GPU \u786c\u9ad4\u4f7f\u7528\u7387\u548c\u8ca0\u8f09\u5206\u4f48\u3002\u6211\u5011\u7684\u7814\u7a76\u767c\u73fe\uff0cMoE \u5c64\u7684\u6700\u4f73\u5316\u5c0d\u65bc\u9032\u4e00\u6b65\u63d0\u5347 LLM \u5fae\u8abf\u6548\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u5229\u7528\u6211\u5011\u7684\u5206\u6790\u7d50\u679c\uff0c\u6211\u5011\u9084\u958b\u767c\u4e26\u9a57\u8b49\u4e86\u4e00\u500b\u5206\u6790\u6a21\u578b\uff0c\u4ee5\u4f30\u8a08\u96f2\u7aef\u4e0a LLM \u5fae\u8abf\u7684\u6210\u672c\u3002\u6b64\u6a21\u578b\u57fa\u65bc\u6a21\u578b\u548c GPU \u67b6\u69cb\u7684\u53c3\u6578\uff0c\u4f30\u8a08 LLM \u541e\u5410\u91cf\u548c\u8a13\u7df4\u6210\u672c\uff0c\u5354\u52a9\u696d\u754c\u548c\u5b78\u8853\u754c\u7684\u5f9e\u696d\u4eba\u54e1\u9810\u7b97\u5fae\u8abf\u7279\u5b9a\u6a21\u578b\u7684\u6210\u672c\u3002", "author": "Yuchen Xia et.al.", "authors": "Yuchen Xia, Jiho Kim, Yuhan Chen, Haojie Ye, Souvik Kundu, Cong, Hao, Nishil Talati", "id": "2408.04693v1", "paper_url": "http://arxiv.org/abs/2408.04693v1", "repo": "https://github.com/stsxxx/finetune"}}