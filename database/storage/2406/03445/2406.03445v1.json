{"2406.03445": {"publish_time": "2024-06-05", "title": "Pre-trained Large Language Models Use Fourier Features to Compute Addition", "paper_summary": "Pre-trained large language models (LLMs) exhibit impressive mathematical\nreasoning capabilities, yet how they compute basic arithmetic, such as\naddition, remains unclear. This paper shows that pre-trained LLMs add numbers\nusing Fourier features -- dimensions in the hidden state that represent numbers\nvia a set of features sparse in the frequency domain. Within the model, MLP and\nattention layers use Fourier features in complementary ways: MLP layers\nprimarily approximate the magnitude of the answer using low-frequency features,\nwhile attention layers primarily perform modular addition (e.g., computing\nwhether the answer is even or odd) using high-frequency features. Pre-training\nis crucial for this mechanism: models trained from scratch to add numbers only\nexploit low-frequency features, leading to lower accuracy. Introducing\npre-trained token embeddings to a randomly initialized model rescues its\nperformance. Overall, our analysis demonstrates that appropriate pre-trained\nrepresentations (e.g., Fourier features) can unlock the ability of Transformers\nto learn precise mechanisms for algorithmic tasks.", "paper_summary_zh": "\u9810\u5148\u8a13\u7df4\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6578\u5b78\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u5982\u4f55\u8a08\u7b97\u57fa\u672c\u7b97\u8853\uff0c\u4f8b\u5982\u52a0\u6cd5\uff0c\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u672c\u6587\u986f\u793a\u9810\u5148\u8a13\u7df4\u7684 LLM \u4f7f\u7528\u5085\u7acb\u8449\u7279\u5fb5\u4f86\u52a0\u6578\u5b57\u2014\u2014\u96b1\u85cf\u72c0\u614b\u4e2d\u7684\u7dad\u5ea6\uff0c\u900f\u904e\u4e00\u7d44\u5728\u983b\u57df\u4e2d\u7a00\u758f\u7684\u7279\u5fb5\u4f86\u8868\u793a\u6578\u5b57\u3002\u5728\u6a21\u578b\u4e2d\uff0cMLP \u548c\u6ce8\u610f\u529b\u5c64\u4ee5\u4e92\u88dc\u7684\u65b9\u5f0f\u4f7f\u7528\u5085\u7acb\u8449\u7279\u5fb5\uff1aMLP \u5c64\u4e3b\u8981\u4f7f\u7528\u4f4e\u983b\u7387\u7279\u5fb5\u4f86\u8fd1\u4f3c\u7b54\u6848\u7684\u5e45\u5ea6\uff0c\u800c\u6ce8\u610f\u529b\u5c64\u4e3b\u8981\u4f7f\u7528\u9ad8\u983b\u7387\u7279\u5fb5\u4f86\u57f7\u884c\u6a21\u7d44\u52a0\u6cd5\uff08\u4f8b\u5982\uff0c\u8a08\u7b97\u7b54\u6848\u662f\u5076\u6578\u9084\u662f\u5947\u6578\uff09\u3002\u9810\u8a13\u7df4\u5c0d\u65bc\u9019\u500b\u6a5f\u5236\u81f3\u95dc\u91cd\u8981\uff1a\u5f9e\u982d\u8a13\u7df4\u7684\u6a21\u578b\u53ea\u80fd\u4f7f\u7528\u4f4e\u983b\u7387\u7279\u5fb5\u4f86\u52a0\u6578\u5b57\uff0c\u5c0e\u81f4\u6e96\u78ba\u5ea6\u8f03\u4f4e\u3002\u5c07\u9810\u5148\u8a13\u7df4\u7684\u7b26\u865f\u5d4c\u5165\u5f15\u5165\u96a8\u6a5f\u521d\u59cb\u5316\u7684\u6a21\u578b\u53ef\u4ee5\u633d\u6551\u5176\u6548\u80fd\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u7684\u5206\u6790\u8868\u660e\u9069\u7576\u7684\u9810\u5148\u8a13\u7df4\u8868\u793a\uff08\u4f8b\u5982\uff0c\u5085\u7acb\u8449\u7279\u5fb5\uff09\u53ef\u4ee5\u89e3\u9396 Transformer \u5b78\u7fd2\u6f14\u7b97\u6cd5\u4efb\u52d9\u7684\u7cbe\u78ba\u6a5f\u5236\u7684\u53ef\u80fd\u6027\u3002", "author": "Tianyi Zhou et.al.", "authors": "Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia", "id": "2406.03445v1", "paper_url": "http://arxiv.org/abs/2406.03445v1", "repo": "null"}}