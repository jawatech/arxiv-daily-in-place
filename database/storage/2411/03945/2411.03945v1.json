{"2411.03945": {"publish_time": "2024-11-06", "title": "Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks", "paper_summary": "In-Context Learning (ICL) is a phenomenon where task learning occurs through\na prompt sequence without the necessity of parameter updates. ICL in\nMulti-Headed Attention (MHA) with absolute positional embedding has been the\nfocus of more study than other sequence model varieties. We examine\nimplications of architectural differences between GPT-2 and LLaMa as well as\nLlaMa and Mamba. We extend work done by Garg et al. (2022) and Park et al.\n(2024) to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models - examining the\ninterplay between sequence transformation blocks and regressive performance\nin-context. We note that certain architectural changes cause degraded training\nefficiency/ICL accuracy by converging to suboptimal predictors or converging\nslower. We also find certain hybrids showing optimistic performance\nimprovements, informing potential future ICL-focused architecture\nmodifications. Additionally, we propose the \"ICL regression score\", a scalar\nmetric describing a model's whole performance on a specific task. Compute\nlimitations impose restrictions on our architecture-space, training duration,\nnumber of training runs, function class complexity, and benchmark complexity.\nTo foster reproducible and extensible research, we provide a typed, modular,\nand extensible Python package on which we run all experiments.", "paper_summary_zh": "\u60c5\u5883\u5b78\u7fd2 (ICL) \u662f\u4e00\u7a2e\u73fe\u8c61\uff0c\u5176\u4e2d\u4efb\u52d9\u5b78\u7fd2\u900f\u904e\u63d0\u793a\u5e8f\u5217\u767c\u751f\uff0c\u800c\u7121\u9700\u53c3\u6578\u66f4\u65b0\u3002\u5177\u6709\u7d55\u5c0d\u4f4d\u7f6e\u5d4c\u5165\u7684\u591a\u982d\u6ce8\u610f\u529b (MHA) \u4e2d\u7684 ICL \u4e00\u76f4\u6bd4\u5176\u4ed6\u5e8f\u5217\u6a21\u578b\u7a2e\u985e\u53d7\u5230\u66f4\u591a\u7814\u7a76\u7684\u95dc\u6ce8\u3002\u6211\u5011\u63a2\u8a0e\u4e86 GPT-2 \u548c LLaMa \u4e4b\u9593\u7684\u67b6\u69cb\u5dee\u7570\u4ee5\u53ca LLaMa \u548c Mamba \u4e4b\u9593\u7684\u67b6\u69cb\u5dee\u7570\u7684\u542b\u7fa9\u3002\u6211\u5011\u5c07 Garg \u7b49\u4eba (2022) \u548c Park \u7b49\u4eba (2024) \u6240\u505a\u7684\u5de5\u4f5c\u64f4\u5c55\u5230 GPT-2/LLaMa \u6df7\u5408\u6a21\u578b\u548c LLaMa/Mamba \u6df7\u5408\u6a21\u578b\uff0c\u63a2\u8a0e\u4e86\u5e8f\u5217\u8f49\u63db\u5340\u584a\u548c\u60c5\u5883\u4e2d\u56de\u6b78\u6548\u80fd\u4e4b\u9593\u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u6211\u5011\u6ce8\u610f\u5230\u67d0\u4e9b\u67b6\u69cb\u8b8a\u66f4\u6703\u5c0e\u81f4\u8a13\u7df4\u6548\u7387/ICL \u6e96\u78ba\u5ea6\u4e0b\u964d\uff0c\u9019\u662f\u56e0\u70ba\u5b83\u5011\u6536\u6582\u5230\u6b21\u4f73\u9810\u6e2c\u5668\u6216\u6536\u6582\u901f\u5ea6\u8f03\u6162\u3002\u6211\u5011\u9084\u767c\u73fe\u67d0\u4e9b\u6df7\u5408\u6a21\u578b\u986f\u793a\u6a02\u89c0\u7684\u6548\u80fd\u6539\u5584\uff0c\u9019\u70ba\u6f5b\u5728\u7684\u672a\u4f86\u4ee5 ICL \u70ba\u91cd\u9ede\u7684\u67b6\u69cb\u4fee\u6539\u63d0\u4f9b\u4e86\u8cc7\u8a0a\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u300cICL \u56de\u6b78\u5206\u6578\u300d\uff0c\u9019\u662f\u4e00\u500b\u63cf\u8ff0\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52d9\u4e0a\u7684\u6574\u9ad4\u6548\u80fd\u7684\u6a19\u91cf\u6307\u6a19\u3002\u904b\u7b97\u9650\u5236\u5c0d\u6211\u5011\u7684\u67b6\u69cb\u7a7a\u9593\u3001\u8a13\u7df4\u6301\u7e8c\u6642\u9593\u3001\u8a13\u7df4\u57f7\u884c\u6b21\u6578\u3001\u51fd\u6578\u985e\u5225\u8907\u96dc\u5ea6\u548c\u57fa\u6e96\u8907\u96dc\u5ea6\u65bd\u52a0\u4e86\u9650\u5236\u3002\u70ba\u4e86\u4fc3\u9032\u53ef\u8907\u88fd\u4e14\u53ef\u64f4\u5145\u7684\u7814\u7a76\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u985e\u578b\u5316\u3001\u6a21\u7d44\u5316\u4e14\u53ef\u64f4\u5145\u7684 Python \u5957\u4ef6\uff0c\u6211\u5011\u5728\u5176\u4e2d\u57f7\u884c\u6240\u6709\u5be6\u9a57\u3002", "author": "Ryan Campbell et.al.", "authors": "Ryan Campbell, Nelson Lojo, Kesava Viswanadha, Christoffer Grondal Tryggestad, Derrick Han Sun, Sriteja Vijapurapu, August Rolfsen, Anant Sahai", "id": "2411.03945v1", "paper_url": "http://arxiv.org/abs/2411.03945v1", "repo": "null"}}