{"2412.08109": {"publish_time": "2024-12-11", "title": "Unseen Horizons: Unveiling the Real Capability of LLM Code Generation Beyond the Familiar", "paper_summary": "Recently, large language models (LLMs) have shown strong potential in code\ngeneration tasks. However, there are still gaps before they can be fully\napplied in actual software development processes. Accurately assessing the code\ngeneration capabilities of large language models has become an important basis\nfor evaluating and improving the models. Some existing works have constructed\ndatasets to evaluate the capabilities of these models. However, the current\nevaluation process may encounter the illusion of \"Specialist in Familiarity\",\nprimarily due to three gaps: the exposure of target code, case timeliness, and\ndependency availability. The fundamental reason for these gaps is that the code\nin current datasets may have been extensively exposed and exercised during the\ntraining phase, and due to the continuous training and development of LLM,\ntheir timeliness has been severely compromised. The key to solve the problem is\nto, as much as possible, evaluate the LLMs using code that they have not\nencountered before. Thus, the fundamental idea in this paper is to draw on the\nconcept of code obfuscation, changing code at different levels while ensuring\nthe functionality and output. To this end, we build a code-obfuscation based\nbenchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world\nprojects, including function description and code. Then we use three-level\nstrategy (symbol, structure and semantic) to obfuscate descriptions, code and\ncontext dependencies. We evaluate four LLMs on OBFU- SEVAL and compared the\neffectiveness of different obfuscation strategy. We use official test suites of\nthese projects to evaluate the generated code. The results show that after\nobfuscation, the average decrease ratio of test pass rate can up to 62.5%.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5728\u5b83\u4eec\u80fd\u5b8c\u5168\u5e94\u7528\u4e8e\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u4e4b\u524d\uff0c\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002\u51c6\u786e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5df2\u6210\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u7684\u91cd\u8981\u57fa\u7840\u3002\u4e00\u4e9b\u73b0\u6709\u5de5\u4f5c\u5df2\u7ecf\u6784\u5efa\u4e86\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u8bc4\u4f30\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u9047\u5230\u201c\u719f\u80fd\u751f\u5de7\u201d\u7684\u9519\u89c9\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u4e09\u4e2a\u5dee\u8ddd\uff1a\u76ee\u6807\u4ee3\u7801\u7684\u66b4\u9732\u3001\u6848\u4f8b\u65f6\u6548\u6027\u548c\u4f9d\u8d56\u6027\u53ef\u7528\u6027\u3002\u8fd9\u4e9b\u5dee\u8ddd\u7684\u6839\u672c\u539f\u56e0\u5728\u4e8e\uff0c\u5f53\u524d\u6570\u636e\u96c6\u4e2d\u7684\u4ee3\u7801\u53ef\u80fd\u5728\u8bad\u7ec3\u9636\u6bb5\u5df2\u7ecf\u5f97\u5230\u5e7f\u6cdb\u7684\u66b4\u9732\u548c\u7ec3\u4e60\uff0c\u5e76\u4e14\u7531\u4e8e LLM \u7684\u6301\u7eed\u8bad\u7ec3\u548c\u5f00\u53d1\uff0c\u5b83\u4eec\u7684\u65f6\u95f4\u6027\u53d7\u5230\u4e86\u4e25\u91cd\u635f\u5bb3\u3002\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u5173\u952e\u662f\u5c3d\u53ef\u80fd\u4f7f\u7528 LLM \u4e4b\u524d\u672a\u9047\u5230\u8fc7\u7684\u4ee3\u7801\u6765\u8bc4\u4f30 LLM\u3002\u56e0\u6b64\uff0c\u672c\u6587\u7684\u57fa\u672c\u601d\u60f3\u662f\u501f\u9274\u4ee3\u7801\u6df7\u6dc6\u7684\u6982\u5ff5\uff0c\u5728\u786e\u4fdd\u529f\u80fd\u548c\u8f93\u51fa\u7684\u540c\u65f6\u6539\u53d8\u4e0d\u540c\u7ea7\u522b\u7684\u4ee3\u7801\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7801\u6df7\u6dc6\u7684\u57fa\u51c6 OBFUSEVAL\u3002\u6211\u4eec\u9996\u5148\u4ece\u4e94\u4e2a\u771f\u5b9e\u9879\u76ee\u4e2d\u6536\u96c6\u4e86 1,354 \u4e2a\u539f\u59cb\u6848\u4f8b\uff0c\u5305\u62ec\u529f\u80fd\u63cf\u8ff0\u548c\u4ee3\u7801\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u4e09\u7ea7\u7b56\u7565\uff08\u7b26\u53f7\u3001\u7ed3\u6784\u548c\u8bed\u4e49\uff09\u6765\u6df7\u6dc6\u63cf\u8ff0\u3001\u4ee3\u7801\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u3002\u6211\u4eec\u5728 OBFU-SEVAL \u4e0a\u8bc4\u4f30\u4e86\u56db\u4e2a LLM\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6df7\u6dc6\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u9879\u76ee\u7684\u5b98\u65b9\u6d4b\u8bd5\u5957\u4ef6\u6765\u8bc4\u4f30\u751f\u6210\u7684\u4ee3\u7801\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6df7\u6dc6\u540e\uff0c\u6d4b\u8bd5\u901a\u8fc7\u7387\u7684\u5e73\u5747\u4e0b\u964d\u7387\u53ef\u8fbe 62.5%\u3002</paragraph>", "author": "Yuanliang Zhang et.al.", "authors": "Yuanliang Zhang, Yifan Xie, Shanshan Li, Ke Liu, Chong Wang, Zhouyang Jia, Xiangbing Huang, Jie Song, Chaopeng Luo, Zhizheng Zheng, Rulin Xu, Yitong Liu, Si Zheng, Xiangke Liao", "id": "2412.08109v1", "paper_url": "http://arxiv.org/abs/2412.08109v1", "repo": "null"}}