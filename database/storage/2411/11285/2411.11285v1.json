{"2411.11285": {"publish_time": "2024-11-18", "title": "Zero-Shot Automatic Annotation and Instance Segmentation using LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for Deep Learning Model Development", "paper_summary": "Currently, deep learning-based instance segmentation for various applications\n(e.g., Agriculture) is predominantly performed using a labor-intensive process\ninvolving extensive field data collection using sophisticated sensors, followed\nby careful manual annotation of images, presenting significant logistical and\nfinancial challenges to researchers and organizations. The process also slows\ndown the model development and training process. In this study, we presented a\nnovel method for deep learning-based instance segmentation of apples in\ncommercial orchards that eliminates the need for labor-intensive field data\ncollection and manual annotation. Utilizing a Large Language Model (LLM), we\nsynthetically generated orchard images and automatically annotated them using\nthe Segment Anything Model (SAM) integrated with a YOLO11 base model. This\nmethod significantly reduces reliance on physical sensors and manual data\nprocessing, presenting a major advancement in \"Agricultural AI\". The synthetic,\nauto-annotated dataset was used to train the YOLO11 model for Apple instance\nsegmentation, which was then validated on real orchard images. The results\nshowed that the automatically generated annotations achieved a Dice Coefficient\nof 0.9513 and an IoU of 0.9303, validating the accuracy and overlap of the mask\nannotations. All YOLO11 configurations, trained solely on these synthetic\ndatasets with automated annotations, accurately recognized and delineated\napples, highlighting the method's efficacy. Specifically, the YOLO11m-seg\nconfiguration achieved a mask precision of 0.902 and a mask mAP@50 of 0.833 on\ntest images collected from a commercial orchard. Additionally, the YOLO11l-seg\nconfiguration outperformed other models in validation on 40 LLM-generated\nimages, achieving the highest mask precision and mAP@50 metrics.\n  Keywords: YOLO, SAM, SAMv2, YOLO11, YOLOv11, Segment Anything, YOLO-SAM", "paper_summary_zh": "<paragraph>\u76ee\u524d\uff0c\u91dd\u5c0d\u5404\u7a2e\u61c9\u7528\uff08\u4f8b\u5982\u8fb2\u696d\uff09\u7684\u6df1\u5ea6\u5b78\u7fd2\u5be6\u4f8b\u5206\u5272\uff0c\u4e3b\u8981\u900f\u904e\u52de\u529b\u5bc6\u96c6\u7684\u7a0b\u5e8f\u57f7\u884c\uff0c\u5305\u62ec\u4f7f\u7528\u7cbe\u5bc6\u611f\u6e2c\u5668\u5ee3\u6cdb\u6536\u96c6\u73fe\u5834\u8cc7\u6599\uff0c\u63a5\u8457\u4ed4\u7d30\u624b\u52d5\u6a19\u8a3b\u5f71\u50cf\uff0c\u5c0d\u7814\u7a76\u4eba\u54e1\u548c\u7d44\u7e54\u800c\u8a00\uff0c\u9019\u6703\u9020\u6210\u986f\u8457\u7684\u5f8c\u52e4\u548c\u8ca1\u52d9\u6311\u6230\u3002\u6b64\u7a0b\u5e8f\u4e5f\u6703\u6e1b\u7de9\u6a21\u578b\u958b\u767c\u548c\u8a13\u7df4\u7684\u904e\u7a0b\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u53ef\u91dd\u5c0d\u5546\u696d\u679c\u5712\u4e2d\u7684\u860b\u679c\u57f7\u884c\u6df1\u5ea6\u5b78\u7fd2\u5be6\u4f8b\u5206\u5272\uff0c\u7121\u9700\u52de\u529b\u5bc6\u96c6\u7684\u73fe\u5834\u8cc7\u6599\u6536\u96c6\u548c\u624b\u52d5\u6a19\u8a3b\u3002\u6211\u5011\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5408\u6210\u7522\u751f\u679c\u5712\u5f71\u50cf\uff0c\u4e26\u4f7f\u7528\u8207 YOLO11 \u57fa\u790e\u6a21\u578b\u6574\u5408\u7684 Segment Anything Model (SAM) \u81ea\u52d5\u6a19\u8a3b\u9019\u4e9b\u5f71\u50cf\u3002\u9019\u7a2e\u65b9\u6cd5\u5927\u5e45\u964d\u4f4e\u5c0d\u5be6\u9ad4\u611f\u6e2c\u5668\u548c\u624b\u52d5\u8cc7\u6599\u8655\u7406\u7684\u4f9d\u8cf4\u6027\uff0c\u4ee3\u8868\u300c\u8fb2\u696d AI\u300d\u7684\u4e00\u5927\u9032\u6b65\u3002\u5408\u6210\u81ea\u52d5\u6a19\u8a3b\u7684\u8cc7\u6599\u96c6\u7528\u65bc\u8a13\u7df4 YOLO11 \u6a21\u578b\uff0c\u4ee5\u9032\u884c\u860b\u679c\u5be6\u4f8b\u5206\u5272\uff0c\u63a5\u8457\u5728\u771f\u5be6\u679c\u5712\u5f71\u50cf\u4e2d\u9a57\u8b49\u3002\u7d50\u679c\u986f\u793a\uff0c\u81ea\u52d5\u7522\u751f\u7684\u6a19\u8a3b\u9054\u5230\u4e86 0.9513 \u7684 Dice \u4fc2\u6578\u548c 0.9303 \u7684 IoU\uff0c\u9a57\u8b49\u4e86\u906e\u7f69\u6a19\u8a3b\u7684\u6e96\u78ba\u6027\u548c\u91cd\u758a\u6027\u3002\u6240\u6709 YOLO11 \u7d44\u614b\u50c5\u4f7f\u7528\u9019\u4e9b\u5177\u6709\u81ea\u52d5\u5316\u6a19\u8a3b\u7684\u5408\u6210\u8cc7\u6599\u96c6\u9032\u884c\u8a13\u7df4\uff0c\u5c31\u80fd\u6e96\u78ba\u8fa8\u8b58\u548c\u63cf\u7e6a\u860b\u679c\uff0c\u7a81\u986f\u4e86\u6b64\u65b9\u6cd5\u7684\u6548\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cYOLO11m-seg \u7d44\u614b\u5728\u5f9e\u5546\u696d\u679c\u5712\u6536\u96c6\u7684\u6e2c\u8a66\u5f71\u50cf\u4e0a\u9054\u5230\u4e86 0.902 \u7684\u906e\u7f69\u6e96\u78ba\u5ea6\u548c 0.833 \u7684\u906e\u7f69 mAP@50\u3002\u6b64\u5916\uff0cYOLO11l-seg \u7d44\u614b\u5728\u91dd\u5c0d 40 \u5f35 LLM \u751f\u6210\u7684\u5f71\u50cf\u9032\u884c\u9a57\u8b49\u6642\uff0c\u512a\u65bc\u5176\u4ed6\u6a21\u578b\uff0c\u9054\u5230\u4e86\u6700\u9ad8\u7684\u906e\u7f69\u6e96\u78ba\u5ea6\u548c mAP@50 \u6307\u6a19\u3002\n\u95dc\u9375\u5b57\uff1aYOLO\u3001SAM\u3001SAMv2\u3001YOLO11\u3001YOLOv11\u3001Segment Anything\u3001YOLO-SAM</paragraph>", "author": "Ranjan Sapkota et.al.", "authors": "Ranjan Sapkota, Achyut Paudel, Manoj Karkee", "id": "2411.11285v1", "paper_url": "http://arxiv.org/abs/2411.11285v1", "repo": "null"}}