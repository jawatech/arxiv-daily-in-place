{"2407.17226": {"publish_time": "2024-07-24", "title": "Sublinear Regret for An Actor-Critic Algorithm in Continuous-Time Linear-Quadratic Reinforcement Learning", "paper_summary": "We study reinforcement learning (RL) for a class of continuous-time\nlinear-quadratic (LQ) control problems for diffusions where volatility of the\nstate processes depends on both state and control variables. We apply a\nmodel-free approach that relies neither on knowledge of model parameters nor on\ntheir estimations, and devise an actor-critic algorithm to learn the optimal\npolicy parameter directly. Our main contributions include the introduction of a\nnovel exploration schedule and a regret analysis of the proposed algorithm. We\nprovide the convergence rate of the policy parameter to the optimal one, and\nprove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to\na logarithmic factor. We conduct a simulation study to validate the theoretical\nresults and demonstrate the effectiveness and reliability of the proposed\nalgorithm. We also perform numerical comparisons between our method and those\nof the recent model-based stochastic LQ RL studies adapted to the state- and\ncontrol-dependent volatility setting, demonstrating a better performance of the\nformer in terms of regret bounds.", "paper_summary_zh": "\u6211\u5011\u7814\u7a76\u4e00\u500b\u9023\u7e8c\u6642\u9593\u7dda\u6027\u4e8c\u6b21 (LQ) \u63a7\u5236\u554f\u984c\u7684\u5f37\u5316\u5b78\u7fd2 (RL)\uff0c\u5176\u64f4\u6563\u72c0\u614b\u904e\u7a0b\u7684\u6ce2\u52d5\u6027\u53d6\u6c7a\u65bc\u72c0\u614b\u548c\u63a7\u5236\u8b8a\u6578\u3002\u6211\u5011\u63a1\u7528\u4e00\u7a2e\u7121\u6a21\u578b\u65b9\u6cd5\uff0c\u9019\u7a2e\u65b9\u6cd5\u65e2\u4e0d\u4f9d\u8cf4\u6a21\u578b\u53c3\u6578\u7684\u77e5\u8b58\uff0c\u4e5f\u4e0d\u4f9d\u8cf4\u65bc\u5c0d\u6a21\u578b\u53c3\u6578\u7684\u4f30\u8a08\uff0c\u4e26\u8a2d\u8a08\u4e86\u4e00\u500b\u52d5\u4f5c-\u8a55\u8ad6\u6f14\u7b97\u6cd5\u4f86\u76f4\u63a5\u5b78\u7fd2\u6700\u4f73\u7b56\u7565\u53c3\u6578\u3002\u6211\u5011\u7684\u8ca2\u737b\u5305\u62ec\u5f15\u5165\u4e00\u500b\u65b0\u7684\u63a2\u7d22\u6642\u9593\u8868\u548c\u5c0d\u6240\u63d0\u51fa\u6f14\u7b97\u6cd5\u7684\u907a\u61be\u5206\u6790\u3002\u6211\u5011\u63d0\u4f9b\u4e86\u7b56\u7565\u53c3\u6578\u6536\u6582\u5230\u6700\u4f73\u7b56\u7565\u53c3\u6578\u7684\u901f\u7387\uff0c\u4e26\u8b49\u660e\u8a72\u6f14\u7b97\u6cd5\u9054\u5230\u4e86 $O(N^{\\frac{3}{4}})$ \u7684\u907a\u61be\u754c\u9650\uff0c\u8aa4\u5dee\u5c0f\u65bc\u5c0d\u6578\u56e0\u5b50\u3002\u6211\u5011\u9032\u884c\u4e86\u4e00\u9805\u6a21\u64ec\u7814\u7a76\uff0c\u4ee5\u9a57\u8b49\u7406\u8ad6\u7d50\u679c\uff0c\u4e26\u8b49\u660e\u6240\u63d0\u51fa\u6f14\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002\u6211\u5011\u9084\u5c0d\u6211\u5011\u7684\u65b9\u6cd5\u8207\u6700\u8fd1\u57fa\u65bc\u6a21\u578b\u7684\u96a8\u6a5f LQ RL \u7814\u7a76\uff08\u5df2\u9069\u61c9\u72c0\u614b\u548c\u63a7\u5236\u4f9d\u8cf4\u6ce2\u52d5\u6027\u8a2d\u5b9a\uff09\u9032\u884c\u4e86\u6578\u503c\u6bd4\u8f03\uff0c\u8b49\u660e\u4e86\u524d\u8005\u5728\u907a\u61be\u754c\u9650\u65b9\u9762\u7684\u6548\u80fd\u8f03\u4f73\u3002", "author": "Yilie Huang et.al.", "authors": "Yilie Huang, Yanwei Jia, Xun Yu Zhou", "id": "2407.17226v1", "paper_url": "http://arxiv.org/abs/2407.17226v1", "repo": "null"}}