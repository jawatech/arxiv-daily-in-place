{"2410.22257": {"publish_time": "2024-10-29", "title": "FactBench: A Dynamic Benchmark for In-the-Wild Language Model Factuality Evaluation", "paper_summary": "Language models (LMs) are widely used by an increasing number of users,\nunderscoring the challenge of maintaining factuality across a broad range of\ntopics. We first present VERIFY (Verification and Evidence RetrIeval for\nFactualitY evaluation), a pipeline to evaluate LMs' factuality in real-world\nuser interactions. VERIFY considers the verifiability of LM-generated content\nand categorizes content units as supported, unsupported, or undecidable based\non the retrieved evidence from the Web. Importantly, factuality judgment by\nVERIFY correlates better with human evaluations than existing methods. Using\nVERIFY, we identify \"hallucination prompts\" across diverse topics, i.e., those\neliciting the highest rates of incorrect and inconclusive LM responses. These\nprompts form FactBench, a dataset of 1K prompts across 150 fine-grained topics.\nOur dataset captures emerging factuality challenges in real-world LM\ninteractions and can be regularly updated with new prompts. We benchmark\nwidely-used LMs from GPT, Gemini, and Llama3.1 family on FactBench, yielding\nthe following key findings: (i) Proprietary models exhibit better factuality,\nwith performance declining from Easy to Hard hallucination prompts. (ii)\nLlama3.1-405B-Instruct shows comparable or lower factual accuracy than\nLlama3.1-70B-Instruct across all evaluation methods due to its higher\nsubjectivity that leads to more content labeled as undecidable. (iii)\nGemini1.5-Pro shows a significantly higher refusal rate, with over-refusal in\n25% of cases. Our code and data are publicly available at\nhttps://huggingface.co/spaces/launch/factbench.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b (LM) \u88ab\u8d8a\u4f86\u8d8a\u591a\u7684\u4f7f\u7528\u8005\u5ee3\u6cdb\u4f7f\u7528\uff0c\u9019\u7a81\u986f\u4e86\u5728\u5ee3\u6cdb\u4e3b\u984c\u4e2d\u7dad\u8b77\u4e8b\u5be6\u6027\u7684\u6311\u6230\u3002\u6211\u5011\u9996\u5148\u63d0\u51fa VERIFY\uff08\u4e8b\u5be6\u6027\u8a55\u4f30\u7684\u9a57\u8b49\u548c\u8b49\u64da\u6aa2\u7d22\uff09\uff0c\u4e00\u500b\u7528\u65bc\u8a55\u4f30 LM \u5728\u771f\u5be6\u4e16\u754c\u4f7f\u7528\u8005\u4e92\u52d5\u4e2d\u7684\u4e8b\u5be6\u6027\u7684\u7ba1\u9053\u3002VERIFY \u8003\u616e\u4e86 LM \u751f\u6210\u7684\u5167\u5bb9\u7684\u53ef\u9a57\u8b49\u6027\uff0c\u4e26\u6839\u64da\u5f9e\u7db2\u8def\u4e2d\u6aa2\u7d22\u5230\u7684\u8b49\u64da\u5c07\u5167\u5bb9\u55ae\u5143\u5206\u985e\u70ba\u53d7\u652f\u6301\u3001\u4e0d\u53d7\u652f\u6301\u6216\u7121\u6cd5\u5224\u5b9a\u3002\u91cd\u8981\u7684\u662f\uff0cVERIFY \u7684\u4e8b\u5be6\u6027\u5224\u65b7\u6bd4\u73fe\u6709\u65b9\u6cd5\u8207\u4eba\u985e\u8a55\u4f30\u66f4\u76f8\u95dc\u3002\u4f7f\u7528 VERIFY\uff0c\u6211\u5011\u8b58\u5225\u51fa\u5404\u7a2e\u4e3b\u984c\u7684\u300c\u5e7b\u89ba\u63d0\u793a\u300d\uff0c\u5373\u5f15\u767c LM \u7522\u751f\u6700\u9ad8\u932f\u8aa4\u7387\u548c\u4e0d\u78ba\u5b9a\u56de\u61c9\u7684\u63d0\u793a\u3002\u9019\u4e9b\u63d0\u793a\u5f62\u6210\u4e86 FactBench\uff0c\u4e00\u500b\u5305\u542b 150 \u500b\u7d30\u7c92\u5ea6\u4e3b\u984c\u7684 1K \u63d0\u793a\u7684\u8cc7\u6599\u96c6\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u6355\u6349\u4e86\u771f\u5be6\u4e16\u754c LM \u4e92\u52d5\u4e2d\u65b0\u51fa\u73fe\u7684\u4e8b\u5be6\u6027\u6311\u6230\uff0c\u4e26\u4e14\u53ef\u4ee5\u5b9a\u671f\u4f7f\u7528\u65b0\u63d0\u793a\u66f4\u65b0\u3002\u6211\u5011\u5728 FactBench \u4e0a\u5c0d\u4f86\u81ea GPT\u3001Gemini \u548c Llama3.1 \u5bb6\u65cf\u7684\u5ee3\u6cdb\u4f7f\u7528\u7684 LM \u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u5f97\u51fa\u4ee5\u4e0b\u95dc\u9375\u767c\u73fe\uff1a(i) \u5c08\u6709\u6a21\u578b\u8868\u73fe\u51fa\u66f4\u597d\u7684\u4e8b\u5be6\u6027\uff0c\u6027\u80fd\u5f9e\u5bb9\u6613\u5230\u56f0\u96e3\u7684\u5e7b\u89ba\u63d0\u793a\u4e0b\u964d\u3002(ii) Llama3.1-405B-Instruct \u5728\u6240\u6709\u8a55\u4f30\u65b9\u6cd5\u4e2d\u986f\u793a\u51fa\u8207 Llama3.1-70B-Instruct \u76f8\u7576\u6216\u66f4\u4f4e\u7684\u4e8b\u5be6\u6e96\u78ba\u6027\uff0c\u56e0\u70ba\u5b83\u7684\u4e3b\u89c0\u6027\u8f03\u9ad8\uff0c\u5c0e\u81f4\u66f4\u591a\u5167\u5bb9\u88ab\u6a19\u8a18\u70ba\u7121\u6cd5\u5224\u5b9a\u3002(iii) Gemini1.5-Pro \u986f\u793a\u51fa\u986f\u8457\u66f4\u9ad8\u7684\u62d2\u7d55\u7387\uff0c\u5728 25% \u7684\u60c5\u6cc1\u4e0b\u904e\u5ea6\u62d2\u7d55\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728 https://huggingface.co/spaces/launch/factbench \u516c\u958b\u53d6\u5f97\u3002", "author": "Farima Fatahi Bayat et.al.", "authors": "Farima Fatahi Bayat, Lechen Zhang, Sheza Munir, Lu Wang", "id": "2410.22257v1", "paper_url": "http://arxiv.org/abs/2410.22257v1", "repo": "null"}}