{"2502.15224": {"publish_time": "2025-02-21", "title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs", "paper_summary": "Given the remarkable performance of Large Language Models (LLMs), an\nimportant question arises: Can LLMs conduct human-like scientific research and\ndiscover new knowledge, and act as an AI scientist? Scientific discovery is an\niterative process that demands efficient knowledge updating and encoding. It\ninvolves understanding the environment, identifying new hypotheses, and\nreasoning about actions; however, no standardized benchmark specifically\ndesigned for scientific discovery exists for LLM agents. In response to these\nlimitations, we introduce a novel benchmark, \\textit{Auto-Bench}, that\nencompasses necessary aspects to evaluate LLMs for scientific discovery in both\nnatural and social sciences. Our benchmark is based on the principles of causal\ngraph discovery. It challenges models to uncover hidden structures and make\noptimal decisions, which includes generating valid justifications. By engaging\ninteractively with an oracle, the models iteratively refine their understanding\nof underlying interactions, the chemistry and social interactions, through\nstrategic interventions. We evaluate state-of-the-art LLMs, including GPT-4,\nGemini, Qwen, Claude, and Llama, and observe a significant performance drop as\nthe problem complexity increases, which suggests an important gap between\nmachine and human intelligence that future development of LLMs need to take\ninto consideration.", "paper_summary_zh": "\u9274\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5353\u8d8a\u6027\u80fd\uff0c\u4e00\u4e2a\u91cd\u8981\u7684\u95ee\u9898\u51fa\u73b0\u4e86\uff1aLLM \u80fd\u5426\u8fdb\u884c\u7c7b\u4eba\u79d1\u5b66\u7814\u7a76\u5e76\u53d1\u73b0\u65b0\u77e5\u8bc6\uff0c\u5e76\u5145\u5f53\u4eba\u5de5\u667a\u80fd\u79d1\u5b66\u5bb6\uff1f\u79d1\u5b66\u53d1\u73b0\u662f\u4e00\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u9700\u8981\u9ad8\u6548\u7684\u77e5\u8bc6\u66f4\u65b0\u548c\u7f16\u7801\u3002\u5b83\u6d89\u53ca\u7406\u89e3\u73af\u5883\u3001\u8bc6\u522b\u65b0\u5047\u8bbe\u548c\u63a8\u7406\u884c\u4e3a\uff1b\u7136\u800c\uff0c\u76ee\u524d\u4e0d\u5b58\u5728\u4e13\u95e8\u4e3a\u79d1\u5b66\u53d1\u73b0\u8bbe\u8ba1\u7684\u6807\u51c6\u57fa\u51c6\uff0c\u9002\u7528\u4e8e LLM \u4ee3\u7406\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\\textit{Auto-Bench}\uff0c\u5b83\u5305\u542b\u4e86\u8bc4\u4f30 LLM \u5728\u81ea\u7136\u79d1\u5b66\u548c\u793e\u4f1a\u79d1\u5b66\u4e2d\u8fdb\u884c\u79d1\u5b66\u53d1\u73b0\u6240\u9700\u7684\u65b9\u9762\u3002\u6211\u4eec\u7684\u57fa\u51c6\u57fa\u4e8e\u56e0\u679c\u56fe\u53d1\u73b0\u7684\u539f\u7406\u3002\u5b83\u6311\u6218\u6a21\u578b\u53bb\u53d1\u73b0\u9690\u85cf\u7684\u7ed3\u6784\u5e76\u505a\u51fa\u6700\u4f73\u51b3\u7b56\uff0c\u5176\u4e2d\u5305\u62ec\u751f\u6210\u6709\u6548\u7684\u8bc1\u660e\u3002\u901a\u8fc7\u4e0e\u795e\u8c15\u4ea4\u4e92\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u8fc7\u6218\u7565\u5e72\u9884\u8fed\u4ee3\u5730\u5b8c\u5584\u4e86\u5b83\u4eec\u5bf9\u5e95\u5c42\u4ea4\u4e92\u3001\u5316\u5b66\u548c\u793e\u4f1a\u4ea4\u4e92\u7684\u7406\u89e3\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684 LLM\uff0c\u5305\u62ec GPT-4\u3001Gemini\u3001Qwen\u3001Claude \u548c Llama\uff0c\u5e76\u89c2\u5bdf\u5230\u968f\u7740\u95ee\u9898\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u8fd9\u8868\u660e\u673a\u5668\u667a\u80fd\u548c\u4eba\u7c7b\u667a\u80fd\u4e4b\u95f4\u5b58\u5728\u4e00\u4e2a\u91cd\u8981\u7684\u5dee\u8ddd\uff0c\u672a\u6765 LLM \u7684\u53d1\u5c55\u9700\u8981\u8003\u8651\u8fd9\u4e00\u70b9\u3002", "author": "Tingting Chen et.al.", "authors": "Tingting Chen, Srinivas Anumasa, Beibei Lin, Vedant Shah, Anirudh Goyal, Dianbo Liu", "id": "2502.15224v1", "paper_url": "http://arxiv.org/abs/2502.15224v1", "repo": "null"}}