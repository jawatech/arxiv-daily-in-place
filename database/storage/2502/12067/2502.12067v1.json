{"2502.12067": {"publish_time": "2025-02-17", "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs", "paper_summary": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.", "paper_summary_zh": "<paragraph>\u93c8\u5f0f\u601d\u7dad (CoT) \u5df2\u88ab\u8b49\u660e\u80fd\u6709\u6548\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u80fd\u529b\u3002\u6700\u8fd1\u7684\u9032\u5c55\uff0c\u4f8b\u5982 OpenAI \u7684 o1 \u548c DeepSeek-R1\uff0c\u8868\u660e\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u64f4\u5c55 CoT \u5e8f\u5217\u7684\u9577\u5ea6\u53ef\u4ee5\u9032\u4e00\u6b65\u63d0\u5347 LLM \u7684\u63a8\u7406\u6548\u80fd\u3002\u7136\u800c\uff0c\u7531\u65bc LLM \u89e3\u78bc\u7684\u81ea\u52d5\u56de\u6b78\u7279\u6027\uff0c\u8f03\u9577\u7684 CoT \u8f38\u51fa\u6703\u5c0e\u81f4\u63a8\u7406\u5ef6\u9072\u7dda\u6027\u589e\u52a0\uff0c\u5c0d\u4f7f\u7528\u8005\u9ad4\u9a57\u9020\u6210\u8ca0\u9762\u5f71\u97ff\uff0c\u7279\u5225\u662f\u5728 CoT \u8d85\u904e 10,000 \u500b\u7b26\u865f\u6642\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5206\u6790\u4e86 CoT \u8f38\u51fa\u4e2d\u7b26\u865f\u7684\u8a9e\u7fa9\u91cd\u8981\u6027\uff0c\u4e26\u63ed\u793a\u4e86\u5b83\u5011\u5c0d\u63a8\u7406\u7684\u8ca2\u737b\u5ea6\u4e0d\u540c\u3002\u57fa\u65bc\u9019\u500b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86 TokenSkip\uff0c\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u6280\u8853\uff0c\u4f7f LLM \u80fd\u6709\u9078\u64c7\u5730\u7565\u904e\u8f03\u4e0d\u91cd\u8981\u7684\u7b26\u865f\uff0c\u5f9e\u800c\u5be6\u73fe\u53ef\u63a7\u7684 CoT \u58d3\u7e2e\u3002\u8de8\u8d8a\u5404\u7a2e\u6a21\u578b\u548c\u4efb\u52d9\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 TokenSkip \u5728\u6e1b\u5c11 CoT \u7b26\u865f\u4f7f\u7528\u91cf\u540c\u6642\u4fdd\u6301\u5f37\u5927\u63a8\u7406\u6548\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7576\u61c9\u7528\u65bc Qwen2.5-14B-Instruct \u6642\uff0cTokenSkip \u5c07 GSM8K \u4e0a\u7684\u63a8\u7406\u7b26\u865f\u6e1b\u5c11\u4e86 40%\uff08\u5f9e 313 \u500b\u6e1b\u5c11\u5230 181 \u500b\uff09\uff0c\u6548\u80fd\u4e0b\u964d\u4e0d\u5230 0.4%\u3002</paragraph>", "author": "Heming Xia et.al.", "authors": "Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, Wenjie Li", "id": "2502.12067v1", "paper_url": "http://arxiv.org/abs/2502.12067v1", "repo": "null"}}