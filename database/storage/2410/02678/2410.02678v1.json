{"2410.02678": {"publish_time": "2024-10-03", "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data", "paper_summary": "Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute.", "paper_summary_zh": "\u8a9e\u97f3\u52a9\u7406\uff0c\u4f8b\u5982 Siri \u548c Google Assistant\uff0c\u901a\u5e38\u6703\u5206\u5225\u5efa\u6a21\u97f3\u8a0a\u548c\u6587\u5b57\uff0c\u5c0e\u81f4\u8a9e\u97f3\u8cc7\u8a0a\u907a\u5931\u548c\u8907\u96dc\u6027\u589e\u52a0\u3002\u6700\u8fd1\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u4f7f\u7528\u76e3\u7763\u5fae\u8abf (SFT) \u8a13\u7df4\u7684\u7aef\u5c0d\u7aef\u8a9e\u97f3\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c0e\u81f4\u6a21\u578b\u300c\u5fd8\u8a18\u300d\u7d14\u6587\u5b57 LLM \u7684\u529f\u80fd\u3002\u6211\u5011\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7a2e\u66ff\u4ee3\u7bc4\u4f8b\uff0c\u7528\u65bc\u5728\u6c92\u6709\u6307\u4ee4\u8cc7\u6599\u7684\u60c5\u6cc1\u4e0b\u8a13\u7df4\u8a9e\u97f3 LLM\uff0c\u4f7f\u7528\u7d14\u6587\u5b57 LLM \u5c0d\u8b04\u672c\u7684\u56de\u61c9\u4f5c\u70ba\u81ea\u6211\u76e3\u7763\u3002\u91cd\u8981\u7684\u662f\uff0c\u9019\u500b\u904e\u7a0b\u53ef\u4ee5\u5728\u6c92\u6709\u8a3b\u89e3\u56de\u61c9\u7684\u60c5\u6cc1\u4e0b\u57f7\u884c\u3002\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u7684 Distilled Voice Assistant (DiVA) \u53ef\u4ee5\u6982\u62ec\u70ba\u53e3\u8aaa\u554f\u984c\u89e3\u7b54\u3001\u5206\u985e\u548c\u7ffb\u8b6f\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86 DiVA \u66f4\u80fd\u6eff\u8db3\u4f7f\u7528\u8005\u7684\u504f\u597d\uff0c\u8207 Qwen 2 Audio \u7b49\u6700\u5148\u9032\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u9054\u5230\u4e86 72% \u7684\u7372\u52dd\u7387\uff0c\u5118\u7ba1\u8a13\u7df4\u904b\u7b97\u91cf\u6e1b\u5c11\u4e86 >100 \u500d\u3002", "author": "William Held et.al.", "authors": "William Held, Ella Li, Michael Ryan, Weiyan Shi, Yanzhe Zhang, Diyi Yang", "id": "2410.02678v1", "paper_url": "http://arxiv.org/abs/2410.02678v1", "repo": "null"}}