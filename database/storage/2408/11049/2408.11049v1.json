{"2408.11049": {"publish_time": "2024-08-20", "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding", "paper_summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9577\u8a9e\u5883\u61c9\u7528\u4e2d\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u666e\u904d\uff0c\u4f8b\u5982\u4e92\u52d5\u5f0f\u804a\u5929\u6a5f\u5668\u4eba\u3001\u6587\u4ef6\u5206\u6790\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f46\u4ee5\u4f4e\u5ef6\u9072\u548c\u9ad8\u541e\u5410\u91cf\u63d0\u4f9b\u9577\u8a9e\u5883\u8acb\u6c42\u5177\u6709\u6311\u6230\u6027\u3002\u63a8\u6e2c\u6027\u89e3\u78bc (SD) \u662f\u4e00\u7a2e\u5ee3\u6cdb\u4f7f\u7528\u7684\u6280\u8853\uff0c\u7528\u65bc\u5728\u4e0d\u72a7\u7272\u6027\u80fd\u7684\u60c5\u6cc1\u4e0b\u6e1b\u5c11\u5ef6\u9072\uff0c\u4f46\u50b3\u7d71\u89c0\u5ff5\u8a8d\u70ba\u5176\u6548\u529b\u50c5\u9650\u65bc\u5c0f\u6279\u6b21\u5927\u5c0f\u3002\u5728 MagicDec \u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u5373\u4f7f\u5c0d\u65bc\u4e2d\u7b49\u9577\u5ea6\u7684\u5e8f\u5217\uff0cSD \u4e5f\u53ef\u4ee5\u5be6\u73fe\u9ad8\u541e\u5410\u91cf\u63a8\u7406\u6a21\u5f0f\u7684\u901f\u5ea6\u63d0\u5347\u3002\u66f4\u6709\u8da3\u7684\u662f\uff0c\u57fa\u65bc\u6211\u5011\u56b4\u8b39\u7684\u5206\u6790\uff0c\u4e00\u7a2e\u667a\u80fd\u8d77\u8349\u7b56\u7565\u53ef\u4ee5\u96a8\u8457\u6279\u6b21\u5927\u5c0f\u7684\u589e\u52a0\u5be6\u73fe\u66f4\u597d\u7684\u52a0\u901f\u3002MagicDec \u9996\u5148\u627e\u51fa\u74f6\u9838\u96a8\u8457\u6279\u6b21\u5927\u5c0f\u548c\u5e8f\u5217\u9577\u5ea6\u7684\u589e\u52a0\u800c\u767c\u751f\u8f49\u79fb\uff0c\u4e26\u5229\u7528\u9019\u4e9b\u898b\u89e3\u66f4\u6709\u6548\u5730\u90e8\u7f72\u63a8\u6e2c\u6027\u89e3\u78bc\u4ee5\u9032\u884c\u9ad8\u541e\u5410\u91cf\u63a8\u7406\u3002\u7136\u5f8c\uff0c\u5b83\u5229\u7528\u5177\u6709\u7a00\u758f KV \u5feb\u53d6\u7684\u8349\u7a3f\u6a21\u578b\u4f86\u89e3\u6c7a\u96a8\u8457\u5e8f\u5217\u9577\u5ea6\u548c\u6279\u6b21\u5927\u5c0f\u800c\u64f4\u5c55\u7684 KV \u74f6\u9838\u3002", "author": "Jian Chen et.al.", "authors": "Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen", "id": "2408.11049v1", "paper_url": "http://arxiv.org/abs/2408.11049v1", "repo": "null"}}