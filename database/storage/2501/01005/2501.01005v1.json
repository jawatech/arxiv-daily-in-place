{"2501.01005": {"publish_time": "2025-01-02", "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving", "paper_summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.", "paper_summary_zh": "<paragraph>\u53d7\u6ce8\u610f\u529b\u673a\u5236\u9a71\u52a8\u7684 Transformer \u6784\u6210\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u57fa\u7840\u3002\u968f\u7740\u8fd9\u4e9b\u6a21\u578b\u7684\u6269\u5c55\uff0c\u9ad8\u6548\u7684 GPU \u6ce8\u610f\u529b\u5185\u6838\u5bf9\u4e8e\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u5ef6\u8fdf\u63a8\u7406\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5404\u79cd LLM \u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u7075\u6d3b\u4e14\u9ad8\u6027\u80fd\u7684\u6ce8\u610f\u529b\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u5c55\u793a\u4e86 FlashInfer\uff1a\u4e00\u4e2a\u53ef\u5b9a\u5236\u4e14\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u5f15\u64ce\uff0c\u7528\u4e8e LLM \u670d\u52a1\u3002FlashInfer \u4f7f\u7528\u5757\u7a00\u758f\u683c\u5f0f\u548c\u53ef\u7ec4\u5408\u683c\u5f0f\u6765\u89e3\u51b3 KV \u7f13\u5b58\u5b58\u50a8\u5f02\u6784\u6027\uff0c\u4ee5\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u5e76\u51cf\u5c11\u5197\u4f59\u3002\u5b83\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u5b9a\u5236\u7684\u6ce8\u610f\u529b\u6a21\u677f\uff0c\u901a\u8fc7\u5373\u65f6 (JIT) \u7f16\u8bd1\u5b9e\u73b0\u5bf9\u5404\u79cd\u8bbe\u7f6e\u7684\u9002\u5e94\u3002\u6b64\u5916\uff0cFlashInfer \u7684\u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\u7b97\u6cd5\u53ef\u8c03\u6574\u7528\u6237\u8bf7\u6c42\u7684\u52a8\u6001\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u9700\u8981\u9759\u6001\u914d\u7f6e\u7684 CUDAGraph \u7684\u517c\u5bb9\u6027\u3002FlashInfer \u5df2\u96c6\u6210\u5230\u9886\u5148\u7684 LLM \u670d\u52a1\u6846\u67b6\u4e2d\uff0c\u5982 SGLang\u3001vLLM \u548c MLC-Engine\u3002\u5168\u9762\u7684\u5185\u6838\u7ea7\u548c\u7aef\u5230\u7aef\u8bc4\u4f30\u8bc1\u660e\u4e86 FlashInfer \u5728\u5404\u79cd\u63a8\u7406\u573a\u666f\u4e2d\u663e\u7740\u63d0\u5347\u5185\u6838\u6027\u80fd\u7684\u80fd\u529b\uff1a\u4e0e\u6700\u5148\u8fdb\u7684 LLM \u670d\u52a1\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\uff0cFlashInfer \u5b9e\u73b0 29-69% \u7684\u4ee4\u724c\u95f4\u5ef6\u8fdf\u964d\u4f4e\uff0c\u4e0e LLM \u670d\u52a1\u57fa\u51c6\u7684\u7f16\u8bd1\u5668\u540e\u7aef\u76f8\u6bd4\uff0c\u5ef6\u8fdf\u964d\u4f4e 28-30%\uff0c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e 13-17%\uff0c\u5e76\u52a0\u5feb LLM \u670d\u52a1\u4e0e\u5e76\u884c\u751f\u6210\u7684\u901f\u5ea6 13-17%\u3002</paragraph>", "author": "Zihao Ye et.al.", "authors": "Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze", "id": "2501.01005v1", "paper_url": "http://arxiv.org/abs/2501.01005v1", "repo": "https://github.com/flashinfer-ai/flashinfer"}}