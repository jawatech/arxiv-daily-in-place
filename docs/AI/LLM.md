
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-26**|**Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning**|Xinyang Gu et.al.|[2408.14472v1](http://arxiv.org/abs/2408.14472v1)|null|
|**2024-08-26**|**A Practitioner's Guide to Continual Multimodal Pretraining**|Karsten Roth et.al.|[2408.14471v1](http://arxiv.org/abs/2408.14471v1)|[link](https://github.com/explainableml/fomo_in_flux)|
|**2024-08-26**|**Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models**|Aradhye Agarwal et.al.|[2408.14470v1](http://arxiv.org/abs/2408.14470v1)|null|
|**2024-08-26**|**K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences**|Zhikai Li et.al.|[2408.14468v1](http://arxiv.org/abs/2408.14468v1)|null|
|**2024-08-26**|**Explicit Inductive Inference using Large Language Models**|Tianyang Liu et.al.|[2408.14467v1](http://arxiv.org/abs/2408.14467v1)|null|
|**2024-08-26**|**Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification**|Mahrukh Awan et.al.|[2408.14441v1](http://arxiv.org/abs/2408.14441v1)|null|
|**2024-08-26**|**Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study**|Liuchang Xu Shuo Zhao et.al.|[2408.14438v1](http://arxiv.org/abs/2408.14438v1)|null|
|**2024-08-26**|**Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview**|Ilkin Aliyev et.al.|[2408.14437v1](http://arxiv.org/abs/2408.14437v1)|null|
|**2024-08-26**|**Social perception of faces in a vision-language model**|Carina I. Hausladen et.al.|[2408.14435v1](http://arxiv.org/abs/2408.14435v1)|[link](https://github.com/carinahausladen/clip-face-bias)|
|**2024-08-26**|**Contextual Bandit with Herding Effects: Algorithms and Recommendation Applications**|Luyue Xu et.al.|[2408.14432v1](http://arxiv.org/abs/2408.14432v1)|null|
|**2024-08-26**|**CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models**|Shubham Bharti et.al.|[2408.14419v1](http://arxiv.org/abs/2408.14419v1)|null|
|**2024-08-26**|**MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**|Kuluhan Binici et.al.|[2408.14418v1](http://arxiv.org/abs/2408.14418v1)|null|
|**2024-08-26**|**Language-specific Calibration for Pruning Multilingual Language Models**|Simon Kurz et.al.|[2408.14398v1](http://arxiv.org/abs/2408.14398v1)|null|
|**2024-08-26**|**Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**|Xiaoman Zhang et.al.|[2408.14397v1](http://arxiv.org/abs/2408.14397v1)|[link](https://github.com/rajpurkarlab/rexkg)|
|**2024-08-26**|**Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning**|Sakhinana Sagar Srinivas et.al.|[2408.14387v1](http://arxiv.org/abs/2408.14387v1)|null|
|**2024-08-26**|**Probing Causality Manipulation of Large Language Models**|Chenyang Zhang et.al.|[2408.14380v1](http://arxiv.org/abs/2408.14380v1)|[link](https://github.com/tongjinlp/llm-causality-probing)|
|**2024-08-26**|**SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery**|Sarah Rastegar et.al.|[2408.14371v1](http://arxiv.org/abs/2408.14371v1)|[link](https://github.com/sarahrastegar/selex)|
|**2024-08-26**|**GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal Conditioned Policy**|Peiyan Li et.al.|[2408.14368v1](http://arxiv.org/abs/2408.14368v1)|null|
|**2024-08-26**|**SWE-bench-java: A GitHub Issue Resolving Benchmark for Java**|Daoguang Zan et.al.|[2408.14354v1](http://arxiv.org/abs/2408.14354v1)|[link](https://github.com/multi-swe-bench/multi-swe-bench-env)|
|**2024-08-26**|**Assessing Contamination in Large Language Models: Introducing the LogProber method**|Nicolas Yax et.al.|[2408.14352v1](http://arxiv.org/abs/2408.14352v1)|null|
|**2024-08-26**|**Foundation Models for Music: A Survey**|Yinghao Ma et.al.|[2408.14340v1](http://arxiv.org/abs/2408.14340v1)|[link](https://github.com/nicolaus625/fm4music)|
|**2024-08-26**|**Machine Learning for Quantifier Selection in cvc5**|Jan Jakubův et.al.|[2408.14338v1](http://arxiv.org/abs/2408.14338v1)|null|
|**2024-08-26**|**PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection Dataset**|Ghazal Alinezhad Noghre et.al.|[2408.14329v1](http://arxiv.org/abs/2408.14329v1)|[link](https://github.com/tecsar-uncc/pheva)|
|**2024-08-26**|**Streamline tractography of the fetal brain in utero with machine learning**|Weide Liu et.al.|[2408.14326v1](http://arxiv.org/abs/2408.14326v1)|null|
|**2024-08-26**|**Claim Verification in the Age of Large Language Models: A Survey**|Alphaeus Dmonte et.al.|[2408.14317v1](http://arxiv.org/abs/2408.14317v1)|null|
|**2024-08-26**|**LLM-3D Print: Large Language Models To Monitor and Control 3D Printing**|Yayati Jadhav et.al.|[2408.14307v1](http://arxiv.org/abs/2408.14307v1)|null|
|**2024-08-26**|**Predictability and Causality in Spanish and English Natural Language Generation**|Andrea Busto-Castiñeira et.al.|[2408.14283v1](http://arxiv.org/abs/2408.14283v1)|null|
|**2024-08-26**|**Uncertainties of Latent Representations in Computer Vision**|Michael Kirchhof et.al.|[2408.14281v1](http://arxiv.org/abs/2408.14281v1)|null|
|**2024-08-26**|**Epidemic Information Extraction for Event-Based Surveillance using Large Language Models**|Sergio Consoli et.al.|[2408.14277v1](http://arxiv.org/abs/2408.14277v1)|null|
|**2024-08-26**|**Self-supervised Speech Representations Still Struggle with African American Vernacular English**|Kalvin Chang et.al.|[2408.14262v1](http://arxiv.org/abs/2408.14262v1)|[link](https://github.com/cmu-llab/s3m-aave)|
|**2024-08-26**|**Text3DAug -- Prompted Instance Augmentation for LiDAR Perception**|Laurenz Reichardt et.al.|[2408.14253v1](http://arxiv.org/abs/2408.14253v1)|null|
|**2024-08-26**|**Beyond Few-shot Object Detection: A Detailed Survey**|Vishal Chudasama et.al.|[2408.14249v1](http://arxiv.org/abs/2408.14249v1)|null|
|**2024-08-26**|**Celtibero: Robust Layered Aggregation for Federated Learning**|Borja Molina-Coronado et.al.|[2408.14240v1](http://arxiv.org/abs/2408.14240v1)|null|
|**2024-08-26**|**DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for type classification**|Hanna Abi Akl et.al.|[2408.14236v1](http://arxiv.org/abs/2408.14236v1)|null|
|**2024-08-26**|**Gallery-Aware Uncertainty Estimation For Open-Set Face Recognition**|Leonid Erlygin et.al.|[2408.14229v1](http://arxiv.org/abs/2408.14229v1)|null|
|**2024-08-26**|**MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement**|Xu He et.al.|[2408.14211v1](http://arxiv.org/abs/2408.14211v1)|null|
|**2024-08-26**|**DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**|Ziai Zhou et.al.|[2408.14185v1](http://arxiv.org/abs/2408.14185v1)|null|
|**2024-08-26**|**I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing**|Yiwei Ma et.al.|[2408.14180v1](http://arxiv.org/abs/2408.14180v1)|[link](https://github.com/cocoshe/i2ebench)|
|**2024-08-26**|**SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher**|Trung Dao et.al.|[2408.14176v1](http://arxiv.org/abs/2408.14176v1)|null|
|**2024-08-26**|**Dynamic Pricing for Electric Vehicle Charging**|Arun Kumar Kalakanti et.al.|[2408.14169v1](http://arxiv.org/abs/2408.14169v1)|null|
|**2024-08-26**|**Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning**|Wei An et.al.|[2408.14158v1](http://arxiv.org/abs/2408.14158v1)|null|
|**2024-08-26**|**Investigating the effect of Mental Models in User Interaction with an Adaptive Dialog Agent**|Lindsey Vanderlyn et.al.|[2408.14154v1](http://arxiv.org/abs/2408.14154v1)|null|
|**2024-08-26**|**Explaining Vision-Language Similarities in Dual Encoders with Feature-Pair Attributions**|Lucas Möller et.al.|[2408.14153v1](http://arxiv.org/abs/2408.14153v1)|null|
|**2024-08-26**|**Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in Subjective Tasks?**|Urja Khurana et.al.|[2408.14141v1](http://arxiv.org/abs/2408.14141v1)|null|
|**2024-08-26**|**Multi-Faceted Evaluation of Modeling Languages for Augmented Reality Applications -- The Case of ARWFML**|Fabian Muff et.al.|[2408.14137v1](http://arxiv.org/abs/2408.14137v1)|null|
|**2024-08-26**|**Exploring the Potential of Large Language Models for Heterophilic Graphs**|Yuxia Wu et.al.|[2408.14134v1](http://arxiv.org/abs/2408.14134v1)|null|
|**2024-08-26**|**Contrastive Learning Subspace for Text Clustering**|Qian Yong et.al.|[2408.14119v1](http://arxiv.org/abs/2408.14119v1)|null|
|**2024-08-26**|**Estimating Causal Effects from Learned Causal Networks**|Anna Raichev et.al.|[2408.14101v1](http://arxiv.org/abs/2408.14101v1)|null|
|**2024-08-26**|**SONICS: Synthetic Or Not -- Identifying Counterfeit Songs**|Md Awsafur Rahman et.al.|[2408.14080v1](http://arxiv.org/abs/2408.14080v1)|null|
|**2024-08-26**|**Enhancing Depression Diagnosis with Chain-of-Thought Prompting**|Elysia Shi et.al.|[2408.14053v1](http://arxiv.org/abs/2408.14053v1)|null|
|**2024-08-26**|**Beyond Detection: Leveraging Large Language Models for Cyber Attack Prediction in IoT Networks**|Alaeddine Diaf et.al.|[2408.14045v1](http://arxiv.org/abs/2408.14045v1)|null|
|**2024-08-26**|**PAGE: Parametric Generative Explainer for Graph Neural Network**|Yang Qiu et.al.|[2408.14042v1](http://arxiv.org/abs/2408.14042v1)|null|
|**2024-08-26**|**MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents**|Ruochen Li et.al.|[2408.14033v1](http://arxiv.org/abs/2408.14033v1)|[link](https://github.com/du-nlp-lab/mlr-copilot)|
|**2024-08-26**|**SurGen: Text-Guided Diffusion Model for Surgical Video Generation**|Joseph Cho et.al.|[2408.14028v1](http://arxiv.org/abs/2408.14028v1)|null|
|**2024-08-26**|**Empowering Low-Resource Language ASR via Large-Scale Pseudo Labeling**|Kaushal Santosh Bhogale et.al.|[2408.14026v1](http://arxiv.org/abs/2408.14026v1)|null|
|**2024-08-26**|**Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks for Short and Long Videos**|Jiajun Fei et.al.|[2408.14023v1](http://arxiv.org/abs/2408.14023v1)|[link](https://github.com/qq-mm/video-ccam)|
|**2024-08-26**|**Pixel-Aligned Multi-View Generation with Depth Guided Decoder**|Zhenggang Tang et.al.|[2408.14016v1](http://arxiv.org/abs/2408.14016v1)|null|
|**2024-08-26**|**LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models**|Qihang Ge et.al.|[2408.14008v1](http://arxiv.org/abs/2408.14008v1)|null|
|**2024-08-26**|**Dual-CBA: Improving Online Continual Learning via Dual Continual Bias Adaptors from a Bi-level Optimization Perspective**|Quanziang Wang et.al.|[2408.13991v1](http://arxiv.org/abs/2408.13991v1)|null|
|**2024-08-26**|**Automatic Medical Report Generation: Methods and Applications**|Li Guo et.al.|[2408.13988v1](http://arxiv.org/abs/2408.13988v1)|null|
|**2024-08-26**|**Question answering system of bridge design specification based on large language model**|Leye Zhang et.al.|[2408.13282v1](http://arxiv.org/abs/2408.13282v1)|[link](https://github.com/zhangleye/Bridge-LLM-QA)|
|**2024-08-26**|**Focused Large Language Models are Stable Many-Shot Learners**|Peiwen Yuan et.al.|[2408.13987v1](http://arxiv.org/abs/2408.13987v1)|null|
|**2024-08-26**|**AgentMove: Predicting Human Mobility Anywhere Using Large Language Model based Agentic Framework**|Jie Feng et.al.|[2408.13986v1](http://arxiv.org/abs/2408.13986v1)|[link](https://github.com/tsinghua-fib-lab/agentmove)|
|**2024-08-26**|**TF-Attack: Transferable and Fast Adversarial Attacks on Large Language Models**|Zelin Li et.al.|[2408.13985v1](http://arxiv.org/abs/2408.13985v1)|null|
|**2024-08-26**|**Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models**|Shuai Fu et.al.|[2408.13979v1](http://arxiv.org/abs/2408.13979v1)|[link](https://github.com/shyfoo/nemesis)|
|**2024-08-26**|**Reducing the Cost: Cross-Prompt Pre-Finetuning for Short Answer Scoring**|Hiroaki Funayama et.al.|[2408.13966v1](http://arxiv.org/abs/2408.13966v1)|null|
|**2024-08-25**|**Time Series Analysis for Education: Methods, Applications, and Future Directions**|Shengzhong Mao et.al.|[2408.13960v1](http://arxiv.org/abs/2408.13960v1)|[link](https://github.com/ai-for-edu/time-series-analysis-for-education)|
|**2024-08-25**|**Bidirectional Awareness Induction in Autoregressive Seq2Seq Models**|Jia Cheng Hu et.al.|[2408.13959v1](http://arxiv.org/abs/2408.13959v1)|null|
|**2024-08-25**|**Prediction of COPD Using Machine Learning, Clinical Summary Notes, and Vital Signs**|Negar Orangi-Fard et.al.|[2408.13958v1](http://arxiv.org/abs/2408.13958v1)|null|
|**2024-08-25**|**CoT Rerailer: Enhancing the Reliability of Large Language Models in Complex Reasoning Tasks through Error Detection and Correction**|Guangya Wan et.al.|[2408.13940v1](http://arxiv.org/abs/2408.13940v1)|null|
|**2024-08-25**|**Learning to Move Like Professional Counter-Strike Players**|David Durst et.al.|[2408.13934v1](http://arxiv.org/abs/2408.13934v1)|null|
|**2024-08-25**|**MobileQuant: Mobile-friendly Quantization for On-device Language Models**|Fuwen Tan et.al.|[2408.13933v1](http://arxiv.org/abs/2408.13933v1)|[link](https://github.com/saic-fi/mobilequant)|
|**2024-08-25**|**FedGlu: A personalized federated learning-based glucose forecasting algorithm for improved performance in glycemic excursion regions**|Darpit Dave et.al.|[2408.13926v1](http://arxiv.org/abs/2408.13926v1)|null|
|**2024-08-25**|**Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with Spatiotemporal Constraints**|Siyu Li et.al.|[2408.13918v1](http://arxiv.org/abs/2408.13918v1)|null|
|**2024-08-25**|**LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback**|Tanushree Banerjee et.al.|[2408.13915v1](http://arxiv.org/abs/2408.13915v1)|null|
|**2024-08-25**|**LowCLIP: Adapting the CLIP Model Architecture for Low-Resource Languages in Multimodal Image Retrieval Task**|Ali Asgarov et.al.|[2408.13909v1](http://arxiv.org/abs/2408.13909v1)|[link](https://github.com/aliasgerovs/azclip)|
|**2024-08-25**|**ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models**|Yeji Park et.al.|[2408.13906v1](http://arxiv.org/abs/2408.13906v1)|[link](https://github.com/yejipark-m/convis)|
|**2024-08-25**|**SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning**|Chien-yu Huang et.al.|[2408.13891v1](http://arxiv.org/abs/2408.13891v1)|null|
|**2024-08-25**|**LLM with Relation Classifier for Document-Level Relation Extraction**|Xingzuo Li et.al.|[2408.13889v1](http://arxiv.org/abs/2408.13889v1)|null|
|**2024-08-25**|**Enhancing SQL Query Generation with Neurosymbolic Reasoning**|Henrijs Princis et.al.|[2408.13888v1](http://arxiv.org/abs/2408.13888v1)|null|
|**2024-08-25**|**Flexible game-playing AI with AlphaViT: adapting to multiple games and board sizes**|Kazuhisa Fujita et.al.|[2408.13871v1](http://arxiv.org/abs/2408.13871v1)|null|
|**2024-08-25**|**CodeGraph: Enhancing Graph Reasoning of LLMs with Code**|Qiaolong Cai et.al.|[2408.13863v1](http://arxiv.org/abs/2408.13863v1)|null|
|**2024-08-25**|**Knowledge-Aware Reasoning over Multimodal Semi-structured Tables**|Suyash Vardhan Mathur et.al.|[2408.13860v1](http://arxiv.org/abs/2408.13860v1)|null|
|**2024-08-25**|**Tangram: A Challenging Benchmark for Geometric Element Recognizing**|Jiamin Tang et.al.|[2408.13854v1](http://arxiv.org/abs/2408.13854v1)|null|
|**2024-08-25**|**Condensed Sample-Guided Model Inversion for Knowledge Distillation**|Kuluhan Binici et.al.|[2408.13850v1](http://arxiv.org/abs/2408.13850v1)|null|
|**2024-08-25**|**PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**|Zifan Chen et.al.|[2408.13836v1](http://arxiv.org/abs/2408.13836v1)|null|
|**2024-08-25**|**Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data**|Felix J. Dorfner et.al.|[2408.13833v1](http://arxiv.org/abs/2408.13833v1)|null|
|**2024-08-25**|**RoCP-GNN: Robust Conformal Prediction for Graph Neural Networks in Node-Classification**|S. Akansha et.al.|[2408.13825v1](http://arxiv.org/abs/2408.13825v1)|null|
|**2024-08-25**|**Revisiting the Exit from Nuclear Energy in Germany with NLP**|Sebastian Haunss et.al.|[2408.13810v1](http://arxiv.org/abs/2408.13810v1)|null|
|**2024-08-25**|**Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models**|Duy Khoa Pham et.al.|[2408.13808v1](http://arxiv.org/abs/2408.13808v1)|null|
|**2024-08-25**|**Lecture Notes on Linear Neural Networks: A Tale of Optimization and Generalization in Deep Learning**|Nadav Cohen et.al.|[2408.13767v1](http://arxiv.org/abs/2408.13767v1)|null|
|**2024-08-25**|**Multi-Agent Target Assignment and Path Finding for Intelligent Warehouse: A Cooperative Multi-Agent Deep Reinforcement Learning Perspective**|Qi Liu et.al.|[2408.13750v1](http://arxiv.org/abs/2408.13750v1)|null|
|**2024-08-25**|**DOCE: Finding the Sweet Spot for Execution-Based Code Generation**|Haau-Sing Li et.al.|[2408.13745v1](http://arxiv.org/abs/2408.13745v1)|[link](https://github.com/deep-spin/doce)|
|**2024-08-25**|**Literary and Colloquial Tamil Dialect Identification**|M. Nanmalar et.al.|[2408.13739v1](http://arxiv.org/abs/2408.13739v1)|null|
|**2024-08-25**|**Poor-Supervised Evaluation for SuperLLM via Mutual Consistency**|Peiwen Yuan et.al.|[2408.13738v1](http://arxiv.org/abs/2408.13738v1)|null|
|**2024-08-25**|**LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models**|Aoxiao Zhong et.al.|[2408.13727v1](http://arxiv.org/abs/2408.13727v1)|null|
|**2024-08-25**|**DHP Benchmark: Are LLMs Good NLG Evaluators?**|Yicheng Wang et.al.|[2408.13704v1](http://arxiv.org/abs/2408.13704v1)|null|
|**2024-08-24**|**Evaluating Alternative Training Interventions Using Personalized Computational Models of Learning**|Christopher James MacLellan et.al.|[2408.13684v1](http://arxiv.org/abs/2408.13684v1)|null|
|**2024-08-24**|**Submodular Maximization Approaches for Equitable Client Selection in Federated Learning**|Andrés Catalino Castillo Jiménez et.al.|[2408.13683v1](http://arxiv.org/abs/2408.13683v1)|null|
|**2024-08-24**|**A layer-wise analysis of Mandarin and English suprasegmentals in SSL speech models**|Antón de la Fuente et.al.|[2408.13678v1](http://arxiv.org/abs/2408.13678v1)|null|

#### Abstracts
##### **Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning**
2408.14472v1 by Xinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi, Yanjiang Guo, Yichen Liu, Jianyu Chen

Humanoid robots, with their human-like skeletal structure, are especially
suited for tasks in human-centric environments. However, this structure is
accompanied by additional challenges in locomotion controller design,
especially in complex real-world environments. As a result, existing humanoid
robots are limited to relatively simple terrains, either with model-based
control or model-free reinforcement learning. In this work, we introduce
Denoising World Model Learning (DWL), an end-to-end reinforcement learning
framework for humanoid locomotion control, which demonstrates the world's first
humanoid robot to master real-world challenging terrains such as snowy and
inclined land in the wild, up and down stairs, and extremely uneven terrains.
All scenarios run the same learned neural network with zero-shot sim-to-real
transfer, indicating the superior robustness and generalization capability of
the proposed method.

摘要：類人機器人擁有類似人類的骨骼結構，特別適合在以人類為中心環境中的任務。然而，此結構伴隨著運動控制器設計中的額外挑戰，特別是在複雜的真實世界環境中。因此，現有的類人機器人僅限於相對簡單的地形，無論是基於模型的控制或無模型的強化學習。在這項工作中，我們引入了去噪世界模型學習 (DWL)，這是一種用於類人運動控制的端對端強化學習框架，展示了世界上第一個類人機器人，它能掌握真實世界中具有挑戰性的地形，例如野外白雪皚皚和傾斜的土地、上下樓梯以及極不平坦的地形。所有場景都使用零次學習神經網路運作，並進行模擬到真實的轉移，這表明所提出方法具有優異的穩健性和泛化能力。

##### **A Practitioner's Guide to Continual Multimodal Pretraining**
2408.14471v1 by Karsten Roth, Vishaal Udandarao, Sebastian Dziadzio, Ameya Prabhu, Mehdi Cherti, Oriol Vinyals, Olivier Hénaff, Samuel Albanie, Matthias Bethge, Zeynep Akata

Multimodal foundation models serve numerous applications at the intersection
of vision and language. Still, despite being pretrained on extensive data, they
become outdated over time. To keep models updated, research into continual
pretraining mainly explores scenarios with either (1) infrequent,
indiscriminate updates on large-scale new data, or (2) frequent, sample-level
updates. However, practical model deployment often operates in the gap between
these two limit cases, as real-world applications often demand adaptation to
specific subdomains, tasks or concepts -- spread over the entire, varying life
cycle of a model. In this work, we complement current perspectives on continual
pretraining through a research test bed as well as provide comprehensive
guidance for effective continual model updates in such scenarios. We first
introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with
realistic compute constraints and practical deployment requirements,
constructed over 63 datasets with diverse visual and semantic coverage. Using
FoMo-in-Flux, we explore the complex landscape of practical continual
pretraining through multiple perspectives: (1) A data-centric investigation of
data mixtures and stream orderings that emulate real-world deployment
situations, (2) a method-centric investigation ranging from simple fine-tuning
and traditional continual learning strategies to parameter-efficient updates
and model merging, (3) meta learning rate schedules and mechanistic design
choices, and (4) the influence of model and compute scaling. Together, our
insights provide a practitioner's guide to continual multimodal pretraining for
real-world deployment. Our benchmark and code is here:
https://github.com/ExplainableML/fomo_in_flux.

摘要：<paragraph>多模態基礎模型在視覺和語言的交會點上服務於許多應用程式。儘管經過大量資料的預訓練，它們仍會隨著時間而過時。為了保持模型的更新，對持續預訓練的研究主要探討以下場景：(1) 對大規模新資料進行不頻繁、無差別的更新，或 (2) 頻繁、樣本層級的更新。然而，實際的模型部署通常在兩個限制案例之間運作，因為現實世界的應用程式通常需要適應特定的子網域、任務或概念，這些概念遍佈於模型的整個、多變的生命週期。在這項工作中，我們透過研究測試平台補充了目前對持續預訓練的觀點，並針對此類場景中的有效持續模型更新提供了全面的指導。我們首先介紹 FoMo-in-Flux，這是一個持續多模態預訓練基準，具有實際的運算限制和實際的部署需求，建構在 63 個具有多樣化視覺和語義涵蓋範圍的資料集上。使用 FoMo-in-Flux，我們從多個角度探討了實際持續預訓練的複雜環境：(1) 模擬現實世界部署情況的資料混合和串流排序的以資料為中心的調查，(2) 從簡單的微調和傳統的持續學習策略到參數有效更新和模型合併的方法中心調查，(3) 元學習率排程和機械設計選擇，以及 (4) 模型和運算縮放的影響。我們的見解共同為實務工作者提供了持續多模態預訓練的指南，以便進行實際世界的部署。我們的基準和程式碼在此處：https://github.com/ExplainableML/fomo_in_flux。</paragraph>

##### **Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models**
2408.14470v1 by Aradhye Agarwal, Suhas K Ramesh, Ayan Sengupta, Tanmoy Chakraborty

Fine-tuning large language models (LLMs) on downstream tasks requires
substantial computational resources. A class of parameter-efficient fine-tuning
(PEFT) aims to mitigate these computational challenges by selectively
fine-tuning only a small fraction of the model parameters. Although
computationally efficient, these techniques often fail to match the performance
of fully fine-tuned models, primarily due to inherent biases introduced during
parameter selection. Traditional selective PEFT techniques use a fixed set of
parameters based on a predefined budget (a process also known as unmasking),
failing to capture parameter importance dynamically and often ending up
exceeding the budget. We introduce $\text{ID}^3$, a novel selective PEFT method
that calculates parameter importance continually and dynamically unmasks
parameters by balancing exploration and exploitation in parameter selection.
Our empirical study on 15 tasks spanning natural language understanding and
generative tasks demonstrates the effectiveness of our method compared to
fixed-masking-based PEFT techniques. We analytically show that $\text{ID}^3$
reduces the number of gradient updates by a factor of two, enhancing
computational efficiency. $\text{ID}^3$ is robust to random initialization of
neurons and, therefore, can be seamlessly integrated into existing additive and
reparametrization-based PEFT modules such as adapters and LoRA for dynamic
sparsification.

摘要：微调大型语言模型 (LLM) 以执行下游任务需要大量的计算资源。一类参数高效微调 (PEFT) 旨在通过仅选择性微调模型参数的一小部分来缓解这些计算挑战。尽管这些技术在计算上很有效，但它们通常无法与经过完全微调的模型的性能相匹配，这主要是由于在参数选择过程中引入的固有偏差。传统的 PEFT 选择性技术使用基于预定义预算的一组固定参数（也称为取消掩码的过程），无法动态捕获参数重要性，并且经常超出预算。我们引入了 $\text{ID}^3$，这是一种新颖的选择性 PEFT 方法，它持续计算参数重要性，并通过平衡参数选择中的探索和利用来动态取消参数的掩码。我们对涵盖自然语言理解和生成任务的 15 项任务进行的实证研究证明了我们方法与基于固定掩码的 PEFT 技术相比的有效性。我们通过分析表明 $\text{ID}^3$ 将梯度更新的次数减少了一半，从而提高了计算效率。$\text{ID}^3}$ 对神经元的随机初始化具有鲁棒性，因此可以无缝集成到现有的基于加法和重新参数化的 PEFT 模块中，例如用于动态稀疏化的适配器和 LoRA。

##### **K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences**
2408.14468v1 by Zhikai Li, Xuewen Liu, Dongrong Fu, Jianquan Li, Qingyi Gu, Kurt Keutzer, Zhen Dong

The rapid advancement of visual generative models necessitates efficient and
reliable evaluation methods. Arena platform, which gathers user votes on model
comparisons, can rank models with human preferences. However, traditional Arena
methods, while established, require an excessive number of comparisons for
ranking to converge and are vulnerable to preference noise in voting,
suggesting the need for better approaches tailored to contemporary evaluation
challenges. In this paper, we introduce K-Sort Arena, an efficient and reliable
platform based on a key insight: images and videos possess higher perceptual
intuitiveness than texts, enabling rapid evaluation of multiple samples
simultaneously. Consequently, K-Sort Arena employs K-wise comparisons, allowing
K models to engage in free-for-all competitions, which yield much richer
information than pairwise comparisons. To enhance the robustness of the system,
we leverage probabilistic modeling and Bayesian updating techniques. We propose
an exploration-exploitation-based matchmaking strategy to facilitate more
informative comparisons. In our experiments, K-Sort Arena exhibits 16.3x faster
convergence compared to the widely used ELO algorithm. To further validate the
superiority and obtain a comprehensive leaderboard, we collect human feedback
via crowdsourced evaluations of numerous cutting-edge text-to-image and
text-to-video models. Thanks to its high efficiency, K-Sort Arena can
continuously incorporate emerging models and update the leaderboard with
minimal votes. Our project has undergone several months of internal testing and
is now available at https://huggingface.co/spaces/ksort/K-Sort-Arena

摘要：<paragraph>視覺生成模型的快速進展需要有效且可靠的評估方法。Arena 平台收集使用者對模型比較中的投票，可以根據人類偏好對模型進行排名。然而，傳統的 Arena 方法雖然已經建立，但需要大量的比較才能收斂排名，而且容易受到投票中的偏好雜訊影響，這表明需要更好的方法來應對當前的評估挑戰。在本文中，我們介紹了 K-Sort Arena，這是一個基於一個關鍵見解的有效且可靠的平台：圖像和影片比文字具有更高的感知直觀性，可以快速評估多個樣本。因此，K-Sort Arena 採用 K 次比較，允許 K 個模型進行混戰，這比成對比較產生了更豐富的資訊。為了增強系統的穩健性，我們利用機率模型和貝氏更新技術。我們提出了一個基於探索-開發的配對策略，以促進更多有意義的比較。在我們的實驗中，與廣泛使用的 ELO 演算法相比，K-Sort Arena 的收斂速度快了 16.3 倍。為了進一步驗證其優越性並獲得一個全面的排行榜，我們通過眾包評估大量最先進的文字轉圖像和文字轉影片模型來收集人類回饋。由於其高效率，K-Sort Arena 可以持續納入新興模型，並以最少的投票更新排行榜。我們的專案已經歷數個月的內部測試，現在可以在 https://huggingface.co/spaces/ksort/K-Sort-Arena 取得。</paragraph>

##### **Explicit Inductive Inference using Large Language Models**
2408.14467v1 by Tianyang Liu, Tianyi Li, Liang Cheng, Mark Steedman

Large Language Models (LLMs) are reported to hold undesirable attestation
bias on inference tasks: when asked to predict if a premise P entails a
hypothesis H, instead of considering H's conditional truthfulness entailed by
P, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In
this paper, we propose a pipeline that exploits this bias to do explicit
inductive inference. Our pipeline uses an LLM to transform a premise into a set
of attested alternatives, and then aggregate answers of the derived new
entailment inquiries to support the original inference prediction. On a
directional predicate entailment benchmark, we demonstrate that by applying
this simple pipeline, we can improve the overall performance of LLMs on
inference and substantially alleviate the impact of their attestation bias.

摘要：大型語言模型 (LLM) 被報導在推論任務中存在不良的證明偏見：當被要求預測前提 P 是否蘊含假設 H 時，LLM 傾向於使用 H 的語境外真實標籤作為一個脆弱的代理，而不是考慮 H 的條件真實性由 P 蘊含。在本文中，我們提出了一個利用這種偏見來進行明確歸納推理的管道。我們的管道使用 LLM 將前提轉換為一組經過證實的替代方案，然後彙總衍生的新蘊含查詢的答案來支持原始推理預測。在一個方向謂詞蘊含基準上，我們證明通過應用這個簡單的管道，我們可以提高 LLM 在推理上的整體性能，並大幅減輕其證明偏見的影響。

##### **Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification**
2408.14441v1 by Mahrukh Awan, Asmar Nadeem, Muhammad Junaid Awan, Armin Mustafa, Syed Sameed Husain

Exploiting both audio and visual modalities for video classification is a
challenging task, as the existing methods require large model architectures,
leading to high computational complexity and resource requirements. Smaller
architectures, on the other hand, struggle to achieve optimal performance. In
this paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that
introduces a compact model architecture specifically designed to capture
intricate audio-visual relationships in video data. Through extensive
experiments on the challenging YouTube-8M dataset, we demonstrate that
Attend-Fusion achieves an F1 score of 75.64\% with only 72M parameters, which
is comparable to the performance of larger baseline models such as
Fully-Connected Late Fusion (75.96\% F1 score, 341M parameters). Attend-Fusion
achieves similar performance to the larger baseline model while reducing the
model size by nearly 80\%, highlighting its efficiency in terms of model
complexity. Our work demonstrates that the Attend-Fusion model effectively
combines audio and visual information for video classification, achieving
competitive performance with significantly reduced model size. This approach
opens new possibilities for deploying high-performance video understanding
systems in resource-constrained environments across various applications.

摘要：利用音訊和視覺兩種方式進行影片分類是一項具有挑戰性的任務，因為現有方法需要大型模型架構，導致運算複雜度和資源需求高。另一方面，較小的架構難以達到最佳效能。在本文中，我們提出 Attend-Fusion，這是一種音訊視覺 (AV) 融合方法，它引進了一個專門設計用於擷取影片資料中複雜音訊視覺關係的精簡模型架構。透過在具有挑戰性的 YouTube-8M 資料集上進行廣泛的實驗，我們證明 Attend-Fusion 只使用 72M 個參數就能達到 75.64% 的 F1 分數，這與較大的基準模型（例如全連接後融合（75.96% F1 分數，341M 個參數））的效能相當。Attend-Fusion 達到與較大的基準模型相似的效能，同時將模型大小減少了將近 80%，突顯了它在模型複雜度方面的效率。我們的研究證明 Attend-Fusion 模型有效地結合了音訊和視覺資訊進行影片分類，在顯著減少模型大小的情況下達到具有競爭力的效能。這種方法為在資源受限的環境中部署高性能影片理解系統開啟了新的可能性，適用於各種應用程式。

##### **Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study**
2408.14438v1 by Liuchang Xu Shuo Zhao, Qingming Lin, Luyao Chen, Qianqian Luo, Sensen Wu, Xinyue Ye, Hailin Feng, Zhenhong Du

The advent of large language models such as ChatGPT, Gemini, and others has
underscored the importance of evaluating their diverse capabilities, ranging
from natural language understanding to code generation. However, their
performance on spatial tasks has not been comprehensively assessed. This study
addresses this gap by introducing a novel multi-task spatial evaluation
dataset, designed to systematically explore and compare the performance of
several advanced models on spatial tasks. The dataset encompasses twelve
distinct task types, including spatial understanding and path planning, each
with verified, accurate answers. We evaluated multiple models, including
OpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase
testing approach. Initially, we conducted zero-shot testing, followed by
categorizing the dataset by difficulty and performing prompt tuning tests.
Results indicate that gpt-4o achieved the highest overall accuracy in the first
phase, with an average of 71.3%. Although moonshot-v1-8k slightly
underperformed overall, it surpassed gpt-4o in place name recognition tasks.
The study also highlights the impact of prompt strategies on model performance
in specific tasks. For example, the Chain-of-Thought (COT) strategy increased
gpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot
strategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to
76.3%.

摘要：隨著 ChatGPT、Gemini 等大型語言模型的出現，評估其多樣化的能力變得越來越重要，這些能力包括從自然語言理解到程式碼生成。然而，它們在空間任務上的表現尚未得到全面評估。本研究通過引入一個新穎的多任務空間評估資料集來解決這一差距，該資料集旨在系統地探索和比較幾個先進模型在空間任務上的表現。該資料集包含十二個不同的任務類型，包括空間理解和路徑規劃，每個任務都有經過驗證的準確答案。我們評估了多個模型，包括 OpenAI 的 gpt-3.5-turbo、gpt-4o 和 ZhipuAI 的 glm-4，採用了兩階段測試方法。最初，我們進行了零次學習測試，然後按難度對資料集進行分類並執行提示調整測試。結果表明，gpt-4o 在第一階段取得了最高的總體準確度，平均為 71.3%。儘管 moonshot-v1-8k 的整體表現略低，但在地名識別任務中超越了 gpt-4o。該研究還強調了提示策略對特定任務中模型表現的影響。例如，思考鏈 (COT) 策略將 gpt-4o 在路徑規劃中的準確度從 12.4% 提高到 87.5%，而一次性策略將 moonshot-v1-8k 在映射任務中的準確度從 10.1% 提高到 76.3%。

##### **Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview**
2408.14437v1 by Ilkin Aliyev, Kama Svoboda, Tosiron Adegbija, Jean-Marc Fellous

Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven
nature of biological neural processing, and offer the potential for
ultra-low-power artificial intelligence. However, realizing their efficiency
benefits requires specialized hardware and a co-design approach that
effectively leverages sparsity. We explore the hardware-software co-design of
sparse SNNs, examining how sparsity representation, hardware architectures, and
training techniques influence hardware efficiency. We analyze the impact of
static and dynamic sparsity, discuss the implications of different neuron
models and encoding schemes, and investigate the need for adaptability in
hardware designs. Our work aims to illuminate the path towards embedded
neuromorphic systems that fully exploit the computational advantages of sparse
SNNs.

摘要：尖峰神經網路 (SNN) 受到生物神經處理的稀疏和事件驅動特性的啟發，並提供超低功耗人工智慧的潛力。然而，實現其效率優勢需要專用硬體和一種協同設計方法，該方法有效利用稀疏性。我們探討稀疏 SNN 的硬體軟體協同設計，探討稀疏表示、硬體架構和訓練技術如何影響硬體效率。我們分析靜態和動態稀疏性的影響，討論不同神經元模型和編碼方案的含義，並探討硬體設計中對適應性的需求。我們的研究工作旨在闡明通往嵌入式類腦系統的道路，該系統充分利用稀疏 SNN 的計算優勢。

##### **Social perception of faces in a vision-language model**
2408.14435v1 by Carina I. Hausladen, Manuel Knott, Colin F. Camerer, Pietro Perona

We explore social perception of human faces in CLIP, a widely used
open-source vision-language model. To this end, we compare the similarity in
CLIP embeddings between different textual prompts and a set of face images. Our
textual prompts are constructed from well-validated social psychology terms
denoting social perception. The face images are synthetic and are
systematically and independently varied along six dimensions: the legally
protected attributes of age, gender, and race, as well as facial expression,
lighting, and pose. Independently and systematically manipulating face
attributes allows us to study the effect of each on social perception and
avoids confounds that can occur in wild-collected data due to uncontrolled
systematic correlations between attributes. Thus, our findings are experimental
rather than observational. Our main findings are three. First, while CLIP is
trained on the widest variety of images and texts, it is able to make
fine-grained human-like social judgments on face images. Second, age, gender,
and race do systematically impact CLIP's social perception of faces, suggesting
an undesirable bias in CLIP vis-a-vis legally protected attributes. Most
strikingly, we find a strong pattern of bias concerning the faces of Black
women, where CLIP produces extreme values of social perception across different
ages and facial expressions. Third, facial expression impacts social perception
more than age and lighting as much as age. The last finding predicts that
studies that do not control for unprotected visual attributes may reach the
wrong conclusions on bias. Our novel method of investigation, which is founded
on the social psychology literature and on the experiments involving the
manipulation of individual attributes, yields sharper and more reliable
observations than previous observational methods and may be applied to study
biases in any vision-language model.

摘要：<paragraph>我們在 CLIP 中探索人類臉部的社會感知，CLIP 是一個廣泛使用的開源視覺語言模型。為此，我們比較了不同文字提示和一系列臉部影像在 CLIP 嵌入中的相似性。我們的文字提示是由經過充分驗證的社會心理學術語建構而成，用於表示社會感知。臉部影像為合成影像，並沿著六個面向進行系統且獨立的變化：受法律保護的年齡、性別和種族屬性，以及面部表情、光線和姿勢。獨立且系統地操作臉部屬性，讓我們得以研究每個屬性對社會感知的影響，並避免在野外收集的資料中會出現的混淆因素，原因是屬性之間會出現不受控的系統性關聯。因此，我們的發現是實驗性的，而非觀察性的。我們的發現主要有三個。首先，儘管 CLIP 是在各種影像和文字上進行訓練，但它能夠對臉部影像做出細微的人類社會判斷。其次，年齡、性別和種族確實系統性地影響 CLIP 對臉部的社會感知，這表明 CLIP 在受法律保護的屬性方面存在不良偏見。最令人驚訝的是，我們發現了一個關於黑人女性臉部的強烈偏見模式，其中 CLIP 在不同的年齡和面部表情中產生了極端的社會感知值。第三，面部表情對社會感知的影響大於年齡，對光線的影響則與年齡相當。最後一項發現預測，未控制不受保護視覺屬性的研究可能會對偏見得出錯誤的結論。我們新穎的研究方法建立在社會心理學文獻和涉及操作個別屬性的實驗之上，比先前的觀察方法產生更清晰且更可靠的觀察結果，並可應用於研究任何視覺語言模型中的偏見。</paragraph>

##### **Contextual Bandit with Herding Effects: Algorithms and Recommendation Applications**
2408.14432v1 by Luyue Xu, Liming Wang, Hong Xie, Mingqiang Zhou

Contextual bandits serve as a fundamental algorithmic framework for
optimizing recommendation decisions online. Though extensive attention has been
paid to tailoring contextual bandits for recommendation applications, the
"herding effects" in user feedback have been ignored. These herding effects
bias user feedback toward historical ratings, breaking down the assumption of
unbiased feedback inherent in contextual bandits. This paper develops a novel
variant of the contextual bandit that is tailored to address the feedback bias
caused by the herding effects. A user feedback model is formulated to capture
this feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)
algorithm, which employs posterior sampling to balance the exploration and
exploitation tradeoff. We prove an upper bound for the regret of the algorithm,
revealing the impact of herding effects on learning speed. Extensive
experiments on datasets demonstrate that TS-Conf outperforms four benchmark
algorithms. Analysis reveals that TS-Conf effectively mitigates the negative
impact of herding effects, resulting in faster learning and improved
recommendation accuracy.

摘要：情境式強盜演算法是一個基本演算法架構，用於線上最佳化推薦決策。儘管已廣泛關注調整情境式強盜演算法以用於推薦應用程式，但使用者的回饋中的「從眾效應」卻被忽略了。這些從眾效應會使使用者的回饋偏向歷史評分，打破情境式強盜演算法中固有的無偏回饋假設。本文發展出一種新穎的情境式強盜演算法變體，專門用來解決由從眾效應造成的回饋偏差。制定了一個使用者回饋模型來捕捉這種回饋偏差。我們設計了 TS-Conf（從眾下的湯普森抽樣）演算法，該演算法採用後驗抽樣來平衡探索和開發的權衡。我們證明了該演算法的後悔值上限，揭示了從眾效應對學習速度的影響。在資料集上進行的廣泛實驗表明，TS-Conf 優於四種基準演算法。分析顯示，TS-Conf 有效減輕了從眾效應的負面影響，從而加快了學習速度並提高了推薦準確度。

##### **CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models**
2408.14419v1 by Shubham Bharti, Shiyun Cheng, Jihyun Rho, Martina Rao, Xiaojin Zhu

We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large
language models. CHARTOM consists of specially designed data visualizing
charts. Given a chart, a language model needs to not only correctly comprehend
the chart (the FACT question) but also judge if the chart will be misleading to
a human reader (the MIND question). Both questions have significant societal
benefits. We detail the construction of the CHARTOM benchmark including its
calibration on human performance.

摘要：我們介紹 CHARTOM，這是一個針對多模態大型語言模型的視覺心智理論基準。CHARTOM 包含特別設計的資料視覺化圖表。給定一個圖表，語言模型不僅需要正確理解圖表（事實問題），還需要判斷該圖表是否會對人類讀者造成誤導（心智問題）。這兩個問題都具有重大的社會效益。我們詳細說明了 CHARTOM 基準的建構，包括對人類表現的校準。

##### **MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**
2408.14418v1 by Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler

Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech
into text, yet the errors they introduce can significantly degrade the
performance of downstream tasks like summarization. This issue is particularly
pronounced in clinical dialogue summarization, a low-resource domain where
supervised data for fine-tuning is scarce, necessitating the use of ASR models
as black-box solutions. Employing conventional data augmentation for enhancing
the noise robustness of summarization models is not feasible either due to the
unavailability of sufficient medical dialogue audio recordings and
corresponding ASR transcripts. To address this challenge, we propose MEDSAGE,
an approach for generating synthetic samples for data augmentation using Large
Language Models (LLMs). Specifically, we leverage the in-context learning
capabilities of LLMs and instruct them to generate ASR-like errors based on a
few available medical dialogue examples with audio recordings. Experimental
results show that LLMs can effectively model ASR noise, and incorporating this
noisy data into the training process significantly improves the robustness and
accuracy of medical dialogue summarization systems. This approach addresses the
challenges of noisy ASR outputs in critical applications, offering a robust
solution to enhance the reliability of clinical dialogue summarization.

摘要：自動語音辨識 (ASR) 系統在將語音轉錄為文字方面至關重要，但它們造成的錯誤可能會顯著降低摘要等下游任務的效能。這個問題在臨床對話摘要中特別明顯，這是一個低資源的領域，其中用於微調的監督式資料很稀少，因此必須使用 ASR 模型作為黑盒解決方案。由於無法取得足夠的醫療對話音訊錄音和對應的 ASR 轉錄，因此採用傳統資料擴充來增強摘要模型的抗雜訊性也是不可行的。為了應對這個挑戰，我們提出了 MEDSAGE，這是一種使用大型語言模型 (LLM) 為資料擴充產生合成樣本的方法。具體來說，我們利用 LLM 的語境學習能力，並指示它們根據少數有音訊錄音的可用醫療對話範例產生類似的 ASR 錯誤。實驗結果顯示，LLM 能有效地模擬 ASR 雜訊，並且將這些雜訊資料納入訓練過程中，可以顯著改善醫療對話摘要系統的穩健性和準確性。這種方法應對了關鍵應用中 ASR 輸出有雜訊的挑戰，提供了一個穩健的解決方案來增強臨床對話摘要的可靠性。

##### **Language-specific Calibration for Pruning Multilingual Language Models**
2408.14398v1 by Simon Kurz, Zhixue Zhao, Jian-Jia Chen, Lucie Flek

Recent advances in large language model (LLM) pruning have shown
state-of-the-art compression results in post-training and retraining-free
settings while maintaining high predictive performance. However, such research
mainly considers calibrating pruning using English text, despite the
multilingual nature of modern LLMs and their frequent uses in non-English
languages. In this paper, we set out to explore effective strategies for
calibrating the pruning of multilingual language models. We present the first
comprehensive empirical study, comparing different calibration languages for
pruning multilingual models across diverse tasks, models, and state-of-the-art
pruning techniques. Our results present practical suggestions, for example,
calibrating in the target language can efficiently yield lower perplexity, but
does not necessarily benefit downstream tasks. Our further analysis experiments
unveil that calibration in the target language mainly contributes to preserving
language-specific features related to fluency and coherence, but might not
contribute to capturing language-agnostic features such as language
understanding and reasoning. Last, we provide practical recommendations for
future practitioners.

摘要：大型語言模型 (LLM) 剪枝的最新進展已顯示
在訓練後和無需重新訓練的設定中，最先進的壓縮結果，同時維持高預測效能。然而，此類研究
主要考量使用英文文字校準剪枝，儘管現代 LLM 的多語言性質及其在非英文
語言中的頻繁使用。在本文中，我們著手探索校準多語言語言模型剪枝的有效策略。我們提出第一個
全面的實證研究，針對不同校準語言比較多語言模型的剪枝，涵蓋各種任務、模型和最先進
的剪枝技術。我們的結果提出實用的建議，例如，
以目標語言校準可以有效降低困惑度，但
不一定有利於下游任務。我們進一步的分析實驗
揭示以目標語言校準主要有助於保留與流暢度和連貫性相關的語言特定特徵，但可能
無助於擷取與語言無關的特徵，例如語言理解和推理。最後，我們為
未來的從業人員提供實用的建議。

##### **Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**
2408.14397v1 by Xiaoman Zhang, Julián N. Acosta, Hong-Yu Zhou, Pranav Rajpurkar

Recent advancements in artificial intelligence have significantly improved
the automatic generation of radiology reports. However, existing evaluation
methods fail to reveal the models' understanding of radiological images and
their capacity to achieve human-level granularity in descriptions. To bridge
this gap, we introduce a system, named ReXKG, which extracts structured
information from processed reports to construct a comprehensive radiology
knowledge graph. We then propose three metrics to evaluate the similarity of
nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs
(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative
analysis of AI-generated and human-written radiology reports, assessing the
performance of both specialist and generalist models. Our study provides a
deeper understanding of the capabilities and limitations of current AI models
in radiology report generation, offering valuable insights for improving model
performance and clinical applicability.

摘要：近期人工智能的進展顯著改善了放射報告的自動生成。然而，現有的評估方法無法揭示模型對放射影像的理解，以及它們在描述中達到人類層級精細度的能力。為了彌補這個差距，我們引進一個名為 ReXKG 的系統，它從處理過的報告中萃取出結構化的資訊，以建構一個全面的放射知識圖譜。接著，我們提出三個指標來評估各種知識圖譜中節點的相似性 (ReXKG-NSC)、邊緣的分布 (ReXKG-AMS) 和子圖的涵蓋範圍 (ReXKG-SCS)。我們對 AI 生成的和人類撰寫的放射報告進行深入的比較分析，評估專家和通才模型的效能。我們的研究提供對目前 AI 模型在放射報告生成中的能力和限制更深入的理解，並提供有價值的見解來改善模型效能和臨床應用。

##### **Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning**
2408.14387v1 by Sakhinana Sagar Srinivas, Chidaksh Ravuru, Geethan Sannidhi, Venkataramana Runkana

Spatio-temporal forecasting plays a crucial role in various sectors such as
transportation systems, logistics, and supply chain management. However,
existing methods are limited by their ability to handle large, complex
datasets. To overcome this limitation, we introduce a hybrid approach that
combines the strengths of open-source large and small-scale language models
(LLMs and LMs) with traditional forecasting methods. We augment traditional
methods with dynamic prompting and a grouped-query, multi-head attention
mechanism to more effectively capture both intra-series and inter-series
dependencies in evolving nonlinear time series data. In addition, we facilitate
on-premises customization by fine-tuning smaller open-source LMs for time
series trend analysis utilizing descriptions generated by open-source large LMs
on consumer-grade hardware using Low-Rank Adaptation with Activation Memory
Reduction (LoRA-AMR) technique to reduce computational overhead and activation
storage memory demands while preserving inference latency. We combine language
model processing for time series trend analysis with traditional time series
representation learning method for cross-modal integration, achieving robust
and accurate forecasts. The framework effectiveness is demonstrated through
extensive experiments on various real-world datasets, outperforming existing
methods by significant margins in terms of forecast accuracy.

摘要：時空預測在各種領域中扮演著至關重要的角色，例如運輸系統、物流和供應鏈管理。然而，現有方法受到處理大型複雜資料集的能力限制。為了克服這個限制，我們引入了一個混合方法，結合了開放原始碼大型和小型語言模型（LLM 和 LM）的優勢以及傳統預測方法。我們使用動態提示和分組查詢、多頭注意力機制來擴充傳統方法，以更有效地捕捉不斷變化的非線性時間序列資料中的序列內和序列間依賴性。此外，我們透過微調較小的開放原始碼 LM 來進行內部部署自訂，以利用開放原始碼大型 LM 在消費級硬體上產生的描述來進行時間序列趨勢分析，使用低階適應和激活記憶體減少（LoRA-AMR）技術來降低運算負擔和激活儲存記憶體需求，同時保持推論延遲。我們將語言模型處理與時間序列趨勢分析結合傳統的時間序列表示學習方法進行跨模式整合，實現穩健且準確的預測。透過在各種真實世界資料集上進行廣泛的實驗，證明了這個架構的有效性，在預測準確性方面大幅優於現有方法。

##### **Probing Causality Manipulation of Large Language Models**
2408.14380v1 by Chenyang Zhang, Haibo Tong, Bin Zhang, Dongyu Zhang

Large language models (LLMs) have shown various ability on natural language
processing, including problems about causality. It is not intuitive for LLMs to
command causality, since pretrained models usually work on statistical
associations, and do not focus on causes and effects in sentences. So that
probing internal manipulation of causality is necessary for LLMs. This paper
proposes a novel approach to probe causality manipulation hierarchically, by
providing different shortcuts to models and observe behaviors. We exploit
retrieval augmented generation (RAG) and in-context learning (ICL) for models
on a designed causality classification task. We conduct experiments on
mainstream LLMs, including GPT-4 and some smaller and domain-specific models.
Our results suggest that LLMs can detect entities related to causality and
recognize direct causal relationships. However, LLMs lack specialized cognition
for causality, merely treating them as part of the global semantic of the
sentence.

摘要：大型語言模型 (LLM) 在自然語言處理中展現了各種能力，包括因果關係問題。對於 LLM 來說，理解因果關係並非直覺，因為預訓練模型通常建立於統計關聯，並不專注於句子中的原因和結果。因此，探討 LLM 對因果關係的內部操作是必要的。本文提出了一種新穎的方法，透過提供不同的捷徑給模型並觀察行為，來分層探討因果關係操作。我們利用檢索擴增生成 (RAG) 和情境內學習 (ICL) 來處理模型在設計的因果關係分類任務上。我們對主流 LLM 進行實驗，包括 GPT-4 和一些較小且特定於領域的模型。我們的結果表明，LLM 可以偵測與因果關係相關的實體，並辨識直接的因果關係。然而，LLM 缺乏對因果關係的專門認知，僅將其視為句子整體語意的部分。

##### **SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery**
2408.14371v1 by Sarah Rastegar, Mohammadreza Salehi, Yuki M. Asano, Hazel Doughty, Cees G. M. Snoek

In this paper, we address Generalized Category Discovery, aiming to
simultaneously uncover novel categories and accurately classify known ones.
Traditional methods, which lean heavily on self-supervision and contrastive
learning, often fall short when distinguishing between fine-grained categories.
To address this, we introduce a novel concept called `self-expertise', which
enhances the model's ability to recognize subtle differences and uncover
unknown categories. Our approach combines unsupervised and supervised
self-expertise strategies to refine the model's discernment and generalization.
Initially, hierarchical pseudo-labeling is used to provide `soft supervision',
improving the effectiveness of self-expertise. Our supervised technique differs
from traditional methods by utilizing more abstract positive and negative
samples, aiding in the formation of clusters that can generalize to novel
categories. Meanwhile, our unsupervised strategy encourages the model to
sharpen its category distinctions by considering within-category examples as
`hard' negatives. Supported by theoretical insights, our empirical results
showcase that our method outperforms existing state-of-the-art techniques in
Generalized Category Discovery across several fine-grained datasets. Our code
is available at: https://github.com/SarahRastegar/SelEx.

摘要：在本文中，我们探讨廣義類別發現，旨在同時發現新類別並準確分類已知類別。傳統方法過度依賴自我監督和對比學習，在區分細粒度類別時常常力不從心。為了解決此問題，我們引入了一個稱為「自我專業知識」的新概念，它增強了模型識別細微差異和發現未知類別的能力。我們的做法結合了無監督和監督自我專業知識策略，以優化模型的辨識和概化能力。最初，層次偽標籤用於提供「軟監督」，提高自我專業知識的有效性。我們的監督技術不同於傳統方法，它利用更抽象的正負樣本，協助形成可以概化到新類別的群集。同時，我們的無監督策略鼓勵模型將類別內部範例視為「硬」負例，藉此強化其類別區分能力。在理論見解的支援下，我們的實證結果顯示，我們的模型在廣義類別發現方面優於現有的最先進技術，且涵蓋多個細粒度資料集。我們的程式碼可在 https://github.com/SarahRastegar/SelEx 取得。

##### **GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal Conditioned Policy**
2408.14368v1 by Peiyan Li, Hongtao Wu, Yan Huang, Chilam Cheang, Liang Wang, Tao Kong

The robotics community has consistently aimed to achieve generalizable robot
manipulation with flexible natural language instructions. One of the primary
challenges is that obtaining robot data fully annotated with both actions and
texts is time-consuming and labor-intensive. However, partially annotated data,
such as human activity videos without action labels and robot play data without
language labels, is much easier to collect. Can we leverage these data to
enhance the generalization capability of robots? In this paper, we propose
GR-MG, a novel method which supports conditioning on both a language
instruction and a goal image. During training, GR-MG samples goal images from
trajectories and conditions on both the text and the goal image or solely on
the image when text is unavailable. During inference, where only the text is
provided, GR-MG generates the goal image via a diffusion-based image-editing
model and condition on both the text and the generated image. This approach
enables GR-MG to leverage large amounts of partially annotated data while still
using language to flexibly specify tasks. To generate accurate goal images, we
propose a novel progress-guided goal image generation model which injects task
progress information into the generation process, significantly improving the
fidelity and the performance. In simulation experiments, GR-MG improves the
average number of tasks completed in a row of 5 from 3.35 to 4.04. In
real-robot experiments, GR-MG is able to perform 47 different tasks and
improves the success rate from 62.5% to 75.0% and 42.4% to 57.6% in simple and
generalization settings, respectively. Code and checkpoints will be available
at the project page: https://gr-mg.github.io/.

摘要：機器人社群持續致力於透過靈活的自然語言指令，達成可概化的機器人操作。其中一項主要的挑戰在於，要取得同時標註動作與文字的機器人資料，非常耗時且費力。然而，部分標註的資料，例如沒有動作標籤的人類活動影片，以及沒有語言標籤的機器人遊戲資料，就容易取得許多。我們能利用這些資料來提升機器人的概化能力嗎？在本文中，我們提出 GR-MG，這是一種新穎的方法，支援以語言指令和目標影像為條件。在訓練期間，GR-MG 從軌跡中取樣目標影像，並以文字和目標影像為條件，或是在沒有文字時，僅以影像為條件。在推理期間，僅提供文字時，GR-MG 會透過基於擴散的影像編輯模型生成目標影像，並以文字和生成的影像為條件。這個方法讓 GR-MG 能夠利用大量的部分標註資料，同時仍然使用語言來靈活地指定任務。為了產生精確的目標影像，我們提出一個新穎的進度引導目標影像生成模型，將任務進度資訊注入到生成過程中，大幅提升保真度和效能。在模擬實驗中，GR-MG 將連續完成任務的平均次數從 3.35 提升至 4.04。在真實機器人實驗中，GR-MG 能執行 47 項不同的任務，並將成功率從 62.5% 提升至 75.0%，以及在簡單和概化設定中從 42.4% 提升至 57.6%。程式碼和檢查點將會在專案頁面提供：https://gr-mg.github.io/。

##### **SWE-bench-java: A GitHub Issue Resolving Benchmark for Java**
2408.14354v1 by Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, Dezhi Ran, Muhan Zeng, Bo Shen, Pan Bian, Guangtai Liang, Bei Guan, Pengjie Huang, Tao Xie, Yongji Wang, Qianxiang Wang

GitHub issue resolving is a critical task in software engineering, recently
gaining significant attention in both industry and academia. Within this task,
SWE-bench has been released to evaluate issue resolving capabilities of large
language models (LLMs), but has so far only focused on Python version. However,
supporting more programming languages is also important, as there is a strong
demand in industry. As a first step toward multilingual support, we have
developed a Java version of SWE-bench, called SWE-bench-java. We have publicly
released the dataset, along with the corresponding Docker-based evaluation
environment and leaderboard, which will be continuously maintained and updated
in the coming months. To verify the reliability of SWE-bench-java, we implement
a classic method SWE-agent and test several powerful LLMs on it. As is well
known, developing a high-quality multi-lingual benchmark is time-consuming and
labor-intensive, so we welcome contributions through pull requests or
collaboration to accelerate its iteration and refinement, paving the way for
fully automated programming.

摘要：GitHub 議題解決在軟體工程中是一項關鍵任務，最近在產業和學術界都獲得了顯著的關注。在此任務中，SWE-bench 已被釋出，用於評估大型語言模型 (LLM) 的議題解決能力，但到目前為止僅專注於 Python 版本。然而，支援更多程式語言也很重要，因為產業中有強烈的需求。作為邁向多語言支援的第一步，我們已經開發了 SWE-bench 的 Java 版本，稱為 SWE-bench-java。我們已經公開發布了資料集，以及對應的基於 Docker 的評估環境和排行榜，這些資料將在未來幾個月持續維護和更新。為了驗證 SWE-bench-java 的可靠性，我們實作了一個經典方法 SWE-agent，並在它上面測試了幾個功能強大的 LLM。眾所周知，開發一個高品質的多語言基準是耗時且費力的，因此我們歡迎透過 pull request 或合作來加速它的反覆運算和精進，為全自動化程式設計鋪路。

##### **Assessing Contamination in Large Language Models: Introducing the LogProber method**
2408.14352v1 by Nicolas Yax, Pierre-Yves Oudeyer, Stefano Palminteri

In machine learning, contamination refers to situations where testing data
leak into the training set. The issue is particularly relevant for the
evaluation of the performance of Large Language Models (LLMs), which are
generally trained on gargantuan, and generally opaque, corpora of text scraped
from the world wide web. Developing tools to detect contamination is therefore
crucial to be able to fairly and properly track the evolution of the
performance of LLMs. Most recent works in the field are not tailored to
quantify contamination on short sequences of text like we find in psychology
questionnaires. In the present paper we introduce LogProber, a novel,
efficient, algorithm that we show able to detect contamination using token
probability in given sentences. In the second part we investigate the
limitations of the method and discuss how different training methods can
contaminate models without leaving traces in the token probabilities.

摘要：在機器學習中，污染是指測試資料滲漏到訓練組的情況。這個問題特別與大型語言模型 (LLM) 的效能評估有關，這些模型通常在從萬維網擷取的龐大且通常不透明的文字語料庫上進行訓練。因此，開發用於偵測污染的工具對於能夠公平且適當地追蹤 LLM 效能的演進至關重要。該領域最新的研究並不適合量化我們在心理問卷中發現的短文字序列上的污染。在本篇論文中，我們介紹 LogProber，一種新穎且有效率的演算法，我們展示它能夠使用給定句子中的標記機率來偵測污染。在第二部分中，我們探討該方法的限制，並討論不同的訓練方法如何在不留下標記機率痕跡的情況下污染模型。

##### **Foundation Models for Music: A Survey**
2408.14340v1 by Yinghao Ma, Anders Øland, Anton Ragni, Bleiz MacSen Del Sette, Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elio Quinton, Elona Shatri, Fabio Morreale, Ge Zhang, György Fazekas, Gus Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin, Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B. Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shuqi Dai, Shun Lei, Shiyin Kang, Simon Dixon, Wenhu Chen, Wehhao Huang, Xingjian Du, Xingwei Qu, Xu Tan, Yizhi Li, Zeyue Tian, Zhiyong Wu, Zhizheng Wu, Ziyang Ma, Ziyu Wang

In recent years, foundation models (FMs) such as large language models (LLMs)
and latent diffusion models (LDMs) have profoundly impacted diverse sectors,
including music. This comprehensive review examines state-of-the-art (SOTA)
pre-trained models and foundation models in music, spanning from representation
learning, generative learning and multimodal learning. We first contextualise
the significance of music in various industries and trace the evolution of AI
in music. By delineating the modalities targeted by foundation models, we
discover many of the music representations are underexplored in FM development.
Then, emphasis is placed on the lack of versatility of previous methods on
diverse music applications, along with the potential of FMs in music
understanding, generation and medical application. By comprehensively exploring
the details of the model pre-training paradigm, architectural choices,
tokenisation, finetuning methodologies and controllability, we emphasise the
important topics that should have been well explored, like instruction tuning
and in-context learning, scaling law and emergent ability, as well as
long-sequence modelling etc. A dedicated section presents insights into music
agents, accompanied by a thorough analysis of datasets and evaluations
essential for pre-training and downstream tasks. Finally, by underscoring the
vital importance of ethical considerations, we advocate that following research
on FM for music should focus more on such issues as interpretability,
transparency, human responsibility, and copyright issues. The paper offers
insights into future challenges and trends on FMs for music, aiming to shape
the trajectory of human-AI collaboration in the music realm.

摘要：近年來，諸如大型語言模型（LLM）和潛在擴散模型（LDM）等基礎模型（FM）已對包括音樂在內的不同領域產生了深遠的影響。這篇綜合評論探討了音樂中最先進（SOTA）的預訓練模型和基礎模型，涵蓋表徵學習、生成學習和多模態學習。我們首先將音樂在各產業中的重要性脈絡化，並追溯 AI 在音樂中的演進。透過描繪基礎模型所針對的模態，我們發現許多音樂表徵在 FM 開發中仍未被充分探索。然後，我們強調先前方法在不同音樂應用中缺乏多樣性，以及 FM 在音樂理解、生成和醫療應用中的潛力。透過全面探討模型預訓練範例、架構選擇、符號化、微調方法和可控性的細節，我們強調了應深入探討的重要主題，例如指令調整和情境學習、規模法則和新興能力，以及長序列建模等。專門的部分提供了對音樂代理的見解，並伴隨著對預訓練和下游任務至關重要的資料集和評估的深入分析。最後，透過強調倫理考量的至關重要性，我們主張後續關於音樂 FM 的研究應更專注於可解釋性、透明度、人類責任和版權等問題。本文提供了對音樂 FM 未來挑戰和趨勢的見解，旨在塑造人類與 AI 在音樂領域中合作的軌跡。

##### **Machine Learning for Quantifier Selection in cvc5**
2408.14338v1 by Jan Jakubův, Mikoláš Janota, Jelle Piepenbrock, Josef Urban

In this work we considerably improve the state-of-the-art SMT solving on
first-order quantified problems by efficient machine learning guidance of
quantifier selection. Quantifiers represent a significant challenge for SMT and
are technically a source of undecidability. In our approach, we train an
efficient machine learning model that informs the solver which quantifiers
should be instantiated and which not. Each quantifier may be instantiated
multiple times and the set of the active quantifiers changes as the solving
progresses. Therefore, we invoke the ML predictor many times, during the whole
run of the solver. To make this efficient, we use fast ML models based on
gradient boosting decision trees. We integrate our approach into the
state-of-the-art cvc5 SMT solver and show a considerable increase of the
system's holdout-set performance after training it on a large set of
first-order problems collected from the Mizar Mathematical Library.

摘要：在這項工作中，我們通過有效的量詞選擇機器學習指導，大幅改進了 SMT 解決一階量化問題的最新技術。量詞代表了 SMT 的重大挑戰，並且在技術上是不可判定性的來源。在我們的做法中，我們訓練了一個有效的機器學習模型，告知求解器應該實例化哪些量詞，哪些不應該。每個量詞可以被實例化多次，並且活動量詞的集合會隨著求解進度而改變。因此，我們在求解器的整個運行過程中多次調用 ML 預測器。為了提高效率，我們使用基於梯度提升決策樹的快速 ML 模型。我們將我們的做法整合到最新技術的 cvc5 SMT 求解器中，並在從 Mizar 數學庫收集的大量一階問題上對其進行訓練後，顯示系統的留出集效能大幅提升。

##### **PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection Dataset**
2408.14329v1 by Ghazal Alinezhad Noghre, Shanle Yao, Armin Danesh Pazho, Babak Rahimi Ardabili, Vinit Katariya, Hamed Tabkhi

PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection
dataset. By removing pixel information and providing only de-identified human
annotations, PHEVA safeguards personally identifiable information. The dataset
includes seven indoor/outdoor scenes, featuring one novel, context-specific
camera, and offers over 5x the pose-annotated frames compared to the largest
previous dataset. This study benchmarks state-of-the-art methods on PHEVA using
a comprehensive set of metrics, including the 10% Error Rate (10ER), a metric
used for anomaly detection for the first time providing insights relevant to
real-world deployment. As the first of its kind, PHEVA bridges the gap between
conventional training and real-world deployment by introducing continual
learning benchmarks, with models outperforming traditional methods in 82.14% of
cases. The dataset is publicly available at
https://github.com/TeCSAR-UNCC/PHEVA.git.

摘要：PHEVA，一個注重隱私的人本道德影片異常偵測資料集。透過移除像素資訊並僅提供去識別化的人類註解，PHEVA 保護個人可識別資訊。該資料集包含七個室內/室外場景，具備一個新穎的背景特定相機，並提供比先前最大的資料集多出 5 倍以上的姿勢註解影格。這項研究使用全面的指標集，包括 10% 錯誤率 (10ER)，這是一個首次用於異常偵測的指標，提供與實際世界部署相關的見解，來評量 PHEVA 上的最新方法。作為同類型中的第一個，PHEVA 透過引入持續學習基準，縮小了傳統訓練與實際世界部署之間的差距，在 82.14% 的案例中，模型優於傳統方法。該資料集可於 https://github.com/TeCSAR-UNCC/PHEVA.git 公開取得。

##### **Streamline tractography of the fetal brain in utero with machine learning**
2408.14326v1 by Weide Liu, Camilo Calixto, Simon K. Warfield, Davood Karimi

Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive
tool for studying white matter tracts and structural connectivity of the brain.
These assessments rely heavily on tractography techniques, which reconstruct
virtual streamlines representing white matter fibers. Much effort has been
devoted to improving tractography methodology for adult brains, while
tractography of the fetal brain has been largely neglected. Fetal tractography
faces unique difficulties due to low dMRI signal quality, immature and rapidly
developing brain structures, and paucity of reference data. This work presents
the first machine learning model for fetal tractography. The model input
consists of five sources of information: (1) Fiber orientation, inferred from a
diffusion tensor fit to the dMRI signal; (2) Directions of recent propagation
steps; (3) Global spatial information, encoded as distances to keypoints in the
brain cortex; (4) Tissue segmentation information; and (5) Prior information
about the expected local fiber orientations supplied with an atlas. In order to
mitigate the local tensor estimation error, a large spatial context around the
current point in the diffusion tensor image is encoded using convolutional and
attention neural network modules. Moreover, the diffusion tensor information at
a hypothetical next point is included in the model input. Filtering rules based
on anatomically constrained tractography are applied to prune implausible
streamlines. We trained the model on manually-refined whole-brain fetal
tractograms and validated the trained model on an independent set of 11 test
scans with gestational ages between 23 and 36 weeks. Results show that our
proposed method achieves superior performance across all evaluated tracts. The
new method can significantly advance the capabilities of dMRI for studying
normal and abnormal brain development in utero.

摘要：擴散加權磁振造影 (dMRI) 是唯一用於研究白質束和腦部結構連接性的非侵入性工具。這些評估高度依賴於纖維束描描術，它會重建代表白質纖維的虛擬流線。許多研究致力於改善成人腦部纖維束描描術方法，而胎兒腦部的纖維束描描術則很大程度上被忽略。胎兒纖維束描描術面臨獨特的難題，包括 dMRI 訊號品質低、腦部結構不成熟且快速發展，以及缺乏參考資料。這項研究提出了第一個用於胎兒纖維束描描術的機器學習模型。模型輸入包括五種資訊來源：(1) 纖維方向，從 dMRI 訊號推論而得的擴散張量擬合；(2) 最近傳播步驟的方向；(3) 全域空間資訊，編碼為腦皮質關鍵點的距離；(4) 組織分割資訊；以及 (5) 圖譜提供的預期局部纖維方向的先驗資訊。為了減輕局部張量估計誤差，使用卷積和注意神經網路模組編碼擴散張量影像中當前點周圍的大量空間脈絡。此外，假設下一個點的擴散張量資訊也包含在模型輸入中。基於解剖限制纖維束描描術的過濾規則用於修剪難以置信的流線。我們在人工精修的全腦胎兒纖維束描描圖上訓練模型，並在包含 11 個測試掃描的獨立集上驗證訓練後的模型，這些掃描的胎齡介於 23 到 36 週之間。結果顯示，我們提出的方法在所有評估束中都達到優異的效能。新方法可以大幅提升 dMRI 在研究子宮內正常和異常腦部發育方面的能力。

##### **Claim Verification in the Age of Large Language Models: A Survey**
2408.14317v1 by Alphaeus Dmonte, Roland Oruche, Marcos Zampieri, Prasad Calyam, Isabelle Augenstein

The large and ever-increasing amount of data available on the Internet
coupled with the laborious task of manual claim and fact verification has
sparked the interest in the development of automated claim verification
systems. Several deep learning and transformer-based models have been proposed
for this task over the years. With the introduction of Large Language Models
(LLMs) and their superior performance in several NLP tasks, we have seen a
surge of LLM-based approaches to claim verification along with the use of novel
methods such as Retrieval Augmented Generation (RAG). In this survey, we
present a comprehensive account of recent claim verification frameworks using
LLMs. We describe the different components of the claim verification pipeline
used in these frameworks in detail including common approaches to retrieval,
prompting, and fine-tuning. Finally, we describe publicly available English
datasets created for this task.

摘要：網路上的資料量龐大且持續增加，加上人工查證和事實查證的繁瑣工作，激發了開發自動化查證系統的興趣。多年來，已經針對這項任務提出了多種深度學習和基於轉換器的模型。隨著大型語言模型 (LLM) 的推出及其在多項 NLP 任務中的卓越表現，我們已經看到大量基於 LLM 的查證方法，以及檢索擴充生成 (RAG) 等新方法的使用。在這項調查中，我們全面說明了使用 LLM 的近期查證架構。我們詳細描述了這些架構中使用的查證管線的不同組成部分，包括檢索、提示和微調的常見方法。最後，我們描述了為此任務建立的公開可用的英文資料集。

##### **LLM-3D Print: Large Language Models To Monitor and Control 3D Printing**
2408.14307v1 by Yayati Jadhav, Peter Pak, Amir Barati Farimani

Industry 4.0 has revolutionized manufacturing by driving digitalization and
shifting the paradigm toward additive manufacturing (AM). Fused Deposition
Modeling (FDM), a key AM technology, enables the creation of highly customized,
cost-effective products with minimal material waste through layer-by-layer
extrusion, posing a significant challenge to traditional subtractive methods.
However, the susceptibility of material extrusion techniques to errors often
requires expert intervention to detect and mitigate defects that can severely
compromise product quality. While automated error detection and machine
learning models exist, their generalizability across diverse 3D printer setups,
firmware, and sensors is limited, and deep learning methods require extensive
labeled datasets, hindering scalability and adaptability. To address these
challenges, we present a process monitoring and control framework that
leverages pre-trained Large Language Models (LLMs) alongside 3D printers to
detect and address printing defects. The LLM evaluates print quality by
analyzing images captured after each layer or print segment, identifying
failure modes and querying the printer for relevant parameters. It then
generates and executes a corrective action plan. We validated the effectiveness
of the proposed framework in identifying defects by comparing it against a
control group of engineers with diverse AM expertise. Our evaluation
demonstrated that LLM-based agents not only accurately identify common 3D
printing errors, such as inconsistent extrusion, stringing, warping, and layer
adhesion, but also effectively determine the parameters causing these failures
and autonomously correct them without any need for human intervention.

摘要：工業 4.0 透過推動數位化並將典範轉移到增材製造 (AM) 而徹底改變了製造業。熔融沉積建模 (FDM) 是一項關鍵的 AM 技術，可透過逐層擠壓來建立高度客製化、成本效益高的產品，且材料浪費極少，對傳統的減材法構成重大挑戰。然而，材料擠壓技術容易發生錯誤，因此經常需要專家介入來偵測並減輕可能會嚴重損害產品品質的缺陷。雖然有自動化錯誤偵測和機器學習模型，但它們在不同的 3D 印表機設定、韌體和感測器之間的普遍性有限，而且深度學習方法需要大量的標籤化資料集，這會阻礙可擴充性和適應性。為了應對這些挑戰，我們提出了一個製程監控和控制架構，它利用預先訓練的大語言模型 (LLM) 和 3D 印表機來偵測和處理列印缺陷。LLM 透過分析在每層或列印區段後擷取的影像來評估列印品質，識別故障模式並查詢印表機以取得相關參數。然後，它會產生並執行一個修正動作計畫。我們透過將提議的架構與具備不同 AM 專業知識的工程師控制組進行比較，驗證了其在識別缺陷方面的有效性。我們的評估顯示，基於 LLM 的代理程式不僅可以準確識別常見的 3D 列印錯誤，例如不一致的擠壓、牽絲、翹曲和層黏著，還能有效地確定造成這些故障的參數，並在無需人工介入的情況下自動修正它們。

##### **Predictability and Causality in Spanish and English Natural Language Generation**
2408.14283v1 by Andrea Busto-Castiñeira, Francisco J. González-Castaño, Silvia García-Méndez, Francisco de Arriba-Pérez

In recent years, the field of Natural Language Generation (NLG) has been
boosted by the recent advances in deep learning technologies. Nonetheless,
these new data-intensive methods introduce language-dependent disparities in
NLG as the main training data sets are in English. Also, most neural NLG
systems use decoder-only (causal) transformer language models, which work well
for English, but were not designed with other languages in mind. In this work
we depart from the hypothesis that they may introduce generation bias in target
languages with less rigid word ordering, subject omission, or different
attachment preferences for relative clauses, so that for these target languages
other language generation strategies may be more desirable. This paper first
compares causal and non-causal language modeling for English and Spanish, two
languages with different grammatical structures and over 1.5 billion and 0.5
billion speakers, respectively. For this purpose, we define a novel metric of
average causal and non-causal context-conditioned entropy of the grammatical
category distribution for both languages as an information-theoretic a priori
approach. The evaluation of natural text sources (such as training data) in
both languages reveals lower average non-causal conditional entropy in Spanish
and lower causal conditional entropy in English. According to this experiment,
Spanish is more predictable than English given a non-causal context. Then, by
applying a conditional relative entropy metric to text generation experiments,
we obtain as insights that the best performance is respectively achieved with
causal NLG in English, and with non-causal NLG in Spanish. These insights
support further research in NLG in Spanish using bidirectional transformer
language models.

摘要：近年來，自然語言生成（NLG）領域因深度學習技術的最新進展而獲得提升。儘管如此，這些新的資料密集方法引入了 NLG 中依賴語言的差異，因為主要的訓練資料集都是英文。此外，大多數神經 NLG 系統使用僅解碼器（因果）Transformer語言模型，這些模型適用於英文，但並非針對其他語言設計。在這項工作中，我們跳脫了它們可能會在目標語言中引入生成偏差的假設，這些語言的詞序較不嚴謹、省略主詞，或對關聯子句有不同的依附偏好，因此對於這些目標語言，其他語言生成策略可能更可取。本文首先比較了因果和非因果語言建模，比較英文和西班牙文這兩種語法結構不同的語言，分別有超過 15 億和 5 億的使用者。為此，我們定義了一個新的指標，即語法類別分佈的平均因果和非因果條件熵，作為這兩種語言的資訊理論先驗方法。對這兩種語言的自然文字來源（例如訓練資料）的評估顯示，西班牙文的平均非因果條件熵較低，而英文的因果條件熵較低。根據這個實驗，在非因果條件下，西班牙文比英文更具可預測性。然後，透過將條件相對熵指標應用於文字生成實驗，我們獲得的見解是，在英文中使用因果 NLG，在西班牙文中使用非因果 NLG，分別能獲得最佳效能。這些見解支持使用雙向Transformer語言模型進一步研究西班牙文的 NLG。

##### **Uncertainties of Latent Representations in Computer Vision**
2408.14281v1 by Michael Kirchhof

Uncertainty quantification is a key pillar of trustworthy machine learning.
It enables safe reactions under unsafe inputs, like predicting only when the
machine learning model detects sufficient evidence, discarding anomalous data,
or emitting warnings when an error is likely to be inbound. This is
particularly crucial in safety-critical areas like medical image classification
or self-driving cars. Despite the plethora of proposed uncertainty
quantification methods achieving increasingly higher scores on performance
benchmarks, uncertainty estimates are often shied away from in practice. Many
machine learning projects start from pretrained latent representations that
come without uncertainty estimates. Uncertainties would need to be trained by
practitioners on their own, which is notoriously difficult and
resource-intense.
  This thesis makes uncertainty estimates easily accessible by adding them to
the latent representation vectors of pretrained computer vision models. Besides
proposing approaches rooted in probability and decision theory, such as
Monte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both
theoretical and empirical questions. We show that these unobservable
uncertainties about unobservable latent representations are indeed provably
correct. We also provide an uncertainty-aware representation learning (URL)
benchmark to compare these unobservables against observable ground-truths.
Finally, we compile our findings to pretrain lightweight representation
uncertainties on large-scale computer vision models that transfer to unseen
datasets in a zero-shot manner.
  Our findings do not only advance the current theoretical understanding of
uncertainties over latent variables, but also facilitate the access to
uncertainty quantification for future researchers inside and outside the field,
enabling straightforward but trustworthy machine learning.

摘要：不確定量化是值得信賴機器學習的一大支柱。
它能讓機器學習模型在不安全的輸入下做出安全的反應，例如只在機器學習模型偵測到足夠證據時才進行預測、捨棄異常資料，或是在可能發生錯誤時發出警告。這在醫療影像分類或自駕車等安全關鍵領域中特別重要。儘管有許多已提出的不確定量化方法在效能基準上取得越來越高的分數，但在實務上卻常常迴避不確定性估計。許多機器學習專案從預訓練的潛在表徵開始，而這些表徵沒有不確定性估計。實務工作者需要自行訓練不確定性，這出了名的困難且耗費資源。
本論文透過將不確定性估計新增到預訓練電腦視覺模型的潛在表徵向量中，讓不確定性估計易於取得。除了提出植基於機率和決策理論的方法，例如蒙地卡羅資訊對比估計 (MCInfoNCE) 和損失預測之外，我們還深入探討理論和實證問題。我們證明這些關於不可觀察潛在表徵的不可觀察不確定性確實可以證明是正確的。我們還提供一個不確定性感知表徵學習 (URL) 基準，用來比較這些不可觀察的不確定性與可觀察的真實值。最後，我們將我們的發現彙整起來，在大型電腦視覺模型上預訓練輕量級表徵不確定性，並以零次學習的方式轉移到未見過的資料集。
我們的發現不僅提升了當前對潛在變數不確定性的理論理解，還促進了未來研究人員在該領域內外取得不確定量化，進而實現直接但值得信賴的機器學習。

##### **Epidemic Information Extraction for Event-Based Surveillance using Large Language Models**
2408.14277v1 by Sergio Consoli, Peter Markov, Nikolaos I. Stilianakis, Lorenzo Bertolini, Antonio Puertas Gallardo, Mario Ceresa

This paper presents a novel approach to epidemic surveillance, leveraging the
power of Artificial Intelligence and Large Language Models (LLMs) for effective
interpretation of unstructured big data sources, like the popular ProMED and
WHO Disease Outbreak News. We explore several LLMs, evaluating their
capabilities in extracting valuable epidemic information. We further enhance
the capabilities of the LLMs using in-context learning, and test the
performance of an ensemble model incorporating multiple open-source LLMs. The
findings indicate that LLMs can significantly enhance the accuracy and
timeliness of epidemic modelling and forecasting, offering a promising tool for
managing future pandemic events.

摘要：本文提出了一種流行病監控的新方法，利用人工智能和大型語言模型 (LLM) 的力量，有效解讀非結構化大數據源，例如流行的 ProMED 和 WHO 疾病爆發新聞。我們探討了多個 LLM，評估它們在提取有價值的流行病信息方面的能力。我們進一步使用情境學習增強 LLM 的能力，並測試了一個結合多個開源 LLM 的整體模型的性能。研究結果表明，LLM 可以顯著提高流行病建模和預測的準確性和及時性，為管理未來的流行病事件提供了一個有前途的工具。

##### **Self-supervised Speech Representations Still Struggle with African American Vernacular English**
2408.14262v1 by Kalvin Chang, Yi-Hui Chou, Jiatong Shi, Hsuan-Ming Chen, Nicole Holliday, Odette Scharenborg, David R. Mortensen

Underperformance of ASR systems for speakers of African American Vernacular
English (AAVE) and other marginalized language varieties is a well-documented
phenomenon, and one that reinforces the stigmatization of these varieties. We
investigate whether or not the recent wave of Self-Supervised Learning (SSL)
speech models can close the gap in ASR performance between AAVE and Mainstream
American English (MAE). We evaluate four SSL models (wav2vec 2.0, HuBERT,
WavLM, and XLS-R) on zero-shot Automatic Speech Recognition (ASR) for these two
varieties and find that these models perpetuate the bias in performance against
AAVE. Additionally, the models have higher word error rates on utterances with
more phonological and morphosyntactic features of AAVE. Despite the success of
SSL speech models in improving ASR for low resource varieties, SSL pre-training
alone may not bridge the gap between AAVE and MAE. Our code is publicly
available at https://github.com/cmu-llab/s3m-aave.

摘要：非裔美國方言 (AAVE) 及其他邊緣語言變體的 ASR 系統表現不佳，這是一個有據可查的現象，而且會加劇這些變體的污名化。我們探討最近一波的自我監督學習 (SSL) 語音模型是否能縮小 AAVE 和主流美式英語 (MAE) 之間的 ASR 效能差距。我們針對這兩種變體評估四個 SSL 模型（wav2vec 2.0、HuBERT、WavLM 和 XLS-R）的零次自動語音辨識 (ASR)，發現這些模型會延續對 AAVE 的效能偏見。此外，這些模型對於具有更多 AAVE 音韻和形態句法特徵的語句，其字元錯誤率較高。儘管 SSL 語音模型在改善低資源變體的 ASR 方面取得成功，但僅靠 SSL 預訓練可能無法縮小 AAVE 和 MAE 之間的差距。我們的程式碼已公開發布於 https://github.com/cmu-llab/s3m-aave。

##### **Text3DAug -- Prompted Instance Augmentation for LiDAR Perception**
2408.14253v1 by Laurenz Reichardt, Luca Uhr, Oliver Wasenmüller

LiDAR data of urban scenarios poses unique challenges, such as heterogeneous
characteristics and inherent class imbalance. Therefore, large-scale datasets
are necessary to apply deep learning methods. Instance augmentation has emerged
as an efficient method to increase dataset diversity. However, current methods
require the time-consuming curation of 3D models or costly manual data
annotation. To overcome these limitations, we propose Text3DAug, a novel
approach leveraging generative models for instance augmentation. Text3DAug does
not depend on labeled data and is the first of its kind to generate instances
and annotations from text. This allows for a fully automated pipeline,
eliminating the need for manual effort in practical applications. Additionally,
Text3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor
used. Comprehensive experimental analysis on LiDAR segmentation, detection and
novel class discovery demonstrates that Text3DAug is effective in supplementing
existing methods or as a standalone method, performing on par or better than
established methods, however while overcoming their specific drawbacks. The
code is publicly available.

摘要：城市場景的 LiDAR 資料會帶來獨特挑戰，例如異質特徵和內在類別不平衡。因此，需要大規模的資料集才能應用深度學習方法。實例擴充已成為增加資料集多樣性的有效方法。然而，現有方法需要耗時的 3D 模型整理或昂貴的手動資料標註。為了克服這些限制，我們提出 Text3DAug，一種利用生成模型進行實例擴充的新方法。Text3DAug 不依賴標籤資料，並且是同類方法中第一個從文字中生成實例和標註的方法。這允許建立全自動化流程，無需在實際應用中進行手動工作。此外，Text3DAug 與感測器無關，並且可以應用於任何 LiDAR 感測器。對 LiDAR 分割、偵測和新類別發現的全面實驗分析表明，Text3DAug 可有效補充現有方法或作為獨立方法，效能與既定方法相當或更好，但同時克服了其特定缺點。程式碼已公開。

##### **Beyond Few-shot Object Detection: A Detailed Survey**
2408.14249v1 by Vishal Chudasama, Hiran Sarkar, Pankaj Wasnik, Vineeth N Balasubramanian, Jayateja Kalla

Object detection is a critical field in computer vision focusing on
accurately identifying and locating specific objects in images or videos.
Traditional methods for object detection rely on large labeled training
datasets for each object category, which can be time-consuming and expensive to
collect and annotate. To address this issue, researchers have introduced
few-shot object detection (FSOD) approaches that merge few-shot learning and
object detection principles. These approaches allow models to quickly adapt to
new object categories with only a few annotated samples. While traditional FSOD
methods have been studied before, this survey paper comprehensively reviews
FSOD research with a specific focus on covering different FSOD settings such as
standard FSOD, generalized FSOD, incremental FSOD, open-set FSOD, and domain
adaptive FSOD. These approaches play a vital role in reducing the reliance on
extensive labeled datasets, particularly as the need for efficient machine
learning models continues to rise. This survey paper aims to provide a
comprehensive understanding of the above-mentioned few-shot settings and
explore the methodologies for each FSOD task. It thoroughly compares
state-of-the-art methods across different FSOD settings, analyzing them in
detail based on their evaluation protocols. Additionally, it offers insights
into their applications, challenges, and potential future directions in the
evolving field of object detection with limited data.

摘要：物體偵測是電腦視覺中的一個重要領域，專注於準確識別和定位影像或影片中的特定物體。傳統的物體偵測方法依賴於每個物體類別的大型標籤訓練資料集，而這可能需要花費大量時間和金錢來收集和註解。為了解決這個問題，研究人員引入了少量樣本物體偵測 (FSOD) 方法，結合了少量樣本學習和物體偵測原則。這些方法允許模型僅使用少數標註樣本就能快速適應新的物體類別。雖然傳統的 FSOD 方法以前已經研究過，但這篇綜述論文全面回顧了 FSOD 研究，特別專注於涵蓋不同的 FSOD 設定，例如標準 FSOD、廣義 FSOD、遞增 FSOD、開放式 FSOD 和領域自適應 FSOD。這些方法在減少對大量標籤資料集的依賴方面發揮了至關重要的作用，特別是在對高效機器學習模型的需求持續增加的情況下。這篇綜述論文旨在提供對上述少量樣本設定的全面理解，並探討每項 FSOD 任務的方法。它徹底比較了不同 FSOD 設定中的最先進方法，並根據其評估協定詳細分析它們。此外，它還提供了對其應用、挑戰和在資料有限的物體偵測領域中潛在未來方向的見解。

##### **Celtibero: Robust Layered Aggregation for Federated Learning**
2408.14240v1 by Borja Molina-Coronado

Federated Learning (FL) is an innovative approach to distributed machine
learning. While FL offers significant privacy advantages, it also faces
security challenges, particularly from poisoning attacks where adversaries
deliberately manipulate local model updates to degrade model performance or
introduce hidden backdoors. Existing defenses against these attacks have been
shown to be effective when the data on the nodes is identically and
independently distributed (i.i.d.), but they often fail under less restrictive,
non-i.i.d data conditions. To overcome these limitations, we introduce
Celtibero, a novel defense mechanism that integrates layered aggregation to
enhance robustness against adversarial manipulation. Through extensive
experiments on the MNIST and IMDB datasets, we demonstrate that Celtibero
consistently achieves high main task accuracy (MTA) while maintaining minimal
attack success rates (ASR) across a range of untargeted and targeted poisoning
attacks. Our results highlight the superiority of Celtibero over existing
defenses such as FL-Defender, LFighter, and FLAME, establishing it as a highly
effective solution for securing federated learning systems against
sophisticated poisoning attacks.

摘要：联邦学习 (FL) 是一种分布式机器学习的创新方法。虽然 FL 提供了显著的隐私优势，但它也面临着安全挑战，特别是来自中毒攻击，其中攻击者故意操纵本地模型更新以降低模型性能或引入隐藏的后门。现有的针对这些攻击的防御措施已被证明在节点上的数据是独立同分布 (i.i.d.) 时有效，但它们在不那么严格的非 i.i.d 数据条件下往往会失败。为了克服这些限制，我们引入了 Celtibero，这是一种新颖的防御机制，它集成了分层聚合以增强对对抗性操纵的鲁棒性。通过对 MNIST 和 IMDB 数据集的广泛实验，我们证明了 Celtibero 在一系列无目标和目标中毒攻击中始终实现较高的主任务准确度 (MTA)，同时保持最低的攻击成功率 (ASR)。我们的结果突出了 Celtibero 优于现有防御措施（如 FL-Defender、LFighter 和 FLAME），使其成为保护联邦学习系统免受复杂中毒攻击的高效解决方案。

##### **DSTI at LLMs4OL 2024 Task A: Intrinsic versus extrinsic knowledge for type classification**
2408.14236v1 by Hanna Abi Akl

We introduce semantic towers, an extrinsic knowledge representation method,
and compare it to intrinsic knowledge in large language models for ontology
learning. Our experiments show a trade-off between performance and semantic
grounding for extrinsic knowledge compared to a fine-tuned model intrinsic
knowledge. We report our findings on the Large Language Models for Ontology
Learning (LLMs4OL) 2024 challenge.

摘要：我們引入了語義塔，一種外在知識表示方法，
並將其與大型語言模型中的內在知識進行比較，以進行本体學習。我們的實驗顯示，與經過微調模型的內在知識相比，外在知識在性能和語義基礎之間存在權衡。我們報告了我們在大型語言模型中的發現，以進行本体學習 (LLMs4OL) 2024 挑戰。

##### **Gallery-Aware Uncertainty Estimation For Open-Set Face Recognition**
2408.14229v1 by Leonid Erlygin, Alexey Zaytsev

Accurately estimating image quality and model robustness improvement are
critical challenges in unconstrained face recognition, which can be addressed
through uncertainty estimation via probabilistic face embeddings. Previous
research mainly focused on uncertainty estimation in face verification, leaving
the open-set face recognition task underexplored. In open-set face recognition,
one seeks to classify an image, which could also be unknown. Here, the low
variance of probabilistic embedding does not imply a low error probability: an
image embedding could be close to several classes in a gallery, thus yielding
high uncertainty. We propose a method aware of two sources of ambiguity in the
open-set recognition system: (1) the gallery uncertainty caused by overlapping
classes and (2) the uncertainty of the face embeddings. To detect both types,
we use a Bayesian probabilistic model of embedding distribution, which provides
a principled uncertainty estimate. Challenging open-set face recognition
datasets, such as IJB-C, serve as a testbed for our method. We also propose a
new open-set recognition protocol for whale and dolphin identification. The
proposed approach better identifies recognition errors than uncertainty
estimation methods based solely on image quality.

摘要：準確估計影像品質和模型健壯性提升是無約束人臉辨識的重大挑戰，這可透過機率人臉嵌入的非確定性估計來解決。先前的研究主要集中於人臉驗證中的非確定性估計，而未探討開放式人臉辨識任務。在開放式人臉辨識中，人們試圖對影像進行分類，而影像也可能是未知的。在此，機率嵌入的低變異性並不意味著低錯誤機率：影像嵌入可能接近圖庫中的多個類別，因此產生高非確定性。我們提出一個方法，了解開放式辨識系統中兩個含糊來源：(1) 重疊類別造成的圖庫非確定性，以及 (2) 人臉嵌入的非確定性。為了偵測這兩種類型，我們使用嵌入分佈的貝氏機率模型，這提供了有原則的非確定性估計。具挑戰性的開放式人臉辨識資料集，例如 IJB-C，可做為我們方法的測試平台。我們也提出一個新的開放式辨識協定，用於鯨魚和海豚辨識。所提出的方法比僅根據影像品質的非確定性估計方法更能辨識出辨識錯誤。

##### **MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement**
2408.14211v1 by Xu He, Xiaoyu Li, Di Kang, Jiangnan Ye, Chaopeng Zhang, Liyang Chen, Xiangjun Gao, Han Zhang, Zhiyong Wu, Haolin Zhuang

Existing works in single-image human reconstruction suffer from weak
generalizability due to insufficient training data or 3D inconsistencies for a
lack of comprehensive multi-view knowledge. In this paper, we introduce
MagicMan, a human-specific multi-view diffusion model designed to generate
high-quality novel view images from a single reference image. As its core, we
leverage a pre-trained 2D diffusion model as the generative prior for
generalizability, with the parametric SMPL-X model as the 3D body prior to
promote 3D awareness. To tackle the critical challenge of maintaining
consistency while achieving dense multi-view generation for improved 3D human
reconstruction, we first introduce hybrid multi-view attention to facilitate
both efficient and thorough information interchange across different views.
Additionally, we present a geometry-aware dual branch to perform concurrent
generation in both RGB and normal domains, further enhancing consistency via
geometry cues. Last but not least, to address ill-shaped issues arising from
inaccurate SMPL-X estimation that conflicts with the reference image, we
propose a novel iterative refinement strategy, which progressively optimizes
SMPL-X accuracy while enhancing the quality and consistency of the generated
multi-views. Extensive experimental results demonstrate that our method
significantly outperforms existing approaches in both novel view synthesis and
subsequent 3D human reconstruction tasks.

摘要：現有的單一影像人類重建工作由於訓練數據不足或缺乏全面的多視角知識而導致通用性較差。在本文中，我們介紹了 MagicMan，這是一個特定於人類的多視角擴散模型，旨在從單個參考影像生成高品質的新視角影像。作為其核心，我們利用預訓練的 2D 擴散模型作為生成先驗以提高通用性，並使用參數化 SMPL-X 模型作為 3D 身體先驗以促進 3D 感知。為了應對在改進的 3D 人體重建中保持一致性同時實現密集多視角生成的關鍵挑戰，我們首先引入了混合多視角注意力，以促進不同視角之間高效且徹底的資訊交換。此外，我們提出了一个感知幾何的雙分支，以在 RGB 和法線域中執行並行生成，進一步通過幾何線索增強一致性。最後但並非最不重要的一點是，為了解決因與參考影像衝突的不準確 SMPL-X 估計而產生的形狀不佳問題，我們提出了一種新的迭代精煉策略，該策略在提高生成多視角的品質和一致性的同時，逐步優化 SMPL-X 的準確性。大量的實驗結果表明，我們的方法在新的視角合成和後續的 3D 人體重建任務中都明顯優於現有的方法。

##### **DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**
2408.14185v1 by Ziai Zhou, Bin Zhou, Hao Liu

Real-time dynamic path planning in complex traffic environments presents
challenges, such as varying traffic volumes and signal wait times. Traditional
static routing algorithms like Dijkstra and A* compute shortest paths but often
fail under dynamic conditions. Recent Reinforcement Learning (RL) approaches
offer improvements but tend to focus on local optima, risking dead-ends or
boundary issues. This paper proposes a novel approach based on causal inference
for real-time dynamic path planning, balancing global and local optimality. We
first use the static Dijkstra algorithm to compute a globally optimal baseline
path. A distributed control strategy then guides vehicles along this path. At
intersections, DynamicRouteGPT performs real-time decision-making for local
path selection, considering real-time traffic, driving preferences, and
unexpected events. DynamicRouteGPT integrates Markov chains, Bayesian
inference, and large-scale pretrained language models like Llama3 8B to provide
an efficient path planning solution. It dynamically adjusts to traffic
scenarios and driver preferences and requires no pre-training, offering broad
applicability across road networks. A key innovation is the construction of
causal graphs for counterfactual reasoning, optimizing path decisions.
Experimental results show that our method achieves state-of-the-art performance
in real-time dynamic path planning for multiple vehicles while providing
explainable path selections, offering a novel and efficient solution for
complex traffic environments.

摘要：在複雜交通環境中進行實時動態路徑規劃會面臨挑戰，例如交通流量變化和信號等待時間。傳統的靜態路由演算法，例如 Dijkstra 和 A*，會計算最短路徑，但通常在動態條件下會失敗。最近的強化學習 (RL) 方法提供了改進，但傾向於關注局部最優，冒著陷入死胡同或邊界問題的風險。本文提出了一種基於因果推論的新穎方法，用於實時動態路徑規劃，平衡全局和局部最優性。我們首先使用靜態 Dijkstra 演算法計算全局最優基線路徑。然後，一個分布式控制策略沿著這條路徑引導車輛。在交叉路口，DynamicRouteGPT 針對局部路徑選擇執行實時決策，考量實時交通、駕駛偏好和意外事件。DynamicRouteGPT 整合了馬可夫鏈、貝氏推論和 Llama3 8B 等大規模預先訓練的語言模型，以提供有效的路徑規劃解決方案。它會動態調整到交通狀況和駕駛偏好，並且不需要預先訓練，在道路網路上提供廣泛的適用性。一個關鍵創新是建立反事實推理的因果圖，以最佳化路徑決策。實驗結果顯示，我們的模型在多輛車輛的實時動態路徑規劃中達到最先進的效能，同時提供可解釋的路徑選擇，為複雜的交通環境提供一種新穎且有效的解決方案。

##### **I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing**
2408.14180v1 by Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, Zhibin Wang, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, Rongrong Ji

Significant progress has been made in the field of Instruction-based Image
Editing (IIE). However, evaluating these models poses a significant challenge.
A crucial requirement in this field is the establishment of a comprehensive
evaluation benchmark for accurately assessing editing results and providing
valuable insights for its further development. In response to this need, we
propose I2EBench, a comprehensive benchmark designed to automatically evaluate
the quality of edited images produced by IIE models from multiple dimensions.
I2EBench consists of 2,000+ images for editing, along with 4,000+ corresponding
original and diverse instructions. It offers three distinctive characteristics:
1) Comprehensive Evaluation Dimensions: I2EBench comprises 16 evaluation
dimensions that cover both high-level and low-level aspects, providing a
comprehensive assessment of each IIE model. 2) Human Perception Alignment: To
ensure the alignment of our benchmark with human perception, we conducted an
extensive user study for each evaluation dimension. 3) Valuable Research
Insights: By analyzing the advantages and disadvantages of existing IIE models
across the 16 dimensions, we offer valuable research insights to guide future
development in the field. We will open-source I2EBench, including all
instructions, input images, human annotations, edited images from all evaluated
methods, and a simple script for evaluating the results from new IIE models.
The code, dataset and generated images from all IIE models are provided in
github: https://github.com/cocoshe/I2EBench.

摘要：<paragraph>在基於指令的圖像編輯 (IIE) 領域中已取得顯著進展。然而，評估這些模型構成了一項重大挑戰。此領域的一項關鍵要求是建立一個全面的評估基準，以準確評估編輯結果並為其進一步發展提供寶貴的見解。為了滿足這一需求，我們提出了 I2EBench，這是一個全面的基準，旨在自動評估 IIE 模型產生的編輯圖像品質，並從多個面向進行評估。I2EBench 包含 2,000 多張用於編輯的圖像，以及 4,000 多條對應的原始且多樣化的指令。它提供了三個顯著特徵：1) 全面的評估面向：I2EBench 包含 16 個評估面向，涵蓋高層級和低層級面向，提供對每個 IIE 模型的全面評估。2) 人類感知比對：為了確保我們的基準與人類感知比對，我們針對每個評估面向進行了一項廣泛的使用者研究。3) 有價值的研究見解：透過分析現有 IIE 模型在 16 個面向中的優缺點，我們提供了有價值的研究見解，以指導該領域未來的發展。我們將開放原始碼 I2EBench，包括所有指令、輸入圖像、人類註解、所有評估方法的編輯圖像，以及一個用於評估來自新 IIE 模型的結果的簡單腳本。所有 IIE 模型的程式碼、資料集和產生的圖像都提供在 github 中：https://github.com/cocoshe/I2EBench。</paragraph>

##### **SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its Teacher**
2408.14176v1 by Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, Anh Tran

In this paper, we aim to enhance the performance of SwiftBrush, a prominent
one-step text-to-image diffusion model, to be competitive with its multi-step
Stable Diffusion counterpart. Initially, we explore the quality-diversity
trade-off between SwiftBrush and SD Turbo: the former excels in image
diversity, while the latter excels in image quality. This observation motivates
our proposed modifications in the training methodology, including better weight
initialization and efficient LoRA training. Moreover, our introduction of a
novel clamped CLIP loss enhances image-text alignment and results in improved
image quality. Remarkably, by combining the weights of models trained with
efficient LoRA and full training, we achieve a new state-of-the-art one-step
diffusion model, achieving an FID of 8.14 and surpassing all GAN-based and
multi-step Stable Diffusion models. The evaluation code is available at:
https://github.com/vinairesearch/swiftbrushv2.

摘要：在本文中，我们旨在提升 SwiftBrush 的性能，SwiftBrush 是一个著名的单步文本到图像扩散模型，以与它的多步 Stable Diffusion 对应模型相竞争。最初，我们探索了 SwiftBrush 和 SD Turbo 之间的质量多样性权衡：前者在图像多样性方面表现出色，而后者在图像质量方面表现出色。这一观察促使我们对训练方法提出了建议的修改，包括更好的权重初始化和高效的 LoRA 训练。此外，我们引入了一种新颖的 clamped CLIP 损失，它增强了图像文本对齐，并提高了图像质量。值得注意的是，通过结合使用经过高效 LoRA 和完整训练训练的模型的权重，我们实现了一个新的最先进的单步扩散模型，达到了 8.14 的 FID，并超越了所有基于 GAN 和多步 Stable Diffusion 的模型。评估代码可在以下网址获得：
https://github.com/vinairesearch/swiftbrushv2。

##### **Dynamic Pricing for Electric Vehicle Charging**
2408.14169v1 by Arun Kumar Kalakanti, Shrisha Rao

Dynamic pricing is a promising strategy to address the challenges of smart
charging, as traditional time-of-use (ToU) rates and stationary pricing (SP) do
not dynamically react to changes in operating conditions, reducing revenue for
charging station (CS) vendors and affecting grid stability. Previous studies
evaluated single objectives or linear combinations of objectives for EV CS
pricing solutions, simplifying trade-offs and preferences among objectives. We
develop a novel formulation for the dynamic pricing problem by addressing
multiple conflicting objectives efficiently instead of solely focusing on one
objective or metric, as in earlier works. We find optimal trade-offs or Pareto
solutions efficiently using Non-dominated Sorting Genetic Algorithms (NSGA) II
and NSGA III. A dynamic pricing model quantifies the relationship between
demand and price while simultaneously solving multiple conflicting objectives,
such as revenue, quality of service (QoS), and peak-to-average ratios (PAR). A
single method can only address some of the above aspects of dynamic pricing
comprehensively. We present a three-part dynamic pricing approach using a
Bayesian model, multi-objective optimization, and multi-criteria
decision-making (MCDM) using pseudo-weight vectors. To address the research gap
in CS pricing, our method selects solutions using revenue, QoS, and PAR metrics
simultaneously. Two California charging sites' real-world data validates our
approach.

摘要：動態定價是一種解決智慧充電挑戰的策略，因為傳統的用電時段 (ToU) 費率和固定定價 (SP) 並不會動態反應操作條件的變動，進而降低充電站 (CS) 供應商的收入並影響電網穩定性。先前的研究評估單一目標或線性組合的目標，用於電動車充電站定價解決方案，簡化目標之間的權衡和偏好。我們開發了一種新的動態定價問題公式，透過有效地處理多個相互衝突的目標，而不是像早期研究那樣僅專注於一個目標或指標。我們使用非支配排序遺傳演算法 (NSGA) II 和 NSGA III 有效地找出最佳權衡或帕雷托解。動態定價模型量化需求和價格之間的關係，同時解決多個相互衝突的目標，例如收入、服務品質 (QoS) 和尖峰平均比 (PAR)。單一方法只能全面解決動態定價的上述某些面向。我們提出一個三部分的動態定價方法，使用貝氏模型、多目標最佳化和使用偽權重向量的多準則決策制定 (MCDM)。為了解決充電站定價的研究差距，我們的模型同時使用收入、服務品質和尖峰平均比指標來選出解決方案。加州兩個充電站的真實世界資料驗證了我們的做法。

##### **Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning**
2408.14158v1 by Wei An, Xiao Bi, Guanting Chen, Shanhuang Chen, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Wenjun Gao, Kang Guan, Jianzhong Guo, Yongqiang Guo, Zhe Fu, Ying He, Panpan Huang, Jiashi Li, Wenfeng Liang, Xiaodong Liu, Xin Liu, Yiyuan Liu, Yuxuan Liu, Shanghao Lu, Xuan Lu, Xiaotao Nie, Tian Pei, Junjie Qiu, Hui Qu, Zehui Ren, Zhangli Sha, Xuecheng Su, Xiaowen Sun, Yixuan Tan, Minghui Tang, Shiyu Wang, Yaohui Wang, Yongji Wang, Ziwei Xie, Yiliang Xiong, Yanhong Xu, Shengfeng Ye, Shuiping Yu, Yukun Zha, Liyue Zhang, Haowei Zhang, Mingchuan Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Yuheng Zou

The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has
exponentially increased demands of computational power and bandwidth. This,
combined with the high costs of faster computing chips and interconnects, has
significantly inflated High Performance Computing (HPC) construction costs. To
address these challenges, we introduce the Fire-Flyer AI-HPC architecture, a
synergistic hardware-software co-design framework and its best practices. For
DL training, we deployed the Fire-Flyer 2 with 10,000 PCIe A100 GPUs, achieved
performance approximating the DGX-A100 while reducing costs by half and energy
consumption by 40%. We specifically engineered HFReduce to accelerate allreduce
communication and implemented numerous measures to keep our Computation-Storage
Integrated Network congestion-free. Through our software stack, including
HaiScale, 3FS, and HAI-Platform, we achieved substantial scalability by
overlapping computation and communication. Our system-oriented experience from
DL training provides valuable insights to drive future advancements in AI-HPC.

摘要：深度學習 (DL) 和大型語言模型 (LLM) 的快速進展，已呈指數級增長地增加了對運算能力和頻寬的需求。這與更快速的運算晶片和互連的高成本相結合，已大幅提高了高性能運算 (HPC) 的建置成本。為了應對這些挑戰，我們引入了 Fire-Flyer AI-HPC 架構，這是一個協同的硬體軟體共同設計架構及其最佳實務。對於 DL 訓練，我們部署了配備 10,000 個 PCIe A100 GPU 的 Fire-Flyer 2，在將成本降低一半和能耗降低 40% 的同時，達到了近似於 DGX-A100 的效能。我們特別設計了 HFReduce 來加速 allreduce 通訊，並實施了許多措施來保持我們的運算儲存整合網路暢通無阻。透過我們的軟體堆疊，包括 HaiScale、3FS 和 HAI-Platform，我們透過重疊運算和通訊，達成了大幅度的可擴充性。我們從 DL 訓練中獲得的系統導向經驗，為推動 AI-HPC 未來的進展提供了寶貴的見解。

##### **Investigating the effect of Mental Models in User Interaction with an Adaptive Dialog Agent**
2408.14154v1 by Lindsey Vanderlyn, Dirk Väth, Ngoc Thang Vu

Mental models play an important role in whether user interaction with
intelligent systems, such as dialog systems is successful or not. Adaptive
dialog systems present the opportunity to align a dialog agent's behavior with
heterogeneous user expectations. However, there has been little research into
what mental models users form when interacting with a task-oriented dialog
system, how these models affect users' interactions, or what role system
adaptation can play in this process, making it challenging to avoid damage to
human-AI partnership. In this work, we collect a new publicly available dataset
for exploring user mental models about information seeking dialog systems. We
demonstrate that users have a variety of conflicting mental models about such
systems, the validity of which directly impacts the success of their
interactions and perceived usability of system. Furthermore, we show that
adapting a dialog agent's behavior to better align with users' mental models,
even when done implicitly, can improve perceived usability, dialog efficiency,
and success. To this end, we argue that implicit adaptation can be a valid
strategy for task-oriented dialog systems, so long as developers first have a
solid understanding of users' mental models.

摘要：心智模型在使用者與智慧系統（例如對話系統）的互動是否成功中扮演著重要的角色。適應性對話系統提供了將對話代理的行為與異質使用者期望值對齊的機會。然而，對於使用者在與任務導向對話系統互動時形成的心智模型、這些模型如何影響使用者的互動，或系統適應在這個過程中可以扮演什麼角色的研究很少，這使得避免損害人機夥伴關係變得具有挑戰性。在這項工作中，我們收集了一個新的公開可用資料集，用於探索使用者關於資訊尋求對話系統的心智模型。我們證明了使用者對此類系統有多種相互衝突的心智模型，其有效性直接影響其互動的成功和系統的感知可用性。此外，我們表明，即使是隱式地調整對話代理的行為以更好地與使用者的精神模型保持一致，也可以改善感知可用性、對話效率和成功率。為此，我們認為隱式適應可以成為任務導向對話系統的有效策略，只要開發人員首先對使用者的精神模型有紮實的了解。

##### **Explaining Vision-Language Similarities in Dual Encoders with Feature-Pair Attributions**
2408.14153v1 by Lucas Möller, Pascal Tilli, Ngoc Thang Vu, Sebastian Padó

Dual encoder architectures like CLIP models map two types of inputs into a
shared embedding space and learn similarities between them. However, it is not
understood how such models compare two inputs. Here, we address this research
gap with two contributions. First, we derive a method to attribute predictions
of any differentiable dual encoder onto feature-pair interactions between its
inputs. Second, we apply our method to CLIP-type models and show that they
learn fine-grained correspondences between parts of captions and regions in
images. They match objects across input modes and also account for mismatches.
However, this visual-linguistic grounding ability heavily varies between object
classes, depends on the training data distribution, and largely improves after
in-domain training. Using our method we can identify knowledge gaps about
specific object classes in individual models and can monitor their improvement
upon fine-tuning.

摘要：雙編碼器架構，如 CLIP 模型，將兩種輸入類型映射到共享嵌入空間，並學習它們之間的相似性。然而，尚不清楚此類模型如何比較兩個輸入。在此，我們以兩個貢獻來解決這個研究差距。首先，我們推導出一種方法，將任何可微分雙編碼器的預測歸因於其輸入之間的特徵對交互。其次，我們將我們的模型應用到 CLIP 類型模型，並展示它們學習了標題部分和圖像區域之間的細粒度對應關係。它們跨輸入模式匹配對象，並考慮不匹配。然而，這種視覺語言基礎能力在對象類別之間差異很大，取決於訓練數據分佈，並且在域內訓練後大幅改善。使用我們的模型，我們可以識別個別模型中特定對象類別的知識差距，並監控它們在微調後的改進。

##### **Crowd-Calibrator: Can Annotator Disagreement Inform Calibration in Subjective Tasks?**
2408.14141v1 by Urja Khurana, Eric Nalisnick, Antske Fokkens, Swabha Swayamdipta

Subjective tasks in NLP have been mostly relegated to objective standards,
where the gold label is decided by taking the majority vote. This obfuscates
annotator disagreement and the inherent uncertainty of the label. We argue that
subjectivity should factor into model decisions and play a direct role via
calibration under a selective prediction setting. Specifically, instead of
calibrating confidence purely from the model's perspective, we calibrate models
for subjective tasks based on crowd worker agreement. Our method,
Crowd-Calibrator, models the distance between the distribution of crowd worker
labels and the model's own distribution over labels to inform whether the model
should abstain from a decision. On two highly subjective tasks, hate speech
detection and natural language inference, our experiments show Crowd-Calibrator
either outperforms or achieves competitive performance with existing selective
prediction baselines. Our findings highlight the value of bringing human
decision-making into model predictions.

摘要：NLP 中的主觀任務大多已降級為客觀標準，
其中金標籤是透過多數決來決定。這會模糊標記員的分歧和標籤的內在不確定性。我們認為，主觀性應納入模型決策，並透過在選擇性預測設定下進行校準來發揮直接作用。具體來說，我們不是純粹從模型的角度校準信心，而是根據群眾工作者的共識為主觀任務校準模型。我們的 Crowd-Calibrator 方法對群眾工作者標籤的分布與模型對標籤的分布之間的距離進行建模，以告知模型是否應放棄決策。在兩個高度主觀的任務（仇恨言論偵測和自然語言推論）上，我們的實驗顯示 Crowd-Calibrator 的表現優於或與現有的選擇性預測基準線具有競爭力。我們的發現突顯了將人類決策納入模型預測的價值。

##### **Multi-Faceted Evaluation of Modeling Languages for Augmented Reality Applications -- The Case of ARWFML**
2408.14137v1 by Fabian Muff, Hans-Georg Fill

The evaluation of modeling languages for augmented reality applications poses
particular challenges due to the three-dimensional environment they target. The
previously introduced Augmented Reality Workflow Modeling Language (ARWFML)
enables the model-based creation of augmented reality scenarios without
programming knowledge. Building upon the first design cycle of the language's
specification, this paper presents two further design iterations for refining
the language based on multi-faceted evaluations. These include a comparative
evaluation of implementation options and workflow capabilities, the
introduction of a 3D notation, and the development of a new 3D modeling
environment. On this basis, a comprehensibility study of the language was
conducted. Thereby, we show how modeling languages for augmented reality can be
evolved towards a maturity level suitable for empirical evaluations.

摘要：由於擴增實境應用程式鎖定的三維環境，對建模語言的評估構成了特殊的挑戰。先前導入的擴增實境工作流程建模語言 (ARWFML) 能讓沒有程式設計知識的人，以模型為基礎建立擴增實境場景。本文根據多面向的評估，在語言規範的第一個設計週期上建構，提出了兩個進一步的設計迭代，以精煉語言。這些評估包括實作選項和工作流程功能的比較評估、3D 標記的導入，以及新的 3D 建模環境的開發。在這個基礎上，進行了語言的可理解性研究。因此，我們展示了如何讓擴增實境建模語言演進到適合經驗評估的成熟度等級。

##### **Exploring the Potential of Large Language Models for Heterophilic Graphs**
2408.14134v1 by Yuxia Wu, Shujie Li, Yuan Fang, Chuan Shi

Graph Neural Networks (GNNs) are essential for various graph-based learning
tasks. Notably, classical GNN architectures operate under the assumption of
homophily, which posits that connected nodes are likely to share similar
features. However, this assumption limits the effectiveness of GNNs in handling
heterophilic graphs where connected nodes often exhibit dissimilar
characteristics. Existing approaches for homophily graphs such as non-local
neighbor extension and architectural refinement overlook the rich textual data
associated with nodes, which could unlock deeper insights into these
heterophilic contexts. With advancements in Large Language Models (LLMs), there
is significant promise to enhance GNNs by leveraging the extensive open-world
knowledge within LLMs to more effectively interpret and utilize textual data
for characterizing heterophilic graphs. In this work, we explore the potential
of LLMs for modeling heterophilic graphs and propose a novel two-stage
framework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.
Specifically, in the first stage, we fine-tune the LLM to better identify
homophilic and heterophilic edges based on the textual information of their
nodes. In the second stage, we adaptively manage message propagation in GNNs
for different edge types based on node features, structures, and heterophilic
or homophilic characteristics. To cope with the computational demands when
deploying LLMs in practical scenarios, we further explore model distillation
techniques to fine-tune smaller, more efficient models that maintain
competitive performance. Extensive experiments validate the effectiveness of
our framework, demonstrating the feasibility of using LLMs to enhance GNNs for
node classification on heterophilic graphs.

摘要：圖神經網路 (GNN) 對於各種基於圖形的學習任務至關重要。值得注意的是，傳統的 GNN 架構在同質性的假設下運作，該假設認為連接的節點可能共享類似的特徵。然而，此假設限制了 GNN 在處理異質性圖形中的效能，其中連接的節點通常表現出不同的特徵。現有的同質性圖形方法（例如非局部鄰域延伸和架構改進）忽略了與節點相關的豐富文本資料，這可以深入了解這些異質性脈絡。隨著大型語言模型 (LLM) 的進步，透過利用 LLM 中廣泛的開放世界知識來增強 GNN，對於更有效地詮釋和利用文本資料來表徵異質性圖形有很大的希望。在這項工作中，我們探討了 LLM 在異質性圖形建模中的潛力，並提出了一個新穎的兩階段架構：LLM 增強邊緣判別器和 LLM 引導邊緣重新加權。具體來說，在第一階段，我們微調 LLM 以根據其節點的文本資訊，更好地識別同質性和異質性邊緣。在第二階段，我們根據節點特徵、結構和異質性或同質性特徵，自適應地管理 GNN 中不同邊緣類型的訊息傳遞。為了應對在實際場景中部署 LLM 時的計算需求，我們進一步探討模型萃取技術，以微調較小、更有效率的模型，以維持競爭力。廣泛的實驗驗證了我們架構的有效性，證明了使用 LLM 來增強 GNN 以進行異質性圖形上的節點分類的可行性。

##### **Contrastive Learning Subspace for Text Clustering**
2408.14119v1 by Qian Yong, Chen Chen, Xiabing Zhou

Contrastive learning has been frequently investigated to learn effective
representations for text clustering tasks. While existing contrastive
learning-based text clustering methods only focus on modeling instance-wise
semantic similarity relationships, they ignore contextual information and
underlying relationships among all instances that needs to be clustered. In
this paper, we propose a novel text clustering approach called Subspace
Contrastive Learning (SCL) which models cluster-wise relationships among
instances. Specifically, the proposed SCL consists of two main modules: (1) a
self-expressive module that constructs virtual positive samples and (2) a
contrastive learning module that further learns a discriminative subspace to
capture task-specific cluster-wise relationships among texts. Experimental
results show that the proposed SCL method not only has achieved superior
results on multiple task clustering datasets but also has less complexity in
positive sample construction.

摘要：對比學習已頻繁用於學習文字分群任務的有效表徵。雖然現有的基於對比學習的文字分群方法僅專注於建模實例級語義相似性關係，但它們忽略了所有需要分群的實例之間的背景資訊和基本關係。在本文中，我們提出了一種稱為子空間對比學習 (SCL) 的新文字分群方法，該方法對實例之間的群集關係進行建模。具體來說，提出的 SCL 包含兩個主要模組：(1) 建構虛擬正樣本的自表達模組，以及 (2) 進一步學習判別子空間以擷取文字之間與任務相關的群集關係的對比學習模組。實驗結果顯示，所提出的 SCL 方法不僅在多個任務分群資料集上取得了優異的結果，而且在正樣本建構方面也較不複雜。

##### **Estimating Causal Effects from Learned Causal Networks**
2408.14101v1 by Anna Raichev, Alexander Ihler, Jin Tian, Rina Dechter

The standard approach to answering an identifiable causal-effect query (e.g.,
$P(Y|do(X)$) when given a causal diagram and observational data is to first
generate an estimand, or probabilistic expression over the observable
variables, which is then evaluated using the observational data. In this paper,
we propose an alternative paradigm for answering causal-effect queries over
discrete observable variables. We propose to instead learn the causal Bayesian
network and its confounding latent variables directly from the observational
data. Then, efficient probabilistic graphical model (PGM) algorithms can be
applied to the learned model to answer queries. Perhaps surprisingly, we show
that this \emph{model completion} learning approach can be more effective than
estimand approaches, particularly for larger models in which the estimand
expressions become computationally difficult.
  We illustrate our method's potential using a benchmark collection of Bayesian
networks and synthetically generated causal models.

摘要：標準方法是回答可識別的因果關係查詢（例如，當給定因果圖和觀察資料時，$P(Y|do(X)$），首先產生一個估計量或可觀察變數的機率表達式，然後使用觀察資料評估。在本文中，我們提出了一個替代範例，用於回答離散可觀察變數的因果關係查詢。我們建議直接從觀察資料中學習因果貝氏網路及其混淆潛在變數。然後，可以將有效的機率圖形模型 (PGM) 演算法應用於學習模型以回答查詢。令人驚訝的是，我們展示了這種\emph{模型完成}學習方法比估計量方法更有效，特別是對於估計量表達式在計算上變得困難的大型模型。我們使用貝氏網路和合成生成的因果模型的基準集合來說明我們方法的潛力。

##### **SONICS: Synthetic Or Not -- Identifying Counterfeit Songs**
2408.14080v1 by Md Awsafur Rahman, Zaber Ibn Abdul Hakim, Najibul Haque Sarker, Bishmoy Paul, Shaikh Anowarul Fattah

The recent surge in AI-generated songs presents exciting possibilities and
challenges. While these tools democratize music creation, they also necessitate
the ability to distinguish between human-composed and AI-generated songs for
safeguarding artistic integrity and content curation. Existing research and
datasets in fake song detection only focus on singing voice deepfake detection
(SVDD), where the vocals are AI-generated but the instrumental music is sourced
from real songs. However, this approach is inadequate for contemporary
end-to-end AI-generated songs where all components (vocals, lyrics, music, and
style) could be AI-generated. Additionally, existing datasets lack lyrics-music
diversity, long-duration songs, and open fake songs. To address these gaps, we
introduce SONICS, a novel dataset for end-to-end Synthetic Song Detection
(SSD), comprising over 97k songs with over 49k synthetic songs from popular
platforms like Suno and Udio. Furthermore, we highlight the importance of
modeling long-range temporal dependencies in songs for effective authenticity
detection, an aspect overlooked in existing methods. To capture these patterns,
we propose a novel model, SpecTTTra, that is up to 3 times faster and 6 times
more memory efficient compared to popular CNN and Transformer-based models
while maintaining competitive performance. Finally, we offer both AI-based and
Human evaluation benchmarks, addressing another deficiency in current research.

摘要：最近 AI 生成的歌曲激增，帶來了令人興奮的可能性和挑戰。雖然這些工具讓音樂創作民主化，但它們也需要具備區分由人類創作和由 AI 生成的歌曲的能力，以維護藝術的完整性和內容策劃。現有的研究和偽歌檢測中的數據集只關注歌唱語音深度偽造檢測 (SVDD)，其中人聲是由 AI 生成的，但器樂來自於真實歌曲。然而，這種方法對於當代端對端 AI 生成的歌曲來說是不夠的，因為所有組成部分（人聲、歌詞、音樂和風格）都可以由 AI 生成。此外，現有的數據集缺乏歌詞音樂的多樣性、長時歌曲和開放的偽歌。為了解決這些差距，我們引入了 SONICS，這是一個用於端對端合成歌曲檢測 (SSD) 的新數據集，包含超過 97k 首歌曲，其中超過 49k 首來自於 Suno 和 Udio 等熱門平台的合成歌曲。此外，我們強調了在歌曲中建模長程時間依賴性對於有效真實性檢測的重要性，這是現有方法所忽略的一個方面。為了捕捉這些模式，我們提出了一個新模型 SpecTTTra，與流行的 CNN 和基於 Transformer 的模型相比，它快 3 倍，記憶體效率高 6 倍，同時保持了競爭力。最後，我們提供了基於 AI 和人類評估的基準，解決了當前研究中的另一個缺陷。

##### **Enhancing Depression Diagnosis with Chain-of-Thought Prompting**
2408.14053v1 by Elysia Shi, Adithri Manda, London Chowdhury, Runeema Arun, Kevin Zhu, Michael Lam

When using AI to detect signs of depressive disorder, AI models habitually
draw preemptive conclusions. We theorize that using chain-of-thought (CoT)
prompting to evaluate Patient Health Questionnaire-8 (PHQ-8) scores will
improve the accuracy of the scores determined by AI models. In our findings,
when the models reasoned with CoT, the estimated PHQ-8 scores were consistently
closer on average to the accepted true scores reported by each participant
compared to when not using CoT. Our goal is to expand upon AI models'
understanding of the intricacies of human conversation, allowing them to more
effectively assess a patient's feelings and tone, therefore being able to more
accurately discern mental disorder symptoms; ultimately, we hope to augment AI
models' abilities, so that they can be widely accessible and used in the
medical field.

摘要：在使用 AI 偵測憂鬱症徵兆時，AI 模型習慣性地
做出先入為主的結論。我們假設使用思考鏈（CoT）
提示來評估患者健康問卷-8（PHQ-8）分數將
提升 AI 模型所決定的分數準確性。在我們的研究結果中，
當模型使用 CoT 推理時，估計的 PHQ-8 分數平均而言
始終比每位參與者回報的公認真實分數更接近，相較於
未使用 CoT 時。我們的目標是擴展 AI 模型
對人類對話的複雜性理解，讓它們能更
有效地評估患者的情緒和語氣，因此能夠更
準確地辨別精神疾病症狀；最終，我們希望擴增 AI
模型的能力，讓它們能廣泛地被使用在
醫療領域。

##### **Beyond Detection: Leveraging Large Language Models for Cyber Attack Prediction in IoT Networks**
2408.14045v1 by Alaeddine Diaf, Abdelaziz Amara Korba, Nour Elislem Karabadji, Yacine Ghamri-Doudane

In recent years, numerous large-scale cyberattacks have exploited Internet of
Things (IoT) devices, a phenomenon that is expected to escalate with the
continuing proliferation of IoT technology. Despite considerable efforts in
attack detection, intrusion detection systems remain mostly reactive,
responding to specific patterns or observed anomalies. This work proposes a
proactive approach to anticipate and mitigate malicious activities before they
cause damage. This paper proposes a novel network intrusion prediction
framework that combines Large Language Models (LLMs) with Long Short Term
Memory (LSTM) networks. The framework incorporates two LLMs in a feedback loop:
a fine-tuned Generative Pre-trained Transformer (GPT) model for predicting
network traffic and a fine-tuned Bidirectional Encoder Representations from
Transformers (BERT) for evaluating the predicted traffic. The LSTM classifier
model then identifies malicious packets among these predictions. Our framework,
evaluated on the CICIoT2023 IoT attack dataset, demonstrates a significant
improvement in predictive capabilities, achieving an overall accuracy of 98%,
offering a robust solution to IoT cybersecurity challenges.

摘要：近年來，許多大規模的網路攻擊都利用了物聯網 (IoT) 裝置，而隨著物聯網技術的持續擴散，預計這種現象將會升級。儘管在攻擊偵測方面投入了相當大的努力，但入侵偵測系統仍然大多是反應式的，對特定的模式或觀察到的異常做出回應。這項工作提出了一種主動方法來預測和減輕惡意活動，在它們造成損害之前。本文提出了一個新穎的網路入侵預測架構，它結合了大型語言模型 (LLM) 和長短期記憶 (LSTM) 網路。該架構在一個回饋迴路中結合了兩個 LLM：一個微調的生成式預訓練Transformer (GPT) 模型，用於預測網路流量，以及一個微調的Transformer雙向編碼器表徵 (BERT)，用於評估預測的流量。然後，LSTM 分類器模型在這些預測中識別出惡意封包。我們的架構在 CICIoT2023 IoT 攻擊資料集上進行評估，展示了預測能力的顯著提升，達到了 98% 的整體準確度，為 IoT 網路安全挑戰提供了一個強大的解決方案。

##### **PAGE: Parametric Generative Explainer for Graph Neural Network**
2408.14042v1 by Yang Qiu, Wei Liu, Jun Wang, Ruixuan Li

This article introduces PAGE, a parameterized generative interpretive
framework. PAGE is capable of providing faithful explanations for any graph
neural network without necessitating prior knowledge or internal details.
Specifically, we train the auto-encoder to generate explanatory substructures
by designing appropriate training strategy. Due to the dimensionality reduction
of features in the latent space of the auto-encoder, it becomes easier to
extract causal features leading to the model's output, which can be easily
employed to generate explanations. To accomplish this, we introduce an
additional discriminator to capture the causality between latent causal
features and the model's output. By designing appropriate optimization
objectives, the well-trained discriminator can be employed to constrain the
encoder in generating enhanced causal features. Finally, these features are
mapped to substructures of the input graph through the decoder to serve as
explanations. Compared to existing methods, PAGE operates at the sample scale
rather than nodes or edges, eliminating the need for perturbation or encoding
processes as seen in previous methods. Experimental results on both
artificially synthesized and real-world datasets demonstrate that our approach
not only exhibits the highest faithfulness and accuracy but also significantly
outperforms baseline models in terms of efficiency.

摘要：本文介紹 PAGE，一個參數化生成詮釋框架。PAGE 能夠為任何圖神經網路提供忠實的解釋，而無需先備知識或內部細節。具體來說，我們訓練自動編碼器生成解釋性子結構，方法是設計適當的訓練策略。由於自動編碼器的潛在空間中特徵的維度降低，因此更容易提取導致模型輸出的因果特徵，這些特徵可以很容易地用於生成解釋。為此，我們引入了一個額外的判別器來捕捉潛在因果特徵和模型輸出之間的因果關係。透過設計適當的最佳化目標，訓練良好的判別器可以被用來約束編碼器生成增強的因果特徵。最後，這些特徵透過解碼器映射到輸入圖形中的子結構，作為解釋。與現有方法相比，PAGE 運作在樣本層級，而不是節點或邊緣，消除了先前方法中看到的擾動或編碼過程的需求。在人工合成和真實世界資料集上的實驗結果表明，我們的做法不僅表現出最高的忠實度和準確性，而且在效率方面也明顯優於基準模型。

##### **MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents**
2408.14033v1 by Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du

Machine learning research, crucial for technological advancements and
innovation, often faces significant challenges due to its inherent complexity,
slow pace of experimentation, and the necessity for specialized expertise.
Motivated by this, we present a new systematic framework, autonomous Machine
Learning Research with large language models (MLR-Copilot), designed to enhance
machine learning research productivity through the automatic generation and
implementation of research ideas using Large Language Model (LLM) agents. The
framework consists of three phases: research idea generation, experiment
implementation, and implementation execution. First, existing research papers
are used to generate hypotheses and experimental plans vis IdeaAgent powered by
LLMs. Next, the implementation generation phase translates these plans into
executables with ExperimentAgent. This phase leverages retrieved prototype code
and optionally retrieves candidate models and data. Finally, the execution
phase, also managed by ExperimentAgent, involves running experiments with
mechanisms for human feedback and iterative debugging to enhance the likelihood
of achieving executable research outcomes. We evaluate our framework on five
machine learning research tasks and the experimental results show the
framework's potential to facilitate the research progress and innovations.

摘要：機器學習研究對於技術進步和創新至關重要，但由於其固有的複雜性、實驗進度緩慢以及對專業知識的需求，因此經常面臨重大挑戰。基於此，我們提出了一個新的系統化框架，即使用大型語言模型進行自主機器學習研究 (MLR-Copilot)，旨在通過使用大型語言模型 (LLM) 代理自動生成和實施研究想法來提高機器學習研究的生產力。該框架包含三個階段：研究構想產生、實驗實施和實施執行。首先，現有的研究論文用於生成假設和實驗計畫，由 LLM 提供支援的 IdeaAgent 提供支援。接下來，實施生成階段將這些計畫轉換為可執行檔，並由 ExperimentAgent 執行。此階段利用檢索到的原型程式碼，並可選擇檢索候選模型和資料。最後，由 ExperimentAgent 管理的執行階段涉及執行實驗，並具有人工回饋和反覆除錯的機制，以提高獲得可執行研究成果的可能性。我們在五項機器學習研究任務上評估我們的框架，實驗結果顯示該框架有可能促進研究進度和創新。

##### **SurGen: Text-Guided Diffusion Model for Surgical Video Generation**
2408.14028v1 by Joseph Cho, Samuel Schmidgall, Cyril Zakka, Mrudang Mathur, Rohan Shad, William Hiesinger

Diffusion-based video generation models have made significant strides,
producing outputs with improved visual fidelity, temporal coherence, and user
control. These advancements hold great promise for improving surgical education
by enabling more realistic, diverse, and interactive simulation environments.
In this study, we introduce SurGen, a text-guided diffusion model tailored for
surgical video synthesis, producing the highest resolution and longest duration
videos among existing surgical video generation models. We validate the visual
and temporal quality of the outputs using standard image and video generation
metrics. Additionally, we assess their alignment to the corresponding text
prompts through a deep learning classifier trained on surgical data. Our
results demonstrate the potential of diffusion models to serve as valuable
educational tools for surgical trainees.

摘要：基於擴散的影片生成模型已取得重大進展，能產生視覺保真度、時間相干性和使用者控制都得到改善的輸出。這些進展在改善外科教育方面極具前景，因為它能建構更逼真、多樣且互動的模擬環境。在此研究中，我們介紹了 SurGen，一個專門用於外科影片合成的文字導引擴散模型，能產生現有外科影片生成模型中解析度最高且持續時間最長的影片。我們使用標準影像和影片生成量度驗證了輸出的視覺和時間品質。此外，我們透過訓練於外科資料的深度學習分類器評估它們與對應文字提示的對齊程度。我們的結果證明了擴散模型作為外科受訓者寶貴教育工具的潛力。

##### **Empowering Low-Resource Language ASR via Large-Scale Pseudo Labeling**
2408.14026v1 by Kaushal Santosh Bhogale, Deovrat Mehendale, Niharika Parasa, Sathish Kumar Reddy G, Tahir Javed, Pratyush Kumar, Mitesh M. Khapra

In this study, we tackle the challenge of limited labeled data for
low-resource languages in ASR, focusing on Hindi. Specifically, we explore
pseudo-labeling, by proposing a generic framework combining multiple ideas from
existing works. Our framework integrates multiple base models for transcription
and evaluators for assessing audio-transcript pairs, resulting in robust
pseudo-labeling for low resource languages. We validate our approach with a new
benchmark, IndicYT, comprising diverse YouTube audio files from multiple
content categories. Our findings show that augmenting pseudo labeled data from
YouTube with existing training data leads to significant performance
improvements on IndicYT, without affecting performance on out-of-domain
benchmarks, demonstrating the efficacy of pseudo-labeled data in enhancing ASR
capabilities for low-resource languages. The benchmark, code and models
developed as a part of this work will be made publicly available.

摘要：在這項研究中，我們應對低資源語言在 ASR 中標籤資料有限的挑戰，專注於印地語。具體來說，我們探討偽標籤，提出一個通用框架，結合現有作品中的多個想法。我們的框架整合多個基礎模型，用於轉錄和評估器，用於評估音訊轉錄對，從而為低資源語言提供穩健的偽標籤。我們使用一個新的基準 IndicYT 來驗證我們的做法，其中包含來自多個內容類別的各種 YouTube 音訊檔案。我們的研究結果表明，使用現有訓練資料擴充來自 YouTube 的偽標籤資料，會顯著提升 IndicYT 的效能，而不會影響領域外基準的效能，這證明了偽標籤資料在提升低資源語言的 ASR 能力方面的效用。作為這項工作一部分開發的基準、程式碼和模型將公開提供。

##### **Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks for Short and Long Videos**
2408.14023v1 by Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, Hui Wang

Multi-modal large language models (MLLMs) have demonstrated considerable
potential across various downstream tasks that require cross-domain knowledge.
MLLMs capable of processing videos, known as Video-MLLMs, have attracted broad
interest in video-language understanding. However, videos, especially long
videos, contain more visual tokens than images, making them difficult for LLMs
to process. Existing works either downsample visual features or extend the LLM
context size, risking the loss of high-resolution information or slowing down
inference speed. To address these limitations, we apply cross-attention layers
in the intermediate projector between the visual encoder and the large language
model (LLM). As the naive cross-attention mechanism is insensitive to temporal
order, we further introduce causal cross-attention masks (CCAMs) within the
cross-attention layers. This Video-MLLM, named Video-CCAM, is trained in a
straightforward two-stage fashion: feature alignment and visual instruction
tuning. We develop several Video-CCAM models based on LLMs of different sizes
(4B, 9B, and 14B). Video-CCAM proves to be a robust Video-MLLM and shows
outstanding performance from short videos to long ones. Among standard video
benchmarks like MVBench and VideoChatGPT-QA, Video-CCAM shows outstanding
performances (1st/2nd/3rd in MVBench and TGIF-QA, 2nd/3rd/4th in MSVD-QA,
MSRVTT-QA, and ActivityNet-QA). In benchmarks encompassing long videos,
Video-CCAM models can be directly adapted to long video understanding and still
achieve exceptional scores despite being trained solely with images and
16-frame videos. Using 96 frames (6$\times$ the training number of frames),
Video-CCAM models rank 1st/2nd/3rd in VideoVista and 1st/2nd/4th in MLVU among
all open-source Video-MLLMs, respectively. The code is publicly available in
\url{https://github.com/QQ-MM/Video-CCAM}.

摘要：多模态大语言模型 (MLLM) 已在需要跨领域知识的各种下游任务中展示了巨大的潜力。能够处理视频的 MLLM（称为 Video-MLLM）引起了人们对视频语言理解的广泛兴趣。但是，视频（尤其是长视频）包含比图像更多的视觉标记，这使得 LLM 难以处理。现有的工作要么对视觉特征进行降采样，要么扩展 LLM 上下文大小，冒着丢失高分辨率信息或降低推理速度的风险。为了解决这些限制，我们在视觉编码器和大语言模型 (LLM) 之间的中间投影仪中应用交叉注意层。由于朴素的交叉注意机制对时间顺序不敏感，我们在交叉注意层中进一步引入了因果交叉注意掩码 (CCAM)。这个名为 Video-CCAM 的 Video-MLLM 以简单的两阶段方式进行训练：特征对齐和视觉指令调整。我们基于不同大小的 LLM（4B、9B 和 14B）开发了多个 Video-CCAM 模型。事实证明，Video-CCAM 是一款强大的 Video-MLLM，从短视频到长视频都表现出色。在 MVBench 和 VideoChatGPT-QA 等标准视频基准测试中，Video-CCAM 表现出色（在 MVBench 和 TGIF-QA 中排名第 1/2/3 位，在 MSVD-QA、MSRVTT-QA 和 ActivityNet-QA 中排名第 2/3/4 位）。在包含长视频的基准测试中，Video-CCAM 模型可以直接适应长视频理解，尽管仅使用图像和 16 帧视频进行训练，但仍能获得出色的分数。使用 96 帧（训练帧数的 6 倍），Video-CCAM 模型在所有开源 Video-MLLM 中分别在 VideoVista 中排名第 1/2/3 位，在 MLVU 中排名第 1/2/4 位。该代码可在 \url{https://github.com/QQ-MM/Video-CCAM} 中公开获得。

##### **Pixel-Aligned Multi-View Generation with Depth Guided Decoder**
2408.14016v1 by Zhenggang Tang, Peiye Zhuang, Chaoyang Wang, Aliaksandr Siarohin, Yash Kant, Alexander Schwing, Sergey Tulyakov, Hsin-Ying Lee

The task of image-to-multi-view generation refers to generating novel views
of an instance from a single image. Recent methods achieve this by extending
text-to-image latent diffusion models to multi-view version, which contains an
VAE image encoder and a U-Net diffusion model. Specifically, these generation
methods usually fix VAE and finetune the U-Net only. However, the significant
downscaling of the latent vectors computed from the input images and
independent decoding leads to notable pixel-level misalignment across multiple
views. To address this, we propose a novel method for pixel-level
image-to-multi-view generation. Unlike prior work, we incorporate attention
layers across multi-view images in the VAE decoder of a latent video diffusion
model. Specifically, we introduce a depth-truncated epipolar attention,
enabling the model to focus on spatially adjacent regions while remaining
memory efficient. Applying depth-truncated attn is challenging during inference
as the ground-truth depth is usually difficult to obtain and pre-trained depth
estimation models is hard to provide accurate depth. Thus, to enhance the
generalization to inaccurate depth when ground truth depth is missing, we
perturb depth inputs during training. During inference, we employ a rapid
multi-view to 3D reconstruction approach, NeuS, to obtain coarse depth for the
depth-truncated epipolar attention. Our model enables better pixel alignment
across multi-view images. Moreover, we demonstrate the efficacy of our approach
in improving downstream multi-view to 3D reconstruction tasks.

摘要：影像轉多視圖生成的任務，是指從單張影像生成一個實例的新視圖。最近的方法透過將文字轉影像的潛在擴散模型延伸至多視圖版本，其中包含一個 VAE 影像編碼器和一個 U-Net 擴散模型來達成此目的。具體來說，這些生成方法通常會固定 VAE 並僅微調 U-Net。然而，從輸入影像計算出的潛在向量的顯著縮放和獨立解碼會導致多個視圖之間產生明顯的像素級失準。為了解決這個問題，我們提出了一種用於像素級影像轉多視圖生成的新方法。與先前的研究不同，我們在潛在影片擴散模型的 VAE 解碼器中加入了跨多視圖影像的注意力層。具體來說，我們引入了深度截斷的極線注意力，使模型能夠專注於空間相鄰區域，同時保持記憶體效率。在推論過程中套用深度截斷的注意力具有挑戰性，因為通常難以取得真實深度，而預先訓練的深度估計模型也難以提供準確的深度。因此，為了在沒有真實深度的情況下增強對不準確深度的泛化，我們在訓練期間擾動深度輸入。在推論過程中，我們採用一種快速的從多視圖到 3D 重建方法 NeuS 來取得用於深度截斷的極線注意力的粗略深度。我們的模型能夠讓多視圖影像之間的像素對齊更佳。此外，我們展示了我們的方法在改善下游多視圖到 3D 重建任務方面的效力。

##### **LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models**
2408.14008v1 by Qihang Ge, Wei Sun, Yu Zhang, Yunhao Li, Zhongpeng Ji, Fengyu Sun, Shangling Jui, Xiongkuo Min, Guangtao Zhai

The explosive growth of videos on streaming media platforms has underscored
the urgent need for effective video quality assessment (VQA) algorithms to
monitor and perceptually optimize the quality of streaming videos. However, VQA
remains an extremely challenging task due to the diverse video content and the
complex spatial and temporal distortions, thus necessitating more advanced
methods to address these issues. Nowadays, large multimodal models (LMMs), such
as GPT-4V, have exhibited strong capabilities for various visual understanding
tasks, motivating us to leverage the powerful multimodal representation ability
of LMMs to solve the VQA task. Therefore, we propose the first Large
Multi-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel
spatiotemporal visual modeling strategy for quality-aware feature extraction.
Specifically, we first reformulate the quality regression problem into a
question and answering (Q&A) task and construct Q&A prompts for VQA instruction
tuning. Then, we design a spatiotemporal vision encoder to extract spatial and
temporal features to represent the quality characteristics of videos, which are
subsequently mapped into the language space by the spatiotemporal projector for
modality alignment. Finally, the aligned visual tokens and the quality-inquired
text tokens are aggregated as inputs for the large language model (LLM) to
generate the quality score and level. Extensive experiments demonstrate that
LMM-VQA achieves state-of-the-art performance across five VQA benchmarks,
exhibiting an average improvement of $5\%$ in generalization ability over
existing methods. Furthermore, due to the advanced design of the spatiotemporal
encoder and projector, LMM-VQA also performs exceptionally well on general
video understanding tasks, further validating its effectiveness. Our code will
be released at https://github.com/Sueqk/LMM-VQA.

摘要：串流媒體平台上影片的爆炸性成長，突顯出對有效影片品質評估 (VQA) 演算法的迫切需求，以監控和感知最佳化串流影片的品質。然而，由於影片內容多樣且空間和時間扭曲複雜，因此 VQA 仍然是一項極具挑戰性的任務，因此需要更進階的方法來解決這些問題。現今，大型多模態模型 (LMM)，例如 GPT-4V，已展現出對各種視覺理解任務的強大功能，促使我們利用 LMM 強大的多模態表示能力來解決 VQA 任務。因此，我們提出第一個大型多模態影片品質評估 (LMM-VQA) 模型，它引入了一種新的時空視覺建模策略，用於品質感知特徵萃取。具體來說，我們首先將品質回歸問題重新表述為問答 (Q&A) 任務，並建構 Q&A 提示以進行 VQA 指令調整。然後，我們設計一個時空視覺編碼器來萃取空間和時間特徵，以表示影片的品質特徵，這些特徵隨後由時空投影儀映射到語言空間以進行模態對齊。最後，將對齊的視覺符號和品質探討的文字符號彙集為大型語言模型 (LLM) 的輸入，以產生品質分數和等級。廣泛的實驗證明，LMM-VQA 在五個 VQA 基準測試中達到最先進的效能，與現有方法相比，其泛化能力平均提升了 5%。此外，由於時空編碼器和投影儀的先進設計，LMM-VQA 在一般的影片理解任務上也表現得非常好，進一步驗證了其效能。我們的程式碼將在 https://github.com/Sueqk/LMM-VQA 發布。

##### **Dual-CBA: Improving Online Continual Learning via Dual Continual Bias Adaptors from a Bi-level Optimization Perspective**
2408.13991v1 by Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, Minghao Zhou, Deyu Meng

In online continual learning (CL), models trained on changing distributions
easily forget previously learned knowledge and bias toward newly received
tasks. To address this issue, we present Continual Bias Adaptor (CBA), a
bi-level framework that augments the classification network to adapt to
catastrophic distribution shifts during training, enabling the network to
achieve a stable consolidation of all seen tasks. However, the CBA module
adjusts distribution shifts in a class-specific manner, exacerbating the
stability gap issue and, to some extent, fails to meet the need for continual
testing in online CL. To mitigate this challenge, we further propose a novel
class-agnostic CBA module that separately aggregates the posterior
probabilities of classes from new and old tasks, and applies a stable
adjustment to the resulting posterior probabilities. We combine the two kinds
of CBA modules into a unified Dual-CBA module, which thus is capable of
adapting to catastrophic distribution shifts and simultaneously meets the
real-time testing requirements of online CL. Besides, we propose Incremental
Batch Normalization (IBN), a tailored BN module to re-estimate its population
statistics for alleviating the feature bias arising from the inner loop
optimization problem of our bi-level framework. To validate the effectiveness
of the proposed method, we theoretically provide some insights into how it
mitigates catastrophic distribution shifts, and empirically demonstrate its
superiority through extensive experiments based on four rehearsal-based
baselines and three public continual learning benchmarks.

摘要：在線上持續學習 (CL) 中，在變化的分佈上訓練的模型很容易忘記先前學習的知識，並對新接收的任務產生偏差。為了解決這個問題，我們提出了持續偏差適應器 (CBA)，一個雙層架構，用於擴充分類網路以適應訓練期間的災難性分佈轉移，使網路能夠穩定整合所有已見的任務。然而，CBA 模組以類別特定的方式調整分佈轉移，加劇了穩定性差距問題，並且在某種程度上無法滿足線上 CL 中持續測試的需求。為了減輕這個挑戰，我們進一步提出了一個新穎的類別不可知 CBA 模組，它分別聚合來自新舊任務的類別後驗機率，並對結果後驗機率套用穩定的調整。我們將這兩種 CBA 模組結合到一個統一的 Dual-CBA 模組中，因此能夠適應災難性分佈轉移，並同時滿足線上 CL 的即時測試需求。此外，我們提出了增量批次正規化 (IBN)，一個量身打造的 BN 模組，用於重新估計其母體統計資料，以減輕源自我們雙層架構內部迴圈最佳化問題的功能偏差。為了驗證所提出方法的有效性，我們在理論上提供了一些見解，說明它如何減輕災難性分佈轉移，並透過基於四個基於複習的基準和三個公開持續學習基準的廣泛實驗，實證展示其優越性。

##### **Automatic Medical Report Generation: Methods and Applications**
2408.13988v1 by Li Guo, Anas M. Tahir, Dong Zhang, Z. Jane Wang, Rabab K. Ward

The increasing demand for medical imaging has surpassed the capacity of
available radiologists, leading to diagnostic delays and potential
misdiagnoses. Artificial intelligence (AI) techniques, particularly in
automatic medical report generation (AMRG), offer a promising solution to this
dilemma. This review comprehensively examines AMRG methods from 2021 to 2024.
It (i) presents solutions to primary challenges in this field, (ii) explores
AMRG applications across various imaging modalities, (iii) introduces publicly
available datasets, (iv) outlines evaluation metrics, (v) identifies techniques
that significantly enhance model performance, and (vi) discusses unresolved
issues and potential future research directions. This paper aims to provide a
comprehensive understanding of the existing literature and inspire valuable
future research.

摘要：由於對醫學影像的需求日益增長，已經超過了現有放射科醫師的能力，導致診斷延誤和潛在的誤診。人工智慧 (AI) 技術，特別是在自動醫療報告生成 (AMRG) 方面，為此困境提供了有希望的解決方案。本篇評論全面探討了 2021 年至 2024 年的 AMRG 方法。它 (i) 提出解決此領域中主要挑戰的方案，(ii) 探討 AMRG 在各種影像模式中的應用，(iii) 介紹公開可用的資料集，(iv) 概述評估指標，(v) 找出顯著提升模型效能的技術，以及 (vi) 討論尚未解決的問題和潛在的未來研究方向。本文旨在提供對現有文獻的全面了解，並激發有價值的未來研究。

##### **Question answering system of bridge design specification based on large language model**
2408.13282v1 by Leye Zhang, Xiangxiang Tian, Hongjun Zhang

This paper constructs question answering system for bridge design
specification based on large language model. Three implementation schemes are
tried: full fine-tuning of the Bert pretrained model, parameter-efficient
fine-tuning of the Bert pretrained model, and self-built language model from
scratch. Through the self-built question and answer task dataset, based on the
tensorflow and keras deep learning platform framework, the model is constructed
and trained to predict the start position and end position of the answer in the
bridge design specification given by the user. The experimental results show
that full fine-tuning of the Bert pretrained model achieves 100% accuracy in
the training-dataset, validation-dataset and test-dataset, and the system can
extract the answers from the bridge design specification given by the user to
answer various questions of the user; While parameter-efficient fine-tuning of
the Bert pretrained model and self-built language model from scratch perform
well in the training-dataset, their generalization ability in the test-dataset
needs to be improved. The research of this paper provides a useful reference
for the development of question answering system in professional field.

摘要：本文建立了基于大语言模型的桥梁设计规范问答系统。尝试了三种实现方案：Bert 预训练模型的完全微调、Bert 预训练模型的参数高效微调，以及从头开始构建自建语言模型。通过自建的问答任务数据集，基于 tensorflow 和 keras 深度学习平台框架，构建并训练模型，以预测用户给定的桥梁设计规范中答案的开始位置和结束位置。实验结果表明，Bert 预训练模型的完全微调在训练数据集、验证数据集和测试数据集中均达到 100% 的准确率，并且该系统可以从用户给定的桥梁设计规范中提取答案，以回答用户的各种问题；而 Bert 预训练模型的参数高效微调和从头开始构建的自建语言模型在训练数据集中表现良好，但在测试数据集中其泛化能力有待提高。本文的研究为专业领域问答系统的发展提供了有益的参考。

##### **Focused Large Language Models are Stable Many-Shot Learners**
2408.13987v1 by Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Heda Wang, Yao Hu, Kan Li

In-Context Learning (ICL) enables large language models (LLMs) to achieve
rapid task adaptation by learning from demonstrations. With the increase in
available context length of LLMs, recent experiments have shown that the
performance of ICL does not necessarily scale well in many-shot (demonstration)
settings. We theoretically and experimentally confirm that the reason lies in
more demonstrations dispersing the model attention from the query, hindering
its understanding of key content. Inspired by how humans learn from examples,
we propose a training-free method FocusICL, which conducts triviality filtering
to avoid attention being diverted by unimportant contents at token-level and
operates hierarchical attention to further ensure sufficient attention towards
current query at demonstration-level. We also design an efficient
hyperparameter searching strategy for FocusICL based on model perplexity of
demonstrations. Comprehensive experiments validate that FocusICL achieves an
average performance improvement of 5.2% over vanilla ICL and scales well with
many-shot demonstrations.

摘要：語境學習 (ICL) 讓大型語言模型 (LLM) 能夠透過學習示範來快速適應任務。隨著 LLM 可用語境長度的增加，最近的實驗顯示，在多範例（示範）設定中，ICL 的效能不一定能很好地擴展。我們在理論上和實驗上證實，原因在於更多的示範會分散模型對查詢的注意力，阻礙其理解關鍵內容。受到人類如何從範例中學習的啟發，我們提出了一種免訓練方法 FocusICL，它會進行瑣碎過濾，以避免注意力被代碼層級中不重要的內容分散，並操作階層式注意力，以進一步確保對示範層級中當前查詢有足夠的注意力。我們還根據示範的模型困惑度，為 FocusICL 設計了一種高效的超參數搜尋策略。全面的實驗驗證，FocusICL 在香草 ICL 上實現了平均效能提升 5.2%，並且在多範例示範中也能很好地擴展。

##### **AgentMove: Predicting Human Mobility Anywhere Using Large Language Model based Agentic Framework**
2408.13986v1 by Jie Feng, Yuwei Du, Jie Zhao, Yong Li

Human mobility prediction plays a crucial role in various real-world
applications. Although deep learning based models have shown promising results
over the past decade, their reliance on extensive private mobility data for
training and their inability to perform zero-shot predictions, have hindered
further advancements. Recently, attempts have been made to apply large language
models (LLMs) to mobility prediction task. However, their performance has been
constrained by the absence of a systematic design of workflow. They directly
generate the final output using LLMs, which limits the potential of LLMs to
uncover complex mobility patterns and underestimates their extensive reserve of
global geospatial knowledge. In this paper, we introduce AgentMove, a
systematic agentic prediction framework to achieve generalized mobility
prediction for any cities worldwide. In AgentMove, we first decompose the
mobility prediction task into three sub-tasks and then design corresponding
modules to complete these subtasks, including spatial-temporal memory for
individual mobility pattern mining, world knowledge generator for modeling the
effects of urban structure and collective knowledge extractor for capturing the
shared patterns among population. Finally, we combine the results of three
modules and conduct a reasoning step to generate the final predictions.
Extensive experiments on mobility data from two sources in 12 cities
demonstrate that AgentMove outperforms the best baseline more than 8% in
various metrics and it shows robust predictions with various LLMs as base and
also less geographical bias across cities. Codes and data can be found in
https://github.com/tsinghua-fib-lab/AgentMove.

摘要：人類移動預測在各種現實世界應用中扮演至關重要的角色。儘管基於深度學習的模型在過去十年中已展現出令人滿意的成果，但它們依賴於廣泛的私人移動資料進行訓練，且無法執行零次學習預測，已阻礙了進一步的進展。最近，人們已嘗試將大型語言模型 (LLM) 應用於行動預測任務。然而，它們的效能受到缺乏系統化的工作流程設計所限制。它們直接使用 LLM 產生最終輸出，這限制了 LLM 揭示複雜移動模式的潛力，並低估了它們廣泛的全球地理空間知識儲備。在本文中，我們介紹了 AgentMove，這是一個系統性的代理預測架構，用於達成全球任何城市的概化行動預測。在 AgentMove 中，我們首先將行動預測任務分解為三個子任務，然後設計對應的模組來完成這些子任務，包括用於個人行動模式挖掘的時空記憶體、用於建模城市結構影響的世界知識產生器，以及用於擷取人口中共享模式的集體知識萃取器。最後，我們結合三個模組的結果，並執行推理步驟以產生最終預測。來自 12 個城市中兩個來源的行動資料的廣泛實驗證明，AgentMove 在各種指標上比最佳基準高出 8% 以上，並且它顯示出以各種 LLM 為基礎的穩健預測，並且跨城市的地理偏差也較小。程式碼和資料可以在 https://github.com/tsinghua-fib-lab/AgentMove 中找到。

##### **TF-Attack: Transferable and Fast Adversarial Attacks on Large Language Models**
2408.13985v1 by Zelin Li, Kehai Chen, Xuefeng Bai, Lemao Liu, Mingming Yang, Yang Xiang, Min Zhang

With the great advancements in large language models (LLMs), adversarial
attacks against LLMs have recently attracted increasing attention. We found
that pre-existing adversarial attack methodologies exhibit limited
transferability and are notably inefficient, particularly when applied to LLMs.
In this paper, we analyze the core mechanisms of previous predominant
adversarial attack methods, revealing that 1) the distributions of importance
score differ markedly among victim models, restricting the transferability; 2)
the sequential attack processes induces substantial time overheads. Based on
the above two insights, we introduce a new scheme, named TF-Attack, for
Transferable and Fast adversarial attacks on LLMs. TF-Attack employs an
external LLM as a third-party overseer rather than the victim model to identify
critical units within sentences. Moreover, TF-Attack introduces the concept of
Importance Level, which allows for parallel substitutions of attacks. We
conduct extensive experiments on 6 widely adopted benchmarks, evaluating the
proposed method through both automatic and human metrics. Results show that our
method consistently surpasses previous methods in transferability and delivers
significant speed improvements, up to 20 times faster than earlier attack
strategies.

摘要：隨著大型語言模型 (LLM) 的重大進展，對 LLM 的對抗攻擊最近引起了越來越多的關注。我們發現，預先存在的對抗攻擊方法展示了有限的可轉移性，而且效率顯著低下，特別是應用於 LLM 時。在本文中，我們分析了先前主要對抗攻擊方法的核心機制，揭示了 1) 重要性分數的分布在受害者模型之間顯著不同，限制了可轉移性；2) 順序攻擊過程會造成大量的時間開銷。基於以上兩個見解，我們引入了一個名為 TF-Attack 的新方案，用於對 LLM 進行可轉移且快速的對抗攻擊。TF-Attack 使用外部 LLM 作為第三方監督者，而不是受害者模型，來識別句子中的關鍵單元。此外，TF-Attack 引入了重要性等級的概念，允許對攻擊進行並行替換。我們對 6 個廣泛採用的基準進行了大量的實驗，通過自動和人工指標評估了所提出的方法。結果表明，我們的方法在可轉移性方面始終優於以前的方法，並且提供了顯著的速度改進，比早期的攻擊策略快 20 倍。

##### **Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models**
2408.13979v1 by Shuai Fu, Xiequn Wang, Qiushi Huang, Yu Zhang

With the prevalence of large-scale pretrained vision-language models (VLMs),
such as CLIP, soft-prompt tuning has become a popular method for adapting these
models to various downstream tasks. However, few works delve into the inherent
properties of learnable soft-prompt vectors, specifically the impact of their
norms to the performance of VLMs. This motivates us to pose an unexplored
research question: ``Do we need to normalize the soft prompts in VLMs?'' To
fill this research gap, we first uncover a phenomenon, called the
\textbf{Low-Norm Effect} by performing extensive corruption experiments,
suggesting that reducing the norms of certain learned prompts occasionally
enhances the performance of VLMs, while increasing them often degrades it. To
harness this effect, we propose a novel method named \textbf{N}ormalizing
th\textbf{e} soft-pro\textbf{m}pt v\textbf{e}ctors of vi\textbf{si}on-language
model\textbf{s} (\textbf{Nemesis}) to normalize soft-prompt vectors in VLMs. To
the best of our knowledge, our work is the first to systematically investigate
the role of norms of soft-prompt vector in VLMs, offering valuable insights for
future research in soft-prompt tuning. The code is available at
\texttt{\href{https://github.com/ShyFoo/Nemesis}{https://github.com/ShyFoo/Nemesis}}.

摘要：隨著大規模預訓練視覺語言模型 (VLM) 的普及，例如 CLIP，軟提示調整已成為將這些模型適應到各種下游任務的流行方法。然而，很少有研究深入探討可學習軟提示向量的固有屬性，特別是它們的範數對 VLM 效能的影響。這促使我們提出一個未經探索的研究問題：``我們是否需要標準化 VLM 中的軟提示？'' 為填補這項研究空白，我們首先揭示了一個現象，稱為\textbf{低範數效應}，透過執行廣泛的破壞實驗，表明降低某些學習提示的範數偶爾會增強 VLM 的效能，而增加它們通常會降低效能。為了利用此效應，我們提出了一種名為\textbf{N}ormalizing th\textbf{e} soft-pro\textbf{m}pt v\textbf{e}ctors of vi\textbf{si}on-language model\textbf{s} (\textbf{Nemesis}) 的新方法，用於標準化 VLM 中的軟提示向量。據我們所知，我們的研究是第一個系統性地探討軟提示向量範數在 VLM 中的角色，為軟提示調整的未來研究提供寶貴的見解。程式碼可在\texttt{\href{https://github.com/ShyFoo/Nemesis}{https://github.com/ShyFoo/Nemesis}}取得。

##### **Reducing the Cost: Cross-Prompt Pre-Finetuning for Short Answer Scoring**
2408.13966v1 by Hiroaki Funayama, Yuya Asazuma, Yuichiroh Matsubayashi, Tomoya Mizumoto, Kentaro Inui

Automated Short Answer Scoring (SAS) is the task of automatically scoring a
given input to a prompt based on rubrics and reference answers. Although SAS is
useful in real-world applications, both rubrics and reference answers differ
between prompts, thus requiring a need to acquire new data and train a model
for each new prompt. Such requirements are costly, especially for schools and
online courses where resources are limited and only a few prompts are used. In
this work, we attempt to reduce this cost through a two-phase approach: train a
model on existing rubrics and answers with gold score signals and finetune it
on a new prompt. Specifically, given that scoring rubrics and reference answers
differ for each prompt, we utilize key phrases, or representative expressions
that the answer should contain to increase scores, and train a SAS model to
learn the relationship between key phrases and answers using already annotated
prompts (i.e., cross-prompts). Our experimental results show that finetuning on
existing cross-prompt data with key phrases significantly improves scoring
accuracy, especially when the training data is limited. Finally, our extensive
analysis shows that it is crucial to design the model so that it can learn the
task's general property.

摘要：自動化簡答評分（SAS）是根據評分標準和參考答案自動評分給定提示輸入的任務。儘管 SAS 在實際應用中很有用，但評分標準和參考答案在提示之間有所不同，因此需要獲取新數據並為每個新提示訓練模型。這些要求成本很高，特別是對於資源有限且僅使用少數提示的學校和線上課程。在這項工作中，我們嘗試通過兩階段方法來降低此成本：使用黃金分數信號訓練模型，並在新的提示中對其進行微調。具體來說，鑑於評分標準和參考答案因提示而異，我們利用關鍵詞組或代表性表達式（答案應包含這些內容以提高分數），並訓練 SAS 模型使用已註解提示（即跨提示）學習關鍵詞組和答案之間的關係。我們的實驗結果表明，使用關鍵詞組對現有跨提示數據進行微調可以顯著提高評分準確度，特別是在訓練數據有限的情況下。最後，我們廣泛的分析表明，設計模型以使其能夠學習任務的一般屬性至關重要。

##### **Time Series Analysis for Education: Methods, Applications, and Future Directions**
2408.13960v1 by Shengzhong Mao, Chaoli Zhang, Yichi Song, Jindong Wang, Xiao-Jun Zeng, Zenglin Xu, Qingsong Wen

Recent advancements in the collection and analysis of sequential educational
data have brought time series analysis to a pivotal position in educational
research, highlighting its essential role in facilitating data-driven
decision-making. However, there is a lack of comprehensive summaries that
consolidate these advancements. To the best of our knowledge, this paper is the
first to provide a comprehensive review of time series analysis techniques
specifically within the educational context. We begin by exploring the
landscape of educational data analytics, categorizing various data sources and
types relevant to education. We then review four prominent time series
methods-forecasting, classification, clustering, and anomaly
detection-illustrating their specific application points in educational
settings. Subsequently, we present a range of educational scenarios and
applications, focusing on how these methods are employed to address diverse
educational tasks, which highlights the practical integration of multiple time
series methods to solve complex educational problems. Finally, we conclude with
a discussion on future directions, including personalized learning analytics,
multimodal data fusion, and the role of large language models (LLMs) in
educational time series. The contributions of this paper include a detailed
taxonomy of educational data, a synthesis of time series techniques with
specific educational applications, and a forward-looking perspective on
emerging trends and future research opportunities in educational analysis. The
related papers and resources are available and regularly updated at the project
page.

摘要：近來在收集和分析循序教育資料的進展，使時間序列分析在教育研究中處於樞紐地位，突顯其在促進資料驅動決策中扮演的角色不可或缺。然而，缺乏將這些進展彙整的全面摘要。據我們所知，本文首次針對時間序列分析技術提供全面的回顧，特別是在教育背景中。我們從探討教育資料分析的現況開始，將各種與教育相關的資料來源和類型分類。接下來，我們回顧四種重要的時間序列方法：預測、分類、分群和異常偵測，說明它們在教育環境中的具體應用點。隨後，我們提出各種教育情境和應用，重點說明這些方法如何用於解決不同的教育任務，這突顯了整合多種時間序列方法以解決複雜教育問題的實務做法。最後，我們以討論未來方向作為結論，包括個人化學習分析、多模態資料融合，以及大型語言模型 (LLM) 在教育時間序列中的角色。本文的貢獻包括教育資料的詳細分類法、時間序列技術與特定教育應用之間的綜合，以及對教育分析中新興趨勢和未來研究機會的前瞻性觀點。相關論文和資源可在專案頁面取得，並定期更新。

##### **Bidirectional Awareness Induction in Autoregressive Seq2Seq Models**
2408.13959v1 by Jia Cheng Hu, Roberto Cavicchioli, Alessandro Capotondi

Autoregressive Sequence-To-Sequence models are the foundation of many Deep
Learning achievements in major research fields such as Vision and Natural
Language Processing. Despite that, they still present significant limitations.
For instance, when errors occur in the early steps of the prediction, the whole
output is severely affected. Such reliance on previously predicted tokens and
the inherent computational unfriendliness of sequential algorithms, motivated
researchers to explore different architectures and methods in the search for
bidirectional approaches. In this work, we introduce the Bidirectional
Awareness Induction (BAI), a training method that leverages a subset of
elements in the network, the Pivots, to perform bidirectional learning without
breaking the autoregressive constraints. To showcase its flexibility, we apply
the method to three architectures, the Transformer, ExpansionNet v2 and GPT,
then perform experiments over three tasks. Experimental results showcase BAI's
effectiveness on all selected tasks and architectures. In particular, we
observed an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU in
Neural Machine Translation, and 1.16 ROUGE in Text Summarization compared to
the respective baselines. Notably, BAI not only has a positive impact on models
trained from scratch but on pre-trained models as well. Such an aspect,
combined with the absence of architectural requirements synergizes well with
the current trend of LLMs.

摘要：自迴歸序列對序列模型是深度學習在主要研究領域（例如視覺和自然語言處理）中許多成就的基礎。儘管如此，它們仍然存在顯著的限制。例如，當預測的早期步驟發生錯誤時，整個輸出會受到嚴重影響。這種對先前預測的標記的依賴以及序列演算法固有的計算不友好性，促使研究人員探索不同的架構和方法，以尋找雙向方法。在這項工作中，我們介紹了雙向感知誘導 (BAI)，這是一種訓練方法，它利用網路中元素的子集，樞軸，在不打破自迴歸約束的情況下執行雙向學習。為了展示其靈活性，我們將該方法應用於三種架構，Transformer、ExpansionNet v2 和 GPT，然後對三項任務進行實驗。實驗結果展示了 BAI 在所有選定的任務和架構上的有效性。特別是，我們觀察到與各自的基線相比，圖像字幕增加了 2.4 CIDEr、神經機器翻譯增加了 4.96 BLEU，以及文本摘要增加了 1.16 ROUGE。值得注意的是，BAI 不僅對從頭開始訓練的模型有積極影響，而且對預訓練模型也有積極影響。這種方面與架構需求的缺失相結合，與 LLM 的當前趨勢產生了良好的協同作用。

##### **Prediction of COPD Using Machine Learning, Clinical Summary Notes, and Vital Signs**
2408.13958v1 by Negar Orangi-Fard

Chronic obstructive pulmonary disease (COPD) is a chronic inflammatory lung
disease that causes obstructed airflow from the lungs. In the United States,
more than 15.7 million Americans have been diagnosed with COPD, with 96% of
individuals living with at least one other chronic health condition. It is the
4th leading cause of death in the country. Over 2.2 million patients are
admitted to hospitals annually due to COPD exacerbations. Monitoring and
predicting patient exacerbations on-time could save their life. This paper
presents two different predictive models to predict COPD exacerbation using AI
and natural language processing (NLP) approaches. These models use respiration
summary notes, symptoms, and vital signs. To train and test these models, data
records containing physiologic signals and vital signs time series were used.
These records were captured from patient monitors and comprehensive clinical
data obtained from hospital medical information systems for tens of thousands
of Intensive Care Unit (ICU) patients. We achieved an area under the Receiver
operating characteristic (ROC) curve of 0.82 in detection and prediction of
COPD exacerbation.

摘要：慢性阻塞性肺疾病 (COPD) 是一種慢性發炎性肺部疾病，會導致肺部氣流阻塞。在美國，超過 1570 萬美國人被診斷出患有 COPD，其中 96% 的人至少還患有一種其他慢性健康疾病。它是該國第 4 大死因。每年有超過 220 萬患者因 COPD 急性發作而入院。及時監測和預測患者急性發作可以挽救他們的生命。本文提出了兩種不同的預測模型，使用 AI 和自然語言處理 (NLP) 方法來預測 COPD 急性發作。這些模型使用呼吸摘要筆記、症狀和生命徵象。為了訓練和測試這些模型，使用了包含生理信號和生命徵象時間序列的數據記錄。這些記錄從患者監測儀和從醫院醫療信息系統獲得的數萬名重症監護病房 (ICU) 患者的綜合臨床數據中獲取。我們在檢測和預測 COPD 急性發作方面達到了接收器操作特徵 (ROC) 曲線下的面積為 0.82。

##### **CoT Rerailer: Enhancing the Reliability of Large Language Models in Complex Reasoning Tasks through Error Detection and Correction**
2408.13940v1 by Guangya Wan, Yuqi Wu, Jie Chen, Sheng Li

Chain-of-Thought (CoT) prompting enhances Large Language Models (LLMs)
complex reasoning abilities by generating intermediate steps. However, these
steps can introduce hallucinations and accumulate errors. We propose the CoT
Rerailer to address these challenges, employing self-consistency and
multi-agent debate systems to identify and rectify errors in the reasoning
process. The CoT Rerailer first selects the most logically correct Reasoning
Path (RP) using consistency checks and critical evaluation by automated agents.
It then engages a multi-agent debate system to propose and validate corrections
to ensure the generation of an error-free intermediate logical path. The
corrected steps are then used to generate a revised reasoning chain to further
reduce hallucinations and enhance answer quality. We demonstrate the
effectiveness of our approach across diverse question-answering datasets in
various knowledge domains. The CoT Rerailer enhances the reliability of
LLM-generated reasoning, contributing to more trustworthy AI driven
decision-making processes.

摘要：連續思考 (CoT) 提示增強了大型語言模型 (LLM) 的複雜推理能力，藉由產生中間步驟來達成。然而，這些步驟可能會引入幻覺並累積錯誤。我們提出 CoT Rerailer 來解決這些挑戰，採用自我一致性和多重代理辯論系統來識別並修正推理過程中的錯誤。CoT Rerailer 首先使用一致性檢查和自動代理的批判性評估來選擇最合乎邏輯的推理路徑 (RP)。然後，它會參與多重代理辯論系統來提出並驗證修正，以確保產生無錯誤的中間邏輯路徑。然後使用修正的步驟來產生修改後的推理鏈，以進一步減少幻覺並增強答案品質。我們在各種知識領域中的不同問答資料集上展示了我們方法的有效性。CoT Rerailer 增強了 LLM 生成的推理的可信度，有助於更值得信賴的 AI 驅動決策制定過程。

##### **Learning to Move Like Professional Counter-Strike Players**
2408.13934v1 by David Durst, Feng Xie, Vishnu Sarukkai, Brennan Shacklett, Iuri Frosio, Chen Tessler, Joohwan Kim, Carly Taylor, Gilbert Bernstein, Sanjiban Choudhury, Pat Hanrahan, Kayvon Fatahalian

In multiplayer, first-person shooter games like Counter-Strike: Global
Offensive (CS:GO), coordinated movement is a critical component of high-level
strategic play. However, the complexity of team coordination and the variety of
conditions present in popular game maps make it impractical to author
hand-crafted movement policies for every scenario. We show that it is possible
to take a data-driven approach to creating human-like movement controllers for
CS:GO. We curate a team movement dataset comprising 123 hours of professional
game play traces, and use this dataset to train a transformer-based movement
model that generates human-like team movement for all players in a "Retakes"
round of the game. Importantly, the movement prediction model is efficient.
Performing inference for all players takes less than 0.5 ms per game step
(amortized cost) on a single CPU core, making it plausible for use in
commercial games today. Human evaluators assess that our model behaves more
like humans than both commercially-available bots and procedural movement
controllers scripted by experts (16% to 59% higher by TrueSkill rating of
"human-like"). Using experiments involving in-game bot vs. bot self-play, we
demonstrate that our model performs simple forms of teamwork, makes fewer
common movement mistakes, and yields movement distributions, player lifetimes,
and kill locations similar to those observed in professional CS:GO match play.

摘要：在多人第一人稱射擊遊戲中，例如《反恐精英：全球攻勢》(CS:GO)，協調移動是高階策略性遊戲中的關鍵組成部分。然而，團隊協調的複雜性和熱門遊戲地圖中出現的各種條件，使得為每個場景撰寫手工製作的移動策略變得不切實際。我們展示了採用資料驅動的方法來為 CS:GO 建立類似人類的移動控制器是可行的。我們策劃了一個團隊移動資料集，其中包含 123 小時的專業遊戲播放軌跡，並使用這個資料集來訓練一個基於Transformer的移動模型，該模型會為遊戲的「Retakes」回合中所有玩家產生類似人類的團隊移動。重要的是，移動預測模型是有效的。在單個 CPU 核心上，對所有玩家執行推理每場遊戲步驟不到 0.5 毫秒（攤銷成本），這使得它有可能用於當今的商業遊戲中。人類評估員評估我們的模型比市售機器人和專家編寫的程序移動控制器更像人類（根據「類似人類」的 TrueSkill 評分高出 16% 至 59%）。透過涉及遊戲內機器人對戰機器人自玩的實驗，我們證明我們的模型執行簡單形式的團隊合作，減少常見的移動錯誤，並產生與職業 CS:GO 比賽中觀察到的相似的移動分佈、玩家生命週期和擊殺位置。

##### **MobileQuant: Mobile-friendly Quantization for On-device Language Models**
2408.13933v1 by Fuwen Tan, Royson Lee, Łukasz Dudziak, Shell Xu Hu, Sourav Bhattacharya, Timothy Hospedales, Georgios Tzimiropoulos, Brais Martinez

Large language models (LLMs) have revolutionized language processing,
delivering outstanding results across multiple applications. However, deploying
LLMs on edge devices poses several challenges with respect to memory, energy,
and compute costs, limiting their widespread use in devices such as mobile
phones. A promising solution is to reduce the number of bits used to represent
weights and activations. While existing works have found partial success at
quantizing LLMs to lower bitwidths, e.g. 4-bit weights, quantizing activations
beyond 16 bits often leads to large computational overheads due to poor
on-device quantization support, or a considerable accuracy drop. Yet, 8-bit
activations are very attractive for on-device deployment as they would enable
LLMs to fully exploit mobile-friendly hardware, e.g. Neural Processing Units
(NPUs). In this work, we make a first attempt to facilitate the on-device
deployment of LLMs using integer-only quantization. We first investigate the
limitations of existing quantization methods for on-device deployment, with a
special focus on activation quantization. We then address these limitations by
introducing a simple post-training quantization method, named MobileQuant, that
extends previous weight equivalent transformation works by jointly optimizing
the weight transformation and activation range parameters in an end-to-end
manner. MobileQuant demonstrates superior capabilities over existing methods by
1) achieving near-lossless quantization on a wide range of LLM benchmarks, 2)
reducing latency and energy consumption by 20\%-50\% compared to current
on-device quantization strategies, 3) requiring limited compute budget, 4)
being compatible with mobile-friendly compute units, e.g. NPU.

摘要：大型語言模型 (LLM) 徹底改變了語言處理，在多種應用中提供了傑出的成果。然而，在邊緣裝置上部署 LLM 在記憶體、能源和運算成本方面提出了若干挑戰，限制了它們在行動電話等裝置中的廣泛使用。一個有前途的解決方案是減少用於表示權重和激活的位元數。儘管現有作品在將 LLM 量化為較低位寬度（例如 4 位元權重）方面取得了部分成功，但將激活量化到 16 位元以上通常會導致巨大的運算開銷，這是由於裝置上量化支援不佳或精確度大幅下降所致。然而，8 位元激活對於裝置上部署非常有吸引力，因為它們能讓 LLM 充分利用行動裝置友善的硬體，例如神經處理單元 (NPU)。在這項工作中，我們首次嘗試使用僅整數量化來促進 LLM 的裝置上部署。我們首先探討現有量化方法在裝置上部署的限制，特別關注激活量化。然後，我們透過引入一種名為 MobileQuant 的簡單訓練後量化方法來解決這些限制，該方法透過以端到端的方式共同最佳化權重轉換和激活範圍參數，來延伸先前的權重等效轉換工作。MobileQuant 展示了比現有方法更優越的能力，方法是 1) 在廣泛的 LLM 評量基準上達成近乎無損失的量化，2) 與目前的裝置上量化策略相比，將延遲和能源消耗降低了 20%-50%，3) 需要的運算預算有限，4) 與行動裝置友善的運算單元（例如 NPU）相容。

##### **FedGlu: A personalized federated learning-based glucose forecasting algorithm for improved performance in glycemic excursion regions**
2408.13926v1 by Darpit Dave, Kathan Vyas, Jagadish Kumaran Jayagopal, Alfredo Garcia, Madhav Erraguntla, Mark Lawley

Continuous glucose monitoring (CGM) devices provide real-time glucose
monitoring and timely alerts for glycemic excursions, improving glycemic
control among patients with diabetes. However, identifying rare events like
hypoglycemia and hyperglycemia remain challenging due to their infrequency.
Moreover, limited access to sensitive patient data hampers the development of
robust machine learning models. Our objective is to accurately predict glycemic
excursions while addressing data privacy concerns. To tackle excursion
prediction, we propose a novel Hypo-Hyper (HH) loss function, which
significantly improves performance in the glycemic excursion regions. The HH
loss function demonstrates a 46% improvement over mean-squared error (MSE) loss
across 125 patients. To address privacy concerns, we propose FedGlu, a machine
learning model trained in a federated learning (FL) framework. FL allows
collaborative learning without sharing sensitive data by training models
locally and sharing only model parameters across other patients. FedGlu
achieves a 35% superior glycemic excursion detection rate compared to local
models. This improvement translates to enhanced performance in predicting both,
hypoglycemia and hyperglycemia, for 105 out of 125 patients. These results
underscore the effectiveness of the proposed HH loss function in augmenting the
predictive capabilities of glucose predictions. Moreover, implementing models
within a federated learning framework not only ensures better predictive
capabilities but also safeguards sensitive data concurrently.

摘要：連續血糖監測 (CGM) 裝置提供即時葡萄糖監測和血糖波動的及時警示，改善糖尿病患者的血糖控制。然而，由於低血糖和高血糖事件發生的頻率低，因此識別這些罕見事件仍然具有挑戰性。此外，受限於敏感患者資料的取得，也阻礙了穩健機器學習模型的開發。我們的目標是在解決資料隱私問題的同時，準確預測血糖波動。為了應對血糖波動的預測，我們提出了一種新穎的低血糖-高血糖 (HH) 損失函數，這顯著改善了血糖波動區域的效能。HH 損失函數在 125 位患者中展現出比均方誤差 (MSE) 損失高出 46% 的改善。為了解決隱私問題，我們提出了 FedGlu，一種在聯合學習 (FL) 架構中訓練的機器學習模型。FL 允許透過在本地訓練模型並僅跨其他患者共享模型參數，在不共享敏感資料的情況下進行協作學習。與本地模型相比，FedGlu 達到了高出 35% 的血糖波動偵測率。對於 125 位患者中的 105 位，這種改善轉化為在預測低血糖和高血糖方面增強的效能。這些結果強調了所提出的 HH 損失函數在增強葡萄糖預測的預測能力方面的有效性。此外，在聯合學習架構中實作模型不僅確保了更好的預測能力，同時也保護了敏感資料。

##### **Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with Spatiotemporal Constraints**
2408.13918v1 by Siyu Li, Toan Tran, Haowen Lin, John Khrumm, Cyrus Shahabi, Li Xiong

Simulating human mobility data is essential for various application domains,
including transportation, urban planning, and epidemic control, since real data
are often inaccessible to researchers due to expensive costs and privacy
issues. Several existing deep generative solutions propose learning from real
trajectories to generate synthetic ones. Despite the progress, most of them
suffer from training stability issues and scale poorly with growing data size.
More importantly, they generally lack control mechanisms to steer the generated
trajectories based on spatiotemporal constraints such as fixing specific
visits. To address such limitations, we formally define the controlled
trajectory generation problem with spatiotemporal constraints and propose
Geo-Llama. This novel LLM-inspired framework enforces explicit visit
constraints in a contextually coherent way. It fine-tunes pre-trained LLMs on
trajectories with a visit-wise permutation strategy where each visit
corresponds to a time and location. This enables the model to capture the
spatiotemporal patterns regardless of visit orders and allows flexible and
in-context constraint integration through prompts during generation. Extensive
experiments on real-world and synthetic datasets validate the effectiveness of
Geo-Llama, demonstrating its versatility and robustness in handling a broad
range of constraints to generate more realistic trajectories compared to
existing methods.

摘要：模擬人類流動資料對於各種應用領域至關重要，
包括運輸、都市規劃和疫情控制，因為研究人員通常無法取得真實資料，原因在於成本昂貴且有隱私問題。現有幾個深度生成式解決方案提出從真實軌跡中學習，以產生合成軌跡。儘管有進展，但大多數解決方案都有訓練穩定性的問題，而且隨著資料量增加而擴充性不佳。更重要的是，它們通常缺乏控制機制，無法根據時空限制（例如修正特定拜訪）來引導產生的軌跡。為了解決這些限制，我們正式定義了具有時空限制的受控軌跡產生問題，並提出 Geo-Llama。這個新穎的 LLM 靈感架構以脈絡相符的方式強制執行明確的拜訪限制。它針對軌跡微調預先訓練的 LLM，並採用拜訪明智的排列策略，其中每個拜訪都對應到時間和地點。這讓模型能夠擷取時空模式，而與拜訪順序無關，並允許在產生過程中透過提示靈活地整合情境限制。對真實世界和合成資料集進行的廣泛實驗驗證了 Geo-Llama 的有效性，證明了它在處理各種限制時的多功能性和穩健性，與現有方法相比，它能產生更逼真的軌跡。

##### **LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback**
2408.13915v1 by Tanushree Banerjee, Richard Zhu, Runzhe Yang, Karthik Narasimhan

Large Language Models (LLMs) excel at generating human-like dialogues and
comprehending text. However, understanding the subtleties of complex exchanges
in language remains a challenge. We propose a bootstrapping framework that
leverages self-generated feedback to enhance LLM reasoning capabilities for lie
detection. The framework consists of three stages: suggestion, feedback
collection, and modification. In the suggestion stage, a cost-effective
language model generates initial predictions based on game state and dialogue.
The feedback-collection stage involves a language model providing feedback on
these predictions. In the modification stage, a more advanced language model
refines the initial predictions using the auto-generated feedback. We
investigate the application of the proposed framework for detecting betrayal
and deception in Diplomacy games, and compare it with feedback from
professional human players. The LLM-generated feedback exhibits superior
quality and significantly enhances the performance of the model. Our approach
achieves a 39% improvement over the zero-shot baseline in lying-F1 without the
need for any training data, rivaling state-of-the-art supervised learning
results.

摘要：大型語言模型 (LLM) 擅長生成類似人類的對話和理解文字。然而，理解語言中複雜交流的細微差別仍然是一個挑戰。我們提出一個自舉架構，利用自我生成的回饋來增強 LLM 推論能力以進行謊言偵測。這個架構包含三個階段：建議、回饋收集和修改。在建議階段，一個經濟實惠的語言模型會根據遊戲狀態和對話產生初始預測。回饋收集階段涉及一個語言模型對這些預測提供回饋。在修改階段，一個更先進的語言模型使用自動產生的回饋來改善初始預測。我們研究了所提出的架構在外交遊戲中偵測背叛和欺騙的應用，並將其與專業人類玩家的回饋進行比較。LLM 生成的回饋表現出優異的品質，並顯著提升了模型的效能。我們的做法在不需任何訓練資料的情況下，在說謊 F1 上達到了比零次學習基準高出 39% 的改進，媲美最先進的監督式學習結果。

##### **LowCLIP: Adapting the CLIP Model Architecture for Low-Resource Languages in Multimodal Image Retrieval Task**
2408.13909v1 by Ali Asgarov, Samir Rustamov

This research explores the development of multimodal vision-language models
for image retrieval in low-resource languages, specifically Azerbaijani.
Existing vision-language models primarily support high-resource languages, and
fine-tuning them remains computationally demanding. To address challenges in
vision-language retrieval for low-resource languages, we integrated the CLIP
model architecture and employed several techniques to balance computational
efficiency with performance. These techniques include synthetic data generation
through machine translation, image augmentation, and further training the
attention mechanisms of transformer-based models with domain-specific data. We
integrated Multilingual BERT as a text encoder with image encoders like
ResNet50, EfficientNet0, Vision Transformer (ViT), and Tiny Swin Transformer.
Our study found that models like EfficientNet0 and Tiny Swin Transformer
perform best on the datasets they were trained on, such as COCO, Flickr30k, and
Flickr8k. Augmentation techniques boosted EfficientNet0 MAP on Flickr30k from
0.84 to 0.87 and ResNet50 MAP on MSCOCO from 0.70 to 0.80, contributing to a
new state of the art in vision-language retrieval. We share our configurations
and results to support further research. Code and pre-trained models are
available at https://github.com/aliasgerovs/azclip.

摘要：本研究探索了多模態視覺語言模型在低資源語言（特別是亞塞拜然語）中進行影像檢索的發展。現有的視覺語言模型主要支援高資源語言，且微調它們在運算上仍然要求很高。為了應對低資源語言視覺語言檢索的挑戰，我們整合了 CLIP 模型架構，並採用了幾種技術來平衡運算效率與效能。這些技術包括透過機器翻譯、影像擴充，以及使用特定領域資料進一步訓練基於轉換器的模型的注意力機制，來生成合成資料。我們將多語言 BERT 整合為文本編碼器，並搭配 ResNet50、EfficientNet0、視覺轉換器 (ViT) 和 Tiny Swin Transformer 等影像編碼器。我們的研究發現，EfficientNet0 和 Tiny Swin Transformer 等模型在其受訓的資料集（例如 COCO、Flickr30k 和 Flickr8k）上表現最佳。擴充技術將 Flickr30k 上的 EfficientNet0 MAP 從 0.84 提升到 0.87，並將 MSCOCO 上的 ResNet50 MAP 從 0.70 提升到 0.80，為視覺語言檢索樹立了新的技術標準。我們分享我們的設定和結果，以支持進一步的研究。程式碼和預先訓練的模型可於 https://github.com/aliasgerovs/azclip 取得。

##### **ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models**
2408.13906v1 by Yeji Park, Deokyeong Lee, Junsuk Choe, Buru Chang

Hallucinations in Multimodal Large Language Models (MLLMs) where generated
responses fail to accurately reflect the given image pose a significant
challenge to their reliability. To address this, we introduce ConVis, a novel
training-free contrastive decoding method. ConVis leverages a text-to-image
(T2I) generation model to semantically reconstruct the given image from
hallucinated captions. By comparing the contrasting probability distributions
produced by the original and reconstructed images, ConVis enables MLLMs to
capture visual contrastive signals that penalize hallucination generation.
Notably, this method operates purely within the decoding process, eliminating
the need for additional data or model updates. Our extensive experiments on
five popular benchmarks demonstrate that ConVis effectively reduces
hallucinations across various MLLMs, highlighting its potential to enhance
model reliability.

摘要：多模态大型语言模型 (MLLM) 中的幻觉，其中生成的响应无法准确反映给定的图像，对它们的可靠性构成了重大挑战。为了解决这个问题，我们引入了 ConVis，这是一种新颖的无训练对比解码方法。ConVis 利用文本到图像 (T2I) 生成模型从幻觉标题中语义重建给定图像。通过比较原始图像和重建图像产生的对比概率分布，ConVis 使 MLLM 能够捕获视觉对比信号，从而惩罚幻觉生成。值得注意的是，此方法纯粹在解码过程中运行，无需额外的数据或模型更新。我们在五个流行基准上的广泛实验表明，ConVis 有效地减少了各种 MLLM 中的幻觉，突出了其增强模型可靠性的潜力。

##### **SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning**
2408.13891v1 by Chien-yu Huang, Min-Han Shih, Ke-Han Lu, Chi-Yuan Hsiao, Hung-yi Lee

Instruction-based speech processing is becoming popular. Studies show that
training with multiple tasks boosts performance, but collecting diverse,
large-scale tasks and datasets is expensive. Thus, it is highly desirable to
design a fundamental task that benefits other downstream tasks. This paper
introduces a multi-talker speaking style captioning task to enhance the
understanding of speaker and prosodic information. We used large language
models to generate descriptions for multi-talker speech. Then, we trained our
model with pre-training on this captioning task followed by instruction tuning.
Evaluation on Dynamic-SUPERB shows our model outperforming the baseline
pre-trained only on single-talker tasks, particularly in speaker and emotion
recognition. Additionally, tests on a multi-talker QA task reveal that current
models struggle with attributes such as gender, pitch, and speaking rate. The
code and dataset are available at https://github.com/cyhuang-tw/speechcaps.

摘要：基於指令的語音處理正變得越來越普及。研究顯示，透過多任務訓練可以提升效能，但收集多樣且大規模的任務和資料集成本昂貴。因此，設計一個能讓其他下游任務受益的基本任務非常重要。本文介紹了一項多說話者說話風格字幕任務，以增強對說話者和韻律資訊的理解。我們使用大型語言模型為多說話者語音產生描述。然後，我們使用在這個字幕任務上的預訓練訓練我們的模型，接著進行指令微調。在 Dynamic-SUPERB 上的評估顯示，我們的模型優於僅在單一說話者任務上進行預訓練的基準，特別是在說話者和情緒辨識方面。此外，在多說話者問答任務上的測試顯示，目前的模型在性別、音高和說話速度等屬性上仍有困難。程式碼和資料集可在 https://github.com/cyhuang-tw/speechcaps 取得。

##### **LLM with Relation Classifier for Document-Level Relation Extraction**
2408.13889v1 by Xingzuo Li, Kehai Chen, Yunfei Long, Min Zhang

Large language models (LLMs) create a new paradigm for natural language
processing. Despite their advancement, LLM-based methods still lag behind
traditional approaches in document-level relation extraction (DocRE), a
critical task for understanding complex entity relations. This paper
investigates the causes of this performance gap, identifying the dispersion of
attention by LLMs due to entity pairs without relations as a primary factor. We
then introduce a novel classifier-LLM approach to DocRE. The proposed approach
begins with a classifier specifically designed to select entity pair candidates
exhibiting potential relations and thereby feeds them to LLM for the final
relation extraction. This method ensures that during inference, the LLM's focus
is directed primarily at entity pairs with relations. Experiments on DocRE
benchmarks reveal that our method significantly outperforms recent LLM-based
DocRE models and achieves competitive performance with several leading
traditional DocRE models.

摘要：大型語言模型 (LLM) 為自然語言處理創造了新的典範。儘管它們進步神速，但基於 LLM 的方法在文件層級關係萃取 (DocRE) 方面仍落後於傳統方法，而這項任務對於理解複雜實體關係至關重要。本文探討了這種效能差距的原因，並找出 LLM 由於沒有關係的實體對而分散注意力，是造成這種差距的主要因素。然後，我們介紹一種新的分類器-LLM 方法來進行 DocRE。所提出的方法首先使用一個分類器，該分類器專門用於選取表現出潛在關係的實體對候選項，並將它們提供給 LLM 以進行最終的關係萃取。此方法可確保在推理過程中，LLM 的焦點主要集中在具有關係的實體對上。在 DocRE 基準上的實驗顯示，我們的模型顯著優於最近基於 LLM 的 DocRE 模型，並與多個領先的傳統 DocRE 模型達成競爭性的效能。

##### **Enhancing SQL Query Generation with Neurosymbolic Reasoning**
2408.13888v1 by Henrijs Princis, Cristina David, Alan Mycroft

Neurosymbolic approaches blend the effectiveness of symbolic reasoning with
the flexibility of neural networks. In this work, we propose a neurosymbolic
architecture for generating SQL queries that builds and explores a solution
tree using Best-First Search, with the possibility of backtracking. For this
purpose, it integrates a Language Model (LM) with symbolic modules that help
catch and correct errors made by the LM on SQL queries, as well as guiding the
exploration of the solution tree. We focus on improving the performance of
smaller open-source LMs, and we find that our tool, Xander, increases accuracy
by an average of 10.9% and reduces runtime by an average of 28% compared to the
LM without Xander, enabling a smaller LM (with Xander) to outperform its
four-times larger counterpart (without Xander).

摘要：神經符號方法結合了符號推理的有效性與神經網路的靈活性。在這項工作中，我們提出了一個神經符號架構，用於產生 SQL 查詢，該架構使用最佳優先搜尋建立並探索解決方案樹，並具有回溯的可能性。為了這個目的，它將語言模型 (LM) 與符號模組整合在一起，這些模組有助於捕捉和修正 LM 在 SQL 查詢中產生的錯誤，並引導解決方案樹的探索。我們專注於提升較小的開源 LM 的效能，我們發現我們的工具 Xander 平均將準確度提升了 10.9%，並將執行時間平均減少了 28%，與沒有 Xander 的 LM 相比，讓較小的 LM（搭配 Xander）能超越其大四倍的對手（沒有 Xander）。

##### **Flexible game-playing AI with AlphaViT: adapting to multiple games and board sizes**
2408.13871v1 by Kazuhisa Fujita

This paper presents novel game AI agents based on the AlphaZero framework,
enhanced with Vision Transformers (ViT): AlphaViT, AlphaViD, and AlphaVDA.
These agents are designed to play various board games of different sizes using
a single model, overcoming AlphaZero's limitation of being restricted to a
fixed board size. AlphaViT uses only a transformer encoder, while AlphaViD and
AlphaVDA contain both an encoder and a decoder. AlphaViD's decoder receives
input from the encoder output, while AlphaVDA uses a learnable matrix as
decoder input. Using the AlphaZero framework, the three proposed methods
demonstrate their versatility in different game environments, including
Connect4, Gomoku, and Othello. Experimental results show that these agents,
whether trained on a single game or on multiple games simultaneously,
consistently outperform traditional algorithms such as Minimax and Monte Carlo
tree search using a single DNN with shared weights, while approaching the
performance of AlphaZero. In particular, AlphaViT and AlphaViD show strong
performance across games, with AlphaViD benefiting from an additional decoder
layer that enhances its ability to adapt to different action spaces and board
sizes. These results may suggest the potential of transformer-based
architectures to develop more flexible and robust game AI agents capable of
excelling in multiple games and dynamic environments.

摘要：本文提出了基于 AlphaZero 框架的新型游戏 AI 代理，
并通过视觉转换器 (ViT) 进行了增强：AlphaViT、AlphaViD 和 AlphaVDA。
这些代理被设计为使用
单个模型玩各种不同大小的棋盘游戏，克服了 AlphaZero 仅限于
固定棋盘大小的限制。AlphaViT 仅使用转换器编码器，而 AlphaViD 和
AlphaVDA 同时包含编码器和解码器。AlphaViD 的解码器接收
来自编码器输出的输入，而 AlphaVDA 使用可学习的矩阵作为
解码器输入。使用 AlphaZero 框架，这三种提出的方法
展示了它们在不同游戏环境中的多功能性，包括
Connect4、五子棋和奥赛罗棋。实验结果表明，这些代理，
无论是在单场比赛中接受训练还是同时在多场比赛中接受训练，
都持续优于传统算法，例如 Minimax 和蒙特卡罗
使用具有共享权重的单一 DNN 进行树搜索，同时接近
AlphaZero 的性能。特别是，AlphaViT 和 AlphaViD 在游戏中表现出色，AlphaViD 受益于额外的解码器
层，增强了其适应不同动作空间和棋盘大小的能力
尺寸。这些结果可能表明基于转换器的潜力
架构开发更灵活、更强大的游戏 AI 代理，能够
在多场比赛和动态环境中表现出色。

##### **CodeGraph: Enhancing Graph Reasoning of LLMs with Code**
2408.13863v1 by Qiaolong Cai, Zhaowei Wang, Shizhe Diao, James Kwok, Yangqiu Song

With the increasing popularity of large language models (LLMs), reasoning on
basic graph algorithm problems is an essential intermediate step in assessing
their abilities to process and infer complex graph reasoning tasks. Existing
methods usually convert graph-structured data to textual descriptions and then
use LLMs for reasoning and computation. However, LLMs often produce computation
errors on arithmetic parts in basic graph algorithm problems, such as counting
number of edges. In addition, they struggle to control or understand the output
of the reasoning process, raising concerns about whether LLMs are simply
guessing. In this paper, we introduce CodeGraph, a method that encodes graph
problem solutions as code. The methods solve new graph problems by learning
from exemplars, generating programs, and executing them via a program
interpreter. Using the few-shot setting, we evaluate CodeGraph with the base
LLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and
Mixtral-8x7B Instruct. Experimental results on six tasks with six graph
encoding methods in the GraphQA dataset demonstrate that CodeGraph can boost
performance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on
the task. Compared to the existing methods, CodeGraph demonstrates strong
performance on arithmetic problems in graph tasks and offers a more
controllable and interpretable approach to the reasoning process.

摘要：隨著大型語言模型 (LLM) 的日漸普及，對基本圖形演算法問題進行推理是評估它們處理和推論複雜圖形推理任務的能力中一個重要的中間步驟。現有的方法通常會將圖形結構化的資料轉換成文字描述，然後使用 LLM 進行推理和運算。然而，LLM 通常會在基本圖形演算法問題中，例如計算邊緣數量，對算術部分產生運算錯誤。此外，它們難以控制或理解推理過程的輸出，這引發了 LLM 是否只是在猜測的疑慮。在本文中，我們介紹了 CodeGraph，這是一種將圖形問題解決方案編碼為程式碼的方法。這些方法透過學習範例、產生程式，並透過程式碼直譯器執行它們來解決新的圖形問題。使用少次嘗試設定，我們使用基礎 LLM 為 GPT-3.5 Turbo、Llama3-70B Instruct、Mixtral-8x22B Instruct 和 Mixtral-8x7B Instruct 來評估 CodeGraph。在 GraphQA 資料集中使用六種圖形編碼方法對六項任務進行的實驗結果表明，CodeGraph 可以將 LLM 中的圖形推理任務的效能提升 1.3% 到 58.6%，具體取決於任務。與現有方法相比，CodeGraph 在圖形任務中的算術問題上表現出強勁的效能，並為推理過程提供更具可控性和可解釋性的方法。

##### **Knowledge-Aware Reasoning over Multimodal Semi-structured Tables**
2408.13860v1 by Suyash Vardhan Mathur, Jainit Sushil Bafna, Kunal Kartik, Harshita Khandelwal, Manish Shrivastava, Vivek Gupta, Mohit Bansal, Dan Roth

Existing datasets for tabular question answering typically focus exclusively
on text within cells. However, real-world data is inherently multimodal, often
blending images such as symbols, faces, icons, patterns, and charts with
textual content in tables. With the evolution of AI models capable of
multimodal reasoning, it is pertinent to assess their efficacy in handling such
structured data. This study investigates whether current AI models can perform
knowledge-aware reasoning on multimodal structured data. We explore their
ability to reason on tables that integrate both images and text, introducing
MMTabQA, a new dataset designed for this purpose. Our experiments highlight
substantial challenges for current AI models in effectively integrating and
interpreting multiple text and image inputs, understanding visual context, and
comparing visual content across images. These findings establish our dataset as
a robust benchmark for advancing AI's comprehension and capabilities in
analyzing multimodal structured data.

摘要：現有的表格問答資料集通常只專注於儲存格內的文字。然而，真實世界的資料本質上是多模態的，通常會將符號、人臉、圖示、圖案和圖表等影像與表格中的文字內容混合在一起。隨著具備多模態推理能力的人工智慧模型的發展，評估它們在處理此類結構化資料時的效能至關重要。本研究探討了當前的人工智慧模型是否能夠對多模態結構化資料執行知識感知推理。我們探討了它們對整合影像和文字的表格進行推理的能力，並介紹了為此目的設計的新資料集 MMTabQA。我們的實驗突顯了當前的人工智慧模型在有效整合和詮釋多個文字和影像輸入、理解視覺脈絡以及比較影像中的視覺內容方面面臨的重大挑戰。這些發現確立了我們的資料集作為促進人工智慧理解和分析多模態結構化資料的能力的強大基準。

##### **Tangram: A Challenging Benchmark for Geometric Element Recognizing**
2408.13854v1 by Jiamin Tang, Chao Zhang, Xudong Zhu, Mengchi Liu

Significant advancements in Large Multimodal Models (LMMs) have enabled them
to tackle complex problems involving visual-mathematical reasoning. However,
their ability to identify geometric elements remains understudied. To bridge
this gap, we introduce Tangram, a novel benchmark designed to evaluate the
performance of LMMs on geometric element recognition. Tangram includes 1,080
diverse geometric diagrams sourced from primary and secondary school exams,
competitions, and textbooks, covering from simple basic geometric shapes to
complex combinations. Each diagram is associated with four questions, resulting
in a total of 4,320 visual-question-answer pairs. Unlike existing benchmarks
that seek higher-level cognition and reasoning, Tangram focuses on the
understanding of geometric elements, requiring models to perform a "simple but
interesting" counting task. Systematic evaluation of 10 prominent LMMs, such as
GPT-4o and Claude 3.5 Sonnet, shows that even in the seemingly simple task,
these models still face significant challenges. Notably, the overall accuracy
of the top performer across all tested models is only 56.8%, marking a
significant gap when compared to human performance. These findings highlight
the limitations of current multimodal artificial intelligence systems in
handling basic perception tasks, and will inspire the development of the next
generation of expert-level multimodal foundational models. The Tangram and
evaluation code will be available soon.

摘要：大型多模态模型 (LMM) 的重大进展使它们能够解决涉及视觉数学推理的复杂问题。然而，它们识别几何元素的能力仍未得到充分研究。为了弥合这一差距，我们引入了 Tangram，这是一个新颖的基准，旨在评估 LMM 在几何元素识别上的性能。Tangram 包含 1,080 个来自小学和中学考试、竞赛和教科书的多样化几何图形，涵盖从简单的基本几何形状到复杂的组合。每个图表都与四个问题相关，总共形成 4,320 个视觉问题答案对。与寻求更高级认知和推理的现有基准不同，Tangram 专注于对几何元素的理解，要求模型执行“简单但有趣”的计数任务。对 10 个著名的 LMM（例如 GPT-4o 和 Claude 3.5 Sonnet）进行的系统评估表明，即使在看似简单的任务中，这些模型仍然面临着重大挑战。值得注意的是，在所有测试模型中，表现最佳的模型的整体准确率仅为 56.8%，与人类表现相比存在显着差距。这些发现突出了当前多模态人工智能系统在处理基本感知任务时的局限性，并将激发下一代专家级多模态基础模型的发展。Tangram 和评估代码将很快提供。

##### **Condensed Sample-Guided Model Inversion for Knowledge Distillation**
2408.13850v1 by Kuluhan Binici, Shivam Aggarwal, Cihan Acar, Nam Trung Pham, Karianto Leman, Gim Hee Lee, Tulika Mitra

Knowledge distillation (KD) is a key element in neural network compression
that allows knowledge transfer from a pre-trained teacher model to a more
compact student model. KD relies on access to the training dataset, which may
not always be fully available due to privacy concerns or logistical issues
related to the size of the data. To address this, "data-free" KD methods use
synthetic data, generated through model inversion, to mimic the target data
distribution. However, conventional model inversion methods are not designed to
utilize supplementary information from the target dataset, and thus, cannot
leverage it to improve performance, even when it is available. In this paper,
we consider condensed samples, as a form of supplementary information, and
introduce a method for using them to better approximate the target data
distribution, thereby enhancing the KD performance. Our approach is versatile,
evidenced by improvements of up to 11.4% in KD accuracy across various datasets
and model inversion-based methods. Importantly, it remains effective even when
using as few as one condensed sample per class, and can also enhance
performance in few-shot scenarios where only limited real data samples are
available.

摘要：知識蒸餾 (KD) 是神經網路壓縮中的關鍵元素，它允許將知識從預先訓練好的教師模型傳遞到更精簡的學生模型。KD 依賴於訓練資料集的存取，由於隱私問題或與資料大小相關的後勤問題，這些資料集可能無法總是完全取得。為了解決這個問題，「無資料」KD 方法使用透過模型反演產生的合成資料，來模擬目標資料分佈。然而，傳統的模型反演方法並非設計成要利用目標資料集中的補充資訊，因此，即使在有補充資訊的情況下，也無法利用它來改善效能。在本文中，我們將濃縮樣本視為一種補充資訊，並介紹一種使用它們來更好地近似目標資料分佈的方法，從而增強 KD 效能。我們的做法很靈活，這從各種資料集和基於模型反演的方法中，KD 精確度最高提升了 11.4%，即可證明這一點。重要的是，即使每個類別只使用一個濃縮樣本，它仍然有效，並且可以在僅有少量真實資料樣本可用的少次嘗試情況下，增強效能。

##### **PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**
2408.13836v1 by Zifan Chen, Xinyu Nan, Jiazheng Li, Jie Zhao, Haifeng Li, Zilin Lin, Haoshen Li, Heyun Chen, Yiting Liu, Bin Dong, Li Zhang, Lei Tang

Volumetric segmentation is crucial for medical imaging but is often
constrained by labor-intensive manual annotations and the need for
scenario-specific model training. Furthermore, existing general segmentation
models are inefficient due to their design and inferential approaches.
Addressing this clinical demand, we introduce PropSAM, a propagation-based
segmentation model that optimizes the use of 3D medical structure information.
PropSAM integrates a CNN-based UNet for intra-slice processing with a
Transformer-based module for inter-slice propagation, focusing on structural
and semantic continuities to enhance segmentation across various modalities.
Distinctively, PropSAM operates on a one-view prompt, such as a 2D bounding box
or sketch mask, unlike conventional models that require two-view prompts. It
has demonstrated superior performance, significantly improving the Dice
Similarity Coefficient (DSC) across 44 medical datasets and various imaging
modalities, outperforming models like MedSAM and SegVol with an average DSC
improvement of 18.1%. PropSAM also maintains stable predictions despite prompt
deviations and varying propagation configurations, confirmed by one-way ANOVA
tests with P>0.5985 and P>0.6131, respectively. Moreover, PropSAM's efficient
architecture enables faster inference speeds (Wilcoxon rank-sum test, P<0.001)
and reduces user interaction time by 37.8% compared to two-view prompt models.
Its ability to handle irregular and complex objects with robust performance
further demonstrates its potential in clinical settings, facilitating more
automated and reliable medical imaging analyses with minimal retraining.

摘要：體積分割對於醫學影像至關重要，但通常受到耗費大量人力的標註和特定場景模型訓練需求的限制。此外，現有的通用分割模型由於其設計和推論方法而效率低下。為了滿足這項臨床需求，我們引入了 PropSAM，這是一種基於傳播的分割模型，優化了 3D 醫學結構資訊的使用。PropSAM 整合了一個基於 CNN 的 UNet，用於切片內處理，以及一個基於 Transformer 的模組，用於切片間傳播，重點關注結構和語義連續性，以增強各種模式下的分割。與需要兩視提示的傳統模型不同，PropSAM 獨特地運作於單視提示上，例如 2D 邊界框或草圖遮罩。它已證明具有優異的效能，顯著改善了 44 個醫學資料集和各種影像模式下的骰子相似係數 (DSC)，優於 MedSAM 和 SegVol 等模型，平均 DSC 提升了 18.1%。儘管提示偏差和傳播配置不同，PropSAM 仍能維持穩定的預測，這已通過單向 ANOVA 測試得到證實，分別為 P>0.5985 和 P>0.6131。此外，PropSAM 的高效架構能實現更快的推論速度（Wilcoxon 等級和總和檢定，P<0.001），並將使用者互動時間減少了 37.8%，優於兩視提示模型。它在處理不規則和複雜物件時能展現出穩健的效能，進一步證明了其在臨床環境中的潛力，有助於以最少的重新訓練進行更自動化和可靠的醫學影像分析。

##### **Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data**
2408.13833v1 by Felix J. Dorfner, Amin Dada, Felix Busch, Marcus R. Makowski, Tianyu Han, Daniel Truhn, Jens Kleesiek, Madhumita Sushil, Jacqueline Lammert, Lisa C. Adams, Keno K. Bressem

Large language models (LLMs) have shown potential in biomedical applications,
leading to efforts to fine-tune them on domain-specific data. However, the
effectiveness of this approach remains unclear. This study evaluates the
performance of biomedically fine-tuned LLMs against their general-purpose
counterparts on a variety of clinical tasks. We evaluated their performance on
clinical case challenges from the New England Journal of Medicine (NEJM) and
the Journal of the American Medical Association (JAMA) and on several clinical
tasks (e.g., information extraction, document summarization, and clinical
coding). Using benchmarks specifically chosen to be likely outside the
fine-tuning datasets of biomedical models, we found that biomedical LLMs mostly
perform inferior to their general-purpose counterparts, especially on tasks not
focused on medical knowledge. While larger models showed similar performance on
case tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA
cases), smaller biomedical models showed more pronounced underperformance
(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).
Similar trends were observed across the CLUE (Clinical Language Understanding
Evaluation) benchmark tasks, with general-purpose models often performing
better on text generation, question answering, and coding tasks. Our results
suggest that fine-tuning LLMs to biomedical data may not provide the expected
benefits and may potentially lead to reduced performance, challenging
prevailing assumptions about domain-specific adaptation of LLMs and
highlighting the need for more rigorous evaluation frameworks in healthcare AI.
Alternative approaches, such as retrieval-augmented generation, may be more
effective in enhancing the biomedical capabilities of LLMs without compromising
their general knowledge.

摘要：大型語言模型 (LLM) 在生物醫學應用中展現了潛力，導致人們努力針對特定領域的資料微調這些模型。然而，這種方法的有效性仍不明確。本研究評估了生物醫學微調 LLM 與其一般用途對應模型在各種臨床任務上的表現。我們評估了它們在《新英格蘭醫學雜誌》(NEJM) 和《美國醫學會雜誌》(JAMA) 的臨床案例挑戰以及在幾個臨床任務（例如資訊萃取、文件摘要和臨床編碼）上的表現。使用特別選擇的基準，這些基準很可能超出生物醫學模型的微調資料集，我們發現生物醫學 LLM 的表現大多遜於其一般用途對應模型，特別是在不專注於醫學知識的任務上。雖然較大的模型在案例任務上表現出類似的表現（例如 OpenBioLLM-70B：66.4% 對 Llama-3-70B-Instruct：65% 在 JAMA 案例中），但較小的生物醫學模型表現出更明顯的表現不佳（例如 OpenBioLLM-8B：30% 對 Llama-3-8B-Instruct：64.3% 在 NEJM 案例中）。在 CLUE（臨床語言理解評估）基準任務中觀察到類似的趨勢，一般用途模型通常在文字生成、問題解答和編碼任務上表現得更好。我們的結果表明，將 LLM 微調到生物醫學資料可能無法提供預期的益處，並且可能會導致效能降低，這挑戰了關於 LLM 領域特定適應的普遍假設，並強調了在醫療保健 AI 中需要更嚴謹的評估框架。其他方法，例如檢索增強生成，可能更有效地增強 LLM 的生物醫學能力，同時不損害其一般知識。

##### **RoCP-GNN: Robust Conformal Prediction for Graph Neural Networks in Node-Classification**
2408.13825v1 by S. Akansha

Graph Neural Networks (GNNs) have emerged as powerful tools for predicting
outcomes in graph-structured data. However, a notable limitation of GNNs is
their inability to provide robust uncertainty estimates, which undermines their
reliability in contexts where errors are costly. One way to address this issue
is by providing prediction sets that contain the true label with a predefined
probability margin. Our approach builds upon conformal prediction (CP), a
framework that promises to construct statistically robust prediction sets or
intervals. There are two primary challenges: first, given dependent data like
graphs, it is unclear whether the critical assumption in CP - exchangeability -
still holds when applied to node classification. Second, even if the
exchangeability assumption is valid for conformalized link prediction, we need
to ensure high efficiency, i.e., the resulting prediction set or the interval
length is small enough to provide useful information. In this article, we
propose a novel approach termed Robust Conformal Prediction for GNNs
(RoCP-GNN), which integrates conformal prediction (CP) directly into the GNN
training process. This method generates prediction sets, instead of just point
predictions, that are valid at a user-defined confidence level, assuming only
exchangeability. Our approach robustly predicts outcomes with any predictive
GNN model while quantifying the uncertainty in predictions within the realm of
graph-based semi-supervised learning (SSL). Experimental results demonstrate
that GNN models with size loss provide a statistically significant increase in
performance. We validate our approach on standard graph benchmark datasets by
coupling it with various state-of-the-art GNNs in node classification. The code
will be made available after publication.

摘要：圖形神經網路 (GNN) 已成為預測圖形結構資料中結果的強大工具。然而，GNN 的一個顯著限制是它們無法提供穩健的不確定性估計，這會損害它們在錯誤成本高昂的環境中的可靠性。解決此問題的方法之一是提供包含真標籤和預定義機率邊界的預測集合。我們的做法建立在共形預測 (CP) 之上，這是一個有望建構統計穩健預測集合或區間的框架。有兩個主要的挑戰：首先，給定依賴資料（例如圖形），尚不清楚 CP 中的關鍵假設（可交換性）在應用於節點分類時是否仍然成立。其次，即使可交換性假設對共形化連結預測有效，我們也需要確保高效率，即產生的預測集合或區間長度足夠小以提供有用的資訊。在本文中，我們提出了一種稱為 GNN 的穩健共形預測 (RoCP-GNN) 的新方法，它將共形預測 (CP) 直接整合到 GNN 訓練過程中。此方法會產生預測集合（而非僅點預測），這些集合在使用者定義的信心水準下有效，僅假設可交換性。我們的做法穩健地預測具有任何預測 GNN 模型的結果，同時量化基於圖形的半監督式學習 (SSL) 領域中預測的不確定性。實驗結果表明，具有大小損失的 GNN 模型提供了統計上顯著的效能提升。我們透過將我們的做法與節點分類中的各種最先進的 GNN 結合，在標準圖形基準資料集上驗證我們的做法。程式碼將在出版後提供。

##### **Revisiting the Exit from Nuclear Energy in Germany with NLP**
2408.13810v1 by Sebastian Haunss, André Blessing

Annotation of political discourse is resource-intensive, but recent
developments in NLP promise to automate complex annotation tasks. Fine-tuned
transformer-based models outperform human annotators in some annotation tasks,
but they require large manually annotated training datasets. In our
contribution, we explore to which degree a manually annotated dataset can be
automatically replicated with today's NLP methods, using unsupervised machine
learning and zero- and few-shot learning.

摘要：政治話語的註解需要大量資源，但自然語言處理 (NLP) 的最新發展承諾自動化複雜的註解任務。微調後的基於轉換器的模型在某些註解任務中優於人工註解員，但它們需要大量手動註解的訓練資料集。在我們的貢獻中，我們探討了在使用非監督式機器學習和零次和少次學習的情況下，當今的自然語言處理方法可以在多大程度上自動複製人工註解的資料集。

##### **Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models**
2408.13808v1 by Duy Khoa Pham, Bao Quoc Vo

The rapid advancement of large language models (LLMs) has significantly
impacted various domains, including healthcare and biomedicine. However, the
phenomenon of hallucination, where LLMs generate outputs that deviate from
factual accuracy or context, poses a critical challenge, especially in
high-stakes domains. This paper conducts a scoping study of existing techniques
for mitigating hallucinations in knowledge-based task in general and especially
for medical domains. Key methods covered in the paper include
Retrieval-Augmented Generation (RAG)-based techniques, iterative feedback
loops, supervised fine-tuning, and prompt engineering. These techniques, while
promising in general contexts, require further adaptation and optimization for
the medical domain due to its unique demands for up-to-date, specialized
knowledge and strict adherence to medical guidelines. Addressing these
challenges is crucial for developing trustworthy AI systems that enhance
clinical decision-making and patient safety as well as accuracy of biomedical
scientific research.

摘要：大型語言模型 (LLM) 的快速進展已顯著影響各種領域，包括醫療保健和生物醫學。然而，幻覺現象，其中 LLM 會產生偏離事實準確性或脈絡的輸出，構成了關鍵挑戰，特別是在高風險領域。本文對現有技術進行範圍研究，以減輕基於知識任務中的幻覺，特別是針對醫療領域。本文涵蓋的主要方法包括檢索增強生成 (RAG) 基於技術、反覆回饋迴路、監督微調和提示工程。這些技術雖然在一般情況下很有前景，但由於醫療領域對最新、專業知識和嚴格遵守醫療準則的獨特需求，需要進一步適應和優化。解決這些挑戰對於開發可信賴的人工智慧系統至關重要，這些系統可以增強臨床決策制定和患者安全，以及生物醫學科學研究的準確性。

##### **Lecture Notes on Linear Neural Networks: A Tale of Optimization and Generalization in Deep Learning**
2408.13767v1 by Nadav Cohen, Noam Razin

These notes are based on a lecture delivered by NC on March 2021, as part of
an advanced course in Princeton University on the mathematical understanding of
deep learning. They present a theory (developed by NC, NR and collaborators) of
linear neural networks -- a fundamental model in the study of optimization and
generalization in deep learning. Practical applications born from the presented
theory are also discussed. The theory is based on mathematical tools that are
dynamical in nature. It showcases the potential of such tools to push the
envelope of our understanding of optimization and generalization in deep
learning. The text assumes familiarity with the basics of statistical learning
theory. Exercises (without solutions) are included.

摘要：這些筆記基於 NC 於 2021 年 3 月在普林斯頓大學高級課程中關於深度學習的數學理解所發表的演講。它們提出了一個理論（由 NC、NR 和合作者開發）——線性神經網路——一個在深度學習中研究最佳化和概化的基本模型。理論中衍生的實用應用也已討論過。該理論基於本質上動態的數學工具。它展示了此類工具在推進我們對深度學習中最佳化和概化的理解的潛力。本文假設讀者熟悉統計學習理論的基本知識。其中包括練習（無解答）。

##### **Multi-Agent Target Assignment and Path Finding for Intelligent Warehouse: A Cooperative Multi-Agent Deep Reinforcement Learning Perspective**
2408.13750v1 by Qi Liu, Jianqi Gao, Dongjie Zhu, Xizheng Pang, Pengbin Chen, Jingxiang Guo, Yanjie Li

Multi-agent target assignment and path planning (TAPF) are two key problems
in intelligent warehouse. However, most literature only addresses one of these
two problems separately. In this study, we propose a method to simultaneously
solve target assignment and path planning from a perspective of cooperative
multi-agent deep reinforcement learning (RL). To the best of our knowledge,
this is the first work to model the TAPF problem for intelligent warehouse to
cooperative multi-agent deep RL, and the first to simultaneously address TAPF
based on multi-agent deep RL. Furthermore, previous literature rarely considers
the physical dynamics of agents. In this study, the physical dynamics of the
agents is considered. Experimental results show that our method performs well
in various task settings, which means that the target assignment is solved
reasonably well and the planned path is almost shortest. Moreover, our method
is more time-efficient than baselines.

摘要：多智能體目標指派和路徑規劃 (TAPF) 是智慧型倉庫中的兩個關鍵問題。然而，大多數文獻僅分別探討這兩個問題中的其中一個。在本研究中，我們提出一個方法，從合作多智能體深度強化學習 (RL) 的角度同時解決目標指派和路徑規劃。據我們所知，這是第一個針對智慧型倉庫建立 TAPF 問題模型以進行合作多智能體深度 RL 的研究，也是第一個基於多智能體深度 RL 同時解決 TAPF 的研究。此外，先前的文獻很少考慮智能體的物理動力學。在本研究中，考慮了智能體的物理動力學。實驗結果顯示，我們的模型在各種任務設定中表現良好，這表示目標指派獲得合理的解決，且規劃的路徑幾乎是最短的。此外，我們的模型比基準方法更省時。

##### **DOCE: Finding the Sweet Spot for Execution-Based Code Generation**
2408.13745v1 by Haau-Sing Li, Patrick Fernandes, Iryna Gurevych, André F. T. Martins

Recently, a diverse set of decoding and reranking procedures have been shown
effective for LLM-based code generation. However, a comprehensive framework
that links and experimentally compares these methods is missing. We address
this by proposing Decoding Objectives for Code Execution, a comprehensive
framework that includes candidate generation, $n$-best reranking, minimum Bayes
risk (MBR) decoding, and self-debugging as the core components. We then study
the contributions of these components through execution-based evaluation
metrics. Our findings highlight the importance of execution-based methods and
the difference gap between execution-based and execution-free methods.
Furthermore, we assess the impact of filtering based on trial unit tests, a
simple and effective strategy that has been often overlooked in prior works. We
also propose self-debugging on multiple candidates, obtaining state-of-the-art
performance on reranking for code generation. We expect our framework to
provide a solid guideline for future research on code generation.

摘要：最近，一系列不同的解码和重新排序程序已被证明对基于 LLM 的代码生成有效。然而，一个将这些方法联系起来并进行实验比较的综合框架缺失。我们通过提出代码执行的解码目标来解决这个问题，这是一个综合框架，包括候选生成、n 佳重新排序、最小贝叶斯风险 (MBR) 解码和自调试作为核心组件。然后，我们通过基于执行的评估指标研究这些组件的贡献。我们的发现突出了基于执行的方法的重要性以及基于执行的方法和不基于执行的方法之间的差异差距。此外，我们评估了基于试用单元测试进行过滤的影响，这是一种简单有效的策略，在以前的工作中经常被忽视。我们还提出了对多个候选进行自调试，获得了代码生成重新排序的最新性能。我们希望我们的框架为未来的代码生成研究提供一个可靠的指导方针。

##### **Literary and Colloquial Tamil Dialect Identification**
2408.13739v1 by M. Nanmalar, P. Vijayalakshmi, T. Nagarajan

Culture and language evolve together. The old literary form of Tamil is used
commonly for writing and the contemporary colloquial Tamil is used for
speaking. Human-computer interaction applications require Colloquial Tamil (CT)
to make it more accessible and easy for the everyday user and, it requires
Literary Tamil (LT) when information is needed in a formal written format.
Continuing the use of LT alongside CT in computer aided language learning
applications will both preserve LT, and provide ease of use via CT, at the same
time. Hence there is a need for the conversion between LT and CT dialects,
which demands as a first step, dialect identification. Dialect Identification
(DID) of LT and CT is an unexplored area of research. In the current work,
keeping the nuances of both these dialects in mind, five methods are explored
which include two implicit methods - Gaussian Mixture Model (GMM) and
Convolutional Neural Network (CNN); two explicit methods - Parallel Phone
Recognition (PPR) and Parallel Large Vocabulary Continuous Speech Recognition
(P-LVCSR); two versions of the proposed explicit Unified Phone Recognition
method (UPR-1 and UPR-2). These methods vary based on: the need for annotated
data, the size of the unit, the way in which modelling is carried out, and the
way in which the final decision is made. Even though the average duration of
the test utterances is less - 4.9s for LT and 2.5s for CT - the systems
performed well, offering the following identification accuracies: 87.72% (GMM),
93.97% (CNN), 89.24% (PPR), 94.21% (P-LVCSR), 88.57% (UPR-1), 93.53% (UPR-1
with P-LVCSR), 94.55% (UPR-2), and 95.61% (UPR-2 with P-LVCSR).

摘要：文化和語言共同演化。泰米爾語的舊文學形式通常用於寫作，而現代口語泰米爾語則用於口語。人機互動應用程式需要口語泰米爾語 (CT)，以使其更易於日常使用者存取和使用，並且在需要以正式書面格式提供資訊時，需要使用書面泰米爾語 (LT)。在電腦輔助語言學習應用程式中持續使用 LT 和 CT，既可以保留 LT，又可以透過 CT 提供易用性。因此，需要在 LT 和 CT 方言之間進行轉換，這首先需要方言識別。LT 和 CT 的方言識別 (DID) 是尚未探索的研究領域。在目前的工作中，考慮到這兩種方言的細微差別，探索了五種方法，其中包括兩種隱式方法 - 高斯混合模型 (GMM) 和卷積神經網路 (CNN)；兩種顯式方法 - 平行音素辨識 (PPR) 和平行大詞彙連續語音辨識 (P-LVCSR)；兩種版本的建議顯式統一音素辨識方法 (UPR-1 和 UPR-2)。這些方法根據以下條件而有所不同：對註解資料的需求、單元的規模、建模的方式以及做出最終決定的方式。儘管測試語句的平均持續時間較短 - LT 為 4.9 秒，CT 為 2.5 秒 - 但系統表現良好，提供了以下識別準確度：87.72% (GMM)、93.97% (CNN)、89.24% (PPR)、94.21% (P-LVCSR)、88.57% (UPR-1)、93.53% (UPR-1 與 P-LVCSR)、94.55% (UPR-2) 和 95.61% (UPR-2 與 P-LVCSR)。

##### **Poor-Supervised Evaluation for SuperLLM via Mutual Consistency**
2408.13738v1 by Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda Wang, Yao Hu, Kan Li

The guidance from capability evaluations has greatly propelled the progress
of both human society and Artificial Intelligence. However, as LLMs evolve, it
becomes challenging to construct evaluation benchmarks for them with accurate
labels on hard tasks that approach the boundaries of human capabilities. To
credibly conduct evaluation without accurate labels (denoted as poor-supervised
evaluation), we propose the PoEM framework. We first prove that the capability
of a model can be equivalently assessed by the consistency between it and
certain reference model, when their prediction distributions are independent
and the sample size is infinite. To alleviate the insufficiencies of the
conditions in reality, we further introduce an algorithm that treats humans
(when available) and the models under evaluation as reference models,
alternately conducting model weights calibration and filtering during E-step
and M-step. Comprehensive experiments across 3 types of tasks with 16
mainstream LLMs have shown that PoEM under poor supervision can achieve an
average of 0.98 Pearson correlation coefficient with supervised evaluation
results, demonstrating good effectiveness, efficiency and generalizability.
More generally, PoEM has advanced the evaluation paradigm evolution from
human-centric to human&model-centric by treating both of them as reference
models, mitigating the limitations of human evaluation in the era of LLMs.

摘要：能力评估的指导极大地推动了人类社会和人工智能的进步。然而，随着 LLM 的发展，为它们构建评估基准变得具有挑战性，因为在接近人类能力边界的困难任务上准确标记标签。为了在没有准确标签的情况下可信地进行评估（表示为弱监督评估），我们提出了 PoEM 框架。我们首先证明，当模型的预测分布独立且样本量无穷大时，可以通过模型与特定参考模型之间的一致性来等效评估模型的能力。为了缓解现实条件的不足，我们进一步引入了一种算法，将人类（如果可用）和正在评估的模型视为参考模型，在 E 步和 M 步期间交替进行模型权重校准和过滤。在 16 个主流 LLM 的 3 种类型任务中的综合实验表明，在弱监督下的 PoEM 可以实现与监督评估结果平均 0.98 的 Pearson 相关系数，证明了良好的有效性、效率和通用性。更一般地说，PoEM 通过将人类和模型都视为参考模型，将评估范式演变从以人为中心推进到以人为中心和以模型为中心，从而减轻了在 LLM 时代人类评估的局限性。

##### **LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models**
2408.13727v1 by Aoxiao Zhong, Dengyao Mo, Guiyang Liu, Jinbu Liu, Qingda Lu, Qi Zhou, Jiesheng Wu, Quanzheng Li, Qingsong Wen

Logs are ubiquitous digital footprints, playing an indispensable role in
system diagnostics, security analysis, and performance optimization. The
extraction of actionable insights from logs is critically dependent on the log
parsing process, which converts raw logs into structured formats for downstream
analysis. Yet, the complexities of contemporary systems and the dynamic nature
of logs pose significant challenges to existing automatic parsing techniques.
The emergence of Large Language Models (LLM) offers new horizons. With their
expansive knowledge and contextual prowess, LLMs have been transformative
across diverse applications. Building on this, we introduce LogParser-LLM, a
novel log parser integrated with LLM capabilities. This union seamlessly blends
semantic insights with statistical nuances, obviating the need for
hyper-parameter tuning and labeled training data, while ensuring rapid
adaptability through online parsing. Further deepening our exploration, we
address the intricate challenge of parsing granularity, proposing a new metric
and integrating human interactions to allow users to calibrate granularity to
their specific needs. Our method's efficacy is empirically demonstrated through
evaluations on the Loghub-2k and the large-scale LogPub benchmark. In
evaluations on the LogPub benchmark, involving an average of 3.6 million logs
per dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM
invocations on average, achieving a 90.6% F1 score for grouping accuracy and an
81.1% for parsing accuracy. These results demonstrate the method's high
efficiency and accuracy, outperforming current state-of-the-art log parsers,
including pattern-based, neural network-based, and existing LLM-enhanced
approaches.

摘要：<paragraph>記錄是無處不在的數位足跡，在系統診斷、安全性分析和效能最佳化中扮演不可或缺的角色。從記錄中萃取出可行的見解極度依賴記錄解析程序，它會將原始記錄轉換成結構化格式以進行下游分析。然而，當代系統的複雜性和記錄的動態特性對現有的自動解析技術構成重大挑戰。大型語言模型 (LLM) 的出現提供了新的視野。LLM 具備廣泛的知識和脈絡能力，已在各種應用中帶來變革。在此基礎上，我們引入了 LogParser-LLM，一種整合 LLM 功能的新穎記錄解析器。這種結合無縫地融合了語義見解和統計差異，消除了對超參數調整和標籤訓練資料的需求，同時透過線上解析確保快速適應性。進一步深入探討，我們解決了解析粒度的複雜挑戰，提出了一種新的指標並整合了人類互動，讓使用者可以根據特定需求校準粒度。我們的方法的功效透過在 Loghub-2k 和大型 LogPub 基準上的評估得到實證證明。在 LogPub 基準上的評估中，涉及 14 個資料集的每個資料集平均有 360 萬個記錄，我們的 LogParser-LLM 平均只需要 272.5 次 LLM 呼叫，在群組準確度上達到 90.6% 的 F1 分數，在解析準確度上達到 81.1%。這些結果證明了該方法的高效率和準確性，優於當前的最先進記錄解析器，包括基於模式、基於神經網路和現有的 LLM 增強方法。</paragraph>

##### **DHP Benchmark: Are LLMs Good NLG Evaluators?**
2408.13704v1 by Yicheng Wang, Jiayi Yuan, Yu-Neng Chuang, Zhuoer Wang, Yingchi Liu, Mark Cusick, Param Kulkarni, Zhengping Ji, Yasser Ibrahim, Xia Hu

Large Language Models (LLMs) are increasingly serving as evaluators in
Natural Language Generation (NLG) tasks. However, the capabilities of LLMs in
scoring NLG quality remain inadequately explored. Current studies depend on
human assessments and simple metrics that fail to capture the discernment of
LLMs across diverse NLG tasks. To address this gap, we propose the Discernment
of Hierarchical Perturbation (DHP) benchmarking framework, which provides
quantitative discernment scores for LLMs utilizing hierarchically perturbed
text data and statistical tests to measure the NLG evaluation capabilities of
LLMs systematically. We have re-established six evaluation datasets for this
benchmark, covering four NLG tasks: Summarization, Story Completion, Question
Answering, and Translation. Our comprehensive benchmarking of five major LLM
series provides critical insight into their strengths and limitations as NLG
evaluators.

摘要：大型語言模型 (LLM) 在自然語言生成 (NLG) 任務中越來越常擔任評估者的角色。然而，LLM 在評分 NLG 品質方面的能力仍未得到充分的探討。目前的研究所依賴於無法捕捉 LLM 在不同 NLG 任務中辨別能力的人類評估和簡單指標。為了解決這個差距，我們提出了分層擾動辨別 (DHP) 評量基準架構，該架構利用分層擾動文本資料和統計檢定，提供 LLM 的量化辨別分數，以系統化地衡量 LLM 的 NLG 評估能力。我們為此評量基準重新建立了六個評估資料集，涵蓋四項 NLG 任務：摘要、故事完成、問題解答和翻譯。我們對五個主要 LLM 系列進行的全面評量基準，提供了對其作為 NLG 評估者的優勢和限制的關鍵見解。

##### **Evaluating Alternative Training Interventions Using Personalized Computational Models of Learning**
2408.13684v1 by Christopher James MacLellan, Kimberly Stowers, Lisa Brady

Evaluating different training interventions to determine which produce the
best learning outcomes is one of the main challenges faced by instructional
designers. Typically, these designers use A/B experiments to evaluate each
intervention; however, it is costly and time consuming to run such studies. To
address this issue, we explore how computational models of learning might
support designers in reasoning causally about alternative interventions within
a fractions tutor. We present an approach for automatically tuning models to
specific individuals and show that personalized models make better predictions
of students' behavior than generic ones. Next, we conduct simulations to
generate counterfactual predictions of performance and learning for two
students (high and low performing) in different versions of the fractions
tutor. Our approach makes predictions that align with previous human findings,
as well as testable predictions that might be evaluated with future human
experiments.

摘要：評估不同的訓練介入措施以確定哪一種能產生最佳學習成果，是教學設計師面臨的主要挑戰之一。一般來說，這些設計師會使用 A/B 測試來評估每種介入措施；然而，執行此類研究的成本高昂且耗時。為了解決這個問題，我們探討如何利用運算學習模型來協助設計師在分數家教中對替代介入措施進行因果推理。我們提出了一種自動調整模型以適應特定個體的方法，並證明個人化模型比一般模型更能準確預測學生的行為。接著，我們進行模擬以產生兩個學生（高表現者和低表現者）在不同版本的分數家教中的反事實效能和學習預測。我們的做法做出的預測與先前的研究結果一致，而且可以透過未來的研究進行驗證。

##### **Submodular Maximization Approaches for Equitable Client Selection in Federated Learning**
2408.13683v1 by Andrés Catalino Castillo Jiménez, Ege C. Kaya, Lintao Ye, Abolfazl Hashemi

In a conventional Federated Learning framework, client selection for training
typically involves the random sampling of a subset of clients in each
iteration. However, this random selection often leads to disparate performance
among clients, raising concerns regarding fairness, particularly in
applications where equitable outcomes are crucial, such as in medical or
financial machine learning tasks. This disparity typically becomes more
pronounced with the advent of performance-centric client sampling techniques.
This paper introduces two novel methods, namely SUBTRUNC and UNIONFL, designed
to address the limitations of random client selection. Both approaches utilize
submodular function maximization to achieve more balanced models. By modifying
the facility location problem, they aim to mitigate the fairness concerns
associated with random selection. SUBTRUNC leverages client loss information to
diversify solutions, while UNIONFL relies on historical client selection data
to ensure a more equitable performance of the final model. Moreover, these
algorithms are accompanied by robust theoretical guarantees regarding
convergence under reasonable assumptions. The efficacy of these methods is
demonstrated through extensive evaluations across heterogeneous scenarios,
revealing significant improvements in fairness as measured by a client
dissimilarity metric.

摘要：在傳統的聯合學習架構中，訓練中的用戶端選擇通常涉及在每次迭代中隨機抽取用戶端子集。然而，這種隨機選擇通常會導致用戶端之間的表現差異，引發了關於公平性的擔憂，特別是在公正結果至關重要的應用中，例如在醫療或金融機器學習任務中。隨著以效能為中心的用戶端抽樣技術的出現，這種差異通常會變得更加明顯。本文介紹了兩種新方法，即 SUBTRUNC 和 UNIONFL，旨在解決隨機用戶端選擇的限制。這兩種方法都利用次模函數最大化來實現更平衡的模型。通過修改設施位置問題，它們旨在減輕與隨機選擇相關的公平性問題。SUBTRUNC 利用用戶端損失資訊來分散解決方案，而 UNIONFL 依賴於歷史用戶端選擇資料，以確保最終模型的效能更公平。此外，這些演算法還伴隨著穩健的理論保證，在合理的假設下保證收斂。這些方法的有效性通過在異質場景中的廣泛評估得到證明，揭示了以用戶端相異性指標衡量的公平性顯著改善。

##### **A layer-wise analysis of Mandarin and English suprasegmentals in SSL speech models**
2408.13678v1 by Antón de la Fuente, Dan Jurafsky

This study asks how self-supervised speech models represent suprasegmental
categories like Mandarin lexical tone, English lexical stress, and English
phrasal accents. Through a series of probing tasks, we make layer-wise
comparisons of English and Mandarin 12 layer monolingual models. Our findings
suggest that 1) English and Mandarin wav2vec 2.0 models learn contextual
representations of abstract suprasegmental categories which are strongest in
the middle third of the network. 2) Models are better at representing features
that exist in the language of their training data, and this difference is
driven by enriched context in transformer blocks, not local acoustic
representation. 3) Fine-tuned wav2vec 2.0 improves performance in later layers
compared to pre-trained models mainly for lexically contrastive features like
tone and stress, 4) HuBERT and WavLM learn similar representations to wav2vec
2.0, differing mainly in later layer performance. Our results extend previous
understanding of how models represent suprasegmentals and offer new insights
into the language-specificity and contextual nature of these representations.

摘要：本研究探討自監督語音模型如何呈現超音段範疇，例如漢語詞調、英語詞彙重音和英語片語重音。透過一系列探測任務，我們對英語和漢語 12 層單語模型進行逐層比較。我們的研究結果表明：1) 英語和漢語 wav2vec 2.0 模型學習抽象超音段範疇的語境表示，這些範疇在網路的中間三分之一處最強。2) 模型更擅長表示其訓練資料語言中存在的特徵，而這種差異是由Transformer區塊中的豐富語境推動的，而不是由局部音訊表示推動。3) 微調的 wav2vec 2.0 改善了後層的效能，與預先訓練的模型相比，主要是針對音調和重音等詞彙對比特徵。4) HuBERT 和 WavLM 學習到與 wav2vec 2.0 類似的表示，主要差異在於後層效能。我們的結果擴展了先前對模型如何表示超音段的了解，並對這些表示的語言特異性和語境性質提供了新的見解。

