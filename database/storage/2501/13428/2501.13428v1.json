{"2501.13428": {"publish_time": "2025-01-23", "title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models", "paper_summary": "Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by decomposing the Softmax operation into a non-linear\ntransformation and the $l_1$-norm. We identify the latter as essential for\nmaintaining model performance. By replacing the non-linear transformation with\nthe Softplus activation function and introducing a dynamic length scale factor\nfor different token lengths based on invariance entropy, we create a novel\nattention mechanism with performance better than conventional Softmax attention\nacross various inference lengths. To further improve the length extrapolation\nability of the proposed attention mechanism, we introduce a re-weighting\nmechanism that amplifies significant attention weights while diminishing weaker\nones, enabling the model to concentrate more effectively on relevant tokens.\nWhen combined with our proposed attention mechanism, this approach demonstrates\nsignificant promise in managing longer sequences, maintaining nearly constant\nvalidation loss even at 16$\\times$ the training token length while ensuring\nnumerical stability. Our code is available at:\nhttps://github.com/iminfine/freeatten.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5df2\u53d6\u5f97\u986f\u8457\u6210\u529f\uff0c\n\u9019\u4e3b\u8981\u662f\u56e0\u70ba\u5be6\u4f5c\u4e86\u81ea\u6211\u6ce8\u610f\u6a5f\u5236\u3002\u7136\u800c\uff0c\n\u50b3\u7d71\u7684 Softmax \u6ce8\u610f\u529b\u6703\u96a8\u8457\u63a8\u8ad6\u4ee3\u78bc\u9577\u5ea6\u7684\u589e\u52a0\u800c\u7522\u751f\u6578\u503c\u4e0d\u7a69\u5b9a\u6027\uff0c\u4e26\u964d\u4f4e\u6548\u80fd\u3002\u9019\u7bc7\u8ad6\u6587\u900f\u904e\u5c07 Softmax \u904b\u7b97\u5206\u89e3\u6210\u975e\u7dda\u6027\u8f49\u63db\u548c $l_1$-\u7bc4\u6578\u4f86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u3002\u6211\u5011\u5c07\u5f8c\u8005\u8996\u70ba\u7dad\u6301\u6a21\u578b\u6548\u80fd\u7684\u5fc5\u8981\u689d\u4ef6\u3002\u900f\u904e\u5c07\u975e\u7dda\u6027\u8f49\u63db\u66ff\u63db\u6210 Softplus \u555f\u52d5\u51fd\u6578\uff0c\u4e26\u6839\u64da\u4e0d\u8b8a\u6027\u71b5\u70ba\u4e0d\u540c\u7684\u4ee3\u78bc\u9577\u5ea6\u5c0e\u5165\u52d5\u614b\u9577\u5ea6\u6bd4\u4f8b\u56e0\u5b50\uff0c\u6211\u5011\u5275\u9020\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u5176\u6548\u80fd\u512a\u65bc\u50b3\u7d71\u7684 Softmax \u6ce8\u610f\u529b\uff0c\u9069\u7528\u65bc\u5404\u7a2e\u63a8\u8ad6\u9577\u5ea6\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u63d0\u5347\u6240\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u9577\u5ea6\u5916\u63a8\u80fd\u529b\uff0c\u6211\u5011\u5c0e\u5165\u4e00\u500b\u91cd\u65b0\u52a0\u6b0a\u6a5f\u5236\uff0c\u4ee5\u653e\u5927\u91cd\u8981\u7684\u6ce8\u610f\u529b\u6b0a\u91cd\uff0c\u540c\u6642\u7e2e\u5c0f\u8f03\u5f31\u7684\u6b0a\u91cd\uff0c\u8b93\u6a21\u578b\u80fd\u66f4\u6709\u6548\u5730\u96c6\u4e2d\u65bc\u76f8\u95dc\u4ee3\u78bc\u3002\u7576\u8207\u6211\u5011\u6240\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u7d50\u5408\u6642\uff0c\u6b64\u65b9\u6cd5\u5728\u7ba1\u7406\u8f03\u9577\u5e8f\u5217\u65b9\u9762\u5c55\u73fe\u986f\u8457\u7684\u6f5b\u529b\uff0c\u5373\u4f7f\u5728\u8a13\u7df4\u4ee3\u78bc\u9577\u5ea6\u7684 16 \u500d\u4e0b\uff0c\u4e5f\u80fd\u7dad\u6301\u8fd1\u4e4e\u6046\u5b9a\u7684\u9a57\u8b49\u640d\u5931\uff0c\u540c\u6642\u78ba\u4fdd\u6578\u503c\u7a69\u5b9a\u6027\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1a\nhttps://github.com/iminfine/freeatten\u3002", "author": "Bo Gao et.al.", "authors": "Bo Gao, Michael W. Spratling", "id": "2501.13428v1", "paper_url": "http://arxiv.org/abs/2501.13428v1", "repo": "null"}}