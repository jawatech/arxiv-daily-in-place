{"2409.11353": {"publish_time": "2024-09-17", "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models", "paper_summary": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks.", "paper_summary_zh": "\u5e7b\u89ba\uff0c\u5373\u7522\u751f\u4e8b\u5be6\u4e0a\u4e0d\u6b63\u78ba\u7684\u5167\u5bb9\uff0c\u662f\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u65e5\u76ca\u56b4\u5cfb\u7684\u6311\u6230\u3002\u73fe\u6709\u7684\u5075\u6e2c\u548c\u7de9\u89e3\u65b9\u6cd5\u5f80\u5f80\u662f\u5b64\u7acb\u7684\uff0c\u4e14\u4e0d\u8db3\u4ee5\u6eff\u8db3\u7279\u5b9a\u9818\u57df\u7684\u9700\u6c42\uff0c\u7f3a\u4e4f\u6a19\u6e96\u5316\u7684\u7ba1\u9053\u3002\u672c\u6587\u4ecb\u7d39 THaMES\uff08\u5e7b\u89ba\u7de9\u89e3\u548c\u8a55\u4f30\u5de5\u5177\uff09\uff0c\u4e00\u500b\u6574\u5408\u6846\u67b6\u548c\u51fd\u5f0f\u5eab\uff0c\u7528\u65bc\u89e3\u6c7a\u6b64\u5dee\u8ddd\u3002THaMES \u63d0\u4f9b\u4e86\u4e00\u500b\u7aef\u5230\u7aef\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u8a55\u4f30\u548c\u7de9\u89e3 LLM \u4e2d\u7684\u5e7b\u89ba\uff0c\u5177\u5099\u81ea\u52d5\u5316\u6e2c\u8a66\u96c6\u7522\u751f\u3001\u591a\u65b9\u9762\u7684\u57fa\u6e96\u6e2c\u8a66\u548c\u53ef\u9069\u61c9\u7684\u7de9\u89e3\u7b56\u7565\u3002\u5b83\u81ea\u52d5\u5316\u4e86\u5f9e\u4efb\u4f55\u8a9e\u6599\u5eab\u5efa\u7acb\u6e2c\u8a66\u96c6\u7684\u904e\u7a0b\uff0c\u900f\u904e\u6279\u6b21\u8655\u7406\u3001\u52a0\u6b0a\u62bd\u6a23\u548c\u53cd\u4e8b\u5be6\u9a57\u8b49\u7b49\u6280\u8853\uff0c\u78ba\u4fdd\u8cc7\u6599\u7684\u9ad8\u54c1\u8cea\u3001\u591a\u6a23\u6027\u548c\u6210\u672c\u6548\u76ca\u3002THaMES \u8a55\u4f30\u6a21\u578b\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5075\u6e2c\u548c\u6e1b\u5c11\u5e7b\u89ba\u7684\u80fd\u529b\uff0c\u5305\u62ec\u6587\u5b57\u7522\u751f\u548c\u4e8c\u5143\u5206\u985e\uff0c\u61c9\u7528\u6700\u4f73\u7684\u7de9\u89e3\u7b56\u7565\uff0c\u4f8b\u5982\u60c5\u5883\u5b78\u7fd2 (ICL)\u3001\u6aa2\u7d22\u589e\u5f37\u7522\u751f (RAG) \u548c\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT)\u3002\u4f7f\u7528\u5b78\u8853\u8ad6\u6587\u3001\u653f\u6cbb\u65b0\u805e\u548c\u7dad\u57fa\u767e\u79d1\u77e5\u8b58\u5eab\u8a55\u4f30\u6700\u5148\u9032\u7684 LLM\uff0c\u767c\u73fe\u50cf GPT-4o \u7b49\u5546\u696d\u6a21\u578b\u6bd4 ICL \u5f9e RAG \u4e2d\u53d7\u76ca\u66f4\u591a\uff0c\u800c\u50cf Llama-3.1-8B-Instruct \u548c Mistral-Nemo \u7b49\u958b\u653e\u6b0a\u91cd\u6a21\u578b\u5f9e ICL \u4e2d\u53d7\u76ca\u66f4\u591a\u3002\u6b64\u5916\uff0cPEFT \u5728\u5169\u500b\u8a55\u4f30\u4efb\u52d9\u4e2d\u90fd\u986f\u8457\u63d0\u5347\u4e86 Llama-3.1-8B-Instruct \u7684\u6548\u80fd\u3002", "author": "Mengfei Liang et.al.", "authors": "Mengfei Liang, Archish Arun, Zekun Wu, Cristian Munoz, Jonathan Lutch, Emre Kazim, Adriano Koshiyama, Philip Treleaven", "id": "2409.11353v1", "paper_url": "http://arxiv.org/abs/2409.11353v1", "repo": "null"}}