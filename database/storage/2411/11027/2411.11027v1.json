{"2411.11027": {"publish_time": "2024-11-17", "title": "BianCang: A Traditional Chinese Medicine Large Language Model", "paper_summary": "The rise of large language models (LLMs) has driven significant progress in\nmedical applications, including traditional Chinese medicine (TCM). However,\ncurrent medical LLMs struggle with TCM diagnosis and syndrome differentiation\ndue to substantial differences between TCM and modern medical theory, and the\nscarcity of specialized, high-quality corpora. This paper addresses these\nchallenges by proposing BianCang, a TCM-specific LLM, using a two-stage\ntraining process that first injects domain-specific knowledge and then aligns\nit through targeted stimulation. To enhance diagnostic and differentiation\ncapabilities, we constructed pre-training corpora, instruction-aligned datasets\nbased on real hospital records, and the ChP-TCM dataset derived from the\nPharmacopoeia of the People's Republic of China. We compiled extensive TCM and\nmedical corpora for continuous pre-training and supervised fine-tuning,\nbuilding a comprehensive dataset to refine the model's understanding of TCM.\nEvaluations across 11 test sets involving 29 models and 4 tasks demonstrate the\neffectiveness of BianCang, offering valuable insights for future research.\nCode, datasets, and models are available at\nhttps://github.com/QLU-NLP/BianCang.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8208\u8d77\u63a8\u52d5\u4e86\u91ab\u7642\u61c9\u7528\u9818\u57df\u7684\u91cd\u5927\u9032\u5c55\uff0c\u5305\u62ec\u4e2d\u91ab\u5b78 (TCM)\u3002\u7136\u800c\uff0c\u7531\u65bc\u4e2d\u91ab\u5b78\u8207\u73fe\u4ee3\u91ab\u5b78\u7406\u8ad6\u4e4b\u9593\u5b58\u5728\u8457\u5be6\u8cea\u6027\u7684\u5dee\u7570\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u5c08\u696d\u3001\u9ad8\u54c1\u8cea\u7684\u8a9e\u6599\u5eab\uff0c\u7576\u524d\u7684\u91ab\u5b78 LLM \u5728\u4e2d\u91ab\u8a3a\u65b7\u548c\u8b49\u5019\u9451\u5225\u65b9\u9762\u9047\u5230\u4e86\u56f0\u96e3\u3002\u672c\u6587\u901a\u904e\u63d0\u51fa BianCang\uff0c\u4e00\u7a2e\u7279\u5b9a\u65bc\u4e2d\u91ab\u5b78\u7684 LLM\uff0c\u4f86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u4f7f\u7528\u4e00\u500b\u5169\u968e\u6bb5\u8a13\u7df4\u904e\u7a0b\uff0c\u9996\u5148\u6ce8\u5165\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\uff0c\u7136\u5f8c\u901a\u904e\u6709\u91dd\u5c0d\u6027\u7684\u523a\u6fc0\u4f86\u5c0d\u9f4a\u5b83\u3002\u70ba\u4e86\u589e\u5f37\u8a3a\u65b7\u548c\u9451\u5225\u80fd\u529b\uff0c\u6211\u5011\u69cb\u5efa\u4e86\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u3001\u57fa\u65bc\u771f\u5be6\u91ab\u9662\u8a18\u9304\u7684\u6307\u4ee4\u5c0d\u9f4a\u6578\u64da\u96c6\uff0c\u4ee5\u53ca\u6e90\u81ea\u4e2d\u83ef\u4eba\u6c11\u5171\u548c\u570b\u85e5\u5178\u7684 ChP-TCM \u6578\u64da\u96c6\u3002\u6211\u5011\u7de8\u8b6f\u4e86\u5927\u91cf\u7684 TCM \u548c\u91ab\u5b78\u8a9e\u6599\u5eab\uff0c\u7528\u65bc\u6301\u7e8c\u7684\u9810\u8a13\u7df4\u548c\u76e3\u7763\u5fae\u8abf\uff0c\u69cb\u5efa\u4e86\u4e00\u500b\u5168\u9762\u7684\u6578\u64da\u96c6\u4f86\u5b8c\u5584\u6a21\u578b\u5c0d TCM \u7684\u7406\u89e3\u3002\u6d89\u53ca 29 \u500b\u6a21\u578b\u548c 4 \u9805\u4efb\u52d9\u7684 11 \u500b\u6e2c\u8a66\u96c6\u7684\u8a55\u4f30\u8b49\u660e\u4e86 BianCang \u7684\u6709\u6548\u6027\uff0c\u70ba\u672a\u4f86\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\u3002\u7a0b\u5f0f\u78bc\u3001\u6578\u64da\u96c6\u548c\u6a21\u578b\u53ef\u5728 https://github.com/QLU-NLP/BianCang \u7372\u5f97\u3002", "author": "Sibo Wei et.al.", "authors": "Sibo Wei, Xueping Peng, Yi-fei Wang, Jiasheng Si, Weiyu Zhang, Wenpeng Lu, Xiaoming Wu, Yinglong Wang", "id": "2411.11027v1", "paper_url": "http://arxiv.org/abs/2411.11027v1", "repo": "https://github.com/qlu-nlp/biancang"}}