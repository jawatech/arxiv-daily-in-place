{"2410.16239": {"publish_time": "2024-10-21", "title": "MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays, ECGs, and Diagnostic Report", "paper_summary": "In this paper, we introduce a novel Multi-Modal Contrastive Pre-training\nFramework that synergistically combines X-rays, electrocardiograms (ECGs), and\nradiology/cardiology reports. Our approach leverages transformers to encode\nthese diverse modalities into a unified representation space, aiming to enhance\ndiagnostic accuracy and facilitate comprehensive patient assessments. We\nutilize LoRA-Peft to significantly reduce trainable parameters in the LLM and\nincorporate recent linear attention dropping strategy in the Vision\nTransformer(ViT) for smoother attention. Furthermore, we provide novel\nmultimodal attention explanations and retrieval for our model. To the best of\nour knowledge, we are the first to propose an integrated model that combines\nX-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing\ncontrastive loss, MoRE effectively aligns modality-specific features into a\ncoherent embedding, which supports various downstream tasks such as zero-shot\nclassification and multimodal retrieval. Employing our proposed methodology, we\nachieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and\nPtbXl downstream datasets, surpassing existing multimodal approaches. Our\nproposed framework shows significant improvements in capturing intricate\ninter-modal relationships and its robustness in medical diagnosis that\nestablishes a framework for future research in multimodal learning in the\nhealthcare sector.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u591a\u6a21\u614b\u5c0d\u6bd4\u9810\u8a13\u7df4\u6846\u67b6\uff0c\u8a72\u6846\u67b6\u5354\u540c\u7d50\u5408\u4e86 X \u5c04\u7dda\u3001\u5fc3\u96fb\u5716 (ECG) \u548c\u653e\u5c04\u5b78/\u5fc3\u81df\u75c5\u5b78\u5831\u544a\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528Transformer\u5c07\u9019\u4e9b\u4e0d\u540c\u7684\u6a21\u614b\u7de8\u78bc\u6210\u4e00\u500b\u7d71\u4e00\u7684\u8868\u793a\u7a7a\u9593\uff0c\u65e8\u5728\u63d0\u9ad8\u8a3a\u65b7\u6e96\u78ba\u6027\u4e26\u4fc3\u9032\u5168\u9762\u7684\u60a3\u8005\u8a55\u4f30\u3002\u6211\u5011\u5229\u7528 LoRA-Peft \u4f86\u986f\u8457\u6e1b\u5c11 LLM \u4e2d\u53ef\u8a13\u7df4\u7684\u53c3\u6578\uff0c\u4e26\u5728 Vision Transformer (ViT) \u4e2d\u7d0d\u5165\u6700\u8fd1\u7684\u7dda\u6027\u6ce8\u610f\u529b\u4e1f\u68c4\u7b56\u7565\u4ee5\u7372\u5f97\u66f4\u6d41\u66a2\u7684\u6ce8\u610f\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u70ba\u6211\u5011\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7a4e\u7684\u591a\u6a21\u614b\u6ce8\u610f\u529b\u89e3\u91cb\u548c\u6aa2\u7d22\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u6211\u5011\u662f\u7b2c\u4e00\u500b\u63d0\u51fa\u7d50\u5408 X \u5c04\u7dda\u3001ECG \u548c\u653e\u5c04\u5b78/\u5fc3\u81df\u75c5\u5b78\u5831\u544a\u7684\u6574\u5408\u6a21\u578b\u7684\u4eba\u3002\u901a\u904e\u5229\u7528\u5c0d\u6bd4\u640d\u5931\uff0cMoRE \u6709\u6548\u5730\u5c07\u7279\u5b9a\u65bc\u6a21\u614b\u7684\u7279\u5fb5\u5c0d\u9f4a\u5230\u4e00\u500b\u9023\u8cab\u7684\u5d4c\u5165\u4e2d\uff0c\u9019\u652f\u6301\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\uff0c\u4f8b\u5982\u96f6\u6b21\u5206\u985e\u548c\u591a\u6a21\u614b\u6aa2\u7d22\u3002\u63a1\u7528\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u6211\u5011\u5728 Mimic-IV\u3001CheXpert\u3001Edema Severity \u548c PtbXl \u4e0b\u6e38\u6578\u64da\u96c6\u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032 (SOTA)\uff0c\u8d85\u8d8a\u4e86\u73fe\u6709\u7684\u591a\u6a21\u614b\u65b9\u6cd5\u3002\u6211\u5011\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6355\u6349\u8907\u96dc\u7684\u6a21\u9593\u95dc\u4fc2\u548c\u5176\u5728\u91ab\u7642\u8a3a\u65b7\u4e2d\u7684\u9b6f\u68d2\u6027\u65b9\u9762\u986f\u793a\u51fa\u986f\u8457\u7684\u6539\u9032\uff0c\u9019\u70ba\u91ab\u7642\u4fdd\u5065\u90e8\u9580\u7684\u591a\u6a21\u614b\u5b78\u7fd2\u7684\u672a\u4f86\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u500b\u6846\u67b6\u3002", "author": "Samrajya Thapa et.al.", "authors": "Samrajya Thapa, Koushik Howlader, Subhankar Bhattacharjee, Wei le", "id": "2410.16239v2", "paper_url": "http://arxiv.org/abs/2410.16239v2", "repo": "https://github.com/svthapa/more"}}