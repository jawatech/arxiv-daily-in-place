{"2409.06067": {"publish_time": "2024-09-09", "title": "MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data", "paper_summary": "Previous studies on federated learning (FL) often encounter performance\ndegradation due to data heterogeneity among different clients. In light of the\nrecent advances in multimodal large language models (MLLMs), such as GPT-4v and\nLLaVA, which demonstrate their exceptional proficiency in multimodal tasks,\nsuch as image captioning and multimodal question answering. We introduce a\nnovel federated learning framework, named Multimodal Large Language Model\nAssisted Federated Learning (MLLM-FL), which which employs powerful MLLMs at\nthe server end to address the heterogeneous and long-tailed challenges. Owing\nto the advanced cross-modality representation capabilities and the extensive\nopen-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing\nthe extensive, yet previously underexploited, open-source data accessible from\nwebsites and powerful server-side computational resources. Hence, the MLLM-FL\nnot only enhances the performance but also avoids increasing the risk of\nprivacy leakage and the computational burden on local devices, distinguishing\nit from prior methodologies. Our framework has three key stages. Initially,\nprior to local training on local datasets of clients, we conduct global\nvisual-text pretraining of the model. This pretraining is facilitated by\nutilizing the extensive open-source data available online, with the assistance\nof multimodal large language models. Subsequently, the pretrained model is\ndistributed among various clients for local training. Finally, once the locally\ntrained models are transmitted back to the server, a global alignment is\ncarried out under the supervision of MLLMs to further enhance the performance.\nExperimental evaluations on established benchmarks, show that our framework\ndelivers promising performance in the typical scenarios with data heterogeneity\nand long-tail distribution across different clients in FL.", "paper_summary_zh": "\u5148\u524d\u7684\u806f\u5408\u5b78\u7fd2 (FL) \u7814\u7a76\u901a\u5e38\u6703\u9047\u5230\u6548\u80fd\u964d\u4f4e\u7684\u554f\u984c\uff0c\u539f\u56e0\u5728\u65bc\u4e0d\u540c\u7528\u6236\u7aef\u4e4b\u9593\u7684\u8cc7\u6599\u7570\u8cea\u6027\u3002\u6709\u9451\u65bc\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM)\uff08\u4f8b\u5982 GPT-4v \u548c LLaVA\uff09\u7684\u6700\u65b0\u9032\u5c55\uff0c\u9019\u4e9b\u6a21\u578b\u5728\u591a\u6a21\u614b\u4efb\u52d9\uff08\u4f8b\u5982\u5f71\u50cf\u6a19\u984c\u548c\u591a\u6a21\u614b\u554f\u7b54\uff09\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u719f\u7df4\u5ea6\u3002\u6211\u5011\u5f15\u9032\u4e00\u500b\u540d\u70ba\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u8f14\u52a9\u806f\u5408\u5b78\u7fd2 (MLLM-FL) \u7684\u5275\u65b0\u806f\u5408\u5b78\u7fd2\u67b6\u69cb\uff0c\u5b83\u5728\u4f3a\u670d\u5668\u7aef\u63a1\u7528\u5f37\u5927\u7684 MLLM \u4f86\u61c9\u5c0d\u7570\u8cea\u6027\u548c\u9577\u5c3e\u6311\u6230\u3002\u7531\u65bc MLLM \u5177\u5099\u5148\u9032\u7684\u8de8\u6a21\u614b\u8868\u5fb5\u80fd\u529b\u548c\u5ee3\u6cdb\u7684\u958b\u653e\u5f0f\u8a5e\u5f59\u5148\u5099\u77e5\u8b58\uff0c\u6211\u5011\u7684\u67b6\u69cb\u64c5\u9577\u5229\u7528\u5f9e\u7db2\u7ad9\u548c\u5f37\u5927\u7684\u4f3a\u670d\u5668\u7aef\u904b\u7b97\u8cc7\u6e90\u53d6\u5f97\u7684\u5ee3\u6cdb\u4f46\u5148\u524d\u672a\u5145\u5206\u5229\u7528\u7684\u958b\u653e\u539f\u59cb\u78bc\u8cc7\u6599\u3002\u56e0\u6b64\uff0cMLLM-FL \u4e0d\u50c5\u80fd\u63d0\u5347\u6548\u80fd\uff0c\u9084\u80fd\u907f\u514d\u589e\u52a0\u96b1\u79c1\u5916\u6d29\u7684\u98a8\u96aa\u548c\u672c\u5730\u88dd\u7f6e\u7684\u904b\u7b97\u8ca0\u64d4\uff0c\u9019\u8b93\u5b83\u6709\u5225\u65bc\u5148\u524d\u7684\u7814\u7a76\u65b9\u6cd5\u3002\u6211\u5011\u7684\u67b6\u69cb\u6709\u4e09\u500b\u95dc\u9375\u968e\u6bb5\u3002\u6700\u521d\uff0c\u5728\u7528\u6236\u7aef\u7684\u672c\u5730\u8cc7\u6599\u96c6\u9032\u884c\u672c\u5730\u8a13\u7df4\u4e4b\u524d\uff0c\u6211\u5011\u6703\u57f7\u884c\u6a21\u578b\u7684\u5168\u7403\u8996\u89ba\u6587\u5b57\u9810\u8a13\u7df4\u3002\u9019\u500b\u9810\u8a13\u7df4\u6703\u5229\u7528\u7dda\u4e0a\u53d6\u5f97\u7684\u5ee3\u6cdb\u958b\u653e\u539f\u59cb\u78bc\u8cc7\u6599\uff0c\u4e26\u5728\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u5354\u52a9\u4e0b\u9032\u884c\u3002\u96a8\u5f8c\uff0c\u9810\u8a13\u7df4\u6a21\u578b\u6703\u5206\u767c\u7d66\u5404\u500b\u7528\u6236\u7aef\u9032\u884c\u672c\u5730\u8a13\u7df4\u3002\u6700\u5f8c\uff0c\u7576\u672c\u5730\u8a13\u7df4\u7684\u6a21\u578b\u50b3\u56de\u4f3a\u670d\u5668\u5f8c\uff0c\u6703\u5728 MLLM \u7684\u76e3\u7763\u4e0b\u57f7\u884c\u5168\u7403\u6bd4\u5c0d\uff0c\u4ee5\u9032\u4e00\u6b65\u63d0\u5347\u6548\u80fd\u3002\u5728\u65e2\u5b9a\u7684\u57fa\u6e96\u4e0a\u7684\u5be6\u9a57\u8a55\u4f30\u986f\u793a\uff0c\u6211\u5011\u7684\u67b6\u69cb\u5728 FL \u4e2d\u4e0d\u540c\u7528\u6236\u7aef\u5177\u6709\u8cc7\u6599\u7570\u8cea\u6027\u548c\u9577\u5c3e\u5206\u4f48\u7684\u5178\u578b\u5834\u666f\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4ee4\u4eba\u6eff\u610f\u7684\u6548\u80fd\u3002", "author": "Jianyi Zhang et.al.", "authors": "Jianyi Zhang, Hao Frank Yang, Ang Li, Xin Guo, Pu Wang, Haiming Wang, Yiran Chen, Hai Li", "id": "2409.06067v1", "paper_url": "http://arxiv.org/abs/2409.06067v1", "repo": "null"}}