
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-06**|**The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**|Gregory Szumel et.al.|[2409.04368v1](http://arxiv.org/abs/2409.04368v1)|null|
|**2024-09-06**|**CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**|William Knottenbelt et.al.|[2409.04290v1](http://arxiv.org/abs/2409.04290v1)|[link](https://github.com/knottwill/CoxKAN)|
|**2024-09-06**|**Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**|Daniel J. Tan et.al.|[2409.04224v1](http://arxiv.org/abs/2409.04224v1)|null|
|**2024-09-06**|**FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**|Kai Shu et.al.|[2409.03947v1](http://arxiv.org/abs/2409.03947v1)|null|
|**2024-09-05**|**A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**|Esther Lagemann et.al.|[2409.03933v1](http://arxiv.org/abs/2409.03933v1)|null|
|**2024-09-05**|**Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**|Yucong Zhang et.al.|[2409.03597v1](http://arxiv.org/abs/2409.03597v1)|null|
|**2024-09-05**|**Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**|Prerak Mody et.al.|[2409.03470v1](http://arxiv.org/abs/2409.03470v1)|[link](https://github.com/prerakmody/bayesuncertainty-error-correspondence)|
|**2024-09-05**|**Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**|Francisco de Arriba-PÃ©rez et.al.|[2409.03375v1](http://arxiv.org/abs/2409.03375v1)|null|
|**2024-09-05**|**Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**|Juan A. Berrios Moya et.al.|[2409.03147v1](http://arxiv.org/abs/2409.03147v1)|null|
|**2024-09-04**|**MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**|Shehan Perera et.al.|[2409.03062v1](http://arxiv.org/abs/2409.03062v1)|[link](https://github.com/osupcvlab/mobileunetr)|
|**2024-09-04**|**Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**|Junyoung Park et.al.|[2409.02883v1](http://arxiv.org/abs/2409.02883v1)|null|
|**2024-09-04**|**Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**|Ramon Tavares et.al.|[2409.02681v1](http://arxiv.org/abs/2409.02681v1)|null|
|**2024-09-04**|**SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**|Wenwu Guo et.al.|[2409.02598v1](http://arxiv.org/abs/2409.02598v1)|[link](https://github.com/wenwucode/surgtrack)|
|**2024-09-04**|**Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**|Chih-Yuan Li et.al.|[2409.02530v1](http://arxiv.org/abs/2409.02530v1)|null|
|**2024-09-03**|**Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**|Deepak Raina et.al.|[2409.02337v1](http://arxiv.org/abs/2409.02337v1)|null|
|**2024-09-03**|**Action-Based ADHD Diagnosis in Video**|Yichun Li et.al.|[2409.02261v1](http://arxiv.org/abs/2409.02261v1)|null|
|**2024-09-03**|**A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**|Anna L. Trella et.al.|[2409.02069v1](http://arxiv.org/abs/2409.02069v1)|[link](https://github.com/StatisticalReinforcementLearningLab/oralytics-post-deployment-analysis)|
|**2024-09-03**|**TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**|Bobby Azad et.al.|[2409.02018v1](http://arxiv.org/abs/2409.02018v1)|null|
|**2024-09-03**|**A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**|Abdelmalek Mouazer et.al.|[2409.01903v1](http://arxiv.org/abs/2409.01903v1)|null|
|**2024-09-03**|**Training on the Benchmark Is Not All You Need**|Shiwen Ni et.al.|[2409.01790v1](http://arxiv.org/abs/2409.01790v1)|[link](https://github.com/nishiwen1214/Benchmark-leakage-detection)|
|**2024-09-03**|**Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**|Wenyang Hu et.al.|[2409.01676v1](http://arxiv.org/abs/2409.01676v1)|null|
|**2024-09-03**|**A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**|Zekang Yang et.al.|[2409.02145v1](http://arxiv.org/abs/2409.02145v1)|[link](https://github.com/yang-ze-kang/MOC)|
|**2024-09-03**|**A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**|Ruben D. Fonnegra et.al.|[2409.01596v1](http://arxiv.org/abs/2409.01596v1)|null|
|**2024-09-02**|**Kvasir-VQA: A Text-Image Pair GI Tract Dataset**|Sushant Gautam et.al.|[2409.01437v1](http://arxiv.org/abs/2409.01437v1)|[link](https://github.com/simula/Kvasir-VQA)|
|**2024-09-02**|**SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**|Mevan Ekanayake et.al.|[2409.01013v1](http://arxiv.org/abs/2409.01013v1)|null|
|**2024-09-01**|**Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**|Sajib Acharjee Dip et.al.|[2409.00873v1](http://arxiv.org/abs/2409.00873v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**|Mahsa Khosravi et.al.|[2409.00735v1](http://arxiv.org/abs/2409.00735v1)|null|
|**2024-09-01**|**LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**|Zhaojie Fang et.al.|[2409.00726v1](http://arxiv.org/abs/2409.00726v1)|[link](https://github.com/Tinysqua/LPUWF-LDM)|
|**2024-09-01**|**BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**|Shams Nafisa Ali et.al.|[2409.00724v1](http://arxiv.org/abs/2409.00724v1)|[link](https://github.com/sani002/HS-Dataset)|
|**2024-09-01**|**Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**|Pragya Gupta et.al.|[2409.00718v1](http://arxiv.org/abs/2409.00718v1)|null|
|**2024-09-01**|**Curriculum Prompting Foundation Models for Medical Image Segmentation**|Xiuqi Zheng et.al.|[2409.00695v1](http://arxiv.org/abs/2409.00695v1)|[link](https://github.com/annazzz-zxq/curriculum-prompting)|
|**2024-08-31**|**Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**|Jacqueline Lammert et.al.|[2409.00544v1](http://arxiv.org/abs/2409.00544v1)|[link](https://github.com/LammertJ/RGT-Digital-Twin)|
|**2024-08-31**|**Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**|Georgios Ioannides et.al.|[2409.00391v1](http://arxiv.org/abs/2409.00391v1)|null|
|**2024-08-31**|**Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**|Mikhail Borisenkov et.al.|[2409.00310v1](http://arxiv.org/abs/2409.00310v1)|null|
|**2024-08-30**|**Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**|Antonio Rago et.al.|[2408.17401v1](http://arxiv.org/abs/2408.17401v1)|null|
|**2024-08-30**|**Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**|Yuhan Zheng et.al.|[2409.00163v1](http://arxiv.org/abs/2409.00163v1)|null|
|**2024-08-30**|**NDP: Next Distribution Prediction as a More Broad Target**|Junhao Ruan et.al.|[2408.17377v1](http://arxiv.org/abs/2408.17377v1)|null|
|**2024-08-30**|**Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**|Jutika Borah et.al.|[2408.17011v2](http://arxiv.org/abs/2408.17011v2)|null|
|**2024-08-29**|**A Survey for Large Language Models in Biomedicine**|Chong Wang et.al.|[2409.00133v1](http://arxiv.org/abs/2409.00133v1)|null|
|**2024-08-29**|**Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**|Yifei Chen et.al.|[2408.16343v1](http://arxiv.org/abs/2408.16343v1)|[link](https://github.com/justlfc03/mstnet)|
|**2024-08-29**|**Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**|Xia Jiang et.al.|[2408.16256v1](http://arxiv.org/abs/2408.16256v1)|null|
|**2024-08-29**|**M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**|Jonggwon Park et.al.|[2408.16213v1](http://arxiv.org/abs/2408.16213v1)|null|
|**2024-08-28**|**A Survey on Evaluation of Multimodal Large Language Models**|Jiaxing Huang et.al.|[2408.15769v1](http://arxiv.org/abs/2408.15769v1)|null|
|**2024-08-28**|**Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**|Yijun Zhou et.al.|[2408.15498v1](http://arxiv.org/abs/2408.15498v1)|null|
|**2024-08-27**|**What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**|Jana Schaich Borg et.al.|[2408.15354v1](http://arxiv.org/abs/2408.15354v1)|null|
|**2024-08-27**|**Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**|Weiyi Zhang et.al.|[2408.15217v1](http://arxiv.org/abs/2408.15217v1)|null|
|**2024-08-27**|**Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**|Daniil Filienko et.al.|[2409.00112v1](http://arxiv.org/abs/2409.00112v1)|null|
|**2024-08-27**|**Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**|Francesco Sovrano et.al.|[2408.15121v1](http://arxiv.org/abs/2408.15121v1)|null|
|**2024-08-27**|**MiWaves Reinforcement Learning Algorithm**|Susobhan Ghosh et.al.|[2408.15076v1](http://arxiv.org/abs/2408.15076v1)|[link](https://github.com/statisticalreinforcementlearninglab/miwaves_rl_service)|
|**2024-08-27**|**Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**|Yuqi Zhang et.al.|[2408.15032v1](http://arxiv.org/abs/2408.15032v1)|[link](https://github.com/yuqizhang-buaa/mamba2mil)|
|**2024-08-27**|**Sequence-aware Pre-training for Echocardiography Probe Guidance**|Haojun Jiang et.al.|[2408.15026v1](http://arxiv.org/abs/2408.15026v1)|null|
|**2024-08-27**|**Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**|Christos Theodoropoulos et.al.|[2408.15294v2](http://arxiv.org/abs/2408.15294v2)|null|
|**2024-08-27**|**Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**|Qiaoxin Li et.al.|[2408.14754v1](http://arxiv.org/abs/2408.14754v1)|null|
|**2024-08-27**|**Large Language Models for Disease Diagnosis: A Scoping Review**|Shuang Zhou et.al.|[2409.00097v1](http://arxiv.org/abs/2409.00097v1)|null|
|**2024-08-26**|**Improving Clinical Note Generation from Complex Doctor-Patient Conversation**|Yizhan Li et.al.|[2408.14568v1](http://arxiv.org/abs/2408.14568v1)|null|
|**2024-08-26**|**Temporal Ensemble Logic**|Guo-Qiang Zhang et.al.|[2408.14443v2](http://arxiv.org/abs/2408.14443v2)|null|
|**2024-08-26**|**MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**|Kuluhan Binici et.al.|[2408.14418v2](http://arxiv.org/abs/2408.14418v2)|null|
|**2024-08-26**|**Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**|Xiaoman Zhang et.al.|[2408.14397v1](http://arxiv.org/abs/2408.14397v1)|[link](https://github.com/rajpurkarlab/rexkg)|
|**2024-08-26**|**Foundation Models for Music: A Survey**|Yinghao Ma et.al.|[2408.14340v3](http://arxiv.org/abs/2408.14340v3)|[link](https://github.com/nicolaus625/fm4music)|
|**2024-08-26**|**Uncertainties of Latent Representations in Computer Vision**|Michael Kirchhof et.al.|[2408.14281v1](http://arxiv.org/abs/2408.14281v1)|null|
|**2024-08-26**|**Automatic Medical Report Generation: Methods and Applications**|Li Guo et.al.|[2408.13988v1](http://arxiv.org/abs/2408.13988v1)|null|
|**2024-08-25**|**Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**|Seyed Amir Ahmad Safavi-Naini et.al.|[2409.00084v2](http://arxiv.org/abs/2409.00084v2)|[link](https://github.com/sdamirsa/llm-vlm-in-gastroenterology)|
|**2024-08-25**|**PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**|Zifan Chen et.al.|[2408.13836v1](http://arxiv.org/abs/2408.13836v1)|null|
|**2024-08-24**|**Submodular Maximization Approaches for Equitable Client Selection in Federated Learning**|AndrÃ©s Catalino Castillo JimÃ©nez et.al.|[2408.13683v2](http://arxiv.org/abs/2408.13683v2)|null|
|**2024-08-24**|**Towards Case-based Interpretability for Medical Federated Learning**|Laura Latorre et.al.|[2408.13626v1](http://arxiv.org/abs/2408.13626v1)|null|
|**2024-08-23**|**HBIC: A Biclustering Algorithm for Heterogeneous Datasets**|AdÃ¡n JosÃ©-GarcÃ­a et.al.|[2408.13217v1](http://arxiv.org/abs/2408.13217v1)|[link](https://github.com/clementchauvet/py-hbic)|
|**2024-08-23**|**Causal machine learning for sustainable agroecosystems**|Vasileios Sitokonstantinou et.al.|[2408.13155v1](http://arxiv.org/abs/2408.13155v1)|null|
|**2024-08-23**|**Has Multimodal Learning Delivered Universal Intelligence in Healthcare? A Comprehensive Survey**|Qika Lin et.al.|[2408.12880v1](http://arxiv.org/abs/2408.12880v1)|[link](https://github.com/deepreasoning/aihealth)|
|**2024-08-23**|**COVID-19 Probability Prediction Using Machine Learning: An Infectious Approach**|Mohsen Asghari Ilani et.al.|[2408.12841v1](http://arxiv.org/abs/2408.12841v1)|null|
|**2024-08-23**|**Exploring Machine Learning Models for Lung Cancer Level Classification: A comparative ML Approach**|Mohsen Asghari Ilani et.al.|[2408.12838v1](http://arxiv.org/abs/2408.12838v1)|null|
|**2024-08-23**|**Phrasing for UX: Enhancing Information Engagement through Computational Linguistics and Creative Analytics**|Nimrod Dvir et.al.|[2409.00064v1](http://arxiv.org/abs/2409.00064v1)|null|
|**2024-08-22**|**From Radiologist Report to Image Label: Assessing Latent Dirichlet Allocation in Training Neural Networks for Orthopedic Radiograph Classification**|Jakub Olczak et.al.|[2408.13284v1](http://arxiv.org/abs/2408.13284v1)|null|
|**2024-08-22**|**MultiMed: Massively Multimodal and Multitask Medical Understanding**|Shentong Mo et.al.|[2408.12682v1](http://arxiv.org/abs/2408.12682v1)|null|
|**2024-08-22**|**RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment**|Xiaohan Wang et.al.|[2408.12579v1](http://arxiv.org/abs/2408.12579v1)|null|
|**2024-08-22**|**Automatic Organ and Pan-cancer Segmentation in Abdomen CT: the FLARE 2023 Challenge**|Jun Ma et.al.|[2408.12534v1](http://arxiv.org/abs/2408.12534v1)|null|
|**2024-08-22**|**MEDCO: Medical Education Copilots Based on A Multi-Agent Framework**|Hao Wei et.al.|[2408.12496v1](http://arxiv.org/abs/2408.12496v1)|null|
|**2024-08-22**|**AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines**|Douwe J. Spaanderman et.al.|[2408.12491v1](http://arxiv.org/abs/2408.12491v1)|null|
|**2024-08-22**|**WCEbleedGen: A wireless capsule endoscopy dataset and its benchmarking for automatic bleeding classification, detection, and segmentation**|Palak Handa et.al.|[2408.12466v1](http://arxiv.org/abs/2408.12466v1)|[link](https://github.com/misahub2023/benchmarking-codes-of-the-wcebleedgen-dataset)|
|**2024-08-22**|**SAM-SP: Self-Prompting Makes SAM Great Again**|Chunpeng Zhou et.al.|[2408.12364v1](http://arxiv.org/abs/2408.12364v1)|null|
|**2024-08-22**|**Class-balanced Open-set Semi-supervised Object Detection for Medical Images**|Zhanyun Lu et.al.|[2408.12355v1](http://arxiv.org/abs/2408.12355v1)|null|
|**2024-08-22**|**Large Language Models Are Self-Taught Reasoners: Enhancing LLM Applications via Tailored Problem-Solving Demonstrations**|Kai Tzu-iunn Ong et.al.|[2408.12315v1](http://arxiv.org/abs/2408.12315v1)|null|
|**2024-08-22**|**Tipta uzmanlik sinavinda (tus) buyuk dil modelleri insanlardan daha mi basarili?**|Yesim Aygul et.al.|[2408.12305v2](http://arxiv.org/abs/2408.12305v2)|null|
|**2024-08-22**|**Developing vocal system impaired patient-aimed voice quality assessment approach using ASR representation-included multiple features**|Shaoxiang Dang et.al.|[2408.12279v1](http://arxiv.org/abs/2408.12279v1)|null|
|**2024-08-22**|**LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction**|Aishik Nagar et.al.|[2408.12249v1](http://arxiv.org/abs/2408.12249v1)|null|
|**2024-08-22**|**MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**|Yanzeng Li et.al.|[2408.12236v1](http://arxiv.org/abs/2408.12236v1)|null|
|**2024-08-22**|**MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents**|Congchi Yin et.al.|[2408.12142v1](http://arxiv.org/abs/2408.12142v1)|[link](https://github.com/lemonsis/mdd-5k)|
|**2024-08-22**|**DRExplainer: Quantifiable Interpretability in Drug Response Prediction with Directed Graph Convolutional Network**|Haoyuan Shi et.al.|[2408.12139v1](http://arxiv.org/abs/2408.12139v1)|[link](https://github.com/vshy-dream/drexplainer)|
|**2024-08-22**|**Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards**|Shresth Verma et.al.|[2408.12112v1](http://arxiv.org/abs/2408.12112v1)|null|
|**2024-08-22**|**uMedSum: A Unified Framework for Advancing Medical Abstractive Summarization**|Aishik Nagar et.al.|[2408.12095v2](http://arxiv.org/abs/2408.12095v2)|null|
|**2024-08-21**|**Federated Diabetes Prediction in Canadian Adults Using Real-world Cross-Province Primary Care Data**|Guojun Tang et.al.|[2408.12029v1](http://arxiv.org/abs/2408.12029v1)|null|
|**2024-08-21**|**Exploring Large Language Models for Feature Selection: A Data-centric Perspective**|Dawei Li et.al.|[2408.12025v1](http://arxiv.org/abs/2408.12025v1)|null|
|**2024-08-21**|**Clinical Insights: A Comprehensive Review of Language Models in Medicine**|Nikita Neveditsin et.al.|[2408.11735v2](http://arxiv.org/abs/2408.11735v2)|null|
|**2024-08-21**|**BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast Ultrasound Reports**|Yuxuan Chen et.al.|[2408.11334v1](http://arxiv.org/abs/2408.11334v1)|null|
|**2024-08-21**|**Probabilistic Medical Predictions of Large Language Models**|Bowen Gu et.al.|[2408.11316v1](http://arxiv.org/abs/2408.11316v1)|null|
|**2024-08-21**|**Applying and Evaluating Large Language Models in Mental Health Care: A Scoping Review of Human-Assessed Generative Tasks**|Yining Hua et.al.|[2408.11288v1](http://arxiv.org/abs/2408.11288v1)|null|
|**2024-08-21**|**BearLLM: A Prior Knowledge-Enhanced Bearing Health Management Framework with Unified Vibration Signal Representation**|Haotian Peng et.al.|[2408.11281v1](http://arxiv.org/abs/2408.11281v1)|[link](https://github.com/hatton613/bearllm)|
|**2024-08-20**|**From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis**|Guy Lutsker et.al.|[2408.11876v1](http://arxiv.org/abs/2408.11876v1)|null|
|**2024-08-20**|**Fine-Tuning a Local LLaMA-3 Large Language Model for Automated Privacy-Preserving Physician Letter Generation in Radiation Oncology**|Yihao Hou et.al.|[2408.10715v1](http://arxiv.org/abs/2408.10715v1)|null|

#### Abstracts
##### **The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**
2409.04368v1 by Gregory Szumel, Brian Guo, Darui Lu, Rongze Gui, Tingyu Wang, Nicholas Konz, Maciej A. Mazurowski

Purpose: Medical images acquired using different scanners and protocols can
differ substantially in their appearance. This phenomenon, scanner domain
shift, can result in a drop in the performance of deep neural networks which
are trained on data acquired by one scanner and tested on another. This
significant practical issue is well-acknowledged, however, no systematic study
of the issue is available across different modalities and diagnostic tasks.
Materials and Methods: In this paper, we present a broad experimental study
evaluating the impact of scanner domain shift on convolutional neural network
performance for different automated diagnostic tasks. We evaluate this
phenomenon in common radiological modalities, including X-ray, CT, and MRI.
Results: We find that network performance on data from a different scanner is
almost always worse than on same-scanner data, and we quantify the degree of
performance drop across different datasets. Notably, we find that this drop is
most severe for MRI, moderate for X-ray, and quite small for CT, on average,
which we attribute to the standardized nature of CT acquisition systems which
is not present in MRI or X-ray. We also study how injecting varying amounts of
target domain data into the training set, as well as adding noise to the
training data, helps with generalization. Conclusion: Our results provide
extensive experimental evidence and quantification of the extent of performance
drop caused by scanner domain shift in deep learning across different
modalities, with the goal of guiding the future development of robust deep
learning models for medical image analysis.

æè¦ï¼<paragraph>ç®çï¼ä½¿ç¨ä¸åææåååå®åå¾çé«å­¸å½±åï¼å¨å½±åå¤è§ä¸å¯è½ææé¡¯èå·®ç°ãéç¨®ç¾è±¡ç¨±çºææåé ååç§»ï¼å¯è½æå°è´æ·±åº¦ç¥ç¶ç¶²è·¯çæè½ä¸éï¼èéäºç¶²è·¯æ¯éå°ç±ä¸ç¨®ææååå¾çè³æé²è¡è¨ç·´ï¼ä¸¦å¨å¦ä¸ç¨®ææåä¸é²è¡æ¸¬è©¦ãéåéè¦çå¯¦éåé¡å·²ç²å¾å»£æ³èªå¯ï¼ä½ç®åå°æªéå°ä¸åå½¢å¼åè¨ºæ·ä»»åé²è¡ç³»çµ±æ§ç ç©¶ãææåæ¹æ³ï¼å¨æ¬æä¸­ï¼æåæåºäºä¸é å»£æ³çå¯¦é©ç ç©¶ï¼è©ä¼°ææåé ååç§»å°ä¸åèªååè¨ºæ·ä»»åçå·ç©ç¥ç¶ç¶²è·¯æè½çå½±é¿ãæåå¨å¸¸è¦çæ¾å°å­¸å½¢å¼ä¸­è©ä¼°éç¨®ç¾è±¡ï¼åæ¬ X åãé»è¦æ·å±¤ææåç£æ¯é å½±ãçµæï¼æåç¼ç¾ï¼ä¾èªä¸åææåçè³æå¨ç¶²è·¯ä¸çæè½å¹¾ä¹ç¸½æ¯æ¯ä¾èªç¸åææåçè³æå·®ï¼æåéåäºä¸åè³æéæè½ä¸éçç¨åº¦ãå¼å¾æ³¨æçæ¯ï¼æåç¼ç¾éç¨®ä¸éå¨ç£æ¯é å½±ä¸­æä¸ºå´éï¼å¨ X åä¸­çºä¸­ç­ï¼å¨é»è¦æ·å±¤ææä¸­ç¸ç¶å°ï¼å¹³åèè¨ï¼æåå°å¶æ­¸å æ¼é»è¦æ·å±¤ææåå¾ç³»çµ±çæ¨æºåæ§è³ªï¼èç£æ¯é å½±æ X åä¸­ä¸å­å¨éç¨®æ§è³ªãæåéç ç©¶äºå°ä¸åæ¸éçç®æ¨é åè³ææ³¨å¥è¨ç·´éï¼ä»¥ååè¨ç·´è³æå å¥éè¨ï¼å¦ä½æå©æ¼æ³åãçµè«ï¼æåççµææä¾äºå»£æ³çå¯¦é©è­æï¼ä¸¦éåäºæ·±åº¦å­¸ç¿ä¸­ç±ææåé ååç§»é æçæè½ä¸éç¨åº¦ï¼ç®æ¨æ¯å¼å°æªä¾éå°é«å­¸å½±ååæçå¼·å¥æ·±åº¦å­¸ç¿æ¨¡åçç¼å±ã</paragraph>

##### **CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**
2409.04290v1 by William Knottenbelt, Zeyu Gao, Rebecca Wray, Woody Zhidong Zhang, Jiashuai Liu, Mireia Crispin-Ortuzar

Survival analysis is a branch of statistics used for modeling the time until
a specific event occurs and is widely used in medicine, engineering, finance,
and many other fields. When choosing survival models, there is typically a
trade-off between performance and interpretability, where the highest
performance is achieved by black-box models based on deep learning. This is a
major problem in fields such as medicine where practitioners are reluctant to
blindly trust black-box models to make important patient decisions.
Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable
and accurate alternative to multi-layer perceptrons (MLPs). We introduce
CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,
high-performance survival analysis. We evaluate the proposed CoxKAN on 4
synthetic datasets and 9 real medical datasets. The synthetic experiments
demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for
the hazard function, and effectively performs automatic feature selection.
Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the
Cox proportional hazards model and achieves performance that is superior or
comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies
complex interactions between predictor variables that would be extremely
difficult to recognise using existing survival methods, and automatically finds
symbolic formulae which uncover the precise effect of important biomarkers on
patient risk.

æè¦ï¼çå­åææ¯çµ±è¨å­¸çä¸ååæ¯ï¼ç¨æ¼å»ºæ¨¡ç¹å®äºä»¶ç¼ççæéï¼ä¸¦å»£æ³ç¨æ¼é«å­¸ãå·¥ç¨ãéèåè¨±å¤å¶ä»é åãå¨é¸æçå­æ¨¡åæï¼éå¸¸å¨æ§è½åå¯è§£éæ§ä¹éé²è¡æ¬è¡¡ï¼å¶ä¸­æé«æ§è½æ¯ç±åºæ¼æ·±åº¦å­¸ç¿çé»çæ¨¡åå¯¦ç¾çãéå¨é«å­¸ç­é åæ¯ä¸åä¸»è¦åé¡ï¼å çºå¾æ¥­èä¸é¡æç²ç®ä¿¡ä»»é»çæ¨¡åä¾ååºéè¦çæ£èæ±ºç­ãKolmogorov-é¿è«¾å¾·ç¶²çµ¡ (KAN) æè¿è¢«æè­°ä½çºå¤å±¤æç¥å¨ (MLP) çå¯è§£éä¸æºç¢ºçæ¿ä»£æ¹æ¡ãæåå¼å¥äº CoxKANï¼éæ¯ä¸åç¨æ¼å¯è§£éãé«æ§è½çå­åæç Cox æ¯ä¾é¢¨éª Kolmogorov-Arnold ç¶²çµ¡ãæåå¨ 4 ååææ¸æéå 9 åçå¯¦é«çæ¸æéä¸è©ä¼°äºææåºç CoxKANãåæå¯¦é©è¡¨æï¼CoxKAN æºç¢ºå°æ¢å¾©äºé¢¨éªå½æ¸çå¯è§£éç¬¦èå¬å¼ï¼ä¸¦ææå°å·è¡èªåç¹å¾µé¸æãå° 9 åçå¯¦æ¸æéçè©ä¼°è¡¨æï¼CoxKAN å§çµåªæ¼ Cox æ¯ä¾é¢¨éªæ¨¡åï¼ä¸¦ä¸éå°äºåªæ¼æèèª¿æ´å¾ç MLP ç¸ç¶çæ§è½ãæ­¤å¤ï¼æåç¼ç¾ CoxKAN è­å¥äºé æ¸¬è®éä¹éçè¤éäº¤äºä½ç¨ï¼éäºäº¤äºä½ç¨ä½¿ç¨ç¾æççå­æ¹æ³æ¥µé£è­å¥ï¼ä¸¦èªåæ¾å°æ­ç¤ºéè¦çç©æ¨èªç©å°æ£èé¢¨éªçæºç¢ºå½±é¿çç¬¦èå¬å¼ã

##### **Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**
2409.04224v1 by Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng

Multi-organ diseases present significant challenges due to their simultaneous
impact on multiple organ systems, necessitating complex and adaptive treatment
strategies. Despite recent advancements in AI-powered healthcare decision
support systems, existing solutions are limited to individual organ systems.
They often ignore the intricate dependencies between organ system and thereby
fails to provide holistic treatment recommendations that are useful in
practice. We propose a novel hierarchical multi-agent reinforcement learning
(HMARL) framework to address these challenges. This framework uses dedicated
agents for each organ system, and model dynamic through explicit inter-agent
communication channels, enabling coordinated treatment strategies across
organs. Furthermore, we introduce a dual-layer state representation technique
to contextualize patient conditions at various hierarchical levels, enhancing
the treatment accuracy and relevance. Through extensive qualitative and
quantitative evaluations in managing sepsis (a complex multi-organ disease),
our approach demonstrates its ability to learn effective treatment policies
that significantly improve patient survival rates. This framework marks a
substantial advancement in clinical decision support systems, pioneering a
comprehensive approach for multi-organ treatment recommendations.

æè¦ï¼å¤å¨å®ç¾çç±æ¼åæå½±é¿å¤åå¨å®ç³»çµ±ï¼å æ­¤æå¸¶ä¾éå¤§çææ°ï¼éè¦è¤éä¸å·æé©ææ§çæ²»çç­ç¥ãåç®¡ AI é©åçé«çä¿å¥æ±ºç­æ¯æ´ç³»çµ±æè¿æé²å±ï¼ä½ç¾æè§£æ±ºæ¹æ¡åéæ¼åå¥å¨å®ç³»çµ±ãå®åå¸¸å¸¸å¿½ç¥å¨å®ç³»çµ±ä¹éçè¤éä¾è³´æ§ï¼å æ­¤ç¡æ³æä¾å¯¦åä¸æç¨çæ´é«æ²»çå»ºè­°ãæåæåºä¸åæ°ç©çåå±¤å¤æºè½é«å¼·åå­¸ç¿ (HMARL) æ¶æ§ä¾è§£æ±ºéäºææ°ãæ­¤æ¶æ§çºæ¯åå¨å®ç³»çµ±ä½¿ç¨å°ç¨æºè½é«ï¼ä¸¦ééæç¢ºçæºè½é«ééè¨ç®¡éå»ºæ¨¡åæï¼è®ä¸åå¨å®ä¹éçæ²»çç­ç¥è½å¤ åèª¿ãæ­¤å¤ï¼æåå¼å¥éå±¤çæè¡¨ç¤ºæè¡ï¼å¨åç¨®å±¤ç´èªå¢åçæ£çæ³ï¼ä»¥æåæ²»çæºç¢ºæ§åç¸éæ§ãééå¨æè¡çï¼ä¸ç¨®è¤éçå¤å¨å®ç¾çï¼ç®¡çä¸­é²è¡å»£æ³çå®æ§åå®éè©ä¼°ï¼æåçåæ³å±ç¤ºäºå®å­¸ç¿æææ²»çæ¿ç­çè½åï¼å¯é¡¯èæ¹åçæ£å­æ´»çãæ­¤æ¶æ§æ¨èªèè¨åºæ±ºç­æ¯æ´ç³»çµ±çä¸å¤§é²æ­¥ï¼éåµäºå¤å¨å®æ²»çå»ºè­°çå¨é¢æ§æ¹æ³ã

##### **FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**
2409.03947v1 by Kai Shu, Yuzhuo Jia, Ziyang Zhang, Jiechao Gao

Automatic Medical Imaging Narrative generation aims to alleviate the workload
of radiologists by producing accurate clinical descriptions directly from
radiological images. However, the subtle visual nuances and domain-specific
terminology in medical images pose significant challenges compared to generic
image captioning tasks. Existing approaches often neglect the vital distinction
between normal and abnormal findings, leading to suboptimal performance. In
this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive
Partitioning Graph framework that addresses these limitations through
domain-adaptive learning. FODA-PG constructs a granular graphical
representation of radiological findings by separating disease-related
attributes into distinct "disease-specific" and "disease-free" categories based
on their clinical significance and location. This adaptive partitioning enables
our model to capture the nuanced differences between normal and pathological
states, mitigating the impact of data biases. By integrating this fine-grained
semantic knowledge into a powerful transformer-based architecture and providing
rigorous mathematical justifications for its effectiveness, FODA-PG generates
precise and clinically coherent reports with enhanced generalization
capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks
demonstrate the superiority of our approach over state-of-the-art methods,
highlighting the importance of domain adaptation in medical report generation.

æè¦ï¼èªåé«å­¸å½±åæè¿°çææ¨å¨ééç´æ¥å¾æ¾å°å½±åç¢çç²¾ç¢ºçè¨åºæè¿°ï¼æ¸è¼æ¾å°ç§é«å¸«çå·¥ä½è² æãç¶èï¼èä¸è¬å½±åæ¨é¡ä»»åç¸æ¯ï¼é«å­¸å½±åä¸­çç´°å¾®è¦è¦ºå·®ç°åç¹å®é åè¡èªæå¸¶ä¾éå¤§ææ°ãç¾ææ¹æ³å¸¸å¸¸å¿½ç¥æ­£å¸¸èç°å¸¸ç¼ç¾ä¹éçéè¦åå¥ï¼å°è´æ¬¡ä½³æè½ãå¨éé å·¥ä½ä¸­ï¼æåæåº FODA-PGï¼éæ¯ä¸åæ°ç©çç´°ç²åº¦å¨å®ç¾çèªé©æåå²åå½¢æ¶æ§ï¼ééé åèªé©æå­¸ç¿ä¾è§£æ±ºéäºéå¶ãFODA-PG ééå°ç¾çç¸éå±¬æ§ä¾æå¶è¨åºéè¦æ§åä½ç½®åçºä¸åçãç¹å®ç¾çãåãç¡ç¾çãé¡å¥ï¼ä¾å»ºæ§æ¾å°å­¸ç¼ç¾çç´°ç²åº¦åå½¢è¡¨ç¤ºãéç¨®èªé©æåå²ä½¿æåçæ¨¡åè½å¤ æææ­£å¸¸èçççæä¹éçç´°å¾®å·®ç°ï¼æ¸è¼è³æåå·®çå½±é¿ãééå°éç¨®ç´°ç²åº¦èªç¾©ç¥è­æ´åå°å¼·å¤§çåºæ¼è½æå¨çæ¶æ§ä¸­ï¼ä¸¦æä¾å¶æææ§çå´è¬¹æ¸å­¸è­æï¼FODA-PG è½å¤ çæç²¾ç¢ºä¸è¨åºä¸é£è²«çå ±åï¼ä¸¦å·åå¢å¼·çæ¦æ¬è½åãå¨ IU-Xray å MIMIC-CXR åºæºä¸çå»£æ³å¯¦é©è­æäºæåçæ¹æ³åªæ¼æåé²çæ¹æ³ï¼çªé¡¯äºé åé©æå¨é«å­¸å ±åçæä¸­çéè¦æ§ã

##### **A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**
2409.03933v1 by Esther Lagemann, Julia Roeb, Steven L. Brunton, Christian Lagemann

The accurate quantification of wall-shear stress dynamics is of substantial
importance for various applications in fundamental and applied research,
spanning areas from human health to aircraft design and optimization. Despite
significant progress in experimental measurement techniques and post-processing
algorithms, temporally resolved wall-shear stress dynamics with adequate
spatial resolution and within a suitable spatial domain remain an elusive goal.
To address this gap, we introduce a deep learning architecture that ingests
wall-parallel velocity fields from the logarithmic layer of turbulent
wall-bounded flows and outputs the corresponding 2D wall-shear stress fields
with identical spatial resolution and domain size. From a physical perspective,
our framework acts as a surrogate model encapsulating the various mechanisms
through which highly energetic outer-layer flow structures influence the
governing wall-shear stress dynamics. The network is trained in a supervised
fashion on a unified dataset comprising direct numerical simulations of
statistically 1D turbulent channel and spatially developing turbulent boundary
layer flows at friction Reynolds numbers ranging from 390 to 1,500. We
demonstrate a zero-shot applicability to experimental velocity fields obtained
from Particle-Image Velocimetry measurements and verify the physical accuracy
of the wall-shear stress estimates with synchronized wall-shear stress
measurements using the Micro-Pillar Shear-Stress Sensor for Reynolds numbers up
to 2,000. In summary, the presented framework lays the groundwork for
extracting inaccessible experimental wall-shear stress information from readily
available velocity measurements and thus, facilitates advancements in a variety
of experimental applications.

æè¦ï¼<paragraph>æºç¢ºéåå£é¢åªæååæå°æ¼åºç¤åæç¨ç ç©¶ä¸­çåç¨®æç¨å·æå¯¦è³ªæ§çéè¦æ§ï¼æ¶µèå¾äººé¡å¥åº·å°é£æ©è¨­è¨ååªåçé åãåç®¡å¨å¯¦é©æ¸¬éæè¡åå¾èçæ¼ç®æ³æ¹é¢åå¾äºé¡¯èé²å±ï¼ä½æéè§£æå£é¢åªæååæä»å·æè¶³å¤ çç©ºéè§£æåº¦åå¨åé©çç©ºéåä¸­ä»ç¶æ¯ä¸åé£ä»¥ææ¸çç®æ¨ãçºäºè§£æ±ºéåå·®è·ï¼æåå¼å¥äºä¸åæ·±åº¦å­¸ç¿æ¶æ§ï¼å®å¾æ¹æµå£é¢ç´ææµçå°æ¸å±¤ä¸­æåå£é¢å¹³è¡éåº¦å ´ï¼ä¸¦è¼¸åºç¸æç 2D å£é¢åªæåå ´ï¼å·æç¸åçç©ºéè§£æåº¦ååå¤§å°ãå¾ç©çè§åº¦ä¾çï¼æåçæ¡æ¶åç¶ä¸åä»£çæ¨¡åï¼æ¦æ¬äºé«è½éå¤å±¤æµçµæ§å½±é¿æ§å¶å£é¢åªæååæçåç¨®æ©å¶ãè©²ç¶²è·¯ä»¥ç£ç£æ¹å¼å¨ä¸åçµ±ä¸çæ¸æéä¸é²è¡è¨ç·´ï¼è©²æ¸æéåå«çµ±è¨ 1D æ¹æµééçç´æ¥æ¸å¼æ¨¡æ¬åç©ºéç¼å±çæ¹æµéçå±¤æµï¼æ©æ¦é·è«¾æ¸ç¯åå¾ 390 å° 1,500ãæåå±ç¤ºäºå°å¾ç²å­å½±åæ¸¬éæ¸¬éä¸­ç²å¾çå¯¦é©éåº¦å ´çé¶æ¬¡æç¨ï¼ä¸¦ä½¿ç¨å¾®æ±åªæåææ¸¬å¨å°é·è«¾æ¸æé« 2,000 çåæ­¥å£é¢åªæåæ¸¬éé©è­äºå£é¢åªæåä¼°è¨çç©çæºç¢ºæ§ãç¸½ä¹ï¼ææåºçæ¡æ¶çºå¾å®¹æç²å¾çéåº¦æ¸¬éä¸­æåç¡æ³ç²å¾çå¯¦é©å£é¢åªæåè³è¨å¥ å®äºåºç¤ï¼å¾èä¿é²äºåç¨®å¯¦é©æç¨ä¸­çé²å±ã</paragraph>

##### **Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**
2409.03597v1 by Yucong Zhang, Xin Zou, Jinshan Yang, Wenjun Chen, Faya Liang, Ming Li

This paper presents the Multimodal Analyzing System for Laryngoscope (MASL),
a system that combines audio and video data to automatically extract key
segments and metrics from laryngeal videostroboscopic videos for clinical
assessment. MASL integrates glottis detection with keyword spotting to analyze
patient vocalizations and refine video highlights for better inspection of
vocal cord movements. The system includes a strobing video extraction module
that identifies frames by analyzing hue, saturation, and value fluctuations.
MASL also provides effective metrics for vocal cord paralysis detection,
employing a two-stage glottis segmentation process using U-Net followed by
diffusion-based refinement to reduce false positives. Instead of glottal area
waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis
masks, evaluating both left and right vocal cords to detect unilateral vocal
cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between
left and right paralysis. Ablation studies and experiments on public and
real-world datasets validate MASL's segmentation module and demonstrate its
ability to provide reliable metrics for UVFP diagnosis.

æè¦ï¼æ¬ææåºäºåéå¤æ¨¡æåæç³»ç» (MASL)ï¼
è¯¥ç³»ç»ç»åé³é¢åè§é¢æ°æ®ï¼èªå¨ä»åé¨è§é¢é¢éªéè§é¢ä¸­æåå³é®
çæ®µåææ ï¼ç¨äºä¸´åºè¯ä¼°ãMASL å°å£°é¨æ£æµä¸å³é®è¯è¯å«ç¸ç»åï¼ä»¥åæ
æ£èåå£°å¹¶ç»åè§é¢éç¹ï¼ä»¥ä¾¿æ´å¥½å°æ£æ¥å£°å¸¦è¿å¨ãè¯¥ç³»ç»åæ¬ä¸ä¸ªé¢éªè§é¢æåæ¨¡åï¼
è¯¥æ¨¡åéè¿åæè²ç¸ãé¥±ååº¦åå¼æ³¢å¨æ¥è¯å«å¸§ã
MASL è¿ä¸ºå£°å¸¦éº»ç¹æ£æµæä¾äºææçææ ï¼
éç¨ä¸¤é¶æ®µå£°é¨åå²è¿ç¨ï¼ä½¿ç¨ U-Netï¼ç¶åè¿è¡åºäºæ©æ£çç»åä»¥åå°è¯¯æ¥ãMASL ä¸ä½¿ç¨å£°é¨é¢ç§¯æ³¢å½¢ï¼èæ¯ä»å£°é¨æ©æ¨¡ä¸­ä¼°è®¡åå£°é¨è§æ³¢å½¢ (AGAW)ï¼è¯ä¼°å·¦å³å£°å¸¦ä»¥æ£æµåä¾§å£°å¸¦éº»ç¹ (UVFP)ãéè¿æ¯è¾ AGAW æ¹å·®ï¼MASL åºåå·¦å³éº»ç¹ãæ¶èç ç©¶åå¯¹å¬å±åçå®ä¸çæ°æ®éçå®éªéªè¯äº MASL çåå²æ¨¡åï¼å¹¶è¯æäºå¶æä¾å¯é ç UVFP è¯æ­ææ çè½åã

##### **Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**
2409.03470v1 by Prerak Mody, Nicolas F. Chaves-de-Plaza, Chinmay Rao, Eleftheria Astrenidou, Mischa de Ridder, Nienke Hoekstra, Klaus Hildebrandt, Marius Staring

Increased usage of automated tools like deep learning in medical image
segmentation has alleviated the bottleneck of manual contouring. This has
shifted manual labour to quality assessment (QA) of automated contours which
involves detecting errors and correcting them. A potential solution to
semi-automated QA is to use deep Bayesian uncertainty to recommend potentially
erroneous regions, thus reducing time spent on error detection. Previous work
has investigated the correspondence between uncertainty and error, however, no
work has been done on improving the "utility" of Bayesian uncertainty maps such
that it is only present in inaccurate regions and not in the accurate ones. Our
work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which
promotes uncertainty to be present only in inaccurate regions. We apply this
method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and
prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated
against voxel inaccuracies using Receiver Operating Characteristic (ROC) and
Precision-Recall (PR) curves. Numerical results show that when compared to the
Bayesian baseline the proposed method successfully suppresses uncertainty for
accurate voxels, with similar presence of uncertainty for inaccurate voxels.
Code to reproduce experiments is available at
https://github.com/prerakmody/bayesuncertainty-error-correspondence

æè¦ï¼æ·±åº¦å­¸ç¿ç­èªååå·¥å·å¨é«å­¸å½±ååå²ä¸­ä½¿ç¨çæåï¼æ¸è¼äºæåè¼ªå»æç¹ªçç¶é ¸ãéå·²å°æåååè½ç§»å°èªåè¼ªå»çåè³ªè©ä¼° (QA)ï¼å¶ä¸­åå«åµæ¸¬é¯èª¤ä¸¦ä¿®æ­£å®åãåèªåå QA çæ½å¨è§£æ±ºæ¹æ¡æ¯ä½¿ç¨æ·±åº¦è²æ°ä¸ç¢ºå®æ§ä¾å»ºè­°æ½å¨çé¯èª¤ååï¼å¾èæ¸å°è±è²»å¨é¯èª¤åµæ¸¬ä¸çæéãååçç ç©¶å·²èª¿æ¥ä¸ç¢ºå®æ§åé¯èª¤ä¹éçå°æéä¿ï¼ç¶èï¼å°æªå°æ¹åè²æ°ä¸ç¢ºå®æ§å°åçãæç¨ãé²è¡ç ç©¶ï¼ä»¥ä½¿å¶ååºç¾å¨ä¸æºç¢ºååï¼èä¸åºç¾å¨æºç¢ºååãæåçç ç©¶ä½¿ç¨æºç¢ºåº¦å°æä¸ç¢ºå®æ§ (AvU) æå¤±ä¾è¨ç·´ FlipOut æ¨¡åï¼éæä¿ä½¿ä¸ç¢ºå®æ§ååºç¾å¨ä¸æºç¢ºååãæåå°æ­¤æ¹æ³æç¨æ¼å©åæ¾å°æ²»çé¨ä½çè³æéï¼å³é ­é ¸é¨é»è¦æ·å±¤ææåååèºæ ¸ç£å±æ¯ææãä½¿ç¨æ¥æ¶å¨æä½ç¹æ§ (ROC) åç²¾ç¢ºåº¦å¬åç (PR) æ²ç·ï¼éå°é«ç´ ä¸æºç¢ºæ§è©ä¼°ä¸ç¢ºå®æ§ç±åï¼å³é æ¸¬çµï¼ãæ¸å¼çµæé¡¯ç¤ºï¼èè²æ°åºæºç¸æ¯ï¼ææåºçæ¹æ³æåå°æå¶æºç¢ºé«ç´ çä¸ç¢ºå®æ§ï¼å°æ¼ä¸æºç¢ºé«ç´ çä¸ç¢ºå®æ§å­å¨é¡ä¼¼ææ³ãå¯å¨ https://github.com/prerakmody/bayesuncertainty-error-correspondence åå¾éç¾å¯¦é©çç¨å¼ç¢¼

##### **Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**
2409.03375v1 by Francisco de Arriba-PÃ©rez, Silvia GarcÃ­a-MÃ©ndez

Based on official estimates, 50 million people worldwide are affected by
dementia, and this number increases by 10 million new patients every year.
Without a cure, clinical prognostication and early intervention represent the
most effective ways to delay its progression. To this end, Artificial
Intelligence and computational linguistics can be exploited for natural
language analysis, personalized assessment, monitoring, and treatment. However,
traditional approaches need more semantic knowledge management and
explicability capabilities. Moreover, using Large Language Models (LLMs) for
cognitive decline diagnosis is still scarce, even though these models represent
the most advanced way for clinical-patient communication using intelligent
systems. Consequently, we leverage an LLM using the latest Natural Language
Processing (NLP) techniques in a chatbot solution to provide interpretable
Machine Learning prediction of cognitive decline in real-time.
Linguistic-conceptual features are exploited for appropriate natural language
analysis. Through explainability, we aim to fight potential biases of the
models and improve their potential to help clinical workers in their diagnosis
decisions. More in detail, the proposed pipeline is composed of (i) data
extraction employing NLP-based prompt engineering; (ii) stream-based data
processing including feature engineering, analysis, and selection; (iii)
real-time classification; and (iv) the explainability dashboard to provide
visual and natural language descriptions of the prediction outcome.
Classification results exceed 80 % in all evaluation metrics, with a recall
value for the mental deterioration class about 85 %. To sum up, we contribute
with an affordable, flexible, non-invasive, personalized diagnostic system to
this work.

æè¦ï¼<paragraph>æ ¹æå®æ¹çä¼°è¨ï¼å¨çç´æ 5000 è¬äººç½¹æ£å¤±æºçï¼ä¸éåæ¸å­æ¯å¹´å¢å  1000 è¬åæ°æ£èãå¨æ²ææ²»çæ¹æ³çææ³ä¸ï¼è¨åºé å¾åæ©æä»å¥æ¯å»¶ç·©å¶æ¡åçææææ¹æ³ãçºæ­¤ï¼äººå·¥æºæ§åè¨ç®èªè¨å­¸å¯è¢«ç¨æ¼èªç¶èªè¨åæãåäººåè©ä¼°ãç£æ§åæ²»çãç¶èï¼å³çµ±æ¹æ³éè¦æ´å¤èªç¾©ç¥è­ç®¡çåå¯è§£éæ§è½åãæ­¤å¤ï¼åç®¡éäºæ¨¡åä»£è¡¨äºä½¿ç¨æºæ§ç³»çµ±é²è¡è¨åºæ£èæºéçæåé²æ¹å¼ï¼ä½å°å¤§åèªè¨æ¨¡å (LLM) ç¨æ¼èªç¥è½åä¸éè¨ºæ·ä»ç¶å¾å°è¦ãå æ­¤ï¼æåå©ç¨èå¤©æ©å¨äººè§£æ±ºæ¹æ¡ä¸­ä½¿ç¨ææ°èªç¶èªè¨èç (NLP) æè¡ç LLMï¼ä»¥æä¾å°èªç¥è½åä¸éçæ©å¨å­¸ç¿é æ¸¬ãèªè¨æ¦å¿µç¹å¾µè¢«ç¨æ¼é©ç¶çèªç¶èªè¨åæãééå¯è§£éæ§ï¼æåæ¨å¨æ¶é¤æ¨¡åçæ½å¨åå·®ï¼ä¸¦æé«å¶å¨è¨ºæ·æ±ºç­ä¸­åå©è¨åºå·¥ä½èçæ½åãæ´è©³ç´°å°èªªï¼ææåºçç®¡éåæ¬ï¼(i) ä½¿ç¨åºæ¼ NLP çæç¤ºå·¥ç¨é²è¡è³æèåï¼(ii) ä¸²æµå¼è³æèçï¼åæ¬ç¹å¾µå·¥ç¨ãåæåé¸æï¼(iii) å³æåé¡ï¼ä»¥å (iv) å¯è§£éæ§åè¡¨æ¿ï¼ä»¥æä¾é æ¸¬çµæçå¯è¦ååèªç¶èªè¨æè¿°ãåé¡çµæå¨ææè©ä¼°ææ¨ä¸­é½è¶é 80%ï¼å¿æºéåé¡å¥çå¬åçç´çº 85%ãç¸½èè¨ä¹ï¼æåçºéé å·¥ä½è²¢ç»äºä¸åç¶æ¿å¯¦æ ãéæ´»ãéä¾µå¥æ§ãåäººåçè¨ºæ·ç³»çµ±ã</paragraph>

##### **Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**
2409.03147v1 by Juan A. Berrios Moya

The rapid global aging trend has led to an increase in dementia cases,
including Alzheimer's disease, underscoring the urgent need for early and
accurate diagnostic methods. Traditional diagnostic techniques, such as
cognitive tests, neuroimaging, and biomarker analysis, face significant
limitations in sensitivity, accessibility, and cost, particularly in the early
stages. This study explores the potential of machine learning (ML) as a
transformative approach to enhance early dementia detection by leveraging ML
models to analyze and integrate complex multimodal datasets, including
cognitive assessments, neuroimaging, and genetic information. A comprehensive
review of existing literature was conducted to evaluate various ML models,
including supervised learning, deep learning, and advanced techniques such as
ensemble learning and transformer models, assessing their accuracy,
interpretability, and potential for clinical integration. The findings indicate
that while ML models show significant promise in improving diagnostic precision
and enabling earlier interventions, challenges remain in their
generalizability, interpretability, and ethical deployment. This research
concludes by outlining future directions aimed at enhancing the clinical
utility of ML models in dementia detection, emphasizing interdisciplinary
collaboration and ethically sound frameworks to improve early detection and
intervention strategies for Alzheimer's disease and other forms of dementia.

æè¦ï¼å¨çäººå£å¿«éèåè¶¨å¢å°è´å¤±æºççä¾å¢å ï¼åæ¬é¿è²æµ·é»çï¼çªé¡¯åºæ©æä¸æºç¢ºçè¨ºæ·æ¹æ³çè¿«åéæ±ãå³çµ±çè¨ºæ·æè¡ï¼ä¾å¦èªç¥æ¸¬é©ãç¥ç¶å½±ååçç©æ¨è¨åæï¼å¨æææ§ãå¯åæ§åææ¬æ¹é¢é¢è¨éå¤§éå¶ï¼ç¹å¥æ¯å¨æ©æéæ®µãæ¬ç ç©¶æ¢è¨æ©å¨å­¸ç¿ (ML) ä½çºä¸ç¨®è®é©æ§æ¹æ³çæ½åï¼ééå©ç¨ ML æ¨¡ååæåæ´åè¤éçå¤æ¨¡å¼æ¸æéï¼åæ¬èªç¥è©ä¼°ãç¥ç¶å½±ååéºå³ä¿¡æ¯ï¼ä¾å¢å¼·æ©æå¤±æºçæª¢æ¸¬ãå°ç¾ææç»é²è¡äºå¨é¢åé¡§ï¼ä»¥è©ä¼°åç¨® ML æ¨¡åï¼åæ¬ç£ç£å­¸ç¿ãæ·±åº¦å­¸ç¿ååé²æè¡ï¼ä¾å¦éæå­¸ç¿åTransformeræ¨¡åï¼è©ä¼°å¶æºç¢ºæ§ãå¯è§£éæ§åè¨åºæ´åçæ½åãç ç©¶çµæè¡¨æï¼åç®¡ ML æ¨¡åå¨æé«è¨ºæ·ç²¾åº¦åå¯¦ç¾æ©æå¹²é æ¹é¢é¡¯ç¤ºåºé¡¯èçå¸æï¼ä½å¶å¯æ¦åæ§ãå¯è§£éæ§åéå¾·é¨ç½²ä»ç¶å­å¨ææ°ãæ¬ç ç©¶æå¾æ¦è¿°äºæ¨å¨å¢å¼· ML æ¨¡åå¨å¤±æºçæª¢æ¸¬ä¸­çè¨åºæç¨çæªä¾æ¹åï¼å¼·èª¿è·¨å­¸ç§åä½åéå¾·å¥å¨çæ¡æ¶ï¼ä»¥æ¹åé¿è²æµ·é»çåå¶ä»å½¢å¼å¤±æºççæ©ææª¢æ¸¬åå¹²é ç­ç¥ã

##### **MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**
2409.03062v1 by Shehan Perera, Yunus Erzurumlu, Deepak Gulati, Alper Yilmaz

Skin cancer segmentation poses a significant challenge in medical image
analysis. Numerous existing solutions, predominantly CNN-based, face issues
related to a lack of global contextual understanding. Alternatively, some
approaches resort to large-scale Transformer models to bridge the global
contextual gaps, but at the expense of model size and computational complexity.
Finally many Transformer based approaches rely primarily on CNN based decoders
overlooking the benefits of Transformer based decoding models. Recognizing
these limitations, we address the need efficient lightweight solutions by
introducing MobileUNETR, which aims to overcome the performance constraints
associated with both CNNs and Transformers while minimizing model size,
presenting a promising stride towards efficient image segmentation. MobileUNETR
has 3 main features. 1) MobileUNETR comprises of a lightweight hybrid
CNN-Transformer encoder to help balance local and global contextual feature
extraction in an efficient manner; 2) A novel hybrid decoder that
simultaneously utilizes low-level and global features at different resolutions
within the decoding stage for accurate mask generation; 3) surpassing large and
complex architectures, MobileUNETR achieves superior performance with 3 million
parameters and a computational complexity of 1.3 GFLOP resulting in 10x and 23x
reduction in parameters and FLOPS, respectively. Extensive experiments have
been conducted to validate the effectiveness of our proposed method on four
publicly available skin lesion segmentation datasets, including ISIC 2016, ISIC
2017, ISIC 2018, and PH2 datasets. The code will be publicly available at:
https://github.com/OSUPCVLab/MobileUNETR.git

æè¦ï¼ç®èçåå²å¨é«å­¸å½±ååæä¸­æ§æä¸é éå¤§ææ°ãç¾æè¨±å¤è§£æ±ºæ¹æ¡ï¼ä¸»è¦æ¯åºæ¼ CNNï¼é¢è¨ç¼ºä¹æ´é«èæ¯çè§£çåé¡ãæèï¼ä¸äºæ¹æ³è¨´è«¸æ¼å¤§è¦æ¨¡ Transformer æ¨¡åä¾å½åæ´é«èæ¯å·®è·ï¼ä½ç§ç²äºæ¨¡åå¤§å°åè¨ç®è¤éåº¦ãæå¾ï¼è¨±å¤åºæ¼ Transformer çæ¹æ³ä¸»è¦ä¾è³´æ¼åºæ¼ CNN çè§£ç¢¼å¨ï¼èå¿½è¦äºåºæ¼ Transformer çè§£ç¢¼æ¨¡åçåªé»ãèªè­å°éäºéå¶ï¼æåééå¼å¥ MobileUNETR ä¾è§£æ±ºå°é«æè¼éç´è§£æ±ºæ¹æ¡çéæ±ï¼å¶ç®æ¨æ¯åæè CNN å Transformer ç¸éçæè½éå¶ï¼åææå°åæ¨¡åå¤§å°ï¼çºé«æå½±ååå²éåºæå¸æçä¸æ­¥ãMobileUNETR æ 3 åä¸»è¦ç¹é»ã1) MobileUNETR åå«ä¸åè¼éç´æ··å CNN-Transformer ç·¨ç¢¼å¨ï¼ä»¥ææçæ¹å¼å¹«å©å¹³è¡¡å±é¨åæ´é«èæ¯ç¹å¾µæåï¼2) ä¸åæ°ç©çæ··åè§£ç¢¼å¨ï¼å¨è§£ç¢¼éæ®µåæå©ç¨ä¸åè§£æåº¦ä¸çä½éåæ´é«ç¹å¾µï¼ä»¥é²è¡ç²¾ç¢ºçé®ç½©çæï¼3) è¶è¶å¤§åèè¤éçæ¶æ§ï¼MobileUNETR ä»¥ 300 è¬ååæ¸å 1.3 GFLOP çè¨ç®è¤éåº¦å¯¦ç¾äºåè¶çæè½ï¼åå¥æ¸å°äº 10 åå 23 åçåæ¸å FLOPãå·²ç¶é²è¡äºå»£æ³çå¯¦é©ï¼ä»¥é©è­æåæåºçæ¹æ³å¨ååå¬éå¯ç¨çç®èçè®åå²è³æéï¼åæ¬ ISIC 2016ãISIC 2017ãISIC 2018 å PH2 è³æéï¼ä¸çæææ§ãç¨å¼ç¢¼å°å¬éæ¼ï¼https://github.com/OSUPCVLab/MobileUNETR.git

##### **Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**
2409.02883v1 by Junyoung Park, Eun Hyun Seo, Sunjun Kim, SangHak Yi, Kun Ho Lee, Sungho Won

Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to
assess cognitive functions such as visuospatial skills and memory, making them
valuable tools for detecting mild cognitive impairment (MCI). Despite their
utility, existing predictive models based on these tests often suffer from
limitations like small sample sizes and lack of external validation, which
undermine their reliability. We developed a multi-stream deep learning
framework that integrates two distinct processing streams: a multi-head
self-attention based spatial stream using raw RCFT images and a scoring stream
employing a previously developed automated scoring system. Our model was
trained on data from 1,740 subjects in the Korean cohort and validated on an
external hospital dataset of 222 subjects from Korea. The proposed multi-stream
model demonstrated superior performance over baseline models (AUC = 0.872,
Accuracy = 0.781) in external validation. The integration of both spatial and
scoring streams enables the model to capture intricate visual details from the
raw images while also incorporating structured scoring data, which together
enhance its ability to detect subtle cognitive impairments. This dual approach
not only improves predictive accuracy but also increases the robustness of the
model, making it more reliable in diverse clinical settings. Our model has
practical implications for clinical settings, where it could serve as a
cost-effective tool for early MCI screening.

æè¦ï¼é·æ°è¤éåå½¢æ¸¬é© (RCFT) ç­ç¹ªç«æ¸¬é©å»£æ³ç¨æ¼è©ä¼°è¦è¦ºç©ºéæè½åè¨æ¶åç­èªç¥åè½ï¼ä½¿å¶æçºæª¢æ¸¬è¼åº¦èªç¥éç¤ (MCI) çå¯¶è²´å·¥å·ãåç®¡å®åå¾æç¨ï¼ä½åºæ¼éäºæ¸¬é©çç¾æé æ¸¬æ¨¡åéå¸¸æåå°æ¨£æ¬éå°åç¼ºä¹å¤é¨é©è­ç­éå¶ï¼éææå®³å¶å¯é æ§ãæåéç¼äºä¸åå¤ä¸²æµæ·±åº¦å­¸ç¿æ¡æ¶ï¼å®æ´åäºå©åä¸åçèçä¸²æµï¼ä¸ååºæ¼å¤é ­èªæ³¨æåï¼ä½¿ç¨åå§ RCFT å½±åçç©ºéä¸²æµï¼ä»¥åä¸åæ¡ç¨ååéç¼çèªåè©åç³»çµ±çè©åä¸²æµãæåçæ¨¡åå¨éåç¾¤çµä¸­ 1,740 ååè©¦èçè³æä¸é²è¡è¨ç·´ï¼ä¸¦å¨ä¾èªéåç 222 ååè©¦èçå¤é¨é«é¢è³æéä¸é²è¡é©è­ãææåºçå¤ä¸²æµæ¨¡åå¨å¤é¨é©è­ä¸­è¡¨ç¾åºåªæ¼åºæºæ¨¡åçæè½ (AUC = 0.872ï¼æºç¢ºç = 0.781)ãç©ºéåè©åä¸²æµçæ´åä½¿æ¨¡åè½å¤ å¾åå§å½±åæ·åè¤éçè¦è¦ºç´°ç¯ï¼åæä¹è½ç´å¥çµæ§åçè©åè³æï¼éå±åå¢å¼·äºå®æª¢æ¸¬ç´°å¾®èªç¥éç¤çè½åãéç¨®ééæ¹æ³ä¸åæé«äºé æ¸¬æºç¢ºæ§ï¼ä¹å¢å äºæ¨¡åçç©©å¥æ§ï¼ä½¿å¶å¨ä¸åçè¨åºç°å¢ä¸­æ´å¯é ãæåçæ¨¡åå°è¨åºç°å¢æå¯¦éçæç¾©ï¼å®å¯ä»¥å¨å¶ä¸­ä½çºæ©æ MCI ç¯©æª¢çå·ææ¬æççå·¥å·ã

##### **Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**
2409.02681v1 by Ramon Tavares

This study presents a comprehensive methodology for modeling and forecasting
the historical time series of fire spots detected by the AQUA_M-T satellite in
the Amazon, Brazil. The approach utilizes a mixed Recurrent Neural Network
(RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit
(GRU) architectures to predict monthly accumulations of daily detected fire
spots. A summary of the data revealed a consistent seasonality over time, with
annual maximum and minimum fire spot values tending to repeat at the same
periods each year. The primary objective is to verify whether the forecasts
capture this inherent seasonality through rigorous statistical analysis. The
methodology involved careful data preparation, model configuration, and
training using cross-validation with two seeds, ensuring that the data
generalizes well to the test and validation sets, and confirming the
convergence of the model parameters. The results indicate that the mixed LSTM
and GRU model offers improved accuracy in forecasting 12 months ahead,
demonstrating its effectiveness in capturing complex temporal patterns and
modeling the observed time series. This research significantly contributes to
the application of deep learning techniques in environmental monitoring,
specifically in fire spot forecasting. In addition to improving forecast
accuracy, the proposed approach highlights the potential for adaptation to
other time series forecasting challenges, opening new avenues for research and
development in machine learning and natural phenomenon prediction. Keywords:
Time Series Forecasting, Recurrent Neural Networks, Deep Learning.

æè¦ï¼æ¬ç ç©¶æåºäºä¸åå¨é¢çæ¹æ³ï¼ç¨æ¼å»ºæ¨¡åé æ¸¬å·´è¥¿äºé¦¬éå°åç± AQUA_M-T è¡æåµæ¸¬å°çæ­·å²ç«ç½é»æéåºåãè©²æ¹æ³æ¡ç¨æ··åéè¿´ç¥ç¶ç¶²è·¯ (RNN) æ¨¡åï¼çµåé·ç­æè¨æ¶ (LSTM) åéæ§éè¿´å®å (GRU) æ¶æ§ï¼ä»¥é æ¸¬æ¯æ¥åµæ¸¬ç«ç½é»çæç´¯è¨å¼ãå°è³æçæè¦é¡¯ç¤ºåºé¨èæéæ¨ç§»èåºç¾çä¸è´å­£ç¯æ§ï¼æ¯å¹´çå¹´åº¦æå¤§åæå°ç«ç½é»å¼å¾åæ¼å¨åä¸ææéè¤åºç¾ãä¸»è¦ç®æ¨æ¯ééå´è¬¹ççµ±è¨åæé©è­é æ¸¬æ¯å¦ææå°éç¨®åºæçå­£ç¯æ§ãè©²æ¹æ³æ¶åä»ç´°çè³ææºåãæ¨¡åéç½®ï¼ä»¥åä½¿ç¨å©åç¨®å­çäº¤åé©è­é²è¡è¨ç·´ï¼ç¢ºä¿è³æè½å¾å¥½å°æ¨å»£å°æ¸¬è©¦åé©è­éï¼ä¸¦ç¢ºèªæ¨¡ååæ¸çæ¶ææ§ãçµæè¡¨æï¼æ··å LSTM å GRU æ¨¡åå¨é æ¸¬ 12 åæå¾æä¾äºæ´é«çæºç¢ºåº¦ï¼è­æäºå¶å¨ææè¤éæéæ¨¡å¼åå»ºæ¨¡è§æ¸¬æéåºåæ¹é¢çæææ§ãæ¬ç ç©¶é¡¯èå°ä¿è¿äºæ·±åº¦å­¸ç¿æè¡å¨ç°å¢ç£æ¸¬ä¸­çæç¨ï¼ç¹å¥æ¯å¨ç«ç½é»é æ¸¬æ¹é¢ãé¤äºæé«é æ¸¬æºç¢ºåº¦å¤ï¼ææåºçæ¹æ³éå¼·èª¿äºé©æå¶ä»æéåºåé æ¸¬ææ°çæ½åï¼çºæ©å¨å­¸ç¿åèªç¶ç¾è±¡é æ¸¬çç ç©¶åéç¼éé¢äºæ°çéå¾ãééµå­ï¼æéåºåé æ¸¬ãéè¿´ç¥ç¶ç¶²è·¯ãæ·±åº¦å­¸ç¿ã

##### **SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**
2409.02598v1 by Wenwu Guo, Jinlin Wu, Zhen Chen, Qingxiang Zhao, Miao Xu, Zhen Lei, Hongbin Liu

Vision-based surgical navigation has received increasing attention due to its
non-invasive, cost-effective, and flexible advantages. In particular, a
critical element of the vision-based navigation system is tracking surgical
instruments. Compared with 2D instrument tracking methods, 3D instrument
tracking has broader value in clinical practice, but is also more challenging
due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models
for 3D registration. To solve these challenges, we propose the SurgTrack, a
two-stage 3D instrument tracking method for CAD-free and robust real-world
applications. In the first registration stage, we incorporate an Instrument
Signed Distance Field (SDF) modeling the 3D representation of instruments,
achieving CAD-freed 3D registration. Due to this, we can obtain the location
and orientation of instruments in the 3D space by matching the video stream
with the registered SDF model. In the second tracking stage, we devise a
posture graph optimization module, leveraging the historical tracking results
of the posture memory pool to optimize the tracking results and improve the
occlusion robustness. Furthermore, we collect the Instrument3D dataset to
comprehensively evaluate the 3D tracking of surgical instruments. The extensive
experiments validate the superiority and scalability of our SurgTrack, by
outperforming the state-of-the-arts with a remarkable improvement. The code and
dataset are available at https://github.com/wenwucode/SurgTrack.

æè¦ï¼<paragraph>åºæ¼è¦è¦ºçå¤ç§å°èªç±æ¼å¶éä¾µå¥æ§ãææ¬æçåéæ´»æ§åªå¢èåå°è¶ä¾è¶å¤çéæ³¨ãç¹å¥æ¯ï¼åºæ¼è¦è¦ºçå°èªç³»çµ±çä¸åééµåç´ æ¯è¿½è¹¤æè¡å¨æ¢°ãè 2D å¨æ¢°è¿½è¹¤æ¹æ³ç¸æ¯ï¼3D å¨æ¢°è¿½è¹¤å¨è¨åºå¯¦åä¸­å·ææ´å»£æ³çå¹å¼ï¼ä½ç±æ¼ç´çå¼±ãé®æåç¼ºä¹ç¨æ¼ 3D éæºçé»è¦è¼å©è¨­è¨ (CAD) æ¨¡åï¼å æ­¤ä¹æ´å·ææ°æ§ãçºäºè§£æ±ºéäºææ°ï¼æåæåº SurgTrackï¼ä¸ç¨®é©ç¨æ¼ç¡ CAD åç©©å¥ççå¯¦ä¸çæç¨ç¨å¼çå©éæ®µ 3D å¨æ¢°è¿½è¹¤æ¹æ³ãå¨ç¬¬ä¸åéæºéæ®µï¼æåæ´åä¸åå¨æ¢°ç°½ç½²è·é¢å ´ (SDF)ï¼å°å¨æ¢°ç 3D è¡¨å¾µé²è¡å»ºæ¨¡ï¼å¯¦ç¾ç¡ CAD ç 3D éæºãå æ­¤ï¼æåå¯ä»¥ééå°è¦è¨ä¸²æµèå·²éæºç SDF æ¨¡åé²è¡å¹éï¼åå¾å¨æ¢°å¨ 3D ç©ºéä¸­çä½ç½®åæ¹åãå¨ç¬¬äºåè¿½è¹¤éæ®µï¼æåè¨­è¨ä¸åå§¿å¢åæä½³åæ¨¡çµï¼å©ç¨å§¿å¢è¨æ¶æ± çæ­·å²è¿½è¹¤çµæä¾æä½³åè¿½è¹¤çµæä¸¦æ¹åé®æçç©©å¥æ§ãæ­¤å¤ï¼æåæ¶é Instrument3D è³æéï¼ä»¥å¨é¢è©ä¼°æè¡å¨æ¢°ç 3D è¿½è¹¤ãå»£æ³çå¯¦é©é©è­äºæå SurgTrack çåªè¶æ§åå¯æ´åæ§ï¼ä»¥é¡¯èçæ¹é²åªæ¼ç¾ææè¡ãç¨å¼ç¢¼åè³æéå¯å¨ https://github.com/wenwucode/SurgTrack åå¾ã</paragraph>

##### **Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**
2409.02530v1 by Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang

The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of
kidney function in clinical practice. Although traditional equations and
Machine Learning (ML) models using clinical and laboratory data can estimate
eGFR, accurately predicting future eGFR levels remains a significant challenge
for nephrologists and ML researchers. Recent advances demonstrate that Large
Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust
foundation models for diverse applications. This study investigates the
potential of LMMs to predict future eGFR levels with a dataset consisting of
laboratory and clinical values from 50 patients. By integrating various
prompting techniques and ensembles of LMMs, our findings suggest that these
models, when combined with precise prompts and visual representations of eGFR
trajectories, offer predictive performance comparable to existing ML models.
This research extends the application of foundation models and suggests avenues
for future studies to harness these models in addressing complex medical
forecasting challenges.

æè¦ï¼ä¼°è¨çèå°çéæ¿¾ç (eGFR) æ¯è¨åºå¯¦åä¸­èèåè½çéè¦ææ¨ãéç¶å³çµ±æ¹ç¨å¼åä½¿ç¨è¨åºèå¯¦é©å®¤è³æçæ©å¨å­¸ç¿ (ML) æ¨¡åå¯ä»¥ä¼°è¨ eGFRï¼ä½æºç¢ºé æ¸¬æªä¾ eGFR æ°´å¹³ä»ç¶æ¯èèç§é«å¸«å ML ç ç©¶äººå¡çä¸å¤§ææ°ãæè¿çç ç©¶é²å±é¡¯ç¤ºï¼å¤§åèªè¨æ¨¡å (LLM) åå¤§åå¤æ¨¡ææ¨¡å (LMM) å¯ä»¥ä½çºåç¨®æç¨ç¨å¼çå¼·å¥åºç¤æ¨¡åãæ¬ç ç©¶æ¢è¨ LMM é æ¸¬æªä¾ eGFR æ°´å¹³çæ½åï¼å¶è³æéåå« 50 ä½çæ£çå¯¦é©å®¤åè¨åºæ¸å¼ãééæ´ååç¨®æç¤ºæè¡å LMM çåå¥ï¼æåçç ç©¶çµæé¡¯ç¤ºï¼éäºæ¨¡åå¨çµåç²¾ç¢ºæç¤ºå eGFR è»è·¡çè¦è¦ºåè¡¨ç¤ºæï¼å¯æä¾èç¾æ ML æ¨¡åç¸è¿çé æ¸¬æè½ãéé ç ç©¶æ´å±äºåºç¤æ¨¡åçæç¨ï¼ä¸¦çºæªä¾ç ç©¶å©ç¨éäºæ¨¡åä¾æå°è¤éçé«çé æ¸¬ææ°æä¾äºéå¾ã

##### **Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**
2409.02337v1 by Deepak Raina, Mythra V. Balakuntala, Byung Wook Kim, Juan Wachs, Richard Voyles

Ultrasound is widely employed for clinical intervention and diagnosis, due to
its advantages of offering non-invasive, radiation-free, and real-time imaging.
However, the accessibility of this dexterous procedure is limited due to the
substantial training and expertise required of operators. The robotic
ultrasound (RUS) offers a viable solution to address this limitation;
nonetheless, achieving human-level proficiency remains challenging. Learning
from demonstrations (LfD) methods have been explored in RUS, which learns the
policy prior from a dataset of offline demonstrations to encode the mental
model of the expert sonographer. However, active engagement of experts, i.e.
Coaching, during the training of RUS has not been explored thus far. Coaching
is known for enhancing efficiency and performance in human training. This paper
proposes a coaching framework for RUS to amplify its performance. The framework
combines DRL (self-supervised practice) with sparse expert's feedback through
coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a
reward based on image quality rating. The coaching by experts is modeled as a
Partially Observable Markov Decision Process (POMDP), which updates the policy
parameters based on the correction by the expert. The validation study on
phantoms showed that coaching increases the learning rate by $25\%$ and the
number of high-quality image acquisition by $74.5\%$.

æè¦ï¼è¶é³æ³¢å å¶æä¾éä¾µå¥æ§ãç¡è¼»å°ä¸å³æå½±åçåªé»ï¼èå»£æ³ç¨æ¼è¨åºä»å¥åè¨ºæ·ã
ç¶èï¼ç±æ¼æä½å¡éè¦å¤§éçè¨ç·´åå°æ¥­ç¥è­ï¼éå¶äºæ­¤éæ´»ç¨åºçå¯åæ§ãæ©å¨äººè¶é³æ³¢ (RUS) æä¾äºä¸åå¯è¡çè§£æ±ºæ¹æ¡ä¾è§£æ±ºæ­¤éå¶ï¼
åç®¡å¦æ­¤ï¼è¦éå°äººé¡ç­ç´ççç·´åº¦ä»ç¶å·æææ°æ§ãå­¸ç¿ç¤ºç¯ (LfD) æ¹æ³å·²å¨ RUS ä¸­é²è¡æ¢è¨ï¼å®å¾é¢ç·ç¤ºç¯çè³æéå­¸ç¿åé©ç­ç¥ï¼ä»¥ç·¨ç¢¼å°å®¶è¶é³æ³¢æª¢æ¥å¡çå¿æºæ¨¡åãç¶èï¼è¿ä»å°æªæ¢è¨å°å®¶å¨ RUS è¨ç·´æéçç©æ¥µåèï¼å³æå°ãæå°å·²ç¥å¯ä»¥æé«äººé¡è¨ç·´çæçåç¸¾æãæ¬ææåºäºä¸å RUS æå°æ¶æ§ï¼ä»¥æåå¶ç¸¾æãæ­¤æ¶æ§çµåäº DRLï¼èªæç£ç£å¯¦åï¼èééæå°æä¾çå°å®¶ç¨çåé¥ãDRL ä½¿ç¨é¢ç·ç­ç¥è»æ§åä½-è©è« (SAC) ç¶²è·¯ï¼ä¸¦æ ¹æå½±ååè³ªè©åçµ¦äºçåµãå°å®¶çæå°è¢«å»ºæ¨¡çºé¨åå¯è§å¯é¦¬å¯å¤«æ±ºç­éç¨ (POMDP)ï¼å®æ ¹æå°å®¶çä¿®æ­£ä¾æ´æ°ç­ç¥åæ¸ãå¨æ¨¡æ¬äººé«æ¨¡åä¸çé©è­ç ç©¶é¡¯ç¤ºï¼æå°å°å­¸ç¿çæé«äº $25\%$ï¼é«åè³ªå½±åæ·åæ¸éæé«äº $74.5\%$ã

##### **Action-Based ADHD Diagnosis in Video**
2409.02261v1 by Yichun Li, Yuxing Yang, Syed Nohsen Naqvi

Attention Deficit Hyperactivity Disorder (ADHD) causes significant impairment
in various domains. Early diagnosis of ADHD and treatment could significantly
improve the quality of life and functioning. Recently, machine learning methods
have improved the accuracy and efficiency of the ADHD diagnosis process.
However, the cost of the equipment and trained staff required by the existing
methods are generally huge. Therefore, we introduce the video-based frame-level
action recognition network to ADHD diagnosis for the first time. We also record
a real multi-modal ADHD dataset and extract three action classes from the video
modality for ADHD diagnosis. The whole process data have been reported to
CNTW-NHS Foundation Trust, which would be reviewed by medical
consultants/professionals and will be made public in due course.

æè¦ï¼æ³¨æåç¼ºé·éåç (ADHD) æå¨åç¨®é åé æé¡¯èçæå®³ãææ©è¨ºæ· ADHD ä¸¦æ¥åæ²»çå¯ä»¥å¤§å¹æ¹åçæ´»åè³ªååè½ãæè¿ï¼æ©å¨å­¸ç¿æ¹æ³å·²ç¶æåäº ADHD è¨ºæ·ç¨åºçæºç¢ºåº¦åæçãç¶èï¼ç¾ææ¹æ³æéçè¨­ååè¨ç·´æç´ çäººå¡ææ¬éå¸¸å¾é«ãå æ­¤ï¼æåé¦æ¬¡å°åºæ¼å½±ççå¹ç´åä½è¾¨è­ç¶²è·¯å¼å¥ ADHD è¨ºæ·ãæåä¹è¨éäºä¸åçæ­£çå¤æ¨¡å¼ ADHD è³æéï¼ä¸¦å¾å½±çæ¨¡å¼ä¸­èååºä¸ååä½é¡å¥ä»¥é²è¡ ADHD è¨ºæ·ãæ´åæµç¨çè³æå·²ç¶åå ±çµ¦ CNTW-NHS åºéæï¼å°ç±é«çé¡§å/å°æ¥­äººå£«å¯©æ¥ï¼ä¸¦å°é©æå¬éã

##### **A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**
2409.02069v1 by Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy

Dental disease is a prevalent chronic condition associated with substantial
financial burden, personal suffering, and increased risk of systemic diseases.
Despite widespread recommendations for twice-daily tooth brushing, adherence to
recommended oral self-care behaviors remains sub-optimal due to factors such as
forgetfulness and disengagement. To address this, we developed Oralytics, a
mHealth intervention system designed to complement clinician-delivered
preventative care for marginalized individuals at risk for dental disease.
Oralytics incorporates an online reinforcement learning algorithm to determine
optimal times to deliver intervention prompts that encourage oral self-care
behaviors. We have deployed Oralytics in a registered clinical trial. The
deployment required careful design to manage challenges specific to the
clinical trials setting in the U.S. In this paper, we (1) highlight key design
decisions of the RL algorithm that address these challenges and (2) conduct a
re-sampling analysis to evaluate algorithm design decisions. A second phase
(randomized control trial) of Oralytics is planned to start in spring 2025.

æè¦ï¼çç§ç¾çæ¯ä¸ç¨®æ®éçæ¢æ§ç¾çï¼èå¤§éçç¶æ¿è² æãåäººçè¦åå¢å çå¨èº«ç¾çé¢¨éªæéãåç®¡æ®éå»ºè­°æ¯å¤©å·çå©æ¬¡ï¼ä½ç±æ¼å¥å¿åè«é¢ç­å ç´ ï¼å°å»ºè­°çå£èèªæä¿å¥è¡çºçä¾å¾æ§ä»ç¶ä½æ¼æä½³æ°´å¹³ãçºäºè§£æ±ºéååé¡ï¼æåéç¼äº Oralyticsï¼ä¸å mHealth ä»å¥ç³»çµ±ï¼æ¨å¨è£åè¨åºé«çæä¾çé é²ä¿å¥ï¼ä»¥é é²æçç§ç¾çé¢¨éªçéç·£ååäººãOralytics çµåäºä¸åå¨ç·å¼·åå­¸ç¿æ¼ç®æ³ï¼ä»¥ç¢ºå®æä¾ä»å¥æç¤ºçæä½³æéï¼éäºæç¤ºé¼åµå£èèªæä¿å¥è¡çºãæåå·²å¨è¨»åçè¨åºè©¦é©ä¸­é¨ç½²äº Oralyticsãè©²é¨ç½²éè¦ä»ç´°çè¨­è¨ä¾ç®¡çç¾åè¨åºè©¦é©è¨­ç½®ä¸­å·é«çææ°ãå¨æ¬æä¸­ï¼æåï¼1ï¼éé»ä»ç´¹äºè§£æ±ºéäºææ°ç RL æ¼ç®æ³çééµè¨­è¨æ±ºç­ï¼ä»¥åï¼2ï¼é²è¡éæ°æ½æ¨£åæä»¥è©ä¼°æ¼ç®æ³è¨­è¨æ±ºç­ãOralytics çç¬¬äºéæ®µï¼é¨æ©å°ç§è©¦é©ï¼è¨åæ¼ 2025 å¹´æ¥å­£éå§ã

##### **TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**
2409.02018v1 by Bobby Azad, Pourya Adibfar, Kaiqun Fu

In healthcare, medical image segmentation is crucial for accurate disease
diagnosis and the development of effective treatment strategies. Early
detection can significantly aid in managing diseases and potentially prevent
their progression. Machine learning, particularly deep convolutional neural
networks, has emerged as a promising approach to addressing segmentation
challenges. Traditional methods like U-Net use encoding blocks for local
representation modeling and decoding blocks to uncover semantic relationships.
However, these models often struggle with multi-scale objects exhibiting
significant variations in texture and shape, and they frequently fail to
capture long-range dependencies in the input data. Transformers designed for
sequence-to-sequence predictions have been proposed as alternatives, utilizing
global self-attention mechanisms. Yet, they can sometimes lack precise
localization due to insufficient granular details. To overcome these
limitations, we introduce TransDAE: a novel approach that reimagines the
self-attention mechanism to include both spatial and channel-wise associations
across the entire feature space, while maintaining computational efficiency.
Additionally, TransDAE enhances the skip connection pathway with an inter-scale
interaction module, promoting feature reuse and improving localization
accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on
the Synaps multi-organ dataset, even without relying on pre-trained weights.

æè¦ï¼å¨å»çä¿å¥é¢åï¼å»å­¦å½±ååå²å¯¹äºåç¡®çç¾çè¯æ­åæææ²»çç­ç¥çå¼åè³å³éè¦ãæ©ææ£æµå¯ä»¥æå¤§å°å¸®å©æ§å¶ç¾çï¼å¹¶å¯è½é²æ­¢ç¾çè¿å±ãæºå¨å­¦ä¹ ï¼å°¤å¶æ¯æ·±åº¦å·ç§¯ç¥ç»ç½ç»ï¼å·²æä¸ºè§£å³åå²ææçä¸ç§æåéçæ¹æ³ãU-Net ç­ä¼ ç»æ¹æ³ä½¿ç¨ç¼ç åè¿è¡å±é¨è¡¨ç¤ºå»ºæ¨¡åè§£ç åæ¥æ­ç¤ºè¯­ä¹å³ç³»ãç¶èï¼è¿äºæ¨¡åéå¸¸é¾ä»¥å¤çå¨çº¹çåå½¢ç¶ä¸è¡¨ç°åºæ¾çååçå¤å°ºåº¦å¯¹è±¡ï¼å¹¶ä¸å®ä»¬ç»å¸¸æ æ³æè·è¾å¥æ°æ®ä¸­çè¿ç¨ä¾èµå³ç³»ãä¸ä¸ºåºåå°åºåé¢æµèè®¾è®¡ç Transformer å·²è¢«æåºä½ä¸ºæ¿ä»£æ¹æ¡ï¼å©ç¨å¨å±èªæ³¨æåæºå¶ãç¶èï¼ç±äºç²åº¦ç»èä¸è¶³ï¼å®ä»¬ææ¶å¯è½ç¼ºä¹ç²¾ç¡®çå®ä½ãä¸ºäºåæè¿äºéå¶ï¼æä»¬å¼å¥äº TransDAEï¼ä¸ç§æ°é¢çæ¹æ³ï¼å®éæ°ææ³äºèªæ³¨æåæºå¶ï¼ä»¥åå«æ´ä¸ªç¹å¾ç©ºé´ä¸­çç©ºé´åééå³èï¼åæ¶ä¿æè®¡ç®æçãæ­¤å¤ï¼TransDAE éè¿å°ºåº¦é´äº¤äºæ¨¡åå¢å¼ºäºè·³è·è¿æ¥è·¯å¾ï¼ä¿è¿äºç¹å¾éç¨å¹¶æé«äºå®ä½ç²¾åº¦ãå¼å¾æ³¨æçæ¯ï¼å³ä½¿ä¸ä¾èµé¢è®­ç»æéï¼TransDAE å¨ Synaps å¤å¨å®æ°æ®éä¸ä¹ä¼äºç°æçæåè¿æ¹æ³ã

##### **A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**
2409.01903v1 by Abdelmalek Mouazer, Sophie Dubois, Romain LÃ©guillon, Nada Boudegzdame, Thibaud Levrard, Yoann Le Bars, Christian Simon, Brigitte SÃ©roussi, Julien Grosjean, Romain Lelong, Catherine Letord, StÃ©fan Darmoni, Karima Sedki, Pierre Meneton, Rosy Tsopra, Hector Falcoff, Jean-Baptiste Lamy

Background: Medication review is a structured interview of the patient,
performed by the pharmacist and aimed at optimizing drug treatments. In
practice, medication review is a long and cognitively-demanding task that
requires specific knowledge. Clinical practice guidelines have been proposed,
but their application is tedious. Methods: We designed ABiMed, a clinical
decision support system for medication reviews, based on the implementation of
the STOPP/START v2 guidelines and on the visual presentation of aggregated drug
knowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39
community pharmacists during a randomized simulation trial, each pharmacist
performing a medication review for two fictitious patients without ABiMed, and
two others with ABiMed. We recorded the problems identified by the pharmacists,
the interventions proposed, the response time, the perceived usability and the
comments. Pharmacists' medication reviews were compared to an expert-designed
gold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant
drug-related problems during the medication review (p=1.1e-12) and proposed
better interventions (p=9.8e-9), without needing more time (p=0.56). The System
Usability Scale score is 82.7, which is ranked "excellent". In their comments,
pharmacists appreciated the visual aspect of ABiMed and its ability to compare
the current treatment with the proposed one. A multifactor analysis showed no
difference in the support offered by ABiMed according to the pharmacist's age
or sex, in terms of percentage of problems identified or quality of the
proposed interventions. Conclusions: The use of an intelligent and visual
clinical decision support system can help pharmacists when they perform
medication reviews. Our main perspective is the validation of the system in
clinical conditions.

æè¦ï¼<paragraph>èæ¯ï¼ç¨è¥å¯©æ¥æ¯ç±è¥å¸«å·è¡çä¸ç¨®çµæ§åæ£èè¨ªè«ï¼ç®çå¨æ¼åªåè¥ç©æ²»çãå¨å¯¦åä¸ï¼ç¨è¥å¯©æ¥æ¯ä¸é åé·ä¸èªç¥éæ±é«çä»»åï¼éè¦å·åç¹å®ç¥è­ãéç¶å·²æåºè¨åºå¯¦åæå¼ï¼ä½å¶æç¨å¾ç¹ç£ãæ¹æ³ï¼æåæ ¹æ STOPP/START v2 æå¼çå¯¦ä½ï¼ä¸¦ä½¿ç¨è¡¨æ ¼ãåè¡¨åè±å½¢ç¬¦èè¦è¦ºååç¾å½æ´çè¥ç©ç¥è­ï¼è¨­è¨äºä¸å¥ç¨è¥å¯©æ¥çè¨åºæ±ºç­æ¯æ´ç³»çµ± ABiMedãæåå¨é¨æ©æ¨¡æ¬è©¦é©ä¸­ï¼è® 39 ä½ç¤¾åè¥å¸«è©ä¼° ABiMedï¼æ¯ä½è¥å¸«éå°å©ä½èæ§æ£èå·è¡ç¨è¥å¯©æ¥ï¼å©æ¬¡æ²æä½¿ç¨ ABiMedï¼å©æ¬¡ä½¿ç¨ ABiMedãæåè¨éäºè¥å¸«è­å¥åºçåé¡ãå»ºè­°çä»å¥æªæ½ãåææéãæç¥å¯ç¨æ§åè©è«ãå°è¥å¸«çç¨è¥å¯©æ¥èå°å®¶è¨­è¨çéæ¨æºé²è¡æ¯è¼ãçµæï¼ä½¿ç¨ ABiMed å¾ï¼è¥å¸«å¨ç¨è¥å¯©æ¥æéç¼ç¾äºå¤ 1.6 åç¸éçè¥ç©ç¸éåé¡ï¼p=1.1e-12ï¼ï¼ä¸¦æåºæ´å¥½çä»å¥æªæ½ï¼p=9.8e-9ï¼ï¼èç¡éè±è²»æ´å¤æéï¼p=0.56ï¼ãç³»çµ±å¯ç¨æ§è©åçº 82.7ï¼è¢«è©çºãåªè¯ããå¨ä»åçè©è«ä¸­ï¼è¥å¸«è®è³ ABiMed çè¦è¦ºåé¢åï¼ä»¥åå®æ¯è¼ç®åæ²»çèå»ºè­°æ²»ççè½åãå¤å ç´ åæé¡¯ç¤ºï¼ABiMed æä¾çæ¯æ´å¨è¥å¸«çå¹´é½¡ææ§å¥æ¹é¢æ²æå·®ç°ï¼å°±è­å¥åºçåé¡ç¾åæ¯æå»ºè­°ä»å¥æªæ½çåè³ªèè¨ãçµè«ï¼ä½¿ç¨æºæ§ä¸è¦è¦ºåçè¨åºæ±ºç­æ¯æ´ç³»çµ±ï¼å¯ä»¥åå©è¥å¸«å·è¡ç¨è¥å¯©æ¥ãæåçè§é»ä¸»è¦æ¯é©è­ç³»çµ±å¨è¨åºæ¢ä»¶ä¸çæåº¦ã</paragraph>

##### **Training on the Benchmark Is Not All You Need**
2409.01790v1 by Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang

The success of Large Language Models (LLMs) relies heavily on the huge amount
of pre-training data learned in the pre-training phase. The opacity of the
pre-training process and the training data causes the results of many benchmark
tests to become unreliable. If any model has been trained on a benchmark test
set, it can seriously hinder the health of the field. In order to automate and
efficiently test the capabilities of large language models, numerous mainstream
benchmarks adopt a multiple-choice format. As the swapping of the contents of
multiple-choice options does not affect the meaning of the question itself, we
propose a simple and effective data leakage detection method based on this
property. Specifically, we shuffle the contents of the options in the data to
generate the corresponding derived data sets, and then detect data leakage
based on the model's log probability distribution over the derived data sets.
If there is a maximum and outlier in the set of log probabilities, it indicates
that the data is leaked. Our method is able to work under black-box conditions
without access to model training data or weights, effectively identifying data
leakage from benchmark test sets in model pre-training data, including both
normal scenarios and complex scenarios where options may have been shuffled
intentionally or unintentionally. Through experiments based on two LLMs and
benchmark designs, we demonstrate the effectiveness of our method. In addition,
we evaluate the degree of data leakage of 31 mainstream open-source LLMs on
four benchmark datasets and give a ranking of the leaked LLMs for each
benchmark, and we find that the Qwen family of LLMs has the highest degree of
data leakage.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çæåå¨å¾å¤§ç¨åº¦ä¸åæ±ºæ¼é è¨ç·´éæ®µä¸­å­¸ç¿å°çæµ·éé è¨ç·´æ¸æãé è¨ç·´éç¨åè¨ç·´æ¸æçä¸éææ§å°è´è¨±å¤åºæºæ¸¬è©¦ççµæè®å¾ä¸å¯é ãå¦æä»»ä½æ¨¡åå·²å¨åºæºæ¸¬è©¦éä¸­é²è¡è¨ç·´ï¼åå¯è½æå´éé»ç¤è©²é åçç¼å±ãçºäºèªååä¸ææå°æ¸¬è©¦å¤§åèªè¨æ¨¡åçè½åï¼è¨±å¤ä¸»æµåºæºæ¡ç¨å¤é¸é¡æ ¼å¼ãç±æ¼å¤é¸é¡é¸é å§å®¹çäºæä¸å½±é¿åé¡æ¬èº«çå«ç¾©ï¼å æ­¤æåæåºäºä¸ç¨®åºæ¼æ­¤å±¬æ§çç°¡å®ä¸ææçæ°æ®æ´©æ¼æª¢æ¸¬æ¹æ³ãå·é«ä¾èªªï¼æåå°æ¸æä¸­é¸é çå§å®¹é¨æ©æåä»¥çæå°æçæ´¾çæ¸æéï¼ç¶å¾æ ¹ææ¨¡åå¨æ´¾çæ¸æéä¸çå°æ¸æ¦çåä½æª¢æ¸¬æ¸ææ´©æ¼ãå¦æå°æ¸æ¦çéä¸­å­å¨æå¤§å¼åç°å¸¸å¼ï¼åè¡¨ç¤ºæ¸æå·²æ´©æ¼ãæåçæ¹æ³è½å¤ å¨ä¸è¨ªåæ¨¡åè¨ç·´æ¸æææ¬éçé»çæ¢ä»¶ä¸å·¥ä½ï¼ææå°è­å¥æ¨¡åé è¨ç·´æ¸æä¸­åºæºæ¸¬è©¦éçæ¸ææ´©æ¼ï¼åæ¬é¸é å¯è½å·²æææç¡æå°è¢«æäºçæ­£å¸¸å ´æ¯åè¤éå ´æ¯ãééåºæ¼å©å LLM ååºæºè¨­è¨çå¯¦é©ï¼æåè­æäºæåæ¹æ³çæææ§ãæ­¤å¤ï¼æåè©ä¼°äº 31 åä¸»æµéæº LLM å¨åååºæºæ¸æéä¸çæ¸ææ´©æ¼ç¨åº¦ï¼ä¸¦å°æ¯ååºæºçæ´©æ¼ LLM é²è¡äºæåï¼æåç¼ç¾ Qwen å®¶æç LLM å·ææé«çæ¸ææ´©æ¼ç¨åº¦ã

##### **Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**
2409.01676v1 by Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink

Deriving health indicators of rotating machines is crucial for their
maintenance. However, this process is challenging for the prevalent adopted
intelligent methods since they may take the whole data distributions, not only
introducing noise interference but also lacking the explainability. To address
these issues, we propose a diffusion-based weakly-supervised approach for
deriving health indicators of rotating machines, enabling early fault detection
and continuous monitoring of condition evolution. This approach relies on a
classifier-free diffusion model trained using healthy samples and a few
anomalies. This model generates healthy samples. and by comparing the
differences between the original samples and the generated ones in the envelope
spectrum, we construct an anomaly map that clearly identifies faults. Health
indicators are then derived, which can explain the fault types and mitigate
noise interference. Comparative studies on two cases demonstrate that the
proposed method offers superior health monitoring effectiveness and robustness
compared to baseline models.

æè¦ï¼æ¨å°æè½æ©å¨çå¥åº·ææ¨å°æ¼å¶ç¶­è­·è³ééè¦ãç¶èï¼éåéç¨å°æ®éæ¡ç¨çæºè½æ¹æ³ä¾èªªå·æææ°æ§ï¼å çºå®åå¯è½ææ¡ç¨æ´åè³æåä½ï¼ä¸åæå¼å¥éè¨å¹²æ¾ï¼èä¸ç¼ºä¹å¯è§£éæ§ãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäºä¸ç¨®åºæ¼æ´æ£çå¼±ç£ç£å¼æ¹æ³ï¼ç¨æ¼æ¨å°æè½æ©å¨çå¥åº·ææ¨ï¼å¯¦ç¾æ©ææéæª¢æ¸¬åçææ¼è®çæçºç£æ§ãéç¨®æ¹æ³ä¾è³´æ¼ä½¿ç¨å¥åº·æ¨£æ¬åä¸äºç°å¸¸å¼è¨ç·´çç¡åé¡å¨æ´æ£æ¨¡åãéåæ¨¡åæç¢çå¥åº·æ¨£æ¬ãä¸¦ä¸ééæ¯è¼å°å¥è­ä¸­åå§æ¨£æ¬åçææ¨£æ¬ä¹éçå·®ç°ï¼æåæ§å»ºäºä¸åç°å¸¸åï¼å¯ä»¥æ¸æ¥å°è­å¥æéãç¶å¾æ¨å°åºå¥åº·ææ¨ï¼å¯ä»¥è§£éæéé¡åä¸¦æ¸è¼éè¨å¹²æ¾ãå°å©åæ¡ä¾çæ¯è¼ç ç©¶è¡¨æï¼èåºæºæ¨¡åç¸æ¯ï¼ææåºçæ¹æ³æä¾äºåè¶çå¥åº·ç£æ§æææ§åé­¯æ£æ§ã

##### **A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**
2409.02145v1 by Zekang Yang, Hong Liu, Xiangdong Wang

Computer-aided cancer survival risk prediction plays an important role in the
timely treatment of patients. This is a challenging weakly supervised ordinal
regression task associated with multiple clinical factors involved such as
pathological images, genomic data and etc. In this paper, we propose a new
training method, multimodal object-level contrast learning, for cancer survival
risk prediction. First, we construct contrast learning pairs based on the
survival risk relationship among the samples in the training sample set. Then
we introduce the object-level contrast learning method to train the survival
risk predictor. We further extend it to the multimodal scenario by applying
cross-modal constrast. Considering the heterogeneity of pathological images and
genomics data, we construct a multimodal survival risk predictor employing
attention-based and self-normalizing based nerural network respectively.
Finally, the survival risk predictor trained by our proposed method outperforms
state-of-the-art methods on two public multimodal cancer datasets for survival
risk prediction.

æè¦ï¼é»è¦è¼å©ççå­æ´»é¢¨éªé æ¸¬å¨çæ£çåææ²»çä¸­æ®æ¼èéè¦çè§è²ãéæ¯ä¸åå°é£çå¼±ç£ç£åºæ¸åæ­¸ä»»åï¼èå¤éè¨åºå ç´ æéï¼ä¾å¦ççååãåºå çµæ¸æç­ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°çè¨ç·´æ¹æ³ï¼å¤æ¨¡æç©ä»¶å±¤ç´å°æ¯å­¸ç¿ï¼ç¨æ¼ççå­æ´»é¢¨éªé æ¸¬ãé¦åï¼æåæ ¹æè¨ç·´æ¨£æ¬éä¸­æ¨£æ¬ä¹éçå­æ´»é¢¨éªéä¿å»ºç«å°æ¯å­¸ç¿å°ãæ¥èï¼æåå¼å¥ç©ä»¶å±¤ç´å°æ¯å­¸ç¿æ¹æ³ä¾è¨ç·´å­æ´»é¢¨éªé æ¸¬å¨ãæåé²ä¸æ­¥å°å¶å»¶ä¼¸è³å¤æ¨¡æå ´æ¯ï¼ééæç¨è·¨æ¨¡æå°æ¯ãèéå°ççååååºå é«æ¸æçç°è³ªæ§ï¼æååå¥æ¡ç¨åºæ¼æ³¨æåçåèªæ¨æºåçç¥ç¶ç¶²è·¯ä¾å»ºæ§å¤æ¨¡æå­æ´»é¢¨éªé æ¸¬å¨ãæå¾ï¼æåæåºçæ¹æ³æè¨ç·´çå­æ´»é¢¨éªé æ¸¬å¨å¨å©åå¬éçå¤æ¨¡æççè³æéä¸ï¼å¨å­æ´»é¢¨éªé æ¸¬æ¹é¢åªæ¼æåé²çæ¹æ³ã

##### **A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**
2409.01596v1 by Ruben D. Fonnegra, Maria Liliana HernÃ¡ndez, Juan C. Caicedo, Gloria M. DÃ­az

Contrast-enhancement pattern analysis is critical in breast magnetic
resonance imaging (MRI) to distinguish benign from probably malignant tumors.
However, contrast-enhanced image acquisitions are time-consuming and very
expensive. As an alternative to physical acquisition, this paper proposes a
comprehensive pipeline for the generation of accurate long-term (late)
contrast-enhanced breast MRI from the early counterpart. The proposed strategy
focuses on preserving the contrast agent pattern in the enhanced regions while
maintaining visual properties in the entire synthesized images. To that end, a
novel loss function that leverages the biological behavior of contrast agent
(CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed
to optimize a pixel-attention based generative model. In addition, unlike
traditional normalization and standardization methods, we developed a new
normalization strategy that maintains the contrast enhancement pattern across
the image sequences at multiple timestamps. This ensures the prevalence of the
CA pattern after image preprocessing, unlike conventional approaches.
Furthermore, in order to objectively evaluate the clinical quality of the
synthesized images, two metrics are also introduced to measure the differences
between the TI curves of enhanced regions of the acquired and synthesized
images. The experimental results showed that the proposed strategy generates
images that significantly outperform diagnostic quality in contrast-enhanced
regions while maintaining the spatial features of the entire image. This
results suggest a potential use of synthetic late enhanced images generated via
deep learning in clinical scenarios.

æè¦ï¼å°æ¯å¢å¼·æ¨¡å¼åæå¨ä¹³æ¿ç£å±æ¯å½±å (MRI) ä¸­è³ééè¦ï¼å¯ç¨æ¼ååè¯æ§è«ç¤åå¯è½æ¯æ¡æ§è«ç¤ã
ç¶èï¼å°æ¯å¢å¼·å½±åçæ·åéå¸¸èæä¸æè²´ãä½çºç©çæ·åçæ¿ä»£æ¹æ¡ï¼æ¬ææåºäºä¸åå¨é¢çç®¡éï¼ç¨æ¼å¾æ©æå°æç©çææºç¢ºçé·æï¼ææï¼å°æ¯å¢å¼·ä¹³æ¿ MRIãææåºçç­ç¥èéæ¼å¨å¢å¼·ååä¸­ä¿çå°æ¯åæ¨¡å¼ï¼åæå¨æ´ååæå½±åä¸­ç¶­æè¦è¦ºå±¬æ§ãçºæ­¤ï¼æåºäºä¸ç¨®æ°ç©çæå¤±å½æ¸ï¼å©ç¨å°æ¯å (CA) å¨çµç¹ä¸­ççç©è¡çºï¼ç±æéå¼·åº¦ (TI) å¢å¼·æ²ç·çµ¦åºï¼ï¼ä»¥æä½³ååºæ¼åç´ æ³¨æåççææ¨¡åãæ­¤å¤ï¼èå³çµ±çæ­£è¦ååæ¨æºåæ¹æ³ä¸åï¼æåéç¼äºä¸ç¨®æ°çæ­£è¦åç­ç¥ï¼å¯å¨å¤åæéæ³çå½±ååºåä¸­ç¶­æå°æ¯å¢å¼·æ¨¡å¼ãéç¢ºä¿äºå½±ååèçå¾ CA æ¨¡å¼çæ®éæ§ï¼éèå³çµ±æ¹æ³ä¸åãæ­¤å¤ï¼çºäºå®¢è§è©ä¼°åæå½±åçè¨åºåè³ªï¼éå¼å¥äºå©åææ¨ä¾æ¸¬éæ·åååæå½±åçå¢å¼·ååç TI æ²ç·ä¹éçå·®ç°ãå¯¦é©çµæé¡¯ç¤ºï¼ææåºçç­ç¥ç¢ççå½±åå¨å°æ¯å¢å¼·ååä¸­çè¨ºæ·åè³ªæé¡¯åªæ¼å¶ä»å½±åï¼åæç¶­æäºæ´åå½±åçç©ºéç¹å¾µãéäºçµæè¡¨æï¼å¨è¨åºå ´æ¯ä¸­ï¼ééæ·±åº¦å­¸ç¿çæçåæææå¢å¼·å½±åå·ææ½å¨ç¨éã

##### **Kvasir-VQA: A Text-Image Pair GI Tract Dataset**
2409.01437v1 by Sushant Gautam, Andrea StorÃ¥s, Cise Midoglu, Steven A. Hicks, Vajira Thambawita, PÃ¥l Halvorsen, Michael A. Riegler

We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and
Kvasir-Instrument datasets, augmented with question-and-answer annotations to
facilitate advanced machine learning tasks in Gastrointestinal (GI)
diagnostics. This dataset comprises 6,500 annotated images spanning various GI
tract conditions and surgical instruments, and it supports multiple question
types including yes/no, choice, location, and numerical count. The dataset is
intended for applications such as image captioning, Visual Question Answering
(VQA), text-based generation of synthetic medical images, object detection, and
classification. Our experiments demonstrate the dataset's effectiveness in
training models for three selected tasks, showcasing significant applications
in medical image analysis and diagnostics. We also present evaluation metrics
for each task, highlighting the usability and versatility of our dataset. The
dataset and supporting artifacts are available at
https://datasets.simula.no/kvasir-vqa.

æè¦ï¼æåå¼é² Kvasir-VQAï¼ä¸åç± HyperKvasir å Kvasir-Instrument è³æéè¡ççå»¶ä¼¸è³æéï¼ä¸¦å å¥åé¡èè§£ç­è¨»è§£ï¼ä»¥ä¿é²å¨èè¸ (GI) è¨ºæ·ä¸­çé²éæ©å¨å­¸ç¿ä»»åãæ­¤è³æéåå« 6,500 åè¨»è§£å½±åï¼æ¶µèåç¨® GI éçæ³åæè¡å¨æ¢°ï¼ä¸¦ä¸æ¯æ´åæ¬æ¯éé¡ãé¸æé¡ãä½ç½®åæ¸å­è¨æ¸ç­å¤ç¨®é¡åçåé¡ãæ­¤è³æéé©ç¨æ¼å½±åæ¨é¡ãè¦è¦ºåç­ (VQA)ãåæé«å­¸å½±åçæå­çæãç©ä»¶åµæ¸¬ååé¡ç­æç¨ç¨å¼ãæåçå¯¦é©è­ææ­¤è³æéå¨è¨ç·´ä¸åé¸å®ä»»åçæ¨¡åä¸­å·æææï¼å±ç¤ºäºå¨é«å­¸å½±ååæåè¨ºæ·ä¸­éè¦çæç¨ãæåä¹çºæ¯åä»»åæä¾è©ä¼°ææ¨ï¼çªé¡¯æåè³æéçå¯ç¨æ§åå¤åè½æ§ãæ­¤è³æéåæ¯æ´å·¥ä»¶å¯æ¼ https://datasets.simula.no/kvasir-vqa åå¾ã

##### **SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**
2409.01013v1 by Mevan Ekanayake, Zhifeng Chen, Gary Egan, Mehrtash Harandi, Zhaolin Chen

Implicit Neural Representations (INRs) have recently advanced the field of
deep learning due to their ability to learn continuous representations of
signals without the need for large training datasets. Although INR methods have
been studied for medical image super-resolution, their adaptability to
localized priors in medical images has not been extensively explored. Medical
images contain rich anatomical divisions that could provide valuable local
prior information to enhance the accuracy and robustness of INRs. In this work,
we propose a novel framework, referred to as the Semantically Conditioned INR
(SeCo-INR), that conditions an INR using local priors from a medical image,
enabling accurate model fitting and interpolation capabilities to achieve
super-resolution. Our framework learns a continuous representation of the
semantic segmentation features of a medical image and utilizes it to derive the
optimal INR for each semantic region of the image. We tested our framework
using several medical imaging modalities and achieved higher quantitative
scores and more realistic super-resolution outputs compared to state-of-the-art
methods.

æè¦ï¼é±å¼ç¥ç¶è¡¨å¾µ (INR) è¿æç±æ¼å¶ç¡éå¤§éè¨ç·´è³æéå°±è½å­¸ç¿è¨èçé£çºè¡¨å¾µçè½åï¼èæ¨åäºæ·±åº¦å­¸ç¿é åçé²å±ãåç®¡ INR æ¹æ³å·²è¢«ç ç©¶ç¨æ¼é«å­¸å½±åè¶è§£æåº¦ï¼ä½å¶å°æ¼é«å­¸å½±åä¸­å±é¨åé©çé©ææ§å°æªè¢«å»£æ³æ¢è¨ãé«å­¸å½±ååå«è±å¯çè§£åå­¸ååï¼éäºååå¯ä»¥æä¾æå¹å¼çå±é¨åé©è³è¨ï¼ä»¥å¢å¼· INR çæºç¢ºæ§åç©©å¥æ§ãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸åæ°ç©çæ¶æ§ï¼ç¨±çºèªç¾©æ¢ä»¶ INR (SeCo-INR)ï¼å®ä½¿ç¨é«å­¸å½±åä¸­çå±é¨åé©ä¾èª¿æ´ INRï¼å¯¦ç¾æºç¢ºçæ¨¡åæ¬ååæå¼è½åï¼ä»¥å¯¦ç¾è¶è§£æåº¦ãæåçæ¶æ§å­¸ç¿é«å­¸å½±åçèªæåå²ç¹å¾µçé£çºè¡¨å¾µï¼ä¸¦å©ç¨å®çºå½±åçæ¯åèªæååæ¨å°æä½³ INRãæåä½¿ç¨å¤ç¨®é«å­¸å½±åæ¹å¼æ¸¬è©¦æåçæ¶æ§ï¼ä¸¦èæåé²çæ¹æ³ç¸æ¯ï¼éå°äºæ´é«çéåè©ååæ´é¼ççè¶è§£æåº¦è¼¸åºã

##### **Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**
2409.00873v1 by Sajib Acharjee Dip, Kazi Hasan Ibn Arif, Uddip Acharjee Shuvo, Ishtiaque Ahmed Khan, Na Meng

In the realm of dermatology, the complexity of diagnosing skin conditions
manually necessitates the expertise of dermatologists. Accurate identification
of various skin ailments, ranging from cancer to inflammatory diseases, is
paramount. However, existing artificial intelligence (AI) models in dermatology
face challenges, particularly in accurately diagnosing diseases across diverse
skin tones, with a notable performance gap in darker skin. Additionally, the
scarcity of publicly available, unbiased datasets hampers the development of
inclusive AI diagnostic tools. To tackle the challenges in accurately
predicting skin conditions across diverse skin tones, we employ a
transfer-learning approach that capitalizes on the rich, transferable knowledge
from various image domains. Our method integrates multiple pre-trained models
from a wide range of sources, including general and specific medical images, to
improve the robustness and inclusiveness of the skin condition predictions. We
rigorously evaluated the effectiveness of these models using the Diverse
Dermatology Images (DDI) dataset, which uniquely encompasses both
underrepresented and common skin tones, making it an ideal benchmark for
assessing our approach. Among all methods, Med-ViT emerged as the top performer
due to its comprehensive feature representation learned from diverse image
sources. To further enhance performance, we conducted domain adaptation using
additional skin image datasets such as HAM10000. This adaptation significantly
improved model performance across all models.

æè¦ï¼<paragraph>å¨ç®è¤çå­¦é¢åï¼äººå·¥è¯æ­ç®è¤ç¶åµçå¤ææ§éè¦ç®è¤ç§å»å¸çä¸ä¸ç¥è¯ãä»ççå°ççæ§ç¾çï¼å¯¹åç§ç®è¤ç¾ççåç¡®è¯å«è³å³éè¦ãç¶èï¼ç°æçç®è¤çå­¦äººå·¥æºè½ (AI) æ¨¡åé¢ä¸´ææï¼å°¤å¶æ¯å¨åç¡®è¯æ­ä¸åè¤è²çç¾çæ¶ï¼å¨è¾æ·±çè¤è²ä¸å­å¨ææ¾çæ§è½å·®è·ãæ­¤å¤ï¼å¬å¼å¯ç¨çæ åæ°æ®éçç¨ç¼ºæ§é»ç¢äºåå®¹æ§ AI è¯æ­å·¥å·çå¼åãä¸ºäºåºå¯¹åç¡®é¢æµä¸åè¤è²ç®è¤ç¶åµçææï¼æä»¬éç¨äºä¸ç§è¿ç§»å­¦ä¹ æ¹æ³ï¼è¯¥æ¹æ³å©ç¨äºæ¥èªåç§å¾ååçä¸°å¯å¯è½¬ç§»ç¥è¯ãæä»¬çæ¹æ³éæäºæ¥èªå¹¿æ³æ¥æºçå¤ä¸ªé¢è®­ç»æ¨¡åï¼åæ¬ä¸è¬åç¹å®çå»å­¦å¾åï¼ä»¥æé«ç®è¤ç¶åµé¢æµçç¨³å¥æ§ååå®¹æ§ãæä»¬ä½¿ç¨ Diverse Dermatology Images (DDI) æ°æ®éä¸¥æ ¼è¯ä¼°äºè¿äºæ¨¡åçæææ§ï¼è¯¥æ°æ®éç¬ç¹å°åå«äºä»£è¡¨æ§ä¸è¶³åå¸¸è§çè¤è²ï¼ä½¿å¶æä¸ºè¯ä¼°æä»¬æ¹æ³ççæ³åºåãå¨æææ¹æ³ä¸­ï¼Med-ViT ç±äºå¶ä»åç§å¾åæ¥æºä¸­å­¦å°çç»¼åç¹å¾è¡¨ç¤ºèæä¸ºè¡¨ç°æå¥½çæ¹æ³ãä¸ºäºè¿ä¸æ­¥æé«æ§è½ï¼æä»¬ä½¿ç¨ HAM10000 ç­å¶ä»ç®è¤å¾åæ°æ®éè¿è¡äºåéåºãè¿ç§éåºæ¾çæé«äºæææ¨¡åçæ¨¡åæ§è½ã</paragraph>

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ç¶å¸¸ç¼ºä¹ç¹å®é åçç¥è­ï¼å³ä½¿ç¶éå¾®èª¿çæ¨¡åä¹å®¹æç¢çå¹»è¦ºãå æ­¤ï¼éè¦æ´å¤å¯é çæ¨¡åä¾ç´å¥å¤é¨ç¥è­ãæåæåºäºä¸åæµç¨ 4StepFocusï¼ç¹å¥æ¯é èçæ­¥é©ï¼å¯ä»¥å¤§å¹æ¹å LLM çç­æ¡ãéæ¯ééæä¾åå¼å°çå¤é¨ç¥è­å­åï¼å©ç¨æ¨¡åèªè¡æ·åéè¯æ§èçµ¡åé²è¡åºæ¬æ¨ççè½åä¾å¯¦ç¾çãæ­¤æ¹æ³ééå¨åçµæ§åç¥è­åº«ä¸­é²è¡åºæ¼ä¸åçµçæå°ï¼ä»¥ç´æ¥ä¸å¯è¿½è¹¤çæ¹å¼ç¸®å°æ½å¨æ­£ç¢ºç­æ¡çç¯åï¼ç¶å¾ååæå°æ½å¨è¡¨å¾µï¼æ ¹æéçµæ§åè³æå°éäºåé¸ç­æ¡é²è¡æåãéèç´ç²¹åºæ¼æ½å¨è¡¨å¾µçç¸éæ¹æ³ææåå¥ã4StepFocus åå«ä»¥ä¸æ­¥é©ï¼1) ç± LLM é²è¡ä¸åçµç¢çä»¥æ·åéè¯è³æï¼2) å¨éäºä¸åçµä¸­æ¿æè®æ¸ï¼ä»¥æ¡ç¨ç¥è­åè¡¨ç¸®å°ç­æ¡åé¸ç¯åï¼3) ä½¿ç¨æ¶åéè¯éçµæ§åè³æçåéç¸ä¼¼æ§æå°å°å©é¤åé¸ç­æ¡é²è¡æåºï¼4) ç± LLM éæ°å°æä½³åé¸ç­æ¡é²è¡æåï¼ä¸¦æä¾èæ¯è³æãå¨é«çãç¢åæ¨è¦åå­¸è¡è«ææå°æ¸¬è©¦éä¸­é²è¡çå¯¦é©è­æï¼éç¨®æ¹æ³ç¢ºå¯¦æ¯ä¸ç¨®å¼·å¤§çæ´åãå®ä¸åå¢å äºä¾èªè³è¨æª¢ç´¢çç¸å³å¯è¿½è¹¤èæ¯è³è¨ï¼èä¸èæåé²çæ¹æ³ç¸æ¯ï¼ä¹å¤§å¹æåäºæè½ãæ¬ææåºäºä¸åæ°ç©ä¸é®®å°æ¢ç´¢çæ¹åï¼å æ­¤æä¾äºå»£æ³çæªä¾å·¥ä½æ©æãä½¿ç¨çåå§ç¢¼å¯å¨ https://github.com/kramerlab/4StepFocus åå¾ã

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

æè¦ï¼æ¬ææåºäºä¸åç¥è­å·¥ç¨åå¤èªè¨èªç¾©æ¨çæè¡çæ¬ä½è¨­è¨ï¼ç¨æ¼å»ºç«ä¸åèªååç³»çµ±ï¼ä»¥ç¥è­åè­çå½¢å¼å¸æ¶å°åº¦æççç¹é£ªè³è¨ãéé»å¨æ¼è¨­è¨æºæ§æ¹æ³ï¼ä»¥æ¨å°æ¬ä½è¨­è¨ï¼ä¸¦å¨é¢æ·åéæ¼é£ç©ãé£è­ãé£æãç¹é£ªç¹æ§ï¼ä»¥åæéè¦ççé¤çç¥è­ï¼ä¸¦æ´å¤§è¦æ¨¡ãæåå¨éåç è¨æè«æä¸­ä»ç´¹äºæåæ­£å¨é²è¡çå·¥ä½ï¼è©³ç´°æè¿°äºæ´çå°åº¦æçç¥è­ç¸éçææ°ï¼ä¸¦æåºäºæåçé«éæ¬ä½è¨­è¨ãæåä¹æåºäºä¸ç¨®æ°çå·¥ä½æµç¨ï¼å®ä½¿ç¨ AIãLLM åèªè¨æè¡ï¼å¾å¬å±é åçé£è­é¨è½æ ¼ç¶²ç«ä¸­æ´çè³è¨ï¼ä»¥å»ºç«å°åº¦æççç¥è­åè­ãæ¬ææåºçç¥è­æ´çæ¹æ³æ¯éç¨çï¼å¯ä»¥è¤è£½å°ä»»ä½é åãè¨­è¨èæç¨ç¡éï¼å¯ç¨æ¼ AI é©åçæºæ§åæãå»ºç«åäººåæ¸ä½å¥åº·æ¨è¦ç³»çµ±ï¼ä»¥åä½¿ç¨ä½¿ç¨èè³è¨ãé£ç©çç©åå­¸ãå°çè³è¨ãè¾²æ¥­è³è¨ç­èçµ¡è³è¨ï¼ä¾è£åå°åº¦æççç¥è­åè­ã

##### **AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**
2409.00735v1 by Mahsa Khosravi, Matthew Carroll, Kai Liang Tan, Liza Van der Laan, Joscif Raigne, Daren S. Mueller, Arti Singh, Aditya Balu, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar

Agricultural production requires careful management of inputs such as
fungicides, insecticides, and herbicides to ensure a successful crop that is
high-yielding, profitable, and of superior seed quality. Current
state-of-the-art field crop management relies on coarse-scale crop management
strategies, where entire fields are sprayed with pest and disease-controlling
chemicals, leading to increased cost and sub-optimal soil and crop management.
To overcome these challenges and optimize crop production, we utilize machine
learning tools within a virtual field environment to generate localized
management plans for farmers to manage biotic threats while maximizing profits.
Specifically, we present AgGym, a modular, crop and stress agnostic simulation
framework to model the spread of biotic stresses in a field and estimate yield
losses with and without chemical treatments. Our validation with real data
shows that AgGym can be customized with limited data to simulate yield outcomes
under various biotic stress conditions. We further demonstrate that deep
reinforcement learning (RL) policies can be trained using AgGym for designing
ultra-precise biotic stress mitigation strategies with potential to increase
yield recovery with less chemicals and lower cost. Our proposed framework
enables personalized decision support that can transform biotic stress
management from being schedule based and reactive to opportunistic and
prescriptive. We also release the AgGym software implementation as a community
resource and invite experts to contribute to this open-sourced publicly
available modular environment framework. The source code can be accessed at:
https://github.com/SCSLabISU/AgGym.

æè¦ï¼è¾²æ¥­çç¢éè¦å°å¿ç®¡çè¼¸å¥ï¼ä¾å¦æ®ºèåãæ®ºè²ååé¤èåï¼ä»¥ç¢ºä¿ä½ç©æåãé«ç¢ãæå©å¯åä¸å·æåªè¯çç¨®å­åè³ªãç®åæåé²çç°éä½ç©ç®¡çä¾è³´æ¼ç²ç¥çä½ç©ç®¡çç­ç¥ï¼å¶ä¸­æ´åç°å°é½å´çäºæ§å¶çè²å®³çåå­¸ç©è³ªï¼å°è´ææ¬å¢å ååå£¤åä½ç©ç®¡çä¸ä½³ãçºäºåæéäºææ°ä¸¦åªåä½ç©çç¢ï¼æåå¨èæ¬ç°éç°å¢ä¸­å©ç¨æ©å¨å­¸ç¿å·¥å·çºè¾²æ°çæå±é¨ç®¡çè¨ç«ï¼ä»¥ç®¡ççç©å¨èä¸¦åææå¤§åå©æ½¤ãå·é«ä¾èªªï¼æåæåºäº AgGymï¼ä¸åæ¨¡çµåãä½ç©åå£åä¸å¯ç¥çæ¨¡æ¬æ¶æ§ï¼ç¨æ¼æ¨¡æ¬ç°éçç©å£åçæ´æ£ï¼ä¸¦ä¼°ç®æåæ²æåå­¸èççç¢éæå¤±ãæåä½¿ç¨çå¯¦æ¸æé²è¡é©è­ï¼é¡¯ç¤º AgGym å¯ä»¥ä½¿ç¨æéçæ¸æé²è¡èªè¨ï¼ä»¥æ¨¡æ¬åç¨®çç©å£åæ¢ä»¶ä¸çç¢éçµæãæåé²ä¸æ­¥è­æï¼æ·±åº¦å¼·åå­¸ç¿ (RL) æ¿ç­å¯ä»¥ä½¿ç¨ AgGym é²è¡è¨ç·´ï¼ä»¥è¨­è¨è¶ç²¾ç¢ºççç©å£åç·©è§£ç­ç¥ï¼ä¸¦æå¯è½ä»¥æ´å°çåå­¸ç©è³ªåæ´ä½çææ¬å¢å ç¢éæ¢å¾©ãæåæåºçæ¶æ§åç¨äºåäººåæ±ºç­æ¯æ´ï¼å¯ä»¥å°çç©å£åç®¡çå¾åºæ¼æéè¡¨åè¢«åè½è®çºæ©æä¸»ç¾©åè¦ç¯æ§ãæåéå° AgGym è»é«å¯¦ä½ä½çºç¤¾åè³æºéåºï¼ä¸¦éè«å°å®¶çºéåéæ¾åå§ç¢¼ä¸å¬éå¯ç¨çæ¨¡çµåç°å¢æ¶æ§ååºè²¢ç»ãå¯ä»¥å¨ä»¥ä¸ä½ç½®åå¾åå§ç¢¼ï¼https://github.com/SCSLabISU/AgGymã

##### **LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**
2409.00726v1 by Zhaojie Fang, Xiao Yu, Guanyu Zhou, Ke Zhuang, Yifei Chen, Ruiquan Ge, Changmiao Wang, Gangyong Jia, Qing Wu, Juan Ye, Maimaiti Nuliqiman, Peifang Xu, Ahmed Elazab

Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise
identification of ocular diseases using sodium fluorescein, which can be
potentially harmful. Existing research has developed methods to generate UWF-FA
from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the
adverse reactions associated with injections. However, these methods have been
less effective in producing high-quality late-phase UWF-FA, particularly in
lesion areas and fine details. Two primary challenges hinder the generation of
high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and
early/late-phase UWF-FA datasets, and the need for realistic generation at
lesion sites and potential blood leakage regions. This study introduces an
improved latent diffusion model framework to generate high-quality late-phase
UWF-FA from limited paired UWF images. To address the challenges as mentioned
earlier, our approach employs a module utilizing Cross-temporal Regional
Difference Loss, which encourages the model to focus on the differences between
early and late phases. Additionally, we introduce a low-frequency enhanced
noise strategy in the diffusion forward process to improve the realism of
medical images. To further enhance the mapping capability of the variational
autoencoder module, especially with limited datasets, we implement a Gated
Convolutional Encoder to extract additional information from conditional
images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein
Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase
UWF-FA and achieves state-of-the-art results compared to other existing methods
when working with limited datasets. Our source code is available at:
https://github.com/Tinysqua/****.

æè¦ï¼è¶å»£è§è¢åè¡ç®¡é å½±ï¼UWF-FAï¼ä½¿ç¨å¯è½å·ææ½å¨å±å®³çéè¢åç´ ï¼å¯ç²¾ç¢ºè­å¥ç¼ç¾ãç¾æç ç©¶å·²éç¼åºå¾è¶å»£è§ææé·å°ç¼ç§é¡ï¼UWF-SLOï¼ç¢ç UWF-FA çæ¹æ³ï¼ä»¥æ¸å°èæ³¨å°ç¸éçä¸è¯åæãç¶èï¼éäºæ¹æ³å¨ç¢çé«åè³ªçå¾æ UWF-FA æ¹é¢ææè¼å·®ï¼ç¹å¥æ¯å¨çç¶åååç²¾ç´°ç´°ç¯æ¹é¢ãç¢çé«åè³ªå¾æ UWF-FA é¢è¨å©é ä¸»è¦ææ°ï¼éå°ç UWF-SLO åæ©æ/å¾æ UWF-FA è³æéç¨å°ï¼ä»¥åéè¦å¨çç¶é¨ä½åæ½å¨åºè¡ååé²è¡é¼ççç¢çãæ¬ç ç©¶å¼é²ä¸ç¨®æ¹è¯çæ½å¨æ´æ£æ¨¡åæ¶æ§ï¼å¾æééå°ç UWF å½±åç¢çé«åè³ªçå¾æ UWF-FAãçºäºæå°åé¢æå°çææ°ï¼æåçæ¹æ³æ¡ç¨ä¸åæ¨¡çµï¼å©ç¨è·¨æéååå·®ç°æå¤±ï¼é¼åµæ¨¡åå°æ³¨æ¼æ©æåå¾æä¹éçå·®ç°ãæ­¤å¤ï¼æåå¨æ´æ£ååéç¨ä¸­å¼é²ä¸ç¨®ä½é »å¢å¼·éè¨ç­ç¥ï¼ä»¥æ¹åé«å­¸å½±åççå¯¦æ§ãçºäºé²ä¸æ­¥å¢å¼·è®ç°èªåç·¨ç¢¼å¨æ¨¡çµçå°æè½åï¼ç¹å¥æ¯å¨è³æéæéçææ³ä¸ï¼æåå¯¦ä½ä¸åéæ§å·ç©ç·¨ç¢¼å¨ï¼å¾æ¢ä»¶å½±åä¸­èåé¡å¤è³è¨ãæåéå°è¶å»£è§å¾æè¢åè¡ç®¡é å½±ï¼LPUWF-LDMï¼çæ½å¨æ´æ£æ¨¡åææéå»ºå¾æ UWF-FA ä¸­çç²¾ç´°ç´°ç¯ï¼ä¸¦å¨ä½¿ç¨æéè³æéæï¼èå¶ä»ç¾ææ¹æ³ç¸æ¯ï¼éå°æåé²ççµæãæåçåå§ç¢¼å¯å¨ä»¥ä¸ç¶²ååå¾ï¼
https://github.com/Tinysqua/****ã

##### **BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**
2409.00724v1 by Shams Nafisa Ali, Afia Zahin, Samiul Based Shuvo, Nusrat Binta Nizam, Shoyad Ibn Sabur Khan Nuhash, Sayeed Sajjad Razin, S. M. Sakeef Sani, Farihin Rahman, Nawshad Binta Nizam, Farhat Binte Azam, Rakib Hossen, Sumaiya Ohab, Nawsabah Noor, Taufiq Hasan

Cardiac auscultation, an integral tool in diagnosing cardiovascular diseases
(CVDs), often relies on the subjective interpretation of clinicians, presenting
a limitation in consistency and accuracy. Addressing this, we introduce the
BUET Multi-disease Heart Sound (BMD-HS) dataset - a comprehensive and
meticulously curated collection of heart sound recordings. This dataset,
encompassing 864 recordings across five distinct classes of common heart
sounds, represents a broad spectrum of valvular heart diseases, with a focus on
diagnostically challenging cases. The standout feature of the BMD-HS dataset is
its innovative multi-label annotation system, which captures a diverse range of
diseases and unique disease states. This system significantly enhances the
dataset's utility for developing advanced machine learning models in automated
heart sound classification and diagnosis. By bridging the gap between
traditional auscultation practices and contemporary data-driven diagnostic
methods, the BMD-HS dataset is poised to revolutionize CVD diagnosis and
management, providing an invaluable resource for the advancement of cardiac
health research. The dataset is publicly available at this link:
https://github.com/mHealthBuet/BMD-HS-Dataset.

æè¦ï¼å¿èè½è¨ºæ¯è¨ºæ·å¿è¡ç®¡ç¾ç (CVD) çä¸é æ´åå·¥å·ï¼éå¸¸ä¾è³´æ¼è¨åºé«å¸«çä¸»è§è©®éï¼å¨ä¸è´æ§åæºç¢ºæ§æ¹é¢å­å¨éå¶ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº BUET å¤éç¾çå¿é³ (BMD-HS) è³æéï¼éæ¯ä¸åå¨é¢ä¸ç¶éç²¾å¿ç­åçå¿é³éé³è³æéãæ­¤è³æéåå«äºç¨®å¸¸è¦å¿é³ç 864 åéé³ï¼ä»£è¡¨äºå»£æ³çå¿ç£èç¾çï¼éé»å¨æ¼è¨ºæ·å°é£ççä¾ãBMD-HS è³æéççªåºç¹é»æ¯å¶åµæ°çå¤æ¨ç±¤è¨»è§£ç³»çµ±ï¼å®æ¶µèäºåç¨®ç¾çåç¨ç¹çç¾ççæãéåç³»çµ±é¡¯èå¢å¼·äºè³æéå¨éç¼èªåå¿é³åé¡åè¨ºæ·ä¸­é²éæ©å¨å­¸ç¿æ¨¡åçæç¨ãééå½åå³çµ±è½è¨ºå¯¦åèç¶ä»£è³æé©åè¨ºæ·æ¹æ³ä¹éçå·®è·ï¼BMD-HS è³æéæºåå¥½é©æ°å¿è¡ç®¡ç¾ççè¨ºæ·åç®¡çï¼çºå¿èå¥åº·ç ç©¶çé²å±æä¾å¯¶è²´çè³æºãæ­¤è³æéå¯ééä»¥ä¸é£çµå¬éåå¾ï¼https://github.com/mHealthBuet/BMD-HS-Datasetã

##### **Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**
2409.00718v1 by Pragya Gupta, Subhamoy Mandal, Debashree Guha, Debjani Chakraborty

Automatic diagnosis techniques have evolved to identify age-related macular
degeneration (AMD) by employing single modality Fundus images or optical
coherence tomography (OCT). To classify ocular diseases, fundus and OCT images
are the most crucial imaging modalities used in the clinical setting. Most deep
learning-based techniques are established on a single imaging modality, which
contemplates the ocular disorders to a specific extent and disregards other
modality that comprises exhaustive information among distinct imaging
modalities. This paper proposes a modality-specific multiscale color space
embedding integrated with the attention mechanism based on transfer learning
for classification (MCGAEc), which can efficiently extract the distinct
modality information at various scales using the distinct color spaces. In this
work, we first introduce the modality-specific multiscale color space encoder
model, which includes diverse feature representations by integrating distinct
characteristic color spaces on a multiscale into a unified framework. The
extracted features from the prior encoder module are incorporated with the
attention mechanism to extract the global features representation, which is
integrated with the prior extracted features and transferred to the random
forest classifier for the classification of AMD. To analyze the performance of
the proposed MCGAEc method, a publicly available multi-modality dataset from
Project Macula for AMD is utilized and compared with the existing models.

æè¦ï¼èªåè¨ºæ·æè¡å·²æ¼é²å°è½ééä½¿ç¨å®ä¸æ¨¡å¼ç¼åºå½±åæåå­¸ç¸å¹²æ·å±¤ææ (OCT) ä¾è¾¨è­å¹´é½¡ç¸éæ§é»æé¨çè® (AMD)ãçºäºåé¡ç¼ç¾ï¼ç¼åºå OCT å½±åæ¯è¨åºç°å¢ä¸­ä½¿ç¨æééµçå½±åæ¨¡å¼ãå¤§å¤æ¸åºæ¼æ·±åº¦å­¸ç¿çæè¡å»ºç«å¨å®ä¸å½±åæ¨¡å¼ä¸ï¼å®å¨ä¸å®ç¨åº¦ä¸èéäºç¼ç¾ï¼å»å¿½ç¥äºå¶ä»æ¨¡å¼ï¼èå¶ä»æ¨¡å¼åå«äºä¸åå½±åæ¨¡å¼ä¹éçè©³ç¡è³è¨ãæ¬ææåºäºä¸ç¨®æ¨¡å¼ç¹å®çå¤å°ºåº¦è²å½©ç©ºéåµå¥æ´åï¼ä¸¦åºæ¼ç¨æ¼åé¡çè½ç§»å­¸ç¿çæ³¨æåæ©å¶ (MCGAEc)ï¼å®è½ä½¿ç¨ä¸åçè²å½©ç©ºéå¨ä¸åçå°ºåº¦ä¸æææåä¸åçæ¨¡å¼è³è¨ãå¨éé å·¥ä½ä¸­ï¼æåé¦åä»ç´¹äºæ¨¡å¼ç¹å®çå¤å°ºåº¦è²å½©ç©ºéç·¨ç¢¼å¨æ¨¡åï¼å®ééå°ä¸åçç¹å¾µè²å½©ç©ºéæ´åå°å¤å°ºåº¦ä¸­ï¼ä¾ç´å¥ä¸åçç¹å¾µè¡¨å¾µå°ä¸åçµ±ä¸çæ¶æ§ä¸­ãå¾ååçç·¨ç¢¼å¨æ¨¡çµä¸­æåçç¹å¾µèæ³¨æåæ©å¶çµåï¼ä»¥æåå¨åç¹å¾µè¡¨å¾µï¼å®èååæåçç¹å¾µæ´åï¼ä¸¦è½ç§»å°é¨æ©æ£®æåé¡å¨ï¼ä»¥é²è¡ AMD åé¡ãçºäºåæææåºç MCGAEc æ¹æ³çæè½ï¼æåå©ç¨äºä¾èª Project Macula for AMD çå¬éå¤æ¨¡å¼è³æéï¼ä¸¦èç¾ææ¨¡åé²è¡æ¯è¼ã

##### **Curriculum Prompting Foundation Models for Medical Image Segmentation**
2409.00695v1 by Xiuqi Zheng, Yuhang Zhang, Haoran Zhang, Hongrui Liang, Xueqi Bao, Zhuqing Jiang, Qicheng Lao

Adapting large pre-trained foundation models, e.g., SAM, for medical image
segmentation remains a significant challenge. A crucial step involves the
formulation of a series of specialized prompts that incorporate specific
clinical instructions. Past works have been heavily reliant on a singular type
of prompt for each instance, necessitating manual input of an ideally correct
prompt, which is less efficient. To tackle this issue, we propose to utilize
prompts of different granularity, which are sourced from original images to
provide a broader scope of clinical insights. However, combining prompts of
varying types can pose a challenge due to potential conflicts. In response, we
have designed a coarse-to-fine mechanism, referred to as curriculum prompting,
that progressively integrates prompts of different types. Through extensive
experiments on three public medical datasets across various modalities, we
demonstrate the effectiveness of our proposed approach, which not only
automates the prompt generation process but also yields superior performance
compared to other SAM-based medical image segmentation methods. Code is
available at: https://github.com/AnnaZzz-zxq/Curriculum-Prompting.

æè¦ï¼èª¿æ´å¤§åé è¨ç·´åºç¤æ¨¡åï¼ä¾å¦ SAMï¼ä»¥é²è¡é«å­¸å½±ååå²ä»æ¯ä¸é éå¤§ææ°ãééµæ­¥é©æ¶åå¶å®ä¸ç³»ååå«ç¹å®è¨åºèªªæçå°éæç¤ºãéå»çå·¥ä½å¨å¾å¤§ç¨åº¦ä¸ä¾è³´æ¼æ¯åä¾é çå®ä¸æç¤ºé¡åï¼ééè¦æåè¼¸å¥çæ³çæ­£ç¢ºæç¤ºï¼æçè¼ä½ãçºäºè§£æ±ºéååé¡ï¼æåå»ºè­°å©ç¨ä¸åç²åº¦çæç¤ºï¼éäºæç¤ºä¾èªåå§å½±åï¼ä»¥æä¾æ´å»£æ³çè¨åºè¦è§£ãç¶èï¼ç±æ¼æ½å¨è¡çªï¼çµåä¸åé¡åçæç¤ºå¯è½ææ§æææ°ãçºäºè§£æ±ºéååé¡ï¼æåè¨­è¨äºä¸ç¨®ç±ç²å°ç´°çæ©å¶ï¼ç¨±çºèª²ç¨æç¤ºï¼å®éæ­¥æ´åä¸åé¡åçæç¤ºãééå°åç¨®æ¨¡å¼ä¸çä¸åå¬å±é«å­¸è³æéé²è¡å»£æ³çå¯¦é©ï¼æåè­æäºæåæåºçæ¹æ³çæææ§ï¼å®ä¸åèªååæç¤ºçæéç¨ï¼èä¸èå¶ä»åºæ¼ SAM çé«å­¸å½±ååå²æ¹æ³ç¸æ¯ï¼éç¢çäºæ´å¥½çæè½ãç¨å¼ç¢¼å¯å¨ä»¥ä¸ä½ç½®åå¾ï¼https://github.com/AnnaZzz-zxq/Curriculum-Promptingã

##### **Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**
2409.00544v1 by Jacqueline Lammert, Nicole Pfarr, Leonid Kuligin, Sonja Mathes, Tobias Dreyer, Luise Modersohn, Patrick Metzger, Dyke Ferber, Jakob Nikolas Kather, Daniel Truhn, Lisa Christine Adams, Keno Kyrill Bressem, Sebastian Lange, Kristina Schwamborn, Martin Boeker, Marion Kiechle, Ulrich A. Schatz, Holger Bronger, Maximilian Tschochohei

Rare gynecological tumors (RGTs) present major clinical challenges due to
their low incidence and heterogeneity. The lack of clear guidelines leads to
suboptimal management and poor prognosis. Molecular tumor boards accelerate
access to effective therapies by tailoring treatment based on biomarkers,
beyond cancer type. Unstructured data that requires manual curation hinders
efficient use of biomarker profiling for therapy matching. This study explores
the use of large language models (LLMs) to construct digital twins for
precision medicine in RGTs.
  Our proof-of-concept digital twin system integrates clinical and biomarker
data from institutional and published cases (n=21) and literature-derived data
(n=655 publications with n=404,265 patients) to create tailored treatment plans
for metastatic uterine carcinosarcoma, identifying options potentially missed
by traditional, single-source analysis. LLM-enabled digital twins efficiently
model individual patient trajectories. Shifting to a biology-based rather than
organ-based tumor definition enables personalized care that could advance RGT
management and thus enhance patient outcomes.

æè¦ï¼ç½è¦å©¦ç§è«ç¤ (RGT) ç±æ¼å¶ä½ç¼ççåç°è³ªæ§ï¼å°è¨åºå¸¶ä¾éå¤§ææ°ãç¼ºä¹æç¢ºçæå¼å°è´æ¬¡ä½³ç®¡çåä¸è¯é å¾ãåå­è«ç¤å§å¡æééæ ¹æçç©æ¨è¨å®¢è£½åæ²»çï¼å éåå¾ææçæ³ï¼è¶è¶ççé¡åãéè¦æåæ´ççéçµæ§åè³æé»ç¤äºçç©æ¨è¨åæå¨çæ³éå°ä¸­çææä½¿ç¨ãæ¬ç ç©¶æ¢è¨ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) çº RGT çç²¾æºé«çå»ºæ§æ¸ä½éèèã
æåçæ¦å¿µé©è­æ¸ä½éèèç³»çµ±æ´åäºä¾èªæ©æ§åå·²ç¼è¡¨çæ¡ä¾ (n=21) çè¨åºåçç©æ¨è¨è³æï¼ä»¥åä¾èªæç»çè³æ (n=655 ç¯åºçç©ï¼n=404,265 åæ£è)ï¼çºè½ç§»æ§å­å®®èç¤çå¶å®å®¢è£½åæ²»çè¨ç«ï¼æ¾åºå³çµ±å®ä¸ä¾æºåæå¯è½éºæ¼çé¸é ãLLM åç¨çæ¸ä½éèèææå°æ¨¡æ¬åå¥æ£èçè»è·¡ãå¾åºæ¼å¨å®çè«ç¤å®ç¾©è½è®çºåºæ¼çç©å­¸çå®ç¾©ï¼è½å¯¦ç¾åäººåç§è­·ï¼é²èæå RGT ç®¡çä¸¦æ¹åæ£èé å¾ã

##### **Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**
2409.00391v1 by Georgios Ioannides, Adrian Kieback, Aman Chadha, Aaron Elkins

Speech-based depression detection poses significant challenges for automated
detection due to its unique manifestation across individuals and data scarcity.
Addressing these challenges, we introduce DAAMAudioCNNLSTM and
DAAMAudioTransformer, two parameter efficient and explainable models for audio
feature extraction and depression detection. DAAMAudioCNNLSTM features a novel
CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM),
focusing dynamically on informative speech segments. DAAMAudioTransformer,
leveraging a transformer encoder in place of the CNN-LSTM architecture,
incorporates the same DAAM module for enhanced attention and interpretability.
These approaches not only enhance detection robustness and interpretability but
also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro
score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the
DAIC-WOZ dataset, without reliance on supplementary information such as vowel
positions and speaker information during training/validation as in previous
approaches. Both models' significant explainability and efficiency in
leveraging speech signals for depression detection represent a leap towards
more reliable, clinically useful diagnostic tools, promising advancements in
speech and mental health care. To foster further research in this domain, we
make our code publicly available.

æè¦ï¼èªé³åæé¬±æª¢æ¸¬å°èªååæª¢æ¸¬ä¾èªªæ¯ä¸å¤§ææ°ï¼å çºå®å¨ä¸ååé«éçè¡¨ç¾ç¨ç¹ï¼ä¸è³æç¨å°ãçºäºæå°éäºææ°ï¼æåå¼å¥äº DAAMAudioCNNLSTM å DAAMAudioTransformerï¼éå©ååæ¸ææä¸å¯è§£éçæ¨¡åï¼ç¨æ¼é³è¨ç¹å¾µèååæé¬±æª¢æ¸¬ãDAAMAudioCNNLSTM æ¡ç¨åµæ°ç CNN-LSTM æ¶æ§ï¼æ­éå¤é ­å¯åº¦èªé©ææ³¨æåæ©å¶ (DAAM)ï¼åæéæ³¨æ¼ææç¾©çèªé³åæ®µãDAAMAudioTransformer å©ç¨Transformerç·¨ç¢¼å¨åä»£ CNN-LSTM æ¶æ§ï¼ä¸¦ç´å¥ç¸åç DAAM æ¨¡çµï¼ä»¥å¢å¼·æ³¨æååå¯è§£éæ§ãéäºæ¹æ³ä¸åå¢å¼·äºæª¢æ¸¬çç©©å¥æ§åå¯è§£éæ§ï¼ééå°äºæåé²çæè½ï¼DAAMAudioCNNLSTM ç F1 å·¨è§åæ¸çº 0.702ï¼DAAMAudioTransformer å¨ DAIC-WOZ è³æéä¸ç F1 å·¨è§åæ¸çº 0.72ï¼å¨è¨ç·´/é©è­æéä¸ä¾è³´æ¼è¼å©è³è¨ï¼ä¾å¦æ¯é³ä½ç½®åèªªè©±èè³è¨ï¼éèååçåæ³ä¸åãéå©åæ¨¡åå¨å©ç¨èªé³è¨èé²è¡æé¬±æª¢æ¸¬æ¹é¢å·æé¡¯èçå¯è§£éæ§åæçï¼ä»£è¡¨èæåæ´å¯é ãè¨åºä¸æç¨çè¨ºæ·å·¥å·éé²äºä¸å¤§æ­¥ï¼ä¸¦é ç¤ºèèªé³åå¿çä¿å¥çé²æ­¥ãçºäºä¿é²éåé åçé²ä¸æ­¥ç ç©¶ï¼æåå¬éäºæåçç¨å¼ç¢¼ã

##### **Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**
2409.00310v1 by Mikhail Borisenkov, Andrei Velichko, Maksim Belyaev, Dmitry Korzun, Tatyana Tserne, Larisa Bakutova, Denis Gubin

This study investigates machine learning algorithms to identify objective
features for diagnosing food addiction (FA) and assessing confirmed symptoms
(SC). Data were collected from 81 participants (mean age: 21.5 years, range:
18-61 years, women: 77.8%) whose FA and SC were measured using the Yale Food
Addiction Scale (YFAS). Participants provided demographic and anthropometric
data, completed the YFAS, the Zung Self-Rating Depression Scale, and the Dutch
Eating Behavior Questionnaire, and wore an actimeter on the non-dominant wrist
for a week to record motor activity. Analysis of the actimetric data identified
significant statistical and entropy-based features that accurately predicted FA
and SC using ML. The Matthews correlation coefficient (MCC) was the primary
metric. Activity-related features were more effective for FA prediction
(MCC=0.88) than rest-related features (MCC=0.68). For SC, activity segments
yielded MCC=0.47, rest segments MCC=0.38, and their combination MCC=0.51.
Significant correlations were also found between actimetric features related to
FA, emotional, and restrained eating behaviors, supporting the model's
validity. Our results support the concept of a human bionic suite composed of
IoT devices and ML sensors, which implements health digital assistance with
real-time monitoring and analysis of physiological indicators related to FA and
SC.

æè¦ï¼æ¬ç ç©¶èª¿æ¥æ©å¨å­¸ç¿æ¼ç®æ³ï¼ä»¥è­å¥è¨ºæ·é£ç©æç® (FA) åè©ä¼°å·²ç¢ºèªçç (SC) çå®¢è§ç¹å¾µãè³æä¾èª 81 ä½åèèï¼å¹³åå¹´é½¡ï¼21.5 æ­²ï¼ç¯åï¼18-61 æ­²ï¼å¥³æ§ï¼77.8%ï¼ï¼å¶ FA å SC æ¯ä½¿ç¨è¶é­¯é£ç©æç®éè¡¨ (YFAS) æ¸¬éçãåèèæä¾äºäººå£çµ±è¨åäººé¡æ¸¬éè³æï¼å®æäº YFASãZung èªæè©éæé¬±éè¡¨åè·è­é£²é£è¡çºåå·ï¼ä¸¦å¨éæ£ç¨æèä¸ä½©æ´æ´»åè¨ä¸é±ä»¥è¨ééåæ´»åãå°æ´»åè¨è³æçåæè­å¥åºéè¦ççµ±è¨ååºæ¼çµçç¹å¾µï¼éäºç¹å¾µä½¿ç¨æ©å¨å­¸ç¿æºç¢ºé æ¸¬äº FA å SCãé¦¬ä¿®æ¯ç¸éä¿æ¸ (MCC) æ¯ä¸»è¦ææ¨ãèä¼æ¯ç¸éçç¹å¾µï¼MCC=0.68ï¼ç¸æ¯ï¼èæ´»åç¸éçç¹å¾µå°æ¼ FA é æ¸¬æ´ææï¼MCC=0.88ï¼ãå°æ¼ SCï¼æ´»ååæ®µç¢çç MCC=0.47ï¼ä¼æ¯åæ®µ MCC=0.38ï¼çµåå¾ MCC=0.51ãéç¼ç¾è FAãæç·ååéé£²é£è¡çºç¸éçæ´»åè¨ç¹å¾µä¹éå­å¨é¡¯èç¸éæ§ï¼éæ¯æäºæ¨¡åçæææ§ãæåççµææ¯æç±ç©è¯ç¶²è£ç½®åæ©å¨å­¸ç¿ææ¸¬å¨çµæçäººé«ä»¿çå¥ä»¶çæ¦å¿µï¼è©²å¥ä»¶å¯¦ä½äºå¥åº·æ¸ä½åå©ï¼ä¸¦å°è FA å SC ç¸éçççææ¨é²è¡å³æç£æ§ååæã

##### **Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**
2408.17401v1 by Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni

In recent years, various methods have been introduced for explaining the
outputs of "black-box" AI models. However, it is not well understood whether
users actually comprehend and trust these explanations. In this paper, we focus
on explanations for a regression tool for assessing cancer risk and examine the
effect of the explanations' content and format on the user-centric metrics of
comprehension and trust. Regarding content, we experiment with two explanation
methods: the popular SHAP, based on game-theoretic notions and thus potentially
complex for everyday users to comprehend, and occlusion-1, based on feature
occlusion which may be more comprehensible. Regarding format, we present SHAP
explanations as charts (SC), as is conventional, and occlusion-1 explanations
as charts (OC) as well as text (OT), to which their simpler nature also lends
itself. The experiments amount to user studies questioning participants, with
two different levels of expertise (the general population and those with some
medical training), on their subjective and objective comprehension of and trust
in explanations for the outputs of the regression tool. In both studies we
found a clear preference in terms of subjective comprehension and trust for
occlusion-1 over SHAP explanations in general, when comparing based on content.
However, direct comparisons of explanations when controlling for format only
revealed evidence for OT over SC explanations in most cases, suggesting that
the dominance of occlusion-1 over SHAP explanations may be driven by a
preference for text over charts as explanations. Finally, we found no evidence
of a difference between the explanation types in terms of objective
comprehension. Thus overall, the choice of the content and format of
explanations needs careful attention, since in some contexts format, rather
than content, may play the critical role in improving user experience.

æè¦ï¼<paragraph>è¿å¹´ä¾ï¼å·²ç¶å¼é²åç¨®æ¹æ³ä¾è§£éãé»ç®±ãAI æ¨¡åçè¼¸åºãç¶èï¼ç®åä¸¦ä¸æ¸æ¥ä½¿ç¨èæ¯å¦å¯¦éçè§£åä¿¡ä»»éäºè§£éãå¨æ¬æä¸­ï¼æåå°æ³¨æ¼è©ä¼°ççé¢¨éªçåæ­¸å·¥å·çè§£éï¼ä¸¦æ¢è¨è§£éçå§å®¹åæ ¼å¼å°ä»¥ä½¿ç¨èçºä¸­å¿ççè§£åä¿¡ä»»ææ¨çå½±é¿ãéæ¼å§å®¹ï¼æåå¯¦é©äºå©ç¨®è§£éæ¹æ³ï¼æµè¡ç SHAPï¼åºæ¼åå¼è«æ¦å¿µï¼å æ­¤å°æ¼æ¥å¸¸ä½¿ç¨èä¾èªªå¯è½å¾è¤éï¼ä»¥ååºæ¼ç¹å¾µé®è½ç occlusion-1ï¼å¯è½æ´ææ¼çè§£ãéæ¼æ ¼å¼ï¼æåå° SHAP è§£éåç¾çºåè¡¨ (SC)ï¼éæ¯æ£ä¾ï¼èå° occlusion-1 è§£éåç¾çºåè¡¨ (OC) ä»¥åæå­ (OT)ï¼å¶è¼çºç°¡å®çæ§è³ªä¹é©ç¨æ¼æ­¤ãéäºå¯¦é©ç­åæ¼ä½¿ç¨èç ç©¶ï¼è©¢ååèèï¼å·æå©ç¨®ä¸åç¨åº¦çå°æ¥­ç¥è­ï¼ä¸è¬æ°ç¾åå·åä¸äºé«å­¸è¨ç·´çäººï¼ï¼ä»åå°åæ­¸å·¥å·è¼¸åºè§£éçä¸»è§åå®¢è§çè§£åä¿¡ä»»ãå¨å©é ç ç©¶ä¸­ï¼æåç¼ç¾ï¼å¨åºæ¼å§å®¹é²è¡æ¯è¼æï¼ä¸è¬ä¾èªªï¼occlusion-1 åªæ¼ SHAP è§£éï¼å¨ä¸»è§çè§£åä¿¡ä»»æ¹é¢ææé¡¯çåå¥½ãç¶èï¼å¨åæ§å¶æ ¼å¼çææ³ä¸ç´æ¥æ¯è¼è§£éï¼å¨å¤§å¤æ¸ææ³ä¸åªé¡¯ç¤º OT åªæ¼ SC è§£éçè­æï¼éè¡¨æ occlusion-1 åªæ¼ SHAP è§£éçä¸»å°å°ä½å¯è½æ¯ç±åå¥½æå­èéåè¡¨ä½çºè§£éæé©åçãæå¾ï¼æåæ²æç¼ç¾è§£éé¡åå¨å®¢è§çè§£æ¹é¢çå·®ç°è­æãå æ­¤ï¼ç¸½é«èè¨ï¼å°è§£éçå§å®¹åæ ¼å¼çé¸æéè¦ä»ç´°æ³¨æï¼å çºå¨æäºææ³ä¸ï¼æ ¼å¼èéå§å®¹ï¼å¯è½å¨æ¹åä½¿ç¨èé«é©æ¹é¢ç¼æ®ééµä½ç¨ã</paragraph>

##### **Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**
2409.00163v1 by Yuhan Zheng, Jessie A Elliott, John V Reynolds, Sheraz R Markar, BartÅomiej W. PapieÅ¼, ENSURE study group

Esophageal cancer is a major cause of cancer-related mortality
internationally, with high recurrence rates and poor survival even among
patients treated with curative-intent surgery. Investigating relevant
prognostic factors and predicting prognosis can enhance post-operative clinical
decision-making and potentially improve patients' outcomes. In this work, we
assessed prognostic factor identification and discriminative performances of
three models for Disease-Free Survival (DFS) and Overall Survival (OS) using a
large multicenter international dataset from ENSURE study. We first employed
Cox Proportional Hazards (CoxPH) model to assess the impact of each feature on
outcomes. Subsequently, we utilised CoxPH and two deep neural network
(DNN)-based models, DeepSurv and DeepHit, to predict DFS and OS. The
significant prognostic factors identified by our models were consistent with
clinical literature, with post-operative pathologic features showing higher
significance than clinical stage features. DeepSurv and DeepHit demonstrated
comparable discriminative accuracy to CoxPH, with DeepSurv slightly
outperforming in both DFS and OS prediction tasks, achieving C-index of 0.735
and 0.74, respectively. While these results suggested the potential of DNNs as
prognostic tools for improving predictive accuracy and providing personalised
guidance with respect to risk stratification, CoxPH still remains an adequately
good prediction model, with the data used in this study.

æè¦ï¼é£éçæ¯åééççç¸éæ­»äº¡çä¸»è¦åå ï¼å³ä½¿æ¥åæ ¹æ²»æ§æè¡çæ£èï¼å¾©ç¼çé«ä¸å­æ´»çä½ãæ¢è¨ç¸éé å¾å å­ä¸¦é æ¸¬é å¾ï¼å¯ä»¥å¢é²è¡å¾è¨åºæ±ºç­å¶å®ï¼ä¸¦å¯è½æ¹åæ£èççµæãå¨éé å·¥ä½ä¸­ï¼æåè©ä¼°äºä¸ç¨®æ¨¡åçé å¾å å­è­å¥åå¤å¥æè½ï¼åå¥éå°ç¡ç¾çå­æ´»æ (DFS) åæ´é«å­æ´»æ (OS)ï¼ä½¿ç¨ ENSURE ç ç©¶çå¤§åå¤ä¸­å¿åéæ¸æéãæåé¦åæ¡ç¨ Cox æ¯ä¾é¢¨éª (CoxPH) æ¨¡åä¾è©ä¼°æ¯åç¹å¾µå°çµæçå½±é¿ãé¨å¾ï¼æåå©ç¨ CoxPH åå©ååºæ¼æ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) çæ¨¡å DeepSurv å DeepHit ä¾é æ¸¬ DFS å OSãæåçæ¨¡åè­å¥åºçé¡¯èé å¾å å­èè¨åºæç»ä¸è´ï¼è¡å¾ççç¹å¾µé¡¯ç¤ºåºæ¯è¨åºåæç¹å¾µæ´é«çé¡¯èæ§ãDeepSurv å DeepHit å±ç¾åºè CoxPH ç¸ç¶çå¤å¥æºç¢ºåº¦ï¼DeepSurv å¨ DFS å OS é æ¸¬ä»»åä¸­è¡¨ç¾ç¥åä¸ç±ï¼åå¥éå° C ææ¸ 0.735 å 0.74ãéç¶éäºçµæè¡¨æ DNN ä½çºé å¾å·¥å·çæ½åï¼å¯ä»¥æé«é æ¸¬æºç¢ºåº¦ä¸¦éå°é¢¨éªåå±¤æä¾åäººåæå°ï¼ä½ CoxPH ä»ç¶æ¯è¶³å¤ å¥½çé æ¸¬æ¨¡åï¼ä½¿ç¨æ¬ç ç©¶ä¸­çæ¸æã

##### **NDP: Next Distribution Prediction as a More Broad Target**
2408.17377v1 by Junhao Ruan, Abudukeyumu Abudula, Xinyu Liu, Bei Li, Yinqiao Li, Chenglong Wang, Yuchun Fan, Yuan Ge, Tong Xiao, Jingbo Zhu

Large language models (LLMs) trained on next-token prediction (NTP) paradigm
have demonstrated powerful capabilities. However, the existing NTP paradigm
contains several limitations, particularly related to planned task
complications and error propagation during inference. In our work, we extend
the critique of NTP, highlighting its limitation also due to training with a
narrow objective: the prediction of a sub-optimal one-hot distribution. To
support this critique, we conducted a pre-experiment treating the output
distribution from powerful LLMs as efficient world data compression. By
evaluating the similarity between the $n$-gram distribution and the one-hot
distribution with LLMs, we observed that the $n$-gram distributions align more
closely with the output distribution of LLMs. Based on this insight, we
introduce Next Distribution Prediction (NDP), which uses $n$-gram distributions
to replace the one-hot targets, enhancing learning without extra online
training time. We conducted experiments across translation, general task,
language transfer, and medical domain adaptation. Compared to NTP, NDP can
achieve up to +2.97 COMET improvement in translation tasks, +0.61 average
improvement in general tasks, and incredible +10.75 average improvement in the
medical domain. This demonstrates the concrete benefits of addressing the
target narrowing problem, pointing to a new direction for future work on
improving NTP.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) æ ¹æä¸ä¸åç¬¦èé æ¸¬ (NTP) ç¯ä¾é²è¡è¨ç·´ï¼å·²å±ç¾å¼·å¤§çåè½ãç¶èï¼ç¾æç NTP ç¯ä¾åå«äºå¹¾åéå¶ï¼ç¹å¥æ¯èè¨ç«ä»»åè¤éæ§åæ¨è«æéçé¯èª¤å³æ­æéãå¨æåçç ç©¶ä¸­ï¼æåæ´å±äº NTP çæ¹å¤ï¼å¼·èª¿å¶éå¶ä¹ç±æ¼ä½¿ç¨ç¹éçç®æ¨é²è¡è¨ç·´ï¼é æ¸¬æ¬¡ä½³çä¸ç±åä½ãçºäºæ¯æéé æ¹å¤ï¼æåé²è¡äºä¸ååç½®å¯¦é©ï¼å°å¼·å¤§ LLM çè¼¸åºåä½è¦çºææçä¸çè³æå£ç¸®ãééè©ä¼° $n$-gram åä½è LLM çä¸ç±åä½ä¹éçç¸ä¼¼æ§ï¼æåè§å¯å° $n$-gram åä½è LLM çè¼¸åºåä½æ´çºæ¥è¿ãåºæ¼éåè¦è§£ï¼æåå¼å¥äºä¸ä¸ååä½é æ¸¬ (NDP)ï¼å®ä½¿ç¨ $n$-gram åä½ä¾åä»£ä¸ç±ç®æ¨ï¼å¨æ²æé¡å¤ç·ä¸è¨ç·´æéçææ³ä¸ï¼å å¼·å­¸ç¿ãæåé²è¡äºç¿»è­¯ãä¸è¬ä»»åãèªè¨è½ç§»åé«å­¸é åé©æçå¯¦é©ãè NTP ç¸æ¯ï¼NDP å¨ç¿»è­¯ä»»åä¸­å¯ä»¥éå° +2.97 COMET æ¹é²ï¼å¨ä¸è¬ä»»åä¸­å¹³åæ¹é² +0.61ï¼å¨é«å­¸é åä¸­ä»¤äººé£ä»¥ç½®ä¿¡çå¹³åæ¹é² +10.75ãéè­æäºè§£æ±ºç®æ¨æ¶çªåé¡çå·é«å¥½èï¼æåºäºæ¹é² NTP æªä¾å·¥ä½çå¨æ°æ¹åã

##### **Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**
2408.17011v2 by Jutika Borah, Kumaresh Sarmah, Hidam Kumarjit Singh

Imaging techniques such as Chest X-rays, whole slide images, and optical
coherence tomography serve as the initial screening and detection for a wide
variety of medical pulmonary and ophthalmic conditions respectively. This paper
investigates the intricacies of using pretrained deep convolutional neural
networks with transfer learning across diverse medical imaging datasets with
varying modalities for binary and multiclass classification. We conducted a
comprehensive performance analysis with ten network architectures and model
families each with pretraining and random initialization. Our finding showed
that the use of pretrained models as fixed feature extractors yields poor
performance irrespective of the datasets. Contrary, histopathology microscopy
whole slide images have better performance. It is also found that deeper and
more complex architectures did not necessarily result in the best performance.
This observation implies that the improvements in ImageNet are not parallel to
the medical imaging tasks. Within a medical domain, the performance of the
network architectures varies within model families with shifts in datasets.
This indicates that the performance of models within a specific modality may
not be conclusive for another modality within the same domain. This study
provides a deeper understanding of the applications of deep learning techniques
in medical imaging and highlights the impact of pretrained networks across
different medical imaging datasets under five different experimental settings.

æè¦ï¼å½±åæè¡ï¼ä¾å¦è¸é¨ X åãå¨åçå½±åååå­¸ç¸å¹²æ·å±¤ææï¼åå¥ä½çºåç¨®é«å­¸èºé¨åç¼ç§ç¾ççåæ­¥ç¯©æª¢ååµæ¸¬ãæ¬ææ¢è¨äºä½¿ç¨é è¨ç·´æ·±åº¦å·ç©ç¥ç¶ç¶²è·¯æ­éé·ç§»å­¸ç¿ï¼æ©«è·¨ä¸åé«çå½±åè³æéï¼ä»¥é²è¡äºååå¤é¡å¥åé¡çè¤éæ§ãæåå°åç¨®ç¶²è·¯æ¶æ§åæ¨¡åç³»åé²è¡äºå¨é¢çæè½åæï¼æ¯åæ¶æ§åç³»åé½ç¶éé è¨ç·´åé¨æ©åå§åãæåçç¼ç¾é¡¯ç¤ºï¼å°é è¨ç·´æ¨¡åç¨ä½åºå®ç¹å¾µèåå¨æç¢çä¸ä½³çæè½ï¼èè³æéç¡éãç¸åå°ï¼çµç¹ççå­¸é¡¯å¾®é¡å¨åçå½±åæè¼å¥½çæè½ãæåä¹ç¼ç¾ï¼è¼æ·±ä¸è¤éçæ¶æ§ä¸¦éä¸å®æç¢çæä½³æè½ãæ­¤è§å¯çµææå³è ImageNet çæ¹è¯ä¸¦æªèé«çå½±åä»»åå¹³è¡ãå¨é«çé åå§ï¼ç¶²è·¯æ¶æ§çæè½æé¨èè³æéçè½æèæ¹è®æ¨¡åç³»åãéè¡¨ç¤ºå¨ç¹å®æ¨¡å¼ä¸­æ¨¡åçæè½å¯è½ç¡æ³æ±ºå®å¨åä¸åé åä¸­å¦ä¸ç¨®æ¨¡å¼çæè½ãæ¬ç ç©¶æä¾äºå°æ·±åº¦å­¸ç¿æè¡å¨é«çå½±åä¸­çæç¨æ´æ·±å¥ççè§£ï¼ä¸¦å¼·èª¿äºé è¨ç·´ç¶²è·¯å¨äºç¨®ä¸åå¯¦é©è¨­å®ä¸è·¨ä¸åé«çå½±åè³æéçå½±é¿ã

##### **A Survey for Large Language Models in Biomedicine**
2409.00133v1 by Chong Wang, Mengyao Li, Junjun He, Zhongruo Wang, Erfan Darzi, Zan Chen, Jin Ye, Tianbin Li, Yanzhou Su, Jing Ke, Kaili Qu, Shuxin Li, Yi Yu, Pietro LiÃ², Tianyun Wang, Yu Guang Wang, Yiqing Shen

Recent breakthroughs in large language models (LLMs) offer unprecedented
natural language understanding and generation capabilities. However, existing
surveys on LLMs in biomedicine often focus on specific applications or model
architectures, lacking a comprehensive analysis that integrates the latest
advancements across various biomedical domains. This review, based on an
analysis of 484 publications sourced from databases including PubMed, Web of
Science, and arXiv, provides an in-depth examination of the current landscape,
applications, challenges, and prospects of LLMs in biomedicine, distinguishing
itself by focusing on the practical implications of these models in real-world
biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot
learning across a broad spectrum of biomedical tasks, including diagnostic
assistance, drug discovery, and personalized medicine, among others, with
insights drawn from 137 key studies. Then, we discuss adaptation strategies of
LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to
enhance their performance in specialized biomedical contexts where zero-shot
fails to achieve, such as medical question answering and efficient processing
of biomedical literature. Finally, we discuss the challenges that LLMs face in
the biomedicine domain including data privacy concerns, limited model
interpretability, issues with dataset quality, and ethics due to the sensitive
nature of biomedical data, the need for highly reliable model outputs, and the
ethical implications of deploying AI in healthcare. To address these
challenges, we also identify future research directions of LLM in biomedicine
including federated learning methods to preserve data privacy and integrating
explainable AI methodologies to enhance the transparency of LLMs.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çææ°çªç ´æä¾äºåææªæçèªç¶èªè¨çè§£åçæè½åãç¶èï¼ç¾æéæ¼çç©é«å­¸ä¸­ LLM çèª¿æ¥éå¸¸å°æ³¨æ¼ç¹å®æç¨ææ¨¡åæ¶æ§ï¼ç¼ºä¹æ´ååç¨®çç©é«å­¸é åææ°é²å±çå¨é¢åæãæ¬ç¶è¿°åºæ¼å°ä¾èª PubMedãWeb of Science å arXiv ç­æ¸æåº«ç 484 ç¯åºçç©çåæï¼æ·±å¥æ¢è¨äºçç©é«å­¸ä¸­ LLM çç¶åç¾æ³ãæç¨ãææ°ååæ¯ï¼å¶ç¹é»æ¯éæ³¨éäºæ¨¡åå¨ç¾å¯¦ä¸ççç©é«å­¸èæ¯ä¸­çå¯¦éæç¨ãé¦åï¼æåæ¢è¨äº LLM å¨å»£æ³ççç©é«å­¸ä»»åä¸­çé¶æ¬¡å­¸ç¿è½åï¼åæ¬è¨ºæ·è¼å©ãè¥ç©ç¼ç¾ååæ§åé«çç­ï¼ä¸¦å¾ 137 é ééµç ç©¶ä¸­æ±²åè¦è§£ãç¶å¾ï¼æåè¨è«äº LLM çé©æç­ç¥ï¼åæ¬å®æ¨¡æåå¤æ¨¡æ LLM çå¾®èª¿æ¹æ³ï¼ä»¥å¢å¼·å®åå¨é¶æ¬¡å­¸ç¿ç¡æ³å¯¦ç¾çå°æ¥­çç©é«å­¸èæ¯ä¸­çæ§è½ï¼ä¾å¦é«çåé¡è§£ç­åçç©é«å­¸æç»çææèçãæå¾ï¼æåè¨è«äº LLM å¨çç©é«å­¸é åé¢è¨çææ°ï¼åæ¬æ¸æé±ç§åé¡ãæ¨¡åå¯è§£éæ§æéãæ¸æéè³ªéåé¡ä»¥åç±æ¼çç©é«å­¸æ¸æçæææ§ãå°é«åº¦å¯é æ¨¡åè¼¸åºçéæ±ä»¥åå¨é«çä¿å¥ä¸­é¨ç½² AI çå«çå½±é¿èç¢ççå«çåé¡ãçºäºæå°éäºææ°ï¼æåéç¢ºå®äºçç©é«å­¸ä¸­ LLM æªä¾çç ç©¶æ¹åï¼åæ¬ç¨æ¼ä¿è­·æ¸æé±ç§çè¯åå­¸ç¿æ¹æ³ä»¥åæ´åå¯è§£é AI æ¹æ³ä»¥å¢å¼· LLM çéæåº¦ã

##### **Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**
2408.16343v1 by Yifei Chen, Shenghao Zhu, Zhaojie Fang, Chang Liu, Binfeng Zou, Yuhe Wang, Shuo Chang, Fan Jia, Feiwei Qin, Jin Fan, Yong Peng, Changmiao Wang

Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by
memory loss, executive dysfunction, and personality changes. Early diagnosis is
challenging due to subtle symptoms and varied presentations, often leading to
misdiagnosis with traditional unimodal diagnostic methods due to their limited
scope. This study introduces an advanced multimodal classification model that
integrates clinical, cognitive, neuroimaging, and EEG data to enhance
diagnostic accuracy. The model incorporates a feature tagger with a tabular
data coding architecture and utilizes the TimesBlock module to capture
intricate temporal patterns in Electroencephalograms (EEG) data. By employing
Cross-modal Attention Aggregation module, the model effectively fuses Magnetic
Resonance Imaging (MRI) spatial information with EEG temporal data,
significantly improving the distinction between AD, Mild Cognitive Impairment,
and Normal Cognition. Simultaneously, we have constructed the first AD
classification dataset that includes three modalities: EEG, MRI, and tabular
data. Our innovative approach aims to facilitate early diagnosis and
intervention, potentially slowing the progression of AD. The source code and
our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.

æè¦ï¼é¿è²æµ·é»ç (AD) æ¯ä¸ç¨®è¤éçç¥ç¶éåæ§ç¾çï¼ç¹å¾µæ¯è¨æ¶ååªå¤±ãå·è¡åè½éç¤åäººæ ¼æ¹è®ãç±æ¼ççå¾®å¦ä¸è¡¨ç¾å½¢å¼å¤æ¨£ï¼æ©æè¨ºæ·å·æææ°æ§ï¼éå¸¸ç±æ¼å³çµ±å®æ¨¡æè¨ºæ·æ¹æ³çç¯åæéèå°è´èª¤è¨ºãæ¬ç ç©¶å¼å¥äºä¸ååé²çå¤æ¨¡æåé¡æ¨¡åï¼å®æ´åäºè¨åºãèªç¥ãç¥ç¶å½±ååè¦é»åæ¸æï¼ä»¥æé«è¨ºæ·æºç¢ºæ§ãè©²æ¨¡åçµåäºä¸åå·æè¡¨æ ¼æ¸æç·¨ç¢¼æ¶æ§çç¹å¾µæ¨ç±¤å¨ï¼ä¸¦å©ç¨ TimesBlock æ¨¡çµä¾ææè¦é»å (EEG) æ¸æä¸­çè¤éæéæ¨¡å¼ãééæ¡ç¨è·¨æ¨¡ææ³¨æåèåæ¨¡çµï¼è©²æ¨¡åææå°èåäºç£å±æ¯æå (MRI) ç©ºéè³è¨åè¦é»åæéæ¸æï¼é¡¯èæ¹åäº ADãè¼åº¦èªç¥éç¤åæ­£å¸¸èªç¥ä¹éçåå¥ãåæï¼æåæ§å»ºäºç¬¬ä¸å AD åé¡æ¸æéï¼å¶ä¸­åå«ä¸ç¨®æ¨¡æï¼è¦é»åãç£å±æ¯æååè¡¨æ ¼æ¸æãæåçåµæ°æ¹æ³æ¨å¨ä¿é²æ©æè¨ºæ·åå¹²é ï¼æ½å¨å°æ¸ç·© AD çé²å±ãåå§ç¢¼åæåçç§äºº ADMC æ¸æéå¯å¨ https://github.com/JustlfC03/MSTNet ç²å¾ã

##### **Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**
2408.16256v1 by Xia Jiang, Yijun Zhou, Alan Wells, Adam Brufsky

Breast cancer is one of the two cancers responsible for the most deaths in
women, with about 42,000 deaths each year in the US. That there are over
300,000 breast cancers newly diagnosed each year suggests that only a fraction
of the cancers result in mortality. Thus, most of the women undergo seemingly
curative treatment for localized cancers, but a significant later succumb to
metastatic disease for which current treatments are only temporizing for the
vast majority. The current prognostic metrics are of little actionable value
for 4 of the 5 women seemingly cured after local treatment, and many women are
exposed to morbid and even mortal adjuvant therapies unnecessarily, with these
adjuvant therapies reducing metastatic recurrence by only a third. Thus, there
is a need for better prognostics to target aggressive treatment at those who
are likely to relapse and spare those who were actually cured. While there is a
plethora of molecular and tumor-marker assays in use and under-development to
detect recurrence early, these are time consuming, expensive and still often
un-validated as to actionable prognostic utility. A different approach would
use large data techniques to determine clinical and histopathological
parameters that would provide accurate prognostics using existing data. Herein,
we report on machine learning, together with grid search and Bayesian Networks
to develop algorithms that present a AUC of up to 0.9 in ROC analyses, using
only extant data. Such algorithms could be rapidly translated to clinical
management as they do not require testing beyond routine tumor evaluations.

æè¦ï¼ä¹³çæ¯é æå¥³æ§æ­»äº¡äººæ¸æå¤çå©ç¨®ççä¹ä¸ï¼æ¯å¹´ç´æ 42,000 åå¥³æ§æ­»æ¼ä¹³çãæ¯å¹´æè¶é 300,000 ä¾ä¹³çæ°ç¢ºè¨ºï¼éè¡¨ç¤ºåªæå°é¨åççæå°è´æ­»äº¡ãå æ­¤ï¼å¤§å¤æ¸å¥³æ§æ¥åå±é¨çççæ ¹æ²»æ§æ²»çï¼ä½è¨±å¤äººå¾ä¾ä»ææ­»æ¼è½ç§»æ§ç¾çï¼èç®åçæ²»çæ¹æ³å°çµå¤§å¤æ¸æ£èä¾èªªåªæ¯æ«æçãç®åçé å¾ææ¨å°æ¼ 5 åæ¥åå±é¨æ²»çå¾çä¼¼æ²»ççå¥³æ§ä¸­ï¼æ 4 åå¹¾ä¹æ²æå¯¦éå¹å¼ï¼è¨±å¤å¥³æ§ä¸å¿è¦å°æ¥åçæçè³è´å½çè¼å©çæ³ï¼èéäºè¼å©çæ³åè½å°è½ç§»æ§å¾©ç¼çéä½ä¸åä¹ä¸ãå æ­¤ï¼éè¦æ´å¥½çé å¾ææ¨ï¼æè½éå°é£äºå¯è½å¾©ç¼çäººé²è¡ç©æ¥µæ²»çï¼ä¸¦é¿åé£äºå¯¦éä¸å·²ç¶æ²»ççäººæ¥åæ²»çãéç¶æè¨±å¤åå­åè«ç¤æ¨è¨æª¢æ¸¬æ¹æ³æ­£å¨ä½¿ç¨åéç¼ä¸­ï¼å¯ä»¥åæ©ç¼ç¾å¾©ç¼ï¼ä½éäºæ¹æ³èæãæè²´ï¼èä¸ä½çºå¯æä½çé å¾å·¥å·ï¼å¶æç¨ä»ç¶å¸¸å¸¸æªç¶é©è­ãå¦ä¸ç¨®æ¹æ³æä½¿ç¨å¤§éçè³ææè¡ï¼ä¾ç¢ºå®è¨åºåçµç¹ççå­¸åæ¸ï¼ä¸¦ä½¿ç¨ç¾æè³ææä¾æºç¢ºçé å¾ææ¨ãå¨æ­¤ï¼æåå ±åäºæ©å¨å­¸ç¿ï¼ä»¥åç¶²æ ¼æå°åè²æ°ç¶²è·¯ï¼ç¨ä¾éç¼æ¼ç®æ³ï¼å¨ ROC åæä¸­æä¾é«é 0.9 ç AUCï¼åä½¿ç¨ç¾æè³æãæ­¤é¡æ¼ç®æ³å¯ä»¥å¿«éè½æçºè¨åºç®¡çï¼å çºå®åä¸éè¦é²è¡å¸¸è¦è«ç¤è©ä¼°ä»¥å¤çæ¸¬è©¦ã

##### **M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**
2408.16213v1 by Jonggwon Park, Soobum Kim, Byungmu Yoon, Jihun Hyun, Kyoyun Choi

The rapid evolution of artificial intelligence, especially in large language
models (LLMs), has significantly impacted various domains, including
healthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,
but with limitations: either underutilizing the multi-tasking capabilities of
LLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM
designed to enhance CXR interpretation. The model is trained on a visual
instruction-following dataset that integrates various task-specific datasets in
a conversational format. As a result, the model supports multiple tasks such as
medical report generation (MRG), visual grounding, and visual question
answering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by
employing a chain-of-thought prompting strategy, in which it identifies
findings in CXR images and subsequently generates corresponding reports. The
model is adaptable to various MRG scenarios depending on the available inputs,
such as single-image, multi-image, and multi-study contexts. In addition to
MRG, M4CXR performs visual grounding at a level comparable to specialized
models and also demonstrates outstanding performance in VQA. Both quantitative
and qualitative assessments reveal M4CXR's versatility in MRG, visual
grounding, and VQA, while consistently maintaining clinical accuracy.

æè¦ï¼äººå·¥æºæ§çå¿«éç¼å±ï¼ç¹å¥æ¯å¨å¤§åèªè¨æ¨¡å (LLM) ä¸­ï¼å·²å°åæ¬é«çä¿å¥å¨å§çååé åç¢çéå¤§å½±é¿ãå¨è¸é¨ X å (CXR) åæä¸­ï¼ååçç ç©¶å·²æ¡ç¨ LLMï¼ä½æå¶éå¶ï¼ä¸æ¯æªè½ååå©ç¨ LLM çå¤ä»»åèçè½åï¼å°±æ¯ç¼ºä¹è¨åºæºç¢ºæ§ãæ¬ææåº M4CXRï¼ä¸ç¨®å¤æ¨¡æ LLMï¼æ¨å¨å¢å¼· CXR è§£éãè©²æ¨¡åè¨ç·´æ¼è¦è¦ºæä»¤éµå¾ªè³æéï¼å¶ä¸­ä»¥å°è©±æ ¼å¼æ´ååç¨®ç¹å®ä»»åè³æéãå æ­¤ï¼è©²æ¨¡åæ¯æ´å¤é ä»»åï¼ä¾å¦é«çå ±åç¢ç (MRG)ãè¦è¦ºåºç¤åè¦è¦ºåé¡åç­ (VQA)ãM4CXR ééæ¡ç¨æèéæç¤ºç­ç¥ï¼å¨ MRG ä¸­éææåé²çè¨åºæºç¢ºæ§ï¼å¶ä¸­å®æè­å¥ CXR å½±åä¸­çç¼ç¾ï¼ä¸¦é¨å¾ç¢çå°æçå ±åãè©²æ¨¡åå¯æ ¹æå¯ç¨è¼¸å¥ï¼ä¾å¦å®ä¸å½±åãå¤éå½±ååå¤éç ç©¶èçµ¡ï¼é©æåç¨® MRG æå¢ãé¤äº MRG ä¹å¤ï¼M4CXR ä»¥èå°éæ¨¡åç¸ç¶çå±¤ç´å·è¡è¦è¦ºåºç¤ï¼ä¸¦å¨ VQA ä¸­å±ç¾åºè²çæè½ãå®éåå®æ§è©ä¼°åé¡¯ç¤ºåº M4CXR å¨ MRGãè¦è¦ºåºç¤å VQA ä¸­çå¤åè½æ§ï¼åææçºç¶­æè¨åºæºç¢ºæ§ã

##### **A Survey on Evaluation of Multimodal Large Language Models**
2408.15769v1 by Jiaxing Huang, Jingyi Zhang

Multimodal Large Language Models (MLLMs) mimic human perception and reasoning
system by integrating powerful Large Language Models (LLMs) with various
modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and
various modality encoders as sensory organs. This framework endows MLLMs with
human-like capabilities, and suggests a potential pathway towards achieving
artificial general intelligence (AGI). With the emergence of all-round MLLMs
like GPT-4V and Gemini, a multitude of evaluation methods have been developed
to assess their capabilities across different dimensions. This paper presents a
systematic and comprehensive review of MLLM evaluation methods, covering the
following key aspects: (1) the background of MLLMs and their evaluation; (2)
"what to evaluate" that reviews and categorizes existing MLLM evaluation tasks
based on the capabilities assessed, including general multimodal recognition,
perception, reasoning and trustworthiness, and domain-specific applications
such as socioeconomic, natural sciences and engineering, medical usage, AI
agent, remote sensing, video and audio processing, 3D point cloud analysis, and
others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into
general and specific benchmarks; (4) "how to evaluate" that reviews and
illustrates MLLM evaluation steps and metrics; Our overarching goal is to
provide valuable insights for researchers in the field of MLLM evaluation,
thereby facilitating the development of more capable and reliable MLLMs. We
emphasize that evaluation should be regarded as a critical discipline,
essential for advancing the field of MLLMs.

æè¦ï¼å¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM) ééæ´åå¼·å¤§çå¤§åèªè¨æ¨¡å (LLM) èåç¨®æ¨¡æç·¨ç¢¼å¨ï¼ä¾å¦è¦è¦ºãé³è¨ï¼ï¼æ¨¡æ¬äººé¡çæç¥åæ¨çç³»çµ±ï¼å° LLM å®ä½çºãå¤§è¦ãï¼èå°åç¨®æ¨¡æç·¨ç¢¼å¨å®ä½çºæå®å¨å®ãæ­¤æ¶æ§è³¦äº MLLM é¡ä¼¼äººé¡çè½åï¼ä¸¦æåºå¯¦ç¾äººå·¥éç¨æºæ§ (AGI) çæ½å¨éå¾ãé¨è GPT-4V å Gemini ç­å¨æ¹ä½ MLLM çåºç¾ï¼å·²ç¶éç¼åºå¤ç¨®è©ä¼°æ¹æ³ä¾è©ä¼°å®åå¨ä¸åç¶­åº¦ä¸çè½åãæ¬æå° MLLM è©ä¼°æ¹æ³é²è¡äºç³»çµ±ä¸å¨é¢çåé¡§ï¼æ¶µèä»¥ä¸å¹¾åééµé¢åï¼(1) MLLM åå¶è©ä¼°çèæ¯ï¼(2)ãè¦è©ä¼°ä»éº¼ãæ ¹æè©ä¼°çè½åï¼åé¡§ä¸¦åé¡ç¾æç MLLM è©ä¼°ä»»åï¼åæ¬ä¸è¬å¤æ¨¡æè¾¨è­ãæç¥ãæ¨çåå¯ä¿¡åº¦ï¼ä»¥åç¹å®é åçæç¨ï¼ä¾å¦ç¤¾æç¶æ¿ãèªç¶ç§å­¸åå·¥ç¨ãé«çç¨éãAI ä»£çãéæ¸¬ãå½±çåé³è¨èçã3D é»é²åæç­ï¼(3)ãå¨åªè£¡è©ä¼°ãå° MLLM è©ä¼°åºæºç¸½çµçºä¸è¬åºæºåç¹å®åºæºï¼(4)ãå¦ä½è©ä¼°ãåé¡§ä¸¦èªªæ MLLM è©ä¼°æ­¥é©åææ¨ãæåçé¦è¦ç®æ¨æ¯çº MLLM è©ä¼°é åçç ç©¶äººå¡æä¾æå¹å¼çè¦è§£ï¼å¾èä¿é²æ´å¼·å¤§ä¸å¯é ç MLLM çéç¼ãæåå¼·èª¿è©ä¼°æè¢«è¦çºä¸é ééµçå­¸ç§ï¼å°æ¼æ¨é² MLLM é åè³ééè¦ã

##### **Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**
2408.15498v1 by Yijun Zhou, Om Arora-Jain, Xia Jiang

While machine learning has advanced in medicine, its widespread use in
clinical applications, especially in predicting breast cancer metastasis, is
still limited. We have been dedicated to constructing a DFNN model to predict
breast cancer metastasis n years in advance. However, the challenge lies in
efficiently identifying optimal hyperparameter values through grid search,
given the constraints of time and resources. Issues such as the infinite
possibilities for continuous hyperparameters like l1 and l2, as well as the
time-consuming and costly process, further complicate the task. To address
these challenges, we developed Single Hyperparameter Grid Search (SHGS)
strategy, serving as a preselection method before grid search. Our experiments
with SHGS applied to DFNN models for breast cancer metastasis prediction focus
on analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2,
learning rate, decay, and momentum. We created three figures, each depicting
the experiment results obtained from three LSM-I-10-Plus-year datasets. These
figures illustrate the relationship between model performance and the target
hyperparameter values. For each hyperparameter, we analyzed whether changes in
this hyperparameter would affect model performance, examined if there were
specific patterns, and explored how to choose values for the particular
hyperparameter. Our experimental findings reveal that the optimal value of a
hyperparameter is not only dependent on the dataset but is also significantly
influenced by the settings of other hyperparameters. Additionally, our
experiments suggested some reduced range of values for a target hyperparameter,
which may be helpful for low-budget grid search. This approach serves as a
prior experience and foundation for subsequent use of grid search to enhance
model performance.

æè¦ï¼åç®¡æ©å¨å­¸ç¿å¨é«å­¸é åå·²ææé²å±ï¼ä½å¶å¨è¨åºæç¨ä¸­çå»£æ³ä½¿ç¨ï¼ç¹å¥æ¯å¨é æ¸¬ä¹³çè½ç§»æ¹é¢ï¼ä»æå¶éå¶ãæåè´åæ¼å»ºæ§ DFNN æ¨¡åï¼ä»¥é æ¸¬ä¹³çè½ç§» n å¹´ãç¶èï¼ææ°å¨æ¼ééç¶²æ ¼æå°ææçå°æ¾åºæä½³è¶åæ¸å¼ï¼éåå°æéåè³æºçéå¶ãè«¸å¦ l1 å l2 ç­é£çºè¶åæ¸çå¯è½æ§ç¡çª®ï¼ä»¥åèæä¸æè²´çéç¨ç­åé¡ï¼æ´è®éé ä»»åè®å¾è¤éãçºäºæå°éäºææ°ï¼æåéç¼äºå®ä¸è¶åæ¸ç¶²æ ¼æå° (SHGS) ç­ç¥ï¼ä½çºç¶²æ ¼æå°åçé é¸æ¹æ³ãæåéå°ä¹³çè½ç§»é æ¸¬æç¨ç DFNN æ¨¡åé²è¡ SHGS å¯¦é©ï¼éé»åæå«åç®æ¨è¶åæ¸ï¼epoch æ¬¡æ¸ãæ¹æ¬¡å¤§å°ãä¸­æ·ãL1ãL2ãå­¸ç¿çãè¡°æ¸ååéãæåè£½ä½äºä¸å¹åï¼æ¯å¹åé½æç¹ªäºå¾ä¸å LSM-I-10-Plus-year è³æéç²å¾çå¯¦é©çµæãéäºåè¡¨èªªæäºæ¨¡åæè½èç®æ¨è¶åæ¸å¼ä¹éçéä¿ãå°æ¼æ¯åè¶åæ¸ï¼æååæäºè¶åæ¸çè®åæ¯å¦æå½±é¿æ¨¡åæè½ï¼ä¸¦æª¢è¦æ¯å¦æç¹å®æ¨¡å¼ï¼ä»¥åå¦ä½éå°ç¹å®è¶åæ¸é¸æå¼ãæåçå¯¦é©çµæé¡¯ç¤ºï¼è¶åæ¸çæä½³å¼ä¸ååæ±ºæ¼è³æéï¼ä¹åå°å¶ä»è¶åæ¸è¨­å®çé¡¯èå½±é¿ãæ­¤å¤ï¼æåçå¯¦é©å»ºè­°ç¸®å°ç®æ¨è¶åæ¸å¼çç¯åï¼éå¯è½æå©æ¼ä½é ç®çç¶²æ ¼æå°ãæ­¤æ¹æ³å¯ä½çºå¾çºä½¿ç¨ç¶²æ ¼æå°ä»¥å¢å¼·æ¨¡åæè½çååç¶é©ååºç¤ã

##### **What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**
2408.15354v1 by Jana Schaich Borg, Hannah Read

Interest is growing in artificial empathy, but so is confusion about what
artificial empathy is or needs to be. This confusion makes it challenging to
navigate the technical and ethical issues that accompany empathic AI
development. Here, we outline a framework for thinking about empathic AI based
on the premise that different constellations of capabilities associated with
empathy are important for different empathic AI applications. We describe
distinctions of capabilities that we argue belong under the empathy umbrella,
and show how three medical empathic AI use cases require different sets of
these capabilities. We conclude by discussing why appreciation of the diverse
capabilities under the empathy umbrella is important for both AI creators and
users.

æè¦ï¼å°äººå·¥åçå¿è¶ä¾è¶æèè¶£ï¼ä½å°æ¼äººå·¥åçå¿æ¯ä»éº¼æéè¦ä»éº¼ä¹è¶ä¾è¶å°æãéç¨®æ··æ·ä½¿å¾é£ä»¥è§£æ±ºä¼´é¨åçå¿ AI éç¼èä¾çæè¡åå«çåé¡ãå¨æ­¤ï¼æåæ¦è¿°äºä¸åæèåçå¿ AI çæ¶æ§ï¼å¶åºæ¼éæ¨£ä¸ååæï¼èåçå¿ç¸éçä¸åè½åçµåå°æ¼ä¸åçåçå¿ AI æç¨å¾éè¦ãæåæè¿°äºæåèªçºå±¬æ¼åçå¿ç¯ççè½ååå¥ï¼ä¸¦å±ç¤ºäºä¸åé«çåçå¿ AI ä½¿ç¨æ¡ä¾éè¦éäºè½åçä¸åçµåãæåæå¾è¨è«äºçºä»éº¼æ¬£è³åçå¿ç¯çä¸çåç¨®è½åå°æ¼ AI åµé èåä½¿ç¨èé½å¾éè¦çåå ã

##### **Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**
2408.15217v1 by Weiyi Zhang, Siyu Huang, Jiancheng Yang, Ruoyu Chen, Zongyuan Ge, Yingfeng Zheng, Danli Shi, Mingguang He

Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal
vascular dynamics and aiding in the diagnosis of eye diseases. However, its
invasive nature and less accessibility compared to Color Fundus (CF) images
pose significant challenges. Current CF to FFA translation methods are limited
to static generation. In this work, we pioneer dynamic FFA video generation
from static CF images. We introduce an autoregressive GAN for smooth,
memory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic
lesion changes in FFA regions, we design a knowledge mask based on clinical
experience. Leveraging this mask, our approach integrates innovative knowledge
mask-guided techniques, including knowledge-boosted attention, knowledge-aware
discriminators, and mask-enhanced patchNCE loss, aimed at refining generation
in critical areas and addressing the pixel misalignment challenge. Our method
achieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common
video generation approaches. Human assessment by an ophthalmologist confirms
its high generation quality. Notably, our knowledge mask surpasses supervised
lesion segmentation masks, offering a promising non-invasive alternative to
traditional FFA for research and clinical applications. The code is available
at https://github.com/Michi-3000/Fundus2Video.

æè¦ï¼ç¼åºè¢åè¡ç®¡æå½± (FFA) æ¯è©ä¼°è¦ç¶²èè¡ç®¡ååå­¸ååå©è¨ºæ·ç¼ç¾çéè¦å·¥å·ãç¶èï¼èå½©è²ç¼åº (CF) å½±åç¸æ¯ï¼å¶ä¾µå¥æ§è¼é«ä¸åå¾ä¸æï¼å æ­¤é æéå¤§ææ°ãç®å CF è½ææ FFA çç¿»è­¯æ¹æ³åéæ¼éæç¢çãå¨éé å·¥ä½ä¸­ï¼æåçåå¾éæ CF å½±åç¢çåæ FFA å½±çãæåå¼å¥ä¸åèªè¿´æ­¸ GANï¼ä»¥é²è¡æµæ¢ä¸ç¯çè¨æ¶é«çéå¹ FFA åæãçºäºå å¼·å° FFA ååä¸­åæçç¶è®åçéæ³¨ï¼æåæ ¹æè¨åºç¶é©è¨­è¨äºä¸åç¥è­é®ç½©ãééå©ç¨éåé®ç½©ï¼æåçåæ³æ´åäºåµæ°çç¥è­é®ç½©å¼å°æè¡ï¼åæ¬ç¥è­å¢å¼·çæ³¨æåãç¥è­æç¥çè¾¨å¥å¨ä»¥åé®ç½©å¢å¼·ç patchNCE æå¤±ï¼æ¨å¨æ¹åééµååççæä¸¦è§£æ±ºåç´ æªå°é½çææ°ãèå¶ä»å¸¸è¦çå½±ççææ¹æ³ç¸æ¯ï¼æåçåæ³éå°äºæä½³ç FVD 1503.21 å PSNR 11.81ãç¼ç§é«å¸«çäººçºè©ä¼°è­å¯¦äºå¶çæåè³ªå¾é«ãå¼å¾æ³¨æçæ¯ï¼æåçç¥è­é®ç½©è¶è¶äºæç£ç£ççç¶åå²é®ç½©ï¼çºå³çµ± FFA æä¾äºä¸åæåéçéä¾µå¥æ§æ¿ä»£æ¹æ¡ï¼å¯ç¨æ¼ç ç©¶åè¨åºæç¨ãç¨å¼ç¢¼å¯å¨ https://github.com/Michi-3000/Fundus2Video åå¾ã

##### **Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**
2409.00112v1 by Daniil Filienko, Yinzhou Wang, Caroline El Jazmi, Serena Xie, Trevor Cohen, Martine De Cock, Weichao Yuwen

While Large Language Models (LLMs) are being quickly adapted to many domains,
including healthcare, their strengths and pitfalls remain under-explored. In
our study, we examine the effects of prompt engineering to guide Large Language
Models (LLMs) in delivering parts of a Problem-Solving Therapy (PST) session
via text, particularly during the symptom identification and assessment phase
for personalized goal setting. We present evaluation results of the models'
performances by automatic metrics and experienced medical professionals. We
demonstrate that the models' capability to deliver protocolized therapy can be
improved with the proper use of prompt engineering methods, albeit with
limitations. To our knowledge, this study is among the first to assess the
effects of various prompting techniques in enhancing a generalist model's
ability to deliver psychotherapy, focusing on overall quality, consistency, and
empathy. Exploring LLMs' potential in delivering psychotherapy holds promise
with the current shortage of mental health professionals amid significant
needs, enhancing the potential utility of AI-based and AI-enhanced care
services.

æè¦ï¼é¨èå¤§åèªè¨æ¨¡åï¼LLMï¼å¿«éé©ææ¼è¨±å¤é åï¼åæ¬é«çä¿å¥ï¼å®åçåªå¢åç¼ºé·ä»æªå¾å°ååæ¢ç´¢ãå¨æåçç ç©¶ä¸­ï¼æåæ¢è¨äºæç¤ºå·¥ç¨å¨å¼å°å¤§åèªè¨æ¨¡åï¼LLMï¼ééæå­æä¾åé¡è§£æ±ºçæ³ï¼PSTï¼ç°ç¯çé¨åå§å®¹çææï¼ç¹å¥æ¯å¨ççè­å¥åè©ä¼°éæ®µï¼ç¨æ¼åæ§åç®æ¨è¨­å®ãæåééèªååææ¨åç¶é©è±å¯çé«çå°æ¥­äººå¡å±ç¤ºäºæ¨¡åæ§è½çè©ä¼°çµæãæåè­æäºæ¨¡åæä¾ç¨å¼åæ²»ççè½åå¯ä»¥ä½¿ç¨æç¤ºå·¥ç¨æ¹æ³é©ç¶ä½¿ç¨ä¾æ¹é²ï¼åç®¡æå±éæ§ãææåæç¥ï¼éé ç ç©¶æ¯ç¬¬ä¸æ¹è©ä¼°åç¨®æç¤ºæè¡å°å¢å¼·éææ¨¡åæä¾å¿çæ²»çè½åçå½±é¿çç ç©¶ä¹ä¸ï¼éé»éæ³¨æ´é«åè³ªãä¸è´æ§ååçå¿ãå¨å¿çå¥åº·å°æ¥­äººå¡å´éç­ç¼ºä¸éæ±å·¨å¤§çææ³ä¸ï¼æ¢ç´¢ LLM å¨æä¾å¿çæ²»çæ¹é¢çæ½åå¾æåæ¯ï¼å¢å¼·äºåºæ¼ AI å AI å¢å¼·çè­·çæåçæ½å¨æç¨ã

##### **Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**
2408.15121v1 by Francesco Sovrano, Michael Lognoul, Giulia Vilone

Significant investment and development have gone into integrating Artificial
Intelligence (AI) in medical and healthcare applications, leading to advanced
control systems in medical technology. However, the opacity of AI systems
raises concerns about essential characteristics needed in such sensitive
applications, like transparency and trustworthiness. Our study addresses these
concerns by investigating a process for selecting the most adequate Explainable
AI (XAI) methods to comply with the explanation requirements of key EU
regulations in the context of smart bioelectronics for medical devices. The
adopted methodology starts with categorising smart devices by their control
mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving
into their technology. Then, we analyse these regulations to define their
explainability requirements for the various devices and related goals.
Simultaneously, we classify XAI methods by their explanatory objectives. This
allows for matching legal explainability requirements with XAI explanatory
goals and determining the suitable XAI algorithms for achieving them. Our
findings provide a nuanced understanding of which XAI algorithms align better
with EU regulations for different types of medical devices. We demonstrate this
through practical case studies on different neural implants, from chronic
disease management to advanced prosthetics. This study fills a crucial gap in
aligning XAI applications in bioelectronics with stringent provisions of EU
regulations. It provides a practical framework for developers and researchers,
ensuring their AI innovations advance healthcare technology and adhere to legal
and ethical standards.

æè¦ï¼äººå·¥æºæ§ï¼AIï¼å¨é«çåä¿å¥æç¨ä¸­æå¥äºå¤§éçæè³åéç¼ï¼é²èå°è´é«çæè¡ä¸­çåé²æ§å¶ç³»çµ±ãç¶èï¼AI ç³»çµ±çä¸éææ§å¼ç¼äºå°æ­¤é¡æææç¨ä¸­æéåºæ¬ç¹æ§çææï¼ä¾å¦éæåº¦åå¯ä¿¡åº¦ãæåçç ç©¶ééèª¿æ¥ä¸åç¨åºä¾è§£æ±ºéäºåé¡ï¼ç¨æ¼é¸ææååçå¯è§£é AIï¼XAIï¼æ¹æ³ï¼ä»¥ç¬¦åæ­çæ³è¦å¨é«çå¨æçæºæ§åçç©é»å­å­¸ä¸­çèªªæè¦æ±ãæ¡ç¨çæ¹æ³å¾ééå¶æ§å¶æ©å¶ï¼éè¿´è·¯ãéè¿´è·¯ååéè¿´è·¯ç³»çµ±ï¼å°æºæ§åè£ç½®é²è¡åé¡ï¼ä¸¦æ·±å¥æ¢è¨å¶æè¡éå§ãç¶å¾ï¼æååæéäºæ³è¦ä»¥å®ç¾©å¶å°åç¨®è£ç½®åç¸éç®æ¨çå¯è§£éæ§è¦æ±ãåæï¼æåééå¶èªªæç®æ¨å° XAI æ¹æ³é²è¡åé¡ãéåè¨±å°æ³å¾å¯è§£éæ§è¦æ±è XAI èªªæç®æ¨ç¸å¹éï¼ä¸¦ç¢ºå®é©ç¶ç XAI æ¼ç®æ³ä¾éæå®åãæåçç ç©¶çµææä¾äºå°åªäº XAI æ¼ç®æ³æ´ç¬¦åæ­çæ³è¦ä»¥é©ç¨æ¼ä¸åé¡åçé«çå¨æçç´°ç·»çè§£ãæåééä¸åç¥ç¶æ¤å¥ç©çå¯¦éæ¡ä¾ç ç©¶ä¾è­æéä¸é»ï¼å¾æ¢æ§ç¾çç®¡çå°åé²çç¾©è¢ãéé ç ç©¶å¡«è£äºå°çç©é»å­å­¸ä¸­ç XAI æç¨èæ­çæ³è¦çå´æ ¼è¦å®ç¸ç¬¦çéè¦ç©ºç½ãå®çºéç¼äººå¡åç ç©¶äººå¡æä¾äºä¸åå¯¦ç¨çæ¶æ§ï¼ç¢ºä¿å¶ AI åµæ°è½ä¿é²é«çæè¡ä¸¦éµå®æ³å¾åéå¾·æ¨æºã

##### **MiWaves Reinforcement Learning Algorithm**
2408.15076v1 by Susobhan Ghosh, Yongyi Guo, Pei-Yao Hung, Lara Coughlin, Erin Bonar, Inbal Nahum-Shani, Maureen Walton, Susan Murphy

The escalating prevalence of cannabis use poses a significant public health
challenge globally. In the U.S., cannabis use is more prevalent among emerging
adults (EAs) (ages 18-25) than any other age group, with legalization in the
multiple states contributing to a public perception that cannabis is less risky
than in prior decades. To address this growing concern, we developed MiWaves, a
reinforcement learning (RL) algorithm designed to optimize the delivery of
personalized intervention prompts to reduce cannabis use among EAs. MiWaves
leverages domain expertise and prior data to tailor the likelihood of delivery
of intervention messages. This paper presents a comprehensive overview of the
algorithm's design, including key decisions and experimental outcomes. The
finalized MiWaves RL algorithm was deployed in a clinical trial from March to
May 2024.

æè¦ï¼å¤§éº»ä½¿ç¨çä¸æ·ä¸åï¼å°å¨çå¬å±è¡çæ§æéå¤§ææ°ãå¨ç¾åï¼å¤§éº»ä½¿ç¨çå¨å¹´è¼æå¹´äººï¼EAï¼ï¼18-25 æ­²ï¼ä¸­æ¯ä»»ä½å¶ä»å¹´é½¡çµé½è¦æ®éï¼å¤åå·çåæ³åå°è´å¬ç¾èªçºå¤§éº»æ¯éå»å¹¾åå¹´é¢¨éªè¼ä½ãçºäºè§£æ±ºéåæ¥çå´éçåé¡ï¼æåéç¼äº MiWavesï¼éæ¯ä¸ç¨®å¢å¼·å­¸ç¿ (RL) æ¼ç®æ³ï¼æ¨å¨åªååæ§åå¹²é æç¤ºçå³éï¼ä»¥æ¸å° EA ä¸­çå¤§éº»ä½¿ç¨ãMiWaves å©ç¨é åå°æ¥­ç¥è­åååçæ¸æä¾èª¿æ´å¹²é è¨æ¯å³éçå¯è½æ§ãæ¬æå¨é¢æ¦è¿°äºæ¼ç®æ³çè¨­è¨ï¼åæ¬ééµæ±ºç­åå¯¦é©çµæãæçµç MiWaves RL æ¼ç®æ³å·²æ¼ 2024 å¹´ 3 æè³ 5 æå¨è¨åºè©¦é©ä¸­é¨ç½²ã

##### **Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**
2408.15032v1 by Yuqi Zhang, Xiaoqian Zhang, Jiakai Wang, Yuancheng Yang, Taiying Peng, Chao Tong

Computational pathology (CPath) has significantly advanced the clinical
practice of pathology. Despite the progress made, Multiple Instance Learning
(MIL), a promising paradigm within CPath, continues to face challenges,
particularly related to incomplete information utilization. Existing
frameworks, such as those based on Convolutional Neural Networks (CNNs),
attention, and selective scan space state sequential model (SSM), lack
sufficient flexibility and scalability in fusing diverse features, and cannot
effectively fuse diverse features. Additionally, current approaches do not
adequately exploit order-related and order-independent features, resulting in
suboptimal utilization of sequence information. To address these limitations,
we propose a novel MIL framework called Mamba2MIL. Our framework utilizes the
state space duality model (SSD) to model long sequences of patches of whole
slide images (WSIs), which, combined with weighted feature selection, supports
the fusion processing of more branching features and can be extended according
to specific application needs. Moreover, we introduce a sequence transformation
method tailored to varying WSI sizes, which enhances sequence-independent
features while preserving local sequence information, thereby improving
sequence information utilization. Extensive experiments demonstrate that
Mamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive
experiments across multiple datasets, achieving improvements in nearly all
performance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a
binary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the
BRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an
accuracy of 0.4981. The code is available at
https://github.com/YuqiZhang-Buaa/Mamba2MIL.

æè¦ï¼<paragraph>è¨ç®ççå­¸ (CPath) å·²é¡¯èæåççå­¸çè¨åºå¯¦åãåç®¡å·²æé²å±ï¼ä½çº CPath ä¸­ä¸åæåéçç¯ä¾ï¼å¤éå¯¦ä¾å­¸ç¿ (MIL) æçºé¢è¨ææ°ï¼ç¹å¥æ¯èä¸å®æ´è³è¨ä½¿ç¨æéãç¾æçæ¶æ§ï¼ä¾å¦åºæ¼å·ç©ç¥ç¶ç¶²è·¯ (CNN)ãæ³¨æååé¸ææ§ææç©ºéçæåºåæ¨¡å (SSM) çæ¶æ§ï¼å¨èååç¨®ç¹å¾µæç¼ºä¹è¶³å¤ çå½æ§åå¯æ´åæ§ï¼ä¸ç¡æ³ææèååç¨®ç¹å¾µãæ­¤å¤ï¼ç®åçä½æ³ä¸¦æªååå©ç¨èé åºç¸éåèé åºç¡éçç¹å¾µï¼å°è´åºåè³è¨ä½¿ç¨çä¸ä½³ãçºäºè§£æ±ºéäºéå¶ï¼æåæåºä¸ååçº Mamba2MIL çæ° MIL æ¶æ§ãæåçæ¶æ§å©ç¨çæç©ºéå°å¶æ¨¡å (SSD) ä¾å»ºæ¨¡å¨å¹»ççå½±å (WSI) çé·åºåè²¼çï¼éèå æ¬ç¹å¾µé¸åçµåä½¿ç¨ï¼æ¯æ´æ´å¤åæ¯ç¹å¾µçèåèçï¼ä¸å¯æ ¹æç¹å®æç¨éæ±é²è¡å»¶ä¼¸ãæ­¤å¤ï¼æåå¼å¥ä¸ç¨®éå°ä¸å WSI å¤§å°éèº«æé çåºåè½ææ¹æ³ï¼éå¢å¼·äºèåºåç¡éçç¹å¾µï¼åæä¿çäºå±é¨åºåè³è¨ï¼é²èæ¹ååºåè³è¨ä½¿ç¨çãå»£æ³çå¯¦é©è­æ Mamba2MIL è¶è¶äºæåé²ç MIL æ¹æ³ãæåå¨å¤åè³æéä¸é²è¡å»£æ³çå¯¦é©ï¼å¨å¹¾ä¹æææè½ææ¨ä¸åç²å¾æ¹åãç¹å¥æ¯å¨ NSCLC è³æéä¸ï¼Mamba2MIL éå° 0.9533 çäºåè«ç¤åé¡ AUC å 0.8794 çæºç¢ºåº¦ãå¨ BRACS è³æéä¸ï¼å®éå° 0.7986 çå¤é¡å¥åé¡ AUC å 0.4981 çæºç¢ºåº¦ãç¨å¼ç¢¼å¯å¨ https://github.com/YuqiZhang-Buaa/Mamba2MIL åå¾ã</paragraph>

##### **Sequence-aware Pre-training for Echocardiography Probe Guidance**
2408.15026v1 by Haojun Jiang, Zhenguo Sun, Yu Sun, Ning Jia, Meng Li, Shaqi Luo, Shiji Song, Gao Huang

Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe
pose to obtain high-quality sectional images. Cardiac ultrasound faces two
major challenges: (1) the inherently complex structure of the heart, and (2)
significant individual variations. Previous works have only learned the
population-averaged 2D and 3D structures of the heart rather than personalized
cardiac structural features, leading to a performance bottleneck. Clinically,
we observed that sonographers adjust their understanding of a patient's cardiac
structure based on prior scanning sequences, thereby modifying their scanning
strategies. Inspired by this, we propose a sequence-aware self-supervised
pre-training method. Specifically, our approach learns personalized 2D and 3D
cardiac structural features by predicting the masked-out images and actions in
a scanning sequence. We hypothesize that if the model can predict the missing
content it has acquired a good understanding of the personalized cardiac
structure. In the downstream probe guidance task, we also introduced a sequence
modeling approach that models individual cardiac structural information based
on the images and actions from historical scan data, enabling more accurate
navigation decisions. Experiments on a large-scale dataset with 1.36 million
samples demonstrated that our proposed sequence-aware paradigm can
significantly reduce navigation errors, with translation errors decreasing by
15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared
to state-of-the-art methods.

æè¦ï¼<paragraph>å¿èè¶é³æ³¢æ¢é ­å¼å°æ¨å¨å¹«å©æ°æèª¿æ´ 6-DOF æ¢é ­å§¿å¢ï¼ä»¥åå¾é«åè³ªçæ·é¢å½±åãå¿èè¶é³æ³¢é¢è¨å©é ä¸»è¦ææ°ï¼(1) å¿èçµæ§è¤éä¸åºæï¼ä»¥å (2) åé«å·®ç°é¡¯èãååçç ç©¶åå­¸ç¿äºæ´é«å¹³åç 2D å 3D å¿èçµæ§ï¼èéåäººåçè§£åç¹å¾µï¼å°è´æè½ç¶é ¸ãè¨åºä¸ï¼æåè§å¯å°è¶é³æ³¢æå¸«ææ ¹æååçææåºåèª¿æ´ä»åå°æ£èå¿èçµæ§ççè§£ï¼é²èä¿®æ¹ä»åçææç­ç¥ãåå°æ­¤åç¼ï¼æåæåºä¸åå·åºåæç¥çèªç£ç£é è¨ç·´æ¹æ³ãå·é«ä¾èªªï¼æåçåæ³ééé æ¸¬ææåºåä¸­é®ç½©çå½±åååä½ï¼ä¾å­¸ç¿åäººåç 2D å 3D å¿èè§£åç¹å¾µãæååè¨­ï¼å¦ææ¨¡åå¯ä»¥é æ¸¬éºæ¼çå§å®¹ï¼å®ä¾¿å°åäººåçè§£åçµæ§æäºè¯å¥½ççè§£ãå¨ä¸æ¸¸çæ¢é ­å¼å°ä»»åä¸­ï¼æåä¹å°å¥ä¸ååºåå»ºæ¨¡æ¹æ³ï¼è©²æ¹æ³æ ¹ææ­·å²ææè³æä¸­çå½±åååä½ï¼æ¨¡æ¬åå¥å¿èè§£åè³è¨ï¼é²èååºæ´ç²¾ç¢ºçå°èªæ±ºç­ãå¨æ 136 è¬åæ¨£æ¬çå¤§è¦æ¨¡è³æéä¸é²è¡çå¯¦é©è­æï¼æåæåºçå·åºåæç¥çå¸ç¯å¯ä»¥å¤§å¹æ¸å°å°èªé¯èª¤ï¼å¶ä¸­å¹³ç§»é¯èª¤æ¸å°äº 15.90% è³ 36.87%ï¼æè½é¯èª¤æ¸å°äº 11.13% è³ 20.77%ï¼èæåé²çæ¹æ³ç¸æ¯ã</paragraph>

##### **Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**
2408.15294v2 by Christos Theodoropoulos, Natasha Mulligan, Joao Bettencourt-Silva

Developing novel predictive models with complex biomedical information is
challenging due to various idiosyncrasies related to heterogeneity,
standardization or sparseness of the data. We previously introduced a
person-centric ontology to organize information about individual patients, and
a representation learning framework to extract person-centric knowledge graphs
(PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose a
systematic approach to examine the results of GNN models trained with both
structured and unstructured information from the MIMIC-III dataset. Through
ablation studies on different clinical, demographic, and social data, we show
the robustness of this approach in identifying predictive features in PKGs for
the task of readmission prediction.

æè¦ï¼éç¼å·æè¤éçç©é«å­¸è³è¨çæ°ç©é æ¸¬æ¨¡åï¼ç±æ¼è³æçç°è³ªæ§ãæ¨æºåæç¨çæ§ï¼å æ­¤å·æææ°æ§ãæåååä»ç´¹äºä¸åä»¥äººçºä¸­å¿çæ¬é«ï¼ç¨æ¼çµç¹æéåå¥æ£èçè³è¨ï¼ä»¥åä¸åè¡¨ç¤ºå­¸ç¿æ¶æ§ï¼ç¨æ¼æåä»¥äººçºä¸­å¿çç¥è­åè­ (PKG) åè¨ç·´åå½¢ç¥ç¶ç¶²è·¯ (GNN)ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®ç³»çµ±æ§çæ¹æ³ä¾æª¢é©ä½¿ç¨ MIMIC-III è³æéä¸­ççµæ§ååéçµæ§åè³è¨è¨ç·´ç GNN æ¨¡åççµæãééå°ä¸åçè¨åºãäººå£çµ±è¨åç¤¾æè³æé²è¡æ¶èç ç©¶ï¼æåå±ç¤ºäºéç¨®æ¹æ³å¨è­å¥ PKG ä¸­çé æ¸¬ç¹å¾µä»¥é²è¡åå¥é¢é æ¸¬ä»»åæçç©©å¥æ§ã

##### **Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**
2408.14754v1 by Qiaoxin Li, Ruifeng Chen, Peng Wang, Guotao Quan, Yanfeng Du, Dong Liang, Yinsheng Li

Dual-energy computed tomography (DECT) has been widely used to obtain
quantitative elemental composition of imaged subjects for personalized and
precise medical diagnosis. Compared with DECT leveraging advanced X-ray source
and/or detector technologies, the use of the sequential-scanning data
acquisition scheme to implement DECT may make a broader impact on clinical
practice because this scheme requires no specialized hardware designs and can
be directly implemented into conventional CT systems. However, since the
concentration of iodinated contrast agent in the imaged subject varies over
time, sequentially scanned data sets acquired at two tube potentials are
temporally inconsistent. As existing material basis image reconstruction
approaches assume that the data sets acquired at two tube potentials are
temporally consistent, the violation of this assumption results in inaccurate
quantification of material concentration. In this work, we developed
sequential-scanning DECT imaging using high temporal resolution image
reconstruction and error-compensated material basis image generation,
ACCELERATION in short, to address the technical challenge induced by temporal
inconsistency of sequentially scanned data sets and improve quantification
accuracy of material concentration in sequential-scanning DECT. ACCELERATION
has been validated and evaluated using numerical simulation data sets generated
from clinical human subject exams and experimental human subject studies.
Results demonstrated the improvement of quantification accuracy and image
quality using ACCELERATION.

æè¦ï¼éè½éé»è¦æ·å±¤ææ (DECT) å·²å»£æ³ç¨æ¼åå¾å½±åååè©¦èçå®éåç´ çµæï¼ä»¥é²è¡åäººåä¸ç²¾ç¢ºçé«çè¨ºæ·ãèä½¿ç¨é²é X åæºå/æåµæ¸¬å¨æè¡ç DECT ç¸æ¯ï¼ä½¿ç¨é£çºææè³ææ·åæ¹æ¡ä¾å¯¦ä½ DECT å¯è½å°è¨åºå¯¦åç¢çæ´å»£æ³çå½±é¿ï¼å çºæ­¤æ¹æ¡ä¸éè¦å°éçç¡¬é«è¨­è¨ï¼ä¸å¯ç´æ¥å¯¦ä½å°å³çµ± CT ç³»çµ±ä¸­ãç¶èï¼ç±æ¼å½±åååè©¦èä¸­ç¢åå°æ¯åçæ¿åº¦æé¨æéèè®åï¼å æ­¤å¨å©åç®¡é»ä½ä¸æ·åçé£çºææè³æéå¨æéä¸ä¸¦ä¸ä¸è´ãç±æ¼ç¾æçææåºç¤å½±åéå»ºæ¹æ³åè¨­å¨å©åç®¡é»ä½ä¸æ·åçè³æéå¨æéä¸æ¯ä¸è´çï¼å æ­¤éåæ­¤åè¨­æå°è´æææ¿åº¦çéåä¸æºç¢ºãå¨éé å·¥ä½ä¸­ï¼æåéç¼äºä½¿ç¨é«æéè§£æåº¦å½±åéå»ºåèª¤å·®è£åææåºç¤å½±åç¢ççé£çºææ DECT å½±åï¼ç°¡ç¨± ACCELERATIONï¼ä»¥è§£æ±ºé£çºææè³æéæéä¸ä¸è´æå¼ç¼çæè¡ææ°ï¼ä¸¦æ¹åé£çºææ DECT ä¸­æææ¿åº¦çéåæºç¢ºåº¦ãACCELERATION å·²ä½¿ç¨å¾è¨åºäººé«åè©¦èæª¢æ¥åå¯¦é©äººé«åè©¦èç ç©¶ç¢ççæ¸å¼æ¨¡æ¬è³æéé²è¡é©è­åè©ä¼°ãçµæè­æä½¿ç¨ ACCELERATION å¯æ¹åéåæºç¢ºåº¦åå½±ååè³ªã

##### **Large Language Models for Disease Diagnosis: A Scoping Review**
2409.00097v1 by Shuang Zhou, Zidu Xu, Mian Zhang, Chunpu Xu, Yawen Guo, Zaifu Zhan, Sirui Ding, Jiashuo Wang, Kaishuai Xu, Yi Fang, Liqiao Xia, Jeremy Yeung, Daochen Zha, Mingquan Lin, Rui Zhang

Automatic disease diagnosis has become increasingly valuable in clinical
practice. The advent of large language models (LLMs) has catalyzed a paradigm
shift in artificial intelligence, with growing evidence supporting the efficacy
of LLMs in diagnostic tasks. Despite the growing attention in this field, many
critical research questions remain under-explored. For instance, what diseases
and LLM techniques have been investigated for diagnostic tasks? How can
suitable LLM techniques and evaluation methods be selected for clinical
decision-making? To answer these questions, we performed a comprehensive
analysis of LLM-based methods for disease diagnosis. This scoping review
examined the types of diseases, associated organ systems, relevant clinical
data, LLM techniques, and evaluation methods reported in existing studies.
Furthermore, we offered guidelines for data preprocessing and the selection of
appropriate LLM techniques and evaluation strategies for diagnostic tasks. We
also assessed the limitations of current research and delineated the challenges
and future directions in this research field. In summary, our review outlined a
blueprint for LLM-based disease diagnosis, helping to streamline and guide
future research endeavors.

æè¦ï¼èªåç¾çè¨ºæ·å¨è¨åºå¯¦åä¸­è®å¾è¶ä¾è¶æå¹å¼ãå¤§èªè¨æ¨¡å (LLM) çåºç¾å¬åäºäººå·¥æºè½çå¸ç¯è½ç§»ï¼è¶ä¾è¶å¤è­ææ¯æ LLM å¨è¨ºæ·ä»»åä¸­çæè½ãåç®¡éåé ååå°è¶ä¾è¶å¤çéæ³¨ï¼ä½è¨±å¤ééµçç ç©¶åé¡ä»æªå¾å°ååæ¢è¨ãä¾å¦ï¼åªäºç¾çå LLM æè¡å·²è¢«èª¿æ¥ç¨æ¼è¨ºæ·ä»»åï¼å¦ä½çºè¨åºæ±ºç­å¶å®é¸æåé©ç LLM æè¡åè©ä¼°æ¹æ³ï¼çºäºåç­éäºåé¡ï¼æåå°åºæ¼ LLM çç¾çè¨ºæ·æ¹æ³é²è¡äºå¨é¢çåæãéé ç¯åæ¢è¨åé¡§äºç¾æç ç©¶ä¸­å ±åçç¾çé¡åãç¸éå¨å®ç³»çµ±ãç¸éè¨åºè³æãLLM æè¡åè©ä¼°æ¹æ³ãæ­¤å¤ï¼æåæä¾äºè³æåèçåé¸æé©ç¶ç LLM æè¡åè©ä¼°ç­ç¥ä»¥é²è¡è¨ºæ·ä»»åçæåãæåéè©ä¼°äºç¶åç ç©¶çéå¶ï¼ä¸¦æè¿°äºéåç ç©¶é åçææ°åæªä¾æ¹åãç¸½ä¹ï¼æåçåé¡§æ¦è¿°äºåºæ¼ LLM çç¾çè¨ºæ·èåï¼æå©æ¼ç°¡ååæå°æªä¾çç ç©¶å·¥ä½ã

##### **Improving Clinical Note Generation from Complex Doctor-Patient Conversation**
2408.14568v1 by Yizhan Li, Sifan Wu, Christopher Smith, Thomas Lo, Bang Liu

Writing clinical notes and documenting medical exams is a critical task for
healthcare professionals, serving as a vital component of patient care
documentation. However, manually writing these notes is time-consuming and can
impact the amount of time clinicians can spend on direct patient interaction
and other tasks. Consequently, the development of automated clinical note
generation systems has emerged as a clinically meaningful area of research
within AI for health. In this paper, we present three key contributions to the
field of clinical note generation using large language models (LLMs). First, we
introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex
doctor-patient conversations paired with their full clinical notes. This
dataset, created and curated by medical experts with the help of modern neural
networks, provides a valuable resource for training and evaluating models in
clinical note generation tasks. Second, we propose the K-SOAP (Keyword,
Subjective, Objective, Assessment, and Plan) note format, which enhances
traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and
Plan) notes by adding a keyword section at the top, allowing for quick
identification of essential information. Third, we develop an automatic
pipeline to generate K-SOAP notes from doctor-patient conversations and
benchmark various modern LLMs using various metrics. Our results demonstrate
significant improvements in efficiency and performance compared to standard LLM
finetuning methods.

æè¦ï¼æ°å¯«è¨åºç­è¨åè¨éé«çæª¢æ¥æ¯é«çä¿å¥å°æ¥­äººå¡çä¸é éè¦ä»»åï¼æ¯æ£èç§è­·æä»¶ä¸­çéè¦çµæé¨åãç¶èï¼æåæ°å¯«éäºç­è¨å¾èæï¼ä¸¦ä¸æå½±é¿è¨åºé«çè±å¨ç´æ¥æ£èäºååå¶ä»ä»»åä¸çæéãå æ­¤ï¼èªååè¨åºç­è¨çæç³»çµ±çéç¼å·²æçº AI å¨å¥åº·é åä¸­å·æè¨åºæç¾©çç ç©¶é åãå¨æ¬æä¸­ï¼æåæåºäºä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) é²è¡è¨åºç­è¨çæçé åç 3 é ééµè²¢ç»ãé¦åï¼æåä»ç´¹äº CliniKnoteï¼éæ¯ä¸åç¶åæ§æ¸æéï¼åå« 1,200 åè¤éçé«æ£å°è©±åå¶å®æ´çè¨åºç­è¨ãæ­¤æ¸æéç±é«å­¸å°å®¶å¨ç¾ä»£ç¥ç¶ç¶²è·¯çå¹«å©ä¸åµå»ºåç­åï¼çºè¨åºç­è¨çæä»»åä¸­çæ¨¡åè¨ç·´åè©ä¼°æä¾äºå¯¶è²´çè³æºãå¶æ¬¡ï¼æåæåºäº K-SOAPï¼ééµå­ãä¸»è§ãå®¢è§ãè©ä¼°åè¨ç«ï¼ç­è¨æ ¼å¼ï¼å®ééå¨é é¨æ·»å ä¸åééµå­é¨åä¾å¢å¼·å³çµ±ç SOAP~\cite{podder2023soap}ï¼ä¸»è§ãå®¢è§ãè©ä¼°åè¨ç«ï¼ç­è¨ï¼ä»¥ä¾¿å¿«éè­å¥åºæ¬è³è¨ãç¬¬ä¸ï¼æåéç¼äºä¸åèªååç®¡éï¼å¾é«æ£å°è©±ä¸­çæ K-SOAP ç­è¨ï¼ä¸¦ä½¿ç¨åç¨®ææ¨å°åç¨®ç¾ä»£ LLM é²è¡åºæºæ¸¬è©¦ãæåççµæè¡¨æï¼èæ¨æº LLM å¾®èª¿æ¹æ³ç¸æ¯ï¼æçåæ§è½æäºé¡¯èçæåã

##### **Temporal Ensemble Logic**
2408.14443v2 by Guo-Qiang Zhang

We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal
logic for linear-time temporal reasoning. TEL includes primitive temporal
constructs such as ``always up to $t$ time later'' ($\Box_t$), ``sometimes
before $t$ time in the future'' ($\Diamond_t$), and ``$t$-time later''
$\varphi_t$. TEL has been motivated from the requirement for rigor and
reproducibility for cohort specification and discovery in clinical and
population health research, to fill a gap in formalizing temporal reasoning in
biomedicine. Existing logical frameworks such as linear temporal logic are too
restrictive to express temporal and sequential properties in biomedicine, or
too permissive in semantic constructs, such as in Halpern-Shoham logic, to
serve this purpose. In this paper, we first introduce TEL in a general set up,
with discrete and dense time as special cases. We then focus on the theoretical
development of discrete TEL on the temporal domain of positive integers
$\mathbb{N}^+$, denoted as ${\rm TEL}_{\mathbb{N}^+}$. ${\rm
TEL}_{\mathbb{N}^+}$ is strictly more expressive than the standard monadic
second order logic, characterized by B\"{u}chi automata. We present its formal
semantics, a proof system, and provide a proof for the undecidability of the
satisfiability of ${\rm TEL}_{\mathbb{N}^+}$. We also include initial results
on expressiveness and decidability fragments for ${\rm TEL}_{\mathbb{N}^+}$,
followed by application outlook and discussions.

æè¦ï¼<paragraph>æåä»ç´¹æééåéè¼¯ (TEL)ï¼ä¸ç¨®ç¨æ¼ç·æ§æéæææ¨ççä¸éå®å­æ¨¡æéè¼¯ãTEL åå«åå§ææçµæ§ï¼ä¾å¦ãå§çµå¨ $t$ æéå¾ã($\Box_t$)ããææå¨æªä¾ $t$ æéåã($\Diamond_t$) åã$t$ æéå¾ã$\varphi_t$ãTEL çåæ©æ¯çºäºè¨åºåäººå£å¥åº·ç ç©¶ä¸­ç¾¤çµè¦ç¯åç¼ç¾çå´è¬¹æ§åå¯åç¾æ§ï¼ä»¥å¡«è£çç©é«å­¸ä¸­æææ¨çå½¢å¼åçç©ºç½ãç¾æçéè¼¯æ¡æ¶ï¼ä¾å¦ç·æ§ææéè¼¯ï¼å°æ¼è¡¨éçç©é«å­¸ä¸­çææåé åºå±¬æ§éæ¼å´æ ¼ï¼æèå¨èªç¾©çµæ§ä¸éæ¼å¯¬é¬ï¼ä¾å¦å¨ Halpern-Shoham éè¼¯ä¸­ï¼ï¼ç¡æ³éå°æ­¤ç®çãå¨æ¬æä¸­ï¼æåé¦åå¨ä¸è¬è¨­ç½®ä¸­ä»ç´¹ TELï¼å¶ä¸­é¢æ£æéåç¨ å¯æéçºç¹æ®ææ³ãç¶å¾ï¼æåå°æ³¨æ¼æ­£æ´æ¸æéå $\mathbb{N}^+$ ä¸é¢æ£ TEL ççè«ç¼å±ï¼è¡¨ç¤ºçº ${\rm TEL}_{\mathbb{N}^+}$. ${\rm TEL}_{\mathbb{N}^+}$ æ¯æ¨æºå®å­äºééè¼¯æ´å·è¡¨éåï¼å¶ç¹å¾µå¨æ¼ B\"{u}chi èªåæ©ãæåå±ç¤ºå¶å½¢å¼èªç¾©ãè­æç³»çµ±ï¼ä¸¦æä¾ ${\rm TEL}_{\mathbb{N}^+}$ å¯æ»¿è¶³æ§çä¸å¯å¤å®æ§çè­æãæåéåæ¬ ${\rm TEL}_{\mathbb{N}^+}$ çè¡¨éååå¯å¤å®çæ®µçåæ­¥çµæï¼ç¶å¾æ¯æç¨åæ¯åè¨è«ã</paragraph>

##### **MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**
2408.14418v2 by Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler

Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech
into text, yet the errors they introduce can significantly degrade the
performance of downstream tasks like summarization. This issue is particularly
pronounced in clinical dialogue summarization, a low-resource domain where
supervised data for fine-tuning is scarce, necessitating the use of ASR models
as black-box solutions. Employing conventional data augmentation for enhancing
the noise robustness of summarization models is not feasible either due to the
unavailability of sufficient medical dialogue audio recordings and
corresponding ASR transcripts. To address this challenge, we propose MEDSAGE,
an approach for generating synthetic samples for data augmentation using Large
Language Models (LLMs). Specifically, we leverage the in-context learning
capabilities of LLMs and instruct them to generate ASR-like errors based on a
few available medical dialogue examples with audio recordings. Experimental
results show that LLMs can effectively model ASR noise, and incorporating this
noisy data into the training process significantly improves the robustness and
accuracy of medical dialogue summarization systems. This approach addresses the
challenges of noisy ASR outputs in critical applications, offering a robust
solution to enhance the reliability of clinical dialogue summarization.

æè¦ï¼èªåèªé³è¾¨è­ (ASR) ç³»çµ±å¨å°èªé³è½éææå­æ¹é¢è³ééè¦ï¼ä½å®åæç¢ççé¯èª¤å¯è½æå¤§å¹éä½æè¦ç­ä¸æ¸¸ä»»åçæè½ãéååé¡å¨è¨åºå°è©±æè¦ä¸­ç¹å¥æé¡¯ï¼éæ¯ä¸åä½è³æºçé åï¼å¶ä¸­ç¨æ¼å¾®èª¿çç£ç£è³æå¾ç¨å°ï¼å æ­¤å¿é ä½¿ç¨ ASR æ¨¡åä½çºé»çè§£æ±ºæ¹æ¡ãç±æ¼ç¼ºä¹è¶³å¤ çé«çå°è©±é³è¨éé³åå°æç ASR è½éï¼æ¡ç¨å³çµ±è³ææ´åä¾å¢å¼·æè¦æ¨¡åçæåªæ§ä¹ä¸å¯è¡ãçºäºæå°éåææ°ï¼æåæåºäº MEDSAGEï¼éæ¯ä¸ç¨®ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) ç¢çåææ¨£æ¬é²è¡è³ææ´åçæ¹æ³ãå·é«ä¾èªªï¼æåå©ç¨ LLM çæå¢å­¸ç¿è½åï¼ä¸¦æç¤ºå®åæ ¹æå°æ¸å¸¶æé³è¨éé³çå¯ç¨é«çå°è©±ç¯ä¾ç¢çé¡ä¼¼ç ASR é¯èª¤ãå¯¦é©çµæé¡¯ç¤ºï¼LLM å¯ä»¥ææå°æ¨¡æ¬ ASR éè¨ï¼èå°éäºéè¨è³æç´å¥è¨ç·´éç¨ä¸­å¯ä»¥é¡¯èæé«é«çå°è©±æè¦ç³»çµ±çç©©å¥æ§åæºç¢ºæ§ãéç¨®æ¹æ³æå°äºééµæç¨ä¸­ ASR è¼¸åºéè¨çåé¡ï¼æä¾äºä¸åç©©å¥çè§£æ±ºæ¹æ¡ä¾å¢å¼·è¨åºå°è©±æè¦çå¯é æ§ã

##### **Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**
2408.14397v1 by Xiaoman Zhang, JuliÃ¡n N. Acosta, Hong-Yu Zhou, Pranav Rajpurkar

Recent advancements in artificial intelligence have significantly improved
the automatic generation of radiology reports. However, existing evaluation
methods fail to reveal the models' understanding of radiological images and
their capacity to achieve human-level granularity in descriptions. To bridge
this gap, we introduce a system, named ReXKG, which extracts structured
information from processed reports to construct a comprehensive radiology
knowledge graph. We then propose three metrics to evaluate the similarity of
nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs
(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative
analysis of AI-generated and human-written radiology reports, assessing the
performance of both specialist and generalist models. Our study provides a
deeper understanding of the capabilities and limitations of current AI models
in radiology report generation, offering valuable insights for improving model
performance and clinical applicability.

æè¦ï¼è¿æäººå·¥æºè½çé²å±é¡¯èæ¹åäºæ¾å°å ±åçèªåçæãç¶èï¼ç¾æçè©ä¼°æ¹æ³ç¡æ³æ­ç¤ºæ¨¡åå°æ¾å°å½±åççè§£ï¼ä»¥åå®åå¨æè¿°ä¸­éå°äººé¡å±¤ç´ç²¾ç´°åº¦çè½åãçºäºå½è£éåå·®è·ï¼æåå¼é²ä¸ååçº ReXKG çç³»çµ±ï¼å®å¾èçéçå ±åä¸­èååºçµæ§åçè³è¨ï¼ä»¥å»ºæ§ä¸åå¨é¢çæ¾å°ç¥è­åè­ãæ¥èï¼æåæåºä¸åææ¨ä¾è©ä¼°åç¨®ç¥è­åè­ä¸­ç¯é»çç¸ä¼¼æ§ (ReXKG-NSC)ãéç·£çåå¸ (ReXKG-AMS) åå­åçæ¶µèç¯å (ReXKG-SCS)ãæåå° AI çæçåäººé¡æ°å¯«çæ¾å°å ±åé²è¡æ·±å¥çæ¯è¼åæï¼è©ä¼°å°å®¶åéææ¨¡åçæè½ãæåçç ç©¶æä¾å°ç®å AI æ¨¡åå¨æ¾å°å ±åçæä¸­çè½ååéå¶æ´æ·±å¥ççè§£ï¼ä¸¦æä¾æå¹å¼çè¦è§£ä¾æ¹åæ¨¡åæè½åè¨åºæç¨ã

##### **Foundation Models for Music: A Survey**
2408.14340v3 by Yinghao Ma, Anders Ãland, Anton Ragni, Bleiz MacSen Del Sette, Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elona Shatri, Fabio Morreale, Ge Zhang, GyÃ¶rgy Fazekas, Gus Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin, Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B. Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shuqi Dai, Shun Lei, Shiyin Kang, Simon Dixon, Wenhu Chen, Wenhao Huang, Xingjian Du, Xingwei Qu, Xu Tan, Yizhi Li, Zeyue Tian, Zhiyong Wu, Zhizheng Wu, Ziyang Ma, Ziyu Wang

In recent years, foundation models (FMs) such as large language models (LLMs)
and latent diffusion models (LDMs) have profoundly impacted diverse sectors,
including music. This comprehensive review examines state-of-the-art (SOTA)
pre-trained models and foundation models in music, spanning from representation
learning, generative learning and multimodal learning. We first contextualise
the significance of music in various industries and trace the evolution of AI
in music. By delineating the modalities targeted by foundation models, we
discover many of the music representations are underexplored in FM development.
Then, emphasis is placed on the lack of versatility of previous methods on
diverse music applications, along with the potential of FMs in music
understanding, generation and medical application. By comprehensively exploring
the details of the model pre-training paradigm, architectural choices,
tokenisation, finetuning methodologies and controllability, we emphasise the
important topics that should have been well explored, like instruction tuning
and in-context learning, scaling law and emergent ability, as well as
long-sequence modelling etc. A dedicated section presents insights into music
agents, accompanied by a thorough analysis of datasets and evaluations
essential for pre-training and downstream tasks. Finally, by underscoring the
vital importance of ethical considerations, we advocate that following research
on FM for music should focus more on such issues as interpretability,
transparency, human responsibility, and copyright issues. The paper offers
insights into future challenges and trends on FMs for music, aiming to shape
the trajectory of human-AI collaboration in the music realm.

æè¦ï¼è¿å¹´ä¾ï¼åºç¤æ¨¡å (FM)ï¼ä¾å¦å¤§åèªè¨æ¨¡å (LLM) åæ½å¨æ´æ£æ¨¡å (LDM)ï¼å·²å°åæ¬é³æ¨å¨å§çä¸åç¢æ¥­ç¢çæ·±é å½±é¿ãéç¯å¨é¢æ§çè©è«æ¢è¨äºé³æ¨é åä¸­æåé² (SOTA) çé è¨ç·´æ¨¡åååºç¤æ¨¡åï¼æ¶µèäºè¡¨å¾µå­¸ç¿ãçæå¼å­¸ç¿åå¤æ¨¡æå­¸ç¿ãæåé¦åå°é³æ¨å¨åç¢æ¥­çéè¦æ§èçµ¡åï¼ä¸¦è¿½æº¯ AI å¨é³æ¨ä¸­çæ¼é²ãééæç¹ªåºç¤æ¨¡åæéå°çæ¨¡æï¼æåç¼ç¾è¨±å¤é³æ¨è¡¨å¾µå¨ FM éç¼ä¸­å°æªè¢«ååæ¢ç´¢ãæ¥èï¼æåå¼·èª¿ååæ¹æ³å¨ä¸åé³æ¨æç¨ä¸­ç¼ºä¹å¤æ¨£æ§ï¼ä»¥å FM å¨é³æ¨çè§£ãçæåé«çæç¨ä¸­çæ½åãééå¨é¢æ¢è¨æ¨¡åé è¨ç·´å¸ç¯ãæ¶æ§é¸æãæ¨è¨åãå¾®èª¿æ¹æ³åå¯æ§æ§çç´°ç¯ï¼æåå¼·èª¿äºææ·±å¥æ¢è¨çéè¦ä¸»é¡ï¼ä¾å¦æä»¤å¾®èª¿åæå¢å­¸ç¿ãè¦æ¨¡å®å¾åæ°èè½åï¼ä»¥åé·åºåå»ºæ¨¡ç­ãå°éçç« ç¯æä¾äºå°é³æ¨ä»£ççè¦è§£ï¼ä¸¦éæå°é è¨ç·´åä¸æ¸¸ä»»åè³ééè¦çè³æéåè©ä¼°çæ·±å¥åæãæå¾ï¼ééå¼·èª¿å«çèéçè³ééè¦æ§ï¼æåä¸»å¼µå¾çºéæ¼é³æ¨ FM çç ç©¶ææ´å°æ³¨æ¼å¯è§£éæ§ãéæåº¦ãäººé¡è²¬ä»»åçæ¬åé¡ç­è­°é¡ãæ¬ææä¾äºå°é³æ¨ FM æªä¾ææ°åè¶¨å¢çè¦è§£ï¼æ¨å¨å½¢å¡äººé¡è AI å¨é³æ¨é åä¸­åä½çè»è·¡ã

##### **Uncertainties of Latent Representations in Computer Vision**
2408.14281v1 by Michael Kirchhof

Uncertainty quantification is a key pillar of trustworthy machine learning.
It enables safe reactions under unsafe inputs, like predicting only when the
machine learning model detects sufficient evidence, discarding anomalous data,
or emitting warnings when an error is likely to be inbound. This is
particularly crucial in safety-critical areas like medical image classification
or self-driving cars. Despite the plethora of proposed uncertainty
quantification methods achieving increasingly higher scores on performance
benchmarks, uncertainty estimates are often shied away from in practice. Many
machine learning projects start from pretrained latent representations that
come without uncertainty estimates. Uncertainties would need to be trained by
practitioners on their own, which is notoriously difficult and
resource-intense.
  This thesis makes uncertainty estimates easily accessible by adding them to
the latent representation vectors of pretrained computer vision models. Besides
proposing approaches rooted in probability and decision theory, such as
Monte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both
theoretical and empirical questions. We show that these unobservable
uncertainties about unobservable latent representations are indeed provably
correct. We also provide an uncertainty-aware representation learning (URL)
benchmark to compare these unobservables against observable ground-truths.
Finally, we compile our findings to pretrain lightweight representation
uncertainties on large-scale computer vision models that transfer to unseen
datasets in a zero-shot manner.
  Our findings do not only advance the current theoretical understanding of
uncertainties over latent variables, but also facilitate the access to
uncertainty quantification for future researchers inside and outside the field,
enabling straightforward but trustworthy machine learning.

æè¦ï¼ä¸ç¢ºå®éåæ¯å¼å¾ä¿¡è³´æ©å¨å­¸ç¿çä¸å¤§æ¯æ±ã
å®è½è®æ©å¨å­¸ç¿æ¨¡åå¨ä¸å®å¨çè¼¸å¥ä¸ååºå®å¨çåæï¼ä¾å¦åªå¨æ©å¨å­¸ç¿æ¨¡ååµæ¸¬å°è¶³å¤ è­æææé²è¡é æ¸¬ãæ¨æ£ç°å¸¸è³æï¼ææ¯å¨å¯è½ç¼çé¯èª¤æç¼åºè­¦åãéå¨é«çå½±ååé¡æèªé§è»ç­å®å¨ééµé åä¸­ç¹å¥éè¦ãåç®¡æè¨±å¤å·²æåºçä¸ç¢ºå®éåæ¹æ³å¨æè½åºæºä¸åå¾è¶ä¾è¶é«çåæ¸ï¼ä½å¨å¯¦åä¸å»å¸¸å¸¸è¿´é¿ä¸ç¢ºå®æ§ä¼°è¨ãè¨±å¤æ©å¨å­¸ç¿å°æ¡å¾é è¨ç·´çæ½å¨è¡¨å¾µéå§ï¼èéäºè¡¨å¾µæ²æä¸ç¢ºå®æ§ä¼°è¨ãå¯¦åå·¥ä½èéè¦èªè¡è¨ç·´ä¸ç¢ºå®æ§ï¼éåºäºåçå°é£ä¸èè²»è³æºã
æ¬è«æééå°ä¸ç¢ºå®æ§ä¼°è¨æ°å¢å°é è¨ç·´é»è¦è¦è¦ºæ¨¡åçæ½å¨è¡¨å¾µåéä¸­ï¼è®ä¸ç¢ºå®æ§ä¼°è¨ææ¼åå¾ãé¤äºæåºæ¤åºæ¼æ©çåæ±ºç­çè«çæ¹æ³ï¼ä¾å¦èå°å¡ç¾è³è¨å°æ¯ä¼°è¨ (MCInfoNCE) åæå¤±é æ¸¬ä¹å¤ï¼æåéæ·±å¥æ¢è¨çè«åå¯¦è­åé¡ãæåè­æéäºéæ¼ä¸å¯è§å¯æ½å¨è¡¨å¾µçä¸å¯è§å¯ä¸ç¢ºå®æ§ç¢ºå¯¦å¯ä»¥è­ææ¯æ­£ç¢ºçãæåéæä¾ä¸åä¸ç¢ºå®æ§æç¥è¡¨å¾µå­¸ç¿ (URL) åºæºï¼ç¨ä¾æ¯è¼éäºä¸å¯è§å¯çä¸ç¢ºå®æ§èå¯è§å¯ççå¯¦å¼ãæå¾ï¼æåå°æåçç¼ç¾å½æ´èµ·ä¾ï¼å¨å¤§åé»è¦è¦è¦ºæ¨¡åä¸é è¨ç·´è¼éç´è¡¨å¾µä¸ç¢ºå®æ§ï¼ä¸¦ä»¥é¶æ¬¡å­¸ç¿çæ¹å¼è½ç§»å°æªè¦éçè³æéã
æåçç¼ç¾ä¸åæåäºç¶åå°æ½å¨è®æ¸ä¸ç¢ºå®æ§ççè«çè§£ï¼éä¿é²äºæªä¾ç ç©¶äººå¡å¨è©²é åå§å¤åå¾ä¸ç¢ºå®éåï¼é²èå¯¦ç¾ç´æ¥ä½å¼å¾ä¿¡è³´çæ©å¨å­¸ç¿ã

##### **Automatic Medical Report Generation: Methods and Applications**
2408.13988v1 by Li Guo, Anas M. Tahir, Dong Zhang, Z. Jane Wang, Rabab K. Ward

The increasing demand for medical imaging has surpassed the capacity of
available radiologists, leading to diagnostic delays and potential
misdiagnoses. Artificial intelligence (AI) techniques, particularly in
automatic medical report generation (AMRG), offer a promising solution to this
dilemma. This review comprehensively examines AMRG methods from 2021 to 2024.
It (i) presents solutions to primary challenges in this field, (ii) explores
AMRG applications across various imaging modalities, (iii) introduces publicly
available datasets, (iv) outlines evaluation metrics, (v) identifies techniques
that significantly enhance model performance, and (vi) discusses unresolved
issues and potential future research directions. This paper aims to provide a
comprehensive understanding of the existing literature and inspire valuable
future research.

æè¦ï¼ç±æ¼å°é«å­¸å½±åçéæ±æ¥çå¢é·ï¼å·²ç¶è¶éäºç¾ææ¾å°ç§é«å¸«çè½åï¼å°è´è¨ºæ·å»¶èª¤åæ½å¨çèª¤è¨ºãäººå·¥æºæ§ (AI) æè¡ï¼ç¹å¥æ¯å¨èªåé«çå ±åçæ (AMRG) æ¹é¢ï¼çºæ­¤å°å¢æä¾äºæå¸æçè§£æ±ºæ¹æ¡ãæ¬ç¯è©è«å¨é¢æ¢è¨äº 2021 å¹´è³ 2024 å¹´ç AMRG æ¹æ³ãå® (i) æåºè§£æ±ºæ­¤é åä¸­ä¸»è¦ææ°çæ¹æ¡ï¼(ii) æ¢è¨ AMRG å¨åç¨®å½±åæ¨¡å¼ä¸­çæç¨ï¼(iii) ä»ç´¹å¬éå¯ç¨çè³æéï¼(iv) æ¦è¿°è©ä¼°ææ¨ï¼(v) æ¾åºé¡¯èæåæ¨¡åæè½çæè¡ï¼ä»¥å (vi) è¨è«å°æªè§£æ±ºçåé¡åæ½å¨çæªä¾ç ç©¶æ¹åãæ¬ææ¨å¨æä¾å°ç¾ææç»çå¨é¢äºè§£ï¼ä¸¦æ¿ç¼æå¹å¼çæªä¾ç ç©¶ã

##### **Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**
2409.00084v2 by Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O Yang, Juan Echavarria, Sumbal Babar, Aasma Shaukat, Samuel Margolis, Nicholas P Tatonetti, Girish Nadkarni, Bara El Kurdi, Ali Soroush

Background and Aims: This study evaluates the medical reasoning performance
of large language models (LLMs) and vision language models (VLMs) in
gastroenterology.
  Methods: We used 300 gastroenterology board exam-style multiple-choice
questions, 138 of which contain images to systematically assess the impact of
model configurations and parameters and prompt engineering strategies utilizing
GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs
(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),
Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces
(web and API), computing environments (cloud and local), and model precisions
(with and without quantization). Finally, we assessed accuracy using a
semiautomated pipeline.
  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet
(74.0%) achieved the highest accuracy, outperforming the top open-source
models: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).
Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)
performed best. The scores of the quantized models were comparable to those of
the full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM
performance on image-containing questions did not improve when the images were
provided and worsened when LLM-generated captions were provided. In contrast, a
10% increase in accuracy was observed when images were accompanied by
human-crafted image descriptions.
  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in
medical reasoning, the integration of visual data remains a challenge for VLMs.
Effective deployment involves carefully determining optimal model
configurations, encouraging users to consider either the high performance of
proprietary models or the flexible adaptability of open-source models.

æè¦ï¼<paragraph>èæ¯èç®æ¨ï¼æ¬ç ç©¶è©ä¼°å¤§åèªè¨æ¨¡å (LLM) åè¦è¦ºèªè¨æ¨¡å (VLM) å¨è¸èçå­¸ä¸­çé«çæ¨çè¡¨ç¾ã
æ¹æ³ï¼æåä½¿ç¨ 300 åè¸èçå­¸å°ç§èè©¦é¢¨æ ¼çå¤é¸é¡ï¼å¶ä¸­ 138 ååå«å½±åï¼ä»¥ç³»çµ±æ§å°è©ä¼°æ¨¡åéç½®ååæ¸ä»¥åå©ç¨ GPT-3.5 çæç¤ºå·¥ç¨ç­ç¥çå½±é¿ãæ¥ä¸ä¾ï¼æåè©ä¼°å°æåéæº LLMï¼çæ¬ï¼çè¡¨ç¾ï¼åæ¬ GPTï¼3.5ã4ã4oã4ominiï¼ãClaudeï¼3ã3.5ï¼ãGeminiï¼1.0ï¼ãMistralãLlamaï¼2ã3ã3.1ï¼ãMixtral å Phiï¼3ï¼ï¼è·¨ä¸åä»é¢ï¼ç¶²è·¯å APIï¼ãéç®ç°å¢ï¼é²ç«¯åæ¬å°ï¼åæ¨¡åç²¾ç¢ºåº¦ï¼æåæ²æéåï¼ãæå¾ï¼æåä½¿ç¨åèªååç®¡éè©ä¼°æºç¢ºåº¦ã
çµæï¼å¨å°ææ¨¡åä¸­ï¼GPT-4oï¼73.7%ï¼å Claude3.5-Sonnetï¼74.0%ï¼éå°æé«æºç¢ºåº¦ï¼åªæ¼é å°çéæºæ¨¡åï¼Llama3.1-405bï¼64%ï¼ãLlama3.1-70bï¼58.3%ï¼å Mixtral-8x7bï¼54.3%ï¼ãå¨éåçéæºæ¨¡åä¸­ï¼6 ä½åéåç Phi3-14bï¼48.7%ï¼è¡¨ç¾æä½³ãéåæ¨¡åçåæ¸èå¨ç²¾åº¦æ¨¡å Llama2-7bãLlama2--13b å Gemma2-9b ç¸ç¶ãå¼å¾æ³¨æçæ¯ï¼ç¶æä¾å½±åæï¼VLM å¨åå«å½±åçåé¡ä¸çè¡¨ç¾ä¸¦æªæ¹åï¼èå¨æä¾ LLM çæçæ¨é¡æè¡¨ç¾æ¡åãç¸åå°ï¼ç¶å½±åéæäººå·¥è£½ä½çå½±åæè¿°æï¼æºç¢ºåº¦è§å¯å°å¢å äº 10%ã
çµè«ï¼çµè«èè¨ï¼éç¶ LLM å¨é«çæ¨çä¸­è¡¨ç¾åºå¼·å¥çé¶æ¬¡å­¸ç¿è¡¨ç¾ï¼ä½è¦è¦ºè³æçæ´åä»ç¶æ¯ VLM çä¸é ææ°ãææçé¨ç½²æ¶åä»ç´°ç¢ºå®æä½³æ¨¡åéç½®ï¼é¼åµä½¿ç¨èèæ®å°ææ¨¡åçé«æè½æéæºæ¨¡åçéæ´»é©ææ§ã</paragraph>

##### **PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**
2408.13836v1 by Zifan Chen, Xinyu Nan, Jiazheng Li, Jie Zhao, Haifeng Li, Zilin Lin, Haoshen Li, Heyun Chen, Yiting Liu, Bin Dong, Li Zhang, Lei Tang

Volumetric segmentation is crucial for medical imaging but is often
constrained by labor-intensive manual annotations and the need for
scenario-specific model training. Furthermore, existing general segmentation
models are inefficient due to their design and inferential approaches.
Addressing this clinical demand, we introduce PropSAM, a propagation-based
segmentation model that optimizes the use of 3D medical structure information.
PropSAM integrates a CNN-based UNet for intra-slice processing with a
Transformer-based module for inter-slice propagation, focusing on structural
and semantic continuities to enhance segmentation across various modalities.
Distinctively, PropSAM operates on a one-view prompt, such as a 2D bounding box
or sketch mask, unlike conventional models that require two-view prompts. It
has demonstrated superior performance, significantly improving the Dice
Similarity Coefficient (DSC) across 44 medical datasets and various imaging
modalities, outperforming models like MedSAM and SegVol with an average DSC
improvement of 18.1%. PropSAM also maintains stable predictions despite prompt
deviations and varying propagation configurations, confirmed by one-way ANOVA
tests with P>0.5985 and P>0.6131, respectively. Moreover, PropSAM's efficient
architecture enables faster inference speeds (Wilcoxon rank-sum test, P<0.001)
and reduces user interaction time by 37.8% compared to two-view prompt models.
Its ability to handle irregular and complex objects with robust performance
further demonstrates its potential in clinical settings, facilitating more
automated and reliable medical imaging analyses with minimal retraining.

æè¦ï¼é«ç©åå²å°æ¼é«å­¸å½±åè³ééè¦ï¼ä½éå¸¸åå°èè²»å¤§éäººåçæ¨è¨»åç¹å®å ´æ¯æ¨¡åè¨ç·´éæ±çéå¶ãæ­¤å¤ï¼ç¾æçéç¨åå²æ¨¡åç±æ¼å¶è¨­è¨åæ¨è«æ¹æ³èæçä½ä¸ãçºäºæ»¿è¶³éé è¨åºéæ±ï¼æåå¼å¥äº PropSAMï¼éæ¯ä¸ç¨®åºæ¼å³æ­çåå²æ¨¡åï¼åªåäº 3D é«å­¸çµæ§è³è¨çä½¿ç¨ãPropSAM æ´åäºä¸ååºæ¼ CNN ç UNetï¼ç¨æ¼åçå§èçï¼ä»¥åä¸ååºæ¼ Transformer çæ¨¡çµï¼ç¨æ¼åçéå³æ­ï¼éé»éæ³¨çµæ§åèªç¾©é£çºæ§ï¼ä»¥å¢å¼·åç¨®æ¨¡å¼ä¸çåå²ãèéè¦å©è¦æç¤ºçå³çµ±æ¨¡åä¸åï¼PropSAM ç¨ç¹å°éä½æ¼å®è¦æç¤ºä¸ï¼ä¾å¦ 2D éçæ¡æèåé®ç½©ãå®å·²è­æå·æåªç°çæè½ï¼é¡¯èæ¹åäº 44 åé«å­¸è³æéååç¨®å½±åæ¨¡å¼ä¸çéª°å­ç¸ä¼¼ä¿æ¸ (DSC)ï¼åªæ¼ MedSAM å SegVol ç­æ¨¡åï¼å¹³å DSC æåäº 18.1%ãåç®¡æç¤ºåå·®åå³æ­éç½®ä¸åï¼PropSAM ä»è½ç¶­æç©©å®çé æ¸¬ï¼éå·²ééå®å ANOVA æ¸¬è©¦å¾å°è­å¯¦ï¼åå¥çº P>0.5985 å P>0.6131ãæ­¤å¤ï¼PropSAM çé«ææ¶æ§è½å¯¦ç¾æ´å¿«çæ¨è«éåº¦ï¼Wilcoxon ç­ç´åç¸½åæª¢å®ï¼P<0.001ï¼ï¼ä¸¦å°ä½¿ç¨èäºåæéæ¸å°äº 37.8%ï¼åªæ¼å©è¦æç¤ºæ¨¡åãå®å¨èçä¸è¦ååè¤éç©ä»¶æè½å±ç¾åºç©©å¥çæè½ï¼é²ä¸æ­¥è­æäºå¶å¨è¨åºç°å¢ä¸­çæ½åï¼æå©æ¼ä»¥æå°çéæ°è¨ç·´é²è¡æ´èªåååå¯é çé«å­¸å½±ååæã

##### **Submodular Maximization Approaches for Equitable Client Selection in Federated Learning**
2408.13683v2 by AndrÃ©s Catalino Castillo JimÃ©nez, Ege C. Kaya, Lintao Ye, Abolfazl Hashemi

In a conventional Federated Learning framework, client selection for training
typically involves the random sampling of a subset of clients in each
iteration. However, this random selection often leads to disparate performance
among clients, raising concerns regarding fairness, particularly in
applications where equitable outcomes are crucial, such as in medical or
financial machine learning tasks. This disparity typically becomes more
pronounced with the advent of performance-centric client sampling techniques.
This paper introduces two novel methods, namely SUBTRUNC and UNIONFL, designed
to address the limitations of random client selection. Both approaches utilize
submodular function maximization to achieve more balanced models. By modifying
the facility location problem, they aim to mitigate the fairness concerns
associated with random selection. SUBTRUNC leverages client loss information to
diversify solutions, while UNIONFL relies on historical client selection data
to ensure a more equitable performance of the final model. Moreover, these
algorithms are accompanied by robust theoretical guarantees regarding
convergence under reasonable assumptions. The efficacy of these methods is
demonstrated through extensive evaluations across heterogeneous scenarios,
revealing significant improvements in fairness as measured by a client
dissimilarity metric.

æè¦ï¼å¨å³çµ±çè¯é¦å­¸ç¿æ¡æ¶ä¸­ï¼è¨ç·´çç¨æ¶ç«¯é¸æéå¸¸æ¶åå¨æ¯æ¬¡è¿­ä»£ä¸­é¨æ©æ½åç¨æ¶ç«¯å­éãç¶èï¼éç¨®é¨æ©é¸æéå¸¸æå°è´ç¨æ¶ç«¯ä¹éçè¡¨ç¾å·®ç°ï¼å¼ç¼äºå¬å¹³æ§çææï¼ç¹å¥æ¯å¨å¬å¹³çµæè³ééè¦çæç¨ä¸­ï¼ä¾å¦é«çæéèæ©å¨å­¸ç¿ä»»åãéç¨®å·®ç°éå¸¸æé¨èä»¥æè½çºä¸­å¿çç¨æ¶ç«¯æ½æ¨£æè¡çåºç¾èè®å¾æ´å æé¡¯ãæ¬æä»ç´¹äºå©ç¨®æ°æ¹æ³ï¼å³ SUBTRUNC å UNIONFLï¼æ¨å¨è§£æ±ºé¨æ©ç¨æ¶ç«¯é¸æçéå¶ãéå©ç¨®æ¹æ³é½å©ç¨æ¬¡æ¨¡å½æ¸æå¤§åä¾å¯¦ç¾æ´å¹³è¡¡çæ¨¡åãééä¿®æ¹è¨­æ½ä½ç½®åé¡ï¼å®åæ¨å¨ç·©è§£èé¨æ©é¸æç¸éçå¬å¹³æ§åé¡ãSUBTRUNC å©ç¨ç¨æ¶ç«¯æå¤±è³è¨ä¾åæ£è§£æ±ºæ¹æ¡ï¼è UNIONFL ä¾è³´æ¼æ­·å²ç¨æ¶ç«¯é¸æè³æï¼ä»¥ç¢ºä¿æçµæ¨¡åçæè½æ´å¬å¹³ãæ­¤å¤ï¼éäºæ¼ç®æ³éå¸¶äºéæ¼åçåè¨­ä¸æ¶ææ§çå¼·å¤§çè«ä¿è­ãéäºæ¹æ³çæåééåç¨®ç°è³ªå ´æ¯çå»£æ³è©ä¼°å¾å°è­æï¼é¡¯ç¤ºåºå¬å¹³æ§æé¡¯èæ¹åï¼éæ¯ééç¨æ¶ç«¯å·®ç°åº¦éææ¨æ¸¬éçã

##### **Towards Case-based Interpretability for Medical Federated Learning**
2408.13626v1 by Laura Latorre, Liliana Petrychenko, Regina Beets-Tan, Taisiya Kopytova, Wilson Silva

We explore deep generative models to generate case-based explanations in a
medical federated learning setting. Explaining AI model decisions through
case-based interpretability is paramount to increasing trust and allowing
widespread adoption of AI in clinical practice. However, medical AI training
paradigms are shifting towards federated learning settings in order to comply
with data protection regulations. In a federated scenario, past data is
inaccessible to the current user. Thus, we use a deep generative model to
generate synthetic examples that protect privacy and explain decisions. Our
proof-of-concept focuses on pleural effusion diagnosis and uses publicly
available Chest X-ray data.

æè¦ï¼æåæ¢ç´¢æ·±åº¦çææ¨¡åï¼å¨é«çè¯é¦å­¸ç¿è¨­ç½®ä¸­çæåºæ¼æ¡ä¾çèªªæãééåºæ¼æ¡ä¾çå¯è§£éæ§ä¾è§£é AI æ¨¡åæ±ºç­ï¼å°æ¼å¢å ä¿¡ä»»ä¸¦åè¨± AI å¨è¨åºå¯¦åä¸­å»£æ³æ¡ç¨è³ééè¦ãç¶èï¼é«ç AI è¨ç·´ç¯ä¾æ­£è½åè¯é¦å­¸ç¿è¨­ç½®ï¼ä»¥ç¬¦åè³æä¿è­·æ³è¦ãå¨è¯é¦æå¢ä¸­ï¼éå»çè³æå°ç®åçä½¿ç¨èèè¨æ¯ç¡æ³åå¾çãå æ­¤ï¼æåä½¿ç¨æ·±åº¦çææ¨¡åä¾ç¢çä¿è­·é±ç§åè§£éæ±ºç­çåæç¯ä¾ãæåçæ¦å¿µé©è­èéæ¼è¸èç©æ¶²è¨ºæ·ï¼ä¸¦ä½¿ç¨å¬éå¯åå¾çè¸é¨ X åè³æã

##### **HBIC: A Biclustering Algorithm for Heterogeneous Datasets**
2408.13217v1 by AdÃ¡n JosÃ©-GarcÃ­a, Julie Jacques, ClÃ©ment Chauvet, Vincent Sobanski, Clarisse Dhaenens

Biclustering is an unsupervised machine-learning approach aiming to cluster
rows and columns simultaneously in a data matrix. Several biclustering
algorithms have been proposed for handling numeric datasets. However,
real-world data mining problems often involve heterogeneous datasets with mixed
attributes. To address this challenge, we introduce a biclustering approach
called HBIC, capable of discovering meaningful biclusters in complex
heterogeneous data, including numeric, binary, and categorical data. The
approach comprises two stages: bicluster generation and bicluster model
selection. In the initial stage, several candidate biclusters are generated
iteratively by adding and removing rows and columns based on the frequency of
values in the original matrix. In the second stage, we introduce two approaches
for selecting the most suitable biclusters by considering their size and
homogeneity. Through a series of experiments, we investigated the suitability
of our approach on a synthetic benchmark and in a biomedical application
involving clinical data of systemic sclerosis patients. The evaluation
comparing our method to existing approaches demonstrates its ability to
discover high-quality biclusters from heterogeneous data. Our biclustering
approach is a starting point for heterogeneous bicluster discovery, leading to
a better understanding of complex underlying data structures.

æè¦ï¼éèé¡æ¯ä¸ç¨®éç£ç£æ©å¨å­¸ç¿æ¹æ³ï¼æ¨å¨åæå°è³æç©é£ä¸­çååè¡é²è¡èé¡ãå·²æåºå¤ç¨®éèé¡æ¼ç®æ³ä¾èçæ¸å¼è³æéãç¶èï¼ç¾å¯¦ä¸ççè³ææ¢ååé¡éå¸¸æ¶åå·ææ··åå±¬æ§çç°è³ªè³æéãçºäºæå°éä¸ææ°ï¼æåå¼å¥äºä¸ç¨®åçº HBIC çéèé¡æ¹æ³ï¼å®è½å¤ å¨è¤éçç°è³ªè³æï¼åæ¬æ¸å¼ãäºé²å¶åé¡å¥è³æï¼ä¸­ç¼ç¾ææç¾©çéèé¡ãè©²æ¹æ³åæ¬å©åéæ®µï¼éèé¡çæåéèé¡æ¨¡åé¸æãå¨åå§éæ®µï¼ééæ ¹æåå§ç©é£ä¸­çå¼é »çæ°å¢åç§»é¤ååè¡ï¼åè¦çæå¤ååé¸éèé¡ãå¨ç¬¬äºéæ®µï¼æåå¼å¥äºå©ç¨®æ¹æ³ï¼ééèæ®éèé¡çå¤§å°ååè³ªæ§ä¾é¸ææåé©çéèé¡ãééä¸ç³»åå¯¦é©ï¼æåç ç©¶äºæåçæ¹æ³å¨åæåºæºåæ¶åå¨èº«æ§ç¡¬åçæ£èè¨åºè³æççç©é«å­¸æç¨ä¸­çé©ç¨æ§ãå°æåçæ¼ç®æ³èç¾ææ¹æ³é²è¡æ¯è¼ï¼è©ä¼°çµæè­æäºå¶å¾ç°è³ªè³æä¸­ç¼ç¾é«åè³ªéèé¡çè½åãæåçéèé¡æ¹æ³æ¯ç°è³ªéèé¡ç¼ç¾çèµ·é»ï¼æå©æ¼æ´å¥½å°çè§£è¤éçåºå±¤è³æçµæ§ã

##### **Causal machine learning for sustainable agroecosystems**
2408.13155v1 by Vasileios Sitokonstantinou, Emiliano DÃ­az Salas Porras, Jordi CerdÃ  Bautista, Maria Piles, Ioannis Athanasiadis, Hannah Kerner, Giulia Martini, Lily-belle Sweet, Ilias Tsoumas, Jakob Zscheischler, Gustau Camps-Valls

In a changing climate, sustainable agriculture is essential for food security
and environmental health. However, it is challenging to understand the complex
interactions among its biophysical, social, and economic components. Predictive
machine learning (ML), with its capacity to learn from data, is leveraged in
sustainable agriculture for applications like yield prediction and weather
forecasting. Nevertheless, it cannot explain causal mechanisms and remains
descriptive rather than prescriptive. To address this gap, we propose causal
ML, which merges ML's data processing with causality's ability to reason about
change. This facilitates quantifying intervention impacts for evidence-based
decision-making and enhances predictive model robustness. We showcase causal ML
through eight diverse applications that benefit stakeholders across the
agri-food chain, including farmers, policymakers, and researchers.

æè¦ï¼å¨æ°£åè®é·çå½±é¿ä¸ï¼æ°¸çºè¾²æ¥­å°æ¼ç³§é£å®å¨åç°å¢å¥åº·è³ééè¦ãç¶èï¼è¦äºè§£å¶çç©ç©çãç¤¾æåç¶æ¿æåä¹éçè¤éäºåå»æ¯ä¸é ææ°ãé æ¸¬æ©å¨å­¸ç¿ (ML) å·åå¾è³æä¸­å­¸ç¿çè½åï¼å æ­¤è¢«éç¨æ¼æ°¸çºè¾²æ¥­ä¸­ï¼ä¾å¦ç¢éé æ¸¬åå¤©æ°£é å ±ç­æç¨ãåç®¡å¦æ­¤ï¼å®ç¡æ³è§£éå ææ©å¶ï¼ä¸ä»ç¶æ¯æè¿°æ§çèéè¦ç¯æ§çãçºäºè§£æ±ºéåå·®è·ï¼æåæåºäºå æ MLï¼å®å° ML çè³æèçèå æéä¿æ¨çè®åçè½åçµåå¨ä¸èµ·ãéæå©æ¼éåå¹²é å½±é¿ï¼ä»¥å©æ¼åºæ¼è­æçæ±ºç­å¶å®ï¼ä¸¦å¢å¼·é æ¸¬æ¨¡åçç©©å¥æ§ãæåééå«åä¸åçæç¨å±ç¤ºå æ MLï¼éäºæç¨æ åè¾²æ¥­é£åéä¸­çå©å®³éä¿äººï¼åæ¬è¾²æ°ãæ¿ç­å¶å®èåç ç©¶äººå¡ã

##### **Has Multimodal Learning Delivered Universal Intelligence in Healthcare? A Comprehensive Survey**
2408.12880v1 by Qika Lin, Yifan Zhu, Xin Mei, Ling Huang, Jingying Ma, Kai He, Zhen Peng, Erik Cambria, Mengling Feng

The rapid development of artificial intelligence has constantly reshaped the
field of intelligent healthcare and medicine. As a vital technology, multimodal
learning has increasingly garnered interest due to data complementarity,
comprehensive modeling form, and great application potential. Currently,
numerous researchers are dedicating their attention to this field, conducting
extensive studies and constructing abundant intelligent systems. Naturally, an
open question arises that has multimodal learning delivered universal
intelligence in healthcare? To answer the question, we adopt three unique
viewpoints for a holistic analysis. Firstly, we conduct a comprehensive survey
of the current progress of medical multimodal learning from the perspectives of
datasets, task-oriented methods, and universal foundation models. Based on
them, we further discuss the proposed question from five issues to explore the
real impacts of advanced techniques in healthcare, from data and technologies
to performance and ethics. The answer is that current technologies have NOT
achieved universal intelligence and there remains a significant journey to
undertake. Finally, in light of the above reviews and discussions, we point out
ten potential directions for exploration towards the goal of universal
intelligence in healthcare.

æè¦ï¼äººå·¥æºè½çå¿«éåå±æç»­éå¡çæºè½å»çåå»å­¦é¢åãä½ä¸ºä¸é¡¹è³å³éè¦çææ¯ï¼å¤æ¨¡æå­¦ä¹ ç±äºæ°æ®äºè¡¥æ§ãç»¼åå»ºæ¨¡å½¢å¼åå·¨å¤§çåºç¨æ½åèæ¥çåå°å³æ³¨ãç®åï¼ä¼å¤ç ç©¶èå°æ³¨æåæåè¿ä¸é¢åï¼å¼å±äºå¹¿æ³çç ç©¶å¹¶æå»ºäºä¸°å¯çæºè½ç³»ç»ãèªç¶èç¶å°ï¼ä¸ä¸ªå¼æ¾çé®é¢åºç°äºï¼å³å¤æ¨¡æå­¦ä¹ æ¯å¦å¨å»çä¿å¥ä¸­æä¾äºéç¨æºè½ï¼ä¸ºäºåç­è¿ä¸ªé®é¢ï¼æä»¬éç¨ä¸ä¸ªç¬ç¹çè§è§è¿è¡æ´ä½åæãé¦åï¼æä»¬ä»æ°æ®éãé¢åä»»å¡çæ¹æ³åéç¨åºç¡æ¨¡åçè§åº¦å¯¹å»å­¦å¤æ¨¡æå­¦ä¹ çå½åè¿å±è¿è¡äºå¨é¢çè°æ¥ãå¨æ­¤åºç¡ä¸ï¼æä»¬è¿ä¸æ­¥ä»äºä¸ªé®é¢è®¨è®ºäºæåºçé®é¢ï¼ä»¥æ¢è®¨åè¿ææ¯å¨å»çä¿å¥ä¸­çå®éå½±åï¼ä»æ°æ®åææ¯å°æ§è½åä¼¦çãç­æ¡æ¯ï¼å½åææ¯å°æªå®ç°éç¨æºè½ï¼å¹¶ä¸ä»æå¾é¿çè·¯è¦èµ°ãæåï¼æ ¹æ®ä¸è¿°åé¡¾åè®¨è®ºï¼æä»¬æåºäºå®ç°å»çä¿å¥éç¨æºè½ç®æ çåä¸ªæ½å¨æ¢ç´¢æ¹åã

##### **COVID-19 Probability Prediction Using Machine Learning: An Infectious Approach**
2408.12841v1 by Mohsen Asghari Ilani, Saba Moftakhar Tehran, Ashkan Kavei, Arian Radmehr

The ongoing COVID-19 pandemic continues to pose significant challenges to
global public health, despite the widespread availability of vaccines. Early
detection of the disease remains paramount in curbing its transmission and
mitigating its impact on public health systems. In response, this study delves
into the application of advanced machine learning (ML) techniques for
predicting COVID-19 infection probability. We conducted a rigorous
investigation into the efficacy of various ML models, including XGBoost, LGBM,
AdaBoost, Logistic Regression, Decision Tree, RandomForest, CatBoost, KNN, and
Deep Neural Networks (DNN). Leveraging a dataset comprising 4000 samples, with
3200 allocated for training and 800 for testing, our experiment offers
comprehensive insights into the performance of these models in COVID-19
prediction. Our findings reveal that Deep Neural Networks (DNN) emerge as the
top-performing model, exhibiting superior accuracy and recall metrics. With an
impressive accuracy rate of 89%, DNN demonstrates remarkable potential in early
COVID-19 detection. This underscores the efficacy of deep learning approaches
in leveraging complex data patterns to identify COVID-19 infections accurately.
This study underscores the critical role of machine learning, particularly deep
learning methodologies, in augmenting early detection efforts amidst the
ongoing pandemic. The success of DNN in accurately predicting COVID-19
infection probability highlights the importance of continued research and
development in leveraging advanced technologies to combat infectious diseases.

æè¦ï¼æçºé²è¡ç COVID-19 å¤§æµè¡çæçºå°å¨çå¬å±è¡çæ§æéå¤§ææ°ï¼åç®¡ç«èå·²å»£æ³æä¾ãæ©æç¼ç¾ç¾çä»ç¶æ¯éå¶å¶å³æ­åæ¸è¼å¶å°å¬å±è¡çç³»çµ±å½±é¿çé¦è¦ä»»åãçºæ­¤ï¼æ¬ç ç©¶æ·±å¥æ¢è¨åé²æ©å¨å­¸ç¿ (ML) æè¡å¨é æ¸¬ COVID-19 æææ©çæ¹é¢çæç¨ãæåå°åç¨® ML æ¨¡åçæè½é²è¡äºå´è¬¹çèª¿æ¥ï¼åæ¬ XGBoostãLGBMãAdaBoostãéè¼¯è¿´æ­¸ãæ±ºç­æ¨¹ãé¨æ©æ£®æãCatBoostãKNN åæ·±åº¦ç¥ç¶ç¶²è·¯ (DNN)ãå©ç¨åå« 4000 åæ¨£æ¬çè³æéï¼å¶ä¸­ 3200 ååéçµ¦è¨ç·´ï¼800 ååéçµ¦æ¸¬è©¦ï¼æåçå¯¦é©å°éäºæ¨¡åå¨ COVID-19 é æ¸¬ä¸­çæè½æä¾äºå¨é¢çè¦è§£ãæåçç ç©¶çµæé¡¯ç¤ºï¼æ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) æçºè¡¨ç¾æä½³çæ¨¡åï¼å±ç¾åºåªç°çæºç¢ºåº¦åå¬åçææ¨ãDNN ä»¥ 89% çé©äººæºç¢ºåº¦ï¼è­æäºå¨æ©æ COVID-19 æª¢æ¸¬ä¸­çååºæ½åãéçªé¡¯äºæ·±åº¦å­¸ç¿æ¹æ³å¨å©ç¨è¤éè³ææ¨¡å¼æºç¢ºè­å¥ COVID-19 æææ¹é¢çæè½ãæ¬ç ç©¶å¼·èª¿äºæ©å¨å­¸ç¿ï¼ç¹å¥æ¯æ·±åº¦å­¸ç¿æ¹æ³ï¼å¨æçºçå¤§æµè¡çä¸­æ´å¢æ©ææª¢æ¸¬å·¥ä½ä¸­çééµä½ç¨ãDNN å¨æºç¢ºé æ¸¬ COVID-19 æææ©çæ¹é¢çæåï¼çªé¡¯äºæçºç ç©¶åéç¼å©ç¨åé²æè¡ä¾å°æå³æççéè¦æ§ã

##### **Exploring Machine Learning Models for Lung Cancer Level Classification: A comparative ML Approach**
2408.12838v1 by Mohsen Asghari Ilani, Saba Moftakhar Tehran, Ashkan Kavei, Hamed Alizadegan

This paper explores machine learning (ML) models for classifying lung cancer
levels to improve diagnostic accuracy and prognosis. Through parameter tuning
and rigorous evaluation, we assess various ML algorithms. Techniques like
minimum child weight and learning rate monitoring were used to reduce
overfitting and optimize performance. Our findings highlight the robust
performance of Deep Neural Network (DNN) models across all phases. Ensemble
methods, including voting and bagging, also showed promise in enhancing
predictive accuracy and robustness. However, Support Vector Machine (SVM)
models with the Sigmoid kernel faced challenges, indicating a need for further
refinement. Overall, our study provides insights into ML-based lung cancer
classification, emphasizing the importance of parameter tuning to optimize
model performance and improve diagnostic accuracy in oncological care.

æè¦ï¼æ¬è«ææ¢è¨æ©å¨å­¸ç¿ (ML) æ¨¡åï¼ç¨æ¼åé¡èºçç­ç´ä»¥æåè¨ºæ·æºç¢ºåº¦åé å¾ãééåæ¸èª¿æ´åå´è¬¹è©ä¼°ï¼æåè©ä¼°åç¨® ML æ¼ç®æ³ãä½¿ç¨æå°å­æ¬éåå­¸ç¿çç£æ§ç­æè¡ä¾æ¸å°éåº¦æ¬åä¸¦æä½³åæè½ãæåçç ç©¶çµæå¼·èª¿æ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) æ¨¡åå¨ææéæ®µçå¼·å¥æè½ãåæ¬æç¥¨å bagging å¨å§çæ´é«æ¹æ³ï¼ä¹å¨æåé æ¸¬æºç¢ºåº¦åå¼·å¥æ§æ¹é¢å±ç¾åªå¢ãç¶èï¼ä½¿ç¨ Sigmoid æ ¸å¿çæ¯æ´åéæ© (SVM) æ¨¡åé¢è¨ææ°ï¼é¡¯ç¤ºéè¦é²ä¸æ­¥æ¹è¯ãæ´é«èè¨ï¼æåçç ç©¶æä¾æ©å¨å­¸ç¿çºåºç¤çèºçåé¡è¦è§£ï¼å¼·èª¿åæ¸èª¿æ´å°æ¼æä½³åæ¨¡åæè½åæåè«ç¤ç§è­·è¨ºæ·æºç¢ºåº¦çéè¦æ§ã

##### **Phrasing for UX: Enhancing Information Engagement through Computational Linguistics and Creative Analytics**
2409.00064v1 by Nimrod Dvir

This study explores the relationship between textual features and Information
Engagement (IE) on digital platforms. It highlights the impact of computational
linguistics and analytics on user interaction. The READ model is introduced to
quantify key predictors like representativeness, ease of use, affect, and
distribution, which forecast engagement levels. The model's effectiveness is
validated through AB testing and randomized trials, showing strong predictive
performance in participation (accuracy: 0.94), perception (accuracy: 0.85),
perseverance (accuracy: 0.81), and overall IE (accuracy: 0.97).
  While participation metrics are strong, perception and perseverance show
slightly lower recall and F1-scores, indicating some challenges. The study
demonstrates that modifying text based on the READ model's insights leads to
significant improvements. For example, increasing representativeness and
positive affect boosts selection rates by 11 percent, raises evaluation
averages from 3.98 to 4.46, and improves retention rates by 11 percent. These
findings highlight the importance of linguistic factors in IE, providing a
framework for enhancing digital text engagement. The research offers practical
strategies applicable to fields like education, health, and media.

æè¦ï¼æ¬ç ç©¶æ¢è¨äºæ¸ä½å¹³å°ä¸ææ¬ç¹å¾µèè³è¨åèåº¦ (IE) ä¹éçéä¿ãå®å¼·èª¿äºè¨ç®èªè¨å­¸ååæå°ä½¿ç¨èäºåçå½±é¿ãREAD æ¨¡åè¢«å¼å¥ç¨æ¼éåééµé æ¸¬å å­ï¼ä¾å¦ä»£è¡¨æ§ãæç¨æ§ãå½±é¿åï¼ä»¥åé æ¸¬åèç¨åº¦çåå¸ãè©²æ¨¡åçæææ§å·²éé AB æ¸¬è©¦åé¨æ©è©¦é©å¾å°é©è­ï¼å¨åèåº¦ï¼æºç¢ºåº¦ï¼0.94ï¼ãæç¥ï¼æºç¢ºåº¦ï¼0.85ï¼ãå æåº¦ï¼æºç¢ºåº¦ï¼0.81ï¼åæ´é« IEï¼æºç¢ºåº¦ï¼0.97ï¼ä¸­é¡¯ç¤ºåºå¼·å¤§çé æ¸¬æè½ãéç¶åèåº¦ææ¨å¾å¼·ï¼ä½æç¥åå æåº¦é¡¯ç¤ºåºç¨ä½çå¬åçå F1 åæ¸ï¼è¡¨æå­å¨ä¸äºææ°ãç ç©¶è¡¨æï¼æ ¹æ READ æ¨¡åçè¦è§£ä¿®æ¹æå­æå¸¶ä¾é¡¯èçæ¹é²ãä¾å¦ï¼æé«ä»£è¡¨æ§åæ­£é¢å½±é¿åå¯å°é¸æçæé« 11%ï¼å°è©åå¹³åå¼å¾ 3.98 æé«å° 4.46ï¼ä¸¦å°ä¿ççæé« 11%ãéäºç¼ç¾å¼·èª¿äºèªè¨å ç´ å¨ IE ä¸­çéè¦æ§ï¼æä¾äºä¸åå¢å¼·æ¸ä½æå­åèåº¦çæ¡æ¶ãè©²ç ç©¶æä¾äºé©ç¨æ¼æè²ãå¥åº·ååªé«ç­é åçå¯¦ç¨ç­ç¥ã

##### **From Radiologist Report to Image Label: Assessing Latent Dirichlet Allocation in Training Neural Networks for Orthopedic Radiograph Classification**
2408.13284v1 by Jakub Olczak, Max Gordon

Background: Radiography (X-rays) is the dominant modality in orthopedics, and
improving the interpretation of radiographs is clinically relevant. Machine
learning (ML) has revolutionized data analysis and has been applied to
medicine, with some success, in the form of natural language processing (NLP)
and artificial neural networks (ANN). Latent Dirichlet allocation (LDA) is an
NLP method that automatically categorizes documents into topics. Successfully
applying ML to orthopedic radiography could enable the creation of
computer-aided decision systems for use in the clinic. We studied how an
automated ML pipeline could classify orthopedic trauma radiographs from
radiologist reports. Methods: Wrist and ankle radiographs from Danderyd
Hospital in Sweden taken between 2002 and 2015, with radiologist reports. LDA
was used to create image labels for radiographs from the radiologist reports.
Radiographs and labels were used to train an image recognition ANN. The ANN
outcomes were manually reviewed to get an accurate estimate of the method's
utility and accuracy. Results: Image Labels generated via LDA could
successfully train the ANN. The ANN reached an accuracy between 91% and 60%
compared to a gold standard, depending on the label. Conclusions: We found that
LDA was unsuited to label orthopedic radiographs from reports with high
accuracy. However, despite this, the ANN could learn to detect some features in
radiographs with high accuracy. The study also illustrates how ML and ANN can
be applied to medical research.

æè¦ï¼<paragraph>èæ¯ï¼æ¾å°ç§ä¸­ï¼æ¾å°ç·ç§ç¸ï¼X åï¼æ¯ä¸»è¦çæ¨¡å¼ï¼èæ¹åæ¾å°ç·ç§ç¸çè§£è®å¨è¨åºä¸å·æç¸éæ§ãæ©å¨å­¸ç¿ï¼MLï¼å¾¹åºæ¹è®äºæ¸æåæï¼ä¸¦ä»¥èªç¶èªè¨èçï¼NLPï¼åäººå·¥ç¥ç¶ç¶²è·¯ï¼ANNï¼çå½¢å¼æç¨æ¼é«å­¸ï¼ä¸¦åå¾äºä¸äºæåãæ½å¨çå©åé·éç½®ï¼LDAï¼æ¯ä¸ç¨® NLP æ¹æ³ï¼å¯èªåå°æä»¶åé¡çºä¸»é¡ãæåå° ML æç¨æ¼éª¨ç§æ¾å°ç·ç§ç¸å¯ä»¥åµå»ºé»è¦è¼å©æ±ºç­ç³»çµ±ï¼ä¾è¨ºæä½¿ç¨ãæåç ç©¶äºèªåå ML ç®¡ç·å¦ä½å¾æ¾å°ç§é«å¸«çå ±åä¸­å°éª¨ç§åµå·æ¾å°ç·ç§ç¸é²è¡åé¡ãæ¹æ³ï¼ä½¿ç¨æ¾å°ç§é«å¸«çå ±åï¼æ¼ 2002 å¹´è³ 2015 å¹´éå¨çå¸ Danderyd é«é¢ææçæèåè³è¸æ¾å°ç·ç§ç¸ãLDA ç¨æ¼æ ¹ææ¾å°ç§é«å¸«çå ±åçºæ¾å°ç·ç§ç¸å»ºç«å½±åæ¨ç±¤ãæ¾å°ç·ç§ç¸åæ¨ç±¤ç¨æ¼è¨ç·´å½±åè¾¨è­ ANNãæåæª¢é± ANN çµæä»¥æºç¢ºä¼°è¨æ¹æ³çæç¨åæºç¢ºæ§ãçµæï¼éé LDA çæçå½±åæ¨ç±¤å¯ä»¥æåè¨ç·´ ANNãèé»éæ¨æºç¸æ¯ï¼ANN éå°äº 91% å° 60% çæºç¢ºåº¦ï¼å·é«åæ±ºæ¼æ¨ç±¤ãçµè«ï¼æåç¼ç¾ LDA ä¸é©åå¾å ±åä¸­æ¨è¨éª¨ç§æ¾å°ç·ç§ç¸ï¼æºç¢ºåº¦é«ãç¶èï¼åç®¡å¦æ­¤ï¼ANN ä»å¯ä»¥å­¸ç¿ä»¥é«æºç¢ºåº¦åµæ¸¬æ¾å°ç·ç§ç¸ä¸­çä¸äºç¹å¾µãè©²ç ç©¶éèªªæäº ML å ANN å¦ä½æç¨æ¼é«å­¸ç ç©¶ã</paragraph>

##### **MultiMed: Massively Multimodal and Multitask Medical Understanding**
2408.12682v1 by Shentong Mo, Paul Pu Liang

Biomedical data is inherently multimodal, consisting of electronic health
records, medical imaging, digital pathology, genome sequencing, wearable
sensors, and more. The application of artificial intelligence tools to these
multifaceted sensing technologies has the potential to revolutionize the
prognosis, diagnosis, and management of human health and disease. However,
current approaches to biomedical AI typically only train and evaluate with one
or a small set of medical modalities and tasks. This limitation hampers the
development of comprehensive tools that can leverage the rich interconnected
information across many heterogeneous biomedical sensors. To address this
challenge, we present MultiMed, a benchmark designed to evaluate and enable
large-scale learning across a wide spectrum of medical modalities and tasks.
MultiMed consists of 2.56 million samples across ten medical modalities such as
medical reports, pathology, genomics, and protein data, and is structured into
eleven challenging tasks, including disease prognosis, protein structure
prediction, and medical question answering. Using MultiMed, we conduct
comprehensive experiments benchmarking state-of-the-art unimodal, multimodal,
and multitask models. Our analysis highlights the advantages of training
large-scale medical models across many related modalities and tasks. Moreover,
MultiMed enables studies of generalization across related medical concepts,
robustness to real-world noisy data and distribution shifts, and novel modality
combinations to improve prediction performance. MultiMed will be publicly
available and regularly updated and welcomes inputs from the community.

æè¦ï¼çç©å»å­¦æ°æ®æ¬è´¨ä¸æ¯å¤æ¨¡æçï¼ç±çµå­å¥åº·è®°å½ãå»å­¦å½±åãæ°å­ççå­¦ãåºå ç»æµåºãå¯ç©¿æ´ä¼ æå¨ç­ç»æãå°äººå·¥æºè½å·¥å·åºç¨äºè¿äºå¤æ¹é¢çä¼ æææ¯æå¯è½å½»åºæ¹åäººç±»å¥åº·åç¾ççé¢åãè¯æ­åç®¡çãç¶èï¼å½åå¯¹çç©å»å­¦äººå·¥æºè½çæ¹æ³éå¸¸åªéå¯¹ä¸ç§æä¸å°ç»å»å­¦æ¹å¼åä»»å¡è¿è¡è®­ç»åè¯ä¼°ãè¿ç§éå¶é»ç¢äºè½å¤å©ç¨è®¸å¤å¼æçç©å»å­¦ä¼ æå¨ä¹é´çä¸°å¯äºèä¿¡æ¯æ¥å¼åç»¼åå·¥å·ãä¸ºäºåºå¯¹è¿ä¸ææï¼æä»¬æåºäº MultiMedï¼è¿æ¯ä¸ä¸ªæ¨å¨è¯ä¼°åæ¯æè·¨å¹¿æ³å»å­¦æ¹å¼åä»»å¡è¿è¡å¤§è§æ¨¡å­¦ä¹ çåºåãMultiMed åå«äºåç§å»å­¦æ¹å¼ï¼ä¾å¦å»å­¦æ¥åãççå­¦ãåºå ç»å­¦åèç½è´¨æ°æ®ï¼ä¸­ç 256 ä¸ä¸ªæ ·æ¬ï¼å¹¶è¢«æå»ºæåä¸ä¸ªå·ææææ§çä»»å¡ï¼åæ¬ç¾çé¢åãèç½è´¨ç»æé¢æµåå»å­¦é®é¢è§£ç­ãä½¿ç¨ MultiMedï¼æä»¬è¿è¡äºå¨é¢çå®éªï¼å¯¹æåè¿çåæ¨¡æãå¤æ¨¡æåå¤ä»»å¡æ¨¡åè¿è¡äºåºåæµè¯ãæä»¬çåæçªåºäºè·¨è®¸å¤ç¸å³æ¹å¼åä»»å¡è®­ç»å¤§è§æ¨¡å»å­¦æ¨¡åçä¼å¿ãæ­¤å¤ï¼MultiMed æ¯æå¯¹ç¸å³å»å­¦æ¦å¿µçæ³åãå¯¹çå®ä¸çåªå£°æ°æ®ååå¸ååçé²æ£æ§ä»¥åæ°çæ¹å¼ç»åä»¥æé«é¢æµæ§è½çç ç©¶ãMultiMed å°å¬å¼æä¾å¹¶å®ææ´æ°ï¼å¹¶æ¬¢è¿ç¤¾åºçæè§ã

##### **RuleAlign: Making Large Language Models Better Physicians with Diagnostic Rule Alignment**
2408.12579v1 by Xiaohan Wang, Xiaoyan Yang, Yuqi Zhu, Yue Shen, Jian Wang, Peng Wei, Lei Liang, Jinjie Gu, Huajun Chen, Ningyu Zhang

Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve
performance competitively with human experts across various medical benchmarks.
However, they still face challenges in making professional diagnoses akin to
physicians, particularly in efficiently gathering patient information and
reasoning the final diagnosis. To this end, we introduce the RuleAlign
framework, designed to align LLMs with specific diagnostic rules. We develop a
medical dialogue dataset comprising rule-based communications between patients
and physicians and design an alignment learning approach through preference
learning. Experimental results demonstrate the effectiveness of the proposed
approach. We hope that our work can serve as an inspiration for exploring the
potential of LLMs as AI physicians.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼ï¼ä¾å¦ GPT-4ãMedPaLM-2 å Med-Geminiï¼å¨åç¨®é«çåºæºä¸éå°äºèäººé¡å°å®¶ç«¶ç­çè¡¨ç¾ã
ç¶èï¼ä»åå¨ååºé¡ä¼¼æ¼é«å¸«çå°æ¥­è¨ºæ·æ¹é¢ä»é¢è¨ææ°ï¼ç¹å¥æ¯å¨æææ¶éæ£èè³è¨åæ¨è«æçµè¨ºæ·æ¹é¢ãçºæ­¤ï¼æåå¼å¥äº RuleAlign æ¡æ¶ï¼æ¨å¨å° LLM èç¹å®è¨ºæ·è¦åä¿æä¸è´ãæåéç¼äºä¸åé«çå°è©±è³æéï¼å¶ä¸­åå«æ£èèé«å¸«ä¹éåºæ¼è¦åçæºéï¼ä¸¦ééåå¥½å­¸ç¿è¨­è¨äºä¸ç¨®æ¯å°å­¸ç¿æ¹æ³ãå¯¦é©çµæè­æäºæææ¹æ³çæææ§ãæåå¸ææåçå·¥ä½è½æ¿åµæ¢ç´¢ LLM ä½çº AI é«å¸«çæ½åã

##### **Automatic Organ and Pan-cancer Segmentation in Abdomen CT: the FLARE 2023 Challenge**
2408.12534v1 by Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Ershuai Wang, Qin Zhou, Ziyan Huang, Pengju Lyu, Jian He, Bo Wang

Organ and cancer segmentation in abdomen Computed Tomography (CT) scans is
the prerequisite for precise cancer diagnosis and treatment. Most existing
benchmarks and algorithms are tailored to specific cancer types, limiting their
ability to provide comprehensive cancer analysis. This work presents the first
international competition on abdominal organ and pan-cancer segmentation by
providing a large-scale and diverse dataset, including 4650 CT scans with
various cancer types from over 40 medical centers. The winning team established
a new state-of-the-art with a deep learning-based cascaded framework, achieving
average Dice Similarity Coefficient scores of 92.3% for organs and 64.9% for
lesions on the hidden multi-national testing set. The dataset and code of top
teams are publicly available, offering a benchmark platform to drive further
innovations https://codalab.lisn.upsaclay.fr/competitions/12239.

æè¦ï¼å¨å®åççåå²å¨è¹é¨é»è¦æ·å±¤ææ (CT) ä¸­æ¯ç²¾ç¢ºççè¨ºæ·åæ²»ççåæ±ºæ¢ä»¶ãå¤§å¤æ¸ç¾æçåºæºåæ¼ç®æ³é½æ¯éå°ç¹å®ççé¡åéèº«æé ï¼ééå¶äºå®åæä¾å¨é¢ççåæçè½åãéé å·¥ä½æä¾äºç¬¬ä¸åéæ¼è¹é¨å¨å®åæ³çåå²çåéç«¶è³½ï¼æ¹æ³æ¯æä¾ä¸åå¤§åä¸å¤æ¨£åçè³æéï¼å¶ä¸­åæ¬ä¾èª 40 å¤åé«çä¸­å¿ç 4650 å CT ææï¼å¶ä¸­åå«åç¨®ççé¡åãç²ååéå»ºç«äºä¸åæ°çæåé²çæ·±åº¦å­¸ç¿ç´è¯æ¡æ¶ï¼å¨é±èçå¤åæ¸¬è©¦éä¸­å¯¦ç¾äºå¨å®çå¹³å Dice ç¸ä¼¼æ§ç³»æ¸åæ¸ 92.3%ï¼çç¶çå¹³å Dice ç¸ä¼¼æ§ç³»æ¸åæ¸ 64.9%ãé å°åéçè³æéåç¨å¼ç¢¼å¬éæä¾ï¼æä¾äºä¸ååºæºå¹³å°ï¼ä»¥æ¨åé²ä¸æ­¥çåµæ° https://codalab.lisn.upsaclay.fr/competitions/12239ã

##### **MEDCO: Medical Education Copilots Based on A Multi-Agent Framework**
2408.12496v1 by Hao Wei, Jianing Qiu, Haibao Yu, Wu Yuan

Large language models (LLMs) have had a significant impact on diverse
research domains, including medicine and healthcare. However, the potential of
LLMs as copilots in medical education remains underexplored. Current
AI-assisted educational tools are limited by their solitary learning approach
and inability to simulate the multi-disciplinary and interactive nature of
actual medical training. To address these limitations, we propose MEDCO
(Medical EDucation COpilots), a novel multi-agent-based copilot system
specially developed to emulate real-world medical training environments. MEDCO
incorporates three primary agents: an agentic patient, an expert doctor, and a
radiologist, facilitating a multi-modal and interactive learning environment.
Our framework emphasizes the learning of proficient question-asking skills,
multi-disciplinary collaboration, and peer discussions between students. Our
experiments show that simulated virtual students who underwent training with
MEDCO not only achieved substantial performance enhancements comparable to
those of advanced models, but also demonstrated human-like learning behaviors
and improvements, coupled with an increase in the number of learning samples.
This work contributes to medical education by introducing a copilot that
implements an interactive and collaborative learning approach. It also provides
valuable insights into the effectiveness of AI-integrated training paradigms.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å°ä¸åç ç©¶é åç¢çéå¤§å½±é¿ï¼åæ¬é«å­¸åä¿å¥ãç¶èï¼LLM ä½çºé«å­¸æè²å¯æçæ½åä»æªè¢«ååæ¢è¨ãç®åç AI è¼å©æè²å·¥å·åå°å¶å®ç¨å­¸ç¿æ¹æ³çéå¶ï¼ä¸ç¡æ³æ¨¡æ¬å¯¦éé«å­¸è¨ç·´çå¤å­¸ç§åäºåæ§è³ªãçºäºè§£æ±ºéäºéå¶ï¼æåæåº MEDCOï¼é«å­¸æè²å¯æï¼ï¼ä¸ç¨®æ°ç©çå¤ä»£çäººåä½ç³»çµ±ï¼å°ééç¼ç¨æ¼æ¨¡æ¬çå¯¦ä¸ççé«å­¸è¨ç·´ç°å¢ãMEDCO çµåäºä¸åä¸»è¦ä»£çäººï¼ä¸åä»£çäººæ£èãä¸åå°å®¶é«çåä¸åæ¾å°ç§é«çï¼ä¿é²å¤æ¨¡å¼åäºåå­¸ç¿ç°å¢ãæåçæ¶æ§å¼·èª¿å­¸ç¿çç·´çæåæå·§ãè·¨å­¸ç§åä½åå­¸çä¹éçååè¨è«ãæåçå¯¦é©è¡¨æï¼æ¥å MEDCO è¨ç·´çæ¨¡æ¬èæ¬å­¸çä¸åç²å¾èé²éæ¨¡åç¸ç¶çé¡¯èæè½æåï¼éè¡¨ç¾åºé¡ä¼¼äººé¡çå­¸ç¿è¡çºåé²æ­¥ï¼ä¸¦ä¼´é¨èå­¸ç¿æ¨£æ¬æ¸éçå¢å ãéé å·¥ä½ééå¼å¥ä¸åå¯¦æ½äºåååä½å­¸ç¿æ¹æ³çå¯æï¼å°é«å­¸æè²ææè²¢ç»ãå®ä¹æä¾äºå° AI æ´åè¨ç·´æ¨¡å¼æææ§çå¯¶è²´è¦è§£ã

##### **AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines**
2408.12491v1 by Douwe J. Spaanderman, Matthew Marzetti, Xinyi Wan, Andrew F. Scarsbrook, Philip Robinson, Edwin H. G. Oei, Jacob J. Visser, Robert Hemke, Kirsten van Langevelde, David F. Hanff, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. GruÃ¼hagen, Wiro J. Niessen, Stefan Klein, Martijn P. A. Starmans

Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging
lesions with variable clinical behaviours and treatment approaches. This
systematic review provides an overview of Artificial Intelligence (AI) methods
using radiological imaging for diagnosis and prognosis of these tumours,
highlighting challenges in clinical translation, and evaluating study alignment
with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI
international consensus guidelines for trustworthy and deployable AI to promote
the clinical translation of AI methods. The review covered literature from
several bibliographic databases, including papers published before 17/07/2024.
Original research in peer-reviewed journals focused on radiology-based AI for
diagnosing or prognosing primary STBT was included. Exclusion criteria were
animal, cadaveric, or laboratory studies, and non-English papers. Abstracts
were screened by two of three independent reviewers for eligibility. Eligible
papers were assessed against guidelines by one of three independent reviewers.
The search identified 15,015 abstracts, from which 325 articles were included
for evaluation. Most studies performed moderately on CLAIM, averaging a score
of 28.9$\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\pm$2.1 out
of 30. Imaging-AI tools for STBT remain at the proof-of-concept stage,
indicating significant room for improvement. Future efforts by AI developers
should focus on design (e.g. define unmet clinical need, intended clinical
setting and how AI would be integrated in clinical workflow), development (e.g.
build on previous work, explainability), evaluation (e.g. evaluating and
addressing biases, evaluating AI against best practices), and data
reproducibility and availability (making documented code and data publicly
available). Following these recommendations could improve clinical translation
of AI methods.

æè¦ï¼è»çµç¹åéª¨éª¼è«ç¤ï¼STBTï¼æ¯ç½è¦ãè¨ºæ·å·æææ°æ§ççç¶ï¼å¶è¨åºè¡çºåæ²»çæ¹æ³åä¸ç¸åãéç¯ç³»çµ±æ§åé¡§æä¾äºä½¿ç¨æ¾å°å½±åé²è¡è¨ºæ·åé å¾çäººå·¥æºæ§ (AI) æ¹æ³çæ¦è§ï¼éé»èªªæäºè¨åºè½è­¯çææ°ï¼ä¸¦è©ä¼°ç ç©¶èé«çå½±å AI æ ¸æ¥è¡¨ (CLAIM) å FUTURE-AI å¯ä¿¡è³´ä¸å¯é¨ç½² AI çåéå±è­æºåçä¸è´æ§ï¼ä»¥ä¿é² AI æ¹æ³çè¨åºè½è­¯ãéç¯åé¡§æ¶µèäºå¹¾åæ¸ç®è³æåº«ä¸­çæç»ï¼åæ¬å¨ 2024 å¹´ 7 æ 17 æ¥ä¹åç¼è¡¨çè«æãç´å¥äºä»¥æ¾å°çºåºç¤ç AI è¨ºæ·æé å¾åç¼æ§ STBT çåè¡è©å¯©æåä¸­çåå§ç ç©¶ãæé¤æ¨æºæ¯åç©ãå±é«æå¯¦é©å®¤ç ç©¶ï¼ä»¥åéè±æè«æãæè¦ç±ä¸ä½ç¨ç«å¯©æ¥å¡ä¸­çå©ä½ç¯©é¸è³æ ¼ãåæ ¼çè«æç±ä¸ä½ç¨ç«å¯©æ¥å¡ä¸­çä¸ä½æ ¹ææºåé²è¡è©ä¼°ãæç´¢è­å¥åº 15,015 ç¯æè¦ï¼å¶ä¸­ 325 ç¯æç« è¢«ç´å¥è©ä¼°ãå¤§å¤æ¸ç ç©¶å¨ CLAIM ä¸­è¡¨ç¾ä¸­ç­ï¼å¹³åå¾åçº 53 åä¸­ç 28.9Â±7.5 åï¼ä½å¨ FUTURE-AI ä¸­è¡¨ç¾ä¸ä½³ï¼å¹³åå¾åçº 30 åä¸­ç 5.1Â±2.1 åãSTBT çå½±å AI å·¥å·ä»èæ¼æ¦å¿µé©è­éæ®µï¼è¡¨ææé¡¯èçæ¹é²ç©ºéãAI éç¼äººå¡æªä¾çåªåæéä¸­å¨è¨­è¨ï¼ä¾å¦å®ç¾©æªæ»¿è¶³çè¨åºéæ±ãé æçè¨åºç°å¢ä»¥å AI å¦ä½æ´åå°è¨åºå·¥ä½æµç¨ä¸­ï¼ãéç¼ï¼ä¾å¦å»ºç«å¨ååçå·¥ä½ãå¯è§£éæ§ï¼ãè©ä¼°ï¼ä¾å¦è©ä¼°åè§£æ±ºåå·®ãè©ä¼° AI èæä½³å¯¦åï¼ãä»¥åæ¸æå¯è¤è£½æ§åå¯ç¨æ§ï¼å¬éæä¾æä»¶åçä»£ç¢¼åæ¸æï¼ãéµå¾ªéäºå»ºè­°å¯ä»¥æ¹å AI æ¹æ³çè¨åºè½è­¯ã

##### **WCEbleedGen: A wireless capsule endoscopy dataset and its benchmarking for automatic bleeding classification, detection, and segmentation**
2408.12466v1 by Palak Handa, Manas Dhir, Amirreza Mahbod, Florian Schwarzhans, Ramona Woitek, Nidhi Goel, Deepak Gunjan

Computer-based analysis of Wireless Capsule Endoscopy (WCE) is crucial.
However, a medically annotated WCE dataset for training and evaluation of
automatic classification, detection, and segmentation of bleeding and
non-bleeding frames is currently lacking. The present work focused on
development of a medically annotated WCE dataset called WCEbleedGen for
automatic classification, detection, and segmentation of bleeding and
non-bleeding frames. It comprises 2,618 WCE bleeding and non-bleeding frames
which were collected from various internet resources and existing WCE datasets.
A comprehensive benchmarking and evaluation of the developed dataset was done
using nine classification-based, three detection-based, and three
segmentation-based deep learning models. The dataset is of high-quality, is
class-balanced and contains single and multiple bleeding sites. Overall, our
standard benchmark results show that Visual Geometric Group (VGG) 19, You Only
Look Once version 8 nano (YOLOv8n), and Link network (Linknet) performed best
in automatic classification, detection, and segmentation-based evaluations,
respectively. Automatic bleeding diagnosis is crucial for WCE video
interpretations. This diverse dataset will aid in developing of real-time,
multi-task learning-based innovative solutions for automatic bleeding diagnosis
in WCE. The dataset and code are publicly available at
https://zenodo.org/records/10156571 and
https://github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset.

æè¦ï¼<paragraph>ç¡ç·è åå§è¦é¡ (WCE) çé»è¦åæè³ééè¦ã
ç¶èï¼ç®åç¼ºä¹ä¸åé«å­¸æ¨è¨»ç WCE è³æéï¼ç¨æ¼è¨ç·´åè©ä¼°åºè¡åéåºè¡å¹çèªååé¡ãæª¢æ¸¬ååå²ãæ¬ç ç©¶å°æ³¨æ¼éç¼ä¸ååçº WCEbleedGen çé«å­¸æ¨è¨» WCE è³æéï¼ç¨æ¼åºè¡åéåºè¡å¹çèªååé¡ãæª¢æ¸¬ååå²ãå®åå« 2,618 å WCE åºè¡åéåºè¡å¹ï¼éäºå¹æ¯å¾åç¨®ç¶²è·¯è³æºåç¾æç WCE è³æéä¸­æ¶éçãä½¿ç¨ä¹ååºæ¼åé¡ãä¸ååºæ¼æª¢æ¸¬åä¸ååºæ¼åå²çæ·±åº¦å­¸ç¿æ¨¡åå°éç¼çè³æéé²è¡äºå¨é¢çåºæºæ¸¬è©¦åè©ä¼°ãè©²è³æéè³ªéé«ãé¡å¥å¹³è¡¡ä¸åå«å®ååå¤ååºè¡é¨ä½ãç¸½é«èè¨ï¼æåçæ¨æºåºæºæ¸¬è©¦çµæè¡¨æï¼Visual Geometric Group (VGG) 19ãYou Only Look Once çæ¬ 8 nano (YOLOv8n) å Link ç¶²è·¯ (Linknet) å¨èªååé¡ãæª¢æ¸¬ååºæ¼åå²çè©ä¼°ä¸­è¡¨ç¾æä½³ï¼åå¥ãèªååºè¡è¨ºæ·å°æ¼ WCE è¦è¨è§£è®è³ééè¦ãéåå¤æ¨£åçè³æéå°æå©æ¼éç¼ç¨æ¼ WCE ä¸­èªååºè¡è¨ºæ·çåºæ¼å¯¦æãå¤ä»»åå­¸ç¿çåµæ°è§£æ±ºæ¹æ¡ãè©²è³æéåç¨å¼ç¢¼å¯å¨ https://zenodo.org/records/10156571 å https://github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset å¬éç²å¾ã</paragraph>

##### **SAM-SP: Self-Prompting Makes SAM Great Again**
2408.12364v1 by Chunpeng Zhou, Kangjie Ning, Qianqian Shen, Sheng Zhou, Zhi Yu, Haishuai Wang

The recently introduced Segment Anything Model (SAM), a Visual Foundation
Model (VFM), has demonstrated impressive capabilities in zero-shot segmentation
tasks across diverse natural image datasets. Despite its success, SAM
encounters noticeably performance degradation when applied to specific domains,
such as medical images. Current efforts to address this issue have involved
fine-tuning strategies, intended to bolster the generalizability of the vanilla
SAM. However, these approaches still predominantly necessitate the utilization
of domain specific expert-level prompts during the evaluation phase, which
severely constrains the model's practicality.
  To overcome this limitation, we introduce a novel self-prompting based
fine-tuning approach, called SAM-SP, tailored for extending the vanilla SAM
model. Specifically, SAM-SP leverages the output from the previous iteration of
the model itself as prompts to guide subsequent iteration of the model. This
self-prompting module endeavors to learn how to generate useful prompts
autonomously and alleviates the dependence on expert prompts during the
evaluation phase, significantly broadening SAM's applicability. Additionally,
we integrate a self-distillation module to enhance the self-prompting process
further. Extensive experiments across various domain specific datasets validate
the effectiveness of the proposed SAM-SP. Our SAM-SP not only alleviates the
reliance on expert prompts but also exhibits superior segmentation performance
comparing to the state-of-the-art task-specific segmentation approaches, the
vanilla SAM, and SAM-based approaches.

æè¦ï¼æè¿æ¨åºç Segment Anything Model (SAM)ï¼ä¸ç¨®è¦è¦ºåºç¤æ¨¡å (VFM)ï¼å¨åç¨®èªç¶å½±åè³æéçé¶æ¬¡åæ®µä»»åä¸­å±ç¾åºä»¤äººå°è±¡æ·±å»çè½åãåç®¡ SAM æåï¼ä½æç¨æ¼ç¹å®é åï¼ä¾å¦é«å­¸å½±åï¼æï¼æè½æé¡¯ä¸éãç®åè§£æ±ºæ­¤åé¡çæ¹æ³åæ¬å¾®èª¿ç­ç¥ï¼æ¨å¨å å¼·é¦è SAM çæ¦æ¬æ§ãç¶èï¼éäºæ¹æ³å¨è©ä¼°éæ®µä»ç¶ä¸»è¦éè¦ä½¿ç¨ç¹å®é åçå°å®¶ç´æç¤ºï¼éå´ééå¶äºæ¨¡åçå¯¦ç¨æ§ã
çºäºåææ­¤éå¶ï¼æåå¼å¥ä¸ç¨®åºæ¼èªææç¤ºçæ°åå¾®èª¿æ¹æ³ï¼ç¨±çº SAM-SPï¼å°éç¨æ¼æ´åé¦è SAM æ¨¡åãå·é«ä¾èªªï¼SAM-SP å©ç¨æ¨¡åæ¬èº«åååè¦éç®çè¼¸åºä½çºæç¤ºï¼å¼å°æ¨¡åå¾çºåè¦éç®ãæ­¤èªææç¤ºæ¨¡çµåªåå­¸ç¿å¦ä½èªä¸»ç¢çæç¨çæç¤ºï¼ä¸¦æ¸è¼è©ä¼°éæ®µå°å°å®¶æç¤ºçä¾è³´æ§ï¼é¡¯èæ´å± SAM çé©ç¨æ§ãæ­¤å¤ï¼æåæ´åä¸åèªæè¸é¤¾æ¨¡çµï¼é²ä¸æ­¥å¢å¼·èªææç¤ºçéç¨ãå¨åç¨®ç¹å®é åè³æéä¸­çå¤§éå¯¦é©é©è­äºææåºç SAM-SP çæææ§ãæåç SAM-SP ä¸åæ¸è¼äºå°å°å®¶æç¤ºçä¾è³´æ§ï¼èä¸èæåé²çç¹å®ä»»ååå²æ¹æ³ãé¦è SAM ååºæ¼ SAM çæ¹æ³ç¸æ¯ï¼éå±ç¾åºåªç°çåå²æè½ã

##### **Class-balanced Open-set Semi-supervised Object Detection for Medical Images**
2408.12355v1 by Zhanyun Lu, Renshu Gu, Huimin Cheng, Siyu Pang, Mingyu Xu, Peifang Xu, Yaqi Wang, Yuichiro Kinoshita, Juan Ye, Gangyong Jia, Qing Wu

Medical image datasets in the real world are often unlabeled and imbalanced,
and Semi-Supervised Object Detection (SSOD) can utilize unlabeled data to
improve an object detector. However, existing approaches predominantly assumed
that the unlabeled data and test data do not contain out-of-distribution (OOD)
classes. The few open-set semi-supervised object detection methods have two
weaknesses: first, the class imbalance is not considered; second, the OOD
instances are distinguished and simply discarded during pseudo-labeling. In
this paper, we consider the open-set semi-supervised object detection problem
which leverages unlabeled data that contain OOD classes to improve object
detection for medical images. Our study incorporates two key innovations:
Category Control Embed (CCE) and out-of-distribution Detection Fusion
Classifier (OODFC). CCE is designed to tackle dataset imbalance by constructing
a Foreground information Library, while OODFC tackles open-set challenges by
integrating the ``unknown'' information into basic pseudo-labels. Our method
outperforms the state-of-the-art SSOD performance, achieving a 4.25 mAP
improvement on the public Parasite dataset.

æè¦ï¼çå¯¦ä¸ççé«å­¸å½±åè³æééå¸¸æªæ¨ç±¤ä¸ä¸å¹³è¡¡ï¼èåç£ç£ç©ä»¶åµæ¸¬ (SSOD) å¯ä»¥å©ç¨æªæ¨ç±¤è³æä¾æ¹åç©ä»¶åµæ¸¬å¨ãç¶èï¼ç¾ææ¹æ³ä¸»è¦åè¨­æªæ¨ç±¤è³æåæ¸¬è©¦è³æä¸åå«åä½å¤ (OOD) é¡å¥ãå°æ¸éæ¾å¼åç£ç£ç©ä»¶åµæ¸¬æ¹æ³æå©åç¼ºé»ï¼é¦åï¼é¡å¥ä¸å¹³è¡¡æªè¢«èæ®ï¼å¶æ¬¡ï¼OOD å¯¦ä¾å¨å½æ¨ç±¤æéè¢«ååä¸¦ç°¡å®å°æ¨æ£ãå¨æ¬æä¸­ï¼æåèæ®äºéæ¾å¼åç£ç£ç©ä»¶åµæ¸¬åé¡ï¼å®å©ç¨åå« OOD é¡å¥çæªæ¨ç±¤è³æä¾æ¹åé«å­¸å½±åçç©ä»¶åµæ¸¬ãæåçç ç©¶åå«å©é ééµåµæ°ï¼é¡å¥æ§å¶åµå¥ (CCE) ååä½å¤åµæ¸¬èååé¡å¨ (OODFC)ãCCE æ¨å¨ééå»ºæ§åæ¯è³è¨åº«ä¾è§£æ±ºè³æéä¸å¹³è¡¡ï¼è OODFC ééå°ãæªç¥ãè³è¨æ´åå°åºæ¬å½æ¨ç±¤ä¸­ä¾è§£æ±ºéæ¾å¼ææ°ãæåçæ¨¡ååªæ¼æåé²ç SSOD æè½ï¼å¨å¬éå¯çè²è³æéä¸éå° 4.25 mAP çæåã

##### **Large Language Models Are Self-Taught Reasoners: Enhancing LLM Applications via Tailored Problem-Solving Demonstrations**
2408.12315v1 by Kai Tzu-iunn Ong, Taeyoon Kwon, Jinyoung Yeo

Guiding large language models with a selected set of human-authored
demonstrations is a common practice for improving LLM applications. However,
human effort can be costly, especially in specialized domains (e.g., clinical
diagnosis), and does not guarantee optimal performance due to the potential
discrepancy of target skills between selected demonstrations and real test
instances. Motivated by these, this paper explores the automatic creation of
customized demonstrations, whose target skills align with the given target
instance. We present SELF-TAUGHT, a problem-solving framework, which
facilitates demonstrations that are "tailored" to the target problem and
"filtered" for better quality (i.e., correctness) in a zero-shot manner. In 15
tasks of multiple-choice questions of diverse domains and the diagnosis of
Alzheimer's disease (AD) with real-world patients, SELF-TAUGHT achieves
superior performance to strong baselines (e.g., Few-shot CoT, Plan-and-Solve,
Auto-CoT). We conduct comprehensive analyses on SELF-TAUGHT, including its
generalizability to existing prompting methods and different LLMs, the quality
of its intermediate generation, and more.

æè¦ï¼ä½¿ç¨ä¸çµç±äººé¡æ°å¯«çç¤ºç¯ä¾æå°å¤§åèªè¨æ¨¡åæ¯ä¸ç¨®æ¹å LLM æç¨ç¨å¼çå¸¸è¦åæ³ãç¶èï¼äººåææ¬å¯è½å¾é«ï¼ç¹å¥æ¯å¨å°æ¥­é åï¼ä¾å¦è¨åºè¨ºæ·ï¼ä¸­ï¼èä¸ç±æ¼é¸å®çç¤ºç¯èå¯¦éæ¸¬è©¦å¯¦ä¾ä¹éç®æ¨æè½çæ½å¨å·®ç°ï¼ä¸¦ä¸è½ä¿è­æä½³æè½ãåºæ¼éäºåæ©ï¼æ¬ææ¢è¨äºèªåå»ºç«èªè¨ç¤ºç¯ï¼å¶ç®æ¨æè½èçµ¦å®çç®æ¨å¯¦ä¾ä¸è´ãæåæåºäºä¸ååé¡è§£æ±ºæ¶æ§ SELF-TAUGHTï¼å®å¯ä»¥ä¿é²ãéå°ãç®æ¨åé¡ãç¯©é¸ãåºæ´é«åè³ªï¼å³æ­£ç¢ºæ§ï¼çç¤ºç¯ï¼ä¸æ¡ç¨é¶æ¬¡å­¸ç¿çæ¹å¼ãå¨å¤åé åçå¤é¸é¡ä»»ååå°çå¯¦ä¸çæ£èé²è¡é¿è²æµ·é»ç (AD) è¨ºæ·ç 15 é ä»»åä¸­ï¼SELF-TAUGHT éå°äºåªæ¼å¼·å¤§åºæºï¼ä¾å¦ Few-shot CoTãPlan-and-SolveãAuto-CoTï¼çæè½ãæåå° SELF-TAUGHT é²è¡äºå¨é¢çåæï¼åæ¬å¶å°ç¾ææç¤ºæ¹æ³åä¸å LLM çæ¦æ¬æ§ãå¶ä¸­éç¢ççåè³ªç­ç­ã

##### **Tipta uzmanlik sinavinda (tus) buyuk dil modelleri insanlardan daha mi basarili?**
2408.12305v2 by Yesim Aygul, Muge Olucoglu, Adil Alpkocak

The potential of artificial intelligence in medical education and assessment
has been made evident by recent developments in natural language processing and
artificial intelligence. Medical questions can now be successfully answered by
artificial intelligence algorithms. It can help medical practitioners. This
study evaluates the performance of three different artificial intelligence
models in answering Turkish medical questions in the 2021 1st Term Medical
Specialization Examination (MSE). MSE consists of a total of 240 questions
across clinical (CMST) and basic (BMST) medical sciences. According to the
results in CMST, it was concluded that Gemini correctly answered 82 questions,
ChatGPT-4 answered 105 questions and ChatGPT-4o answered 117 questions. In
BMST, Gemini and ChatGPT-4 answered 93 questions and ChatGPT-4o answered 107
questions correctly according to the answer key. ChatGPT-4o outperformed the
candidate with the highest scores of 113 and 106 according to CMST and BMST
respectively. This study highlights the importance of the potential of
artificial intelligence in medical education and assessment. It demonstrates
that advanced models can achieve high accuracy and contextual understanding,
demonstrating their potential role in medical education and evaluation.

æè¦ï¼äººå·¥æºè½å¨å»å­¦æè²åè©ä¼°ä¸­çæ½å
æè¿èªç¶èªè¨èçåäººå·¥æºè½çç¼å±ï¼è­æäºäººå·¥æºè½å¨é«å­¸æè²åè©ä¼°ä¸­çæ½åãäººå·¥æºè½æ¼ç®æ³ç¾å¨å¯ä»¥æååç­é«å­¸åé¡ãå®å¯ä»¥å¹«å©é«çå¾æ¥­äººå¡ãéé ç ç©¶è©ä¼°äºä¸ç¨®ä¸åçäººå·¥æºè½æ¨¡åå¨åç­ 2021 å¹´ç¬¬ 1 å­¸æé«å­¸å°ç§èè©¦ (MSE) ä¸­çåè³å¶é«å­¸åé¡æçè¡¨ç¾ãMSE ç¸½å±åå« 240 é¡ï¼æ¶µèè¨åº (CMST) ååºç¤ (BMST) é«å­¸ç§å­¸ãæ ¹æ CMST ççµæï¼çµè«æ¯ Gemini æ­£ç¢ºåç­äº 82 é¡ï¼ChatGPT-4 åç­äº 105 é¡ï¼ChatGPT-4o åç­äº 117 é¡ãå¨ BMST ä¸­ï¼æ ¹æç­æ¡ééµï¼Gemini å ChatGPT-4 åç­äº 93 é¡ï¼ChatGPT-4o åç­äº 107 é¡ãæ ¹æ CMST å BMSTï¼ChatGPT-4o çè¡¨ç¾åªæ¼åå¥ç²å¾ 113 åå 106 åçæé«åèçãéé ç ç©¶çªé¡¯äºäººå·¥æºè½å¨é«å­¸æè²åè©ä¼°ä¸­çæ½åãå®è­æäºé²éæ¨¡åå¯ä»¥éå°é«æºç¢ºåº¦åæå¢çè§£ï¼å±ç¾äºå®åå¨é«å­¸æè²åè©ä¼°ä¸­çæ½å¨ä½ç¨ã

##### **Developing vocal system impaired patient-aimed voice quality assessment approach using ASR representation-included multiple features**
2408.12279v1 by Shaoxiang Dang, Tetsuya Matsumoto, Yoshinori Takeuchi, Takashi Tsuboi, Yasuhiro Tanaka, Daisuke Nakatsubo, Satoshi Maesawa, Ryuta Saito, Masahisa Katsuno, Hiroaki Kudo

The potential of deep learning in clinical speech processing is immense, yet
the hurdles of limited and imbalanced clinical data samples loom large. This
article addresses these challenges by showcasing the utilization of automatic
speech recognition and self-supervised learning representations, pre-trained on
extensive datasets of normal speech. This innovative approach aims to estimate
voice quality of patients with impaired vocal systems. Experiments involve
checks on PVQD dataset, covering various causes of vocal system damage in
English, and a Japanese dataset focusing on patients with Parkinson's disease
before and after undergoing subthalamic nucleus deep brain stimulation
(STN-DBS) surgery. The results on PVQD reveal a notable correlation (>0.8 on
PCC) and an extraordinary accuracy (<0.5 on MSE) in predicting Grade, Breathy,
and Asthenic indicators. Meanwhile, progress has been achieved in predicting
the voice quality of patients in the context of STN-DBS.

æè¦ï¼æ·±åº¦å­¸ç¿å¨è¨åºèªé³èççæ½åå·¨å¤§ï¼ä½
åéä¸ä¸å¹³è¡¡çè¨åºè³ææ¨£æ¬çéç¤å»å¾å¤§ãé
ç¯æç« ééå±ç¤ºèªåèªé³è¾¨è­åèªæç£ç£å­¸ç¿è¡¨å¾µçæç¨ï¼ä¾è§£æ±ºéäºææ°ï¼éäºè¡¨å¾µæ¯é åå¨æ­£å¸¸çèªé³çå»£æ³è³æéä¸è¨ç·´éçãéç¨®åµæ°çæ¹æ³æ¨å¨è©ä¼°ç¼è²ç³»çµ±åæççæ£çè²é³åè³ªãå¯¦é©åå«å¨ PVQD è³æéä¸çæª¢æ¥ï¼æ¶µèè±æä¸­åç¨®é æç¼è²ç³»çµ±æå·çåå ï¼ä»¥åä¸åæ¥æè³æéï¼å°æ³¨æ¼å¨æ¥åä¸è¦ä¸æ ¸æ·±é¨è¦é¨åºæ¿ (STN-DBS) æè¡åå¾çå¸éæ£®æ°ççæ£ãPVQD ççµæé¡¯ç¤ºåºé¡¯èç¸éæ§ï¼PCC ä¸ >0.8ï¼ï¼ä»¥åé æ¸¬ç­ç´ãæ°£é³åç¡åææ¨çéå¡æºç¢ºæ§ï¼MSE ä¸ <0.5ï¼ãåæï¼å¨é æ¸¬ STN-DBS èæ¯ä¸çæ£çè²é³åè³ªæ¹é¢ä¹åå¾äºé²å±ã

##### **LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction**
2408.12249v1 by Aishik Nagar, Viktor Schlegel, Thanh-Tung Nguyen, Hao Li, Yuping Wu, Kuluhan Binici, Stefan Winkler

Large Language Models (LLMs) are increasingly adopted for applications in
healthcare, reaching the performance of domain experts on tasks such as
question answering and document summarisation. Despite their success on these
tasks, it is unclear how well LLMs perform on tasks that are traditionally
pursued in the biomedical domain, such as structured information extration. To
breach this gap, in this paper, we systematically benchmark LLM performance in
Medical Classification and Named Entity Recognition (NER) tasks. We aim to
disentangle the contribution of different factors to the performance,
particularly the impact of LLMs' task knowledge and reasoning capabilities,
their (parametric) domain knowledge, and addition of external knowledge. To
this end we evaluate various open LLMs -- including BioMistral and Llama-2
models -- on a diverse set of biomedical datasets, using standard prompting,
Chain-of-Thought (CoT) and Self-Consistency based reasoning as well as
Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.
Counter-intuitively, our results reveal that standard prompting consistently
outperforms more complex techniques across both tasks, laying bare the
limitations in the current application of CoT, self-consistency and RAG in the
biomedical domain. Our findings suggest that advanced prompting methods
developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are
not easily portable to biomedical tasks where precise structured outputs are
required. This highlights the need for more effective integration of external
knowledge and reasoning mechanisms in LLMs to enhance their performance in
real-world biomedical applications.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) æä¾æå¤ç¨æ¼é«çä¿å¥æç¨ï¼å¨åç­åé¡åæä»¶æè¦ç­ä»»åä¸éå°é åå°å®¶çè¡¨ç¾ãåç®¡éäºä»»åç²å¾æåï¼ä½å°ä¸æ¸æ¥ LLM å¨çç©é«å­¸é åå³çµ±ä¸å·è¡çä»»åï¼ä¾å¦çµæ§åè³è¨èåï¼è¡¨ç¾å¦ä½ãçºäºå½è£éåå·®è·ï¼æåå¨éç¯è«æä¸­ç³»çµ±æ§å°è©é LLM å¨é«å­¸åé¡åå½åå¯¦é«è¾¨è­ (NER) ä»»åä¸­çè¡¨ç¾ãæåçç®æ¨æ¯éæ¸ä¸åå ç´ å°è¡¨ç¾çè²¢ç»ï¼ç¹å¥æ¯ LLM çä»»åç¥è­åæ¨çè½åãå®åçï¼åæ¸ï¼é åç¥è­ï¼ä»¥åå¤é¨ç¥è­çå å¥ãçºæ­¤ï¼æåè©ä¼°åç¨®éæ¾ç LLMï¼åæ¬ BioMistral å Llama-2 æ¨¡åï¼ï¼ä½¿ç¨æ¨æºæç¤ºãåºæ¼æèé (CoT) åèªæ´½æ§çæ¨çä»¥åä½¿ç¨ PubMed åç¶­åºç¾ç§èªæåº«çæª¢ç´¢å¢å¼·çæ (RAG) å¨å¤æ¨£åççç©é«å­¸è³æéä¸ãèç´è¦ºç¸åï¼æåççµæé¡¯ç¤ºæ¨æºæç¤ºå¨å©é ä»»åä¸­å§çµåªæ¼æ´è¤éçæè¡ï¼æ­é²äºå¨çç©é«å­¸é åä¸­ CoTãèªæ´½æ§å RAG çç¶åæç¨ä¸­çéå¶ãæåçç¼ç¾è¡¨æï¼çºç¥è­ææ¨çå¯éåä»»åï¼ä¾å¦ CoT æ RAGï¼éç¼çé«éæç¤ºæ¹æ³ä¸å®¹æç§»æ¤å°éè¦ç²¾ç¢ºçµæ§åè¼¸åºççç©é«å­¸ä»»åãéçªé¡¯åºéè¦æ´ææå°æ´åå¤é¨ç¥è­åæ¨çæ©å¶å° LLM ä¸­ï¼ä»¥å¢å¼·å®åå¨å¯¦éçç©é«å­¸æç¨ä¸­çè¡¨ç¾ã

##### **MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**
2408.12236v1 by Yanzeng Li, Cheng Zeng, Jinchao Zhang, Jie Zhou, Lei Zou

Medical education relies heavily on Simulated Patients (SPs) to provide a
safe environment for students to practice clinical skills, including medical
image analysis. However, the high cost of recruiting qualified SPs and the lack
of diverse medical imaging datasets have presented significant challenges. To
address these issues, this paper introduces MedDiT, a novel
knowledge-controlled conversational framework that can dynamically generate
plausible medical images aligned with simulated patient symptoms, enabling
diverse diagnostic skill training. Specifically, MedDiT integrates various
patient Knowledge Graphs (KGs), which describe the attributes and symptoms of
patients, to dynamically prompt Large Language Models' (LLMs) behavior and
control the patient characteristics, mitigating hallucination during medical
conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is
incorporated to generate medical images according to the specified patient
attributes in the KG. In this paper, we present the capabilities of MedDiT
through a practical demonstration, showcasing its ability to act in diverse
simulated patient cases and generate the corresponding medical images. This can
provide an abundant and interactive learning experience for students, advancing
medical education by offering an immersive simulation platform for future
healthcare professionals. The work sheds light on the feasibility of
incorporating advanced technologies like LLM, KG, and DiT in education
applications, highlighting their potential to address the challenges faced in
simulated patient-based medical education.

æè¦ï¼é«å­¸æè²é«åº¦ä¾è³´æ¨¡æ¬çäºº (SP) æä¾ä¸åå®å¨çç°å¢ï¼è®å­¸çç·´ç¿è¨åºæè½ï¼åæ¬é«å­¸å½±ååæãç¶èï¼æååæ ¼ SP çé«ææ¬åç¼ºä¹å¤æ¨£çé«å­¸å½±åè³æéå·²é æé¡¯èçææ°ãçºäºè§£æ±ºéäºåé¡ï¼æ¬æä»ç´¹ MedDiTï¼ä¸åæ°ç©çç¥è­æ§å¶å°è©±æ¶æ§ï¼å®å¯ä»¥åæç¢çç¬¦åæ¨¡æ¬çäººçççåçé«å­¸å½±åï¼å¯¦ç¾å¤æ¨£çè¨ºæ·æè½è¨ç·´ãå·é«ä¾èªªï¼MedDiT æ´åäºåç¨®çäººç¥è­åè­ (KG)ï¼æè¿°çäººçå±¬æ§åççï¼ä»¥åææç¤ºå¤§åèªè¨æ¨¡å (LLM) çè¡çºï¼ä¸¦æ§å¶çäººç¹å¾µï¼æ¸è¼é«å­¸å°è©±ä¸­çå¹»è¦ºãæ­¤å¤ï¼éç´å¥ä¸åç¶éå¾®èª¿çæ´æ£Transformer (DiT) æ¨¡åï¼æ ¹æ KG ä¸­æå®ççäººå±¬æ§ç¢çé«å­¸å½±åãå¨æ¬æä¸­ï¼æåééå¯¦éç¤ºç¯å±ç¤º MedDiT çåè½ï¼å±ç¤ºå®å¨ä¸åæ¨¡æ¬çäººæ¡ä¾ä¸­ä½ç¨ä¸¦ç¢çç¸æé«å­¸å½±åçè½åãéå¯ä»¥çºå­¸çæä¾è±å¯ä¸äºåçå­¸ç¿é«é©ï¼ééæä¾èº«æ­·å¶å¢çæ¨¡æ¬å¹³å°ï¼æåé«å­¸æè²ï¼é ç¦æªä¾çé«çä¿å¥å°æ¥­äººå¡ãéé å·¥ä½é¡æäºå¨æè²æç¨ä¸­æ´å LLMãKG å DiT ç­åé²æè¡çå¯è¡æ§ï¼çªé¡¯å®åå¨è§£æ±ºæ¨¡æ¬çäººçºåºç¤çé«å­¸æè²æé¢è¨ææ°çæ½åã

##### **MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents**
2408.12142v1 by Congchi Yin, Feng Li, Shu Zhang, Zike Wang, Jun Shao, Piji Li, Jianhua Chen, Xun Jiang

The clinical diagnosis of most mental disorders primarily relies on the
conversations between psychiatrist and patient. The creation of such diagnostic
conversation datasets is promising to boost the AI mental healthcare community.
However, directly collecting the conversations in real diagnosis scenarios is
near impossible due to stringent privacy and ethical considerations. To address
this issue, we seek to synthesize diagnostic conversation by exploiting
anonymous patient cases that are easier to access. Specifically, we design a
neuro-symbolic multi-agent framework for synthesizing the diagnostic
conversation of mental disorders with large language models. It takes patient
case as input and is capable of generating multiple diverse conversations with
one single patient case. The framework basically involves the interaction
between a doctor agent and a patient agent, and achieves text generation under
symbolic control via a dynamic diagnosis tree from a tool agent. By applying
the proposed framework, we develop the largest Chinese mental disorders
diagnosis dataset MDD-5k, which is built upon 1000 cleaned real patient cases
by cooperating with a pioneering psychiatric hospital, and contains 5000
high-quality long conversations with diagnosis results as labels. To the best
of our knowledge, it's also the first labelled Chinese mental disorders
diagnosis dataset. Human evaluation demonstrates the proposed MDD-5k dataset
successfully simulates human-like diagnostic process of mental disorders. The
dataset and code will become publicly accessible in
https://github.com/lemonsis/MDD-5k.

æè¦ï¼å¤§å¤æ¸ç²¾ç¥ç¾ççè¨åºè¨ºæ·ä¸»è¦ä¾è³´æ¼ç²¾ç¥ç§é«å¸«èçæ£ä¹éçå°è©±ãå»ºç«æ­¤é¡è¨ºæ·å°è©±è³æéæææå AI å¿çä¿å¥ç¤¾ç¾¤ãç¶èï¼ç±æ¼å´æ ¼çé±ç§åå«çèéï¼ç´æ¥å¨çå¯¦è¨ºæ·æå¢ä¸­æ¶éå°è©±å¹¾ä¹æ¯ä¸å¯è½çãçºäºè§£æ±ºéååé¡ï¼æåè©¦åå©ç¨è¼å®¹æåå¾çå¿åçæ£æ¡ä¾ä¾åæè¨ºæ·å°è©±ãå·é«ä¾èªªï¼æåè¨­è¨äºä¸åç¥ç¶ç¬¦èå¤ä¸»é«æ¶æ§ï¼ç¨æ¼åæå·æå¤§åèªè¨æ¨¡åçç²¾ç¥ç¾çè¨ºæ·å°è©±ãå®ä»¥çæ£æ¡ä¾ä½çºè¼¸å¥ï¼ä¸¦ä¸è½å¤ éå°å®ä¸çæ£æ¡ä¾ç¢çå¤åä¸åçå°è©±ãæ­¤æ¶æ§åºæ¬ä¸æ¶åé«å¸«ä¸»é«åçæ£ä¸»é«ä¹éçäºåï¼ä¸¦ééä¾èªå·¥å·ä¸»é«çåæè¨ºæ·æ¨¹ä¾å¯¦ç¾ç¬¦èæ§å¶ä¸çæå­ç¢çãééæç¨å»ºè­°çæ¶æ§ï¼æåéç¼äºæå¤§çä¸­æç²¾ç¥ç¾çè¨ºæ·è³æé MDD-5kï¼å®æ¯å»ºç«å¨ 1000 åééèä¸å®¶åé©ç²¾ç¥çé¢åä½èæ´çéççå¯¦çæ£æ¡ä¾ä¸ï¼ä¸¦åå« 5000 åå¸¶æè¨ºæ·çµææ¨ç±¤çé«åè³ªé·å°è©±ãææåæç¥ï¼å®ä¹æ¯ç¬¬ä¸åæ¨è¨çä¸­æç²¾ç¥ç¾çè¨ºæ·è³æéãäººé¡è©ä¼°è­æï¼å»ºè­°ç MDD-5k è³æéæåæ¨¡æ¬äºé¡äººçç²¾ç¥ç¾çè¨ºæ·æµç¨ãè³æéåç¨å¼ç¢¼å°å¨ https://github.com/lemonsis/MDD-5k å¬éã

##### **DRExplainer: Quantifiable Interpretability in Drug Response Prediction with Directed Graph Convolutional Network**
2408.12139v1 by Haoyuan Shi, Tao Xu, Xiaodi Li, Qian Gao, Junfeng Xia, Zhenyu Yue

Predicting the response of a cancer cell line to a therapeutic drug is
pivotal for personalized medicine. Despite numerous deep learning methods that
have been developed for drug response prediction, integrating diverse
information about biological entities and predicting the directional response
remain major challenges. Here, we propose a novel interpretable predictive
model, DRExplainer, which leverages a directed graph convolutional network to
enhance the prediction in a directed bipartite network framework. DRExplainer
constructs a directed bipartite network integrating multi-omics profiles of
cell lines, the chemical structure of drugs and known drug response to achieve
directed prediction. Then, DRExplainer identifies the most relevant subgraph to
each prediction in this directed bipartite network by learning a mask,
facilitating critical medical decision-making. Additionally, we introduce a
quantifiable method for model interpretability that leverages a ground truth
benchmark dataset curated from biological features. In computational
experiments, DRExplainer outperforms state-of-the-art predictive methods and
another graph-based explanation method under the same experimental setting.
Finally, the case studies further validate the interpretability and the
effectiveness of DRExplainer in predictive novel drug response. Our code is
available at: https://github.com/vshy-dream/DRExplainer.

æè¦ï¼é æ¸¬ççç´°èæ ªå°æ²»çè¥ç©çåæå°æ¼åäººåé«çè³ééè¦ãåç®¡å·²ç¶éç¼åºè¨±å¤ç¨æ¼è¥ç©åæé æ¸¬çæ·±åº¦å­¸ç¿æ¹æ³ï¼ä½æ´åçç©å¯¦é«çå¤æ¨£ä¿¡æ¯åé æ¸¬æ¹ååæä»ç¶æ¯ä¸»è¦çææ°ãå¨éè£¡ï¼æåæåºäºä¸åæ°ç©çå¯è§£éé æ¸¬æ¨¡å DRExplainerï¼å®å©ç¨æååå·ç©ç¶²è·¯å¨æåäºé¨ç¶²è·¯æ¡æ¶ä¸­å¢å¼·é æ¸¬ãDRExplainer æ§å»ºäºä¸åæåäºé¨ç¶²è·¯ï¼æ´åäºç´°èç³»ççµå­¸æ¦æ³ãè¥ç©çåå­¸çµæ§åå·²ç¥çè¥ç©åæï¼ä»¥å¯¦ç¾æåé æ¸¬ãç¶å¾ï¼DRExplainer ééå­¸ç¿é®ç½©è­å¥æ­¤æåäºé¨ç¶²è·¯ä¸­èæ¯åé æ¸¬æç¸éçå­åï¼ä¿é²ééµçé«çæ±ºç­å¶å®ãæ­¤å¤ï¼æåå¼å¥äºä¸ç¨®éåæ¨¡åå¯è§£éæ§çæ¹æ³ï¼å©ç¨å¾çç©ç¹å¾µä¸­ç­åçå°é¢å¯¦æ³åºæºè³æéãå¨è¨ç®å¯¦é©ä¸­ï¼DRExplainer å¨ç¸åçå¯¦é©è¨­ç½®ä¸åªæ¼æåé²çé æ¸¬æ¹æ³åå¦ä¸ç¨®åºæ¼åè¡¨çè§£éæ¹æ³ãæå¾ï¼æ¡ä¾ç ç©¶é²ä¸æ­¥é©è­äº DRExplainer å¨é æ¸¬æ°è¥ç©åæä¸­çå¯è§£éæ§åæææ§ãæåçç¨å¼ç¢¼å¯å¨ https://github.com/vshy-dream/DRExplainer åå¾ã

##### **Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards**
2408.12112v1 by Shresth Verma, Niclas Boehmer, Lingkai Kong, Milind Tambe

LLMs are increasingly used to design reward functions based on human
preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards
for Restless Multi-Armed Bandits, a framework for allocating limited resources
among agents. In applications such as public health, this approach empowers
grassroots health workers to tailor automated allocation decisions to community
needs. In the presence of multiple agents, altering the reward function based
on human preferences can impact subpopulations very differently, leading to
complex tradeoffs and a multi-objective resource allocation problem. We are the
first to present a principled method termed Social Choice Language Model for
dealing with these tradeoffs for LLM-designed rewards for multiagent planners
in general and restless bandits in particular. The novel part of our model is a
transparent and configurable selection component, called an adjudicator,
external to the LLM that controls complex tradeoffs via a user-selected social
welfare function. Our experiments demonstrate that our model reliably selects
more effective, aligned, and balanced reward functions compared to purely
LLM-based approaches.

æè¦ï¼LLM  zunehmend verwendet werden, um Belohnungsfunktionen basierend auf menschlichen PrÃ¤ferenzen in Reinforcement Learning (RL) zu entwerfen. Wir konzentrieren uns auf LLM-entworfene Belohnungen fÃ¼r Restless Multi-Armed Bandits, ein Framework zur Zuweisung begrenzter Ressourcen unter Agenten. In Anwendungen wie dem Ã¶ffentlichen Gesundheitswesen ermÃ¶glicht dieser Ansatz BasisgesundheitsfachkrÃ¤ften, automatisierte Zuweisungsentscheidungen auf die BedÃ¼rfnisse der Gemeinschaft zuzuschneiden. In Gegenwart mehrerer Agenten kann die Ãnderung der Belohnungsfunktion basierend auf menschlichen PrÃ¤ferenzen Untergruppen sehr unterschiedlich beeinflussen, was zu komplexen Kompromissen und einem mehrzielgerichteten Ressourcenzuweisungsproblem fÃ¼hrt. Wir sind die Ersten, die eine prinzipielle Methode namens Social Choice Language Model fÃ¼r den Umgang mit diesen Kompromissen fÃ¼r LLM-entworfene Belohnungen fÃ¼r Multiagentenplaner im Allgemeinen und unruhige Banditen im Besonderen vorstellen. Der neuartige Teil unseres Modells ist eine transparente und konfigurierbare Auswahlkomponente, die als Schiedsrichter bezeichnet wird und sich auÃerhalb des LLM befindet und komplexe Kompromisse Ã¼ber eine vom Benutzer ausgewÃ¤hlte soziale Wohlfahrtsfunktion steuert. Unsere Experimente zeigen, dass unser Modell im Vergleich zu rein LLM-basierten AnsÃ¤tzen zuverlÃ¤ssig effektivere, ausgerichtete und ausgewogene Belohnungsfunktionen auswÃ¤hlt.

##### **uMedSum: A Unified Framework for Advancing Medical Abstractive Summarization**
2408.12095v2 by Aishik Nagar, Yutong Liu, Andy T. Liu, Viktor Schlegel, Vijay Prakash Dwivedi, Arun-Kumar Kaliya-Perumal, Guna Pratheep Kalanchiam, Yili Tang, Robby T. Tan

Medical abstractive summarization faces the challenge of balancing
faithfulness and informativeness. Current methods often sacrifice key
information for faithfulness or introduce confabulations when prioritizing
informativeness. While recent advancements in techniques like in-context
learning (ICL) and fine-tuning have improved medical summarization, they often
overlook crucial aspects such as faithfulness and informativeness without
considering advanced methods like model reasoning and self-improvement.
Moreover, the field lacks a unified benchmark, hindering systematic evaluation
due to varied metrics and datasets. This paper addresses these gaps by
presenting a comprehensive benchmark of six advanced abstractive summarization
methods across three diverse datasets using five standardized metrics. Building
on these findings, we propose uMedSum, a modular hybrid summarization framework
that introduces novel approaches for sequential confabulation removal followed
by key missing information addition, ensuring both faithfulness and
informativeness. Our work improves upon previous GPT-4-based state-of-the-art
(SOTA) medical summarization methods, significantly outperforming them in both
quantitative metrics and qualitative domain expert evaluations. Notably, we
achieve an average relative performance improvement of 11.8% in reference-free
metrics over the previous SOTA. Doctors prefer uMedSum's summaries 6 times more
than previous SOTA in difficult cases where there are chances of confabulations
or missing information. These results highlight uMedSum's effectiveness and
generalizability across various datasets and metrics, marking a significant
advancement in medical summarization.

æè¦ï¼<paragraph>é«å­¸æè¦æè¦é¢è¨å¹³è¡¡å¿ å¯¦åº¦åè³è¨æ§çææ°ãç®åçè¨±å¤æ¹æ³ç¶å¸¸ç§ç²ééµè³è¨ä»¥æåå¿ å¯¦åº¦ï¼æå¨åªåèéè³è¨æ§æå¼å¥èæ§ãéç¶æå¢å§å­¸ç¿ (ICL) åå¾®èª¿ç­æè¡çææ°é²å±å·²æ¹åäºé«å­¸æè¦ï¼ä½å®åç¶å¸¸å¿½ç¥ééµå±¤é¢ï¼ä¾å¦å¿ å¯¦åº¦åè³è¨æ§ï¼èæªèæ®æ¨¡åæ¨çåèªææ¹åç­é²éæ¹æ³ãæ­¤å¤ï¼è©²é åç¼ºä¹çµ±ä¸çåºæºï¼ç±æ¼ä¸åçææ¨åè³æéèé»ç¤äºç³»çµ±æ§è©ä¼°ãæ¬æééæåºå­ç¨®é²éæ½è±¡æè¦æ¹æ³çç¶ååºæºï¼ä½¿ç¨äºç¨®æ¨æºåææ¨è·¨è¶ä¸åä¸åçè³æéï¼ä¾è§£æ±ºéäºå·®è·ãæ ¹æéäºç¼ç¾ï¼æåæåºäº uMedSumï¼éæ¯ä¸åæ¨¡çµåçæ··åæè¦æ¶æ§ï¼å®å¼å¥äºç¨æ¼é£çºèæ§ç§»é¤çåµæ°æ¹æ³ï¼æ¥èæ¯ééµéºæ¼è³è¨çå å¥ï¼ç¢ºä¿å¿ å¯¦åº¦åè³è¨æ§ãæåçç ç©¶æ¹é²äºååçåºæ¼ GPT-4 çæåé² (SOTA) é«å­¸æè¦æ¹æ³ï¼å¨éåææ¨åå®æ§é åå°å®¶è©ä¼°ä¸­é½é¡¯èåªæ¼å®åãå¼å¾æ³¨æçæ¯ï¼æåå¨ç¡åèææ¨ä¸­å¯¦ç¾äºç¸å°æ¼åå SOTA 11.8% çå¹³åç¸å°æè½æåãå¨å¯è½åºç¾èæ§æéºæ¼è³è¨çå°é£æ¡ä¾ä¸­ï¼é«çæ¯ååç SOTA æ´åæ­¡ uMedSum çæè¦ 6 åãéäºçµæçªé¡¯äº uMedSum å¨åç¨®è³æéåææ¨ä¸­çæææ§åæ¦æ¬æ§ï¼æ¨èªèé«å­¸æè¦çéå¤§é²å±ã</paragraph>

##### **Federated Diabetes Prediction in Canadian Adults Using Real-world Cross-Province Primary Care Data**
2408.12029v1 by Guojun Tang, Jason E. Black, Tyler S. Williamson, Steve H. Drew

Integrating Electronic Health Records (EHR) and the application of machine
learning present opportunities for enhancing the accuracy and accessibility of
data-driven diabetes prediction. In particular, developing data-driven machine
learning models can provide early identification of patients with high risk for
diabetes, potentially leading to more effective therapeutic strategies and
reduced healthcare costs. However, regulation restrictions create barriers to
developing centralized predictive models. This paper addresses the challenges
by introducing a federated learning approach, which amalgamates predictive
models without centralized data storage and processing, thus avoiding privacy
issues. This marks the first application of federated learning to predict
diabetes using real clinical datasets in Canada extracted from the Canadian
Primary Care Sentinel Surveillance Network (CPCSSN) without crossprovince
patient data sharing. We address class-imbalance issues through downsampling
techniques and compare federated learning performance against province-based
and centralized models. Experimental results show that the federated MLP model
presents a similar or higher performance compared to the model trained with the
centralized approach. However, the federated logistic regression model showed
inferior performance compared to its centralized peer.

æè¦ï¼æ´åé»å­å¥åº·è¨é (EHR) åæ©å¨å­¸ç¿çæç¨çºå¢å¼·è³æé©åç³å°¿çé æ¸¬çæºç¢ºæ§åå¯åæ§æä¾äºæ©æãç¹å¥æ¯ï¼éç¼è³æé©åçæ©å¨å­¸ç¿æ¨¡åå¯ä»¥åæ©æ¾åºç³å°¿çé«é¢¨éªæ£èï¼é²èå¯è½å°è´æ´ææçæ²»çç­ç¥åéä½é«çä¿å¥ææ¬ãç¶èï¼æ³è¦éå¶æçºéç¼éä¸­å¼é æ¸¬æ¨¡åè£½é éç¤ãæ¬æééä»ç´¹è¯é¦å­¸ç¿æ¹æ³ä¾è§£æ±ºææ°ï¼éç¨®æ¹æ³çµåé æ¸¬æ¨¡åï¼èç¡ééä¸­å¼è³æå²å­åèçï¼å¾èé¿åé±ç§åé¡ãéæ¨èªèé¦æ¬¡æç¨è¯é¦å­¸ç¿ä¾é æ¸¬ç³å°¿çï¼æ¹æ³æ¯ä½¿ç¨å¾å æ¿å¤§åç´ç§è­·å¨åµç£æ§ç¶²çµ¡ (CPCSSN) èåçå æ¿å¤§çå¯¦è¨åºè³æéï¼èç¡éè·¨çä»½åäº«æ£èè³æãæåéééæ¡æ¨£æè¡ä¾è§£æ±ºé¡å¥ä¸å¹³è¡¡åé¡ï¼ä¸¦æ¯è¼è¯é¦å­¸ç¿æè½èåºæ¼çä»½åéä¸­å¼çæ¨¡åãå¯¦é©çµæé¡¯ç¤ºï¼èä½¿ç¨éä¸­å¼æ¹æ³è¨ç·´çæ¨¡åç¸æ¯ï¼è¯é¦ MLP æ¨¡åè¡¨ç¾åºç¸ä¼¼ææ´é«çæè½ãç¶èï¼èå¶éä¸­å¼çååç¸æ¯ï¼è¯é¦éè¼¯è¿´æ­¸æ¨¡åè¡¨ç¾åºè¼å·®çæè½ã

##### **Exploring Large Language Models for Feature Selection: A Data-centric Perspective**
2408.12025v1 by Dawei Li, Zhen Tan, Huan Liu

The rapid advancement of Large Language Models (LLMs) has significantly
influenced various domains, leveraging their exceptional few-shot and zero-shot
learning capabilities. In this work, we aim to explore and understand the
LLMs-based feature selection methods from a data-centric perspective. We begin
by categorizing existing feature selection methods with LLMs into two groups:
data-driven feature selection which requires samples values to do statistical
inference and text-based feature selection which utilizes prior knowledge of
LLMs to do semantical associations using descriptive context. We conduct
extensive experiments in both classification and regression tasks with LLMs in
various sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the
effectiveness and robustness of text-based feature selection methods and
showcase their potentials using a real-world medical application. We also
discuss the challenges and future opportunities in employing LLMs for feature
selection, offering insights for further research and development in this
emerging field.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çå¿«éé²æ­¥é¡¯èå°å½±é¿äºåç¨®é åï¼å©ç¨å®ååè¶çå°æ¨£æ¬åé¶æ¨£æ¬å­¸ç¿è½åãå¨éé å·¥ä½ä¸­ï¼æåæ¨å¨å¾ä»¥æ¸æçºä¸­å¿çè§é»æ¢ç´¢åçè§£åºæ¼ LLM çç¹å¾µé¸ææ¹æ³ãæåé¦åå°ç¾æç LLM ç¹å¾µé¸ææ¹æ³åé¡çºå©çµï¼éè¦æ¨£æ¬å¼ä¾é²è¡çµ±è¨æ¨è«çæ¸æé©åç¹å¾µé¸æï¼ä»¥åå©ç¨ LLM çåé©ç¥è­ä½¿ç¨æè¿°æ§ä¸ä¸æé²è¡èªç¾©éè¯çåºæ¼ææ¬çç¹å¾µé¸æãæåå¨åç¨®è¦æ¨¡ç LLMï¼ä¾å¦ GPT-4ãChatGPT å LLaMA-2ï¼ä¸­å°åé¡åè¿´æ­¸ä»»åé²è¡äºå»£æ³çå¯¦é©ãæåçç ç©¶çµæå¼·èª¿äºåºæ¼ææ¬çç¹å¾µé¸ææ¹æ³çæææ§åç©©å¥æ§ï¼ä¸¦å±ç¤ºäºå®åå¨ç¾å¯¦ä¸çé«çæç¨ä¸­çæ½åãæåéè¨è«äºå¨ç¹å¾µé¸æä¸­æ¡ç¨ LLM çææ°åæªä¾æ©æï¼çºéåæ°èé åçé²ä¸æ­¥ç ç©¶åéç¼æä¾äºè¦è§£ã

##### **Clinical Insights: A Comprehensive Review of Language Models in Medicine**
2408.11735v2 by Nikita Neveditsin, Pawan Lingras, Vijay Mago

This paper provides a detailed examination of the advancements and
applications of large language models in the healthcare sector, with a
particular emphasis on clinical applications. The study traces the evolution of
LLMs from their foundational technologies to the latest developments in
domain-specific models and multimodal integration. It explores the technical
progression from encoder-based models requiring fine-tuning to sophisticated
approaches that integrate textual, visual, and auditory data, thereby
facilitating comprehensive AI solutions in healthcare. The paper discusses both
the opportunities these technologies present for enhancing clinical efficiency
and the challenges they pose in terms of ethics, data privacy, and
implementation. Additionally, it critically evaluates the deployment strategies
of LLMs, emphasizing the necessity of open-source models to ensure data privacy
and adaptability within healthcare environments. Future research directions are
proposed, focusing on empirical studies to evaluate the real-world efficacy of
LLMs in healthcare and the development of open datasets for further research.
This review aims to provide a comprehensive resource for both newcomers and
multidisciplinary researchers interested in the intersection of AI and
healthcare.

æè¦ï¼æ¬æè©³ç´°æ¢è¨äºå¤§åèªè¨æ¨¡åå¨é«çä¿å¥é åçé²å±èæç¨ï¼ç¹å¥å¼·èª¿è¨åºæç¨ãéé ç ç©¶è¿½æº¯äºå¤§åèªè¨æ¨¡åå¾åºç¤æè¡æ¼è®å°ç¹å®é åæ¨¡ååå¤æ¨¡ææ´åçææ°ç¼å±ãå®æ¢è¨äºæè¡é²å±ï¼å¾éè¦å¾®èª¿çç·¨ç¢¼å¨æ¨¡åå°æ´åææ¬ãè¦è¦ºåè½è¦ºè³æçè¤éæ¹æ³ï¼å¾èä¿é²é«çä¿å¥ä¸­çå¨é¢äººå·¥æºæ§è§£æ±ºæ¹æ¡ãæ¬æè¨è«äºéäºæè¡å¨æåè¨åºæçæ¹é¢å¸¶ä¾çæ©éï¼ä»¥åå®åå¨å«çãè³æé±ç§åå¯¦æ½æ¹é¢å¸¶ä¾çææ°ãæ­¤å¤ï¼å®æ¹å¤æ§å°è©ä¼°äºå¤§åèªè¨æ¨¡åçé¨ç½²ç­ç¥ï¼å¼·èª¿äºéæºæ¨¡åå°æ¼ç¢ºä¿é«çä¿å¥ç°å¢ä¸­çè³æé±ç§åé©ææ§çå¿è¦æ§ãæåºäºæªä¾çç ç©¶æ¹åï¼éé»æ¯å¯¦è­ç ç©¶ï¼ä»¥è©ä¼°å¤§åèªè¨æ¨¡åå¨é«çä¿å¥ä¸­çå¯¦éåæï¼ä»¥åéç¼éæ¾å¼è³æéä»¥é²è¡é²ä¸æ­¥çç ç©¶ãæ¬ç¯è©è«æ¨å¨çºå°äººå·¥æºæ§åé«çä¿å¥äº¤åé åæèè¶£çæ°æåè·¨é åç ç©¶äººå¡æä¾å¨é¢çè³æºã

##### **BURExtract-Llama: An LLM for Clinical Concept Extraction in Breast Ultrasound Reports**
2408.11334v1 by Yuxuan Chen, Haoyan Yang, Hengkai Pan, Fardeen Siddiqui, Antonio Verdone, Qingyang Zhang, Sumit Chopra, Chen Zhao, Yiqiu Shen

Breast ultrasound is essential for detecting and diagnosing abnormalities,
with radiology reports summarizing key findings like lesion characteristics and
malignancy assessments. Extracting this critical information is challenging due
to the unstructured nature of these reports, with varied linguistic styles and
inconsistent formatting. While proprietary LLMs like GPT-4 are effective, they
are costly and raise privacy concerns when handling protected health
information. This study presents a pipeline for developing an in-house LLM to
extract clinical information from radiology reports. We first use GPT-4 to
create a small labeled dataset, then fine-tune a Llama3-8B model on it.
Evaluated on clinician-annotated reports, our model achieves an average F1
score of 84.6%, which is on par with GPT-4. Our findings demonstrate the
feasibility of developing an in-house LLM that not only matches GPT-4's
performance but also offers cost reductions and enhanced data privacy.

æè¦ï¼ä¹³æ¿è¶é³æ³¢å°æ¼åµæ¸¬åè¨ºæ·ç°å¸¸è³ééè¦ï¼
æ¾å°ç§å ±åæç¸½çµééµç¼ç¾ï¼ä¾å¦çç¶ç¹å¾µåæ¡æ§è©ä¼°ãç±æ¼éäºå ±åçéçµæ§åæ§è³ªãèªè¨é¢¨æ ¼å¤è®ä¸æ ¼å¼ä¸ä¸è´ï¼å æ­¤æåéäºééµè³è¨å·æææ°æ§ãéç¶å GPT-4 éæ¨£çå°æ LLM å¾ææï¼ä½å®åå¨èçåä¿è­·çå¥åº·è³è¨æææ¬é«æä¸æå¼èµ·é±ç§åé¡ãéé ç ç©¶æåºäºä¸åéç¼å§é¨ LLM çç®¡éï¼ä»¥å¾æ¾å°ç§å ±åä¸­æåè¨åºè³è¨ãæåé¦åä½¿ç¨ GPT-4 å»ºç«ä¸åå°åæ¨ç±¤è³æéï¼ç¶å¾å° Llama3-8B æ¨¡åé²è¡å¾®èª¿ãæ ¹æè¨åºé«å¸«è¨»è§£çå ±åé²è¡è©ä¼°ï¼æåçæ¨¡åéå°å¹³å F1 åæ¸çº 84.6%ï¼éè GPT-4 ç¸ç¶ãæåçç ç©¶çµæè­æäºéç¼å§é¨ LLM çå¯è¡æ§ï¼å®ä¸åè½è GPT-4 çæè½ç¸å¹éï¼éè½éä½ææ¬ä¸¦å¢å¼·è³æé±ç§ã

##### **Probabilistic Medical Predictions of Large Language Models**
2408.11316v1 by Bowen Gu, Rishi J. Desai, Kueiyu Joshua Lin, Jie Yang

Large Language Models (LLMs) have demonstrated significant potential in
clinical applications through prompt engineering, which enables the generation
of flexible and diverse clinical predictions. However, they pose challenges in
producing prediction probabilities, which are essential for transparency and
allowing clinicians to apply flexible probability thresholds in
decision-making. While explicit prompt instructions can lead LLMs to provide
prediction probability numbers through text generation, LLMs' limitations in
numerical reasoning raise concerns about the reliability of these
text-generated probabilities. To assess this reliability, we compared explicit
probabilities derived from text generation to implicit probabilities calculated
based on the likelihood of predicting the correct label token. Experimenting
with six advanced open-source LLMs across five medical datasets, we found that
the performance of explicit probabilities was consistently lower than implicit
probabilities with respect to discrimination, precision, and recall. Moreover,
these differences were enlarged on small LLMs and imbalanced datasets,
emphasizing the need for cautious interpretation and applications, as well as
further research into robust probability estimation methods for LLMs in
clinical contexts.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ééæç¤ºå·¥ç¨å±ç¤ºäºå¨è¨åºæç¨ä¸çé¡¯èæ½åï¼éä½¿å¾ç¢çéæ´»å¤æ¨£çè¨åºé æ¸¬æçºå¯è½ãç¶èï¼å®åå¨ç¢çé æ¸¬æ©çä¸éå°äºææ°ï¼èéå°æ¼éæåº¦ååè¨±è¨åºé«å¸«å¨æ±ºç­ä¸­å¥ç¨éæ´»çæ©çé¾å¼è³ééè¦ãåç®¡æç¢ºçæç¤ºèªªæå¯ä»¥å¼å° LLM ééæå­ç¢çæä¾é æ¸¬æ©çæ¸å­ï¼ä½ LLM å¨æ¸å­æ¨çä¸çéå¶å¼ç¼äºå°æ¼éäºæå­ç¢ççæ©çå¯é æ§ççæ®ãçºäºè©ä¼°éç¨®å¯é æ§ï¼æåå°å¾æå­ç¢çä¸­è¡ççæç¢ºæ©çèæ ¹æé æ¸¬æ­£ç¢ºæ¨è¨ç¬¦èçå¯è½æ§è¨ç®çé±å«æ©çé²è¡æ¯è¼ãæåä½¿ç¨å­ç¨®åé²çéæº LLM éå°äºåé«çè³æéé²è¡å¯¦é©ï¼ç¼ç¾æç¢ºæ©ççè¡¨ç¾å§çµä½æ¼é±å«æ©çï¼ç¡è«æ¯å¨å¤å¥ãç²¾æºåº¦åå¬åçæ¹é¢çæ¯å¦æ­¤ãæ­¤å¤ï¼éäºå·®ç°å¨å°å LLM åä¸å¹³è¡¡è³æéä¸è¢«æ¾å¤§äºï¼éå¼·èª¿äºè¬¹æè§£è®åæç¨ä»¥åé²ä¸æ­¥ç ç©¶ LLM å¨è¨åºæå¢ä¸­ç©©å¥æ©çä¼°è¨æ¹æ³çå¿è¦æ§ã

##### **Applying and Evaluating Large Language Models in Mental Health Care: A Scoping Review of Human-Assessed Generative Tasks**
2408.11288v1 by Yining Hua, Hongbin Na, Zehan Li, Fenglin Liu, Xiao Fang, David Clifton, John Torous

Large language models (LLMs) are emerging as promising tools for mental
health care, offering scalable support through their ability to generate
human-like responses. However, the effectiveness of these models in clinical
settings remains unclear. This scoping review aimed to assess the current
generative applications of LLMs in mental health care, focusing on studies
where these models were tested with human participants in real-world scenarios.
A systematic search across APA PsycNet, Scopus, PubMed, and Web of Science
identified 726 unique articles, of which 17 met the inclusion criteria. These
studies encompassed applications such as clinical assistance, counseling,
therapy, and emotional support. However, the evaluation methods were often
non-standardized, with most studies relying on ad hoc scales that limit
comparability and robustness. Privacy, safety, and fairness were also
frequently underexplored. Moreover, reliance on proprietary models, such as
OpenAI's GPT series, raises concerns about transparency and reproducibility.
While LLMs show potential in expanding mental health care access, especially in
underserved areas, the current evidence does not fully support their use as
standalone interventions. More rigorous, standardized evaluations and ethical
oversight are needed to ensure these tools can be safely and effectively
integrated into clinical practice.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) æ­£ä½çºå¿çä¿å¥çå·¥å·æµ®ç¾ï¼ééç¢çé¡äººçåææä¾å¯æ´åçæ¯æãç¶èï¼éäºæ¨¡åå¨è¨åºç°å¢ä¸­çæææ§ä»ä¸æç¢ºãæ¬ç¯åæ¢è¨æ¨å¨è©ä¼° LLM å¨å¿çä¿å¥ä¸­çç¾æçææç¨ï¼éé»å¨æ¼å¨çå¯¦ä¸çæå¢ä¸­ä»¥äººé¡åèèæ¸¬è©¦éäºæ¨¡åçç ç©¶ãç³»çµ±æ§æå° APA PsycNetãScopusãPubMed å Web of Science æ¾åº 726 ç¯ç¨ç¹æç« ï¼å¶ä¸­ 17 ç¯ç¬¦åç´å¥æ¨æºãéäºç ç©¶æ¶µèè¨åºåå©ãè«®è©¢ãæ²»çåæç·æ¯æç­æç¨ãç¶èï¼è©ä¼°æ¹æ³éå¸¸æªæ¨æºåï¼å¤§å¤æ¸ç ç©¶ä¾è³´æ¼éå¶å¯æ¯è¼æ§åç©©å¥æ§çè¨æéè¡¨ãé±ç§ãå®å¨åå¬å¹³æ§ä¹ç¶å¸¸æªååæ¢è¨ãæ­¤å¤ï¼ä¾è³´æ¼å°ææ¨¡åï¼ä¾å¦ OpenAI ç GPT ç³»åï¼æå¼ç¼å°éæåº¦åå¯è¤è£½æ§ççæ®ãéç¶ LLM å¨æ´å±å¿çä¿å¥æåæ¹é¢å±ç¾æ½åï¼ç¹å¥æ¯å¨æåä¸è¶³çå°åï¼ä½ç®åçè­æä¸¦ä¸å®å¨æ¯æå°å¶ç¨ä½ç¨ç«å¹²é æªæ½ãéè¦æ´å´è¬¹ãæ¨æºåçè©ä¼°åå«çç£ç£ï¼ä»¥ç¢ºä¿éäºå·¥å·è½å®å¨ä¸ææå°æ´åå°è¨åºå¯¦åä¸­ã

##### **BearLLM: A Prior Knowledge-Enhanced Bearing Health Management Framework with Unified Vibration Signal Representation**
2408.11281v1 by Haotian Peng, Jiawei Liu, Jinsong Du, Jie Gao, Wei Wang

We propose a bearing health management framework leveraging large language
models (BearLLM), a novel multimodal model that unifies multiple
bearing-related tasks by processing user prompts and vibration signals.
Specifically, we introduce a prior knowledge-enhanced unified vibration signal
representation to handle various working conditions across multiple datasets.
This involves adaptively sampling the vibration signals based on the sampling
rate of the sensor, incorporating the frequency domain to unify input
dimensions, and using a fault-free reference signal as an auxiliary input. To
extract features from vibration signals, we first train a fault classification
network, then convert and align the extracted features into word embedding, and
finally concatenate these with text embedding as input to an LLM. To evaluate
the performance of the proposed method, we constructed the first large-scale
multimodal bearing health management (MBHM) dataset, including paired vibration
signals and textual descriptions. With our unified vibration signal
representation, BearLLM using one set of pre-trained weights achieves
state-of-the-art performance on nine publicly available fault diagnosis
benchmarks, outperforming specific methods designed for individual datasets. We
provide a dataset, our model, and code to inspire future research on building
more capable industrial multimodal models
(https://github.com/hatton613/BearLLM).

æè¦ï¼æåæåºä¸åè»¸æ¿å¥åº·ç®¡çæ¶æ§ï¼å©ç¨å¤§åèªè¨æ¨¡å (BearLLM)ï¼éæ¯ä¸åæ°ç©çå¤æ¨¡ææ¨¡åï¼å®ééèçä½¿ç¨èæç¤ºåæ¯åè¨èï¼çµ±ä¸å¤åèè»¸æ¿ç¸éçä»»åãå·é«ä¾èªªï¼æåå¼å¥ä¸ååé©ç¥è­å¢å¼·ççµ±ä¸æ¯åè¨èè¡¨ç¤ºï¼ä»¥èçå¤åè³æéä¸­çåç¨®å·¥ä½æ¢ä»¶ãéåå«æ ¹æææ¸¬å¨çåæ¨£çèªé©æåæ¨£æ¯åè¨èãçµåé »çåä»¥çµ±ä¸è¼¸å¥ç¶­åº¦ï¼ä»¥åä½¿ç¨ç¡æéåèè¨èä½çºè¼å©è¼¸å¥ãçºäºå¾æ¯åè¨èä¸­æåç¹å¾µï¼æåé¦åè¨ç·´ä¸åæéåé¡ç¶²è·¯ï¼ç¶å¾å°æåçç¹å¾µè½æä¸¦å°é½å°å­è©åµå¥ä¸­ï¼æå¾å°éäºç¹å¾µèæå­åµå¥ä¸²æ¥ä½çº LLM çè¼¸å¥ãçºäºè©ä¼°ææåºæ¹æ³çæè½ï¼æåå»ºæ§äºç¬¬ä¸åå¤§åå¤æ¨¡æè»¸æ¿å¥åº·ç®¡ç (MBHM) è³æéï¼å¶ä¸­åæ¬éå°çæ¯åè¨èåæå­æè¿°ãééæåççµ±ä¸æ¯åè¨èè¡¨ç¤ºï¼ä½¿ç¨ä¸çµé è¨ç·´æ¬éç BearLLM å¨ä¹åå¬éå¯ç¨çæéè¨ºæ·åºæºæ¸¬è©¦ä¸­éææåé²çæè½ï¼åªæ¼å°éçºåå¥è³æéè¨­è¨çç¹å®æ¹æ³ãæåæä¾ä¸åè³æéãæåçæ¨¡ååç¨å¼ç¢¼ï¼ä»¥æ¿åµæªä¾å¨å»ºæ§æ´å¼·å¤§çç¢æ¥­å¤æ¨¡ææ¨¡åæ¹é¢çç ç©¶ (https://github.com/hatton613/BearLLM)ã

##### **From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis**
2408.11876v1 by Guy Lutsker, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R Greenfield, Dorit Samocha-Bonet, Shie Mannor, Eli Meirom, Gal Chechik, Hagai Rossman, Eran Segal

Recent advances in self-supervised learning enabled novel medical AI models,
known as foundation models (FMs) that offer great potential for characterizing
health from diverse biomedical data. Continuous glucose monitoring (CGM)
provides rich, temporal data on glycemic patterns, but its full potential for
predicting broader health outcomes remains underutilized. Here, we present
GluFormer, a generative foundation model on biomedical temporal data based on a
transformer architecture, and trained on over 10 million CGM measurements from
10,812 non-diabetic individuals. We tokenized the CGM training data and trained
GluFormer using next token prediction in a generative, autoregressive manner.
We demonstrate that GluFormer generalizes effectively to 15 different external
datasets, including 4936 individuals across 5 different geographical regions, 6
different CGM devices, and several metabolic disorders, including
normoglycemic, prediabetic, and diabetic populations, as well as those with
gestational diabetes and obesity. GluFormer produces embeddings which
outperform traditional CGM analysis tools, and achieves high Pearson
correlations in predicting clinical parameters such as HbA1c, liver-related
parameters, blood lipids, and sleep-related indices. Notably, GluFormer can
also predict onset of future health outcomes even 4 years in advance. We also
show that CGM embeddings from pre-intervention periods in Randomized Clinical
Trials (RCTs) outperform other methods in predicting primary and secondary
outcomes. When integrating dietary data into GluFormer, we show that the
enhanced model can accurately generate CGM data based only on dietary intake
data, simulate outcomes of dietary interventions, and predict individual
responses to specific foods. Overall, we show that GluFormer accurately
predicts health outcomes which generalize across different populations
metabolic conditions.

æè¦ï¼<paragraph>èªæç£ç£å¼å­¸ç¿çææ°é²å±ä¿æäºæ°åé«çAIæ¨¡åï¼
ç¨±çºåºç¤æ¨¡åï¼FMï¼ï¼å®æä¾äºå¾å¤æ¨£åççç©é«å­¸æ¸æä¸­è¡¨å¾µ
å¥åº·çå·¨å¤§æ½åãé£çºè¡ç³ç£æ¸¬ï¼CGMï¼
æä¾è±å¯çæéæ¸æï¼äºè§£è¡ç³æ¨¡å¼ï¼ä½å¶å¨
é æ¸¬æ´å»£æ³çå¥åº·çµææ¹é¢çå¨é¨æ½åä»æªå¾å°ååå©ç¨ãå¨æ­¤ï¼æåæåº
GluFormerï¼ä¸ååºæ¼Transformeræ¶æ§ççç©é«å­¸æéæ¸æççæåºç¤æ¨¡åï¼ä¸¦å¨è¶é 1000 è¬å CGM æ¸¬éå¼ä¸é²è¡è¨ç·´ï¼ä¾èª
10,812 åéç³å°¿çæ£èãæåå° CGM è¨ç·´æ¸æé²è¡äºæ¨è¨åï¼ä¸¦ä½¿ç¨çæå¼ãèªè¿´æ­¸æ¹å¼è¨ç·´äº GluFormerï¼ä»¥é²è¡ä¸ä¸åæ¨è¨é æ¸¬ã
æåè­æ GluFormer ææå°æ¦æ¬çº 15 åä¸åçå¤é¨
æ¸æéï¼åæ¬ä¾èª 5 åä¸åå°çååç 4936 äººã6
ä¸åç CGM è¨­åï¼ä»¥åå¹¾ç¨®ä»£è¬ç¾çï¼åæ¬
å¸¸è¡ç³ãç³å°¿çåæåç³å°¿çäººç¾¤ï¼ä»¥å
æ£æå¦å¨ ç³å°¿çåè¥èççäººãGluFormer ç¢ççåµå¥
åªæ¼å³çµ±ç CGM åæå·¥å·ï¼ä¸¦ä¸å¨é æ¸¬ HbA1cãèèç¸é
åæ¸ãè¡èåç¡ç ç¸éææ¨ç­è¨åºåæ¸æå¯¦ç¾äºå¾é«ç Pearson
ç¸éæ§ãå¼å¾æ³¨æçæ¯ï¼GluFormer çè³å¯ä»¥
é æ¸¬æªä¾å¥åº·çµæçç¼ä½ï¼å³ä½¿æå 4 å¹´ãæåé
è¡¨æï¼å¨é¨æ©è¨åºè©¦é© (RCT) ä¸­å¹²é åæç CGM åµå¥
å¨é æ¸¬ä¸»è¦åæ¬¡è¦æ¹é¢åªæ¼å¶ä»æ¹æ³
çµæãç¶å°é£²é£æ¸ææ´åå° GluFormer ä¸­æï¼æåè¡¨æå¢å¼·çæ¨¡åå¯ä»¥åæ ¹æé£²é£æå¥æºç¢ºçæ CGM æ¸æ
æ¸æï¼æ¨¡æ¬é£²é£å¹²é ççµæï¼ä¸¦é æ¸¬åé«
å°ç¹å®é£ç©çåæãç¸½çä¾èªªï¼æåè¡¨æ GluFormer æºç¢º
é æ¸¬å¥åº·çµæï¼éäºçµææ¦æ¬äºä¸åçäººç¾¤
ä»£è¬çæ³ã</paragraph>

##### **Fine-Tuning a Local LLaMA-3 Large Language Model for Automated Privacy-Preserving Physician Letter Generation in Radiation Oncology**
2408.10715v1 by Yihao Hou, Christoph Bert, Ahmed Gomaa, Godehard Lahmer, Daniel Hoefler, Thomas Weissmann, Raphaela Voigt, Philipp Schubert, Charlotte Schmitter, Alina Depardon, Sabine Semrau, Andreas Maier, Rainer Fietkau, Yixing Huang, Florian Putz

Generating physician letters is a time-consuming task in daily clinical
practice. This study investigates local fine-tuning of large language models
(LLMs), specifically LLaMA models, for physician letter generation in a
privacy-preserving manner within the field of radiation oncology. Our findings
demonstrate that base LLaMA models, without fine-tuning, are inadequate for
effectively generating physician letters. The QLoRA algorithm provides an
efficient method for local intra-institutional fine-tuning of LLMs with limited
computational resources (i.e., a single 48 GB GPU workstation within the
hospital). The fine-tuned LLM successfully learns radiation oncology-specific
information and generates physician letters in an institution-specific style.
ROUGE scores of the generated summary reports highlight the superiority of the
8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician
evaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has
limited capacity to generate content beyond the provided input data, it
successfully generates salutations, diagnoses and treatment histories,
recommendations for further treatment, and planned schedules. Overall, clinical
benefit was rated highly by the clinical experts (average score of 3.44 on a
4-point scale). With careful physician review and correction, automated
LLM-based physician letter generation has significant practical value.

æè¦ï¼<paragraph>å¨æ¥å¸¸è¨åºå¯¦åä¸­ï¼çæé«å¸«ä¿¡å½æ¯ä¸é èæçä»»åãæ¬ç ç©¶æ¢è¨å¤§åèªè¨æ¨¡å (LLM) çå±é¨å¾®èª¿ï¼ç¹å¥æ¯ LLaMA æ¨¡åï¼å¨æ¾å°è«ç¤å­¸é åä¸­ä»¥é±ç§ä¿è­·çæ¹å¼çæé«å¸«ä¿¡å½ãæåçç ç©¶çµæè¡¨æï¼åºç¤ LLaMA æ¨¡åå¨æ²æå¾®èª¿çææ³ä¸ï¼ä¸è¶³ä»¥ææçæé«å¸«ä¿¡å½ãQLoRA æ¼ç®æ³æä¾äºä¸ç¨®ææçæ¹æ³ï¼å¯ä»¥å¨æéçéç®è³æºï¼å³é«é¢å§å®ä¸ 48 GB GPU å·¥ä½ç«ï¼ä¸ï¼é²è¡ LLM çå±é¨é¢å§å¾®èª¿ãå¾®èª¿å¾ç LLM æåå­¸ç¿äºæ¾å°è«ç¤å­¸çç¹å®è³è¨ï¼ä¸¦ä»¥ç¹å®æ¼æ©æ§çé¢¨æ ¼çæé«å¸«ä¿¡å½ãçæçæè¦å ±åç ROUGE åæ¸çªé¡¯äº 8B LLaMA-3 æ¨¡ååªæ¼ 13B LLaMA-2 æ¨¡åãé²ä¸æ­¥çå¤ç¶­é«å¸«è©ä¼°é¡¯ç¤ºï¼åç®¡å¾®èª¿å¾ç LLaMA-3 æ¨¡åçæè¶åºæä¾è¼¸å¥è³æçå§å®¹çè½åæéï¼ä½å®æåå°çæäºååèªãè¨ºæ·åæ²»ççå²ãé²ä¸æ­¥æ²»çå»ºè­°åè¨ç«è¡ç¨ãæ´é«èè¨ï¼è¨åºå°å®¶å°è¨åºæççè©åå¾é«ï¼å¨ 4 åå¶ä¸­å¹³åå¾åçº 3.44ï¼ãééä»ç´°çé«å¸«å¯©æ¥åæ´æ­£ï¼åºæ¼ LLM çèªååé«å¸«ä¿¡å½çæå·æé¡¯èçå¯¦ç¨å¹å¼ã</paragraph>

