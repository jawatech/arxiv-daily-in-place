
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-06**|**LLaVA-OneVision: Easy Visual Task Transfer**|Bo Li et.al.|[2408.03326v1](http://arxiv.org/abs/2408.03326v1)|null|
|**2024-08-06**|**CoverBench: A Challenging Benchmark for Complex Claim Verification**|Alon Jacovi et.al.|[2408.03325v1](http://arxiv.org/abs/2408.03325v1)|null|
|**2024-08-06**|**Training LLMs to Recognize Hedges in Spontaneous Narratives**|Amie J. Paige et.al.|[2408.03319v1](http://arxiv.org/abs/2408.03319v1)|null|
|**2024-08-06**|**Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters**|Charlie Snell et.al.|[2408.03314v1](http://arxiv.org/abs/2408.03314v1)|null|
|**2024-08-06**|**KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models**|Ruizhe Zhang et.al.|[2408.03297v1](http://arxiv.org/abs/2408.03297v1)|null|
|**2024-08-06**|**Static IR Drop Prediction with Attention U-Net and Saliency-Based Explainability**|Lizi Zhang et.al.|[2408.03292v1](http://arxiv.org/abs/2408.03292v1)|null|
|**2024-08-06**|**SARA: Singular-Value Based Adaptive Low-Rank Adaption**|Jihao Gu et.al.|[2408.03290v1](http://arxiv.org/abs/2408.03290v1)|null|
|**2024-08-06**|**StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation**|Boxi Cao et.al.|[2408.03281v2](http://arxiv.org/abs/2408.03281v2)|[link](https://github.com/c-box/structeval)|
|**2024-08-06**|**Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments**|Angie Boggust et.al.|[2408.03274v1](http://arxiv.org/abs/2408.03274v1)|null|
|**2024-08-06**|**Synthesizing Text-to-SQL Data from Weak and Strong LLMs**|Jiaxi Yang et.al.|[2408.03256v1](http://arxiv.org/abs/2408.03256v1)|null|
|**2024-08-06**|**Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons**|Yifei Wang et.al.|[2408.03247v1](http://arxiv.org/abs/2408.03247v1)|null|
|**2024-08-06**|**Making Long-Context Language Models Better Multi-Hop Reasoners**|Yanyang Li et.al.|[2408.03246v1](http://arxiv.org/abs/2408.03246v1)|[link](https://github.com/lavi-lab/longcontextreasoner)|
|**2024-08-06**|**Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery**|Jialang Xu et.al.|[2408.03208v1](http://arxiv.org/abs/2408.03208v1)|null|
|**2024-08-06**|**A Debiased Nearest Neighbors Framework for Multi-Label Text Classification**|Zifeng Cheng et.al.|[2408.03202v1](http://arxiv.org/abs/2408.03202v1)|null|
|**2024-08-06**|**Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors**|Kunkun Hao et.al.|[2408.03200v2](http://arxiv.org/abs/2408.03200v2)|null|
|**2024-08-06**|**Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi**|Pranita Deshmukh et.al.|[2408.03172v1](http://arxiv.org/abs/2408.03172v1)|null|
|**2024-08-06**|**Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW**|Elia Cereda et.al.|[2408.03168v1](http://arxiv.org/abs/2408.03168v1)|null|
|**2024-08-06**|**Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study**|Rabih Chamas et.al.|[2408.03164v1](http://arxiv.org/abs/2408.03164v1)|[link](https://github.com/rabihchamas/dcls-gradcam-eval)|
|**2024-08-06**|**Conditioning LLMs with Emotion in Neural Machine Translation**|Charles Brazier et.al.|[2408.03150v1](http://arxiv.org/abs/2408.03150v1)|null|
|**2024-08-06**|**Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization**|Yanghai Zhang et.al.|[2408.03149v1](http://arxiv.org/abs/2408.03149v1)|null|
|**2024-08-06**|**Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations**|Leo Donisch et.al.|[2408.03130v1](http://arxiv.org/abs/2408.03130v1)|null|
|**2024-08-06**|**Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation**|Artur Guimar√£es et.al.|[2408.03127v1](http://arxiv.org/abs/2408.03127v1)|null|
|**2024-08-06**|**COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework**|Rajvee Sheth et.al.|[2408.03125v1](http://arxiv.org/abs/2408.03125v1)|[link](https://github.com/lingo-iitgn/commentator)|
|**2024-08-06**|**Evaluating the Translation Performance of Large Language Models Based on Euas-20**|Yan Huang et.al.|[2408.03119v1](http://arxiv.org/abs/2408.03119v1)|null|
|**2024-08-06**|**Topic Modeling with Fine-tuning LLMs and Bag of Sentences**|Johannes Schneider et.al.|[2408.03099v1](http://arxiv.org/abs/2408.03099v1)|[link](https://github.com/johntailor/ft-topic)|
|**2024-08-06**|**500xCompressor: Generalized Prompt Compression for Large Language Models**|Zongqian Li et.al.|[2408.03094v1](http://arxiv.org/abs/2408.03094v1)|null|
|**2024-08-06**|**Learning Provably Robust Policies in Uncertain Parametric Environments**|Yannik Schnitzer et.al.|[2408.03093v1](http://arxiv.org/abs/2408.03093v1)|null|
|**2024-08-06**|**Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement**|Le Yu et.al.|[2408.03092v1](http://arxiv.org/abs/2408.03092v1)|null|
|**2024-08-06**|**Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion**|Jinglong Gao et.al.|[2408.03079v1](http://arxiv.org/abs/2408.03079v1)|null|
|**2024-08-06**|**BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications**|G. Manni et.al.|[2408.03078v1](http://arxiv.org/abs/2408.03078v1)|null|
|**2024-08-06**|**Towards an Analysis of Discourse and Interactional Pragmatic Reasoning Capabilities of Large Language Models**|Amelie Robrecht et.al.|[2408.03074v1](http://arxiv.org/abs/2408.03074v1)|null|
|**2024-08-06**|**Probing structural constraints of negation in Pretrained Language Models**|David Kletz et.al.|[2408.03070v1](http://arxiv.org/abs/2408.03070v1)|null|
|**2024-08-06**|**Analysis of Argument Structure Constructions in a Deep Recurrent Language Model**|Pegah Ramezani et.al.|[2408.03062v1](http://arxiv.org/abs/2408.03062v1)|null|
|**2024-08-06**|**OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents**|Qiang Sun et.al.|[2408.03047v1](http://arxiv.org/abs/2408.03047v1)|null|
|**2024-08-06**|**L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification & Summarization**|Elvys Linhares Pontes et.al.|[2408.03033v1](http://arxiv.org/abs/2408.03033v1)|null|
|**2024-08-06**|**Integrating Controllable Motion Skills from Demonstrations**|Honghao Liao et.al.|[2408.03018v1](http://arxiv.org/abs/2408.03018v1)|null|
|**2024-08-06**|**Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs**|Daniel Steinigen et.al.|[2408.03010v1](http://arxiv.org/abs/2408.03010v1)|null|
|**2024-08-06**|**LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning**|Lekai Chen et.al.|[2408.02999v1](http://arxiv.org/abs/2408.02999v1)|null|
|**2024-08-06**|**ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval**|Ruixiang Zhao et.al.|[2408.02978v1](http://arxiv.org/abs/2408.02978v1)|null|
|**2024-08-06**|**Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation**|Hui Ma et.al.|[2408.02976v1](http://arxiv.org/abs/2408.02976v1)|null|
|**2024-08-06**|**EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and Quantization**|Zhaopeng Feng et.al.|[2408.02970v1](http://arxiv.org/abs/2408.02970v1)|[link](https://github.com/fzp0424/ec-guide-kddup-2024)|
|**2024-08-06**|**Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval**|Iman Azimi et.al.|[2408.02964v1](http://arxiv.org/abs/2408.02964v1)|null|
|**2024-08-06**|**Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment Gaps**|Yifan Zhu et.al.|[2408.02949v1](http://arxiv.org/abs/2408.02949v1)|null|
|**2024-08-06**|**Are Female Carpenters like Blue Bananas? A Corpus Investigation of Occupation Gender Typicality**|Da Ju et.al.|[2408.02948v1](http://arxiv.org/abs/2408.02948v1)|null|
|**2024-08-06**|**Scaling Laws for Data Poisoning in LLMs**|Dillon Bowen et.al.|[2408.02946v1](http://arxiv.org/abs/2408.02946v1)|null|
|**2024-08-06**|**Self-Supervised Learning for Multi-Channel Neural Transducer**|Atsushi Kojima et.al.|[2408.02945v1](http://arxiv.org/abs/2408.02945v1)|null|
|**2024-08-06**|**LLM-Empowered Resource Allocation in Wireless Communications Systems**|Woongsup Lee et.al.|[2408.02944v1](http://arxiv.org/abs/2408.02944v1)|null|
|**2024-08-06**|**HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection**|Yuxin Wang et.al.|[2408.02927v1](http://arxiv.org/abs/2408.02927v1)|null|
|**2024-08-06**|**Intermediate direct preference optimization**|Atsushi Kojima et.al.|[2408.02923v1](http://arxiv.org/abs/2408.02923v1)|null|
|**2024-08-06**|**A Taxonomy of Architecture Options for Foundation Model-based Agents: Analysis and Decision Model**|Jingwen Zhou et.al.|[2408.02920v1](http://arxiv.org/abs/2408.02920v1)|null|
|**2024-08-06**|**Data Checklist: On Unit-Testing Datasets with Usable Information**|Heidi C. Zhang et.al.|[2408.02919v1](http://arxiv.org/abs/2408.02919v1)|null|
|**2024-08-06**|**KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance**|Jingxian Lu et.al.|[2408.02912v1](http://arxiv.org/abs/2408.02912v1)|null|
|**2024-08-06**|**Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large Language Model-Based Question Answering**|Tiezheng Guo et.al.|[2408.02907v1](http://arxiv.org/abs/2408.02907v1)|null|
|**2024-08-06**|**Enabling Intelligent Traffic Systems: A Deep Learning Method for Accurate Arabic License Plate Recognition**|M. A. Sayedelahl et.al.|[2408.02904v1](http://arxiv.org/abs/2408.02904v1)|null|
|**2024-08-06**|**Lighthouse: A User-Friendly Library for Reproducible Video Moment Retrieval and Highlight Detection**|Taichi Nishimura et.al.|[2408.02901v1](http://arxiv.org/abs/2408.02901v1)|[link](https://github.com/line/lighthouse)|
|**2024-08-06**|**SETN: Stock Embedding Enhanced with Textual and Network Information**|Takehiro Takayanagi et.al.|[2408.02899v1](http://arxiv.org/abs/2408.02899v1)|null|
|**2024-08-06**|**A Metric Driven Approach to Mixed Precision Training**|Mitchelle Rasquinha et.al.|[2408.02897v1](http://arxiv.org/abs/2408.02897v1)|null|
|**2024-08-06**|**VizECGNet: Visual ECG Image Network for Cardiovascular Diseases Classification with Multi-Modal Training and Knowledge Distillation**|Ju-Hyeon Nam et.al.|[2408.02888v1](http://arxiv.org/abs/2408.02888v1)|null|
|**2024-08-06**|**Compromising Embodied Agents with Contextual Backdoor Attacks**|Aishan Liu et.al.|[2408.02882v1](http://arxiv.org/abs/2408.02882v1)|null|
|**2024-08-06**|**Hide and Seek: Fingerprinting Large Language Models with Evolutionary Learning**|Dmitri Iourovitski et.al.|[2408.02871v1](http://arxiv.org/abs/2408.02871v1)|null|
|**2024-08-05**|**VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge**|Zihan Li et.al.|[2408.02865v1](http://arxiv.org/abs/2408.02865v1)|null|
|**2024-08-05**|**A Framework for Fine-Tuning LLMs using Heterogeneous Feedback**|Ryan Aponte et.al.|[2408.02861v1](http://arxiv.org/abs/2408.02861v1)|null|
|**2024-08-05**|**Multistain Pretraining for Slide Representation Learning in Pathology**|Guillaume Jaume et.al.|[2408.02859v1](http://arxiv.org/abs/2408.02859v1)|null|
|**2024-08-05**|**Development of REGAI: Rubric Enabled Generative Artificial Intelligence**|Zach Johnson et.al.|[2408.02811v1](http://arxiv.org/abs/2408.02811v1)|null|
|**2024-08-05**|**Examining Gender and Power on Wikipedia Through Face and Politeness**|Adil Soubki et.al.|[2408.02798v1](http://arxiv.org/abs/2408.02798v1)|null|
|**2024-08-05**|**LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory**|Jillian Ross et.al.|[2408.02784v1](http://arxiv.org/abs/2408.02784v1)|null|
|**2024-08-05**|**Self-Taught Evaluators**|Tianlu Wang et.al.|[2408.02666v1](http://arxiv.org/abs/2408.02666v1)|null|
|**2024-08-05**|**Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?**|Mohammad Bahrami Karkevandi et.al.|[2408.02651v1](http://arxiv.org/abs/2408.02651v1)|null|
|**2024-08-05**|**SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models**|Muxi Diao et.al.|[2408.02632v1](http://arxiv.org/abs/2408.02632v1)|null|
|**2024-08-05**|**Language Model Can Listen While Speaking**|Ziyang Ma et.al.|[2408.02622v1](http://arxiv.org/abs/2408.02622v1)|null|
|**2024-08-05**|**BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba**|Ling Yue et.al.|[2408.02600v1](http://arxiv.org/abs/2408.02600v1)|null|
|**2024-08-05**|**Progressively Selective Label Enhancement for Language Model Alignment**|Biao Liu et.al.|[2408.02599v1](http://arxiv.org/abs/2408.02599v1)|null|
|**2024-08-05**|**Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection**|Sajal Aggarwal et.al.|[2408.02595v1](http://arxiv.org/abs/2408.02595v1)|null|
|**2024-08-05**|**Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization**|Ankan Mullick et.al.|[2408.02584v1](http://arxiv.org/abs/2408.02584v1)|null|
|**2024-08-05**|**Clustering and Mining Accented Speech for Inclusive and Fair Speech Recognition**|Jaeyoung Kim et.al.|[2408.02582v1](http://arxiv.org/abs/2408.02582v1)|null|
|**2024-08-05**|**Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs**|Ananya Pandey et.al.|[2408.02571v1](http://arxiv.org/abs/2408.02571v1)|null|
|**2024-08-05**|**Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information**|Yauwai Yim et.al.|[2408.02559v1](http://arxiv.org/abs/2408.02559v1)|null|
|**2024-08-05**|**MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization**|Yiwen Chen et.al.|[2408.02555v1](http://arxiv.org/abs/2408.02555v1)|null|
|**2024-08-05**|**The Role of Functional Muscle Networks in Improving Hand Gesture Perception for Human-Machine Interfaces**|Costanza Armanini et.al.|[2408.02547v1](http://arxiv.org/abs/2408.02547v1)|null|
|**2024-08-05**|**RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation**|Daniel Fleischer et.al.|[2408.02545v1](http://arxiv.org/abs/2408.02545v1)|null|
|**2024-08-05**|**Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions**|Xinbei Ma et.al.|[2408.02544v1](http://arxiv.org/abs/2408.02544v1)|null|
|**2024-08-05**|**OneLove beyond the field -- A few-shot pipeline for topic and sentiment analysis during the FIFA World Cup in Qatar**|Christoph Rauchegger et.al.|[2408.02520v1](http://arxiv.org/abs/2408.02520v1)|null|
|**2024-08-05**|**UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model**|Zhaowei Li et.al.|[2408.02503v1](http://arxiv.org/abs/2408.02503v1)|[link](https://github.com/lzw-lzw/unifiedmllm)|
|**2024-08-05**|**MDM: Advancing Multi-Domain Distribution Matching for Automatic Modulation Recognition Dataset Synthesis**|Dongwei Xu et.al.|[2408.02714v1](http://arxiv.org/abs/2408.02714v1)|null|
|**2024-08-05**|**A First Look at License Compliance Capability of LLMs in Code Generation**|Weiwei Xu et.al.|[2408.02487v1](http://arxiv.org/abs/2408.02487v1)|null|
|**2024-08-05**|**A Review on Organ Deformation Modeling Approaches for Reliable Surgical Navigation using Augmented Reality**|Zheng Han et.al.|[2408.02713v1](http://arxiv.org/abs/2408.02713v1)|null|
|**2024-08-05**|**From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future**|Haolin Jin et.al.|[2408.02479v1](http://arxiv.org/abs/2408.02479v1)|null|
|**2024-08-05**|**Automatic Voice Identification after Speech Resynthesis using PPG**|Thibault Gaudier et.al.|[2408.02712v1](http://arxiv.org/abs/2408.02712v1)|null|
|**2024-08-05**|**An investigation into the causes of race bias in AI-based cine CMR segmentation**|Tiarna Lee et.al.|[2408.02462v1](http://arxiv.org/abs/2408.02462v1)|null|
|**2024-08-05**|**Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based Approach**|Wanxu Wei et.al.|[2408.02456v1](http://arxiv.org/abs/2408.02456v1)|null|
|**2024-08-05**|**Text Conditioned Symbolic Drumbeat Generation using Latent Diffusion Models**|Pushkar Jajoria et.al.|[2408.02711v1](http://arxiv.org/abs/2408.02711v1)|null|
|**2024-08-05**|**Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models**|Zhi Rui Tam et.al.|[2408.02442v1](http://arxiv.org/abs/2408.02442v1)|null|
|**2024-08-05**|**Long Input Benchmark for Russian Analysis**|Igor Churin et.al.|[2408.02439v1](http://arxiv.org/abs/2408.02439v1)|null|
|**2024-08-05**|**Infusing Emotions into Task-oriented Dialogue Systems: Understanding, Management, and Generation**|Shutong Feng et.al.|[2408.02417v1](http://arxiv.org/abs/2408.02417v1)|null|
|**2024-08-05**|**Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models**|Zi Liang et.al.|[2408.02416v1](http://arxiv.org/abs/2408.02416v1)|null|
|**2024-08-05**|**Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models**|Tongtong Feng et.al.|[2408.02408v1](http://arxiv.org/abs/2408.02408v1)|null|
|**2024-08-05**|**SnapE -- Training Snapshot Ensembles of Link Prediction Models**|Ali Shaban et.al.|[2408.02707v1](http://arxiv.org/abs/2408.02707v1)|null|
|**2024-08-05**|**Enhancing AI-based Generation of Software Exploits with Contextual Information**|Pietro Liguori et.al.|[2408.02402v2](http://arxiv.org/abs/2408.02402v2)|null|
|**2024-08-05**|**A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models**|Vanni Zavarella et.al.|[2408.02377v1](http://arxiv.org/abs/2408.02377v1)|null|
|**2024-08-05**|**Operationalizing Contextual Integrity in Privacy-Conscious Assistants**|Sahra Ghalebikesabi et.al.|[2408.02373v1](http://arxiv.org/abs/2408.02373v1)|null|

#### Abstracts
##### **LLaVA-OneVision: Easy Visual Task Transfer**
2408.03326v1 by Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li

We present LLaVA-OneVision, a family of open large multimodal models (LMMs)
developed by consolidating our insights into data, models, and visual
representations in the LLaVA-NeXT blog series. Our experimental results
demonstrate that LLaVA-OneVision is the first single model that can
simultaneously push the performance boundaries of open LMMs in three important
computer vision scenarios: single-image, multi-image, and video scenarios.
Importantly, the design of LLaVA-OneVision allows strong transfer learning
across different modalities/scenarios, yielding new emerging capabilities. In
particular, strong video understanding and cross-scenario capabilities are
demonstrated through task transfer from images to videos.

ÊëòË¶ÅÔºöÊàëÂÄëÂ±ïÁ§∫ LLaVA-OneVisionÔºå‰∏ÄÂÄãÈñãÊîæÂºèÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°Âûã (LMM) ÂÆ∂ÊóèÔºå
ÈÄèÈÅéÊï¥ÂêàÊàëÂÄëÂ∞ç LLaVA-NeXT ÈÉ®ËêΩÊ†ºÁ≥ªÂàó‰∏≠Ë≥áÊñô„ÄÅÊ®°ÂûãÂíåË¶ñË¶∫
Ë°®ÁèæÁöÑË¶ãËß£ËÄåÈñãÁôº„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé LLaVA-OneVision ÊòØÁ¨¨‰∏ÄÂÄãÂñÆ‰∏ÄÊ®°ÂûãÔºåÂèØ‰ª•
ÂêåÊôÇÂú®‰∏âÂÄãÈáçË¶ÅÁöÑÈõªËÖ¶Ë¶ñË¶∫Â†¥ÊôØ‰∏≠Êì¥Â±ïÈñãÊîæÂºè LMM ÁöÑÊïàËÉΩÁïåÈôêÔºöÂñÆ‰∏ÄÂΩ±ÂÉè„ÄÅÂ§öÈáçÂΩ±ÂÉèÂíåÂΩ±ÁâáÂ†¥ÊôØ„ÄÇ
ÈáçË¶ÅÁöÑÊòØÔºåLLaVA-OneVision ÁöÑË®≠Ë®àÂÖÅË®±Âú®‰∏çÂêåÁöÑÊ®°ÊÖã/Â†¥ÊôØ‰∏≠ÈÄ≤Ë°åÂº∑Â§ßÁöÑËΩâÁßªÂ≠∏ÁøíÔºåÁî¢ÁîüÊñ∞ÁöÑÊñ∞ËààËÉΩÂäõ„ÄÇÁâπÂà•ÊòØÔºå
ÈÄèÈÅéÂæûÂΩ±ÂÉèÂà∞ÂΩ±ÁâáÁöÑ‰ªªÂãôËΩâÁßªÔºåÂ±ïÁ§∫‰∫ÜÂº∑Â§ßÁöÑÂΩ±ÁâáÁêÜËß£ÂíåË∑®Â†¥ÊôØËÉΩÂäõ„ÄÇ

##### **CoverBench: A Challenging Benchmark for Complex Claim Verification**
2408.03325v1 by Alon Jacovi, Moran Ambar, Eyal Ben-David, Uri Shaham, Amir Feder, Mor Geva, Dror Marcus, Avi Caciularu

There is a growing line of research on verifying the correctness of language
models' outputs. At the same time, LMs are being used to tackle complex queries
that require reasoning. We introduce CoverBench, a challenging benchmark
focused on verifying LM outputs in complex reasoning settings. Datasets that
can be used for this purpose are often designed for other complex reasoning
tasks (e.g., QA) targeting specific use-cases (e.g., financial tables),
requiring transformations, negative sampling and selection of hard examples to
collect such a benchmark. CoverBench provides a diversified evaluation for
complex claim verification in a variety of domains, types of reasoning,
relatively long inputs, and a variety of standardizations, such as multiple
representations for tables where available, and a consistent schema. We
manually vet the data for quality to ensure low levels of label noise. Finally,
we report a variety of competitive baseline results to show CoverBench is
challenging and has very significant headroom. The data is available at
https://huggingface.co/datasets/google/coverbench .

ÊëòË¶ÅÔºöÂ∞çÊñºÈ©óË≠âË™ûË®ÄÊ®°ÂûãËº∏Âá∫ÁöÑÊ≠£Á¢∫ÊÄßÔºåÊúâË∂ä‰æÜË∂äÂ§öÁöÑÁ†îÁ©∂„ÄÇÂêåÊôÇÔºåLM Ë¢´Áî®ÊñºËß£Ê±∫ÈúÄË¶ÅÊé®ÁêÜÁöÑË§áÈõúÊü•Ë©¢„ÄÇÊàëÂÄë‰ªãÁ¥π CoverBenchÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂü∫Ê∫ñÔºåÂ∞àÊ≥®ÊñºÈ©óË≠âË§áÈõúÊé®ÁêÜË®≠ÂÆö‰∏≠ÁöÑ LM Ëº∏Âá∫„ÄÇÂèØÁî®ÊñºÊ≠§ÁõÆÁöÑÁöÑË≥áÊñôÈõÜÈÄöÂ∏∏ÊòØÁÇ∫ÂÖ∂‰ªñË§áÈõúÊé®ÁêÜ‰ªªÂãôÔºà‰æãÂ¶ÇÔºåQAÔºâËÄåË®≠Ë®àÁöÑÔºåÈÄô‰∫õ‰ªªÂãôÈáùÂ∞çÁâπÂÆöÁî®‰æãÔºà‰æãÂ¶ÇÔºåË≤°ÂãôË°®Ê†ºÔºâÔºåÈúÄË¶ÅËΩâÊèõ„ÄÅË≤†Èù¢ÊäΩÊ®£ÂíåÈÅ∏ÊìáÂõ∞Èõ£ÁØÑ‰æã‰æÜÊî∂ÈõÜÊ≠§È°ûÂü∫Ê∫ñ„ÄÇCoverBench Êèê‰æõ‰∫ÜÂ∞çÂêÑÁ®ÆÈ†òÂüü„ÄÅÊé®ÁêÜÈ°ûÂûã„ÄÅÁõ∏Â∞çËºÉÈï∑ÁöÑËº∏ÂÖ•‰ª•ÂèäÂêÑÁ®ÆÊ®ôÊ∫ñÂåñÁöÑÂ§öÂÖÉË©ï‰º∞Ôºå‰æãÂ¶ÇË°®Ê†ºÁöÑÂêÑÁ®ÆË°®Á§∫ÔºàÂ¶ÇÊûúÂèØÁî®ÔºâÂíå‰∏ÄËá¥ÁöÑÊû∂Êßã„ÄÇÊàëÂÄëÊâãÂãïÂØ©Êü•Ë≥áÊñôÁöÑÂìÅË≥™Ôºå‰ª•Á¢∫‰øù‰ΩéÁ®ãÂ∫¶ÁöÑÊ®ôÁ±§ÈõúË®ä„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ†±Âëä‰∫ÜÂêÑÁ®ÆÁ´∂Áà≠ÊÄßÁöÑÂü∫Ê∫ñÁµêÊûúÔºå‰ª•È°ØÁ§∫ CoverBench ÂÖ∑ÊúâÊåëÊà∞ÊÄß‰∏îÊúâÈùûÂ∏∏È°ØËëóÁöÑÈÄ≤Ê≠•Á©∫Èñì„ÄÇË≥áÊñôÂèØÂú® https://huggingface.co/datasets/google/coverbench ÂèñÂæó„ÄÇ

##### **Training LLMs to Recognize Hedges in Spontaneous Narratives**
2408.03319v1 by Amie J. Paige, Adil Soubki, John Murzaku, Owen Rambow, Susan E. Brennan

Hedges allow speakers to mark utterances as provisional, whether to signal
non-prototypicality or "fuzziness", to indicate a lack of commitment to an
utterance, to attribute responsibility for a statement to someone else, to
invite input from a partner, or to soften critical feedback in the service of
face-management needs. Here we focus on hedges in an experimentally
parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced
from memory by 21 speakers for co-present addressees, transcribed to text
(Galati and Brennan, 2010). We created a gold standard of hedges annotated by
human coders (the Roadrunner-Hedge corpus) and compared three LLM-based
approaches for hedge detection: fine-tuning BERT, and zero and few-shot
prompting with GPT-4o and LLaMA-3. The best-performing approach was a
fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on
the top performing approaches, we used an LLM-in-the-Loop approach to improve
the gold standard coding, as well as to highlight cases in which hedges are
ambiguous in linguistically interesting ways that will guide future research.
This is the first step in our research program to train LLMs to interpret and
generate collateral signals appropriately and meaningfully in conversation.

ÊëòË¶ÅÔºö<paragraph>Â∞çÊ≤ñÂÖÅË®±Ë™™Ë©±ËÄÖÂ∞áË®ÄË´ñÊ®ôË®òÁÇ∫Êö´ÂÆöÁöÑÔºåÁÑ°Ë´ñÊòØË¶ÅÊ®ôÁ§∫ÈùûÂéüÂûãÊàñ„ÄåÊ®°Á≥äÊÄß„ÄçÔºåË°®Á§∫Â∞çË®ÄË´ñÁº∫‰πèÊâøË´æÔºåÂ∞áËÅ≤ÊòéÁöÑË≤¨‰ªªÊ≠∏ÂíéÊñº‰ªñ‰∫∫ÔºåÈÇÄË´ãÂ§•‰º¥Êèê‰æõÊÑèË¶ãÔºåÊàñÂú®Èù¢Â≠êÁÆ°ÁêÜÈúÄÊ±ÇÁöÑÊúçÂãô‰∏≠ËªüÂåñÊâπË©ïÊÄßÁöÑÂõûÈ•ã„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëÂ∞àÊ≥®Êñº 21 ‰ΩçË™™Ë©±ËÄÖÁÇ∫ÂÖ±ÂêåÂá∫Â∏≠ÁöÑÂèóË©±ËÄÖËá™ÁôºÊÄßÂú∞ÂæûË®òÊÜ∂‰∏≠Áî¢Áîü 63 ÂÄã Roadrunner Âç°ÈÄöÊïò‰∫ãÁöÑÂØ¶È©óÊÄßÂèÉÊï∏ÂåñË™ûÊñôÂ∫´‰∏≠ÁöÑÂ∞çÊ≤ñÔºå‰∏¶ËΩâÈåÑÊàêÊñáÂ≠óÔºàGalati Âíå BrennanÔºå2010 Âπ¥Ôºâ„ÄÇÊàëÂÄëÂª∫Á´ã‰∫ÜÁî±‰∫∫È°ûÁ∑®Á¢ºÂô®Ë®ªÈáãÁöÑÂ∞çÊ≤ñÈªÉÈáëÊ®ôÊ∫ñÔºàRoadrunner-Hedge Ë™ûÊñôÂ∫´ÔºâÔºå‰∏¶ÊØîËºÉ‰∫Ü‰∏âÁ®ÆÂü∫Êñº LLM ÁöÑÂ∞çÊ≤ñÂÅµÊ∏¨ÊñπÊ≥ïÔºöÂæÆË™ø BERTÔºå‰ª•Âèä‰ΩøÁî® GPT-4o Âíå LLaMA-3 ÈÄ≤Ë°åÈõ∂Ê¨°ÂíåÂ∞ëÊ¨°ÊèêÁ§∫„ÄÇË°®ÁèæÊúÄ‰Ω≥ÁöÑÊñπÊ≥ïÊòØÂæÆË™øÂæåÁöÑ BERT Ê®°ÂûãÔºåÂÖ∂Ê¨°ÊòØÂ∞ëÊ¨°ÊèêÁ§∫ÁöÑ GPT-4o„ÄÇÂú®Â∞çË°®ÁèæÊúÄ‰Ω≥ÁöÑÊñπÊ≥ïÈÄ≤Ë°åÈåØË™§ÂàÜÊûêÂæåÔºåÊàëÂÄë‰ΩøÁî® LLM-in-the-Loop ÊñπÊ≥ï‰æÜÊîπÈÄ≤ÈªÉÈáëÊ®ôÊ∫ñÁ∑®Á¢ºÔºå‰∏¶Âº∑Ë™øÂ∞çÊ≤ñÂú®Ë™ûË®ÄÂ≠∏‰∏äÊúâË∂£ÁöÑÊñπÈù¢‰∏≠Ê®°Á®úÂÖ©ÂèØÁöÑÊÉÖÊ≥ÅÔºåÈÄôÂ∞áÊåáÂ∞éÊú™‰æÜÁöÑÁ†îÁ©∂„ÄÇÈÄôÊòØÊàëÂÄëÁ†îÁ©∂Ë®àÁï´ÁöÑÁ¨¨‰∏ÄÊ≠•ÔºåÁõÆÁöÑÊòØË®ìÁ∑¥ LLM Âú®Â∞çË©±‰∏≠ÈÅ©Áï∂Âú∞‰∏îÊúâÊÑèÁæ©Âú∞Ëß£ÈáãÂíåÁî¢ÁîüÈôÑÂ∏∂‰ø°Ëôü„ÄÇ</paragraph>

##### **Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters**
2408.03314v1 by Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar

Enabling LLMs to improve their outputs by using more test-time computation is
a critical step towards building generally self-improving agents that can
operate on open-ended natural language. In this paper, we study the scaling of
inference-time computation in LLMs, with a focus on answering the question: if
an LLM is allowed to use a fixed but non-trivial amount of inference-time
compute, how much can it improve its performance on a challenging prompt?
Answering this question has implications not only on the achievable performance
of LLMs, but also on the future of LLM pretraining and how one should tradeoff
inference-time and pre-training compute. Despite its importance, little
research attempted to understand the scaling behaviors of various test-time
inference methods. Moreover, current work largely provides negative results for
a number of these strategies. In this work, we analyze two primary mechanisms
to scale test-time computation: (1) searching against dense, process-based
verifier reward models; and (2) updating the model's distribution over a
response adaptively, given the prompt at test time. We find that in both cases,
the effectiveness of different approaches to scaling test-time compute
critically varies depending on the difficulty of the prompt. This observation
motivates applying a "compute-optimal" scaling strategy, which acts to most
effectively allocate test-time compute adaptively per prompt. Using this
compute-optimal strategy, we can improve the efficiency of test-time compute
scaling by more than 4x compared to a best-of-N baseline. Additionally, in a
FLOPs-matched evaluation, we find that on problems where a smaller base model
attains somewhat non-trivial success rates, test-time compute can be used to
outperform a 14x larger model.

ÊëòË¶ÅÔºöËÆì LLM ËÉΩÂ§†ÈÄèÈÅé‰ΩøÁî®Êõ¥Â§öÊ∏¨Ë©¶ÊôÇÈñìÈÅãÁÆó‰æÜÊîπÂñÑÂÖ∂Áî¢Âá∫ÔºåÊòØÂª∫Á´ã‰∏ÄËà¨Ëá™ÊîπÂñÑ‰ª£ÁêÜÁ®ãÂºèÁöÑÈáçË¶ÅÊ≠•È©üÔºåË©≤‰ª£ÁêÜÁ®ãÂºèÂèØ‰ª•Âú®ÈñãÊîæÂºèËá™ÁÑ∂Ë™ûË®Ä‰∏≠ÈÅã‰Ωú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÁ†îÁ©∂‰∫Ü LLM ‰∏≠Êé®Ë´ñÊôÇÈñìÈÅãÁÆóÁöÑÊì¥ÂÖÖÔºåÈáçÈªûÂú®ÂõûÁ≠î‰∏ãÂàóÂïèÈ°åÔºöÂ¶ÇÊûúÂÖÅË®± LLM ‰ΩøÁî®Âõ∫ÂÆö‰ΩÜÈùûÁë£Á¢éÁöÑÊé®Ë´ñÊôÇÈñìÈÅãÁÆóÈáèÔºåÂÆÉÂèØ‰ª•Âú®ÂÖ∑ÊåëÊà∞ÊÄßÁöÑÊèêÁ§∫‰∏≠ÊîπÂñÑÂ§öÂ∞ëÊïàËÉΩÔºüÂõûÁ≠îÈÄôÂÄãÂïèÈ°å‰∏çÂÉÖÂ∞ç LLM ÂèØÈÅîÂà∞ÁöÑÊïàËÉΩÊúâÂΩ±ÈüøÔºå‰πüÂ∞ç LLM È†êË®ìÁ∑¥ÁöÑÊú™‰æÜ‰ª•ÂèäÂ¶Ç‰ΩïÊ¨äË°°Êé®Ë´ñÊôÇÈñìËàáÈ†êË®ìÁ∑¥ÈÅãÁÆóÁî¢ÁîüÂΩ±Èüø„ÄÇÂÑòÁÆ°ÂÖ∂ÈáçË¶ÅÊÄßÔºå‰ΩÜÂæàÂ∞ëÊúâÁ†îÁ©∂ÂòóË©¶‰∫ÜËß£ÂêÑÁ®ÆÊ∏¨Ë©¶ÊôÇÈñìÊé®Ë´ñÊñπÊ≥ïÁöÑÊì¥ÂÖÖË°åÁÇ∫„ÄÇÊ≠§Â§ñÔºåÁõÆÂâçÁöÑÂ∑•‰ΩúÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÁÇ∫Ë®±Â§öÈÄô‰∫õÁ≠ñÁï•Êèê‰æõ‰∫ÜË≤†Èù¢ÁµêÊûú„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂàÜÊûê‰∫ÜÂÖ©Á®ÆÊì¥ÂÖÖÊ∏¨Ë©¶ÊôÇÈñìÈÅãÁÆóÁöÑ‰∏ªË¶ÅÊ©üÂà∂Ôºö(1) ÈáùÂ∞çÂØÜÈõÜÁöÑ„ÄÅÂü∫ÊñºÁ®ãÂ∫èÁöÑÈ©óË≠âÂô®ÁçéÂãµÊ®°ÂûãÈÄ≤Ë°åÊêúÂ∞ãÔºõ‰ª•Âèä (2) Ê†πÊìöÊ∏¨Ë©¶ÊôÇÈñìÁöÑÊèêÁ§∫ÔºåËá™ÈÅ©ÊáâÂú∞Êõ¥Êñ∞Ê®°ÂûãÂú®ÂõûÊáâ‰∏äÁöÑÂàÜ‰Ωà„ÄÇÊàëÂÄëÁôºÁèæÔºåÂú®ÈÄôÂÖ©Á®ÆÊÉÖÊ≥Å‰∏ãÔºåÊì¥ÂÖÖÊ∏¨Ë©¶ÊôÇÈñìÈÅãÁÆóÁöÑ‰∏çÂêåÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊúÉÊ†πÊìöÊèêÁ§∫ÁöÑÈõ£Â∫¶ËÄåÊúâÈ°ØËëóÂ∑ÆÁï∞„ÄÇÈÄôÂÄãËßÄÂØüÁµêÊûú‰øÉ‰ΩøÊàëÂÄëÊé°Áî®„ÄåÈÅãÁÆóÊúÄ‰Ω≥Âåñ„ÄçÊì¥ÂÖÖÁ≠ñÁï•ÔºåË©≤Á≠ñÁï•ÁöÑ‰ΩúÁî®ÊòØÈáùÂ∞çÊØèÂÄãÊèêÁ§∫Ëá™ÈÅ©ÊáâÂú∞ÊúÄÊúâÊïàÂú∞ÈÖçÁΩÆÊ∏¨Ë©¶ÊôÇÈñìÈÅãÁÆó„ÄÇ‰ΩøÁî®ÈÄôÂÄãÈÅãÁÆóÊúÄ‰Ω≥ÂåñÁ≠ñÁï•ÔºåËàáÊúÄ‰Ω≥ N Âü∫Ê∫ñÁ∑öÁõ∏ÊØîÔºåÊàëÂÄëÂèØ‰ª•Â∞áÊ∏¨Ë©¶ÊôÇÈñìÈÅãÁÆóÊì¥ÂÖÖÁöÑÊïàÁéáÊèêÈ´ò 4 ÂÄç‰ª•‰∏ä„ÄÇÊ≠§Â§ñÔºåÂú® FLOPs ÂåπÈÖçÁöÑË©ï‰º∞‰∏≠ÔºåÊàëÂÄëÁôºÁèæÂ∞çÊñºËºÉÂ∞èÁöÑÂü∫Á§éÊ®°ÂûãÈÅîÂà∞Áõ∏Áï∂‰∏çÂπ≥Âá°ÁöÑÊàêÂäüÁéáÁöÑÂïèÈ°åÔºåÊ∏¨Ë©¶ÊôÇÈñìÈÅãÁÆóÂèØÁî®ÊñºÂÑ™ÊñºÂ§ß 14 ÂÄçÁöÑÊ®°Âûã„ÄÇ

##### **KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models**
2408.03297v1 by Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu Chu, Junfeng Zhao, Yasha Wang

By integrating external knowledge, Retrieval-Augmented Generation (RAG) has
become an effective strategy for mitigating the hallucination problems that
large language models (LLMs) encounter when dealing with knowledge-intensive
tasks. However, in the process of integrating external non-parametric
supporting evidence with internal parametric knowledge, inevitable knowledge
conflicts may arise, leading to confusion in the model's responses. To enhance
the knowledge selection of LLMs in various contexts, some research has focused
on refining their behavior patterns through instruction-tuning. Nonetheless,
due to the absence of explicit negative signals and comparative objectives,
models fine-tuned in this manner may still exhibit undesirable behaviors in the
intricate and realistic retrieval scenarios. To this end, we propose a
Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving
controllable knowledge selection in real retrieval scenarios. Concretely, we
explore and simulate error types across diverse context combinations and learn
how to avoid these negative signals through preference optimization methods.
Simultaneously, by adjusting the balance between response length and the
proportion of preference data representing different behavior patterns, we
enhance the adherence capabilities and noise robustness of LLMs in a balanced
manner. Experimental results show that KaPO outperforms previous methods for
handling knowledge conflicts by over 37%, while also exhibiting robust
generalization across various out-of-distribution datasets.

ÊëòË¶ÅÔºöÈÄèÈÅéÊï¥ÂêàÂ§ñÈÉ®Áü•Ë≠òÔºåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºàRAGÔºâÂ∑≤ÊàêÁÇ∫Ê∏õËºïÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ËôïÁêÜÁü•Ë≠òÂØÜÈõÜÂûã‰ªªÂãôÊôÇÊâÄÈÅáÂà∞ÁöÑÂπªË¶∫ÂïèÈ°åÁöÑÊúâÊïàÁ≠ñÁï•„ÄÇÁÑ∂ËÄåÔºåÂú®Â∞áÂ§ñÈÉ®ÈùûÂèÉÊï∏ÊîØÊåÅË≠âÊìöËàáÂÖßÈÉ®ÂèÉÊï∏Áü•Ë≠òÊï¥ÂêàÁöÑÈÅéÁ®ã‰∏≠ÔºåÂèØËÉΩÊúÉÁî¢Áîü‰∏çÂèØÈÅøÂÖçÁöÑÁü•Ë≠òË°ùÁ™ÅÔºåÂ∞éËá¥Ê®°ÂûãÂõûÊáâÊ∑∑Ê∑Ü„ÄÇÁÇ∫‰∫ÜÂ¢ûÂº∑ LLM Âú®ÂêÑÁ®ÆÊÉÖÂ¢É‰∏≠ÁöÑÁü•Ë≠òÈÅ∏ÊìáÔºå‰∏Ä‰∫õÁ†îÁ©∂Â∑≤Â∞àÊ≥®ÊñºÈÄèÈÅéÊåá‰ª§ÂæÆË™ø‰æÜÊîπÂñÑÂÖ∂Ë°åÁÇ∫Ê®°Âºè„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁî±ÊñºÁº∫‰πèÊòéÁ¢∫ÁöÑË≤†Èù¢Ë®äËôüÂíåÊØîËºÉÁõÆÊ®ôÔºå‰ª•ÈÄôÁ®ÆÊñπÂºèÂæÆË™øÁöÑÊ®°ÂûãÂú®Ë§áÈõú‰∏îÁúüÂØ¶ÁöÑÊ™¢Á¥¢ÊÉÖÂ¢É‰∏≠‰ªçÂèØËÉΩË°®ÁèæÂá∫‰∏çËâØË°åÁÇ∫„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ®±ÁÇ∫ KaPO ÁöÑÁü•Ë≠òÊÑüÁü•ÂÅèÂ•ΩÊúÄ‰Ω≥ÂåñÔºåÊó®Âú®ÂØ¶ÁèæÂØ¶ÈöõÊ™¢Á¥¢ÊÉÖÂ¢É‰∏≠ÁöÑÂèØÊéßÁü•Ë≠òÈÅ∏Êìá„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé¢Á¥¢‰∏¶Ê®°Êì¨ÂêÑÁ®ÆÊÉÖÂ¢ÉÁµÑÂêà‰∏≠ÁöÑÈåØË™§È°ûÂûãÔºå‰∏¶Â≠∏ÁøíÂ¶Ç‰ΩïÈÄèÈÅéÂÅèÂ•ΩÊúÄ‰Ω≥ÂåñÊñπÊ≥ï‰æÜÈÅøÂÖçÈÄô‰∫õË≤†Èù¢Ë®äËôü„ÄÇÂêåÊôÇÔºåÈÄèÈÅéË™øÊï¥ÂõûÊáâÈï∑Â∫¶Âíå‰ª£Ë°®‰∏çÂêåË°åÁÇ∫Ê®°ÂºèÁöÑÂÅèÂ•ΩË≥áÊñôÊØî‰æã‰πãÈñìÁöÑÂπ≥Ë°°ÔºåÊàëÂÄë‰ª•Âπ≥Ë°°ÁöÑÊñπÂºèÂ¢ûÂº∑‰∫Ü LLM ÁöÑÈÅµÂÆàËÉΩÂäõÂíåÊäóÂô™ÊÄß„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåKaPO Âú®ËôïÁêÜÁü•Ë≠òË°ùÁ™ÅÊñπÈù¢ÂÑ™ÊñºÂÖàÂâçÁöÑÂêÑÁ®ÆÊñπÊ≥ïÔºåÂÑ™Âã¢Ë∂ÖÈÅé 37%ÔºåÂêåÊôÇÂú®ÂêÑÁ®ÆÈùûÂàÜ‰ΩàË≥áÊñôÈõÜ‰∏ä‰πüË°®ÁèæÂá∫Âº∑Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ

##### **Static IR Drop Prediction with Attention U-Net and Saliency-Based Explainability**
2408.03292v1 by Lizi Zhang, Azadeh Davoodi

There has been significant recent progress to reduce the computational effort
of static IR drop analysis using neural networks, and modeling as an
image-to-image translation task. A crucial issue is the lack of sufficient data
from real industry designs to train these networks. Additionally, there is no
methodology to explain a high-drop pixel in a predicted IR drop image to its
specific root-causes. In this work, we first propose a U-Net neural network
model with attention gates which is specifically tailored to achieve fast and
accurate image-based static IR drop prediction. Attention gates allow selective
emphasis on relevant parts of the input data without supervision which is
desired because of the often sparse nature of the IR drop map. We propose a
two-phase training process which utilizes a mix of artificially-generated data
and a limited number of points from real designs. The results are, on-average,
18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner of
the ICCAD 2023 contest (and U-Net only) when tested on real designs. Second, we
propose a fast method using saliency maps which can explain a predicted IR drop
in terms of specific input pixels contributing the most to a drop. In our
experiments, we show the number of high IR drop pixels can be reduced
on-average by 18% by mimicking upsize of a tiny portion of PDN's resistive
edges.

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®Á•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ∞áÂÖ∂Âª∫Ê®°ÁÇ∫ÂΩ±ÂÉèËΩâÊèõ‰ªªÂãôÔºåÊúÄËøëÂú®Èôç‰ΩéÈùúÊÖã IR ÈôçÂ£ìÂàÜÊûêÁöÑË®àÁÆóÂ∑•‰Ωú‰∏äÂèñÂæóÈ°ØËëóÈÄ≤Â±ï„ÄÇ‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÊòØÁº∫‰πè‰æÜËá™ÁúüÂØ¶Áî¢Ê•≠Ë®≠Ë®àÁöÑË∂≥Â§†Ë≥áÊñô‰æÜË®ìÁ∑¥ÈÄô‰∫õÁ∂≤Ë∑Ø„ÄÇÊ≠§Â§ñÔºåÊ≤íÊúâÊñπÊ≥ïÂèØ‰ª•Ëß£ÈáãÈ†êÊ∏¨ IR ÈôçÂ£ìÂΩ±ÂÉè‰∏≠ÁöÑÈ´òÈôçÂ£ìÂÉèÁ¥†ÂèäÂÖ∂ÂÖ∑È´îÊ†πÊú¨ÂéüÂõ†„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÊèêÂá∫‰∏ÄÂÄãÂÖ∑ÊúâÊ≥®ÊÑèÂäõÈñòÁöÑ U-Net Á•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÔºåÂÆÉÂ∞àÈñÄË®≠Ë®àÁî®ÊñºÂØ¶ÁèæÂø´ÈÄü‰∏îÊ∫ñÁ¢∫ÁöÑÂü∫ÊñºÂΩ±ÂÉèÁöÑÈùúÊÖã IR ÈôçÂ£ìÈ†êÊ∏¨„ÄÇÊ≥®ÊÑèÂäõÈñòÂÖÅË®±ÈÅ∏ÊìáÊÄßÂú∞Âº∑Ë™øËº∏ÂÖ•Ë≥áÊñô‰∏≠Áõ∏ÈóúÈÉ®ÂàÜÔºåËÄåÁÑ°ÈúÄÁõ£Áù£ÔºåÈÄôÊòØÂõ†ÁÇ∫ IR ÈôçÂ£ìÂúñÈÄöÂ∏∏ÂÖ∑ÊúâÁ®ÄÁñèÁâπÊÄßÁöÑÁ∑£ÊïÖ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ©ÈöéÊÆµË®ìÁ∑¥ÈÅéÁ®ãÔºåÂÆÉÂà©Áî®‰∫∫Â∑•ÁîüÊàêË≥áÊñôÂíå‰æÜËá™ÁúüÂØ¶Ë®≠Ë®àÁöÑÊúâÈôêÊï∏ÈáèÁöÑÈªû„ÄÇËàá ICCAD 2023 Á´∂Ë≥ΩÁöÑÁç≤ÂãùËÄÖÔºàÂÉÖ U-NetÔºâÁõ∏ÊØîÔºåÂú®ÁúüÂØ¶Ë®≠Ë®à‰∏äÊ∏¨Ë©¶ÊôÇÔºåÁµêÊûúÂú® MAE ‰∏äÂπ≥ÂùáÊèêÈ´ò 18%Ôºà53%ÔºâÔºåÂú® F1 ÂàÜÊï∏‰∏äÊèêÈ´ò 14%Ôºà113%Ôºâ„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®È°ØËëóÊÄßÂúñÁöÑÂø´ÈÄüÊñπÊ≥ïÔºåÂÆÉÂèØ‰ª•Áî®Ë≤¢ÁçªÈôçÂ£ìÊúÄÂ§öÁöÑÁâπÂÆöËº∏ÂÖ•ÂÉèÁ¥†‰æÜËß£ÈáãÈ†êÊ∏¨ÁöÑ IR ÈôçÂ£ì„ÄÇÂú®ÊàëÂÄëÁöÑÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëË°®ÊòéÔºåÈÄöÈÅéÊ®°Êì¨Á∏ÆÂ∞è PDN ÈõªÈòªÈÇäÁ∑£ÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜÔºåÂèØ‰ª•Â∞áÈ´ò IR ÈôçÂ£ìÂÉèÁ¥†ÁöÑÊï∏ÈáèÂπ≥ÂùáÊ∏õÂ∞ë 18%„ÄÇ</paragraph>

##### **SARA: Singular-Value Based Adaptive Low-Rank Adaption**
2408.03290v1 by Jihao Gu, Shuai Chen, Zelin Wang, Yibo Zhang, Ping Gong

With the increasing number of parameters in large pre-trained models, LoRA as
a parameter-efficient fine-tuning(PEFT) method is widely used for not adding
inference overhead. The LoRA method assumes that weight changes during
fine-tuning can be approximated by low-rank matrices. However, the rank values
need to be manually verified to match different downstream tasks, and they
cannot accommodate the varying importance of different layers in the model. In
this work, we first analyze the relationship between the performance of
different layers and their ranks using SVD. Based on this, we design the
Singular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds
the rank during initialization by performing SVD on the pre-trained weights.
Additionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly
reduces the number of parameters by fine-tuning only multiple parallel sets of
singular values controlled by a router. Extensive experiments on various
complex tasks demonstrate the simplicity and parameter efficiency of our
methods. They can effectively and adaptively find the most suitable rank for
each layer of each model.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°Âûã‰∏≠ÂèÉÊï∏Êï∏ÈáèÁöÑÂ¢ûÂä†ÔºåLoRA ‰ΩúÁÇ∫‰∏ÄÁ®ÆÂèÉÊï∏È´òÊïàÁöÑÂæÆË™ø (PEFT) ÊñπÊ≥ïË¢´Âª£Ê≥õÁî®Êñº‰∏çÂ¢ûÂä†Êé®ÁêÜÈñãÈä∑„ÄÇLoRA ÊñπÊ≥ïÂÅáË®≠ÂæÆË™øÊúüÈñìÁöÑÊ¨äÈáçËÆäÂåñÂèØ‰ª•Áî®‰ΩéÁß©Áü©Èô£Ëøë‰ºº„ÄÇÁÑ∂ËÄåÔºåÁß©ÂÄºÈúÄË¶ÅÊâãÂãïÈ©óË≠â‰ª•ÂåπÈÖç‰∏çÂêåÁöÑ‰∏ãÊ∏∏‰ªªÂãôÔºå‰∏¶‰∏îÂÆÉÂÄëÁÑ°Ê≥ïÈÅ©ÊáâÊ®°Âûã‰∏≠‰∏çÂêåÂ±§ÁöÑ‰∏çÂêåÈáçË¶ÅÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖà‰ΩøÁî® SVD ÂàÜÊûê‰∏çÂêåÂ±§ÁöÑÊÄßËÉΩËàáÂÖ∂Áß©‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÂü∫ÊñºÊ≠§ÔºåÊàëÂÄëË®≠Ë®à‰∫ÜÂü∫ÊñºÂ•áÁï∞ÂÄºÁöÑËá™ÈÅ©Êáâ‰ΩéÁß©ÈÅ©Êáâ (SARA)ÔºåÂÆÉÈÄöÈÅéÂ∞çÈ†êË®ìÁ∑¥Ê¨äÈáçÂü∑Ë°å SVD ‰æÜËá™ÈÅ©ÊáâÂú∞ÊâæÂà∞ÂàùÂßãÂåñÊúüÈñìÁöÑÁß©„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Á¥¢‰∫ÜÊ∑∑Âêà SARA (Mo-SARA)ÔºåÂÆÉÈÄöÈÅéÂÉÖÂæÆË™øÁî±Ë∑ØÁî±Âô®ÊéßÂà∂ÁöÑÂ•áÁï∞ÂÄºÁöÑÂ§öÂÄã‰∏¶Ë°åÈõÜ‰æÜÈ°ØËëóÊ∏õÂ∞ëÂèÉÊï∏Êï∏Èáè„ÄÇÂú®ÂêÑÁ®ÆË§áÈõú‰ªªÂãô‰∏äÁöÑÂ§ßÈáèÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÁ∞°ÊΩîÊÄßÂíåÂèÉÊï∏ÊïàÁéá„ÄÇÂÆÉÂÄëÂèØ‰ª•ÊúâÊïà‰∏îËá™ÈÅ©ÊáâÂú∞ÊâæÂà∞ÊØèÂÄãÊ®°ÂûãÊØè‰∏ÄÂ±§ÊúÄÂêàÈÅ©ÁöÑÁß©„ÄÇ

##### **StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation**
2408.03281v2 by Boxi Cao, Mengjie Ren, Hongyu Lin, Xianpei Han, Feng Zhang, Junfeng Zhan, Le Sun

Evaluation is the baton for the development of large language models. Current
evaluations typically employ a single-item assessment paradigm for each atomic
test objective, which struggles to discern whether a model genuinely possesses
the required capabilities or merely memorizes/guesses the answers to specific
questions. To this end, we propose a novel evaluation framework referred to as
StructEval. Starting from an atomic test objective, StructEval deepens and
broadens the evaluation by conducting a structured assessment across multiple
cognitive levels and critical concepts, and therefore offers a comprehensive,
robust and consistent evaluation for LLMs. Experiments on three widely-used
benchmarks demonstrate that StructEval serves as a reliable tool for resisting
the risk of data contamination and reducing the interference of potential
biases, thereby providing more reliable and consistent conclusions regarding
model capabilities. Our framework also sheds light on the design of future
principled and trustworthy LLM evaluation protocols.

ÊëòË¶ÅÔºöË©ï‰º∞ÊòØÂ§ßË™ûË®ÄÊ®°ÂûãÁôºÂ±ïÁöÑÊåáÊ®ô„ÄÇÁõÆÂâçÁöÑË©ï‰º∞ÈÄöÂ∏∏Â∞çÊØèÂÄãÂéüÂ≠êÊ∏¨Ë©¶ÁõÆÊ®ôÊé°Áî®ÂñÆ‰∏ÄÈ†ÖÁõÆË©ï‰º∞ÁØÑ‰æãÔºåÈÄôÂæàÈõ£Ëæ®Âà•Ê®°ÂûãÊòØÂê¶ÁúüÊ≠£ÂÖ∑ÂÇôÊâÄÈúÄÁöÑÊäÄËÉΩÔºåÊàñÂÉÖÂÉÖÊòØË®òÊÜ∂/ÁåúÊ∏¨ÁâπÂÆöÂïèÈ°åÁöÑÁ≠îÊ°à„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ®±ÁÇ∫ StructEval ÁöÑÊñ∞Ë©ï‰º∞Êû∂Êßã„ÄÇÂæû‰∏ÄÂÄãÂéüÂ≠êÊ∏¨Ë©¶ÁõÆÊ®ôÈñãÂßãÔºåStructEval ÈÄèÈÅéÂú®Â§öÂÄãË™çÁü•Â±§Èù¢ÂíåÈóúÈçµÊ¶ÇÂøµ‰∏≠ÈÄ≤Ë°åÁµêÊßãÂåñË©ï‰º∞‰æÜÂä†Ê∑±ÂíåÊì¥Â±ïË©ï‰º∞ÔºåÂõ†Ê≠§ÁÇ∫ LLM Êèê‰æõÂÖ®Èù¢„ÄÅÁ©©ÂÅ•‰∏î‰∏ÄËá¥ÁöÑË©ï‰º∞„ÄÇÂú®‰∏âÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåStructEval ÂèØ‰ΩúÁÇ∫ÊäµÊäóË≥áÊñôÊ±°ÊüìÈ¢®Èö™ÂíåÊ∏õÂ∞ëÊΩõÂú®ÂÅèÂ∑ÆÂπ≤ÊìæÁöÑÂèØÈù†Â∑•ÂÖ∑ÔºåÂæûËÄåÂ∞çÊ®°ÂûãËÉΩÂäõÊèê‰æõÊõ¥ÂèØÈù†‰∏î‰∏ÄËá¥ÁöÑÁµêË´ñ„ÄÇÊàëÂÄëÁöÑÊû∂Êßã‰πüÁÇ∫Êú™‰æÜÂü∫ÊñºÂéüÂâá‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑ LLM Ë©ï‰º∞ÂçîÂÆöÁöÑË®≠Ë®àÊèê‰æõ‰∫ÜÂïüÁ§∫„ÄÇ

##### **Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments**
2408.03274v1 by Angie Boggust, Venkatesh Sivaraman, Yannick Assogba, Donghao Ren, Dominik Moritz, Fred Hohman

To deploy machine learning models on-device, practitioners use compression
algorithms to shrink and speed up models while maintaining their high-quality
output. A critical aspect of compression in practice is model comparison,
including tracking many compression experiments, identifying subtle changes in
model behavior, and negotiating complex accuracy-efficiency trade-offs.
However, existing compression tools poorly support comparison, leading to
tedious and, sometimes, incomplete analyses spread across disjoint tools. To
support real-world comparative workflows, we develop an interactive visual
system called Compress and Compare. Within a single interface, Compress and
Compare surfaces promising compression strategies by visualizing provenance
relationships between compressed models and reveals compression-induced
behavior changes by comparing models' predictions, weights, and activations. We
demonstrate how Compress and Compare supports common compression analysis tasks
through two case studies, debugging failed compression on generative language
models and identifying compression artifacts in image classification models. We
further evaluate Compress and Compare in a user study with eight compression
experts, illustrating its potential to provide structure to compression
workflows, help practitioners build intuition about compression, and encourage
thorough analysis of compression's effect on model behavior. Through these
evaluations, we identify compression-specific challenges that future visual
analytics tools should consider and Compress and Compare visualizations that
may generalize to broader model comparison tasks.

ÊëòË¶ÅÔºöÁÇ∫‰∫ÜÂú®Ë£ùÁΩÆ‰∏äÈÉ®ÁΩ≤Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÔºåÂØ¶ÂãôÂ∑•‰ΩúËÄÖÊúÉ‰ΩøÁî®Â£ìÁ∏ÆÊºîÁÆóÊ≥ï‰æÜÁ∏ÆÂ∞èÊ®°ÂûãÁöÑË¶èÊ®°‰∏¶Âä†Âø´Ê®°ÂûãÁöÑÈÄüÂ∫¶ÔºåÂêåÊôÇÁ∂≠ÊåÅÂÖ∂È´òÂìÅË≥™ÁöÑËº∏Âá∫„ÄÇÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÔºåÂ£ìÁ∏ÆÁöÑ‰∏ÄÂÄãÈáçË¶ÅÈù¢ÂêëÊòØÊ®°ÂûãÊØîËºÉÔºåÂåÖÊã¨ËøΩËπ§Ë®±Â§öÂ£ìÁ∏ÆÂØ¶È©ó„ÄÅÊâæÂá∫Ê®°ÂûãË°åÁÇ∫‰∏≠ÁöÑÁ¥∞ÂæÆËÆäÂåñÔºå‰ª•ÂèäÂçîÂïÜË§áÈõúÁöÑÊ∫ñÁ¢∫Â∫¶ËàáÊïàÁéáÊäòË°∑„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂ£ìÁ∏ÆÂ∑•ÂÖ∑Â∞çÊñºÊØîËºÉÁöÑÊîØÊåÅÂæàÂ∑ÆÔºåÂ∞éËá¥ÁπÅÁë£‰∏îÊúâÊôÇ‰∏çÂÆåÊï¥ÁöÑÂàÜÊûêÊï£Â∏ÉÂú®‰∏çÂêåÁöÑÂ∑•ÂÖ∑‰∏≠„ÄÇÁÇ∫‰∫ÜÊîØÊè¥ÁúüÂØ¶‰∏ñÁïåÁöÑÊØîËºÉÂ∑•‰ΩúÊµÅÁ®ãÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã‰∫íÂãïÂºèË¶ñË¶∫Á≥ªÁµ±ÔºåÁ®±ÁÇ∫„ÄåÂ£ìÁ∏ÆËàáÊØîËºÉ„Äç„ÄÇÂú®ÂñÆ‰∏Ä‰ªãÈù¢‰∏≠Ôºå„ÄåÂ£ìÁ∏ÆËàáÊØîËºÉ„ÄçÊúÉÈÄèÈÅéË¶ñË¶∫ÂåñÂ£ìÁ∏ÆÊ®°Âûã‰πãÈñìÁöÑ‰æÜÊ∫êÈóú‰øÇÔºåÊâæÂá∫ÊúâÂâçÊôØÁöÑÂ£ìÁ∏ÆÁ≠ñÁï•Ôºå‰∏¶ÈÄèÈÅéÊØîËºÉÊ®°ÂûãÁöÑÈ†êÊ∏¨„ÄÅÊ¨äÈáçÂíåÊøÄÂãµÔºåÊè≠Èú≤Â£ìÁ∏ÆÂºïÁôºÁöÑË°åÁÇ∫ËÆäÂåñ„ÄÇÊàëÂÄëÈÄèÈÅéÂÖ©ÂÄãÊ°à‰æãÁ†îÁ©∂Â±ïÁ§∫„ÄåÂ£ìÁ∏ÆËàáÊØîËºÉ„ÄçÂ¶Ç‰ΩïÊîØÊè¥Â∏∏Ë¶ãÁöÑÂ£ìÁ∏ÆÂàÜÊûê‰ªªÂãôÔºåÂåÖÊã¨ÂÅµÈåØÁîüÊàêË™ûË®ÄÊ®°Âûã‰∏≠Â§±ÊïóÁöÑÂ£ìÁ∏ÆÔºå‰ª•ÂèäÊâæÂá∫ÂΩ±ÂÉèÂàÜÈ°ûÊ®°Âûã‰∏≠ÁöÑÂ£ìÁ∏Æ‰∫∫Â∑•Ë£ΩÂìÅ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Âú®‰∏ÄÂÄã‰ΩøÁî®ËÄÖÁ†îÁ©∂‰∏≠Ë©ï‰º∞„ÄåÂ£ìÁ∏ÆËàáÊØîËºÉ„ÄçÔºåË©≤Á†îÁ©∂ÊúâÂÖ´‰ΩçÂ£ìÁ∏ÆÂ∞àÂÆ∂ÂèÉËàáÔºåË™™ÊòéÂÖ∂Êèê‰æõÁµêÊßãÁµ¶Â£ìÁ∏ÆÂ∑•‰ΩúÊµÅÁ®ã„ÄÅÂçîÂä©ÂØ¶ÂãôÂ∑•‰ΩúËÄÖÂª∫Á´ãÈóúÊñºÂ£ìÁ∏ÆÁöÑÁõ¥Ë¶∫Ôºå‰ª•ÂèäÈºìÂãµÂæπÂ∫ïÂàÜÊûêÂ£ìÁ∏ÆÂ∞çÊ®°ÂûãË°åÁÇ∫ÁöÑÂΩ±ÈüøÁöÑÊΩõÂäõ„ÄÇÈÄèÈÅéÈÄô‰∫õË©ï‰º∞ÔºåÊàëÂÄëÊâæÂá∫Êú™‰æÜÁöÑË¶ñË¶∫ÂàÜÊûêÂ∑•ÂÖ∑ÊáâËÄÉÈáèÁöÑÁâπÂÆöÊñºÂ£ìÁ∏ÆÁöÑÊåëÊà∞Ôºå‰ª•ÂèäÂèØËÉΩÊúÉÊé®Âª£Âà∞Êõ¥Âª£Ê≥õÁöÑÊ®°ÂûãÊØîËºÉ‰ªªÂãôÁöÑ„ÄåÂ£ìÁ∏ÆËàáÊØîËºÉ„ÄçË¶ñË¶∫Âåñ„ÄÇ

##### **Synthesizing Text-to-SQL Data from Weak and Strong LLMs**
2408.03256v1 by Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, Chang Zhou

The capability gap between open-source and closed-source large language
models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we
introduce a synthetic data approach that combines data produced by larger, more
powerful models (strong models) with error information data generated by
smaller, not well-aligned models (weak models). The method not only enhances
the domain generalization of text-to-SQL models but also explores the potential
of error data supervision through preference learning. Furthermore, we employ
the synthetic data approach for instruction tuning on open-source LLMs,
resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is
demonstrated through state-of-the-art results on the SPIDER and BIRD
benchmarks, bridging the performance gap between open-source models and methods
prompted by closed-source models.

ÊëòË¶ÅÔºöÈñãÊîæÂéüÂßãÁ¢ºËàáÈñâÊ∫êÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰πãÈñìÁöÑËÉΩÂäõÂ∑ÆË∑ùÔºå‰ªçÁÑ∂ÊòØÊñáÂ≠óËΩâ SQL ‰ªªÂãô‰∏≠ÁöÑ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÂêàÊàêË≥áÊñôÊñπÊ≥ïÔºåÂÆÉÁµêÂêà‰∫ÜÁî±Êõ¥Â§ß„ÄÅÊõ¥Âº∑Â§ßÁöÑÊ®°Âûã (Âº∑Ê®°Âûã) ÊâÄÁî¢ÁîüÁöÑË≥áÊñôÔºå‰ª•ÂèäÁî±ËºÉÂ∞è„ÄÅÂ∞çÈΩä‰∏ç‰Ω≥ÁöÑÊ®°Âûã (Âº±Ê®°Âûã) ÊâÄÁî¢ÁîüÁöÑÈåØË™§Ë≥áË®äË≥áÊñô„ÄÇÊ≠§ÊñπÊ≥ï‰∏çÂÉÖÂ¢ûÂº∑‰∫ÜÊñáÂ≠óËΩâ SQL Ê®°ÂûãÁöÑÈ†òÂüüÊ¶ÇÊã¨ÊÄßÔºåÈÇÑÈÄèÈÅéÂÅèÂ•ΩÂ≠∏ÁøíÊé¢Á¥¢‰∫ÜÈåØË™§Ë≥áÊñôÁõ£Áù£ÁöÑÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞áÂêàÊàêË≥áÊñôÊñπÊ≥ïÁî®ÊñºÈñãÊîæÂéüÂßãÁ¢º LLM ‰∏äÁöÑÊåá‰ª§Ë™øÊï¥ÔºåÁî¢Áîü‰∫Ü SENSEÔºå‰∏ÄÁ®ÆÂ∞àÈñÄÁöÑÊñáÂ≠óËΩâ SQL Ê®°Âûã„ÄÇSENSE ÁöÑÊúâÊïàÊÄßÈÄèÈÅé SPIDER Âíå BIRD Âü∫Ê∫ñÊ∏¨Ë©¶ÁöÑÊúÄÊñ∞ÁµêÊûúÂæóÂà∞Ë≠âÊòéÔºåÁ∏ÆÂ∞è‰∫ÜÈñãÊîæÂéüÂßãÁ¢ºÊ®°ÂûãËàáÈñâÊ∫êÊ®°ÂûãÊèêÁ§∫ÊñπÊ≥ï‰πãÈñìÁöÑÊïàËÉΩÂ∑ÆË∑ù„ÄÇ

##### **Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons**
2408.03247v1 by Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, Daniel Dajun Zeng

In this paper, we investigate whether Large Language Models (LLMs) actively
recall or retrieve their internal repositories of factual knowledge when faced
with reasoning tasks. Through an analysis of LLMs' internal factual recall at
each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness
the critical factual associations under certain circumstances. Instead, they
tend to opt for alternative, shortcut-like pathways to answer reasoning
questions. By manually manipulating the recall process of parametric knowledge
in LLMs, we demonstrate that enhancing this recall process directly improves
reasoning performance whereas suppressing it leads to notable degradation.
Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a
powerful technique for addressing complex reasoning tasks. Our findings
indicate that CoT can intensify the recall of factual knowledge by encouraging
LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how
contextual conflicts affect the retrieval of facts during the reasoning process
to gain a comprehensive understanding of the factual recall behaviors of LLMs.
Code and data will be available soon.

ÊëòË¶ÅÔºö<paragraph>Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Èù¢Â∞çÊé®ÁêÜ‰ªªÂãôÊôÇÔºåÊòØÂê¶ÊúÉ‰∏ªÂãïÂõûÊÜ∂ÊàñÊ™¢Á¥¢ÂÖ∂ÂÖßÈÉ®‰∫ãÂØ¶Áü•Ë≠òÂ∫´„ÄÇÈÄèÈÅéÁü•Ë≠òÁ•ûÁ∂ìÂÖÉÂàÜÊûê LLM Âú®ÊØèÂÄãÊé®ÁêÜÊ≠•È©ü‰∏≠ÁöÑÂÖßÈÉ®‰∫ãÂØ¶ÂõûÊÜ∂ÔºåÊàëÂÄëÁôºÁèæ LLM Âú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÁÑ°Ê≥ïÂà©Áî®ÈóúÈçµÁöÑ‰∫ãÂØ¶ÈóúËÅØ„ÄÇÁõ∏ÂèçÂú∞Ôºå‰ªñÂÄëÂÇæÂêëÊñºÈÅ∏ÊìáÊõø‰ª£ÁöÑÊç∑ÂæëÈÄîÂæë‰æÜÂõûÁ≠îÊé®ÁêÜÂïèÈ°å„ÄÇÈÄèÈÅéÊâãÂãïÊìç‰Ωú LLM ‰∏≠ÂèÉÊï∏ÂåñÁü•Ë≠òÁöÑÂõûÊÜ∂ÈÅéÁ®ãÔºåÊàëÂÄëË≠âÊòé‰∫ÜÂ¢ûÂº∑Ê≠§ÂõûÊÜ∂ÈÅéÁ®ãÊúÉÁõ¥Êé•ÊîπÂñÑÊé®ÁêÜË°®ÁèæÔºåËÄåÊäëÂà∂ÂÆÉÂâáÊúÉÂ∞éËá¥È°ØËëóÁöÑÈÄÄÂåñ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË©ï‰º∞‰∫ÜÊÄùÊÉ≥Èèà (CoT) ÊèêÁ§∫ÁöÑÂΩ±ÈüøÔºåÈÄôÊòØ‰∏ÄÁ®ÆËß£Ê±∫Ë§áÈõúÊé®ÁêÜ‰ªªÂãôÁöÑÂº∑Â§ßÊäÄË°ì„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåCoT ÂèØ‰ª•ÈÄèÈÅéÈºìÂãµ LLM Âæû‰∫ãÊúâÂ∫è‰∏îÂèØÈù†ÁöÑÊé®ÁêÜ‰æÜÂä†Âº∑Â∞ç‰∫ãÂØ¶Áü•Ë≠òÁöÑÂõûÊÜ∂„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜËÉåÊôØË°ùÁ™ÅÂ¶Ç‰ΩïÂΩ±ÈüøÊé®ÁêÜÈÅéÁ®ã‰∏≠‰∫ãÂØ¶ÁöÑÊ™¢Á¥¢Ôºå‰ª•ÂÖ®Èù¢‰∫ÜËß£ LLM ÁöÑ‰∫ãÂØ¶ÂõûÊÜ∂Ë°åÁÇ∫„ÄÇ‰ª£Á¢ºÂíåÊï∏ÊìöÂ∞áÂæàÂø´Êèê‰æõ„ÄÇ</paragraph>

##### **Making Long-Context Language Models Better Multi-Hop Reasoners**
2408.03246v1 by Yanyang Li, Shuo Liang, Michael R. Lyu, Liwei Wang

Recent advancements in long-context modeling have enhanced language models
(LMs) for complex tasks across multiple NLP applications. Despite this
progress, we find that these models struggle with multi-hop reasoning and
exhibit decreased performance in the presence of noisy contexts. In this paper,
we introduce Reasoning with Attributions, a novel approach that prompts LMs to
supply attributions for each assertion during their reasoning. We validate our
approach through experiments on three multi-hop datasets, employing both
proprietary and open-source models, and demonstrate its efficacy and
resilience. Furthermore, we explore methods to augment reasoning capabilities
via fine-tuning and offer an attribution-annotated dataset and a specialized
training strategy. Our fine-tuned model achieves competitive performance on
multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as
ChatGPT and Claude-instant.

ÊëòË¶ÅÔºöÊúÄËøëÂú®Èï∑ÊñáÊú¨Âª∫Ê®°ÊñπÈù¢ÁöÑÈÄ≤Â±ïÂ¢ûÂº∑‰∫ÜË™ûË®ÄÊ®°Âûã (LM) Âú®Â§öÂÄã NLP ÊáâÁî®‰∏≠ËôïÁêÜË§áÈõú‰ªªÂãôÁöÑËÉΩÂäõ„ÄÇÂÑòÁÆ°ÊúâÈÄô‰∫õÈÄ≤Â±ïÔºåÊàëÂÄëÁôºÁèæÈÄô‰∫õÊ®°ÂûãÂú®Â§öË∑≥Êé®ÁêÜÊñπÈù¢ÈÅáÂà∞Âõ∞Èõ£Ôºå‰∏¶‰∏îÂú®Â≠òÂú®ÂòàÈõúÁí∞Â¢ÉÊôÇË°®Áèæ‰∏ãÈôç„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÊ≠∏Âõ†Êé®ÁêÜÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÂÆÉÊèêÁ§∫ LM Âú®Êé®ÁêÜÈÅéÁ®ã‰∏≠ÁÇ∫ÊØèÂÄãÊñ∑Ë®ÄÊèê‰æõÊ≠∏Âõ†„ÄÇÊàëÂÄëÈÄöÈÅéÂú®‰∏âÂÄãÂ§öË∑≥Êï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°åÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÂêåÊôÇÊé°Áî®Â∞àÊúâÊ®°ÂûãÂíåÈñãÊ∫êÊ®°ÂûãÔºå‰∏¶Â±ïÁ§∫‰∫ÜÂÖ∂ÊúâÊïàÊÄßÂíåÂΩàÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Á¥¢‰∫ÜÈÄöÈÅéÂæÆË™ø‰æÜÊì¥ÂÖÖÊé®ÁêÜËÉΩÂäõÁöÑÊñπÊ≥ïÔºå‰∏¶Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊ≠∏Âõ†Ë®ªÈáãÊï∏ÊìöÈõÜÂíå‰∏ÄÂÄãÂ∞àÈñÄÁöÑË®ìÁ∑¥Á≠ñÁï•„ÄÇÊàëÂÄëÂæÆË™øÂæåÁöÑÊ®°ÂûãÂú®Â§öË∑≥Êé®ÁêÜÂü∫Ê∫ñ‰∏äÂèñÂæó‰∫ÜÊúâÁ´∂Áà≠ÂäõÁöÑË°®ÁèæÔºåËàáÂ∞àÊúâ LMÔºà‰æãÂ¶Ç ChatGPT Âíå Claude-instantÔºâÂØÜÂàáÁõ∏Èóú„ÄÇ

##### **Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery**
2408.03208v1 by Jialang Xu, Jiacheng Wang, Lequan Yu, Danail Stoyanov, Yueming Jin, Evangelos B. Mazomenos

Personalized federated learning (PFL) for surgical instrument segmentation
(SIS) is a promising approach. It enables multiple clinical sites to
collaboratively train a series of models in privacy, with each model tailored
to the individual distribution of each site. Existing PFL methods rarely
consider the personalization of multi-headed self-attention, and do not account
for appearance diversity and instrument shape similarity, both inherent in
surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait
priors for SIS, incorporating global-personalized disentanglement (GPD),
appearance-regulation personalized enhancement (APE), and shape-similarity
global enhancement (SGE), to boost SIS performance in each site. GPD represents
the first attempt at head-wise assignment for multi-headed self-attention
personalization. To preserve the unique appearance representation of each site
and gradually leverage the inter-site difference, APE introduces appearance
regulation and provides customized layer-wise aggregation solutions via
hypernetworks for each site's personalized parameters. The mutual shape
information of instruments is maintained and shared via SGE, which enhances the
cross-style shape consistency on the image level and computes the
shape-similarity contribution of each site on the prediction level for updating
the global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%
Dice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding
code and models will be released at https://github.com/wzjialang/PFedSIS.

ÊëòË¶ÅÔºö<paragraph>ÈáùÂ∞çÊâãË°ìÂô®Ê¢∞ÂàÜÂâ≤ÔºàSISÔºâÁöÑÂÄã‰∫∫ÂåñËÅØÈÇ¶Â≠∏ÁøíÔºàPFLÔºâÊòØ‰∏ÄÁ®ÆÊúâÂâçÊôØÁöÑÊñπÊ≥ï„ÄÇÂÆÉËÆìÂ§öÂÄãËá®Â∫äÂú∞ÈªûËÉΩÂ§†Âú®Èö±ÁßÅÁöÑÊ¢ù‰ª∂‰∏ãÂÖ±ÂêåË®ìÁ∑¥‰∏ÄÁ≥ªÂàóÊ®°ÂûãÔºåÊØèÂÄãÊ®°ÂûãÈÉΩÊ†πÊìöÊØèÂÄãÂú∞ÈªûÁöÑÂÄãÂà•ÂàÜ‰ΩàÈÄ≤Ë°åË™øÊï¥„ÄÇÁèæÊúâÁöÑ PFL ÊñπÊ≥ïÂæàÂ∞ëËÄÉÊÖÆÂ§öÈ†≠Ëá™ÊàëÊ≥®ÊÑèÂäõÁöÑÂÄã‰∫∫ÂåñÔºåËÄå‰∏îÊ≤íÊúâËÄÉÊÖÆÂ§ñËßÄÁöÑÂ§öÊ®£ÊÄßÂíåÂô®Ê¢∞ÂΩ¢ÁãÄÁöÑÁõ∏‰ººÊÄßÔºåÈÄôÂÖ©ËÄÖÈÉΩÂ≠òÂú®ÊñºÊâãË°ìÂ†¥ÊôØ‰∏≠„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü PFedSISÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂÖ∑ÊúâË¶ñË¶∫ÁâπÂæµÂÖàÈ©óÁöÑ SIS ÁöÑÊñ∞Âûã PFL ÊñπÊ≥ïÔºåÂÆÉÁµêÂêà‰∫ÜÂÖ®Â±ÄÂÄãÊÄßÂåñËß£Á≥æÁ∫èÔºàGPDÔºâ„ÄÅÂ§ñËßÄË™øÁØÄÂÄãÊÄßÂåñÂ¢ûÂº∑ÔºàAPEÔºâÂíåÂΩ¢ÁãÄÁõ∏‰ººÊÄßÂÖ®Â±ÄÂ¢ûÂº∑ÔºàSGEÔºâÔºå‰ª•ÊèêÂçáÊØèÂÄãÂú∞ÈªûÁöÑ SIS ÊïàËÉΩ„ÄÇGPD ‰ª£Ë°®‰∫ÜÈáùÂ∞çÂ§öÈ†≠Ëá™ÊàëÊ≥®ÊÑèÂäõÂÄãÊÄßÂåñÈÄ≤Ë°åÈ†≠ÈÉ®ÂàÜÈÖçÁöÑÈ¶ñÊ¨°ÂòóË©¶„ÄÇÁÇ∫‰∫Ü‰øùÁïôÊØèÂÄãÂú∞ÈªûÁöÑÁç®ÁâπÂ§ñËßÄË°®Á§∫‰∏¶ÈÄêÊº∏Âà©Áî®Âú∞ÈªûÈñìÁöÑÂ∑ÆÁï∞ÔºåAPE ÂºïÂÖ•‰∫ÜÂ§ñËßÄË™øÁØÄÔºå‰∏¶ÈÄèÈÅéË∂ÖÁ∂≤Ë∑ØÁÇ∫ÊØèÂÄãÂú∞ÈªûÁöÑÂÄãÊÄßÂåñÂèÉÊï∏Êèê‰æõËá™Ë®ÇÁöÑÈÄêÂ±§ËÅöÂêàËß£Ê±∫ÊñπÊ°à„ÄÇÂô®Ê¢∞ÁöÑÁõ∏‰∫íÂΩ¢ÁãÄË≥áË®äÈÄèÈÅé SGE ÈÄ≤Ë°åÁ∂≠Ë≠∑ÂíåÂÖ±‰∫´ÔºåÈÄôÂ¢ûÂº∑‰∫ÜÂΩ±ÂÉèÂ±§Á¥ö‰∏äÁöÑË∑®È¢®Ê†ºÂΩ¢ÁãÄ‰∏ÄËá¥ÊÄßÔºå‰∏¶Ë®àÁÆóÊØèÂÄãÂú∞ÈªûÂú®È†êÊ∏¨Â±§Á¥ö‰∏äÁöÑÂΩ¢ÁãÄÁõ∏‰ººÊÄßË≤¢ÁçªÔºå‰ª•Êõ¥Êñ∞ÂÖ®Â±ÄÂèÉÊï∏„ÄÇPFedSIS Âú®È™∞Â≠êÁ≥ªÊï∏‰∏äÂÑ™ÊñºÁèæÊúâÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÔºåÂàÜÂà•ÊèêÂçá‰∫Ü +1.51%„ÄÅIoU ÊèêÂçá‰∫Ü +2.11%„ÄÅASSD Èôç‰Ωé‰∫Ü -2.79„ÄÅHD95 ÊïàËÉΩÊèêÂçá‰∫Ü -15.55„ÄÇÂ∞çÊáâÁöÑÁ®ãÂºèÁ¢ºÂíåÊ®°ÂûãÂ∞áÂú® https://github.com/wzjialang/PFedSIS ‰∏äÁôºÂ∏É„ÄÇ</paragraph>

##### **A Debiased Nearest Neighbors Framework for Multi-Label Text Classification**
2408.03202v1 by Zifeng Cheng, Zhiwei Jiang, Yafeng Yin, Zhaoling Chen, Cong Wang, Shiping Ge, Qiguo Huang, Qing Gu

Multi-Label Text Classification (MLTC) is a practical yet challenging task
that involves assigning multiple non-exclusive labels to each document.
Previous studies primarily focus on capturing label correlations to assist
label prediction by introducing special labeling schemes, designing specific
model structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor
($k$NN) framework has shown promise by retrieving labeled samples as references
to mine label co-occurrence information in the embedding space. However, two
critical biases, namely embedding alignment bias and confidence estimation
bias, are often overlooked, adversely affecting prediction performance. In this
paper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC,
specifically designed to mitigate these biases. To address embedding alignment
bias, we propose a debiased contrastive learning strategy, enhancing neighbor
consistency on label co-occurrence. For confidence estimation bias, we present
a debiased confidence estimation strategy, improving the adaptive combination
of predictions from $k$NN and inductive binary classifications. Extensive
experiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2,
Amazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method.
Besides, our method does not introduce any extra parameters.

ÊëòË¶ÅÔºöÂ§öÊ®ôÁ±§ÊñáÂ≠óÂàÜÈ°û (MLTC) ÊòØ‰∏ÄÈ†ÖÂØ¶Áî®ÂçªÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãôÔºåÊ∂âÂèäÂ∞áÂ§öÂÄãÈùûÁç®‰ΩîÊ®ôÁ±§ÊåáÊ¥æÁµ¶ÊØèÂÄãÊñá‰ª∂„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÊì∑ÂèñÊ®ôÁ±§ÈóúËÅØÊÄßÔºåËóâÁî±ÂºïÂÖ•ÁâπÊÆäÊ®ôÁ±§Êû∂Êßã„ÄÅË®≠Ë®àÁâπÂÆöÊ®°ÂûãÁµêÊßãÊàñÊñ∞Â¢ûËºîÂä©‰ªªÂãô‰æÜÂçîÂä©Ê®ôÁ±§È†êÊ∏¨„ÄÇÊúÄËøëÔºåk ÊúÄËøëÈÑ∞ ($k$NN) Êû∂ÊßãÈÄèÈÅéÊì∑ÂèñÊ®ôÁ±§Ê®£Êú¨‰ΩúÁÇ∫ÂèÉËÄÉÔºåÂú®ÂµåÂÖ•Á©∫Èñì‰∏≠ÊåñÊéòÊ®ôÁ±§ÂÖ±ÁèæË≥áË®äÔºåÂ±ïÁèæ‰∫ÜÂâçÊôØ„ÄÇÁÑ∂ËÄåÔºåÂÖ©ÂÄãÈóúÈçµÂÅèÂ∑ÆÔºåÂç≥ÂµåÂÖ•Â∞çÈΩäÂÅèÂ∑ÆÂíå‰ø°ÂøÉ‰º∞Ë®àÂÅèÂ∑ÆÔºåÁ∂ìÂ∏∏Ë¢´ÂøΩÁï•ÔºåÂ∞çÈ†êÊ∏¨ÊïàËÉΩÁî¢ÁîüË≤†Èù¢ÂΩ±Èüø„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÁî®Êñº MLTC ÁöÑÂéªÂÅèÂ∑ÆÊúÄËøëÈÑ∞ (DENN) Êû∂ÊßãÔºåÁâπÂà•Ë®≠Ë®àÁî®ÊñºÊ∏õËºïÈÄô‰∫õÂÅèÂ∑Æ„ÄÇÁÇ∫‰∫ÜËôïÁêÜÂµåÂÖ•Â∞çÈΩäÂÅèÂ∑ÆÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂéªÂÅèÂ∑ÆÂ∞çÊØîÂ≠∏ÁøíÁ≠ñÁï•ÔºåÂ¢ûÂº∑ÈÑ∞Â±ÖÂú®Ê®ôÁ±§ÂÖ±Áèæ‰∏äÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂ∞çÊñº‰ø°ÂøÉ‰º∞Ë®àÂÅèÂ∑ÆÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂéªÂÅèÂ∑Æ‰ø°ÂøÉ‰º∞Ë®àÁ≠ñÁï•ÔºåÊîπÂñÑ‰∫Ü‰æÜËá™ $k$NN ÂíåÊ≠∏Á¥ç‰∫åÂÖÉÂàÜÈ°ûÁöÑÈ†êÊ∏¨ÁöÑÈÅ©ÊáâÊÄßÁµÑÂêà„ÄÇÂú®ÂõõÂÄãÂÖ¨ÈñãÂü∫Ê∫ñË≥áÊñôÈõÜÔºàÂç≥ AAPD„ÄÅRCV1-V2„ÄÅAmazon-531 Âíå EUR-LEX57KÔºâ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÊ≤íÊúâÂºïÂÖ•‰ªª‰ΩïÈ°çÂ§ñÂèÉÊï∏„ÄÇ

##### **Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors**
2408.03200v2 by Kunkun Hao, Yonggang Luo, Wen Cui, Yuqiao Bai, Jucheng Yang, Songyang Yan, Yuxi Pan, Zijiang Yang

Evaluating the decision-making system is indispensable in developing
autonomous vehicles, while realistic and challenging safety-critical test
scenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks
to the long-tailed distribution, sparsity, and rarity in real-world data sets.
To tackle this problem, in this paper, we introduce a natural adversarial
scenario generation solution using naturalistic human driving priors and
reinforcement learning techniques. By doing this, we can obtain large-scale
test scenarios that are both diverse and realistic. Specifically, we build a
simulation environment that mimics natural traffic interaction scenarios.
Informed by this environment, we implement a two-stage procedure. The first
stage incorporates conventional rule-based models, e.g., IDM~(Intelligent
Driver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes)
model, to coarsely and discretely capture and calibrate key control parameters
from the real-world dataset. Next, we leverage GAIL~(Generative Adversarial
Imitation Learning) to represent driver behaviors continuously. The derived
GAIL can be further used to design a PPO~(Proximal Policy Optimization)-based
actor-critic network framework to fine-tune the reward function, and then
optimizes our natural adversarial scenario generation solution. Extensive
experiments have been conducted in the NGSIM dataset including the trajectory
of 3,000 vehicles. Essential traffic parameters were measured in comparison
with the baseline model, e.g., the collision rate, accelerations, steering, and
the number of lane changes. Our findings demonstrate that the proposed model
can generate realistic safety-critical test scenarios covering both naturalness
and adversariality, which can be a cornerstone for the development of
autonomous vehicles.

ÊëòË¶ÅÔºöÂú®ÈñãÁôºËá™ÂãïÈßïÈßõËªäËºõÊôÇÔºåË©ï‰º∞Ê±∫Á≠ñÁ≥ªÁµ±ÊòØ‰∏çÂèØÊàñÁº∫ÁöÑÔºåËÄåÈÄºÁúü‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂÆâÂÖ®ÈóúÈçµÊ∏¨Ë©¶ÊÉÖÂ¢ÉÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇË¶ÅÂèñÂæóÈÄô‰∫õÊÉÖÂ¢É‰∏¶ÈùûÊòì‰∫ãÔºåÈÄôÊòØÂõ†ÁÇ∫Âú®ÁèæÂØ¶‰∏ñÁïåÁöÑË≥áÊñôÈõÜ‰∏≠Â≠òÂú®Èï∑Â∞æÂàÜÂ∏É„ÄÅÁ®ÄÁñèÊÄßÂíåÁ®ÄÊúâÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂú®Êú¨Êñá‰∏≠‰ªãÁ¥π‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Ëá™ÁÑ∂Â∞çÊäóÊÉÖÂ¢ÉÁîüÊàêËß£Ê±∫ÊñπÊ°àÔºåË©≤Ëß£Ê±∫ÊñπÊ°àÊé°Áî®Ëá™ÁÑ∂ÁöÑ‰∫∫È°ûÈßïÈßõÂÖàÈ©óÂíåÂº∑ÂåñÂ≠∏ÁøíÊäÄË°ì„ÄÇÈÄèÈÅéÈÄôÊ®£ÂÅöÔºåÊàëÂÄëÂèØ‰ª•ÂèñÂæóÊó¢Â§öÊ®£ÂåñÂèàÈÄºÁúüÁöÑÂ§ßË¶èÊ®°Ê∏¨Ë©¶ÊÉÖÂ¢É„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÊ®°Êì¨Áí∞Â¢ÉÔºåÊ®°Êì¨‰∫ÜËá™ÁÑ∂ÁöÑ‰∫§ÈÄö‰∫íÂãïÊÉÖÂ¢É„ÄÇÂú®ÈÄôÂÄãÁí∞Â¢ÉÁöÑÊåáÂ∞é‰∏ãÔºåÊàëÂÄëÂØ¶ÊñΩ‰∫Ü‰∏ÄÂÄãÂÖ©ÈöéÊÆµÁ®ãÂ∫è„ÄÇÁ¨¨‰∏ÄÈöéÊÆµÁ¥çÂÖ•‰∫ÜÂÇ≥Áµ±ÁöÑÂü∫ÊñºË¶èÂâáÁöÑÊ®°ÂûãÔºå‰æãÂ¶Ç IDMÔºàÊô∫ÊÖßÈßïÈßõÊ®°ÂûãÔºâÂíå MOBILÔºàÊúÄÂ∞èÂåñÊèõËªäÈÅìÈÄ†ÊàêÁöÑÊï¥È´îÁÖûËªäÔºâÊ®°ÂûãÔºå‰ª•Á≤óÁï•‰∏îÈõ¢Êï£ÁöÑÊñπÂºèÊì∑ÂèñÂíåÊ†°Ê∫ñ‰æÜËá™ÁèæÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÁöÑ‰∏ªË¶ÅÊéßÂà∂ÂèÉÊï∏„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÂà©Áî® GAILÔºàÁîüÊàêÂ∞çÊäóÊ®°‰ªøÂ≠∏ÁøíÔºâ‰æÜÊåÅÁ∫åË°®Á§∫ÈßïÈßõË°åÁÇ∫„ÄÇË°çÁîüÁöÑ GAIL ÂèØÈÄ≤‰∏ÄÊ≠•Áî®ÊñºË®≠Ë®à‰∏ÄÂÄãÂü∫Êñº PPOÔºàËøëÁ´ØÁ≠ñÁï•ÊúÄ‰Ω≥ÂåñÔºâÁöÑÂãï‰Ωú-Ë©ïË´ñÁ∂≤Ë∑ØÊû∂ÊßãÔºå‰ª•ÂæÆË™øÁçéÂãµÂáΩÊï∏ÔºåÁÑ∂ÂæåÊúÄ‰Ω≥ÂåñÊàëÂÄëÁöÑËá™ÁÑ∂Â∞çÊäóÊÉÖÂ¢ÉÁîüÊàêËß£Ê±∫ÊñπÊ°à„ÄÇÊàëÂÄëÂú® NGSIM Ë≥áÊñôÈõÜ‰∏≠ÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåÂÖ∂‰∏≠ÂåÖÊã¨ 3,000 ËºõËªäËºõÁöÑËªåË∑°„ÄÇËàáÂü∫Ê∫ñÊ®°ÂûãÁõ∏ÊØîÔºåÊàëÂÄëÊ∏¨Èáè‰∫ÜÈáçË¶ÅÁöÑ‰∫§ÈÄöÂèÉÊï∏Ôºå‰æãÂ¶ÇÁ¢∞ÊíûÁéá„ÄÅÂä†ÈÄüÂ∫¶„ÄÅËΩâÂêëÂíåÊèõËªäÈÅìÊ¨°Êï∏„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂèØ‰ª•ÁîüÊàêÊ∂µËìãËá™ÁÑ∂ÊÄßÂíåÂ∞çÊäóÊÄßÁöÑÈÄºÁúüÂÆâÂÖ®ÈóúÈçµÊ∏¨Ë©¶ÊÉÖÂ¢ÉÔºåÈÄôÂèØËÉΩÊòØÈñãÁôºËá™ÂãïÈßïÈßõËªäËºõÁöÑÂü∫Áü≥„ÄÇ

##### **Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi**
2408.03172v1 by Pranita Deshmukh, Nikita Kulkarni, Sanhita Kulkarni, Kareena Manghani, Raviraj Joshi

With the surge in digital content in low-resource languages, there is an
escalating demand for advanced Natural Language Processing (NLP) techniques
tailored to these languages. BERT (Bidirectional Encoder Representations from
Transformers), serving as the foundational framework for numerous NLP
architectures and language models, is increasingly employed for the development
of low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method
for fine-tuning Large Language Models (LLMs) and reducing the training
parameters to some extent to decrease the computational costs needed for
training the model and achieve results comparable to a fully fine-tuned model.
In this work, we present a study of PEFT methods for the Indic low-resource
language Marathi. We conduct a comprehensive analysis of PEFT methods applied
to various monolingual and multilingual Marathi BERT models. These approaches
are evaluated on prominent text classification datasets like MahaSent,
MahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to
significantly expedite the training speed of the models, addressing a critical
aspect of model development and deployment. In this study, we explore Low-Rank
Adaptation of Large Language Models (LoRA) and adapter methods for low-resource
text classification. We show that these methods are competitive with full
fine-tuning and can be used without loss in accuracy. This study contributes
valuable insights into the effectiveness of Marathi BERT models, offering a
foundation for the continued advancement of NLP capabilities in Marathi and
similar Indic languages.

ÊëòË¶ÅÔºö<paragraph>Èö®Ëëó‰ΩéË≥áÊ∫êË™ûË®ÄÁöÑÊï∏‰ΩçÂÖßÂÆπÊøÄÂ¢ûÔºåÂ∞çÊñºÈáùÂ∞çÈÄô‰∫õË™ûË®ÄÈáèË∫´ÊâìÈÄ†ÁöÑÂÖàÈÄ≤Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÊäÄË°ìÁöÑÈúÄÊ±Ç‰πüËàáÊó•‰ø±Â¢û„ÄÇBERTÔºà‰æÜËá™ Transformer ÁöÑÈõôÂêëÁ∑®Á¢ºÂô®Ë°®Á§∫Ê≥ïÔºâ‰ΩúÁÇ∫Ë®±Â§ö NLP Êû∂ÊßãÂíåË™ûË®ÄÊ®°ÂûãÁöÑÂü∫Êú¨Êû∂ÊßãÔºåÊ≠£Êó•ÁõäÁî®ÊñºÈñãÁôº‰ΩéË≥áÊ∫ê NLP Ê®°Âûã„ÄÇÂèÉÊï∏È´òÊïàÂæÆË™ø (PEFT) ÊòØ‰∏ÄÁ®ÆÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏¶Âú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÊ∏õÂ∞ëË®ìÁ∑¥ÂèÉÊï∏ÁöÑÊñπÊ≥ïÔºå‰ª•Èôç‰ΩéË®ìÁ∑¥Ê®°ÂûãÊâÄÈúÄÁöÑÈÅãÁÆóÊàêÊú¨Ôºå‰∏¶Áç≤ÂæóËàáÂÆåÂÖ®ÂæÆË™øÊ®°ÂûãÁõ∏Áï∂ÁöÑÁµêÊûú„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫Â∞çÂç∞Â∫¶‰ΩéË≥áÊ∫êË™ûË®ÄÈ¶¨ÊãâÊèêË™ûÁöÑ PEFT ÊñπÊ≥ïÈÄ≤Ë°åÁ†îÁ©∂„ÄÇÊàëÂÄëÂ∞çÊáâÁî®ÊñºÂêÑÁ®ÆÂñÆË™ûÂíåÂ§öË™ûÈ¶¨ÊãâÊèêË™û BERT Ê®°ÂûãÁöÑ PEFT ÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÂàÜÊûê„ÄÇÈÄô‰∫õÊñπÊ≥ïÂú®ËëóÂêçÁöÑÊñáÊú¨ÂàÜÈ°ûË≥áÊñôÈõÜÔºà‰æãÂ¶Ç MahaSent„ÄÅMahaHate Âíå MahaNewsÔºâ‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇPEFT ÊäÄË°ìÁöÑÊï¥ÂêàË¢´Ë≠âÊòéÂèØ‰ª•È°ØËëóÂä†Âø´Ê®°ÂûãÁöÑË®ìÁ∑¥ÈÄüÂ∫¶ÔºåËß£Ê±∫‰∫ÜÊ®°ÂûãÈñãÁôºÂíåÈÉ®ÁΩ≤ÁöÑ‰∏ÄÂÄãÈóúÈçµÊñπÈù¢„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LoRA) ÁöÑ‰ΩéÁß©ÈÅ©ÊáâÂíå‰ΩéË≥áÊ∫êÊñáÊú¨ÂàÜÈ°ûÁöÑÈÅ©ÈÖçÂô®ÊñπÊ≥ï„ÄÇÊàëÂÄëË°®ÊòéÔºåÈÄô‰∫õÊñπÊ≥ïËàáÂÆåÂÖ®ÂæÆË™øÂÖ∑ÊúâÁ´∂Áà≠ÂäõÔºå‰∏¶‰∏îÂèØ‰ª•Âú®‰∏çÊêçÂ§±Ê∫ñÁ¢∫ÊÄßÁöÑÊÉÖÊ≥Å‰∏ã‰ΩøÁî®„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Â∞çÈ¶¨ÊãâÊèêË™û BERT Ê®°ÂûãÁöÑÊúâÊïàÊÄßÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£ÔºåÁÇ∫È¶¨ÊãâÊèêË™ûÂíåÈ°û‰ººÂç∞Â∫¶Ë™ûË®ÄÁöÑ NLP ËÉΩÂäõÊåÅÁ∫åÈÄ≤Ê≠•Â•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ</paragraph>

##### **Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW**
2408.03168v1 by Elia Cereda, Alessandro Giusti, Daniele Palossi

Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning
(TinyML), such as nano-drones, are becoming an increasingly attractive
technology. Their small form factor (i.e., ~10cm diameter) ensures vast
applicability, ranging from the exploration of narrow disaster scenarios to
safe human-robot interaction. Simple electronics make these CPSes inexpensive,
but strongly limit the computational, memory, and sensing resources available
on board. In real-world applications, these limitations are further exacerbated
by domain shift. This fundamental machine learning problem implies that model
perception performance drops when moving from the training domain to a
different deployment one. To cope with and mitigate this general problem, we
present a novel on-device fine-tuning approach that relies only on the limited
ultra-low power resources available aboard nano-drones. Then, to overcome the
lack of ground-truth training labels aboard our CPS, we also employ a
self-supervised method based on ego-motion consistency. Albeit our work builds
on top of a specific real-world vision-based human pose estimation task, it is
widely applicable for many embedded TinyML use cases. Our 512-image on-device
training procedure is fully deployed aboard an ultra-low power GWT GAP9
System-on-Chip and requires only 1MB of memory while consuming as low as 19mW
or running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our
on-device learning approach by field-testing our closed-loop CPS, showing a
reduction in horizontal position error of up to 26% vs. a non-fine-tuned
state-of-the-art baseline. In the most challenging never-seen-before
environment, our on-device learning procedure makes the difference between
succeeding or failing the mission.

ÊëòË¶ÅÔºö<paragraph>Áî±ÂæÆÂûãÊ©üÂô®Â≠∏ÁøíÔºàTinyMLÔºâÈ©ÖÂãïÁöÑÂæÆÂûãÂåñÁ∂≤ÈöõÁ∂≤Ë∑ØÂØ¶È´îÁ≥ªÁµ±ÔºàCPSÔºâÔºå‰æãÂ¶ÇÂ•àÁ±≥ÁÑ°‰∫∫Ê©üÔºåÊ≠£ÊàêÁÇ∫Ë∂ä‰æÜË∂äÊúâÂê∏ÂºïÂäõÁöÑÊäÄË°ì„ÄÇÂÆÉÂÄëÁöÑÂ∞èÂ§ñÂΩ¢ÔºàÂç≥Áõ¥ÂæëÁ¥Ñ 10 ÂÖ¨ÂàÜÔºâÁ¢∫‰øù‰∫ÜÂª£Ê≥õÁöÑÈÅ©Áî®ÊÄßÔºåÂæûÊé¢Á¥¢ÁãπÁ™ÑÁöÑÁÅΩÈõ£Â†¥ÊôØÂà∞ÂÆâÂÖ®ÁöÑ‰∫∫Ê©ü‰∫íÂãï„ÄÇÁ∞°ÂñÆÁöÑÈõªÂ≠êÁî¢ÂìÅËÆìÈÄô‰∫õ CPS ÂÉπÊ†º‰ΩéÂªâÔºå‰ΩÜÊ•µÂ§ßÂú∞ÈôêÂà∂‰∫ÜÊ©üËºâÁöÑÈÅãÁÆó„ÄÅË®òÊÜ∂È´îÂíåÊÑüÊ∏¨Ë≥áÊ∫ê„ÄÇÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÔºåÈÄô‰∫õÈôêÂà∂Âõ†È†òÂüüËΩâÁßªËÄåÈÄ≤‰∏ÄÊ≠•ÊÉ°Âåñ„ÄÇÈÄôÂÄãÂü∫Êú¨ÁöÑÊ©üÂô®Â≠∏ÁøíÂïèÈ°åÊÑèÂë≥ËëóÔºåÁï∂ÂæûË®ìÁ∑¥È†òÂüüËΩâÁßªÂà∞‰∏çÂêåÁöÑÈÉ®ÁΩ≤È†òÂüüÊôÇÔºåÊ®°ÂûãÊÑüÁü•ÊïàËÉΩÊúÉ‰∏ãÈôç„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÂíåÊ∏õËºïÈÄôÂÄãÊôÆÈÅçÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑË£ùÁΩÆ‰∏äÂæÆË™øÊñπÊ≥ïÔºåÂÉÖ‰æùË≥¥ÊñºÂ•àÁ±≥ÁÑ°‰∫∫Ê©ü‰∏äÂèØÁî®ÁöÑÊúâÈôêË∂Ö‰ΩéÂäüÁéáË≥áÊ∫ê„ÄÇÁÑ∂ÂæåÔºåÁÇ∫‰∫ÜÂÖãÊúçÊàëÂÄëÁöÑ CPS ‰∏äÁº∫‰πèÂú∞Èù¢ÁúüÂØ¶Ë®ìÁ∑¥Ê®ôÁ±§ÔºåÊàëÂÄëÈÇÑÊé°Áî®‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºËá™ÊàëÈÅãÂãï‰∏ÄËá¥ÊÄßÁöÑËá™Áõ£Áù£ÊñπÊ≥ï„ÄÇÂÑòÁÆ°ÊàëÂÄëÁöÑÁ†îÁ©∂Âª∫Á´ãÂú®ÂÖ∑È´îÁöÑÁúüÂØ¶‰∏ñÁïåÂü∫ÊñºË¶ñË¶∫ÁöÑ‰∫∫È´îÂßøÂã¢‰º∞Ë®à‰ªªÂãô‰πã‰∏äÔºå‰ΩÜÂÆÉÂª£Ê≥õÈÅ©Áî®ÊñºË®±Â§öÂµåÂÖ•Âºè TinyML ‰ΩøÁî®Ê°à‰æã„ÄÇÊàëÂÄëÁöÑ 512 ÂΩ±ÂÉèË£ùÁΩÆ‰∏äË®ìÁ∑¥Á®ãÂ∫èÂÆåÂÖ®ÈÉ®ÁΩ≤Âú®Ë∂Ö‰ΩéÂäüÁéá GWT GAP9 Êô∂ÁâáÁµÑ‰∏äÔºåÂè™ÈúÄË¶Å 1MB ÁöÑË®òÊÜ∂È´îÔºåÂêåÊôÇÊ∂àËÄó‰ΩéËá≥ 19mWÔºåÊàñÂÉÖÂú® 510msÔºà38mWÔºâ‰∏ãÂü∑Ë°å„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈÄèÈÅéÂØ¶Âú∞Ê∏¨Ë©¶ÊàëÂÄëÁöÑÈñâÁí∞ CPS ‰æÜÂ±ïÁ§∫ÊàëÂÄëÁöÑË£ùÁΩÆ‰∏äÂ≠∏ÁøíÊñπÊ≥ïÁöÑÂÑ™ÈªûÔºåËàáÈùûÂæÆË™øÁöÑÊúÄÊñ∞Âü∫Ê∫ñÁõ∏ÊØîÔºåÈ°ØÁ§∫Ê∞¥Âπ≥‰ΩçÁΩÆË™§Â∑ÆÊ∏õÂ∞ë‰∫Ü 26%„ÄÇÂú®ÊúÄÂÖ∑ÊåëÊà∞ÊÄßÁöÑÂâçÊâÄÊú™Ë¶ãÁí∞Â¢É‰∏≠ÔºåÊàëÂÄëÁöÑË£ùÁΩÆ‰∏äÂ≠∏ÁøíÁ®ãÂ∫èÂú®‰ªªÂãôÁöÑÊàêÂäüÊàñÂ§±Êïó‰πãÈñìÁî¢Áîü‰∫ÜÂ∑ÆÁï∞„ÄÇ</paragraph>

##### **Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study**
2408.03164v1 by Rabih Chamas, Ismail Khalfaoui-Hassani, Timothee Masquelier

Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced
convolution method that allows enlarging the receptive fields (RF) without
increasing the number of parameters, like the dilated convolution, yet without
imposing a regular grid. DCLS has been shown to outperform the standard and
dilated convolutions on several computer vision benchmarks. Here, we show that,
in addition, DCLS increases the models' interpretability, defined as the
alignment with human visual strategies. To quantify it, we use the Spearman
correlation between the models' GradCAM heatmaps and the ClickMe dataset
heatmaps, which reflect human visual attention. We took eight reference models
- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and
36) - and drop-in replaced the standard convolution layers with DCLS ones. This
improved the interpretability score in seven of them. Moreover, we observed
that Grad-CAM generated random heatmaps for two models in our study: CAFormer
and ConvFormer models, leading to low interpretability scores. We addressed
this issue by introducing Threshold-Grad-CAM, a modification built on top of
Grad-CAM that enhanced interpretability across nearly all models. The code and
checkpoints to reproduce this study are available at:
https://github.com/rabihchamas/DCLS-GradCAM-Eval.

ÊëòË¶ÅÔºöÂèØÂ≠∏ÁøíÈñìË∑ùÊì¥Â±ïÂç∑Á©ç (DCLS) ÊòØ‰∏ÄÁ®ÆËøëÊúüÈÄ≤ÈöéÁöÑÂç∑Á©çÊñπÊ≥ïÔºåÂÖÅË®±Êì¥Â±ïÊÑüÂèóÈáé (RF)ÔºåËÄåÁÑ°È†àÂ¢ûÂä†ÂèÉÊï∏Êï∏ÈáèÔºåÂ∞±ÂÉèÊì¥Â±ïÂç∑Á©ç‰∏ÄÊ®£Ôºå‰ΩÜÁÑ°È†àÂº∑Âà∂‰ΩøÁî®Ë¶èÂâáÁ∂≤Ê†º„ÄÇÂ∑≤Ë≠âÂØ¶ DCLS Âú®Â§öÂÄãÈõªËÖ¶Ë¶ñË¶∫Âü∫Ê∫ñ‰∏äÂÑ™ÊñºÊ®ôÊ∫ñÂç∑Á©çÂíåÊì¥Â±ïÂç∑Á©ç„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂ±ïÁ§∫ DCLS Èô§‰∫ÜËÉΩÊèêÂçáÊ®°ÂûãÁöÑÂèØË©ÆÈáãÊÄßÔºåÂÆöÁæ©ÁÇ∫Ëàá‰∫∫È°ûË¶ñË¶∫Á≠ñÁï•ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÁÇ∫ÈáèÂåñÂÆÉÔºåÊàëÂÄë‰ΩøÁî®Ê®°ÂûãÁöÑ GradCAM ÁÜ±ÂúñËàá ClickMe Ë≥áÊñôÈõÜÁÜ±Âúñ‰πãÈñìÁöÑ Spearman Áõ∏ÈóúÊÄßÔºåÂÆÉÂèçÊò†‰∫Ü‰∫∫È°ûÁöÑË¶ñË¶∫Ê≥®ÊÑèÂäõ„ÄÇÊàëÂÄëÊé°Áî®‰∫ÜÂÖ´ÂÄãÂèÉËÄÉÊ®°Âûã - ResNet50„ÄÅConvNeXt (T„ÄÅS Âíå B)„ÄÅCAFormer„ÄÅConvFormer Âíå FastViT (sa 24 Âíå 36) - ‰∏¶‰ª• DCLS Âèñ‰ª£Ê®ôÊ∫ñÂç∑Á©çÂ±§„ÄÇÈÄôÊèêÂçá‰∫ÜÂÖ∂‰∏≠‰∏ÉÂÄãÊ®°ÂûãÁöÑÂèØË©ÆÈáãÊÄßÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëËßÄÂØüÂà∞ Grad-CAM ÁÇ∫ÊàëÂÄëÁ†îÁ©∂‰∏≠ÁöÑÂÖ©ÂÄãÊ®°ÂûãÁî¢Áîü‰∫ÜÈö®Ê©üÁÜ±ÂúñÔºöCAFormer Âíå ConvFormer Ê®°ÂûãÔºåÂ∞éËá¥ÂèØË©ÆÈáãÊÄßÂàÜÊï∏‰Ωé„ÄÇÊàëÂÄëÈÄèÈÅéÂºïÂÖ• Threshold-Grad-CAM ‰æÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂª∫Á´ãÂú® Grad-CAM ‰πã‰∏äÁöÑ‰øÆÊîπÔºåÂèØÂ¢ûÂº∑Âπæ‰πéÊâÄÊúâÊ®°ÂûãÁöÑÂèØË©ÆÈáãÊÄß„ÄÇÂèØÊñº‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÈáçÁèæÊ≠§Á†îÁ©∂ÁöÑÁ®ãÂºèÁ¢ºÂíåÊ™¢Êü•ÈªûÔºöhttps://github.com/rabihchamas/DCLS-GradCAM-Eval„ÄÇ

##### **Conditioning LLMs with Emotion in Neural Machine Translation**
2408.03150v1 by Charles Brazier, Jean-Luc Rouas

Large Language Models (LLMs) have shown remarkable performance in Natural
Language Processing tasks, including Machine Translation (MT). In this work, we
propose a novel MT pipeline that integrates emotion information extracted from
a Speech Emotion Recognition (SER) model into LLMs to enhance translation
quality. We first fine-tune five existing LLMs on the Libri-trans dataset and
select the most performant model. Subsequently, we augment LLM prompts with
different dimensional emotions and train the selected LLM under these different
configurations. Our experiments reveal that integrating emotion information,
especially arousal, into LLM prompts leads to notable improvements in
translation quality.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãôÔºåÂåÖÊã¨Ê©üÂô®ÁøªË≠Ø (MT)Ôºå‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑË°®Áèæ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂâµÊñ∞ÁöÑ MT ÁÆ°ÈÅìÔºåÂ∞áÂæûË™ûÈü≥ÊÉÖÁ∑íËæ®Ë≠ò (SER) Ê®°Âûã‰∏≠ÊèêÂèñÁöÑÊÉÖÁ∑íË≥áË®äÊï¥ÂêàÂà∞ LLM ‰∏≠Ôºå‰ª•ÊèêÂçáÁøªË≠ØÂìÅË≥™„ÄÇÊàëÂÄëÈ¶ñÂÖàÂú® Libri-trans Ë≥áÊñôÈõÜ‰∏äÂæÆË™ø‰∫îÂÄãÁèæÊúâÁöÑ LLMÔºå‰∏¶ÈÅ∏Âá∫Ë°®ÁèæÊúÄ‰Ω≥ÁöÑÊ®°Âûã„ÄÇÊé•ËëóÔºåÊàëÂÄë‰ΩøÁî®‰∏çÂêåÁ∂≠Â∫¶ÁöÑÊÉÖÁ∑íÊì¥ÂÖÖ LLM ÊèêÁ§∫Ôºå‰∏¶Âú®ÈÄô‰∫õ‰∏çÂêåÁöÑË®≠ÂÆö‰∏ãË®ìÁ∑¥ÈÅ∏ÂÆöÁöÑ LLM„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÂ∞áÊÉÖÁ∑íË≥áË®äÔºåÂ∞§ÂÖ∂ÊòØÂñöÈÜíÔºåÊï¥ÂêàÂà∞ LLM ÊèêÁ§∫‰∏≠ÔºåÊúÉÈ°ØËëóÊèêÂçáÁøªË≠ØÂìÅË≥™„ÄÇ

##### **Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization**
2408.03149v1 by Yanghai Zhang, Ye Liu, Shiwei Wu, Kai Zhang, Xukai Liu, Qi Liu, Enhong Chen

The rapid increase in multimedia data has spurred advancements in Multimodal
Summarization with Multimodal Output (MSMO), which aims to produce a multimodal
summary that integrates both text and relevant images. The inherent
heterogeneity of content within multimodal inputs and outputs presents a
significant challenge to the execution of MSMO. Traditional approaches
typically adopt a holistic perspective on coarse image-text data or individual
visual objects, overlooking the essential connections between objects and the
entities they represent. To integrate the fine-grained entity knowledge, we
propose an Entity-Guided Multimodal Summarization model (EGMS). Our model,
building on BART, utilizes dual multimodal encoders with shared weights to
process text-image and entity-image information concurrently. A gating
mechanism then combines visual data for enhanced textual summary generation,
while image selection is refined through knowledge distillation from a
pre-trained vision-language model. Extensive experiments on public MSMO dataset
validate the superiority of the EGMS method, which also prove the necessity to
incorporate entity information into MSMO problem.

ÊëòË¶ÅÔºöÈö®ËëóÂ§öÂ™íÈ´îË≥áÊñôÁöÑÂø´ÈÄüÂ¢ûÂä†ÔºåÂà∫ÊøÄ‰∫ÜÂ§öÊ®°ÊÖãËº∏Âá∫Â§öÊ®°ÊÖãÊëòË¶Å (MSMO) ÁöÑÈÄ≤Â±ïÔºåÂÖ∂ÁõÆÊ®ôÊòØÁî¢Áîü‰∏ÄÂÄãÊï¥ÂêàÊñáÂ≠óÂíåÁõ∏ÈóúÂΩ±ÂÉèÁöÑÂ§öÊ®°ÊÖãÊëòË¶Å„ÄÇÂ§öÊ®°ÊÖãËº∏ÂÖ•ÂíåËº∏Âá∫ÂÖßÂú®ÁöÑÁï∞Ë≥™ÊÄßÔºåÂ∞ç MSMO ÁöÑÂü∑Ë°åÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÈÄöÂ∏∏Êé°Áî®Êï¥È´îËßÄÈªûÔºåÈáùÂ∞çÁ≤óÁï•ÁöÑÂΩ±ÂÉèÊñáÂ≠óË≥áÊñôÊàñÂÄãÂà•Ë¶ñË¶∫Áâ©‰ª∂ÔºåÂøΩÁï•‰∫ÜÁâ©‰ª∂ËàáÂÖ∂ÊâÄ‰ª£Ë°®ÂØ¶È´î‰πãÈñìÁöÑÊú¨Ë≥™ÈóúËÅØÊÄß„ÄÇÁÇ∫‰∫ÜÊï¥ÂêàÁ¥∞Á≤íÂ∫¶ÁöÑÂØ¶È´îÁü•Ë≠òÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂØ¶È´îÂºïÂ∞éÂºèÂ§öÊ®°ÊÖãÊëòË¶ÅÊ®°Âûã (EGMS)„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂª∫ÊßãÂú® BART ‰∏äÔºåÂà©Áî®ÂÖ∑ÊúâÂÖ±‰∫´Ê¨äÈáçÁöÑÈõôÈáçÂ§öÊ®°ÊÖãÁ∑®Á¢ºÂô®ÔºåÂêåÊôÇËôïÁêÜÊñáÂ≠óÂΩ±ÂÉèÂíåÂØ¶È´îÂΩ±ÂÉèË≥áË®ä„ÄÇ‰∏ÄÂÄãÈñòÊéßÊ©üÂà∂Êé•ËëóÁµêÂêàË¶ñË¶∫Ë≥áÊñôÔºå‰ª•Â¢ûÂº∑ÊñáÂ≠óÊëòË¶ÅÁöÑÁîüÊàêÔºåÂêåÊôÇÈÄèÈÅéÈ†êÂÖàË®ìÁ∑¥ÁöÑË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÁü•Ë≠òËêÉÂèñÔºåÁ≤æÁÖâÂΩ±ÂÉèÈÅ∏Êìá„ÄÇÂú®ÂÖ¨Èñã MSMO Ë≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óÈ©óË≠â‰∫Ü EGMS ÊñπÊ≥ïÁöÑÂÑ™Ë∂äÊÄßÔºå‰πüË≠âÊòé‰∫ÜÂ∞áÂØ¶È´îË≥áË®äÁ¥çÂÖ• MSMO ÂïèÈ°åÁöÑÂøÖË¶ÅÊÄß„ÄÇ

##### **Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations**
2408.03130v1 by Leo Donisch, Sigurd Schacht, Carsten Lanquillon

Large language models are ubiquitous in natural language processing because
they can adapt to new tasks without retraining. However, their sheer scale and
complexity present unique challenges and opportunities, prompting researchers
and practitioners to explore novel model training, optimization, and deployment
methods. This literature review focuses on various techniques for reducing
resource requirements and compressing large language models, including
quantization, pruning, knowledge distillation, and architectural optimizations.
The primary objective is to explore each method in-depth and highlight its
unique challenges and practical applications. The discussed methods are
categorized into a taxonomy that presents an overview of the optimization
landscape and helps navigate it to understand the research trajectory better.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÁÑ°Ëôï‰∏çÂú®ÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁÑ°ÈúÄÈáçÊñ∞Ë®ìÁ∑¥Âç≥ÂèØÈÅ©ÊáâÊñ∞‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÁöÑË¶èÊ®°ÂíåË§áÈõúÊÄßÂ∏∂‰æÜ‰∫ÜÁç®ÁâπÁöÑÊåëÊà∞ÂíåÊ©üÈÅáÔºå‰øÉ‰ΩøÁ†îÁ©∂‰∫∫Âì°ÂíåÂæûÊ•≠ËÄÖÊé¢Á¥¢Êñ∞ÁöÑÊ®°ÂûãË®ìÁ∑¥„ÄÅÊúÄ‰Ω≥ÂåñÂíåÈÉ®ÁΩ≤ÊñπÊ≥ï„ÄÇÈÄôÁØáÊñáÁçªÂõûÈ°ßÈáçÈªûÊé¢Ë®éÂêÑÁ®ÆÊäÄË°ìÔºå‰ª•Ê∏õÂ∞ëË≥áÊ∫êÈúÄÊ±Ç‰∏¶Â£ìÁ∏ÆÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåÂåÖÊã¨ÈáèÂåñ„ÄÅÂâ™Êûù„ÄÅÁü•Ë≠òËí∏È§æÂíåÊû∂ÊßãÊúÄ‰Ω≥Âåñ„ÄÇ‰∏ªË¶ÅÁõÆÊ®ôÊòØÊ∑±ÂÖ•Êé¢Ë®éÊØèÁ®ÆÊñπÊ≥ïÔºå‰∏¶Âº∑Ë™øÂÖ∂Áç®ÁâπÁöÑÊåëÊà∞ÂíåÂØ¶ÈöõÊáâÁî®„ÄÇÊâÄË®éË´ñÁöÑÊñπÊ≥ïË¢´ÂàÜÈ°ûÂà∞‰∏ÄÂÄãÂàÜÈ°ûÊ≥ï‰∏≠ÔºåË©≤ÂàÜÈ°ûÊ≥ïÊ¶ÇËø∞‰∫ÜÊúÄ‰Ω≥ÂåñÈ†òÂüüÔºå‰∏¶ÊúâÂä©ÊñºÁÄèË¶ΩÂÆÉ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Á†îÁ©∂ËªåË∑°„ÄÇ

##### **Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation**
2408.03127v1 by Artur Guimar√£es, Bruno Martins, Jo√£o Magalh√£es

This paper describes our approach to the SemEval-2024 safe biomedical Natural
Language Inference for Clinical Trials (NLI4CT) task, which concerns
classifying statements about Clinical Trial Reports (CTRs). We explored the
capabilities of Mistral-7B, a generalist open-source Large Language Model
(LLM). We developed a prompt for the NLI4CT task, and fine-tuned a quantized
version of the model using an augmented version of the training dataset. The
experimental results show that this approach can produce notable results in
terms of the macro F1-score, while having limitations in terms of faithfulness
and consistency. All the developed code is publicly available on a GitHub
repository

ÊëòË¶ÅÔºöÊú¨ÊñáÊèèËø∞‰∫ÜÊàëÂÄëÂ∞ç SemEval-2024 ÂÆâÂÖ®ÁîüÁâ©ÈÜ´Â≠∏Ëá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñËá®Â∫äË©¶È©ó (NLI4CT) ‰ªªÂãôÁöÑÊñπÊ≥ïÔºåË©≤‰ªªÂãôÊ∂âÂèäÂ∞çËá®Â∫äË©¶È©óÂ†±Âëä (CTR) ‰∏≠ÁöÑÈô≥Ëø∞ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÊàëÂÄëÊé¢Ë®é‰∫Ü Mistral-7B ÁöÑËÉΩÂäõÔºåÈÄôÊòØ‰∏ÄÂÄãÈÄöÊâçÁöÑÈñãÊ∫êÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇÊàëÂÄëÁÇ∫ NLI4CT ‰ªªÂãôÈñãÁôº‰∫Ü‰∏ÄÂÄãÊèêÁ§∫Ôºå‰∏¶‰ΩøÁî®Ë®ìÁ∑¥Êï∏ÊìöÈõÜÁöÑÊì¥ÂÖÖÁâàÊú¨Â∞çÊ®°ÂûãÁöÑÈáèÂåñÁâàÊú¨ÈÄ≤Ë°å‰∫ÜÂæÆË™ø„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÈÄôÁ®ÆÊñπÊ≥ïÂú®ÂÆèËßÄ F1 ÂàÜÊï∏ÊñπÈù¢ÂèØ‰ª•Áî¢ÁîüÈ°ØËëóÁöÑÁµêÊûúÔºåÂêåÊôÇÂú®‰øùÁúüÂ∫¶Âíå‰∏ÄËá¥ÊÄßÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄß„ÄÇÊâÄÊúâÂ∑≤ÈñãÁôºÁöÑ‰ª£Á¢ºÈÉΩÂÖ¨ÈñãÁôºÂ∏ÉÂú® GitHub ÂÄâÂ∫´‰∏≠

##### **COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework**
2408.03125v1 by Rajvee Sheth, Shubh Nisar, Heenaben Prajapati, Himanshu Beniwal, Mayank Singh

As the NLP community increasingly addresses challenges associated with
multilingualism, robust annotation tools are essential to handle multilingual
datasets efficiently. In this paper, we introduce a code-mixed multilingual
text annotation framework, COMMENTATOR, specifically designed for annotating
code-mixed text. The tool demonstrates its effectiveness in token-level and
sentence-level language annotation tasks for Hinglish text. We perform robust
qualitative human-based evaluations to showcase COMMENTATOR led to 5x faster
annotations than the best baseline. Our code is publicly available at
\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is
available at \url{https://bit.ly/commentator_video}.

ÊëòË¶ÅÔºöÈö®Ëëó NLP Á§æÁæ§Êó•ÁõäËëóÈáçÊñºÂõ†Â§öË™ûË®ÄÊÄßËÄåÈù¢Ëá®ÁöÑÊåëÊà∞ÔºåÁ©©ÂÅ•ÁöÑË®ªËß£Â∑•ÂÖ∑Â∞çÊñºÊúâÊïàËôïÁêÜÂ§öË™ûË®ÄË≥áÊñôÈõÜËá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÁ®ãÂºèÁ¢ºÊ∑∑ÂêàÂ§öË™ûË®ÄÊñáÂ≠óË®ªËß£Êû∂Êßã COMMENTATORÔºåÂ∞àÈñÄË®≠Ë®àÁî®ÊñºË®ªËß£Á®ãÂºèÁ¢ºÊ∑∑ÂêàÊñáÂ≠ó„ÄÇÊ≠§Â∑•ÂÖ∑Â±ïÁèæÂÖ∂Âú® Hinglish ÊñáÂ≠óÁöÑË©ûÂÖÉÂ±§Á¥öÂíåÂè•Â≠êÂ±§Á¥öË™ûË®ÄË®ªËß£‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÂü∑Ë°åÁ©©ÂÅ•ÁöÑÂÆöÊÄß‰∫∫Â∑•Ë©ï‰º∞Ôºå‰ª•Â±ïÁ§∫ COMMENTATOR ÁöÑË®ªËß£ÈÄüÂ∫¶ÊØîÊúÄ‰Ω≥Âü∫Á∑öÂø´ 5 ÂÄç„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº \url{https://github.com/lingo-iitgn/commentator}„ÄÇÁ§∫ÁØÑÂΩ±ÁâáÂèØÊñº \url{https://bit.ly/commentator_video} ÂèñÂæó„ÄÇ

##### **Evaluating the Translation Performance of Large Language Models Based on Euas-20**
2408.03119v1 by Yan Huang, Wei Liu

In recent years, with the rapid development of deep learning technology,
large language models (LLMs) such as BERT and GPT have achieved breakthrough
results in natural language processing tasks. Machine translation (MT), as one
of the core tasks of natural language processing, has also benefited from the
development of large language models and achieved a qualitative leap. Despite
the significant progress in translation performance achieved by large language
models, machine translation still faces many challenges. Therefore, in this
paper, we construct the dataset Euas-20 to evaluate the performance of large
language models on translation tasks, the translation ability on different
languages, and the effect of pre-training data on the translation ability of
LLMs for researchers and developers.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÈö®ËëóÊ∑±Â∫¶Â≠∏ÁøíÊäÄË°ìÁöÑÂø´ÈÄüÁôºÂ±ïÔºå
BERT Âíå GPT Á≠âÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠ÂèñÂæó‰∫ÜÁ™ÅÁ†¥ÊÄßÁöÑ
ÊàêÊûú„ÄÇÊ©üÂô®ÁøªË≠ØÔºàMTÔºâ‰ΩúÁÇ∫Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÊ†∏ÂøÉ‰ªªÂãô‰πã‰∏ÄÔºå‰πüÂèóÁõäÊñº
Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÁôºÂ±ïËÄåÂèñÂæó‰∫ÜË≥™ÁöÑÈ£õË∫ç„ÄÇÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ÁøªË≠ØÊÄßËÉΩ‰∏äÂèñÂæó‰∫ÜÈ°ØËëóÈÄ≤Â±ïÔºå
Ê©üÂô®ÁøªË≠Ø‰ªçÁÑ∂Èù¢Ëá®ËëóË´∏Â§öÊåëÊà∞„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊßãÂª∫‰∫Ü Euas-20 Êï∏ÊìöÈõÜÔºåÁî®ÊñºË©ï‰º∞Â§ßÂûã
Ë™ûË®ÄÊ®°ÂûãÂú®ÁøªË≠Ø‰ªªÂãô‰∏äÁöÑË°®Áèæ„ÄÅ‰∏çÂêåË™ûË®ÄÁöÑÁøªË≠ØËÉΩÂäõÔºå‰ª•ÂèäÈ†êË®ìÁ∑¥Êï∏ÊìöÂ∞ç LLM ÁøªË≠ØËÉΩÂäõÁöÑÂΩ±ÈüøÔºå‰æõÁ†îÁ©∂‰∫∫Âì°ÂíåÈñãÁôº‰∫∫Âì°‰ΩøÁî®„ÄÇ

##### **Topic Modeling with Fine-tuning LLMs and Bag of Sentences**
2408.03099v1 by Johannes Schneider

Large language models (LLM)'s are increasingly used for topic modeling
outperforming classical topic models such as LDA. Commonly, pre-trained LLM
encoders such as BERT are used out-of-the-box despite the fact that fine-tuning
is known to improve LLMs considerably. The challenge lies in obtaining a
suitable (labeled) dataset for fine-tuning. In this paper, we use the recent
idea to use bag of sentences as the elementary unit in computing topics. In
turn, we derive an approach FT-Topic to perform unsupervised fine-tuning
relying primarily on two steps for constructing a training dataset in an
automatic fashion. First, a heuristic method to identifies pairs of sentence
groups that are either assumed to be of the same or different topics. Second,
we remove sentence pairs that are likely labeled incorrectly. The dataset is
then used to fine-tune an encoder LLM, which can be leveraged by any topic
modeling approach using embeddings. However, in this work, we demonstrate its
effectiveness by deriving a novel state-of-the-art topic modeling method called
SenClu, which achieves fast inference through an expectation-maximization
algorithm and hard assignments of sentence groups to a single topic, while
giving users the possibility to encode prior knowledge on the topic-document
distribution. Code is at \url{https://github.com/JohnTailor/FT-Topic}

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊÑà‰æÜÊÑàÂ∏∏Ë¢´Áî®Êñº‰∏ªÈ°åÊ®°ÂûãÔºåÂÖ∂ÊïàËÉΩÂÑ™Êñº LDA Á≠âÂÇ≥Áµ±‰∏ªÈ°åÊ®°Âûã„ÄÇÂÑòÁÆ°ÂæÆË™øÂ∑≤Áü•ËÉΩÂ§ßÂπÖÊîπÂñÑ LLMÔºå‰ΩÜÈÄöÂ∏∏ÊúÉÁõ¥Êé•‰ΩøÁî®È†êÂÖàË®ìÁ∑¥Â•ΩÁöÑ LLM Á∑®Á¢ºÂô®Ôºå‰æãÂ¶Ç BERT„ÄÇÊåëÊà∞Âú®ÊñºÂèñÂæóÈÅ©ÂêàÁöÑÔºàÊ®ôÁ±§ÔºâË≥áÊñôÈõÜ‰ª•ÈÄ≤Ë°åÂæÆË™ø„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé°Áî®ÊúÄËøëÁöÑÊÉ≥Ê≥ïÔºåÂ∞áÂè•Â≠êÁµÑÁï∂‰ΩúÈÅãÁÆó‰∏ªÈ°åÁöÑÂü∫Êú¨ÂñÆ‰Ωç„ÄÇÂèçÈÅé‰æÜÔºåÊàëÂÄëË°çÁîüÂá∫ FT-Topic ÊñπÊ≥ïÔºå‰ª•Âü∑Ë°åÈùûÁõ£Áù£ÂºèÂæÆË™øÔºå‰∏ªË¶Å‰æùË≥¥ÂÖ©ÂÄãÊ≠•È©üËá™ÂãïÂª∫ÊßãË®ìÁ∑¥Ë≥áÊñôÈõÜ„ÄÇÈ¶ñÂÖàÔºåÊé°Áî®ÂïüÁôºÂºèÊñπÊ≥ï‰æÜË≠òÂà•ÂÅáË®≠ÁÇ∫Âêå‰∏ªÈ°åÊàñ‰∏çÂêå‰∏ªÈ°åÁöÑÂè•Â≠êÁµÑÂ∞ç„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÁßªÈô§Ê®ôÁ±§ÂèØËÉΩ‰∏çÊ≠£Á¢∫ÁöÑÂè•Â≠êÂ∞ç„ÄÇÁÑ∂Âæå‰ΩøÁî®Ë≥áÊñôÈõÜÂæÆË™øÁ∑®Á¢ºÂô® LLMÔºå‰ªª‰Ωï‰ΩøÁî®ÂµåÂÖ•ÂºèÁöÑ‰∏ªÈ°åÊ®°ÂûãÊñπÊ≥ïÈÉΩËÉΩÈÅãÁî®Ë©≤Á∑®Á¢ºÂô®„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÂú®Ê≠§Â∑•‰Ωú‰∏≠ÈÄèÈÅéË°çÁîüÂá∫Á®±ÁÇ∫ SenClu ÁöÑÊñ∞ÂºèÊúÄÂÖàÈÄ≤‰∏ªÈ°åÊ®°ÂûãÊñπÊ≥ïÔºå‰æÜË≠âÊòéÂÖ∂ÊúâÊïàÊÄß„ÄÇË©≤ÊñπÊ≥ïÈÄèÈÅéÊúüÊúõÂÄºÊúÄÂ§ßÂåñÊºîÁÆóÊ≥ïÂíåÂè•Â≠êÁµÑÂ∞çÂñÆ‰∏Ä‰∏ªÈ°åÁöÑÁ°¨ÂºèÊåáÊ¥æÔºåÂø´ÈÄüÈÄ≤Ë°åÊé®Ë´ñÔºåÂêåÊôÇËÆì‰ΩøÁî®ËÄÖËÉΩÂ§†Â∞ç‰∏ªÈ°åÊñá‰ª∂ÂàÜÂ∏ÉÁ∑®Á¢ºÂÖàÈ©óÁü•Ë≠ò„ÄÇÁ®ãÂºèÁ¢º‰ΩçÊñº \url{https://github.com/JohnTailor/FT-Topic}

##### **500xCompressor: Generalized Prompt Compression for Large Language Models**
2408.03094v1 by Zongqian Li, Yixuan Su, Nigel Collier

Prompt compression is crucial for enhancing inference speed, reducing costs,
and improving user experience. However, current methods face challenges such as
low compression ratios and potential data leakage during evaluation. To address
these issues, we propose 500xCompressor, a method that compresses extensive
natural language contexts into a minimum of one single special token. The
500xCompressor introduces approximately 0.3% additional parameters and achieves
compression ratios ranging from 6x to 480x. It is designed to compress any
text, answer various types of questions, and could be utilized by the original
large language model (LLM) without requiring fine-tuning. Initially,
500xCompressor was pretrained on the Arxiv Corpus, followed by fine-tuning on
the ArxivQA dataset, and subsequently evaluated on strictly unseen and
classical question answering (QA) datasets. The results demonstrate that the
LLM retained 62.26-72.89% of its capabilities compared to using non-compressed
prompts. This study also shows that not all the compressed tokens are equally
utilized and that K V values have significant advantages over embeddings in
preserving information at high compression ratios. The highly compressive
nature of natural language prompts, even for fine-grained complex information,
suggests promising potential for future applications and further research into
developing a new LLM language.

ÊëòË¶ÅÔºöÊèêÁ§∫Â£ìÁ∏ÆÂ∞çÊñºÊèêÂçáÊé®Ë´ñÈÄüÂ∫¶„ÄÅÈôç‰ΩéÊàêÊú¨ÂíåÊîπÂñÑ‰ΩøÁî®ËÄÖÈ´îÈ©óËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÊñπÊ≥ïÈù¢Ëá®‰ΩéÂ£ìÁ∏ÆÁéáÂíåË©ï‰º∞ÊúüÈñìÊΩõÂú®Ë≥áÊñôÂ§ñÊ¥©Á≠âÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ 500xCompressorÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞áÂª£Ê≥õÁöÑËá™ÁÑ∂Ë™ûË®ÄËÑàÁµ°Â£ìÁ∏ÆÊàêÊúÄÂ∞ë‰∏ÄÂÄãÁâπÊÆäÁ¨¶ËôüÁöÑÊñπÊ≥ï„ÄÇ500xCompressor ÂºïÂÖ•‰∫ÜÁ¥Ñ 0.3% ÁöÑÈ°çÂ§ñÂèÉÊï∏Ôºå‰∏¶ÈÅîÂà∞‰∫Ü 6 ÂÄçÂà∞ 480 ÂÄçÁöÑÂ£ìÁ∏ÆÁéá„ÄÇÂÆÉË¢´Ë®≠Ë®àÁî®ÊñºÂ£ìÁ∏Æ‰ªª‰ΩïÊñáÂ≠ó„ÄÅÂõûÁ≠îÂêÑÁ®ÆÈ°ûÂûãÁöÑÂïèÈ°åÔºå‰∏¶‰∏îÂèØ‰ª•Âú®‰∏çÈúÄÂæÆË™øÁöÑÊÉÖÊ≥Å‰∏ãÁî±ÂéüÂßãÁöÑÂ§ßË™ûË®ÄÊ®°Âûã (LLM) ‰ΩøÁî®„ÄÇÊúÄÂàùÔºå500xCompressor Âú® Arxiv Corpus ‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥ÔºåÁÑ∂ÂæåÂú® ArxivQA Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂæÆË™øÔºåÊúÄÂæåÂú®ÂÆåÂÖ®Êú™Ë¶ãÈÅéÂíåÁ∂ìÂÖ∏ÁöÑÂïèÈ°åËß£Á≠î (QA) Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇÁµêÊûúË°®ÊòéÔºåËàá‰ΩøÁî®Êú™Â£ìÁ∏ÆÊèêÁ§∫Áõ∏ÊØîÔºåLLM ‰øùÁïô‰∫Ü 62.26-72.89% ÁöÑËÉΩÂäõ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÈÇÑË°®ÊòéÔºå‰∏¶ÈùûÊâÄÊúâÂ£ìÁ∏ÆÁöÑÁ¨¶ËôüÈÉΩÂæóÂà∞Âπ≥Á≠âÂà©Áî®Ôºå‰∏¶‰∏î K V ÂÄºÂú®È´òÂ£ìÁ∏ÆÁéá‰∏ãÊØîÂµåÂÖ•ÂºèÊõ¥ÂÖ∑ÂÇô‰øùÁïôË≥áË®äÁöÑÂÑ™Âã¢„ÄÇÂç≥‰ΩøÂ∞çÊñºÁ¥∞Á∑ªË§áÈõúÁöÑË≥áË®äÔºåËá™ÁÑ∂Ë™ûË®ÄÊèêÁ§∫ÁöÑÈ´òÂ∫¶Â£ìÁ∏ÆÁâπÊÄß‰πüÈ°ØÁ§∫Âá∫Êú™‰æÜÊáâÁî®ÂíåÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÈñãÁôºÊñ∞ÁöÑ LLM Ë™ûË®ÄÁöÑÊΩõÂäõ„ÄÇ

##### **Learning Provably Robust Policies in Uncertain Parametric Environments**
2408.03093v1 by Yannik Schnitzer, Alessandro Abate, David Parker

We present a data-driven approach for learning MDP policies that are robust
across stochastic environments whose transition probabilities are defined by
parameters with an unknown distribution. We produce probably approximately
correct (PAC) guarantees for the performance of these learned policies in a
new, unseen environment over the unknown distribution. Our approach is based on
finite samples of the MDP environments, for each of which we build an
approximation of the model as an interval MDP, by exploring a set of generated
trajectories. We use the built approximations to synthesise a single policy
that performs well (meets given requirements) across the sampled environments,
and furthermore bound its risk (of not meeting the given requirements) when
deployed in an unseen environment. Our procedure offers a trade-off between the
guaranteed performance of the learned policy and the risk of not meeting the
guarantee in an unseen environment. Our approach exploits knowledge of the
environment's state space and graph structure, and we show how additional
knowledge of its parametric structure can be leveraged to optimize learning and
to obtain tighter guarantees from less samples. We evaluate our approach on a
diverse range of established benchmarks, demonstrating that we can generate
highly performing and robust policies, along with guarantees that tightly
quantify their performance and the associated risk.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãË≥áÊñôÈ©ÖÂãïÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ≠∏ÁøíÂú®Èö®Ê©üÁí∞Â¢É‰∏≠Á©©ÂÅ•ÁöÑ MDP ÊîøÁ≠ñÔºåÂÖ∂ËΩâÊèõÊ©üÁéáÁî±ÂÖ∑ÊúâÊú™Áü•ÂàÜ‰ΩàÁöÑÂèÉÊï∏ÂÆöÁæ©„ÄÇÊàëÂÄëÁÇ∫ÈÄô‰∫õÂ≠∏ÁøíÊîøÁ≠ñÂú®Êú™Áü•ÂàÜ‰ΩàÁöÑÊñ∞Áí∞Â¢É‰∏≠Áî¢ÁîüÁöÑÊïàËÉΩÔºåÊèê‰æõËøë‰ººÊ≠£Á¢∫ (PAC) ÁöÑ‰øùË≠â„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊòØÂü∫Êñº MDP Áí∞Â¢ÉÁöÑÊúâÈôêÊ®£Êú¨ÔºåÂ∞çÊñºÊØèÂÄãÊ®£Êú¨ÔºåÊàëÂÄëÈÄèÈÅéÊé¢Á¥¢‰∏ÄÁµÑÂ∑≤Áî¢ÁîüÁöÑËªåË∑°ÔºåÂª∫Á´ã‰∏ÄÂÄãÂçÄÈñì MDP Ê®°ÂûãËøë‰ººÂÄº„ÄÇÊàëÂÄë‰ΩøÁî®Âª∫Á´ãÁöÑËøë‰ººÂÄº‰æÜÁ∂úÂêà‰∏ÄÂÄãÂú®Êé°Ê®£Áí∞Â¢É‰∏≠Ë°®ÁèæËâØÂ•ΩÁöÑÂñÆ‰∏ÄÊîøÁ≠ñÔºàÁ¨¶ÂêàÁµ¶ÂÆöÁöÑÈúÄÊ±ÇÔºâÔºå‰∏¶‰∏îÈÄ≤‰∏ÄÊ≠•ÈôêÂà∂ÂÖ∂È¢®Èö™Ôºà‰∏çÁ¨¶ÂêàÁµ¶ÂÆöÈúÄÊ±ÇÔºâÊôÇÔºåÂú®Êú™Ë¶ãÁí∞Â¢É‰∏≠ÈÉ®ÁΩ≤„ÄÇÊàëÂÄëÁöÑÁ®ãÂ∫èÂú®Â≠∏ÁøíÊîøÁ≠ñÁöÑ‰øùË≠âÊïàËÉΩËàáÂú®Êú™Ë¶ãÁí∞Â¢É‰∏≠‰∏çÁ¨¶Âêà‰øùË≠âÁöÑÈ¢®Èö™‰πãÈñìÔºåÊèê‰æõ‰∏ÄÂÄãÊäòË°∑ÊñπÊ°à„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂà©Áî®Áí∞Â¢ÉÁãÄÊÖãÁ©∫ÈñìÂíåÂúñÂΩ¢ÁµêÊßãÁöÑÁü•Ë≠òÔºå‰∏¶‰∏îÊàëÂÄëÂ±ïÁ§∫Â¶Ç‰ΩïÂà©Áî®ÂÖ∂ÂèÉÊï∏ÁµêÊßãÁöÑÈ°çÂ§ñÁü•Ë≠òÔºå‰æÜÊúÄ‰Ω≥ÂåñÂ≠∏Áøí‰∏¶ÂæûÊõ¥Â∞ëÁöÑÊ®£Êú¨‰∏≠Áç≤ÂæóÊõ¥Âö¥Ê†ºÁöÑ‰øùË≠â„ÄÇÊàëÂÄëÂú®ÂêÑÁ®ÆÂ∑≤Âª∫Á´ãÁöÑÂü∫Ê∫ñ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåË≠âÊòéÊàëÂÄëÂèØ‰ª•Áî¢ÁîüÈ´òÂü∑Ë°åÊïàËÉΩÂíåÁ©©ÂÅ•ÁöÑÊîøÁ≠ñÔºå‰ª•ÂèäÂö¥Ê†ºÈáèÂåñÂÖ∂ÊïàËÉΩÂíåÁõ∏ÈóúÈ¢®Èö™ÁöÑ‰øùË≠â„ÄÇ</paragraph>

##### **Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement**
2408.03092v1 by Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li

Merging Large Language Models (LLMs) aims to amalgamate multiple homologous
LLMs into one with all the capabilities. Ideally, any LLMs sharing the same
backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT)
with minor parameter changes or Pre-Trained (PT) with substantial parameter
shifts. However, existing methods often manually assign the model importance,
rendering them feasible only for LLMs with similar parameter alterations, such
as multiple FT LLMs. The diverse parameter changed ranges between FT and PT
LLMs pose challenges for current solutions in empirically determining the
optimal combination. In this paper, we make a pioneering effort to broaden the
applicability of merging techniques from FT to PT LLMs. We initially examine
the efficacy of current methods in merging FT and PT LLMs, discovering that
they struggle to deal with PT LLMs. Subsequently, we introduce an approach
based on WeIght DisENtanglement (WIDEN) to effectively extend the merging
scope, which first disentangles model weights into magnitude and direction
components, and then performs adaptive fusion by considering their respective
contributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with
instruction-following skills) with Sailor (a PT LLM with multilingual
abilities) across 7B and 14B model scales. Results reveal that: (1) existing
solutions usually fail when merging Sailor, either losing both abilities or
only retaining instruction-following skills; (2) WIDEN successfully injects the
multilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in
Southeast Asian languages, achieving enhancements in the fundamental
capabilities. In light of previous research, we also merge multiple 13B FT LLMs
and observe that WIDEN achieves a balanced amalgamation of instruction
following, mathematical reasoning, and code generation skills.

ÊëòË¶ÅÔºöÂêà‰ΩµÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êó®Âú®Â∞áÂ§öÂÄãÂêåÊ∫ê LLM Êï¥ÂêàÁÇ∫‰∏ÄÂÄãÂÖ∑ÂÇôÊâÄÊúâÂäüËÉΩÁöÑ LLM„ÄÇÁêÜÊÉ≥ÊÉÖÊ≥Å‰∏ãÔºå‰ªª‰ΩïÂÖ±‰∫´Áõ∏Âêå‰∏ªÂππÁöÑ LLM ÈÉΩÊáâË©≤ÊòØÂèØÂêà‰ΩµÁöÑÔºåÁÑ°Ë´ñÂÆÉÂÄëÊòØÂê¶Á∂ìÈÅéÂæÆË™ø (FT) ‰ª•ÈÄ≤Ë°åËºÉÂ∞èÁöÑÂèÉÊï∏ËÆäÊõ¥ÔºåÊàñÁ∂ìÈÅéÈ†êË®ìÁ∑¥ (PT) ‰ª•ÈÄ≤Ë°åÂ§ßÂπÖÂ∫¶ÁöÑÂèÉÊï∏ËΩâÁßª„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÊâãÂãïÊåáÂÆöÊ®°ÂûãÈáçË¶ÅÊÄßÔºå‰ΩøÂÖ∂ÂÉÖÈÅ©Áî®ÊñºÂèÉÊï∏ËÆäÊõ¥Áõ∏‰ººÁöÑ LLMÔºå‰æãÂ¶ÇÂ§öÂÄã FT LLM„ÄÇFT Âíå PT LLM ‰πãÈñì‰∏çÂêåÁöÑÂèÉÊï∏ËÆäÊõ¥ÁØÑÂúçÂ∞çÁï∂ÂâçËß£Ê±∫ÊñπÊ°àÂú®Á∂ìÈ©ó‰∏äÁ¢∫ÂÆöÊúÄ‰Ω≥ÁµÑÂêàÊßãÊàêÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂÅöÂá∫‰∫ÜÈñãÂâµÊÄßÁöÑÂä™ÂäõÔºåÂ∞áÂêà‰ΩµÊäÄË°ìÁöÑÈÅ©Áî®ÊÄßÂæû FT Êì¥Â±ïÂà∞ PT LLM„ÄÇÊàëÂÄëÊúÄÂàùÊ™¢Êü•‰∫ÜÁï∂ÂâçÊñπÊ≥ïÂú®Âêà‰Ωµ FT Âíå PT LLM ‰∏≠ÁöÑÂäüÊïàÔºåÁôºÁèæÂÆÉÂÄëÈõ£‰ª•ËôïÁêÜ PT LLM„ÄÇÈö®ÂæåÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÊ¨äÈáçËß£Èñã (WIDEN) ÁöÑÊñπÊ≥ï‰æÜÊúâÊïàÊì¥Â±ïÂêà‰ΩµÁØÑÂúçÔºåË©≤ÊñπÊ≥ïÈ¶ñÂÖàÂ∞áÊ®°ÂûãÊ¨äÈáçËß£ÈñãÁÇ∫Â§ßÂ∞èÂíåÊñπÂêëÂàÜÈáèÔºåÁÑ∂ÂæåÈÄöÈÅéËÄÉÊÖÆÂÆÉÂÄëÂêÑËá™ÁöÑË≤¢Áçª‰æÜÂü∑Ë°åËá™ÈÅ©ÊáâËûçÂêà„ÄÇÂú®ÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëÂ∞á Qwen1.5-ChatÔºàÂÖ∑ÊúâÊåá‰ª§ÈÅµÂæ™ÊäÄËÉΩÁöÑ FT LLMÔºâËàá SailorÔºàÂÖ∑ÊúâÂ§öË™ûË®ÄËÉΩÂäõÁöÑ PT LLMÔºâÂêà‰ΩµÂú® 7B Âíå 14B Ê®°ÂûãË¶èÊ®°‰∏≠„ÄÇÁµêÊûúË°®ÊòéÔºö(1) ÁèæÊúâÁöÑËß£Ê±∫ÊñπÊ°àÂú®Âêà‰Ωµ Sailor ÊôÇÈÄöÂ∏∏ÊúÉÂ§±ÊïóÔºåË¶ÅÈ∫ºÂ§±ÂéªÂÖ©Á®ÆËÉΩÂäõÔºåË¶ÅÈ∫ºÂÉÖ‰øùÁïôÊåá‰ª§ÈÅµÂæ™ÊäÄËÉΩÔºõ(2) WIDEN ÊàêÂäüÂú∞Â∞á Sailor ÁöÑÂ§öË™ûË®ÄËÉΩÂäõÊ≥®ÂÖ• Qwen1.5-ChatÔºå‰ΩøÂÖ∂Á≤æÈÄöÊù±Âçó‰∫ûË™ûË®ÄÔºå‰∏¶Â¢ûÂº∑‰∫ÜÂü∫Êú¨ËÉΩÂäõ„ÄÇÊ†πÊìöÂÖàÂâçÁöÑÁ†îÁ©∂ÔºåÊàëÂÄëÈÇÑÂêà‰Ωµ‰∫ÜÂ§öÂÄã 13B FT LLMÔºå‰∏¶ËßÄÂØüÂà∞ WIDEN ÈÅîÂà∞‰∫ÜÊåá‰ª§ÈÅµÂæ™„ÄÅÊï∏Â≠∏Êé®ÁêÜÂíåÁ®ãÂºèÁ¢ºÁîüÊàêÊäÄËÉΩÁöÑÂπ≥Ë°°ËûçÂêà„ÄÇ

##### **Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion**
2408.03079v1 by Jinglong Gao, Chen Lu, Xiao Ding, Zhongyang Li, Ting Liu, Bing Qin

Event Causality Extraction (ECE) aims at extracting causal event pairs from
texts. Despite ChatGPT's recent success, fine-tuning small models remains the
best approach for the ECE task. However, existing fine-tuning based ECE methods
cannot address all three key challenges in ECE simultaneously: 1) Complex
Causality Extraction, where multiple causal-effect pairs occur within a single
sentence; 2) Subtask~ Interaction, which involves modeling the mutual
dependence between the two subtasks of ECE, i.e., extracting events and
identifying the causal relationship between extracted events; and 3) Knowledge
Fusion, which requires effectively fusing the knowledge in two modalities,
i.e., the expressive pretrained language models and the structured knowledge
graphs. In this paper, we propose a unified ECE framework (UniCE to address all
three issues in ECE simultaneously. Specifically, we design a subtask
interaction mechanism to enable mutual interaction between the two ECE
subtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in
the two modalities. Furthermore, we employ separate decoders for each subtask
to facilitate complex causality extraction. Experiments on three benchmark
datasets demonstrate that our method achieves state-of-the-art performance and
outperforms ChatGPT with a margin of at least 30% F1-score. More importantly,
our model can also be used to effectively improve the ECE performance of
ChatGPT via in-context learning.

ÊëòË¶ÅÔºö‰∫ã‰ª∂Âõ†ÊûúÈóú‰øÇËêÉÂèñ (ECE) ÁöÑÁõÆÊ®ôÊòØÂæûÊñáÊú¨‰∏≠ËêÉÂèñÂá∫Âõ†Êûú‰∫ã‰ª∂Â∞ç„ÄÇÂÑòÁÆ° ChatGPT ÊúÄËøëÁç≤ÂæóÊàêÂäüÔºåÂæÆË™øÂ∞èÂûãÊ®°Âûã‰ªçÊòØ ECE ‰ªªÂãôÁöÑÊúÄ‰Ω≥ÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂü∫ÊñºÂæÆË™øÁöÑ ECE ÊñπÊ≥ïÁÑ°Ê≥ïÂêåÊôÇËß£Ê±∫ ECE ‰∏≠ÁöÑ‰∏âÂÄã‰∏ªË¶ÅÊåëÊà∞Ôºö1) Ë§áÈõúÂõ†ÊûúÈóú‰øÇËêÉÂèñÔºåÂÖ∂‰∏≠Â§öÂÄãÂõ†ÊûúÈóú‰øÇÂ∞çÂá∫ÁèæÂú®ÂñÆ‰∏ÄÂè•Â≠ê‰∏≠Ôºõ2) Â≠ê‰ªªÂãô‰∫íÂãïÔºåÈÄôÊ∂âÂèäÂ∞ç ECE ÁöÑÂÖ©ÂÄãÂ≠ê‰ªªÂãôÔºàÂç≥ËêÉÂèñ‰∫ã‰ª∂ÂíåË≠òÂà•ËêÉÂèñ‰∫ã‰ª∂‰πãÈñìÁöÑÂõ†ÊûúÈóú‰øÇÔºâ‰πãÈñìÁöÑÁõ∏‰∫í‰æùË≥¥ÊÄßÈÄ≤Ë°åÂª∫Ê®°Ôºõ3) Áü•Ë≠òËûçÂêàÔºåÈÄôÈúÄË¶ÅÊúâÊïàÂú∞ËûçÂêàÂÖ©Á®ÆÊ®°Âºè‰∏≠ÁöÑÁü•Ë≠òÔºåÂç≥Ë°®ÈÅîÂºèÁöÑÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÂíåÁµêÊßãÂåñÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁµ±‰∏ÄÁöÑ ECE Ê°ÜÊû∂ (UniCE)Ôºå‰ª•ÂêåÊôÇËß£Ê±∫ ECE ‰∏≠ÁöÑÊâÄÊúâ‰∏âÂÄãÂïèÈ°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂ≠ê‰ªªÂãô‰∫íÂãïÊ©üÂà∂Ôºå‰ª•ÂØ¶ÁèæÂÖ©ÂÄã ECE Â≠ê‰ªªÂãô‰πãÈñìÁöÑÁõ∏‰∫í‰∫íÂãï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÁü•Ë≠òËûçÂêàÊ©üÂà∂‰æÜËûçÂêàÂÖ©Á®ÆÊ®°Âºè‰∏≠ÁöÑÁü•Ë≠ò„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈáùÂ∞çÊØèÂÄãÂ≠ê‰ªªÂãôÊé°Áî®ÂñÆÁç®ÁöÑËß£Á¢ºÂô®Ôºå‰ª•‰øÉÈÄ≤Ë§áÈõúÂõ†ÊûúÈóú‰øÇÁöÑËêÉÂèñ„ÄÇÂú®‰∏âÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºå‰∏¶‰∏î‰ª•Ëá≥Â∞ë 30% ÁöÑ F1 ÂàÜÊï∏ÂÑ™Êñº ChatGPT„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÁöÑÊ®°Âûã‰πüÂèØ‰ª•ÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏ÁøíÊúâÊïàÂú∞ÊèêÂçá ChatGPT ÁöÑ ECE ÊïàËÉΩ„ÄÇ

##### **BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications**
2408.03078v1 by G. Manni, C. Lauretti, F. Prata, R. Papalia, L. Zollo, P. Soda

Endoscopic surgery relies on two-dimensional views, posing challenges for
surgeons in depth perception and instrument manipulation. While Simultaneous
Localization and Mapping (SLAM) has emerged as a promising solution to address
these limitations, its implementation in endoscopic procedures presents
significant challenges due to hardware limitations, such as the use of a
monocular camera and the absence of odometry sensors. This study presents a
robust deep learning-based SLAM approach that combines state-of-the-art and
newly developed models. It consists of three main parts: the Monocular Pose
Estimation Module that introduces a novel unsupervised method based on the
CycleGAN architecture, the Monocular Depth Estimation Module that leverages the
novel Zoe architecture, and the 3D Reconstruction Module which uses information
from the previous models to create a coherent surgical map. The performance of
the procedure was rigorously evaluated using three publicly available datasets
(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art
methods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM
demonstrated superior performance compared to state-of-the-art depth estimation
algorithms in endoscopy, whereas the novel approach in the MPEM exhibited
competitive performance and the lowest inference time. The results showcase the
robustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three
different scenarios in endoscopic surgery. The proposed SLAM approach has the
potential to improve the accuracy and efficiency of endoscopic procedures by
providing surgeons with enhanced depth perception and 3D reconstruction
capabilities.

ÊëòË¶ÅÔºöÂÖßË¶ñÈè°ÊâãË°ì‰æùË≥¥Êñº‰∫åÁ∂≠Ë¶ñÂúñÔºåÂ∞çÂ§ñÁßëÈÜ´ÁîüÂú®Ê∑±Â∫¶ÊÑüÁü•ÂíåÂô®Ê¢∞Êìç‰ΩúÊñπÈù¢ÊßãÊàêÊåëÊà∞„ÄÇÈõñÁÑ∂ÂêåÊôÇÂÆö‰ΩçËàáÂª∫Âúñ (SLAM) Â∑≤ÊàêÁÇ∫Ëß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÁöÑÊúâÂ∏åÊúõÁöÑÊñπÊ°àÔºå‰ΩÜÁî±ÊñºÁ°¨È´îÈôêÂà∂Ôºå‰æãÂ¶Ç‰ΩøÁî®ÂñÆÁúºÁõ∏Ê©üÂíåÁº∫‰πèÈáåÁ®ãË®àÊÑüÊ∏¨Âô®ÔºåÂÖ∂Âú®ÂÖßË¶ñÈè°ÊâãË°ì‰∏≠ÁöÑÂØ¶ÊñΩÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂº∑Â§ßÁöÑÊ∑±Â∫¶Â≠∏ÁøíÂºè SLAM ÊñπÊ≥ïÔºåÁµêÂêà‰∫ÜÊúÄÂÖàÈÄ≤ÂíåÊñ∞ÈñãÁôºÁöÑÊ®°Âûã„ÄÇÂÆÉÂåÖÂê´‰∏âÂÄã‰∏ªË¶ÅÈÉ®ÂàÜÔºöÂñÆÁúºÂßøÂã¢‰º∞Ë®àÊ®°ÁµÑÔºåÂºïÂÖ•‰∫ÜÂü∫Êñº CycleGAN Êû∂ÊßãÁöÑÊñ∞ÂûãÈùûÁõ£Áù£ÂºèÊñπÊ≥ïÔºõÂñÆÁúºÊ∑±Â∫¶‰º∞Ë®àÊ®°ÁµÑÔºåÂà©Áî®‰∫ÜÊñ∞Âûã Zoe Êû∂ÊßãÔºõ‰ª•Âèä 3D ÈáçÂª∫Ê®°ÁµÑÔºåÂÆÉ‰ΩøÁî®Ââç‰∏ÄÊ®°Âûã‰∏≠ÁöÑË≥áË®ä‰æÜÂª∫Á´ã‰∏ÄÂÄãÈÄ£Ë≤´ÁöÑÊâãË°ìÂú∞Âúñ„ÄÇ‰ΩøÁî®‰∏âÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑË≥áÊñôÈõÜÔºàHamlyn„ÄÅEndoSLAM Âíå SCAREDÔºâÂö¥Ê†ºË©ï‰º∞‰∫ÜË©≤ÊâãË°ìÁöÑÊïàËÉΩÔºå‰∏¶Ê†πÊìöÂÖ©ÂÄãÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï EndoSFMLearner Âíå EndoDepth ÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇËàáÂÖßË¶ñÈè°‰∏≠ÊúÄÂÖàÈÄ≤ÁöÑÊ∑±Â∫¶‰º∞Ë®àÊºîÁÆóÊ≥ïÁõ∏ÊØîÔºåZoe Âú® MDEM ‰∏≠ÁöÑÊï¥ÂêàË°®ÁèæÂá∫ÂÑ™Áï∞ÁöÑÊïàËÉΩÔºåËÄå MPEM ‰∏≠ÁöÑÊñ∞ÊñπÊ≥ïË°®ÁèæÂá∫Á´∂Áà≠Âäõ‰∏îÊé®Ë´ñÊôÇÈñìÊúÄÁü≠„ÄÇÁµêÊûúÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ËÖπËÖîÈè°Ê™¢Êü•„ÄÅËÉÉÈè°Ê™¢Êü•ÂíåÁµêËÖ∏Èè°Ê™¢Êü•‰∏≠ÁöÑÁ©©ÂÅ•ÊÄßÔºåÈÄôÊòØÂÖßË¶ñÈè°ÊâãË°ì‰∏≠ÁöÑ‰∏âÁ®Æ‰∏çÂêåÂ†¥ÊôØ„ÄÇÊâÄÊèêÂá∫ÁöÑ SLAM ÊñπÊ≥ïÊúâÂèØËÉΩÈÄèÈÅéÁÇ∫Â§ñÁßëÈÜ´ÁîüÊèê‰æõÂ¢ûÂº∑ÁöÑÊ∑±Â∫¶ÊÑüÁü•Âíå 3D ÈáçÂª∫ËÉΩÂäõÔºå‰æÜÊèêÈ´òÂÖßË¶ñÈè°ÊâãË°ìÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊïàÁéá„ÄÇ

##### **Towards an Analysis of Discourse and Interactional Pragmatic Reasoning Capabilities of Large Language Models**
2408.03074v1 by Amelie Robrecht, Judith Sieker, Clara Lachenmaier, Sina Zarie√ü, Stefan Kopp

In this work, we want to give an overview on which pragmatic abilities have
been tested in LLMs so far and how these tests have been carried out. To do
this, we first discuss the scope of the field of pragmatics and suggest a
subdivision into discourse pragmatics and interactional pragmatics. We give a
non-exhaustive overview of the phenomena of those two subdomains and the
methods traditionally used to analyze them. We subsequently consider the
resulting heterogeneous set of phenomena and methods as a starting point for
our survey of work on discourse pragmatics and interactional pragmatics in the
context of LLMs.

ÊëòË¶ÅÔºöÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∏åÊúõÊ¶ÇËø∞Âà∞ÁõÆÂâçÁÇ∫Ê≠¢Âú® LLM ‰∏≠Ê∏¨Ë©¶ÈÅéÂì™‰∫õË™ûÁî®ËÉΩÂäõÔºå‰ª•ÂèäÈÄô‰∫õÊ∏¨Ë©¶ÊòØÂ¶Ç‰ΩïÈÄ≤Ë°åÁöÑ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÈ¶ñÂÖàË®éË´ñË™ûÁî®Â≠∏È†òÂüüÁöÑÁØÑÂúçÔºå‰∏¶Âª∫Ë≠∞Â∞áÂÖ∂Á¥∞ÂàÜÁÇ∫Ë™ûÁØáË™ûÁî®Â≠∏Âíå‰∫íÂãïË™ûÁî®Â≠∏„ÄÇÊàëÂÄëÂ∞çÈÄôÂÖ©ÂÄãÂ≠êÈ†òÂüüÁöÑÁèæË±°‰ª•ÂèäÂÇ≥Áµ±‰∏äÁî®ÊñºÂàÜÊûêÂÆÉÂÄëÁöÑÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÈùûË©≥Áõ°ÁöÑÊ¶ÇËø∞„ÄÇÈö®ÂæåÔºåÊàëÂÄëÂ∞áÁî±Ê≠§Áî¢ÁîüÁöÑÁï∞Ë≥™ÁèæË±°ÂíåÊñπÊ≥ïÈõÜÂêà‰ΩúÁÇ∫ÊàëÂÄëÂú® LLM ËÉåÊôØ‰∏ãÂ∞çË™ûÁØáË™ûÁî®Â≠∏Âíå‰∫íÂãïË™ûÁî®Â≠∏Â∑•‰ΩúÁöÑË™øÊü•ÁöÑËµ∑Èªû„ÄÇ

##### **Probing structural constraints of negation in Pretrained Language Models**
2408.03070v1 by David Kletz, Marie Candito, Pascal Amsili

Contradictory results about the encoding of the semantic impact of negation
in pretrained language models (PLMs). have been drawn recently (e.g. Kassner
and Sch{\"u}tze (2020); Gubelmann and Handschuh (2022)). In this paper we focus
rather on the way PLMs encode negation and its formal impact, through the
phenomenon of the Negative Polarity Item (NPI) licensing in English. More
precisely, we use probes to identify which contextual representations best
encode 1) the presence of negation in a sentence, and 2) the polarity of a
neighboring masked polarity item. We find that contextual representations of
tokens inside the negation scope do allow for (i) a better prediction of the
presence of not compared to those outside the scope and (ii) a better
prediction of the right polarity of a masked polarity item licensed by not,
although the magnitude of the difference varies from PLM to PLM. Importantly,
in both cases the trend holds even when controlling for distance to not. This
tends to indicate that the embeddings of these models do reflect the notion of
negation scope, and do encode the impact of negation on NPI licensing. Yet,
further control experiments reveal that the presence of other lexical items is
also better captured when using the contextual representation of a token within
the same syntactic clause than outside from it, suggesting that PLMs simply
capture the more general notion of syntactic clause.

ÊëòË¶ÅÔºöÊúÄËøëÂÖ≥‰∫éÂê¶ÂÆöËØ≠‰πâÂΩ±ÂìçÁºñÁ†ÅÁöÑÁüõÁõæÁªìÊûúÂú®È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã (PLM) ‰∏≠Ë¢´ÊèêÂá∫Ôºà‰æãÂ¶Ç Kassner Âíå Schutze (2020)ÔºõGubelmann Âíå Handschuh (2022)Ôºâ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Êõ¥‰∏ìÊ≥®‰∫é PLM ÁºñÁ†ÅÂê¶ÂÆöÂèäÂÖ∂ÂΩ¢ÂºèÂΩ±ÂìçÁöÑÊñπÂºèÔºåÈÄöËøáËã±ËØ≠‰∏≠Âê¶ÂÆöÊûÅÊÄßÈ°πÁõÆ (NPI) ËÆ∏ÂèØÁöÑÁé∞Ë±°„ÄÇÊõ¥ÂáÜÁ°ÆÂú∞ËØ¥ÔºåÊàë‰ª¨‰ΩøÁî®Êé¢ÊµãÊù•ËØÜÂà´Âì™‰∫õ‰∏ä‰∏ãÊñáË°®Á§∫ÊúÄËÉΩÁºñÁ†Å 1) Âè•Â≠ê‰∏≠Âê¶ÂÆöÁöÑÂ≠òÂú®Ôºå‰ª•Âèä 2) Áõ∏ÈÇªÊé©Á†ÅÊûÅÊÄßÈ°πÁõÆÁöÑÊûÅÊÄß„ÄÇÊàë‰ª¨ÂèëÁé∞Âê¶ÂÆöËåÉÂõ¥ÂÜÖÁöÑÊ†áËÆ∞ÁöÑ‰∏ä‰∏ãÊñáË°®Á§∫Á°ÆÂÆûÂÖÅËÆ∏ (i) ‰∏éËåÉÂõ¥Â§ñÁöÑÊ†áËÆ∞Áõ∏ÊØîÔºåÊõ¥Â•ΩÂú∞È¢ÑÊµã not ÁöÑÂ≠òÂú®Ôºå‰ª•Âèä (ii) Êõ¥Â•ΩÂú∞È¢ÑÊµãÁî± not ËÆ∏ÂèØÁöÑÊé©Á†ÅÊûÅÊÄßÈ°πÁõÆÁöÑÊ≠£Á°ÆÊûÅÊÄßÔºåÂ∞ΩÁÆ°Â∑ÆÂºÇÁöÑÂπÖÂ∫¶Âõ† PLM ËÄåÂºÇ„ÄÇÈáçË¶ÅÁöÑÊòØÔºåÂú®Ëøô‰∏§ÁßçÊÉÖÂÜµ‰∏ãÔºåÂç≥‰ΩøÂú®ÊéßÂà∂‰∏é not ÁöÑË∑ùÁ¶ªÊó∂ÔºåË∂ãÂäø‰ªçÁÑ∂ÊàêÁ´ã„ÄÇËøôÂÄæÂêë‰∫éË°®ÊòéËøô‰∫õÊ®°ÂûãÁöÑÂµåÂÖ•Á°ÆÂÆûÂèçÊò†‰∫ÜÂê¶ÂÆöËåÉÂõ¥ÁöÑÊ¶ÇÂøµÔºåÂπ∂Á°ÆÂÆûÂØπÂê¶ÂÆöÂØπ NPI ËÆ∏ÂèØÁöÑÂΩ±ÂìçËøõË°åÁºñÁ†Å„ÄÇÁÑ∂ËÄåÔºåËøõ‰∏ÄÊ≠•ÁöÑÊéßÂà∂ÂÆûÈ™åË°®ÊòéÔºåÂú®Âêå‰∏ÄËØ≠Ê≥ïÂ≠êÂè•‰∏≠‰ΩøÁî®Ê†áËÆ∞ÁöÑ‰∏ä‰∏ãÊñáË°®Á§∫ÊØîÂú®Â≠êÂè•Â§ñ‰ΩøÁî®Ê†áËÆ∞ÁöÑ‰∏ä‰∏ãÊñáË°®Á§∫Êó∂ÔºåÂÖ∂‰ªñËØçÊ≥ïÈ°πÁõÆÁöÑÂá∫Áé∞‰πüËÉΩÂæóÂà∞Êõ¥Â•ΩÁöÑÊçïÊçâÔºåËøôË°®Êòé PLM ‰ªÖ‰ªÖÊçïÊçâÂà∞ËØ≠Ê≥ïÂ≠êÂè•ÁöÑÊõ¥‰∏ÄËà¨Ê¶ÇÂøµ„ÄÇ

##### **Analysis of Argument Structure Constructions in a Deep Recurrent Language Model**
2408.03062v1 by Pegah Ramezani, Achim Schilling, Patrick Krauss

Understanding how language and linguistic constructions are processed in the
brain is a fundamental question in cognitive computational neuroscience. In
this study, we explore the representation and processing of Argument Structure
Constructions (ASCs) in a recurrent neural language model. We trained a Long
Short-Term Memory (LSTM) network on a custom-made dataset consisting of 2000
sentences, generated using GPT-4, representing four distinct ASCs: transitive,
ditransitive, caused-motion, and resultative constructions.
  We analyzed the internal activations of the LSTM model's hidden layers using
Multidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding
(t-SNE) to visualize the sentence representations. The Generalized
Discrimination Value (GDV) was calculated to quantify the degree of clustering
within these representations. Our results show that sentence representations
form distinct clusters corresponding to the four ASCs across all hidden layers,
with the most pronounced clustering observed in the last hidden layer before
the output layer. This indicates that even a relatively simple,
brain-constrained recurrent neural network can effectively differentiate
between various construction types.
  These findings are consistent with previous studies demonstrating the
emergence of word class and syntax rule representations in recurrent language
models trained on next word prediction tasks. In future work, we aim to
validate these results using larger language models and compare them with
neuroimaging data obtained during continuous speech perception. This study
highlights the potential of recurrent neural language models to mirror
linguistic processing in the human brain, providing valuable insights into the
computational and neural mechanisms underlying language understanding.

ÊëòË¶ÅÔºö<paragraph>‰∫ÜËß£Ë™ûË®ÄÂíåË™ûË®ÄÂª∫ÊßãÂ¶Ç‰ΩïÂú®ËÖ¶‰∏≠Ë¢´ËôïÁêÜÔºåÊòØË™çÁü•Ë®àÁÆóÁ•ûÁ∂ìÁßëÂ≠∏‰∏≠ÁöÑÂü∫Êú¨ÂïèÈ°å„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Á¥¢‰∫ÜÈÅûËø¥Á•ûÁ∂ìË™ûË®ÄÊ®°Âûã‰∏≠Ë´ñÂÖÉÁµêÊßãÂª∫ÊßãÔºàASCÔºâÁöÑË°®Á§∫ÂíåËôïÁêÜ„ÄÇÊàëÂÄëË®ìÁ∑¥‰∫Ü‰∏ÄÂÄãÈï∑Áü≠ÊúüË®òÊÜ∂ÔºàLSTMÔºâÁ∂≤Ë∑ØÔºå‰ΩøÁî®‰∏ÄÂÄãÁî± GPT-4 ÁîüÊàêÁöÑÔºåÂåÖÂê´ 2000 ÂÄãÂè•Â≠êÁöÑËá™Ë®ÇË≥áÊñôÈõÜÔºå‰ª£Ë°®ÂõõÂÄã‰∏çÂêåÁöÑ ASCÔºöÂèäÁâ©„ÄÅÈõôÂèäÁâ©„ÄÅËá¥ÂãïÂíåÁµêÊûúÂª∫Êßã„ÄÇ
  ÊàëÂÄë‰ΩøÁî®Â§öÁ∂≠Â∫¶Á∏ÆÊîæÔºàMDSÔºâÂíå t ÂàÜÂ∏ÉÈö®Ê©üÈÑ∞ÂüüÂµåÂÖ•Ôºàt-SNEÔºâÂàÜÊûê‰∫Ü LSTM Ê®°ÂûãÈö±ËóèÂ±§ÁöÑÂÖßÈÉ®ÊøÄÊ¥ªÔºå‰ª•Ë¶ñË¶∫ÂåñÂè•Â≠êË°®Á§∫„ÄÇË®àÁÆóÂª£Áæ©Âà§Âà•ÂÄºÔºàGDVÔºâ‰ª•ÈáèÂåñÈÄô‰∫õË°®Á§∫‰∏≠ÁöÑËÅöÈ°ûÁ®ãÂ∫¶„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂè•Â≠êË°®Á§∫Âú®ÊâÄÊúâÈö±ËóèÂ±§‰∏≠ÂΩ¢ÊàêÂ∞çÊáâÊñºÂõõÂÄã ASC ÁöÑ‰∏çÂêåÁæ§ÈõÜÔºåÂú®Ëº∏Âá∫Â±§‰πãÂâçÁöÑÊúÄÂæå‰∏ÄÂÄãÈö±ËóèÂ±§‰∏≠ËßÄÂØüÂà∞ÊúÄÊòéÈ°ØÁöÑËÅöÈ°û„ÄÇÈÄôË°®ÊòéÂç≥‰ΩøÊòØ‰∏ÄÂÄãÁõ∏Â∞çÁ∞°ÂñÆ„ÄÅÂèóÂ§ßËÖ¶Á¥ÑÊùüÁöÑÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑Ø‰πüËÉΩÊúâÊïàÂçÄÂàÜÂêÑÁ®ÆÂª∫ÊßãÈ°ûÂûã„ÄÇ
  ÈÄô‰∫õÁôºÁèæËàáÂÖàÂâçÁöÑÁ†îÁ©∂‰∏ÄËá¥ÔºåÈÄô‰∫õÁ†îÁ©∂Ë≠âÊòé‰∫ÜÂú®Ë®ìÁ∑¥Êñº‰∏ã‰∏ÄÂÄãÂ≠óÈ†êÊ∏¨‰ªªÂãôÁöÑÈÅûËø¥Ë™ûË®ÄÊ®°Âûã‰∏≠ÔºåË©ûÈ°ûÂíåÂè•Ê≥ïË¶èÂâáË°®Á§∫ÁöÑÂá∫Áèæ„ÄÇÂú®Êú™‰æÜÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊó®Âú®‰ΩøÁî®Êõ¥Â§ßÁöÑË™ûË®ÄÊ®°ÂûãÈ©óË≠âÈÄô‰∫õÁµêÊûúÔºå‰∏¶Â∞áÂÆÉÂÄëËàáÂú®ÈÄ£Á∫åË™ûÈü≥ÊÑüÁü•ÈÅéÁ®ã‰∏≠Áç≤ÂæóÁöÑÁ•ûÁ∂ìÂΩ±ÂÉèÊï∏ÊìöÈÄ≤Ë°åÊØîËºÉ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÈÅûËø¥Á•ûÁ∂ìË™ûË®ÄÊ®°ÂûãÂèçÊò†‰∫∫ËÖ¶‰∏≠Ë™ûË®ÄËôïÁêÜÁöÑÊΩõÂäõÔºåÁÇ∫Ë™ûË®ÄÁêÜËß£ÁöÑË®àÁÆóÂíåÁ•ûÁ∂ìÊ©üÂà∂Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË¶ãËß£„ÄÇ</paragraph>

##### **OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents**
2408.03047v1 by Qiang Sun, Yuanyi Luo, Sirui Li, Wenxiao Zhang, Wei Liu

Multimodal conversational agents are highly desirable because they offer
natural and human-like interaction. However, there is a lack of comprehensive
end-to-end solutions to support collaborative development and benchmarking.
While proprietary systems like GPT-4o and Gemini demonstrating impressive
integration of audio, video, and text with response times of 200-250ms,
challenges remain in balancing latency, accuracy, cost, and data privacy. To
better understand and quantify these issues, we developed OpenOmni, an
open-source, end-to-end pipeline benchmarking tool that integrates advanced
technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented
Generation, Large Language Models, along with the ability to integrate
customized models. OpenOmni supports local and cloud deployment, ensuring data
privacy and supporting latency and accuracy benchmarking. This flexible
framework allows researchers to customize the pipeline, focusing on real
bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can
significantly enhance applications like indoor assistance for visually impaired
individuals, advancing human-computer interaction. Our demonstration video is
available https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via
https://openomni.ai4wa.com, code is available via
https://github.com/AI4WA/OpenOmniFramework.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂØπËØù‰ª£ÁêÜÊûÅÂÖ∑Âê∏ÂºïÂäõÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Êèê‰æõËá™ÁÑ∂‰∏îÁ±ª‰ºº‰∫∫Á±ªÁöÑ‰∫íÂä®„ÄÇÁÑ∂ËÄåÔºåÁº∫‰πèÂÖ®Èù¢ÁöÑÁ´ØÂà∞Á´ØËß£ÂÜ≥ÊñπÊ°àÊù•ÊîØÊåÅÂçè‰ΩúÂºÄÂèëÂíåÂü∫ÂáÜÊµãËØï„ÄÇËôΩÁÑ∂ÂÉè GPT-4o Âíå Gemini ËøôÊ†∑ÁöÑ‰∏ìÊúâÁ≥ªÁªüÂ±ïÁ§∫‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈü≥È¢ë„ÄÅËßÜÈ¢ëÂíåÊñáÊú¨ÈõÜÊàêÔºåÂìçÂ∫îÊó∂Èó¥‰∏∫ 200-250 ÊØ´ÁßíÔºå‰ΩÜÂú®Âπ≥Ë°°Âª∂Ëøü„ÄÅÂáÜÁ°ÆÊÄß„ÄÅÊàêÊú¨ÂíåÊï∞ÊçÆÈöêÁßÅÊñπÈù¢‰ªçÁÑ∂Â≠òÂú®ÊåëÊàò„ÄÇ‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÈáèÂåñËøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü OpenOmniÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÁ´ØÂà∞Á´ØÁÆ°ÈÅìÂü∫ÂáÜÊµãËØïÂ∑•ÂÖ∑ÔºåÂÆÉÈõÜÊàê‰∫ÜÂÖàËøõÁöÑÊäÄÊúØÔºåÂ¶ÇËØ≠Èü≥ËΩ¨ÊñáÊú¨„ÄÅÊÉÖÊÑüÊ£ÄÊµã„ÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê„ÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰ª•ÂèäÈõÜÊàêËá™ÂÆö‰πâÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇOpenOmni ÊîØÊåÅÊú¨Âú∞Âíå‰∫ëÈÉ®ÁΩ≤ÔºåÁ°Æ‰øùÊï∞ÊçÆÈöêÁßÅÂπ∂ÊîØÊåÅÂª∂ËøüÂíåÂáÜÁ°ÆÊÄßÂü∫ÂáÜÊµãËØï„ÄÇËøô‰∏™ÁÅµÊ¥ªÁöÑÊ°ÜÊû∂ÂÖÅËÆ∏Á†îÁ©∂‰∫∫ÂëòËá™ÂÆö‰πâÁÆ°ÈÅìÔºå‰∏ìÊ≥®‰∫éÁúüÊ≠£ÁöÑÁì∂È¢àÂπ∂‰øÉËøõÂø´ÈÄüÁöÑÊ¶ÇÂøµÈ™åËØÅÂºÄÂèë„ÄÇOpenOmni ÂèØ‰ª•ÊòæËëóÂ¢ûÂº∫Â∫îÁî®Á®ãÂ∫èÔºåÂ¶Ç‰∏∫ËßÜÈöú‰∫∫Â£´Êèê‰æõÂÆ§ÂÜÖÂ∏ÆÂä©ÔºåÊé®Ëøõ‰∫∫Êú∫‰∫§‰∫í„ÄÇÊàë‰ª¨ÁöÑÊºîÁ§∫ËßÜÈ¢ëÂèØÂú® https://www.youtube.com/watch?v=zaSiT3clWqY Ëé∑ÂæóÔºåÊºîÁ§∫ÂèØÂú® https://openomni.ai4wa.com Ëé∑ÂæóÔºå‰ª£Á†ÅÂèØÂú® https://github.com/AI4WA/OpenOmniFramework Ëé∑Âæó„ÄÇ

##### **L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification & Summarization**
2408.03033v1 by Elvys Linhares Pontes, Carlos-Emiliano Gonz√°lez-Gallardo, Mohamed Benjannet, Caryn Qu, Antoine Doucet

This article details our participation (L3iTC) in the FinLLM Challenge Task
2024, focusing on two key areas: Task 1, financial text classification, and
Task 2, financial text summarization. To address these challenges, we
fine-tuned several large language models (LLMs) to optimize performance for
each task. Specifically, we used 4-bit quantization and LoRA to determine which
layers of the LLMs should be trained at a lower precision. This approach not
only accelerated the fine-tuning process on the training data provided by the
organizers but also enabled us to run the models on low GPU memory. Our
fine-tuned models achieved third place for the financial classification task
with an F1-score of 0.7543 and secured sixth place in the financial
summarization task on the official test datasets.

ÊëòË¶ÅÔºöÊú¨ÊñáË©≥Á¥∞Ë™™ÊòéÊàëÂÄëÔºàL3iTCÔºâÂèÉËàá 2024 Âπ¥ FinLLM ÊåëÊà∞‰ªªÂãôÔºåÈáçÈªûÈóúÊ≥®ÂÖ©ÂÄãÈóúÈçµÈ†òÂüüÔºö‰ªªÂãô 1ÔºåË≤°ÂãôÊñáÂ≠óÂàÜÈ°ûÔºå‰ª•Âèä‰ªªÂãô 2ÔºåË≤°ÂãôÊñáÂ≠óÊëòË¶Å„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂæÆË™ø‰∫ÜÂ§öÂÄãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•ÂÑ™ÂåñÊØèÂÄã‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ΩøÁî® 4 ‰ΩçÂÖÉÈáèÂåñÂíå LoRA ‰æÜÁ¢∫ÂÆö LLM ÁöÑÂì™‰∫õÂ±§Ê¨°Êáâ‰ª•ËºÉ‰ΩéÁöÑÁ≤æÂ∫¶ÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊ≠§ÊñπÊ≥ï‰∏çÂÉÖÂä†ÈÄü‰∫ÜÁµÑÁπîËÄÖÊèê‰æõÁöÑË®ìÁ∑¥Ë≥áÊñôÁöÑÂæÆË™øÈÅéÁ®ãÔºåÈÇÑ‰ΩøÊàëÂÄëËÉΩÂ§†Âú®‰Ωé GPU Ë®òÊÜ∂È´î‰∏äÂü∑Ë°åÊ®°Âûã„ÄÇÊàëÂÄëÂæÆË™øÁöÑÊ®°ÂûãÂú®Ë≤°ÂãôÂàÜÈ°û‰ªªÂãô‰∏≠‰ª• 0.7543 ÁöÑ F1 ÂàÜÊï∏Áç≤ÂæóÁ¨¨‰∏âÂêçÔºå‰∏¶Âú®ÂÆòÊñπÊ∏¨Ë©¶Ë≥áÊñôÈõÜ‰∏≠ÁöÑË≤°ÂãôÊëòË¶Å‰ªªÂãô‰∏≠Áç≤ÂæóÁ¨¨ÂÖ≠Âêç„ÄÇ

##### **Integrating Controllable Motion Skills from Demonstrations**
2408.03018v1 by Honghao Liao, Zhiheng Li, Ziyu Meng, Ran Song, Yibin Li, Wei Zhang

The expanding applications of legged robots require their mastery of
versatile motion skills. Correspondingly, researchers must address the
challenge of integrating multiple diverse motion skills into controllers. While
existing reinforcement learning (RL)-based approaches have achieved notable
success in multi-skill integration for legged robots, these methods often
require intricate reward engineering or are restricted to integrating a
predefined set of motion skills constrained by specific task objectives,
resulting in limited flexibility. In this work, we introduce a flexible
multi-skill integration framework named Controllable Skills Integration (CSI).
CSI enables the integration of a diverse set of motion skills with varying
styles into a single policy without the need for complex reward tuning.
Furthermore, in a hierarchical control manner, the trained low-level policy can
be coupled with a high-level Natural Language Inference (NLI) module to enable
preliminary language-directed skill control. Our experiments demonstrate that
CSI can flexibly integrate a diverse array of motion skills more
comprehensively and facilitate the transitions between different skills.
Additionally, CSI exhibits good scalability as the number of motion skills to
be integrated increases significantly.

ÊëòË¶ÅÔºöÂõõË∂≥Ê©üÂô®‰∫∫ÁöÑÊáâÁî®ÁØÑÂúç‰∏çÊñ∑Êì¥Â±ïÔºåÂõ†Ê≠§ÈúÄË¶ÅÂÆÉÂÄëÁ≤æÈÄöÂêÑÁ®ÆÈÅãÂãïÊäÄËÉΩ„ÄÇÁõ∏ÊáâÂú∞ÔºåÁ†îÁ©∂‰∫∫Âì°ÂøÖÈ†àËß£Ê±∫Â∞áÂ§öÁ®Æ‰∏çÂêåÁöÑÈÅãÂãïÊäÄËÉΩÊï¥ÂêàÂà∞ÊéßÂà∂Âô®‰∏≠ÁöÑÊåëÊà∞„ÄÇÈõñÁÑ∂ÁèæÊúâÁöÑÂü∫ÊñºÂº∑ÂåñÂ≠∏Áøí (RL) ÁöÑÊñπÊ≥ïÂú®ÂõõË∂≥Ê©üÂô®‰∫∫ÁöÑÂ§öÊäÄËÉΩÊï¥Âêà‰∏≠ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊàêÂäüÔºå‰ΩÜÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅË§áÈõúÁöÑÁçéÂãµÂ∑•Á®ãÔºåÊàñËÄÖÂÉÖÈôêÊñºÊï¥ÂêàÂèóÁâπÂÆö‰ªªÂãôÁõÆÊ®ôÁ¥ÑÊùüÁöÑÈ†êÂÆöÁæ©ÈÅãÂãïÊäÄËÉΩÈõÜÔºåÂ∞éËá¥ÈùàÊ¥ªÊÄßÂèóÈôê„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ÂèØÊéßÊäÄËÉΩÊï¥Âêà (CSI) ÁöÑÈùàÊ¥ªÂ§öÊäÄËÉΩÊï¥ÂêàÊ°ÜÊû∂„ÄÇCSI ËÉΩÂ§†Â∞áÂÖ∑Êúâ‰∏çÂêåÈ¢®Ê†ºÁöÑÂ§öÁ®ÆÈÅãÂãïÊäÄËÉΩÊï¥ÂêàÂà∞ÂñÆ‰∏ÄÁ≠ñÁï•‰∏≠ÔºåËÄåÁÑ°ÈúÄÈÄ≤Ë°åË§áÈõúÁöÑÁçéÂãµË™øÊï¥„ÄÇÊ≠§Â§ñÔºåÂú®ÂàÜÂ±§ÊéßÂà∂ÊñπÂºè‰∏≠ÔºåË®ìÁ∑¥ÊúâÁ¥†ÁöÑ‰ΩéÈöéÁ≠ñÁï•ÂèØ‰ª•ËàáÈ´òÈöéËá™ÁÑ∂Ë™ûË®ÄÊé®ÁêÜ (NLI) Ê®°ÁµÑÁµêÂêàÔºå‰ª•ÂØ¶ÁèæÂàùÊ≠•ÁöÑË™ûË®ÄÊåáÂ∞éÊäÄËÉΩÊéßÂà∂„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåCSI ËÉΩÂ§†ÈùàÊ¥ªÂú∞Êï¥ÂêàÊõ¥Â§öÊ®£ÂåñÁöÑÈÅãÂãïÊäÄËÉΩÔºå‰∏¶‰øÉÈÄ≤‰∏çÂêåÊäÄËÉΩ‰πãÈñìÁöÑËΩâÊèõ„ÄÇÊ≠§Â§ñÔºåÈö®ËëóÈúÄË¶ÅÊï¥ÂêàÁöÑÈÅãÂãïÊäÄËÉΩÊï∏ÈáèÂ§ßÂπÖÂ¢ûÂä†ÔºåCSI Â±ïÁèæÂá∫ËâØÂ•ΩÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇ

##### **Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs**
2408.03010v1 by Daniel Steinigen, Roman Teucher, Timm Heine Ruland, Max Rudat, Nicolas Flores-Herr, Peter Fischer, Nikola Milosevic, Christopher Schymura, Angelo Ziletti

Recent advancements in Large Language Models (LLMs) have showcased their
proficiency in answering natural language queries. However, their effectiveness
is hindered by limited domain-specific knowledge, raising concerns about the
reliability of their responses. We introduce a hybrid system that augments LLMs
with domain-specific knowledge graphs (KGs), thereby aiming to enhance factual
correctness using a KG-based retrieval approach. We focus on a medical KG to
demonstrate our methodology, which includes (1) pre-processing, (2) Cypher
query generation, (3) Cypher query processing, (4) KG retrieval, and (5)
LLM-enhanced response generation. We evaluate our system on a curated dataset
of 69 samples, achieving a precision of 78\% in retrieving correct KG nodes.
Our findings indicate that the hybrid system surpasses a standalone LLM in
accuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.
This positions the system as a promising tool for applications that demand
factual correctness and completeness, such as target identification -- a
critical process in pinpointing biological entities for disease treatment or
crop enhancement. Moreover, its intuitive search interface and ability to
provide accurate responses within seconds make it well-suited for
time-sensitive, precision-focused research contexts. We publish the source code
together with the dataset and the prompt templates used.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ±ïÁ§∫‰∫ÜÂÆÉÂÄëÂú®ÂõûÁ≠îËá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÁöÑÊúâÊïàÊÄßÂèóÂà∞ÁâπÂÆöÈ†òÂüüÁü•Ë≠òÊúâÈôêÁöÑÈòªÁ§ôÔºåÈÄôÂºïËµ∑‰∫ÜÂ∞çÂÖ∂ÂõûÊáâÂèØÈù†ÊÄßÁöÑÊìîÊÜÇ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊ∑∑ÂêàÁ≥ªÁµ±ÔºåË©≤Á≥ªÁµ±‰ΩøÁî®ÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÂúñË≠ú (KG) ‰æÜÊì¥ÂÖÖ LLMÔºåÂæûËÄåÊó®Âú®‰ΩøÁî®Âü∫Êñº KG ÁöÑÊ™¢Á¥¢ÊñπÊ≥ï‰æÜÂ¢ûÂº∑‰∫ãÂØ¶Ê≠£Á¢∫ÊÄß„ÄÇÊàëÂÄëÂ∞àÊ≥®Êñº‰∏ÄÂÄãÈÜ´Â≠∏ KG ‰æÜÊºîÁ§∫ÊàëÂÄëÁöÑ methodologyÔºåÂÖ∂‰∏≠ÂåÖÊã¨ (1) È†êËôïÁêÜÔºå(2) Cypher Êü•Ë©¢ÁîüÊàêÔºå(3) Cypher Êü•Ë©¢ËôïÁêÜÔºå(4) KG Ê™¢Á¥¢Ôºå‰ª•Âèä (5) LLM Â¢ûÂº∑ÁöÑÂõûÊáâÁîüÊàê„ÄÇÊàëÂÄëÂú®‰∏ÄÂÄãÁî± 69 ÂÄãÊ®£Êú¨ÁµÑÊàêÁöÑÁ≤æÈÅ∏Êï∏ÊìöÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÁ≥ªÁµ±ÔºåÂú®Ê™¢Á¥¢Ê≠£Á¢∫ÁöÑ KG ÁØÄÈªûÊôÇÈÅîÂà∞‰∫Ü 78% ÁöÑÁ≤æÂ∫¶„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÊ∑∑ÂêàÁ≥ªÁµ±Âú®Ê∫ñÁ¢∫ÊÄßÂíåÂÆåÊï¥ÊÄßÊñπÈù¢ÈÉΩË∂ÖÈÅé‰∫ÜÂñÆÁç®ÁöÑ LLMÔºåÈÄôÈÄöÈÅé LLM ‰ΩúÁÇ∫Ë©ïÂØ©Ë©ï‰º∞ÊñπÊ≥ïÂæóÂà∞È©óË≠â„ÄÇÈÄôÂ∞áÁ≥ªÁµ±ÂÆö‰ΩçÁÇ∫Â∞çÊáâÁî®Á®ãÂºè‰æÜË™™‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÂ∑•ÂÖ∑ÔºåÈÄô‰∫õÊáâÁî®Á®ãÂºèÈúÄË¶Å‰∫ãÂØ¶Ê≠£Á¢∫ÊÄßÂíåÂÆåÊï¥ÊÄßÔºå‰æãÂ¶ÇÁõÆÊ®ôË≠òÂà•‚Äî‚ÄîÂú®ÁñæÁóÖÊ≤ªÁôÇÊàñ‰ΩúÁâ©ÊîπËâØ‰∏≠Á≤æÁ¢∫ÂÆö‰ΩçÁîüÁâ©ÂØ¶È´îÁöÑÈóúÈçµÈÅéÁ®ã„ÄÇÊ≠§Â§ñÔºåÂÖ∂Áõ¥ËßÄÁöÑÊêúÂ∞ã‰ªãÈù¢ÂíåÂú®Êï∏ÁßíÂÖßÊèê‰æõÊ∫ñÁ¢∫ÂõûÊáâÁöÑËÉΩÂäõ‰ΩøÂÖ∂ÈùûÂ∏∏ÈÅ©ÂêàÊôÇÈñìÊïèÊÑü„ÄÅÊ≥®ÈáçÁ≤æÁ¢∫Â∫¶ÁöÑÁ†îÁ©∂ÊÉÖÂ¢É„ÄÇÊàëÂÄëÂ∞áÂéüÂßãÁ¢ºËàáÊï∏ÊìöÈõÜÂíå‰ΩøÁî®ÁöÑÊèêÁ§∫ÁØÑÊú¨‰∏ÄËµ∑ÁôºÂ∏É„ÄÇ

##### **LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning**
2408.02999v1 by Lekai Chen, Ashutosh Trivedi, Alvaro Velasquez

The emergence of intelligence in large language models (LLMs) has inspired
investigations into their integration into automata learning. This paper
introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,
which leverages a probabilistic oracle that could give persistent errors
randomly during answering the membership queries for deterministic finite
automata (DFA) learning. Given the tendency of LLMs to produce hallucinatory
content, we have developed techniques to improve answer accuracy and ensure the
correctness of the learned automata. We propose the $\mathtt{Discrimination}$
prompt as well as the $\mathtt{Verification}$ prompt and explore their
advantages over common prompts. Additionally, we compare DFA learning
performance between the TTT algorithm and common active learning algorithms. To
address the exponential number of persistent errors, we implement a dynamic
query cache refinement algorithm that identifies and corrects conflicting
queries by combining the active and passive learning algorithms. The empirical
results demonstrate the robustness and efficiency of our approach, providing a
theoretical foundation for automata learning with LLMs in the loop.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Êô∫ÊÖßÁöÑÂá∫ÁèæÔºåÊøÄÂãµ‰∫ÜÈáùÂ∞çÂÆÉÂÄëÊï¥ÂêàÂà∞Ëá™ÂãïÊ©üÂ≠∏ÁøíÁöÑÁ†îÁ©∂„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫ÜÊ©üÁéáÊÄßÁöÑÊúÄ‰ΩéÈôêÂ∫¶ÈÅ©Áï∂ÊïôÂ∏´ (pMAT) ÂÖ¨ÂºèÔºåÂÆÉÂà©Áî®‰∫Ü‰∏ÄÂÄãÊ©üÁéáÊÄßÁöÑÁ•ûË´≠ÔºåÂÆÉÂú®ÂõûÁ≠îÁ¢∫ÂÆöÊúâÈôêËá™ÂãïÊ©ü (DFA) Â≠∏ÁøíÁöÑÊàêÂì°Êü•Ë©¢ÊôÇÔºåÂèØËÉΩÊúÉÈö®Ê©üÁµ¶Âá∫ÊåÅÁ∫åÊÄßÁöÑÈåØË™§„ÄÇÈëëÊñº LLM Áî¢ÁîüÂπªË¶∫ÂÖßÂÆπÁöÑÂÇæÂêëÔºåÊàëÂÄëÂ∑≤Á∂ìÈñãÁôº‰∫ÜÊäÄË°ì‰æÜÊîπÂñÑÁ≠îÊ°àÁöÑÊ∫ñÁ¢∫ÊÄßÔºå‰∏¶Á¢∫‰øùÂ≠∏ÁøíÂà∞ÁöÑËá™ÂãïÊ©üÁöÑÊ≠£Á¢∫ÊÄß„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü $\mathtt{Discrimination}$ ÊèêÁ§∫‰ª•Âèä $\mathtt{Verification}$ ÊèêÁ§∫Ôºå‰∏¶Êé¢Ë®éÂÆÉÂÄëÁõ∏Â∞çÊñºÂ∏∏Ë¶ãÊèêÁ§∫ÁöÑÂÑ™Èªû„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊØîËºÉ‰∫Ü TTT ÊºîÁÆóÊ≥ïÂíåÂ∏∏Ë¶ã‰∏ªÂãïÂ≠∏ÁøíÊºîÁÆóÊ≥ï‰πãÈñìÁöÑ DFA Â≠∏ÁøíÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÊåÅÁ∫åÊÄßÈåØË™§ÁöÑÊåáÊï∏Êï∏ÈáèÔºåÊàëÂÄëÂØ¶‰Ωú‰∫Ü‰∏ÄÂÄãÂãïÊÖãÊü•Ë©¢Âø´ÂèñÁ≤æÁ∑ªÂåñÊºîÁÆóÊ≥ïÔºåÂÆÉÈÄèÈÅéÁµêÂêà‰∏ªÂãïÂíåË¢´ÂãïÂ≠∏ÁøíÊºîÁÆóÊ≥ï‰æÜË≠òÂà•‰∏¶‰øÆÊ≠£Ë°ùÁ™ÅÁöÑÊü•Ë©¢„ÄÇÂØ¶Ë≠âÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÁ©©ÂÅ•ÊÄßÂíåÊïàÁéáÔºåÁÇ∫Ëø¥Âúà‰∏≠ÁöÑ LLM Ëá™ÂãïÊ©üÂ≠∏ÁøíÊèê‰æõ‰∫ÜÁêÜË´ñÂü∫Á§é„ÄÇ

##### **ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval**
2408.02978v1 by Ruixiang Zhao, Jian Jia, Yan Li, Xuehan Bai, Quan Chen, Han Li, Peng Jiang, Xirong Li

E-commerce is increasingly multimedia-enriched, with products exhibited in a
broad-domain manner as images, short videos, or live stream promotions. A
unified and vectorized cross-domain production representation is essential. Due
to large intra-product variance and high inter-product similarity in the
broad-domain scenario, a visual-only representation is inadequate. While
Automatic Speech Recognition (ASR) text derived from the short or live-stream
videos is readily accessible, how to de-noise the excessively noisy text for
multimodal representation learning is mostly untouched. We propose ASR-enhanced
Multimodal Product Representation Learning (AMPere). In order to extract
product-specific information from the raw ASR text, AMPere uses an
easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,
together with visual data, is then fed into a multi-branch network to generate
compact multimodal embeddings. Extensive experiments on a large-scale
tri-domain dataset verify the effectiveness of AMPere in obtaining a unified
multimodal product representation that clearly improves cross-domain product
retrieval.

ÊëòË¶ÅÔºöÈõªÂ≠êÂïÜÂãôÊó•ÁõäË±êÂØåÂ§öÂ™íÈ´îÔºåÁî¢ÂìÅ‰ª•ÂΩ±ÂÉè„ÄÅÁü≠ÁâáÊàñÁõ¥Êí≠‰øÉÈä∑Á≠âÂª£ÂüüÊñπÂºèÂ±ïÁ§∫„ÄÇÁµ±‰∏Ä‰∏îÂêëÈáèÂåñÁöÑË∑®ÂüüË£Ω‰ΩúË°®Á§∫Ëá≥ÈóúÈáçË¶Å„ÄÇÁî±ÊñºÂª£ÂüüÂ†¥ÊôØ‰∏≠Áî¢ÂìÅÂÖßÈÉ®Â∑ÆÁï∞Â§ßÔºåÁî¢ÂìÅÈñìÁõ∏‰ººÊÄßÈ´òÔºåÂÉÖË¶ñË¶∫Ë°®Á§∫ÊòØ‰∏çÂ§†ÁöÑ„ÄÇÈõñÁÑ∂ÂèØ‰ª•ËºïÊòìÂèñÂæóÁü≠ÁâáÊàñÁõ¥Êí≠ÂΩ±ÁâáÁöÑËá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) ÊñáÂ≠óÔºå‰ΩÜÂ¶Ç‰ΩïÁÇ∫Â§öÊ®°ÊÖãË°®Á§∫Â≠∏ÁøíÂéªÈõúË®äÂåñÈÅéÂ∫¶ÈõúË®äÁöÑÊñáÂ≠óÔºåÂ§ßÂ§öÊï∏‰ªçÊú™Ëß∏Âèä„ÄÇÊàëÂÄëÊèêÂá∫ ASR Â¢ûÂº∑ÁöÑÂ§öÊ®°ÊÖãÁî¢ÂìÅË°®Á§∫Â≠∏Áøí (AMPere)„ÄÇÁÇ∫‰∫ÜÂæûÂéüÂßã ASR ÊñáÂ≠ó‰∏≠ËêÉÂèñÁî¢ÂìÅÁâπÂÆöË≥áË®äÔºåAMPere ‰ΩøÁî®ÂÆπÊòìÂØ¶‰ΩúÁöÑ LLM-based ASR ÊñáÂ≠óÊëòË¶ÅÂô®„ÄÇLLM ÊëòË¶ÅÁöÑÊñáÂ≠óËàáË¶ñË¶∫Ë≥áÊñô‰∏ÄËµ∑ÔºåÊé•ËëóËº∏ÂÖ•Â§öÂàÜÊîØÁ∂≤Ë∑ØÔºåÁî¢ÁîüÁ∑äÊπäÁöÑÂ§öÊ®°ÊÖãÂµåÂÖ•„ÄÇÂú®Â§ßÂûã‰∏âÂüüË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºåÈ©óË≠â‰∫Ü AMPere Âú®ÂèñÂæóÁµ±‰∏ÄÁöÑÂ§öÊ®°ÊÖãÁî¢ÂìÅË°®Á§∫‰∏äÁöÑÊïàËÉΩÔºåÊòéÈ°ØÊîπÂñÑ‰∫ÜË∑®ÂüüÁî¢ÂìÅÊ™¢Á¥¢„ÄÇ

##### **Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation**
2408.02976v1 by Hui Ma, Bo Zhang, Bo Xu, Jian Wang, Hongfei Lin, Xiao Sun

Empathetic response generation, aiming at understanding the user's situation
and feelings and respond empathically, is crucial in building human-like
dialogue systems. Previous methods mainly focus on using maximum likelihood
estimation as the optimization objective for training response generation
models, without taking into account the empathy level alignment between
generated responses and target responses. To this end, we propose an empathetic
response generation using reinforcement learning (EmpRL) framework. The
framework designs an effective empathy reward function and generates empathetic
responses by maximizing the expected reward through reinforcement learning.
Given the powerful text generation capability of pre-trained language models,
EmpRL utilizes the pre-trained T5 model as the generator and conducts further
training to initialize the policy. To align the empathy level between generated
responses and target responses in the context, an empathy reward function
containing three empathy communication mechanisms, i.e., emotional reaction,
interpretation, and exploration, is constructed using pre-designed and
pre-trained empathy identifiers. Finally, the proximal policy optimization
algorithm is used to further train the policy to produce empathetic responses.
Both automatic and manual evaluations demonstrate that the proposed EmpRL
framework can improve the quality of generated responses, enhance the empathy
level similarity between generated and target responses, and produce empathetic
responses covering both affective and cognitive aspects.

ÊëòË¶ÅÔºöÂêåÁêÜÂøÉÂõûÊáâÁîüÊàêÔºåÊó®Âú®ÁêÜËß£‰ΩøÁî®ËÄÖÁöÑËôïÂ¢ÉÂíåÊÑüÂèóÔºå‰∏¶‰ª•ÂêåÁêÜÂøÉÂõûÊáâÔºåÂ∞çÊñºÂª∫Á´ãÈ°û‰∫∫Â∞çË©±Á≥ªÁµ±Ëá≥ÈóúÈáçË¶Å„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂ÊñπÊ≥ï‰∏ªË¶ÅËëóÈáçÊñº‰ΩøÁî®ÊúÄÂ§ß‰ººÁÑ∂‰º∞Ë®à‰ΩúÁÇ∫Ë®ìÁ∑¥ÂõûÊáâÁîüÊàêÊ®°ÂûãÁöÑÊúÄ‰Ω≥ÂåñÁõÆÊ®ôÔºåËÄåÊú™ËÄÉÈáèÁîüÊàêÂõûÊáâËàáÁõÆÊ®ôÂõûÊáâ‰πãÈñìÁöÑÂêåÁêÜÂøÉÂ±§Á¥öÂ∞çÈΩä„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰ΩøÁî®Âº∑ÂåñÂ≠∏ÁøíÔºàEmpRLÔºâÊû∂ÊßãÁöÑÂêåÁêÜÂøÉÂõûÊáâÁîüÊàêÊñπÊ≥ï„ÄÇË©≤Êû∂ÊßãË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊúâÊïàÁöÑÂêåÁêÜÂøÉÂõûÈ•ãÂáΩÊï∏Ôºå‰∏¶ÈÄèÈÅéÂº∑ÂåñÂ≠∏ÁøíÊúÄÂ§ßÂåñÈ†êÊúüÂõûÈ•ã‰æÜÁî¢ÁîüÂêåÁêÜÂøÉÂõûÊáâ„ÄÇÈëëÊñºÈ†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÂº∑Â§ßÁöÑÊñáÂ≠óÁîüÊàêËÉΩÂäõÔºåEmpRL ‰ΩøÁî®È†êÂÖàË®ìÁ∑¥ÁöÑ T5 Ê®°Âûã‰ΩúÁÇ∫ÁîüÊàêÂô®Ôºå‰∏¶ÈÄ≤Ë°åÈÄ≤‰∏ÄÊ≠•ÁöÑË®ìÁ∑¥‰æÜÂàùÂßãÂåñÊîøÁ≠ñ„ÄÇÁÇ∫‰∫ÜÂú®ËÑàÁµ°‰∏≠Â∞çÈΩäÁîüÊàêÂõûÊáâËàáÁõÆÊ®ôÂõûÊáâÁöÑÂêåÁêÜÂøÉÂ±§Á¥öÔºåÊàëÂÄë‰ΩøÁî®È†êÂÖàË®≠Ë®àÂíåÈ†êÂÖàË®ìÁ∑¥ÁöÑÂêåÁêÜÂøÉË≠òÂà•Á¨¶ËôüÔºåÂª∫Êßã‰∫Ü‰∏ÄÂÄãÂåÖÂê´‰∏âÁ®ÆÂêåÁêÜÂøÉÊ∫ùÈÄöÊ©üÂà∂ÁöÑÂêåÁêÜÂøÉÂõûÈ•ãÂáΩÊï∏ÔºåÂç≥ÊÉÖÁ∑íÂèçÊáâ„ÄÅË©ÆÈáãÂíåÊé¢Á¥¢„ÄÇÊúÄÂæåÔºåÊàëÂÄë‰ΩøÁî®ËøëÁ´ØÊîøÁ≠ñÊúÄ‰Ω≥ÂåñÊºîÁÆóÊ≥ïÈÄ≤‰∏ÄÊ≠•Ë®ìÁ∑¥ÊîøÁ≠ñÔºå‰ª•Áî¢ÁîüÂêåÁêÜÂøÉÂõûÊáâ„ÄÇËá™ÂãïÂíåÊâãÂãïË©ï‰º∞ÂùáÈ°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑ EmpRL Êû∂ÊßãÂèØ‰ª•ÊèêÂçáÁîüÊàêÂõûÊáâÁöÑÂìÅË≥™ÔºåÂ¢ûÂº∑ÁîüÊàêÂõûÊáâËàáÁõÆÊ®ôÂõûÊáâ‰πãÈñìÁöÑÂêåÁêÜÂøÉÂ±§Á¥öÁõ∏‰ººÊÄßÔºå‰∏¶Áî¢ÁîüÊ∂µËìãÊÉÖÊÑüÂíåË™çÁü•Â±§Èù¢ÁöÑÂêåÁêÜÂøÉÂõûÊáâ„ÄÇ

##### **EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and Quantization**
2408.02970v1 by Zhaopeng Feng, Zijie Meng, Zuozhu Liu

Large language models (LLMs) have attracted considerable attention in various
fields for their cost-effective solutions to diverse challenges, especially
with advancements in instruction tuning and quantization. E-commerce, with its
complex tasks and extensive product-user interactions, presents a promising
application area for LLMs. However, the domain-specific concepts and knowledge
inherent in e-commerce pose significant challenges for adapting general LLMs.
To address this issue, we developed EC-Guide
\href{https://github.com/fzp0424/EC-Guide-KDDUP-2024}, a comprehensive
e-commerce guide for instruction tuning and quantization of LLMs. We also
heuristically integrated Chain-of-Thought (CoT) during inference to enhance
arithmetic performance. Our approach achieved the 2nd place in Track 2 and 5th
place in Track 5 at the Amazon KDD Cup'24
\href{https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms}.
Additionally, our solution is model-agnostic, enabling effective scalability
across larger systems.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âõ†ÂÖ∂Âú®ÂêÑÁ®ÆÈ†òÂüü‰∏≠‰ª•ÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÊñπÂºèËß£Ê±∫ÂêÑÁ®ÆÊåëÊà∞ËÄåÂÇôÂèóÈóúÊ≥®ÔºåÁâπÂà•ÊòØÂú®Êåá‰ª§Ë™øÊï¥ÂíåÈáèÂåñÊñπÈù¢ÂèñÂæóÈÄ≤Â±ï„ÄÇÈõªÂ≠êÂïÜÂãôÂèäÂÖ∂Ë§áÈõúÁöÑ‰ªªÂãôÂíåÂª£Ê≥õÁöÑÁî¢ÂìÅ‰ΩøÁî®ËÄÖ‰∫íÂãïÔºåÁÇ∫ LLM ÂëàÁèæ‰∫Ü‰∏ÄÂÄãÂ§ßÊúâÂèØÁÇ∫ÁöÑÊáâÁî®È†òÂüü„ÄÇÁÑ∂ËÄåÔºåÈõªÂ≠êÂïÜÂãô‰∏≠Âõ∫ÊúâÁöÑÁâπÂÆöÈ†òÂüüÊ¶ÇÂøµÂíåÁü•Ë≠òÔºåÂ∞ç‰∏ÄËà¨ LLM ÁöÑÈÅ©ÊáâÊèêÂá∫‰∫ÜÈáçÂ§ßÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÈñãÁôº‰∫Ü EC-Guide \href{https://github.com/fzp0424/EC-Guide-KDDUP-2024}ÔºåÈÄôÊòØ‰∏ÄÂÄãÈáùÂ∞ç LLM Êåá‰ª§Ë™øÊï¥ÂíåÈáèÂåñÁöÑÂÖ®Èù¢ÈõªÂ≠êÂïÜÂãôÊåáÂçó„ÄÇÊàëÂÄëÈÇÑÂú®Êé®ÁêÜÊúüÈñìÂïüÁôºÂºèÂú∞Êï¥Âêà‰∫ÜÊÄùËÄÉÈèà (CoT)Ôºå‰ª•Â¢ûÂº∑ÁÆóË°ìÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® Amazon KDD Cup'24 ÁöÑËªåÈÅì 2 ‰∏≠Áç≤ÂæóÁ¨¨ 2 ÂêçÔºåÂú®ËªåÈÅì 5 ‰∏≠Áç≤ÂæóÁ¨¨ 5 Âêç \href{https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms}„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑËß£Ê±∫ÊñπÊ°àËàáÊ®°ÂûãÁÑ°ÈóúÔºåÂèØ‰ª•Âú®Êõ¥Â§ßÁöÑÁ≥ªÁµ±‰∏≠ÊúâÊïàÊì¥Â±ï„ÄÇ

##### **Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval**
2408.02964v1 by Iman Azimi, Mohan Qi, Li Wang, Amir M. Rahmani, Youlin Li

Large language models (LLMs) are fundamentally transforming human-facing
applications in the health and well-being domains: boosting patient engagement,
accelerating clinical decision-making, and facilitating medical education.
Although state-of-the-art LLMs have shown superior performance in several
conversational applications, evaluations within nutrition and diet applications
are still insufficient. In this paper, we propose to employ the Registered
Dietitian (RD) exam to conduct a standard and comprehensive evaluation of
state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing
both accuracy and consistency in nutrition queries. Our evaluation includes
1050 RD exam questions encompassing several nutrition topics and proficiency
levels. In addition, for the first time, we examine the impact of Zero-Shot
(ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC),
and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the
responses. Our findings revealed that while these LLMs obtained acceptable
overall performance, their results varied considerably with different prompts
and question domains. GPT-4o with CoT-SC prompting outperformed the other
approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.
For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both
accuracy and consistency. RAP was particularly effective for GPT-4o to answer
Expert level questions. Consequently, choosing the appropriate LLM and
prompting technique, tailored to the proficiency level and specific domain, can
mitigate errors and potential risks in diet and nutrition chatbots.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ê≠£Âú®ÂæûÊ†πÊú¨‰∏äËΩâËÆäÂÅ•Â∫∑ÂíåÁ¶èÁ•âÈ†òÂüü‰∏≠Èù¢Âêë‰∫∫È°ûÁöÑÊáâÁî®ÔºöÊèêÂçáÊÇ£ËÄÖÂèÉËàáÂ∫¶„ÄÅÂä†ÈÄüËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÔºå‰∏¶‰øÉÈÄ≤ÈÜ´Â≠∏ÊïôËÇ≤„ÄÇÂÑòÁÆ°ÊúÄÂÖàÈÄ≤ÁöÑ LLM Â∑≤Âú®Â§öÈ†ÖÂ∞çË©±ÂºèÊáâÁî®‰∏≠Â±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÊïàËÉΩÔºå‰ΩÜÂú®ÁáüÈ§äÂíåÈ£≤È£üÊáâÁî®‰∏≠ÁöÑË©ï‰º∞‰ªç‰∏çË∂≥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêË≠∞Êé°Áî®Ë®ªÂÜäÁáüÈ§äÂ∏´ (RD) ËÄÉË©¶‰æÜÂ∞çÊúÄÂÖàÈÄ≤ÁöÑ LLM„ÄÅGPT-4o„ÄÅClaude 3.5 Sonnet Âíå Gemini 1.5 Pro ÈÄ≤Ë°åÊ®ôÊ∫ñ‰∏îÂÖ®Èù¢ÁöÑË©ï‰º∞ÔºåË©ï‰º∞ÁáüÈ§äÊü•Ë©¢ÁöÑÊ∫ñÁ¢∫ÊÄßÂíå‰∏ÄËá¥ÊÄß„ÄÇÊàëÂÄëÁöÑË©ï‰º∞ÂåÖÂê´ 1050 ÂÄã RD ËÄÉË©¶È°åÁõÆÔºåÊ∂µËìãÂ§öÂÄãÁáüÈ§ä‰∏ªÈ°åÂíåÁÜüÁ∑¥Á®ãÂ∫¶„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈ¶ñÊ¨°Êé¢Ë®éÈõ∂Ê¨°Â≠∏Áøí (ZS)„ÄÅÊÄùËÄÉÈèà (CoT)„ÄÅÂÖ∑ÊúâËá™Êàë‰∏ÄËá¥ÊÄßÁöÑÊÄùËÄÉÈèà (CoT-SC) ÂíåÊ™¢Á¥¢Â¢ûÂº∑ÊèêÁ§∫ (RAP) Â∞çÂõûÊáâÊ∫ñÁ¢∫ÊÄßÂíå‰∏ÄËá¥ÊÄßÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÂÑòÁÆ°ÈÄô‰∫õ LLM Áç≤ÂæóÂèØÊé•ÂèóÁöÑÊï¥È´îÊïàËÉΩÔºå‰ΩÜÂÖ∂ÁµêÊûúÂõ†‰∏çÂêåÁöÑÊèêÁ§∫ÂíåÂïèÈ°åÈ†òÂüüËÄåÊúâÂæàÂ§ßÂ∑ÆÁï∞„ÄÇ‰ΩøÁî® CoT-SC ÊèêÁ§∫ÁöÑ GPT-4o ÂÑ™ÊñºÂÖ∂‰ªñÊñπÊ≥ïÔºåËÄå‰ΩøÁî® ZS ÁöÑ Gemini 1.5 Pro ÂâáË®òÈåÑÂà∞ÊúÄÈ´òÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂ∞çÊñº GPT-4o Âíå Claude 3.5ÔºåCoT ÊèêÈ´ò‰∫ÜÊ∫ñÁ¢∫ÊÄßÔºåËÄå CoT-SC ÂâáÂêåÊôÇÊèêÈ´ò‰∫ÜÊ∫ñÁ¢∫ÊÄßÂíå‰∏ÄËá¥ÊÄß„ÄÇRAP Â∞çÊñº GPT-4o ÂõûÁ≠îÂ∞àÂÆ∂Á¥öÂà•ÁöÑÂïèÈ°åÁâπÂà•ÊúâÊïà„ÄÇÂõ†Ê≠§ÔºåÈÅ∏ÊìáÈÅ©Áï∂ÁöÑ LLM ÂíåÊèêÁ§∫ÊäÄÂ∑ßÔºå‰∏¶Ê†πÊìöÁÜüÁ∑¥Á®ãÂ∫¶ÂíåÁâπÂÆöÈ†òÂüüÈÄ≤Ë°åË™øÊï¥ÔºåÂèØ‰ª•Ê∏õËºïÈ£≤È£üÂíåÁáüÈ§äËÅäÂ§©Ê©üÂô®‰∫∫ÁöÑÈåØË™§ÂíåÊΩõÂú®È¢®Èö™„ÄÇ

##### **Few-shot Scooping Under Domain Shift via Simulated Maximal Deployment Gaps**
2408.02949v1 by Yifan Zhu, Pranay Thangeda, Erica L Tevere, Ashish Goel, Erik Kramer, Hari D Nayar, Melkior Ornik, Kris Hauser

Autonomous lander missions on extraterrestrial bodies need to sample granular
materials while coping with domain shifts, even when sampling strategies are
extensively tuned on Earth. To tackle this challenge, this paper studies the
few-shot scooping problem and proposes a vision-based adaptive scooping
strategy that uses the deep kernel Gaussian process method trained with a novel
meta-training strategy to learn online from very limited experience on
out-of-distribution target terrains. Our Deep Kernel Calibration with Maximal
Deployment Gaps (kCMD) strategy explicitly trains a deep kernel model to adapt
to large domain shifts by creating simulated maximal deployment gaps from an
offline training dataset and training models to overcome these deployment gaps
during training. Employed in a Bayesian Optimization sequential decision-making
framework, the proposed method allows the robot to perform high-quality
scooping actions on out-of-distribution terrains after a few attempts,
significantly outperforming non-adaptive methods proposed in the excavation
literature as well as other state-of-the-art meta-learning methods. The
proposed method also demonstrates zero-shot transfer capability, successfully
adapting to the NASA OWLAT platform, which serves as a state-of-the-art
simulator for potential future planetary missions. These results demonstrate
the potential of training deep models with simulated deployment gaps for more
generalizable meta-learning in high-capacity models. Furthermore, they
highlight the promise of our method in autonomous lander sampling missions by
enabling landers to overcome the deployment gap between Earth and
extraterrestrial bodies.

ÊëòË¶ÅÔºö<paragraph>Â§ñÊòüÂ§©È´î‰∏äÁöÑËá™‰∏ªÁôªÈô∏‰ªªÂãôÈúÄË¶ÅÊé°ÈõÜÈ°ÜÁ≤íÊùêÊñôÔºåÂêåÊôÇÊáâÂ∞çÈ†òÂüüËΩâËÆäÔºåÂç≥‰ΩøÂú®Âª£Ê≥õË™øÊï¥Âú∞ÁêÉ‰∏äÁöÑÊé°ÈõÜÁ≠ñÁï•ÊôÇ‰πüÊòØÂ¶ÇÊ≠§„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊú¨ÊñáÁ†îÁ©∂‰∫ÜÂ∞ëÊ¨°ÂòóË©¶ÁöÑÊåñÂèñÂïèÈ°åÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºË¶ñË¶∫ÁöÑËá™ÈÅ©ÊáâÊåñÂèñÁ≠ñÁï•ÔºåË©≤Á≠ñÁï•‰ΩøÁî®Ê∑±Â∫¶Ê†∏È´òÊñØÈÅéÁ®ãÊñπÊ≥ïÈÄ≤Ë°åË®ìÁ∑¥ÔºåÊé°Áî®Êñ∞Á©éÁöÑÂÖÉË®ìÁ∑¥Á≠ñÁï•ÔºåÂæûÂàÜÂ∏ÉÁõÆÊ®ôÂú∞ÂΩ¢‰∏äÁöÑÈùûÂ∏∏ÊúâÈôêÁöÑÁ∂ìÈ©ó‰∏≠Âú®Á∑öÂ≠∏Áøí„ÄÇÊàëÂÄëÊé°Áî®ÊúÄÂ§ßÈÉ®ÁΩ≤Â∑ÆË∑ùÁöÑÊ∑±Â∫¶Ê†∏Ê†°Ê∫ñ (kCMD) Á≠ñÁï•ÔºåÊòéÁ¢∫Ë®ìÁ∑¥Ê∑±Â∫¶Ê†∏Ê®°Âûã‰ª•ÈÅ©ÊáâÂ§ßÁöÑÈ†òÂüüËΩâËÆäÔºåÊñπÊ≥ïÊòØÂæûÈõ¢Á∑öË®ìÁ∑¥Êï∏ÊìöÈõÜÂâµÂª∫Ê®°Êì¨ÁöÑÊúÄÂ§ßÈÉ®ÁΩ≤Â∑ÆË∑ùÔºå‰∏¶Ë®ìÁ∑¥Ê®°ÂûãÂú®Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠ÂÖãÊúçÈÄô‰∫õÈÉ®ÁΩ≤Â∑ÆË∑ù„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊé°Áî®Ë≤ùËëâÊñØÂÑ™ÂåñÂ∫èË≤´Ê±∫Á≠ñÂà∂ÂÆöÊ°ÜÊû∂Ôºå‰ΩøÊ©üÂô®‰∫∫Âú®Á∂ìÈÅéÂπæÊ¨°ÂòóË©¶ÂæåËÉΩÂ§†Âú®Â§ñÈÉ®ÂàÜÂ∏ÉÂú∞ÂΩ¢‰∏äÂü∑Ë°åÈ´òÂìÅË≥™ÁöÑÊåñÂèñÂãï‰ΩúÔºåÈ°ØËëóÂÑ™ÊñºÊåñÊéòÊñáÁçª‰∏≠ÊèêÂá∫ÁöÑÈùûËá™ÈÅ©ÊáâÊñπÊ≥ï‰ª•ÂèäÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑÂÖÉÂ≠∏ÁøíÊñπÊ≥ï„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÈÇÑÂ±ïÁ§∫‰∫ÜÈõ∂Ê¨°ÂòóË©¶ËΩâÁßªËÉΩÂäõÔºåÊàêÂäüÈÅ©Êáâ‰∫Ü NASA OWLAT Âπ≥Âè∞ÔºåË©≤Âπ≥Âè∞ÊòØÊΩõÂú®Êú™‰æÜË°åÊòü‰ªªÂãôÁöÑÊúÄÊñ∞Ê®°Êì¨Âô®„ÄÇÈÄô‰∫õÁµêÊûúÂ±ïÁ§∫‰∫Ü‰ΩøÁî®Ê®°Êì¨ÈÉ®ÁΩ≤Â∑ÆË∑ùË®ìÁ∑¥Ê∑±Â∫¶Ê®°Âûã‰ª•Âú®È´òÂÆπÈáèÊ®°Âûã‰∏≠ÈÄ≤Ë°åÊõ¥ÂÖ∑Ê≥õÂåñÊÄßÁöÑÂÖÉÂ≠∏ÁøíÁöÑÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÄëÈÄöÈÅé‰ΩøÁôªÈô∏Âô®ËÉΩÂ§†ÂÖãÊúçÂú∞ÁêÉÂíåÂ§ñÊòüÂ§©È´î‰πãÈñìÁöÑÈÉ®ÁΩ≤Â∑ÆË∑ùÔºåÁ™ÅÂá∫‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Ëá™‰∏ªÁôªÈô∏Âô®Êé°Ê®£‰ªªÂãô‰∏≠ÁöÑÂâçÊôØ„ÄÇ</paragraph>

##### **Are Female Carpenters like Blue Bananas? A Corpus Investigation of Occupation Gender Typicality**
2408.02948v1 by Da Ju, Karen Ulrich, Adina Williams

People tend to use language to mention surprising properties of events: for
example, when a banana is blue, we are more likely to mention color than when
it is yellow. This fact is taken to suggest that yellowness is somehow a
typical feature of bananas, and blueness is exceptional. Similar to how a
yellow color is typical of bananas, there may also be genders that are typical
of occupations. In this work, we explore this question using information
theoretic techniques coupled with corpus statistic analysis. In two distinct
large corpora, we do not find strong evidence that occupations and gender
display the same patterns of mentioning as do bananas and color. Instead, we
find that gender mentioning is correlated with femaleness of occupation in
particular, suggesting perhaps that woman-dominated occupations are seen as
somehow ``more gendered'' than male-dominated ones, and thereby they encourage
more gender mentioning overall.

ÊëòË¶ÅÔºö‰∫∫ÂÄëÂÇæÂêëÊñºÁî®Ë™ûË®ÄÊèêÂèä‰∫ã‰ª∂ÁöÑÈ©ö‰∫∫Â±¨ÊÄßÔºö‰æãÂ¶ÇÔºåÁï∂È¶ôËïâÊòØËóçËâ≤ÁöÑÊôÇÂÄôÔºåÊàëÂÄëÊõ¥ÊúâÂèØËÉΩÊèêÂà∞È°èËâ≤ÔºåËÄå‰∏çÊòØÁï∂ÂÆÉËÆäÈªÉÁöÑÊôÇÂÄô„ÄÇÈÄôÂÄã‰∫ãÂØ¶Ë°®ÊòéÔºåÈªÉËâ≤Âú®ÊüêÁ®ÆÁ®ãÂ∫¶‰∏äÊòØÈ¶ôËïâÁöÑÂÖ∏ÂûãÁâπÂæµÔºåËÄåËóçËâ≤ÂâáÊòØÈùûÂÖ∏ÂûãÁöÑ„ÄÇÈ°û‰ººÊñºÈªÉËâ≤ÊòØÈ¶ôËïâÁöÑÂÖ∏ÂûãÈ°èËâ≤ÔºåÊüê‰∫õÊÄßÂà•‰πüÂèØËÉΩÊòØËÅ∑Ê•≠ÁöÑÂÖ∏ÂûãÁâπÂæµ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ΩøÁî®Ë≥áË®äÁêÜË´ñÊäÄË°ìÁµêÂêàË™ûÊñôÂ∫´Áµ±Ë®àÂàÜÊûê‰æÜÊé¢Ë®éÈÄôÂÄãÂïèÈ°å„ÄÇÂú®ÂÖ©ÂÄã‰∏çÂêåÁöÑÂ§ßÂûãË™ûÊñôÂ∫´‰∏≠ÔºåÊàëÂÄëÊ≤íÊúâÁôºÁèæÂº∑ÊúâÂäõÁöÑË≠âÊìöË°®ÊòéËÅ∑Ê•≠ÂíåÊÄßÂà•ËàáÈ¶ôËïâÂíåÈ°èËâ≤È°ØÁ§∫Áõ∏ÂêåÁöÑÊèêÂèäÊ®°Âºè„ÄÇÁõ∏ÂèçÔºåÊàëÂÄëÁôºÁèæÊÄßÂà•ÊèêÂèäËàáËÅ∑Ê•≠ÁöÑÂ•≥ÊÄßÂåñÁâπÂà•Áõ∏ÈóúÔºåÈÄôÂèØËÉΩË°®ÊòéÂ•≥ÊÄß‰∏ªÂ∞éÁöÑËÅ∑Ê•≠Âú®ÊüêÁ®ÆÁ®ãÂ∫¶‰∏äË¢´Ë¶ñÁÇ∫„ÄåÊõ¥ÂÖ∑ÊÄßÂà•„ÄçÔºåÂæûËÄåÊï¥È´î‰∏äÈºìÂãµÊõ¥Â§öÂú∞ÊèêÂèäÊÄßÂà•„ÄÇ

##### **Scaling Laws for Data Poisoning in LLMs**
2408.02946v1 by Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine

Recent work shows that LLMs are vulnerable to data poisoning, in which they
are trained on partially corrupted or harmful data. Poisoned data is hard to
detect, breaks guardrails, and leads to undesirable and harmful behavior. Given
the intense efforts by leading labs to train and deploy increasingly larger and
more capable LLMs, it is critical to ask if the risk of data poisoning will be
naturally mitigated by scale, or if it is an increasing threat. We consider
three threat models by which data poisoning can occur: malicious fine-tuning,
imperfect data curation, and intentional data contamination. Our experiments
evaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72
billion parameters on three datasets which speak to each of our threat models.
We find that larger LLMs are increasingly vulnerable, learning harmful behavior
-- including sleeper agent behavior -- significantly more quickly than smaller
LLMs with even minimal data poisoning. These results underscore the need for
robust safeguards against data poisoning in larger LLMs.

ÊëòË¶ÅÔºöÊúÄËøëÁöÑÁ†îÁ©∂ÊòæÁ§∫ÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÂÆπÊòìÂèóÂà∞Êï∞ÊçÆ‰∏≠ÊØíÁöÑÂΩ±ÂìçÔºåÂç≥Âú®ÈÉ®ÂàÜÊçüÂùèÊàñÊúâÂÆ≥ÁöÑÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇ‰∏≠ÊØíÊï∞ÊçÆÈöæ‰ª•Ê£ÄÊµãÔºå‰ºöÁ†¥ÂùèÈò≤Êä§Êé™ÊñΩÔºåÂπ∂ÂØºËá¥‰∏çËâØÂíåÊúâÂÆ≥ÁöÑË°å‰∏∫„ÄÇÈâ¥‰∫éÈ¢ÜÂÖàÂÆûÈ™åÂÆ§Âú®ËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤Ë∂äÊù•Ë∂äÂ§ß„ÄÅÂäüËÉΩÊõ¥Âº∫Â§ßÁöÑ LLM ÊñπÈù¢‰ªòÂá∫‰∫ÜÂ∑®Â§ßÂä™ÂäõÔºåÂõ†Ê≠§Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØË¶ÅÈóÆÊï∞ÊçÆ‰∏≠ÊØíÁöÑÈ£éÈô©ÊòØÂê¶‰ºöÈöèÁùÄËßÑÊ®°ÁöÑÊâ©Â§ßËÄåËá™ÁÑ∂ÂáèËΩªÔºåÊàñËÄÖÊòØÂê¶‰ºöÊàê‰∏∫Ë∂äÊù•Ë∂äÂ§ßÁöÑÂ®ÅËÉÅ„ÄÇÊàë‰ª¨ËÄÉËôë‰∫Ü‰∏âÁßçÊï∞ÊçÆ‰∏≠ÊØíÂèØËÉΩÂèëÁîüÁöÑÂ®ÅËÉÅÊ®°ÂûãÔºöÊÅ∂ÊÑèÂæÆË∞É„ÄÅ‰∏çÂÆåÁæéÁöÑÊï∞ÊçÆÊï¥ÁêÜÂíåÊïÖÊÑèÁöÑÊï∞ÊçÆÊ±°Êüì„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åËØÑ‰º∞‰∫ÜÊï∞ÊçÆ‰∏≠ÊØíÂØπ 23 ‰∏™ÂâçÊ≤ø LLM ÁöÑÂΩ±ÂìçÔºåËøô‰∫õ LLM ÁöÑÂèÇÊï∞ËåÉÂõ¥‰ªé 15 ‰∫øÂà∞ 720 ‰∫øÔºåÈíàÂØπ‰∏â‰∏™Êï∞ÊçÆÈõÜÔºåÊØè‰∏™Êï∞ÊçÆÈõÜÈÉΩÈíàÂØπÊàë‰ª¨ÁöÑÂ®ÅËÉÅÊ®°Âûã‰πã‰∏Ä„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåËæÉÂ§ßÁöÑ LLM Ë∂äÊù•Ë∂äÂÆπÊòìÂèóÂà∞ÊîªÂáªÔºåÂ≠¶‰π†ÊúâÂÆ≥Ë°å‰∏∫ÔºàÂåÖÊã¨‰ºëÁú†‰ª£ÁêÜË°å‰∏∫ÔºâÁöÑÈÄüÂ∫¶ÊòéÊòæÂø´‰∫éÂç≥‰ΩøÊòØÊúÄÂ∞èÁöÑÊï∞ÊçÆ‰∏≠ÊØíÁöÑËæÉÂ∞è LLM„ÄÇËøô‰∫õÁªìÊûúÂº∫Ë∞É‰∫ÜÈúÄË¶ÅÈíàÂØπÂ§ßÂûã LLM ‰∏≠ÁöÑÊï∞ÊçÆ‰∏≠ÊØíÈááÂèñÂº∫ÊúâÂäõÁöÑ‰øùÈöúÊé™ÊñΩ„ÄÇ

##### **Self-Supervised Learning for Multi-Channel Neural Transducer**
2408.02945v1 by Atsushi Kojima

Self-supervised learning, such as with the wav2vec 2.0 framework
significantly improves the accuracy of end-to-end automatic speech recognition
(ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In
this work, we explored a self-supervised learning method for a multi-channel
end-to-end ASR model based on the wav2vec 2.0 framework. As the multi-channel
end-to-end ASR model, we focused on a multi-channel neural transducer. In
pre-training, we compared three different methods for feature quantization to
train a multi-channel conformer audio encoder: joint quantization, feature-wise
quantization and channel-wise quantization. In fine-tuning, we trained the
multi-channel conformer-transducer. All experiments were conducted using the
far-field in-house and CHiME-4 datasets. The results of the experiments showed
that feature-wise quantization was the most effective among the methods. We
observed a 66% relative reduction in character error rate compared with the
model without any pre-training for the far-field in-house dataset.

ÊëòË¶ÅÔºöËá™Áõ£Áù£Â≠∏ÁøíÔºå‰æãÂ¶Ç‰ΩøÁî® wav2vec 2.0 Êû∂ÊßãÔºåÂ§ßÂπÖÊèêÂçáÁ´ØÂ∞çÁ´ØËá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇWav2vec 2.0 Â∑≤ÊáâÁî®ÊñºÂñÆËÅ≤ÈÅìÁ´ØÂ∞çÁ´Ø ASR Ê®°Âûã„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂü∫Êñº wav2vec 2.0 Êû∂ÊßãÁöÑÂ§öËÅ≤ÈÅìÁ´ØÂ∞çÁ´Ø ASR Ê®°ÂûãÁöÑËá™ÊàëÁõ£Áù£Â≠∏ÁøíÊñπÊ≥ï„ÄÇ‰ΩúÁÇ∫Â§öËÅ≤ÈÅìÁ´ØÂ∞çÁ´Ø ASR Ê®°ÂûãÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂ§öËÅ≤ÈÅìÁ•ûÁ∂ìËΩâÊèõÂô®„ÄÇÂú®È†êË®ìÁ∑¥‰∏≠ÔºåÊàëÂÄëÊØîËºÉ‰∫Ü‰∏âÁ®Æ‰∏çÂêåÁöÑÁâπÂæµÈáèÂåñÊñπÊ≥ïÔºå‰ª•Ë®ìÁ∑¥Â§öËÅ≤ÈÅìÊßãÂΩ¢Èü≥Ë®äÁ∑®Á¢ºÂô®ÔºöËÅØÂêàÈáèÂåñ„ÄÅÁâπÂæµÈáèÂåñÂíåÈÄöÈÅìÈáèÂåñ„ÄÇÂú®ÂæÆË™ø‰∏≠ÔºåÊàëÂÄëË®ìÁ∑¥‰∫ÜÂ§öËÅ≤ÈÅìÊßãÂΩ¢ËΩâÊèõÂô®„ÄÇÊâÄÊúâÂØ¶È©óÂùá‰ΩøÁî®ÈÅ†Â†¥ÂÖßÈÉ®Âíå CHiME-4 Ë≥áÊñôÈõÜÈÄ≤Ë°å„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÂú®ÈÄô‰∫õÊñπÊ≥ï‰∏≠ÔºåÁâπÂæµÈáèÂåñÊòØÊúÄÊúâÊïàÁöÑ„ÄÇÊàëÂÄëËßÄÂØüÂà∞ÔºåËàáÊ≤íÊúâ‰ªª‰ΩïÈ†êË®ìÁ∑¥ÁöÑÈÅ†Â†¥ÂÖßÈÉ®Ë≥áÊñôÈõÜÊ®°ÂûãÁõ∏ÊØîÔºåÂ≠óÂÖÉÈåØË™§ÁéáÁõ∏Â∞çÊ∏õÂ∞ë‰∫Ü 66%„ÄÇ

##### **LLM-Empowered Resource Allocation in Wireless Communications Systems**
2408.02944v1 by Woongsup Lee, Jeonghun Park

The recent success of large language models (LLMs) has spurred their
application in various fields. In particular, there have been efforts to
integrate LLMs into various aspects of wireless communication systems. The use
of LLMs in wireless communication systems has the potential to realize
artificial general intelligence (AGI)-enabled wireless networks. In this paper,
we investigate an LLM-based resource allocation scheme for wireless
communication systems. Specifically, we formulate a simple resource allocation
problem involving two transmit pairs and develop an LLM-based resource
allocation approach that aims to maximize either energy efficiency or spectral
efficiency. Additionally, we consider the joint use of low-complexity resource
allocation techniques to compensate for the reliability shortcomings of the
LLM-based scheme. After confirming the applicability and feasibility of
LLM-based resource allocation, we address several key technical challenges that
remain in applying LLMs in practice.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊúÄËøëÁöÑÊàêÂäüÔºå‰øÉ‰ΩøÂÆÉÂÄëÂú®ÂêÑÂÄãÈ†òÂüüÁöÑÊáâÁî®„ÄÇÁâπÂà•ÊòØÔºå‰∏ÄÁõ¥ÊúâÂä™ÂäõÂ∞á LLM Êï¥ÂêàÂà∞ÁÑ°Á∑öÈÄö‰ø°Á≥ªÁµ±ÁöÑÂêÑÂÄãÊñπÈù¢„ÄÇÂú®ÁÑ°Á∑öÈÄö‰ø°Á≥ªÁµ±‰∏≠‰ΩøÁî® LLM ÊúâÂèØËÉΩÂØ¶Áèæ‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖß (AGI) ÂïüÁî®ÁöÑÁÑ°Á∑öÁ∂≤Ë∑Ø„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÁ†îÁ©∂‰∫Ü‰∏ÄÁ®ÆÂü∫Êñº LLM ÁöÑÁÑ°Á∑öÈÄö‰ø°Á≥ªÁµ±Ë≥áÊ∫êÂàÜÈÖçÊñπÊ°à„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂà∂ÂÆö‰∫Ü‰∏ÄÂÄãÊ∂âÂèäÂÖ©ÂÄãÂÇ≥Ëº∏Â∞çÁöÑÁ∞°ÂñÆË≥áÊ∫êÂàÜÈÖçÂïèÈ°åÔºå‰∏¶ÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÂü∫Êñº LLM ÁöÑË≥áÊ∫êÂàÜÈÖçÊñπÊ≥ïÔºåÊó®Âú®ÊúÄÂ§ßÂåñËÉΩÈáèÊïàÁéáÊàñÈ†ªË≠úÊïàÁéá„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëËÄÉÊÖÆ‰∫Ü‰ΩéË§áÈõúÂ∫¶Ë≥áÊ∫êÂàÜÈÖçÊäÄË°ìÁöÑËÅØÂêà‰ΩøÁî®Ôºå‰ª•ÂΩåË£úÂü∫Êñº LLM ÁöÑÊñπÊ°àÁöÑÂèØÈù†ÊÄß‰∏çË∂≥„ÄÇÂú®Á¢∫Ë™ç‰∫ÜÂü∫Êñº LLM ÁöÑË≥áÊ∫êÂàÜÈÖçÁöÑÈÅ©Áî®ÊÄßÂíåÂèØË°åÊÄßÂæåÔºåÊàëÂÄëËß£Ê±∫‰∫ÜÂú®ÂØ¶Âãô‰∏≠ÊáâÁî® LLM ÊôÇ‰ªçÂ≠òÂú®ÁöÑÂπæÂÄãÈóúÈçµÊäÄË°ìÊåëÊà∞„ÄÇ

##### **HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection**
2408.02927v1 by Yuxin Wang, Duanyu Feng, Yongfu Dai, Zhengyu Chen, Jimin Huang, Sophia Ananiadou, Qianqian Xie, Hao Wang

Data serves as the fundamental foundation for advancing deep learning,
particularly tabular data presented in a structured format, which is highly
conducive to modeling. However, even in the era of LLM, obtaining tabular data
from sensitive domains remains a challenge due to privacy or copyright
concerns. Hence, exploring how to effectively use models like LLMs to generate
realistic and privacy-preserving synthetic tabular data is urgent. In this
paper, we take a step forward to explore LLMs for tabular data synthesis and
privacy protection, by introducing a new framework HARMONIC for tabular data
generation and evaluation. In the tabular data generation of our framework,
unlike previous small-scale LLM-based methods that rely on continued
pre-training, we explore the larger-scale LLMs with fine-tuning to generate
tabular data and enhance privacy. Based on idea of the k-nearest neighbors
algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to
discover inter-row relationships. Then, with fine-tuning, LLMs are trained to
remember the format and connections of the data rather than the data itself,
which reduces the risk of privacy leakage. In the evaluation part of our
framework, we develop specific privacy risk metrics DLT for LLM synthetic data
generation, as well as performance evaluation metrics LLE for downstream LLM
tasks. Our experiments find that this tabular data generation framework
achieves equivalent performance to existing methods with better privacy, which
also demonstrates our evaluation framework for the effectiveness of synthetic
data and privacy risks in LLM scenarios.

ÊëòË¶ÅÔºö<paragraph>Ë≥áÊñô‰ΩúÁÇ∫Êé®ÂãïÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂü∫Á§éÔºå
ÁâπÂà•ÊòØÁµêÊßãÂåñÂëàÁèæÁöÑË°®Ê†ºË≥áÊñôÔºåÈùûÂ∏∏ÈÅ©ÂêàÂª∫Ê®°„ÄÇÁÑ∂ËÄåÔºåÂç≥‰ΩøÂú® LLM ÊôÇ‰ª£ÔºåÁî±ÊñºÈö±ÁßÅÊàñÁâàÊ¨äÂïèÈ°åÔºåÂæûÊïèÊÑüÈ†òÂüüÂèñÂæóË°®Ê†ºË≥áÊñô‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂõ†Ê≠§ÔºåÊé¢Ë®éÂ¶Ç‰ΩïÊúâÊïà‰ΩøÁî® LLM Á≠âÊ®°Âûã‰æÜÁî¢ÁîüÈÄºÁúü‰∏î‰øùË≠∑Èö±ÁßÅÁöÑÂêàÊàêË°®Ê†ºË≥áÊñôÈùûÂ∏∏Ëø´Âàá„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Êé¢Ë®é LLM Âú®Ë°®Ê†ºË≥áÊñôÂêàÊàêÂíåÈö±ÁßÅ‰øùË≠∑ÊñπÈù¢ÁöÑÊáâÁî®Ôºå‰∏¶‰ªãÁ¥π‰∏ÄÂÄãÊñ∞ÁöÑË°®Ê†ºË≥áÊñôÁî¢ÁîüÂíåË©ï‰º∞Ê°ÜÊû∂ HARMONIC„ÄÇÂú®ÊàëÂÄëÊ°ÜÊû∂ÁöÑË°®Ê†ºË≥áÊñôÁî¢Áîü‰∏≠ÔºåËàá‰æùË≥¥ÊåÅÁ∫åÈ†êË®ìÁ∑¥ÁöÑÂÖàÂâçÂ∞èË¶èÊ®°Âü∫Êñº LLM ÁöÑÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÊé¢Ë®é‰ΩøÁî®ÂæÆË™øÁöÑÂ§ßË¶èÊ®° LLM ‰æÜÁî¢ÁîüË°®Ê†ºË≥áÊñô‰∏¶Â¢ûÂº∑Èö±ÁßÅ„ÄÇÂü∫Êñº k ÊúÄËøëÈÑ∞ÊºîÁÆóÊ≥ïÁöÑÊÉ≥Ê≥ïÔºåÊßãÂª∫‰∫Ü‰∏ÄÂÄãÊåá‰ª§ÂæÆË™øË≥áÊñôÈõÜÔºå‰ª•ÊøÄÂãµ LLM ÁôºÁèæÂàóÈñìÈóú‰øÇ„ÄÇÁÑ∂ÂæåÔºåÈÄèÈÅéÂæÆË™øÔºåË®ìÁ∑¥ LLM Ë®ò‰ΩèË≥áÊñôÁöÑÊ†ºÂºèÂíåÈóúËÅØÊÄßÔºåËÄå‰∏çÊòØË≥áÊñôÊú¨Ë∫´ÔºåÈÄôÈôç‰Ωé‰∫ÜÈö±ÁßÅÊ¥©Èú≤ÁöÑÈ¢®Èö™„ÄÇÂú®ÊàëÂÄëÊ°ÜÊû∂ÁöÑË©ï‰º∞ÈÉ®ÂàÜÔºåÊàëÂÄëÈñãÁôº‰∫ÜÈáùÂ∞ç LLM ÂêàÊàêË≥áÊñôÁî¢ÁîüÁöÑÁâπÂÆöÈö±ÁßÅÈ¢®Èö™ÊåáÊ®ô DLTÔºå‰ª•Âèä‰∏ãÊ∏∏ LLM ‰ªªÂãôÁöÑÊïàËÉΩË©ï‰º∞ÊåáÊ®ô LLE„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁôºÁèæÔºåÊ≠§Ë°®Ê†ºË≥áÊñôÁî¢ÁîüÊ°ÜÊû∂Âú®ÂÖ∑ÊúâÊõ¥Â•ΩÈö±ÁßÅÊÄßÁöÑÊÉÖÊ≥Å‰∏ãÔºåÈÅîÂà∞‰∫ÜËàáÁèæÊúâÊñπÊ≥ïÁõ∏Áï∂ÁöÑÊïàËÉΩÔºåÈÄô‰πüË≠âÊòé‰∫ÜÊàëÂÄëË©ï‰º∞Ê°ÜÊû∂Âú® LLM Â†¥ÊôØ‰∏≠ÂêàÊàêË≥áÊñôÂíåÈö±ÁßÅÈ¢®Èö™ÁöÑÊúâÊïàÊÄß„ÄÇ</paragraph>

##### **Intermediate direct preference optimization**
2408.02923v1 by Atsushi Kojima

We propose the intermediate direct preference optimization (DPO) method to
calculate the DPO loss at selected intermediate layers as an auxiliary loss for
finetuning large language models (LLMs). The conventional DPO method fine-tunes
a supervised fine-tuning (SFT) model by calculating the DPO loss using logits
from the final layer. In our intermediate DPO approach, DPO losses are
calculated using the logits from K-selected intermediate layers and averaged to
obtain the intermediate DPO loss. For training the intermediate DPO model, the
final loss is obtained by calculating the weighted sum of the DPO and
intermediate DPO losses. During inference, the intermediate DPO model decodes
using the final layer logits similarly to the conventional DPO model. In
experiments using the ultrafeedback dataset, the performance of the
intermediate DPO model was evaluated using GPT-4. As a result, the intermediate
DPO model trained using the intermediate DPO loss calculated at the 22nd layer
of a 32-layer SFT model achieved win rates of 52.5% and 67.5% against the
conventional DPO and SFT models, respectively, demonstrating the effectiveness
of the proposed method. Furthermore, we report the relationships among the
position of the selected intermediate layers, the number of layers, and
performance.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏≠ÈñìÁõ¥Êé•ÂÅèÂ•ΩÊúÄ‰Ω≥Âåñ (DPO) ÊñπÊ≥ïÔºå‰ª•Âú®ÈÅ∏ÂÆöÁöÑ‰∏≠ÈñìÂ±§Ë®àÁÆó DPO ÊêçÂ§±Ôºå‰ΩúÁÇ∫ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËºîÂä©ÊêçÂ§±„ÄÇÂÇ≥Áµ±ÁöÑ DPO ÊñπÊ≥ïÈÄöÈÅé‰ΩøÁî®‰æÜËá™ÊúÄÁµÇÂ±§ÁöÑ logit Ë®àÁÆó DPO ÊêçÂ§±ÔºåÂ∞çÁõ£Áù£ÂæÆË™ø (SFT) Ê®°ÂûãÈÄ≤Ë°åÂæÆË™ø„ÄÇÂú®ÊàëÂÄëÁöÑ‰∏≠Èñì DPO ÊñπÊ≥ï‰∏≠ÔºåDPO ÊêçÂ§±‰ΩøÁî®‰æÜËá™ K ÂÄãÈÅ∏ÂÆö‰∏≠ÈñìÂ±§ÁöÑ logit Ë®àÁÆóÔºå‰∏¶ÂèñÂπ≥ÂùáÂÄº‰ª•Áç≤Âæó‰∏≠Èñì DPO ÊêçÂ§±„ÄÇÁÇ∫‰∫ÜË®ìÁ∑¥‰∏≠Èñì DPO Ê®°ÂûãÔºåÊúÄÁµÇÊêçÂ§±ÊòØÈÄöÈÅéË®àÁÆó DPO Âíå‰∏≠Èñì DPO ÊêçÂ§±ÁöÑÂä†Ê¨äÂíå‰æÜÁç≤ÂæóÁöÑ„ÄÇÂú®Êé®ÁêÜÈÅéÁ®ã‰∏≠Ôºå‰∏≠Èñì DPO Ê®°Âûã‰ΩøÁî®ÊúÄÁµÇÂ±§ logit Ëß£Á¢ºÔºåÈ°û‰ººÊñºÂÇ≥Áµ±ÁöÑ DPO Ê®°Âûã„ÄÇÂú®‰ΩøÁî® ultrafeedback Ë≥áÊñôÈõÜÁöÑÂØ¶È©ó‰∏≠Ôºå‰ΩøÁî® GPT-4 Ë©ï‰º∞‰∫Ü‰∏≠Èñì DPO Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÁµêÊûúÔºå‰ΩøÁî®Âú® 32 Â±§ SFT Ê®°ÂûãÁöÑÁ¨¨ 22 Â±§Ë®àÁÆóÁöÑ‰∏≠Èñì DPO ÊêçÂ§±Ë®ìÁ∑¥ÁöÑ‰∏≠Èñì DPO Ê®°ÂûãÔºåÂàÜÂà•Â∞çÂÇ≥Áµ± DPO Âíå SFT Ê®°ÂûãÈÅîÂà∞‰∫Ü 52.5% Âíå 67.5% ÁöÑÁç≤ÂãùÁéáÔºåË≠âÊòé‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ†±Âëä‰∫ÜÈÅ∏ÂÆöÁöÑ‰∏≠ÈñìÂ±§ÁöÑ‰ΩçÁΩÆ„ÄÅÂ±§Êï∏ÂíåÊïàËÉΩ‰πãÈñìÁöÑÈóú‰øÇ„ÄÇ

##### **A Taxonomy of Architecture Options for Foundation Model-based Agents: Analysis and Decision Model**
2408.02920v1 by Jingwen Zhou, Qinghua Lu, Jieshan Chen, Liming Zhu, Xiwei Xu, Zhenchang Xing, Stefan Harrer

The rapid advancement of AI technology has led to widespread applications of
agent systems across various domains. However, the need for detailed
architecture design poses significant challenges in designing and operating
these systems. This paper introduces a taxonomy focused on the architectures of
foundation-model-based agents, addressing critical aspects such as functional
capabilities and non-functional qualities. We also discuss the operations
involved in both design-time and run-time phases, providing a comprehensive
view of architectural design and operational characteristics. By unifying and
detailing these classifications, our taxonomy aims to improve the design of
foundation-model-based agents. Additionally, the paper establishes a decision
model that guides critical design and runtime decisions, offering a structured
approach to enhance the development of foundation-model-based agents. Our
contributions include providing a structured architecture design option and
guiding the development process of foundation-model-based agents, thereby
addressing current fragmentation in the field.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖßÊäÄË°ìÁöÑÂø´ÈÄüÈÄ≤Ê≠•ÔºåÂ∑≤Â∞éËá¥‰ª£ÁêÜÁ≥ªÁµ±Âú®ÂêÑÂÄãÈ†òÂüüÂª£Ê≥õÊáâÁî®„ÄÇÁÑ∂ËÄåÔºåÂ∞çË©≥Á¥∞Êû∂ÊßãË®≠Ë®àÁöÑÈúÄÊ±ÇÂú®Ë®≠Ë®àÂíåÊìç‰ΩúÈÄô‰∫õÁ≥ªÁµ±ÊôÇÊèêÂá∫‰∫ÜÈáçÂ§ßÊåëÊà∞„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂàÜÈ°ûÔºåÈáçÈªûÈóúÊ≥®Âü∫ÊñºÂü∫Á§éÊ®°ÂûãÁöÑ‰ª£ÁêÜÊû∂ÊßãÔºåÊé¢Ë®éÂäüËÉΩËÉΩÂäõÂíåÈùûÂäüËÉΩÂìÅË≥™Á≠âÈóúÈçµÊñπÈù¢„ÄÇÊàëÂÄëÈÇÑË®éË´ñ‰∫ÜË®≠Ë®àÊôÇÂíåÈÅãË°åÊôÇÈöéÊÆµÊ∂âÂèäÁöÑÊìç‰ΩúÔºåÊèê‰æõ‰∫ÜÊû∂ÊßãË®≠Ë®àÂíåÈÅã‰ΩúÁâπÊÄßÁöÑÂÖ®Èù¢ËßÄÈªû„ÄÇÈÄöÈÅéÁµ±‰∏ÄÂíåË©≥Á¥∞Ë™™ÊòéÈÄô‰∫õÂàÜÈ°ûÔºåÊàëÂÄëÁöÑÂàÜÈ°ûÊ≥ïÊó®Âú®ÊîπÈÄ≤Âü∫ÊñºÂü∫Á§éÊ®°ÂûãÁöÑ‰ª£ÁêÜÁöÑË®≠Ë®à„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÊ±∫Á≠ñÊ®°ÂûãÔºåÁî®ÊñºÊåáÂ∞éÈóúÈçµË®≠Ë®àÂíåÈÅãË°åÊôÇÊ±∫Á≠ñÔºåÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÁµêÊßãÂåñÊñπÊ≥ï‰æÜÂ¢ûÂº∑Âü∫ÊñºÂü∫Á§éÊ®°ÂûãÁöÑ‰ª£ÁêÜÁöÑÈñãÁôº„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÂåÖÊã¨Êèê‰æõ‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÊû∂ÊßãË®≠Ë®àÈÅ∏È†ÖÂíåÊåáÂ∞éÂü∫ÊñºÂü∫Á§éÊ®°ÂûãÁöÑ‰ª£ÁêÜÁöÑÈñãÁôºÈÅéÁ®ãÔºåÂæûËÄåËß£Ê±∫Ë©≤È†òÂüüÁï∂ÂâçÁöÑÁ¢éÁâáÂåñÂïèÈ°å„ÄÇ

##### **Data Checklist: On Unit-Testing Datasets with Usable Information**
2408.02919v1 by Heidi C. Zhang, Shabnam Behzad, Kawin Ethayarajh, Dan Jurafsky

Model checklists (Ribeiro et al., 2020) have emerged as a useful tool for
understanding the behavior of LLMs, analogous to unit-testing in software
engineering. However, despite datasets being a key determinant of model
behavior, evaluating datasets, e.g., for the existence of annotation artifacts,
is largely done ad hoc, once a problem in model behavior has already been found
downstream. In this work, we take a more principled approach to unit-testing
datasets by proposing a taxonomy based on the V-information literature. We call
a collection of such unit tests a data checklist. Using a checklist, not only
are we able to recover known artifacts in well-known datasets such as SNLI, but
we also discover previously unknown artifacts in preference datasets for LLM
alignment. Data checklists further enable a new kind of data filtering, which
we use to improve the efficacy and data efficiency of preference alignment.

ÊëòË¶ÅÔºöÊ®°ÂûãÊ†∏Â∞çÊ∏ÖÂñÆÔºàRibeiro Á≠â‰∫∫Ôºå2020 Âπ¥ÔºâÂ∑≤ÊàêÁÇ∫‰∫ÜËß£ LLM Ë°åÁÇ∫ÁöÑÊúâÁî®Â∑•ÂÖ∑ÔºåÈ°û‰ººÊñºËªüÈ´îÂ∑•Á®ã‰∏≠ÁöÑÂñÆÂÖÉÊ∏¨Ë©¶„ÄÇÁÑ∂ËÄåÔºåÂÑòÁÆ°Ë≥áÊñôÈõÜÊòØÊ®°ÂûãË°åÁÇ∫ÁöÑ‰∏ªË¶ÅÊ±∫ÂÆöÂõ†Á¥†Ôºå‰ΩÜË©ï‰º∞Ë≥áÊñôÈõÜÔºà‰æãÂ¶ÇÔºåÊòØÂê¶Â≠òÂú®Ê®ôË®ª‰∫∫Â∑•Ë£ΩÂìÅÔºâÈÄöÂ∏∏ÊòØËá®ÊôÇÈÄ≤Ë°åÁöÑÔºå‰∏ÄÊó¶ÁôºÁèæÊ®°ÂûãË°åÁÇ∫‰∏≠ÁöÑÂïèÈ°åÔºåÂ∞±ÊúÉÂú®ÂæåÁ∫åÊµÅÁ®ã‰∏≠ÁôºÁèæ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé°Áî®Âü∫Êñº V ÂûãË≥áË®äÊñáÁçªÁöÑÂàÜÈ°ûÊ≥ïÔºåÂ∞çÂñÆÂÖÉÊ∏¨Ë©¶Ë≥áÊñôÈõÜÊé°ÂèñÊõ¥ÂÖ∑ÂéüÂâáÊÄßÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁ®±Ê≠§È°ûÂñÆÂÖÉÊ∏¨Ë©¶ÁöÑÈõÜÂêàÁÇ∫Ë≥áÊñôÊ†∏Â∞çÊ∏ÖÂñÆ„ÄÇÈÄèÈÅé‰ΩøÁî®Ê†∏Â∞çÊ∏ÖÂñÆÔºåÊàëÂÄë‰∏çÂÉÖËÉΩÂ§†Âú® SNLI Á≠âÁü•ÂêçË≥áÊñôÈõÜ‰∏≠ÊÅ¢Âæ©Â∑≤Áü•ÁöÑÁöÑ‰∫∫Â∑•Ë£ΩÂìÅÔºåÈÇÑËÉΩÁôºÁèæ LLM Â∞çÈΩäÂÅèÂ•ΩË≥áÊñôÈõÜ‰∏≠ÂÖàÂâçÊú™Áü•ÁöÑ‰∫∫Â∑•Ë£ΩÂìÅ„ÄÇË≥áÊñôÊ†∏Â∞çÊ∏ÖÂñÆÈÄ≤‰∏ÄÊ≠•ÂïüÁî®‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑË≥áÊñôÈÅéÊøæÔºåÊàëÂÄë‰ΩøÁî®ÂÆÉ‰æÜÊîπÂñÑÂÅèÂ•ΩÂ∞çÈΩäÁöÑÊïàËÉΩÂíåË≥áÊñôÊïàÁéá„ÄÇ

##### **KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance**
2408.02912v1 by Jingxian Lu, Wenke Xia, Dong Wang, Zhigang Wang, Bin Zhao, Di Hu, Xuelong Li

Online Imitation Learning methods struggle with the gap between extensive
online exploration space and limited expert trajectories, which hinder
efficient exploration due to inaccurate task-aware reward estimation. Inspired
by the findings from cognitive neuroscience that task decomposition could
facilitate cognitive processing for efficient learning, we hypothesize that an
agent could estimate precise task-aware imitation rewards for efficient online
exploration by decomposing the target task into the objectives of "what to do"
and the mechanisms of "how to do". In this work, we introduce the hybrid
Key-state guided Online Imitation (KOI) learning approach, which leverages the
integration of semantic and motion key states as guidance for task-aware reward
estimation. Initially, we utilize the visual-language models to segment the
expert trajectory into semantic key states, indicating the objectives of "what
to do". Within the intervals between semantic key states, optical flow is
employed to capture motion key states to understand the process of "how to do".
By integrating a thorough grasp of both semantic and motion key states, we
refine the trajectory-matching reward computation, encouraging task-aware
exploration for efficient online imitation learning. Our experiment results
prove that our method is more sample efficient in the Meta-World and LIBERO
environments. We also conduct real-world robotic manipulation experiments to
validate the efficacy of our method, demonstrating the practical applicability
of our KOI method.

ÊëòË¶ÅÔºöÁ∑ö‰∏äÊ®°‰ªøÂ≠∏ÁøíÊñπÊ≥ïÂú®ÊñºÂª£Ê≥õÁöÑÁ∑ö‰∏äÊé¢Á¥¢Á©∫ÈñìÂíåÊúâÈôêÁöÑÂ∞àÂÆ∂ËªåË∑°‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÈÄôÊúÉÈòªÁ§ôÊúâÊïàÁöÑÊé¢Á¥¢ÔºåÂõ†ÁÇ∫‰ªªÂãôÊÑüÁü•ÁçéÂãµ‰º∞Ë®à‰∏çÊ∫ñÁ¢∫„ÄÇÂèóË™çÁü•Á•ûÁ∂ìÁßëÂ≠∏ÁöÑÁôºÁèæÂïüÁôºÔºåÂç≥‰ªªÂãôÂàÜËß£ÂèØ‰ª•‰øÉÈÄ≤Ë™çÁü•ËôïÁêÜ‰ª•ÈÄ≤Ë°åÊúâÊïàÁöÑÂ≠∏ÁøíÔºåÊàëÂÄëÂÅáË®≠‰∏ÄÂÄã‰ª£ÁêÜÂèØ‰ª•ÈÄöÈÅéÂ∞áÁõÆÊ®ô‰ªªÂãôÂàÜËß£ÁÇ∫„ÄåË¶ÅÂÅö‰ªÄ‰πà„ÄçÁöÑÁõÆÊ®ôÂíå„ÄåÂ¶Ç‰ΩïÂÅö„ÄçÁöÑÊ©üÂà∂Ôºå‰º∞Ë®àÁ≤æÁ¢∫ÁöÑ‰ªªÂãôÊÑüÁü•Ê®°‰ªøÁçéÂãµ‰ª•ÈÄ≤Ë°åÊúâÊïàÁöÑÁ∑ö‰∏äÊé¢Á¥¢„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊ∑∑ÂêàÈóúÈçµÁãÄÊÖãÂºïÂ∞éÁöÑÁ∑ö‰∏äÊ®°‰ªø (KOI) Â≠∏ÁøíÊñπÊ≥ïÔºåÂÆÉÂà©Áî®Ë™ûÁæ©ÂíåÂãï‰ΩúÈóúÈçµÁãÄÊÖãÁöÑÊï¥Âêà‰ΩúÁÇ∫‰ªªÂãôÊÑüÁü•ÁçéÂãµ‰º∞Ë®àÁöÑÊåáÂ∞é„ÄÇÊúÄÂàùÔºåÊàëÂÄëÂà©Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÂ∞áÂ∞àÂÆ∂ËªåË∑°ÂàÜÂâ≤ÊàêË™ûÁæ©ÈóúÈçµÁãÄÊÖãÔºåË°®Á§∫„ÄåË¶ÅÂÅö‰ªÄ‰πà„ÄçÁöÑÁõÆÊ®ô„ÄÇÂú®Ë™ûÁæ©ÈóúÈçµÁãÄÊÖã‰πãÈñìÁöÑÈñìÈöîÂÖßÔºå‰ΩøÁî®ÂÖâÊµÅ‰æÜÊçïÊçâÂãï‰ΩúÈóúÈçµÁãÄÊÖã‰ª•‰∫ÜËß£„ÄåÂ¶Ç‰ΩïÂÅö„ÄçÁöÑÈÅéÁ®ã„ÄÇÈÄöÈÅéÊï¥ÂêàÂ∞çË™ûÁæ©ÂíåÂãï‰ΩúÈóúÈçµÁãÄÊÖãÁöÑÈÄèÂæπÊéåÊè°ÔºåÊàëÂÄëÊîπÈÄ≤‰∫ÜËªåË∑°ÂåπÈÖçÁçéÂãµË®àÁÆóÔºåÈºìÂãµ‰ªªÂãôÊÑüÁü•Êé¢Á¥¢‰ª•ÈÄ≤Ë°åÊúâÊïàÁöÑÁ∑ö‰∏äÊ®°‰ªøÂ≠∏Áøí„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú® Meta-World Âíå LIBERO Áí∞Â¢É‰∏≠ÂÖ∑ÊúâÊõ¥È´òÁöÑÊ®£Êú¨ÊïàÁéá„ÄÇÊàëÂÄëÈÇÑÈÄ≤Ë°å‰∫ÜÁúüÂØ¶‰∏ñÁïåÁöÑÊ©üÂô®‰∫∫Êìç‰ΩúÂØ¶È©óÔºå‰ª•È©óË≠âÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåË≠âÊòé‰∫ÜÊàëÂÄë KOI ÊñπÊ≥ïÁöÑÂØ¶ÈöõÈÅ©Áî®ÊÄß„ÄÇ

##### **Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large Language Model-Based Question Answering**
2408.02907v1 by Tiezheng Guo, Chen Wang, Yanyi Liu, Jiawei Tang, Pan Li, Sai Xu, Qingwen Yang, Xianlin Gao, Zhi Li, Yingyou Wen

Retrieving external knowledge and prompting large language models with
relevant information is an effective paradigm to enhance the performance of
question-answering tasks. Previous research typically handles paragraphs from
external documents in isolation, resulting in a lack of context and ambiguous
references, particularly in multi-document and complex tasks. To overcome these
challenges, we propose a new retrieval framework IIER, that leverages
Inter-chunk Interactions to Enhance Retrieval. This framework captures the
internal connections between document chunks by considering three types of
interactions: structural, keyword, and semantic. We then construct a unified
Chunk-Interaction Graph to represent all external documents comprehensively.
Additionally, we design a graph-based evidence chain retriever that utilizes
previous paths and chunk interactions to guide the retrieval process. It
identifies multiple seed nodes based on the target question and iteratively
searches for relevant chunks to gather supporting evidence. This retrieval
process refines the context and reasoning chain, aiding the large language
model in reasoning and answer generation. Extensive experiments demonstrate
that IIER outperforms strong baselines across four datasets, highlighting its
effectiveness in improving retrieval and reasoning capabilities.

ÊëòË¶ÅÔºöÂèñÂæóÂ§ñÈÉ®Áü•Ë≠ò‰∏¶ÊèêÁ§∫Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊèê‰æõÁõ∏ÈóúË≥áË®äÔºåÊòØÊèêÂçáÂïèÁ≠î‰ªªÂãôÊïàËÉΩÁöÑÊúâÊïàÂÖ∏ÁØÑ„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂ÈÄöÂ∏∏Â≠§Á´ãÂú∞ËôïÁêÜÂ§ñÈÉ®Êñá‰ª∂‰∏≠ÁöÑÊÆµËêΩÔºåÂ∞éËá¥Áº∫‰πèËÑàÁµ°ÂíåÊ®°Á®úÂÖ©ÂèØÁöÑÂèÉËÄÉÔºåÁâπÂà•ÊòØÂú®Â§öÊñá‰ª∂ÂíåË§áÈõúÁöÑ‰ªªÂãô‰∏≠„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÊ™¢Á¥¢Êû∂Êßã IIERÔºåÂà©Áî®ÂçÄÂ°äÈñì‰∫íÂãï‰æÜÂ¢ûÂº∑Ê™¢Á¥¢„ÄÇÈÄôÂÄãÊû∂ÊßãÈÄèÈÅéËÄÉÈáè‰∏âÁ®ÆÈ°ûÂûãÁöÑ‰∫íÂãï‰æÜÊì∑ÂèñÊñá‰ª∂ÂçÄÂ°ä‰πãÈñìÁöÑÂÖßÈÉ®ÈÄ£ÁµêÔºöÁµêÊßã„ÄÅÈóúÈçµÂ≠óÂíåË™ûÊÑè„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂª∫Êßã‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÂçÄÂ°ä‰∫íÂãïÂúñÔºå‰ª•ÂÖ®Èù¢Ë°®Á§∫ÊâÄÊúâÂ§ñÈÉ®Êñá‰ª∂„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∏ÄÂÄãÂü∫ÊñºÂúñÂΩ¢ÁöÑË≠âÊìöÈèàÊ™¢Á¥¢Âô®ÔºåÂà©Áî®ÂÖàÂâçÁöÑË∑ØÂæëÂíåÂçÄÂ°ä‰∫íÂãï‰æÜÂºïÂ∞éÊ™¢Á¥¢Á®ãÂ∫è„ÄÇÂÆÉÊ†πÊìöÁõÆÊ®ôÂïèÈ°åË≠òÂà•Â§öÂÄãÁ®ÆÂ≠êÁØÄÈªûÔºå‰∏¶ÂèçË¶ÜÊêúÂ∞ãÁõ∏ÈóúÂçÄÂ°ä‰ª•Êî∂ÈõÜ‰ΩêË≠âË≠âÊìö„ÄÇÈÄôÂÄãÊ™¢Á¥¢Á®ãÂ∫èÁ≤æÁÖâ‰∫ÜËÑàÁµ°ÂíåÊé®ÁêÜÈèàÔºåÂçîÂä©Â§ßÂûãË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°åÊé®ÁêÜÂíåÁ≠îÊ°àÁî¢Áîü„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåIIER Âú®ÂõõÂÄãË≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÂº∑Â§ßÁöÑÂü∫Ê∫ñÔºåÁ™ÅÈ°ØÂÖ∂Âú®ÊîπÂñÑÊ™¢Á¥¢ÂíåÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑÊïàËÉΩ„ÄÇ

##### **Enabling Intelligent Traffic Systems: A Deep Learning Method for Accurate Arabic License Plate Recognition**
2408.02904v1 by M. A. Sayedelahl

This paper introduces a novel two-stage framework for accurate Egyptian
Vehicle License Plate Recognition (EVLPR). The first stage employs image
processing techniques to reliably localize license plates, while the second
stage utilizes a custom-designed deep learning model for robust Arabic
character recognition. The proposed system achieves a remarkable 99.3% accuracy
on a diverse dataset, surpassing existing approaches. Its potential
applications extend to intelligent traffic management, including traffic
violation detection and parking optimization. Future research will focus on
enhancing the system's capabilities through architectural refinements, expanded
datasets, and addressing system dependencies.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂÖ©ÈöéÊÆµÊû∂ÊßãÔºåÁî®ÊñºÁ≤æÁ¢∫ÁöÑÂüÉÂèäËªäÁâåË≠òÂà• (EVLPR)„ÄÇÁ¨¨‰∏ÄÈöéÊÆµÊé°Áî®ÂΩ±ÂÉèËôïÁêÜÊäÄË°ì‰æÜÂèØÈù†Âú∞ÂÆö‰ΩçËªäÁâåÔºåËÄåÁ¨¨‰∫åÈöéÊÆµÂâáÂà©Áî®ÂÆ¢Ë£ΩÂåñË®≠Ë®àÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºåÁî®ÊñºÂº∑Â§ßÁöÑÈòøÊãâ‰ºØÊñáÂ≠óËæ®Ë≠ò„ÄÇÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±Âú®‰∏ÄÂÄãÂ§öÊ®£ÂåñÁöÑË≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫Ü 99.3% ÁöÑÈ©ö‰∫∫Ê∫ñÁ¢∫Â∫¶ÔºåË∂ÖË∂ä‰∫ÜÁèæÊúâÁöÑÊñπÊ≥ï„ÄÇÂÖ∂ÊΩõÂú®ÊáâÁî®Âª∂‰º∏Âà∞Êô∫ÊÖß‰∫§ÈÄöÁÆ°ÁêÜÔºåÂåÖÊã¨‰∫§ÈÄöÈÅïË¶èÂÅµÊ∏¨ÂíåÂÅúËªäÂÑ™Âåñ„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂Â∞áÂ∞àÊ≥®ÊñºÈÄèÈÅéÊû∂ÊßãÊîπËâØ„ÄÅÊì¥ÂÖÖË≥áÊñôÈõÜÂíåËß£Ê±∫Á≥ªÁµ±‰æùË≥¥ÊÄß‰æÜÂ¢ûÂº∑Á≥ªÁµ±ÁöÑÂäüËÉΩ„ÄÇ

##### **Lighthouse: A User-Friendly Library for Reproducible Video Moment Retrieval and Highlight Detection**
2408.02901v1 by Taichi Nishimura, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu

We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫ LighthouseÔºå‰∏ÄÂÄãÁî®ÊñºÂèØÈáçË£ΩÂΩ±ÁâáÊôÇÂàªÊ™¢Á¥¢ÂíåÈáçÈªûÂÅµÊ∏¨ (MR-HD) ÁöÑ‰ΩøÁî®ËÄÖÂèãÂñÑÁ®ãÂºèÂ∫´„ÄÇÂÑòÁÆ°Á†îÁ©∂‰∫∫Âì°ÊèêÂá∫‰∫ÜÂêÑÁ®Æ MR-HD ÊñπÊ≥ïÔºå‰ΩÜÁ†îÁ©∂Á§æÁæ§‰ªçÊúâÂÖ©ÂÄã‰∏ªË¶ÅÂïèÈ°å„ÄÇÁ¨¨‰∏ÄÂÄãÂïèÈ°åÊòØÁº∫‰πèË∑®ÂêÑÁ®ÆÊñπÊ≥ï„ÄÅË≥áÊñôÈõÜÂíåÂΩ±ÁâáÊñáÂ≠óÁâπÂæµÁöÑÂÖ®Èù¢‰∏îÂèØÈáçË£ΩÁöÑÂØ¶È©ó„ÄÇÈÄôÊòØÂõ†ÁÇ∫Ê≤íÊúâÁµ±‰∏ÄÁöÑË®ìÁ∑¥ÂíåË©ï‰º∞Á®ãÂºèÁ¢ºÂ∫´Ê∂µËìãÂ§öÁ®ÆË®≠ÂÆö„ÄÇÁ¨¨‰∫åÂÄãÂïèÈ°åÊòØ‰ΩøÁî®ËÄÖ‰∏çÂèãÂñÑÁöÑË®≠Ë®à„ÄÇÁî±ÊñºÂÖàÂâçÁöÑËëó‰Ωú‰ΩøÁî®‰∏çÂêåÁöÑÁ®ãÂºèÂ∫´ÔºåÂõ†Ê≠§Á†îÁ©∂‰∫∫Âì°ÊúÉË®≠ÂÆöÂÄãÂà•ÁöÑÁí∞Â¢É„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏Ëëó‰ΩúÂÉÖÁôºÂ∏ÉË®ìÁ∑¥Á®ãÂºèÁ¢ºÔºåË¶ÅÊ±Ç‰ΩøÁî®ËÄÖÂØ¶‰Ωú MR-HD ÁöÑÊï¥ÂÄãÊé®Ë´ñÊµÅÁ®ã„ÄÇLighthouse ÈÄèÈÅéÂØ¶‰Ωú‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÂèØÈáçË£ΩÁ®ãÂºèÁ¢ºÂ∫´‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂÖ≠ÂÄãÊ®°Âûã„ÄÅ‰∏âÂÄãÁâπÂæµÂíå‰∫îÂÄãË≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºåÂÆÉÊèê‰æõ‰∏ÄÂÄãÊé®Ë´ñ API ÂíåÁ∂≤Ë∑ØÂ±ïÁ§∫ÔºåËÆìÁ†îÁ©∂‰∫∫Âì°ÂíåÈñãÁôº‰∫∫Âì°ÂèØ‰ª•ËºïÈ¨ÜÂ≠òÂèñÈÄô‰∫õÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé Lighthouse ÈÄöÂ∏∏ÊúÉÈáçË£ΩÂèÉËÄÉË´ñÊñá‰∏≠Â†±ÂëäÁöÑÂàÜÊï∏„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/line/lighthouse ÂèñÂæó„ÄÇ

##### **SETN: Stock Embedding Enhanced with Textual and Network Information**
2408.02899v1 by Takehiro Takayanagi, Hiroki Sakaji, Kiyoshi Izumi

Stock embedding is a method for vector representation of stocks. There is a
growing demand for vector representations of stock, i.e., stock embedding, in
wealth management sectors, and the method has been applied to various tasks
such as stock price prediction, portfolio optimization, and similar fund
identifications. Stock embeddings have the advantage of enabling the
quantification of relative relationships between stocks, and they can extract
useful information from unstructured data such as text and network data. In
this study, we propose stock embedding enhanced with textual and network
information (SETN) using a domain-adaptive pre-trained transformer-based model
to embed textual information and a graph neural network model to grasp network
information. We evaluate the performance of our proposed model on related
company information extraction tasks. We also demonstrate that stock embeddings
obtained from the proposed model perform better in creating thematic funds than
those obtained from baseline methods, providing a promising pathway for various
applications in the wealth management industry.

ÊëòË¶ÅÔºöËÇ°Á•®ÂµåÂÖ•ÊòØ‰∏ÄÁ®ÆËÇ°Á•®ÂêëÈáèË°®Á§∫ÊñπÊ≥ï„ÄÇÂú®Ë≤°ÂØåÁÆ°ÁêÜÈÉ®ÈñÄ‰∏≠ÔºåÂ∞çËÇ°Á•®ÂêëÈáèË°®Á§∫ÔºàÂç≥ËÇ°Á•®ÂµåÂÖ•ÔºâÁöÑÈúÄÊ±ÇÊó•ÁõäÂ¢ûÈï∑Ôºå‰∏¶‰∏îË©≤ÊñπÊ≥ïÂ∑≤ÊáâÁî®ÊñºÂêÑÁ®Æ‰ªªÂãôÔºå‰æãÂ¶ÇËÇ°Á•®ÂÉπÊ†ºÈ†êÊ∏¨„ÄÅÊäïË≥áÁµÑÂêàÊúÄ‰Ω≥ÂåñÂíåÈ°û‰ººÂü∫ÈáëË≠òÂà•„ÄÇËÇ°Á•®ÂµåÂÖ•ÁöÑÂÑ™ÈªûÂú®ÊñºËÉΩÂ§†ÈáèÂåñËÇ°Á•®‰πãÈñìÁöÑÁõ∏Â∞çÈóú‰øÇÔºå‰∏¶‰∏îÂÆÉÂÄëÂèØ‰ª•ÂæûÈùûÁµêÊßãÂåñÊï∏ÊìöÔºà‰æãÂ¶ÇÊñáÊú¨ÂíåÁ∂≤Ë∑ØÊï∏ÊìöÔºâ‰∏≠ÊèêÂèñÊúâÁî®ÁöÑË≥áË®ä„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰ΩøÁî®È†òÂüüËá™ÈÅ©ÊáâÈ†êË®ìÁ∑¥ËΩâÊèõÂô®Ê®°Âûã‰æÜÂµåÂÖ•ÊñáÊú¨Ë≥áË®äÔºå‰∏¶‰ΩøÁî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°Âûã‰æÜÊéåÊè°Á∂≤Ë∑ØË≥áË®äÔºå‰ª•Â¢ûÂº∑ÊñáÊú¨ÂíåÁ∂≤Ë∑ØË≥áË®äÁöÑËÇ°Á•®ÂµåÂÖ• (SETN)„ÄÇÊàëÂÄëÂú®Áõ∏ÈóúÂÖ¨Âè∏Ë≥áË®äËêÉÂèñ‰ªªÂãô‰∏äË©ï‰º∞ÊàëÂÄëÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊàëÂÄë‰πüË≠âÊòéÔºåÂæûÊâÄÊèêÂá∫ÁöÑÊ®°Âûã‰∏≠ÂèñÂæóÁöÑËÇ°Á•®ÂµåÂÖ•Âú®Âª∫Á´ã‰∏ªÈ°åÂü∫ÈáëÊôÇÔºåË°®ÁèæÂÑ™ÊñºÂæûÂü∫Á∑öÊñπÊ≥ï‰∏≠ÂèñÂæóÁöÑËÇ°Á•®ÂµåÂÖ•ÔºåÁÇ∫Ë≤°ÂØåÁÆ°ÁêÜÁî¢Ê•≠‰∏≠ÁöÑÂêÑÁ®ÆÊáâÁî®Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÈÄîÂæë„ÄÇ

##### **A Metric Driven Approach to Mixed Precision Training**
2408.02897v1 by Mitchelle Rasquinha, Gil Tabak

As deep learning methodologies have developed, it has been generally agreed
that increasing neural network size improves model quality. However, this is at
the expense of memory and compute requirements, which also need to be
increased. Various efficiency techniques have been proposed to rein in hardware
costs, one being the use of low precision numerics. Recent accelerators have
introduced several different 8-bit data types to help accommodate DNNs in terms
of numerics. In this paper, we identify a metric driven methodology to aid in
the choice of numerics. We demonstrate how such a methodology can help scale
training of a language representation model. The technique can be generalized
to other model architectures.

ÊëòË¶ÅÔºöÈö®ËëóÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÁöÑÁôºÂ±ïÔºå‰∫∫ÂÄëÊôÆÈÅçË™çÁÇ∫Â¢ûÂä†Á•ûÁ∂ìÁ∂≤Ë∑ØË¶èÊ®°ÂèØ‰ª•ÊèêÂçáÊ®°ÂûãÂìÅË≥™„ÄÇÁÑ∂ËÄåÔºåÈÄôÊúÉ‰ª•Ë®òÊÜ∂È´îÂíåÈÅãÁÆóÈúÄÊ±ÇÁÇ∫‰ª£ÂÉπÔºåËÄåÈÄô‰∫õÈúÄÊ±Ç‰πüÈúÄË¶ÅÂ¢ûÂä†„ÄÇÁÇ∫‰∫ÜÊéßÂà∂Á°¨È´îÊàêÊú¨ÔºåÂ∑≤Á∂ìÊèêÂá∫‰∫ÜÂêÑÁ®ÆÊïàÁéáÊäÄË°ìÔºåÂÖ∂‰∏≠‰∏ÄÁ®ÆÂ∞±ÊòØ‰ΩøÁî®‰ΩéÁ≤æÂ∫¶Êï∏ÂÄº„ÄÇÊúÄËøëÁöÑÂä†ÈÄüÂô®ÂºïÂÖ•‰∫ÜÊï∏Á®Æ‰∏çÂêåÁöÑ 8 ‰ΩçÂÖÉË≥áÊñôÈ°ûÂûãÔºå‰ª•Âπ´Âä©Âú®Êï∏ÂÄºÊñπÈù¢ÂÆπÁ¥ç DNN„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊâæÂá∫‰∏ÄÂÄãÁî±ÊåáÊ®ôÈ©ÖÂãïÁöÑÊñπÊ≥ïÔºå‰ª•ÂçîÂä©ÈÅ∏ÊìáÊï∏ÂÄº„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÈÄôÁ®ÆÊñπÊ≥ïÂ¶Ç‰ΩïÊúâÂä©ÊñºÊì¥ÂÖÖË™ûË®ÄË°®Á§∫Ê®°ÂûãÁöÑË®ìÁ∑¥„ÄÇÊ≠§ÊäÄË°ìÂèØ‰ª•Âª£Ê≥õÊáâÁî®ÊñºÂÖ∂‰ªñÊ®°ÂûãÊû∂Êßã„ÄÇ

##### **VizECGNet: Visual ECG Image Network for Cardiovascular Diseases Classification with Multi-Modal Training and Knowledge Distillation**
2408.02888v1 by Ju-Hyeon Nam, Seo-Hyung Park, Su Jung Kim, Sang-Chul Lee

An electrocardiogram (ECG) captures the heart's electrical signal to assess
various heart conditions. In practice, ECG data is stored as either digitized
signals or printed images. Despite the emergence of numerous deep learning
models for digitized signals, many hospitals prefer image storage due to cost
considerations. Recognizing the unavailability of raw ECG signals in many
clinical settings, we propose VizECGNet, which uses only printed ECG graphics
to determine the prognosis of multiple cardiovascular diseases. During
training, cross-modal attention modules (CMAM) are used to integrate
information from two modalities - image and signal, while self-modality
attention modules (SMAM) capture inherent long-range dependencies in ECG data
of each modality. Additionally, we utilize knowledge distillation to improve
the similarity between two distinct predictions from each modality stream. This
innovative multi-modal deep learning architecture enables the utilization of
only ECG images during inference. VizECGNet with image input achieves higher
performance in precision, recall, and F1-Score compared to signal-based ECG
classification models, with improvements of 3.50%, 8.21%, and 7.38%,
respectively.

ÊëòË¶ÅÔºöÂøÉÈõªÂúñ (ECG) ÂèØÊì∑ÂèñÂøÉËáüÁöÑÈõªÊ∞£Ë®äËôüÔºåÁî®ÊñºË©ï‰º∞ÂêÑÁ®ÆÂøÉËáüÁñæÁóÖ„ÄÇÂØ¶Èöõ‰∏äÔºåÂøÉÈõªÂúñË≥áÊñôÂÑ≤Â≠òÂú®Êï∏‰ΩçÂåñË®äËôüÊàñÂàóÂç∞ÂΩ±ÂÉè‰∏≠„ÄÇÂÑòÁÆ°Â∑≤Âá∫ÁèæË®±Â§öÈáùÂ∞çÊï∏‰ΩçÂåñË®äËôüÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰ΩÜË®±Â§öÈÜ´Èô¢Âü∫ÊñºÊàêÊú¨ËÄÉÈáèÔºå‰ªçÂÅèÂ•ΩÂΩ±ÂÉèÂÑ≤Â≠ò„ÄÇÈëëÊñºË®±Â§öËá®Â∫äÁí∞Â¢É‰∏≠Áº∫‰πèÂéüÂßãÂøÉÈõªÂúñË®äËôüÔºåÊàëÂÄëÊèêÂá∫ VizECGNetÔºåÂÆÉÂÉÖ‰ΩøÁî®ÂàóÂç∞ÁöÑÂøÉÈõªÂúñÂúñÂΩ¢‰æÜÂà§Êñ∑Â§öÁ®ÆÂøÉË°ÄÁÆ°ÁñæÁóÖÁöÑÈ†êÂæå„ÄÇÂú®Ë®ìÁ∑¥ÊúüÈñìÔºåË∑®Ê®°ÊÖãÊ≥®ÊÑèÂäõÊ®°ÁµÑ (CMAM) Áî®ÊñºÊï¥Âêà‰æÜËá™ÂÖ©Á®ÆÊ®°ÊÖãÔºàÂΩ±ÂÉèÂíåË®äËôüÔºâÁöÑË≥áË®äÔºåËÄåËá™ÊàëÊ®°ÊÖãÊ≥®ÊÑèÂäõÊ®°ÁµÑ (SMAM) ÂâáÊì∑ÂèñÊØèÂÄãÊ®°ÊÖã‰∏≠ÂøÉÈõªÂúñË≥áÊñô‰∏≠Âõ∫ÊúâÁöÑÈï∑Á®ã‰æùË≥¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®Áü•Ë≠òËêÉÂèñ‰æÜÊîπÂñÑÊØèÂÄãÊ®°ÊÖã‰∏≤ÊµÅ‰∏≠ÂÖ©ÂÄã‰∏çÂêåÈ†êÊ∏¨‰πãÈñìÁöÑÁõ∏‰ººÊÄß„ÄÇÈÄôÁ®ÆÂâµÊñ∞ÁöÑÂ§öÊ®°ÊÖãÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÔºåÂèØ‰ª•Âú®Êé®Ë´ñÊúüÈñìÂÉÖ‰ΩøÁî®ÂøÉÈõªÂúñÂΩ±ÂÉè„ÄÇËàáÂü∫ÊñºË®äËôüÁöÑÂøÉÈõªÂúñÂàÜÈ°ûÊ®°ÂûãÁõ∏ÊØîÔºåËº∏ÂÖ•ÂΩ±ÂÉèÁöÑ VizECGNet Âú®Á≤æÊ∫ñÂ∫¶„ÄÅÂè¨ÂõûÁéáÂíå F1 ÂàÜÊï∏ÊñπÈù¢Áç≤ÂæóÊõ¥È´òÁöÑÊïàËÉΩÔºåÂàÜÂà•ÊèêÂçá‰∫Ü 3.50%„ÄÅ8.21% Âíå 7.38%„ÄÇ

##### **Compromising Embodied Agents with Contextual Backdoor Attacks**
2408.02882v1 by Aishan Liu, Yuguang Zhou, Xianglong Liu, Tianyuan Zhang, Siyuan Liang, Jiakai Wang, Yanjun Pu, Tianlin Li, Junqi Zhang, Wenbo Zhou, Qing Guo, Dacheng Tao

Large language models (LLMs) have transformed the development of embodied
intelligence. By providing a few contextual demonstrations, developers can
utilize the extensive internal knowledge of LLMs to effortlessly translate
complex tasks described in abstract language into sequences of code snippets,
which will serve as the execution logic for embodied agents. However, this
paper uncovers a significant backdoor security threat within this process and
introduces a novel method called \method{}. By poisoning just a few contextual
demonstrations, attackers can covertly compromise the contextual environment of
a black-box LLM, prompting it to generate programs with context-dependent
defects. These programs appear logically sound but contain defects that can
activate and induce unintended behaviors when the operational agent encounters
specific triggers in its interactive environment. To compromise the LLM's
contextual environment, we employ adversarial in-context generation to optimize
poisoned demonstrations, where an LLM judge evaluates these poisoned prompts,
reporting to an additional LLM that iteratively optimizes the demonstration in
a two-player adversarial game using chain-of-thought reasoning. To enable
context-dependent behaviors in downstream agents, we implement a dual-modality
activation strategy that controls both the generation and execution of program
defects through textual and visual triggers. We expand the scope of our attack
by developing five program defect modes that compromise key aspects of
confidentiality, integrity, and availability in embodied agents. To validate
the effectiveness of our approach, we conducted extensive experiments across
various tasks, including robot planning, robot manipulation, and compositional
visual reasoning. Additionally, we demonstrate the potential impact of our
approach by successfully attacking real-world autonomous driving systems.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ËΩâËÆäÂÖ∑Ë∫´Êô∫ËÉΩÁöÑÁôºÂ±ï„ÄÇÈÄèÈÅéÊèê‰æõ‰∏Ä‰∫õË™ûÂ¢ÉÁ§∫ÁØÑÔºåÈñãÁôº‰∫∫Âì°ÂèØ‰ª•Âà©Áî® LLM Âª£Ê≥õÁöÑÂÖßÈÉ®Áü•Ë≠òÔºåÊØ´‰∏çË≤ªÂäõÂú∞Â∞á‰ª•ÊäΩË±°Ë™ûË®ÄÊèèËø∞ÁöÑË§áÈõú‰ªªÂãôËΩâÊèõÁÇ∫Á®ãÂºèÁ¢ºÁâáÊÆµÂ∫èÂàóÔºåÈÄô‰∫õÂ∫èÂàóÂ∞á‰ΩúÁÇ∫ÂÖ∑Ë∫´‰ª£ÁêÜÁöÑÂü∑Ë°åÈÇèËºØ„ÄÇÁÑ∂ËÄåÔºåÊú¨ÊñáÊè≠Èú≤‰∫ÜÈÄôÂÄãÈÅéÁ®ã‰∏≠‰∏ÄÂÄãÈáçÂ§ßÁöÑÂæåÈñÄÂÆâÂÖ®Â®ÅËÑÖÔºå‰∏¶‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫ \method{} ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÈÄèÈÅéÊØíÂÆ≥ÂÉÖÂ∞ëÊï∏ÂπæÂÄãË™ûÂ¢ÉÁ§∫ÁØÑÔºåÊîªÊìäËÄÖÂèØ‰ª•ÁßòÂØÜÂú∞Âç±ÂÆ≥ÈªëÁõí LLM ÁöÑË™ûÂ¢ÉÁí∞Â¢ÉÔºå‰øÉ‰ΩøÂÆÉÁî¢ÁîüÂÖ∑ÊúâË™ûÂ¢É‰æùË≥¥ÊÄßÁº∫Èô∑ÁöÑÁ®ãÂºè„ÄÇÈÄô‰∫õÁ®ãÂºèÁúãËµ∑‰æÜÂêà‰πéÈÇèËºØÔºå‰ΩÜÂåÖÂê´Áº∫Èô∑ÔºåÁï∂ÈÅã‰Ωú‰ª£ÁêÜÂú®‰∫íÂãïÁí∞Â¢É‰∏≠ÈÅáÂà∞ÁâπÂÆöËß∏ÁôºÂô®ÊôÇÔºåÈÄô‰∫õÁº∫Èô∑ÊúÉË¢´ÂïüÂãï‰∏¶ÂºïÁôºÊÑèÂ§ñÁöÑË°åÁÇ∫„ÄÇÁÇ∫‰∫ÜÂç±ÂÆ≥ LLM ÁöÑË™ûÂ¢ÉÁí∞Â¢ÉÔºåÊàëÂÄëÊé°Áî®Â∞çÊäóÊÄßÁöÑË™ûÂ¢É‰∏≠Áî¢Áîü‰æÜÊúÄ‰Ω≥Âåñ‰∏≠ÊØíÁöÑÁ§∫ÁØÑÔºåÂÖ∂‰∏≠‰∏ÄÂÄã LLM Ë©ïÂà§Âì°ÊúÉË©ï‰º∞ÈÄô‰∫õ‰∏≠ÊØíÁöÑÊèêÁ§∫ÔºåÂêëÂè¶‰∏ÄÂÄã LLM Â†±ÂëäÔºåÂæåËÄÖÊúÉÂú®‰∏ÄÂÄãÈõô‰∫∫Â∞çÊäóÈÅäÊà≤‰∏≠‰ΩøÁî®ÊÄùÁ∂≠ÈèàÊé®ÁêÜÂèçË¶ÜÊúÄ‰Ω≥ÂåñÁ§∫ÁØÑ„ÄÇÁÇ∫‰∫ÜÂú®‰∏ãÊ∏∏‰ª£ÁêÜ‰∏≠ÂïüÁî®Ë™ûÂ¢É‰æùË≥¥ÊÄßË°åÁÇ∫ÔºåÊàëÂÄëÂØ¶‰Ωú‰∫Ü‰∏ÄÂÄãÈõôÊ®°ÊÖãÂïüÂãïÁ≠ñÁï•ÔºåÈÄèÈÅéÊñáÂ≠óÂíåË¶ñË¶∫Ëß∏ÁôºÂô®ÊéßÂà∂Á®ãÂºèÁº∫Èô∑ÁöÑÁî¢ÁîüÂíåÂü∑Ë°å„ÄÇÈÄèÈÅéÈñãÁôº‰∫îÁ®ÆÂç±ÂÆ≥ÂÖ∑Ë∫´‰ª£ÁêÜ‰∏≠Ê©üÂØÜÊÄß„ÄÅÂÆåÊï¥ÊÄßÂèäÂèØÁî®ÊÄßÁöÑÈóúÈçµÈù¢ÂêëÁöÑÁ®ãÂºèÁº∫Èô∑Ê®°ÂºèÔºåÊàëÂÄëÊì¥Â§ß‰∫ÜÊîªÊìäÁØÑÂúç„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊàëÂÄëÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåÂåÖÊã¨Ê©üÂô®‰∫∫Ë¶èÂäÉ„ÄÅÊ©üÂô®‰∫∫Êìç‰ΩúÂíåÁµÑÂêàË¶ñË¶∫Êé®ÁêÜ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄèÈÅéÊàêÂäüÊîªÊìäÁúüÂØ¶‰∏ñÁïåÁöÑËá™ÂãïÈßïÈßõÁ≥ªÁµ±ÔºåÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊΩõÂú®ÂΩ±Èüø„ÄÇ

##### **Hide and Seek: Fingerprinting Large Language Models with Evolutionary Learning**
2408.02871v1 by Dmitri Iourovitski, Sanat Sharma, Rakshak Talwar

As content generated by Large Language Model (LLM) has grown exponentially,
the ability to accurately identify and fingerprint such text has become
increasingly crucial. In this work, we introduce a novel black-box approach for
fingerprinting LLMs, achieving an impressive 72% accuracy in identifying the
correct family of models (Such as Llama, Mistral, Gemma, etc) among a lineup of
LLMs. We present an evolutionary strategy that leverages the capabilities of
one LLM to discover the most salient features for identifying other LLMs. Our
method employs a unique "Hide and Seek" algorithm, where an Auditor LLM
generates discriminative prompts, and a Detective LLM analyzes the responses to
fingerprint the target models. This approach not only demonstrates the
feasibility of LLM-driven model identification but also reveals insights into
the semantic manifolds of different LLM families. By iteratively refining
prompts through in-context learning, our system uncovers subtle distinctions
between model outputs, providing a powerful tool for LLM analysis and
verification. This research opens new avenues for understanding LLM behavior
and has significant implications for model attribution, security, and the
broader field of AI transparency.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁîüÊàêÁöÑÂÖßÂÆπÂëàÊåáÊï∏Á¥öÂ¢ûÈï∑ÔºåÊ∫ñÁ¢∫Ë≠òÂà•ÂíåËæ®Ë≠òÊ≠§È°ûÊñáÊú¨ÁöÑËÉΩÂäõËÆäÂæóË∂ä‰æÜË∂äÈóúÈçµ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈªëÁõíÊñπÊ≥ï‰æÜËæ®Ë≠ò LLMÔºåÂú®Ëæ®Ë≠ò‰∏ÄÁµÑ LLM ‰∏≠Ê≠£Á¢∫ÁöÑÊ®°ÂûãÂÆ∂ÊóèÔºà‰æãÂ¶Ç Llama„ÄÅMistral„ÄÅGemma Á≠âÔºâÊñπÈù¢ÈÅîÂà∞‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑ 72% Ê∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊºîÂåñÁ≠ñÁï•ÔºåË©≤Á≠ñÁï•Âà©Áî®‰∏ÄÂÄã LLM ÁöÑËÉΩÂäõ‰æÜÁôºÁèæËæ®Ë≠òÂÖ∂‰ªñ LLM ÊúÄÈ°ØËëóÁöÑÁâπÂæµ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÊé°Áî®Áç®ÁâπÁöÑ„ÄåÊçâËø∑Ëóè„ÄçÊºîÁÆóÊ≥ïÔºåÂÖ∂‰∏≠ÂØ©Ê†∏ LLM ÊúÉÁî¢ÁîüÂçÄÂàÜÊèêÁ§∫ÔºåËÄåÂÅµÊé¢ LLM ÊúÉÂàÜÊûêÂõûÊáâ‰ª•Ëæ®Ë≠òÁõÆÊ®ôÊ®°Âûã„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰∏çÂÉÖÂ±ïÁ§∫‰∫Ü LLM È©ÖÂãïÊ®°ÂûãË≠òÂà•ÁöÑÂèØË°åÊÄßÔºåÈÇÑÊè≠Á§∫‰∫Ü‰∏çÂêå LLM ÂÆ∂ÊóèÁöÑË™ûÁæ©ÊµÅÂΩ¢„ÄÇÈÄöÈÅéÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏ÁøíÂèçË¶ÜË™øÊï¥ÊèêÁ§∫ÔºåÊàëÂÄëÁöÑÁ≥ªÁµ±Êè≠Á§∫‰∫ÜÊ®°ÂûãËº∏Âá∫‰πãÈñìÁöÑÁ¥∞ÂæÆÂçÄÂà•ÔºåÁÇ∫ LLM ÂàÜÊûêÂíåÈ©óË≠âÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂº∑Â§ßÁöÑÂ∑•ÂÖ∑„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁÇ∫ÁêÜËß£ LLM Ë°åÁÇ∫ÈñãÈó¢‰∫ÜÊñ∞ÈÄîÂæëÔºå‰∏¶Â∞çÊ®°ÂûãÊ≠∏Âõ†„ÄÅÂÆâÂÖ®ÊÄß‰ª•ÂèäÊõ¥Âª£Ê≥õÁöÑ‰∫∫Â∑•Êô∫ÊÖßÈÄèÊòéÂ∫¶È†òÂüüÁî¢ÁîüÈáçÂ§ßÂΩ±Èüø„ÄÇ

##### **VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge**
2408.02865v1 by Zihan Li, Diping Song, Zefeng Yang, Deming Wang, Fei Li, Xiulan Zhang, Paul E. Kinahan, Yu Qiao

The need for improved diagnostic methods in ophthalmology is acute,
especially in the less developed regions with limited access to specialists and
advanced equipment. Therefore, we introduce VisionUnite, a novel
vision-language foundation model for ophthalmology enhanced with clinical
knowledge. VisionUnite has been pretrained on an extensive dataset comprising
1.24 million image-text pairs, and further refined using our proposed MMFundus
dataset, which includes 296,379 high-quality fundus image-text pairs and
889,137 simulated doctor-patient dialogue instances. Our experiments indicate
that VisionUnite outperforms existing generative foundation models such as
GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable
to junior ophthalmologists. VisionUnite performs well in various clinical
scenarios including open-ended multi-disease diagnosis, clinical explanation,
and patient interaction, making it a highly versatile tool for initial
ophthalmic disease screening. VisionUnite can also serve as an educational aid
for junior ophthalmologists, accelerating their acquisition of knowledge
regarding both common and rare ophthalmic conditions. VisionUnite represents a
significant advancement in ophthalmology, with broad implications for
diagnostics, medical education, and understanding of disease mechanisms.

ÊëòË¶ÅÔºöÁúºÁßëË®∫Êñ∑ÊñπÊ≥ïÊîπËâØÁöÑÂøÖË¶ÅÊÄßÂçÅÂàÜËø´ÂàáÔºåÁâπÂà•ÊòØÂú®ËºÉ‰∏çÁôºÈÅîÂú∞ÂçÄÔºåÈÇ£Ë£°Â∞àÁßëÈÜ´Â∏´ÂíåÂÖàÈÄ≤Ë®≠ÂÇôÂèñÂæó‰∏çÊòì„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÈÄ≤ VisionUniteÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑË¶ñË¶∫Ë™ûË®ÄÂü∫Á§éÊ®°ÂûãÔºå‰∏¶‰ª•Ëá®Â∫äÁü•Ë≠òÂº∑ÂåñÁúºÁßë„ÄÇVisionUnite Â∑≤Âú®ÂåÖÂê´ 124 Ëê¨ÂºµÂΩ±ÂÉèÊñáÂ≠óÂ∞çÁöÑÂ§ßÂûãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥Ôºå‰∏¶ÈÄèÈÅéÊàëÂÄëÂª∫Ë≠∞ÁöÑ MMFundus Ë≥áÊñôÈõÜÈÄ≤‰∏ÄÊ≠•ÂÑ™ÂåñÔºåÂÖ∂‰∏≠ÂåÖÂê´ 296,379 ÂºµÈ´òÂìÅË≥™ÁúºÂ∫ïÂΩ±ÂÉèÊñáÂ≠óÂ∞çÂíå 889,137 ÂÄãÊ®°Êì¨ÁöÑÈÜ´Â∏´ÁóÖÊÇ£Â∞çË©±ÂØ¶‰æã„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÊåáÂá∫ VisionUnite ÂÑ™ÊñºÁèæÊúâÁöÑÁîüÊàêÂºèÂü∫Á§éÊ®°ÂûãÔºå‰æãÂ¶Ç GPT-4V Âíå Gemini Pro„ÄÇÂÆÉ‰πüÂ±ïÁèæÂá∫ËàáÂàùÈöéÁúºÁßëÈÜ´Â∏´Áõ∏Áï∂ÁöÑË®∫Êñ∑ËÉΩÂäõ„ÄÇVisionUnite Âú®ÂêÑÁ®ÆËá®Â∫äÊÉÖÂ¢É‰∏≠Ë°®ÁèæËâØÂ•ΩÔºåÂåÖÊã¨ÈñãÊîæÂºèÂ§öÁñæÁóÖË®∫Êñ∑„ÄÅËá®Â∫äË™™ÊòéÂíåÁóÖÊÇ£‰∫íÂãïÔºå‰ΩøÂÖ∂ÊàêÁÇ∫ÂàùÊ≠•ÁúºÁßëÁñæÁóÖÁØ©Ê™¢ÁöÑÈ´òÂ∫¶Â§öÂäüËÉΩÂ∑•ÂÖ∑„ÄÇVisionUnite ‰πüÂèØÁî®‰ΩúÂàùÈöéÁúºÁßëÈÜ´Â∏´ÁöÑÊïôËÇ≤ËºîÂä©Â∑•ÂÖ∑ÔºåÂä†ÈÄü‰ªñÂÄëÂ∞çÊñºÂ∏∏Ë¶ãÂíåÁΩïË¶ãÁúºÁßëÁñæÁóÖÁü•Ë≠òÁöÑÁøíÂæó„ÄÇVisionUnite ‰ª£Ë°®‰∫ÜÁúºÁßëÁöÑÈáçÂ§ßÈÄ≤Â±ïÔºåÂ∞çË®∫Êñ∑„ÄÅÈÜ´Â≠∏ÊïôËÇ≤ÂíåÁñæÁóÖÊ©üËΩâÁöÑÁêÜËß£ÂÖ∑ÊúâÂª£Ê≥õÁöÑÂΩ±Èüø„ÄÇ

##### **A Framework for Fine-Tuning LLMs using Heterogeneous Feedback**
2408.02861v1 by Ryan Aponte, Ryan A. Rossi, Shunan Guo, Franck Dernoncourt, Tong Yu, Xiang Chen, Subrata Mitra, Nedim Lipka

Large language models (LLMs) have been applied to a wide range of tasks,
including text summarization, web navigation, and chatbots. They have
benefitted from supervised fine-tuning (SFT) and reinforcement learning from
human feedback (RLHF) following an unsupervised pretraining. These datasets can
be difficult to collect, limited in scope, and vary in sample quality.
Additionally, datasets can vary extensively in supervision format, from
numerical to binary as well as multi-dimensional with many different values. We
present a framework for fine-tuning LLMs using heterogeneous feedback, which
has two main components. First, we combine the heterogeneous feedback data into
a single supervision format, compatible with methods like SFT and RLHF. Next,
given this unified feedback dataset, we extract a high-quality and diverse
subset to obtain performance increases potentially exceeding the full dataset.
We conduct extensive experiments to understand the effectiveness of these
techniques for incorporating heterogeneous feedback, and demonstrate
improvements from using a high-quality and diverse subset of the data. We find
that our framework is able to improve models in multiple areas simultaneously,
such as in instruction following and bias reduction.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊáâÁî®ÊñºÂêÑÁ®Æ‰ªªÂãôÔºå
ÂåÖÊã¨ÊñáÂ≠óÊëòË¶Å„ÄÅÁ∂≤È†ÅÁÄèË¶ΩÂíåËÅäÂ§©Ê©üÂô®‰∫∫„ÄÇÂÆÉÂÄë
ÂèóÁõäÊñºÁÑ°Áõ£Áù£È†êË®ìÁ∑¥ÂæåÁöÑÁõ£Áù£ÂæÆË™ø (SFT) Âíå‰∫∫È°ûÂõûÈ•ãÁöÑÂº∑ÂåñÂ≠∏Áøí (RLHF)„ÄÇÈÄô‰∫õË≥áÊñôÈõÜÂèØËÉΩ
Èõ£‰ª•Êî∂ÈõÜ„ÄÅÁØÑÂúçÂèóÈôê‰∏îÊ®£Êú¨ÂìÅË≥™‰∏ç‰∏Ä„ÄÇ
Ê≠§Â§ñÔºåË≥áÊñôÈõÜÂú®Áõ£Áù£Ê†ºÂºè‰∏äÂèØËÉΩÂ∑ÆÁï∞ÂæàÂ§ßÔºåÂæû
Êï∏Â≠óÂà∞‰∫åÂÖÉÔºå‰ª•ÂèäÂÖ∑ÊúâË®±Â§ö‰∏çÂêåÂÄºÁöÑÂ§öÂàÜÈáè„ÄÇÊàëÂÄë
ÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰ΩøÁî®Áï∞Ë≥™ÂõûÈ•ãÂæÆË™ø LLM ÁöÑÊ°ÜÊû∂ÔºåÂÆÉ
ÊúâÂÖ©ÂÄã‰∏ªË¶ÅÁµÑÊàêÈÉ®ÂàÜ„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂ∞áÁï∞Ë≥™ÂõûÈ•ãË≥áÊñôÂêà‰ΩµÁÇ∫
ÂñÆ‰∏ÄÁõ£Áù£Ê†ºÂºèÔºåËàá SFT Âíå RLHF Á≠âÊñπÊ≥ïÁõ∏ÂÆπ„ÄÇÊé•‰∏ã‰æÜÔºå
Áµ¶ÂÆöÈÄôÂÄãÁµ±‰∏ÄÁöÑÂõûÈ•ãË≥áÊñôÈõÜÔºåÊàëÂÄëÊèêÂèñ‰∏ÄÂÄãÈ´òÂìÅË≥™‰∏îÂ§öÊ®£ÂåñÁöÑ
Â≠êÈõÜÔºå‰ª•Áç≤ÂæóÊΩõÂú®Ë∂ÖÈÅéÂÆåÊï¥Ë≥áÊñôÈõÜÁöÑÊïàËÉΩÊèêÂçá„ÄÇ
ÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•‰∫ÜËß£ÈÄô‰∫õ
ÊäÄË°ìÂú®Êï¥ÂêàÁï∞Ë≥™ÂõûÈ•ãÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºå‰∏¶Â±ïÁ§∫
‰ΩøÁî®È´òÂìÅË≥™‰∏îÂ§öÊ®£ÂåñÁöÑË≥áÊñôÂ≠êÈõÜÊâÄÂ∏∂‰æÜÁöÑÊîπÈÄ≤„ÄÇÊàëÂÄëÁôºÁèæ
ÊàëÂÄëÁöÑÊ°ÜÊû∂ËÉΩÂ§†ÂêåÊôÇÊîπÂñÑÊ®°ÂûãÁöÑÂêÑÂÄãÈ†òÂüüÔºå
‰æãÂ¶ÇÊåá‰ª§ÈÅµÂæ™ÂíåÂÅèÂ∑ÆÊ∏õÂ∞ë„ÄÇ

##### **Multistain Pretraining for Slide Representation Learning in Pathology**
2408.02859v1 by Guillaume Jaume, Anurag Vaidya, Andrew Zhang, Andrew H. Song, Richard J. Chen, Sharifa Sahai, Dandan Mo, Emilio Madrigal, Long Phi Le, Faisal Mahmood

Developing self-supervised learning (SSL) models that can learn universal and
transferable representations of H&E gigapixel whole-slide images (WSIs) is
becoming increasingly valuable in computational pathology. These models hold
the potential to advance critical tasks such as few-shot classification, slide
retrieval, and patient stratification. Existing approaches for slide
representation learning extend the principles of SSL from small images (e.g.,
224 x 224 patches) to entire slides, usually by aligning two different
augmentations (or views) of the slide. Yet the resulting representation remains
constrained by the limited clinical and biological diversity of the views.
Instead, we postulate that slides stained with multiple markers, such as
immunohistochemistry, can be used as different views to form a rich
task-agnostic training signal. To this end, we introduce Madeleine, a
multimodal pretraining strategy for slide representation learning. Madeleine is
trained with a dual global-local cross-stain alignment objective on large
cohorts of breast cancer samples (N=4,211 WSIs across five stains) and kidney
transplant samples (N=12,070 WSIs across four stains). We demonstrate the
quality of slide representations learned by Madeleine on various downstream
evaluations, ranging from morphological and molecular classification to
prognostic prediction, comprising 21 tasks using 7,299 WSIs from multiple
medical centers. Code is available at https://github.com/mahmoodlab/MADELEINE.

ÊëòË¶ÅÔºöÈñãÁôºËá™Áõ£Áù£Â≠∏Áøí (SSL) Ê®°ÂûãÔºåÂèØ‰ª•Â≠∏Áøí H&E ÂêâÂÉèÁ¥†ÂÖ®ÂàáÁâáÂΩ±ÂÉè (WSI) ÁöÑÈÄöÁî®‰∏îÂèØËΩâÁßªË°®Á§∫ÔºåÂú®Ë®àÁÆóÁóÖÁêÜÂ≠∏‰∏≠Ê≠£ËÆäÂæóË∂ä‰æÜË∂äÊúâÂÉπÂÄº„ÄÇÈÄô‰∫õÊ®°ÂûãÊúâÊΩõÂäõÊé®ÈÄ≤ÈóúÈçµ‰ªªÂãôÔºå‰æãÂ¶ÇÂ∞ëÊ¨°ÂàÜÈ°û„ÄÅÂàáÁâáÊ™¢Á¥¢ÂíåÊÇ£ËÄÖÂàÜÂ±§„ÄÇÁèæÊúâÁöÑÂàáÁâáË°®Á§∫Â≠∏ÁøíÊñπÊ≥ïÂ∞á SSL ÁöÑÂéüÁêÜÂæûÂ∞èÂΩ±ÂÉèÔºà‰æãÂ¶Ç 224 x 224 Ë£ú‰∏ÅÔºâÂª∂‰º∏Âà∞Êï¥ÂÄãÂàáÁâáÔºåÈÄöÂ∏∏ÈÄèÈÅéÂ∞çÈΩäÂàáÁâáÁöÑÂÖ©ÂÄã‰∏çÂêåÊì¥Â¢ûÔºàÊàñË¶ñÂúñÔºâ„ÄÇÁÑ∂ËÄåÔºåÁîüÊàêÁöÑË°®Á§∫‰ªçÂèóÂà∞Ë¶ñÂúñÊúâÈôêÁöÑËá®Â∫äÂíåÁîüÁâ©Â§öÊ®£ÊÄßÁöÑÈôêÂà∂„ÄÇÁõ∏ÂèçÔºåÊàëÂÄëÂÅáË®≠‰ΩøÁî®Â§öÁ®ÆÊ®ôË®òÊüìËâ≤ÁöÑÂàáÁâáÔºå‰æãÂ¶ÇÂÖçÁñ´ÁµÑÁπîÂåñÂ≠∏ÊüìËâ≤ÔºåÂèØ‰ª•Áî®‰Ωú‰∏çÂêåÁöÑË¶ñÂúñ‰æÜÂΩ¢ÊàêË±êÂØåÁöÑËàá‰ªªÂãôÁÑ°ÈóúÁöÑË®ìÁ∑¥Ë®äËôü„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄë‰ªãÁ¥π MadeleineÔºå‰∏ÄÁ®ÆÁî®ÊñºÂàáÁâáË°®Á§∫Â≠∏ÁøíÁöÑÂ§öÊ®°ÂºèÈ†êË®ìÁ∑¥Á≠ñÁï•„ÄÇMadeleine ‰ΩøÁî®ÈõôÈáçÂÖ®Â±Ä-Â±ÄÈÉ®Ë∑®ÊüìËâ≤Â∞çÈΩäÁõÆÊ®ôÂú®Â§ßÈáè‰π≥ÁôåÊ®£Êú¨ÔºàN=4,211 ÂÄãÊ©´Ë∑®‰∫îÁ®ÆÊüìËâ≤ÁöÑ WSIÔºâÂíåËÖéËáüÁßªÊ§çÊ®£Êú¨ÔºàN=12,070 ÂÄãÊ©´Ë∑®ÂõõÁ®ÆÊüìËâ≤ÁöÑ WSIÔºâ‰∏äÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÂú®ÂêÑÁ®Æ‰∏ãÊ∏∏Ë©ï‰º∞‰∏≠Â±ïÁ§∫‰∫Ü Madeleine Â≠∏ÁøíÁöÑÂàáÁâáË°®Á§∫ÁöÑÂìÅË≥™ÔºåÂæûÂΩ¢ÊÖãÂíåÂàÜÂ≠êÂàÜÈ°ûÂà∞È†êÂæåÈ†êÊ∏¨ÔºåÂåÖÊã¨‰ΩøÁî®‰æÜËá™Â§öÂÄãÈÜ´ÁôÇ‰∏≠ÂøÉÁöÑ 7,299 ÂÄã WSI ÁöÑ 21 È†Ö‰ªªÂãô„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/mahmoodlab/MADELEINE ÂèñÂæó„ÄÇ

##### **Development of REGAI: Rubric Enabled Generative Artificial Intelligence**
2408.02811v1 by Zach Johnson, Jeremy Straub

This paper presents and evaluates a new retrieval augmented generation (RAG)
and large language model (LLM)-based artificial intelligence (AI) technique:
rubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics,
which can be created manually or automatically by the system, to enhance the
performance of LLMs for evaluation purposes. REGAI improves on the performance
of both classical LLMs and RAG-based LLM techniques. This paper describes
REGAI, presents data regarding its performance and discusses several possible
application areas for the technology.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁ¥π‰∏¶Ë©ï‰º∞‰∏ÄÁ®ÆÊñ∞ÁöÑÊ™¢Á¥¢Êì¥Â¢ûÁîüÊàê (RAG) ÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁÇ∫Âü∫Á§éÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊäÄË°ìÔºöÊ®ôÊ∫ñÂïüÁî®ÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖß (REGAI)„ÄÇREGAI ‰ΩøÁî®Ê®ôÊ∫ñÔºåÈÄô‰∫õÊ®ôÊ∫ñÂèØ‰ª•Áî±Á≥ªÁµ±ÊâãÂãïÊàñËá™ÂãïÂª∫Á´ãÔºå‰ª•Â¢ûÂº∑ LLM ÁöÑÊïàËÉΩÔºå‰ª•Áî®ÊñºË©ïÈáèÁõÆÁöÑ„ÄÇREGAI ÊîπÂñÑ‰∫ÜÂÇ≥Áµ± LLM ÂíåÂü∫Êñº RAG ÁöÑ LLM ÊäÄË°ìÁöÑÊïàËÉΩ„ÄÇÊú¨ÊñáË™™Êòé REGAIÔºåÊèê‰æõÊúâÈóúÂÖ∂ÊïàËÉΩÁöÑË≥áÊñôÔºå‰∏¶Ë®éË´ñË©≤ÊäÄË°ìÁöÑÂπæÂÄãÂèØËÉΩÁöÑÊáâÁî®È†òÂüü„ÄÇ

##### **Examining Gender and Power on Wikipedia Through Face and Politeness**
2408.02798v1 by Adil Soubki, Shyne Choi, Owen Rambow

We propose a framework for analyzing discourse by combining two
interdependent concepts from sociolinguistic theory: face acts and politeness.
While politeness has robust existing tools and data, face acts are less
resourced. We introduce a new corpus created by annotating Wikipedia talk pages
with face acts and we use this to train a face act tagger. We then employ our
framework to study how face and politeness interact with gender and power in
discussions between Wikipedia editors. Among other findings, we observe that
female Wikipedians are not only more polite, which is consistent with prior
studies, but that this difference corresponds with significantly more language
directed at humbling aspects of their own face. Interestingly, the distinction
nearly vanishes once limiting to editors with administrative power.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂàÜÊûêË©±Ë™ûÁöÑÊû∂ÊßãÔºåÁµêÂêàÁ§æÊúÉË™ûË®ÄÂ≠∏ÁêÜË´ñ‰∏≠ÁöÑÂÖ©ÂÄãÁõ∏‰∫í‰æùË≥¥ÁöÑÊ¶ÇÂøµÔºöÈù¢Â≠êË°åÁÇ∫ÂíåÁ¶ÆË≤å„ÄÇ
ÈõñÁÑ∂Á¶ÆË≤åÊúâÂº∑Â§ßÁöÑÁèæÊúâÂ∑•ÂÖ∑ÂíåË≥áÊñôÔºå‰ΩÜÈù¢Â≠êË°åÁÇ∫ÁöÑË≥áÊ∫êËºÉÂ∞ë„ÄÇÊàëÂÄëÂºïÂÖ•‰∏ÄÂÄãÊñ∞ÁöÑË™ûÊñôÂ∫´ÔºåÂÆÉÊòØÈÄöÈÅéÁî®Èù¢Â≠êË°åÁÇ∫Ë®ªÈáãÁ∂≠Âü∫ÁôæÁßëË®éË´ñÈ†ÅÈù¢ÂâµÂª∫ÁöÑÔºåÊàëÂÄë‰ΩøÁî®ÂÆÉ‰æÜË®ìÁ∑¥‰∏ÄÂÄãÈù¢Â≠êË°åÁÇ∫Ê®ôË®òÂô®„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄë‰ΩøÁî®ÊàëÂÄëÁöÑÊ°ÜÊû∂‰æÜÁ†îÁ©∂Èù¢Â≠êÂíåÁ¶ÆË≤åÂ¶Ç‰ΩïÂú®Á∂≠Âü∫ÁôæÁßëÁ∑®ËºØ‰πãÈñìÁöÑË®éË´ñ‰∏≠ËàáÊÄßÂà•ÂíåÊ¨äÂäõ‰∫íÂãï„ÄÇÂú®ÂÖ∂‰ªñÁôºÁèæ‰∏≠ÔºåÊàëÂÄëËßÄÂØüÂà∞Â•≥ÊÄßÁ∂≠Âü∫ÁôæÁßëÁ∑®ËºØËÄÖ‰∏çÂÉÖÊõ¥Á¶ÆË≤åÔºåÈÄôËàáÂÖàÂâçÁöÑÁ†îÁ©∂‰∏ÄËá¥ÔºåËÄå‰∏îÈÄôÁ®ÆÂ∑ÆÁï∞ËàáÊõ¥Â§öÈáùÂ∞çËá™Â∑±Èù¢Â≠êË¨ôÂçëÊñπÈù¢ÁöÑË™ûË®ÄÁõ∏‰∏ÄËá¥„ÄÇÊúâË∂£ÁöÑÊòØÔºå‰∏ÄÊó¶ÈôêÂà∂Âú®ÂÖ∑ÊúâÁÆ°ÁêÜÊ¨äÂäõÁöÑÁ∑®ËºØËÄÖÔºåÈÄôÁ®ÆÂçÄÂà•Âπæ‰πéÊ∂àÂ§±„ÄÇ

##### **LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory**
2408.02784v1 by Jillian Ross, Yoon Kim, Andrew W. Lo

Humans are not homo economicus (i.e., rational economic beings). As humans,
we exhibit systematic behavioral biases such as loss aversion, anchoring,
framing, etc., which lead us to make suboptimal economic decisions. Insofar as
such biases may be embedded in text data on which large language models (LLMs)
are trained, to what extent are LLMs prone to the same behavioral biases?
Understanding these biases in LLMs is crucial for deploying LLMs to support
human decision-making. We propose utility theory-a paradigm at the core of
modern economic theory-as an approach to evaluate the economic biases of LLMs.
Utility theory enables the quantification and comparison of economic behavior
against benchmarks such as perfect rationality or human behavior. To
demonstrate our approach, we quantify and compare the economic behavior of a
variety of open- and closed-source LLMs. We find that the economic behavior of
current LLMs is neither entirely human-like nor entirely economicus-like. We
also find that most current LLMs struggle to maintain consistent economic
behavior across settings. Finally, we illustrate how our approach can measure
the effect of interventions such as prompting on economic biases.

ÊëòË¶ÅÔºö‰∫∫È°û‰∏¶ÈùûÁ∂ìÊøü‰∫∫ÔºàÂç≥ÁêÜÊÄßÁöÑÁ∂ìÊøüÂÄãÈ´îÔºâ„ÄÇ‰ΩúÁÇ∫‰∫∫È°ûÔºå
ÊàëÂÄëË°®ÁèæÂá∫Á≥ªÁµ±ÊÄßÁöÑË°åÁÇ∫ÂÅèÂ∑ÆÔºå‰æãÂ¶ÇÂé≠ÊÉ°ÊêçÂ§±„ÄÅÈå®ÂÆö„ÄÅ
Ê°ÜÊû∂Á≠âÔºåÈÄô‰∫õÂÅèÂ∑ÆÂ∞éËá¥ÊàëÂÄëÂÅöÂá∫Ê¨°ÂÑ™ÁöÑÁ∂ìÊøüÊ±∫Á≠ñ„ÄÇÂú®
Ê≠§È°ûÂÅèÂ∑ÆÂèØËÉΩÂµåÂÖ•Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êé•ÂèóË®ìÁ∑¥ÁöÑÊñáÊú¨Êï∏Êìö‰∏≠ÔºåLLM Âú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÂÆπÊòìÂá∫ÁèæÁõ∏ÂêåÁöÑË°åÁÇ∫ÂÅèÂ∑ÆÔºü
‰∫ÜËß£ LLM ‰∏≠ÁöÑÈÄô‰∫õÂÅèÂ∑ÆÂ∞çÊñºÈÉ®ÁΩ≤ LLM ‰ª•ÊîØÊåÅ
‰∫∫È°ûÊ±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÊèêÂá∫ÊïàÁî®ÁêÜË´ñ‚Äî‚ÄîÁèæ‰ª£Á∂ìÊøüÁêÜË´ñÁöÑÊ†∏ÂøÉÁØÑÂºè‚Äî‚Äî‰ΩúÁÇ∫‰∏ÄÁ®ÆË©ï‰º∞ LLM Á∂ìÊøüÂÅèÂ∑ÆÁöÑÊñπÊ≥ï„ÄÇ
ÊïàÁî®ÁêÜË´ñËÉΩÂ§†ÈáèÂåñÂíåÊØîËºÉÁ∂ìÊøüË°åÁÇ∫
ËàáÂÆåÁæéÁêÜÊÄßÊàñ‰∫∫È°ûË°åÁÇ∫Á≠âÂü∫Ê∫ñ„ÄÇÁÇ∫‰∫Ü
Â±ïÁ§∫ÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÊàëÂÄëÈáèÂåñÂíåÊØîËºÉ‰∫Ü
ÂêÑÁ®ÆÈñãÊ∫êÂíåÈñâÊ∫ê LLM ÁöÑÁ∂ìÊøüË°åÁÇ∫„ÄÇÊàëÂÄëÁôºÁèæÔºåÁï∂Ââç LLM ÁöÑÁ∂ìÊøüË°åÁÇ∫Êó¢‰∏çÂÉè‰∫∫È°û‰πü‰∏çÂÆåÂÖ®ÂÉèÁ∂ìÊøü‰∫∫„ÄÇÊàëÂÄë
ÈÇÑÁôºÁèæÔºåÂ§ßÂ§öÊï∏Áï∂Ââç LLM Èõ£‰ª•Âú®‰∏çÂêåË®≠ÁΩÆ‰∏≠‰øùÊåÅ‰∏ÄËá¥ÁöÑÁ∂ìÊøüË°åÁÇ∫„ÄÇÊúÄÂæåÔºåÊàëÂÄëË™™Êòé‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÂ¶Ç‰ΩïË°°Èáè
ÊèêÁ§∫Á≠âÂπ≤È†êÊé™ÊñΩÂ∞çÁ∂ìÊøüÂÅèÂ∑ÆÁöÑÂΩ±Èüø„ÄÇ

##### **Self-Taught Evaluators**
2408.02666v1 by Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li

Model-based evaluation is at the heart of successful model development -- as
a reward model for training, and as a replacement for human evaluation. To
train such evaluators, the standard approach is to collect a large amount of
human preference judgments over model responses, which is costly and the data
becomes stale as models improve. In this work, we present an approach that aims
to im-prove evaluators without human annotations, using synthetic training data
only. Starting from unlabeled instructions, our iterative self-improvement
scheme generates contrasting model outputs and trains an LLM-as-a-Judge to
produce reasoning traces and final judgments, repeating this training at each
new iteration using the improved predictions. Without any labeled preference
data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)
from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms
commonly used LLM judges such as GPT-4 and matches the performance of the
top-performing reward models trained with labeled examples.

ÊëòË¶ÅÔºöÂü∫ÊñºÊ®°ÂûãÁöÑË©ï‰º∞ÊòØÊàêÂäüÊ®°ÂûãÈñãÁôºÁöÑÊ†∏ÂøÉÔºå‰ΩúÁÇ∫Ë®ìÁ∑¥ÁöÑÁçéÂãµÊ®°ÂûãÔºå‰ª•Âèä‰ΩúÁÇ∫‰∫∫Â∑•Ë©ï‰º∞ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÁÇ∫‰∫ÜË®ìÁ∑¥Ê≠§È°ûË©ï‰º∞Âô®ÔºåÊ®ôÊ∫ñÊñπÊ≥ïÊòØÊî∂ÈõÜÂ§ßÈáè‰∫∫È°ûÂÅèÂ•ΩÂà§Êñ∑Ôºå‰ª•Â∞çÊ®°ÂûãÂèçÊáâÈÄ≤Ë°åË©ï‰º∞ÔºåÈÄôÊó¢ÊòÇË≤¥ÔºåËÄå‰∏îÈö®ËëóÊ®°ÂûãÁöÑÊîπÈÄ≤ÔºåÊï∏Êìö‰πüÊúÉËÆäÂæóÈô≥Ëàä„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊó®Âú®Âú®Ê≤íÊúâ‰∫∫Â∑•Ë®ªËß£ÁöÑÊÉÖÊ≥Å‰∏ãÊîπÈÄ≤Ë©ï‰º∞Âô®ÁöÑÊñπÊ≥ïÔºåÂÉÖ‰ΩøÁî®ÂêàÊàêË®ìÁ∑¥Êï∏Êìö„ÄÇÂæûÊú™Ê®ôË®òÁöÑË™™ÊòéÈñãÂßãÔºåÊàëÂÄëÁöÑËø≠‰ª£ÂºèËá™ÊàëÊîπÈÄ≤ÊñπÊ°àÊúÉÁî¢ÁîüÂ∞çÊØîÁöÑÊ®°ÂûãËº∏Âá∫Ôºå‰∏¶Ë®ìÁ∑¥ LLM-as-a-Judge ‰ª•Áî¢ÁîüÊé®ÁêÜËøΩËπ§ÂíåÊúÄÁµÇÂà§Êñ∑ÔºåÂú®ÊØèÊ¨°Êñ∞ÁöÑËø≠‰ª£‰∏≠‰ΩøÁî®ÊîπÈÄ≤ÁöÑÈ†êÊ∏¨ÈáçË§áÊ≠§Ë®ìÁ∑¥„ÄÇÂú®Ê≤íÊúâ‰ªª‰ΩïÊ®ôË®òÁöÑÂÅèÂ•ΩÊï∏ÊìöÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊàëÂÄëÁöÑËá™Â≠∏Ë©ï‰º∞Âô®ÂèØ‰ª•Â∞áÂº∑Â§ßÁöÑ LLMÔºàLlama3-70B-InstructÔºâÂæû RewardBench ‰∏äÁöÑ 75.4 ÊèêÂçáÂà∞ 88.3ÔºàÂ§öÊï∏ÊäïÁ•®ÁÇ∫ 88.7Ôºâ„ÄÇÈÄôÂÑ™ÊñºÂ∏∏Áî®ÁöÑ LLM Ë©ïÂØ©Ôºå‰æãÂ¶Ç GPT-4Ôºå‰∏¶‰∏îËàá‰ΩøÁî®Ê®ôË®òÁØÑ‰æãË®ìÁ∑¥ÁöÑÊïàËÉΩÊúÄ‰Ω≥ÁöÑÁçéÂãµÊ®°ÂûãÁöÑÊïàËÉΩÁõ∏ÂåπÈÖç„ÄÇ

##### **Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?**
2408.02651v1 by Mohammad Bahrami Karkevandi, Nishant Vishwamitra, Peyman Najafirad

Large Language Models (LLMs) have demonstrated impressive capabilities in
natural language tasks, but their safety and morality remain contentious due to
their training on internet text corpora. To address these concerns, alignment
techniques have been developed to improve the public usability and safety of
LLMs. Yet, the potential for generating harmful content through these models
seems to persist. This paper explores the concept of jailbreaking
LLMs-reversing their alignment through adversarial triggers. Previous methods,
such as soft embedding prompts, manually crafted prompts, and gradient-based
automatic prompts, have had limited success on black-box models due to their
requirements for model access and for producing a low variety of manually
crafted prompts, making them susceptible to being blocked. This paper
introduces a novel approach using reinforcement learning to optimize
adversarial triggers, requiring only inference API access to the target model
and a small surrogate model. Our method, which leverages a BERTScore-based
reward function, enhances the transferability and effectiveness of adversarial
triggers on new black-box models. We demonstrate that this approach improves
the performance of adversarial triggers on a previously untested language
model.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá™ÁÑ∂Ë™ûË®Ä‰ªªÂãô‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºå‰ΩÜÁî±ÊñºÂÆÉÂÄëÂú®Á∂≤ÈöõÁ∂≤Ë∑ØÊñáÂ≠óË™ûÊñôÂ∫´‰∏äË®ìÁ∑¥ÔºåÂÆâÂÖ®ÊÄßËàáÈÅìÂæ∑ÊÄß‰ªçÊúâÁà≠Ë≠∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÂ∑≤ÈñãÁôºÂá∫Ê†°Ê∫ñÊäÄË°ì‰æÜÊîπÂñÑ LLM ÁöÑÂÖ¨ÈñãÂèØÁî®ÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÁÑ∂ËÄåÔºåÈÄèÈÅéÈÄô‰∫õÊ®°ÂûãÁî¢ÁîüÊúâÂÆ≥ÂÖßÂÆπÁöÑÂèØËÉΩÊÄß‰ºº‰πé‰ªçÁÑ∂Â≠òÂú®„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜË∂äÁçÑ LLM ÁöÑÊ¶ÇÂøµÔºåÈÄèÈÅéÂ∞çÊäóÊÄßËß∏ÁôºÂô®‰æÜÈÄÜËΩâÂÆÉÂÄëÁöÑÊ†°Ê∫ñ„ÄÇÂÖàÂâçÁöÑÊäÄË°ìÔºå‰æãÂ¶ÇËªüÂµåÂÖ•ÊèêÁ§∫„ÄÅÊâãÂãïË£Ω‰ΩúÊèêÁ§∫ÂíåÂü∫ÊñºÊ¢ØÂ∫¶ÁöÑËá™ÂãïÊèêÁ§∫ÔºåÁî±ÊñºÈúÄË¶ÅÂ≠òÂèñÊ®°ÂûãÂíåÁî¢ÁîüÂ∞ëÈáèÊâãÂãïË£Ω‰ΩúÊèêÁ§∫ÔºåÂ∞éËá¥Âú®ÈªëÁÆ±Ê®°Âûã‰∏äÊàêÂäüÊúâÈôêÔºå‰ΩøÂÖ∂ÂÆπÊòìÂèóÂà∞Â∞ÅÈéñ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Âº∑ÂåñÂ≠∏Áøí‰æÜÊúÄ‰Ω≥ÂåñÂ∞çÊäóÊÄßËß∏ÁôºÂô®ÁöÑÊñ∞ÊñπÊ≥ïÔºåÂè™ÈúÄË¶ÅÊé®Ë´ñ API Â≠òÂèñÁõÆÊ®ôÊ®°ÂûãÂíå‰∏ÄÂÄãÂ∞èÂûã‰ª£ÁêÜÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂà©Áî®Âü∫Êñº BERTScore ÁöÑÁçéÂãµÂáΩÊï∏ÔºåÂ¢ûÂº∑‰∫ÜÂ∞çÊäóÊÄßËß∏ÁôºÂô®Âú®Êñ∞ÈªëÁÆ±Ê®°Âûã‰∏äÁöÑÂèØÂÇ≥ÈÅûÊÄßÂíåÊúâÊïàÊÄß„ÄÇÊàëÂÄëË≠âÊòé‰∫ÜÈÄôÁ®ÆÊñπÊ≥ïÊîπÂñÑ‰∫ÜÂ∞çÊäóÊÄßËß∏ÁôºÂô®Âú®ÂÖàÂâçÊú™Ê∏¨Ë©¶ÈÅéÁöÑË™ûË®ÄÊ®°Âûã‰∏äÁöÑÊïàËÉΩ„ÄÇ

##### **SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models**
2408.02632v1 by Muxi Diao, Rumei Li, Shiyang Liu, Guogang Liao, Jingang Wang, Xunliang Cai, Weiran Xu

As large language models (LLMs) continue to advance in capability and
influence, ensuring their security and preventing harmful outputs has become
crucial. A promising approach to address these concerns involves training
models to automatically generate adversarial prompts for red teaming. However,
the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness
of current adversarial methods, which struggle to specifically target and
explore the weaknesses of these models. To tackle these challenges, we
introduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving
}\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$
optimization framework, which enhances security by leveraging data generated by
the model itself. SEAS operates through three iterative stages: Initialization,
Attack, and Adversarial Optimization, refining both the Red Team and Target
models to improve robustness and safety. This framework reduces reliance on
manual testing and significantly enhances the security capabilities of LLMs.
Our contributions include a novel adversarial framework, a comprehensive safety
dataset, and after three iterations, the Target model achieves a security level
comparable to GPT-4, while the Red Team model shows a marked increase in attack
success rate (ASR) against advanced models.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ËÉΩÂäõÂíåÂΩ±ÈüøÂäõÊñπÈù¢ÊåÅÁ∫åÈÄ≤Ê≠•ÔºåÁ¢∫‰øùÂÖ∂ÂÆâÂÖ®ÊÄß‰∏¶Èò≤Ê≠¢ÊúâÂÆ≥Ëº∏Âá∫Â∑≤ËÆäÂæóËá≥ÈóúÈáçË¶Å„ÄÇËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÁöÑ‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÊ∂âÂèäË®ìÁ∑¥Ê®°Âûã‰ª•Ëá™ÂãïÁîüÊàêÂ∞çÊäóÊÄßÊèêÁ§∫‰ª•ÈÄ≤Ë°åÁ¥ÖÈöäÊºîÁ∑¥„ÄÇÁÑ∂ËÄåÔºåLLM ‰∏≠ÊºèÊ¥ûÁöÑÂæÆÂ¶ôÊÄß‰∏çÊñ∑ÊºîËÆäÔºåÊåëÊà∞‰∫ÜÁï∂ÂâçÂ∞çÊäóÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÈÄô‰∫õÊñπÊ≥ïÈõ£‰ª•ÈáùÂ∞çÈÄô‰∫õÊ®°ÂûãÁöÑÂº±Èªû‰∏¶Âä†‰ª•Êé¢Á¥¢„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü $\mathbf{S}\text{elf-}\mathbf{E}\text{volving
}\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$ ÊúÄ‰Ω≥ÂåñÊû∂ÊßãÔºåÂÆÉÈÄöÈÅéÂà©Áî®Ê®°ÂûãÊú¨Ë∫´Áî¢ÁîüÁöÑÊï∏Êìö‰æÜÂ¢ûÂº∑ÂÆâÂÖ®ÊÄß„ÄÇSEAS ÈÄöÈÅé‰∏âÂÄãÂèçË¶ÜÈÅãÁÆóÈöéÊÆµÈÅã‰ΩúÔºöÂàùÂßãÂåñ„ÄÅÊîªÊìäÂíåÂ∞çÊäóÊÄßÊúÄ‰Ω≥ÂåñÔºåÂÑ™ÂåñÁ¥ÖÈöäÂíåÁõÆÊ®ôÊ®°Âûã‰ª•ÊèêÈ´òÁ©©ÂÅ•ÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÊ≠§Êû∂ÊßãÊ∏õÂ∞ë‰∫ÜÂ∞çÊâãÂãïÊ∏¨Ë©¶ÁöÑ‰æùË≥¥Ôºå‰∏¶È°ØËëóÂ¢ûÂº∑‰∫Ü LLM ÁöÑÂÆâÂÖ®ÂäüËÉΩ„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÂåÖÊã¨‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ∞çÊäóÊÄßÊû∂Êßã„ÄÅ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂÆâÂÖ®ÊÄßË≥áÊñôÈõÜÔºå‰∏¶‰∏îÁ∂ìÈÅé‰∏âÊ¨°ÂèçË¶ÜÈÅãÁÆóÂæåÔºåÁõÆÊ®ôÊ®°ÂûãÈÅîÂà∞‰∫ÜËàá GPT-4 Áõ∏Áï∂ÁöÑÂÆâÂÖ®ÊÄßÁ≠âÁ¥öÔºåËÄåÁ¥ÖÈöäÊ®°ÂûãÈ°ØÁ§∫Âá∫Â∞çÊäóÈ´òÁ¥öÊ®°ÂûãÁöÑÊîªÊìäÊàêÂäüÁéá (ASR) ÊòéÈ°ØÂ¢ûÂä†„ÄÇ

##### **Language Model Can Listen While Speaking**
2408.02622v1 by Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen

Dialogue serves as the most natural manner of human-computer interaction
(HCI). Recent advancements in speech language models (SLM) have significantly
enhanced speech-based conversational AI. However, these models are limited to
turn-based conversation, lacking the ability to interact with humans in
real-time spoken scenarios, for example, being interrupted when the generated
content is not satisfactory. To address these limitations, we explore full
duplex modeling (FDM) in interactive speech language models (iSLM), focusing on
enhancing real-time interaction and, more explicitly, exploring the
quintessential ability of interruption. We introduce a novel model design,
namely listening-while-speaking language model (LSLM), an end-to-end system
equipped with both listening and speaking channels. Our LSLM employs a
token-based decoder-only TTS for speech generation and a streaming
self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses
both channels for autoregressive generation and detects turn-taking in real
time. Three fusion strategies -- early fusion, middle fusion, and late fusion
-- are explored, with middle fusion achieving an optimal balance between speech
generation and real-time interaction. Two experimental settings, command-based
FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity
to diverse instructions. Our results highlight LSLM's capability to achieve
duplex communication with minimal impact on existing systems. This study aims
to advance the development of interactive speech dialogue systems, enhancing
their applicability in real-world contexts.

ÊëòË¶ÅÔºöÂ∞çË©±ÊòØ‰∫∫Ê©ü‰∫íÂãï (HCI) ÊúÄËá™ÁÑ∂ÁöÑÊñπÂºè„ÄÇË™ûÈü≥Ë™ûË®ÄÊ®°Âûã (SLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ∑≤Â§ßÂπÖÊèêÂçáÂü∫ÊñºË™ûÈü≥ÁöÑÂ∞çË©±Âºè AI„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®°ÂûãÂÉÖÈôêÊñºÂõûÂêàÂà∂Â∞çË©±ÔºåÁÑ°Ê≥ïËàá‰∫∫È°ûÈÄ≤Ë°åÂç≥ÊôÇÂè£Ë™û‰∫íÂãïÔºå‰æãÂ¶ÇÂú®Áî¢ÁîüÁöÑÂÖßÂÆπ‰∏ç‰ª§‰∫∫ÊªøÊÑèÊôÇË¢´ÊâìÊñ∑„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂú®‰∫íÂãïÂºèË™ûÈü≥Ë™ûË®ÄÊ®°Âûã (iSLM) ‰∏≠Êé¢Á¥¢ÂÖ®ÈõôÂ∑•Âª∫Ê®° (FDM)ÔºåÈáçÈªûÂú®ÊñºÂ¢ûÂº∑Âç≥ÊôÇ‰∫íÂãïÔºåÊõ¥ÊòéÁ¢∫Âú∞Ë™™ÔºåÊé¢Á¥¢‰∏≠Êñ∑ÁöÑÁ≤æÈ´ìËÉΩÂäõ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ®°ÂûãË®≠Ë®àÔºåÂç≥ËÅÜËÅΩÊôÇË™™Ë©±Ë™ûË®ÄÊ®°Âûã (LSLM)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ∑ÂÇôËÅÜËÅΩÂíåË™™Ë©±ÈÄöÈÅìÁöÑÁ´ØÂ∞çÁ´ØÁ≥ªÁµ±„ÄÇÊàëÂÄëÁöÑ LSLM ‰ΩøÁî®Âü∫Êñº‰ª£Á¢ºÁöÑÂÉÖËß£Á¢ºÂô® TTS ÈÄ≤Ë°åË™ûÈü≥Áî¢ÁîüÔºå‰∏¶‰ΩøÁî®‰∏≤ÊµÅËá™Áõ£Áù£Â≠∏Áøí (SSL) Á∑®Á¢ºÂô®ÈÄ≤Ë°åÂç≥ÊôÇÈü≥Ë®äËº∏ÂÖ•„ÄÇLSLM ËûçÂêà‰∫ÜÂÖ©ÂÄãÈÄöÈÅìÈÄ≤Ë°åËá™Ëø¥Ê≠∏Áî¢ÁîüÔºå‰∏¶Âç≥ÊôÇÂÅµÊ∏¨Ëº™ÊµÅÁôºË®Ä„ÄÇÊé¢Á¥¢‰∫Ü‰∏âÂÄãËûçÂêàÁ≠ñÁï•‚Äî‚ÄîÊó©ÊúüËûçÂêà„ÄÅ‰∏≠ÊúüËûçÂêàÂíåÂæåÊúüËûçÂêà‚Äî‚ÄîÂÖ∂‰∏≠‰∏≠ÊúüËûçÂêàÂú®Ë™ûÈü≥Áî¢ÁîüÂíåÂç≥ÊôÇ‰∫íÂãï‰πãÈñìÂèñÂæó‰∫ÜÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇÂÖ©Á®ÆÂØ¶È©óË®≠ÂÆöÔºåÂü∫ÊñºÊåá‰ª§ÁöÑ FDM ÂíåÂü∫ÊñºË™ûÈü≥ÁöÑ FDMÔºåÂ±ïÁ§∫‰∫Ü LSLM Â∞çÈõúË®äÁöÑÁ©©ÂÅ•ÊÄßÂíåÂ∞ç‰∏çÂêåÊåá‰ª§ÁöÑÊïèÊÑüÊÄß„ÄÇÊàëÂÄëÁöÑÁµêÊûúÁ™ÅÈ°Ø‰∫Ü LSLM Âú®Â∞çÁèæÊúâÁ≥ªÁµ±ÂΩ±ÈüøÊúÄÂ∞èÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶ÁèæÈõôÂ∑•ÈÄöË®äÁöÑËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Êé®Âãï‰∫íÂãïÂºèË™ûÈü≥Â∞çË©±Á≥ªÁµ±ÁöÑÁôºÂ±ïÔºåÂ¢ûÂº∑ÂÖ∂Âú®ÁèæÂØ¶‰∏ñÁïå‰∏≠ÁöÑÊáâÁî®ÊÄß„ÄÇ

##### **BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba**
2408.02600v1 by Ling Yue, Sixue Xing, Yingzhou Lu, Tianfan Fu

The advancement of natural language processing (NLP) in biology hinges on
models' ability to interpret intricate biomedical literature. Traditional
models often struggle with the complex and domain-specific language in this
field. In this paper, we present BioMamba, a pre-trained model specifically
designed for biomedical text mining. BioMamba builds upon the Mamba
architecture and is pre-trained on an extensive corpus of biomedical
literature. Our empirical studies demonstrate that BioMamba significantly
outperforms models like BioBERT and general-domain Mamba across various
biomedical tasks. For instance, BioMamba achieves a 100 times reduction in
perplexity and a 4 times reduction in cross-entropy loss on the BioASQ test
set. We provide an overview of the model architecture, pre-training process,
and fine-tuning techniques. Additionally, we release the code and trained model
to facilitate further research.

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) Âú®ÁîüÁâ©Â≠∏‰∏≠ÁöÑÈÄ≤Â±ïÂèñÊ±∫Êñº
Ê®°ÂûãË©ÆÈáãË§áÈõúÁîüÁâ©ÈÜ´Â≠∏ÊñáÁçªÁöÑËÉΩÂäõ„ÄÇÂÇ≥Áµ±
Ê®°ÂûãÁ∂ìÂ∏∏Èõ£‰ª•Êáâ‰ªòÈÄôÂÄãÈ†òÂüü‰∏≠Ë§áÈõú‰∏îÈ†òÂüüÁâπÂÆöÁöÑË™ûË®Ä„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ BioMambaÔºå‰∏ÄÁ®ÆÂ∞àÈñÄ
Ë®≠Ë®àÁî®ÊñºÁîüÁâ©ÈÜ´Â≠∏ÊñáÊú¨Êé¢ÂãòÁöÑÈ†êË®ìÁ∑¥Ê®°Âûã„ÄÇBioMamba Âª∫Á´ãÂú® Mamba
Êû∂Êßã‰πã‰∏äÔºå‰∏¶Âú®Â§ßÈáèÁöÑÁîüÁâ©ÈÜ´Â≠∏È†êË®ìÁ∑¥
ÊñáÁçª‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÂØ¶Ë≠âÁ†îÁ©∂Ë≠âÊòéÔºåBioMamba Âú®ÂêÑÁ®Æ
ÁîüÁâ©ÈÜ´Â≠∏‰ªªÂãô‰∏äÈÉΩÊòéÈ°ØÂÑ™Êñº BioBERT Âíå‰∏ÄËà¨È†òÂüüÁöÑ Mamba„ÄÇ‰æãÂ¶ÇÔºåBioMamba Âú® BioASQ Ê∏¨Ë©¶
ÈõÜ‰∏≠Â∞áÂõ∞ÊÉëÂ∫¶Èôç‰Ωé‰∫Ü 100 ÂÄçÔºå‰∏¶Â∞á‰∫§ÂèâÁÜµÊêçÂ§±Èôç‰Ωé‰∫Ü 4 ÂÄç„ÄÇÊàëÂÄëÊèê‰æõ‰∫ÜÊ®°ÂûãÊû∂Êßã„ÄÅÈ†êË®ìÁ∑¥ÊµÅÁ®ãÁöÑÊ¶ÇËø∞Ôºå
‰ª•ÂèäÂæÆË™øÊäÄË°ì„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈáãÂá∫Á®ãÂºèÁ¢ºÂíåË®ìÁ∑¥Â•ΩÁöÑÊ®°Âûã
‰ª•‰øÉÈÄ≤ÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂„ÄÇ

##### **Progressively Selective Label Enhancement for Language Model Alignment**
2408.02599v1 by Biao Liu, Ning Xu, Xin Geng

Large Language Models have demonstrated impressive capabilities in various
language tasks but may produce content that misaligns with human expectations,
raising ethical and legal concerns. Therefore, it is important to explore the
limitations and implement restrictions on the models to ensure safety and
compliance, with Reinforcement Learning from Human Feedback (RLHF) being the
primary method. Due to challenges in stability and scalability with the RLHF
stages, researchers are exploring alternative methods to achieve effects
comparable to those of RLHF. However, these methods often depend on large
high-quality datasets and inefficiently utilize generated data. To deal with
this problem, we propose PSLE, i.e., Progressively Selective Label Enhancement
for Language Model Alignment, a framework that fully utilizes all generated
data by guiding the model with principles to align outputs with human
expectations. Using a dynamically updated threshold, our approach ensures
efficient data utilization by incorporating all generated responses and
weighting them based on their corresponding reward scores. Experimental results
on multiple datasets demonstrate the effectiveness of PSLE compared to existing
language model alignment methods.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂ∑≤Âú®ÂêÑÁ®ÆË™ûË®Ä‰ªªÂãô‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºå‰ΩÜÂèØËÉΩÊúÉÁî¢ÁîüËàá‰∫∫È°ûÊúüÊúõ‰∏çÁ¨¶ÁöÑÂÖßÂÆπÔºåÂºïÁôºÈÅìÂæ∑ÂíåÊ≥ïÂæãÊñπÈù¢ÁöÑÁñëÊÖÆ„ÄÇÂõ†Ê≠§ÔºåÊé¢Á¥¢Ê®°ÂûãÁöÑÈôêÂà∂‰∏¶ÂØ¶ÊñΩÈôêÂà∂‰ª•Á¢∫‰øùÂÆâÂÖ®ÊÄßÂíåÂêàË¶èÊÄßÈùûÂ∏∏ÈáçË¶ÅÔºåËÄåÈÄèÈÅé‰∫∫È°ûÂõûÈ•ãÁöÑÂº∑ÂåñÂ≠∏Áøí (RLHF) ÊòØ‰∏ªË¶ÅÊñπÊ≥ï„ÄÇÁî±Êñº RLHF ÈöéÊÆµÂú®Á©©ÂÆöÊÄßÂíåÂèØÊì¥ÂÖÖÊÄßÊñπÈù¢Â≠òÂú®ÊåëÊà∞ÔºåÁ†îÁ©∂‰∫∫Âì°Ê≠£Âú®Êé¢Á¥¢Êõø‰ª£ÊñπÊ≥ï‰ª•ÂØ¶ÁèæËàá RLHF Áõ∏Áï∂ÁöÑÊïàÊûú„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏‰æùË≥¥ÊñºÂ§ßÂûãÈ´òÂìÅË≥™ÁöÑË≥áÊñôÈõÜÔºå‰∏îÁÑ°Ê≥ïÊúâÊïàÂà©Áî®Áî¢ÁîüÁöÑË≥áÊñô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ PSLEÔºåÂç≥Ë™ûË®ÄÊ®°ÂûãÊØîÂ∞çÁöÑÊº∏ÈÄ≤ÂºèÈÅ∏ÊìáÊ®ôÁ±§Â¢ûÂº∑ÔºåÈÄôÊòØ‰∏ÄÂÄãÈÄèÈÅéÊåáÂ∞éÊ®°Âûã‰ΩøÁî®ÂéüÂâá‰æÜÊØîÂ∞çËº∏Âá∫Ëàá‰∫∫È°ûÊúüÊúõÔºåÈÄ≤ËÄåÂÖÖÂàÜÂà©Áî®ÊâÄÊúâÁî¢ÁîüË≥áÊñôÁöÑÊ°ÜÊû∂„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰ΩøÁî®ÂãïÊÖãÊõ¥Êñ∞ÁöÑÈñæÂÄºÔºåÈÄèÈÅéÁ¥çÂÖ•ÊâÄÊúâÁî¢ÁîüÁöÑÂõûÊáâ‰∏¶Ê†πÊìöÂÖ∂Â∞çÊáâÁöÑÁçéÂãµÂàÜÊï∏Âä†Ê¨äÔºåÁ¢∫‰øùÊúâÊïàÂà©Áî®Ë≥áÊñô„ÄÇÂú®Â§öÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫Ü PSLE ËàáÁèæÊúâÁöÑË™ûË®ÄÊ®°ÂûãÊØîÂ∞çÊñπÊ≥ïÁõ∏ÊØîÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection**
2408.02595v1 by Sajal Aggarwal, Ananya Pandey, Dinesh Kumar Vishwakarma

Sarcasm is a type of irony, characterized by an inherent mismatch between the
literal interpretation and the intended connotation. Though sarcasm detection
in text has been extensively studied, there are situations in which textual
input alone might be insufficient to perceive sarcasm. The inclusion of
additional contextual cues, such as images, is essential to recognize sarcasm
in social media data effectively. This study presents a novel framework for
multimodal sarcasm detection that can process input triplets. Two components of
these triplets comprise the input text and its associated image, as provided in
the datasets. Additionally, a supplementary modality is introduced in the form
of descriptive image captions. The motivation behind incorporating this visual
semantic representation is to more accurately capture the discrepancies between
the textual and visual content, which are fundamental to the sarcasm detection
task. The primary contributions of this study are: (1) a robust textual feature
extraction branch that utilizes a cross-lingual language model; (2) a visual
feature extraction branch that incorporates a self-regulated residual ConvNet
integrated with a lightweight spatially aware attention module; (3) an
additional modality in the form of image captions generated using an
encoder-decoder architecture capable of reading text embedded in images; (4)
distinct attention modules to effectively identify the incongruities between
the text and two levels of image representations; (5) multi-level cross-domain
semantic incongruity representation achieved through feature fusion. Compared
with cutting-edge baselines, the proposed model achieves the best accuracy of
92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and
MultiBully datasets.

ÊëòË¶ÅÔºö<paragraph>Ë´∑Âà∫ÊòØ‰∏ÄÁ®ÆÂèçË´∑ÔºåÂÖ∂ÁâπÈªûÊòØÂ≠óÈù¢ÊÑèÊÄùËàáÈ†êÊúüÁöÑÂê´Áæ©‰πãÈñìÂ≠òÂú®Âõ∫ÊúâÁöÑ‰∏çÂåπÈÖç„ÄÇÈõñÁÑ∂ÊñáÊú¨‰∏≠ÁöÑË´∑Âà∫ÂÅµÊ∏¨Â∑≤Ë¢´Âª£Ê≥õÁ†îÁ©∂Ôºå‰ΩÜÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºåÂñÆÁç®ÁöÑÊñáÊú¨Ëº∏ÂÖ•ÂèØËÉΩ‰∏çË∂≥‰ª•ÊÑüÁü•Ë´∑Âà∫„ÄÇÂä†ÂÖ•È°çÂ§ñÁöÑ‰∏ä‰∏ãÊñáÁ∑öÁ¥¢Ôºå‰æãÂ¶ÇÂúñÁâáÔºåÂ∞çÊñºÊúâÊïàË≠òÂà•Á§æÁæ§Â™íÈ´îË≥áÊñô‰∏≠ÁöÑË´∑Âà∫Ëá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ§öÊ®°ÊÖãË´∑Âà∫ÂÅµÊ∏¨ÁöÑÊñ∞Ê°ÜÊû∂ÔºåÂèØ‰ª•ËôïÁêÜËº∏ÂÖ•‰∏âÂÖÉÁµÑ„ÄÇÈÄô‰∫õ‰∏âÂÖÉÁµÑÁöÑÂÖ©ÂÄãÁµÑÊàêÈÉ®ÂàÜÂåÖÊã¨Ëº∏ÂÖ•ÊñáÊú¨ÂèäÂÖ∂ÈóúËÅØÂúñÁâáÔºåÂ¶ÇË≥áÊñôÈõÜ‰∏≠ÊâÄÊèê‰æõÁöÑ„ÄÇÊ≠§Â§ñÔºå‰ª•ÊèèËø∞ÊÄßÂúñÁâáÊ®ôÈ°åÁöÑÂΩ¢ÂºèÂºïÂÖ•‰∫Ü‰∏ÄÂÄãË£úÂÖÖÊ®°ÊÖã„ÄÇÂä†ÂÖ•ÈÄôÁ®ÆË¶ñË¶∫Ë™ûÁæ©Ë°®Á§∫ÁöÑÂãïÊ©üÊòØÊõ¥Ê∫ñÁ¢∫Âú∞ÊçïÊçâÊñáÊú¨ÂíåË¶ñË¶∫ÂÖßÂÆπ‰πãÈñìÁöÑÂ∑ÆÁï∞ÔºåÈÄôÂ∞çÊñºË´∑Âà∫ÂÅµÊ∏¨‰ªªÂãôËá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ÁöÑ‰∏ªË¶ÅË≤¢ÁçªÂåÖÊã¨Ôºö(1) ‰∏ÄÂÄãÂº∑ÂÅ•ÁöÑÊñáÊú¨ÁâπÂæµÊèêÂèñÂàÜÊîØÔºåÂÆÉÂà©Áî®‰∫Ü‰∏ÄÂÄãË∑®Ë™ûË®ÄË™ûË®ÄÊ®°ÂûãÔºõ(2) ‰∏ÄÂÄãË¶ñË¶∫ÁâπÂæµÊèêÂèñÂàÜÊîØÔºåÂÆÉÁµêÂêà‰∫Ü‰∏ÄÂÄãËá™Ë™øÁØÄÊÆòÂ∑Æ ConvNetÔºå‰∏¶ÈõÜÊàê‰∫ÜËºïÈáèÁ¥öÁ©∫ÈñìÊÑüÁü•Ê≥®ÊÑèÂäõÊ®°ÁµÑÔºõ(3) ‰∏ÄÂÄãÈ°çÂ§ñÁöÑÊ®°ÊÖãÔºå‰ª•ÂúñÁâáÊ®ôÈ°åÁöÑÂΩ¢ÂºèÔºå‰ΩøÁî®‰∏ÄÂÄãËÉΩÂ§†ËÆÄÂèñÂúñÁâá‰∏≠ÂµåÂÖ•ÊñáÊú¨ÁöÑÁ∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Êû∂ÊßãÁî¢ÁîüÔºõ(4) ‰∏çÂêåÁöÑÊ≥®ÊÑèÂäõÊ®°ÁµÑÔºå‰ª•ÊúâÊïàË≠òÂà•ÊñáÊú¨ÂíåÂÖ©ÂÄãÂ±§Á¥öÁöÑÂúñÁâáË°®Á§∫‰πãÈñìÁöÑ‰∏ç‰∏ÄËá¥Ôºõ(5) ÈÄöÈÅéÁâπÂæµËûçÂêàÂØ¶ÁèæÁöÑÂ§öÂ±§Á¥öË∑®È†òÂüüË™ûÁæ©‰∏ç‰∏ÄËá¥Ë°®Á§∫„ÄÇËàáÂ∞ñÁ´ØÁöÑÂü∫Ê∫ñÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂàÜÂà•Âú® Twitter Â§öÊ®°ÊÖãË´∑Âà∫Âíå MultiBully Ë≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫Ü 92.89% Âíå 64.48% ÁöÑÊúÄ‰Ω≥Ê∫ñÁ¢∫Â∫¶„ÄÇ</paragraph>

##### **Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization**
2408.02584v1 by Ankan Mullick, Sombit Bose, Rounak Saha, Ayan Kumar Bhowmick, Aditya Vempaty, Pawan Goyal, Niloy Ganguly, Prasenjit Dey, Ravi Kokku

The ever-increasing volume of digital information necessitates efficient
methods for users to extract key insights from lengthy documents. Aspect-based
summarization offers a targeted approach, generating summaries focused on
specific aspects within a document. Despite advancements in aspect-based
summarization research, there is a continuous quest for improved model
performance. Given that large language models (LLMs) have demonstrated the
potential to revolutionize diverse tasks within natural language processing,
particularly in the problem of summarization, this paper explores the potential
of fine-tuning LLMs for the aspect-based summarization task. We evaluate the
impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,
Gemma and Aya, on a publicly available domain-specific aspect based summary
dataset. We hypothesize that this approach will enable these models to
effectively identify and extract aspect-related information, leading to
superior quality aspect-based summaries compared to the state-of-the-art. We
establish a comprehensive evaluation framework to compare the performance of
fine-tuned LLMs against competing aspect-based summarization methods and
vanilla counterparts of the fine-tuned LLMs. Our work contributes to the field
of aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs
for generating high-quality aspect-based summaries. Furthermore, it opens doors
for further exploration of using LLMs for targeted information extraction tasks
across various NLP domains.

ÊëòË¶ÅÔºöÈö®ËëóÊï∏‰ΩçË≥áË®äÈáè‰∏çÊñ∑Â¢ûÂä†Ôºå‰ΩøÁî®ËÄÖÈúÄË¶ÅÊúâÊïàÁéáÁöÑÊñπÊ≥ïÂæûÂÜóÈï∑ÁöÑÊñá‰ª∂‰∏≠ËêÉÂèñÂá∫ÈáçÈªûÊ¥ûÂØü„ÄÇÂü∫ÊñºÈù¢ÂêëÈù¢ÂêëÁöÑÊëòË¶ÅÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁõÆÊ®ôÂ∞éÂêëÁöÑÊñπÊ≥ïÔºåÁî¢ÁîüÊëòË¶ÅÔºåÂ∞àÊ≥®ÊñºÊñá‰ª∂‰∏≠ÁöÑÁâπÂÆöÈù¢Âêë„ÄÇÂÑòÁÆ°Âú®Âü∫ÊñºÈù¢ÂêëÁöÑÊëòË¶ÅÁ†îÁ©∂‰∏≠ÊúâÈÄ≤Â±ïÔºå‰ΩÜ‰ªçÊåÅÁ∫åËøΩÊ±ÇÊîπÂñÑÊ®°ÂûãÊïàËÉΩ„ÄÇËÄÉÈáèÂà∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫Èù©Êñ∞Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÂêÑÁ®Æ‰ªªÂãôÁöÑÊΩõÂäõÔºåÁâπÂà•ÊòØÂú®ÊëòË¶ÅÂïèÈ°å‰∏≠ÔºåÊú¨ÊñáÊé¢Ë®é‰∫ÜÂæÆË™ø LLM ‰ª•Áî®ÊñºÂü∫ÊñºÈù¢ÂêëÁöÑÊëòË¶Å‰ªªÂãôÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëË©ï‰º∞ÂæÆË™øÈñãÊîæÂéüÂßãÁ¢ºÂü∫Á§é LLMÔºàÂåÖÊã¨ Llama2„ÄÅMistral„ÄÅGemma Âíå AyaÔºâÂ∞çÂÖ¨ÈñãÂèØÁî®ÁöÑÁâπÂÆöÈ†òÂüüÈù¢ÂêëÊëòË¶ÅË≥áÊñôÈõÜÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÂÅáË®≠ÈÄôÁ®ÆÊñπÊ≥ïÂ∞á‰ΩøÈÄô‰∫õÊ®°ÂûãËÉΩÂ§†ÊúâÊïàÂú∞Ë≠òÂà•ÂíåËêÉÂèñÂá∫ËàáÈù¢ÂêëÁõ∏ÈóúÁöÑË≥áË®äÔºåÁî¢ÁîüËàáÁèæÊúâÊäÄË°ìÁõ∏ÊØîÂìÅË≥™Êõ¥ÂÑ™Áï∞ÁöÑÂü∫ÊñºÈù¢ÂêëÁöÑÊëòË¶Å„ÄÇÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑË©ï‰º∞Êû∂ÊßãÔºå‰ª•ÊØîËºÉÂæÆË™ø LLM ËàáÁ´∂Áà≠ÁöÑÂü∫ÊñºÈù¢ÂêëÁöÑÊëòË¶ÅÊñπÊ≥ï‰ª•ÂèäÂæÆË™ø LLM ÁöÑÂéüÂßãÁâàÊú¨‰πãÈñìÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÈÄèÈÅéÂ±ïÁ§∫ÂæÆË™ø LLM ‰ª•Áî¢ÁîüÈ´òÂìÅË≥™ÁöÑÂü∫ÊñºÈù¢ÂêëÁöÑÊëòË¶ÅÁöÑÊïàËÉΩÔºåÁÇ∫Âü∫ÊñºÈù¢ÂêëÁöÑÊëòË¶ÅÈ†òÂüüÂÅöÂá∫Ë≤¢Áçª„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰πüÈñãÂïü‰∫ÜÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢‰ΩøÁî® LLM ÈÄ≤Ë°åË∑®ÂêÑÁ®Æ NLP È†òÂüüÁöÑÁõÆÊ®ôË≥áË®äËêÉÂèñ‰ªªÂãôÁöÑÂ§ßÈñÄ„ÄÇ

##### **Clustering and Mining Accented Speech for Inclusive and Fair Speech Recognition**
2408.02582v1 by Jaeyoung Kim, Han Lu, Soheil Khorram, Anshuman Tripathi, Qian Zhang, Hasim Sak

Modern automatic speech recognition (ASR) systems are typically trained on
more than tens of thousands hours of speech data, which is one of the main
factors for their great success. However, the distribution of such data is
typically biased towards common accents or typical speech patterns. As a
result, those systems often poorly perform on atypical accented speech. In this
paper, we present accent clustering and mining schemes for fair speech
recognition systems which can perform equally well on under-represented
accented speech. For accent recognition, we applied three schemes to overcome
limited size of supervised accent data: supervised or unsupervised
pre-training, distributionally robust optimization (DRO) and unsupervised
clustering. Three schemes can significantly improve the accent recognition
model especially for unbalanced and small accented speech. Fine-tuning ASR on
the mined Indian accent speech using the proposed supervised or unsupervised
clustering schemes showed 10.0% and 5.3% relative improvements compared to
fine-tuning on the randomly sampled speech, respectively.

ÊëòË¶ÅÔºöÁèæ‰ª£Ëá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) Á≥ªÁµ±ÈÄöÂ∏∏ÊúÉÂú®Ë∂ÖÈÅéÊï∏Ëê¨Â∞èÊôÇÁöÑË™ûÈü≥Ë≥áÊñô‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÔºåÈÄôÊòØÂÆÉÂÄëÊàêÂäüÁöÑ‰∏ªË¶ÅÂõ†Á¥†‰πã‰∏Ä„ÄÇÁÑ∂ËÄåÔºåÊ≠§È°ûË≥áÊñôÁöÑÂàÜÈÖçÈÄöÂ∏∏ÊúÉÂÅèÂêëÊñºÂ∏∏Ë¶ãÁöÑÂè£Èü≥ÊàñÂÖ∏ÂûãÁöÑË™ûÈü≥Ê®°Âºè„ÄÇÂõ†Ê≠§ÔºåÈÄô‰∫õÁ≥ªÁµ±Âú®ÈùûÂÖ∏ÂûãÂè£Èü≥ÁöÑË™ûÈü≥‰∏äÂæÄÂæÄË°®Áèæ‰∏ç‰Ω≥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ÂÖ¨Âπ≥Ë™ûÈü≥Ëæ®Ë≠òÁ≥ªÁµ±ÁöÑÂè£Èü≥ÂàÜÁæ§ÂíåÊåñÊéòÊñπÊ°àÔºåÈÄô‰∫õÊñπÊ°àÂèØ‰ª•Âú®‰ª£Ë°®ÊÄß‰∏çË∂≥ÁöÑÂè£Èü≥Ë™ûÈü≥‰∏äË°®ÁèæÂæó‰∏ÄÊ®£Â•Ω„ÄÇÂ∞çÊñºÂè£Èü≥Ëæ®Ë≠òÔºåÊàëÂÄëÊáâÁî®‰∏âÁ®ÆÊñπÊ°à‰æÜÂÖãÊúçÁõ£Áù£ÂºèÂè£Èü≥Ë≥áÊñôÁöÑË¶èÊ®°ÈôêÂà∂ÔºöÁõ£Áù£ÂºèÊàñÈùûÁõ£Áù£ÂºèÈ†êË®ìÁ∑¥„ÄÅÂàÜ‰ΩàÁ©©ÂÅ•ÊúÄ‰Ω≥Âåñ (DRO) ÂíåÈùûÁõ£Áù£ÂºèÂàÜÁæ§„ÄÇ‰∏âÁ®ÆÊñπÊ°àÂèØ‰ª•È°ØËëóÊîπÂñÑÂè£Èü≥Ëæ®Ë≠òÊ®°ÂûãÔºåÁâπÂà•ÊòØÂ∞çÊñº‰∏çÂπ≥Ë°°‰∏îË¶èÊ®°Â∞èÁöÑÂè£Èü≥Ë™ûÈü≥„ÄÇ‰ΩøÁî®Âª∫Ë≠∞ÁöÑÁõ£Áù£ÂºèÊàñÈùûÁõ£Áù£ÂºèÂàÜÁæ§ÊñπÊ°àÂ∞çÊåñÊéòÁöÑÂç∞Â∫¶Âè£Èü≥Ë™ûÈü≥ÈÄ≤Ë°å ASR ÂæÆË™øÔºåËàáÂ∞çÈö®Ê©üÂèñÊ®£ÁöÑË™ûÈü≥ÈÄ≤Ë°åÂæÆË™øÁõ∏ÊØîÔºåÂàÜÂà•È°ØÁ§∫Âá∫ 10.0% Âíå 5.3% ÁöÑÁõ∏Â∞çÊîπÂñÑ„ÄÇ

##### **Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs**
2408.02571v1 by Ananya Pandey, Dinesh Kumar Vishwakarma

The emoticons are symbolic representations that generally accompany the
textual content to visually enhance or summarize the true intention of a
written message. Although widely utilized in the realm of social media, the
core semantics of these emoticons have not been extensively explored based on
multiple modalities. Incorporating textual and visual information within a
single message develops an advanced way of conveying information. Hence, this
research aims to analyze the relationship among sentences, visuals, and
emoticons. For an orderly exposition, this paper initially provides a detailed
examination of the various techniques for extracting multimodal features,
emphasizing the pros and cons of each method. Through conducting a
comprehensive examination of several multimodal algorithms, with specific
emphasis on the fusion approaches, we have proposed a novel contrastive
learning based multimodal architecture. The proposed model employs the joint
training of dual-branch encoder along with the contrastive learning to
accurately map text and images into a common latent space. Our key finding is
that by integrating the principle of contrastive learning with that of the
other two branches yields superior results. The experimental results
demonstrate that our suggested methodology surpasses existing multimodal
approaches in terms of accuracy and robustness. The proposed model attained an
accuracy of 91% and an MCC-score of 90% while assessing emoticons using the
Multimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidence
that deep features acquired by contrastive learning are more efficient,
suggesting that the proposed fusion technique also possesses strong
generalisation capabilities for recognising emoticons across several modes.

ÊëòË¶ÅÔºöË°®ÊÉÖÁ¨¶ËôüÊòØË±°ÂæµÊÄßÁöÑË°®Á§∫ÔºåÈÄöÂ∏∏‰º¥Èö®ËëóÊñáÂ≠óÂÖßÂÆπÔºå‰ª•Ë¶ñË¶∫ÊñπÂºèÂ¢ûÂº∑ÊàñÁ∏ΩÁµêÊõ∏Èù¢Ë®äÊÅØÁöÑÁúüÂØ¶ÊÑèÂúñ„ÄÇÈõñÁÑ∂Âú®Á§æÁæ§Â™íÈ´îÈ†òÂüüÂª£Ê≥õ‰ΩøÁî®Ôºå‰ΩÜÈÄô‰∫õË°®ÊÉÖÁ¨¶ËôüÁöÑÊ†∏ÂøÉË™ûÁæ©Â∞öÊú™Ê†πÊìöÂ§öÁ®ÆÊ®°ÂºèÂª£Ê≥õÊé¢Ë®é„ÄÇÂú®ÂñÆ‰∏ÄË®äÊÅØ‰∏≠Á¥çÂÖ•ÊñáÂ≠óÂíåË¶ñË¶∫Ë≥áË®äÔºåÁôºÂ±ïÂá∫ÂÇ≥ÈÅîË≥áË®äÁöÑÂÖàÈÄ≤ÊñπÂºè„ÄÇÂõ†Ê≠§ÔºåÊú¨Á†îÁ©∂Êó®Âú®ÂàÜÊûêÂè•Â≠ê„ÄÅË¶ñË¶∫ÂíåË°®ÊÉÖÁ¨¶Ëôü‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÊúâÊ¢ùÁêÜÂú∞Ë™™ÊòéÔºåÊú¨ÊñáÊúÄÂàùË©≥Á¥∞Êé¢Ë®é‰∫ÜÂêÑÁ®ÆËêÉÂèñÂ§öÊ®°ÊÖãÁâπÂæµÁöÑÊäÄË°ìÔºå‰∏¶Âº∑Ë™ø‰∫ÜÊØèÁ®ÆÊñπÊ≥ïÁöÑÂÑ™Áº∫Èªû„ÄÇÈÄèÈÅéÂ∞çÂ§öÁ®ÆÂ§öÊ®°ÊÖãÊºîÁÆóÊ≥ïÈÄ≤Ë°åÂÖ®Èù¢Êé¢Ë®éÔºåÁâπÂà•Âº∑Ë™øËûçÂêàÊñπÊ≥ïÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂ∞çÊØîÂ≠∏ÁøíÁöÑÊñ∞Á©éÂ§öÊ®°ÊÖãÊû∂Êßã„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÊé°Áî®ÈõôÂàÜÊîØÁ∑®Á¢ºÂô®ÁöÑËÅØÂêàË®ìÁ∑¥Ôºå‰ª•ÂèäÂ∞çÊØîÂ≠∏ÁøíÔºå‰ª•Ê∫ñÁ¢∫Âú∞Â∞áÊñáÂ≠óÂíåÂΩ±ÂÉèÂ∞çÊáâÂà∞‰∏ÄÂÄãÂÖ±ÂêåÁöÑÊΩõÂú®Á©∫Èñì„ÄÇÊàëÂÄëÁöÑÈóúÈçµÁôºÁèæÊòØÔºåÂ∞áÂ∞çÊØîÂ≠∏ÁøíÁöÑÂéüÂâáËàáÂÖ∂‰ªñÂÖ©ÂÄãÂàÜÊîØÁöÑÂéüÂâáÊï¥ÂêàËµ∑‰æÜÔºåÊúÉÁî¢ÁîüÊõ¥Â•ΩÁöÑÁµêÊûú„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÂª∫Ë≠∞ÁöÑÊñπÊ≥ïÂú®Ê∫ñÁ¢∫ÊÄßÂíåÁ©©ÂÅ•ÊÄßÊñπÈù¢ÂÑ™ÊñºÁèæÊúâÁöÑÂ§öÊ®°ÊÖãÊñπÊ≥ï„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂú®‰ΩøÁî®Âæû Twitter ÂèñÂæóÁöÑÂ§öÊ®°ÊÖã Twitter Ë°®ÊÉÖÁ¨¶ËôüË≥áÊñôÈõÜË©ï‰º∞Ë°®ÊÉÖÁ¨¶ËôüÊôÇÔºåÈÅîÂà∞‰∫Ü 91% ÁöÑÊ∫ñÁ¢∫Â∫¶Âíå 90% ÁöÑ MCC ÂàÜÊï∏„ÄÇÊàëÂÄëÊèê‰æõ‰∫ÜË≠âÊìöÔºåË≠âÊòéÈÄèÈÅéÂ∞çÊØîÂ≠∏ÁøíÁç≤ÂæóÁöÑÊ∑±Â∫¶ÁâπÂæµÊõ¥ÊúâÊïàÁéáÔºåÈÄôË°®Á§∫ÊâÄÊèêÂá∫ÁöÑËûçÂêàÊäÄË°ì‰πüÂÖ∑ÂÇôË∑®Â§öÁ®ÆÊ®°ÂºèËæ®Ë≠òË°®ÊÉÖÁ¨¶ËôüÁöÑÂº∑Â§ßÊ≥õÂåñËÉΩÂäõ„ÄÇ

##### **Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information**
2408.02559v1 by Yauwai Yim, Chunkit Chan, Tianyu Shi, Zheye Deng, Wei Fan, Tianshi Zheng, Yangqiu Song

Large language models (LLMs) have shown success in handling simple games with
imperfect information and enabling multi-agent coordination, but their ability
to facilitate practical collaboration against other agents in complex,
imperfect information environments, especially in a non-English environment,
still needs to be explored. This study investigates the applicability of
knowledge acquired by open-source and API-based LLMs to sophisticated
text-based games requiring agent collaboration under imperfect information,
comparing their performance to established baselines using other types of
agents. We propose a Theory of Mind (ToM) planning technique that allows LLM
agents to adapt their strategy against various adversaries using only game
rules, current state, and historical context as input. An external tool was
incorporated to mitigate the challenge of dynamic and extensive action spaces
in this card game. Our results show that although a performance gap exists
between current LLMs and state-of-the-art reinforcement learning (RL) models,
LLMs demonstrate ToM capabilities in this game setting. It consistently
improves their performance against opposing agents, suggesting their ability to
understand the actions of allies and adversaries and establish collaboration
with allies. To encourage further research and understanding, we have made our
codebase openly accessible.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫Âú®ËôïÁêÜÂÖ∑Êúâ‰∏çÂÆåÁæéË≥áË®äÁöÑÁ∞°ÂñÆÈÅäÊà≤‰ª•ÂèäÂïüÁî®Â§öÈáç‰ª£ÁêÜÂçîË™øÊñπÈù¢ÁöÑÊàêÂäüÔºå‰ΩÜÂÖ∂‰øÉÈÄ≤Âú®Ë§áÈõú„ÄÅ‰∏çÂÆåÁæéË≥áË®äÁí∞Â¢É‰∏≠ËàáÂÖ∂‰ªñ‰ª£ÁêÜÈÄ≤Ë°åÂØ¶ÈöõÂçî‰ΩúÁöÑËÉΩÂäõÔºåÁâπÂà•ÊòØÂú®ÈùûËã±Ë™ûÁí∞Â¢É‰∏≠Ôºå‰ªçÊúâÂæÖÊé¢Ë®é„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÈñãÊîæÂéüÂßãÁ¢ºÂíåÂü∫Êñº API ÁöÑ LLM ÊâÄÁç≤ÂæóÁöÑÁü•Ë≠òÔºåÂú®ÈúÄË¶Å‰ª£ÁêÜÂú®‰∏çÂÆåÁæéË≥áË®ä‰∏ãÂçî‰ΩúÁöÑÁ≤æÁ∑ªÊñáÂ≠óÈÅäÊà≤‰∏≠ÁöÑÈÅ©Áî®ÊÄßÔºå‰∏¶Â∞áÂÖ∂ÊïàËÉΩËàá‰ΩøÁî®ÂÖ∂‰ªñÈ°ûÂûã‰ª£ÁêÜÁöÑÂ∑≤Âª∫Á´ãÂü∫Ê∫ñÈÄ≤Ë°åÊØîËºÉ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂøÉÊô∫ÁêÜË´ñ (ToM) Ë¶èÂäÉÊäÄË°ìÔºåÂÖÅË®± LLM ‰ª£ÁêÜÂÉÖ‰ΩøÁî®ÈÅäÊà≤Ë¶èÂâá„ÄÅÁõÆÂâçÁãÄÊÖãÂíåÊ≠∑Âè≤ËÉåÊôØ‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰æÜË™øÊï¥ÂÖ∂Â∞çÊäóÂêÑÁ®ÆÂ∞çÊâãÁöÑÁ≠ñÁï•„ÄÇÂú®ÈÄôÂÄãÁ¥ôÁâåÈÅäÊà≤‰∏≠ÔºåÊï¥Âêà‰∫Ü‰∏ÄÂÄãÂ§ñÈÉ®Â∑•ÂÖ∑‰æÜÊ∏õËºïÂãïÊÖã‰∏îÂª£Ê≥õÂãï‰ΩúÁ©∫ÈñìÁöÑÊåëÊà∞„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÂÑòÁÆ°ÁõÆÂâçÁöÑ LLM ÂíåÊúÄÂÖàÈÄ≤ÁöÑÂº∑ÂåñÂ≠∏Áøí (RL) Ê®°Âûã‰πãÈñìÂ≠òÂú®ÊïàËÉΩÂ∑ÆË∑ùÔºå‰ΩÜ LLM Âú®Ê≠§ÈÅäÊà≤Ë®≠ÂÆö‰∏≠Â±ïÁèæ‰∫Ü ToM ËÉΩÂäõ„ÄÇÂÆÉÊåÅÁ∫åÊèêÂçáÂÖ∂Â∞çÊäóÊïµÂ∞ç‰ª£ÁêÜÁöÑÊïàËÉΩÔºåË°®ÊòéÂÖ∂‰∫ÜËß£ÁõüÂèãÂíåÊïµ‰∫∫ÁöÑË°åÂãïÔºå‰∏¶ËàáÁõüÂèãÂª∫Á´ãÂêà‰ΩúÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÈºìÂãµÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂ÂíåÁêÜËß£ÔºåÊàëÂÄëÂ∑≤ÂÖ¨ÈñãÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∫´„ÄÇ

##### **MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization**
2408.02555v1 by Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, Guosheng Lin

We introduce MeshAnything V2, an autoregressive transformer that generates
Artist-Created Meshes (AM) aligned to given shapes. It can be integrated with
various 3D asset production pipelines to achieve high-quality, highly
controllable AM generation. MeshAnything V2 surpasses previous methods in both
efficiency and performance using models of the same size. These improvements
are due to our newly proposed mesh tokenization method: Adjacent Mesh
Tokenization (AMT). Different from previous methods that represent each face
with three vertices, AMT uses a single vertex whenever possible. Compared to
previous methods, AMT requires about half the token sequence length to
represent the same mesh in average. Furthermore, the token sequences from AMT
are more compact and well-structured, fundamentally benefiting AM generation.
Our extensive experiments show that AMT significantly improves the efficiency
and performance of AM generation. Project Page:
https://buaacyw.github.io/meshanything-v2/

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π‰∫Ü MeshAnything V2ÔºåÈÄôÊòØ‰∏ÄÁ®ÆËá™Ëø¥Ê≠∏ËΩâÊèõÂô®ÔºåÂèØÁîüÊàêËàáÁµ¶ÂÆöÂΩ¢ÁãÄÂ∞çÈΩäÁöÑËóùË°ìÂÆ∂ÂâµÂª∫Á∂≤Ê†º (AM)„ÄÇÂÆÉÂèØ‰ª•ËàáÂêÑÁ®Æ 3D Ë≥áÁî¢Ë£Ω‰ΩúÁÆ°ÈÅìÊï¥ÂêàÔºå‰ª•ÂØ¶ÁèæÈ´òÂìÅË≥™„ÄÅÈ´òÂ∫¶ÂèØÊéßÁöÑ AM ÁîüÊàê„ÄÇMeshAnything V2 Âú®ÊïàÁéáÂíåÊïàËÉΩÊñπÈù¢ÈÉΩË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑÊ®°ÂûãÔºå‰ΩøÁî®Áõ∏ÂêåÂ§ßÂ∞èÁöÑÊ®°Âûã„ÄÇÈÄô‰∫õÊîπÈÄ≤Ê≠∏ÂäüÊñºÊàëÂÄëÊñ∞ÊèêÂá∫ÁöÑÁ∂≤Ê†ºÊ®ôË®òÂåñÊñπÊ≥ïÔºöÁõ∏ÈÑ∞Á∂≤Ê†ºÊ®ôË®òÂåñ (AMT)„ÄÇËàá‰πãÂâçÁî®‰∏âÂÄãÈ†ÇÈªûË°®Á§∫ÊØèÂÄãÈù¢ÁöÑÊñπÊ≥ï‰∏çÂêåÔºåAMT Áõ°ÂèØËÉΩ‰ΩøÁî®ÂñÆÂÄãÈ†ÇÈªû„ÄÇËàá‰πãÂâçÁöÑÊ®°ÂûãÁõ∏ÊØîÔºåAMT Âπ≥ÂùáÈúÄË¶ÅÂ§ßÁ¥Ñ‰∏ÄÂçäÁöÑÊ®ôË®òÂ∫èÂàóÈï∑Â∫¶‰æÜË°®Á§∫Áõ∏ÂêåÁöÑÁ∂≤Ê†º„ÄÇÊ≠§Â§ñÔºå‰æÜËá™ AMT ÁöÑÊ®ôË®òÂ∫èÂàóÊõ¥Á∑äÊπä‰∏îÁµêÊßãËâØÂ•ΩÔºåÂæûÊ†πÊú¨‰∏äÊúâÂà©Êñº AM ÁîüÊàê„ÄÇÊàëÂÄëÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåAMT Â§ßÂπÖÊèêÈ´ò‰∫Ü AM ÁîüÊàêÊïàÁéáÂíåÊïàËÉΩ„ÄÇÂ∞àÊ°àÈ†ÅÈù¢Ôºö
https://buaacyw.github.io/meshanything-v2/

##### **The Role of Functional Muscle Networks in Improving Hand Gesture Perception for Human-Machine Interfaces**
2408.02547v1 by Costanza Armanini, Tuka Alhanai, Farah E. Shamout, S. Farokh Atashzar

Developing accurate hand gesture perception models is critical for various
robotic applications, enabling effective communication between humans and
machines and directly impacting neurorobotics and interactive robots. Recently,
surface electromyography (sEMG) has been explored for its rich informational
context and accessibility when combined with advanced machine learning
approaches and wearable systems. The literature presents numerous approaches to
boost performance while ensuring robustness for neurorobots using sEMG, often
resulting in models requiring high processing power, large datasets, and less
scalable solutions. This paper addresses this challenge by proposing the
decoding of muscle synchronization rather than individual muscle activation. We
study coherence-based functional muscle networks as the core of our perception
model, proposing that functional synchronization between muscles and the
graph-based network of muscle connectivity encode contextual information about
intended hand gestures. This can be decoded using shallow machine learning
approaches without the need for deep temporal networks. Our technique could
impact myoelectric control of neurorobots by reducing computational burdens and
enhancing efficiency. The approach is benchmarked on the Ninapro database,
which contains 12 EMG signals from 40 subjects performing 17 hand gestures. It
achieves an accuracy of 85.1%, demonstrating improved performance compared to
existing methods while requiring much less computational power. The results
support the hypothesis that a coherence-based functional muscle network encodes
critical information related to gesture execution, significantly enhancing hand
gesture perception with potential applications for neurorobotic systems and
interactive machines.

ÊëòË¶ÅÔºöÈñãÁôºÊ∫ñÁ¢∫ÁöÑÊâãÈÉ®ÊâãÂã¢ÊÑüÁü•Ê®°ÂûãÂ∞çÊñºÂêÑÁ®ÆÊ©üÂô®‰∫∫ÊáâÁî®Ëá≥ÈóúÈáçË¶ÅÔºåËÉΩËÆì‰∫∫È°ûËàáÊ©üÂô®‰πãÈñìÊúâÊïàÊ∫ùÈÄöÔºå‰∏¶Áõ¥Êé•ÂΩ±ÈüøÁ•ûÁ∂ìÊ©üÂô®‰∫∫Âíå‰∫íÂãïÂºèÊ©üÂô®‰∫∫„ÄÇÊúÄËøëÔºåË°®Èù¢ËÇåÈõªÂúñ (sEMG) Â∑≤Ë¢´Êé¢Á¥¢ÂÖ∂Ë±êÂØåÁöÑË≥áË®äËÑàÁµ°ÂíåÂèØÂèäÊÄßÔºå‰∏¶ËàáÂÖàÈÄ≤ÁöÑÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÂíåÂèØÁ©øÊà¥Á≥ªÁµ±ÁµêÂêà‰ΩøÁî®„ÄÇÊñáÁçªÊèêÂá∫‰∫ÜË®±Â§öÊñπÊ≥ï‰æÜÊèêÂçáÊïàËÉΩÔºåÂêåÊôÇÁ¢∫‰øù‰ΩøÁî® sEMG ÁöÑÁ•ûÁ∂ìÊ©üÂô®‰∫∫ÁöÑÁ©©ÂÅ•ÊÄßÔºåÈÄöÂ∏∏ÊúÉÁî¢ÁîüÈúÄË¶ÅÈ´òËôïÁêÜËÉΩÂäõ„ÄÅÂ§ßÂûãË≥áÊñôÈõÜÂíåËºÉ‰∏çÂÖ∑Êì¥ÂÖÖÊÄßÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÊú¨ÊñáÈÄèÈÅéÊèêÂá∫Ëß£Á¢ºËÇåËÇâÂêåÊ≠•ÂåñÔºåËÄåÈùûÂÄãÂà•ËÇåËÇâÊ¥ªÂåñÔºå‰æÜËß£Ê±∫Ê≠§ÊåëÊà∞„ÄÇÊàëÂÄëÁ†îÁ©∂Âü∫ÊñºÁõ∏Âπ≤ÊÄßÁöÑÂäüËÉΩÊÄßËÇåËÇâÁ∂≤Ë∑ØÔºå‰ΩúÁÇ∫ÊàëÂÄëÊÑüÁü•Ê®°ÂûãÁöÑÊ†∏ÂøÉÔºåÊèêÂá∫ËÇåËÇâ‰πãÈñìÁöÑÂäüËÉΩÊÄßÂêåÊ≠•ÂåñÂíåÂü∫ÊñºÂúñÂΩ¢ÁöÑËÇåËÇâÈÄ£Êé•Á∂≤Ë∑ØÁ∑®Á¢ºÊúâÈóúÈ†êÊúüÊâãÈÉ®ÊâãÂã¢ÁöÑËÉåÊôØË≥áË®ä„ÄÇÈÄôÂèØ‰ª•‰ΩøÁî®Ê∑∫Â±§Ê©üÂô®Â≠∏ÁøíÊñπÊ≥ïËß£Á¢ºÔºåËÄå‰∏çÈúÄË¶ÅÊ∑±Â∫¶ÊôÇÈñìÁ∂≤Ë∑Ø„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂèØ‰ª•ÈÄèÈÅéÊ∏õÂ∞ëÈÅãÁÆóË≤†ÊìîÂíåÊèêÈ´òÊïàÁéáÔºåÂΩ±ÈüøÁ•ûÁ∂ìÊ©üÂô®‰∫∫ÁöÑËÇåÈõªÊéßÂà∂„ÄÇÊ≠§ÊñπÊ≥ï‰ª• Ninapro Ë≥áÊñôÂ∫´ÁÇ∫Âü∫Ê∫ñÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™ 40 ‰ΩçÂèóË©¶ËÄÖÂü∑Ë°å 17 Á®ÆÊâãÈÉ®ÊâãÂã¢ÁöÑ 12 ÂÄã EMG Ë®äËôü„ÄÇÂÆÉÈÅîÂà∞‰∫Ü 85.1% ÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåË°®ÁèæÊúâÊâÄÊèêÂçáÔºåÂêåÊôÇÈúÄË¶ÅÁöÑÈÅãÁÆóËÉΩÂäõ‰πüÂ∞ë‰∫ÜÂæàÂ§ö„ÄÇÁµêÊûúÊîØÊåÅ‰∫ÜÂü∫ÊñºÁõ∏Âπ≤ÊÄßÁöÑÂäüËÉΩÊÄßËÇåËÇâÁ∂≤Ë∑ØÁ∑®Á¢ºËàáÊâãÂã¢Âü∑Ë°åÁõ∏ÈóúÁöÑÈáçË¶ÅË≥áË®äÁöÑÂÅáË®≠ÔºåÈ°ØËëóÂ¢ûÂº∑‰∫ÜÊâãÈÉ®ÊâãÂã¢ÊÑüÁü•Ôºå‰∏¶ÂÖ∑ÊúâÁ•ûÁ∂ìÊ©üÂô®‰∫∫Á≥ªÁµ±Âíå‰∫íÂãïÂºèÊ©üÂô®‰∫∫ÁöÑÊΩõÂú®ÊáâÁî®„ÄÇ

##### **RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation**
2408.02545v1 by Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat, Peter Izsak

Implementing Retrieval-Augmented Generation (RAG) systems is inherently
complex, requiring deep understanding of data, use cases, and intricate design
decisions. Additionally, evaluating these systems presents significant
challenges, necessitating assessment of both retrieval accuracy and generative
quality through a multi-faceted approach. We introduce RAG Foundry, an
open-source framework for augmenting large language models for RAG use cases.
RAG Foundry integrates data creation, training, inference and evaluation into a
single workflow, facilitating the creation of data-augmented datasets for
training and evaluating large language models in RAG settings. This integration
enables rapid prototyping and experimentation with various RAG techniques,
allowing users to easily generate datasets and train RAG models using internal
or specialized knowledge sources. We demonstrate the framework effectiveness by
augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG
configurations, showcasing consistent improvements across three
knowledge-intensive datasets. Code is released as open-source in
https://github.com/IntelLabs/RAGFoundry.

ÊëòË¶ÅÔºöÂØ¶‰ΩúÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºàRAGÔºâÁ≥ªÁµ±Êú¨Ë≥™‰∏äÂæàË§áÈõúÔºåÈúÄË¶ÅÊ∑±ÂÖ•‰∫ÜËß£Ë≥áÊñô„ÄÅ‰ΩøÁî®Ê°à‰æãÂíåË§áÈõúÁöÑË®≠Ë®àÊ±∫Á≠ñ„ÄÇÊ≠§Â§ñÔºåË©ï‰º∞ÈÄô‰∫õÁ≥ªÁµ±ÊúÉÂ∏∂‰æÜÈ°ØËëóÁöÑÊåëÊà∞ÔºåÈúÄË¶ÅÈÄèÈÅéÂ§öÈù¢ÂêëÁöÑÊñπÊ≥ïË©ï‰º∞Ê™¢Á¥¢Ê∫ñÁ¢∫Â∫¶ÂíåÁîüÊàêÂìÅË≥™„ÄÇÊàëÂÄë‰ªãÁ¥π RAG FoundryÔºå‰∏ÄÂÄãÁî®ÊñºÊì¥ÂÖÖÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰ª•ÈÄ≤Ë°å RAG ‰ΩøÁî®Ê°à‰æãÁöÑÈñãÊ∫êÊû∂Êßã„ÄÇRAG Foundry Â∞áË≥áÊñôÂª∫Á´ã„ÄÅË®ìÁ∑¥„ÄÅÊé®Ë´ñÂíåË©ï‰º∞Êï¥ÂêàÂà∞ÂñÆ‰∏ÄÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÔºåÁ∞°Âåñ‰∫ÜË≥áÊñôÂ¢ûÂº∑Ë≥áÊñôÈõÜÁöÑÂª∫Á´ãÔºå‰ª•‰æøÂú® RAG Ë®≠ÂÆö‰∏≠Ë®ìÁ∑¥ÂíåË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÇÈÄôÁ®ÆÊï¥ÂêàËÉΩÂø´ÈÄüË£Ω‰ΩúÂéüÂûãÂíå‰ΩøÁî®ÂêÑÁ®Æ RAG ÊäÄË°ìÈÄ≤Ë°åÂØ¶È©óÔºåËÆì‰ΩøÁî®ËÄÖËÉΩËºïÈ¨Ü‰ΩøÁî®ÂÖßÈÉ®ÊàñÂ∞àÊ•≠Áü•Ë≠ò‰æÜÊ∫ê‰æÜÁî¢ÁîüË≥áÊñôÈõÜ‰∏¶Ë®ìÁ∑¥ RAG Ê®°Âûã„ÄÇÊàëÂÄëÈÄèÈÅé‰ΩøÁî®Â§öÊ®£ÂåñÁöÑ RAG ÁµÑÊÖã‰æÜÊì¥ÂÖÖÂíåÂæÆË™ø Llama-3 Âíå Phi-3 Ê®°Âûã‰æÜÂ±ïÁ§∫Êû∂ÊßãÁöÑÊúâÊïàÊÄßÔºåÂ±ïÁ§∫‰∫ÜÂú®‰∏âÂÄãÁü•Ë≠òÂØÜÈõÜÂûãË≥áÊñôÈõÜ‰∏≠ÁöÑÊåÅÁ∫åÊîπÈÄ≤„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤Âú® https://github.com/IntelLabs/RAGFoundry ‰∏≠‰ª•ÈñãÊ∫êÊñπÂºèÈáãÂá∫„ÄÇ

##### **Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions**
2408.02544v1 by Xinbei Ma, Yiting Wang, Yao Yao, Tongxin Yuan, Aston Zhang, Zhuosheng Zhang, Hai Zhao

This paper investigates the faithfulness of multimodal large language model
(MLLM) agents in the graphical user interface (GUI) environment, aiming to
address the research question of whether multimodal GUI agents can be
distracted by environmental context. A general setting is proposed where both
the user and the agent are benign, and the environment, while not malicious,
contains unrelated content. A wide range of MLLMs are evaluated as GUI agents
using our simulated dataset, following three working patterns with different
levels of perception. Experimental results reveal that even the most powerful
models, whether generalist agents or specialist GUI agents, are susceptible to
distractions. While recent studies predominantly focus on the helpfulness
(i.e., action accuracy) of multimodal agents, our findings indicate that these
agents are prone to environmental distractions, resulting in unfaithful
behaviors. Furthermore, we switch to the adversarial perspective and implement
environment injection, demonstrating that such unfaithfulness can be exploited,
leading to unexpected risks.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫ÜÂúñÂΩ¢‰ΩøÁî®ËÄÖ‰ªãÈù¢ (GUI) Áí∞Â¢É‰∏≠Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ‰ª£ÁêÜÁöÑÂø†ÂØ¶Â∫¶ÔºåÊó®Âú®Ëß£Ê±∫Â§öÊ®°ÊÖã GUI ‰ª£ÁêÜÊòØÂê¶ÊúÉÂèóÂà∞Áí∞Â¢ÉËÉåÊôØÂΩ±ÈüøÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄËà¨Ë®≠ÂÆöÔºåÂÖ∂‰∏≠‰ΩøÁî®ËÄÖÂíå‰ª£ÁêÜÈÉΩÊòØËâØÊÄßÁöÑÔºåËÄåÁí∞Â¢ÉÈõñÁÑ∂Ê≤íÊúâÊÉ°ÊÑèÔºå‰ΩÜÂåÖÂê´‰∏çÁõ∏ÈóúÁöÑÂÖßÂÆπ„ÄÇÊàëÂÄë‰ΩøÁî®Ê®°Êì¨Ë≥áÊñôÈõÜË©ï‰º∞‰∫ÜÂª£Ê≥õÁöÑ MLLM ‰ΩúÁÇ∫ GUI ‰ª£ÁêÜÔºåÈÅµÂæ™ÂÖ∑Êúâ‰∏çÂêåÊÑüÁü•Â±§Á¥öÁöÑ‰∏âÁ®ÆÂ∑•‰ΩúÊ®°Âºè„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÂç≥‰ΩøÊòØÊúÄÂº∑Â§ßÁöÑÊ®°ÂûãÔºàÁÑ°Ë´ñÊòØÈÄöÊâç‰ª£ÁêÜÈÇÑÊòØÂ∞àÂÆ∂ GUI ‰ª£ÁêÜÔºâÈÉΩÂÆπÊòìÂèóÂà∞Âπ≤Êìæ„ÄÇÂÑòÁÆ°ÊúÄËøëÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈóúÊ≥®Â§öÊ®°ÊÖã‰ª£ÁêÜÁöÑÊúâÁî®ÊÄßÔºàÂç≥Âãï‰ΩúÊ∫ñÁ¢∫ÊÄßÔºâÔºå‰ΩÜÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÈÄô‰∫õ‰ª£ÁêÜÂÆπÊòìÂèóÂà∞Áí∞Â¢ÉÂπ≤ÊìæÔºåÂ∞éËá¥‰∏çÂø†ÂØ¶ÁöÑË°åÁÇ∫„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëËΩâÂêëÂ∞çÊäóËßÄÈªû‰∏¶ÂØ¶‰ΩúÁí∞Â¢ÉÊ≥®ÂÖ•ÔºåË≠âÊòéÈÄôÁ®Æ‰∏çÂø†ÂØ¶ÂèØ‰ª•Ë¢´Âà©Áî®ÔºåÂ∞éËá¥ÊÑèÂ§ñÁöÑÈ¢®Èö™„ÄÇ

##### **OneLove beyond the field -- A few-shot pipeline for topic and sentiment analysis during the FIFA World Cup in Qatar**
2408.02520v1 by Christoph Rauchegger, Sonja Mei Wang, Pieter Delobelle

The FIFA World Cup in Qatar was discussed extensively in the news and on
social media. Due to news reports with allegations of human rights violations,
there were calls to boycott it. Wearing a OneLove armband was part of a planned
protest activity. Controversy around the armband arose when FIFA threatened to
sanction captains who wear it. To understand what topics Twitter users Tweeted
about and what the opinion of German Twitter users was towards the OneLove
armband, we performed an analysis of German Tweets published during the World
Cup using in-context learning with LLMs. We validated the labels on human
annotations. We found that Twitter users initially discussed the armband's
impact, LGBT rights, and politics; after the ban, the conversation shifted
towards politics in sports in general, accompanied by a subtle shift in
sentiment towards neutrality. Our evaluation serves as a framework for future
research to explore the impact of sports activism and evolving public
sentiment. This is especially useful in settings where labeling datasets for
specific opinions is unfeasible, such as when events are unfolding.

ÊëòË¶ÅÔºöÂç°ÈÅîÁöÑ FIFA ‰∏ñÁïåÁõÉÂú®Êñ∞ËÅûÂíåÁ§æÁæ§Â™íÈ´î‰∏äË¢´Âª£Ê≥õË®éË´ñ„ÄÇÁî±ÊñºÊñ∞ËÅûÂ†±Â∞é‰∏≠Êúâ‰∫∫ÊåáÊéßÊúâ‰æµÁäØ‰∫∫Ê¨äÁöÑË°åÁÇ∫ÔºåÂõ†Ê≠§Êúâ‰∫∫ÂëºÁ±≤ÊäµÂà∂„ÄÇ‰Ω©Êà¥ OneLove ËáÇÁ´†ÊòØË®àÁï´ÊäóË≠∞Ê¥ªÂãïÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÁï∂ FIFA Â®ÅËÑÖË¶ÅÂà∂Ë£Å‰Ω©Êà¥ËáÇÁ´†ÁöÑÈöäÈï∑ÊôÇÔºåÂúçÁπûËáÇÁ´†ÁöÑÁà≠Ë≠∞Èö®‰πãËÄåËµ∑„ÄÇÁÇ∫‰∫Ü‰∫ÜËß£ Twitter Áî®Êà∂Êé®ÊñáÁöÑ‰∏ªÈ°å‰ª•ÂèäÂæ∑Âúã Twitter Áî®Êà∂Â∞ç OneLove ËáÇÁ´†ÁöÑÁúãÊ≥ïÔºåÊàëÂÄë‰ΩøÁî® LLM ÁöÑË™ûÂ¢ÉÂ≠∏ÁøíÂ∞ç‰∏ñÁïåÁõÉÊúüÈñìÁôºÂ∏ÉÁöÑÂæ∑ÊñáÊé®ÊñáÈÄ≤Ë°å‰∫ÜÂàÜÊûê„ÄÇÊàëÂÄëÈ©óË≠â‰∫Ü‰∫∫Â∑•Ê®ôË®ªÁöÑÊ®ôÁ±§„ÄÇÊàëÂÄëÁôºÁèæÔºåTwitter Áî®Êà∂ÊúÄÂàùË®éË´ñÁöÑÊòØËáÇÁ´†ÁöÑÂΩ±Èüø„ÄÅLGBT Ê¨äÂà©ÂíåÊîøÊ≤ªÔºõÂú®Á¶Å‰ª§‰πãÂæåÔºåÂ∞çË©±ËΩâÂêëÈ´îËÇ≤‰∏≠ÁöÑÊîøÊ≤ªÔºåÂêåÊôÇÊÉÖÁ∑íÂæÆÂ¶ôÂú∞ËΩâÂêë‰∏≠Á´ã„ÄÇÊàëÂÄëÁöÑË©ï‰º∞‰ΩúÁÇ∫Êú™‰æÜÁ†îÁ©∂Êé¢Á¥¢ÈÅãÂãïÊøÄÈÄ≤‰∏ªÁæ©ÂíåÂÖ¨ÁúæÊÉÖÁ∑íÊºîËÆäÁöÑÂΩ±ÈüøÁöÑÊ°ÜÊû∂„ÄÇÈÄôÂú®ÁÇ∫ÁâπÂÆöÊÑèË¶ãÊ®ôË®òË≥áÊñôÈõÜ‰∏çÂèØË°åÁöÑÊÉÖÊ≥Å‰∏ãÁâπÂà•ÊúâÁî®Ôºå‰æãÂ¶ÇÂú®‰∫ã‰ª∂Ê≠£Âú®Â±ïÈñãÊôÇ„ÄÇ

##### **UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model**
2408.02503v1 by Zhaowei Li, Wei Wang, YiQing Cai, Xu Qi, Pengyu Wang, Dong Zhang, Hang Song, Botian Jiang, Zhida Huang, Tao Wang

Significant advancements has recently been achieved in the field of
multi-modal large language models (MLLMs), demonstrating their remarkable
capabilities in understanding and reasoning across diverse tasks. However,
these models are often trained for specific tasks and rely on task-specific
input-output formats, limiting their applicability to a broader range of tasks.
This raises a fundamental question: Can we develop a unified approach to
represent and handle different multi-modal tasks to maximize the
generalizability of MLLMs? In this paper, we propose UnifiedMLLM, a
comprehensive model designed to represent various tasks using a unified
representation. Our model exhibits strong capabilities in comprehending the
implicit intent of user instructions and preforming reasoning. In addition to
generating textual responses, our model also outputs task tokens and grounding
tokens, serving as indicators of task types and task granularity. These outputs
are subsequently routed through the task router and directed to specific expert
models for task completion. To train our model, we construct a task-specific
dataset and an 100k multi-task dataset encompassing complex scenarios.
Employing a three-stage training strategy, we equip our model with robust
reasoning and task processing capabilities while preserving its generalization
capacity and knowledge reservoir. Extensive experiments showcase the impressive
performance of our unified representation approach across various tasks,
surpassing existing methodologies. Furthermore, our approach exhibits
exceptional scalability and generality. Our code, model, and dataset will be
available at \url{https://github.com/lzw-lzw/UnifiedMLLM}.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºåÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (MLLM) È¢ÜÂüüÂèñÂæó‰∫ÜÈáçÂ§ßËøõÂ±ïÔºåÂ±ïÁ§∫‰∫ÜÂÆÉ‰ª¨Âú®ÁêÜËß£ÂíåÊé®ÁêÜÂêÑÁßç‰ªªÂä°ÊñπÈù¢ÁöÑÂçìË∂äËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏ÈíàÂØπÁâπÂÆö‰ªªÂä°ËøõË°åËÆ≠ÁªÉÔºåÂπ∂‰æùËµñ‰∫éÁâπÂÆö‰ªªÂä°ÁöÑËæìÂÖ•ËæìÂá∫Ê†ºÂºèÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Êõ¥ÂπøÊ≥õÁöÑ‰ªªÂä°ËåÉÂõ¥ÂÜÖÁöÑÈÄÇÁî®ÊÄß„ÄÇËøôÂºïÂèë‰∫Ü‰∏Ä‰∏™Âü∫Êú¨ÈóÆÈ¢òÔºöÊàë‰ª¨ËÉΩÂê¶ÂºÄÂèë‰∏ÄÁßçÁªü‰∏ÄÁöÑÊñπÊ≥ïÊù•Ë°®Á§∫ÂíåÂ§ÑÁêÜ‰∏çÂêåÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°Ôºå‰ª•ÊúÄÂ§ßÂåñ MLLM ÁöÑÊ≥õÂåñËÉΩÂäõÔºüÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü UnifiedMLLMÔºåËøôÊòØ‰∏Ä‰∏™ÁªºÂêàÊ®°ÂûãÔºåÊó®Âú®‰ΩøÁî®Áªü‰∏ÄË°®Á§∫Êù•Ë°®Á§∫ÂêÑÁßç‰ªªÂä°„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®ÁêÜËß£Áî®Êà∑Êåá‰ª§ÁöÑÈöêÂê´ÊÑèÂõæÂíåËøõË°åÊé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Âº∫Â§ßÁöÑËÉΩÂäõ„ÄÇÈô§‰∫ÜÁîüÊàêÊñáÊú¨ÂìçÂ∫î‰πãÂ§ñÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãËøòËæìÂá∫‰ªªÂä°Ê†áËÆ∞ÂíåÂü∫Á°ÄÊ†áËÆ∞Ôºå‰Ωú‰∏∫‰ªªÂä°Á±ªÂûãÂíå‰ªªÂä°Á≤íÂ∫¶ÁöÑÊåáÊ†á„ÄÇËøô‰∫õËæìÂá∫ÈöèÂêéÈÄöËøá‰ªªÂä°Ë∑ØÁî±Âô®Ë∑ØÁî±ÔºåÂπ∂ÂÆöÂêëÂà∞ÁâπÂÆö‰∏ìÂÆ∂Ê®°Âûã‰ª•ÂÆåÊàê‰ªªÂä°„ÄÇ‰∏∫‰∫ÜËÆ≠ÁªÉÊàë‰ª¨ÁöÑÊ®°ÂûãÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÁâπÂÆö‰∫é‰ªªÂä°ÁöÑÊï∞ÊçÆÈõÜÂíå‰∏Ä‰∏™ÂåÖÂê´Â§çÊùÇÂú∫ÊôØÁöÑ 100k Â§ö‰ªªÂä°Êï∞ÊçÆÈõÜ„ÄÇÈááÁî®‰∏âÈò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊàë‰ª¨‰∏∫Êàë‰ª¨ÁöÑÊ®°ÂûãÈÖçÂ§á‰∫ÜÂº∫Â§ßÁöÑÊé®ÁêÜÂíå‰ªªÂä°Â§ÑÁêÜËÉΩÂäõÔºåÂêåÊó∂‰øùÁïô‰∫ÜÂÆÉÁöÑÊ≥õÂåñËÉΩÂäõÂíåÁü•ËØÜÂÇ®Â§á„ÄÇÂπøÊ≥õÁöÑÂÆûÈ™åÂ±ïÁ§∫‰∫ÜÊàë‰ª¨ÁöÑÁªü‰∏ÄË°®Á§∫ÊñπÊ≥ïÂú®ÂêÑÁßç‰ªªÂä°‰∏≠ÁöÑÂá∫Ëâ≤Ë°®Áé∞ÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïË°®Áé∞Âá∫ÂçìË∂äÁöÑÂèØÊâ©Â±ïÊÄßÂíåÈÄöÁî®ÊÄß„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†Å„ÄÅÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜÂ∞ÜÂú® \url{https://github.com/lzw-lzw/UnifiedMLLM} ‰∏äÊèê‰æõ„ÄÇ</paragraph>

##### **MDM: Advancing Multi-Domain Distribution Matching for Automatic Modulation Recognition Dataset Synthesis**
2408.02714v1 by Dongwei Xu, Jiajun Chen, Yao Lu, Tianhao Xia, Qi Xuan, Wei Wang, Yun Lin, Xiaoniu Yang

Recently, deep learning technology has been successfully introduced into
Automatic Modulation Recognition (AMR) tasks. However, the success of deep
learning is all attributed to the training on large-scale datasets. Such a
large amount of data brings huge pressure on storage, transmission and model
training. In order to solve the problem of large amount of data, some
researchers put forward the method of data distillation, which aims to compress
large training data into smaller synthetic datasets to maintain its
performance. While numerous data distillation techniques have been developed
within the realm of image processing, the unique characteristics of signals set
them apart. Signals exhibit distinct features across various domains,
necessitating specialized approaches for their analysis and processing. To this
end, a novel dataset distillation method--Multi-domain Distribution Matching
(MDM) is proposed. MDM employs the Discrete Fourier Transform (DFT) to
translate timedomain signals into the frequency domain, and then uses a model
to compute distribution matching losses between the synthetic and real
datasets, considering both the time and frequency domains. Ultimately, these
two losses are integrated to update the synthetic dataset. We conduct extensive
experiments on three AMR datasets. Experimental results show that, compared
with baseline methods, our method achieves better performance under the same
compression ratio. Furthermore, we conduct crossarchitecture generalization
experiments on several models, and the experimental results show that our
synthetic datasets can generalize well on other unseen models.

ÊëòË¶ÅÔºö<paragraph>ËøëÊúüÔºåÊ∑±Â∫¶Â≠¶‰π†ÊäÄÊúØÂ∑≤ÊàêÂäüÂºïÂÖ•Âà∞Ëá™Âä®Ë∞ÉÂà∂ËØÜÂà´ÔºàAMRÔºâ‰ªªÂä°‰∏≠„ÄÇÁÑ∂ËÄåÔºåÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊàêÂäüÈÉΩÂΩíÂäü‰∫éÂú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äÁöÑËÆ≠ÁªÉ„ÄÇÂ¶ÇÊ≠§Â§ßÈáèÁöÑÊï∞ÊçÆÁªôÂ≠òÂÇ®„ÄÅ‰º†ËæìÂíåÊ®°ÂûãËÆ≠ÁªÉÂ∏¶Êù•‰∫ÜÂ∑®Â§ßÁöÑÂéãÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Êï∞ÊçÆÈáèÂ§ßÁöÑÈóÆÈ¢òÔºå‰∏Ä‰∫õÁ†îÁ©∂ËÄÖÊèêÂá∫‰∫ÜÊï∞ÊçÆËí∏È¶èÁöÑÊñπÊ≥ïÔºåÂÖ∂ÁõÆÁöÑÊòØÂ∞ÜÂ§ßÂûãËÆ≠ÁªÉÊï∞ÊçÆÂéãÁº©ÊàêÊõ¥Â∞èÁöÑÂêàÊàêÊï∞ÊçÆÈõÜÔºå‰ª•‰øùÊåÅÂÖ∂ÊÄßËÉΩ„ÄÇËôΩÁÑ∂Âú®ÂõæÂÉèÂ§ÑÁêÜÈ¢ÜÂüüÂ∑≤ÁªèÂºÄÂèëÂá∫‰∫ÜËÆ∏Â§öÊï∞ÊçÆËí∏È¶èÊäÄÊúØÔºå‰ΩÜ‰ø°Âè∑ÁöÑÁã¨ÁâπÁâπÊÄß‰ΩøÂÆÉ‰ª¨‰∏é‰ºó‰∏çÂêå„ÄÇ‰ø°Âè∑Âú®ÂêÑ‰∏™Âüü‰∏≠Ë°®Áé∞Âá∫‰∏çÂêåÁöÑÁâπÂæÅÔºåÈúÄË¶Å‰∏ìÈó®ÁöÑÊñπÊ≥ïÊù•ÂØπÂÖ∂ËøõË°åÂàÜÊûêÂíåÂ§ÑÁêÜ„ÄÇ‰∏∫Ê≠§ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊï∞ÊçÆÈõÜËí∏È¶èÊñπÊ≥ï‚Äî‚ÄîÂ§öÂüüÂàÜÂ∏ÉÂåπÈÖçÔºàMDMÔºâ„ÄÇMDM ÈááÁî®Á¶ªÊï£ÂÇÖÈáåÂè∂ÂèòÊç¢ÔºàDFTÔºâÂ∞ÜÊó∂Âüü‰ø°Âè∑ËΩ¨Êç¢‰∏∫È¢ëÂüüÔºåÁÑ∂Âêé‰ΩøÁî®Ê®°ÂûãÊù•ËÆ°ÁÆóÂêàÊàêÊï∞ÊçÆÈõÜÂíåÁúüÂÆûÊï∞ÊçÆÈõÜ‰πãÈó¥ÁöÑÂàÜÂ∏ÉÂåπÈÖçÊçüÂ§±ÔºåÂêåÊó∂ËÄÉËôëÊó∂ÂüüÂíåÈ¢ëÂüü„ÄÇÊúÄÁªàÔºåËøô‰∏§‰∏™ÊçüÂ§±Ë¢´ÈõÜÊàêËµ∑Êù•Êõ¥Êñ∞ÂêàÊàêÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨Âú®‰∏â‰∏™ AMR Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™å„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÂü∫Á∫øÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Áõ∏ÂêåÁöÑÂéãÁº©ÊØî‰∏ãËé∑Âæó‰∫ÜÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂØπÂá†‰∏™Ê®°ÂûãËøõË°å‰∫ÜË∑®Êû∂ÊûÑÊ≥õÂåñÂÆûÈ™åÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÂêàÊàêÊï∞ÊçÆÈõÜÂèØ‰ª•Âú®ÂÖ∂‰ªñÊú™ËßÅÊ®°Âûã‰∏äÂæàÂ•ΩÂú∞Ê≥õÂåñ„ÄÇ</paragraph>

##### **A First Look at License Compliance Capability of LLMs in Code Generation**
2408.02487v1 by Weiwei Xu, Kai Gao, Hao He, Minghui Zhou

Recent advances in Large Language Models (LLMs) have revolutionized code
generation, leading to widespread adoption of AI coding tools by developers.
However, LLMs can generate license-protected code without providing the
necessary license information, leading to potential intellectual property
violations during software production. This paper addresses the critical, yet
underexplored, issue of license compliance in LLM-generated code by
establishing a benchmark to evaluate the ability of LLMs to provide accurate
license information for their generated code. To establish this benchmark, we
conduct an empirical study to identify a reasonable standard for "striking
similarity" that excludes the possibility of independent creation, indicating a
copy relationship between the LLM output and certain open-source code. Based on
this standard, we propose an evaluation benchmark LiCoEval, to evaluate the
license compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popular
LLMs, finding that even top-performing LLMs produce a non-negligible proportion
(0.88% to 2.01%) of code strikingly similar to existing open-source
implementations. Notably, most LLMs fail to provide accurate license
information, particularly for code under copyleft licenses. These findings
underscore the urgent need to enhance LLM compliance capabilities in code
generation tasks. Our study provides a foundation for future research and
development to improve license compliance in AI-assisted software development,
contributing to both the protection of open-source software copyrights and the
mitigation of legal risks for LLM users.

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËøëÊúüÈÄ≤Â±ïÂæπÂ∫ïÊîπËÆä‰∫ÜÁ®ãÂºèÁ¢ºÁîüÊàêÔºåÂ∞éËá¥ÈñãÁôº‰∫∫Âì°Âª£Ê≥õÊé°Áî® AI Á∑®Á¢ºÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåLLM ÂèØ‰ª•ÁîüÊàêÂèóË®±ÂèØË≠â‰øùË≠∑ÁöÑÁ®ãÂºèÁ¢ºÔºåËÄåÊ≤íÊúâÊèê‰æõÂøÖË¶ÅÁöÑË®±ÂèØË≠âË≥áË®äÔºåÈÄôÊúÉÂú®ËªüÈ´îË£Ω‰ΩúÈÅéÁ®ã‰∏≠Â∞éËá¥ÊΩõÂú®ÁöÑÊô∫ÊÖßË≤°Áî¢Ê¨ä‰æµÊ¨ä„ÄÇÊú¨ÊñáÊé¢Ë®é LLM ÁîüÊàêÁöÑÁ®ãÂºèÁ¢º‰∏≠Ë®±ÂèØË≠âÂêàË¶èÊÄßÁöÑÈóúÈçµË≠∞È°åÔºà‰ΩÜÂ∞öÊú™ÂÖÖÂàÜÊé¢Ë®éÔºâÔºåÊñπÊ≥ïÊòØÂª∫Á´ãÂü∫Ê∫ñ‰æÜË©ï‰º∞ LLM ÁÇ∫ÂÖ∂ÁîüÊàêÁöÑÁ®ãÂºèÁ¢ºÊèê‰æõÊ∫ñÁ¢∫Ë®±ÂèØË≠âË≥áË®äÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÂª∫Á´ãÈÄôÂÄãÂü∫Ê∫ñÔºåÊàëÂÄëÈÄ≤Ë°åÂØ¶Ë≠âÁ†îÁ©∂Ôºå‰ª•ÊâæÂá∫„ÄåÊ•µÂ∫¶Áõ∏‰ºº„ÄçÁöÑÂêàÁêÜÊ®ôÊ∫ñÔºåÊéíÈô§Áç®Á´ãÂâµ‰ΩúÁöÑÂèØËÉΩÊÄßÔºåË°®Á§∫ LLM Ëº∏Âá∫ËàáÁâπÂÆöÈñãÊ∫êÁ®ãÂºèÁ¢º‰πãÈñìÁöÑË§áË£ΩÈóú‰øÇ„ÄÇÂü∫ÊñºÈÄôÂÄãÊ®ôÊ∫ñÔºåÊàëÂÄëÊèêÂá∫Ë©ï‰º∞Âü∫Ê∫ñ LiCoEvalÔºå‰ª•Ë©ï‰º∞ LLM ÁöÑË®±ÂèØË≠âÂêàË¶èËÉΩÂäõ„ÄÇ‰ΩøÁî® LiCoEvalÔºåÊàëÂÄëË©ï‰º∞‰∫Ü 14 ÂÄãÊµÅË°åÁöÑ LLMÔºåÁôºÁèæÂç≥‰ΩøÊïàËÉΩÊúÄÂ•ΩÁöÑ LLM ‰ªçÊúÉÁî¢ÁîüËàáÁèæÊúâÈñãÊ∫êÂØ¶‰ΩúÊ•µÂ∫¶Áõ∏‰ººÁöÑÁ®ãÂºèÁ¢ºÔºåÊØî‰æã‰∏çÂèØÂøΩÁï•Ôºà0.88% Ëá≥ 2.01%Ôºâ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂ§ßÂ§öÊï∏ LLM ÁÑ°Ê≥ïÊèê‰æõÊ∫ñÁ¢∫ÁöÑË®±ÂèØË≠âË≥áË®äÔºåÁâπÂà•ÊòØÂèó copyleft Ë®±ÂèØË≠â‰øùË≠∑ÁöÑÁ®ãÂºèÁ¢º„ÄÇÈÄô‰∫õÁôºÁèæÂº∑Ë™ø‰∫ÜÂú®Á®ãÂºèÁ¢ºÁîüÊàê‰ªªÂãô‰∏≠Â¢ûÂº∑ LLM ÂêàË¶èËÉΩÂäõÁöÑËø´ÂàáÈúÄÊ±Ç„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁÇ∫Êú™‰æÜÁöÑÁ†îÁ©∂ÂíåÈñãÁôºÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºå‰ª•ÊîπÂñÑ AI ËºîÂä©ËªüÈ´îÈñãÁôº‰∏≠ÁöÑË®±ÂèØË≠âÂêàË¶èÊÄßÔºåÊúâÂä©Êñº‰øùË≠∑ÈñãÊ∫êËªüÈ´îÁöÑËëó‰ΩúÊ¨äÔºå‰∏¶Ê∏õËºï LLM ‰ΩøÁî®ËÄÖÁöÑÊ≥ïÂæãÈ¢®Èö™„ÄÇ</paragraph>

##### **A Review on Organ Deformation Modeling Approaches for Reliable Surgical Navigation using Augmented Reality**
2408.02713v1 by Zheng Han, Qi Dou

Augmented Reality (AR) holds the potential to revolutionize surgical
procedures by allowing surgeons to visualize critical structures within the
patient's body. This is achieved through superimposing preoperative organ
models onto the actual anatomy. Challenges arise from dynamic deformations of
organs during surgery, making preoperative models inadequate for faithfully
representing intraoperative anatomy. To enable reliable navigation in augmented
surgery, modeling of intraoperative deformation to obtain an accurate alignment
of the preoperative organ model with the intraoperative anatomy is
indispensable. Despite the existence of various methods proposed to model
intraoperative organ deformation, there are still few literature reviews that
systematically categorize and summarize these approaches. This review aims to
fill this gap by providing a comprehensive and technical-oriented overview of
modeling methods for intraoperative organ deformation in augmented reality in
surgery. Through a systematic search and screening process, 112 closely
relevant papers were included in this review. By presenting the current status
of organ deformation modeling methods and their clinical applications, this
review seeks to enhance the understanding of organ deformation modeling in
AR-guided surgery, and discuss the potential topics for future advancements.

ÊëòË¶ÅÔºöÊì¥Â¢ûÂØ¶Â¢É (AR) ÂÖ∑ÊúâÈÄèÈÅéËÆìÂ§ñÁßëÈÜ´ÁîüÂèØË¶ñÂåñÊÇ£ËÄÖÈ´îÂÖßÈóúÈçµÁµêÊßã‰æÜÈù©Êñ∞Â§ñÁßëÊâãË°ìÁ®ãÂ∫èÁöÑÊΩõÂäõ„ÄÇÈÄôÊòØÈÄèÈÅéÂ∞áË°ìÂâçÂô®ÂÆòÊ®°ÂûãÁñäÂä†Âà∞ÂØ¶ÈöõËß£ÂâñÁµêÊßã‰∏ä‰æÜÂØ¶ÁèæÁöÑ„ÄÇÊâãË°ìÈÅéÁ®ã‰∏≠Âô®ÂÆòÁöÑÂãïÊÖãËÆäÂΩ¢Â∏∂‰æÜ‰∫ÜÊåëÊà∞ÔºåÈÄô‰ΩøÂæóË°ìÂâçÊ®°Âûã‰∏çË∂≥‰ª•Âø†ÂØ¶Âú∞ÂëàÁèæË°ì‰∏≠Ëß£ÂâñÁµêÊßã„ÄÇÁÇ∫‰∫ÜÂú®Êì¥Â¢ûÊâãË°ì‰∏≠ÂØ¶ÁèæÂèØÈù†ÁöÑÂ∞éËà™ÔºåÂ∞çË°ì‰∏≠ËÆäÂΩ¢ÈÄ≤Ë°åÂª∫Ê®°‰ª•Áç≤ÂæóË°ìÂâçÂô®ÂÆòÊ®°ÂûãËàáË°ì‰∏≠Ëß£ÂâñÁµêÊßãÁöÑÊ∫ñÁ¢∫Â∞çÈΩäÊòØ‰∏çÂèØÊàñÁº∫ÁöÑ„ÄÇÂÑòÁÆ°Â≠òÂú®ÂêÑÁ®ÆÁî®ÊñºÂª∫Ê®°Ë°ì‰∏≠Âô®ÂÆòËÆäÂΩ¢ÁöÑÊñπÊ≥ïÔºå‰ΩÜÁ≥ªÁµ±Âú∞Â∞çÈÄô‰∫õÊñπÊ≥ïÈÄ≤Ë°åÂàÜÈ°ûÂíåÁ∏ΩÁµêÁöÑÊñáÁçªÂõûÈ°ß‰ªçÁÑ∂ÂæàÂ∞ë„ÄÇÊú¨Á∂úËø∞Êó®Âú®ÈÄöÈÅéÊèê‰æõÂ∞çÊì¥Â¢ûÂØ¶Â¢ÉÊâãË°ì‰∏≠Ë°ì‰∏≠Âô®ÂÆòËÆäÂΩ¢ÁöÑÂª∫Ê®°ÊñπÊ≥ïÁöÑÂÖ®Èù¢‰∏îÊäÄË°ìÂ∞éÂêëÁöÑÊ¶ÇËø∞‰æÜÂ°´Ë£úÈÄô‰∏ÄÁ©∫ÁôΩ„ÄÇÈÄöÈÅéÁ≥ªÁµ±ÁöÑÊêúÂ∞ãÂíåÁØ©ÈÅ∏ÈÅéÁ®ãÔºåÊú¨Á∂úËø∞Á¥çÂÖ•‰∫Ü 112 ÁØáÂØÜÂàáÁõ∏ÈóúÁöÑË´ñÊñá„ÄÇÈÄöÈÅéÂëàÁèæÂô®ÂÆòËÆäÂΩ¢Âª∫Ê®°ÊñπÊ≥ïÁöÑÁèæÁãÄÂèäÂÖ∂Ëá®Â∫äÊáâÁî®ÔºåÊú¨Á∂úËø∞Êó®Âú®Âä†Ê∑±Â∞ç AR ÂºïÂ∞éÊâãË°ì‰∏≠Âô®ÂÆòËÆäÂΩ¢Âª∫Ê®°ÁöÑÁêÜËß£Ôºå‰∏¶Êé¢Ë®éÊú™‰æÜÈÄ≤Â±ïÁöÑÊΩõÂú®‰∏ªÈ°å„ÄÇ

##### **From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future**
2408.02479v1 by Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen

With the rise of large language models (LLMs), researchers are increasingly
exploring their applications in var ious vertical domains, such as software
engineering. LLMs have achieved remarkable success in areas including code
generation and vulnerability detection. However, they also exhibit numerous
limitations and shortcomings. LLM-based agents, a novel tech nology with the
potential for Artificial General Intelligence (AGI), combine LLMs as the core
for decision-making and action-taking, addressing some of the inherent
limitations of LLMs such as lack of autonomy and self-improvement. Despite
numerous studies and surveys exploring the possibility of using LLMs in
software engineering, it lacks a clear distinction between LLMs and LLM based
agents. It is still in its early stage for a unified standard and benchmarking
to qualify an LLM solution as an LLM-based agent in its domain. In this survey,
we broadly investigate the current practice and solutions for LLMs and
LLM-based agents for software engineering. In particular we summarise six key
topics: requirement engineering, code generation, autonomous decision-making,
software design, test generation, and software maintenance. We review and
differentiate the work of LLMs and LLM-based agents from these six topics,
examining their differences and similarities in tasks, benchmarks, and
evaluation metrics. Finally, we discuss the models and benchmarks used,
providing a comprehensive analysis of their applications and effectiveness in
software engineering. We anticipate this work will shed some lights on pushing
the boundaries of LLM-based agents in software engineering for future research.

ÊëòË¶ÅÔºö<paragraph>Èö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËààËµ∑ÔºåÁ†îÁ©∂‰∫∫Âì°Ê≠£Ë∂ä‰æÜË∂äÊé¢Á¥¢ÂÆÉÂÄëÂú®ÂêÑÁ®ÆÂûÇÁõ¥È†òÂüüÁöÑÊáâÁî®Ôºå‰æãÂ¶ÇËªüÈ´îÂ∑•Á®ã„ÄÇLLM Âú®ÂåÖÊã¨Á®ãÂºèÁ¢ºÁîüÊàêÂíåÊºèÊ¥ûÂÅµÊ∏¨Á≠âÈ†òÂüüÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊàêÂäü„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰πüË°®ÁèæÂá∫Ë®±Â§öÈôêÂà∂ÂíåÁº∫Èªû„ÄÇÂü∫Êñº LLM ÁöÑ‰ª£ÁêÜÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊäÄË°ìÔºåÂÖ∑ÊúâÂÖ∑ÂÇô‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖß (AGI) ÁöÑÊΩõÂäõÔºåÁµêÂêà LLM ‰ΩúÁÇ∫Ê±∫Á≠ñÂà∂ÂÆöÂíåÊé°ÂèñË°åÂãïÁöÑÊ†∏ÂøÉÔºåËß£Ê±∫‰∫Ü LLM ÁöÑ‰∏Ä‰∫õÂõ∫ÊúâÈôêÂà∂Ôºå‰æãÂ¶ÇÁº∫‰πèËá™‰∏ªÊÄßÂíåËá™ÊàëÂÆåÂñÑ„ÄÇÂÑòÁÆ°ÊúâË®±Â§öÁ†îÁ©∂ÂíåË™øÊü•Êé¢Á¥¢Âú®ËªüÈ´îÂ∑•Á®ã‰∏≠‰ΩøÁî® LLM ÁöÑÂèØËÉΩÊÄßÔºå‰ΩÜÂÆÉÁº∫‰πè LLM ÂíåÂü∫Êñº LLM ÁöÑ‰ª£ÁêÜ‰πãÈñìÁöÑÊòéÁ¢∫ÂçÄÂà•„ÄÇÂ∞çÊñºÁµ±‰∏ÄÊ®ôÊ∫ñÂíåÂü∫Ê∫ñ‰ª•‰ΩøÂÖ∂Âüü‰∏≠ÁöÑ LLM Ëß£ÂÜ≥ÊñπÊ°àÊúâË≥áÊ†ºÊàêÁÇ∫Âü∫Êñº LLM ÁöÑ‰ª£ÁêÜÔºåÂÆÉ‰ªçËôïÊñºÊó©ÊúüÈöéÊÆµ„ÄÇÂú®Êú¨Ê¨°Ë™øÊü•‰∏≠ÔºåÊàëÂÄëÂª£Ê≥õË™øÊü•‰∫Ü LLM ÂíåÂü∫Êñº LLM ÁöÑËªüÈ´îÂ∑•Á®ã‰ª£ÁêÜÁöÑÁï∂ÂâçÂØ¶ÂãôÂíåËß£Ê±∫ÊñπÊ°à„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁ∏ΩÁµê‰∫ÜÂÖ≠ÂÄãÈóúÈçµ‰∏ªÈ°åÔºöÈúÄÊ±ÇÂ∑•Á®ã„ÄÅÁ®ãÂºèÁ¢ºÁîüÊàê„ÄÅËá™‰∏ªÊ±∫Á≠ñÂà∂ÂÆö„ÄÅËªüÈ´îË®≠Ë®à„ÄÅÊ∏¨Ë©¶ÁîüÊàêÂíåËªüÈ´îÁ∂≠Ë≠∑„ÄÇÊàëÂÄëÂõûÈ°ß‰∏¶ÂçÄÂàÜ‰∫Ü LLM ÂíåÂü∫Êñº LLM ÁöÑ‰ª£ÁêÜÂú®ÈÄôÂÖ≠ÂÄã‰∏ªÈ°å‰∏≠ÁöÑÂ∑•‰ΩúÔºåÊ™¢Ë¶ñÂÆÉÂÄëÂú®‰ªªÂãô„ÄÅÂü∫Ê∫ñÂíåË©ï‰º∞ÊåáÊ®ô‰∏äÁöÑÂ∑ÆÁï∞ÂíåÁõ∏‰ººÊÄß„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÊâÄ‰ΩøÁî®ÁöÑÊ®°ÂûãÂíåÂü∫Ê∫ñÔºåÂÖ®Èù¢ÂàÜÊûê‰∫ÜÂÆÉÂÄëÂú®ËªüÈ´îÂ∑•Á®ã‰∏≠ÁöÑÊáâÁî®ÂíåÊúâÊïàÊÄß„ÄÇÊàëÂÄëÈ†êÊúüÈÄôÈ†ÖÂ∑•‰ΩúÂ∞áÁÇ∫Êé®ÂãïÂü∫Êñº LLM ÁöÑ‰ª£ÁêÜÂú®ËªüÈ´îÂ∑•Á®ã‰∏≠ÁöÑÁïåÈôêÊèê‰æõ‰∏Ä‰∫õÂïüÁ§∫Ôºå‰ª•Âà©Êú™‰æÜÁöÑÁ†îÁ©∂„ÄÇ</paragraph>

##### **Automatic Voice Identification after Speech Resynthesis using PPG**
2408.02712v1 by Thibault Gaudier, Marie Tahon, Anthony Larcher, Yannick Est√®ve

Speech resynthesis is a generic task for which we want to synthesize audio
with another audio as input, which finds applications for media monitors and
journalists.Among different tasks addressed by speech resynthesis, voice
conversion preserves the linguistic information while modifying the identity of
the speaker, and speech edition preserves the identity of the speaker but some
words are modified.In both cases, we need to disentangle speaker and phonetic
contents in intermediate representations.Phonetic PosteriorGrams (PPG) are a
frame-level probabilistic representation of phonemes, and are usually
considered speaker-independent.This paper presents a PPG-based speech
resynthesis system.A perceptive evaluation assesses that it produces correct
audio quality.Then, we demonstrate that an automatic speaker verification model
is not able to recover the source speaker after re-synthesis with PPG, even
when the model is trained on synthetic data.

ÊëòË¶ÅÔºöË™ûÈü≥ÈáçÂêàÊàêÊòØ‰∏ÄÈ†ÖÈÄöÁî®‰ªªÂãôÔºåÊàëÂÄëÂ∏åÊúõ‰ΩøÁî®ÂÖ∂‰ªñÈü≥Ë®ä‰ΩúÁÇ∫Ëº∏ÂÖ•‰æÜÂêàÊàêÈü≥Ë®äÔºåÈÄôÂú®Â™íÈ´îÁõ£ÊéßÂíåÊñ∞ËÅûË®òËÄÖ‰∏≠ÊúÉÊúâÊâÄÊáâÁî®„ÄÇÂú®Ë™ûÈü≥ÈáçÂêàÊàêËß£Ê±∫ÁöÑ‰∏çÂêå‰ªªÂãô‰∏≠ÔºåË™ûÈü≥ËΩâÊèõÊúÉ‰øùÁïôË™ûË®ÄË≥áË®äÔºåÂêåÊôÇ‰øÆÊîπË™™Ë©±ËÄÖÁöÑË∫´ÂàÜÔºåËÄåË™ûÈü≥Á∑®ËºØÊúÉ‰øùÁïôË™™Ë©±ËÄÖÁöÑË∫´ÂàÜÔºå‰ΩÜÊúÉ‰øÆÊîπ‰∏Ä‰∫õÂñÆÂ≠ó„ÄÇÈÄôÂÖ©Á®ÆÊÉÖÊ≥Å‰∏ãÔºåÊàëÂÄëÈÉΩÈúÄË¶ÅÂú®‰∏≠ÈñìË°®Âæµ‰∏≠Ëß£ÈñãË™™Ë©±ËÄÖÂíåË™ûÈü≥ÂÖßÂÆπ„ÄÇË™ûÈü≥ÂæåÈÉ®Ë™ûÊ≥ïÔºàPPGÔºâÊòØÈü≥Á¥†ÁöÑÊ°ÜÊû∂Â±§Á¥öÊ©üÁéáË°®ÂæµÔºåÈÄöÂ∏∏Ë¢´Ë™çÁÇ∫ËàáË™™Ë©±ËÄÖÁÑ°Èóú„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº PPG ÁöÑË™ûÈü≥ÈáçÂêàÊàêÁ≥ªÁµ±„ÄÇ‰∏ÄÂÄãÁü•Ë¶∫Ë©ï‰º∞Ë©ï‰º∞ÂÆÉÁî¢ÁîüÊ≠£Á¢∫ÁöÑÈü≥Ë®äÂìÅË≥™„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË≠âÊòé‰∫Ü‰∏ÄÂÄãËá™ÂãïË™™Ë©±ËÄÖÈ©óË≠âÊ®°ÂûãÁÑ°Ê≥ïÂú®‰ΩøÁî® PPG ÈáçÊñ∞ÂêàÊàêÂæåÊÅ¢Âæ©‰æÜÊ∫êË™™Ë©±ËÄÖÔºåÂç≥‰ΩøË©≤Ê®°ÂûãÊòØÂú®ÂêàÊàêË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑ„ÄÇ

##### **An investigation into the causes of race bias in AI-based cine CMR segmentation**
2408.02462v1 by Tiarna Lee, Esther Puyol-Anton, Bram Ruijsink, Sebastien Roujol, Theodore Barfoot, Shaheim Ogbomo-Harmitt, Miaojing Shi, Andrew P. King

Artificial intelligence (AI) methods are being used increasingly for the
automated segmentation of cine cardiac magnetic resonance (CMR) imaging.
However, these methods have been shown to be subject to race bias, i.e. they
exhibit different levels of performance for different races depending on the
(im)balance of the data used to train the AI model. In this paper we
investigate the source of this bias, seeking to understand its root cause(s) so
that it can be effectively mitigated. We perform a series of classification and
segmentation experiments on short-axis cine CMR images acquired from Black and
White subjects from the UK Biobank and apply AI interpretability methods to
understand the results. In the classification experiments, we found that race
can be predicted with high accuracy from the images alone, but less accurately
from ground truth segmentations, suggesting that the distributional shift
between races, which is often the cause of AI bias, is mostly image-based
rather than segmentation-based. The interpretability methods showed that most
attention in the classification models was focused on non-heart regions, such
as subcutaneous fat. Cropping the images tightly around the heart reduced
classification accuracy to around chance level. Similarly, race can be
predicted from the latent representations of a biased segmentation model,
suggesting that race information is encoded in the model. Cropping images
tightly around the heart reduced but did not eliminate segmentation bias. We
also investigate the influence of possible confounders on the bias observed.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÊ≠£Êó•ÁõäÁî®ÊñºËá™ÂãïÂàÜÂâ≤ÂøÉËáüÈõªÂΩ±ÂºèÁ£ÅÊåØÈÄ†ÂΩ± (CMR) ÂΩ±ÂÉè„ÄÇ
ÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂ∑≤Ë¢´Ë≠âÊòéÊúÉÂèóÂà∞Á®ÆÊóèÂÅèË¶ãÁöÑÂΩ±ÈüøÔºåÂç≥Ê†πÊìöÁî®ÊñºË®ìÁ∑¥ AI Ê®°ÂûãÁöÑË≥áÊñôÁöÑ (‰∏ç) Âπ≥Ë°°ÔºåÂÆÉÂÄëÂ∞ç‰∏çÂêåÁ®ÆÊóèÂ±ïÁèæÂá∫‰∏çÂêåÂ±§Á¥öÁöÑÊïàËÉΩ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË™øÊü•Ê≠§ÂÅèË¶ãÁöÑ‰æÜÊ∫êÔºåË©¶Âúñ‰∫ÜËß£ÂÖ∂Ê†πÊú¨ÂéüÂõ†Ôºå‰ª•‰æøËÉΩÊúâÊïàÊ∏õËºïÂÆÉ„ÄÇÊàëÂÄëÂ∞ç‰æÜËá™Ëã±ÂúãÁîüÁâ©ÈäÄË°åÁöÑÈªë‰∫∫ÂíåÁôΩ‰∫∫ÂèóË©¶ËÄÖÊâÄÂèñÂæóÁöÑÁü≠Ëª∏ÂøÉËáüÈõªÂΩ±Âºè CMR ÂΩ±ÂÉèÂü∑Ë°å‰∏ÄÁ≥ªÂàóÂàÜÈ°ûÂíåÂàÜÂâ≤ÂØ¶È©óÔºå‰∏¶Â•óÁî® AI ÂèØËß£ÈáãÊÄßÊñπÊ≥ï‰æÜ‰∫ÜËß£ÁµêÊûú„ÄÇÂú®ÂàÜÈ°ûÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëÁôºÁèæÂÉÖÂæûÂΩ±ÂÉèÂ∞±ËÉΩ‰ª•È´òÊ∫ñÁ¢∫Â∫¶È†êÊ∏¨Á®ÆÊóèÔºå‰ΩÜÂæûÁúüÂØ¶ÂàÜÂâ≤ÁöÑÊ∫ñÁ¢∫Â∫¶ËºÉ‰ΩéÔºåÈÄôË°®Á§∫Á®ÆÊóè‰πãÈñìÁöÑÂàÜÈÖçËΩâÁßªÔºàÈÄöÂ∏∏ÊòØ AI ÂÅèË¶ãÁöÑÂéüÂõ†Ôºâ‰∏ªË¶ÅÊòØÂü∫ÊñºÂΩ±ÂÉèÔºåËÄåÈùûÂü∫ÊñºÂàÜÂâ≤„ÄÇÂèØËß£ÈáãÊÄßÊñπÊ≥ïÈ°ØÁ§∫ÔºåÂàÜÈ°ûÊ®°Âûã‰∏≠ÁöÑÂ§ßÈÉ®ÂàÜÊ≥®ÊÑèÂäõÈÉΩÈõÜ‰∏≠Âú®ÈùûÂøÉËáüÂçÄÂüüÔºå‰æãÂ¶ÇÁöÆ‰∏ãËÑÇËÇ™„ÄÇÁ∑äÂØÜË£ÅÂâ™ÂΩ±ÂÉè‰ª•ÂúçÁπûÂøÉËáüÊúÉÂ∞áÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶Èôç‰ΩéËá≥Êé•ËøëÈö®Ê©üÂ±§Á¥ö„ÄÇÂêåÊ®£Âú∞ÔºåÁ®ÆÊóèÂèØ‰ª•ÂæûÊúâÂÅèË¶ãÁöÑÂàÜÂâ≤Ê®°ÂûãÁöÑÊΩõÂú®Ë°®Á§∫‰∏≠È†êÊ∏¨ÔºåÈÄôË°®Á§∫Á®ÆÊóèË≥áË®äÂ∑≤Á∑®Á¢ºÂú®Ê®°Âûã‰∏≠„ÄÇÁ∑äÂØÜË£ÅÂâ™ÂΩ±ÂÉè‰ª•ÂúçÁπûÂøÉËáüÊúÉÈôç‰ΩéÂàÜÂâ≤ÂÅèË¶ãÔºå‰ΩÜÁÑ°Ê≥ïÊ∂àÈô§ÂÆÉ„ÄÇÊàëÂÄë‰πüË™øÊü•ÂèØËÉΩÊ∑∑Ê∑ÜÂõ†Â≠êÂ∞çÊâÄËßÄÂØüÂà∞ÁöÑÂÅèË¶ãÁöÑÂΩ±Èüø„ÄÇ

##### **Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based Approach**
2408.02456v1 by Wanxu Wei, Yitong Song, Bin Yao

Knowledge graphs (KGs) play a vital role in enhancing search results and
recommendation systems. With the rapid increase in the size of the KGs, they
are becoming inaccuracy and incomplete. This problem can be solved by the
knowledge graph completion methods, of which graph attention network
(GAT)-based methods stand out since their superior performance. However,
existing GAT-based knowledge graph completion methods often suffer from
overfitting issues when dealing with heterogeneous knowledge graphs, primarily
due to the unbalanced number of samples. Additionally, these methods
demonstrate poor performance in predicting the tail (head) entity that shares
the same relation and head (tail) entity with others. To solve these problems,
we propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH
incorporates two separate attention network modules that work synergistically
to predict the missing entities. We also introduce novel encoding and feature
transformation approaches, enabling the robust performance of GATH in scenarios
with imbalanced samples. Comprehensive experiments are conducted to evaluate
the GATH's performance. Compared with the existing SOTA GAT-based model on
Hits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the
FB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) Âú®ÊèêÂçáÊêúÂ∞ãÁµêÊûúÂíåÊé®Ëñ¶Á≥ªÁµ±‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÈö®Ëëó KG Ë¶èÊ®°ÁöÑÂø´ÈÄüÂ¢ûÈï∑ÔºåÂÆÉÂÄëÊ≠£ËÆäÂæó‰∏çÊ∫ñÁ¢∫‰∏î‰∏çÂÆåÊï¥„ÄÇÈÄôÂÄãÂïèÈ°åÂèØ‰ª•ÈÄèÈÅéÁü•Ë≠òÂúñË≠úÂÆåÊàêÊñπÊ≥ï‰æÜËß£Ê±∫ÔºåÂÖ∂‰∏≠Âü∫ÊñºÂúñÊ≥®ÊÑèÂäõÁ∂≤Ë∑Ø (GAT) ÁöÑÊñπÊ≥ïÂõ†ÂÖ∂ÂçìË∂äÁöÑÊïàËÉΩËÄåËÑ´Á©éËÄåÂá∫„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂü∫Êñº GAT ÁöÑÁü•Ë≠òÂúñË≠úÂÆåÊàêÊñπÊ≥ïÂú®ËôïÁêÜÁï∞Ë≥™Áü•Ë≠òÂúñË≠úÊôÇÔºåÈÄöÂ∏∏ÊúÉÈÅáÂà∞ÈÅéÂ∫¶Êì¨ÂêàÁöÑÂïèÈ°åÔºåÈÄô‰∏ªË¶ÅÊòØÁî±ÊñºÊ®£Êú¨Êï∏‰∏çÂπ≥Ë°°ÊâÄËá¥„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÊñπÊ≥ïÂú®È†êÊ∏¨Ëàá‰ªñ‰∫∫ÂÖ±‰∫´Áõ∏ÂêåÈóú‰øÇÂíåÈ†≠ÈÉ® (Â∞æÈÉ®) ÂØ¶È´îÁöÑÂ∞æÈÉ® (È†≠ÈÉ®) ÂØ¶È´îÊôÇÔºåË°®Áèæ‰∏ç‰Ω≥„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü GATHÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞àÁÇ∫Áï∞Ë≥™ KG Ë®≠Ë®àÁöÑÂâµÊñ∞ GAT Âü∫Á§éÊñπÊ≥ï„ÄÇGATH ÁµêÂêà‰∫ÜÂÖ©ÂÄãÁç®Á´ãÁöÑÊ≥®ÊÑèÂäõÁ∂≤Ë∑ØÊ®°ÁµÑÔºåÂÆÉÂÄëÂçîÂêåÂ∑•‰Ωú‰ª•È†êÊ∏¨Áº∫Â§±ÁöÑÂØ¶È´î„ÄÇÊàëÂÄëÈÇÑÂºïÂÖ•‰∫ÜÂâµÊñ∞ÁöÑÁ∑®Á¢ºÂíåÁâπÂæµËΩâÊèõÊñπÊ≥ïÔºåËÆì GATH Âú®Ê®£Êú¨‰∏çÂπ≥Ë°°ÁöÑÂ†¥ÊôØ‰∏≠ËÉΩÊúâÁ©©ÂÅ•ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶È©ó‰æÜË©ï‰º∞ GATH ÁöÑÊïàËÉΩ„ÄÇËàáÁèæÊúâÁöÑ SOTA GAT Âü∫Á§éÊ®°ÂûãÂú® Hits@10 Âíå MRR ÊåáÊ®ô‰∏äÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú® FB15K-237 Ë≥áÊñôÈõÜ‰∏äÂàÜÂà•ÊèêÂçá‰∫Ü 5.2% Âíå 5.2% ÁöÑÊïàËÉΩÔºåÂú® WN18RR Ë≥áÊñôÈõÜ‰∏äÂàÜÂà•ÊèêÂçá‰∫Ü 4.5% Âíå 14.6%„ÄÇ

##### **Text Conditioned Symbolic Drumbeat Generation using Latent Diffusion Models**
2408.02711v1 by Pushkar Jajoria, James McDermott

This study introduces a text-conditioned approach to generating drumbeats
with Latent Diffusion Models (LDMs). It uses informative conditioning text
extracted from training data filenames. By pretraining a text and drumbeat
encoder through contrastive learning within a multimodal network, aligned
following CLIP, we align the modalities of text and music closely.
Additionally, we examine an alternative text encoder based on multihot text
encodings. Inspired by musics multi-resolution nature, we propose a novel LSTM
variant, MultiResolutionLSTM, designed to operate at various resolutions
independently. In common with recent LDMs in the image space, it speeds up the
generation process by running diffusion in a latent space provided by a
pretrained unconditional autoencoder. We demonstrate the originality and
variety of the generated drumbeats by measuring distance (both over binary
pianorolls and in the latent space) versus the training dataset and among the
generated drumbeats. We also assess the generated drumbeats through a listening
test focused on questions of quality, aptness for the prompt text, and novelty.
We show that the generated drumbeats are novel and apt to the prompt text, and
comparable in quality to those created by human musicians.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÂºïÂÖ•‰∏ÄÁ®Æ‰ª•ÊñáÂ≠óÁÇ∫Ê¢ù‰ª∂ÁöÑÊñπÊ≥ïÔºåÁî®ÊΩõÂú®Êì¥Êï£Ê®°Âûã (LDM) Áî¢ÁîüÈºìÈªû„ÄÇÂÆÉ‰ΩøÁî®ÂæûË®ìÁ∑¥Ë≥áÊñôÊ™îÂêç‰∏≠ÊèêÂèñÁöÑË≥áË®äÊÄßÊ¢ù‰ª∂ÊñáÂ≠ó„ÄÇÈÄèÈÅéÂú®Â§öÊ®°ÊÖãÁ∂≤Ë∑Ø‰∏≠‰ΩøÁî®Â∞çÊØîÂ≠∏ÁøíÈ†êË®ìÁ∑¥ÊñáÂ≠óÂíåÈºìÈªûÁ∑®Á¢ºÂô®Ôºå‰∏¶ÊåâÁÖß CLIP Â∞çÈΩäÔºåÊàëÂÄëÁ∑äÂØÜÂ∞çÈΩäÊñáÂ≠óÂíåÈü≥Ê®ÇÁöÑÊ®°ÊÖã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∏ÄÁ®ÆÂü∫ÊñºÂ§öÁÜ±ÊñáÂ≠óÁ∑®Á¢ºÁöÑÊõø‰ª£ÊñáÂ≠óÁ∑®Á¢ºÂô®„ÄÇÂèóÈü≥Ê®ÇÁöÑÂ§öËß£ÊûêÂ∫¶ÁâπÊÄßÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑ LSTM ËÆäÈ´î MultiResolutionLSTMÔºåÊó®Âú®Áç®Á´ãÈÅã‰ΩúÊñºÂêÑÁ®ÆËß£ÊûêÂ∫¶„ÄÇËàáÂΩ±ÂÉèÁ©∫Èñì‰∏≠ÊúÄËøëÁöÑ LDM Áõ∏ÂêåÔºåÂÆÉÈÄèÈÅéÂú®È†êË®ìÁ∑¥ÁÑ°Ê¢ù‰ª∂Ëá™ÂãïÁ∑®Á¢ºÂô®Êèê‰æõÁöÑÊΩõÂú®Á©∫Èñì‰∏≠Âü∑Ë°åÊì¥Êï£‰æÜÂä†ÈÄüÁîüÊàêÈÅéÁ®ã„ÄÇÊàëÂÄëÈÄèÈÅéÊ∏¨ÈáèË∑ùÈõ¢ÔºàÂú®‰∫åÈÄ≤Âà∂ÈãºÁê¥Êç≤Ëª∏ÂíåÊΩõÂú®Á©∫Èñì‰∏≠ÔºâÁõ∏Â∞çÊñºË®ìÁ∑¥Ë≥áÊñôÈõÜÂíåÁîüÊàêÁöÑÈºìÈªûÔºå‰æÜË≠âÊòéÁîüÊàêÈºìÈªûÁöÑÁç®ÂâµÊÄßÂíåÂ§öÊ®£ÊÄß„ÄÇÊàëÂÄëÈÇÑÈÄèÈÅéÂ∞àÊ≥®ÊñºÂìÅË≥™„ÄÅÂ∞çÊèêÁ§∫ÊñáÂ≠óÁöÑÈÅ©Áï∂ÊÄßÂíåÊñ∞Á©éÊÄßÁöÑËÅÜËÅΩÊ∏¨Ë©¶Ë©ï‰º∞ÁîüÊàêÁöÑÈºìÈªû„ÄÇÊàëÂÄëÂ±ïÁ§∫ÁîüÊàêÁöÑÈºìÈªûÊñ∞Á©é‰∏îÈÅ©ÊñºÊèêÁ§∫ÊñáÂ≠óÔºå‰∏¶‰∏îÂìÅË≥™Ëàá‰∫∫È°ûÈü≥Ê®ÇÂÆ∂Ââµ‰ΩúÁöÑÈºìÈªûÁõ∏Áï∂„ÄÇ

##### **Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models**
2408.02442v1 by Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, Yun-Nung Chen

Structured generation, the process of producing content in standardized
formats like JSON and XML, is widely utilized in real-world applications to
extract key output information from large language models (LLMs). This study
investigates whether such constraints on generation space impact LLMs'
abilities, including reasoning and domain knowledge comprehension.
Specifically, we evaluate LLMs' performance when restricted to adhere to
structured formats versus generating free-form responses across various common
tasks. Surprisingly, we observe a significant decline in LLMs' reasoning
abilities under format restrictions. Furthermore, we find that stricter format
constraints generally lead to greater performance degradation in reasoning
tasks.

ÊëòË¶ÅÔºöÁµêÊßãÂåñÁîüÊàêÔºåÂç≥‰ª• JSON Âíå XML Á≠âÊ®ôÊ∫ñÂåñÊ†ºÂºèË£Ω‰ΩúÂÖßÂÆπÁöÑÈÅéÁ®ãÔºåÂª£Ê≥õÁî®ÊñºÂØ¶ÈöõÊáâÁî®‰∏≠ÔºåÂæûÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÊèêÂèñÈóúÈçµËº∏Âá∫Ë≥áË®ä„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÊ≠§È°ûÁîüÊàêÁ©∫ÈñìÈôêÂà∂ÊòØÂê¶ÊúÉÂΩ±Èüø LLM ÁöÑËÉΩÂäõÔºåÂåÖÊã¨Êé®ÁêÜÂíåÈ†òÂüüÁü•Ë≠òÁêÜËß£„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË©ï‰º∞ LLM Âú®ÂèóÈôêÊñºÈÅµÂæ™ÁµêÊßãÂåñÊ†ºÂºèËàáÂú®ÂêÑÁ®ÆÂ∏∏Ë¶ã‰ªªÂãô‰∏≠Áî¢ÁîüËá™Áî±ÂΩ¢ÂºèÂõûÊáâÊôÇÁöÑË°®Áèæ„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëËßÄÂØüÂà∞ LLM Âú®Ê†ºÂºèÈôêÂà∂‰∏ãÁöÑÊé®ÁêÜËÉΩÂäõÈ°ØËëó‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁôºÁèæÊõ¥Âö¥Ê†ºÁöÑÊ†ºÂºèÈôêÂà∂ÈÄöÂ∏∏ÊúÉÂ∞éËá¥Êé®ÁêÜ‰ªªÂãôÁöÑÊïàËÉΩÊõ¥Â∑Æ„ÄÇ

##### **Long Input Benchmark for Russian Analysis**
2408.02439v1 by Igor Churin, Murat Apishev, Maria Tikhonova, Denis Shevelev, Aydar Bulatov, Yuri Kuratov, Sergej Averkiev, Alena Fenogenova

Recent advancements in Natural Language Processing (NLP) have fostered the
development of Large Language Models (LLMs) that can solve an immense variety
of tasks. One of the key aspects of their application is their ability to work
with long text documents and to process long sequences of tokens. This has
created a demand for proper evaluation of long-context understanding. To
address this need for the Russian language, we propose LIBRA (Long Input
Benchmark for Russian Analysis), which comprises 21 adapted datasets to study
the LLM's abilities to understand long texts thoroughly. The tests are divided
into four complexity groups and allow the evaluation of models across various
context lengths ranging from 4k up to 128k tokens. We provide the open-source
datasets, codebase, and public leaderboard for LIBRA to guide forthcoming
research.

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ï‰øÉÈÄ≤‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÁôºÂ±ïÔºåÂÆÉËÉΩËß£Ê±∫ÂêÑÁ®ÆÂêÑÊ®£ÁöÑ‰ªªÂãô„ÄÇÂÖ∂ÊáâÁî®ÁöÑ‰∏ÄÂÄãÈóúÈçµÊñπÈù¢ÊòØÂÆÉÂÄëËôïÁêÜÈï∑ÊñáÊú¨Êñá‰ª∂ÂíåËôïÁêÜÈï∑Â∫èÂàó‰ª£Âπ£ÁöÑËÉΩÂäõ„ÄÇÈÄôÂ∞çÈï∑Ë™ûÂ¢ÉÁêÜËß£ÁöÑÈÅ©Áï∂Ë©ï‰º∞Áî¢Áîü‰∫ÜÈúÄÊ±Ç„ÄÇÁÇ∫‰∫ÜÊªøË∂≥‰øÑË™ûÁöÑÈúÄÊ±ÇÔºåÊàëÂÄëÊèêÂá∫‰∫Ü LIBRAÔºà‰øÑË™ûÂàÜÊûêÈï∑Ëº∏ÂÖ•Âü∫Ê∫ñÔºâÔºåÂÆÉÂåÖÂê´ 21 ÂÄãÈÅ©Áî®ÁöÑÊï∏ÊìöÈõÜÔºåÁî®ÊñºÁ†îÁ©∂ LLM ÂÖ®Èù¢ÁêÜËß£Èï∑ÊñáÊú¨ÁöÑËÉΩÂäõ„ÄÇÊ∏¨Ë©¶ÂàÜÁÇ∫ÂõõÂÄãË§áÈõúÂ∫¶ÁµÑÔºå‰∏¶ÂÖÅË®±Ë©ï‰º∞Ê®°ÂûãÂú®Âæû 4k Âà∞ 128k ‰ª£Âπ£ÁöÑÂêÑÁ®ÆË™ûÂ¢ÉÈï∑Â∫¶„ÄÇÊàëÂÄëÊèê‰æõ LIBRA ÁöÑÈñãÊ∫êÊï∏ÊìöÈõÜ„ÄÅ‰ª£Á¢ºÂ∫´ÂíåÂÖ¨ÈñãÊéíË°åÊ¶úÔºå‰ª•ÊåáÂ∞éÂæåÁ∫åÁ†îÁ©∂„ÄÇ

##### **Infusing Emotions into Task-oriented Dialogue Systems: Understanding, Management, and Generation**
2408.02417v1 by Shutong Feng, Hsien-chin Lin, Christian Geishauser, Nurul Lubis, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Renato Vukovic, Milica Ga≈°iƒá

Emotions are indispensable in human communication, but are often overlooked
in task-oriented dialogue (ToD) modelling, where the task success is the
primary focus. While existing works have explored user emotions or similar
concepts in some ToD tasks, none has so far included emotion modelling into a
fully-fledged ToD system nor conducted interaction with human or simulated
users. In this work, we incorporate emotion into the complete ToD processing
loop, involving understanding, management, and generation. To this end, we
extend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour
labels. Through interactive experimentation involving both simulated and human
users, we demonstrate that our proposed framework significantly enhances the
user's emotional experience as well as the task success.

ÊëòË¶ÅÔºöÊÉÖÁ∑íÂú®‰∫∫È°ûÊ∫ùÈÄö‰∏≠‰∏çÂèØÊàñÁº∫Ôºå‰ΩÜÂú®‰ª•‰ªªÂãôÁÇ∫Â∞éÂêëÁöÑÂ∞çË©± (ToD) Âª∫Ê®°‰∏≠ÂçªÁ∂ìÂ∏∏Ë¢´ÂøΩÁï•ÔºåËÄå‰ªªÂãôÊàêÂäüÊòØ‰∏ªË¶ÅÁÑ¶Èªû„ÄÇÈõñÁÑ∂ÁèæÊúâÁ†îÁ©∂Â∑≤Êé¢Ë®é‰ΩøÁî®ËÄÖÊÉÖÁ∑íÊàñÈ°û‰ººÊ¶ÇÂøµÂú®Êüê‰∫õ ToD ‰ªªÂãô‰∏≠Ôºå‰ΩÜÁõÆÂâçÂ∞öÊú™Â∞áÊÉÖÁ∑íÂª∫Ê®°Á¥çÂÖ•‰∏ÄÂÄãÊàêÁÜüÁöÑ ToD Á≥ªÁµ±Ôºå‰πüÊú™Ëàá‰∫∫È°ûÊàñÊ®°Êì¨‰ΩøÁî®ËÄÖÈÄ≤Ë°å‰∫íÂãï„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞áÊÉÖÁ∑íÁ¥çÂÖ•ÂÆåÊï¥ÁöÑ ToD ËôïÁêÜËø¥ÂúàÔºåÂåÖÊã¨ÁêÜËß£„ÄÅÁÆ°ÁêÜÂíåÁî¢Áîü„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÈÄèÈÅéÁ≥ªÁµ±ÊÉÖÊÑüË°åÁÇ∫Ê®ôÁ±§Âª∂‰º∏ EmoWOZ Ë≥áÊñôÈõÜ (Feng Á≠â‰∫∫Ôºå2022)„ÄÇÈÄèÈÅéÊ∂âÂèäÊ®°Êì¨Âíå‰∫∫È°û‰ΩøÁî®ËÄÖÁöÑ‰∫íÂãïÂºèÂØ¶È©óÔºåÊàëÂÄëË≠âÊòéÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÂ§ßÂπÖÊèêÂçá‰ΩøÁî®ËÄÖÁöÑÊÉÖÁ∑íÈ´îÈ©ó‰ª•Âèä‰ªªÂãôÊàêÂäü„ÄÇ

##### **Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models**
2408.02416v1 by Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li

The drastic increase of large language models' (LLMs) parameters has led to a
new research direction of fine-tuning-free downstream customization by prompts,
i.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)
play an important role in many businesses, there has emerged growing concerns
about the prompt leakage, which undermines the intellectual properties of these
services and causes downstream attacks. In this paper, we analyze the
underlying mechanism of prompt leakage, which we refer to as prompt
memorization, and develop corresponding defending strategies. By exploring the
scaling laws in prompt extraction, we analyze key attributes that influence
prompt extraction, including model sizes, prompt lengths, as well as the types
of prompts. Then we propose two hypotheses that explain how LLMs expose their
prompts. The first is attributed to the perplexity, i.e. the familiarity of
LLMs to texts, whereas the second is based on the straightforward token
translation path in attention matrices. To defend against such threats, we
investigate whether alignments can undermine the extraction of prompts. We find
that current LLMs, even those with safety alignments like GPT-4, are highly
vulnerable to prompt extraction attacks, even under the most straightforward
user attacks. Therefore, we put forward several defense strategies with the
inspiration of our findings, which achieve 83.8\% and 71.0\% drop in the prompt
extraction rate for Llama2-7B and GPT-3.5, respectively. Source code is
avaliable at \url{https://github.com/liangzid/PromptExtractionEval}.

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂèÉÊï∏ÁöÑÊÄ•ÂäáÂ¢ûÂä†ÔºåÂ∞éËá¥‰∫ÜÊèêÁ§∫Á¨¶ÈÄ≤Ë°åÂæÆË™øËá™Áî±‰∏ãÊ∏∏Ëá™Ë®ÇÁöÑÊñ∞Á†îÁ©∂ÊñπÂêëÔºåÂç≥‰ªªÂãôÊèèËø∞„ÄÇÈõñÁÑ∂ÈÄô‰∫õÂü∫ÊñºÊèêÁ§∫Á¨¶ÁöÑÊúçÂãôÔºà‰æãÂ¶Ç OpenAI ÁöÑ GPTÔºâÂú®Ë®±Â§ö‰ºÅÊ•≠‰∏≠ÊâÆÊºîËëóÈáçË¶ÅÁöÑËßíËâ≤Ôºå‰ΩÜÂ∞çÊñºÊèêÁ§∫Á¨¶Ê¥©ÊºèÁöÑÊìîÊÜÇËàáÊó•‰ø±Â¢ûÔºåÈÄôÊúÉÁ†¥Â£ûÈÄô‰∫õÊúçÂãôÁöÑÊô∫ÊÖßË≤°Áî¢Ê¨ä‰∏¶Â∞éËá¥‰∏ãÊ∏∏ÊîªÊìä„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂàÜÊûê‰∫ÜÊèêÁ§∫Á¨¶Ê¥©ÊºèÁöÑÂ∫ïÂ±§Ê©üÂà∂ÔºåÊàëÂÄëÁ®±‰πãÁÇ∫ÊèêÁ§∫Á¨¶Ë®òÊÜ∂Ôºå‰∏¶Âà∂ÂÆö‰∫ÜÁõ∏ÊáâÁöÑÈò≤Á¶¶Á≠ñÁï•„ÄÇÈÄöÈÅéÊé¢Á¥¢ÊèêÁ§∫Á¨¶ÊèêÂèñ‰∏≠ÁöÑË¶èÊ®°ÂÆöÂæãÔºåÊàëÂÄëÂàÜÊûê‰∫ÜÂΩ±ÈüøÊèêÁ§∫Á¨¶ÊèêÂèñÁöÑÈóúÈçµÂ±¨ÊÄßÔºåÂåÖÊã¨Ê®°ÂûãÂ§ßÂ∞è„ÄÅÊèêÁ§∫Á¨¶Èï∑Â∫¶‰ª•ÂèäÊèêÁ§∫Á¨¶È°ûÂûã„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂÖ©ÂÄãÂÅáË®≠‰æÜËß£Èáã LLM Â¶Ç‰ΩïÂÖ¨ÈñãÂÖ∂ÊèêÁ§∫Á¨¶„ÄÇÁ¨¨‰∏ÄÂÄãÊ≠∏Âõ†ÊñºÂõ∞ÊÉëÔºåÂç≥ LLM Â∞çÊñáÊú¨ÁöÑÁÜüÊÇâÂ∫¶ÔºåËÄåÁ¨¨‰∫åÂÄãÂâáÂü∫ÊñºÊ≥®ÊÑèÂäõÁü©Èô£‰∏≠ÁöÑÁõ¥Êé•‰ª£Âπ£ËΩâÊèõË∑ØÂæë„ÄÇÁÇ∫‰∫ÜÈò≤Á¶¶Ê≠§È°ûÂ®ÅËÑÖÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÂ∞çÈΩäÊòØÂê¶ÊúÉÁ†¥Â£ûÊèêÁ§∫Á¨¶ÁöÑÊèêÂèñ„ÄÇÊàëÂÄëÁôºÁèæÁï∂ÂâçÁöÑ LLMÔºåÂç≥‰ΩøÊòØÈÇ£‰∫õÂÖ∑ÊúâÂÆâÂÖ®Â∞çÈΩäÂäüËÉΩÔºà‰æãÂ¶Ç GPT-4ÔºâÁöÑ LLMÔºå‰πüÊ•µÊòìÂèóÂà∞ÊèêÁ§∫Á¨¶ÊèêÂèñÊîªÊìäÔºåÂç≥‰ΩøÊòØÂú®ÊúÄÁõ¥Êé•ÁöÑ‰ΩøÁî®ËÄÖÊîªÊìä‰∏ã‰πüÊòØÂ¶ÇÊ≠§„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊ†πÊìöÊàëÂÄëÁöÑÁôºÁèæÊèêÂá∫‰∫ÜÂπæÁ®ÆÈò≤Á¶¶Á≠ñÁï•ÔºåÈÄô‰∫õÁ≠ñÁï•ÂàÜÂà•ÁÇ∫ Llama2-7B Âíå GPT-3.5 ÁöÑÊèêÁ§∫Á¨¶ÊèêÂèñÁéáÈôç‰Ωé‰∫Ü 83.8% Âíå 71.0%„ÄÇÂéüÂßãÁ¢ºÂèØÊñº\url{https://github.com/liangzid/PromptExtractionEval}ÂèñÂæó„ÄÇ</paragraph>

##### **Multi-weather Cross-view Geo-localization Using Denoising Diffusion Models**
2408.02408v1 by Tongtong Feng, Qing Li, Xin Wang, Mingzi Wang, Guangyao Li, Wenwu Zhu

Cross-view geo-localization in GNSS-denied environments aims to determine an
unknown location by matching drone-view images with the correct geo-tagged
satellite-view images from a large gallery. Recent research shows that learning
discriminative image representations under specific weather conditions can
significantly enhance performance. However, the frequent occurrence of unseen
extreme weather conditions hinders progress. This paper introduces MCGF, a
Multi-weather Cross-view Geo-localization Framework designed to dynamically
adapt to unseen weather conditions. MCGF establishes a joint optimization
between image restoration and geo-localization using denoising diffusion
models. For image restoration, MCGF incorporates a shared encoder and a
lightweight restoration module to help the backbone eliminate weather-specific
information. For geo-localization, MCGF uses EVA-02 as a backbone for feature
extraction, with cross-entropy loss for training and cosine distance for
testing. Extensive experiments on University160k-WX demonstrate that MCGF
achieves competitive results for geo-localization in varying weather
conditions.

ÊëòË¶ÅÔºöÂú® GNSS ÊãíÁµïÁöÑÁí∞Â¢É‰∏≠ÈÄ≤Ë°åË∑®Ë¶ñÂúñÂú∞ÁêÜÂÆö‰ΩçÊó®Âú®ÈÄöÈÅéÂ∞áÁÑ°‰∫∫Ê©üË¶ñÂúñÂΩ±ÂÉèËàá‰æÜËá™Â§ßÂûãÂúñÂ∫´ÁöÑÊ≠£Á¢∫Âú∞ÁêÜÊ®ôË®òË°õÊòüË¶ñÂúñÂΩ±ÂÉèÈÄ≤Ë°åÂåπÈÖç‰æÜÁ¢∫ÂÆöÊú™Áü•‰ΩçÁΩÆ„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂú®ÁâπÂÆöÂ§©Ê∞£Ê¢ù‰ª∂‰∏ãÂ≠∏ÁøíÂà§Âà•ÂΩ±ÂÉèË°®ÂæµÂèØ‰ª•È°ØËëóÊèêÈ´òÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÊ•µÁ´ØÂ§©Ê∞£Ê¢ù‰ª∂ÁöÑÈ†ªÁπÅÂá∫ÁèæÈòªÁ§ô‰∫ÜÈÄ≤Â±ï„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü MCGFÔºå‰∏ÄÂÄãÂ§öÂ§©Ê∞£Ë∑®Ë¶ñÂúñÂú∞ÁêÜÂÆö‰ΩçÊ°ÜÊû∂ÔºåÊó®Âú®ÂãïÊÖãÈÅ©ÊáâÊú™Áü•Â§©Ê∞£Ê¢ù‰ª∂„ÄÇMCGF Âú®ÂΩ±ÂÉè‰øÆÂæ©ÂíåÂú∞ÁêÜÂÆö‰Ωç‰πãÈñìÂª∫Á´ã‰∫ÜËÅØÂêàÂÑ™ÂåñÔºå‰ΩøÁî®ÂéªÂô™Êì¥Êï£Ê®°Âûã„ÄÇÂ∞çÊñºÂΩ±ÂÉè‰øÆÂæ©ÔºåMCGF ÁµêÂêà‰∫Ü‰∏ÄÂÄãÂÖ±‰∫´Á∑®Á¢ºÂô®Âíå‰∏ÄÂÄãËºïÈáèÁ¥ö‰øÆÂæ©Ê®°ÁµÑÔºå‰ª•Âπ´Âä©‰∏ªÂππÊ∂àÈô§ÁâπÂÆöÂ§©Ê∞£Ë≥áË®ä„ÄÇÂ∞çÊñºÂú∞ÁêÜÂÆö‰ΩçÔºåMCGF ‰ΩøÁî® EVA-02 ‰ΩúÁÇ∫ÁâπÂæµÊèêÂèñÁöÑ‰∏ªÂππÔºå‰ΩøÁî®‰∫§ÂèâÁÜµÊêçÂ§±ÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰ΩøÁî®È§òÂº¶Ë∑ùÈõ¢ÈÄ≤Ë°åÊ∏¨Ë©¶„ÄÇÂú® University160k-WX ‰∏äÈÄ≤Ë°åÁöÑÂ§ßÈáèÂØ¶È©óË°®ÊòéÔºåMCGF Âú®‰∏çÂêåÁöÑÂ§©Ê∞£Ê¢ù‰ª∂‰∏ãÂØ¶Áèæ‰∫ÜÂú∞ÁêÜÂÆö‰ΩçÁöÑÁ´∂Áà≠ÁµêÊûú„ÄÇ

##### **SnapE -- Training Snapshot Ensembles of Link Prediction Models**
2408.02707v1 by Ali Shaban, Heiko Paulheim

Snapshot ensembles have been widely used in various fields of prediction.
They allow for training an ensemble of prediction models at the cost of
training a single one. They are known to yield more robust predictions by
creating a set of diverse base models. In this paper, we introduce an approach
to transfer the idea of snapshot ensembles to link prediction models in
knowledge graphs. Moreover, since link prediction in knowledge graphs is a
setup without explicit negative examples, we propose a novel training loop that
iteratively creates negative examples using previous snapshot models. An
evaluation with four base models across four datasets shows that this approach
constantly outperforms the single model approach, while keeping the training
time constant.

ÊëòË¶ÅÔºöÂø´ÁÖßÂêàÂ•èÂ∑≤ÂπøÊ≥õÁî®‰∫éÂêÑÁßçÈ¢ÑÊµãÈ¢ÜÂüü„ÄÇ
ÂÆÉ‰ª¨ÂÖÅËÆ∏‰ª•ËÆ≠ÁªÉÂçï‰∏™Ê®°ÂûãÁöÑÊàêÊú¨ËÆ≠ÁªÉÈ¢ÑÊµãÊ®°ÂûãÂêàÂ•è„ÄÇ
‰ºóÊâÄÂë®Áü•ÔºåÂÆÉ‰ª¨ÈÄöËøáÂàõÂª∫‰∏ÄÁªÑ‰∏çÂêåÁöÑÂü∫Á°ÄÊ®°ÂûãÊù•‰∫ßÁîüÊõ¥Á®≥ÂÅ•ÁöÑÈ¢ÑÊµã„ÄÇ
Âú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨‰ªãÁªç‰∫Ü‰∏ÄÁßçÂ∞ÜÂø´ÁÖßÂêàÂ•èÊÄùÊÉ≥ËΩ¨ÁßªÂà∞Áü•ËØÜÂõæË∞±‰∏≠ÈìæÊé•È¢ÑÊµãÊ®°ÂûãÁöÑÊñπÊ≥ï„ÄÇ
Ê≠§Â§ñÔºåÁî±‰∫éÁü•ËØÜÂõæË∞±‰∏≠ÁöÑÈìæÊé•È¢ÑÊµãÊòØ‰∏Ä‰∏™Ê≤°ÊúâÊòéÁ°ÆË¥üÈù¢Á§∫‰æãÁöÑËÆæÁΩÆÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞È¢ñÁöÑËÆ≠ÁªÉÂæ™ÁéØÔºå
ÂÆÉ‰ΩøÁî®‰ª•ÂâçÁöÑÂø´ÁÖßÊ®°ÂûãËø≠‰ª£ÂàõÂª∫Ë¥üÈù¢Á§∫‰æã„ÄÇ
ÂØπÂõõ‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂõõ‰∏™Âü∫Á°ÄÊ®°ÂûãÁöÑËØÑ‰º∞Ë°®ÊòéÔºåËøôÁßçÊñπÊ≥ïÂßãÁªà‰ºò‰∫éÂçï‰∏ÄÊ®°ÂûãÊñπÊ≥ïÔºåÂêåÊó∂‰øùÊåÅËÆ≠ÁªÉÊó∂Èó¥‰∏çÂèò„ÄÇ

##### **Enhancing AI-based Generation of Software Exploits with Contextual Information**
2408.02402v2 by Pietro Liguori, Cristina Improta, Roberto Natella, Bojan Cukic, Domenico Cotroneo

This practical experience report explores Neural Machine Translation (NMT)
models' capability to generate offensive security code from natural language
(NL) descriptions, highlighting the significance of contextual understanding
and its impact on model performance. Our study employs a dataset comprising
real shellcodes to evaluate the models across various scenarios, including
missing information, necessary context, and unnecessary context. The
experiments are designed to assess the models' resilience against incomplete
descriptions, their proficiency in leveraging context for enhanced accuracy,
and their ability to discern irrelevant information. The findings reveal that
the introduction of contextual data significantly improves performance.
However, the benefits of additional context diminish beyond a certain point,
indicating an optimal level of contextual information for model training.
Moreover, the models demonstrate an ability to filter out unnecessary context,
maintaining high levels of accuracy in the generation of offensive security
code. This study paves the way for future research on optimizing context use in
AI-driven code generation, particularly for applications requiring a high
degree of technical precision such as the generation of offensive code.

ÊëòË¶ÅÔºöÈÄô‰ªΩÂØ¶ÂãôÁ∂ìÈ©óÂ†±ÂëäÊé¢Ë®é‰∫ÜÁ•ûÁ∂ìÊ©üÂô®ÁøªË≠Ø (NMT)
Ê®°ÂûãÂæûËá™ÁÑ∂Ë™ûË®Ä (NL) ÊèèËø∞Áî¢ÁîüÊîªÊìäÊÄßÂÆâÂÖ®Á®ãÂºèÁ¢ºÁöÑËÉΩÂäõÔºåÂº∑Ë™ø‰∫ÜËÑàÁµ°ÁêÜËß£ÁöÑÈáçË¶ÅÊÄß
ÂèäÂÖ∂Â∞çÊ®°ÂûãÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Êé°Áî®ÂåÖÂê´
ÁúüÂØ¶ shellcode ÁöÑË≥áÊñôÈõÜÔºå‰ª•Ë©ï‰º∞Ê®°ÂûãÂú®ÂêÑÁ®ÆÂ†¥ÊôØ‰∏≠ÁöÑË°®ÁèæÔºåÂåÖÊã¨
ÈÅ∫ÊºèË≥áË®ä„ÄÅÂøÖË¶ÅÁöÑËÑàÁµ°Âíå‰∏çÂøÖË¶ÅÁöÑËÑàÁµ°„ÄÇÈÄô‰∫õ
ÂØ¶È©óÊó®Âú®Ë©ï‰º∞Ê®°ÂûãÂ∞ç‰∏çÂÆåÊï¥ÊèèËø∞ÁöÑÈüåÊÄß„ÄÅÂÆÉÂÄëÂú®Âà©Áî®ËÑàÁµ°‰ª•ÊèêÈ´òÊ∫ñÁ¢∫Â∫¶ÊñπÈù¢ÁöÑÁÜüÁ∑¥Á®ãÂ∫¶Ôºå
‰ª•ÂèäÂÆÉÂÄëËæ®Âà•‰∏çÁõ∏ÈóúË≥áË®äÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫
ËÑàÁµ°Ë≥áÊñôÁöÑÂºïÂÖ•È°ØËëóÊèêÂçá‰∫ÜÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÈ°çÂ§ñËÑàÁµ°ÁöÑÂ•ΩËôïÂú®ÊüêÂÄãÈªû‰πãÂæåÊúÉÈÅûÊ∏õÔºå
ÊåáÂá∫Ê®°ÂûãË®ìÁ∑¥ÊúâÊúÄ‰Ω≥ËÑàÁµ°Ë≥áË®äÈáè„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÊ®°ÂûãÂ±ïÁèæ‰∫ÜÈÅéÊøæ‰∏çÂøÖË¶ÅËÑàÁµ°ÁöÑËÉΩÂäõÔºå
Âú®Áî¢ÁîüÊîªÊìäÊÄßÂÆâÂÖ®Á®ãÂºèÁ¢ºÊôÇÁ∂≠ÊåÅÈ´òÊ∫ñÁ¢∫Â∫¶„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁÇ∫Êú™‰æÜÂú® AI È©ÖÂãïÁ®ãÂºèÁ¢ºÁî¢Áîü‰∏≠ÊúÄ‰Ω≥ÂåñËÑàÁµ°‰ΩøÁî®Èã™Ë∑ØÔºåÁâπÂà•ÊòØÂ∞çÊñºÈúÄË¶ÅÈ´ò
ÊäÄË°ìÁ≤æÊ∫ñÂ∫¶ÁöÑÊáâÁî®Ôºå‰æãÂ¶ÇÁî¢ÁîüÊîªÊìäÊÄßÁ®ãÂºèÁ¢º„ÄÇ

##### **A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models**
2408.02377v1 by Vanni Zavarella, Juan Carlos Gamero-Salinas, Sergio Consoli

Knowledge graphs (KGs) have been successfully applied to the analysis of
complex scientific and technological domains, with automatic KG generation
methods typically building upon relation extraction models capturing
fine-grained relations between domain entities in text. While these relations
are fully applicable across scientific areas, existing models are trained on
few domain-specific datasets such as SciERC and do not perform well on new
target domains. In this paper, we experiment with leveraging in-context
learning capabilities of Large Language Models to perform schema-constrained
data annotation, collecting in-domain training instances for a
Transformer-based relation extraction model deployed on titles and abstracts of
research papers in the Architecture, Construction, Engineering and Operations
(AECO) domain. By assessing the performance gain with respect to a baseline
Deep Learning architecture trained on off-domain data, we show that by using a
few-shot learning strategy with structured prompts and only minimal expert
annotation the presented approach can potentially support domain adaptation of
a science KG generation model.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) Â∑≤ÊàêÂäüÊáâÁî®ÊñºÂàÜÊûêË§áÈõúÁöÑÁßëÂ≠∏ÊäÄË°ìÈ†òÂüüÔºåËá™Âãï KG ÁîüÊàêÊñπÊ≥ïÈÄöÂ∏∏Âª∫ÊßãÊñºÈóú‰øÇËêÉÂèñÊ®°Âûã‰∏äÔºåÊçïÊçâÊñáÊú¨‰∏≠È†òÂüüÂØ¶È´î‰πãÈñìÁöÑÁ¥∞Á≤íÂ∫¶Èóú‰øÇ„ÄÇÈõñÁÑ∂ÈÄô‰∫õÈóú‰øÇÂÆåÂÖ®ÈÅ©Áî®ÊñºÂêÑÁßëÂ≠∏È†òÂüüÔºå‰ΩÜÁèæÊúâÊ®°ÂûãÊòØÁî® SciERC Á≠âÂ∞ëÊï∏ÁâπÂÆöÈ†òÂüüÁöÑË≥áÊñôÈõÜË®ìÁ∑¥ÔºåËÄå‰∏îÂú®Êñ∞ÁõÆÊ®ôÈ†òÂüüÁöÑË°®Áèæ‰∏ç‰Ω≥„ÄÇÂú®Êú¨Ë´ñÊñá‰∏≠ÔºåÊàëÂÄëÂòóË©¶Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑËÑàÁµ°Â≠∏ÁøíËÉΩÂäõÔºåÂü∑Ë°åÂèóÊû∂ÊßãÁ¥ÑÊùüÁöÑË≥áÊñôÊ®ôË®ªÔºåÊî∂ÈõÜÈ†òÂüüÂÖßË®ìÁ∑¥ÂØ¶‰æãÔºåÁî®ÊñºÈÉ®ÁΩ≤Âú®Âª∫ÁØâ„ÄÅÁáüÈÄ†„ÄÅÂ∑•Á®ãÂíåÁáüÈÅã (AECO) È†òÂüüÁ†îÁ©∂Ë´ñÊñáÊ®ôÈ°åÂíåÊëòË¶ÅÁöÑÂü∫Êñº Transformer ÁöÑÈóú‰øÇËêÉÂèñÊ®°Âûã„ÄÇÈÄèÈÅéË©ï‰º∞Áõ∏Â∞çÊñºÂú®È†òÂüüÂ§ñË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÂü∫Ê∫ñÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÁöÑÊïàËÉΩÊèêÂçáÔºåÊàëÂÄëÂ±ïÁ§∫ÈÄèÈÅé‰ΩøÁî®Â∏∂ÊúâÁµêÊßãÂåñÊèêÁ§∫ÁöÑÂ∞ëÈáèÂ≠∏ÁøíÁ≠ñÁï•Ôºå‰ª•ÂèäÂÉÖÊúÄÂ∞ëÁöÑÂ∞àÂÆ∂Ê®ôË®ªÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊúâÂèØËÉΩÊîØÊè¥ÁßëÂ≠∏ KG ÁîüÊàêÊ®°ÂûãÁöÑÈ†òÂüüÈÅ©Êáâ„ÄÇ

##### **Operationalizing Contextual Integrity in Privacy-Conscious Assistants**
2408.02373v1 by Sahra Ghalebikesabi, Eugene Bagdasaryan, Ren Yi, Itay Yona, Ilia Shumailov, Aneesh Pappu, Chongyang Shi, Laura Weidinger, Robert Stanforth, Leonard Berrada, Pushmeet Kohli, Po-Sen Huang, Borja Balle

Advanced AI assistants combine frontier LLMs and tool access to autonomously
perform complex tasks on behalf of users. While the helpfulness of such
assistants can increase dramatically with access to user information including
emails and documents, this raises privacy concerns about assistants sharing
inappropriate information with third parties without user supervision. To steer
information-sharing assistants to behave in accordance with privacy
expectations, we propose to operationalize $\textit{contextual integrity}$
(CI), a framework that equates privacy with the appropriate flow of information
in a given context. In particular, we design and evaluate a number of
strategies to steer assistants' information-sharing actions to be CI compliant.
Our evaluation is based on a novel form filling benchmark composed of synthetic
data and human annotations, and it reveals that prompting frontier LLMs to
perform CI-based reasoning yields strong results.

ÊëòË¶ÅÔºöÈÄ≤Èöé‰∫∫Â∑•Êô∫ÊÖßÂä©ÁêÜÁµêÂêàÂâçÊ≤ø LLM ËàáÂ∑•ÂÖ∑Â≠òÂèñÔºå‰ª•Ëá™ÂãïÂåñÂü∑Ë°åË§áÈõú‰ªªÂãôÔºå‰ª£Ë°®‰ΩøÁî®ËÄÖÂü∑Ë°å„ÄÇÈõñÁÑ∂Ê≠§È°ûÂä©ÁêÜÁöÑ‰æøÂà©ÊÄßÊúÉÈö®ËëóÂ≠òÂèñ‰ΩøÁî®ËÄÖË≥áË®äÔºàÂåÖÊã¨ÈõªÂ≠êÈÉµ‰ª∂ÂíåÊñá‰ª∂ÔºâËÄåÂ§ßÂπÖÊèêÂçáÔºå‰ΩÜÈÄô‰πüÂºïÁôº‰∫ÜÈö±ÁßÅÁñëÊÖÆÔºåÊìîÂøÉÂä©ÁêÜÊúÉÂú®Êú™Á∂ì‰ΩøÁî®ËÄÖÁõ£Áù£ÁöÑÊÉÖÊ≥Å‰∏ãÔºåËàáÁ¨¨‰∏âÊñπÂàÜ‰∫´‰∏çÈÅ©Áï∂ÁöÑË≥áË®ä„ÄÇÁÇ∫‰∫ÜÂºïÂ∞éË≥áË®äÂÖ±‰∫´Âä©ÁêÜÔºå‰ΩøÂÖ∂Ë°åÁÇ∫Á¨¶ÂêàÈö±ÁßÅÊúüÊúõÔºåÊàëÂÄëÊèêË≠∞Â∞á„ÄåËÑàÁµ°ÂÆåÊï¥ÊÄß„ÄçÔºàCIÔºâÊìç‰ΩúÂåñÔºåÊ≠§Êû∂ÊßãÂ∞áÈö±ÁßÅËàáÁâπÂÆöËÑàÁµ°‰∏≠ÈÅ©Áï∂ÁöÑË≥áË®äÊµÅÁï´‰∏äÁ≠âËôü„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëË®≠Ë®à‰∏¶Ë©ï‰º∞Â§öÁ®ÆÁ≠ñÁï•Ôºå‰ª•ÂºïÂ∞éÂä©ÁêÜÁöÑË≥áË®äÂÖ±‰∫´Ë°åÁÇ∫ÔºåÁ¨¶Âêà CI Ë¶èÁØÑ„ÄÇÊàëÂÄëÁöÑË©ï‰º∞ÊòØÂü∫ÊñºÁî±ÂêàÊàêË≥áÊñôÂíå‰∫∫ÁÇ∫Ë®ªËß£ÁµÑÊàêÁöÑÊñ∞Á©éË°®ÂñÆÂ°´ÂØ´Âü∫Ê∫ñÔºåÁµêÊûúÈ°ØÁ§∫ÊèêÁ§∫ÂâçÊ≤ø LLM Âü∑Ë°åÂü∫Êñº CI ÁöÑÊé®ÁêÜÔºåÂèØÁî¢ÁîüÂº∑ËÄåÊúâÂäõÁöÑÊàêÊûú„ÄÇ

