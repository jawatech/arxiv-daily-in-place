{"2412.15622": {"publish_time": "2024-12-20", "title": "TouchASP: Elastic Automatic Speech Perception that Everyone Can Touch", "paper_summary": "Large Automatic Speech Recognition (ASR) models demand a vast number of\nparameters, copious amounts of data, and significant computational resources\nduring the training process. However, such models can merely be deployed on\nhigh-compute cloud platforms and are only capable of performing speech\nrecognition tasks. This leads to high costs and restricted capabilities. In\nthis report, we initially propose the elastic mixture of the expert (eMoE)\nmodel. This model can be trained just once and then be elastically scaled in\naccordance with deployment requirements. Secondly, we devise an unsupervised\ndata creation and validation procedure and gather millions of hours of audio\ndata from diverse domains for training. Using these two techniques, our system\nachieves elastic deployment capabilities while reducing the Character Error\nRate (CER) on the SpeechIO testsets from 4.98\\% to 2.45\\%. Thirdly, our model\nis not only competent in Mandarin speech recognition but also proficient in\nmultilingual, multi-dialect, emotion, gender, and sound event perception. We\nrefer to this as Automatic Speech Perception (ASP), and the perception results\nare presented in the experimental section.", "paper_summary_zh": "\u5927\u578b\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u6a21\u578b\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u9700\u8981\u5927\u91cf\u7684\u53c3\u6578\u3001\u5927\u91cf\u7684\u8cc7\u6599\uff0c\u4ee5\u53ca\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u53ea\u80fd\u90e8\u7f72\u5728\u9ad8\u904b\u7b97\u96f2\u7aef\u5e73\u53f0\u4e0a\uff0c\u800c\u4e14\u53ea\u80fd\u57f7\u884c\u8a9e\u97f3\u8fa8\u8b58\u4efb\u52d9\u3002\u9019\u6703\u5c0e\u81f4\u6210\u672c\u9ad8\u6602\u4e14\u529f\u80fd\u53d7\u9650\u3002\u5728\u672c\u5831\u544a\u4e2d\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u5c08\u5bb6\u5f48\u6027\u6df7\u5408 (eMoE) \u6a21\u578b\u3002\u6b64\u6a21\u578b\u53ef\u4ee5\u8a13\u7df4\u4e00\u6b21\uff0c\u7136\u5f8c\u6839\u64da\u90e8\u7f72\u9700\u6c42\u9032\u884c\u5f48\u6027\u64f4\u5145\u3002\u5176\u6b21\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u975e\u76e3\u7763\u5f0f\u8cc7\u6599\u5efa\u7acb\u548c\u9a57\u8b49\u7a0b\u5e8f\uff0c\u4e26\u5f9e\u4e0d\u540c\u7684\u9818\u57df\u6536\u96c6\u4e86\u6578\u767e\u842c\u5c0f\u6642\u7684\u97f3\u8a0a\u8cc7\u6599\u9032\u884c\u8a13\u7df4\u3002\u4f7f\u7528\u9019\u5169\u7a2e\u6280\u8853\uff0c\u6211\u5011\u7684\u7cfb\u7d71\u5728\u964d\u4f4e SpeechIO \u6e2c\u8a66\u96c6\u7684\u5b57\u5143\u932f\u8aa4\u7387 (CER) \u5f9e 4.98% \u5230 2.45% \u7684\u540c\u6642\uff0c\u5be6\u73fe\u4e86\u5f48\u6027\u90e8\u7f72\u529f\u80fd\u3002\u7b2c\u4e09\uff0c\u6211\u5011\u7684\u6a21\u578b\u4e0d\u50c5\u52dd\u4efb\u666e\u901a\u8a71\u8a9e\u97f3\u8fa8\u8b58\uff0c\u800c\u4e14\u7cbe\u901a\u591a\u8a9e\u8a00\u3001\u591a\u65b9\u8a00\u3001\u60c5\u7dd2\u3001\u6027\u5225\u548c\u8072\u97f3\u4e8b\u4ef6\u611f\u77e5\u3002\u6211\u5011\u5c07\u5176\u7a31\u70ba\u81ea\u52d5\u8a9e\u97f3\u611f\u77e5 (ASP)\uff0c\u4e26\u5728\u5be6\u9a57\u90e8\u5206\u5448\u73fe\u611f\u77e5\u7d50\u679c\u3002", "author": "Xingchen Song et.al.", "authors": "Xingchen Song, Chengdong Liang, Binbin Zhang, Pengshen Zhang, ZiYu Wang, Youcheng Ma, Menglong Xu, Lin Wang, Di Wu, Fuping Pan, Dinghao Zhou, Zhendong Peng", "id": "2412.15622v1", "paper_url": "http://arxiv.org/abs/2412.15622v1", "repo": "null"}}