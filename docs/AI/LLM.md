
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-17**|**AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs**|Basel Mousi et.al.|[2409.11404v1](http://arxiv.org/abs/2409.11404v1)|null|
|**2024-09-17**|**NVLM: Open Frontier-Class Multimodal LLMs**|Wenliang Dai et.al.|[2409.11402v1](http://arxiv.org/abs/2409.11402v1)|null|
|**2024-09-17**|**LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents**|Amine B. Hassouna et.al.|[2409.11393v1](http://arxiv.org/abs/2409.11393v1)|null|
|**2024-09-17**|**Says Who? Effective Zero-Shot Annotation of Focalization**|Rebecca M. M. Hicke et.al.|[2409.11390v1](http://arxiv.org/abs/2409.11390v1)|null|
|**2024-09-17**|**Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement**|Simon Yu et.al.|[2409.11378v1](http://arxiv.org/abs/2409.11378v1)|null|
|**2024-09-17**|**Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**|Fatema-E- Jannat et.al.|[2409.11375v1](http://arxiv.org/abs/2409.11375v1)|null|
|**2024-09-17**|**CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration**|Jiahui Gao et.al.|[2409.11365v1](http://arxiv.org/abs/2409.11365v1)|null|
|**2024-09-17**|**CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark**|Zachary S. Siegel et.al.|[2409.11363v1](http://arxiv.org/abs/2409.11363v1)|[link](https://github.com/siegelz/core-bench)|
|**2024-09-17**|**AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances**|Dhruv Agarwal et.al.|[2409.11360v1](http://arxiv.org/abs/2409.11360v1)|null|
|**2024-09-17**|**RenderWorld: World Model with Self-Supervised 3D Label**|Ziyang Yan et.al.|[2409.11356v1](http://arxiv.org/abs/2409.11356v1)|null|
|**2024-09-17**|**THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models**|Mengfei Liang et.al.|[2409.11353v1](http://arxiv.org/abs/2409.11353v1)|null|
|**2024-09-17**|**Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry**|Lauren M. Zuromski et.al.|[2409.11350v1](http://arxiv.org/abs/2409.11350v1)|null|
|**2024-09-17**|**OmniGen: Unified Image Generation**|Shitao Xiao et.al.|[2409.11340v1](http://arxiv.org/abs/2409.11340v1)|[link](https://github.com/vectorspacelab/omnigen)|
|**2024-09-17**|**SOAP: Improving and Stabilizing Shampoo using Adam**|Nikhil Vyas et.al.|[2409.11321v1](http://arxiv.org/abs/2409.11321v1)|[link](https://github.com/nikhilvyas/soap)|
|**2024-09-17**|**MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping**|Amirreza Fateh et.al.|[2409.11316v1](http://arxiv.org/abs/2409.11316v1)|[link](https://github.com/amirrezafateh/msdnet)|
|**2024-09-17**|**SpMis: An Investigation of Synthetic Spoken Misinformation Detection**|Peizhuo Liu et.al.|[2409.11308v1](http://arxiv.org/abs/2409.11308v1)|null|
|**2024-09-17**|**TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation**|Rong Zhou et.al.|[2409.11299v1](http://arxiv.org/abs/2409.11299v1)|null|
|**2024-09-17**|**EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage**|Zeyi Liao et.al.|[2409.11295v1](http://arxiv.org/abs/2409.11295v1)|null|
|**2024-09-17**|**Navigating Process Mining: A Case study using pm4py**|Ali Jlidi et.al.|[2409.11294v1](http://arxiv.org/abs/2409.11294v1)|null|
|**2024-09-17**|**Neural Networks for Vehicle Routing Problem**|László Kovács et.al.|[2409.11290v1](http://arxiv.org/abs/2409.11290v1)|null|
|**2024-09-17**|**Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling**|Xinyue Fang et.al.|[2409.11283v2](http://arxiv.org/abs/2409.11283v2)|null|
|**2024-09-17**|**Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5**|Marcel Lamott et.al.|[2409.11282v1](http://arxiv.org/abs/2409.11282v1)|null|
|**2024-09-17**|**P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task**|Weiye Xu et.al.|[2409.11279v1](http://arxiv.org/abs/2409.11279v1)|null|
|**2024-09-17**|**Machine Learning and Theory Ladenness -- A Phenomenological Account**|Alberto Termine et.al.|[2409.11277v1](http://arxiv.org/abs/2409.11277v1)|null|
|**2024-09-17**|**Task Arithmetic for Language Expansion in Speech Translation**|Yao-Fei Cheng et.al.|[2409.11274v1](http://arxiv.org/abs/2409.11274v1)|null|
|**2024-09-17**|**LOLA -- An Open-Source Massively Multilingual Large Language Model**|Nikit Srivastava et.al.|[2409.11272v2](http://arxiv.org/abs/2409.11272v2)|[link](https://github.com/dice-group/lola)|
|**2024-09-17**|**Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids**|Caio Fabio Oliveira da Silva et.al.|[2409.11267v1](http://arxiv.org/abs/2409.11267v1)|null|
|**2024-09-17**|**Bio-Inspired Mamba: Temporal Locality and Bioplausible Learning in Selective State Space Models**|Jiahao Qin et.al.|[2409.11263v1](http://arxiv.org/abs/2409.11263v1)|null|
|**2024-09-17**|**The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound Event Detection**|Gabriel Bibbó et.al.|[2409.11262v1](http://arxiv.org/abs/2409.11262v1)|[link](https://github.com/gbibbo/voice_anonymization)|
|**2024-09-17**|**The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives**|Samee Arif et.al.|[2409.11261v2](http://arxiv.org/abs/2409.11261v2)|[link](https://github.com/ulrs0/The-Art-of-Story-Telling)|
|**2024-09-17**|**Attacking Slicing Network via Side-channel Reinforcement Learning Attack**|Wei Shao et.al.|[2409.11258v1](http://arxiv.org/abs/2409.11258v1)|null|
|**2024-09-17**|**Norm of Mean Contextualized Embeddings Determines their Variance**|Hiroaki Yamagiwa et.al.|[2409.11253v1](http://arxiv.org/abs/2409.11253v1)|null|
|**2024-09-17**|**WER We Stand: Benchmarking Urdu ASR Models**|Samee Arif et.al.|[2409.11252v1](http://arxiv.org/abs/2409.11252v1)|null|
|**2024-09-17**|**Linear Recency Bias During Training Improves Transformers' Fit to Reading Times**|Christian Clark et.al.|[2409.11250v1](http://arxiv.org/abs/2409.11250v1)|null|
|**2024-09-17**|**Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse**|Maojia Song et.al.|[2409.11242v1](http://arxiv.org/abs/2409.11242v1)|[link](https://github.com/declare-lab/trust-align)|
|**2024-09-17**|**Spontaneous Informal Speech Dataset for Punctuation Restoration**|Xing Yi Liu et.al.|[2409.11241v1](http://arxiv.org/abs/2409.11241v1)|[link](https://github.com/githubaccountanonymous/pr)|
|**2024-09-17**|**LLM-as-a-Judge & Reward Model: What They Can and Cannot Do**|Guijin Son et.al.|[2409.11239v1](http://arxiv.org/abs/2409.11239v1)|null|
|**2024-09-17**|**Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models**|Bishwash Khanal et.al.|[2409.11233v1](http://arxiv.org/abs/2409.11233v1)|null|
|**2024-09-17**|**Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?**|Raffaele Marino et.al.|[2409.11232v1](http://arxiv.org/abs/2409.11232v1)|[link](https://github.com/raffaelemarino/analysisopenaio1modelksat)|
|**2024-09-17**|**Learning Source Disentanglement in Neural Audio Codec**|Xiaoyu Bie et.al.|[2409.11228v1](http://arxiv.org/abs/2409.11228v1)|null|
|**2024-09-17**|**Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis**|Lingling Xu et.al.|[2409.11218v1](http://arxiv.org/abs/2409.11218v1)|null|
|**2024-09-17**|**Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization**|Jianing Wang et.al.|[2409.11212v1](http://arxiv.org/abs/2409.11212v1)|null|
|**2024-09-17**|**SDP: Spiking Diffusion Policy for Robotic Manipulation with Learnable Channel-Wise Membrane Thresholds**|Zhixing Hou et.al.|[2409.11195v1](http://arxiv.org/abs/2409.11195v1)|null|
|**2024-09-17**|**Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory**|Eunhae Lee et.al.|[2409.11192v1](http://arxiv.org/abs/2409.11192v1)|null|
|**2024-09-17**|**SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as Autonomous Programmer**|Anmol Gautam et.al.|[2409.11190v1](http://arxiv.org/abs/2409.11190v1)|null|
|**2024-09-17**|**Identifying Influential nodes in Brain Networks via Self-Supervised Graph-Transformer**|Yanqing Kang et.al.|[2409.11174v1](http://arxiv.org/abs/2409.11174v1)|null|
|**2024-09-17**|**SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with Customisable Fairness Calibration**|Xin Guan et.al.|[2409.11149v1](http://arxiv.org/abs/2409.11149v1)|null|
|**2024-09-17**|**Improving the Efficiency of Visually Augmented Language Models**|Paula Ontalvilla et.al.|[2409.11148v1](http://arxiv.org/abs/2409.11148v1)|null|
|**2024-09-17**|**Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**|Yukang Lin et.al.|[2409.11147v1](http://arxiv.org/abs/2409.11147v1)|[link](https://github.com/yukang-lin/rger)|
|**2024-09-17**|**High-Resolution Speech Restoration with Latent Diffusion Model**|Tushar Dhyani et.al.|[2409.11145v1](http://arxiv.org/abs/2409.11145v1)|null|
|**2024-09-17**|**Semformer: Transformer Language Models with Semantic Planning**|Yongjing Yin et.al.|[2409.11143v1](http://arxiv.org/abs/2409.11143v1)|null|
|**2024-09-17**|**Learning Generalized Hamiltonians using fully Symplectic Mappings**|Harsh Choudhary et.al.|[2409.11138v1](http://arxiv.org/abs/2409.11138v1)|null|
|**2024-09-17**|**Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models**|Orion Weller et.al.|[2409.11136v1](http://arxiv.org/abs/2409.11136v1)|[link](https://github.com/orionw/promptriever)|
|**2024-09-17**|**Gradient-free Post-hoc Explainability Using Distillation Aided Learnable Approach**|Debarpan Bhattacharya et.al.|[2409.11123v1](http://arxiv.org/abs/2409.11123v1)|[link](https://github.com/iiscleap/dax)|
|**2024-09-17**|**Diversity-grounded Channel Prototypical Learning for Out-of-Distribution Intent Detection**|Bo Liu et.al.|[2409.11114v1](http://arxiv.org/abs/2409.11114v1)|null|
|**2024-09-17**|**Strategic Insights in Human and Large Language Model Tactics at Word Guessing Games**|Matīss Rikters et.al.|[2409.11112v1](http://arxiv.org/abs/2409.11112v1)|null|
|**2024-09-17**|**MonoKAN: Certified Monotonic Kolmogorov-Arnold Network**|Alejandro Polo-Molina et.al.|[2409.11078v1](http://arxiv.org/abs/2409.11078v1)|null|
|**2024-09-17**|**RoMath: A Mathematical Reasoning Benchmark in Romanian**|Adrian Cosma et.al.|[2409.11074v1](http://arxiv.org/abs/2409.11074v1)|[link](https://github.com/cosmaadrian/romath)|
|**2024-09-17**|**Improve Machine Learning carbon footprint using Parquet dataset format and Mixed Precision training for regression algorithms**|Andrew Antonopoulos et.al.|[2409.11071v1](http://arxiv.org/abs/2409.11071v1)|null|
|**2024-09-17**|**KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models**|Bo Lv et.al.|[2409.11057v1](http://arxiv.org/abs/2409.11057v1)|null|
|**2024-09-17**|**Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts**|Teng Wang et.al.|[2409.11056v1](http://arxiv.org/abs/2409.11056v1)|null|
|**2024-09-17**|**A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B**|Jemin Lee et.al.|[2409.11055v1](http://arxiv.org/abs/2409.11055v1)|null|
|**2024-09-17**|**Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming**|Chalamalasetti Kranti et.al.|[2409.11041v2](http://arxiv.org/abs/2409.11041v2)|null|
|**2024-09-17**|**Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI**|Riona Matsuoka et.al.|[2409.11032v1](http://arxiv.org/abs/2409.11032v1)|null|
|**2024-09-17**|**D2Vformer: A Flexible Time Series Prediction Model Based on Time Position Embedding**|Xiaobao Song et.al.|[2409.11024v1](http://arxiv.org/abs/2409.11024v1)|null|
|**2024-09-17**|**GEIC: Universal and Multilingual Named Entity Recognition with Large Language Models**|Hanjun Luo et.al.|[2409.11022v2](http://arxiv.org/abs/2409.11022v2)|null|
|**2024-09-17**|**Enhanced segmentation of femoral bone metastasis in CT scans of patients using synthetic data generation with 3D diffusion models**|Emile Saillard et.al.|[2409.11011v1](http://arxiv.org/abs/2409.11011v1)|null|
|**2024-09-17**|**CAST: Cross-modal Alignment Similarity Test for Vision Language Models**|Gautier Dagan et.al.|[2409.11007v1](http://arxiv.org/abs/2409.11007v1)|[link](https://github.com/gautierdag/cast)|
|**2024-09-17**|**Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation**|Gerard I. Gállego et.al.|[2409.11003v1](http://arxiv.org/abs/2409.11003v1)|null|
|**2024-09-17**|**Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models**|Potsawee Manakul et.al.|[2409.10999v1](http://arxiv.org/abs/2409.10999v1)|null|
|**2024-09-17**|**Contextual Breach: Assessing the Robustness of Transformer-based QA Models**|Asir Saadat et.al.|[2409.10997v2](http://arxiv.org/abs/2409.10997v2)|null|
|**2024-09-17**|**Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs**|Dingjie Song et.al.|[2409.10994v1](http://arxiv.org/abs/2409.10994v1)|null|
|**2024-09-17**|**Control-flow Reconstruction Attacks on Business Process Models**|Henrik Kirchmann et.al.|[2409.10986v1](http://arxiv.org/abs/2409.10986v1)|[link](https://github.com/henrikkirchmann/control-flow-reconstruction)|
|**2024-09-17**|**Improving Speech Emotion Recognition in Under-Resourced Languages via Speech-to-Speech Translation with Bootstrapping Data Selection**|Hsi-Che Lin et.al.|[2409.10985v1](http://arxiv.org/abs/2409.10985v1)|null|
|**2024-09-17**|**Enhancing Multilingual Speech Generation and Recognition Abilities in LLMs with Constructed Code-switched Data**|Jing Xu et.al.|[2409.10969v1](http://arxiv.org/abs/2409.10969v1)|null|
|**2024-09-17**|**Cross-lingual transfer of multilingual models on low resource African Languages**|Harish Thangaraj et.al.|[2409.10965v1](http://arxiv.org/abs/2409.10965v1)|null|
|**2024-09-17**|**Active learning for energy-based antibody optimization and enhanced screening**|Kairi Furui et.al.|[2409.10964v2](http://arxiv.org/abs/2409.10964v2)|null|
|**2024-09-17**|**Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning**|Min-Yeong Park et.al.|[2409.10956v1](http://arxiv.org/abs/2409.10956v1)|[link](https://github.com/khu-agi/vil)|
|**2024-09-17**|**Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style**|Yuepei Li et.al.|[2409.10955v1](http://arxiv.org/abs/2409.10955v1)|null|
|**2024-09-17**|**Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach**|Mehroush Banday et.al.|[2409.10932v1](http://arxiv.org/abs/2409.10932v1)|null|
|**2024-09-17**|**Propulsion: Steering LLM with Tiny Fine-Tuning**|Md Kowsher et.al.|[2409.10927v2](http://arxiv.org/abs/2409.10927v2)|[link](https://github.com/Kowsher/Propulsion)|
|**2024-09-17**|**KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**|Yanbei Jiang et.al.|[2409.10921v1](http://arxiv.org/abs/2409.10921v1)|[link](https://github.com/yanbei-jiang/artwork-interpretation)|
|**2024-09-17**|**GenCRF: Generative Clustering and Reformulation Framework for Enhanced Intent-Driven Information Retrieval**|Wonduk Seo et.al.|[2409.10909v1](http://arxiv.org/abs/2409.10909v1)|null|
|**2024-09-17**|**Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised Keyphrase Extraction**|Erwin D. López Z. et.al.|[2409.10907v1](http://arxiv.org/abs/2409.10907v1)|null|
|**2024-09-17**|**WaterQualityNeT: Prediction of Seasonal Water Quality of Nepal Using Hybrid Deep Learning Models**|Biplov Paneru et.al.|[2409.10898v1](http://arxiv.org/abs/2409.10898v1)|null|
|**2024-09-17**|**Shaking the Fake: Detecting Deepfake Videos in Real Time via Active Probes**|Zhixin Xie et.al.|[2409.10889v1](http://arxiv.org/abs/2409.10889v1)|null|
|**2024-09-17**|**CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation for Meeting Summarization**|Ziwei Gong et.al.|[2409.10883v1](http://arxiv.org/abs/2409.10883v1)|null|
|**2024-09-17**|**American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM**|Gregorius Guntur Sunardi Putra et.al.|[2409.10874v1](http://arxiv.org/abs/2409.10874v1)|null|
|**2024-09-17**|**Adaptive Large Language Models By Layerwise Attention Shortcuts**|Prateek Verma et.al.|[2409.10870v1](http://arxiv.org/abs/2409.10870v1)|null|
|**2024-09-17**|**SIFToM: Robust Spoken Instruction Following through Theory of Mind**|Lance Ying et.al.|[2409.10849v1](http://arxiv.org/abs/2409.10849v1)|null|
|**2024-09-17**|**3DFacePolicy: Speech-Driven 3D Facial Animation with Diffusion Policy**|Xuanmeng Sha et.al.|[2409.10848v1](http://arxiv.org/abs/2409.10848v1)|null|
|**2024-09-17**|**BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation**|S. Rohollah Hosseyni et.al.|[2409.10847v1](http://arxiv.org/abs/2409.10847v1)|null|
|**2024-09-17**|**PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing**|Phillip Long et.al.|[2409.10831v1](http://arxiv.org/abs/2409.10831v1)|null|
|**2024-09-17**|**ReXErr: Synthesizing Clinically Meaningful Errors in Diagnostic Radiology Reports**|Vishwanatha M. Rao et.al.|[2409.10829v1](http://arxiv.org/abs/2409.10829v1)|null|
|**2024-09-17**|**Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based Recommendations**|Shahnewaz Karim Sakib et.al.|[2409.10825v1](http://arxiv.org/abs/2409.10825v1)|null|
|**2024-09-16**|**Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering**|Qingru Zhang et.al.|[2409.10790v1](http://arxiv.org/abs/2409.10790v1)|null|
|**2024-09-16**|**Predicting Punctuation in Ancient Chinese Texts: A Multi-Layered LSTM and Attention-Based Approach**|Tracy Cai et.al.|[2409.10783v1](http://arxiv.org/abs/2409.10783v1)|null|
|**2024-09-16**|**Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?**|Kaleb Kassaw et.al.|[2409.10775v1](http://arxiv.org/abs/2409.10775v1)|null|
|**2024-09-16**|**Semantics Preserving Emoji Recommendation with Large Language Models**|Zhongyi Qiu et.al.|[2409.10760v1](http://arxiv.org/abs/2409.10760v1)|null|
|**2024-09-16**|**VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching**|Arastoo Zibaeirad et.al.|[2409.10756v1](http://arxiv.org/abs/2409.10756v1)|null|

#### Abstracts
##### **AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs**
2409.11404v1 by Basel Mousi, Nadir Durrani, Fatema Ahmad, Md. Arid Hasan, Maram Hasanain, Tameem Kabbani, Fahim Dalvi, Shammur Absar Chowdhury, Firoj Alam

Arabic, with its rich diversity of dialects, remains significantly
underrepresented in Large Language Models, particularly in dialectal
variations. We address this gap by introducing seven synthetic datasets in
dialects alongside Modern Standard Arabic (MSA), created using Machine
Translation (MT) combined with human post-editing. We present AraDiCE, a
benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on
dialect comprehension and generation, focusing specifically on low-resource
Arabic dialects. Additionally, we introduce the first-ever fine-grained
benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and
Levant regions, providing a novel dimension to LLM evaluation. Our findings
demonstrate that while Arabic-specific models like Jais and AceGPT outperform
multilingual models on dialectal tasks, significant challenges persist in
dialect identification, generation, and translation. This work contributes ~45K
post-edited samples, a cultural benchmark, and highlights the importance of
tailored training to improve LLM performance in capturing the nuances of
diverse Arabic dialects and cultural contexts. We will release the dialectal
translation models and benchmarks curated in this study.

摘要：阿拉伯語擁有豐富多樣的方言，在大型語言模型中仍然顯著不足，特別是在方言變體中。我們通過在方言中引入七個合成數據集以及現代標準阿拉伯語 (MSA) 來解決這個差距，這些數據集是使用機器翻譯 (MT) 結合人工後編輯創建的。我們展示了 AraDiCE，這是阿拉伯方言和文化評估的基準。我們評估了 LLM 對方言理解和生成的表現，特別關注資源較少的阿拉伯方言。此外，我們還引入了有史以來第一個細粒度基準，旨在評估海灣、埃及和黎凡特地區的文化意識，為 LLM 評估提供了新的維度。我們的研究結果表明，儘管像 Jais 和 AceGPT 這樣的阿拉伯語特定模型在方言任務上優於多語言模型，但在方言識別、生成和翻譯中仍然存在重大挑戰。這項工作貢獻了約 45K 個後編輯樣本、一個文化基準，並強調了定制訓練對於提高 LLM 在捕捉不同阿拉伯方言和文化背景的細微差別方面的性能的重要性。我們將發布本研究中策劃的方言翻譯模型和基準。

##### **NVLM: Open Frontier-Class Multimodal LLMs**
2409.11402v1 by Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping

We introduce NVLM 1.0, a family of frontier-class multimodal large language
models (LLMs) that achieve state-of-the-art results on vision-language tasks,
rivaling the leading proprietary models (e.g., GPT-4o) and open-access models
(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved
text-only performance over its LLM backbone after multimodal training. In terms
of model design, we perform a comprehensive comparison between decoder-only
multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,
Flamingo). Based on the strengths and weaknesses of both approaches, we propose
a novel architecture that enhances both training efficiency and multimodal
reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for
tile-based dynamic high-resolution images, which significantly boosts
performance on multimodal reasoning and OCR-related tasks. Regarding training
data, we meticulously curate and provide detailed information on our multimodal
pretraining and supervised fine-tuning datasets. Our findings indicate that
dataset quality and task diversity are more important than scale, even during
the pretraining phase, across all architectures. Notably, we develop
production-grade multimodality for the NVLM-1.0 models, enabling them to excel
in vision-language tasks while maintaining and even improving text-only
performance compared to their LLM backbones. To achieve this, we craft and
integrate a high-quality text-only dataset into multimodal training, alongside
a substantial amount of multimodal math and reasoning data, leading to enhanced
math and coding capabilities across modalities. To advance research in the
field, we are releasing the model weights and will open-source the code for the
community: https://nvlm-project.github.io/.

摘要：<paragraph>我們推出 NVLM 1.0，這是一個前沿級多模態大型語言模型 (LLM) 家族，在視覺語言任務上取得了最先進的成果，與領先的專有模型（例如 GPT-4o）和開放訪問模型（例如 Llama 3-V 405B 和 InternVL 2）相媲美。值得注意的是，NVLM 1.0 在多模態訓練後，其純文字表現優於其 LLM 主幹。在模型設計方面，我們對僅解碼器多模態 LLM（例如 LLaVA）和基於交叉注意力的模型（例如 Flamingo）進行了全面比較。根據兩種方法的優缺點，我們提出了一種新穎的架構，既能提高訓練效率，又能增強多模態推理能力。此外，我們為基於圖塊的動態高解析度影像引入了 1-D 磁磚標籤設計，這顯著提升了多模態推理和 OCR 相關任務的效能。關於訓練資料，我們精心策劃並提供了有關我們多模態預訓練和監督微調資料集的詳細資訊。我們的研究結果表明，資料集品質和任務多樣性比規模更重要，即使在預訓練階段，也適用於所有架構。值得注意的是，我們為 NVLM-1.0 模型開發了生產級多模態，讓它們能夠在視覺語言任務中表現出色，同時與其 LLM 主幹相比，維持甚至提升純文字表現。為了實現這一點，我們在多模態訓練中製作並整合了一個高品質的純文字資料集，以及大量的多模態數學和推理資料，從而增強了跨模態的數學和編碼能力。為了推動該領域的研究，我們將釋出模型權重，並將為社群開放原始碼：https://nvlm-project.github.io/。</paragraph>

##### **LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents**
2409.11393v1 by Amine B. Hassouna, Hana Chaari, Ines Belhaj

The integration of tools in LLM-based agents overcame the difficulties of
standalone LLMs and traditional agents' limited capabilities. However, the
conjunction of these technologies and the proposed enhancements in several
state-of-the-art works followed a non-unified software architecture resulting
in a lack of modularity. Indeed, they focused mainly on functionalities and
overlooked the definition of the component's boundaries within the agent. This
caused terminological and architectural ambiguities between researchers which
we addressed in this paper by proposing a unified framework that establishes a
clear foundation for LLM-based agents' development from both functional and
software architectural perspectives.
  Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),
clearly distinguishes between the different components of an agent, setting
LLMs, and tools apart from a newly introduced element: the core-agent, playing
the role of the central coordinator of the agent which comprises five modules:
planning, memory, profile, action, and security, the latter often neglected in
previous works. Differences in the internal structure of core-agents led us to
classify them into a taxonomy of passive and active types. Based on this, we
proposed different multi-core agent architectures combining unique
characteristics of various individual agents.
  For evaluation purposes, we applied this framework to a selection of
state-of-the-art agents, thereby demonstrating its alignment with their
functionalities and clarifying the overlooked architectural aspects. Moreover,
we thoroughly assessed four of our proposed architectures by integrating
distinctive agents into hybrid active/passive core-agents' systems. This
analysis provided clear insights into potential improvements and highlighted
the challenges involved in the combination of specific agents.

摘要：<paragraph>整合 LLM 為基礎的代理程式中的工具克服了獨立 LLM 和傳統代理程式功能有限的困難。然而，這些技術的結合和幾個最先進作品中提出的增強功能遵循非統一軟體架構，導致缺乏模組化。事實上，它們主要關注功能，而忽略了代理程式內元件邊界的定義。這導致研究人員之間術語和架構上的歧義，我們在本文中提出了一個統一框架來解決這個問題，該框架為 LLM 為基礎的代理程式的開發建立了一個清晰的基礎，無論是從功能還是軟體架構的角度來看。
我們的框架 LLM-Agent-UMF（LLM 為基礎的代理程式統一建模框架），清楚地區分了代理程式的不同元件，設定 LLM 和工具，除了新引入的元素：核心代理程式，扮演代理程式中央協調者的角色，包含五個模組：規劃、記憶體、設定檔、動作和安全性，後者在之前的作品中經常被忽略。核心代理程式內部結構的差異導致我們將它們分類為被動和主動類型。基於此，我們提出了不同的多核心代理程式架構，結合了各種個別代理程式的獨特特徵。
為了評估目的，我們將這個框架應用於一系列最先進的代理程式，從而證明它與它們的功能一致，並釐清被忽略的架構方面。此外，我們透過將獨特的代理程式整合到混合主動/被動核心代理程式系統中，徹底評估了我們提出的四種架構。這個分析提供了對潛在改進的清晰見解，並強調了結合特定代理程式所涉及的挑戰。</paragraph>

##### **Says Who? Effective Zero-Shot Annotation of Focalization**
2409.11390v1 by Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan

Focalization, the perspective through which narrative is presented, is
encoded via a wide range of lexico-grammatical features and is subject to
reader interpretation. Moreover, trained readers regularly disagree on
interpretations, suggesting that this problem may be computationally
intractable. In this paper, we provide experiments to test how well
contemporary Large Language Models (LLMs) perform when annotating literary
texts for focalization mode. Despite the challenging nature of the task, LLMs
show comparable performance to trained human annotators in our experiments. We
provide a case study working with the novels of Stephen King to demonstrate the
usefulness of this approach for computational literary studies, illustrating
how focalization can be studied at scale.

摘要：焦點化，敘事呈現的視角，是透過廣泛的詞彙語法特徵編碼，並受到讀者詮釋的影響。此外，受過訓練的讀者經常對詮釋有分歧，這表明這個問題在計算上可能是難以解決的。在本文中，我們提供實驗來測試當代大型語言模型 (LLM) 在為文學文本標註焦點化模式時的表現。儘管任務的挑戰性，LLM 在我們的實驗中顯示出與受過訓練的人類標註員相當的表現。我們提供了一個案例研究，使用史蒂芬金的小說來展示這種方法對計算文學研究的有用性，說明焦點化如何大規模地被研究。

##### **Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement**
2409.11378v1 by Simon Yu, Liangyu Chen, Sara Ahmadian, Marzieh Fadaee

Finetuning large language models on instruction data is crucial for enhancing
pre-trained knowledge and improving instruction-following capabilities. As
instruction datasets proliferate, selecting optimal data for effective training
becomes increasingly important. This work addresses the question: How can we
determine the optimal subset of data for effective training? While existing
research often emphasizes local criteria like instance quality for subset
selection, we argue that a global approach focused on data diversity is more
critical. Our method employs k-means clustering to ensure the selected subset
effectively represents the full dataset. We propose an iterative refinement
method inspired by active learning techniques to resample instances from
clusters, reassessing each cluster's importance and sampling weight in every
training iteration. This approach reduces the effect of outliers and
automatically filters out clusters containing low-quality data. Through
extensive evaluation across natural language reasoning, general world
knowledge, code and math reasoning tasks, and by fine-tuning models from
various families, we observe consistent improvements, achieving a 7% increase
over random selection and a 3.8% improvement over state-of-the-art sampling
methods. Our work highlights the significance of diversity-first sampling when
finetuning LLMs to enhance performance across a broad array of evaluation
tasks. Our code is available at
https://github.com/for-ai/iterative-data-selection.

摘要：微調大型語言模型的指令資料對於增強預訓練知識和改進指令遵循能力至關重要。隨著指令資料集的激增，選擇最佳資料以進行有效訓練變得越來越重要。這項工作解決了以下問題：我們如何確定最佳資料子集以進行有效訓練？雖然現有的研究通常強調子集選擇的局部標準，例如例項品質，但我們認為專注於資料多樣性的整體方法更為關鍵。我們的模型採用 k 均值聚類法，以確保所選子集有效代表整個資料集。我們提出了一種受主動學習技術啟發的迭代改進方法，從群集中重新抽取例項，在每次訓練迭代中重新評估每個群集的重要性及抽樣權重。這種方法降低了異常值的效果，並自動過濾掉包含低品質資料的群集。透過自然語言推理、一般世界知識、程式碼和數學推理任務的廣泛評估，以及微調來自不同系列的模型，我們觀察到了一致的改進，隨機選擇增加了 7%，最先進的抽樣方法改進了 3.8%。我們的研究強調了在微調 LLM 以增強廣泛評估任務的效能時，以多樣性為優先的抽樣的顯著性。我們的程式碼可以在 https://github.com/for-ai/iterative-data-selection 取得。

##### **Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification**
2409.11375v1 by Fatema-E- Jannat, Sina Gholami, Jennifer I. Lim, Theodore Leng, Minhaj Nur Alam, Hamed Tabkhi

In the medical domain, acquiring large datasets poses significant challenges
due to privacy concerns. Nonetheless, the development of a robust deep-learning
model for retinal disease diagnosis necessitates a substantial dataset for
training. The capacity to generalize effectively on smaller datasets remains a
persistent challenge. The scarcity of data presents a significant barrier to
the practical implementation of scalable medical AI solutions. To address this
issue, we've combined a wide range of data sources to improve performance and
generalization to new data by giving it a deeper understanding of the data
representation from multi-modal datasets and developed a self-supervised
framework based on large language models (LLMs), SwinV2 to gain a deeper
understanding of multi-modal dataset representations, enhancing the model's
ability to extrapolate to new data for the detection of eye diseases using
optical coherence tomography (OCT) images. We adopt a two-phase training
methodology, self-supervised pre-training, and fine-tuning on a downstream
supervised classifier. An ablation study conducted across three datasets
employing various encoder backbones, without data fusion, with low data
availability setting, and without self-supervised pre-training scenarios,
highlights the robustness of our method. Our findings demonstrate consistent
performance across these diverse conditions, showcasing superior generalization
capabilities compared to the baseline model, ResNet-50.

摘要：在醫療領域，由於隱私問題，獲取大型資料集會造成重大挑戰。儘管如此，對於視網膜疾病診斷的強健深度學習模型的開發需要一個龐大的資料集進行訓練。對較小的資料集進行有效概括的能力仍然是一個持續的挑戰。資料的稀缺性對可擴充醫療 AI 解決方案的實際實施構成重大障礙。為了解決此問題，我們結合了各種資料來源，通過讓其更深入地了解多模式資料集的資料表示，來改善效能和對新資料的概括性，並開發了一個基於大型語言模型 (LLM) 的自監督框架，SwinV2，以更深入地了解多模式資料集表示，增強模型推斷新資料的能力，以使用光學相干斷層掃描 (OCT) 影像偵測眼疾。我們採用兩階段訓練方法，自監督預訓練和對下游監督分類器進行微調。在三個資料集上進行的消融研究，採用各種編碼主幹，沒有資料融合，在資料可用性設定較低的情況下，以及沒有自監督預訓練場景，突出了我們方法的穩健性。我們的研究結果證明了在這些不同條件下的一致效能，與基準模型 ResNet-50 相比，展示了卓越的概括能力。

##### **CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration**
2409.11365v1 by Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, Zhenguo Li

The deployment of multimodal large language models (MLLMs) has demonstrated
remarkable success in engaging in conversations involving visual inputs, thanks
to the superior power of large language models (LLMs). Those MLLMs are
typically built based on the LLMs, with an image encoder to process images into
the token embedding space of the LLMs. However, the integration of visual
modality has introduced a unique vulnerability: the MLLM becomes susceptible to
malicious visual inputs and prone to generating sensitive or harmful responses,
even though the LLM has been trained on textual dataset to align with human
value. In this paper, we first raise the question: ``Do the MLLMs possess
safety-awareness against malicious image inputs?". We find that after adding a
principle that specifies the safety requirement into the input of the MLLM, the
model's safety awareness becomes boosted. This phenomenon verifies the
existence of MLLM's safety-awareness against image inputs, it is only weakened
by the modality gap. We then introduce a simple yet effective technique termed
CoCA, which amplifies the safety-awareness of the MLLM by calibrating its
output distribution. Our proposed strategy helps the model reclaim its original
safety awareness without losing its original capabilities. We verify the
effectiveness of our approach on both multimodal safety and understanding
benchmarks.

摘要：多模态大型语言模型 (MLLM) 的部署已展示出在涉及视觉输入的对话中进行交互的显着成功，这要归功于大型语言模型 (LLM) 的强大功能。这些 MLLM 通常基于 LLM 构建，并使用图像编码器将图像处理到 LLM 的标记嵌入空间中。然而，视觉模态的集成引入了一个独特的漏洞：MLLM 变得容易受到恶意视觉输入的影响，并且容易产生敏感或有害的反应，即使 LLM 已在文本数据集上进行训练以符合人类价值观。在本文中，我们首先提出了这个问题：“MLLM 是否对恶意图像输入具有安全意识？”。我们发现，在将指定安全要求的原则添加到 MLLM 的输入中后，模型的安全意识得到了提升。这种现象验证了 MLLM 对图像输入的安全意识的存在，它只是被模态差距削弱了。然后，我们引入了一种简单但有效的技术，称为 CoCA，它通过校准其输出分布来放大 MLLM 的安全意识。我们提出的策略帮助模型在不丧失其原始能力的情况下恢复其原始安全意识。我们在多模态安全和理解基准上验证了我们方法的有效性。

##### **CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark**
2409.11363v1 by Zachary S. Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan

AI agents have the potential to aid users on a variety of consequential
tasks, including conducting scientific research. To spur the development of
useful agents, we need benchmarks that are challenging, but more crucially,
directly correspond to real-world tasks of interest. This paper introduces such
a benchmark, designed to measure the accuracy of AI agents in tackling a
crucial yet surprisingly challenging aspect of scientific research:
computational reproducibility. This task, fundamental to the scientific
process, involves reproducing the results of a study using the provided code
and data. We introduce CORE-Bench (Computational Reproducibility Agent
Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers
across three disciplines (computer science, social science, and medicine).
Tasks in CORE-Bench consist of three difficulty levels and include both
language-only and vision-language tasks. We provide an evaluation system to
measure the accuracy of agents in a fast and parallelizable way, saving days of
evaluation time for each run compared to a sequential implementation. We
evaluated two baseline agents: the general-purpose AutoGPT and a task-specific
agent called CORE-Agent. We tested both variants using two underlying language
models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% on
the hardest task, showing the vast scope for improvement in automating routine
scientific tasks. Having agents that can reproduce existing work is a necessary
step towards building agents that can conduct novel research and could verify
and improve the performance of other research agents. We hope that CORE-Bench
can improve the state of reproducibility and spur the development of future
research agents.

摘要：人工智慧代理程式有潛力協助使用者執行各種後續任務，包括進行科學研究。為了刺激有用的代理程式開發，我們需要具有挑戰性，但更重要的是，直接對應於感興趣的真實世界任務的基準。本文介紹了一個這樣的基準，旨在衡量人工智慧代理程式在應對科學研究中一個至關重要但令人驚訝地具有挑戰性的方面：計算重現性。此任務對於科學過程至關重要，包括使用提供的程式碼和資料重現研究結果。我們介紹了 CORE-Bench（計算重現性代理程式基準），一個基準，包含 270 個任務，基於三個學科（電腦科學、社會科學和醫學）的 90 篇科學論文。CORE-Bench 中的任務包含三個難度等級，包括僅語言和視覺語言任務。我們提供一個評估系統，以快速且可並行的方式衡量代理程式的準確性，與循序實作相比，每次執行可節省數天的評估時間。我們評估了兩個基準代理程式：通用 AutoGPT 和稱為 CORE-Agent 的特定任務代理程式。我們使用兩個基礎語言模型測試了這兩種變體：GPT-4o 和 GPT-4o-mini。在最困難的任務中，最好的代理程式達到了 21% 的準確度，顯示了在自動化例行科學任務中改進的廣闊範圍。擁有能夠重現現有工作的代理程式是建構能夠進行新研究並驗證和改進其他研究代理程式效能的代理程式的必要步驟。我們希望 CORE-Bench 能夠改善重現性狀態，並刺激未來研究代理程式的開發。

##### **AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances**
2409.11360v1 by Dhruv Agarwal, Mor Naaman, Aditya Vashistha

Large language models (LLMs) are being increasingly integrated into everyday
products and services, such as coding tools and writing assistants. As these
embedded AI applications are deployed globally, there is a growing concern that
the AI models underlying these applications prioritize Western values. This
paper investigates what happens when a Western-centric AI model provides
writing suggestions to users from a different cultural background. We conducted
a cross-cultural controlled experiment with 118 participants from India and the
United States who completed culturally grounded writing tasks with and without
AI suggestions. Our analysis reveals that AI provided greater efficiency gains
for Americans compared to Indians. Moreover, AI suggestions led Indian
participants to adopt Western writing styles, altering not just what is written
but also how it is written. These findings show that Western-centric AI models
homogenize writing toward Western norms, diminishing nuances that differentiate
cultural expression.

摘要：大型語言模型 (LLM) 正逐漸整合到日常產品和服務中，例如編碼工具和寫作助理。隨著這些嵌入式 AI 應用程式在全球部署，愈來愈多人擔心這些應用程式底層的 AI 模型會優先考慮西方價值觀。本文探討以西方為中心的 AI 模型向不同文化背景的使用者提供寫作建議時會發生什麼情況。我們針對來自印度和美國的 118 位參與者進行了一項跨文化對照實驗，他們在有和沒有 AI 建議的情況下完成了以文化為基礎的寫作任務。我們的分析顯示，與印度人相比，AI 為美國人提供了更高的效率提升。此外，AI 建議導致印度參與者採用西方的寫作風格，不僅改變了寫作內容，也改變了寫作方式。這些發現表明，以西方為中心的 AI 模型會將寫作同質化為西方規範，減少區分文化表達的細微差別。

##### **RenderWorld: World Model with Self-Supervised 3D Label**
2409.11356v1 by Ziyang Yan, Wenzhen Dong, Yihua Shao, Yuhang Lu, Liu Haiyang, Jingwen Liu, Haozhe Wang, Zhe Wang, Yan Wang, Fabio Remondino, Yuexin Ma

End-to-end autonomous driving with vision-only is not only more
cost-effective compared to LiDAR-vision fusion but also more reliable than
traditional methods. To achieve a economical and robust purely visual
autonomous driving system, we propose RenderWorld, a vision-only end-to-end
autonomous driving framework, which generates 3D occupancy labels using a
self-supervised gaussian-based Img2Occ Module, then encodes the labels by
AM-VAE, and uses world model for forecasting and planning. RenderWorld employs
Gaussian Splatting to represent 3D scenes and render 2D images greatly improves
segmentation accuracy and reduces GPU memory consumption compared with
NeRF-based methods. By applying AM-VAE to encode air and non-air separately,
RenderWorld achieves more fine-grained scene element representation, leading to
state-of-the-art performance in both 4D occupancy forecasting and motion
planning from autoregressive world model.

摘要：僅使用視覺的端對端自動駕駛不僅比 LiDAR 視覺融合更具成本效益，也比傳統方法更可靠。為了實現經濟且強大的純視覺自動駕駛系統，我們提出了 RenderWorld，一個僅使用視覺的端對端自動駕駛框架，它使用自監督的基於高斯的 Img2Occ 模組生成 3D 佔用標籤，然後透過 AM-VAE 編碼標籤，並使用世界模型進行預測和規劃。RenderWorld 使用高斯噴繪來表示 3D 場景，並渲染 2D 影像，與基於 NeRF 的方法相比，大幅提升分割準確度並減少 GPU 記憶體消耗。透過將 AM-VAE 應用於分別編碼空氣和非空氣，RenderWorld 達到了更細緻的場景元素表示，在自迴歸世界模型中實現了 4D 佔用預測和動作規劃的最新效能。

##### **THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models**
2409.11353v1 by Mengfei Liang, Archish Arun, Zekun Wu, Cristian Munoz, Jonathan Lutch, Emre Kazim, Adriano Koshiyama, Philip Treleaven

Hallucination, the generation of factually incorrect content, is a growing
challenge in Large Language Models (LLMs). Existing detection and mitigation
methods are often isolated and insufficient for domain-specific needs, lacking
a standardized pipeline. This paper introduces THaMES (Tool for Hallucination
Mitigations and EvaluationS), an integrated framework and library addressing
this gap. THaMES offers an end-to-end solution for evaluating and mitigating
hallucinations in LLMs, featuring automated test set generation, multifaceted
benchmarking, and adaptable mitigation strategies. It automates test set
creation from any corpus, ensuring high data quality, diversity, and
cost-efficiency through techniques like batch processing, weighted sampling,
and counterfactual validation. THaMES assesses a model's ability to detect and
reduce hallucinations across various tasks, including text generation and
binary classification, applying optimal mitigation strategies like In-Context
Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient
Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base
of academic papers, political news, and Wikipedia reveal that commercial models
like GPT-4o benefit more from RAG than ICL, while open-weight models like
Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT
significantly enhances the performance of Llama-3.1-8B-Instruct in both
evaluation tasks.

摘要：幻覺，即產生事實上不正確的內容，是大語言模型 (LLM) 中日益嚴峻的挑戰。現有的偵測和緩解方法往往是孤立的，且不足以滿足特定領域的需求，缺乏標準化的管道。本文介紹 THaMES（幻覺緩解和評估工具），一個整合框架和函式庫，用於解決此差距。THaMES 提供了一個端到端的解決方案，用於評估和緩解 LLM 中的幻覺，具備自動化測試集產生、多方面的基準測試和可適應的緩解策略。它自動化了從任何語料庫建立測試集的過程，透過批次處理、加權抽樣和反事實驗證等技術，確保資料的高品質、多樣性和成本效益。THaMES 評估模型在各種任務中偵測和減少幻覺的能力，包括文字產生和二元分類，應用最佳的緩解策略，例如情境學習 (ICL)、檢索增強產生 (RAG) 和參數有效微調 (PEFT)。使用學術論文、政治新聞和維基百科知識庫評估最先進的 LLM，發現像 GPT-4o 等商業模型比 ICL 從 RAG 中受益更多，而像 Llama-3.1-8B-Instruct 和 Mistral-Nemo 等開放權重模型從 ICL 中受益更多。此外，PEFT 在兩個評估任務中都顯著提升了 Llama-3.1-8B-Instruct 的效能。

##### **Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry**
2409.11350v1 by Lauren M. Zuromski, Jacob Durtschi, Aimal Aziz, Jeffrey Chumley, Mark Dewey, Paul English, Muir Morrison, Keith Simmon, Blaine Whipple, Brendan O'Fallon, David P. Ng

Machine-learning (ML) models in flow cytometry have the potential to reduce
error rates, increase reproducibility, and boost the efficiency of clinical
labs. While numerous ML models for flow cytometry data have been proposed, few
studies have described the clinical deployment of such models. Realizing the
potential gains of ML models in clinical labs requires not only an accurate
model, but infrastructure for automated inference, error detection, analytics
and monitoring, and structured data extraction. Here, we describe an ML model
for detection of Acute Myeloid Leukemia (AML), along with the infrastructure
supporting clinical implementation. Our infrastructure leverages the resilience
and scalability of the cloud for model inference, a Kubernetes-based workflow
system that provides model reproducibility and resource management, and a
system for extracting structured diagnoses from full-text reports. We also
describe our model monitoring and visualization platform, an essential element
for ensuring continued model accuracy. Finally, we present a post-deployment
analysis of impacts on turn-around time and compare production accuracy to the
original validation statistics.

摘要：機器學習 (ML) 模型在流式細胞術中具有降低錯誤率、提高可重現性和提升臨床實驗室效率的潛力。雖然已經提出許多用於流式細胞術數據的 ML 模型，但很少有研究描述此類模型的臨床部署。要實現 ML 模型在臨床實驗室中的潛在收益，不僅需要準確的模型，還需要用於自動推理、錯誤檢測、分析和監控以及結構化數據提取的基礎設施。在這裡，我們描述了一個用於檢測急性髓性白血病 (AML) 的 ML 模型，以及支持臨床實施的基礎設施。我們的基礎設施利用雲端的復原力和可擴充性進行模型推理，一個基於 Kubernetes 的工作流程系統提供模型可重現性和資源管理，以及一個從全文報告中提取結構化診斷的系統。我們還描述了我們的模型監控和視覺化平台，這是確保持續模型準確性的基本要素。最後，我們提出了對周轉時間影響的部署後分析，並將生產準確度與原始驗證統計數據進行比較。

##### **OmniGen: Unified Image Generation**
2409.11340v1 by Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, Zheng Liu

In this work, we introduce OmniGen, a new diffusion model for unified image
generation. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen
no longer requires additional modules such as ControlNet or IP-Adapter to
process diverse control conditions. OmniGenis characterized by the following
features: 1) Unification: OmniGen not only demonstrates text-to-image
generation capabilities but also inherently supports other downstream tasks,
such as image editing, subject-driven generation, and visual-conditional
generation. Additionally, OmniGen can handle classical computer vision tasks by
transforming them into image generation tasks, such as edge detection and human
pose recognition. 2) Simplicity: The architecture of OmniGen is highly
simplified, eliminating the need for additional text encoders. Moreover, it is
more user-friendly compared to existing diffusion models, enabling complex
tasks to be accomplished through instructions without the need for extra
preprocessing steps (e.g., human pose estimation), thereby significantly
simplifying the workflow of image generation. 3) Knowledge Transfer: Through
learning in a unified format, OmniGen effectively transfers knowledge across
different tasks, manages unseen tasks and domains, and exhibits novel
capabilities. We also explore the model's reasoning capabilities and potential
applications of chain-of-thought mechanism. This work represents the first
attempt at a general-purpose image generation model, and there remain several
unresolved issues. We will open-source the related resources at
https://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.

摘要：<paragraph>在這項工作中，我們介紹 OmniGen，一種用於統一影像生成的全新擴散模型。與流行的擴散模型（例如 Stable Diffusion）不同，OmniGen 不再需要 ControlNet 或 IP-Adapter 等額外模組來處理不同的控制條件。OmniGen 的特徵如下：1) 統一性：OmniGen 不僅展示了文字轉影像的生成能力，而且本質上也支援其他下游任務，例如影像編輯、主體驅動生成和視覺條件生成。此外，OmniGen 可以透過將它們轉換為影像生成任務來處理經典的電腦視覺任務，例如邊緣偵測和人類姿勢辨識。2) 簡潔性：OmniGen 的架構經過高度簡化，無需額外的文字編碼器。此外，與現有的擴散模型相比，它更使用者友善，能夠透過指令完成複雜的任務，無需額外的預處理步驟（例如人體姿勢估計），從而大幅簡化影像生成的流程。3) 知識轉移：透過以統一的格式學習，OmniGen 有效地將知識轉移到不同的任務中，管理未見的任務和領域，並展現新穎的能力。我們也探討了模型的推理能力和思考鏈機制的潛在應用。這項工作代表了通用影像生成模型的首次嘗試，仍有許多未解決的問題。我們將在 https://github.com/VectorSpaceLab/OmniGen 開放相關資源，以促進此領域的進展。</paragraph>

##### **SOAP: Improving and Stabilizing Shampoo using Adam**
2409.11321v1 by Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham Kakade

There is growing evidence of the effectiveness of Shampoo, a higher-order
preconditioning method, over Adam in deep learning optimization tasks. However,
Shampoo's drawbacks include additional hyperparameters and computational
overhead when compared to Adam, which only updates running averages of first-
and second-moment quantities. This work establishes a formal connection between
Shampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient
approximation of Adam -- showing that Shampoo is equivalent to running
Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to
the design of a simpler and computationally efficient algorithm:
$\textbf{S}$hampo$\textbf{O}$ with $\textbf{A}$dam in the
$\textbf{P}$reconditioner's eigenbasis (SOAP).
  With regards to improving Shampoo's computational efficiency, the most
straightforward approach would be to simply compute Shampoo's
eigendecomposition less frequently. Unfortunately, as our empirical results
show, this leads to performance degradation that worsens with this frequency.
SOAP mitigates this degradation by continually updating the running average of
the second moment, just as Adam does, but in the current (slowly changing)
coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a
rotated space, it introduces only one additional hyperparameter (the
preconditioning frequency) compared to Adam. We empirically evaluate SOAP on
language model pre-training with 360m and 660m sized models. In the large batch
regime, SOAP reduces the number of iterations by over 40% and wall clock time
by over 35% compared to AdamW, with approximately 20% improvements in both
metrics compared to Shampoo. An implementation of SOAP is available at
https://github.com/nikhilvyas/SOAP.

摘要：越來越多的證據顯示，Shampoo 是一種高階預處理方法，在深度學習最佳化任務中優於 Adam。然而，與僅更新一階和二階矩平均值的 Adam 相比，Shampoo 的缺點包括額外的超參數和運算負擔。這項工作在 Shampoo（以 1/2 次方實作）和 Adafactor（一種記憶體效率佳的 Adam 近似值）之間建立正式的關聯，顯示 Shampoo 等於在 Shampoo 預處理器的特徵基底中執行 Adafactor。這個見解引導我們設計一種更簡單且運算效率更高的演算法：$\textbf{S}$hampo$\textbf{O}$ with $\textbf{A}$dam in the $\textbf{P}$reconditioner's eigenbasis (SOAP)。
關於改善 Shampoo 的運算效率，最直接的方法就是減少計算 Shampoo 特徵分解的頻率。不幸的是，正如我們的實證結果顯示，這會導致效能降低，而且頻率越高，效能就會越差。SOAP 透過持續更新二階矩的執行平均值來減輕這種效能降低，就像 Adam 所做的一樣，但使用的是目前的（緩慢變化的）座標基底。此外，由於 SOAP 等於在旋轉空間中執行 Adam，因此與 Adam 相比，它只引入一個額外的超參數（預處理頻率）。我們實證評估 SOAP 在語言模型預訓練中，使用 360m 和 660m 大小的模型。在大批次模式中，與 AdamW 相比，SOAP 將迭代次數減少超過 40%，將實際執行時間減少超過 35%，與 Shampoo 相比，兩項指標皆改善約 20%。SOAP 的實作可於 https://github.com/nikhilvyas/SOAP 取得。

##### **MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping**
2409.11316v1 by Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh

Few-shot Semantic Segmentation addresses the challenge of segmenting objects
in query images with only a handful of annotated examples. However, many
previous state-of-the-art methods either have to discard intricate local
semantic features or suffer from high computational complexity. To address
these challenges, we propose a new Few-shot Semantic Segmentation framework
based on the transformer architecture. Our approach introduces the spatial
transformer decoder and the contextual mask generation module to improve the
relational understanding between support and query images. Moreover, we
introduce a multi-scale decoder to refine the segmentation mask by
incorporating features from different resolutions in a hierarchical manner.
Additionally, our approach integrates global features from intermediate encoder
stages to improve contextual understanding, while maintaining a lightweight
structure to reduce complexity. This balance between performance and efficiency
enables our method to achieve state-of-the-art results on benchmark datasets
such as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings.
Notably, our model with only 1.5 million parameters demonstrates competitive
performance while overcoming limitations of existing methodologies.
https://github.com/amirrezafateh/MSDNet

摘要：小样本语义分割解决了仅用少量标注示例分割查询图像中对象的挑战。然而，许多以前最先进的方法要么必须丢弃复杂的局部语义特征，要么遭受高计算复杂度。为了解决这些挑战，我们提出了一种基于 transformer 架构的新小样本语义分割框架。我们的方法引入了空间 transformer 解码器和上下文掩码生成模块，以改善 support 和 query 图像之间的关系理解。此外，我们引入了多尺度解码器，通过分层方式结合不同分辨率的特征来细化分割掩码。此外，我们的方法集成了来自中间编码器阶段的全局特征，以改善上下文理解，同时保持轻量级结构以降低复杂度。性能和效率之间的这种平衡使我们的方法能够在基准数据集（如 1 次和 5 次设置中的 $PASCAL-5^i$ 和 $COCO-20^i$）上实现最先进的结果。值得注意的是，我们只有 150 万个参数的模型展示了竞争性能，同时克服了现有方法的限制。https://github.com/amirrezafateh/MSDNet

##### **SpMis: An Investigation of Synthetic Spoken Misinformation Detection**
2409.11308v1 by Peizhuo Liu, Li Wang, Renqiang He, Haorui He, Lei Wang, Huadi Zheng, Jie Shi, Tong Xiao, Zhizheng Wu

In recent years, speech generation technology has advanced rapidly, fueled by
generative models and large-scale training techniques. While these developments
have enabled the production of high-quality synthetic speech, they have also
raised concerns about the misuse of this technology, particularly for
generating synthetic misinformation. Current research primarily focuses on
distinguishing machine-generated speech from human-produced speech, but the
more urgent challenge is detecting misinformation within spoken content. This
task requires a thorough analysis of factors such as speaker identity, topic,
and synthesis. To address this need, we conduct an initial investigation into
synthetic spoken misinformation detection by introducing an open-source
dataset, SpMis. SpMis includes speech synthesized from over 1,000 speakers
across five common topics, utilizing state-of-the-art text-to-speech systems.
Although our results show promising detection capabilities, they also reveal
substantial challenges for practical implementation, underscoring the
importance of ongoing research in this critical area.

摘要：近年來，語音生成技術進展神速，推動力來自生成模型與大規模訓練技術。雖然這些發展已能產生高品質的合成語音，但也引發對此技術遭濫用的疑慮，特別是拿來產生合成錯誤資訊。目前的研究主要關注在區分機器產生的語音和人類產生的語音，但更迫切的挑戰是偵測口說內容中的錯誤資訊。這項任務需要徹底分析各種因素，例如說話者身分、主題和合成。為了解決這個需求，我們對合成口說錯誤資訊偵測進行初步調查，並推出一個開放原始碼資料集 SpMis。SpMis 包含來自 1,000 多位說話者、橫跨五個常見主題的合成語音，並利用最先進的文字轉語音系統。雖然我們的結果顯示有前景的偵測能力，但也揭露實際執行的重大挑戰，強調持續研究這個關鍵領域的重要性。

##### **TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation**
2409.11299v1 by Rong Zhou, Zhengqing Yuan, Zhiling Yan, Weixiang Sun, Kai Zhang, Yiwei Li, Yanfang Ye, Xiang Li, Lifang He, Lichao Sun

Biomedical image segmentation is crucial for accurately diagnosing and
analyzing various diseases. However, Convolutional Neural Networks (CNNs) and
Transformers, the most commonly used architectures for this task, struggle to
effectively capture long-range dependencies due to the inherent locality of
CNNs and the computational complexity of Transformers. To address this
limitation, we introduce TTT-Unet, a novel framework that integrates Test-Time
Training (TTT) layers into the traditional U-Net architecture for biomedical
image segmentation. TTT-Unet dynamically adjusts model parameters during the
testing time, enhancing the model's ability to capture both local and
long-range features. We evaluate TTT-Unet on multiple medical imaging datasets,
including 3D abdominal organ segmentation in CT and MR images, instrument
segmentation in endoscopy images, and cell segmentation in microscopy images.
The results demonstrate that TTT-Unet consistently outperforms state-of-the-art
CNN-based and Transformer-based segmentation models across all tasks. The code
is available at https://github.com/rongzhou7/TTT-Unet.

摘要：生物醫學影像分割對於準確診斷和分析各種疾病至關重要。然而，卷積神經網路（CNN）和 Transformer，作為此任務最常用的架構，由於 CNN 的固有局部性和 Transformer 的計算複雜性，難以有效擷取長程依賴性。為了解決這個限制，我們引入了 TTT-Unet，一個創新的框架，將測試時間訓練（TTT）層整合到傳統的 U-Net 架構中，用於生物醫學影像分割。TTT-Unet 在測試時間動態調整模型參數，增強模型擷取局部和長程特徵的能力。我們在多個醫學影像資料集上評估 TTT-Unet，包括電腦斷層掃描和磁振造影中的 3D 腹腔器官分割、內視鏡影像中的儀器分割以及顯微鏡影像中的細胞分割。結果表明，TTT-Unet 在所有任務中都持續優於最先進的基於 CNN 和基於 Transformer 的分割模型。程式碼可在 https://github.com/rongzhou7/TTT-Unet 取得。

##### **EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage**
2409.11295v1 by Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun

Generalist web agents have evolved rapidly and demonstrated remarkable
potential. However, there are unprecedented safety risks associated with these
them, which are nearly unexplored so far. In this work, we aim to narrow this
gap by conducting the first study on the privacy risks of generalist web agents
in adversarial environments. First, we present a threat model that discusses
the adversarial targets, constraints, and attack scenarios. Particularly, we
consider two types of adversarial targets: stealing users' specific personally
identifiable information (PII) or stealing the entire user request. To achieve
these objectives, we propose a novel attack method, termed Environmental
Injection Attack (EIA). This attack injects malicious content designed to adapt
well to different environments where the agents operate, causing them to
perform unintended actions. This work instantiates EIA specifically for the
privacy scenario. It inserts malicious web elements alongside persuasive
instructions that mislead web agents into leaking private information, and can
further leverage CSS and JavaScript features to remain stealthy. We collect 177
actions steps that involve diverse PII categories on realistic websites from
the Mind2Web dataset, and conduct extensive experiments using one of the most
capable generalist web agent frameworks to date, SeeAct. The results
demonstrate that EIA achieves up to 70% ASR in stealing users' specific PII.
Stealing full user requests is more challenging, but a relaxed version of EIA
can still achieve 16% ASR. Despite these concerning results, it is important to
note that the attack can still be detectable through careful human inspection,
highlighting a trade-off between high autonomy and security. This leads to our
detailed discussion on the efficacy of EIA under different levels of human
supervision as well as implications on defenses for generalist web agents.

摘要：<paragraph>通用網路代理快速演化，展現出驚人的潛力。然而，這些代理伴隨著前所未有的安全風險，而這些風險目前幾乎尚未被探索。在這項工作中，我們旨在透過執行通用網路代理在對抗環境中的隱私風險的第一個研究來縮小這個差距。首先，我們提出一個威脅模型，討論對抗目標、限制和攻擊情境。特別是，我們考慮兩種類型的對抗目標：竊取使用者的特定個人可識別資訊 (PII) 或竊取整個使用者要求。為了達成這些目標，我們提出了一種新穎的攻擊方法，稱為環境注入攻擊 (EIA)。此攻擊注入惡意內容，旨在適應代理運作的不同環境，導致代理執行非預期的動作。這項工作特別針對隱私情境實例化 EIA。它在具有說服力的指令旁插入惡意網路元素，誤導網路代理洩露私人資訊，並可進一步利用 CSS 和 JavaScript 功能保持隱密。我們從 Mind2Web 資料集的現實網站收集了 177 個涉及不同 PII 類別的動作步驟，並使用迄今為止功能最強大的通用網路代理框架 SeeAct 進行廣泛的實驗。結果證明，EIA 在竊取使用者的特定 PII 方面達到了 70% 的 ASR。竊取完整的使用者要求更具挑戰性，但 EIA 的放寬版本仍可達到 16% 的 ASR。儘管有這些令人擔憂的結果，但重要的是要注意，攻擊仍然可以透過仔細的人工檢查來偵測，突顯了高度自主性與安全性之間的權衡。這導致我們詳細討論了 EIA 在不同層級的人工監督下的效能，以及對通用網路代理防禦的影響。</paragraph>

##### **Navigating Process Mining: A Case study using pm4py**
2409.11294v1 by Ali Jlidi, László Kovács

Process-mining techniques have emerged as powerful tools for analyzing event
data to gain insights into business processes. In this paper, we present a
comprehensive analysis of road traffic fine management processes using the
pm4py library in Python. We start by importing an event log dataset and explore
its characteristics, including the distribution of activities and process
variants. Through filtering and statistical analysis, we uncover key patterns
and variations in the process executions. Subsequently, we apply various
process-mining algorithms, including the Alpha Miner, Inductive Miner, and
Heuristic Miner, to discover process models from the event log data. We
visualize the discovered models to understand the workflow structures and
dependencies within the process. Additionally, we discuss the strengths and
limitations of each mining approach in capturing the underlying process
dynamics. Our findings shed light on the efficiency and effectiveness of road
traffic fine management processes, providing valuable insights for process
optimization and decision-making. This study demonstrates the utility of pm4py
in facilitating process mining tasks and its potential for analyzing real-world
business processes.

摘要：流程挖掘技術已成為分析事件資料以深入了解業務流程的強大工具。在本文中，我們使用 Python 中的 pm4py 函式庫，對道路交通罰款管理流程進行全面分析。我們首先匯入事件日誌資料集，並探討其特徵，包括活動分佈和流程變體。透過過濾和統計分析，我們揭露了流程執行中的關鍵模式和變異。隨後，我們應用各種流程挖掘演算法，包括 Alpha Miner、Inductive Miner 和 Heuristic Miner，從事件日誌資料中發現流程模型。我們將發現的模型視覺化，以了解流程中的工作流程結構和依賴性。此外，我們討論了每種挖掘方法在捕捉基礎流程動態方面的優缺點。我們的研究結果闡明了道路交通罰款管理流程的效率和效能，為流程最佳化和決策制定提供了有價值的見解。本研究展示了 pm4py 在促進流程挖掘任務和分析現實世界業務流程方面的效用。

##### **Neural Networks for Vehicle Routing Problem**
2409.11290v1 by László Kovács, Ali Jlidi

The Vehicle Routing Problem is about optimizing the routes of vehicles to
meet the needs of customers at specific locations. The route graph consists of
depots on several levels and customer positions. Several optimization methods
have been developed over the years, most of which are based on some type of
classic heuristic: genetic algorithm, simulated annealing, tabu search, ant
colony optimization, firefly algorithm. Recent developments in machine learning
provide a new toolset, the rich family of neural networks, for tackling complex
problems. The main area of application of neural networks is the area of
classification and regression. Route optimization can be viewed as a new
challenge for neural networks. The article first presents an analysis of the
applicability of neural network tools, then a novel graphical neural network
model is presented in detail. The efficiency analysis based on test experiments
shows the applicability of the proposed NN architecture.

摘要：車輛路線問題是關於最佳化車輛路線，以滿足特定地點客戶的需求。路線圖包含多個層級的車輛存放處和客戶位置。多年來已經開發出多種最佳化方法，其中大多數基於某種類型的經典啟發式方法：遺傳演算法、模擬退火、禁忌搜尋、蟻群最佳化、螢火蟲演算法。機器學習的最新發展提供了一組新的工具，即豐富的神經網路家族，用於解決複雜的問題。神經網路的主要應用領域是分類和回歸領域。路線最佳化可以視為神經網路的新挑戰。本文首先分析了神經網路工具的適用性，然後詳細介紹了一個新穎的圖形神經網路模型。基於測試實驗的效率分析顯示了所提出的神經網路架構的適用性。

##### **Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling**
2409.11283v2 by Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li

LLMs obtain remarkable performance but suffer from hallucinations. Most
research on detecting hallucination focuses on the questions with short and
concrete correct answers that are easy to check the faithfulness. Hallucination
detections for text generation with open-ended answers are more challenging.
Some researchers use external knowledge to detect hallucinations in generated
texts, but external resources for specific scenarios are hard to access. Recent
studies on detecting hallucinations in long text without external resources
conduct consistency comparison among multiple sampled outputs. To handle long
texts, researchers split long texts into multiple facts and individually
compare the consistency of each pairs of facts. However, these methods (1)
hardly achieve alignment among multiple facts; (2) overlook dependencies
between multiple contextual facts. In this paper, we propose a graph-based
context-aware (GCA) hallucination detection for text generations, which aligns
knowledge facts and considers the dependencies between contextual knowledge
triples in consistency comparison. Particularly, to align multiple facts, we
conduct a triple-oriented response segmentation to extract multiple knowledge
triples. To model dependencies among contextual knowledge triple (facts), we
construct contextual triple into a graph and enhance triples' interactions via
message passing and aggregating via RGCN. To avoid the omission of knowledge
triples in long text, we conduct a LLM-based reverse verification via
reconstructing the knowledge triples. Experiments show that our model enhances
hallucination detection and excels all baselines.

摘要：LLM 獲得顯著的效能，但會出現幻覺。大多數關於偵測幻覺的研究都集中在問題的答案簡短且具體，且易於檢查其真實性。對於開放式答案的文字生成，幻覺的偵測更具挑戰性。一些研究人員使用外部知識來偵測生成的文字中的幻覺，但特定場景的外部資源難以取得。最近關於在沒有外部資源的情況下偵測長篇文字幻覺的研究，在多個取樣輸出之間進行一致性比較。為了處理長篇文字，研究人員將長篇文字拆分成多個事實，並個別比較每對事實的一致性。然而，這些方法（1）難以在多個事實之間取得一致性；（2）忽略多個脈絡事實之間的依賴性。在本文中，我們提出一個基於圖形、具脈絡感知（GCA）的文字生成幻覺偵測，它會比對知識事實，並在一致性比較中考慮脈絡知識三元組之間的依賴性。特別是，為了比對多個事實，我們進行三元導向的回應區隔，以萃取多個知識三元組。為了模擬脈絡知識三元組（事實）之間的依賴性，我們將脈絡三元組建構為一個圖形，並透過訊息傳遞和透過 RGCN 進行彙總來增強三元組的互動。為了避免遺漏長篇文字中的知識三元組，我們透過重建知識三元組進行基於 LLM 的反向驗證。實驗顯示，我們的模型增強了幻覺偵測，並優於所有基準。

##### **Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5**
2409.11282v1 by Marcel Lamott, Muhammad Armaghan Shakir

The surge of digital documents in various formats, including less
standardized documents such as business reports and environmental assessments,
underscores the growing importance of Document Understanding. While Large
Language Models (LLMs) have showcased prowess across diverse natural language
processing tasks, their direct application to Document Understanding remains a
challenge. Previous research has demonstrated the utility of LLMs in this
domain, yet their significant computational demands make them challenging to
deploy effectively. Additionally, proprietary Blackbox LLMs often outperform
their open-source counterparts, posing a barrier to widespread accessibility.
In this paper, we delve into the realm of document understanding, leveraging
distillation methods to harness the power of large LLMs while accommodating
computational limitations. Specifically, we present a novel approach wherein we
distill document understanding knowledge from the proprietary LLM ChatGPT into
FLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms
to facilitate efficient knowledge transfer. This work contributes to the
advancement of document understanding methodologies by offering a scalable
solution that bridges the gap between resource-intensive LLMs and practical
applications. Our findings underscore the potential of distillation techniques
in facilitating the deployment of sophisticated language models in real-world
scenarios, thereby fostering advancements in natural language processing and
document comprehension domains.

摘要：隨著各種格式數位文件激增，包括商業報告和環境評估等標準化程度較低的檔案，文件理解的重要性與日俱增。儘管大型語言模型 (LLM) 已在各種自然語言處理任務中展現其優勢，但它們在文件理解中的直接應用仍是一項挑戰。先前的研究已證明 LLM 在此領域的效用，但其龐大的運算需求使其難以有效部署。此外，專有的黑盒子 LLM 通常優於其開源對應版本，對廣泛的可及性構成障礙。在本文中，我們深入探討文件理解領域，利用知識蒸餾方法來利用大型 LLM 的功能，同時適應運算限制。具體來說，我們提出了一種新方法，其中我們將文件理解知識從專有的 LLM ChatGPT 蒸餾到 FLAN-T5 中。我們的技術整合了標籤和課程學習機制，以促進有效的知識傳遞。這項工作透過提供一個可擴充解決方案來推進文件理解方法，彌合了資源密集型 LLM 和實際應用之間的差距。我們的研究結果強調了知識蒸餾技術在促進複雜語言模型在真實世界場景中部署的潛力，從而促進自然語言處理和文件理解領域的進步。

##### **P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task**
2409.11279v1 by Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li

Embodied Everyday Task is a popular task in the embodied AI community,
requiring agents to make a sequence of actions based on natural language
instructions and visual observations. Traditional learning-based approaches
face two challenges. Firstly, natural language instructions often lack explicit
task planning. Secondly, extensive training is required to equip models with
knowledge of the task environment. Previous works based on Large Language Model
(LLM) either suffer from poor performance due to the lack of task-specific
knowledge or rely on ground truth as few-shot samples. To address the above
limitations, we propose a novel approach called Progressive Retrieval Augmented
Generation (P-RAG), which not only effectively leverages the powerful language
processing capabilities of LLMs but also progressively accumulates
task-specific knowledge without ground-truth. Compared to the conventional RAG
methods, which retrieve relevant information from the database in a one-shot
manner to assist generation, P-RAG introduces an iterative approach to
progressively update the database. In each iteration, P-RAG retrieves the
latest database and obtains historical information from the previous
interaction as experiential references for the current interaction. Moreover,
we also introduce a more granular retrieval scheme that not only retrieves
similar tasks but also incorporates retrieval of similar situations to provide
more valuable reference experiences. Extensive experiments reveal that P-RAG
achieves competitive results without utilizing ground truth and can even
further improve performance through self-iterations.

摘要：具身日常任務是具身 AI 社群中常見的任務，
要求代理根據自然語言指令和視覺觀察進行一系列動作。傳統的基於學習的方法
面臨兩項挑戰。首先，自然語言指令通常缺乏明確的
任務規劃。其次，需要大量的訓練才能讓模型具備
任務環境的知識。基於大型語言模型 (LLM) 的先前作品
由於缺乏特定任務的知識而導致效能不佳或依賴於少數樣本的真實數據。為了解決上述
限制，我們提出了一種稱為漸進式檢索增強生成 (P-RAG) 的新方法，它不僅有效地利用了 LLM 強大的語言
處理能力，而且還逐步累積
任務特定知識，而無需真實數據。與傳統的 RAG 方法相比，後者以一次性的方式從資料庫中檢索相關資訊以協助生成，P-RAG 採用一種反覆運算的方法來
逐步更新資料庫。在每個反覆運算中，P-RAG 檢索最新的資料庫並從先前的
互動中獲取歷史資訊作為當前互動的經驗參考。此外，
我們還引入了一個更精細的檢索方案，它不僅檢索
類似的任務，還包含檢索類似的狀況，以提供
更有價值的參考經驗。大量的實驗表明，P-RAG
在不使用真實數據的情況下實現了具有競爭力的結果，甚至
可以透過自我反覆運算進一步提高效能。

##### **Machine Learning and Theory Ladenness -- A Phenomenological Account**
2409.11277v1 by Alberto Termine, Emanuele Ratti, Alessandro Facchini

In recent years, the dissemination of machine learning (ML) methodologies in
scientific research has prompted discussions on theory ladenness. More
specifically, the issue of theory ladenness has remerged as questions about
whether and how ML models (MLMs) and ML modelling strategies are impacted by
the domain theory of the scientific field in which ML is used and implemented
(e.g., physics, chemistry, biology, etc). On the one hand, some have argued
that there is no difference between traditional (pre ML) and ML assisted
science. In both cases, theory plays an essential and unavoidable role in the
analysis of phenomena and the construction and use of models. Others have
argued instead that ML methodologies and models are theory independent and, in
some cases, even theory free. In this article, we argue that both positions are
overly simplistic and do not advance our understanding of the interplay between
ML methods and domain theories. Specifically, we provide an analysis of theory
ladenness in ML assisted science. Our analysis reveals that, while the
construction of MLMs can be relatively independent of domain theory, the
practical implementation and interpretation of these models within a given
specific domain still relies on fundamental theoretical assumptions and
background knowledge.

摘要：近年来，机器学习（ML）方法在科学研究中的传播引发了关于理论负载的讨论。更具体地说，理论负载的问题已经重新出现，因为关于 ML 模型 (MLM) 和 ML 建模策略是否以及如何受到 ML 所使用和实施的科学领域（例如物理学、化学、生物学等）的领域理论的影响的问题。一方面，一些人认为传统（ML 之前）和 ML 辅助科学之间没有区别。在这两种情况下，理论在现象分析以及模型的构建和使用中都扮演着至关重要的不可避免的角色。其他人则认为 ML 方法和模型与理论无关，在某些情况下甚至没有理论。在本文中，我们认为这两种立场过于简单化，并没有促进我们对 ML 方法和领域理论之间相互作用的理解。具体来说，我们对 ML 辅助科学中的理论负载进行了分析。我们的分析表明，虽然 ML 模型的构建可以相对独立于领域理论，但这些模型在给定特定领域内的实际实施和解释仍然依赖于基本的理论假设和背景知识。

##### **Task Arithmetic for Language Expansion in Speech Translation**
2409.11274v1 by Yao-Fei Cheng, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Wen Shen Teo, Siddhant Arora, Shinji Watanabe

Recent advances in large language models (LLMs) have gained interest in
speech-text multimodal foundation models, achieving strong performance on
instruction-based speech translation (ST). However, expanding language pairs
from an existing instruction-tuned ST system is costly due to the necessity of
re-training on a combination of new and previous datasets. We propose to expand
new language pairs by merging the model trained on new language pairs and the
existing model, using task arithmetic. We find that the direct application of
task arithmetic for ST causes the merged model to fail to follow instructions;
thus, generating translation in incorrect languages. To eliminate language
confusion, we propose an augmented task arithmetic method that merges an
additional language control model. It is trained to generate the correct target
language token following the instructions. Our experiments demonstrate that our
proposed language control model can achieve language expansion by eliminating
language confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66
and 4.92 BLEU scores improvement, respectively. In addition, we demonstrate the
use of our task arithmetic framework can expand to a language pair where
neither paired ST training data nor a pre-trained ST model is available. We
first synthesize the ST system from machine translation (MT) systems via task
analogy, then merge the synthesized ST system to the existing ST model.

摘要：最近，大型语言模型 (LLM) 的最新进展引起了人们对语音文本多模态基础模型的兴趣，在基于指令的语音翻译 (ST) 中取得了强劲的性能。然而，由于需要在新的和以前的数据集的组合上重新训练，因此从现有的指令调整的 ST 系统扩展语言对的成本很高。我们建议通过使用任务算术合并针对新语言对训练的模型和现有模型来扩展新语言对。我们发现，直接将任务算术应用于 ST 会导致合并后的模型无法遵循指令；因此，生成错误语言的翻译。为了消除语言混淆，我们提出了一种增强任务算术的方法，该方法合并了一个额外的语言控制模型。它经过训练，可以按照指令生成正确的目标语言标记。我们的实验表明，我们提出的语言控制模型可以通过消除语言混淆来实现语言扩展。在我们的 MuST-C 和 CoVoST-2 实验中，它分别显示出高达 4.66 和 4.92 的 BLEU 分数改进。此外，我们展示了我们的任务算术框架的使用可以扩展到既没有配对 ST 训练数据也没有预训练 ST 模型的语言对。我们首先通过任务类比从机器翻译 (MT) 系统中合成 ST 系统，然后将合成的 ST 系统合并到现有的 ST 模型中。

##### **LOLA -- An Open-Source Massively Multilingual Large Language Model**
2409.11272v2 by Nikit Srivastava, Denis Kuchelev, Tatiana Moteu, Kshitij Shetty, Michael Röder, Diego Moussallem, Hamada Zahera, Axel-Cyrille Ngonga Ngomo

This paper presents LOLA, a massively multilingual large language model
trained on more than 160 languages using a sparse Mixture-of-Experts
Transformer architecture. Our architectural and implementation choices address
the challenge of harnessing linguistic diversity while maintaining efficiency
and avoiding the common pitfalls of multilinguality. Our analysis of the
evaluation results shows competitive performance in natural language generation
and understanding tasks. Additionally, we demonstrate how the learned
expert-routing mechanism exploits implicit phylogenetic linguistic patterns to
potentially alleviate the curse of multilinguality. We provide an in-depth look
at the training process, an analysis of the datasets, and a balanced
exploration of the model's strengths and limitations. As an open-source model,
LOLA promotes reproducibility and serves as a robust foundation for future
research. Our findings enable the development of compute-efficient multilingual
models with strong, scalable performance across languages.

摘要：本論文提出 LOLA，一種大規模多語言大型語言模型，使用稀疏混合專家Transformer架構，針對超過 160 種語言進行訓練。我們的架構和實作選擇，解決了在維持效率和避免多語言常見陷阱的同時，利用語言多樣性的挑戰。我們對評估結果的分析，顯示在自然語言生成和理解任務中具有競爭力的表現。此外，我們展示了學習到的專家路由機制，如何利用隱含的系統發生語言模式，來潛在減輕多語言的詛咒。我們深入探討訓練過程、對資料集的分析，以及對模型優缺點的平衡探討。作為一個開放原始碼模型，LOLA 推廣可複製性，並作為未來研究的穩固基礎。我們的發現，使開發出跨語言具有強大、可擴充性能的計算高效多語言模型成為可能。

##### **Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids**
2409.11267v1 by Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter

This work proposes an approach that integrates reinforcement learning and
model predictive control (MPC) to efficiently solve finite-horizon optimal
control problems in mixed-logical dynamical systems. Optimization-based control
of such systems with discrete and continuous decision variables entails the
online solution of mixed-integer quadratic or linear programs, which suffer
from the curse of dimensionality. Our approach aims at mitigating this issue by
effectively decoupling the decision on the discrete variables and the decision
on the continuous variables. Moreover, to mitigate the combinatorial growth in
the number of possible actions due to the prediction horizon, we conceive the
definition of decoupled Q-functions to make the learning problem more
tractable. The use of reinforcement learning reduces the online optimization
problem of the MPC controller from a mixed-integer linear (quadratic) program
to a linear (quadratic) program, greatly reducing the computational time.
Simulation experiments for a microgrid, based on real-world data, demonstrate
that the proposed method significantly reduces the online computation time of
the MPC approach and that it generates policies with small optimality gaps and
high feasibility rates.

摘要：本研究提出一种方法，整合强化学习和模型预测控制 (MPC)，以有效解决混合逻辑动态系统中的有限时间最优控制问题。此类具有离散和连续决策变量的基于优化控制的系统需要在线解决混合整数二次或线性规划，这会受到维度灾难的影响。我们的方法旨在通过有效解耦离散变量的决策和连续变量的决策来缓解此问题。此外，为了减轻预测范围导致的可能动作数量的组合增长，我们构想了解耦 Q 函数的定义，以使学习问题更容易处理。强化学习的使用将 MPC 控制器的在线优化问题从混合整数线性（二次）规划减少到线性（二次）规划，从而大大减少了计算时间。基于真实世界数据的微电网仿真实验表明，所提出的方法显著减少了 MPC 方法的在线计算时间，并且它生成的策略具有较小的最优性差距和较高的可行性比率。

##### **Bio-Inspired Mamba: Temporal Locality and Bioplausible Learning in Selective State Space Models**
2409.11263v1 by Jiahao Qin

This paper introduces Bio-Inspired Mamba (BIM), a novel online learning
framework for selective state space models that integrates biological learning
principles with the Mamba architecture. BIM combines Real-Time Recurrent
Learning (RTRL) with Spike-Timing-Dependent Plasticity (STDP)-like local
learning rules, addressing the challenges of temporal locality and biological
plausibility in training spiking neural networks. Our approach leverages the
inherent connection between backpropagation through time and STDP, offering a
computationally efficient alternative that maintains the ability to capture
long-range dependencies. We evaluate BIM on language modeling, speech
recognition, and biomedical signal analysis tasks, demonstrating competitive
performance against traditional methods while adhering to biological learning
principles. Results show improved energy efficiency and potential for
neuromorphic hardware implementation. BIM not only advances the field of
biologically plausible machine learning but also provides insights into the
mechanisms of temporal information processing in biological neural networks.

摘要：這篇論文介紹了生物靈感曼巴 (BIM)，這是一個針對選擇性狀態空間模型的新型線上學習架構，它將生物學習原則與曼巴架構整合在一起。BIM 結合了即時遞迴學習 (RTRL) 與類尖峰時序依賴可塑性 (STDP) 的局部學習規則，解決了訓練脈衝神經網路中時間局部性和生物可信度的挑戰。我們的做法利用了反向傳播與 STDP 之間的內在關聯，提供了一個計算效率高的替代方案，同時保持了捕捉長程依賴性的能力。我們在語言建模、語音識別和生物醫學訊號分析任務上評估了 BIM，證明了它在遵循生物學習原則的同時，與傳統方法相比具有競爭力。結果顯示出改善的能源效率和神經形態硬體實作的潛力。BIM 不僅推動了生物可信機器學習領域的進步，也提供了對生物神經網路中時間資訊處理機制的見解。

##### **The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound Event Detection**
2409.11262v1 by Gabriel Bibbó, Thomas Deacon, Arshdeep Singh, Mark D. Plumbley

This paper presents a residential audio dataset to support sound event
detection research for smart home applications aimed at promoting wellbeing for
older adults. The dataset is constructed by deploying audio recording systems
in the homes of 8 participants aged 55-80 years for a 7-day period. Acoustic
characteristics are documented through detailed floor plans and construction
material information to enable replication of the recording environments for AI
model deployment. A novel automated speech removal pipeline is developed, using
pre-trained audio neural networks to detect and remove segments containing
spoken voice, while preserving segments containing other sound events. The
resulting dataset consists of privacy-compliant audio recordings that
accurately capture the soundscapes and activities of daily living within
residential spaces. The paper details the dataset creation methodology, the
speech removal pipeline utilizing cascaded model architectures, and an analysis
of the vocal label distribution to validate the speech removal process. This
dataset enables the development and benchmarking of sound event detection
models tailored specifically for in-home applications.

摘要：本論文提出一個住宅音訊資料集，以支援智慧家庭應用之音訊事件偵測研究，旨在促進老年人的福祉。此資料集係透過在 8 位 55 至 80 歲參與者的家中部署音訊錄製系統，為期 7 天而建置。音響特性透過詳細的平面圖和建材資訊進行記錄，以利複製錄音環境以進行 AI 模型部署。並開發出一個新穎的自動化語音移除管線，利用預先訓練的音訊神經網路來偵測並移除包含語音的片段，同時保留包含其他音訊事件的片段。所得資料集包含符合隱私規範的音訊錄音，可精確擷取住宅空間內日常生活中的音景和活動。本文詳述資料集建立方法、利用串聯模型架構的語音移除管線，以及對語音標籤分佈的分析，以驗證語音移除流程。此資料集可協助開發和評量專門針對居家應用所量身打造的音訊事件偵測模型。

##### **The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives**
2409.11261v2 by Samee Arif, Taimoor Arif, Aamina Jamal Khan, Muhammad Saad Haroon, Agha Ali Raza, Awais Athar

This paper introduces the concept of an education tool that utilizes
Generative Artificial Intelligence (GenAI) to enhance storytelling for
children. The system combines GenAI-driven narrative co-creation,
text-to-speech conversion, and text-to-video generation to produce an engaging
experience for learners. We describe the co-creation process, the adaptation of
narratives into spoken words using text-to-speech models, and the
transformation of these narratives into contextually relevant visuals through
text-to-video technology. Our evaluation covers the linguistics of the
generated stories, the text-to-speech conversion quality, and the accuracy of
the generated visuals.

摘要：本論文介紹了一種教育工具的概念，該工具利用生成式人工智慧 (GenAI) 來增強兒童的故事敘述。此系統結合了由 GenAI 驅動的敘事共同創作、文字轉語音轉換以及文字轉影片生成，為學習者帶來引人入勝的體驗。我們描述了共同創作的過程、使用文字轉語音模型將敘事改編為有聲書，以及透過文字轉影片技術將這些敘事轉換為與脈絡相關的視覺效果。我們的評量涵蓋了生成故事的語言學、文字轉語音轉換品質以及生成視覺效果的準確性。

##### **Attacking Slicing Network via Side-channel Reinforcement Learning Attack**
2409.11258v1 by Wei Shao, Chandra Thapa, Rayne Holland, Sarah Ali Siddiqui, Seyit Camtepe

Network slicing in 5G and the future 6G networks will enable the creation of
multiple virtualized networks on a shared physical infrastructure. This
innovative approach enables the provision of tailored networks to accommodate
specific business types or industry users, thus delivering more customized and
efficient services. However, the shared memory and cache in network slicing
introduce security vulnerabilities that have yet to be fully addressed. In this
paper, we introduce a reinforcement learning-based side-channel cache attack
framework specifically designed for network slicing environments. Unlike
traditional cache attack methods, our framework leverages reinforcement
learning to dynamically identify and exploit cache locations storing sensitive
information, such as authentication keys and user registration data. We assume
that one slice network is compromised and demonstrate how the attacker can
induce another shared slice to send registration requests, thereby estimating
the cache locations of critical data. By formulating the cache timing channel
attack as a reinforcement learning-driven guessing game between the attack
slice and the victim slice, our model efficiently explores possible actions to
pinpoint memory blocks containing sensitive information. Experimental results
showcase the superiority of our approach, achieving a success rate of
approximately 95\% to 98\% in accurately identifying the storage locations of
sensitive data. This high level of accuracy underscores the potential risks in
shared network slicing environments and highlights the need for robust security
measures to safeguard against such advanced side-channel attacks.

摘要：5G 和未来的 6G 网络中的网络切片将能够在共享的物理基础设施上创建多个虚拟化网络。这种创新方法能够提供定制化网络，以适应特定的业务类型或行业用户，从而提供更多定制化和高效的服务。然而，网络切片中的共享内存和缓存引入了尚未得到充分解决的安全漏洞。在本文中，我们引入了一个基于强化学习的旁路缓存攻击框架，该框架专门设计用于网络切片环境。与传统的缓存攻击方法不同，我们的框架利用强化学习来动态识别和利用存储敏感信息的缓存位置，例如身份验证密钥和用户注册数据。我们假设一个切片网络遭到破坏，并演示攻击者如何诱使另一个共享切片发送注册请求，从而估计关键数据的缓存位置。通过将缓存时序信道攻击表述为攻击切片和受害切片之间的强化学习驱动的猜谜游戏，我们的模型有效地探索了可能的动作，以精确定位包含敏感信息的内存块。实验结果展示了我们方法的优越性，在准确识别敏感数据的存储位置方面实现了大约 95% 到 98% 的成功率。这种高准确性强调了共享网络切片环境中的潜在风险，并凸显了对稳健的安全措施的需求，以防范此类高级旁路攻击。

##### **Norm of Mean Contextualized Embeddings Determines their Variance**
2409.11253v1 by Hiroaki Yamagiwa, Hidetoshi Shimodaira

Contextualized embeddings vary by context, even for the same token, and form
a distribution in the embedding space. To analyze this distribution, we focus
on the norm of the mean embedding and the variance of the embeddings. In this
study, we first demonstrate that these values follow the well-known formula for
variance in statistics and provide an efficient sequential computation method.
Then, by observing embeddings from intermediate layers of several Transformer
models, we found a strong trade-off relationship between the norm and the
variance: as the mean embedding becomes closer to the origin, the variance
increases. This trade-off is likely influenced by the layer normalization
mechanism used in Transformer models. Furthermore, when the sets of token
embeddings are treated as clusters, we show that the variance of the entire
embedding set can theoretically be decomposed into the within-cluster variance
and the between-cluster variance. We found experimentally that as the layers of
Transformer models deepen, the embeddings move farther from the origin, the
between-cluster variance relatively decreases, and the within-cluster variance
relatively increases. These results are consistent with existing studies on the
anisotropy of the embedding spaces across layers.

摘要：上下文嵌入會因上下文而異，即使是同一個標記，也會在嵌入空間中形成一個分佈。為了分析此分佈，我們專注於平均嵌入的範數和嵌入的變異數。在此研究中，我們首先證明這些值遵循統計學中眾所周知的變異數公式，並提供一種有效的順序計算方法。然後，通過觀察幾個 Transformer 模型的中間層嵌入，我們發現範數和變異數之間存在強烈的折衷關係：隨著平均嵌入越來越接近原點，變異數就會增加。此折衷可能受 Transformer 模型中使用的層正規化機制影響。此外，當將標記嵌入集合視為叢集時，我們證明整個嵌入集合的變異數在理論上可以分解為叢集內變異數和叢集間變異數。我們透過實驗發現，隨著 Transformer 模型的層數加深，嵌入會遠離原點，叢集間變異數相對減少，而叢集內變異數相對增加。這些結果與現有關於跨層嵌入空間各向異性的研究一致。

##### **WER We Stand: Benchmarking Urdu ASR Models**
2409.11252v1 by Samee Arif, Aamina Jamal Khan, Mustafa Abbas, Agha Ali Raza, Awais Athar

This paper presents a comprehensive evaluation of Urdu Automatic Speech
Recognition (ASR) models. We analyze the performance of three ASR model
families: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along
with a detailed examination of the most frequent wrong words and error types
including insertions, deletions, and substitutions. Our analysis is conducted
using two types of datasets, read speech and conversational speech. Notably, we
present the first conversational speech dataset designed for benchmarking Urdu
ASR models. We find that seamless-large outperforms other ASR models on the
read speech dataset, while whisper-large performs best on the conversational
speech dataset. Furthermore, this evaluation highlights the complexities of
assessing ASR models for low-resource languages like Urdu using quantitative
metrics alone and emphasizes the need for a robust Urdu text normalization
system. Our findings contribute valuable insights for developing robust ASR
systems for low-resource languages like Urdu.

摘要：這篇論文提供了一個全面的烏爾都語自動語音辨識 (ASR) 模型評估。我們分析了三個 ASR 模型家族的效能：Whisper、MMS 和 Seamless-M4T，使用詞語錯誤率 (WER)，以及對最常錯的詞語和錯誤類型進行詳細檢查，包括插入、刪除和替換。我們的分析使用兩種類型的資料集進行：朗讀語音和對話語音。值得注意的是，我們提供了第一個針對烏爾都語 ASR 模型基準測試而設計的對話語音資料集。我們發現 seamless-large 在朗讀語音資料集上的表現優於其他 ASR 模型，而 whisper-large 在對話語音資料集上的表現最佳。此外，此評估強調了僅使用量化指標評估烏爾都語等低資源語言的 ASR 模型的複雜性，並強調了對強健的烏爾都語文字正規化系統的需求。我們的發現為開發針對烏爾都語等低資源語言的強健 ASR 系統提供了寶貴的見解。

##### **Linear Recency Bias During Training Improves Transformers' Fit to Reading Times**
2409.11250v1 by Christian Clark, Byung-Doh Oh, William Schuler

Recent psycholinguistic research has compared human reading times to
surprisal estimates from language models to study the factors shaping human
sentence processing difficulty. Previous studies have shown a strong fit
between surprisal values from Transformers and reading times. However, standard
Transformers work with a lossless representation of the entire previous
linguistic context, unlike models of human language processing that include
memory decay. To bridge this gap, this paper evaluates a modification of the
Transformer model that uses ALiBi (Press et al., 2022), a recency bias added to
attention scores. Surprisal estimates with ALiBi show an improved fit to human
reading times compared to a standard Transformer baseline. A subsequent
analysis of attention heads suggests that ALiBi's mixture of slopes -- which
determine the rate of memory decay in each attention head -- may play a role in
the improvement by helping models with ALiBi to track different kinds of
linguistic dependencies.

摘要：最近的心理语言学研究将人类阅读时间与语言模型的惊奇估计值进行比较，以研究影响人类句子处理难度的因素。先前的研究表明，来自 Transformer 的惊奇值与阅读时间之间有很强的契合度。然而，标准 Transformer 使用整个先前语言上下文的无损表示，这与包含记忆衰减的人类语言处理模型不同。为了弥合这一差距，本文评估了 Transformer 模型的修改，该模型使用 ALiBi（Press 等人，2022 年），一种添加到注意力分数的近期偏差。与标准 Transformer 基线相比，使用 ALiBi 的惊奇估计显示出与人类阅读时间更好的拟合。对注意力头的后续分析表明，ALiBi 的斜率混合——它决定了每个注意力头中记忆衰减的速率——可能通过帮助带有 ALiBi 的模型跟踪不同类型的语言依赖关系而在改进中发挥作用。

##### **Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse**
2409.11242v1 by Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria

LLMs are an integral part of retrieval-augmented generation (RAG) systems.
While many studies focus on evaluating the quality of end-to-end RAG systems,
there is a lack of research on understanding the appropriateness of an LLM for
the RAG task. Thus, we introduce a new metric, Trust-Score, that provides a
holistic evaluation of the trustworthiness of LLMs in an RAG framework. We show
that various prompting methods, such as in-context learning, fail to adapt LLMs
effectively to the RAG task. Thus, we propose Trust-Align, a framework to align
LLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly
outperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up
29.2) and ELI5 (up 14.9). We release our code at:
https://github.com/declare-lab/trust-align.

摘要：大型语言模型 (LLM) 是检索增强生成 (RAG) 系统中不可或缺的一部分。
虽然许多研究专注于评估端到端 RAG 系统的质量，
但对于理解 LLM 对 RAG 任务的适用性却缺乏研究。因此，我们引入了一个新指标 Trust-Score，该指标提供了对 RAG 框架中 LLM 可信度的全面评估。我们表明，各种提示方法（例如上下文学习）都无法有效地使 LLM 适应 RAG 任务。因此，我们提出了 Trust-Align，这是一个用于调整 LLM 以获得更高 Trust-Score 的框架。与我们的方法相一致的 LLaMA-3-8b 在 ASQA（提升 10.7）、QAMPARI（提升 29.2）和 ELI5（提升 14.9）上明显优于同等规模的开源 LLM。我们在以下位置发布我们的代码：
https://github.com/declare-lab/trust-align。

##### **Spontaneous Informal Speech Dataset for Punctuation Restoration**
2409.11241v1 by Xing Yi Liu, Homayoon Beigi

Presently, punctuation restoration models are evaluated almost solely on
well-structured, scripted corpora. On the other hand, real-world ASR systems
and post-processing pipelines typically apply towards spontaneous speech with
significant irregularities, stutters, and deviations from perfect grammar. To
address this discrepancy, we introduce SponSpeech, a punctuation restoration
dataset derived from informal speech sources, which includes punctuation and
casing information. In addition to publicly releasing the dataset, we
contribute a filtering pipeline that can be used to generate more data. Our
filtering pipeline examines the quality of both speech audio and transcription
text. We also carefully construct a ``challenging" test set, aimed at
evaluating models' ability to leverage audio information to predict otherwise
grammatically ambiguous punctuation. SponSpeech is available at
https://github.com/GitHubAccountAnonymous/PR, along with all code for dataset
building and model runs.

摘要：目前，標點符號還原模型幾乎只針對結構良好的腳本語料庫進行評估。另一方面，現實世界的 ASR 系統和後處理管道通常適用於自發性語音，其中存在顯著的不規則性、口吃和語法缺陷。為了解決這種差異，我們引入了 SponSpeech，這是一個從非正式語音來源派生的標點符號還原資料集，其中包括標點符號和大小寫資訊。除了公開發佈資料集外，我們還提供了一個過濾管道，可用於生成更多資料。我們的過濾管道檢查語音音訊和轉錄文字的品質。我們還仔細構建了一個「具挑戰性」的測試集，旨在評估模型利用音訊資訊來預測在語法上模稜兩可的標點符號的能力。SponSpeech 可在 https://github.com/GitHubAccountAnonymous/PR 取得，其中包含所有用於資料集建置和模型運行的程式碼。

##### **LLM-as-a-Judge & Reward Model: What They Can and Cannot Do**
2409.11239v1 by Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong

LLM-as-a-Judge and reward models are widely used alternatives of
multiple-choice questions or human annotators for large language model (LLM)
evaluation. Their efficacy shines in evaluating long-form responses, serving a
critical role as evaluators of leaderboards and as proxies to align LLMs via
reinforcement learning. However, despite their popularity, their effectiveness
outside of English remains largely unexplored. In this paper, we conduct a
comprehensive analysis on automated evaluators, reporting key findings on their
behavior in a non-English environment. First, we discover that English
evaluation capabilities significantly influence language-specific capabilities,
often more than the language proficiency itself, enabling evaluators trained in
English to easily transfer their skills to other languages. Second, we identify
critical shortcomings, where LLMs fail to detect and penalize errors, such as
factual inaccuracies, cultural misrepresentations, and the presence of unwanted
language. Finally, we release Kudge, the first non-English meta-evaluation
dataset containing 5,012 human annotations in Korean.

摘要：LLM 即法官和獎勵模型是廣泛用於多選題或人類註釋者的大型語言模型 (LLM) 評估的替代方案。它們在評估長篇回應方面發揮著顯著作用，作為排行榜的評估者和通過強化學習調整 LLM 的代理扮演著至關重要的角色。然而，儘管它們很受歡迎，但它們在英語以外的有效性在很大程度上仍未得到探索。在本文中，我們對自動化評估器進行了全面分析，報告了它們在非英語環境中的行為的關鍵發現。首先，我們發現英語評估能力顯著影響特定語言的能力，通常比語言能力本身影響更大，這使得接受過英語培訓的評估者能夠輕鬆地將他們的技能轉移到其他語言。其次，我們發現了嚴重的缺點，LLM 無法檢測和懲罰錯誤，例如事實不准確、文化誤解和存在不需要的語言。最後，我們發布了 Kudge，這是第一個非英語元評估數據集，其中包含 5,012 個韓語人類註釋。

##### **Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models**
2409.11233v1 by Bishwash Khanal, Jeffery M. Capone

Large language models (LLMs) offer powerful capabilities but incur
substantial computational costs, driving the need for efficient compression
techniques. This study evaluates the impact of popular compression methods -
Magnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on
the trade-offs between model size reduction, downstream task performance, and
the role of calibration data. Our findings reveal that while SparseGPT and
Wanda preserve perplexity even at 50% sparsity, they suffer significant
degradation on downstream tasks, highlighting the inadequacy of perplexity as
the sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)
Divergence as a more comprehensive metric that captures nuanced changes in
model behavior post-compression. We further demonstrate that task-specific
calibration data significantly enhances the downstream performance of
compressed models compared to general calibration data. This research
underscores the necessity for diverse evaluation metrics and careful
calibration data selection to fully understand the complexities of LLM
compression and its implications for practical applications.

摘要：大型語言模型 (LLM) 提供強大的功能，但會產生大量的計算成本，因此需要高效的壓縮技術。本研究評估了流行的壓縮方法對 LLaMA-2-7B 模型的影響，包括幅度剪枝、SparseGPT 和 Wanda，重點在於模型大小縮減、下游任務效能和校正資料的角色之間的權衡。我們的研究結果顯示，儘管 SparseGPT 和 Wanda 即使在 50% 的稀疏度下也能維持困惑度，但它們在下游任務中會顯著惡化，這突顯了困惑度作為唯一評估指標的不足。為了解決這個問題，我們引入了 Jensen-Shannon (JS) 距離作為一個更全面的指標，它能捕捉模型行為在壓縮後發生的細微變化。我們進一步證明，與一般校正資料相比，特定於任務的校正資料可以顯著提升壓縮模型的下游效能。這項研究強調了使用多元評估指標和仔細選擇校正資料的必要性，才能充分了解 LLM 壓縮的複雜性及其對實際應用程式造成的影響。

##### **Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?**
2409.11232v1 by Raffaele Marino

In this manuscript I present an analysis on the performance of OpenAI
O1-preview model in solving random K-SAT instances for K$\in {2,3,4}$ as a
function of $\alpha=M/N$ where $M$ is the number of clauses and $N$ is the
number of variables of the satisfiable problem. I show that the model can call
an external SAT solver to solve the instances, rather than solving them
directly. Despite using external solvers, the model reports incorrect
assignments as output. Moreover, I propose and present an analysis to quantify
whether the OpenAI O1-preview model demonstrates a spark of intelligence or
merely makes random guesses when outputting an assignment for a Boolean
satisfiability problem.

摘要：在本文中，我針對 OpenAI O1-preview 模型在解決隨機 K-SAT 實例的效能進行分析，其中 K∈ {2,3,4}，作為 α=M/N 的函數，其中 M 是子句的數量，而 N 是可滿足問題中變數的數量。我顯示模型可以呼叫外部 SAT 求解器來解決實例，而不是直接解決它們。儘管使用外部求解器，但模型會報告不正確的指派作為輸出。此外，我提出並提出一個分析，以量化 OpenAI O1-preview 模型是否展現出智慧的火花，或僅在為布林可滿足性問題輸出指派時進行隨機猜測。

##### **Learning Source Disentanglement in Neural Audio Codec**
2409.11228v1 by Xiaoyu Bie, Xubo Liu, Gaël Richard

Neural audio codecs have significantly advanced audio compression by
efficiently converting continuous audio signals into discrete tokens. These
codecs preserve high-quality sound and enable sophisticated sound generation
through generative models trained on these tokens. However, existing neural
codec models are typically trained on large, undifferentiated audio datasets,
neglecting the essential discrepancies between sound domains like speech,
music, and environmental sound effects. This oversight complicates data
modeling and poses additional challenges to the controllability of sound
generation. To tackle these issues, we introduce the Source-Disentangled Neural
Audio Codec (SD-Codec), a novel approach that combines audio coding and source
separation. By jointly learning audio resynthesis and separation, SD-Codec
explicitly assigns audio signals from different domains to distinct codebooks,
sets of discrete representations. Experimental results indicate that SD-Codec
not only maintains competitive resynthesis quality but also, supported by the
separation results, demonstrates successful disentanglement of different
sources in the latent space, thereby enhancing interpretability in audio codec
and providing potential finer control over the audio generation process.

摘要：神經音訊編解碼器透過有效率地將連續音訊訊號轉換為離散代幣，顯著提升音訊壓縮能力。這些編解碼器保留高品質音訊，並能透過訓練這些代幣的生成模型來進行精密的音訊產生。然而，現有的神經編解碼器模型通常訓練於大型、未區分的音訊資料集上，忽略了語音、音樂和環境音效等音訊領域間的本質差異。這種疏忽使得資料建模變得複雜，並對音訊產生的可控性造成額外的挑戰。為了解決這些問題，我們引入了來源分離神經音訊編解碼器 (SD-Codec)，這是一種結合音訊編碼和來源分離的新穎方法。透過共同學習音訊重新合成和分離，SD-Codec 明確地將不同領域的音訊訊號分配給不同的代碼簿，即離散表示集。實驗結果表明，SD-Codec 不僅維持了具競爭力的重新合成品質，而且在分離結果的支援下，證明成功地將潛在空間中不同的來源分離出來，從而增強了音訊編解碼器的可解釋性，並提供了對音訊產生過程更精細的控制。

##### **Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis**
2409.11218v1 by Lingling Xu, Haoran Xie, S. Joe Qin, Fu Lee Wang, Xiaohui Tao

Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards
specific aspect terms in a sentence and allows us to uncover nuanced
perspectives and attitudes on particular aspects of a product, service, or
topic. However, the scarcity of labeled data poses a significant challenge to
training high-quality models. To address this issue, we explore the potential
of data augmentation using ChatGPT, a well-performing large language model
(LLM), to enhance the sentiment classification performance towards aspect
terms. Specifically, we explore three data augmentation strategies based on
ChatGPT: context-focused, aspect-focused, and context-aspect data augmentation
techniques. Context-focused data augmentation focuses on changing the word
expression of context words in the sentence while keeping aspect terms
unchanged. In contrast, aspect-focused data augmentation aims to change aspect
terms but keep context words unchanged. Context-Aspect data augmentation
integrates the above two data augmentations to generate augmented samples.
Furthermore, we incorporate contrastive learning into the ABSA tasks to improve
performance. Extensive experiments show that all three data augmentation
techniques lead to performance improvements, with the context-aspect data
augmentation strategy performing best and surpassing the performance of the
baseline models.

摘要：基於面向面向觀點的情感分析（ABSA）涉及識別句子中特定面向詞彙的情感，並讓我們能夠揭示對產品、服務或主題特定面向的細緻觀點和態度。然而，標記資料的稀少對訓練高品質模型構成重大挑戰。為了解決此問題，我們探討使用 ChatGPT（一種表現良好的大型語言模型，LLM）進行資料擴充的潛力，以增強面向詞彙的情感分類效能。具體來說，我們探討了三種基於 ChatGPT 的資料擴充策略：以脈絡為重點、以面向為重點和脈絡-面向資料擴充技術。以脈絡為重點的資料擴充專注於改變句子中脈絡詞彙的詞彙表達，同時保持面向詞彙不變。相反地，以面向為重點的資料擴充旨在改變面向詞彙，但保持脈絡詞彙不變。脈絡-面向資料擴充整合上述兩種資料擴充，以產生擴充範例。此外，我們將對比學習納入 ABSA 任務，以提升效能。廣泛的實驗顯示，所有三種資料擴充技術都能提升效能，其中以脈絡-面向資料擴充策略表現最佳，並超越基準模型的效能。

##### **Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization**
2409.11212v1 by Jianing Wang, Yang Zhou, Xiaocheng Zhang, Mengjiao Bao, Peng Yan

Iterative preference optimization has recently become one of the de-facto
training paradigms for large language models (LLMs), but the performance is
still underwhelming due to too much noisy preference data yielded in the loop.
To combat this issue, we present an \textbf{U}ncertainty-enhanced
\textbf{P}reference \textbf{O}ptimization (UPO) framework to make the LLM
self-evolve with reliable feedback. The key idea is mitigating the noisy
preference data derived from the current policy and reward models by performing
pair-wise uncertainty estimation and judiciously reliable feedback sampling. To
reach this goal, we thus introduce an estimator model, which incorporates Monte
Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty
estimation for the preference data derived from the LLM policy. Compared to the
existing methods that directly filter generated responses based on the reward
score, the estimator focuses on the model uncertainty in a pair-wise manner and
effectively bypasses the confirmation bias problem of the reward model.
Additionally, we also propose an uncertainty-enhanced self-evolution algorithm
to improve the robustness of preference optimization and encourage the LLM to
generate responses with both high reward and certainty. Extensive experiments
over multiple benchmarks demonstrate that our framework substantially
alleviates the noisy problem and improves the performance of iterative
preference optimization.

摘要：迭代偏好优化最近已成为大型语言模型（LLM）的实际训练范例之一，但由于循环中产生的太多噪声偏好数据，性能仍然令人失望。为了解决这个问题，我们提出了一个不确定性增强的偏好优化（UPO）框架，使 LLM 能够通过可靠的反馈进行自我演化。关键思想是通过执行成对不确定性估计和明智可靠的反馈采样来减轻从当前策略和奖励模型中得出的嘈杂偏好数据。为了达到这个目标，我们因此引入了一个估计器模型，该模型将蒙特卡罗 (MC) dropout 纳入贝叶斯神经网络 (BNN) 中，以对从 LLM 策略中得出的偏好数据执行不确定性估计。与直接根据奖励分数过滤生成响应的现有方法相比，估计器以成对方式关注模型的不确定性，并有效绕过了奖励模型的确认偏差问题。此外，我们还提出了一种不确定性增强的自进化算法，以提高偏好优化的鲁棒性，并鼓励 LLM 生成具有高奖励和确定性的响应。在多个基准上的广泛实验表明，我们的框架大大缓解了噪声问题，并提高了迭代偏好优化的性能。

##### **SDP: Spiking Diffusion Policy for Robotic Manipulation with Learnable Channel-Wise Membrane Thresholds**
2409.11195v1 by Zhixing Hou, Maoxu Gao, Hang Yu, Mengyu Yang, Chio-In Ieong

This paper introduces a Spiking Diffusion Policy (SDP) learning method for
robotic manipulation by integrating Spiking Neurons and Learnable Channel-wise
Membrane Thresholds (LCMT) into the diffusion policy model, thereby enhancing
computational efficiency and achieving high performance in evaluated tasks.
Specifically, the proposed SDP model employs the U-Net architecture as the
backbone for diffusion learning within the Spiking Neural Network (SNN). It
strategically places residual connections between the spike convolution
operations and the Leaky Integrate-and-Fire (LIF) nodes, thereby preventing
disruptions to the spiking states. Additionally, we introduce a temporal
encoding block and a temporal decoding block to transform static and dynamic
data with timestep $T_S$ into each other, enabling the transmission of data
within the SNN in spike format. Furthermore, we propose LCMT to enable the
adaptive acquisition of membrane potential thresholds, thereby matching the
conditions of varying membrane potentials and firing rates across channels and
avoiding the cumbersome process of manually setting and tuning hyperparameters.
Evaluating the SDP model on seven distinct tasks with SNN timestep $T_S=4$, we
achieve results comparable to those of the ANN counterparts, along with faster
convergence speeds than the baseline SNN method. This improvement is
accompanied by a reduction of 94.3\% in dynamic energy consumption estimated on
45nm hardware.

摘要：本文介绍了一种尖峰扩散策略 (SDP) 学习方法，通过将尖峰神经元和可学习通道阈值 (LCMT) 集成到扩散策略模型中，从而提高计算效率并在评估任务中实现高性能，用于机器人操作。具体而言，所提出的 SDP 模型采用 U-Net 架构作为尖峰神经网络 (SNN) 内扩散学习的主干。它策略性地在尖峰卷积运算和漏电流积分放电 (LIF) 节点之间放置残差连接，从而防止尖峰状态中断。此外，我们引入一个时间编码块和一个时间解码块，以将时间步长为 $T_S$ 的静态和动态数据相互转换，从而能够以尖峰格式在 SNN 内传输数据。此外，我们提出了 LCMT 以实现膜电位阈值的自适应获取，从而匹配不同通道的膜电位和放电速率的变化条件，并避免手动设置和调整超参数的繁琐过程。在 SNN 时间步长 $T_S=4$ 的七个不同任务上评估 SDP 模型，我们取得了与 ANN 对应模型相当的结果，并且比基线 SNN 方法具有更快的收敛速度。这一改进伴随着在 45nm 硬件上估计的动态能耗降低了 94.3%。

##### **Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory**
2409.11192v1 by Eunhae Lee

One application area of long-term memory (LTM) capabilities with increasing
traction is personal AI companions and assistants. With the ability to retain
and contextualize past interactions and adapt to user preferences, personal AI
companions and assistants promise a profound shift in how we interact with AI
and are on track to become indispensable in personal and professional settings.
However, this advancement introduces new challenges and vulnerabilities that
require careful consideration regarding the deployment and widespread use of
these systems. The goal of this paper is to explore the broader implications of
building and deploying personal AI applications with LTM capabilities using a
holistic evaluation approach. This will be done in three ways: 1) reviewing the
technological underpinnings of LTM in Large Language Models, 2) surveying
current personal AI companions and assistants, and 3) analyzing critical
considerations and implications of deploying and using these applications.

摘要：長期記憶 (LTM) 能力的應用領域之一是個人 AI 伴侶和助理，其吸引力正與日俱增。個人 AI 伴侶和助理具備保留和將過去互動脈絡化，以及適應使用者偏好的能力，承諾將徹底改變我們與 AI 互動的方式，並有望在個人和專業領域中變得不可或缺。然而，這項進展帶來了新的挑戰和漏洞，需要仔細考量這些系統的部署和廣泛使用。本文的目標是利用整體評估方法探討建構和部署具備 LTM 能力的個人 AI 應用程式的廣泛影響。這將透過三種方式進行：1) 檢視大型語言模型中 LTM 的技術基礎，2) 調查目前的個人 AI 伴侶和助理，以及 3) 分析部署和使用這些應用程式的關鍵考量和影響。

##### **SuperCoder2.0: Technical Report on Exploring the feasibility of LLMs as Autonomous Programmer**
2409.11190v1 by Anmol Gautam, Kishore Kumar, Adarsh Jha, Mukunda NS, Ishaan Bhola

We present SuperCoder2.0, an advanced autonomous system designed to enhance
software development through artificial intelligence. The system combines an
AI-native development approach with intelligent agents to enable fully
autonomous coding. Key focus areas include a retry mechanism with error output
traceback, comprehensive code rewriting and replacement using Abstract Syntax
Tree (ast) parsing to minimize linting issues, code embedding technique for
retrieval-augmented generation, and a focus on localizing methods for
problem-solving rather than identifying specific line numbers. The methodology
employs a three-step hierarchical search space reduction approach for code base
navigation and bug localization:utilizing Retrieval Augmented Generation (RAG)
and a Repository File Level Map to identify candidate files, (2) narrowing down
to the most relevant files using a File Level Schematic Map, and (3) extracting
'relevant locations' within these files. Code editing is performed through a
two-part module comprising CodeGeneration and CodeEditing, which generates
multiple solutions at different temperature values and replaces entire methods
or classes to maintain code integrity. A feedback loop executes
repository-level test cases to validate and refine solutions. Experiments
conducted on the SWE-bench Lite dataset demonstrate SuperCoder2.0's
effectiveness, achieving correct file localization in 84.33% of cases within
the top 5 candidates and successfully resolving 34% of test instances. This
performance places SuperCoder2.0 fourth globally on the SWE-bench leaderboard.
The system's ability to handle diverse repositories and problem types
highlights its potential as a versatile tool for autonomous software
development. Future work will focus on refining the code editing process and
exploring advanced embedding models for improved natural language to code
mapping.

摘要：<paragraph>我們提出 SuperCoder2.0，一個先進的自主系統，旨在透過人工智慧提升軟體開發。該系統結合了原生 AI 開發方法與智慧代理人，以實現完全自主編碼。主要的重點領域包括具有錯誤輸出追蹤的重試機制、使用抽象語法樹 (ast) 解析的全面程式碼改寫和替換，以最小化程式碼檢查問題、用於檢索擴充產生技術的程式碼嵌入技術，以及專注於定位方法以解決問題，而非識別特定行號。該方法採用三步驟階層式搜尋空間縮減方法，用於程式碼庫導覽和錯誤定位：利用檢索擴充產生 (RAG) 和儲存庫檔案層級對應，以識別候選檔案，(2) 使用檔案層級概觀對應縮小範圍至最相關的檔案，以及 (3) 從這些檔案中擷取「相關位置」。程式碼編輯透過包含程式碼產生和程式碼編輯的兩部分模組執行，該模組會在不同的溫度值產生多重解，並替換整個方法或類別以維護程式碼完整性。回饋迴路會執行儲存庫層級測試案例，以驗證並改善解。在 SWE-bench Lite 資料集上執行的實驗證明了 SuperCoder2.0 的效能，在 84.33% 的案例中，於前 5 名候選者中達成正確檔案定位，並成功解決 34% 的測試案例。此效能讓 SuperCoder2.0 在 SWE-bench 排行榜上排名全球第四。該系統處理不同儲存庫和問題類型的能力，突顯了其作為自主軟體開發通用工具的潛力。未來的研究將專注於改善程式碼編輯流程，並探索進階嵌入模型，以提升自然語言對程式碼的對應。</paragraph>

##### **Identifying Influential nodes in Brain Networks via Self-Supervised Graph-Transformer**
2409.11174v1 by Yanqing Kang, Di Zhu, Haiyang Zhang, Enze Shi, Sigang Yu, Jinru Wu, Xuhui Wang, Xuan Liu, Geng Chen, Xi Jiang, Tuo Zhang, Shu Zhang

Studying influential nodes (I-nodes) in brain networks is of great
significance in the field of brain imaging. Most existing studies consider
brain connectivity hubs as I-nodes. However, this approach relies heavily on
prior knowledge from graph theory, which may overlook the intrinsic
characteristics of the brain network, especially when its architecture is not
fully understood. In contrast, self-supervised deep learning can learn
meaningful representations directly from the data. This approach enables the
exploration of I-nodes for brain networks, which is also lacking in current
studies. This paper proposes a Self-Supervised Graph Reconstruction framework
based on Graph-Transformer (SSGR-GT) to identify I-nodes, which has three main
characteristics. First, as a self-supervised model, SSGR-GT extracts the
importance of brain nodes to the reconstruction. Second, SSGR-GT uses
Graph-Transformer, which is well-suited for extracting features from brain
graphs, combining both local and global characteristics. Third, multimodal
analysis of I-nodes uses graph-based fusion technology, combining functional
and structural brain information. The I-nodes we obtained are distributed in
critical areas such as the superior frontal lobe, lateral parietal lobe, and
lateral occipital lobe, with a total of 56 identified across different
experiments. These I-nodes are involved in more brain networks than other
regions, have longer fiber connections, and occupy more central positions in
structural connectivity. They also exhibit strong connectivity and high node
efficiency in both functional and structural networks. Furthermore, there is a
significant overlap between the I-nodes and both the structural and functional
rich-club. These findings enhance our understanding of the I-nodes within the
brain network, and provide new insights for future research in further
understanding the brain working mechanisms.

摘要：<paragraph>在腦部影像領域中，研究腦網路中的影響力節點（I 節點）具有重要意義。現有研究大多將腦部連接樞紐視為 I 節點。然而，這種方法嚴重依賴於圖論的先驗知識，可能會忽略腦網路的內在特徵，特別是在其架構尚未完全了解的情況下。相比之下，自監督深度學習可以直接從數據中學習有意義的表示。這種方法能夠探索腦網路的 I 節點，這也是當前研究中所欠缺的。本文提出一個基於圖形轉換器的自監督圖形重建框架（SSGR-GT）來識別 I 節點，其具有三大特點。首先，作為一個自監督模型，SSGR-GT 提取腦節點對重建的重要性。其次，SSGR-GT 使用圖形轉換器，它非常適合從腦圖形中提取特徵，結合局部和全局特徵。第三，I 節點的多模態分析使用基於圖形的融合技術，結合功能和結構腦部資訊。我們獲得的 I 節點分佈在關鍵區域，例如額上葉、頂葉外側和枕葉外側，在不同的實驗中總共識別出 56 個。這些 I 節點參與的腦網路比其他區域更多，具有更長的纖維連接，並且在結構連接中佔據更中心的位置。它們在功能和結構網路中也表現出強連接性和高節點效率。此外，I 節點與結構和功能富俱樂部之間存在顯著重疊。這些發現增強了我們對腦網路中 I 節點的理解，並為進一步了解腦工作機制提供了新的見解。</paragraph>

##### **SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with Customisable Fairness Calibration**
2409.11149v1 by Xin Guan, Nathaniel Demchak, Saloni Gupta, Ze Wang, Ediz Ertekin Jr., Adriano Koshiyama, Emre Kazim, Zekun Wu

The development of unbiased large language models is widely recognized as
crucial, yet existing benchmarks fall short in detecting biases due to limited
scope, contamination, and lack of a fairness baseline. SAGED(-Bias) is the
first holistic benchmarking pipeline to address these problems. The pipeline
encompasses five core stages: scraping materials, assembling benchmarks,
generating responses, extracting numeric features, and diagnosing with
disparity metrics. SAGED includes metrics for max disparity, such as impact
ratio, and bias concentration, such as Max Z-scores. Noticing that assessment
tool bias and contextual bias in prompts can distort evaluation, SAGED
implements counterfactual branching and baseline calibration for mitigation.
For demonstration, we use SAGED on G20 Countries with popular 8b-level models
including Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we
find that while Mistral and Qwen2 show lower max disparity and higher bias
concentration than Gemma2 and Llama3.1, all models are notably biased against
countries like Russia and (except for Qwen2) China. With further experiments to
have models role-playing U.S. (vice-/former-) presidents, we see bias amplifies
and shifts in heterogeneous directions. Moreover, we see Qwen2 and Mistral not
engage in role-playing, while Llama3.1 and Gemma2 role-play Trump notably more
intensively than Biden and Harris, indicating role-playing performance bias in
these models.

摘要：<paragraph>無偏見大型語言模型的發展被廣泛認為至關重要，但現有的基準在檢測偏見方面存在不足，原因在於範圍有限、汙染以及缺乏公平基準。SAGED(-Bias) 是第一個解決這些問題的整體基準管道。該管道包含五個核心階段：擷取素材、組裝基準、產生回應、萃取數值特徵，以及使用差異指標診斷。SAGED 包含最大差異指標，例如影響比率，以及偏見集中度指標，例如最大 Z 分數。SAGED 觀察到評量工具偏見和提示中的脈絡偏見會扭曲評量，因此實作反事實分支和基準校正以進行緩解。為了示範，我們在 G20 國家使用 SAGED，其中包括熱門的 8b 級別模型，例如 Gemma2、Llama3.1、Mistral 和 Qwen2。透過情緒分析，我們發現，儘管 Mistral 和 Qwen2 顯示出比 Gemma2 和 Llama3.1 更低的最大差異和更高的偏見集中度，但所有模型都明顯偏向於俄羅斯等國家，而 Qwen2 除外，則偏向於中國。透過進一步的實驗讓模型扮演美國（現任/前任）總統，我們看到偏見會在異質方向上擴大和轉移。此外，我們看到 Qwen2 和 Mistral 沒有參與角色扮演，而 Llama3.1 和 Gemma2 扮演川普的角色明顯比拜登和賀錦麗更為深入，這表示這些模型存在角色扮演表現偏見。</paragraph>

##### **Improving the Efficiency of Visually Augmented Language Models**
2409.11148v1 by Paula Ontalvilla, Aitor Ormazabal, Gorka Azkune

Despite the impressive performance of autoregressive Language Models (LM) it
has been shown that due to reporting bias, LMs lack visual knowledge, i.e. they
do not know much about the visual world and its properties. To augment LMs with
visual knowledge, existing solutions often rely on explicit images, requiring
time-consuming retrieval or image generation systems. This paper shows that
explicit images are not necessary to visually augment an LM. Instead, we use
visually-grounded text representations obtained from the well-known CLIP
multimodal system. For a fair comparison, we modify VALM, a visually-augmented
LM which uses image retrieval and representation, to work directly with
visually-grounded text representations. We name this new model BLIND-VALM. We
show that BLIND-VALM performs on par with VALM for Visual Language
Understanding (VLU), Natural Language Understanding (NLU) and Language Modeling
tasks, despite being significantly more efficient and simpler. We also show
that scaling up our model within the compute budget of VALM, either increasing
the model or pre-training corpus size, we outperform VALM for all the
evaluation tasks.

摘要：儘管自迴歸語言模型 (LM) 具有令人印象深刻的效能，但已顯示出由於回報偏差，LM 缺乏視覺知識，亦即它們對視覺世界及其屬性所知甚少。為了擴充 LM 的視覺知識，現有的解決方案通常依賴明確的影像，需要耗時的擷取或影像產生系統。本文顯示明確的影像並非視覺擴充 LM 所必需。相反地，我們使用從著名的 CLIP 多模態系統取得的視覺基礎文字表徵。為了公平比較，我們修改了視覺擴充 LM VALM，它使用影像擷取和表徵，以直接使用視覺基礎文字表徵。我們將這個新模型命名為 BLIND-VALM。我們顯示 BLIND-VALM 在視覺語言理解 (VLU)、自然語言理解 (NLU) 和語言模型任務中，表現與 VALM 相當，儘管它的效率顯著更高且更簡單。我們也顯示在 VALM 的運算預算內擴充我們的模型，無論是增加模型或預訓練語料庫大小，我們在所有評估任務中都優於 VALM。

##### **Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**
2409.11147v1 by Yukang Lin, Bingchen Zhong, Shuoran Jiang, Joanna Siebert, Qingcai Chen

Large language models(LLMs) have exhibited remarkable few-shot learning
capabilities and unified the paradigm of NLP tasks through the in-context
learning(ICL) technique. Despite the success of ICL, the quality of the
exemplar demonstrations can significantly influence the LLM's performance.
Existing exemplar selection methods mainly focus on the semantic similarity
between queries and candidate exemplars. On the other hand, the logical
connections between reasoning steps can be beneficial to depict the
problem-solving process as well. In this paper, we proposes a novel method
named Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM
to generate an initial response, then expresses intermediate problem-solving
steps to a graph structure. After that, it employs graph kernel to select
exemplars with semantic and structural similarity. Extensive experiments
demonstrate the structural relationship is helpful to the alignment of queries
and candidate exemplars. The efficacy of RGER on math and logit reasoning tasks
showcases its superiority over state-of-the-art retrieval-based approaches. Our
code is released at https://github.com/Yukang-Lin/RGER.

摘要：大型語言模型 (LLM) 已展現出卓越的少量學習能力，並透過情境學習 (ICL) 技術統一了 NLP 任務的範例。儘管 ICL 已成功，範例示範的品質會顯著影響 LLM 的效能。現有的範例選擇方法主要著重於查詢與候選範例之間的語意相似性。另一方面，推理步驟之間的邏輯連結有助於描繪問題解決流程。在本文中，我們提出了一種稱為推理圖增強範例檢索 (RGER) 的新方法。RGER 首先要求 LLM 產生一個初始回應，然後將中間問題解決步驟表示為圖形結構。之後，它採用圖形核選取具有語意和結構相似性的範例。廣泛的實驗證明，結構關係有助於查詢和候選範例的對齊。RGER 在數學和邏輯推理任務上的功效展示了它優於最先進的基於檢索的方法。我們的程式碼已發布於 https://github.com/Yukang-Lin/RGER。

##### **High-Resolution Speech Restoration with Latent Diffusion Model**
2409.11145v1 by Tushar Dhyani, Florian Lux, Michele Mancusi, Giorgio Fabbro, Fritz Hohl, Ngoc Thang Vu

Traditional speech enhancement methods often oversimplify the task of
restoration by focusing on a single type of distortion. Generative models that
handle multiple distortions frequently struggle with phone reconstruction and
high-frequency harmonics, leading to breathing and gasping artifacts that
reduce the intelligibility of reconstructed speech. These models are also
computationally demanding, and many solutions are restricted to producing
outputs in the wide-band frequency range, which limits their suitability for
professional applications. To address these challenges, we propose Hi-ResLDM, a
novel generative model based on latent diffusion designed to remove multiple
distortions and restore speech recordings to studio quality, sampled at 48kHz.
We benchmark Hi-ResLDM against state-of-the-art methods that leverage GAN and
Conditional Flow Matching (CFM) components, demonstrating superior performance
in regenerating high-frequency-band details. Hi-ResLDM not only excels in
non-instrusive metrics but is also consistently preferred in human evaluation
and performs competitively on intrusive evaluations, making it ideal for
high-resolution speech restoration.

摘要：傳統的語音增強方法通常過度簡化復原任務，專注於單一類型的失真。處理多重失真的生成模型經常在語音重建和高頻諧波方面遇到困難，導致喘息和喘氣的偽影，降低了重建語音的可懂度。這些模型在計算上也很要求，許多解決方案僅限於產生寬頻頻率範圍的輸出，這限制了它們在專業應用中的適用性。為了應對這些挑戰，我們提出了 Hi-ResLDM，一種基於潛在擴散的新型生成模型，旨在消除多重失真並將語音錄音恢復到錄音室品質，採樣率為 48kHz。我們將 Hi-ResLDM 與利用 GAN 和條件流匹配 (CFM) 組件的最新方法進行基準測試，證明了在再生高頻帶細節方面的卓越性能。Hi-ResLDM 不僅在非侵入性指標中表現出色，而且在人類評估中也始終如一地受到青睞，並且在侵入性評估中表現出色，使其成為高解析度語音還原的理想選擇。

##### **Semformer: Transformer Language Models with Semantic Planning**
2409.11143v1 by Yongjing Yin, Junran Ding, Kai Song, Yue Zhang

Next-token prediction serves as the dominant component in current neural
language models. During the training phase, the model employs teacher forcing,
which predicts tokens based on all preceding ground truth tokens. However, this
approach has been found to create shortcuts, utilizing the revealed prefix to
spuriously fit future tokens, potentially compromising the accuracy of the
next-token predictor. In this paper, we introduce Semformer, a novel method of
training a Transformer language model that explicitly models the semantic
planning of response. Specifically, we incorporate a sequence of planning
tokens into the prefix, guiding the planning token representations to predict
the latent semantic representations of the response, which are induced by an
autoencoder. In a minimal planning task (i.e., graph path-finding), our model
exhibits near-perfect performance and effectively mitigates shortcut learning,
a feat that standard training methods and baseline models have been unable to
accomplish. Furthermore, we pretrain Semformer from scratch with 125M
parameters, demonstrating its efficacy through measures of perplexity,
in-context learning, and fine-tuning on summarization tasks.

摘要：在當前的語言模型中，下一個詞彙預測是主導組成部分。在訓練階段，模型採用教師強制法，根據所有前一個的真實詞彙來預測詞彙。然而，發現這種方法會產生捷徑，利用已揭露的前綴來虛假地符合後續的詞彙，潛在會危害下一個詞彙預測器的準確度。在本文中，我們介紹 Semformer，一種訓練 Transformer 語言模型的新方法，明確地建構回應的語意規劃。具體來說，我們將一系列規劃詞彙納入前綴，引導規劃詞彙的表徵去預測回應的潛在語意表徵，這些表徵是由自動編碼器誘導的。在一個最小的規劃任務（即圖形路徑尋找）中，我們的模型表現出接近完美的效能，並有效地減輕捷徑學習，這是標準訓練方法和基線模型無法達成的壯舉。此外，我們從頭開始使用 1.25 億個參數預訓練 Semformer，透過困惑度、語境學習和在摘要任務上的微調來證明其功效。

##### **Learning Generalized Hamiltonians using fully Symplectic Mappings**
2409.11138v1 by Harsh Choudhary, Chandan Gupta, Vyacheslav kungrutsev, Melvin Leok, Georgios Korpas

Many important physical systems can be described as the evolution of a
Hamiltonian system, which has the important property of being conservative,
that is, energy is conserved throughout the evolution. Physics Informed Neural
Networks and in particular Hamiltonian Neural Networks have emerged as a
mechanism to incorporate structural inductive bias into the NN model. By
ensuring physical invariances are conserved, the models exhibit significantly
better sample complexity and out-of-distribution accuracy than standard NNs.
Learning the Hamiltonian as a function of its canonical variables, typically
position and velocity, from sample observations of the system thus becomes a
critical task in system identification and long-term prediction of system
behavior. However, to truly preserve the long-run physical conservation
properties of Hamiltonian systems, one must use symplectic integrators for a
forward pass of the system's simulation. While symplectic schemes have been
used in the literature, they are thus far limited to situations when they
reduce to explicit algorithms, which include the case of separable Hamiltonians
or augmented non-separable Hamiltonians. We extend it to generalized
non-separable Hamiltonians, and noting the self-adjoint property of symplectic
integrators, we bypass computationally intensive backpropagation through an ODE
solver. We show that the method is robust to noise and provides a good
approximation of the system Hamiltonian when the state variables are sampled
from a noisy observation. In the numerical results, we show the performance of
the method concerning Hamiltonian reconstruction and conservation, indicating
its particular advantage for non-separable systems.

摘要：許多重要的物理系統都可以描述為哈密頓系統的演化，它具有守恆的重要性質，也就是說，能量在整個演化過程中守恆。物理訊息神經網路，特別是哈密頓神經網路，已成為將結構歸納偏差納入神經網路模型的一種機制。通過確保物理不變量守恆，這些模型展現出比標準神經網路顯著更好的樣本複雜度和分佈外準確度。從系統的樣本觀測中學習哈密頓量作為其正則變數（通常是位置和速度）的函數，因此成為系統辨識和系統行為長期預測中的一項關鍵任務。然而，為了真正保留哈密頓系統的長期物理守恆性質，必須對系統模擬的正向傳遞使用辛積分器。雖然文獻中已經使用了辛格式，但到目前為止，它們僅限於它們簡化為明確演算法的情況，其中包括可分離哈密頓量或擴增的不可分離哈密頓量。我們將其擴展到廣義的不可分離哈密頓量，並注意到辛積分器的自伴隨性質，我們繞過透過常微分方程求解器進行計算密集的反向傳播。我們表明，該方法對雜訊具有魯棒性，並且當狀態變數從雜訊觀測中取樣時，可以提供系統哈密頓量的良好近似值。在數值結果中，我們展示了該方法在哈密頓量重建和守恆方面的效能，表明其對不可分離系統的特殊優勢。

##### **Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models**
2409.11136v1 by Orion Weller, Benjamin Van Durme, Dawn Lawrie, Ashwin Paranjape, Yuhao Zhang, Jack Hessel

Instruction-tuned language models (LM) are able to respond to imperative
commands, providing a more natural user interface compared to their base
counterparts. In this work, we present Promptriever, the first retrieval model
able to be prompted like an LM. To train Promptriever, we curate and release a
new instance-level instruction training set from MS MARCO, spanning nearly 500k
instances. Promptriever not only achieves strong performance on standard
retrieval tasks, but also follows instructions. We observe: (1) large gains
(reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR /
+3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical
choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR),
and (3) the ability to perform hyperparameter search via prompting to reliably
improve retrieval performance (+1.4 average increase on BEIR). Promptriever
demonstrates that retrieval models can be controlled with prompts on a
per-query basis, setting the stage for future work aligning LM prompting
techniques with information retrieval.

摘要：指令調整語言模型 (LM) 能夠回應命令式指令，與其基礎對應物相比，提供了更自然的使用者介面。在這項工作中，我們展示了 Promptriever，第一個能夠像 LM 一樣提示的檢索模型。為了訓練 Promptriever，我們從 MS MARCO 整理並釋出一個新的實例層級指令訓練集，涵蓋了將近 50 萬個實例。Promptriever 不僅在標準檢索任務中獲得強勁的效能，還能遵循指令。我們觀察到：(1) 在遵循詳細相關性指令方面獲得大幅提升（達到 SoTA，在 FollowIR 上的 p-MRR 增加 +14.3 / nDCG 增加 +3.1），(2) 在查詢+指令中的字彙選擇/措辭方面大幅提升健壯性（在 InstructIR 上的 Robustness@10 增加 +12.9），以及 (3) 能夠透過提示執行超參數搜尋，以可靠地改善檢索效能（在 BEIR 上平均增加 +1.4）。Promptriever 證明了檢索模型可以用提示在每個查詢的基礎上進行控制，為未來的工作奠定了基礎，將 LM 提示技術與資訊檢索結合起來。

##### **Gradient-free Post-hoc Explainability Using Distillation Aided Learnable Approach**
2409.11123v1 by Debarpan Bhattacharya, Amir H. Poorjam, Deepak Mittal, Sriram Ganapathy

The recent advancements in artificial intelligence (AI), with the release of
several large models having only query access, make a strong case for
explainability of deep models in a post-hoc gradient free manner. In this
paper, we propose a framework, named distillation aided explainability (DAX),
that attempts to generate a saliency-based explanation in a model agnostic
gradient free application. The DAX approach poses the problem of explanation in
a learnable setting with a mask generation network and a distillation network.
The mask generation network learns to generate the multiplier mask that finds
the salient regions of the input, while the student distillation network aims
to approximate the local behavior of the black-box model. We propose a joint
optimization of the two networks in the DAX framework using the locally
perturbed input samples, with the targets derived from input-output access to
the black-box model. We extensively evaluate DAX across different modalities
(image and audio), in a classification setting, using a diverse set of
evaluations (intersection over union with ground truth, deletion based and
subjective human evaluation based measures) and benchmark it with respect to
$9$ different methods. In these evaluations, the DAX significantly outperforms
the existing approaches on all modalities and evaluation metrics.

摘要：隨著人工智慧 (AI) 的最新進展，以及發布了幾種僅具有查詢存取權的大型模型，這為事後無梯度深度模型的可解釋性提供了強有力的論據。在本文中，我們提出了一個名為蒸餾輔助可解釋性 (DAX) 的框架，它嘗試在與模型無關的無梯度應用中產生基於顯著性的解釋。DAX 方法以可學習的設定提出了解釋問題，並使用遮罩生成網路和蒸餾網路。遮罩生成網路會學習產生倍率遮罩，以找出輸入的顯著區域，而學生蒸餾網路則旨在近似黑盒模型的局部行為。我們提出在 DAX 框架中對兩個網路進行聯合最佳化，使用局部擾動的輸入樣本，目標源自對黑盒模型的輸入輸出存取。我們在分類設定中，使用多種評估（與真實情況的交集並集、基於刪除和基於主觀人類評估的測量）廣泛評估不同模式（影像和音訊）中的 DAX，並針對 $9$ 種不同的方法對其進行評量。在這些評估中，DAX 在所有模式和評估指標上都顯著優於現有方法。

##### **Diversity-grounded Channel Prototypical Learning for Out-of-Distribution Intent Detection**
2409.11114v1 by Bo Liu, Liming Zhan, Yujie Feng, Zexin Lu, Chengqiang Xie, Lei Xue, Xiao-Ming Wu, Albert Y. S. Lam

In the realm of task-oriented dialogue systems, a robust intent detection
mechanism must effectively handle malformed utterances encountered in
real-world scenarios. This study presents a novel fine-tuning framework for
large language models (LLMs) aimed at enhancing in-distribution (ID) intent
classification and out-of-distribution (OOD) intent detection, which utilizes
semantic matching with prototypes derived from ID class names. By harnessing
the highly distinguishable representations of LLMs, we construct semantic
prototypes for each ID class using a diversity-grounded prompt tuning approach.
We rigorously test our framework in a challenging OOD context, where ID and OOD
classes are semantically close yet distinct, referred to as \emph{near} OOD
detection. For a thorough assessment, we benchmark our method against the
prevalent fine-tuning approaches. The experimental findings reveal that our
method demonstrates superior performance in both few-shot ID intent
classification and near-OOD intent detection tasks.

摘要：在面向任務的對話系統領域中，強健的意圖偵測機制必須有效地處理在真實世界場景中遇到的格式錯誤的語句。本研究提出了一個針對大型語言模型 (LLM) 的新微調框架，旨在增強分布內 (ID) 意圖分類和分布外 (OOD) 意圖偵測，它利用語義匹配與從 ID 類別名稱衍生的原型。透過利用 LLM 極具區別性的表示，我們使用基於多樣性的提示調整方法為每個 ID 類別建構語義原型。我們在具有挑戰性的 OOD 環境中嚴格測試我們的框架，其中 ID 和 OOD 類別在語義上接近但卻不同，稱為\emph{near} OOD 偵測。為了進行徹底的評估，我們將我們的模型與流行的微調方法進行基準測試。實驗結果顯示，我們的模型在少次嘗試的 ID 意圖分類和 near-OOD 意圖偵測任務中都展現出優異的效能。

##### **Strategic Insights in Human and Large Language Model Tactics at Word Guessing Games**
2409.11112v1 by Matīss Rikters, Sanita Reinsone

At the beginning of 2022, a simplistic word-guessing game took the world by
storm and was further adapted to many languages beyond the original English
version. In this paper, we examine the strategies of daily word-guessing game
players that have evolved during a period of over two years. A survey gathered
from 25% of frequent players reveals their strategies and motivations for
continuing the daily journey. We also explore the capability of several popular
open-access large language model systems and open-source models at
comprehending and playing the game in two different languages. Results
highlight the struggles of certain models to maintain correct guess length and
generate repetitions, as well as hallucinations of non-existent words and
inflections.

摘要：2022 年初，一款簡化的猜字遊戲席捲全球，並在原版的英文版本之外被改編為許多語言。在本文中，我們探討了在兩年多的時間裡演變出的每日猜字遊戲玩家的策略。一項調查收集了 25% 的常客玩家，揭示了他們繼續每日旅程的策略和動機。我們還探討了幾種流行的開放式大型語言模型系統和開源模型在理解和玩兩種不同語言的遊戲方面的能力。結果強調了某些模型在保持正確猜測長度和產生重複以及對不存在的單詞和變形的幻覺方面遇到的困難。

##### **MonoKAN: Certified Monotonic Kolmogorov-Arnold Network**
2409.11078v1 by Alejandro Polo-Molina, David Alfaya, Jose Portela

Artificial Neural Networks (ANNs) have significantly advanced various fields
by effectively recognizing patterns and solving complex problems. Despite these
advancements, their interpretability remains a critical challenge, especially
in applications where transparency and accountability are essential. To address
this, explainable AI (XAI) has made progress in demystifying ANNs, yet
interpretability alone is often insufficient. In certain applications, model
predictions must align with expert-imposed requirements, sometimes exemplified
by partial monotonicity constraints. While monotonic approaches are found in
the literature for traditional Multi-layer Perceptrons (MLPs), they still face
difficulties in achieving both interpretability and certified partial
monotonicity. Recently, the Kolmogorov-Arnold Network (KAN) architecture, based
on learnable activation functions parametrized as splines, has been proposed as
a more interpretable alternative to MLPs. Building on this, we introduce a
novel ANN architecture called MonoKAN, which is based on the KAN architecture
and achieves certified partial monotonicity while enhancing interpretability.
To achieve this, we employ cubic Hermite splines, which guarantee monotonicity
through a set of straightforward conditions. Additionally, by using positive
weights in the linear combinations of these splines, we ensure that the network
preserves the monotonic relationships between input and output. Our experiments
demonstrate that MonoKAN not only enhances interpretability but also improves
predictive performance across the majority of benchmarks, outperforming
state-of-the-art monotonic MLP approaches.

摘要：人工神经網路 (ANN) 透過有效辨識模式和解決複雜問題，在各個領域取得顯著進展。儘管有這些進展，其可解釋性仍然是一項重大挑戰，特別是在透明度和問責制至關重要的應用中。為了解決這個問題，可解釋 AI (XAI) 已在破解 ANN 方面取得進展，但僅有可解釋性通常是不夠的。在特定應用中，模型預測必須符合專家施加的要求，有時以部分單調性約束為例。雖然在傳統多層感知器 (MLP) 的文獻中發現了單調方法，但它們在實現可解釋性和經過認證的部分單調性方面仍然面臨困難。最近，基於可學習的激活函數參數化為樣條的 Kolmogorov-Arnold 網路 (KAN) 架構已被提出作為 MLP 的更具可解釋性的替代方案。在此基礎上，我們引入了一種稱為 MonoKAN 的新型 ANN 架構，它基於 KAN 架構，並在增強可解釋性的同時實現了經過認證的部分單調性。為達成此目的，我們採用三次 Hermite 樣條，它透過一組簡單的條件來保證單調性。此外，透過在這些樣條的線性組合中使用正權重，我們確保網路保留輸入和輸出之間的單調關係。我們的實驗表明，MonoKAN 不僅增強了可解釋性，而且還改善了大多數基準測試的預測效能，優於最先進的單調 MLP 方法。

##### **RoMath: A Mathematical Reasoning Benchmark in Romanian**
2409.11074v1 by Adrian Cosma, Ana-Maria Bucur, Emilian Radoi

Mathematics has long been conveyed through natural language, primarily for
human understanding. With the rise of mechanized mathematics and proof
assistants, there is a growing need to understand informal mathematical text,
yet most existing benchmarks focus solely on English, overlooking other
languages. This paper introduces RoMath, a Romanian mathematical reasoning
benchmark suite comprising three datasets: RoMath-Baccalaureate,
RoMath-Competitions and RoMath-Synthetic, which cover a range of mathematical
domains and difficulty levels, aiming to improve non-English language models
and promote multilingual AI development. By focusing on Romanian, a
low-resource language with unique linguistic features, RoMath addresses the
limitations of Anglo-centric models and emphasizes the need for dedicated
resources beyond simple automatic translation. We benchmark several open-weight
language models, highlighting the importance of creating resources for
underrepresented languages. We make the code and dataset available.

摘要：數學長期以來透過自然語言傳達，主要供人類理解。隨著機械化數學和證明輔助的興起，理解非正式數學文本的需求日益增加，但現有的基準測試大多只關注英文，忽略了其他語言。本文介紹 RoMath，一個羅馬尼亞數學推理基準測試套件，包含三個資料集：RoMath-Baccalaureate、RoMath-Competitions 和 RoMath-Synthetic，涵蓋範圍廣泛的數學領域和難度等級，旨在改善非英語語言模型並促進多語言 AI 開發。RoMath 專注於羅馬尼亞語，一種具有獨特語言特徵的低資源語言，解決了以英語為中心的模型的限制，並強調了除了簡單的自動翻譯之外，需要專門的資源。我們對幾個開放權重的語言模型進行基準測試，強調為代表性不足的語言建立資源的重要性。我們提供程式碼和資料集。

##### **Improve Machine Learning carbon footprint using Parquet dataset format and Mixed Precision training for regression algorithms**
2409.11071v1 by Andrew Antonopoulos

This study was the 2nd part of my dissertation for my master degree and
compared the power consumption using the Comma-Separated-Values (CSV) and
parquet dataset format with the default floating point (32bit) and Nvidia mixed
precision (16bit and 32bit) while training a regression ML model. The same
custom PC as per the 1st part, which was dedicated to the classification
testing and analysis, was built to perform the experiments, and different ML
hyper-parameters, such as batch size, neurons, and epochs, were chosen to build
Deep Neural Networks (DNN). A benchmarking test with default hyper-parameter
values for the DNN was used as a reference, while the experiments used a
combination of different settings. The results were recorded in Excel, and
descriptive statistics were chosen to calculate the mean between the groups and
compare them using graphs and tables. The outcome was positive when using mixed
precision combined with specific hyper-parameters. Compared to the
benchmarking, optimising the regression models reduced the power consumption
between 7 and 11 Watts. The regression results show that while mixed precision
can help improve power consumption, we must carefully consider the
hyper-parameters. A high number of batch sizes and neurons will negatively
affect power consumption. However, this research required inferential
statistics, specifically ANOVA and T-test, to compare the relationship between
the means. The results reported no statistical significance between the means
in the regression tests and accepted H0. Therefore, choosing different ML
techniques and the Parquet dataset format will not improve the computational
power consumption and the overall ML carbon footprint. However, a more
extensive implementation with a cluster of GPUs can increase the sample size
significantly, as it is an essential factor and can change the outcome of the
statistical analysis.

摘要：本研究是我碩士論文的第二部分，比較了使用逗號分隔值 (CSV) 和 Parquet 資料集格式與預設浮點數 (32 位元) 和 Nvidia 混合精度 (16 位元和 32 位元) 在訓練回歸 ML 模型時的功耗。與第一部分相同，專門用於分類測試和分析的客製化個人電腦用於執行實驗，並選擇不同的 ML 超參數，例如批次大小、神經元和次數，以建立深度神經網路 (DNN)。使用預設 DNN 超參數值進行基準測試作為參考，而實驗則使用不同的設定組合。結果記錄在 Excel 中，並選擇描述性統計數據來計算組間平均值，並使用圖形和表格進行比較。結合特定超參數使用混合精度時，結果為正向。與基準測試相比，最佳化回歸模型可將功耗降低 7 到 11 瓦。回歸結果顯示，雖然混合精度有助於改善功耗，但我們必須仔細考慮超參數。大量的批次大小和神經元會對功耗產生負面影響。然而，本研究需要推論統計，特別是 ANOVA 和 T 檢定，以比較平均值之間的關係。結果顯示回歸測試的平均值之間沒有統計顯著性，並接受 H0。因此，選擇不同的 ML 技術和 Parquet 資料集格式不會改善運算功耗和整體 ML 碳足跡。然而，使用 GPU 群集進行更廣泛的實作可以顯著增加樣本大小，因為這是一個重要的因素，並且可以改變統計分析的結果。

##### **KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models**
2409.11057v1 by Bo Lv, Quan Zhou, Xuanang Ding, Yan Wang, Zeming Ma

The bottleneck associated with the key-value(KV) cache presents a significant
challenge during the inference processes of large language models. While depth
pruning accelerates inference, it requires extensive recovery training, which
can take up to two weeks. On the other hand, width pruning retains much of the
performance but offers slight speed gains. To tackle these challenges, we
propose KVPruner to improve model efficiency while maintaining performance. Our
method uses global perplexity-based analysis to determine the importance ratio
for each block and provides multiple strategies to prune non-essential KV
channels within blocks. Compared to the original model, KVPruner reduces
runtime memory usage by 50% and boosts throughput by over 35%. Additionally,
our method requires only two hours of LoRA fine-tuning on small datasets to
recover most of the performance.

摘要：與鍵值 (KV) 快取相關的瓶頸在大型語言模型的推論過程中會造成重大的挑戰。儘管深度剪枝可以加速推論，但需要廣泛的恢復訓練，這可能需要長達兩週的時間。另一方面，寬度剪枝保留了大部分效能，但提供了輕微的速度提升。為了應對這些挑戰，我們提出 KVPruner，以在維持效能的同時提升模型效率。我們的做法使用基於全局困惑度的分析來確定每個區塊的重要性比例，並提供多種策略來剪除區塊中非必要的 KV 通道。與原始模型相比，KVPruner 將執行時間記憶體使用量減少了 50%，並將吞吐量提升了 35% 以上。此外，我們的做法只需要在小型資料集上進行兩小時的 LoRA 微調，就能夠恢復大部分效能。

##### **Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts**
2409.11056v1 by Teng Wang, Zhenqi He, Wing-Yin Yu, Xiaojin Fu, Xiongwei Han

With the advent of Large Language Models (LLMs), generating rule-based data
for real-world applications has become more accessible. Due to the inherent
ambiguity of natural language and the complexity of rule sets, especially in
long contexts, LLMs often struggle to follow all specified rules, frequently
omitting at least one. To enhance the reasoning and understanding of LLMs on
long and complex contexts, we propose a novel prompting strategy Multi-Lingual
Prompt, namely MLPrompt, which automatically translates the error-prone rule
that an LLM struggles to follow into another language, thus drawing greater
attention to it. Experimental results on public datasets across various tasks
have shown MLPrompt can outperform state-of-the-art prompting methods such as
Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we
introduce a framework integrating MLPrompt with an auto-checking mechanism for
structured data generation, with a specific case study in text-to-MIP
instances. Further, we extend the proposed framework for text-to-SQL to
demonstrate its generation ability towards structured data synthesis.

摘要：隨著大型語言模型 (LLM) 的出現，生成基於規則的資料
對於真實世界的應用程式來說變得更容易取得。由於自然語言的內在
模糊性和規則集的複雜性，特別是在長語境中，LLM 常常難以遵循所有指定的規則，經常
至少遺漏一個。為了增強 LLM 在長且複雜的語境中的推理和理解，我們提出一個新穎的提示策略多語言
提示，即 MLPrompt，它自動將 LLM 難以遵循的容易出錯的規則翻譯成另一種語言，從而引起更大的
注意它。在各種任務的公用資料集上的實驗結果顯示，MLPrompt 可以優於最先進的提示方法，例如
思考鏈、思考樹和自我一致性。此外，我們
引入了將 MLPrompt 與自動檢查機制整合的框架，用於結構化資料生成，並以文字轉換為 MIP
實例為具體案例研究。此外，我們擴充了建議的文字轉換為 SQL 框架，以展示其對結構化資料合成的生成能力。

##### **A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B**
2409.11055v1 by Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon

Prior research works have evaluated quantized LLMs using limited metrics such
as perplexity or a few basic knowledge tasks and old datasets. Additionally,
recent large-scale models such as Llama 3.1 with up to 405B have not been
thoroughly examined. This paper evaluates the performance of instruction-tuned
LLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on
models ranging from 7B to 405B. Using 13 benchmarks, we assess performance
across six task types: commonsense Q\&A, knowledge and language understanding,
instruction following, hallucination detection, mathematics, and dialogue. Our
key findings reveal that (1) quantizing a larger LLM to a similar size as a
smaller FP16 LLM generally performs better across most benchmarks, except for
hallucination detection and instruction following; (2) performance varies
significantly with different quantization methods, model size, and bit-width,
with weight-only methods often yielding better results in larger models; (3)
task difficulty does not significantly impact accuracy degradation due to
quantization; and (4) the MT-Bench evaluation method has limited discriminatory
power among recent high-performing LLMs.

摘要：先前的研究工作已使用有限的指標，例如困惑度或一些基本的知識任務和舊的資料集，來評估量化的 LLM。此外，像 Llama 3.1 這樣高達 405B 的最新大型模型尚未經過徹底檢查。本文評估了從 7B 到 405B 的模型在各種量化方法（GPTQ、AWQ、SmoothQuant 和 FP8）上，經過指令調整的 LLM 的效能。使用 13 個基準，我們評估了六種任務類型的效能：常識問答、知識和語言理解、指令遵循、幻覺檢測、數學和對話。我們的關鍵發現顯示，（1）將較大的 LLM 量化為與較小的 FP16 LLM 相似的尺寸，通常在除幻覺檢測和指令遵循之外的大多數基準上表現得更好；（2）效能會隨著不同的量化方法、模型大小和位元寬度而有顯著差異，僅權重的量化方法通常在較大的模型中產生更好的結果；（3）任務難度不會顯著影響量化造成的準確度下降；（4）MT-Bench 評估方法在最近效能良好的 LLM 中具有有限的區分能力。

##### **Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming**
2409.11041v2 by Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen

While there has been a lot of research recently on robots in household
environments, at the present time, most robots in existence can be found on
shop floors, and most interactions between humans and robots happen there.
``Collaborative robots'' (cobots) designed to work alongside humans on assembly
lines traditionally require expert programming, limiting ability to make
changes, or manual guidance, limiting expressivity of the resulting programs.
To address these limitations, we explore using Large Language Models (LLMs),
and in particular, their abilities of doing in-context learning, for
conversational code generation. As a first step, we define RATS, the
``Repetitive Assembly Task'', a 2D building task designed to lay the foundation
for simulating industry assembly scenarios. In this task, a `programmer'
instructs a cobot, using natural language, on how a certain assembly is to be
built; that is, the programmer induces a program, through natural language. We
create a dataset that pairs target structures with various example instructions
(human-authored, template-based, and model-generated) and example code. With
this, we systematically evaluate the capabilities of state-of-the-art LLMs for
synthesising this kind of code, given in-context examples. Evaluating in a
simulated environment, we find that LLMs are capable of generating accurate
`first order code' (instruction sequences), but have problems producing
`higher-order code' (abstractions such as functions, or use of loops).

摘要：<paragraph>儘管最近在家庭環境中對機器人的研究很多，但目前為止，大多數現存的機器人都可以在商店樓層中找到，而且人類與機器人之間的大多數互動都發生在那裡。傳統上，設計為與人類在組裝線上並肩工作的「協作機器人」（協作機器人）需要專家程式設計，限制了做出變更的能力，或手動指導，限制了所產生程式的表達能力。為了解決這些限制，我們探索使用大型語言模型 (LLM)，特別是它們在情境學習中的能力，用於對話式程式碼產生。作為第一步，我們定義 RATS，「重複組裝任務」，一個 2D 建構任務，旨在為模擬產業組裝場景奠定基礎。在這個任務中，一個「程式設計師」使用自然語言指導協作機器人如何建構某個組件；也就是說，程式設計師透過自然語言誘導一個程式。我們建立一個將目標結構與各種範例指令（人工撰寫、基於範本和模型產生）和範例程式碼配對的資料集。有了這個，我們系統性地評估最先進的 LLM 在給予情境範例的情況下綜合此類程式碼的能力。在模擬環境中評估，我們發現 LLM 能夠產生準確的「一階程式碼」（指令序列），但產生「高階程式碼」（例如函數或迴圈使用的抽象）時有問題。</paragraph>

##### **Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI**
2409.11032v1 by Riona Matsuoka, Hiroki Matsumoto, Takahiro Yoshida, Tomohiro Watanabe, Ryoma Kondo, Ryohei Hisano

Written texts reflect an author's perspective, making the thorough analysis
of literature a key research method in fields such as the humanities and social
sciences. However, conventional text mining techniques like sentiment analysis
and topic modeling are limited in their ability to capture the hierarchical
narrative structures that reveal deeper argumentative patterns. To address this
gap, we propose a method that leverages large language models (LLMs) to extract
and organize these structures into a hierarchical framework. We validate this
approach by analyzing public opinions on generative AI collected by Japan's
Agency for Cultural Affairs, comparing the narratives of supporters and
critics. Our analysis provides clearer visualization of the factors influencing
divergent opinions on generative AI, offering deeper insights into the
structures of agreement and disagreement.

摘要：書面文本反映作者的觀點，因此徹底分析文學成為人文學科和社會科學等領域的一種關鍵研究方法。然而，傳統的文本探勘技術（例如情緒分析和主題建模）在捕捉揭示更深層論證模式的分層敘事結構方面能力有限。為了解決這個差距，我們提出了一種方法，利用大型語言模型 (LLM) 將這些結構提取並組織成一個分層框架。我們通過分析日本文化廳收集的關於生成式 AI 的公眾意見來驗證這種方法，比較支持者和批評者的敘述。我們的分析提供了更清晰的視覺化，說明了影響對生成式 AI 不同意見的因素，對同意和不同意的結構提供了更深入的見解。

##### **D2Vformer: A Flexible Time Series Prediction Model Based on Time Position Embedding**
2409.11024v1 by Xiaobao Song, Hao Wang, Liwei Deng, Yuxin He, Wenming Cao, Chi-Sing Leungc

Time position embeddings capture the positional information of time steps,
often serving as auxiliary inputs to enhance the predictive capabilities of
time series models. However, existing models exhibit limitations in capturing
intricate time positional information and effectively utilizing these
embeddings. To address these limitations, this paper proposes a novel model
called D2Vformer. Unlike typical prediction methods that rely on RNNs or
Transformers, this approach can directly handle scenarios where the predicted
sequence is not adjacent to the input sequence or where its length dynamically
changes. In comparison to conventional methods, D2Vformer undoubtedly saves a
significant amount of training resources. In D2Vformer, the Date2Vec module
uses the timestamp information and feature sequences to generate time position
embeddings. Afterward, D2Vformer introduces a new fusion block that utilizes an
attention mechanism to explore the similarity in time positions between the
embeddings of the input sequence and the predicted sequence, thereby generating
predictions based on this similarity. Through extensive experiments on six
datasets, we demonstrate that Date2Vec outperforms other time position
embedding methods, and D2Vformer surpasses state-of-the-art methods in both
fixed-length and variable-length prediction tasks.

摘要：時間位置嵌入擷取時間步驟的位置資訊，
通常作為輔助輸入，以增強時間序列模型的預測能力。
然而，現有模型在擷取複雜的時間位置資訊和有效利用這些
嵌入方面表現出限制。為了解決這些限制，本文提出了一個名為 D2Vformer 的新模型。
與依賴 RNN 或 Transformer 的典型預測方法不同，這種方法可以直接處理預測序列不鄰近輸入序列或其長度動態變化的場景。
與傳統方法相比，D2Vformer 無疑節省了大量的訓練資源。在 D2Vformer 中，Date2Vec 模組使用時間戳記資訊和特徵序列來產生時間位置嵌入。
之後，D2Vformer 介紹了一個新的融合區塊，該區塊利用注意力機制來探索輸入序列的嵌入和預測序列之間的時間位置相似性，從而根據此相似性產生預測。
通過對六個資料集的廣泛實驗，我們證明 Date2Vec 優於其他時間位置嵌入方法，而 D2Vformer 在固定長度和變長度預測任務中都超越了最先進的方法。

##### **GEIC: Universal and Multilingual Named Entity Recognition with Large Language Models**
2409.11022v2 by Hanjun Luo, Yingbin Jin, Xuecheng Liu, Tong Shang, Ruizhe Chen, Zuozhu Liu

Large Language Models (LLMs) have supplanted traditional methods in numerous
natural language processing tasks. Nonetheless, in Named Entity Recognition
(NER), existing LLM-based methods underperform compared to baselines and
require significantly more computational resources, limiting their application.
In this paper, we introduce the task of generation-based extraction and
in-context classification (GEIC), designed to leverage LLMs' prior knowledge
and self-attention mechanisms for NER tasks. We then propose CascadeNER, a
universal and multilingual GEIC framework for few-shot and zero-shot NER.
CascadeNER employs model cascading to utilize two small-parameter LLMs to
extract and classify independently, reducing resource consumption while
enhancing accuracy. We also introduce AnythingNER, the first NER dataset
specifically designed for LLMs, including 8 languages, 155 entity types and a
novel dynamic categorization system. Experiments show that CascadeNER achieves
state-of-the-art performance on low-resource and fine-grained scenarios,
including CrossNER and FewNERD. Our work is openly accessible.

摘要：大型語言模型（LLM）已經取代傳統方法，用於大量的自然語言處理任務。然而，在命名實體識別（NER）中，現有的基於 LLM 的方法與基線相比表現不佳，並且需要大量計算資源，從而限制了它們的應用。在本文中，我們引入了基於生成的抽取和上下文分類（GEIC）的任務，旨在利用 LLM 的先驗知識和自注意力機制來執行 NER 任務。然後，我們提出了 CascadeNER，一個通用的多語言 GEIC 框架，用於小樣本和零樣本 NER。CascadeNER 採用模型串聯來利用兩個小參數 LLM 獨立地提取和分類，從而在提高準確性的同時減少資源消耗。我們還引入了 AnythingNER，這是第一個專門為 LLM 設計的 NER 數據集，包括 8 種語言、155 種實體類型和一個新穎的動態分類系統。實驗表明，CascadeNER 在低資源和細粒度場景（包括 CrossNER 和 FewNERD）中實現了最先進的性能。我們的成果是公開可用的。

##### **Enhanced segmentation of femoral bone metastasis in CT scans of patients using synthetic data generation with 3D diffusion models**
2409.11011v1 by Emile Saillard, Aurélie Levillain, David Mitton, Jean-Baptiste Pialat, Cyrille Confavreux, Hélène Follet, Thomas Grenier

Purpose: Bone metastasis have a major impact on the quality of life of
patients and they are diverse in terms of size and location, making their
segmentation complex. Manual segmentation is time-consuming, and expert
segmentations are subject to operator variability, which makes obtaining
accurate and reproducible segmentations of bone metastasis on CT-scans a
challenging yet important task to achieve. Materials and Methods: Deep learning
methods tackle segmentation tasks efficiently but require large datasets along
with expert manual segmentations to generalize on new images. We propose an
automated data synthesis pipeline using 3D Denoising Diffusion Probabilistic
Models (DDPM) to enchance the segmentation of femoral metastasis from CT-scan
volumes of patients. We used 29 existing lesions along with 26 healthy femurs
to create new realistic synthetic metastatic images, and trained a DDPM to
improve the diversity and realism of the simulated volumes. We also
investigated the operator variability on manual segmentation. Results: We
created 5675 new volumes, then trained 3D U-Net segmentation models on real and
synthetic data to compare segmentation performance, and we evaluated the
performance of the models depending on the amount of synthetic data used in
training. Conclusion: Our results showed that segmentation models trained with
synthetic data outperformed those trained on real volumes only, and that those
models perform especially well when considering operator variability.

摘要：目的：骨轉移會對患者的生活品質造成重大影響，且在大小和位置上差異很大，這使得其分割變得複雜。手動分割非常耗時，而且專家分割會受到操作員變異性的影響，這使得在 CT 掃描中獲得骨轉移的準確且可重現的分割成為一項具有挑戰性但重要的任務。材料和方法：深度學習方法可以有效地處理分割任務，但需要大型資料集以及專家手動分割才能在新的影像上進行概化。我們提出了一個使用 3D 去噪擴散機率模型 (DDPM) 的自動化資料合成管道，以增強從患者 CT 掃描體積中分割股骨轉移。我們使用 29 個現有病灶和 26 個健康的股骨來創建新的逼真合成轉移影像，並訓練 DDPM 以提高模擬體積的多樣性和真實性。我們還研究了手動分割中的操作員變異性。結果：我們創建了 5675 個新體積，然後在真實和合成資料上訓練 3D U-Net 分割模型以比較分割效能，並根據訓練中使用的合成資料量評估模型的效能。結論：我們的結果表明，使用合成資料訓練的分割模型優於僅在真實體積上訓練的模型，並且這些模型在考慮操作員變異性時表現得特別好。

##### **CAST: Cross-modal Alignment Similarity Test for Vision Language Models**
2409.11007v1 by Gautier Dagan, Olga Loginova, Anil Batra

Vision Language Models (VLMs) are typically evaluated with Visual Question
Answering (VQA) tasks which assess a model's understanding of scenes. Good VQA
performance is taken as evidence that the model will perform well on a broader
range of tasks that require both visual and language inputs. However,
scene-aware VQA does not fully capture input biases or assess hallucinations
caused by a misalignment between modalities. To address this, we propose a
Cross-modal Alignment Similarity Test (CAST) to probe VLMs for self-consistency
across modalities. This test involves asking the models to identify
similarities between two scenes through text-only, image-only, or both and then
assess the truthfulness of the similarities they generate. Since there is no
ground-truth to compare against, this evaluation does not focus on objective
accuracy but rather on whether VLMs are internally consistent in their outputs.
We argue that while not all self-consistent models are capable or accurate, all
capable VLMs must be self-consistent.

摘要：視覺語言模型 (VLM) 通常使用視覺問題回答 (VQA) 任務進行評估，這些任務會評估模型對場景的理解。良好的 VQA 效能被視為模型在需要視覺和語言輸入的更廣泛任務中表現良好的證據。然而，場景感知 VQA 無法完全捕捉輸入偏差或評估由模態之間的錯位造成的幻覺。為了解決這個問題，我們提出跨模態對齊相似性測試 (CAST)，以探測 VLM 在不同模態之間的自洽性。此測試涉及要求模型透過純文字、純圖片或兩者來識別兩個場景之間的相似性，然後評估他們產生的相似性的真實性。由於沒有真實情況可以比較，因此此評估不著重於客觀準確性，而是著重於 VLM 在其輸出中是否內部一致。我們認為，雖然並非所有自洽模型都有能力或準確，但所有有能力的 VLM 都必須自洽。

##### **Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation**
2409.11003v1 by Gerard I. Gállego, Roy Fejgin, Chunghsin Yeh, Xiaoyu Liu, Gautam Bhattacharya

Audio token modeling has become a powerful framework for speech synthesis,
with two-stage approaches employing semantic tokens remaining prevalent. In
this paper, we aim to simplify this process by introducing a semantic knowledge
distillation method that enables high-quality speech generation in a single
stage. Our proposed model improves speech quality, intelligibility, and speaker
similarity compared to a single-stage baseline. Although two-stage systems
still lead in intelligibility, our model significantly narrows the gap while
delivering comparable speech quality. These findings showcase the potential of
single-stage models to achieve efficient, high-quality TTS with a more compact
and streamlined architecture.

摘要：語音代幣建模已成為語音合成的強大架構，其中採用語義代幣的兩階段方法仍然普遍。在本文中，我們旨在通過引入語義知識蒸餾方法來簡化此過程，該方法可以在單個階段中實現高品質的語音生成。與單階段基準相比，我們提出的模型改進了語音品質、清晰度和說話者相似度。儘管兩階段系統在清晰度方面仍然領先，但我們的模型顯著縮小了差距，同時提供了可比較的語音品質。這些發現展示了單階段模型在使用更緊湊、更簡化的架構實現高效、高品質的 TTS 的潛力。

##### **Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models**
2409.10999v1 by Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul

Audio language models can understand audio inputs and perform a range of
audio-related tasks based on instructions, such as speech recognition and audio
captioning, where the instructions are usually textual prompts. Audio language
models are mostly initialized from pre-trained audio encoders and large
language models (LLMs). Although these pre-trained components were developed to
support multiple languages, audio-language models are trained predominantly on
English data, which may limit their usability to only English instructions or
English speech inputs. First, this paper examines the performance of existing
audio language models in an underserved language using Thai as an example. This
paper demonstrates that, despite being built on multilingual backbones, audio
language models do not exhibit cross-lingual emergent abilities to low-resource
languages. Second, this paper studies data mixture for developing audio
language models that are optimized for a target language as well as English. In
addition. this paper integrates audio comprehension and speech
instruction-following capabilities into a single unified model. Our experiments
provide insights into data mixture for enhancing instruction-following
capabilities in both a low-resource language and English. Our model,
Typhoon-Audio, outperforms existing open-source audio language models by a
considerable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in
both English and Thai languages.

摘要：音訊語言模型可以理解音訊輸入，並根據指示執行一系列與音訊相關的任務，例如語音辨識和音訊字幕，其中指示通常是文字提示。音訊語言模型大多從預先訓練的音訊編碼器和大型語言模型 (LLM) 初始化。儘管這些預先訓練的元件是為了支援多種語言而開發，但音訊語言模型主要在英語資料上訓練，這可能會將其可用性限制在僅限於英語指示或英語語音輸入。首先，本文使用泰語為例，探討現有音訊語言模型在服務不足語言中的效能。本文證明，儘管建立在多語言主幹上，但音訊語言模型並未對低資源語言表現出跨語言的浮現能力。其次，本文研究資料混合，以開發針對目標語言和英語進行最佳化的音訊語言模型。此外，本文將音訊理解和語音指令遵循功能整合到單一統一模型中。我們的實驗提供了對資料混合的見解，以增強低資源語言和英語的指令遵循功能。我們的模型 Typhoon-Audio 在很大程度上優於現有的開源音訊語言模型，並且在英語和泰語中都與最先進的 Gemini-1.5-Pro 相當。

##### **Contextual Breach: Assessing the Robustness of Transformer-based QA Models**
2409.10997v2 by Asir Saadat, Nahian Ibn Asad, Md Farhan Ishmam

Contextual question-answering models are susceptible to adversarial
perturbations to input context, commonly observed in real-world scenarios.
These adversarial noises are designed to degrade the performance of the model
by distorting the textual input. We introduce a unique dataset that
incorporates seven distinct types of adversarial noise into the context, each
applied at five different intensity levels on the SQuAD dataset. To quantify
the robustness, we utilize robustness metrics providing a standardized measure
for assessing model performance across varying noise types and levels.
Experiments on transformer-based question-answering models reveal robustness
vulnerabilities and important insights into the model's performance in
realistic textual input.

摘要：情境式問答模型容易受到對抗性的輸入情境擾動，這在現實世界中很常見。這些對抗性雜訊旨在透過扭曲文字輸入來降低模型的效能。我們引入了一個獨特的資料集，其中包含七種類型的對抗性雜訊，每種類型在 SQuAD 資料集上以五個不同的強度等級應用。為了量化穩健性，我們利用穩健性指標提供標準化的測量，用於評估模型在不同雜訊類型和層級下的效能。在基於Transformer的問答模型上進行的實驗揭示了穩健性漏洞，並對模型在現實文字輸入中的效能提供了重要的見解。

##### **Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs**
2409.10994v1 by Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael Guan, Benyou Wang

The rapid advancement of Multimodal Large Language Models (MLLMs) has led to
remarkable performances across various domains. However, this progress is
accompanied by a substantial surge in the resource consumption of these models.
We address this pressing issue by introducing a new approach, Token Reduction
using CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without
sacrificing their performance. Inspired by human attention patterns in Visual
Question Answering (VQA) tasks, TRIM presents a fresh perspective on the
selection and reduction of image tokens. The TRIM method has been extensively
tested across 12 datasets, and the results demonstrate a significant reduction
in computational overhead while maintaining a consistent level of performance.
This research marks a critical stride in efficient MLLM development, promoting
greater accessibility and sustainability of high-performing models.

摘要：多模态大型语言模型 (MLLM) 的快速发展导致了在各个领域的卓越表现。然而，这一进展伴随着这些模型资源消耗的大幅增加。我们通过引入一种新方法来解决这一紧迫问题，即使用 CLIP 指标 (TRIM) 进行标记减少，旨在提高 MLLM 的效率而不牺牲其性能。受视觉问答 (VQA) 任务中人类注意力模式的启发，TRIM 提供了对图像标记选择和减少的新视角。TRIM 方法已在 12 个数据集上进行了广泛测试，结果表明在保持一致性能水平的同时，计算开销显著减少。这项研究标志着高效 MLLM 开发的关键一步，促进了高性能模型的更大可访问性和可持续性。

##### **Control-flow Reconstruction Attacks on Business Process Models**
2409.10986v1 by Henrik Kirchmann, Stephan A. Fahrenkrog-Petersen, Felix Mannhardt, Matthias Weidlich

Process models may be automatically generated from event logs that contain
as-is data of a business process. While such models generalize over the
control-flow of specific, recorded process executions, they are often also
annotated with behavioural statistics, such as execution frequencies.Based
thereon, once a model is published, certain insights about the original process
executions may be reconstructed, so that an external party may extract
confidential information about the business process. This work is the first to
empirically investigate such reconstruction attempts based on process models.
To this end, we propose different play-out strategies that reconstruct the
control-flow from process trees, potentially exploiting frequency annotations.
To assess the potential success of such reconstruction attacks on process
models, and hence the risks imposed by publishing them, we compare the
reconstructed process executions with those of the original log for several
real-world datasets.

摘要：流程模型可以從包含業務流程原樣資料的事件日誌中自動產生。雖然此類模型概括了特定已記錄流程執行的控制流程，但它們通常也會附加行為統計資料，例如執行頻率。基於此，一旦模型發布，就可以重建關於原始流程執行的特定見解，以便外部方可以提取有關業務流程的機密資訊。這項工作是第一個根據流程模型實證調查此類重建嘗試的工作。為此，我們提出不同的播放策略，這些策略從流程樹中重建控制流程，並可能利用頻率註解。為了評估此類重建攻擊對流程模型的潛在成功率，以及因此發布它們所帶來的風險，我們將重建的流程執行與多個真實世界資料集的原始日誌中的執行進行比較。

##### **Improving Speech Emotion Recognition in Under-Resourced Languages via Speech-to-Speech Translation with Bootstrapping Data Selection**
2409.10985v1 by Hsi-Che Lin, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee

Speech Emotion Recognition (SER) is a crucial component in developing
general-purpose AI agents capable of natural human-computer interaction.
However, building robust multilingual SER systems remains challenging due to
the scarcity of labeled data in languages other than English and Chinese. In
this paper, we propose an approach to enhance SER performance in low SER
resource languages by leveraging data from high-resource languages.
Specifically, we employ expressive Speech-to-Speech translation (S2ST) combined
with a novel bootstrapping data selection pipeline to generate labeled data in
the target language. Extensive experiments demonstrate that our method is both
effective and generalizable across different upstream models and languages. Our
results suggest that this approach can facilitate the development of more
scalable and robust multilingual SER systems.

摘要：語音情緒辨識 (SER) 是開發具備自然人機互動能力之通用 AI 代理程式中至關重要的組成部分。
然而，由於英語和中文以外語言標籤資料的稀少性，建構強健的多語言 SER 系統仍然具有挑戰性。
在本文中，我們提出了一種方法，透過利用高資源語言的資料來增強低 SER 資源語言中的 SER 效能。
具體來說，我們採用表達式的語音轉語音翻譯 (S2ST)，並結合一個新穎的引導資料選取管道，以產生目標語言中的標籤資料。
廣泛的實驗證明了我們的模型在不同的上游模型和語言中既有效又具有普遍性。
我們的結果表明，這種方法可以促進更具可擴充性和強健性的多語言 SER 系統的開發。

##### **Enhancing Multilingual Speech Generation and Recognition Abilities in LLMs with Constructed Code-switched Data**
2409.10969v1 by Jing Xu, Daxin Tan, Jiaqi Wang, Xiao Chen

While large language models (LLMs) have been explored in the speech domain
for both generation and recognition tasks, their applications are predominantly
confined to the monolingual scenario, with limited exploration in multilingual
and code-switched (CS) contexts. Additionally, speech generation and
recognition tasks are often handled separately, such as VALL-E and Qwen-Audio.
In this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating
multilingual speech generation and recognition tasks within the single LLM.
Furthermore, we develop an effective data construction approach that splits and
concatenates words from different languages to equip LLMs with CS synthesis
ability without relying on CS data. The experimental results demonstrate that
our model outperforms other baselines with a comparable data scale.
Furthermore, our data construction approach not only equips LLMs with CS speech
synthesis capability with comparable speaker consistency and similarity to any
given speaker, but also improves the performance of LLMs in multilingual speech
generation and recognition tasks.

摘要：儘管大型語言模型 (LLM) 已在語音領域中探討生成和辨識任務，但其應用主要侷限於單語境，在多語言和代碼轉換 (CS) 語境中的探索有限。此外，語音生成和辨識任務通常是分開處理的，例如 VALL-E 和 Qwen-Audio。在本文中，我們提出一個多語言多任務 (MLMT) 模型，將多語言語音生成和辨識任務整合在單一 LLM 中。此外，我們開發一種有效資料建構方法，將不同語言的字詞拆分和串接，以具備 CS 合成能力的 LLM，而無需依賴 CS 資料。實驗結果證明，我們的模型優於其他基準，資料規模相當。此外，我們的資料建構方法不僅讓 LLM 具備 CS 語音合成能力，具有與任何特定說話者相當的說話者一致性和相似性，而且還提升了 LLM 在多語言語音生成和辨識任務中的效能。

##### **Cross-lingual transfer of multilingual models on low resource African Languages**
2409.10965v1 by Harish Thangaraj, Ananya Chenat, Jaskaran Singh Walia, Vukosi Marivate

Large multilingual models have significantly advanced natural language
processing (NLP) research. However, their high resource demands and potential
biases from diverse data sources have raised concerns about their effectiveness
across low-resource languages. In contrast, monolingual models, trained on a
single language, may better capture the nuances of the target language,
potentially providing more accurate results. This study benchmarks the
cross-lingual transfer capabilities from a high-resource language to a
low-resource language for both, monolingual and multilingual models, focusing
on Kinyarwanda and Kirundi, two Bantu languages. We evaluate the performance of
transformer based architectures like Multilingual BERT (mBERT), AfriBERT, and
BantuBERTa against neural-based architectures such as BiGRU, CNN, and char-CNN.
The models were trained on Kinyarwanda and tested on Kirundi, with fine-tuning
applied to assess the extent of performance improvement and catastrophic
forgetting. AfriBERT achieved the highest cross-lingual accuracy of 88.3% after
fine-tuning, while BiGRU emerged as the best-performing neural model with 83.3%
accuracy. We also analyze the degree of forgetting in the original language
post-fine-tuning. While monolingual models remain competitive, this study
highlights that multilingual models offer strong cross-lingual transfer
capabilities in resource limited settings.

摘要：大型多語言模型已大幅提升自然語言處理 (NLP) 研究。然而，其高資源需求和來自不同資料來源的潛在偏差已引發人們對其在低資源語言中的有效性的擔憂。相比之下，針對單一語言訓練的單語言模型可能更能捕捉目標語言的細微差別，潛在提供更準確的結果。本研究基準測試了從高資源語言到低資源語言的跨語言轉移能力，適用於單語言模型和多語言模型，重點關注班圖語族的盧安達語和基隆迪語。我們評估了基於 Transformer 架構的效能，例如多語言 BERT (mBERT)、AfriBERT 和 BantuBERTa，以及基於神經網路的架構，例如 BiGRU、CNN 和 char-CNN。這些模型在盧安達語上訓練，並在基隆迪語上測試，並應用微調來評估效能改善的程度和災難性遺忘。AfriBERT 在微調後達到了 88.3% 的最高跨語言準確度，而 BiGRU 則以 83.3% 的準確度成為表現最佳的神經網路模型。我們還分析了微調後原始語言的遺忘程度。雖然單語言模型仍然具有競爭力，但本研究強調多語言模型在資源受限的環境中提供了強大的跨語言轉移能力。

##### **Active learning for energy-based antibody optimization and enhanced screening**
2409.10964v2 by Kairi Furui, Masahito Ohue

Accurate prediction and optimization of protein-protein binding affinity is
crucial for therapeutic antibody development. Although machine learning-based
prediction methods $\Delta\Delta G$ are suitable for large-scale mutant
screening, they struggle to predict the effects of multiple mutations for
targets without existing binders. Energy function-based methods, though more
accurate, are time consuming and not ideal for large-scale screening. To
address this, we propose an active learning workflow that efficiently trains a
deep learning model to learn energy functions for specific targets, combining
the advantages of both approaches. Our method integrates the RDE-Network deep
learning model with Rosetta's energy function-based Flex ddG to efficiently
explore mutants. In a case study targeting HER2-binding Trastuzumab mutants,
our approach significantly improved the screening performance over random
selection and demonstrated the ability to identify mutants with better binding
properties without experimental $\Delta\Delta G$ data. This workflow advances
computational antibody design by combining machine learning, physics-based
computations, and active learning to achieve more efficient antibody
development.

摘要：準確預測和最佳化蛋白質-蛋白質結合親和力對於治療性抗體開發至關重要。儘管基於機器學習的預測方法 $\Delta\Delta G$ 適用於大規模突變篩選，但它們難以預測沒有現有結合物的目標物的多重突變效應。基於能量函數的方法雖然更準確，但耗時且不適用於大規模篩選。為了解決這個問題，我們提出一個主動學習工作流程，有效訓練深度學習模型以學習特定目標物的能量函數，結合兩種方法的優點。我們的模型整合了 RDE-Network 深度學習模型與 Rosetta 基於能量函數的 Flex ddG，以有效探索突變。在針對 HER2 結合 Trastuzumab 突變的案例研究中，我們的模型明顯改善了隨機選擇的篩選效能，並證明了在沒有實驗 $\Delta\Delta G$ 資料的情況下識別具有更好結合特性的突變體的能力。這個工作流程結合機器學習、基於物理的運算和主動學習，推進了計算抗體設計，以實現更有效的抗體開發。

##### **Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning**
2409.10956v1 by Min-Yeong Park, Jae-Ho Lee, Gyeong-Moon Park

Incremental Learning (IL) aims to accumulate knowledge from sequential input
tasks while overcoming catastrophic forgetting. Existing IL methods typically
assume that an incoming task has only increments of classes or domains,
referred to as Class IL (CIL) or Domain IL (DIL), respectively. In this work,
we consider a more challenging and realistic but under-explored IL scenario,
named Versatile Incremental Learning (VIL), in which a model has no prior of
which of the classes or domains will increase in the next task. In the proposed
VIL scenario, the model faces intra-class domain confusion and inter-domain
class confusion, which makes the model fail to accumulate new knowledge without
interference with learned knowledge. To address these issues, we propose a
simple yet effective IL framework, named Incremental Classifier with Adaptation
Shift cONtrol (ICON). Based on shifts of learnable modules, we design a novel
regularization method called Cluster-based Adaptation Shift conTrol (CAST) to
control the model to avoid confusion with the previously learned knowledge and
thereby accumulate the new knowledge more effectively. Moreover, we introduce
an Incremental Classifier (IC) which expands its output nodes to address the
overwriting issue from different domains corresponding to a single class while
maintaining the previous knowledge. We conducted extensive experiments on three
benchmarks, showcasing the effectiveness of our method across all the
scenarios, particularly in cases where the next task can be randomly altered.
Our implementation code is available at https://github.com/KHU-AGI/VIL.

摘要：增量學習 (IL) 的目標是從序列輸入任務中累積知識，同時克服災難性遺忘。現有的 IL 方法通常假設新進任務僅有類別或領域的增量，分別稱為類別 IL (CIL) 或領域 IL (DIL)。在這項工作中，我們考慮一個更具挑戰性和現實性但未充分探索的 IL 場景，稱為多功能增量學習 (VIL)，其中模型沒有任何先驗知識，不知道哪個類別或領域會在下一項任務中增加。在提議的 VIL 場景中，模型面臨類內領域混淆和領域間類別混淆，這使得模型無法累積新知識，而不會受到學習知識的干擾。為了解決這些問題，我們提出了一個簡單但有效的 IL 框架，稱為帶適應位移控制的增量分類器 (ICON)。基於可學習模組的位移，我們設計了一種稱為基於群集的適應位移控制 (CAST) 的新正則化方法，以控制模型避免與先前學習的知識混淆，從而更有效地累積新知識。此外，我們引入了一個增量分類器 (IC)，它擴展其輸出節點以解決對應於單一類別的不同領域的覆寫問題，同時維護先前的知識。我們在三個基準上進行了廣泛的實驗，展示了我們的方法在所有場景中的有效性，特別是在下一項任務可以隨機更改的情況下。我們的實作程式碼可在 https://github.com/KHU-AGI/VIL 取得。

##### **Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style**
2409.10955v1 by Yuepei Li, Kang Zhou, Qiao Qiao, Bach Nguyen, Qing Wang, Qi Li

Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by
incorporating external information into the response generation process.
However, how context-faithful LLMs are and what factors influence LLMs'
context-faithfulness remain largely unexplored. In this study, we investigate
the impact of memory strength and evidence presentation on LLMs' receptiveness
to external evidence. We introduce a method to quantify the memory strength of
LLMs by measuring the divergence in LLMs' responses to different paraphrases of
the same question, which is not considered by previous works. We also generate
evidence in various styles to evaluate the effects of evidence in different
styles. Two datasets are used for evaluation: Natural Questions (NQ) with
popular questions and popQA featuring long-tail questions. Our results show
that for questions with high memory strength, LLMs are more likely to rely on
internal memory, particularly for larger LLMs such as GPT-4. On the other hand,
presenting paraphrased evidence significantly increases LLMs' receptiveness
compared to simple repetition or adding details.

摘要：檢索增強生成 (RAG) 透過將外部資訊納入回應生成程序中，改善大型語言模型 (LLM)。
然而，LLM 的上下文忠實度如何以及哪些因素會影響 LLM 的上下文忠實度，在很大程度上仍未被探索。在這個研究中，我們探討記憶強度和證據呈現對 LLM 對外部證據的接受程度的影響。我們引入一種方法，透過測量 LLM 對同一個問題的不同同義詞改寫的回應的分歧，來量化 LLM 的記憶強度，這是先前研究未考慮的。我們也生成不同風格的證據，以評估不同風格證據的效果。我們使用兩個資料集進行評估：包含熱門問題的自然問題 (NQ) 和包含長尾問題的 popQA。我們的結果顯示，對於記憶強度高的問題，LLM 更可能依賴內部記憶，特別是對於較大的 LLM，例如 GPT-4。另一方面，與單純的重複或增加細節相比，呈現同義詞改寫的證據會顯著增加 LLM 的接受度。

##### **Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach**
2409.10932v1 by Mehroush Banday, Sherin Zafar, Parul Agarwal, M Afshar Alam, Abubeker K M

Coronary heart disease (CHD) is a severe cardiac disease, and hence, its
early diagnosis is essential as it improves treatment results and saves money
on medical care. The prevailing development of quantum computing and machine
learning (ML) technologies may bring practical improvement to the performance
of CHD diagnosis. Quantum machine learning (QML) is receiving tremendous
interest in various disciplines due to its higher performance and capabilities.
A quantum leap in the healthcare industry will increase processing power and
optimise multiple models. Techniques for QML have the potential to forecast
cardiac disease and help in early detection. To predict the risk of coronary
heart disease, a hybrid approach utilizing an ensemble machine learning model
based on QML classifiers is presented in this paper. Our approach, with its
unique ability to address multidimensional healthcare data, reassures the
method's robustness by fusing quantum and classical ML algorithms in a
multi-step inferential framework. The marked rise in heart disease and death
rates impacts worldwide human health and the global economy. Reducing cardiac
morbidity and mortality requires early detection of heart disease. In this
research, a hybrid approach utilizes techniques with quantum computing
capabilities to tackle complex problems that are not amenable to conventional
machine learning algorithms and to minimize computational expenses. The
proposed method has been developed in the Raspberry Pi 5 Graphics Processing
Unit (GPU) platform and tested on a broad dataset that integrates clinical and
imaging data from patients suffering from CHD and healthy controls. Compared to
classical machine learning models, the accuracy, sensitivity, F1 score, and
specificity of the proposed hybrid QML model used with CHD are manifold higher.

摘要：冠狀動脈心臟病 (CHD) 是一種嚴重的疾病，因此，早期診斷至關重要，因為它可以改善治療結果並節省醫療保健費用。量子計算和機器學習 (ML) 技術的盛行發展可能會對 CHD 診斷的性能帶來實際改善。量子機器學習 (QML) 由於其更高的性能和能力，在各個領域引起了極大的興趣。醫療保健行業的量子飛躍將增加處理能力並優化多個模型。QML 的技術有潛力預測心臟病並幫助早期發現。為了預測冠狀動脈心臟病的風險，本文提出了一種基於 QML 分類器的混合機器學習模型的混合方法。我們的這種方法具備處理多維醫療保健數據的獨特能力，通過在多步驟推理框架中融合量子和經典 ML 演算法，確保了該方法的穩健性。心臟病和死亡率的顯著上升影響了全球人類健康和全球經濟。降低心臟發病率和死亡率需要對心臟病進行早期發現。在這項研究中，一種混合方法利用具有量子計算能力的技術來解決傳統機器學習演算法無法解決的複雜問題，並最大程度地減少計算開銷。所提出的方法已在 Raspberry Pi 5 繪圖處理單元 (GPU) 平臺上開發，並在一個廣泛的資料集上進行了測試，該資料集整合了患有 CHD 和健康對照者的臨床和影像數據。與經典機器學習模型相比，所提出的混合 QML 模型與 CHD 一起使用的準確性、敏感性、F1 分數和特異性更高。

##### **Propulsion: Steering LLM with Tiny Fine-Tuning**
2409.10927v2 by Md Kowsher, Nusrat Jahan Prottasha, Prakash Bhat

The rapid advancements in Large Language Models (LLMs) have revolutionized
natural language processing (NLP) and related fields. However, fine-tuning
these models for specific tasks remains computationally expensive and risks
degrading pre-learned features. To address these challenges, we propose
Propulsion, a novel parameter efficient fine-tuning (PEFT) method designed to
optimize task-specific performance while drastically reducing computational
overhead. Inspired by the concept of controlled adjustments in physical motion,
Propulsion selectively re-scales specific dimensions of a pre-trained model,
guiding output predictions toward task objectives without modifying the model's
parameters. By introducing lightweight, trainable Propulsion parameters at the
pre-trained layer, we minimize the number of parameters updated during
fine-tuning, preventing overfitting or overwriting of existing knowledge. Our
theoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows
that Propulsion approximates the performance of full fine-tuning with far fewer
trainable parameters. Empirically, Propulsion reduces the parameter count from
355.3 million to just 0.086 million, achieving over a 10x reduction compared to
standard approaches like LoRA while maintaining competitive performance across
benchmarks.

摘要：大型語言模型 (LLM) 的快速進展徹底改變了自然語言處理 (NLP) 和相關領域。然而，針對特定任務微調這些模型在計算上仍然很昂貴，並且有損害預先學習到的特徵的風險。為了應對這些挑戰，我們提出了推進，一種新穎的參數高效微調 (PEFT) 方法，旨在優化特定於任務的效能，同時大幅減少計算開銷。推進的靈感來自於物理運動中受控調整的概念，它有選擇性地重新調整預先訓練模型的特定維度，將輸出預測引導至任務目標，而不會修改模型的參數。透過在預先訓練的層中引入輕量級、可訓練的推進參數，我們將微調期間更新的參數數量減至最少，防止過度擬合或覆寫現有知識。我們的理論分析，在神經切線核 (NTK) 理論的支援下，顯示推進近似於完全微調的效能，但可訓練參數卻少得多。根據經驗，推進將參數數量從 3.553 億減少到僅 0.086 萬，與 LoRA 等標準方法相比，減少了 10 倍以上，同時在各種基準測試中維持有競爭力的效能。

##### **KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**
2409.10921v1 by Yanbei Jiang, Krista A. Ehinger, Jey Han Lau

Exploring the narratives conveyed by fine-art paintings is a challenge in
image captioning, where the goal is to generate descriptions that not only
precisely represent the visual content but also offer a in-depth interpretation
of the artwork's meaning. The task is particularly complex for artwork images
due to their diverse interpretations and varied aesthetic principles across
different artistic schools and styles. In response to this, we present KALE
Knowledge-Augmented vision-Language model for artwork Elaborations), a novel
approach that enhances existing vision-language models by integrating artwork
metadata as additional knowledge. KALE incorporates the metadata in two ways:
firstly as direct textual input, and secondly through a multimodal
heterogeneous knowledge graph. To optimize the learning of graph
representations, we introduce a new cross-modal alignment loss that maximizes
the similarity between the image and its corresponding metadata. Experimental
results demonstrate that KALE achieves strong performance (when evaluated with
CIDEr, in particular) over existing state-of-the-art work across several
artwork datasets. Source code of the project is available at
https://github.com/Yanbei-Jiang/Artwork-Interpretation.

摘要：探索由美术绘画传达的叙事是图像字幕中的挑战，其目标是生成不仅准确地表示视觉内容而且还提供对艺术品含义的深入解释的描述。由于其不同的解释和跨不同艺术流派和风格的不同美学原则，这项任务对于艺术品图像来说尤其复杂。为了应对这种情况，我们提出了 KALE 知识增强视觉语言模型用于艺术品阐释，一种通过将艺术品元数据作为附加知识来增强现有视觉语言模型的新方法。KALE 以两种方式合并元数据：首先作为直接文本输入，其次通过多模态异构知识图。为了优化图表的学习表示，我们引入了一种新的跨模态对齐损失，它最大化图像与其对应元数据之间的相似性。实验结果表明，KALE 在使用 CIDEr 评估时，在几个艺术品数据集上取得了优异的性能（特别是与现有的最先进的工作相比）。该项目的源代码可在 https://github.com/Yanbei-Jiang/Artwork-Interpretation 获得。

##### **GenCRF: Generative Clustering and Reformulation Framework for Enhanced Intent-Driven Information Retrieval**
2409.10909v1 by Wonduk Seo, Haojie Zhang, Yueyang Zhang, Changhao Zhang, Songyao Duan, Lixin Su, Daiting Shi, Jiashu Zhao, Dawei Yin

Query reformulation is a well-known problem in Information Retrieval (IR)
aimed at enhancing single search successful completion rate by automatically
modifying user's input query. Recent methods leverage Large Language Models
(LLMs) to improve query reformulation, but often generate limited and redundant
expansions, potentially constraining their effectiveness in capturing diverse
intents. In this paper, we propose GenCRF: a Generative Clustering and
Reformulation Framework to capture diverse intentions adaptively based on
multiple differentiated, well-generated queries in the retrieval phase for the
first time. GenCRF leverages LLMs to generate variable queries from the initial
query using customized prompts, then clusters them into groups to distinctly
represent diverse intents. Furthermore, the framework explores to combine
diverse intents query with innovative weighted aggregation strategies to
optimize retrieval performance and crucially integrates a novel Query
Evaluation Rewarding Model (QERM) to refine the process through feedback loops.
Empirical experiments on the BEIR benchmark demonstrate that GenCRF achieves
state-of-the-art performance, surpassing previous query reformulation SOTAs by
up to 12% on nDCG@10. These techniques can be adapted to various LLMs,
significantly boosting retriever performance and advancing the field of
Information Retrieval.

摘要：查詢重新表述是資訊檢索 (IR) 中一個眾所周知的問題，旨在透過自動修改使用者的輸入查詢，來提升單一搜尋的成功完成率。最近的方法利用大型語言模型 (LLM) 來改善查詢重新表述，但通常會產生受限且重複的擴充，潛在地限制其在捕捉不同意圖方面的效能。在本文中，我們提出 GenCRF：一個生成式分群和重新表述架構，以在檢索階段根據多個差異化、產生良好的查詢，來自適應地捕捉不同的意圖。GenCRF 利用 LLM 從初始查詢產生變數查詢，使用自訂提示，然後將它們分群到群組中，以明確地代表不同的意圖。此外，該架構探索將不同的意圖查詢與創新的加權聚合策略相結合，以最佳化檢索效能，並至關重要的是整合一個新的查詢評估回饋模型 (QERM)，以透過回饋迴路來改善這個流程。在 BEIR 基準上的實證實驗證明，GenCRF 達到了最先進的效能，在 nDCG@10 上超越了先前的查詢重新表述 SOTA，最高達 12%。這些技術可以適應各種 LLM，大幅提升檢索器效能，並推動資訊檢索領域的進步。

##### **Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised Keyphrase Extraction**
2409.10907v1 by Erwin D. López Z., Cheng Tang, Atsushi Shimada

This paper proposes Attention-Seeker, an unsupervised keyphrase extraction
method that leverages self-attention maps from a Large Language Model to
estimate the importance of candidate phrases. Our approach identifies specific
components - such as layers, heads, and attention vectors - where the model
pays significant attention to the key topics of the text. The attention weights
provided by these components are then used to score the candidate phrases.
Unlike previous models that require manual tuning of parameters (e.g.,
selection of heads, prompts, hyperparameters), Attention-Seeker dynamically
adapts to the input text without any manual adjustments, enhancing its
practical applicability. We evaluate Attention-Seeker on four publicly
available datasets: Inspec, SemEval2010, SemEval2017, and Krapivin. Our results
demonstrate that, even without parameter tuning, Attention-Seeker outperforms
most baseline models, achieving state-of-the-art performance on three out of
four datasets, particularly excelling in extracting keyphrases from long
documents.

摘要：本文提出了 Attention-Seeker，這是一種無監督關鍵字萃取方法，它利用大型語言模型中的自注意力圖來估計候選詞組的重要性。我們的做法找出特定組成部分（例如層、頭和注意力向量），模型在這些組成部分中會對文字的主要主題給予顯著的注意力。然後使用這些組成部分提供的注意力權重來為候選詞組評分。與需要手動調整參數（例如選擇頭、提示、超參數）的先前模型不同，Attention-Seeker 會動態調整輸入文字，而無需任何手動調整，從而增強其實用性。我們在四個公開可用的資料集上評估 Attention-Seeker：Inspec、SemEval2010、SemEval2017 和 Krapivin。我們的結果證明，即使不調整參數，Attention-Seeker 也優於大多數基線模型，在四個資料集中有三個資料集達到最先進的效能，特別是在從長文件中萃取關鍵字時表現出色。

##### **WaterQualityNeT: Prediction of Seasonal Water Quality of Nepal Using Hybrid Deep Learning Models**
2409.10898v1 by Biplov Paneru, Bishwash Paneru

Ensuring a safe and uncontaminated water supply is contingent upon the
monitoring of water quality, especially in developing countries such as Nepal,
where water sources are susceptible to pollution. This paper presents a hybrid
deep learning model for predicting Nepal's seasonal water quality using a small
dataset with many water quality parameters. The model integrates convolutional
neural networks (CNN) and recurrent neural networks (RNN) to exploit temporal
and spatial patterns in the data. The results demonstrate significant
improvements in forecast accuracy over traditional methods, providing a
reliable tool for proactive control of water quality. The model that used WQI
parameters to classify people into good, poor, and average groups performed 92%
of the time in testing. Similarly, the R2 score was 0.97 and the root mean
square error was 2.87 when predicting WQI values using regression analysis.
Additionally, a multifunctional application that uses both a regression and a
classification approach is built to predict WQI values.

摘要：確保安全且未受污染的水源供應取決於水質監測，尤其是在尼泊爾等開發中國家，那裡的水源容易受到污染。本文提出了一種混合深度學習模型，用於使用具有許多水質參數的小型資料集預測尼泊爾的季節性水質。該模型整合了卷積神經網路 (CNN) 和遞迴神經網路 (RNN) 以利用資料中的時間和空間模式。結果顯示，與傳統方法相比，預測準確度有了顯著提高，為主動控制水質提供了一個可靠的工具。使用 WQI 參數將人們分類為良好、不良和一般組別的模型在測試中表現為 92%。同樣地，在使用回歸分析預測 WQI 值時，R2 分數為 0.97，均方根誤差為 2.87。此外，還建立了一個多功能應用程式，它同時使用回歸和分類方法來預測 WQI 值。

##### **Shaking the Fake: Detecting Deepfake Videos in Real Time via Active Probes**
2409.10889v1 by Zhixin Xie, Jun Luo

Real-time deepfake, a type of generative AI, is capable of "creating"
non-existing contents (e.g., swapping one's face with another) in a video. It
has been, very unfortunately, misused to produce deepfake videos (during web
conferences, video calls, and identity authentication) for malicious purposes,
including financial scams and political misinformation. Deepfake detection, as
the countermeasure against deepfake, has attracted considerable attention from
the academic community, yet existing works typically rely on learning passive
features that may perform poorly beyond seen datasets. In this paper, we
propose SFake, a new real-time deepfake detection method that innovatively
exploits deepfake models' inability to adapt to physical interference.
Specifically, SFake actively sends probes to trigger mechanical vibrations on
the smartphone, resulting in the controllable feature on the footage.
Consequently, SFake determines whether the face is swapped by deepfake based on
the consistency of the facial area with the probe pattern. We implement SFake,
evaluate its effectiveness on a self-built dataset, and compare it with six
other detection methods. The results show that SFake outperforms other
detection methods with higher detection accuracy, faster process speed, and
lower memory consumption.

摘要：即時深度偽造是一種生成式 AI，能夠「創造」影片中不存在的內容（例如，將某人的臉換成另一個人）。不幸的是，它已被濫用於製作深度偽造影片（在網路會議、視訊通話和身分驗證中），用於惡意目的，包括金融詐騙和政治錯誤訊息。深度偽造偵測作為對抗深度偽造的對策，已引起學術界的廣泛關注，但現有的作品通常依賴於學習被動特徵，這些特徵在超出已見資料集時可能會表現不佳。在本文中，我們提出 SFake，這是一種新的即時深度偽造偵測方法，創新地利用深度偽造模型無法適應物理干擾的特性。具體來說，SFake 主動發送探針以觸發智慧型手機上的機械振動，從而導致影片中可控的特徵。因此，SFake 根據臉部區域與探針模式的一致性來判斷臉部是否被深度偽造替換。我們實作 SFake，評估其在自建資料集上的有效性，並將其與其他六種偵測方法進行比較。結果表明，SFake 在更高的偵測準確度、更快的處理速度和更低的記憶體消耗方面優於其他偵測方法。

##### **CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation for Meeting Summarization**
2409.10883v1 by Ziwei Gong, Lin Ai, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Zehui Wu, Ahmad Emami, Julia Hirschberg

Large Language Models (LLMs) have spurred interest in automatic evaluation
methods for summarization, offering a faster, more cost-effective alternative
to human evaluation. However, existing methods often fall short when applied to
complex tasks like long-context summarizations and dialogue-based meeting
summarizations. In this paper, we introduce CREAM (Comparison-Based
Reference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a
novel framework that addresses the unique challenges of evaluating meeting
summaries. CREAM leverages a combination of chain-of-thought reasoning and key
facts alignment to assess conciseness and completeness of model-generated
summaries without requiring reference. By employing an ELO ranking system, our
approach provides a robust mechanism for comparing the quality of different
models or prompt configurations.

摘要：大型語言模型 (LLM) 促使人們對摘要的自動評估方法產生興趣，提供比人工評估更快速、更具成本效益的替代方案。然而，現有的方法在應用於複雜任務（例如長文脈摘要和基於對話的會議摘要）時，往往會有所不足。在本文中，我們介紹了 CREAM（基於比較的無參考 Elo 排名自動評估會議摘要），這是一個新穎的框架，用於解決評估會議摘要的獨特挑戰。CREAM 採用思想鏈推理和關鍵事實對齊的組合，來評估模型生成的摘要的簡潔性和完整性，而無需參考。通過採用 ELO 排名系統，我們的做法提供了一個強大的機制，用於比較不同模型或提示配置的品質。

##### **American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM**
2409.10874v1 by Gregorius Guntur Sunardi Putra, Adifa Widyadhani Chanda D'Layla, Dimas Wahono, Riyanarto Sarno, Agus Tri Haryono

Sign language translation is one of the important issues in communication
between deaf and hearing people, as it expresses words through hand, body, and
mouth movements. American Sign Language is one of the sign languages used, one
of which is the alphabetic sign. The development of neural machine translation
technology is moving towards sign language translation. Transformer became the
state-of-the-art in natural language processing. This study compares the
Transformer with the Sequence-to-Sequence (Seq2Seq) model in translating sign
language to text. In addition, an experiment was conducted by adding Residual
Long Short-Term Memory (ResidualLSTM) in the Transformer. The addition of
ResidualLSTM to the Transformer reduces the performance of the Transformer
model by 23.37% based on the BLEU Score value. In comparison, the Transformer
itself increases the BLEU Score value by 28.14 compared to the Seq2Seq model.

摘要：手語翻譯是聾人和聽人溝通中的重要議題之一，因為它透過手、身體和嘴巴的動作來表達單字。美國手語是其中一種手語，其中一種是字母手語。神經機器翻譯技術的發展正朝向手語翻譯邁進。Transformer 成為了自然語言處理的最新技術。本研究比較了 Transformer 與序列到序列 (Seq2Seq) 模型在將手語翻譯成文字的表現。此外，還透過在 Transformer 中加入殘差長短期記憶 (ResidualLSTM) 進行實驗。在 Transformer 中加入 ResidualLSTM 會讓 Transformer 模型的表現降低 23.37%，根據 BLEU 評分值。相較之下，與 Seq2Seq 模型相比，Transformer 本身讓 BLEU 評分值增加了 28.14。

##### **Adaptive Large Language Models By Layerwise Attention Shortcuts**
2409.10870v1 by Prateek Verma, Mert Pilanci

Transformer architectures are the backbone of the modern AI revolution.
However, they are based on simply stacking the same blocks in dozens of layers
and processing information sequentially from one block to another. In this
paper, we propose to challenge this and introduce adaptive computations for
LLM-like setups, which allow the final layer to attend to all of the
intermediate layers as it deems fit through the attention mechanism, thereby
introducing computational \textbf{attention shortcuts}. These shortcuts can
thus make the architecture depth and context adaptive. We showcase four
different datasets, namely acoustic tokens, natural language, and symbolic
music, and we achieve superior performance for GPT-like architecture. We give
evidence via attention maps that the models learn complex dependencies across
layers that are adaptive in context and depth depending on the input tokens.

摘要：Transformer 架構是現代 AI 革命的骨幹。
然而，它們僅僅是將相同的區塊堆疊在數十層中，
並將資訊從一個區塊順序處理到另一個區塊。在本文中，
我們提議挑戰這一點，並為 LLM 類型的設定引入適應性運算，
這允許最後一層透過注意機制，在它認為合適時注意所有中間層，
從而引入了運算「注意捷徑」。這些捷徑因此可以使架構深度和內容適應性。
我們展示了四個不同的資料集，即音響符號、自然語言和符號音樂，
並且我們為 GPT 類型的架構達到了卓越的效能。我們透過注意地圖提供證據，
證明模型學習了複雜的依賴性，這些依賴性在內容和深度上會根據輸入符號而適應。

##### **SIFToM: Robust Spoken Instruction Following through Theory of Mind**
2409.10849v1 by Lance Ying, Jason Xinyu Liu, Shivam Aarya, Yizirui Fang, Stefanie Tellex, Joshua B. Tenenbaum, Tianmin Shu

Spoken language instructions are ubiquitous in agent collaboration. However,
in human-robot collaboration, recognition accuracy for human speech is often
influenced by various speech and environmental factors, such as background
noise, the speaker's accents, and mispronunciation. When faced with noisy or
unfamiliar auditory inputs, humans use context and prior knowledge to
disambiguate the stimulus and take pragmatic actions, a process referred to as
top-down processing in cognitive science. We present a cognitively inspired
model, Speech Instruction Following through Theory of Mind (SIFToM), to enable
robots to pragmatically follow human instructions under diverse speech
conditions by inferring the human's goal and joint plan as prior for speech
perception and understanding. We test SIFToM in simulated home experiments
(VirtualHome 2). Results show that the SIFToM model outperforms
state-of-the-art speech and language models, approaching human-level accuracy
on challenging speech instruction following tasks. We then demonstrate its
ability at the task planning level on a mobile manipulator for breakfast
preparation tasks.

摘要：在代理協作中，口語指令無所不在。然而，在人機協作中，人類語音的辨識準確度通常會受到各種語音和環境因素的影響，例如背景噪音、說話者的口音和發音不正確。當面對嘈雜或不熟悉的聽覺輸入時，人類會使用上下文和先驗知識來消除刺激的歧義並採取務實的行動，這是一個在認知科學中稱為自上而下處理的過程。我們提出了一個受認知啟發的模型，即通過心智理論（SIFToM）進行語音指令，以使機器人在不同的語音條件下通過推斷人類的目標和聯合計劃來務實地遵循人類指令，作為語音感知和理解的先驗。我們在模擬家庭實驗（VirtualHome 2）中測試了 SIFToM。結果表明，SIFToM 模型優於最先進的語音和語言模型，在具有挑戰性的語音指令後續任務中接近人類級別的準確度。然後，我們展示了它在移動機械手上執行早餐準備任務的任務規劃級別的能力。

##### **3DFacePolicy: Speech-Driven 3D Facial Animation with Diffusion Policy**
2409.10848v1 by Xuanmeng Sha, Liyun Zhang, Tomohiro Mashita, Yuki Uranishi

Audio-driven 3D facial animation has made immersive progress both in research
and application developments. The newest approaches focus on Transformer-based
methods and diffusion-based methods, however, there is still gap in the
vividness and emotional expression between the generated animation and real
human face. To tackle this limitation, we propose 3DFacePolicy, a diffusion
policy model for 3D facial animation prediction. This method generates variable
and realistic human facial movements by predicting the 3D vertex trajectory on
the 3D facial template with diffusion policy instead of facial generation for
every frame. It takes audio and vertex states as observations to predict the
vertex trajectory and imitate real human facial expressions, which keeps the
continuous and natural flow of human emotions. The experiments show that our
approach is effective in variable and dynamic facial motion synthesizing.

摘要：音訊驅動的 3D 臉部動畫在研究和應用開發上都有顯著進展。最新的方法專注於基於 Transformer 的方法和基於擴散的方法，然而，在生成的動畫和真實人臉之間在生動性和情緒表達上仍有差距。為了應對這個限制，我們提出了 3DFacePolicy，一種用於 3D 臉部動畫預測的擴散策略模型。此方法通過在 3D 臉部範本上預測 3D 頂點軌跡，而不是為每個幀生成臉部，來生成可變且逼真的真人臉部動作。它將音訊和頂點狀態作為觀察值，以預測頂點軌跡並模仿真實的人臉表情，從而保持人類情緒的連續性和自然流動。實驗表明，我們的做法在可變且動態的臉部動作合成方面是有效的。

##### **BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation**
2409.10847v1 by S. Rohollah Hosseyni, Ali Ahmad Rahmani, S. Jamal Seyedmohammadi, Sanaz Seyedin, Arash Mohammadi

Autoregressive models excel in modeling sequential dependencies by enforcing
causal constraints, yet they struggle to capture complex bidirectional patterns
due to their unidirectional nature. In contrast, mask-based models leverage
bidirectional context, enabling richer dependency modeling. However, they often
assume token independence during prediction, which undermines the modeling of
sequential dependencies. Additionally, the corruption of sequences through
masking or absorption can introduce unnatural distortions, complicating the
learning process. To address these issues, we propose Bidirectional
Autoregressive Diffusion (BAD), a novel approach that unifies the strengths of
autoregressive and mask-based generative models. BAD utilizes a
permutation-based corruption technique that preserves the natural sequence
structure while enforcing causal dependencies through randomized ordering,
enabling the effective capture of both sequential and bidirectional
relationships. Comprehensive experiments show that BAD outperforms
autoregressive and mask-based models in text-to-motion generation, suggesting a
novel pre-training strategy for sequence modeling. The codebase for BAD is
available on https://github.com/RohollahHS/BAD.

摘要：自回归模型通过强制因果约束在建模顺序依赖关系方面表现出色，但由于其单向性，它们难以捕捉复杂的双向模式。相比之下，基于掩码的模型利用双向上下文，支持更丰富的依赖关系建模。然而，它们在预测期间通常假设标记独立性，这会破坏顺序依赖关系的建模。此外，通过掩码或吸收对序列的破坏可能会引入不自然的失真，使学习过程复杂化。为了解决这些问题，我们提出了双向自回归扩散 (BAD)，这是一种新颖的方法，它统一了自回归和基于掩码的生成模型的优势。BAD 利用基于置换的破坏技术，该技术保留了自然序列结构，同时通过随机排序来强制因果依赖关系，从而能够有效捕捉顺序和双向关系。综合实验表明，BAD 在文本到动作生成中优于自回归和基于掩码的模型，这表明了一种用于序列建模的新型预训练策略。BAD 的代码库可在 https://github.com/RohollahHS/BAD 上获得。

##### **PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing**
2409.10831v1 by Phillip Long, Zachary Novack, Taylor Berg-Kirkpatrick, Julian McAuley

The recent explosion of generative AI-Music systems has raised numerous
concerns over data copyright, licensing music from musicians, and the conflict
between open-source AI and large prestige companies. Such issues highlight the
need for publicly available, copyright-free musical data, in which there is a
large shortage, particularly for symbolic music data. To alleviate this issue,
we present PDMX: a large-scale open-source dataset of over 250K public domain
MusicXML scores collected from the score-sharing forum MuseScore, making it the
largest available copyright-free symbolic music dataset to our knowledge. PDMX
additionally includes a wealth of both tag and user interaction metadata,
allowing us to efficiently analyze the dataset and filter for high quality
user-generated scores. Given the additional metadata afforded by our data
collection process, we conduct multitrack music generation experiments
evaluating how different representative subsets of PDMX lead to different
behaviors in downstream models, and how user-rating statistics can be used as
an effective measure of data quality. Examples can be found at
https://pnlong.github.io/PDMX.demo/.

摘要：最近生成式 AI-Music 系统的爆炸性增长引起了许多关于数据版权、从音乐人处获得音乐许可以及开源 AI 与大型声望公司之间的冲突的担忧。此类问题凸显了对公开可用、无版权音乐数据的需求，而此类数据严重短缺，特别是符号音乐数据。为了缓解此问题，我们提出了 PDMX：一个从乐谱分享论坛 MuseScore 收集的超过 250K 公共领域 MusicXML 乐谱的大型开源数据集，使其成为我们所知最大的可用的无版权符号音乐数据集。此外，PDMX 还包含大量标签和用户交互元数据，使我们能够有效地分析数据集并筛选高质量的用户生成乐谱。鉴于我们的数据收集过程提供的附加元数据，我们进行了多轨音乐生成实验，评估 PDMX 的不同代表性子集如何导致下游模型中的不同行为，以及如何将用户评级统计数据用作数据质量的有效衡量标准。可以在 https://pnlong.github.io/PDMX.demo/ 中找到示例。

##### **ReXErr: Synthesizing Clinically Meaningful Errors in Diagnostic Radiology Reports**
2409.10829v1 by Vishwanatha M. Rao, Serena Zhang, Julian N. Acosta, Subathra Adithan, Pranav Rajpurkar

Accurately interpreting medical images and writing radiology reports is a
critical but challenging task in healthcare. Both human-written and
AI-generated reports can contain errors, ranging from clinical inaccuracies to
linguistic mistakes. To address this, we introduce ReXErr, a methodology that
leverages Large Language Models to generate representative errors within chest
X-ray reports. Working with board-certified radiologists, we developed error
categories that capture common mistakes in both human and AI-generated reports.
Our approach uses a novel sampling scheme to inject diverse errors while
maintaining clinical plausibility. ReXErr demonstrates consistency across error
categories and produces errors that closely mimic those found in real-world
scenarios. This method has the potential to aid in the development and
evaluation of report correction algorithms, potentially enhancing the quality
and reliability of radiology reporting.

摘要：精準地解讀醫學影像並撰寫放射科報告是醫療保健中一項至關重要但具有挑戰性的任務。人寫的報告和 AI 生成的報告都可能包含錯誤，從臨床上的不準確到語言上的錯誤。為了解決這個問題，我們引入了 ReXErr，這是一個利用大型語言模型在胸部 X 光報告中產生代表性錯誤的方法。我們與通過認證的放射科醫師合作，制定了錯誤類別，以找出人寫的報告和 AI 生成的報告中常見的錯誤。我們的做法使用一種新穎的抽樣方案來注入各種錯誤，同時保持臨床上的合理性。ReXErr 在錯誤類別中展現了一致性，並產生了與現實世界場景中發現的錯誤非常相似的錯誤。此方法有可能幫助開發和評估報告修正演算法，進而提升放射科報告的品質和可靠性。

##### **Challenging Fairness: A Comprehensive Exploration of Bias in LLM-Based Recommendations**
2409.10825v1 by Shahnewaz Karim Sakib, Anindya Bijoy Das

Large Language Model (LLM)-based recommendation systems provide more
comprehensive recommendations than traditional systems by deeply analyzing
content and user behavior. However, these systems often exhibit biases,
favoring mainstream content while marginalizing non-traditional options due to
skewed training data. This study investigates the intricate relationship
between bias and LLM-based recommendation systems, with a focus on music, song,
and book recommendations across diverse demographic and cultural groups.
Through a comprehensive analysis conducted over different LLM-models, this
paper evaluates the impact of bias on recommendation outcomes. Our findings
reveal that bias is so deeply ingrained within these systems that even a
simpler intervention like prompt engineering can significantly reduce bias,
underscoring the pervasive nature of the issue. Moreover, factors like
intersecting identities and contextual information, such as socioeconomic
status, further amplify these biases, demonstrating the complexity and depth of
the challenges faced in creating fair recommendations across different groups.

摘要：大型語言模型 (LLM) 為基礎的推薦系統透過深入分析內容和使用者行為，提供比傳統系統更全面的推薦。然而，這些系統經常展現偏見，由於訓練資料的偏差，偏好主流內容，同時將非傳統選項邊緣化。本研究探討偏見與 LLM 為基礎的推薦系統之間複雜的關係，重點在於不同人口統計和文化群體中的音樂、歌曲和書籍推薦。本論文透過針對不同 LLM 模型進行的全面分析，評估偏見對推薦結果的影響。我們的發現揭露偏見深植於這些系統之中，即使是提示工程等較簡單的介入，也能顯著降低偏見，強調了此問題的普遍性。此外，相交身分和脈絡資訊等因素（例如社會經濟地位），進一步擴大了這些偏見，顯示出在不同群體之間建立公平推薦所面臨的挑戰的複雜性和深度。

##### **Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering**
2409.10790v1 by Qingru Zhang, Xiaodong Yu, Chandan Singh, Xiaodong Liu, Liyuan Liu, Jianfeng Gao, Tuo Zhao, Dan Roth, Hao Cheng

Large language models (LLMs) have demonstrated remarkable performance across
various real-world tasks. However, they often struggle to fully comprehend and
effectively utilize their input contexts, resulting in responses that are
unfaithful or hallucinated. This difficulty increases for contexts that are
long or contain distracting information, which can divert LLMs from fully
capturing essential evidence. To address this issue, many works use prompting
to help LLMs utilize contextual information more faithfully. For instance,
iterative prompting highlights key information in two steps that first ask the
LLM to identify important pieces of context and then derive answers
accordingly. However, prompting methods are constrained to highlighting key
information implicitly in token space, which is often insufficient to fully
steer the model's attention. To improve model faithfulness more reliably, we
propose AutoPASTA, a method that automatically identifies key contextual
information and explicitly highlights it by steering an LLM's attention scores.
Like prompting, AutoPASTA is applied at inference time and does not require
changing any model parameters. Our experiments on open-book QA demonstrate that
AutoPASTA effectively enables models to grasp essential contextual information,
leading to substantially improved model faithfulness and performance, e.g., an
average improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly
available at https://github.com/QingruZhang/AutoPASTA .

摘要：大型語言模型 (LLM) 已在各種真實世界任務中展現出卓越的效能。然而，它們經常難以完全理解並有效利用其輸入內容，導致回應不忠實或出現幻覺。對於長篇內容或包含分散注意力的資訊的內容，這種困難會增加，這可能會讓 LLM 無法完全擷取必要的證據。為了解決這個問題，許多作品使用提示來幫助 LLM 更忠實地利用內容資訊。例如，反覆提示會在兩個步驟中強調關鍵資訊，首先要求 LLM 找出重要的內容部分，然後據此推導答案。然而，提示方法僅限於在標記空間中隱含地強調關鍵資訊，這通常不足以完全引導模型的注意力。為了更可靠地改善模型的忠實度，我們提出 AutoPASTA，這是一種自動找出關鍵內容資訊並透過引導 LLM 的注意力分數來明確強調它的方法。與提示類似，AutoPASTA 應用於推論時間，不需要變更任何模型參數。我們在開放式問答上的實驗顯示，AutoPASTA 有效地讓模型掌握必要的內容資訊，大幅改善模型的忠實度和效能，例如，LLAMA3-70B-Instruct 的平均改善幅度為 7.95%。程式碼將公開於 https://github.com/QingruZhang/AutoPASTA。

##### **Predicting Punctuation in Ancient Chinese Texts: A Multi-Layered LSTM and Attention-Based Approach**
2409.10783v1 by Tracy Cai, Kimmy Chang, Fahad Nabi

It was only until the 20th century when the Chinese language began using
punctuation. In fact, many ancient Chinese texts contain thousands of lines
with no distinct punctuation marks or delimiters in sight. The lack of
punctuation in such texts makes it difficult for humans to identify when there
pauses or breaks between particular phrases and understand the semantic meaning
of the written text (Mogahed, 2012). As a result, unless one was educated in
the ancient time period, many readers of ancient Chinese would have
significantly different interpretations of the texts. We propose an approach to
predict the location (and type) of punctuation in ancient Chinese texts that
extends the work of Oh et al (2017) by leveraging a bidirectional multi-layered
LSTM with a multi-head attention mechanism as inspired by Luong et al.'s (2015)
discussion of attention-based architectures. We find that the use of
multi-layered LSTMs and multi-head attention significantly outperforms RNNs
that don't incorporate such components when evaluating ancient Chinese texts.

摘要：直到 20 世紀，中文才開始使用標點符號。事實上，許多古代中文文本包含數千行，沒有明顯的標點符號或分隔符號。此類文本中缺乏標點符號，讓人們難以識別特定短語之間的停頓或間隔，並理解書面文本的語義含義 (Mogahed, 2012)。因此，除非受過古代時期的教育，否則許多古代中文讀者對文本的詮釋將有顯著差異。我們提出了一種方法來預測古代中文文本中標點符號的位置 (和類型)，該方法通過利用雙向多層 LSTM 與多頭注意力機制來擴展 Oh 等人 (2017) 的工作，靈感來自 Luong 等人 (2015) 對基於注意力的架構的討論。我們發現，在評估古代中文文本時，使用多層 LSTM 和多頭注意力明顯優於未整合此類組件的 RNN。

##### **Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?**
2409.10775v1 by Kaleb Kassaw, Francesco Luzi, Leslie M. Collins, Jordan M. Malof

Image classification models, including convolutional neural networks (CNNs),
perform well on a variety of classification tasks but struggle under conditions
of partial occlusion, i.e., conditions in which objects are partially covered
from the view of a camera. Methods to improve performance under occlusion,
including data augmentation, part-based clustering, and more inherently robust
architectures, including Vision Transformer (ViT) models, have, to some extent,
been evaluated on their ability to classify objects under partial occlusion.
However, evaluations of these methods have largely relied on images containing
artificial occlusion, which are typically computer-generated and therefore
inexpensive to label. Additionally, methods are rarely compared against each
other, and many methods are compared against early, now outdated, deep learning
models. We contribute the Image Recognition Under Occlusion (IRUO) dataset,
based on the recently developed Occluded Video Instance Segmentation (OVIS)
dataset (arXiv:2102.01558). IRUO utilizes real-world and artificially occluded
images to test and benchmark leading methods' robustness to partial occlusion
in visual recognition tasks. In addition, we contribute the design and results
of a human study using images from IRUO that evaluates human classification
performance at multiple levels and types of occlusion. We find that modern
CNN-based models show improved recognition accuracy on occluded images compared
to earlier CNN-based models, and ViT-based models are more accurate than
CNN-based models on occluded images, performing only modestly worse than human
accuracy. We also find that certain types of occlusion, including diffuse
occlusion, where relevant objects are seen through "holes" in occluders such as
fences and leaves, can greatly reduce the accuracy of deep recognition models
as compared to humans, especially those with CNN backbones.

摘要：影像分類模型，包括卷積神經網路 (CNN)，在各種分類任務中表現良好，但在部分遮擋的情況下表現不佳，也就是物件在相機視角中被部分遮住的情況。改善遮擋下的表現的方法，包括資料擴充、基於部分的群集，以及本質上更穩健的架構，包括視覺Transformer (ViT) 模型，在某種程度上已根據其在部分遮擋下分類物件的能力進行評估。然而，對這些方法的評估主要依賴包含人工遮擋的影像，這些影像通常是電腦產生的，因此標記成本低廉。此外，很少將方法相互比較，而且許多方法都是與現在已過時的早期深度學習模型進行比較。我們貢獻了遮擋下的影像辨識 (IRUO) 資料集，它是基於最近開發的遮擋視訊實例分割 (OVIS) 資料集 (arXiv:2102.01558)。IRUO 利用真實世界和人工遮擋的影像來測試和評量領先方法在視覺辨識任務中對部分遮擋的穩健性。此外，我們貢獻了使用來自 IRUO 的影像的人類研究的設計和結果，該研究評估了人類在多個層級和遮擋類型下的分類表現。我們發現，與早期的基於 CNN 的模型相比，現代的基於 CNN 的模型在遮擋影像上展現出更高的辨識準確率，而基於 ViT 的模型在遮擋影像上的準確率高於基於 CNN 的模型，僅比人類準確率低一點。我們還發現，某些類型的遮擋，包括漫射遮擋，其中相關物件透過遮擋物（例如圍欄和樹葉）的「孔洞」可見，與人類相比，會大幅降低深度辨識模型的準確率，特別是那些具有 CNN 主幹的模型。

##### **Semantics Preserving Emoji Recommendation with Large Language Models**
2409.10760v1 by Zhongyi Qiu, Kangyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo

Emojis have become an integral part of digital communication, enriching text
by conveying emotions, tone, and intent. Existing emoji recommendation methods
are primarily evaluated based on their ability to match the exact emoji a user
chooses in the original text. However, they ignore the essence of users'
behavior on social media in that each text can correspond to multiple
reasonable emojis. To better assess a model's ability to align with such
real-world emoji usage, we propose a new semantics preserving evaluation
framework for emoji recommendation, which measures a model's ability to
recommend emojis that maintain the semantic consistency with the user's text.
To evaluate how well a model preserves semantics, we assess whether the
predicted affective state, demographic profile, and attitudinal stance of the
user remain unchanged. If these attributes are preserved, we consider the
recommended emojis to have maintained the original semantics. The advanced
abilities of Large Language Models (LLMs) in understanding and generating
nuanced, contextually relevant output make them well-suited for handling the
complexities of semantics preserving emoji recommendation. To this end, we
construct a comprehensive benchmark to systematically assess the performance of
six proprietary and open-source LLMs using different prompting techniques on
our task. Our experiments demonstrate that GPT-4o outperforms other LLMs,
achieving a semantics preservation score of 79.23%. Additionally, we conduct
case studies to analyze model biases in downstream classification tasks and
evaluate the diversity of the recommended emojis.

摘要：表情符已成為數位溝通中不可或缺的一部分，透過傳達情緒、語氣和意圖，豐富了文字。現有的表情符推薦方法主要根據其匹配使用者在原始文字中選擇的確切表情符的能力來評估。然而，它們忽略了使用者在社群媒體上的行為本質，因為每則文字都可能對應到多個合理的表情符。為了更佳評估模型與此類真實世界表情符使用方式一致的能力，我們提出一個新的語意保留評估架構，用於表情符推薦，用以衡量模型推薦與使用者文字維持語意一致性的能力。為了評估模型保留語意的程度，我們評估使用者預測的情感狀態、人口統計資料和態度立場是否保持不變。如果這些屬性得以保留，我們認為推薦的表情符已維持原始語意。大型語言模型 (LLM) 在理解和產生細微差別且與脈絡相關的輸出方面具有進階能力，使其非常適合處理語意保留表情符推薦的複雜性。為此，我們建構一個全面的基準，以系統性評估六個專有和開源 LLM 在我們任務中使用不同提示技術的效能。我們的實驗證明，GPT-4o 優於其他 LLM，達到 79.23% 的語意保留分數。此外，我們進行個案研究，分析下游分類任務中的模型偏誤，並評估推薦表情符的多樣性。

##### **VulnLLMEval: A Framework for Evaluating Large Language Models in Software Vulnerability Detection and Patching**
2409.10756v1 by Arastoo Zibaeirad, Marco Vieira

Large Language Models (LLMs) have shown promise in tasks like code
translation, prompting interest in their potential for automating software
vulnerability detection (SVD) and patching (SVP). To further research in this
area, establishing a benchmark is essential for evaluating the strengths and
limitations of LLMs in these tasks. Despite their capabilities, questions
remain regarding whether LLMs can accurately analyze complex vulnerabilities
and generate appropriate patches. This paper introduces VulnLLMEval, a
framework designed to assess the performance of LLMs in identifying and
patching vulnerabilities in C code. Our study includes 307 real-world
vulnerabilities extracted from the Linux kernel, creating a well-curated
dataset that includes both vulnerable and patched code. This dataset, based on
real-world code, provides a diverse and representative testbed for evaluating
LLM performance in SVD and SVP tasks, offering a robust foundation for rigorous
assessment. Our results reveal that LLMs often struggle with distinguishing
between vulnerable and patched code. Furthermore, in SVP tasks, these models
tend to oversimplify the code, producing solutions that may not be directly
usable without further refinement.

摘要：大型語言模型 (LLM) 已在程式碼翻譯等任務中展現出前景，引發人們對其自動化軟體漏洞偵測 (SVD) 和修補 (SVP) 潛力的興趣。為進一步研究此領域，建立基準對於評估 LLM 在這些任務中的優缺點至關重要。儘管它們有能力，但仍有疑問，例如 LLM 是否能準確分析複雜的漏洞並產生適當的修補程式。本文介紹 VulnLLMEval，一個旨在評估 LLM 在識別和修補 C 程式碼漏洞方面的效能的框架。我們的研究包括從 Linux 核心提取的 307 個真實漏洞，建立一個經過精心策劃的資料集，其中包括有漏洞的程式碼和已修補的程式碼。這個基於真實程式碼的資料集提供了一個多樣化且具代表性的測試平台，用於評估 LLM 在 SVD 和 SVP 任務中的效能，為嚴謹的評估提供穩固的基礎。我們的結果顯示，LLM 經常難以區分有漏洞的程式碼和已修補的程式碼。此外，在 SVP 任務中，這些模型傾向於過度簡化程式碼，產生可能無法直接使用的解決方案，而無需進一步修改。

