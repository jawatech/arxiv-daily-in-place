{"2408.04140": {"publish_time": "2024-08-08", "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models", "paper_summary": "Given the prevalence of large language models (LLMs) and the prohibitive cost\nof training these models from scratch, dynamically forgetting specific\nknowledge e.g., private or proprietary, without retraining the model has become\nan important capability. This paper proposes a novel method to achieve this\nobjective called UNLEARN. The approach builds upon subspace methods to identify\nand specifically target the removal of knowledge without adversely affecting\nother knowledge in the LLM. Results demonstrate 96% of targeted knowledge can\nbe forgotten while maintaining performance on other knowledge within 2.5% of\nthe original model, significantly outperforming the discriminatory abilities of\nthe previous state-of-the-art. A dual method called LEARN is also proposed for\ntargeted knowledge addition. Results show LEARN can match the fine-tuning\naccuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar\ntasks.", "paper_summary_zh": "\u9451\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u76db\u884c\uff0c\u4ee5\u53ca\u5f9e\u982d\u8a13\u7df4\u9019\u4e9b\u6a21\u578b\u7684\u9ad8\u6602\u6210\u672c\uff0c\u52d5\u614b\u907a\u5fd8\u7279\u5b9a\u77e5\u8b58\uff08\u4f8b\u5982\u79c1\u4eba\u6216\u5c08\u6709\u77e5\u8b58\uff09\uff0c\u800c\u7121\u9700\u91cd\u65b0\u8a13\u7df4\u6a21\u578b\uff0c\u5df2\u6210\u70ba\u4e00\u9805\u91cd\u8981\u7684\u529f\u80fd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\u4f86\u5be6\u73fe\u9019\u500b\u76ee\u6a19\uff0c\u7a31\u70ba UNLEARN\u3002\u6b64\u65b9\u6cd5\u5efa\u7acb\u5728\u5b50\u7a7a\u9593\u65b9\u6cd5\u4e4b\u4e0a\uff0c\u7528\u65bc\u8b58\u5225\u4e26\u7279\u5225\u91dd\u5c0d\u77e5\u8b58\u79fb\u9664\uff0c\u800c\u4e0d\u6703\u5c0d LLM \u4e2d\u7684\u5176\u4ed6\u77e5\u8b58\u9020\u6210\u4e0d\u5229\u5f71\u97ff\u3002\u7d50\u679c\u8868\u660e\uff0c96% \u7684\u76ee\u6a19\u77e5\u8b58\u53ef\u4ee5\u88ab\u907a\u5fd8\uff0c\u540c\u6642\u5728\u5176\u4ed6\u77e5\u8b58\u4e0a\u7684\u6548\u80fd\u7dad\u6301\u5728\u539f\u59cb\u6a21\u578b\u7684 2.5% \u4ee5\u5167\uff0c\u986f\u8457\u512a\u65bc\u5148\u524d\u6700\u5148\u9032\u6280\u8853\u7684\u8b58\u5225\u80fd\u529b\u3002\u9084\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba LEARN \u7684\u96d9\u91cd\u65b9\u6cd5\uff0c\u7528\u65bc\u76ee\u6a19\u77e5\u8b58\u65b0\u589e\u3002\u7d50\u679c\u986f\u793a\uff0cLEARN \u53ef\u4ee5\u5339\u914d\u4f4e\u79e9\u9069\u61c9 (LoRA) \u7684\u5fae\u8abf\u6e96\u78ba\u5ea6\uff0c\u800c\u4e0d\u6703\u5c0d\u985e\u4f3c\u4efb\u52d9\u9020\u6210\u4e0d\u5229\u5f71\u97ff\u3002", "author": "Tyler Lizzo et.al.", "authors": "Tyler Lizzo, Larry Heck", "id": "2408.04140v1", "paper_url": "http://arxiv.org/abs/2408.04140v1", "repo": "null"}}