{"2410.02660": {"publish_time": "2024-10-03", "title": "How to Train Long-Context Language Models (Effectively)", "paper_summary": "We study continued training and supervised fine-tuning (SFT) of a language\nmodel (LM) to make effective use of long-context information. We first\nestablish a reliable evaluation protocol to guide model development -- Instead\nof perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set\nof long-context tasks, and we evaluate models after SFT with instruction data\nas this better reveals long-context abilities. Supported by our robust\nevaluations, we run thorough experiments to decide the data mix for continued\npre-training, the instruction tuning dataset, and many other design choices. We\nfind that (1) code repositories and books are excellent sources of long data,\nbut it is crucial to combine them with high-quality short data; (2) training\nwith a sequence length beyond the evaluation length boosts long-context\nperformance; (3) for SFT, using only short instruction datasets yields strong\nperformance on long-context tasks. Our final model, ProLong-8B, which is\ninitialized from Llama-3 and trained on 40B tokens, demonstrates\nstate-of-the-art long-context performance among similarly sized models at a\nlength of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of\nlong-context tasks despite having seen only 5% as many tokens during\nlong-context training. Additionally, ProLong can effectively process up to 512K\ntokens, one of the longest context windows of publicly available LMs.", "paper_summary_zh": "<paragraph>\u6211\u5011\u7814\u7a76\u6301\u7e8c\u8a13\u7df4\u548c\u76e3\u7763\u5fae\u8abf (SFT) \u8a9e\u8a00\u6a21\u578b (LM)\uff0c\u4ee5\u6709\u6548\u5229\u7528\u9577\u8a9e\u5883\u8cc7\u8a0a\u3002\u6211\u5011\u9996\u5148\u5efa\u7acb\u4e00\u500b\u53ef\u9760\u7684\u8a55\u4f30\u5354\u5b9a\uff0c\u4ee5\u6307\u5c0e\u6a21\u578b\u958b\u767c\u2014\u2014\u6211\u5011\u4f7f\u7528\u5ee3\u6cdb\u7684\u9577\u8a9e\u5883\u4efb\u52d9\uff0c\u800c\u4e0d\u662f\u56f0\u60d1\u5ea6\u6216\u7c21\u55ae\u7684\u5927\u6d77\u6488\u91dd (NIAH) \u6e2c\u8a66\uff0c\u4e26\u5728 SFT \u5f8c\u4f7f\u7528\u8aaa\u660e\u8cc7\u6599\u8a55\u4f30\u6a21\u578b\uff0c\u56e0\u70ba\u9019\u80fd\u66f4\u597d\u5730\u63ed\u793a\u9577\u8a9e\u5883\u80fd\u529b\u3002\u5728\u6211\u5011\u7a69\u5065\u7684\u8a55\u4f30\u652f\u6301\u4e0b\uff0c\u6211\u5011\u9032\u884c\u4e86\u5fb9\u5e95\u7684\u5be6\u9a57\uff0c\u4ee5\u6c7a\u5b9a\u6301\u7e8c\u9810\u8a13\u7df4\u7684\u8cc7\u6599\u7d44\u5408\u3001\u8aaa\u660e\u5fae\u8abf\u8cc7\u6599\u96c6\u548c\u8a31\u591a\u5176\u4ed6\u8a2d\u8a08\u9078\u64c7\u3002\u6211\u5011\u767c\u73fe (1) \u7a0b\u5f0f\u78bc\u5132\u5b58\u5eab\u548c\u66f8\u7c4d\u662f\u9577\u8cc7\u6599\u7684\u7d55\u4f73\u4f86\u6e90\uff0c\u4f46\u5c07\u5b83\u5011\u8207\u9ad8\u54c1\u8cea\u7684\u77ed\u8cc7\u6599\u7d50\u5408\u8d77\u4f86\u81f3\u95dc\u91cd\u8981\uff1b(2) \u4f7f\u7528\u8d85\u904e\u8a55\u4f30\u9577\u5ea6\u7684\u5e8f\u5217\u9577\u5ea6\u9032\u884c\u8a13\u7df4\u6703\u63d0\u5347\u9577\u8a9e\u5883\u6548\u80fd\uff1b(3) \u5c0d\u65bc SFT\uff0c\u50c5\u4f7f\u7528\u77ed\u8aaa\u660e\u8cc7\u6599\u96c6\u6703\u5728\u9577\u8a9e\u5883\u4efb\u52d9\u4e2d\u7522\u751f\u5f37\u52c1\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u6700\u7d42\u6a21\u578b ProLong-8B\uff0c\u7531 Llama-3 \u521d\u59cb\u5316\u4e26\u5728 40B \u500b\u4ee3\u5e63\u4e0a\u8a13\u7df4\uff0c\u5728\u9577\u5ea6\u70ba 128K \u6642\u5c55\u73fe\u51fa\u8207\u985e\u4f3c\u898f\u6a21\u6a21\u578b\u76f8\u6bd4\u6700\u5148\u9032\u7684\u9577\u8a9e\u5883\u6548\u80fd\u3002\u5118\u7ba1\u5728\u9577\u8a9e\u5883\u8a13\u7df4\u671f\u9593\u50c5\u770b\u5230 5% \u7684\u4ee3\u5e63\uff0c\u4f46 ProLong \u5728\u5927\u591a\u6578\u9577\u8a9e\u5883\u4efb\u52d9\u4e0a\u90fd\u512a\u65bc Llama-3.18B-Instruct\u3002\u6b64\u5916\uff0cProLong \u53ef\u4ee5\u6709\u6548\u8655\u7406\u591a\u9054 512K \u500b\u4ee3\u5e63\uff0c\u9019\u662f\u516c\u958b LM \u4e2d\u6700\u9577\u7684\u4e0a\u4e0b\u6587\u8996\u7a97\u4e4b\u4e00\u3002</paragraph>", "author": "Tianyu Gao et.al.", "authors": "Tianyu Gao, Alexander Wettig, Howard Yen, Danqi Chen", "id": "2410.02660v1", "paper_url": "http://arxiv.org/abs/2410.02660v1", "repo": "https://github.com/princeton-nlp/prolong"}}