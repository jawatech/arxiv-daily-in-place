{"2411.07990": {"publish_time": "2024-11-12", "title": "Derivational Morphology Reveals Analogical Generalization in Large Language Models", "paper_summary": "What mechanisms underlie linguistic generalization in large language models\n(LLMs)? This question has attracted considerable attention, with most studies\nanalyzing the extent to which the language skills of LLMs resemble rules. As of\nyet, it is not known whether linguistic generalization in LLMs could equally\nwell be explained as the result of analogical processes, which can be\nformalized as similarity operations on stored exemplars. A key shortcoming of\nprior research is its focus on linguistic phenomena with a high degree of\nregularity, for which rule-based and analogical approaches make the same\npredictions. Here, we instead examine derivational morphology, specifically\nEnglish adjective nominalization, which displays notable variability. We\nintroduce a new method for investigating linguistic generalization in LLMs:\nfocusing on GPT-J, we fit cognitive models that instantiate rule-based and\nanalogical learning to the LLM training data and compare their predictions on a\nset of nonce adjectives with those of the LLM, allowing us to draw direct\nconclusions regarding underlying mechanisms. As expected, rule-based and\nanalogical models explain the predictions of GPT-J equally well for adjectives\nwith regular nominalization patterns. However, for adjectives with variable\nnominalization patterns, the analogical model provides a much better match.\nFurthermore, GPT-J's behavior is sensitive to the individual word frequencies,\neven for regular forms, a behavior that is consistent with an analogical\naccount of regular forms but not a rule-based one. These findings refute the\nhypothesis that GPT-J's linguistic generalization on adjective nominalization\ninvolves rules, suggesting similarity operations on stored exemplars as the\nunderlying mechanism. Overall, our study suggests that analogical processes\nplay a bigger role in the linguistic generalization of LLMs than previously\nthought.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u8a9e\u8a00\u6982\u62ec\u5316\u7684\u5e95\u5c64\u6a5f\u5236\u662f\u4ec0\u9ebc\uff1f\u9019\u500b\u554f\u984c\u5f15\u8d77\u4e86\u76f8\u7576\u5927\u7684\u95dc\u6ce8\uff0c\u5927\u591a\u6578\u7814\u7a76\u5206\u6790\u4e86 LLM \u7684\u8a9e\u8a00\u6280\u80fd\u8207\u898f\u5247\u7684\u76f8\u4f3c\u7a0b\u5ea6\u3002\u5230\u76ee\u524d\u70ba\u6b62\uff0c\u6211\u5011\u9084\u4e0d\u77e5\u9053 LLM \u4e2d\u7684\u8a9e\u8a00\u6982\u62ec\u5316\u662f\u5426\u53ef\u4ee5\u540c\u6a23\u89e3\u91cb\u70ba\u985e\u6bd4\u904e\u7a0b\u7684\u7d50\u679c\uff0c\u985e\u6bd4\u904e\u7a0b\u53ef\u4ee5\u5f62\u5f0f\u5316\u70ba\u5132\u5b58\u7bc4\u4f8b\u7684\u76f8\u4f3c\u6027\u904b\u7b97\u3002\u5148\u524d\u7814\u7a76\u7684\u4e00\u500b\u4e3b\u8981\u7f3a\u9ede\u662f\u5176\u91cd\u9ede\u5728\u65bc\u9ad8\u5ea6\u898f\u5f8b\u6027\u7684\u8a9e\u8a00\u73fe\u8c61\uff0c\u5c0d\u65bc\u9019\u7a2e\u73fe\u8c61\uff0c\u57fa\u65bc\u898f\u5247\u548c\u985e\u6bd4\u7684\u65b9\u6cd5\u6703\u505a\u51fa\u76f8\u540c\u7684\u9810\u6e2c\u3002\u5728\u9019\u88e1\uff0c\u6211\u5011\u6539\u70ba\u6aa2\u9a57\u6d3e\u751f\u5f62\u614b\uff0c\u7279\u5225\u662f\u82f1\u8a9e\u5f62\u5bb9\u8a5e\u540d\u8a5e\u5316\uff0c\u5b83\u986f\u793a\u51fa\u986f\u8457\u7684\u53ef\u8b8a\u6027\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\u4f86\u7814\u7a76 LLM \u4e2d\u7684\u8a9e\u8a00\u6982\u62ec\u5316\uff1a\u5c08\u6ce8\u65bc GPT-J\uff0c\u6211\u5011\u5c07\u5be6\u4f8b\u5316\u57fa\u65bc\u898f\u5247\u548c\u985e\u6bd4\u5b78\u7fd2\u7684\u8a8d\u77e5\u6a21\u578b\u5957\u7528\u5230 LLM \u8a13\u7df4\u8cc7\u6599\uff0c\u4e26\u5c07\u5176\u9810\u6e2c\u8207 LLM \u5728\u4e00\u7d44\u65b0\u9020\u5f62\u5bb9\u8a5e\u4e0a\u9032\u884c\u6bd4\u8f03\uff0c\u8b93\u6211\u5011\u80fd\u5920\u5c0d\u5e95\u5c64\u6a5f\u5236\u5f97\u51fa\u76f4\u63a5\u7d50\u8ad6\u3002\u6b63\u5982\u9810\u671f\u7684\u90a3\u6a23\uff0c\u5c0d\u65bc\u5177\u6709\u898f\u5247\u540d\u8a5e\u5316\u6a21\u5f0f\u7684\u5f62\u5bb9\u8a5e\uff0c\u57fa\u65bc\u898f\u5247\u548c\u985e\u6bd4\u7684\u6a21\u578b\u5c0d GPT-J \u7684\u9810\u6e2c\u89e3\u91cb\u5f97\u4e00\u6a23\u597d\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u5177\u6709\u53ef\u8b8a\u540d\u8a5e\u5316\u6a21\u5f0f\u7684\u5f62\u5bb9\u8a5e\uff0c\u985e\u6bd4\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5339\u914d\u3002\u6b64\u5916\uff0cGPT-J \u7684\u884c\u70ba\u5c0d\u500b\u5225\u5b57\u8a5e\u983b\u7387\u5f88\u654f\u611f\uff0c\u5373\u4f7f\u662f\u898f\u5247\u5f62\u5f0f\u4e5f\u662f\u5982\u6b64\uff0c\u9019\u7a2e\u884c\u70ba\u8207\u985e\u6bd4\u898f\u5247\u7684\u8aaa\u660e\u4e00\u81f4\uff0c\u4f46\u8207\u57fa\u65bc\u898f\u5247\u7684\u8aaa\u660e\u4e0d\u4e00\u81f4\u3002\u9019\u4e9b\u767c\u73fe\u99c1\u65a5\u4e86 GPT-J \u5728\u5f62\u5bb9\u8a5e\u540d\u8a5e\u5316\u4e0a\u7684\u8a9e\u8a00\u6982\u62ec\u5316\u6d89\u53ca\u898f\u5247\u7684\u5047\u8a2d\uff0c\u8868\u660e\u5c0d\u5132\u5b58\u7bc4\u4f8b\u7684\u76f8\u4f3c\u6027\u904b\u7b97\u624d\u662f\u5e95\u5c64\u6a5f\u5236\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u7684\u7814\u7a76\u8868\u660e\uff0c\u985e\u6bd4\u904e\u7a0b\u5728 LLM \u7684\u8a9e\u8a00\u6982\u62ec\u5316\u4e2d\u6240\u626e\u6f14\u7684\u89d2\u8272\u6bd4\u5148\u524d\u60f3\u50cf\u7684\u66f4\u5927\u3002", "author": "Valentin Hofmann et.al.", "authors": "Valentin Hofmann, Leonie Weissweiler, David Mortensen, Hinrich Sch\u00fctze, Janet Pierrehumbert", "id": "2411.07990v1", "paper_url": "http://arxiv.org/abs/2411.07990v1", "repo": "null"}}