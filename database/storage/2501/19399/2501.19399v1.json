{"2501.19399": {"publish_time": "2025-01-31", "title": "Scalable-Softmax Is Superior for Attention", "paper_summary": "The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.", "paper_summary_zh": "\u968f\u7740\u8f93\u5165\u5411\u91cf\u5927\u5c0f\u7684\u589e\u52a0\uff0cSoftmax \u51fd\u6570\u8f93\u51fa\u5411\u91cf\u7684\u6700\u5927\u5143\u7d20\u63a5\u8fd1\u4e8e\u96f6\u3002\u57fa\u4e8e Transformer \u7684\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56 Softmax \u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u5206\u5e03\u5728\u4e0a\u4e0b\u6587\u5927\u5c0f\u589e\u957f\u65f6\u8d8b\u4e8e\u5e73\u5766\u3002\u8fd9\u964d\u4f4e\u4e86\u6a21\u578b\u6709\u6548\u4f18\u5148\u8003\u8651\u5173\u952e\u4fe1\u606f\u7684\u7684\u80fd\u529b\uff0c\u5e76\u53ef\u80fd\u9650\u5236\u5176\u957f\u5ea6\u6cdb\u5316\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53ef\u6269\u5c55 Softmax (SSMax)\uff0c\u5b83\u5728\u8f93\u5165\u5411\u91cf\u5927\u5c0f\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u66ff\u6362 Softmax\u3002SSMax \u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u57fa\u4e8e Transformer \u7684\u67b6\u6784\u4e2d\u3002\u8bed\u8a00\u5efa\u6a21\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528 SSMax \u7684\u6a21\u578b\u4e0d\u4ec5\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u635f\u5931\u964d\u4f4e\uff0c\u800c\u4e14\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u5173\u952e\u4fe1\u606f\u68c0\u7d22\u65b9\u9762\u4e5f\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5bf9\u6ce8\u610f\u529b\u5206\u6570\u7684\u5206\u6790\u8868\u660e\uff0cSSMax \u4f7f\u6a21\u578b\u80fd\u591f\u5373\u4f7f\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u4e5f\u80fd\u5c06\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u5173\u952e\u4fe1\u606f\u4e0a\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u4ece\u9884\u8bad\u7ec3\u5f00\u59cb\u5c31\u4f7f\u7528 SSMax \u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u957f\u5ea6\u6cdb\u5316\uff0c\u4f46\u5df2\u7ecf\u5f00\u59cb\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u4ecd\u7136\u53ef\u4ee5\u901a\u8fc7\u5728\u6ce8\u610f\u529b\u5c42\u4e2d\u7528 SSMax \u66ff\u6362 Softmax\uff08\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u6216\u4e4b\u540e\uff09\u6765\u83b7\u5f97\u4e00\u4e9b\u8fd9\u79cd\u80fd\u529b\u3002", "author": "Ken M. Nakanishi et.al.", "authors": "Ken M. Nakanishi", "id": "2501.19399v1", "paper_url": "http://arxiv.org/abs/2501.19399v1", "repo": "null"}}