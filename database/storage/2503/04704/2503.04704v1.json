{"2503.04704": {"publish_time": "2025-03-06", "title": "Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size", "paper_summary": "We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures-from 1.6B to 70B parameters-showcasing consistent\nimprovements in the quality-compression trade-off regardless of model scale or\narchitectural design. A surprising finding of EWQ is its ability to reduce\nperplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u9078\u64c7\u6027\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\uff0c\u7a31\u70ba\u71b5\u6b0a\u91cd\u91cf\u5316 (EWQ)\uff0c\u5b83\u8d85\u8d8a\u4e86\u91dd\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u7279\u5b9a\u67b6\u69cb\u548c\u5927\u5c0f\u76f8\u95dc\u7684\u58d3\u7e2e\u65b9\u6cd5\u7684\u9650\u5236\u3002\u901a\u904e\u5206\u6790 transformer \u6a21\u584a\u4e2d\u7684\u71b5\u5206\u4f48\uff0cEWQ \u53ef\u4ee5\u78ba\u5b9a\u54ea\u4e9b\u6a21\u584a\u53ef\u4ee5\u5b89\u5168\u5730\u91cf\u5316\u800c\u4e0d\u6703\u5c0e\u81f4\u986f\u8457\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u9019\u8207\u6a21\u578b\u67b6\u69cb\u6216\u5927\u5c0f\u7121\u95dc\u3002\u6211\u5011\u7684\u65b9\u6cd5\u512a\u65bc\u7d71\u4e00\u91cf\u5316\u65b9\u6cd5\uff0c\u5728\u5c07\u5927\u898f\u6a21\u591a\u4efb\u52d9\u8a9e\u8a00\u7406\u89e3 (MMLU) \u7684\u6e96\u78ba\u5ea6\u5206\u6578\u4fdd\u6301\u5728\u8207\u672a\u91cf\u5316\u6a21\u578b\u7684 0.5% \u4ee5\u5167\uff0c\u540c\u6642\u6e1b\u5c11\u9ad8\u9054 18% \u7684\u5167\u5b58\u4f7f\u7528\u91cf\u3002\u6211\u5011\u5c55\u793a\u4e86 EWQ \u5728\u591a\u500b\u67b6\u69cb\uff08\u5f9e 1.6B \u5230 70B \u53c3\u6578\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u7121\u8ad6\u6a21\u578b\u898f\u6a21\u6216\u67b6\u69cb\u8a2d\u8a08\u5982\u4f55\uff0c\u5728\u8cea\u91cf-\u58d3\u7e2e\u6b0a\u8861\u65b9\u9762\u59cb\u7d42\u5982\u4e00\u7684\u6539\u9032\u3002EWQ \u7684\u4e00\u500b\u9a5a\u4eba\u767c\u73fe\u662f\u5b83\u80fd\u5920\u964d\u4f4e\u8207\u672a\u91cf\u5316\u6a21\u578b\u76f8\u6bd4\u7684\u56f0\u60d1\u5ea6\uff0c\u9019\u8868\u660e\u901a\u904e\u9078\u64c7\u6027\u7cbe\u5ea6\u964d\u4f4e\u5b58\u5728\u6709\u76ca\u7684\u6b63\u5247\u5316\u3002\u9019\u7a2e\u6539\u9032\u9069\u7528\u65bc\u4e0d\u540c\u7684\u6a21\u578b\u7cfb\u5217\uff0c\u8868\u660e\u5c64\u7d1a\u71b5\u548c\u6700\u4f73\u7cbe\u5ea6\u8981\u6c42\u4e4b\u9593\u5b58\u5728\u6839\u672c\u7684\u95dc\u4fc2\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86 FastEWQ\uff0c\u9019\u662f\u4e00\u7a2e\u7528\u65bc\u71b5\u5206\u4f48\u5206\u6790\u7684\u5feb\u901f\u65b9\u6cd5\uff0c\u5b83\u6d88\u9664\u4e86\u52a0\u8f09\u6a21\u578b\u6b0a\u91cd\u7684\u9700\u8981\u3002\u8a72\u6280\u8853\u5229\u7528\u4e86\u71b5\u5206\u4f48\u7684\u666e\u904d\u7279\u6027\uff0c\u9019\u4e9b\u7279\u6027\u5728\u5404\u7a2e\u67b6\u69cb\u548c\u898f\u6a21\u4e2d\u90fd\u5b58\u5728\uff0c\u5f9e\u800c\u5be6\u73fe\u4e86\u8fd1\u4e4e\u5373\u6642\u7684\u91cf\u5316\u6c7a\u7b56\uff0c\u540c\u6642\u5728\u5b8c\u6574\u71b5\u5206\u6790\u7684\u60c5\u6cc1\u4e0b\u4fdd\u6301\u4e86 80% \u7684\u5206\u985e\u6e96\u78ba\u7387\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u53ef\u4ee5\u7368\u7acb\u65bc\u7279\u5b9a\u67b6\u69cb\u9078\u64c7\u6216\u6a21\u578b\u5927\u5c0f\u4f86\u958b\u767c\u6709\u6548\u7684\u91cf\u5316\u7b56\u7565\uff0c\u5f9e\u800c\u70ba\u9ad8\u6548\u7684 LLM \u90e8\u7f72\u958b\u95e2\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\n", "author": "Alireza Behtash et.al.", "authors": "Alireza Behtash, Marijan Fofonjka, Ethan Baird, Tyler Mauer, Hossein Moghimifam, David Stout, Joel Dennison", "id": "2503.04704v1", "paper_url": "http://arxiv.org/abs/2503.04704v1", "repo": "null"}}