{"2405.00332": {"publish_time": "2024-05-01", "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic", "paper_summary": "Large language models (LLMs) have achieved impressive success on many\nbenchmarks for mathematical reasoning. However, there is growing concern that\nsome of this performance actually reflects dataset contamination, where data\nclosely resembling benchmark questions leaks into the training data, instead of\ntrue reasoning ability. To investigate this claim rigorously, we commission\nGrade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and\ncomplexity of the established GSM8k benchmark, the gold standard for measuring\nelementary mathematical reasoning. We ensure that the two benchmarks are\ncomparable across important metrics such as human solve rates, number of steps\nin solution, answer magnitude, and more. When evaluating leading open- and\nclosed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with\nseveral families of models (e.g., Phi and Mistral) showing evidence of\nsystematic overfitting across almost all model sizes. At the same time, many\nmodels, especially those on the frontier, (e.g., Gemini/GPT/Claude) show\nminimal signs of overfitting. Further analysis suggests a positive relationship\n(Spearman's r^2=0.32) between a model's probability of generating an example\nfrom GSM8k and its performance gap between GSM8k and GSM1k, suggesting that\nmany models may have partially memorized GSM8k.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a31\u591a\u6578\u5b78\u63a8\u7406\u57fa\u6e96\u6e2c\u8a66\u4e2d\u90fd\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u65e5\u76ca\u4ee4\u4eba\u64d4\u6182\u7684\u662f\uff0c\u5176\u4e2d\u4e00\u4e9b\u8868\u73fe\u5be6\u969b\u4e0a\u53cd\u6620\u4e86\u8cc7\u6599\u96c6\u6c61\u67d3\uff0c\u5176\u4e2d\u8207\u57fa\u6e96\u6e2c\u8a66\u554f\u984c\u975e\u5e38\u76f8\u4f3c\u7684\u8cc7\u6599\u6703\u6ef2\u5165\u8a13\u7df4\u8cc7\u6599\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u3002\u70ba\u4e86\u56b4\u8b39\u5730\u8abf\u67e5\u6b64\u8aaa\u6cd5\uff0c\u6211\u5011\u59d4\u8a17\u5c0f\u5b78\u6578\u5b78 1000 (GSM1k)\u3002GSM1k \u7684\u8a2d\u8a08\u65e8\u5728\u53cd\u6620\u5df2\u5efa\u7acb\u7684 GSM8k \u57fa\u6e96\u7684\u98a8\u683c\u548c\u8907\u96dc\u6027\uff0c\u9019\u662f\u8861\u91cf\u5c0f\u5b78\u6578\u5b78\u63a8\u7406\u7684\u9ec3\u91d1\u6a19\u6e96\u3002\u6211\u5011\u78ba\u4fdd\u9019\u5169\u500b\u57fa\u6e96\u5728\u4eba\u985e\u89e3\u6c7a\u7387\u3001\u89e3\u6c7a\u6b65\u9a5f\u6578\u3001\u7b54\u6848\u5927\u5c0f\u7b49\u91cd\u8981\u6307\u6a19\u4e0a\u5177\u6709\u53ef\u6bd4\u6027\u3002\u5728 GSM1k \u4e0a\u8a55\u4f30\u9818\u5148\u7684\u958b\u6e90\u548c\u9589\u6e90 LLM \u6642\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u6e96\u78ba\u5ea6\u4e0b\u964d\u4e86 13%\uff0c\u5176\u4e2d\u5e7e\u7d44\u6a21\u578b\uff08\u4f8b\u5982 Phi \u548c Mistral\uff09\u986f\u793a\u51fa\u5e7e\u4e4e\u6240\u6709\u6a21\u578b\u5927\u5c0f\u7684\u7cfb\u7d71\u6027\u904e\u5ea6\u64ec\u5408\u7684\u8b49\u64da\u3002\u8207\u6b64\u540c\u6642\uff0c\u8a31\u591a\u6a21\u578b\uff0c\u5c24\u5176\u662f\u524d\u6cbf\u6a21\u578b\uff08\u4f8b\u5982 Gemini/GPT/Claude\uff09\uff0c\u8868\u73fe\u51fa\u6700\u5c0f\u7684\u904e\u5ea6\u64ec\u5408\u8de1\u8c61\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u5f9e GSM8k \u751f\u6210\u7bc4\u4f8b\u7684\u6a5f\u7387\u8207\u5176\u5728 GSM8k \u548c GSM1k \u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\u4e4b\u9593\u5b58\u5728\u6b63\u76f8\u95dc\u95dc\u4fc2\uff08Spearman's r^2=0.32\uff09\uff0c\u9019\u8868\u660e\u8a31\u591a\u6a21\u578b\u53ef\u80fd\u5df2\u7d93\u90e8\u5206\u8a18\u61b6\u4e86 GSM8k\u3002", "author": "Hugh Zhang et.al.", "authors": "Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue", "id": "2405.00332v3", "paper_url": "http://arxiv.org/abs/2405.00332v3", "repo": "null"}}