{"2410.08829": {"publish_time": "2024-10-11", "title": "Unveiling Molecular Secrets: An LLM-Augmented Linear Model for Explainable and Calibratable Molecular Property Prediction", "paper_summary": "Explainable molecular property prediction is essential for various scientific\nfields, such as drug discovery and material science. Despite delivering\nintrinsic explainability, linear models struggle with capturing complex,\nnon-linear patterns. Large language models (LLMs), on the other hand, yield\naccurate predictions through powerful inference capabilities yet fail to\nprovide chemically meaningful explanations for their predictions. This work\nproposes a novel framework, called MoleX, which leverages LLM knowledge to\nbuild a simple yet powerful linear model for accurate molecular property\nprediction with faithful explanations. The core of MoleX is to model\ncomplicated molecular structure-property relationships using a simple linear\nmodel, augmented by LLM knowledge and a crafted calibration strategy.\nSpecifically, to extract the maximum amount of task-relevant knowledge from LLM\nembeddings, we employ information bottleneck-inspired fine-tuning and\nsparsity-inducing dimensionality reduction. These informative embeddings are\nthen used to fit a linear model for explainable inference. Moreover, we\nintroduce residual calibration to address prediction errors stemming from\nlinear models' insufficient expressiveness of complex LLM embeddings, thus\nrecovering the LLM's predictive power and boosting overall accuracy.\nTheoretically, we provide a mathematical foundation to justify MoleX's\nexplainability. Extensive experiments demonstrate that MoleX outperforms\nexisting methods in molecular property prediction, establishing a new milestone\nin predictive performance, explainability, and efficiency. In particular, MoleX\nenables CPU inference and accelerates large-scale dataset processing, achieving\ncomparable performance 300x faster with 100,000 fewer parameters than LLMs.\nAdditionally, the calibration improves model performance by up to 12.7% without\ncompromising explainability.", "paper_summary_zh": "\u53ef\u89e3\u91cb\u7684\u5206\u5b50\u6027\u8cea\u9810\u6e2c\u5c0d\u65bc\u5404\u7a2e\u79d1\u5b78\u9818\u57df\u81f3\u95dc\u91cd\u8981\uff0c\u4f8b\u5982\u85e5\u7269\u767c\u73fe\u548c\u6750\u6599\u79d1\u5b78\u3002\u5118\u7ba1\u7dda\u6027\u6a21\u578b\u63d0\u4f9b\u4e86\u5167\u5728\u7684\u53ef\u89e3\u91cb\u6027\uff0c\u4f46\u5b83\u5011\u96e3\u4ee5\u6355\u6349\u8907\u96dc\u7684\u975e\u7dda\u6027\u6a21\u5f0f\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u5f37\u5927\u7684\u63a8\u8ad6\u80fd\u529b\u7522\u751f\u6e96\u78ba\u7684\u9810\u6e2c\uff0c\u4f46\u7121\u6cd5\u70ba\u5176\u9810\u6e2c\u63d0\u4f9b\u6709\u610f\u7fa9\u7684\u5316\u5b78\u89e3\u91cb\u3002\u9019\u9805\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba MoleX \u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5229\u7528 LLM \u77e5\u8b58\u70ba\u6e96\u78ba\u7684\u5206\u5b50\u6027\u8cea\u9810\u6e2c\u69cb\u5efa\u4e00\u500b\u7c21\u55ae\u4f46\u5f37\u5927\u7684\u7dda\u6027\u6a21\u578b\uff0c\u4e26\u63d0\u4f9b\u5fe0\u5be6\u7684\u89e3\u91cb\u3002Molex \u7684\u6838\u5fc3\u662f\u4f7f\u7528\u4e00\u500b\u7c21\u55ae\u7684\u7dda\u6027\u6a21\u578b\u5c0d\u8907\u96dc\u7684\u5206\u5b50\u7d50\u69cb-\u6027\u8cea\u95dc\u4fc2\u9032\u884c\u5efa\u6a21\uff0c\u4e26\u7531 LLM \u77e5\u8b58\u548c\u7cbe\u5fc3\u88fd\u4f5c\u7684\u6821\u6e96\u7b56\u7565\u9032\u884c\u64f4\u5145\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u70ba\u4e86\u5f9e LLM \u5167\u5d4c\u4e2d\u63d0\u53d6\u6700\u5927\u91cf\u7684\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u77e5\u8b58\uff0c\u6211\u5011\u63a1\u7528\u4e86\u53d7\u8cc7\u8a0a\u74f6\u9838\u555f\u767c\u7684\u5fae\u8abf\u548c\u7a00\u758f\u6027\u8a98\u5c0e\u964d\u7dad\u3002\u7136\u5f8c\u4f7f\u7528\u9019\u4e9b\u8cc7\u8a0a\u6027\u5167\u5d4c\u4f86\u64ec\u5408\u4e00\u500b\u7dda\u6027\u6a21\u578b\u4ee5\u9032\u884c\u53ef\u89e3\u91cb\u7684\u63a8\u8ad6\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u6b98\u5dee\u6821\u6e96\u4f86\u89e3\u6c7a\u6e90\u81ea\u7dda\u6027\u6a21\u578b\u5c0d\u8907\u96dc LLM \u5167\u5d4c\u8868\u9054\u4e0d\u8db3\u7684\u9810\u6e2c\u8aa4\u5dee\uff0c\u5f9e\u800c\u6062\u5fa9 LLM \u7684\u9810\u6e2c\u80fd\u529b\u4e26\u63d0\u9ad8\u6574\u9ad4\u6e96\u78ba\u6027\u3002\u5728\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u6578\u5b78\u57fa\u790e\u4f86\u8b49\u660e MoleX \u7684\u53ef\u89e3\u91cb\u6027\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8868\u660e\uff0cMolex \u5728\u5206\u5b50\u6027\u8cea\u9810\u6e2c\u65b9\u9762\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u5728\u9810\u6e2c\u6027\u80fd\u3001\u53ef\u89e3\u91cb\u6027\u548c\u6548\u7387\u65b9\u9762\u6a39\u7acb\u4e86\u65b0\u7684\u91cc\u7a0b\u7891\u3002\u7279\u5225\u662f\uff0cMolex \u652f\u63f4 CPU \u63a8\u8ad6\u4e26\u52a0\u901f\u5927\u898f\u6a21\u8cc7\u6599\u96c6\u8655\u7406\uff0c\u8207 LLM \u76f8\u6bd4\uff0c\u4ee5\u5c11 100,000 \u500b\u53c3\u6578\u7684\u901f\u5ea6\u5be6\u73fe\u4e86\u5feb 300 \u500d\u7684\u540c\u7b49\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6821\u6e96\u5728\u4e0d\u640d\u5bb3\u53ef\u89e3\u91cb\u6027\u7684\u60c5\u6cc1\u4e0b\u5c07\u6a21\u578b\u6027\u80fd\u63d0\u9ad8\u4e86 12.7%\u3002", "author": "Zhuoran Li et.al.", "authors": "Zhuoran Li, Xu Sun, Wanyu Lin, Jiannong Cao", "id": "2410.08829v1", "paper_url": "http://arxiv.org/abs/2410.08829v1", "repo": "null"}}