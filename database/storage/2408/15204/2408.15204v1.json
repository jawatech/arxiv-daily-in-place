{"2408.15204": {"publish_time": "2024-08-27", "title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?", "paper_summary": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5df2\u5c55\u73fe\u51fa\u8207\u4eba\u985e\u8a55\u5206\u8005\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8b49\u660e\u4e86\u5176\u6e1b\u8f15\u4eba\u985e\u8cc7\u6599\u6536\u96c6\u6311\u6230\u7684\u6f5b\u529b\u3002\u5728\u8a08\u7b97\u793e\u6703\u79d1\u5b78 (CSS) \u4e2d\uff0c\u7814\u7a76\u4eba\u54e1\u6b63\u65e5\u76ca\u5229\u7528 LLM \u6a19\u8a3b\u4f86\u88dc\u5145\u7de9\u6162\u4e14\u6602\u8cb4\u7684\u4eba\u985e\u6a19\u8a3b\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u5728\u4e0d\u640d\u5bb3\u4e0b\u6e38\u7d50\u8ad6\u7684\u6709\u6548\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u6536\u96c6\u548c\u4f7f\u7528 LLM \u6a19\u8a3b\u7684\u6e96\u5247\u4ecd\u7136\u6709\u9650\u3002\u6211\u5011\u5f15\u5165\u4e86\u4fe1\u5fc3\u9a45\u52d5\u63a8\u8ad6\uff1a\u4e00\u7a2e\u7d50\u5408 LLM \u6a19\u8a3b\u548c LLM \u4fe1\u5fc3\u6307\u6a19\u7684\u65b9\u6cd5\uff0c\u4ee5\u7b56\u7565\u6027\u5730\u9078\u64c7\u61c9\u6536\u96c6\u54ea\u4e9b\u4eba\u985e\u6a19\u8a3b\uff0c\u76ee\u6a19\u662f\u5728\u6e1b\u5c11\u6240\u9700\u4eba\u985e\u6a19\u8a3b\u6578\u91cf\u7684\u540c\u6642\u7522\u751f\u6e96\u78ba\u7684\u7d71\u8a08\u4f30\u8a08\u548c\u53ef\u8b49\u660e\u6709\u6548\u7684\u4fe1\u5fc3\u5340\u9593\u3002\u6211\u5011\u7684\u505a\u6cd5\u5e36\u6709\u91dd\u5c0d\u54c1\u8cea\u4e0d\u4f73\u7684 LLM \u6a19\u8a3b\u7684\u4fdd\u969c\u63aa\u65bd\uff0c\u78ba\u4fdd\u7d50\u8ad6\u65e2\u6709\u6548\u53c8\u4e0d\u6703\u6bd4\u6211\u5011\u50c5\u4f9d\u8cf4\u4eba\u985e\u6a19\u8a3b\u6642\u4e0d\u6e96\u78ba\u3002\u6211\u5011\u5728\u4e09\u500b CSS \u8a2d\u5b9a\uff08\u6587\u5b57\u79ae\u8c8c\u3001\u7acb\u5834\u548c\u504f\u898b\uff09\u4e2d\uff0c\u8b49\u660e\u4e86\u4fe1\u5fc3\u9a45\u52d5\u63a8\u8ad6\u512a\u65bc\u57fa\u7dda\u7684\u6709\u6548\u6027\uff0c\u5728\u6bcf\u500b\u8a2d\u5b9a\u4e2d\u5c07\u6240\u9700\u7684\u4eba\u985e\u6a19\u8a3b\u6578\u91cf\u6e1b\u5c11\u4e86 25% \u4ee5\u4e0a\u3002\u5118\u7ba1\u6211\u5011\u4f7f\u7528 CSS \u8a2d\u5b9a\u9032\u884c\u793a\u7bc4\uff0c\u4f46\u4fe1\u5fc3\u9a45\u52d5\u63a8\u8ad6\u53ef\u7528\u65bc\u4f30\u8a08\u5ee3\u6cdb NLP \u554f\u984c\u4e2d\u7684\u5927\u591a\u6578\u6a19\u6e96\u91cf\u3002", "author": "Kristina Gligori\u0107 et.al.", "authors": "Kristina Gligori\u0107, Tijana Zrnic, Cinoo Lee, Emmanuel J. Cand\u00e8s, Dan Jurafsky", "id": "2408.15204v1", "paper_url": "http://arxiv.org/abs/2408.15204v1", "repo": "null"}}