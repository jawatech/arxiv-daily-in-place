{"2501.18801": {"publish_time": "2025-01-30", "title": "Every Image Listens, Every Image Dances: Music-Driven Image Animation", "paper_summary": "Image animation has become a promising area in multimodal research, with a\nfocus on generating videos from reference images. While prior work has largely\nemphasized generic video generation guided by text, music-driven dance video\ngeneration remains underexplored. In this paper, we introduce MuseDance, an\ninnovative end-to-end model that animates reference images using both music and\ntext inputs. This dual input enables MuseDance to generate personalized videos\nthat follow text descriptions and synchronize character movements with the\nmusic. Unlike existing approaches, MuseDance eliminates the need for complex\nmotion guidance inputs, such as pose or depth sequences, making flexible and\ncreative video generation accessible to users of all expertise levels. To\nadvance research in this field, we present a new multimodal dataset comprising\n2,904 dance videos with corresponding background music and text descriptions.\nOur approach leverages diffusion-based methods to achieve robust\ngeneralization, precise control, and temporal consistency, setting a new\nbaseline for the music-driven image animation task.", "paper_summary_zh": "\u5f71\u50cf\u52d5\u756b\u5df2\u6210\u70ba\u591a\u6a21\u614b\u7814\u7a76\u4e2d\u4e00\u500b\u6709\u524d\u666f\u7684\u9818\u57df\uff0c\u91cd\u9ede\u5728\u65bc\u5f9e\u53c3\u8003\u5f71\u50cf\u4e2d\u7522\u751f\u5f71\u7247\u3002\u5118\u7ba1\u5148\u524d\u7684\u7814\u7a76\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5f37\u8abf\u7531\u6587\u5b57\u5f15\u5c0e\u7684\u901a\u7528\u5f71\u7247\u7522\u751f\uff0c\u4f46\u7531\u97f3\u6a02\u9a45\u52d5\u7684\u821e\u8e48\u5f71\u7247\u7522\u751f\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 MuseDance\uff0c\u4e00\u500b\u5275\u65b0\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u97f3\u6a02\u548c\u6587\u5b57\u8f38\u5165\u5c0d\u53c3\u8003\u5f71\u50cf\u9032\u884c\u52d5\u756b\u8655\u7406\u3002\u9019\u7a2e\u96d9\u91cd\u8f38\u5165\u4f7f MuseDance \u80fd\u5920\u7522\u751f\u7b26\u5408\u6587\u5b57\u63cf\u8ff0\u4e26\u5c07\u89d2\u8272\u52d5\u4f5c\u8207\u97f3\u6a02\u540c\u6b65\u7684\u500b\u4eba\u5316\u5f71\u7247\u3002\u8207\u73fe\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cMuseDance \u6d88\u9664\u4e86\u5c0d\u8907\u96dc\u52d5\u4f5c\u6307\u5c0e\u8f38\u5165\uff08\u4f8b\u5982\u59ff\u52e2\u6216\u6df1\u5ea6\u5e8f\u5217\uff09\u7684\u9700\u6c42\uff0c\u8b93\u5404\u500b\u5c08\u696d\u6c34\u6e96\u7684\u4f7f\u7528\u8005\u90fd\u80fd\u8f15\u9b06\u9032\u884c\u9748\u6d3b\u4e14\u6709\u5275\u610f\u7684\u5f71\u7247\u7522\u751f\u3002\u70ba\u4e86\u63a8\u9032\u6b64\u9818\u57df\u7684\u7814\u7a76\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5305\u542b 2,904 \u500b\u821e\u8e48\u5f71\u7247\u7684\u65b0\u591a\u6a21\u614b\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5c0d\u61c9\u7684\u80cc\u666f\u97f3\u6a02\u548c\u6587\u5b57\u63cf\u8ff0\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u57fa\u65bc\u64f4\u6563\u7684\u65b9\u6cd5\u4f86\u5be6\u73fe\u7a69\u5065\u7684\u6cdb\u5316\u3001\u7cbe\u78ba\u63a7\u5236\u548c\u6642\u9593\u4e00\u81f4\u6027\uff0c\u70ba\u7531\u97f3\u6a02\u9a45\u52d5\u7684\u5f71\u50cf\u52d5\u756b\u4efb\u52d9\u8a2d\u5b9a\u4e86\u65b0\u7684\u57fa\u6e96\u3002", "author": "Zhikang Dong et.al.", "authors": "Zhikang Dong, Weituo Hao, Ju-Chiang Wang, Peng Zhang, Pawel Polak", "id": "2501.18801v1", "paper_url": "http://arxiv.org/abs/2501.18801v1", "repo": "null"}}