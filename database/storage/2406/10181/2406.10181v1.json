{"2406.10181": {"publish_time": "2024-06-14", "title": "Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors", "paper_summary": "Fine-tuning large language models (LLMs) requires significant memory, often\nexceeding the capacity of a single GPU. A common solution to this memory\nchallenge is offloading compute and data from the GPU to the CPU. However, this\napproach is hampered by the limited bandwidth of commodity hardware, which\nconstrains communication between the CPU and GPU.\n  In this paper, we present an offloading framework, LSP_Offload, that enables\nnear-native speed LLM fine-tuning on commodity hardware through learned\nsubspace projectors. Our data-driven approach involves learning an efficient\nsparse compressor that minimizes communication with minimal precision loss.\nAdditionally, we introduce a novel layer-wise communication schedule to\nmaximize parallelism between communication and computation. As a result, our\nframework can fine-tune a 1.3 billion parameter model on a 4GB laptop GPU and a\n7 billion parameter model on an NVIDIA RTX 4090 GPU with 24GB memory, achieving\nonly a 31% slowdown compared to fine-tuning with unlimited memory. Compared to\nstate-of-the-art offloading frameworks, our approach increases fine-tuning\nthroughput by up to 3.33 times and reduces end-to-end fine-tuning time by\n33.1%~62.5% when converging to the same accuracy.", "paper_summary_zh": "\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9700\u8981\u5927\u91cf\u7684\u8a18\u61b6\u9ad4\uff0c\u901a\u5e38\u6703\u8d85\u904e\u55ae\u4e00 GPU \u7684\u5bb9\u91cf\u3002\u89e3\u6c7a\u6b64\u8a18\u61b6\u9ad4\u6311\u6230\u7684\u5e38\u898b\u65b9\u6cd5\u662f\u5c07\u904b\u7b97\u548c\u8cc7\u6599\u5f9e GPU \u5378\u8f09\u5230 CPU\u3002\u7136\u800c\uff0c\u6b64\u65b9\u6cd5\u53d7\u5230\u5546\u54c1\u786c\u9ad4\u6709\u9650\u983b\u5bec\u7684\u963b\u7919\uff0c\u9019\u6703\u9650\u5236 CPU \u548c GPU \u4e4b\u9593\u7684\u901a\u8a0a\u3002\n  \u5728\u9019\u7bc7\u8ad6\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5378\u8f09\u6846\u67b6 LSP_Offload\uff0c\u5b83\u80fd\u900f\u904e\u5b78\u7fd2\u7684\u5b50\u7a7a\u9593\u6295\u5f71\u5668\uff0c\u5728\u5546\u54c1\u786c\u9ad4\u4e0a\u4ee5\u63a5\u8fd1\u539f\u751f\u901f\u5ea6\u5fae\u8abf LLM\u3002\u6211\u5011\u7684\u8cc7\u6599\u9a45\u52d5\u65b9\u6cd5\u6d89\u53ca\u5b78\u7fd2\u4e00\u500b\u6709\u6548\u7684\u7a00\u758f\u58d3\u7e2e\u5668\uff0c\u5b83\u80fd\u4ee5\u6700\u5c0f\u7684\u7cbe\u5ea6\u640d\u5931\u6700\u5c0f\u5316\u901a\u8a0a\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u65b0\u7a4e\u7684\u5206\u5c64\u901a\u8a0a\u6392\u7a0b\uff0c\u4ee5\u6700\u5927\u5316\u901a\u8a0a\u548c\u904b\u7b97\u4e4b\u9593\u7684\u5e73\u884c\u6027\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7684\u6846\u67b6\u53ef\u4ee5\u5728 4GB \u7b46\u96fb GPU \u4e0a\u5fae\u8abf 13 \u5104\u500b\u53c3\u6578\u7684\u6a21\u578b\uff0c\u4e26\u5728\u5177\u6709 24GB \u8a18\u61b6\u9ad4\u7684 NVIDIA RTX 4090 GPU \u4e0a\u5fae\u8abf 70 \u5104\u500b\u53c3\u6578\u7684\u6a21\u578b\uff0c\u8207\u5728\u4e0d\u53d7\u9650\u8a18\u61b6\u9ad4\u7684\u60c5\u6cc1\u4e0b\u9032\u884c\u5fae\u8abf\u76f8\u6bd4\uff0c\u53ea\u6703\u6e1b\u6162 31%\u3002\u8207\u6700\u5148\u9032\u7684\u5378\u8f09\u6846\u67b6\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5c07\u5fae\u8abf\u8655\u7406\u91cf\u63d0\u9ad8\u4e86 3.33 \u500d\uff0c\u4e26\u5728\u6536\u6582\u5230\u76f8\u540c\u6e96\u78ba\u5ea6\u6642\u5c07\u7aef\u5230\u7aef\u5fae\u8abf\u6642\u9593\u6e1b\u5c11\u4e86 33.1%~62.5%\u3002", "author": "Siyuan Chen et.al.", "authors": "Siyuan Chen, Zelong Guan, Yudong Liu, Phillip B. Gibbons", "id": "2406.10181v1", "paper_url": "http://arxiv.org/abs/2406.10181v1", "repo": "null"}}