{"2407.20756": {"publish_time": "2024-07-30", "title": "SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models", "paper_summary": "Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u96a8\u8457\u7db2\u8def\u5f71\u50cf\u7684\u8208\u8d77\uff0c\u7ba1\u7406\u548c\u7406\u89e3\u5927\u898f\u6a21\u5f71\u50cf\u8cc7\u6599\u96c6\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u91cd\u8981\u3002\u8996\u89ba\u5927\u578b\u8a9e\u8a00\u6a21\u578b (VLLM) \u8fd1\u671f\u56e0\u5176\u5f37\u5927\u7684\u8996\u89ba\u7406\u89e3\u80fd\u529b\u800c\u5099\u53d7\u77da\u76ee\u3002\u7136\u800c\uff0c\u8a13\u7df4\u9019\u4e9b\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8cc7\u6599\uff0c\u5c0d\u6548\u7387\u3001\u6548\u80fd\u3001\u8cc7\u6599\u54c1\u8cea\u548c\u96b1\u79c1\u90fd\u63d0\u51fa\u4e86\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 SynthVLM\uff0c\u4e00\u7a2e\u7528\u65bc VLLM \u7684\u65b0\u8cc7\u6599\u5408\u6210\u7ba1\u9053\u3002\u8207\u5f9e\u5f71\u50cf\u7522\u751f\u6a19\u984c\u7684\u73fe\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cSynthVLM \u63a1\u7528\u5148\u9032\u7684\u64f4\u6563\u6a21\u578b\u548c\u9ad8\u54c1\u8cea\u6a19\u984c\uff0c\u5f9e\u6a19\u984c\u81ea\u52d5\u7522\u751f\u4e26\u9078\u64c7\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\uff0c\u5efa\u7acb\u7cbe\u78ba\u5c0d\u9f4a\u7684\u5f71\u50cf\u6587\u5b57\u914d\u5c0d\u3002\u900f\u904e\u5229\u7528\u9019\u4e9b\u914d\u5c0d\uff0c\u6211\u5011\u5728\u5404\u7a2e\u8996\u89ba\u554f\u984c\u89e3\u7b54\u4efb\u52d9\u4e2d\u9054\u5230\u4e86\u6700\u5148\u9032 (SoTA) \u7684\u6548\u80fd\uff0c\u7dad\u6301\u4e86\u9ad8\u5c0d\u9f4a\u54c1\u8cea\u4e26\u4fdd\u7559\u4e86\u5148\u9032\u7684\u8a9e\u8a00\u80fd\u529b\u3002\u6b64\u5916\uff0cSynthVLM \u5728\u6548\u80fd\u4e0a\u8d85\u8d8a\u4e86\u50b3\u7d71\u7684 GPT-4 \u8996\u89ba\u5316\u6a19\u984c\u7522\u751f\u65b9\u6cd5\uff0c\u540c\u6642\u5927\u5e45\u964d\u4f4e\u4e86\u904b\u7b97\u958b\u92b7\u3002\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u7d14\u7cb9\u7522\u751f\u7684\u8cc7\u6599\uff0c\u78ba\u4fdd\u4e86\u96b1\u79c1\u7684\u4fdd\u8b77\uff0c\u50c5\u4f7f\u7528 10 \u842c\u500b\u8cc7\u6599\u9ede\uff08\u50c5\u70ba\u5b98\u65b9\u8cc7\u6599\u96c6\u5927\u5c0f\u7684 18%\uff09\u5c31\u9054\u5230\u4e86 SoTA \u6548\u80fd\u3002</paragraph>", "author": "Zheng Liu et.al.", "authors": "Zheng Liu, Hao Liang, Wentao Xiong, Qinhan Yu, Conghui He, Bin Cui, Wentao Zhang", "id": "2407.20756v1", "paper_url": "http://arxiv.org/abs/2407.20756v1", "repo": "https://github.com/starriver030515/synthvlm"}}