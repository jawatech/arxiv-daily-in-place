{"2501.13927": {"publish_time": "2025-01-23", "title": "CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation", "paper_summary": "Large language models (LLMs) have shown great potential in natural language\nprocessing tasks, but their application to machine translation (MT) remains\nchallenging due to pretraining on English-centric data and the complexity of\nreinforcement learning from human feedback (RLHF). Direct Preference\nOptimization (DPO) has emerged as a simpler and more efficient alternative, but\nits performance depends heavily on the quality of preference data. To address\nthis, we propose Confidence-Reward driven Preference Optimization (CRPO), a\nnovel method that combines reward scores with model confidence to improve data\nselection for fine-tuning. CRPO selects challenging sentence pairs where the\nmodel is uncertain or underperforms, leading to more effective learning. While\nprimarily designed for LLMs, CRPO also generalizes to encoder-decoder models\nlike NLLB, demonstrating its versatility. Empirical results show that CRPO\noutperforms existing methods such as RS-DPO, RSO and MBR score in both\ntranslation accuracy and data efficiency.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u6975\u5927\u7684\u6f5b\u529b\uff0c\u4f46\u7531\u65bc\u9810\u8a13\u7df4\u6642\u4ee5\u82f1\u8a9e\u70ba\u4e2d\u5fc3\u8cc7\u6599\uff0c\u4ee5\u53ca\u5f9e\u4eba\u985e\u56de\u994b\u4e2d\u9032\u884c\u5f37\u5316\u5b78\u7fd2\u7684\u8907\u96dc\u6027\uff0c\u5176\u5728\u6a5f\u5668\u7ffb\u8b6f (MT) \u4e2d\u7684\u61c9\u7528\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u5df2\u6210\u70ba\u4e00\u7a2e\u66f4\u7c21\u55ae\u4e14\u66f4\u6709\u6548\u7387\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u6548\u80fd\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u504f\u597d\u8cc7\u6599\u7684\u54c1\u8cea\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4ee5\u4fe1\u5fc3\u734e\u52f5\u70ba\u9a45\u52d5\u529b\u7684\u504f\u597d\u6700\u4f73\u5316 (CRPO)\uff0c\u9019\u662f\u4e00\u7a2e\u7d50\u5408\u734e\u52f5\u5206\u6578\u8207\u6a21\u578b\u4fe1\u5fc3\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u5fae\u8abf\u7684\u8cc7\u6599\u9078\u53d6\u3002CRPO \u9078\u64c7\u6a21\u578b\u4e0d\u78ba\u5b9a\u6216\u8868\u73fe\u4e0d\u4f73\u7684\u5177\u6311\u6230\u6027\u53e5\u5b50\u5c0d\uff0c\u9032\u800c\u5e36\u4f86\u66f4\u6709\u6548\u7684\u5b78\u7fd2\u3002\u5118\u7ba1 CRPO \u4e3b\u8981\u8a2d\u8a08\u7528\u65bc LLM\uff0c\u4f46\u5b83\u4e5f\u9069\u7528\u65bc\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b\uff0c\u4f8b\u5982 NLLB\uff0c\u8b49\u660e\u4e86\u5176\u591a\u529f\u80fd\u6027\u3002\u5be6\u8b49\u7d50\u679c\u986f\u793a\uff0cCRPO \u5728\u7ffb\u8b6f\u6e96\u78ba\u5ea6\u548c\u8cc7\u6599\u6548\u7387\u65b9\u9762\u5747\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u4f8b\u5982 RS-DPO\u3001RSO \u548c MBR \u5206\u6578\u3002", "author": "Guofeng Cui et.al.", "authors": "Guofeng Cui, Pichao Wang, Yang Liu, Zemian Ke, Zhu Liu, Vimal Bhat", "id": "2501.13927v1", "paper_url": "http://arxiv.org/abs/2501.13927v1", "repo": "null"}}