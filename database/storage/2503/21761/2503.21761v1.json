{"2503.21761": {"publish_time": "2025-03-27", "title": "Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video", "paper_summary": "This paper presents a unified approach to understanding dynamic scenes from\ncasual videos. Large pretrained vision foundation models, such as\nvision-language, video depth prediction, motion tracking, and segmentation\nmodels, offer promising capabilities. However, training a single model for\ncomprehensive 4D understanding remains challenging. We introduce Uni4D, a\nmulti-stage optimization framework that harnesses multiple pretrained models to\nadvance dynamic 3D modeling, including static/dynamic reconstruction, camera\npose estimation, and dense 3D motion tracking. Our results show\nstate-of-the-art performance in dynamic 4D modeling with superior visual\nquality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the\neffectiveness of repurposing visual foundation models for 4D understanding.", "paper_summary_zh": "<paragraph>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u7d71\u4e00\u7684\u65b9\u6cd5\u4f86\u7406\u89e3\u4f11\u9592\u5f71\u7247\u4e2d\u7684\u52d5\u614b\u5834\u666f\u3002\u5927\u578b\u9810\u8a13\u7df4\u8996\u89ba\u57fa\u790e\u6a21\u578b\uff0c\u4f8b\u5982\u8996\u89ba\u8a9e\u8a00\u3001\u5f71\u7247\u6df1\u5ea6\u9810\u6e2c\u3001\u904b\u52d5\u8ffd\u8e64\u548c\u5206\u5272\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u5f88\u6709\u524d\u666f\u7684\u529f\u80fd\u3002\u7136\u800c\uff0c\u8a13\u7df4\u4e00\u500b\u55ae\u4e00\u6a21\u578b\u4f86\u9032\u884c\u5168\u9762\u7684 4D \u7406\u89e3\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u6211\u5011\u63a8\u51fa\u4e86 Uni4D\uff0c\u9019\u662f\u4e00\u500b\u591a\u968e\u6bb5\u512a\u5316\u6846\u67b6\uff0c\u5b83\u5229\u7528\u591a\u500b\u9810\u8a13\u7df4\u6a21\u578b\u4f86\u63a8\u9032\u52d5\u614b 3D \u5efa\u6a21\uff0c\u5305\u62ec\u975c\u614b\uff0f\u52d5\u614b\u91cd\u5efa\u3001\u76f8\u6a5f\u59ff\u614b\u4f30\u8a08\u548c\u5bc6\u96c6 3D \u904b\u52d5\u8ffd\u8e64\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u5728\u52d5\u614b 4D \u5efa\u6a21\u65b9\u9762\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6027\u80fd\uff0c\u4e26\u5177\u6709\u5353\u8d8a\u7684\u8996\u89ba\u54c1\u8cea\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cUni4D \u4e0d\u9700\u8981\u91cd\u65b0\u8a13\u7df4\u6216\u5fae\u8abf\uff0c\u9019\u7a81\u51fa\u4e86\u91cd\u65b0\u5229\u7528\u8996\u89ba\u57fa\u790e\u6a21\u578b\u9032\u884c 4D \u7406\u89e3\u7684\u6709\u6548\u6027\u3002</paragraph>\n", "author": "David Yifan Yao et.al.", "authors": "David Yifan Yao, Albert J. Zhai, Shenlong Wang", "id": "2503.21761v1", "paper_url": "http://arxiv.org/abs/2503.21761v1", "repo": "null"}}