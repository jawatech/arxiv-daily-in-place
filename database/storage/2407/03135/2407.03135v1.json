{"2407.03135": {"publish_time": "2024-07-03", "title": "GMM-ResNext: Combining Generative and Discriminative Models for Speaker Verification", "paper_summary": "With the development of deep learning, many different network architectures\nhave been explored in speaker verification. However, most network architectures\nrely on a single deep learning architecture, and hybrid networks combining\ndifferent architectures have been little studied in ASV tasks. In this paper,\nwe propose the GMM-ResNext model for speaker verification. Conventional GMM\ndoes not consider the score distribution of each frame feature over all\nGaussian components and ignores the relationship between neighboring speech\nframes. So, we extract the log Gaussian probability features based on the raw\nacoustic features and use ResNext-based network as the backbone to extract the\nspeaker embedding. GMM-ResNext combines Generative and Discriminative Models to\nimprove the generalization ability of deep learning models and allows one to\nmore easily specify meaningful priors on model parameters. A two-path\nGMM-ResNext model based on two gender-related GMMs has also been proposed. The\nExperimental results show that the proposed GMM-ResNext achieves relative\nimprovements of 48.1\\% and 11.3\\% in EER compared with ResNet34 and ECAPA-TDNN\non VoxCeleb1-O test set.", "paper_summary_zh": "\u968f\u7740\u6df1\u5ea6\u5b78\u7fd2\u7684\u767c\u5c55\uff0c\u5728\u8aaa\u8a71\u8005\u9a57\u8b49\u4e2d\u5df2\u7d93\u63a2\u7d22\u4e86\u8a31\u591a\u4e0d\u540c\u7684\u7db2\u8def\u67b6\u69cb\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u7db2\u8def\u67b6\u69cb\u4f9d\u8cf4\u65bc\u55ae\u4e00\u7684\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\uff0c\u800c\u7d50\u5408\u4e0d\u540c\u67b6\u69cb\u7684\u6df7\u5408\u7db2\u8def\u5728 ASV \u4efb\u52d9\u4e2d\u7814\u7a76\u5f97\u5f88\u5c11\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa GMM-ResNext \u6a21\u578b\u9032\u884c\u8aaa\u8a71\u8005\u9a57\u8b49\u3002\u50b3\u7d71\u7684 GMM \u6c92\u6709\u8003\u616e\u6bcf\u500b\u5e40\u7279\u5fb5\u5728\u6240\u6709\u9ad8\u65af\u5206\u91cf\u4e0a\u7684\u5206\u6578\u5206\u4f48\uff0c\u4e26\u4e14\u5ffd\u7565\u4e86\u76f8\u9130\u8a9e\u97f3\u5e40\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\u56e0\u6b64\uff0c\u6211\u5011\u57fa\u65bc\u539f\u59cb\u8072\u5b78\u7279\u5fb5\u63d0\u53d6\u5c0d\u6578\u9ad8\u65af\u6a5f\u7387\u7279\u5fb5\uff0c\u4e26\u4f7f\u7528\u57fa\u65bc ResNext \u7684\u7db2\u8def\u4f5c\u70ba\u4e3b\u5e79\u4f86\u63d0\u53d6\u8aaa\u8a71\u8005\u5d4c\u5165\u3002GMM-ResNext \u7d50\u5408\u4e86\u751f\u6210\u6a21\u578b\u548c\u5224\u5225\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e26\u5141\u8a31\u4eba\u5011\u66f4\u8f15\u9b06\u5730\u6307\u5b9a\u6a21\u578b\u53c3\u6578\u4e0a\u7684\u6709\u610f\u7fa9\u5148\u9a57\u3002\u9084\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u5169\u500b\u6027\u5225\u76f8\u95dc GMM \u7684\u96d9\u8def\u5f91 GMM-ResNext \u6a21\u578b\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207 ResNet34 \u548c ECAPA-TDNN \u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684 GMM-ResNext \u5728 VoxCeleb1-O \u6e2c\u8a66\u96c6\u4e0a\u5728 EER \u4e2d\u5206\u5225\u53d6\u5f97\u4e86 48.1% \u548c 11.3% \u7684\u76f8\u5c0d\u6539\u9032\u3002", "author": "Hui Yan et.al.", "authors": "Hui Yan, Zhenchun Lei, Changhong Liu, Yong Zhou", "id": "2407.03135v1", "paper_url": "http://arxiv.org/abs/2407.03135v1", "repo": "null"}}