{"2406.14481": {"publish_time": "2024-06-20", "title": "Revealing Vision-Language Integration in the Brain with Multimodal Networks", "paper_summary": "We use (multi)modal deep neural networks (DNNs) to probe for sites of\nmultimodal integration in the human brain by predicting stereoencephalography\n(SEEG) recordings taken while human subjects watched movies. We operationalize\nsites of multimodal integration as regions where a multimodal vision-language\nmodel predicts recordings better than unimodal language, unimodal vision, or\nlinearly-integrated language-vision models. Our target DNN models span\ndifferent architectures (e.g., convolutional networks and transformers) and\nmultimodal training techniques (e.g., cross-attention and contrastive\nlearning). As a key enabling step, we first demonstrate that trained vision and\nlanguage models systematically outperform their randomly initialized\ncounterparts in their ability to predict SEEG signals. We then compare unimodal\nand multimodal models against one another. Because our target DNN models often\nhave different architectures, number of parameters, and training sets (possibly\nobscuring those differences attributable to integration), we carry out a\ncontrolled comparison of two models (SLIP and SimCLR), which keep all of these\nattributes the same aside from input modality. Using this approach, we identify\na sizable number of neural sites (on average 141 out of 1090 total sites or\n12.94%) and brain regions where multimodal integration seems to occur.\nAdditionally, we find that among the variants of multimodal training techniques\nwe assess, CLIP-style training is the best suited for downstream prediction of\nthe neural activity in these sites.", "paper_summary_zh": "<paragraph>\u6211\u5011\u4f7f\u7528\uff08\u591a\uff09\u6a21\u614b\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\uff08DNN\uff09\u900f\u904e\u9810\u6e2c\u4eba\u985e\u53d7\u8a66\u8005\u89c0\u770b\u96fb\u5f71\u6642\u6240\u9032\u884c\u7684\u7acb\u9ad4\u8166\u96fb\u5716\uff08SEEG\uff09\u7d00\u9304\uff0c\u4f86\u63a2\u6e2c\u4eba\u985e\u5927\u8166\u4e2d\u591a\u6a21\u614b\u6574\u5408\u7684\u90e8\u4f4d\u3002\u6211\u5011\u5c07\u591a\u6a21\u614b\u6574\u5408\u7684\u90e8\u4f4d\u64cd\u4f5c\u5316\u70ba\u591a\u6a21\u614b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u9810\u6e2c\u7d00\u9304\u512a\u65bc\u55ae\u6a21\u614b\u8a9e\u8a00\u3001\u55ae\u6a21\u614b\u8996\u89ba\u6216\u7dda\u6027\u6574\u5408\u8a9e\u8a00\u8996\u89ba\u6a21\u578b\u7684\u5340\u57df\u3002\u6211\u5011\u7684\u76ee\u6a19 DNN \u6a21\u578b\u6db5\u84cb\u4e0d\u540c\u7684\u67b6\u69cb\uff08\u4f8b\u5982\u5377\u7a4d\u7db2\u8def\u548cTransformer\uff09\u548c\u591a\u6a21\u614b\u8a13\u7df4\u6280\u8853\uff08\u4f8b\u5982\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u5c0d\u6bd4\u5b78\u7fd2\uff09\u3002\u4f5c\u70ba\u4e00\u500b\u95dc\u9375\u7684\u555f\u7528\u6b65\u9a5f\uff0c\u6211\u5011\u9996\u5148\u8b49\u660e\u53d7\u904e\u8a13\u7df4\u7684\u8996\u89ba\u548c\u8a9e\u8a00\u6a21\u578b\u5728\u9810\u6e2c SEEG \u8a0a\u865f\u7684\u80fd\u529b\u4e0a\u7cfb\u7d71\u6027\u5730\u512a\u65bc\u5b83\u5011\u96a8\u6a5f\u521d\u59cb\u5316\u7684\u5c0d\u61c9\u6a21\u578b\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07\u55ae\u6a21\u614b\u548c\u591a\u6a21\u614b\u6a21\u578b\u76f8\u4e92\u6bd4\u8f03\u3002\u7531\u65bc\u6211\u5011\u7684\u76ee\u6a19 DNN \u6a21\u578b\u901a\u5e38\u6709\u4e0d\u540c\u7684\u67b6\u69cb\u3001\u53c3\u6578\u6578\u91cf\u548c\u8a13\u7df4\u96c6\uff08\u53ef\u80fd\u6a21\u7cca\u4e86\u90a3\u4e9b\u53ef\u6b78\u56e0\u65bc\u6574\u5408\u7684\u5dee\u7570\uff09\uff0c\u6211\u5011\u5c0d\u5169\u500b\u6a21\u578b\uff08SLIP \u548c SimCLR\uff09\u9032\u884c\u53d7\u63a7\u6bd4\u8f03\uff0c\u9664\u4e86\u8f38\u5165\u6a21\u614b\u4e4b\u5916\uff0c\u5b83\u5011\u4fdd\u6301\u6240\u6709\u9019\u4e9b\u5c6c\u6027\u76f8\u540c\u3002\u4f7f\u7528\u9019\u7a2e\u65b9\u6cd5\uff0c\u6211\u5011\u8b58\u5225\u51fa\u5927\u91cf\u7684\u795e\u7d93\u90e8\u4f4d\uff08\u5e73\u5747 1090 \u500b\u7e3d\u90e8\u4f4d\u4e2d\u7684 141 \u500b\u6216 12.94%\uff09\u548c\u770b\u4f3c\u767c\u751f\u591a\u6a21\u614b\u6574\u5408\u7684\u5927\u8166\u5340\u57df\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\uff0c\u5728\u6211\u5011\u8a55\u4f30\u7684\u591a\u6a21\u614b\u8a13\u7df4\u6280\u8853\u8b8a\u9ad4\u4e2d\uff0cCLIP \u98a8\u683c\u7684\u8a13\u7df4\u6700\u9069\u5408\u9810\u6e2c\u9019\u4e9b\u90e8\u4f4d\u7684\u795e\u7d93\u6d3b\u52d5\u3002</paragraph>", "author": "Vighnesh Subramaniam et.al.", "authors": "Vighnesh Subramaniam, Colin Conwell, Christopher Wang, Gabriel Kreiman, Boris Katz, Ignacio Cases, Andrei Barbu", "id": "2406.14481v1", "paper_url": "http://arxiv.org/abs/2406.14481v1", "repo": "null"}}