{"2410.23726": {"publish_time": "2024-10-31", "title": "Towards Reliable Alignment: Uncertainty-aware RLHF", "paper_summary": "Recent advances in aligning Large Language Models with human preferences have\nbenefited from larger reward models and better preference data. However, most\nof these methodologies rely on the accuracy of the reward model. The reward\nmodels used in Reinforcement Learning with Human Feedback (RLHF) are typically\nlearned from small datasets using stochastic optimization algorithms, making\nthem prone to high variability. We illustrate the inconsistencies between\nreward models empirically on numerous open-source datasets.\n  We theoretically show that the fluctuation of the reward models can be\ndetrimental to the alignment problem because the derived policies are more\noverfitted to the reward model and, hence, are riskier if the reward model\nitself is uncertain. We use concentration of measure to motivate an\nuncertainty-aware, conservative algorithm for policy optimization. We show that\nsuch policies are more risk-averse in the sense that they are more cautious of\nuncertain rewards. We theoretically prove that our proposed methodology has\nless risk than the vanilla method.\n  We corroborate our theoretical results with experiments based on designing an\nensemble of reward models. We use this ensemble of reward models to align a\nlanguage model using our methodology and observe that our empirical findings\nmatch our theoretical predictions.", "paper_summary_zh": "\u8fd1\u4f86\uff0c\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u8207\u4eba\u985e\u504f\u597d\u76f8\u7d50\u5408\u7684\u9032\u5c55\u53d7\u76ca\u65bc\u66f4\u5927\u7684\u734e\u52f5\u6a21\u578b\u548c\u66f4\u597d\u7684\u504f\u597d\u6578\u64da\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u5927\u591a\u4f9d\u8cf4\u65bc\u734e\u52f5\u6a21\u578b\u7684\u6e96\u78ba\u6027\u3002\u5728\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u4e2d\u4f7f\u7528\u7684\u734e\u52f5\u6a21\u578b\u901a\u5e38\u662f\u4f7f\u7528\u96a8\u6a5f\u512a\u5316\u6f14\u7b97\u6cd5\u5f9e\u5c0f\u578b\u8cc7\u6599\u96c6\u5b78\u7fd2\u800c\u5f97\uff0c\u9019\u4f7f\u5f97\u5b83\u5011\u5bb9\u6613\u7522\u751f\u9ad8\u5ea6\u8b8a\u7570\u6027\u3002\u6211\u5011\u5728\u8a31\u591a\u958b\u6e90\u8cc7\u6599\u96c6\u4e0a\u6839\u64da\u7d93\u9a57\u8aaa\u660e\u4e86\u734e\u52f5\u6a21\u578b\u4e4b\u9593\u7684\u4e0d\u4e00\u81f4\u6027\u3002\n\u6211\u5011\u5728\u7406\u8ad6\u4e0a\u8868\u660e\uff0c\u734e\u52f5\u6a21\u578b\u7684\u6ce2\u52d5\u53ef\u80fd\u5c0d\u6821\u6e96\u554f\u984c\u6709\u5bb3\uff0c\u56e0\u70ba\u884d\u751f\u7684\u653f\u7b56\u66f4\u904e\u5ea6\u64ec\u5408\u734e\u52f5\u6a21\u578b\uff0c\u56e0\u6b64\uff0c\u5982\u679c\u734e\u52f5\u6a21\u578b\u672c\u8eab\u4e0d\u78ba\u5b9a\uff0c\u5247\u98a8\u96aa\u66f4\u9ad8\u3002\u6211\u5011\u4f7f\u7528\u6e2c\u5ea6\u96c6\u4e2d\u4f86\u6fc0\u52f5\u4e00\u7a2e\u5177\u6709\u4e0d\u78ba\u5b9a\u6027\u611f\u77e5\u7684\u4fdd\u5b88\u653f\u7b56\u512a\u5316\u6f14\u7b97\u6cd5\u3002\u6211\u5011\u8868\u660e\uff0c\u6b64\u985e\u653f\u7b56\u5728\u98a8\u96aa\u898f\u907f\u65b9\u9762\u66f4\u70ba\u6709\u6548\uff0c\u56e0\u70ba\u5b83\u5011\u5c0d\u4e0d\u78ba\u5b9a\u7684\u734e\u52f5\u66f4\u70ba\u8b39\u614e\u3002\u6211\u5011\u5728\u7406\u8ad6\u4e0a\u8b49\u660e\uff0c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u9999\u8349\u65b9\u6cd5\u98a8\u96aa\u66f4\u4f4e\u3002\n\u6211\u5011\u4f7f\u7528\u57fa\u65bc\u8a2d\u8a08\u734e\u52f5\u6a21\u578b\u7684\u96c6\u5408\u7684\u5be6\u9a57\u4f86\u8b49\u5be6\u6211\u5011\u7684\u7406\u8ad6\u7d50\u679c\u3002\u6211\u5011\u4f7f\u7528\u9019\u500b\u734e\u52f5\u6a21\u578b\u7684\u96c6\u5408\u4f86\u4f7f\u7528\u6211\u5011\u7684\u65b9\u6cd5\u6821\u6e96\u8a9e\u8a00\u6a21\u578b\uff0c\u4e26\u89c0\u5bdf\u5230\u6211\u5011\u7684\u7d93\u9a57\u767c\u73fe\u8207\u6211\u5011\u7684\u7406\u8ad6\u9810\u6e2c\u76f8\u7b26\u3002", "author": "Debangshu Banerjee et.al.", "authors": "Debangshu Banerjee, Aditya Gopalan", "id": "2410.23726v1", "paper_url": "http://arxiv.org/abs/2410.23726v1", "repo": "null"}}