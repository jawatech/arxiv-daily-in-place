{"2409.16652": {"publish_time": "2024-09-25", "title": "Progressive Representation Learning for Real-Time UAV Tracking", "paper_summary": "Visual object tracking has significantly promoted autonomous applications for\nunmanned aerial vehicles (UAVs). However, learning robust object\nrepresentations for UAV tracking is especially challenging in complex dynamic\nenvironments, when confronted with aspect ratio change and occlusion. These\nchallenges severely alter the original information of the object. To handle the\nabove issues, this work proposes a novel progressive representation learning\nframework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided\ninto coarse representation learning and fine representation learning. For\ncoarse representation learning, two innovative regulators, which rely on\nappearance and semantic information, are designed to mitigate appearance\ninterference and capture semantic information. Furthermore, for fine\nrepresentation learning, a new hierarchical modeling generator is developed to\nintertwine coarse object representations. Exhaustive experiments demonstrate\nthat the proposed PRL-Track delivers exceptional performance on three\nauthoritative UAV tracking benchmarks. Real-world tests indicate that the\nproposed PRL-Track realizes superior tracking performance with 42.6 frames per\nsecond on the typical UAV platform equipped with an edge smart camera. The\ncode, model, and demo videos are available at\n\\url{https://github.com/vision4robotics/PRL-Track}.", "paper_summary_zh": "\u8996\u89ba\u7269\u4ef6\u8ffd\u8e64\u5df2\u5927\u5e45\u63d0\u5347\u7121\u4eba\u6a5f (UAV) \u7684\u81ea\u4e3b\u61c9\u7528\u3002\u7136\u800c\uff0c\u5728\u8907\u96dc\u7684\u52d5\u614b\u74b0\u5883\u4e2d\uff0c\u5b78\u7fd2\u7a69\u5065\u7684\u7269\u4ef6\u8868\u793a\u4ee5\u9032\u884c\u7121\u4eba\u6a5f\u8ffd\u8e64\u7279\u5225\u5177\u6709\u6311\u6230\u6027\uff0c\u7279\u5225\u662f\u5728\u9762\u5c0d\u9577\u5bec\u6bd4\u8b8a\u52d5\u548c\u906e\u64cb\u6642\u3002\u9019\u4e9b\u6311\u6230\u6703\u56b4\u91cd\u6539\u8b8a\u7269\u4ef6\u7684\u539f\u59cb\u8cc7\u8a0a\u3002\u70ba\u4e86\u8655\u7406\u4e0a\u8ff0\u554f\u984c\uff0c\u9019\u9805\u7814\u7a76\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u6f38\u9032\u5f0f\u8868\u793a\u5b78\u7fd2\u67b6\u69cb\u4ee5\u9032\u884c\u7121\u4eba\u6a5f\u8ffd\u8e64\uff0c\u4ea6\u5373 PRL-Track\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cPRL-Track \u5206\u70ba\u7c97\u7565\u8868\u793a\u5b78\u7fd2\u548c\u7cbe\u7d30\u8868\u793a\u5b78\u7fd2\u3002\u5c0d\u65bc\u7c97\u7565\u8868\u793a\u5b78\u7fd2\uff0c\u8a2d\u8a08\u4e86\u5169\u500b\u5275\u65b0\u7684\u8abf\u7bc0\u5668\uff0c\u5b83\u5011\u4f9d\u8cf4\u65bc\u5916\u89c0\u548c\u8a9e\u7fa9\u8cc7\u8a0a\uff0c\u4ee5\u6e1b\u8f15\u5916\u89c0\u5e72\u64fe\u4e26\u64f7\u53d6\u8a9e\u7fa9\u8cc7\u8a0a\u3002\u6b64\u5916\uff0c\u5c0d\u65bc\u7cbe\u7d30\u8868\u793a\u5b78\u7fd2\uff0c\u958b\u767c\u4e86\u4e00\u500b\u65b0\u7684\u968e\u5c64\u5f0f\u5efa\u6a21\u7522\u751f\u5668\uff0c\u4ee5\u4ea4\u7e54\u7c97\u7565\u7269\u4ef6\u8868\u793a\u3002\u8a73\u76e1\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6240\u63d0\u51fa\u7684 PRL-Track \u5728\u4e09\u500b\u6b0a\u5a01\u7684\u7121\u4eba\u6a5f\u8ffd\u8e64\u57fa\u6e96\u4e0a\u63d0\u4f9b\u4e86\u975e\u51e1\u7684\u6548\u80fd\u3002\u771f\u5be6\u4e16\u754c\u7684\u6e2c\u8a66\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 PRL-Track \u5728\u914d\u5099\u908a\u7de3\u667a\u6167\u76f8\u6a5f\u7684\u5178\u578b\u7121\u4eba\u6a5f\u5e73\u53f0\u4e0a\u4ee5\u6bcf\u79d2 42.6 \u5e40\u7684\u901f\u5ea6\u5be6\u73fe\u4e86\u512a\u7570\u7684\u8ffd\u8e64\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u3001\u6a21\u578b\u548c\u793a\u7bc4\u5f71\u7247\u53ef\u5728 \\url{https://github.com/vision4robotics/PRL-Track} \u53d6\u5f97\u3002", "author": "Changhong Fu et.al.", "authors": "Changhong Fu, Xiang Lei, Haobo Zuo, Liangliang Yao, Guangze Zheng, Jia Pan", "id": "2409.16652v1", "paper_url": "http://arxiv.org/abs/2409.16652v1", "repo": "https://github.com/vision4robotics/prl-track"}}