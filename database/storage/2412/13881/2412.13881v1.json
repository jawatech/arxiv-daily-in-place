{"2412.13881": {"publish_time": "2024-12-18", "title": "Understanding and Analyzing Model Robustness and Knowledge-Transfer in Multilingual Neural Machine Translation using TX-Ray", "paper_summary": "Neural networks have demonstrated significant advancements in Neural Machine\nTranslation (NMT) compared to conventional phrase-based approaches. However,\nMultilingual Neural Machine Translation (MNMT) in extremely low-resource\nsettings remains underexplored. This research investigates how knowledge\ntransfer across languages can enhance MNMT in such scenarios. Using the Tatoeba\ntranslation challenge dataset from Helsinki NLP, we perform English-German,\nEnglish-French, and English-Spanish translations, leveraging minimal parallel\ndata to establish cross-lingual mappings. Unlike conventional methods relying\non extensive pre-training for specific language pairs, we pre-train our model\non English-English translations, setting English as the source language for all\ntasks. The model is fine-tuned on target language pairs using joint multi-task\nand sequential transfer learning strategies. Our work addresses three key\nquestions: (1) How can knowledge transfer across languages improve MNMT in\nextremely low-resource scenarios? (2) How does pruning neuron knowledge affect\nmodel generalization, robustness, and catastrophic forgetting? (3) How can\nTX-Ray interpret and quantify knowledge transfer in trained models? Evaluation\nusing BLEU-4 scores demonstrates that sequential transfer learning outperforms\nbaselines on a 40k parallel sentence corpus, showcasing its efficacy. However,\npruning neuron knowledge degrades performance, increases catastrophic\nforgetting, and fails to improve robustness or generalization. Our findings\nprovide valuable insights into the potential and limitations of knowledge\ntransfer and pruning in MNMT for extremely low-resource settings.", "paper_summary_zh": "\u795e\u7d93\u7db2\u8def\u5728\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f (NMT) \u65b9\u9762\u5df2\u5c55\u73fe\u51fa\u6bd4\u50b3\u7d71\u57fa\u65bc\u8a5e\u5f59\u7684\u6280\u8853\u66f4\u986f\u8457\u7684\u9032\u5c55\u3002\u7136\u800c\uff0c\u5728\u6975\u5ea6\u4f4e\u8cc7\u6e90\u7684\u74b0\u5883\u4e2d\uff0c\u591a\u8a9e\u8a00\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f (MNMT) \u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8a0e\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u5728\u9019\u7a2e\u60c5\u6cc1\u4e0b\uff0c\u8de8\u8a9e\u8a00\u7684\u77e5\u8b58\u8f49\u79fb\u5982\u4f55\u589e\u5f37 MNMT\u3002\u6211\u5011\u4f7f\u7528\u8d6b\u723e\u8f9b\u57fa NLP \u7684 Tatoeba \u7ffb\u8b6f\u6311\u6230\u8cc7\u6599\u96c6\uff0c\u57f7\u884c\u82f1\u5fb7\u3001\u82f1\u6cd5\u548c\u82f1\u897f\u7ffb\u8b6f\uff0c\u5229\u7528\u6700\u5c11\u7684\u5e73\u884c\u8cc7\u6599\u4f86\u5efa\u7acb\u8de8\u8a9e\u8a00\u5c0d\u61c9\u3002\u6709\u5225\u65bc\u4f9d\u8cf4\u7279\u5b9a\u8a9e\u8a00\u5c0d\u9032\u884c\u5ee3\u6cdb\u9810\u8a13\u7df4\u7684\u50b3\u7d71\u65b9\u6cd5\uff0c\u6211\u5011\u5728\u82f1\u82f1\u7ffb\u8b6f\u4e0a\u9810\u8a13\u7df4\u6211\u5011\u7684\u6a21\u578b\uff0c\u5c07\u82f1\u8a9e\u8a2d\u5b9a\u70ba\u6240\u6709\u4efb\u52d9\u7684\u539f\u59cb\u8a9e\u8a00\u3002\u8a72\u6a21\u578b\u4f7f\u7528\u806f\u5408\u591a\u4efb\u52d9\u548c\u9806\u5e8f\u8f49\u79fb\u5b78\u7fd2\u7b56\u7565\uff0c\u91dd\u5c0d\u76ee\u6a19\u8a9e\u8a00\u5c0d\u9032\u884c\u5fae\u8abf\u3002\u6211\u5011\u7684\u7814\u7a76\u63a2\u8a0e\u4e86\u4e09\u500b\u95dc\u9375\u554f\u984c\uff1a(1) \u5728\u6975\u5ea6\u4f4e\u8cc7\u6e90\u7684\u60c5\u6cc1\u4e0b\uff0c\u8de8\u8a9e\u8a00\u7684\u77e5\u8b58\u8f49\u79fb\u5982\u4f55\u6539\u5584 MNMT\uff1f(2) \u4fee\u526a\u795e\u7d93\u5143\u77e5\u8b58\u5982\u4f55\u5f71\u97ff\u6a21\u578b\u7684\u6cdb\u5316\u6027\u3001\u7a69\u5065\u6027\u548c\u707d\u96e3\u6027\u907a\u5fd8\uff1f(3) TX-Ray \u5982\u4f55\u89e3\u8b80\u548c\u91cf\u5316\u8a13\u7df4\u6a21\u578b\u4e2d\u7684\u77e5\u8b58\u8f49\u79fb\uff1f\u4f7f\u7528 BLEU-4 \u5206\u6578\u9032\u884c\u7684\u8a55\u4f30\u986f\u793a\uff0c\u9806\u5e8f\u8f49\u79fb\u5b78\u7fd2\u5728 40k \u5e73\u884c\u53e5\u5b50\u8a9e\u6599\u5eab\u4e0a\u512a\u65bc\u57fa\u7dda\uff0c\u5c55\u793a\u4e86\u5176\u6548\u80fd\u3002\u7136\u800c\uff0c\u4fee\u526a\u795e\u7d93\u5143\u77e5\u8b58\u6703\u964d\u4f4e\u6548\u80fd\u3001\u589e\u52a0\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u4e26\u4e14\u7121\u6cd5\u6539\u5584\u7a69\u5065\u6027\u6216\u6cdb\u5316\u6027\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\uff0c\u8aaa\u660e\u4e86\u5728\u6975\u5ea6\u4f4e\u8cc7\u6e90\u7684\u74b0\u5883\u4e2d\uff0c\u77e5\u8b58\u8f49\u79fb\u548c\u4fee\u526a\u5728 MNMT \u4e2d\u7684\u6f5b\u529b\u548c\u9650\u5236\u3002", "author": "Vageesh Saxena et.al.", "authors": "Vageesh Saxena, Sharid Lo\u00e1iciga, Nils Rethmeier", "id": "2412.13881v1", "paper_url": "http://arxiv.org/abs/2412.13881v1", "repo": "null"}}