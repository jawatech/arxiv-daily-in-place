{"2408.17253": {"publish_time": "2024-08-30", "title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters", "paper_summary": "Foundation models have emerged as a promising approach in time series\nforecasting (TSF). Existing approaches either fine-tune large language models\n(LLMs) or build large-scale time-series datasets to develop TSF foundation\nmodels. However, these methods face challenges due to the severe cross-domain\ngap or in-domain heterogeneity. In this paper, we explore a new road to\nbuilding a TSF foundation model from rich and high-quality natural images,\nbased on the intrinsic similarities between images and time series. To bridge\nthe gap between the two domains, we reformulate the TSF task as an image\nreconstruction task, which is further processed by a visual masked autoencoder\n(MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly,\nwithout further adaptation in the time-series domain, the proposed VisionTS\ncould achieve superior zero-shot forecasting performance compared to existing\nTSF foundation models. With minimal fine-tuning, VisionTS could further improve\nthe forecasting and achieve state-of-the-art performance in most cases. These\nfindings suggest that visual models could be a free lunch for TSF and highlight\nthe potential for future cross-domain research between computer vision and TSF.\nOur code is publicly available at https://github.com/Keytoyze/VisionTS.", "paper_summary_zh": "\u57fa\u790e\u6a21\u578b\u5df2\u6210\u70ba\u6642\u9593\u5e8f\u5217\u9810\u6e2c (TSF) \u4e2d\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002\u73fe\u6709\u65b9\u6cd5\u6703\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u6216\u5efa\u7acb\u5927\u578b\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u96c6\u4f86\u958b\u767c TSF \u57fa\u790e\u6a21\u578b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u6703\u56e0\u56b4\u91cd\u7684\u8de8\u9818\u57df\u5dee\u8ddd\u6216\u9818\u57df\u5167\u7570\u8cea\u6027\u800c\u9762\u81e8\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u7d22\u4e86\u4e00\u689d\u5f9e\u8c50\u5bcc\u4e14\u9ad8\u54c1\u8cea\u81ea\u7136\u5f71\u50cf\u5efa\u7acb TSF \u57fa\u790e\u6a21\u578b\u7684\u65b0\u9014\u5f91\uff0c\u57fa\u790e\u662f\u5f71\u50cf\u548c\u6642\u9593\u5e8f\u5217\u4e4b\u9593\u7684\u5167\u5728\u76f8\u4f3c\u6027\u3002\u70ba\u4e86\u5f4c\u5408\u5169\u500b\u9818\u57df\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u6211\u5011\u5c07 TSF \u4efb\u52d9\u91cd\u65b0\u8868\u8ff0\u70ba\u5f71\u50cf\u91cd\u5efa\u4efb\u52d9\uff0c\u9019\u500b\u4efb\u52d9\u9032\u4e00\u6b65\u7531\u5728 ImageNet \u8cc7\u6599\u96c6\u4e0a\u9810\u5148\u8a13\u7df4\u7684\u8996\u89ba\u906e\u7f69\u81ea\u52d5\u7de8\u78bc\u5668 (MAE) \u81ea\u6211\u76e3\u7763\u8655\u7406\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u5728\u6642\u9593\u5e8f\u5217\u9818\u57df\u6c92\u6709\u9032\u4e00\u6b65\u9069\u61c9\u7684\u60c5\u6cc1\u4e0b\uff0c\u6240\u63d0\u51fa\u7684 VisionTS \u80fd\u5920\u9054\u6210\u512a\u65bc\u73fe\u6709 TSF \u57fa\u790e\u6a21\u578b\u7684\u96f6\u6b21\u5b78\u7fd2\u9810\u6e2c\u6548\u80fd\u3002\u900f\u904e\u6700\u5c0f\u7684\u5fae\u8abf\uff0cVisionTS \u80fd\u5920\u9032\u4e00\u6b65\u6539\u5584\u9810\u6e2c\uff0c\u4e26\u5728\u5927\u90e8\u5206\u60c5\u6cc1\u4e0b\u9054\u6210\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u9019\u4e9b\u767c\u73fe\u8868\u660e\u8996\u89ba\u6a21\u578b\u53ef\u80fd\u662f TSF \u7684\u514d\u8cbb\u5348\u9910\uff0c\u4e26\u5f37\u8abf\u4e86\u96fb\u8166\u8996\u89ba\u548c TSF \u4e4b\u9593\u672a\u4f86\u8de8\u9818\u57df\u7814\u7a76\u7684\u6f5b\u529b\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u516c\u958b\u65bc https://github.com/Keytoyze/VisionTS\u3002", "author": "Mouxiang Chen et.al.", "authors": "Mouxiang Chen, Lefei Shen, Zhuo Li, Xiaoyun Joy Wang, Jianling Sun, Chenghao Liu", "id": "2408.17253v1", "paper_url": "http://arxiv.org/abs/2408.17253v1", "repo": "https://github.com/keytoyze/visionts"}}