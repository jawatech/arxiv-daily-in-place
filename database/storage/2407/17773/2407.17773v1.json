{"2407.17773": {"publish_time": "2024-07-25", "title": "KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models", "paper_summary": "This paper investigates visual analogical reasoning in large multimodal\nmodels (LMMs) compared to human adults and children. A \"visual analogy\" is an\nabstract rule inferred from one image and applied to another. While benchmarks\nexist for testing visual reasoning in LMMs, they require advanced skills and\nomit basic visual analogies that even young children can make. Inspired by\ndevelopmental psychology, we propose a new benchmark of 1,400 visual\ntransformations of everyday objects to test LMMs on visual analogical reasoning\nand compare them to children and adults. We structure the evaluation into three\nstages: identifying what changed (e.g., color, number, etc.), how it changed\n(e.g., added one object), and applying the rule to new scenarios. Our findings\nshow that while models like GPT-4V, LLaVA-1.5, and MANTIS identify the \"what\"\neffectively, they struggle with quantifying the \"how\" and extrapolating this\nrule to new objects. In contrast, children and adults exhibit much stronger\nanalogical reasoning at all three stages. Additionally, the strongest tested\nmodel, GPT-4V, performs better in tasks involving simple visual attributes like\ncolor and size, correlating with quicker human adult response times.\nConversely, more complex tasks such as number, rotation, and reflection, which\nnecessitate extensive cognitive processing and understanding of the 3D physical\nworld, present more significant challenges. Altogether, these findings\nhighlight the limitations of training models on data that primarily consists of\n2D images and text.", "paper_summary_zh": "<paragraph>\u672c\u7bc7\u8ad6\u6587\u63a2\u8a0e\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u8207\u4eba\u985e\u6210\u4eba\u548c\u5152\u7ae5\u5728\u8996\u89ba\u985e\u6bd4\u63a8\u7406\u4e0a\u7684\u5dee\u7570\u3002\u300c\u8996\u89ba\u985e\u6bd4\u300d\u662f\u4e00\u7a2e\u5f9e\u4e00\u5e45\u5716\u50cf\u63a8\u8ad6\u51fa\u7684\u62bd\u8c61\u898f\u5247\uff0c\u4e26\u61c9\u7528\u65bc\u53e6\u4e00\u5e45\u5716\u50cf\u3002\u96d6\u7136\u73fe\u6709\u6e2c\u8a66 LMM \u8996\u89ba\u63a8\u7406\u80fd\u529b\u7684\u57fa\u6e96\uff0c\u4f46\u9019\u4e9b\u57fa\u6e96\u9700\u8981\u9032\u968e\u6280\u80fd\uff0c\u4e14\u7701\u7565\u4e86\u5373\u4f7f\u5e7c\u7ae5\u4e5f\u80fd\u505a\u51fa\u7684\u57fa\u672c\u8996\u89ba\u985e\u6bd4\u3002\u53d7\u767c\u5c55\u5fc3\u7406\u5b78\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u5305\u542b 1,400 \u500b\u65e5\u5e38\u7269\u9ad4\u7684\u8996\u89ba\u8f49\u63db\uff0c\u4ee5\u6e2c\u8a66 LMM \u7684\u8996\u89ba\u985e\u6bd4\u63a8\u7406\u80fd\u529b\uff0c\u4e26\u5c07\u5176\u8207\u5152\u7ae5\u548c\u6210\u4eba\u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u5c07\u8a55\u4f30\u5206\u70ba\u4e09\u500b\u968e\u6bb5\uff1a\u627e\u51fa\u6539\u8b8a\u7684\u5167\u5bb9\uff08\u4f8b\u5982\u984f\u8272\u3001\u6578\u91cf\u7b49\uff09\u3001\u6539\u8b8a\u7684\u65b9\u5f0f\uff08\u4f8b\u5982\u589e\u52a0\u4e00\u500b\u7269\u4ef6\uff09\uff0c\u4ee5\u53ca\u5c07\u898f\u5247\u61c9\u7528\u65bc\u65b0\u7684\u5834\u666f\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u5118\u7ba1 GPT-4V\u3001LLaVA-1.5 \u548c MANTIS \u7b49\u6a21\u578b\u80fd\u6709\u6548\u8b58\u5225\u300c\u662f\u4ec0\u9ebc\u300d\uff0c\u4f46\u5b83\u5011\u5728\u91cf\u5316\u300c\u5982\u4f55\u300d\u4ee5\u53ca\u5c07\u6b64\u898f\u5247\u63a8\u5ee3\u5230\u65b0\u7269\u4ef6\u65b9\u9762\u6709\u56f0\u96e3\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5152\u7ae5\u548c\u6210\u4eba\u5247\u5728\u6240\u6709\u4e09\u500b\u968e\u6bb5\u90fd\u5c55\u73fe\u51fa\u66f4\u5f37\u7684\u985e\u6bd4\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6e2c\u8a66\u4e2d\u8868\u73fe\u6700\u597d\u7684\u6a21\u578b GPT-4V \u5728\u6d89\u53ca\u7c21\u55ae\u8996\u89ba\u5c6c\u6027\uff08\u4f8b\u5982\u984f\u8272\u548c\u5927\u5c0f\uff09\u7684\u4efb\u52d9\u4e2d\u8868\u73fe\u8f03\u4f73\uff0c\u9019\u8207\u4eba\u985e\u6210\u4eba\u7684\u53cd\u61c9\u6642\u9593\u8f03\u5feb\u6709\u95dc\u3002\u76f8\u53cd\u5730\uff0c\u66f4\u8907\u96dc\u7684\u4efb\u52d9\uff0c\u4f8b\u5982\u6578\u5b57\u3001\u65cb\u8f49\u548c\u53cd\u5c04\uff0c\u9700\u8981\u5ee3\u6cdb\u7684\u8a8d\u77e5\u8655\u7406\u548c\u5c0d 3D \u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\uff0c\u5c0d\u6a21\u578b\u4f86\u8aaa\u66f4\u5177\u6311\u6230\u6027\u3002\u7e3d\u800c\u8a00\u4e4b\uff0c\u9019\u4e9b\u7814\u7a76\u7d50\u679c\u7a81\u986f\u4e86\u5728\u4e3b\u8981\u7531 2D \u5f71\u50cf\u548c\u6587\u5b57\u7d44\u6210\u7684\u8cc7\u6599\u4e0a\u8a13\u7df4\u6a21\u578b\u7684\u9650\u5236\u3002</paragraph>", "author": "Eunice Yiu et.al.", "authors": "Eunice Yiu, Maan Qraitem, Charlie Wong, Anisa Noor Majhi, Yutong Bai, Shiry Ginosar, Alison Gopnik, Kate Saenko", "id": "2407.17773v1", "paper_url": "http://arxiv.org/abs/2407.17773v1", "repo": "https://github.com/ey242/kiva"}}