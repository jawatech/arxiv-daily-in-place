{"2405.17104": {"publish_time": "2024-05-27", "title": "LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding", "paper_summary": "Visual grounding is an essential tool that links user-provided text queries\nwith query-specific regions within an image. Despite advancements in visual\ngrounding models, their ability to comprehend complex queries remains limited.\nTo overcome this limitation, we introduce LLM-Optic, an innovative method that\nutilizes Large Language Models (LLMs) as an optical lens to enhance existing\nvisual grounding models in comprehending complex text queries involving\nintricate text structures, multiple objects, or object spatial relationships,\nsituations that current models struggle with. LLM-Optic first employs an LLM as\na Text Grounder to interpret complex text queries and accurately identify\nobjects the user intends to locate. Then a pre-trained visual grounding model\nis used to generate candidate bounding boxes given the refined query by the\nText Grounder. After that, LLM-Optic annotates the candidate bounding boxes\nwith numerical marks to establish a connection between text and specific image\nregions, thereby linking two distinct modalities. Finally, it employs a Large\nMultimodal Model (LMM) as a Visual Grounder to select the marked candidate\nobjects that best correspond to the original text query. Through LLM-Optic, we\nhave achieved universal visual grounding, which allows for the detection of\narbitrary objects specified by arbitrary human language input. Importantly, our\nmethod achieves this enhancement without requiring additional training or\nfine-tuning. Extensive experiments across various challenging benchmarks\ndemonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding\ncapabilities. Project Page: https://haoyu-zhao.github.io/LLM-Optic.github.io/.", "paper_summary_zh": "\u8996\u89ba\u57fa\u790e\u662f\u4e00\u500b\u91cd\u8981\u7684\u5de5\u5177\uff0c\u5b83\u5c07\u4f7f\u7528\u8005\u63d0\u4f9b\u7684\u6587\u5b57\u67e5\u8a62\u8207\u5f71\u50cf\u4e2d\u7279\u5b9a\u65bc\u67e5\u8a62\u7684\u5340\u57df\u9023\u7d50\u8d77\u4f86\u3002\u5118\u7ba1\u8996\u89ba\u57fa\u790e\u6a21\u578b\u6709\u9032\u5c55\uff0c\u5b83\u5011\u7406\u89e3\u8907\u96dc\u67e5\u8a62\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 LLM-Optic\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f5c\u70ba\u4e00\u500b\u5149\u5b78\u900f\u93e1\uff0c\u4ee5\u589e\u5f37\u73fe\u6709\u7684\u8996\u89ba\u57fa\u790e\u6a21\u578b\uff0c\u7406\u89e3\u6d89\u53ca\u8907\u96dc\u6587\u5b57\u7d50\u69cb\u3001\u591a\u500b\u7269\u4ef6\u6216\u7269\u4ef6\u7a7a\u9593\u95dc\u4fc2\u7684\u8907\u96dc\u6587\u5b57\u67e5\u8a62\uff0c\u9019\u662f\u76ee\u524d\u6a21\u578b\u96e3\u4ee5\u8655\u7406\u7684\u60c5\u6cc1\u3002LLM-Optic \u9996\u5148\u4f7f\u7528 LLM \u4f5c\u70ba\u6587\u5b57\u57fa\u790e\uff0c\u4f86\u8a6e\u91cb\u8907\u96dc\u7684\u6587\u5b57\u67e5\u8a62\uff0c\u4e26\u6e96\u78ba\u8fa8\u8b58\u4f7f\u7528\u8005\u60f3\u8981\u5b9a\u4f4d\u7684\u7269\u4ef6\u3002\u7136\u5f8c\u4f7f\u7528\u9810\u5148\u8a13\u7df4\u7684\u8996\u89ba\u57fa\u790e\u6a21\u578b\uff0c\u6839\u64da\u6587\u5b57\u57fa\u790e\u6240\u7cbe\u7149\u7684\u67e5\u8a62\u7522\u751f\u5019\u9078\u908a\u754c\u6846\u3002\u5728\u90a3\u4e4b\u5f8c\uff0cLLM-Optic \u4f7f\u7528\u6578\u5b57\u6a19\u8a18\u8a3b\u89e3\u5019\u9078\u908a\u754c\u6846\uff0c\u4ee5\u4fbf\u5728\u6587\u5b57\u548c\u7279\u5b9a\u5f71\u50cf\u5340\u57df\u4e4b\u9593\u5efa\u7acb\u95dc\u806f\uff0c\u5f9e\u800c\u9023\u7d50\u5169\u500b\u4e0d\u540c\u7684\u6a21\u614b\u3002\u6700\u5f8c\uff0c\u5b83\u4f7f\u7528\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u4f5c\u70ba\u8996\u89ba\u57fa\u790e\uff0c\u4f86\u9078\u64c7\u6a19\u8a18\u7684\u5019\u9078\u7269\u4ef6\uff0c\u9019\u4e9b\u7269\u4ef6\u6700\u7b26\u5408\u539f\u59cb\u6587\u5b57\u67e5\u8a62\u3002\u900f\u904e LLM-Optic\uff0c\u6211\u5011\u9054\u5230\u4e86\u901a\u7528\u7684\u8996\u89ba\u57fa\u790e\uff0c\u9019\u5141\u8a31\u5075\u6e2c\u7531\u4efb\u610f\u4eba\u985e\u8a9e\u8a00\u8f38\u5165\u6307\u5b9a\u7684\u4efb\u610f\u7269\u4ef6\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5728\u4e0d\u9700\u8981\u984d\u5916\u8a13\u7df4\u6216\u5fae\u8abf\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u4e86\u9019\u7a2e\u589e\u5f37\u3002\u900f\u904e\u5404\u7a2e\u5177\u6709\u6311\u6230\u6027\u7684\u57fa\u6e96\u6e2c\u8a66\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0cLLM-Optic \u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u96f6\u6b21\u8996\u89ba\u57fa\u790e\u80fd\u529b\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://haoyu-zhao.github.io/LLM-Optic.github.io/\u3002", "author": "Haoyu Zhao et.al.", "authors": "Haoyu Zhao, Wenhang Ge, Ying-cong Chen", "id": "2405.17104v2", "paper_url": "http://arxiv.org/abs/2405.17104v2", "repo": "null"}}