{"2411.03934": {"publish_time": "2024-11-06", "title": "Interactions Across Blocks in Post-Training Quantization of Large Language Models", "paper_summary": "Post-training quantization is widely employed to reduce the computational\ndemands of neural networks. Typically, individual substructures, such as layers\nor blocks of layers, are quantized with the objective of minimizing\nquantization errors in their pre-activations by fine-tuning the corresponding\nweights. Deriving this local objective from the global objective of minimizing\ntask loss involves two key simplifications: assuming substructures are mutually\nindependent and ignoring the knowledge of subsequent substructures as well as\nthe task loss. In this work, we assess the effects of these simplifications on\nweight-only quantization of large language models. We introduce two multi-block\nfine-tuning strategies and compare them against the baseline of fine-tuning\nsingle transformer blocks. The first captures correlations of weights across\nblocks by jointly optimizing multiple quantized blocks. The second incorporates\nknowledge of subsequent blocks by minimizing the error in downstream\npre-activations rather than focusing solely on the quantized block. Our\nfindings indicate that the effectiveness of these methods depends on the\nspecific network model, with no impact on some models but demonstrating\nsignificant benefits for others.", "paper_summary_zh": "\u8a13\u7df4\u5f8c\u91cf\u5316\u5ee3\u6cdb\u7528\u65bc\u6e1b\u5c11\u795e\u7d93\u7db2\u8def\u7684\u904b\u7b97\u9700\u6c42\u3002\u901a\u5e38\uff0c\u500b\u5225\u5b50\u7d50\u69cb\uff08\u4f8b\u5982\u5c64\u6216\u5c64\u584a\uff09\u6703\u91cf\u5316\uff0c\u76ee\u7684\u662f\u900f\u904e\u5fae\u8abf\u5c0d\u61c9\u7684\u6b0a\u91cd\u4f86\u6700\u5c0f\u5316\u5176\u9810\u6fc0\u6d3b\u4e2d\u7684\u91cf\u5316\u8aa4\u5dee\u3002\u5f9e\u6700\u5c0f\u5316\u4efb\u52d9\u640d\u5931\u7684\u6574\u9ad4\u76ee\u6a19\u4e2d\u63a8\u5c0e\u51fa\u9019\u500b\u5c40\u90e8\u76ee\u6a19\u6d89\u53ca\u5169\u500b\u95dc\u9375\u7c21\u5316\uff1a\u5047\u8a2d\u5b50\u7d50\u69cb\u76f8\u4e92\u7368\u7acb\uff0c\u4e26\u5ffd\u7565\u5f8c\u7e8c\u5b50\u7d50\u69cb\u4ee5\u53ca\u4efb\u52d9\u640d\u5931\u7684\u77e5\u8b58\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a55\u4f30\u9019\u4e9b\u7c21\u5316\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u50c5\u6b0a\u91cd\u91cf\u5316\u7684\u5f71\u97ff\u3002\u6211\u5011\u5f15\u5165\u4e86\u5169\u7a2e\u591a\u5340\u584a\u5fae\u8abf\u7b56\u7565\uff0c\u4e26\u5c07\u5b83\u5011\u8207\u5fae\u8abf\u55ae\u4e00Transformer\u5340\u584a\u7684\u57fa\u6e96\u9032\u884c\u6bd4\u8f03\u3002\u7b2c\u4e00\u500b\u900f\u904e\u5171\u540c\u6700\u4f73\u5316\u591a\u500b\u91cf\u5316\u5340\u584a\u4f86\u64f7\u53d6\u8de8\u5340\u584a\u7684\u6b0a\u91cd\u76f8\u95dc\u6027\u3002\u7b2c\u4e8c\u500b\u900f\u904e\u6700\u5c0f\u5316\u4e0b\u6e38\u9810\u6fc0\u6d3b\u4e2d\u7684\u8aa4\u5dee\uff08\u800c\u975e\u50c5\u5c08\u6ce8\u65bc\u91cf\u5316\u5340\u584a\uff09\u4f86\u7d0d\u5165\u5f8c\u7e8c\u5340\u584a\u7684\u77e5\u8b58\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u9019\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53d6\u6c7a\u65bc\u7279\u5b9a\u7684\u7db2\u8def\u6a21\u578b\uff0c\u5c0d\u67d0\u4e9b\u6a21\u578b\u6c92\u6709\u5f71\u97ff\uff0c\u4f46\u5c0d\u5176\u4ed6\u6a21\u578b\u5247\u5c55\u73fe\u51fa\u986f\u8457\u7684\u512a\u9ede\u3002", "author": "Khasmamad Shabanovi et.al.", "authors": "Khasmamad Shabanovi, Lukas Wiest, Vladimir Golkov, Daniel Cremers, Thomas Pfeil", "id": "2411.03934v1", "paper_url": "http://arxiv.org/abs/2411.03934v1", "repo": "null"}}