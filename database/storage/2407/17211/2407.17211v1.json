{"2407.17211": {"publish_time": "2024-07-24", "title": "Testing Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles", "paper_summary": "Handling long tail corner cases is a major challenge faced by autonomous\nvehicles (AVs). While large language models (LLMs) hold great potentials to\nhandle the corner cases with excellent generalization and explanation\ncapabilities and received increasing research interest on application to\nautonomous driving, there are still technical barriers to be tackled, such as\nstrict model performance and huge computing resource requirements of LLMs. In\nthis paper, we investigate a new approach of applying remote or edge LLMs to\nsupport autonomous driving. A key issue for such LLM assisted driving system is\nthe assessment of LLMs on their understanding of driving theory and skills,\nensuring they are qualified to undertake safety critical driving assistance\ntasks for CAVs. We design and run driving theory tests for several proprietary\nLLM models (OpenAI GPT models, Baidu Ernie and Ali QWen) and open-source LLM\nmodels (Tsinghua MiniCPM-2B and MiniCPM-Llama3-V2.5) with more than 500\nmultiple-choices theory test questions. Model accuracy, cost and processing\nlatency are measured from the experiments. Experiment results show that while\nmodel GPT-4 passes the test with improved domain knowledge and Ernie has an\naccuracy of 85% (just below the 86% passing threshold), other LLM models\nincluding GPT-3.5 fail the test. For the test questions with images, the\nmultimodal model GPT4-o has an excellent accuracy result of 96%, and the\nMiniCPM-Llama3-V2.5 achieves an accuracy of 76%. While GPT-4 holds stronger\npotential for CAV driving assistance applications, the cost of using model GPT4\nis much higher, almost 50 times of that of using GPT3.5. The results can help\nmake decision on the use of the existing LLMs for CAV applications and\nbalancing on the model performance and cost.", "paper_summary_zh": "<paragraph>\u8655\u7406\u9577\u5c3e\u89d2\u843d\u6848\u4f8b\u662f\u81ea\u52d5\u99d5\u99db\u8eca\u8f1b (AV) \u9762\u81e8\u7684\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u96d6\u7136\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5177\u6709\u6975\u4f73\u7684\u6982\u62ec\u548c\u8aaa\u660e\u80fd\u529b\uff0c\u8db3\u4ee5\u8655\u7406\u89d2\u843d\u6848\u4f8b\uff0c\u4e26\u5728\u61c9\u7528\u65bc\u81ea\u52d5\u99d5\u99db\u65b9\u9762\u53d7\u5230\u8d8a\u4f86\u8d8a\u591a\u7684\u7814\u7a76\u8208\u8da3\uff0c\u4f46\u4ecd\u6709\u6280\u8853\u969c\u7919\u9700\u8981\u514b\u670d\uff0c\u4f8b\u5982 LLM \u7684\u56b4\u683c\u6a21\u578b\u6548\u80fd\u548c\u9f90\u5927\u904b\u7b97\u8cc7\u6e90\u9700\u6c42\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4e00\u7a2e\u5c07\u9060\u7aef\u6216\u908a\u7de3 LLM \u61c9\u7528\u65bc\u652f\u63f4\u81ea\u52d5\u99d5\u99db\u7684\u65b0\u65b9\u6cd5\u3002\u6b64\u985e LLM \u5354\u52a9\u99d5\u99db\u7cfb\u7d71\u7684\u4e00\u500b\u95dc\u9375\u554f\u984c\u662f\u8a55\u4f30 LLM \u5c0d\u99d5\u99db\u7406\u8ad6\u548c\u6280\u80fd\u7684\u7406\u89e3\uff0c\u78ba\u4fdd\u5b83\u5011\u6709\u8cc7\u683c\u627f\u64d4 CAV \u7684\u5b89\u5168\u95dc\u9375\u99d5\u99db\u8f14\u52a9\u4efb\u52d9\u3002\u6211\u5011\u91dd\u5c0d\u591a\u500b\u5c08\u6709 LLM \u6a21\u578b\uff08OpenAI GPT \u6a21\u578b\u3001\u767e\u5ea6 Ernie \u548c\u963f\u91cc QWen\uff09\u548c\u958b\u6e90 LLM \u6a21\u578b\uff08\u6e05\u83ef\u5927\u5b78 MiniCPM-2B \u548c MiniCPM-Llama3-V2.5\uff09\u8a2d\u8a08\u4e26\u57f7\u884c\u99d5\u99db\u7406\u8ad6\u6e2c\u8a66\uff0c\u6e2c\u8a66\u984c\u76ee\u8d85\u904e 500 \u984c\u591a\u9078\u984c\u7406\u8ad6\u8003\u984c\u3002\u5f9e\u5be6\u9a57\u4e2d\u6e2c\u91cf\u6a21\u578b\u6e96\u78ba\u5ea6\u3001\u6210\u672c\u548c\u8655\u7406\u5ef6\u9072\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u5118\u7ba1\u6a21\u578b GPT-4 \u901a\u904e\u6e2c\u8a66\uff0c\u4e14\u9818\u57df\u77e5\u8b58\u6709\u6240\u63d0\u5347\uff0c\u800c Ernie \u7684\u6e96\u78ba\u5ea6\u70ba 85%\uff08\u7565\u4f4e\u65bc 86% \u7684\u53ca\u683c\u9580\u6abb\uff09\uff0c\u4f46\u5305\u62ec GPT-3.5 \u5728\u5167\u7684\u5176\u4ed6 LLM \u6a21\u578b\u672a\u901a\u904e\u6e2c\u8a66\u3002\u5c0d\u65bc\u5e36\u6709\u5716\u7247\u7684\u6e2c\u8a66\u984c\u76ee\uff0c\u591a\u6a21\u614b\u6a21\u578b GPT4-o \u7684\u6e96\u78ba\u5ea6\u7d50\u679c\u6975\u4f73\uff0c\u9054\u5230 96%\uff0c\u800c MiniCPM-Llama3-V2.5 \u7684\u6e96\u78ba\u5ea6\u9054\u5230 76%\u3002\u5118\u7ba1 GPT-4 \u5c0d\u65bc CAV \u99d5\u99db\u8f14\u52a9\u61c9\u7528\u7a0b\u5f0f\u5177\u6709\u66f4\u5f37\u5927\u7684\u6f5b\u529b\uff0c\u4f46\u4f7f\u7528\u6a21\u578b GPT4 \u7684\u6210\u672c\u537b\u9ad8\u5f97\u591a\uff0c\u5e7e\u4e4e\u662f\u4f7f\u7528 GPT3.5 \u7684 50 \u500d\u3002\u9019\u4e9b\u7d50\u679c\u6709\u52a9\u65bc\u91dd\u5c0d CAV \u61c9\u7528\u7a0b\u5f0f\u4f7f\u7528\u73fe\u6709 LLM \u505a\u51fa\u6c7a\u7b56\uff0c\u4e26\u5728\u6a21\u578b\u6548\u80fd\u548c\u6210\u672c\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002</paragraph>", "author": "Zuoyin Tang et.al.", "authors": "Zuoyin Tang, Jianhua He, Dashuai Pei, Kezhong Liu, Tao Gao", "id": "2407.17211v1", "paper_url": "http://arxiv.org/abs/2407.17211v1", "repo": "null"}}