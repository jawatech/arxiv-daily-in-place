{"2405.20527": {"publish_time": "2024-05-30", "title": "Towards Ontology-Enhanced Representation Learning for Large Language Models", "paper_summary": "Taking advantage of the widespread use of ontologies to organise and\nharmonize knowledge across several distinct domains, this paper proposes a\nnovel approach to improve an embedding-Large Language Model (embedding-LLM) of\ninterest by infusing the knowledge formalized by a reference ontology:\nontological knowledge infusion aims at boosting the ability of the considered\nLLM to effectively model the knowledge domain described by the infused\nontology. The linguistic information (i.e. concept synonyms and descriptions)\nand structural information (i.e. is-a relations) formalized by the ontology are\nutilized to compile a comprehensive set of concept definitions, with the\nassistance of a powerful generative LLM (i.e. GPT-3.5-turbo). These concept\ndefinitions are then employed to fine-tune the target embedding-LLM using a\ncontrastive learning framework. To demonstrate and evaluate the proposed\napproach, we utilize the biomedical disease ontology MONDO. The results show\nthat embedding-LLMs enhanced by ontological disease knowledge exhibit an\nimproved capability to effectively evaluate the similarity of in-domain\nsentences from biomedical documents mentioning diseases, without compromising\ntheir out-of-domain performance.", "paper_summary_zh": "<paragraph>\u5229\u7528\u672c\u4f53\u8ad6\u5ee3\u6cdb\u7528\u65bc\u7d44\u7e54\u548c\u8abf\u548c\u4e0d\u540c\u9818\u57df\u9593\u7684\u77e5\u8b58\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\u4f86\u6539\u9032\u5d4c\u5165\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (embedding-LLM) \u7684\u8208\u8da3\uff0c\u65b9\u6cd5\u662f\u6ce8\u5165\u7531\u53c3\u8003\u672c\u4f53\u8ad6\u5f62\u5f0f\u5316\u7684\u77e5\u8b58\uff1a\u672c\u4f53\u8ad6\u77e5\u8b58\u6ce8\u5165\u65e8\u5728\u63d0\u5347\u6240\u8003\u616e\u7684 LLM \u6709\u6548\u5efa\u6a21\u7531\u6ce8\u5165\u7684\u672c\u4f53\u8ad6\u6240\u63cf\u8ff0\u7684\u77e5\u8b58\u9818\u57df\u7684\u80fd\u529b\u3002\u7531\u672c\u4f53\u8ad6\u5f62\u5f0f\u5316\u7684\u8a9e\u8a00\u8cc7\u8a0a\uff08\u5373\u6982\u5ff5\u540c\u7fa9\u8a5e\u548c\u63cf\u8ff0\uff09\u548c\u7d50\u69cb\u8cc7\u8a0a\uff08\u5373 is-a \u95dc\u4fc2\uff09\u7528\u65bc\u7de8\u8b6f\u4e00\u7d44\u5168\u9762\u7684\u6982\u5ff5\u5b9a\u7fa9\uff0c\u4e26\u5728\u5f37\u5927\u7684\u751f\u6210\u5f0f LLM\uff08\u5373 GPT-3.5-turbo\uff09\u7684\u5354\u52a9\u4e0b\u9032\u884c\u3002\u7136\u5f8c\uff0c\u9019\u4e9b\u6982\u5ff5\u5b9a\u7fa9\u88ab\u7528\u65bc\u4f7f\u7528\u5c0d\u6bd4\u5b78\u7fd2\u67b6\u69cb\u5fae\u8abf\u76ee\u6a19\u5d4c\u5165\u5f0f LLM\u3002\u70ba\u4e86\u5c55\u793a\u548c\u8a55\u4f30\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u6211\u5011\u5229\u7528\u751f\u7269\u91ab\u5b78\u75be\u75c5\u672c\u4f53\u8ad6 MONDO\u3002\u7d50\u679c\u8868\u660e\uff0c\u7531\u672c\u4f53\u8ad6\u75be\u75c5\u77e5\u8b58\u589e\u5f37\u7684\u5d4c\u5165\u5f0f LLM \u8868\u73fe\u51fa\u6709\u6548\u8a55\u4f30\u63d0\u53ca\u75be\u75c5\u7684\u751f\u7269\u91ab\u5b78\u6587\u4ef6\u4e2d\u7684\u540c\u57df\u53e5\u5b50\u76f8\u4f3c\u6027\u7684\u6539\u9032\u80fd\u529b\uff0c\u800c\u4e0d\u6703\u640d\u5bb3\u5176\u57df\u5916\u6548\u80fd\u3002</paragraph>", "author": "Francesco Ronzano et.al.", "authors": "Francesco Ronzano, Jay Nanavati", "id": "2405.20527v1", "paper_url": "http://arxiv.org/abs/2405.20527v1", "repo": "null"}}