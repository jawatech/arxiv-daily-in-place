{"2407.08112": {"publish_time": "2024-07-11", "title": "How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities", "paper_summary": "Long sequences occur in abundance within real-world scenarios, hence properly\nmodelling them opens numerous down-stream use-cases. Deep neural networks,\nhowever, have often struggled with these for a variety of reasons. Recent\nadvances, both in system engineering as well as model design, have enabled the\nscaling up of model that are purported to support extended context length. In\nparticular, the state-space and linear recurrent neural network families of\nmodels hypothetically can entend to infinite sequence lenth. However, is this\ntoo good to be true? We conduct an evaluation to show that while such claims\nmay be sound theoretically, there remain large practical gaps that are\nempirically observed. In particular, recurrent models still suffer in the same\nsettings as long-context LLMs with attention. We further show that different\ninductive biases have inconsistent extrapolation capabilities, highlighting the\nneed to further study such paradigms and investigate why long-context models\nseemingly fail to behave as one might expect.", "paper_summary_zh": "\u9577\u5e8f\u5217\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u5927\u91cf\u51fa\u73fe\uff0c\u56e0\u6b64\u9069\u7576\u7684\u5efa\u6a21\u70ba\u6211\u5011\u958b\u555f\u4e86\u8a31\u591a\u4e0b\u6e38\u7528\u4f8b\u3002\u7136\u800c\uff0c\u7531\u65bc\u5404\u7a2e\u539f\u56e0\uff0c\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u7d93\u5e38\u96e3\u4ee5\u8655\u7406\u9019\u4e9b\u5e8f\u5217\u3002\u7cfb\u7d71\u5de5\u7a0b\u548c\u6a21\u578b\u8a2d\u8a08\u7684\u6700\u65b0\u9032\u5c55\uff0c\u4f7f\u5f97\u64f4\u5c55\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u9577\u5ea6\u6210\u70ba\u53ef\u80fd\uff0c\u800c\u9019\u4e9b\u6a21\u578b\u64da\u7a31\u53ef\u4ee5\u652f\u63f4\u64f4\u5c55\u7684\u4e0a\u4e0b\u6587\u9577\u5ea6\u3002\u7279\u5225\u662f\uff0c\u72c0\u614b\u7a7a\u9593\u548c\u7dda\u6027\u905e\u8ff4\u795e\u7d93\u7db2\u8def\u7684\u6a21\u578b\u65cf\u5728\u7406\u8ad6\u4e0a\u53ef\u4ee5\u5ef6\u4f38\u5230\u7121\u9650\u5e8f\u5217\u9577\u5ea6\u3002\u7136\u800c\uff0c\u9019\u662f\u5426\u597d\u5f97\u4ee4\u4eba\u96e3\u4ee5\u7f6e\u4fe1\uff1f\u6211\u5011\u9032\u884c\u4e86\u4e00\u9805\u8a55\u4f30\uff0c\u4ee5\u8868\u660e\u96d6\u7136\u9019\u4e9b\u8aaa\u6cd5\u5728\u7406\u8ad6\u4e0a\u53ef\u80fd\u662f\u5408\u7406\u7684\uff0c\u4f46\u4ecd\u5b58\u5728\u5927\u91cf\u5728\u7d93\u9a57\u4e0a\u89c0\u5bdf\u5230\u7684\u5be6\u969b\u5dee\u8ddd\u3002\u7279\u5225\u662f\uff0c\u905e\u8ff4\u6a21\u578b\u5728\u8207\u5177\u6709\u6ce8\u610f\u529b\u7684\u9577\u4e0a\u4e0b\u6587 LLM \u76f8\u540c\u7684\u8a2d\u5b9a\u4e2d\u4ecd\u7136\u8868\u73fe\u4e0d\u4f73\u3002\u6211\u5011\u9032\u4e00\u6b65\u8868\u660e\uff0c\u4e0d\u540c\u7684\u6b78\u7d0d\u504f\u5dee\u5177\u6709\u4e0d\u4e00\u81f4\u7684\u5916\u63a8\u80fd\u529b\uff0c\u5f37\u8abf\u4e86\u9032\u4e00\u6b65\u7814\u7a76\u9019\u4e9b\u7bc4\u4f8b\u548c\u8abf\u67e5\u70ba\u4ec0\u9ebc\u9577\u4e0a\u4e0b\u6587\u6a21\u578b\u770b\u4f3c\u7121\u6cd5\u6309\u7167\u9810\u671f\u884c\u70ba\u7684\u5fc5\u8981\u6027\u3002", "author": "Jerry Huang et.al.", "authors": "Jerry Huang", "id": "2407.08112v1", "paper_url": "http://arxiv.org/abs/2407.08112v1", "repo": "null"}}