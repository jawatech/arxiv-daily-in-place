{"2407.08196": {"publish_time": "2024-07-11", "title": "SoupLM: Model Integration in Large Language and Multi-Modal Models", "paper_summary": "Training large language models (LLMs) and multimodal LLMs necessitates\nsignificant computing resources, and existing publicly available LLMs are\ntypically pre-trained on diverse, privately curated datasets spanning various\ntasks. For instance, LLaMA, Vicuna, and LLaVA are three LLM variants trained\nwith LLaMA base models using very different training recipes, tasks, and data\nmodalities. The training cost and complexity for such LLM variants grow\nrapidly. In this study, we propose to use a soup strategy to assemble these LLM\nvariants into a single well-generalized multimodal LLM (SoupLM) in a\ncost-efficient manner. Assembling these LLM variants efficiently brings\nknowledge and specialities trained from different domains and data modalities\ninto an integrated one (e.g., chatbot speciality from user-shared conversations\nfor Vicuna, and visual capacity from vision-language data for LLaVA),\ntherefore, to avoid computing costs of repetitive training on several different\ndomains. We propose series of soup strategies to systematically benchmark\nperformance gains across various configurations, and probe the soup behavior\nacross base models in the interpolation space.", "paper_summary_zh": "\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u591a\u6a21\u614b LLM \u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\uff0c\u800c\u73fe\u6709\u7684\u516c\u958b LLM \u901a\u5e38\u4f7f\u7528\u591a\u5143\u4e14\u79c1\u6709\u7b56\u5c55\u7684\u8cc7\u6599\u96c6\u9032\u884c\u9810\u5148\u8a13\u7df4\uff0c\u6db5\u84cb\u5404\u7a2e\u4efb\u52d9\u3002\u4f8b\u5982\uff0cLLaMA\u3001Vicuna \u548c LLaVA \u662f\u4e09\u500b LLM \u8b8a\u9ad4\uff0c\u4f7f\u7528 LLaMA \u57fa\u790e\u6a21\u578b\u9032\u884c\u8a13\u7df4\uff0c\u63a1\u7528\u975e\u5e38\u4e0d\u540c\u7684\u8a13\u7df4\u914d\u65b9\u3001\u4efb\u52d9\u548c\u8cc7\u6599\u6a21\u5f0f\u3002\u6b64\u985e LLM \u8b8a\u9ad4\u7684\u8a13\u7df4\u6210\u672c\u548c\u8907\u96dc\u5ea6\u6703\u5feb\u901f\u589e\u52a0\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u6e6f\u7b56\u7565\u5c07\u9019\u4e9b LLM \u8b8a\u9ad4\u7d44\u88dd\u6210\u4e00\u500b\u55ae\u4e00\u7684\u826f\u597d\u6cdb\u5316\u591a\u6a21\u614b LLM (SoupLM)\uff0c\u4ee5\u4e00\u7a2e\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u65b9\u5f0f\u9032\u884c\u3002\u6709\u6548\u5730\u7d44\u88dd\u9019\u4e9b LLM \u8b8a\u9ad4\uff0c\u5c07\u5f9e\u4e0d\u540c\u9818\u57df\u548c\u8cc7\u6599\u6a21\u5f0f\u8a13\u7df4\u800c\u4f86\u7684\u77e5\u8b58\u548c\u5c08\u9577\u5e36\u5165\u4e00\u500b\u6574\u5408\u7684\u6a21\u5f0f\uff08\u4f8b\u5982\uff0cVicuna \u7684\u4f7f\u7528\u8005\u5171\u4eab\u5c0d\u8a71\u4e2d\u7684\u804a\u5929\u6a5f\u5668\u4eba\u5c08\u9577\uff0c\u4ee5\u53ca LLaVA \u7684\u8996\u89ba\u8a9e\u8a00\u8cc7\u6599\u4e2d\u7684\u8996\u89ba\u80fd\u529b\uff09\uff0c\u56e0\u6b64\uff0c\u53ef\u4ee5\u907f\u514d\u5728\u591a\u500b\u4e0d\u540c\u9818\u57df\u9032\u884c\u91cd\u8907\u8a13\u7df4\u7684\u904b\u7b97\u6210\u672c\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6e6f\u7b56\u7565\uff0c\u4ee5\u7cfb\u7d71\u6027\u5730\u6bd4\u8f03\u5404\u7a2e\u7d44\u614b\u7684\u6548\u80fd\u63d0\u5347\uff0c\u4e26\u63a2\u8a0e\u63d2\u503c\u7a7a\u9593\u4e2d\u57fa\u790e\u6a21\u578b\u4e4b\u9593\u7684\u6e6f\u884c\u70ba\u3002", "author": "Yue Bai et.al.", "authors": "Yue Bai, Zichen Zhang, Jiasen Lu, Yun Fu", "id": "2407.08196v1", "paper_url": "http://arxiv.org/abs/2407.08196v1", "repo": "null"}}