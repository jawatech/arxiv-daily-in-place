{"2412.09627": {"publish_time": "2024-12-12", "title": "Doe-1: Closed-Loop Autonomous Driving with Large World Model", "paper_summary": "End-to-end autonomous driving has received increasing attention due to its\npotential to learn from large amounts of data. However, most existing methods\nare still open-loop and suffer from weak scalability, lack of high-order\ninteractions, and inefficient decision-making. In this paper, we explore a\nclosed-loop framework for autonomous driving and propose a large Driving wOrld\nmodEl (Doe-1) for unified perception, prediction, and planning. We formulate\nautonomous driving as a next-token generation problem and use multi-modal\ntokens to accomplish different tasks. Specifically, we use free-form texts\n(i.e., scene descriptions) for perception and generate future predictions\ndirectly in the RGB space with image tokens. For planning, we employ a\nposition-aware tokenizer to effectively encode action into discrete tokens. We\ntrain a multi-modal transformer to autoregressively generate perception,\nprediction, and planning tokens in an end-to-end and unified manner.\nExperiments on the widely used nuScenes dataset demonstrate the effectiveness\nof Doe-1 in various tasks including visual question-answering,\naction-conditioned video generation, and motion planning. Code:\nhttps://github.com/wzzheng/Doe.", "paper_summary_zh": "\u7aef\u5230\u7aef\u81ea\u52d5\u99d5\u99db\u7531\u65bc\u5176\u5f9e\u5927\u91cf\u8cc7\u6599\u4e2d\u5b78\u7fd2\u7684\u6f5b\u529b\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u73fe\u6709\u65b9\u6cd5\u4ecd\u7136\u662f\u958b\u8ff4\u8def\uff0c\u4e26\u53d7\u5230\u53ef\u64f4\u5145\u6027\u5f31\u3001\u7f3a\u4e4f\u9ad8\u968e\u4e92\u52d5\u4ee5\u53ca\u6c7a\u7b56\u6548\u7387\u4f4e\u4e0b\u7684\u56f0\u64fe\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4e00\u500b\u7528\u65bc\u81ea\u52d5\u99d5\u99db\u7684\u9589\u8ff4\u8def\u67b6\u69cb\uff0c\u4e26\u63d0\u51fa\u4e86\u4e00\u500b\u5927\u578b\u99d5\u99db\u4e16\u754c\u6a21\u578b (Doe-1) \u4ee5\u5be6\u73fe\u7d71\u4e00\u7684\u611f\u77e5\u3001\u9810\u6e2c\u548c\u898f\u5283\u3002\u6211\u5011\u5c07\u81ea\u52d5\u99d5\u99db\u8868\u8ff0\u70ba\u4e00\u500b\u4e0b\u4e00\u500b\u7b26\u865f\u751f\u6210\u554f\u984c\uff0c\u4e26\u4f7f\u7528\u591a\u6a21\u614b\u7b26\u865f\u4f86\u5b8c\u6210\u4e0d\u540c\u7684\u4efb\u52d9\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4f7f\u7528\u81ea\u7531\u5f62\u5f0f\u6587\u672c\uff08\u5373\u5834\u666f\u63cf\u8ff0\uff09\u9032\u884c\u611f\u77e5\uff0c\u4e26\u76f4\u63a5\u5728 RGB \u7a7a\u9593\u4e2d\u4f7f\u7528\u5f71\u50cf\u7b26\u865f\u751f\u6210\u672a\u4f86\u7684\u9810\u6e2c\u3002\u5c0d\u65bc\u898f\u5283\uff0c\u6211\u5011\u63a1\u7528\u4e00\u500b\u4f4d\u7f6e\u611f\u77e5\u7b26\u865f\u5316\u5668\uff0c\u5c07\u52d5\u4f5c\u6709\u6548\u7de8\u78bc\u6210\u96e2\u6563\u7b26\u865f\u3002\u6211\u5011\u8a13\u7df4\u4e00\u500b\u591a\u6a21\u614bTransformer\uff0c\u4ee5\u81ea\u8ff4\u6b78\u65b9\u5f0f\u751f\u6210\u611f\u77e5\u3001\u9810\u6e2c\u548c\u898f\u5283\u7b26\u865f\uff0c\u4e26\u4ee5\u7aef\u5230\u7aef\u548c\u7d71\u4e00\u7684\u65b9\u5f0f\u9032\u884c\u3002\u5728\u5ee3\u6cdb\u4f7f\u7528\u7684 nuScenes \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\u4e86 Doe-1 \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u8996\u89ba\u554f\u7b54\u3001\u52d5\u4f5c\u689d\u4ef6\u5f71\u7247\u751f\u6210\u548c\u52d5\u4f5c\u898f\u5283\u3002\u7a0b\u5f0f\u78bc\uff1ahttps://github.com/wzzheng/Doe\u3002", "author": "Wenzhao Zheng et.al.", "authors": "Wenzhao Zheng, Zetian Xia, Yuanhui Huang, Sicheng Zuo, Jie Zhou, Jiwen Lu", "id": "2412.09627v1", "paper_url": "http://arxiv.org/abs/2412.09627v1", "repo": "https://github.com/wzzheng/doe"}}