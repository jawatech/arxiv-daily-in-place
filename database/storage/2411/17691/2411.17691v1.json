{"2411.17691": {"publish_time": "2024-11-26", "title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens", "paper_summary": "We reveal that low-bit quantization favors undertrained large language models\n(LLMs) by observing that models with larger sizes or fewer training tokens\nexperience less quantization-induced degradation (QiD) when applying low-bit\nquantization, whereas smaller models with extensive training tokens suffer\nsignificant QiD. To gain deeper insights into this trend, we study over 1500\nquantized LLM checkpoints of various sizes and at different training levels\n(undertrained or fully trained) in a controlled setting, deriving scaling laws\nfor understanding the relationship between QiD and factors such as the number\nof training tokens, model size and bit width.\n  With the derived scaling laws, we propose a novel perspective that we can use\nQiD to measure an LLM's training levels and determine the number of training\ntokens required for fully training LLMs of various sizes. Moreover, we use the\nscaling laws to predict the quantization performance of different-sized LLMs\ntrained with 100 trillion tokens. Our projection shows that the low-bit\nquantization performance of future models, which are expected to be trained\nwith over 100 trillion tokens, may NOT be desirable. This poses a potential\nchallenge for low-bit quantization in the future and highlights the need for\nawareness of a model's training level when evaluating low-bit quantization\nresearch. To facilitate future research on this problem, we release all the\n1500+ quantized checkpoints used in this work at\nhttps://huggingface.co/Xu-Ouyang.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63ed\u793a\u4e86\u4f4e\u4f4d\u5143\u91cf\u5316\u6709\u5229\u65bc\u8a13\u7df4\u4e0d\u8db3\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u900f\u904e\u89c0\u5bdf\u5177\u6709\u8f03\u5927\u898f\u6a21\u6216\u8f03\u5c11\u8a13\u7df4\u4ee3\u5e63\u7684\u6a21\u578b\u5728\u61c9\u7528\u4f4e\u4f4d\u5143\u91cf\u5316\u6642\uff0c\u6703\u7d93\u6b77\u8f03\u5c11\u7684\u91cf\u5316\u8a98\u767c\u7684\u9000\u5316\uff08QiD\uff09\uff0c\u800c\u5177\u6709\u5927\u91cf\u8a13\u7df4\u4ee3\u5e63\u7684\u8f03\u5c0f\u6a21\u578b\u5247\u6703\u906d\u53d7\u986f\u8457\u7684 QiD\u3002\u70ba\u4e86\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u6b64\u8da8\u52e2\uff0c\u6211\u5011\u5728\u53d7\u63a7\u74b0\u5883\u4e2d\u7814\u7a76\u4e86 1500 \u591a\u500b\u5404\u7a2e\u898f\u6a21\u548c\u4e0d\u540c\u8a13\u7df4\u5c64\u7d1a\uff08\u8a13\u7df4\u4e0d\u8db3\u6216\u5b8c\u5168\u8a13\u7df4\uff09\u7684\u91cf\u5316 LLM \u6aa2\u67e5\u9ede\uff0c\u63a8\u5c0e\u51fa\u6bd4\u4f8b\u5b9a\u5f8b\uff0c\u4ee5\u4e86\u89e3 QiD \u8207\u8a13\u7df4\u4ee3\u5e63\u6578\u91cf\u3001\u6a21\u578b\u898f\u6a21\u548c\u4f4d\u5143\u5bec\u5ea6\u7b49\u56e0\u7d20\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\n\u6709\u4e86\u63a8\u5c0e\u51fa\u7684\u6bd4\u4f8b\u5b9a\u5f8b\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u89c0\u9ede\uff0c\u5373\u6211\u5011\u53ef\u4ee5\u4f7f\u7528 QiD \u4f86\u8861\u91cf LLM \u7684\u8a13\u7df4\u5c64\u7d1a\uff0c\u4e26\u78ba\u5b9a\u5b8c\u5168\u8a13\u7df4\u5404\u7a2e\u898f\u6a21\u7684 LLM \u6240\u9700\u7684\u8a13\u7df4\u4ee3\u5e63\u6578\u91cf\u3002\u6b64\u5916\uff0c\u6211\u5011\u4f7f\u7528\u6bd4\u4f8b\u5b9a\u5f8b\u4f86\u9810\u6e2c\u4f7f\u7528 100 \u5146\u500b\u4ee3\u5e63\u8a13\u7df4\u7684\u4e0d\u540c\u898f\u6a21 LLM \u7684\u91cf\u5316\u6548\u80fd\u3002\u6211\u5011\u7684\u9810\u6e2c\u986f\u793a\uff0c\u9810\u8a08\u5c07\u4f7f\u7528\u8d85\u904e 100 \u5146\u500b\u4ee3\u5e63\u8a13\u7df4\u7684\u672a\u4f86\u6a21\u578b\u7684\u4f4e\u4f4d\u5143\u91cf\u5316\u6548\u80fd\u53ef\u80fd\u4e26\u975e\u7406\u60f3\u3002\u9019\u5c0d\u672a\u4f86\u7684\u4f4e\u4f4d\u5143\u91cf\u5316\u69cb\u6210\u6f5b\u5728\u6311\u6230\uff0c\u4e26\u5f37\u8abf\u5728\u8a55\u4f30\u4f4e\u4f4d\u5143\u91cf\u5316\u7814\u7a76\u6642\u9700\u8981\u4e86\u89e3\u6a21\u578b\u7684\u8a13\u7df4\u5c64\u7d1a\u3002\u70ba\u4e86\u4fc3\u9032\u672a\u4f86\u5c0d\u6b64\u554f\u984c\u7684\u7814\u7a76\uff0c\u6211\u5011\u5728 https://huggingface.co/Xu-Ouyang \u4e0a\u767c\u5e03\u4e86\u9019\u9805\u5de5\u4f5c\u4e2d\u4f7f\u7528\u7684\u6240\u6709 1500 \u591a\u500b\u91cf\u5316\u6aa2\u67e5\u9ede\u3002</paragraph>", "author": "Xu Ouyang et.al.", "authors": "Xu Ouyang, Tao Ge, Thomas Hartvigsen, Zhisong Zhang, Haitao Mi, Dong Yu", "id": "2411.17691v1", "paper_url": "http://arxiv.org/abs/2411.17691v1", "repo": "null"}}