{"2407.07276": {"publish_time": "2024-07-09", "title": "Exploring Camera Encoder Designs for Autonomous Driving Perception", "paper_summary": "The cornerstone of autonomous vehicles (AV) is a solid perception system,\nwhere camera encoders play a crucial role. Existing works usually leverage\npre-trained Convolutional Neural Networks (CNN) or Vision Transformers (ViTs)\ndesigned for general vision tasks, such as image classification, segmentation,\nand 2D detection. Although those well-known architectures have achieved\nstate-of-the-art accuracy in AV-related tasks, e.g., 3D Object Detection, there\nremains significant potential for improvement in network design due to the\nnuanced complexities of industrial-level AV dataset. Moreover, existing public\nAV benchmarks usually contain insufficient data, which might lead to inaccurate\nevaluation of those architectures.To reveal the AV-specific model insights, we\nstart from a standard general-purpose encoder, ConvNeXt and progressively\ntransform the design. We adjust different design parameters including width and\ndepth of the model, stage compute ratio, attention mechanisms, and input\nresolution, supported by systematic analysis to each modifications. This\ncustomization yields an architecture optimized for AV camera encoder achieving\n8.79% mAP improvement over the baseline. We believe our effort could become a\nsweet cookbook of image encoders for AV and pave the way to the next-level\ndrive system.", "paper_summary_zh": "\u81ea\u52d5\u99d5\u99db\u8eca\u8f1b (AV) \u7684\u57fa\u77f3\u662f\u4e00\u500b\u7a69\u56fa\u7684\u611f\u77e5\u7cfb\u7d71\uff0c\u5176\u4e2d\u76f8\u6a5f\u7de8\u78bc\u5668\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u73fe\u6709\u7684\u4f5c\u54c1\u901a\u5e38\u5229\u7528\u70ba\u4e00\u822c\u8996\u89ba\u4efb\u52d9\uff08\u4f8b\u5982\u5f71\u50cf\u5206\u985e\u3001\u5206\u5272\u548c 2D \u5075\u6e2c\uff09\u8a2d\u8a08\u7684\u9810\u5148\u8a13\u7df4\u7684\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN) \u6216\u8996\u89baTransformer (ViT)\u3002\u96d6\u7136\u9019\u4e9b\u8457\u540d\u7684\u67b6\u69cb\u5728 AV \u76f8\u95dc\u4efb\u52d9\uff08\u4f8b\u5982 3D \u7269\u4ef6\u5075\u6e2c\uff09\u4e2d\u5df2\u9054\u5230\u6700\u5148\u9032\u7684\u6e96\u78ba\u5ea6\uff0c\u4f46\u7531\u65bc\u7522\u696d\u7d1a AV \u8cc7\u6599\u96c6\u7684\u7d30\u5fae\u8907\u96dc\u6027\uff0c\u7db2\u8def\u8a2d\u8a08\u4ecd\u6709\u986f\u8457\u7684\u6539\u9032\u6f5b\u529b\u3002\u6b64\u5916\uff0c\u73fe\u6709\u7684\u516c\u958b AV \u57fa\u6e96\u901a\u5e38\u5305\u542b\u4e0d\u8db3\u7684\u8cc7\u6599\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u9019\u4e9b\u67b6\u69cb\u7684\u8a55\u4f30\u4e0d\u6e96\u78ba\u3002\u70ba\u4e86\u63ed\u793a AV \u7279\u5b9a\u7684\u6a21\u578b\u898b\u89e3\uff0c\u6211\u5011\u5f9e\u4e00\u500b\u6a19\u6e96\u7684\u901a\u7528\u7de8\u78bc\u5668 ConvNeXt \u958b\u59cb\uff0c\u4e26\u9010\u6b65\u8f49\u63db\u8a2d\u8a08\u3002\u6211\u5011\u8abf\u6574\u4e0d\u540c\u7684\u8a2d\u8a08\u53c3\u6578\uff0c\u5305\u62ec\u6a21\u578b\u7684\u5bec\u5ea6\u548c\u6df1\u5ea6\u3001\u968e\u6bb5\u904b\u7b97\u6bd4\u3001\u6ce8\u610f\u529b\u6a5f\u5236\u548c\u8f38\u5165\u89e3\u6790\u5ea6\uff0c\u4e26\u900f\u904e\u7cfb\u7d71\u5206\u6790\u652f\u63f4\u6bcf\u500b\u4fee\u6539\u3002\u6b64\u81ea\u8a02\u5316\u7522\u751f\u4e86\u4e00\u500b\u91dd\u5c0d AV \u76f8\u6a5f\u7de8\u78bc\u5668\u6700\u4f73\u5316\u7684\u67b6\u69cb\uff0c\u8207\u57fa\u6e96\u76f8\u6bd4\uff0cmAP \u63d0\u5347\u4e86 8.79%\u3002\u6211\u5011\u76f8\u4fe1\u6211\u5011\u7684\u52aa\u529b\u53ef\u4ee5\u6210\u70ba AV \u5f71\u50cf\u7de8\u78bc\u5668\u7684\u751c\u871c\u98df\u8b5c\uff0c\u4e26\u70ba\u4e0b\u4e00\u7d1a\u7684\u9a45\u52d5\u7cfb\u7d71\u92ea\u8def\u3002", "author": "Barath Lakshmanan et.al.", "authors": "Barath Lakshmanan, Joshua Chen, Shiyi Lan, Maying Shen, Zhiding Yu, Jose M. Alvarez", "id": "2407.07276v1", "paper_url": "http://arxiv.org/abs/2407.07276v1", "repo": "null"}}