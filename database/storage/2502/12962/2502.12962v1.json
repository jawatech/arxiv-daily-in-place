{"2502.12962": {"publish_time": "2025-02-18", "title": "Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing", "paper_summary": "Limited by the context window size of Large Language Models(LLMs), handling\nvarious tasks with input tokens exceeding the upper limit has been challenging,\nwhether it is a simple direct retrieval task or a complex multi-hop reasoning\ntask. Although various methods have been proposed to enhance the long-context\nprocessing capabilities of LLMs, they either incur substantial post-training\ncosts, or require additional tool modules(e.g.,RAG), or have not shown\nsignificant improvement in realistic tasks. Our work observes the correlation\nbetween the attention distribution and generated answers across each layer, and\nestablishes the attention allocation aligns with retrieval-augmented\ncapabilities through experiments. Drawing on the above insights, we propose a\nnovel method InfiniRetri that leverages the LLMs's own attention information to\nenable accurate retrieval across inputs of infinitely length. Our evaluations\nindicate that InfiniRetri achieves 100% accuracy in the\nNeedle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model,\nsurpassing other method or larger models and setting a new\nstate-of-the-art(SOTA). Moreover, our method achieves significant performance\nimprovements on real-world benchmarks, with a maximum 288% improvement. In\naddition, InfiniRetri can be applied to any Transformer-based LLMs without\nadditional training and substantially reduces inference latency and compute\noverhead in long texts. In summary, our comprehensive studies show\nInfiniRetri's potential for practical applications and creates a paradigm for\nretrievaling information using LLMs own capabilities under infinite-length\ntokens. Code will be released in link.", "paper_summary_zh": "\u53d7\u9650\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f\uff0c\u5904\u7406\u8d85\u51fa\u4e0a\u9650\u7684\u8f93\u5165\u6807\u8bb0\u7684\u5404\u79cd\u4efb\u52a1\u4e00\u76f4\u5177\u6709\u6311\u6218\u6027\uff0c\u65e0\u8bba\u662f\u7b80\u5355\u7684\u76f4\u63a5\u68c0\u7d22\u4efb\u52a1\u8fd8\u662f\u590d\u6742\u7684\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u3002\u867d\u7136\u5df2\u7ecf\u63d0\u51fa\u4e86\u5404\u79cd\u65b9\u6cd5\u6765\u589e\u5f3a LLM \u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u8981\u4e48\u4ea7\u751f\u5927\u91cf\u7684\u540e\u8bad\u7ec3\u6210\u672c\uff0c\u8981\u4e48\u9700\u8981\u989d\u5916\u7684\u5de5\u5177\u6a21\u5757\uff08\u4f8b\u5982\uff0cRAG\uff09\uff0c\u8981\u4e48\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u6ca1\u6709\u663e\u793a\u51fa\u663e\u7740\u7684\u6539\u8fdb\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u89c2\u5bdf\u4e86\u6bcf\u5c42\u6ce8\u610f\u529b\u5206\u5e03\u548c\u751f\u6210\u7b54\u6848\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5efa\u7acb\u4e86\u6ce8\u610f\u529b\u5206\u914d\u4e0e\u68c0\u7d22\u589e\u5f3a\u80fd\u529b\u4fdd\u6301\u4e00\u81f4\u3002\u6839\u636e\u4e0a\u8ff0\u89c1\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5 InfiniRetri\uff0c\u8be5\u65b9\u6cd5\u5229\u7528 LLM \u81ea\u8eab\u7684\u6ce8\u610f\u529b\u4fe1\u606f\u6765\u5b9e\u73b0\u5bf9\u65e0\u9650\u957f\u5ea6\u8f93\u5165\u7684\u51c6\u786e\u68c0\u7d22\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0cInfiniRetri \u5728\u4f7f\u7528 0.5B \u53c2\u6570\u6a21\u578b\u5bf9\u8d85\u8fc7 100 \u4e07\u4e2a\u6807\u8bb0\u7684\u9488\u5934\u5e72\u8349\u5806 (NIH) \u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86 100% \u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u65b9\u6cd5\u6216\u66f4\u5927\u7684\u6a21\u578b\uff0c\u5e76\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb (SOTA)\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5b9e\u9645\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6700\u5927\u63d0\u5347\u4e86 288%\u3002\u6b64\u5916\uff0cInfiniRetri \u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u57fa\u4e8e Transformer \u7684 LLM\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\uff0c\u5e76\u4e14\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u548c\u957f\u6587\u672c\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u603b\u4e4b\uff0c\u6211\u4eec\u7684\u7efc\u5408\u7814\u7a76\u8868\u660e\u4e86 InfiniRetri \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u4f7f\u7528 LLM \u81ea\u8eab\u80fd\u529b\u5728\u65e0\u9650\u957f\u5ea6\u6807\u8bb0\u4e0b\u68c0\u7d22\u4fe1\u606f\u521b\u9020\u4e86\u4e00\u4e2a\u8303\u4f8b\u3002\u4ee3\u7801\u5c06\u5728\u94fe\u63a5\u4e2d\u53d1\u5e03\u3002", "author": "Xiaoju Ye et.al.", "authors": "Xiaoju Ye, Zhichun Wang, Jingyuan Wang", "id": "2502.12962v1", "paper_url": "http://arxiv.org/abs/2502.12962v1", "repo": "null"}}