{"2412.13877": {"publish_time": "2024-12-18", "title": "RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation", "paper_summary": "Developing robust and general-purpose robotic manipulation policies is a key\ngoal in the field of robotics. To achieve effective generalization, it is\nessential to construct comprehensive datasets that encompass a large number of\ndemonstration trajectories and diverse tasks. Unlike vision or language data\nthat can be collected from the Internet, robotic datasets require detailed\nobservations and manipulation actions, necessitating significant investment in\nhardware-software infrastructure and human labor. While existing works have\nfocused on assembling various individual robot datasets, there remains a lack\nof a unified data collection standard and insufficient diversity in tasks,\nscenarios, and robot types. In this paper, we introduce RoboMIND\n(Multi-embodiment Intelligence Normative Data for Robot manipulation),\nfeaturing 55k real-world demonstration trajectories across 279 diverse tasks\ninvolving 61 different object classes. RoboMIND is collected through human\nteleoperation and encompasses comprehensive robotic-related information,\nincluding multi-view RGB-D images, proprioceptive robot state information, end\neffector details, and linguistic task descriptions. To ensure dataset\nconsistency and reliability during policy learning, RoboMIND is built on a\nunified data collection platform and standardized protocol, covering four\ndistinct robotic embodiments. We provide a thorough quantitative and\nqualitative analysis of RoboMIND across multiple dimensions, offering detailed\ninsights into the diversity of our datasets. In our experiments, we conduct\nextensive real-world testing with four state-of-the-art imitation learning\nmethods, demonstrating that training with RoboMIND data results in a high\nmanipulation success rate and strong generalization. Our project is at\nhttps://x-humanoid-robomind.github.io/.", "paper_summary_zh": "<paragraph>\u958b\u767c\u5065\u5168\u4e14\u901a\u7528\u7684\u6a5f\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u662f\u6a5f\u5668\u4eba\u9818\u57df\u7684\u4e00\u9805\u95dc\u9375\u76ee\u6a19\u3002\u70ba\u4e86\u9054\u5230\u6709\u6548\u7684\u6982\u5316\uff0c\u5efa\u69cb\u5305\u542b\u5927\u91cf\u793a\u7bc4\u8ecc\u8de1\u548c\u591a\u6a23\u4efb\u52d9\u7684\u5168\u9762\u8cc7\u6599\u96c6\u81f3\u95dc\u91cd\u8981\u3002\u8207\u53ef\u4ee5\u5f9e\u7db2\u969b\u7db2\u8def\u4e0a\u6536\u96c6\u7684\u8996\u89ba\u6216\u8a9e\u8a00\u8cc7\u6599\u4e0d\u540c\uff0c\u6a5f\u5668\u4eba\u8cc7\u6599\u96c6\u9700\u8981\u8a73\u7d30\u7684\u89c0\u5bdf\u548c\u64cd\u4f5c\u52d5\u4f5c\uff0c\u9019\u9700\u8981\u5c0d\u786c\u9ad4\u8edf\u9ad4\u57fa\u790e\u8a2d\u65bd\u548c\u4eba\u529b\u9032\u884c\u5927\u91cf\u6295\u8cc7\u3002\u5118\u7ba1\u73fe\u6709\u4f5c\u54c1\u5c08\u6ce8\u65bc\u7d44\u88dd\u5404\u7a2e\u500b\u5225\u6a5f\u5668\u4eba\u8cc7\u6599\u96c6\uff0c\u4f46\u4ecd\u7136\u7f3a\u4e4f\u7d71\u4e00\u7684\u8cc7\u6599\u6536\u96c6\u6a19\u6e96\uff0c\u4e26\u4e14\u4efb\u52d9\u3001\u5834\u666f\u548c\u6a5f\u5668\u4eba\u985e\u578b\u7f3a\u4e4f\u591a\u6a23\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 RoboMIND\uff08\u6a5f\u5668\u4eba\u64cd\u4f5c\u7684\u591a\u5177\u73fe\u667a\u80fd\u898f\u7bc4\u8cc7\u6599\uff09\uff0c\u5b83\u5305\u542b 61 \u500b\u4e0d\u540c\u7269\u9ad4\u985e\u5225\u7684 279 \u500b\u591a\u6a23\u5316\u4efb\u52d9\u4e2d 55k \u500b\u771f\u5be6\u4e16\u754c\u7684\u793a\u7bc4\u8ecc\u8de1\u3002RoboMIND \u662f\u900f\u904e\u4eba\u985e\u9060\u7aef\u64cd\u4f5c\u6536\u96c6\u7684\uff0c\u4e26\u5305\u542b\u5168\u9762\u7684\u6a5f\u5668\u4eba\u76f8\u95dc\u8cc7\u8a0a\uff0c\u5305\u62ec\u591a\u8996\u89d2 RGB-D \u5f71\u50cf\u3001\u672c\u9ad4\u611f\u89ba\u6a5f\u5668\u4eba\u72c0\u614b\u8cc7\u8a0a\u3001\u672b\u7aef\u57f7\u884c\u5668\u8a73\u7d30\u8cc7\u8a0a\u548c\u8a9e\u8a00\u4efb\u52d9\u63cf\u8ff0\u3002\u70ba\u4e86\u78ba\u4fdd\u5728\u7b56\u7565\u5b78\u7fd2\u671f\u9593\u8cc7\u6599\u96c6\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\uff0cRoboMIND \u5efa\u69cb\u5728\u7d71\u4e00\u7684\u8cc7\u6599\u6536\u96c6\u5e73\u53f0\u548c\u6a19\u6e96\u5316\u5354\u5b9a\u4e0a\uff0c\u6db5\u84cb\u56db\u7a2e\u4e0d\u540c\u7684\u6a5f\u5668\u4eba\u5177\u73fe\u3002\u6211\u5011\u63d0\u4f9b\u4e86 RoboMIND \u5728\u591a\u500b\u7dad\u5ea6\u4e0a\u7684\u5168\u9762\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\uff0c\u5c0d\u6211\u5011\u8cc7\u6599\u96c6\u7684\u591a\u6a23\u6027\u63d0\u4f9b\u4e86\u8a73\u7d30\u7684\u898b\u89e3\u3002\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u56db\u7a2e\u6700\u5148\u9032\u7684\u6a21\u4eff\u5b78\u7fd2\u65b9\u6cd5\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u771f\u5be6\u4e16\u754c\u6e2c\u8a66\uff0c\u8b49\u660e\u4f7f\u7528 RoboMIND \u8cc7\u6599\u9032\u884c\u8a13\u7df4\u6703\u7522\u751f\u5f88\u9ad8\u7684\u64cd\u4f5c\u6210\u529f\u7387\u548c\u5f37\u5927\u7684\u6982\u5316\u80fd\u529b\u3002\u6211\u5011\u7684\u5c08\u6848\u7db2\u5740\u70ba https://x-humanoid-robomind.github.io/\u3002</paragraph>", "author": "Kun Wu et.al.", "authors": "Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, Zhen Zhao, Guangyu Li, Zhao Jin, Lecheng Wang, Jilei Mao, Xinhua Wang, Shichao Fan, Ning Liu, Pei Ren, Qiang Zhang, Yaoxu Lyu, Mengzhen Liu, Jingyang He, Yulin Luo, Zeyu Gao, Chenxuan Li, Chenyang Gu, Yankai Fu, Di Wu, Xingyu Wang, Sixiang Chen, Zhenyu Wang, Pengju An, Siyuan Qian, Shanghang Zhang, Jian Tang", "id": "2412.13877v1", "paper_url": "http://arxiv.org/abs/2412.13877v1", "repo": "null"}}