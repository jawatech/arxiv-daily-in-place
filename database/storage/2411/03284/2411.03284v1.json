{"2411.03284": {"publish_time": "2024-11-05", "title": "SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents", "paper_summary": "While multi-agent systems have been shown to significantly enhance the\nperformance of Large Language Models (LLMs) across various tasks and\napplications, the dense interaction between scaling agents potentially hampers\ntheir efficiency and diversity. To address these challenges, we draw\ninspiration from the sparse mixture-of-agents (SMoE) and propose a sparse\nmixture-of-agents (SMoA) framework to improve the efficiency and diversity of\nmulti-agent LLMs. Unlike completely connected structures, SMoA introduces novel\nResponse Selection and Early Stopping mechanisms to sparsify information flows\namong individual LLM agents, striking a balance between performance and\nefficiency. Additionally, inspired by the expert diversity principle in SMoE\nframeworks for workload balance between experts, we assign distinct role\ndescriptions to each LLM agent, fostering diverse and divergent thinking.\nExtensive experiments on reasoning, alignment, and fairness benchmarks\ndemonstrate that SMoA achieves performance comparable to traditional\nmixture-of-agents approaches but with significantly lower computational costs.\nFurther analysis reveals that SMoA is more stable, has a greater capacity to\nscale, and offers considerable potential through hyper-parameter optimization.\nCode and data will be available at: https://github.com/David-Li0406/SMoA.", "paper_summary_zh": "\u5118\u7ba1\u591a\u667a\u80fd\u9ad4\u7cfb\u7d71\u5df2\u88ab\u8b49\u660e\u80fd\u986f\u8457\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u548c\u61c9\u7528\u4e2d\u7684\u6548\u80fd\uff0c\u4f46\u64f4\u5145\u667a\u80fd\u9ad4\u4e4b\u9593\u7684\u5bc6\u96c6\u4e92\u52d5\u6f5b\u5728\u6703\u963b\u7919\u5176\u6548\u7387\u548c\u591a\u6a23\u6027\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f9e\u7a00\u758f\u6df7\u5408\u667a\u80fd\u9ad4 (SMoE) \u6c72\u53d6\u9748\u611f\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u7a00\u758f\u6df7\u5408\u667a\u80fd\u9ad4 (SMoA) \u67b6\u69cb\u4f86\u63d0\u5347\u591a\u667a\u80fd\u9ad4 LLM \u7684\u6548\u7387\u548c\u591a\u6a23\u6027\u3002\u8207\u5b8c\u5168\u9023\u63a5\u7684\u7d50\u69cb\u4e0d\u540c\uff0cSMoA \u5f15\u9032\u4e86\u65b0\u7a4e\u7684\u56de\u61c9\u9078\u64c7\u548c\u63d0\u524d\u505c\u6b62\u6a5f\u5236\uff0c\u4ee5\u7a00\u758f\u5316\u500b\u5225 LLM \u667a\u80fd\u9ad4\u4e4b\u9593\u7684\u8cc7\u8a0a\u6d41\uff0c\u5728\u6548\u80fd\u548c\u6548\u7387\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002\u6b64\u5916\uff0c\u5728 SMoE \u67b6\u69cb\u4e2d\u53d7\u5230\u5c08\u5bb6\u591a\u6a23\u6027\u539f\u5247\u7684\u555f\u767c\uff0c\u4ee5\u5e73\u8861\u5c08\u5bb6\u4e4b\u9593\u7684\u5de5\u4f5c\u8ca0\u8f09\uff0c\u6211\u5011\u70ba\u6bcf\u500b LLM \u667a\u80fd\u9ad4\u5206\u914d\u4e86\u4e0d\u540c\u7684\u89d2\u8272\u63cf\u8ff0\uff0c\u4ee5\u4fc3\u9032\u591a\u6a23\u5316\u548c\u767c\u6563\u6027\u601d\u8003\u3002\u5728\u63a8\u7406\u3001\u5c0d\u9f4a\u548c\u516c\u5e73\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0cSMoA \u9054\u5230\u4e86\u8207\u50b3\u7d71\u6df7\u5408\u667a\u80fd\u9ad4\u65b9\u6cd5\u76f8\u7576\u7684\u6548\u80fd\uff0c\u4f46\u8a08\u7b97\u6210\u672c\u537b\u986f\u8457\u964d\u4f4e\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u986f\u793a\uff0cSMoA \u66f4\u52a0\u7a69\u5b9a\uff0c\u5177\u6709\u66f4\u5927\u7684\u64f4\u5145\u80fd\u529b\uff0c\u4e26\u900f\u904e\u8d85\u53c3\u6578\u6700\u4f73\u5316\u63d0\u4f9b\u4e86\u53ef\u89c0\u7684\u6f5b\u529b\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u5c07\u65bc\u4ee5\u4e0b\u7db2\u5740\u63d0\u4f9b\uff1ahttps://github.com/David-Li0406/SMoA\u3002", "author": "Dawei Li et.al.", "authors": "Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, Jiayi Shen", "id": "2411.03284v1", "paper_url": "http://arxiv.org/abs/2411.03284v1", "repo": "https://github.com/david-li0406/smoa"}}