{"2405.03435": {"publish_time": "2024-05-06", "title": "A method for quantifying the generalization capabilities of generative models for solving Ising models", "paper_summary": "For Ising models with complex energy landscapes, whether the ground state can\nbe found by neural networks depends heavily on the Hamming distance between the\ntraining datasets and the ground state. Despite the fact that various recently\nproposed generative models have shown good performance in solving Ising models,\nthere is no adequate discussion on how to quantify their generalization\ncapabilities. Here we design a Hamming distance regularizer in the framework of\na class of generative models, variational autoregressive networks (VAN), to\nquantify the generalization capabilities of various network architectures\ncombined with VAN. The regularizer can control the size of the overlaps between\nthe ground state and the training datasets generated by networks, which,\ntogether with the success rates of finding the ground state, form a\nquantitative metric to quantify their generalization capabilities. We conduct\nnumerical experiments on several prototypical network architectures combined\nwith VAN, including feed-forward neural networks, recurrent neural networks,\nand graph neural networks, to quantify their generalization capabilities when\nsolving Ising models. Moreover, considering the fact that the quantification of\nthe generalization capabilities of networks on small-scale problems can be used\nto predict their relative performance on large-scale problems, our method is of\ngreat significance for assisting in the Neural Architecture Search field of\nsearching for the optimal network architectures when solving large-scale Ising\nmodels.", "paper_summary_zh": "", "author": "Qunlong Ma et.al.", "authors": "Qunlong Ma,Zhi Ma,Ming Gao", "id": "2405.03435v1", "paper_url": "http://arxiv.org/abs/2405.03435v1", "repo": "null"}}