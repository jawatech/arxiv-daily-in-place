{"2412.04447": {"publish_time": "2024-12-05", "title": "EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios", "paper_summary": "The advent of Multimodal Large Language Models, leveraging the power of Large\nLanguage Models, has recently demonstrated superior multimodal understanding\nand reasoning abilities, heralding a new era for artificial general\nintelligence. However, achieving AGI necessitates more than just comprehension\nand reasoning. A crucial capability required is effective planning in diverse\nscenarios, which involves making reasonable decisions based on complex\nenvironments to solve real-world problems. Despite its importance, the planning\nabilities of current MLLMs in varied scenarios remain underexplored. In this\npaper, we introduce EgoPlan-Bench2, a rigorous and comprehensive benchmark\ndesigned to assess the planning capabilities of MLLMs across a wide range of\nreal-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4\nmajor domains and 24 detailed scenarios, closely aligned with human daily life.\nEgoPlan-Bench2 is constructed through a semi-automatic process utilizing\negocentric videos, complemented by manual verification. Grounded in a\nfirst-person perspective, it mirrors the way humans approach problem-solving in\neveryday life. We evaluate 21 competitive MLLMs and provide an in-depth\nanalysis of their limitations, revealing that they face significant challenges\nin real-world planning. To further improve the planning proficiency of current\nMLLMs, we propose a training-free approach using multimodal Chain-of-Thought\n(CoT) prompting through investigating the effectiveness of various multimodal\nprompts in complex planning. Our approach enhances the performance of GPT-4V by\n10.24 on EgoPlan-Bench2 without additional training. Our work not only sheds\nlight on the current limitations of MLLMs in planning, but also provides\ninsights for future enhancements in this critical area. We have made data and\ncode available at https://qiulu66.github.io/egoplanbench2/.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u5229\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u529b\u91cf\uff0c\u6700\u8fd1\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u9884\u793a\u7740\u4eba\u5de5\u667a\u80fd\u65b0\u65f6\u4ee3\u7684\u5230\u6765\u3002\u7136\u800c\uff0c\u5b9e\u73b0 AGI \u4e0d\u4ec5\u4ec5\u9700\u8981\u7406\u89e3\u548c\u63a8\u7406\u3002\u4e00\u4e2a\u81f3\u5173\u91cd\u8981\u7684\u80fd\u529b\u662f\u6709\u6548\u89c4\u5212\u5404\u79cd\u573a\u666f\uff0c\u8fd9\u6d89\u53ca\u5728\u590d\u6742\u73af\u5883\u4e2d\u505a\u51fa\u5408\u7406\u7684\u51b3\u7b56\u6765\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u5176\u91cd\u8981\u6027\uff0c\u5f53\u524d MLLM \u5728\u5404\u79cd\u573a\u666f\u4e2d\u7684\u89c4\u5212\u80fd\u529b\u4ecd\u7136\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 EgoPlan-Bench2\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e25\u683c\u4e14\u5168\u9762\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30 MLLM \u5728\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u89c4\u5212\u80fd\u529b\u3002EgoPlan-Bench2 \u6db5\u76d6\u4e86\u8de8\u8d8a 4 \u4e2a\u4e3b\u8981\u9886\u57df\u548c 24 \u4e2a\u8be6\u7ec6\u573a\u666f\u7684\u65e5\u5e38\u4efb\u52a1\uff0c\u4e0e\u4eba\u7c7b\u65e5\u5e38\u751f\u6d3b\u7d27\u5bc6\u76f8\u5173\u3002EgoPlan-Bench2 \u662f\u901a\u8fc7\u5229\u7528\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7684\u534a\u81ea\u52a8\u8fc7\u7a0b\u6784\u5efa\u7684\uff0c\u5e76\u8f85\u4ee5\u4eba\u5de5\u9a8c\u8bc1\u3002\u57fa\u4e8e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff0c\u5b83\u53cd\u6620\u4e86\u4eba\u7c7b\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u89e3\u51b3\u95ee\u9898\u7684\u65b9\u5f0f\u3002\u6211\u4eec\u8bc4\u4f30\u4e86 21 \u4e2a\u6709\u7ade\u4e89\u529b\u7684 MLLM\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u5b83\u4eec\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u89c4\u5212\u4e2d\u9762\u4e34\u7740\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5f53\u524d MLLM \u7684\u89c4\u5212\u80fd\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u591a\u6a21\u6001\u601d\u60f3\u94fe (CoT) \u63d0\u793a\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u67e5\u5404\u79cd\u591a\u6a21\u6001\u63d0\u793a\u5728\u590d\u6742\u89c4\u5212\u4e2d\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001 Chain-of-Thought (CoT) \u63d0\u793a\u5c06 GPT-4V \u7684\u6027\u80fd\u63d0\u9ad8\u4e86 10.24\uff0c\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u4e0d\u4ec5\u63ed\u793a\u4e86 MLLM \u5728\u89c4\u5212\u65b9\u9762\u7684\u5f53\u524d\u5c40\u9650\u6027\uff0c\u8fd8\u4e3a\u8fd9\u4e00\u5173\u952e\u9886\u57df\u7684\u672a\u6765\u589e\u5f3a\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002\u6211\u4eec\u5df2\u5728 https://qiulu66.github.io/egoplanbench2/ \u63d0\u4f9b\u6570\u636e\u548c\u4ee3\u7801\u3002", "author": "Lu Qiu et.al.", "authors": "Lu Qiu, Yuying Ge, Yi Chen, Yixiao Ge, Ying Shan, Xihui Liu", "id": "2412.04447v1", "paper_url": "http://arxiv.org/abs/2412.04447v1", "repo": "null"}}