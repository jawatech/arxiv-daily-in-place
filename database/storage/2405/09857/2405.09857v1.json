{"2405.09857": {"publish_time": "2024-05-16", "title": "IGOT: Information Gain Optimized Tokenizer on Domain Adaptive Pretraining", "paper_summary": "Pretrained Large Language Models (LLM) such as ChatGPT, Claude, etc. have\ndemonstrated strong capabilities in various fields of natural language\ngeneration. However, there are still many problems when using LLM in\nspecialized domain-specific fields. When using generative AI to process\ndownstream tasks, a common approach is to add new knowledge (e.g., private\ndomain knowledge, cutting-edge information) to a pretrained model through\ncontinued training or fine-tuning. However, whether there is a universal\nparadigm for domain adaptation training is still an open question. In this\narticle, we proposed Information Gain Optimized Tokenizer (IGOT), which\nanalyzes the special token set of downstream tasks, constructs a new subset\nusing heuristic function $\\phi$ with the special token and its information\ngain, to build new domain-specific tokenizer, and continues pretraining on the\ndownstream task data. We explored the many positive effects of this method's\ncustomized tokenizer on domain-adaptive pretraining and verified this method\ncan perform better than the ordinary method of just collecting data and\nfine-tuning. Based on our experiment, the continued pretraining process of IGOT\nwith LLaMA-7B achieved 11.9\\% token saving, 12.2\\% training time saving, and\n5.8\\% maximum GPU VRAM usage saving, combined with the T5 model, we can even\nreach a 31.5\\% of training time saving, making porting general generative AI to\nspecific domains more effective than before. In domain-specific tasks,\nsupervised $IGOT_\\tau$ shows great performance on reducing both the convergence\nradius and convergence point during keep pretraining.", "paper_summary_zh": "<paragraph>\u5927\u578b\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f8b\u5982 ChatGPT\u3001Claude \u7b49\uff0c\u5df2\u5728\u81ea\u7136\u8a9e\u8a00\u751f\u6210\u7684\u5404\u500b\u9818\u57df\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u7279\u5b9a\u9818\u57df\u4e2d\u4f7f\u7528 LLM \u6642\uff0c\u4ecd\u5b58\u5728\u8a31\u591a\u554f\u984c\u3002\u4f7f\u7528\u751f\u6210\u5f0f AI \u8655\u7406\u4e0b\u6e38\u4efb\u52d9\u6642\uff0c\u4e00\u500b\u5e38\u898b\u7684\u65b9\u6cd5\u662f\u900f\u904e\u6301\u7e8c\u8a13\u7df4\u6216\u5fae\u8abf\uff0c\u5c07\u65b0\u77e5\u8b58\uff08\u4f8b\u5982\u79c1\u4eba\u9818\u57df\u77e5\u8b58\u3001\u524d\u6cbf\u8cc7\u8a0a\uff09\u65b0\u589e\u5230\u9810\u8a13\u7df4\u6a21\u578b\u4e2d\u3002\u7136\u800c\uff0c\u662f\u5426\u6709\u4e00\u500b\u901a\u7528\u7bc4\u4f8b\u9069\u7528\u65bc\u9818\u57df\u9069\u61c9\u8a13\u7df4\uff0c\u4ecd\u662f\u4e00\u500b\u958b\u653e\u6027\u7684\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u8cc7\u8a0a\u589e\u76ca\u6700\u4f73\u5316\u6a19\u8a18\u5668 (IGOT)\uff0c\u5b83\u5206\u6790\u4e0b\u6e38\u4efb\u52d9\u7684\u7279\u6b8a\u6a19\u8a18\u96c6\uff0c\u4f7f\u7528\u555f\u767c\u5f0f\u51fd\u6578 $\\phi$ \u548c\u7279\u6b8a\u6a19\u8a18\u53ca\u5176\u8cc7\u8a0a\u589e\u76ca\u5efa\u69cb\u4e00\u500b\u65b0\u7684\u5b50\u96c6\uff0c\u4ee5\u5efa\u69cb\u65b0\u7684\u7279\u5b9a\u9818\u57df\u6a19\u8a18\u5668\uff0c\u4e26\u7e7c\u7e8c\u5728\u4e0b\u6e38\u4efb\u52d9\u8cc7\u6599\u4e0a\u9032\u884c\u9810\u8a13\u7df4\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u9019\u7a2e\u65b9\u6cd5\u7684\u5ba2\u88fd\u5316\u6a19\u8a18\u5668\u5c0d\u9818\u57df\u9069\u61c9\u9810\u8a13\u7df4\u7684\u8a31\u591a\u6b63\u9762\u5f71\u97ff\uff0c\u4e26\u9a57\u8b49\u6b64\u65b9\u6cd5\u7684\u8868\u73fe\u512a\u65bc\u50c5\u6536\u96c6\u8cc7\u6599\u548c\u5fae\u8abf\u7684\u50b3\u7d71\u65b9\u6cd5\u3002\u6839\u64da\u6211\u5011\u7684\u5be6\u9a57\uff0cIGOT \u8207 LLaMA-7B \u7684\u6301\u7e8c\u9810\u8a13\u7df4\u904e\u7a0b\u7bc0\u7701\u4e86 11.9% \u7684\u6a19\u8a18\u300112.2% \u7684\u8a13\u7df4\u6642\u9593\uff0c\u4ee5\u53ca 5.8% \u7684\u6700\u5927 GPU VRAM \u4f7f\u7528\u91cf\uff0c\u7d50\u5408 T5 \u6a21\u578b\uff0c\u6211\u5011\u751a\u81f3\u53ef\u4ee5\u7bc0\u7701 31.5% \u7684\u8a13\u7df4\u6642\u9593\uff0c\u4f7f\u5c07\u901a\u7528\u751f\u6210\u5f0f AI \u79fb\u690d\u5230\u7279\u5b9a\u9818\u57df\u6bd4\u4ee5\u524d\u66f4\u6709\u6548\u3002\u5728\u7279\u5b9a\u9818\u57df\u4efb\u52d9\u4e2d\uff0c\u76e3\u7763\u5f0f $IGOT_\\tau$ \u5728\u6301\u7e8c\u9810\u8a13\u7df4\u671f\u9593\u5c55\u73fe\u51fa\u6975\u4f73\u7684\u6548\u80fd\uff0c\u53ef\u6e1b\u5c11\u6536\u6582\u534a\u5f91\u548c\u6536\u6582\u9ede\u3002</paragraph>", "author": "Dawei Feng et.al.", "authors": "Dawei Feng, Yihai Zhang, Zhixuan Xu", "id": "2405.09857v1", "paper_url": "http://arxiv.org/abs/2405.09857v1", "repo": "null"}}