{"2501.13467": {"publish_time": "2025-01-23", "title": "Multi-Level Attention and Contrastive Learning for Enhanced Text Classification with an Optimized Transformer", "paper_summary": "This paper studies a text classification algorithm based on an improved\nTransformer to improve the performance and efficiency of the model in text\nclassification tasks. Aiming at the shortcomings of the traditional Transformer\nmodel in capturing deep semantic relationships and optimizing computational\ncomplexity, this paper introduces a multi-level attention mechanism and a\ncontrastive learning strategy. The multi-level attention mechanism effectively\nmodels the global semantics and local features in the text by combining global\nattention with local attention; the contrastive learning strategy enhances the\nmodel's ability to distinguish between different categories by constructing\npositive and negative sample pairs while improving the classification effect.\nIn addition, in order to improve the training and inference efficiency of the\nmodel on large-scale text data, this paper designs a lightweight module to\noptimize the feature transformation process and reduce the computational cost.\nExperimental results on the dataset show that the improved Transformer model\noutperforms the comparative models such as BiLSTM, CNN, standard Transformer,\nand BERT in terms of classification accuracy, F1 score, and recall rate,\nshowing stronger semantic representation ability and generalization\nperformance. The method proposed in this paper provides a new idea for\nalgorithm optimization in the field of text classification and has good\napplication potential and practical value. Future work will focus on studying\nthe performance of this model in multi-category imbalanced datasets and\ncross-domain tasks and explore the integration wi", "paper_summary_zh": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb Transformer \u7684\u6587\u672c\u5206\u7c7b\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002\u9488\u5bf9\u4f20\u7edf Transformer \u6a21\u578b\u5728\u6355\u6349\u6df1\u5ea6\u8bed\u4e49\u5173\u7cfb\u548c\u4f18\u5316\u8ba1\u7b97\u590d\u6742\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u7ea7\u6ce8\u610f\u529b\u673a\u5236\u548c\u4e00\u79cd\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u3002\u591a\u7ea7\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u5c06\u5168\u5c40\u6ce8\u610f\u529b\u4e0e\u5c40\u90e8\u6ce8\u610f\u529b\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u5730\u5bf9\u6587\u672c\u4e2d\u7684\u5168\u5c40\u8bed\u4e49\u548c\u5c40\u90e8\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\uff1b\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u901a\u8fc7\u6784\u5efa\u6b63\u8d1f\u6837\u672c\u5bf9\uff0c\u5728\u63d0\u9ad8\u5206\u7c7b\u6548\u679c\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u533a\u5206\u4e0d\u540c\u7c7b\u522b\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u4ee5\u4f18\u5316\u7279\u5f81\u8f6c\u6362\u8fc7\u7a0b\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u5728\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6539\u8fdb\u540e\u7684 Transformer \u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u7387\u3001F1 \u5f97\u5206\u548c\u53ec\u56de\u7387\u65b9\u9762\u4f18\u4e8e BiLSTM\u3001CNN\u3001\u6807\u51c6 Transformer \u548c BERT \u7b49\u5bf9\u6bd4\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8bed\u4e49\u8868\u793a\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u6587\u672c\u5206\u7c7b\u9886\u57df\u4e2d\u7684\u7b97\u6cd5\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u6f5c\u529b\u548c\u5b9e\u7528\u4ef7\u503c\u3002\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u91cd\u70b9\u7814\u7a76\u8be5\u6a21\u578b\u5728\u591a\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u548c\u8de8\u57df\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u4e0e", "author": "Jia Gao et.al.", "authors": "Jia Gao, Guiran Liu, Binrong Zhu, Shicheng Zhou, Hongye Zheng, Xiaoxuan Liao", "id": "2501.13467v1", "paper_url": "http://arxiv.org/abs/2501.13467v1", "repo": "null"}}