{"2407.06023": {"publish_time": "2024-07-08", "title": "Distilling System 2 into System 1", "paper_summary": "Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u82b1\u8cbb\u984d\u5916\u7684\u904b\u7b97\u4f86\u7522\u751f\u4e2d\u9593\u60f3\u6cd5\uff0c\u9019\u6709\u52a9\u65bc\u7522\u751f\u66f4\u597d\u7684\u6700\u7d42\u56de\u61c9\u3002\u81ea\u5f9e\u6709\u4e86\u601d\u60f3\u93c8 (Wei \u7b49\u4eba\uff0c2022 \u5e74) \u4ee5\u4f86\uff0c\u8a31\u591a\u6b64\u985e\u7cfb\u7d71 2 \u6280\u8853\u5df2\u88ab\u63d0\u51fa\uff0c\u4f8b\u5982\u6539\u8ff0\u548c\u56de\u61c9 (Deng \u7b49\u4eba\uff0c2023a)\u3001\u7cfb\u7d71 2 \u6ce8\u610f\u529b (Weston \u548c Sukhbaatar\uff0c2023 \u5e74) \u548c\u5206\u652f\u6c42\u89e3\u5408\u4f75 (Saha \u7b49\u4eba\uff0c2023 \u5e74)\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86\u81ea\u76e3\u7763\u65b9\u6cd5\uff0c\u7528\u65bc\u300c\u7de8\u8b6f\u300d(\u63d0\u7149) \u4f86\u81ea\u7cfb\u7d71 2 \u6280\u8853\u7684\u9ad8\u54c1\u8cea\u8f38\u51fa\uff0c\u4e26\u5c07\u5176\u8fd4\u56de LLM \u751f\u6210\uff0c\u800c\u7121\u9700\u4e2d\u9593\u63a8\u7406\u6a19\u8a18\u5e8f\u5217\uff0c\u56e0\u70ba\u9019\u7a2e\u63a8\u7406\u5df2\u63d0\u7149\u5230\u7cfb\u7d71 1 \u4e2d\u3002\u6211\u5011\u5c55\u793a\u4e86\u5e7e\u7a2e\u6b64\u985e\u6280\u8853\u53ef\u4ee5\u6210\u529f\u5730\u9032\u884c\u63d0\u7149\uff0c\u8207\u539f\u59cb\u7cfb\u7d71 1 \u6027\u80fd\u76f8\u6bd4\uff0c\u6539\u9032\u4e86\u7d50\u679c\uff0c\u800c\u4e14\u63a8\u7406\u6210\u672c\u4f4e\u65bc\u7cfb\u7d71 2\u3002\u6211\u5011\u8a8d\u70ba\uff0c\u9019\u7a2e\u7cfb\u7d71 2 \u63d0\u7149\u5c07\u6210\u70ba\u672a\u4f86\u6301\u7e8c\u5b78\u7fd2 AI \u7cfb\u7d71\u7684\u4e00\u9805\u91cd\u8981\u529f\u80fd\uff0c\u4f7f\u5b83\u5011\u80fd\u5920\u5c07\u7cfb\u7d71 2 \u80fd\u529b\u96c6\u4e2d\u65bc\u5b83\u5011\u76ee\u524d\u9084\u7121\u6cd5\u5f88\u597d\u57f7\u884c\u7684\u63a8\u7406\u4efb\u52d9\u4e0a\u3002", "author": "Ping Yu et.al.", "authors": "Ping Yu, Jing Xu, Jason Weston, Ilia Kulikov", "id": "2407.06023v1", "paper_url": "http://arxiv.org/abs/2407.06023v1", "repo": "null"}}