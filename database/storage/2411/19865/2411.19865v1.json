{"2411.19865": {"publish_time": "2024-11-29", "title": "Reverse Thinking Makes LLMs Stronger Reasoners", "paper_summary": "Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets.", "paper_summary_zh": "\u9006\u5411\u601d\u8003\u5728\u4eba\u985e\u63a8\u7406\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u4eba\u985e\u4e0d\u50c5\u80fd\u5f9e\u554f\u984c\u63a8\u8ad6\u51fa\u89e3\u7b54\uff0c\u4e5f\u80fd\u53cd\u5411\u63a8\u7406\uff0c\u5373\u5f9e\u89e3\u7b54\u51fa\u767c\uff0c\u63a8\u8ad6\u51fa\u554f\u984c\u3002\u9019\u5e38\u5e38\u80fd\u63d0\u5347\u6574\u9ad4\u63a8\u7406\u8868\u73fe\uff0c\u56e0\u70ba\u5b83\u80fd\u8b93\u6b63\u5411\u8207\u53cd\u5411\u601d\u8003\u4e4b\u9593\u9032\u884c\u4e00\u81f4\u6027\u6aa2\u67e5\u3002\u70ba\u4e86\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u9032\u884c\u53cd\u5411\u601d\u8003\uff0c\u6211\u5011\u5f15\u5165\u4e86\u53cd\u5411\u589e\u5f37\u601d\u8003\uff08RevThink\uff09\uff0c\u4e00\u500b\u7531\u8cc7\u6599\u64f4\u5145\u8207\u5b78\u7fd2\u76ee\u6a19\u7d44\u6210\u7684\u67b6\u69cb\u3002\u5728 RevThink \u4e2d\uff0c\u6211\u5011\u900f\u904e\u5f9e\u6559\u5e2b\u6a21\u578b\u6536\u96c6\u7d50\u69cb\u5316\u7684\u6b63\u5411\u53cd\u5411\u63a8\u7406\u4f86\u64f4\u5145\u8cc7\u6599\u96c6\uff0c\u5305\u542b\uff1a(1) \u539f\u59cb\u554f\u984c\uff0c(2) \u6b63\u5411\u63a8\u7406\uff0c(3) \u53cd\u5411\u554f\u984c\uff0c\u4ee5\u53ca (4) \u53cd\u5411\u63a8\u7406\u3002\u63a5\u8457\u6211\u5011\u63a1\u7528\u4e09\u500b\u76ee\u6a19\uff0c\u4ee5\u591a\u4efb\u52d9\u5b78\u7fd2\u7684\u65b9\u5f0f\u8a13\u7df4\u8f03\u5c0f\u7684\u5b78\u751f\u6a21\u578b\uff1a(a) \u5f9e\u554f\u984c\u7522\u751f\u6b63\u5411\u63a8\u7406\uff0c(b) \u5f9e\u554f\u984c\u7522\u751f\u53cd\u5411\u554f\u984c\uff0c\u4ee5\u53ca (c) \u5f9e\u53cd\u5411\u554f\u984c\u7522\u751f\u53cd\u5411\u63a8\u7406\u3002\u6db5\u84cb\u5e38\u8b58\u3001\u6578\u5b78\u548c\u908f\u8f2f\u63a8\u7406\u7684 12 \u500b\u8cc7\u6599\u96c6\u7684\u5be6\u9a57\u986f\u793a\uff0c\u8207\u5b78\u751f\u6a21\u578b\u7684\u96f6\u6b21\u5b78\u7fd2\u8868\u73fe\u76f8\u6bd4\uff0c\u5e73\u5747\u63d0\u5347\u4e86 13.53%\uff0c\u8207\u6700\u5f37\u7684\u77e5\u8b58\u63d0\u7149\u57fa\u6e96\u76f8\u6bd4\uff0c\u63d0\u5347\u4e86 6.84%\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6a21\u578b\u5c55\u73fe\u51fa\u6a23\u672c\u6548\u7387\u2014\u2014\u50c5\u4f7f\u7528\u8a13\u7df4\u8cc7\u6599\u4e2d 10% \u7684\u6b63\u78ba\u6b63\u5411\u63a8\u7406\uff0c\u5c31\u512a\u65bc\u5728\u591a 10 \u500d\u6b63\u5411\u63a8\u7406\u4e0a\u8a13\u7df4\u7684\u6a19\u6e96\u5fae\u8abf\u65b9\u6cd5\u3002RevThink \u4e5f\u5c55\u73fe\u51fa\u5c0d\u5206\u5e03\u5916\u4fdd\u7559\u8cc7\u6599\u96c6\u7684\u5f37\u5927\u6cdb\u5316\u80fd\u529b\u3002", "author": "Justin Chih-Yao Chen et.al.", "authors": "Justin Chih-Yao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, Chen-Yu Lee, Tomas Pfister", "id": "2411.19865v1", "paper_url": "http://arxiv.org/abs/2411.19865v1", "repo": "null"}}