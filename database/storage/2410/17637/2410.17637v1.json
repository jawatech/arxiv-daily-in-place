{"2410.17637": {"publish_time": "2024-10-23", "title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models", "paper_summary": "Visual preference alignment involves training Large Vision-Language Models\n(LVLMs) to predict human preferences between visual inputs. This is typically\nachieved by using labeled datasets of chosen/rejected pairs and employing\noptimization algorithms like direct preference optimization (DPO). Existing\nvisual alignment methods, primarily designed for single-image scenarios,\nstruggle to effectively handle the complexity of multi-image tasks due to the\nscarcity of diverse training data and the high cost of annotating\nchosen/rejected pairs. We present Multi-Image Augmented Direct Preference\nOptimization (MIA-DPO), a visual preference alignment approach that effectively\nhandles multi-image inputs. MIA-DPO mitigates the scarcity of diverse\nmulti-image training data by extending single-image data with unrelated images\narranged in grid collages or pic-in-pic formats, significantly reducing the\ncosts associated with multi-image data annotations. Our observation reveals\nthat attention values of LVLMs vary considerably across different images. We\nuse attention values to identify and filter out rejected responses the model\nmay have mistakenly focused on. Our attention-aware selection for constructing\nthe chosen/rejected pairs without relying on (i) human annotation, (ii) extra\ndata, and (iii) external models or APIs. MIA-DPO is compatible with various\narchitectures and outperforms existing methods on five multi-image benchmarks,\nachieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the\nrecent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's\nability to understand single images.", "paper_summary_zh": "\u8996\u89ba\u504f\u597d\u5c0d\u9f4a\u6d89\u53ca\u8a13\u7df4\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u4ee5\u9810\u6e2c\u4eba\u985e\u5728\u8996\u89ba\u8f38\u5165\u4e4b\u9593\u7684\u504f\u597d\u3002\u9019\u901a\u5e38\u900f\u904e\u4f7f\u7528\u5df2\u9078\u64c7/\u5df2\u62d2\u7d55\u914d\u5c0d\u7684\u6a19\u7c64\u8cc7\u6599\u96c6\uff0c\u4e26\u63a1\u7528\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u7b49\u6700\u4f73\u5316\u6f14\u7b97\u6cd5\u4f86\u9054\u6210\u3002\u73fe\u6709\u7684\u8996\u89ba\u5c0d\u9f4a\u65b9\u6cd5\u4e3b\u8981\u8a2d\u8a08\u7528\u65bc\u55ae\u4e00\u5f71\u50cf\u5834\u666f\uff0c\u7531\u65bc\u7f3a\u4e4f\u591a\u6a23\u5316\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u4ee5\u53ca\u6a19\u8a3b\u5df2\u9078\u64c7/\u5df2\u62d2\u7d55\u914d\u5c0d\u7684\u9ad8\u6210\u672c\uff0c\u56e0\u6b64\u96e3\u4ee5\u6709\u6548\u8655\u7406\u591a\u91cd\u5f71\u50cf\u4efb\u52d9\u7684\u8907\u96dc\u6027\u3002\u6211\u5011\u63d0\u51fa\u591a\u91cd\u5f71\u50cf\u64f4\u5145\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (MIA-DPO)\uff0c\u9019\u662f\u4e00\u7a2e\u8996\u89ba\u504f\u597d\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u80fd\u5920\u6709\u6548\u8655\u7406\u591a\u91cd\u5f71\u50cf\u8f38\u5165\u3002MIA-DPO \u900f\u904e\u5c07\u55ae\u4e00\u5f71\u50cf\u8cc7\u6599\u64f4\u5145\u70ba\u6392\u5217\u6210\u7db2\u683c\u62fc\u8cbc\u6216\u756b\u4e2d\u756b\u683c\u5f0f\u7684\u7121\u95dc\u5f71\u50cf\uff0c\u4f86\u6e1b\u8f15\u591a\u6a23\u5316\u591a\u91cd\u5f71\u50cf\u8a13\u7df4\u8cc7\u6599\u7684\u7a00\u7f3a\u6027\uff0c\u5927\u5e45\u964d\u4f4e\u591a\u91cd\u5f71\u50cf\u8cc7\u6599\u6a19\u8a3b\u76f8\u95dc\u7684\u6210\u672c\u3002\u6211\u5011\u7684\u89c0\u5bdf\u986f\u793a\uff0cLVLMs \u7684\u6ce8\u610f\u529b\u503c\u5728\u4e0d\u540c\u5f71\u50cf\u4e4b\u9593\u6709\u986f\u8457\u5dee\u7570\u3002\u6211\u5011\u4f7f\u7528\u6ce8\u610f\u529b\u503c\u4f86\u8b58\u5225\u548c\u7be9\u9078\u51fa\u6a21\u578b\u53ef\u80fd\u932f\u8aa4\u95dc\u6ce8\u7684\u5df2\u62d2\u7d55\u56de\u61c9\u3002\u6211\u5011\u4f7f\u7528\u6ce8\u610f\u529b\u611f\u77e5\u9078\u64c7\u4f86\u5efa\u69cb\u5df2\u9078\u64c7/\u5df2\u62d2\u7d55\u914d\u5c0d\uff0c\u800c\u4e0d\u4f9d\u8cf4 (i) \u4eba\u5de5\u6a19\u8a3b\u3001(ii) \u984d\u5916\u8cc7\u6599\uff0c\u4ee5\u53ca (iii) \u5916\u90e8\u6a21\u578b\u6216 API\u3002MIA-DPO \u8207\u5404\u7a2e\u67b6\u69cb\u76f8\u5bb9\uff0c\u4e26\u4e14\u5728\u4e94\u500b\u591a\u91cd\u5f71\u50cf\u57fa\u6e96\u4e0a\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u5728 LLaVA-v1.5 \u4e0a\u9054\u5230\u5e73\u5747\u6548\u80fd\u63d0\u5347 3.0%\uff0c\u5728\u6700\u8fd1\u7684 InternLM-XC2.5 \u4e0a\u9054\u5230 4.3%\u3002\u6b64\u5916\uff0cMIA-DPO \u5c0d\u6a21\u578b\u7406\u89e3\u55ae\u4e00\u5f71\u50cf\u7684\u80fd\u529b\u5f71\u97ff\u751a\u5fae\u3002", "author": "Ziyu Liu et.al.", "authors": "Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He, Yuanjun Xiong, Dahua Lin, Jiaqi Wang", "id": "2410.17637v1", "paper_url": "http://arxiv.org/abs/2410.17637v1", "repo": "https://github.com/liuziyu77/mia-dpo"}}