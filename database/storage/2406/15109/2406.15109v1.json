{"2406.15109": {"publish_time": "2024-06-21", "title": "Brain-Like Language Processing via a Shallow Untrained Multihead Attention Network", "paper_summary": "Large Language Models (LLMs) have been shown to be effective models of the\nhuman language system, with some models predicting most explainable variance of\nbrain activity in current datasets. Even in untrained models, the\nrepresentations induced by architectural priors can exhibit reasonable\nalignment to brain data. In this work, we investigate the key architectural\ncomponents driving the surprising alignment of untrained models. To estimate\nLLM-to-brain similarity, we first select language-selective units within an\nLLM, similar to how neuroscientists identify the language network in the human\nbrain. We then benchmark the brain alignment of these LLM units across five\ndifferent brain recording datasets. By isolating critical components of the\nTransformer architecture, we identify tokenization strategy and multihead\nattention as the two major components driving brain alignment. A simple form of\nrecurrence further improves alignment. We further demonstrate this quantitative\nbrain alignment of our model by reproducing landmark studies in the language\nneuroscience field, showing that localized model units -- just like language\nvoxels measured empirically in the human brain -- discriminate more reliably\nbetween lexical than syntactic differences, and exhibit similar response\nprofiles under the same experimental conditions. Finally, we demonstrate the\nutility of our model's representations for language modeling, achieving\nimproved sample and parameter efficiency over comparable architectures. Our\nmodel's estimates of surprisal sets a new state-of-the-art in the behavioral\nalignment to human reading times. Taken together, we propose a highly brain-\nand behaviorally-aligned model that conceptualizes the human language system as\nan untrained shallow feature encoder, with structural priors, combined with a\ntrained decoder to achieve efficient and performant language processing.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u88ab\u8b49\u660e\u662f\u4eba\u985e\u8a9e\u8a00\u7cfb\u7d71\u7684\u6709\u6548\u6a21\u578b\uff0c\u5176\u4e2d\u4e00\u4e9b\u6a21\u578b\u9810\u6e2c\u4e86\u7576\u524d\u6578\u64da\u96c6\u4e2d\u5927\u591a\u6578\u53ef\u89e3\u91cb\u7684\u8166\u90e8\u6d3b\u52d5\u8b8a\u7570\u3002\u5373\u4f7f\u5728\u672a\u7d93\u8a13\u7df4\u7684\u6a21\u578b\u4e2d\uff0c\u7531\u67b6\u69cb\u5148\u9a57\u8a98\u5c0e\u7684\u8868\u5fb5\u4e5f\u6703\u8868\u73fe\u51fa\u8207\u8166\u90e8\u6578\u64da\u7684\u5408\u7406\u4e00\u81f4\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u9a45\u52d5\u672a\u7d93\u8a13\u7df4\u6a21\u578b\u9a5a\u4eba\u4e00\u81f4\u6027\u7684\u95dc\u9375\u67b6\u69cb\u7d44\u6210\u90e8\u5206\u3002\u70ba\u4e86\u4f30\u8a08 LLM \u8207\u5927\u8166\u7684\u76f8\u4f3c\u6027\uff0c\u6211\u5011\u9996\u5148\u5728 LLM \u4e2d\u9078\u64c7\u8a9e\u8a00\u9078\u64c7\u6027\u55ae\u4f4d\uff0c\u985e\u4f3c\u65bc\u795e\u7d93\u79d1\u5b78\u5bb6\u8b58\u5225\u4eba\u985e\u5927\u8166\u4e2d\u7684\u8a9e\u8a00\u7db2\u8def\u7684\u65b9\u5f0f\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c0d\u9019\u4e9b LLM \u55ae\u5143\u7684\u5927\u8166\u5c0d\u9f4a\u5728\u4e94\u500b\u4e0d\u540c\u7684\u8166\u90e8\u8a18\u9304\u6578\u64da\u96c6\u4e2d\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u3002\u901a\u904e\u5206\u96e2 Transformer \u67b6\u69cb\u7684\u95dc\u9375\u7d44\u6210\u90e8\u5206\uff0c\u6211\u5011\u5c07\u6a19\u8a18\u5316\u7b56\u7565\u548c\u591a\u982d\u6ce8\u610f\u529b\u8b58\u5225\u70ba\u9a45\u52d5\u5927\u8166\u5c0d\u9f4a\u7684\u5169\u500b\u4e3b\u8981\u7d44\u6210\u90e8\u5206\u3002\u4e00\u7a2e\u7c21\u55ae\u7684\u905e\u8ff4\u5f62\u5f0f\u9032\u4e00\u6b65\u6539\u5584\u4e86\u5c0d\u9f4a\u3002\u6211\u5011\u901a\u904e\u91cd\u73fe\u8a9e\u8a00\u795e\u7d93\u79d1\u5b78\u9818\u57df\u7684\u6a19\u8a8c\u6027\u7814\u7a76\u9032\u4e00\u6b65\u5c55\u793a\u4e86\u6211\u5011\u6a21\u578b\u7684\u9019\u7a2e\u5b9a\u91cf\u5927\u8166\u5c0d\u9f4a\uff0c\u8868\u660e\u5c40\u90e8\u6a21\u578b\u55ae\u5143\u2014\u2014\u5c31\u50cf\u5728\u5927\u8166\u4e2d\u7d93\u9a57\u6e2c\u91cf\u7684\u4eba\u985e\u8a9e\u8a00\u9ad4\u7d20\u4e00\u6a23\u2014\u2014\u5728\u8a5e\u5f59\u5dee\u7570\u548c\u53e5\u6cd5\u5dee\u7570\u4e4b\u9593\u9032\u884c\u66f4\u53ef\u9760\u7684\u5340\u5206\uff0c\u4e26\u4e14\u5728\u76f8\u540c\u7684\u5be6\u9a57\u689d\u4ef6\u4e0b\u8868\u73fe\u51fa\u76f8\u4f3c\u7684\u97ff\u61c9\u7279\u5fb5\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u6a21\u578b\u7684\u8868\u5fb5\u5728\u8a9e\u8a00\u5efa\u6a21\u4e2d\u7684\u6548\u7528\uff0c\u8207\u53ef\u6bd4\u8f03\u7684\u67b6\u69cb\u76f8\u6bd4\uff0c\u5be6\u73fe\u4e86\u6539\u9032\u7684\u6a23\u672c\u548c\u53c3\u6578\u6548\u7387\u3002\u6211\u5011\u6a21\u578b\u5c0d\u9a5a\u8a1d\u7684\u4f30\u8a08\u5728\u8207\u4eba\u985e\u95b1\u8b80\u6642\u9593\u7684\u884c\u70ba\u5c0d\u9f4a\u65b9\u9762\u8a2d\u5b9a\u4e86\u4e00\u500b\u65b0\u7684\u6700\u5148\u9032\u6c34\u5e73\u3002\u7d9c\u4e0a\u6240\u8ff0\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u9ad8\u5ea6\u8207\u5927\u8166\u548c\u884c\u70ba\u5c0d\u9f4a\u7684\u6a21\u578b\uff0c\u5b83\u5c07\u4eba\u985e\u8a9e\u8a00\u7cfb\u7d71\u6982\u5ff5\u5316\u4e3a\u4e00\u500b\u672a\u7d93\u8a13\u7df4\u7684\u6dfa\u5c64\u7279\u5fb5\u7de8\u78bc\u5668\uff0c\u5177\u6709\u7d50\u69cb\u5148\u9a57\uff0c\u4e26\u8207\u4e00\u500b\u8a13\u7df4\u904e\u7684\u89e3\u78bc\u5668\u76f8\u7d50\u5408\uff0c\u4ee5\u5be6\u73fe\u9ad8\u6548\u548c\u9ad8\u6027\u80fd\u7684\u8a9e\u8a00\u8655\u7406\u3002", "author": "Badr AlKhamissi et.al.", "authors": "Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf", "id": "2406.15109v1", "paper_url": "http://arxiv.org/abs/2406.15109v1", "repo": "null"}}