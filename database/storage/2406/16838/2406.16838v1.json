{"2406.16838": {"publish_time": "2024-06-24", "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models", "paper_summary": "One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.", "paper_summary_zh": "\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u73fe\u4ee3\u7814\u7a76\u4e2d\uff0c\u6700\u5f15\u4eba\u6ce8\u76ee\u7684\u767c\u73fe\u4e4b\u4e00\u662f\uff0c\u8a13\u7df4\u671f\u9593\u64f4\u5927\u904b\u7b97\u6703\u5e36\u4f86\u66f4\u597d\u7684\u7d50\u679c\u3002\u7136\u800c\uff0c\u5728\u63a8\u7406\u671f\u9593\u64f4\u5927\u904b\u7b97\u7684\u597d\u8655\u537b\u8f03\u5c11\u53d7\u5230\u95dc\u6ce8\u3002\u672c\u8abf\u67e5\u91cd\u9ede\u95dc\u6ce8\u9019\u4e9b\u63a8\u7406\u6642\u9593\u65b9\u6cd5\u3002\u6211\u5011\u5728\u4e00\u500b\u7d71\u4e00\u7684\u6578\u5b78\u5f62\u5f0f\u4e3b\u7fa9\u4e0b\u63a2\u8a0e\u4e86\u4e09\u500b\u9818\u57df\uff1a\u7b26\u865f\u7d1a\u5225\u751f\u6210\u6f14\u7b97\u6cd5\u3001\u5143\u751f\u6210\u6f14\u7b97\u6cd5\u548c\u9ad8\u6548\u751f\u6210\u3002\u7b26\u865f\u7d1a\u5225\u751f\u6210\u6f14\u7b97\u6cd5\uff08\u901a\u5e38\u7a31\u70ba\u89e3\u78bc\u6f14\u7b97\u6cd5\uff09\u901a\u904e\u4e00\u6b21\u62bd\u53d6\u4e00\u500b\u7b26\u865f\u6216\u5efa\u69cb\u7b26\u865f\u7d1a\u5225\u641c\u5c0b\u7a7a\u9593\uff0c\u7136\u5f8c\u9078\u64c7\u4e00\u500b\u8f38\u51fa\u9032\u884c\u64cd\u4f5c\u3002\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u5047\u8a2d\u53ef\u4ee5\u5b58\u53d6\u8a9e\u8a00\u6a21\u578b\u7684 logit\u3001\u4e0b\u4e00\u500b\u7b26\u865f\u5206\u4f48\u6216\u6a5f\u7387\u5206\u6578\u3002\u5143\u751f\u6210\u6f14\u7b97\u6cd5\u8655\u7406\u90e8\u5206\u6216\u5b8c\u6574\u5e8f\u5217\uff0c\u6574\u5408\u9818\u57df\u77e5\u8b58\uff0c\u555f\u7528\u56de\u6eaf\uff0c\u4e26\u6574\u5408\u5916\u90e8\u8cc7\u8a0a\u3002\u9ad8\u6548\u751f\u6210\u65b9\u6cd5\u65e8\u5728\u964d\u4f4e\u7b26\u865f\u6210\u672c\u4e26\u63d0\u9ad8\u751f\u6210\u901f\u5ea6\u3002\u6211\u5011\u7684\u8abf\u67e5\u7d71\u4e00\u4e86\u4f86\u81ea\u4e09\u500b\u7814\u7a76\u793e\u7fa4\u7684\u89c0\u9ede\uff1a\u50b3\u7d71\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3001\u73fe\u4ee3 LLM \u548c\u6a5f\u5668\u5b78\u7fd2\u7cfb\u7d71\u3002", "author": "Sean Welleck et.al.", "authors": "Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui", "id": "2406.16838v1", "paper_url": "http://arxiv.org/abs/2406.16838v1", "repo": "null"}}