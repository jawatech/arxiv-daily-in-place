{"2410.05052": {"publish_time": "2024-10-07", "title": "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "paper_summary": "Loss spikes, a phenomenon in which the loss value diverges suddenly, is a\nfundamental issue in the pre-training of large language models. This paper\nsupposes that the non-uniformity of the norm of the parameters is one of the\ncauses of loss spikes. Here, in training of neural networks, the scale of the\ngradients is required to be kept constant throughout the layers to avoid the\nvanishing and exploding gradients problem. However, to meet these requirements\nin the Transformer model, the norm of the model parameters must be non-uniform,\nand thus, parameters whose norm is smaller are more sensitive to the parameter\nupdate. To address this issue, we propose a novel technique, weight scaling as\nreparameterization (WeSaR). WeSaR introduces a gate parameter per parameter\nmatrix and adjusts it to the value satisfying the requirements. Because of the\ngate parameter, WeSaR sets the norm of the original parameters uniformly, which\nresults in stable training. Experimental results with the Transformer decoders\nconsisting of 130 million, 1.3 billion, and 13 billion parameters showed that\nWeSaR stabilizes and accelerates training and that it outperformed compared\nmethods including popular initialization methods.", "paper_summary_zh": "\u640d\u5931\u98c6\u5347\u662f\u4e00\u7a2e\u640d\u5931\u503c\u7a81\u7136\u767c\u6563\u7684\u73fe\u8c61\uff0c\u662f\u5927\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u4e2d\u7684\u57fa\u672c\u554f\u984c\u3002\u672c\u6587\u5047\u8a2d\u53c3\u6578\u7bc4\u6578\u7684\u4e0d\u5747\u52fb\u6027\u662f\u640d\u5931\u98c6\u5347\u7684\u539f\u56e0\u4e4b\u4e00\u3002\u5728\u9019\u88e1\uff0c\u5728\u795e\u7d93\u7db2\u8def\u7684\u8a13\u7df4\u4e2d\uff0c\u68af\u5ea6\u7684\u898f\u6a21\u9700\u8981\u5728\u5404\u5c64\u4e2d\u4fdd\u6301\u6046\u5b9a\uff0c\u4ee5\u907f\u514d\u68af\u5ea6\u6d88\u5931\u548c\u7206\u70b8\u554f\u984c\u3002\u7136\u800c\uff0c\u70ba\u4e86\u6eff\u8db3 Transformer \u6a21\u578b\u4e2d\u7684\u9019\u4e9b\u8981\u6c42\uff0c\u6a21\u578b\u53c3\u6578\u7684\u7bc4\u6578\u5fc5\u9808\u662f\u4e0d\u5747\u52fb\u7684\uff0c\u56e0\u6b64\uff0c\u7bc4\u6578\u8f03\u5c0f\u7684\u53c3\u6578\u5c0d\u53c3\u6578\u66f4\u65b0\u66f4\u654f\u611f\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u6280\u8853\uff0c\u5c07\u6b0a\u91cd\u7e2e\u653e\u4f5c\u70ba\u91cd\u65b0\u53c3\u6578\u5316\uff08WeSaR\uff09\u3002WeSaR \u70ba\u6bcf\u500b\u53c3\u6578\u77e9\u9663\u5f15\u5165\u4e00\u500b\u9598\u9580\u53c3\u6578\uff0c\u4e26\u5c07\u5176\u8abf\u6574\u70ba\u6eff\u8db3\u8981\u6c42\u7684\u503c\u3002\u7531\u65bc\u9598\u9580\u53c3\u6578\uff0cWeSaR \u5c07\u539f\u59cb\u53c3\u6578\u7684\u7bc4\u6578\u8a2d\u5b9a\u70ba\u5747\u52fb\uff0c\u5f9e\u800c\u5c0e\u81f4\u7a69\u5b9a\u7684\u8a13\u7df4\u3002\u5305\u542b 1.3 \u5104\u300113 \u5104\u548c 130 \u5104\u500b\u53c3\u6578\u7684 Transformer \u89e3\u78bc\u5668\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cWeSaR \u7a69\u5b9a\u4e26\u52a0\u901f\u4e86\u8a13\u7df4\uff0c\u4e26\u4e14\u5b83\u512a\u65bc\u5305\u62ec\u6d41\u884c\u521d\u59cb\u5316\u65b9\u6cd5\u5728\u5167\u7684\u6bd4\u8f03\u65b9\u6cd5\u3002", "author": "Kosuke Nishida et.al.", "authors": "Kosuke Nishida, Kyosuke Nishida, Kuniko Saito", "id": "2410.05052v1", "paper_url": "http://arxiv.org/abs/2410.05052v1", "repo": "null"}}