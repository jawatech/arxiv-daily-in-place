{"2410.08209": {"publish_time": "2024-10-10", "title": "Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision", "paper_summary": "Current large multimodal models (LMMs) face challenges in grounding, which\nrequires the model to relate language components to visual entities. Contrary\nto the common practice that fine-tunes LMMs with additional grounding\nsupervision, we find that the grounding ability can in fact emerge in LMMs\ntrained without explicit grounding supervision. To reveal this emerging\ngrounding, we introduce an \"attend-and-segment\" method which leverages\nattention maps from standard LMMs to perform pixel-level segmentation.\nFurthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM\nutilizing a diffusion-based visual encoder, as opposed to the standard CLIP\nvisual encoder, and trained with the same weak supervision. Without being\nconstrained by the biases and limited scale of grounding-specific supervision\ndata, our approach is more generalizable and scalable. We achieve competitive\nperformance on both grounding-specific and general visual question answering\nbenchmarks, compared with grounding LMMs and generalist LMMs, respectively.\nNotably, we achieve a 44.2 grounding mask recall on grounded conversation\ngeneration without any grounding supervision, outperforming the extensively\nsupervised model GLaMM. Project page: https://groundLMM.github.io.", "paper_summary_zh": "\u76ee\u524d\u7684\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u9762\u81e8\u8457\u57fa\u790e\u554f\u984c\uff0c\u9019\u9700\u8981\u6a21\u578b\u5c07\u8a9e\u8a00\u7d44\u6210\u90e8\u5206\u8207\u8996\u89ba\u5be6\u9ad4\u95dc\u806f\u8d77\u4f86\u3002\u8207\u4f7f\u7528\u984d\u5916\u7684\u57fa\u790e\u76e3\u7763\u5fae\u8abf LMM \u7684\u5e38\u898b\u505a\u6cd5\u76f8\u53cd\uff0c\u6211\u5011\u767c\u73fe\u57fa\u790e\u80fd\u529b\u5be6\u969b\u4e0a\u53ef\u4ee5\u5728\u6c92\u6709\u660e\u78ba\u57fa\u790e\u76e3\u7763\u7684\u60c5\u6cc1\u4e0b\u8a13\u7df4\u7684 LMM \u4e2d\u51fa\u73fe\u3002\u70ba\u4e86\u63ed\u793a\u9019\u7a2e\u65b0\u8208\u7684\u57fa\u790e\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u300c\u95dc\u6ce8\u4e26\u5206\u5272\u300d\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u5229\u7528\u6a19\u6e96 LMM \u7684\u6ce8\u610f\u529b\u5716\u4f86\u57f7\u884c\u50cf\u7d20\u7d1a\u5206\u5272\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u589e\u5f37\u57fa\u790e\u80fd\u529b\uff0c\u6211\u5011\u63d0\u51fa\u4e86 DIFFLMM\uff0c\u9019\u662f\u4e00\u7a2e LMM\uff0c\u5b83\u4f7f\u7528\u57fa\u65bc\u64f4\u6563\u7684\u8996\u89ba\u7de8\u78bc\u5668\uff0c\u800c\u4e0d\u662f\u6a19\u6e96 CLIP \u8996\u89ba\u7de8\u78bc\u5668\uff0c\u4e26\u4f7f\u7528\u76f8\u540c\u7684\u5f31\u76e3\u7763\u9032\u884c\u8a13\u7df4\u3002\u5728\u4e0d\u53d7\u57fa\u790e\u7279\u5b9a\u76e3\u7763\u6578\u64da\u7684\u504f\u5dee\u548c\u6709\u9650\u898f\u6a21\u7684\u7d04\u675f\u4e0b\uff0c\u6211\u5011\u7684\u505a\u6cd5\u66f4\u5177\u901a\u7528\u6027\u548c\u53ef\u64f4\u5c55\u6027\u3002\u8207\u57fa\u790e LMM \u548c\u901a\u624d LMM \u76f8\u6bd4\uff0c\u6211\u5011\u5728\u57fa\u790e\u7279\u5b9a\u548c\u4e00\u822c\u8996\u89ba\u554f\u984c\u89e3\u7b54\u57fa\u6e96\u4e0a\u5be6\u73fe\u4e86\u7af6\u722d\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u5728\u6c92\u6709\u4efb\u4f55\u57fa\u790e\u76e3\u7763\u7684\u60c5\u6cc1\u4e0b\uff0c\u5c0d\u57fa\u790e\u5c0d\u8a71\u751f\u6210\u5be6\u73fe\u4e86 44.2 \u7684\u57fa\u790e\u906e\u7f69\u53ec\u56de\u7387\uff0c\u512a\u65bc\u5ee3\u6cdb\u76e3\u7763\u7684\u6a21\u578b GLaMM\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://groundLMM.github.io\u3002", "author": "Shengcao Cao et.al.", "authors": "Shengcao Cao, Liang-Yan Gui, Yu-Xiong Wang", "id": "2410.08209v1", "paper_url": "http://arxiv.org/abs/2410.08209v1", "repo": "null"}}