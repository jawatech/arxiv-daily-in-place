{"2410.22184": {"publish_time": "2024-10-29", "title": "Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets", "paper_summary": "We propose a novel teacher-student framework to distill knowledge from\nmultiple teachers trained on distinct datasets. Each teacher is first trained\nfrom scratch on its own dataset. Then, the teachers are combined into a joint\narchitecture, which fuses the features of all teachers at multiple\nrepresentation levels. The joint teacher architecture is fine-tuned on samples\nfrom all datasets, thus gathering useful generic information from all data\nsamples. Finally, we employ a multi-level feature distillation procedure to\ntransfer the knowledge to a student model for each of the considered datasets.\nWe conduct image classification experiments on seven benchmarks, and action\nrecognition experiments on three benchmarks. To illustrate the power of our\nfeature distillation procedure, the student architectures are chosen to be\nidentical to those of the individual teachers. To demonstrate the flexibility\nof our approach, we combine teachers with distinct architectures. We show that\nour novel Multi-Level Feature Distillation (MLFD) can significantly surpass\nequivalent architectures that are either trained on individual datasets, or\njointly trained on all datasets at once. Furthermore, we confirm that each step\nof the proposed training procedure is well motivated by a comprehensive\nablation study. We publicly release our code at\nhttps://github.com/AdrianIordache/MLFD.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u6559\u5e2b-\u5b78\u751f\u67b6\u69cb\uff0c\u5f9e\u5728\u4e0d\u540c\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u591a\u500b\u6559\u5e2b\u4e2d\u8403\u53d6\u77e5\u8b58\u3002\u6bcf\u500b\u6559\u5e2b\u9996\u5148\u5f9e\u982d\u958b\u59cb\u5728\u81ea\u5df1\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a13\u7df4\u3002\u7136\u5f8c\uff0c\u5c07\u6559\u5e2b\u7d44\u5408\u6210\u4e00\u500b\u806f\u5408\u67b6\u69cb\uff0c\u8a72\u67b6\u69cb\u878d\u5408\u4e86\u6240\u6709\u6559\u5e2b\u5728\u591a\u500b\u8868\u793a\u5c64\u7d1a\u4e2d\u7684\u7279\u5fb5\u3002\u806f\u5408\u6559\u5e2b\u67b6\u69cb\u91dd\u5c0d\u4f86\u81ea\u6240\u6709\u8cc7\u6599\u96c6\u7684\u6a23\u672c\u9032\u884c\u5fae\u8abf\uff0c\u5f9e\u800c\u5f9e\u6240\u6709\u8cc7\u6599\u6a23\u672c\u4e2d\u6536\u96c6\u6709\u7528\u7684\u901a\u7528\u8cc7\u8a0a\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63a1\u7528\u591a\u5c64\u7d1a\u7279\u5fb5\u8403\u53d6\u7a0b\u5e8f\uff0c\u5c07\u77e5\u8b58\u8f49\u79fb\u5230\u6bcf\u500b\u8003\u616e\u8cc7\u6599\u96c6\u7684\u5b78\u751f\u6a21\u578b\u3002\u6211\u5011\u5c0d\u4e03\u500b\u57fa\u6e96\u9032\u884c\u5f71\u50cf\u5206\u985e\u5be6\u9a57\uff0c\u4e26\u5c0d\u4e09\u500b\u57fa\u6e96\u9032\u884c\u52d5\u4f5c\u8fa8\u8b58\u5be6\u9a57\u3002\u70ba\u4e86\u8aaa\u660e\u6211\u5011\u7279\u5fb5\u8403\u53d6\u7a0b\u5e8f\u7684\u5f37\u5927\u529f\u80fd\uff0c\u5b78\u751f\u67b6\u69cb\u88ab\u9078\u70ba\u8207\u500b\u5225\u6559\u5e2b\u7684\u67b6\u69cb\u76f8\u540c\u3002\u70ba\u4e86\u5c55\u793a\u6211\u5011\u65b9\u6cd5\u7684\u9748\u6d3b\u6027\uff0c\u6211\u5011\u7d50\u5408\u4e86\u5177\u6709\u4e0d\u540c\u67b6\u69cb\u7684\u6559\u5e2b\u3002\u6211\u5011\u5c55\u793a\u6211\u5011\u7684\u5275\u65b0\u591a\u5c64\u7d1a\u7279\u5fb5\u8403\u53d6 (MLFD) \u53ef\u4ee5\u986f\u8457\u8d85\u8d8a\u50c5\u5728\u500b\u5225\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u6216\u4e00\u6b21\u5728\u6240\u6709\u8cc7\u6599\u96c6\u4e0a\u806f\u5408\u8a13\u7df4\u7684\u7b49\u6548\u67b6\u69cb\u3002\u6b64\u5916\uff0c\u6211\u5011\u78ba\u8a8d\u6240\u63d0\u8b70\u8a13\u7df4\u7a0b\u5e8f\u7684\u6bcf\u4e00\u6b65\u90fd\u53d7\u5230\u5168\u9762\u6d88\u878d\u7814\u7a76\u7684\u5145\u5206\u6fc0\u52f5\u3002\u6211\u5011\u5728 https://github.com/AdrianIordache/MLFD \u516c\u958b\u767c\u5e03\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u3002", "author": "Adrian Iordache et.al.", "authors": "Adrian Iordache, Bogdan Alexe, Radu Tudor Ionescu", "id": "2410.22184v1", "paper_url": "http://arxiv.org/abs/2410.22184v1", "repo": "https://github.com/adrianiordache/mlfd"}}