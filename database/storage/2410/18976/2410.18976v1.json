{"2410.18976": {"publish_time": "2024-10-24", "title": "CAMEL-Bench: A Comprehensive Arabic LMM Benchmark", "paper_summary": "Recent years have witnessed a significant interest in developing large\nmultimodal models (LMMs) capable of performing various visual reasoning and\nunderstanding tasks. This has led to the introduction of multiple LMM\nbenchmarks to evaluate LMMs on different tasks. However, most existing LMM\nevaluation benchmarks are predominantly English-centric. In this work, we\ndevelop a comprehensive LMM evaluation benchmark for the Arabic language to\nrepresent a large population of over 400 million speakers. The proposed\nbenchmark, named CAMEL-Bench, comprises eight diverse domains and 38\nsub-domains including, multi-image understanding, complex visual perception,\nhandwritten document understanding, video understanding, medical imaging, plant\ndiseases, and remote sensing-based land use understanding to evaluate broad\nscenario generalizability. Our CAMEL-Bench comprises around 29,036 questions\nthat are filtered from a larger pool of samples, where the quality is manually\nverified by native speakers to ensure reliable model assessment. We conduct\nevaluations of both closed-source, including GPT-4 series, and open-source\nLMMs. Our analysis reveals the need for substantial improvement, especially\namong the best open-source models, with even the closed-source GPT-4o achieving\nan overall score of 62%. Our benchmark and evaluation scripts are open-sourced.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u958b\u767c\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u4ee5\u57f7\u884c\u5404\u7a2e\u8996\u89ba\u63a8\u7406\u548c\u7406\u89e3\u4efb\u52d9\u5f15\u8d77\u4e86\u6975\u5927\u7684\u8208\u8da3\u3002\u9019\u5c0e\u81f4\u5f15\u5165\u4e86\u591a\u500b LMM \u57fa\u6e96\u4f86\u8a55\u4f30 LMM \u5728\u4e0d\u540c\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 LMM \u8a55\u4f30\u57fa\u6e96\u5927\u591a\u4ee5\u82f1\u8a9e\u70ba\u4e2d\u5fc3\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u5168\u9762\u7684\u963f\u62c9\u4f2f\u8a9e LMM \u8a55\u4f30\u57fa\u6e96\uff0c\u4ee5\u4ee3\u8868\u8d85\u904e 4 \u5104\u4eba\u53e3\u7684\u9f90\u5927\u7fa4\u9ad4\u3002\u63d0\u51fa\u7684\u57fa\u6e96\u7a31\u70ba CAMEL-Bench\uff0c\u5305\u542b\u516b\u500b\u4e0d\u540c\u7684\u9818\u57df\u548c 38 \u500b\u5b50\u9818\u57df\uff0c\u5305\u62ec\u591a\u5716\u50cf\u7406\u89e3\u3001\u8907\u96dc\u8996\u89ba\u611f\u77e5\u3001\u624b\u5beb\u6587\u4ef6\u7406\u89e3\u3001\u8996\u983b\u7406\u89e3\u3001\u91ab\u5b78\u5f71\u50cf\u3001\u690d\u7269\u75be\u75c5\u548c\u57fa\u65bc\u9059\u611f\u7684\u571f\u5730\u5229\u7528\u7406\u89e3\uff0c\u4ee5\u8a55\u4f30\u5ee3\u6cdb\u5834\u666f\u7684\u53ef\u6982\u62ec\u6027\u3002\u6211\u5011\u7684 CAMEL-Bench \u5305\u542b\u7d04 29,036 \u500b\u554f\u984c\uff0c\u9019\u4e9b\u554f\u984c\u5f9e\u66f4\u5927\u7684\u6a23\u672c\u6c60\u4e2d\u904e\u6ffe\u51fa\u4f86\uff0c\u5176\u8cea\u91cf\u7531\u6bcd\u8a9e\u4eba\u58eb\u624b\u52d5\u9a57\u8b49\u4ee5\u78ba\u4fdd\u53ef\u9760\u7684\u6a21\u578b\u8a55\u4f30\u3002\u6211\u5011\u5c0d\u5c01\u9589\u6e90\u78bc\uff08\u5305\u62ec GPT-4 \u7cfb\u5217\uff09\u548c\u958b\u6e90 LMM \u9032\u884c\u4e86\u8a55\u4f30\u3002\u6211\u5011\u7684\u5206\u6790\u63ed\u793a\u4e86\u9700\u8981\u5927\u5e45\u6539\u9032\uff0c\u7279\u5225\u662f\u5728\u6700\u597d\u7684\u958b\u6e90\u6a21\u578b\u4e2d\uff0c\u5373\u4f7f\u662f\u5c01\u9589\u6e90\u78bc\u7684 GPT-4o \u4e5f\u53ea\u9054\u5230\u4e86 62% \u7684\u7e3d\u5206\u3002\u6211\u5011\u7684\u57fa\u6e96\u548c\u8a55\u4f30\u8173\u672c\u662f\u958b\u6e90\u7684\u3002", "author": "Sara Ghaboura et.al.", "authors": "Sara Ghaboura, Ahmed Heakl, Omkar Thawakar, Ali Alharthi, Ines Riahi, Abduljalil Saif, Jorma Laaksonen, Fahad S. Khan, Salman Khan, Rao M. Anwer", "id": "2410.18976v1", "paper_url": "http://arxiv.org/abs/2410.18976v1", "repo": "null"}}