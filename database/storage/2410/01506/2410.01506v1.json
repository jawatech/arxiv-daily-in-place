{"2410.01506": {"publish_time": "2024-10-02", "title": "LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion", "paper_summary": "In computer vision tasks, features often come from diverse representations,\ndomains, and modalities, such as text, images, and videos. Effectively fusing\nthese features is essential for robust performance, especially with the\navailability of powerful pre-trained models like vision-language models.\nHowever, common fusion methods, such as concatenation, element-wise operations,\nand non-linear techniques, often fail to capture structural relationships, deep\nfeature interactions, and suffer from inefficiency or misalignment of features\nacross domains. In this paper, we shift from high-dimensional feature space to\na lower-dimensional, interpretable graph space by constructing similarity\ngraphs that encode feature relationships at different levels, e.g., clip,\nframe, patch, token, etc. To capture deeper interactions, we use graph power\nexpansions and introduce a learnable graph fusion operator to combine these\ngraph powers for more effective fusion. Our approach is relationship-centric,\noperates in a homogeneous space, and is mathematically principled, resembling\nelement-wise similarity score aggregation via multilinear polynomials. We\ndemonstrate the effectiveness of our graph-based fusion method on video anomaly\ndetection, showing strong performance across multi-representational,\nmulti-modal, and multi-domain feature fusion tasks.", "paper_summary_zh": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0c\u7279\u5f81\u901a\u5e38\u6765\u81ea\u4e0d\u540c\u7684\u8868\u793a\u3001\u9886\u57df\u548c\u6a21\u6001\uff0c\u4f8b\u5982\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u3002\u6709\u6548\u878d\u5408\u8fd9\u4e9b\u7279\u5f81\u5bf9\u4e8e\u7a33\u5065\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u7684\u60c5\u51b5\u4e0b\u3002\u7136\u800c\uff0c\u5e38\u89c1\u7684\u878d\u5408\u65b9\u6cd5\uff0c\u5982\u4e32\u8054\u3001\u9010\u5143\u7d20\u8fd0\u7b97\u548c\u975e\u7ebf\u6027\u6280\u672f\uff0c\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u7ed3\u6784\u5173\u7cfb\u3001\u6df1\u5ea6\u7279\u5f81\u4ea4\u4e92\uff0c\u5e76\u4e14\u5728\u8de8\u57df\u7279\u5f81\u4e0a\u6548\u7387\u4f4e\u4e0b\u6216\u672a\u5bf9\u9f50\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\u8f6c\u79fb\u5230\u4f4e\u7ef4\u7684\u53ef\u89e3\u91ca\u56fe\u7a7a\u95f4\uff0c\u901a\u8fc7\u6784\u5efa\u76f8\u4f3c\u6027\u56fe\u6765\u5bf9\u4e0d\u540c\u7ea7\u522b\u7684\u7279\u5f81\u5173\u7cfb\u8fdb\u884c\u7f16\u7801\uff0c\u4f8b\u5982\u526a\u8f91\u3001\u5e27\u3001\u5757\u3001\u6807\u8bb0\u7b49\u3002\u4e3a\u4e86\u6355\u6349\u66f4\u6df1\u5c42\u6b21\u7684\u4ea4\u4e92\uff0c\u6211\u4eec\u4f7f\u7528\u56fe\u5e42\u5c55\u5f00\u5e76\u5f15\u5165\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u56fe\u878d\u5408\u7b97\u5b50\u6765\u7ec4\u5408\u8fd9\u4e9b\u56fe\u5e42\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u878d\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4ee5\u5173\u7cfb\u4e3a\u4e2d\u5fc3\uff0c\u5728\u540c\u6784\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u5e76\u4e14\u5177\u6709\u6570\u5b66\u539f\u7406\uff0c\u7c7b\u4f3c\u4e8e\u901a\u8fc7\u591a\u7ebf\u6027\u591a\u9879\u5f0f\u8fdb\u884c\u9010\u5143\u7d20\u76f8\u4f3c\u6027\u5206\u6570\u805a\u5408\u3002\u6211\u4eec\u5728\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5c55\u793a\u4e86\u57fa\u4e8e\u56fe\u7684\u878d\u5408\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u591a\u8868\u793a\u3001\u591a\u6a21\u6001\u548c\u591a\u57df\u7279\u5f81\u878d\u5408\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002", "author": "Dexuan Ding et.al.", "authors": "Dexuan Ding, Lei Wang, Liyun Zhu, Tom Gedeon, Piotr Koniusz", "id": "2410.01506v1", "paper_url": "http://arxiv.org/abs/2410.01506v1", "repo": "null"}}