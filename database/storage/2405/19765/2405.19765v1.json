{"2405.19765": {"publish_time": "2024-05-30", "title": "Towards Unified Multi-granularity Text Detection with Interactive Attention", "paper_summary": "Existing OCR engines or document image analysis systems typically rely on\ntraining separate models for text detection in varying scenarios and\ngranularities, leading to significant computational complexity and resource\ndemands. In this paper, we introduce \"Detect Any Text\" (DAT), an advanced\nparadigm that seamlessly unifies scene text detection, layout analysis, and\ndocument page detection into a cohesive, end-to-end model. This design enables\nDAT to efficiently manage text instances at different granularities, including\n*word*, *line*, *paragraph* and *page*. A pivotal innovation in DAT is the\nacross-granularity interactive attention module, which significantly enhances\nthe representation learning of text instances at varying granularities by\ncorrelating structural information across different text queries. As a result,\nit enables the model to achieve mutually beneficial detection performances\nacross multiple text granularities. Additionally, a prompt-based segmentation\nmodule refines detection outcomes for texts of arbitrary curvature and complex\nlayouts, thereby improving DAT's accuracy and expanding its real-world\napplicability. Experimental results demonstrate that DAT achieves\nstate-of-the-art performances across a variety of text-related benchmarks,\nincluding multi-oriented/arbitrarily-shaped scene text detection, document\nlayout analysis and page detection tasks.", "paper_summary_zh": "\u73fe\u6709\u7684 OCR \u5f15\u64ce\u6216\u6587\u4ef6\u5716\u50cf\u5206\u6790\u7cfb\u7d71\u901a\u5e38\u4f9d\u8cf4\u65bc\u8a13\u7df4\u4e0d\u540c\u7684\u6a21\u578b\uff0c\u7528\u65bc\u5728\u4e0d\u540c\u7684\u5834\u666f\u548c\u7c92\u5ea6\u4e2d\u9032\u884c\u6587\u5b57\u5075\u6e2c\uff0c\u5c0e\u81f4\u986f\u8457\u7684\u8a08\u7b97\u8907\u96dc\u6027\u548c\u8cc7\u6e90\u9700\u6c42\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u300c\u5075\u6e2c\u4efb\u4f55\u6587\u5b57\u300d(DAT)\uff0c\u4e00\u7a2e\u5148\u9032\u7684\u5178\u7bc4\uff0c\u5b83\u7121\u7e2b\u5730\u5c07\u5834\u666f\u6587\u5b57\u5075\u6e2c\u3001\u7248\u9762\u5206\u6790\u548c\u6587\u4ef6\u9801\u9762\u5075\u6e2c\u7d71\u4e00\u5230\u4e00\u500b\u6709\u51dd\u805a\u529b\u7684\u7aef\u5230\u7aef\u6a21\u578b\u4e2d\u3002\u9019\u500b\u8a2d\u8a08\u4f7f DAT \u80fd\u5920\u6709\u6548\u5730\u7ba1\u7406\u4e0d\u540c\u7c92\u5ea6\u7684\u6587\u5b57\u5be6\u4f8b\uff0c\u5305\u62ec *\u5b57\u8a5e*\u3001*\u884c*\u3001*\u6bb5\u843d* \u548c *\u9801\u9762*\u3002DAT \u4e2d\u7684\u4e00\u500b\u95dc\u9375\u5275\u65b0\u662f\u8de8\u7c92\u5ea6\u4e92\u52d5\u6ce8\u610f\u529b\u6a21\u7d44\uff0c\u5b83\u901a\u904e\u5c07\u4e0d\u540c\u6587\u5b57\u67e5\u8a62\u7684\u7d50\u69cb\u8cc7\u8a0a\u76f8\u4e92\u95dc\u806f\uff0c\u986f\u8457\u589e\u5f37\u4e86\u4e0d\u540c\u7c92\u5ea6\u6587\u5b57\u5be6\u4f8b\u7684\u8868\u5fb5\u5b78\u7fd2\u3002\u56e0\u6b64\uff0c\u5b83\u4f7f\u6a21\u578b\u80fd\u5920\u5728\u591a\u500b\u6587\u5b57\u7c92\u5ea6\u4e0a\u5be6\u73fe\u4e92\u60e0\u7684\u5075\u6e2c\u6548\u80fd\u3002\u6b64\u5916\uff0c\u57fa\u65bc\u63d0\u793a\u7684\u5206\u5272\u6a21\u7d44\u6539\u9032\u4e86\u4efb\u610f\u66f2\u7387\u548c\u8907\u96dc\u7248\u9762\u7684\u6587\u5b57\u7684\u5075\u6e2c\u7d50\u679c\uff0c\u5f9e\u800c\u63d0\u9ad8\u4e86 DAT \u7684\u6e96\u78ba\u6027\u4e26\u64f4\u5c55\u4e86\u5176\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u7684\u9069\u7528\u6027\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cDAT \u5728\u5404\u7a2e\u8207\u6587\u5b57\u76f8\u95dc\u7684\u57fa\u6e96\u6e2c\u8a66\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u5305\u62ec\u591a\u65b9\u5411/\u4efb\u610f\u5f62\u72c0\u7684\u5834\u666f\u6587\u5b57\u5075\u6e2c\u3001\u6587\u4ef6\u7248\u9762\u5206\u6790\u548c\u9801\u9762\u5075\u6e2c\u4efb\u52d9\u3002", "author": "Xingyu Wan et.al.", "authors": "Xingyu Wan, Chengquan Zhang, Pengyuan Lyu, Sen Fan, Zihan Ni, Kun Yao, Errui Ding, Jingdong Wang", "id": "2405.19765v1", "paper_url": "http://arxiv.org/abs/2405.19765v1", "repo": "null"}}