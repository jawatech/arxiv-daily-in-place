{"2501.19353": {"publish_time": "2025-01-31", "title": "Do Large Multimodal Models Solve Caption Generation for Scientific Figures? Lessons Learned from SCICAP Challenge 2023", "paper_summary": "Since the SCICAP datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SCICAP Challenge took place, inviting global teams\nto use an expanded SCICAP dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SCICAP\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?", "paper_summary_zh": "\u81ea 2021 \u5e74 SCICAP \u8cc7\u6599\u96c6\u63a8\u51fa\u4ee5\u4f86\uff0c\u7814\u7a76\u793e\u7fa4\u5728\u70ba\u5b78\u8853\u671f\u520a\u4e2d\u7684\u79d1\u5b78\u5716\u8868\u7522\u751f\u6a19\u984c\u65b9\u9762\u53d6\u5f97\u4e86\u986f\u8457\u9032\u5c55\u30022023 \u5e74\uff0c\u7b2c\u4e00\u5c46 SCICAP \u6311\u6230\u8cfd\u8209\u884c\uff0c\u9080\u8acb\u5168\u7403\u5718\u968a\u4f7f\u7528\u64f4\u5145\u7684 SCICAP \u8cc7\u6599\u96c6\uff0c\u70ba\u5404\u500b\u5b78\u8853\u9818\u57df\u4e2d\u4e0d\u540c\u7684\u5716\u8868\u985e\u578b\u958b\u767c\u6a19\u984c\u6a21\u578b\u3002\u540c\u6642\uff0c\u6587\u5b57\u751f\u6210\u6a21\u578b\u5feb\u901f\u9032\u6b65\uff0c\u51fa\u73fe\u8a31\u591a\u529f\u80fd\u5f37\u5927\u7684\u9810\u8a13\u7df4\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM)\uff0c\u5728\u5404\u7a2e\u8996\u89ba\u548c\u8a9e\u8a00\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u672c\u6587\u6982\u8ff0\u4e86\u7b2c\u4e00\u5c46 SCICAP \u6311\u6230\u8cfd\uff0c\u4e26\u8a73\u7d30\u8aaa\u660e\u4e86\u5404\u7a2e\u6a21\u578b\u5728\u5176\u8cc7\u6599\u4e0a\u7684\u8868\u73fe\uff0c\u6355\u6349\u4e86\u8a72\u9818\u57df\u7684\u73fe\u6cc1\u3002\u6211\u5011\u767c\u73fe\uff0c\u5c08\u696d\u7de8\u8f2f\u4eba\u54e1\u666e\u904d\u504f\u597d\u7531 GPT-4V \u751f\u6210\u7684\u5716\u8868\u6a19\u984c\uff0c\u52dd\u904e\u6240\u6709\u5176\u4ed6\u6a21\u578b\uff0c\u751a\u81f3\u52dd\u904e\u4f5c\u8005\u64b0\u5beb\u7684\u539f\u59cb\u6a19\u984c\u3002\u6839\u64da\u9019\u4e00\u95dc\u9375\u767c\u73fe\uff0c\u6211\u5011\u9032\u884c\u4e86\u8a73\u7d30\u5206\u6790\uff0c\u4ee5\u56de\u7b54\u9019\u500b\u554f\u984c\uff1a\u5148\u9032\u7684 LMM \u662f\u5426\u5df2\u89e3\u6c7a\u70ba\u79d1\u5b78\u5716\u8868\u7522\u751f\u6a19\u984c\u7684\u4efb\u52d9\uff1f", "author": "Ting-Yao E. Hsu et.al.", "authors": "Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang", "id": "2501.19353v1", "paper_url": "http://arxiv.org/abs/2501.19353v1", "repo": "null"}}