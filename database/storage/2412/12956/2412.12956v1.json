{"2412.12956": {"publish_time": "2024-12-17", "title": "SnakModel: Lessons Learned from Training an Open Danish Large Language Model", "paper_summary": "We present SnakModel, a Danish large language model (LLM) based on Llama2-7B,\nwhich we continuously pre-train on 13.6B Danish words, and further tune on 3.7M\nDanish instructions. As best practices for creating LLMs for smaller language\ncommunities have yet to be established, we examine the effects of early\nmodeling and training decisions on downstream performance throughout the entire\ntraining pipeline, including (1) the creation of a strictly curated corpus of\nDanish text from diverse sources; (2) the language modeling and\ninstruction-tuning training process itself, including the analysis of\nintermediate training dynamics, and ablations across different hyperparameters;\n(3) an evaluation on eight language and culturally-specific tasks. Across these\nexperiments SnakModel achieves the highest overall performance, outperforming\nmultiple contemporary Llama2-7B-based models. By making SnakModel, the majority\nof our pre-training corpus, and the associated code available under open\nlicenses, we hope to foster further research and development in Danish Natural\nLanguage Processing, and establish training guidelines for languages with\nsimilar resource constraints.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa SnakModel\uff0c\u4e00\u500b\u57fa\u65bc Llama2-7B \u7684\u4e39\u9ea5\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\n\u6211\u5011\u6301\u7e8c\u5c0d 13.6B \u500b\u4e39\u9ea5\u8a9e\u55ae\u5b57\u9032\u884c\u9810\u8a13\u7df4\uff0c\u4e26\u9032\u4e00\u6b65\u91dd\u5c0d 3.7M\n\u500b\u4e39\u9ea5\u8a9e\u6307\u4ee4\u9032\u884c\u5fae\u8abf\u3002\u7531\u65bc\u76ee\u524d\u5c1a\u672a\u5efa\u7acb\u91dd\u5c0d\u8f03\u5c0f\u8a9e\u8a00\u793e\u7fa4\u5efa\u7acb LLM \u7684\u6700\u4f73\u5be6\u52d9\uff0c\u6211\u5011\u6aa2\u67e5\u4e86\u65e9\u671f\n\u5efa\u6a21\u548c\u8a13\u7df4\u6c7a\u7b56\u5c0d\u6574\u500b\u8a13\u7df4\u7ba1\u7dda\u4e2d\u4e0b\u6e38\u6548\u80fd\u7684\u5f71\u97ff\uff0c\u5305\u62ec (1) \u5f9e\u4e0d\u540c\u4f86\u6e90\u5efa\u7acb\u56b4\u683c\u7b56\u5283\u7684\u4e39\u9ea5\u8a9e\u8a9e\u6599\u5eab\uff1b(2) \u8a9e\u8a00\u5efa\u6a21\u548c\n\u6307\u4ee4\u5fae\u8abf\u8a13\u7df4\u6d41\u7a0b\u672c\u8eab\uff0c\u5305\u62ec\u5c0d\u4e2d\u9593\u8a13\u7df4\u52d5\u614b\u7684\u5206\u6790\uff0c\u4ee5\u53ca\u4e0d\u540c\u8d85\u53c3\u6578\u7684\u6d88\u878d\uff1b\n(3) \u5c0d\u516b\u7a2e\u8a9e\u8a00\u548c\u6587\u5316\u7279\u5b9a\u4efb\u52d9\u7684\u8a55\u4f30\u3002\u5728\u9019\u4e9b\n\u5be6\u9a57\u4e2d\uff0cSnakModel \u9054\u5230\u4e86\u6700\u9ad8\u7684\u6574\u9ad4\u6548\u80fd\uff0c\u8d85\u8d8a\u4e86\u591a\u500b\u7576\u4ee3\u57fa\u65bc Llama2-7B \u7684\u6a21\u578b\u3002\u900f\u904e\u8b93 SnakModel\u3001\u6211\u5011\u5927\u90e8\u5206\u7684\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\uff0c\u4ee5\u53ca\u76f8\u95dc\u7a0b\u5f0f\u78bc\u5728\u958b\u653e\n\u6388\u6b0a\u4e0b\u53ef\u7528\uff0c\u6211\u5011\u5e0c\u671b\u4fc3\u9032\u4e39\u9ea5\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u9032\u4e00\u6b65\u7814\u7a76\u548c\u958b\u767c\uff0c\u4e26\u70ba\u8cc7\u6e90\u53d7\u9650\u7684\u8a9e\u8a00\u5efa\u7acb\u8a13\u7df4\u6e96\u5247\u3002", "author": "Mike Zhang et.al.", "authors": "Mike Zhang, Max M\u00fcller-Eberstein, Elisa Bassignana, Rob van der Goot", "id": "2412.12956v1", "paper_url": "http://arxiv.org/abs/2412.12956v1", "repo": "https://github.com/nlpnorth/snakmodel"}}