{"2406.08222": {"publish_time": "2024-06-12", "title": "A Sociotechnical Lens for Evaluating Computer Vision Models: A Case Study on Detecting and Reasoning about Gender and Emotion", "paper_summary": "In the evolving landscape of computer vision (CV) technologies, the automatic\ndetection and interpretation of gender and emotion in images is a critical area\nof study. This paper investigates social biases in CV models, emphasizing the\nlimitations of traditional evaluation metrics such as precision, recall, and\naccuracy. These metrics often fall short in capturing the complexities of\ngender and emotion, which are fluid and culturally nuanced constructs. Our\nstudy proposes a sociotechnical framework for evaluating CV models,\nincorporating both technical performance measures and considerations of social\nfairness. Using a dataset of 5,570 images related to vaccination and climate\nchange, we empirically compared the performance of various CV models, including\ntraditional models like DeepFace and FER, and generative models like GPT-4\nVision. Our analysis involved manually validating the gender and emotional\nexpressions in a subset of images to serve as benchmarks. Our findings reveal\nthat while GPT-4 Vision outperforms other models in technical accuracy for\ngender classification, it exhibits discriminatory biases, particularly in\nresponse to transgender and non-binary personas. Furthermore, the model's\nemotion detection skew heavily towards positive emotions, with a notable bias\ntowards associating female images with happiness, especially when prompted by\nmale personas. These findings underscore the necessity of developing more\ncomprehensive evaluation criteria that address both validity and discriminatory\nbiases in CV models. Our proposed framework provides guidelines for researchers\nto critically assess CV tools, ensuring their application in communication\nresearch is both ethical and effective. The significant contribution of this\nstudy lies in its emphasis on a sociotechnical approach, advocating for CV\ntechnologies that support social good and mitigate biases rather than\nperpetuate them.", "paper_summary_zh": "\u5728\u4e0d\u65b7\u6f14\u9032\u7684\u96fb\u8166\u8996\u89ba (CV) \u6280\u8853\u9818\u57df\u4e2d\uff0c\u81ea\u52d5\u5075\u6e2c\u548c\u89e3\u8b80\u5716\u50cf\u4e2d\u7684\u6027\u5225\u548c\u60c5\u7dd2\u662f\u4e00\u9805\u91cd\u8981\u7684\u7814\u7a76\u9818\u57df\u3002\u672c\u6587\u63a2\u8a0e CV \u6a21\u578b\u4e2d\u7684\u793e\u6703\u504f\u898b\uff0c\u5f37\u8abf\u50b3\u7d71\u8a55\u4f30\u6307\u6a19\uff08\u4f8b\u5982\u6e96\u78ba\u5ea6\u3001\u53ec\u56de\u7387\u548c\u6e96\u78ba\u6027\uff09\u7684\u5c40\u9650\u6027\u3002\u9019\u4e9b\u6307\u6a19\u901a\u5e38\u7121\u6cd5\u6355\u6349\u6027\u5225\u548c\u60c5\u7dd2\u7684\u8907\u96dc\u6027\uff0c\u800c\u6027\u5225\u548c\u60c5\u7dd2\u662f\u6d41\u52d5\u4e14\u5177\u6709\u6587\u5316\u5dee\u7570\u7684\u5efa\u69cb\u3002\u6211\u5011\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u500b\u793e\u6703\u6280\u8853\u6846\u67b6\u4f86\u8a55\u4f30 CV \u6a21\u578b\uff0c\u7d50\u5408\u6280\u8853\u6027\u80fd\u6307\u6a19\u548c\u793e\u6703\u516c\u5e73\u6027\u8003\u91cf\u3002\u6211\u5011\u4f7f\u7528\u4e00\u500b\u5305\u542b 5,570 \u5f35\u8207\u75ab\u82d7\u63a5\u7a2e\u548c\u6c23\u5019\u8b8a\u9077\u76f8\u95dc\u7684\u5716\u50cf\u7684\u8cc7\u6599\u96c6\uff0c\u5c0d\u5404\u7a2e CV \u6a21\u578b\u7684\u6548\u80fd\u9032\u884c\u5be6\u8b49\u6bd4\u8f03\uff0c\u5305\u62ec DeepFace \u548c FER \u7b49\u50b3\u7d71\u6a21\u578b\uff0c\u4ee5\u53ca GPT-4 Vision \u7b49\u751f\u6210\u6a21\u578b\u3002\u6211\u5011\u7684\u5206\u6790\u6d89\u53ca\u624b\u52d5\u9a57\u8b49\u90e8\u5206\u5716\u50cf\u4e2d\u7684\u6027\u5225\u548c\u60c5\u7dd2\u8868\u9054\uff0c\u4f5c\u70ba\u57fa\u6e96\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u5118\u7ba1 GPT-4 Vision \u5728\u6027\u5225\u5206\u985e\u7684\u6280\u8853\u6e96\u78ba\u5ea6\u65b9\u9762\u512a\u65bc\u5176\u4ed6\u6a21\u578b\uff0c\u4f46\u5b83\u8868\u73fe\u51fa\u6b67\u8996\u6027\u504f\u898b\uff0c\u7279\u5225\u662f\u5728\u56de\u61c9\u8de8\u6027\u5225\u548c\u975e\u4e8c\u5143\u6027\u5225\u89d2\u8272\u6642\u3002\u6b64\u5916\uff0c\u8a72\u6a21\u578b\u7684\u60c5\u7dd2\u5075\u6e2c\u56b4\u91cd\u504f\u5411\u6b63\u9762\u60c5\u7dd2\uff0c\u7279\u5225\u662f\u5728\u7537\u6027\u89d2\u8272\u63d0\u793a\u6642\uff0c\u5c07\u5973\u6027\u5716\u50cf\u8207\u5feb\u6a02\u806f\u7e6b\u8d77\u4f86\u3002\u9019\u4e9b\u767c\u73fe\u5f37\u8abf\u4e86\u5236\u5b9a\u66f4\u5168\u9762\u7684\u8a55\u91cf\u6a19\u6e96\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u89e3\u6c7a CV \u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u548c\u6b67\u8996\u6027\u504f\u898b\u3002\u6211\u5011\u63d0\u51fa\u7684\u6846\u67b6\u70ba\u7814\u7a76\u4eba\u54e1\u63d0\u4f9b\u6e96\u5247\uff0c\u4ee5\u6279\u5224\u6027\u5730\u8a55\u4f30 CV \u5de5\u5177\uff0c\u78ba\u4fdd\u5b83\u5011\u5728\u50b3\u64ad\u7814\u7a76\u4e2d\u7684\u61c9\u7528\u65e2\u7b26\u5408\u502b\u7406\u9053\u5fb7\uff0c\u53c8\u6709\u6548\u3002\u672c\u7814\u7a76\u7684\u91cd\u8981\u8ca2\u737b\u5728\u65bc\u5f37\u8abf\u793e\u6703\u6280\u8853\u65b9\u6cd5\uff0c\u5021\u5c0e CV \u6280\u8853\u652f\u6301\u793e\u6703\u516c\u76ca\u4e26\u6e1b\u8f15\u504f\u898b\uff0c\u800c\u4e0d\u662f\u8b93\u504f\u898b\u6c38\u5b58\u3002", "author": "Sha Luo et.al.", "authors": "Sha Luo, Sang Jung Kim, Zening Duan, Kaiping Chen", "id": "2406.08222v1", "paper_url": "http://arxiv.org/abs/2406.08222v1", "repo": "null"}}