{"2405.18886": {"publish_time": "2024-05-29", "title": "Compressing Large Language Models using Low Rank and Low Precision Decomposition", "paper_summary": "The prohibitive sizes of Large Language Models (LLMs) today make it difficult\nto deploy them on memory-constrained edge devices. This work introduces $\\rm\nCALDERA$ -- a new post-training LLM compression algorithm that harnesses the\ninherent low-rank structure of a weight matrix $\\mathbf{W}$ by approximating it\nvia a low-rank, low-precision decomposition as $\\mathbf{W} \\approx \\mathbf{Q} +\n\\mathbf{L}\\mathbf{R}$. Here, $\\mathbf{L}$ and $\\mathbf{R}$ are low rank\nfactors, and the entries of $\\mathbf{Q}$, $\\mathbf{L}$ and $\\mathbf{R}$ are\nquantized. The model is compressed by substituting each layer with its\n$\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ decomposition, and the zero-shot\nperformance of the compressed model is evaluated. Additionally, $\\mathbf{L}$\nand $\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently\nenhancing the zero-shot performance. $\\rm CALDERA$ obtains this decomposition\nby formulating it as an optimization problem\n$\\min_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\lVert(\\mathbf{Q} +\n\\mathbf{L}\\mathbf{R} - \\mathbf{W})\\mathbf{X}^\\top\\rVert_{\\rm F}^2$, where\n$\\mathbf{X}$ is the calibration data, and $\\mathbf{Q}, \\mathbf{L}, \\mathbf{R}$\nare constrained to be representable using low-precision formats. Theoretical\nupper bounds on the approximation error of $\\rm CALDERA$ are established using\na rank-constrained regression framework, and the tradeoff between compression\nratio and model performance is studied by analyzing the impact of target rank\nand quantization bit budget. Results illustrate that compressing LlaMa-$2$\n$7$B/$70$B and LlaMa-$3$ $8$B models obtained using $\\rm CALDERA$ outperforms\nexisting post-training LLM compression techniques in the regime of less than\n$2.5$ bits per parameter. The implementation is available at:\n\\href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}.", "paper_summary_zh": "\u7531\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u898f\u6a21\u904e\u65bc\u9f90\u5927\uff0c\u56e0\u6b64\u96e3\u4ee5\u5c07\u5176\u90e8\u7f72\u5728\u8a18\u61b6\u9ad4\u53d7\u9650\u7684\u908a\u7de3\u88dd\u7f6e\u4e0a\u3002\u672c\u7814\u7a76\u4ecb\u7d39\u4e86 $\\rm CALDERA$\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u8a13\u7df4\u5f8c LLM \u58d3\u7e2e\u6f14\u7b97\u6cd5\uff0c\u5b83\u5229\u7528\u6b0a\u91cd\u77e9\u9663 $\\mathbf{W}$ \u56fa\u6709\u7684\u4f4e\u79e9\u7d50\u69cb\uff0c\u4e26\u900f\u904e\u4f4e\u79e9\u3001\u4f4e\u7cbe\u5ea6\u7684\u5206\u89e3\u5c07\u5176\u8fd1\u4f3c\u70ba $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$\u3002\u5176\u4e2d\uff0c$\\mathbf{L}$ \u548c $\\mathbf{R}$ \u662f\u4f4e\u79e9\u56e0\u5b50\uff0c\u800c $\\mathbf{Q}$\u3001$\\mathbf{L}$ \u548c $\\mathbf{R}$ \u7684\u689d\u76ee\u5247\u5df2\u91cf\u5316\u3002\u6a21\u578b\u900f\u904e\u5c07\u6bcf\u4e00\u5c64\u66ff\u63db\u70ba\u5176 $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ \u5206\u89e3\u4f86\u9032\u884c\u58d3\u7e2e\uff0c\u4e26\u8a55\u4f30\u58d3\u7e2e\u6a21\u578b\u7684\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\u3002\u6b64\u5916\uff0c$\\mathbf{L}$ \u548c $\\mathbf{R}$ \u5bb9\u6613\u9069\u61c9\u4f4e\u79e9\uff0c\u56e0\u6b64\u589e\u5f37\u4e86\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\u3002$\\rm CALDERA$ \u5c07\u6b64\u5206\u89e3\u8868\u8ff0\u70ba\u6700\u4f73\u5316\u554f\u984c $\\min_{\\mathbf{Q},\\mathbf{L},\\mathbf{R}}\\lVert(\\mathbf{Q} + \\mathbf{L}\\mathbf{R} - \\mathbf{W})\\mathbf{X}^\\top\\rVert_{\\rm F}^2$ \u4f86\u53d6\u5f97\u6b64\u5206\u89e3\uff0c\u5176\u4e2d $\\mathbf{X}$ \u662f\u6821\u6b63\u8cc7\u6599\uff0c\u800c $\\mathbf{Q}, \\mathbf{L}, \\mathbf{R}$ \u5247\u53d7\u9650\u65bc\u4f7f\u7528\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u8868\u793a\u3002$\\rm CALDERA$ \u7684\u8fd1\u4f3c\u8aa4\u5dee\u7684\u7406\u8ad6\u4e0a\u9650\u662f\u4f7f\u7528\u79e9\u7d04\u675f\u8ff4\u6b78\u67b6\u69cb\u5efa\u7acb\u7684\uff0c\u800c\u58d3\u7e2e\u6bd4\u548c\u6a21\u578b\u6548\u80fd\u4e4b\u9593\u7684\u6b0a\u8861\u5247\u662f\u900f\u904e\u5206\u6790\u76ee\u6a19\u79e9\u548c\u91cf\u5316\u4f4d\u5143\u9810\u7b97\u7684\u5f71\u97ff\u4f86\u7814\u7a76\u7684\u3002\u7d50\u679c\u986f\u793a\uff0c\u4f7f\u7528 $\\rm CALDERA$ \u53d6\u5f97\u7684 LlaMa-$2$ $7$B/$70$B \u548c LlaMa-$3$ $8$B \u6a21\u578b\u7684\u58d3\u7e2e\u512a\u65bc\u73fe\u6709\u7684\u8a13\u7df4\u5f8c LLM \u58d3\u7e2e\u6280\u8853\uff0c\u6548\u80fd\u4f4e\u65bc\u6bcf\u53c3\u6578 $2.5$ \u4f4d\u5143\u3002\u5be6\u4f5c\u53ef\u65bc\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1a\\href{https://github.com/pilancilab/caldera}{https://github.com/pilancilab/caldera}\u3002", "author": "Rajarshi Saha et.al.", "authors": "Rajarshi Saha, Naomi Sagan, Varun Srivastava, Andrea J. Goldsmith, Mert Pilanci", "id": "2405.18886v1", "paper_url": "http://arxiv.org/abs/2405.18886v1", "repo": "null"}}