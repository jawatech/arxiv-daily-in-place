{"2410.04798": {"publish_time": "2024-10-07", "title": "DAPE V2: Process Attention Score as Feature Map for Length Extrapolation", "paper_summary": "The attention mechanism is a fundamental component of the Transformer model,\ncontributing to interactions among distinct tokens, in contrast to earlier\nfeed-forward neural networks. In general, the attention scores are determined\nsimply by the key-query products. However, this work's occasional trial\n(combining DAPE and NoPE) of including additional MLPs on attention scores\nwithout position encoding indicates that the classical key-query multiplication\nmay limit the performance of Transformers. In this work, we conceptualize\nattention as a feature map and apply the convolution operator (for neighboring\nattention scores across different heads) to mimic the processing methods in\ncomputer vision. Specifically, the main contribution of this paper is\nidentifying and interpreting the Transformer length extrapolation problem as a\nresult of the limited expressiveness of the naive query and key dot product,\nand we successfully translate the length extrapolation issue into a\nwell-understood feature map processing problem. The novel insight, which can be\nadapted to various attention-related models, reveals that the current\nTransformer architecture has the potential for further evolution. Extensive\nexperiments demonstrate that treating attention as a feature map and applying\nconvolution as a processing method significantly enhances Transformer\nperformance.", "paper_summary_zh": "\u6ce8\u610f\u529b\u673a\u5236\u662f Transformer \u6a21\u578b\u7684\u57fa\u672c\u7d44\u6210\u90e8\u5206\uff0c\u8207\u65e9\u671f\u7684\u524d\u994b\u795e\u7d93\u7db2\u8def\u4e0d\u540c\uff0c\u5b83\u6709\u52a9\u65bc\u4e0d\u540c\u7b26\u865f\u4e4b\u9593\u7684\u4e92\u52d5\u3002\u4e00\u822c\u4f86\u8aaa\uff0c\u6ce8\u610f\u529b\u5206\u6578\u50c5\u7531\u9375\u67e5\u8a62\u4e58\u7a4d\u6c7a\u5b9a\u3002\u7136\u800c\uff0c\u9019\u9805\u5de5\u4f5c\u5076\u723e\u5617\u8a66\uff08\u7d50\u5408 DAPE \u548c NoPE\uff09\u5728\u6ce8\u610f\u529b\u5206\u6578\u4e2d\u52a0\u5165\u984d\u5916\u7684 MLP\uff0c\u800c\u6c92\u6709\u4f4d\u7f6e\u7de8\u78bc\uff0c\u9019\u8868\u793a\u50b3\u7d71\u7684\u9375\u67e5\u8a62\u4e58\u6cd5\u53ef\u80fd\u6703\u9650\u5236 Transformer \u7684\u6548\u80fd\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c07\u6ce8\u610f\u529b\u6982\u5ff5\u5316\u70ba\u7279\u5fb5\u5716\uff0c\u4e26\u5957\u7528\u5377\u7a4d\u904b\u7b97\u5b50\uff08\u91dd\u5c0d\u4e0d\u540c\u982d\u90e8\u4e4b\u9593\u7684\u9130\u8fd1\u6ce8\u610f\u529b\u5206\u6578\uff09\u4f86\u6a21\u64ec\u96fb\u8166\u8996\u89ba\u4e2d\u7684\u8655\u7406\u65b9\u6cd5\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u672c\u6587\u7684\u4e3b\u8981\u8ca2\u737b\u662f\u5c07 Transformer \u9577\u5ea6\u5916\u63a8\u554f\u984c\u8b58\u5225\u4e26\u8a6e\u91cb\u70ba\u55ae\u7d14\u7684\u67e5\u8a62\u548c\u9375\u9ede\u7a4d\u8868\u73fe\u529b\u6709\u9650\u7684\u7d50\u679c\uff0c\u800c\u4e14\u6211\u5011\u6210\u529f\u5730\u5c07\u9577\u5ea6\u5916\u63a8\u554f\u984c\u8f49\u63db\u70ba\u4e00\u500b\u7406\u89e3\u826f\u597d\u7684\u7279\u5fb5\u5716\u8655\u7406\u554f\u984c\u3002\u9019\u500b\u65b0\u7a4e\u7684\u898b\u89e3\u53ef\u4ee5\u8abf\u6574\u70ba\u5404\u7a2e\u8207\u6ce8\u610f\u529b\u76f8\u95dc\u7684\u6a21\u578b\uff0c\u5b83\u63ed\u793a\u4e86\u76ee\u524d\u7684 Transformer \u67b6\u69cb\u6709\u9032\u4e00\u6b65\u6f14\u5316\u7684\u6f5b\u529b\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u5c07\u6ce8\u610f\u529b\u8996\u70ba\u7279\u5fb5\u5716\u4e26\u5957\u7528\u5377\u7a4d\u4f5c\u70ba\u8655\u7406\u65b9\u6cd5\uff0c\u53ef\u4ee5\u986f\u8457\u63d0\u5347 Transformer \u7684\u6548\u80fd\u3002", "author": "Chuanyang Zheng et.al.", "authors": "Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li", "id": "2410.04798v1", "paper_url": "http://arxiv.org/abs/2410.04798v1", "repo": "null"}}