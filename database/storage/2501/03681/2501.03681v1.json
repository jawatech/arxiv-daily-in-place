{"2501.03681": {"publish_time": "2025-01-07", "title": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language Alignment", "paper_summary": "Despite the significant improvements achieved by large language models (LLMs)\nin English reasoning tasks, these models continue to struggle with multilingual\nreasoning. Recent studies leverage a full-parameter and two-stage training\nparadigm to teach models to first understand non-English questions and then\nreason. However, this method suffers from both substantial computational\nresource computing and catastrophic forgetting. The fundamental cause is that,\nwith the primary goal of enhancing multilingual comprehension, an excessive\nnumber of irrelevant layers and parameters are tuned during the first stage.\nGiven our findings that the representation learning of languages is merely\nconducted in lower-level layers, we propose an efficient multilingual reasoning\nalignment approach that precisely identifies and fine-tunes the layers\nresponsible for handling multilingualism. Experimental results show that our\nmethod, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of\nall parameters within 7B and 13B LLMs, achieving superior average performance\nthan all strong baselines across 10 languages. Meanwhile, SLAM only involves\none training stage, reducing training time by 4.1-11.9 compared to the\ntwo-stage method.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u82f1\u6587\u63a8\u7406\u4efb\u52d9\u4e2d\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u9019\u4e9b\u6a21\u578b\u5728\u591a\u8a9e\u8a00\u63a8\u7406\u4e2d\u4ecd\u9762\u81e8\u6311\u6230\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5229\u7528\u5168\u53c3\u6578\u548c\u5169\u968e\u6bb5\u8a13\u7df4\u7bc4\u4f8b\uff0c\u6559\u5c0e\u6a21\u578b\u5148\u7406\u89e3\u975e\u82f1\u6587\u554f\u984c\uff0c\u7136\u5f8c\u63a8\u7406\u3002\u7136\u800c\uff0c\u6b64\u65b9\u6cd5\u65e2\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\uff0c\u53c8\u6703\u767c\u751f\u707d\u96e3\u6027\u907a\u5fd8\u3002\u6839\u672c\u539f\u56e0\u5728\u65bc\uff0c\u7531\u65bc\u4e3b\u8981\u76ee\u6a19\u662f\u589e\u5f37\u591a\u8a9e\u8a00\u7406\u89e3\uff0c\u56e0\u6b64\u5728\u7b2c\u4e00\u968e\u6bb5\u8abf\u6574\u4e86\u904e\u591a\u7684\u7121\u95dc\u5c64\u548c\u53c3\u6578\u3002\u9451\u65bc\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u8a9e\u8a00\u7684\u8868\u5fb5\u5b78\u7fd2\u50c5\u5728\u8f03\u4f4e\u5c64\u7d1a\u9032\u884c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u9ad8\u6548\u7684\u591a\u8a9e\u8a00\u63a8\u7406\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7cbe\u78ba\u8b58\u5225\u4e26\u5fae\u8abf\u8ca0\u8cac\u8655\u7406\u591a\u8a9e\u8a00\u7684\u5c64\u7d1a\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684 SLAM \u65b9\u6cd5\u50c5\u5fae\u8abf 6 \u500b\u5c64\u7d1a\u7684\u524d\u994b\u5b50\u5c64\uff0c\u5305\u62ec 7B \u548c 13B LLM \u4e2d 6.5-8% \u7684\u6240\u6709\u53c3\u6578\uff0c\u5728 10 \u7a2e\u8a9e\u8a00\u4e2d\u9054\u5230\u512a\u65bc\u6240\u6709\u5f37\u5927\u57fa\u6e96\u7684\u5e73\u5747\u6548\u80fd\u3002\u8207\u6b64\u540c\u6642\uff0cSLAM \u53ea\u6d89\u53ca\u4e00\u500b\u8a13\u7df4\u968e\u6bb5\uff0c\u8207\u5169\u968e\u6bb5\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8a13\u7df4\u6642\u9593\u6e1b\u5c11\u4e86 4.1-11.9 \u500d\u3002", "author": "Yuchun Fan et.al.", "authors": "Yuchun Fan, Yongyu Mu, Yilin Wang, Lei Huang, Junhao Ruan, Bei Li, Tong Xiao, Shujian Huang, Xiaocheng Feng, Jingbo Zhu", "id": "2501.03681v1", "paper_url": "http://arxiv.org/abs/2501.03681v1", "repo": "null"}}