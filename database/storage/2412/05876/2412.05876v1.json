{"2412.05876": {"publish_time": "2024-12-08", "title": "MG-3D: Multi-Grained Knowledge-Enhanced 3D Medical Vision-Language Pre-training", "paper_summary": "3D medical image analysis is pivotal in numerous clinical applications.\nHowever, the scarcity of labeled data and limited generalization capabilities\nhinder the advancement of AI-empowered models. Radiology reports are easily\naccessible and can serve as weakly-supervised signals. However, large-scale\nvision-language pre-training (VLP) remains underexplored in 3D medical image\nanalysis. Specifically, the insufficient investigation into multi-grained\nradiology semantics and their correlations across patients leads to\nunderutilization of large-scale volume-report data.\n  Considering intra-patient cross-modal semantic consistency and inter-patient\nsemantic correlations, we propose a multi-task VLP method, MG-3D, pre-trained\non large-scale data (47.1K), addressing the challenges by the following two\naspects: 1) Establishing the correspondence between volume semantics and\nmulti-grained medical knowledge of each patient with cross-modal global\nalignment and complementary modality-guided local reconstruction, ensuring\nintra-patient features of different modalities cohesively represent the same\nsemantic content; 2) Correlating inter-patient visual semantics based on\nfine-grained report correlations across patients, and keeping sensitivity to\nglobal individual differences via contrastive learning, enhancing the\ndiscriminative feature representation. Furthermore, we delve into the scaling\nlaw to explore potential performance improvements. Comprehensive evaluations\nacross nine uni- and cross-modal clinical tasks are carried out to assess model\nefficacy. Extensive experiments on both internal and external datasets\ndemonstrate the superior transferability, scalability, and generalization of\nMG-3D, showcasing its potential in advancing feature representation for 3D\nmedical image analysis. Code will be available:\nhttps://github.com/Xuefeng-Ni/MG-3D.", "paper_summary_zh": "<paragraph>3D \u91ab\u5b78\u5f71\u50cf\u5206\u6790\u5728\u773e\u591a\u81e8\u5e8a\u61c9\u7528\u4e2d\u81f3\u95dc\u91cd\u8981\u3002\n\u7136\u800c\uff0c\u6a19\u8a18\u8cc7\u6599\u7684\u7a00\u7f3a\u548c\u6709\u9650\u7684\u6982\u5316\u80fd\u529b\n\u963b\u7919\u4e86 AI \u8ce6\u80fd\u6a21\u578b\u7684\u9032\u6b65\u3002\u653e\u5c04\u5831\u544a\u5bb9\u6613\u7372\u5f97\uff0c\u53ef\u4ee5\u7528\u4f5c\u5f31\u76e3\u7763\u4fe1\u865f\u3002\u7136\u800c\uff0c\u5927\u898f\u6a21\n\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4 (VLP) \u5728 3D \u91ab\u5b78\u5f71\u50cf\n\u5206\u6790\u4e2d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5c0d\u591a\u7c92\u5ea6\n\u653e\u5c04\u8a9e\u7fa9\u53ca\u5176\u5728\u60a3\u8005\u4e4b\u9593\u7684\u76f8\u95dc\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u5c0e\u81f4\u5927\u898f\u6a21\u9ad4\u7a4d\u5831\u544a\u6578\u64da\u5229\u7528\u4e0d\u8db3\u3002\n\u8003\u616e\u5230\u60a3\u8005\u5167\u90e8\u8de8\u6a21\u614b\u8a9e\u7fa9\u4e00\u81f4\u6027\u548c\u60a3\u8005\u9593\n\u8a9e\u7fa9\u76f8\u95dc\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u591a\u4efb\u52d9 VLP \u65b9\u6cd5 MG-3D\uff0c\u9810\u8a13\u7df4\n\u5728\u5927\u578b\u6578\u64da (47.1K) \u4e0a\uff0c\u901a\u904e\u4ee5\u4e0b\u5169\u500b\u65b9\u9762\u89e3\u6c7a\u6311\u6230\uff1a1) \u5efa\u7acb\u9ad4\u7a4d\u8a9e\u7fa9\u548c\n\u6bcf\u500b\u60a3\u8005\u7684\u591a\u7c92\u5ea6\u91ab\u5b78\u77e5\u8b58\u4e4b\u9593\u7684\u5c0d\u61c9\u95dc\u4fc2\uff0c\u901a\u904e\u8de8\u6a21\u614b\u5168\u5c40\n\u5c0d\u9f4a\u548c\u4e92\u88dc\u6a21\u614b\u5f15\u5c0e\u7684\u5c40\u90e8\u91cd\u5efa\uff0c\u78ba\u4fdd\u4e0d\u540c\u6a21\u614b\u7684\u60a3\u8005\u5167\u90e8\u7279\u5fb5\u4e00\u81f4\u5730\u8868\u793a\u76f8\u540c\u7684\n\u8a9e\u7fa9\u5167\u5bb9\uff1b2) \u57fa\u65bc\u60a3\u8005\u4e4b\u9593\u7684\u7d30\u7c92\u5ea6\u5831\u544a\u76f8\u95dc\u6027\u5c0d\u60a3\u8005\u9593\u7684\u8996\u89ba\u8a9e\u7fa9\u9032\u884c\u95dc\u806f\uff0c\u4e26\u901a\u904e\u5c0d\u6bd4\u5b78\u7fd2\u4fdd\u6301\u5c0d\n\u5168\u5c40\u500b\u9ad4\u5dee\u7570\u7684\u654f\u611f\u6027\uff0c\u589e\u5f37\u5224\u5225\u7279\u5fb5\u8868\u793a\u3002\u6b64\u5916\uff0c\u6211\u5011\u6df1\u5165\u7814\u7a76\u4e86\u64f4\u5c55\n\u5b9a\u5f8b\u4ee5\u63a2\u7d22\u6f5b\u5728\u7684\u6027\u80fd\u6539\u9032\u3002\u8de8\u8d8a\u4e5d\u9805\u55ae\u6a21\u614b\u548c\u8de8\u6a21\u614b\u81e8\u5e8a\u4efb\u52d9\u7684\u7d9c\u5408\u8a55\u4f30\u662f\u9032\u884c\u7684\uff0c\u4ee5\u8a55\u4f30\u6a21\u578b\n\u6548\u80fd\u3002\u5728\u5167\u90e8\u548c\u5916\u90e8\u6578\u64da\u96c6\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\n\u8b49\u660e\u4e86 MG-3D \u7684\u5353\u8d8a\u53ef\u50b3\u905e\u6027\u3001\u53ef\u64f4\u5c55\u6027\u548c\u6cdb\u5316\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63a8\u9032 3D\n\u91ab\u5b78\u5f71\u50cf\u5206\u6790\u7279\u5fb5\u8868\u793a\u65b9\u9762\u7684\u6f5b\u529b\u3002\u4ee3\u78bc\u5c07\u63d0\u4f9b\uff1a\nhttps://github.com/Xuefeng-Ni/MG-3D\u3002</paragraph>", "author": "Xuefeng Ni et.al.", "authors": "Xuefeng Ni, Linshan Wu, Jiaxin Zhuang, Qiong Wang, Mingxiang Wu, Varut Vardhanabhuti, Lihai Zhang, Hanyu Gao, Hao Chen", "id": "2412.05876v1", "paper_url": "http://arxiv.org/abs/2412.05876v1", "repo": "https://github.com/xuefeng-ni/mg-3d"}}