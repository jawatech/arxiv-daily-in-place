{"2405.18729": {"publish_time": "2024-05-29", "title": "Preferred-Action-Optimized Diffusion Policies for Offline Reinforcement Learning", "paper_summary": "Offline reinforcement learning (RL) aims to learn optimal policies from\npreviously collected datasets. Recently, due to their powerful representational\ncapabilities, diffusion models have shown significant potential as policy\nmodels for offline RL issues. However, previous offline RL algorithms based on\ndiffusion policies generally adopt weighted regression to improve the policy.\nThis approach optimizes the policy only using the collected actions and is\nsensitive to Q-values, which limits the potential for further performance\nenhancement. To this end, we propose a novel preferred-action-optimized\ndiffusion policy for offline RL. In particular, an expressive conditional\ndiffusion model is utilized to represent the diverse distribution of a behavior\npolicy. Meanwhile, based on the diffusion model, preferred actions within the\nsame behavior distribution are automatically generated through the critic\nfunction. Moreover, an anti-noise preference optimization is designed to\nachieve policy improvement by using the preferred actions, which can adapt to\nnoise-preferred actions for stable training. Extensive experiments demonstrate\nthat the proposed method provides competitive or superior performance compared\nto previous state-of-the-art offline RL methods, particularly in sparse reward\ntasks such as Kitchen and AntMaze. Additionally, we empirically prove the\neffectiveness of anti-noise preference optimization.", "paper_summary_zh": "\u96e2\u7dda\u5f37 reinforcement learning (RL) \u65e8\u5728\u5f9e\u5148\u524d\u6536\u96c6\u7684\u8cc7\u6599\u96c6\u5b78\u7fd2\u6700\u4f73\u7b56\u7565\u3002\u6700\u8fd1\uff0c\u7531\u65bc\u5176\u5f37\u5927\u7684\u8868\u5fb5\u80fd\u529b\uff0c\u64f4\u6563\u6a21\u578b\u5df2\u986f\u793a\u51fa\u4f5c\u70ba\u96e2\u7dda RL \u554f\u984c\u7684\u7b56\u7565\u6a21\u578b\u7684\u986f\u8457\u6f5b\u529b\u3002\u7136\u800c\uff0c\u57fa\u65bc\u64f4\u6563\u7b56\u7565\u7684\u5148\u524d\u96e2\u7dda RL \u6f14\u7b97\u6cd5\u901a\u5e38\u63a1\u7528\u52a0\u6b0a\u56de\u6b78\u4f86\u6539\u5584\u7b56\u7565\u3002\u6b64\u65b9\u6cd5\u50c5\u4f7f\u7528\u6536\u96c6\u7684\u52d5\u4f5c\u4f86\u6700\u4f73\u5316\u7b56\u7565\uff0c\u4e14\u5c0d Q \u503c\u5f88\u654f\u611f\uff0c\u9019\u9650\u5236\u4e86\u9032\u4e00\u6b65\u6548\u80fd\u63d0\u5347\u7684\u53ef\u80fd\u6027\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7528\u65bc\u96e2\u7dda RL \u7684\u65b0\u7a4e\u504f\u597d\u52d5\u4f5c\u6700\u4f73\u5316\u64f4\u6563\u7b56\u7565\u3002\u7279\u5225\u662f\uff0c\u5229\u7528\u8868\u9054\u5f0f\u689d\u4ef6\u64f4\u6563\u6a21\u578b\u4f86\u8868\u793a\u884c\u70ba\u7b56\u7565\u7684\u591a\u5143\u5206\u4f48\u3002\u540c\u6642\uff0c\u6839\u64da\u64f4\u6563\u6a21\u578b\uff0c\u900f\u904e\u8a55\u8ad6\u529f\u80fd\u81ea\u52d5\u7522\u751f\u76f8\u540c\u884c\u70ba\u5206\u4f48\u4e2d\u7684\u504f\u597d\u52d5\u4f5c\u3002\u6b64\u5916\uff0c\u53cd\u566a\u8072\u504f\u597d\u6700\u4f73\u5316\u65e8\u5728\u900f\u904e\u4f7f\u7528\u504f\u597d\u52d5\u4f5c\u4f86\u5be6\u73fe\u7b56\u7565\u6539\u5584\uff0c\u5b83\u53ef\u4ee5\u9069\u61c9\u7a69\u5b9a\u8a13\u7df4\u7684\u566a\u8072\u504f\u597d\u52d5\u4f5c\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u5148\u524d\u7684\u6700\u5148\u9032\u96e2\u7dda RL \u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5177\u6709\u7af6\u722d\u529b\u6216\u512a\u8d8a\u7684\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u7a00\u758f\u734e\u52f5\u4efb\u52d9\u4e2d\uff0c\u4f8b\u5982 Kitchen \u548c AntMaze\u3002\u6b64\u5916\uff0c\u6211\u5011\u6191\u7d93\u9a57\u8b49\u660e\u4e86\u53cd\u566a\u8072\u504f\u597d\u6700\u4f73\u5316\u7684\u6709\u6548\u6027\u3002", "author": "Tianle Zhang et.al.", "authors": "Tianle Zhang, Jiayi Guan, Lin Zhao, Yihang Li, Dongjiang Li, Zecui Zeng, Lei Sun, Yue Chen, Xuelong Wei, Lusong Li, Xiaodong He", "id": "2405.18729v1", "paper_url": "http://arxiv.org/abs/2405.18729v1", "repo": "null"}}