{"2410.17736": {"publish_time": "2024-10-23", "title": "MojoBench: Language Modeling and Benchmarks for Mojo", "paper_summary": "The recently introduced Mojo programming language (PL) by Modular, has\nreceived significant attention in the scientific community due to its claimed\nsignificant speed boost over Python. Despite advancements in code Large\nLanguage Models (LLMs) across various PLs, Mojo remains unexplored in this\ncontext. To address this gap, we introduce MojoBench, the first framework for\nMojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset\ndesigned for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM\npretrained and finetuned for Mojo code generation, which supports instructions\nin 5 natural languages (NLs). Our results show that Mojo-Coder achieves a\n30-35% performance improvement over leading models like GPT-4o and\nClaude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with\nunderrepresented and unseen PLs, offering potential strategies for enhancing\nmodel adaptability. MojoBench contributes to our understanding of LLM\ncapabilities and limitations in emerging programming paradigms fostering more\nrobust code generation systems.", "paper_summary_zh": "\u6700\u8fd1\u7531 Modular \u63a8\u51fa\u7684 Mojo \u7a0b\u5f0f\u8a9e\u8a00 (PL) \u7531\u65bc\u8072\u7a31\u6bd4 Python \u6709\u986f\u8457\u7684\u52a0\u901f\u63d0\u5347\uff0c\u56e0\u6b64\u5728\u79d1\u5b78\u754c\u5099\u53d7\u95dc\u6ce8\u3002\u5118\u7ba1\u5404\u7a2e PL \u7684\u7a0b\u5f0f\u78bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6709\u9032\u5c55\uff0c\u4f46 Mojo \u5728\u9019\u500b\u80cc\u666f\u4e0b\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63a8\u51fa\u4e86 MojoBench\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u7528\u65bc Mojo \u7a0b\u5f0f\u78bc\u751f\u6210\u7684\u67b6\u69cb\u3002MojoBench \u5305\u542b HumanEval-Mojo\uff0c\u9019\u662f\u4e00\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\uff0c\u7528\u65bc\u8a55\u4f30 Mojo \u4e0a\u7684\u7a0b\u5f0f\u78bc LLM\uff0c\u4ee5\u53ca Mojo-Coder\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u91dd\u5c0d Mojo \u7a0b\u5f0f\u78bc\u751f\u6210\u9032\u884c\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u7684 LLM\uff0c\u5b83\u652f\u63f4 5 \u7a2e\u81ea\u7136\u8a9e\u8a00 (NL) \u7684\u6307\u4ee4\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0cMojo-Coder \u6bd4 GPT-4o \u548c Claude-3.5-Sonnet \u7b49\u9818\u5148\u6a21\u578b\u7684\u6548\u80fd\u63d0\u5347\u4e86 30-35%\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86 LLM \u884c\u70ba\u7684\u898b\u89e3\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u4e0d\u8db3\u548c\u672a\u898b\u904e\u7684 PL\uff0c\u63d0\u4f9b\u4e86\u589e\u5f37\u6a21\u578b\u9069\u61c9\u6027\u7684\u6f5b\u5728\u7b56\u7565\u3002MojoBench \u6709\u52a9\u65bc\u6211\u5011\u4e86\u89e3 LLM \u5728\u65b0\u8208\u7a0b\u5f0f\u8a2d\u8a08\u7bc4\u4f8b\u4e2d\u7684\u80fd\u529b\u548c\u9650\u5236\uff0c\u5f9e\u800c\u4fc3\u9032\u66f4\u5f37\u5927\u7684\u7a0b\u5f0f\u78bc\u751f\u6210\u7cfb\u7d71\u3002", "author": "Nishat Raihan et.al.", "authors": "Nishat Raihan, Joanna C. S. Santos, Marcos Zampieri", "id": "2410.17736v1", "paper_url": "http://arxiv.org/abs/2410.17736v1", "repo": "null"}}