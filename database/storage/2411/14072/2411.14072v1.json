{"2411.14072": {"publish_time": "2024-11-21", "title": "The Master-Slave Encoder Model for Improving Patent Text Summarization: A New Approach to Combining Specifications and Claims", "paper_summary": "In order to solve the problem of insufficient generation quality caused by\ntraditional patent text abstract generation models only originating from patent\nspecifications, the problem of new terminology OOV caused by rapid patent\nupdates, and the problem of information redundancy caused by insufficient\nconsideration of the high professionalism, accuracy, and uniqueness of patent\ntexts, we proposes a patent text abstract generation model (MSEA) based on a\nmaster-slave encoder architecture; Firstly, the MSEA model designs a\nmaster-slave encoder, which combines the instructions in the patent text with\nthe claims as input, and fully explores the characteristics and details between\nthe two through the master-slave encoder; Then, the model enhances the\nconsideration of new technical terms in the input sequence based on the pointer\nnetwork, and further enhances the correlation with the input text by re\nweighing the \"remembered\" and \"for-gotten\" parts of the input sequence from the\nencoder; Finally, an enhanced repetition suppression mechanism for patent text\nwas introduced to ensure accurate and non redundant abstracts generated. On a\npublicly available patent text dataset, compared to the state-of-the-art model,\nImproved Multi-Head Attention Mechanism (IMHAM), the MSEA model achieves an\nimprovement of 0.006, 0.005, and 0.005 in Rouge-1, Rouge-2, and Rouge-L scores,\nrespectively. MSEA leverages the characteristics of patent texts to effectively\nenhance the quality of patent text generation, demonstrating its advancement\nand effectiveness in the experiments.", "paper_summary_zh": "<paragraph>\u70ba\u4e86\u89e3\u6c7a\u50b3\u7d71\u5c08\u5229\u6587\u672c\u6458\u8981\u751f\u6210\u6a21\u578b\u50c5\u6e90\u81ea\u5c08\u5229\u8aaa\u660e\u66f8\u5c0e\u81f4\u751f\u6210\u54c1\u8cea\u4e0d\u8db3\u7684\u554f\u984c\u3001\u5c08\u5229\u66f4\u65b0\u5feb\u901f\u5c0e\u81f4\u65b0\u8853\u8a9e OOV \u7684\u554f\u984c\uff0c\u4ee5\u53ca\u5c0d\u5c08\u5229\u6587\u672c\u5c08\u696d\u6027\u3001\u6e96\u78ba\u6027\u3001\u7368\u7279\u6027\u8003\u91cf\u4e0d\u8db3\u5c0e\u81f4\u8cc7\u8a0a\u5197\u9918\u7684\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc\u4e3b\u5f9e\u7de8\u78bc\u5668\u67b6\u69cb\u7684\u5c08\u5229\u6587\u672c\u6458\u8981\u751f\u6210\u6a21\u578b (MSEA)\uff1b\u9996\u5148\uff0cMSEA \u6a21\u578b\u8a2d\u8a08\u4e00\u500b\u4e3b\u5f9e\u7de8\u78bc\u5668\uff0c\u5c07\u5c08\u5229\u6587\u672c\u4e2d\u7684\u8aaa\u660e\u66f8\u8207\u6b0a\u5229\u8981\u6c42\u66f8\u7d50\u5408\u4f5c\u70ba\u8f38\u5165\uff0c\u4e26\u900f\u904e\u4e3b\u5f9e\u7de8\u78bc\u5668\u5145\u5206\u63a2\u7d22\u5169\u8005\u4e4b\u9593\u7684\u7279\u6027\u8207\u7d30\u7bc0\uff1b\u63a5\u8457\uff0c\u6a21\u578b\u57fa\u65bc pointer network \u589e\u5f37\u8f38\u5165\u5e8f\u5217\u4e2d\u65b0\u6280\u8853\u8a5e\u5f59\u7684\u8003\u91cf\uff0c\u4e26\u900f\u904e\u91cd\u65b0\u52a0\u6b0a\u7de8\u78bc\u5668\u4e2d\u300c\u8a18\u4f4f\u300d\u8207\u300c\u907a\u5fd8\u300d\u7684\u8f38\u5165\u5e8f\u5217\u90e8\u5206\uff0c\u9032\u4e00\u6b65\u63d0\u5347\u8207\u8f38\u5165\u6587\u672c\u7684\u95dc\u806f\u6027\uff1b\u6700\u5f8c\uff0c\u5c0e\u5165\u589e\u5f37\u7684\u5c08\u5229\u6587\u672c\u91cd\u8907\u6291\u5236\u6a5f\u5236\uff0c\u4ee5\u78ba\u4fdd\u751f\u6210\u6458\u8981\u7684\u6e96\u78ba\u6027\u8207\u975e\u5197\u9918\u6027\u3002\u5728\u4e00\u500b\u516c\u958b\u7684\u5c08\u5229\u6587\u672c\u8cc7\u6599\u96c6\u4e0a\uff0c\u8207\u76ee\u524d\u6700\u5148\u9032\u7684\u6a21\u578b Improved Multi-Head Attention Mechanism (IMHAM) \u76f8\u6bd4\uff0cMSEA \u6a21\u578b\u5728 Rouge-1\u3001Rouge-2\u3001Rouge-L \u5206\u6578\u4e0a\u5206\u5225\u63d0\u5347\u4e86 0.006\u30010.005\u30010.005\u3002MSEA \u5145\u5206\u5229\u7528\u5c08\u5229\u6587\u672c\u7684\u7279\u6027\uff0c\u6709\u6548\u63d0\u5347\u5c08\u5229\u6587\u672c\u751f\u6210\u7684\u54c1\u8cea\uff0c\u5728\u5be6\u9a57\u4e2d\u5c55\u73fe\u5176\u5148\u9032\u6027\u8207\u6709\u6548\u6027\u3002</paragraph>", "author": "Shu Zhou et.al.", "authors": "Shu Zhou, Xin Wang, Zhengda Zhou, Haohan Yi, Xuhui Zheng, Hao Wan", "id": "2411.14072v1", "paper_url": "http://arxiv.org/abs/2411.14072v1", "repo": "null"}}