{"2412.10103": {"publish_time": "2024-12-13", "title": "AMuSeD: An Attentive Deep Neural Network for Multimodal Sarcasm Detection Incorporating Bi-modal Data Augmentation", "paper_summary": "Detecting sarcasm effectively requires a nuanced understanding of context,\nincluding vocal tones and facial expressions. The progression towards\nmultimodal computational methods in sarcasm detection, however, faces\nchallenges due to the scarcity of data. To address this, we present AMuSeD\n(Attentive deep neural network for MUltimodal Sarcasm dEtection incorporating\nbi-modal Data augmentation). This approach utilizes the Multimodal Sarcasm\nDetection Dataset (MUStARD) and introduces a two-phase bimodal data\naugmentation strategy. The first phase involves generating varied text samples\nthrough Back Translation from several secondary languages. The second phase\ninvolves the refinement of a FastSpeech 2-based speech synthesis system,\ntailored specifically for sarcasm to retain sarcastic intonations. Alongside a\ncloud-based Text-to-Speech (TTS) service, this Fine-tuned FastSpeech 2 system\nproduces corresponding audio for the text augmentations. We also investigate\nvarious attention mechanisms for effectively merging text and audio data,\nfinding self-attention to be the most efficient for bimodal integration. Our\nexperiments reveal that this combined augmentation and attention approach\nachieves a significant F1-score of 81.0% in text-audio modalities, surpassing\neven models that use three modalities from the MUStARD dataset.", "paper_summary_zh": "\u5075\u6e2c\u8af7\u523a\u9700\u8981\u5c0d\u8108\u7d61\u6709\u7d30\u7dfb\u7684\u7406\u89e3\uff0c\u5305\u62ec\u8a9e\u8abf\u548c\u8868\u60c5\u3002\u7136\u800c\uff0c\u671d\u5411\u591a\u6a21\u614b\u8a08\u7b97\u65b9\u6cd5\u7684\u9032\u5c55\u5728\u8af7\u523a\u5075\u6e2c\u4e0a\u6703\u9762\u81e8\u6311\u6230\uff0c\u56e0\u70ba\u8cc7\u6599\u7a00\u5c11\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 AMuSeD (\u7528\u65bc\u591a\u6a21\u614b\u8af7\u523a\u5075\u6e2c\u7684\u6ce8\u610f\u529b\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\uff0c\u7d50\u5408\u96d9\u6a21\u614b\u8cc7\u6599\u64f4\u5145)\u3002\u6b64\u65b9\u6cd5\u5229\u7528\u591a\u6a21\u614b\u8af7\u523a\u5075\u6e2c\u8cc7\u6599\u96c6 (MUStARD)\uff0c\u4e26\u5f15\u5165\u4e00\u500b\u5169\u968e\u6bb5\u7684\u96d9\u6a21\u614b\u8cc7\u6599\u64f4\u5145\u7b56\u7565\u3002\u7b2c\u4e00\u968e\u6bb5\u5305\u542b\u900f\u904e\u5f9e\u591a\u7a2e\u6b21\u8981\u8a9e\u8a00\u9032\u884c\u53cd\u5411\u7ffb\u8b6f\u4f86\u7522\u751f\u4e0d\u540c\u7684\u6587\u5b57\u7bc4\u4f8b\u3002\u7b2c\u4e8c\u968e\u6bb5\u5305\u542b\u6539\u9032 FastSpeech 2 \u70ba\u57fa\u790e\u7684\u8a9e\u97f3\u5408\u6210\u7cfb\u7d71\uff0c\u7279\u5225\u91dd\u5c0d\u8af7\u523a\u9032\u884c\u8abf\u6574\uff0c\u4ee5\u4fdd\u7559\u8af7\u523a\u6027\u7684\u8a9e\u8abf\u3002\u9019\u500b\u5fae\u8abf\u904e\u7684 FastSpeech 2 \u7cfb\u7d71\u8207\u96f2\u7aef\u6587\u5b57\u8f49\u8a9e\u97f3 (TTS) \u670d\u52d9\u4e00\u8d77\uff0c\u70ba\u6587\u5b57\u64f4\u5145\u7522\u751f\u5c0d\u61c9\u7684\u97f3\u8a0a\u3002\u6211\u5011\u4e5f\u7814\u7a76\u4e86\u5404\u7a2e\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4ee5\u6709\u6548\u5730\u5408\u4f75\u6587\u5b57\u548c\u97f3\u8a0a\u8cc7\u6599\uff0c\u767c\u73fe\u81ea\u6211\u6ce8\u610f\u529b\u5c0d\u65bc\u96d9\u6a21\u614b\u6574\u5408\u6700\u6709\u6548\u7387\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0c\u9019\u7a2e\u7d50\u5408\u64f4\u5145\u548c\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u5728\u6587\u5b57\u97f3\u8a0a\u6a21\u614b\u4e2d\u9054\u5230\u986f\u8457\u7684 F1 \u5206\u6578 81.0%\uff0c\u751a\u81f3\u8d85\u8d8a\u4f7f\u7528 MUStARD \u8cc7\u6599\u96c6\u4e09\u500b\u6a21\u614b\u7684\u6a21\u578b\u3002", "author": "Xiyuan Gao et.al.", "authors": "Xiyuan Gao, Shubhi Bansal, Kushaan Gowda, Zhu Li, Shekhar Nayak, Nagendra Kumar, Matt Coler", "id": "2412.10103v1", "paper_url": "http://arxiv.org/abs/2412.10103v1", "repo": "null"}}