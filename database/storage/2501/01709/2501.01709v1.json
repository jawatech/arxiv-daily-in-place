{"2501.01709": {"publish_time": "2025-01-03", "title": "MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders", "paper_summary": "Visual encoders are fundamental components in vision-language models (VLMs),\neach showcasing unique strengths derived from various pre-trained visual\nfoundation models. To leverage the various capabilities of these encoders,\nrecent studies incorporate multiple encoders within a single VLM, leading to a\nconsiderable increase in computational cost. In this paper, we present\nMixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a novel framework\nthat distills the unique proficiencies of multiple vision encoders into a\nsingle, efficient encoder model. Specifically, to mitigate conflicts and retain\nthe unique characteristics of each teacher encoder, we employ low-rank\nadaptation (LoRA) and mixture-of-experts (MoEs) to selectively activate\nspecialized knowledge based on input features, enhancing both adaptability and\nefficiency. To regularize the KD process and enhance performance, we propose an\nattention-based distillation strategy that adaptively weighs the different\nvisual encoders and emphasizes valuable visual tokens, reducing the burden of\nreplicating comprehensive but distinct features from multiple teachers.\nComprehensive experiments on popular VLMs, such as LLaVA and LLaVA-NeXT,\nvalidate the effectiveness of our method. The code will be released.", "paper_summary_zh": "\u8996\u89ba\u7de8\u78bc\u5668\u662f\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4e2d\u7684\u57fa\u672c\u7d44\u6210\u90e8\u5206\uff0c\n\u6bcf\u500b\u7de8\u78bc\u5668\u90fd\u5c55\u793a\u4e86\u6e90\u81ea\u5404\u7a2e\u9810\u5148\u8a13\u7df4\u7684\u8996\u89ba\n\u57fa\u790e\u6a21\u578b\u7684\u7368\u7279\u512a\u52e2\u3002\u70ba\u4e86\u5229\u7528\u9019\u4e9b\u7de8\u78bc\u5668\u7684\u5404\u7a2e\u529f\u80fd\uff0c\n\u6700\u8fd1\u7684\u7814\u7a76\u5728\u55ae\u4e00 VLM \u4e2d\u7d0d\u5165\u4e86\u591a\u500b\u7de8\u78bc\u5668\uff0c\u5c0e\u81f4\u8a08\u7b97\u6210\u672c\u5927\u5e45\u589e\u52a0\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\n\u6df7\u5408\u8996\u89ba\u7de8\u78bc\u5668\u77e5\u8b58\u8403\u53d6 (MoVE-KD)\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u67b6\u69cb\n\u5b83\u5c07\u591a\u500b\u8996\u89ba\u7de8\u78bc\u5668\u7684\u7368\u7279\u80fd\u529b\u8403\u53d6\u5230\u4e00\u500b\n\u55ae\u4e00\u3001\u9ad8\u6548\u7684\u7de8\u78bc\u5668\u6a21\u578b\u4e2d\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u70ba\u4e86\u7de9\u89e3\u885d\u7a81\u4e26\u4fdd\u7559\n\u6bcf\u500b\u6559\u5e2b\u7de8\u78bc\u5668\u7684\u7368\u7279\u7279\u5fb5\uff0c\u6211\u5011\u63a1\u7528\u4f4e\u79e9\n\u9069\u61c9 (LoRA) \u548c\u5c08\u5bb6\u6df7\u5408 (MoE) \u4f86\u9078\u64c7\u6027\u5730\u6839\u64da\u8f38\u5165\u7279\u5fb5\u555f\u52d5\n\u5c08\u696d\u77e5\u8b58\uff0c\u5f9e\u800c\u63d0\u9ad8\u9069\u61c9\u6027\u548c\n\u6548\u7387\u3002\u70ba\u4e86\u898f\u7bc4 KD \u7a0b\u5e8f\u4e26\u589e\u5f37\u6027\u80fd\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\n\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u8403\u53d6\u7b56\u7565\uff0c\u5b83\u81ea\u9069\u61c9\u5730\u6b0a\u8861\u4e0d\u540c\u7684\n\u8996\u89ba\u7de8\u78bc\u5668\u4e26\u5f37\u8abf\u6709\u50f9\u503c\u7684\u8996\u89ba\u4ee3\u5e63\uff0c\u6e1b\u8f15\u4e86\n\u8907\u88fd\u4f86\u81ea\u591a\u500b\u6559\u5e2b\u7684\u5168\u9762\u4f46\u4e0d\u540c\u7684\u7279\u5fb5\u7684\u8ca0\u64d4\u3002\n\u5c0d\u6d41\u884c\u7684 VLM\uff08\u4f8b\u5982 LLaVA \u548c LLaVA-NeXT\uff09\u9032\u884c\u7684\u5168\u9762\u5be6\u9a57\uff0c\n\u9a57\u8b49\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ee3\u78bc\u5c07\u6703\u767c\u5e03\u3002", "author": "Jiajun Cao et.al.", "authors": "Jiajun Cao, Yuan Zhang, Tao Huang, Ming Lu, Qizhe Zhang, Ruichuan An, Ningning MA, Shanghang Zhang", "id": "2501.01709v1", "paper_url": "http://arxiv.org/abs/2501.01709v1", "repo": "null"}}