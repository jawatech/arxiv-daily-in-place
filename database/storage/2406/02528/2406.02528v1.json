{"2406.02528": {"publish_time": "2024-06-04", "title": "Scalable MatMul-free Language Modeling", "paper_summary": "Matrix multiplication (MatMul) typically dominates the overall computational\ncost of large language models (LLMs). This cost only grows as LLMs scale to\nlarger embedding dimensions and context lengths. In this work, we show that\nMatMul operations can be completely eliminated from LLMs while maintaining\nstrong performance at billion-parameter scales. Our experiments show that our\nproposed MatMul-free models achieve performance on-par with state-of-the-art\nTransformers that require far more memory during inference at a scale up to at\nleast 2.7B parameters. We investigate the scaling laws and find that the\nperformance gap between our MatMul-free models and full precision Transformers\nnarrows as the model size increases. We also provide a GPU-efficient\nimplementation of this model which reduces memory usage by up to 61% over an\nunoptimized baseline during training. By utilizing an optimized kernel during\ninference, our model's memory consumption can be reduced by more than 10x\ncompared to unoptimized models. To properly quantify the efficiency of our\narchitecture, we build a custom hardware solution on an FPGA which exploits\nlightweight operations beyond what GPUs are capable of. We processed\nbillion-parameter scale models at 13W beyond human readable throughput, moving\nLLMs closer to brain-like efficiency. This work not only shows how far LLMs can\nbe stripped back while still performing effectively, but also points at the\ntypes of operations future accelerators should be optimized for in processing\nthe next generation of lightweight LLMs. Our code implementation is available\nat \\url{https://github.com/ridgerchu/matmulfreellm}.", "paper_summary_zh": "\u77e9\u9663\u4e58\u6cd5 (MatMul) \u901a\u5e38\u6703\u4e3b\u5c0e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6574\u9ad4\u904b\u7b97\u6210\u672c\u3002\u96a8\u8457 LLM \u64f4\u5c55\u5230\u66f4\u5927\u7684\u5d4c\u5165\u7dad\u5ea6\u548c\u5167\u5bb9\u9577\u5ea6\uff0c\u6b64\u6210\u672c\u53ea\u6703\u589e\u52a0\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 MatMul \u904b\u7b97\u53ef\u4ee5\u5f9e LLM \u4e2d\u5b8c\u5168\u6d88\u9664\uff0c\u540c\u6642\u5728\u5341\u5104\u500b\u53c3\u6578\u898f\u6a21\u4e0b\u7dad\u6301\u5f37\u52c1\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u63d0\u51fa\u7684\u7121 MatMul \u6a21\u578b\u5728\u63a8\u8ad6\u6642\u9054\u5230\u8207\u6700\u5148\u9032\u7684 Transformer \u76f8\u7576\u7684\u6548\u80fd\uff0c\u800c\u5f8c\u8005\u5728\u898f\u6a21\u81f3\u5c11\u9054\u5230 2.7B \u500b\u53c3\u6578\u6642\u9700\u8981\u66f4\u591a\u7684\u8a18\u61b6\u9ad4\u3002\u6211\u5011\u7814\u7a76\u4e86\u64f4\u5c55\u5b9a\u5f8b\uff0c\u767c\u73fe\u96a8\u8457\u6a21\u578b\u5927\u5c0f\u7684\u589e\u52a0\uff0c\u6211\u5011\u7684\u7121 MatMul \u6a21\u578b\u8207\u5b8c\u5168\u7cbe\u5ea6\u7684 Transformer \u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\u7e2e\u5c0f\u3002\u6211\u5011\u9084\u63d0\u4f9b\u4e86\u6b64\u6a21\u578b\u7684 GPU \u9ad8\u6548\u5be6\u4f5c\uff0c\u5728\u8a13\u7df4\u671f\u9593\u5c07\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86 61%\uff0c\u800c\u512a\u65bc\u672a\u6700\u4f73\u5316\u7684\u57fa\u6e96\u3002\u900f\u904e\u5728\u63a8\u8ad6\u671f\u9593\u4f7f\u7528\u6700\u4f73\u5316\u7684\u6838\u5fc3\uff0c\u8207\u672a\u6700\u4f73\u5316\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u7684\u8a18\u61b6\u9ad4\u6d88\u8017\u91cf\u53ef\u4ee5\u6e1b\u5c11 10 \u500d\u4ee5\u4e0a\u3002\u70ba\u4e86\u9069\u7576\u5730\u91cf\u5316\u6211\u5011\u67b6\u69cb\u7684\u6548\u7387\uff0c\u6211\u5011\u5728 FPGA \u4e0a\u5efa\u69cb\u4e00\u500b\u81ea\u8a02\u786c\u9ad4\u89e3\u6c7a\u65b9\u6848\uff0c\u5229\u7528 GPU \u7121\u6cd5\u57f7\u884c\u7684\u8f15\u91cf\u7d1a\u904b\u7b97\u3002\u6211\u5011\u4ee5 13W \u7684\u901f\u5ea6\u8655\u7406\u4e86\u5341\u5104\u500b\u53c3\u6578\u898f\u6a21\u7684\u6a21\u578b\uff0c\u8d85\u8d8a\u4e86\u4eba\u985e\u53ef\u8b80\u7684\u8655\u7406\u91cf\uff0c\u8b93 LLM \u66f4\u63a5\u8fd1\u5927\u8166\u822c\u7684\u6548\u7387\u3002\u9019\u9805\u5de5\u4f5c\u4e0d\u50c5\u5c55\u793a\u4e86 LLM \u5728\u4ecd\u80fd\u6709\u6548\u57f7\u884c\u7684\u60c5\u6cc1\u4e0b\u53ef\u4ee5\u7cbe\u7c21\u5230\u4ec0\u9ebc\u7a0b\u5ea6\uff0c\u4e5f\u6307\u51fa\u672a\u4f86\u52a0\u901f\u5668\u5728\u8655\u7406\u4e0b\u4e00\u4ee3\u8f15\u91cf\u7d1a LLM \u6642\u61c9\u6700\u4f73\u5316\u7684\u904b\u7b97\u985e\u578b\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5be6\u4f5c\u53ef\u4ee5\u5728 \\url{https://github.com/ridgerchu/matmulfreellm} \u53d6\u5f97\u3002", "author": "Rui-Jie Zhu et.al.", "authors": "Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian", "id": "2406.02528v1", "paper_url": "http://arxiv.org/abs/2406.02528v1", "repo": "https://github.com/ridgerchu/matmulfreellm"}}