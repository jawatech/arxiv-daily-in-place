{"2410.16232": {"publish_time": "2024-10-21", "title": "Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping", "paper_summary": "Sketches are a natural and accessible medium for UI designers to\nconceptualize early-stage ideas. However, existing research on UI/UX automation\noften requires high-fidelity inputs like Figma designs or detailed screenshots,\nlimiting accessibility and impeding efficient design iteration. To bridge this\ngap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art\nVision Language Models (VLMs) on automating the conversion of rudimentary\nsketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code\nsupports interactive agent evaluation that mimics real-world design workflows,\nwhere a VLM-based agent iteratively refines its generations by communicating\nwith a simulated user, either passively receiving feedback instructions or\nproactively asking clarification questions. We comprehensively analyze ten\ncommercial and open-source models, showing that Sketch2Code is challenging for\nexisting VLMs; even the most capable models struggle to accurately interpret\nsketches and formulate effective questions that lead to steady improvement.\nNevertheless, a user study with UI/UX experts reveals a significant preference\nfor proactive question-asking over passive feedback reception, highlighting the\nneed to develop more effective paradigms for multi-turn conversational agents.", "paper_summary_zh": "\u8349\u5716\u662f UI \u8a2d\u8a08\u5e2b\u6982\u5ff5\u5316\u65e9\u671f\u968e\u6bb5\u60f3\u6cd5\u7684\u81ea\u7136\u4e14\u6613\u65bc\u4f7f\u7528\u7684\u5a92\u4ecb\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 UI/UX \u81ea\u52d5\u5316\u7814\u7a76\u901a\u5e38\u9700\u8981\u9ad8\u4fdd\u771f\u8f38\u5165\uff0c\u4f8b\u5982 Figma \u8a2d\u8a08\u6216\u8a73\u7d30\u87a2\u5e55\u622a\u5716\uff0c\u9019\u9650\u5236\u4e86\u53ef\u53ca\u6027\u4e26\u963b\u7919\u4e86\u9ad8\u6548\u7684\u8a2d\u8a08\u8fed\u4ee3\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 Sketch2Code\uff0c\u9019\u662f\u4e00\u500b\u57fa\u6e96\u6e2c\u8a66\uff0c\u7528\u65bc\u8a55\u4f30\u6700\u5148\u9032\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u81ea\u52d5\u5c07\u57fa\u672c\u8349\u5716\u8f49\u63db\u70ba\u7db2\u9801\u539f\u578b\u65b9\u9762\u7684\u80fd\u529b\u3002\u9664\u4e86\u7aef\u5230\u7aef\u57fa\u6e96\u6e2c\u8a66\u4e4b\u5916\uff0cSketch2Code \u9084\u652f\u63f4\u4e92\u52d5\u5f0f\u4ee3\u7406\u8a55\u4f30\uff0c\u6a21\u64ec\u771f\u5be6\u4e16\u754c\u7684\u8a2d\u8a08\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5176\u4e2d\u57fa\u65bc VLM \u7684\u4ee3\u7406\u901a\u904e\u8207\u6a21\u64ec\u4f7f\u7528\u8005\u6e9d\u901a\uff0c\u88ab\u52d5\u63a5\u6536\u56de\u994b\u8aaa\u660e\u6216\u4e3b\u52d5\u8a62\u554f\u6f84\u6e05\u554f\u984c\uff0c\u4ee5\u53cd\u8986\u512a\u5316\u5176\u751f\u6210\u3002\u6211\u5011\u5168\u9762\u5206\u6790\u4e86\u5341\u500b\u5546\u696d\u548c\u958b\u6e90\u6a21\u578b\uff0c\u8868\u660e Sketch2Code \u5c0d\u73fe\u6709\u7684 VLM \u4f86\u8aaa\u5177\u6709\u6311\u6230\u6027\uff1b\u5373\u4f7f\u662f\u6700\u6709\u80fd\u529b\u7684\u6a21\u578b\u4e5f\u5f88\u96e3\u6e96\u78ba\u5730\u89e3\u91cb\u8349\u5716\u4e26\u5236\u5b9a\u6709\u6548\u7684\u554f\u984c\uff0c\u5f9e\u800c\u5c0e\u81f4\u7a69\u6b65\u6539\u9032\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u4e00\u9805\u91dd\u5c0d UI/UX \u5c08\u5bb6\u7684\u4f7f\u7528\u8005\u7814\u7a76\u986f\u793a\uff0c\u8207\u88ab\u52d5\u63a5\u6536\u56de\u994b\u76f8\u6bd4\uff0c\u4e3b\u52d5\u63d0\u554f\u5177\u6709\u986f\u8457\u7684\u504f\u597d\uff0c\u9019\u51f8\u986f\u4e86\u958b\u767c\u66f4\u6709\u6548\u7684\u591a\u8f2a\u5c0d\u8a71\u4ee3\u7406\u7bc4\u4f8b\u7684\u5fc5\u8981\u6027\u3002", "author": "Ryan Li et.al.", "authors": "Ryan Li, Yanzhe Zhang, Diyi Yang", "id": "2410.16232v1", "paper_url": "http://arxiv.org/abs/2410.16232v1", "repo": "null"}}