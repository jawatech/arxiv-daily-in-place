{"2410.01623": {"publish_time": "2024-10-02", "title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?", "paper_summary": "Low-rank training has emerged as a promising approach for reducing memory\nusage in training Large Language Models (LLMs). Previous methods either rely on\ndecomposing weight matrices (e.g., LoRA), or seek to decompose gradient\nmatrices (e.g., GaLore) to ensure reduced memory consumption. However, both of\nthem constrain the training in a low-rank subspace, thus inevitably leading to\nsub-optimal performance. This raises a question: whether it is possible to\nconsistently preserve the low-rank constraint for memory efficiency, while\nachieving full-rank training (i.e., training with full-rank gradients of\nfull-rank weights) to avoid inferior outcomes? In this paper, we propose a new\nplug-and-play training framework for LLMs called Fira, as the first attempt to\nachieve this goal. First, we observe an interesting phenomenon during LLM\ntraining: the scaling impact of adaptive optimizers (e.g., Adam) on the\ngradient norm remains similar from low-rank to full-rank training. Based on\nthis observation, we propose a norm-based scaling method, which utilizes the\nscaling impact of low-rank optimizers as substitutes for that of original\nfull-rank optimizers to enable full-rank training. In this way, we can preserve\nthe low-rank constraint in the optimizer while achieving full-rank training for\nbetter performance. Moreover, we find that there are sudden gradient rises\nduring the optimization process, potentially causing loss spikes. To address\nthis, we further put forward a norm-growth limiter to smooth the gradient via\nregulating the relative increase of gradient norms. Extensive experiments on\nthe pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA\nand GaLore, achieving performance that is comparable to or even better than\nfull-rank training.", "paper_summary_zh": "\u4f4e\u79e9\u8a13\u7df4\u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u53ef\u6e1b\u5c11\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8a13\u7df4\u4e2d\u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u5148\u524d\u7684\u8a13\u7df4\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u5206\u89e3\u6b0a\u91cd\u77e9\u9663 (\u4f8b\u5982 LoRA)\uff0c\u6216\u5c0b\u6c42\u5206\u89e3\u68af\u5ea6\u77e9\u9663 (\u4f8b\u5982 GaLore) \u4ee5\u78ba\u4fdd\u6e1b\u5c11\u8a18\u61b6\u9ad4\u6d88\u8017\u3002\u7136\u800c\uff0c\u9019\u5169\u7a2e\u65b9\u6cd5\u90fd\u5c07\u8a13\u7df4\u9650\u5236\u5728\u4f4e\u79e9\u5b50\u7a7a\u9593\u4e2d\uff0c\u56e0\u6b64\u4e0d\u53ef\u907f\u514d\u5730\u5c0e\u81f4\u6b21\u512a\u6548\u80fd\u3002\u9019\u5f15\u767c\u4e86\u4e00\u500b\u554f\u984c\uff1a\u662f\u5426\u53ef\u4ee5\u5728\u4fdd\u6301\u4f4e\u79e9\u7d04\u675f\u4ee5\u63d0\u9ad8\u8a18\u61b6\u9ad4\u6548\u7387\u7684\u540c\u6642\uff0c\u5be6\u73fe\u5168\u79e9\u8a13\u7df4\uff08\u5373\u4f7f\u7528\u5168\u79e9\u6b0a\u91cd\u7684\u5168\u79e9\u68af\u5ea6\u9032\u884c\u8a13\u7df4\uff09\u4ee5\u907f\u514d\u8f03\u5dee\u7684\u7d50\u679c\uff1f\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba Fira \u7684 LLM \u65b0\u7684\u5373\u63d2\u5373\u7528\u8a13\u7df4\u6846\u67b6\uff0c\u4f5c\u70ba\u5be6\u73fe\u6b64\u76ee\u6a19\u7684\u9996\u6b21\u5617\u8a66\u3002\u9996\u5148\uff0c\u6211\u5011\u5728 LLM \u8a13\u7df4\u671f\u9593\u89c0\u5bdf\u5230\u4e00\u500b\u6709\u8da3\u7684\u73fe\u8c61\uff1a\u81ea\u9069\u61c9\u512a\u5316\u5668\uff08\u4f8b\u5982 Adam\uff09\u5c0d\u68af\u5ea6\u7bc4\u6578\u7684\u7e2e\u653e\u5f71\u97ff\u5728\u4f4e\u79e9\u8a13\u7df4\u5230\u5168\u79e9\u8a13\u7df4\u4e2d\u4fdd\u6301\u76f8\u4f3c\u3002\u57fa\u65bc\u6b64\u89c0\u5bdf\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc\u7bc4\u6578\u7684\u7e2e\u653e\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4f4e\u79e9\u512a\u5316\u5668\u7684\u7e2e\u653e\u5f71\u97ff\u4f5c\u70ba\u539f\u59cb\u5168\u79e9\u512a\u5316\u5668\u7684\u66ff\u4ee3\u54c1\uff0c\u4ee5\u5be6\u73fe\u5168\u79e9\u8a13\u7df4\u3002\u9019\u6a23\uff0c\u6211\u5011\u53ef\u4ee5\u5728\u512a\u5316\u5668\u4e2d\u4fdd\u7559\u4f4e\u79e9\u7d04\u675f\uff0c\u540c\u6642\u5be6\u73fe\u5168\u79e9\u8a13\u7df4\u4ee5\u7372\u5f97\u66f4\u597d\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u512a\u5316\u904e\u7a0b\u4e2d\u5b58\u5728\u7a81\u7136\u7684\u68af\u5ea6\u4e0a\u5347\uff0c\u53ef\u80fd\u6703\u5c0e\u81f4\u640d\u5931\u6fc0\u589e\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u4e86\u7bc4\u6578\u589e\u9577\u9650\u5236\u5668\uff0c\u901a\u904e\u8abf\u7bc0\u68af\u5ea6\u7bc4\u6578\u7684\u76f8\u5c0d\u589e\u52a0\u4f86\u5e73\u6ed1\u68af\u5ea6\u3002\u5728 LLM \u7684\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u4e0a\u7684\u5927\u91cf\u5be6\u9a57\u8868\u660e\uff0cFira \u512a\u65bc LoRA \u548c GaLore\uff0c\u5176\u6548\u80fd\u8207\u5168\u79e9\u8a13\u7df4\u76f8\u7576\u751a\u81f3\u66f4\u597d\u3002", "author": "Xi Chen et.al.", "authors": "Xi Chen, Kaituo Feng, Changsheng Li, Xunhao Lai, Xiangyu Yue, Ye Yuan, Guoren Wang", "id": "2410.01623v1", "paper_url": "http://arxiv.org/abs/2410.01623v1", "repo": "https://github.com/xichen-fy/fira"}}