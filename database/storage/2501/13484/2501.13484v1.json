{"2501.13484": {"publish_time": "2025-01-23", "title": "MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods", "paper_summary": "Mamba is an efficient sequence model that rivals Transformers and\ndemonstrates significant potential as a foundational architecture for various\ntasks. Quantization is commonly used in neural networks to reduce model size\nand computational latency. However, applying quantization to Mamba remains\nunderexplored, and existing quantization methods, which have been effective for\nCNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot\nsuffers a 21% accuracy drop on Vim-T$^\\dagger$ even under W8A8). We have\npioneered the exploration of this issue and identified several key challenges.\nFirst, significant outliers are present in gate projections, output\nprojections, and matrix multiplications. Second, Mamba's unique parallel scan\nfurther amplifies these outliers, leading to uneven and heavy-tailed data\ndistributions. Third, even with the application of the Hadamard transform, the\nvariance across channels in weights and activations still remains inconsistent.\nTo these ends, we propose MambaQuant, a post-training quantization (PTQ)\nframework consisting of: 1) Karhunen-Loeve Transformation (KLT) enhanced\nrotation, rendering the rotation matrix adaptable to diverse channel\ndistributions. 2) Smooth-Fused rotation, which equalizes channel variances and\ncan merge additional parameters into model weights. Experiments show that\nMambaQuant can quantize both weights and activations into 8-bit with less than\n1% accuracy loss for Mamba-based vision and language tasks. To the best of our\nknowledge, MambaQuant is the first comprehensive PTQ design for the Mamba\nfamily, paving the way for further advancements in its application.", "paper_summary_zh": "Mamba \u662f\u4e00\u7a2e\u9ad8\u6548\u7684\u5e8f\u5217\u6a21\u578b\uff0c\u53ef\u8207 Transformers \u76f8\u5ab2\u7f8e\uff0c\u4e26\u5c55\u793a\u51fa\u4f5c\u70ba\u5404\u7a2e\u4efb\u52d9\u57fa\u790e\u67b6\u69cb\u7684\u5de8\u5927\u6f5b\u529b\u3002\u91cf\u5316\u901a\u5e38\u7528\u65bc\u795e\u7d93\u7db2\u8def\uff0c\u4ee5\u6e1b\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u8a08\u7b97\u5ef6\u9072\u3002\u7136\u800c\uff0c\u5c07\u91cf\u5316\u61c9\u7528\u65bc Mamba \u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u800c\u73fe\u6709\u7684\u91cf\u5316\u65b9\u6cd5\uff08\u5c0d CNN \u548c Transformer \u6a21\u578b\u6709\u6548\uff09\u4f3c\u4e4e\u4e0d\u9069\u5408 Mamba \u6a21\u578b\uff08\u4f8b\u5982\uff0c\u5373\u4f7f\u5728 W8A8 \u4e0b\uff0cQuarot \u5728 Vim-T$^\\dagger$ \u4e0a\u7684\u6e96\u78ba\u5ea6\u4e5f\u6703\u4e0b\u964d 21%\uff09\u3002\u6211\u5011\u7387\u5148\u63a2\u7d22\u4e86\u9019\u500b\u554f\u984c\uff0c\u4e26\u627e\u51fa\u5e7e\u500b\u95dc\u9375\u6311\u6230\u3002\u9996\u5148\uff0c\u9598\u9580\u6295\u5f71\u3001\u8f38\u51fa\u6295\u5f71\u548c\u77e9\u9663\u4e58\u6cd5\u4e2d\u5b58\u5728\u986f\u8457\u7570\u5e38\u503c\u3002\u5176\u6b21\uff0cMamba \u7368\u7279\u7684\u4e26\u884c\u6383\u63cf\u9032\u4e00\u6b65\u653e\u5927\u4e86\u9019\u4e9b\u7570\u5e38\u503c\uff0c\u5c0e\u81f4\u6578\u64da\u5206\u4f48\u4e0d\u5747\u4e14\u5c3e\u90e8\u8f03\u91cd\u3002\u7b2c\u4e09\uff0c\u5373\u4f7f\u61c9\u7528 Hadamard \u8b8a\u63db\uff0c\u6b0a\u91cd\u548c\u6fc0\u6d3b\u4e2d\u7684\u901a\u9053\u9593\u5dee\u7570\u4ecd\u7136\u4e0d\u4e00\u81f4\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 MambaQuant\uff0c\u4e00\u500b\u7531\u4ee5\u4e0b\u90e8\u5206\u7d44\u6210\u7684\u8a13\u7df4\u5f8c\u91cf\u5316 (PTQ) \u6846\u67b6\uff1a1) \u5361\u502b-\u7f85\u592b\u8b8a\u63db (KLT) \u589e\u5f37\u65cb\u8f49\uff0c\u4f7f\u65cb\u8f49\u77e9\u9663\u9069\u61c9\u4e0d\u540c\u7684\u901a\u9053\u5206\u4f48\u30022) \u5e73\u6ed1\u878d\u5408\u65cb\u8f49\uff0c\u5b83\u4f7f\u901a\u9053\u65b9\u5dee\u76f8\u7b49\uff0c\u4e26\u4e14\u53ef\u4ee5\u5c07\u984d\u5916\u53c3\u6578\u5408\u4f75\u5230\u6a21\u578b\u6b0a\u91cd\u4e2d\u3002\u5be6\u9a57\u8868\u660e\uff0cMambaQuant \u53ef\u4ee5\u5c07\u6b0a\u91cd\u548c\u6fc0\u6d3b\u91cf\u5316\u70ba 8 \u4f4d\uff0c\u800c\u57fa\u65bc Mamba \u7684\u8996\u89ba\u548c\u8a9e\u8a00\u4efb\u52d9\u7684\u6e96\u78ba\u5ea6\u640d\u5931\u5c0f\u65bc 1%\u3002\u64da\u6211\u5011\u6240\u77e5\uff0cMambaQuant \u662f Mamba \u5bb6\u65cf\u7684\u7b2c\u4e00\u500b\u7d9c\u5408\u6027 PTQ \u8a2d\u8a08\uff0c\u70ba\u5176\u61c9\u7528\u9032\u4e00\u6b65\u767c\u5c55\u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Zukang Xu et.al.", "authors": "Zukang Xu, Yuxuan Yue, Xing Hu, Zhihang Yuan, Zixu Jiang, Zhixuan Chen, Jiangyong Yu, Chen Xu, Sifan Zhou, Dawei Yang", "id": "2501.13484v1", "paper_url": "http://arxiv.org/abs/2501.13484v1", "repo": "https://github.com/mambaquant/mambaquant"}}