{"2409.11538": {"publish_time": "2024-09-17", "title": "Chain-of-Thought Prompting for Speech Translation", "paper_summary": "Large language models (LLMs) have demonstrated remarkable advancements in\nlanguage understanding and generation. Building on the success of text-based\nLLMs, recent research has adapted these models to use speech embeddings for\nprompting, resulting in Speech-LLM models that exhibit strong performance in\nautomatic speech recognition (ASR) and automatic speech translation (AST). In\nthis work, we propose a novel approach to leverage ASR transcripts as prompts\nfor AST in a Speech-LLM built on an encoder-decoder text LLM. The Speech-LLM\nmodel consists of a speech encoder and an encoder-decoder structure\nMegatron-T5. By first decoding speech to generate ASR transcripts and\nsubsequently using these transcripts along with encoded speech for prompting,\nwe guide the speech translation in a two-step process like chain-of-thought\n(CoT) prompting. Low-rank adaptation (LoRA) is used for the T5 LLM for model\nadaptation and shows superior performance to full model fine-tuning.\nExperimental results show that the proposed CoT prompting significantly\nimproves AST performance, achieving an average increase of 2.4 BLEU points\nacross 6 En->X or X->En AST tasks compared to speech prompting alone.\nAdditionally, compared to a related CoT prediction method that predicts a\nconcatenated sequence of ASR and AST transcripts, our method performs better by\nan average of 2 BLEU points.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u5df2\u7d93\u5c55\u73fe\u51fa\u986f\u8457\u7684\u9032\u6b65\u3002\u5efa\u7acb\u5728\u57fa\u65bc\u6587\u5b57\u7684 LLM \u7684\u6210\u529f\u57fa\u790e\u4e0a\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u7d93\u6539\u7de8\u9019\u4e9b\u6a21\u578b\u4ee5\u4f7f\u7528\u8a9e\u97f3\u5d4c\u5165\u9032\u884c\u63d0\u793a\uff0c\u7522\u751f\u5728\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u548c\u81ea\u52d5\u8a9e\u97f3\u7ffb\u8b6f (AST) \u4e2d\u8868\u73fe\u51fa\u5f37\u52c1\u6548\u80fd\u7684 Speech-LLM \u6a21\u578b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528 ASR \u8f49\u9304\u4f5c\u70ba\u63d0\u793a\uff0c\u7528\u65bc\u5efa\u7acb\u5728\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6587\u5b57 LLM \u4e0a\u7684 Speech-LLM \u4e2d\u7684 AST\u3002Speech-LLM \u6a21\u578b\u5305\u542b\u4e00\u500b\u8a9e\u97f3\u7de8\u78bc\u5668\u548c\u4e00\u500b\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u7d50\u69cb Megatron-T5\u3002\u900f\u904e\u5148\u89e3\u78bc\u8a9e\u97f3\u4ee5\u7522\u751f ASR \u8f49\u9304\uff0c\u7136\u5f8c\u5c07\u9019\u4e9b\u8f49\u9304\u8207\u7de8\u78bc\u8a9e\u97f3\u4e00\u8d77\u7528\u65bc\u63d0\u793a\uff0c\u6211\u5011\u6307\u5c0e\u8a9e\u97f3\u7ffb\u8b6f\u9032\u5165\u4e00\u500b\u985e\u4f3c\u65bc\u601d\u8003\u93c8 (CoT) \u63d0\u793a\u7684\u5169\u6b65\u9a5f\u7a0b\u5e8f\u3002\u4f4e\u79e9\u9069\u61c9 (LoRA) \u7528\u65bc T5 LLM \u7684\u6a21\u578b\u9069\u61c9\uff0c\u4e26\u4e14\u986f\u793a\u51fa\u512a\u65bc\u5b8c\u6574\u6a21\u578b\u5fae\u8abf\u7684\u6548\u80fd\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6240\u63d0\u51fa\u7684 CoT \u63d0\u793a\u986f\u8457\u6539\u5584\u4e86 AST \u6548\u80fd\uff0c\u5728 6 \u500b En->X \u6216 X->En AST \u4efb\u52d9\u4e2d\uff0c\u8207\u55ae\u7368\u7684\u8a9e\u97f3\u63d0\u793a\u76f8\u6bd4\uff0c\u5e73\u5747\u589e\u52a0\u4e86 2.4 \u500b BLEU \u9ede\u3002\u6b64\u5916\uff0c\u8207\u9810\u6e2c ASR \u548c AST \u8f49\u9304\u7684\u4e32\u806f\u5e8f\u5217\u7684\u76f8\u95dc CoT \u9810\u6e2c\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u5e73\u5747\u6548\u80fd\u9ad8\u51fa 2 \u500b BLEU \u9ede\u3002", "author": "Ke Hu et.al.", "authors": "Ke Hu, Zhehuai Chen, Chao-Han Huck Yang, Piotr \u017belasko, Oleksii Hrinchuk, Vitaly Lavrukhin, Jagadeesh Balam, Boris Ginsburg", "id": "2409.11538v1", "paper_url": "http://arxiv.org/abs/2409.11538v1", "repo": "null"}}