{"2406.09295": {"publish_time": "2024-06-13", "title": "AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models", "paper_summary": "Evaluating the alignment capabilities of large Vision-Language Models (VLMs)\nis essential for determining their effectiveness as helpful assistants.\nHowever, existing benchmarks primarily focus on basic abilities using nonverbal\nmethods, such as yes-no and multiple-choice questions. In this paper, we\naddress this gap by introducing AlignMMBench, a comprehensive alignment\nbenchmark specifically designed for emerging Chinese VLMs. This benchmark is\nmeticulously curated from real-world scenarios and Chinese Internet sources,\nencompassing thirteen specific tasks across three categories, and includes both\nsingle-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite\nstrategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer\npairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, a\nrule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, we\nreport the performance of representative VLMs on AlignMMBench, offering\ninsights into the capabilities and limitations of different VLM architectures.\nAll evaluation codes and data are available on https://alignmmbench.github.io.", "paper_summary_zh": "\u8a55\u4f30\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u7684\u5c0d\u9f4a\u80fd\u529b\u5c0d\u65bc\u78ba\u5b9a\u5176\u4f5c\u70ba\u6709\u7528\u52a9\u7406\u7684\u6709\u6548\u6027\u81f3\u95dc\u91cd\u8981\u3002\n\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u6e96\u4e3b\u8981\u96c6\u4e2d\u65bc\u4f7f\u7528\u975e\u8a9e\u8a00\u65b9\u6cd5\u7684\u57fa\u672c\u80fd\u529b\uff0c\u4f8b\u5982\u662f\u975e\u984c\u548c\u591a\u9078\u984c\u3002\n\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u901a\u904e\u5f15\u5165 AlignMMBench \u4f86\u89e3\u6c7a\u9019\u4e00\u5dee\u8ddd\uff0cAlignMMBench \u662f\u4e00\u500b\u5c08\u9580\u70ba\u65b0\u8208\u4e2d\u6587 VLM \u8a2d\u8a08\u7684\u7d9c\u5408\u5c0d\u9f4a\u57fa\u6e96\u3002\n\u9019\u500b\u57fa\u6e96\u662f\u5f9e\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u548c\u4e2d\u6587\u7db2\u8def\u4f86\u6e90\u7cbe\u5fc3\u7b56\u5283\u7684\uff0c\u6db5\u84cb\u4e86\u4e09\u500b\u985e\u5225\u4e2d\u7684 13 \u500b\u5177\u9ad4\u4efb\u52d9\uff0c\u5305\u62ec\u55ae\u8f2a\u548c\u591a\u8f2a\u5c0d\u8a71\u5834\u666f\u3002\nAlignMMBench \u63a1\u7528\u63d0\u793a\u91cd\u5beb\u7b56\u7565\uff0c\u5305\u542b 1,054 \u5f35\u5716\u7247\u548c 4,978 \u500b\u554f\u7b54\u5c0d\u3002\n\u70ba\u4e86\u4fc3\u9032\u8a55\u4f30\u7ba1\u9053\uff0c\u6211\u5011\u63d0\u51fa\u4e86 CritiqueVLM\uff0c\u9019\u662f\u4e00\u500b\u898f\u5247\u6821\u6e96\u7684\u8a55\u4f30\u5668\uff0c\u5176\u8a55\u4f30\u80fd\u529b\u8d85\u904e\u4e86 GPT-4\u3002\n\u6700\u5f8c\uff0c\u6211\u5011\u5831\u544a\u4e86\u4ee3\u8868\u6027 VLM \u5728 AlignMMBench \u4e0a\u7684\u8868\u73fe\uff0c\u63d0\u4f9b\u4e86\u5c0d\u4e0d\u540c VLM \u67b6\u69cb\u7684\u80fd\u529b\u548c\u9650\u5236\u7684\u898b\u89e3\u3002\n\u6240\u6709\u8a55\u4f30\u4ee3\u78bc\u548c\u6578\u64da\u90fd\u53ef\u4ee5\u5728 https://alignmmbench.github.io/ \u4e0a\u627e\u5230\u3002", "author": "Yuhang Wu et.al.", "authors": "Yuhang Wu, Wenmeng Yu, Yean Cheng, Yan Wang, Xiaohan Zhang, Jiazheng Xu, Ming Ding, Yuxiao Dong", "id": "2406.09295v1", "paper_url": "http://arxiv.org/abs/2406.09295v1", "repo": "null"}}