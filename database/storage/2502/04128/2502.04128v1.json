{"2502.04128": {"publish_time": "2025-02-06", "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis", "paper_summary": "Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable.", "paper_summary_zh": "\u6700\u8fd1\u5728\u4ee5 GPT \u7cfb\u5217\u548c o1 \u6a21\u578b\u70ba\u9996\u7684\u6587\u672c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e0a\u7684\u9032\u5c55\uff0c\u5df2\u8b49\u660e\u4e86\u64f4\u5145\u8a13\u7df4\u6642\u9593\u548c\u63a8\u8ad6\u6642\u9593\u904b\u7b97\u7684\u6709\u6548\u6027\u3002\u7136\u800c\uff0c\u76ee\u524d\u5229\u7528 LLM \u7684\u6700\u5148\u9032 TTS \u7cfb\u7d71\u901a\u5e38\u662f\u591a\u968e\u6bb5\u7684\uff0c\u9700\u8981\u500b\u5225\u7684\u6a21\u578b\uff08\u4f8b\u5982\uff0cLLM \u4e4b\u5f8c\u7684\u64f4\u6563\u6a21\u578b\uff09\uff0c\u9019\u4f7f\u5f97\u5728\u8a13\u7df4\u6216\u6e2c\u8a66\u671f\u9593\u64f4\u5145\u7279\u5b9a\u6a21\u578b\u7684\u6c7a\u5b9a\u8b8a\u5f97\u8907\u96dc\u3002\u672c\u7814\u7a76\u505a\u51fa\u4e86\u4ee5\u4e0b\u8ca2\u737b\uff1a\u9996\u5148\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u8a9e\u97f3\u5408\u6210\u7684\u8a13\u7df4\u6642\u9593\u548c\u63a8\u8ad6\u6642\u9593\u904b\u7b97\u7684\u64f4\u5145\u3002\u5176\u6b21\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Llasa \u7684\u7c21\u55ae\u67b6\u69cb\uff0c\u7528\u65bc\u8a9e\u97f3\u5408\u6210\uff0c\u63a1\u7528\u55ae\u5c64\u5411\u91cf\u91cf\u5316\u5668 (VQ) \u7de8\u89e3\u78bc\u5668\u548c\u55ae\u4e00 Transformer \u67b6\u69cb\uff0c\u4ee5\u5b8c\u5168\u7b26\u5408 Llama \u7b49\u6a19\u6e96 LLM\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u64f4\u5145 Llasa \u7684\u8a13\u7df4\u6642\u9593\u904b\u7b97\u6301\u7e8c\u6539\u5584\u4e86\u5408\u6210\u8a9e\u97f3\u7684\u81ea\u7136\u6027\uff0c\u4e26\u80fd\u7522\u751f\u66f4\u8907\u96dc\u4e14\u6e96\u78ba\u7684\u97fb\u5f8b\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u5f9e\u64f4\u5145\u63a8\u8ad6\u6642\u9593\u904b\u7b97\u7684\u89d2\u5ea6\u4f86\u770b\uff0c\u6211\u5011\u5728\u641c\u5c0b\u671f\u9593\u63a1\u7528\u8a9e\u97f3\u7406\u89e3\u6a21\u578b\u4f5c\u70ba\u9a57\u8b49\u5668\uff0c\u767c\u73fe\u64f4\u5145\u63a8\u8ad6\u6642\u9593\u904b\u7b97\u5c07\u62bd\u6a23\u6a21\u5f0f\u8f49\u79fb\u5230\u7279\u5b9a\u9a57\u8b49\u5668\u7684\u504f\u597d\uff0c\u5f9e\u800c\u6539\u5584\u60c5\u7dd2\u8868\u9054\u529b\u3001\u97f3\u8272\u4e00\u81f4\u6027\u548c\u5167\u5bb9\u6e96\u78ba\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u516c\u958b\u767c\u5e03\u4e86\u6211\u5011\u7684 TTS \u6a21\u578b (1B\u30013B\u30018B) \u548c\u7de8\u89e3\u78bc\u5668\u6a21\u578b\u7684\u6aa2\u67e5\u9ede\u548c\u8a13\u7df4\u7a0b\u5f0f\u78bc\u3002", "author": "Zhen Ye et.al.", "authors": "Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi DAI, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue", "id": "2502.04128v1", "paper_url": "http://arxiv.org/abs/2502.04128v1", "repo": "null"}}