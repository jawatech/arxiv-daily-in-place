{"2405.03666": {"publish_time": "2024-05-06", "title": "ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection", "paper_summary": "Bimanual manipulation is a longstanding challenge in robotics due to the\nlarge number of degrees of freedom and the strict spatial and temporal\nsynchronization required to generate meaningful behavior. Humans learn bimanual\nmanipulation skills by watching other humans and by refining their abilities\nthrough play. In this work, we aim to enable robots to learn bimanual\nmanipulation behaviors from human video demonstrations and fine-tune them\nthrough interaction. Inspired by seminal work in psychology and biomechanics,\nwe propose modeling the interaction between two hands as a serial kinematic\nlinkage -- as a screw motion, in particular, that we use to define a new action\nspace for bimanual manipulation: screw actions. We introduce ScrewMimic, a\nframework that leverages this novel action representation to facilitate\nlearning from human demonstration and self-supervised policy fine-tuning. Our\nexperiments demonstrate that ScrewMimic is able to learn several complex\nbimanual behaviors from a single human video demonstration, and that it\noutperforms baselines that interpret demonstrations and fine-tune directly in\nthe original space of motion of both arms. For more information and video\nresults, https://robin-lab.cs.utexas.edu/ScrewMimic/", "paper_summary_zh": "", "author": "Arpit Bahety et.al.", "authors": "Arpit Bahety,Priyanka Mandikal,Ben Abbatematteo,Roberto Mart\u00edn-Mart\u00edn", "id": "2405.03666v1", "paper_url": "http://arxiv.org/abs/2405.03666v1", "repo": "null"}}