{"2410.12759": {"publish_time": "2024-10-16", "title": "Unitary Multi-Margin BERT for Robust Natural Language Processing", "paper_summary": "Recent developments in adversarial attacks on deep learning leave many\nmission-critical natural language processing (NLP) systems at risk of\nexploitation. To address the lack of computationally efficient adversarial\ndefense methods, this paper reports a novel, universal technique that\ndrastically improves the robustness of Bidirectional Encoder Representations\nfrom Transformers (BERT) by combining the unitary weights with the multi-margin\nloss. We discover that the marriage of these two simple ideas amplifies the\nprotection against malicious interference. Our model, the unitary multi-margin\nBERT (UniBERT), boosts post-attack classification accuracies significantly by\n5.3% to 73.8% while maintaining competitive pre-attack accuracies. Furthermore,\nthe pre-attack and post-attack accuracy tradeoff can be adjusted via a single\nscalar parameter to best fit the design requirements for the target\napplications.", "paper_summary_zh": "\u6df1\u5ea6\u5b78\u7fd2\u4e2d\u5c0d\u6297\u653b\u64ca\u7684\u6700\u65b0\u767c\u5c55\u8b93\u8a31\u591a\u4efb\u52d9\u95dc\u9375\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u7cfb\u7d71\u9762\u81e8\u88ab\u5229\u7528\u7684\u98a8\u96aa\u3002\u70ba\u4e86\u89e3\u6c7a\u7f3a\u4e4f\u8a08\u7b97\u6548\u7387\u9ad8\u7684\u5c0d\u6297\u9632\u79a6\u65b9\u6cd5\uff0c\u672c\u6587\u5831\u544a\u4e86\u4e00\u7a2e\u65b0\u7a4e\u3001\u901a\u7528\u7684\u6280\u8853\uff0c\u5b83\u900f\u904e\u7d50\u5408\u55ae\u5143\u6b0a\u91cd\u548c\u591a\u908a\u754c\u640d\u5931\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u4f86\u81ea Transformer \u7684\u96d9\u5411\u7de8\u78bc\u5668\u8868\u793a (BERT) \u7684\u7a69\u5065\u6027\u3002\u6211\u5011\u767c\u73fe\u9019\u5169\u500b\u7c21\u55ae\u60f3\u6cd5\u7684\u7d50\u5408\u653e\u5927\u4e86\u5c0d\u60e1\u610f\u5e72\u64fe\u7684\u9632\u8b77\u3002\u6211\u5011\u7684\u6a21\u578b\uff0c\u55ae\u5143\u591a\u908a\u754c BERT (UniBERT)\uff0c\u5c07\u5f8c\u653b\u64ca\u5206\u985e\u6e96\u78ba\u7387\u5927\u5e45\u63d0\u5347\u4e86 5.3% \u81f3 73.8%\uff0c\u540c\u6642\u7dad\u6301\u6709\u7af6\u722d\u529b\u7684\u524d\u653b\u64ca\u6e96\u78ba\u7387\u3002\u6b64\u5916\uff0c\u524d\u653b\u64ca\u548c\u5f8c\u653b\u64ca\u6e96\u78ba\u7387\u7684\u6b0a\u8861\u53ef\u4ee5\u900f\u904e\u55ae\u4e00\u6a19\u91cf\u53c3\u6578\u9032\u884c\u8abf\u6574\uff0c\u4ee5\u6700\u4f73\u7b26\u5408\u76ee\u6a19\u61c9\u7528\u7a0b\u5f0f\u7684\u8a2d\u8a08\u9700\u6c42\u3002", "author": "Hao-Yuan Chang et.al.", "authors": "Hao-Yuan Chang, Kang L. Wang", "id": "2410.12759v1", "paper_url": "http://arxiv.org/abs/2410.12759v1", "repo": "null"}}