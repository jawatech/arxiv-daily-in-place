{"2406.11817": {"publish_time": "2024-06-17", "title": "Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level", "paper_summary": "Direct Preference Optimization (DPO), a standard method for aligning language\nmodels with human preferences, is traditionally applied to offline preferences.\nRecent studies show that DPO benefits from iterative training with online\npreferences labeled by a trained reward model. In this work, we identify a\npitfall of vanilla iterative DPO - improved response quality can lead to\nincreased verbosity. To address this, we introduce iterative length-regularized\nDPO (iLR-DPO) to penalize response length. Our empirical results show that\niLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing\nverbosity. Specifically, our 7B model achieves a $50.5\\%$ length-controlled win\nrate against $\\texttt{GPT-4 Preview}$ on AlpacaEval 2.0, and excels across\nstandard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.\nThese results demonstrate the effectiveness of iterative DPO in aligning\nlanguage models with human feedback.", "paper_summary_zh": "\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\uff08DPO\uff09\u662f\u4e00\u7a2e\u5c07\u8a9e\u8a00\u6a21\u578b\u8207\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u7684\u6a19\u6e96\u65b9\u6cd5\uff0c\u50b3\u7d71\u4e0a\u61c9\u7528\u65bc\u96e2\u7dda\u504f\u597d\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cDPO \u53d7\u76ca\u65bc\u4f7f\u7528\u53d7\u904e\u8a13\u7df4\u7684\u734e\u52f5\u6a21\u578b\u6a19\u8a18\u7684\u7dda\u4e0a\u504f\u597d\u7684\u53cd\u8986\u8a13\u7df4\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u4e86\u9999\u8349\u53cd\u8986 DPO \u7684\u4e00\u500b\u9677\u9631 - \u6539\u5584\u7684\u56de\u61c9\u54c1\u8cea\u53ef\u80fd\u5c0e\u81f4\u5197\u9577\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u53cd\u8986\u9577\u5ea6\u6b63\u898f\u5316\u7684 DPO (iLR-DPO) \u4f86\u61f2\u7f70\u56de\u61c9\u9577\u5ea6\u3002\u6211\u5011\u7684\u5be6\u8b49\u7d50\u679c\u8868\u660e\uff0ciLR-DPO \u53ef\u4ee5\u589e\u5f37 7B \u6a21\u578b\uff0c\u4f7f\u5176\u5728\u4e0d\u589e\u52a0\u5197\u9577\u7684\u60c5\u6cc1\u4e0b\u8207 GPT-4 \u76f8\u5ab2\u7f8e\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684 7B \u6a21\u578b\u5728 AlpacaEval 2.0 \u4e0a\u5be6\u73fe\u4e86\u5c0d $\\texttt{GPT-4 \u9810\u89bd}$ \u7684 $50.5\\%$ \u9577\u5ea6\u63a7\u5236\u52dd\u7387\uff0c\u4e26\u5728\u5305\u62ec MT-Bench\u3001Arena-Hard \u548c OpenLLM \u6392\u884c\u699c\u5728\u5167\u7684\u6a19\u6e96\u57fa\u6e96\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u9019\u4e9b\u7d50\u679c\u8b49\u660e\u4e86\u53cd\u8986 DPO \u5728\u5c07\u8a9e\u8a00\u6a21\u578b\u8207\u4eba\u985e\u56de\u994b\u5c0d\u9f4a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Jie Liu et.al.", "authors": "Jie Liu, Zhanhui Zhou, Jiaheng Liu, Xingyuan Bu, Chao Yang, Han-Sen Zhong, Wanli Ouyang", "id": "2406.11817v1", "paper_url": "http://arxiv.org/abs/2406.11817v1", "repo": "null"}}