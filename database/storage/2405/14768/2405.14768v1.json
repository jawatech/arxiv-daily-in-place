{"2405.14768": {"publish_time": "2024-05-23", "title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models", "paper_summary": "Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code will be\nreleased at https://github.com/zjunlp/EasyEdit.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9700\u8981\u77e5\u8b58\u66f4\u65b0\u624d\u80fd\u6eff\u8db3\u4e0d\u65b7\u589e\u9577\u7684\n\u4e16\u754c\u4e8b\u5be6\u4e26\u4fee\u6b63\u865b\u69cb\u7684\u56de\u61c9\uff0c\u4fc3\u9032\u7d42\u8eab\u6a21\u578b\u7de8\u8f2f\u7684\u65b9\u6cd5\u3002\u66f4\u65b0\u7684\u77e5\u8b58\u5b58\u653e\u5728\u54ea\u500b\u8a18\u61b6\u9ad4\u4e2d\u662f\n\u6a21\u578b\u7de8\u8f2f\u7684\u57fa\u672c\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u7de8\u8f2f\n\u9577\u671f\u8a18\u61b6\uff08\u76f4\u63a5\u6a21\u578b\u53c3\u6578\uff09\u6216\u5de5\u4f5c\u8a18\u61b6\n\uff08\u795e\u7d93\u7db2\u8def\u6fc0\u6d3b/\u8868\u793a\u7684\u975e\u53c3\u6578\u77e5\u8b58\uff0c\u900f\u904e\u64f7\u53d6\uff09\u5c07\u5c0e\u81f4\u4e0d\u53ef\u80fd\u4e09\u89d2\u2014\u2014\u53ef\u9760\u6027\u3001\n\u6982\u5316\u548c\u5c40\u90e8\u6027\u7121\u6cd5\u5728\u7d42\u8eab\n\u7de8\u8f2f\u8a2d\u5b9a\u4e2d\u540c\u6642\u5be6\u73fe\u3002\u5c0d\u65bc\u9577\u671f\u8a18\u61b6\uff0c\u76f4\u63a5\u7de8\u8f2f\u53c3\u6578\u5c07\n\u6703\u8207\u4e0d\u76f8\u95dc\u7684\u9810\u8a13\u7df4\u77e5\u8b58\u6216\u5148\u524d\u7684\u7de8\u8f2f\u7522\u751f\u885d\u7a81\uff08\u53ef\u9760\u6027\u548c\u5c40\u90e8\u6027\u4e0d\u4f73\uff09\u3002\u5c0d\u65bc\u5de5\u4f5c\u8a18\u61b6\uff0c\u57fa\u65bc\u64f7\u53d6\u7684\u6fc0\u6d3b\u96e3\u4ee5\u8b93\u6a21\u578b\u4e86\u89e3\u7de8\u8f2f\u4e26\u6982\u5316\uff08\u6982\u5316\u4e0d\u4f73\uff09\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa WISE \u4f86\u5f4c\u88dc\u8a18\u61b6\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\n\u5728 WISE \u4e2d\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u96d9\u53c3\u6578\u8a18\u61b6\u67b6\u69cb\uff0c\u5176\u4e2d\u5305\u542b\u7528\u65bc\u9810\u8a13\u7df4\u77e5\u8b58\u7684\u4e3b\u8a18\u61b6\u9ad4\u548c\u7528\u65bc\u7de8\u8f2f\u77e5\u8b58\u7684\u5074\u908a\u8a18\u61b6\u9ad4\u3002\n\u6211\u5011\u53ea\u7de8\u8f2f\u5074\u908a\u8a18\u61b6\u9ad4\u4e2d\u7684\u77e5\u8b58\uff0c\u4e26\u8a13\u7df4\u4e00\u500b\u8def\u7531\u5668\u4f86\u6c7a\u5b9a\u5728\u7d66\u5b9a\u67e5\u8a62\u6642\u8981\u901a\u904e\u54ea\u500b\u8a18\u61b6\u9ad4\u3002\u5c0d\u65bc\u6301\u7e8c\u7de8\u8f2f\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u77e5\u8b58\u5206\u7247\u6a5f\u5236\uff0c\u5176\u4e2d\u4e0d\u540c\u7684\u7de8\u8f2f\u7d44\u4f4d\u65bc\u53c3\u6578\u7684\u4e0d\u540c\u5b50\u7a7a\u9593\u4e2d\uff0c\u7136\u5f8c\u5408\u4f75\u5230\u4e00\u500b\u5171\u7528\u8a18\u61b6\u9ad4\u4e2d\uff0c\u800c\u4e0d\u6703\u767c\u751f\u885d\u7a81\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8868\u660e\uff0cWISE \u53ef\u4ee5\u512a\u65bc\u5148\u524d\u7684\n\u6a21\u578b\u7de8\u8f2f\u65b9\u6cd5\uff0c\u4e26\u5728\u554f\u984c\u89e3\u7b54\u3001\u865b\u69cb\u548c\u8da8\u52e2 LLM \u67b6\u69cb\uff08\u4f8b\u5982 GPT\u3001LLaMA \u548c Mistral\uff09\u7684\u5206\u5e03\u5916\u8a2d\u5b9a\u4e0b\u7684\u7d42\u8eab\u6a21\u578b\u7de8\u8f2f\u4e2d\u514b\u670d\u4e0d\u53ef\u80fd\u4e09\u89d2\u3002\u7a0b\u5f0f\u78bc\u5c07\u65bc https://github.com/zjunlp/EasyEdit \u767c\u5e03\u3002", "author": "Peng Wang et.al.", "authors": "Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen", "id": "2405.14768v1", "paper_url": "http://arxiv.org/abs/2405.14768v1", "repo": "https://github.com/zjunlp/easyedit"}}