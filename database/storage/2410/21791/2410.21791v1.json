{"2410.21791": {"publish_time": "2024-10-29", "title": "Enhancing Adversarial Attacks through Chain of Thought", "paper_summary": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains but remain susceptible to safety concerns. Prior research\nindicates that gradient-based adversarial attacks are particularly effective\nagainst aligned LLMs and the chain of thought (CoT) prompting can elicit\ndesired answers through step-by-step reasoning. This paper proposes enhancing\nthe robustness of adversarial attacks on aligned LLMs by integrating CoT\nprompts with the greedy coordinate gradient (GCG) technique. Using CoT triggers\ninstead of affirmative targets stimulates the reasoning abilities of backend\nLLMs, thereby improving the transferability and universality of adversarial\nattacks. We conducted an ablation study comparing our CoT-GCG approach with\nAmazon Web Services auto-cot. Results revealed our approach outperformed both\nthe baseline GCG attack and CoT prompting. Additionally, we used Llama Guard to\nevaluate potentially harmful interactions, providing a more objective risk\nassessment of entire conversations compared to matching outputs to rejection\nphrases. The code of this paper is available at\nhttps://github.com/sujingbo0217/CS222W24-LLM-Attack.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u500b\u9818\u57df\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u5b89\u5168\u554f\u984c\u7684\u5f71\u97ff\u3002\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u57fa\u65bc\u68af\u5ea6\u7684\u5c0d\u6297\u653b\u64ca\u5c0d\u7d93\u904e\u6821\u6e96\u7684 LLM \u7279\u5225\u6709\u6548\uff0c\u800c\u601d\u8003\u93c8 (CoT) \u63d0\u793a\u53ef\u4ee5\u900f\u904e\u9010\u6b65\u63a8\u7406\u5f15\u51fa\u6240\u9700\u7684\u7b54\u6848\u3002\u672c\u6587\u63d0\u51fa\u900f\u904e\u5c07 CoT \u63d0\u793a\u8207\u8caa\u5a6a\u5ea7\u6a19\u68af\u5ea6 (GCG) \u6280\u8853\u6574\u5408\uff0c\u4f86\u52a0\u5f37\u7d93\u904e\u6821\u6e96\u7684 LLM \u5c0d\u6297\u653b\u64ca\u7684\u7a69\u5065\u6027\u3002\u4f7f\u7528 CoT \u89f8\u767c\u5668\uff0c\u800c\u4e0d\u662f\u80af\u5b9a\u76ee\u6a19\uff0c\u6703\u523a\u6fc0\u5f8c\u7aef LLM \u7684\u63a8\u7406\u80fd\u529b\uff0c\u5f9e\u800c\u6539\u5584\u5c0d\u6297\u653b\u64ca\u7684\u53ef\u50b3\u8f38\u6027\u548c\u901a\u7528\u6027\u3002\u6211\u5011\u9032\u884c\u4e86\u4e00\u9805\u6d88\u878d\u7814\u7a76\uff0c\u5c07\u6211\u5011\u7684 CoT-GCG \u65b9\u6cd5\u8207 Amazon Web Services auto-cot \u9032\u884c\u6bd4\u8f03\u3002\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u505a\u6cd5\u512a\u65bc\u57fa\u6e96 GCG \u653b\u64ca\u548c CoT \u63d0\u793a\u3002\u6b64\u5916\uff0c\u6211\u5011\u4f7f\u7528 Llama Guard \u4f86\u8a55\u4f30\u6f5b\u5728\u6709\u5bb3\u7684\u4e92\u52d5\uff0c\u8207\u5c07\u8f38\u51fa\u8207\u62d2\u7d55\u77ed\u8a9e\u9032\u884c\u6bd4\u5c0d\u76f8\u6bd4\uff0c\u9019\u63d0\u4f9b\u4e86\u5c0d\u6574\u500b\u5c0d\u8a71\u7684\u66f4\u5ba2\u89c0\u98a8\u96aa\u8a55\u4f30\u3002\u672c\u6587\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/sujingbo0217/CS222W24-LLM-Attack \u53d6\u5f97\u3002", "author": "Jingbo Su et.al.", "authors": "Jingbo Su", "id": "2410.21791v1", "paper_url": "http://arxiv.org/abs/2410.21791v1", "repo": "https://github.com/sujingbo0217/cs222w24-llm-attack"}}