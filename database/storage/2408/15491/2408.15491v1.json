{"2408.15491": {"publish_time": "2024-08-28", "title": "Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression", "paper_summary": "Large Language Models (LLMs) have garnered widespread attention due to their\nremarkable performance across various tasks. However, to mitigate the issue of\nhallucinations, LLMs often incorporate retrieval-augmented pipeline to provide\nthem with rich external knowledge and context. Nevertheless, challenges stem\nfrom inaccurate and coarse-grained context retrieved from the retriever.\nSupplying irrelevant context to the LLMs can result in poorer responses,\nincreased inference latency, and higher costs. This paper introduces a method\ncalled Instruction-Aware Contextual Compression, which filters out less\ninformative content, thereby accelerating and enhancing the use of LLMs. The\nexperimental results demonstrate that Instruction-Aware Contextual Compression\nnotably reduces memory consumption and minimizes generation latency while\nmaintaining performance levels comparable to those achieved with the use of the\nfull context. Specifically, we achieved a 50% reduction in context-related\ncosts, resulting in a 5% reduction in inference memory usage and a 2.2-fold\nincrease in inference speed, with only a minor drop of 0.047 in Rouge-1. These\nfindings suggest that our method strikes an effective balance between\nefficiency and performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u56e0\u5176\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u51fa\u8272\u8868\u73fe\u800c\u5ee3\u53d7\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u70ba\u4e86\u6e1b\u8f15\u5e7b\u89ba\u554f\u984c\uff0cLLM \u901a\u5e38\u6703\u6574\u5408\u6aa2\u7d22\u589e\u5f37\u7ba1\u9053\uff0c\u70ba\u5b83\u5011\u63d0\u4f9b\u8c50\u5bcc\u7684\u5916\u90e8\u77e5\u8b58\u548c\u80cc\u666f\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u6311\u6230\u4f86\u81ea\u6aa2\u7d22\u5668\u6aa2\u7d22\u5230\u7684\u4e0d\u6e96\u78ba\u4e14\u7c97\u7565\u7684\u80cc\u666f\u3002\u5411 LLM \u63d0\u4f9b\u7121\u95dc\u7684\u80cc\u666f\u53ef\u80fd\u6703\u5c0e\u81f4\u8f03\u5dee\u7684\u56de\u61c9\u3001\u589e\u52a0\u7684\u63a8\u8ad6\u5ef6\u9072\u548c\u66f4\u9ad8\u7684\u6210\u672c\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u7a31\u70ba\u6307\u4ee4\u611f\u77e5\u4e0a\u4e0b\u6587\u58d3\u7e2e\u7684\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u904e\u6ffe\u6389\u4fe1\u606f\u91cf\u8f03\u5c11\u7684\u5185\u5bb9\uff0c\u5f9e\u800c\u52a0\u901f\u548c\u589e\u5f37 LLM \u7684\u4f7f\u7528\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6307\u4ee4\u611f\u77e5\u4e0a\u4e0b\u6587\u58d3\u7e2e\u986f\u8457\u964d\u4f4e\u4e86\u5167\u5b58\u6d88\u8017\uff0c\u4e26\u6700\u5927\u7a0b\u5ea6\u5730\u6e1b\u5c11\u4e86\u751f\u6210\u5ef6\u9072\uff0c\u540c\u6642\u4fdd\u6301\u8207\u4f7f\u7528\u5b8c\u6574\u80cc\u666f\u6642\u76f8\u7576\u7684\u6027\u80fd\u6c34\u5e73\u3002\u5177\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u5c07\u8207\u4e0a\u4e0b\u6587\u76f8\u95dc\u7684\u6210\u672c\u964d\u4f4e\u4e86 50%\uff0c\u5f9e\u800c\u5c07\u63a8\u8ad6\u5167\u5b58\u4f7f\u7528\u91cf\u964d\u4f4e\u4e86 5%\uff0c\u5c07\u63a8\u8ad6\u901f\u5ea6\u63d0\u9ad8\u4e86 2.2 \u500d\uff0c\u800c Rouge-1 \u50c5\u4e0b\u964d\u4e86 0.047\u3002\u9019\u4e9b\u767c\u73fe\u8868\u660e\uff0c\u6211\u5011\u7684\u9019\u7a2e\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6027\u80fd\u4e4b\u9593\u53d6\u5f97\u4e86\u6709\u6548\u7684\u5e73\u8861\u3002", "author": "Haowen Hou et.al.", "authors": "Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, Fei Yu", "id": "2408.15491v1", "paper_url": "http://arxiv.org/abs/2408.15491v1", "repo": "https://github.com/howard-hou/instruction-aware-contextual-compressor"}}