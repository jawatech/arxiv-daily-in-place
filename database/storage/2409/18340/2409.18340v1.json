{"2409.18340": {"publish_time": "2024-09-26", "title": "DRL-STNet: Unsupervised Domain Adaptation for Cross-modality Medical Image Segmentation via Disentangled Representation Learning", "paper_summary": "Unsupervised domain adaptation (UDA) is essential for medical image\nsegmentation, especially in cross-modality data scenarios. UDA aims to transfer\nknowledge from a labeled source domain to an unlabeled target domain, thereby\nreducing the dependency on extensive manual annotations. This paper presents\nDRL-STNet, a novel framework for cross-modality medical image segmentation that\nleverages generative adversarial networks (GANs), disentangled representation\nlearning (DRL), and self-training (ST). Our method leverages DRL within a GAN\nto translate images from the source to the target modality. Then, the\nsegmentation model is initially trained with these translated images and\ncorresponding source labels and then fine-tuned iteratively using a combination\nof synthetic and real images with pseudo-labels and real labels. The proposed\nframework exhibits superior performance in abdominal organ segmentation on the\nFLARE challenge dataset, surpassing state-of-the-art methods by 11.4% in the\nDice similarity coefficient and by 13.1% in the Normalized Surface Dice metric,\nachieving scores of 74.21% and 80.69%, respectively. The average running time\nis 41 seconds, and the area under the GPU memory-time curve is 11,292 MB. These\nresults indicate the potential of DRL-STNet for enhancing cross-modality\nmedical image segmentation tasks.", "paper_summary_zh": "\u7121\u76e3\u7763\u57df\u9069\u61c9 (UDA) \u5c0d\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u81f3\u95dc\u91cd\u8981\uff0c\u7279\u5225\u662f\u5728\u8de8\u6a21\u614b\u6578\u64da\u5834\u666f\u4e2d\u3002UDA \u65e8\u5728\u5c07\u6a19\u8a18\u4f86\u6e90\u57df\u7684\u77e5\u8b58\u8f49\u79fb\u5230\u672a\u6a19\u8a18\u76ee\u6a19\u57df\uff0c\u5f9e\u800c\u6e1b\u5c11\u5c0d\u5927\u91cf\u4eba\u5de5\u6a19\u8a3b\u7684\u4f9d\u8cf4\u3002\u672c\u6587\u63d0\u51fa DRL-STNet\uff0c\u9019\u662f\u4e00\u500b\u7528\u65bc\u8de8\u6a21\u614b\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u7684\u65b0\u7a4e\u67b6\u69cb\uff0c\u5b83\u5229\u7528\u751f\u6210\u5c0d\u6297\u7db2\u8def (GAN)\u3001\u89e3\u7cfe\u7e8f\u8868\u793a\u5b78\u7fd2 (DRL) \u548c\u81ea\u6211\u8a13\u7df4 (ST)\u3002\u6211\u5011\u7684\u6a21\u578b\u5728 GAN \u4e2d\u5229\u7528 DRL \u5c07\u5f71\u50cf\u5f9e\u4f86\u6e90\u8f49\u63db\u5230\u76ee\u6a19\u6a21\u614b\u3002\u7136\u5f8c\uff0c\u5206\u5272\u6a21\u578b\u6700\u521d\u4f7f\u7528\u9019\u4e9b\u8f49\u63db\u5f8c\u7684\u5f71\u50cf\u548c\u5c0d\u61c9\u7684\u4f86\u6e90\u6a19\u7c64\u9032\u884c\u8a13\u7df4\uff0c\u7136\u5f8c\u4f7f\u7528\u5408\u6210\u5f71\u50cf\u548c\u5e36\u6709\u507d\u6a19\u7c64\u548c\u771f\u5be6\u6a19\u7c64\u7684\u771f\u5be6\u5f71\u50cf\u7684\u7d44\u5408\u9032\u884c\u53cd\u8986\u5fae\u8abf\u3002\u6240\u63d0\u51fa\u7684\u67b6\u69cb\u5728 FLARE \u6311\u6230\u8cc7\u6599\u96c6\u4e0a\u7684\u8179\u90e8\u5668\u5b98\u5206\u5272\u4e2d\u8868\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\uff0c\u5728 Dice \u76f8\u4f3c\u4fc2\u6578\u4e0a\u8d85\u8d8a\u73fe\u6709\u6280\u8853 11.4%\uff0c\u5728\u6a19\u6e96\u5316\u8868\u9762 Dice \u6307\u6a19\u4e0a\u8d85\u8d8a 13.1%\uff0c\u5206\u5225\u9054\u5230 74.21% \u548c 80.69% \u7684\u5206\u6578\u3002\u5e73\u5747\u57f7\u884c\u6642\u9593\u70ba 41 \u79d2\uff0cGPU \u8a18\u61b6\u9ad4\u6642\u9593\u66f2\u7dda\u4e0b\u7684\u9762\u7a4d\u70ba 11,292 MB\u3002\u9019\u4e9b\u7d50\u679c\u8868\u660e DRL-STNet \u5728\u589e\u5f37\u8de8\u6a21\u614b\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u4efb\u52d9\u65b9\u9762\u5177\u6709\u6f5b\u529b\u3002", "author": "Hui Lin et.al.", "authors": "Hui Lin, Florian Schiffers, Santiago L\u00f3pez-Tapia, Neda Tavakoli, Daniel Kim, Aggelos K. Katsaggelos", "id": "2409.18340v1", "paper_url": "http://arxiv.org/abs/2409.18340v1", "repo": "null"}}