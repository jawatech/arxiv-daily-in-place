{"2405.19265": {"publish_time": "2024-05-29", "title": "AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data", "paper_summary": "Open-source Large Language Models (LLMs) and their specialized variants,\nparticularly Code LLMs, have recently delivered impressive performance.\nHowever, previous Code LLMs are typically fine-tuned on single-source data with\nlimited quality and diversity, which may insufficiently elicit the potential of\npre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of\nCode LLMs with enhanced code generation and generalization capabilities\nfine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent\nconflicts among the various styles and qualities in multi-source code corpora\nand introduce data-specific prompts with hindsight relabeling, termed\nAlchemistPrompts, to harmonize different data sources and instruction-response\npairs. Additionally, we propose incorporating the data construction process\ninto the fine-tuning data as code comprehension tasks, including instruction\nevolution, data filtering, and code review. Extensive experiments demonstrate\nthat AlchemistCoder holds a clear lead among all models of the same size\n(6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing\nthe efficacy of our method in refining instruction-following capabilities and\nadvancing the boundaries of code intelligence.", "paper_summary_zh": "<paragraph>\u958b\u653e\u539f\u59cb\u78bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ca\u5176\u5c08\u7528\u8b8a\u9ad4\uff0c\u5c24\u5176\u662f\u7a0b\u5f0f\u78bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u6700\u8fd1\u5c55\u73fe\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u7a0b\u5f0f\u78bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u901a\u5e38\u6703\u91dd\u5c0d\u55ae\u4e00\u4f86\u6e90\u8cc7\u6599\u9032\u884c\u5fae\u8abf\uff0c\u4f46\u54c1\u8cea\u548c\u591a\u6a23\u6027\u6709\u9650\uff0c\u53ef\u80fd\u7121\u6cd5\u5145\u5206\u5f15\u767c\u9810\u5148\u8a13\u7df4\u7684\u7a0b\u5f0f\u78bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u6f5b\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa AlchemistCoder\uff0c\u5b83\u662f\u4e00\u7cfb\u5217\u7a0b\u5f0f\u78bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u5177\u5099\u589e\u5f37\u7684\u7a0b\u5f0f\u78bc\u7522\u751f\u548c\u6982\u5316\u80fd\u529b\uff0c\u91dd\u5c0d\u591a\u4f86\u6e90\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u3002\u70ba\u9054\u6210\u6b64\u76ee\u7684\uff0c\u6211\u5011\u7387\u5148\u63ed\u793a\u591a\u4f86\u6e90\u7a0b\u5f0f\u78bc\u8a9e\u6599\u5eab\u4e2d\u5404\u7a2e\u98a8\u683c\u548c\u54c1\u8cea\u4e4b\u9593\u7684\u5167\u5728\u885d\u7a81\uff0c\u4e26\u5f15\u5165\u5177\u6709\u56de\u9867\u6027\u91cd\u65b0\u6a19\u8a18\u7684\u8cc7\u6599\u7279\u5b9a\u63d0\u793a\uff0c\u7a31\u70ba AlchemistPrompts\uff0c\u4ee5\u5354\u8abf\u4e0d\u540c\u7684\u8cc7\u6599\u4f86\u6e90\u548c\u6307\u4ee4\u56de\u61c9\u914d\u5c0d\u3002\u6b64\u5916\uff0c\u6211\u5011\u5efa\u8b70\u5c07\u8cc7\u6599\u5efa\u69cb\u7a0b\u5e8f\u7d0d\u5165\u5fae\u8abf\u8cc7\u6599\u4e2d\uff0c\u4f5c\u70ba\u7a0b\u5f0f\u78bc\u7406\u89e3\u4efb\u52d9\uff0c\u5305\u62ec\u6307\u4ee4\u6f14\u9032\u3001\u8cc7\u6599\u7be9\u9078\u548c\u7a0b\u5f0f\u78bc\u6aa2\u95b1\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e AlchemistCoder \u5728\u6240\u6709\u76f8\u540c\u5927\u5c0f\u7684\u6a21\u578b\u4e2d\uff086.7B/7B\uff09\u90fd\u5177\u6709\u660e\u986f\u9818\u5148\u5730\u4f4d\uff0c\u4e26\u8207\u8f03\u5927\u7684\u6a21\u578b\uff0815B/33B/70B\uff09\u7af6\u722d\u751a\u81f3\u8d85\u8d8a\u5b83\u5011\uff0c\u5c55\u793a\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u7cbe\u9032\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u548c\u63a8\u9032\u7a0b\u5f0f\u78bc\u667a\u6167\u908a\u754c\u65b9\u9762\u7684\u6548\u529b\u3002</paragraph>", "author": "Zifan Song et.al.", "authors": "Zifan Song, Yudong Wang, Wenwei Zhang, Kuikun Liu, Chengqi Lyu, Demin Song, Qipeng Guo, Hang Yan, Dahua Lin, Kai Chen, Cairong Zhao", "id": "2405.19265v1", "paper_url": "http://arxiv.org/abs/2405.19265v1", "repo": "https://github.com/internlm/alchemistcoder"}}