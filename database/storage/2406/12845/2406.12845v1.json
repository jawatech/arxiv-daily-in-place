{"2406.12845": {"publish_time": "2024-06-18", "title": "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "paper_summary": "Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. The\nRLHF process typically starts by training a reward model (RM) using human\npreference data. Conventional RMs are trained on pairwise responses to the same\nuser request, with relative ratings indicating which response humans prefer.\nThe trained RM serves as a proxy for human preferences. However, due to the\nblack-box nature of RMs, their outputs lack interpretability, as humans cannot\nintuitively understand why an RM thinks a response is good or not. As RMs act\nas human preference proxies, we believe they should be human-interpretable to\nensure that their internal decision processes are consistent with human\npreferences and to prevent reward hacking in LLM alignment. To build RMs with\ninterpretable preferences, we propose a two-stage approach: i) train an\nAbsolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional\nabsolute-rating data, each dimension corresponding to a human-interpretable\nobjective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts\n(MoE) strategy with a gating network that automatically selects the most\nsuitable reward objectives based on the context. We efficiently trained an\nArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top\nof the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art\nperformance on RewardBench, a benchmark evaluating RMs for language modeling.\nNotably, the performance of our model surpasses the LLM-as-a-judge method with\nGPT-4 judges by a margin, and approaches the performance of the much larger\nNemotron-4 340B reward model.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5df2\u6210\u70ba\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u7684\u4e3b\u8981\u65b9\u6cd5\u3002RLHF \u904e\u7a0b\u901a\u5e38\u5f9e\u4f7f\u7528\u4eba\u985e\u504f\u597d\u6578\u64da\u8a13\u7df4\u734e\u52f5\u6a21\u578b (RM) \u958b\u59cb\u3002\u50b3\u7d71\u7684 RM \u662f\u91dd\u5c0d\u540c\u4e00\u500b\u4f7f\u7528\u8005\u8981\u6c42\u7684\u6210\u5c0d\u56de\u61c9\u9032\u884c\u8a13\u7df4\uff0c\u76f8\u5c0d\u8a55\u5206\u8868\u793a\u4eba\u985e\u504f\u597d\u7684\u56de\u61c9\u3002\u8a13\u7df4\u5f8c\u7684 RM \u4f5c\u70ba\u4eba\u985e\u504f\u597d\u7684\u4ee3\u7406\u3002\u7136\u800c\uff0c\u7531\u65bc RM \u7684\u9ed1\u76d2\u6027\u8cea\uff0c\u5176\u8f38\u51fa\u7f3a\u4e4f\u53ef\u89e3\u91cb\u6027\uff0c\u56e0\u70ba\u4eba\u985e\u7121\u6cd5\u76f4\u89c0\u5730\u7406\u89e3 RM \u8a8d\u70ba\u4e00\u500b\u56de\u61c9\u662f\u597d\u9084\u662f\u4e0d\u597d\u7684\u539f\u56e0\u3002\u7531\u65bc RM \u4f5c\u70ba\u4eba\u985e\u504f\u597d\u4ee3\u7406\uff0c\u6211\u5011\u76f8\u4fe1\u5b83\u5011\u61c9\u8a72\u662f\u4eba\u985e\u53ef\u89e3\u91cb\u7684\uff0c\u4ee5\u78ba\u4fdd\u5176\u5167\u90e8\u6c7a\u7b56\u904e\u7a0b\u8207\u4eba\u985e\u504f\u597d\u4e00\u81f4\uff0c\u4e26\u9632\u6b62 LLM \u5c0d\u9f4a\u4e2d\u7684\u734e\u52f5\u7834\u89e3\u3002\u70ba\u4e86\u5efa\u7acb\u5177\u6709\u53ef\u89e3\u91cb\u504f\u597d\u7684 RM\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5169\u968e\u6bb5\u65b9\u6cd5\uff1ai) \u4f7f\u7528\u591a\u7dad\u7d55\u5c0d\u8a55\u5206\u6578\u64da\u8a13\u7df4\u4e00\u500b\u7d55\u5c0d\u8a55\u5206\u591a\u76ee\u6a19\u734e\u52f5\u6a21\u578b (ArmoRM)\uff0c\u6bcf\u500b\u7dad\u5ea6\u5c0d\u61c9\u4e00\u500b\u4eba\u985e\u53ef\u89e3\u91cb\u7684\u76ee\u6a19\uff08\u4f8b\u5982\uff0c\u8aa0\u5be6\u3001\u5197\u9577\u3001\u5b89\u5168\uff09\uff1bii) \u63a1\u7528\u6df7\u5408\u5c08\u5bb6 (MoE) \u7b56\u7565\uff0c\u4f7f\u7528\u9598\u63a7\u7db2\u8def\uff0c\u6839\u64da\u4e0a\u4e0b\u6587\u81ea\u52d5\u9078\u64c7\u6700\u5408\u9069\u7684\u734e\u52f5\u76ee\u6a19\u3002\u6211\u5011\u4f7f\u7528 Llama-3 8B \u6709\u6548\u8a13\u7df4\u4e86\u4e00\u500b ArmoRM\uff0c\u4e26\u5728 ArmoRM \u4e0a\u5efa\u7acb\u4e86\u4e00\u500b\u7531\u6dfa\u5c64 MLP \u7d44\u6210\u7684\u9598\u63a7\u7db2\u8def\u3002\u6211\u5011\u8a13\u7df4\u597d\u7684\u6a21\u578b ArmoRM-Llama3-8B \u5728 RewardBench \u4e0a\u7372\u5f97\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0cRewardBench \u662f\u8a55\u4f30\u8a9e\u8a00\u5efa\u6a21\u7684 RM \u7684\u57fa\u6e96\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u6a21\u578b\u7684\u6548\u80fd\u8d85\u8d8a\u4e86\u4f7f\u7528 GPT-4 \u8a55\u5be9\u54e1\u7684 LLM \u4f5c\u70ba\u8a55\u5be9\u54e1\u7684\u65b9\u6cd5\uff0c\u4e26\u63a5\u8fd1\u66f4\u5927\u898f\u6a21\u7684 Nemotron-4 340B \u734e\u52f5\u6a21\u578b\u7684\u6548\u80fd\u3002", "author": "Haoxiang Wang et.al.", "authors": "Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang", "id": "2406.12845v1", "paper_url": "http://arxiv.org/abs/2406.12845v1", "repo": "https://github.com/RLHFlow/RLHF-Reward-Modeling"}}