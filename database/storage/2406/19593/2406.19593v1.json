{"2406.19593": {"publish_time": "2024-06-28", "title": "SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs", "paper_summary": "Synthetic data generation has gained significant attention recently for its\nutility in training large vision and language models. However, the application\nof synthetic data to the training of multimodal context-augmented generation\nsystems has been relatively unexplored. This gap in existing work is important\nbecause existing vision and language models (VLMs) are not trained specifically\nfor context-augmented generation. Resources for adapting such models are\ntherefore crucial for enabling their use in retrieval-augmented generation\n(RAG) settings, where a retriever is used to gather relevant information that\nis then subsequently provided to a generative model via context augmentation.\nTo address this challenging problem, we generate SK-VQA: a large synthetic\nmultimodal dataset containing over 2 million question-answer pairs which\nrequire external knowledge to determine the final answer. Our dataset is both\nlarger and significantly more diverse than existing resources of its kind,\npossessing over 11x more unique questions and containing images from a greater\nvariety of sources than previously-proposed datasets. Through extensive\nexperiments, we demonstrate that our synthetic dataset can not only serve as a\nchallenging benchmark, but is also highly effective for adapting existing\ngenerative multimodal models for context-augmented generation.", "paper_summary_zh": "\u5408\u6210\u8cc7\u6599\u751f\u6210\u6700\u8fd1\u56e0\u5176\u5728\u8a13\u7df4\u5927\u578b\u8996\u89ba\u548c\u8a9e\u8a00\u6a21\u578b\u4e2d\u7684\u6548\u7528\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u5408\u6210\u8cc7\u6599\u5728\u591a\u6a21\u614b\u4e0a\u4e0b\u6587\u64f4\u5145\u751f\u6210\u7cfb\u7d71\u7684\u8a13\u7df4\u4e2d\u7684\u61c9\u7528\u76f8\u5c0d\u672a\u7d93\u63a2\u7d22\u3002\u73fe\u6709\u5de5\u4f5c\u4e2d\u7684\u9019\u500b\u5dee\u8ddd\u5f88\u91cd\u8981\uff0c\u56e0\u70ba\u73fe\u6709\u7684\u8996\u89ba\u548c\u8a9e\u8a00\u6a21\u578b (VLM) \u4e26\u975e\u5c08\u9580\u91dd\u5c0d\u4e0a\u4e0b\u6587\u64f4\u5145\u751f\u6210\u9032\u884c\u8a13\u7df4\u3002\u56e0\u6b64\uff0c\u8abf\u6574\u6b64\u985e\u6a21\u578b\u7684\u8cc7\u6e90\u5c0d\u65bc\u8b93\u5b83\u5011\u80fd\u5920\u7528\u65bc\u6aa2\u7d22\u64f4\u5145\u751f\u6210 (RAG) \u8a2d\u5b9a\u81f3\u95dc\u91cd\u8981\uff0c\u5176\u4e2d\u6aa2\u7d22\u5668\u7528\u65bc\u6536\u96c6\u76f8\u95dc\u8cc7\u8a0a\uff0c\u7136\u5f8c\u901a\u904e\u4e0a\u4e0b\u6587\u64f4\u5145\u63d0\u4f9b\u7d66\u751f\u6210\u6a21\u578b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u554f\u984c\uff0c\u6211\u5011\u751f\u6210\u4e86 SK-VQA\uff1a\u4e00\u500b\u5927\u578b\u5408\u6210\u591a\u6a21\u614b\u8cc7\u6599\u96c6\uff0c\u5305\u542b\u8d85\u904e 200 \u842c\u500b\u554f\u984c\u89e3\u7b54\u5c0d\uff0c\u9700\u8981\u5916\u90e8\u77e5\u8b58\u624d\u80fd\u78ba\u5b9a\u6700\u7d42\u7b54\u6848\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u6bd4\u540c\u985e\u73fe\u6709\u8cc7\u6e90\u66f4\u5927\u4e14\u66f4\u70ba\u591a\u5143\uff0c\u64c1\u6709\u8d85\u904e 11 \u500d\u7684\u7368\u7279\u554f\u984c\uff0c\u4e14\u5305\u542b\u4f86\u81ea\u6bd4\u5148\u524d\u63d0\u51fa\u7684\u8cc7\u6599\u96c6\u66f4\u591a\u6a23\u5316\u4f86\u6e90\u7684\u5f71\u50cf\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u6211\u5011\u7684\u5408\u6210\u8cc7\u6599\u96c6\u4e0d\u50c5\u53ef\u4ee5\u7528\u4f5c\u5177\u6709\u6311\u6230\u6027\u7684\u57fa\u6e96\uff0c\u800c\u4e14\u5c0d\u65bc\u8abf\u6574\u73fe\u6709\u7684\u751f\u6210\u591a\u6a21\u614b\u6a21\u578b\u4ee5\u9032\u884c\u4e0a\u4e0b\u6587\u64f4\u5145\u751f\u6210\u4e5f\u975e\u5e38\u6709\u6548\u3002", "author": "Xin Su et.al.", "authors": "Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard", "id": "2406.19593v1", "paper_url": "http://arxiv.org/abs/2406.19593v1", "repo": "null"}}