{"2412.08158": {"publish_time": "2024-12-11", "title": "How Vision-Language Tasks Benefit from Large Pre-trained Models: A Survey", "paper_summary": "The exploration of various vision-language tasks, such as visual captioning,\nvisual question answering, and visual commonsense reasoning, is an important\narea in artificial intelligence and continuously attracts the research\ncommunity's attention. Despite the improvements in overall performance, classic\nchallenges still exist in vision-language tasks and hinder the development of\nthis area. In recent years, the rise of pre-trained models is driving the\nresearch on vision-language tasks. Thanks to the massive scale of training data\nand model parameters, pre-trained models have exhibited excellent performance\nin numerous downstream tasks. Inspired by the powerful capabilities of\npre-trained models, new paradigms have emerged to solve the classic challenges.\nSuch methods have become mainstream in current research with increasing\nattention and rapid advances. In this paper, we present a comprehensive\noverview of how vision-language tasks benefit from pre-trained models. First,\nwe review several main challenges in vision-language tasks and discuss the\nlimitations of previous solutions before the era of pre-training. Next, we\nsummarize the recent advances in incorporating pre-trained models to address\nthe challenges in vision-language tasks. Finally, we analyze the potential\nrisks associated with the inherent limitations of pre-trained models and\ndiscuss possible solutions, attempting to provide future research directions.", "paper_summary_zh": "\u5404\u7a2e\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u7684\u63a2\u7d22\uff0c\u4f8b\u5982\u8996\u89ba\u5b57\u5e55\u3001\u8996\u89ba\u554f\u7b54\u548c\u8996\u89ba\u5e38\u8b58\u63a8\u7406\uff0c\u662f\u4eba\u5de5\u667a\u6167\u4e2d\u4e00\u500b\u91cd\u8981\u7684\u9818\u57df\uff0c\u4e26\u6301\u7e8c\u5438\u5f15\u7814\u7a76\u793e\u7fa4\u7684\u95dc\u6ce8\u3002\u5118\u7ba1\u6574\u9ad4\u6548\u80fd\u6709\u6240\u63d0\u5347\uff0c\u7d93\u5178\u6311\u6230\u4ecd\u5b58\u5728\u65bc\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e2d\uff0c\u4e26\u963b\u7919\u9019\u500b\u9818\u57df\u7684\u767c\u5c55\u3002\u8fd1\u5e74\u4f86\uff0c\u9810\u8a13\u7df4\u6a21\u578b\u7684\u8208\u8d77\u5e36\u52d5\u4e86\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u7684\u7814\u7a76\u3002\u5f97\u76ca\u65bc\u8a13\u7df4\u8cc7\u6599\u548c\u6a21\u578b\u53c3\u6578\u7684\u5927\u898f\u6a21\u898f\u6a21\uff0c\u9810\u8a13\u7df4\u6a21\u578b\u5728\u8a31\u591a\u4e0b\u6e38\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u7684\u6548\u80fd\u3002\u53d7\u5230\u9810\u8a13\u7df4\u6a21\u578b\u5f37\u5927\u529f\u80fd\u7684\u555f\u767c\uff0c\u65b0\u7684\u5178\u7bc4\u5df2\u7d93\u51fa\u73fe\u4f86\u89e3\u6c7a\u7d93\u5178\u6311\u6230\u3002\u9019\u4e9b\u65b9\u6cd5\u5df2\u6210\u70ba\u7576\u524d\u7814\u7a76\u7684\u4e3b\u6d41\uff0c\u4e26\u53d7\u5230\u8d8a\u4f86\u8d8a\u591a\u7684\u95dc\u6ce8\u548c\u5feb\u901f\u9032\u5c55\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u9810\u8a13\u7df4\u6a21\u578b\u5982\u4f55\u4f7f\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u53d7\u76ca\u7684\u5168\u9762\u6982\u8ff0\u3002\u9996\u5148\uff0c\u6211\u5011\u56de\u9867\u4e86\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e2d\u7684\u5e7e\u500b\u4e3b\u8981\u6311\u6230\uff0c\u4e26\u8a0e\u8ad6\u4e86\u9810\u8a13\u7df4\u6642\u4ee3\u4e4b\u524d\u5148\u524d\u89e3\u6c7a\u65b9\u6848\u7684\u9650\u5236\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u7e3d\u7d50\u4e86\u5c07\u9810\u8a13\u7df4\u6a21\u578b\u7d0d\u5165\u4ee5\u61c9\u5c0d\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e2d\u6311\u6230\u7684\u6700\u65b0\u9032\u5c55\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5206\u6790\u4e86\u8207\u9810\u8a13\u7df4\u6a21\u578b\u56fa\u6709\u9650\u5236\u76f8\u95dc\u7684\u6f5b\u5728\u98a8\u96aa\uff0c\u4e26\u8a0e\u8ad6\u53ef\u80fd\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u8a66\u5716\u63d0\u4f9b\u672a\u4f86\u7684\u7814\u7a76\u65b9\u5411\u3002", "author": "Yayun Qi et.al.", "authors": "Yayun Qi, Hongxi Li, Yiqi Song, Xinxiao Wu, Jiebo Luo", "id": "2412.08158v1", "paper_url": "http://arxiv.org/abs/2412.08158v1", "repo": "null"}}