{"2406.16495": {"publish_time": "2024-06-24", "title": "OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser", "paper_summary": "Recent research has shown that combining Mamba with Transformer architecture,\nwhich has selective state space and quadratic self-attention mechanism,\noutperforms using Mamba or Transformer architecture alone in language modeling\ntasks. The quadratic self-attention mechanism effectively alleviates the\nshortcomings of selective state space in handling long-term dependencies of any\nelement in the sequence. We propose a position information injection method\nthat connects the selective state space model with the quadratic attention, and\nintegrates these two architectures with hybrid experts with cross-sharing\ndomains, so that we can enjoy the advantages of both. We design a new\narchitecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser\n(OTCE), which can compete with well-known medium-scale open-source language\nmodels on a small scale in language modeling tasks.", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5c06 Mamba \u4e0e Transformer \u67b6\u69cb\u7d50\u5408\uff0c\n\u5b83\u5177\u6709\u9078\u64c7\u6027\u7684\u72c0\u614b\u7a7a\u9593\u548c\u4e8c\u6b21\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\n\u5728\u8a9e\u8a00\u5efa\u6a21\u4efb\u52d9\u4e2d\u512a\u65bc\u55ae\u7368\u4f7f\u7528 Mamba \u6216 Transformer \u67b6\u69cb\u3002\n\u4e8c\u6b21\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\u6709\u6548\u5730\u7de9\u89e3\u4e86\u9078\u64c7\u6027\u72c0\u614b\u7a7a\u9593\u5728\u8655\u7406\u5e8f\u5217\u4e2d\u4efb\u4f55\u5143\u7d20\u7684\u9577\u671f\u4f9d\u8cf4\u6027\u65b9\u9762\u7684\u7f3a\u9ede\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u4f4d\u7f6e\u8cc7\u8a0a\u6ce8\u5165\u65b9\u6cd5\uff0c\u5c07\u9078\u64c7\u6027\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u8207\u4e8c\u6b21\u6ce8\u610f\u529b\u9023\u63a5\u8d77\u4f86\uff0c\u4e26\u4f7f\u7528\u5177\u6709\u8de8\u5171\u4eab\u57df\u7684\u6df7\u5408\u5c08\u5bb6\u5c07\u9019\u5169\u500b\u67b6\u69cb\u6574\u5408\u5728\u4e00\u8d77\uff0c\u9019\u6a23\u6211\u5011\u5c31\u53ef\u4ee5\u4eab\u53d7\u5169\u8005\u7684\u512a\u9ede\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u5177\u6709\u66f4\u591a\u4eff\u751f\u5b78\u601d\u60f3\u7684\u65b0\u67b6\u69cb\uff1a\u89c0\u5bdf\u8005-\u601d\u8003\u8005-\u69cb\u601d\u8005-\u8868\u9054\u8005 (OTCE)\uff0c\u5b83\u53ef\u4ee5\u5728\u5c0f\u898f\u6a21\u4e0a\u8207\u8457\u540d\u7684\u4e2d\u578b\u958b\u6e90\u8a9e\u8a00\u6a21\u578b\u5728\u8a9e\u8a00\u5efa\u6a21\u4efb\u52d9\u4e2d\u7af6\u722d\u3002", "author": "Jingze Shi et.al.", "authors": "Jingze Shi, Ting Xie, Bingheng Wu, Chunjun Zheng, Kai Wang", "id": "2406.16495v1", "paper_url": "http://arxiv.org/abs/2406.16495v1", "repo": "https://github.com/LoserCheems/OTCE"}}