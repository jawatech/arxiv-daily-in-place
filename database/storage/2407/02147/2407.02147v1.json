{"2407.02147": {"publish_time": "2024-07-02", "title": "LlamAr & GemmAr: Enhancing LLMs Through Arabic Instruction-Tuning", "paper_summary": "Large language models (LLMs) have greatly impacted the natural language\nprocessing (NLP) field, particularly for the English language. These models\nhave demonstrated capabilities in understanding and generating human-like text.\nThe success of language models largely depends on the availability of\nhigh-quality instruction datasets, which consist of detailed task descriptions\nand corresponding responses that are essential for training the models to\naccurately address a variety of prompts. However, the availability and quality\nof these resources vary by language. While models perform well in English, they\noften struggle with languages like Arabic, due to the lack of datasets for\nfine-tuning Arabic-specific tasks. To address this issue, we introduce\nInstAr-500k, a new Arabic instruction dataset created by generating and\ncollecting content that covers several domains and instruction types. We then\nassess this dataset by fine-tuning two open-source models, Llama-3-8B-Instruct\nand Gemma-7B-IT, on several downstream tasks to scale improvements in their\nfunctionality. Based on multiple evaluations, our fine-tuned models achieve\nstate-of-the-art performance on several Arabic NLP benchmarks. These outcomes\nemphasize the effectiveness of our dataset in elevating the capabilities of\nlanguage models for Arabic. Our instruction dataset bridges the performance gap\nbetween English and Arabic language models by providing resources that amplify\nArabic NLP development. Building on this foundation, we developed two\nstate-of-the-art models, LlamAr-8B and GemmAr-7B, which are specifically tuned\nto excel at a wide range of Arabic NLP tasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6975\u5927\u5730\u5f71\u97ff\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u9818\u57df\uff0c\u7279\u5225\u662f\u5c0d\u82f1\u8a9e\u800c\u8a00\u3002\u9019\u4e9b\u6a21\u578b\u5df2\u8b49\u660e\u5728\u7406\u89e3\u548c\u7522\u751f\u985e\u4eba\u6587\u672c\u65b9\u9762\u5177\u6709\u80fd\u529b\u3002\u8a9e\u8a00\u6a21\u578b\u7684\u6210\u529f\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u6c7a\u65bc\u9ad8\u54c1\u8cea\u6559\u5b78\u8cc7\u6599\u96c6\u7684\u53ef\u7528\u6027\uff0c\u5176\u4e2d\u5305\u542b\u8a73\u7d30\u7684\u4efb\u52d9\u8aaa\u660e\u548c\u76f8\u61c9\u7684\u56de\u61c9\uff0c\u9019\u4e9b\u8aaa\u660e\u548c\u56de\u61c9\u5c0d\u65bc\u8a13\u7df4\u6a21\u578b\u4ee5\u6e96\u78ba\u8655\u7406\u5404\u7a2e\u63d0\u793a\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u9019\u4e9b\u8cc7\u6e90\u7684\u53ef\u7528\u6027\u548c\u54c1\u8cea\u56e0\u8a9e\u8a00\u800c\u7570\u3002\u5118\u7ba1\u6a21\u578b\u5728\u82f1\u8a9e\u65b9\u9762\u8868\u73fe\u826f\u597d\uff0c\u4f46\u7531\u65bc\u7f3a\u4e4f\u7528\u65bc\u5fae\u8abf\u963f\u62c9\u4f2f\u8a9e\u7279\u5b9a\u4efb\u52d9\u7684\u8cc7\u6599\u96c6\uff0c\u5b83\u5011\u901a\u5e38\u96e3\u4ee5\u8655\u7406\u963f\u62c9\u4f2f\u8a9e\u7b49\u8a9e\u8a00\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 InstAr-500k\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u963f\u62c9\u4f2f\u8a9e\u6559\u5b78\u8cc7\u6599\u96c6\uff0c\u5b83\u662f\u900f\u904e\u7522\u751f\u548c\u6536\u96c6\u6db5\u84cb\u591a\u500b\u9818\u57df\u548c\u6559\u5b78\u985e\u578b\u7684\u5167\u5bb9\u800c\u5efa\u7acb\u7684\u3002\u7136\u5f8c\uff0c\u6211\u5011\u900f\u904e\u5fae\u8abf\u5169\u500b\u958b\u6e90\u6a21\u578b\uff0c\u5373 Llama-3-8B-Instruct \u548c Gemma-7B-IT\uff0c\u5728\u5e7e\u500b\u4e0b\u6e38\u4efb\u52d9\u4e2d\u8a55\u4f30\u9019\u500b\u8cc7\u6599\u96c6\uff0c\u4ee5\u64f4\u5c55\u5176\u529f\u80fd\u7684\u6539\u9032\u3002\u6839\u64da\u591a\u9805\u8a55\u4f30\uff0c\u6211\u5011\u5fae\u8abf\u7684\u6a21\u578b\u5728\u5e7e\u500b\u963f\u62c9\u4f2f\u8a9e NLP \u57fa\u6e96\u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u9019\u4e9b\u7d50\u679c\u5f37\u8abf\u4e86\u6211\u5011\u7684\u8cc7\u6599\u96c6\u5728\u63d0\u5347\u963f\u62c9\u4f2f\u8a9e\u8a9e\u8a00\u6a21\u578b\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u6559\u5b78\u8cc7\u6599\u96c6\u900f\u904e\u63d0\u4f9b\u64f4\u5927\u963f\u62c9\u4f2f\u8a9e NLP \u958b\u767c\u7684\u8cc7\u6e90\uff0c\u5f4c\u88dc\u4e86\u82f1\u8a9e\u548c\u963f\u62c9\u4f2f\u8a9e\u8a9e\u8a00\u6a21\u578b\u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u958b\u767c\u4e86\u5169\u500b\u6700\u5148\u9032\u7684\u6a21\u578b\uff0c\u5373 LlamAr-8B \u548c GemmAr-7B\uff0c\u5b83\u5011\u7d93\u904e\u7279\u5225\u8abf\u6574\uff0c\u53ef\u4ee5\u5728\u5ee3\u6cdb\u7684\u963f\u62c9\u4f2f\u8a9e NLP \u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002", "author": "Hasna Chouikhi et.al.", "authors": "Hasna Chouikhi, Manel Aloui, Cyrine Ben Hammou, Ghaith Chaabane, Haithem Kchaou, Chehir Dhaouadi", "id": "2407.02147v1", "paper_url": "http://arxiv.org/abs/2407.02147v1", "repo": "null"}}