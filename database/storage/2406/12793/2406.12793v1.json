{"2406.12793": {"publish_time": "2024-06-18", "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools", "paper_summary": "We introduce ChatGLM, an evolving family of large language models that we\nhave been developing over time. This report primarily focuses on the GLM-4\nlanguage series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent\nour most capable models that are trained with all the insights and lessons\ngained from the preceding three generations of ChatGLM. To date, the GLM-4\nmodels are pre-trained on ten trillions of tokens mostly in Chinese and\nEnglish, along with a small set of corpus from 24 languages, and aligned\nprimarily for Chinese and English usage. The high-quality alignment is achieved\nvia a multi-stage post-training process, which involves supervised fine-tuning\nand learning from human feedback. Evaluations show that GLM-4 1) closely rivals\nor outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH,\nBBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following\nas measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long\ncontext tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by\nAlignBench. The GLM-4 All Tools model is further aligned to understand user\nintent and autonomously decide when and which tool(s) touse -- including web\nbrowser, Python interpreter, text-to-image model, and user-defined functions --\nto effectively complete complex tasks. In practical applications, it matches\nand even surpasses GPT-4 All Tools in tasks like accessing online information\nvia web browsing and solving math problems using Python interpreter. Over the\ncourse, we have open-sourced a series of models, including ChatGLM-6B (three\ngenerations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting\nover 10 million downloads on Hugging face in the year 2023 alone. The open\nmodels can be accessed through https://github.com/THUDM and\nhttps://huggingface.co/THUDM.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a8\u51fa ChatGLM\uff0c\u9019\u662f\u4e00\u500b\u96a8\u8457\u6642\u9593\u767c\u5c55\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7cfb\u5217\u3002\u672c\u5831\u544a\u4e3b\u8981\u95dc\u6ce8 GLM-4 \u8a9e\u8a00\u7cfb\u5217\uff0c\u5176\u4e2d\u5305\u62ec GLM-4\u3001GLM-4-Air \u548c GLM-4-9B\u3002\u5b83\u5011\u4ee3\u8868\u6211\u5011\u6700\u512a\u79c0\u7684\u6a21\u578b\uff0c\u4e26\u5229\u7528\u5f9e\u524d\u4e09\u4ee3 ChatGLM \u4e2d\u7372\u5f97\u7684\u6240\u6709\u898b\u89e3\u548c\u7d93\u9a57\u6559\u8a13\u9032\u884c\u8a13\u7df4\u3002\u8fc4\u4eca\u70ba\u6b62\uff0cGLM-4 \u6a21\u578b\u5df2\u5728\u6578\u842c\u5104\u500b\u4e3b\u8981\u70ba\u4e2d\u6587\u548c\u82f1\u6587\u7684\u7b26\u865f\u4e0a\u9032\u884c\u9810\u8a13\u7df4\uff0c\u4e26\u5305\u542b\u4e00\u5c0f\u7d44\u4f86\u81ea 24 \u7a2e\u8a9e\u8a00\u7684\u8a9e\u6599\u5eab\uff0c\u4e26\u4e3b\u8981\u91dd\u5c0d\u4e2d\u6587\u548c\u82f1\u6587\u4f7f\u7528\u9032\u884c\u5c0d\u9f4a\u3002\u9ad8\u54c1\u8cea\u5c0d\u9f4a\u662f\u901a\u904e\u591a\u968e\u6bb5\u5f8c\u8a13\u7df4\u904e\u7a0b\u5be6\u73fe\u7684\uff0c\u5176\u4e2d\u6d89\u53ca\u76e3\u7763\u5fae\u8abf\u548c\u5f9e\u4eba\u985e\u56de\u994b\u4e2d\u5b78\u7fd2\u3002\u8a55\u4f30\u986f\u793a GLM-4 1) \u5728 MMLU\u3001GSM8K\u3001MATH\u3001BBH\u3001GPQA \u548c HumanEval \u7b49\u4e00\u822c\u6307\u6a19\u65b9\u9762\u8207 GPT-4 \u63a5\u8fd1\u6216\u512a\u65bc GPT-4\uff0c2) \u5728 IFEval \u8861\u91cf\u7684\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u63a5\u8fd1 GPT-4-Turbo\uff0c3) \u5728\u9577\u4e0a\u4e0b\u6587\u4efb\u52d9\u4e2d\u8207 GPT-4 Turbo (128K) \u548c Claude 3 \u76f8\u5339\u914d\uff0c\u4ee5\u53ca 4) \u5728 AlignBench \u8861\u91cf\u7684\u4e2d\u6587\u5c0d\u9f4a\u65b9\u9762\u512a\u65bc GPT-4\u3002GLM-4 All Tools \u6a21\u578b\u9032\u4e00\u6b65\u5c0d\u9f4a\u4ee5\u4e86\u89e3\u7528\u6236\u610f\u5716\u4e26\u81ea\u4e3b\u6c7a\u5b9a\u4f55\u6642\u4ee5\u53ca\u4f7f\u7528\u54ea\u7a2e\u5de5\u5177\uff08\u5305\u62ec\u7db2\u8def\u700f\u89bd\u5668\u3001Python \u89e3\u8b6f\u5668\u3001\u6587\u5b57\u8f49\u5716\u50cf\u6a21\u578b\u548c\u4f7f\u7528\u8005\u5b9a\u7fa9\u51fd\u5f0f\uff09\u4f86\u6709\u6548\u5b8c\u6210\u8907\u96dc\u4efb\u52d9\u3002\u5728\u5be6\u969b\u61c9\u7528\u4e2d\uff0c\u5b83\u5728\u901a\u904e\u7db2\u8def\u700f\u89bd\u5b58\u53d6\u7dda\u4e0a\u8cc7\u8a0a\u548c\u4f7f\u7528 Python \u89e3\u8b6f\u5668\u89e3\u6c7a\u6578\u5b78\u554f\u984c\u7b49\u4efb\u52d9\u4e2d\u8207 GPT-4 All Tools \u76f8\u5339\u914d\u751a\u81f3\u8d85\u8d8a GPT-4 All Tools\u3002\u5728\u6b64\u904e\u7a0b\u4e2d\uff0c\u6211\u5011\u958b\u6e90\u4e86\u4e00\u7cfb\u5217\u6a21\u578b\uff0c\u5305\u62ec ChatGLM-6B\uff08\u4e09\u4ee3\uff09\u3001GLM-4-9B\uff08128K\u30011M\uff09\u3001GLM-4V-9B\u3001WebGLM \u548c CodeGeeX\uff0c\u50c5\u5728 2023 \u5e74\u4e00\u5e74\u5c31\u5728 Hugging face \u4e0a\u5438\u5f15\u4e86\u8d85\u904e 1000 \u842c\u6b21\u7684\u4e0b\u8f09\u3002\u958b\u653e\u6a21\u578b\u53ef\u900f\u904e https://github.com/THUDM \u548c https://huggingface.co/THUDM \u5b58\u53d6\u3002</paragraph>", "author": "Team GLM et.al.", "authors": "Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang", "id": "2406.12793v1", "paper_url": "http://arxiv.org/abs/2406.12793v1", "repo": "null"}}