{"2411.04109": {"publish_time": "2024-11-06", "title": "Self-Consistency Preference Optimization", "paper_summary": "Self-alignment, whereby models learn to improve themselves without human\nannotation, is a rapidly growing research area. However, existing techniques\noften fail to improve complex reasoning tasks due to the difficulty of\nassigning correct rewards. An orthogonal approach that is known to improve\ncorrectness is self-consistency, a method applied at inference time based on\nmultiple sampling in order to find the most consistent answer. In this work, we\nextend the self-consistency concept to help train models. We thus introduce\nself-consistency preference optimization (ScPO), which iteratively trains\nconsistent answers to be preferred over inconsistent ones on unsupervised new\nproblems. We show ScPO leads to large improvements over conventional reward\nmodel training on reasoning tasks such as GSM8K and MATH, closing the gap with\nsupervised training with gold answers or preferences, and that combining ScPO\nwith standard supervised learning improves results even further. On ZebraLogic,\nScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and\nClaude-3 Haiku.", "paper_summary_zh": "\u81ea\u6211\u5c0d\u9f4a\uff0c\u5373\u6a21\u578b\u5728\u6c92\u6709\u4eba\u5de5\u6a19\u8a3b\u7684\u60c5\u6cc1\u4e0b\u5b78\u6703\u81ea\u6211\u6539\u9032\uff0c\u662f\u4e00\u500b\u5feb\u901f\u767c\u5c55\u7684\u7814\u7a76\u9818\u57df\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u6280\u8853\u7531\u65bc\u96e3\u4ee5\u5206\u914d\u6b63\u78ba\u7684\u734e\u52f5\uff0c\u5e38\u5e38\u7121\u6cd5\u6539\u9032\u8907\u96dc\u7684\u63a8\u7406\u4efb\u52d9\u3002\u4e00\u7a2e\u5df2\u77e5\u53ef\u4ee5\u63d0\u9ad8\u6b63\u78ba\u6027\u7684\u6b63\u4ea4\u65b9\u6cd5\u662f\u81ea\u4e00\u81f4\u6027\uff0c\u9019\u662f\u4e00\u7a2e\u5728\u63a8\u7406\u6642\u9593\u61c9\u7528\u65bc\u591a\u91cd\u62bd\u6a23\u7684\uff0c\u7528\u65bc\u627e\u5230\u6700\u4e00\u81f4\u7b54\u6848\u7684\u65b9\u6cd5\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c07\u81ea\u4e00\u81f4\u6027\u6982\u5ff5\u5ef6\u4f38\u5230\u5e6b\u52a9\u8a13\u7df4\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u81ea\u4e00\u81f4\u6027\u504f\u597d\u512a\u5316\uff08ScPO\uff09\uff0c\u5b83\u53cd\u8986\u8a13\u7df4\u4e00\u81f4\u7684\u7b54\u6848\uff0c\u4f7f\u5176\u5728\u7121\u76e3\u7763\u7684\u65b0\u554f\u984c\u4e0a\u6bd4\u4e0d\u4e00\u81f4\u7684\u7b54\u6848\u66f4\u53d7\u9752\u775e\u3002\u6211\u5011\u5c55\u793a\u4e86 ScPO \u5728\u63a8\u7406\u4efb\u52d9\uff08\u4f8b\u5982 GSM8K \u548c MATH\uff09\u4e0a\u6bd4\u50b3\u7d71\u734e\u52f5\u6a21\u578b\u8a13\u7df4\u6709\u4e86\u5f88\u5927\u6539\u9032\uff0c\u7e2e\u5c0f\u4e86\u8207\u4f7f\u7528\u9ec3\u91d1\u7b54\u6848\u6216\u504f\u597d\u7684\u76e3\u7763\u8a13\u7df4\u7684\u5dee\u8ddd\uff0c\u4e26\u4e14\u5c07 ScPO \u8207\u6a19\u6e96\u76e3\u7763\u5b78\u7fd2\u76f8\u7d50\u5408\u9032\u4e00\u6b65\u6539\u9032\u4e86\u7d50\u679c\u3002\u5728 ZebraLogic \u4e0a\uff0cScPO \u5c0d Llama-3 8B \u9032\u884c\u5fae\u8abf\uff0c\u4f7f\u5176\u512a\u65bc Llama-3 70B\u3001Gemma-2 27B \u548c Claude-3 Haiku\u3002", "author": "Archiki Prasad et.al.", "authors": "Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, Jane Yu", "id": "2411.04109v1", "paper_url": "http://arxiv.org/abs/2411.04109v1", "repo": "null"}}