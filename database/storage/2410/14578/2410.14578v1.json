{"2410.14578": {"publish_time": "2024-10-18", "title": "Large Language Models Are Overparameterized Text Encoders", "paper_summary": "Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u7d93\u904e\u76e3\u7763\u5c0d\u6bd4\u8a13\u7df4\u5fae\u8abf\u5f8c\uff0c\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\u6548\u80fd\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u9f90\u5927\u898f\u6a21\u6703\u81a8\u8139\u63a8\u8ad6\u6642\u9593\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u51fa\u900f\u904e\u5728\u76e3\u7763\u8a13\u7df4\u524d\u4fee\u526a LLM \u7684\u6700\u5f8c $p\\%$ \u5c64\uff0c\u50c5\u9032\u884c 1000 \u500b\u6b65\u9a5f\uff0c\u6211\u5011\u53ef\u4ee5\u6309\u6bd4\u4f8b\u6e1b\u5c11\u8a18\u61b6\u9ad4\u548c\u63a8\u8ad6\u6642\u9593\u3002\u6211\u5011\u5728\u6587\u672c\u5d4c\u5165\u4efb\u52d9\u4e2d\u8a55\u4f30\u4e86\u56db\u7a2e\u4e0d\u540c\u7684\u6700\u5148\u9032 LLM\uff0c\u4e26\u767c\u73fe\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u4fee\u526a\u591a\u9054 30% \u7684\u5c64\uff0c\u5c0d\u6548\u80fd\u5f71\u97ff\u5fae\u4e4e\u5176\u5fae\uff0c\u800c\u4fee\u526a\u591a\u9054 80% \u6642\u6548\u80fd\u50c5\u7565\u5fae\u4e0b\u964d\u3002\u6211\u5011\u7684\u6a21\u578b\u50c5\u9700\u4e09\u884c\u7a0b\u5f0f\u78bc\uff0c\u4fbf\u80fd\u8f15\u9b06\u5be6\u4f5c\u5728\u4efb\u4f55\u5c07 LLM \u8f49\u63db\u70ba\u6587\u5b57\u7de8\u78bc\u5668\u7684\u7ba1\u9053\u4e2d\u3002\u6211\u5011\u4e5f\u63d0\u51fa $\\text{L}^3 \\text{Prune}$\uff0c\u9019\u662f\u4e00\u7a2e\u57fa\u65bc\u6a21\u578b\u521d\u59cb\u640d\u5931\u7684\u65b0\u7a4e\u5c64\u4fee\u526a\u7b56\u7565\uff0c\u53ef\u63d0\u4f9b\u5169\u7a2e\u6700\u4f73\u4fee\u526a\u914d\u7f6e\uff1a\u6548\u80fd\u640d\u5931\u5fae\u4e4e\u5176\u5fae\u7684\u5927\u578b\u8b8a\u9ad4\uff0c\u4ee5\u53ca\u9069\u7528\u65bc\u8cc7\u6e90\u53d7\u9650\u8a2d\u5b9a\u7684\u5c0f\u578b\u8b8a\u9ad4\u3002\u5e73\u5747\u800c\u8a00\uff0c\u5927\u578b\u8b8a\u9ad4\u4fee\u526a\u4e86 21% \u7684\u53c3\u6578\uff0c\u6548\u80fd\u4e0b\u964d -0.3\uff0c\u800c\u5c0f\u578b\u8b8a\u9ad4\u50c5\u4e0b\u964d -5.1\uff0c\u540c\u6642\u4fee\u526a\u4e86\u6a21\u578b\u7684 74%\u3002\u6211\u5011\u8a8d\u70ba\u9019\u4e9b\u7d50\u679c\u6709\u529b\u5730\u8b49\u660e\u4e86 LLM \u5c0d\u65bc\u6587\u672c\u5d4c\u5165\u4efb\u52d9\u4f86\u8aaa\u904e\u5ea6\u53c3\u6578\u5316\uff0c\u4e26\u4e14\u53ef\u4ee5\u8f15\u9b06\u5730\u9032\u884c\u4fee\u526a\u3002", "author": "Thennal D K et.al.", "authors": "Thennal D K, Tim Fischer, Chris Biemann", "id": "2410.14578v1", "paper_url": "http://arxiv.org/abs/2410.14578v1", "repo": "null"}}