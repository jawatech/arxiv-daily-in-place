{"2410.18517": {"publish_time": "2024-10-24", "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing", "paper_summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u767c\u5c55\u5927\u5e45\u64f4\u5c55\u4e86\u6a21\u578b\u898f\u6a21\uff0c\u5c0e\u81f4\u63a8\u8ad6\u671f\u9593\u9700\u8981\u5927\u91cf\u7684 GPU \u8a18\u61b6\u9ad4\u3002\nKV (key-value) \u5feb\u53d6\u4e2d\u6ce8\u610f\u529b\u5716\u7684 key \u548c value \u5132\u5b58\u4f54\u7528\u6b64\u8a18\u61b6\u9ad4\u6d88\u8017\u7684 80% \u4ee5\u4e0a\u3002\u73fe\u4eca\uff0c\u5927\u591a\u6578\u73fe\u6709\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u65b9\u6cd5\u90fd\u5c08\u6ce8\u65bc\u55ae\u4e00 Transformer \u5c64\u5167\u7684\u5c64\u5167\u58d3\u7e2e\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u8003\u616e\u5c64\u7d1a\u58d3\u7e2e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba \\textit{KVSharer} \u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u5b83\u5728\u5c64\u4e4b\u9593\u5171\u7528 KV \u5feb\u53d6\u4ee5\u5be6\u73fe\u5c64\u7d1a\u58d3\u7e2e\u3002\u6211\u5011\u4e26\u975e\u76f4\u89ba\u5730\u6839\u64da\u8f03\u9ad8\u7684\u76f8\u4f3c\u6027\u4f86\u5171\u7528\uff0c\u800c\u662f\u767c\u73fe\u4e86\u4e00\u500b\u53cd\u76f4\u89ba\u7684\u73fe\u8c61\uff1a\u5171\u7528\u4e0d\u985e\u4f3c\u7684 KV \u5feb\u53d6\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u6a21\u578b\u6548\u80fd\u3002\u5be6\u9a57\u986f\u793a\uff0c\\textit{KVSharer} \u53ef\u4ee5\u5c07 KV \u5feb\u53d6\u904b\u7b97\u6e1b\u5c11 30%\uff0c\u5f9e\u800c\u964d\u4f4e\u8a18\u61b6\u9ad4\u6d88\u8017\uff0c\u800c\u4e0d\u6703\u986f\u8457\u5f71\u97ff\u6a21\u578b\u6548\u80fd\uff0c\u4e14\u9084\u80fd\u5c07\u751f\u6210\u52a0\u901f\u81f3\u5c11\u63d0\u5347 1.3 \u500d\u3002\u6b64\u5916\uff0c\u6211\u5011\u9a57\u8b49\u4e86 \\textit{KVSharer} \u8207\u73fe\u6709\u7684\u5c64\u5167 KV \u5feb\u53d6\u58d3\u7e2e\u65b9\u6cd5\u76f8\u5bb9\uff0c\u4e14\u7d50\u5408\u5169\u8005\u53ef\u4ee5\u9032\u4e00\u6b65\u7bc0\u7701\u8a18\u61b6\u9ad4\u3002", "author": "Yifei Yang et.al.", "authors": "Yifei Yang, Zouying Cao, Qiguang Chen, Libo Qin, Dongjie Yang, Hai Zhao, Zhi Chen", "id": "2410.18517v1", "paper_url": "http://arxiv.org/abs/2410.18517v1", "repo": "https://github.com/yangyifei729/kvsharer"}}