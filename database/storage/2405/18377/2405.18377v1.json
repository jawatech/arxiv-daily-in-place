{"2405.18377": {"publish_time": "2024-05-28", "title": "LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models", "paper_summary": "The abilities of modern large language models (LLMs) in solving natural\nlanguage processing, complex reasoning, sentiment analysis and other tasks have\nbeen extraordinary which has prompted their extensive adoption. Unfortunately,\nthese abilities come with very high memory and computational costs which\nprecludes the use of LLMs on most hardware platforms. To mitigate this, we\npropose an effective method of finding Pareto-optimal network architectures\nbased on LLaMA2-7B using one-shot NAS. In particular, we fine-tune LLaMA2-7B\nonly once and then apply genetic algorithm-based search to find smaller, less\ncomputationally complex network architectures. We show that, for certain\nstandard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily\nlarge and complex. More specifically, we demonstrate a 1.5x reduction in model\nsize and 1.3x speedup in throughput for certain tasks with negligible drop in\naccuracy. In addition to finding smaller, higher-performing network\narchitectures, our method does so more effectively and efficiently than certain\npruning or sparsification techniques. Finally, we demonstrate how quantization\nis complementary to our method and that the size and complexity of the networks\nwe find can be further decreased using quantization. We believe that our work\nprovides a way to automatically create LLMs which can be used on less expensive\nand more readily available hardware platforms.", "paper_summary_zh": "\u73fe\u4ee3\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u89e3\u6c7a\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3001\u8907\u96dc\u63a8\u7406\u3001\u60c5\u7dd2\u5206\u6790\u548c\u5176\u4ed6\u4efb\u52d9\u7684\u80fd\u529b\u975e\u51e1\uff0c\u9019\u4fc3\u4f7f\u5b83\u5011\u88ab\u5ee3\u6cdb\u63a1\u7528\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u9019\u4e9b\u80fd\u529b\u4f34\u96a8\u8457\u6975\u9ad8\u7684\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u6210\u672c\uff0c\u9019\u4f7f\u5f97 LLM \u7121\u6cd5\u5728\u5927\u591a\u6578\u786c\u9ad4\u5e73\u53f0\u4e0a\u4f7f\u7528\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc LLaMA2-7B \u4f7f\u7528\u4e00\u6b21\u6027 NAS \u5c0b\u627e Pareto \u6700\u4f73\u7db2\u8def\u67b6\u69cb\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u53ea\u5fae\u8abf LLaMA2-7B \u4e00\u6b21\uff0c\u7136\u5f8c\u61c9\u7528\u57fa\u65bc\u907a\u50b3\u6f14\u7b97\u6cd5\u7684\u641c\u5c0b\u4f86\u5c0b\u627e\u8f03\u5c0f\u3001\u904b\u7b97\u8907\u96dc\u5ea6\u8f03\u4f4e\u7684\u7db2\u8def\u67b6\u69cb\u3002\u6211\u5011\u5c55\u793a\u4e86\uff0c\u5c0d\u65bc\u67d0\u4e9b\u6a19\u6e96\u57fa\u6e96\u4efb\u52d9\uff0c\u9810\u5148\u8a13\u7df4\u7684 LLaMA2-7B \u7db2\u8def\u904e\u65bc\u9f90\u5927\u4e14\u8907\u96dc\u3002\u66f4\u5177\u9ad4\u5730\u8aaa\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6a21\u578b\u5927\u5c0f\u6e1b\u5c11\u4e86 1.5 \u500d\uff0c\u67d0\u4e9b\u4efb\u52d9\u7684\u8655\u7406\u901f\u5ea6\u52a0\u5feb\u4e86 1.3 \u500d\uff0c\u800c\u6e96\u78ba\u5ea6\u5e7e\u4e4e\u6c92\u6709\u4e0b\u964d\u3002\u9664\u4e86\u627e\u5230\u66f4\u5c0f\u3001\u6548\u80fd\u66f4\u9ad8\u7684\u7db2\u8def\u67b6\u69cb\u4e4b\u5916\uff0c\u6211\u5011\u7684\u9019\u7a2e\u65b9\u6cd5\u6bd4\u67d0\u4e9b\u526a\u679d\u6216\u7a00\u758f\u5316\u6280\u8853\u66f4\u6709\u6548\u7387\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86\u91cf\u5316\u5982\u4f55\u88dc\u5145\u6211\u5011\u7684\u9019\u7a2e\u65b9\u6cd5\uff0c\u4ee5\u53ca\u6211\u5011\u627e\u5230\u7684\u7db2\u8def\u7684\u5927\u5c0f\u548c\u8907\u96dc\u5ea6\u53ef\u4ee5\u4f7f\u7528\u91cf\u5316\u9032\u4e00\u6b65\u964d\u4f4e\u3002\u6211\u5011\u76f8\u4fe1\uff0c\u6211\u5011\u7684\u9019\u9805\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u7a2e\u81ea\u52d5\u5efa\u7acb LLM \u7684\u65b9\u6cd5\uff0c\u9019\u4e9b LLM \u53ef\u7528\u65bc\u50f9\u683c\u8f03\u4f4e\u4e14\u66f4\u5bb9\u6613\u53d6\u5f97\u7684\u786c\u9ad4\u5e73\u53f0\u4e0a\u3002", "author": "Anthony Sarah et.al.", "authors": "Anthony Sarah, Sharath Nittur Sridhar, Maciej Szankin, Sairam Sundaresan", "id": "2405.18377v1", "paper_url": "http://arxiv.org/abs/2405.18377v1", "repo": "null"}}