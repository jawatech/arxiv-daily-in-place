{"2412.15188": {"publish_time": "2024-12-19", "title": "LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation", "paper_summary": "We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa\u4e86 LlamaFusion\uff0c\u4e00\u500b\u7528\u65bc\u8ce6\u4e88\u9810\u8a13\u7df4\u7d14\u6587\u5b57\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u591a\u6a21\u614b\u751f\u6210\u529f\u80fd\u7684\u6846\u67b6\uff0c\u4f7f\u5b83\u5011\u80fd\u5920\u7406\u89e3\u4e26\u751f\u6210\u4efb\u610f\u5e8f\u5217\u4e2d\u7684\u6587\u5b57\u548c\u5f71\u50cf\u3002\nLlamaFusion \u5145\u5206\u5229\u7528\u4e86\u73fe\u6709 Llama-3 \u7684\u6b0a\u91cd\u4f86\u81ea\u52d5\u8ff4\u6b78\u5730\u8655\u7406\u6587\u5b57\uff0c\u540c\u6642\u5f15\u5165\u4e86\u984d\u5916\u7684\u5e73\u884cTransformer\u6a21\u7d44\u4f86\u8655\u7406\u5f71\u50cf\u64f4\u6563\u3002\u5728\u8a13\u7df4\u671f\u9593\uff0c\u6bcf\u500b\u6a21\u614b\u7684\u8cc7\u6599\u90fd\u6703\u8def\u7531\u5230\u5176\u5c08\u5c6c\u6a21\u7d44\uff1a\u6a21\u614b\u7279\u5b9a\u7684\u524d\u994b\u5c64\u3001\u67e5\u8a62\u9375\u503c\u6295\u5f71\u548c\u6b63\u898f\u5316\u5c64\u6703\u7368\u7acb\u8655\u7406\u6bcf\u500b\u6a21\u614b\uff0c\u800c\u5171\u4eab\u7684\u81ea\u6211\u6ce8\u610f\u5c64\u5247\u5141\u8a31\u6587\u5b57\u548c\u5f71\u50cf\u7279\u5fb5\u4e4b\u9593\u7684\u4e92\u52d5\u3002\u900f\u904e\u51cd\u7d50\u6587\u5b57\u7279\u5b9a\u6a21\u7d44\u4e26\u53ea\u8a13\u7df4\u5f71\u50cf\u7279\u5b9a\u6a21\u7d44\uff0cLlamaFusion \u4fdd\u7559\u4e86\u7d14\u6587\u5b57 LLM \u7684\u8a9e\u8a00\u80fd\u529b\uff0c\u540c\u6642\u767c\u5c55\u51fa\u5f37\u5927\u7684\u8996\u89ba\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002\u8207\u5f9e\u982d\u958b\u59cb\u9810\u8a13\u7df4\u591a\u6a21\u614b\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0cLlamaFusion \u5728\u50c5\u4f7f\u7528 50% \u7684 FLOP \u7684\u60c5\u6cc1\u4e0b\uff0c\u5c07\u5f71\u50cf\u7406\u89e3\u529b\u63d0\u5347\u4e86 20%\uff0c\u5f71\u50cf\u751f\u6210\u80fd\u529b\u63d0\u5347\u4e86 3.6%\uff0c\u540c\u6642\u7dad\u6301\u4e86 Llama-3 \u7684\u8a9e\u8a00\u80fd\u529b\u3002\u6211\u5011\u4e5f\u8b49\u660e\u4e86\u9019\u500b\u6846\u67b6\u53ef\u4ee5\u8b93\u73fe\u6709\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u9069\u61c9\u591a\u6a21\u614b\u751f\u6210\u80fd\u529b\u3002\u7e3d\u7684\u4f86\u8aaa\uff0c\u9019\u500b\u6846\u67b6\u4e0d\u50c5\u5229\u7528\u4e86\u5728\u7d14\u6587\u5b57 LLM \u4e0a\u65e2\u6709\u7684\u904b\u7b97\u6295\u8cc7\uff0c\u4e5f\u8b93\u8a9e\u8a00\u548c\u8996\u89ba\u80fd\u529b\u53ef\u4ee5\u5e73\u884c\u767c\u5c55\uff0c\u70ba\u9ad8\u6548\u7684\u591a\u6a21\u614b\u6a21\u578b\u958b\u767c\u5c55\u793a\u4e86\u4e00\u500b\u6709\u524d\u666f\u7684\u65b9\u5411\u3002</paragraph>", "author": "Weijia Shi et.al.", "authors": "Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu", "id": "2412.15188v1", "paper_url": "http://arxiv.org/abs/2412.15188v1", "repo": "null"}}