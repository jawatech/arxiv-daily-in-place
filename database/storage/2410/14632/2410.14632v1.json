{"2410.14632": {"publish_time": "2024-10-18", "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?", "paper_summary": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.", "paper_summary_zh": "\u6211\u5011\u6aa2\u8996\u4eba\u985e\u6a19\u8a18\u504f\u597d\u8cc7\u6599\u96c6\u4e2d\u4e0d\u540c\u7684\u504f\u597d\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u6db5\u84cb\u56db\u500b\u9ad8\u7d1a\u5225\u985e\u5225\u4e2d 10 \u500b\u985e\u5225\u7684\u5206\u6b67\u4f86\u6e90\u5206\u985e\u6cd5\u2014\u2014\u4efb\u52d9\u672a\u660e\u78ba\u3001\u56de\u61c9\u98a8\u683c\u3001\u62d2\u7d55\u548c\u8a3b\u89e3\u932f\u8aa4\u3002\u6211\u5011\u767c\u73fe\u5927\u591a\u6578\u5206\u6b67\u8207\u6a19\u6e96\u734e\u52f5\u5efa\u6a21\u65b9\u6cd5\u76f8\u53cd\uff0c\u9019\u4e9b\u65b9\u6cd5\u7684\u8a2d\u8a08\u57fa\u65bc\u8a3b\u89e3\u8005\u7684\u5206\u6b67\u662f\u96dc\u8a0a\u7684\u5047\u8a2d\u3002\u7136\u5f8c\uff0c\u6211\u5011\u63a2\u8a0e\u9019\u4e9b\u767c\u73fe\u5982\u4f55\u5f71\u97ff LLM \u958b\u767c\u7684\u5169\u500b\u9818\u57df\uff1a\u734e\u52f5\u5efa\u6a21\u548c\u8a55\u4f30\u3002\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6a19\u6e96\u734e\u52f5\u5efa\u6a21\u65b9\u6cd5\uff08\u4f8b\u5982 Bradley-Terry \u6a21\u578b\uff09\u5982\u4f55\u7121\u6cd5\u5340\u5206\u7d66\u5b9a\u7684\u504f\u597d\u5224\u65b7\u662f\u8a3b\u89e3\u8005\u4e00\u81f4\u540c\u610f\u9084\u662f\u4e0d\u540c\u4f7f\u7528\u8005\u504f\u597d\u4e2d\u7684\u591a\u6578\u610f\u898b\u3002\u6211\u5011\u9084\u767c\u73fe\u9019\u4e9b\u8da8\u52e2\u4e5f\u53cd\u6620\u5728\u6d41\u884c\u7684 LLM-as-Judge \u8a55\u4f30\u65b9\u6cd5\u4e2d\uff0c\u9019\u4e9b\u65b9\u6cd5\u5728\u4e0d\u540c\u504f\u597d\u7684\u60c5\u6cc1\u4e0b\u59cb\u7d42\u78ba\u5b9a\u7372\u52dd\u7684\u56de\u61c9\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u51fa\u4e86 LLM \u8a55\u4f30\u4e2d\u4ecd\u7136\u5b58\u5728\u7684\u6311\u6230\uff0c\u9019\u4e9b\u6311\u6230\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u5230\u56de\u61c9\u98a8\u683c\u7b49\u5206\u88c2\u7279\u5fb5\u7684\u5f71\u97ff\uff0c\u4e26\u4e14\u5728\u958b\u767c\u591a\u5143\u5316\u7684 LLM \u6642\u9047\u5230\u4e86\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u958b\u767c\u4e86\u8b58\u5225\u4e0d\u540c\u504f\u597d\u7684\u65b9\u6cd5\uff0c\u4ee5\u6e1b\u8f15\u5176\u5c0d\u8a55\u4f30\u548c\u8a13\u7df4\u7684\u5f71\u97ff\u3002", "author": "Michael JQ Zhang et.al.", "authors": "Michael JQ Zhang, Zhilin Wang, Jena D. Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, Valentina Pyatkin", "id": "2410.14632v1", "paper_url": "http://arxiv.org/abs/2410.14632v1", "repo": "null"}}