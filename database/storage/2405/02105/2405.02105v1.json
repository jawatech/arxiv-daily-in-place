{"2405.02105": {"publish_time": "2024-05-03", "title": "Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph", "paper_summary": "Structured science summaries or research contributions using properties or\ndimensions beyond traditional keywords enhances science findability. Current\nmethods, such as those used by the Open Research Knowledge Graph (ORKG),\ninvolve manually curating properties to describe research papers' contributions\nin a structured manner, but this is labor-intensive and inconsistent between\nthe domain expert human curators. We propose using Large Language Models (LLMs)\nto automatically suggest these properties. However, it's essential to assess\nthe readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before\napplication. Our study performs a comprehensive comparative analysis between\nORKG's manually curated properties and those generated by the aforementioned\nstate-of-the-art LLMs. We evaluate LLM performance through four unique\nperspectives: semantic alignment and deviation with ORKG properties,\nfine-grained properties mapping accuracy, SciNCL embeddings-based cosine\nsimilarity, and expert surveys comparing manual annotations with LLM outputs.\nThese evaluations occur within a multidisciplinary science setting. Overall,\nLLMs show potential as recommendation systems for structuring science, but\nfurther finetuning is recommended to improve their alignment with scientific\ntasks and mimicry of human expertise.", "paper_summary_zh": "\u7d50\u69cb\u5316\u7684\u79d1\u5b78\u6458\u8981\u6216\u7814\u7a76\u8ca2\u737b\u4f7f\u7528\u8d85\u8d8a\u50b3\u7d71\u95dc\u9375\u5b57\u7684\u5c6c\u6027\u6216\u7dad\u5ea6\uff0c\u63d0\u5347\u79d1\u5b78\u7684\u53ef\u67e5\u627e\u6027\u3002\u76ee\u524d\u7684\u6280\u8853\uff0c\u50cf\u662f\u958b\u653e\u7814\u7a76\u77e5\u8b58\u5716\u8b5c (ORKG) \u6240\u4f7f\u7528\u7684\u6280\u8853\uff0c\u6d89\u53ca\u4eba\u5de5\u7b56\u5c55\u5c6c\u6027\u4ee5\u7d50\u69cb\u5316\u65b9\u5f0f\u63cf\u8ff0\u7814\u7a76\u8ad6\u6587\u7684\u8ca2\u737b\uff0c\u4f46\u9019\u9805\u5de5\u4f5c\u8cbb\u6642\u4e14\u5728\u9818\u57df\u5c08\u5bb6\u7684\u4eba\u5de5\u7b56\u5c55\u8005\u4e4b\u9593\u4e0d\u4e00\u81f4\u3002\u6211\u5011\u63d0\u51fa\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u81ea\u52d5\u5efa\u8b70\u9019\u4e9b\u5c6c\u6027\u3002\u7136\u800c\uff0c\u5728\u61c9\u7528\u4e4b\u524d\uff0c\u8a55\u4f30 GPT-3.5\u3001Llama 2 \u548c Mistral \u7b49 LLM \u7684\u6e96\u5099\u5ea6\u5c0d\u9019\u9805\u4efb\u52d9\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u7684\u7814\u7a76\u5c0d ORKG \u4eba\u5de5\u7b56\u5c55\u7684\u5c6c\u6027\u548c\u524d\u8ff0\u6700\u5148\u9032\u7684 LLM \u6240\u7522\u751f\u7684\u5c6c\u6027\u4e4b\u9593\u9032\u884c\u5168\u9762\u7684\u6bd4\u8f03\u5206\u6790\u3002\u6211\u5011\u900f\u904e\u56db\u500b\u7368\u7279\u7684\u89c0\u9ede\u8a55\u4f30 LLM \u7684\u6548\u80fd\uff1a\u8a9e\u610f\u5c0d\u9f4a\u548c\u8207 ORKG \u5c6c\u6027\u7684\u504f\u5dee\u3001\u7d30\u7c92\u5ea6\u5c6c\u6027\u5c0d\u61c9\u6e96\u78ba\u5ea6\u3001\u57fa\u65bc SciNCL \u5d4c\u5165\u7684\u9918\u5f26\u76f8\u4f3c\u5ea6\uff0c\u4ee5\u53ca\u6bd4\u8f03\u4eba\u5de5\u8a3b\u89e3\u548c LLM \u8f38\u51fa\u7684\u5c08\u5bb6\u8abf\u67e5\u3002\u9019\u4e9b\u8a55\u91cf\u767c\u751f\u5728\u591a\u5b78\u79d1\u79d1\u5b78\u74b0\u5883\u4e2d\u3002\u7e3d\u9ad4\u800c\u8a00\uff0cLLM \u4f5c\u70ba\u7d50\u69cb\u5316\u79d1\u5b78\u7684\u63a8\u85a6\u7cfb\u7d71\u986f\u793a\u51fa\u6f5b\u529b\uff0c\u4f46\u5efa\u8b70\u9032\u4e00\u6b65\u5fae\u8abf\u4ee5\u6539\u5584\u5b83\u5011\u8207\u79d1\u5b78\u4efb\u52d9\u7684\u4e00\u81f4\u6027\uff0c\u4e26\u6a21\u64ec\u4eba\u985e\u7684\u5c08\u696d\u77e5\u8b58\u3002", "author": "Vladyslav Nechakhin et.al.", "authors": "Vladyslav Nechakhin, Jennifer D'Souza, Steffen Eger", "id": "2405.02105v1", "paper_url": "http://arxiv.org/abs/2405.02105v1", "repo": "null"}}