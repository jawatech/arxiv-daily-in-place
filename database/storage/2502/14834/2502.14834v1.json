{"2502.14834": {"publish_time": "2025-02-20", "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models", "paper_summary": "Existing Large Vision-Language Models (LVLMs) can process inputs with context\nlengths up to 128k visual and text tokens, yet they struggle to generate\ncoherent outputs beyond 1,000 words. We find that the primary limitation is the\nabsence of long output examples during supervised fine-tuning (SFT). To tackle\nthis issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158\nexamples, each with multiple input images, an instruction, and corresponding\noutputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that\nmaintain high-fidelity to the input images, we employ Direct Preference\nOptimization (DPO) to the SFT model. Given the high cost of collecting human\nfeedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which\nbreaks long outputs into segments and uses iterative corrections to form\npreference pairs with the original outputs. Additionally, we develop\nMMLongBench-Write, a benchmark featuring six tasks to evaluate the\nlong-generation capabilities of VLMs. Our 7B parameter model, trained with\nLongWriter-V-22k and IterDPO, achieves impressive performance on this\nbenchmark, outperforming larger proprietary models like GPT-4o. Code and data:\nhttps://github.com/THU-KEG/LongWriter-V", "paper_summary_zh": "\u73fe\u6709\u7684\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u80fd\u8655\u7406\u9577\u5ea6\u9054 128k \u8996\u89ba\u548c\u6587\u5b57\u7b26\u865f\u7684\u8f38\u5165\u5167\u5bb9\uff0c\u4f46\u537b\u96e3\u4ee5\u7522\u751f\u8d85\u904e 1,000 \u5b57\u7684\u9023\u8cab\u8f38\u51fa\u3002\u6211\u5011\u767c\u73fe\uff0c\u4e3b\u8981\u9650\u5236\u5728\u65bc\u76e3\u7763\u5fae\u8abf (SFT) \u671f\u9593\u7f3a\u5c11\u9577\u8f38\u51fa\u7bc4\u4f8b\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 LongWriter-V-22k\uff0c\u9019\u662f\u4e00\u500b SFT \u8cc7\u6599\u96c6\uff0c\u5305\u542b 22,158 \u500b\u7bc4\u4f8b\uff0c\u6bcf\u500b\u7bc4\u4f8b\u90fd\u6709\u591a\u500b\u8f38\u5165\u5f71\u50cf\u3001\u4e00\u500b\u8aaa\u660e\u548c\u5c0d\u61c9\u7684\u8f38\u51fa\uff0c\u7bc4\u570d\u5f9e 0 \u5230 10,000 \u5b57\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u7522\u751f\u8207\u8f38\u5165\u5f71\u50cf\u9ad8\u5ea6\u4fdd\u771f\u7684\u9577\u8f38\u51fa\uff0c\u6211\u5011\u5c0d SFT \u6a21\u578b\u63a1\u7528\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\u3002\u8003\u91cf\u5230\u6536\u96c6\u4eba\u985e\u56de\u994b\u7684\u6210\u672c\u5f88\u9ad8\uff08\u4f8b\u5982 3,000 \u5b57\uff09\uff0c\u6211\u5011\u63d0\u51fa IterDPO\uff0c\u5b83\u6703\u5c07\u9577\u8f38\u51fa\u5340\u5206\u6210\u5e7e\u500b\u5340\u584a\uff0c\u4e26\u4f7f\u7528\u53cd\u8986\u4fee\u6b63\u4f86\u5f62\u6210\u8207\u539f\u59cb\u8f38\u51fa\u7684\u504f\u597d\u914d\u5c0d\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86 MMLongBench-Write\uff0c\u9019\u662f\u4e00\u500b\u57fa\u6e96\uff0c\u5305\u542b\u516d\u9805\u4efb\u52d9\uff0c\u7528\u65bc\u8a55\u4f30 VLM \u7684\u9577\u751f\u6210\u80fd\u529b\u3002\u6211\u5011\u7684 7B \u53c3\u6578\u6a21\u578b\u4f7f\u7528 LongWriter-V-22k \u548c IterDPO \u9032\u884c\u8a13\u7df4\uff0c\u5728\u9019\u500b\u57fa\u6e96\u4e0a\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\uff0c\u8d85\u8d8a\u4e86 GPT-4o \u7b49\u5927\u578b\u5c08\u6709\u6a21\u578b\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\uff1ahttps://github.com/THU-KEG/LongWriter-V", "author": "Shangqing Tu et.al.", "authors": "Shangqing Tu, Yucheng Wang, Daniel Zhang-Li, Yushi Bai, Jifan Yu, Yuhao Wu, Lei Hou, Huiqin Liu, Zhiyuan Liu, Bin Xu, Juanzi Li", "id": "2502.14834v1", "paper_url": "http://arxiv.org/abs/2502.14834v1", "repo": "null"}}