{"2503.16419": {"publish_time": "2025-03-20", "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8655\u7406\u8907\u96dc\u4efb\u52d9\u65b9\u9762\u5c55\u73fe\u4e86\u975e\u51e1\u7684\u80fd\u529b\u3002\u5927\u578b\u63a8\u7406\u6a21\u578b (LRM) \u7684\u6700\u65b0\u9032\u5c55\uff0c\u4f8b\u5982 OpenAI o1 \u548c DeepSeek-R1\uff0c\u900f\u904e\u5229\u7528\u76e3\u7763\u5f0f\u5fae\u8abf (SFT) \u548c\u5f37\u5316\u5b78\u7fd2 (RL) \u6280\u8853\u4f86\u589e\u5f37\u601d\u7dad\u93c8 (CoT) \u63a8\u7406\uff0c\u9032\u4e00\u6b65\u63d0\u5347\u4e86\u7cfb\u7d71 2 \u63a8\u7406\u9818\u57df\uff08\u5982\u6578\u5b78\u548c\u7a0b\u5f0f\u8a2d\u8a08\uff09\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u96d6\u7136\u8f03\u9577\u7684 CoT \u63a8\u7406\u5e8f\u5217\u53ef\u4ee5\u63d0\u9ad8\u6548\u80fd\uff0c\u4f46\u5b83\u5011\u4e5f\u6703\u56e0\u70ba\u5197\u9577\u4e14\u591a\u9918\u7684\u8f38\u51fa\u800c\u5f15\u5165\u986f\u8457\u7684\u8a08\u7b97\u958b\u92b7\uff0c\u9019\u7a2e\u73fe\u8c61\u88ab\u7a31\u70ba\u300c\u904e\u5ea6\u601d\u8003\u73fe\u8c61\u300d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u6b21\u63d0\u4f9b\u4e86\u7d50\u69cb\u5316\u7684\u7d9c\u8ff0\uff0c\u7cfb\u7d71\u5730\u7814\u7a76\u548c\u63a2\u8a0e\u4e86\u76ee\u524d\u5728\u5be6\u73fe LLM \u9ad8\u6548\u63a8\u7406\u65b9\u9762\u7684\u9032\u5c55\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u57fa\u65bc LLM \u7684\u5167\u5728\u6a5f\u5236\uff0c\u6211\u5011\u5c07\u73fe\u6709\u5de5\u4f5c\u6b78\u7d0d\u70ba\u5e7e\u500b\u4e3b\u8981\u65b9\u5411\uff1a(1) \u57fa\u65bc\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u8003\u616e\u5c07\u5b8c\u6574\u63a8\u7406\u6a21\u578b\u512a\u5316\u70ba\u66f4\u7c21\u6f54\u7684\u63a8\u7406\u6a21\u578b\uff0c\u6216\u76f4\u63a5\u8a13\u7df4\u9ad8\u6548\u63a8\u7406\u6a21\u578b\uff1b(2) \u57fa\u65bc\u63a8\u7406\u8f38\u51fa\u7d50\u679c\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u65e8\u5728\u52d5\u614b\u6e1b\u5c11\u63a8\u7406\u6b65\u9a5f\u548c\u63a8\u7406\u904e\u7a0b\u4e2d\u7684\u9577\u5ea6\uff1b(3) \u57fa\u65bc\u8f38\u5165\u63d0\u793a\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u65e8\u5728\u6839\u64da\u8f38\u5165\u63d0\u793a\u7684\u5c6c\u6027\uff08\u4f8b\u5982\u96e3\u5ea6\u6216\u9577\u5ea6\u63a7\u5236\uff09\u4f86\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u4ecb\u7d39\u4e86\u4f7f\u7528\u9ad8\u6548\u6578\u64da\u4f86\u8a13\u7df4\u63a8\u7406\u6a21\u578b\uff0c\u63a2\u8a0e\u4e86\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e26\u8a0e\u8ad6\u4e86\u8a55\u4f30\u65b9\u6cd5\u548c\u57fa\u6e96\u6e2c\u8a66\u3002\n", "author": "Yang Sui et.al.", "authors": "Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen, Zhong, Hanjie Chen, Xia Hu", "id": "2503.16419v1", "paper_url": "http://arxiv.org/abs/2503.16419v1", "repo": "null"}}