{"2410.15966": {"publish_time": "2024-10-21", "title": "Self-Explained Keywords Empower Large Language Models for Code Generation", "paper_summary": "Large language models (LLMs) have achieved impressive performance in code\ngeneration. However, due to the long-tail distribution of LLMs' training data,\nlow-frequency terms are typically underrepresented in the training process.\nConsequently, LLMs often misunderstand or overlook problem-specific,\nlow-frequency keywords during code generation, compromising the accuracy of the\ngenerated code. To address this, we propose a novel technique named\nSEK(\\textbf{S}elf-\\textbf{E}xplained \\textbf{K}eywords), which empowers an LLM\nfor better code generation by extracting and explaining the key terms in the\nproblem description with the LLM itself and ranking them based on frequency.\nComprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+),\nand APPS, with five representative LLMs, show that SEK can significantly\nimprove LLMs in code generation, yielding substantial and consistent gains. For\ninstance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4\\% to\n93.3\\% on the Humaneval benchmark. Further analysis confirms that SEK enables\nthe LLMs to shift their attention from low-frequency keywords to their\ncorresponding high-frequency counterparts.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u4e2d\u5df2\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8868\u73fe\u3002\u7136\u800c\uff0c\u7531\u65bc LLM \u8a13\u7df4\u8cc7\u6599\u7684\u9577\u5c3e\u5206\u5e03\uff0c\u4f4e\u983b\u7387\u8853\u8a9e\u901a\u5e38\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u56e0\u6b64\uff0cLLM \u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u904e\u7a0b\u4e2d\u7d93\u5e38\u8aa4\u89e3\u6216\u5ffd\u7565\u7279\u5b9a\u65bc\u554f\u984c\u7684\u4f4e\u983b\u7387\u95dc\u9375\u5b57\uff0c\u640d\u5bb3\u4e86\u6240\u751f\u6210\u7a0b\u5f0f\u78bc\u7684\u6e96\u78ba\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba SEK\uff08**S**elf-**E**xplained **K**eywords\uff09\u7684\u65b0\u6280\u8853\uff0c\u5b83\u901a\u904e\u4f7f\u7528 LLM \u672c\u8eab\u5f9e\u554f\u984c\u63cf\u8ff0\u4e2d\u63d0\u53d6\u548c\u89e3\u91cb\u95dc\u9375\u8853\u8a9e\u4e26\u6839\u64da\u983b\u7387\u5c0d\u5b83\u5011\u9032\u884c\u6392\u5e8f\uff0c\u8ce6\u4e88 LLM \u66f4\u597d\u7684\u7a0b\u5f0f\u78bc\u751f\u6210\u80fd\u529b\u3002\u5728\u4e09\u500b\u57fa\u6e96\uff0c\u5373 HumanEval(+)\u3001MBPP(+) \u548c APPS\uff0c\u4ee5\u53ca\u4e94\u500b\u5177\u6709\u4ee3\u8868\u6027\u7684 LLM \u4e0a\u9032\u884c\u7684\u7d9c\u5408\u5be6\u9a57\u8868\u660e\uff0cSEK \u53ef\u4ee5\u986f\u8457\u6539\u5584 LLM \u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u4e2d\u7684\u8868\u73fe\uff0c\u7522\u751f\u986f\u8457\u4e14\u4e00\u81f4\u7684\u6536\u76ca\u3002\u4f8b\u5982\uff0cSEK \u5c07 DeepSeek-Coder-V2-Instruct \u5728 Humaneval \u57fa\u6e96\u4e0a\u7684 Pass@1 \u5f9e 85.4% \u63d0\u9ad8\u5230 93.3%\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u8b49\u5be6\uff0cSEK \u4f7f LLM \u80fd\u5920\u5c07\u6ce8\u610f\u529b\u5f9e\u4f4e\u983b\u7387\u95dc\u9375\u5b57\u8f49\u79fb\u5230\u5b83\u5011\u5c0d\u61c9\u7684\u9ad8\u983b\u7387\u5c0d\u61c9\u8a5e\u3002", "author": "Lishui Fan et.al.", "authors": "Lishui Fan, Mouxiang Chen, Zhongxin Liu", "id": "2410.15966v1", "paper_url": "http://arxiv.org/abs/2410.15966v1", "repo": "null"}}