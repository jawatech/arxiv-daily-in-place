{"2408.00620": {"publish_time": "2024-08-01", "title": "Are Bigger Encoders Always Better in Vision Large Models?", "paper_summary": "In recent years, multimodal large language models (MLLMs) have shown strong\npotential in real-world applications. They are developing rapidly due to their\nremarkable ability to comprehend multimodal information and their inherent\npowerful cognitive and reasoning capabilities. Among MLLMs, vision language\nmodels (VLM) stand out for their ability to understand vision information.\nHowever, the scaling trend of VLMs under the current mainstream paradigm has\nnot been extensively studied. Whether we can achieve better performance by\ntraining even larger models is still unclear. To address this issue, we\nconducted experiments on the pretraining stage of MLLMs. We conduct our\nexperiment using different encoder sizes and large language model (LLM) sizes.\nOur findings indicate that merely increasing the size of encoders does not\nnecessarily enhance the performance of VLMs. Moreover, we analyzed the effects\nof LLM backbone parameter size and data quality on the pretraining outcomes.\nAdditionally, we explored the differences in scaling laws between LLMs and\nVLMs.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u5c55\u73fe\u5f37\u5927\u7684\u6f5b\u529b\u3002\u7531\u65bc\u5176\u7406\u89e3\u591a\u6a21\u614b\u8cc7\u8a0a\u7684\u975e\u51e1\u80fd\u529b\uff0c\u4ee5\u53ca\u5176\u5167\u5728\u5f37\u5927\u7684\u8a8d\u77e5\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5b83\u5011\u6b63\u5feb\u901f\u767c\u5c55\u3002\u5728 MLLM \u4e2d\uff0c\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff08VLM\uff09\u56e0\u5176\u7406\u89e3\u8996\u89ba\u8cc7\u8a0a\u7684\u80fd\u529b\u800c\u812b\u7a4e\u800c\u51fa\u3002\u7136\u800c\uff0c\u5728\u76ee\u524d\u7684\u4e3b\u6d41\u7bc4\u4f8b\u4e0b\uff0cVLM \u7684\u64f4\u5145\u8da8\u52e2\u5c1a\u672a\u5ee3\u6cdb\u7814\u7a76\u3002\u6211\u5011\u662f\u5426\u80fd\u900f\u904e\u8a13\u7df4\u66f4\u5927\u898f\u6a21\u7684\u6a21\u578b\u4f86\u7372\u5f97\u66f4\u597d\u7684\u6548\u80fd\uff0c\u4ecd\u4e0d\u6e05\u695a\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5c0d MLLM \u7684\u9810\u8a13\u7df4\u968e\u6bb5\u9032\u884c\u4e86\u5be6\u9a57\u3002\u6211\u5011\u4f7f\u7528\u4e0d\u540c\u7684\u7de8\u78bc\u5668\u5927\u5c0f\u548c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5927\u5c0f\u4f86\u9032\u884c\u5be6\u9a57\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u55ae\u7d14\u589e\u52a0\u7de8\u78bc\u5668\u7684\u898f\u6a21\u4e26\u4e0d\u6703\u5fc5\u7136\u63d0\u5347 VLM \u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u5206\u6790\u4e86 LLM \u4e3b\u5e79\u53c3\u6578\u5927\u5c0f\u548c\u8cc7\u6599\u54c1\u8cea\u5c0d\u9810\u8a13\u7df4\u7d50\u679c\u7684\u5f71\u97ff\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a2\u8a0e\u4e86 LLM \u548c VLM \u4e4b\u9593\u7684\u64f4\u5145\u6cd5\u5247\u5dee\u7570\u3002", "author": "Bozhou Li et.al.", "authors": "Bozhou Li, Hao Liang, Zimo Meng, Wentao Zhang", "id": "2408.00620v1", "paper_url": "http://arxiv.org/abs/2408.00620v1", "repo": "null"}}