{"2410.19385": {"publish_time": "2024-10-25", "title": "Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models", "paper_summary": "Large Language Models (LLMs) are powerful computational models trained on\nextensive corpora of human-readable text, enabling them to perform\ngeneral-purpose language understanding and generation. LLMs have garnered\nsignificant attention in both industry and academia due to their exceptional\nperformance across various natural language processing (NLP) tasks. Despite\nthese successes, LLMs often produce inaccuracies, commonly referred to as\nhallucinations. Prompt engineering, the process of designing and formulating\ninstructions for LLMs to perform specific tasks, has emerged as a key approach\nto mitigating hallucinations. This paper provides a comprehensive empirical\nevaluation of different prompting strategies and frameworks aimed at reducing\nhallucinations in LLMs. Various prompting techniques are applied to a broad set\nof benchmark datasets to assess the accuracy and hallucination rate of each\nmethod. Additionally, the paper investigates the influence of tool-calling\nagents (LLMs augmented with external tools to enhance their capabilities beyond\nlanguage generation) on hallucination rates in the same benchmarks. The\nfindings demonstrate that the optimal prompting technique depends on the type\nof problem, and that simpler techniques often outperform more complex methods\nin reducing hallucinations. Furthermore, it is shown that LLM agents can\nexhibit significantly higher hallucination rates due to the added complexity of\nexternal tool usage.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u662f\u5728\u5927\u91cf\u4eba\u985e\u53ef\u8b80\u6587\u672c\u7684\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\u7684\u5f37\u5927\u904b\u7b97\u6a21\u578b\uff0c\u4f7f\u5b83\u5011\u80fd\u5920\u57f7\u884c\u901a\u7528\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u3002LLM \u5728\u7522\u696d\u548c\u5b78\u8853\u754c\u90fd\u7372\u5f97\u4e86\u986f\u8457\u7684\u95dc\u6ce8\uff0c\u56e0\u70ba\u5b83\u5011\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u6210\u529f\uff0cLLM \u7d93\u5e38\u7522\u751f\u4e0d\u6e96\u78ba\u7684\u7d50\u679c\uff0c\u901a\u5e38\u7a31\u70ba\u5e7b\u89ba\u3002\u63d0\u793a\u5de5\u7a0b\uff0c\u4e5f\u5c31\u662f\u8a2d\u8a08\u548c\u5236\u5b9a LLM \u57f7\u884c\u7279\u5b9a\u4efb\u52d9\u7684\u6307\u4ee4\u7684\u904e\u7a0b\uff0c\u5df2\u6210\u70ba\u6e1b\u8f15\u5e7b\u89ba\u7684\u4e00\u7a2e\u95dc\u9375\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u5c0d\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u548c\u6846\u67b6\u7684\u5168\u9762\u5be6\u8b49\u8a55\u4f30\uff0c\u65e8\u5728\u6e1b\u5c11 LLM \u4e2d\u7684\u5e7b\u89ba\u3002\u5c07\u5404\u7a2e\u63d0\u793a\u6280\u8853\u61c9\u7528\u65bc\u5ee3\u6cdb\u7684\u57fa\u6e96\u8cc7\u6599\u96c6\uff0c\u4ee5\u8a55\u4f30\u6bcf\u7a2e\u65b9\u6cd5\u7684\u6e96\u78ba\u6027\u548c\u5e7b\u89ba\u7387\u3002\u6b64\u5916\uff0c\u672c\u6587\u9084\u63a2\u8a0e\u4e86\u5de5\u5177\u547c\u53eb\u4ee3\u7406\uff08\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u589e\u5f37\u5176\u8a9e\u8a00\u751f\u6210\u80fd\u529b\u7684 LLM\uff09\u5c0d\u76f8\u540c\u57fa\u6e96\u4e2d\u7684\u5e7b\u89ba\u7387\u7684\u5f71\u97ff\u3002\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u6700\u4f73\u63d0\u793a\u6280\u8853\u53d6\u6c7a\u65bc\u554f\u984c\u985e\u578b\uff0c\u800c\u4e14\u8f03\u7c21\u55ae\u7684\u6280\u8853\u5728\u6e1b\u5c11\u5e7b\u89ba\u65b9\u9762\u901a\u5e38\u512a\u65bc\u8f03\u8907\u96dc\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\uff0c\u7531\u65bc\u5916\u90e8\u5de5\u5177\u4f7f\u7528\u7684\u8907\u96dc\u6027\u589e\u52a0\uff0cLLM \u4ee3\u7406\u53ef\u80fd\u6703\u8868\u73fe\u51fa\u986f\u8457\u66f4\u9ad8\u7684\u5e7b\u89ba\u7387\u3002", "author": "Liam Barkley et.al.", "authors": "Liam Barkley, Brink van der Merwe", "id": "2410.19385v1", "paper_url": "http://arxiv.org/abs/2410.19385v1", "repo": "null"}}