{"2502.13127": {"publish_time": "2025-02-18", "title": "Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning", "paper_summary": "Recent advances in Large Language Models (LLMs) have enabled them to process\nincreasingly longer sequences, ranging from 2K to 2M tokens and even beyond.\nHowever, simply extending the input sequence length does not necessarily lead\nto effective long-context understanding. In this study, we integrate\nChain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate\neffective long-context understanding. To achieve this, we introduce\nLongFinanceQA, a synthetic dataset in the financial domain designed to improve\nlong-context reasoning. Unlike existing long-context synthetic data,\nLongFinanceQA includes intermediate CoT reasoning before the final conclusion,\nwhich encourages LLMs to perform explicit reasoning, improving accuracy and\ninterpretability in long-context understanding. To generate synthetic CoT\nreasoning, we propose Property-driven Agentic Inference (PAI), an agentic\nframework that simulates human-like reasoning steps, including property\nextraction, retrieval, and summarization. We evaluate PAI's reasoning\ncapabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark,\noutperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune\nLLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's\nfinancial subset.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u8b93\u5b83\u5011\u80fd\u5920\u8655\u7406\u8d8a\u4f86\u8d8a\u9577\u7684\u5e8f\u5217\uff0c\u7bc4\u570d\u5f9e 2K \u5230 2M \u500b\u7b26\u865f\uff0c\u751a\u81f3\u66f4\u9577\u3002\n\u7136\u800c\uff0c\u50c5\u50c5\u5ef6\u9577\u8f38\u5165\u5e8f\u5217\u9577\u5ea6\u4e26\u4e0d\u6703\u5fc5\u7136\u5c0e\u81f4\u6709\u6548\u7684\u9577\u8a9e\u5883\u7406\u89e3\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u4ee5\u76e3\u7763\u7684\u65b9\u5f0f\u5c07\u601d\u8003\u93c8 (CoT) \u63a8\u7406\u6574\u5408\u5230 LLM \u4e2d\uff0c\u4ee5\u4fc3\u9032\u6709\u6548\u7684\u9577\u8a9e\u5883\u7406\u89e3\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 LongFinanceQA\uff0c\u9019\u662f\u4e00\u500b\u5728\u91d1\u878d\u9818\u57df\u4e2d\u7684\u5408\u6210\u6578\u64da\u96c6\uff0c\u65e8\u5728\u6539\u9032\u9577\u8a9e\u5883\u63a8\u7406\u3002\u8207\u73fe\u6709\u7684\u9577\u8a9e\u5883\u5408\u6210\u6578\u64da\u4e0d\u540c\uff0cLongFinanceQA \u5728\u6700\u7d42\u7d50\u8ad6\u4e4b\u524d\u5305\u542b\u4e86\u4e2d\u9593\u7684 CoT \u63a8\u7406\uff0c\u9019\u9f13\u52f5 LLM \u57f7\u884c\u660e\u78ba\u7684\u63a8\u7406\uff0c\u5f9e\u800c\u63d0\u9ad8\u9577\u8a9e\u5883\u7406\u89e3\u7684\u6e96\u78ba\u6027\u548c\u53ef\u89e3\u91cb\u6027\u3002\u70ba\u4e86\u751f\u6210\u5408\u6210\u7684 CoT \u63a8\u7406\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u57fa\u65bc\u5c6c\u6027\u7684\u4e3b\u9ad4\u63a8\u7406 (PAI)\uff0c\u9019\u662f\u4e00\u500b\u6a21\u64ec\u985e\u4eba\u63a8\u7406\u6b65\u9a5f\u7684\u4e3b\u9ad4\u6846\u67b6\uff0c\u5305\u62ec\u5c6c\u6027\u63d0\u53d6\u3001\u6aa2\u7d22\u548c\u7e3d\u7d50\u3002\u6211\u5011\u901a\u904e\u8a55\u4f30\u642d\u8f09 PAI \u7684 GPT-4o-mini \u5728 Loong \u57fa\u6e96\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u6bd4\u6a19\u6e96\u7684 GPT-4o-mini \u9ad8\u51fa 20.0%\uff0c\u4f86\u8a55\u4f30 PAI \u7684\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c0d LLaMA-3.1-8B-Instruct \u9032\u884c\u4e86\u5fae\u8abf\uff0c\u5728 Loong \u7684\u91d1\u878d\u5b50\u96c6\u4e2d\u5be6\u73fe\u4e86 24.6% \u7684\u589e\u76ca\u3002", "author": "Jingyang Lin et.al.", "authors": "Jingyang Lin, Andy Wong, Tian Xia, Shenghua He, Hui Wei, Mei Han, Jiebo Luo", "id": "2502.13127v1", "paper_url": "http://arxiv.org/abs/2502.13127v1", "repo": "null"}}