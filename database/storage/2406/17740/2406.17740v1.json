{"2406.17740": {"publish_time": "2024-06-25", "title": "Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning", "paper_summary": "Recent efforts to scale Transformer models have demonstrated rapid progress\nacross a wide range of tasks (Wei et al., 2022). However, fine-tuning these\nmodels for downstream tasks is expensive due to their large parameter counts.\nParameter-efficient fine-tuning (PEFT) approaches have emerged as a viable\nalternative by allowing us to fine-tune models by updating only a small number\nof parameters. In this work, we propose a general framework for parameter\nefficient fine-tuning (PEFT), based on structured unrestricted-rank matrices\n(SURM) which can serve as a drop-in replacement for popular approaches such as\nAdapters and LoRA. Unlike other methods like LoRA, SURMs provides more\nflexibility in finding the right balance between compactness and\nexpressiveness. This is achieved by using low displacement rank matrices\n(LDRMs), which hasn't been used in this context before. SURMs remain\ncompetitive with baselines, often providing significant quality improvements\nwhile using a smaller parameter budget. SURMs achieve 5-7% accuracy gains on\nvarious image classification tasks while replacing low-rank matrices in LoRA.\nIt also results in up to 12x reduction of the number of parameters in adapters\n(with virtually no loss in quality) on the GLUE benchmark.", "paper_summary_zh": "\u6700\u8fd1\u5bf9 Transformer \u6a21\u578b\u8fdb\u884c\u6269\u5c55\u7684\u5c1d\u8bd5\u5c55\u793a\u4e86\u5404\u79cd\u4efb\u52a1\u7684\u5feb\u901f\u8fdb\u5c55\uff08Wei \u7b49\u4eba\uff0c2022 \u5e74\uff09\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8fd9\u4e9b\u6a21\u578b\u7684\u53c2\u6570\u6570\u91cf\u5e9e\u5927\uff0c\u56e0\u6b64\u4e3a\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u975e\u5e38\u6602\u8d35\u3002\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u65b9\u6cd5\u5df2\u7ecf\u6210\u4e3a\u4e00\u79cd\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u4ec5\u901a\u8fc7\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\u6765\u5fae\u8c03\u6a21\u578b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ed3\u6784\u5316\u65e0\u9650\u5236\u79e9\u77e9\u9635 (SURM) \u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u7684\u901a\u7528\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u4f5c\u4e3a\u6d41\u884c\u65b9\u6cd5\uff08\u4f8b\u5982\u9002\u914d\u5668\u548c LoRA\uff09\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u4e0e LoRA \u7b49\u5176\u4ed6\u65b9\u6cd5\u4e0d\u540c\uff0cSURM \u5728\u5bfb\u627e\u7d27\u51d1\u6027\u548c\u8868\u73b0\u529b\u4e4b\u95f4\u7684\u9002\u5f53\u5e73\u8861\u65b9\u9762\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\u3002\u8fd9\u662f\u901a\u8fc7\u4f7f\u7528\u4f4e\u7f6e\u6362\u79e9\u77e9\u9635 (LDRM) \u5b9e\u73b0\u7684\uff0c\u8fd9\u5728\u4ee5\u524d\u4ece\u672a\u5728\u8be5\u4e0a\u4e0b\u6587\u4e2d\u4f7f\u7528\u8fc7\u3002SURM \u4ecd\u7136\u5177\u6709\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u7ade\u4e89\u529b\uff0c\u901a\u5e38\u5728\u4f7f\u7528\u8f83\u5c0f\u53c2\u6570\u9884\u7b97\u7684\u540c\u65f6\u63d0\u4f9b\u663e\u8457\u7684\u8d28\u91cf\u6539\u8fdb\u3002SURM \u5728\u5404\u79cd\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86 5-7% \u7684\u51c6\u786e\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u66ff\u6362\u4e86 LoRA \u4e2d\u7684\u4f4e\u79e9\u77e9\u9635\u3002\u5728 GLUE \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b83\u8fd8\u5bfc\u81f4\u9002\u914d\u5668\u4e2d\u53c2\u6570\u6570\u91cf\u51cf\u5c11\u4e86 12 \u500d\uff08\u51e0\u4e4e\u6ca1\u6709\u8d28\u91cf\u635f\u5931\uff09\u3002", "author": "Arijit Sehanobish et.al.", "authors": "Arijit Sehanobish, Avinava Dubey, Krzysztof Choromanski, Somnath Basu Roy Chowdhury, Deepali Jain, Vikas Sindhwani, Snigdha Chaturvedi", "id": "2406.17740v1", "paper_url": "http://arxiv.org/abs/2406.17740v1", "repo": "null"}}