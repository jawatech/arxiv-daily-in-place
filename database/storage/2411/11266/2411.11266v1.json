{"2411.11266": {"publish_time": "2024-11-18", "title": "VersaTune: Fine-Tuning Multi-Ability LLMs Efficiently", "paper_summary": "Large Language Models (LLMs) exhibit remarkable capabilities in handling\nmultiple tasks across domains due to their emergent properties. These\ncapabilities are further augmented during the Supervised Fine-Tuning (SFT)\nphase. Despite their potential, existing work mainly focuses on domain-specific\nenhancements during fine-tuning, the challenge of which lies in catastrophic\nforgetting of knowledge across other domains. In this study, we introduce\nVersaTune, a novel data composition framework designed for enhancing LLMs'\noverall multi-ability performances during fine-tuning. We categorize knowledge\ninto distinct domains including law, medicine, finance, science, code. We begin\nwith detecting the distribution of domain-specific knowledge within the base\nmodel, followed by the composition of training data that aligns with the\nmodel's existing knowledge distribution. During the fine-tuning process,\nweights of different domains are dynamically adjusted based on their learnable\npotential and forgetting degree. Experimental results demonstrate that\nVersaTune achieves significant improvements in multi-domain performance, with a\n35.21% enhancement in comprehensive multi-domain tasks. Additionally, in\nscenarios where specific domain optimization is required, VersaTune reduces the\ndegradation of performance in other domains by 38.77%, without compromising the\ntarget domain's training efficacy.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7531\u65bc\u5176\u65b0\u8208\u5c6c\u6027\uff0c\u5728\u8655\u7406\u8de8\u9818\u57df\u7684\u591a\u9805\u4efb\u52d9\u6642\u8868\u73fe\u51fa\u986f\u8457\u7684\u80fd\u529b\u3002\u9019\u4e9b\u80fd\u529b\u5728\u76e3\u7763\u5fae\u8abf\uff08SFT\uff09\u968e\u6bb5\u9032\u4e00\u6b65\u589e\u5f37\u3002\u5118\u7ba1\u5177\u6709\u6f5b\u529b\uff0c\u73fe\u6709\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u65bc\u5fae\u8abf\u671f\u9593\u7279\u5b9a\u9818\u57df\u7684\u589e\u5f37\uff0c\u5176\u6311\u6230\u5728\u65bc\u8de8\u5176\u4ed6\u9818\u57df\u7684\u77e5\u8b58\u707d\u96e3\u6027\u907a\u5fd8\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 VersaTune\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6578\u64da\u7d44\u5408\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f37 LLM \u5728\u5fae\u8abf\u671f\u9593\u7684\u6574\u9ad4\u591a\u80fd\u529b\u8868\u73fe\u3002\u6211\u5011\u5c07\u77e5\u8b58\u5206\u985e\u70ba\u4e0d\u540c\u7684\u9818\u57df\uff0c\u5305\u62ec\u6cd5\u5f8b\u3001\u91ab\u5b78\u3001\u91d1\u878d\u3001\u79d1\u5b78\u3001\u7a0b\u5f0f\u78bc\u3002\u6211\u5011\u5f9e\u6aa2\u6e2c\u57fa\u790e\u6a21\u578b\u4e2d\u7279\u5b9a\u9818\u57df\u77e5\u8b58\u7684\u5206\u5e03\u958b\u59cb\uff0c\u7136\u5f8c\u7d44\u5408\u8207\u6a21\u578b\u73fe\u6709\u77e5\u8b58\u5206\u5e03\u4e00\u81f4\u7684\u8a13\u7df4\u8cc7\u6599\u3002\u5728\u5fae\u8abf\u904e\u7a0b\u4e2d\uff0c\u4e0d\u540c\u9818\u57df\u7684\u6b0a\u91cd\u6703\u6839\u64da\u5176\u53ef\u5b78\u7fd2\u7684\u6f5b\u529b\u8207\u907a\u5fd8\u7a0b\u5ea6\u9032\u884c\u52d5\u614b\u8abf\u6574\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cVersaTune \u5728\u591a\u9818\u57df\u6548\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u986f\u8457\u7684\u9032\u6b65\uff0c\u5728\u7d9c\u5408\u591a\u9818\u57df\u4efb\u52d9\u4e2d\u63d0\u5347\u4e86 35.21%\u3002\u6b64\u5916\uff0c\u5728\u9700\u8981\u7279\u5b9a\u9818\u57df\u6700\u4f73\u5316\u7684\u5834\u666f\u4e2d\uff0cVersaTune \u5c07\u5176\u4ed6\u9818\u57df\u7684\u6548\u80fd\u4e0b\u964d\u964d\u4f4e\u4e86 38.77%\uff0c\u800c\u4e0d\u6703\u5f71\u97ff\u76ee\u6a19\u9818\u57df\u7684\u8a13\u7df4\u6548\u80fd\u3002", "author": "Keer Lu et.al.", "authors": "Keer Lu, Keshi Zhao, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, Wentao Zhang", "id": "2411.11266v1", "paper_url": "http://arxiv.org/abs/2411.11266v1", "repo": "null"}}