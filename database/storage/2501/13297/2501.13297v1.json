{"2501.13297": {"publish_time": "2025-01-23", "title": "RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question Answering", "paper_summary": "Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text\nand images, has gained significant attention in information retrieval (IR) and\nnatural language processing (NLP). Traditional ranking methods rely on small\nencoder-based language models, which are incompatible with modern decoder-based\ngenerative large language models (LLMs) that have advanced various NLP tasks.\nTo bridge this gap, we propose RAMQA, a unified framework combining\nlearning-to-rank methods with generative permutation-enhanced ranking\ntechniques. We first train a pointwise multi-modal ranker using LLaVA as the\nbackbone. Then, we apply instruction tuning to train a LLaMA model for\nre-ranking the top-k documents using an innovative autoregressive multi-task\nlearning approach. Our generative ranking model generates re-ranked document\nIDs and specific answers from document candidates in various permutations.\nExperiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant\nimprovements over strong baselines, highlighting the effectiveness of our\napproach. Code and data are available at: https://github.com/TonyBY/RAMQA", "paper_summary_zh": "\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u578b\u95ee\u7b54 (MRAQA) \u96c6\u6210\u6587\u672c\u548c\u56fe\u50cf\uff0c\u5728\u4fe1\u606f\u68c0\u7d22 (IR) \u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u4e2d\u5907\u53d7\u5173\u6ce8\u3002\u4f20\u7edf\u6392\u540d\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5c0f\u578b\u7f16\u7801\u5668\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e0e\u5148\u8fdb\u7684\u5404\u79cd NLP \u4efb\u52a1\u7684\u73b0\u4ee3\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b (LLM) \u4e0d\u517c\u5bb9\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 RAMQA\uff0c\u4e00\u4e2a\u5c06\u5b66\u4e60\u6392\u5e8f\u65b9\u6cd5\u4e0e\u751f\u6210\u7f6e\u6362\u589e\u5f3a\u6392\u5e8f\u6280\u672f\u76f8\u7ed3\u5408\u7684\u7edf\u4e00\u6846\u67b6\u3002\u6211\u4eec\u9996\u5148\u4f7f\u7528 LLaVA \u4f5c\u4e3a\u4e3b\u5e72\u8bad\u7ec3\u4e00\u4e2a\u70b9\u5f0f\u591a\u6a21\u6001\u6392\u5e8f\u5668\u3002\u7136\u540e\uff0c\u6211\u4eec\u5e94\u7528\u6307\u4ee4\u5fae\u8c03\u6765\u8bad\u7ec3\u4e00\u4e2a LLaMA \u6a21\u578b\uff0c\u4f7f\u7528\u521b\u65b0\u7684\u81ea\u56de\u5f52\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5bf9\u524d k \u4e2a\u6587\u6863\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\u3002\u6211\u4eec\u7684\u751f\u6210\u5f0f\u6392\u5e8f\u6a21\u578b\u4ece\u6587\u6863\u5019\u9009\u8005\u4e2d\u751f\u6210\u91cd\u65b0\u6392\u5e8f\u7684\u6587\u6863 ID \u548c\u7279\u5b9a\u7b54\u6848\uff0c\u6392\u5217\u65b9\u5f0f\u591a\u79cd\u591a\u6837\u3002\u5728\u4e24\u4e2a MRAQA \u57fa\u51c6 WebQA \u548c MultiModalQA \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5f3a\u5927\u7684\u57fa\u51c6\u76f8\u6bd4\u6709\u663e\u8457\u7684\u6539\u8fdb\uff0c\u7a81\u51fa\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/TonyBY/RAMQA", "author": "Yang Bai et.al.", "authors": "Yang Bai, Christan Earl Grant, Daisy Zhe Wang", "id": "2501.13297v1", "paper_url": "http://arxiv.org/abs/2501.13297v1", "repo": "https://github.com/tonyby/ramqa"}}