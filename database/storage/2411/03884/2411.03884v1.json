{"2411.03884": {"publish_time": "2024-11-06", "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models", "paper_summary": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.", "paper_summary_zh": "Transformer\u56e0\u5176\u5f37\u5927\u7684\u64ec\u5408\u80fd\u529b\uff0c\u5728\u5404\u500b\u9818\u57df\u4e2d\u5df2\u5ee3\u6cdb\u61c9\u7528\u3002\u9019\u7a2e\u6210\u529f\u90e8\u5206\u6b78\u529f\u65bc\u5176\u56fa\u6709\u7684\u975e\u7dda\u6027\u3002\u56e0\u6b64\uff0c\u9664\u4e86\u539f\u59cbTransformer\u67b6\u69cb\u4e2d\u4f7f\u7528\u7684 ReLU \u51fd\u6578\u5916\uff0c\u7814\u7a76\u4eba\u54e1\u9084\u63a2\u7d22\u4e86 GeLU \u548c SwishGLU \u7b49\u66ff\u4ee3\u6a21\u7d44\uff0c\u4ee5\u589e\u5f37\u975e\u7dda\u6027\uff0c\u5f9e\u800c\u64f4\u5145\u8868\u793a\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u985e\u65b0\u7684\u591a\u9805\u5f0f\u7d44\u5408\u6fc0\u6d3b\u51fd\u6578 (PolyCom)\uff0c\u65e8\u5728\u6700\u4f73\u5316Transformer\u7684\u52d5\u614b\u3002\u5728\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u63d0\u4f9b\u4e86 PolyCom \u7684\u5168\u9762\u6578\u5b78\u5206\u6790\uff0c\u7a81\u51fa\u4e86\u5176\u76f8\u5c0d\u65bc\u5176\u4ed6\u6fc0\u6d3b\u51fd\u6578\u7684\u589e\u5f37\u8868\u9054\u529b\u548c\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u8b49\u660e\u4e86\u7d50\u5408 PolyCom \u7684\u7db2\u8def\u53ef\u9054\u5230\u6700\u4f73\u8fd1\u4f3c\u7387\uff0c\u9019\u8868\u793a PolyCom \u7db2\u8def\u53ea\u9700\u8981\u6700\u5c11\u7684\u53c3\u6578\uff0c\u5373\u53ef\u903c\u8fd1 Sobolev \u7a7a\u9593\u4e2d\u7684\u4e00\u822c\u5e73\u6ed1\u51fd\u6578\u3002\u6211\u5011\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9810\u8a13\u7df4\u7d44\u614b\u9032\u884c\u4e86\u5be6\u8b49\u5be6\u9a57\uff0c\u5305\u62ec\u7a20\u5bc6\u548c\u7a00\u758f\u67b6\u69cb\u3002\u900f\u904e\u7528 PolyCom \u53d6\u4ee3\u50b3\u7d71\u7684\u6fc0\u6d3b\u51fd\u6578\uff0c\u6211\u5011\u8b93 LLM \u80fd\u5920\u64f7\u53d6\u8cc7\u6599\u4e2d\u7684\u9ad8\u968e\u4e92\u52d5\uff0c\u5f9e\u800c\u63d0\u5347\u6e96\u78ba\u5ea6\u548c\u6536\u6582\u7387\u7b49\u6548\u80fd\u6307\u6a19\u3002\u5927\u91cf\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u986f\u793a\u51fa\u76f8\u8f03\u65bc\u5176\u4ed6\u6fc0\u6d3b\u51fd\u6578\u6709\u986f\u8457\u7684\u9032\u6b65\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/BryceZhuo/PolyCom \u53d6\u5f97\u3002", "author": "Zhijian Zhuo et.al.", "authors": "Zhijian Zhuo, Ya Wang, Yutao Zeng, Xiaoqing Li, Xun Zhou, Jinwen Ma", "id": "2411.03884v1", "paper_url": "http://arxiv.org/abs/2411.03884v1", "repo": "null"}}