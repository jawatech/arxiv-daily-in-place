{"2405.15116": {"publish_time": "2024-05-24", "title": "Quantifying the Gain in Weak-to-Strong Generalization", "paper_summary": "Recent advances in large language models have shown capabilities that are\nextraordinary and near-superhuman. These models operate with such complexity\nthat reliably evaluating and aligning them proves challenging for humans. This\nleads to the natural question: can guidance from weak models (like humans)\nadequately direct the capabilities of strong models? In a recent and somewhat\nsurprising work, Burns et al. (2023) empirically demonstrated that when strong\nmodels (like GPT-4) are finetuned using labels generated by weak supervisors\n(like GPT-2), the strong models outperform their weaker counterparts -- a\nphenomenon they term weak-to-strong generalization.\n  In this work, we present a theoretical framework for understanding\nweak-to-strong generalization. Specifically, we show that the improvement in\nperformance achieved by strong models over their weaker counterparts is\nquantified by the misfit error incurred by the strong model on labels generated\nby the weaker model. Our theory reveals several curious algorithmic insights.\nFor instance, we can predict the amount by which the strong model will improve\nover the weak model, and also choose among different weak models to train the\nstrong model, based on its misfit error. We validate our theoretical findings\nthrough various empirical assessments.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u6700\u65b0\u9032\u5c55\u5c55\u73fe\u51fa\u975e\u51e1\u4e14\u8fd1\u4e4e\u8d85\u4eba\u7684\u80fd\u529b\u3002\u9019\u4e9b\u6a21\u578b\u4ee5\u6975\u9ad8\u7684\u8907\u96dc\u5ea6\u904b\u4f5c\uff0c\u53ef\u9760\u5730\u8a55\u4f30\u548c\u8abf\u6574\u5b83\u5011\u5c0d\u4eba\u985e\u4f86\u8aaa\u5177\u6709\u6311\u6230\u6027\u3002\u9019\u5c0e\u81f4\u4e86\u4e00\u500b\u81ea\u7136\u7684\u554f\u984c\uff1a\u4f86\u81ea\u5f31\u6a21\u578b\uff08\u5982\u4eba\u985e\uff09\u7684\u6307\u5c0e\u80fd\u5426\u5145\u5206\u6307\u5c0e\u5f37\u6a21\u578b\u7684\u80fd\u529b\uff1f\u5728\u6700\u8fd1\u4e00\u9805\u6709\u9ede\u4ee4\u4eba\u9a5a\u8a1d\u7684\u7814\u7a76\u4e2d\uff0cBurns \u7b49\u4eba\uff082023 \u5e74\uff09\u901a\u904e\u5be6\u8b49\u8b49\u660e\uff0c\u7576\u5f37\u6a21\u578b\uff08\u5982 GPT-4\uff09\u4f7f\u7528\u7531\u5f31\u76e3\u7763\u5668\uff08\u5982 GPT-2\uff09\u7522\u751f\u7684\u6a19\u7c64\u9032\u884c\u5fae\u8abf\u6642\uff0c\u5f37\u6a21\u578b\u7684\u8868\u73fe\u512a\u65bc\u5b83\u5011\u8f03\u5f31\u7684\u5c0d\u61c9\u6a21\u578b\u2014\u2014\u4ed6\u5011\u5c07\u9019\u7a2e\u73fe\u8c61\u7a31\u70ba\u5f31\u5230\u5f37\u7684\u6cdb\u5316\u3002\n\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7406\u8ad6\u6846\u67b6\u4f86\u7406\u89e3\u5f31\u5230\u5f37\u7684\u6cdb\u5316\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8868\u660e\uff0c\u5f37\u6a21\u578b\u76f8\u5c0d\u65bc\u5b83\u5011\u8f03\u5f31\u7684\u5c0d\u61c9\u6a21\u578b\u6240\u53d6\u5f97\u7684\u6548\u80fd\u63d0\u5347\uff0c\u662f\u7531\u5f37\u6a21\u578b\u5728\u8f03\u5f31\u6a21\u578b\u7522\u751f\u7684\u6a19\u7c64\u4e0a\u7522\u751f\u7684\u5931\u914d\u8aa4\u5dee\u91cf\u5316\u7684\u3002\u6211\u5011\u7684\u7406\u8ad6\u63ed\u793a\u4e86\u5e7e\u500b\u6709\u8da3\u7684\u6f14\u7b97\u6cd5\u898b\u89e3\u3002\u4f8b\u5982\uff0c\u6211\u5011\u53ef\u4ee5\u9810\u6e2c\u5f37\u6a21\u578b\u5c07\u6bd4\u5f31\u6a21\u578b\u6539\u5584\u7684\u7a0b\u5ea6\uff0c\u4e26\u4e14\u9084\u53ef\u4ee5\u6839\u64da\u5176\u5931\u914d\u8aa4\u5dee\uff0c\u5728\u4e0d\u540c\u7684\u5f31\u6a21\u578b\u4e2d\u9078\u64c7\u7528\u65bc\u8a13\u7df4\u5f37\u6a21\u578b\u7684\u6a21\u578b\u3002\u6211\u5011\u901a\u904e\u5404\u7a2e\u5be6\u8b49\u8a55\u4f30\u9a57\u8b49\u4e86\u6211\u5011\u7684\u7406\u8ad6\u767c\u73fe\u3002</paragraph>", "author": "Moses Charikar et.al.", "authors": "Moses Charikar, Chirag Pabbaraju, Kirankumar Shiragur", "id": "2405.15116v1", "paper_url": "http://arxiv.org/abs/2405.15116v1", "repo": "null"}}