{"2405.18428": {"publish_time": "2024-05-28", "title": "DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention", "paper_summary": "Diffusion models with large-scale pre-training have achieved significant\nsuccess in the field of visual content generation, particularly exemplified by\nDiffusion Transformers (DiT). However, DiT models have faced challenges with\nscalability and quadratic complexity efficiency. In this paper, we aim to\nleverage the long sequence modeling capability of Gated Linear Attention (GLA)\nTransformers, expanding its applicability to diffusion models. We introduce\nDiffusion Gated Linear Attention Transformers (DiG), a simple, adoptable\nsolution with minimal parameter overhead, following the DiT design, but\noffering superior efficiency and effectiveness. In addition to better\nperformance than DiT, DiG-S/2 exhibits $2.5\\times$ higher training speed than\nDiT-S/2 and saves $75.7\\%$ GPU memory at a resolution of $1792 \\times 1792$.\nMoreover, we analyze the scalability of DiG across a variety of computational\ncomplexity. DiG models, with increased depth/width or augmentation of input\ntokens, consistently exhibit decreasing FID. We further compare DiG with other\nsubquadratic-time diffusion models. With the same model size, DiG-XL/2 is\n$4.2\\times$ faster than the recent Mamba-based diffusion model at a $1024$\nresolution, and is $1.8\\times$ faster than DiT with CUDA-optimized\nFlashAttention-2 under the $2048$ resolution. All these results demonstrate its\nsuperior efficiency among the latest diffusion models. Code is released at\nhttps://github.com/hustvl/DiG.", "paper_summary_zh": "<paragraph>\u5177\u6709\u5927\u898f\u6a21\u9810\u8a13\u7df4\u7684\u64f4\u6563\u6a21\u578b\u5728\u8996\u89ba\u5167\u5bb9\u751f\u6210\u9818\u57df\u53d6\u5f97\u4e86\u986f\u8457\u7684\u6210\u529f\uff0c\u7279\u5225\u662f\u64f4\u6563Transformer (DiT) \u7684\u7bc4\u4f8b\u3002\u7136\u800c\uff0cDiT \u6a21\u578b\u5728\u53ef\u64f4\u5145\u6027\u548c\u4e8c\u6b21\u8907\u96dc\u5ea6\u6548\u7387\u65b9\u9762\u9762\u81e8\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u5229\u7528\u9580\u63a7\u7dda\u6027\u6ce8\u610f\u529b (GLA) Transformer\u7684\u9577\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u64f4\u5c55\u5176\u5728\u64f4\u6563\u6a21\u578b\u4e2d\u7684\u61c9\u7528\u6027\u3002\u6211\u5011\u5f15\u5165\u4e86\u64f4\u6563\u9580\u63a7\u7dda\u6027\u6ce8\u610f\u529bTransformer (DiG)\uff0c\u9019\u662f\u4e00\u500b\u7c21\u55ae\u3001\u53ef\u63a1\u7528\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u5177\u6709\u6700\u5c0f\u7684\u53c3\u6578\u958b\u92b7\uff0c\u9075\u5faa DiT \u8a2d\u8a08\uff0c\u4f46\u63d0\u4f9b\u5353\u8d8a\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002\u9664\u4e86\u6bd4 DiT \u66f4\u597d\u7684\u6027\u80fd\u5916\uff0cDiG-S/2 \u7684\u8a13\u7df4\u901f\u5ea6\u6bd4 DiT-S/2 \u9ad8\u51fa $2.5\\times$\uff0c\u4e26\u5728 $1792 \\times 1792$ \u7684\u89e3\u6790\u5ea6\u4e0b\u7bc0\u7701\u4e86 $75.7\\%$ \u7684 GPU \u8a18\u61b6\u9ad4\u3002\u6b64\u5916\uff0c\u6211\u5011\u5206\u6790\u4e86 DiG \u5728\u5404\u7a2e\u8a08\u7b97\u8907\u96dc\u5ea6\u4e0b\u7684\u53ef\u64f4\u5145\u6027\u3002DiG \u6a21\u578b\u5728\u589e\u52a0\u6df1\u5ea6/\u5bec\u5ea6\u6216\u8f38\u5165\u6a19\u8a18\u7684\u589e\u5f37\u4e0b\uff0c\u59cb\u7d42\u8868\u73fe\u51fa\u905e\u6e1b\u7684 FID\u3002\u6211\u5011\u9032\u4e00\u6b65\u5c07 DiG \u8207\u5176\u4ed6\u6b21\u4e8c\u6b21\u6642\u9593\u64f4\u6563\u6a21\u578b\u9032\u884c\u6bd4\u8f03\u3002\u5728\u76f8\u540c\u6a21\u578b\u5927\u5c0f\u4e0b\uff0cDiG-XL/2 \u5728 $1024$ \u89e3\u6790\u5ea6\u4e0b\u6bd4\u6700\u8fd1\u57fa\u65bc Mamba \u7684\u64f4\u6563\u6a21\u578b\u5feb $4.2\\times$\uff0c\u4e26\u4e14\u5728 $2048$ \u89e3\u6790\u5ea6\u4e0b\u6bd4\u5177\u6709 CUDA \u512a\u5316\u7684 FlashAttention-2 \u7684 DiT \u5feb $1.8\\times$\u3002\u6240\u6709\u9019\u4e9b\u7d50\u679c\u90fd\u8b49\u660e\u4e86\u5b83\u5728\u6700\u65b0\u64f4\u6563\u6a21\u578b\u4e2d\u7684\u5353\u8d8a\u6548\u7387\u3002\u7a0b\u5f0f\u78bc\u5df2\u767c\u5e03\u5728 https://github.com/hustvl/DiG\u3002</paragraph>", "author": "Lianghui Zhu et.al.", "authors": "Lianghui Zhu, Zilong Huang, Bencheng Liao, Jun Hao Liew, Hanshu Yan, Jiashi Feng, Xinggang Wang", "id": "2405.18428v1", "paper_url": "http://arxiv.org/abs/2405.18428v1", "repo": "https://github.com/hustvl/dig"}}