{"2407.10446": {"publish_time": "2024-07-15", "title": "DDFAD: Dataset Distillation Framework for Audio Data", "paper_summary": "Deep neural networks (DNNs) have achieved significant success in numerous\napplications. The remarkable performance of DNNs is largely attributed to the\navailability of massive, high-quality training datasets. However, processing\nsuch massive training data requires huge computational and storage resources.\nDataset distillation is a promising solution to this problem, offering the\ncapability to compress a large dataset into a smaller distilled dataset. The\nmodel trained on the distilled dataset can achieve comparable performance to\nthe model trained on the whole dataset.\n  While dataset distillation has been demonstrated in image data, none have\nexplored dataset distillation for audio data. In this work, for the first time,\nwe propose a Dataset Distillation Framework for Audio Data (DDFAD).\nSpecifically, we first propose the Fused Differential MFCC (FD-MFCC) as\nextracted features for audio data. After that, the FD-MFCC is distilled through\nthe matching training trajectory distillation method. Finally, we propose an\naudio signal reconstruction algorithm based on the Griffin-Lim Algorithm to\nreconstruct the audio signal from the distilled FD-MFCC. Extensive experiments\ndemonstrate the effectiveness of DDFAD on various audio datasets. In addition,\nwe show that DDFAD has promising application prospects in many applications,\nsuch as continual learning and neural architecture search.", "paper_summary_zh": "\u6df1\u5ea6\u795e\u7d93\u7db2\u8def (DNN) \u5728\u8a31\u591a\u61c9\u7528\u7a0b\u5f0f\u4e2d\u7372\u5f97\u986f\u8457\u7684\u6210\u529f\u3002DNN \u7684\u5353\u8d8a\u6548\u80fd\u4e3b\u8981\u6b78\u529f\u65bc\u5927\u91cf\u3001\u9ad8\u54c1\u8cea\u8a13\u7df4\u8cc7\u6599\u96c6\u7684\u53ef\u7528\u6027\u3002\u7136\u800c\uff0c\u8655\u7406\u5982\u6b64\u5927\u91cf\u7684\u8a13\u7df4\u8cc7\u6599\u9700\u8981\u9f90\u5927\u7684\u904b\u7b97\u548c\u5132\u5b58\u8cc7\u6e90\u3002\u8cc7\u6599\u96c6\u8403\u53d6\u662f\u89e3\u6c7a\u9019\u500b\u554f\u984c\u7684\u4e00\u500b\u6709\u524d\u9014\u7684\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u5c07\u5927\u578b\u8cc7\u6599\u96c6\u58d3\u7e2e\u6210\u8f03\u5c0f\u7684\u8403\u53d6\u8cc7\u6599\u96c6\u7684\u80fd\u529b\u3002\u5728\u8403\u53d6\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u53ef\u4ee5\u9054\u5230\u8207\u5728\u6574\u500b\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u76f8\u7576\u7684\u6548\u80fd\u3002\n\u96d6\u7136\u8cc7\u6599\u96c6\u8403\u53d6\u5df2\u5728\u5f71\u50cf\u8cc7\u6599\u4e2d\u5f97\u5230\u9a57\u8b49\uff0c\u4f46\u6c92\u6709\u4eba\u63a2\u7d22\u904e\u97f3\u8a0a\u8cc7\u6599\u7684\u8cc7\u6599\u96c6\u8403\u53d6\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u6b21\u63d0\u51fa\u97f3\u8a0a\u8cc7\u6599\u7684\u8cc7\u6599\u96c6\u8403\u53d6\u67b6\u69cb (DDFAD)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u878d\u5408\u5dee\u5206 MFCC (FD-MFCC) \u4f5c\u70ba\u97f3\u8a0a\u8cc7\u6599\u7684\u8403\u53d6\u7279\u5fb5\u3002\u5728\u90a3\u4e4b\u5f8c\uff0cFD-MFCC \u900f\u904e\u5339\u914d\u8a13\u7df4\u8ecc\u8de1\u8403\u53d6\u65b9\u6cd5\u9032\u884c\u8403\u53d6\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc Griffin-Lim \u6f14\u7b97\u6cd5\u7684\u97f3\u8a0a\u8a0a\u865f\u91cd\u5efa\u6f14\u7b97\u6cd5\uff0c\u5f9e\u8403\u53d6\u7684 FD-MFCC \u91cd\u5efa\u97f3\u8a0a\u8a0a\u865f\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86 DDFAD \u5728\u5404\u7a2e\u97f3\u8a0a\u8cc7\u6599\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86 DDFAD \u5728\u8a31\u591a\u61c9\u7528\u7a0b\u5f0f\u4e2d\u5177\u6709\u6709\u524d\u9014\u7684\u61c9\u7528\u524d\u666f\uff0c\u4f8b\u5982\u6301\u7e8c\u5b78\u7fd2\u548c\u795e\u7d93\u67b6\u69cb\u641c\u5c0b\u3002", "author": "Wenbo Jiang et.al.", "authors": "Wenbo Jiang, Rui Zhang, Hongwei Li, Xiaoyuan Liu, Haomiao Yang, Shui Yu", "id": "2407.10446v1", "paper_url": "http://arxiv.org/abs/2407.10446v1", "repo": "null"}}