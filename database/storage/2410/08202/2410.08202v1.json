{"2410.08202": {"publish_time": "2024-10-10", "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training", "paper_summary": "The rapid advancement of Large Language Models (LLMs) has led to an influx of\nefforts to extend their capabilities to multimodal tasks. Among them, growing\nattention has been focused on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nDespite the structural simplicity and deployment-friendliness, training a\nmonolithic MLLM with promising performance still remains challenging. In\nparticular, the popular approaches adopt continuous pre-training to extend a\npre-trained LLM to a monolithic MLLM, which suffers from catastrophic\nforgetting and leads to performance degeneration. In this paper, we aim to\novercome this limitation from the perspective of delta tuning. Specifically,\nour core idea is to embed visual parameters into a pre-trained LLM, thereby\nincrementally learning visual knowledge from massive data via delta tuning,\ni.e., freezing the LLM when optimizing the visual parameters. Based on this\nprinciple, we present Mono-InternVL, a novel monolithic MLLM that seamlessly\nintegrates a set of visual experts via a multimodal mixture-of-experts\nstructure. Moreover, we propose an innovative pre-training strategy to maximize\nthe visual capability of Mono-InternVL, namely Endogenous Visual Pre-training\n(EViP). In particular, EViP is designed as a progressive learning process for\nvisual experts, which aims to fully exploit the visual knowledge from noisy\ndata to high-quality data. To validate our approach, we conduct extensive\nexperiments on 16 benchmarks. Experimental results not only validate the\nsuperior performance of Mono-InternVL compared to the state-of-the-art MLLM on\n6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but\nalso confirm its better deployment efficiency, with first token latency reduced\nby up to 67%.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u9032\u5c55\u5df2\u5c0e\u81f4\u5927\u91cf\u52aa\u529b\u5c07\u5176\u80fd\u529b\u64f4\u5c55\u5230\u591a\u6a21\u614b\u4efb\u52d9\u3002\u5176\u4e2d\uff0c\u8d8a\u4f86\u8d8a\u591a\u7684\u95dc\u6ce8\u96c6\u4e2d\u5728\u55ae\u9ad4\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4e0a\uff0c\u5b83\u5c07\u8996\u89ba\u7de8\u78bc\u548c\u8a9e\u8a00\u89e3\u78bc\u6574\u5408\u5230\u4e00\u500b LLM \u4e2d\u3002\u5118\u7ba1\u7d50\u69cb\u7c21\u55ae\u4e14\u6613\u65bc\u90e8\u7f72\uff0c\u4f46\u8a13\u7df4\u4e00\u500b\u5177\u6709\u826f\u597d\u6027\u80fd\u7684\u55ae\u9ad4 MLLM \u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u5177\u9ad4\u800c\u8a00\uff0c\u6d41\u884c\u7684\u65b9\u6cd5\u63a1\u7528\u9023\u7e8c\u9810\u8a13\u7df4\u5c07\u9810\u8a13\u7df4\u7684 LLM \u64f4\u5c55\u5230\u55ae\u9ad4 MLLM\uff0c\u9019\u6703\u5c0e\u81f4\u707d\u96e3\u6027\u907a\u5fd8\u4e26\u5c0e\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u5f9e\u589e\u91cf\u8abf\u6574\u7684\u89d2\u5ea6\u514b\u670d\u9019\u4e00\u9650\u5236\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5c07\u8996\u89ba\u53c3\u6578\u5d4c\u5165\u5230\u9810\u8a13\u7df4\u7684 LLM \u4e2d\uff0c\u5f9e\u800c\u901a\u904e\u589e\u91cf\u8abf\u6574\u5f9e\u6d77\u91cf\u6578\u64da\u4e2d\u589e\u91cf\u5b78\u7fd2\u8996\u89ba\u77e5\u8b58\uff0c\u5373\u5728\u512a\u5316\u8996\u89ba\u53c3\u6578\u6642\u51cd\u7d50 LLM\u3002\u57fa\u65bc\u9019\u4e00\u539f\u7406\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Mono-InternVL\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u55ae\u9ad4 MLLM\uff0c\u5b83\u901a\u904e\u591a\u6a21\u614b\u5c08\u5bb6\u6df7\u5408\u7d50\u69cb\u7121\u7e2b\u6574\u5408\u4e86\u4e00\u7d44\u8996\u89ba\u5c08\u5bb6\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u9810\u8a13\u7df4\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316 Mono-InternVL \u7684\u8996\u89ba\u80fd\u529b\uff0c\u5373\u5167\u751f\u8996\u89ba\u9810\u8a13\u7df4 (EViP)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cEViP \u88ab\u8a2d\u8a08\u70ba\u8996\u89ba\u5c08\u5bb6\u7684\u6f38\u9032\u5b78\u7fd2\u904e\u7a0b\uff0c\u65e8\u5728\u5145\u5206\u5229\u7528\u5f9e\u566a\u8072\u6578\u64da\u5230\u9ad8\u8cea\u91cf\u6578\u64da\u7684\u8996\u89ba\u77e5\u8b58\u3002\u70ba\u4e86\u9a57\u8b49\u6211\u5011\u7684\u505a\u6cd5\uff0c\u6211\u5011\u5728 16 \u500b\u57fa\u6e96\u4e0a\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u5be6\u9a57\u7d50\u679c\u4e0d\u50c5\u9a57\u8b49\u4e86 Mono-InternVL \u8207\u6700\u5148\u9032\u7684 MLLM \u5728 6 \u500b\u591a\u6a21\u614b\u57fa\u6e96\u4e0a\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u4f8b\u5982\uff0c\u5728 OCRBench \u4e0a\u6bd4 InternVL-1.5 \u9ad8 113 \u5206\uff0c\u800c\u4e14\u9084\u78ba\u8a8d\u4e86\u5176\u66f4\u597d\u7684\u90e8\u7f72\u6548\u7387\uff0c\u9996\u500b\u4ee4\u724c\u5ef6\u9072\u964d\u4f4e\u4e86\u9ad8\u9054 67%\u3002", "author": "Gen Luo et.al.", "authors": "Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, Xizhou Zhu", "id": "2410.08202v1", "paper_url": "http://arxiv.org/abs/2410.08202v1", "repo": "null"}}