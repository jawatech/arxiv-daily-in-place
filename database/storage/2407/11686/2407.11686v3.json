{"2407.11686": {"publish_time": "2024-07-16", "title": "CCoE: A Compact LLM with Collaboration of Experts", "paper_summary": "In the domain of Large Language Model (LLM), LLMs demonstrate significant\ncapabilities in natural language understanding and generation. With the growing\nneeds of applying LLMs on various domains, it is a research question that how\nto efficiently train and build a model that has expertise in different domains\nbut with a low training cost. We propose CCoE architecture, a framework of\neasily coupling multiple strong domain experts together to fuse into a big LLM,\nprovides a collective way of utilizing the different domain expert LLMs.\nBesides, training a large collaborative of multiple expert LLMs requires a high\nrequirements on training sources. CCoE bypasses this problem through isolating\nother experts and train each expert separately. The design of CCoE assembles\nmultiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoE\nlayer could have one or more expert LLMs. Expert LLMs have different number of\nlayers and have been well-trained for different domain tasks. Each expert is\nfine-tuned to be able to achieve the comparable results with SOTA domain LLMs.\nWe start from 5 experts in the domain of Code, Math, Law, text-to-SQL and\nMedical. The results indicate that our CCoE framework can easily and\nefficiently boost nearly 10%-20% performance on original base model in\ndifferent domains but using less resources on training, as well as inference.", "paper_summary_zh": "\u5728\u5927\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u9818\u57df\u4e2d\uff0cLLM \u5728\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u5c55\u73fe\u51fa\u986f\u8457\u7684\u80fd\u529b\u3002\u96a8\u8457 LLM \u5728\u5404\u7a2e\u9818\u57df\u61c9\u7528\u7684\u9700\u6c42\u65e5\u76ca\u589e\u9577\uff0c\u5982\u4f55\u6709\u6548\u8a13\u7df4\u548c\u5efa\u7acb\u4e00\u500b\u5728\u4e0d\u540c\u9818\u57df\u5177\u6709\u5c08\u696d\u77e5\u8b58\u4f46\u8a13\u7df4\u6210\u672c\u4f4e\u7684\u6a21\u578b\uff0c\u662f\u4e00\u500b\u7814\u7a76\u554f\u984c\u3002\u6211\u5011\u63d0\u51fa CCoE \u67b6\u69cb\uff0c\u4e00\u500b\u5c07\u591a\u500b\u5f37\u5927\u7684\u9818\u57df\u5c08\u5bb6\u8f15\u9b06\u7d50\u5408\u5728\u4e00\u8d77\u4ee5\u878d\u5408\u6210\u4e00\u500b\u5927\u578b LLM \u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4e00\u7a2e\u5229\u7528\u4e0d\u540c\u9818\u57df\u5c08\u5bb6 LLM \u7684\u96c6\u5408\u65b9\u5f0f\u3002\u6b64\u5916\uff0c\u8a13\u7df4\u591a\u500b\u5c08\u5bb6 LLM \u7684\u5927\u578b\u5354\u4f5c\u9700\u8981\u5c0d\u8a13\u7df4\u4f86\u6e90\u63d0\u51fa\u5f88\u9ad8\u7684\u8981\u6c42\u3002CCoE \u901a\u904e\u9694\u96e2\u5176\u4ed6\u5c08\u5bb6\u4e26\u5206\u5225\u8a13\u7df4\u6bcf\u500b\u5c08\u5bb6\u4f86\u7e5e\u904e\u9019\u500b\u554f\u984c\u3002CCoE \u7684\u8a2d\u8a08\u901a\u904e CoE\uff08\u5c08\u5bb6\u5354\u4f5c\uff09\u5c64\u7d44\u88dd\u591a\u500b\u5c08\u5bb6 LLM\u3002\u6bcf\u500b CoE \u5c64\u53ef\u4ee5\u6709\u4e00\u500b\u6216\u591a\u500b\u5c08\u5bb6 LLM\u3002\u5c08\u5bb6 LLM \u5177\u6709\u4e0d\u540c\u7684\u5c64\u6578\uff0c\u4e26\u4e14\u5df2\u7d93\u91dd\u5c0d\u4e0d\u540c\u7684\u9818\u57df\u4efb\u52d9\u63a5\u53d7\u904e\u826f\u597d\u7684\u8a13\u7df4\u3002\u6bcf\u500b\u5c08\u5bb6\u90fd\u7d93\u904e\u5fae\u8abf\uff0c\u80fd\u5920\u9054\u5230\u8207 SOTA \u9818\u57df LLM \u76f8\u7576\u7684\u7d50\u679c\u3002\u6211\u5011\u5f9e\u7a0b\u5f0f\u78bc\u3001\u6578\u5b78\u3001\u6cd5\u5f8b\u3001\u6587\u5b57\u8f49 SQL \u548c\u91ab\u5b78\u9818\u57df\u7684 5 \u4f4d\u5c08\u5bb6\u958b\u59cb\u3002\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684 CCoE \u6846\u67b6\u53ef\u4ee5\u8f15\u9b06\u6709\u6548\u5730\u63d0\u5347\u4e0d\u540c\u9818\u57df\u4e2d\u539f\u59cb\u57fa\u790e\u6a21\u578b\u7684\u6027\u80fd\u8fd1 10%-20%\uff0c\u4f46\u8a13\u7df4\u548c\u63a8\u7406\u4f7f\u7528\u7684\u8cc7\u6e90\u66f4\u5c11\u3002", "author": "Shaomang Huang et.al.", "authors": "Shaomang Huang, Jianfeng Pan, Hanzhong Zheng", "id": "2407.11686v3", "paper_url": "http://arxiv.org/abs/2407.11686v3", "repo": "null"}}