{"2406.07887": {"publish_time": "2024-06-12", "title": "An Empirical Study of Mamba-based Language Models", "paper_summary": "Selective state-space models (SSMs) like Mamba overcome some of the\nshortcomings of Transformers, such as quadratic computational complexity with\nsequence length and large inference-time memory requirements from the key-value\ncache. Moreover, recent studies have shown that SSMs can match or exceed the\nlanguage modeling capabilities of Transformers, making them an attractive\nalternative. In a controlled setting (e.g., same data), however, studies so far\nhave only presented small scale experiments comparing SSMs to Transformers. To\nunderstand the strengths and weaknesses of these architectures at larger\nscales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and\nTransformer models trained on the same datasets of up to 3.5T tokens. We also\ncompare these models to a hybrid architecture consisting of 43% Mamba-2, 7%\nattention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks,\nwe answer the question of whether Mamba models can match Transformers at larger\ntraining budgets. Our results show that while pure SSMs match or exceed\nTransformers on many tasks, they lag behind Transformers on tasks which require\nstrong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook)\nor long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid\nexceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points\non average) and is predicted to be up to 8x faster when generating tokens at\ninference time. To validate long-context capabilities, we provide additional\nexperiments evaluating variants of the Mamba-2-Hybrid and Transformer extended\nto support 16K, 32K, and 128K sequences. On an additional 23 long-context\ntasks, the hybrid model continues to closely match or exceed the Transformer on\naverage. To enable further study, we release the checkpoints as well as the\ncode used to train our models as part of NVIDIA's Megatron-LM project.", "paper_summary_zh": "\u9078\u64c7\u6027\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM)\uff0c\u4f8b\u5982 Mamba\uff0c\u514b\u670d\u4e86 Transformer \u7684\u4e00\u4e9b\u7f3a\u9ede\uff0c\u4f8b\u5982\u5e8f\u5217\u9577\u5ea6\u7684\u4e8c\u6b21\u8a08\u7b97\u8907\u96dc\u5ea6\u548c\u4f86\u81ea\u9375\u503c\u5feb\u53d6\u7684\u5927\u91cf\u63a8\u8ad6\u6642\u9593\u8a18\u61b6\u9ad4\u9700\u6c42\u3002\u6b64\u5916\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cSSM \u53ef\u4ee5\u5339\u914d\u6216\u8d85\u904e Transformer \u7684\u8a9e\u8a00\u5efa\u6a21\u80fd\u529b\uff0c\u4f7f\u5176\u6210\u70ba\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5728\u53d7\u63a7\u74b0\u5883\uff08\u4f8b\u5982\uff0c\u76f8\u540c\u8cc7\u6599\uff09\u4e2d\uff0c\u8fc4\u4eca\u70ba\u6b62\u7684\u7814\u7a76\u50c5\u63d0\u51fa\u4e86\u5c0f\u898f\u6a21\u5be6\u9a57\uff0c\u5c07 SSM \u8207 Transformer \u9032\u884c\u6bd4\u8f03\u3002\u70ba\u4e86\u4e86\u89e3\u9019\u4e9b\u67b6\u69cb\u5728\u66f4\u5927\u898f\u6a21\u4e0a\u7684\u512a\u7f3a\u9ede\uff0c\u6211\u5011\u5c0d\u5728\u6700\u591a 3.5T \u500b\u6a19\u8a18\u7684\u76f8\u540c\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684 8B \u53c3\u6578 Mamba\u3001Mamba-2 \u548c Transformer \u6a21\u578b\u9032\u884c\u76f4\u63a5\u6bd4\u8f03\u3002\u6211\u5011\u9084\u5c07\u9019\u4e9b\u6a21\u578b\u8207\u7531 43% Mamba-2\u30017% \u6ce8\u610f\u529b\u548c 50% MLP \u5c64\uff08Mamba-2-Hybrid\uff09\u7d44\u6210\u7684\u6df7\u5408\u67b6\u69cb\u9032\u884c\u6bd4\u8f03\u3002\u4f7f\u7528\u591a\u6a23\u5316\u7684\u4efb\u52d9\u96c6\uff0c\u6211\u5011\u56de\u7b54\u4e86 Mamba \u6a21\u578b\u662f\u5426\u53ef\u4ee5\u5728\u66f4\u5927\u7684\u8a13\u7df4\u9810\u7b97\u4e0b\u5339\u914d Transformer \u7684\u554f\u984c\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u5118\u7ba1\u7d14 SSM \u5728\u8a31\u591a\u4efb\u52d9\u4e0a\u5339\u914d\u6216\u8d85\u904e Transformer\uff0c\u4f46\u5b83\u5011\u5728\u9700\u8981\u5f37\u5927\u7684\u8907\u88fd\u6216\u60c5\u5883\u5b78\u7fd2\u80fd\u529b\uff08\u4f8b\u5982\uff0c5 \u6b21 MMLU\u3001\u96fb\u8a71\u7c3f\uff09\u6216\u9577\u60c5\u5883\u63a8\u7406\u7684\u4efb\u52d9\u4e0a\u843d\u5f8c\u65bc Transformer\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u5011\u767c\u73fe 8B Mamba-2-Hybrid \u5728\u6211\u5011\u8a55\u4f30\u7684\u6240\u6709 12 \u500b\u6a19\u6e96\u4efb\u52d9\u4e0a\u90fd\u8d85\u904e\u4e86 8B Transformer\uff08\u5e73\u5747\u9ad8\u51fa 2.65 \u5206\uff09\uff0c\u4e26\u4e14\u9810\u6e2c\u5728\u63a8\u8ad6\u6642\u9593\u751f\u6210\u6a19\u8a18\u6642\u901f\u5ea6\u6700\u9ad8\u53ef\u9054 8 \u500d\u3002\u70ba\u4e86\u9a57\u8b49\u9577\u60c5\u5883\u80fd\u529b\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u984d\u5916\u7684\u5be6\u9a57\uff0c\u8a55\u4f30\u4e86 Mamba-2-Hybrid \u548c Transformer \u7684\u8b8a\u9ad4\uff0c\u4ee5\u652f\u63f4 16K\u300132K \u548c 128K \u5e8f\u5217\u3002\u5728\u53e6\u5916 23 \u500b\u9577\u60c5\u5883\u4efb\u52d9\u4e2d\uff0c\u6df7\u5408\u6a21\u578b\u7e7c\u7e8c\u8207 Transformer \u7dca\u5bc6\u5339\u914d\u6216\u8d85\u904e Transformer\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u5011\u91cb\u51fa\u4e86\u6aa2\u67e5\u9ede\u4ee5\u53ca\u7528\u65bc\u8a13\u7df4\u6211\u5011\u6a21\u578b\u7684\u7a0b\u5f0f\u78bc\uff0c\u4f5c\u70ba NVIDIA \u7684 Megatron-LM \u9805\u76ee\u7684\u5176\u4e2d\u4e00\u90e8\u5206\u3002", "author": "Roger Waleffe et.al.", "authors": "Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, Bryan Catanzaro", "id": "2406.07887v1", "paper_url": "http://arxiv.org/abs/2406.07887v1", "repo": "null"}}