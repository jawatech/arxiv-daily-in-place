{"2504.15610": {"publish_time": "2025-04-22", "title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings", "paper_summary": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy.", "paper_summary_zh": "<paragraph>\u76ee\u524d\u7684\u7814\u7a76\u63cf\u8ff0\u4e86\u4e00\u7a2e\u7d93\u6fdf\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u8abf\u6574\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4f7f\u5176\u9069\u7528\u65bc\u5b78\u8853 advising\uff0c\u4e26\u8003\u616e\u5230\u6d77\u5916\u7559\u5b78\u7684\u80cc\u666f\uff0c\u4ee5\u53ca\u61c9\u7528\u65bc\u4f4e\u8cc7\u6e90\u7684\u6587\u5316\u9069\u61c9\u65b9\u6cd5\u3002\u900f\u904e\u61c9\u7528\u4f4e\u79e9\u9069\u61c9 (LoRA) \u65b9\u6cd5\u548c 4 \u4f4d\u5143\u91cf\u5316\u65b9\u6cd5\u7684 Mistral-7B-Instruct \u6a21\u578b\uff0c\u8a72\u6a21\u578b\u91dd\u5c0d\u672c\u7814\u7a76\u76ee\u7684\u9032\u884c\u4e86\u5169\u500b\u4e0d\u540c\u968e\u6bb5\u7684\u8a13\u7df4\uff0c\u4ee5\u589e\u5f37\u9818\u57df\u7279\u7570\u6027\uff0c\u540c\u6642\u4fdd\u6301\u8a08\u7b97\u6548\u7387\u3002\u5728\u7b2c\u4e00\u968e\u6bb5\uff0c\u900f\u904e Gemini Pro API \u4f7f\u7528\u5408\u6210\u6578\u64da\u96c6\u5c0d\u6a21\u578b\u9032\u884c\u8abf\u6574\uff1b\u5728\u7b2c\u4e8c\u968e\u6bb5\uff0c\u4f7f\u7528 StudyAbroadGPT \u5c08\u6848\u4e2d\u624b\u52d5\u6574\u7406\u7684\u6578\u64da\u96c6\u5c0d\u6a21\u578b\u9032\u884c\u8a13\u7df4\uff0c\u4ee5\u7372\u5f97\u589e\u5f37\u7684\u3001\u60c5\u5883\u5316\u7684\u56de\u61c9\u3002\u6280\u8853\u5275\u65b0\u5305\u62ec\u9ad8\u6548\u8a18\u61b6\u9ad4\u7684\u91cf\u5316\u3001\u9ad8\u6548\u53c3\u6578\u7684\u9069\u61c9\uff0c\u4ee5\u53ca\u900f\u904e Weights & Biases \u9032\u884c\u7684\u9023\u7e8c\u8a13\u7df4\u5206\u6790\u3002\u8a13\u7df4\u5f8c\uff0c\u9019\u9805\u7814\u7a76\u986f\u793a\u8a13\u7df4\u640d\u5931\u6e1b\u5c11\u4e86 52.7%\uff0c\u9818\u57df\u7279\u5b9a\u5efa\u8b70\u7684\u6e96\u78ba\u5ea6\u9054\u5230 92%\uff0c\u5be6\u73fe\u4e86 95% \u57fa\u65bc markdown \u7684\u683c\u5f0f\u652f\u63f4\uff0c\u4e26\u4e14\u5728\u73fe\u6210 GPU \u8a2d\u5099\u4e0a\u7684\u4e2d\u4f4d\u6578\u904b\u884c\u7387\u70ba\u6bcf\u79d2 100 \u500b\u6a23\u672c\u3002\u9019\u4e9b\u767c\u73fe\u652f\u63f4\u5728\u6559\u80b2\u9867\u554f\u4e2d\u6709\u6548\u61c9\u7528\u6307\u4ee4\u5fae\u8abf\u7684 LLM\uff0c\u5c24\u5176\u662f\u5728\u8cc7\u6e90\u5331\u4e4f\u7684\u6a5f\u69cb\u5834\u666f\u4e2d\u3002\u9650\u5236\u5305\u62ec\u6cdb\u5316\u6027\u964d\u4f4e\u548c\u5408\u6210\u6578\u64da\u96c6\u7684\u61c9\u7528\uff0c\u4f46\u6b64\u6846\u67b6\u53ef\u64f4\u5c55\u4ee5\u65b0\u589e\u591a\u8a9e\u8a00\u589e\u5f37\u548c\u5373\u6642\u5b78\u8853 advising \u6d41\u7a0b\u3002\u672a\u4f86\u7684\u65b9\u5411\u53ef\u80fd\u5305\u62ec\u6574\u5408\u6aa2\u7d22\u589e\u5f37\u751f\u6210\u3001\u61c9\u7528\u52d5\u614b\u91cf\u5316\u4f8b\u7a0b\uff0c\u4ee5\u53ca\u9023\u63a5\u5230\u5373\u6642\u5b78\u8853\u8cc7\u6599\u5eab\u4ee5\u63d0\u9ad8\u9069\u61c9\u6027\u548c\u6e96\u78ba\u6027\u7684\u8a08\u756b\u3002</paragraph>\n", "author": "Md Millat Hosen et.al.", "authors": "Md Millat Hosen", "id": "2504.15610v2", "paper_url": "http://arxiv.org/abs/2504.15610v2", "repo": "https://github.com/codermillat/StudyAbroadGPT"}}