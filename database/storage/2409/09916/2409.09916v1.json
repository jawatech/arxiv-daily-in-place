{"2409.09916": {"publish_time": "2024-09-16", "title": "SFR-RAG: Towards Contextually Faithful LLMs", "paper_summary": "Retrieval Augmented Generation (RAG), a paradigm that integrates external\ncontextual information with large language models (LLMs) to enhance factual\naccuracy and relevance, has emerged as a pivotal area in generative AI. The\nLLMs used in RAG applications are required to faithfully and completely\ncomprehend the provided context and users' questions, avoid hallucination,\nhandle unanswerable, counterfactual or otherwise low-quality and irrelevant\ncontexts, perform complex multi-hop reasoning and produce reliable citations.\nIn this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with\nan emphasis on context-grounded generation and hallucination minimization. We\nalso present ContextualBench, a new evaluation framework compiling multiple\npopular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with\nconsistent RAG settings to ensure reproducibility and consistency in model\nassessments. Experimental results demonstrate that our SFR-RAG-9B model\noutperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving\nstate-of-the-art results in 3 out of 7 benchmarks in ContextualBench with\nsignificantly fewer parameters. The model is also shown to be resilient to\nalteration in the contextual information and behave appropriately when relevant\ncontext is removed. Additionally, the SFR-RAG model maintains competitive\nperformance in general instruction-following tasks and function-calling\ncapabilities.", "paper_summary_zh": "\u6aa2\u7d22\u64f4\u589e\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u7a2e\u5c07\u5916\u90e8\u4e0a\u4e0b\u6587\u8cc7\u8a0a\u8207\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u6574\u5408\u4ee5\u589e\u5f37\u4e8b\u5be6\u6e96\u78ba\u5ea6\u548c\u76f8\u95dc\u6027\u7684\u7bc4\u4f8b\uff0c\u5df2\u6210\u70ba\u751f\u6210\u5f0f AI \u4e2d\u7684\u95dc\u9375\u9818\u57df\u3002RAG \u61c9\u7528\u7a0b\u5f0f\u4e2d\u4f7f\u7528\u7684 LLM \u5fc5\u9808\u5fe0\u5be6\u4e14\u5b8c\u6574\u5730\u7406\u89e3\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u548c\u4f7f\u7528\u8005\u7684\u554f\u984c\uff0c\u907f\u514d\u7522\u751f\u5e7b\u89ba\u3001\u8655\u7406\u7121\u6cd5\u56de\u7b54\u3001\u53cd\u4e8b\u5be6\u6216\u5176\u4ed6\u4f4e\u54c1\u8cea\u548c\u4e0d\u76f8\u95dc\u7684\u4e0a\u4e0b\u6587\uff0c\u57f7\u884c\u8907\u96dc\u7684\u591a\u8df3\u63a8\u7406\u4e26\u7522\u751f\u53ef\u9760\u7684\u5f15\u6587\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 SFR-RAG\uff0c\u9019\u662f\u4e00\u500b\u7d93\u904e\u6307\u4ee4\u5fae\u8abf\u7684\u5c0f\u578b LLM\uff0c\u91cd\u9ede\u5728\u65bc\u4ee5\u4e0a\u4e0b\u6587\u70ba\u57fa\u790e\u7684\u751f\u6210\u548c\u5e7b\u89ba\u6700\u5c0f\u5316\u3002\u6211\u5011\u9084\u63d0\u4f9b\u4e86 ContextualBench\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u8a55\u4f30\u6846\u67b6\uff0c\u7de8\u5236\u4e86\u591a\u500b\u6d41\u884c\u4e14\u591a\u6a23\u5316\u7684 RAG \u57fa\u6e96\uff0c\u4f8b\u5982 HotpotQA \u548c TriviaQA\uff0c\u4e26\u63a1\u7528\u4e00\u81f4\u7684 RAG \u8a2d\u5b9a\uff0c\u4ee5\u78ba\u4fdd\u6a21\u578b\u8a55\u4f30\u7684\u53ef\u91cd\u8907\u6027\u548c\u4e00\u81f4\u6027\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684 SFR-RAG-9B \u6a21\u578b\u512a\u65bc\u9818\u5148\u7684\u57fa\u6e96\uff0c\u4f8b\u5982 Command-R+\uff08104B\uff09\u548c GPT-4o\uff0c\u5728 ContextualBench \u4e2d\u7684 7 \u500b\u57fa\u6e96\u4e2d\u7684 3 \u500b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u7d50\u679c\uff0c\u4e14\u53c3\u6578\u660e\u986f\u8f03\u5c11\u3002\u8a72\u6a21\u578b\u9084\u986f\u793a\u51fa\u5c0d\u4e0a\u4e0b\u6587\u8cc7\u8a0a\u8b8a\u5316\u7684\u97cc\u6027\uff0c\u4e26\u5728\u79fb\u9664\u76f8\u95dc\u4e0a\u4e0b\u6587\u6642\u8868\u73fe\u5f97\u7576\u3002\u6b64\u5916\uff0cSFR-RAG \u6a21\u578b\u5728\u4e00\u822c\u7684\u6307\u4ee4\u9075\u5faa\u4efb\u52d9\u548c\u51fd\u5f0f\u547c\u53eb\u80fd\u529b\u4e2d\u7dad\u6301\u4e86\u7af6\u722d\u529b\u3002", "author": "Xuan-Phi Nguyen et.al.", "authors": "Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Silvio Savarese, Caiming Xong, Shafiq Joty", "id": "2409.09916v1", "paper_url": "http://arxiv.org/abs/2409.09916v1", "repo": "null"}}