{"2412.06581": {"publish_time": "2024-12-09", "title": "EmoSpeech: A Corpus of Emotionally Rich and Contextually Detailed Speech Annotations", "paper_summary": "Advances in text-to-speech (TTS) technology have significantly improved the\nquality of generated speech, closely matching the timbre and intonation of the\ntarget speaker. However, due to the inherent complexity of human emotional\nexpression, the development of TTS systems capable of controlling subtle\nemotional differences remains a formidable challenge. Existing emotional speech\ndatabases often suffer from overly simplistic labelling schemes that fail to\ncapture a wide range of emotional states, thus limiting the effectiveness of\nemotion synthesis in TTS applications. To this end, recent efforts have\nfocussed on building databases that use natural language annotations to\ndescribe speech emotions. However, these approaches are costly and require more\nemotional depth to train robust systems. In this paper, we propose a novel\nprocess aimed at building databases by systematically extracting emotion-rich\nspeech segments and annotating them with detailed natural language descriptions\nthrough a generative model. This approach enhances the emotional granularity of\nthe database and significantly reduces the reliance on costly manual\nannotations by automatically augmenting the data with high-level language\nmodels. The resulting rich database provides a scalable and economically viable\nsolution for developing a more nuanced and dynamic basis for developing\nemotionally controlled TTS systems.", "paper_summary_zh": "\u6587\u5b57\u8f49\u8a9e\u97f3 (TTS) \u6280\u8853\u7684\u9032\u5c55\u986f\u8457\u63d0\u5347\u4e86\u5408\u6210\u8a9e\u97f3\u7684\u54c1\u8cea\uff0c\u80fd\u7dca\u5bc6\u8cbc\u5408\u76ee\u6a19\u8aaa\u8a71\u8005\u7684\u97f3\u8272\u548c\u8a9e\u8abf\u3002\u7136\u800c\uff0c\u7531\u65bc\u4eba\u985e\u60c5\u7dd2\u8868\u9054\u7684\u8907\u96dc\u6027\uff0c\u958b\u767c\u51fa\u80fd\u63a7\u5236\u7d30\u5fae\u60c5\u7dd2\u5dee\u7570\u7684 TTS \u7cfb\u7d71\u4ecd\u7136\u662f\u4e00\u9805\u8271\u9245\u7684\u6311\u6230\u3002\u73fe\u6709\u7684\u60c5\u7dd2\u5316\u8a9e\u97f3\u8cc7\u6599\u5eab\u5e38\u5e38\u63a1\u7528\u904e\u65bc\u7c21\u5316\u7684\u6a19\u8a18\u65b9\u6848\uff0c\u7121\u6cd5\u6355\u6349\u5ee3\u6cdb\u7684\u60c5\u7dd2\u72c0\u614b\uff0c\u56e0\u6b64\u9650\u5236\u4e86 TTS \u61c9\u7528\u4e2d\u60c5\u7dd2\u5408\u6210\u7684\u6709\u6548\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5c08\u6ce8\u65bc\u5efa\u7acb\u4f7f\u7528\u81ea\u7136\u8a9e\u8a00\u8a3b\u89e3\u4f86\u63cf\u8ff0\u8a9e\u97f3\u60c5\u7dd2\u7684\u8cc7\u6599\u5eab\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\uff0c\u800c\u4e14\u9700\u8981\u66f4\u591a\u7684\u60c5\u7dd2\u6df1\u5ea6\u4f86\u8a13\u7df4\u5f37\u5065\u7684\u7cfb\u7d71\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u6d41\u7a0b\uff0c\u65e8\u5728\u900f\u904e\u7cfb\u7d71\u6027\u5730\u8403\u53d6\u60c5\u7dd2\u8c50\u5bcc\u7684\u8a9e\u97f3\u7247\u6bb5\uff0c\u4e26\u900f\u904e\u751f\u6210\u6a21\u578b\u70ba\u5176\u52a0\u4e0a\u8a73\u7d30\u7684\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\uff0c\u4f86\u5efa\u7acb\u8cc7\u6599\u5eab\u3002\u9019\u7a2e\u65b9\u6cd5\u589e\u5f37\u4e86\u8cc7\u6599\u5eab\u7684\u60c5\u7dd2\u7c92\u5ea6\uff0c\u4e26\u900f\u904e\u81ea\u52d5\u4f7f\u7528\u9ad8\u968e\u8a9e\u8a00\u6a21\u578b\u4f86\u64f4\u5145\u8cc7\u6599\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u5c0d\u6602\u8cb4\u7684\u4eba\u5de5\u8a3b\u89e3\u7684\u4f9d\u8cf4\u3002\u7522\u751f\u7684\u8c50\u5bcc\u8cc7\u6599\u5eab\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u4e14\u7d93\u6fdf\u53ef\u884c\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u5efa\u7acb\u4e00\u500b\u66f4\u7d30\u7dfb\u4e14\u52d5\u614b\u7684\u57fa\u790e\uff0c\u4f86\u958b\u767c\u53d7\u60c5\u7dd2\u63a7\u5236\u7684 TTS \u7cfb\u7d71\u3002", "author": "Weizhen Bian et.al.", "authors": "Weizhen Bian, Yubo Zhou, Kaitai Zhang, Xiaohan Gu", "id": "2412.06581v1", "paper_url": "http://arxiv.org/abs/2412.06581v1", "repo": "null"}}