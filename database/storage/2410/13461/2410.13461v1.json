{"2410.13461": {"publish_time": "2024-10-17", "title": "Progressive Mixed-Precision Decoding for Efficient LLM Inference", "paper_summary": "In spite of the great potential of large language models (LLMs) across\nvarious tasks, their deployment on resource-constrained devices remains\nchallenging due to their excessive computational and memory demands.\nQuantization has emerged as an effective solution by storing weights in reduced\nprecision. However, utilizing low precisions (i.e.~2/3-bit) to substantially\nalleviate the memory-boundedness of LLM decoding, still suffers from\nprohibitive performance drop. In this work, we argue that existing approaches\nfail to explore the diversity in computational patterns, redundancy, and\nsensitivity to approximations of the different phases of LLM inference,\nresorting to a uniform quantization policy throughout. Instead, we propose a\nnovel phase-aware method that selectively allocates precision during different\nphases of LLM inference, achieving both strong context extraction during\nprefill and efficient memory bandwidth utilization during decoding. To further\naddress the memory-boundedness of the decoding phase, we introduce Progressive\nMixed-Precision Decoding (PMPD), a technique that enables the gradual lowering\nof precision deeper in the generated sequence, together with a spectrum of\nprecision-switching schedulers that dynamically drive the precision-lowering\ndecisions in either task-adaptive or prompt-adaptive manner. Extensive\nevaluation across diverse language tasks shows that when targeting Nvidia GPUs,\nPMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over\nfp16 models, while when targeting an LLM-optimized NPU, our approach delivers a\nthroughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$\nover uniform quantization approaches while preserving the output quality.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5177\u6709\u6975\u5927\u7684\u6f5b\u529b\uff0c\u4f46\u7531\u65bc\u5176\u904e\u9ad8\u7684\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u5b83\u5011\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u88dd\u7f6e\u4e0a\u90e8\u7f72\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u91cf\u5316\u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u6548\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u65b9\u6cd5\u662f\u5c07\u6b0a\u91cd\u5132\u5b58\u5728\u964d\u4f4e\u7684\u7cbe\u5ea6\u4e2d\u3002\u7136\u800c\uff0c\u5229\u7528\u4f4e\u7cbe\u5ea6\uff08\u5373 2/3 \u4f4d\uff09\u4f86\u5927\u5e45\u6e1b\u8f15 LLM \u89e3\u78bc\u7684\u8a18\u61b6\u9ad4\u53d7\u9650\u6027\uff0c\u4ecd\u7136\u6703\u5c0e\u81f4\u6548\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a8d\u70ba\u73fe\u6709\u65b9\u6cd5\u672a\u80fd\u63a2\u7d22\u4e0d\u540c\u968e\u6bb5\u7684 LLM \u63a8\u8ad6\u7684\u904b\u7b97\u6a21\u5f0f\u3001\u5197\u9918\u548c\u5c0d\u8fd1\u4f3c\u503c\u7684\u654f\u611f\u6027\uff0c\u800c\u662f\u5728\u6574\u500b\u904e\u7a0b\u4e2d\u63a1\u7528\u7d71\u4e00\u7684\u91cf\u5316\u7b56\u7565\u3002\u76f8\u53cd\u5730\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u76f8\u4f4d\u611f\u77e5\u65b9\u6cd5\uff0c\u5b83\u5728 LLM \u63a8\u8ad6\u7684\u4e0d\u540c\u968e\u6bb5\u4e2d\u9078\u64c7\u6027\u5730\u5206\u914d\u7cbe\u5ea6\uff0c\u5728\u9810\u5148\u586b\u5165\u671f\u9593\u5be6\u73fe\u5f37\u5927\u7684\u5167\u5bb9\u63d0\u53d6\uff0c\u4e26\u5728\u89e3\u78bc\u671f\u9593\u6709\u6548\u5229\u7528\u8a18\u61b6\u9ad4\u983b\u5bec\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u89e3\u6c7a\u89e3\u78bc\u968e\u6bb5\u7684\u8a18\u61b6\u9ad4\u53d7\u9650\u6027\uff0c\u6211\u5011\u5f15\u5165\u4e86\u6f38\u9032\u6df7\u5408\u7cbe\u5ea6\u89e3\u78bc (PMPD)\uff0c\u9019\u662f\u4e00\u7a2e\u6280\u8853\uff0c\u53ef\u4ee5\u9010\u6f38\u964d\u4f4e\u751f\u6210\u5e8f\u5217\u4e2d\u8f03\u6df1\u7684\u7cbe\u5ea6\uff0c\u540c\u6642\u5177\u6709\u4e00\u7cfb\u5217\u7cbe\u5ea6\u5207\u63db\u6392\u7a0b\u5668\uff0c\u4ee5\u4efb\u52d9\u81ea\u9069\u61c9\u6216\u63d0\u793a\u81ea\u9069\u61c9\u7684\u65b9\u5f0f\u52d5\u614b\u9a45\u52d5\u7cbe\u5ea6\u964d\u4f4e\u6c7a\u7b56\u3002\u5728\u5404\u7a2e\u8a9e\u8a00\u4efb\u52d9\u4e2d\u7684\u5ee3\u6cdb\u8a55\u4f30\u986f\u793a\uff0c\u7576\u76ee\u6a19\u662f Nvidia GPU \u6642\uff0cPMPD \u5728\u77e9\u9663\u5411\u91cf\u4e58\u6cd5\u4e2d\u6bd4 fp16 \u6a21\u578b\u5feb 1.4-12.2 \u500d\uff0c\u800c\u7576\u76ee\u6a19\u662f LLM \u6700\u4f73\u5316 NPU \u6642\uff0c\u6211\u5011\u7684\u505a\u6cd5\u6bd4 fp16 \u6a21\u578b\u63d0\u9ad8\u4e86 3.8-8.0 \u500d\u7684\u541e\u5410\u91cf\uff0c\u4e26\u4e14\u6bd4\u7d71\u4e00\u91cf\u5316\u65b9\u6cd5\u63d0\u9ad8\u4e86 1.54 \u500d\uff0c\u540c\u6642\u4fdd\u7559\u4e86\u8f38\u51fa\u54c1\u8cea\u3002", "author": "Hao Mark Chen et.al.", "authors": "Hao Mark Chen, Fuwen Tan, Alexandros Kouris, Royson Lee, Hongxiang Fan, Stylianos I. Venieris", "id": "2410.13461v1", "paper_url": "http://arxiv.org/abs/2410.13461v1", "repo": "null"}}