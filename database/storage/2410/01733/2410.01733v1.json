{"2410.01733": {"publish_time": "2024-10-02", "title": "Visual Perception in Text Strings", "paper_summary": "Understanding visual semantics embedded in consecutive characters is a\ncrucial capability for both large language models (LLMs) and multi-modal large\nlanguage models (MLLMs). This type of artifact possesses the unique\ncharacteristic that identical information can be readily formulated in both\ntexts and images, making them a significant proxy for analyzing modern LLMs'\nand MLLMs' capabilities in modality-agnostic vision understanding. In this\nwork, we select ASCII art as a representative artifact, where the lines and\nbrightness used to depict each concept are rendered by characters, and we frame\nthe problem as an ASCII art recognition task. We benchmark model performance on\nthis task by constructing an evaluation dataset with an elaborate\ncategorization tree and also collect a training set to elicit the models'\nvisual perception ability. Through a comprehensive analysis of dozens of\nmodels, results reveal that although humans can achieve nearly 100% accuracy,\nthe state-of-the-art LLMs and MLLMs lag far behind. Models are capable of\nrecognizing concepts depicted in the ASCII arts given only text inputs\nindicated by over 60% accuracy for some concepts, but most of them achieves\nmerely around 30% accuracy when averaged across all categories. When provided\nwith images as inputs, GPT-4o gets 82.68%, outperforming the strongest\nopen-source MLLM by 21.95%. Although models favor different kinds of ASCII art\ndepending on the modality provided, none of the MLLMs successfully benefit when\nboth modalities are supplied simultaneously. Moreover, supervised fine-tuning\nhelps improve models' accuracy especially when provided with the image\nmodality, but also highlights the need for better training techniques to\nenhance the information fusion among modalities.", "paper_summary_zh": "<paragraph>\u7406\u89e3\u9023\u7e8c\u5b57\u5143\u4e2d\u5167\u542b\u7684\u8996\u89ba\u8a9e\u610f\uff0c\u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u4e00\u9805\u95dc\u9375\u80fd\u529b\u3002\u9019\u985e\u4eba\u5de5\u88fd\u54c1\u5177\u5099\u4e00\u500b\u7368\u7279\u7279\u6027\uff0c\u5373\u76f8\u540c\u7684\u8cc7\u8a0a\u53ef\u4ee5\u8f15\u6613\u5730\u4ee5\u6587\u5b57\u548c\u5f71\u50cf\u7684\u5f62\u5f0f\u8868\u9054\uff0c\u8b93\u5b83\u5011\u6210\u70ba\u5206\u6790\u73fe\u4ee3 LLM \u548c MLLM \u5728\u8207\u6a21\u614b\u7121\u95dc\u7684\u8996\u89ba\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u7684\u91cd\u8981\u4ee3\u7406\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u9078\u64c7 ASCII \u85dd\u8853\u4f5c\u70ba\u4ee3\u8868\u6027\u4eba\u5de5\u88fd\u54c1\uff0c\u5176\u4e2d\u7528\u65bc\u63cf\u7e6a\u6bcf\u500b\u6982\u5ff5\u7684\u7dda\u689d\u548c\u4eae\u5ea6\u90fd\u662f\u7531\u5b57\u5143\u6e32\u67d3\u7684\uff0c\u6211\u5011\u5c07\u554f\u984c\u754c\u5b9a\u70ba ASCII \u85dd\u8853\u8fa8\u8b58\u4efb\u52d9\u3002\u6211\u5011\u900f\u904e\u5efa\u69cb\u4e00\u500b\u5177\u6709\u8a73\u7d30\u5206\u985e\u6a39\u7684\u8a55\u4f30\u8cc7\u6599\u96c6\uff0c\u5c0d\u6b64\u4efb\u52d9\u4e2d\u7684\u6a21\u578b\u6548\u80fd\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u4e26\u6536\u96c6\u4e00\u500b\u8a13\u7df4\u96c6\u4ee5\u5f15\u51fa\u6a21\u578b\u7684\u8996\u89ba\u611f\u77e5\u80fd\u529b\u3002\u900f\u904e\u5c0d\u6578\u5341\u500b\u6a21\u578b\u9032\u884c\u5168\u9762\u5206\u6790\uff0c\u7d50\u679c\u986f\u793a\uff0c\u5118\u7ba1\u4eba\u985e\u53ef\u4ee5\u9054\u5230\u8fd1 100% \u7684\u6e96\u78ba\u5ea6\uff0c\u4f46\u6700\u5148\u9032\u7684 LLM \u548c MLLM \u4ecd\u9060\u9060\u843d\u5f8c\u3002\u6a21\u578b\u80fd\u5920\u8fa8\u8b58 ASCII \u85dd\u8853\u4e2d\u63cf\u7e6a\u7684\u6982\u5ff5\uff0c\u50c5\u7d66\u4e88\u6587\u5b57\u8f38\u5165\uff0c\u67d0\u4e9b\u6982\u5ff5\u7684\u6e96\u78ba\u5ea6\u8d85\u904e 60%\uff0c\u4f46\u5927\u591a\u6578\u6a21\u578b\u5728\u6240\u6709\u985e\u5225\u7684\u5e73\u5747\u6e96\u78ba\u5ea6\u50c5\u7d04 30%\u3002\u7576\u63d0\u4f9b\u5f71\u50cf\u4f5c\u70ba\u8f38\u5165\u6642\uff0cGPT-4o \u7372\u5f97 82.68%\uff0c\u6bd4\u6700\u5f37\u7684\u958b\u6e90 MLLM \u9ad8\u51fa 21.95%\u3002\u5118\u7ba1\u6a21\u578b\u504f\u597d\u4e0d\u540c\u985e\u578b\u7684 ASCII \u85dd\u8853\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u63d0\u4f9b\u7684\u6a21\u614b\uff0c\u4f46\u7576\u540c\u6642\u63d0\u4f9b\u5169\u7a2e\u6a21\u614b\u6642\uff0c\u6c92\u6709\u4efb\u4f55 MLLM \u6210\u529f\u53d7\u76ca\u3002\u6b64\u5916\uff0c\u76e3\u7763\u5fae\u8abf\u6709\u52a9\u65bc\u63d0\u9ad8\u6a21\u578b\u7684\u6e96\u78ba\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u63d0\u4f9b\u5f71\u50cf\u6a21\u614b\u6642\uff0c\u4f46\u4e5f\u7a81\u986f\u4e86\u9700\u8981\u66f4\u597d\u7684\u8a13\u7df4\u6280\u8853\u4f86\u589e\u5f37\u6a21\u614b\u4e4b\u9593\u7684\u8cc7\u8a0a\u878d\u5408\u3002</paragraph>", "author": "Qi Jia et.al.", "authors": "Qi Jia, Xiang Yue, Shanshan Huang, Ziheng Qin, Yizhu Liu, Bill Yuchen Lin, Yang You", "id": "2410.01733v1", "paper_url": "http://arxiv.org/abs/2410.01733v1", "repo": "null"}}