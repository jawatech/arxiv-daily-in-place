{"2405.11613": {"publish_time": "2024-05-19", "title": "Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts", "paper_summary": "The knowledge within large language models (LLMs) may become outdated\nquickly. While in-context editing (ICE) is currently the most effective method\nfor knowledge editing (KE), it is constrained by the black-box modeling of LLMs\nand thus lacks interpretability. Our work aims to elucidate the superior\nperformance of ICE on the KE by analyzing the impacts of in-context new\nknowledge on token-wise distributions. We observe that despite a significant\nboost in logits of the new knowledge, the performance of is still hindered by\nstubborn knowledge. Stubborn knowledge refers to as facts that have gained\nexcessive confidence during pretraining, making it hard to edit effectively. To\naddress this issue and further enhance the performance of ICE, we propose a\nnovel approach termed $\\textbf{De}$coding by $\\textbf{C}$ontrasting\n$\\textbf{K}$nowledge (DeCK). DeCK derives the distribution of the next token by\ncontrasting the logits obtained from the newly edited knowledge guided by ICE\nwith those from the unedited parametric knowledge. Our experiments consistently\ndemonstrate that DeCK enhances the confidence of LLMs in edited facts. For\ninstance, it improves the performance of LLaMA3-8B-instruct on MQuAKE by up to\n219%, demonstrating its capability to strengthen ICE in the editing of stubborn\nknowledge. Our work paves the way to develop the both effective and accountable\nKE methods for LLMs. (The source code is available at:\n$\\href{https://github.com/byronBBL/DeCK}{\\text{this https URL.}}$ )", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u77e5\u8b58\u53ef\u80fd\u6703\u8fc5\u901f\u904e\u6642\u3002\u96d6\u7136\u76ee\u524d\u60c5\u5883\u7de8\u8f2f (ICE) \u662f\u77e5\u8b58\u7de8\u8f2f (KE) \u6700\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u53d7\u5230 LLM \u9ed1\u7bb1\u5efa\u6a21\u7684\u9650\u5236\uff0c\u56e0\u6b64\u7f3a\u4e4f\u53ef\u89e3\u91cb\u6027\u3002\u6211\u5011\u7684\u7814\u7a76\u65e8\u5728\u901a\u904e\u5206\u6790\u60c5\u5883\u4e2d\u65b0\u77e5\u8b58\u5c0d\u4ee4\u724c\u7d1a\u5206\u4f48\u7684\u5f71\u97ff\uff0c\u95e1\u660e ICE \u5728 KE \u4e0a\u7684\u512a\u7570\u6027\u80fd\u3002\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u5118\u7ba1\u65b0\u77e5\u8b58\u7684\u5c0d\u6578\u5e7e\u7387\u6709\u4e86\u986f\u8457\u63d0\u5347\uff0c\u4f46\u9811\u56fa\u77e5\u8b58\u4ecd\u7136\u963b\u7919\u4e86\u6027\u80fd\u3002\u9811\u56fa\u77e5\u8b58\u662f\u6307\u5728\u9810\u8a13\u7df4\u904e\u7a0b\u4e2d\u7372\u5f97\u904e\u5ea6\u4fe1\u5fc3\u7684\u4e8b\u5be6\uff0c\u4f7f\u5176\u96e3\u4ee5\u6709\u6548\u7de8\u8f2f\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\u4e26\u9032\u4e00\u6b65\u63d0\u9ad8 ICE \u7684\u6027\u80fd\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u7a31\u70ba\u901a\u904e\u5c0d\u6bd4\u77e5\u8b58\u9032\u884c\u89e3\u78bc (DeCK)\u3002DeCK \u901a\u904e\u5c0d\u6bd4\u5f9e ICE \u5f15\u5c0e\u7684\u65b0\u7de8\u8f2f\u77e5\u8b58\u7372\u5f97\u7684\u5c0d\u6578\u5e7e\u7387\u548c\u672a\u7de8\u8f2f\u53c3\u6578\u77e5\u8b58\u7372\u5f97\u7684\u5c0d\u6578\u5e7e\u7387\uff0c\u63a8\u5c0e\u51fa\u4e0b\u4e00\u500b\u4ee4\u724c\u7684\u5206\u5e03\u3002\u6211\u5011\u7684\u5be6\u9a57\u6301\u7e8c\u8b49\u660e\uff0cDeCK \u589e\u5f37\u4e86 LLM \u5c0d\u7de8\u8f2f\u4e8b\u5be6\u7684\u4fe1\u5fc3\u3002\u4f8b\u5982\uff0c\u5b83\u5c07 LLaMA3-8B-instruct \u5728 MQuAKE \u4e0a\u7684\u6027\u80fd\u63d0\u9ad8\u4e86\u591a\u9054 219%\uff0c\u8b49\u660e\u4e86\u5b83\u5728\u7de8\u8f2f\u9811\u56fa\u77e5\u8b58\u4e2d\u52a0\u5f37 ICE \u7684\u80fd\u529b\u3002\u6211\u5011\u7684\u7814\u7a76\u70ba\u958b\u767c LLM \u7684\u6709\u6548\u4e14\u8ca0\u8cac\u4efb\u7684 KE \u65b9\u6cd5\u92ea\u5e73\u4e86\u9053\u8def\u3002\uff08\u6e90\u4ee3\u78bc\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u7372\u5f97\uff1a\n$\\href{https://github.com/byronBBL/DeCK}{\\text{this https URL.}}$\uff09</paragraph>", "author": "Baolong Bi et.al.", "authors": "Baolong Bi, Shenghua Liu, Lingrui Mei, Yiwei Wang, Pengliang Ji, Xueqi Cheng", "id": "2405.11613v1", "paper_url": "http://arxiv.org/abs/2405.11613v1", "repo": "null"}}