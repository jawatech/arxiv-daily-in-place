{"2502.05076": {"publish_time": "2025-02-07", "title": "Paying Attention to Facts: Quantifying the Knowledge Capacity of Attention Layers", "paper_summary": "In this paper, we investigate the ability of single-layer attention-only\ntransformers (i.e. attention layers) to memorize facts contained in databases\nfrom a linear-algebraic perspective. We associate with each database a\n3-tensor, propose the rank of this tensor as a measure of the size of the\ndatabase, and provide bounds on the rank in terms of properties of the\ndatabase. We also define a 3-tensor corresponding to an attention layer, and\nempirically demonstrate the relationship between its rank and database rank on\na dataset of toy models and random databases. By highlighting the roles played\nby the value-output and query-key weights, and the effects of argmax and\nsoftmax on rank, our results shed light on the `additive motif' of factual\nrecall in transformers, while also suggesting a way of increasing layer\ncapacity without increasing the number of parameters.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f9e\u7dda\u6027\u4ee3\u6578\u7684\u89d2\u5ea6\u63a2\u8a0e\u50c5\u6ce8\u610f\u529b\u8f49\u63db\u5668\uff08\u5373\u6ce8\u610f\u529b\u5c64\uff09\u55ae\u5c64\u8a18\u61b6\u8cc7\u6599\u5eab\u4e2d\u4e8b\u5be6\u7684\u80fd\u529b\u3002\u6211\u5011\u5c07\u6bcf\u500b\u8cc7\u6599\u5eab\u8207\u4e00\u500b 3 \u5f35\u91cf\u95dc\u806f\u8d77\u4f86\uff0c\u63d0\u51fa\u6b64\u5f35\u91cf\u7684\u79e9\u4f5c\u70ba\u8cc7\u6599\u5eab\u5927\u5c0f\u7684\u5ea6\u91cf\uff0c\u4e26\u63d0\u4f9b\u79e9\u7684\u754c\u9650\uff0c\u6839\u64da\u8cc7\u6599\u5eab\u7684\u5c6c\u6027\u3002\u6211\u5011\u9084\u5b9a\u7fa9\u4e86\u4e00\u500b\u5c0d\u61c9\u65bc\u6ce8\u610f\u529b\u5c64\u7684 3 \u5f35\u91cf\uff0c\u4e26\u5728\u73a9\u5177\u6a21\u578b\u548c\u96a8\u6a5f\u8cc7\u6599\u5eab\u7684\u8cc7\u6599\u96c6\u4e0a\u7d93\u9a57\u6027\u5730\u8b49\u660e\u4e86\u5176\u79e9\u548c\u8cc7\u6599\u5eab\u79e9\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\u901a\u904e\u5f37\u8abf\u503c\u8f38\u51fa\u548c\u67e5\u8a62\u9375\u6b0a\u91cd\u6240\u626e\u6f14\u7684\u89d2\u8272\uff0c\u4ee5\u53ca argmax \u548c softmax \u5c0d\u79e9\u7684\u5f71\u97ff\uff0c\u6211\u5011\u7684\u7d50\u679c\u95e1\u660e\u4e86\u8f49\u63db\u5668\u4e2d\u4e8b\u5be6\u6027\u56de\u61b6\u7684\u300c\u52a0\u6cd5\u57fa\u5e8f\u300d\uff0c\u540c\u6642\u4e5f\u63d0\u51fa\u4e86\u4e00\u7a2e\u5728\u4e0d\u589e\u52a0\u53c3\u6578\u6578\u91cf\u7684\u60c5\u6cc1\u4e0b\u589e\u52a0\u5c64\u5bb9\u91cf\u7684\u65b9\u6cd5\u3002", "author": "Liang Ze Wong et.al.", "authors": "Liang Ze Wong", "id": "2502.05076v1", "paper_url": "http://arxiv.org/abs/2502.05076v1", "repo": "null"}}