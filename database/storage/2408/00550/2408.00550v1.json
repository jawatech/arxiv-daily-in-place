{"2408.00550": {"publish_time": "2024-08-01", "title": "Mitigating Multilingual Hallucination in Large Vision-Language Models", "paper_summary": "While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR", "paper_summary_zh": "<paragraph>\u5118\u7ba1\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u537b\u98fd\u53d7\u5e7b\u89ba\u554f\u984c\u6240\u82e6\uff0c\u4e5f\u5c31\u662f\u6a21\u578b\u6839\u64da\u8f38\u5165\u7684\u5f71\u50cf\u67e5\u8a62\u5c0d\u7522\u751f\u770b\u4f3c\u5408\u7406\u4f46\u5be6\u969b\u4e0a\u4e0d\u6b63\u78ba\u7684\u7b54\u6848\u3002\u9019\u7a2e\u5e7b\u89ba\u73fe\u8c61\u5728\u4ee5\u975e\u82f1\u8a9e\u8a9e\u8a00\u67e5\u8a62\u5f71\u50cf\u6642\u6703\u66f4\u52a0\u56b4\u91cd\uff0c\u800c\u73fe\u6709\u91dd\u5c0d LVLMs \u4e2d\u5e7b\u89ba\u554f\u984c\u7684\u7de9\u89e3\u65b9\u6cd5\u50c5\u8003\u616e\u82f1\u8a9e\u60c5\u5883\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u6b21\u5617\u8a66\u7de9\u89e3 LVLMs \u4e2d\u9019\u7a2e\u91cd\u8981\u7684\u591a\u8a9e\u8a00\u5e7b\u89ba\u554f\u984c\u3002\u900f\u904e\u5fb9\u5e95\u7684\u5be6\u9a57\u5206\u6790\uff0c\u6211\u5011\u767c\u73fe LVLMs \u4e2d\u7684\u591a\u8a9e\u8a00\u5e7b\u89ba\u662f\u4e00\u500b\u7cfb\u7d71\u6027\u554f\u984c\uff0c\u53ef\u80fd\u6e90\u81ea\u591a\u8a9e\u8a00\u80fd\u529b\u7684\u4e0d\u8db3\u6216\u591a\u6a21\u614b\u80fd\u529b\u7684\u4e0d\u8db3\u3002\u6709\u9451\u65bc\u6b64\uff0c\u6211\u5011\u91dd\u5c0d LVLMs \u63d0\u51fa\u4e00\u500b\u5169\u968e\u6bb5\u7684\u591a\u8a9e\u8a00\u5e7b\u89ba\u79fb\u9664 (MHR) \u67b6\u69cb\uff0c\u76ee\u6a19\u662f\u63d0\u5347\u9ad8\u8cc7\u6e90\u8a9e\u8a00\u548c\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u5c0d\u5e7b\u89ba\u7684\u62b5\u6297\u529b\u3002\u6211\u5011\u4e26\u672a\u4f9d\u8cf4\u591a\u8a9e\u8a00\u8cc7\u6e90\u7684\u8907\u96dc\u624b\u52d5\u8a3b\u89e3\uff0c\u800c\u662f\u5145\u5206\u5229\u7528 LVLM \u7684\u5167\u5728\u80fd\u529b\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u5275\u65b0\u7684\u8de8\u8a9e\u8a00\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u70ba\u6bcf\u500b\u5f71\u50cf\u67e5\u8a62\u8f38\u5165\u7522\u751f\u591a\u500b\u56de\u61c9\uff0c\u7136\u5f8c\u91dd\u5c0d\u6bcf\u7a2e\u8a9e\u8a00\u627e\u51fa\u5177\u5099\u5e7b\u89ba\u611f\u77e5\u80fd\u529b\u7684\u914d\u5c0d\u3002\u9019\u4e9b\u8cc7\u6599\u914d\u5c0d\u6700\u5f8c\u7528\u65bc\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\uff0c\u4fc3\u4f7f LVLMs \u504f\u597d\u975e\u5e7b\u89ba\u7684\u56de\u61c9\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684 MHR \u5927\u5e45\u6e1b\u5c11\u4e86 LVLMs \u4e2d\u7684\u5e7b\u89ba\u7522\u751f\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u6211\u5011\u64f4\u5145\u7684\u591a\u8a9e\u8a00 POPE \u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0c\u6211\u5011\u7684\u67b6\u69cb\u5728 13 \u7a2e\u4e0d\u540c\u8a9e\u8a00\u4e2d\u5e73\u5747\u63d0\u5347\u4e86 19.0% \u7684\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u6b0a\u91cd\u53ef\u5728 https://github.com/ssmisya/MHR \u53d6\u5f97</paragraph>", "author": "Xiaoye Qu et.al.", "authors": "Xiaoye Qu, Mingyang Song, Wei Wei, Jianfeng Dong, Yu Cheng", "id": "2408.00550v1", "paper_url": "http://arxiv.org/abs/2408.00550v1", "repo": "null"}}