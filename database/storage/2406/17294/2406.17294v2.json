{"2406.17294": {"publish_time": "2024-06-25", "title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "paper_summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, particularly in textual mathematical problem-solving. However,\nexisting open-source image instruction fine-tuning datasets, containing limited\nquestion-answer pairs per image, do not fully exploit visual information to\nenhance the multimodal mathematical reasoning capabilities of Multimodal LLMs\n(MLLMs). To bridge this gap, we address the lack of high-quality, diverse\nmultimodal mathematical datasets by collecting 40K high-quality images with\nquestion-answer pairs from 24 existing datasets and synthesizing 320K new\npairs, creating the MathV360K dataset, which enhances both the breadth and\ndepth of multimodal mathematical questions. We introduce Math-LLaVA, a\nLLaVA-1.5-based model fine-tuned with MathV360K. This novel approach\nsignificantly improves the multimodal mathematical reasoning capabilities of\nLLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V\non MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced\ngeneralizability, showing substantial improvements on the MMMU benchmark. Our\nresearch highlights the importance of dataset diversity and synthesis in\nadvancing MLLMs' mathematical reasoning abilities. The code and data are\navailable at: \\url{https://github.com/HZQ950419/Math-LLaVA}.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u5728\u6587\u5b57\u6578\u5b78\u554f\u984c\u6c42\u89e3\u65b9\u9762\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u958b\u653e\u539f\u59cb\u78bc\u5f71\u50cf\u6307\u4ee4\u5fae\u8abf\u8cc7\u6599\u96c6\u5305\u542b\u6bcf\u500b\u5f71\u50cf\u6709\u9650\u7684\u554f\u984c\u89e3\u7b54\u914d\u5c0d\uff0c\u4e26\u672a\u5145\u5206\u5229\u7528\u8996\u89ba\u8cc7\u8a0a\u4f86\u589e\u5f37\u591a\u6a21\u614b LLM (MLLM) \u7684\u591a\u6a21\u614b\u6578\u5b78\u63a8\u7406\u80fd\u529b\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u900f\u904e\u5f9e 24 \u500b\u73fe\u6709\u8cc7\u6599\u96c6\u4e2d\u6536\u96c6 40K \u500b\u9644\u6709\u554f\u984c\u89e3\u7b54\u914d\u5c0d\u7684\u9ad8\u54c1\u8cea\u5f71\u50cf\uff0c\u4e26\u5408\u6210 320K \u500b\u65b0\u914d\u5c0d\uff0c\u89e3\u6c7a\u4e86\u7f3a\u4e4f\u9ad8\u54c1\u8cea\u3001\u591a\u6a23\u5316\u7684\u591a\u6a21\u614b\u6578\u5b78\u8cc7\u6599\u96c6\u7684\u554f\u984c\uff0c\u9032\u800c\u5efa\u7acb\u4e86 MathV360K \u8cc7\u6599\u96c6\uff0c\u8a72\u8cc7\u6599\u96c6\u64f4\u5c55\u4e86\u591a\u6a21\u614b\u6578\u5b78\u554f\u984c\u7684\u5ee3\u5ea6\u548c\u6df1\u5ea6\u3002\u6211\u5011\u5f15\u5165\u4e86 Math-LLaVA\uff0c\u9019\u662f\u4e00\u500b\u4ee5 MathV360K \u5fae\u8abf\u7684 LLaVA-1.5 \u57fa\u790e\u6a21\u578b\u3002\u9019\u7a2e\u65b0\u65b9\u6cd5\u5927\u5e45\u6539\u5584\u4e86 LLaVA-1.5 \u7684\u591a\u6a21\u614b\u6578\u5b78\u63a8\u7406\u80fd\u529b\uff0c\u5728 MathVista \u7684\u8ff7\u4f60\u6e2c\u8a66\u5206\u5272\u4e2d\u7372\u5f97\u4e86 19 \u5206\u7684\u63d0\u5347\uff0c\u4e26\u9054\u5230\u8207 GPT-4V \u76f8\u7576\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0cMath-LLaVA \u5c55\u73fe\u51fa\u589e\u5f37\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728 MMMU \u8a55\u91cf\u57fa\u6e96\u4e0a\u7372\u5f97\u4e86\u986f\u8457\u7684\u6539\u5584\u3002\u6211\u5011\u7684\u7814\u7a76\u7a81\u986f\u4e86\u8cc7\u6599\u96c6\u591a\u6a23\u6027\u548c\u5408\u6210\u5728\u63d0\u5347 MLLM \u6578\u5b78\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u65bc\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1a\\url{https://github.com/HZQ950419/Math-LLaVA}\u3002", "author": "Wenhao Shi et.al.", "authors": "Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee", "id": "2406.17294v2", "paper_url": "http://arxiv.org/abs/2406.17294v2", "repo": "null"}}