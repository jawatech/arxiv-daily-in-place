# arxiv-daily
 Automated deployment @ 2024-09-16 09:05:44 Asia/Taipei
> Welcome to contribute! Add your topics and keywords in [`topic.yml`](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/topic.yml).
> You can also view historical data through the [storage](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/storage).

## AI

### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-12**|**Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**|Fuchen Zheng et.al.|[2409.07793v1](http://arxiv.org/abs/2409.07793v1)|[link](https://github.com/lzeeorno/lagrange-duality-and-cmaformer)|
|**2024-09-12**|**ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**|Fuchen Zheng et.al.|[2409.07779v1](http://arxiv.org/abs/2409.07779v1)|[link](https://github.com/lzeeorno/assnet)|
|**2024-09-11**|**SoK: Security and Privacy Risks of Medical AI**|Yuanhaur Chang et.al.|[2409.07415v1](http://arxiv.org/abs/2409.07415v1)|null|
|**2024-09-11**|**Federated Impression for Learning with Distributed Heterogeneous Data**|Sana Ayromlou et.al.|[2409.07351v1](http://arxiv.org/abs/2409.07351v1)|null|
|**2024-09-11**|**MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**|Praveen K Kanithi et.al.|[2409.07314v1](http://arxiv.org/abs/2409.07314v1)|null|
|**2024-09-11**|**Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**|Sheng Chen et.al.|[2409.07186v1](http://arxiv.org/abs/2409.07186v1)|null|
|**2024-09-11**|**CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**|Feiyang Jia et.al.|[2409.07092v1](http://arxiv.org/abs/2409.07092v1)|null|
|**2024-09-11**|**Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**|Daeun Kyung et.al.|[2409.07012v1](http://arxiv.org/abs/2409.07012v1)|null|
|**2024-09-11**|**Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**|Jianmei Jiang et.al.|[2409.06928v1](http://arxiv.org/abs/2409.06928v1)|[link](https://github.com/jjm1589/dstct)|
|**2024-09-10**|**Bifurcation Identification for Ultrasound-driven Robotic Cannulation**|Cecilia G. Morales et.al.|[2409.06817v1](http://arxiv.org/abs/2409.06817v1)|null|
|**2024-09-10**|**Personalized Federated Learning Techniques: Empirical Analysis**|Azal Ahmad Khan et.al.|[2409.06805v1](http://arxiv.org/abs/2409.06805v1)|null|
|**2024-09-10**|**Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**|Cristian Trout et.al.|[2409.06672v1](http://arxiv.org/abs/2409.06672v1)|null|
|**2024-09-10**|**EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**|Danli Shi et.al.|[2409.06644v2](http://arxiv.org/abs/2409.06644v2)|null|
|**2024-09-10**|**Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**|Zoe Hancox et.al.|[2409.06585v1](http://arxiv.org/abs/2409.06585v1)|[link](https://github.com/zoehancox/sparse_tgcnn)|
|**2024-09-10**|**MAGDA: Multi-agent guideline-driven diagnostic assistance**|David Bani-Harouni et.al.|[2409.06351v1](http://arxiv.org/abs/2409.06351v1)|null|
|**2024-09-10**|**Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**|Xin Zhang et.al.|[2409.06209v1](http://arxiv.org/abs/2409.06209v1)|[link](https://github.com/xinz0419/unisurv)|
|**2024-09-10**|**Can Large Language Models Unlock Novel Scientific Research Ideas?**|Sandeep Kumar et.al.|[2409.06185v1](http://arxiv.org/abs/2409.06185v1)|null|
|**2024-09-10**|**Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**|Georgios Chochlakis et.al.|[2409.06173v1](http://arxiv.org/abs/2409.06173v1)|[link](https://github.com/gchochla/cot-priors)|
|**2024-09-10**|**Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**|Dong Han et.al.|[2409.06147v1](http://arxiv.org/abs/2409.06147v1)|null|
|**2024-09-09**|**ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**|Zhaoyue Sun et.al.|[2409.05592v1](http://arxiv.org/abs/2409.05592v1)|null|
|**2024-09-09**|**Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**|Camilo Thorne et.al.|[2409.05486v1](http://arxiv.org/abs/2409.05486v1)|null|
|**2024-09-09**|**KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**|Yingshu Li et.al.|[2409.05370v1](http://arxiv.org/abs/2409.05370v1)|null|
|**2024-09-09**|**Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**|Javad Hassannataj Joloudari et.al.|[2409.07493v1](http://arxiv.org/abs/2409.07493v1)|null|
|**2024-09-09**|**Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**|Nirmalya Thakur et.al.|[2409.05292v2](http://arxiv.org/abs/2409.05292v2)|null|
|**2024-09-09**|**RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**|Quoc-Bao Nguyen-Le et.al.|[2409.05280v1](http://arxiv.org/abs/2409.05280v1)|[link](https://github.com/kyle-paul/RotCAtt-TransUNet-plusplus)|
|**2024-09-07**|**Activation Function Optimization Scheme for Image Classification**|Abdur Rahman et.al.|[2409.04915v1](http://arxiv.org/abs/2409.04915v1)|null|
|**2024-09-07**|**LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**|Yongxin Deng et.al.|[2409.04744v1](http://arxiv.org/abs/2409.04744v1)|null|
|**2024-09-07**|**NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**|Debaditya Shome et.al.|[2409.04723v1](http://arxiv.org/abs/2409.04723v1)|null|
|**2024-09-07**|**A Comprehensive Survey on Evidential Deep Learning and Its Applications**|Junyu Gao et.al.|[2409.04720v1](http://arxiv.org/abs/2409.04720v1)|[link](https://github.com/mengyuanchen21/awesome-evidential-deep-learning)|
|**2024-09-07**|**A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**|Cheng Wan et.al.|[2409.04704v1](http://arxiv.org/abs/2409.04704v1)|null|
|**2024-09-06**|**The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**|Gregory Szumel et.al.|[2409.04368v1](http://arxiv.org/abs/2409.04368v1)|null|
|**2024-09-06**|**CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**|William Knottenbelt et.al.|[2409.04290v1](http://arxiv.org/abs/2409.04290v1)|[link](https://github.com/knottwill/CoxKAN)|
|**2024-09-06**|**Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**|Daniel J. Tan et.al.|[2409.04224v1](http://arxiv.org/abs/2409.04224v1)|null|
|**2024-09-06**|**Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**|Yizhen Zheng et.al.|[2409.04481v1](http://arxiv.org/abs/2409.04481v1)|null|
|**2024-09-06**|**FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**|Kai Shu et.al.|[2409.03947v1](http://arxiv.org/abs/2409.03947v1)|null|
|**2024-09-05**|**A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**|Esther Lagemann et.al.|[2409.03933v1](http://arxiv.org/abs/2409.03933v1)|null|
|**2024-09-05**|**Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**|Yucong Zhang et.al.|[2409.03597v1](http://arxiv.org/abs/2409.03597v1)|null|
|**2024-09-05**|**Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**|Prerak Mody et.al.|[2409.03470v1](http://arxiv.org/abs/2409.03470v1)|[link](https://github.com/prerakmody/bayesuncertainty-error-correspondence)|
|**2024-09-05**|**Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**|Francisco de Arriba-PÃ©rez et.al.|[2409.03375v1](http://arxiv.org/abs/2409.03375v1)|null|
|**2024-09-05**|**Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**|Juan A. Berrios Moya et.al.|[2409.03147v1](http://arxiv.org/abs/2409.03147v1)|null|
|**2024-09-04**|**MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**|Shehan Perera et.al.|[2409.03062v1](http://arxiv.org/abs/2409.03062v1)|[link](https://github.com/osupcvlab/mobileunetr)|
|**2024-09-04**|**Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**|Junyoung Park et.al.|[2409.02883v1](http://arxiv.org/abs/2409.02883v1)|null|
|**2024-09-04**|**Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**|Ramon Tavares et.al.|[2409.02681v1](http://arxiv.org/abs/2409.02681v1)|null|
|**2024-09-04**|**SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**|Wenwu Guo et.al.|[2409.02598v1](http://arxiv.org/abs/2409.02598v1)|[link](https://github.com/wenwucode/surgtrack)|
|**2024-09-04**|**Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**|Chih-Yuan Li et.al.|[2409.02530v1](http://arxiv.org/abs/2409.02530v1)|null|
|**2024-09-03**|**Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**|Deepak Raina et.al.|[2409.02337v1](http://arxiv.org/abs/2409.02337v1)|null|
|**2024-09-03**|**Action-Based ADHD Diagnosis in Video**|Yichun Li et.al.|[2409.02261v1](http://arxiv.org/abs/2409.02261v1)|null|
|**2024-09-03**|**A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**|Anna L. Trella et.al.|[2409.02069v1](http://arxiv.org/abs/2409.02069v1)|[link](https://github.com/StatisticalReinforcementLearningLab/oralytics-post-deployment-analysis)|
|**2024-09-03**|**TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**|Bobby Azad et.al.|[2409.02018v1](http://arxiv.org/abs/2409.02018v1)|null|
|**2024-09-03**|**A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**|Abdelmalek Mouazer et.al.|[2409.01903v1](http://arxiv.org/abs/2409.01903v1)|null|
|**2024-09-03**|**Training on the Benchmark Is Not All You Need**|Shiwen Ni et.al.|[2409.01790v1](http://arxiv.org/abs/2409.01790v1)|[link](https://github.com/nishiwen1214/Benchmark-leakage-detection)|
|**2024-09-03**|**Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**|Wenyang Hu et.al.|[2409.01676v1](http://arxiv.org/abs/2409.01676v1)|null|
|**2024-09-03**|**A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**|Zekang Yang et.al.|[2409.02145v1](http://arxiv.org/abs/2409.02145v1)|[link](https://github.com/yang-ze-kang/MOC)|
|**2024-09-03**|**A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**|Ruben D. Fonnegra et.al.|[2409.01596v1](http://arxiv.org/abs/2409.01596v1)|null|
|**2024-09-02**|**Kvasir-VQA: A Text-Image Pair GI Tract Dataset**|Sushant Gautam et.al.|[2409.01437v1](http://arxiv.org/abs/2409.01437v1)|[link](https://github.com/simula/Kvasir-VQA)|
|**2024-09-02**|**EEG-Language Modeling for Pathology Detection**|Sam Gijsen et.al.|[2409.07480v1](http://arxiv.org/abs/2409.07480v1)|null|
|**2024-09-02**|**SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**|Mevan Ekanayake et.al.|[2409.01013v1](http://arxiv.org/abs/2409.01013v1)|null|
|**2024-09-01**|**Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**|Sajib Acharjee Dip et.al.|[2409.00873v1](http://arxiv.org/abs/2409.00873v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**|Mahsa Khosravi et.al.|[2409.00735v1](http://arxiv.org/abs/2409.00735v1)|[link](https://github.com/scslabisu/aggym)|
|**2024-09-01**|**LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**|Zhaojie Fang et.al.|[2409.00726v1](http://arxiv.org/abs/2409.00726v1)|[link](https://github.com/Tinysqua/LPUWF-LDM)|
|**2024-09-01**|**BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**|Shams Nafisa Ali et.al.|[2409.00724v1](http://arxiv.org/abs/2409.00724v1)|[link](https://github.com/sani002/HS-Dataset)|
|**2024-09-01**|**Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**|Pragya Gupta et.al.|[2409.00718v1](http://arxiv.org/abs/2409.00718v1)|null|
|**2024-09-01**|**Curriculum Prompting Foundation Models for Medical Image Segmentation**|Xiuqi Zheng et.al.|[2409.00695v1](http://arxiv.org/abs/2409.00695v1)|[link](https://github.com/annazzz-zxq/curriculum-prompting)|
|**2024-08-31**|**Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**|Jacqueline Lammert et.al.|[2409.00544v1](http://arxiv.org/abs/2409.00544v1)|[link](https://github.com/LammertJ/RGT-Digital-Twin)|
|**2024-08-31**|**Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**|Georgios Ioannides et.al.|[2409.00391v1](http://arxiv.org/abs/2409.00391v1)|null|
|**2024-08-31**|**Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**|Mikhail Borisenkov et.al.|[2409.00310v1](http://arxiv.org/abs/2409.00310v1)|null|
|**2024-08-30**|**Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**|Antonio Rago et.al.|[2408.17401v1](http://arxiv.org/abs/2408.17401v1)|null|
|**2024-08-30**|**Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**|Yuhan Zheng et.al.|[2409.00163v1](http://arxiv.org/abs/2409.00163v1)|null|
|**2024-08-30**|**NDP: Next Distribution Prediction as a More Broad Target**|Junhao Ruan et.al.|[2408.17377v1](http://arxiv.org/abs/2408.17377v1)|null|
|**2024-08-30**|**Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**|Jutika Borah et.al.|[2408.17011v2](http://arxiv.org/abs/2408.17011v2)|null|
|**2024-08-29**|**A Survey for Large Language Models in Biomedicine**|Chong Wang et.al.|[2409.00133v1](http://arxiv.org/abs/2409.00133v1)|null|
|**2024-08-29**|**Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**|Yifei Chen et.al.|[2408.16343v1](http://arxiv.org/abs/2408.16343v1)|[link](https://github.com/justlfc03/mstnet)|
|**2024-08-29**|**Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**|Xia Jiang et.al.|[2408.16256v1](http://arxiv.org/abs/2408.16256v1)|null|
|**2024-08-29**|**M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**|Jonggwon Park et.al.|[2408.16213v1](http://arxiv.org/abs/2408.16213v1)|null|
|**2024-08-28**|**A Survey on Evaluation of Multimodal Large Language Models**|Jiaxing Huang et.al.|[2408.15769v1](http://arxiv.org/abs/2408.15769v1)|null|
|**2024-08-28**|**Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**|Yijun Zhou et.al.|[2408.15498v1](http://arxiv.org/abs/2408.15498v1)|null|
|**2024-08-27**|**What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**|Jana Schaich Borg et.al.|[2408.15354v1](http://arxiv.org/abs/2408.15354v1)|null|
|**2024-08-27**|**Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**|Weiyi Zhang et.al.|[2408.15217v1](http://arxiv.org/abs/2408.15217v1)|null|
|**2024-08-27**|**Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**|Daniil Filienko et.al.|[2409.00112v1](http://arxiv.org/abs/2409.00112v1)|null|
|**2024-08-27**|**Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**|Francesco Sovrano et.al.|[2408.15121v1](http://arxiv.org/abs/2408.15121v1)|null|
|**2024-08-27**|**MiWaves Reinforcement Learning Algorithm**|Susobhan Ghosh et.al.|[2408.15076v1](http://arxiv.org/abs/2408.15076v1)|[link](https://github.com/statisticalreinforcementlearninglab/miwaves_rl_service)|
|**2024-08-27**|**Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**|Yuqi Zhang et.al.|[2408.15032v1](http://arxiv.org/abs/2408.15032v1)|[link](https://github.com/yuqizhang-buaa/mamba2mil)|
|**2024-08-27**|**Sequence-aware Pre-training for Echocardiography Probe Guidance**|Haojun Jiang et.al.|[2408.15026v1](http://arxiv.org/abs/2408.15026v1)|null|
|**2024-08-27**|**Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**|Christos Theodoropoulos et.al.|[2408.15294v2](http://arxiv.org/abs/2408.15294v2)|null|
|**2024-08-27**|**Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**|Qiaoxin Li et.al.|[2408.14754v1](http://arxiv.org/abs/2408.14754v1)|null|
|**2024-08-27**|**Large Language Models for Disease Diagnosis: A Scoping Review**|Shuang Zhou et.al.|[2409.00097v1](http://arxiv.org/abs/2409.00097v1)|null|
|**2024-08-26**|**Elementary School Students' and Teachers' Perceptions Towards Creative Mathematical Writing with Generative AI**|Yukyeong Song et.al.|[2409.06723v1](http://arxiv.org/abs/2409.06723v1)|null|
|**2024-08-26**|**Improving Clinical Note Generation from Complex Doctor-Patient Conversation**|Yizhan Li et.al.|[2408.14568v1](http://arxiv.org/abs/2408.14568v1)|null|
|**2024-08-26**|**Temporal Ensemble Logic**|Guo-Qiang Zhang et.al.|[2408.14443v2](http://arxiv.org/abs/2408.14443v2)|null|
|**2024-08-26**|**MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**|Kuluhan Binici et.al.|[2408.14418v2](http://arxiv.org/abs/2408.14418v2)|null|
|**2024-08-26**|**Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**|Xiaoman Zhang et.al.|[2408.14397v1](http://arxiv.org/abs/2408.14397v1)|[link](https://github.com/rajpurkarlab/rexkg)|
|**2024-08-26**|**Foundation Models for Music: A Survey**|Yinghao Ma et.al.|[2408.14340v3](http://arxiv.org/abs/2408.14340v3)|[link](https://github.com/nicolaus625/fm4music)|
|**2024-08-26**|**Uncertainties of Latent Representations in Computer Vision**|Michael Kirchhof et.al.|[2408.14281v1](http://arxiv.org/abs/2408.14281v1)|null|
|**2024-08-26**|**Automatic Medical Report Generation: Methods and Applications**|Li Guo et.al.|[2408.13988v1](http://arxiv.org/abs/2408.13988v1)|null|
|**2024-08-25**|**Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**|Seyed Amir Ahmad Safavi-Naini et.al.|[2409.00084v2](http://arxiv.org/abs/2409.00084v2)|[link](https://github.com/sdamirsa/llm-vlm-in-gastroenterology)|
|**2024-08-25**|**PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**|Zifan Chen et.al.|[2408.13836v1](http://arxiv.org/abs/2408.13836v1)|null|
|**2024-08-24**|**Submodular Maximization Approaches for Equitable Client Selection in Federated Learning**|AndrÃ©s Catalino Castillo JimÃ©nez et.al.|[2408.13683v2](http://arxiv.org/abs/2408.13683v2)|null|
|**2024-08-24**|**Towards Case-based Interpretability for Medical Federated Learning**|Laura Latorre et.al.|[2408.13626v1](http://arxiv.org/abs/2408.13626v1)|null|

#### Abstracts
##### **Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**
2409.07793v1 by Fuchen Zheng, Quanjun Li, Weixuan Li, Xuhang Chen, Yihang Dong, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a critical application of semantic segmentation
in healthcare, has seen significant advancements through specialized computer
vision techniques. While deep learning-based medical image segmentation is
essential for assisting in medical diagnosis, the lack of diverse training data
causes the long-tail problem. Moreover, most previous hybrid CNN-ViT
architectures have limited ability to combine various attentions in different
layers of the Convolutional Neural Network. To address these issues, we propose
a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware
Contrastive Loss, as the overall training objective for semi-supervised
learning to mitigate the long-tail problem. Additionally, we introduce
CMAformer, a novel network that synergizes the strengths of ResUNet and
Transformer. The cross-attention block in CMAformer effectively integrates
spatial attention and channel attention for multi-scale feature fusion.
Overall, our results indicate that CMAformer, combined with the feature fusion
framework and the new consistency loss, demonstrates strong complementarity in
semi-supervised learning ensembles. We achieve state-of-the-art results on
multiple public medical image datasets. Example code are available at:
\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.

æè¦ï¼é«å­¸å½±ååå²æ¯èªæåå²å¨é«çä¿å¥é åä¸­çä¸é éè¦æç¨ï¼å·²ééå°æ¥­çé»è¦è¦è¦ºæè¡ç²å¾é¡¯èé²å±ãéç¶åºæ¼æ·±åº¦å­¸ç¿çé«å­¸å½±ååå²å°æ¼åå©é«çè¨ºæ·è³ééè¦ï¼ä½ç¼ºä¹å¤æ¨£åçè¨ç·´è³ææå°è´é·å°¾åé¡ãæ­¤å¤ï¼å¤§å¤æ¸ååçæ··åå¼ CNN-ViT æ¶æ§å¨çµåå·ç©ç¥ç¶ç¶²è·¯ä¸åå±¤ä¸­çåç¨®æ³¨æåæ¹é¢è½åæéãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäºä¸ç¨®ææ ¼ææ¥å°å¶ä¸è´æ§ (LDC) æå¤±ï¼ä¸¦èéçæç¥å°æ¯æå¤±æ´åï¼ä½çºåç£ç£å¼å­¸ç¿çæ´é«è¨ç·´ç®æ¨ï¼ä»¥æ¸è¼é·å°¾åé¡ãæ­¤å¤ï¼æåä»ç´¹äº CMAformerï¼éæ¯ä¸åæ°ç©çç¶²è·¯ï¼å®ååäº ResUNet å Transformer çåªé»ãCMAformer ä¸­çäº¤åæ³¨æååå¡ææå°æ´åäºç©ºéæ³¨æååééæ³¨æåï¼ä»¥é²è¡å¤å°ºåº¦ç¹å¾µèåãç¸½çä¾èªªï¼æåççµæè¡¨æï¼CMAformer çµåç¹å¾µèåæ¶æ§åæ°çç¨ å¯æå¤±ï¼å¨åç£ç£å¼å­¸ç¿éåä¸­å±ç¾åºå¼·å¤§çäºè£æ§ãæåå¨å¤åå¬éé«å­¸å½±åè³æéä¸åå¾äºæåé²çææãç¯ä¾ç¨å¼ç¢¼å¯å¨ä»¥ä¸ç¶²ååå¾ï¼\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}ã

##### **ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**
2409.07779v1 by Fuchen Zheng, Xinyi Chen, Xuhang Chen, Haolun Li, Xiaojiao Guo, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a crucial task in computer vision, facilitates
the automated delineation of anatomical structures and pathologies, supporting
clinicians in diagnosis, treatment planning, and disease monitoring. Notably,
transformers employing shifted window-based self-attention have demonstrated
exceptional performance. However, their reliance on local window attention
limits the fusion of local and global contextual information, crucial for
segmenting microtumors and miniature organs. To address this limitation, we
propose the Adaptive Semantic Segmentation Network (ASSNet), a transformer
architecture that effectively integrates local and global features for precise
medical image segmentation. ASSNet comprises a transformer-based U-shaped
encoder-decoder network. The encoder utilizes shifted window self-attention
across five resolutions to extract multi-scale features, which are then
propagated to the decoder through skip connections. We introduce an augmented
multi-layer perceptron within the encoder to explicitly model long-range
dependencies during feature extraction. Recognizing the constraints of
conventional symmetrical encoder-decoder designs, we propose an Adaptive
Feature Fusion (AFF) decoder to complement our encoder. This decoder
incorporates three key components: the Long Range Dependencies (LRD) block, the
Multi-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)
block. These components synergistically facilitate the effective fusion of
multi-scale features extracted by the decoder while capturing long-range
dependencies and refining object boundaries. Comprehensive experiments on
diverse medical image segmentation tasks, including multi-organ, liver tumor,
and bladder tumor segmentation, demonstrate that ASSNet achieves
state-of-the-art results. Code and models are available at:
\url{https://github.com/lzeeorno/ASSNet}.

æè¦ï¼<paragraph>é«å­¸å½±ååå²æ¯é»è¦è¦è¦ºä¸­ä¸é éè¦çä»»åï¼æå©æ¼èªåæç¹ªè§£åçµæ§åççï¼åå©è¨åºé«å¸«é²è¡è¨ºæ·ãæ²»çè¨ç«åç¾çç£æ§ãå¼å¾æ³¨æçæ¯ï¼æ¡ç¨ä½ç§»è¦çªèªæ³¨æåæ©å¶çTransformerå±ç¾åºéå¡çæè½ãç¶èï¼å®åä¾è³´æ¼ååè¦çªæ³¨æåï¼ééå¶äºåååå¨åèçµ¡è³è¨çèåï¼èéå°æ¼åå²å¾®å°è«ç¤åå¾®åå¨å®è³ééè¦ãçºäºè§£æ±ºéåéå¶ï¼æåæåºäºèªé©æèªç¾©åå²ç¶²è·¯ (ASSNet)ï¼éæ¯ä¸åTransformeræ¶æ§ï¼å¯ä»¥æææ´ååååå¨åç¹å¾µï¼ä»¥é²è¡ç²¾ç¢ºçé«å­¸å½±ååå²ãASSNet åå«ä¸ååºæ¼Transformerç U åç·¨ç¢¼å¨-è§£ç¢¼å¨ç¶²è·¯ãç·¨ç¢¼å¨å©ç¨äºåè§£æåº¦çä½ç§»è¦çªèªæ³¨æåä¾èåå¤å°ºåº¦ç¹å¾µï¼ç¶å¾ééè·³èºé£ç·å°éäºç¹å¾µå³æ­å°è§£ç¢¼å¨ãæåå¨ç·¨ç¢¼å¨ä¸­å¼å¥äºæ´å¢çå¤å±¤æç¥å¨ï¼ä»¥ä¾¿å¨ç¹å¾µèåæéæç¢ºå°å»ºæ¨¡é·ç¨ä¾è³´æ§ãéæ¼å³çµ±å°ç¨±ç·¨ç¢¼å¨-è§£ç¢¼å¨è¨­è¨çéå¶ï¼æåæåºäºä¸åèªé©æç¹å¾µèå (AFF) è§£ç¢¼å¨ä¾è£åæåçç·¨ç¢¼å¨ãæ­¤è§£ç¢¼å¨åå«ä¸åééµçµæé¨åï¼é·ç¨ä¾è³´æ§ (LRD) åå¡ãå¤å°ºåº¦ç¹å¾µèå (MFF) åå¡åèªé©æèªç¾©ä¸­å¿ (ASC) åå¡ãéäºçµæé¨åç¸äºéåï¼ä¿æè§£ç¢¼å¨èåçå¤å°ºåº¦ç¹å¾µææèåï¼åæææé·ç¨ä¾è³´æ§ä¸¦å¾®èª¿ç©ä»¶éçãå¨å¤å¨å®ãèèè«ç¤åèè±è«ç¤åå²ç­åç¨®é«å­¸å½±ååå²ä»»åä¸çå¨é¢å¯¦é©è­æï¼ASSNet éå°äºæåé²çææãç¨å¼ç¢¼åæ¨¡åå¯æ¼ä»¥ä¸ç¶²ååå¾ï¼\url{https://github.com/lzeeorno/ASSNet}ã</paragraph>

##### **SoK: Security and Privacy Risks of Medical AI**
2409.07415v1 by Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang

The integration of technology and healthcare has ushered in a new era where
software systems, powered by artificial intelligence and machine learning, have
become essential components of medical products and services. While these
advancements hold great promise for enhancing patient care and healthcare
delivery efficiency, they also expose sensitive medical data and system
integrity to potential cyberattacks. This paper explores the security and
privacy threats posed by AI/ML applications in healthcare. Through a thorough
examination of existing research across a range of medical domains, we have
identified significant gaps in understanding the adversarial attacks targeting
medical AI systems. By outlining specific adversarial threat models for medical
settings and identifying vulnerable application domains, we lay the groundwork
for future research that investigates the security and resilience of AI-driven
medical systems. Through our analysis of different threat models and
feasibility studies on adversarial attacks in different medical domains, we
provide compelling insights into the pressing need for cybersecurity research
in the rapidly evolving field of AI healthcare technology.

æè¦ï¼ç§æèé«ççæ´åéåäºä¸åæ°ç´åï¼ç±äººå·¥æºæ§åæ©å¨å­¸ç¿é©åçè»é«ç³»çµ±å·²æçºé«çç¢ååæåçå¿è¦çµæé¨åãéç¶éäºé²æ­¥å°æ¹åæ£èç§è­·åé«çä¿å¥æä¾æçæå¾å¤§çå¹«å©ï¼ä½å®åä¹è®ææçé«çè³æåç³»çµ±å®æ´æ§é¢è¨æ½å¨çç¶²è·¯æ»æé¢¨éªãæ¬ææ¢è¨äºäººå·¥æºæ§/æ©å¨å­¸ç¿æç¨å¨é«çä¿å¥ä¸­å¸¶ä¾çå®å¨æ§åé±ç§å¨èãééå¾¹åºæª¢è¦åé é«çé åç¾æçç ç©¶ï¼æåç¼ç¾äºå¨äºè§£éå°é«çäººå·¥æºæ§ç³»çµ±çå°ææ§æ»ææ¹é¢æé¡¯èçå·®è·ãééæ¦è¿°é«çç°å¢çç¹å®å°ææ§å¨èæ¨¡åä¸¦æ¾åºå®¹æåæ»æçæç¨é åï¼æåçºæªä¾ç ç©¶å¥ å®åºç¤ï¼æ¢è¨äººå·¥æºæ§é©åé«çç³»çµ±çå®å¨æ§èå¾©ååãééåæä¸åçå¨èæ¨¡ååéå°ä¸åé«çé åçå°ææ§æ»æå¯è¡æ§ç ç©¶ï¼æåå°äººå·¥æºæ§é«çä¿å¥æè¡å¿«éç¼å±é åä¸­ç¶²è·¯å®å¨ç ç©¶çè¿«åéæ±æä¾äºä»¤äººä¿¡æçè¦è§£ã

##### **Federated Impression for Learning with Distributed Heterogeneous Data**
2409.07351v1 by Sana Ayromlou, Atrin Arya, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li

Standard deep learning-based classification approaches may not always be
practical in real-world clinical applications, as they require a centralized
collection of all samples. Federated learning (FL) provides a paradigm that can
learn from distributed datasets across clients without requiring them to share
data, which can help mitigate privacy and data ownership issues. In FL,
sub-optimal convergence caused by data heterogeneity is common among data from
different health centers due to the variety in data collection protocols and
patient demographics across centers. Through experimentation in this study, we
show that data heterogeneity leads to the phenomenon of catastrophic forgetting
during local training. We propose FedImpres which alleviates catastrophic
forgetting by restoring synthetic data that represents the global information
as federated impression. To achieve this, we distill the global model resulting
from each communication round. Subsequently, we use the synthetic data
alongside the local data to enhance the generalization of local training.
Extensive experiments show that the proposed method achieves state-of-the-art
performance on both the BloodMNIST and Retina datasets, which contain label
imbalance and domain shift, with an improvement in classification accuracy of
up to 20%.

æè¦ï¼æ¨æºçæ·±åº¦å­¸ç¿åé¡æ¹æ³å¨å¯¦éçè¨åºæç¨ä¸­å¯è½ä¸¦ä¸ç¸½æ¯å¯¦ç¨çï¼å çºå®åéè¦éä¸­æ¶éæææ¨£æ¬ãè¯é¦å­¸ç¿ (FL) æä¾äºä¸åç¯ä¾ï¼å¯ä»¥å¨ä¸è®å®¢æ¶ç«¯åäº«æ¸æçææ³ä¸å¾åå¸å¼æ¸æéå­¸ç¿ï¼éæå©æ¼æ¸è¼é±ç§åæ¸ææææ¬åé¡ãå¨ FL ä¸­ï¼ç±æ¼ä¸åé«çä¸­å¿çæ¸ææ¶éåå®åæ£èäººå£çµ±è¨è³æçå·®ç°ï¼ä¾èªä¸åé«çä¸­å¿çæ¸æä¹éå¸¸è¦çæ¸æç°è³ªæ§æå°è´æ¬¡æä½³æ¶æãééæ¬ç ç©¶ä¸­çå¯¦é©ï¼æåè¡¨ææ¸æç°è³ªæ§æå°è´å±é¨è¨ç·´æéç¼çç½é£æ§éºå¿ç¾è±¡ãæåæåº FedImpresï¼å®éééåè¡¨ç¤ºå¨çè³è¨çåæè³æä½çºè¯é¦å°è±¡ä¾æ¸è¼ç½é£æ§éºå¿ãçºæ­¤ï¼æåæçåºæ¯ä¸è¼ªéè¨æç¢ççå¨çæ¨¡åãé¨å¾ï¼æåä½¿ç¨åæè³æåæ¬å°è³æä¾å¢å¼·æ¬å°è¨ç·´çæ¦æ¬æ§ãå»£æ³çå¯¦é©è¡¨æï¼ææåºçæ¹æ³å¨ BloodMNIST å Retina æ¸æéä¸é½éå°äºæåé²çæè½ï¼éäºæ¸æéåå«æ¨ç±¤ä¸å¹³è¡¡åé åè½ç§»ï¼åé¡æºç¢ºåº¦æé«äº 20%ã

##### **MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**
2409.07314v1 by Praveen K Kanithi, ClÃ©ment Christophe, Marco AF Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan

The rapid development of Large Language Models (LLMs) for healthcare
applications has spurred calls for holistic evaluation beyond frequently-cited
benchmarks like USMLE, to better reflect real-world performance. While
real-world assessments are valuable indicators of utility, they often lag
behind the pace of LLM evolution, likely rendering findings obsolete upon
deployment. This temporal disconnect necessitates a comprehensive upfront
evaluation that can guide model selection for specific clinical applications.
We introduce MEDIC, a framework assessing LLMs across five critical dimensions
of clinical competence: medical reasoning, ethics and bias, data and language
understanding, in-context learning, and clinical safety. MEDIC features a novel
cross-examination framework quantifying LLM performance across areas like
coverage and hallucination detection, without requiring reference outputs. We
apply MEDIC to evaluate LLMs on medical question-answering, safety,
summarization, note generation, and other tasks. Our results show performance
disparities across model sizes, baseline vs medically finetuned models, and
have implications on model selection for applications requiring specific model
strengths, such as low hallucination or lower cost of inference. MEDIC's
multifaceted evaluation reveals these performance trade-offs, bridging the gap
between theoretical capabilities and practical implementation in healthcare
settings, ensuring that the most promising models are identified and adapted
for diverse healthcare applications.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨é«çä¿å¥æç¨æ¹é¢çå¿«éç¼å±ï¼ä¿ä½¿äººåå¼ç±²é²è¡æ´é«è©ä¼°ï¼è¶è¶ç¶å¸¸å¼ç¨çåºæºï¼ä¾å¦ USMLEï¼ï¼ä»¥æ´å¥½å°åæ å¯¦éæè½ãåç®¡å¯¦éè©ä¼°æ¯å¯¦ç¨æ§çå¯¶è²´ææ¨ï¼ä½å®åéå¸¸è½å¾æ¼ LLM æ¼åçéåº¦ï¼å¨é¨ç½²å¾å¯è½æä½¿ç ç©¶çµæéæãéç¨®æéä¸çè«ç¯éè¦é²è¡å¨é¢çåæè©ä¼°ï¼ä»¥æå°ç¹å®è¨åºæç¨ç¨å¼çæ¨¡åé¸æãæåå¼é² MEDICï¼ä¸åè©ä¼° LLM è·¨è¶è¨åºè½åçäºåééµé¢åçæ¶æ§ï¼é«çæ¨çãå«çååå·®ãè³æåèªè¨çè§£ãæå¢å­¸ç¿åè¨åºå®å¨æ§ãMEDIC æ¡ç¨ä¸ç¨®æ°ç©çäº¤äºå¼æª¢æ¥æ¶æ§ï¼éå LLM å¨æ¶µèç¯ååå¹»è¦ºåµæ¸¬ç­é åçæè½ï¼èä¸éè¦åèè¼¸åºãæåä½¿ç¨ MEDIC ä¾è©ä¼° LLM å¨é«çåé¡è§£ç­ãå®å¨æ§ãæè¦ãç­è¨ç¢çåå¶ä»ä»»åä¸çè¡¨ç¾ãæåççµæé¡¯ç¤ºï¼ä¸åæ¨¡åå¤§å°ãåºæºèç¶éé«çå¾®èª¿çæ¨¡åä¹éçæè½å·®ç°ï¼ä¸¦å°éè¦ç¹å®æ¨¡ååªå¢çæç¨ç¨å¼ï¼ä¾å¦ä½å¹»è¦ºæè¼ä½çæ¨è«ææ¬ï¼çæ¨¡åé¸æç¢çå½±é¿ãMEDIC çå¤é¢åè©ä¼°æ­ç¤ºäºéäºæè½æ¬è¡¡ï¼ç¸®å°äºçè«è½åèé«çä¿å¥ç°å¢ä¸­çå¯¦éå¯¦ä½ä¹éçå·®è·ï¼ç¢ºä¿æ¾åºææå¸æçæ¨¡åï¼ä¸¦éå°ä¸åçé«çä¿å¥æç¨ç¨å¼é²è¡èª¿æ´ã

##### **Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**
2409.07186v1 by Sheng Chen, Zihao Tang, Mariano Cabezas, Xinyi Wang, Arkiev D'Souza, Michael Barnett, Fernando Calamante, Weidong Cai, Chenyu Wang

Diffusion-weighted imaging (DWI) is a type of Magnetic Resonance Imaging
(MRI) technique sensitised to the diffusivity of water molecules, offering the
capability to inspect tissue microstructures and is the only in-vivo method to
reconstruct white matter fiber tracts non-invasively. The DWI signal can be
analysed with the diffusion tensor imaging (DTI) model to estimate the
directionality of water diffusion within voxels. Several scalar metrics,
including axial diffusivity (AD), mean diffusivity (MD), radial diffusivity
(RD), and fractional anisotropy (FA), can be further derived from DTI to
quantitatively summarise the microstructural integrity of brain tissue. These
scalar metrics have played an important role in understanding the organisation
and health of brain tissue at a microscopic level in clinical studies. However,
reliable DTI metrics rely on DWI acquisitions with high gradient directions,
which often go beyond the commonly used clinical protocols. To enhance the
utility of clinically acquired DWI and save scanning time for robust DTI
analysis, this work proposes DirGeo-DTI, a deep learning-based method to
estimate reliable DTI metrics even from a set of DWIs acquired with the minimum
theoretical number (6) of gradient directions. DirGeo-DTI leverages directional
encoding and geometric constraints to facilitate the training process. Two
public DWI datasets were used for evaluation, demonstrating the effectiveness
of the proposed method. Extensive experimental results show that the proposed
method achieves the best performance compared to existing DTI enhancement
methods and potentially reveals further clinical insights with routine clinical
DWI scans.

æè¦ï¼æ´æ£å æ¬å½±åï¼DWIï¼æ¯ä¸ç¨®ç£æ¯é å½±ï¼MRIï¼æè¡ï¼å°æ°´åå­æ´æ£ææï¼è½æª¢æ¸¬çµç¹å¾®çµæ§ï¼æ¯å¯ä¸éä¾µå¥æ§éå»ºç½è³ªçºç¶­æçé«å§æ¹æ³ãDWI è¨èå¯ç¨æ´æ£å¼µéå½±åï¼DTIï¼æ¨¡ååæï¼ä»¥ä¼°è¨é«ç´ å§æ°´æ´æ£çæ¹åæ§ãå¾ DTI å¯é²ä¸æ­¥è¡çåºæ¸åæ¨ééæ¸¬ï¼åæ¬è»¸åæ´æ£çï¼ADï¼ãå¹³åæ´æ£çï¼MDï¼ãå¾åæ´æ£çï¼RDï¼ååæ¸ååç°æ§ï¼FAï¼ï¼ä»¥éåç¸½çµè¦çµç¹çå¾®çµæ§å®æ´æ§ãéäºæ¨ééæ¸¬å¨è¨åºç ç©¶ä¸­å°æ¼äºè§£è¦çµç¹å¨å¾®è§å±¤é¢ççµç¹åå¥åº·æ®æ¼éè¦è§è²ãç¶èï¼å¯é ç DTI éæ¸¬ä»°è³´å·æé«æ¢¯åº¦æ¹åç DWI æ·åï¼ééå¸¸è¶åºè¨åºä¸å¸¸ç¨çåå®ãçºäºæåè¨åºæ·å DWI çæç¨ï¼ä¸¦çºç©©å¥ç DTI åæç¯çæææéï¼æ¬ç ç©¶æåº DirGeo-DTIï¼ä¸ç¨®åºæ¼æ·±åº¦å­¸ç¿çæ¹æ³ï¼å³ä½¿å¾å·åæå°çè«æ¸éï¼6ï¼çæ¢¯åº¦æ¹åæ·åç DWI çµä¹è½ä¼°è¨å¯é ç DTI éæ¸¬ãDirGeo-DTI å©ç¨æ¹åç·¨ç¢¼åå¹¾ä½ç´æä¾ä¿é²è¨ç·´éç¨ãä½¿ç¨å©åå¬éç DWI è³æéé²è¡è©ä¼°ï¼è­æäºææåºæ¹æ³çæææ§ãå»£æ³çå¯¦é©çµæé¡¯ç¤ºï¼èç¾æç DTI å¢å¼·æ¹æ³ç¸æ¯ï¼ææåºçæ¹æ³åå¾æä½³æè½ï¼ä¸¦æ½å¨æ­é²ä¾è¡è¨åº DWI ææçé²ä¸æ­¥è¨åºè¦è§£ã

##### **CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**
2409.07092v1 by Feiyang Jia, Zhineng Chen, Ziying Song, Lin Liu, Caiyan Jia

Super-resolution (SR) aims to enhance the quality of low-resolution images
and has been widely applied in medical imaging. We found that the design
principles of most existing methods are influenced by SR tasks based on
real-world images and do not take into account the significance of the
multi-level structure in pathological images, even if they can achieve
respectable objective metric evaluations. In this work, we delve into two
super-resolution working paradigms and propose a novel network called CWT-Net,
which leverages cross-scale image wavelet transform and Transformer
architecture. Our network consists of two branches: one dedicated to learning
super-resolution and the other to high-frequency wavelet features. To generate
high-resolution histopathology images, the Transformer module shares and fuses
features from both branches at various stages. Notably, we have designed a
specialized wavelet reconstruction module to effectively enhance the wavelet
domain features and enable the network to operate in different modes, allowing
for the introduction of additional relevant information from cross-scale
images. Our experimental results demonstrate that our model significantly
outperforms state-of-the-art methods in both performance and visualization
evaluations and can substantially boost the accuracy of image diagnostic
networks.

æè¦ï¼è¶è§£æåº¦ (SR) æ¨å¨æåä½è§£æåº¦å½±åçåè³ªï¼ä¸¦å·²å»£æ³æç¨æ¼é«å­¸å½±åãæåç¼ç¾ç¾ææ¹æ³çå¤§é¨åè¨­è¨ååé½åå°åºæ¼çå¯¦å½±åç SR ä»»åå½±é¿ï¼èä¸å³ä½¿å®åè½éå°å¯è§çå®¢è§ææ¨è©ä¼°ï¼ä¹ä¸æèæ®ççå½±åä¸­å¤å±¤ç´çµæ§çéè¦æ§ãå¨éé å·¥ä½ä¸­ï¼æåæ·±å¥æ¢è¨å©ç¨®è¶è§£æåº¦å·¥ä½ç¯ä¾ï¼ä¸¦æåºä¸ååçº CWT-Net çæ°åç¶²è·¯ï¼å®å©ç¨è·¨å°ºåº¦å½±åå°æ³¢è½æå Transformer æ¶æ§ãæåçç¶²è·¯åå«å©ååæ¯ï¼ä¸åå°éç¨æ¼å­¸ç¿è¶è§£æåº¦ï¼å¦ä¸ååç¨æ¼é«é »å°æ³¢ç¹å¾µãçºäºç¢çé«è§£æåº¦çµç¹ççå­¸å½±åï¼Transformer æ¨¡çµæå¨ä¸åéæ®µåäº«åèåä¾èªå©ååæ¯çç¹å¾µãå¼å¾æ³¨æçæ¯ï¼æåè¨­è¨äºä¸åå°éçå°æ³¢éå»ºæ¨¡çµï¼ä»¥ææå¢å¼·å°æ³¢åç¹å¾µï¼ä¸¦è®ç¶²è·¯è½å¤ å¨ä¸åæ¨¡å¼ä¸éä½ï¼åè¨±å¾è·¨å°ºåº¦å½±åä¸­å¼å¥å¶ä»ç¸éè³è¨ãæåçå¯¦é©çµæè­æï¼æåçæ¨¡åå¨æè½åè¦è¦ºåè©ä¼°æ¹é¢é½å¤§å¹åªæ¼ç¾ææè¡ï¼èä¸è½å¤§å¹æåå½±åè¨ºæ·ç¶²è·¯çæºç¢ºåº¦ã

##### **Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**
2409.07012v1 by Daeun Kyung, Junu Kim, Tackeun Kim, Edward Choi

Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals
to assess patient conditions and monitor changes over time. Generative models,
specifically diffusion-based models, have shown promise in generating realistic
synthetic X-rays. However, these models mainly focus on conditional generation
using single-time-point data, i.e., typically CXRs taken at a specific time
with their corresponding reports, limiting their clinical utility, particularly
for capturing temporal changes. To address this limitation, we propose a novel
framework, EHRXDiff, which predicts future CXR images by integrating previous
CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc.
Our framework dynamically tracks and predicts disease progression based on a
latent diffusion model, conditioned on the previous CXR image and a history of
medical events. We comprehensively evaluate the performance of our framework
across three key aspects, including clinical consistency, demographic
consistency, and visual realism. We demonstrate that our framework generates
high-quality, realistic future images that capture potential temporal changes,
suggesting its potential for further development as a clinical simulation tool.
This could offer valuable insights for patient monitoring and treatment
planning in the medical field.

æè¦ï¼è¸é¨ X åå½±åï¼CXRï¼æ¯ä¸ç¨®éè¦çè¨ºæ·å·¥å·ï¼ç¨æ¼é«é¢è©ä¼°çæ£çæ³ä¸¦ç£æ§å¶é¨èæéçè®åãçææ¨¡åï¼ç¹å¥æ¯åºæ¼æ´æ£çæ¨¡åï¼å·²å¨çæé¼ççåæ X åå½±åæ¹é¢å±ç¾åºæ½åãç¶èï¼éäºæ¨¡åä¸»è¦å°æ³¨æ¼ä½¿ç¨å®ä¸æéé»è³æé²è¡æ¢ä»¶çæï¼å³éå¸¸å¨ç¹å®æéé»ææç CXR åå¶å°æå ±åï¼ééå¶äºå¶è¨åºæç¨ï¼ç¹å¥æ¯å°æ¼æææéè®åãçºäºè§£æ±ºæ­¤éå¶ï¼æåæåºäºä¸åæ°çæ¡æ¶ EHRXDiffï¼å®ééæ´åååç CXR èå¾çºçé«çäºä»¶ï¼ä¾å¦èæ¹ãå¯¦é©å®¤æª¢æ¸¬ç­ï¼ä¾é æ¸¬æªä¾ç CXR å½±åãæåçæ¡æ¶åºæ¼æ½å¨æ´æ£æ¨¡ååæè¿½è¹¤ä¸¦é æ¸¬ç¾çé²å±ï¼æ¢ä»¶åæ±ºæ¼ååç CXR å½±ååé«çäºä»¶çæ­·å²è¨éãæåå¨é¢è©ä¼°äºæåæ¡æ¶å¨ä¸åééµæ¹é¢çæè½ï¼åæ¬è¨åºä¸è´æ§ãäººå£çµ±è¨ä¸è´æ§åè¦è¦ºé¼çåº¦ãæåè­ææåçæ¡æ¶çæäºé«åè³ªãé¼ççæªä¾å½±åï¼ææäºæ½å¨çæéè®åï¼éè¡¨æå¶é²ä¸æ­¥ç¼å±çºè¨åºæ¨¡æ¬å·¥å·çæ½åãéå¯ä»¥çºé«çé åççæ£ç£æ§åæ²»çè¦åæä¾æå¹å¼çè¦è§£ã

##### **Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**
2409.06928v1 by Jianmei Jiang, Huijin Wang, Jieyun Bai, Shun Long, Shuangping Chen, Victor M. Campello, Karim Lekadir

The segmentation of the pubic symphysis and fetal head (PSFH) constitutes a
pivotal step in monitoring labor progression and identifying potential delivery
complications. Despite the advances in deep learning, the lack of annotated
medical images hinders the training of segmentation. Traditional
semi-supervised learning approaches primarily utilize a unified network model
based on Convolutional Neural Networks (CNNs) and apply consistency
regularization to mitigate the reliance on extensive annotated data. However,
these methods often fall short in capturing the discriminative features of
unlabeled data and in delineating the long-range dependencies inherent in the
ambiguous boundaries of PSFH within ultrasound images. To address these
limitations, we introduce a novel framework, the Dual-Student and Teacher
Combining CNN and Transformer (DSTCT), which synergistically integrates the
capabilities of CNNs and Transformers. Our framework comprises a Vision
Transformer (ViT) as the teacher and two student mod ls one ViT and one CNN.
This dual-student setup enables mutual supervision through the generation of
both hard and soft pseudo-labels, with the consistency in their predictions
being refined by minimizing the classifier determinacy discrepancy. The teacher
model further reinforces learning within this architecture through the
imposition of consistency regularization constraints. To augment the
generalization abilities of our approach, we employ a blend of data and model
perturbation techniques. Comprehensive evaluations on the benchmark dataset of
the PSFH Segmentation Grand Challenge at MICCAI 2023 demonstrate our DSTCT
framework outperformed ten contemporary semi-supervised segmentation methods.
Code available at https://github.com/jjm1589/DSTCT.

æè¦ï¼æ¥éª¨è¯ååèé ­ï¼PSFHï¼çåå²æ¯ç£æ¸¬ç¢ç¨é²åº¦åè­å¥æ½å¨åå¨©ä½µç¼ççééµæ­¥é©ãåç®¡æ·±åº¦å­¸ç¿åå¾é²å±ï¼ä½æ¨è¨»é«å­¸å½±åçç¼ºä¹é»ç¤äºåå²çè¨ç·´ãå³çµ±çåç£ç£å¼å­¸ç¿æ¹æ³ä¸»è¦å©ç¨åºæ¼å·ç©ç¥ç¶ç¶²è·¯ï¼CNNï¼ççµ±ä¸ç¶²è·¯æ¨¡åï¼ä¸¦æç¨ä¸è´æ§æ­£ååä¾æ¸è¼å°å¤§éæ¨è¨»æ¸æçä¾è³´ãç¶èï¼éäºæ¹æ³éå¸¸ç¡æ³æææªæ¨è¨»æ¸æçåå¥æ§ç¹å¾µï¼ä¹ç¡æ³æç¹ªè¶é³æ³¢å½±åä¸­ PSFH æ¨¡ç³éçä¸­åºæçé·ç¨ä¾è³´æ§ãçºäºè§£æ±ºéäºéå¶ï¼æåå¼å¥äºä¸åæ°çæ¡æ¶ï¼å³éå­¸çåæå¸«çµå CNN å Transformerï¼DSTCTï¼ï¼å®ååæ´åäº CNN å Transformer çåè½ãæåçæ¡æ¶åå«ä¸åè¦è¦º Transformerï¼ViTï¼ä½çºæå¸«åå©åå­¸çæ¨¡åï¼ä¸å ViT åä¸å CNNãéç¨®éå­¸çè¨­ç½®ééçæç¡¬å½æ¨ç±¤åè»å½æ¨ç±¤å¯¦ç¾ç¸äºç£ç£ï¼ä¸¦ééæå°ååé¡å¨ç¢ºå®æ§å·®ç°ä¾åªåå¶é æ¸¬çä¸è´æ§ãæå¸«æ¨¡åééæ½å ä¸è´æ§æ­£ååç´æé²ä¸æ­¥å å¼·äºæ­¤æ¶æ§ä¸­çå­¸ç¿ãçºäºå¢å¼·æåæ¹æ³çæ³åè½åï¼æåæ¡ç¨äºæ¸æåæ¨¡åæ¾åæè¡çæ··åãå¨ MICCAI 2023 ç PSFH åå²å¤§ææ°åºæºæ¸æéä¸çç¶åè©ä¼°è¡¨æï¼æåç DSTCT æ¡æ¶åªæ¼åç¨®ç¶ä»£åç£ç£å¼åå²æ¹æ³ãç¨å¼ç¢¼å¯å¨ https://github.com/jjm1589/DSTCT åå¾ã

##### **Bifurcation Identification for Ultrasound-driven Robotic Cannulation**
2409.06817v1 by Cecilia G. Morales, Dhruv Srikanth, Jack H. Good, Keith A. Dufendach, Artur Dubrawski

In trauma and critical care settings, rapid and precise intravascular access
is key to patients' survival. Our research aims at ensuring this access, even
when skilled medical personnel are not readily available. Vessel bifurcations
are anatomical landmarks that can guide the safe placement of catheters or
needles during medical procedures. Although ultrasound is advantageous in
navigating anatomical landmarks in emergency scenarios due to its portability
and safety, to our knowledge no existing algorithm can autonomously extract
vessel bifurcations using ultrasound images. This is primarily due to the
limited availability of ground truth data, in particular, data from live
subjects, needed for training and validating reliable models. Researchers often
resort to using data from anatomical phantoms or simulations. We introduce
BIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a
novel algorithm that identifies vessel bifurcations and provides optimal needle
insertion sites for an autonomous robotic cannulation system. BIFURC integrates
expert knowledge with deep learning techniques to efficiently detect vessel
bifurcations within the femoral region and can be trained on a limited amount
of in-vivo data. We evaluated our algorithm using a medical phantom as well as
real-world experiments involving live pigs. In all cases, BIFURC consistently
identified bifurcation points and needle insertion locations in alignment with
those identified by expert clinicians.

æè¦ï¼å¨åµå·åéçç§è­·ç°å¢ä¸­ï¼å¿«éä¸ç²¾ç¢ºçè¡ç®¡å§éè·¯æ¯æ£èå­æ´»çééµãæåçç ç©¶æ¨å¨ç¢ºä¿éç¨®éè·¯ï¼å³ä½¿å¨çç·´çé«çäººå¡ç¡æ³ç«å³ç²å¾çææ³ä¸ãè¡ç®¡ååæ¯è§£åæ¨èªï¼å¯ä»¥æå°å¨é«çéç¨ä¸­å®å¨æ¾ç½®å°ç®¡æéé ­ãåç®¡è¶é³æ³¢ç±æ¼å¶å¯ææ§åå®å¨æ§èå¨ç·æ¥ææ³ä¸å°èªè§£åæ¨èªå·æåªå¢ï¼ä½ææåæç¥ï¼æ²æç¾ææ¼ç®æ³å¯ä»¥ä½¿ç¨è¶é³æ³¢å½±åèªåæåè¡ç®¡ååãéä¸»è¦æ¯ç±æ¼å°é¢å¯¦æ³è³æçå¯ç¨æ§æéï¼ç¹å¥æ¯ä¾èªæ´»é«åè©¦èçè³æï¼èéå°æ¼è¨ç·´åé©è­å¯é æ¨¡åæ¯å¿éçãç ç©¶äººå¡ç¶å¸¸æ±å©æ¼ä½¿ç¨è§£åæ¨¡åææ¨¡æ¬çè³æãæåå¼å¥äº BIFURCï¼å³è¶é³æ³¢é©åæ©å¨äººæç®¡çååè­å¥ï¼éæ¯ä¸ç¨®æ°ç©çæ¼ç®æ³ï¼å¯ä»¥è­å¥è¡ç®¡ååï¼ä¸¦çºèªåæ©å¨äººæç®¡ç³»çµ±æä¾æä½³éé ­æå¥ä½ç½®ãBIFURC å°å°å®¶ç¥è­èæ·±åº¦å­¸ç¿æè¡ç¸çµåï¼ä»¥æææª¢æ¸¬è¡éª¨ååå§çè¡ç®¡ååï¼ä¸¦ä¸å¯ä»¥å¨æéçé«å§è³æä¸é²è¡è¨ç·´ãæåä½¿ç¨é«ç¨æ¨¡åä»¥åæ¶åæ´»é«è±¬ççå¯¦ä¸çå¯¦é©è©ä¼°äºæåçæ¼ç®æ³ãå¨ææææ³ä¸ï¼BIFURC é½ä¸è´å°è­å¥åºååé»åéé ­æå¥ä½ç½®ï¼èå°å®¶è¨åºé«çè­å¥çä½ç½®ä¸è´ã

##### **Personalized Federated Learning Techniques: Empirical Analysis**
2409.06805v1 by Azal Ahmad Khan, Ahmad Faraz Khan, Haider Ali, Ali Anwar

Personalized Federated Learning (pFL) holds immense promise for tailoring
machine learning models to individual users while preserving data privacy.
However, achieving optimal performance in pFL often requires a careful
balancing act between memory overhead costs and model accuracy. This paper
delves into the trade-offs inherent in pFL, offering valuable insights for
selecting the right algorithms for diverse real-world scenarios. We empirically
evaluate ten prominent pFL techniques across various datasets and data splits,
uncovering significant differences in their performance. Our study reveals
interesting insights into how pFL methods that utilize personalized (local)
aggregation exhibit the fastest convergence due to their efficiency in
communication and computation. Conversely, fine-tuning methods face limitations
in handling data heterogeneity and potential adversarial attacks while
multi-objective learning methods achieve higher accuracy at the cost of
additional training and resource consumption. Our study emphasizes the critical
role of communication efficiency in scaling pFL, demonstrating how it can
significantly affect resource usage in real-world deployments.

æè¦ï¼åäººåè¯åå­¸ç¿ (pFL) å¨ç¶­è­·è³æé±ç§çåæï¼çºå®¢è£½åæ©å¨å­¸ç¿æ¨¡åçµ¦åå¥ä½¿ç¨èå¸¶ä¾æ¥µå¤§çå¸æãç¶èï¼è¦éæ pFL çæä½³æè½ï¼éå¸¸éè¦å¨è¨æ¶é«éé·ææ¬åæ¨¡åæºç¢ºåº¦ä¹éåå¾ä»ç´°çå¹³è¡¡ãæ¬ææ·±å¥æ¢è¨ pFL ä¸­åºæçæ¬è¡¡åæ¨ï¼çºå¨åç¨®å¯¦éå ´æ¯ä¸­é¸ææ­£ç¢ºçæ¼ç®æ³æä¾å¯¶è²´çè¦è§£ãæåæ ¹æåç¨®è³æéåè³æåå²ï¼å°åç¨®ååºç pFL æè¡é²è¡å¯¦è­è©ä¼°ï¼æ­é²å¶æè½çé¡¯èå·®ç°ãæåçç ç©¶æ­é²äºæè¶£çè¦è§£ï¼èªªæå©ç¨åäººå (å±é¨) èåç pFL æ¹æ³ï¼ç±æ¼å¶å¨éè¨åéç®æ¹é¢çæçï¼å±ç¾åºæå¿«çæ¶æéåº¦ãç¸åå°ï¼å¾®èª¿æ¹æ³å¨èçè³æç°è³ªæ§åæ½å¨å°ææ»ææ¹é¢é¢è¨éå¶ï¼èå¤ç®æ¨å­¸ç¿æ¹æ³åä»¥é¡å¤çè¨ç·´åè³æºæ¶èçºä»£å¹ï¼éå°æ´é«çæºç¢ºåº¦ãæåçç ç©¶å¼·èª¿äºéè¨æçå¨æ´å pFL ä¸­çééµè§è²ï¼å±ç¤ºå®å¦ä½å¨å¯¦éé¨ç½²ä¸­é¡¯èå½±é¿è³æºä½¿ç¨ã

##### **Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**
2409.06672v1 by Cristian Trout

Many experts believe that AI systems will sooner or later pose uninsurable
risks, including existential risks. This creates an extreme judgment-proof
problem: few if any parties can be held accountable ex post in the event of
such a catastrophe. This paper proposes a novel solution: a
government-provided, mandatory indemnification program for AI developers. The
program uses risk-priced indemnity fees to induce socially optimal levels of
care. Risk-estimates are determined by surveying experts, including indemnified
developers. The Bayesian Truth Serum mechanism is employed to incent honest and
effortful responses. Compared to alternatives, this approach arguably better
leverages all private information, and provides a clearer signal to indemnified
developers regarding what risks they must mitigate to lower their fees. It's
recommended that collected fees be used to help fund the safety research
developers need, employing a fund matching mechanism (Quadratic Financing) to
induce an optimal supply of this public good. Under Quadratic Financing, safety
research projects would compete for private contributions from developers,
signaling how much each is to be supplemented with public funds.

æè¦ï¼è¨±å¤å°å®¶ç¸ä¿¡ AI ç³»çµ±é²æ©æé æç¡æ³æ¿ä¿çé¢¨éªï¼åæ¬çå­é¢¨éªãéæé ææ¥µç«¯çç¡æ³è¿½ç©¶è²¬ä»»åé¡ï¼å¨ç¼çæ­¤é¡ç½é£æï¼å¹¾ä¹æ²æä»»ä½ä¸æ¹å¯ä»¥äºå¾è¢«è¿½ç©¶è²¬ä»»ãæ¬ææåºäºä¸ååµæ°çè§£æ±ºæ¹æ¡ï¼æ¿åºæä¾ç AI éç¼äººå¡å¼·å¶æ§è£åè¨ç«ãè©²è¨ç«ä½¿ç¨é¢¨éªå®å¹çè£åè²»ç¨ä¾èªä½¿éå°ç¤¾ææé©ç¨åº¦çç§è­·ãé¢¨éªä¼°è¨å¼æ¯ç±èª¿æ¥å°å®¶ï¼åæ¬ç²å¾è£åçéç¼äººå¡ï¼ä¾æ±ºå®ãè²æ°çè©±è¡æ¸æ©å¶è¢«ç¨ä¾æ¿åµèª å¯¦ä¸åªåçåæãèå¶ä»æ¹æ³ç¸æ¯ï¼éç¨®æ¹æ³å¯ä»¥èªªè½æ´å¥½å°å©ç¨ææç§äººè³è¨ï¼ä¸¦åç²å¾è£åçéç¼äººå¡æä¾æ´æç¢ºçè¨èï¼èªªæä»åå¿é æ¸è¼åªäºé¢¨éªæè½éä½è²»ç¨ãå»ºè­°å°æ¶åçè²»ç¨ç¨æ¼è³å©å®å¨ç ç©¶éç¼äººå¡æéçç ç©¶ï¼ä¸¦æ¡ç¨åºééå°æ©å¶ï¼äºæ¬¡æ¹èè³ï¼ä¾èªä½¿æä¾éç¨®å¬å±è²¡çæä½³ä¾æãå¨äºæ¬¡æ¹èè³ä¸ï¼å®å¨ç ç©¶è¨ç«å°ç«¶ç­éç¼äººå¡çç§äººææ¬¾ï¼ä¸¦è¡¨ç¤ºå¶ä¸­æå¤å°å°ç±å¬å±è³éè£åã

##### **EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**
2409.06644v2 by Danli Shi, Weiyi Zhang, Jiancheng Yang, Siyu Huang, Xiaolan Chen, Mayinuer Yusufu, Kai Jin, Shan Lin, Shunming Liu, Qing Zhang, Mingguang He

Early detection of eye diseases like glaucoma, macular degeneration, and
diabetic retinopathy is crucial for preventing vision loss. While artificial
intelligence (AI) foundation models hold significant promise for addressing
these challenges, existing ophthalmic foundation models primarily focus on a
single modality, whereas diagnosing eye diseases requires multiple modalities.
A critical yet often overlooked aspect is harnessing the multi-view information
across various modalities for the same patient. Additionally, due to the
long-tail nature of ophthalmic diseases, standard fully supervised or
unsupervised learning approaches often struggle. Therefore, it is essential to
integrate clinical text to capture a broader spectrum of diseases. We propose
EyeCLIP, a visual-language foundation model developed using over 2.77 million
multi-modal ophthalmology images with partial text data. To fully leverage the
large multi-modal unlabeled and labeled data, we introduced a pretraining
strategy that combines self-supervised reconstructions, multi-modal image
contrastive learning, and image-text contrastive learning to learn a shared
representation of multiple modalities. Through evaluation using 14 benchmark
datasets, EyeCLIP can be transferred to a wide range of downstream tasks
involving ocular and systemic diseases, achieving state-of-the-art performance
in disease classification, visual question answering, and cross-modal
retrieval. EyeCLIP represents a significant advancement over previous methods,
especially showcasing few-shot, even zero-shot capabilities in real-world
long-tail scenarios.

æè¦ï¼æ©æåµæ¸¬éåç¼ãé»æé¨çè®åç³å°¿çè¦ç¶²èçè®ç­ç¼ç¾å°æ¼é é²è¦ååªå¤±è³ééè¦ãåç®¡äººå·¥æºæ§ (AI) åºç¤æ¨¡åå¨æå°éäºææ°æ¹é¢æ¥µå·åæ¯ï¼ä½ç¾æçç¼ç§åºç¤æ¨¡åä¸»è¦éæ³¨æ¼å®ä¸æ¨¡å¼ï¼èè¨ºæ·ç¼ç¾éè¦å¤ç¨®æ¨¡å¼ãä¸åéè¦ä½ç¶å¸¸è¢«å¿½è¦çæ¹é¢æ¯å©ç¨åä¸æ£èä¸åæ¨¡å¼çå¤è¦åè³è¨ãæ­¤å¤ï¼ç±æ¼ç¼ç§ç¾ççé·å°¾æ§è³ªï¼æ¨æºçå¨ç£ç£æç¡ç£ç£å­¸ç¿æ¹æ³éå¸¸é£ä»¥æä»ãå æ­¤ï¼æ´åè¨åºææ¬ä»¥æ¶µèæ´å»£æ³çç¾çè­ç³»è³ééè¦ãæåæåº EyeCLIPï¼éæ¯ä¸åè¦è¦ºèªè¨åºç¤æ¨¡åï¼ä½¿ç¨è¶é 277 è¬å¼µå·æé¨åæå­è³æçå¤æ¨¡å¼ç¼ç§å½±åéç¼èæãçºäºååå©ç¨å¤§éçå¤æ¨¡å¼æªæ¨è¨åæ¨è¨è³æï¼æåå¼å¥äºä¸ç¨®é è¨ç·´ç­ç¥ï¼çµåäºèªæç£ç£éå»ºãå¤æ¨¡å¼å½±åå°æ¯å­¸ç¿åå½±åæå­å°æ¯å­¸ç¿ï¼ä»¥å­¸ç¿å¤ç¨®æ¨¡å¼çå±äº«è¡¨å¾µãééä½¿ç¨ 14 ååºæºè³æéé²è¡è©ä¼°ï¼EyeCLIP å¯ä»¥è½ç§»å°æ¶åç¼é¨åå¨èº«ç¾ççå»£æ³ä¸æ¸¸ä»»åï¼å¨ç¾çåé¡ãè¦è¦ºåé¡è§£ç­åè·¨æ¨¡å¼æª¢ç´¢ä¸­å¯¦ç¾æåé²çæè½ãEyeCLIP ä»£è¡¨äºå°ååæ¹æ³çéå¤§é²å±ï¼ç¹å¥æ¯å¨ç¾å¯¦ä¸çé·å°¾å ´æ¯ä¸­å±ç¤ºäºå°æ¨£æ¬ï¼çè³é¶æ¨£æ¬çè½åã

##### **Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**
2409.06585v1 by Zoe Hancox, Sarah R. Kingsbury, Andrew Clegg, Philip G. Conaghan, Samuel D. Relton

Background: Hip replacement procedures improve patient lives by relieving
pain and restoring mobility. Predicting hip replacement in advance could reduce
pain by enabling timely interventions, prioritising individuals for surgery or
rehabilitation, and utilising physiotherapy to potentially delay the need for
joint replacement. This study predicts hip replacement a year in advance to
enhance quality of life and health service efficiency. Methods: Adapting
previous work using Temporal Graph Convolutional Neural Network (TG-CNN)
models, we construct temporal graphs from primary care medical event codes,
sourced from ResearchOne EHRs of 40-75-year-old patients, to predict hip
replacement risk. We match hip replacement cases to controls by age, sex, and
Index of Multiple Deprivation. The model, trained on 9,187 cases and 9,187
controls, predicts hip replacement one year in advance. We validate the model
on two unseen datasets, recalibrating for class imbalance. Additionally, we
conduct an ablation study and compare against four baseline models. Results:
Our best model predicts hip replacement risk one year in advance with an AUROC
of 0.724 (95% CI: 0.715-0.733) and an AUPRC of 0.185 (95% CI: 0.160-0.209),
achieving a calibration slope of 1.107 (95% CI: 1.074-1.139) after
recalibration. Conclusions: The TG-CNN model effectively predicts hip
replacement risk by identifying patterns in patient trajectories, potentially
improving understanding and management of hip-related conditions.

æè¦ï¼èæ¯ï¼é«éç¯ç½®ææè¡å¯æ¸è¼ç¼çä¸¦æ¢å¾©è¡åè½åï¼é²èæ¹åæ£èçæ´»ãé æ¸¬é«éç¯ç½®ææè¡æå©æ¼åæä»å¥ãåªåå®æåäººé²è¡æè¡æå¾©å¥ï¼ä¸¦å©ç¨ç©çæ²»çä¾å»¶ç·©éç¯ç½®ææè¡çå¿è¦æ§ï¼é²èæ¸å°ç¼çãæ¬ç ç©¶é æ¸¬ä¸å¹´å¾çé«éç¯ç½®ææè¡ï¼ä»¥æåçæ´»åè³ªåé«çæåæçãæ¹æ³ï¼æ¡ç¨æéåå½¢å·ç©ç¥ç¶ç¶²è·¯ (TG-CNN) æ¨¡åæ¹ç·¨ååçç ç©¶ï¼æåå¾ ResearchOne EHR 40-75 æ­²æ£èçä¸»è¦ç§è­·é«çäºä»¶ä»£ç¢¼å»ºæ§æéåå½¢ï¼ä»¥é æ¸¬é«éç¯ç½®ææè¡é¢¨éªãæåæ ¹æå¹´é½¡ãæ§å¥åå¤éåå¥ªææ¸ï¼å°é«éç¯ç½®ææè¡çä¾èå°ç§çµé²è¡éå°ãè©²æ¨¡åéå° 9,187 åçä¾å 9,187 åå°ç§çµé²è¡è¨ç·´ï¼é æ¸¬ä¸å¹´å¾çé«éç¯ç½®ææè¡ãæåå¨å©åæªè¦æ¸æéé©è­æ¨¡åï¼ä¸¦éæ°æ ¡æºä»¥è§£æ±ºé¡å¥ä¸å¹³è¡¡åé¡ãæ­¤å¤ï¼æåé²è¡æ¶èç ç©¶ï¼ä¸¦èåååºæºæ¨¡åé²è¡æ¯è¼ãçµæï¼æåæä½³çæ¨¡åé æ¸¬ä¸å¹´å¾çé«éç¯ç½®ææè¡é¢¨éªï¼AUROC çº 0.724 (95% CIï¼0.715-0.733)ï¼AUPRC çº 0.185 (95% CIï¼0.160-0.209)ï¼éæ°æ ¡æºå¾æ ¡æºæççº 1.107 (95% CIï¼1.074-1.139)ãçµè«ï¼TG-CNN æ¨¡åå¯ææé æ¸¬é«éç¯ç½®ææè¡é¢¨éªï¼æ¹æ³æ¯æ¾åºæ£èè»è·¡ä¸­çæ¨¡å¼ï¼é²èæ½å¨æ¹åå°é«éç¯ç¸éç¾ççäºè§£åç®¡çã

##### **MAGDA: Multi-agent guideline-driven diagnostic assistance**
2409.06351v1 by David Bani-Harouni, Nassir Navab, Matthias Keicher

In emergency departments, rural hospitals, or clinics in less developed
regions, clinicians often lack fast image analysis by trained radiologists,
which can have a detrimental effect on patients' healthcare. Large Language
Models (LLMs) have the potential to alleviate some pressure from these
clinicians by providing insights that can help them in their decision-making.
While these LLMs achieve high test results on medical exams showcasing their
great theoretical medical knowledge, they tend not to follow medical
guidelines. In this work, we introduce a new approach for zero-shot
guideline-driven decision support. We model a system of multiple LLM agents
augmented with a contrastive vision-language model that collaborate to reach a
patient diagnosis. After providing the agents with simple diagnostic
guidelines, they will synthesize prompts and screen the image for findings
following these guidelines. Finally, they provide understandable
chain-of-thought reasoning for their diagnosis, which is then self-refined to
consider inter-dependencies between diseases. As our method is zero-shot, it is
adaptable to settings with rare diseases, where training data is limited, but
expert-crafted disease descriptions are available. We evaluate our method on
two chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing
performance improvement over existing zero-shot methods and generalizability to
rare diseases.

æè¦ï¼å¨æ¥è¨ºå®¤ãéæé«é¢ææ¬ ç¼éå°åçè¨ºæï¼è¨åºé«å¸«å¸¸å¸¸ç¼ºä¹åéè¨ç·´çæ¾å°ç§é«å¸«é²è¡å¿«éçå½±ååæï¼éå¯è½æå°çæ£çé«çä¿å¥é æä¸å©å½±é¿ãå¤§åèªè¨æ¨¡å (LLM) ææ½åæ¸è¼éäºè¨åºé«å¸«çä¸äºå£åï¼æ¹æ³æ¯æä¾è¦è§£ï¼åå©ä»åé²è¡æ±ºç­ãåç®¡éäº LLM å¨å±ç¤ºå¶è±å¯ççè«é«å­¸ç¥è­çé«å­¸èè©¦ä¸­ç²å¾äºå¾é«çæ¸¬è©¦çµæï¼ä½å®åå¾å¾ä¸éµå¾ªé«çæåãå¨éé å·¥ä½ä¸­ï¼æåä»ç´¹äºä¸ç¨®æ°çé¶æ¬¡å­¸ç¿æå°æ¹éé©åæ±ºç­æ¯æ´æ¹æ³ãæåæ¨¡æ¬äºä¸åå¤å LLM ä»£çç³»çµ±ï¼ä¸¦å¢å¼·äºä¸åå°æ¯è¦è¦ºèªè¨æ¨¡åï¼è©²æ¨¡ååä½ä»¥éæçæ£è¨ºæ·ãå¨çºä»£çæä¾ç°¡å®çè¨ºæ·æåå¾ï¼å®åå°ç¶åæç¤ºä¸¦æ ¹æéäºæåç¯©é¸å½±åä»¥æ¾åºç¼ç¾ãæå¾ï¼å®åçºå¶è¨ºæ·æä¾å¯ä»¥çè§£çæè·¯æ¨çï¼ç¶å¾èªæç²¾é²ä»¥èéç¾çä¹éçç¸äºä¾å­éä¿ãç±æ¼æåçæ¨¡åæ¯é¶æ¬¡å­¸ç¿ï¼å æ­¤å®å¯ä»¥é©æç½è¦ç¾ççè¨­å®ï¼å¨éç¨®è¨­å®ä¸­ï¼è¨ç·´è³ææéï¼ä½æå°å®¶è£½ä½çç¾çæè¿°å¯ç¨ãæåå¨å©åè¸é¨ X åçè³æéï¼CheXpert å ChestX-ray 14 Longtailï¼è©ä¼°æåçæ¨¡åï¼å±ç¤ºäºç¸è¼æ¼ç¾æçé¶æ¬¡å­¸ç¿æ¹æ³çæè½æåï¼ä»¥åå°ç½è¦ç¾ççæ¦æ¬æ§ã

##### **Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**
2409.06209v1 by Xin Zhang, Deval Mehta, Yanan Hu, Chao Zhu, David Darby, Zhen Yu, Daniel Merlo, Melissa Gresle, Anneke Van Der Walt, Helmut Butzkueven, Zongyuan Ge

Survival analysis holds a crucial role across diverse disciplines, such as
economics, engineering and healthcare. It empowers researchers to analyze both
time-invariant and time-varying data, encompassing phenomena like customer
churn, material degradation and various medical outcomes. Given the complexity
and heterogeneity of such data, recent endeavors have demonstrated successful
integration of deep learning methodologies to address limitations in
conventional statistical approaches. However, current methods typically involve
cluttered probability distribution function (PDF), have lower sensitivity in
censoring prediction, only model static datasets, or only rely on recurrent
neural networks for dynamic modelling. In this paper, we propose a novel
survival regression method capable of producing high-quality unimodal PDFs
without any prior distribution assumption, by optimizing novel
Margin-Mean-Variance loss and leveraging the flexibility of Transformer to
handle both temporal and non-temporal data, coined UniSurv. Extensive
experiments on several datasets demonstrate that UniSurv places a significantly
higher emphasis on censoring compared to other methods.

æè¦ï¼å­æ´»åæå¨ç¶æ¿ãå·¥ç¨åé«çä¿å¥ç­ä¸åå­¸ç§ä¸­æ®æ¼èè³ééè¦çè§è²ãå®è®ç ç©¶äººå¡è½å¤ åææä¸è®åæè®æ¸æï¼åå«å®¢æ¶æµå¤±ãææéè§£ååç¨®é«ççµæç­ç¾è±¡ãéæ¼æ­¤é¡æ¸æçè¤éæ§åç°è³ªæ§ï¼æè¿çåªåå·²è­ææåæ´åæ·±åº¦å­¸ç¿æ¹æ³ä»¥è§£æ±ºå³çµ±çµ±è¨æ¹æ³çéå¶ãç¶èï¼ç®åçæ¹æ³éå¸¸æ¶åéäºçæ©çåä½å½æ¸ (PDF)ï¼å¨å¯©æ¥é æ¸¬ä¸­å·æè¼ä½çæææ§ï¼åå°éææ¸æéé²è¡å»ºæ¨¡ï¼æåä¾è³´éè¿´ç¥ç¶ç¶²è·¯é²è¡åæå»ºæ¨¡ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°ç©çå­æ´»è¿´æ­¸æ¹æ³ï¼è½å¤ å¨æ²æä»»ä½åé©åä½åè¨­çææ³ä¸ç¢çé«åè³ªçå®å³° PDFï¼èç±æä½³åæ°ç©çééå¹³åå¼è®ç°æå¤±ï¼ä¸¦å©ç¨ Transformer çéæ´»æ§ä¾èçæéåéæéæ¸æï¼ç¨±çº UniSurvãå¨å¹¾åæ¸æéä¸çå»£æ³å¯¦é©è­æï¼èå¶ä»æ¹æ³ç¸æ¯ï¼UniSurv å°å¯©æ¥çéè¦ç¨åº¦é¡¯èæé«ã

##### **Can Large Language Models Unlock Novel Scientific Research Ideas?**
2409.06185v1 by Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal

"An idea is nothing more nor less than a new combination of old elements"
(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and
publicly available ChatGPT have marked a significant turning point in the
integration of Artificial Intelligence (AI) into people's everyday lives. This
study explores the capability of LLMs in generating novel research ideas based
on information from research papers. We conduct a thorough examination of 4
LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and
Physics). We found that the future research ideas generated by Claude-2 and
GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.
We also found that Claude-2 generates more diverse future research ideas than
GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the
novelty, relevancy, and feasibility of the generated future research ideas.
This investigation offers insights into the evolving role of LLMs in idea
generation, highlighting both its capability and limitations. Our work
contributes to the ongoing efforts in evaluating and utilizing language models
for generating future research ideas. We make our datasets and codes publicly
available.

æè¦ï¼ãä¸åæ³æ³ä¸éå°±æ¯èåç´ çæ°çµåèå·²ã
(Young, J.W.)ãå¤§åèªè¨æ¨¡å (LLM) åå¬éç ChatGPT å»£æ³æ¡ç¨ï¼æ¨èªèäººå·¥æºè½ (AI) æ´åå°äººåæ¥å¸¸çæ´»ä¸­çéè¦è½æé»ãæ¬ç ç©¶æ¢è¨äº LLM å¨æ ¹æç ç©¶è«æè³è¨ç¢çæ°ç ç©¶æ³æ³æ¹é¢çè½åãæåå°äºåé åï¼ä¾å¦åå­¸ãé»è¦ãç¶æ¿ãé«å­¸åç©çï¼ä¸­ç 4 å LLM é²è¡äºå¾¹åºæª¢æ¥ãæåç¼ç¾ Claude-2 å GPT-4 ç¢ççæªä¾ç ç©¶æ³æ³æ¯ GPT-3.5 å Gemini æ´ç¬¦åä½èçè§é»ãæåéç¼ç¾ï¼Claude-2 ç¢ççæªä¾ç ç©¶æ³æ³æ¯ GPT-4ãGPT-3.5 å Gemini 1.0 æ´çºå¤æ¨£åãæåé²ä¸æ­¥å°ç¢ççæªä¾ç ç©¶æ³æ³çæ°ç©æ§ãç¸éæ§åå¯è¡æ§é²è¡äºäººå·¥è©ä¼°ãæ¬èª¿æ¥æä¾äºå° LLM å¨ç¢çæ³æ³ä¸­ä¸æ·æ¼è®çè§è²çè¦è§£ï¼çªåºäºå¶è½ååéå¶ãæåçç ç©¶æå©æ¼è©ä¼°åå©ç¨èªè¨æ¨¡åä¾ç¢çæªä¾ç ç©¶æ³æ³çæçºåªåãæåå¬éæä¾æåçæ¸æéåç¨å¼ç¢¼ã

##### **Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**
2409.06173v1 by Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan

In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the
dominant technique for performing natural language tasks, as it does not
require updating the model parameters with gradient-based methods. ICL promises
to "adapt" the LLM to perform the present task at a competitive or
state-of-the-art level at a fraction of the computational cost. ICL can be
augmented by incorporating the reasoning process to arrive at the final label
explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.
However, recent work has found that ICL relies mostly on the retrieval of task
priors and less so on "learning" to perform tasks, especially for complex
subjective domains like emotion and morality, where priors ossify posterior
predictions. In this work, we examine whether "enabling" reasoning also creates
the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors
that remain relatively unchanged despite the evidence in the prompt. We find
that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL
for larger language models. Code is avalaible at
https://github.com/gchochla/cot-priors.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ä¸­çèªå¢å­¸ç¿ (ICL) å·²æçºå·è¡èªç¶èªè¨ä»»åçä¸»æµæè¡ï¼å çºå®ä¸éè¦ä½¿ç¨åºæ¼æ¢¯åº¦çæ¨¡ååæ¸ä¾æ´æ°ãICL æ¿è«¾ä»¥æ¥µä½çè¨ç®ææ¬ãèª¿æ´ãLLMï¼ä»¥å¨ç«¶ç­ææåé²çå±¤ç´å·è¡ç¶åä»»åãICL å¯ä»¥ééå¨æç¤ºä¸­æç¢ºå°ç´å¥æ¨çéç¨ä¾æ´åï¼ä»¥å¾åºæçµæ¨ç±¤ï¼éé æè¡ç¨±çºæèé (CoT) æç¤ºãç¶èï¼æè¿çç ç©¶ç¼ç¾ï¼ICL ä¸»è¦ä¾è³´æ¼ä»»ååé©çæª¢ç´¢ï¼è¼å°ä¾è³´æ¼ãå­¸ç¿ãå·è¡ä»»åï¼ç¹å¥æ¯å°æ¼æç·åéå¾·ç­è¤éçä¸»è§é åï¼å¶ä¸­åé©æåµåå¾é©é æ¸¬ãå¨éé ç ç©¶ä¸­ï¼æåæ¢è¨ãåç¨ãæ¨çæ¯å¦ä¹æå¨ LLM ä¸­ç¢çç¸åçè¡çºï¼å¶ä¸­ CoT çæ ¼å¼ææª¢ç´¢æ¨çåé©ï¼åç®¡æç¤ºä¸­çè­æä¸åï¼ä½éäºåé©ä»ç¶ç¸å°ä¸è®ãæåç¼ç¾ï¼ä»¤äººé©è¨çæ¯ï¼å°æ¼è¼å¤§çèªè¨æ¨¡åï¼CoT ç¢ºå¯¦æè ICL ç¼çç¸åçå¾é©å´©æ½°ãç¨å¼ç¢¼å¯æ¼ https://github.com/gchochla/cot-priors åå¾ã

##### **Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**
2409.06147v1 by Dong Han, Jihye Moon, LuÃ­s Roberto Mercado DÃ­az, Darren Chen, Devan Williams, Eric Y. Ding, Khanh-Van Tran, David D. McManus, Ki H. Chon

Most deep learning models of multiclass arrhythmia classification are tested
on fingertip photoplethysmographic (PPG) data, which has higher signal-to-noise
ratios compared to smartwatch-derived PPG, and the best reported sensitivity
value for premature atrial/ventricular contraction (PAC/PVC) detection is only
75%. To improve upon PAC/PVC detection sensitivity while maintaining high AF
detection, we use multi-modal data which incorporates 1D PPG, accelerometers,
and heart rate data as the inputs to a computationally efficient 1D
bi-directional Gated Recurrent Unit (1D-Bi-GRU) model to detect three
arrhythmia classes. We used motion-artifact prone smartwatch PPG data from the
NIH-funded Pulsewatch clinical trial. Our multimodal model tested on 72
subjects achieved an unprecedented 83% sensitivity for PAC/PVC detection while
maintaining a high accuracy of 97.31% for AF detection. These results
outperformed the best state-of-the-art model by 20.81% for PAC/PVC and 2.55%
for AF detection even while our model was computationally more efficient (14
times lighter and 2.7 faster).

æè¦ï¼å¤§å¤æ¸å¤é¡å¿å¾ä¸æ´åé¡çæ·±åº¦å­¸ç¿æ¨¡åé½æ¯å¨æå°åé»å®¹ç©æè¨æ³ (PPG) è³æä¸é²è¡æ¸¬è©¦ï¼èæºæ§æé¶è¡çç PPG ç¸æ¯ï¼å¶è¨èéè¨æ¯æ´é«ï¼èå°æ¼æåå¿æ¿/å¿å®¤æ¶ç¸® (PAC/PVC) åµæ¸¬æå ±åçæä½³ææåº¦å¼åçº 75%ãçºäºå¨ç¶­æé«æ¿é¡«åµæ¸¬çåææé« PAC/PVC åµæ¸¬ææåº¦ï¼æåä½¿ç¨å¤æ¨¡å¼è³æï¼å° 1D PPGãå éåº¦è¨åå¿çè³æä½çºè¨ç®æçé«ç 1D éåéæ§éè¿´å®å (1D-Bi-GRU) æ¨¡åçè¼¸å¥ï¼ä»¥åµæ¸¬ä¸é¡å¿å¾ä¸æ´ãæåä½¿ç¨äºç¾ååå®¶è¡çç ç©¶é¢è³å©ç Pulsewatch è¨åºè©¦é©ä¸­çéåå½å½±æææºæ§æé¶ PPG è³æãæåå¨ 72 ååè©¦èèº«ä¸æ¸¬è©¦çå¤æ¨¡å¼æ¨¡åï¼å°æ¼ PAC/PVC åµæ¸¬éå°äºåææªæç 83% ææåº¦ï¼åæå°æ¼æ¿é¡«åµæ¸¬ç¶­æäº 97.31% çé«æºç¢ºåº¦ãå³ä½¿æåçæ¨¡åå¨è¨ç®ä¸æ´ææçï¼è¼ 14 åï¼å¿« 2.7 åï¼ï¼éäºçµæä»æ¯æåé²çæ¨¡åå¨ PAC/PVC åµæ¸¬ä¸é«åº 20.81%ï¼å¨æ¿é¡«åµæ¸¬ä¸é«åº 2.55%ã

##### **ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**
2409.05592v1 by Zhaoyue Sun, Jiazheng Li, Gabriele Pergola, Yulan He

Predicting unknown drug-drug interactions (DDIs) is crucial for improving
medication safety. Previous efforts in DDI prediction have typically focused on
binary classification or predicting DDI categories, with the absence of
explanatory insights that could enhance trust in these predictions. In this
work, we propose to generate natural language explanations for DDI predictions,
enabling the model to reveal the underlying pharmacodynamics and
pharmacokinetics mechanisms simultaneously as making the prediction. To do
this, we have collected DDI explanations from DDInter and DrugBank and
developed various models for extensive experiments and analysis. Our models can
provide accurate explanations for unknown DDIs between known drugs. This paper
contributes new tools to the field of DDI prediction and lays a solid
foundation for further research on generating explanations for DDI predictions.

æè¦ï¼é æ¸¬æªç¥çè¥ç©äº¤äºä½ç¨ (DDI) å°æ¼æ¹åè¥ç©å®å¨è³ééè¦ãååå¨ DDI é æ¸¬æ¹é¢æåçåªåéå¸¸éä¸­æ¼äºååé¡æé æ¸¬ DDI é¡å¥ï¼èç¼ºä¹è½å¤ å¢å¼·éäºé æ¸¬çå¯ä¿¡åº¦çè§£éæ§è¦è§£ãå¨éé å·¥ä½ä¸­ï¼æåå»ºè­°çº DDI é æ¸¬ç¢çèªç¶èªè¨è§£éï¼ä½¿æ¨¡åè½å¤ åææ­ç¤ºè¥æå­¸åè¥ç©ååå­¸æ©å¶ï¼ä¸¦é²è¡é æ¸¬ãçºæ­¤ï¼æåå¾ DDInter å DrugBank æ¶éäº DDI è§£éï¼ä¸¦éç¼äºåç¨®æ¨¡åé²è¡å»£æ³çå¯¦é©ååæãæåçæ¨¡åå¯ä»¥çºå·²ç¥è¥ç©ä¹éæªç¥ç DDI æä¾æºç¢ºçè§£éãæ¬æçº DDI é æ¸¬é åè²¢ç»äºæ°çå·¥å·ï¼ä¸¦çºé²ä¸æ­¥ç ç©¶ DDI é æ¸¬çè§£éçæå¥ å®äºå å¯¦çåºç¤ã

##### **Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**
2409.05486v1 by Camilo Thorne, Christian Druckenbrodt, Kinga Szarkowska, Deepika Goyal, Pranita Marajan, Vijay Somanath, Corey Harper, Mao Yan, Tony Scerri

The quality and capabilities of large language models cannot be currently
fully assessed with automated, benchmark evaluations. Instead, human
evaluations that expand on traditional qualitative techniques from natural
language generation literature are required. One recent best-practice consists
in using A/B-testing frameworks, which capture preferences of human evaluators
for specific models. In this paper we describe a human evaluation experiment
focused on the biomedical domain (health, biology, chemistry/pharmacology)
carried out at Elsevier. In it a large but not massive (8.8B parameter)
decoder-only foundational transformer trained on a relatively small (135B
tokens) but highly curated collection of Elsevier datasets is compared to
OpenAI's GPT-3.5-turbo and Meta's foundational 7B parameter Llama 2 model
against multiple criteria. Results indicate -- even if IRR scores were
generally low -- a preference towards GPT-3.5-turbo, and hence towards models
that possess conversational abilities, are very large and were trained on very
large datasets. But at the same time, indicate that for less massive models
training on smaller but well-curated training sets can potentially give rise to
viable alternatives in the biomedical domain.

æè¦ï¼å¤§åèªè¨æ¨¡åçåè³ªåè½åç®åç¡æ³ä½¿ç¨èªåååºæºè©ä¼°ä¾å®å¨è©ä¼°ãç¸åï¼éè¦ä½¿ç¨æ´å±èªç¶èªè¨çææç»ä¸­å³çµ±å®æ§æè¡çäººé¡è©ä¼°ãä¸åæè¿çæä½³å¯¦åæ¯ä½¿ç¨ A/B æ¸¬è©¦æ¡æ¶ï¼è©²æ¡æ¶ææ·åäººé¡è©ä¼°èå°ç¹å®æ¨¡åçåå¥½ãå¨æ¬æä¸­ï¼æåæè¿°äºå°æ³¨æ¼çç©é«å­¸é åï¼å¥åº·ãçç©å­¸ãåå­¸/è¥çå­¸ï¼çäººé¡è©ä¼°å¯¦é©ï¼è©²å¯¦é©å¨ Elsevier é²è¡ãå¶ä¸­ï¼ä¸åå¤§åä½ä¸¦éé¾å¤§ï¼8.8B åæ¸ï¼åè§£ç¢¼å¨åºç¤Transformerå¨ç¸å°è¼å°ï¼135B ä»¤çï¼ä½ç¶éé«åº¦ç­å±ç Elsevier è³æéä¸è¨ç·´ï¼è OpenAI ç GPT-3.5-turbo å Meta çåºç¤ 7B åæ¸ Llama 2 æ¨¡åé²è¡æ¯è¼ï¼éå°å¤åæ¨æºãçµæè¡¨æââå³ä½¿ IRR åæ¸æ®éè¼ä½ââåå¥½ GPT-3.5-turboï¼å æ­¤åå¥½å·åå°è©±è½åãéå¸¸é¾å¤§ä¸å¨éå¸¸é¾å¤§çè³æéä¸è¨ç·´çæ¨¡åãä½åæï¼è¡¨æå°æ¼è¼å°è¦æ¨¡çæ¨¡åï¼å¨è¼å°ä½ç¶éè¯å¥½ç­å±çè¨ç·´éä¸è¨ç·´ï¼æå¯è½å¨çç©é«å­¸é åç¢çå¯è¡çæ¿ä»£æ¹æ¡ã

##### **KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**
2409.05370v1 by Yingshu Li, Zhanyu Wang, Yunyi Liu, Lei Wang, Lingqiao Liu, Luping Zhou

Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

æè¦ï¼<paragraph>å©ç¨å¤§åèªè¨æ¨¡å (LLM) å¼·å¤§çåè½ï¼é²è¡æäºçæãéè¼¯æ¨çåå¸¸è­ç¥è­æ´åï¼æ¬ç ç©¶æ·±å¥æ¢è¨å©ç¨ LLM ä¾å¢å¼·èªååæ¾å°å ±åçæ (R2Gen)ãåç®¡ LLM ææè±å¯çç¥è­ï¼ä½è¦ææè§¸ç¼éäºå¤§åæ¨¡åä¸­èç¹å®ä»»åï¼å¦ R2Genï¼ç¸éçç¥è­ï¼æ¯ä¸åéè¦çç ç©¶ææ°ãæ¬ææåºäº KARGENï¼ä¸ååºæ¼ LLM çç¥è­å¢å¼·èªååæ¾å°å ±åçææ¡æ¶ãå©ç¨åçµç LLM ä¾çæå ±åï¼è©²æ¡æ¶æ´åäºä¸åç¥è­åè­ï¼ä»¥è§£é LLM ä¸­èè¸é¨ç¾çç¸éçç¥è­ï¼ä»¥å¢å¼·çæå ±åçè¨åºæç¨ãéæ¯ééå©ç¨ç¥è­åè­ä»¥è¨­è¨çæ¹å¼æåèç¾çç¸éçç¹å¾µä¾å¯¦ç¾çãç±æ¼æ¾å°å ±ååå«æ­£å¸¸åç¾çç¸éçç¼ç¾ï¼å æ­¤æåçåå½¢å¢å¼·ç¾çç¸éç¹å¾µèååå½±åç¹å¾µæ´åï¼å¼é¡§å©åæ¹é¢ãæåæ¢ç´¢äºå©ç¨®èåæ¹æ³ï¼ä»¥èªååªåæåºåé¸ææç¸éçç¹å¾µãèåçç¹å¾µç± LLM ä½¿ç¨ï¼ä»¥çæå°ç¾çæ´ææä¸åè³ªæ´é«çå ±åãæåçåæ³å¨ MIMIC-CXR å IU-Xray è³æéä¸å±ç¤ºäºæå¸æççµæã</paragraph>

##### **Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**
2409.07493v1 by Javad Hassannataj Joloudari, Mohammad Maftoun, Bahareh Nakisa, Roohallah Alizadehsani, Meisam Yadollahzadeh-Tabari

The Complex Emotion Recognition System (CERS) deciphers complex emotional
states by examining combinations of basic emotions expressed, their
interconnections, and the dynamic variations. Through the utilization of
advanced algorithms, CERS provides profound insights into emotional dynamics,
facilitating a nuanced understanding and customized responses. The attainment
of such a level of emotional recognition in machines necessitates the knowledge
distillation and the comprehension of novel concepts akin to human cognition.
The development of AI systems for discerning complex emotions poses a
substantial challenge with significant implications for affective computing.
Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task
due to the intricacies involved in capturing subtle emotions, necessitating
specialized methods for data collection and processing. Incorporating
physiological signals such as Electrocardiogram (ECG) and Electroencephalogram
(EEG) can notably enhance CERS by furnishing valuable insights into the user's
emotional state, enhancing the quality of datasets, and fortifying system
dependability. A comprehensive literature review was conducted in this study to
assess the efficacy of machine learning, deep learning, and meta-learning
approaches in both basic and complex emotion recognition utilizing EEG, ECG
signals, and facial expression datasets. The chosen research papers offer
perspectives on potential applications, clinical implications, and results of
CERSs, with the objective of promoting their acceptance and integration into
clinical decision-making processes. This study highlights research gaps and
challenges in understanding CERSs, encouraging further investigation by
relevant studies and organizations. Lastly, the significance of meta-learning
approaches in improving CERS performance and guiding future research endeavors
is underscored.

æè¦ï¼è¤éæç·è¾¨è­ç³»çµ± (CERS) ééæª¢é©è¡¨éçåºæ¬æç·çµåãå®åçç¸äºé£çµï¼ä»¥ååæè®åä¾è§£ç¢¼è¤éçæç·çæãééä½¿ç¨é²éæ¼ç®æ³ï¼CERS æä¾äºå°æç·åæçæ·±å¥è¦è§£ï¼ä¿é²ç´°ç·»ççè§£åå®¢è£½åçåæãå¨æ©å¨ä¸­éæéç¨®ç¨åº¦çæç·è¾¨è­éè¦ç¥è­æçåçè§£é¡ä¼¼æ¼äººé¡èªç¥çæ°æ¦å¿µãç¼å±ç¨æ¼è¾¨å¥è¤éæç·çäººå·¥æºæ§ç³»çµ±å°ææéç®ä¾èªªæ¯ä¸åéå¤§çææ°ï¼ä¸¦å·æéè¦çå½±é¿ãæ­¤å¤ï¼ç±æ¼ææå¾®å¦æç·ææ¶åçè¤éæ§ï¼åå¾ CERS çå¤§éè³æéè¢«è­ææ¯ä¸é è±éçä»»åï¼å æ­¤éè¦æ¡ç¨ç¹æ®çæ¹æ³ä¾æ¶éåèçè³æãç´å¥ççè¨èï¼ä¾å¦å¿é»å (ECG) åè¦é»å (EEG)ï¼å¯ä»¥ééæä¾å°ä½¿ç¨èæç·çæçå¯¶è²´è¦è§£ãæåè³æéçåè³ªä»¥åå¼·åç³»çµ±çå¯é æ§ï¼ä¾é¡¯èå¢å¼· CERSãæ¬ç ç©¶é²è¡äºä¸é å¨é¢çæç»æ¢è¨ï¼ä»¥è©ä¼°æ©å¨å­¸ç¿ãæ·±åº¦å­¸ç¿ååå­¸ç¿æ¹æ³å¨å©ç¨è¦é»åãå¿é»åè¨èåé¢é¨è¡¨æè³æéé²è¡åºæ¬åè¤éæç·è¾¨è­æ¹é¢çæè½ãæé¸çç ç©¶è«ææä¾äºéæ¼ CERS çæ½å¨æç¨ãè¨åºå½±é¿åçµæçè§é»ï¼ç®çæ¯ä¿é²å®åè¢«æ¥åä¸¦æ´åå°è¨åºæ±ºç­å¶å®éç¨ä¸­ãæ¬ç ç©¶çªåºäºçè§£ CERS çç ç©¶å·®è·åææ°ï¼é¼åµç¸éç ç©¶åçµç¹é²ä¸æ­¥èª¿æ¥ãæå¾ï¼å¼·èª¿äºåå­¸ç¿æ¹æ³å¨æ¹å CERS æè½åæå°æªä¾ç ç©¶å·¥ä½ä¸­çéè¦æ§ã

##### **Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**
2409.05292v2 by Nirmalya Thakur

The world is currently experiencing an outbreak of mpox, which has been
declared a Public Health Emergency of International Concern by WHO. No prior
work related to social media mining has focused on the development of a dataset
of Instagram posts about the mpox outbreak. The work presented in this paper
aims to address this research gap and makes two scientific contributions to
this field. First, it presents a multilingual dataset of 60,127 Instagram posts
about mpox, published between July 23, 2022, and September 5, 2024. The
dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram
posts about mpox in 52 languages. For each of these posts, the Post ID, Post
Description, Date of publication, language, and translated version of the post
(translation to English was performed using the Google Translate API) are
presented as separate attributes in the dataset. After developing this dataset,
sentiment analysis, hate speech detection, and anxiety or stress detection were
performed. This process included classifying each post into (i) one of the
sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or
neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no
anxiety/stress detected. These results are presented as separate attributes in
the dataset. Second, this paper presents the results of performing sentiment
analysis, hate speech analysis, and anxiety or stress analysis. The variation
of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and
neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and
50.64%, respectively. In terms of hate speech detection, 95.75% of the posts
did not contain hate and the remaining 4.25% of the posts contained hate.
Finally, 72.05% of the posts did not indicate any anxiety/stress, and the
remaining 27.95% of the posts represented some form of anxiety/stress.

æè¦ï¼ä¸çç®åæ­£å¨ç»åç´çç«æï¼ä¸çå«çç»ç»å·²å®£å¸ç´çç«æä¸ºå½éå³æ³¨ççªåå¬å±å«çäºä»¶ãæ­¤åæ²¡æä¸ç¤¾äº¤åªä½ææç¸å³çç ç©¶éä¸­äºå¼åæå³ç´çç«æç Instagram å¸å­çæ°æ®éãæ¬æä»ç»çç ç©¶æ¨å¨è§£å³è¿ä¸ç ç©¶ç©ºç½ï¼å¹¶å¯¹è¯¥é¢åååºä¸¤é¡¹ç§å­¦è´¡ç®ãé¦åï¼å®æä¾äº 60,127 æ¡æå³ç´çç Instagram å¸å­çå¤è¯­è¨æ°æ®éï¼è¿äºå¸å­åå¸äº 2022 å¹´ 7 æ 23 æ¥è³ 2024 å¹´ 9 æ 5 æ¥ä¹é´ãè¯¥æ°æ®éå¯å¨ https://dx.doi.org/10.21227/7fvc-y093 å¤è·å¾ï¼å¶ä¸­åå« 52 ç§è¯­è¨çæå³ç´çç Instagram å¸å­ãå¯¹äºå¶ä¸­æ¯ç¯å¸å­ï¼å¸å­ IDãå¸å­æè¿°ãåå¸æ¥æãè¯­è¨åå¸å­çç¿»è¯çæ¬ï¼ä½¿ç¨ Google ç¿»è¯ API ç¿»è¯æè±æï¼ä½ä¸ºåç¬çå±æ§æ¾ç¤ºå¨æ°æ®éä¸­ãå¨å¼åæ­¤æ°æ®éåï¼è¿è¡äºææåæãä»æ¨è¨è®ºæ£æµä»¥åç¦èæååæ£æµãæ­¤è¿ç¨åæ¬å°æ¯ç¯å¸å­åç±»ä¸º (i) ææç±»å«ä¹ä¸ï¼å³ææ§ãæè®¶ãå¿«ä¹ãæ²ä¼¤ãæ¤æãåæ¶æä¸­ç«ï¼(ii) ä»æ¨æéä»æ¨ï¼ä»¥å (iii) æ£æµå°ç¦è/ååææªæ£æµå°ç¦è/ååãè¿äºç»æä½ä¸ºåç¬çå±æ§æ¾ç¤ºå¨æ°æ®éä¸­ãå¶æ¬¡ï¼æ¬æä»ç»äºæ§è¡ææåæãä»æ¨è¨è®ºåæåç¦èæåååæçç»æãè§å¯å°ææç±»å«çååââææ§ãæè®¶ãå¿«ä¹ãæ²ä¼¤ãæ¤æãåæ¶åä¸­ç«åå«ä¸º 27.95%ã2.57%ã8.69%ã5.94%ã2.69%ã1.53% å 50.64%ãå¨ä»æ¨è¨è®ºæ£æµæ¹é¢ï¼95.75% çå¸å­ä¸åå«ä»æ¨ï¼å¶ä½ 4.25% çå¸å­åå«ä»æ¨ãæåï¼72.05% çå¸å­æ²¡æè¡¨ç°åºä»»ä½ç¦è/ååï¼å¶ä½ 27.95% çå¸å­ä»£è¡¨æç§å½¢å¼çç¦è/ååã

##### **RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**
2409.05280v1 by Quoc-Bao Nguyen-Le, Tuan-Hy Le, Anh-Triet Do, Quoc-Huy Trinh

Cardiovascular disease is a major global health concern, contributing
significantly to global mortality. Accurately segmenting cardiac medical
imaging data is crucial for reducing fatality rates associated with these
conditions. However, current state-of-the-art (SOTA) neural networks, including
CNN-based and Transformer-based approaches, face challenges in capturing both
inter-slice connections and intra-slice details, especially in datasets
featuring intricate, long-range details along the z-axis like coronary
arteries. Existing methods also struggle with differentiating non-cardiac
components from the myocardium, resulting in segmentation inaccuracies and the
"spraying" phenomenon. To address these issues, we introduce
RotCAtt-TransUNet++, a novel architecture designed for robust segmentation of
intricate cardiac structures. Our approach enhances global context modeling
through multiscale feature aggregation and nested skip connections in the
encoder. Transformer layers facilitate capturing intra-slice interactions,
while a rotatory attention mechanism handles inter-slice connectivity. A
channel-wise cross-attention gate integrates multiscale information and decoder
features, effectively bridging semantic gaps. Experimental results across
multiple datasets demonstrate superior performance over current methods,
achieving near-perfect annotation of coronary arteries and myocardium. Ablation
studies confirm that our rotatory attention mechanism significantly improves
segmentation accuracy by transforming embedded vectorized patches in semantic
dimensional space.

æè¦ï¼å¿è¡ç®¡ç¾çæ¯å¨çä¸»è¦çå¥åº·åé¡ï¼å°å¨çæ­»äº¡çæé¡¯èçå½±é¿ãæºç¢ºåå²å¿èé«å­¸å½±åè³æå°æ¼éä½éäºç¾çç¸éçæ­»äº¡çè³ééè¦ãç¶èï¼ç®åçåé²ç¥ç¶ç¶²è·¯ï¼åæ¬åºæ¼ CNN ååºæ¼ Transformer çæ¹æ³ï¼å¨æ·åå±¤éé£æ¥åå±¤å§ç´°ç¯æ¹é¢é¢è¨ææ°ï¼ç¹å¥æ¯å¨å·ææ²¿è z è»¸çè¤éãé·ç¨ç´°ç¯çè³æéï¼ä¾å¦å çåèãç¾ææ¹æ³ä¹é£ä»¥ååéå¿èæååå¿èï¼å°è´åå²ä¸æºç¢ºåãå´çãç¾è±¡ãçºäºè§£æ±ºéäºåé¡ï¼æåå¼å¥äº RotCAtt-TransUNet++ï¼ä¸ç¨®å°çºè¤éå¿èçµæ§çç©©å¥åå²èè¨­è¨çæ°ç©æ¶æ§ãæåçåæ³ééç·¨ç¢¼å¨ä¸­çå¤å°ºåº¦ç¹å¾µèååå·¢çè·³èºé£æ¥å¢å¼·äºå¨å±èæ¯å»ºæ¨¡ãTransformer å±¤ä¿é²æ·åå±¤å§äº¤äºä½ç¨ï¼èæè½æ³¨ææ©å¶åèçå±¤éé£æ¥ãééå¼äº¤åæ³¨æééæ´åäºå¤å°ºåº¦è³è¨åè§£ç¢¼å¨ç¹å¾µï¼ææå°å½åäºèªç¾©å·®è·ãè·¨å¤åè³æéçå¯¦é©çµæè­æäºå¶åªæ¼ç®åæ¹æ³çæè½ï¼å¯¦ç¾äºå çåèåå¿èçè¿ä¹å®ç¾çè¨»è§£ãæ¶èç ç©¶è­å¯¦ï¼æåçæè½æ³¨ææ©å¶ééè½æèªç¾©ç¶­åº¦ç©ºéä¸­çåµå¥åéåè£ä¸ï¼é¡¯èå°æé«äºåå²æºç¢ºåº¦ã

##### **Activation Function Optimization Scheme for Image Classification**
2409.04915v1 by Abdur Rahman, Lu He, Haifeng Wang

Activation function has a significant impact on the dynamics, convergence,
and performance of deep neural networks. The search for a consistent and
high-performing activation function has always been a pursuit during deep
learning model development. Existing state-of-the-art activation functions are
manually designed with human expertise except for Swish. Swish was developed
using a reinforcement learning-based search strategy. In this study, we propose
an evolutionary approach for optimizing activation functions specifically for
image classification tasks, aiming to discover functions that outperform
current state-of-the-art options. Through this optimization framework, we
obtain a series of high-performing activation functions denoted as Exponential
Error Linear Unit (EELU). The developed activation functions are evaluated for
image classification tasks from two perspectives: (1) five state-of-the-art
neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and
Compact Convolutional Transformer which cover computationally heavy to light
neural networks, and (2) eight standard datasets, including CIFAR10,
Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15,
and TinyImageNet which cover from typical machine vision benchmark,
agricultural image applications to medical image applications. Finally, we
statistically investigate the generalization of the resultant activation
functions developed through the optimization scheme. With a Friedman test, we
conclude that the optimization scheme is able to generate activation functions
that outperform the existing standard ones in 92.8% cases among 28 different
cases studied, and $-x\cdot erf(e^{-x})$ is found to be the best activation
function for image classification generated by the optimization scheme.

æè¦ï¼<paragraph>æ¿æ´»å½æ¸å°æ·±åº¦ç¥ç¶ç¶²è·¯çåæãæ¶æåæè½æé¡¯èçå½±é¿ãå¨æ·±åº¦å­¸ç¿æ¨¡åéç¼éç¨ä¸­ï¼ä¸ç´è´åæ¼å°æ¾ä¸è´ä¸æè½é«çæ¿æ´»å½æ¸ãç¾æçæåé²æ¿æ´»å½æ¸ï¼é¤äº Swish ä¹å¤ï¼é½æ¯ç±äººé¡å°å®¶æåè¨­è¨çãSwish æ¯ä½¿ç¨åºæ¼å¼·åå­¸ç¿çæå°ç­ç¥éç¼çãå¨æ¬ç ç©¶ä¸­ï¼æåæåºäºä¸ç¨®æ¼åæ¹æ³ï¼å°ééå°åååé¡ä»»åæä½³åæ¿æ´»å½æ¸ï¼æ¨å¨ç¼ç¾æè½åªæ¼ç¾ææåé²é¸é çå½æ¸ãéééåæä½³åæ¶æ§ï¼æåç²å¾äºä¸ç³»åæè½é«çæ¿æ´»å½æ¸ï¼è¡¨ç¤ºçºææ¸èª¤å·®ç·æ§å®å (EELU)ãå·²éå°å©åè§é»è©ä¼°å·²éç¼çæ¿æ´»å½æ¸ï¼ç¨æ¼åååé¡ä»»åï¼(1) äºç¨®æåé²çç¥ç¶ç¶²è·¯æ¶æ§ï¼ä¾å¦ ResNet50ãAlexNetãVGG16ãMobileNet å Compact Convolutional Transformerï¼æ¶µèå¾è¨ç®ééçå°è¼éçç¶²è·¯ï¼(2) å«åæ¨æºè³æéï¼åæ¬ CIFAR10ãImagenetteãMNISTãFashion MNISTãBeansãColorectal HistologyãCottonWeedID15 å TinyImageNetï¼æ¶µèå¾å¸åçæ©å¨è¦è¦ºåºæºãè¾²æ¥­å½±åæç¨å°é«å­¸å½±åæç¨ãæå¾ï¼æåçµ±è¨èª¿æ¥äºééæä½³åæ¹æ¡éç¼ççµææ¿æ´»å½æ¸çæ¦åãéé Friedman æª¢å®ï¼æåå¾åºçµè«ï¼æä½³åæ¹æ¡è½å¤ ç¢çå¨ 28 åä¸åçç ç©¶æ¡ä¾ä¸­ï¼æ 92.8% çæ¡ä¾æè½åªæ¼ç¾ææ¨æºå½æ¸ï¼ä¸¦ä¸ç¼ç¾ $-x\cdot erf(e^{-x})$ æ¯æä½³åæ¹æ¡ç¢ççæä½³å½±ååé¡æ¿æ´»å½æ¸ã</paragraph>

##### **LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**
2409.04744v1 by Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Wei Chu, Yinghui Xu

The uncertainty inherent in the environmental transition model of
Reinforcement Learning (RL) necessitates a careful balance between exploration
and exploitation to optimize the use of computational resources for accurately
estimating an agent's expected reward. Achieving balance in control systems is
particularly challenging in scenarios with sparse rewards. However, given the
extensive prior knowledge available for many environments, it is redundant to
begin learning from scratch in such settings. To address this, we introduce
\textbf{L}anguage \textbf{M}odel \textbf{G}uided \textbf{T}rade-offs (i.e.,
\textbf{LMGT}), a novel, sample-efficient framework that leverages the
comprehensive prior knowledge embedded in Large Language Models (LLMs) and
their adeptness at processing non-standard data forms, such as wiki tutorials.
LMGT proficiently manages the exploration-exploitation trade-off by employing
reward shifts guided by LLMs, which direct agents' exploration endeavors,
thereby improving sample efficiency. We have thoroughly tested LMGT across
various RL tasks and deployed it in industrial-grade RL recommendation systems,
where it consistently outperforms baseline methods. The results indicate that
our framework can significantly reduce the time cost required during the
training phase in RL.

æè¦ï¼å¨å¼·åå­¸ç¿ï¼RLï¼çç°å¢è½ææ¨¡åä¸­ï¼åºæçä¸ç¢ºå®æ§éè¦å¨æ¢ç´¢åå©ç¨ä¹éåå¾ä»ç´°çå¹³è¡¡ï¼ä»¥æä½³åè¨ç®è³æºçä½¿ç¨ï¼ä»¥ç²¾æºä¼°è¨ä»£çé æççåµãå¨æ§å¶ç³»çµ±ä¸­åå¾å¹³è¡¡å¨çåµç¨ççææ³ä¸ç¹å¥å·æææ°æ§ãç¶èï¼ç±æ¼è¨±å¤ç°å¢é½æå»£æ³çåé©ç¥è­ï¼å æ­¤å¨éç¨®è¨­å®ä¸­å¾é ­éå§å­¸ç¿æ¯å¤é¤çãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº**L**anguage **M**odel **G**uided **T**rade-offsï¼å³**LMGT**ï¼ï¼éæ¯ä¸åæ°ç©ä¸æ¨£æ¬æçé«çæ¶æ§ï¼å®å©ç¨äºå¤§åèªè¨æ¨¡åï¼LLMï¼ä¸­åµå¥çå¨é¢åé©ç¥è­ï¼ä»¥åå®åèçéæ¨æºæ¸æå½¢å¼ï¼ä¾å¦ wiki æç¨ï¼çéæ´»æ§ãLMGT ééæ¡ç¨ç± LLM å¼å°ççåµè½ç§»ä¾çç·´å°ç®¡çæ¢ç´¢-å©ç¨æ¬è¡¡ï¼æå°ä»£ççæ¢ç´¢å·¥ä½ï¼å¾èæé«æ¨£æ¬æçãæåå·²ç¶å¾¹åºæ¸¬è©¦äº LMGT å¨åç¨® RL ä»»åä¸­çè¡¨ç¾ï¼ä¸¦å°å¶é¨ç½²å¨å·¥æ¥­ç´ RL æ¨è¦ç³»çµ±ä¸­ï¼å¨éäºç³»çµ±ä¸­ï¼å®å§çµåªæ¼åºç·æ¹æ³ãçµæè¡¨æï¼æåçæ¶æ§å¯ä»¥é¡¯èæ¸å° RL è¨ç·´éæ®µæéçæéææ¬ã

##### **NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**
2409.04723v1 by Debaditya Shome, Nasim Montazeri Ghahjaverestan, Ali Etemad

Sleep is known to be a key factor in emotional regulation and overall mental
health. In this study, we explore the integration of sleep measures from the
previous night into wearable-based mood recognition. To this end, we propose
NapTune, a novel prompt-tuning framework that utilizes sleep-related measures
as additional inputs to a frozen pre-trained wearable time-series encoder by
adding and training lightweight prompt parameters to each Transformer layer.
Through rigorous empirical evaluation, we demonstrate that the inclusion of
sleep data using NapTune not only improves mood recognition performance across
different wearable time-series namely ECG, PPG, and EDA, but also makes it more
sample-efficient. Our method demonstrates significant improvements over the
best baselines and unimodal variants. Furthermore, we analyze the impact of
adding sleep-related measures on recognizing different moods as well as the
influence of individual sleep-related measures.

æè¦ï¼ç¡ç å·²ç¥æ¯æç·èª¿ç¯åæ´é«å¿çå¥åº·ä¸­çééµå ç´ ãå¨æ¬ç ç©¶ä¸­ï¼æåæ¢è¨å°åä¸æçç¡ç æ¸¬éæ´åå°å¯ç©¿æ´å¼æç·è¾¨è­ä¸­ãçºæ­¤ï¼æåæåºäº NapTuneï¼éæ¯ä¸åæ°ç©çæç¤ºèª¿æ´æ¡æ¶ï¼å®å©ç¨èç¡ç ç¸éçæ¸¬éä½çºåçµé è¨ç·´å¯ç©¿æ´æéåºåç·¨ç¢¼å¨çéå è¼¸å¥ï¼æ¹æ³æ¯å°è¼éç´æç¤ºåæ¸æ°å¢ä¸¦è¨ç·´å°æ¯å Transformer å±¤ãééå´è¬¹çç¶é©è©ä¼°ï¼æåè­æä½¿ç¨ NapTune ç´å¥ç¡ç æ¸æä¸åæ¹åäºä¸åå¯ç©¿æ´æéåºåï¼å³å¿é»åãåé»å®¹ç©æè¨åç®é»æ´»åï¼çæç·è¾¨è­æè½ï¼éè®å®æ´å·æ¨£æ¬æçãæåçæ¨¡åè­æäºç¸è¼æ¼æä½³åºç·åå®æ¨¡æè®ç°ï¼æé¡¯èçæ¹åãæ­¤å¤ï¼æååæäºæ°å¢èç¡ç ç¸éçæ¸¬éå°è¾¨è­ä¸åæç·çå½±é¿ï¼ä»¥ååå¥èç¡ç ç¸éçæ¸¬éçå½±é¿ã

##### **A Comprehensive Survey on Evidential Deep Learning and Its Applications**
2409.04720v1 by Junyu Gao, Mengyuan Chen, Liangyu Xiang, Changsheng Xu

Reliable uncertainty estimation has become a crucial requirement for the
industrial deployment of deep learning algorithms, particularly in high-risk
applications such as autonomous driving and medical diagnosis. However,
mainstream uncertainty estimation methods, based on deep ensembling or Bayesian
neural networks, generally impose substantial computational overhead. To
address this challenge, a novel paradigm called Evidential Deep Learning (EDL)
has emerged, providing reliable uncertainty estimation with minimal additional
computation in a single forward pass. This survey provides a comprehensive
overview of the current research on EDL, designed to offer readers a broad
introduction to the field without assuming prior knowledge. Specifically, we
first delve into the theoretical foundation of EDL, the subjective logic
theory, and discuss its distinctions from other uncertainty estimation
frameworks. We further present existing theoretical advancements in EDL from
four perspectives: reformulating the evidence collection process, improving
uncertainty estimation via OOD samples, delving into various training
strategies, and evidential regression networks. Thereafter, we elaborate on its
extensive applications across various machine learning paradigms and downstream
tasks. In the end, an outlook on future directions for better performances and
broader adoption of EDL is provided, highlighting potential research avenues.

æè¦ï¼å¯é çä¸ç¢ºå®æ§ä¼°è¨å·²æçºæ·±åº¦å­¸ç¿æ¼ç®æ³ç¢æ¥­é¨ç½²çééµéæ±ï¼ç¹å¥æ¯å¨é«é¢¨éªæç¨ä¸­ï¼ä¾å¦èªåé§é§åé«çè¨ºæ·ãç¶èï¼åºæ¼æ·±åº¦éææè²æ°ç¥ç¶ç¶²è·¯çä¸»æµä¸ç¢ºå®æ§ä¼°è¨æ¹æ³éå¸¸æé æå¤§éçè¨ç®è² æãçºäºæå°éé ææ°ï¼ä¸ç¨®ç¨±çºè­ææ·±åº¦å­¸ç¿ (EDL) çæ°ç¯ä¾æéèçï¼å®å¨å®æ¬¡ååå³éä¸­ä»¥æå°çé¡å¤éç®æä¾å¯é çä¸ç¢ºå®æ§ä¼°è¨ãéé èª¿æ¥å° EDL çç¾æç ç©¶æä¾å¨é¢çæ¦è¿°ï¼æ¨å¨çºè®èæä¾è©²é åçå»£æ³ä»ç´¹ï¼èç¡éåè¨­ååç¥è­ãå·é«ä¾èªªï¼æåé¦åæ·±å¥æ¢è¨ EDL ççè«åºç¤ï¼å³ä¸»è§éè¼¯çè«ï¼ä¸¦è¨è«å¶èå¶ä»ä¸ç¢ºå®æ§ä¼°è¨æ¶æ§çåå¥ãæåé²ä¸æ­¥å¾ååè§åº¦ä»ç´¹ EDL ä¸­ç¾æççè«é²å±ï¼éæ°å¶å®è­ææ¶ééç¨ãéé OOD æ¨£æ¬æ¹åä¸ç¢ºå®æ§ä¼°è¨ãæ·±å¥æ¢è¨åç¨®è¨ç·´ç­ç¥ä»¥åè­æåæ­¸ç¶²è·¯ãæ­¤å¾ï¼æåè©³ç´°èªªæå®å¨åç¨®æ©å¨å­¸ç¿ç¯ä¾åä¸æ¸¸ä»»åä¸­çå»£æ³æç¨ãæå¾ï¼æåæä¾äºå°æªä¾æ¹åçå±æï¼ä»¥æç²å¾æ´å¥½çæè½åæ´å»£æ³å°æ¡ç¨ EDLï¼ä¸¦éé»ä»ç´¹æ½å¨çç ç©¶éå¾ã

##### **A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**
2409.04704v1 by Cheng Wan, Chenjie Xie, Longfei Liu, Dan Wu, Ye Li

Continuous blood pressure (BP) monitoring is essential for timely diagnosis
and intervention in critical care settings. However, BP varies significantly
across individuals, this inter-patient variability motivates the development of
personalized models tailored to each patient's physiology. In this work, we
propose a personalized BP forecasting model mainly using electrocardiogram
(ECG) and photoplethysmogram (PPG) signals. This time-series model incorporates
2D representation learning to capture complex physiological relationships.
Experiments are conducted on datasets collected from three diverse scenarios
with BP measurements from 60 subjects total. Results demonstrate that the model
achieves accurate and robust BP forecasts across scenarios within the
Association for the Advancement of Medical Instrumentation (AAMI) standard
criteria. This reliable early detection of abnormal fluctuations in BP is
crucial for at-risk patients undergoing surgery or intensive care. The proposed
model provides a valuable addition for continuous BP tracking to reduce
mortality and improve prognosis.

æè¦ï¼æçºçè¡å£ (BP) ç£æ§å°æ¼éçç£è­·ç°å¢ä¸­çåæè¨ºæ·åå¹²é è³ééè¦ãç¶èï¼BP å äººèç°ï¼éç¨®æ£èéè®ç°æ§ä¿ä½¿éç¼éå°æ¯ä½æ£èçççæ³éèº«æé çåäººåæ¨¡åãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸ç¨®åäººå BP é æ¸¬æ¨¡åï¼ä¸»è¦ä½¿ç¨å¿é»å (ECG) ååé»å®¹ç©æè¨æ³ (PPG) ä¿¡èãæ­¤æéåºåæ¨¡åçµåäº 2D è¡¨å¾µå­¸ç¿ä»¥ææè¤éçççéä¿ãå¯¦é©æ¯å¨å¾ä¸ç¨®ä¸åæå¢æ¶éçè³æéä¸é²è¡ï¼ç¸½å±ä¾èª 60 ä½åè©¦èç BP æ¸¬éãçµæè¡¨æï¼è©²æ¨¡åå¨é«å­¸åå¨ä¿é²åæ (AAMI) æ¨æºæ¨æºå§å¯¦ç¾äºè·¨æå¢çæºç¢ºä¸ç©©å¥ç BP é æ¸¬ãå°æ¼æ¥åæè¡æéçç£è­·çé«é¢¨éªæ£èèè¨ï¼éç¨®å° BP ç°å¸¸æ³¢åçå¯é æ©ææª¢æ¸¬è³ééè¦ãææåºçæ¨¡åçºæçº BP è¿½è¹¤æä¾äºæå¹å¼çè£åï¼ä»¥éä½æ­»äº¡çä¸¦æ¹åé å¾ã

##### **The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**
2409.04368v1 by Gregory Szumel, Brian Guo, Darui Lu, Rongze Gui, Tingyu Wang, Nicholas Konz, Maciej A. Mazurowski

Purpose: Medical images acquired using different scanners and protocols can
differ substantially in their appearance. This phenomenon, scanner domain
shift, can result in a drop in the performance of deep neural networks which
are trained on data acquired by one scanner and tested on another. This
significant practical issue is well-acknowledged, however, no systematic study
of the issue is available across different modalities and diagnostic tasks.
Materials and Methods: In this paper, we present a broad experimental study
evaluating the impact of scanner domain shift on convolutional neural network
performance for different automated diagnostic tasks. We evaluate this
phenomenon in common radiological modalities, including X-ray, CT, and MRI.
Results: We find that network performance on data from a different scanner is
almost always worse than on same-scanner data, and we quantify the degree of
performance drop across different datasets. Notably, we find that this drop is
most severe for MRI, moderate for X-ray, and quite small for CT, on average,
which we attribute to the standardized nature of CT acquisition systems which
is not present in MRI or X-ray. We also study how injecting varying amounts of
target domain data into the training set, as well as adding noise to the
training data, helps with generalization. Conclusion: Our results provide
extensive experimental evidence and quantification of the extent of performance
drop caused by scanner domain shift in deep learning across different
modalities, with the goal of guiding the future development of robust deep
learning models for medical image analysis.

æè¦ï¼<paragraph>ç®çï¼ä½¿ç¨ä¸åææåååå®åå¾çé«å­¸å½±åï¼å¨å½±åå¤è§ä¸å¯è½ææé¡¯èå·®ç°ãéç¨®ç¾è±¡ç¨±çºææåé ååç§»ï¼å¯è½æå°è´æ·±åº¦ç¥ç¶ç¶²è·¯çæè½ä¸éï¼èéäºç¶²è·¯æ¯éå°ç±ä¸ç¨®ææååå¾çè³æé²è¡è¨ç·´ï¼ä¸¦å¨å¦ä¸ç¨®ææåä¸é²è¡æ¸¬è©¦ãéåéè¦çå¯¦éåé¡å·²ç²å¾å»£æ³èªå¯ï¼ä½ç®åå°æªéå°ä¸åå½¢å¼åè¨ºæ·ä»»åé²è¡ç³»çµ±æ§ç ç©¶ãææåæ¹æ³ï¼å¨æ¬æä¸­ï¼æåæåºäºä¸é å»£æ³çå¯¦é©ç ç©¶ï¼è©ä¼°ææåé ååç§»å°ä¸åèªååè¨ºæ·ä»»åçå·ç©ç¥ç¶ç¶²è·¯æè½çå½±é¿ãæåå¨å¸¸è¦çæ¾å°å­¸å½¢å¼ä¸­è©ä¼°éç¨®ç¾è±¡ï¼åæ¬ X åãé»è¦æ·å±¤ææåç£æ¯é å½±ãçµæï¼æåç¼ç¾ï¼ä¾èªä¸åææåçè³æå¨ç¶²è·¯ä¸çæè½å¹¾ä¹ç¸½æ¯æ¯ä¾èªç¸åææåçè³æå·®ï¼æåéåäºä¸åè³æéæè½ä¸éçç¨åº¦ãå¼å¾æ³¨æçæ¯ï¼æåç¼ç¾éç¨®ä¸éå¨ç£æ¯é å½±ä¸­æä¸ºå´éï¼å¨ X åä¸­çºä¸­ç­ï¼å¨é»è¦æ·å±¤ææä¸­ç¸ç¶å°ï¼å¹³åèè¨ï¼æåå°å¶æ­¸å æ¼é»è¦æ·å±¤ææåå¾ç³»çµ±çæ¨æºåæ§è³ªï¼èç£æ¯é å½±æ X åä¸­ä¸å­å¨éç¨®æ§è³ªãæåéç ç©¶äºå°ä¸åæ¸éçç®æ¨é åè³ææ³¨å¥è¨ç·´éï¼ä»¥ååè¨ç·´è³æå å¥éè¨ï¼å¦ä½æå©æ¼æ³åãçµè«ï¼æåççµææä¾äºå»£æ³çå¯¦é©è­æï¼ä¸¦éåäºæ·±åº¦å­¸ç¿ä¸­ç±ææåé ååç§»é æçæè½ä¸éç¨åº¦ï¼ç®æ¨æ¯å¼å°æªä¾éå°é«å­¸å½±ååæçå¼·å¥æ·±åº¦å­¸ç¿æ¨¡åçç¼å±ã</paragraph>

##### **CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**
2409.04290v1 by William Knottenbelt, Zeyu Gao, Rebecca Wray, Woody Zhidong Zhang, Jiashuai Liu, Mireia Crispin-Ortuzar

Survival analysis is a branch of statistics used for modeling the time until
a specific event occurs and is widely used in medicine, engineering, finance,
and many other fields. When choosing survival models, there is typically a
trade-off between performance and interpretability, where the highest
performance is achieved by black-box models based on deep learning. This is a
major problem in fields such as medicine where practitioners are reluctant to
blindly trust black-box models to make important patient decisions.
Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable
and accurate alternative to multi-layer perceptrons (MLPs). We introduce
CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,
high-performance survival analysis. We evaluate the proposed CoxKAN on 4
synthetic datasets and 9 real medical datasets. The synthetic experiments
demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for
the hazard function, and effectively performs automatic feature selection.
Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the
Cox proportional hazards model and achieves performance that is superior or
comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies
complex interactions between predictor variables that would be extremely
difficult to recognise using existing survival methods, and automatically finds
symbolic formulae which uncover the precise effect of important biomarkers on
patient risk.

æè¦ï¼çå­åææ¯çµ±è¨å­¸çä¸ååæ¯ï¼ç¨æ¼å»ºæ¨¡ç¹å®äºä»¶ç¼ççæéï¼ä¸¦å»£æ³ç¨æ¼é«å­¸ãå·¥ç¨ãéèåè¨±å¤å¶ä»é åãå¨é¸æçå­æ¨¡åæï¼éå¸¸å¨æ§è½åå¯è§£éæ§ä¹éé²è¡æ¬è¡¡ï¼å¶ä¸­æé«æ§è½æ¯ç±åºæ¼æ·±åº¦å­¸ç¿çé»çæ¨¡åå¯¦ç¾çãéå¨é«å­¸ç­é åæ¯ä¸åä¸»è¦åé¡ï¼å çºå¾æ¥­èä¸é¡æç²ç®ä¿¡ä»»é»çæ¨¡åä¾ååºéè¦çæ£èæ±ºç­ãKolmogorov-é¿è«¾å¾·ç¶²çµ¡ (KAN) æè¿è¢«æè­°ä½çºå¤å±¤æç¥å¨ (MLP) çå¯è§£éä¸æºç¢ºçæ¿ä»£æ¹æ¡ãæåå¼å¥äº CoxKANï¼éæ¯ä¸åç¨æ¼å¯è§£éãé«æ§è½çå­åæç Cox æ¯ä¾é¢¨éª Kolmogorov-Arnold ç¶²çµ¡ãæåå¨ 4 ååææ¸æéå 9 åçå¯¦é«çæ¸æéä¸è©ä¼°äºææåºç CoxKANãåæå¯¦é©è¡¨æï¼CoxKAN æºç¢ºå°æ¢å¾©äºé¢¨éªå½æ¸çå¯è§£éç¬¦èå¬å¼ï¼ä¸¦ææå°å·è¡èªåç¹å¾µé¸æãå° 9 åçå¯¦æ¸æéçè©ä¼°è¡¨æï¼CoxKAN å§çµåªæ¼ Cox æ¯ä¾é¢¨éªæ¨¡åï¼ä¸¦ä¸éå°äºåªæ¼æèèª¿æ´å¾ç MLP ç¸ç¶çæ§è½ãæ­¤å¤ï¼æåç¼ç¾ CoxKAN è­å¥äºé æ¸¬è®éä¹éçè¤éäº¤äºä½ç¨ï¼éäºäº¤äºä½ç¨ä½¿ç¨ç¾æççå­æ¹æ³æ¥µé£è­å¥ï¼ä¸¦èªåæ¾å°æ­ç¤ºéè¦çç©æ¨èªç©å°æ£èé¢¨éªçæºç¢ºå½±é¿çç¬¦èå¬å¼ã

##### **Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**
2409.04224v1 by Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng

Multi-organ diseases present significant challenges due to their simultaneous
impact on multiple organ systems, necessitating complex and adaptive treatment
strategies. Despite recent advancements in AI-powered healthcare decision
support systems, existing solutions are limited to individual organ systems.
They often ignore the intricate dependencies between organ system and thereby
fails to provide holistic treatment recommendations that are useful in
practice. We propose a novel hierarchical multi-agent reinforcement learning
(HMARL) framework to address these challenges. This framework uses dedicated
agents for each organ system, and model dynamic through explicit inter-agent
communication channels, enabling coordinated treatment strategies across
organs. Furthermore, we introduce a dual-layer state representation technique
to contextualize patient conditions at various hierarchical levels, enhancing
the treatment accuracy and relevance. Through extensive qualitative and
quantitative evaluations in managing sepsis (a complex multi-organ disease),
our approach demonstrates its ability to learn effective treatment policies
that significantly improve patient survival rates. This framework marks a
substantial advancement in clinical decision support systems, pioneering a
comprehensive approach for multi-organ treatment recommendations.

æè¦ï¼å¤å¨å®ç¾çç±æ¼åæå½±é¿å¤åå¨å®ç³»çµ±ï¼å æ­¤æå¸¶ä¾éå¤§çææ°ï¼éè¦è¤éä¸å·æé©ææ§çæ²»çç­ç¥ãåç®¡ AI é©åçé«çä¿å¥æ±ºç­æ¯æ´ç³»çµ±æè¿æé²å±ï¼ä½ç¾æè§£æ±ºæ¹æ¡åéæ¼åå¥å¨å®ç³»çµ±ãå®åå¸¸å¸¸å¿½ç¥å¨å®ç³»çµ±ä¹éçè¤éä¾è³´æ§ï¼å æ­¤ç¡æ³æä¾å¯¦åä¸æç¨çæ´é«æ²»çå»ºè­°ãæåæåºä¸åæ°ç©çåå±¤å¤æºè½é«å¼·åå­¸ç¿ (HMARL) æ¶æ§ä¾è§£æ±ºéäºææ°ãæ­¤æ¶æ§çºæ¯åå¨å®ç³»çµ±ä½¿ç¨å°ç¨æºè½é«ï¼ä¸¦ééæç¢ºçæºè½é«ééè¨ç®¡éå»ºæ¨¡åæï¼è®ä¸åå¨å®ä¹éçæ²»çç­ç¥è½å¤ åèª¿ãæ­¤å¤ï¼æåå¼å¥éå±¤çæè¡¨ç¤ºæè¡ï¼å¨åç¨®å±¤ç´èªå¢åçæ£çæ³ï¼ä»¥æåæ²»çæºç¢ºæ§åç¸éæ§ãééå¨æè¡çï¼ä¸ç¨®è¤éçå¤å¨å®ç¾çï¼ç®¡çä¸­é²è¡å»£æ³çå®æ§åå®éè©ä¼°ï¼æåçåæ³å±ç¤ºäºå®å­¸ç¿æææ²»çæ¿ç­çè½åï¼å¯é¡¯èæ¹åçæ£å­æ´»çãæ­¤æ¶æ§æ¨èªèè¨åºæ±ºç­æ¯æ´ç³»çµ±çä¸å¤§é²æ­¥ï¼éåµäºå¤å¨å®æ²»çå»ºè­°çå¨é¢æ§æ¹æ³ã

##### **Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**
2409.04481v1 by Yizhen Zheng, Huan Yee Koh, Maddie Yang, Li Li, Lauren T. May, Geoffrey I. Webb, Shirui Pan, George Church

The integration of Large Language Models (LLMs) into the drug discovery and
development field marks a significant paradigm shift, offering novel
methodologies for understanding disease mechanisms, facilitating drug
discovery, and optimizing clinical trial processes. This review highlights the
expanding role of LLMs in revolutionizing various stages of the drug
development pipeline. We investigate how these advanced computational models
can uncover target-disease linkage, interpret complex biomedical data, enhance
drug molecule design, predict drug efficacy and safety profiles, and facilitate
clinical trial processes. Our paper aims to provide a comprehensive overview
for researchers and practitioners in computational biology, pharmacology, and
AI4Science by offering insights into the potential transformative impact of
LLMs on drug discovery and development.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼æ´åå°è¥ç©ç¼ç¾åéç¼é åæ¨èªèéå¤§çå¸ç¯è½ç§»ï¼æä¾äºè§£ç¾çæ©å¶ãä¿é²è¥ç©ç¼ç¾ååªåè¨åºè©¦é©æµç¨çæ°æ¹æ³ãæ¬ç¶è¿°éé»ä»ç´¹äº LLM å¨é©æ°è¥ç©éç¼ç®¡ç·ååéæ®µä¸­æ¥çéè¦çä½ç¨ãæåæ¢è¨äºéäºåé²çè¨ç®æ¨¡åå¦ä½æ­ç¤ºé¶é»ç¾çéè¯æ§ãè§£éè¤éççç©é«å­¸æ¸æãå¢å¼·è¥ç©åå­è¨­è¨ãé æ¸¬è¥ç©çæåå®å¨æ§ï¼ä»¥åä¿é²è¨åºè©¦é©æµç¨ãæåçè«ææ¨å¨çºè¨ç®çç©å­¸ãè¥çå­¸å AI4Science çç ç©¶äººå¡åå¾æ¥­èæä¾å¨é¢çæ¦è¿°ï¼æ·±å¥äºè§£ LLM å°è¥ç©ç¼ç¾åéç¼çæ½å¨è®é©æ§å½±é¿ã

##### **FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**
2409.03947v1 by Kai Shu, Yuzhuo Jia, Ziyang Zhang, Jiechao Gao

Automatic Medical Imaging Narrative generation aims to alleviate the workload
of radiologists by producing accurate clinical descriptions directly from
radiological images. However, the subtle visual nuances and domain-specific
terminology in medical images pose significant challenges compared to generic
image captioning tasks. Existing approaches often neglect the vital distinction
between normal and abnormal findings, leading to suboptimal performance. In
this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive
Partitioning Graph framework that addresses these limitations through
domain-adaptive learning. FODA-PG constructs a granular graphical
representation of radiological findings by separating disease-related
attributes into distinct "disease-specific" and "disease-free" categories based
on their clinical significance and location. This adaptive partitioning enables
our model to capture the nuanced differences between normal and pathological
states, mitigating the impact of data biases. By integrating this fine-grained
semantic knowledge into a powerful transformer-based architecture and providing
rigorous mathematical justifications for its effectiveness, FODA-PG generates
precise and clinically coherent reports with enhanced generalization
capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks
demonstrate the superiority of our approach over state-of-the-art methods,
highlighting the importance of domain adaptation in medical report generation.

æè¦ï¼èªåé«å­¸å½±åæè¿°çææ¨å¨ééç´æ¥å¾æ¾å°å½±åç¢çç²¾ç¢ºçè¨åºæè¿°ï¼æ¸è¼æ¾å°ç§é«å¸«çå·¥ä½è² æãç¶èï¼èä¸è¬å½±åæ¨é¡ä»»åç¸æ¯ï¼é«å­¸å½±åä¸­çç´°å¾®è¦è¦ºå·®ç°åç¹å®é åè¡èªæå¸¶ä¾éå¤§ææ°ãç¾ææ¹æ³å¸¸å¸¸å¿½ç¥æ­£å¸¸èç°å¸¸ç¼ç¾ä¹éçéè¦åå¥ï¼å°è´æ¬¡ä½³æè½ãå¨éé å·¥ä½ä¸­ï¼æåæåº FODA-PGï¼éæ¯ä¸åæ°ç©çç´°ç²åº¦å¨å®ç¾çèªé©æåå²åå½¢æ¶æ§ï¼ééé åèªé©æå­¸ç¿ä¾è§£æ±ºéäºéå¶ãFODA-PG ééå°ç¾çç¸éå±¬æ§ä¾æå¶è¨åºéè¦æ§åä½ç½®åçºä¸åçãç¹å®ç¾çãåãç¡ç¾çãé¡å¥ï¼ä¾å»ºæ§æ¾å°å­¸ç¼ç¾çç´°ç²åº¦åå½¢è¡¨ç¤ºãéç¨®èªé©æåå²ä½¿æåçæ¨¡åè½å¤ æææ­£å¸¸èçççæä¹éçç´°å¾®å·®ç°ï¼æ¸è¼è³æåå·®çå½±é¿ãééå°éç¨®ç´°ç²åº¦èªç¾©ç¥è­æ´åå°å¼·å¤§çåºæ¼è½æå¨çæ¶æ§ä¸­ï¼ä¸¦æä¾å¶æææ§çå´è¬¹æ¸å­¸è­æï¼FODA-PG è½å¤ çæç²¾ç¢ºä¸è¨åºä¸é£è²«çå ±åï¼ä¸¦å·åå¢å¼·çæ¦æ¬è½åãå¨ IU-Xray å MIMIC-CXR åºæºä¸çå»£æ³å¯¦é©è­æäºæåçæ¹æ³åªæ¼æåé²çæ¹æ³ï¼çªé¡¯äºé åé©æå¨é«å­¸å ±åçæä¸­çéè¦æ§ã

##### **A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**
2409.03933v1 by Esther Lagemann, Julia Roeb, Steven L. Brunton, Christian Lagemann

The accurate quantification of wall-shear stress dynamics is of substantial
importance for various applications in fundamental and applied research,
spanning areas from human health to aircraft design and optimization. Despite
significant progress in experimental measurement techniques and post-processing
algorithms, temporally resolved wall-shear stress dynamics with adequate
spatial resolution and within a suitable spatial domain remain an elusive goal.
To address this gap, we introduce a deep learning architecture that ingests
wall-parallel velocity fields from the logarithmic layer of turbulent
wall-bounded flows and outputs the corresponding 2D wall-shear stress fields
with identical spatial resolution and domain size. From a physical perspective,
our framework acts as a surrogate model encapsulating the various mechanisms
through which highly energetic outer-layer flow structures influence the
governing wall-shear stress dynamics. The network is trained in a supervised
fashion on a unified dataset comprising direct numerical simulations of
statistically 1D turbulent channel and spatially developing turbulent boundary
layer flows at friction Reynolds numbers ranging from 390 to 1,500. We
demonstrate a zero-shot applicability to experimental velocity fields obtained
from Particle-Image Velocimetry measurements and verify the physical accuracy
of the wall-shear stress estimates with synchronized wall-shear stress
measurements using the Micro-Pillar Shear-Stress Sensor for Reynolds numbers up
to 2,000. In summary, the presented framework lays the groundwork for
extracting inaccessible experimental wall-shear stress information from readily
available velocity measurements and thus, facilitates advancements in a variety
of experimental applications.

æè¦ï¼<paragraph>æºç¢ºéåå£é¢åªæååæå°æ¼åºç¤åæç¨ç ç©¶ä¸­çåç¨®æç¨å·æå¯¦è³ªæ§çéè¦æ§ï¼æ¶µèå¾äººé¡å¥åº·å°é£æ©è¨­è¨ååªåçé åãåç®¡å¨å¯¦é©æ¸¬éæè¡åå¾èçæ¼ç®æ³æ¹é¢åå¾äºé¡¯èé²å±ï¼ä½æéè§£æå£é¢åªæååæä»å·æè¶³å¤ çç©ºéè§£æåº¦åå¨åé©çç©ºéåä¸­ä»ç¶æ¯ä¸åé£ä»¥ææ¸çç®æ¨ãçºäºè§£æ±ºéåå·®è·ï¼æåå¼å¥äºä¸åæ·±åº¦å­¸ç¿æ¶æ§ï¼å®å¾æ¹æµå£é¢ç´ææµçå°æ¸å±¤ä¸­æåå£é¢å¹³è¡éåº¦å ´ï¼ä¸¦è¼¸åºç¸æç 2D å£é¢åªæåå ´ï¼å·æç¸åçç©ºéè§£æåº¦ååå¤§å°ãå¾ç©çè§åº¦ä¾çï¼æåçæ¡æ¶åç¶ä¸åä»£çæ¨¡åï¼æ¦æ¬äºé«è½éå¤å±¤æµçµæ§å½±é¿æ§å¶å£é¢åªæååæçåç¨®æ©å¶ãè©²ç¶²è·¯ä»¥ç£ç£æ¹å¼å¨ä¸åçµ±ä¸çæ¸æéä¸é²è¡è¨ç·´ï¼è©²æ¸æéåå«çµ±è¨ 1D æ¹æµééçç´æ¥æ¸å¼æ¨¡æ¬åç©ºéç¼å±çæ¹æµéçå±¤æµï¼æ©æ¦é·è«¾æ¸ç¯åå¾ 390 å° 1,500ãæåå±ç¤ºäºå°å¾ç²å­å½±åæ¸¬éæ¸¬éä¸­ç²å¾çå¯¦é©éåº¦å ´çé¶æ¬¡æç¨ï¼ä¸¦ä½¿ç¨å¾®æ±åªæåææ¸¬å¨å°é·è«¾æ¸æé« 2,000 çåæ­¥å£é¢åªæåæ¸¬éé©è­äºå£é¢åªæåä¼°è¨çç©çæºç¢ºæ§ãç¸½ä¹ï¼ææåºçæ¡æ¶çºå¾å®¹æç²å¾çéåº¦æ¸¬éä¸­æåç¡æ³ç²å¾çå¯¦é©å£é¢åªæåè³è¨å¥ å®äºåºç¤ï¼å¾èä¿é²äºåç¨®å¯¦é©æç¨ä¸­çé²å±ã</paragraph>

##### **Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**
2409.03597v1 by Yucong Zhang, Xin Zou, Jinshan Yang, Wenjun Chen, Faya Liang, Ming Li

This paper presents the Multimodal Analyzing System for Laryngoscope (MASL),
a system that combines audio and video data to automatically extract key
segments and metrics from laryngeal videostroboscopic videos for clinical
assessment. MASL integrates glottis detection with keyword spotting to analyze
patient vocalizations and refine video highlights for better inspection of
vocal cord movements. The system includes a strobing video extraction module
that identifies frames by analyzing hue, saturation, and value fluctuations.
MASL also provides effective metrics for vocal cord paralysis detection,
employing a two-stage glottis segmentation process using U-Net followed by
diffusion-based refinement to reduce false positives. Instead of glottal area
waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis
masks, evaluating both left and right vocal cords to detect unilateral vocal
cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between
left and right paralysis. Ablation studies and experiments on public and
real-world datasets validate MASL's segmentation module and demonstrate its
ability to provide reliable metrics for UVFP diagnosis.

æè¦ï¼æ¬ææåºäºåéå¤æ¨¡æåæç³»ç» (MASL)ï¼
è¯¥ç³»ç»ç»åé³é¢åè§é¢æ°æ®ï¼èªå¨ä»åé¨è§é¢é¢éªéè§é¢ä¸­æåå³é®
çæ®µåææ ï¼ç¨äºä¸´åºè¯ä¼°ãMASL å°å£°é¨æ£æµä¸å³é®è¯è¯å«ç¸ç»åï¼ä»¥åæ
æ£èåå£°å¹¶ç»åè§é¢éç¹ï¼ä»¥ä¾¿æ´å¥½å°æ£æ¥å£°å¸¦è¿å¨ãè¯¥ç³»ç»åæ¬ä¸ä¸ªé¢éªè§é¢æåæ¨¡åï¼
è¯¥æ¨¡åéè¿åæè²ç¸ãé¥±ååº¦åå¼æ³¢å¨æ¥è¯å«å¸§ã
MASL è¿ä¸ºå£°å¸¦éº»ç¹æ£æµæä¾äºææçææ ï¼
éç¨ä¸¤é¶æ®µå£°é¨åå²è¿ç¨ï¼ä½¿ç¨ U-Netï¼ç¶åè¿è¡åºäºæ©æ£çç»åä»¥åå°è¯¯æ¥ãMASL ä¸ä½¿ç¨å£°é¨é¢ç§¯æ³¢å½¢ï¼èæ¯ä»å£°é¨æ©æ¨¡ä¸­ä¼°è®¡åå£°é¨è§æ³¢å½¢ (AGAW)ï¼è¯ä¼°å·¦å³å£°å¸¦ä»¥æ£æµåä¾§å£°å¸¦éº»ç¹ (UVFP)ãéè¿æ¯è¾ AGAW æ¹å·®ï¼MASL åºåå·¦å³éº»ç¹ãæ¶èç ç©¶åå¯¹å¬å±åçå®ä¸çæ°æ®éçå®éªéªè¯äº MASL çåå²æ¨¡åï¼å¹¶è¯æäºå¶æä¾å¯é ç UVFP è¯æ­ææ çè½åã

##### **Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**
2409.03470v1 by Prerak Mody, Nicolas F. Chaves-de-Plaza, Chinmay Rao, Eleftheria Astrenidou, Mischa de Ridder, Nienke Hoekstra, Klaus Hildebrandt, Marius Staring

Increased usage of automated tools like deep learning in medical image
segmentation has alleviated the bottleneck of manual contouring. This has
shifted manual labour to quality assessment (QA) of automated contours which
involves detecting errors and correcting them. A potential solution to
semi-automated QA is to use deep Bayesian uncertainty to recommend potentially
erroneous regions, thus reducing time spent on error detection. Previous work
has investigated the correspondence between uncertainty and error, however, no
work has been done on improving the "utility" of Bayesian uncertainty maps such
that it is only present in inaccurate regions and not in the accurate ones. Our
work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which
promotes uncertainty to be present only in inaccurate regions. We apply this
method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and
prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated
against voxel inaccuracies using Receiver Operating Characteristic (ROC) and
Precision-Recall (PR) curves. Numerical results show that when compared to the
Bayesian baseline the proposed method successfully suppresses uncertainty for
accurate voxels, with similar presence of uncertainty for inaccurate voxels.
Code to reproduce experiments is available at
https://github.com/prerakmody/bayesuncertainty-error-correspondence

æè¦ï¼æ·±åº¦å­¸ç¿ç­èªååå·¥å·å¨é«å­¸å½±ååå²ä¸­ä½¿ç¨çæåï¼æ¸è¼äºæåè¼ªå»æç¹ªçç¶é ¸ãéå·²å°æåååè½ç§»å°èªåè¼ªå»çåè³ªè©ä¼° (QA)ï¼å¶ä¸­åå«åµæ¸¬é¯èª¤ä¸¦ä¿®æ­£å®åãåèªåå QA çæ½å¨è§£æ±ºæ¹æ¡æ¯ä½¿ç¨æ·±åº¦è²æ°ä¸ç¢ºå®æ§ä¾å»ºè­°æ½å¨çé¯èª¤ååï¼å¾èæ¸å°è±è²»å¨é¯èª¤åµæ¸¬ä¸çæéãååçç ç©¶å·²èª¿æ¥ä¸ç¢ºå®æ§åé¯èª¤ä¹éçå°æéä¿ï¼ç¶èï¼å°æªå°æ¹åè²æ°ä¸ç¢ºå®æ§å°åçãæç¨ãé²è¡ç ç©¶ï¼ä»¥ä½¿å¶ååºç¾å¨ä¸æºç¢ºååï¼èä¸åºç¾å¨æºç¢ºååãæåçç ç©¶ä½¿ç¨æºç¢ºåº¦å°æä¸ç¢ºå®æ§ (AvU) æå¤±ä¾è¨ç·´ FlipOut æ¨¡åï¼éæä¿ä½¿ä¸ç¢ºå®æ§ååºç¾å¨ä¸æºç¢ºååãæåå°æ­¤æ¹æ³æç¨æ¼å©åæ¾å°æ²»çé¨ä½çè³æéï¼å³é ­é ¸é¨é»è¦æ·å±¤ææåååèºæ ¸ç£å±æ¯ææãä½¿ç¨æ¥æ¶å¨æä½ç¹æ§ (ROC) åç²¾ç¢ºåº¦å¬åç (PR) æ²ç·ï¼éå°é«ç´ ä¸æºç¢ºæ§è©ä¼°ä¸ç¢ºå®æ§ç±åï¼å³é æ¸¬çµï¼ãæ¸å¼çµæé¡¯ç¤ºï¼èè²æ°åºæºç¸æ¯ï¼ææåºçæ¹æ³æåå°æå¶æºç¢ºé«ç´ çä¸ç¢ºå®æ§ï¼å°æ¼ä¸æºç¢ºé«ç´ çä¸ç¢ºå®æ§å­å¨é¡ä¼¼ææ³ãå¯å¨ https://github.com/prerakmody/bayesuncertainty-error-correspondence åå¾éç¾å¯¦é©çç¨å¼ç¢¼

##### **Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**
2409.03375v1 by Francisco de Arriba-PÃ©rez, Silvia GarcÃ­a-MÃ©ndez

Based on official estimates, 50 million people worldwide are affected by
dementia, and this number increases by 10 million new patients every year.
Without a cure, clinical prognostication and early intervention represent the
most effective ways to delay its progression. To this end, Artificial
Intelligence and computational linguistics can be exploited for natural
language analysis, personalized assessment, monitoring, and treatment. However,
traditional approaches need more semantic knowledge management and
explicability capabilities. Moreover, using Large Language Models (LLMs) for
cognitive decline diagnosis is still scarce, even though these models represent
the most advanced way for clinical-patient communication using intelligent
systems. Consequently, we leverage an LLM using the latest Natural Language
Processing (NLP) techniques in a chatbot solution to provide interpretable
Machine Learning prediction of cognitive decline in real-time.
Linguistic-conceptual features are exploited for appropriate natural language
analysis. Through explainability, we aim to fight potential biases of the
models and improve their potential to help clinical workers in their diagnosis
decisions. More in detail, the proposed pipeline is composed of (i) data
extraction employing NLP-based prompt engineering; (ii) stream-based data
processing including feature engineering, analysis, and selection; (iii)
real-time classification; and (iv) the explainability dashboard to provide
visual and natural language descriptions of the prediction outcome.
Classification results exceed 80 % in all evaluation metrics, with a recall
value for the mental deterioration class about 85 %. To sum up, we contribute
with an affordable, flexible, non-invasive, personalized diagnostic system to
this work.

æè¦ï¼<paragraph>æ ¹æå®æ¹çä¼°è¨ï¼å¨çç´æ 5000 è¬äººç½¹æ£å¤±æºçï¼ä¸éåæ¸å­æ¯å¹´å¢å  1000 è¬åæ°æ£èãå¨æ²ææ²»çæ¹æ³çææ³ä¸ï¼è¨åºé å¾åæ©æä»å¥æ¯å»¶ç·©å¶æ¡åçææææ¹æ³ãçºæ­¤ï¼äººå·¥æºæ§åè¨ç®èªè¨å­¸å¯è¢«ç¨æ¼èªç¶èªè¨åæãåäººåè©ä¼°ãç£æ§åæ²»çãç¶èï¼å³çµ±æ¹æ³éè¦æ´å¤èªç¾©ç¥è­ç®¡çåå¯è§£éæ§è½åãæ­¤å¤ï¼åç®¡éäºæ¨¡åä»£è¡¨äºä½¿ç¨æºæ§ç³»çµ±é²è¡è¨åºæ£èæºéçæåé²æ¹å¼ï¼ä½å°å¤§åèªè¨æ¨¡å (LLM) ç¨æ¼èªç¥è½åä¸éè¨ºæ·ä»ç¶å¾å°è¦ãå æ­¤ï¼æåå©ç¨èå¤©æ©å¨äººè§£æ±ºæ¹æ¡ä¸­ä½¿ç¨ææ°èªç¶èªè¨èç (NLP) æè¡ç LLMï¼ä»¥æä¾å°èªç¥è½åä¸éçæ©å¨å­¸ç¿é æ¸¬ãèªè¨æ¦å¿µç¹å¾µè¢«ç¨æ¼é©ç¶çèªç¶èªè¨åæãééå¯è§£éæ§ï¼æåæ¨å¨æ¶é¤æ¨¡åçæ½å¨åå·®ï¼ä¸¦æé«å¶å¨è¨ºæ·æ±ºç­ä¸­åå©è¨åºå·¥ä½èçæ½åãæ´è©³ç´°å°èªªï¼ææåºçç®¡éåæ¬ï¼(i) ä½¿ç¨åºæ¼ NLP çæç¤ºå·¥ç¨é²è¡è³æèåï¼(ii) ä¸²æµå¼è³æèçï¼åæ¬ç¹å¾µå·¥ç¨ãåæåé¸æï¼(iii) å³æåé¡ï¼ä»¥å (iv) å¯è§£éæ§åè¡¨æ¿ï¼ä»¥æä¾é æ¸¬çµæçå¯è¦ååèªç¶èªè¨æè¿°ãåé¡çµæå¨ææè©ä¼°ææ¨ä¸­é½è¶é 80%ï¼å¿æºéåé¡å¥çå¬åçç´çº 85%ãç¸½èè¨ä¹ï¼æåçºéé å·¥ä½è²¢ç»äºä¸åç¶æ¿å¯¦æ ãéæ´»ãéä¾µå¥æ§ãåäººåçè¨ºæ·ç³»çµ±ã</paragraph>

##### **Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**
2409.03147v1 by Juan A. Berrios Moya

The rapid global aging trend has led to an increase in dementia cases,
including Alzheimer's disease, underscoring the urgent need for early and
accurate diagnostic methods. Traditional diagnostic techniques, such as
cognitive tests, neuroimaging, and biomarker analysis, face significant
limitations in sensitivity, accessibility, and cost, particularly in the early
stages. This study explores the potential of machine learning (ML) as a
transformative approach to enhance early dementia detection by leveraging ML
models to analyze and integrate complex multimodal datasets, including
cognitive assessments, neuroimaging, and genetic information. A comprehensive
review of existing literature was conducted to evaluate various ML models,
including supervised learning, deep learning, and advanced techniques such as
ensemble learning and transformer models, assessing their accuracy,
interpretability, and potential for clinical integration. The findings indicate
that while ML models show significant promise in improving diagnostic precision
and enabling earlier interventions, challenges remain in their
generalizability, interpretability, and ethical deployment. This research
concludes by outlining future directions aimed at enhancing the clinical
utility of ML models in dementia detection, emphasizing interdisciplinary
collaboration and ethically sound frameworks to improve early detection and
intervention strategies for Alzheimer's disease and other forms of dementia.

æè¦ï¼å¨çäººå£å¿«éèåè¶¨å¢å°è´å¤±æºççä¾å¢å ï¼åæ¬é¿è²æµ·é»çï¼çªé¡¯åºæ©æä¸æºç¢ºçè¨ºæ·æ¹æ³çè¿«åéæ±ãå³çµ±çè¨ºæ·æè¡ï¼ä¾å¦èªç¥æ¸¬é©ãç¥ç¶å½±ååçç©æ¨è¨åæï¼å¨æææ§ãå¯åæ§åææ¬æ¹é¢é¢è¨éå¤§éå¶ï¼ç¹å¥æ¯å¨æ©æéæ®µãæ¬ç ç©¶æ¢è¨æ©å¨å­¸ç¿ (ML) ä½çºä¸ç¨®è®é©æ§æ¹æ³çæ½åï¼ééå©ç¨ ML æ¨¡ååæåæ´åè¤éçå¤æ¨¡å¼æ¸æéï¼åæ¬èªç¥è©ä¼°ãç¥ç¶å½±ååéºå³ä¿¡æ¯ï¼ä¾å¢å¼·æ©æå¤±æºçæª¢æ¸¬ãå°ç¾ææç»é²è¡äºå¨é¢åé¡§ï¼ä»¥è©ä¼°åç¨® ML æ¨¡åï¼åæ¬ç£ç£å­¸ç¿ãæ·±åº¦å­¸ç¿ååé²æè¡ï¼ä¾å¦éæå­¸ç¿åTransformeræ¨¡åï¼è©ä¼°å¶æºç¢ºæ§ãå¯è§£éæ§åè¨åºæ´åçæ½åãç ç©¶çµæè¡¨æï¼åç®¡ ML æ¨¡åå¨æé«è¨ºæ·ç²¾åº¦åå¯¦ç¾æ©æå¹²é æ¹é¢é¡¯ç¤ºåºé¡¯èçå¸æï¼ä½å¶å¯æ¦åæ§ãå¯è§£éæ§åéå¾·é¨ç½²ä»ç¶å­å¨ææ°ãæ¬ç ç©¶æå¾æ¦è¿°äºæ¨å¨å¢å¼· ML æ¨¡åå¨å¤±æºçæª¢æ¸¬ä¸­çè¨åºæç¨çæªä¾æ¹åï¼å¼·èª¿è·¨å­¸ç§åä½åéå¾·å¥å¨çæ¡æ¶ï¼ä»¥æ¹åé¿è²æµ·é»çåå¶ä»å½¢å¼å¤±æºççæ©ææª¢æ¸¬åå¹²é ç­ç¥ã

##### **MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**
2409.03062v1 by Shehan Perera, Yunus Erzurumlu, Deepak Gulati, Alper Yilmaz

Skin cancer segmentation poses a significant challenge in medical image
analysis. Numerous existing solutions, predominantly CNN-based, face issues
related to a lack of global contextual understanding. Alternatively, some
approaches resort to large-scale Transformer models to bridge the global
contextual gaps, but at the expense of model size and computational complexity.
Finally many Transformer based approaches rely primarily on CNN based decoders
overlooking the benefits of Transformer based decoding models. Recognizing
these limitations, we address the need efficient lightweight solutions by
introducing MobileUNETR, which aims to overcome the performance constraints
associated with both CNNs and Transformers while minimizing model size,
presenting a promising stride towards efficient image segmentation. MobileUNETR
has 3 main features. 1) MobileUNETR comprises of a lightweight hybrid
CNN-Transformer encoder to help balance local and global contextual feature
extraction in an efficient manner; 2) A novel hybrid decoder that
simultaneously utilizes low-level and global features at different resolutions
within the decoding stage for accurate mask generation; 3) surpassing large and
complex architectures, MobileUNETR achieves superior performance with 3 million
parameters and a computational complexity of 1.3 GFLOP resulting in 10x and 23x
reduction in parameters and FLOPS, respectively. Extensive experiments have
been conducted to validate the effectiveness of our proposed method on four
publicly available skin lesion segmentation datasets, including ISIC 2016, ISIC
2017, ISIC 2018, and PH2 datasets. The code will be publicly available at:
https://github.com/OSUPCVLab/MobileUNETR.git

æè¦ï¼ç®èçåå²å¨é«å­¸å½±ååæä¸­æ§æä¸é éå¤§ææ°ãç¾æè¨±å¤è§£æ±ºæ¹æ¡ï¼ä¸»è¦æ¯åºæ¼ CNNï¼é¢è¨ç¼ºä¹æ´é«èæ¯çè§£çåé¡ãæèï¼ä¸äºæ¹æ³è¨´è«¸æ¼å¤§è¦æ¨¡ Transformer æ¨¡åä¾å½åæ´é«èæ¯å·®è·ï¼ä½ç§ç²äºæ¨¡åå¤§å°åè¨ç®è¤éåº¦ãæå¾ï¼è¨±å¤åºæ¼ Transformer çæ¹æ³ä¸»è¦ä¾è³´æ¼åºæ¼ CNN çè§£ç¢¼å¨ï¼èå¿½è¦äºåºæ¼ Transformer çè§£ç¢¼æ¨¡åçåªé»ãèªè­å°éäºéå¶ï¼æåééå¼å¥ MobileUNETR ä¾è§£æ±ºå°é«æè¼éç´è§£æ±ºæ¹æ¡çéæ±ï¼å¶ç®æ¨æ¯åæè CNN å Transformer ç¸éçæè½éå¶ï¼åææå°åæ¨¡åå¤§å°ï¼çºé«æå½±ååå²éåºæå¸æçä¸æ­¥ãMobileUNETR æ 3 åä¸»è¦ç¹é»ã1) MobileUNETR åå«ä¸åè¼éç´æ··å CNN-Transformer ç·¨ç¢¼å¨ï¼ä»¥ææçæ¹å¼å¹«å©å¹³è¡¡å±é¨åæ´é«èæ¯ç¹å¾µæåï¼2) ä¸åæ°ç©çæ··åè§£ç¢¼å¨ï¼å¨è§£ç¢¼éæ®µåæå©ç¨ä¸åè§£æåº¦ä¸çä½éåæ´é«ç¹å¾µï¼ä»¥é²è¡ç²¾ç¢ºçé®ç½©çæï¼3) è¶è¶å¤§åèè¤éçæ¶æ§ï¼MobileUNETR ä»¥ 300 è¬ååæ¸å 1.3 GFLOP çè¨ç®è¤éåº¦å¯¦ç¾äºåè¶çæè½ï¼åå¥æ¸å°äº 10 åå 23 åçåæ¸å FLOPãå·²ç¶é²è¡äºå»£æ³çå¯¦é©ï¼ä»¥é©è­æåæåºçæ¹æ³å¨ååå¬éå¯ç¨çç®èçè®åå²è³æéï¼åæ¬ ISIC 2016ãISIC 2017ãISIC 2018 å PH2 è³æéï¼ä¸çæææ§ãç¨å¼ç¢¼å°å¬éæ¼ï¼https://github.com/OSUPCVLab/MobileUNETR.git

##### **Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**
2409.02883v1 by Junyoung Park, Eun Hyun Seo, Sunjun Kim, SangHak Yi, Kun Ho Lee, Sungho Won

Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to
assess cognitive functions such as visuospatial skills and memory, making them
valuable tools for detecting mild cognitive impairment (MCI). Despite their
utility, existing predictive models based on these tests often suffer from
limitations like small sample sizes and lack of external validation, which
undermine their reliability. We developed a multi-stream deep learning
framework that integrates two distinct processing streams: a multi-head
self-attention based spatial stream using raw RCFT images and a scoring stream
employing a previously developed automated scoring system. Our model was
trained on data from 1,740 subjects in the Korean cohort and validated on an
external hospital dataset of 222 subjects from Korea. The proposed multi-stream
model demonstrated superior performance over baseline models (AUC = 0.872,
Accuracy = 0.781) in external validation. The integration of both spatial and
scoring streams enables the model to capture intricate visual details from the
raw images while also incorporating structured scoring data, which together
enhance its ability to detect subtle cognitive impairments. This dual approach
not only improves predictive accuracy but also increases the robustness of the
model, making it more reliable in diverse clinical settings. Our model has
practical implications for clinical settings, where it could serve as a
cost-effective tool for early MCI screening.

æè¦ï¼é·æ°è¤éåå½¢æ¸¬é© (RCFT) ç­ç¹ªç«æ¸¬é©å»£æ³ç¨æ¼è©ä¼°è¦è¦ºç©ºéæè½åè¨æ¶åç­èªç¥åè½ï¼ä½¿å¶æçºæª¢æ¸¬è¼åº¦èªç¥éç¤ (MCI) çå¯¶è²´å·¥å·ãåç®¡å®åå¾æç¨ï¼ä½åºæ¼éäºæ¸¬é©çç¾æé æ¸¬æ¨¡åéå¸¸æåå°æ¨£æ¬éå°åç¼ºä¹å¤é¨é©è­ç­éå¶ï¼éææå®³å¶å¯é æ§ãæåéç¼äºä¸åå¤ä¸²æµæ·±åº¦å­¸ç¿æ¡æ¶ï¼å®æ´åäºå©åä¸åçèçä¸²æµï¼ä¸ååºæ¼å¤é ­èªæ³¨æåï¼ä½¿ç¨åå§ RCFT å½±åçç©ºéä¸²æµï¼ä»¥åä¸åæ¡ç¨ååéç¼çèªåè©åç³»çµ±çè©åä¸²æµãæåçæ¨¡åå¨éåç¾¤çµä¸­ 1,740 ååè©¦èçè³æä¸é²è¡è¨ç·´ï¼ä¸¦å¨ä¾èªéåç 222 ååè©¦èçå¤é¨é«é¢è³æéä¸é²è¡é©è­ãææåºçå¤ä¸²æµæ¨¡åå¨å¤é¨é©è­ä¸­è¡¨ç¾åºåªæ¼åºæºæ¨¡åçæè½ (AUC = 0.872ï¼æºç¢ºç = 0.781)ãç©ºéåè©åä¸²æµçæ´åä½¿æ¨¡åè½å¤ å¾åå§å½±åæ·åè¤éçè¦è¦ºç´°ç¯ï¼åæä¹è½ç´å¥çµæ§åçè©åè³æï¼éå±åå¢å¼·äºå®æª¢æ¸¬ç´°å¾®èªç¥éç¤çè½åãéç¨®ééæ¹æ³ä¸åæé«äºé æ¸¬æºç¢ºæ§ï¼ä¹å¢å äºæ¨¡åçç©©å¥æ§ï¼ä½¿å¶å¨ä¸åçè¨åºç°å¢ä¸­æ´å¯é ãæåçæ¨¡åå°è¨åºç°å¢æå¯¦éçæç¾©ï¼å®å¯ä»¥å¨å¶ä¸­ä½çºæ©æ MCI ç¯©æª¢çå·ææ¬æççå·¥å·ã

##### **Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**
2409.02681v1 by Ramon Tavares

This study presents a comprehensive methodology for modeling and forecasting
the historical time series of fire spots detected by the AQUA_M-T satellite in
the Amazon, Brazil. The approach utilizes a mixed Recurrent Neural Network
(RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit
(GRU) architectures to predict monthly accumulations of daily detected fire
spots. A summary of the data revealed a consistent seasonality over time, with
annual maximum and minimum fire spot values tending to repeat at the same
periods each year. The primary objective is to verify whether the forecasts
capture this inherent seasonality through rigorous statistical analysis. The
methodology involved careful data preparation, model configuration, and
training using cross-validation with two seeds, ensuring that the data
generalizes well to the test and validation sets, and confirming the
convergence of the model parameters. The results indicate that the mixed LSTM
and GRU model offers improved accuracy in forecasting 12 months ahead,
demonstrating its effectiveness in capturing complex temporal patterns and
modeling the observed time series. This research significantly contributes to
the application of deep learning techniques in environmental monitoring,
specifically in fire spot forecasting. In addition to improving forecast
accuracy, the proposed approach highlights the potential for adaptation to
other time series forecasting challenges, opening new avenues for research and
development in machine learning and natural phenomenon prediction. Keywords:
Time Series Forecasting, Recurrent Neural Networks, Deep Learning.

æè¦ï¼æ¬ç ç©¶æåºäºä¸åå¨é¢çæ¹æ³ï¼ç¨æ¼å»ºæ¨¡åé æ¸¬å·´è¥¿äºé¦¬éå°åç± AQUA_M-T è¡æåµæ¸¬å°çæ­·å²ç«ç½é»æéåºåãè©²æ¹æ³æ¡ç¨æ··åéè¿´ç¥ç¶ç¶²è·¯ (RNN) æ¨¡åï¼çµåé·ç­æè¨æ¶ (LSTM) åéæ§éè¿´å®å (GRU) æ¶æ§ï¼ä»¥é æ¸¬æ¯æ¥åµæ¸¬ç«ç½é»çæç´¯è¨å¼ãå°è³æçæè¦é¡¯ç¤ºåºé¨èæéæ¨ç§»èåºç¾çä¸è´å­£ç¯æ§ï¼æ¯å¹´çå¹´åº¦æå¤§åæå°ç«ç½é»å¼å¾åæ¼å¨åä¸ææéè¤åºç¾ãä¸»è¦ç®æ¨æ¯ééå´è¬¹ççµ±è¨åæé©è­é æ¸¬æ¯å¦ææå°éç¨®åºæçå­£ç¯æ§ãè©²æ¹æ³æ¶åä»ç´°çè³ææºåãæ¨¡åéç½®ï¼ä»¥åä½¿ç¨å©åç¨®å­çäº¤åé©è­é²è¡è¨ç·´ï¼ç¢ºä¿è³æè½å¾å¥½å°æ¨å»£å°æ¸¬è©¦åé©è­éï¼ä¸¦ç¢ºèªæ¨¡ååæ¸çæ¶ææ§ãçµæè¡¨æï¼æ··å LSTM å GRU æ¨¡åå¨é æ¸¬ 12 åæå¾æä¾äºæ´é«çæºç¢ºåº¦ï¼è­æäºå¶å¨ææè¤éæéæ¨¡å¼åå»ºæ¨¡è§æ¸¬æéåºåæ¹é¢çæææ§ãæ¬ç ç©¶é¡¯èå°ä¿è¿äºæ·±åº¦å­¸ç¿æè¡å¨ç°å¢ç£æ¸¬ä¸­çæç¨ï¼ç¹å¥æ¯å¨ç«ç½é»é æ¸¬æ¹é¢ãé¤äºæé«é æ¸¬æºç¢ºåº¦å¤ï¼ææåºçæ¹æ³éå¼·èª¿äºé©æå¶ä»æéåºåé æ¸¬ææ°çæ½åï¼çºæ©å¨å­¸ç¿åèªç¶ç¾è±¡é æ¸¬çç ç©¶åéç¼éé¢äºæ°çéå¾ãééµå­ï¼æéåºåé æ¸¬ãéè¿´ç¥ç¶ç¶²è·¯ãæ·±åº¦å­¸ç¿ã

##### **SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**
2409.02598v1 by Wenwu Guo, Jinlin Wu, Zhen Chen, Qingxiang Zhao, Miao Xu, Zhen Lei, Hongbin Liu

Vision-based surgical navigation has received increasing attention due to its
non-invasive, cost-effective, and flexible advantages. In particular, a
critical element of the vision-based navigation system is tracking surgical
instruments. Compared with 2D instrument tracking methods, 3D instrument
tracking has broader value in clinical practice, but is also more challenging
due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models
for 3D registration. To solve these challenges, we propose the SurgTrack, a
two-stage 3D instrument tracking method for CAD-free and robust real-world
applications. In the first registration stage, we incorporate an Instrument
Signed Distance Field (SDF) modeling the 3D representation of instruments,
achieving CAD-freed 3D registration. Due to this, we can obtain the location
and orientation of instruments in the 3D space by matching the video stream
with the registered SDF model. In the second tracking stage, we devise a
posture graph optimization module, leveraging the historical tracking results
of the posture memory pool to optimize the tracking results and improve the
occlusion robustness. Furthermore, we collect the Instrument3D dataset to
comprehensively evaluate the 3D tracking of surgical instruments. The extensive
experiments validate the superiority and scalability of our SurgTrack, by
outperforming the state-of-the-arts with a remarkable improvement. The code and
dataset are available at https://github.com/wenwucode/SurgTrack.

æè¦ï¼<paragraph>åºæ¼è¦è¦ºçå¤ç§å°èªç±æ¼å¶éä¾µå¥æ§ãææ¬æçåéæ´»æ§åªå¢èåå°è¶ä¾è¶å¤çéæ³¨ãç¹å¥æ¯ï¼åºæ¼è¦è¦ºçå°èªç³»çµ±çä¸åééµåç´ æ¯è¿½è¹¤æè¡å¨æ¢°ãè 2D å¨æ¢°è¿½è¹¤æ¹æ³ç¸æ¯ï¼3D å¨æ¢°è¿½è¹¤å¨è¨åºå¯¦åä¸­å·ææ´å»£æ³çå¹å¼ï¼ä½ç±æ¼ç´çå¼±ãé®æåç¼ºä¹ç¨æ¼ 3D éæºçé»è¦è¼å©è¨­è¨ (CAD) æ¨¡åï¼å æ­¤ä¹æ´å·ææ°æ§ãçºäºè§£æ±ºéäºææ°ï¼æåæåº SurgTrackï¼ä¸ç¨®é©ç¨æ¼ç¡ CAD åç©©å¥ççå¯¦ä¸çæç¨ç¨å¼çå©éæ®µ 3D å¨æ¢°è¿½è¹¤æ¹æ³ãå¨ç¬¬ä¸åéæºéæ®µï¼æåæ´åä¸åå¨æ¢°ç°½ç½²è·é¢å ´ (SDF)ï¼å°å¨æ¢°ç 3D è¡¨å¾µé²è¡å»ºæ¨¡ï¼å¯¦ç¾ç¡ CAD ç 3D éæºãå æ­¤ï¼æåå¯ä»¥ééå°è¦è¨ä¸²æµèå·²éæºç SDF æ¨¡åé²è¡å¹éï¼åå¾å¨æ¢°å¨ 3D ç©ºéä¸­çä½ç½®åæ¹åãå¨ç¬¬äºåè¿½è¹¤éæ®µï¼æåè¨­è¨ä¸åå§¿å¢åæä½³åæ¨¡çµï¼å©ç¨å§¿å¢è¨æ¶æ± çæ­·å²è¿½è¹¤çµæä¾æä½³åè¿½è¹¤çµæä¸¦æ¹åé®æçç©©å¥æ§ãæ­¤å¤ï¼æåæ¶é Instrument3D è³æéï¼ä»¥å¨é¢è©ä¼°æè¡å¨æ¢°ç 3D è¿½è¹¤ãå»£æ³çå¯¦é©é©è­äºæå SurgTrack çåªè¶æ§åå¯æ´åæ§ï¼ä»¥é¡¯èçæ¹é²åªæ¼ç¾ææè¡ãç¨å¼ç¢¼åè³æéå¯å¨ https://github.com/wenwucode/SurgTrack åå¾ã</paragraph>

##### **Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**
2409.02530v1 by Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang

The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of
kidney function in clinical practice. Although traditional equations and
Machine Learning (ML) models using clinical and laboratory data can estimate
eGFR, accurately predicting future eGFR levels remains a significant challenge
for nephrologists and ML researchers. Recent advances demonstrate that Large
Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust
foundation models for diverse applications. This study investigates the
potential of LMMs to predict future eGFR levels with a dataset consisting of
laboratory and clinical values from 50 patients. By integrating various
prompting techniques and ensembles of LMMs, our findings suggest that these
models, when combined with precise prompts and visual representations of eGFR
trajectories, offer predictive performance comparable to existing ML models.
This research extends the application of foundation models and suggests avenues
for future studies to harness these models in addressing complex medical
forecasting challenges.

æè¦ï¼ä¼°è¨çèå°çéæ¿¾ç (eGFR) æ¯è¨åºå¯¦åä¸­èèåè½çéè¦ææ¨ãéç¶å³çµ±æ¹ç¨å¼åä½¿ç¨è¨åºèå¯¦é©å®¤è³æçæ©å¨å­¸ç¿ (ML) æ¨¡åå¯ä»¥ä¼°è¨ eGFRï¼ä½æºç¢ºé æ¸¬æªä¾ eGFR æ°´å¹³ä»ç¶æ¯èèç§é«å¸«å ML ç ç©¶äººå¡çä¸å¤§ææ°ãæè¿çç ç©¶é²å±é¡¯ç¤ºï¼å¤§åèªè¨æ¨¡å (LLM) åå¤§åå¤æ¨¡ææ¨¡å (LMM) å¯ä»¥ä½çºåç¨®æç¨ç¨å¼çå¼·å¥åºç¤æ¨¡åãæ¬ç ç©¶æ¢è¨ LMM é æ¸¬æªä¾ eGFR æ°´å¹³çæ½åï¼å¶è³æéåå« 50 ä½çæ£çå¯¦é©å®¤åè¨åºæ¸å¼ãééæ´ååç¨®æç¤ºæè¡å LMM çåå¥ï¼æåçç ç©¶çµæé¡¯ç¤ºï¼éäºæ¨¡åå¨çµåç²¾ç¢ºæç¤ºå eGFR è»è·¡çè¦è¦ºåè¡¨ç¤ºæï¼å¯æä¾èç¾æ ML æ¨¡åç¸è¿çé æ¸¬æè½ãéé ç ç©¶æ´å±äºåºç¤æ¨¡åçæç¨ï¼ä¸¦çºæªä¾ç ç©¶å©ç¨éäºæ¨¡åä¾æå°è¤éçé«çé æ¸¬ææ°æä¾äºéå¾ã

##### **Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**
2409.02337v1 by Deepak Raina, Mythra V. Balakuntala, Byung Wook Kim, Juan Wachs, Richard Voyles

Ultrasound is widely employed for clinical intervention and diagnosis, due to
its advantages of offering non-invasive, radiation-free, and real-time imaging.
However, the accessibility of this dexterous procedure is limited due to the
substantial training and expertise required of operators. The robotic
ultrasound (RUS) offers a viable solution to address this limitation;
nonetheless, achieving human-level proficiency remains challenging. Learning
from demonstrations (LfD) methods have been explored in RUS, which learns the
policy prior from a dataset of offline demonstrations to encode the mental
model of the expert sonographer. However, active engagement of experts, i.e.
Coaching, during the training of RUS has not been explored thus far. Coaching
is known for enhancing efficiency and performance in human training. This paper
proposes a coaching framework for RUS to amplify its performance. The framework
combines DRL (self-supervised practice) with sparse expert's feedback through
coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a
reward based on image quality rating. The coaching by experts is modeled as a
Partially Observable Markov Decision Process (POMDP), which updates the policy
parameters based on the correction by the expert. The validation study on
phantoms showed that coaching increases the learning rate by $25\%$ and the
number of high-quality image acquisition by $74.5\%$.

æè¦ï¼è¶é³æ³¢å å¶æä¾éä¾µå¥æ§ãç¡è¼»å°ä¸å³æå½±åçåªé»ï¼èå»£æ³ç¨æ¼è¨åºä»å¥åè¨ºæ·ã
ç¶èï¼ç±æ¼æä½å¡éè¦å¤§éçè¨ç·´åå°æ¥­ç¥è­ï¼éå¶äºæ­¤éæ´»ç¨åºçå¯åæ§ãæ©å¨äººè¶é³æ³¢ (RUS) æä¾äºä¸åå¯è¡çè§£æ±ºæ¹æ¡ä¾è§£æ±ºæ­¤éå¶ï¼
åç®¡å¦æ­¤ï¼è¦éå°äººé¡ç­ç´ççç·´åº¦ä»ç¶å·æææ°æ§ãå­¸ç¿ç¤ºç¯ (LfD) æ¹æ³å·²å¨ RUS ä¸­é²è¡æ¢è¨ï¼å®å¾é¢ç·ç¤ºç¯çè³æéå­¸ç¿åé©ç­ç¥ï¼ä»¥ç·¨ç¢¼å°å®¶è¶é³æ³¢æª¢æ¥å¡çå¿æºæ¨¡åãç¶èï¼è¿ä»å°æªæ¢è¨å°å®¶å¨ RUS è¨ç·´æéçç©æ¥µåèï¼å³æå°ãæå°å·²ç¥å¯ä»¥æé«äººé¡è¨ç·´çæçåç¸¾æãæ¬ææåºäºä¸å RUS æå°æ¶æ§ï¼ä»¥æåå¶ç¸¾æãæ­¤æ¶æ§çµåäº DRLï¼èªæç£ç£å¯¦åï¼èééæå°æä¾çå°å®¶ç¨çåé¥ãDRL ä½¿ç¨é¢ç·ç­ç¥è»æ§åä½-è©è« (SAC) ç¶²è·¯ï¼ä¸¦æ ¹æå½±ååè³ªè©åçµ¦äºçåµãå°å®¶çæå°è¢«å»ºæ¨¡çºé¨åå¯è§å¯é¦¬å¯å¤«æ±ºç­éç¨ (POMDP)ï¼å®æ ¹æå°å®¶çä¿®æ­£ä¾æ´æ°ç­ç¥åæ¸ãå¨æ¨¡æ¬äººé«æ¨¡åä¸çé©è­ç ç©¶é¡¯ç¤ºï¼æå°å°å­¸ç¿çæé«äº $25\%$ï¼é«åè³ªå½±åæ·åæ¸éæé«äº $74.5\%$ã

##### **Action-Based ADHD Diagnosis in Video**
2409.02261v1 by Yichun Li, Yuxing Yang, Syed Nohsen Naqvi

Attention Deficit Hyperactivity Disorder (ADHD) causes significant impairment
in various domains. Early diagnosis of ADHD and treatment could significantly
improve the quality of life and functioning. Recently, machine learning methods
have improved the accuracy and efficiency of the ADHD diagnosis process.
However, the cost of the equipment and trained staff required by the existing
methods are generally huge. Therefore, we introduce the video-based frame-level
action recognition network to ADHD diagnosis for the first time. We also record
a real multi-modal ADHD dataset and extract three action classes from the video
modality for ADHD diagnosis. The whole process data have been reported to
CNTW-NHS Foundation Trust, which would be reviewed by medical
consultants/professionals and will be made public in due course.

æè¦ï¼æ³¨æåç¼ºé·éåç (ADHD) æå¨åç¨®é åé æé¡¯èçæå®³ãææ©è¨ºæ· ADHD ä¸¦æ¥åæ²»çå¯ä»¥å¤§å¹æ¹åçæ´»åè³ªååè½ãæè¿ï¼æ©å¨å­¸ç¿æ¹æ³å·²ç¶æåäº ADHD è¨ºæ·ç¨åºçæºç¢ºåº¦åæçãç¶èï¼ç¾ææ¹æ³æéçè¨­ååè¨ç·´æç´ çäººå¡ææ¬éå¸¸å¾é«ãå æ­¤ï¼æåé¦æ¬¡å°åºæ¼å½±ççå¹ç´åä½è¾¨è­ç¶²è·¯å¼å¥ ADHD è¨ºæ·ãæåä¹è¨éäºä¸åçæ­£çå¤æ¨¡å¼ ADHD è³æéï¼ä¸¦å¾å½±çæ¨¡å¼ä¸­èååºä¸ååä½é¡å¥ä»¥é²è¡ ADHD è¨ºæ·ãæ´åæµç¨çè³æå·²ç¶åå ±çµ¦ CNTW-NHS åºéæï¼å°ç±é«çé¡§å/å°æ¥­äººå£«å¯©æ¥ï¼ä¸¦å°é©æå¬éã

##### **A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**
2409.02069v1 by Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy

Dental disease is a prevalent chronic condition associated with substantial
financial burden, personal suffering, and increased risk of systemic diseases.
Despite widespread recommendations for twice-daily tooth brushing, adherence to
recommended oral self-care behaviors remains sub-optimal due to factors such as
forgetfulness and disengagement. To address this, we developed Oralytics, a
mHealth intervention system designed to complement clinician-delivered
preventative care for marginalized individuals at risk for dental disease.
Oralytics incorporates an online reinforcement learning algorithm to determine
optimal times to deliver intervention prompts that encourage oral self-care
behaviors. We have deployed Oralytics in a registered clinical trial. The
deployment required careful design to manage challenges specific to the
clinical trials setting in the U.S. In this paper, we (1) highlight key design
decisions of the RL algorithm that address these challenges and (2) conduct a
re-sampling analysis to evaluate algorithm design decisions. A second phase
(randomized control trial) of Oralytics is planned to start in spring 2025.

æè¦ï¼çç§ç¾çæ¯ä¸ç¨®æ®éçæ¢æ§ç¾çï¼èå¤§éçç¶æ¿è² æãåäººçè¦åå¢å çå¨èº«ç¾çé¢¨éªæéãåç®¡æ®éå»ºè­°æ¯å¤©å·çå©æ¬¡ï¼ä½ç±æ¼å¥å¿åè«é¢ç­å ç´ ï¼å°å»ºè­°çå£èèªæä¿å¥è¡çºçä¾å¾æ§ä»ç¶ä½æ¼æä½³æ°´å¹³ãçºäºè§£æ±ºéååé¡ï¼æåéç¼äº Oralyticsï¼ä¸å mHealth ä»å¥ç³»çµ±ï¼æ¨å¨è£åè¨åºé«çæä¾çé é²ä¿å¥ï¼ä»¥é é²æçç§ç¾çé¢¨éªçéç·£ååäººãOralytics çµåäºä¸åå¨ç·å¼·åå­¸ç¿æ¼ç®æ³ï¼ä»¥ç¢ºå®æä¾ä»å¥æç¤ºçæä½³æéï¼éäºæç¤ºé¼åµå£èèªæä¿å¥è¡çºãæåå·²å¨è¨»åçè¨åºè©¦é©ä¸­é¨ç½²äº Oralyticsãè©²é¨ç½²éè¦ä»ç´°çè¨­è¨ä¾ç®¡çç¾åè¨åºè©¦é©è¨­ç½®ä¸­å·é«çææ°ãå¨æ¬æä¸­ï¼æåï¼1ï¼éé»ä»ç´¹äºè§£æ±ºéäºææ°ç RL æ¼ç®æ³çééµè¨­è¨æ±ºç­ï¼ä»¥åï¼2ï¼é²è¡éæ°æ½æ¨£åæä»¥è©ä¼°æ¼ç®æ³è¨­è¨æ±ºç­ãOralytics çç¬¬äºéæ®µï¼é¨æ©å°ç§è©¦é©ï¼è¨åæ¼ 2025 å¹´æ¥å­£éå§ã

##### **TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**
2409.02018v1 by Bobby Azad, Pourya Adibfar, Kaiqun Fu

In healthcare, medical image segmentation is crucial for accurate disease
diagnosis and the development of effective treatment strategies. Early
detection can significantly aid in managing diseases and potentially prevent
their progression. Machine learning, particularly deep convolutional neural
networks, has emerged as a promising approach to addressing segmentation
challenges. Traditional methods like U-Net use encoding blocks for local
representation modeling and decoding blocks to uncover semantic relationships.
However, these models often struggle with multi-scale objects exhibiting
significant variations in texture and shape, and they frequently fail to
capture long-range dependencies in the input data. Transformers designed for
sequence-to-sequence predictions have been proposed as alternatives, utilizing
global self-attention mechanisms. Yet, they can sometimes lack precise
localization due to insufficient granular details. To overcome these
limitations, we introduce TransDAE: a novel approach that reimagines the
self-attention mechanism to include both spatial and channel-wise associations
across the entire feature space, while maintaining computational efficiency.
Additionally, TransDAE enhances the skip connection pathway with an inter-scale
interaction module, promoting feature reuse and improving localization
accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on
the Synaps multi-organ dataset, even without relying on pre-trained weights.

æè¦ï¼å¨å»çä¿å¥é¢åï¼å»å­¦å½±ååå²å¯¹äºåç¡®çç¾çè¯æ­åæææ²»çç­ç¥çå¼åè³å³éè¦ãæ©ææ£æµå¯ä»¥æå¤§å°å¸®å©æ§å¶ç¾çï¼å¹¶å¯è½é²æ­¢ç¾çè¿å±ãæºå¨å­¦ä¹ ï¼å°¤å¶æ¯æ·±åº¦å·ç§¯ç¥ç»ç½ç»ï¼å·²æä¸ºè§£å³åå²ææçä¸ç§æåéçæ¹æ³ãU-Net ç­ä¼ ç»æ¹æ³ä½¿ç¨ç¼ç åè¿è¡å±é¨è¡¨ç¤ºå»ºæ¨¡åè§£ç åæ¥æ­ç¤ºè¯­ä¹å³ç³»ãç¶èï¼è¿äºæ¨¡åéå¸¸é¾ä»¥å¤çå¨çº¹çåå½¢ç¶ä¸è¡¨ç°åºæ¾çååçå¤å°ºåº¦å¯¹è±¡ï¼å¹¶ä¸å®ä»¬ç»å¸¸æ æ³æè·è¾å¥æ°æ®ä¸­çè¿ç¨ä¾èµå³ç³»ãä¸ä¸ºåºåå°åºåé¢æµèè®¾è®¡ç Transformer å·²è¢«æåºä½ä¸ºæ¿ä»£æ¹æ¡ï¼å©ç¨å¨å±èªæ³¨æåæºå¶ãç¶èï¼ç±äºç²åº¦ç»èä¸è¶³ï¼å®ä»¬ææ¶å¯è½ç¼ºä¹ç²¾ç¡®çå®ä½ãä¸ºäºåæè¿äºéå¶ï¼æä»¬å¼å¥äº TransDAEï¼ä¸ç§æ°é¢çæ¹æ³ï¼å®éæ°ææ³äºèªæ³¨æåæºå¶ï¼ä»¥åå«æ´ä¸ªç¹å¾ç©ºé´ä¸­çç©ºé´åééå³èï¼åæ¶ä¿æè®¡ç®æçãæ­¤å¤ï¼TransDAE éè¿å°ºåº¦é´äº¤äºæ¨¡åå¢å¼ºäºè·³è·è¿æ¥è·¯å¾ï¼ä¿è¿äºç¹å¾éç¨å¹¶æé«äºå®ä½ç²¾åº¦ãå¼å¾æ³¨æçæ¯ï¼å³ä½¿ä¸ä¾èµé¢è®­ç»æéï¼TransDAE å¨ Synaps å¤å¨å®æ°æ®éä¸ä¹ä¼äºç°æçæåè¿æ¹æ³ã

##### **A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**
2409.01903v1 by Abdelmalek Mouazer, Sophie Dubois, Romain LÃ©guillon, Nada Boudegzdame, Thibaud Levrard, Yoann Le Bars, Christian Simon, Brigitte SÃ©roussi, Julien Grosjean, Romain Lelong, Catherine Letord, StÃ©fan Darmoni, Karima Sedki, Pierre Meneton, Rosy Tsopra, Hector Falcoff, Jean-Baptiste Lamy

Background: Medication review is a structured interview of the patient,
performed by the pharmacist and aimed at optimizing drug treatments. In
practice, medication review is a long and cognitively-demanding task that
requires specific knowledge. Clinical practice guidelines have been proposed,
but their application is tedious. Methods: We designed ABiMed, a clinical
decision support system for medication reviews, based on the implementation of
the STOPP/START v2 guidelines and on the visual presentation of aggregated drug
knowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39
community pharmacists during a randomized simulation trial, each pharmacist
performing a medication review for two fictitious patients without ABiMed, and
two others with ABiMed. We recorded the problems identified by the pharmacists,
the interventions proposed, the response time, the perceived usability and the
comments. Pharmacists' medication reviews were compared to an expert-designed
gold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant
drug-related problems during the medication review (p=1.1e-12) and proposed
better interventions (p=9.8e-9), without needing more time (p=0.56). The System
Usability Scale score is 82.7, which is ranked "excellent". In their comments,
pharmacists appreciated the visual aspect of ABiMed and its ability to compare
the current treatment with the proposed one. A multifactor analysis showed no
difference in the support offered by ABiMed according to the pharmacist's age
or sex, in terms of percentage of problems identified or quality of the
proposed interventions. Conclusions: The use of an intelligent and visual
clinical decision support system can help pharmacists when they perform
medication reviews. Our main perspective is the validation of the system in
clinical conditions.

æè¦ï¼<paragraph>èæ¯ï¼ç¨è¥å¯©æ¥æ¯ç±è¥å¸«å·è¡çä¸ç¨®çµæ§åæ£èè¨ªè«ï¼ç®çå¨æ¼åªåè¥ç©æ²»çãå¨å¯¦åä¸ï¼ç¨è¥å¯©æ¥æ¯ä¸é åé·ä¸èªç¥éæ±é«çä»»åï¼éè¦å·åç¹å®ç¥è­ãéç¶å·²æåºè¨åºå¯¦åæå¼ï¼ä½å¶æç¨å¾ç¹ç£ãæ¹æ³ï¼æåæ ¹æ STOPP/START v2 æå¼çå¯¦ä½ï¼ä¸¦ä½¿ç¨è¡¨æ ¼ãåè¡¨åè±å½¢ç¬¦èè¦è¦ºååç¾å½æ´çè¥ç©ç¥è­ï¼è¨­è¨äºä¸å¥ç¨è¥å¯©æ¥çè¨åºæ±ºç­æ¯æ´ç³»çµ± ABiMedãæåå¨é¨æ©æ¨¡æ¬è©¦é©ä¸­ï¼è® 39 ä½ç¤¾åè¥å¸«è©ä¼° ABiMedï¼æ¯ä½è¥å¸«éå°å©ä½èæ§æ£èå·è¡ç¨è¥å¯©æ¥ï¼å©æ¬¡æ²æä½¿ç¨ ABiMedï¼å©æ¬¡ä½¿ç¨ ABiMedãæåè¨éäºè¥å¸«è­å¥åºçåé¡ãå»ºè­°çä»å¥æªæ½ãåææéãæç¥å¯ç¨æ§åè©è«ãå°è¥å¸«çç¨è¥å¯©æ¥èå°å®¶è¨­è¨çéæ¨æºé²è¡æ¯è¼ãçµæï¼ä½¿ç¨ ABiMed å¾ï¼è¥å¸«å¨ç¨è¥å¯©æ¥æéç¼ç¾äºå¤ 1.6 åç¸éçè¥ç©ç¸éåé¡ï¼p=1.1e-12ï¼ï¼ä¸¦æåºæ´å¥½çä»å¥æªæ½ï¼p=9.8e-9ï¼ï¼èç¡éè±è²»æ´å¤æéï¼p=0.56ï¼ãç³»çµ±å¯ç¨æ§è©åçº 82.7ï¼è¢«è©çºãåªè¯ããå¨ä»åçè©è«ä¸­ï¼è¥å¸«è®è³ ABiMed çè¦è¦ºåé¢åï¼ä»¥åå®æ¯è¼ç®åæ²»çèå»ºè­°æ²»ççè½åãå¤å ç´ åæé¡¯ç¤ºï¼ABiMed æä¾çæ¯æ´å¨è¥å¸«çå¹´é½¡ææ§å¥æ¹é¢æ²æå·®ç°ï¼å°±è­å¥åºçåé¡ç¾åæ¯æå»ºè­°ä»å¥æªæ½çåè³ªèè¨ãçµè«ï¼ä½¿ç¨æºæ§ä¸è¦è¦ºåçè¨åºæ±ºç­æ¯æ´ç³»çµ±ï¼å¯ä»¥åå©è¥å¸«å·è¡ç¨è¥å¯©æ¥ãæåçè§é»ä¸»è¦æ¯é©è­ç³»çµ±å¨è¨åºæ¢ä»¶ä¸çæåº¦ã</paragraph>

##### **Training on the Benchmark Is Not All You Need**
2409.01790v1 by Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang

The success of Large Language Models (LLMs) relies heavily on the huge amount
of pre-training data learned in the pre-training phase. The opacity of the
pre-training process and the training data causes the results of many benchmark
tests to become unreliable. If any model has been trained on a benchmark test
set, it can seriously hinder the health of the field. In order to automate and
efficiently test the capabilities of large language models, numerous mainstream
benchmarks adopt a multiple-choice format. As the swapping of the contents of
multiple-choice options does not affect the meaning of the question itself, we
propose a simple and effective data leakage detection method based on this
property. Specifically, we shuffle the contents of the options in the data to
generate the corresponding derived data sets, and then detect data leakage
based on the model's log probability distribution over the derived data sets.
If there is a maximum and outlier in the set of log probabilities, it indicates
that the data is leaked. Our method is able to work under black-box conditions
without access to model training data or weights, effectively identifying data
leakage from benchmark test sets in model pre-training data, including both
normal scenarios and complex scenarios where options may have been shuffled
intentionally or unintentionally. Through experiments based on two LLMs and
benchmark designs, we demonstrate the effectiveness of our method. In addition,
we evaluate the degree of data leakage of 31 mainstream open-source LLMs on
four benchmark datasets and give a ranking of the leaked LLMs for each
benchmark, and we find that the Qwen family of LLMs has the highest degree of
data leakage.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çæåå¨å¾å¤§ç¨åº¦ä¸åæ±ºæ¼é è¨ç·´éæ®µä¸­å­¸ç¿å°çæµ·éé è¨ç·´æ¸æãé è¨ç·´éç¨åè¨ç·´æ¸æçä¸éææ§å°è´è¨±å¤åºæºæ¸¬è©¦ççµæè®å¾ä¸å¯é ãå¦æä»»ä½æ¨¡åå·²å¨åºæºæ¸¬è©¦éä¸­é²è¡è¨ç·´ï¼åå¯è½æå´éé»ç¤è©²é åçç¼å±ãçºäºèªååä¸ææå°æ¸¬è©¦å¤§åèªè¨æ¨¡åçè½åï¼è¨±å¤ä¸»æµåºæºæ¡ç¨å¤é¸é¡æ ¼å¼ãç±æ¼å¤é¸é¡é¸é å§å®¹çäºæä¸å½±é¿åé¡æ¬èº«çå«ç¾©ï¼å æ­¤æåæåºäºä¸ç¨®åºæ¼æ­¤å±¬æ§çç°¡å®ä¸ææçæ°æ®æ´©æ¼æª¢æ¸¬æ¹æ³ãå·é«ä¾èªªï¼æåå°æ¸æä¸­é¸é çå§å®¹é¨æ©æåä»¥çæå°æçæ´¾çæ¸æéï¼ç¶å¾æ ¹ææ¨¡åå¨æ´¾çæ¸æéä¸çå°æ¸æ¦çåä½æª¢æ¸¬æ¸ææ´©æ¼ãå¦æå°æ¸æ¦çéä¸­å­å¨æå¤§å¼åç°å¸¸å¼ï¼åè¡¨ç¤ºæ¸æå·²æ´©æ¼ãæåçæ¹æ³è½å¤ å¨ä¸è¨ªåæ¨¡åè¨ç·´æ¸æææ¬éçé»çæ¢ä»¶ä¸å·¥ä½ï¼ææå°è­å¥æ¨¡åé è¨ç·´æ¸æä¸­åºæºæ¸¬è©¦éçæ¸ææ´©æ¼ï¼åæ¬é¸é å¯è½å·²æææç¡æå°è¢«æäºçæ­£å¸¸å ´æ¯åè¤éå ´æ¯ãééåºæ¼å©å LLM ååºæºè¨­è¨çå¯¦é©ï¼æåè­æäºæåæ¹æ³çæææ§ãæ­¤å¤ï¼æåè©ä¼°äº 31 åä¸»æµéæº LLM å¨åååºæºæ¸æéä¸çæ¸ææ´©æ¼ç¨åº¦ï¼ä¸¦å°æ¯ååºæºçæ´©æ¼ LLM é²è¡äºæåï¼æåç¼ç¾ Qwen å®¶æç LLM å·ææé«çæ¸ææ´©æ¼ç¨åº¦ã

##### **Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**
2409.01676v1 by Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink

Deriving health indicators of rotating machines is crucial for their
maintenance. However, this process is challenging for the prevalent adopted
intelligent methods since they may take the whole data distributions, not only
introducing noise interference but also lacking the explainability. To address
these issues, we propose a diffusion-based weakly-supervised approach for
deriving health indicators of rotating machines, enabling early fault detection
and continuous monitoring of condition evolution. This approach relies on a
classifier-free diffusion model trained using healthy samples and a few
anomalies. This model generates healthy samples. and by comparing the
differences between the original samples and the generated ones in the envelope
spectrum, we construct an anomaly map that clearly identifies faults. Health
indicators are then derived, which can explain the fault types and mitigate
noise interference. Comparative studies on two cases demonstrate that the
proposed method offers superior health monitoring effectiveness and robustness
compared to baseline models.

æè¦ï¼æ¨å°æè½æ©å¨çå¥åº·ææ¨å°æ¼å¶ç¶­è­·è³ééè¦ãç¶èï¼éåéç¨å°æ®éæ¡ç¨çæºè½æ¹æ³ä¾èªªå·æææ°æ§ï¼å çºå®åå¯è½ææ¡ç¨æ´åè³æåä½ï¼ä¸åæå¼å¥éè¨å¹²æ¾ï¼èä¸ç¼ºä¹å¯è§£éæ§ãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäºä¸ç¨®åºæ¼æ´æ£çå¼±ç£ç£å¼æ¹æ³ï¼ç¨æ¼æ¨å°æè½æ©å¨çå¥åº·ææ¨ï¼å¯¦ç¾æ©ææéæª¢æ¸¬åçææ¼è®çæçºç£æ§ãéç¨®æ¹æ³ä¾è³´æ¼ä½¿ç¨å¥åº·æ¨£æ¬åä¸äºç°å¸¸å¼è¨ç·´çç¡åé¡å¨æ´æ£æ¨¡åãéåæ¨¡åæç¢çå¥åº·æ¨£æ¬ãä¸¦ä¸ééæ¯è¼å°å¥è­ä¸­åå§æ¨£æ¬åçææ¨£æ¬ä¹éçå·®ç°ï¼æåæ§å»ºäºä¸åç°å¸¸åï¼å¯ä»¥æ¸æ¥å°è­å¥æéãç¶å¾æ¨å°åºå¥åº·ææ¨ï¼å¯ä»¥è§£éæéé¡åä¸¦æ¸è¼éè¨å¹²æ¾ãå°å©åæ¡ä¾çæ¯è¼ç ç©¶è¡¨æï¼èåºæºæ¨¡åç¸æ¯ï¼ææåºçæ¹æ³æä¾äºåè¶çå¥åº·ç£æ§æææ§åé­¯æ£æ§ã

##### **A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**
2409.02145v1 by Zekang Yang, Hong Liu, Xiangdong Wang

Computer-aided cancer survival risk prediction plays an important role in the
timely treatment of patients. This is a challenging weakly supervised ordinal
regression task associated with multiple clinical factors involved such as
pathological images, genomic data and etc. In this paper, we propose a new
training method, multimodal object-level contrast learning, for cancer survival
risk prediction. First, we construct contrast learning pairs based on the
survival risk relationship among the samples in the training sample set. Then
we introduce the object-level contrast learning method to train the survival
risk predictor. We further extend it to the multimodal scenario by applying
cross-modal constrast. Considering the heterogeneity of pathological images and
genomics data, we construct a multimodal survival risk predictor employing
attention-based and self-normalizing based nerural network respectively.
Finally, the survival risk predictor trained by our proposed method outperforms
state-of-the-art methods on two public multimodal cancer datasets for survival
risk prediction.

æè¦ï¼é»è¦è¼å©ççå­æ´»é¢¨éªé æ¸¬å¨çæ£çåææ²»çä¸­æ®æ¼èéè¦çè§è²ãéæ¯ä¸åå°é£çå¼±ç£ç£åºæ¸åæ­¸ä»»åï¼èå¤éè¨åºå ç´ æéï¼ä¾å¦ççååãåºå çµæ¸æç­ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°çè¨ç·´æ¹æ³ï¼å¤æ¨¡æç©ä»¶å±¤ç´å°æ¯å­¸ç¿ï¼ç¨æ¼ççå­æ´»é¢¨éªé æ¸¬ãé¦åï¼æåæ ¹æè¨ç·´æ¨£æ¬éä¸­æ¨£æ¬ä¹éçå­æ´»é¢¨éªéä¿å»ºç«å°æ¯å­¸ç¿å°ãæ¥èï¼æåå¼å¥ç©ä»¶å±¤ç´å°æ¯å­¸ç¿æ¹æ³ä¾è¨ç·´å­æ´»é¢¨éªé æ¸¬å¨ãæåé²ä¸æ­¥å°å¶å»¶ä¼¸è³å¤æ¨¡æå ´æ¯ï¼ééæç¨è·¨æ¨¡æå°æ¯ãèéå°ççååååºå é«æ¸æçç°è³ªæ§ï¼æååå¥æ¡ç¨åºæ¼æ³¨æåçåèªæ¨æºåçç¥ç¶ç¶²è·¯ä¾å»ºæ§å¤æ¨¡æå­æ´»é¢¨éªé æ¸¬å¨ãæå¾ï¼æåæåºçæ¹æ³æè¨ç·´çå­æ´»é¢¨éªé æ¸¬å¨å¨å©åå¬éçå¤æ¨¡æççè³æéä¸ï¼å¨å­æ´»é¢¨éªé æ¸¬æ¹é¢åªæ¼æåé²çæ¹æ³ã

##### **A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**
2409.01596v1 by Ruben D. Fonnegra, Maria Liliana HernÃ¡ndez, Juan C. Caicedo, Gloria M. DÃ­az

Contrast-enhancement pattern analysis is critical in breast magnetic
resonance imaging (MRI) to distinguish benign from probably malignant tumors.
However, contrast-enhanced image acquisitions are time-consuming and very
expensive. As an alternative to physical acquisition, this paper proposes a
comprehensive pipeline for the generation of accurate long-term (late)
contrast-enhanced breast MRI from the early counterpart. The proposed strategy
focuses on preserving the contrast agent pattern in the enhanced regions while
maintaining visual properties in the entire synthesized images. To that end, a
novel loss function that leverages the biological behavior of contrast agent
(CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed
to optimize a pixel-attention based generative model. In addition, unlike
traditional normalization and standardization methods, we developed a new
normalization strategy that maintains the contrast enhancement pattern across
the image sequences at multiple timestamps. This ensures the prevalence of the
CA pattern after image preprocessing, unlike conventional approaches.
Furthermore, in order to objectively evaluate the clinical quality of the
synthesized images, two metrics are also introduced to measure the differences
between the TI curves of enhanced regions of the acquired and synthesized
images. The experimental results showed that the proposed strategy generates
images that significantly outperform diagnostic quality in contrast-enhanced
regions while maintaining the spatial features of the entire image. This
results suggest a potential use of synthetic late enhanced images generated via
deep learning in clinical scenarios.

æè¦ï¼å°æ¯å¢å¼·æ¨¡å¼åæå¨ä¹³æ¿ç£å±æ¯å½±å (MRI) ä¸­è³ééè¦ï¼å¯ç¨æ¼ååè¯æ§è«ç¤åå¯è½æ¯æ¡æ§è«ç¤ã
ç¶èï¼å°æ¯å¢å¼·å½±åçæ·åéå¸¸èæä¸æè²´ãä½çºç©çæ·åçæ¿ä»£æ¹æ¡ï¼æ¬ææåºäºä¸åå¨é¢çç®¡éï¼ç¨æ¼å¾æ©æå°æç©çææºç¢ºçé·æï¼ææï¼å°æ¯å¢å¼·ä¹³æ¿ MRIãææåºçç­ç¥èéæ¼å¨å¢å¼·ååä¸­ä¿çå°æ¯åæ¨¡å¼ï¼åæå¨æ´ååæå½±åä¸­ç¶­æè¦è¦ºå±¬æ§ãçºæ­¤ï¼æåºäºä¸ç¨®æ°ç©çæå¤±å½æ¸ï¼å©ç¨å°æ¯å (CA) å¨çµç¹ä¸­ççç©è¡çºï¼ç±æéå¼·åº¦ (TI) å¢å¼·æ²ç·çµ¦åºï¼ï¼ä»¥æä½³ååºæ¼åç´ æ³¨æåççææ¨¡åãæ­¤å¤ï¼èå³çµ±çæ­£è¦ååæ¨æºåæ¹æ³ä¸åï¼æåéç¼äºä¸ç¨®æ°çæ­£è¦åç­ç¥ï¼å¯å¨å¤åæéæ³çå½±ååºåä¸­ç¶­æå°æ¯å¢å¼·æ¨¡å¼ãéç¢ºä¿äºå½±ååèçå¾ CA æ¨¡å¼çæ®éæ§ï¼éèå³çµ±æ¹æ³ä¸åãæ­¤å¤ï¼çºäºå®¢è§è©ä¼°åæå½±åçè¨åºåè³ªï¼éå¼å¥äºå©åææ¨ä¾æ¸¬éæ·åååæå½±åçå¢å¼·ååç TI æ²ç·ä¹éçå·®ç°ãå¯¦é©çµæé¡¯ç¤ºï¼ææåºçç­ç¥ç¢ççå½±åå¨å°æ¯å¢å¼·ååä¸­çè¨ºæ·åè³ªæé¡¯åªæ¼å¶ä»å½±åï¼åæç¶­æäºæ´åå½±åçç©ºéç¹å¾µãéäºçµæè¡¨æï¼å¨è¨åºå ´æ¯ä¸­ï¼ééæ·±åº¦å­¸ç¿çæçåæææå¢å¼·å½±åå·ææ½å¨ç¨éã

##### **Kvasir-VQA: A Text-Image Pair GI Tract Dataset**
2409.01437v1 by Sushant Gautam, Andrea StorÃ¥s, Cise Midoglu, Steven A. Hicks, Vajira Thambawita, PÃ¥l Halvorsen, Michael A. Riegler

We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and
Kvasir-Instrument datasets, augmented with question-and-answer annotations to
facilitate advanced machine learning tasks in Gastrointestinal (GI)
diagnostics. This dataset comprises 6,500 annotated images spanning various GI
tract conditions and surgical instruments, and it supports multiple question
types including yes/no, choice, location, and numerical count. The dataset is
intended for applications such as image captioning, Visual Question Answering
(VQA), text-based generation of synthetic medical images, object detection, and
classification. Our experiments demonstrate the dataset's effectiveness in
training models for three selected tasks, showcasing significant applications
in medical image analysis and diagnostics. We also present evaluation metrics
for each task, highlighting the usability and versatility of our dataset. The
dataset and supporting artifacts are available at
https://datasets.simula.no/kvasir-vqa.

æè¦ï¼æåå¼é² Kvasir-VQAï¼ä¸åç± HyperKvasir å Kvasir-Instrument è³æéè¡ççå»¶ä¼¸è³æéï¼ä¸¦å å¥åé¡èè§£ç­è¨»è§£ï¼ä»¥ä¿é²å¨èè¸ (GI) è¨ºæ·ä¸­çé²éæ©å¨å­¸ç¿ä»»åãæ­¤è³æéåå« 6,500 åè¨»è§£å½±åï¼æ¶µèåç¨® GI éçæ³åæè¡å¨æ¢°ï¼ä¸¦ä¸æ¯æ´åæ¬æ¯éé¡ãé¸æé¡ãä½ç½®åæ¸å­è¨æ¸ç­å¤ç¨®é¡åçåé¡ãæ­¤è³æéé©ç¨æ¼å½±åæ¨é¡ãè¦è¦ºåç­ (VQA)ãåæé«å­¸å½±åçæå­çæãç©ä»¶åµæ¸¬ååé¡ç­æç¨ç¨å¼ãæåçå¯¦é©è­ææ­¤è³æéå¨è¨ç·´ä¸åé¸å®ä»»åçæ¨¡åä¸­å·æææï¼å±ç¤ºäºå¨é«å­¸å½±ååæåè¨ºæ·ä¸­éè¦çæç¨ãæåä¹çºæ¯åä»»åæä¾è©ä¼°ææ¨ï¼çªé¡¯æåè³æéçå¯ç¨æ§åå¤åè½æ§ãæ­¤è³æéåæ¯æ´å·¥ä»¶å¯æ¼ https://datasets.simula.no/kvasir-vqa åå¾ã

##### **EEG-Language Modeling for Pathology Detection**
2409.07480v1 by Sam Gijsen, Kerstin Ritter

Multimodal language modeling constitutes a recent breakthrough which
leverages advances in large language models to pretrain capable multimodal
models. The integration of natural language during pretraining has been shown
to significantly improve learned representations, particularly in computer
vision. However, the efficacy of multimodal language modeling in the realm of
functional brain data, specifically for advancing pathology detection, remains
unexplored. This study pioneers EEG-language models trained on clinical reports
and 15000 EEGs. We extend methods for multimodal alignment to this novel domain
and investigate which textual information in reports is useful for training
EEG-language models. Our results indicate that models learn richer
representations from being exposed to a variety of report segments, including
the patient's clinical history, description of the EEG, and the physician's
interpretation. Compared to models exposed to narrower clinical text
information, we find such models to retrieve EEGs based on clinical reports
(and vice versa) with substantially higher accuracy. Yet, this is only observed
when using a contrastive learning approach. Particularly in regimes with few
annotations, we observe that representations of EEG-language models can
significantly improve pathology detection compared to those of EEG-only models,
as demonstrated by both zero-shot classification and linear probes. In sum,
these results highlight the potential of integrating brain activity data with
clinical text, suggesting that EEG-language models represent significant
progress for clinical applications.

æè¦ï¼å¤æ¨¡æè¯­è¨å»ºæ¨¡æ¯ä¸é¡¹æè¿ççªç ´ï¼å®å©ç¨å¤§åè¯­è¨æ¨¡åçè¿æ­¥æ¥é¢è®­ç»æè½åçå¤æ¨¡ææ¨¡åãäºå®è¯æï¼å¨é¢è®­ç»æé´æ´åèªç¶è¯­è¨å¯ä»¥æ¾èæ¹åå­¦ä¹ å°çè¡¨å¾ï¼ç¹å«æ¯å¨è®¡ç®æºè§è§ä¸­ãç¶èï¼å¤æ¨¡æè¯­è¨å»ºæ¨¡å¨åè½æ§èæ°æ®é¢åä¸­çåæï¼ç¹å«æ¯å¯¹äºæ¨è¿ççæ£æµï¼ä»ç¶æªå¾å°æ¢ç´¢ãæ¬ç ç©¶å¼åäºå¨ä¸´åºæ¥åå 15000 ä¸ªèçµå¾ä¸è®­ç»çèçµå¾è¯­è¨æ¨¡åãæä»¬å°å¤æ¨¡æå¯¹é½çæ¹æ³æ©å±å°è¿ä¸ªæ°é¢åï¼å¹¶ç ç©¶æ¥åä¸­çåªäºææ¬ä¿¡æ¯å¯¹äºè®­ç»èçµå¾è¯­è¨æ¨¡åæ¯æç¨çãæä»¬çç»æè¡¨æï¼éè¿æ¥è§¦åç§æ¥åçæ®µï¼åæ¬æ£èççå²ãèçµå¾æè¿°åå»ççè§£éï¼ï¼æ¨¡åå¯ä»¥å­¦ä¹ å°æ´ä¸°å¯çè¡¨å¾ãä¸æ¥è§¦å°è¾çªçä¸´åºææ¬ä¿¡æ¯çæ¨¡åç¸æ¯ï¼æä»¬åç°æ­¤ç±»æ¨¡åå¯ä»¥æ ¹æ®ä¸´åºæ¥åï¼åä¹äº¦ç¶ï¼æ£ç´¢èçµå¾ï¼å¶åç¡®æ§å¤§å¤§æé«ãç¶èï¼åªæå¨ä½¿ç¨å¯¹æ¯å­¦ä¹ æ¹æ³æ¶æä¼è§å¯å°è¿ä¸ç¹ãç¹å«æ¯å¨æ³¨éè¾å°çæ¹æ¡ä¸­ï¼æä»¬è§å¯å°èçµå¾è¯­è¨æ¨¡åçè¡¨å¾å¯ä»¥æ¾èæ¹åççæ£æµï¼ä¸ä»èçµå¾æ¨¡åç¸æ¯ï¼é¶æ ·æ¬åç±»åçº¿æ§æ¢éé½è¯æäºè¿ä¸ç¹ãæ»ä¹ï¼è¿äºç»æçªåºäºå°èæ´»å¨æ°æ®ä¸ä¸´åºææ¬ç¸ç»åçæ½åï¼è¡¨æèçµå¾è¯­è¨æ¨¡åä»£è¡¨äºä¸´åºåºç¨çéå¤§è¿å±ã

##### **SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**
2409.01013v1 by Mevan Ekanayake, Zhifeng Chen, Gary Egan, Mehrtash Harandi, Zhaolin Chen

Implicit Neural Representations (INRs) have recently advanced the field of
deep learning due to their ability to learn continuous representations of
signals without the need for large training datasets. Although INR methods have
been studied for medical image super-resolution, their adaptability to
localized priors in medical images has not been extensively explored. Medical
images contain rich anatomical divisions that could provide valuable local
prior information to enhance the accuracy and robustness of INRs. In this work,
we propose a novel framework, referred to as the Semantically Conditioned INR
(SeCo-INR), that conditions an INR using local priors from a medical image,
enabling accurate model fitting and interpolation capabilities to achieve
super-resolution. Our framework learns a continuous representation of the
semantic segmentation features of a medical image and utilizes it to derive the
optimal INR for each semantic region of the image. We tested our framework
using several medical imaging modalities and achieved higher quantitative
scores and more realistic super-resolution outputs compared to state-of-the-art
methods.

æè¦ï¼é±å¼ç¥ç¶è¡¨å¾µ (INR) è¿æç±æ¼å¶ç¡éå¤§éè¨ç·´è³æéå°±è½å­¸ç¿è¨èçé£çºè¡¨å¾µçè½åï¼èæ¨åäºæ·±åº¦å­¸ç¿é åçé²å±ãåç®¡ INR æ¹æ³å·²è¢«ç ç©¶ç¨æ¼é«å­¸å½±åè¶è§£æåº¦ï¼ä½å¶å°æ¼é«å­¸å½±åä¸­å±é¨åé©çé©ææ§å°æªè¢«å»£æ³æ¢è¨ãé«å­¸å½±ååå«è±å¯çè§£åå­¸ååï¼éäºååå¯ä»¥æä¾æå¹å¼çå±é¨åé©è³è¨ï¼ä»¥å¢å¼· INR çæºç¢ºæ§åç©©å¥æ§ãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸åæ°ç©çæ¶æ§ï¼ç¨±çºèªç¾©æ¢ä»¶ INR (SeCo-INR)ï¼å®ä½¿ç¨é«å­¸å½±åä¸­çå±é¨åé©ä¾èª¿æ´ INRï¼å¯¦ç¾æºç¢ºçæ¨¡åæ¬ååæå¼è½åï¼ä»¥å¯¦ç¾è¶è§£æåº¦ãæåçæ¶æ§å­¸ç¿é«å­¸å½±åçèªæåå²ç¹å¾µçé£çºè¡¨å¾µï¼ä¸¦å©ç¨å®çºå½±åçæ¯åèªæååæ¨å°æä½³ INRãæåä½¿ç¨å¤ç¨®é«å­¸å½±åæ¹å¼æ¸¬è©¦æåçæ¶æ§ï¼ä¸¦èæåé²çæ¹æ³ç¸æ¯ï¼éå°äºæ´é«çéåè©ååæ´é¼ççè¶è§£æåº¦è¼¸åºã

##### **Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**
2409.00873v1 by Sajib Acharjee Dip, Kazi Hasan Ibn Arif, Uddip Acharjee Shuvo, Ishtiaque Ahmed Khan, Na Meng

In the realm of dermatology, the complexity of diagnosing skin conditions
manually necessitates the expertise of dermatologists. Accurate identification
of various skin ailments, ranging from cancer to inflammatory diseases, is
paramount. However, existing artificial intelligence (AI) models in dermatology
face challenges, particularly in accurately diagnosing diseases across diverse
skin tones, with a notable performance gap in darker skin. Additionally, the
scarcity of publicly available, unbiased datasets hampers the development of
inclusive AI diagnostic tools. To tackle the challenges in accurately
predicting skin conditions across diverse skin tones, we employ a
transfer-learning approach that capitalizes on the rich, transferable knowledge
from various image domains. Our method integrates multiple pre-trained models
from a wide range of sources, including general and specific medical images, to
improve the robustness and inclusiveness of the skin condition predictions. We
rigorously evaluated the effectiveness of these models using the Diverse
Dermatology Images (DDI) dataset, which uniquely encompasses both
underrepresented and common skin tones, making it an ideal benchmark for
assessing our approach. Among all methods, Med-ViT emerged as the top performer
due to its comprehensive feature representation learned from diverse image
sources. To further enhance performance, we conducted domain adaptation using
additional skin image datasets such as HAM10000. This adaptation significantly
improved model performance across all models.

æè¦ï¼<paragraph>å¨ç®è¤çå­¦é¢åï¼äººå·¥è¯æ­ç®è¤ç¶åµçå¤ææ§éè¦ç®è¤ç§å»å¸çä¸ä¸ç¥è¯ãä»ççå°ççæ§ç¾çï¼å¯¹åç§ç®è¤ç¾ççåç¡®è¯å«è³å³éè¦ãç¶èï¼ç°æçç®è¤çå­¦äººå·¥æºè½ (AI) æ¨¡åé¢ä¸´ææï¼å°¤å¶æ¯å¨åç¡®è¯æ­ä¸åè¤è²çç¾çæ¶ï¼å¨è¾æ·±çè¤è²ä¸å­å¨ææ¾çæ§è½å·®è·ãæ­¤å¤ï¼å¬å¼å¯ç¨çæ åæ°æ®éçç¨ç¼ºæ§é»ç¢äºåå®¹æ§ AI è¯æ­å·¥å·çå¼åãä¸ºäºåºå¯¹åç¡®é¢æµä¸åè¤è²ç®è¤ç¶åµçææï¼æä»¬éç¨äºä¸ç§è¿ç§»å­¦ä¹ æ¹æ³ï¼è¯¥æ¹æ³å©ç¨äºæ¥èªåç§å¾ååçä¸°å¯å¯è½¬ç§»ç¥è¯ãæä»¬çæ¹æ³éæäºæ¥èªå¹¿æ³æ¥æºçå¤ä¸ªé¢è®­ç»æ¨¡åï¼åæ¬ä¸è¬åç¹å®çå»å­¦å¾åï¼ä»¥æé«ç®è¤ç¶åµé¢æµçç¨³å¥æ§ååå®¹æ§ãæä»¬ä½¿ç¨ Diverse Dermatology Images (DDI) æ°æ®éä¸¥æ ¼è¯ä¼°äºè¿äºæ¨¡åçæææ§ï¼è¯¥æ°æ®éç¬ç¹å°åå«äºä»£è¡¨æ§ä¸è¶³åå¸¸è§çè¤è²ï¼ä½¿å¶æä¸ºè¯ä¼°æä»¬æ¹æ³ççæ³åºåãå¨æææ¹æ³ä¸­ï¼Med-ViT ç±äºå¶ä»åç§å¾åæ¥æºä¸­å­¦å°çç»¼åç¹å¾è¡¨ç¤ºèæä¸ºè¡¨ç°æå¥½çæ¹æ³ãä¸ºäºè¿ä¸æ­¥æé«æ§è½ï¼æä»¬ä½¿ç¨ HAM10000 ç­å¶ä»ç®è¤å¾åæ°æ®éè¿è¡äºåéåºãè¿ç§éåºæ¾çæé«äºæææ¨¡åçæ¨¡åæ§è½ã</paragraph>

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ç¶å¸¸ç¼ºä¹ç¹å®é åçç¥è­ï¼å³ä½¿ç¶éå¾®èª¿çæ¨¡åä¹å®¹æç¢çå¹»è¦ºãå æ­¤ï¼éè¦æ´å¤å¯é çæ¨¡åä¾ç´å¥å¤é¨ç¥è­ãæåæåºäºä¸åæµç¨ 4StepFocusï¼ç¹å¥æ¯é èçæ­¥é©ï¼å¯ä»¥å¤§å¹æ¹å LLM çç­æ¡ãéæ¯ééæä¾åå¼å°çå¤é¨ç¥è­å­åï¼å©ç¨æ¨¡åèªè¡æ·åéè¯æ§èçµ¡åé²è¡åºæ¬æ¨ççè½åä¾å¯¦ç¾çãæ­¤æ¹æ³ééå¨åçµæ§åç¥è­åº«ä¸­é²è¡åºæ¼ä¸åçµçæå°ï¼ä»¥ç´æ¥ä¸å¯è¿½è¹¤çæ¹å¼ç¸®å°æ½å¨æ­£ç¢ºç­æ¡çç¯åï¼ç¶å¾ååæå°æ½å¨è¡¨å¾µï¼æ ¹æéçµæ§åè³æå°éäºåé¸ç­æ¡é²è¡æåãéèç´ç²¹åºæ¼æ½å¨è¡¨å¾µçç¸éæ¹æ³ææåå¥ã4StepFocus åå«ä»¥ä¸æ­¥é©ï¼1) ç± LLM é²è¡ä¸åçµç¢çä»¥æ·åéè¯è³æï¼2) å¨éäºä¸åçµä¸­æ¿æè®æ¸ï¼ä»¥æ¡ç¨ç¥è­åè¡¨ç¸®å°ç­æ¡åé¸ç¯åï¼3) ä½¿ç¨æ¶åéè¯éçµæ§åè³æçåéç¸ä¼¼æ§æå°å°å©é¤åé¸ç­æ¡é²è¡æåºï¼4) ç± LLM éæ°å°æä½³åé¸ç­æ¡é²è¡æåï¼ä¸¦æä¾èæ¯è³æãå¨é«çãç¢åæ¨è¦åå­¸è¡è«ææå°æ¸¬è©¦éä¸­é²è¡çå¯¦é©è­æï¼éç¨®æ¹æ³ç¢ºå¯¦æ¯ä¸ç¨®å¼·å¤§çæ´åãå®ä¸åå¢å äºä¾èªè³è¨æª¢ç´¢çç¸å³å¯è¿½è¹¤èæ¯è³è¨ï¼èä¸èæåé²çæ¹æ³ç¸æ¯ï¼ä¹å¤§å¹æåäºæè½ãæ¬ææåºäºä¸åæ°ç©ä¸é®®å°æ¢ç´¢çæ¹åï¼å æ­¤æä¾äºå»£æ³çæªä¾å·¥ä½æ©æãä½¿ç¨çåå§ç¢¼å¯å¨ https://github.com/kramerlab/4StepFocus åå¾ã

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

æè¦ï¼æ¬ææåºäºä¸åç¥è­å·¥ç¨åå¤èªè¨èªç¾©æ¨çæè¡çæ¬ä½è¨­è¨ï¼ç¨æ¼å»ºç«ä¸åèªååç³»çµ±ï¼ä»¥ç¥è­åè­çå½¢å¼å¸æ¶å°åº¦æççç¹é£ªè³è¨ãéé»å¨æ¼è¨­è¨æºæ§æ¹æ³ï¼ä»¥æ¨å°æ¬ä½è¨­è¨ï¼ä¸¦å¨é¢æ·åéæ¼é£ç©ãé£è­ãé£æãç¹é£ªç¹æ§ï¼ä»¥åæéè¦ççé¤çç¥è­ï¼ä¸¦æ´å¤§è¦æ¨¡ãæåå¨éåç è¨æè«æä¸­ä»ç´¹äºæåæ­£å¨é²è¡çå·¥ä½ï¼è©³ç´°æè¿°äºæ´çå°åº¦æçç¥è­ç¸éçææ°ï¼ä¸¦æåºäºæåçé«éæ¬ä½è¨­è¨ãæåä¹æåºäºä¸ç¨®æ°çå·¥ä½æµç¨ï¼å®ä½¿ç¨ AIãLLM åèªè¨æè¡ï¼å¾å¬å±é åçé£è­é¨è½æ ¼ç¶²ç«ä¸­æ´çè³è¨ï¼ä»¥å»ºç«å°åº¦æççç¥è­åè­ãæ¬ææåºçç¥è­æ´çæ¹æ³æ¯éç¨çï¼å¯ä»¥è¤è£½å°ä»»ä½é åãè¨­è¨èæç¨ç¡éï¼å¯ç¨æ¼ AI é©åçæºæ§åæãå»ºç«åäººåæ¸ä½å¥åº·æ¨è¦ç³»çµ±ï¼ä»¥åä½¿ç¨ä½¿ç¨èè³è¨ãé£ç©çç©åå­¸ãå°çè³è¨ãè¾²æ¥­è³è¨ç­èçµ¡è³è¨ï¼ä¾è£åå°åº¦æççç¥è­åè­ã

##### **AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**
2409.00735v1 by Mahsa Khosravi, Matthew Carroll, Kai Liang Tan, Liza Van der Laan, Joscif Raigne, Daren S. Mueller, Arti Singh, Aditya Balu, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar

Agricultural production requires careful management of inputs such as
fungicides, insecticides, and herbicides to ensure a successful crop that is
high-yielding, profitable, and of superior seed quality. Current
state-of-the-art field crop management relies on coarse-scale crop management
strategies, where entire fields are sprayed with pest and disease-controlling
chemicals, leading to increased cost and sub-optimal soil and crop management.
To overcome these challenges and optimize crop production, we utilize machine
learning tools within a virtual field environment to generate localized
management plans for farmers to manage biotic threats while maximizing profits.
Specifically, we present AgGym, a modular, crop and stress agnostic simulation
framework to model the spread of biotic stresses in a field and estimate yield
losses with and without chemical treatments. Our validation with real data
shows that AgGym can be customized with limited data to simulate yield outcomes
under various biotic stress conditions. We further demonstrate that deep
reinforcement learning (RL) policies can be trained using AgGym for designing
ultra-precise biotic stress mitigation strategies with potential to increase
yield recovery with less chemicals and lower cost. Our proposed framework
enables personalized decision support that can transform biotic stress
management from being schedule based and reactive to opportunistic and
prescriptive. We also release the AgGym software implementation as a community
resource and invite experts to contribute to this open-sourced publicly
available modular environment framework. The source code can be accessed at:
https://github.com/SCSLabISU/AgGym.

æè¦ï¼è¾²æ¥­çç¢éè¦å°å¿ç®¡çè¼¸å¥ï¼ä¾å¦æ®ºèåãæ®ºè²ååé¤èåï¼ä»¥ç¢ºä¿ä½ç©æåãé«ç¢ãæå©å¯åä¸å·æåªè¯çç¨®å­åè³ªãç®åæåé²çç°éä½ç©ç®¡çä¾è³´æ¼ç²ç¥çä½ç©ç®¡çç­ç¥ï¼å¶ä¸­æ´åç°å°é½å´çäºæ§å¶çè²å®³çåå­¸ç©è³ªï¼å°è´ææ¬å¢å ååå£¤åä½ç©ç®¡çä¸ä½³ãçºäºåæéäºææ°ä¸¦åªåä½ç©çç¢ï¼æåå¨èæ¬ç°éç°å¢ä¸­å©ç¨æ©å¨å­¸ç¿å·¥å·çºè¾²æ°çæå±é¨ç®¡çè¨ç«ï¼ä»¥ç®¡ççç©å¨èä¸¦åææå¤§åå©æ½¤ãå·é«ä¾èªªï¼æåæåºäº AgGymï¼ä¸åæ¨¡çµåãä½ç©åå£åä¸å¯ç¥çæ¨¡æ¬æ¶æ§ï¼ç¨æ¼æ¨¡æ¬ç°éçç©å£åçæ´æ£ï¼ä¸¦ä¼°ç®æåæ²æåå­¸èççç¢éæå¤±ãæåä½¿ç¨çå¯¦æ¸æé²è¡é©è­ï¼é¡¯ç¤º AgGym å¯ä»¥ä½¿ç¨æéçæ¸æé²è¡èªè¨ï¼ä»¥æ¨¡æ¬åç¨®çç©å£åæ¢ä»¶ä¸çç¢éçµæãæåé²ä¸æ­¥è­æï¼æ·±åº¦å¼·åå­¸ç¿ (RL) æ¿ç­å¯ä»¥ä½¿ç¨ AgGym é²è¡è¨ç·´ï¼ä»¥è¨­è¨è¶ç²¾ç¢ºççç©å£åç·©è§£ç­ç¥ï¼ä¸¦æå¯è½ä»¥æ´å°çåå­¸ç©è³ªåæ´ä½çææ¬å¢å ç¢éæ¢å¾©ãæåæåºçæ¶æ§åç¨äºåäººåæ±ºç­æ¯æ´ï¼å¯ä»¥å°çç©å£åç®¡çå¾åºæ¼æéè¡¨åè¢«åè½è®çºæ©æä¸»ç¾©åè¦ç¯æ§ãæåéå° AgGym è»é«å¯¦ä½ä½çºç¤¾åè³æºéåºï¼ä¸¦éè«å°å®¶çºéåéæ¾åå§ç¢¼ä¸å¬éå¯ç¨çæ¨¡çµåç°å¢æ¶æ§ååºè²¢ç»ãå¯ä»¥å¨ä»¥ä¸ä½ç½®åå¾åå§ç¢¼ï¼https://github.com/SCSLabISU/AgGymã

##### **LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**
2409.00726v1 by Zhaojie Fang, Xiao Yu, Guanyu Zhou, Ke Zhuang, Yifei Chen, Ruiquan Ge, Changmiao Wang, Gangyong Jia, Qing Wu, Juan Ye, Maimaiti Nuliqiman, Peifang Xu, Ahmed Elazab

Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise
identification of ocular diseases using sodium fluorescein, which can be
potentially harmful. Existing research has developed methods to generate UWF-FA
from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the
adverse reactions associated with injections. However, these methods have been
less effective in producing high-quality late-phase UWF-FA, particularly in
lesion areas and fine details. Two primary challenges hinder the generation of
high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and
early/late-phase UWF-FA datasets, and the need for realistic generation at
lesion sites and potential blood leakage regions. This study introduces an
improved latent diffusion model framework to generate high-quality late-phase
UWF-FA from limited paired UWF images. To address the challenges as mentioned
earlier, our approach employs a module utilizing Cross-temporal Regional
Difference Loss, which encourages the model to focus on the differences between
early and late phases. Additionally, we introduce a low-frequency enhanced
noise strategy in the diffusion forward process to improve the realism of
medical images. To further enhance the mapping capability of the variational
autoencoder module, especially with limited datasets, we implement a Gated
Convolutional Encoder to extract additional information from conditional
images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein
Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase
UWF-FA and achieves state-of-the-art results compared to other existing methods
when working with limited datasets. Our source code is available at:
https://github.com/Tinysqua/****.

æè¦ï¼è¶å»£è§è¢åè¡ç®¡é å½±ï¼UWF-FAï¼ä½¿ç¨å¯è½å·ææ½å¨å±å®³çéè¢åç´ ï¼å¯ç²¾ç¢ºè­å¥ç¼ç¾ãç¾æç ç©¶å·²éç¼åºå¾è¶å»£è§ææé·å°ç¼ç§é¡ï¼UWF-SLOï¼ç¢ç UWF-FA çæ¹æ³ï¼ä»¥æ¸å°èæ³¨å°ç¸éçä¸è¯åæãç¶èï¼éäºæ¹æ³å¨ç¢çé«åè³ªçå¾æ UWF-FA æ¹é¢ææè¼å·®ï¼ç¹å¥æ¯å¨çç¶åååç²¾ç´°ç´°ç¯æ¹é¢ãç¢çé«åè³ªå¾æ UWF-FA é¢è¨å©é ä¸»è¦ææ°ï¼éå°ç UWF-SLO åæ©æ/å¾æ UWF-FA è³æéç¨å°ï¼ä»¥åéè¦å¨çç¶é¨ä½åæ½å¨åºè¡ååé²è¡é¼ççç¢çãæ¬ç ç©¶å¼é²ä¸ç¨®æ¹è¯çæ½å¨æ´æ£æ¨¡åæ¶æ§ï¼å¾æééå°ç UWF å½±åç¢çé«åè³ªçå¾æ UWF-FAãçºäºæå°åé¢æå°çææ°ï¼æåçæ¹æ³æ¡ç¨ä¸åæ¨¡çµï¼å©ç¨è·¨æéååå·®ç°æå¤±ï¼é¼åµæ¨¡åå°æ³¨æ¼æ©æåå¾æä¹éçå·®ç°ãæ­¤å¤ï¼æåå¨æ´æ£ååéç¨ä¸­å¼é²ä¸ç¨®ä½é »å¢å¼·éè¨ç­ç¥ï¼ä»¥æ¹åé«å­¸å½±åççå¯¦æ§ãçºäºé²ä¸æ­¥å¢å¼·è®ç°èªåç·¨ç¢¼å¨æ¨¡çµçå°æè½åï¼ç¹å¥æ¯å¨è³æéæéçææ³ä¸ï¼æåå¯¦ä½ä¸åéæ§å·ç©ç·¨ç¢¼å¨ï¼å¾æ¢ä»¶å½±åä¸­èåé¡å¤è³è¨ãæåéå°è¶å»£è§å¾æè¢åè¡ç®¡é å½±ï¼LPUWF-LDMï¼çæ½å¨æ´æ£æ¨¡åææéå»ºå¾æ UWF-FA ä¸­çç²¾ç´°ç´°ç¯ï¼ä¸¦å¨ä½¿ç¨æéè³æéæï¼èå¶ä»ç¾ææ¹æ³ç¸æ¯ï¼éå°æåé²ççµæãæåçåå§ç¢¼å¯å¨ä»¥ä¸ç¶²ååå¾ï¼
https://github.com/Tinysqua/****ã

##### **BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**
2409.00724v1 by Shams Nafisa Ali, Afia Zahin, Samiul Based Shuvo, Nusrat Binta Nizam, Shoyad Ibn Sabur Khan Nuhash, Sayeed Sajjad Razin, S. M. Sakeef Sani, Farihin Rahman, Nawshad Binta Nizam, Farhat Binte Azam, Rakib Hossen, Sumaiya Ohab, Nawsabah Noor, Taufiq Hasan

Cardiac auscultation, an integral tool in diagnosing cardiovascular diseases
(CVDs), often relies on the subjective interpretation of clinicians, presenting
a limitation in consistency and accuracy. Addressing this, we introduce the
BUET Multi-disease Heart Sound (BMD-HS) dataset - a comprehensive and
meticulously curated collection of heart sound recordings. This dataset,
encompassing 864 recordings across five distinct classes of common heart
sounds, represents a broad spectrum of valvular heart diseases, with a focus on
diagnostically challenging cases. The standout feature of the BMD-HS dataset is
its innovative multi-label annotation system, which captures a diverse range of
diseases and unique disease states. This system significantly enhances the
dataset's utility for developing advanced machine learning models in automated
heart sound classification and diagnosis. By bridging the gap between
traditional auscultation practices and contemporary data-driven diagnostic
methods, the BMD-HS dataset is poised to revolutionize CVD diagnosis and
management, providing an invaluable resource for the advancement of cardiac
health research. The dataset is publicly available at this link:
https://github.com/mHealthBuet/BMD-HS-Dataset.

æè¦ï¼å¿èè½è¨ºæ¯è¨ºæ·å¿è¡ç®¡ç¾ç (CVD) çä¸é æ´åå·¥å·ï¼éå¸¸ä¾è³´æ¼è¨åºé«å¸«çä¸»è§è©®éï¼å¨ä¸è´æ§åæºç¢ºæ§æ¹é¢å­å¨éå¶ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº BUET å¤éç¾çå¿é³ (BMD-HS) è³æéï¼éæ¯ä¸åå¨é¢ä¸ç¶éç²¾å¿ç­åçå¿é³éé³è³æéãæ­¤è³æéåå«äºç¨®å¸¸è¦å¿é³ç 864 åéé³ï¼ä»£è¡¨äºå»£æ³çå¿ç£èç¾çï¼éé»å¨æ¼è¨ºæ·å°é£ççä¾ãBMD-HS è³æéççªåºç¹é»æ¯å¶åµæ°çå¤æ¨ç±¤è¨»è§£ç³»çµ±ï¼å®æ¶µèäºåç¨®ç¾çåç¨ç¹çç¾ççæãéåç³»çµ±é¡¯èå¢å¼·äºè³æéå¨éç¼èªåå¿é³åé¡åè¨ºæ·ä¸­é²éæ©å¨å­¸ç¿æ¨¡åçæç¨ãééå½åå³çµ±è½è¨ºå¯¦åèç¶ä»£è³æé©åè¨ºæ·æ¹æ³ä¹éçå·®è·ï¼BMD-HS è³æéæºåå¥½é©æ°å¿è¡ç®¡ç¾ççè¨ºæ·åç®¡çï¼çºå¿èå¥åº·ç ç©¶çé²å±æä¾å¯¶è²´çè³æºãæ­¤è³æéå¯ééä»¥ä¸é£çµå¬éåå¾ï¼https://github.com/mHealthBuet/BMD-HS-Datasetã

##### **Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**
2409.00718v1 by Pragya Gupta, Subhamoy Mandal, Debashree Guha, Debjani Chakraborty

Automatic diagnosis techniques have evolved to identify age-related macular
degeneration (AMD) by employing single modality Fundus images or optical
coherence tomography (OCT). To classify ocular diseases, fundus and OCT images
are the most crucial imaging modalities used in the clinical setting. Most deep
learning-based techniques are established on a single imaging modality, which
contemplates the ocular disorders to a specific extent and disregards other
modality that comprises exhaustive information among distinct imaging
modalities. This paper proposes a modality-specific multiscale color space
embedding integrated with the attention mechanism based on transfer learning
for classification (MCGAEc), which can efficiently extract the distinct
modality information at various scales using the distinct color spaces. In this
work, we first introduce the modality-specific multiscale color space encoder
model, which includes diverse feature representations by integrating distinct
characteristic color spaces on a multiscale into a unified framework. The
extracted features from the prior encoder module are incorporated with the
attention mechanism to extract the global features representation, which is
integrated with the prior extracted features and transferred to the random
forest classifier for the classification of AMD. To analyze the performance of
the proposed MCGAEc method, a publicly available multi-modality dataset from
Project Macula for AMD is utilized and compared with the existing models.

æè¦ï¼èªåè¨ºæ·æè¡å·²æ¼é²å°è½ééä½¿ç¨å®ä¸æ¨¡å¼ç¼åºå½±åæåå­¸ç¸å¹²æ·å±¤ææ (OCT) ä¾è¾¨è­å¹´é½¡ç¸éæ§é»æé¨çè® (AMD)ãçºäºåé¡ç¼ç¾ï¼ç¼åºå OCT å½±åæ¯è¨åºç°å¢ä¸­ä½¿ç¨æééµçå½±åæ¨¡å¼ãå¤§å¤æ¸åºæ¼æ·±åº¦å­¸ç¿çæè¡å»ºç«å¨å®ä¸å½±åæ¨¡å¼ä¸ï¼å®å¨ä¸å®ç¨åº¦ä¸èéäºç¼ç¾ï¼å»å¿½ç¥äºå¶ä»æ¨¡å¼ï¼èå¶ä»æ¨¡å¼åå«äºä¸åå½±åæ¨¡å¼ä¹éçè©³ç¡è³è¨ãæ¬ææåºäºä¸ç¨®æ¨¡å¼ç¹å®çå¤å°ºåº¦è²å½©ç©ºéåµå¥æ´åï¼ä¸¦åºæ¼ç¨æ¼åé¡çè½ç§»å­¸ç¿çæ³¨æåæ©å¶ (MCGAEc)ï¼å®è½ä½¿ç¨ä¸åçè²å½©ç©ºéå¨ä¸åçå°ºåº¦ä¸æææåä¸åçæ¨¡å¼è³è¨ãå¨éé å·¥ä½ä¸­ï¼æåé¦åä»ç´¹äºæ¨¡å¼ç¹å®çå¤å°ºåº¦è²å½©ç©ºéç·¨ç¢¼å¨æ¨¡åï¼å®ééå°ä¸åçç¹å¾µè²å½©ç©ºéæ´åå°å¤å°ºåº¦ä¸­ï¼ä¾ç´å¥ä¸åçç¹å¾µè¡¨å¾µå°ä¸åçµ±ä¸çæ¶æ§ä¸­ãå¾ååçç·¨ç¢¼å¨æ¨¡çµä¸­æåçç¹å¾µèæ³¨æåæ©å¶çµåï¼ä»¥æåå¨åç¹å¾µè¡¨å¾µï¼å®èååæåçç¹å¾µæ´åï¼ä¸¦è½ç§»å°é¨æ©æ£®æåé¡å¨ï¼ä»¥é²è¡ AMD åé¡ãçºäºåæææåºç MCGAEc æ¹æ³çæè½ï¼æåå©ç¨äºä¾èª Project Macula for AMD çå¬éå¤æ¨¡å¼è³æéï¼ä¸¦èç¾ææ¨¡åé²è¡æ¯è¼ã

##### **Curriculum Prompting Foundation Models for Medical Image Segmentation**
2409.00695v1 by Xiuqi Zheng, Yuhang Zhang, Haoran Zhang, Hongrui Liang, Xueqi Bao, Zhuqing Jiang, Qicheng Lao

Adapting large pre-trained foundation models, e.g., SAM, for medical image
segmentation remains a significant challenge. A crucial step involves the
formulation of a series of specialized prompts that incorporate specific
clinical instructions. Past works have been heavily reliant on a singular type
of prompt for each instance, necessitating manual input of an ideally correct
prompt, which is less efficient. To tackle this issue, we propose to utilize
prompts of different granularity, which are sourced from original images to
provide a broader scope of clinical insights. However, combining prompts of
varying types can pose a challenge due to potential conflicts. In response, we
have designed a coarse-to-fine mechanism, referred to as curriculum prompting,
that progressively integrates prompts of different types. Through extensive
experiments on three public medical datasets across various modalities, we
demonstrate the effectiveness of our proposed approach, which not only
automates the prompt generation process but also yields superior performance
compared to other SAM-based medical image segmentation methods. Code is
available at: https://github.com/AnnaZzz-zxq/Curriculum-Prompting.

æè¦ï¼èª¿æ´å¤§åé è¨ç·´åºç¤æ¨¡åï¼ä¾å¦ SAMï¼ä»¥é²è¡é«å­¸å½±ååå²ä»æ¯ä¸é éå¤§ææ°ãééµæ­¥é©æ¶åå¶å®ä¸ç³»ååå«ç¹å®è¨åºèªªæçå°éæç¤ºãéå»çå·¥ä½å¨å¾å¤§ç¨åº¦ä¸ä¾è³´æ¼æ¯åä¾é çå®ä¸æç¤ºé¡åï¼ééè¦æåè¼¸å¥çæ³çæ­£ç¢ºæç¤ºï¼æçè¼ä½ãçºäºè§£æ±ºéååé¡ï¼æåå»ºè­°å©ç¨ä¸åç²åº¦çæç¤ºï¼éäºæç¤ºä¾èªåå§å½±åï¼ä»¥æä¾æ´å»£æ³çè¨åºè¦è§£ãç¶èï¼ç±æ¼æ½å¨è¡çªï¼çµåä¸åé¡åçæç¤ºå¯è½ææ§æææ°ãçºäºè§£æ±ºéååé¡ï¼æåè¨­è¨äºä¸ç¨®ç±ç²å°ç´°çæ©å¶ï¼ç¨±çºèª²ç¨æç¤ºï¼å®éæ­¥æ´åä¸åé¡åçæç¤ºãééå°åç¨®æ¨¡å¼ä¸çä¸åå¬å±é«å­¸è³æéé²è¡å»£æ³çå¯¦é©ï¼æåè­æäºæåæåºçæ¹æ³çæææ§ï¼å®ä¸åèªååæç¤ºçæéç¨ï¼èä¸èå¶ä»åºæ¼ SAM çé«å­¸å½±ååå²æ¹æ³ç¸æ¯ï¼éç¢çäºæ´å¥½çæè½ãç¨å¼ç¢¼å¯å¨ä»¥ä¸ä½ç½®åå¾ï¼https://github.com/AnnaZzz-zxq/Curriculum-Promptingã

##### **Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**
2409.00544v1 by Jacqueline Lammert, Nicole Pfarr, Leonid Kuligin, Sonja Mathes, Tobias Dreyer, Luise Modersohn, Patrick Metzger, Dyke Ferber, Jakob Nikolas Kather, Daniel Truhn, Lisa Christine Adams, Keno Kyrill Bressem, Sebastian Lange, Kristina Schwamborn, Martin Boeker, Marion Kiechle, Ulrich A. Schatz, Holger Bronger, Maximilian Tschochohei

Rare gynecological tumors (RGTs) present major clinical challenges due to
their low incidence and heterogeneity. The lack of clear guidelines leads to
suboptimal management and poor prognosis. Molecular tumor boards accelerate
access to effective therapies by tailoring treatment based on biomarkers,
beyond cancer type. Unstructured data that requires manual curation hinders
efficient use of biomarker profiling for therapy matching. This study explores
the use of large language models (LLMs) to construct digital twins for
precision medicine in RGTs.
  Our proof-of-concept digital twin system integrates clinical and biomarker
data from institutional and published cases (n=21) and literature-derived data
(n=655 publications with n=404,265 patients) to create tailored treatment plans
for metastatic uterine carcinosarcoma, identifying options potentially missed
by traditional, single-source analysis. LLM-enabled digital twins efficiently
model individual patient trajectories. Shifting to a biology-based rather than
organ-based tumor definition enables personalized care that could advance RGT
management and thus enhance patient outcomes.

æè¦ï¼ç½è¦å©¦ç§è«ç¤ (RGT) ç±æ¼å¶ä½ç¼ççåç°è³ªæ§ï¼å°è¨åºå¸¶ä¾éå¤§ææ°ãç¼ºä¹æç¢ºçæå¼å°è´æ¬¡ä½³ç®¡çåä¸è¯é å¾ãåå­è«ç¤å§å¡æééæ ¹æçç©æ¨è¨å®¢è£½åæ²»çï¼å éåå¾ææçæ³ï¼è¶è¶ççé¡åãéè¦æåæ´ççéçµæ§åè³æé»ç¤äºçç©æ¨è¨åæå¨çæ³éå°ä¸­çææä½¿ç¨ãæ¬ç ç©¶æ¢è¨ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) çº RGT çç²¾æºé«çå»ºæ§æ¸ä½éèèã
æåçæ¦å¿µé©è­æ¸ä½éèèç³»çµ±æ´åäºä¾èªæ©æ§åå·²ç¼è¡¨çæ¡ä¾ (n=21) çè¨åºåçç©æ¨è¨è³æï¼ä»¥åä¾èªæç»çè³æ (n=655 ç¯åºçç©ï¼n=404,265 åæ£è)ï¼çºè½ç§»æ§å­å®®èç¤çå¶å®å®¢è£½åæ²»çè¨ç«ï¼æ¾åºå³çµ±å®ä¸ä¾æºåæå¯è½éºæ¼çé¸é ãLLM åç¨çæ¸ä½éèèææå°æ¨¡æ¬åå¥æ£èçè»è·¡ãå¾åºæ¼å¨å®çè«ç¤å®ç¾©è½è®çºåºæ¼çç©å­¸çå®ç¾©ï¼è½å¯¦ç¾åäººåç§è­·ï¼é²èæå RGT ç®¡çä¸¦æ¹åæ£èé å¾ã

##### **Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**
2409.00391v1 by Georgios Ioannides, Adrian Kieback, Aman Chadha, Aaron Elkins

Speech-based depression detection poses significant challenges for automated
detection due to its unique manifestation across individuals and data scarcity.
Addressing these challenges, we introduce DAAMAudioCNNLSTM and
DAAMAudioTransformer, two parameter efficient and explainable models for audio
feature extraction and depression detection. DAAMAudioCNNLSTM features a novel
CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM),
focusing dynamically on informative speech segments. DAAMAudioTransformer,
leveraging a transformer encoder in place of the CNN-LSTM architecture,
incorporates the same DAAM module for enhanced attention and interpretability.
These approaches not only enhance detection robustness and interpretability but
also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro
score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the
DAIC-WOZ dataset, without reliance on supplementary information such as vowel
positions and speaker information during training/validation as in previous
approaches. Both models' significant explainability and efficiency in
leveraging speech signals for depression detection represent a leap towards
more reliable, clinically useful diagnostic tools, promising advancements in
speech and mental health care. To foster further research in this domain, we
make our code publicly available.

æè¦ï¼èªé³åæé¬±æª¢æ¸¬å°èªååæª¢æ¸¬ä¾èªªæ¯ä¸å¤§ææ°ï¼å çºå®å¨ä¸ååé«éçè¡¨ç¾ç¨ç¹ï¼ä¸è³æç¨å°ãçºäºæå°éäºææ°ï¼æåå¼å¥äº DAAMAudioCNNLSTM å DAAMAudioTransformerï¼éå©ååæ¸ææä¸å¯è§£éçæ¨¡åï¼ç¨æ¼é³è¨ç¹å¾µèååæé¬±æª¢æ¸¬ãDAAMAudioCNNLSTM æ¡ç¨åµæ°ç CNN-LSTM æ¶æ§ï¼æ­éå¤é ­å¯åº¦èªé©ææ³¨æåæ©å¶ (DAAM)ï¼åæéæ³¨æ¼ææç¾©çèªé³åæ®µãDAAMAudioTransformer å©ç¨Transformerç·¨ç¢¼å¨åä»£ CNN-LSTM æ¶æ§ï¼ä¸¦ç´å¥ç¸åç DAAM æ¨¡çµï¼ä»¥å¢å¼·æ³¨æååå¯è§£éæ§ãéäºæ¹æ³ä¸åå¢å¼·äºæª¢æ¸¬çç©©å¥æ§åå¯è§£éæ§ï¼ééå°äºæåé²çæè½ï¼DAAMAudioCNNLSTM ç F1 å·¨è§åæ¸çº 0.702ï¼DAAMAudioTransformer å¨ DAIC-WOZ è³æéä¸ç F1 å·¨è§åæ¸çº 0.72ï¼å¨è¨ç·´/é©è­æéä¸ä¾è³´æ¼è¼å©è³è¨ï¼ä¾å¦æ¯é³ä½ç½®åèªªè©±èè³è¨ï¼éèååçåæ³ä¸åãéå©åæ¨¡åå¨å©ç¨èªé³è¨èé²è¡æé¬±æª¢æ¸¬æ¹é¢å·æé¡¯èçå¯è§£éæ§åæçï¼ä»£è¡¨èæåæ´å¯é ãè¨åºä¸æç¨çè¨ºæ·å·¥å·éé²äºä¸å¤§æ­¥ï¼ä¸¦é ç¤ºèèªé³åå¿çä¿å¥çé²æ­¥ãçºäºä¿é²éåé åçé²ä¸æ­¥ç ç©¶ï¼æåå¬éäºæåçç¨å¼ç¢¼ã

##### **Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**
2409.00310v1 by Mikhail Borisenkov, Andrei Velichko, Maksim Belyaev, Dmitry Korzun, Tatyana Tserne, Larisa Bakutova, Denis Gubin

This study investigates machine learning algorithms to identify objective
features for diagnosing food addiction (FA) and assessing confirmed symptoms
(SC). Data were collected from 81 participants (mean age: 21.5 years, range:
18-61 years, women: 77.8%) whose FA and SC were measured using the Yale Food
Addiction Scale (YFAS). Participants provided demographic and anthropometric
data, completed the YFAS, the Zung Self-Rating Depression Scale, and the Dutch
Eating Behavior Questionnaire, and wore an actimeter on the non-dominant wrist
for a week to record motor activity. Analysis of the actimetric data identified
significant statistical and entropy-based features that accurately predicted FA
and SC using ML. The Matthews correlation coefficient (MCC) was the primary
metric. Activity-related features were more effective for FA prediction
(MCC=0.88) than rest-related features (MCC=0.68). For SC, activity segments
yielded MCC=0.47, rest segments MCC=0.38, and their combination MCC=0.51.
Significant correlations were also found between actimetric features related to
FA, emotional, and restrained eating behaviors, supporting the model's
validity. Our results support the concept of a human bionic suite composed of
IoT devices and ML sensors, which implements health digital assistance with
real-time monitoring and analysis of physiological indicators related to FA and
SC.

æè¦ï¼æ¬ç ç©¶èª¿æ¥æ©å¨å­¸ç¿æ¼ç®æ³ï¼ä»¥è­å¥è¨ºæ·é£ç©æç® (FA) åè©ä¼°å·²ç¢ºèªçç (SC) çå®¢è§ç¹å¾µãè³æä¾èª 81 ä½åèèï¼å¹³åå¹´é½¡ï¼21.5 æ­²ï¼ç¯åï¼18-61 æ­²ï¼å¥³æ§ï¼77.8%ï¼ï¼å¶ FA å SC æ¯ä½¿ç¨è¶é­¯é£ç©æç®éè¡¨ (YFAS) æ¸¬éçãåèèæä¾äºäººå£çµ±è¨åäººé¡æ¸¬éè³æï¼å®æäº YFASãZung èªæè©éæé¬±éè¡¨åè·è­é£²é£è¡çºåå·ï¼ä¸¦å¨éæ£ç¨æèä¸ä½©æ´æ´»åè¨ä¸é±ä»¥è¨ééåæ´»åãå°æ´»åè¨è³æçåæè­å¥åºéè¦ççµ±è¨ååºæ¼çµçç¹å¾µï¼éäºç¹å¾µä½¿ç¨æ©å¨å­¸ç¿æºç¢ºé æ¸¬äº FA å SCãé¦¬ä¿®æ¯ç¸éä¿æ¸ (MCC) æ¯ä¸»è¦ææ¨ãèä¼æ¯ç¸éçç¹å¾µï¼MCC=0.68ï¼ç¸æ¯ï¼èæ´»åç¸éçç¹å¾µå°æ¼ FA é æ¸¬æ´ææï¼MCC=0.88ï¼ãå°æ¼ SCï¼æ´»ååæ®µç¢çç MCC=0.47ï¼ä¼æ¯åæ®µ MCC=0.38ï¼çµåå¾ MCC=0.51ãéç¼ç¾è FAãæç·ååéé£²é£è¡çºç¸éçæ´»åè¨ç¹å¾µä¹éå­å¨é¡¯èç¸éæ§ï¼éæ¯æäºæ¨¡åçæææ§ãæåççµææ¯æç±ç©è¯ç¶²è£ç½®åæ©å¨å­¸ç¿ææ¸¬å¨çµæçäººé«ä»¿çå¥ä»¶çæ¦å¿µï¼è©²å¥ä»¶å¯¦ä½äºå¥åº·æ¸ä½åå©ï¼ä¸¦å°è FA å SC ç¸éçççææ¨é²è¡å³æç£æ§ååæã

##### **Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**
2408.17401v1 by Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni

In recent years, various methods have been introduced for explaining the
outputs of "black-box" AI models. However, it is not well understood whether
users actually comprehend and trust these explanations. In this paper, we focus
on explanations for a regression tool for assessing cancer risk and examine the
effect of the explanations' content and format on the user-centric metrics of
comprehension and trust. Regarding content, we experiment with two explanation
methods: the popular SHAP, based on game-theoretic notions and thus potentially
complex for everyday users to comprehend, and occlusion-1, based on feature
occlusion which may be more comprehensible. Regarding format, we present SHAP
explanations as charts (SC), as is conventional, and occlusion-1 explanations
as charts (OC) as well as text (OT), to which their simpler nature also lends
itself. The experiments amount to user studies questioning participants, with
two different levels of expertise (the general population and those with some
medical training), on their subjective and objective comprehension of and trust
in explanations for the outputs of the regression tool. In both studies we
found a clear preference in terms of subjective comprehension and trust for
occlusion-1 over SHAP explanations in general, when comparing based on content.
However, direct comparisons of explanations when controlling for format only
revealed evidence for OT over SC explanations in most cases, suggesting that
the dominance of occlusion-1 over SHAP explanations may be driven by a
preference for text over charts as explanations. Finally, we found no evidence
of a difference between the explanation types in terms of objective
comprehension. Thus overall, the choice of the content and format of
explanations needs careful attention, since in some contexts format, rather
than content, may play the critical role in improving user experience.

æè¦ï¼<paragraph>è¿å¹´ä¾ï¼å·²ç¶å¼é²åç¨®æ¹æ³ä¾è§£éãé»ç®±ãAI æ¨¡åçè¼¸åºãç¶èï¼ç®åä¸¦ä¸æ¸æ¥ä½¿ç¨èæ¯å¦å¯¦éçè§£åä¿¡ä»»éäºè§£éãå¨æ¬æä¸­ï¼æåå°æ³¨æ¼è©ä¼°ççé¢¨éªçåæ­¸å·¥å·çè§£éï¼ä¸¦æ¢è¨è§£éçå§å®¹åæ ¼å¼å°ä»¥ä½¿ç¨èçºä¸­å¿ççè§£åä¿¡ä»»ææ¨çå½±é¿ãéæ¼å§å®¹ï¼æåå¯¦é©äºå©ç¨®è§£éæ¹æ³ï¼æµè¡ç SHAPï¼åºæ¼åå¼è«æ¦å¿µï¼å æ­¤å°æ¼æ¥å¸¸ä½¿ç¨èä¾èªªå¯è½å¾è¤éï¼ä»¥ååºæ¼ç¹å¾µé®è½ç occlusion-1ï¼å¯è½æ´ææ¼çè§£ãéæ¼æ ¼å¼ï¼æåå° SHAP è§£éåç¾çºåè¡¨ (SC)ï¼éæ¯æ£ä¾ï¼èå° occlusion-1 è§£éåç¾çºåè¡¨ (OC) ä»¥åæå­ (OT)ï¼å¶è¼çºç°¡å®çæ§è³ªä¹é©ç¨æ¼æ­¤ãéäºå¯¦é©ç­åæ¼ä½¿ç¨èç ç©¶ï¼è©¢ååèèï¼å·æå©ç¨®ä¸åç¨åº¦çå°æ¥­ç¥è­ï¼ä¸è¬æ°ç¾åå·åä¸äºé«å­¸è¨ç·´çäººï¼ï¼ä»åå°åæ­¸å·¥å·è¼¸åºè§£éçä¸»è§åå®¢è§çè§£åä¿¡ä»»ãå¨å©é ç ç©¶ä¸­ï¼æåç¼ç¾ï¼å¨åºæ¼å§å®¹é²è¡æ¯è¼æï¼ä¸è¬ä¾èªªï¼occlusion-1 åªæ¼ SHAP è§£éï¼å¨ä¸»è§çè§£åä¿¡ä»»æ¹é¢ææé¡¯çåå¥½ãç¶èï¼å¨åæ§å¶æ ¼å¼çææ³ä¸ç´æ¥æ¯è¼è§£éï¼å¨å¤§å¤æ¸ææ³ä¸åªé¡¯ç¤º OT åªæ¼ SC è§£éçè­æï¼éè¡¨æ occlusion-1 åªæ¼ SHAP è§£éçä¸»å°å°ä½å¯è½æ¯ç±åå¥½æå­èéåè¡¨ä½çºè§£éæé©åçãæå¾ï¼æåæ²æç¼ç¾è§£éé¡åå¨å®¢è§çè§£æ¹é¢çå·®ç°è­æãå æ­¤ï¼ç¸½é«èè¨ï¼å°è§£éçå§å®¹åæ ¼å¼çé¸æéè¦ä»ç´°æ³¨æï¼å çºå¨æäºææ³ä¸ï¼æ ¼å¼èéå§å®¹ï¼å¯è½å¨æ¹åä½¿ç¨èé«é©æ¹é¢ç¼æ®ééµä½ç¨ã</paragraph>

##### **Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**
2409.00163v1 by Yuhan Zheng, Jessie A Elliott, John V Reynolds, Sheraz R Markar, BartÅomiej W. PapieÅ¼, ENSURE study group

Esophageal cancer is a major cause of cancer-related mortality
internationally, with high recurrence rates and poor survival even among
patients treated with curative-intent surgery. Investigating relevant
prognostic factors and predicting prognosis can enhance post-operative clinical
decision-making and potentially improve patients' outcomes. In this work, we
assessed prognostic factor identification and discriminative performances of
three models for Disease-Free Survival (DFS) and Overall Survival (OS) using a
large multicenter international dataset from ENSURE study. We first employed
Cox Proportional Hazards (CoxPH) model to assess the impact of each feature on
outcomes. Subsequently, we utilised CoxPH and two deep neural network
(DNN)-based models, DeepSurv and DeepHit, to predict DFS and OS. The
significant prognostic factors identified by our models were consistent with
clinical literature, with post-operative pathologic features showing higher
significance than clinical stage features. DeepSurv and DeepHit demonstrated
comparable discriminative accuracy to CoxPH, with DeepSurv slightly
outperforming in both DFS and OS prediction tasks, achieving C-index of 0.735
and 0.74, respectively. While these results suggested the potential of DNNs as
prognostic tools for improving predictive accuracy and providing personalised
guidance with respect to risk stratification, CoxPH still remains an adequately
good prediction model, with the data used in this study.

æè¦ï¼é£éçæ¯åééççç¸éæ­»äº¡çä¸»è¦åå ï¼å³ä½¿æ¥åæ ¹æ²»æ§æè¡çæ£èï¼å¾©ç¼çé«ä¸å­æ´»çä½ãæ¢è¨ç¸éé å¾å å­ä¸¦é æ¸¬é å¾ï¼å¯ä»¥å¢é²è¡å¾è¨åºæ±ºç­å¶å®ï¼ä¸¦å¯è½æ¹åæ£èççµæãå¨éé å·¥ä½ä¸­ï¼æåè©ä¼°äºä¸ç¨®æ¨¡åçé å¾å å­è­å¥åå¤å¥æè½ï¼åå¥éå°ç¡ç¾çå­æ´»æ (DFS) åæ´é«å­æ´»æ (OS)ï¼ä½¿ç¨ ENSURE ç ç©¶çå¤§åå¤ä¸­å¿åéæ¸æéãæåé¦åæ¡ç¨ Cox æ¯ä¾é¢¨éª (CoxPH) æ¨¡åä¾è©ä¼°æ¯åç¹å¾µå°çµæçå½±é¿ãé¨å¾ï¼æåå©ç¨ CoxPH åå©ååºæ¼æ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) çæ¨¡å DeepSurv å DeepHit ä¾é æ¸¬ DFS å OSãæåçæ¨¡åè­å¥åºçé¡¯èé å¾å å­èè¨åºæç»ä¸è´ï¼è¡å¾ççç¹å¾µé¡¯ç¤ºåºæ¯è¨åºåæç¹å¾µæ´é«çé¡¯èæ§ãDeepSurv å DeepHit å±ç¾åºè CoxPH ç¸ç¶çå¤å¥æºç¢ºåº¦ï¼DeepSurv å¨ DFS å OS é æ¸¬ä»»åä¸­è¡¨ç¾ç¥åä¸ç±ï¼åå¥éå° C ææ¸ 0.735 å 0.74ãéç¶éäºçµæè¡¨æ DNN ä½çºé å¾å·¥å·çæ½åï¼å¯ä»¥æé«é æ¸¬æºç¢ºåº¦ä¸¦éå°é¢¨éªåå±¤æä¾åäººåæå°ï¼ä½ CoxPH ä»ç¶æ¯è¶³å¤ å¥½çé æ¸¬æ¨¡åï¼ä½¿ç¨æ¬ç ç©¶ä¸­çæ¸æã

##### **NDP: Next Distribution Prediction as a More Broad Target**
2408.17377v1 by Junhao Ruan, Abudukeyumu Abudula, Xinyu Liu, Bei Li, Yinqiao Li, Chenglong Wang, Yuchun Fan, Yuan Ge, Tong Xiao, Jingbo Zhu

Large language models (LLMs) trained on next-token prediction (NTP) paradigm
have demonstrated powerful capabilities. However, the existing NTP paradigm
contains several limitations, particularly related to planned task
complications and error propagation during inference. In our work, we extend
the critique of NTP, highlighting its limitation also due to training with a
narrow objective: the prediction of a sub-optimal one-hot distribution. To
support this critique, we conducted a pre-experiment treating the output
distribution from powerful LLMs as efficient world data compression. By
evaluating the similarity between the $n$-gram distribution and the one-hot
distribution with LLMs, we observed that the $n$-gram distributions align more
closely with the output distribution of LLMs. Based on this insight, we
introduce Next Distribution Prediction (NDP), which uses $n$-gram distributions
to replace the one-hot targets, enhancing learning without extra online
training time. We conducted experiments across translation, general task,
language transfer, and medical domain adaptation. Compared to NTP, NDP can
achieve up to +2.97 COMET improvement in translation tasks, +0.61 average
improvement in general tasks, and incredible +10.75 average improvement in the
medical domain. This demonstrates the concrete benefits of addressing the
target narrowing problem, pointing to a new direction for future work on
improving NTP.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) æ ¹æä¸ä¸åç¬¦èé æ¸¬ (NTP) ç¯ä¾é²è¡è¨ç·´ï¼å·²å±ç¾å¼·å¤§çåè½ãç¶èï¼ç¾æç NTP ç¯ä¾åå«äºå¹¾åéå¶ï¼ç¹å¥æ¯èè¨ç«ä»»åè¤éæ§åæ¨è«æéçé¯èª¤å³æ­æéãå¨æåçç ç©¶ä¸­ï¼æåæ´å±äº NTP çæ¹å¤ï¼å¼·èª¿å¶éå¶ä¹ç±æ¼ä½¿ç¨ç¹éçç®æ¨é²è¡è¨ç·´ï¼é æ¸¬æ¬¡ä½³çä¸ç±åä½ãçºäºæ¯æéé æ¹å¤ï¼æåé²è¡äºä¸ååç½®å¯¦é©ï¼å°å¼·å¤§ LLM çè¼¸åºåä½è¦çºææçä¸çè³æå£ç¸®ãééè©ä¼° $n$-gram åä½è LLM çä¸ç±åä½ä¹éçç¸ä¼¼æ§ï¼æåè§å¯å° $n$-gram åä½è LLM çè¼¸åºåä½æ´çºæ¥è¿ãåºæ¼éåè¦è§£ï¼æåå¼å¥äºä¸ä¸ååä½é æ¸¬ (NDP)ï¼å®ä½¿ç¨ $n$-gram åä½ä¾åä»£ä¸ç±ç®æ¨ï¼å¨æ²æé¡å¤ç·ä¸è¨ç·´æéçææ³ä¸ï¼å å¼·å­¸ç¿ãæåé²è¡äºç¿»è­¯ãä¸è¬ä»»åãèªè¨è½ç§»åé«å­¸é åé©æçå¯¦é©ãè NTP ç¸æ¯ï¼NDP å¨ç¿»è­¯ä»»åä¸­å¯ä»¥éå° +2.97 COMET æ¹é²ï¼å¨ä¸è¬ä»»åä¸­å¹³åæ¹é² +0.61ï¼å¨é«å­¸é åä¸­ä»¤äººé£ä»¥ç½®ä¿¡çå¹³åæ¹é² +10.75ãéè­æäºè§£æ±ºç®æ¨æ¶çªåé¡çå·é«å¥½èï¼æåºäºæ¹é² NTP æªä¾å·¥ä½çå¨æ°æ¹åã

##### **Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**
2408.17011v2 by Jutika Borah, Kumaresh Sarmah, Hidam Kumarjit Singh

Imaging techniques such as Chest X-rays, whole slide images, and optical
coherence tomography serve as the initial screening and detection for a wide
variety of medical pulmonary and ophthalmic conditions respectively. This paper
investigates the intricacies of using pretrained deep convolutional neural
networks with transfer learning across diverse medical imaging datasets with
varying modalities for binary and multiclass classification. We conducted a
comprehensive performance analysis with ten network architectures and model
families each with pretraining and random initialization. Our finding showed
that the use of pretrained models as fixed feature extractors yields poor
performance irrespective of the datasets. Contrary, histopathology microscopy
whole slide images have better performance. It is also found that deeper and
more complex architectures did not necessarily result in the best performance.
This observation implies that the improvements in ImageNet are not parallel to
the medical imaging tasks. Within a medical domain, the performance of the
network architectures varies within model families with shifts in datasets.
This indicates that the performance of models within a specific modality may
not be conclusive for another modality within the same domain. This study
provides a deeper understanding of the applications of deep learning techniques
in medical imaging and highlights the impact of pretrained networks across
different medical imaging datasets under five different experimental settings.

æè¦ï¼å½±åæè¡ï¼ä¾å¦è¸é¨ X åãå¨åçå½±åååå­¸ç¸å¹²æ·å±¤ææï¼åå¥ä½çºåç¨®é«å­¸èºé¨åç¼ç§ç¾ççåæ­¥ç¯©æª¢ååµæ¸¬ãæ¬ææ¢è¨äºä½¿ç¨é è¨ç·´æ·±åº¦å·ç©ç¥ç¶ç¶²è·¯æ­éé·ç§»å­¸ç¿ï¼æ©«è·¨ä¸åé«çå½±åè³æéï¼ä»¥é²è¡äºååå¤é¡å¥åé¡çè¤éæ§ãæåå°åç¨®ç¶²è·¯æ¶æ§åæ¨¡åç³»åé²è¡äºå¨é¢çæè½åæï¼æ¯åæ¶æ§åç³»åé½ç¶éé è¨ç·´åé¨æ©åå§åãæåçç¼ç¾é¡¯ç¤ºï¼å°é è¨ç·´æ¨¡åç¨ä½åºå®ç¹å¾µèåå¨æç¢çä¸ä½³çæè½ï¼èè³æéç¡éãç¸åå°ï¼çµç¹ççå­¸é¡¯å¾®é¡å¨åçå½±åæè¼å¥½çæè½ãæåä¹ç¼ç¾ï¼è¼æ·±ä¸è¤éçæ¶æ§ä¸¦éä¸å®æç¢çæä½³æè½ãæ­¤è§å¯çµææå³è ImageNet çæ¹è¯ä¸¦æªèé«çå½±åä»»åå¹³è¡ãå¨é«çé åå§ï¼ç¶²è·¯æ¶æ§çæè½æé¨èè³æéçè½æèæ¹è®æ¨¡åç³»åãéè¡¨ç¤ºå¨ç¹å®æ¨¡å¼ä¸­æ¨¡åçæè½å¯è½ç¡æ³æ±ºå®å¨åä¸åé åä¸­å¦ä¸ç¨®æ¨¡å¼çæè½ãæ¬ç ç©¶æä¾äºå°æ·±åº¦å­¸ç¿æè¡å¨é«çå½±åä¸­çæç¨æ´æ·±å¥ççè§£ï¼ä¸¦å¼·èª¿äºé è¨ç·´ç¶²è·¯å¨äºç¨®ä¸åå¯¦é©è¨­å®ä¸è·¨ä¸åé«çå½±åè³æéçå½±é¿ã

##### **A Survey for Large Language Models in Biomedicine**
2409.00133v1 by Chong Wang, Mengyao Li, Junjun He, Zhongruo Wang, Erfan Darzi, Zan Chen, Jin Ye, Tianbin Li, Yanzhou Su, Jing Ke, Kaili Qu, Shuxin Li, Yi Yu, Pietro LiÃ², Tianyun Wang, Yu Guang Wang, Yiqing Shen

Recent breakthroughs in large language models (LLMs) offer unprecedented
natural language understanding and generation capabilities. However, existing
surveys on LLMs in biomedicine often focus on specific applications or model
architectures, lacking a comprehensive analysis that integrates the latest
advancements across various biomedical domains. This review, based on an
analysis of 484 publications sourced from databases including PubMed, Web of
Science, and arXiv, provides an in-depth examination of the current landscape,
applications, challenges, and prospects of LLMs in biomedicine, distinguishing
itself by focusing on the practical implications of these models in real-world
biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot
learning across a broad spectrum of biomedical tasks, including diagnostic
assistance, drug discovery, and personalized medicine, among others, with
insights drawn from 137 key studies. Then, we discuss adaptation strategies of
LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to
enhance their performance in specialized biomedical contexts where zero-shot
fails to achieve, such as medical question answering and efficient processing
of biomedical literature. Finally, we discuss the challenges that LLMs face in
the biomedicine domain including data privacy concerns, limited model
interpretability, issues with dataset quality, and ethics due to the sensitive
nature of biomedical data, the need for highly reliable model outputs, and the
ethical implications of deploying AI in healthcare. To address these
challenges, we also identify future research directions of LLM in biomedicine
including federated learning methods to preserve data privacy and integrating
explainable AI methodologies to enhance the transparency of LLMs.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çææ°çªç ´æä¾äºåææªæçèªç¶èªè¨çè§£åçæè½åãç¶èï¼ç¾æéæ¼çç©é«å­¸ä¸­ LLM çèª¿æ¥éå¸¸å°æ³¨æ¼ç¹å®æç¨ææ¨¡åæ¶æ§ï¼ç¼ºä¹æ´ååç¨®çç©é«å­¸é åææ°é²å±çå¨é¢åæãæ¬ç¶è¿°åºæ¼å°ä¾èª PubMedãWeb of Science å arXiv ç­æ¸æåº«ç 484 ç¯åºçç©çåæï¼æ·±å¥æ¢è¨äºçç©é«å­¸ä¸­ LLM çç¶åç¾æ³ãæç¨ãææ°ååæ¯ï¼å¶ç¹é»æ¯éæ³¨éäºæ¨¡åå¨ç¾å¯¦ä¸ççç©é«å­¸èæ¯ä¸­çå¯¦éæç¨ãé¦åï¼æåæ¢è¨äº LLM å¨å»£æ³ççç©é«å­¸ä»»åä¸­çé¶æ¬¡å­¸ç¿è½åï¼åæ¬è¨ºæ·è¼å©ãè¥ç©ç¼ç¾ååæ§åé«çç­ï¼ä¸¦å¾ 137 é ééµç ç©¶ä¸­æ±²åè¦è§£ãç¶å¾ï¼æåè¨è«äº LLM çé©æç­ç¥ï¼åæ¬å®æ¨¡æåå¤æ¨¡æ LLM çå¾®èª¿æ¹æ³ï¼ä»¥å¢å¼·å®åå¨é¶æ¬¡å­¸ç¿ç¡æ³å¯¦ç¾çå°æ¥­çç©é«å­¸èæ¯ä¸­çæ§è½ï¼ä¾å¦é«çåé¡è§£ç­åçç©é«å­¸æç»çææèçãæå¾ï¼æåè¨è«äº LLM å¨çç©é«å­¸é åé¢è¨çææ°ï¼åæ¬æ¸æé±ç§åé¡ãæ¨¡åå¯è§£éæ§æéãæ¸æéè³ªéåé¡ä»¥åç±æ¼çç©é«å­¸æ¸æçæææ§ãå°é«åº¦å¯é æ¨¡åè¼¸åºçéæ±ä»¥åå¨é«çä¿å¥ä¸­é¨ç½² AI çå«çå½±é¿èç¢ççå«çåé¡ãçºäºæå°éäºææ°ï¼æåéç¢ºå®äºçç©é«å­¸ä¸­ LLM æªä¾çç ç©¶æ¹åï¼åæ¬ç¨æ¼ä¿è­·æ¸æé±ç§çè¯åå­¸ç¿æ¹æ³ä»¥åæ´åå¯è§£é AI æ¹æ³ä»¥å¢å¼· LLM çéæåº¦ã

##### **Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**
2408.16343v1 by Yifei Chen, Shenghao Zhu, Zhaojie Fang, Chang Liu, Binfeng Zou, Yuhe Wang, Shuo Chang, Fan Jia, Feiwei Qin, Jin Fan, Yong Peng, Changmiao Wang

Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by
memory loss, executive dysfunction, and personality changes. Early diagnosis is
challenging due to subtle symptoms and varied presentations, often leading to
misdiagnosis with traditional unimodal diagnostic methods due to their limited
scope. This study introduces an advanced multimodal classification model that
integrates clinical, cognitive, neuroimaging, and EEG data to enhance
diagnostic accuracy. The model incorporates a feature tagger with a tabular
data coding architecture and utilizes the TimesBlock module to capture
intricate temporal patterns in Electroencephalograms (EEG) data. By employing
Cross-modal Attention Aggregation module, the model effectively fuses Magnetic
Resonance Imaging (MRI) spatial information with EEG temporal data,
significantly improving the distinction between AD, Mild Cognitive Impairment,
and Normal Cognition. Simultaneously, we have constructed the first AD
classification dataset that includes three modalities: EEG, MRI, and tabular
data. Our innovative approach aims to facilitate early diagnosis and
intervention, potentially slowing the progression of AD. The source code and
our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.

æè¦ï¼é¿è²æµ·é»ç (AD) æ¯ä¸ç¨®è¤éçç¥ç¶éåæ§ç¾çï¼ç¹å¾µæ¯è¨æ¶ååªå¤±ãå·è¡åè½éç¤åäººæ ¼æ¹è®ãç±æ¼ççå¾®å¦ä¸è¡¨ç¾å½¢å¼å¤æ¨£ï¼æ©æè¨ºæ·å·æææ°æ§ï¼éå¸¸ç±æ¼å³çµ±å®æ¨¡æè¨ºæ·æ¹æ³çç¯åæéèå°è´èª¤è¨ºãæ¬ç ç©¶å¼å¥äºä¸ååé²çå¤æ¨¡æåé¡æ¨¡åï¼å®æ´åäºè¨åºãèªç¥ãç¥ç¶å½±ååè¦é»åæ¸æï¼ä»¥æé«è¨ºæ·æºç¢ºæ§ãè©²æ¨¡åçµåäºä¸åå·æè¡¨æ ¼æ¸æç·¨ç¢¼æ¶æ§çç¹å¾µæ¨ç±¤å¨ï¼ä¸¦å©ç¨ TimesBlock æ¨¡çµä¾ææè¦é»å (EEG) æ¸æä¸­çè¤éæéæ¨¡å¼ãééæ¡ç¨è·¨æ¨¡ææ³¨æåèåæ¨¡çµï¼è©²æ¨¡åææå°èåäºç£å±æ¯æå (MRI) ç©ºéè³è¨åè¦é»åæéæ¸æï¼é¡¯èæ¹åäº ADãè¼åº¦èªç¥éç¤åæ­£å¸¸èªç¥ä¹éçåå¥ãåæï¼æåæ§å»ºäºç¬¬ä¸å AD åé¡æ¸æéï¼å¶ä¸­åå«ä¸ç¨®æ¨¡æï¼è¦é»åãç£å±æ¯æååè¡¨æ ¼æ¸æãæåçåµæ°æ¹æ³æ¨å¨ä¿é²æ©æè¨ºæ·åå¹²é ï¼æ½å¨å°æ¸ç·© AD çé²å±ãåå§ç¢¼åæåçç§äºº ADMC æ¸æéå¯å¨ https://github.com/JustlfC03/MSTNet ç²å¾ã

##### **Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**
2408.16256v1 by Xia Jiang, Yijun Zhou, Alan Wells, Adam Brufsky

Breast cancer is one of the two cancers responsible for the most deaths in
women, with about 42,000 deaths each year in the US. That there are over
300,000 breast cancers newly diagnosed each year suggests that only a fraction
of the cancers result in mortality. Thus, most of the women undergo seemingly
curative treatment for localized cancers, but a significant later succumb to
metastatic disease for which current treatments are only temporizing for the
vast majority. The current prognostic metrics are of little actionable value
for 4 of the 5 women seemingly cured after local treatment, and many women are
exposed to morbid and even mortal adjuvant therapies unnecessarily, with these
adjuvant therapies reducing metastatic recurrence by only a third. Thus, there
is a need for better prognostics to target aggressive treatment at those who
are likely to relapse and spare those who were actually cured. While there is a
plethora of molecular and tumor-marker assays in use and under-development to
detect recurrence early, these are time consuming, expensive and still often
un-validated as to actionable prognostic utility. A different approach would
use large data techniques to determine clinical and histopathological
parameters that would provide accurate prognostics using existing data. Herein,
we report on machine learning, together with grid search and Bayesian Networks
to develop algorithms that present a AUC of up to 0.9 in ROC analyses, using
only extant data. Such algorithms could be rapidly translated to clinical
management as they do not require testing beyond routine tumor evaluations.

æè¦ï¼ä¹³çæ¯é æå¥³æ§æ­»äº¡äººæ¸æå¤çå©ç¨®ççä¹ä¸ï¼æ¯å¹´ç´æ 42,000 åå¥³æ§æ­»æ¼ä¹³çãæ¯å¹´æè¶é 300,000 ä¾ä¹³çæ°ç¢ºè¨ºï¼éè¡¨ç¤ºåªæå°é¨åççæå°è´æ­»äº¡ãå æ­¤ï¼å¤§å¤æ¸å¥³æ§æ¥åå±é¨çççæ ¹æ²»æ§æ²»çï¼ä½è¨±å¤äººå¾ä¾ä»ææ­»æ¼è½ç§»æ§ç¾çï¼èç®åçæ²»çæ¹æ³å°çµå¤§å¤æ¸æ£èä¾èªªåªæ¯æ«æçãç®åçé å¾ææ¨å°æ¼ 5 åæ¥åå±é¨æ²»çå¾çä¼¼æ²»ççå¥³æ§ä¸­ï¼æ 4 åå¹¾ä¹æ²æå¯¦éå¹å¼ï¼è¨±å¤å¥³æ§ä¸å¿è¦å°æ¥åçæçè³è´å½çè¼å©çæ³ï¼èéäºè¼å©çæ³åè½å°è½ç§»æ§å¾©ç¼çéä½ä¸åä¹ä¸ãå æ­¤ï¼éè¦æ´å¥½çé å¾ææ¨ï¼æè½éå°é£äºå¯è½å¾©ç¼çäººé²è¡ç©æ¥µæ²»çï¼ä¸¦é¿åé£äºå¯¦éä¸å·²ç¶æ²»ççäººæ¥åæ²»çãéç¶æè¨±å¤åå­åè«ç¤æ¨è¨æª¢æ¸¬æ¹æ³æ­£å¨ä½¿ç¨åéç¼ä¸­ï¼å¯ä»¥åæ©ç¼ç¾å¾©ç¼ï¼ä½éäºæ¹æ³èæãæè²´ï¼èä¸ä½çºå¯æä½çé å¾å·¥å·ï¼å¶æç¨ä»ç¶å¸¸å¸¸æªç¶é©è­ãå¦ä¸ç¨®æ¹æ³æä½¿ç¨å¤§éçè³ææè¡ï¼ä¾ç¢ºå®è¨åºåçµç¹ççå­¸åæ¸ï¼ä¸¦ä½¿ç¨ç¾æè³ææä¾æºç¢ºçé å¾ææ¨ãå¨æ­¤ï¼æåå ±åäºæ©å¨å­¸ç¿ï¼ä»¥åç¶²æ ¼æå°åè²æ°ç¶²è·¯ï¼ç¨ä¾éç¼æ¼ç®æ³ï¼å¨ ROC åæä¸­æä¾é«é 0.9 ç AUCï¼åä½¿ç¨ç¾æè³æãæ­¤é¡æ¼ç®æ³å¯ä»¥å¿«éè½æçºè¨åºç®¡çï¼å çºå®åä¸éè¦é²è¡å¸¸è¦è«ç¤è©ä¼°ä»¥å¤çæ¸¬è©¦ã

##### **M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**
2408.16213v1 by Jonggwon Park, Soobum Kim, Byungmu Yoon, Jihun Hyun, Kyoyun Choi

The rapid evolution of artificial intelligence, especially in large language
models (LLMs), has significantly impacted various domains, including
healthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,
but with limitations: either underutilizing the multi-tasking capabilities of
LLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM
designed to enhance CXR interpretation. The model is trained on a visual
instruction-following dataset that integrates various task-specific datasets in
a conversational format. As a result, the model supports multiple tasks such as
medical report generation (MRG), visual grounding, and visual question
answering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by
employing a chain-of-thought prompting strategy, in which it identifies
findings in CXR images and subsequently generates corresponding reports. The
model is adaptable to various MRG scenarios depending on the available inputs,
such as single-image, multi-image, and multi-study contexts. In addition to
MRG, M4CXR performs visual grounding at a level comparable to specialized
models and also demonstrates outstanding performance in VQA. Both quantitative
and qualitative assessments reveal M4CXR's versatility in MRG, visual
grounding, and VQA, while consistently maintaining clinical accuracy.

æè¦ï¼äººå·¥æºæ§çå¿«éç¼å±ï¼ç¹å¥æ¯å¨å¤§åèªè¨æ¨¡å (LLM) ä¸­ï¼å·²å°åæ¬é«çä¿å¥å¨å§çååé åç¢çéå¤§å½±é¿ãå¨è¸é¨ X å (CXR) åæä¸­ï¼ååçç ç©¶å·²æ¡ç¨ LLMï¼ä½æå¶éå¶ï¼ä¸æ¯æªè½ååå©ç¨ LLM çå¤ä»»åèçè½åï¼å°±æ¯ç¼ºä¹è¨åºæºç¢ºæ§ãæ¬ææåº M4CXRï¼ä¸ç¨®å¤æ¨¡æ LLMï¼æ¨å¨å¢å¼· CXR è§£éãè©²æ¨¡åè¨ç·´æ¼è¦è¦ºæä»¤éµå¾ªè³æéï¼å¶ä¸­ä»¥å°è©±æ ¼å¼æ´ååç¨®ç¹å®ä»»åè³æéãå æ­¤ï¼è©²æ¨¡åæ¯æ´å¤é ä»»åï¼ä¾å¦é«çå ±åç¢ç (MRG)ãè¦è¦ºåºç¤åè¦è¦ºåé¡åç­ (VQA)ãM4CXR ééæ¡ç¨æèéæç¤ºç­ç¥ï¼å¨ MRG ä¸­éææåé²çè¨åºæºç¢ºæ§ï¼å¶ä¸­å®æè­å¥ CXR å½±åä¸­çç¼ç¾ï¼ä¸¦é¨å¾ç¢çå°æçå ±åãè©²æ¨¡åå¯æ ¹æå¯ç¨è¼¸å¥ï¼ä¾å¦å®ä¸å½±åãå¤éå½±ååå¤éç ç©¶èçµ¡ï¼é©æåç¨® MRG æå¢ãé¤äº MRG ä¹å¤ï¼M4CXR ä»¥èå°éæ¨¡åç¸ç¶çå±¤ç´å·è¡è¦è¦ºåºç¤ï¼ä¸¦å¨ VQA ä¸­å±ç¾åºè²çæè½ãå®éåå®æ§è©ä¼°åé¡¯ç¤ºåº M4CXR å¨ MRGãè¦è¦ºåºç¤å VQA ä¸­çå¤åè½æ§ï¼åææçºç¶­æè¨åºæºç¢ºæ§ã

##### **A Survey on Evaluation of Multimodal Large Language Models**
2408.15769v1 by Jiaxing Huang, Jingyi Zhang

Multimodal Large Language Models (MLLMs) mimic human perception and reasoning
system by integrating powerful Large Language Models (LLMs) with various
modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and
various modality encoders as sensory organs. This framework endows MLLMs with
human-like capabilities, and suggests a potential pathway towards achieving
artificial general intelligence (AGI). With the emergence of all-round MLLMs
like GPT-4V and Gemini, a multitude of evaluation methods have been developed
to assess their capabilities across different dimensions. This paper presents a
systematic and comprehensive review of MLLM evaluation methods, covering the
following key aspects: (1) the background of MLLMs and their evaluation; (2)
"what to evaluate" that reviews and categorizes existing MLLM evaluation tasks
based on the capabilities assessed, including general multimodal recognition,
perception, reasoning and trustworthiness, and domain-specific applications
such as socioeconomic, natural sciences and engineering, medical usage, AI
agent, remote sensing, video and audio processing, 3D point cloud analysis, and
others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into
general and specific benchmarks; (4) "how to evaluate" that reviews and
illustrates MLLM evaluation steps and metrics; Our overarching goal is to
provide valuable insights for researchers in the field of MLLM evaluation,
thereby facilitating the development of more capable and reliable MLLMs. We
emphasize that evaluation should be regarded as a critical discipline,
essential for advancing the field of MLLMs.

æè¦ï¼å¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM) ééæ´åå¼·å¤§çå¤§åèªè¨æ¨¡å (LLM) èåç¨®æ¨¡æç·¨ç¢¼å¨ï¼ä¾å¦è¦è¦ºãé³è¨ï¼ï¼æ¨¡æ¬äººé¡çæç¥åæ¨çç³»çµ±ï¼å° LLM å®ä½çºãå¤§è¦ãï¼èå°åç¨®æ¨¡æç·¨ç¢¼å¨å®ä½çºæå®å¨å®ãæ­¤æ¶æ§è³¦äº MLLM é¡ä¼¼äººé¡çè½åï¼ä¸¦æåºå¯¦ç¾äººå·¥éç¨æºæ§ (AGI) çæ½å¨éå¾ãé¨è GPT-4V å Gemini ç­å¨æ¹ä½ MLLM çåºç¾ï¼å·²ç¶éç¼åºå¤ç¨®è©ä¼°æ¹æ³ä¾è©ä¼°å®åå¨ä¸åç¶­åº¦ä¸çè½åãæ¬æå° MLLM è©ä¼°æ¹æ³é²è¡äºç³»çµ±ä¸å¨é¢çåé¡§ï¼æ¶µèä»¥ä¸å¹¾åééµé¢åï¼(1) MLLM åå¶è©ä¼°çèæ¯ï¼(2)ãè¦è©ä¼°ä»éº¼ãæ ¹æè©ä¼°çè½åï¼åé¡§ä¸¦åé¡ç¾æç MLLM è©ä¼°ä»»åï¼åæ¬ä¸è¬å¤æ¨¡æè¾¨è­ãæç¥ãæ¨çåå¯ä¿¡åº¦ï¼ä»¥åç¹å®é åçæç¨ï¼ä¾å¦ç¤¾æç¶æ¿ãèªç¶ç§å­¸åå·¥ç¨ãé«çç¨éãAI ä»£çãéæ¸¬ãå½±çåé³è¨èçã3D é»é²åæç­ï¼(3)ãå¨åªè£¡è©ä¼°ãå° MLLM è©ä¼°åºæºç¸½çµçºä¸è¬åºæºåç¹å®åºæºï¼(4)ãå¦ä½è©ä¼°ãåé¡§ä¸¦èªªæ MLLM è©ä¼°æ­¥é©åææ¨ãæåçé¦è¦ç®æ¨æ¯çº MLLM è©ä¼°é åçç ç©¶äººå¡æä¾æå¹å¼çè¦è§£ï¼å¾èä¿é²æ´å¼·å¤§ä¸å¯é ç MLLM çéç¼ãæåå¼·èª¿è©ä¼°æè¢«è¦çºä¸é ééµçå­¸ç§ï¼å°æ¼æ¨é² MLLM é åè³ééè¦ã

##### **Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**
2408.15498v1 by Yijun Zhou, Om Arora-Jain, Xia Jiang

While machine learning has advanced in medicine, its widespread use in
clinical applications, especially in predicting breast cancer metastasis, is
still limited. We have been dedicated to constructing a DFNN model to predict
breast cancer metastasis n years in advance. However, the challenge lies in
efficiently identifying optimal hyperparameter values through grid search,
given the constraints of time and resources. Issues such as the infinite
possibilities for continuous hyperparameters like l1 and l2, as well as the
time-consuming and costly process, further complicate the task. To address
these challenges, we developed Single Hyperparameter Grid Search (SHGS)
strategy, serving as a preselection method before grid search. Our experiments
with SHGS applied to DFNN models for breast cancer metastasis prediction focus
on analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2,
learning rate, decay, and momentum. We created three figures, each depicting
the experiment results obtained from three LSM-I-10-Plus-year datasets. These
figures illustrate the relationship between model performance and the target
hyperparameter values. For each hyperparameter, we analyzed whether changes in
this hyperparameter would affect model performance, examined if there were
specific patterns, and explored how to choose values for the particular
hyperparameter. Our experimental findings reveal that the optimal value of a
hyperparameter is not only dependent on the dataset but is also significantly
influenced by the settings of other hyperparameters. Additionally, our
experiments suggested some reduced range of values for a target hyperparameter,
which may be helpful for low-budget grid search. This approach serves as a
prior experience and foundation for subsequent use of grid search to enhance
model performance.

æè¦ï¼åç®¡æ©å¨å­¸ç¿å¨é«å­¸é åå·²ææé²å±ï¼ä½å¶å¨è¨åºæç¨ä¸­çå»£æ³ä½¿ç¨ï¼ç¹å¥æ¯å¨é æ¸¬ä¹³çè½ç§»æ¹é¢ï¼ä»æå¶éå¶ãæåè´åæ¼å»ºæ§ DFNN æ¨¡åï¼ä»¥é æ¸¬ä¹³çè½ç§» n å¹´ãç¶èï¼ææ°å¨æ¼ééç¶²æ ¼æå°ææçå°æ¾åºæä½³è¶åæ¸å¼ï¼éåå°æéåè³æºçéå¶ãè«¸å¦ l1 å l2 ç­é£çºè¶åæ¸çå¯è½æ§ç¡çª®ï¼ä»¥åèæä¸æè²´çéç¨ç­åé¡ï¼æ´è®éé ä»»åè®å¾è¤éãçºäºæå°éäºææ°ï¼æåéç¼äºå®ä¸è¶åæ¸ç¶²æ ¼æå° (SHGS) ç­ç¥ï¼ä½çºç¶²æ ¼æå°åçé é¸æ¹æ³ãæåéå°ä¹³çè½ç§»é æ¸¬æç¨ç DFNN æ¨¡åé²è¡ SHGS å¯¦é©ï¼éé»åæå«åç®æ¨è¶åæ¸ï¼epoch æ¬¡æ¸ãæ¹æ¬¡å¤§å°ãä¸­æ·ãL1ãL2ãå­¸ç¿çãè¡°æ¸ååéãæåè£½ä½äºä¸å¹åï¼æ¯å¹åé½æç¹ªäºå¾ä¸å LSM-I-10-Plus-year è³æéç²å¾çå¯¦é©çµæãéäºåè¡¨èªªæäºæ¨¡åæè½èç®æ¨è¶åæ¸å¼ä¹éçéä¿ãå°æ¼æ¯åè¶åæ¸ï¼æååæäºè¶åæ¸çè®åæ¯å¦æå½±é¿æ¨¡åæè½ï¼ä¸¦æª¢è¦æ¯å¦æç¹å®æ¨¡å¼ï¼ä»¥åå¦ä½éå°ç¹å®è¶åæ¸é¸æå¼ãæåçå¯¦é©çµæé¡¯ç¤ºï¼è¶åæ¸çæä½³å¼ä¸ååæ±ºæ¼è³æéï¼ä¹åå°å¶ä»è¶åæ¸è¨­å®çé¡¯èå½±é¿ãæ­¤å¤ï¼æåçå¯¦é©å»ºè­°ç¸®å°ç®æ¨è¶åæ¸å¼çç¯åï¼éå¯è½æå©æ¼ä½é ç®çç¶²æ ¼æå°ãæ­¤æ¹æ³å¯ä½çºå¾çºä½¿ç¨ç¶²æ ¼æå°ä»¥å¢å¼·æ¨¡åæè½çååç¶é©ååºç¤ã

##### **What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**
2408.15354v1 by Jana Schaich Borg, Hannah Read

Interest is growing in artificial empathy, but so is confusion about what
artificial empathy is or needs to be. This confusion makes it challenging to
navigate the technical and ethical issues that accompany empathic AI
development. Here, we outline a framework for thinking about empathic AI based
on the premise that different constellations of capabilities associated with
empathy are important for different empathic AI applications. We describe
distinctions of capabilities that we argue belong under the empathy umbrella,
and show how three medical empathic AI use cases require different sets of
these capabilities. We conclude by discussing why appreciation of the diverse
capabilities under the empathy umbrella is important for both AI creators and
users.

æè¦ï¼å°äººå·¥åçå¿è¶ä¾è¶æèè¶£ï¼ä½å°æ¼äººå·¥åçå¿æ¯ä»éº¼æéè¦ä»éº¼ä¹è¶ä¾è¶å°æãéç¨®æ··æ·ä½¿å¾é£ä»¥è§£æ±ºä¼´é¨åçå¿ AI éç¼èä¾çæè¡åå«çåé¡ãå¨æ­¤ï¼æåæ¦è¿°äºä¸åæèåçå¿ AI çæ¶æ§ï¼å¶åºæ¼éæ¨£ä¸ååæï¼èåçå¿ç¸éçä¸åè½åçµåå°æ¼ä¸åçåçå¿ AI æç¨å¾éè¦ãæåæè¿°äºæåèªçºå±¬æ¼åçå¿ç¯ççè½ååå¥ï¼ä¸¦å±ç¤ºäºä¸åé«çåçå¿ AI ä½¿ç¨æ¡ä¾éè¦éäºè½åçä¸åçµåãæåæå¾è¨è«äºçºä»éº¼æ¬£è³åçå¿ç¯çä¸çåç¨®è½åå°æ¼ AI åµé èåä½¿ç¨èé½å¾éè¦çåå ã

##### **Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**
2408.15217v1 by Weiyi Zhang, Siyu Huang, Jiancheng Yang, Ruoyu Chen, Zongyuan Ge, Yingfeng Zheng, Danli Shi, Mingguang He

Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal
vascular dynamics and aiding in the diagnosis of eye diseases. However, its
invasive nature and less accessibility compared to Color Fundus (CF) images
pose significant challenges. Current CF to FFA translation methods are limited
to static generation. In this work, we pioneer dynamic FFA video generation
from static CF images. We introduce an autoregressive GAN for smooth,
memory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic
lesion changes in FFA regions, we design a knowledge mask based on clinical
experience. Leveraging this mask, our approach integrates innovative knowledge
mask-guided techniques, including knowledge-boosted attention, knowledge-aware
discriminators, and mask-enhanced patchNCE loss, aimed at refining generation
in critical areas and addressing the pixel misalignment challenge. Our method
achieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common
video generation approaches. Human assessment by an ophthalmologist confirms
its high generation quality. Notably, our knowledge mask surpasses supervised
lesion segmentation masks, offering a promising non-invasive alternative to
traditional FFA for research and clinical applications. The code is available
at https://github.com/Michi-3000/Fundus2Video.

æè¦ï¼ç¼åºè¢åè¡ç®¡æå½± (FFA) æ¯è©ä¼°è¦ç¶²èè¡ç®¡ååå­¸ååå©è¨ºæ·ç¼ç¾çéè¦å·¥å·ãç¶èï¼èå½©è²ç¼åº (CF) å½±åç¸æ¯ï¼å¶ä¾µå¥æ§è¼é«ä¸åå¾ä¸æï¼å æ­¤é æéå¤§ææ°ãç®å CF è½ææ FFA çç¿»è­¯æ¹æ³åéæ¼éæç¢çãå¨éé å·¥ä½ä¸­ï¼æåçåå¾éæ CF å½±åç¢çåæ FFA å½±çãæåå¼å¥ä¸åèªè¿´æ­¸ GANï¼ä»¥é²è¡æµæ¢ä¸ç¯çè¨æ¶é«çéå¹ FFA åæãçºäºå å¼·å° FFA ååä¸­åæçç¶è®åçéæ³¨ï¼æåæ ¹æè¨åºç¶é©è¨­è¨äºä¸åç¥è­é®ç½©ãééå©ç¨éåé®ç½©ï¼æåçåæ³æ´åäºåµæ°çç¥è­é®ç½©å¼å°æè¡ï¼åæ¬ç¥è­å¢å¼·çæ³¨æåãç¥è­æç¥çè¾¨å¥å¨ä»¥åé®ç½©å¢å¼·ç patchNCE æå¤±ï¼æ¨å¨æ¹åééµååççæä¸¦è§£æ±ºåç´ æªå°é½çææ°ãèå¶ä»å¸¸è¦çå½±ççææ¹æ³ç¸æ¯ï¼æåçåæ³éå°äºæä½³ç FVD 1503.21 å PSNR 11.81ãç¼ç§é«å¸«çäººçºè©ä¼°è­å¯¦äºå¶çæåè³ªå¾é«ãå¼å¾æ³¨æçæ¯ï¼æåçç¥è­é®ç½©è¶è¶äºæç£ç£ççç¶åå²é®ç½©ï¼çºå³çµ± FFA æä¾äºä¸åæåéçéä¾µå¥æ§æ¿ä»£æ¹æ¡ï¼å¯ç¨æ¼ç ç©¶åè¨åºæç¨ãç¨å¼ç¢¼å¯å¨ https://github.com/Michi-3000/Fundus2Video åå¾ã

##### **Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**
2409.00112v1 by Daniil Filienko, Yinzhou Wang, Caroline El Jazmi, Serena Xie, Trevor Cohen, Martine De Cock, Weichao Yuwen

While Large Language Models (LLMs) are being quickly adapted to many domains,
including healthcare, their strengths and pitfalls remain under-explored. In
our study, we examine the effects of prompt engineering to guide Large Language
Models (LLMs) in delivering parts of a Problem-Solving Therapy (PST) session
via text, particularly during the symptom identification and assessment phase
for personalized goal setting. We present evaluation results of the models'
performances by automatic metrics and experienced medical professionals. We
demonstrate that the models' capability to deliver protocolized therapy can be
improved with the proper use of prompt engineering methods, albeit with
limitations. To our knowledge, this study is among the first to assess the
effects of various prompting techniques in enhancing a generalist model's
ability to deliver psychotherapy, focusing on overall quality, consistency, and
empathy. Exploring LLMs' potential in delivering psychotherapy holds promise
with the current shortage of mental health professionals amid significant
needs, enhancing the potential utility of AI-based and AI-enhanced care
services.

æè¦ï¼é¨èå¤§åèªè¨æ¨¡åï¼LLMï¼å¿«éé©ææ¼è¨±å¤é åï¼åæ¬é«çä¿å¥ï¼å®åçåªå¢åç¼ºé·ä»æªå¾å°ååæ¢ç´¢ãå¨æåçç ç©¶ä¸­ï¼æåæ¢è¨äºæç¤ºå·¥ç¨å¨å¼å°å¤§åèªè¨æ¨¡åï¼LLMï¼ééæå­æä¾åé¡è§£æ±ºçæ³ï¼PSTï¼ç°ç¯çé¨åå§å®¹çææï¼ç¹å¥æ¯å¨ççè­å¥åè©ä¼°éæ®µï¼ç¨æ¼åæ§åç®æ¨è¨­å®ãæåééèªååææ¨åç¶é©è±å¯çé«çå°æ¥­äººå¡å±ç¤ºäºæ¨¡åæ§è½çè©ä¼°çµæãæåè­æäºæ¨¡åæä¾ç¨å¼åæ²»ççè½åå¯ä»¥ä½¿ç¨æç¤ºå·¥ç¨æ¹æ³é©ç¶ä½¿ç¨ä¾æ¹é²ï¼åç®¡æå±éæ§ãææåæç¥ï¼éé ç ç©¶æ¯ç¬¬ä¸æ¹è©ä¼°åç¨®æç¤ºæè¡å°å¢å¼·éææ¨¡åæä¾å¿çæ²»çè½åçå½±é¿çç ç©¶ä¹ä¸ï¼éé»éæ³¨æ´é«åè³ªãä¸è´æ§ååçå¿ãå¨å¿çå¥åº·å°æ¥­äººå¡å´éç­ç¼ºä¸éæ±å·¨å¤§çææ³ä¸ï¼æ¢ç´¢ LLM å¨æä¾å¿çæ²»çæ¹é¢çæ½åå¾æåæ¯ï¼å¢å¼·äºåºæ¼ AI å AI å¢å¼·çè­·çæåçæ½å¨æç¨ã

##### **Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**
2408.15121v1 by Francesco Sovrano, Michael Lognoul, Giulia Vilone

Significant investment and development have gone into integrating Artificial
Intelligence (AI) in medical and healthcare applications, leading to advanced
control systems in medical technology. However, the opacity of AI systems
raises concerns about essential characteristics needed in such sensitive
applications, like transparency and trustworthiness. Our study addresses these
concerns by investigating a process for selecting the most adequate Explainable
AI (XAI) methods to comply with the explanation requirements of key EU
regulations in the context of smart bioelectronics for medical devices. The
adopted methodology starts with categorising smart devices by their control
mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving
into their technology. Then, we analyse these regulations to define their
explainability requirements for the various devices and related goals.
Simultaneously, we classify XAI methods by their explanatory objectives. This
allows for matching legal explainability requirements with XAI explanatory
goals and determining the suitable XAI algorithms for achieving them. Our
findings provide a nuanced understanding of which XAI algorithms align better
with EU regulations for different types of medical devices. We demonstrate this
through practical case studies on different neural implants, from chronic
disease management to advanced prosthetics. This study fills a crucial gap in
aligning XAI applications in bioelectronics with stringent provisions of EU
regulations. It provides a practical framework for developers and researchers,
ensuring their AI innovations advance healthcare technology and adhere to legal
and ethical standards.

æè¦ï¼äººå·¥æºæ§ï¼AIï¼å¨é«çåä¿å¥æç¨ä¸­æå¥äºå¤§éçæè³åéç¼ï¼é²èå°è´é«çæè¡ä¸­çåé²æ§å¶ç³»çµ±ãç¶èï¼AI ç³»çµ±çä¸éææ§å¼ç¼äºå°æ­¤é¡æææç¨ä¸­æéåºæ¬ç¹æ§çææï¼ä¾å¦éæåº¦åå¯ä¿¡åº¦ãæåçç ç©¶ééèª¿æ¥ä¸åç¨åºä¾è§£æ±ºéäºåé¡ï¼ç¨æ¼é¸ææååçå¯è§£é AIï¼XAIï¼æ¹æ³ï¼ä»¥ç¬¦åæ­çæ³è¦å¨é«çå¨æçæºæ§åçç©é»å­å­¸ä¸­çèªªæè¦æ±ãæ¡ç¨çæ¹æ³å¾ééå¶æ§å¶æ©å¶ï¼éè¿´è·¯ãéè¿´è·¯ååéè¿´è·¯ç³»çµ±ï¼å°æºæ§åè£ç½®é²è¡åé¡ï¼ä¸¦æ·±å¥æ¢è¨å¶æè¡éå§ãç¶å¾ï¼æååæéäºæ³è¦ä»¥å®ç¾©å¶å°åç¨®è£ç½®åç¸éç®æ¨çå¯è§£éæ§è¦æ±ãåæï¼æåééå¶èªªæç®æ¨å° XAI æ¹æ³é²è¡åé¡ãéåè¨±å°æ³å¾å¯è§£éæ§è¦æ±è XAI èªªæç®æ¨ç¸å¹éï¼ä¸¦ç¢ºå®é©ç¶ç XAI æ¼ç®æ³ä¾éæå®åãæåçç ç©¶çµææä¾äºå°åªäº XAI æ¼ç®æ³æ´ç¬¦åæ­çæ³è¦ä»¥é©ç¨æ¼ä¸åé¡åçé«çå¨æçç´°ç·»çè§£ãæåééä¸åç¥ç¶æ¤å¥ç©çå¯¦éæ¡ä¾ç ç©¶ä¾è­æéä¸é»ï¼å¾æ¢æ§ç¾çç®¡çå°åé²çç¾©è¢ãéé ç ç©¶å¡«è£äºå°çç©é»å­å­¸ä¸­ç XAI æç¨èæ­çæ³è¦çå´æ ¼è¦å®ç¸ç¬¦çéè¦ç©ºç½ãå®çºéç¼äººå¡åç ç©¶äººå¡æä¾äºä¸åå¯¦ç¨çæ¶æ§ï¼ç¢ºä¿å¶ AI åµæ°è½ä¿é²é«çæè¡ä¸¦éµå®æ³å¾åéå¾·æ¨æºã

##### **MiWaves Reinforcement Learning Algorithm**
2408.15076v1 by Susobhan Ghosh, Yongyi Guo, Pei-Yao Hung, Lara Coughlin, Erin Bonar, Inbal Nahum-Shani, Maureen Walton, Susan Murphy

The escalating prevalence of cannabis use poses a significant public health
challenge globally. In the U.S., cannabis use is more prevalent among emerging
adults (EAs) (ages 18-25) than any other age group, with legalization in the
multiple states contributing to a public perception that cannabis is less risky
than in prior decades. To address this growing concern, we developed MiWaves, a
reinforcement learning (RL) algorithm designed to optimize the delivery of
personalized intervention prompts to reduce cannabis use among EAs. MiWaves
leverages domain expertise and prior data to tailor the likelihood of delivery
of intervention messages. This paper presents a comprehensive overview of the
algorithm's design, including key decisions and experimental outcomes. The
finalized MiWaves RL algorithm was deployed in a clinical trial from March to
May 2024.

æè¦ï¼å¤§éº»ä½¿ç¨çä¸æ·ä¸åï¼å°å¨çå¬å±è¡çæ§æéå¤§ææ°ãå¨ç¾åï¼å¤§éº»ä½¿ç¨çå¨å¹´è¼æå¹´äººï¼EAï¼ï¼18-25 æ­²ï¼ä¸­æ¯ä»»ä½å¶ä»å¹´é½¡çµé½è¦æ®éï¼å¤åå·çåæ³åå°è´å¬ç¾èªçºå¤§éº»æ¯éå»å¹¾åå¹´é¢¨éªè¼ä½ãçºäºè§£æ±ºéåæ¥çå´éçåé¡ï¼æåéç¼äº MiWavesï¼éæ¯ä¸ç¨®å¢å¼·å­¸ç¿ (RL) æ¼ç®æ³ï¼æ¨å¨åªååæ§åå¹²é æç¤ºçå³éï¼ä»¥æ¸å° EA ä¸­çå¤§éº»ä½¿ç¨ãMiWaves å©ç¨é åå°æ¥­ç¥è­åååçæ¸æä¾èª¿æ´å¹²é è¨æ¯å³éçå¯è½æ§ãæ¬æå¨é¢æ¦è¿°äºæ¼ç®æ³çè¨­è¨ï¼åæ¬ééµæ±ºç­åå¯¦é©çµæãæçµç MiWaves RL æ¼ç®æ³å·²æ¼ 2024 å¹´ 3 æè³ 5 æå¨è¨åºè©¦é©ä¸­é¨ç½²ã

##### **Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**
2408.15032v1 by Yuqi Zhang, Xiaoqian Zhang, Jiakai Wang, Yuancheng Yang, Taiying Peng, Chao Tong

Computational pathology (CPath) has significantly advanced the clinical
practice of pathology. Despite the progress made, Multiple Instance Learning
(MIL), a promising paradigm within CPath, continues to face challenges,
particularly related to incomplete information utilization. Existing
frameworks, such as those based on Convolutional Neural Networks (CNNs),
attention, and selective scan space state sequential model (SSM), lack
sufficient flexibility and scalability in fusing diverse features, and cannot
effectively fuse diverse features. Additionally, current approaches do not
adequately exploit order-related and order-independent features, resulting in
suboptimal utilization of sequence information. To address these limitations,
we propose a novel MIL framework called Mamba2MIL. Our framework utilizes the
state space duality model (SSD) to model long sequences of patches of whole
slide images (WSIs), which, combined with weighted feature selection, supports
the fusion processing of more branching features and can be extended according
to specific application needs. Moreover, we introduce a sequence transformation
method tailored to varying WSI sizes, which enhances sequence-independent
features while preserving local sequence information, thereby improving
sequence information utilization. Extensive experiments demonstrate that
Mamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive
experiments across multiple datasets, achieving improvements in nearly all
performance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a
binary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the
BRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an
accuracy of 0.4981. The code is available at
https://github.com/YuqiZhang-Buaa/Mamba2MIL.

æè¦ï¼<paragraph>è¨ç®ççå­¸ (CPath) å·²é¡¯èæåççå­¸çè¨åºå¯¦åãåç®¡å·²æé²å±ï¼ä½çº CPath ä¸­ä¸åæåéçç¯ä¾ï¼å¤éå¯¦ä¾å­¸ç¿ (MIL) æçºé¢è¨ææ°ï¼ç¹å¥æ¯èä¸å®æ´è³è¨ä½¿ç¨æéãç¾æçæ¶æ§ï¼ä¾å¦åºæ¼å·ç©ç¥ç¶ç¶²è·¯ (CNN)ãæ³¨æååé¸ææ§ææç©ºéçæåºåæ¨¡å (SSM) çæ¶æ§ï¼å¨èååç¨®ç¹å¾µæç¼ºä¹è¶³å¤ çå½æ§åå¯æ´åæ§ï¼ä¸ç¡æ³ææèååç¨®ç¹å¾µãæ­¤å¤ï¼ç®åçä½æ³ä¸¦æªååå©ç¨èé åºç¸éåèé åºç¡éçç¹å¾µï¼å°è´åºåè³è¨ä½¿ç¨çä¸ä½³ãçºäºè§£æ±ºéäºéå¶ï¼æåæåºä¸ååçº Mamba2MIL çæ° MIL æ¶æ§ãæåçæ¶æ§å©ç¨çæç©ºéå°å¶æ¨¡å (SSD) ä¾å»ºæ¨¡å¨å¹»ççå½±å (WSI) çé·åºåè²¼çï¼éèå æ¬ç¹å¾µé¸åçµåä½¿ç¨ï¼æ¯æ´æ´å¤åæ¯ç¹å¾µçèåèçï¼ä¸å¯æ ¹æç¹å®æç¨éæ±é²è¡å»¶ä¼¸ãæ­¤å¤ï¼æåå¼å¥ä¸ç¨®éå°ä¸å WSI å¤§å°éèº«æé çåºåè½ææ¹æ³ï¼éå¢å¼·äºèåºåç¡éçç¹å¾µï¼åæä¿çäºå±é¨åºåè³è¨ï¼é²èæ¹ååºåè³è¨ä½¿ç¨çãå»£æ³çå¯¦é©è­æ Mamba2MIL è¶è¶äºæåé²ç MIL æ¹æ³ãæåå¨å¤åè³æéä¸é²è¡å»£æ³çå¯¦é©ï¼å¨å¹¾ä¹æææè½ææ¨ä¸åç²å¾æ¹åãç¹å¥æ¯å¨ NSCLC è³æéä¸ï¼Mamba2MIL éå° 0.9533 çäºåè«ç¤åé¡ AUC å 0.8794 çæºç¢ºåº¦ãå¨ BRACS è³æéä¸ï¼å®éå° 0.7986 çå¤é¡å¥åé¡ AUC å 0.4981 çæºç¢ºåº¦ãç¨å¼ç¢¼å¯å¨ https://github.com/YuqiZhang-Buaa/Mamba2MIL åå¾ã</paragraph>

##### **Sequence-aware Pre-training for Echocardiography Probe Guidance**
2408.15026v1 by Haojun Jiang, Zhenguo Sun, Yu Sun, Ning Jia, Meng Li, Shaqi Luo, Shiji Song, Gao Huang

Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe
pose to obtain high-quality sectional images. Cardiac ultrasound faces two
major challenges: (1) the inherently complex structure of the heart, and (2)
significant individual variations. Previous works have only learned the
population-averaged 2D and 3D structures of the heart rather than personalized
cardiac structural features, leading to a performance bottleneck. Clinically,
we observed that sonographers adjust their understanding of a patient's cardiac
structure based on prior scanning sequences, thereby modifying their scanning
strategies. Inspired by this, we propose a sequence-aware self-supervised
pre-training method. Specifically, our approach learns personalized 2D and 3D
cardiac structural features by predicting the masked-out images and actions in
a scanning sequence. We hypothesize that if the model can predict the missing
content it has acquired a good understanding of the personalized cardiac
structure. In the downstream probe guidance task, we also introduced a sequence
modeling approach that models individual cardiac structural information based
on the images and actions from historical scan data, enabling more accurate
navigation decisions. Experiments on a large-scale dataset with 1.36 million
samples demonstrated that our proposed sequence-aware paradigm can
significantly reduce navigation errors, with translation errors decreasing by
15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared
to state-of-the-art methods.

æè¦ï¼<paragraph>å¿èè¶é³æ³¢æ¢é ­å¼å°æ¨å¨å¹«å©æ°æèª¿æ´ 6-DOF æ¢é ­å§¿å¢ï¼ä»¥åå¾é«åè³ªçæ·é¢å½±åãå¿èè¶é³æ³¢é¢è¨å©é ä¸»è¦ææ°ï¼(1) å¿èçµæ§è¤éä¸åºæï¼ä»¥å (2) åé«å·®ç°é¡¯èãååçç ç©¶åå­¸ç¿äºæ´é«å¹³åç 2D å 3D å¿èçµæ§ï¼èéåäººåçè§£åç¹å¾µï¼å°è´æè½ç¶é ¸ãè¨åºä¸ï¼æåè§å¯å°è¶é³æ³¢æå¸«ææ ¹æååçææåºåèª¿æ´ä»åå°æ£èå¿èçµæ§ççè§£ï¼é²èä¿®æ¹ä»åçææç­ç¥ãåå°æ­¤åç¼ï¼æåæåºä¸åå·åºåæç¥çèªç£ç£é è¨ç·´æ¹æ³ãå·é«ä¾èªªï¼æåçåæ³ééé æ¸¬ææåºåä¸­é®ç½©çå½±åååä½ï¼ä¾å­¸ç¿åäººåç 2D å 3D å¿èè§£åç¹å¾µãæååè¨­ï¼å¦ææ¨¡åå¯ä»¥é æ¸¬éºæ¼çå§å®¹ï¼å®ä¾¿å°åäººåçè§£åçµæ§æäºè¯å¥½ççè§£ãå¨ä¸æ¸¸çæ¢é ­å¼å°ä»»åä¸­ï¼æåä¹å°å¥ä¸ååºåå»ºæ¨¡æ¹æ³ï¼è©²æ¹æ³æ ¹ææ­·å²ææè³æä¸­çå½±åååä½ï¼æ¨¡æ¬åå¥å¿èè§£åè³è¨ï¼é²èååºæ´ç²¾ç¢ºçå°èªæ±ºç­ãå¨æ 136 è¬åæ¨£æ¬çå¤§è¦æ¨¡è³æéä¸é²è¡çå¯¦é©è­æï¼æåæåºçå·åºåæç¥çå¸ç¯å¯ä»¥å¤§å¹æ¸å°å°èªé¯èª¤ï¼å¶ä¸­å¹³ç§»é¯èª¤æ¸å°äº 15.90% è³ 36.87%ï¼æè½é¯èª¤æ¸å°äº 11.13% è³ 20.77%ï¼èæåé²çæ¹æ³ç¸æ¯ã</paragraph>

##### **Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**
2408.15294v2 by Christos Theodoropoulos, Natasha Mulligan, Joao Bettencourt-Silva

Developing novel predictive models with complex biomedical information is
challenging due to various idiosyncrasies related to heterogeneity,
standardization or sparseness of the data. We previously introduced a
person-centric ontology to organize information about individual patients, and
a representation learning framework to extract person-centric knowledge graphs
(PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose a
systematic approach to examine the results of GNN models trained with both
structured and unstructured information from the MIMIC-III dataset. Through
ablation studies on different clinical, demographic, and social data, we show
the robustness of this approach in identifying predictive features in PKGs for
the task of readmission prediction.

æè¦ï¼éç¼å·æè¤éçç©é«å­¸è³è¨çæ°ç©é æ¸¬æ¨¡åï¼ç±æ¼è³æçç°è³ªæ§ãæ¨æºåæç¨çæ§ï¼å æ­¤å·æææ°æ§ãæåååä»ç´¹äºä¸åä»¥äººçºä¸­å¿çæ¬é«ï¼ç¨æ¼çµç¹æéåå¥æ£èçè³è¨ï¼ä»¥åä¸åè¡¨ç¤ºå­¸ç¿æ¶æ§ï¼ç¨æ¼æåä»¥äººçºä¸­å¿çç¥è­åè­ (PKG) åè¨ç·´åå½¢ç¥ç¶ç¶²è·¯ (GNN)ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®ç³»çµ±æ§çæ¹æ³ä¾æª¢é©ä½¿ç¨ MIMIC-III è³æéä¸­ççµæ§ååéçµæ§åè³è¨è¨ç·´ç GNN æ¨¡åççµæãééå°ä¸åçè¨åºãäººå£çµ±è¨åç¤¾æè³æé²è¡æ¶èç ç©¶ï¼æåå±ç¤ºäºéç¨®æ¹æ³å¨è­å¥ PKG ä¸­çé æ¸¬ç¹å¾µä»¥é²è¡åå¥é¢é æ¸¬ä»»åæçç©©å¥æ§ã

##### **Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**
2408.14754v1 by Qiaoxin Li, Ruifeng Chen, Peng Wang, Guotao Quan, Yanfeng Du, Dong Liang, Yinsheng Li

Dual-energy computed tomography (DECT) has been widely used to obtain
quantitative elemental composition of imaged subjects for personalized and
precise medical diagnosis. Compared with DECT leveraging advanced X-ray source
and/or detector technologies, the use of the sequential-scanning data
acquisition scheme to implement DECT may make a broader impact on clinical
practice because this scheme requires no specialized hardware designs and can
be directly implemented into conventional CT systems. However, since the
concentration of iodinated contrast agent in the imaged subject varies over
time, sequentially scanned data sets acquired at two tube potentials are
temporally inconsistent. As existing material basis image reconstruction
approaches assume that the data sets acquired at two tube potentials are
temporally consistent, the violation of this assumption results in inaccurate
quantification of material concentration. In this work, we developed
sequential-scanning DECT imaging using high temporal resolution image
reconstruction and error-compensated material basis image generation,
ACCELERATION in short, to address the technical challenge induced by temporal
inconsistency of sequentially scanned data sets and improve quantification
accuracy of material concentration in sequential-scanning DECT. ACCELERATION
has been validated and evaluated using numerical simulation data sets generated
from clinical human subject exams and experimental human subject studies.
Results demonstrated the improvement of quantification accuracy and image
quality using ACCELERATION.

æè¦ï¼éè½éé»è¦æ·å±¤ææ (DECT) å·²å»£æ³ç¨æ¼åå¾å½±åååè©¦èçå®éåç´ çµæï¼ä»¥é²è¡åäººåä¸ç²¾ç¢ºçé«çè¨ºæ·ãèä½¿ç¨é²é X åæºå/æåµæ¸¬å¨æè¡ç DECT ç¸æ¯ï¼ä½¿ç¨é£çºææè³ææ·åæ¹æ¡ä¾å¯¦ä½ DECT å¯è½å°è¨åºå¯¦åç¢çæ´å»£æ³çå½±é¿ï¼å çºæ­¤æ¹æ¡ä¸éè¦å°éçç¡¬é«è¨­è¨ï¼ä¸å¯ç´æ¥å¯¦ä½å°å³çµ± CT ç³»çµ±ä¸­ãç¶èï¼ç±æ¼å½±åååè©¦èä¸­ç¢åå°æ¯åçæ¿åº¦æé¨æéèè®åï¼å æ­¤å¨å©åç®¡é»ä½ä¸æ·åçé£çºææè³æéå¨æéä¸ä¸¦ä¸ä¸è´ãç±æ¼ç¾æçææåºç¤å½±åéå»ºæ¹æ³åè¨­å¨å©åç®¡é»ä½ä¸æ·åçè³æéå¨æéä¸æ¯ä¸è´çï¼å æ­¤éåæ­¤åè¨­æå°è´æææ¿åº¦çéåä¸æºç¢ºãå¨éé å·¥ä½ä¸­ï¼æåéç¼äºä½¿ç¨é«æéè§£æåº¦å½±åéå»ºåèª¤å·®è£åææåºç¤å½±åç¢ççé£çºææ DECT å½±åï¼ç°¡ç¨± ACCELERATIONï¼ä»¥è§£æ±ºé£çºææè³æéæéä¸ä¸è´æå¼ç¼çæè¡ææ°ï¼ä¸¦æ¹åé£çºææ DECT ä¸­æææ¿åº¦çéåæºç¢ºåº¦ãACCELERATION å·²ä½¿ç¨å¾è¨åºäººé«åè©¦èæª¢æ¥åå¯¦é©äººé«åè©¦èç ç©¶ç¢ççæ¸å¼æ¨¡æ¬è³æéé²è¡é©è­åè©ä¼°ãçµæè­æä½¿ç¨ ACCELERATION å¯æ¹åéåæºç¢ºåº¦åå½±ååè³ªã

##### **Large Language Models for Disease Diagnosis: A Scoping Review**
2409.00097v1 by Shuang Zhou, Zidu Xu, Mian Zhang, Chunpu Xu, Yawen Guo, Zaifu Zhan, Sirui Ding, Jiashuo Wang, Kaishuai Xu, Yi Fang, Liqiao Xia, Jeremy Yeung, Daochen Zha, Mingquan Lin, Rui Zhang

Automatic disease diagnosis has become increasingly valuable in clinical
practice. The advent of large language models (LLMs) has catalyzed a paradigm
shift in artificial intelligence, with growing evidence supporting the efficacy
of LLMs in diagnostic tasks. Despite the growing attention in this field, many
critical research questions remain under-explored. For instance, what diseases
and LLM techniques have been investigated for diagnostic tasks? How can
suitable LLM techniques and evaluation methods be selected for clinical
decision-making? To answer these questions, we performed a comprehensive
analysis of LLM-based methods for disease diagnosis. This scoping review
examined the types of diseases, associated organ systems, relevant clinical
data, LLM techniques, and evaluation methods reported in existing studies.
Furthermore, we offered guidelines for data preprocessing and the selection of
appropriate LLM techniques and evaluation strategies for diagnostic tasks. We
also assessed the limitations of current research and delineated the challenges
and future directions in this research field. In summary, our review outlined a
blueprint for LLM-based disease diagnosis, helping to streamline and guide
future research endeavors.

æè¦ï¼èªåç¾çè¨ºæ·å¨è¨åºå¯¦åä¸­è®å¾è¶ä¾è¶æå¹å¼ãå¤§èªè¨æ¨¡å (LLM) çåºç¾å¬åäºäººå·¥æºè½çå¸ç¯è½ç§»ï¼è¶ä¾è¶å¤è­ææ¯æ LLM å¨è¨ºæ·ä»»åä¸­çæè½ãåç®¡éåé ååå°è¶ä¾è¶å¤çéæ³¨ï¼ä½è¨±å¤ééµçç ç©¶åé¡ä»æªå¾å°ååæ¢è¨ãä¾å¦ï¼åªäºç¾çå LLM æè¡å·²è¢«èª¿æ¥ç¨æ¼è¨ºæ·ä»»åï¼å¦ä½çºè¨åºæ±ºç­å¶å®é¸æåé©ç LLM æè¡åè©ä¼°æ¹æ³ï¼çºäºåç­éäºåé¡ï¼æåå°åºæ¼ LLM çç¾çè¨ºæ·æ¹æ³é²è¡äºå¨é¢çåæãéé ç¯åæ¢è¨åé¡§äºç¾æç ç©¶ä¸­å ±åçç¾çé¡åãç¸éå¨å®ç³»çµ±ãç¸éè¨åºè³æãLLM æè¡åè©ä¼°æ¹æ³ãæ­¤å¤ï¼æåæä¾äºè³æåèçåé¸æé©ç¶ç LLM æè¡åè©ä¼°ç­ç¥ä»¥é²è¡è¨ºæ·ä»»åçæåãæåéè©ä¼°äºç¶åç ç©¶çéå¶ï¼ä¸¦æè¿°äºéåç ç©¶é åçææ°åæªä¾æ¹åãç¸½ä¹ï¼æåçåé¡§æ¦è¿°äºåºæ¼ LLM çç¾çè¨ºæ·èåï¼æå©æ¼ç°¡ååæå°æªä¾çç ç©¶å·¥ä½ã

##### **Elementary School Students' and Teachers' Perceptions Towards Creative Mathematical Writing with Generative AI**
2409.06723v1 by Yukyeong Song, Jinhee Kim, Wanli Xing, Zifeng Liu, Chenglu Li, Hyunju Oh

While mathematical creative writing can potentially engage students in
expressing mathematical ideas in an imaginative way, some elementary school-age
students struggle in this process. Generative AI (GenAI) offers possibilities
for supporting creative writing activities, such as providing story generation.
However, the design of GenAI-powered learning technologies requires careful
consideration of the technology reception in the actual classrooms. This study
explores students' and teachers' perceptions of creative mathematical writing
with the developed GenAI-powered technology. The study adopted a qualitative
thematic analysis of the interviews, triangulated with open-ended survey
responses and classroom observation of 79 elementary school students, resulting
in six themes and 19 subthemes. This study contributes by investigating the
lived experience of GenAI-supported learning and the design considerations for
GenAI-powered learning technologies and instructions.

æè¦ï¼<paragraph>éç¶æ¸å­¸åµæå¯«ä½æå¯è½è®å­¸çä»¥å¯ææ³ååçæ¹å¼è¡¨éæ¸å­¸æ¦å¿µï¼ä½æäºå°å­¸å¹´é½¡çå­¸çå¨éåéç¨ä¸­æéå°å°é£ãçæå¼ AI (GenAI) æä¾äºæ¯æåµæå¯«ä½æ´»åçå¯è½æ§ï¼ä¾å¦æä¾æäºçæãç¶èï¼GenAI é©åçå­¸ç¿æè¡çè¨­è¨éè¦ä»ç´°èæ®æè¡å¨å¯¦éæå®¤ä¸­çæ¥ååº¦ãæ¬ç ç©¶æ¢è¨äºå­¸çåæå¸«å°ä½¿ç¨å·²éç¼ç GenAI é©åæè¡é²è¡åµææ¸å­¸å¯«ä½ççæ³ãæ¬ç ç©¶æ¡ç¨äºè¨ªè«çå®æ§ä¸»é¡åæï¼ä¸¦è 79 åå°å­¸ççéæ¾å¼èª¿æ¥åæåæå®¤è§å¯é²è¡ä¸è§é©è­ï¼ç¢çäºå­åä¸»é¡å 19 åå­ä¸»é¡ãæ¬ç ç©¶ééèª¿æ¥ GenAI æ¯æçå­¸ç¿ççå¯¦é«é©ä»¥å GenAI é©åçå­¸ç¿æè¡åèªªæçè¨­è¨èéï¼ååºäºè²¢ç»ã</paragraph>

##### **Improving Clinical Note Generation from Complex Doctor-Patient Conversation**
2408.14568v1 by Yizhan Li, Sifan Wu, Christopher Smith, Thomas Lo, Bang Liu

Writing clinical notes and documenting medical exams is a critical task for
healthcare professionals, serving as a vital component of patient care
documentation. However, manually writing these notes is time-consuming and can
impact the amount of time clinicians can spend on direct patient interaction
and other tasks. Consequently, the development of automated clinical note
generation systems has emerged as a clinically meaningful area of research
within AI for health. In this paper, we present three key contributions to the
field of clinical note generation using large language models (LLMs). First, we
introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex
doctor-patient conversations paired with their full clinical notes. This
dataset, created and curated by medical experts with the help of modern neural
networks, provides a valuable resource for training and evaluating models in
clinical note generation tasks. Second, we propose the K-SOAP (Keyword,
Subjective, Objective, Assessment, and Plan) note format, which enhances
traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and
Plan) notes by adding a keyword section at the top, allowing for quick
identification of essential information. Third, we develop an automatic
pipeline to generate K-SOAP notes from doctor-patient conversations and
benchmark various modern LLMs using various metrics. Our results demonstrate
significant improvements in efficiency and performance compared to standard LLM
finetuning methods.

æè¦ï¼æ°å¯«è¨åºç­è¨åè¨éé«çæª¢æ¥æ¯é«çä¿å¥å°æ¥­äººå¡çä¸é éè¦ä»»åï¼æ¯æ£èç§è­·æä»¶ä¸­çéè¦çµæé¨åãç¶èï¼æåæ°å¯«éäºç­è¨å¾èæï¼ä¸¦ä¸æå½±é¿è¨åºé«çè±å¨ç´æ¥æ£èäºååå¶ä»ä»»åä¸çæéãå æ­¤ï¼èªååè¨åºç­è¨çæç³»çµ±çéç¼å·²æçº AI å¨å¥åº·é åä¸­å·æè¨åºæç¾©çç ç©¶é åãå¨æ¬æä¸­ï¼æåæåºäºä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) é²è¡è¨åºç­è¨çæçé åç 3 é ééµè²¢ç»ãé¦åï¼æåä»ç´¹äº CliniKnoteï¼éæ¯ä¸åç¶åæ§æ¸æéï¼åå« 1,200 åè¤éçé«æ£å°è©±åå¶å®æ´çè¨åºç­è¨ãæ­¤æ¸æéç±é«å­¸å°å®¶å¨ç¾ä»£ç¥ç¶ç¶²è·¯çå¹«å©ä¸åµå»ºåç­åï¼çºè¨åºç­è¨çæä»»åä¸­çæ¨¡åè¨ç·´åè©ä¼°æä¾äºå¯¶è²´çè³æºãå¶æ¬¡ï¼æåæåºäº K-SOAPï¼ééµå­ãä¸»è§ãå®¢è§ãè©ä¼°åè¨ç«ï¼ç­è¨æ ¼å¼ï¼å®ééå¨é é¨æ·»å ä¸åééµå­é¨åä¾å¢å¼·å³çµ±ç SOAP~\cite{podder2023soap}ï¼ä¸»è§ãå®¢è§ãè©ä¼°åè¨ç«ï¼ç­è¨ï¼ä»¥ä¾¿å¿«éè­å¥åºæ¬è³è¨ãç¬¬ä¸ï¼æåéç¼äºä¸åèªååç®¡éï¼å¾é«æ£å°è©±ä¸­çæ K-SOAP ç­è¨ï¼ä¸¦ä½¿ç¨åç¨®ææ¨å°åç¨®ç¾ä»£ LLM é²è¡åºæºæ¸¬è©¦ãæåççµæè¡¨æï¼èæ¨æº LLM å¾®èª¿æ¹æ³ç¸æ¯ï¼æçåæ§è½æäºé¡¯èçæåã

##### **Temporal Ensemble Logic**
2408.14443v2 by Guo-Qiang Zhang

We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal
logic for linear-time temporal reasoning. TEL includes primitive temporal
constructs such as ``always up to $t$ time later'' ($\Box_t$), ``sometimes
before $t$ time in the future'' ($\Diamond_t$), and ``$t$-time later''
$\varphi_t$. TEL has been motivated from the requirement for rigor and
reproducibility for cohort specification and discovery in clinical and
population health research, to fill a gap in formalizing temporal reasoning in
biomedicine. Existing logical frameworks such as linear temporal logic are too
restrictive to express temporal and sequential properties in biomedicine, or
too permissive in semantic constructs, such as in Halpern-Shoham logic, to
serve this purpose. In this paper, we first introduce TEL in a general set up,
with discrete and dense time as special cases. We then focus on the theoretical
development of discrete TEL on the temporal domain of positive integers
$\mathbb{N}^+$, denoted as ${\rm TEL}_{\mathbb{N}^+}$. ${\rm
TEL}_{\mathbb{N}^+}$ is strictly more expressive than the standard monadic
second order logic, characterized by B\"{u}chi automata. We present its formal
semantics, a proof system, and provide a proof for the undecidability of the
satisfiability of ${\rm TEL}_{\mathbb{N}^+}$. We also include initial results
on expressiveness and decidability fragments for ${\rm TEL}_{\mathbb{N}^+}$,
followed by application outlook and discussions.

æè¦ï¼<paragraph>æåä»ç´¹æééåéè¼¯ (TEL)ï¼ä¸ç¨®ç¨æ¼ç·æ§æéæææ¨ççä¸éå®å­æ¨¡æéè¼¯ãTEL åå«åå§ææçµæ§ï¼ä¾å¦ãå§çµå¨ $t$ æéå¾ã($\Box_t$)ããææå¨æªä¾ $t$ æéåã($\Diamond_t$) åã$t$ æéå¾ã$\varphi_t$ãTEL çåæ©æ¯çºäºè¨åºåäººå£å¥åº·ç ç©¶ä¸­ç¾¤çµè¦ç¯åç¼ç¾çå´è¬¹æ§åå¯åç¾æ§ï¼ä»¥å¡«è£çç©é«å­¸ä¸­æææ¨çå½¢å¼åçç©ºç½ãç¾æçéè¼¯æ¡æ¶ï¼ä¾å¦ç·æ§ææéè¼¯ï¼å°æ¼è¡¨éçç©é«å­¸ä¸­çææåé åºå±¬æ§éæ¼å´æ ¼ï¼æèå¨èªç¾©çµæ§ä¸éæ¼å¯¬é¬ï¼ä¾å¦å¨ Halpern-Shoham éè¼¯ä¸­ï¼ï¼ç¡æ³éå°æ­¤ç®çãå¨æ¬æä¸­ï¼æåé¦åå¨ä¸è¬è¨­ç½®ä¸­ä»ç´¹ TELï¼å¶ä¸­é¢æ£æéåç¨ å¯æéçºç¹æ®ææ³ãç¶å¾ï¼æåå°æ³¨æ¼æ­£æ´æ¸æéå $\mathbb{N}^+$ ä¸é¢æ£ TEL ççè«ç¼å±ï¼è¡¨ç¤ºçº ${\rm TEL}_{\mathbb{N}^+}$. ${\rm TEL}_{\mathbb{N}^+}$ æ¯æ¨æºå®å­äºééè¼¯æ´å·è¡¨éåï¼å¶ç¹å¾µå¨æ¼ B\"{u}chi èªåæ©ãæåå±ç¤ºå¶å½¢å¼èªç¾©ãè­æç³»çµ±ï¼ä¸¦æä¾ ${\rm TEL}_{\mathbb{N}^+}$ å¯æ»¿è¶³æ§çä¸å¯å¤å®æ§çè­æãæåéåæ¬ ${\rm TEL}_{\mathbb{N}^+}$ çè¡¨éååå¯å¤å®çæ®µçåæ­¥çµæï¼ç¶å¾æ¯æç¨åæ¯åè¨è«ã</paragraph>

##### **MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**
2408.14418v2 by Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler

Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech
into text, yet the errors they introduce can significantly degrade the
performance of downstream tasks like summarization. This issue is particularly
pronounced in clinical dialogue summarization, a low-resource domain where
supervised data for fine-tuning is scarce, necessitating the use of ASR models
as black-box solutions. Employing conventional data augmentation for enhancing
the noise robustness of summarization models is not feasible either due to the
unavailability of sufficient medical dialogue audio recordings and
corresponding ASR transcripts. To address this challenge, we propose MEDSAGE,
an approach for generating synthetic samples for data augmentation using Large
Language Models (LLMs). Specifically, we leverage the in-context learning
capabilities of LLMs and instruct them to generate ASR-like errors based on a
few available medical dialogue examples with audio recordings. Experimental
results show that LLMs can effectively model ASR noise, and incorporating this
noisy data into the training process significantly improves the robustness and
accuracy of medical dialogue summarization systems. This approach addresses the
challenges of noisy ASR outputs in critical applications, offering a robust
solution to enhance the reliability of clinical dialogue summarization.

æè¦ï¼èªåèªé³è¾¨è­ (ASR) ç³»çµ±å¨å°èªé³è½éææå­æ¹é¢è³ééè¦ï¼ä½å®åæç¢ççé¯èª¤å¯è½æå¤§å¹éä½æè¦ç­ä¸æ¸¸ä»»åçæè½ãéååé¡å¨è¨åºå°è©±æè¦ä¸­ç¹å¥æé¡¯ï¼éæ¯ä¸åä½è³æºçé åï¼å¶ä¸­ç¨æ¼å¾®èª¿çç£ç£è³æå¾ç¨å°ï¼å æ­¤å¿é ä½¿ç¨ ASR æ¨¡åä½çºé»çè§£æ±ºæ¹æ¡ãç±æ¼ç¼ºä¹è¶³å¤ çé«çå°è©±é³è¨éé³åå°æç ASR è½éï¼æ¡ç¨å³çµ±è³ææ´åä¾å¢å¼·æè¦æ¨¡åçæåªæ§ä¹ä¸å¯è¡ãçºäºæå°éåææ°ï¼æåæåºäº MEDSAGEï¼éæ¯ä¸ç¨®ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) ç¢çåææ¨£æ¬é²è¡è³ææ´åçæ¹æ³ãå·é«ä¾èªªï¼æåå©ç¨ LLM çæå¢å­¸ç¿è½åï¼ä¸¦æç¤ºå®åæ ¹æå°æ¸å¸¶æé³è¨éé³çå¯ç¨é«çå°è©±ç¯ä¾ç¢çé¡ä¼¼ç ASR é¯èª¤ãå¯¦é©çµæé¡¯ç¤ºï¼LLM å¯ä»¥ææå°æ¨¡æ¬ ASR éè¨ï¼èå°éäºéè¨è³æç´å¥è¨ç·´éç¨ä¸­å¯ä»¥é¡¯èæé«é«çå°è©±æè¦ç³»çµ±çç©©å¥æ§åæºç¢ºæ§ãéç¨®æ¹æ³æå°äºééµæç¨ä¸­ ASR è¼¸åºéè¨çåé¡ï¼æä¾äºä¸åç©©å¥çè§£æ±ºæ¹æ¡ä¾å¢å¼·è¨åºå°è©±æè¦çå¯é æ§ã

##### **Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**
2408.14397v1 by Xiaoman Zhang, JuliÃ¡n N. Acosta, Hong-Yu Zhou, Pranav Rajpurkar

Recent advancements in artificial intelligence have significantly improved
the automatic generation of radiology reports. However, existing evaluation
methods fail to reveal the models' understanding of radiological images and
their capacity to achieve human-level granularity in descriptions. To bridge
this gap, we introduce a system, named ReXKG, which extracts structured
information from processed reports to construct a comprehensive radiology
knowledge graph. We then propose three metrics to evaluate the similarity of
nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs
(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative
analysis of AI-generated and human-written radiology reports, assessing the
performance of both specialist and generalist models. Our study provides a
deeper understanding of the capabilities and limitations of current AI models
in radiology report generation, offering valuable insights for improving model
performance and clinical applicability.

æè¦ï¼è¿æäººå·¥æºè½çé²å±é¡¯èæ¹åäºæ¾å°å ±åçèªåçæãç¶èï¼ç¾æçè©ä¼°æ¹æ³ç¡æ³æ­ç¤ºæ¨¡åå°æ¾å°å½±åççè§£ï¼ä»¥åå®åå¨æè¿°ä¸­éå°äººé¡å±¤ç´ç²¾ç´°åº¦çè½åãçºäºå½è£éåå·®è·ï¼æåå¼é²ä¸ååçº ReXKG çç³»çµ±ï¼å®å¾èçéçå ±åä¸­èååºçµæ§åçè³è¨ï¼ä»¥å»ºæ§ä¸åå¨é¢çæ¾å°ç¥è­åè­ãæ¥èï¼æåæåºä¸åææ¨ä¾è©ä¼°åç¨®ç¥è­åè­ä¸­ç¯é»çç¸ä¼¼æ§ (ReXKG-NSC)ãéç·£çåå¸ (ReXKG-AMS) åå­åçæ¶µèç¯å (ReXKG-SCS)ãæåå° AI çæçåäººé¡æ°å¯«çæ¾å°å ±åé²è¡æ·±å¥çæ¯è¼åæï¼è©ä¼°å°å®¶åéææ¨¡åçæè½ãæåçç ç©¶æä¾å°ç®å AI æ¨¡åå¨æ¾å°å ±åçæä¸­çè½ååéå¶æ´æ·±å¥ççè§£ï¼ä¸¦æä¾æå¹å¼çè¦è§£ä¾æ¹åæ¨¡åæè½åè¨åºæç¨ã

##### **Foundation Models for Music: A Survey**
2408.14340v3 by Yinghao Ma, Anders Ãland, Anton Ragni, Bleiz MacSen Del Sette, Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elona Shatri, Fabio Morreale, Ge Zhang, GyÃ¶rgy Fazekas, Gus Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin, Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B. Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shuqi Dai, Shun Lei, Shiyin Kang, Simon Dixon, Wenhu Chen, Wenhao Huang, Xingjian Du, Xingwei Qu, Xu Tan, Yizhi Li, Zeyue Tian, Zhiyong Wu, Zhizheng Wu, Ziyang Ma, Ziyu Wang

In recent years, foundation models (FMs) such as large language models (LLMs)
and latent diffusion models (LDMs) have profoundly impacted diverse sectors,
including music. This comprehensive review examines state-of-the-art (SOTA)
pre-trained models and foundation models in music, spanning from representation
learning, generative learning and multimodal learning. We first contextualise
the significance of music in various industries and trace the evolution of AI
in music. By delineating the modalities targeted by foundation models, we
discover many of the music representations are underexplored in FM development.
Then, emphasis is placed on the lack of versatility of previous methods on
diverse music applications, along with the potential of FMs in music
understanding, generation and medical application. By comprehensively exploring
the details of the model pre-training paradigm, architectural choices,
tokenisation, finetuning methodologies and controllability, we emphasise the
important topics that should have been well explored, like instruction tuning
and in-context learning, scaling law and emergent ability, as well as
long-sequence modelling etc. A dedicated section presents insights into music
agents, accompanied by a thorough analysis of datasets and evaluations
essential for pre-training and downstream tasks. Finally, by underscoring the
vital importance of ethical considerations, we advocate that following research
on FM for music should focus more on such issues as interpretability,
transparency, human responsibility, and copyright issues. The paper offers
insights into future challenges and trends on FMs for music, aiming to shape
the trajectory of human-AI collaboration in the music realm.

æè¦ï¼è¿å¹´ä¾ï¼åºç¤æ¨¡å (FM)ï¼ä¾å¦å¤§åèªè¨æ¨¡å (LLM) åæ½å¨æ´æ£æ¨¡å (LDM)ï¼å·²å°åæ¬é³æ¨å¨å§çä¸åç¢æ¥­ç¢çæ·±é å½±é¿ãéç¯å¨é¢æ§çè©è«æ¢è¨äºé³æ¨é åä¸­æåé² (SOTA) çé è¨ç·´æ¨¡åååºç¤æ¨¡åï¼æ¶µèäºè¡¨å¾µå­¸ç¿ãçæå¼å­¸ç¿åå¤æ¨¡æå­¸ç¿ãæåé¦åå°é³æ¨å¨åç¢æ¥­çéè¦æ§èçµ¡åï¼ä¸¦è¿½æº¯ AI å¨é³æ¨ä¸­çæ¼é²ãééæç¹ªåºç¤æ¨¡åæéå°çæ¨¡æï¼æåç¼ç¾è¨±å¤é³æ¨è¡¨å¾µå¨ FM éç¼ä¸­å°æªè¢«ååæ¢ç´¢ãæ¥èï¼æåå¼·èª¿ååæ¹æ³å¨ä¸åé³æ¨æç¨ä¸­ç¼ºä¹å¤æ¨£æ§ï¼ä»¥å FM å¨é³æ¨çè§£ãçæåé«çæç¨ä¸­çæ½åãééå¨é¢æ¢è¨æ¨¡åé è¨ç·´å¸ç¯ãæ¶æ§é¸æãæ¨è¨åãå¾®èª¿æ¹æ³åå¯æ§æ§çç´°ç¯ï¼æåå¼·èª¿äºææ·±å¥æ¢è¨çéè¦ä¸»é¡ï¼ä¾å¦æä»¤å¾®èª¿åæå¢å­¸ç¿ãè¦æ¨¡å®å¾åæ°èè½åï¼ä»¥åé·åºåå»ºæ¨¡ç­ãå°éçç« ç¯æä¾äºå°é³æ¨ä»£ççè¦è§£ï¼ä¸¦éæå°é è¨ç·´åä¸æ¸¸ä»»åè³ééè¦çè³æéåè©ä¼°çæ·±å¥åæãæå¾ï¼ééå¼·èª¿å«çèéçè³ééè¦æ§ï¼æåä¸»å¼µå¾çºéæ¼é³æ¨ FM çç ç©¶ææ´å°æ³¨æ¼å¯è§£éæ§ãéæåº¦ãäººé¡è²¬ä»»åçæ¬åé¡ç­è­°é¡ãæ¬ææä¾äºå°é³æ¨ FM æªä¾ææ°åè¶¨å¢çè¦è§£ï¼æ¨å¨å½¢å¡äººé¡è AI å¨é³æ¨é åä¸­åä½çè»è·¡ã

##### **Uncertainties of Latent Representations in Computer Vision**
2408.14281v1 by Michael Kirchhof

Uncertainty quantification is a key pillar of trustworthy machine learning.
It enables safe reactions under unsafe inputs, like predicting only when the
machine learning model detects sufficient evidence, discarding anomalous data,
or emitting warnings when an error is likely to be inbound. This is
particularly crucial in safety-critical areas like medical image classification
or self-driving cars. Despite the plethora of proposed uncertainty
quantification methods achieving increasingly higher scores on performance
benchmarks, uncertainty estimates are often shied away from in practice. Many
machine learning projects start from pretrained latent representations that
come without uncertainty estimates. Uncertainties would need to be trained by
practitioners on their own, which is notoriously difficult and
resource-intense.
  This thesis makes uncertainty estimates easily accessible by adding them to
the latent representation vectors of pretrained computer vision models. Besides
proposing approaches rooted in probability and decision theory, such as
Monte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both
theoretical and empirical questions. We show that these unobservable
uncertainties about unobservable latent representations are indeed provably
correct. We also provide an uncertainty-aware representation learning (URL)
benchmark to compare these unobservables against observable ground-truths.
Finally, we compile our findings to pretrain lightweight representation
uncertainties on large-scale computer vision models that transfer to unseen
datasets in a zero-shot manner.
  Our findings do not only advance the current theoretical understanding of
uncertainties over latent variables, but also facilitate the access to
uncertainty quantification for future researchers inside and outside the field,
enabling straightforward but trustworthy machine learning.

æè¦ï¼ä¸ç¢ºå®éåæ¯å¼å¾ä¿¡è³´æ©å¨å­¸ç¿çä¸å¤§æ¯æ±ã
å®è½è®æ©å¨å­¸ç¿æ¨¡åå¨ä¸å®å¨çè¼¸å¥ä¸ååºå®å¨çåæï¼ä¾å¦åªå¨æ©å¨å­¸ç¿æ¨¡ååµæ¸¬å°è¶³å¤ è­æææé²è¡é æ¸¬ãæ¨æ£ç°å¸¸è³æï¼ææ¯å¨å¯è½ç¼çé¯èª¤æç¼åºè­¦åãéå¨é«çå½±ååé¡æèªé§è»ç­å®å¨ééµé åä¸­ç¹å¥éè¦ãåç®¡æè¨±å¤å·²æåºçä¸ç¢ºå®éåæ¹æ³å¨æè½åºæºä¸åå¾è¶ä¾è¶é«çåæ¸ï¼ä½å¨å¯¦åä¸å»å¸¸å¸¸è¿´é¿ä¸ç¢ºå®æ§ä¼°è¨ãè¨±å¤æ©å¨å­¸ç¿å°æ¡å¾é è¨ç·´çæ½å¨è¡¨å¾µéå§ï¼èéäºè¡¨å¾µæ²æä¸ç¢ºå®æ§ä¼°è¨ãå¯¦åå·¥ä½èéè¦èªè¡è¨ç·´ä¸ç¢ºå®æ§ï¼éåºäºåçå°é£ä¸èè²»è³æºã
æ¬è«æééå°ä¸ç¢ºå®æ§ä¼°è¨æ°å¢å°é è¨ç·´é»è¦è¦è¦ºæ¨¡åçæ½å¨è¡¨å¾µåéä¸­ï¼è®ä¸ç¢ºå®æ§ä¼°è¨ææ¼åå¾ãé¤äºæåºæ¤åºæ¼æ©çåæ±ºç­çè«çæ¹æ³ï¼ä¾å¦èå°å¡ç¾è³è¨å°æ¯ä¼°è¨ (MCInfoNCE) åæå¤±é æ¸¬ä¹å¤ï¼æåéæ·±å¥æ¢è¨çè«åå¯¦è­åé¡ãæåè­æéäºéæ¼ä¸å¯è§å¯æ½å¨è¡¨å¾µçä¸å¯è§å¯ä¸ç¢ºå®æ§ç¢ºå¯¦å¯ä»¥è­ææ¯æ­£ç¢ºçãæåéæä¾ä¸åä¸ç¢ºå®æ§æç¥è¡¨å¾µå­¸ç¿ (URL) åºæºï¼ç¨ä¾æ¯è¼éäºä¸å¯è§å¯çä¸ç¢ºå®æ§èå¯è§å¯ççå¯¦å¼ãæå¾ï¼æåå°æåçç¼ç¾å½æ´èµ·ä¾ï¼å¨å¤§åé»è¦è¦è¦ºæ¨¡åä¸é è¨ç·´è¼éç´è¡¨å¾µä¸ç¢ºå®æ§ï¼ä¸¦ä»¥é¶æ¬¡å­¸ç¿çæ¹å¼è½ç§»å°æªè¦éçè³æéã
æåçç¼ç¾ä¸åæåäºç¶åå°æ½å¨è®æ¸ä¸ç¢ºå®æ§ççè«çè§£ï¼éä¿é²äºæªä¾ç ç©¶äººå¡å¨è©²é åå§å¤åå¾ä¸ç¢ºå®éåï¼é²èå¯¦ç¾ç´æ¥ä½å¼å¾ä¿¡è³´çæ©å¨å­¸ç¿ã

##### **Automatic Medical Report Generation: Methods and Applications**
2408.13988v1 by Li Guo, Anas M. Tahir, Dong Zhang, Z. Jane Wang, Rabab K. Ward

The increasing demand for medical imaging has surpassed the capacity of
available radiologists, leading to diagnostic delays and potential
misdiagnoses. Artificial intelligence (AI) techniques, particularly in
automatic medical report generation (AMRG), offer a promising solution to this
dilemma. This review comprehensively examines AMRG methods from 2021 to 2024.
It (i) presents solutions to primary challenges in this field, (ii) explores
AMRG applications across various imaging modalities, (iii) introduces publicly
available datasets, (iv) outlines evaluation metrics, (v) identifies techniques
that significantly enhance model performance, and (vi) discusses unresolved
issues and potential future research directions. This paper aims to provide a
comprehensive understanding of the existing literature and inspire valuable
future research.

æè¦ï¼ç±æ¼å°é«å­¸å½±åçéæ±æ¥çå¢é·ï¼å·²ç¶è¶éäºç¾ææ¾å°ç§é«å¸«çè½åï¼å°è´è¨ºæ·å»¶èª¤åæ½å¨çèª¤è¨ºãäººå·¥æºæ§ (AI) æè¡ï¼ç¹å¥æ¯å¨èªåé«çå ±åçæ (AMRG) æ¹é¢ï¼çºæ­¤å°å¢æä¾äºæå¸æçè§£æ±ºæ¹æ¡ãæ¬ç¯è©è«å¨é¢æ¢è¨äº 2021 å¹´è³ 2024 å¹´ç AMRG æ¹æ³ãå® (i) æåºè§£æ±ºæ­¤é åä¸­ä¸»è¦ææ°çæ¹æ¡ï¼(ii) æ¢è¨ AMRG å¨åç¨®å½±åæ¨¡å¼ä¸­çæç¨ï¼(iii) ä»ç´¹å¬éå¯ç¨çè³æéï¼(iv) æ¦è¿°è©ä¼°ææ¨ï¼(v) æ¾åºé¡¯èæåæ¨¡åæè½çæè¡ï¼ä»¥å (vi) è¨è«å°æªè§£æ±ºçåé¡åæ½å¨çæªä¾ç ç©¶æ¹åãæ¬ææ¨å¨æä¾å°ç¾ææç»çå¨é¢äºè§£ï¼ä¸¦æ¿ç¼æå¹å¼çæªä¾ç ç©¶ã

##### **Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**
2409.00084v2 by Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O Yang, Juan Echavarria, Sumbal Babar, Aasma Shaukat, Samuel Margolis, Nicholas P Tatonetti, Girish Nadkarni, Bara El Kurdi, Ali Soroush

Background and Aims: This study evaluates the medical reasoning performance
of large language models (LLMs) and vision language models (VLMs) in
gastroenterology.
  Methods: We used 300 gastroenterology board exam-style multiple-choice
questions, 138 of which contain images to systematically assess the impact of
model configurations and parameters and prompt engineering strategies utilizing
GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs
(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),
Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces
(web and API), computing environments (cloud and local), and model precisions
(with and without quantization). Finally, we assessed accuracy using a
semiautomated pipeline.
  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet
(74.0%) achieved the highest accuracy, outperforming the top open-source
models: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).
Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)
performed best. The scores of the quantized models were comparable to those of
the full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM
performance on image-containing questions did not improve when the images were
provided and worsened when LLM-generated captions were provided. In contrast, a
10% increase in accuracy was observed when images were accompanied by
human-crafted image descriptions.
  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in
medical reasoning, the integration of visual data remains a challenge for VLMs.
Effective deployment involves carefully determining optimal model
configurations, encouraging users to consider either the high performance of
proprietary models or the flexible adaptability of open-source models.

æè¦ï¼<paragraph>èæ¯èç®æ¨ï¼æ¬ç ç©¶è©ä¼°å¤§åèªè¨æ¨¡å (LLM) åè¦è¦ºèªè¨æ¨¡å (VLM) å¨è¸èçå­¸ä¸­çé«çæ¨çè¡¨ç¾ã
æ¹æ³ï¼æåä½¿ç¨ 300 åè¸èçå­¸å°ç§èè©¦é¢¨æ ¼çå¤é¸é¡ï¼å¶ä¸­ 138 ååå«å½±åï¼ä»¥ç³»çµ±æ§å°è©ä¼°æ¨¡åéç½®ååæ¸ä»¥åå©ç¨ GPT-3.5 çæç¤ºå·¥ç¨ç­ç¥çå½±é¿ãæ¥ä¸ä¾ï¼æåè©ä¼°å°æåéæº LLMï¼çæ¬ï¼çè¡¨ç¾ï¼åæ¬ GPTï¼3.5ã4ã4oã4ominiï¼ãClaudeï¼3ã3.5ï¼ãGeminiï¼1.0ï¼ãMistralãLlamaï¼2ã3ã3.1ï¼ãMixtral å Phiï¼3ï¼ï¼è·¨ä¸åä»é¢ï¼ç¶²è·¯å APIï¼ãéç®ç°å¢ï¼é²ç«¯åæ¬å°ï¼åæ¨¡åç²¾ç¢ºåº¦ï¼æåæ²æéåï¼ãæå¾ï¼æåä½¿ç¨åèªååç®¡éè©ä¼°æºç¢ºåº¦ã
çµæï¼å¨å°ææ¨¡åä¸­ï¼GPT-4oï¼73.7%ï¼å Claude3.5-Sonnetï¼74.0%ï¼éå°æé«æºç¢ºåº¦ï¼åªæ¼é å°çéæºæ¨¡åï¼Llama3.1-405bï¼64%ï¼ãLlama3.1-70bï¼58.3%ï¼å Mixtral-8x7bï¼54.3%ï¼ãå¨éåçéæºæ¨¡åä¸­ï¼6 ä½åéåç Phi3-14bï¼48.7%ï¼è¡¨ç¾æä½³ãéåæ¨¡åçåæ¸èå¨ç²¾åº¦æ¨¡å Llama2-7bãLlama2--13b å Gemma2-9b ç¸ç¶ãå¼å¾æ³¨æçæ¯ï¼ç¶æä¾å½±åæï¼VLM å¨åå«å½±åçåé¡ä¸çè¡¨ç¾ä¸¦æªæ¹åï¼èå¨æä¾ LLM çæçæ¨é¡æè¡¨ç¾æ¡åãç¸åå°ï¼ç¶å½±åéæäººå·¥è£½ä½çå½±åæè¿°æï¼æºç¢ºåº¦è§å¯å°å¢å äº 10%ã
çµè«ï¼çµè«èè¨ï¼éç¶ LLM å¨é«çæ¨çä¸­è¡¨ç¾åºå¼·å¥çé¶æ¬¡å­¸ç¿è¡¨ç¾ï¼ä½è¦è¦ºè³æçæ´åä»ç¶æ¯ VLM çä¸é ææ°ãææçé¨ç½²æ¶åä»ç´°ç¢ºå®æä½³æ¨¡åéç½®ï¼é¼åµä½¿ç¨èèæ®å°ææ¨¡åçé«æè½æéæºæ¨¡åçéæ´»é©ææ§ã</paragraph>

##### **PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**
2408.13836v1 by Zifan Chen, Xinyu Nan, Jiazheng Li, Jie Zhao, Haifeng Li, Zilin Lin, Haoshen Li, Heyun Chen, Yiting Liu, Bin Dong, Li Zhang, Lei Tang

Volumetric segmentation is crucial for medical imaging but is often
constrained by labor-intensive manual annotations and the need for
scenario-specific model training. Furthermore, existing general segmentation
models are inefficient due to their design and inferential approaches.
Addressing this clinical demand, we introduce PropSAM, a propagation-based
segmentation model that optimizes the use of 3D medical structure information.
PropSAM integrates a CNN-based UNet for intra-slice processing with a
Transformer-based module for inter-slice propagation, focusing on structural
and semantic continuities to enhance segmentation across various modalities.
Distinctively, PropSAM operates on a one-view prompt, such as a 2D bounding box
or sketch mask, unlike conventional models that require two-view prompts. It
has demonstrated superior performance, significantly improving the Dice
Similarity Coefficient (DSC) across 44 medical datasets and various imaging
modalities, outperforming models like MedSAM and SegVol with an average DSC
improvement of 18.1%. PropSAM also maintains stable predictions despite prompt
deviations and varying propagation configurations, confirmed by one-way ANOVA
tests with P>0.5985 and P>0.6131, respectively. Moreover, PropSAM's efficient
architecture enables faster inference speeds (Wilcoxon rank-sum test, P<0.001)
and reduces user interaction time by 37.8% compared to two-view prompt models.
Its ability to handle irregular and complex objects with robust performance
further demonstrates its potential in clinical settings, facilitating more
automated and reliable medical imaging analyses with minimal retraining.

æè¦ï¼é«ç©åå²å°æ¼é«å­¸å½±åè³ééè¦ï¼ä½éå¸¸åå°èè²»å¤§éäººåçæ¨è¨»åç¹å®å ´æ¯æ¨¡åè¨ç·´éæ±çéå¶ãæ­¤å¤ï¼ç¾æçéç¨åå²æ¨¡åç±æ¼å¶è¨­è¨åæ¨è«æ¹æ³èæçä½ä¸ãçºäºæ»¿è¶³éé è¨åºéæ±ï¼æåå¼å¥äº PropSAMï¼éæ¯ä¸ç¨®åºæ¼å³æ­çåå²æ¨¡åï¼åªåäº 3D é«å­¸çµæ§è³è¨çä½¿ç¨ãPropSAM æ´åäºä¸ååºæ¼ CNN ç UNetï¼ç¨æ¼åçå§èçï¼ä»¥åä¸ååºæ¼ Transformer çæ¨¡çµï¼ç¨æ¼åçéå³æ­ï¼éé»éæ³¨çµæ§åèªç¾©é£çºæ§ï¼ä»¥å¢å¼·åç¨®æ¨¡å¼ä¸çåå²ãèéè¦å©è¦æç¤ºçå³çµ±æ¨¡åä¸åï¼PropSAM ç¨ç¹å°éä½æ¼å®è¦æç¤ºä¸ï¼ä¾å¦ 2D éçæ¡æèåé®ç½©ãå®å·²è­æå·æåªç°çæè½ï¼é¡¯èæ¹åäº 44 åé«å­¸è³æéååç¨®å½±åæ¨¡å¼ä¸çéª°å­ç¸ä¼¼ä¿æ¸ (DSC)ï¼åªæ¼ MedSAM å SegVol ç­æ¨¡åï¼å¹³å DSC æåäº 18.1%ãåç®¡æç¤ºåå·®åå³æ­éç½®ä¸åï¼PropSAM ä»è½ç¶­æç©©å®çé æ¸¬ï¼éå·²ééå®å ANOVA æ¸¬è©¦å¾å°è­å¯¦ï¼åå¥çº P>0.5985 å P>0.6131ãæ­¤å¤ï¼PropSAM çé«ææ¶æ§è½å¯¦ç¾æ´å¿«çæ¨è«éåº¦ï¼Wilcoxon ç­ç´åç¸½åæª¢å®ï¼P<0.001ï¼ï¼ä¸¦å°ä½¿ç¨èäºåæéæ¸å°äº 37.8%ï¼åªæ¼å©è¦æç¤ºæ¨¡åãå®å¨èçä¸è¦ååè¤éç©ä»¶æè½å±ç¾åºç©©å¥çæè½ï¼é²ä¸æ­¥è­æäºå¶å¨è¨åºç°å¢ä¸­çæ½åï¼æå©æ¼ä»¥æå°çéæ°è¨ç·´é²è¡æ´èªåååå¯é çé«å­¸å½±ååæã

##### **Submodular Maximization Approaches for Equitable Client Selection in Federated Learning**
2408.13683v2 by AndrÃ©s Catalino Castillo JimÃ©nez, Ege C. Kaya, Lintao Ye, Abolfazl Hashemi

In a conventional Federated Learning framework, client selection for training
typically involves the random sampling of a subset of clients in each
iteration. However, this random selection often leads to disparate performance
among clients, raising concerns regarding fairness, particularly in
applications where equitable outcomes are crucial, such as in medical or
financial machine learning tasks. This disparity typically becomes more
pronounced with the advent of performance-centric client sampling techniques.
This paper introduces two novel methods, namely SUBTRUNC and UNIONFL, designed
to address the limitations of random client selection. Both approaches utilize
submodular function maximization to achieve more balanced models. By modifying
the facility location problem, they aim to mitigate the fairness concerns
associated with random selection. SUBTRUNC leverages client loss information to
diversify solutions, while UNIONFL relies on historical client selection data
to ensure a more equitable performance of the final model. Moreover, these
algorithms are accompanied by robust theoretical guarantees regarding
convergence under reasonable assumptions. The efficacy of these methods is
demonstrated through extensive evaluations across heterogeneous scenarios,
revealing significant improvements in fairness as measured by a client
dissimilarity metric.

æè¦ï¼å¨å³çµ±çè¯é¦å­¸ç¿æ¡æ¶ä¸­ï¼è¨ç·´çç¨æ¶ç«¯é¸æéå¸¸æ¶åå¨æ¯æ¬¡è¿­ä»£ä¸­é¨æ©æ½åç¨æ¶ç«¯å­éãç¶èï¼éç¨®é¨æ©é¸æéå¸¸æå°è´ç¨æ¶ç«¯ä¹éçè¡¨ç¾å·®ç°ï¼å¼ç¼äºå¬å¹³æ§çææï¼ç¹å¥æ¯å¨å¬å¹³çµæè³ééè¦çæç¨ä¸­ï¼ä¾å¦é«çæéèæ©å¨å­¸ç¿ä»»åãéç¨®å·®ç°éå¸¸æé¨èä»¥æè½çºä¸­å¿çç¨æ¶ç«¯æ½æ¨£æè¡çåºç¾èè®å¾æ´å æé¡¯ãæ¬æä»ç´¹äºå©ç¨®æ°æ¹æ³ï¼å³ SUBTRUNC å UNIONFLï¼æ¨å¨è§£æ±ºé¨æ©ç¨æ¶ç«¯é¸æçéå¶ãéå©ç¨®æ¹æ³é½å©ç¨æ¬¡æ¨¡å½æ¸æå¤§åä¾å¯¦ç¾æ´å¹³è¡¡çæ¨¡åãééä¿®æ¹è¨­æ½ä½ç½®åé¡ï¼å®åæ¨å¨ç·©è§£èé¨æ©é¸æç¸éçå¬å¹³æ§åé¡ãSUBTRUNC å©ç¨ç¨æ¶ç«¯æå¤±è³è¨ä¾åæ£è§£æ±ºæ¹æ¡ï¼è UNIONFL ä¾è³´æ¼æ­·å²ç¨æ¶ç«¯é¸æè³æï¼ä»¥ç¢ºä¿æçµæ¨¡åçæè½æ´å¬å¹³ãæ­¤å¤ï¼éäºæ¼ç®æ³éå¸¶äºéæ¼åçåè¨­ä¸æ¶ææ§çå¼·å¤§çè«ä¿è­ãéäºæ¹æ³çæåééåç¨®ç°è³ªå ´æ¯çå»£æ³è©ä¼°å¾å°è­æï¼é¡¯ç¤ºåºå¬å¹³æ§æé¡¯èæ¹åï¼éæ¯ééç¨æ¶ç«¯å·®ç°åº¦éææ¨æ¸¬éçã

##### **Towards Case-based Interpretability for Medical Federated Learning**
2408.13626v1 by Laura Latorre, Liliana Petrychenko, Regina Beets-Tan, Taisiya Kopytova, Wilson Silva

We explore deep generative models to generate case-based explanations in a
medical federated learning setting. Explaining AI model decisions through
case-based interpretability is paramount to increasing trust and allowing
widespread adoption of AI in clinical practice. However, medical AI training
paradigms are shifting towards federated learning settings in order to comply
with data protection regulations. In a federated scenario, past data is
inaccessible to the current user. Thus, we use a deep generative model to
generate synthetic examples that protect privacy and explain decisions. Our
proof-of-concept focuses on pleural effusion diagnosis and uses publicly
available Chest X-ray data.

æè¦ï¼æåæ¢ç´¢æ·±åº¦çææ¨¡åï¼å¨é«çè¯é¦å­¸ç¿è¨­ç½®ä¸­çæåºæ¼æ¡ä¾çèªªæãééåºæ¼æ¡ä¾çå¯è§£éæ§ä¾è§£é AI æ¨¡åæ±ºç­ï¼å°æ¼å¢å ä¿¡ä»»ä¸¦åè¨± AI å¨è¨åºå¯¦åä¸­å»£æ³æ¡ç¨è³ééè¦ãç¶èï¼é«ç AI è¨ç·´ç¯ä¾æ­£è½åè¯é¦å­¸ç¿è¨­ç½®ï¼ä»¥ç¬¦åè³æä¿è­·æ³è¦ãå¨è¯é¦æå¢ä¸­ï¼éå»çè³æå°ç®åçä½¿ç¨èèè¨æ¯ç¡æ³åå¾çãå æ­¤ï¼æåä½¿ç¨æ·±åº¦çææ¨¡åä¾ç¢çä¿è­·é±ç§åè§£éæ±ºç­çåæç¯ä¾ãæåçæ¦å¿µé©è­èéæ¼è¸èç©æ¶²è¨ºæ·ï¼ä¸¦ä½¿ç¨å¬éå¯åå¾çè¸é¨ X åè³æã


### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-12**|**AnySkin: Plug-and-play Skin Sensing for Robotic Touch**|Raunaq Bhirangi et.al.|[2409.08276v1](http://arxiv.org/abs/2409.08276v1)|null|
|**2024-09-12**|**Hand-Object Interaction Pretraining from Videos**|Himanshu Gaurav Singh et.al.|[2409.08273v1](http://arxiv.org/abs/2409.08273v1)|null|
|**2024-09-12**|**Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale**|Rogerio Bonatti et.al.|[2409.08264v1](http://arxiv.org/abs/2409.08264v1)|[link](https://github.com/microsoft/windowsagentarena)|
|**2024-09-12**|**LoRID: Low-Rank Iterative Diffusion for Adversarial Purification**|Geigh Zollicoffer et.al.|[2409.08255v1](http://arxiv.org/abs/2409.08255v1)|null|
|**2024-09-12**|**The Design of Informative Take-Over Requests for Semi-Autonomous Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a Drone-Controller Setting**|Ashwini Gundappa et.al.|[2409.08253v2](http://arxiv.org/abs/2409.08253v2)|null|
|**2024-09-12**|**OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering**|Jiahao Nick Li et.al.|[2409.08250v1](http://arxiv.org/abs/2409.08250v1)|null|
|**2024-09-12**|**IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation**|Yinwei Wu et.al.|[2409.08240v1](http://arxiv.org/abs/2409.08240v1)|null|
|**2024-09-12**|**Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources**|Alisia Lupidi et.al.|[2409.08239v1](http://arxiv.org/abs/2409.08239v1)|null|
|**2024-09-12**|**LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems**|Hakan T. Otal et.al.|[2409.08234v1](http://arxiv.org/abs/2409.08234v1)|[link](https://github.com/ai-in-complex-systems-lab/llm-honeypot)|
|**2024-09-12**|**CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs**|Davide Buffelli et.al.|[2409.08217v1](http://arxiv.org/abs/2409.08217v1)|null|
|**2024-09-12**|**LT3SD: Latent Trees for 3D Scene Diffusion**|Quan Meng et.al.|[2409.08215v1](http://arxiv.org/abs/2409.08215v1)|null|
|**2024-09-12**|**What Makes a Maze Look Like a Maze?**|Joy Hsu et.al.|[2409.08202v1](http://arxiv.org/abs/2409.08202v1)|null|
|**2024-09-12**|**AudioBERT: Audio Knowledge Augmented Language Model**|Hyunjong Ok et.al.|[2409.08199v1](http://arxiv.org/abs/2409.08199v1)|[link](https://github.com/hj-ok/audiobert)|
|**2024-09-12**|**Fine-tuning Large Language Models for Entity Matching**|Aaron Steiner et.al.|[2409.08185v1](http://arxiv.org/abs/2409.08185v1)|[link](https://github.com/wbsg-uni-mannheim/tailormatch)|
|**2024-09-12**|**On the Role of Context in Reading Time Prediction**|Andreas Opedal et.al.|[2409.08160v1](http://arxiv.org/abs/2409.08160v1)|[link](https://github.com/rycolab/context-reading-time)|
|**2024-09-12**|**LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models**|Zhengliang Liu et.al.|[2409.08147v1](http://arxiv.org/abs/2409.08147v1)|null|
|**2024-09-12**|**Towards a graph-based foundation model for network traffic analysis**|Louis Van Langendonck et.al.|[2409.08111v1](http://arxiv.org/abs/2409.08111v1)|null|
|**2024-09-12**|**WhisperNER: Unified Open Named Entity and Speech Recognition**|Gil Ayache et.al.|[2409.08107v1](http://arxiv.org/abs/2409.08107v1)|null|
|**2024-09-12**|**The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language**|Michael Ong et.al.|[2409.08103v1](http://arxiv.org/abs/2409.08103v1)|null|
|**2024-09-12**|**The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal**|Huiyuan Xie et.al.|[2409.08098v1](http://arxiv.org/abs/2409.08098v1)|null|
|**2024-09-12**|**TravelAgent: An AI Assistant for Personalized Travel Planning**|Aili Chen et.al.|[2409.08069v1](http://arxiv.org/abs/2409.08069v1)|null|
|**2024-09-12**|**AI-accelerated discovery of high critical temperature superconductors**|Xiao-Qi Han et.al.|[2409.08065v1](http://arxiv.org/abs/2409.08065v1)|null|
|**2024-09-12**|**Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking**|Stav Cohen et.al.|[2409.08045v1](http://arxiv.org/abs/2409.08045v1)|null|
|**2024-09-12**|**Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms**|Fatemeh Askari et.al.|[2409.07989v1](http://arxiv.org/abs/2409.07989v1)|[link](https://github.com/FatemehAskari/MSENet)|
|**2024-09-12**|**Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols**|Charlie Griffin et.al.|[2409.07985v1](http://arxiv.org/abs/2409.07985v1)|[link](https://github.com/cj-griffin/gamesforaicontrol)|
|**2024-09-12**|**ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE**|Sichun Wu et.al.|[2409.07966v1](http://arxiv.org/abs/2409.07966v1)|[link](https://github.com/uuembodiedsocialai/probtalk3d)|
|**2024-09-12**|**WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks**|Jingwen Tong et.al.|[2409.07964v1](http://arxiv.org/abs/2409.07964v1)|[link](https://github.com/weiiguo/wireless-agent)|
|**2024-09-12**|**Enhanced Online Grooming Detection Employing Context Determination and Message-Level Analysis**|Jake Street et.al.|[2409.07958v1](http://arxiv.org/abs/2409.07958v1)|null|
|**2024-09-12**|**Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies**|Alexei Pisacane et.al.|[2409.07932v1](http://arxiv.org/abs/2409.07932v1)|null|
|**2024-09-12**|**A convolutional neural network approach to deblending seismic data**|Jing Sun et.al.|[2409.07930v1](http://arxiv.org/abs/2409.07930v1)|null|
|**2024-09-12**|**A framework for measuring the training efficiency of a neural architecture**|Eduardo Cueto-Mendoza et.al.|[2409.07925v1](http://arxiv.org/abs/2409.07925v1)|null|
|**2024-09-12**|**Tidal MerzA: Combining affective modelling and autonomous code generation through Reinforcement Learning**|Elizabeth Wilson et.al.|[2409.07918v1](http://arxiv.org/abs/2409.07918v1)|null|
|**2024-09-12**|**UGAD: Universal Generative AI Detector utilizing Frequency Fingerprints**|Inzamamul Alam et.al.|[2409.07913v1](http://arxiv.org/abs/2409.07913v1)|null|
|**2024-09-12**|**A corpus-based investigation of pitch contours of monosyllabic words in conversational Taiwan Mandarin**|Xiaoyun Jin et.al.|[2409.07891v1](http://arxiv.org/abs/2409.07891v1)|null|
|**2024-09-12**|**Learning Rules from KGs Guided by Language Models**|Zihang Peng et.al.|[2409.07869v1](http://arxiv.org/abs/2409.07869v1)|[link](https://github.com/pzh97/learning-rules-from-kgs-guided-by-language-models)|
|**2024-09-12**|**Enhancing Cross-Market Recommendation System with Graph Isomorphism Networks: A Novel Approach to Personalized User Experience**|SÃ¼meyye ÃztÃ¼rk et.al.|[2409.07850v1](http://arxiv.org/abs/2409.07850v1)|null|
|**2024-09-12**|**FPMT: Enhanced Semi-Supervised Model for Traffic Incident Detection**|Xinying Lu et.al.|[2409.07839v1](http://arxiv.org/abs/2409.07839v1)|null|
|**2024-09-12**|**A Comprehensive Survey on Deep Multimodal Learning with Missing Modality**|Renjie Wu et.al.|[2409.07825v2](http://arxiv.org/abs/2409.07825v2)|null|
|**2024-09-12**|**Online vs Offline: A Comparative Study of First-Party and Third-Party Evaluations of Social Chatbots**|Ekaterina Svikhnushina et.al.|[2409.07823v1](http://arxiv.org/abs/2409.07823v1)|null|
|**2024-09-12**|**Controllable Synthetic Clinical Note Generation with Privacy Guarantees**|Tal Baumel et.al.|[2409.07809v1](http://arxiv.org/abs/2409.07809v1)|null|
|**2024-09-12**|**In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for Efficient Adaptation**|Mohammad Mehdi Rastikerdar et.al.|[2409.07796v1](http://arxiv.org/abs/2409.07796v1)|null|
|**2024-09-12**|**Full-text Error Correction for Chinese Speech Recognition with Large Language Model**|Zhiyuan Tang et.al.|[2409.07790v1](http://arxiv.org/abs/2409.07790v1)|null|
|**2024-09-12**|**Stable Language Model Pre-training by Reducing Embedding Variability**|Woojin Chung et.al.|[2409.07787v1](http://arxiv.org/abs/2409.07787v1)|null|
|**2024-09-12**|**ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**|Fuchen Zheng et.al.|[2409.07779v1](http://arxiv.org/abs/2409.07779v1)|[link](https://github.com/lzeeorno/assnet)|
|**2024-09-12**|**Training Spiking Neural Networks via Augmented Direct Feedback Alignment**|Yongbo Zhang et.al.|[2409.07776v1](http://arxiv.org/abs/2409.07776v1)|null|
|**2024-09-12**|**Universal Pooling Method of Multi-layer Features from Pretrained Models for Speaker Verification**|Jin Sob Kim et.al.|[2409.07770v1](http://arxiv.org/abs/2409.07770v1)|[link](https://github.com/sadpororo/unipool-sv)|
|**2024-09-12**|**Reimagining Linear Probing: Kolmogorov-Arnold Networks in Transfer Learning**|Sheng Shen et.al.|[2409.07763v1](http://arxiv.org/abs/2409.07763v1)|null|
|**2024-09-12**|**Top-down Activity Representation Learning for Video Question Answering**|Yanan Wang et.al.|[2409.07748v1](http://arxiv.org/abs/2409.07748v1)|null|
|**2024-09-12**|**Multi-object event graph representation learning for Video Question Answering**|Yanan Wang et.al.|[2409.07747v1](http://arxiv.org/abs/2409.07747v1)|null|
|**2024-09-12**|**Ruri: Japanese General Text Embeddings**|Hayato Tsukagoshi et.al.|[2409.07737v1](http://arxiv.org/abs/2409.07737v1)|null|
|**2024-09-12**|**Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities**|Aaryan Panda et.al.|[2409.07736v1](http://arxiv.org/abs/2409.07736v1)|null|
|**2024-09-12**|**GRE^2-MDCL: Graph Representation Embedding Enhanced via Multidimensional Contrastive Learning**|Kaizhe Fan et.al.|[2409.07725v1](http://arxiv.org/abs/2409.07725v1)|null|
|**2024-09-12**|**Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy**|Bojian Li et.al.|[2409.07723v1](http://arxiv.org/abs/2409.07723v1)|null|
|**2024-09-12**|**FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments**|Devansh Dhrafani et.al.|[2409.07715v1](http://arxiv.org/abs/2409.07715v1)|null|
|**2024-09-12**|**Experimenting with Legal AI Solutions: The Case of Question-Answering for Access to Justice**|Jonathan Li et.al.|[2409.07713v1](http://arxiv.org/abs/2409.07713v1)|null|
|**2024-09-12**|**Attack End-to-End Autonomous Driving through Module-Wise Noise**|Lu Wang et.al.|[2409.07706v1](http://arxiv.org/abs/2409.07706v1)|null|
|**2024-09-12**|**DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?**|Liqiang Jing et.al.|[2409.07703v1](http://arxiv.org/abs/2409.07703v1)|[link](https://github.com/liqiangjing/dsbench)|
|**2024-09-12**|**Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG**|Gabriel de Souza P. Moreira et.al.|[2409.07691v1](http://arxiv.org/abs/2409.07691v1)|null|
|**2024-09-12**|**Modeling Information Narrative Detection and Evolution on Telegram during the Russia-Ukraine War**|Patrick Gerard et.al.|[2409.07684v1](http://arxiv.org/abs/2409.07684v1)|null|
|**2024-09-12**|**Open-Vocabulary Remote Sensing Image Semantic Segmentation**|Qinglong Cao et.al.|[2409.07683v1](http://arxiv.org/abs/2409.07683v1)|null|
|**2024-09-12**|**An Unsupervised Dialogue Topic Segmentation Model Based on Utterance Rewriting**|Xia Hou et.al.|[2409.07672v1](http://arxiv.org/abs/2409.07672v1)|null|
|**2024-09-11**|**Passed the Turing Test: Living in Turing Futures**|Bernardo GonÃ§alves et.al.|[2409.07656v1](http://arxiv.org/abs/2409.07656v1)|null|
|**2024-09-11**|**Feature Importance in Pedestrian Intention Prediction: A Context-Aware Review**|Mohsen Azarmi et.al.|[2409.07645v1](http://arxiv.org/abs/2409.07645v1)|null|
|**2024-09-11**|**SimulBench: Evaluating Language Models with Creative Simulation Tasks**|Qi Jia et.al.|[2409.07641v1](http://arxiv.org/abs/2409.07641v1)|null|
|**2024-09-11**|**Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities**|Thomas Ball et.al.|[2409.07638v1](http://arxiv.org/abs/2409.07638v1)|null|
|**2024-09-11**|**Weather-Informed Probabilistic Forecasting and Scenario Generation in Power Systems**|Hanyu Zhang et.al.|[2409.07637v1](http://arxiv.org/abs/2409.07637v1)|null|
|**2024-09-11**|**Dividable Configuration Performance Learning**|Jingzhi Gong et.al.|[2409.07629v1](http://arxiv.org/abs/2409.07629v1)|null|
|**2024-09-11**|**Leveraging User-Generated Reviews for Recommender Systems with Dynamic Headers**|Shanu Vashishtha et.al.|[2409.07627v1](http://arxiv.org/abs/2409.07627v1)|null|
|**2024-09-11**|**Ensemble Methods for Sequence Classification with Hidden Markov Models**|Maxime Kawawa-Beaudan et.al.|[2409.07619v1](http://arxiv.org/abs/2409.07619v1)|null|
|**2024-09-11**|**Understanding Foundation Models: Are We Back in 1924?**|Alan F. Smeaton et.al.|[2409.07618v1](http://arxiv.org/abs/2409.07618v1)|null|
|**2024-09-11**|**Zero-Shot Machine-Generated Text Detection Using Mixture of Large Language Models**|Matthieu Dubois et.al.|[2409.07615v1](http://arxiv.org/abs/2409.07615v1)|null|
|**2024-09-11**|**Efficient Localized Adaptation of Neural Weather Forecasting: A Case Study in the MENA Region**|Muhammad Akhtar Munir et.al.|[2409.07585v1](http://arxiv.org/abs/2409.07585v1)|[link](https://github.com/akhtarvision/weather-regional)|
|**2024-09-11**|**DS-ViT: Dual-Stream Vision Transformer for Cross-Task Distillation in Alzheimer's Early Diagnosis**|Ke Chen et.al.|[2409.07584v1](http://arxiv.org/abs/2409.07584v1)|null|
|**2024-09-11**|**A Novel Mathematical Framework for Objective Evaluation of Ideas using a Conversational AI (CAI) System**|B. Sankar et.al.|[2409.07578v1](http://arxiv.org/abs/2409.07578v1)|null|
|**2024-09-11**|**Machine Learning and Constraint Programming for Efficient Healthcare Scheduling**|Aymen Ben Said et.al.|[2409.07547v1](http://arxiv.org/abs/2409.07547v1)|null|
|**2024-09-11**|**"My Grade is Wrong!": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays**|Shengxin Hong et.al.|[2409.07453v1](http://arxiv.org/abs/2409.07453v1)|null|
|**2024-09-11**|**Still More Shades of Null: A Benchmark for Responsible Missing Value Imputation**|Falaah Arif Khan et.al.|[2409.07510v1](http://arxiv.org/abs/2409.07510v1)|[link](https://github.com/falaaharifkhan/data-cleaning-stability)|
|**2024-09-11**|**SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories**|Ben Bogin et.al.|[2409.07440v1](http://arxiv.org/abs/2409.07440v1)|[link](https://github.com/allenai/super-benchmark)|
|**2024-09-11**|**A Suite for Acoustic Language Model Evaluation**|Gallil Maimon et.al.|[2409.07437v1](http://arxiv.org/abs/2409.07437v1)|[link](https://github.com/slp-rl/salmon)|
|**2024-09-11**|**Synthetic continued pretraining**|Zitong Yang et.al.|[2409.07431v1](http://arxiv.org/abs/2409.07431v1)|[link](https://github.com/zitongyang/synthetic_continued_pretraining)|
|**2024-09-11**|**Agent Workflow Memory**|Zora Zhiruo Wang et.al.|[2409.07429v1](http://arxiv.org/abs/2409.07429v1)|[link](https://github.com/zorazrw/agent-workflow-memory)|
|**2024-09-11**|**Towards Fairer Health Recommendations: finding informative unbiased samples via Word Sense Disambiguation**|Gavin Butts et.al.|[2409.07424v1](http://arxiv.org/abs/2409.07424v1)|null|
|**2024-09-11**|**Enhancing adversarial robustness in Natural Language Inference using explanations**|Alexandros Koulakos et.al.|[2409.07423v1](http://arxiv.org/abs/2409.07423v1)|null|
|**2024-09-11**|**Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise Recommendation**|Luo Ji et.al.|[2409.07416v1](http://arxiv.org/abs/2409.07416v1)|null|
|**2024-09-11**|**SoK: Security and Privacy Risks of Medical AI**|Yuanhaur Chang et.al.|[2409.07415v1](http://arxiv.org/abs/2409.07415v1)|null|
|**2024-09-11**|**CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification**|Zeqing Qin et.al.|[2409.07407v1](http://arxiv.org/abs/2409.07407v1)|null|
|**2024-09-11**|**AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge**|Han Wang et.al.|[2409.07394v1](http://arxiv.org/abs/2409.07394v1)|[link](https://github.com/hannight/adacad)|
|**2024-09-11**|**Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring System via Language Model Coordination**|Daniel Zhang-Li et.al.|[2409.07372v1](http://arxiv.org/abs/2409.07372v1)|null|
|**2024-09-11**|**Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**|Khiem Ton et.al.|[2409.07368v1](http://arxiv.org/abs/2409.07368v1)|null|
|**2024-09-11**|**Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation**|SeongYeub Chu et.al.|[2409.07355v1](http://arxiv.org/abs/2409.07355v1)|[link](https://github.com/BBeeChu/InteractEval)|
|**2024-09-11**|**Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks**|Md Zarif Hossain et.al.|[2409.07353v1](http://arxiv.org/abs/2409.07353v1)|[link](https://github.com/speedlab-git/robust-encoder-against-jailbreak-attack)|
|**2024-09-11**|**Federated Impression for Learning with Distributed Heterogeneous Data**|Sana Ayromlou et.al.|[2409.07351v1](http://arxiv.org/abs/2409.07351v1)|null|
|**2024-09-11**|**Online Decision MetaMorphFormer: A Casual Transformer-Based Reinforcement Learning Framework of Universal Embodied Intelligence**|Luo Ji et.al.|[2409.07341v1](http://arxiv.org/abs/2409.07341v1)|null|
|**2024-09-11**|**Explanation, Debate, Align: A Weak-to-Strong Framework for Language Model Generalization**|Mehrdad Zakershahrak et.al.|[2409.07335v1](http://arxiv.org/abs/2409.07335v1)|null|
|**2024-09-11**|**Module-wise Adaptive Adversarial Training for End-to-end Autonomous Driving**|Tianyuan Zhang et.al.|[2409.07321v1](http://arxiv.org/abs/2409.07321v1)|null|
|**2024-09-11**|**MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**|Praveen K Kanithi et.al.|[2409.07314v1](http://arxiv.org/abs/2409.07314v1)|null|
|**2024-09-11**|**Exploring User-level Gradient Inversion with a Diffusion Prior**|Zhuohang Li et.al.|[2409.07291v1](http://arxiv.org/abs/2409.07291v1)|null|
|**2024-09-11**|**Using Generative Agents to Create Tip Sheets for Investigative Data Reporting**|Joris Veerbeek et.al.|[2409.07286v1](http://arxiv.org/abs/2409.07286v1)|null|
|**2024-09-11**|**Cross-Dialect Text-To-Speech in Pitch-Accent Language Incorporating Multi-Dialect Phoneme-Level BERT**|Kazuki Yamauchi et.al.|[2409.07265v1](http://arxiv.org/abs/2409.07265v1)|null|
|**2024-09-11**|**Propaganda to Hate: A Multimodal Analysis of Arabic Memes with Multi-Agent LLMs**|Firoj Alam et.al.|[2409.07246v1](http://arxiv.org/abs/2409.07246v1)|null|

#### Abstracts
##### **AnySkin: Plug-and-play Skin Sensing for Robotic Touch**
2409.08276v1 by Raunaq Bhirangi, Venkatesh Pattabiraman, Enes Erciyes, Yifeng Cao, Tess Hellebrekers, Lerrel Pinto

While tactile sensing is widely accepted as an important and useful sensing
modality, its use pales in comparison to other sensory modalities like vision
and proprioception. AnySkin addresses the critical challenges that impede the
use of tactile sensing -- versatility, replaceability, and data reusability.
Building on the simplistic design of ReSkin, and decoupling the sensing
electronics from the sensing interface, AnySkin simplifies integration making
it as straightforward as putting on a phone case and connecting a charger.
Furthermore, AnySkin is the first uncalibrated tactile-sensor with
cross-instance generalizability of learned manipulation policies. To summarize,
this work makes three key contributions: first, we introduce a streamlined
fabrication process and a design tool for creating an adhesive-free, durable
and easily replaceable magnetic tactile sensor; second, we characterize slip
detection and policy learning with the AnySkin sensor; and third, we
demonstrate zero-shot generalization of models trained on one instance of
AnySkin to new instances, and compare it with popular existing tactile
solutions like DIGIT and ReSkin.https://any-skin.github.io/

æè¦ï¼åç®¡è§¸è¦ºææ¸¬è¢«å»£æ³æ¥åçºä¸ç¨®éè¦ä¸æç¨çææ¸¬æ¹å¼ï¼ä½èè¦è¦ºåæ¬é«æè¦ºç­å¶ä»æå®æ¹å¼ç¸æ¯ï¼å®çä½¿ç¨é¡¯å¾ç¸å½¢è¦çµãAnySkin è§£æ±ºäºé»ç¤è§¸è¦ºææ¸¬ä½¿ç¨çééµææ°ï¼åæ¬å¤åè½æ§ãå¯æ¿ææ§åè³æå¯éç¨æ§ãAnySkin å»ºç«å¨ ReSkin çç°¡åè¨­è¨ä¹ä¸ï¼ä¸¦å°ææ¸¬é»å­åä»¶èææ¸¬ä»é¢åéï¼ç°¡åäºæ´åï¼ä½¿å¶åæ´ä¸ææ©æ®¼ä¸¦é£æ¥åé»å¨ä¸æ¨£ç°¡å®ãæ­¤å¤ï¼AnySkin æ¯ç¬¬ä¸åæªæ ¡æºçè§¸è¦ºææ¸¬å¨ï¼å·æè·¨ä¾å­¸ç¿æä½ç­ç¥çæ¦æ¬æ§ãç¸½ä¹ï¼éé å·¥ä½ååºäºä¸é ééµè²¢ç»ï¼é¦åï¼æåå¼å¥äºä¸åç°¡åçè£½é æµç¨åä¸åè¨­è¨å·¥å·ï¼ç¨æ¼å»ºç«ç¡é»èåãèç¨ä¸ææ¼æ´æçç£æ§è§¸è¦ºææ¸¬å¨ï¼å¶æ¬¡ï¼æåå©ç¨ AnySkin ææ¸¬å¨ä¾è¡¨å¾µæ»ååµæ¸¬åç­ç¥å­¸ç¿ï¼ç¬¬ä¸ï¼æåå±ç¤ºäºå¨ä¸å AnySkin å¯¦ä¾ä¸è¨ç·´çæ¨¡åå°æ°å¯¦ä¾çé¶æ¬¡å­¸ç¿æ¦æ¬æ§ï¼ä¸¦å°å¶è DIGIT å ReSkin ç­ç¾æçç±éè§¸è¦ºè§£æ±ºæ¹æ¡é²è¡äºæ¯è¼ãhttps://any-skin.github.io/

##### **Hand-Object Interaction Pretraining from Videos**
2409.08273v1 by Himanshu Gaurav Singh, Antonio Loquercio, Carmelo Sferrazza, Jane Wu, Haozhi Qi, Pieter Abbeel, Jitendra Malik

We present an approach to learn general robot manipulation priors from 3D
hand-object interaction trajectories. We build a framework to use in-the-wild
videos to generate sensorimotor robot trajectories. We do so by lifting both
the human hand and the manipulated object in a shared 3D space and retargeting
human motions to robot actions. Generative modeling on this data gives us a
task-agnostic base policy. This policy captures a general yet flexible
manipulation prior. We empirically demonstrate that finetuning this policy,
with both reinforcement learning (RL) and behavior cloning (BC), enables
sample-efficient adaptation to downstream tasks and simultaneously improves
robustness and generalizability compared to prior approaches. Qualitative
experiments are available at: \url{https://hgaurav2k.github.io/hop/}.

æè¦ï¼æåæåºä¸åæ¹æ³ï¼å¾ 3D æé¨ç©ä»¶äºåè»è·¡ä¸­å­¸ç¿ä¸è¬æ©å¨äººææ§åé©ãæåå»ºç«ä¸åæ¶æ§ï¼ä½¿ç¨éå¤å½±çä¾ç¢çææ¸¬éåæ©å¨äººè»è·¡ãæåéæ¨£åæ¯ééæåäººé¡çæåè¢«ææ§çç©ä»¶å¨ä¸åå±ç¨ç 3D ç©ºéä¸­ï¼ä¸¦å°äººé¡åä½éæ°è¨­å®çºæ©å¨äººåä½ãéåè³æä¸ççææ¨¡åçµ¦äºæåä¸åèä»»åç¡éçåºæ¬ç­ç¥ãæ­¤ç­ç¥ææå°ä¸è¬ä½éæ´»çææ§åé©ãæåç¶é©æ§å°è­æï¼å¾®èª¿æ­¤ç­ç¥ï¼åæä½¿ç¨å¼·åå­¸ç¿ (RL) åè¡çºè¤è£½ (BC)ï¼è½ææçå°èª¿æ´ä¸æ¸¸ä»»åï¼ä¸¦åææ¹åå¥å£¯æ§åæ¦æ¬æ§ï¼èååçåæ³ç¸æ¯ãå®æ§å¯¦é©å¯ä»¥å¨ä»¥ä¸ç¶²ååå¾ï¼\url{https://hgaurav2k.github.io/hop/}ã

##### **Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale**
2409.08264v1 by Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, Zack Hui

Large language models (LLMs) show remarkable potential to act as computer
agents, enhancing human productivity and software accessibility in multi-modal
tasks that require planning and reasoning. However, measuring agent performance
in realistic environments remains a challenge since: (i) most benchmarks are
limited to specific modalities or domains (e.g. text-only, web navigation, Q&A,
coding) and (ii) full benchmark evaluations are slow (on order of magnitude of
days) given the multi-step sequential nature of tasks. To address these
challenges, we introduce the Windows Agent Arena: a reproducible, general
environment focusing exclusively on the Windows operating system (OS) where
agents can operate freely within a real Windows OS and use the same wide range
of applications, tools, and web browsers available to human users when solving
tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse
Windows tasks across representative domains that require agent abilities in
planning, screen understanding, and tool usage. Our benchmark is scalable and
can be seamlessly parallelized in Azure for a full benchmark evaluation in as
little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we
also introduce a new multi-modal agent, Navi. Our agent achieves a success rate
of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted
human. Navi also demonstrates strong performance on another popular web-based
benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis
of Navi's performance, and provide insights into the opportunities for future
research in agent development and data generation using Windows Agent Arena.
  Webpage: https://microsoft.github.io/WindowsAgentArena
  Code: https://github.com/microsoft/WindowsAgentArena

æè¦ï¼<paragraph>å¤§åèªè¨æ¨¡å (LLM) å±ç¾åºä½çºé»è¦ä»£ççéå¡æ½åï¼å¨éè¦è¦ååæ¨ççå¤æ¨¡æä»»åä¸­æåäººé¡çç¢ååè»é«å¯åæ§ãç¶èï¼å¨é¼ççç°å¢ä¸­æ¸¬éä»£çæè½ä»ç¶æ¯ä¸é ææ°ï¼åå å¦ä¸ï¼(i) å¤§å¤æ¸åºæºæ¸¬è©¦åéæ¼ç¹å®æ¨¡ææé å (ä¾å¦ç´æå­ãç¶²é çè¦½ãåç­ãç·¨ç¢¼)ï¼ä»¥å (ii) ç±æ¼ä»»åçå¤æ­¥é©é åºæ§è³ªï¼å®æ´çåºæºæ¸¬è©¦è©ä¼°å¾æ¢ (ä»¥å¤©çºå®ä½)ãçºäºæå°éäºææ°ï¼æåå¼å¥äº Windows ä»£çç«¶æå ´ï¼ä¸åå¯éè¤ãéç¨çç°å¢ï¼å°æ³¨æ¼ Windows ä½æ¥­ç³»çµ± (OS)ï¼ä»£çå¯ä»¥å¨çæ­£ç Windows ä½æ¥­ç³»çµ±ä¸­èªç±éä½ï¼ä¸¦ä½¿ç¨èäººé¡ä½¿ç¨èå¨è§£æ±ºä»»åæå¯ç¨çç¸åå»£æ³çæç¨ç¨å¼ãå·¥å·åç¶²è·¯çè¦½å¨ãæåèª¿æ´ OSWorld æ¡æ¶ (Xie ç­äººï¼2024 å¹´) ä¾å»ºç« 150 å¤é è·¨ä»£è¡¨æ§é åçå¤å Windows ä»»åï¼éäºä»»åéè¦ä»£çå·åè¦åãè¢å¹çè§£åå·¥å·ä½¿ç¨æ¹é¢çè½åãæåçåºæºæ¸¬è©¦å·æå¯æ´å±æ§ï¼å¯ä»¥å¨ Azure ä¸­ç¡ç¸«ä¸¦è¡èçï¼å¨ç­ç­ 20 åéå§å®æå®æ´çåºæºæ¸¬è©¦è©ä¼°ãçºäºå±ç¤º Windows ä»£çç«¶æå ´çè½åï¼æåéå¼å¥äºä¸åæ°çå¤æ¨¡æä»£ç Naviãæåçä»£çå¨ Windows é åéå°äº 19.5% çæåçï¼èæªååå©çäººé¡æè½çº 74.5%ãNavi ä¹å¨å¦ä¸åæµè¡çç¶²è·¯åºæºæ¸¬è©¦ Mind2Web ä¸å±ç¾äºå¼·åçæè½ãæåå° Navi çæè½é²è¡äºå»£æ³çéååè³ªååæï¼ä¸¦æä¾äºå°ä½¿ç¨ Windows ä»£çç«¶æå ´é²è¡ä»£çéç¼åè³æçæçæªä¾ç ç©¶æ©æçè¦è§£ã
ç¶²ç«ï¼https://microsoft.github.io/WindowsAgentArena
ç¨å¼ç¢¼ï¼https://github.com/microsoft/WindowsAgentArena</paragraph>

##### **LoRID: Low-Rank Iterative Diffusion for Adversarial Purification**
2409.08255v1 by Geigh Zollicoffer, Minh Vu, Ben Nebgen, Juan Castorena, Boian Alexandrov, Manish Bhattarai

This work presents an information-theoretic examination of diffusion-based
purification methods, the state-of-the-art adversarial defenses that utilize
diffusion models to remove malicious perturbations in adversarial examples. By
theoretically characterizing the inherent purification errors associated with
the Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank
Iterative Diffusion purification method designed to remove adversarial
perturbation with low intrinsic purification errors. LoRID centers around a
multi-stage purification process that leverages multiple rounds of
diffusion-denoising loops at the early time-steps of the diffusion models, and
the integration of Tucker decomposition, an extension of matrix factorization,
to remove adversarial noise at high-noise regimes. Consequently, LoRID
increases the effective diffusion time-steps and overcomes strong adversarial
attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ,
and ImageNet datasets under both white-box and black-box settings.

æè¦ï¼éé å·¥ä½æåºäºåºæ¼æ´æ£çæ·¨åæ¹æ³çä¿¡æ¯çè«æª¢é©ï¼éæ¯ä¸ç¨®æåé²çå°ææ§é²ç¦¦ï¼å©ç¨æ´æ£æ¨¡åä¾ç§»é¤å°ææ§ç¯ä¾ä¸­çæ¡ææ¾åãééçè«ä¸è¡¨å¾µèåºæ¼é¦¬å¯å¤«çæ´æ£æ·¨åç¸éçåºææ·¨åèª¤å·®ï¼æåå¼å¥äº LoRIDï¼ä¸ç¨®æ°ç©çä½ç§©åè¦æ´æ£æ·¨åæ¹æ³ï¼æ¨å¨ä»¥ä½å§å¨æ·¨åèª¤å·®ç§»é¤å°ææ§æ¾åãLoRID ä»¥å¤éæ®µæ·¨åéç¨çºä¸­å¿ï¼å©ç¨æ´æ£æ¨¡åçæ©ææéæ­¥é·é²è¡å¤è¼ªæ´æ£å»åªè¿´åï¼ä¸¦æ´åäºç©é£åè§£çå»¶ä¼¸ â å¡ååè§£ï¼ä»¥å¨é«åªè²çæä¸ç§»é¤å°ææ§éè¨ãå æ­¤ï¼LoRIDå¢å äºææçæ´æ£æéæ­¥é·ï¼ä¸¦åæäºå¼·å¤§çå°ææ§æ»æï¼å¨ç½çåé»çè¨­å®ä¸ï¼å¨ CIFAR-10/100ãCelebA-HQ å ImageNet è³æéä¸éå°äºåªç°çç©©å¥æ§è¡¨ç¾ã

##### **The Design of Informative Take-Over Requests for Semi-Autonomous Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a Drone-Controller Setting**
2409.08253v2 by Ashwini Gundappa, Emilia Ellsiepen, Lukas Schmitz, Frederik Wiehr, Vera Demberg

The question of how cyber-physical systems should interact with human
partners that can take over control or exert oversight is becoming more
pressing, as these systems are deployed for an ever larger range of tasks.
Drawing on the literatures on handing over control during semi-autonomous
driving and human-robot interaction, we propose a design of a take-over request
that combines an abstract pre-alert with an informative TOR: Relevant sensor
information is highlighted on the controller's display, while a spoken message
verbalizes the reason for the TOR. We conduct our study in the context of a
semi-autonomous drone control scenario as our testbed. The goal of our online
study is to assess in more detail what form a language-based TOR should take.
Specifically, we compare a full sentence condition to shorter fragments, and
test whether the visual highlighting should be done synchronously or
asynchronously with the speech. Participants showed a higher accuracy in
choosing the correct solution with our bi-modal TOR and felt that they were
better able to recognize the critical situation. Using only fragments in the
spoken message rather than full sentences did not lead to improved accuracy or
faster reactions. Also, synchronizing the visual highlighting with the spoken
message did not result in better accuracy and response times were even
increased in this condition.

æè¦ï¼é¨èéäºç³»çµ±è¢«é¨ç½²å°è¶ä¾è¶å»£æ³çä»»åä¸­ï¼å¦ä½è®ç¶²è·¯ç©çç³»çµ±èå¯ä»¥æ¥ç®¡æ§å¶æå·è¡ç£ç£çäººé¡å¤¥ä¼´äºåçåé¡è®å¾è¶ä¾è¶ç·è¿«ã
ééå©ç¨éæ¼å¨åèªåé§é§åäººæ©äºåä¸­ç§»äº¤æ§å¶æ¬çæç»ï¼æåæåºäºä¸åæ¥ç®¡è«æ±çè¨­è¨ï¼å®çµåäºä¸åæ½è±¡çé è­¦åä¸åè³è¨æ§ç TORï¼ç¸éçææ¸¬å¨è³è¨æå¨æ§å¶å¨çé¡¯ç¤ºå¨ä¸è¢«å¸é¡¯ï¼èä¸åå£èªè¨æ¯åæè¡¨é TOR çåå ãæåå¨ä¸ååèªåç¡äººæ©æ§å¶å ´æ¯çèæ¯ä¸é²è¡æåçç ç©¶ï¼ä½çºæåçæ¸¬è©¦å¹³å°ãæåçç·ä¸ç ç©¶çç®æ¨æ¯æ´è©³ç´°å°è©ä¼°åºæ¼èªè¨ç TOR æè©²æ¡åä»éº¼å½¢å¼ã
å·é«ä¾èªªï¼æåæ¯è¼ä¸åå®æ´å¥å­çæ¢ä»¶åè¼ç­ççæ®µï¼ä¸¦æ¸¬è©¦è¦è¦ºå¸é¡¯æ¯å¦æè©²èèªé³åæ­¥æç°æ­¥é²è¡ãåèèå¨ä½¿ç¨æåçéæ¨¡å¼ TOR æå±ç¾åºæ´é«çæºç¢ºæ§ï¼ä¸¦æè¦ºä»åæ´è½å¤ è­å¥ééµææ³ãå¨å£èªè¨æ¯ä¸­åªä½¿ç¨çæ®µèä¸æ¯å®æ´å¥å­ä¸¦æªå°è´æºç¢ºæ§æé«æåææ´å¿«ãæ­¤å¤ï¼å°è¦è¦ºå¸é¡¯èå£èªè¨æ¯åæ­¥ä¸¦æªå°è´æ´å¥½çæºç¢ºæ§ï¼èä¸å¨æ­¤æ¢ä»¶ä¸ï¼åææéçè³å¢å ã

##### **OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering**
2409.08250v1 by Jiahao Nick Li, Zhuohao Jerry Zhang, Jiaju Ma

People often capture memories through photos, screenshots, and videos. While
existing AI-based tools enable querying this data using natural language, they
mostly only support retrieving individual pieces of information like certain
objects in photos and struggle with answering more complex queries that involve
interpreting interconnected memories like event sequences. We conducted a
one-month diary study to collect realistic user queries and generated a
taxonomy of necessary contextual information for integrating with captured
memories. We then introduce OmniQuery, a novel system that is able to answer
complex personal memory-related questions that require extracting and inferring
contextual information. OmniQuery augments single captured memories through
integrating scattered contextual information from multiple interconnected
memories, retrieves relevant memories, and uses a large language model (LLM) to
comprehensive answers. In human evaluations, we show the effectiveness of
OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG
system, winning or tying in 74.5% of the time.

æè¦ï¼äººåç¶å¸¸ééç§çãè¢å¹æªååå½±çä¾ææåæ¶ãç¾æçåºæ¼äººå·¥æºæ§çå·¥å·ï¼éç¶è½ä½¿ç¨èªç¶èªè¨ä¾æ¥è©¢éäºè³æï¼ä½å®åå¤§å¤åªæ¯æ´æ·ååå¥è³è¨ï¼ä¾å¦ç§çä¸­çç¹å®ç©ä»¶ï¼èä¸å¾é£åç­æ¶åè§£è®ç¸äºé£çµåæ¶ï¼ä¾å¦äºä»¶é åºï¼çæ´è¤éæ¥è©¢ãæåé²è¡äºä¸é çºæä¸åæçæ¥è¨ç ç©¶ï¼ä»¥æ¶éå¯¦éçä½¿ç¨èæ¥è©¢ï¼ä¸¦ç¢çäºä¸åå¿è¦çèçµ¡è³è¨åé¡æ³ï¼ç¨æ¼èæ·åçåæ¶æ´åãç¶å¾ï¼æåä»ç´¹ OmniQueryï¼éæ¯ä¸åæ°ç©çç³»çµ±ï¼è½å¤ åç­è¤éçåäººè¨æ¶ç¸éåé¡ï¼éè¦æ·ååæ¨æ·èçµ¡è³è¨ãOmniQuery ééæ´åä¾èªå¤åç¸äºé£çµåæ¶çé¶æ£èçµ¡è³è¨ï¼ä¾æ´åå®ä¸çæ·ååæ¶ï¼æ·åç¸éåæ¶ï¼ä¸¦ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾æä¾å¨é¢çç­æ¡ãå¨äººé¡è©éä¸­ï¼æåä»¥ 71.5% çæºç¢ºåº¦å±ç¤ºäº OmniQuery çæææ§ï¼ä¸¦ä¸å®åªæ¼å³çµ±ç RAG ç³»çµ±ï¼å¨ 74.5% çæéä¸­ç²åææå¹³ã

##### **IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation**
2409.08240v1 by Yinwei Wu, Xianpan Zhou, Bing Ma, Xuefeng Su, Kai Ma, Xinchao Wang

While Text-to-Image (T2I) diffusion models excel at generating visually
appealing images of individual instances, they struggle to accurately position
and control the features generation of multiple instances. The Layout-to-Image
(L2I) task was introduced to address the positioning challenges by
incorporating bounding boxes as spatial control signals, but it still falls
short in generating precise instance features. In response, we propose the
Instance Feature Generation (IFG) task, which aims to ensure both positional
accuracy and feature fidelity in generated instances. To address the IFG task,
we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances
feature depiction by incorporating additional appearance tokens and utilizing
an Instance Semantic Map to align instance-level features with spatial
locations. The IFAdapter guides the diffusion process as a plug-and-play
module, making it adaptable to various community models. For evaluation, we
contribute an IFG benchmark and develop a verification pipeline to objectively
compare models' abilities to generate instances with accurate positioning and
features. Experimental results demonstrate that IFAdapter outperforms other
models in both quantitative and qualitative evaluations.

æè¦ï¼ææ¬å°å¾å (T2I) æ©æ£æ¨¡åè½ç¶å¾æé¿çæè§è§ä¸å¸å¼äººçä¸ªä½å®ä¾å¾åï¼ä½å®ä»¬å¨åç¡®å®ä½åæ§å¶å¤ä¸ªå®ä¾çç¹å¾çææ¹é¢å´éå°äºå°é¾ãå¸å±å°å¾å (L2I) ä»»å¡éè¿å°è¾¹çæ¡ä½ä¸ºç©ºé´æ§å¶ä¿¡å·æ¥è§£å³å®ä½é¾é¢ï¼ä½å®å¨çæç²¾ç¡®çå®ä¾ç¹å¾æ¹é¢ä»ç¶å­å¨ä¸è¶³ãå¯¹æ­¤ï¼æä»¬æåºäºå®ä¾ç¹å¾çæ (IFG) ä»»å¡ï¼æ¨å¨ç¡®ä¿çæå®ä¾ä¸­çä½ç½®åç¡®æ§åç¹å¾ä¿çåº¦ãä¸ºäºè§£å³ IFG ä»»å¡ï¼æä»¬å¼å¥äºå®ä¾ç¹å¾ééå¨ (IFAdapter)ãIFAdapter éè¿åå¹¶é¢å¤çå¤è§æ è®°å¹¶å©ç¨å®ä¾è¯­ä¹å¾å°å®ä¾çº§ç¹å¾ä¸ç©ºé´ä½ç½®å¯¹é½æ¥å¢å¼ºç¹å¾æè¿°ãIFAdapter ä»¥å³æå³ç¨æ¨¡åçå½¢å¼æå¯¼æ©æ£è¿ç¨ï¼ä½¿å¶è½å¤éåºåç§ç¤¾åºæ¨¡åãä¸ºäºè¿è¡è¯ä¼°ï¼æä»¬è´¡ç®äºä¸ä¸ª IFG åºåå¹¶å¼åäºä¸ä¸ªéªè¯ç®¡éï¼ä»¥å®¢è§å°æ¯è¾æ¨¡åçæå·æåç¡®å®ä½åç¹å¾çå®ä¾çè½åãå®éªç»æè¡¨æï¼IFAdapter å¨å®éåå®æ§è¯ä¼°ä¸­é½ä¼äºå¶ä»æ¨¡åã

##### **Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources**
2409.08239v1 by Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli

Large Language Models still struggle in challenging scenarios that leverage
structured data, complex reasoning, or tool usage. In this paper, we propose
Source2Synth: a new method that can be used for teaching LLMs new skills
without relying on costly human annotations. Source2Synth takes as input a
custom data source and produces synthetic data points with intermediate
reasoning steps grounded in real-world sources. Source2Synth improves the
dataset quality by discarding low-quality generations based on their
answerability. We demonstrate the generality of this approach by applying it to
two challenging domains: we test reasoning abilities in multi-hop question
answering (MHQA), and tool usage in tabular question answering (TQA). Our
method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on
HotPotQA compared to the fine-tuned baselines.

æè¦ï¼å¤§åèªè¨æ¨¡åå¨å©ç¨çµæ§åè³æãè¤éæ¨çæå·¥å·ä½¿ç¨çææ³ä¸ä»é¢è¨ææ°ãå¨æ¬æä¸­ï¼æåæåº Source2Synthï¼ä¸ç¨®æ°çæ¹æ³ï¼å¯ç¨æ¼ææ LLM æ°æè½ï¼èç¡éä¾è³´æè²´çäººå·¥è¨»è§£ãSource2Synth ä»¥èªè¨è³æä¾æºä½çºè¼¸å¥ï¼ä¸¦ç¢çä»¥çå¯¦ä¸çä¾æºçºåºç¤çåæè³æé»ï¼å¶ä¸­åå«ä¸­éæ¨çæ­¥é©ãSource2Synth ééæ ¹æå¶å¯åç­æ§ä¾æ¨æ£ä½åè³ªççæï¼é²èæåè³æéåè³ªãæåééå°æ­¤æ¹æ³æç¨æ¼å©åå·æææ°æ§çé åä¾å±ç¤ºæ­¤æ¹æ³çæ®éæ§ï¼æåå¨å¤è·³åé¡åç­ (MHQA) ä¸­æ¸¬è©¦æ¨çè½åï¼ä»¥åå¨è¡¨æ ¼åé¡åç­ (TQA) ä¸­æ¸¬è©¦å·¥å·ä½¿ç¨ãèå¾®èª¿åºç·ç¸æ¯ï¼æåçæ¨¡åå¨ WikiSQL ä¸ç TQA ä»»åä¸­å°æè½æåäº 25.51%ï¼å¨ HotPotQA ä¸ç MHQA ä»»åä¸­æåäº 22.57%ã

##### **LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems**
2409.08234v1 by Hakan T. Otal, M. Abdullah Canbaz

The rapid evolution of cyber threats necessitates innovative solutions for
detecting and analyzing malicious activity. Honeypots, which are decoy systems
designed to lure and interact with attackers, have emerged as a critical
component in cybersecurity. In this paper, we present a novel approach to
creating realistic and interactive honeypot systems using Large Language Models
(LLMs). By fine-tuning a pre-trained open-source language model on a diverse
dataset of attacker-generated commands and responses, we developed a honeypot
capable of sophisticated engagement with attackers. Our methodology involved
several key steps: data collection and processing, prompt engineering, model
selection, and supervised fine-tuning to optimize the model's performance.
Evaluation through similarity metrics and live deployment demonstrated that our
approach effectively generates accurate and informative responses. The results
highlight the potential of LLMs to revolutionize honeypot technology, providing
cybersecurity professionals with a powerful tool to detect and analyze
malicious activity, thereby enhancing overall security infrastructure.

æè¦ï¼ç¶²è·¯å¨èçå¿«éæ¼è®ï¼éè¦åµæ°çè§£æ±ºæ¹æ¡ä¾åµæ¸¬ååææ¡ææ´»åãHoneypot æ¯ç¨ä¾å¼èªåèæ»æèäºåçèªé¤ç³»çµ±ï¼å·²æçºç¶²è·¯å®å¨ä¸­è³ééè¦ççµæé¨åãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾å»ºç«é¼çä¸äºåå¼ honeypot ç³»çµ±çæ°æ¹æ³ãééå¾®èª¿é åè¨ç·´çéæºèªè¨æ¨¡åï¼ä½¿ç¨æ»æèç¢ççæä»¤ååæçå¤åè³æéï¼æåéç¼äºä¸å honeypotï¼è½å¤ èæ»æèé²è¡è¤éçäºåãæåçåæ³åå«å¹¾åééµæ­¥é©ï¼è³ææ¶éåèçãæç¤ºå·¥ç¨ãæ¨¡åé¸æï¼ä»¥åç£ç£å¾®èª¿ä»¥æä½³åæ¨¡åçæè½ãééç¸ä¼¼æ§ææ¨åå¯¦éé¨ç½²çè©ä¼°ï¼è­æäºæåçåæ³ææå°ç¢çäºæºç¢ºä¸ææç¾©çåæãçµæå¸é¡¯äº LLM å¾¹åºæ¹è® honeypot æè¡çæ½åï¼çºç¶²è·¯å®å¨å°æ¥­äººå¡æä¾äºä¸åå¼·å¤§çå·¥å·ä¾åµæ¸¬ååææ¡ææ´»åï¼é²èå¢å¼·æ´é«çå®å¨åºç¤æ¶æ§ã

##### **CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs**
2409.08217v1 by Davide Buffelli, Farzin Soleymani, Bastian Rieck

Graph neural networks have become the default choice by practitioners for
graph learning tasks such as graph classification and node classification.
Nevertheless, popular graph neural network models still struggle to capture
higher-order information, i.e., information that goes \emph{beyond} pairwise
interactions. Recent work has shown that persistent homology, a tool from
topological data analysis, can enrich graph neural networks with topological
information that they otherwise could not capture. Calculating such features is
efficient for dimension 0 (connected components) and dimension 1 (cycles).
However, when it comes to higher-order structures, it does not scale well, with
a complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order
of the structures. In this work, we introduce a novel method that extracts
information about higher-order structures in the graph while still using the
efficient low-dimensional persistent homology algorithm. On standard benchmark
datasets, we show that our method can lead to up to $31\%$ improvements in test
accuracy.

æè¦ï¼åå½¢ç¥ç¶ç¶²è·¯å·²æçºå¾æ¥­äººå¡å¨åå½¢å­¸ç¿ä»»åï¼ä¾å¦åå½¢åé¡åç¯é»åé¡ï¼ä¸­çé è¨­é¸æã
åç®¡å¦æ­¤ï¼æµè¡çåå½¢ç¥ç¶ç¶²è·¯æ¨¡åä»é£ä»¥æ·åé«éè³è¨ï¼å³è¶è¶æå°äºåçè³è¨ãæè¿çç ç©¶é¡¯ç¤ºï¼ä½çºææ²è³æåæå·¥å·çæçºåèª¿ï¼å¯ä»¥ç¨ææ²è³è¨è±å¯åå½¢ç¥ç¶ç¶²è·¯ï¼èéäºè³è¨æ¯å®åç¡æ³æ·åçãè¨ç®æ­¤é¡ç¹å¾µå°æ¼ç¶­åº¦ 0ï¼é£éåä»¶ï¼åç¶­åº¦ 1ï¼å¾ªç°ï¼ä¾èªªå¾ææçã
ç¶èï¼ç¶æ¶åå°é«éçµæ§æï¼å®çæ´åæ§ä¸ä½³ï¼è¤éåº¦çº $O(n^d)$ï¼å¶ä¸­ $n$ æ¯ç¯é»æ¸ï¼$d$ æ¯çµæ§çéæ¸ãå¨éé å·¥ä½ä¸­ï¼æåä»ç´¹äºä¸ç¨®æ°æ¹æ³ï¼ç¨æ¼èååå½¢ä¸­é«éçµæ§çè³è¨ï¼åæä»ä½¿ç¨é«æçä½ç¶­æçºåèª¿æ¼ç®æ³ãå¨æ¨æºåºæºè³æéä¸ï¼æåé¡¯ç¤ºæåçæ¨¡åå¯ä»¥ä½¿æ¸¬è©¦æºç¢ºåº¦æåå¤é $31\%$ã

##### **LT3SD: Latent Trees for 3D Scene Diffusion**
2409.08215v1 by Quan Meng, Lei Li, Matthias NieÃner, Angela Dai

We present LT3SD, a novel latent diffusion model for large-scale 3D scene
generation. Recent advances in diffusion models have shown impressive results
in 3D object generation, but are limited in spatial extent and quality when
extended to 3D scenes. To generate complex and diverse 3D scene structures, we
introduce a latent tree representation to effectively encode both
lower-frequency geometry and higher-frequency detail in a coarse-to-fine
hierarchy. We can then learn a generative diffusion process in this latent 3D
scene space, modeling the latent components of a scene at each resolution
level. To synthesize large-scale scenes with varying sizes, we train our
diffusion model on scene patches and synthesize arbitrary-sized output 3D
scenes through shared diffusion generation across multiple scene patches.
Through extensive experiments, we demonstrate the efficacy and benefits of
LT3SD for large-scale, high-quality unconditional 3D scene generation and for
probabilistic completion for partial scene observations.

æè¦ï¼æåæåº LT3SDï¼ä¸ç¨®é©ç¨æ¼å¤§è¦æ¨¡ 3D å ´æ¯çæçæ°åæ½å¨æ´æ£æ¨¡åãæ´æ£æ¨¡åçææ°é²å±å·²å¨ 3D ç©ä»¶çæä¸­å±ç¾åºä»¤äººå°è±¡æ·±å»çææï¼ä½ç¶æ´å±å° 3D å ´æ¯æï¼å¶ç©ºéç¯åååè³ªå»åå°éå¶ãçºäºçæè¤éä¸å¤æ¨£ç 3D å ´æ¯çµæ§ï¼æåå¼å¥æ½å¨æ¨¹çè¡¨ç¤ºæ³ï¼ä»¥å¨ç±ç²è³ç´°çéå±¤çµæ§ä¸­ææç·¨ç¢¼ä½é »å¹¾ä½åé«é »ç´°ç¯ãç¶å¾ï¼æåå¯ä»¥å¨éåæ½å¨ 3D å ´æ¯ç©ºéä¸­å­¸ç¿çææ´æ£éç¨ï¼å°å ´æ¯çæ½å¨çµæé¨åé²è¡å»ºæ¨¡ï¼ä¸¦å¨æ¯åè§£æåº¦å±¤ç´ä¸­å»ºæ¨¡ãçºäºåæå·æä¸åå¤§å°çå¤§è¦æ¨¡å ´æ¯ï¼æåå¨å ´æ¯è²¼çä¸è¨ç·´æåçæ´æ£æ¨¡åï¼ä¸¦ééå¤åå ´æ¯è²¼ççå±äº«æ´æ£çæåæä»»æå¤§å°çè¼¸åº 3D å ´æ¯ãééå»£æ³çå¯¦é©ï¼æåè­æäº LT3SD å¨å¤§è¦æ¨¡ãé«åè³ªç¡æ¢ä»¶ 3D å ´æ¯çæä»¥åé¨åå ´æ¯è§å¯çæ©çæ§å®ææ¹é¢çæè½ååªé»ã

##### **What Makes a Maze Look Like a Maze?**
2409.08202v1 by Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu

A unique aspect of human visual understanding is the ability to flexibly
interpret abstract concepts: acquiring lifted rules explaining what they
symbolize, grounding them across familiar and unfamiliar contexts, and making
predictions or reasoning about them. While off-the-shelf vision-language models
excel at making literal interpretations of images (e.g., recognizing object
categories such as tree branches), they still struggle to make sense of such
visual abstractions (e.g., how an arrangement of tree branches may form the
walls of a maze). To address this challenge, we introduce Deep Schema Grounding
(DSG), a framework that leverages explicit structured representations of visual
abstractions for grounding and reasoning. At the core of DSG are
schemas--dependency graph descriptions of abstract concepts that decompose them
into more primitive-level symbols. DSG uses large language models to extract
schemas, then hierarchically grounds concrete to abstract components of the
schema onto images with vision-language models. The grounded schema is used to
augment visual abstraction understanding. We systematically evaluate DSG and
different methods in reasoning on our new Visual Abstractions Dataset, which
consists of diverse, real-world images of abstract concepts and corresponding
question-answer pairs labeled by humans. We show that DSG significantly
improves the abstract visual reasoning performance of vision-language models,
and is a step toward human-aligned understanding of visual abstractions.

æè¦ï¼äººé¡è¦è¦ºçè§£çç¨ç¹é¢åå¨æ¼éæ´»è©®éæ½è±¡æ¦å¿µçè½åï¼ç²åè§£éå¶è±¡å¾µæç¾©çæåè¦åï¼å¨çæåä¸çæçèæ¯ä¸å¥ å®å¶åºç¤ï¼ä¸¦å°å¶é²è¡é æ¸¬ææ¨çãéç¶ç¾æçè¦è¦ºèªè¨æ¨¡åæé·å°å½±åé²è¡å­é¢è©®éï¼ä¾å¦è¾¨è­æ¨¹æç­ç©é«é¡å¥ï¼ï¼ä½å®åå¨çè§£æ­¤é¡è¦è¦ºæ½è±¡æ¦å¿µæä»æå°é£ï¼ä¾å¦æ¨¹æçæåå¦ä½å½¢æè¿·å®®ççå£ï¼ãçºäºæå°æ­¤ææ°ï¼æåå¼å¥äºæ·±åº¦æ¨¡å¼åºç¤ï¼DSGï¼ï¼éæ¯ä¸åæ¡æ¶ï¼å©ç¨è¦è¦ºæ½è±¡æ¦å¿µçæç¢ºçµæ§åè¡¨ç¤ºä¾é²è¡åºç¤åæ¨çãDSG çæ ¸å¿æ¯æ¨¡å¼ââæ½è±¡æ¦å¿µçä¾è³´åæè¿°ï¼å°å¶åè§£çºæ´åå§å±¤ç´çç¬¦èãDSG ä½¿ç¨å¤§åèªè¨æ¨¡åä¾æåæ¨¡å¼ï¼ç¶å¾å°æ¨¡å¼çå·é«çµæé¨ååå±¤åºç¤å°å½±åä¸ï¼ä¸¦ä½¿ç¨è¦è¦ºèªè¨æ¨¡åãåºç¤æ¨¡å¼ç¨æ¼æ´åè¦è¦ºæ½è±¡çè§£ãæåç³»çµ±æ§å°è©ä¼°äº DSG åæåçæ°è¦è¦ºæ½è±¡è³æéä¸çä¸åæ¨çæ¹æ³ï¼è©²è³æéåå«åç¨®çå¯¦ä¸ççæ½è±¡æ¦å¿µå½±åï¼ä»¥åç±äººé¡æ¨è¨çå°æåé¡è§£ç­å°ãæåè­æ DSG å¤§å¹æåäºè¦è¦ºèªè¨æ¨¡åçæ½è±¡è¦è¦ºæ¨çæè½ï¼ä¸¦ä¸æèèäººé¡ä¸è´çè¦è¦ºæ½è±¡çè§£éé²ä¸æ­¥ã

##### **AudioBERT: Audio Knowledge Augmented Language Model**
2409.08199v1 by Hyunjong Ok, Suho Yoo, Jaeho Lee

Recent studies have identified that language models, pretrained on text-only
datasets, often lack elementary visual knowledge, \textit{e.g.,} colors of
everyday objects. Motivated by this observation, we ask whether a similar
shortcoming exists in terms of the \textit{auditory} knowledge. To answer this
question, we construct a new dataset called AuditoryBench, which consists of
two novel tasks for evaluating auditory knowledge. Based on our analysis using
the benchmark, we find that language models also suffer from a severe lack of
auditory knowledge. To address this limitation, we propose AudioBERT, a novel
method to augment the auditory knowledge of BERT through a retrieval-based
approach. First, we detect auditory knowledge spans in prompts to query our
retrieval model efficiently. Then, we inject audio knowledge into BERT and
switch on low-rank adaptation for effective adaptation when audio knowledge is
required. Our experiments demonstrate that AudioBERT is quite effective,
achieving superior performance on the AuditoryBench. The dataset and code are
available at \bulurl{https://github.com/HJ-Ok/AudioBERT}.

æè¦ï¼æè¿çç ç©¶ç¼ç¾ï¼å¨ç´æå­è³æéä¸é²è¡é è¨ç·´çèªè¨æ¨¡åï¼éå¸¸ç¼ºä¹åºæ¬çè¦è¦ºç¥è­ï¼ä¾å¦æ¥å¸¸ç©åçé¡è²ãåæ­¤è§å¯çµæçåç¼ï¼æåæ³ç¥éå¨è½è¦ºç¥è­æ¹é¢æ¯å¦å­å¨é¡ä¼¼çç¼ºé»ãçºäºåç­éååé¡ï¼æåå»ºæ§äºä¸ååçº AuditoryBench çæ°è³æéï¼å¶ä¸­åå«å©åè©ä¼°è½è¦ºç¥è­çæ°ç©ä»»åãæ ¹ææåä½¿ç¨åºæºé²è¡çåæï¼æåç¼ç¾èªè¨æ¨¡åä¹å´éç¼ºä¹è½è¦ºç¥è­ãçºäºè§£æ±ºéåéå¶ï¼æåæåºäº AudioBERTï¼éæ¯ä¸ç¨®ééæª¢ç´¢çºåºç¤çæ¹æ³ä¾æ´å BERT è½è¦ºç¥è­çæ°æ¹æ³ãé¦åï¼æåå¨æç¤ºä¸­åµæ¸¬è½è¦ºç¥è­ç¯åï¼ä»¥æææ¥è©¢æåçæª¢ç´¢æ¨¡åãç¶å¾ï¼æåå°é³è¨ç¥è­æ³¨å¥ BERTï¼ä¸¦å¨éè¦é³è¨ç¥è­æéåä½éé©æä»¥é²è¡ææçé©æãæåçå¯¦é©è­æ AudioBERT éå¸¸ææï¼å¨ AuditoryBench ä¸åå¾åªç°çæè½ãè³æéåç¨å¼ç¢¼å¯å¨ https://github.com/HJ-Ok/AudioBERT åå¾ã

##### **Fine-tuning Large Language Models for Entity Matching**
2409.08185v1 by Aaron Steiner, Ralph Peeters, Christian Bizer

Generative large language models (LLMs) are a promising alternative to
pre-trained language models for entity matching due to their high zero-shot
performance and their ability to generalize to unseen entities. Existing
research on using LLMs for entity matching has focused on prompt engineering
and in-context learning. This paper explores the potential of fine-tuning LLMs
for entity matching. We analyze fine-tuning along two dimensions: 1) The
representation of training examples, where we experiment with adding different
types of LLM-generated explanations to the training set, and 2) the selection
and generation of training examples using LLMs. In addition to the matching
performance on the source dataset, we investigate how fine-tuning affects the
model's ability to generalize to other in-domain datasets as well as across
topical domains. Our experiments show that fine-tuning significantly improves
the performance of the smaller models while the results for the larger models
are mixed. Fine-tuning also improves the generalization to in-domain datasets
while hurting cross-domain transfer. We show that adding structured
explanations to the training set has a positive impact on the performance of
three out of four LLMs, while the proposed example selection and generation
methods only improve the performance of Llama 3.1 8B while decreasing the
performance of GPT-4o Mini.

æè¦ï¼çæå¼å¤§åèªè¨æ¨¡å (LLM) ç±æ¼å¶é«é¶æ¬¡å­¸ç¿è¡¨ç¾ä»¥åå°æªè¦å¯¦é«é²è¡æ³åçè½åï¼æ¯å¯¦é«éå°ä¸­é åè¨ç·´èªè¨æ¨¡åçæå¸æçæ¿ä»£æ¹æ¡ãç¾æéæ¼ä½¿ç¨ LLM é²è¡å¯¦é«éå°çç ç©¶å·²å°æ³¨æ¼æç¤ºå·¥ç¨åæå¢å­¸ç¿ãæ¬ææ¢è¨äºå¾®èª¿ LLM ä»¥é²è¡å¯¦é«éå°çæ½åãæåæ²¿èå©åé¢ååæå¾®èª¿ï¼1) è¨ç·´ç¯ä¾çè¡¨å¾µï¼æåå¨å¶ä¸­åè©¦å°ä¸åé¡åç LLM çæçèªªææ°å¢å°è¨ç·´çµï¼ä»¥å 2) ä½¿ç¨ LLM é¸æåç¢çè¨ç·´ç¯ä¾ãé¤äºå¨ä¾æºè³æéä¸çéå°æè½ä¹å¤ï¼æåæ¢è¨å¾®èª¿å¦ä½å½±é¿æ¨¡åå°å¶ä»é åå§è³æéä»¥åè·¨ä¸»é¡é åé²è¡æ³åçè½åãæåçå¯¦é©é¡¯ç¤ºï¼å¾®èª¿é¡¯èæåè¼å°åæ¨¡åçæè½ï¼èè¼å¤§åæ¨¡åççµæåå¥½å£ååãå¾®èª¿ä¹æåäºå°é åå§è³æéçæ³åï¼åææå®³äºè·¨é åè½ç§»ãæåé¡¯ç¤ºï¼å°çµæ§åèªªææ°å¢å°è¨ç·´çµå°åå LLM ä¸­çä¸åçæè½ææ­£é¢çå½±é¿ï¼èææåºçç¯ä¾é¸æåç¢çæ¹æ³åæåäº Llama 3.1 8B çæè½ï¼åæéä½äº GPT-4o Mini çæè½ã

##### **On the Role of Context in Reading Time Prediction**
2409.08160v1 by Andreas Opedal, Eleanor Chodroff, Ryan Cotterell, Ethan Gotlieb Wilcox

We present a new perspective on how readers integrate context during
real-time language comprehension. Our proposals build on surprisal theory,
which posits that the processing effort of a linguistic unit (e.g., a word) is
an affine function of its in-context information content. We first observe that
surprisal is only one out of many potential ways that a contextual predictor
can be derived from a language model. Another one is the pointwise mutual
information (PMI) between a unit and its context, which turns out to yield the
same predictive power as surprisal when controlling for unigram frequency.
Moreover, both PMI and surprisal are correlated with frequency. This means that
neither PMI nor surprisal contains information about context alone. In response
to this, we propose a technique where we project surprisal onto the orthogonal
complement of frequency, yielding a new contextual predictor that is
uncorrelated with frequency. Our experiments show that the proportion of
variance in reading times explained by context is a lot smaller when context is
represented by the orthogonalized predictor. From an interpretability
standpoint, this indicates that previous studies may have overstated the role
that context has in predicting reading times.

æè¦ï¼æåæåºäºä¸åæ°çè§é»ï¼èªªæè®èå¦ä½å¨å¯¦æèªè¨çè§£éç¨ä¸­æ´åèçµ¡ãæåçææ¡å»ºç«å¨é©å¥çè«ä¹ä¸ï¼è©²çè«åè¨­èªè¨å®åï¼ä¾å¦å®è©ï¼çèçå·¥ä½éæ¯å¶ä¸ä¸æä¿¡æ¯å§å®¹çä»¿å°å½æ¸ãæåé¦åè§å¯å°ï¼é©å¥åªæ¯å¾èªè¨æ¨¡åä¸­æ¨å°ä¸ä¸æé æ¸¬å¨çè¨±å¤æ½å¨æ¹æ³ä¹ä¸ãå¦ä¸ç¨®æ¹æ³æ¯å®ååå¶ä¸ä¸æä¹éçéé»äºä¿¡æ¯ (PMI)ï¼ç¶æ§å¶å®å­é »çæï¼å®ç¢ççé æ¸¬è½åèé©å¥ç¸åãæ­¤å¤ï¼PMI åé©å¥é½èé »çç¸éãéè¡¨ç¤º PMI åé©å¥é½ä¸åå«åéæ¼èçµ¡çä¿¡æ¯ãéå°æ­¤åé¡ï¼æåæåºäºä¸ç¨®æè¡ï¼å°é©å¥æå½±å°é »ççæ­£äº¤è£éä¸ï¼ç¢çä¸åèé »çç¡éçæ°ä¸ä¸æé æ¸¬å¨ãæåçå¯¦é©è¡¨æï¼ç¶èçµ¡ç±æ­£äº¤åé æ¸¬å¨è¡¨ç¤ºæï¼é±è®æéä¸­ç±èçµ¡è§£éçè®ç°æ¯ä¾è¦å°å¾å¤ãå¾å¯è§£éæ§çè§åº¦ä¾çï¼éè¡¨æååçç ç©¶å¯è½èªå¤§äºèçµ¡å¨é æ¸¬é±è®æéä¸­ææ®æ¼çè§è²ã

##### **LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models**
2409.08147v1 by Zhengliang Liu, Yiwei Li, Oleksandra Zolotarevych, Rongwei Yang, Tianming Liu

Large language models have demonstrated remarkable capabilities in natural
language processing, yet their application to political discourse analysis
remains underexplored. This paper introduces a novel approach to evaluating
presidential debate performances using LLMs, addressing the longstanding
challenge of objectively assessing debate outcomes. We propose a framework that
analyzes candidates' "Policies, Persona, and Perspective" (3P) and how they
resonate with the "Interests, Ideologies, and Identity" (3I) of four key
audience groups: voters, businesses, donors, and politicians. Our method
employs large language models to generate the LLM-POTUS Score, a quantitative
measure of debate performance based on the alignment between 3P and 3I. We
apply this framework to analyze transcripts from recent U.S. presidential
debates, demonstrating its ability to provide nuanced, multi-dimensional
assessments of candidate performances. Our results reveal insights into the
effectiveness of different debating strategies and their impact on various
audience segments. This study not only offers a new tool for political analysis
but also explores the potential and limitations of using LLMs as impartial
judges in complex social contexts. In addition, this framework provides
individual citizens with an independent tool to evaluate presidential debate
performances, which enhances democratic engagement and reduces reliance on
potentially biased media interpretations and institutional influence, thereby
strengthening the foundation of informed civic participation.

æè¦ï¼å¤§åèªè¨æ¨¡åå¨èªç¶èªè¨èçä¸­å±ç¾äºéå¡çè½åï¼ä½å¶æç¨æ¼æ¿æ²»è«è¿°åæä»æªå¾å°ååæ¢è¨ãæ¬æä»ç´¹äºä¸ç¨®ä½¿ç¨å¤§åèªè¨æ¨¡åè©ä¼°ç¸½çµ±è¾¯è«è¡¨ç¾çæ°ç©æ¹æ³ï¼ä»¥è§£æ±ºå®¢è§è©ä¼°è¾¯è«çµæçé·æææ°ãæåæåºäºåæåé¸äººçãæ¿ç­ãäººæ ¼åè§é»ã(3P) çæ¶æ§ï¼ä»¥åå®åå¦ä½èååä¸»è¦åç¾ç¾¤é«ï¼ãé¸æ°ãä¼æ¥­ãæå©èåæ¿æ²»å®¶ãçãå©çãæè­å½¢æåèªåã(3I) ç¢çå±é³´ãæåçæ¨¡åæ¡ç¨å¤§åèªè¨æ¨¡åä¾ç¢ç LLM-POTUS åæ¸ï¼éæ¯ä¸ååºæ¼ 3P å 3I ä¹éçä¸è´æ§çè¾¯è«è¡¨ç¾éåæ¸¬éãæåå°æ­¤æ¶æ§æç¨æ¼åææè¿ç¾åç¸½çµ±è¾¯è«çéå­ç¨¿ï¼è­æäºå®æä¾ç´°ç·»ãå¤ç¶­åº¦åé¸äººè¡¨ç¾è©ä¼°çè½åãæåççµææ­ç¤ºäºå°ä¸åè¾¯è«ç­ç¥çæææ§åå¶å°ä¸ååç¾ç¾¤é«çå½±é¿çè¦è§£ãæ¬ç ç©¶ä¸åæä¾äºæ¿æ²»åæçæ°å·¥å·ï¼éæ¢è¨äºå¨è¤éçç¤¾æç°å¢ä¸­ä½¿ç¨å¤§åèªè¨æ¨¡åä½çºå¬æ­£è©å¤çæ½ååå±éæ§ãæ­¤å¤ï¼æ­¤æ¶æ§çºåäººå¬æ°æä¾äºä¸åç¨ç«å·¥å·ä¾è©ä¼°ç¸½çµ±è¾¯è«è¡¨ç¾ï¼éå¢å¼·äºæ°ä¸»åèï¼æ¸å°äºå°æ½å¨æåè¦çåªé«è§£è®åå¶åº¦å½±é¿çä¾è³´ï¼å¾èå å¼·äºç¥æå¬æ°åèçåºç¤ã

##### **Towards a graph-based foundation model for network traffic analysis**
2409.08111v1 by Louis Van Langendonck, Ismael Castell-Uroz, Pere Barlet-Ros

Foundation models have shown great promise in various fields of study. A
potential application of such models is in computer network traffic analysis,
where these models can grasp the complexities of network traffic dynamics and
adapt to any specific task or network environment with minimal fine-tuning.
Previous approaches have used tokenized hex-level packet data and the model
architecture of large language transformer models. We propose a new, efficient
graph-based alternative at the flow-level. Our approach represents network
traffic as a dynamic spatio-temporal graph, employing a self-supervised link
prediction pretraining task to capture the spatial and temporal dynamics in
this network graph framework. To evaluate the effectiveness of our approach, we
conduct a few-shot learning experiment for three distinct downstream network
tasks: intrusion detection, traffic classification, and botnet classification.
Models finetuned from our pretrained base achieve an average performance
increase of 6.87\% over training from scratch, demonstrating their ability to
effectively learn general network traffic dynamics during pretraining. This
success suggests the potential for a large-scale version to serve as an
operational foundational model.

æè¦ï¼åºç¤æ¨¡åå·²å¨ååç ç©¶é åä¸­å±ç¾åºæ¥µå¤§çåæ¯ãæ­¤é¡æ¨¡åçæ½å¨æç¨ä¹ä¸å¨æ¼é»è¦ç¶²è·¯æµéåæï¼å¶ä¸­éäºæ¨¡åå¯ä»¥ææ¡ç¶²è·¯æµéåæçè¤éæ§ï¼ä¸¦ä»¥æå°çå¾®èª¿é©æä»»ä½ç¹å®ä»»åæç¶²è·¯ç°å¢ãååçåæ³å·²ä½¿ç¨æ¨è¨ååå­é²ä½å±¤ç´å°åè³æåå¤§åèªè¨è½æå¨æ¨¡åçæ¨¡åæ¶æ§ãæåæåºä¸åæ°çãææçæµç¨å±¤ç´åå½¢åæ¿ä»£æ¹æ¡ãæåçåæ³å°ç¶²è·¯æµéè¡¨ç¤ºçºåææç©ºåå½¢ï¼æ¡ç¨èªæç£ç£é£çµé æ¸¬é è¨ç·´ä»»åä¾æææ­¤ç¶²è·¯åå½¢æ¶æ§ä¸­çç©ºéåæéåæãçºäºè©ä¼°æååæ³çæææ§ï¼æåå°ä¸åä¸åçä¸æ¸¸ç¶²è·¯ä»»åï¼å¥ä¾µåµæ¸¬ãæµéåé¡åæ®­å±ç¶²è·¯åé¡ï¼é²è¡å°éå­¸ç¿å¯¦é©ãå¾æåçé è¨ç·´åºç¤å¾®èª¿çæ¨¡åï¼å¶å¹³åæè½æå 6.87%ï¼é«æ¼å¾é ­è¨ç·´ï¼éè­æäºå®åå¨é è¨ç·´æéææå­¸ç¿ä¸è¬ç¶²è·¯æµéåæçè½åãéé æåé¡¯ç¤ºåºå¤§è¦æ¨¡çæ¬ææ½åä½çºéä½åºç¤æ¨¡åã

##### **WhisperNER: Unified Open Named Entity and Speech Recognition**
2409.08107v1 by Gil Ayache, Menachem Pirchi, Aviv Navon, Aviv Shamsian, Gill Hetz, Joseph Keshet

Integrating named entity recognition (NER) with automatic speech recognition
(ASR) can significantly enhance transcription accuracy and informativeness. In
this paper, we introduce WhisperNER, a novel model that allows joint speech
transcription and entity recognition. WhisperNER supports open-type NER,
enabling recognition of diverse and evolving entities at inference. Building on
recent advancements in open NER research, we augment a large synthetic dataset
with synthetic speech samples. This allows us to train WhisperNER on a large
number of examples with diverse NER tags. During training, the model is
prompted with NER labels and optimized to output the transcribed utterance
along with the corresponding tagged entities. To evaluate WhisperNER, we
generate synthetic speech for commonly used NER benchmarks and annotate
existing ASR datasets with open NER tags. Our experiments demonstrate that
WhisperNER outperforms natural baselines on both out-of-domain open type NER
and supervised finetuning.

æè¦ï¼æ´åå½åå¯¦é«è¾¨è­ (NER) èèªåèªé³è¾¨è­ (ASR) è½å¤ å¤§å¹æåè½éçæºç¢ºåº¦åè³è¨éãå¨éç¯è«æä¸­ï¼æåä»ç´¹äº WhisperNERï¼ä¸ç¨®æ°ç©çæ¨¡åï¼å¯åæé²è¡èªé³è½éåå¯¦é«è¾¨è­ãWhisperNER æ¯æ´éæ¾å¼ NERï¼å¯å¨æ¨è«æè¾¨è­åºå¤æ¨£ä¸ä¸æ·æ¼é²çå¯¦é«ãå»ºæ§å¨éæ¾å¼ NER ç ç©¶çææ°é²å±ä¸ï¼æåæ´åäºä¸åå¤§ååæè³æéï¼å å¥äºåæèªé³ç¯ä¾ãéè®æåè½å¤ ä½¿ç¨å¤§éæ¨æä¸å NER æ¨ç±¤çç¯ä¾è¨ç·´ WhisperNERãå¨è¨ç·´æéï¼æ¨¡åææç¤º NER æ¨ç±¤ï¼ä¸¦æä½³åè¼¸åºè½éçèªå¥ä»¥åå°ææ¨è¨çå¯¦é«ãçºäºè©ä¼° WhisperNERï¼æåçºå¸¸ç¨ç NER åºæºç¢çåæèªé³ï¼ä¸¦ä½¿ç¨éæ¾å¼ NER æ¨ç±¤è¨»è§£ç¾æç ASR è³æéãæåçå¯¦é©è­æï¼WhisperNER å¨éæ¾å¼ NER åç£ç£å¼å¾®èª¿çéé åå¤ä»»åä¸é½åªæ¼èªç¶åºæºã

##### **The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language**
2409.08103v1 by Michael Ong, Sean Robertson, Leo Peckham, Alba Jorquera Jimenez de Aberasturi, Paula Arkhangorodsky, Robin Huo, Aman Sakhardande, Mark Hallap, Naomi Nagy, Ewan Dunbar

We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark
corpus designed to push the limits of current approaches to low-resource speech
recognition. Faetar, a Franco-Proven\c{c}al variety spoken primarily in Italy,
has no standard orthography, has virtually no existing textual or speech
resources other than what is included in the benchmark, and is quite different
from other forms of Franco-Proven\c{c}al. The corpus comes from field
recordings, most of which are noisy, for which only 5 hrs have matching
transcriptions, and for which forced alignment is of variable quality. The
corpus contains an additional 20 hrs of unlabelled speech. We report baseline
results from state-of-the-art multilingual speech foundation models with a best
phone error rate of 30.4%, using a pipeline that continues pre-training on the
foundation model using the unlabelled set.

æè¦ï¼æåä»ç´¹ Faetar èªåèªé³è¾¨è­åºæºï¼ä¸ååºæºèªæåº«æ¨å¨çªç ´ç¶åä½è³æºèªé³è¾¨è­æ¹æ³çéå¶ãFaetarï¼ä¸ç¨®ä¸»è¦å¨ç¾©å¤§å©ä½¿ç¨çæ³è­å-æ®ç¾æºæ¯èªè®é«ï¼æ²ææ¨æºæ­£å­æ³ï¼é¤äºåå«å¨åºæºä¸­çå§å®¹å¤ï¼å¹¾ä¹æ²æç¾æçæå­æèªé³è³æºï¼ä¸¦ä¸èæ³è­å-æ®ç¾æºæ¯èªçå¶ä»å½¢å¼ç¸ç¶ä¸åãèªæåº«ä¾èªç¾å ´éé³ï¼å¶ä¸­å¤§é¨åæéè¨ï¼åªæ 5 å°ææå¹éçè½éï¼èä¸å¼·å¶å°é½çåè³ªåå·®ä¸é½ãèªæåº«åå«é¡å¤ 20 å°æçæªæ¨è¨èªé³ãæåå ±åäºæåé²çå¤èªè¨èªé³åºç¤æ¨¡åçåºæºçµæï¼ä½¿ç¨å¨åºç¤æ¨¡åä¸ä½¿ç¨æªæ¨è¨éåç¹¼çºé è¨ç·´çç®¡éï¼æä½³é³ç´ é¯èª¤ççº 30.4%ã

##### **The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal**
2409.08098v1 by Huiyuan Xie, Felix Steffek, Joana Ribeiro de Faria, Christine Carter, Jonathan Rutherford

This paper explores the intersection of technological innovation and access
to justice by developing a benchmark for predicting case outcomes in the UK
Employment Tribunal (UKET). To address the challenge of extensive manual
annotation, the study employs a large language model (LLM) for automatic
annotation, resulting in the creation of the CLC-UKET dataset. The dataset
consists of approximately 19,000 UKET cases and their metadata. Comprehensive
legal annotations cover facts, claims, precedent references, statutory
references, case outcomes, reasons and jurisdiction codes. Facilitated by the
CLC-UKET data, we examine a multi-class case outcome prediction task in the
UKET. Human predictions are collected to establish a performance reference for
model comparison. Empirical results from baseline models indicate that
finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET
prediction task. The performance of zero-shot LLMs can be enhanced by
integrating task-related information into few-shot examples. We hope that the
CLC-UKET dataset, along with human annotations and empirical findings, can
serve as a valuable benchmark for employment-related dispute resolution.

æè¦ï¼æ¬ææ¢è¨äºç§æåµæ°èå¸æ³æ­£ç¾©çäº¤éï¼æ¹æ³æ¯çºè±åå°±æ¥­æ³åº­ï¼UKETï¼çæ¡ä»¶çµæå»ºç«ä¸ååºæºä¾é æ¸¬ãçºäºæå°å¤§éæåè¨»è§£çææ°ï¼æ¬ç ç©¶æ¡ç¨å¤§åèªè¨æ¨¡åï¼LLMï¼é²è¡èªåè¨»è§£ï¼é²èå»ºç« CLC-UKET è³æéãè©²è³æéåå«ç´ 19,000 ä»¶è±åå°±æ¥­æ³åº­æ¡ä»¶åå¶åè³æãå¨é¢çæ³å¾è¨»è§£æ¶µèäºå¯¦ãä¸»å¼µãå¤ä¾å¼ç¨ãæ³å®å¼ç¨ãæ¡ä»¶çµæãçç±åå¸æ³ç®¡è½ä»£ç¢¼ãå¨ CLC-UKET è³æçåå©ä¸ï¼æåæª¢è¦äºè±åå°±æ¥­æ³åº­ä¸­çå¤é¡æ¡ä»¶çµæé æ¸¬ä»»åãæ¶éäººé¡é æ¸¬ä»¥å»ºç«æ¨¡åæ¯è¼æè½çåºæºãåºæºæ¨¡åçå¯¦è­çµæé¡¯ç¤ºï¼å¾®èª¿çè½æå¨æ¨¡åå¨è±åå°±æ¥­æ³åº­é æ¸¬ä»»åä¸åªæ¼é¶æ¬¡å­¸ç¿åå°æ¬¡å­¸ç¿ç LLMãé¶æ¬¡å­¸ç¿ LLM çæè½å¯ééå°èä»»åç¸éçè³è¨æ´åå°å°æ¬¡å­¸ç¿ç¯ä¾ä¸­ä¾æåãæåå¸æ CLC-UKET è³æéï¼é£åäººé¡è¨»è§£åå¯¦è­ç¼ç¾ï¼è½ä½çºèå°±æ¥­ç¸éçç­ç«¯è§£æ±ºçå¯¶è²´åºæºã

##### **TravelAgent: An AI Assistant for Personalized Travel Planning**
2409.08069v1 by Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, Jiangjie Chen

As global tourism expands and artificial intelligence technology advances,
intelligent travel planning services have emerged as a significant research
focus. Within dynamic real-world travel scenarios with multi-dimensional
constraints, services that support users in automatically creating practical
and customized travel itineraries must address three key objectives:
Rationality, Comprehensiveness, and Personalization. However, existing systems
with rule-based combinations or LLM-based planning methods struggle to fully
satisfy these criteria. To overcome the challenges, we introduce TravelAgent, a
travel planning system powered by large language models (LLMs) designed to
provide reasonable, comprehensive, and personalized travel itineraries grounded
in dynamic scenarios. TravelAgent comprises four modules: Tool-usage,
Recommendation, Planning, and Memory Module. We evaluate TravelAgent's
performance with human and simulated users, demonstrating its overall
effectiveness in three criteria and confirming the accuracy of personalized
recommendations.

æè¦ï¼é¨èå¨çæéæ¥­çæ´å¼µåäººå·¥æºæ§æè¡çé²æ­¥ï¼
æºæ§åæéè¦åæåå·²æçºéè¦çç ç©¶éé»ãå¨å·æå¤ç¶­åº¦éå¶çåæçå¯¦ä¸çæéå ´æ¯ä¸­ï¼æ¯æ´ä½¿ç¨èèªåå»ºç«å¯¦ç¨ä¸å®¢è£½åçæéè¡ç¨çæåå¿é è§£æ±ºä¸åééµç®æ¨ï¼åçæ§ãå¨é¢æ§ãåäººåãç¶èï¼ç¾æåºæ¼è¦åçµåæ LLM çè¦åæ¹æ³çç³»çµ±é£ä»¥å®å¨æ»¿è¶³éäºæ¨æºãçºäºåæéäºææ°ï¼æåå¼å¥äº TravelAgentï¼ä¸åç±å¤§åèªè¨æ¨¡å (LLM) æ¯æ´çæéè¦åç³»çµ±ï¼æ¨å¨æä¾åçãå¨é¢ä¸åäººåçæéè¡ç¨ï¼ä¸¦ä»¥åæå ´æ¯çºåºç¤ãTravelAgent åå«ååæ¨¡çµï¼å·¥å·ä½¿ç¨ãæ¨è¦ãè¦ååè¨æ¶é«æ¨¡çµãæåééäººé¡åæ¨¡æ¬ä½¿ç¨èè©ä¼° TravelAgent çæè½ï¼è­æå¶å¨ä¸åæ¨æºä¸­çæ´é«æè½ï¼ä¸¦ç¢ºèªåäººåå»ºè­°çæºç¢ºæ§ã

##### **AI-accelerated discovery of high critical temperature superconductors**
2409.08065v1 by Xiao-Qi Han, Zhenfeng Ouyang, Peng-Jie Guo, Hao Sun, Ze-Feng Gao, Zhong-Yi Lu

The discovery of new superconducting materials, particularly those exhibiting
high critical temperature ($T_c$), has been a vibrant area of study within the
field of condensed matter physics. Conventional approaches primarily rely on
physical intuition to search for potential superconductors within the existing
databases. However, the known materials only scratch the surface of the
extensive array of possibilities within the realm of materials. Here, we
develop an AI search engine that integrates deep model pre-training and
fine-tuning techniques, diffusion models, and physics-based approaches (e.g.,
first-principles electronic structure calculation) for discovery of high-$T_c$
superconductors. Utilizing this AI search engine, we have obtained 74
dynamically stable materials with critical temperatures predicted by the AI
model to be $T_c \geq$ 15 K based on a very small set of samples. Notably,
these materials are not contained in any existing dataset. Furthermore, we
analyze trends in our dataset and individual materials including B$_4$CN$_3$
and B$_5$CN$_2$ whose $T_c$s are 24.08 K and 15.93 K, respectively. We
demonstrate that AI technique can discover a set of new high-$T_c$
superconductors, outline its potential for accelerating discovery of the
materials with targeted properties.

æè¦ï¼æ°è¶å°ææçç¼ç¾ï¼ç¹å¥æ¯é£äºè¡¨ç¾åºé«è¨çæº«åº¦ ($T_c$) çææï¼ä¸ç´æ¯åèæç©çå­¸é åä¸­ä¸ååæ»¿æ´»åçç ç©¶é åãå³çµ±æ¹æ³ä¸»è¦ä¾è³´æ¼ç©çç´è¦ºï¼å¨ç¾æè³æåº«ä¸­æå°æ½å¨çè¶å°é«ãç¶èï¼å·²ç¥çææåè§¸åææé åå§å»£æ³å¯è½æ§çè¡¨é¢ãå¨éè£¡ï¼æåéç¼äºä¸å AI æå°å¼æï¼å®æ´åäºæ·±åº¦æ¨¡åé è¨ç·´åå¾®èª¿æè¡ãæ´æ£æ¨¡åååºæ¼ç©ççæ¹æ³ï¼ä¾å¦ï¼ç¬¬ä¸åçé»å­çµæ§è¨ç®ï¼ï¼ç¨æ¼ç¼ç¾é« $T_c$ è¶å°é«ãå©ç¨éå AI æå°å¼æï¼æåå·²ç¶ç²å¾äº 74 ç¨®åæç©©å®ææï¼å¶è¨çæº«åº¦ç± AI æ¨¡åé æ¸¬çº $T_c \ge$ 15 Kï¼åºæ¼éå¸¸å°çæ¨£æ¬éãå¼å¾æ³¨æçæ¯ï¼éäºææä¸åå«å¨ä»»ä½ç¾æè³æéä¸­ãæ­¤å¤ï¼æååæäºæåè³æéååå¥ææï¼åæ¬ B$_4$CN$_3$ å B$_5$CN$_2$ï¼çè¶¨å¢ï¼å¶ $T_c$ åå¥çº 24.08 K å 15.93 Kãæåè­æäº AI æè¡å¯ä»¥ç¼ç¾ä¸çµæ°çé« $T_c$ è¶å°é«ï¼æ¦è¿°äºå¶å éç¼ç¾å·æç®æ¨ç¹æ§çææçæ½åã

##### **Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking**
2409.08045v1 by Stav Cohen, Ron Bitton, Ben Nassi

In this paper, we show that with the ability to jailbreak a GenAI model,
attackers can escalate the outcome of attacks against RAG-based GenAI-powered
applications in severity and scale. In the first part of the paper, we show
that attackers can escalate RAG membership inference attacks and RAG entity
extraction attacks to RAG documents extraction attacks, forcing a more severe
outcome compared to existing attacks. We evaluate the results obtained from
three extraction methods, the influence of the type and the size of five
embeddings algorithms employed, the size of the provided context, and the GenAI
engine. We show that attackers can extract 80%-99.8% of the data stored in the
database used by the RAG of a Q&A chatbot. In the second part of the paper, we
show that attackers can escalate the scale of RAG data poisoning attacks from
compromising a single GenAI-powered application to compromising the entire
GenAI ecosystem, forcing a greater scale of damage. This is done by crafting an
adversarial self-replicating prompt that triggers a chain reaction of a
computer worm within the ecosystem and forces each affected application to
perform a malicious activity and compromise the RAG of additional applications.
We evaluate the performance of the worm in creating a chain of confidential
data extraction about users within a GenAI ecosystem of GenAI-powered email
assistants and analyze how the performance of the worm is affected by the size
of the context, the adversarial self-replicating prompt used, the type and size
of the embeddings algorithm employed, and the number of hops in the
propagation. Finally, we review and analyze guardrails to protect RAG-based
inference and discuss the tradeoffs.

æè¦ï¼<paragraph>å¨æ¬æä¸­ï¼æåå±ç¤ºäºæ»æèå¯ä»¥å©ç¨è¶ç GenAI æ¨¡åçè½åï¼æåéå°åºæ¼ RAG ç GenAI é©åæç¨ç¨å¼çæ»æçµæçå´éæ§åè¦æ¨¡ãå¨æ¬æçç¬¬ä¸é¨åï¼æåå±ç¤ºäºæ»æèå¯ä»¥å° RAG æå¡æ¨è«æ»æå RAG å¯¦é«æ½åæ»ææåå° RAG æä»¶æ½åæ»æï¼èç¾ææ»æç¸æ¯ï¼éæé ææ´å´éçå¾æãæåè©ä¼°äºå¾ä¸ç¨®æ½åæ¹æ³ç²å¾ççµæãæä½¿ç¨çäºç¨®åµå¥æ¼ç®æ³çé¡ååå¤§å°ãæä¾çå§å®¹å¤§å°ä»¥å GenAI å¼æçå½±é¿ãæåå±ç¤ºäºæ»æèå¯ä»¥æ½å Q&A èå¤©æ©å¨äººç RAG æä½¿ç¨çè³æåº«ä¸­å²å­ç 80%-99.8% çè³æãå¨æ¬æçç¬¬äºé¨åï¼æåå±ç¤ºäºæ»æèå¯ä»¥å° RAG è³æææ¯æ»æçè¦æ¨¡å¾å±å®³å®ä¸ GenAI é©åæç¨ç¨å¼æåå°å±å®³æ´å GenAI çæç³»çµ±ï¼é ææ´å¤§è¦æ¨¡çæå®³ãéæ¯ééè£½ä½ä¸åå°ææ§çèªæè¤è£½æç¤ºä¾å®æçï¼è©²æç¤ºæå¨çæç³»çµ±ä¸­è§¸ç¼é»è¦è è²çé£éåæï¼ä¸¦è¿«ä½¿æ¯ååå½±é¿çæç¨ç¨å¼å·è¡æ¡ææ´»åä¸¦å±å®³å¶ä»æç¨ç¨å¼ç RAGãæåè©ä¼°äºè è²å¨ GenAI é©åé»å­éµä»¶å©çç GenAI çæç³»çµ±ä¸­å»ºç«æ©å¯è³ææ½åéçæè½ï¼ä¸¦åæè è²çæè½å¦ä½åå°å§å®¹å¤§å°ãæä½¿ç¨çå°ææ§èªæè¤è£½æç¤ºãæä½¿ç¨çåµå¥æ¼ç®æ³çé¡ååå¤§å°ä»¥åå³æ­ä¸­çè·³èºæ¬¡æ¸çå½±é¿ãæå¾ï¼æåæª¢è¦ä¸¦åæäºä¿è­·åºæ¼ RAG çæ¨è«çè­·æ¬ï¼ä¸¦è¨è«äºæ¬è¡¡åæ¨ã</paragraph>

##### **Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms**
2409.07989v1 by Fatemeh Askari, Amirreza Fateh, Mohammad Reza Mohammadi

In the context of few-shot classification, the goal is to train a classifier
using a limited number of samples while maintaining satisfactory performance.
However, traditional metric-based methods exhibit certain limitations in
achieving this objective. These methods typically rely on a single distance
value between the query feature and support feature, thereby overlooking the
contribution of shallow features. To overcome this challenge, we propose a
novel approach in this paper. Our approach involves utilizing multi-output
embedding network that maps samples into distinct feature spaces. The proposed
method extract feature vectors at different stages, enabling the model to
capture both global and abstract features. By utilizing these diverse feature
spaces, our model enhances its performance. Moreover, employing a
self-attention mechanism improves the refinement of features at each stage,
leading to even more robust representations and improved overall performance.
Furthermore, assigning learnable weights to each stage significantly improved
performance and results. We conducted comprehensive evaluations on the
MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way
5-shot scenarios. Additionally, we performed a cross-domain task from
MiniImageNet to the CUB dataset, achieving high accuracy in the testing domain.
These evaluations demonstrate the efficacy of our proposed method in comparison
to state-of-the-art approaches. https://github.com/FatemehAskari/MSENet

æè¦ï¼å¨å°æ ·æ¬åç±»çèæ¯ä¸ï¼ç®æ æ¯ä½¿ç¨æéæ°éçæ ·æ¬è®­ç»åç±»å¨ï¼åæ¶ä¿æä»¤äººæ»¡æçæ§è½ãç¶èï¼ä¼ ç»çåºäºåº¦éçæ¨¡åå¨å®ç°è¿ä¸ç®æ æ¶è¡¨ç°åºä¸å®çå±éæ§ãè¿äºæ¹æ³éå¸¸ä¾èµäºæ¥è¯¢ç¹å¾åæ¯æç¹å¾ä¹é´çåä¸ªè·ç¦»å¼ï¼ä»èå¿½ç¥äºæµå±ç¹å¾çè´¡ç®ãä¸ºäºåæè¿ä¸ææï¼æä»¬å¨æ¬æä¸­æåºäºä¸ç§æ°é¢çæ¹æ³ãæä»¬çæ¹æ³æ¶åå©ç¨å°æ ·æ¬æ å°å°ä¸åç¹å¾ç©ºé´çå¤è¾åºåµå¥ç½ç»ãææåºçæ¹æ³å¨ä¸åçé¶æ®µæåç¹å¾åéï¼ä½¿æ¨¡åè½å¤ææå¨å±åæ½è±¡ç¹å¾ãéè¿å©ç¨è¿äºä¸åçç¹å¾ç©ºé´ï¼æä»¬çæ¨¡åå¢å¼ºäºå¶æ§è½ãæ­¤å¤ï¼éç¨èªæ³¨æåæºå¶æ¹è¿äºæ¯ä¸ªé¶æ®µçç¹å¾ç»åï¼ä»èäº§çäºæ´é²æ£çè¡¨ç¤ºå¹¶æé«äºæ´ä½æ§è½ãæ­¤å¤ï¼ä¸ºæ¯ä¸ªé¶æ®µåéå¯å­¦ä¹ çæéæ¾èæé«äºæ§è½åç»æãæä»¬å¯¹ MiniImageNet å FC100 æ°æ®éè¿è¡äºå¨é¢çè¯ä¼°ï¼ç¹å«æ¯å¨ 5-way 1-shot å 5-way 5-shot åºæ¯ä¸­ãæ­¤å¤ï¼æä»¬ä» MiniImageNet æ§è¡äºä¸ä¸ªè·¨åä»»å¡å° CUB æ°æ®éï¼å¨æµè¯åä¸­å®ç°äºé«ç²¾åº¦ãè¿äºè¯ä¼°è¯æäºæä»¬æåºçæ¹æ³ä¸æåè¿çæ¹æ³ç¸æ¯çæææ§ãhttps://github.com/FatemehAskari/MSENet

##### **Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols**
2409.07985v1 by Charlie Griffin, Louis Thomson, Buck Shlegeris, Alessandro Abate

To evaluate the safety and usefulness of deployment protocols for untrusted
AIs, AI Control uses a red-teaming exercise played between a protocol designer
and an adversary. This paper introduces AI-Control Games, a formal
decision-making model of the red-teaming exercise as a multi-objective,
partially observable, stochastic game. We also introduce methods for finding
optimal protocols in AI-Control Games, by reducing them to a set of zero-sum
partially observable stochastic games. We apply our formalism to model,
evaluate and synthesise protocols for deploying untrusted language models as
programming assistants, focusing on Trusted Monitoring protocols, which use
weaker language models and limited human assistance. Finally, we demonstrate
the utility of our formalism by showcasing improvements over empirical studies
in existing settings, evaluating protocols in new settings, and analysing how
modelling assumptions affect the safety and usefulness of protocols.

æè¦ï¼çºäºè©ä¼°ä¸ä¿¡ä»» AI çé¨ç½²åå®çå®å¨æ§èå¯¦ç¨æ§ï¼AI æ§å¶ä½¿ç¨ç´éæ¼ç¿ï¼ç±åå®è¨­è¨èèå°æä¹éé²è¡ãæ¬æä»ç´¹ AI æ§å¶éæ²ï¼ä½çºç´éæ¼ç¿çå¤ç®æ¨ãé¨åå¯è§å¯ãé¨æ©éæ²çæ­£å¼æ±ºç­æ¨¡åãæåä¹ä»ç´¹å¨ AI æ§å¶éæ²ä¸­å°æ¾æä½³åå®çæ¹æ³ï¼æ¹æ³æ¯å°å¶ç°¡åçºä¸çµé¶åé¨åå¯è§å¯é¨æ©éæ²ãæåå°å½¢å¼ä¸»ç¾©æç¨æ¼å»ºæ¨¡ãè©ä¼°åç¶åé¨ç½²ä¸ä¿¡ä»»èªè¨æ¨¡åçåå®ï¼ä½çºç¨å¼è¨­è¨å©çï¼éé»æ¾å¨åä¿¡ä»»ç£æ§åå®ï¼ä½¿ç¨è¼å¼±çèªè¨æ¨¡ååæéçäººå·¥åå©ãæå¾ï¼æåå±ç¤ºå½¢å¼ä¸»ç¾©çå¯¦ç¨æ§ï¼å±ç¤ºå°ç¾æè¨­å®ä¸­ç¶é©ç ç©¶çæ¹é²ãè©ä¼°æ°è¨­å®ä¸­çåå®ï¼ä»¥ååæå»ºæ¨¡åè¨­å¦ä½å½±é¿åå®çå®å¨æ§èå¯¦ç¨æ§ã

##### **ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE**
2409.07966v1 by Sichun Wu, Kazi Injamamul Haque, Zerrin Yumak

Audio-driven 3D facial animation synthesis has been an active field of
research with attention from both academia and industry. While there are
promising results in this area, recent approaches largely focus on lip-sync and
identity control, neglecting the role of emotions and emotion control in the
generative process. That is mainly due to the lack of emotionally rich facial
animation data and algorithms that can synthesize speech animations with
emotional expressions at the same time. In addition, majority of the models are
deterministic, meaning given the same audio input, they produce the same output
motion. We argue that emotions and non-determinism are crucial to generate
diverse and emotionally-rich facial animations. In this paper, we propose
ProbTalk3D a non-deterministic neural network approach for emotion controllable
speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and
an emotionally rich facial animation dataset 3DMEAD. We provide an extensive
comparative analysis of our model against the recent 3D facial animation
synthesis approaches, by evaluating the results objectively, qualitatively, and
with a perceptual user study. We highlight several objective metrics that are
more suitable for evaluating stochastic outputs and use both in-the-wild and
ground truth data for subjective evaluation. To our knowledge, that is the
first non-deterministic 3D facial animation synthesis method incorporating a
rich emotion dataset and emotion control with emotion labels and intensity
levels. Our evaluation demonstrates that the proposed model achieves superior
performance compared to state-of-the-art emotion-controlled, deterministic and
non-deterministic models. We recommend watching the supplementary video for
quality judgement. The entire codebase is publicly available
(https://github.com/uuembodiedsocialai/ProbTalk3D/).

æè¦ï¼<paragraph>é³é¢é©±å¨ç 3D é¢é¨å¨ç»åæä¸ç´æ¯å­¦æ¯çåäº§ä¸çå³æ³¨çä¸ä¸ªæ´»è·çç ç©¶é¢åãè½ç¶è¯¥é¢åæä»¤äººæ¯å¥çç ç©¶ææï¼ä½æè¿çæ¹æ³ä¸»è¦éä¸­å¨åå½¢åæ­¥åèº«ä»½æ§å¶ä¸ï¼å¿½ç¥äºææåæææ§å¶å¨çæè¿ç¨ä¸­çä½ç¨ãè¿ä¸»è¦æ¯å ä¸ºç¼ºä¹ææä¸°å¯çé¢é¨å¨ç»æ°æ®åè½å¤åæ¶åæå·æææè¡¨è¾¾çè¯­é³å¨ç»çç®æ³ãæ­¤å¤ï¼å¤§å¤æ°æ¨¡åé½æ¯ç¡®å®æ§çï¼è¿æå³çç»å®ç¸åçé³é¢è¾å¥ï¼å®ä»¬ä¼äº§çç¸åçè¾åºå¨ä½ãæä»¬è®¤ä¸ºï¼ææåéç¡®å®æ§å¯¹äºçæå¤æ ·åä¸ææä¸°å¯çé¢é¨å¨ç»è³å³éè¦ãå¨æ¬æä¸­ï¼æä»¬æåºäº ProbTalk3Dï¼è¿æ¯ä¸ç§éç¡®å®æ§ç¥ç»ç½ç»æ¹æ³ï¼ç¨äºä½¿ç¨ä¸¤é¶æ®µ VQ-VAE æ¨¡ååææä¸°å¯çé¢é¨å¨ç»æ°æ®é 3DMEAD è¿è¡ææå¯æ§çè¯­é³é©±å¨ç 3D é¢é¨å¨ç»åæãæä»¬éè¿å®¢è§ãå®æ§åæç¥ç¨æ·ç ç©¶è¯ä¼°ç»æï¼å¯¹æä»¬çæ¨¡åä¸æè¿ç 3D é¢é¨å¨ç»åææ¹æ³è¿è¡äºå¹¿æ³çæ¯è¾åæãæä»¬éç¹ä»ç»äºå ä¸ªæ´éåè¯ä¼°éæºè¾åºçç®æ ææ ï¼å¹¶å¨ä¸»è§è¯ä¼°ä¸­ä½¿ç¨äºéå¤æ°æ®åçå®æ°æ®ãæ®æä»¬æç¥ï¼è¿æ¯ç¬¬ä¸ç§éç¡®å®æ§ 3D é¢é¨å¨ç»åææ¹æ³ï¼å®ç»åäºä¸°å¯çè¡¨ææ°æ®éåå¸¦æè¡¨ææ ç­¾åå¼ºåº¦ç­çº§çè¡¨ææ§å¶ãæä»¬çè¯ä¼°è¡¨æï¼ä¸æåè¿çæææ§å¶ãç¡®å®æ§åéç¡®å®æ§æ¨¡åç¸æ¯ï¼ææåºçæ¨¡ååå¾äºåè¶çæ§è½ãæä»¬å»ºè®®è§çè¡¥åè§é¢ä»¥è¿è¡è´¨éå¤æ­ãæ´ä¸ªä»£ç åºå·²å¬å¼ï¼https://github.com/uuembodiedsocialai/ProbTalk3D/ï¼ã</paragraph>

##### **WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks**
2409.07964v1 by Jingwen Tong, Jiawei Shao, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang

Wireless networks are increasingly facing challenges due to their expanding
scale and complexity. These challenges underscore the need for advanced
AI-driven strategies, particularly in the upcoming 6G networks. In this
article, we introduce WirelessAgent, a novel approach leveraging large language
models (LLMs) to develop AI agents capable of managing complex tasks in
wireless networks. It can effectively improve network performance through
advanced reasoning, multimodal data processing, and autonomous decision making.
Thereafter, we demonstrate the practical applicability and benefits of
WirelessAgent for network slicing management. The experimental results show
that WirelessAgent is capable of accurately understanding user intent,
effectively allocating slice resources, and consistently maintaining optimal
performance.

æè¦ï¼é¨èç¡ç·ç¶²è·¯è¦æ¨¡èè¤éåº¦çæ´å¼µï¼ç¡ç·ç¶²è·¯é¢è¨çææ°è¶ä¾è¶å¤ãéäºææ°å¸é¡¯åºå°åé² AI é©åç­ç¥çéæ±ï¼ç¹å¥æ¯å¨å³å°å°ä¾ç 6G ç¶²è·¯ä¸­ãå¨æ¬æä¸­ï¼æåä»ç´¹ WirelessAgentï¼éæ¯ä¸ç¨®å©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾éç¼ AI ä»£ççæ°æ¹æ³ï¼æ­¤ä»£çè½å¤ ç®¡çç¡ç·ç¶²è·¯ä¸­çè¤éä»»åãå®å¯ä»¥ééåé²çæ¨çãå¤æ¨¡ææ¸æèçåèªä¸»æ±ºç­å¶å®ï¼ææå°æ¹åç¶²è·¯æè½ãé¨å¾ï¼æåå±ç¤º WirelessAgent å¨ç¶²è·¯åçç®¡çä¸­çå¯¦éé©ç¨æ§åå¥½èãå¯¦é©çµæé¡¯ç¤ºï¼WirelessAgent è½å¤ æºç¢ºçè§£ä½¿ç¨èæåãææåéåçè³æºï¼ä¸¦æçºç¶­ææä½³æè½ã

##### **Enhanced Online Grooming Detection Employing Context Determination and Message-Level Analysis**
2409.07958v1 by Jake Street, Isibor Ihianle, Funminiyi Olajide, Ahmad Lotfi

Online Grooming (OG) is a prevalent threat facing predominately children
online, with groomers using deceptive methods to prey on the vulnerability of
children on social media/messaging platforms. These attacks can have severe
psychological and physical impacts, including a tendency towards
revictimization. Current technical measures are inadequate, especially with the
advent of end-to-end encryption which hampers message monitoring. Existing
solutions focus on the signature analysis of child abuse media, which does not
effectively address real-time OG detection. This paper proposes that OG attacks
are complex, requiring the identification of specific communication patterns
between adults and children. It introduces a novel approach leveraging advanced
models such as BERT and RoBERTa for Message-Level Analysis and a Context
Determination approach for classifying actor interactions, including the
introduction of Actor Significance Thresholds and Message Significance
Thresholds. The proposed method aims to enhance accuracy and robustness in
detecting OG by considering the dynamic and multi-faceted nature of these
attacks. Cross-dataset experiments evaluate the robustness and versatility of
our approach. This paper's contributions include improved detection
methodologies and the potential for application in various scenarios,
addressing gaps in current literature and practices.

æè¦ï¼ç¶²è·¯èªé¨ï¼OGï¼æ¯ä¸ç¨®æ®éçå¨èï¼ä¸»è¦éå°åç«¥ï¼èªé¨èä½¿ç¨æ¬ºé¨æ§æ¹æ³å¨ç¤¾ç¾¤åªé«ï¼å³è¨å¹³å°ä¸å©ç¨åç«¥çå¼±é»ãéäºæ»æå¯è½é æå´éçççåå¿çå½±é¿ï¼åæ¬å®¹æåæ¬¡åå®³ãç®åçæè¡æªæ½ä¸è¶³ï¼ç¹å¥æ¯å¨ç«¯å°ç«¯å å¯çåºç¾é»ç¤äºè¨æ¯ç£æ§ãç¾æçè§£æ±ºæ¹æ¡å°æ³¨æ¼åç«¥æ§èå¾åªé«çç°½ååæï¼ç¡æ³ææè§£æ±ºå³æ OG åµæ¸¬ãæ¬ææåº OG æ»æå¾è¤éï¼éè¦æ¾åºæäººååç«¥ä¹éçç¹å®æºéæ¨¡å¼ãæ¬æä»ç´¹ä¸ç¨®æ°æ¹æ³ï¼å©ç¨ BERT å RoBERTa ç­é²éæ¨¡åé²è¡è¨æ¯å±¤ç´åæï¼ä»¥åä¸ç¨®æå¢å¤æ·æ¹æ³ï¼ç¨æ¼åé¡åèèçäºåï¼åæ¬å¼å¥åèèé¡¯èæ§é¾å¼åè¨æ¯é¡¯èæ§é¾å¼ãæåºçæ¹æ³æ¨å¨ééèééäºæ»æçåæåå¤é¢åæ§è³ªä¾æååµæ¸¬ OG çæºç¢ºæ§åç©©å¥æ§ãè·¨è³æéå¯¦é©è©ä¼°äºæåæ¹æ³çç©©å¥æ§åå¤æ¨£æ§ãæ¬æçè²¢ç»åæ¬æ¹è¯çåµæ¸¬æ¹æ³ï¼ä»¥åå¨åç¨®æå¢ä¸­æç¨çæ½åï¼è§£æ±ºäºç®åæç»åå¯¦åä¸­çå·®è·ã

##### **Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies**
2409.07932v1 by Alexei Pisacane, Victor-Alexandru Darvariu, Mirco Musolesi

Graph path search is a classic computer science problem that has been
recently approached with Reinforcement Learning (RL) due to its potential to
outperform prior methods. Existing RL techniques typically assume a global view
of the network, which is not suitable for large-scale, dynamic, and
privacy-sensitive settings. An area of particular interest is search in social
networks due to its numerous applications. Inspired by seminal work in
experimental sociology, which showed that decentralized yet efficient search is
possible in social networks, we frame the problem as a collaborative task
between multiple agents equipped with a limited local view of the network. We
propose a multi-agent approach for graph path search that successfully
leverages both homophily and structural heterogeneity. Our experiments, carried
out over synthetic and real-world social networks, demonstrate that our model
significantly outperforms learned and heuristic baselines. Furthermore, our
results show that meaningful embeddings for graph navigation can be constructed
using reward-driven learning.

æè¦ï¼åå½¢è·¯å¾æå°æ¯ä¸åç¶å¸çé»è¦ç§å­¸åé¡ï¼æè¿å·²ééå¼·åå­¸ç¿ (RL) ä¾èçï¼å çºå®æè¶è¶ååæ¹æ³çæ½åãç¾æç RL æè¡éå¸¸åè¨­ç¶²è·¯çå¨å±è§é»ï¼éä¸é©åæ¼å¤§è¦æ¨¡ãåæåæ³¨éé±ç§çè¨­å®ãä¸åç¹å¥æèè¶£çé åæ¯ç¤¾ç¾¤ç¶²è·¯ä¸­çæå°ï¼å çºå®çæç¨å¾å»£æ³ãåå°å¯¦é©ç¤¾æå­¸ä¸­å·æéåµæ§çç ç©¶åç¼ï¼è©²ç ç©¶é¡¯ç¤ºå¨ç¤¾ç¾¤ç¶²è·¯ä¸­åæ£ä½ææççæå°æ¯å¯è¡çï¼æåå°åé¡è¨­å®çºå¤åå·åç¶²è·¯æéå±é¨è§é»çä»£çä¹éçåä½ä»»åãæåæåºä¸ååå½¢è·¯å¾æå°çå¤ä»£çæ¹æ³ï¼æåå°åæå©ç¨åè³ªæ§åçµæ§ç°è³ªæ§ãæåçå¯¦é©å¨åæåçå¯¦ä¸ççç¤¾ç¾¤ç¶²è·¯ä¸­é²è¡ï¼è­ææåçæ¨¡åæé¡¯åªæ¼å·²å­¸ç¿ååç¼å¼çåºæºãæ­¤å¤ï¼æåççµæé¡¯ç¤ºï¼å¯ä»¥ç¨çåµé©åçå­¸ç¿ä¾å»ºæ§ææç¾©çåå½¢å°èªåµå¥ã

##### **A convolutional neural network approach to deblending seismic data**
2409.07930v1 by Jing Sun, Sigmund Slang, Thomas Elboth, Thomas Larsen Greiner, Steven McDonald, Leiv-J Gelius

For economic and efficiency reasons, blended acquisition of seismic data is
becoming more and more commonplace. Seismic deblending methods are always
computationally demanding and normally consist of multiple processing steps.
Besides, the parameter setting is not always trivial. Machine learning-based
processing has the potential to significantly reduce processing time and to
change the way seismic deblending is carried out. We present a data-driven deep
learning-based method for fast and efficient seismic deblending. The blended
data are sorted from the common source to the common channel domain to
transform the character of the blending noise from coherent events to
incoherent distributions. A convolutional neural network (CNN) is designed
according to the special character of seismic data, and performs deblending
with comparable results to those obtained with conventional industry deblending
algorithms. To ensure authenticity, the blending was done numerically and only
field seismic data were employed, including more than 20000 training examples.
After training and validation of the network, seismic deblending can be
performed in near real time. Experiments also show that the initial signal to
noise ratio (SNR) is the major factor controlling the quality of the final
deblended result. The network is also demonstrated to be robust and adaptive by
using the trained model to firstly deblend a new data set from a different
geological area with a slightly different delay time setting, and secondly
deblend shots with blending noise in the top part of the data.

æè¦ï¼<paragraph>åºæ¼ç¶æ¿åæçèéï¼æ··åå¼å°éè³æåå¾æ­£è®å¾è¶ä¾è¶æ®éãå°éå»æ··åæ¹æ³éå¸¸å¨è¨ç®ä¸è¦æ±å¾é«ï¼ä¸éå¸¸åå«å¤åèçæ­¥é©ãæ­¤å¤ï¼åæ¸è¨­å®ä¸¦ä¸ç¸½æ¯å¾®ä¸è¶³éçãåºæ¼æ©å¨å­¸ç¿çèçææ½åé¡¯èæ¸å°èçæéï¼ä¸¦æ¹è®å°éå»æ··åçå·è¡æ¹å¼ãæåæåºä¸åè³æé©åçæ·±åº¦å­¸ç¿æ¹æ³ï¼ç¨æ¼å¿«éä¸ææççå°éå»æ··åãæ··åè³æå¾å±ç¨ä¾æºæåºå°å±ç¨ééç¶²åï¼å°æ··åéè¨çç¹æ§å¾ç¸å¹²äºä»¶è½æçºéç¸å¹²åä½ãæ ¹æå°éè³æçç¹æ®ç¹æ§è¨­è¨äºä¸åå·ç©ç¥ç¶ç¶²è·¯ (CNN)ï¼ä¸¦å·è¡å»æ··åï¼å¶çµæå¯èå³çµ±ç¢æ¥­å»æ··åæ¼ç®æ³æç²å¾ççµæç¸æä¸¦è«ãçºäºç¢ºä¿çå¯¦æ§ï¼æ··åæ¯ä»¥æ¸å¼æ¹å¼å®æï¼ä¸åæ¡ç¨ç¾å ´å°éè³æï¼å¶ä¸­åå«è¶é 20000 åè¨ç·´ç¯ä¾ãå¨ç¶²è·¯è¨ç·´åé©è­å¾ï¼å°éå»æ··åå¯ä»¥å¨è¿ä¹å³æçææ³ä¸å·è¡ãå¯¦é©ä¹é¡¯ç¤ºï¼åå§è¨èéè¨æ¯ (SNR) æ¯æ§å¶æçµå»æ··åçµæåè³ªçä¸»è¦å ç´ ãééä½¿ç¨è¨ç·´å¥½çæ¨¡åï¼é¦åå»æ··åä¾èªä¸åå°è³ªååçæ°è³æéï¼ä¸¦ä½¿ç¨ç¨å¾®ä¸åçå»¶é²æéè¨­å®ï¼å¶æ¬¡å»æ··åè³æé é¨å·ææ··åéè¨çå°æï¼è­æäºè©²ç¶²è·¯çå¼·å¥æ§åé©ææ§ã</paragraph>

##### **A framework for measuring the training efficiency of a neural architecture**
2409.07925v1 by Eduardo Cueto-Mendoza, John D. Kelleher

Measuring Efficiency in neural network system development is an open research
problem. This paper presents an experimental framework to measure the training
efficiency of a neural architecture. To demonstrate our approach, we analyze
the training efficiency of Convolutional Neural Networks and Bayesian
equivalents on the MNIST and CIFAR-10 tasks. Our results show that training
efficiency decays as training progresses and varies across different stopping
criteria for a given neural model and learning task. We also find a non-linear
relationship between training stopping criteria, training Efficiency, model
size, and training Efficiency.
  Furthermore, we illustrate the potential confounding effects of overtraining
on measuring the training efficiency of a neural architecture. Regarding
relative training efficiency across different architectures, our results
indicate that CNNs are more efficient than BCNNs on both datasets. More
generally, as a learning task becomes more complex, the relative difference in
training efficiency between different architectures becomes more pronounced.

æè¦ï¼ç¥ç¶ç¶²è·¯ç³»çµ±éç¼ä¸­ï¼è¡¡éæçæ¯ä¸åéæ¾çç ç©¶åé¡ãæ¬ææåºäºä¸åå¯¦é©æ¶æ§ï¼ç¨æ¼è¡¡éç¥ç¶æ¶æ§çè¨ç·´æçãçºäºå±ç¤ºæåçåæ³ï¼æååæäºå·ç©ç¥ç¶ç¶²è·¯åè²æ°ç­å¹å¨ MNIST å CIFAR-10 ä»»åä¸çè¨ç·´æçãæåççµæé¡¯ç¤ºï¼è¨ç·´æçæé¨èè¨ç·´é²åº¦èè¡°æ¸ï¼ä¸¦ä¸ææ ¹æçµ¦å®ç¥ç¶æ¨¡ååå­¸ç¿ä»»åçä¸ååæ­¢æºåèææä¸åãæåéç¼ç¾è¨ç·´åæ­¢æºåãè¨ç·´æçãæ¨¡åå¤§å°åè¨ç·´æçä¹éå­å¨éç·æ§éä¿ãæ­¤å¤ï¼æåèªªæäºéåº¦è¨ç·´å°è¡¡éç¥ç¶æ¶æ§è¨ç·´æççæ½å¨æ··æ·å½±é¿ãéæ¼ä¸åæ¶æ§ä¹éçç¸å°è¨ç·´æçï¼æåççµæè¡¨æï¼CNN å¨å©åè³æéä¸é½æ¯ BCNN æ´ææçãæ´ä¸è¬å°èªªï¼é¨èå­¸ç¿ä»»åè®å¾æ´è¤éï¼ä¸åæ¶æ§ä¹éçè¨ç·´æçç¸å°å·®ç°è®å¾æ´å æé¡¯ã

##### **Tidal MerzA: Combining affective modelling and autonomous code generation through Reinforcement Learning**
2409.07918v1 by Elizabeth Wilson, GyÃ¶rgy Fazekas, Geraint Wiggins

This paper presents Tidal-MerzA, a novel system designed for collaborative
performances between humans and a machine agent in the context of live coding,
specifically focusing on the generation of musical patterns. Tidal-MerzA fuses
two foundational models: ALCAA (Affective Live Coding Autonomous Agent) and
Tidal Fuzz, a computational framework. By integrating affective modelling with
computational generation, this system leverages reinforcement learning
techniques to dynamically adapt music composition parameters within the
TidalCycles framework, ensuring both affective qualities to the patterns and
syntactical correctness. The development of Tidal-MerzA introduces two distinct
agents: one focusing on the generation of mini-notation strings for musical
expression, and another on the alignment of music with targeted affective
states through reinforcement learning. This approach enhances the adaptability
and creative potential of live coding practices and allows exploration of
human-machine creative interactions. Tidal-MerzA advances the field of
computational music generation, presenting a novel methodology for
incorporating artificial intelligence into artistic practices.

æè¦ï¼æ¬æä»ç´¹ Tidal-MerzAï¼éæ¯ä¸åæ°ç©çç³»çµ±ï¼å°çºç¾å ´ç·¨ç¢¼ä¸­äººé¡èæ©å¨ä»£çä¹éçåä½è¡¨æ¼èè¨­è¨ï¼ç¹å¥èéæ¼é³æ¨æ¨¡å¼çç¢çãTidal-MerzA èåäºå©ååºç¤æ¨¡åï¼ALCAAï¼ææç¾å ´ç·¨ç¢¼èªä¸»ä»£çï¼å Tidal Fuzzï¼ä¸åè¨ç®æ¡æ¶ãééæ´åææå»ºæ¨¡èè¨ç®ç¢çï¼æ­¤ç³»çµ±å©ç¨å¼·åå­¸ç¿æè¡å¨ TidalCycles æ¡æ¶å§åæèª¿æ´é³æ¨çµæåæ¸ï¼ç¢ºä¿æ¨¡å¼çææåè³ªåèªæ³æ­£ç¢ºæ§ãTidal-MerzA çéç¼å¼å¥äºå©åä¸åçä»£çï¼ä¸åå°æ³¨æ¼ç¢çé³æ¨è¡¨éçè¿·ä½ ç¬¦èå­ä¸²ï¼å¦ä¸åééå¼·åå­¸ç¿å°é³æ¨èç®æ¨ææçæå°é½ãéç¨®æ¹æ³å¢å¼·äºç¾å ´ç·¨ç¢¼å¯¦åçé©ææ§ååµé æ½åï¼ä¸¦åè¨±æ¢ç´¢äººæ©åµé äºåãTidal-MerzA æ¨åäºè¨ç®é³æ¨çæçé åï¼æåºäºä¸ç¨®å°äººå·¥æºæ§ç´å¥èè¡å¯¦åçæ°æ¹æ³ã

##### **UGAD: Universal Generative AI Detector utilizing Frequency Fingerprints**
2409.07913v1 by Inzamamul Alam, Muhammad Shahid Muneer, Simon S. Woo

In the wake of a fabricated explosion image at the Pentagon, an ability to
discern real images from fake counterparts has never been more critical. Our
study introduces a novel multi-modal approach to detect AI-generated images
amidst the proliferation of new generation methods such as Diffusion models.
Our method, UGAD, encompasses three key detection steps: First, we transform
the RGB images into YCbCr channels and apply an Integral Radial Operation to
emphasize salient radial features. Secondly, the Spatial Fourier Extraction
operation is used for a spatial shift, utilizing a pre-trained deep learning
network for optimal feature extraction. Finally, the deep neural network
classification stage processes the data through dense layers using softmax for
classification. Our approach significantly enhances the accuracy of
differentiating between real and AI-generated images, as evidenced by a 12.64%
increase in accuracy and 28.43% increase in AUC compared to existing
state-of-the-art methods.

æè¦ï¼å¨äºè§å¤§å¦ä¼ªé çç¸å¾åäºä»¶åçåï¼è¾¨å«çåå¾åçè½åä»æªå¦æ­¤å³é®ãæä»¬çç ç©¶å¼å¥äºä¸ç§æ°é¢çå¤æ¨¡ææ¹æ³ï¼ä»¥å¨æ©æ£æ¨¡åç­æ°ä¸ä»£æ¹æ³æ¿å¢çæåµä¸æ£æµ AI çæçå¾åãæä»¬çæ¹æ³ UGAD åå«ä¸ä¸ªå³é®æ£æµæ­¥éª¤ï¼é¦åï¼æä»¬å° RGB å¾åè½¬æ¢ä¸º YCbCr ééï¼å¹¶åºç¨ç§¯åå¾åè¿ç®æ¥å¼ºè°æ¾ççå¾åç¹å¾ãå¶æ¬¡ï¼ç©ºé´åéå¶æåæä½ç¨äºç©ºé´ä½ç§»ï¼å©ç¨é¢è®­ç»çæ·±åº¦å­¦ä¹ ç½ç»è¿è¡æä¼ç¹å¾æåãæåï¼æ·±åº¦ç¥ç»ç½ç»åç±»é¶æ®µä½¿ç¨ softmax å¯¹æ°æ®è¿è¡å¯éå±å¤çä»¥è¿è¡åç±»ãæä»¬çæ¹æ³æ¾çæé«äºåºåçå®å¾åå AI çæçå¾åçåç¡®æ§ï¼ä¸ç°æçæåè¿æ¹æ³ç¸æ¯ï¼åç¡®æ§æé«äº 12.64%ï¼AUC æé«äº 28.43%ã

##### **A corpus-based investigation of pitch contours of monosyllabic words in conversational Taiwan Mandarin**
2409.07891v1 by Xiaoyun Jin, Mirjam Ernestus, R. Harald Baayen

In Mandarin, the tonal contours of monosyllabic words produced in isolation
or in careful speech are characterized by four lexical tones: a high-level tone
(T1), a rising tone (T2), a dipping tone (T3) and a falling tone (T4). However,
in spontaneous speech, the actual tonal realization of monosyllabic words can
deviate significantly from these canonical tones due to intra-syllabic
co-articulation and inter-syllabic co-articulation with adjacent tones. In
addition, Chuang et al. (2024) recently reported that the tonal contours of
disyllabic Mandarin words with T2-T4 tone pattern are co-determined by their
meanings. Following up on their research, we present a corpus-based
investigation of how the pitch contours of monosyllabic words are realized in
spontaneous conversational Mandarin, focusing on the effects of contextual
predictors on the one hand, and the way in words' meanings co-determine pitch
contours on the other hand. We analyze the F0 contours of 3824 tokens of 63
different word types in a spontaneous Taiwan Mandarin corpus, using the
generalized additive (mixed) model to decompose a given observed pitch contour
into a set of component pitch contours. We show that the tonal context
substantially modify a word's canonical tone. Once the effect of tonal context
is controlled for, T2 and T3 emerge as low flat tones, contrasting with T1 as a
high tone, and with T4 as a high-to-mid falling tone. The neutral tone (T0),
which in standard descriptions, is realized based on the preceding tone,
emerges as a low tone in its own right, modified by the other predictors in the
same way as the standard tones T1, T2, T3, and T4. We also show that word, and
even more so, word sense, co-determine words' F0 contours. Analyses of variable
importance using random forests further supported the substantial effect of
tonal context and an effect of word sense.

æè¦ï¼<paragraph>å¨æ®éè©±ä¸­ï¼å®é³ç¯è©å¨å­¤ç«çææå¨è¬¹æçèªé³ä¸­ç¢ççé³èª¿è¼ªå»ï¼å·æåç¨®èªèª¿ï¼é«å¹³èª¿ï¼T1ï¼ãåèª¿ï¼T2ï¼ãéèª¿ï¼T3ï¼åéèª¿ï¼T4ï¼ãç¶èï¼å¨èªç¼çèªé³ä¸­ï¼å®é³ç¯è©çå¯¦éé³èª¿å¯¦ç¾å¯è½æå é³ç¯å§å±ç¼é³åèç¸é°é³èª¿çé³ç¯éå±ç¼é³èèéäºè¦ç¯é³èª¿æé¡¯èçåå·®ãæ­¤å¤ï¼èç­äººï¼2024 å¹´ï¼æè¿å ±å°èªªï¼å·æ T2-T4 é³èª¿æ¨¡å¼çéé³ç¯æ®éè©±è©çé³èª¿è¼ªå»æ¯ç±å¶å«ç¾©å±åæ±ºå®çãå¨ä»åçç ç©¶åºç¤ä¸ï¼æåæåºäºä¸é åºæ¼èªæåº«çèª¿æ¥ï¼æ¢è¨å®é³ç¯è©çé³é«è¼ªå»å¦ä½å¨èªç¼çå°è©±å¼æ®éè©±ä¸­å¯¦ç¾ï¼ä¸æ¹é¢èéæ¼ä¸ä¸æé æ¸¬å ç´ çå½±é¿ï¼å¦ä¸æ¹é¢èéæ¼è©å½å«ç¾©å±åæ±ºå®é³é«è¼ªå»çæ¹å¼ãæååæäºä¸åèªç¼çå°ç£æ®éè©±èªæåº«ä¸­ 63 ç¨®ä¸åè©é¡ç 3824 åè©æ¢ç F0 è¼ªå»ï¼ä½¿ç¨å»£ç¾©å æ³ï¼æ··åï¼æ¨¡åå°çµ¦å®çè§å¯å°çé³é«è¼ªå»åè§£çºä¸çµçµæé³é«è¼ªå»ãæåè¡¨æé³èª¿èªå¢æå¤§å¹ä¿®æ¹ä¸åè©çè¦ç¯é³èª¿ãä¸æ¦æ§å¶äºé³èª¿èªå¢çå½±é¿ï¼T2 å T3 å°±æåç¾çºä½å¹³èª¿ï¼è T1 ä½çºé«é³èª¿å½¢æå°æ¯ï¼è T4 ä½çºé«å°ä¸­éèª¿å½¢æå°æ¯ãå¨æ¨æºæè¿°ä¸­åºæ¼åä¸åé³èª¿å¯¦ç¾çè¼è²èª¿ï¼T0ï¼ä»¥å¶èªèº«çä½é³èª¿åºç¾ï¼èæ¨æºé³èª¿ T1ãT2ãT3 å T4 ä»¥ç¸åçæ¹å¼åå°å¶ä»é æ¸¬å ç´ çä¿®æ¹ãæåéè¡¨æï¼è©å½ï¼çè³è©ç¾©ï¼å±åæ±ºå®è©å½ç F0 è¼ªå»ãä½¿ç¨é¨æ©æ£®æé²è¡çå¯è®éè¦æ§åæé²ä¸æ­¥æ¯æäºé³èª¿èªå¢åè©ç¾©çé¡¯èå½±é¿ã</paragraph>

##### **Learning Rules from KGs Guided by Language Models**
2409.07869v1 by Zihang Peng, Daria Stepanova, Vinh Thinh Ho, Heike Adel, Alessandra Russo, Simon Ott

Advances in information extraction have enabled the automatic construction of
large knowledge graphs (e.g., Yago, Wikidata or Google KG), which are widely
used in many applications like semantic search or data analytics. However, due
to their semi-automatic construction, KGs are often incomplete. Rule learning
methods, concerned with the extraction of frequent patterns from KGs and
casting them into rules, can be applied to predict potentially missing facts. A
crucial step in this process is rule ranking. Ranking of rules is especially
challenging over highly incomplete or biased KGs (e.g., KGs predominantly
storing facts about famous people), as in this case biased rules might fit the
data best and be ranked at the top based on standard statistical metrics like
rule confidence. To address this issue, prior works proposed to rank rules not
only relying on the original KG but also facts predicted by a KG embedding
model. At the same time, with the recent rise of Language Models (LMs), several
works have claimed that LMs can be used as alternative means for KG completion.
In this work, our goal is to verify to which extent the exploitation of LMs is
helpful for improving the quality of rule learning systems.

æè¦ï¼è³è¨èåçé²å±å·²è½èªåå»ºæ§å¤§åç¥è­åè­ï¼ä¾å¦ YagoãWikidata æ Google KGï¼ï¼éäºç¥è­åè­å»£æ³ç¨æ¼è¨±å¤æç¨ç¨å¼ï¼ä¾å¦èªææå°æè³æåæãç¶èï¼ç±æ¼éäºç¥è­åè­æ¯åèªåå»ºæ§çï¼å æ­¤éå¸¸ä¸¦ä¸å®æ´ãè¦åå­¸ç¿æ¹æ³èéæ¼å¾ç¥è­åè­ä¸­èåé »ç¹æ¨¡å¼ï¼ä¸¦å°å®åè½æçºè¦åï¼å¯æç¨æ¼é æ¸¬æ½å¨éºå¤±çäºå¯¦ãæ­¤éç¨ä¸­çä¸åééµæ­¥é©æ¯è¦åæåºãè¦åæåºå¨é«åº¦ä¸å®æ´ææåå·®çç¥è­åè­ï¼ä¾å¦ï¼ä¸»è¦å²å­åäººäºå¯¦çç¥è­åè­ï¼ä¸­ç¹å¥å·æææ°æ§ï¼å çºå¨éç¨®ææ³ä¸ï¼æåå·®çè¦åå¯è½æç¬¦åè³æï¼ä¸¦æ ¹ææ¨æºçµ±è¨éåº¦ï¼ä¾å¦è¦åä¿¡å¿ï¼æå¨æåé¢ãçºäºè§£æ±ºéååé¡ï¼ååçç ç©¶æåºä¸åªä¾è³´åå§ç¥è­åè­ï¼éè¦ä¾è³´ç¥è­åè­åµå¥æ¨¡åé æ¸¬çäºå¯¦ä¾å°è¦åé²è¡æåºãåæï¼é¨èèªè¨æ¨¡å (LM) çèèµ·ï¼ä¸äºç ç©¶è²ç¨± LM å¯ç¨ä½ç¥è­åè­å®æçæ¿ä»£æ¹æ³ãå¨éé ç ç©¶ä¸­ï¼æåçç®æ¨æ¯é©è­å©ç¨ LM å¨å¤å¤§ç¨åº¦ä¸æå©æ¼æåè¦åå­¸ç¿ç³»çµ±çåè³ªã

##### **Enhancing Cross-Market Recommendation System with Graph Isomorphism Networks: A Novel Approach to Personalized User Experience**
2409.07850v1 by SÃ¼meyye ÃztÃ¼rk, Ahmed Burak Ercan, Resul Tugay, Åule GÃ¼ndÃ¼z ÃÄÃ¼dÃ¼cÃ¼

In today's world of globalized commerce, cross-market recommendation systems
(CMRs) are crucial for providing personalized user experiences across diverse
market segments. However, traditional recommendation algorithms have
difficulties dealing with market specificity and data sparsity, especially in
new or emerging markets. In this paper, we propose the CrossGR model, which
utilizes Graph Isomorphism Networks (GINs) to improve CMR systems. It
outperforms existing benchmarks in NDCG@10 and HR@10 metrics, demonstrating its
adaptability and accuracy in handling diverse market segments. The CrossGR
model is adaptable and accurate, making it well-suited for handling the
complexities of cross-market recommendation tasks. Its robustness is
demonstrated by consistent performance across different evaluation timeframes,
indicating its potential to cater to evolving market trends and user
preferences. Our findings suggest that GINs represent a promising direction for
CMRs, paving the way for more sophisticated, personalized, and context-aware
recommendation systems in the dynamic landscape of global e-commerce.

æè¦ï¼å¨å¨çååæ¥­çç¶ä»ä¸çä¸­ï¼è·¨å¸å ´æ¨è¦ç³»çµ± (CMR) å°æ¼å¨ååä¸åçå¸å ´åéä¸­æä¾åäººåçä½¿ç¨èé«é©è³ééè¦ãç¶èï¼å³çµ±çæ¨è¦æ¼ç®æ³å¨èçå¸å ´ç¹æ®æ§åè³æç¨çæ§æ¹é¢æå°é£ï¼ç¹å¥æ¯å¨æ°èæéç¼ä¸­å¸å ´ãå¨æ¬æä¸­ï¼æåæåº CrossGR æ¨¡åï¼å®å©ç¨ååæ§ç¶²è·¯ (GIN) ä¾æ¹å CMR ç³»çµ±ãå®å¨ NDCG@10 å HR@10 ææ¨ä¸åªæ¼ç¾æçåºæºï¼è­æäºå®å¨èçä¸åå¸å ´åéæçé©ææ§åæºç¢ºæ§ãCrossGR æ¨¡åå·æé©ææ§åæºç¢ºæ§ï¼ä½¿å¶éå¸¸é©åèçè·¨å¸å ´æ¨è¦ä»»åçè¤éæ§ãå®çç©©å¥æ§å¨ä¸åçè©ä¼°æéç¯åå§è¡¨ç¾åºä¸è´çæè½ï¼è¡¨æå®ææ½åæ»¿è¶³ä¸æ·è®åçå¸å ´è¶¨å¢åä½¿ç¨èåå¥½ãæåçç ç©¶çµæè¡¨æï¼GIN ä»£è¡¨äº CMR çä¸åæå¸æçæ¹åï¼çºå¨å¨çé»å­ååçåæç°å¢ä¸­å»ºç«æ´ç²¾ç·»ãåæ§ååå·åæå¢æç¥çæ¨è¦ç³»çµ±éªå¹³äºéè·¯ã

##### **FPMT: Enhanced Semi-Supervised Model for Traffic Incident Detection**
2409.07839v1 by Xinying Lu, Jianli Xiao

For traffic incident detection, the acquisition of data and labels is notably
resource-intensive, rendering semi-supervised traffic incident detection both a
formidable and consequential challenge. Thus, this paper focuses on traffic
incident detection with a semi-supervised learning way. It proposes a
semi-supervised learning model named FPMT within the framework of MixText. The
data augmentation module introduces Generative Adversarial Networks to balance
and expand the dataset. During the mix-up process in the hidden space, it
employs a probabilistic pseudo-mixing mechanism to enhance regularization and
elevate model precision. In terms of training strategy, it initiates with
unsupervised training on all data, followed by supervised fine-tuning on a
subset of labeled data, and ultimately completing the goal of semi-supervised
training. Through empirical validation on four authentic datasets, our FPMT
model exhibits outstanding performance across various metrics. Particularly
noteworthy is its robust performance even in scenarios with low label rates.

æè¦ï¼å°æ¼äº¤éäºä»¶åµæ¸¬ï¼è³æèæ¨ç±¤çåå¾é¡¯èå°èè²»è³æºï¼éä½¿å¾åç£ç£å¼äº¤éäºä»¶åµæ¸¬æçºä¸é æ¢è±éä¸éè¦çææ°ãå æ­¤ï¼æ¬æå°æ³¨æ¼æ¡ç¨åç£ç£å¼å­¸ç¿æ¹å¼çäº¤éäºä»¶åµæ¸¬ãå®å¨ MixText æ¡æ¶å§æåºä¸ååçº FPMT çåç£ç£å¼å­¸ç¿æ¨¡åãè³ææ´åæ¨¡çµå¼å¥äºçæå°æç¶²è·¯ï¼ä»¥å¹³è¡¡ä¸¦æ´åè³æéãå¨é±èç©ºéç mix-up ç¨åºä¸­ï¼å®æ¡ç¨æ©çå½æ··åæ©å¶ä¾å¢å¼·æ­£ååä¸¦æåæ¨¡åç²¾ç¢ºåº¦ãå¨è¨ç·´ç­ç¥æ¹é¢ï¼å®å¾ææè³æçç¡ç£ç£å¼è¨ç·´éå§ï¼æ¥èå¨æ¨ç±¤è³æçå­éä¸­é²è¡ç£ç£å¼å¾®èª¿ï¼æå¾å®æåç£ç£å¼è¨ç·´çç®æ¨ãééå¨ååçå¯¦è³æéä¸çå¯¦è­é©è­ï¼æåç FPMT æ¨¡åå±ç¾åºè·¨è¶åç¨®ææ¨çååºæè½ãç¹å¥å¼å¾æ³¨æçæ¯ï¼å³ä½¿å¨æ¨ç±¤çä½çææ³ä¸ï¼å®ä»æç©©å¥çæè½ã

##### **A Comprehensive Survey on Deep Multimodal Learning with Missing Modality**
2409.07825v2 by Renjie Wu, Hu Wang, Hsiang-Ting Chen

During multimodal model training and reasoning, data samples may miss certain
modalities and lead to compromised model performance due to sensor limitations,
cost constraints, privacy concerns, data loss, and temporal and spatial
factors. This survey provides an overview of recent progress in Multimodal
Learning with Missing Modality (MLMM), focusing on deep learning techniques. It
is the first comprehensive survey that covers the historical background and the
distinction between MLMM and standard multimodal learning setups, followed by a
detailed analysis of current MLMM methods, applications, and datasets,
concluding with a discussion about challenges and potential future directions
in the field.

æè¦ï¼å¨å¤æ¨¡ææ¨¡åè®­ç»åæ¨çæé´ï¼æ°æ®æ ·æ¬å¯è½ä¼ç¼ºå°æäºæ¨¡æï¼å¹¶ä¸ç±äºä¼ æå¨éå¶ãææ¬éå¶ãéç§é®é¢ãæ°æ®ä¸¢å¤±ä»¥åæ¶é´åç©ºé´å ç´ èå¯¼è´æ¨¡åæ§è½åæãæ¬è°æ¥æ¦è¿°äºç¼ºå¤±æ¨¡æå¤æ¨¡æå­¦ä¹  (MLMM) çææ°è¿å±ï¼éç¹æ¯æ·±åº¦å­¦ä¹ ææ¯ãè¿æ¯ç¬¬ä¸ä»½æ¶µçåå²èæ¯å MLMM ä¸æ åå¤æ¨¡æå­¦ä¹ è®¾ç½®ä¹é´åºå«çç»¼åè°æ¥ï¼éåè¯¦ç»åæäºå½åç MLMM æ¹æ³ãåºç¨ç¨åºåæ°æ®éï¼æåè®¨è®ºäºè¯¥é¢åçææåæ½å¨æªæ¥æ¹åã

##### **Online vs Offline: A Comparative Study of First-Party and Third-Party Evaluations of Social Chatbots**
2409.07823v1 by Ekaterina Svikhnushina, Pearl Pu

This paper explores the efficacy of online versus offline evaluation methods
in assessing conversational chatbots, specifically comparing first-party direct
interactions with third-party observational assessments. By extending a
benchmarking dataset of user dialogs with empathetic chatbots with offline
third-party evaluations, we present a systematic comparison between the
feedback from online interactions and the more detached offline third-party
evaluations. Our results reveal that offline human evaluations fail to capture
the subtleties of human-chatbot interactions as effectively as online
assessments. In comparison, automated third-party evaluations using a GPT-4
model offer a better approximation of first-party human judgments given
detailed instructions. This study highlights the limitations of third-party
evaluations in grasping the complexities of user experiences and advocates for
the integration of direct interaction feedback in conversational AI evaluation
to enhance system development and user satisfaction.

æè¦ï¼æ¬ææ¢è¨ç·ä¸èç·ä¸è©ä¼°æ¹æ³å¨è©ä¼°å°è©±å¼èå¤©æ©å¨äººæçæè½ï¼ç¹å¥æ¯æ¯è¼ç¬¬ä¸æ¹ç´æ¥äºåèç¬¬ä¸æ¹è§å¯è©ä¼°ãééæ´åä½¿ç¨èå°è©±çåºåè³æéï¼ä¸¦å å¥é¢ç·çç¬¬ä¸æ¹è©ä¼°ï¼æåå°ç·ä¸äºåçåé¥èè¼çºæ½é¢çé¢ç·ç¬¬ä¸æ¹è©ä¼°é²è¡ç³»çµ±æ§çæ¯è¼ãæåççµæé¡¯ç¤ºï¼èç·ä¸è©ä¼°ç¸æ¯ï¼é¢ç·çäººå·¥è©ä¼°æªè½ææææäººèèå¤©æ©å¨äººäºåçç´°å¾®å·®å¥ãç¸è¼ä¹ä¸ï¼ä½¿ç¨ GPT-4 æ¨¡åçèªååç¬¬ä¸æ¹è©ä¼°ï¼å¨çµ¦äºè©³ç´°èªªæçææ³ä¸ï¼è½æä¾è¼ä½³çè¿ä¼¼å¼ï¼ä»¥å¤æ·ç¬¬ä¸æ¹çäººé¡å¤æ·ãæ¬ç ç©¶å¼·èª¿äºç¬¬ä¸æ¹è©ä¼°å¨ææ¡ä½¿ç¨èé«é©çè¤éæ§æ¹é¢çéå¶ï¼ä¸¦ä¸»å¼µå¨å°è©±å¼ AI è©ä¼°ä¸­æ´åç´æ¥äºååé¥ï¼ä»¥å¢å¼·ç³»çµ±éç¼åä½¿ç¨èæ»¿æåº¦ã

##### **Controllable Synthetic Clinical Note Generation with Privacy Guarantees**
2409.07809v1 by Tal Baumel, Andre Manoel, Daniel Jones, Shize Su, Huseyin Inan, Aaron, Bornstein, Robert Sim

In the field of machine learning, domain-specific annotated data is an
invaluable resource for training effective models. However, in the medical
domain, this data often includes Personal Health Information (PHI), raising
significant privacy concerns. The stringent regulations surrounding PHI limit
the availability and sharing of medical datasets, which poses a substantial
challenge for researchers and practitioners aiming to develop advanced machine
learning models. In this paper, we introduce a novel method to "clone" datasets
containing PHI. Our approach ensures that the cloned datasets retain the
essential characteristics and utility of the original data without compromising
patient privacy. By leveraging differential-privacy techniques and a novel
fine-tuning task, our method produces datasets that are free from identifiable
information while preserving the statistical properties necessary for model
training. We conduct utility testing to evaluate the performance of machine
learning models trained on the cloned datasets. The results demonstrate that
our cloned datasets not only uphold privacy standards but also enhance model
performance compared to those trained on traditional anonymized datasets. This
work offers a viable solution for the ethical and effective utilization of
sensitive medical data in machine learning, facilitating progress in medical
research and the development of robust predictive models.

æè¦ï¼å¨æ©å¨å­¸ç¿é åä¸­ï¼ç¹å®é åçè¨»è§£è³ææ¯è¨ç·´æææ¨¡åçå¯¶è²´è³æºãç¶èï¼å¨é«å­¸é åä¸­ï¼æ­¤é¡è³æç¶å¸¸åå«åäººå¥åº·è³è¨ (PHI)ï¼å¼ç¼éå¤§çé±ç§åé¡ãåç¹ PHI çå´æ ¼æ³è¦éå¶äºé«çè³æéçå¯ç¨æ§åå±äº«ï¼éå°æ¨å¨éç¼é²éæ©å¨å­¸ç¿æ¨¡åçç ç©¶äººå¡åå¾æ¥­èæ§æéå¤§ææ°ãå¨æ¬æä¸­ï¼æåä»ç´¹äºä¸ç¨®ãè¤è£½ãåå« PHI çè³æéçæ°æ¹æ³ãæåçåæ³ç¢ºä¿è¤è£½çè³æéä¿çåå§è³æçåºæ¬ç¹å¾µåæç¨ï¼åæä¸æå®³æ£èé±ç§ãééå©ç¨å·®åé±ç§æè¡åæ°çå¾®èª¿ä»»åï¼æåçåæ³ç¢çäºç¡å¯è¾¨è­è³è¨çè³æéï¼åæä¿çäºæ¨¡åè¨ç·´æéççµ±è¨ç¹æ§ãæåé²è¡æç¨æ¸¬è©¦ä»¥è©ä¼°å¨è¤è£½è³æéä¸è¨ç·´çæ©å¨å­¸ç¿æ¨¡åçæè½ãçµæè¡¨æï¼æåè¤è£½çè³æéä¸åç¬¦åé±ç§æ¨æºï¼èä¸èå¨å³çµ±å¿ååè³æéä¸è¨ç·´çè³æéç¸æ¯ï¼éè½æåæ¨¡åæè½ãéé å·¥ä½çºæ©å¨å­¸ç¿ä¸­ææé«çè³æçéå¾·åææå©ç¨æä¾äºå¯è¡çè§£æ±ºæ¹æ¡ï¼ä¿é²äºé«å­¸ç ç©¶åç©©å¥é æ¸¬æ¨¡åçç¼å±ã

##### **In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for Efficient Adaptation**
2409.07796v1 by Mohammad Mehdi Rastikerdar, Jin Huang, Hui Guan, Deepak Ganesan

Wildlife monitoring via camera traps has become an essential tool in ecology,
but the deployment of machine learning models for on-device animal
classification faces significant challenges due to domain shifts and resource
constraints. This paper introduces WildFit, a novel approach that reconciles
the conflicting goals of achieving high domain generalization performance and
ensuring efficient inference for camera trap applications. WildFit leverages
continuous background-aware model fine-tuning to deploy ML models tailored to
the current location and time window, allowing it to maintain robust
classification accuracy in the new environment without requiring significant
computational resources. This is achieved by background-aware data synthesis,
which generates training images representing the new domain by blending
background images with animal images from the source domain. We further enhance
fine-tuning effectiveness through background drift detection and class
distribution drift detection, which optimize the quality of synthesized data
and improve generalization performance. Our extensive evaluation across
multiple camera trap datasets demonstrates that WildFit achieves significant
improvements in classification accuracy and computational efficiency compared
to traditional approaches.

æè¦ï¼éçåç©ç£æ¸¬ééç¸æ©é·é±å·²æçºçæå­¸ä¸­ä¸å¯æç¼ºçå·¥å·ï¼
ä½æ©å¨å­¸ç¿æ¨¡åå¨è£ç½®ä¸é²è¡åç©åé¡çé¨ç½²é¢è¨äºé åè½ç§»åè³æºéå¶çéå¤§ææ°ãæ¬æä»ç´¹äº WildFitï¼éæ¯ä¸ç¨®æ°ç©çæ¹æ³ï¼å®èª¿åäºå¯¦ç¾é«é åæ³åæè½åç¢ºä¿ç¸æ©é·é±æç¨ç¨å¼æææ¨è«çè¡çªç®æ¨ãWildFit å©ç¨æçºçèæ¯æç¥æ¨¡åå¾®èª¿ä¾é¨ç½²éå°ç¶åä½ç½®åæéè¦çªéèº«æé ç ML æ¨¡åï¼ä½¿å¶è½å¤ å¨æ°çç°å¢ä¸­ç¶­æç©©å¥çåé¡æºç¢ºæ§ï¼èç¡éå¤§éçéç®è³æºãéééèæ¯æç¥è³æåæä¾éæï¼å®ééå°èæ¯å½±åèä¾èªåå§é åçåç©å½±åæ··åï¼ç¢çä»£è¡¨æ°é åçè¨ç·´å½±åãæåééèæ¯æ¼ç§»åµæ¸¬åé¡å¥åä½æ¼ç§»åµæ¸¬é²ä¸æ­¥æåå¾®èª¿çæææ§ï¼å®åæä½³åäºåæè³æçåè³ªä¸¦æåäºæ³åæè½ãæåå¨å¤åç¸æ©é·é±è³æéä¸çå»£æ³è©ä¼°é¡¯ç¤ºï¼èå³çµ±æ¹æ³ç¸æ¯ï¼WildFit å¨åé¡æºç¢ºæ§åéç®æçæ¹é¢é½æé¡¯èçæåã

##### **Full-text Error Correction for Chinese Speech Recognition with Large Language Model**
2409.07790v1 by Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang

Large Language Models (LLMs) have demonstrated substantial potential for
error correction in Automatic Speech Recognition (ASR). However, most research
focuses on utterances from short-duration speech recordings, which are the
predominant form of speech data for supervised ASR training. This paper
investigates the effectiveness of LLMs for error correction in full-text
generated by ASR systems from longer speech recordings, such as transcripts
from podcasts, news broadcasts, and meetings. First, we develop a Chinese
dataset for full-text error correction, named ChFT, utilizing a pipeline that
involves text-to-speech synthesis, ASR, and error-correction pair extractor.
This dataset enables us to correct errors across contexts, including both
full-text and segment, and to address a broader range of error types, such as
punctuation restoration and inverse text normalization, thus making the
correction process comprehensive. Second, we fine-tune a pre-trained LLM on the
constructed dataset using a diverse set of prompts and target formats, and
evaluate its performance on full-text error correction. Specifically, we design
prompts based on full-text and segment, considering various output formats,
such as directly corrected text and JSON-based error-correction pairs. Through
various test settings, including homogeneous, up-to-date, and hard test sets,
we find that the fine-tuned LLMs perform well in the full-text setting with
different prompts, each presenting its own strengths and weaknesses. This
establishes a promising baseline for further research. The dataset is available
on the website.

æè¦ï¼<paragraph>å¤§åèªè¨æ¨¡å (LLM) å·²å±ç¾åºå¨èªåèªé³è¾¨è­ (ASR) ä¸­é²è¡é¯èª¤æ ¡æ­£çå·¨å¤§æ½åãç¶èï¼å¤§å¤æ¸ç ç©¶é½èéæ¼ç­æèªé³éé³ä¸­çèªå¥ï¼èéæ¯ç£ç£å¼ ASR è¨ç·´ä¸­ä¸»è¦çèªé³è³æå½¢å¼ãæ¬ææ¢è¨äº LLM å¨ ASR ç³»çµ±ç¢ççé·ç¯èªé³éé³å¨æå­é¯èª¤æ ¡æ­£ä¸­çæææ§ï¼ä¾å¦æ­å®¢ãæ°èå»£æ­åæè­°çéå­ç¨¿ãé¦åï¼æåéç¼äºä¸ååçº ChFT çä¸­æå¨æå­é¯èª¤æ ¡æ­£è³æéï¼å©ç¨äºä¸ååå«æå­è½èªé³åæãASR åé¯èª¤æ ¡æ­£éå°èåå¨çç®¡éãéåè³æéè®æåè½å¤ è·¨èçµ¡ï¼åæ¬å¨æå­åçæ®µï¼æ ¡æ­£é¯èª¤ï¼ä¸¦è§£æ±ºæ´å»£æ³çé¯èª¤é¡åï¼ä¾å¦æ¨é»ç¬¦èéååååæå­æ­£è¦åï¼å¾èä½¿æ ¡æ­£éç¨æ´å¨é¢ãå¶æ¬¡ï¼æåä½¿ç¨ä¸å¥ä¸åçæç¤ºåç®æ¨æ ¼å¼ï¼éå°å»ºæ§çè³æéå¾®èª¿ä¸åé åè¨ç·´ç LLMï¼ä¸¦è©ä¼°å¶å¨å¨æå­é¯èª¤æ ¡æ­£ä¸­çè¡¨ç¾ãå·é«ä¾èªªï¼æåæ ¹æå¨æå­åçæ®µè¨­è¨æç¤ºï¼èéåç¨®è¼¸åºæ ¼å¼ï¼ä¾å¦ç´æ¥æ ¡æ­£çæå­ååºæ¼ JSON çé¯èª¤æ ¡æ­£éå°ãééåç¨®æ¸¬è©¦è¨­å®ï¼åæ¬åè³ªãææ°åå°é£çæ¸¬è©¦éï¼æåç¼ç¾å¾®èª¿å¾ç LLM å¨å¨æå­è¨­å®ä¸­è¡¨ç¾è¯å¥½ï¼ä¸¦æ¡ç¨ä¸åçæç¤ºï¼æ¯åæç¤ºé½å±ç¾åºèªå·±çåªå¢åå£å¢ãéçºé²ä¸æ­¥çç ç©¶å»ºç«äºä¸åæåæ¯çåºæºãè©²è³æéå¯å¨ç¶²ç«ä¸åå¾ã</paragraph>

##### **Stable Language Model Pre-training by Reducing Embedding Variability**
2409.07787v1 by Woojin Chung, Jiwoo Hong, Na Min An, James Thorne, Se-Young Yun

Stable pre-training is essential for achieving better-performing language
models. However, tracking pre-training stability by calculating gradient
variance at every step is impractical due to the significant computational
costs. We explore Token Embedding Variability (TEV) as a simple and efficient
proxy for assessing pre-training stability in language models with pre-layer
normalization, given that shallower layers are more prone to gradient explosion
(section 2.2). Moreover, we propose Multi-head Low-Rank Attention (MLRA) as an
architecture to alleviate such instability by limiting the exponential growth
of output embedding variance, thereby preventing the gradient explosion
(section 3.2). Empirical results on GPT-2 with MLRA demonstrate increased
stability and lower perplexity, particularly in deeper models.

æè¦ï¼ç©©å®çé è¨ç·´å°æ¼éææè½æ´å¥½çèªè¨æ¨¡åè³ééè¦ãç¶èï¼ç±æ¼é¡¯èçéç®ææ¬ï¼ééè¨ç®æ¯åæ­¥é©çæ¢¯åº¦è®ç°æ¸ä¾è¿½è¹¤é è¨ç·´ç©©å®æ§ä¸¦ä¸å¯¦éãæåæ¢è¨ Token Embedding Variability (TEV) ä½çºä¸åç°¡å®ä¸ææçæ¹æ³ï¼ç¨æ¼è©ä¼°å·æé å±¤æ¨æºåçèªè¨æ¨¡åä¸­çé è¨ç·´ç©©å®æ§ï¼å çºè¼æ·ºçå±¤æ´å®¹æç¼çæ¢¯åº¦çç¸ï¼ç¬¬ 2.2 ç¯ï¼ãæ­¤å¤ï¼æåæåºå¤é ­ä½ç§©æ³¨æå (MLRA) ä½çºä¸ç¨®æ¶æ§ï¼éééå¶è¼¸åºåµå¥è®ç°æ¸çææ¸æé·ä¾æ¸è¼éç¨®ä¸ç©©å®æ§ï¼å¾èé²æ­¢æ¢¯åº¦çç¸ï¼ç¬¬ 3.2 ç¯ï¼ãå¨ GPT-2 ä¸ä½¿ç¨ MLRA çç¶é©çµæè­æäºå¢å çç©©å®æ§åè¼ä½çå°æåº¦ï¼ç¹å¥æ¯å¨è¼æ·±çæ¨¡åä¸­ã

##### **ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**
2409.07779v1 by Fuchen Zheng, Xinyi Chen, Xuhang Chen, Haolun Li, Xiaojiao Guo, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a crucial task in computer vision, facilitates
the automated delineation of anatomical structures and pathologies, supporting
clinicians in diagnosis, treatment planning, and disease monitoring. Notably,
transformers employing shifted window-based self-attention have demonstrated
exceptional performance. However, their reliance on local window attention
limits the fusion of local and global contextual information, crucial for
segmenting microtumors and miniature organs. To address this limitation, we
propose the Adaptive Semantic Segmentation Network (ASSNet), a transformer
architecture that effectively integrates local and global features for precise
medical image segmentation. ASSNet comprises a transformer-based U-shaped
encoder-decoder network. The encoder utilizes shifted window self-attention
across five resolutions to extract multi-scale features, which are then
propagated to the decoder through skip connections. We introduce an augmented
multi-layer perceptron within the encoder to explicitly model long-range
dependencies during feature extraction. Recognizing the constraints of
conventional symmetrical encoder-decoder designs, we propose an Adaptive
Feature Fusion (AFF) decoder to complement our encoder. This decoder
incorporates three key components: the Long Range Dependencies (LRD) block, the
Multi-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)
block. These components synergistically facilitate the effective fusion of
multi-scale features extracted by the decoder while capturing long-range
dependencies and refining object boundaries. Comprehensive experiments on
diverse medical image segmentation tasks, including multi-organ, liver tumor,
and bladder tumor segmentation, demonstrate that ASSNet achieves
state-of-the-art results. Code and models are available at:
\url{https://github.com/lzeeorno/ASSNet}.

æè¦ï¼<paragraph>é«å­¸å½±ååå²æ¯é»è¦è¦è¦ºä¸­ä¸é éè¦çä»»åï¼æå©æ¼èªåæç¹ªè§£åçµæ§åççï¼åå©è¨åºé«å¸«é²è¡è¨ºæ·ãæ²»çè¨ç«åç¾çç£æ§ãå¼å¾æ³¨æçæ¯ï¼æ¡ç¨ä½ç§»è¦çªèªæ³¨æåæ©å¶çTransformerå±ç¾åºéå¡çæè½ãç¶èï¼å®åä¾è³´æ¼ååè¦çªæ³¨æåï¼ééå¶äºåååå¨åèçµ¡è³è¨çèåï¼èéå°æ¼åå²å¾®å°è«ç¤åå¾®åå¨å®è³ééè¦ãçºäºè§£æ±ºéåéå¶ï¼æåæåºäºèªé©æèªç¾©åå²ç¶²è·¯ (ASSNet)ï¼éæ¯ä¸åTransformeræ¶æ§ï¼å¯ä»¥æææ´ååååå¨åç¹å¾µï¼ä»¥é²è¡ç²¾ç¢ºçé«å­¸å½±ååå²ãASSNet åå«ä¸ååºæ¼Transformerç U åç·¨ç¢¼å¨-è§£ç¢¼å¨ç¶²è·¯ãç·¨ç¢¼å¨å©ç¨äºåè§£æåº¦çä½ç§»è¦çªèªæ³¨æåä¾èåå¤å°ºåº¦ç¹å¾µï¼ç¶å¾ééè·³èºé£ç·å°éäºç¹å¾µå³æ­å°è§£ç¢¼å¨ãæåå¨ç·¨ç¢¼å¨ä¸­å¼å¥äºæ´å¢çå¤å±¤æç¥å¨ï¼ä»¥ä¾¿å¨ç¹å¾µèåæéæç¢ºå°å»ºæ¨¡é·ç¨ä¾è³´æ§ãéæ¼å³çµ±å°ç¨±ç·¨ç¢¼å¨-è§£ç¢¼å¨è¨­è¨çéå¶ï¼æåæåºäºä¸åèªé©æç¹å¾µèå (AFF) è§£ç¢¼å¨ä¾è£åæåçç·¨ç¢¼å¨ãæ­¤è§£ç¢¼å¨åå«ä¸åééµçµæé¨åï¼é·ç¨ä¾è³´æ§ (LRD) åå¡ãå¤å°ºåº¦ç¹å¾µèå (MFF) åå¡åèªé©æèªç¾©ä¸­å¿ (ASC) åå¡ãéäºçµæé¨åç¸äºéåï¼ä¿æè§£ç¢¼å¨èåçå¤å°ºåº¦ç¹å¾µææèåï¼åæææé·ç¨ä¾è³´æ§ä¸¦å¾®èª¿ç©ä»¶éçãå¨å¤å¨å®ãèèè«ç¤åèè±è«ç¤åå²ç­åç¨®é«å­¸å½±ååå²ä»»åä¸çå¨é¢å¯¦é©è­æï¼ASSNet éå°äºæåé²çææãç¨å¼ç¢¼åæ¨¡åå¯æ¼ä»¥ä¸ç¶²ååå¾ï¼\url{https://github.com/lzeeorno/ASSNet}ã</paragraph>

##### **Training Spiking Neural Networks via Augmented Direct Feedback Alignment**
2409.07776v1 by Yongbo Zhang, Katsuma Inoue, Mitsumasa Nakajima, Toshikazu Hashimoto, Yasuo Kuniyoshi, Kohei Nakajima

Spiking neural networks (SNNs), the models inspired by the mechanisms of real
neurons in the brain, transmit and represent information by employing discrete
action potentials or spikes. The sparse, asynchronous properties of information
processing make SNNs highly energy efficient, leading to SNNs being promising
solutions for implementing neural networks in neuromorphic devices. However,
the nondifferentiable nature of SNN neurons makes it a challenge to train them.
The current training methods of SNNs that are based on error backpropagation
(BP) and precisely designing surrogate gradient are difficult to implement and
biologically implausible, hindering the implementation of SNNs on neuromorphic
devices. Thus, it is important to train SNNs with a method that is both
physically implementatable and biologically plausible. In this paper, we
propose using augmented direct feedback alignment (aDFA), a gradient-free
approach based on random projection, to train SNNs. This method requires only
partial information of the forward process during training, so it is easy to
implement and biologically plausible. We systematically demonstrate the
feasibility of the proposed aDFA-SNNs scheme, propose its effective working
range, and analyze its well-performing settings by employing genetic algorithm.
We also analyze the impact of crucial features of SNNs on the scheme, thus
demonstrating its superiority and stability over BP and conventional direct
feedback alignment. Our scheme can achieve competitive performance without
accurate prior knowledge about the utilized system, thus providing a valuable
reference for physically training SNNs.

æè¦ï¼å°å³°ç¥ç»ç¶²è·¯ï¼SNNï¼æ¯åå¤§è¦ä¸­çå¯¦ç¥ç¶åæ©å¶åç¼çæ¨¡åï¼å®ééä½¿ç¨é¢æ£åä½é»ä½æå°å³°ä¾å³è¼¸åè¡¨ç¤ºè³è¨ãè³è¨èççç¨çæ§åéåæ­¥æ§ä½¿ SNN å·æé«åº¦çè½æºæçï¼éä½¿å¾ SNN æçºå¨ç¥ç¶å½¢æè£ç½®ä¸­å¯¦ä½ç¥ç¶ç¶²è·¯çæåéçè§£æ±ºæ¹æ¡ãç¶èï¼SNN ç¥ç¶åçä¸å¯å¾®åæ§è³ªä½¿å¾è¨ç·´å®åæçºä¸é ææ°ãç®ååºæ¼èª¤å·®ååå³æ­ï¼BPï¼åç²¾ç¢ºè¨­è¨æ¿ä»£æ¢¯åº¦ç SNN è¨ç·´æ¹æ³é£ä»¥å¯¦ä½ä¸å¨çç©å­¸ä¸é£ä»¥å¯¦ç¾ï¼éé»ç¤äº SNN å¨ç¥ç¶å½¢æè£ç½®ä¸çå¯¦ä½ãå æ­¤ï¼ä½¿ç¨ä¸ç¨®å¨ç©çä¸å¯å¯¦ä½ä¸å¨çç©å­¸ä¸åççè¨ç·´ SNN çæ¹æ³éå¸¸éè¦ãå¨æ¬æä¸­ï¼æåå»ºè­°ä½¿ç¨åºæ¼é¨æ©æå½±çç¡æ¢¯åº¦æ¹æ³æ´å¢ç´æ¥åé¥å°é½ï¼aDFAï¼ä¾è¨ç·´ SNNãæ­¤æ¹æ³å¨è¨ç·´éç¨ä¸­åéè¦ååéç¨çé¨åè³è¨ï¼å æ­¤ææ¼å¯¦ä½ä¸å¨çç©å­¸ä¸åçãæåç³»çµ±æ§å°è­æäºææåºç aDFA-SNN æ¶æ§çå¯è¡æ§ï¼æåºå¶ææçéä½ç¯åï¼ä¸¦ééæ¡ç¨éºå³æ¼ç®æ³åæå¶æè½è¯å¥½çè¨­å®ãæåéåæäº SNN çééµç¹å¾µå°æ­¤æ¶æ§çå½±é¿ï¼å¾èè­æäºå®åªæ¼ BP åå³çµ±ç´æ¥åé¥å°é½ï¼ä¸ç©©å®æ§ä¹è¼ä½³ãæåçæ¶æ§å¯ä»¥å¨æ²æéæ¼æä½¿ç¨ç³»çµ±çæºç¢ºåé©ç¥è­çææ³ä¸ï¼éå°å·æç«¶ç­åçæè½ï¼å¾èçºç©çè¨ç·´ SNN æä¾äºå¯¶è²´çåèã

##### **Universal Pooling Method of Multi-layer Features from Pretrained Models for Speaker Verification**
2409.07770v1 by Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han

Recent advancements in automatic speaker verification (ASV) studies have been
achieved by leveraging large-scale pretrained networks. In this study, we
analyze the approaches toward such a paradigm and underline the significance of
interlayer information processing as a result. Accordingly, we present a novel
approach for exploiting the multilayered nature of pretrained models for ASV,
which comprises a layer/frame-level network and two steps of pooling
architectures for each layer and frame axis. Specifically, we let convolutional
architecture directly processes a stack of layer outputs.Then, we present a
channel attention-based scheme of gauging layer significance and squeeze the
layer level with the most representative value. Finally, attentive statistics
over frame-level representations yield a single vector speaker embedding.
Comparative experiments are designed using versatile data environments and
diverse pretraining models to validate the proposed approach. The experimental
results demonstrate the stability of the approach using multi-layer outputs in
leveraging pretrained architectures. Then, we verify the superiority of the
proposed ASV backend structure, which involves layer-wise operations, in terms
of performance improvement along with cost efficiency compared to the
conventional method. The ablation study shows how the proposed interlayer
processing aids in maximizing the advantage of utilizing pretrained models.

æè¦ï¼æè¿å¨èªåèªé³é©è­ (ASV) ç ç©¶ä¸­çé²å±ï¼æ¯ééå©ç¨å¤§è¦æ¨¡é è¨ç·´ç¶²è·¯æéæãå¨æ¬ç ç©¶ä¸­ï¼æååæäºæ­¤ç¯ä¾çæ¹æ³ï¼ä¸¦å¼·èª¿äºå±¤éè³è¨èççéè¦æ§ãå æ­¤ï¼æåæåºä¸åæ°çæ¹æ³ï¼ç¨æ¼å©ç¨é è¨ç·´æ¨¡åçå¤å±¤ç¹æ§ä¾é²è¡ ASVï¼å¶ä¸­åå«ä¸åå±¤ç´/å¹ç´ç¶²è·¯ï¼ä»¥åéå°æ¯åå±¤ç´åå¹è»¸çå©åæ± åæ¶æ§æ­¥é©ãå·é«ä¾èªªï¼æåè®å·ç©æ¶æ§ç´æ¥èçä¸çå±¤ç´è¼¸åºãç¶å¾ï¼æåæåºä¸ååºæ¼ééæ³¨æåçæ¹æ¡ï¼ç¨æ¼è©ä¼°å±¤ç´éè¦æ§ï¼ä¸¦ä½¿ç¨æå·ä»£è¡¨æ§çå¼å£ç¸®å±¤ç´ãæå¾ï¼å°å¹ç´è¡¨ç¤ºçéæ³¨çµ±è¨æç¢çä¸åå®ä¸çåéèªªè©±èåµå¥ãæåä½¿ç¨å¤åè½è³æç°å¢åä¸åçé è¨ç·´æ¨¡åè¨­è¨äºæ¯è¼å¯¦é©ï¼ä»¥é©è­ææåºçæ¹æ³ãå¯¦é©çµæè­æäºè©²æ¹æ³ä½¿ç¨å¤å±¤è¼¸åºå¨å©ç¨é è¨ç·´æ¶æ§æçç©©å®æ§ãç¶å¾ï¼æåé©è­äºææåºç ASV å¾ç«¯çµæ§çåªè¶æ§ï¼å¶ä¸­æ¶åå±¤ç´éç®ï¼å¨æè½æååææ¬æçæ¹é¢ï¼åªæ¼å³çµ±æ¹æ³ãæ¶èç ç©¶é¡¯ç¤ºï¼ææåºçå±¤éèçå¦ä½æå©æ¼æå¤§åå©ç¨é è¨ç·´æ¨¡åçåªå¢ã

##### **Reimagining Linear Probing: Kolmogorov-Arnold Networks in Transfer Learning**
2409.07763v1 by Sheng Shen, Rabih Younes

This paper introduces Kolmogorov-Arnold Networks (KAN) as an enhancement to
the traditional linear probing method in transfer learning. Linear probing,
often applied to the final layer of pre-trained models, is limited by its
inability to model complex relationships in data. To address this, we propose
substituting the linear probing layer with KAN, which leverages spline-based
representations to approximate intricate functions. In this study, we integrate
KAN with a ResNet-50 model pre-trained on ImageNet and evaluate its performance
on the CIFAR-10 dataset. We perform a systematic hyperparameter search,
focusing on grid size and spline degree (k), to optimize KAN's flexibility and
accuracy. Our results demonstrate that KAN consistently outperforms traditional
linear probing, achieving significant improvements in accuracy and
generalization across a range of configurations. These findings indicate that
KAN offers a more powerful and adaptable alternative to conventional linear
probing techniques in transfer learning.

æè¦ï¼æ¬æä»ç´¹äº Kolmogorov-Arnold ç¶²è·¯ (KAN)ï¼ä½çºè½ç§»å­¸ç¿ä¸­å³çµ±ç·æ§æ¢æ¸¬æ¹æ³çå¼·åãç·æ§æ¢æ¸¬éå¸¸æç¨æ¼é è¨ç·´æ¨¡åçæå¾ä¸å±¤ï¼ä½å¶å»ºæ¨¡è³æä¸­è¤ééä¿çè½åæéãçºäºè§£æ±ºéååé¡ï¼æåå»ºè­°ç¨ KAN åä»£ç·æ§æ¢æ¸¬å±¤ï¼KAN å©ç¨åºæ¼æ¨£æ¢ç·çè¡¨ç¤ºä¾è¿ä¼¼è¤éå½æ¸ãå¨éé ç ç©¶ä¸­ï¼æåå° KAN èå¨ ImageNet ä¸é è¨ç·´ç ResNet-50 æ¨¡åæ´åï¼ä¸¦å¨ CIFAR-10 è³æéä¸è©ä¼°å¶æè½ãæåå·è¡ç³»çµ±æ§çè¶åæ¸æå°ï¼å°æ³¨æ¼ç¶²æ ¼å¤§å°åæ¨£æ¢ç·æ¬¡æ¸ (k)ï¼ä»¥æä½³å KAN çå½æ§åæºç¢ºåº¦ãæåççµæè­æï¼KAN æçºåªæ¼å³çµ±ç·æ§æ¢æ¸¬ï¼å¨åç¨®çµæä¸­é½é¡¯èæåæºç¢ºåº¦åæ³åè½åãéäºç¼ç¾è¡¨æï¼å¨è½ç§»å­¸ç¿ä¸­ï¼KAN æä¾äºä¸åæ¯å³çµ±ç·æ§æ¢æ¸¬æè¡æ´å¼·å¤§ãæ´é©ææ§çæ¿ä»£æ¹æ¡ã

##### **Top-down Activity Representation Learning for Video Question Answering**
2409.07748v1 by Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Vizcarra, Mori Kurokawa

Capturing complex hierarchical human activities, from atomic actions (e.g.,
picking up one present, moving to the sofa, unwrapping the present) to
contextual events (e.g., celebrating Christmas) is crucial for achieving
high-performance video question answering (VideoQA). Recent works have expanded
multimodal models (e.g., CLIP, LLaVA) to process continuous video sequences,
enhancing the model's temporal reasoning capabilities. However, these
approaches often fail to capture contextual events that can be decomposed into
multiple atomic actions non-continuously distributed over relatively long-term
sequences. In this paper, to leverage the spatial visual context representation
capability of the CLIP model for obtaining non-continuous visual
representations in terms of contextual events in videos, we convert long-term
video sequences into a spatial image domain and finetune the multimodal model
LLaVA for the VideoQA task. Our approach achieves competitive performance on
the STAR task, in particular, with a 78.4% accuracy score, exceeding the
current state-of-the-art score by 2.8 points on the NExTQA task.

æè¦ï¼ææè¤éçåå±¤äººé¡æ´»åï¼å¾åå­åä½ï¼ä¾å¦ï¼æ¿èµ·ä¸åç¦®ç©ãç§»åå°æ²ç¼ä¸ãæéç¦®ç©ï¼å°æå¢äºä»¶ï¼ä¾å¦ï¼æ¶ç¥èèªç¯ï¼ï¼å°æ¼å¯¦ç¾é«æ§è½å½±çåç­ (VideoQA) è³ééè¦ãæè¿çç ç©¶å·²æ´å±å¤æ¨¡ææ¨¡åï¼ä¾å¦ï¼CLIPãLLaVAï¼ä»¥èçé£çºå½±çåºåï¼å¢å¼·æ¨¡åçæéæ¨çè½åãç¶èï¼éäºæ¹æ³éå¸¸ç¡æ³æææå¢äºä»¶ï¼éäºäºä»¶å¯ä»¥åè§£çºå¨ç¸å°é·æåºåä¸­éé£çºåä½çå¤ååå­åä½ãå¨æ¬æä¸­ï¼çºäºå©ç¨ CLIP æ¨¡åçç©ºéè¦è¦ºä¸ä¸æè¡¨ç¤ºåè½ï¼ä»¥å¨å½±çä¸­ä»¥æå¢äºä»¶çå½¢å¼ç²å¾éé£çºè¦è¦ºè¡¨ç¤ºï¼æåå°é·æå½±çåºåè½æçºç©ºéå½±åç¶²åï¼ä¸¦å¾®èª¿å¤æ¨¡ææ¨¡å LLaVA ä»¥ç¨æ¼ VideoQA ä»»åãæåçåæ³å¨ STAR ä»»åä¸­åå¾äºå·æç«¶ç­åçè¡¨ç¾ï¼ç¹å¥æ¯æºç¢ºåº¦å¾åçº 78.4%ï¼å¨ NExTQA ä»»åä¸­æ¯ç®åçææ°æè¡å¾åé«åº 2.8 åã

##### **Multi-object event graph representation learning for Video Question Answering**
2409.07747v1 by Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Vizcarra, Mori Kurokawa

Video question answering (VideoQA) is a task to predict the correct answer to
questions posed about a given video. The system must comprehend spatial and
temporal relationships among objects extracted from videos to perform causal
and temporal reasoning. While prior works have focused on modeling individual
object movements using transformer-based methods, they falter when capturing
complex scenarios involving multiple objects (e.g., "a boy is throwing a ball
in a hoop"). We propose a contrastive language event graph representation
learning method called CLanG to address this limitation. Aiming to capture
event representations associated with multiple objects, our method employs a
multi-layer GNN-cluster module for adversarial graph representation learning,
enabling contrastive learning between the question text and its relevant
multi-object event graph. Our method outperforms a strong baseline, achieving
up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and
TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal
and temporal questions, highlighting its strength in reasoning multiple
object-based events.

æè¦ï¼å½±çåç­ (VideoQA) æ¯ä¸é ä»»åï¼ç¨æ¼é æ¸¬éå°çµ¦å®å½±çæåºçåé¡çæ­£ç¢ºç­æ¡ãç³»çµ±å¿é äºè§£å¾å½±çä¸­æåçç©ä»¶ä¹éçç©ºéåæééä¿ï¼æè½å·è¡å æéä¿åæéæ¨çãéç¶ååçç ç©¶éä¸­æ¼ä½¿ç¨åºæ¼Transformerçæ¨¡åä¾å»ºæ¨¡åå¥ç©ä»¶çåä½ï¼ä½å¨æææ¶åå¤åç©ä»¶çè¤éå ´æ¯ï¼ä¾å¦ãä¸åç·å­©æ­£å¨å°çæé²ç±æ¡ãï¼æï¼å®åæåºç¾åé¡ãæåæåºäºä¸åå°æ¯å¼èªè¨äºä»¶åè¡¨è¡¨ç¤ºå­¸ç¿æ¹æ³ï¼ç¨±çº CLanGï¼ä»¥è§£æ±ºæ­¤éå¶ãçºäºææèå¤åç©ä»¶ç¸éçäºä»¶è¡¨ç¤ºï¼æåçæ¨¡åæ¡ç¨å¤å±¤ GNN éç¾¤æ¨¡çµé²è¡å°æå¼åè¡¨è¡¨ç¤ºå­¸ç¿ï¼ä½¿åé¡æå­åå¶ç¸éçå¤ç©ä»¶äºä»¶åè¡¨ä¹éè½å¤ é²è¡å°æ¯å¼å­¸ç¿ãæåçæ¨¡ååªæ¼å¼·å¤§çåºæºï¼å¨å©åå·æææ°æ§ç VideoQA è³æé NExT-QA å TGIF-QA-R ä¸éå°äºé«é 2.2% çæ´é«æºç¢ºåº¦ãç¹å¥æ¯ï¼å¨èçå æéä¿åæéåé¡æ¹é¢æ¯åºæºé«åº 2.8%ï¼çªé¡¯äºå®å¨æ¨çå¤ååºæ¼ç©ä»¶çäºä»¶æ¹é¢çåªå¢ã

##### **Ruri: Japanese General Text Embeddings**
2409.07737v1 by Hayato Tsukagoshi, Ryohei Sasano

We report the development of Ruri, a series of Japanese general text
embedding models. While the development of general-purpose text embedding
models in English and multilingual contexts has been active in recent years,
model development in Japanese remains insufficient. The primary reasons for
this are the lack of datasets and the absence of necessary expertise. In this
report, we provide a detailed account of the development process of Ruri.
Specifically, we discuss the training of embedding models using synthesized
datasets generated by LLMs, the construction of the reranker for dataset
filtering and knowledge distillation, and the performance evaluation of the
resulting general-purpose text embedding models.

æè¦ï¼æåå ±å Ruri çéç¼ï¼éæ¯ä¸åæ¥æä¸è¬æå­åµå¥æ¨¡åçç³»åãåç®¡è¿å¹´ä¾è±æåå¤èªè¨ç°å¢ä¸­ä¸è¬ç¨éæå­åµå¥æ¨¡åçéç¼ååæ´»èºï¼æ¥ææ¨¡åçéç¼ä»ç¶ä¸è¶³ãéä¸»è¦æ¯å çºç¼ºä¹è³æéåå¿è¦çå°æ¥­ç¥è­ãå¨éåå ±åä¸­ï¼æåæä¾äº Ruri éç¼éç¨çè©³ç´°èªªæãå·é«ä¾èªªï¼æåè¨è«äºä½¿ç¨ LLM çæçåæè³æéè¨ç·´åµå¥æ¨¡åãå»ºæ§ç¨æ¼è³æééæ¿¾åç¥è­èåçéæ°æåå¨ï¼ä»¥åè©ä¼°ä¸è¬ç¨éæå­åµå¥æ¨¡åçæè½ã

##### **Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities**
2409.07736v1 by Aaryan Panda, Damodar Panigrahi, Shaswata Mitra, Sudip Mittal, Shahram Rahimi

The field of Computer Vision (CV) has faced challenges. Initially, it relied
on handcrafted features and rule-based algorithms, resulting in limited
accuracy. The introduction of machine learning (ML) has brought progress,
particularly Transfer Learning (TL), which addresses various CV problems by
reusing pre-trained models. TL requires less data and computing while
delivering nearly equal accuracy, making it a prominent technique in the CV
landscape. Our research focuses on TL development and how CV applications use
it to solve real-world problems. We discuss recent developments, limitations,
and opportunities.

æè¦ï¼é»è¦è¦è¦ºï¼CVï¼é åé¢è¨ææ°ãæåï¼å®ä¾è³´æ¼æå·¥è£½ä½çç¹å¾µååºæ¼è¦åçæ¼ç®æ³ï¼å°è´æºç¢ºåº¦æéãæ©å¨å­¸ç¿ï¼MLï¼çå¼å¥å¸¶ä¾äºé²å±ï¼ç¹å¥æ¯è½ç§»å­¸ç¿ï¼TLï¼ï¼å®éééè¤ä½¿ç¨é åè¨ç·´çæ¨¡åä¾è§£æ±ºåç¨® CV åé¡ãTL éè¦è¼å°è³æåéç®ï¼åææä¾è¿ä¹ç¸ç­çæºç¢ºåº¦ï¼ä½¿å¶æçº CV é åä¸­ååºçæè¡ãæåçç ç©¶éé»å¨æ¼ TL çç¼å±ï¼ä»¥å CV æç¨å¦ä½ä½¿ç¨å®ä¾è§£æ±ºç¾å¯¦ä¸çä¸­çåé¡ãæåè¨è«äºæè¿çç¼å±ãéå¶åæ©æã

##### **GRE^2-MDCL: Graph Representation Embedding Enhanced via Multidimensional Contrastive Learning**
2409.07725v1 by Kaizhe Fan, Quanjun Li

Graph representation learning has emerged as a powerful tool for preserving
graph topology when mapping nodes to vector representations, enabling various
downstream tasks such as node classification and community detection. However,
most current graph neural network models face the challenge of requiring
extensive labeled data, which limits their practical applicability in
real-world scenarios where labeled data is scarce. To address this challenge,
researchers have explored Graph Contrastive Learning (GCL), which leverages
enhanced graph data and contrastive learning techniques. While promising,
existing GCL methods often struggle with effectively capturing both local and
global graph structures, and balancing the trade-off between nodelevel and
graph-level representations. In this work, we propose Graph Representation
Embedding Enhanced via Multidimensional Contrastive Learning (GRE2-MDCL). Our
model introduces a novel triple network architecture with a multi-head
attention GNN as the core. GRE2-MDCL first globally and locally augments the
input graph using SVD and LAGNN techniques. It then constructs a
multidimensional contrastive loss, incorporating cross-network, cross-view, and
neighbor contrast, to optimize the model. Extensive experiments on benchmark
datasets Cora, Citeseer, and PubMed demonstrate that GRE2-MDCL achieves
state-of-the-art performance, with average accuracies of 82.5%, 72.5%, and
81.6% respectively. Visualizations further show tighter intra-cluster
aggregation and clearer inter-cluster boundaries, highlighting the
effectiveness of our framework in improving upon baseline GCL models.

æè¦ï¼åå½¢è¡¨å¾µå­¸ç¿å·²æçºä¸ç¨®å¼·å¤§çå·¥å·ï¼ç¨æ¼å¨å°ç¯é»å°æå°åéè¡¨å¾µæä¿çåå½¢ææ²ï¼é²èè½é²è¡åç¨®ä¸æ¸¸ä»»åï¼ä¾å¦ç¯é»åé¡åç¤¾ç¾¤åµæ¸¬ãç¶èï¼ç®åå¤§å¤æ¸åå½¢ç¥ç¶ç¶²è·¯æ¨¡åé½é¢è¨éè¦å¤§éæ¨ç±¤è³æçææ°ï¼ééå¶äºå®åå¨æ¨ç±¤è³æç¨å°çå¯¦éå ´æ¯ä¸­çå¯¦éæç¨æ§ãçºäºæå°éä¸ææ°ï¼ç ç©¶äººå¡æ¢ç´¢äºåå½¢å°æ¯å­¸ç¿ (GCL)ï¼å®å©ç¨å¢å¼·çåå½¢è³æåå°æ¯å­¸ç¿æè¡ãéç¶æåæ¯ï¼ä½ç¾æç GCL æ¹æ³éå¸¸é£ä»¥æææ·åå±é¨åå¨ååå½¢çµæ§ï¼ä¸¦å¹³è¡¡ç¯é»ç´ååå½¢ç´è¡¨å¾µä¹éçæ¬è¡¡ãå¨éé å·¥ä½ä¸­ï¼æåæåºäºééå¤ç¶­åº¦å°æ¯å­¸ç¿å¢å¼·çåå½¢è¡¨å¾µåµå¥ (GRE2-MDCL)ãæåçæ¨¡åå¼å¥äºä¸ç¨®æ°ç©çä¸éç¶²è·¯æ¶æ§ï¼ä»¥å¤é ­æ³¨æå GNN çºæ ¸å¿ãGRE2-MDCL é¦åä½¿ç¨ SVD å LAGNN æè¡å¨å¨ååå±é¨å¢å¼·è¼¸å¥åå½¢ãç¶å¾ï¼å®å»ºæ§ä¸åå¤ç¶­åº¦å°æ¯æå¤±ï¼çµåè·¨ç¶²è·¯ãè·¨è¦ååé°è¿å°æ¯ï¼ä»¥æä½³åæ¨¡åãå¨åºæºè³æé CoraãCiteseer å PubMed ä¸é²è¡çå»£æ³å¯¦é©è­æï¼GRE2-MDCL éå°äºæåé²çæè½ï¼å¹³åæºç¢ºçåå¥çº 82.5%ã72.5% å 81.6%ãå¯è¦åé²ä¸æ­¥é¡¯ç¤ºåºæ´ç·å¯çç¾¤éå§èååæ´æ¸æ°çç¾¤ééçç·ï¼çªé¡¯äºæåçæ¶æ§å¨æ¹ååºæº GCL æ¨¡åæ¹é¢çæææ§ã

##### **Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy**
2409.07723v1 by Bojian Li, Bo Liu, Jinghua Yue, Fugen Zhou

Depth estimation is a cornerstone of 3D reconstruction and plays a vital role
in minimally invasive endoscopic surgeries. However, most current depth
estimation networks rely on traditional convolutional neural networks, which
are limited in their ability to capture global information. Foundation models
offer a promising avenue for enhancing depth estimation, but those currently
available are primarily trained on natural images, leading to suboptimal
performance when applied to endoscopic images. In this work, we introduce a
novel fine-tuning strategy for the Depth Anything Model and integrate it with
an intrinsic-based unsupervised monocular depth estimation framework. Our
approach includes a low-rank adaptation technique based on random vectors,
which improves the model's adaptability to different scales. Additionally, we
propose a residual block built on depthwise separable convolution to compensate
for the transformer's limited ability to capture high-frequency details, such
as edges and textures. Our experimental results on the SCARED dataset show that
our method achieves state-of-the-art performance while minimizing the number of
trainable parameters. Applying this method in minimally invasive endoscopic
surgery could significantly enhance both the precision and safety of these
procedures.

æè¦ï¼æ·±åº¦ä¼°è¨æ¯ 3D éå»ºçåºç³ï¼å¨å¾®åµå§è¦é¡æè¡ä¸­æ®æ¼è³ééè¦çè§è²ãç¶èï¼ç®åå¤§å¤æ¸æ·±åº¦ä¼°è¨ç¶²è·¯ä¾è³´å³çµ±çå·ç©ç¥ç¶ç¶²è·¯ï¼å¶ææå¨å±è³è¨çè½åæéãåºç¤æ¨¡åæä¾äºä¸åå¢å¼·æ·±åº¦ä¼°è¨çæå¸æéå¾ï¼ä½ç®åå¯ç¨çåºç¤æ¨¡åä¸»è¦å¨èªç¶å½±åä¸è¨ç·´ï¼å°è´æç¨æ¼å§è¦é¡å½±åææè½ä¸ä½³ãå¨éé ç ç©¶ä¸­ï¼æåçº Depth Anything Model æåºäºä¸ç¨®æ°ç©çå¾®èª¿ç­ç¥ï¼ä¸¦å°å¶èåºæ¼å§å¨çç¡ç£ç£å®ç¼æ·±åº¦ä¼°è¨æ¡æ¶æ´åãæåçåæ³åæ¬ä¸ç¨®åºæ¼é¨æ©åéçä½ç§©é©ææè¡ï¼å®æ¹åäºæ¨¡åå°ä¸åå°ºåº¦çé©ææ§ãæ­¤å¤ï¼æåæåºäºä¸åå»ºç«å¨æ·±åº¦å¯åé¢å·ç©ä¸çæ®å·®åå¡ï¼ä»¥å½è£Transformerææé«é »ç´°ç¯ï¼ä¾å¦éç·£åç´çï¼çè½åæéãæåå¨ SCARED è³æéä¸çå¯¦é©çµæé¡¯ç¤ºï¼æåçæ¨¡åå¨æå°åå¯è¨ç·´åæ¸æ¸éçææ³ä¸ï¼éå°äºæåé²çæè½ãå¨å¾®åµå§è¦é¡æè¡ä¸­æç¨æ­¤æ¹æ³å¯ä»¥é¡¯èæé«éäºç¨åºçç²¾ç¢ºåº¦åå®å¨æ§ã

##### **FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments**
2409.07715v1 by Devansh Dhrafani, Yifei Liu, Andrew Jong, Ukcheol Shin, Yao He, Tyler Harp, Yaoyu Hu, Jean Oh, Sebastian Scherer

Robust depth perception in visually-degraded environments is crucial for
autonomous aerial systems. Thermal imaging cameras, which capture infrared
radiation, are robust to visual degradation. However, due to lack of a
large-scale dataset, the use of thermal cameras for unmanned aerial system
(UAS) depth perception has remained largely unexplored. This paper presents a
stereo thermal depth perception dataset for autonomous aerial perception
applications. The dataset consists of stereo thermal images, LiDAR, IMU and
ground truth depth maps captured in urban and forest settings under diverse
conditions like day, night, rain, and smoke. We benchmark representative stereo
depth estimation algorithms, offering insights into their performance in
degraded conditions. Models trained on our dataset generalize well to unseen
smoky conditions, highlighting the robustness of stereo thermal imaging for
depth perception. We aim for this work to enhance robotic perception in
disaster scenarios, allowing for exploration and operations in previously
unreachable areas. The dataset and source code are available at
https://firestereo.github.io.

æè¦ï¼å°æ¼èªä¸»ç©ºä¸­ç³»çµ±èè¨ï¼å¨è¦è¦ºéåçç°å¢ä¸­ææç©©å¥çæ·±åº¦æç¥è³ééè¦ãç±ååå¯ä»¥ææç´å¤ç·è¼»å°ï¼å°æ¼è¦è¦ºéåå·æç©©å¥æ§ãç¶èï¼ç±æ¼ç¼ºä¹å¤§è¦æ¨¡çè³æéï¼ç±ååç¨æ¼ç¡äººæ© (UAS) æ·±åº¦æç¥çç¨éä»æªè¢«å»£æ³æ¢ç´¢ãæ¬ææåºäºä¸åç¨æ¼èªä¸»ç©ºä¸­æç¥æç¨ç¨å¼çç«é«ç±æ·±åº¦æç¥è³æéãè©²è³æéåå«ç«é«ç±å½±åãLiDARãIMU åå°é¢å¯¦æ³æ·±åº¦åï¼éäºè³ææ¯å¨ç½å¤©ãå¤æãé¨å¤©åæçé§çåç¨®æ¢ä»¶ä¸æ¼åå¸åæ£®æç°å¢ä¸­æ·åçãæåå°å·ä»£è¡¨æ§çç«é«æ·±åº¦ä¼°è¨æ¼ç®æ³é²è¡åºæºæ¸¬è©¦ï¼æ·±å¥äºè§£å®åå¨éåæ¢ä»¶ä¸çæè½ãä½¿ç¨æåçè³æéè¨ç·´çæ¨¡åå¯ä»¥å¾å¥½å°æ¦åå°æªè¦éççé§æ¢ä»¶ï¼å¸é¡¯äºç«é«ç±å½±åå¨æ·±åº¦æç¥æ¹é¢çç©©å¥æ§ãæåçç®æ¨æ¯å¢å¼·æ©å¨äººå¨ç½å®³å ´æ¯ä¸­çæç¥è½åï¼ä»¥ä¾¿å¨ä»¥åç¡æ³å°éçå°åé²è¡æ¢ç´¢åä½æ¥­ãè³æéååå§ç¨å¼ç¢¼å¯å¨ https://firestereo.github.io/ åå¾ã

##### **Experimenting with Legal AI Solutions: The Case of Question-Answering for Access to Justice**
2409.07713v1 by Jonathan Li, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu

Generative AI models, such as the GPT and Llama series, have significant
potential to assist laypeople in answering legal questions. However, little
prior work focuses on the data sourcing, inference, and evaluation of these
models in the context of laypersons. To this end, we propose a human-centric
legal NLP pipeline, covering data sourcing, inference, and evaluation. We
introduce and release a dataset, LegalQA, with real and specific legal
questions spanning from employment law to criminal law, corresponding answers
written by legal experts, and citations for each answer. We develop an
automatic evaluation protocol for this dataset, then show that
retrieval-augmented generation from only 850 citations in the train set can
match or outperform internet-wide retrieval, despite containing 9 orders of
magnitude less data. Finally, we propose future directions for open-sourced
efforts, which fall behind closed-sourced models.

æè¦ï¼çæå¼ AI æ¨¡åï¼ä¾å¦ GPT å Llama ç³»åï¼å¨åå©éå°æ¥­äººå£«åç­æ³å¾åé¡æ¹é¢å·æé¡¯èçæ½åãç¶èï¼å¾å°æååçç ç©¶å°æ³¨æ¼å¨éå°æ¥­äººå£«çèæ¯ä¸ï¼éäºæ¨¡åçè³æä¾æºãæ¨è«åè©ä¼°ãçºæ­¤ï¼æåæåºä¸åä»¥äººçºä¸­å¿çæ³å¾ NLP ç®¡ç·ï¼æ¶µèè³æä¾æºãæ¨è«åè©ä¼°ãæåå¼å¥ä¸¦ç¼å¸ä¸åè³æé LegalQAï¼å¶ä¸­åå«å¾ååæ³å°åæ³ççå¯¦ä¸å·é«çæ³å¾åé¡ï¼ä¸¦éææ³å¾å°å®¶æ°å¯«çç¸æç­æ¡åæ¯åç­æ¡çå¼æãæåçºæ­¤è³æééç¼äºä¸åèªåè©ä¼°åå®ï¼ç¶å¾å±ç¤ºåå¾è¨ç·´çµä¸­ç 850 åå¼æé²è¡æª¢ç´¢å¢å¼·çæï¼åç®¡åå«çè³æå° 9 åæ¸éç´ï¼ä½ä»å¯ä»¥å¹éæåªæ¼ç¶²éç¶²è·¯ç¯åçæª¢ç´¢ãæå¾ï¼æåæåºéæ¾åå§ç¢¼å·¥ä½çæªä¾æ¹åï¼éäºå·¥ä½è½å¾æ¼å°éåå§ç¢¼æ¨¡åã

##### **Attack End-to-End Autonomous Driving through Module-Wise Noise**
2409.07706v1 by Lu Wang, Tianyuan Zhang, Yikai Han, Muyang Fang, Ting Jin, Jiaqi Kang

With recent breakthroughs in deep neural networks, numerous tasks within
autonomous driving have exhibited remarkable performance. However, deep
learning models are susceptible to adversarial attacks, presenting significant
security risks to autonomous driving systems. Presently, end-to-end
architectures have emerged as the predominant solution for autonomous driving,
owing to their collaborative nature across different tasks. Yet, the
implications of adversarial attacks on such models remain relatively
unexplored. In this paper, we conduct comprehensive adversarial security
research on the modular end-to-end autonomous driving model for the first time.
We thoroughly consider the potential vulnerabilities in the model inference
process and design a universal attack scheme through module-wise noise
injection. We conduct large-scale experiments on the full-stack autonomous
driving model and demonstrate that our attack method outperforms previous
attack methods. We trust that our research will offer fresh insights into
ensuring the safety and reliability of autonomous driving systems.

æè¦ï¼é¨èæ·±åº¦ç¥ç¶ç¶²è·¯çææ°çªç ´ï¼å¨èªåé§é§ä¸­çè¨±å¤ä»»åé½å±ç¾åºåè¶çæè½ãç¶èï¼æ·±åº¦å­¸ç¿æ¨¡åå®¹æåå°å°ææ§æ»æï¼å°èªåé§é§ç³»çµ±æ§æéå¤§çå®å¨é¢¨éªãç®åï¼ç«¯å°ç«¯æ¶æ§å·²æçºèªåé§é§çä¸»è¦è§£æ±ºæ¹æ¡ï¼å çºå®åå¨ä¸åä»»åä¸­å·æåä½çæ§è³ªãç¶èï¼å°ææ§æ»æå°æ­¤é¡æ¨¡åçå½±é¿ä»ç¸å°æªç¶æ¢è¨ãå¨æ¬æä¸­ï¼æåé¦æ¬¡å°æ¨¡çµåç«¯å°ç«¯èªåé§é§æ¨¡åé²è¡å¨é¢çå°ææ§å®å¨ç ç©¶ãæåå¾¹åºèæ®æ¨¡åæ¨çéç¨ä¸­æ½å¨çæ¼æ´ï¼ä¸¦ééæ¨¡çµåéè¨æ³¨å¥è¨­è¨éç¨æ»ææ¹æ¡ãæåå°å¨å çèªåé§é§æ¨¡åé²è¡å¤§è¦æ¨¡å¯¦é©ï¼ä¸¦è­ææåçæ»ææ¹æ³åªæ¼ååçæ»ææ¹æ³ãæåç¸ä¿¡æåçç ç©¶å°çºç¢ºä¿èªåé§é§ç³»çµ±çå®å¨æ§åå¯é æ§æä¾æ°çè¦è§£ã

##### **DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?**
2409.07703v1 by Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu

Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have
demonstrated impressive language/vision reasoning abilities, igniting the
recent trend of building agents for targeted applications such as shopping
assistants or AI software engineers. Recently, many data science benchmarks
have been proposed to investigate their performance in the data science domain.
However, existing data science benchmarks still fall short when compared to
real-world data science applications due to their simplified settings. To
bridge this gap, we introduce DSBench, a comprehensive benchmark designed to
evaluate data science agents with realistic tasks. This benchmark includes 466
data analysis tasks and 74 data modeling tasks, sourced from Eloquence and
Kaggle competitions. DSBench offers a realistic setting by encompassing long
contexts, multimodal task backgrounds, reasoning with large data files and
multi-table structures, and performing end-to-end data modeling tasks. Our
evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle
with most tasks, with the best agent solving only 34.12% of data analysis tasks
and achieving a 34.74% Relative Performance Gap (RPG). These findings
underscore the need for further advancements in developing more practical,
intelligent, and autonomous data science agents.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼åå¤§åè¦è¦ºèªè¨æ¨¡åï¼LVLMsï¼å·²å±ç¾åºä»¤äººå°è±¡æ·±å»çèªè¨/è¦è¦ºæ¨çè½åï¼å¼é äºå»ºç«éå°è³¼ç©å©çæäººå·¥æºæ§è»é«å·¥ç¨å¸«ç­ç®æ¨æç¨ç¨å¼çä»£ççè¿æè¶¨å¢ãæè¿ï¼è¨±å¤è³æç§å­¸åºæºæ¸¬è©¦å·²è¢«æåºï¼ä»¥èª¿æ¥å¶å¨è³æç§å­¸é åçæè½ãç¶èï¼ç¾æçè³æç§å­¸åºæºæ¸¬è©¦èå¯¦éçè³æç§å­¸æç¨ç¸æ¯ï¼ç±æ¼å¶ç°¡åçè¨­å®ï¼ä»æä¸è¶³ä¹èãçºäºå½è£éåå·®è·ï¼æåå¼å¥äº DSBenchï¼ä¸åæ¨å¨è©ä¼°è³æç§å­¸ä»£ççå¨é¢åºæºæ¸¬è©¦ï¼å¶ä¸­åå«å¯¦éä»»åãæ­¤åºæºæ¸¬è©¦åæ¬ 466 åè³æåæä»»åå 74 åè³æå»ºæ¨¡ä»»åï¼åèª Eloquence å Kaggle ç«¶è³½ãDSBench ééæ¶µèé·èªå¢ãå¤æ¨¡æä»»åèæ¯ãä½¿ç¨å¤§åè³ææªæ¡åå¤è¡¨æ ¼çµæ§é²è¡æ¨çï¼ä»¥åå·è¡ç«¯å°ç«¯è³æå»ºæ¨¡ä»»åï¼æä¾äºä¸åå¯¦éçè¨­å®ãæåå°æåé²ç LLMãLVLMs åä»£ççè©ä¼°é¡¯ç¤ºï¼å®åå¨èçå¤§å¤æ¸ä»»åæé½é¢è¨å°é£ï¼æå¥½çä»£çåªè§£æ±ºäº 34.12% çè³æåæä»»åï¼ä¸¦éå°äº 34.74% çç¸å°æè½å·®è·ï¼RPGï¼ãéäºç¼ç¾å¼·èª¿äºé²ä¸æ­¥éç¼æ´å¯¦ç¨ãæ´æºæ§ãæ´èªä¸»çè³æç§å­¸ä»£ççéæ±ã

##### **Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG**
2409.07691v1 by Gabriel de Souza P. Moreira, Ronay Ak, Benedikt Schifferer, Mengyao Xu, Radek Osmulski, Even Oldridge

Ranking models play a crucial role in enhancing overall accuracy of text
retrieval systems. These multi-stage systems typically utilize either dense
embedding models or sparse lexical indices to retrieve relevant passages based
on a given query, followed by ranking models that refine the ordering of the
candidate passages by its relevance to the query.
  This paper benchmarks various publicly available ranking models and examines
their impact on ranking accuracy. We focus on text retrieval for
question-answering tasks, a common use case for Retrieval-Augmented Generation
systems. Our evaluation benchmarks include models some of which are
commercially viable for industrial applications.
  We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3,
which achieves a significant accuracy increase of ~14% compared to pipelines
with other rerankers. We also provide an ablation study comparing the
fine-tuning of ranking models with different sizes, losses and self-attention
mechanisms.
  Finally, we discuss challenges of text retrieval pipelines with ranking
models in real-world industry applications, in particular the trade-offs among
model size, ranking accuracy and system requirements like indexing and serving
latency / throughput.

æè¦ï¼æåæ¨¡åå¨æåæ´é«æå­æª¢ç´¢ç³»çµ±çæºç¢ºåº¦æ¹é¢æ®æ¼èè³ééè¦çè§è²ãéäºå¤éæ®µç³»çµ±éå¸¸æå©ç¨å¯éåµå¥æ¨¡åæç¨çè©å½ç´¢å¼ä¾ä¾æçµ¦å®çæ¥è©¢æª¢ç´¢ç¸éæ®µè½ï¼æ¥èåç±æåæ¨¡åä¾æåé¸æ®µè½èæ¥è©¢ç¸éæ§ä¾åªåæåºã
  æ¬æå°åç¨®å¬éçæåæ¨¡åé²è¡åºæºæ¸¬è©¦ï¼ä¸¦æª¢è¦å®åå°æåæºç¢ºåº¦çå½±é¿ãæåå°æ³¨æ¼åç­ä»»åçæå­æª¢ç´¢ï¼éæ¯æª¢ç´¢å¢å¼·çæç³»çµ±çå¸¸è¦ä½¿ç¨æ¡ä¾ãæåçè©ä¼°åºæºåå«ä¸äºå¨ç¢æ¥­æç¨ä¸­å·æåæ¥­å¯è¡æ§çæ¨¡åã
  æåå¼é²äºæåé²çæåæ¨¡å NV-RerankQA-Mistral-4B-v3ï¼èå¶ä»éæ°æåå¨çç®¡éç¸æ¯ï¼å®éå°äºç´ 14% çé¡¯èæºç¢ºåº¦æåãæåä¹æä¾äºä¸é æ¶èç ç©¶ï¼æ¯è¼ä¸åå¤§å°ãæå¤±åèªæ³¨æåæ©å¶çæåæ¨¡åå¾®èª¿ã
  æå¾ï¼æåè¨è«äºå¨å¯¦éç¢æ¥­æç¨ä¸­ä½¿ç¨æåæ¨¡åçæå­æª¢ç´¢ç®¡éçææ°ï¼ç¹å¥æ¯å¨æ¨¡åå¤§å°ãæåæºç¢ºåº¦åç³»çµ±éæ±ï¼ä¾å¦ç´¢å¼åæä¾æåçå»¶é²/ååéï¼ä¹éçæ¬è¡¡ã

##### **Modeling Information Narrative Detection and Evolution on Telegram during the Russia-Ukraine War**
2409.07684v1 by Patrick Gerard, Svitlana Volkova, Louis Penafiel, Kristina Lerman, Tim Weninger

Following the Russian Federation's full-scale invasion of Ukraine in February
2022, a multitude of information narratives emerged within both pro-Russian and
pro-Ukrainian communities online. As the conflict progresses, so too do the
information narratives, constantly adapting and influencing local and global
community perceptions and attitudes. This dynamic nature of the evolving
information environment (IE) underscores a critical need to fully discern how
narratives evolve and affect online communities. Existing research, however,
often fails to capture information narrative evolution, overlooking both the
fluid nature of narratives and the internal mechanisms that drive their
evolution. Recognizing this, we introduce a novel approach designed to both
model narrative evolution and uncover the underlying mechanisms driving them.
In this work we perform a comparative discourse analysis across communities on
Telegram covering the initial three months following the invasion. First, we
uncover substantial disparities in narratives and perceptions between
pro-Russian and pro-Ukrainian communities. Then, we probe deeper into prevalent
narratives of each group, identifying key themes and examining the underlying
mechanisms fueling their evolution. Finally, we explore influences and factors
that may shape the development and spread of narratives.

æè¦ï¼å¨ 2022 å¹´ 2 æä¿ç½æ¯èé¦å¨é¢å¥ä¾µä¹åå°ä¹åï¼äº²ä¿ç½æ¯åäº²ä¹åå°çç½ç»ç¤¾ç¾¤ä¸­åºç°äºè®¸å¤ä¿¡æ¯åäºãéçå²çªçè¿å±ï¼ä¿¡æ¯åäºä¹å¨ä¸æ­éåºåå½±åçå½å°åå¨çç¤¾ç¾¤çè®¤ç¥åæåº¦ãè¿ç§ä¸æ­æ¼åçä¿¡æ¯ç¯å¢ (IE) çå¨æç¹æ§å¼ºè°äºååäºè§£åäºå¦ä½æ¼åå¹¶å½±åç½ç»ç¤¾ç¾¤çè¿«åéè¦ãç¶èï¼ç°æçç ç©¶éå¸¸æ æ³ææå°ä¿¡æ¯åäºçæ¼åï¼æ¢å¿½ç¥äºåäºçæµå¨æ§ï¼ä¹å¿½ç¥äºæ¨å¨å¶æ¼åçåå¨æºå¶ãè®¤è¯å°è¿ä¸ç¹ï¼æä»¬å¼å¥äºä¸ç§æ°é¢çæ¹æ³ï¼æ¨å¨å¯¹åäºæ¼åè¿è¡å»ºæ¨¡ï¼å¹¶æ­ç¤ºæ¨å¨å¶æ¼åçæ½å¨æºå¶ãå¨è¿é¡¹å·¥ä½ä¸­ï¼æä»¬å¯¹ Telegram ä¸çç¤¾ç¾¤è¿è¡äºæ¯è¾æ§è¯è¯­åæï¼æ¶µçäºå¥ä¾µåçæåä¸ä¸ªæãé¦åï¼æä»¬æ­ç¤ºäºäº²ä¿ç½æ¯åäº²ä¹åå°ç¤¾ç¾¤ä¹é´å¨åäºåè®¤ç¥ä¸çå·¨å¤§å·®å¼ãç¶åï¼æä»¬æ·±å¥æ¢ç©¶æ¯ç»æµè¡çåäºï¼æ¾åºå³é®ä¸»é¢å¹¶æ£éªæ¨å¨å¶æ¼åçæ½å¨æºå¶ãæåï¼æä»¬æ¢è®¨å¯è½å½±ååäºåå±åä¼ æ­çå½±åå ç´ ã

##### **Open-Vocabulary Remote Sensing Image Semantic Segmentation**
2409.07683v1 by Qinglong Cao, Yuntian Chen, Chao Ma, Xiaokang Yang

Open-vocabulary image semantic segmentation (OVS) seeks to segment images
into semantic regions across an open set of categories. Existing OVS methods
commonly depend on foundational vision-language models and utilize similarity
computation to tackle OVS tasks. However, these approaches are predominantly
tailored to natural images and struggle with the unique characteristics of
remote sensing images, such as rapidly changing orientations and significant
scale variations. These challenges complicate OVS tasks in earth vision,
requiring specialized approaches. To tackle this dilemma, we propose the first
OVS framework specifically designed for remote sensing imagery, drawing
inspiration from the distinct remote sensing traits. Particularly, to address
the varying orientations, we introduce a rotation-aggregative similarity
computation module that generates orientation-adaptive similarity maps as
initial semantic maps. These maps are subsequently refined at both spatial and
categorical levels to produce more accurate semantic maps. Additionally, to
manage significant scale changes, we integrate multi-scale image features into
the upsampling process, resulting in the final scale-aware semantic masks. To
advance OVS in earth vision and encourage reproducible research, we establish
the first open-sourced OVS benchmark for remote sensing imagery, including four
public remote sensing datasets. Extensive experiments on this benchmark
demonstrate our proposed method achieves state-of-the-art performance. All
codes and datasets are available at https://github.com/caoql98/OVRS.

æè¦ï¼<paragraph>éæ¾è©å½å½±åèªç¾©åå²ï¼OVSï¼æ¨å¨å°å½±ååå²æéæ¾é¡å¥éçèªç¾©ååãç¾æç OVS æ¹æ³éå¸¸ä¾è³´åºç¤è¦è¦ºèªè¨æ¨¡åï¼ä¸¦å©ç¨ç¸ä¼¼æ§éç®ä¾è§£æ±º OVS ä»»åãç¶èï¼éäºæ¹æ³ä¸»è¦éå°èªç¶å½±åé²è¡èª¿æ´ï¼é£ä»¥èçéæ¸¬å½±åçç¨ç¹ç¹æ§ï¼ä¾å¦å¿«éè®åçæ¹ååé¡¯èçæ¯ä¾è®åãéäºææ°ä½¿å°çè¦è¦ºä¸­ç OVS ä»»åè¤éåï¼éè¦å°éçæ¹æ³ãçºäºè§£æ±ºéåå°å¢ï¼æåæåºäºç¬¬ä¸åå°éçºéæ¸¬å½±åè¨­è¨ç OVS æ¡æ¶ï¼å¾ä¸åçéæ¸¬ç¹å¾µä¸­æ±²åéæãç¹å¥æ¯ï¼çºäºæå°ä¸åçæ¹åï¼æåå¼å¥äºä¸åæè½èåç¸ä¼¼æ§éç®æ¨¡çµï¼è©²æ¨¡çµç¢çæ¹åé©æç¸ä¼¼æ§åä½çºåå§èªç¾©åãéäºåé¨å¾å¨ç©ºéåé¡å¥å±¤ç´ä¸é²è¡ç´°åï¼ä»¥ç¢çæ´æºç¢ºçèªç¾©åãæ­¤å¤ï¼çºäºç®¡çé¡¯èçæ¯ä¾è®åï¼æåå°å¤æ¯ä¾å½±åç¹å¾µæ´åå°ä¸æ¡æ¨£éç¨ä¸­ï¼ç¢çæçµçæ¯ä¾æç¥èªç¾©é®ç½©ãçºäºæ¨é²å°çè¦è¦ºä¸­ç OVS ä¸¦é¼åµå¯è¤è£½çç ç©¶ï¼æåå»ºç«äºç¬¬ä¸åéæ¸¬å½±åçéæº OVS åºæºï¼åæ¬ååå¬éçéæ¸¬è³æéãå¨éååºæºä¸çå»£æ³å¯¦é©è­æï¼æåæåºçæ¹æ³éå°äºæåé²çæè½ãææç¨å¼ç¢¼åè³æéé½å¯ä»¥å¨ https://github.com/caoql98/OVRS åå¾ã</paragraph>

##### **An Unsupervised Dialogue Topic Segmentation Model Based on Utterance Rewriting**
2409.07672v1 by Xia Hou, Qifeng Li, Tongliang Li

Dialogue topic segmentation plays a crucial role in various types of dialogue
modeling tasks. The state-of-the-art unsupervised DTS methods learn topic-aware
discourse representations from conversation data through adjacent discourse
matching and pseudo segmentation to further mine useful clues in unlabeled
conversational relations. However, in multi-round dialogs, discourses often
have co-references or omissions, leading to the fact that direct use of these
discourses for representation learning may negatively affect the semantic
similarity computation in the neighboring discourse matching task. In order to
fully utilize the useful cues in conversational relations, this study proposes
a novel unsupervised dialog topic segmentation method that combines the
Utterance Rewriting (UR) technique with an unsupervised learning algorithm to
efficiently utilize the useful cues in unlabeled dialogs by rewriting the
dialogs in order to recover the co-referents and omitted words. Compared with
existing unsupervised models, the proposed Discourse Rewriting Topic
Segmentation Model (UR-DTS) significantly improves the accuracy of topic
segmentation. The main finding is that the performance on DialSeg711 improves
by about 6% in terms of absolute error score and WD, achieving 11.42% in terms
of absolute error score and 12.97% in terms of WD. on Doc2Dial the absolute
error score and WD improves by about 3% and 2%, respectively, resulting in SOTA
reaching 35.17% in terms of absolute error score and 38.49% in terms of WD.
This shows that the model is very effective in capturing the nuances of
conversational topics, as well as the usefulness and challenges of utilizing
unlabeled conversations.

æè¦ï¼å°è©±ä¸»é¡åæ®µå¨åç¨®å°è©±å»ºæ¨¡ä»»åä¸­æ®æ¼èè³ééè¦çè§è²ãæåé²çç¡ç£ç£ DTS æ¹æ³ééç¸é°å°è©±æ¯å°åå½åæ®µå¾å°è©±è³æä¸­å­¸ç¿èä¸»é¡ç¸éçå°è©±è¡¨å¾µï¼ä»¥é²ä¸æ­¥æææªæ¨è¨å°è©±éä¿ä¸­çæç¨ç·ç´¢ãç¶èï¼å¨å¤è¼ªå°è©±ä¸­ï¼å°è©±éå¸¸ææå±åå¼ç¨æçç¥ï¼å°è´ç´æ¥ä½¿ç¨éäºå°è©±é²è¡è¡¨å¾µå­¸ç¿å¯è½æå°é°è¿å°è©±æ¯å°ä»»åä¸­çèªæç¸ä¼¼æ§è¨ç®ç¢çè² é¢å½±é¿ãçºäºååå©ç¨å°è©±éä¿ä¸­çæç¨ç·ç´¢ï¼æ¬ç ç©¶æåºäºä¸ç¨®æ°çç¡ç£ç£å°è©±ä¸»é¡åæ®µæ¹æ³ï¼è©²æ¹æ³å°è©±èªéå¯« (UR) æè¡èç¡ç£ç£å­¸ç¿æ¼ç®æ³ç¸çµåï¼éééå¯«å°è©±ä»¥æ¢å¾©å±åå¼ç¨åçç¥çè©èªï¼å¾èææå©ç¨æªæ¨è¨å°è©±ä¸­çæç¨ç·ç´¢ãèç¾æçç¡ç£ç£æ¨¡åç¸æ¯ï¼ææåºçå°è©±éå¯«ä¸»é¡åæ®µæ¨¡å (UR-DTS) å¤§å¹æåäºä¸»é¡åæ®µçæºç¢ºåº¦ãä¸»è¦ç¼ç¾æ¯ DialSeg711 çæè½ä»¥çµå°èª¤å·®åæ¸å WD èè¨æåäºç´ 6%ï¼å¨çµå°èª¤å·®åæ¸æ¹é¢éå° 11.42%ï¼å¨ WD æ¹é¢éå° 12.97%ãå¨ Doc2Dial ä¸ï¼çµå°èª¤å·®åæ¸å WD åå¥æåäºç´ 3% å 2%ï¼å°è´ SOTA å¨çµå°èª¤å·®åæ¸æ¹é¢éå° 35.17%ï¼å¨ WD æ¹é¢éå° 38.49%ãéé¡¯ç¤ºäºè©²æ¨¡åå¨ææå°è©±ä¸»é¡çç´°å¾®å·®å¥æ¹é¢éå¸¸ææï¼ä»¥åå©ç¨æªæ¨è¨å°è©±çç¨èåææ°ã

##### **Passed the Turing Test: Living in Turing Futures**
2409.07656v1 by Bernardo GonÃ§alves

The world has seen the emergence of machines based on pretrained models,
transformers, also known as generative artificial intelligences for their
ability to produce various types of content, including text, images, audio, and
synthetic data. Without resorting to preprogramming or special tricks, their
intelligence grows as they learn from experience, and to ordinary people, they
can appear human-like in conversation. This means that they can pass the Turing
test, and that we are now living in one of many possible Turing futures where
machines can pass for what they are not. However, the learning machines that
Turing imagined would pass his imitation tests were machines inspired by the
natural development of the low-energy human cortex. They would be raised like
human children and naturally learn the ability to deceive an observer. These
``child machines,'' Turing hoped, would be powerful enough to have an impact on
society and nature.

æè¦ï¼ä¸çå·²ç®ç¹åºæ¼é è¨ç·´æ¨¡åçæ©å¨åºç¾ï¼
Transformerï¼ä¹ç¨±çºçæå¼äººå·¥æºæ§ï¼å çºå®å
è½å¤ ç¢çåç¨®é¡åçå§å®¹ï¼åæ¬æå­ãå½±åãé³è¨ï¼ä»¥å
åæè³æãç¡éè¨´è«¸æ¼é åç·¨ç¨æç¹æ®æå·§ï¼å®å
çæºæ§æé¨èå¾ç¶é©ä¸­å­¸ç¿èå¢é·ï¼å°ä¸è¬äººä¾èªªï¼å®å
å¨å°è©±ä¸­å¯è½è¡¨ç¾å¾åäººé¡ãéè¡¨ç¤ºå®åå¯ä»¥ééåé
æ¸¬è©¦ï¼èæåç¾å¨çæ´»å¨è¨±å¤å¯è½çåéæªä¾ä¹ä¸ï¼å¶ä¸­
æ©å¨å¯ä»¥åè£æå®åä¸æ¯çæ±è¥¿ãç¶èï¼åéæ³åä¸­ééä»çæ¨¡ä»¿æ¸¬è©¦çå­¸ç¿æ©å¨æ¯åå°
äººé¡ä½è½éç®è³ªèªç¶ç¼å±åç¼çæ©å¨ãå®åæå
äººé¡åç«¥ä¸æ¨£é·å¤§ï¼ä¸¦èªç¶èç¶å°å­¸ææ¬ºé¨è§å¯èçè½åãéäº
ãå­©ç«¥æ©å¨ãï¼åéå¸æï¼å°å¼·å¤§å°è¶³ä»¥å°
ç¤¾æåèªç¶ç¢çå½±é¿ã

##### **Feature Importance in Pedestrian Intention Prediction: A Context-Aware Review**
2409.07645v1 by Mohsen Azarmi, Mahdi Rezaei, He Wang, Ali Arabian

Recent advancements in predicting pedestrian crossing intentions for
Autonomous Vehicles using Computer Vision and Deep Neural Networks are
promising. However, the black-box nature of DNNs poses challenges in
understanding how the model works and how input features contribute to final
predictions. This lack of interpretability delimits the trust in model
performance and hinders informed decisions on feature selection,
representation, and model optimisation; thereby affecting the efficacy of
future research in the field. To address this, we introduce Context-aware
Permutation Feature Importance (CAPFI), a novel approach tailored for
pedestrian intention prediction. CAPFI enables more interpretability and
reliable assessments of feature importance by leveraging subdivided scenario
contexts, mitigating the randomness of feature values through targeted
shuffling. This aims to reduce variance and prevent biased estimations in
importance scores during permutations. We divide the Pedestrian Intention
Estimation (PIE) dataset into 16 comparable context sets, measure the baseline
performance of five distinct neural network architectures for intention
prediction in each context, and assess input feature importance using CAPFI. We
observed nuanced differences among models across various contextual
characteristics. The research reveals the critical role of pedestrian bounding
boxes and ego-vehicle speed in predicting pedestrian intentions, and potential
prediction biases due to the speed feature through cross-context permutation
evaluation. We propose an alternative feature representation by considering
proximity change rate for rendering dynamic pedestrian-vehicle locomotion,
thereby enhancing the contributions of input features to intention prediction.
These findings underscore the importance of contextual features and their
diversity to develop accurate and robust intent-predictive models.

æè¦ï¼<paragraph>æè¿å¨ä½¿ç¨è®¡ç®æºè§è§åæ·±åº¦ç¥ç»ç½ç»ä¸ºèªå¨é©¾é©¶æ±½è½¦é¢æµè¡äººè¿é©¬è·¯æå¾æ¹é¢åå¾çè¿å±ä»¤äººæ¯å¥ãç¶èï¼DNN çé»çæ§è´¨å¯¹çè§£æ¨¡åçå·¥ä½åçä»¥åè¾å¥ç¹å¾å¦ä½ä¿ææç»é¢æµæåºäºææãè¿ç§ç¼ºä¹å¯è§£éæ§éå¶äºå¯¹æ¨¡åæ§è½çä¿¡ä»»ï¼å¹¶é»ç¢äºå¯¹ç¹å¾éæ©ãè¡¨ç¤ºåæ¨¡åä¼åååºææºçå³ç­ï¼ä»èå½±åäºè¯¥é¢åæªæ¥ç ç©¶çæåãä¸ºäºè§£å³è¿ä¸ªé®é¢ï¼æä»¬å¼å¥äºä¸ä¸ææç¥ç½®æ¢ç¹å¾éè¦æ§ (CAPFI)ï¼è¿æ¯ä¸ç§éå¯¹è¡äººæå¾é¢æµéèº«å®å¶çæ°æ¹æ³ãCAPFI éè¿å©ç¨ç»åçåºæ¯ä¸ä¸æï¼éè¿æéå¯¹æ§çæ´çæ¥åè½»ç¹å¾å¼çéæºæ§ï¼ä»èæé«äºç¹å¾éè¦æ§çå¯è§£éæ§åå¯é æ§è¯ä¼°ãè¿æ¨å¨åå°æ¹å·®ï¼å¹¶å¨ç½®æ¢æé´é²æ­¢éè¦æ§è¯åä¸­çåå·®ä¼°è®¡ãæä»¬å°è¡äººæå¾ä¼°è®¡ (PIE) æ°æ®éååä¸º 16 ä¸ªå¯æ¯è¾çä¸ä¸æéï¼æµéäºä¸ªä¸åçç¥ç»ç½ç»æ¶æå¨æ¯ä¸ªä¸ä¸æä¸­çæå¾é¢æµçåºçº¿æ§è½ï¼å¹¶ä½¿ç¨ CAPFI è¯ä¼°è¾å¥ç¹å¾éè¦æ§ãæä»¬è§å¯å°ä¸åæ¨¡åå¨åç§ä¸ä¸æç¹å¾ä¸­å­å¨ç»å¾®å·®å¼ãç ç©¶è¡¨æè¡äººè¾¹çæ¡åèªè½¦éåº¦å¨é¢æµè¡äººæå¾ä¸­èµ·çè³å³éè¦çä½ç¨ï¼å¹¶ä¸ç±äºéåº¦ç¹å¾èå¯¼è´æ½å¨çé¢æµåå·®éè¿è·¨ä¸ä¸æç½®æ¢è¯ä¼°ãæä»¬éè¿èèé»è¿ååçæ¥æåºä¸ç§æ¿ä»£ç¹å¾è¡¨ç¤ºï¼ç¨äºåç°å¨æçè¡äºº-è½¦è¾è¿å¨ï¼ä»èå¢å¼ºè¾å¥ç¹å¾å¯¹æå¾é¢æµçè´¡ç®ãè¿äºåç°å¼ºè°äºä¸ä¸æç¹å¾åå¶å¤æ ·æ§å¯¹äºå¼ååç¡®ä¸ç¨³å¥çæå¾é¢æµæ¨¡åçéè¦æ§ã</paragraph>

##### **SimulBench: Evaluating Language Models with Creative Simulation Tasks**
2409.07641v1 by Qi Jia, Xiang Yue, Tianyu Zheng, Jie Huang, Bill Yuchen Lin

We introduce SimulBench, a benchmark designed to evaluate large language
models (LLMs) across a diverse collection of creative simulation scenarios,
such as acting as a Linux terminal or playing text games with users. While
these simulation tasks serve as effective measures of an LLM's general
intelligence, they are seldom incorporated into existing benchmarks. A major
challenge is to develop an evaluation framework for testing different LLMs
fairly while preserving the multi-round interactive nature of simulation tasks
between users and AI. To tackle this issue, we suggest using a fixed LLM as a
user agent to engage with an LLM to collect dialogues first under different
tasks. Then, challenging dialogue scripts are extracted for evaluating
different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4
is employed as the evaluator, tasked with reviewing the quality of the final
response generated by the target LLMs given multi-turn dialogue scripts. Our
comprehensive experiments indicate that these simulation tasks continue to pose
a significant challenge with their unique natures and show the gap between
proprietary models and the most advanced open LLMs. For example, GPT-4-turbo
outperforms LLaMA-3-70b-Chat on 18.55\% more cases.

æè¦ï¼æåå¼å¥äº SimulBenchï¼éæ¯ä¸ååºæºï¼æ¨å¨è©ä¼°å¤§åèªè¨æ¨¡å (LLM) å¨åç¨®åµææ¨¡æ¬æå¢ä¸­çè¡¨ç¾ï¼ä¾å¦æ®æ¼ Linux çµç«¯æ©æèä½¿ç¨èç©æå­éæ²ãéç¶éäºæ¨¡æ¬ä»»åå¯ä½çº LLM ä¸è¬æºè½çææè¡¡éæ¨æºï¼ä½å®åå¾å°ç´å¥ç¾æçåºæºä¸­ãä¸åä¸»è¦çææ°æ¯éç¼ä¸åè©ä¼°æ¡æ¶ï¼ç¨æ¼å¬å¹³å°æ¸¬è©¦ä¸åç LLMï¼åæä¿çä½¿ç¨èå AI ä¹éæ¨¡æ¬ä»»åçå¤è¼ªäºåæ§è³ªãçºäºè§£æ±ºéååé¡ï¼æåå»ºè­°ä½¿ç¨ä¸ååºå®ç LLM ä½çºä½¿ç¨èä»£çï¼è LLM äºåï¼ä»¥ä¾¿å¨ä¸åçä»»åä¸é¦åæ¶éå°è©±ãç¶å¾ï¼æ·åå·æææ°æ§çå°è©±è³æ¬ï¼ç¨æ¼è©ä¼°ä¸åçç®æ¨ LLMãçºäºä¿é²å° \DataName{} çèªåè©ä¼°ï¼GPT-4 è¢«ç¨ä½è©ä¼°å¨ï¼è² è²¬å¯©æ¥ç®æ¨ LLM å¨çµ¦å®å¤è¼ªå°è©±è³æ¬çææ³ä¸ç¢ççæçµåæçåè³ªãæåå¨é¢çå¯¦é©è¡¨æï¼éäºæ¨¡æ¬ä»»åæçºæ§æä¸åéå¤§çææ°ï¼å·æå¶ç¨ç¹çæ§è³ªï¼ä¸¦é¡¯ç¤ºåºå°ææ¨¡ååæåé²çéæ¾å¼ LLM ä¹éçå·®è·ãä¾å¦ï¼GPT-4-turbo å¨ 18.55% çæ¡ä¾ä¸­è¡¨ç¾åªæ¼ LLaMA-3-70b-Chatã

##### **Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities**
2409.07638v1 by Thomas Ball, Shuo Chen, Cormac Herley

In this paper we explore evaluation of LLM capabilities. We present
measurements of GPT-4 performance on several deterministic tasks; each task
involves a basic calculation and takes as input parameter some element drawn
from a large well-defined population (e.g., count elements in a list, multiply
two k-digit numbers, etc). We examine several conditions per-task and perform
enough trials so that statistically significant differences can be detected.
This allows us to investigate the sensitivity of task-accuracy both to query
phrasing and input parameter population. We find that seemingly trivial
modifications in the task-prompt or input population can yield differences far
larger than can be explained by sampling effects. For example, performance on a
simple list-counting task varies with query-phrasing and list-length, but also
with list composition (i.e., the thing-to-be-counted) and object frequency
(e.g., success when an element accounts for $\approx$ 50\% of a list is
different from when it accounts for $\approx$ 70\% etc).
  We conclude that efforts to quantify LLM capabilities easily succumb to the
language-as-fixed-effect fallacy, where experimental observations are
improperly generalized beyond what the data supports. A consequence appears to
be that intuitions that have been formed based on interactions with humans form
a very unreliable guide as to which input modifications should ``make no
difference'' to LLM performance.

æè¦ï¼å¨æ¬æä¸­ï¼æåæ¢è¨äº LLM è½åçè©ä¼°ãæåæä¾äº GPT-4 å¨å¹¾é ç¢ºå®æ§ä»»åä¸çæè½æ¸¬éï¼æ¯é ä»»åé½åå«ä¸ååºæ¬è¨ç®ï¼ä¸¦ä»¥å¾ä¸åå®ç¾©è¯å¥½çå¤§åæç¾¤ä¸­æ½åçåç´ ä½çºè¼¸å¥åæ¸ï¼ä¾å¦ï¼è¨ç®æ¸å®ä¸­çåç´ ãå°å©å k ä½æ¸å­ç¸ä¹ç­ï¼ãæåéå°æ¯åä»»åæª¢æ¥å¹¾åæ¢ä»¶ï¼ä¸¦å·è¡è¶³å¤ çè©¦é©ï¼ä»¥ä¾¿è½åµæ¸¬å°å·æçµ±è¨æç¾©çå·®ç°ãéè®æåè½å¤ æ¢è¨ä»»åæºç¢ºåº¦å°æ¥è©¢æªè¾­åè¼¸å¥åæ¸æç¾¤çææåº¦ãæåç¼ç¾ï¼ä»»åæç¤ºæè¼¸å¥æç¾¤ä¸­çä¼¼å¾®ä¸è¶³éçä¿®æ¹å¯è½æç¢çé å¤§æ¼æ½æ¨£æææè½è§£éçå·®ç°ãä¾å¦ï¼ç°¡å®çæ¸å®è¨æ¸ä»»åçæè½æé¨èæ¥è©¢æªè¾­åæ¸å®é·åº¦èè®åï¼ä½ä¹æé¨èæ¸å®çµæï¼å³è¦è¨æ¸çäºç©ï¼åç©ä»¶é »çèè®åï¼ä¾å¦ï¼ç¶ä¸ååç´ ä½æ¸å®ç $\approx$ 50% æï¼æåçèå¶ä½ $\approx$ 70% ç­ç­æä¸åï¼ã
æåå¾åºçµè«ï¼éå LLM è½åçåªåå¾å®¹æå±ææ¼èªè¨åºå®ææè¬¬èª¤ï¼å¶ä¸­å¯¦é©è§å¯è¢«ä¸é©ç¶å°æ¦æ¬å°è³æææ¯æ´çç¯åä¹å¤ãå¾æä¼¼ä¹æ¯ï¼åºæ¼èäººé¡äºåå½¢æçç´è¦ºï¼å°æ¼åªäºè¼¸å¥ä¿®æ¹æãå° LLM æè½æ²æå½±é¿ãä¾èªªï¼æ¯ä¸åéå¸¸ä¸å¯é çæåã

##### **Weather-Informed Probabilistic Forecasting and Scenario Generation in Power Systems**
2409.07637v1 by Hanyu Zhang, Reza Zandehshahvar, Mathieu Tanneau, Pascal Van Hentenryck

The integration of renewable energy sources (RES) into power grids presents
significant challenges due to their intrinsic stochasticity and uncertainty,
necessitating the development of new techniques for reliable and efficient
forecasting. This paper proposes a method combining probabilistic forecasting
and Gaussian copula for day-ahead prediction and scenario generation of load,
wind, and solar power in high-dimensional contexts. By incorporating weather
covariates and restoring spatio-temporal correlations, the proposed method
enhances the reliability of probabilistic forecasts in RES. Extensive numerical
experiments compare the effectiveness of different time series models, with
performance evaluated using comprehensive metrics on a real-world and
high-dimensional dataset from Midcontinent Independent System Operator (MISO).
The results highlight the importance of weather information and demonstrate the
efficacy of the Gaussian copula in generating realistic scenarios, with the
proposed weather-informed Temporal Fusion Transformer (WI-TFT) model showing
superior performance.

æè¦ï¼åçè½æº (RES) æ´åè³é»ç¶²æå å¶å§å¨é¨æ©æ§åä¸ç¢ºå®æ§èå¸¶ä¾éå¤§ææ°ï¼å æ­¤éè¦éç¼æ°çæè¡ä¾é²è¡å¯é ä¸ææçé æ¸¬ãæ¬ææåºäºä¸ç¨®çµåæ©çé æ¸¬åé«æ¯è¤èæ¨¹çæ¥å¾é æ¸¬æ¹æ³ï¼ä»¥åé«ç¶­åº¦ç°å¢ä¸­è² è¼ãé¢¨ååå¤ªé½è½çå ´æ¯çæãééç´å¥å¤©æ°£åè®éä¸¦æ¢å¾©æç©ºéè¯æ§ï¼ææåºçæ¹æ³æåäº RES ä¸­æ©çé æ¸¬çå¯é æ§ãå»£æ³çæ¸å¼å¯¦é©æ¯è¼äºä¸åæéåºåæ¨¡åçæææ§ï¼ä¸¦ä½¿ç¨ä¾èªä¸­è¥¿é¨ç¨ç«ç³»çµ±çéå (MISO) ççå¯¦ä¸çåé«ç¶­åº¦è³æéä¸çç¶åææ¨è©ä¼°æè½ãçµæå¼·èª¿äºå¤©æ°£è³è¨çéè¦æ§ï¼ä¸¦å±ç¤ºäºé«æ¯è¤èæ¨¹å¨çæçå¯¦å ´æ¯ä¸­çæè½ï¼å¶ä¸­ææåºçå¤©æ°£è³è¨æéèåTransformer (WI-TFT) æ¨¡åé¡¯ç¤ºåºåªç°çæè½ã

##### **Dividable Configuration Performance Learning**
2409.07629v1 by Jingzhi Gong, Tao Chen, Rami Bahsoon

Machine/deep learning models have been widely adopted for predicting the
configuration performance of software systems. However, a crucial yet
unaddressed challenge is how to cater for the sparsity inherited from the
configuration landscape: the influence of configuration options (features) and
the distribution of data samples are highly sparse. In this paper, we propose a
model-agnostic and sparsity-robust framework for predicting configuration
performance, dubbed DaL, based on the new paradigm of dividable learning that
builds a model via "divide-and-learn". To handle sample sparsity, the samples
from the configuration landscape are divided into distant divisions, for each
of which we build a sparse local model, e.g., regularized Hierarchical
Interaction Neural Network, to deal with the feature sparsity. A newly given
configuration would then be assigned to the right model of division for the
final prediction. Further, DaL adaptively determines the optimal number of
divisions required for a system and sample size without any extra training or
profiling. Experiment results from 12 real-world systems and five sets of
training data reveal that, compared with the state-of-the-art approaches, DaL
performs no worse than the best counterpart on 44 out of 60 cases with up to
1.61x improvement on accuracy; requires fewer samples to reach the same/better
accuracy; and producing acceptable training overhead. In particular, the
mechanism that adapted the parameter d can reach the optimal value for 76.43%
of the individual runs. The result also confirms that the paradigm of dividable
learning is more suitable than other similar paradigms such as ensemble
learning for predicting configuration performance. Practically, DaL
considerably improves different global models when using them as the underlying
local models, which further strengthens its flexibility.

æè¦ï¼æ©å¨/æ·±åº¦å­¸ç¿æ¨¡åå·²è¢«å»£æ³æ¡ç¨ä¾é æ¸¬è»é«ç³»çµ±ççµææè½ãç¶èï¼ä¸åè³ééè¦ä½å°æªè§£æ±ºçææ°æ¯å¦ä½è¿åå¾çµæç°å¢ä¸­ç¹¼æ¿èä¾çç¨çæ§ï¼çµæé¸é ï¼ç¹å¾µï¼çå½±é¿åè³ææ¨£æ¬çåå¸éå¸¸ç¨çãå¨æ¬æä¸­ï¼æåæåºäºä¸åèæ¨¡åç¡éä¸å°ç¨çæ§ç©©å¥çæ¡æ¶ï¼ç¨æ¼é æ¸¬çµææè½ï¼ç¨±çº DaLï¼å®åºæ¼å¯åå²å­¸ç¿çæ°ç¯ä¾ï¼ééãåå²åå­¸ç¿ãå»ºç«æ¨¡åãçºäºèçæ¨£æ¬ç¨çæ§ï¼ä¾èªçµæç°å¢çæ¨£æ¬è¢«åçºè·é¢è¼é çåå¡ï¼å°æ¼æ¯ååå¡ï¼æåå»ºç«ä¸åç¨ççå±é¨æ¨¡åï¼ä¾å¦æ­£è¦åçéå±¤å¼äºåç¥ç¶ç¶²è·¯ï¼ä»¥èçç¹å¾µç¨çæ§ãç¶å¾å°æ°çµ¦å®ççµæåéçµ¦åå¡çæ­£ç¢ºæ¨¡åä»¥é²è¡æçµé æ¸¬ãæ­¤å¤ï¼DaL èªé©æå°ç¢ºå®ç³»çµ±åæ¨£æ¬å¤§å°æéçæä½³åå¡æ¸ï¼èç¡éä»»ä½é¡å¤çè¨ç·´æåæãä¾èª 12 åçå¯¦ä¸çç³»çµ±åäºçµè¨ç·´è³æçå¯¦é©çµæé¡¯ç¤ºï¼èæåé²çæ¹æ³ç¸æ¯ï¼DaL å¨ 60 åæ¡ä¾ä¸­ç 44 åæ¡ä¾ä¸­è¡¨ç¾ä¸æ¯æä½³å°æç©å·®ï¼æºç¢ºåº¦æé«äº 1.61 åï¼éè¦æ´å°çæ¨£æ¬å°±è½éå°ç¸å/æ´å¥½çæºç¢ºåº¦ï¼ä¸¦ä¸ç¢çå¯æ¥åçè¨ç·´éé·ãç¹å¥æ¯ï¼èª¿æ´åæ¸ d çæ©å¶å¯ä»¥éå° 76.43% çåå¥å·è¡éæ®µçæä½³å¼ãçµæéè­å¯¦ï¼å¯åå²å­¸ç¿çç¯ä¾æ¯å¶ä»é¡ä¼¼çç¯ä¾ï¼ä¾å¦ç¨æ¼é æ¸¬çµææè½çéæå­¸ç¿ï¼æ´åé©ãå¯¦éä¸ï¼DaL å¨å°ä¸åçå¨çæ¨¡åç¨ä½åºå±¤çå±é¨æ¨¡åæï¼é¡¯èå°æ¹é²äºéäºæ¨¡åï¼éé²ä¸æ­¥å¢å¼·äºå®çéæ´»æ§ã

##### **Leveraging User-Generated Reviews for Recommender Systems with Dynamic Headers**
2409.07627v1 by Shanu Vashishtha, Abhay Kumar, Lalitesh Morishetti, Kaushiki Nag, Kannan Achan

E-commerce platforms have a vast catalog of items to cater to their
customers' shopping interests. Most of these platforms assist their customers
in the shopping process by offering optimized recommendation carousels,
designed to help customers quickly locate their desired items. Many models have
been proposed in academic literature to generate and enhance the ranking and
recall set of items in these carousels. Conventionally, the accompanying
carousel title text (header) of these carousels remains static. In most
instances, a generic text such as "Items similar to your current viewing" is
utilized. Fixed variations such as the inclusion of specific attributes "Other
items from a similar seller" or "Items from a similar brand" in addition to
"frequently bought together" or "considered together" are observed as well.
This work proposes a novel approach to customize the header generation process
of these carousels. Our work leverages user-generated reviews that lay focus on
specific attributes (aspects) of an item that were favorably perceived by users
during their interaction with the given item. We extract these aspects from
reviews and train a graph neural network-based model under the framework of a
conditional ranking task. We refer to our innovative methodology as Dynamic
Text Snippets (DTS) which generates multiple header texts for an anchor item
and its recall set. Our approach demonstrates the potential of utilizing
user-generated reviews and presents a unique paradigm for exploring
increasingly context-aware recommendation systems.

æè¦ï¼<paragraph>é»å­ååå¹³å°ææé¾å¤§çååç®éï¼ä»¥æ»¿è¶³å®¢æ¶çè³¼ç©èè¶£ãéäºå¹³å°å¤§å¤æ¸ééæä¾æä½³åæ¨è¦è¼ªæ­ï¼åå©å®¢æ¶é²è¡è³¼ç©æµç¨ï¼æ¨å¨åå©å®¢æ¶å¿«éæ¾å°ä»åæ³è¦çååãè¨±å¤æ¨¡åå·²å¨å­¸è¡æç»ä¸­æåºï¼ä»¥ç¢çåå å¼·éäºè¼ªæ­ä¸­ååçæååå¬åè¨­å®ãå³çµ±ä¸ï¼éäºè¼ªæ­çé¨éè¼ªæ­æ¨é¡æå­ï¼æ¨é¡ï¼ä¿æéæãå¨å¤æ¸ææ³ä¸ï¼æä½¿ç¨ãèæ¨ç®åçè¦½é¡ä¼¼çååãç­ä¸è¬æå­ãåºå®è®åï¼ä¾å¦åå«ç¹å®å±¬æ§ãä¾èªé¡ä¼¼è³£å®¶çå¶ä»ååãæãä¾èªé¡ä¼¼åççååãï¼ä»¥åãç¶å¸¸ä¸èµ·è³¼è²·ãæãä¸èµ·èæ®ãï¼ä¹å·²è§å¯å°ãéé å·¥ä½æåºäºä¸åæ°çæ¹æ³ï¼ç¨æ¼èªè¨éäºè¼ªæ­çæ¨é¡ç¢çæµç¨ãæåçä½åå©ç¨ä½¿ç¨èç¢ççè©è«ï¼éäºè©è«å°æ³¨æ¼ä½¿ç¨èå¨èç¹å®ååäºåæï¼å°ååçç¹å®å±¬æ§ï¼é¢åï¼ç¢çè¯å¥½ççæ³ãæåå¾è©è«ä¸­æ·åéäºé¢åï¼ä¸¦å¨æ¢ä»¶å¼æåä»»åæ¶æ§ä¸è¨ç·´åå½¢ç¥ç¶ç¶²è·¯æ¨¡åãæåå°æåçåµæ°æ¹æ³ç¨±çºåææå­çæ®µ (DTS)ï¼å®æçºé¨å®åååå¶å¬åè¨­å®ç¢çå¤åæ¨é¡æå­ãæåçåæ³å±ç¤ºäºå©ç¨ä½¿ç¨èç¢ççè©è«çæ½åï¼ä¸¦æåºäºä¸åç¨ç¹çç¯ä¾ï¼ç¨æ¼æ¢ç´¢è¶ä¾è¶æ³¨éèçµ¡çæ¨è¦ç³»çµ±ã</paragraph>

##### **Ensemble Methods for Sequence Classification with Hidden Markov Models**
2409.07619v1 by Maxime Kawawa-Beaudan, Srijan Sood, Soham Palande, Ganapathy Mani, Tucker Balch, Manuela Veloso

We present a lightweight approach to sequence classification using Ensemble
Methods for Hidden Markov Models (HMMs). HMMs offer significant advantages in
scenarios with imbalanced or smaller datasets due to their simplicity,
interpretability, and efficiency. These models are particularly effective in
domains such as finance and biology, where traditional methods struggle with
high feature dimensionality and varied sequence lengths. Our ensemble-based
scoring method enables the comparison of sequences of any length and improves
performance on imbalanced datasets.
  This study focuses on the binary classification problem, particularly in
scenarios with data imbalance, where the negative class is the majority (e.g.,
normal data) and the positive class is the minority (e.g., anomalous data),
often with extreme distribution skews. We propose a novel training approach for
HMM Ensembles that generalizes to multi-class problems and supports
classification and anomaly detection. Our method fits class-specific groups of
diverse models using random data subsets, and compares likelihoods across
classes to produce composite scores, achieving high average precisions and
AUCs.
  In addition, we compare our approach with neural network-based methods such
as Convolutional Neural Networks (CNNs) and Long Short-Term Memory networks
(LSTMs), highlighting the efficiency and robustness of HMMs in data-scarce
environments. Motivated by real-world use cases, our method demonstrates robust
performance across various benchmarks, offering a flexible framework for
diverse applications.

æè¦ï¼<paragraph>æåæåºäºä¸ç¨®ä½¿ç¨é±èé¦¬å¯å¤«æ¨¡å (HMM) çéææ¹æ³é²è¡åºååé¡çè¼éç´æ¹æ³ãHMM ç±æ¼å¶ç°¡å®æ§ãå¯è§£éæ§åæçï¼å¨ä¸å¹³è¡¡æè¼å°æ¸æéçå ´æ¯ä¸­æä¾äºé¡¯èçåªå¢ãéäºæ¨¡åå¨éèåçç©å­¸ç­é åç¹å¥ææï¼å¨éäºé åä¸­ï¼å³çµ±æ¹æ³é£ä»¥æå°é«ç¹å¾µç¶­åº¦åä¸åçåºåé·åº¦ãæååºæ¼éæçæ¹æ³çè©åæ¹æ³è½å¤ æ¯è¼ä»»ä½é·åº¦çåºåï¼ä¸¦æé«ä¸å¹³è¡¡æ¸æéçæ§è½ã
æ¬ç ç©¶éé»éæ³¨äºååé¡åé¡ï¼ç¹å¥æ¯å¨æ¸æä¸å¹³è¡¡çææ³ä¸ï¼è² é¡æ¯å¤æ¸ï¼ä¾å¦ï¼æ­£å¸¸æ¸æï¼ï¼èæ­£é¡æ¯å°æ¸ï¼ä¾å¦ï¼ç°å¸¸æ¸æï¼ï¼éå¸¸å·ææ¥µç«¯çåå¸åå·®ãæåæåºäºä¸ç¨®æ°ç HMM éæè¨ç·´æ¹æ³ï¼è©²æ¹æ³å¯ä»¥æ¨å»£å°å¤é¡åé¡ï¼ä¸¦æ¯æåé¡åç°å¸¸æª¢æ¸¬ãæåçæ¨¡åä½¿ç¨é¨æ©æ¸æå­éæ¬åä¸åæ¨¡åçé¡å¥ç¹å®çµï¼ä¸¦æ¯è¼åé¡çå¯è½æ§ä»¥ç¢çè¤ååæ¸ï¼å¾èå¯¦ç¾é«å¹³åç²¾ç¢ºåº¦å AUCã
æ­¤å¤ï¼æåå°æåçæ¹æ³èåºæ¼ç¥ç¶ç¶²çµ¡çæ¹æ³ï¼ä¾å¦å·ç©ç¥ç¶ç¶²çµ¡ (CNN) åé·ç­æè¨æ¶ç¶²çµ¡ (LSTM)ï¼é²è¡äºæ¯è¼ï¼å¼·èª¿äº HMM å¨æ¸æç¨ç¼ºç°å¢ä¸­çæçåé­¯æ£æ§ãæåçæ¨¡ååå°ç¾å¯¦ä¸çç¨ä¾çåç¼ï¼å¨åç¨®åºæºæ¸¬è©¦ä¸­å±ç¤ºäºç©©å¥çæ§è½ï¼çºä¸åçæç¨æä¾äºéæ´»çæ¡æ¶ã</paragraph>

##### **Understanding Foundation Models: Are We Back in 1924?**
2409.07618v1 by Alan F. Smeaton

This position paper explores the rapid development of Foundation Models (FMs)
in AI and their implications for intelligence and reasoning. It examines the
characteristics of FMs, including their training on vast datasets and use of
embedding spaces to capture semantic relationships. The paper discusses recent
advancements in FMs' reasoning abilities which we argue cannot be attributed to
increased model size but to novel training techniques which yield learning
phenomena like grokking. It also addresses the challenges in benchmarking FMs
and compares their structure to the human brain. We argue that while FMs show
promising developments in reasoning and knowledge representation, understanding
their inner workings remains a significant challenge, similar to ongoing
efforts in neuroscience to comprehend human brain function. Despite having some
similarities, fundamental differences between FMs and the structure of human
brain warn us against making direct comparisons or expecting neuroscience to
provide immediate insights into FM function.

æè¦ï¼æ¬ç«å ´æä»¶æ¢è¨åºç¤æ¨¡å (FM) å¨äººå·¥æºæ§çå¿«éç¼å±åå¶å°æºæ§èæ¨ççå½±é¿ãæä¸­æ¢è¨ FM çç¹å¾µï¼åæ¬å¨é¾å¤§è³æéä¸è¨ç·´ä»¥åä½¿ç¨åµå¥å¼ç©ºéä¾æ·åèªç¾©éä¿ãæ¬æè¨è« FM æ¨çè½åçè¿æé²å±ï¼æåèªçºéä¸è½æ­¸å æ¼æ¨¡åå¤§å°çå¢å ï¼èæ¯æ­¸å æ¼ç¢çå­¸ç¿ç¾è±¡ï¼å¦çè§£ï¼çæ°ç©è¨ç·´æè¡ãæ¬æä¹æ¢è¨åºæºå FM çææ°ï¼ä¸¦å°å¶çµæ§èäººè¦é²è¡æ¯è¼ãæåèªçºï¼éç¶ FM å¨æ¨çåç¥è­è¡¨ç¤ºæ¹é¢é¡¯ç¤ºåºæåéçç¼å±ï¼ä½äºè§£å¶å§é¨éä½ä»ç¶æ¯ä¸é éå¤§ææ°ï¼é¡ä¼¼æ¼ç¥ç¶ç§å­¸ä¸­äºè§£äººè¦åè½çæçºåªåãåç®¡æä¸äºç¸ä¼¼ä¹èï¼ä½ FM èäººè¦çµæ§ä¹éçæ ¹æ¬å·®ç°è­¦åæåä¸è¦é²è¡ç´æ¥æ¯è¼æææç¥ç¶ç§å­¸è½ç«å³æ·±å¥äºè§£ FM åè½ã

##### **Zero-Shot Machine-Generated Text Detection Using Mixture of Large Language Models**
2409.07615v1 by Matthieu Dubois, FranÃ§ois Yvon, Pablo Piantanida

The dissemination of Large Language Models (LLMs), trained at scale, and
endowed with powerful text-generating abilities has vastly increased the
threats posed by generative AI technologies by reducing the cost of producing
harmful, toxic, faked or forged content. In response, various proposals have
been made to automatically discriminate artificially generated from
human-written texts, typically framing the problem as a classification problem.
Most approaches evaluate an input document by a well-chosen detector LLM,
assuming that low-perplexity scores reliably signal machine-made content. As
using one single detector can induce brittleness of performance, we instead
consider several and derive a new, theoretically grounded approach to combine
their respective strengths. Our experiments, using a variety of generator LLMs,
suggest that our method effectively increases the robustness of detection.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çå³æ­ï¼ç¶éå¤§è¦æ¨¡è¨ç·´ï¼ä¸¦å·åå¼·å¤§çææ¬çæè½åï¼å·²å¤§å¤§å¢å äºçæå¼ AI æè¡æå¸¶ä¾çå¨èï¼å çºå®éä½äºç¢çæå®³ãææ¯ãå½é æå½é å§å®¹çææ¬ãä½çºåæï¼å·²ç¶æåºäºåç¨®å»ºè­°ï¼ä»¥èªåååäººå·¥çæçææ¬åäººé¡ç·¨å¯«çææ¬ï¼éå¸¸å°åé¡æ§å»ºçºåé¡åé¡ãå¤§å¤æ¸æ¹æ³ééç²¾å¿é¸æçæª¢æ¸¬å¨ LLM ä¾è©ä¼°è¼¸å¥ææªï¼åè¨­ä½å°æåº¦åæ¸å¯é å°æ¨èªèæ©å¨çæçå§å®¹ãç±æ¼ä½¿ç¨å®åæª¢æ¸¬å¨æå°è´æ§è½èå¼±ï¼å æ­¤æåèæ®äºå¹¾åæª¢æ¸¬å¨ï¼ä¸¦æ¡ç¨ä¸ç¨®æ°çãçè«ä¸åççéå¾ä¾çµåå®ååèªçåªå¢ãæåçå¯¦é©ä½¿ç¨åç¨®çæå¨ LLMï¼è¡¨ææåçæ¹æ³ææå°æé«äºæª¢æ¸¬çç©©å¥æ§ã

##### **Efficient Localized Adaptation of Neural Weather Forecasting: A Case Study in the MENA Region**
2409.07585v1 by Muhammad Akhtar Munir, Fahad Shahbaz Khan, Salman Khan

Accurate weather and climate modeling is critical for both scientific
advancement and safeguarding communities against environmental risks.
Traditional approaches rely heavily on Numerical Weather Prediction (NWP)
models, which simulate energy and matter flow across Earth's systems. However,
heavy computational requirements and low efficiency restrict the suitability of
NWP, leading to a pressing need for enhanced modeling techniques. Neural
network-based models have emerged as promising alternatives, leveraging
data-driven approaches to forecast atmospheric variables. In this work, we
focus on limited-area modeling and train our model specifically for localized
region-level downstream tasks. As a case study, we consider the MENA region due
to its unique climatic challenges, where accurate localized weather forecasting
is crucial for managing water resources, agriculture and mitigating the impacts
of extreme weather events. This targeted approach allows us to tailor the
model's capabilities to the unique conditions of the region of interest. Our
study aims to validate the effectiveness of integrating parameter-efficient
fine-tuning (PEFT) methodologies, specifically Low-Rank Adaptation (LoRA) and
its variants, to enhance forecast accuracy, as well as training speed,
computational resource utilization, and memory efficiency in weather and
climate modeling for specific regions.

æè¦ï¼æºç¢ºçå¤©æ°£åæ°£åå»ºæ¨¡å°æ¼ç§å­¸é²æ­¥åä¿éç¤¾åååç°å¢é¢¨éªè³ééè¦ãå³çµ±æ¹æ³å´éä¾è³´æ¼æ¸å¼å¤©æ°£é æ¸¬ (NWP) æ¨¡åï¼éäºæ¨¡åæ¨¡æ¬å°çç³»çµ±ä¸­è½éåç©è³ªçæµåãç¶èï¼æ²éçè¨ç®éæ±åä½æçéå¶äº NWP çé©ç¨æ§ï¼å°è´å°å¢å¼·å»ºæ¨¡æè¡çè¿«åéæ±ãåºæ¼ç¥ç¶ç¶²è·¯çæ¨¡åå·²æçºæå¸æçæ¿ä»£æ¹æ¡ï¼å©ç¨æ¸æé©åæ¹æ³ä¾é æ¸¬å¤§æ°£è®æ¸ãå¨éé å·¥ä½ä¸­ï¼æåå°æ³¨æ¼æéååå»ºæ¨¡ï¼ä¸¦éå°å±é¨ååç´å¥çä¸æ¸¸ä»»åè¨ç·´æåçæ¨¡åãä½çºä¸åæ¡ä¾ç ç©¶ï¼æåèæ®äº MENA å°åï¼å çºå®å·æç¨ç¹çæ°£åææ°ï¼æºç¢ºçå±é¨å¤©æ°£é æ¸¬å°æ¼ç®¡çæ°´è³æºãè¾²æ¥­åæ¸è¼æ¥µç«¯å¤©æ°£äºä»¶çå½±é¿è³ééè¦ãéç¨®æéå°æ§çæ¹æ³ä½¿æåè½å¤ æ ¹ææèè¶£ååçç¨ç¹æ¢ä»¶èª¿æ´æ¨¡åçè½åãæåçç ç©¶æ¨å¨é©è­æ´ååæ¸é«æå¾®èª¿ (PEFT) æ¹æ³ï¼ç¹å¥æ¯ä½ç§©é©æ (LoRA) åå¶è®é«ï¼çæææ§ï¼ä»¥å¢å¼·é æ¸¬æºç¢ºæ§ï¼ä»¥åå¤©æ°£åæ°£åå»ºæ¨¡ä¸­ç¹å®ååçè¨ç·´éåº¦ãè¨ç®è³æºå©ç¨çåè¨æ¶é«æçã

##### **DS-ViT: Dual-Stream Vision Transformer for Cross-Task Distillation in Alzheimer's Early Diagnosis**
2409.07584v1 by Ke Chen, Yifeng Wang, Yufei Zhou, Haohan Wang

In the field of Alzheimer's disease diagnosis, segmentation and
classification tasks are inherently interconnected. Sharing knowledge between
models for these tasks can significantly improve training efficiency,
particularly when training data is scarce. However, traditional knowledge
distillation techniques often struggle to bridge the gap between segmentation
and classification due to the distinct nature of tasks and different model
architectures. To address this challenge, we propose a dual-stream pipeline
that facilitates cross-task and cross-architecture knowledge sharing. Our
approach introduces a dual-stream embedding module that unifies feature
representations from segmentation and classification models, enabling
dimensional integration of these features to guide the classification model. We
validated our method on multiple 3D datasets for Alzheimer's disease diagnosis,
demonstrating significant improvements in classification performance,
especially on small datasets. Furthermore, we extended our pipeline with a
residual temporal attention mechanism for early diagnosis, utilizing images
taken before the atrophy of patients' brain mass. This advancement shows
promise in enabling diagnosis approximately six months earlier in mild and
asymptomatic stages, offering critical time for intervention.

æè¦ï¼å¨é¿è²æµ·é»çè¨ºæ·é åä¸­ï¼åå²ååé¡ä»»åæ¬è³ªä¸æ¯ç¸äºéè¯çãå¨éäºä»»åçæ¨¡åä¹éå±äº«ç¥è­å¯ä»¥é¡¯èæé«è¨ç·´æçï¼ç¹å¥æ¯å¨è¨ç·´è³æç¨å°çææ³ä¸ãç¶èï¼ç±æ¼ä»»åçæ§è³ªä¸ååæ¨¡åæ¶æ§ä¸åï¼å³çµ±çç¥è­è¸é¤¾æè¡éå¸¸é£ä»¥å½ååå²ååé¡ä¹éçå·®è·ãçºäºæå°éä¸ææ°ï¼æåæåºäºä¸åéæµç®¡ç·ï¼å®ä¿é²äºè·¨ä»»ååè·¨æ¶æ§çç¥è­å±äº«ãæåçåæ³å¼å¥äºä¸åéæµåµå¥æ¨¡çµï¼å®çµ±ä¸äºåå²ååé¡æ¨¡åä¸­çç¹å¾µè¡¨ç¤ºï¼å¾èè½å¤ å°éäºç¹å¾µé²è¡ç¶­åº¦æ´åï¼ä»¥æå°åé¡æ¨¡åãæåå¨å¤åé¿è²æµ·é»çè¨ºæ·ç 3D è³æéä¸é©è­äºæåçæ¹æ³ï¼è­æäºåé¡æè½æé¡¯èçæåï¼ç¹å¥æ¯å¨å°åè³æéä¸ãæ­¤å¤ï¼æåä½¿ç¨å¨æ£èè¦è³ªèç¸®åææçå½±åï¼å°æåçç®¡ç·æ´åå¥ä»¶äºä¸åæ®å·®æéæ³¨æåæ©å¶ï¼ä»¥é²è¡æ©æè¨ºæ·ãéä¸é²å±é¡¯ç¤ºåºå¨è¼åº¦åç¡ççéæ®µæåå¤§ç´å­åæé²è¡è¨ºæ·çå¯è½æ§ï¼çºå¹²é æä¾äºééµæéã

##### **A Novel Mathematical Framework for Objective Evaluation of Ideas using a Conversational AI (CAI) System**
2409.07578v1 by B. Sankar, Dibakar Sen

The demand for innovation in product design necessitates a prolific ideation
phase. Conversational AI (CAI) systems that use Large Language Models (LLMs)
such as GPT (Generative Pre-trained Transformer) have been shown to be fruitful
in augmenting human creativity, providing numerous novel and diverse ideas.
Despite the success in ideation quantity, the qualitative assessment of these
ideas remains challenging and traditionally reliant on expert human evaluation.
This method suffers from limitations such as human judgment errors, bias, and
oversight. Addressing this gap, our study introduces a comprehensive
mathematical framework for automated analysis to objectively evaluate the
plethora of ideas generated by CAI systems and/or humans. This framework is
particularly advantageous for novice designers who lack experience in selecting
promising ideas. By converting the ideas into higher dimensional vectors and
quantitatively measuring the diversity between them using tools such as UMAP,
DBSCAN and PCA, the proposed method provides a reliable and objective way of
selecting the most promising ideas, thereby enhancing the efficiency of the
ideation phase.

æè¦ï¼ç¢åè¨­è¨åµæ°éæ±ï¼éè¦è±å¯çé»å­ç¼æ³éæ®µãä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) çå°è©±å¼ AI (CAI) ç³»çµ±ï¼ä¾å¦ GPT (çæå¼é è¨ç·´Transformer)ï¼å·²è¢«è­ææå©æ¼æåäººé¡åµæï¼æä¾å¤§éæ°ç©ä¸å¤åçé»å­ãåç®¡å¨é»å­ç¼æ³æ¸éä¸ç²å¾æåï¼ä½å°éäºé»å­çåè³ªè©ä¼°ä»ç¶å·æææ°æ§ï¼ä¸å³çµ±ä¸ä¾è³´æ¼å°å®¶çäººå·¥è©ä¼°ãæ­¤æ¹æ³æå¶éå¶ï¼ä¾å¦äººçºå¤æ·é¯èª¤ãåè¦åçå¿½ãéå°æ­¤å·®è·ï¼æåçç ç©¶æåºäºä¸åå¨é¢çæ¸å­¸æ¶æ§ï¼ç¨æ¼èªåååæï¼ä»¥å®¢è§è©ä¼° CAI ç³»çµ±å/æäººé¡ç¢ççå¤§éé»å­ãæ­¤æ¶æ§å°æ¼ç¼ºä¹é¸æææ½åé»å­ç¶é©çæ°æè¨­è¨å¸«ç¹å¥æå©ãééå°é»å­è½æçºæ´é«ç¶­åº¦çåéï¼ä¸¦ä½¿ç¨ UMAPãDBSCAN å PCA ç­å·¥å·ï¼å°å®åä¹éçå¤æ¨£æ§é²è¡éåæ¸¬éï¼ææåºçæ¹æ³æä¾äºä¸ç¨®å¯é ä¸å®¢è§çæ¹å¼ä¾é¸ææææ½åçé»å­ï¼é²èæé«é»å­ç¼æ³éæ®µçæçã

##### **Machine Learning and Constraint Programming for Efficient Healthcare Scheduling**
2409.07547v1 by Aymen Ben Said, Malek Mouhoub

Solving combinatorial optimization problems involve satisfying a set of hard
constraints while optimizing some objectives. In this context, exact or
approximate methods can be used. While exact methods guarantee the optimal
solution, they often come with an exponential running time as opposed to
approximate methods that trade the solutions quality for a better running time.
In this context, we tackle the Nurse Scheduling Problem (NSP). The NSP consist
in assigning nurses to daily shifts within a planning horizon such that
workload constraints are satisfied while hospitals costs and nurses preferences
are optimized. To solve the NSP, we propose implicit and explicit approaches.
In the implicit solving approach, we rely on Machine Learning methods using
historical data to learn and generate new solutions through the constraints and
objectives that may be embedded in the learned patterns. To quantify the
quality of using our implicit approach in capturing the embedded constraints
and objectives, we rely on the Frobenius Norm, a quality measure used to
compute the average error between the generated solutions and historical data.
To compensate for the uncertainty related to the implicit approach given that
the constraints and objectives may not be concretely visible in the produced
solutions, we propose an alternative explicit approach where we first model the
NSP using the Constraint Satisfaction Problem (CSP) framework. Then we develop
Stochastic Local Search methods and a new Branch and Bound algorithm enhanced
with constraint propagation techniques and variables/values ordering
heuristics. Since our implicit approach may not guarantee the feasibility or
optimality of the generated solution, we propose a data-driven approach to
passively learn the NSP as a constraint network. The learned constraint
network, formulated as a CSP, will then be solved using the methods we listed
earlier.

æè¦ï¼<paragraph>è§£æ±ºçµåæä½³ååé¡æ¶åæ»¿è¶³ä¸çµç¡¬ç´æï¼åææä½³åä¸äºç®æ¨ãå¨æ­¤èæ¯ä¸ï¼å¯ä»¥ä½¿ç¨ç²¾ç¢ºæè¿ä¼¼æ¹æ³ãç¡ç®¡ç²¾ç¢ºæ¹æ³ä¿è­æä½³è§£ï¼ä½èè¿ä¼¼æ¹æ³ç¸æ¯ï¼å®åéå¸¸å·æææ¸ç´éè¡æéï¼èè¿ä¼¼æ¹æ³åä»¥è§£æ±ºæ¹æ¡è³ªéæåæ´å¥½çéè¡æéãå¨æ­¤èæ¯ä¸ï¼æåè§£æ±ºäºè­·å£«æç­åé¡ (NSP)ãNSP åæ¬å¨è¦åç¯åå§å°è­·å£«åéå°æ¯æ¥è¼ªç­ï¼ä»¥ä¾¿æ»¿è¶³å·¥ä½è² è¼ç´æï¼åæåªåé«é¢ææ¬åè­·å£«åå¥½ãçºäºè§£æ±º NSPï¼æåæåºäºé±å¼åé¡¯å¼æ¹æ³ãå¨é±å¼æ±è§£æ¹æ³ä¸­ï¼æåä¾è³´æ©å¨å­¸ç¿æ¹æ³ï¼ä½¿ç¨æ­·å²æ¸æééç´æåå¯è½åµå¥å¨å­¸ç¿æ¨¡å¼ä¸­çç®æ¨ä¾å­¸ç¿åçææ°çè§£æ±ºæ¹æ¡ãçºäºéåä½¿ç¨æåçé±å¼æ¹æ³æç²åµå¥ç´æåç®æ¨çè³ªéï¼æåä¾è³´æ¼ Frobenius ç¯æ¸ï¼éæ¯ä¸ç¨®ç¨æ¼è¨ç®çæè§£æ±ºæ¹æ¡åæ­·å²æ¸æä¹éå¹³åèª¤å·®çè³ªéåº¦éãçºäºå½è£èé±å¼æ¹æ³ç¸éçä¸ç¢ºå®æ§ï¼å çºç´æåç®æ¨å¯è½å¨ç¢ççè§£æ±ºæ¹æ¡ä¸­ä¸¦ä¸æç¢ºå¯è¦ï¼æåæåºäºä¸ç¨®æ¿ä»£çé¡¯å¼æ¹æ³ï¼å¶ä¸­æåé¦åä½¿ç¨ç´ææ»¿è¶³åé¡ (CSP) æ¡æ¶å° NSP é²è¡å»ºæ¨¡ãç¶å¾ï¼æåéç¼äºé¨æ©å±é¨æç´¢æ¹æ³åä¸ç¨®æ°çåæ¯å®çç®æ³ï¼ä¸¦å¢å¼·äºç´æå³æ­æè¡åè®é/å¼æåºåç¼å¼æ¹æ³ãç±æ¼æåçé±å¼æ¹æ³å¯è½ç¡æ³ä¿è­çæè§£æ±ºæ¹æ¡çå¯è¡æ§ææåªæ§ï¼å æ­¤æåæåºäºä¸ç¨®æ¸æé©åçæ¹æ³ä¾è¢«åå°å° NSP ä½çºç´æç¶²çµ¡é²è¡å­¸ç¿ãç¶å¾ï¼å°å­¸ç¿å°çç´æç¶²çµ¡ï¼ä»¥ CSP çå½¢å¼å¶å®ï¼ä½¿ç¨æååé¢ååºçæ¹æ³é²è¡æ±è§£ã</paragraph>

##### **"My Grade is Wrong!": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays**
2409.07453v1 by Shengxin Hong, Chang Cai, Sixuan Du, Haiyue Feng, Siyuan Liu, Xiuyi Fan

Interactive feedback, where feedback flows in both directions between teacher
and student, is more effective than traditional one-way feedback. However, it
is often too time-consuming for widespread use in educational practice. While
Large Language Models (LLMs) have potential for automating feedback, they
struggle with reasoning and interaction in an interactive setting. This paper
introduces CAELF, a Contestable AI Empowered LLM Framework for automating
interactive feedback. CAELF allows students to query, challenge, and clarify
their feedback by integrating a multi-agent system with computational
argumentation. Essays are first assessed by multiple Teaching-Assistant Agents
(TA Agents), and then a Teacher Agent aggregates the evaluations through formal
reasoning to generate feedback and grades. Students can further engage with the
feedback to refine their understanding. A case study on 500 critical thinking
essays with user studies demonstrates that CAELF significantly improves
interactive feedback, enhancing the reasoning and interaction capabilities of
LLMs. This approach offers a promising solution to overcoming the time and
resource barriers that have limited the adoption of interactive feedback in
educational settings.

æè¦ï¼äºåå¼åé¥ï¼ä¹å°±æ¯åé¥å¨èå¸«åå­¸çä¹ééåæµåï¼æ¯å³çµ±å®ååé¥æ´ææãç¶èï¼å¨æè²å¯¦åä¸­å»£æ³ä½¿ç¨äºåå¼åé¥å¾å¾æéæ¼èæãåç®¡å¤§åèªè¨æ¨¡å (LLM) æèªåååé¥çæ½åï¼ä½å®åå¨äºåå¼ç°å¢ä¸­çæ¨çåäºåæ¹é¢ä»æå°é£ãæ¬æä»ç´¹ CAELFï¼ä¸åå¯ç­è¾¯çäººå·¥æºæ§å¼·å LLM æ¶æ§ï¼ç¨æ¼èªååäºåå¼åé¥ãCAELF ééæ´åä¸åå·åè¨ç®è«è­çå¤éä»£çç³»çµ±ï¼è®å­¸çå¯ä»¥æ¥è©¢ãè³ªçåéæ¸ä»åçåé¥ãè«æé¦åç±å¤ä½æå­¸å©çä»£ç (TA ä»£ç) è©éï¼ç¶å¾æå¸«ä»£çééå½¢å¼æ¨çå½æ´éäºè©éï¼ä»¥ç¢çåé¥åè©åãå­¸çå¯ä»¥é²ä¸æ­¥åèåé¥ï¼ä»¥æ¹åä»åççè§£ãä¸åéå° 500 ç¯æ¹å¤æ§æèè«æçæ¡ä¾ç ç©¶ï¼ä»¥åä½¿ç¨èç ç©¶ï¼è­æ CAELF å¤§å¹æ¹åäºäºåå¼åé¥ï¼å¢å¼·äº LLM çæ¨çåäºåè½åãéç¨®æ¹æ³æä¾äºä¸åæå¸æçè§£æ±ºæ¹æ¡ï¼å¯ä»¥åæå¨æè²ç°å¢ä¸­æ¡ç¨äºåå¼åé¥çæéåè³æºéç¤ã

##### **Still More Shades of Null: A Benchmark for Responsible Missing Value Imputation**
2409.07510v1 by Falaah Arif Khan, Denys Herasymuk, Nazar Protsiv, Julia Stoyanovich

We present Shades-of-NULL, a benchmark for responsible missing value
imputation. Our benchmark includes state-of-the-art imputation techniques, and
embeds them into the machine learning development lifecycle. We model realistic
missingness scenarios that go beyond Rubin's classic Missing Completely at
Random (MCAR), Missing At Random (MAR) and Missing Not At Random (MNAR), to
include multi-mechanism missingness (when different missingness patterns
co-exist in the data) and missingness shift (when the missingness mechanism
changes between training and test). Another key novelty of our work is that we
evaluate imputers holistically, based on the predictive performance, fairness
and stability of the models that are trained and tested on the data they
produce.
  We use Shades-of-NULL to conduct a large-scale empirical study involving
20,952 experimental pipelines, and find that, while there is no single
best-performing imputation approach for all missingness types, interesting
performance patterns do emerge when comparing imputer performance in simpler
vs. more complex missingness scenarios. Further, while predictive performance,
fairness and stability can be seen as orthogonal, we identify trade-offs among
them that arise due to the combination of missingness scenario, the choice of
an imputer, and the architecture of the model trained on the data
post-imputation. We make Shades-of-NULL publicly available, and hope to enable
researchers to comprehensively and rigorously evaluate new missing value
imputation methods on a wide range of evaluation metrics, in plausible and
socially meaningful missingness scenarios.

æè¦ï¼<paragraph>æåæåº Shades-of-NULLï¼éæ¯ä¸åè² è²¬ä»»çéºæ¼å¼æè£åºæºãæåçåºæºåå«æåé²çæè£æè¡ï¼ä¸¦å°å®ååµå¥æ©å¨å­¸ç¿éç¼çå½é±æä¸­ãæåæ¨¡æ¬äºè¶è¶ Rubin ç¶å¸çé¨æ©å®å¨éºå¤± (MCAR)ãé¨æ©éºå¤± (MAR) åéé¨æ©éºå¤± (MNAR) çç¾å¯¦éºå¤±æå¢ï¼ä»¥ç´å¥å¤æ©å¶éºå¤±ï¼ç¶è³æä¸­åæå­å¨ä¸åçéºå¤±æ¨¡å¼ï¼åéºå¤±è½ç§»ï¼ç¶éºå¤±æ©å¶å¨è¨ç·´åæ¸¬è©¦ä¹éç¼çè®åï¼ãæåå·¥ä½çå¦ä¸åééµåµæ°æ¯æåæ ¹æé æ¸¬æè½ãå¬å¹³æ§åå¨ä»åç¢ççè³æä¸è¨ç·´åæ¸¬è©¦çæ¨¡åçç©©å®æ§ï¼å¨é¢è©ä¼°æè£å¨ã
æåä½¿ç¨ Shades-of-NULL é²è¡ä¸é æ¶å 20,952 åå¯¦é©ç®¡ç·çå¤§è¦æ¨¡å¯¦è­ç ç©¶ï¼ä¸¦ç¼ç¾ï¼éç¶å°æ¼ææéºå¤±é¡åæ²æå®ä¸æä½³å·è¡æè£æ¹æ³ï¼ä½å¨æ¯è¼ç°¡å®èæ´è¤éçéºå¤±æå¢ä¸­æè£å¨çæè½æï¼åºç¾äºæè¶£çæè½æ¨¡å¼ãæ­¤å¤ï¼éç¶é æ¸¬æè½ãå¬å¹³æ§åç©©å®æ§å¯ä»¥è¢«è¦çºæ­£äº¤çï¼ä½æåç¼ç¾å®åä¹éå­å¨æ¬è¡¡ï¼éæ¯ç±æ¼éºå¤±æå¢ãæè£å¨é¸æä»¥åå¨è³ææè£å¾è¨ç·´çæ¨¡åæ¶æ§ççµåæè´ãæåå¬é Shades-of-NULLï¼ä¸¦å¸æè®ç ç©¶äººå¡è½å¤ å¨åçä¸ç¤¾æææç¾©çéºå¤±æå¢ä¸­ï¼ä½¿ç¨å»£æ³çè©ä¼°ææ¨ï¼å¨é¢ä¸å´è¬¹å°è©ä¼°æ°çéºå¤±å¼æè£æ¹æ³ã</paragraph>

##### **SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories**
2409.07440v1 by Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot

Given that Large Language Models (LLMs) have made significant progress in
writing code, can they now be used to autonomously reproduce results from
research repositories? Such a capability would be a boon to the research
community, helping researchers validate, understand, and extend prior work. To
advance towards this goal, we introduce SUPER, the first benchmark designed to
evaluate the capability of LLMs in setting up and executing tasks from research
repositories. SUPERaims to capture the realistic challenges faced by
researchers working with Machine Learning (ML) and Natural Language Processing
(NLP) research repositories. Our benchmark comprises three distinct problem
sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems
derived from the expert set that focus on specific challenges (e.g.,
configuring a trainer), and 602 automatically generated problems for
larger-scale development. We introduce various evaluation measures to assess
both task success and progress, utilizing gold solutions when available or
approximations otherwise. We show that state-of-the-art approaches struggle to
solve these problems with the best model (GPT-4o) solving only 16.3% of the
end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of
this task, and suggests that SUPER can serve as a valuable resource for the
community to make and measure progress.

æè¦ï¼é´äºå¤§åè¯­è¨æ¨¡å (LLM) å¨ç¼åä»£ç æ¹é¢åå¾äºæ¾èè¿å±ï¼å®ä»¬ç°å¨å¯å¦ç¨äºèªä¸»å¤å¶ç ç©¶å­å¨åºä¸­çç»æï¼è¿ç§è½åå°å¯¹ç ç©¶çå¤§æè£¨çï¼æå©äºç ç©¶äººåéªè¯ãçè§£åæ©å±ååçå·¥ä½ãä¸ºäºå®ç°è¿ä¸ç®æ ï¼æä»¬å¼å¥äº SUPERï¼è¿æ¯ç¬¬ä¸ä¸ªæ¨å¨è¯ä¼° LLM å¨è®¾ç½®åæ§è¡ç ç©¶å­å¨åºä¸­çä»»å¡çè½åçåºåãSUPER æ¨å¨è§£å³ç ç©¶äººåå¨ä½¿ç¨æºå¨å­¦ä¹  (ML) åèªç¶è¯­è¨å¤ç (NLP) ç ç©¶å­å¨åºæ¶é¢ä¸´çå®éææãæä»¬çåºååå«ä¸ä¸ªä¸åçé®é¢éï¼45 ä¸ªå¸¦æä¸å®¶è§£å³æ¹æ¡æ³¨éçç«¯å°ç«¯é®é¢ã152 ä¸ªä»ä¸å®¶éä¸­æ´¾çåºæ¥çå­é®é¢ï¼è¿äºå­é®é¢ä¾§éäºç¹å®ææï¼ä¾å¦éç½®è®­ç»å¨ï¼ï¼ä»¥å 602 ä¸ªä¸ºæ´å¤§è§æ¨¡å¼åèªå¨çæçé®é¢ãæä»¬å¼å¥äºåç§è¯ä¼°æªæ½æ¥è¯ä¼°ä»»å¡æååè¿åº¦ï¼å¨æé»éè§£å³æ¹æ¡æ¶ä½¿ç¨é»éè§£å³æ¹æ¡ï¼å¦åä½¿ç¨è¿ä¼¼å¼ãæä»¬è¡¨æï¼æåè¿çæ¹æ³é¾ä»¥è§£å³è¿äºé®é¢ï¼æå¥½çæ¨¡å (GPT-4o) ä»è§£å³äº 16.3% çç«¯å°ç«¯éå 46.1% çåºæ¯ãè¿è¯´æäºè¿é¡¹ä»»å¡çæææ§ï¼å¹¶è¡¨æ SUPER å¯ä»¥ä½ä¸ºç¤¾åºååºåè¡¡éè¿æ­¥çå®è´µèµæºã

##### **A Suite for Acoustic Language Model Evaluation**
2409.07437v1 by Gallil Maimon, Amit Roth, Yossi Adi

Speech language models have recently demonstrated great potential as
universal speech processing systems. Such models have the ability to model the
rich acoustic information existing in audio signals, beyond spoken content,
such as emotion, background noise, etc. Despite this, evaluation benchmarks
which evaluate awareness to a wide range of acoustic aspects, are lacking. To
help bridge this gap, we introduce SALMon, a novel evaluation suite
encompassing background noise, emotion, speaker identity and room impulse
response. The proposed benchmarks both evaluate the consistency of the
inspected element and how much it matches the spoken text. We follow a
modelling based approach, measuring whether a model gives correct samples
higher scores than incorrect ones. This approach makes the benchmark fast to
compute even for large models. We evaluated several speech language models on
SALMon, thus highlighting the strengths and weaknesses of each evaluated
method. Code and data are publicly available at
https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .

æè¦ï¼èªé³èªè¨æ¨¡åæè¿å·²å±ç¾åºä½çºéç¨èªé³èçç³»çµ±çå·¨å¤§æ½åãæ­¤é¡æ¨¡åæè½åå°é³è¨è¨èä¸­å­å¨çè±å¯è²å­¸è³è¨é²è¡å»ºæ¨¡ï¼é¤äºå£èªªå§å®¹ä¹å¤ï¼ä¾å¦æç·ãèæ¯åªé³ç­ãåç®¡å¦æ­¤ï¼è©ä¼°åºæºæè©ä¼°å°å»£æ³è²å­¸å±¤é¢çèªè­ï¼å»æææ¬ ç¼ºãçºäºå¹«å©å½è£æ­¤å·®è·ï¼æåå¼é² SALMonï¼ä¸åæ°ç©çè©ä¼°å¥ä»¶ï¼åå«èæ¯åªé³ãæç·ãèªªè©±èèº«ååæ¿éèè¡é¿æãå»ºè­°çåºæºåæè©ä¼°ææª¢æ¥åç´ çä¸è´æ§ï¼ä»¥åå®èå£èªªæå­çå¹éç¨åº¦ãæåæ¡ç¨åºæ¼å»ºæ¨¡çæ¹æ³ï¼æ¸¬éæ¨¡åæ¯å¦çµ¦äºæ­£ç¢ºæ¨£æ¬é«æ¼ä¸æ­£ç¢ºæ¨£æ¬çåæ¸ãæ­¤æ¹æ³è®åºæºå³ä½¿å°æ¼å¤§åæ¨¡åä¹è½å¿«ééç®ãæåå¨ SALMon ä¸è©ä¼°äºæ¸åèªé³èªè¨æ¨¡åï¼å¾èçªé¡¯æ¯åè©ä¼°æ¹æ³çåªç¼ºé»ãç¨å¼ç¢¼åè³æå·²å¬éæ¼ https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ã

##### **Synthetic continued pretraining**
2409.07431v1 by Zitong Yang, Neil Band, Shuangping Li, Emmanuel CandÃ¨s, Tatsunori Hashimoto

Pretraining on large-scale, unstructured internet text has enabled language
models to acquire a significant amount of world knowledge. However, this
knowledge acquisition is data-inefficient -- to learn a given fact, models must
be trained on hundreds to thousands of diverse representations of it. This
poses a challenge when adapting a pretrained model to a small corpus of
domain-specific documents, where each fact may appear rarely or only once. We
propose to bridge this gap with synthetic continued pretraining: using the
small domain-specific corpus to synthesize a large corpus more amenable to
learning, and then performing continued pretraining on the synthesized corpus.
We instantiate this proposal with EntiGraph, a synthetic data augmentation
algorithm that extracts salient entities from the source documents and then
generates diverse text by drawing connections between the sampled entities.
Synthetic continued pretraining using EntiGraph enables a language model to
answer questions and follow generic instructions related to the source
documents without access to them. If instead, the source documents are
available at inference time, we show that the knowledge acquired through our
approach compounds with retrieval-augmented generation. To better understand
these results, we build a simple mathematical model of EntiGraph, and show how
synthetic data augmentation can "rearrange" knowledge to enable more
data-efficient learning.

æè¦ï¼å¨è§æ¨¡åºå¤§ãç»ææ¾æ£çäºèç½ææ¬ä¸è¿è¡é¢è®­ç»ï¼ä½¿è¯­è¨æ¨¡åè½å¤è·åå¤§éçä¸çç¥è¯ãç¶èï¼è¿ç§ç¥è¯è·åæçä½ä¸ââä¸ºäºå­¦ä¹ ä¸ä¸ªç»å®çäºå®ï¼æ¨¡åå¿é¡»æ¥åæ°ç¾å°æ°åä¸ªä¸åè¡¨ç¤ºå½¢å¼çè®­ç»ãå½å°é¢è®­ç»æ¨¡åéåºå°ä¸ä¸ªåå«ç¹å®é¢åææ¡£çå°è¯­æåºæ¶ï¼è¿ä¼å¸¦æ¥ä¸ä¸ªææï¼å ä¸ºæ¯ä¸ªäºå®å¯è½å¾å°åºç°æåªåºç°ä¸æ¬¡ãæä»¬å»ºè®®ç¨åææç»­é¢è®­ç»æ¥å¼¥åçè®ºå·®è·ï¼ä½¿ç¨ç¹å®é¢åçå°è¯­æåºæ¥åæä¸ä¸ªæ´å®¹æå­¦ä¹ çå¤§è¯­æåºï¼ç¶åå¯¹åæçè¯­æåºè¿è¡æç»­é¢è®­ç»ãæä»¬ç¨ EntiGraph å®ä¾åäºæ­¤æè®®ï¼è¿æ¯ä¸ç§åææ°æ®æ©åç®æ³ï¼å®ä»æºææ¡£ä¸­æåæ¾èå®ä½ï¼ç¶åéè¿ç»å¶æ½æ ·å®ä½ä¹é´çèç³»æ¥çæä¸åçææ¬ãä½¿ç¨ EntiGraph è¿è¡åææç»­é¢è®­ç»ä½¿è¯­è¨æ¨¡åè½å¤åç­é®é¢å¹¶éµå¾ªä¸æºææ¡£ç¸å³çéç¨è¯´æï¼èæ éè®¿é®å®ä»¬ãå¦æç¸åï¼æºææ¡£å¨æ¨çæ¶å¯ç¨ï¼æä»¬è¡¨æéè¿æä»¬çæ¹æ³è·å¾çç¥è¯ä¸æ£ç´¢å¢å¼ºçæç¸ç»åãä¸ºäºæ´å¥½å°çè§£è¿äºç»æï¼æä»¬å»ºç«äºä¸ä¸ª EntiGraph çç®åæ°å­¦æ¨¡åï¼å¹¶å±ç¤ºäºåææ°æ®æ©åå¦ä½âéæ°æåâç¥è¯ä»¥å®ç°æ´ææççæ°æ®å­¦ä¹ ã

##### **Agent Workflow Memory**
2409.07429v1 by Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, Graham Neubig

Despite the potential of language model-based agents to solve real-world
tasks such as web navigation, current methods still struggle with long-horizon
tasks with complex action trajectories. In contrast, humans can flexibly solve
complex tasks by learning reusable task workflows from past experiences and
using them to guide future actions. To build agents that can similarly benefit
from this process, we introduce Agent Workflow Memory (AWM), a method for
inducing commonly reused routines, i.e., workflows, and selectively providing
workflows to the agent to guide subsequent generations. AWM flexibly applies to
both offline and online scenarios, where agents induce workflows from training
examples beforehand or from test queries on the fly. We experiment on two major
web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover
1000+ tasks from 200+ domains across travel, shopping, and social media, among
others. AWM substantially improves the baseline results by 24.6% and 51.1%
relative success rate on Mind2Web and WebArena while reducing the number of
steps taken to solve WebArena tasks successfully. Furthermore, online AWM
robustly generalizes in cross-task, website, and domain evaluations, surpassing
baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps
widen.

æè¦ï¼åç®¡åºæ¼èªè¨æ¨¡åçä»£çç¨å¼å·æè§£æ±ºå¯¦éä¸çä»»åï¼ä¾å¦ç¶²é å°è¦½ï¼çæ½åï¼ä½ç®åçæ¹æ³ä»é£ä»¥æä»å·æè¤éåä½è»è·¡çé·æç¨ä»»åãç¸åå°ï¼äººé¡å¯ä»¥ééå¾éå»ç¶é©ä¸­å­¸ç¿å¯éè¤ä½¿ç¨çä»»åå·¥ä½æµç¨ä¸¦ä½¿ç¨å®åä¾æå°æªä¾è¡åï¼éæ´»å°è§£æ±ºè¤éä»»åãçºäºå»ºæ§å¯ä»¥å¾æ­¤éç¨ä¸­åççä»£çç¨å¼ï¼æåå¼å¥äºä»£çç¨å¼å·¥ä½æµç¨è¨æ¶é« (AWM)ï¼éæ¯ä¸ç¨®èªå°å¸¸ç¨éè¤å¸¸å¼ï¼å³å·¥ä½æµç¨ï¼ä¸¦æé¸ææ§å°æä¾å·¥ä½æµç¨çµ¦ä»£çç¨å¼ä»¥æå°å¾çºä¸ä»£çæ¹æ³ãAWM å¯éæ´»æç¨æ¼é¢ç·åç·ä¸å ´æ¯ï¼å¶ä¸­ä»£çç¨å¼äºåå¾è¨ç·´ç¯ä¾æå¾å³ææ¸¬è©¦æ¥è©¢ä¸­èªå°å·¥ä½æµç¨ãæåå¨å©åä¸»è¦çç¶²é å°è¦½åºæº -- Mind2Web å WebArena -- ä¸é²è¡å¯¦é©ï¼éäºåºæºå±åæ¶µèäºä¾èªæéãè³¼ç©åç¤¾ç¾¤åªé«ç­ 200 å¤åç¶²åç 1000 å¤é ä»»åãAWM å¤§å¹æ¹åäºåºæºçµæï¼å¨ Mind2Web å WebArena ä¸çç¸å°æåçåå¥æé«äº 24.6% å 51.1%ï¼åææ¸å°äºæåè§£æ±º WebArena ä»»åæéçæ­¥é©æ¸ãæ­¤å¤ï¼ç·ä¸ AWM å¨è·¨ä»»åãç¶²ç«åç¶²åè©ä¼°ä¸­å¼·å¥å°æ¦æ¬ï¼è¶è¶äºåºæº 8.9 è³ 14.0 åçµå°é»ï¼å çºè¨ç·´æ¸¬è©¦ä»»ååä½å·®è·æ´å¤§ã

##### **Towards Fairer Health Recommendations: finding informative unbiased samples via Word Sense Disambiguation**
2409.07424v1 by Gavin Butts, Pegah Emdad, Jethro Lee, Shannon Song, Chiman Salavati, Willmar Sosa Diaz, Shiri Dori-Hacohen, Fabricio Murai

There have been growing concerns around high-stake applications that rely on
models trained with biased data, which consequently produce biased predictions,
often harming the most vulnerable. In particular, biased medical data could
cause health-related applications and recommender systems to create outputs
that jeopardize patient care and widen disparities in health outcomes. A recent
framework titled Fairness via AI posits that, instead of attempting to correct
model biases, researchers must focus on their root causes by using AI to debias
data. Inspired by this framework, we tackle bias detection in medical curricula
using NLP models, including LLMs, and evaluate them on a gold standard dataset
containing 4,105 excerpts annotated by medical experts for bias from a large
corpus. We build on previous work by coauthors which augments the set of
negative samples with non-annotated text containing social identifier terms.
However, some of these terms, especially those related to race and ethnicity,
can carry different meanings (e.g., "white matter of spinal cord"). To address
this issue, we propose the use of Word Sense Disambiguation models to refine
dataset quality by removing irrelevant sentences. We then evaluate fine-tuned
variations of BERT models as well as GPT models with zero- and few-shot
prompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for
bias detection, while fine-tuned BERT models generally perform well across all
evaluated metrics.

æè¦ï¼é¨èä»°è³´ç±æåå·®è³æè¨ç·´çæ¨¡åçé«é¢¨éªæç¨ç¨å¼è¶ä¾è¶å¤ï¼äººåä¹å°æ­¤è¶ä¾è¶æå°æå¿ï¼å çºéäºæ¨¡åæç¢çæåå·®çé æ¸¬ï¼èéäºé æ¸¬éå¸¸æå·å®³å°æå¼±å¢çæç¾¤ãç¹å¥æ¯ï¼æåå·®çé«çè³æå¯è½æå°è´èå¥åº·ç¸éçæç¨ç¨å¼åæ¨è¦ç³»çµ±ç¢çå±å®³çæ£ç§è­·ä¸¦æ´å¤§å¥åº·çµæå·®ç°çè¼¸åºãæè¿ä¸ååçºãéé AI éæå¬å¹³æ§ãçæ¶æ§ä¸»å¼µï¼ç ç©¶äººå¡ä¸æåè©¦ä¿®æ­£æ¨¡ååå·®ï¼èå¿é ééä½¿ç¨ AI ä¾æ¶é¤è³æåå·®ï¼é²èæ¾åºåå·®çæ ¹æºãåå°éåæ¶æ§çåç¼ï¼æåä½¿ç¨åæ¬ LLM å¨å§ç NLP æ¨¡åä¾èçé«å­¸èª²ç¨ä¸­çåå·®åµæ¸¬ï¼ä¸¦å¨ä¸åç±é«å­¸å°å®¶æ¨è¨»äº 4,105 æ®µæéçé»éæ¨æºè³æéä¸å°éäºæ¨¡åé²è¡è©ä¼°ï¼è©²è³æéä¾èªä¸ååå«å¤§éèªæåº«çåå·®ãæåå»ºç«å¨å±åä½èä¹åçå·¥ä½åºç¤ä¸ï¼è©²å·¥ä½ééåå«ç¤¾ææ¨è­ç¬¦è©å½çæªæ¨è¨»æå­ä¾æ´åè² é¢ç¯ä¾çéåãç¶èï¼å¶ä¸­æäºè©å½ï¼ç¹å¥æ¯èç¨®æåæ°æç¸éçè©å½ï¼å¯è½æå¸¶æä¸åçææï¼ä¾å¦ï¼ãèé«ç½è³ªãï¼ãçºäºè§£æ±ºéååé¡ï¼æåå»ºè­°ä½¿ç¨è©å½è¾¨ç¾©æ¨¡åä¾ç§»é¤ä¸ç¸éçå¥å­ï¼é²èæ¹åè³æéåè³ªãæ¥èï¼æåè©ä¼°å¾®èª¿å¾ç BERT æ¨¡åè®é«ï¼ä»¥åæ¡ç¨é¶æ¬¡åå°éæç¤ºç GPT æ¨¡åãæåç¼ç¾ï¼åç®¡ LLM å¨è¨±å¤ NLP ä»»åä¸è¢«èªçºæ¯ SOTAï¼ä½ä¸¦ä¸é©åç¨æ¼åå·®åµæ¸¬ï¼èå¾®èª¿å¾ç BERT æ¨¡åå¨ææè©ä¼°ææ¨ä¸éå¸¸é½æè¯å¥½çè¡¨ç¾ã

##### **Enhancing adversarial robustness in Natural Language Inference using explanations**
2409.07423v1 by Alexandros Koulakos, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou

The surge of state-of-the-art Transformer-based models has undoubtedly pushed
the limits of NLP model performance, excelling in a variety of tasks. We cast
the spotlight on the underexplored task of Natural Language Inference (NLI),
since models trained on popular well-suited datasets are susceptible to
adversarial attacks, allowing subtle input interventions to mislead the model.
In this work, we validate the usage of natural language explanation as a
model-agnostic defence strategy through extensive experimentation: only by
fine-tuning a classifier on the explanation rather than premise-hypothesis
inputs, robustness under various adversarial attacks is achieved in comparison
to explanation-free baselines. Moreover, since there is no standard strategy of
testing the semantic validity of the generated explanations, we research the
correlation of widely used language generation metrics with human perception,
in order for them to serve as a proxy towards robust NLI models. Our approach
is resource-efficient and reproducible without significant computational
limitations.

æè¦ï¼é¨èæåé²ç Transformer æ¨¡åçèèµ·ï¼ç¡çå°æ¨åäº NLP æ¨¡åæè½çæ¥µéï¼å¨åç¨®ä»»åä¸­è¡¨ç¾åºè²ãæåå°ç¦é»æ¾å¨èªç¶èªè¨æ¨ç (NLI) éåå°æªååæ¢ç´¢çä»»åä¸ï¼å çºå¨å»£åæ­¡è¿ä¸é©ç¨çè³æéä¸è¨ç·´çæ¨¡åå®¹æåå°å°ææ§æ»æï¼åè¨±å¾®å¦çè¼¸å¥å¹²é ä¾èª¤å°æ¨¡åãå¨éé å·¥ä½ä¸­ï¼æåééå»£æ³çå¯¦é©é©è­äºä½¿ç¨èªç¶èªè¨è§£éä½çºèæ¨¡åç¡éçé²ç¦¦ç­ç¥ï¼åééå¾®èª¿åé¡å¨éå°è§£éï¼èä¸æ¯åæåè¨­è¼¸å¥ï¼èæ²æè§£éçåºç·ç¸æ¯ï¼å¨åç¨®å°ææ§æ»æä¸å¯¦ç¾äºç©©å¥æ§ãæ­¤å¤ï¼ç±æ¼æ²ææ¸¬è©¦çæè§£éçèªç¾©æææ§çæ¨æºç­ç¥ï¼æåç ç©¶äºå»£æ³ä½¿ç¨çèªè¨çæææ¨èäººé¡æç¥ä¹éçéè¯æ§ï¼ä»¥ä¾¿å®åä½çºå¥å¨ NLI æ¨¡åçä»£çãæåçåæ³è³æºææä¸å¯è¤è£½ï¼æ²æé¡¯èçè¨ç®éå¶ã

##### **Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise Recommendation**
2409.07416v1 by Luo Ji, Gao Liu, Mingyang Yin, Hongxia Yang, Jingren Zhou

Modern listwise recommendation systems need to consider both long-term user
perceptions and short-term interest shifts. Reinforcement learning can be
applied on recommendation to study such a problem but is also subject to large
search space, sparse user feedback and long interactive latency. Motivated by
recent progress in hierarchical reinforcement learning, we propose a novel
framework called mccHRL to provide different levels of temporal abstraction on
listwise recommendation. Within the hierarchical framework, the high-level
agent studies the evolution of user perception, while the low-level agent
produces the item selection policy by modeling the process as a sequential
decision-making problem. We argue that such framework has a well-defined
decomposition of the outra-session context and the intra-session context, which
are encoded by the high-level and low-level agents, respectively. To verify
this argument, we implement both a simulator-based environment and an
industrial dataset-based experiment. Results observe significant performance
improvement by our method, compared with several well-known baselines. Data and
codes have been made public.

æè¦ï¼ç¾ä»£çæ¸å®æ¨è¦ç³»çµ±éè¦èæ®é·æçä½¿ç¨èæç¥åç­æçèè¶£è½ç§»ãå¼·åå­¸ç¿å¯ä»¥æç¨å¨æ¨è¦ç³»çµ±ä¸ä¾ç ç©¶æ­¤é¡åé¡ï¼ä½ä¹æåå°å»£éçæå°ç©ºéãç¨ççä½¿ç¨èåé¥åé·çäºåå»¶é²å½±é¿ãåå°å±¤ç´å¼·åå­¸ç¿çææ°é²å±åç¼ï¼æåæåºäºä¸ååçº mccHRL çæ°æ¡æ¶ï¼å¨æ¸å®æ¨è¦ä¸æä¾ä¸åå±¤ç´çæéæ½è±¡ãå¨å±¤ç´æ¡æ¶ä¸­ï¼é«å±¤ç´çä»£çç ç©¶ä½¿ç¨èæç¥çæ¼è®ï¼èä½å±¤ç´çä»£çåééå°æµç¨å»ºæ¨¡çºé åºæ±ºç­åé¡ä¾ç¢çé ç®é¸ææ¿ç­ãæåèªçºæ­¤æ¡æ¶å°è¶ææ®µèçµ¡åææ®µå§èçµ¡ææç¢ºçåè§£ï¼åå¥ç±é«å±¤ç´åä½å±¤ç´ä»£çç·¨ç¢¼ãçºäºé©è­æ­¤è«é»ï¼æååæå¯¦ä½äºåºæ¼æ¨¡æ¬å¨çç°å¢ååºæ¼ç¢æ¥­è³æéçå¯¦é©ãèå¹¾åèåçåºæºç¸æ¯ï¼çµæè§å¯å°æåçæ¹æ³æé¡¯èçæè½æåãè³æåç¨å¼ç¢¼å·²ç¶å¬éã

##### **SoK: Security and Privacy Risks of Medical AI**
2409.07415v1 by Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang

The integration of technology and healthcare has ushered in a new era where
software systems, powered by artificial intelligence and machine learning, have
become essential components of medical products and services. While these
advancements hold great promise for enhancing patient care and healthcare
delivery efficiency, they also expose sensitive medical data and system
integrity to potential cyberattacks. This paper explores the security and
privacy threats posed by AI/ML applications in healthcare. Through a thorough
examination of existing research across a range of medical domains, we have
identified significant gaps in understanding the adversarial attacks targeting
medical AI systems. By outlining specific adversarial threat models for medical
settings and identifying vulnerable application domains, we lay the groundwork
for future research that investigates the security and resilience of AI-driven
medical systems. Through our analysis of different threat models and
feasibility studies on adversarial attacks in different medical domains, we
provide compelling insights into the pressing need for cybersecurity research
in the rapidly evolving field of AI healthcare technology.

æè¦ï¼ç§æèé«ççæ´åéåäºä¸åæ°ç´åï¼ç±äººå·¥æºæ§åæ©å¨å­¸ç¿é©åçè»é«ç³»çµ±å·²æçºé«çç¢ååæåçå¿è¦çµæé¨åãéç¶éäºé²æ­¥å°æ¹åæ£èç§è­·åé«çä¿å¥æä¾æçæå¾å¤§çå¹«å©ï¼ä½å®åä¹è®ææçé«çè³æåç³»çµ±å®æ´æ§é¢è¨æ½å¨çç¶²è·¯æ»æé¢¨éªãæ¬ææ¢è¨äºäººå·¥æºæ§/æ©å¨å­¸ç¿æç¨å¨é«çä¿å¥ä¸­å¸¶ä¾çå®å¨æ§åé±ç§å¨èãééå¾¹åºæª¢è¦åé é«çé åç¾æçç ç©¶ï¼æåç¼ç¾äºå¨äºè§£éå°é«çäººå·¥æºæ§ç³»çµ±çå°ææ§æ»ææ¹é¢æé¡¯èçå·®è·ãééæ¦è¿°é«çç°å¢çç¹å®å°ææ§å¨èæ¨¡åä¸¦æ¾åºå®¹æåæ»æçæç¨é åï¼æåçºæªä¾ç ç©¶å¥ å®åºç¤ï¼æ¢è¨äººå·¥æºæ§é©åé«çç³»çµ±çå®å¨æ§èå¾©ååãééåæä¸åçå¨èæ¨¡ååéå°ä¸åé«çé åçå°ææ§æ»æå¯è¡æ§ç ç©¶ï¼æåå°äººå·¥æºæ§é«çä¿å¥æè¡å¿«éç¼å±é åä¸­ç¶²è·¯å®å¨ç ç©¶çè¿«åéæ±æä¾äºä»¤äººä¿¡æçè¦è§£ã

##### **CLNX: Bridging Code and Natural Language for C/C++ Vulnerability-Contributing Commits Identification**
2409.07407v1 by Zeqing Qin, Yiwei Wu, Lansheng Han

Large Language Models (LLMs) have shown great promise in vulnerability
identification. As C/C++ comprises half of the Open-Source Software (OSS)
vulnerabilities over the past decade and updates in OSS mainly occur through
commits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing
Commits (VCCs) is essential. However, current studies primarily focus on
further pre-training LLMs on massive code datasets, which is resource-intensive
and poses efficiency challenges. In this paper, we enhance the ability of
BERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose
CodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++
programs and LLMs. Based on commits, CLNX efficiently converts the source code
into a more natural representation while preserving key details. Specifically,
CLNX first applies structure-level naturalization to decompose complex
programs, followed by token-level naturalization to interpret complex symbols.
We evaluate CLNX on public datasets of 25,872 C/C++ functions with their
commits. The results show that CLNX significantly enhances the performance of
LLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new
state-of-the-art and identifies 38 OSS vulnerabilities in the real world.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨æ¼æ´è­å¥æ¹é¢å±ç¾åºæ¥µå¤§çåæ¯ãç±æ¼ C/C++ æ¶µèäºéå»åå¹´ä¸­ä¸åçéæºè»é« (OSS) æ¼æ´ï¼è OSS æ´æ°ä¸»è¦ééæäº¤é²è¡ï¼å æ­¤æå LLM è­å¥ C/C++ æ¼æ´è²¢ç»æäº¤ (VCC) çè½åè³ééè¦ãç¶èï¼ç®åçç ç©¶ä¸»è¦éä¸­æ¼å¨å¤§éçç¨å¼ç¢¼è³æéä¸é²ä¸æ­¥é è¨ç·´ LLMï¼ééè¦å¤§éçè³æºä¸æé ææçæ¹é¢çææ°ãå¨æ¬æä¸­ï¼æåä»¥è¼éåçæ¹å¼æå BERT-based LLM è­å¥ C/C++ VCC çè½åãæåæåº CodeLinguaNexus (CLNX) ä½çºä¿é² C/C++ ç¨å¼è LLM ä¹éæºéçæ©æ¨ãCLNX åºæ¼æäº¤ï¼å°åå§ç¨å¼ç¢¼ææå°è½æçºæ´èªç¶çè¡¨ç¤ºå½¢å¼ï¼åæä¿çééµç´°ç¯ãå·é«ä¾èªªï¼CLNX é¦åæç¨çµæ§å±¤æ¬¡èªç¶åä¾åè§£è¤éçç¨å¼ï¼ç¶å¾æç¨ä»£ç¢¼å±¤æ¬¡èªç¶åä¾è§£éè¤éçç¬¦èãæåå¨åå« 25,872 å C/C++ å½æ¸åå¶æäº¤çå¬éè³æéä¸è©ä¼° CLNXãçµæé¡¯ç¤ºï¼CLNX å¤§å¹æå LLM è­å¥ C/C++ VCC çæè½ãæ­¤å¤ï¼éå CLNX ç CodeBERT éå°æ°çæè¡æ°´æºï¼ä¸¦å¨çå¯¦ä¸çä¸­è­å¥åº 38 å OSS æ¼æ´ã

##### **AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge**
2409.07394v1 by Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal

Knowledge conflict arises from discrepancies between information in the
context of a large language model (LLM) and the knowledge stored in its
parameters. This can hurt performance when using standard decoding techniques,
which tend to ignore the context. Existing test-time contrastive methods seek
to address this by comparing the LLM's output distribution with and without the
context and adjust the model according to the contrast between them. However,
we find that these methods frequently misjudge the degree of conflict and
struggle to handle instances that vary in their amount of conflict, with static
methods over-adjusting when conflict is absent. We propose a fine-grained,
instance-level approach called AdaCAD, which dynamically infers the weight of
adjustment based on the degree of conflict, as measured by the Jensen-Shannon
divergence between distributions representing contextual and parametric
knowledge. Our experiments across four models on six diverse question-answering
(QA) datasets and three summarization tasks demonstrate that our training-free
adaptive method consistently outperforms other decoding methods on QA, with
average accuracy gains of 14.21% (absolute) over a static contrastive baseline,
and improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our
analysis shows that while decoding with contrastive baselines hurts performance
when conflict is absent, AdaCAD mitigates these losses, making it more
applicable to real-world datasets in which some examples have conflict and
others do not.

æè¦ï¼ç¥è­è¡çªæºèªæ¼å¤§åèªè¨æ¨¡å (LLM) ä¸­çè³è¨èå²å­å¨å¶åæ¸ä¸­çç¥è­ä¹éçå·®ç°ãå¨ä½¿ç¨æ¨æºè§£ç¢¼æè¡æï¼éå¯è½ææå®³æè½ï¼å çºéäºæè¡å¾å¾æå¿½ç¥ä¸ä¸æãç¾æçæ¸¬è©¦æéå°æ¯æ¹æ³æééæ¯è¼ LLM çè¼¸åºåä½ï¼æä¸ä¸æåç¡ä¸ä¸æï¼ä¾è§£æ±ºéååé¡ï¼ä¸¦æ ¹æå®åä¹éçå°æ¯èª¿æ´æ¨¡åãç¶èï¼æåç¼ç¾éäºæ¹æ³ç¶å¸¸èª¤å¤è¡çªç¨åº¦ï¼ä¸é£ä»¥èçè¡çªéä¸åçå¯¦ä¾ï¼èéææ¹æ³å¨æ²æè¡çªææéåº¦èª¿æ´ãæåæåºä¸åç¨±çº AdaCAD çç´°ç·»åãå¯¦ä¾å±¤ç´æ¹æ³ï¼å®ææ ¹æè¡çªç¨åº¦åææ¨è«èª¿æ´æ¬éï¼èè¡çªç¨åº¦æ¯ç±è¡¨ç¤ºæå¢ååæ¸ç¥è­çåä½ä¹éç Jensen-Shannon æ£åº¦ææ¸¬éãæåå¨å­åä¸åçåç­ (QA) è³æéåä¸åæè¦ä»»åä¸­ï¼éå°ååæ¨¡åæåçå¯¦é©è­æï¼æåçç¡è¨ç·´èªé©ææ¹æ³å¨ QA ä¸å§çµåªæ¼å¶ä»è§£ç¢¼æ¹æ³ï¼å¹³åæºç¢ºåº¦æ¯éæå°æ¯åºæºé«åº 14.21%ï¼çµå°å¼ï¼ï¼ä¸¦å°æè¦çäºå¯¦æ§æé« 5.59ï¼AlignScoreï¼ãæ­¤å¤ï¼æåçåæé¡¯ç¤ºï¼éç¶ä½¿ç¨å°æ¯åºæºé²è¡è§£ç¢¼å¨æ²æè¡çªæææå®³æè½ï¼ä½ AdaCAD æ¸è¼äºéäºæå¤±ï¼ä½¿å¶æ´é©ç¨æ¼æäºç¯ä¾æè¡çªèå¦ä¸äºç¯ä¾æ²æè¡çªççå¯¦ä¸çè³æéã

##### **Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring System via Language Model Coordination**
2409.07372v1 by Daniel Zhang-Li, Zheyuan Zhang, Jifan Yu, Joy Lim Jia Yin, Shangqing Tu, Linlu Gong, Haohua Wang, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li

The vast pre-existing slides serve as rich and important materials to carry
lecture knowledge. However, effectively leveraging lecture slides to serve
students is difficult due to the multi-modal nature of slide content and the
heterogeneous teaching actions. We study the problem of discovering effective
designs that convert a slide into an interactive lecture. We develop
Slide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring
system that can (1) effectively convert an input lecture slide into a
structured teaching agenda consisting of a set of heterogeneous teaching
actions; (2) create and manage an interactive lecture that generates responsive
interactions catering to student learning demands while regulating the
interactions to follow teaching actions. Slide2Lecture contains a complete
pipeline for learners to obtain an interactive classroom experience to learn
the slide. For teachers and developers, Slide2Lecture enables customization to
cater to personalized demands. The evaluation rated by annotators and students
shows that Slide2Lecture is effective in outperforming the remaining
implementation. Slide2Lecture's online deployment has made more than 200K
interaction with students in the 3K lecture sessions. We open source
Slide2Lecture's implementation in
https://anonymous.4open.science/r/slide2lecture-4210/.

æè¦ï¼æ¢æçè±å¯ç°¡å ±æå½±çï¼æ¯å³éèª²ç¨ç¥è­çéè¦ç´ æãä¸éï¼ç±æ¼ç°¡å ±æå½±ççå§å®¹å¤åï¼æå­¸è¡çºåç¸ç¶å¤åï¼è¦ææéç¨ç°¡å ±æå½±çä¾æåå­¸çï¼ç¸ç¶å°é£ãæ¬ç ç©¶æ¢è¨æ¾åºææè¨­è¨ï¼å°ç°¡å ±æå½±çè½åçºäºåå¼èª²ç¨ãæåéç¼åº Slide2Lectureï¼éæ¯ä¸åç¡éèª¿æ´åæ¸ãç¥è­å°åçæºæ§æå­¸ç³»çµ±ï¼å¯ä»¥ï¼(1) ææå°è¼¸å¥çèª²ç¨ç°¡å ±æå½±çï¼è½ææç±ä¸ç³»åå¤åæå­¸è¡çºçµæççµæ§åæå­¸è¨ç«ï¼(2) å»ºç«ä¸¦ç®¡çäºåå¼èª²ç¨ï¼ç¢çåæå¼çäºåï¼ä»¥æ»¿è¶³å­¸ççå­¸ç¿éæ±ï¼åæèª¿æ´äºåï¼ä»¥éµå¾ªæå­¸è¡çºãSlide2Lecture åå«ä¸åå®æ´çæµç¨ï¼è®å­¸ç¿èå¯ä»¥ç²å¾äºåå¼æå®¤é«é©ï¼ä»¥å­¸ç¿ç°¡å ±æå½±çãå°æ¼æå¸«åéç¼äººå¡ï¼Slide2Lecture è½å¤ å®¢è£½åï¼ä»¥æ»¿è¶³åäººéæ±ãç¶ç±è¨»è§£èåå­¸çè©åï¼Slide2Lecture çè¡¨ç¾åªæ¼å¶ä»å¯¦ä½ãSlide2Lecture çç·ä¸é¨ç½²ï¼å·²å¨ 3K å èª²ç¨ä¸­èå­¸çç¢çè¶é 20 è¬æ¬¡çäºåãæåéæ¾ Slide2Lecture çå¯¦ä½åå§ç¢¼ï¼ç¶²åçº https://anonymous.4open.science/r/slide2lecture-4210/ã

##### **Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**
2409.07368v1 by Khiem Ton, Nhi Nguyen, Mahmoud Nazzal, Abdallah Khreishah, Cristian Borcea, NhatHai Phan, Ruoming Jin, Issa Khalil, Yelong Shen

This paper introduces SGCode, a flexible prompt-optimizing system to generate
secure code with large language models (LLMs). SGCode integrates recent
prompt-optimization approaches with LLMs in a unified system accessible through
front-end and back-end APIs, enabling users to 1) generate secure code, which
is free of vulnerabilities, 2) review and share security analysis, and 3)
easily switch from one prompt optimization approach to another, while providing
insights on model and system performance. We populated SGCode on an AWS server
with PromSec, an approach that optimizes prompts by combining an LLM and
security tools with a lightweight generative adversarial graph neural network
to detect and fix security vulnerabilities in the generated code. Extensive
experiments show that SGCode is practical as a public tool to gain insights
into the trade-offs between model utility, secure code generation, and system
cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is
available at: http://3.131.141.63:8501/.

æè¦ï¼æ¬æä»ç´¹ SGCodeï¼éæ¯ä¸åå½æ§çæç¤ºæä½³åç³»çµ±ï¼ç¨æ¼ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) ç¢çå®å¨çç¨å¼ç¢¼ãSGCode å°æè¿çæç¤ºæä½³åæ¹æ³è LLM æ´åå¨ä¸åçµ±ä¸çç³»çµ±ä¸­ï¼å¯ééåç«¯åå¾ç«¯ API å­åï¼ä½¿ç¨æ¶è½å¤  1) ç¢çå®å¨çç¨å¼ç¢¼ï¼æ²ææ¼æ´ï¼2) æª¢é±ä¸¦åäº«å®å¨æ§åæï¼ä»¥å 3) è¼é¬å¾ä¸ç¨®æç¤ºæä½³åæ¹æ³åæå°å¦ä¸ç¨®æ¹æ³ï¼åææä¾æ¨¡ååç³»çµ±æè½çè¦è§£ãæåå¨ AWS ä¼ºæå¨ä¸ä½¿ç¨ PromSec å¡«å SGCodeï¼éæ¯ä¸ç¨®ééçµå LLM åå®å¨æ§å·¥å·èè¼éç´çæå°æåç¥ç¶ç¶²è·¯ä¾æä½³åæç¤ºçæ¹æ³ï¼ç¨æ¼åµæ¸¬ä¸¦ä¿®æ­£ç¢çç¨å¼ç¢¼ä¸­çå®å¨æ§æ¼æ´ãå»£æ³çå¯¦é©é¡¯ç¤ºï¼SGCode æ¯ä¸åå¯¦ç¨çå¬éå·¥å·ï¼å¯ç¨æ¼æ·±å¥äºè§£æ¨¡åæç¨ãå®å¨ç¨å¼ç¢¼ç¢çåç³»çµ±ææ¬ä¹éçæ¬è¡¡ãèæç¤º LLM ç¸æ¯ï¼SGCode åæå¾®å°çææ¬ãSGCode å¯å¨ä»¥ä¸ç¶²ååå¾ï¼http://3.131.141.63:8501/ã

##### **Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation**
2409.07355v1 by SeongYeub Chu, JongWoo Kim, MunYong Yi

This study introduces \textbf{InteractEval}, a framework that integrates
human expertise and Large Language Models (LLMs) using the Think-Aloud (TA)
method to generate attributes for checklist-based text evaluation. By combining
human flexibility and reasoning with LLM consistency, InteractEval outperforms
traditional non-LLM-based and LLM-based baselines across four distinct
dimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The
experiment also investigates the effectiveness of the TA method, showing that
it promotes divergent thinking in both humans and LLMs, leading to the
generation of a wider range of relevant attributes and enhance text evaluation
performance. Comparative analysis reveals that humans excel at identifying
attributes related to internal quality (Coherence and Fluency), but LLMs
perform better at those attributes related to external alignment (Consistency
and Relevance). Consequently, leveraging both humans and LLMs together produces
the best evaluation outcomes. In other words, this study emphasizes the
necessity of effectively combining humans and LLMs in an automated
checklist-based text evaluation framework. The code is available at
\textbf{\url{https://github.com/BBeeChu/InteractEval.git}}.

æè¦ï¼æ¬ç ç©¶å¼å¥äº **InteractEval**ï¼ä¸åæ´åäººé¡å°æ¥­ç¥è­åå¤§åèªè¨æ¨¡å (LLM) çæ¡æ¶ï¼ä½¿ç¨æèåºè² (TA) æ¹æ³çºåºæ¼æ ¸å°æ¸å®çæå­è©ä¼°ç¢çå±¬æ§ãééçµåäººé¡çéæ´»æ§èæ¨çè½åå LLM çä¸è´æ§ï¼InteractEval å¨ååä¸åçé¢åï¼åæ¬ä¸è´æ§ãæµæ¢åº¦ãé£è²«æ§åç¸éæ§ï¼ä¸åªæ¼å³çµ±çé LLM åºæºå LLM åºæºãå¯¦é©ä¹æ¢è¨äº TA æ¹æ³çæææ§ï¼é¡¯ç¤ºå®è½ä¿é²äººé¡å LLM çç¼æ£æ§æèï¼å¾èç¢çæ´å»£æ³ç¸éå±¬æ§ä¸¦æåæå­è©ä¼°çè¡¨ç¾ãæ¯è¼åæé¡¯ç¤ºï¼äººé¡ææ¼æ¾åºèå§é¨åè³ªï¼ä¸è´æ§åæµæ¢åº¦ï¼ç¸éçå±¬æ§ï¼ä½ LLM å¨èå¤é¨å°é½ï¼é£è²«æ§åç¸éæ§ï¼ç¸éçå±¬æ§ä¸è¡¨ç¾å¾æ´å¥½ãå æ­¤ï¼åæå©ç¨äººé¡å LLM è½ç¢çæä½³çè©ä¼°çµæãæå¥è©±èªªï¼æ¬ç ç©¶å¼·èª¿å¨èªååçåºæ¼æ ¸å°æ¸å®çæå­è©ä¼°æ¡æ¶ä¸­ææçµåäººé¡å LLM çå¿è¦æ§ãç¨å¼ç¢¼å¯å¨ **\url{https://github.com/BBeeChu/InteractEval.git}** åå¾ã

##### **Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks**
2409.07353v1 by Md Zarif Hossain, Ahmed Imteaj

Large Vision-Language Models (LVLMs), trained on multimodal big datasets,
have significantly advanced AI by excelling in vision-language tasks. However,
these models remain vulnerable to adversarial attacks, particularly jailbreak
attacks, which bypass safety protocols and cause the model to generate
misleading or harmful responses. This vulnerability stems from both the
inherent susceptibilities of LLMs and the expanded attack surface introduced by
the visual modality. We propose Sim-CLIP+, a novel defense mechanism that
adversarially fine-tunes the CLIP vision encoder by leveraging a Siamese
architecture. This approach maximizes cosine similarity between perturbed and
clean samples, facilitating resilience against adversarial manipulations.
Sim-CLIP+ offers a plug-and-play solution, allowing seamless integration into
existing LVLM architectures as a robust vision encoder. Unlike previous
defenses, our method requires no structural modifications to the LVLM and
incurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness
against both gradient-based adversarial attacks and various jailbreak
techniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack
strategies and perform clean evaluations using standard downstream datasets,
including COCO for image captioning and OKVQA for visual question answering.
Extensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy
while substantially improving robustness against both gradient-based
adversarial attacks and jailbreak techniques. Our code and robust vision
encoders are available at
https://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.

æè¦ï¼<paragraph>å¤§åè¦è¦ºèªè¨æ¨¡å (LVLMs) æ¥åéå¤æ¨¡æå¤§åè³æéçè¨ç·´ï¼å¨è¦è¦ºèªè¨ä»»åä¸­è¡¨ç¾åºè²ï¼å¤§å¹æåäºäººå·¥æºæ§ãç¶èï¼éäºæ¨¡åä»ç¶å®¹æåå°å°ææ§æ»æï¼ç¹å¥æ¯è¶çæ»æï¼éæç¹éå®å¨åå®ä¸¦å°è´æ¨¡åç¢çèª¤å°ææå®³çåæãæ­¤æ¼æ´æºæ¼ LLM æ¬èº«çæåæ»ææ§ï¼ä»¥åè¦è¦ºæ¨¡ææå¸¶ä¾çæ´åæ»æé¢ãæåæåº Sim-CLIP+ï¼éæ¯ä¸ç¨®æ°ç©çé²ç¦¦æ©å¶ï¼ééå©ç¨ Siamese æ¶æ§å°ææ§å¾®èª¿ CLIP è¦è¦ºç·¨ç¢¼å¨ãæ­¤æ¹æ³æå¤§åæ¾ååä¹¾æ·¨æ¨£æ¬ä¹éçé¤å¼¦ç¸ä¼¼æ§ï¼ä»¥å©æ¼å°æå°ææ§æä½ãSim-CLIP+ æä¾å³æå³ç¨è§£æ±ºæ¹æ¡ï¼å¯ä½çºå¼·å¥çè¦è¦ºç·¨ç¢¼å¨ç¡ç¸«æ´åè³ç¾æç LVLM æ¶æ§ãèååçé²ç¦¦æªæ½ä¸åï¼æåçåæ³ä¸éè¦å° LVLM é²è¡çµæ§ä¿®æ¹ï¼ä¸éç®è² ææ¥µå°ãSim-CLIP+ å·²è­å¯¦å°ææ¢¯åº¦å¼å°ææ§æ»æååç¨®è¶çæè¡ææãæåéå°ä¸ç¨®ä¸åçè¶çæ»æç­ç¥è©ä¼° Sim-CLIP+ï¼ä¸¦ä½¿ç¨æ¨æºä¸æ¸¸è³æéï¼åæ¬ç¨æ¼å½±åæ¨é¡ç COCO åç¨æ¼è¦è¦ºåé¡è§£ç­ç OKVQAï¼å·è¡ä¹¾æ·¨è©ä¼°ãå»£æ³çå¯¦é©è­æï¼Sim-CLIP+ å¨å¤§å¹æåå°ææ¢¯åº¦å¼å°ææ§æ»æåè¶çæè¡çå¼·å¥æ§çåæï¼ç¶­æé«åº¦çä¹¾æ·¨æºç¢ºåº¦ãæåçç¨å¼ç¢¼åå¼·å¥çè¦è¦ºç·¨ç¢¼å¨å¯å¨ https://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git åå¾ã</paragraph>

##### **Federated Impression for Learning with Distributed Heterogeneous Data**
2409.07351v1 by Sana Ayromlou, Atrin Arya, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li

Standard deep learning-based classification approaches may not always be
practical in real-world clinical applications, as they require a centralized
collection of all samples. Federated learning (FL) provides a paradigm that can
learn from distributed datasets across clients without requiring them to share
data, which can help mitigate privacy and data ownership issues. In FL,
sub-optimal convergence caused by data heterogeneity is common among data from
different health centers due to the variety in data collection protocols and
patient demographics across centers. Through experimentation in this study, we
show that data heterogeneity leads to the phenomenon of catastrophic forgetting
during local training. We propose FedImpres which alleviates catastrophic
forgetting by restoring synthetic data that represents the global information
as federated impression. To achieve this, we distill the global model resulting
from each communication round. Subsequently, we use the synthetic data
alongside the local data to enhance the generalization of local training.
Extensive experiments show that the proposed method achieves state-of-the-art
performance on both the BloodMNIST and Retina datasets, which contain label
imbalance and domain shift, with an improvement in classification accuracy of
up to 20%.

æè¦ï¼æ¨æºçæ·±åº¦å­¸ç¿åé¡æ¹æ³å¨å¯¦éçè¨åºæç¨ä¸­å¯è½ä¸¦ä¸ç¸½æ¯å¯¦ç¨çï¼å çºå®åéè¦éä¸­æ¶éæææ¨£æ¬ãè¯é¦å­¸ç¿ (FL) æä¾äºä¸åç¯ä¾ï¼å¯ä»¥å¨ä¸è®å®¢æ¶ç«¯åäº«æ¸æçææ³ä¸å¾åå¸å¼æ¸æéå­¸ç¿ï¼éæå©æ¼æ¸è¼é±ç§åæ¸ææææ¬åé¡ãå¨ FL ä¸­ï¼ç±æ¼ä¸åé«çä¸­å¿çæ¸ææ¶éåå®åæ£èäººå£çµ±è¨è³æçå·®ç°ï¼ä¾èªä¸åé«çä¸­å¿çæ¸æä¹éå¸¸è¦çæ¸æç°è³ªæ§æå°è´æ¬¡æä½³æ¶æãééæ¬ç ç©¶ä¸­çå¯¦é©ï¼æåè¡¨ææ¸æç°è³ªæ§æå°è´å±é¨è¨ç·´æéç¼çç½é£æ§éºå¿ç¾è±¡ãæåæåº FedImpresï¼å®éééåè¡¨ç¤ºå¨çè³è¨çåæè³æä½çºè¯é¦å°è±¡ä¾æ¸è¼ç½é£æ§éºå¿ãçºæ­¤ï¼æåæçåºæ¯ä¸è¼ªéè¨æç¢ççå¨çæ¨¡åãé¨å¾ï¼æåä½¿ç¨åæè³æåæ¬å°è³æä¾å¢å¼·æ¬å°è¨ç·´çæ¦æ¬æ§ãå»£æ³çå¯¦é©è¡¨æï¼ææåºçæ¹æ³å¨ BloodMNIST å Retina æ¸æéä¸é½éå°äºæåé²çæè½ï¼éäºæ¸æéåå«æ¨ç±¤ä¸å¹³è¡¡åé åè½ç§»ï¼åé¡æºç¢ºåº¦æé«äº 20%ã

##### **Online Decision MetaMorphFormer: A Casual Transformer-Based Reinforcement Learning Framework of Universal Embodied Intelligence**
2409.07341v1 by Luo Ji, Runji Lin

Interactive artificial intelligence in the motion control field is an
interesting topic, especially when universal knowledge is adaptive to multiple
tasks and universal environments. Despite there being increasing efforts in the
field of Reinforcement Learning (RL) with the aid of transformers, most of them
might be limited by the offline training pipeline, which prohibits exploration
and generalization abilities. To address this limitation, we propose the
framework of Online Decision MetaMorphFormer (ODM) which aims to achieve
self-awareness, environment recognition, and action planning through a unified
model architecture. Motivated by cognitive and behavioral psychology, an ODM
agent is able to learn from others, recognize the world, and practice itself
based on its own experience. ODM can also be applied to any arbitrary agent
with a multi-joint body, located in different environments, and trained with
different types of tasks using large-scale pre-trained datasets. Through the
use of pre-trained datasets, ODM can quickly warm up and learn the necessary
knowledge to perform the desired task, while the target environment continues
to reinforce the universal policy. Extensive online experiments as well as
few-shot and zero-shot environmental tests are used to verify ODM's performance
and generalization ability. The results of our study contribute to the study of
general artificial intelligence in embodied and cognitive fields. Code,
results, and video examples can be found on the website
\url{https://rlodm.github.io/odm/}.

æè¦ï¼éåæ§å¶é åçäºåå¼äººå·¥æºæ§æ¯ä¸åæè¶£çè­°é¡ï¼ç¹å¥æ¯å¨éç¨ç¥è­é©ç¨æ¼å¤é ä»»ååéç¨ç°å¢æãåç®¡å¨å¼·åå­¸ç¿ï¼RLï¼é åä¸­ï¼å¨Transformerçå¹«å©ä¸ä»åºäºè¶ä¾è¶å¤çåªåï¼ä½å¶ä¸­å¤§é¨åå¯è½åå°é¢ç·è¨ç·´ç®¡ç·çéå¶ï¼éæç¦æ­¢æ¢ç´¢åæ¦æ¬è½åãçºäºè§£æ±ºéåéå¶ï¼æåæåºäºç·ä¸æ±ºç­ MetaMorphFormerï¼ODMï¼æ¶æ§ï¼å¶ç®æ¨æ¯ééçµ±ä¸çæ¨¡åæ¶æ§ä¾å¯¦ç¾èªææè­ãç°å¢è­å¥ååä½è¦åãå¨èªç¥åè¡çºå¿çå­¸çæ¿åµä¸ï¼ODM ä»£çè½å¤ å¾ä»äººå­¸ç¿ãèªè­ä¸çä¸¦æ ¹æèªå·±çç¶é©ç·´ç¿ãODM ä¹å¯ä»¥æç¨æ¼ä»»ä½å·æå¤éç¯èº«é«ãä½æ¼ä¸åç°å¢ä¸­çä»»æä»£çï¼ä¸¦ä½¿ç¨å¤§åé åè¨ç·´çè³æéè¨ç·´ä¸åé¡åçä»»åãééä½¿ç¨é åè¨ç·´çè³æéï¼ODM å¯ä»¥å¿«éç±èº«ä¸¦å­¸ç¿å·è¡æéä»»åçå¿è¦ç¥è­ï¼èç®æ¨ç°å¢ææçºå¼·åéç¨æ¿ç­ãå»£æ³çç·ä¸å¯¦é©ä»¥åå°æ¬¡åè©¦åé¶æ¬¡åè©¦çç°å¢æ¸¬è©¦ç¨æ¼é©è­ ODM çæè½åæ¦æ¬è½åãæåç ç©¶ççµææå©æ¼å·èº«åèªç¥é åä¸­éç¨äººå·¥æºæ§çç ç©¶ãç¨å¼ç¢¼ãçµæåå½±çç¯ä¾å¯ä»¥å¨ç¶²ç«ä¸æ¾å°ï¼\url{https://rlodm.github.io/odm/}ã

##### **Explanation, Debate, Align: A Weak-to-Strong Framework for Language Model Generalization**
2409.07335v1 by Mehrdad Zakershahrak, Samira Ghodratnama

The rapid advancement of artificial intelligence systems has brought the
challenge of AI alignment to the forefront of research, particularly in complex
decision-making and task execution. As these systems surpass human-level
performance in sophisticated problems, ensuring their alignment with human
values, intentions, and ethical guidelines becomes crucial. Building on
previous work in explanation generation for human-agent alignment, we address
the more complex dynamics of multi-agent systems and human-AI teams. This paper
introduces a novel approach to model alignment through weak-to-strong
generalization in the context of language models. We present a framework where
a strong model facilitates the improvement of a weaker model, bridging the gap
between explanation generation and model alignment. Our method, formalized as a
facilitation function, allows for the transfer of capabilities from advanced
models to less capable ones without direct access to extensive training data.
Our results suggest that this facilitation-based approach not only enhances
model performance but also provides insights into the nature of model alignment
and the potential for scalable oversight of AI systems.

æè¦ï¼äººå·¥æºæ§ç³»çµ±çå¿«éé²æ­¥ï¼è® AI å°é½çææ°æçºç ç©¶çæåç·ï¼ç¹å¥æ¯å¨è¤éçæ±ºç­å¶å®åä»»åå·è¡ä¸­ãç±æ¼éäºç³»çµ±å¨è¤éåé¡ä¸­è¶è¶äººé¡å±¤ç´çè¡¨ç¾ï¼ç¢ºä¿å®åèäººé¡å¹å¼è§ãæååéå¾·æºåä¿æä¸è´è®å¾è³ééè¦ãå»ºç«å¨ååäººé¡ä»£çå°é½çè§£éç¢çå·¥ä½åºç¤ä¸ï¼æåæ¢è¨äºå¤ä»£çç³»çµ±åäººé¡ AI åéæ´è¤éçåæãæ¬æä»ç´¹äºä¸ç¨®ééèªè¨æ¨¡åä¸­å¾å¼±å°å¼·æ¦æ¬ä¾å»ºæ¨¡å°é½çæ°æ¹æ³ãæåæåºäºä¸åæ¶æ§ï¼å¶ä¸­å¼·å¤§çæ¨¡åä¿é²äºè¼å¼±æ¨¡åçæ¹é²ï¼ç¸®å°äºè§£éç¢çåæ¨¡åå°é½ä¹éçå·®è·ãæåçéåæ¹æ³å½¢å¼åçºä¸åä¿é²åè½ï¼åè¨±å¾é²éæ¨¡åå°è½åè½ç§»å°è¼å¼±çæ¨¡åï¼èç¡éç´æ¥å­åå»£æ³çè¨ç·´è³æãæåççµæè¡¨æï¼éç¨®åºæ¼ä¿é²çæ¹æ³ä¸åå¢å¼·äºæ¨¡åæè½ï¼éæä¾äºå°æ¨¡åå°é½çæ¬è³ªå AI ç³»çµ±å¯æ´åç£ç£çæ½åçè¦è§£ã

##### **Module-wise Adaptive Adversarial Training for End-to-end Autonomous Driving**
2409.07321v1 by Tianyuan Zhang, Lu Wang, Jiaqi Kang, Xinwei Zhang, Siyuan Liang, Yuwei Chen, Aishan Liu, Xianglong Liu

Recent advances in deep learning have markedly improved autonomous driving
(AD) models, particularly end-to-end systems that integrate perception,
prediction, and planning stages, achieving state-of-the-art performance.
However, these models remain vulnerable to adversarial attacks, where
human-imperceptible perturbations can disrupt decision-making processes. While
adversarial training is an effective method for enhancing model robustness
against such attacks, no prior studies have focused on its application to
end-to-end AD models. In this paper, we take the first step in adversarial
training for end-to-end AD models and present a novel Module-wise Adaptive
Adversarial Training (MA2T). However, extending conventional adversarial
training to this context is highly non-trivial, as different stages within the
model have distinct objectives and are strongly interconnected. To address
these challenges, MA2T first introduces Module-wise Noise Injection, which
injects noise before the input of different modules, targeting training models
with the guidance of overall objectives rather than each independent module
loss. Additionally, we introduce Dynamic Weight Accumulation Adaptation, which
incorporates accumulated weight changes to adaptively learn and adjust the loss
weights of each module based on their contributions (accumulated reduction
rates) for better balance and robust training. To demonstrate the efficacy of
our defense, we conduct extensive experiments on the widely-used nuScenes
dataset across several end-to-end AD models under both white-box and black-box
attacks, where our method outperforms other baselines by large margins
(+5-10%). Moreover, we validate the robustness of our defense through
closed-loop evaluation in the CARLA simulation environment, showing improved
resilience even against natural corruption.

æè¦ï¼<paragraph>æ·±åº¦å­¸ç¿çææ°é²å±é¡¯èæ¹åäºèªåé§é§ (AD) æ¨¡åï¼ç¹å¥æ¯æ´åæç¥ãé æ¸¬åè¦åéæ®µçç«¯å°ç«¯ç³»çµ±ï¼éå°äºæåé²çæè½ã
ç¶èï¼éäºæ¨¡åä»ç¶å®¹æåå°å°ææ§æ»æï¼å¶ä¸­äººé¡ç¡æ³å¯è¦ºçæ¾åæç ´å£æ±ºç­å¶å®éç¨ãéç¶å°ææ§è¨ç·´æ¯ä¸ç¨®å¢å¼·æ¨¡åå°ææ­¤é¡æ»æçæææ¹æ³ï¼ä½æ²æååçç ç©¶å°æ³¨æ¼å°å¶æç¨æ¼ç«¯å°ç«¯ AD æ¨¡åãå¨æ¬æä¸­ï¼æåæ¡åäºç«¯å°ç«¯ AD æ¨¡åå°ææ§è¨ç·´çç¬¬ä¸æ­¥ï¼ä¸¦æåºäºä¸ç¨®æ°ç©çæ¨¡çµåèªé©æå°ææ§è¨ç·´ (MA2T)ãç¶èï¼å°å³çµ±çå°ææ§è¨ç·´æ´å±å°éåèæ¯ä¸éå¸¸å°é£ï¼å çºæ¨¡åä¸­çä¸åéæ®µæä¸åçç®æ¨ï¼ä¸¦ä¸ç·å¯ç¸é£ãçºäºæå°éäºææ°ï¼MA2T é¦åå¼å¥äºæ¨¡çµåéè¨æ³¨å¥ï¼å®å¨ä¸åæ¨¡çµçè¼¸å¥ä¹åæ³¨å¥éè¨ï¼ä»¥æ´é«ç®æ¨çæå°è¨ç·´æ¨¡åï¼èä¸æ¯æ¯åç¨ç«æ¨¡çµçæå¤±ãæ­¤å¤ï¼æåå¼å¥äºåææ¬éç´¯ç©é©æï¼å®çµåäºç´¯ç©çæ¬éè®åï¼æ ¹æå¶è²¢ç»ï¼ç´¯ç©çæ¸å°çï¼èªé©æå°å­¸ç¿åèª¿æ´æ¯åæ¨¡çµçæå¤±æ¬éï¼ä»¥ç²å¾æ´å¥½çå¹³è¡¡åç©©å¥çè¨ç·´ãçºäºè­ææåé²ç¦¦çæææ§ï¼æåå¨å»£æ³ä½¿ç¨ç nuScenes è³æéä¸å°å¤åç«¯å°ç«¯ AD æ¨¡åé²è¡äºå»£æ³çå¯¦é©ï¼å¨ç½çåé»çæ»æä¸ï¼æåçæ¨¡åä»¥å¾å¤§çå¹åº¦ï¼+5-10%ï¼åªæ¼å¶ä»åºç·ãæ­¤å¤ï¼æåééå¨ CARLA æ¨¡æ¬ç°å¢ä¸­é²è¡éç°è©ä¼°é©è­äºæåé²ç¦¦çç©©å¥æ§ï¼å³ä½¿é¢å°èªç¶ç ´å£ï¼ä¹é¡¯ç¤ºåºæ¹é²çå¾©ååã</paragraph>

##### **MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**
2409.07314v1 by Praveen K Kanithi, ClÃ©ment Christophe, Marco AF Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan

The rapid development of Large Language Models (LLMs) for healthcare
applications has spurred calls for holistic evaluation beyond frequently-cited
benchmarks like USMLE, to better reflect real-world performance. While
real-world assessments are valuable indicators of utility, they often lag
behind the pace of LLM evolution, likely rendering findings obsolete upon
deployment. This temporal disconnect necessitates a comprehensive upfront
evaluation that can guide model selection for specific clinical applications.
We introduce MEDIC, a framework assessing LLMs across five critical dimensions
of clinical competence: medical reasoning, ethics and bias, data and language
understanding, in-context learning, and clinical safety. MEDIC features a novel
cross-examination framework quantifying LLM performance across areas like
coverage and hallucination detection, without requiring reference outputs. We
apply MEDIC to evaluate LLMs on medical question-answering, safety,
summarization, note generation, and other tasks. Our results show performance
disparities across model sizes, baseline vs medically finetuned models, and
have implications on model selection for applications requiring specific model
strengths, such as low hallucination or lower cost of inference. MEDIC's
multifaceted evaluation reveals these performance trade-offs, bridging the gap
between theoretical capabilities and practical implementation in healthcare
settings, ensuring that the most promising models are identified and adapted
for diverse healthcare applications.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨é«çä¿å¥æç¨æ¹é¢çå¿«éç¼å±ï¼ä¿ä½¿äººåå¼ç±²é²è¡æ´é«è©ä¼°ï¼è¶è¶ç¶å¸¸å¼ç¨çåºæºï¼ä¾å¦ USMLEï¼ï¼ä»¥æ´å¥½å°åæ å¯¦éæè½ãåç®¡å¯¦éè©ä¼°æ¯å¯¦ç¨æ§çå¯¶è²´ææ¨ï¼ä½å®åéå¸¸è½å¾æ¼ LLM æ¼åçéåº¦ï¼å¨é¨ç½²å¾å¯è½æä½¿ç ç©¶çµæéæãéç¨®æéä¸çè«ç¯éè¦é²è¡å¨é¢çåæè©ä¼°ï¼ä»¥æå°ç¹å®è¨åºæç¨ç¨å¼çæ¨¡åé¸æãæåå¼é² MEDICï¼ä¸åè©ä¼° LLM è·¨è¶è¨åºè½åçäºåééµé¢åçæ¶æ§ï¼é«çæ¨çãå«çååå·®ãè³æåèªè¨çè§£ãæå¢å­¸ç¿åè¨åºå®å¨æ§ãMEDIC æ¡ç¨ä¸ç¨®æ°ç©çäº¤äºå¼æª¢æ¥æ¶æ§ï¼éå LLM å¨æ¶µèç¯ååå¹»è¦ºåµæ¸¬ç­é åçæè½ï¼èä¸éè¦åèè¼¸åºãæåä½¿ç¨ MEDIC ä¾è©ä¼° LLM å¨é«çåé¡è§£ç­ãå®å¨æ§ãæè¦ãç­è¨ç¢çåå¶ä»ä»»åä¸çè¡¨ç¾ãæåççµæé¡¯ç¤ºï¼ä¸åæ¨¡åå¤§å°ãåºæºèç¶éé«çå¾®èª¿çæ¨¡åä¹éçæè½å·®ç°ï¼ä¸¦å°éè¦ç¹å®æ¨¡ååªå¢çæç¨ç¨å¼ï¼ä¾å¦ä½å¹»è¦ºæè¼ä½çæ¨è«ææ¬ï¼çæ¨¡åé¸æç¢çå½±é¿ãMEDIC çå¤é¢åè©ä¼°æ­ç¤ºäºéäºæè½æ¬è¡¡ï¼ç¸®å°äºçè«è½åèé«çä¿å¥ç°å¢ä¸­çå¯¦éå¯¦ä½ä¹éçå·®è·ï¼ç¢ºä¿æ¾åºææå¸æçæ¨¡åï¼ä¸¦éå°ä¸åçé«çä¿å¥æç¨ç¨å¼é²è¡èª¿æ´ã

##### **Exploring User-level Gradient Inversion with a Diffusion Prior**
2409.07291v1 by Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Bradley Malin, Kieran Parsons, Ye Wang

We explore user-level gradient inversion as a new attack surface in
distributed learning. We first investigate existing attacks on their ability to
make inferences about private information beyond training data reconstruction.
Motivated by the low reconstruction quality of existing methods, we propose a
novel gradient inversion attack that applies a denoising diffusion model as a
strong image prior in order to enhance recovery in the large batch setting.
Unlike traditional attacks, which aim to reconstruct individual samples and
suffer at large batch and image sizes, our approach instead aims to recover a
representative image that captures the sensitive shared semantic information
corresponding to the underlying user. Our experiments with face images
demonstrate the ability of our methods to recover realistic facial images along
with private user attributes.

æè¦ï¼æåæ¢ç´¢ä½¿ç¨èå±¤ç´æ¢¯åº¦åè½ä½çºåæ£å¼å­¸ç¿ä¸­æ°çæ»æé¢ãæåé¦åèª¿æ¥ç¾ææ»æï¼äºè§£å®åæ¨è«è¨ç·´è³æéå»ºä»¥å¤çç§äººè³è¨çè½åãå¨ç¾ææ¹æ³éå»ºåè³ªä½è½çåæ©ä¸ï¼æåæåºä¸åæ°ç©çæ¢¯åº¦åè½æ»æï¼å®æ¡ç¨å»åªæ´æ£æ¨¡åä½çºå¼·å¤§çå½±ååé©ï¼ä»¥å¢å¼·å¨å¤§æ¹æ¬¡è¨­å®ä¸­çå¾©åè½åãèå³çµ±æ»æä¸åçæ¯ï¼å³çµ±æ»ææ¨å¨éå»ºåå¥æ¨£æ¬ï¼ä¸¦å¨å¤§éçæ¹æ¬¡åå½±åå¤§å°ä¸­åè¦ï¼æåçåæ³åèæ¨å¨å¾©åä¸åä»£è¡¨æ§çå½±åï¼ææèåºå±¤ä½¿ç¨èç¸æçææå±äº«èªç¾©è³è¨ãæåå°äººèå½±åé²è¡çå¯¦é©è­æäºæåçæ¹æ³è½å¤ å¾©åé¼ççèé¨å½±åï¼ä»¥åç§äººä½¿ç¨èå±¬æ§ã

##### **Using Generative Agents to Create Tip Sheets for Investigative Data Reporting**
2409.07286v1 by Joris Veerbeek, Nicholas Diakopoulos

This paper introduces a system using generative AI agents to create tip
sheets for investigative data reporting. Our system employs three specialized
agents--an analyst, a reporter, and an editor--to collaboratively generate and
refine tips from datasets. We validate this approach using real-world
investigative stories, demonstrating that our agent-based system generally
generates more newsworthy and accurate insights compared to a baseline model
without agents, although some variability was noted between different stories.
Our findings highlight the potential of generative AI to provide leads for
investigative data reporting.

æè¦ï¼æ¬è«æä»ç´¹ä¸åç³»çµ±ï¼å©ç¨çæå¼ AI ä»£çä¾çºèª¿æ¥è³æå ±å°å»ºç«æç¤ºæ¸å®ãæåçç³»çµ±æ¡ç¨ä¸åå°æ¥­ä»£çââåæå¸«ãè¨èåç·¨è¼¯ââä¾åä½çæååªåä¾èªè³æéçæç¤ºãæåä½¿ç¨çå¯¦ä¸ççèª¿æ¥å ±å°ä¾é©è­æ­¤æ¹æ³ï¼è­ææåçåºæ¼ä»£ççç³»çµ±éå¸¸æç¢çæ¯æ²æä»£ççåºç·æ¨¡åæ´å¤ææ°èå¹å¼ä¸æºç¢ºçè¦è§£ï¼åç®¡å¨ä¸åçå ±å°ä¹éææä¸äºå·®ç°ãæåçç¼ç¾çªé¡¯äºçæå¼ AI çºèª¿æ¥è³æå ±å°æä¾ç·ç´¢çæ½åã

##### **Cross-Dialect Text-To-Speech in Pitch-Accent Language Incorporating Multi-Dialect Phoneme-Level BERT**
2409.07265v1 by Kazuki Yamauchi, Yuki Saito, Hiroshi Saruwatari

We explore cross-dialect text-to-speech (CD-TTS), a task to synthesize
learned speakers' voices in non-native dialects, especially in pitch-accent
languages. CD-TTS is important for developing voice agents that naturally
communicate with people across regions. We present a novel TTS model comprising
three sub-modules to perform competitively at this task. We first train a
backbone TTS model to synthesize dialect speech from a text conditioned on
phoneme-level accent latent variables (ALVs) extracted from speech by a
reference encoder. Then, we train an ALV predictor to predict ALVs tailored to
a target dialect from input text leveraging our novel multi-dialect
phoneme-level BERT. We conduct multi-dialect TTS experiments and evaluate the
effectiveness of our model by comparing it with a baseline derived from
conventional dialect TTS methods. The results show that our model improves the
dialectal naturalness of synthetic speech in CD-TTS.

æè¦ï¼æåæ¢è¨è·¨æ¹è¨æå­è½èªé³ (CD-TTS)ï¼éé ä»»åç¨æ¼åæå­¸ç¿èå¨éæ¯èªæ¹è¨ä¸­çè²é³ï¼ç¹å¥æ¯å¨é³é«éé³èªè¨ä¸­ãCD-TTS å°æ¼éç¼è½èä¸åå°åçäººèªç¶æºéçèªé³ä»£çéå¸¸éè¦ãæåæåºä¸åæ°ç©ç TTS æ¨¡åï¼åå«ä¸åå­æ¨¡çµï¼ä»¥å¨éåä»»åä¸­è¡¨ç¾åºè²ãæåé¦åè¨ç·´ä¸åä¸»å¹¹ TTS æ¨¡åï¼å¾ä¸ååèªé³ä¸­ç±åèç·¨ç¢¼å¨æåçé³ç´ ç´éé³æ½å¨è®æ¸ (ALV) æ¢ä»¶åçæå­ä¸­åææ¹è¨èªé³ãç¶å¾ï¼æåè¨ç·´ä¸å ALV é æ¸¬å¨ï¼å©ç¨æåæ°ç©çå¤æ¹è¨é³ç´ ç´ BERT å¾è¼¸å¥æå­ä¸­é æ¸¬é©åç®æ¨æ¹è¨ç ALVãæåé²è¡å¤æ¹è¨ TTS å¯¦é©ï¼ä¸¦ééå°æåçæ¨¡åèæºèªå³çµ±æ¹è¨ TTS æ¹æ³çåºæºé²è¡æ¯è¼ï¼ä¾è©ä¼°å¶æææ§ãçµæé¡¯ç¤ºæåçæ¨¡åæ¹åäº CD-TTS ä¸­åæèªé³çæ¹è¨èªç¶æ§ã

##### **Propaganda to Hate: A Multimodal Analysis of Arabic Memes with Multi-Agent LLMs**
2409.07246v1 by Firoj Alam, Md. Rafiul Biswas, Uzair Shah, Wajdi Zaghouani, Georgios Mikros

In the past decade, social media platforms have been used for information
dissemination and consumption. While a major portion of the content is posted
to promote citizen journalism and public awareness, some content is posted to
mislead users. Among different content types such as text, images, and videos,
memes (text overlaid on images) are particularly prevalent and can serve as
powerful vehicles for propaganda, hate, and humor. In the current literature,
there have been efforts to individually detect such content in memes. However,
the study of their intersection is very limited. In this study, we explore the
intersection between propaganda and hate in memes using a multi-agent LLM-based
approach. We extend the propagandistic meme dataset with coarse and
fine-grained hate labels. Our finding suggests that there is an association
between propaganda and hate in memes. We provide detailed experimental results
that can serve as a baseline for future studies. We will make the experimental
resources publicly available to the community.

æè¦ï¼å¨éå»åå¹´ä¸­ï¼ç¤¾ç¾¤åªé«å¹³å°å·²ç¨æ¼è³è¨å³æ­åæ¶è²»ãéç¶å¤§é¨åå§å®¹æ¯çºäºæ¨å»£å¬æ°æ°èåå¬ç¾æè­èç¼å¸ï¼ä½æäºå§å®¹æ¯çºäºèª¤å°ä½¿ç¨èèç¼å¸ãå¨æå­ãåçåå½±çç­ä¸åå§å®¹é¡åä¸­ï¼è¿·å ï¼çå å¨åçä¸çæå­ï¼ç¹å¥æ®éï¼å¯ç¨ä½å®£å³ãä»æ¨åå¹½é»çæååªä»ãå¨ç®åçæç»ä¸­ï¼å·²åè©¦åå¥åµæ¸¬è¿·å ä¸­çæ­¤é¡å§å®¹ãç¶èï¼å°å¶äº¤éçç ç©¶éå¸¸æéãå¨æ¬ç ç©¶ä¸­ï¼æåä½¿ç¨åºæ¼å¤ä»£ç LLM çæ¹æ³æ¢è¨è¿·å ä¸­å®£å³åä»æ¨ä¹éçäº¤éãæåä½¿ç¨ç²ç¥åç´°ç²åº¦çä»æ¨æ¨ç±¤æ´åå®£å³è¿·å è³æéãæåçç ç©¶çµæè¡¨æï¼è¿·å ä¸­çå®£å³åä»æ¨ä¹éå­å¨éè¯ãæåæä¾äºè©³ç´°çå¯¦é©çµæï¼å¯ä½çºæªä¾ç ç©¶çåºæºãæåå°ä½¿å¯¦é©è³æºå¬éæä¾çµ¦ç¤¾ç¾¤ã


### Medical explainable AI
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-30**|**Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**|Antonio Rago et.al.|[2408.17401v1](http://arxiv.org/abs/2408.17401v1)|null|
|**2024-08-29**|**A Survey for Large Language Models in Biomedicine**|Chong Wang et.al.|[2409.00133v1](http://arxiv.org/abs/2409.00133v1)|null|
|**2024-08-27**|**Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**|Francesco Sovrano et.al.|[2408.15121v1](http://arxiv.org/abs/2408.15121v1)|null|
|**2024-08-24**|**Towards Case-based Interpretability for Medical Federated Learning**|Laura Latorre et.al.|[2408.13626v1](http://arxiv.org/abs/2408.13626v1)|null|
|**2024-08-22**|**AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines**|Douwe J. Spaanderman et.al.|[2408.12491v1](http://arxiv.org/abs/2408.12491v1)|null|
|**2024-08-14**|**Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy**|Kimji N. Pellano et.al.|[2409.00001v1](http://arxiv.org/abs/2409.00001v1)|null|
|**2024-08-06**|**MicroXercise: A Micro-Level Comparative and Explainable System for Remote Physical Therapy**|Hanchen David Wang et.al.|[2408.11837v1](http://arxiv.org/abs/2408.11837v1)|null|
|**2024-08-05**|**The Literature Review Network: An Explainable Artificial Intelligence for Systematic Literature Reviews, Meta-analyses, and Method Development**|Joshua Morriss et.al.|[2408.05239v1](http://arxiv.org/abs/2408.05239v1)|null|
|**2024-08-05**|**Enhancing Medical Learning and Reasoning Systems: A Boxology-Based Comparative Analysis of Design Patterns**|Chi Him Ng et.al.|[2408.02709v1](http://arxiv.org/abs/2408.02709v1)|null|
|**2024-08-05**|**Bayesian Kolmogorov Arnold Networks (Bayesian_KANs): A Probabilistic Approach to Enhance Accuracy and Interpretability**|Masoud Muhammed Hassan et.al.|[2408.02706v1](http://arxiv.org/abs/2408.02706v1)|null|
|**2024-07-26**|**MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI**|Shyam Dongre et.al.|[2407.20284v1](http://arxiv.org/abs/2407.20284v1)|null|
|**2024-07-25**|**Introducing Î´-XAI: a novel sensitivity-based method for local AI explanations**|Alessandro De Carlo et.al.|[2407.18343v2](http://arxiv.org/abs/2407.18343v2)|null|
|**2024-07-24**|**Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population**|Nikolaos Ntampakis et.al.|[2407.17324v2](http://arxiv.org/abs/2407.17324v2)|null|
|**2024-07-24**|**Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition**|Michele Fiori et.al.|[2408.06352v1](http://arxiv.org/abs/2408.06352v1)|null|
|**2024-07-21**|**Explainable AI-based Intrusion Detection System for Industry 5.0: An Overview of the Literature, associated Challenges, the existing Solutions, and Potential Research Directions**|Naseem Khan et.al.|[2408.03335v1](http://arxiv.org/abs/2408.03335v1)|null|
|**2024-07-18**|**A Comparative Study on Automatic Coding of Medical Letters with Explainability**|Jamie Glen et.al.|[2407.13638v1](http://arxiv.org/abs/2407.13638v1)|[link](https://github.com/Glenj01/Medical-Coding)|
|**2024-07-09**|**Explainable AI for Enhancing Efficiency of DL-based Channel Estimation**|Abdul Karim Gizzini et.al.|[2407.07009v1](http://arxiv.org/abs/2407.07009v1)|null|
|**2024-07-07**|**Explainable AI: Comparative Analysis of Normal and Dilated ResNet Models for Fundus Disease Classification**|P. N. Karthikayan et.al.|[2407.05440v2](http://arxiv.org/abs/2407.05440v2)|null|
|**2024-07-03**|**A Survey on Trustworthiness in Foundation Models for Medical Image Analysis**|Congzhen Shi et.al.|[2407.15851v1](http://arxiv.org/abs/2407.15851v1)|null|
|**2024-07-01**|**The Impact of an XAI-Augmented Approach on Binary Classification with Scarce Data**|Ximing Wen et.al.|[2407.06206v1](http://arxiv.org/abs/2407.06206v1)|null|
|**2024-06-28**|**Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach**|Sai Krishna Revanth Vuruma et.al.|[2407.00167v1](http://arxiv.org/abs/2407.00167v1)|null|
|**2024-06-25**|**Towards Compositional Interpretability for XAI**|Sean Tull et.al.|[2406.17583v1](http://arxiv.org/abs/2406.17583v1)|null|
|**2024-06-11**|**Unlocking the Potential of Metaverse in Innovative and Immersive Digital Health**|Fatemeh Ebrahimzadeh et.al.|[2406.07114v2](http://arxiv.org/abs/2406.07114v2)|null|
|**2024-06-10**|**AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI**|K M Tawsik Jawad et.al.|[2406.06728v1](http://arxiv.org/abs/2406.06728v1)|null|
|**2024-06-10**|**Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook**|Yusif Ibrahimov et.al.|[2406.05984v1](http://arxiv.org/abs/2406.05984v1)|null|
|**2024-06-09**|**Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance**|Zhan Zhang et.al.|[2406.05746v1](http://arxiv.org/abs/2406.05746v1)|null|
|**2024-06-07**|**Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability**|Faseela Abdullakutty et.al.|[2406.12897v1](http://arxiv.org/abs/2406.12897v1)|null|
|**2024-06-07**|**Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**|Yong-Min Shin et.al.|[2406.04612v1](http://arxiv.org/abs/2406.04612v1)|[link](https://github.com/jordan7186/gatt)|
|**2024-06-04**|**Using Explainable AI for EEG-based Reduced Montage Neonatal Seizure Detection**|Dinuka Sandun Udayantha et.al.|[2406.16908v3](http://arxiv.org/abs/2406.16908v3)|[link](https://github.com/dinuka-1999/braineocare)|
|**2024-06-01**|**Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques**|Samita Bai et.al.|[2406.00532v1](http://arxiv.org/abs/2406.00532v1)|null|
|**2024-06-01**|**Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech Emotion Recognition**|Alaa Nfissi et.al.|[2406.01624v2](http://arxiv.org/abs/2406.01624v2)|[link](https://github.com/alaanfissi/unveiling-hidden-factors-explainable-ai-for-feature-boosting-in-speech-emotion-recognition)|
|**2024-05-31**|**The Explanation Necessity for Healthcare AI**|Michail Mamalakis et.al.|[2406.00216v1](http://arxiv.org/abs/2406.00216v1)|null|
|**2024-05-29**|**Interdisciplinary Expertise to Advance Equitable Explainable AI**|Chloe R. Bennett et.al.|[2406.18563v1](http://arxiv.org/abs/2406.18563v1)|null|
|**2024-05-27**|**"It depends": Configuring AI to Improve Clinical Usefulness Across Contexts**|Hubert D. ZajÄc et.al.|[2407.11978v1](http://arxiv.org/abs/2407.11978v1)|null|
|**2024-05-26**|**Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making**|Min Hun Lee et.al.|[2405.16424v1](http://arxiv.org/abs/2405.16424v1)|null|
|**2024-05-26**|**Exploring Nutritional Impact on Alzheimer's Mortality: An Explainable AI Approach**|Ziming Liu et.al.|[2405.17502v1](http://arxiv.org/abs/2405.17502v1)|null|
|**2024-05-24**|**Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone**|Catalina Gomez et.al.|[2407.11974v1](http://arxiv.org/abs/2407.11974v1)|null|
|**2024-05-23**|**Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery**|Yingying Fang et.al.|[2406.18552v1](http://arxiv.org/abs/2406.18552v1)|null|
|**2024-05-21**|**The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach**|Mohsen Jozani et.al.|[2405.13099v1](http://arxiv.org/abs/2405.13099v1)|null|
|**2024-05-17**|**ChatGPT in Classrooms: Transforming Challenges into Opportunities in Education**|Harris Bin Munawar et.al.|[2405.10645v1](http://arxiv.org/abs/2405.10645v1)|null|
|**2024-05-13**|**Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data**|Camelia Oprea et.al.|[2405.07590v1](http://arxiv.org/abs/2405.07590v1)|null|
|**2024-05-10**|**XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**|Fatemeh Nazary et.al.|[2405.06270v3](http://arxiv.org/abs/2405.06270v3)|null|
|**2024-05-09**|**To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**|Miquel MirÃ³-Nicolau et.al.|[2405.05766v1](http://arxiv.org/abs/2405.05766v1)|null|
|**2024-05-05**|**Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**|Zhusi Zhong et.al.|[2405.02815v1](http://arxiv.org/abs/2405.02815v1)|[link](https://github.com/zzs95/RSP_COVID)|
|**2024-04-26**|**Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**|Francesco Prinzi et.al.|[2405.02334v1](http://arxiv.org/abs/2405.02334v1)|null|
|**2024-04-25**|**Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**|Yunfei Ge et.al.|[2404.16957v1](http://arxiv.org/abs/2404.16957v1)|null|
|**2024-04-19**|**Explainable AI for Fair Sepsis Mortality Predictive Model**|Chia-Hsuan Chang et.al.|[2404.13139v1](http://arxiv.org/abs/2404.13139v1)|null|
|**2024-04-19**|**Multi Class Depression Detection Through Tweets using Artificial Intelligence**|Muhammad Osama Nusrat et.al.|[2404.13104v1](http://arxiv.org/abs/2404.13104v1)|[link](https://github.com/mnusrat786/masters-thesis)|
|**2024-04-19**|**COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**|Dmytro Shvetsov et.al.|[2404.12832v2](http://arxiv.org/abs/2404.12832v2)|[link](https://github.com/dmytro-shvetsov/counterfactual-search)|
|**2024-04-15**|**Hybrid Intelligence for Digital Humanities**|Victor de Boer et.al.|[2406.15374v1](http://arxiv.org/abs/2406.15374v1)|null|
|**2024-04-14**|**Ethical Framework for Responsible Foundational Models in Medical Imaging**|Abhijit Das et.al.|[2406.11868v1](http://arxiv.org/abs/2406.11868v1)|null|
|**2024-04-09**|**Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**|Milad Yousefi et.al.|[2404.07239v1](http://arxiv.org/abs/2404.07239v1)|null|
|**2024-04-06**|**Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**|Taminul Islam et.al.|[2404.04686v1](http://arxiv.org/abs/2404.04686v1)|null|
|**2024-04-05**|**Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**|Maryam Ahmed et.al.|[2404.03892v3](http://arxiv.org/abs/2404.03892v3)|null|
|**2024-03-30**|**Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives**|Xingrui Gu et.al.|[2404.00320v2](http://arxiv.org/abs/2404.00320v2)|null|
|**2024-03-26**|**Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**|Andrea Ferrario et.al.|[2403.17873v1](http://arxiv.org/abs/2403.17873v1)|null|
|**2024-03-26**|**Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**|Han Yuan et.al.|[2403.18871v1](http://arxiv.org/abs/2403.18871v1)|[link](https://github.com/han-yuan-med/template-explanation)|
|**2024-03-03**|**Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**|SÃ©amus Lankford et.al.|[2403.01580v1](http://arxiv.org/abs/2403.01580v1)|null|
|**2024-02-28**|**Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**|Yasin Sadeghi Bazargani et.al.|[2402.18600v1](http://arxiv.org/abs/2402.18600v1)|null|
|**2024-02-22**|**Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**|A. J. Karran et.al.|[2402.15027v2](http://arxiv.org/abs/2402.15027v2)|null|
|**2024-02-12**|**Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**|Aruna Mohan et.al.|[2402.09474v2](http://arxiv.org/abs/2402.09474v2)|null|
|**2024-02-05**|**Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**|Aryan Agrawal et.al.|[2402.05127v1](http://arxiv.org/abs/2402.05127v1)|null|
|**2024-01-24**|**Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**|TimothÃ©e Schmude et.al.|[2401.13324v4](http://arxiv.org/abs/2401.13324v4)|null|
|**2024-01-02**|**Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**|Vahid Ashrafimoghari et.al.|[2401.02985v1](http://arxiv.org/abs/2401.02985v1)|null|
|**2023-12-29**|**XAI for In-hospital Mortality Prediction via Multimodal ICU Data**|Xingqiao Li et.al.|[2312.17624v1](http://arxiv.org/abs/2312.17624v1)|[link](https://github.com/lixingqiao/xai-icu)|
|**2023-12-22**|**Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**|Norman Zerbe et.al.|[2401.09450v2](http://arxiv.org/abs/2401.09450v2)|null|
|**2023-12-18**|**Robust Stochastic Graph Generator for Counterfactual Explanations**|Mario Alfonso Prado-Romero et.al.|[2312.11747v2](http://arxiv.org/abs/2312.11747v2)|null|
|**2023-12-10**|**Evaluating the Utility of Model Explanations for Model Development**|Shawn Im et.al.|[2312.06032v1](http://arxiv.org/abs/2312.06032v1)|null|
|**2023-12-05**|**Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**|Manas Gaur et.al.|[2312.06798v1](http://arxiv.org/abs/2312.06798v1)|null|
|**2023-11-28**|**Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**|Jacob R. Epifano et.al.|[2311.17133v1](http://arxiv.org/abs/2311.17133v1)|null|
|**2023-11-27**|**Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**|Benjamin Keel et.al.|[2311.15719v1](http://arxiv.org/abs/2311.15719v1)|[link](https://github.com/benkeel/vae_lung_lesion_bmvc)|
|**2023-11-24**|**MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**|Nathan Blake et.al.|[2311.14471v1](http://arxiv.org/abs/2311.14471v1)|null|
|**2023-11-21**|**Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**|Robert Gorwa et.al.|[2311.12573v3](http://arxiv.org/abs/2311.12573v3)|null|
|**2023-11-20**|**Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**|Muta Tah Hira et.al.|[2311.11932v1](http://arxiv.org/abs/2311.11932v1)|null|
|**2023-11-18**|**Representing visual classification as a linear combination of words**|Shobhit Agarwal et.al.|[2311.10933v1](http://arxiv.org/abs/2311.10933v1)|[link](https://github.com/lotterlab/task_word_explainability)|
|**2023-11-03**|**Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging**|Emma A. M. Stanley et.al.|[2311.02115v2](http://arxiv.org/abs/2311.02115v2)|[link](https://github.com/estanley16/simba)|
|**2023-10-29**|**Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**|Adam White et.al.|[2310.19174v1](http://arxiv.org/abs/2310.19174v1)|null|
|**2023-10-03**|**Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**|Hossein Shreim et.al.|[2310.01828v2](http://arxiv.org/abs/2310.01828v2)|[link](https://github.com/geoaigroup/geoai-ecrs2023)|
|**2023-09-26**|**Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**|Muhammad Aurangzeb Ahmad et.al.|[2311.01463v1](http://arxiv.org/abs/2311.01463v1)|null|
|**2023-09-20**|**When to Trust AI: Advances and Challenges for Certification of Neural Networks**|Marta Kwiatkowska et.al.|[2309.11196v1](http://arxiv.org/abs/2309.11196v1)|null|
|**2023-09-19**|**Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**|Juan M. GarcÃ­a-GÃ³mez et.al.|[2309.10424v1](http://arxiv.org/abs/2309.10424v1)|null|
|**2023-09-19**|**QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**|Thanveer Shaik et.al.|[2309.10293v3](http://arxiv.org/abs/2309.10293v3)|null|
|**2023-09-18**|**Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**|Adarsa Sivaprasad et.al.|[2309.09917v1](http://arxiv.org/abs/2309.09917v1)|null|
|**2023-08-28**|**Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**|Yanjun Gao et.al.|[2308.14321v1](http://arxiv.org/abs/2308.14321v1)|null|
|**2023-08-18**|**Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**|Yun Xin Teoh et.al.|[2308.09380v1](http://arxiv.org/abs/2308.09380v1)|null|
|**2023-08-16**|**Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**|Munib Mesinovic et.al.|[2308.08407v1](http://arxiv.org/abs/2308.08407v1)|null|
|**2023-08-11**|**FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**|Karim Lekadir et.al.|[2309.12325v3](http://arxiv.org/abs/2309.12325v3)|null|
|**2023-08-10**|**Explainable AI applications in the Medical Domain: a systematic review**|Nicoletta Prentzas et.al.|[2308.05411v1](http://arxiv.org/abs/2308.05411v1)|null|
|**2023-08-01**|**Exploring the Role of Explainability in AI-Assisted Embryo Selection**|Lucia Urcelay et.al.|[2308.02534v1](http://arxiv.org/abs/2308.02534v1)|null|
|**2023-07-26**|**A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**|Timo Speith et.al.|[2307.14246v1](http://arxiv.org/abs/2307.14246v1)|null|
|**2023-07-26**|**Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**|Barnaby Crook et.al.|[2307.14239v1](http://arxiv.org/abs/2307.14239v1)|null|
|**2023-07-26**|**Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**|Henry Fraser et.al.|[2308.02047v1](http://arxiv.org/abs/2308.02047v1)|null|
|**2023-07-21**|**eXplainable Artificial Intelligence (XAI) in aging clock models**|Alena Kalyakulina et.al.|[2307.13704v3](http://arxiv.org/abs/2307.13704v3)|null|
|**2023-07-19**|**Interpreting and Correcting Medical Image Classification with PIP-Net**|Meike Nauta et.al.|[2307.10404v2](http://arxiv.org/abs/2307.10404v2)|[link](https://github.com/m-nauta/pipnet)|
|**2023-07-15**|**Explaining and visualizing black-box models through counterfactual paths**|Bastian Pfeifer et.al.|[2307.07764v3](http://arxiv.org/abs/2307.07764v3)|[link](https://github.com/pievos101/cpath)|
|**2023-07-05**|**Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**|Toygar Tanyel et.al.|[2307.02131v5](http://arxiv.org/abs/2307.02131v5)|[link](https://github.com/toygarr/counterfactual-explanations-for-medical-research)|
|**2023-06-30**|**AI and Non AI Assessments for Dementia**|Mahboobeh Parsapoor et.al.|[2307.01210v1](http://arxiv.org/abs/2307.01210v1)|null|
|**2023-06-12**|**Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**|Ruitao Xie et.al.|[2306.07306v1](http://arxiv.org/abs/2306.07306v1)|null|
|**2023-06-09**|**HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**|Rodrigo Agerri et.al.|[2306.06029v1](http://arxiv.org/abs/2306.06029v1)|null|
|**2023-06-07**|**XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**|Eli Laird et.al.|[2306.04791v1](http://arxiv.org/abs/2306.04791v1)|null|

#### Abstracts
##### **Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**
2408.17401v1 by Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni

In recent years, various methods have been introduced for explaining the
outputs of "black-box" AI models. However, it is not well understood whether
users actually comprehend and trust these explanations. In this paper, we focus
on explanations for a regression tool for assessing cancer risk and examine the
effect of the explanations' content and format on the user-centric metrics of
comprehension and trust. Regarding content, we experiment with two explanation
methods: the popular SHAP, based on game-theoretic notions and thus potentially
complex for everyday users to comprehend, and occlusion-1, based on feature
occlusion which may be more comprehensible. Regarding format, we present SHAP
explanations as charts (SC), as is conventional, and occlusion-1 explanations
as charts (OC) as well as text (OT), to which their simpler nature also lends
itself. The experiments amount to user studies questioning participants, with
two different levels of expertise (the general population and those with some
medical training), on their subjective and objective comprehension of and trust
in explanations for the outputs of the regression tool. In both studies we
found a clear preference in terms of subjective comprehension and trust for
occlusion-1 over SHAP explanations in general, when comparing based on content.
However, direct comparisons of explanations when controlling for format only
revealed evidence for OT over SC explanations in most cases, suggesting that
the dominance of occlusion-1 over SHAP explanations may be driven by a
preference for text over charts as explanations. Finally, we found no evidence
of a difference between the explanation types in terms of objective
comprehension. Thus overall, the choice of the content and format of
explanations needs careful attention, since in some contexts format, rather
than content, may play the critical role in improving user experience.

æè¦ï¼<paragraph>è¿å¹´ä¾ï¼å·²ç¶å¼é²åç¨®æ¹æ³ä¾è§£éãé»ç®±ãAI æ¨¡åçè¼¸åºãç¶èï¼ç®åä¸¦ä¸æ¸æ¥ä½¿ç¨èæ¯å¦å¯¦éçè§£åä¿¡ä»»éäºè§£éãå¨æ¬æä¸­ï¼æåå°æ³¨æ¼è©ä¼°ççé¢¨éªçåæ­¸å·¥å·çè§£éï¼ä¸¦æ¢è¨è§£éçå§å®¹åæ ¼å¼å°ä»¥ä½¿ç¨èçºä¸­å¿ççè§£åä¿¡ä»»ææ¨çå½±é¿ãéæ¼å§å®¹ï¼æåå¯¦é©äºå©ç¨®è§£éæ¹æ³ï¼æµè¡ç SHAPï¼åºæ¼åå¼è«æ¦å¿µï¼å æ­¤å°æ¼æ¥å¸¸ä½¿ç¨èä¾èªªå¯è½å¾è¤éï¼ä»¥ååºæ¼ç¹å¾µé®è½ç occlusion-1ï¼å¯è½æ´ææ¼çè§£ãéæ¼æ ¼å¼ï¼æåå° SHAP è§£éåç¾çºåè¡¨ (SC)ï¼éæ¯æ£ä¾ï¼èå° occlusion-1 è§£éåç¾çºåè¡¨ (OC) ä»¥åæå­ (OT)ï¼å¶è¼çºç°¡å®çæ§è³ªä¹é©ç¨æ¼æ­¤ãéäºå¯¦é©ç­åæ¼ä½¿ç¨èç ç©¶ï¼è©¢ååèèï¼å·æå©ç¨®ä¸åç¨åº¦çå°æ¥­ç¥è­ï¼ä¸è¬æ°ç¾åå·åä¸äºé«å­¸è¨ç·´çäººï¼ï¼ä»åå°åæ­¸å·¥å·è¼¸åºè§£éçä¸»è§åå®¢è§çè§£åä¿¡ä»»ãå¨å©é ç ç©¶ä¸­ï¼æåç¼ç¾ï¼å¨åºæ¼å§å®¹é²è¡æ¯è¼æï¼ä¸è¬ä¾èªªï¼occlusion-1 åªæ¼ SHAP è§£éï¼å¨ä¸»è§çè§£åä¿¡ä»»æ¹é¢ææé¡¯çåå¥½ãç¶èï¼å¨åæ§å¶æ ¼å¼çææ³ä¸ç´æ¥æ¯è¼è§£éï¼å¨å¤§å¤æ¸ææ³ä¸åªé¡¯ç¤º OT åªæ¼ SC è§£éçè­æï¼éè¡¨æ occlusion-1 åªæ¼ SHAP è§£éçä¸»å°å°ä½å¯è½æ¯ç±åå¥½æå­èéåè¡¨ä½çºè§£éæé©åçãæå¾ï¼æåæ²æç¼ç¾è§£éé¡åå¨å®¢è§çè§£æ¹é¢çå·®ç°è­æãå æ­¤ï¼ç¸½é«èè¨ï¼å°è§£éçå§å®¹åæ ¼å¼çé¸æéè¦ä»ç´°æ³¨æï¼å çºå¨æäºææ³ä¸ï¼æ ¼å¼èéå§å®¹ï¼å¯è½å¨æ¹åä½¿ç¨èé«é©æ¹é¢ç¼æ®ééµä½ç¨ã</paragraph>

##### **A Survey for Large Language Models in Biomedicine**
2409.00133v1 by Chong Wang, Mengyao Li, Junjun He, Zhongruo Wang, Erfan Darzi, Zan Chen, Jin Ye, Tianbin Li, Yanzhou Su, Jing Ke, Kaili Qu, Shuxin Li, Yi Yu, Pietro LiÃ², Tianyun Wang, Yu Guang Wang, Yiqing Shen

Recent breakthroughs in large language models (LLMs) offer unprecedented
natural language understanding and generation capabilities. However, existing
surveys on LLMs in biomedicine often focus on specific applications or model
architectures, lacking a comprehensive analysis that integrates the latest
advancements across various biomedical domains. This review, based on an
analysis of 484 publications sourced from databases including PubMed, Web of
Science, and arXiv, provides an in-depth examination of the current landscape,
applications, challenges, and prospects of LLMs in biomedicine, distinguishing
itself by focusing on the practical implications of these models in real-world
biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot
learning across a broad spectrum of biomedical tasks, including diagnostic
assistance, drug discovery, and personalized medicine, among others, with
insights drawn from 137 key studies. Then, we discuss adaptation strategies of
LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to
enhance their performance in specialized biomedical contexts where zero-shot
fails to achieve, such as medical question answering and efficient processing
of biomedical literature. Finally, we discuss the challenges that LLMs face in
the biomedicine domain including data privacy concerns, limited model
interpretability, issues with dataset quality, and ethics due to the sensitive
nature of biomedical data, the need for highly reliable model outputs, and the
ethical implications of deploying AI in healthcare. To address these
challenges, we also identify future research directions of LLM in biomedicine
including federated learning methods to preserve data privacy and integrating
explainable AI methodologies to enhance the transparency of LLMs.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çææ°çªç ´æä¾äºåææªæçèªç¶èªè¨çè§£åçæè½åãç¶èï¼ç¾æéæ¼çç©é«å­¸ä¸­ LLM çèª¿æ¥éå¸¸å°æ³¨æ¼ç¹å®æç¨ææ¨¡åæ¶æ§ï¼ç¼ºä¹æ´ååç¨®çç©é«å­¸é åææ°é²å±çå¨é¢åæãæ¬ç¶è¿°åºæ¼å°ä¾èª PubMedãWeb of Science å arXiv ç­æ¸æåº«ç 484 ç¯åºçç©çåæï¼æ·±å¥æ¢è¨äºçç©é«å­¸ä¸­ LLM çç¶åç¾æ³ãæç¨ãææ°ååæ¯ï¼å¶ç¹é»æ¯éæ³¨éäºæ¨¡åå¨ç¾å¯¦ä¸ççç©é«å­¸èæ¯ä¸­çå¯¦éæç¨ãé¦åï¼æåæ¢è¨äº LLM å¨å»£æ³ççç©é«å­¸ä»»åä¸­çé¶æ¬¡å­¸ç¿è½åï¼åæ¬è¨ºæ·è¼å©ãè¥ç©ç¼ç¾ååæ§åé«çç­ï¼ä¸¦å¾ 137 é ééµç ç©¶ä¸­æ±²åè¦è§£ãç¶å¾ï¼æåè¨è«äº LLM çé©æç­ç¥ï¼åæ¬å®æ¨¡æåå¤æ¨¡æ LLM çå¾®èª¿æ¹æ³ï¼ä»¥å¢å¼·å®åå¨é¶æ¬¡å­¸ç¿ç¡æ³å¯¦ç¾çå°æ¥­çç©é«å­¸èæ¯ä¸­çæ§è½ï¼ä¾å¦é«çåé¡è§£ç­åçç©é«å­¸æç»çææèçãæå¾ï¼æåè¨è«äº LLM å¨çç©é«å­¸é åé¢è¨çææ°ï¼åæ¬æ¸æé±ç§åé¡ãæ¨¡åå¯è§£éæ§æéãæ¸æéè³ªéåé¡ä»¥åç±æ¼çç©é«å­¸æ¸æçæææ§ãå°é«åº¦å¯é æ¨¡åè¼¸åºçéæ±ä»¥åå¨é«çä¿å¥ä¸­é¨ç½² AI çå«çå½±é¿èç¢ççå«çåé¡ãçºäºæå°éäºææ°ï¼æåéç¢ºå®äºçç©é«å­¸ä¸­ LLM æªä¾çç ç©¶æ¹åï¼åæ¬ç¨æ¼ä¿è­·æ¸æé±ç§çè¯åå­¸ç¿æ¹æ³ä»¥åæ´åå¯è§£é AI æ¹æ³ä»¥å¢å¼· LLM çéæåº¦ã

##### **Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**
2408.15121v1 by Francesco Sovrano, Michael Lognoul, Giulia Vilone

Significant investment and development have gone into integrating Artificial
Intelligence (AI) in medical and healthcare applications, leading to advanced
control systems in medical technology. However, the opacity of AI systems
raises concerns about essential characteristics needed in such sensitive
applications, like transparency and trustworthiness. Our study addresses these
concerns by investigating a process for selecting the most adequate Explainable
AI (XAI) methods to comply with the explanation requirements of key EU
regulations in the context of smart bioelectronics for medical devices. The
adopted methodology starts with categorising smart devices by their control
mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving
into their technology. Then, we analyse these regulations to define their
explainability requirements for the various devices and related goals.
Simultaneously, we classify XAI methods by their explanatory objectives. This
allows for matching legal explainability requirements with XAI explanatory
goals and determining the suitable XAI algorithms for achieving them. Our
findings provide a nuanced understanding of which XAI algorithms align better
with EU regulations for different types of medical devices. We demonstrate this
through practical case studies on different neural implants, from chronic
disease management to advanced prosthetics. This study fills a crucial gap in
aligning XAI applications in bioelectronics with stringent provisions of EU
regulations. It provides a practical framework for developers and researchers,
ensuring their AI innovations advance healthcare technology and adhere to legal
and ethical standards.

æè¦ï¼äººå·¥æºæ§ï¼AIï¼å¨é«çåä¿å¥æç¨ä¸­æå¥äºå¤§éçæè³åéç¼ï¼é²èå°è´é«çæè¡ä¸­çåé²æ§å¶ç³»çµ±ãç¶èï¼AI ç³»çµ±çä¸éææ§å¼ç¼äºå°æ­¤é¡æææç¨ä¸­æéåºæ¬ç¹æ§çææï¼ä¾å¦éæåº¦åå¯ä¿¡åº¦ãæåçç ç©¶ééèª¿æ¥ä¸åç¨åºä¾è§£æ±ºéäºåé¡ï¼ç¨æ¼é¸ææååçå¯è§£é AIï¼XAIï¼æ¹æ³ï¼ä»¥ç¬¦åæ­çæ³è¦å¨é«çå¨æçæºæ§åçç©é»å­å­¸ä¸­çèªªæè¦æ±ãæ¡ç¨çæ¹æ³å¾ééå¶æ§å¶æ©å¶ï¼éè¿´è·¯ãéè¿´è·¯ååéè¿´è·¯ç³»çµ±ï¼å°æºæ§åè£ç½®é²è¡åé¡ï¼ä¸¦æ·±å¥æ¢è¨å¶æè¡éå§ãç¶å¾ï¼æååæéäºæ³è¦ä»¥å®ç¾©å¶å°åç¨®è£ç½®åç¸éç®æ¨çå¯è§£éæ§è¦æ±ãåæï¼æåééå¶èªªæç®æ¨å° XAI æ¹æ³é²è¡åé¡ãéåè¨±å°æ³å¾å¯è§£éæ§è¦æ±è XAI èªªæç®æ¨ç¸å¹éï¼ä¸¦ç¢ºå®é©ç¶ç XAI æ¼ç®æ³ä¾éæå®åãæåçç ç©¶çµææä¾äºå°åªäº XAI æ¼ç®æ³æ´ç¬¦åæ­çæ³è¦ä»¥é©ç¨æ¼ä¸åé¡åçé«çå¨æçç´°ç·»çè§£ãæåééä¸åç¥ç¶æ¤å¥ç©çå¯¦éæ¡ä¾ç ç©¶ä¾è­æéä¸é»ï¼å¾æ¢æ§ç¾çç®¡çå°åé²çç¾©è¢ãéé ç ç©¶å¡«è£äºå°çç©é»å­å­¸ä¸­ç XAI æç¨èæ­çæ³è¦çå´æ ¼è¦å®ç¸ç¬¦çéè¦ç©ºç½ãå®çºéç¼äººå¡åç ç©¶äººå¡æä¾äºä¸åå¯¦ç¨çæ¶æ§ï¼ç¢ºä¿å¶ AI åµæ°è½ä¿é²é«çæè¡ä¸¦éµå®æ³å¾åéå¾·æ¨æºã

##### **Towards Case-based Interpretability for Medical Federated Learning**
2408.13626v1 by Laura Latorre, Liliana Petrychenko, Regina Beets-Tan, Taisiya Kopytova, Wilson Silva

We explore deep generative models to generate case-based explanations in a
medical federated learning setting. Explaining AI model decisions through
case-based interpretability is paramount to increasing trust and allowing
widespread adoption of AI in clinical practice. However, medical AI training
paradigms are shifting towards federated learning settings in order to comply
with data protection regulations. In a federated scenario, past data is
inaccessible to the current user. Thus, we use a deep generative model to
generate synthetic examples that protect privacy and explain decisions. Our
proof-of-concept focuses on pleural effusion diagnosis and uses publicly
available Chest X-ray data.

æè¦ï¼æåæ¢ç´¢æ·±åº¦çææ¨¡åï¼å¨é«çè¯é¦å­¸ç¿è¨­ç½®ä¸­çæåºæ¼æ¡ä¾çèªªæãééåºæ¼æ¡ä¾çå¯è§£éæ§ä¾è§£é AI æ¨¡åæ±ºç­ï¼å°æ¼å¢å ä¿¡ä»»ä¸¦åè¨± AI å¨è¨åºå¯¦åä¸­å»£æ³æ¡ç¨è³ééè¦ãç¶èï¼é«ç AI è¨ç·´ç¯ä¾æ­£è½åè¯é¦å­¸ç¿è¨­ç½®ï¼ä»¥ç¬¦åè³æä¿è­·æ³è¦ãå¨è¯é¦æå¢ä¸­ï¼éå»çè³æå°ç®åçä½¿ç¨èèè¨æ¯ç¡æ³åå¾çãå æ­¤ï¼æåä½¿ç¨æ·±åº¦çææ¨¡åä¾ç¢çä¿è­·é±ç§åè§£éæ±ºç­çåæç¯ä¾ãæåçæ¦å¿µé©è­èéæ¼è¸èç©æ¶²è¨ºæ·ï¼ä¸¦ä½¿ç¨å¬éå¯åå¾çè¸é¨ X åè³æã

##### **AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines**
2408.12491v1 by Douwe J. Spaanderman, Matthew Marzetti, Xinyi Wan, Andrew F. Scarsbrook, Philip Robinson, Edwin H. G. Oei, Jacob J. Visser, Robert Hemke, Kirsten van Langevelde, David F. Hanff, Geert J. L. H. van Leenders, Cornelis Verhoef, Dirk J. GruÃ¼hagen, Wiro J. Niessen, Stefan Klein, Martijn P. A. Starmans

Soft-tissue and bone tumours (STBT) are rare, diagnostically challenging
lesions with variable clinical behaviours and treatment approaches. This
systematic review provides an overview of Artificial Intelligence (AI) methods
using radiological imaging for diagnosis and prognosis of these tumours,
highlighting challenges in clinical translation, and evaluating study alignment
with the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AI
international consensus guidelines for trustworthy and deployable AI to promote
the clinical translation of AI methods. The review covered literature from
several bibliographic databases, including papers published before 17/07/2024.
Original research in peer-reviewed journals focused on radiology-based AI for
diagnosing or prognosing primary STBT was included. Exclusion criteria were
animal, cadaveric, or laboratory studies, and non-English papers. Abstracts
were screened by two of three independent reviewers for eligibility. Eligible
papers were assessed against guidelines by one of three independent reviewers.
The search identified 15,015 abstracts, from which 325 articles were included
for evaluation. Most studies performed moderately on CLAIM, averaging a score
of 28.9$\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\pm$2.1 out
of 30. Imaging-AI tools for STBT remain at the proof-of-concept stage,
indicating significant room for improvement. Future efforts by AI developers
should focus on design (e.g. define unmet clinical need, intended clinical
setting and how AI would be integrated in clinical workflow), development (e.g.
build on previous work, explainability), evaluation (e.g. evaluating and
addressing biases, evaluating AI against best practices), and data
reproducibility and availability (making documented code and data publicly
available). Following these recommendations could improve clinical translation
of AI methods.

æè¦ï¼è»çµç¹åéª¨éª¼è«ç¤ï¼STBTï¼æ¯ç½è¦ãè¨ºæ·å·æææ°æ§ççç¶ï¼å¶è¨åºè¡çºåæ²»çæ¹æ³åä¸ç¸åãéç¯ç³»çµ±æ§åé¡§æä¾äºä½¿ç¨æ¾å°å½±åé²è¡è¨ºæ·åé å¾çäººå·¥æºæ§ (AI) æ¹æ³çæ¦è§ï¼éé»èªªæäºè¨åºè½è­¯çææ°ï¼ä¸¦è©ä¼°ç ç©¶èé«çå½±å AI æ ¸æ¥è¡¨ (CLAIM) å FUTURE-AI å¯ä¿¡è³´ä¸å¯é¨ç½² AI çåéå±è­æºåçä¸è´æ§ï¼ä»¥ä¿é² AI æ¹æ³çè¨åºè½è­¯ãéç¯åé¡§æ¶µèäºå¹¾åæ¸ç®è³æåº«ä¸­çæç»ï¼åæ¬å¨ 2024 å¹´ 7 æ 17 æ¥ä¹åç¼è¡¨çè«æãç´å¥äºä»¥æ¾å°çºåºç¤ç AI è¨ºæ·æé å¾åç¼æ§ STBT çåè¡è©å¯©æåä¸­çåå§ç ç©¶ãæé¤æ¨æºæ¯åç©ãå±é«æå¯¦é©å®¤ç ç©¶ï¼ä»¥åéè±æè«æãæè¦ç±ä¸ä½ç¨ç«å¯©æ¥å¡ä¸­çå©ä½ç¯©é¸è³æ ¼ãåæ ¼çè«æç±ä¸ä½ç¨ç«å¯©æ¥å¡ä¸­çä¸ä½æ ¹ææºåé²è¡è©ä¼°ãæç´¢è­å¥åº 15,015 ç¯æè¦ï¼å¶ä¸­ 325 ç¯æç« è¢«ç´å¥è©ä¼°ãå¤§å¤æ¸ç ç©¶å¨ CLAIM ä¸­è¡¨ç¾ä¸­ç­ï¼å¹³åå¾åçº 53 åä¸­ç 28.9Â±7.5 åï¼ä½å¨ FUTURE-AI ä¸­è¡¨ç¾ä¸ä½³ï¼å¹³åå¾åçº 30 åä¸­ç 5.1Â±2.1 åãSTBT çå½±å AI å·¥å·ä»èæ¼æ¦å¿µé©è­éæ®µï¼è¡¨ææé¡¯èçæ¹é²ç©ºéãAI éç¼äººå¡æªä¾çåªåæéä¸­å¨è¨­è¨ï¼ä¾å¦å®ç¾©æªæ»¿è¶³çè¨åºéæ±ãé æçè¨åºç°å¢ä»¥å AI å¦ä½æ´åå°è¨åºå·¥ä½æµç¨ä¸­ï¼ãéç¼ï¼ä¾å¦å»ºç«å¨ååçå·¥ä½ãå¯è§£éæ§ï¼ãè©ä¼°ï¼ä¾å¦è©ä¼°åè§£æ±ºåå·®ãè©ä¼° AI èæä½³å¯¦åï¼ãä»¥åæ¸æå¯è¤è£½æ§åå¯ç¨æ§ï¼å¬éæä¾æä»¶åçä»£ç¢¼åæ¸æï¼ãéµå¾ªéäºå»ºè­°å¯ä»¥æ¹å AI æ¹æ³çè¨åºè½è­¯ã

##### **Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy**
2409.00001v1 by Kimji N. Pellano, Inga StrÃ¼mke, Daniel Groos, Lars Adde, Espen Alexander F. Ihlen

Early detection of Cerebral Palsy (CP) is crucial for effective intervention
and monitoring. This paper tests the reliability and applicability of
Explainable AI (XAI) methods using a deep learning method that predicts CP by
analyzing skeletal data extracted from video recordings of infant movements.
Specifically, we use XAI evaluation metrics -- namely faithfulness and
stability -- to quantitatively assess the reliability of Class Activation
Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) in this
specific medical application. We utilize a unique dataset of infant movements
and apply skeleton data perturbations without distorting the original dynamics
of the infant movements. Our CP prediction model utilizes an ensemble approach,
so we evaluate the XAI metrics performances for both the overall ensemble and
the individual models. Our findings indicate that both XAI methods effectively
identify key body points influencing CP predictions and that the explanations
are robust against minor data perturbations. Grad-CAM significantly outperforms
CAM in the RISv metric, which measures stability in terms of velocity. In
contrast, CAM performs better in the RISb metric, which relates to bone
stability, and the RRS metric, which assesses internal representation
robustness. Individual models within the ensemble show varied results, and
neither CAM nor Grad-CAM consistently outperform the other, with the ensemble
approach providing a representation of outcomes from its constituent models.

æè¦ï¼è¦æ§éº»çº (CP) çæ©æåµæ¸¬å°æ¼ææçä»å¥åç£æ¸¬è³ééè¦ãæ¬ææ¸¬è©¦äºå¯è§£é AI (XAI) æ¹æ³çå¯é æ§åé©ç¨æ§ï¼ä½¿ç¨æ·±åº¦å­¸ç¿æ¹æ³ï¼ééåæå¾å¬°ååä½å½±çè¨éä¸­æåçéª¨éª¼è³æä¾é æ¸¬ CPãå·é«ä¾èªªï¼æåä½¿ç¨ XAI è©ä¼°ææ¨ï¼å³å¿ å¯¦åº¦åç©©å®æ§ï¼ä¾éåè©ä¼°é¡å¥æ¿æ´»æ å° (CAM) åæ¢¯åº¦å æ¬é¡å¥æ¿æ´»æ å° (Grad-CAM) å¨éåç¹å®é«çæç¨ä¸­çå¯é æ§ãæåå©ç¨ä¸åç¨ç¹çå¬°ååä½è³æéï¼ä¸¦æç¨éª¨éª¼è³ææ¾åï¼èä¸ææ­æ²å¬°ååä½çåå§ååãæåç CP é æ¸¬æ¨¡åå©ç¨æ´é«æ¹æ³ï¼å æ­¤æåè©ä¼°äºæ´é«æ´é«ååå¥æ¨¡åç XAI ææ¨è¡¨ç¾ãæåçç ç©¶çµæè¡¨æï¼å©ç¨® XAI æ¹æ³é½è½ææè­å¥å½±é¿ CP é æ¸¬çééµèº«é«é¨ä½ï¼ä¸¦ä¸éäºè§£éå°æ¼å¾®å°çè³ææ¾åå·æé­¯æ£æ§ãGrad-CAM å¨ RISv ææ¨ä¸­é¡¯èåªæ¼ CAMï¼è©²ææ¨è¡¡ééåº¦æ¹é¢çç©©å®æ§ãç¸æ¯ä¹ä¸ï¼CAM å¨ RISb ææ¨ä¸­è¡¨ç¾å¾æ´å¥½ï¼è©²ææ¨èéª¨éª¼ç©©å®æ§æéï¼è RRS ææ¨åè©ä¼°å§é¨è¡¨ç¤ºçé­¯æ£æ§ãæ´é«ä¸­çåå¥æ¨¡åé¡¯ç¤ºåºä¸åççµæï¼CAM å Grad-CAM é½ä¸ä¸è´å°åªæ¼å¦ä¸ç¨®ï¼æ´é«æ¹æ³æä¾äºå¶çµææ¨¡åçµæçè¡¨ç¤ºã

##### **MicroXercise: A Micro-Level Comparative and Explainable System for Remote Physical Therapy**
2408.11837v1 by Hanchen David Wang, Nibraas Khan, Anna Chen, Nilanjan Sarkar, Pamela Wisniewski, Meiyi Ma

Recent global estimates suggest that as many as 2.41 billion individuals have
health conditions that would benefit from rehabilitation services. Home-based
Physical Therapy (PT) faces significant challenges in providing interactive
feedback and meaningful observation for therapists and patients. To fill this
gap, we present MicroXercise, which integrates micro-motion analysis with
wearable sensors, providing therapists and patients with a comprehensive
feedback interface, including video, text, and scores. Crucially, it employs
multi-dimensional Dynamic Time Warping (DTW) and attribution-based explainable
methods to analyze the existing deep learning neural networks in monitoring
exercises, focusing on a high granularity of exercise. This synergistic
approach is pivotal, providing output matching the input size to precisely
highlight critical subtleties and movements in PT, thus transforming complex AI
analysis into clear, actionable feedback. By highlighting these micro-motions
in different metrics, such as stability and range of motion, MicroXercise
significantly enhances the understanding and relevance of feedback for
end-users. Comparative performance metrics underscore its effectiveness over
traditional methods, such as a 39% and 42% improvement in Feature Mutual
Information (FMI) and Continuity. MicroXercise is a step ahead in home-based
physical therapy, providing a technologically advanced and intuitively helpful
solution to enhance patient care and outcomes.

æè¦ï¼æè¿çå¨çä¼°è¨è¡¨æï¼å¤é 24.1 åäººæ
å¥åº·çæ³å¯å¾å¾©å¥æåä¸­åçãå±å®¶
ç©çæ²»ç (PT) å¨æä¾äºåå¼
åé¥åææç¾©çè§å¯æ¹é¢é¢è¨éå¤§ææ°ï¼ä¾æ²»çå¸«åæ£èä½¿ç¨ãçºäºå¡«è£é
åç¼ºå£ï¼æåæåº MicroXerciseï¼å®å°å¾®åä½åæè
å¯ç©¿æ´å¼ææ¸¬å¨æ´åå¨ä¸èµ·ï¼çºæ²»çå¸«åæ£èæä¾ä¸åå¨é¢ç
åé¥ä»é¢ï¼åæ¬å½±çãæå­ååæ¸ãè³ééè¦çæ¯ï¼å®æ¡ç¨
å¤ç¶­åææéè¦æ´ (DTW) ååºæ¼æ­¸å çå¯è§£é
æ¹æ³ä¾åæç£æ§éåä¸­ç¾æçæ·±åº¦å­¸ç¿ç¥ç¶ç¶²è·¯ï¼å°æ³¨æ¼éåçé«ç²åº¦ãéç¨®åå
æ¹æ³è³ééè¦ï¼æä¾èè¼¸å¥å¤§å°å¹éçè¼¸åºï¼ä»¥ç²¾ç¢ºå°
çªåº PT ä¸­ééµçç´°å¾®å·®å¥ååä½ï¼å¾èå°è¤éç AI
åæè½æçºæ¸æ°ãå¯æä½çåé¥ãééå¨ä¸åææ¨ä¸­çªé¡¯éäºå¾®åä½ï¼ä¾å¦ç©©å®æ§ååä½ç¯åï¼MicroXercise
é¡¯èæåæçµä½¿ç¨èå°åé¥ççè§£åç¸éæ§ãæ¯è¼æè½ææ¨å¼·èª¿å¶åªæ¼
å³çµ±æ¹æ³çæææ§ï¼ä¾å¦ç¹å¾µäºæ è³è¨ (FMI) åé£çºæ§åå¥æåäº 39% å 42%ãMicroXercise å¨å±å®¶
ç©çæ²»çæ¹é¢æ´é²ä¸æ­¥ï¼æä¾æè¡åé²ä¸ç´è¦ºæç¨ç
è§£æ±ºæ¹æ¡ï¼ä»¥æåæ£èç§è­·åçµæã

##### **The Literature Review Network: An Explainable Artificial Intelligence for Systematic Literature Reviews, Meta-analyses, and Method Development**
2408.05239v1 by Joshua Morriss, Tod Brindle, Jessica Bah RÃ¶sman, Daniel Reibsamen, Andreas Enz

Systematic literature reviews are the highest quality of evidence in
research. However, the review process is hindered by significant resource and
data constraints. The Literature Review Network (LRN) is the first of its kind
explainable AI platform adhering to PRISMA 2020 standards, designed to automate
the entire literature review process. LRN was evaluated in the domain of
surgical glove practices using 3 search strings developed by experts to query
PubMed. A non-expert trained all LRN models. Performance was benchmarked
against an expert manual review. Explainability and performance metrics
assessed LRN's ability to replicate the experts' review. Concordance was
measured with the Jaccard index and confusion matrices. Researchers were
blinded to the other's results until study completion. Overlapping studies were
integrated into an LRN-generated systematic review. LRN models demonstrated
superior classification accuracy without expert training, achieving 84.78% and
85.71% accuracy. The highest performance model achieved high interrater
reliability (k = 0.4953) and explainability metrics, linking 'reduce',
'accident', and 'sharp' with 'double-gloving'. Another LRN model covered 91.51%
of the relevant literature despite diverging from the non-expert's judgments (k
= 0.2174), with the terms 'latex', 'double' (gloves), and 'indication'. LRN
outperformed the manual review (19,920 minutes over 11 months), reducing the
entire process to 288.6 minutes over 5 days. This study demonstrates that
explainable AI does not require expert training to successfully conduct
PRISMA-compliant systematic literature reviews like an expert. LRN summarized
the results of surgical glove studies and identified themes that were nearly
identical to the clinical researchers' findings. Explainable AI can accurately
expedite our understanding of clinical practices, potentially revolutionizing
healthcare research.

æè¦ï¼ç³»çµ±æ§æç»åé¡§æ¯ç ç©¶ä¸­è­æåè³ªæé«çãç¶èï¼åé¡§éç¨åå°é¡¯èè³æºåè³æéå¶çé»ç¤ãæç»åé¡§ç¶²è·¯ (LRN) æ¯ç¬¬ä¸åéµå¾ª PRISMA 2020 æ¨æºçå¯è§£é AI å¹³å°ï¼æ¨å¨èªååæ´åæç»åé¡§éç¨ãLRN å¨å¤ç§æå¥å¯¦åé åä¸­é²è¡è©ä¼°ï¼ä½¿ç¨å°å®¶éç¼ç 3 åæå°å­ä¸²ä¾æ¥è©¢ PubMedãéå°å®¶è¨ç·´ææ LRN æ¨¡åãæè½ä»¥å°å®¶æååé¡§ä½çºåºæºãå¯è§£éæ§åæè½ææ¨è©ä¼° LRN è¤è£½å°å®¶åé¡§çè½åãä¸è´æ§ä»¥ Jaccard ææ¸åæ··æ·ç©é£æ¸¬éãç ç©¶äººå¡å¨ç ç©¶å®æåå°å½¼æ­¤ççµæä¿å¯ãéççç ç©¶æ´åå° LRN çæçç³»çµ±æ§åé¡§ä¸­ãLRN æ¨¡åå¨æ²æå°å®¶è¨ç·´çææ³ä¸å±ç¾åºåªç°çåé¡æºç¢ºçï¼éå° 84.78% å 85.71% çæºç¢ºçãæè½æé«çæ¨¡åéå°äºé«è©åèéä¿¡è³´åº¦ (k = 0.4953) åå¯è§£éæ§ææ¨ï¼å°ãæ¸å°ãããæå¤ãåãé³å©ãèãééæ´æå¥ãé£çµå¨ä¸èµ·ãå¦ä¸å LRN æ¨¡åæ¶µèäº 91.51% çç¸éæç»ï¼åç®¡èéå°å®¶çå¤æ·ä¸å (k = 0.2174)ï¼ä½åå«äºãä¹³è ãããééãï¼æå¥ï¼åãé©æçãç­è©å½ãLRN åªæ¼æååé¡§ï¼11 åæè¶é 19,920 åéï¼ï¼å°æ´åéç¨ç¸®ç­çº 5 å¤©è¶é 288.6 åéãéé ç ç©¶é¡¯ç¤ºï¼å¯è§£éç AI ä¸éè¦å°å®¶è¨ç·´å³å¯æåé²è¡å°å®¶ç­ç´ç PRISMA ç¸å®¹ç³»çµ±æ§æç»åé¡§ãLRN ç¸½çµäºå¤ç§æå¥ç ç©¶ççµæï¼ä¸¦æ¾åºèè¨åºç ç©¶äººå¡ç¼ç¾å¹¾ä¹ç¸åçä¸»é¢ãå¯è§£éç AI å¯ä»¥æºç¢ºå°å å¿«æåå°è¨åºå¯¦åççè§£ï¼ææ½åé©æ°é«çä¿å¥ç ç©¶ã

##### **Enhancing Medical Learning and Reasoning Systems: A Boxology-Based Comparative Analysis of Design Patterns**
2408.02709v1 by Chi Him Ng

This study analyzes hybrid AI systems' design patterns and their
effectiveness in clinical decision-making using the boxology framework. It
categorizes and copares various architectures combining machine learning and
rule-based reasoning to provide insights into their structural foundations and
healthcare applications. Addressing two main questions, how to categorize these
systems againts established design patterns and how to extract insights through
comparative analysis, the study uses design patterns from software engineering
to understand and optimize healthcare AI systems. Boxology helps identify
commonalities and create reusable solutions, enhancing these systems'
scalability, reliability, and performance. Five primary architectures are
examined: REML, MLRB, RBML, RMLT, and PERML. Each has unique strengths and
weaknesses, highlighting the need for tailored approaches in clinical tasks.
REML excels in high-accuracy prediction for datasets with limited data; MLRB in
handling large datasets and complex data integration; RBML in explainability
and trustworthiness; RMLT in managing high-dimensional data; and PERML, though
limited in analysis, shows promise in urgent care scenarios. The study
introduces four new patterns, creates five abstract categorization patterns,
and refines those five further to specific systems. These contributions enhance
Boxlogy's taxonomical organization and offer novel approaches to integrating
expert knowledge with machine learning. Boxology's structured, modular apporach
offers significant advantages in developing and analyzing hybrid AI systems,
revealing commonalities, and promoting reusable solutions. In conclusion, this
study underscores hybrid AI systems' crucial role in advancing healthcare and
Boxology's potential to drive further innovation in AI integration, ultimately
improving clinical decision support and patient outcomes.

æè¦ï¼æ¬ç ç©¶ä½¿ç¨çå­å­¸æ¡æ¶åææ··åäººå·¥æºæ§ç³»çµ±çè¨­è¨æ¨¡å¼åå¶å¨è¨åºæ±ºç­ä¸­çæææ§ãå®åé¡ä¸¦æ¯è¼çµåæ©å¨å­¸ç¿ååºæ¼è¦åçæ¨ççåç¨®æ¶æ§ï¼ä»¥æ·±å¥äºè§£å¶çµæ§åºç¤åé«çä¿å¥æç¨ãéå°å©åä¸»è¦åé¡ï¼å¦ä½æ ¹ææ¢å®çè¨­è¨æ¨¡å¼å°éäºç³»çµ±é²è¡åé¡ï¼ä»¥åå¦ä½ééæ¯è¼åææåè¦è§£ï¼æ¬ç ç©¶ä½¿ç¨è»é«å·¥ç¨ä¸­çè¨­è¨æ¨¡å¼ä¾äºè§£ååªåé«çä¿å¥äººå·¥æºæ§ç³»çµ±ãçå­å­¸æå©æ¼è­å¥å±æ§ä¸¦å»ºç«å¯éè¤ä½¿ç¨çè§£æ±ºæ¹æ¡ï¼å¾èå¢å¼·éäºç³»çµ±çå¯æ´åæ§ãå¯é æ§åæè½ãæª¢æ¥äºäºç¨®ä¸»è¦çæ¶æ§ï¼REMLãMLRBãRBMLãRMLT å PERMLãæ¯ç¨®æ¶æ§é½æç¨ç¹çåªç¼ºé»ï¼å¼·èª¿äºå¨è¨åºä»»åä¸­éè¦éèº«æé çæ¹æ³ãREML å¨è³ææéçè³æéä¸­è¡¨ç¾åºé«ç²¾åº¦çé æ¸¬ï¼MLRB å¨èçå¤§åè³æéåè¤éè³ææ´åæ¹é¢è¡¨ç¾åºè²ï¼RBML å¨å¯è§£éæ§åå¯ä¿¡åº¦æ¹é¢è¡¨ç¾åºè²ï¼RMLT å¨ç®¡çé«ç¶­è³ææ¹é¢è¡¨ç¾åºè²ï¼è PERML åç®¡å¨åææ¹é¢æéï¼ä½å¨ç·æ¥ç§è­·å ´æ¯ä¸­è¡¨ç¾åºæ½åãæ¬ç ç©¶å¼å¥äºåç¨®æ°æ¨¡å¼ï¼å»ºç«äºäºç¨®æ½è±¡åé¡æ¨¡å¼ï¼ä¸¦é²ä¸æ­¥å°éäºç¨®æ¨¡å¼ç´°åçºå·é«çç³»çµ±ãéäºè²¢ç»å¢å¼·äºçå­å­¸çåé¡çµç¹ï¼ä¸¦æä¾äºå°å°å®¶ç¥è­èæ©å¨å­¸ç¿æ´åçæ°æ¹æ³ãçå­å­¸ççµæ§åãæ¨¡çµåæ¹æ³å¨éç¼ååææ··åäººå·¥æºæ§ç³»çµ±ãæ­ç¤ºå±æ§ä»¥åæ¨å»£å¯éè¤ä½¿ç¨çè§£æ±ºæ¹æ¡æ¹é¢å·æé¡¯èåªå¢ãç¸½ä¹ï¼æ¬ç ç©¶å¼·èª¿äºæ··åäººå·¥æºæ§ç³»çµ±å¨æ¨é²é«çä¿å¥ä¸­çééµä½ç¨ï¼ä»¥åçå­å­¸å¨æ¨åäººå·¥æºæ§æ´åé²ä¸æ­¥åµæ°æ¹é¢çæ½åï¼æçµæ¹åè¨åºæ±ºç­æ¯æ´åæ£èçæ²»çææã

##### **Bayesian Kolmogorov Arnold Networks (Bayesian_KANs): A Probabilistic Approach to Enhance Accuracy and Interpretability**
2408.02706v1 by Masoud Muhammed Hassan

Because of its strong predictive skills, deep learning has emerged as an
essential tool in many industries, including healthcare. Traditional deep
learning models, on the other hand, frequently lack interpretability and omit
to take prediction uncertainty into account two crucial components of clinical
decision making. In order to produce explainable and uncertainty aware
predictions, this study presents a novel framework called Bayesian Kolmogorov
Arnold Networks (BKANs), which combines the expressive capacity of Kolmogorov
Arnold Networks with Bayesian inference. We employ BKANs on two medical
datasets, which are widely used benchmarks for assessing machine learning
models in medical diagnostics: the Pima Indians Diabetes dataset and the
Cleveland Heart Disease dataset. Our method provides useful insights into
prediction confidence and decision boundaries and outperforms traditional deep
learning models in terms of prediction accuracy. Moreover, BKANs' capacity to
represent aleatoric and epistemic uncertainty guarantees doctors receive more
solid and trustworthy decision support. Our Bayesian strategy improves the
interpretability of the model and considerably minimises overfitting, which is
important for tiny and imbalanced medical datasets, according to experimental
results. We present possible expansions to further use BKANs in more
complicated multimodal datasets and address the significance of these
discoveries for future research in building reliable AI systems for healthcare.
This work paves the way for a new paradigm in deep learning model deployment in
vital sectors where transparency and reliability are crucial.

æè¦ï¼ç±æ¼å¶å¼·å¤§çé æ¸¬è½åï¼æ·±åº¦å­¸ç¿å·²æçºè¨±å¤ç¢æ¥­ä¸­ä¸å¯æç¼ºçå·¥å·ï¼åæ¬é«çä¿å¥ãç¶èï¼å³çµ±çæ·±åº¦å­¸ç¿æ¨¡åéå¸¸ç¼ºä¹å¯è§£éæ§ï¼ä¸¦ä¸å¿½ç¥äºå°é æ¸¬ä¸ç¢ºå®æ§ç´å¥èéï¼èéå©åå ç´ æ¯è¨åºæ±ºç­å¶å®çééµçµæé¨åãçºäºç¢çå¯è§£éä¸å·æä¸ç¢ºå®æ§æè­çé æ¸¬ï¼æ¬ç ç©¶æåºäºä¸ååçºè²æ°æ¯ç¾è«å¥æ´å¤«é¿è«¾å¾·ç¶²è·¯ (BKAN) çæ°æ¶æ§ï¼å®çµåäºæ¯ç¾è«å¥æ´å¤«é¿è«¾å¾·ç¶²è·¯çè¡¨éè½åèè²æ°æ¨è«ãæåå¨å©åé«å­¸è³æéä¸ä½¿ç¨ BKANï¼éäºè³æéæ¯è©ä¼°æ©å¨å­¸ç¿æ¨¡åå¨é«å­¸è¨ºæ·ä¸­çå»£æ³ä½¿ç¨åºæºï¼ç®é¦¬å°ç¬¬å®äººç³å°¿çè³æéååéå¤«è­å¿èçè³æéãæåçæ¨¡åæä¾äºå°é æ¸¬ä¿¡å¿åæ±ºç­éççæçè¦è§£ï¼ä¸¦ä¸å¨é æ¸¬æºç¢ºåº¦æ¹é¢åªæ¼å³çµ±çæ·±åº¦å­¸ç¿æ¨¡åãæ­¤å¤ï¼BKAN è¡¨ç¾é¨æ©åèªè­ä¸ç¢ºå®æ§çè½åï¼å¯ç¢ºä¿é«çç²å¾æ´å¯é ä¸å¼å¾ä¿¡è³´çæ±ºç­æ¯æ´ãæ ¹æå¯¦é©çµæï¼æåçè²æ°ç­ç¥æé«äºæ¨¡åçå¯è§£éæ§ï¼ä¸¦å¤§å¹æ¸å°äºéåº¦æ¬åï¼éå°æ¼å°åä¸ä¸å¹³è¡¡çé«å­¸è³æééå¸¸éè¦ãæåæåºäºå¯è½çæ´ååè½ï¼ä»¥é²ä¸æ­¥å° BKAN ç¨æ¼æ´è¤éçå¤æ¨¡å¼è³æéï¼ä¸¦æ¢è¨éäºç¼ç¾å°æ¼æªä¾å»ºç«å¯é çé«çä¿å¥ AI ç³»çµ±ç ç©¶çéè¦æ§ãéé å·¥ä½çºæ·±åº¦å­¸ç¿æ¨¡åé¨ç½²å¨éæåº¦åå¯é æ§è³ééè¦çéè¦é åä¸­éåäºä¸åæ°çå¸ç¯ã

##### **MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI**
2407.20284v1 by Shyam Dongre, Ritesh Chandra, Sonali Agarwal

In modern healthcare, addressing the complexities of accurate disease
prediction and personalized recommendations is both crucial and challenging.
This research introduces MLtoGAI, which integrates Semantic Web technology with
Machine Learning (ML) to enhance disease prediction and offer user-friendly
explanations through ChatGPT. The system comprises three key components: a
reusable disease ontology that incorporates detailed knowledge about various
diseases, a diagnostic classification model that uses patient symptoms to
detect specific diseases accurately, and the integration of Semantic Web Rule
Language (SWRL) with ontology and ChatGPT to generate clear, personalized
health advice. This approach significantly improves prediction accuracy and
ensures results that are easy to understand, addressing the complexity of
diseases and diverse symptoms. The MLtoGAI system demonstrates substantial
advancements in accuracy and user satisfaction, contributing to developing more
intelligent and accessible healthcare solutions. This innovative approach
combines the strengths of ML algorithms with the ability to provide
transparent, human-understandable explanations through ChatGPT, achieving
significant improvements in prediction accuracy and user comprehension. By
leveraging semantic technology and explainable AI, the system enhances the
accuracy of disease prediction and ensures that the recommendations are
relevant and easily understood by individual patients. Our research highlights
the potential of integrating advanced technologies to overcome existing
challenges in medical diagnostics, paving the way for future developments in
intelligent healthcare systems. Additionally, the system is validated using 200
synthetic patient data records, ensuring robust performance and reliability.

æè¦ï¼å¨ç¾ä»£é«çä¿å¥ä¸­ï¼è§£æ±ºæºç¢ºç¾çé æ¸¬ååæ§åå»ºè­°çè¤éæ§æ¢è³ééè¦åå·æææ°æ§ãæ¬ç ç©¶å¼å¥äº MLtoGAIï¼å®å°èªç¾©ç¶²è·¯æè¡èæ©å¨å­¸ç¿ (ML) ç¸çµåï¼ä»¥å¢å¼·ç¾çé æ¸¬ä¸¦éé ChatGPT æä¾ä½¿ç¨èååçèªªæãè©²ç³»çµ±åå«ä¸åééµçµæé¨åï¼ä¸åå¯éè¤ä½¿ç¨çç¾çæ¬ä½ï¼å¶ä¸­åå«æéåç¨®ç¾ççè©³ç´°ç¥è­ï¼ä¸åè¨ºæ·åé¡æ¨¡åï¼å®ä½¿ç¨æ£èççä¾æºç¢ºæª¢æ¸¬ç¹å®ç¾çï¼ä»¥åèªç¾©ç¶²è·¯è¦åèªè¨ (SWRL) èæ¬ä½å ChatGPT çæ´åï¼ä»¥ç¢çæ¸æ°ãåæ§åçå¥åº·å»ºè­°ãéç¨®æ¹æ³é¡¯èæé«äºé æ¸¬æºç¢ºæ§ï¼ä¸¦ç¢ºä¿äºææ¼çè§£ççµæï¼è§£æ±ºäºç¾çåä¸åçççè¤éæ§ãMLtoGAI ç³»çµ±å±ç¤ºäºæºç¢ºæ§åä½¿ç¨èæ»¿æåº¦çå¯¦è³ªæ§é²æ­¥ï¼æå©æ¼éç¼æ´æºæ§ä¸æ´ææ¼åå¾çé«çä¿å¥è§£æ±ºæ¹æ¡ãéç¨®åµæ°çæ¹æ³çµåäº ML æ¼ç®æ³çåªé»ï¼ä»¥åéé ChatGPT æä¾éæä¸äººé¡å¯ä»¥çè§£çèªªæçè½åï¼å¨é æ¸¬æºç¢ºæ§åä½¿ç¨èçè§£æ¹é¢åå¾äºé¡¯èçé²æ­¥ãééå©ç¨èªç¾©æè¡åå¯è§£éç AIï¼è©²ç³»çµ±æé«äºç¾çé æ¸¬çæºç¢ºæ§ï¼ä¸¦ç¢ºä¿äºå»ºè­°èåå¥æ£èç¸éä¸ææ¼çè§£ãæåçç ç©¶å¼·èª¿äºæ´ååé²æè¡ä»¥åæé«çè¨ºæ·ä¸­ç¾æææ°çæ½åï¼çºæºæ§é«çä¿å¥ç³»çµ±çæªä¾ç¼å±éªè·¯ãæ­¤å¤ï¼è©²ç³»çµ±ä½¿ç¨ 200 ååææ£èè³æè¨éé²è¡é©è­ï¼ç¢ºä¿äºç©©å¥çæè½åå¯é æ§ã

##### **Introducing Î´-XAI: a novel sensitivity-based method for local AI explanations**
2407.18343v2 by Alessandro De Carlo, Enea Parimbelli, Nicola Melillo, Giovanna Nicora

Explainable Artificial Intelligence (XAI) is central to the debate on
integrating Artificial Intelligence (AI) and Machine Learning (ML) algorithms
into clinical practice. High-performing AI/ML models, such as ensemble learners
and deep neural networks, often lack interpretability, hampering clinicians'
trust in their predictions. To address this, XAI techniques are being developed
to describe AI/ML predictions in human-understandable terms. One promising
direction is the adaptation of sensitivity analysis (SA) and global sensitivity
analysis (GSA), which inherently rank model inputs by their impact on
predictions. Here, we introduce a novel delta-XAI method that provides local
explanations of ML model predictions by extending the delta index, a GSA
metric. The delta-XAI index assesses the impact of each feature's value on the
predicted output for individual instances in both regression and classification
problems. We formalize the delta-XAI index and provide code for its
implementation. The delta-XAI method was evaluated on simulated scenarios using
linear regression models, with Shapley values serving as a benchmark. Results
showed that the delta-XAI index is generally consistent with Shapley values,
with notable discrepancies in models with highly impactful or extreme feature
values. The delta-XAI index demonstrated higher sensitivity in detecting
dominant features and handling extreme feature values. Qualitatively, the
delta-XAI provides intuitive explanations by leveraging probability density
functions, making feature rankings clearer and more explainable for
practitioners. Overall, the delta-XAI method appears promising for robustly
obtaining local explanations of ML model predictions. Further investigations in
real-world clinical settings will be conducted to evaluate its impact on
AI-assisted clinical workflows.

æè¦ï¼å¯è§£éäººå·¥æºæ§ (XAI) æ¯å°äººå·¥æºæ§ (AI) åæ©å¨å­¸ç¿ (ML) æ¼ç®æ³æ´åå°è¨åºå¯¦åä¸­çè¾¯è«æ ¸å¿ãé«å·è¡æè½ç AI/ML æ¨¡åï¼ä¾å¦æ´é«å­¸ç¿å¨åæ·±åº¦ç¥ç¶ç¶²è·¯ï¼éå¸¸ç¼ºä¹å¯è§£éæ§ï¼é»ç¤è¨åºé«çå°å¶é æ¸¬çä¿¡ä»»ãçºäºè§£æ±ºéååé¡ï¼æ­£å¨éç¼ XAI æè¡ï¼ä»¥äººé¡å¯ä»¥çè§£çè¡èªæè¿° AI/ML é æ¸¬ãä¸åæå¸æçæ¹åæ¯æ¡ç¨ææåº¦åæ (SA) åå¨çææåº¦åæ (GSA)ï¼å®åæ¬è³ªä¸æä¾ææ¨¡åè¼¸å¥å°é æ¸¬çå½±é¿ä¾å°å¶é²è¡æåãå¨æ­¤ï¼æåä»ç´¹ä¸ç¨®æ°ç delta-XAI æ¹æ³ï¼ééæ´å GSA ææ¨ delta ææ¸ä¾æä¾ ML æ¨¡åé æ¸¬çå±é¨è§£éãdelta-XAI ææ¸è©ä¼°æ¯åç¹å¾µå¼å°åæ­¸ååé¡åé¡ä¸­åå¥ä¾é çé æ¸¬è¼¸åºä¹å½±é¿ãæåå° delta-XAI ææ¸å½¢å¼åï¼ä¸¦æä¾å¶å¯¦ä½çç¨å¼ç¢¼ãä½¿ç¨ç·æ§åæ­¸æ¨¡åå°æ¨¡æ¬æå¢è©ä¼° delta-XAI æ¹æ³ï¼ä¸¦ä»¥ Shapley å¼ä½çºåºæºãçµæé¡¯ç¤º delta-XAI ææ¸éå¸¸è Shapley å¼ä¸è´ï¼ä½å¨å·æé«åº¦å½±é¿åææ¥µç«¯ç¹å¾µå¼çæ¨¡åä¸­å­å¨é¡¯èå·®ç°ãdelta-XAI ææ¸å¨åµæ¸¬ä¸»è¦ç¹å¾µåèçæ¥µç«¯ç¹å¾µå¼æ¹é¢è¡¨ç¾åºæ´é«çææåº¦ãå®æ§å°ä¾èªªï¼delta-XAI ééå©ç¨æ©çå¯åº¦å½æ¸æä¾ç´è§çè§£éï¼ä½¿ç¹å¾µæåæ´æ¸æ°ä¸å°å¾æ¥­äººå¡ä¾èªªæ´å·å¯è§£éæ§ãç¸½é«èè¨ï¼delta-XAI æ¹æ³å°æ¼ç©©å¥å°åå¾ ML æ¨¡åé æ¸¬çå±é¨è§£éä¼¼ä¹å¾æå¸æãå°å¨çå¯¦ä¸ççè¨åºç°å¢ä¸­é²è¡é²ä¸æ­¥èª¿æ¥ï¼ä»¥è©ä¼°å¶å° AI è¼å©è¨åºå·¥ä½æµç¨çå½±é¿ã

##### **Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population**
2407.17324v2 by Nikolaos Ntampakis, Konstantinos Diamantaras, Ioanna Chouvarda, Vasileios Argyriou, Panagiotis Sarigianndis

Dementia, a debilitating neurological condition affecting millions worldwide,
presents significant diagnostic challenges. In this work, we introduce a novel
methodology for the classification of demented and non-demented elderly
patients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approach
features a unique technique for selectively processing MRI slices, focusing on
the most relevant brain regions and excluding less informative sections. This
methodology is complemented by a confidence-based classification committee
composed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, and
Dem3D EfficientNet. These models work synergistically to enhance
decision-making accuracy, leveraging their collective strengths. Tested on the
Open Access Series of Imaging Studies(OASIS) dataset, our method achieved an
impressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,
validation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset
confirmed the robustness and generalizability of our approach. The use of
explainable AI (XAI) techniques and comprehensive ablation studies further
substantiate the effectiveness of our techniques, providing insights into the
decision-making process and the importance of our methodology. This research
offers a significant advancement in dementia diagnosis, providing a highly
accurate and efficient tool for clinical applications.

æè¦ï¼å¤±æºçæ¯ä¸ç¨®å½±é¿å¨çæ¸ç¾è¬äººçè¡°å¼±æ§ç¥ç¶ç¾çï¼å¨è¨ºæ·ä¸å·æéå¤§ææ°ãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸ç¨®æ°çæ¹æ³ï¼ç¨æ¼å°å¤±æºåéå¤±æºèå¹´æ£èé²è¡åé¡ï¼ä½¿ç¨ 3D å¤§è¦ç£æ¯é å½± (MRI) ææãæåçåæ³æ¡ç¨äºä¸ç¨®ç¨ç¹æè¡ï¼ç¨æ¼é¸ææ§èç MRI åçï¼éé»éæ³¨æç¸éçå¤§è¦ååï¼ä¸¦æé¤ä¿¡æ¯éè¼å°çé¨åãéç¨®æ¹æ³ç±ä¸ååºæ¼ä¿¡å¿çåé¡å§å¡æè£åï¼è©²å§å¡æç±ä¸åèªå®ç¾©æ·±åº¦å­¸ç¿æ¨¡åçµæï¼Dem3D ResNetãDem3D CNN å Dem3D EfficientNetãéäºæ¨¡åååå·¥ä½ä»¥å¢å¼·æ±ºç­çæºç¢ºæ§ï¼å©ç¨å®åçéé«åªå¢ãå¨å½±åç ç©¶éæ¾å­åç³»å (OASIS) è³æéä¸é²è¡æ¸¬è©¦ï¼æåçæ¨¡åéå°äº 94.12% çé©äººæºç¢ºåº¦ï¼è¶éäºç¾ææ¹æ³ãæ­¤å¤ï¼å¨é¿è²æµ·é»çç¥ç¶å½±åå¡è­° (ADNI) è³æéä¸çé©è­è­å¯¦äºæåæ¹æ³çç©©å¥æ§åæ®éæ§ãå¯è§£é AI (XAI) æè¡åå¨é¢çæ¶èç ç©¶é²ä¸æ­¥è­å¯¦äºæåæè¡çæææ§ï¼æä¾äºå°æ±ºç­éç¨åæåæ¹æ³éè¦æ§çè¦è§£ãéé ç ç©¶çºå¤±æºçè¨ºæ·æä¾äºéå¤§é²å±ï¼çºè¨åºæç¨æä¾äºä¸åé«åº¦æºç¢ºä¸é«æçå·¥å·ã

##### **Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition**
2408.06352v1 by Michele Fiori, Gabriele Civitarese, Claudio Bettini

Recognizing daily activities with unobtrusive sensors in smart environments
enables various healthcare applications. Monitoring how subjects perform
activities at home and their changes over time can reveal early symptoms of
health issues, such as cognitive decline. Most approaches in this field use
deep learning models, which are often seen as black boxes mapping sensor data
to activities. However, non-expert users like clinicians need to trust and
understand these models' outputs. Thus, eXplainable AI (XAI) methods for Human
Activity Recognition have emerged to provide intuitive natural language
explanations from these models. Different XAI methods generate different
explanations, and their effectiveness is typically evaluated through user
surveys, that are often challenging in terms of costs and fairness. This paper
proposes an automatic evaluation method using Large Language Models (LLMs) to
identify, in a pool of candidates, the best XAI approach for non-expert users.
Our preliminary results suggest that LLM evaluation aligns with user surveys.

æè¦ï¼èç±æºæ§ç°å¢ä¸­ä¸å¼äººæ³¨ç®çææ¸¬å¨è¾¨è­æ¥å¸¸æ´»åï¼è½åç¨åç¨®é«çä¿å¥æç¨ãç£æ§åè©¦èå¨å®¶ä¸­å¦ä½å·è¡æ´»åï¼ä»¥åå¶é¨èæéçè®åï¼å¯ä»¥æ­ç¤ºå¥åº·åé¡çæ©æççï¼ä¾å¦èªç¥è½åä¸éãæ­¤é åä¸­çå¤§å¤æ¸æ¹æ³é½ä½¿ç¨æ·±åº¦å­¸ç¿æ¨¡åï¼éäºæ¨¡åéå¸¸è¢«è¦çºå°ææ¸¬å¨è³æå°æè³æ´»åçé»çå­ãç¶èï¼éå°å®¶ä½¿ç¨èï¼ä¾å¦è¨åºé«å¸«ï¼éè¦ä¿¡ä»»ä¸¦äºè§£éäºæ¨¡åçè¼¸åºãå æ­¤ï¼äººé¡æ´»åè¾¨è­çå¯è§£é AI (XAI) æ¹æ³æéèçï¼ä»¥æä¾ä¾èªéäºæ¨¡åçç´è¦ºèªç¶èªè¨èªªæãä¸åç XAI æ¹æ³æç¢çä¸åçèªªæï¼èå¶æææ§éå¸¸ééä½¿ç¨èèª¿æ¥ä¾è©ä¼°ï¼éå¨ææ¬åå¬å¹³æ§æ¹é¢éå¸¸å·æææ°æ§ãæ¬ææåºä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) çèªåè©ä¼°æ¹æ³ï¼ä»¥å¨åé¸èä¸­æ¾åºæé©åéå°å®¶ä½¿ç¨èç XAI æ¹æ³ãæåçåæ­¥çµæè¡¨æï¼LLM è©ä¼°èä½¿ç¨èèª¿æ¥ä¸è´ã

##### **Explainable AI-based Intrusion Detection System for Industry 5.0: An Overview of the Literature, associated Challenges, the existing Solutions, and Potential Research Directions**
2408.03335v1 by Naseem Khan, Kashif Ahmad, Aref Al Tamimi, Mohammed M. Alani, Amine Bermak, Issa Khalil

Industry 5.0, which focuses on human and Artificial Intelligence (AI)
collaboration for performing different tasks in manufacturing, involves a
higher number of robots, Internet of Things (IoTs) devices and
interconnections, Augmented/Virtual Reality (AR), and other smart devices. The
huge involvement of these devices and interconnection in various critical
areas, such as economy, health, education and defense systems, poses several
types of potential security flaws. AI itself has been proven a very effective
and powerful tool in different areas of cybersecurity, such as intrusion
detection, malware detection, and phishing detection, among others. Just as in
many application areas, cybersecurity professionals were reluctant to accept
black-box ML solutions for cybersecurity applications. This reluctance pushed
forward the adoption of eXplainable Artificial Intelligence (XAI) as a tool
that helps explain how decisions are made in ML-based systems. In this survey,
we present a comprehensive study of different XAI-based intrusion detection
systems for industry 5.0, and we also examine the impact of explainability and
interpretability on Cybersecurity practices through the lens of Adversarial
XIDS (Adv-XIDS) approaches. Furthermore, we analyze the possible opportunities
and challenges in XAI cybersecurity systems for industry 5.0 that elicit future
research toward XAI-based solutions to be adopted by high-stakes industry 5.0
applications. We believe this rigorous analysis will establish a foundational
framework for subsequent research endeavors within the specified domain.

æè¦ï¼å·¥æ¥­ 5.0 èéæ¼äººé¡èäººå·¥æºæ§ (AI) åä½å·è¡è£½é ä¸­çä¸åä»»åï¼æ¶åæ´å¤æ©å¨äººãç©è¯ç¶² (IoT) è£ç½®åäºé£ãæ´å¢/èæ¬å¯¦å¢ (AR) åå¶ä»æºæ§è£ç½®ãéäºè£ç½®åäºé£å¨ç¶æ¿ãé«çä¿å¥ãæè²ååé²ç³»çµ±ç­åç¨®ééµé åçå»£æ³åèï¼å¼ç¼äºå¤ç¨®é¡åçæ½å¨å®å¨æ¼æ´ãAI æ¬èº«å·²è¢«è­ææ¯ç¶²è·¯å®å¨ä¸åé åä¸­éå¸¸ææä¸å¼·å¤§çå·¥å·ï¼ä¾å¦å¥ä¾µåµæ¸¬ãæ¡æè»é«åµæ¸¬åç¶²è·¯é£é­åµæ¸¬ç­ãå°±åå¨è¨±å¤æç¨é åä¸æ¨£ï¼ç¶²è·¯å®å¨å°æ¥­äººå¡ä¸é¡ææ¥åé»ç ML è§£æ±ºæ¹æ¡ä¾æç¨æ¼ç¶²è·¯å®å¨ãéç¨®ä¸é¡æä¿ä½¿å¯è§£éäººå·¥æºæ§ (XAI) ä½çºä¸ç¨®å·¥å·è¢«æ¡ç¨ï¼æå©æ¼èªªæå¨åºæ¼ ML çç³»çµ±ä¸­å¦ä½ååºæ±ºç­ãå¨éé èª¿æ¥ä¸­ï¼æåå°å·¥æ¥­ 5.0 çä¸ååºæ¼ XAI çå¥ä¾µåµæ¸¬ç³»çµ±é²è¡äºå¨é¢çç ç©¶ï¼ä¸¦ä¸æåä¹ééå°æå¼ XIDS (Adv-XIDS) æ¹æ³çè§é»ä¾æ¢è¨å¯è§£éæ§åå¯è©®éæ§å°ç¶²è·¯å®å¨å¯¦åçå½±é¿ãæ­¤å¤ï¼æååæäºå·¥æ¥­ 5.0 ç XAI ç¶²è·¯å®å¨ç³»çµ±ä¸­å¯è½å­å¨çæ©æåææ°ï¼å¼ç¼äºæªä¾éå° XAI åºç¤è§£æ±ºæ¹æ¡çç ç©¶ï¼ä»¥ä¾é«é¢¨éªçå·¥æ¥­ 5.0 æç¨æ¡ç¨ãæåç¸ä¿¡éé å´è¬¹çåæå°çºæå®é åå§çå¾çºç ç©¶å·¥ä½å»ºç«åºç¤æ¶æ§ã

##### **A Comparative Study on Automatic Coding of Medical Letters with Explainability**
2407.13638v1 by Jamie Glen, Lifeng Han, Paul Rayson, Goran Nenadic

This study aims to explore the implementation of Natural Language Processing
(NLP) and machine learning (ML) techniques to automate the coding of medical
letters with visualised explainability and light-weighted local computer
settings. Currently in clinical settings, coding is a manual process that
involves assigning codes to each condition, procedure, and medication in a
patient's paperwork (e.g., 56265001 heart disease using SNOMED CT code). There
are preliminary research on automatic coding in this field using
state-of-the-art ML models; however, due to the complexity and size of the
models, the real-world deployment is not achieved. To further facilitate the
possibility of automatic coding practice, we explore some solutions in a local
computer setting; in addition, we explore the function of explainability for
transparency of AI models. We used the publicly available MIMIC-III database
and the HAN/HLAN network models for ICD code prediction purposes. We also
experimented with the mapping between ICD and SNOMED CT knowledge bases. In our
experiments, the models provided useful information for 97.98\% of codes. The
result of this investigation can shed some light on implementing automatic
clinical coding in practice, such as in hospital settings, on the local
computers used by clinicians , project page
\url{https://github.com/Glenj01/Medical-Coding}.

æè¦ï¼æ¬ç ç©¶æ¨å¨æ¢è¨å°èªç¶èªè¨èç (NLP) åæ©å¨å­¸ç¿ (ML) æè¡å¯¦ä½æ¼é«çä¿¡å½ç·¨ç¢¼èªååï¼ä¸¦å·åè¦è¦ºåèªªæè½ååè¼éåçæ¬å°é»è¦è¨­å®ãç®åå¨è¨åºç°å¢ä¸­ï¼ç·¨ç¢¼æ¯ä¸ç¨®æåæµç¨ï¼æ¶åçºçæ£æä»¶ä¸­çæ¯é ççãç¨åºåè¥ç©ææ´¾ä»£ç¢¼ (ä¾å¦ï¼ä½¿ç¨ SNOMED CT ä»£ç¢¼ 56265001 è¡¨ç¤ºå¿èç)ãæ­¤é åæä½¿ç¨ææ° ML æ¨¡åé²è¡èªåç·¨ç¢¼çåæ­¥ç ç©¶ï¼ç¶èï¼ç±æ¼æ¨¡åçè¤éæ§åå¤§å°ï¼ä¸¦æªå¯¦ç¾å¯¦éé¨ç½²ãçºäºé²ä¸æ­¥ä¿é²èªåç·¨ç¢¼å¯¦åçå¯è½æ§ï¼æåå¨æ¬å°é»è¦è¨­å®ä¸­æ¢è¨äºä¸äºè§£æ±ºæ¹æ¡ï¼æ­¤å¤ï¼æåæ¢è¨äºèªªæåè½å¨ AI æ¨¡åéæåº¦ä¸­çåè½ãæåä½¿ç¨å¬éç MIMIC-III è³æåº«å HAN/HLAN ç¶²è·¯æ¨¡åé²è¡ ICD ä»£ç¢¼é æ¸¬ãæåéè©¦é©äº ICD å SNOMED CT ç¥è­åº«ä¹éçå°æãå¨æåçå¯¦é©ä¸­ï¼éäºæ¨¡åæä¾äº 97.98% ä»£ç¢¼çæç¨è³è¨ãéé èª¿æ¥çµæå¯ä»¥çºå¯¦åä¸­çèªåè¨åºç·¨ç¢¼å¯¦ä½æä¾ä¸äºè¦è§£ï¼ä¾å¦å¨é«é¢ç°å¢ä¸­ï¼ç±è¨åºé«çä½¿ç¨çæ¬å°é»è¦ï¼å°æ¡é é¢ \url{https://github.com/Glenj01/Medical-Coding}ã

##### **Explainable AI for Enhancing Efficiency of DL-based Channel Estimation**
2407.07009v1 by Abdul Karim Gizzini, Yahia Medjahdi, Ali J. Ghandour, Laurent Clavier

The support of artificial intelligence (AI) based decision-making is a key
element in future 6G networks, where the concept of native AI will be
introduced. Moreover, AI is widely employed in different critical applications
such as autonomous driving and medical diagnosis. In such applications, using
AI as black-box models is risky and challenging. Hence, it is crucial to
understand and trust the decisions taken by these models. Tackling this issue
can be achieved by developing explainable AI (XAI) schemes that aim to explain
the logic behind the black-box model behavior, and thus, ensure its efficient
and safe deployment. Recently, we proposed a novel perturbation-based XAI-CHEST
framework that is oriented toward channel estimation in wireless
communications. The core idea of the XAI-CHEST framework is to identify the
relevant model inputs by inducing high noise on the irrelevant ones. This
manuscript provides the detailed theoretical foundations of the XAI-CHEST
framework. In particular, we derive the analytical expressions of the XAI-CHEST
loss functions and the noise threshold fine-tuning optimization problem. Hence
the designed XAI-CHEST delivers a smart input feature selection methodology
that can further improve the overall performance while optimizing the
architecture of the employed model. Simulation results show that the XAI-CHEST
framework provides valid interpretations, where it offers an improved bit error
rate performance while reducing the required computational complexity in
comparison to the classical DL-based channel estimation.

æè¦ï¼äººå·¥æºè½ (AI) æ¯æçæ±ºç­å¶å®æ¯æªä¾ 6G ç¶²è·¯ä¸­çééµåç´ ï¼å¶ä¸­å°å¼å¥åç AI çæ¦å¿µãæ­¤å¤ï¼AI å»£æ³ç¨æ¼ä¸åçééµæç¨ä¸­ï¼ä¾å¦èªåé§é§åé«çè¨ºæ·ãå¨éäºæç¨ä¸­ï¼ä½¿ç¨ AI ä½çºé»çæ¨¡åæ¯æé¢¨éªä¸å·æææ°æ§çãå æ­¤ï¼çè§£åä¿¡ä»»éäºæ¨¡åååºçæ±ºç­è³ééè¦ãè§£æ±ºæ­¤åé¡çæ¹æ³æ¯éç¼å¯è§£é AI (XAI) æ¶æ§ï¼æ¨å¨è§£éé»çæ¨¡åè¡çºèå¾çéè¼¯ï¼å¾èç¢ºä¿å¶ææä¸å®å¨çé¨ç½²ãæè¿ï¼æåæåºäºä¸åæ°çåºæ¼æ¾åç XAI-CHEST æ¡æ¶ï¼è©²æ¡æ¶é¢åç¡ç·éä¿¡ä¸­çä¿¡éä¼°è¨ãXAI-CHEST æ¡æ¶çæ ¸å¿ææ³æ¯ééå¨ç¡éè¼¸å¥ä¸å¼å¥é«åªè²ä¾è­å¥ç¸éæ¨¡åè¼¸å¥ãéä»½æç¨¿æä¾äº XAI-CHEST æ¡æ¶çè©³ç´°çè«åºç¤ãç¹å¥æ¯ï¼æåæ¨å°äº XAI-CHEST æå¤±å½æ¸ååªè²é¾å¼å¾®èª¿åªååé¡çè§£æè¡¨éå¼ãå æ­¤ï¼è¨­è¨ç XAI-CHEST æä¾äºä¸ç¨®æºè½è¼¸å¥ç¹å¾µé¸ææ¹æ³ï¼å¯ä»¥å¨åªåæç¨æ¨¡åçæ¶æ§çåæé²ä¸æ­¥æé«æ´é«æ§è½ãæ¨¡æ¬çµæè¡¨æï¼XAI-CHEST æ¡æ¶æä¾äºææçè§£éï¼å¨éä½æéçè¨ç®è¤éåº¦çåæï¼æä¾äºæ¹é²çæ¯ç¹é¯èª¤çæ§è½ï¼èéèåºæ¼å³çµ± DL çä¿¡éä¼°è¨ç¸æ¯ã

##### **Explainable AI: Comparative Analysis of Normal and Dilated ResNet Models for Fundus Disease Classification**
2407.05440v2 by P. N. Karthikayan, Yoga Sri Varshan V, Hitesh Gupta Kattamuri, Umarani Jayaraman

This paper presents dilated Residual Network (ResNet) models for disease
classification from retinal fundus images. Dilated convolution filters are used
to replace normal convolution filters in the higher layers of the ResNet model
(dilated ResNet) in order to improve the receptive field compared to the normal
ResNet model for disease classification. This study introduces
computer-assisted diagnostic tools that employ deep learning, enhanced with
explainable AI techniques. These techniques aim to make the tool's
decision-making process transparent, thereby enabling medical professionals to
understand and trust the AI's diagnostic decision. They are particularly
relevant in today's healthcare landscape, where there is a growing demand for
transparency in AI applications to ensure their reliability and ethical use.
The dilated ResNet is used as a replacement for the normal ResNet to enhance
the classification accuracy of retinal eye diseases and reduce the required
computing time. The dataset used in this work is the Ocular Disease Intelligent
Recognition (ODIR) dataset which is a structured ophthalmic database with eight
classes covering most of the common retinal eye diseases. The evaluation
metrics used in this work include precision, recall, accuracy, and F1 score. In
this work, a comparative study has been made between normal ResNet models and
dilated ResNet models on five variants namely ResNet-18, ResNet-34, ResNet-50,
ResNet-101, and ResNet-152. The dilated ResNet model shows promising results as
compared to normal ResNet with an average F1 score of 0.71, 0.70, 0.69, 0.67,
and 0.70 respectively for the above respective variants in ODIR multiclass
disease classification.

æè¦ï¼è¿ç¯è®ºææåºäºç¨äºä»è§ç½èç¼åºå¾åè¿è¡ç¾çåç±»çæ©å¼ æ®å·®ç½ç» (ResNet) æ¨¡åãæ©å¼ å·ç§¯æ»¤æ³¢å¨ç¨äºæ¿æ¢ ResNet æ¨¡åè¾é«å±ä¸­çæ­£å¸¸å·ç§¯æ»¤æ³¢å¨ï¼æ©å¼  ResNetï¼ï¼ä»¥æ¹åæç¥åºï¼ä»èéå¯¹ç¾çåç±»å¯¹æ­£å¸¸ ResNet æ¨¡åè¿è¡æ¹è¿ãæ¬ç ç©¶å¼å¥äºéç¨æ·±åº¦å­¦ä¹ çè®¡ç®æºè¾å©è¯æ­å·¥å·ï¼å¹¶éè¿å¯è§£éç AI ææ¯è¿è¡äºå¢å¼ºãè¿äºææ¯æ¨å¨ä½¿è¯¥å·¥å·çå³ç­è¿ç¨éæåï¼ä»èä½¿å»å­¦ä¸ä¸äººå£«è½å¤çè§£åä¿¡ä»» AI çè¯æ­å³ç­ãå®ä»¬ä¸å½ä»çå»çä¿å¥é¢åå°¤ä¸ºç¸å³ï¼å¨è¯¥é¢åï¼å¯¹ AI åºç¨çéæåº¦éæ±ä¸æ­å¢é¿ï¼ä»¥ç¡®ä¿å¶å¯é æ§ååä¹éå¾·çä½¿ç¨ãæ©å¼  ResNet ç¨ä½æ­£å¸¸ ResNet çæ¿ä»£åï¼ä»¥æé«è§ç½èç¼é¨ç¾ççåç±»åç¡®æ§å¹¶åå°æéçè®¡ç®æ¶é´ãæ¬å·¥ä½ä¸­ä½¿ç¨çæ°æ®éæ¯ç¼ç§ç¾çæºè½è¯å« (ODIR) æ°æ®éï¼è¿æ¯ä¸ä¸ªç»æåçç¼ç§æ°æ®åºï¼åå«å«ç±»æ¶µçå¤§å¤æ°å¸¸è§è§ç½èç¼é¨ç¾çãæ¬å·¥ä½ä¸­ä½¿ç¨çè¯ä¼°ææ åæ¬ç²¾ç¡®åº¦ãå¬åçãåç¡®åº¦å F1 å¾åãå¨è¿é¡¹å·¥ä½ä¸­ï¼å¯¹ ResNet-18ãResNet-34ãResNet-50ãResNet-101 å ResNet-152 äºä¸ªåä½çæ­£å¸¸ ResNet æ¨¡ååæ©å¼  ResNet æ¨¡åè¿è¡äºæ¯è¾ç ç©¶ãä¸æ­£å¸¸ ResNet ç¸æ¯ï¼æ©å¼  ResNet æ¨¡åæ¾ç¤ºåºæå¸æçç»æï¼å¨ ODIR å¤ç±»ç¾çåç±»ä¸­ï¼ä¸è¿°åä¸ªåä½çå¹³å F1 å¾åä¸º 0.71ã0.70ã0.69ã0.67 å 0.70ã

##### **A Survey on Trustworthiness in Foundation Models for Medical Image Analysis**
2407.15851v1 by Congzhen Shi, Ryan Rezai, Jiaxi Yang, Qi Dou, Xiaoxiao Li

The rapid advancement of foundation models in medical imaging represents a
significant leap toward enhancing diagnostic accuracy and personalized
treatment. However, the deployment of foundation models in healthcare
necessitates a rigorous examination of their trustworthiness, encompassing
privacy, robustness, reliability, explainability, and fairness. The current
body of survey literature on foundation models in medical imaging reveals
considerable gaps, particularly in the area of trustworthiness. Additionally,
extant surveys on the trustworthiness of foundation models fail to address
their specific variations and applications within the medical imaging domain.
This survey paper reviews the current research on foundation models in the
major medical imaging applications, with a focus on segmentation, medical
report generation, medical question and answering (Q&A), and disease diagnosis,
which includes trustworthiness discussion in their manuscripts. We explore the
complex challenges of making foundation models for medical image analysis
trustworthy, associated with each application, and summarize the current
concerns and strategies to enhance trustworthiness. Furthermore, we explore the
future promises of these models in revolutionizing patient care. Our analysis
underscores the imperative for advancing towards trustworthy AI in medical
image analysis, advocating for a balanced approach that fosters innovation
while ensuring ethical and equitable healthcare delivery.

æè¦ï¼åºç¤æ¨¡åå¨é«å­¸å½±åä¸çå¿«éé²å±ä»£è¡¨èå¨å¢å¼·è¨ºæ·æºç¢ºåº¦ååäººåæ²»çæ¹é¢éåºäºä¸å¤§æ­¥ãç¶èï¼åºç¤æ¨¡åå¨é«çä¿å¥ä¸­çé¨ç½²éè¦å´æ ¼æª¢æ¥å¶å¯ä¿¡åº¦ï¼åæ¬é±ç§ãç©©å¥æ§ãå¯é æ§ãå¯è§£éæ§åå¬å¹³æ§ãç¶åéæ¼é«å­¸å½±åä¸­åºç¤æ¨¡åçèª¿æ¥æç»é¡¯ç¤ºåºç¸ç¶å¤§çå·®è·ï¼ç¹å¥æ¯å¨å¯ä¿¡åº¦æ¹é¢ãæ­¤å¤ï¼ç¾æçéæ¼åºç¤æ¨¡åå¯ä¿¡åº¦çèª¿æ¥æªè½è§£æ±ºå¶å¨é«å­¸å½±åé åå§çå·é«è®ååæç¨ãéç¯èª¿æ¥è«æåé¡§äºç¶åéæ¼åºç¤æ¨¡åå¨ä¸»è¦é«å­¸å½±åæç¨ä¸­çç ç©¶ï¼éé»éæ³¨åå²ãé«çå ±åçæãé«çåé¡åè§£ç­ (Q&A) ä»¥åç¾çè¨ºæ·ï¼å¶ä¸­åæ¬æç¨¿ä¸­çå¯ä¿¡åº¦è¨è«ãæåæ¢è¨äºè®ç¨æ¼é«å­¸å½±ååæçåºç¤æ¨¡åå¼å¾ä¿¡è³´çè¤éææ°ï¼èæ¯åæç¨ç¸éï¼ä¸¦ç¸½çµäºç¶åæé«å¯ä¿¡åº¦çåé¡åç­ç¥ãæ­¤å¤ï¼æåæ¢è¨äºéäºæ¨¡åå¨é©æ°æ£èç§è­·æ¹é¢çæªä¾åæ¯ãæåçåæå¼·èª¿äºå¨é«å­¸å½±ååæä¸­æèå¯ä¿¡è³´çäººå·¥æºæ§éé²çå¿è¦æ§ï¼æå¡ä¸ç¨®å¹³è¡¡çæ¹æ³ï¼æ¢è½ä¿é²åµæ°ï¼åè½ç¢ºä¿éå¾·åå¬å¹³çé«çä¿å¥æåã

##### **The Impact of an XAI-Augmented Approach on Binary Classification with Scarce Data**
2407.06206v1 by Ximing Wen, Rosina O. Weber, Anik Sen, Darryl Hannan, Steven C. Nesbit, Vincent Chan, Alberto Goffi, Michael Morris, John C. Hunninghake, Nicholas E. Villalobos, Edward Kim, Christopher J. MacLellan

Point-of-Care Ultrasound (POCUS) is the practice of clinicians conducting and
interpreting ultrasound scans right at the patient's bedside. However, the
expertise needed to interpret these images is considerable and may not always
be present in emergency situations. This reality makes algorithms such as
machine learning classifiers extremely valuable to augment human decisions.
POCUS devices are becoming available at a reasonable cost in the size of a
mobile phone. The challenge of turning POCUS devices into life-saving tools is
that interpretation of ultrasound images requires specialist training and
experience. Unfortunately, the difficulty to obtain positive training images
represents an important obstacle to building efficient and accurate
classifiers. Hence, the problem we try to investigate is how to explore
strategies to increase accuracy of classifiers trained with scarce data. We
hypothesize that training with a few data instances may not suffice for
classifiers to generalize causing them to overfit. Our approach uses an
Explainable AI-Augmented approach to help the algorithm learn more from less
and potentially help the classifier better generalize.

æè¦ï¼åºéè¶é³æ³¢ (POCUS) æ¯è¨åºé«å¸«å¨æ£èåºéé²è¡åè§£è®è¶é³æ³¢ææçå¯¦åãç¶èï¼è§£è®éäºå½±åæéçå°æ¥­ç¥è­ç¸ç¶å¯è§ï¼èä¸å¨ç·æ¥ææ³ä¸å¯è½ä¸¦éé¨æå·åãéç¨®ç¾å¯¦ææ³ä½¿å¾æ©å¨å­¸ç¿åé¡å¨ç­æ¼ç®æ³å°æ¼å å¼·äººé¡æ±ºç­è®å¾æ¥µçºæå¹å¼ãPOCUS è£ç½®æ­£ä»¥åçææ¬æ¨åºï¼å°ºå¯¸çºææ©å¤§å°ãå° POCUS è£ç½®è½è®çºæçå·¥å·çææ°å¨æ¼ï¼è§£è®è¶é³æ³¢å½±åéè¦å°éè¨ç·´åç¶é©ãä¸å¹¸çæ¯ï¼åå¾æ­£åè¨ç·´å½±åçå°é£åº¦ä»£è¡¨èå»ºç½®ææçä¸æºç¢ºçåé¡å¨çä¸å¤§éç¤ãå æ­¤ï¼æååè©¦æ¢è¨çåé¡æ¯å¦ä½æ¢ç´¢ç­ç¥ï¼ä»¥æé«ä½¿ç¨ç¨çè³æè¨ç·´çåé¡å¨çæºç¢ºåº¦ãæååè¨­ä½¿ç¨å°æ¸è³æå¯¦ä¾é²è¡è¨ç·´å¯è½ä¸è¶³ä»¥è®åé¡å¨æ¦æ¬ï¼å°è´å®åéåº¦æ¬åãæåçåæ³ä½¿ç¨å¯è§£é AI å¢å¼·æ¹æ³ï¼ä»¥åå©æ¼ç®æ³å¾è¼å°çè³æä¸­å­¸ç¿æ´å¤ï¼ä¸¦æ½å¨åå©åé¡å¨æ´å¥½å°æ¦æ¬ã

##### **Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach**
2407.00167v1 by Sai Krishna Revanth Vuruma, Dezhi Wu, Saborny Sen Gupta, Lucas Aust, Valerie Lookingbill, Wyatt Bellamy, Yang Ren, Erin Kasson, Li-Shiun Chen, Patricia Cavazos-Rehg, Dian Hu, Ming Huang

In recent years, the United States has witnessed a significant surge in the
popularity of vaping or e-cigarette use, leading to a notable rise in cases of
e-cigarette and vaping use-associated lung injury (EVALI) that caused
hospitalizations and fatalities during the EVALI outbreak in 2019, highlighting
the urgency to comprehend vaping behaviors and develop effective strategies for
cessation. Due to the ubiquity of social media platforms, over 4.7 billion
users worldwide use them for connectivity, communications, news, and
entertainment with a significant portion of the discourse related to health,
thereby establishing social media data as an invaluable organic data resource
for public health research. In this study, we extracted a sample dataset from
one vaping sub-community on Reddit to analyze users' quit-vaping intentions.
Leveraging OpenAI's latest large language model GPT-4 for sentence-level quit
vaping intention detection, this study compares the outcomes of this model
against layman and clinical expert annotations. Using different prompting
strategies such as zero-shot, one-shot, few-shot and chain-of-thought
prompting, we developed 8 prompts with varying levels of detail to explain the
task to GPT-4 and also evaluated the performance of the strategies against each
other. These preliminary findings emphasize the potential of GPT-4 in social
media data analysis, especially in identifying users' subtle intentions that
may elude human detection.

æè¦ï¼è¿å¹´ä¾ï¼ç¾åè¦è­äºé»å­çæé»å­é¦è¸ä½¿ç¨çå¤§å¹æ¿å¢ï¼å°è´é»å­çåé»å­çä½¿ç¨ç¸éèºæå· (EVALI) çä¾é¡¯èå¢å ï¼å¨ 2019 å¹´ EVALI çç¼æéé æä½é¢åæ­»äº¡ï¼å¸é¡¯äºçè§£é»å­çè¡çºåå¶å®æææè¸ç­ç¥çè¿«åæ§ãç±æ¼ç¤¾ç¾¤åªé«å¹³å°çæ®åï¼å¨çè¶é 47 åä½¿ç¨èä½¿ç¨å®åé²è¡é£çµãæºéãæ°èåå¨æ¨ï¼å¶ä¸­å¾å¤§ä¸é¨åèå¥åº·ç¸éï¼å æ­¤å°ç¤¾ç¾¤åªé«è³æå»ºç«çºå¬å±è¡çç ç©¶ä¸­ç¡å¹çææ©è³æè³æºãå¨æ¬ç ç©¶ä¸­ï¼æåå¾ Reddit ä¸ä¸åé»å­çå­ç¤¾ç¾¤ä¸­æåä¸åç¯ä¾è³æéï¼ä»¥åæä½¿ç¨èçæé»å­çæåãå©ç¨ OpenAI ææ°çå¤§åèªè¨æ¨¡å GPT-4 é²è¡å¥å­å±¤ç´çæé»å­çæååµæ¸¬ï¼æ¬ç ç©¶æ¯è¼äºæ­¤æ¨¡åççµæèå¤è¡äººåè¨åºå°å®¶è¨»è§£ãä½¿ç¨ä¸åçæç¤ºç­ç¥ï¼ä¾å¦é¶æ¬¡å­¸ç¿ãä¸æ¬¡å­¸ç¿ãå°æ¬¡å­¸ç¿åæèéæç¤ºï¼æåéç¼äº 8 åæç¤ºï¼è©³ç´°ç¨åº¦ä¸åï¼å GPT-4 è§£éä»»åï¼ä¸¦è©ä¼°éäºç­ç¥å½¼æ­¤ä¹éçæè½ãéäºåæ­¥ç¼ç¾å¼·èª¿äº GPT-4 å¨ç¤¾ç¾¤åªé«è³æåæä¸­çæ½åï¼ç¹å¥æ¯å¨è­å¥äººé¡åµæ¸¬å¯è½ç¡æ³å¯è¦ºçä½¿ç¨èå¾®å¦æåæ¹é¢ã

##### **Towards Compositional Interpretability for XAI**
2406.17583v1 by Sean Tull, Robin Lorenz, Stephen Clark, Ilyas Khan, Bob Coecke

Artificial intelligence (AI) is currently based largely on black-box machine
learning models which lack interpretability. The field of eXplainable AI (XAI)
strives to address this major concern, being critical in high-stakes areas such
as the finance, legal and health sectors.
  We present an approach to defining AI models and their interpretability based
on category theory. For this we employ the notion of a compositional model,
which sees a model in terms of formal string diagrams which capture its
abstract structure together with its concrete implementation. This
comprehensive view incorporates deterministic, probabilistic and quantum
models. We compare a wide range of AI models as compositional models, including
linear and rule-based models, (recurrent) neural networks, transformers, VAEs,
and causal and DisCoCirc models.
  Next we give a definition of interpretation of a model in terms of its
compositional structure, demonstrating how to analyse the interpretability of a
model, and using this to clarify common themes in XAI. We find that what makes
the standard 'intrinsically interpretable' models so transparent is brought out
most clearly diagrammatically. This leads us to the more general notion of
compositionally-interpretable (CI) models, which additionally include, for
instance, causal, conceptual space, and DisCoCirc models.
  We next demonstrate the explainability benefits of CI models. Firstly, their
compositional structure may allow the computation of other quantities of
interest, and may facilitate inference from the model to the modelled
phenomenon by matching its structure. Secondly, they allow for diagrammatic
explanations for their behaviour, based on influence constraints, diagram
surgery and rewrite explanations. Finally, we discuss many future directions
for the approach, raising the question of how to learn such meaningfully
structured models in practice.

æè¦ï¼<paragraph>äººå·¥æºæ§ï¼AIï¼ç®åå¨å¾å¤§ç¨åº¦ä¸ä¾è³´æ¼ç¼ºä¹å¯è§£éæ§çé»çæ©å¨å­¸ç¿æ¨¡åãå¯è§£éæ§äººå·¥æºæ§ï¼XAIï¼é åè´åæ¼è§£æ±ºéåä¸»è¦åé¡ï¼éå¨éèãæ³å¾åå¥åº·ç­é«é¢¨éªé åè³ééè¦ã
æåæåºäºä¸ç¨®åºæ¼ç¯çè«å®ç¾© AI æ¨¡ååå¶å¯è§£éæ§çæ¹æ³ãçºæ­¤ï¼æåæ¡ç¨çµåæ¨¡åçæ¦å¿µï¼å®ä»¥å½¢å¼å¼¦åçå½¢å¼çå¾æ¨¡åï¼éäºå¼¦åæç²äºæ¨¡åçæ½è±¡çµæ§åå¶å·é«å¯¦ç¾ãéç¨®ç¶åè§é»åå«äºç¢ºå®æ§ãæ¦çæ§åéå­æ¨¡åãæåå°åç¨® AI æ¨¡åä½çºçµåæ¨¡åé²è¡æ¯è¼ï¼åæ¬ç·æ§ååºæ¼è¦åçæ¨¡åãï¼éè¿´ï¼ç¥ç¶ç¶²è·¯ãTransformerãVAEï¼ä»¥åå æå DisCoCirc æ¨¡åã
æ¥ä¸ä¾ï¼æåæ ¹ææ¨¡åççµåçµæ§çµ¦åºæ¨¡åè§£éçå®ç¾©ï¼å±ç¤ºå¦ä½åææ¨¡åçå¯è§£éæ§ï¼ä¸¦ä½¿ç¨å®ä¾æ¾æ¸ XAI ä¸­çå¸¸è¦ä¸»é¡ãæåç¼ç¾ï¼è®æ¨æºçãå§å¨å¯è§£éãæ¨¡åå¦æ­¤éæçåå å¨åè¡¨ä¸­è¡¨ç¾å¾æçºæ¸æ¥ãéå¼å°æåå¾åºæ´ä¸è¬ççµåå¯è§£éï¼CIï¼æ¨¡åæ¦å¿µï¼å®å¦å¤éåæ¬å æãæ¦å¿µç©ºéå DisCoCirc æ¨¡åã
æ¥ä¸ä¾ï¼æåå±ç¤ºäº CI æ¨¡åçå¯è§£éæ§åªå¢ãé¦åï¼å®åççµåçµæ§åè¨±è¨ç®å¶ä»æèè¶£çéï¼ä¸¦å¯è½ééå¹éæ¨¡åççµæ§ä¾ä¿é²å¾æ¨¡åå°è¢«å»ºæ¨¡ç¾è±¡çæ¨çãå¶æ¬¡ï¼å®ååè¨±å°å¶è¡çºé²è¡åè§£èªªæï¼éäºèªªæåºæ¼å½±é¿ç´æãåè§£æè¡åéå¯«èªªæãæå¾ï¼æåè¨è«äºéç¨®æ¹æ³çè¨±å¤æªä¾æ¹åï¼æåºäºå¦ä½å¨å¯¦è¸ä¸­å­¸ç¿éç¨®ææç¾©ççµæ§åæ¨¡åçåé¡ã</paragraph>

##### **Unlocking the Potential of Metaverse in Innovative and Immersive Digital Health**
2406.07114v2 by Fatemeh Ebrahimzadeh, Ramin Safa

The concept of Metaverse has attracted a lot of attention in various fields
and one of its important applications is health and treatment. The Metaverse
has enormous potential to transform healthcare by changing patient care,
medical education, and the way teaching/learning and research are done. The
purpose of this research is to provide an introduction to the basic concepts
and fundamental technologies of the Metaverse. This paper examines the pros and
cons of the Metaverse in healthcare context and analyzes its potential from the
technology and AI perspective. In particular, the role of machine learning
methods is discussed; We will explain how machine learning algorithms can be
applied to the Metaverse generated data to gain better insights in healthcare
applications. Additionally, we examine the future visions of the Metaverse in
health delivery, by examining emerging technologies such as blockchain and also
addressing privacy concerns. The findings of this study contribute to a deeper
understanding of the applications of Metaverse in healthcare and its potential
to revolutionize the delivery of medical services.

æè¦ï¼åå®å®çæ¦å¿µå¨ååé åé½ååéæ³¨ï¼å¶éè¦æç¨ä¹ä¸ä¾¿æ¯é«çä¿å¥ãåå®å®æå·¨å¤§çæ½åééæ¹è®çæ£ç§è­·ãé«å­¸æè²ï¼ä»¥åæå­¸/å­¸ç¿åç ç©¶çæ¹å¼ä¾è½åé«çä¿å¥ãæ¬ç ç©¶çç®çæ¯æä¾åå®å®åºæ¬æ¦å¿µååºç¤æè¡çä»ç´¹ãæ¬ææ¢è¨äºåå®å®å¨é«çä¿å¥èæ¯ä¸çåªç¼ºé»ï¼ä¸¦å¾æè¡å AI çè§åº¦åæå¶æ½åãç¹å¥æ¯ï¼è¨è«äºæ©å¨å­¸ç¿æ¹æ³çè§è²ï¼æåå°èªªæå¦ä½å°æ©å¨å­¸ç¿æ¼ç®æ³æç¨æ¼åå®å®ç¢ççè³æï¼ä»¥ç²å¾é«çä¿å¥æç¨æ¹é¢çæ´ä½³è¦è§£ãæ­¤å¤ï¼æåééæ¢è¨åå¡éç­æ°èæè¡ï¼ä¸¦è§£æ±ºé±ç§åé¡ï¼ä¾æ¢è¨åå®å®å¨é«çä¿å¥æ¹é¢çæªä¾é¡æ¯ãæ¬ç ç©¶çç¼ç¾æå©æ¼æ´æ·±å¥å°äºè§£åå®å®å¨é«çä¿å¥ä¸­çæç¨ï¼ä»¥åå¶å¨é«çæåæä¾æ¹é¢ç¼æ®é©å½æ§è®é©çæ½åã

##### **AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI**
2406.06728v1 by K M Tawsik Jawad, Anusha Verma, Fathi Amsaad

Chronic Kidney Disease (CKD) is one of the widespread Chronic diseases with
no known ultimo cure and high morbidity. Research demonstrates that progressive
Chronic Kidney Disease (CKD) is a heterogeneous disorder that significantly
impacts kidney structure and functions, eventually leading to kidney failure.
With the progression of time, chronic kidney disease has moved from a
life-threatening disease affecting few people to a common disorder of varying
severity. The goal of this research is to visualize dominating features,
feature scores, and values exhibited for early prognosis and detection of CKD
using ensemble learning and explainable AI. For that, an AI-driven predictive
analytics approach is proposed to aid clinical practitioners in prescribing
lifestyle modifications for individual patients to reduce the rate of
progression of this disease. Our dataset is collected on body vitals from
individuals with CKD and healthy subjects to develop our proposed AI-driven
solution accurately. In this regard, blood and urine test results are provided,
and ensemble tree-based machine-learning models are applied to predict unseen
cases of CKD. Our research findings are validated after lengthy consultations
with nephrologists. Our experiments and interpretation results are compared
with existing explainable AI applications in various healthcare domains,
including CKD. The comparison shows that our developed AI models, particularly
the Random Forest model, have identified more features as significant
contributors than XgBoost. Interpretability (I), which measures the ratio of
important to masked features, indicates that our XgBoost model achieved a
higher score, specifically a Fidelity of 98\%, in this metric and naturally in
the FII index compared to competing models.

æè¦ï¼æ¢æ§èèçï¼CKDï¼æ¯ä¸ç¨®å»£æ³çæ¢æ§ç¾çï¼æ²æå·²ç¥çæçµçæ³ä¸ç¼ççå¾é«ãç ç©¶è¡¨æï¼é²è¡æ§æ¢æ§èèçï¼CKDï¼æ¯ä¸ç¨®ç°è³ªæ§ç¾çï¼æé¡¯èå½±é¿èèçµæ§ååè½ï¼æçµå°è´èè¡°ç«­ãé¨èæéçæ¨ç§»ï¼æ¢æ§èèçå·²å¾å½±é¿å°æ¸äººçè´å½ç¾çè½è®çºä¸ç¨®å´éç¨åº¦ä¸åçå¸¸è¦ç¾çãæ¬ç ç©¶çç®æ¨æ¯ä½¿ç¨éæå­¸ç¿åå¯è§£éç AI é²è¡æ©æé å¾å CKD æª¢æ¸¬ï¼ä¸¦è¦è¦ºåä¸»å°ç¹å¾µãç¹å¾µåæ¸åè¡¨ç¾åºçå¼ãçºæ­¤ï¼æåºäºä¸ç¨® AI é©åçé æ¸¬åææ¹æ³ï¼ä»¥å¹«å©è¨åºé«ççºåå¥æ£èéå·çæ´»æ¹å¼ä¿®æ¹å»ºè­°ï¼ä»¥éä½éç¨®ç¾ççé²å±éåº¦ãæåçæ¸æéæ¯å¾ CKD æ£èåå¥åº·åè©¦èçèº«é«çå½é«å¾µä¸­æ¶éçï¼ä»¥æºç¢ºéç¼æåæåºç AI é©åçè§£æ±ºæ¹æ¡ãå¨éæ¹é¢ï¼æä¾äºè¡æ¶²åå°¿æ¶²æª¢æ¸¬çµæï¼ä¸¦æç¨åºæ¼éææ¨¹çæ©å¨å­¸ç¿æ¨¡åä¾é æ¸¬æªç¼ç¾ç CKD çä¾ãæåçç ç©¶çµæç¶éèèèç§é«ççé·æè«®è©¢å¾å¾å°é©è­ãæåçå¯¦é©åè§£éçµæèåç¨®é«çä¿å¥é åä¸­ç¾æçå¯è§£é AI æç¨é²è¡äºæ¯è¼ï¼åæ¬ CKDãæ¯è¼è¡¨æï¼æåéç¼ç AI æ¨¡åï¼ç¹å¥æ¯é¨æ©æ£®ææ¨¡åï¼å·²ç¶ç¢ºå®äºæ¯ XgBoost æ´å¤ä½çºéè¦è²¢ç»èçç¹å¾µãå¯è§£éæ§ (I) è¡¡ééè¦ç¹å¾µèæ©èç¹å¾µçæ¯çï¼è¡¨ææåç XgBoost æ¨¡åå¨éåææ¨ä¸­ç²å¾äºæ´é«çåæ¸ï¼ç¹å¥æ¯ 98% çä¿çåº¦ï¼ä¸¦ä¸å¨ FII ææ¸ä¸­èªç¶é«æ¼ç«¶ç­æ¨¡åã

##### **Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook**
2406.05984v1 by Yusif Ibrahimov, Tarique Anwar, Tommy Yuan

Mental health constitutes a complex and pervasive global challenge, affecting
millions of lives and often leading to severe consequences. In this paper, we
conduct a thorough survey to explore the intersection of data science,
artificial intelligence, and mental healthcare, focusing on the recent
developments of mental disorder detection through online social media (OSM). A
significant portion of the population actively engages in OSM platforms,
creating a vast repository of personal data that holds immense potential for
mental health analytics. The paper navigates through traditional diagnostic
methods, state-of-the-art data- and AI-driven research studies, and the
emergence of explainable AI (XAI) models for mental healthcare. We review
state-of-the-art machine learning methods, particularly those based on modern
deep learning, while emphasising the need for explainability in healthcare AI
models. The experimental design section provides insights into prevalent
practices, including available datasets and evaluation approaches. We also
identify key issues and challenges in the field and propose promising future
research directions. As mental health decisions demand transparency,
interpretability, and ethical considerations, this paper contributes to the
ongoing discourse on advancing XAI in mental healthcare through social media.
The comprehensive overview presented here aims to guide researchers,
practitioners, and policymakers in developing the area of mental disorder
detection.

æè¦ï¼å¿çå¥åº·æ§æäºä¸é è¤éä¸æ®éçå¨çææ°ï¼å½±é¿äºæ¸ç¾è¬äººççæ´»ï¼ä¸¦ç¶å¸¸å°è´å´éçå¾æãå¨æ¬æä¸­ï¼æåé²è¡äºä¸é å¾¹åºçèª¿æ¥ï¼ä»¥æ¢ç´¢æ¸æç§å­¸ãäººå·¥æºæ§åå¿çä¿å¥çäº¤éï¼éé»éæ³¨ééç·ä¸ç¤¾äº¤åªé« (OSM) é²è¡å¿çç¾çæª¢æ¸¬çææ°ç¼å±ãå¾å¤§ä¸é¨åäººå£ç©æ¥µåè OSM å¹³å°ï¼åµé äºä¸åé¾å¤§çäººå¡è³æåº«ï¼å°å¿çå¥åº·åæå·æå·¨å¤§çæ½åãæ¬ææ¢è¨äºå³çµ±çè¨ºæ·æ¹æ³ãæåé²çè³æå AI é©åçç ç©¶ï¼ä»¥åå¿çä¿å¥ä¸­å¯è§£é AI (XAI) æ¨¡åçåºç¾ãæååé¡§äºæåé²çæ©å¨å­¸ç¿æ¹æ³ï¼ç¹å¥æ¯é£äºåºæ¼ç¾ä»£æ·±åº¦å­¸ç¿çæ¹æ³ï¼åæå¼·èª¿äºé«çä¿å¥ AI æ¨¡åä¸­å¯è§£éæ§çå¿è¦æ§ãå¯¦é©è¨­è¨é¨åæä¾äºå°æ®éåæ³çè¦è§£ï¼åæ¬å¯ç¨çè³æéåè©ä¼°æ¹æ³ãæåéæ¾åºè©²é åçä¸»è¦åé¡åææ°ï¼ä¸¦æåºäºæå¸æçæªä¾ç ç©¶æ¹åãç±æ¼å¿çå¥åº·æ±ºç­éè¦éæåº¦ãå¯è§£éæ§åéå¾·èéï¼æ¬ææå©æ¼æ¨é²å¿çä¿å¥ä¸­ééç¤¾äº¤åªé«æ¨é² XAI çæçºè¨è«ãéè£¡æåºçå¨é¢æ¦è¿°æ¨å¨å¼å°ç ç©¶äººå¡ãå¾æ¥­äººå¡åæ¿ç­å¶å®èç¼å±å¿çç¾çæª¢æ¸¬é åã

##### **Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance**
2406.05746v1 by Zhan Zhang, Qin Zhang, Yang Jiao, Lin Lu, Lin Ma, Aihua Liu, Xiao Liu, Juan Zhao, Yajun Xue, Bing Wei, Mingxia Zhang, Ru Gao, Hong Zhao, Jie Lu, Fan Li, Yang Zhang, Yiming Wang, Lei Zhang, Fengwei Tian, Jie Hu, Xin Gou

AI-aided clinical diagnosis is desired in medical care. Existing deep
learning models lack explainability and mainly focus on image analysis. The
recently developed Dynamic Uncertain Causality Graph (DUCG) approach is
causality-driven, explainable, and invariant across different application
scenarios, without problems of data collection, labeling, fitting, privacy,
bias, generalization, high cost and high energy consumption. Through close
collaboration between clinical experts and DUCG technicians, 46 DUCG models
covering 54 chief complaints were constructed. Over 1,000 diseases can be
diagnosed without triage. Before being applied in real-world, the 46 DUCG
models were retrospectively verified by third-party hospitals. The verified
diagnostic precisions were no less than 95%, in which the diagnostic precision
for every disease including uncommon ones was no less than 80%. After
verifications, the 46 DUCG models were applied in the real-world in China. Over
one million real diagnosis cases have been performed, with only 17 incorrect
diagnoses identified. Due to DUCG's transparency, the mistakes causing the
incorrect diagnoses were found and corrected. The diagnostic abilities of the
clinicians who applied DUCG frequently were improved significantly. Following
the introduction to the earlier presented DUCG methodology, the recommendation
algorithm for potential medical checks is presented and the key idea of DUCG is
extracted.

æè¦ï¼<paragraph>é«çç§è­·ä¸­éè¦ AI è¼å©çè¨åºè¨ºæ·ãç¾æçæ·±åº¦å­¸ç¿æ¨¡åç¼ºä¹å¯è§£éæ§ï¼ä¸¦ä¸ä¸»è¦å°æ³¨æ¼å½±ååæãæè¿éç¼çåæä¸ç¢ºå®å æéä¿å (DUCG) æ¹æ³æ¯å æé©åçãå¯è§£éçï¼ä¸¦ä¸å¨ä¸åçæç¨å ´æ¯ä¸­æ¯ä¸è®çï¼æ²æè³ææ¶éãæ¨è¨ãæ¬åãé±ç§ãåè¦ãæ¦åãé«ææ¬åé«è½èçåé¡ãééè¨åºå°å®¶å DUCG æè¡äººå¡ä¹éçå¯ååä½ï¼æ§å»ºäºæ¶µè 54 åä¸»è¨´ç 46 å DUCG æ¨¡åãå¯ä»¥å¨æ²æåæµçææ³ä¸è¨ºæ·åº 1,000 å¤ç¨®ç¾çãå¨æç¨æ¼å¯¦éä¸çä¹åï¼46 å DUCG æ¨¡åå·²ç±ç¬¬ä¸æ¹é«é¢åæº¯æ§é©è­ãé©è­çè¨ºæ·ç²¾åº¦ä¸ä½æ¼ 95%ï¼å¶ä¸­åæ¬ç½è¦ç¾çå¨å§çæ¯ç¨®ç¾ççè¨ºæ·ç²¾åº¦ä¸ä½æ¼ 80%ãé©è­å¾ï¼46 å DUCG æ¨¡åå·²å¨ä¸­åå¯¦éæç¨ãå·²ç¶å·è¡äºè¶éä¸ç¾è¬åçå¯¦è¨ºæ·æ¡ä¾ï¼åç¼ç¾ 17 åä¸æ­£ç¢ºçè¨ºæ·ãç±æ¼ DUCG çéææ§ï¼ç¼ç¾ä¸¦ç³¾æ­£äºå°è´ä¸æ­£ç¢ºè¨ºæ·çé¯èª¤ãé »ç¹æç¨ DUCG çè¨åºé«ççè¨ºæ·è½åå¾å°äºé¡¯èæé«ãå¨ä»ç´¹äºåé¢æåºç DUCG æ¹æ³è«ä¹å¾ï¼æåºäºæ½å¨å¥åº·æª¢æ¥çæ¨è¦æ¼ç®æ³ï¼ä¸¦æåäº DUCG çééµææ³ã</paragraph>

##### **Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability**
2406.12897v1 by Faseela Abdullakutty, Younes Akbari, Somaya Al-Maadeed, Ahmed Bouridane, Rifat Hamoudi

It is imperative that breast cancer is detected precisely and timely to
improve patient outcomes. Diagnostic methodologies have traditionally relied on
unimodal approaches; however, medical data analytics is integrating diverse
data sources beyond conventional imaging. Using multi-modal techniques,
integrating both image and non-image data, marks a transformative advancement
in breast cancer diagnosis. The purpose of this review is to explore the
burgeoning field of multimodal techniques, particularly the fusion of
histopathology images with non-image data. Further, Explainable AI (XAI) will
be used to elucidate the decision-making processes of complex algorithms,
emphasizing the necessity of explainability in diagnostic processes. This
review utilizes multi-modal data and emphasizes explainability to enhance
diagnostic accuracy, clinician confidence, and patient engagement, ultimately
fostering more personalized treatment strategies for breast cancer, while also
identifying research gaps in multi-modality and explainability, guiding future
studies, and contributing to the strategic direction of the field.

æè¦ï¼ç²¾ç¢ºä¸åæå°åµæ¸¬ä¹³çå°æ¼æ¹åæ£èé å¾è³ééè¦ãè¨ºæ·æ¹æ³å³çµ±ä¸ä¾è³´æ¼å®ä¸æ¨¡å¼æ¹æ³ï¼ç¶èï¼é«çè³æåææ­£å¨æ´åè¶è¶å³çµ±å½±åçåç¨®è³æä¾æºãä½¿ç¨æ´åå½±ååéå½±åè³æçå¤æ¨¡å¼æè¡ï¼æ¨èªèä¹³çè¨ºæ·çè®é©æ§é²å±ãæ¬ç¯ç¶è¿°çç®çæ¯æ¢è¨å¤æ¨¡å¼æè¡çæ°èé åï¼ç¹å¥æ¯å°çµç¹ççå­¸å½±åèéå½±åè³æèåãæ­¤å¤ï¼å¯è§£éäººå·¥æºæ§ (XAI) å°ç¨æ¼é¡æè¤éæ¼ç®æ³çæ±ºç­éç¨ï¼å¼·èª¿è¨ºæ·éç¨ä¸­å¯è§£éæ§çå¿è¦æ§ãæ¬ç¶è¿°å©ç¨å¤æ¨¡å¼è³æä¸¦å¼·èª¿å¯è§£éæ§ï¼ä»¥æé«è¨ºæ·æºç¢ºæ§ãè¨åºé«å¸«çä¿¡å¿åæ£èåèåº¦ï¼æçµä¿é²ä¹³çæ´åäººåçæ²»çç­ç¥ï¼åæä¹æ¾åºå¤æ¨¡å¼åå¯è§£éæ§çç ç©¶å·®è·ï¼å¼å°æªä¾çç ç©¶ï¼ä¸¦çºè©²é åçç­ç¥æ¹åååºè²¢ç»ã

##### **Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**
2406.04612v1 by Yong-Min Shin, Siqing Li, Xin Cao, Won-Yong Shin

The self-attention mechanism has been adopted in several widely-used
message-passing neural networks (MPNNs) (e.g., GATs), which adaptively controls
the amount of information that flows along the edges of the underlying graph.
This usage of attention has made such models a baseline for studies on
explainable AI (XAI) since interpretations via attention have been popularized
in various domains (e.g., natural language processing and computer vision).
However, existing studies often use naive calculations to derive attribution
scores from attention, and do not take the precise and careful calculation of
edge attribution into consideration. In our study, we aim to fill the gap
between the widespread usage of attention-enabled MPNNs and their potential in
largely under-explored explainability, a topic that has been actively
investigated in other areas. To this end, as the first attempt, we formalize
the problem of edge attribution from attention weights in GNNs. Then, we
propose GATT, an edge attribution calculation method built upon the computation
tree. Through comprehensive experiments, we demonstrate the effectiveness of
our proposed method when evaluating attributions from GATs. Conversely, we
empirically validate that simply averaging attention weights over graph
attention layers is insufficient to interpret the GAT model's behavior. Code is
publicly available at https://github.com/jordan7186/GAtt/tree/main.

æè¦ï¼èªæ³¨æåæ©å¶å·²è¢«æ¡ç¨æ¼å¤åå»£æ³ä½¿ç¨çè¨æ¯å³éç¥ç¶ç¶²è·¯ (MPNN)ï¼ä¾å¦ GATï¼ï¼å®å¯ä»¥èªé©æå°æ§å¶æ²¿èåºå±¤åå½¢éç·£æµåçè³è¨éãéç¨®æ³¨æåçä½¿ç¨ä½¿å¾æ­¤é¡æ¨¡åæçºå¯è§£é AI (XAI) ç ç©¶çåºç·ï¼å çºééæ³¨æåçè©®éå·²å¨åç¨®é åï¼ä¾å¦èªç¶èªè¨èçåé»è¦è¦è¦ºï¼ä¸­æ®åãç¶èï¼ç¾æçç ç©¶éå¸¸ä½¿ç¨å¤©ççè¨ç®æ¹æ³å¾æ³¨æåä¸­æ¨å°åºæ­¸å åæ¸ï¼ä¸¦ä¸æ²æèæ®å°éç·£æ­¸å çç²¾ç¢ºä¸ä»ç´°çè¨ç®ãå¨æåçç ç©¶ä¸­ï¼æåæ¨å¨å¡«è£æ³¨æååç¨ MPNN çå»£æ³ä½¿ç¨èå®åå¨å¾å¤§ç¨åº¦ä¸æªè¢«ååæ¢ç´¢çå¯è§£éæ§ä¹éçå·®è·ï¼éåä¸»é¡å·²å¨å¶ä»é åç©æ¥µç ç©¶ãçºæ­¤ï¼ä½çºç¬¬ä¸æ¬¡åè©¦ï¼æåå° GNN ä¸­æ³¨æåæ¬éçéç·£æ­¸å åé¡å½¢å¼åãç¶å¾ï¼æåæåº GATTï¼ä¸ç¨®å»ºç«å¨è¨ç®æ¨¹ä¸çéç·£æ­¸å è¨ç®æ¹æ³ãééå¨é¢çå¯¦é©ï¼æåå±ç¤ºäºæåæåºçæ¹æ³å¨è©ä¼° GAT çæ­¸å ææå·æçææãç¸åå°ï¼æåæç¶é©é©è­äºåå°åæ³¨æåå±¤ä¸çæ³¨æåæ¬éåå¹³åå¼ä¸è¶³ä»¥è©®é GAT æ¨¡åçè¡çºãç¨å¼ç¢¼å·²å¬éæ¼ https://github.com/jordan7186/GAtt/tree/mainã

##### **Using Explainable AI for EEG-based Reduced Montage Neonatal Seizure Detection**
2406.16908v3 by Dinuka Sandun Udayantha, Kavindu Weerasinghe, Nima Wickramasinghe, Akila Abeyratne, Kithmin Wickremasinghe, Jithangi Wanigasinghe, Anjula De Silva, Chamira U. S. Edussooriya

The neonatal period is the most vulnerable time for the development of
seizures. Seizures in the immature brain lead to detrimental consequences,
therefore require early diagnosis. The gold-standard for neonatal seizure
detection currently relies on continuous video-EEG monitoring; which involves
recording multi-channel electroencephalogram (EEG) alongside real-time video
monitoring within a neonatal intensive care unit (NICU). However, video-EEG
monitoring technology requires clinical expertise and is often limited to
technologically advanced and resourceful settings. Cost-effective new
techniques could help the medical fraternity make an accurate diagnosis and
advocate treatment without delay. In this work, a novel explainable deep
learning model to automate the neonatal seizure detection process with a
reduced EEG montage is proposed, which employs convolutional nets, graph
attention layers, and fully connected layers. Beyond its ability to detect
seizures in real-time with a reduced montage, this model offers the unique
advantage of real-time interpretability. By evaluating the performance on the
Zenodo dataset with 10-fold cross-validation, the presented model achieves an
absolute improvement of 8.31% and 42.86% in area under curve (AUC) and recall,
respectively.

æè¦ï¼æ°çåææ¯å¤§è¦ç¼è²æèå¼±çææï¼å®¹æåºç¾ç²çç¼ä½ãå¤§è¦ç¼è²ä¸æçæåºç¾ç²çç¼ä½æé æä¸è¯å¾æï¼å æ­¤éè¦åæ©è¨ºæ·ãç®åæ°çåç²çç¼ä½çé»éæ¨æºä¾è³´æ¼é£çºçè¦è¨è¦é»å (EEG) ç£æ¸¬ï¼å¶ä¸­åæ¬å¨æ°çåå è­·çæ¿ (NICU) å§åæé²è¡å¤é »éè¦é»å (EEG) è¨éåå³æè¦è¨ç£æ§ãç¶èï¼è¦è¨è¦é»åç£æ§æè¡éè¦è¨åºå°æ¥­ç¥è­ï¼èä¸éå¸¸åéæ¼æè¡åé²ä¸è³æºè±å¯çç°å¢ãå·ææ¬æççæ°æè¡å¯ä»¥å¹«å©é«ççæºç¢ºè¨ºæ·ä¸¦ç«å³æå¡æ²»çãå¨éé å·¥ä½ä¸­ï¼æåºäºä¸åæ°ç©çå¯è§£éæ·±åº¦å­¸ç¿æ¨¡åï¼ä»¥èªååæ°çåç²çç¼ä½åµæ¸¬éç¨ï¼ä¸¦æ¡ç¨æ¸å°çè¦é»åè£ç½®ï¼å¶ä¸­æ¡ç¨äºå·ç©ç¥ç¶ç¶²è·¯ãåå½¢æ³¨æåå±¤åå¨é£æ¥å±¤ãé¤äºè½å¤ ä½¿ç¨æ¸å°çè£ç½®å³æåµæ¸¬ç²çç¼ä½å¤ï¼æ­¤æ¨¡åéæä¾äºå³æå¯è§£éæ§çç¨ç¹åªå¢ãééå¨ Zenodo è³æéä¸ä½¿ç¨ 10 åäº¤åé©è­è©ä¼°æè½ï¼ææåºçæ¨¡åå¨æ²ç·ä¸é¢ç© (AUC) åå¬åçæ¹é¢åå¥éå°äº 8.31% å 42.86% ççµå°æ¹åã

##### **Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques**
2406.00532v1 by Samita Bai, Sidra Nasir, Rizwan Ahmed Khan, Sheeraz Arif, Alexandre Meyer, Hubert Konik

Breast cancer (BC) stands as one of the most common malignancies affecting
women worldwide, necessitating advancements in diagnostic methodologies for
better clinical outcomes. This article provides a comprehensive exploration of
the application of Explainable Artificial Intelligence (XAI) techniques in the
detection and diagnosis of breast cancer. As Artificial Intelligence (AI)
technologies continue to permeate the healthcare sector, particularly in
oncology, the need for transparent and interpretable models becomes imperative
to enhance clinical decision-making and patient care. This review discusses the
integration of various XAI approaches, such as SHAP, LIME, Grad-CAM, and
others, with machine learning and deep learning models utilized in breast
cancer detection and classification. By investigating the modalities of breast
cancer datasets, including mammograms, ultrasounds and their processing with
AI, the paper highlights how XAI can lead to more accurate diagnoses and
personalized treatment plans. It also examines the challenges in implementing
these techniques and the importance of developing standardized metrics for
evaluating XAI's effectiveness in clinical settings. Through detailed analysis
and discussion, this article aims to highlight the potential of XAI in bridging
the gap between complex AI models and practical healthcare applications,
thereby fostering trust and understanding among medical professionals and
improving patient outcomes.

æè¦ï¼ä¹³ç (BC) æ¯å½±é¿å¨çå¥³æ§æå¸¸è¦çæ¡æ§è«ç¤ä¹ä¸ï¼å æ­¤éè¦é²æ­¥çè¨ºæ·æ¹æ³ï¼ä»¥æ¹åè¨åºçµæãæ¬æå¨é¢æ¢è¨äºå¯è§£éäººå·¥æºæ§ (XAI) æè¡å¨ä¹³çåµæ¸¬åè¨ºæ·ä¸­çæç¨ãé¨èäººå·¥æºæ§ (AI) æè¡æçºæ»²éé«çä¿å¥é åï¼ç¹å¥æ¯å¨è«ç¤å­¸ä¸­ï¼éæä¸å¯è§£éçæ¨¡åéæ±è®å¾å¢å¨å¿è¡ï¼ä»¥å¢å¼·è¨åºæ±ºç­å¶å®åæ£èç§è­·ãæ­¤ç¯è©è«æ¢è¨äºåç¨® XAI æ¹æ³çæ´åï¼ä¾å¦ SHAPãLIMEãGrad-CAM ç­ï¼ä»¥åç¨æ¼ä¹³çåµæ¸¬ååé¡çæ©å¨å­¸ç¿åæ·±åº¦å­¸ç¿æ¨¡åãééæ¢è¨ä¹³çè³æéçæ¨¡å¼ï¼åæ¬ä¹³æ¿æå½±ãè¶é³æ³¢åå¶å¨ AI ä¸­çèçï¼æ¬æéé»èªªæ XAI å¦ä½è½å°è´æ´æºç¢ºçè¨ºæ·ååäººåæ²»çè¨ç«ãå®ä¹æ¢è¨äºå¯¦æ½éäºæè¡çææ°ï¼ä»¥åå¶å®æ¨æºåè©éææ¨ä»¥è©ä¼° XAI å¨è¨åºç°å¢ä¸­çæææ§çéè¦æ§ãééè©³ç´°çåæåè¨è«ï¼æ¬ææ¨å¨å¼·èª¿ XAI å¨ç¸®å°è¤é AI æ¨¡åèå¯¦åé«çä¿å¥æç¨ä¹éå·®è·çæ½åï¼é²èä¿é²é«çå°æ¥­äººå¡ä¹éçä¿¡ä»»èçè§£ï¼ä¸¦æ¹åæ£èççµæã

##### **Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech Emotion Recognition**
2406.01624v2 by Alaa Nfissi, Wassim Bouachir, Nizar Bouguila, Brian Mishara

Speech emotion recognition (SER) has gained significant attention due to its
several application fields, such as mental health, education, and
human-computer interaction. However, the accuracy of SER systems is hindered by
high-dimensional feature sets that may contain irrelevant and redundant
information. To overcome this challenge, this study proposes an iterative
feature boosting approach for SER that emphasizes feature relevance and
explainability to enhance machine learning model performance. Our approach
involves meticulous feature selection and analysis to build efficient SER
systems. In addressing our main problem through model explainability, we employ
a feature evaluation loop with Shapley values to iteratively refine feature
sets. This process strikes a balance between model performance and
transparency, which enables a comprehensive understanding of the model's
predictions. The proposed approach offers several advantages, including the
identification and removal of irrelevant and redundant features, leading to a
more effective model. Additionally, it promotes explainability, facilitating
comprehension of the model's predictions and the identification of crucial
features for emotion determination. The effectiveness of the proposed method is
validated on the SER benchmarks of the Toronto emotional speech set (TESS),
Berlin Database of Emotional Speech (EMO-DB), Ryerson Audio-Visual Database of
Emotional Speech and Song (RAVDESS), and Surrey Audio-Visual Expressed Emotion
(SAVEE) datasets, outperforming state-of-the-art methods. To the best of our
knowledge, this is the first work to incorporate model explainability into an
SER framework. The source code of this paper is publicly available via this
https://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognition.

æè¦ï¼èªé³æç·è¾¨è­ (SER) ç±æ¼å¶å¨å¿çå¥åº·ãæè²åäººæ©äºåç­å¤åæç¨é åèååéæ³¨ãç¶èï¼SER ç³»çµ±çæºç¢ºæ§åå°é«ç¶­ç¹å¾µéçé»ç¤ï¼éäºç¹å¾µéå¯è½åå«ä¸ç¸éååé¤çè³è¨ãçºäºåæéåææ°ï¼æ¬ç ç©¶æåºäºä¸ç¨®ç¨æ¼ SER çè¿­ä»£ç¹å¾µæåæ¹æ³ï¼è©²æ¹æ³å¼·èª¿ç¹å¾µç¸éæ§åå¯è§£éæ§ï¼ä»¥å¢å¼·æ©å¨å­¸ç¿æ¨¡åçæè½ãæåçåæ³æ¶åä»ç´°çç¹å¾µé¸æååæï¼ä»¥å»ºç«é«æç SER ç³»çµ±ãçºäºééæ¨¡åå¯è§£éæ§è§£æ±ºæåçæ ¸å¿åé¡ï¼æåæ¡ç¨äºå·æ Shapley å¼çç¹å¾µè©ä¼°è¿´åï¼ä»¥åè¦æ¹åç¹å¾µéãéåéç¨å¨æ¨¡åæè½åéæåº¦ä¹éåå¾å¹³è¡¡ï¼éä½¿å¾æåè½å¤ å¨é¢äºè§£æ¨¡åçé æ¸¬ãææåºçæ¹æ³æä¾äºå¤é åªé»ï¼åæ¬è­å¥åç§»é¤ä¸ç¸éååé¤çç¹å¾µï¼å¾èå»ºç«æ´ææçæ¨¡åãæ­¤å¤ï¼å®ä¿é²äºå¯è§£éæ§ï¼æå©æ¼çè§£æ¨¡åçé æ¸¬ä»¥åè­å¥æç·æ±ºå®çééµç¹å¾µãææåºçæ¹æ³çæææ§å·²å¨å¤å«å¤æç·èªé³é (TESS)ãæææç·èªé³è³æåº« (EMO-DB)ãè³´ç¾æ£®é³è¨è¦è¦ºæç·èªé³åæ­æ²è³æåº« (RAVDESS) åè©éé³è¨è¦è¦ºè¡¨éæç· (SAVEE) è³æéç SER åºæºä¸å¾å°é©è­ï¼å¶æè½åªæ¼ç¾ææ¹æ³ãææåæç¥ï¼éæ¯ç¬¬ä¸åå°æ¨¡åå¯è§£éæ§ç´å¥ SER æ¶æ§çç ç©¶ãæ¬æçåå§ç¢¼å¯ééæ­¤é£çµå¬éåå¾ï¼https://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognitionã

##### **The Explanation Necessity for Healthcare AI**
2406.00216v1 by Michail Mamalakis, HÃ©loÃ¯se de Vareilles, Graham Murray, Pietro Lio, John Suckling

Explainability is often critical to the acceptable implementation of
artificial intelligence (AI). Nowhere is this more important than healthcare
where decision-making directly impacts patients and trust in AI systems is
essential. This trust is often built on the explanations and interpretations
the AI provides. Despite significant advancements in AI interpretability, there
remains the need for clear guidelines on when and to what extent explanations
are necessary in the medical context. We propose a novel categorization system
with four distinct classes of explanation necessity, guiding the level of
explanation required: patient or sample (local) level, cohort or dataset
(global) level, or both levels. We introduce a mathematical formulation that
distinguishes these categories and offers a practical framework for researchers
to determine the necessity and depth of explanations required in medical AI
applications. Three key factors are considered: the robustness of the
evaluation protocol, the variability of expert observations, and the
representation dimensionality of the application. In this perspective, we
address the question: When does an AI medical application need to be explained,
and at what level of detail?

æè¦ï¼å¯è§£éæ§éå¸¸å¯¹äºäººå·¥æºè½ (AI) çå¯æ¥åå®æ½è³å³éè¦ãå¨å»çä¿å¥é¢åï¼è¿ä¸ç¹å°¤ä¸ºéè¦ï¼å ä¸ºå³ç­ç´æ¥å½±åæ£èï¼å¹¶ä¸å¯¹ AI ç³»ç»çä¿¡ä»»è³å³éè¦ãè¿ç§ä¿¡ä»»éå¸¸å»ºç«å¨ AI æä¾çè§£éåè¯ éä¹ä¸ãå°½ç®¡ AI å¯è§£éæ§åå¾äºéå¤§è¿å±ï¼ä½ä»ç¶éè¦æç¡®çæå¯¼æ¹éï¼è¯´æå¨å»çç¯å¢ä¸­ä½æ¶ä»¥åå¨å¤å¤§ç¨åº¦ä¸éè¦è§£éãæä»¬æåºäºä¸ç§æ°é¢çåç±»ç³»ç»ï¼è¯¥ç³»ç»å·æåç§ä¸åçè§£éå¿è¦æ§ç±»å«ï¼æå¯¼æéçè§£éçº§å«ï¼æ£èææ ·æ¬ï¼å±é¨ï¼çº§å«ãéåææ°æ®éï¼å¨å±ï¼çº§å«ï¼æä¸¤ä¸ªçº§å«ãæä»¬å¼å¥äºä¸ä¸ªæ°å­¦å¬å¼ï¼è¯¥å¬å¼åºåäºè¿äºç±»å«ï¼å¹¶ä¸ºç ç©¶äººåæä¾äºä¸ä¸ªå®ç¨æ¡æ¶ï¼ä»¥ç¡®å®å»ç AI åºç¨ä¸­æéçè§£éçå¿è¦æ§åæ·±åº¦ãèèäºä¸ä¸ªå³é®å ç´ ï¼è¯ä¼°åè®®çç¨³å¥æ§ãä¸å®¶è§å¯çå¯åæ§ä»¥ååºç¨ç¨åºçè¡¨ç¤ºç»´æ°ãä»è¿ä¸ªè§åº¦æ¥çï¼æä»¬è§£å³äºè¿ä¸ªé®é¢ï¼AI å»çåºç¨ä½æ¶éè¦è§£éï¼ä»¥åéè¦è§£éå°ä½ç§ç¨åº¦ï¼

##### **Interdisciplinary Expertise to Advance Equitable Explainable AI**
2406.18563v1 by Chloe R. Bennett, Heather Cole-Lewis, Stephanie Farquhar, Naama Haamel, Boris Babenko, Oran Lang, Mat Fleck, Ilana Traynis, Charles Lau, Ivor Horn, Courtney Lyles

The field of artificial intelligence (AI) is rapidly influencing health and
healthcare, but bias and poor performance persists for populations who face
widespread structural oppression. Previous work has clearly outlined the need
for more rigorous attention to data representativeness and model performance to
advance equity and reduce bias. However, there is an opportunity to also
improve the explainability of AI by leveraging best practices of social
epidemiology and health equity to help us develop hypotheses for associations
found. In this paper, we focus on explainable AI (XAI) and describe a framework
for interdisciplinary expert panel review to discuss and critically assess AI
model explanations from multiple perspectives and identify areas of bias and
directions for future research. We emphasize the importance of the
interdisciplinary expert panel to produce more accurate, equitable
interpretations which are historically and contextually informed.
Interdisciplinary panel discussions can help reduce bias, identify potential
confounders, and identify opportunities for additional research where there are
gaps in the literature. In turn, these insights can suggest opportunities for
AI model improvement.

æè¦ï¼äººå·¥æºæ§ (AI) é åæ­£å¿«éå½±é¿èå¥åº·èé«çä¿å¥ï¼ä½å°æ¼é¢è¨å»£æ³çµæ§æ§å£è¿«çäººç¾¤ä¾èªªï¼åè¦åä¸è¯è¡¨ç¾ä¾ç¶å­å¨ãååçç ç©¶å·²æ¸æ¥èªªæï¼éè¦æ´å´æ ¼å°æ³¨æè³æä»£è¡¨æ§åæ¨¡åæè½ï¼ä»¥ä¿é²å¬å¹³æ§ä¸¦æ¸å°åè¦ãç¶èï¼æåææ©æéééç¨ç¤¾ææµè¡çå­¸åå¥åº·å¬å¹³çæä½³å¯¦åï¼ä¾æ¹å AI çå¯è§£éæ§ï¼ä»¥å¹«å©æåéå°ç¼ç¾çéè¯æ§ï¼ç¼å±åè¨­ãå¨æ¬æä¸­ï¼æåå°æ³¨æ¼å¯è§£é AI (XAI)ï¼ä¸¦æè¿°ä¸åè·¨é åå°å®¶å°çµå¯©æ¥æ¶æ§ï¼ä»¥å¾å¤éè§é»è¨è«åæ¹å¤æ§è©ä¼° AI æ¨¡åçè§£éï¼ä¸¦æ¾åºåè¦é ååæªä¾ç ç©¶çæ¹åãæåå¼·èª¿è·¨é åå°å®¶å°çµå°æ¼ç¢çæ´æºç¢ºãå¬å¹³çè©®éè³ééè¦ï¼èéäºè©®éæ¯æ ¹ææ­·å²åèçµ¡èä¾çãè·¨é åå°çµè¨è«æå©æ¼æ¸å°åè¦ãæ¾åºæ½å¨çæ··æ·å ç´ ï¼ä¸¦å¨æç»ä¸­æç¼ºå£ææ¾åºé¡å¤ç ç©¶çæ©æãåéä¾ï¼éäºè¦è§£å¯ä»¥å»ºè­° AI æ¨¡åæ¹é²çæ©æã

##### **"It depends": Configuring AI to Improve Clinical Usefulness Across Contexts**
2407.11978v1 by Hubert D. ZajÄc, Jorge M. N. Ribeiro, Silvia Ingala, Simona Gentile, Ruth Wanjohi, Samuel N. Gitau, Jonathan F. Carlsen, Michael B. Nielsen, Tariq O. Andersen

Artificial Intelligence (AI) repeatedly match or outperform radiologists in
lab experiments. However, real-world implementations of radiological AI-based
systems are found to provide little to no clinical value. This paper explores
how to design AI for clinical usefulness in different contexts. We conducted 19
design sessions and design interventions with 13 radiologists from 7 clinical
sites in Denmark and Kenya, based on three iterations of a functional AI-based
prototype. Ten sociotechnical dependencies were identified as crucial for the
design of AI in radiology. We conceptualised four technical dimensions that
must be configured to the intended clinical context of use: AI functionality,
AI medical focus, AI decision threshold, and AI Explainability. We present four
design recommendations on how to address dependencies pertaining to the medical
knowledge, clinic type, user expertise level, patient context, and user
situation that condition the configuration of these technical dimensions.

æè¦ï¼äººå·¥æºæ§ï¼AIï¼å¨å¯¦é©å®¤å¯¦é©ä¸­ä¸æ·å°èæ¾å°ç§é«å¸«å¹æµæè¡¨ç¾å¾æ´åºè²ãç¶èï¼ç¼ç¾æ¾å°ç§ AI çºåºç¤ç³»çµ±çå¯¦éå·è¡å¹¾ä¹æ²ææä¾è¨åºå¹å¼ãæ¬ææ¢è¨å¦ä½çº AI è¨­è¨å¨ä¸åæå¢ä¸­è¨åºä¸çæç¨ãæåæ ¹æåè½æ§ AI çºåºç¤ååçä¸æ¬¡è¿­ä»£ï¼å¨ä¸¹éº¥åè¯äºç 7 åè¨åºå ´åè 13 ä½æ¾å°ç§é«å¸«é²è¡äº 19 æ¬¡è¨­è¨æè­°åè¨­è¨ä»å¥ãååç¤¾ææè¡ä¾è³´éä¿è¢«èªçºå°æ¼æ¾å°ç§ä¸­ AI çè¨­è¨è³ééè¦ãæåæ¦å¿µåäºååæè¡é¢åï¼å¿é æ ¹æé æçè¨åºä½¿ç¨æå¢é²è¡è¨­å®ï¼AI åè½ãAI é«çéé»ãAI æ±ºç­éæª»ï¼ä»¥å AI å¯è§£éæ§ãæåæåºåé è¨­è¨å»ºè­°ï¼èªªæå¦ä½èçèé«çç¥è­ãè¨ºæé¡åãä½¿ç¨èå°æ¥­ç¥è­ç­ç´ãæ£èæå¢ï¼ä»¥åå½±é¿éäºæè¡é¢åè¨­å®çä½¿ç¨èæå¢ç¸éçä¾è³´éä¿ã

##### **Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making**
2405.16424v1 by Min Hun Lee, Silvana Xin Yi Choo, Shamala D/O Thilarajah

With advanced AI/ML, there has been growing research on explainable AI (XAI)
and studies on how humans interact with AI and XAI for effective human-AI
collaborative decision-making. However, we still have a lack of understanding
of how AI systems and XAI should be first presented to users without technical
backgrounds. In this paper, we present the findings of semi-structured
interviews with health professionals (n=12) and students (n=4) majoring in
medicine and health to study how to improve onboarding with AI and XAI. For the
interviews, we built upon human-AI interaction guidelines to create onboarding
materials of an AI system for stroke rehabilitation assessment and AI
explanations and introduce them to the participants. Our findings reveal that
beyond presenting traditional performance metrics on AI, participants desired
benchmark information, the practical benefits of AI, and interaction trials to
better contextualize AI performance, and refine the objectives and performance
of AI. Based on these findings, we highlight directions for improving
onboarding with AI and XAI and human-AI collaborative decision-making.

æè¦ï¼é¨èåé²ç AI/MLï¼å°å¯è§£é AI (XAI) çç ç©¶ä¸æ·å¢å ï¼ä»¥åéæ¼äººé¡å¦ä½è AI å XAI äºåä»¥é²è¡ææçäººå·¥æºæ§åä½æ±ºç­å¶å®ãç¶èï¼æåä»ç¶ç¼ºä¹å° AI ç³»çµ±å XAI æå¦ä½é¦ååç¾çµ¦æ²ææè¡èæ¯çç¨æ¶çäºè§£ãå¨æ¬æä¸­ï¼æåå±ç¤ºäºèé«çå°æ¥­äººå¡ (n=12) åä¸»ä¿®é«å­¸åå¥åº·çå­¸ç (n=4) é²è¡åçµæ§åè¨ªè«ççµæï¼ä»¥ç ç©¶å¦ä½æ¹å AI å XAI çå¥éãå°æ¼è¨ªè«ï¼æåå»ºç«å¨äººæ©äºåæºåä¹ä¸ï¼çºä¸­é¢¨åº·å¾©è©ä¼°å AI è§£éç AI ç³»çµ±åµå»ºå¥éææï¼ä¸¦å°å®åä»ç´¹çµ¦åèèãæåçç ç©¶çµæè¡¨æï¼é¤äºåç¾å³çµ±ç AI æ§è½ææ¨å¤ï¼åèèéå¸æåºåä¿¡æ¯ãAI çå¯¦éå¥½èä»¥åäº¤äºè©¦é©ï¼ä»¥æ´å¥½å°å° AI æ§è½æå¢åï¼ä¸¦å®å AI çç®æ¨åæ§è½ãæ ¹æéäºç¼ç¾ï¼æåå¼·èª¿äºæ¹é² AI å XAI ä»¥åäººæ©åä½æ±ºç­å¶å®çå¥éæ¹åã

##### **Exploring Nutritional Impact on Alzheimer's Mortality: An Explainable AI Approach**
2405.17502v1 by Ziming Liu, Longjian Liu, Robert E. Heidel, Xiaopeng Zhao

This article uses machine learning (ML) and explainable artificial
intelligence (XAI) techniques to investigate the relationship between
nutritional status and mortality rates associated with Alzheimers disease (AD).
The Third National Health and Nutrition Examination Survey (NHANES III)
database is employed for analysis. The random forest model is selected as the
base model for XAI analysis, and the Shapley Additive Explanations (SHAP)
method is used to assess feature importance. The results highlight significant
nutritional factors such as serum vitamin B12 and glycated hemoglobin. The
study demonstrates the effectiveness of random forests in predicting AD
mortality compared to other diseases. This research provides insights into the
impact of nutrition on AD and contributes to a deeper understanding of disease
progression.

æè¦ï¼æ¬æä½¿ç¨æ©å¨å­¸ç¿ (ML) åå¯è§£éäººå·¥æºæ§ (XAI) æè¡ä¾æ¢è¨çé¤çæ³èé¿è²æµ·é»ç (AD) ç¸éçæ­»äº¡çä¹éçéä¿ãæ¡ç¨ç¬¬ä¸æ¬¡å¨åå¥åº·èçé¤æª¢æ¥èª¿æ¥ (NHANES III) è³æåº«é²è¡åæãé¸æé¨æ©æ£®ææ¨¡åä½çº XAI åæçåºç¤æ¨¡åï¼ä¸¦ä½¿ç¨ Shapley Additive Explanations (SHAP) æ¹æ³ä¾è©ä¼°ç¹å¾µéè¦æ§ãçµæçªé¡¯äºéè¦ççé¤å ç´ ï¼ä¾å¦è¡æ¸ç¶­çç´  B12 åç³åè¡ç´èç½ãè©²ç ç©¶è­æäºé¨æ©æ£®æå¨é æ¸¬ AD æ­»äº¡çæ¹é¢ç¸è¼æ¼å¶ä»ç¾ççæææ§ãæ¬ç ç©¶æä¾äºçé¤å° AD çå½±é¿çè¦è§£ï¼ä¸¦æå©æ¼æ´æ·±å¥å°äºè§£ç¾ççé²å±ã

##### **Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone**
2407.11974v1 by Catalina Gomez, Ruolin Wang, Katharina Breininger, Corinne Casey, Chris Bradley, Mitchell Pavlak, Alex Pham, Jithin Yohannan, Mathias Unberath

Primary care providers are vital for initial triage and referrals to
specialty care. In glaucoma, asymptomatic and fast progression can lead to
vision loss, necessitating timely referrals to specialists. However, primary
eye care providers may not identify urgent cases, potentially delaying care.
Artificial Intelligence (AI) offering explanations could enhance their referral
decisions. We investigate how various AI explanations help providers
distinguish between patients needing immediate or non-urgent specialist
referrals. We built explainable AI algorithms to predict glaucoma surgery needs
from routine eyecare data as a proxy for identifying high-risk patients. We
incorporated intrinsic and post-hoc explainability and conducted an online
study with optometrists to assess human-AI team performance, measuring referral
accuracy and analyzing interactions with AI, including agreement rates, task
time, and user experience perceptions. AI support enhanced referral accuracy
among 87 participants (59.9%/50.8% with/without AI), though Human-AI teams
underperformed compared to AI alone. Participants believed they included AI
advice more when using the intrinsic model, and perceived it more useful and
promising. Without explanations, deviations from AI recommendations increased.
AI support did not increase workload, confidence, and trust, but reduced
challenges. On a separate test set, our black-box and intrinsic models achieved
an accuracy of 77% and 71%, respectively, in predicting surgical outcomes. We
identify opportunities of human-AI teaming for glaucoma management in primary
eye care, noting that while AI enhances referral accuracy, it also shows a
performance gap compared to AI alone, even with explanations. Human involvement
remains essential in medical decision making, underscoring the need for future
research to optimize collaboration, ensuring positive experiences and safe AI
use.

æè¦ï¼<paragraph>åç´ä¿å¥æä¾èå°æ¼æåçåæµåè½è¨ºå°å°ç§ç§è­·è³ééè¦ãå¨éåç¼çææ³ä¸ï¼ç¡ççä¸å¿«éæ¡åå¯è½å°è´è¦ååªå¤±ï¼å æ­¤éè¦åæè½è¨ºçµ¦å°å®¶ãç¶èï¼åç´ç¼ç§ä¿å¥æä¾èå¯è½ç¡æ³è­å¥ç·æ¥ææ³ï¼å¯è½æå»¶èª¤ç§è­·ãæä¾è§£éçäººå·¥æºæ§ (AI) å¯ä»¥å å¼·ä»åçè½è¨ºæ±ºç­ãæåç ç©¶åç¨® AI è§£éå¦ä½å¹«å©æä¾èååéè¦ç«å³æéç·æ¥å°ç§è½è¨ºçæ£èãæåå»ºç«äºè§£éæ§ AI æ¼ç®æ³ï¼ä»¥å¾ä¾è¡ç¼ç§è­·çè³æé æ¸¬éåç¼æè¡éæ±ï¼ä½çºè­å¥é«é¢¨éªæ£èçä»£çãæåç´å¥äºå§å¨åäºå¾è§£éæ§ï¼ä¸¦èé©åå¸«é²è¡äºä¸é ç·ä¸ç ç©¶ï¼ä»¥è©ä¼°äººæ©åéçè¡¨ç¾ï¼è¡¡éè½è¨ºæºç¢ºåº¦ä¸¦åæè AI çäºåï¼åæ¬åæçãä»»åæéåä½¿ç¨èé«é©æç¥ãå¨ 87 ååèèä¸­ï¼AI æ¯æ´æé«äºè½è¨ºæºç¢ºåº¦ï¼ä½¿ç¨ AI/æªä½¿ç¨çæ¯ä¾çº 59.9%/50.8%ï¼ï¼åç®¡äººæ©åéçè¡¨ç¾ä¸å¦å®ç¨ä½¿ç¨ AIãåèèèªçºä»åå¨ä½¿ç¨å§å¨æ¨¡åææ´å¤å°ç´å¥äº AI å»ºè­°ï¼ä¸¦èªçºå®æ´æç¨ä¸æ´æå¸æãæ²æè§£éï¼AI å»ºè­°çåå·®æå¢å ãAI æ¯æ´ä¸¦æªå¢å å·¥ä½éãä¿¡å¿åä¿¡ä»»ï¼ä½æ¸å°äºææ°ãå¨ä¸åå®ç¨çæ¸¬è©¦éä¸­ï¼æåçé»çå­åå§å¨æ¨¡åå¨é æ¸¬æè¡çµææ¹é¢åå¥éå°äº 77% å 71% çæºç¢ºåº¦ãæåæ¾åºå¨åç´ç¼ç§ä¿å¥ä¸­ï¼äººæ©åéåä½ç®¡çéåç¼çæ©æï¼ä¸¦æ³¨æå°éç¶ AI æé«äºè½è¨ºæºç¢ºåº¦ï¼ä½å³ä½¿æè§£éï¼å®ä¹é¡¯ç¤ºåºèå®ç¨ä½¿ç¨ AI ç¸æ¯çæè½å·®è·ãäººé¡åèå¨é«çæ±ºç­ä¸­ä»ç¶è³ééè¦ï¼éå¼·èª¿äºæªä¾ç ç©¶åªååä½ãç¢ºä¿æ­£é¢ç¶é©åå®å¨ä½¿ç¨ AI çå¿è¦æ§ã</paragraph>

##### **Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery**
2406.18552v1 by Yingying Fang, Zihao Jin, Xiaodan Xing, Simon Walsh, Guang Yang

In medical imaging, particularly in early disease detection and prognosis
tasks, discerning the rationale behind an AI model's predictions is crucial for
evaluating the reliability of its decisions. Conventional explanation methods
face challenges in identifying discernible decisive features in medical image
classifications, where discriminative features are subtle or not immediately
apparent. To bridge this gap, we propose an explainable model that is equipped
with both decision reasoning and feature identification capabilities. Our
approach not only detects influential image patterns but also uncovers the
decisive features that drive the model's final predictions. By implementing our
method, we can efficiently identify and visualise class-specific features
leveraged by the data-driven model, providing insights into the decision-making
processes of deep learning models. We validated our model in the demanding
realm of medical prognosis task, demonstrating its efficacy and potential in
enhancing the reliability of AI in healthcare and in discovering new knowledge
in diseases where prognostic understanding is limited.

æè¦ï¼å¨é«å­¸å½±åä¸­ï¼ç¹å¥æ¯å¨æ©æç¾çæª¢æ¸¬åé å¾ä»»åä¸­ï¼è¾¨å¥ AI æ¨¡åé æ¸¬èå¾çåçå°æ¼è©ä¼°å¶æ±ºç­çå¯é æ§è³ééè¦ãå³çµ±çè§£éæ¹æ³å¨è­å¥é«å­¸å½±ååé¡ä¸­å¯è­å¥çæ±ºå®æ§ç¹å¾µæé¢è¨ææ°ï¼å¶ä¸­åå¥æ§ç¹å¾µå¾å¾®å¦æä¸¦ä¸æé¡¯ãçºäºå½åéä¸å·®è·ï¼æåæåºäºä¸åå¯è§£éçæ¨¡åï¼è©²æ¨¡åå·åæ±ºç­æ¨çåç¹å¾µè­å¥è½åãæåçåæ³ä¸åæª¢æ¸¬æå½±é¿åçå½±åæ¨¡å¼ï¼éæ­ç¤ºäºæ¨åæ¨¡åæçµé æ¸¬çæ±ºå®æ§ç¹å¾µãééå¯¦æ½æåçæ¨¡åï¼æåå¯ä»¥ææè­å¥åè¦è¦ºåç±æ¸æé©åæ¨¡åå©ç¨çé¡ç¹å®ç¹å¾µï¼å¾èæ·±å¥äºè§£æ·±åº¦å­¸ç¿æ¨¡åçæ±ºç­éç¨ãæåå¨è¦æ±å´æ ¼çé«å­¸é å¾ä»»åé åé©è­äºæåçæ¨¡åï¼å±ç¤ºäºå¶å¨æé« AI å¨é«çä¿å¥ä¸­çå¯é æ§åç¼ç¾é å¾çè§£åéç¾ççæ°ç¥è­æ¹é¢çåæåæ½åã

##### **The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach**
2405.13099v1 by Mohsen Jozani, Jason A. Williams, Ahmed Aleroud, Sarbottam Bhagat

This study explores the relationship between informational support seeking
questions, responses, and helpfulness ratings in online health communities. We
created a labeled data set of question-response pairs and developed multimodal
machine learning and deep learning models to reliably predict informational
support questions and responses. We employed explainable AI to reveal the
emotions embedded in informational support exchanges, demonstrating the
importance of emotion in providing informational support. This complex
interplay between emotional and informational support has not been previously
researched. The study refines social support theory and lays the groundwork for
the development of user decision aids. Further implications are discussed.

æè¦ï¼æ¬ç ç©¶æ¢è¨ç·ä¸å¥åº·ç¤¾ç¾¤ä¸­å°æ±è³è¨æ¯æçåé¡ãåæï¼ä»¥åæå¹«å©çè©åä¹éçéä¿ãæåå»ºç«äºä¸çµæ¨è¨çåç­éå°è³æéï¼ä¸¦éç¼äºå¤æ¨¡ææ©å¨å­¸ç¿åæ·±åº¦å­¸ç¿æ¨¡åï¼ä»¥å¯é å°é æ¸¬è³è¨æ¯æåé¡ååæãæåæ¡ç¨å¯è§£éç AI ä¾æ­ç¤ºè³è¨æ¯æäº¤æµä¸­èå«çæç·ï¼è­ææç·å¨æä¾è³è¨æ¯æä¸­çéè¦æ§ãéç¨®æç·æ¯æåè³è¨æ¯æä¹éçè¤éäº¤äºä½ç¨ä»¥åä¸¦æªè¢«ç ç©¶éãæ¬ç ç©¶æ¹é²äºç¤¾ææ¯æçè«ï¼ä¸¦çºä½¿ç¨èæ±ºç­è¼å©å·¥å·çéç¼å¥ å®äºåºç¤ãè¨è«äºé²ä¸æ­¥çå½±é¿ã

##### **ChatGPT in Classrooms: Transforming Challenges into Opportunities in Education**
2405.10645v1 by Harris Bin Munawar, Nikolaos Misirlis

In the era of exponential technology growth, one unexpected guest has claimed
a seat in classrooms worldwide, Artificial Intelligence. Generative AI, such as
ChatGPT, promises a revolution in education, yet it arrives with a double-edged
sword. Its potential for personalized learning is offset by issues of cheating,
inaccuracies, and educators struggling to incorporate it effectively into their
lesson design. We are standing on the brink of this educational frontier, and
it is clear that we need to navigate this terrain with a lot of care. This is a
major challenge that could undermine the integrity and value of our educational
process. So, how can we turn these challenges into opportunities? When used
inappropriately, AI tools can become the perfect tool for the cut copy paste
mentality, and quickly begin to corrode critical thinking, creativity, and deep
understanding, the most important skills in our rapidly changing world.
Teachers feel that they are not equipped to leverage this technology, widening
the digital divide among educators and institutions. Addressing these concerns
calls for an in depth research approach. We will employ empirical research,
drawing on the Technology Acceptance Model, to assess the attitudes toward
generative AI among educators and students. Understanding their perceptions,
usage patterns, and hurdles is the first crucial step in creating an effective
solution. The present study will be used as a process manual for future
researchers to apply, running their own data, based on the steps explained here

æè¦ï¼å¨ç§æé£éç¼å±çæä»£ï¼ä¸ä½æå¤çè¨ªå®¢å·²å¨å¨çæå®¤ä¸­ä½æä¸å¸­ä¹å°ï¼é£å°±æ¯äººå·¥æºæ§ãçæå¼ AIï¼ä¾å¦ ChatGPTï¼æ¿è«¾å¨æè²é åæèµ·ä¸å ´é©å½ï¼ä½å®å»æ¯ä¸æéé¢åãå®å¨åäººåå­¸ç¿æ¹é¢çæ½åï¼å»å ä½å¼ãä¸æºç¢ºä»¥åæè²å·¥ä½èé£ä»¥å°å¶ææèå¥æå­¸è¨­è¨ç­åé¡èæµé·ãæåæ­£ç«å¨éæè²åæ²¿çéç·£ï¼é¡¯ç¶æåéè¦éå¸¸å°å¿å°æ¢ç´¢éçé åãéæ¯ä¸åéå¤§çææ°ï¼å¯è½ææå®³æåæè²éç¨çå®æ´æ§åå¹å¼ãé£éº¼ï¼æåå¦ä½å°éäºææ°è½åçºæ©éï¼ç¶ä¸é©ç¶å°ä½¿ç¨æï¼AI å·¥å·å¯è½ææçºè¤è£½è²¼ä¸å¿æçå®ç¾å·¥å·ï¼ä¸¦è¿éèèæ¹å¤æ§æç¶­ãåµé ååæ·±å¥çè§£ï¼éäºé½æ¯æåå¿«éè®åçä¸çä¸­æéè¦çæè½ãæå¸«åè¦ºå¾ä»åæ²æè½åå©ç¨éé æè¡ï¼éæ´å¤§äºæè²å·¥ä½èåæ©æ§ä¹éçæ¸ä½é´»æºãè§£æ±ºéäºåé¡éè¦æ·±å¥çç ç©¶æ¹æ³ãæåå°æ¡ç¨å¯¦è­ç ç©¶ï¼åéæè¡æ¥åæ¨¡åï¼ä¾è©ä¼°æè²å·¥ä½èåå­¸çå°çæå¼ AI çæåº¦ãäºè§£ä»åççæ³ãä½¿ç¨æ¨¡å¼åéç¤æ¯åµé ææè§£æ±ºæ¹æ¡çç¬¬ä¸åééµæ­¥é©ãæ¬ç ç©¶å°ä½çºæªä¾ç ç©¶äººå¡æç¨çæµç¨æåï¼æ ¹ææ­¤èèªªæçæ­¥é©éè¡ä»åèªå·±çæ¸æ

##### **Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data**
2405.07590v1 by Camelia Oprea, Mike GrÃ¼ne, Mateusz Buglowski, Lena Olivier, Thorsten Orlikowsky, Stefan Kowalewski, Mark Schoberer, AndrÃ© Stollenwerk

With the digitalization of health care systems, artificial intelligence
becomes more present in medicine. Especially machine learning shows great
potential for complex tasks such as time series classification, usually at the
cost of transparency and comprehensibility. This leads to a lack of trust by
humans and thus hinders its active usage. Explainable artificial intelligence
tries to close this gap by providing insight into the decision-making process,
the actual usefulness of its different methods is however unclear. This paper
proposes a user study based evaluation of the explanation method Grad-CAM with
application to a neural network for the classification of breaths in time
series neonatal ventilation data. We present the perceived usefulness of the
explainability method by different stakeholders, exposing the difficulty to
achieve actual transparency and the wish for more in-depth explanations by many
of the participants.

æè¦ï¼é¨èé«çä¿å¥ç³»çµ±çæ¸ä½åï¼äººå·¥æºæ§å¨é«å­¸é åä¸­è®å¾æ´å æ®åãç¹å¥æ¯æ©å¨å­¸ç¿å¨æéåºååé¡ç­è¤éä»»åä¸­å±ç¾åºæ¥µå¤§çæ½åï¼ä½éå¸¸æ¯ä»¥éæåº¦åå¯çè§£æ§çºä»£å¹ãéå°è´äººé¡ç¼ºä¹ä¿¡ä»»ï¼å¾èé»ç¤äºå¶ç©æ¥µä½¿ç¨ãå¯è§£éçäººå·¥æºæ§è©¦åééæä¾å°æ±ºç­éç¨çæ´å¯ä¾å½è£éä¸å·®è·ï¼ä½å¶ä¸åæ¹æ³çå¯¦éæç¨å°ä¸æ¸æ¥ãæ¬ææåºäºä¸ååºæ¼ä½¿ç¨èç ç©¶çè©ä¼°ï¼å¶ä¸­åå«äº Grad-CAM è§£éæ¹æ³ï¼ä¸¦å°å¶æç¨æ¼ç¥ç¶ç¶²è·¯ä»¥åé¡æéåºåæ°çåå¼å¸æ¸æä¸­çå¼å¸ãæåå±ç¤ºäºä¸åå©çç¸éèå°å¯è§£éæ§æ¹æ³çæç¥æç¨ï¼æ­ç¤ºäºå¯¦ç¾å¯¦ééæåº¦çé£åº¦ï¼ä»¥åè¨±å¤åèèå¸æç²å¾æ´æ·±å¥çè§£éã

##### **XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**
2405.06270v3 by Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio

The integration of Large Language Models (LLMs) into healthcare diagnostics
offers a promising avenue for clinical decision-making. This study outlines the
development of a novel method for zero-shot/few-shot in-context learning (ICL)
by integrating medical domain knowledge using a multi-layered structured
prompt. We also explore the efficacy of two communication styles between the
user and LLMs: the Numerical Conversational (NC) style, which processes data
incrementally, and the Natural Language Single-Turn (NL-ST) style, which
employs long narrative prompts.
  Our study systematically evaluates the diagnostic accuracy and risk factors,
including gender bias and false negative rates, using a dataset of 920 patient
records in various few-shot scenarios. Results indicate that traditional
clinical machine learning (ML) models generally outperform LLMs in zero-shot
and few-shot settings. However, the performance gap narrows significantly when
employing few-shot examples alongside effective explainable AI (XAI) methods as
sources of domain knowledge. Moreover, with sufficient time and an increased
number of examples, the conversational style (NC) nearly matches the
performance of ML models. Most notably, LLMs demonstrate comparable or superior
cost-sensitive accuracy relative to ML models.
  This research confirms that, with appropriate domain knowledge and tailored
communication strategies, LLMs can significantly enhance diagnostic processes.
The findings highlight the importance of optimizing the number of training
examples and communication styles to improve accuracy and reduce biases in LLM
applications.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) èé«çè¨ºæ·æ´å
çºè¨åºæ±ºç­æä¾äºä¸åæåæ¯çéå¾ãæ¬ç ç©¶æ¦è¿°äºä¸ç¨®æ°ç©æ¹æ³çéç¼ï¼ç¨æ¼é¶æ¬¡å­¸ç¿/å°éå­¸ç¿æå¢å­¸ç¿ (ICL)ï¼æ¹æ³æ¯ä½¿ç¨å¤å±¤çµæ§åæç¤ºæ´åé«çé åç¥è­ãæåéæ¢è¨äºä½¿ç¨èè LLM ä¹éå©ç¨®æºéæ¹å¼çåæï¼æ¸å¼å°è©± (NC) æ¹å¼ï¼å®æéæ­¥èçè³æï¼ä»¥åèªç¶èªè¨å®åå (NL-ST) æ¹å¼ï¼å®æä½¿ç¨é·ç¯æäºæç¤ºã
æåçç ç©¶ç³»çµ±æ§å°è©ä¼°äºè¨ºæ·æºç¢ºæ§åé¢¨éªå å­ï¼åæ¬æ§å¥åè¦ååé°æ§çï¼ä½¿ç¨äºä¸ååå« 920 åæ£èè¨éçè³æéï¼æ¡ç¨åç¨®å°éå­¸ç¿æå¢ãçµæè¡¨æï¼å³çµ±çè¨åºæ©å¨å­¸ç¿ (ML) æ¨¡åéå¸¸å¨é¶æ¬¡å­¸ç¿åå°éå­¸ç¿è¨­å®ä¸­è¡¨ç¾åªæ¼ LLMãç¶èï¼ç¶ä½¿ç¨å°éå­¸ç¿ç¯ä¾ä»¥åææçå¯è§£é AI (XAI) æ¹æ³ä½çºé åç¥è­ä¾æºæï¼æè½å·®è·æé¡¯èç¸®å°ãæ­¤å¤ï¼é¨èæéåè¶³åç¯ä¾æ¸éå¢å ï¼å°è©±æ¹å¼ (NC) å¹¾ä¹å¯ä»¥åª²ç¾ ML æ¨¡åçæè½ãæå¼å¾æ³¨æçæ¯ï¼LLM ç¸å°æ¼ ML æ¨¡åå±ç¾åºç¸ç¶ææ´ä½³çææ¬æææºç¢ºåº¦ã
æ¬ç ç©¶è­å¯¦ï¼ééé©ç¶çé åç¥è­åéèº«æé çæºéç­ç¥ï¼LLM å¯ä»¥é¡¯èå¢å¼·è¨ºæ·ç¨åºãéäºç¼ç¾çªé¡¯äºæä½³åè¨ç·´ç¯ä¾æ¸éåæºéæ¹å¼çéè¦æ§ï¼ä»¥æé«æºç¢ºåº¦ä¸¦æ¸å° LLM æç¨ä¸­çåå·®ã

##### **To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**
2405.05766v1 by Miquel MirÃ³-Nicolau, Gabriel MoyÃ -Alcover, Antoni Jaume-i-CapÃ³, Manuel GonzÃ¡lez-Hidalgo, Maria Gemma Sempere Campello, Juan Antonio Palmer Sancho

The increasing reliance on Deep Learning models, combined with their inherent
lack of transparency, has spurred the development of a novel field of study
known as eXplainable AI (XAI) methods. These methods seek to enhance the trust
of end-users in automated systems by providing insights into the rationale
behind their decisions. This paper presents a novel approach for measuring user
trust in XAI systems, allowing their refinement. Our proposed metric combines
both performance metrics and trust indicators from an objective perspective. To
validate this novel methodology, we conducted a case study in a realistic
medical scenario: the usage of XAI system for the detection of pneumonia from
x-ray images.

æè¦ï¼é¨èå°æ·±åº¦å­¸ç¿æ¨¡åä¾è³´æ§çå¢å ï¼å ä¸å¶åºæçéæåº¦ä¸è¶³ï¼ä¿ä½¿ä¸åæ°çç ç©¶é åç¼å±ï¼ç¨±çºå¯è§£é AI (XAI) æ¹æ³ãéäºæ¹æ³æ¨å¨ééæ·±å¥äºè§£æ±ºç­èå¾çåçï¼ä¾æåæçµä½¿ç¨èå°èªååç³»çµ±çä¿¡è³´ãæ¬ææåºäºä¸ç¨®è¡¡éä½¿ç¨èå° XAI ç³»çµ±ä¿¡è³´åº¦çæ°ç©æ¹æ³ï¼åè¨±å°å¶é²è¡æ¹é²ãæåæåºçææ¨çµåäºå®¢è§è§é»ä¸çæè½ææ¨åä¿¡è³´ææ¨ãçºäºé©è­éåæ°ç©çæ¹æ³ï¼æåå¨ä¸åçå¯¦çé«çå ´æ¯ä¸­é²è¡äºä¸åæ¡ä¾ç ç©¶ï¼ä½¿ç¨ XAI ç³»çµ±å¾ X åå½±åä¸­åµæ¸¬èºçã

##### **Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**
2405.02815v1 by Zhusi Zhong, Jie Li, Zhuoqi Ma, Scott Collins, Harrison Bai, Paul Zhang, Terrance Healey, Xinbo Gao, Michael K. Atalay, Zhicheng Jiao

The COVID-19 pandemic has strained global public health, necessitating
accurate diagnosis and intervention to control disease spread and reduce
mortality rates. This paper introduces an interpretable deep survival
prediction model designed specifically for improved understanding and trust in
COVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale
pretrained image encoder, Risk-specific Grad-CAM, and anatomical region
detection techniques, our approach produces regional interpretable outcomes
that effectively capture essential disease features while focusing on rare but
critical abnormal regions. Our model's predictive results provide enhanced
clarity and transparency through risk area localization, enabling clinicians to
make informed decisions regarding COVID-19 diagnosis with better understanding
of prognostic insights. We evaluate the proposed method on a multi-center
survival dataset and demonstrate its effectiveness via quantitative and
qualitative assessments, achieving superior C-indexes (0.764 and 0.727) and
time-dependent AUCs (0.799 and 0.691). These results suggest that our
explainable deep survival prediction model surpasses traditional survival
analysis methods in risk prediction, improving interpretability for clinical
decision making and enhancing AI system trustworthiness.

æè¦ï¼COVID-19 ç«æå°å¨çå¬å±è¡çé æå£åï¼å¿é é²è¡æºç¢ºçè¨ºæ·åå¹²é ï¼ä»¥æ§å¶ç¾çå³æ­ä¸¦éä½æ­»äº¡çãæ¬æä»ç´¹äºä¸åå¯è§£éçæ·±åº¦çå­é æ¸¬æ¨¡åï¼å°éè¨­è¨ç¨æ¼ééè¸é¨ X å (CXR) å½±åæ¹åå° COVID-19 é å¾ççè§£åä¿¡è³´ãééæ´åå¤§è¦æ¨¡é è¨ç·´å½±åç·¨ç¢¼å¨ãé¢¨éªç¹å® Grad-CAM åè§£ååååµæ¸¬æè¡ï¼æåçåæ³ç¢çååå¯è§£éççµæï¼ææææå¿è¦çç¾çç¹å¾µï¼åæå°æ³¨æ¼ç½è¦ä½ééµçç°å¸¸ååãæåçæ¨¡åé æ¸¬çµæééé¢¨éªååå®ä½æä¾å¢å¼·çæ¸æ°åº¦åéæåº¦ï¼è®è¨åºé«çè½å¤ å¨æ´äºè§£é å¾è¦è§£çææ³ä¸ï¼å°± COVID-19 è¨ºæ·ååºææºçæ±ºç­ãæåå¨å¤ä¸­å¿çå­è³æéä¸è©ä¼°ææåºçæ¹æ³ï¼ä¸¦éééååè³ªåè©ä¼°è­æå¶æææ§ï¼éå°åªç°ç C ææ¸ï¼0.764 å 0.727ï¼åæéç¸é AUCï¼0.799 å 0.691ï¼ãéäºçµæè¡¨æï¼æåå¯è§£éçæ·±åº¦çå­é æ¸¬æ¨¡åå¨é¢¨éªé æ¸¬æ¹é¢è¶è¶å³çµ±ççå­åææ¹æ³ï¼æåè¨åºæ±ºç­çè§£éæ§ï¼ä¸¦å¢å¼· AI ç³»çµ±çä¿¡è³´åº¦ã

##### **Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**
2405.02334v1 by Francesco Prinzi, Carmelo Militello, Calogero Zarcaro, Tommaso Vincenzo Bartolotta, Salvatore Gaglio, Salvatore Vitabile

In the last years, artificial intelligence (AI) in clinical decision support
systems (CDSS) played a key role in harnessing machine learning and deep
learning architectures. Despite their promising capabilities, the lack of
transparency and explainability of AI models poses significant challenges,
particularly in medical contexts where reliability is a mandatory aspect.
Achieving transparency without compromising predictive accuracy remains a key
challenge. This paper presents a novel method, namely Rad4XCNN, to enhance the
predictive power of CNN-derived features with the interpretability inherent in
radiomic features. Rad4XCNN diverges from conventional methods based on
saliency map, by associating intelligible meaning to CNN-derived features by
means of Radiomics, offering new perspectives on explanation methods beyond
visualization maps. Using a breast cancer classification task as a case study,
we evaluated Rad4XCNN on ultrasound imaging datasets, including an online
dataset and two in-house datasets for internal and external validation. Some
key results are: i) CNN-derived features guarantee more robust accuracy when
compared against ViT-derived and radiomic features; ii) conventional
visualization map methods for explanation present several pitfalls; iii)
Rad4XCNN does not sacrifice model accuracy for their explainability; iv)
Rad4XCNN provides global explanation insights enabling the physician to analyze
the model outputs and findings. In addition, we highlight the importance of
integrating interpretability into AI models for enhanced trust and adoption in
clinical practice, emphasizing how our method can mitigate some concerns
related to explainable AI methods.

æè¦ï¼<paragraph>å¨éå»å¹¾å¹´ï¼è¨åºæ±ºç­æ¯æ´ç³»çµ± (CDSS) ä¸­çäººå·¥æºæ§ (AI) å¨å©ç¨æ©å¨å­¸ç¿åæ·±åº¦å­¸ç¿æ¶æ§æ¹é¢ç¼æ®äºééµä½ç¨ãåç®¡ AI æ¨¡åå·æä»¤äººæ»¿æçè½åï¼ä½ç¼ºä¹éæåº¦åå¯è§£éæ§ï¼ç¹å¥æ¯å¨å¯é æ§çºå¿è¦èéçé«çèæ¯ä¸ï¼éå¸¶ä¾äºéå¤§çææ°ãå¨ä¸å½±é¿é æ¸¬ç²¾æºåº¦çææ³ä¸å¯¦ç¾éæåº¦ä»ç¶æ¯ä¸é ééµææ°ãæ¬ææåºäºä¸ç¨®æ°æ¹æ³ï¼å³ Rad4XCNNï¼ä»¥å¢å¼· CNN è¡çç¹å¾µçé æ¸¬è½åï¼åæå·åæ¾å°ç¹å¾µåºæçå¯è§£éæ§ãRad4XCNN ä¸åæ¼åºæ¼é¡¯èæ§åçå³çµ±æ¹æ³ï¼å®ééæ¾å°çµå­¸å°å¯çè§£çå«ç¾©è CNN è¡çç¹å¾µéè¯èµ·ä¾ï¼çºè¶è¶è¦è¦ºååè¡¨çè§£éæ¹æ³æä¾äºæ°çè§é»ãæåä»¥ä¹³çåé¡ä»»åä½çºæ¡ä¾ç ç©¶ï¼å¨è¶é³æ³¢å½±åè³æéä¸è©ä¼° Rad4XCNNï¼åæ¬ä¸åç·ä¸è³æéåå©åç¨æ¼å§é¨åå¤é¨é©è­çå§é¨è³æéãä¸äºééµçµæå¦ä¸ï¼i) è ViT è¡çç¹å¾µåæ¾å°ç¹å¾µç¸æ¯ï¼CNN è¡çç¹å¾µä¿è­äºæ´ç©©å¥çæºç¢ºåº¦ï¼ii) å³çµ±çè¦è¦ºååè§£éæ¹æ³å­å¨ä¸äºç¼ºé·ï¼iii) Rad4XCNN æ²æç§ç²æ¨¡åæºç¢ºåº¦ä¾æåå¶å¯è§£éæ§ï¼iv) Rad4XCNN æä¾äºå¨å±è§£éè¦è§£ï¼ä½¿é«å¸«è½å¤ åææ¨¡åè¼¸åºåç¼ç¾ãæ­¤å¤ï¼æåå¼·èª¿å°å¯è§£éæ§æ´åå° AI æ¨¡åä¸­å°æ¼å¢å¼·è¨åºå¯¦åä¸­çä¿¡ä»»åæ¡ç¨è³ééè¦ï¼ä¸¦å¼·èª¿äºæåçæ¹æ³å¦ä½è½ç·©è§£èå¯è§£é AI æ¹æ³ç¸éçä¸äºçæ®ã</paragraph>

##### **Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**
2404.16957v1 by Yunfei Ge, Quanyan Zhu

The pervasive integration of Artificial Intelligence (AI) has introduced
complex challenges in the responsibility and accountability in the event of
incidents involving AI-enabled systems. The interconnectivity of these systems,
ethical concerns of AI-induced incidents, coupled with uncertainties in AI
technology and the absence of corresponding regulations, have made traditional
responsibility attribution challenging. To this end, this work proposes a
Computational Reflective Equilibrium (CRE) approach to establish a coherent and
ethically acceptable responsibility attribution framework for all stakeholders.
The computational approach provides a structured analysis that overcomes the
limitations of conceptual approaches in dealing with dynamic and multifaceted
scenarios, showcasing the framework's explainability, coherence, and adaptivity
properties in the responsibility attribution process. We examine the pivotal
role of the initial activation level associated with claims in equilibrium
computation. Using an AI-assisted medical decision-support system as a case
study, we illustrate how different initializations lead to diverse
responsibility distributions. The framework offers valuable insights into
accountability in AI-induced incidents, facilitating the development of a
sustainable and resilient system through continuous monitoring, revision, and
reflection.

æè¦ï¼é¨èäººå·¥æºæ§ (AI) çæ®åæ´åï¼å¨æ¶å AI é©åç³»çµ±çäºæä¸­ï¼è²¬ä»»åç¾©åæ­¸å±¬ç¢çäºè¤éçææ°ãéäºç³»çµ±çäºé£æ§ãAI å¼ç¼äºæçå«çåé¡ï¼å ä¸ AI æè¡çä¸ç¢ºå®æ§åç¼ºä¹ç¸ææ³è¦ï¼ä½¿å¾å³çµ±è²¬ä»»æ­¸å±¬é¢è¨ææ°ãçºæ­¤ï¼æ¬ç ç©¶æåºäºä¸ç¨®è¨ç®åæåè¡¡ (CRE) æ¹æ³ï¼ä»¥å»ºç«ä¸åé£è²«ä¸å¨å«çä¸å¯æ¥åçè²¬ä»»æ­¸å±¬æ¶æ§ï¼é©ç¨æ¼ææå©å®³éä¿äººãè¨ç®æ¹æ³æä¾äºçµæ§åçåæï¼åæäºæ¦å¿µæ¹æ³å¨èçåæä¸å¤é¢åæå¢æçéå¶ï¼å±ç¤ºäºè©²æ¶æ§å¨è²¬ä»»æ­¸å±¬éç¨ä¸­å·åçå¯è§£éæ§ãé£è²«æ§åé©ææ§ãæåæ¢è¨äºèåè¡¡è¨ç®ä¸­ç´¢è³ ç¸éçåå§ååå±¤ç´çééµä½ç¨ãæåä»¥ AI è¼å©é«çæ±ºç­æ¯æ´ç³»çµ±çºæ¡ä¾ç ç©¶ï¼èªªæä¸åçåå§åå¦ä½å°è´ä¸åçè²¬ä»»åéãè©²æ¶æ§æä¾äºå° AI å¼ç¼äºæä¸­åè²¬å¶çå¯¶è²´è¦è§£ï¼ééæçºç£æ§ãä¿®è¨ååæï¼ä¿é²äºæ°¸çºä¸æéæ§çç³»çµ±ç¼å±ã

##### **Explainable AI for Fair Sepsis Mortality Predictive Model**
2404.13139v1 by Chia-Hsuan Chang, Xiaoyang Wang, Christopher C. Yang

Artificial intelligence supports healthcare professionals with predictive
modeling, greatly transforming clinical decision-making. This study addresses
the crucial need for fairness and explainability in AI applications within
healthcare to ensure equitable outcomes across diverse patient demographics. By
focusing on the predictive modeling of sepsis-related mortality, we propose a
method that learns a performance-optimized predictive model and then employs
the transfer learning process to produce a model with better fairness. Our
method also introduces a novel permutation-based feature importance algorithm
aiming at elucidating the contribution of each feature in enhancing fairness on
predictions. Unlike existing explainability methods concentrating on explaining
feature contribution to predictive performance, our proposed method uniquely
bridges the gap in understanding how each feature contributes to fairness. This
advancement is pivotal, given sepsis's significant mortality rate and its role
in one-third of hospital deaths. Our method not only aids in identifying and
mitigating biases within the predictive model but also fosters trust among
healthcare stakeholders by improving the transparency and fairness of model
predictions, thereby contributing to more equitable and trustworthy healthcare
delivery.

æè¦ï¼äººå·¥æºæ§ééé æ¸¬æ¨¡ååå©é«çå°æ¥­äººå¡ï¼å¤§å¹è½è®äºè¨åºæ±ºç­å¶å®ãæ¬ç ç©¶æ¢è¨äºå¨é«çä¿å¥ä¸­ä½¿ç¨äººå·¥æºæ§æç¨ç¨å¼æå¬å¹³æ§åå¯è§£éæ§çééµéæ±ï¼ä»¥ç¢ºä¿å¨ä¸åçæ£èäººå£çµ±è¨è³æä¸­ç²å¾å¬å¹³ççµæãééå°æ³¨æ¼æè¡çç¸éæ­»äº¡ççé æ¸¬æ¨¡åï¼æåæåºäºä¸ç¨®æ¹æ³ï¼è©²æ¹æ³æå­¸ç¿ä¸åæè½æä½³åçé æ¸¬æ¨¡åï¼ç¶å¾æ¡ç¨è½ç§»å­¸ç¿éç¨ä¾ç¢çä¸åå·ææ´å¥½å¬å¹³æ§çæ¨¡åãæåçæ¨¡åéå¼å¥äºä¸ç¨®æ°ç©çåºæ¼æåçç¹å¾µéè¦æ§æ¼ç®æ³ï¼æ¨å¨é¡ææ¯åç¹å¾µå¨å¢å¼·é æ¸¬å¬å¹³æ§æ¹é¢çè²¢ç»ãèç¾æçå¯è§£éæ§æ¹æ³å°æ³¨æ¼è§£éç¹å¾µå°é æ¸¬æè½çè²¢ç»ä¸åï¼æåæåºçæ¹æ³ç¨ç¹å°å½è£äºçè§£æ¯åç¹å¾µå¦ä½æå©æ¼å¬å¹³æ§çå·®è·ãéé é²å±è³ééè¦ï¼å çºæè¡ççæ­»äº¡çå¾é«ï¼ä¸å¨ä¸åä¹ä¸çé«é¢æ­»äº¡ä¸­æ®æ¼èè§è²ãæåçæ¨¡åä¸åæå©æ¼è­å¥åæ¸è¼é æ¸¬æ¨¡åä¸­çåå·®ï¼éè½ééæé«æ¨¡åé æ¸¬çéæåº¦åå¬å¹³æ§ä¾å¹é¤é«çä¿å¥å©çç¸éèä¹éçä¿¡ä»»ï¼é²èæå©æ¼æä¾æ´å¬å¹³ä¸å¼å¾ä¿¡è³´çé«çä¿å¥æåã

##### **Multi Class Depression Detection Through Tweets using Artificial Intelligence**
2404.13104v1 by Muhammad Osama Nusrat, Waseem Shahzad, Saad Ahmed Jamal

Depression is a significant issue nowadays. As per the World Health
Organization (WHO), in 2023, over 280 million individuals are grappling with
depression. This is a huge number; if not taken seriously, these numbers will
increase rapidly. About 4.89 billion individuals are social media users. People
express their feelings and emotions on platforms like Twitter, Facebook,
Reddit, Instagram, etc. These platforms contain valuable information which can
be used for research purposes. Considerable research has been conducted across
various social media platforms. However, certain limitations persist in these
endeavors. Particularly, previous studies were only focused on detecting
depression and the intensity of depression in tweets. Also, there existed
inaccuracies in dataset labeling. In this research work, five types of
depression (Bipolar, major, psychotic, atypical, and postpartum) were predicted
using tweets from the Twitter database based on lexicon labeling. Explainable
AI was used to provide reasoning by highlighting the parts of tweets that
represent type of depression. Bidirectional Encoder Representations from
Transformers (BERT) was used for feature extraction and training. Machine
learning and deep learning methodologies were used to train the model. The BERT
model presented the most promising results, achieving an overall accuracy of
0.96.

æè¦ï¼ç¾ä»ï¼æé¬±çæ¯ä¸åéè¦çè­°é¡ãæ ¹æä¸çè¡ççµç¹ (WHO) çè³æï¼å¨ 2023 å¹´ï¼è¶é 2.8 åäººæ­£å¨èæé¬±çæé¬¥ãéæ¯ä¸åé¾å¤§çæ¸å­ï¼å¦æä¸èªççå¾ï¼éäºæ¸å­å°æå¿«éå¢å ãå¤§ç´æ 48.9 åäººæ¯ç¤¾ç¾¤åªé«ä½¿ç¨èãäººåå¨ TwitterãFacebookãRedditãInstagram ç­å¹³å°ä¸è¡¨éèªå·±çæååæç·ãéäºå¹³å°åå«æå¹å¼çè³è¨ï¼å¯ç¨æ¼ç ç©¶ç®çãå·²ç¶å¨åç¨®ç¤¾ç¾¤åªé«å¹³å°ä¸é²è¡äºå¤§éçç ç©¶ãç¶èï¼éäºåªåä»å­å¨æäºéå¶ãç¹å¥æ¯ï¼ååçç ç©¶åå°æ³¨æ¼åµæ¸¬æ¨æä¸­çæé¬±çåæé¬±ççå¼·åº¦ãæ­¤å¤ï¼è³æéæ¨ç±¤ä¸­å­å¨ä¸æºç¢ºçææ³ãå¨éé ç ç©¶å·¥ä½ä¸­ï¼ä½¿ç¨åºæ¼è©å½æ¨ç±¤ç Twitter è³æåº«ä¸­çæ¨æé æ¸¬äºäºç¨®é¡åçæé¬±çï¼éæ¥µåãéåº¦ãç²¾ç¥çåãéå¸ååç¢å¾ï¼ãå¯è§£éç AI ç¨æ¼ééå¼·èª¿ä»£è¡¨æé¬±çé¡åçæ¨æé¨åä¾æä¾æ¨çãå¾ Transformersï¼BERTï¼ä¸­æåçéåç·¨ç¢¼å¨è¡¨ç¤ºç¨æ¼ç¹å¾µæååè¨ç·´ãæ©å¨å­¸ç¿åæ·±åº¦å­¸ç¿æ¹æ³ç¨æ¼è¨ç·´æ¨¡åãBERT æ¨¡ååç¾åºææå¸æççµæï¼éå° 0.96 çæ´é«æºç¢ºåº¦ã

##### **COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**
2404.12832v2 by Dmytro Shvetsov, Joonas Ariva, Marharyta Domnich, Raul Vicente, Dmytro Fishman

Deep learning is dramatically transforming the field of medical imaging and
radiology, enabling the identification of pathologies in medical images,
including computed tomography (CT) and X-ray scans. However, the performance of
deep learning models, particularly in segmentation tasks, is often limited by
the need for extensive annotated datasets. To address this challenge, the
capabilities of weakly supervised semantic segmentation are explored through
the lens of Explainable AI and the generation of counterfactual explanations.
The scope of this research is development of a novel counterfactual inpainting
approach (COIN) that flips the predicted classification label from abnormal to
normal by using a generative model. For instance, if the classifier deems an
input medical image X as abnormal, indicating the presence of a pathology, the
generative model aims to inpaint the abnormal region, thus reversing the
classifier's original prediction label. The approach enables us to produce
precise segmentations for pathologies without depending on pre-existing
segmentation masks. Crucially, image-level labels are utilized, which are
substantially easier to acquire than creating detailed segmentation masks. The
effectiveness of the method is demonstrated by segmenting synthetic targets and
actual kidney tumors from CT images acquired from Tartu University Hospital in
Estonia. The findings indicate that COIN greatly surpasses established
attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an
alternative counterfactual explanation method introduced by Singla et al. This
evidence suggests that COIN is a promising approach for semantic segmentation
of tumors in CT images, and presents a step forward in making deep learning
applications more accessible and effective in healthcare, where annotated data
is scarce.

æè¦ï¼æ·±åº¦å­¦ä¹ æ­£å¤§å¹è½è®é«å­¸å½±ååæ¾å°ç·å­¸é åï¼è½è¾¨è­é«å­¸å½±åä¸­çççï¼åæ¬é»è¦æ·å±¤ææ (CT) å X åææãç¶èï¼æ·±åº¦å­¸ç¿æ¨¡åçæè½ï¼ç¹å¥æ¯å¨åå²ä»»åä¸­ï¼å¸¸å¸¸åå°å»£æ³è¨»è§£è³æééæ±çéå¶ãçºäºæå°æ­¤ææ°ï¼ééå¯è§£é AI ååäºå¯¦è§£éçç¢çï¼æ¢ç´¢å¼±ç£ç£èªæåå²çè½åãæ¬ç ç©¶çç¯åæ¯éç¼ä¸ç¨®æ°çåäºå¯¦å§ææ¹æ³ (COIN)ï¼è©²æ¹æ³ä½¿ç¨çææ¨¡åå°é æ¸¬çåé¡æ¨ç±¤å¾ç°å¸¸ç¿»è½çºæ­£å¸¸ãä¾å¦ï¼å¦æåé¡å¨å°è¼¸å¥çé«å­¸å½±å X è¦çºç°å¸¸ï¼è¡¨ç¤ºå­å¨ççï¼åçææ¨¡åæ¨å¨å§æç°å¸¸ååï¼å¾èéè½åé¡å¨çåå§é æ¸¬æ¨ç±¤ãæ­¤æ¹æ³ä½¿æåè½å¤ ç¢ççççç²¾ç¢ºåå²ï¼èç¡éä¾è³´æ¼é åå­å¨çåå²é®ç½©ãè³ééè¦çæ¯ï¼å©ç¨å½±åå±¤ç´æ¨ç±¤ï¼éæ¯å»ºç«è©³ç´°çåå²é®ç½©å®¹æåå¾ãè©²æ¹æ³çæææ§ééåå²åæç®æ¨åå¾ææ²å°¼äºå¡ç¾åå¤§å­¸é«é¢åå¾ç CT å½±åä¸­çå¯¦éèèè«ç¤ä¾è­æãç ç©¶çµæè¡¨æï¼COIN é é è¶éå·²å»ºç«çæ­¸å æ¹æ³ï¼ä¾å¦ RISEãScoreCAM å LayerCAMï¼ä»¥å Singla ç­äººæåºçå¦ä¸ç¨®åäºå¯¦è§£éæ¹æ³ãæ­¤è­æè¡¨æï¼COIN æ¯ä¸ç¨®å¾æåéç CT å½±åä¸­è«ç¤èªæåå²æ¹æ³ï¼ä¸¦å¨é«çä¿å¥ä¸­è®æ·±åº¦å­¸ç¿æç¨æ´ææ¼åå¾åæ´ææçéé²ä¸æ­¥ï¼å¶ä¸­è¨»è§£è³æå¾ç¨å°ã

##### **Hybrid Intelligence for Digital Humanities**
2406.15374v1 by Victor de Boer, Lise Stork

In this paper, we explore the synergies between Digital Humanities (DH) as a
discipline and Hybrid Intelligence (HI) as a research paradigm. In DH research,
the use of digital methods and specifically that of Artificial Intelligence is
subject to a set of requirements and constraints. We argue that these are
well-supported by the capabilities and goals of HI. Our contribution includes
the identification of five such DH requirements: Successful AI systems need to
be able to 1) collaborate with the (human) scholar; 2) support data criticism;
3) support tool criticism; 4) be aware of and cater to various perspectives and
5) support distant and close reading. We take the CARE principles of Hybrid
Intelligence (collaborative, adaptive, responsible and explainable) as
theoretical framework and map these to the DH requirements. In this mapping, we
include example research projects. We finally address how insights from DH can
be applied to HI and discuss open challenges for the combination of the two
disciplines.

æè¦ï¼å¨æ¬æä¸­ï¼æåæ¢è¨æ¸ä½äººæå­¸ç§ (DH) ä½çºä¸éå­¸ç§èæ··åæºè½ (HI) ä½çºä¸åç ç©¶å¸ç¯ä¹éçååä½ç¨ãå¨ DH ç ç©¶ä¸­ï¼æ¸ä½æ¹æ³çä½¿ç¨ï¼ç¹å¥æ¯äººå·¥æºæ§çä½¿ç¨ï¼åå°ä¸ç³»åè¦æ±åéå¶ãæåèªçºéäºè¦æ±åéå¶ç²å¾ HI çè½ååç®æ¨çååæ¯æãæåçè²¢ç»åæ¬æ¾åºäºåéæ¨£ç DH è¦æ±ï¼æåç AI ç³»çµ±éè¦è½å¤  1) èï¼äººé¡ï¼å­¸èåä½ï¼2) æ¯æ´è³ææ¹è©ï¼3) æ¯æ´å·¥å·æ¹è©ï¼4) å¯è¦ºä¸¦è¿ååç¨®è§é»ï¼5) æ¯æ´é è·åè¿è·é¢é±è®ãæåå°æ··åæºè½ç CARE ååï¼åä½ãé©æãè² è²¬åå¯è§£éï¼ä½çºçè«æ¶æ§ï¼ä¸¦å°éäºååå°æå° DH è¦æ±ãå¨æ­¤å°æä¸­ï¼æåç´å¥ç¯ä¾ç ç©¶å°æ¡ãæå¾ï¼æåæ¢è¨å¦ä½å° DH çè¦è§£æç¨æ¼ HIï¼ä¸¦è¨è«çµåéå©åå­¸ç§çéæ¾ææ°ã

##### **Ethical Framework for Responsible Foundational Models in Medical Imaging**
2406.11868v1 by Abhijit Das, Debesh Jha, Jasmer Sanjotra, Onkar Susladkar, Suramyaa Sarkar, Ashish Rauniyar, Nikhil Tomar, Vanshali Sharma, Ulas Bagci

Foundational models (FMs) have tremendous potential to revolutionize medical
imaging. However, their deployment in real-world clinical settings demands
extensive ethical considerations. This paper aims to highlight the ethical
concerns related to FMs and propose a framework to guide their responsible
development and implementation within medicine. We meticulously examine ethical
issues such as privacy of patient data, bias mitigation, algorithmic
transparency, explainability and accountability. The proposed framework is
designed to prioritize patient welfare, mitigate potential risks, and foster
trust in AI-assisted healthcare.

æè¦ï¼åºç¤æ¨¡å (FM) å·æå¾¹åºæ¹è®é«å­¸å½±åçå·¨å¤§æ½åãç¶èï¼å®åå¨ç¾å¯¦ä¸çè¨åºç°å¢ä¸­çé¨ç½²éè¦å»£æ³çå«çèéãæ¬ææ¨å¨å¼·èª¿è FM ç¸éçå«çåé¡ï¼ä¸¦æåºä¸åæ¡æ¶ä¾æå°å®åå¨é«å­¸ä¸­çè² è²¬ä»»éç¼åå¯¦æ½ãæåä»ç´°å¯©æ¥äºå«çåé¡ï¼ä¾å¦æ£èæ¸æé±ç§ãåå·®ç·©è§£ãæ¼ç®æ³éæåº¦ãå¯è§£éæ§ååè²¬å¶ãææåºçæ¡æ¶æ¨å¨åªåèæ®æ£èç¦å©ãæ¸è¼æ½å¨é¢¨éªï¼ä¸¦å¹é¤å° AI è¼å©é«çä¿å¥çä¿¡ä»»ã

##### **Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**
2404.07239v1 by Milad Yousefi, Shadi Farabi Maleki, Ali Jafarizadeh, Mahya Ahmadpour Youshanlui, Aida Jafari, Siamak Pedrammehr, Roohallah Alizadehsani, Ryszard Tadeusiewicz, Pawel Plawiak

Thyroid cancer is an increasing global health concern that requires advanced
diagnostic methods. The application of AI and radiomics to thyroid cancer
diagnosis is examined in this review. A review of multiple databases was
conducted in compliance with PRISMA guidelines until October 2023. A
combination of keywords led to the discovery of an English academic publication
on thyroid cancer and related subjects. 267 papers were returned from the
original search after 109 duplicates were removed. Relevant studies were
selected according to predetermined criteria after 124 articles were eliminated
based on an examination of their abstract and title. After the comprehensive
analysis, an additional six studies were excluded. Among the 28 included
studies, radiomics analysis, which incorporates ultrasound (US) images,
demonstrated its effectiveness in diagnosing thyroid cancer. Various results
were noted, some of the studies presenting new strategies that outperformed the
status quo. The literature has emphasized various challenges faced by AI
models, including interpretability issues, dataset constraints, and operator
dependence. The synthesized findings of the 28 included studies mentioned the
need for standardization efforts and prospective multicenter studies to address
these concerns. Furthermore, approaches to overcome these obstacles were
identified, such as advances in explainable AI technology and personalized
medicine techniques. The review focuses on how AI and radiomics could transform
the diagnosis and treatment of thyroid cancer. Despite challenges, future
research on multidisciplinary cooperation, clinical applicability validation,
and algorithm improvement holds the potential to improve patient outcomes and
diagnostic precision in the treatment of thyroid cancer.

æè¦ï¼ç²çèºçæ¯ä¸ç¨®æ¥çå´éçå¨çå¥åº·åé¡ï¼éè¦åé²çè¨ºæ·æ¹æ³ãæ¬ç¯è©è«æ¢è¨äºäººå·¥æºè½èæ¾å°ç¹å¾µåæå¨ç²çèºçè¨ºæ·ä¸­çæç¨ãå¨ç¬¦å PRISMA æåçææ³ä¸ï¼å°å¤åè³æåº«é²è¡äºåé¡§ï¼ç´å° 2023 å¹´ 10 æãééçµåééµå­ï¼ç¼ç¾äºä¸ç¯éæ¼ç²çèºçåç¸éä¸»é¡çè±æå­¸è¡åºçç©ãå¨ç§»é¤ 109 ç¯éè¤æç»å¾ï¼åå§æå°å±åå³ 267 ç¯è«æãå¨æ ¹æé åç¢ºå®çæ¨æºï¼æ·æ±°äº 124 ç¯æç« çæè¦åæ¨é¡å¾ï¼é¸åºäºç¸éç ç©¶ãå¨é²è¡å¨é¢åæå¾ï¼é¡å¤æé¤äºå­é ç ç©¶ãå¨ç´å¥ç 28 é ç ç©¶ä¸­ï¼çµåè¶é³æ³¢ (US) å½±åçæ¾å°ç¹å¾µåæï¼è­æäºå¶å¨è¨ºæ·ç²çèºçæ¹é¢çæææ§ãç ç©¶çµæä¸ä¸ï¼æäºç ç©¶æåºäºåªæ¼ç¾ççæ°ç­ç¥ãæç»å¼·èª¿äºäººå·¥æºè½æ¨¡åé¢è¨çåç¨®ææ°ï¼åæ¬å¯è§£éæ§åé¡ãè³æééå¶åæä½å¡ä¾è³´æ§ã28 é ç´å¥ç ç©¶çç¶åç¼ç¾æå°ï¼éè¦æ¨æºåå·¥ä½ååç»æ§å¤ä¸­å¿ç ç©¶ä¾è§£æ±ºéäºåé¡ãæ­¤å¤ï¼éç¢ºå®äºåæéäºéç¤çæ¹æ³ï¼ä¾å¦å¯è§£éäººå·¥æºè½æè¡ååäººåé«çæè¡çé²æ­¥ãæ¬ç¯è©è«éé»æ¢è¨äºäººå·¥æºè½åæ¾å°ç¹å¾µåæå¦ä½è½è®ç²çèºççè¨ºæ·åæ²»çãåç®¡å­å¨ææ°ï¼ä½æªä¾å°å¤å­¸ç§åä½ãè¨åºé©ç¨æ§é©è­åæ¼ç®æ³æ¹é²çç ç©¶ï¼ä»ææ½åæ¹åç²çèºçæ²»çä¸­çæ£èé å¾åè¨ºæ·ç²¾æºåº¦ã

##### **Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**
2404.04686v1 by Taminul Islam, Md. Alif Sheakh, Mst. Sazia Tahosin, Most. Hasna Hena, Shopnil Akash, Yousef A. Bin Jardan, Gezahign Fentahun Wondmie, Hiba-Allah Nafidi, Mohammed Bourhia

Breast cancer has rapidly increased in prevalence in recent years, making it
one of the leading causes of mortality worldwide. Among all cancers, it is by
far the most common. Diagnosing this illness manually requires significant time
and expertise. Since detecting breast cancer is a time-consuming process,
preventing its further spread can be aided by creating machine-based forecasts.
Machine learning and Explainable AI are crucial in classification as they not
only provide accurate predictions but also offer insights into how the model
arrives at its decisions, aiding in the understanding and trustworthiness of
the classification results. In this study, we evaluate and compare the
classification accuracy, precision, recall, and F-1 scores of five different
machine learning methods using a primary dataset (500 patients from Dhaka
Medical College Hospital). Five different supervised machine learning
techniques, including decision tree, random forest, logistic regression, naive
bayes, and XGBoost, have been used to achieve optimal results on our dataset.
Additionally, this study applied SHAP analysis to the XGBoost model to
interpret the model's predictions and understand the impact of each feature on
the model's output. We compared the accuracy with which several algorithms
classified the data, as well as contrasted with other literature in this field.
After final evaluation, this study found that XGBoost achieved the best model
accuracy, which is 97%.

æè¦ï¼<paragraph>è¿å¹´ä¾ï¼ä¹³çççè¡çè¿éå¢å ï¼ä½¿å¶æçºå¨çä¸»è¦çæ­»äº¡åå ä¹ä¸ãå¨ææççä¸­ï¼ä¹³çè¿ä»çºæ­¢æ¯æå¸¸è¦çãæåè¨ºæ·æ­¤ç¾çéè¦å¤§éçæéåå°æ¥­ç¥è­ãç±æ¼ä¹³ççæª¢æ¸¬éç¨èæï¼å æ­¤ééå»ºç«æ©å¨å­¸ç¿æ¨¡åä¾é æ¸¬ï¼æå©æ¼é²æ­¢å¶é²ä¸æ­¥æ´æ£ãæ©å¨å­¸ç¿åå¯è§£é AI å¨åé¡ä¸­è³ééè¦ï¼å çºå®åä¸åå¯ä»¥æä¾æºç¢ºçé æ¸¬ï¼éå¯ä»¥æ·±å¥äºè§£æ¨¡åå¦ä½ååºæ±ºç­ï¼æå©æ¼çè§£åä¿¡è³´åé¡çµæãå¨æ­¤ç ç©¶ä¸­ï¼æåè©ä¼°ä¸¦æ¯è¼äºäºç¨®ä¸åçæ©å¨å­¸ç¿æ¹æ³çåé¡æºç¢ºåº¦ãç²¾ç¢ºåº¦ãå¬åçå F1 åæ¸ï¼ä½¿ç¨äºä¸åä¸»è¦çè³æéï¼éå¡é«å­¸é¢é«é¢ç 500 åæ£èï¼ãäºç¨®ä¸åçç£ç£å¼æ©å¨å­¸ç¿æè¡ï¼åæ¬æ±ºç­æ¨¹ãé¨æ©æ£®æãéè¼¯è¿´æ­¸ãæ´ç´ è²æ°å XGBoostï¼å·²ç¨æ¼å¨æåçè³æéä¸åå¾æä½³çµæãæ­¤å¤ï¼æ¬ç ç©¶å° SHAP åææç¨æ¼ XGBoost æ¨¡åï¼ä»¥è§£éæ¨¡åçé æ¸¬ä¸¦äºè§£æ¯åç¹å¾µå°æ¨¡åè¼¸åºçå½±é¿ãæåæ¯è¼äºå¹¾ç¨®æ¼ç®æ³å°è³æé²è¡åé¡çæºç¢ºåº¦ï¼ä¸¦èè©²é åçå¶ä»æç»é²è¡å°æ¯ãå¨æå¾è©ä¼°å¾ï¼æ¬ç ç©¶ç¼ç¾ XGBoost éå°äºæä½³çæ¨¡åæºç¢ºåº¦ï¼çº 97%ã</paragraph>

##### **Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**
2404.03892v3 by Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir

The Deep learning (DL) models for diagnosing breast cancer from mammographic
images often operate as "black boxes", making it difficult for healthcare
professionals to trust and understand their decision-making processes. The
study presents an integrated framework combining Convolutional Neural Networks
(CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis
of breast cancer using the CBIS-DDSM dataset. The methodology encompasses an
elaborate data preprocessing pipeline and advanced data augmentation techniques
to counteract dataset limitations and transfer learning using pre-trained
networks such as VGG-16, Inception-V3 and ResNet was employed. A focal point of
our study is the evaluation of XAI's effectiveness in interpreting model
predictions, highlighted by utilizing the Hausdorff measure to assess the
alignment between AI-generated explanations and expert annotations
quantitatively. This approach is critical for XAI in promoting trustworthiness
and ethical fairness in AI-assisted diagnostics. The findings from our research
illustrate the effective collaboration between CNNs and XAI in advancing
diagnostic methods for breast cancer, thereby facilitating a more seamless
integration of advanced AI technologies within clinical settings. By enhancing
the interpretability of AI driven decisions, this work lays the groundwork for
improved collaboration between AI systems and medical practitioners, ultimately
enriching patient care. Furthermore, the implications of our research extended
well beyond the current methodologies. It encourages further research into how
to combine multimodal data and improve AI explanations to meet the needs of
clinical practice.

æè¦ï¼æ·±åº¦å­¸ç¿ (DL) ç¨æ¼å¾ä¹³æ¿æå½±è¡å½±åè¨ºæ·ä¹³ççæ¨¡åéå¸¸ä»¥ãé»çå­ãæ¹å¼éä½ï¼éä½¿å¾é«çä¿å¥å°æ¥­äººå¡é£ä»¥ä¿¡ä»»åçè§£å¶æ±ºç­éç¨ãæ¬ç ç©¶æåºä¸åæ´åæ¶æ§ï¼çµåå·ç©ç¥ç¶ç¶²è·¯ (CNN) åå¯è§£éäººå·¥æºæ§ (XAI)ï¼ä»¥ä½¿ç¨ CBIS-DDSM è³æéå¢å¼·ä¹³ççè¨ºæ·ãæ¹æ³åå«ä¸åç²¾ç´°çè³æåèçç®¡ç·åé²éè³ææ´åæè¡ï¼ä»¥å°æè³æééå¶ï¼ä¸¦æ¡ç¨é åè¨ç·´çç¶²è·¯ï¼ä¾å¦ VGG-16ãInception-V3 å ResNetï¼é²è¡é·ç§»å­¸ç¿ãæåç ç©¶çéé»æ¯è©ä¼° XAI å¨è§£éæ¨¡åé æ¸¬ä¸­çæææ§ï¼éé»å©ç¨è±ªæ¯å¤å¤«æ¸¬åº¦éåè©ä¼° AI çæçè§£éåå°å®¶è¨»è§£ä¹éçä¸è´æ§ãéç¨®æ¹æ³å°æ¼ XAI å¨ä¿é² AI è¼å©è¨ºæ·ä¸­çå¯ä¿¡åº¦åå«çå¬å¹³æ§è³ééè¦ãæåç ç©¶çç¼ç¾èªªæäº CNN å XAI å¨æ¨é²ä¹³çè¨ºæ·æ¹æ³ä¸­çææåä½ï¼å¾èä¿é²äºåé² AI æè¡å¨è¨åºç°å¢ä¸­çæ´é æ¢æ´åãééå¢å¼· AI é©åæ±ºç­çå¯è§£éæ§ï¼éé å·¥ä½çº AI ç³»çµ±åé«çå¾æ¥­äººå¡ä¹éçæ¹ååä½å¥ å®äºåºç¤ï¼æçµè±å¯äºæ£èç§è­·ãæ­¤å¤ï¼æåç ç©¶çå½±é¿é é è¶åºäºç®åçæè¡ãå®é¼åµé²ä¸æ­¥ç ç©¶å¦ä½çµåå¤æ¨¡å¼è³æä¸¦æ¹å AI è§£éï¼ä»¥æ»¿è¶³è¨åºå¯¦åçéæ±ã

##### **Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives**
2404.00320v2 by Xingrui Gu, Zhixuan Wang, Irisa Jin, Zekun Wu

This research presents a novel multimodal data fusion methodology for pain
behavior recognition, integrating statistical correlation analysis with
human-centered insights. Our approach introduces two key innovations: 1)
integrating data-driven statistical relevance weights into the fusion strategy
to effectively utilize complementary information from heterogeneous modalities,
and 2) incorporating human-centric movement characteristics into multimodal
representation learning for detailed modeling of pain behaviors. Validated
across various deep learning architectures, our method demonstrates superior
performance and broad applicability. We propose a customizable framework that
aligns each modality with a suitable classifier based on statistical
significance, advancing personalized and effective multimodal fusion.
Furthermore, our methodology provides explainable analysis of multimodal data,
contributing to interpretable and explainable AI in healthcare. By highlighting
the importance of data diversity and modality-specific representations, we
enhance traditional fusion techniques and set new standards for recognizing
complex pain behaviors. Our findings have significant implications for
promoting patient-centered healthcare interventions and supporting explainable
clinical decision-making.

æè¦ï¼æ¬ç ç©¶æåºäºä¸ç¨®åµæ°çå¤æ¨¡ææ¸æèåæ¹æ³ï¼ç¨æ¼ç¼çè¡çºè­å¥ï¼å°çµ±è¨ç¸éåæèä»¥äººçºä¸­å¿çè¦è§£ç¸çµåãæåçåæ³å¼å¥äºå©é ééµåµæ°ï¼1) å°æ¸æé©åççµ±è¨ç¸éæ¬éæ´åå°èåç­ç¥ä¸­ï¼ä»¥ææå©ç¨ä¾èªç°è³ªæ¨¡æçè£åä¿¡æ¯ï¼ä»¥å 2) å°ä»¥äººçºä¸­å¿çéåç¹å¾µç´å¥å¤æ¨¡æè¡¨ç¤ºå­¸ç¿ä¸­ï¼ä»¥è©³ç´°å»ºæ¨¡ç¼çè¡çºãæåçæ¨¡åå¨åç¨®æ·±åº¦å­¸ç¿æ¶æ§ä¸­å¾å°é©è­ï¼å±ç¤ºäºåè¶çæ§è½åå»£æ³çé©ç¨æ§ãæåæåºäºä¸åå¯èªå®ç¾©çæ¡æ¶ï¼æ ¹æçµ±è¨é¡¯èæ§å°æ¯åæ¨¡æèåé©çåé¡å¨å°é½ï¼æ¨é²åæ§ååææçå¤æ¨¡æèåãæ­¤å¤ï¼æåçæ¨¡åæä¾å°å¤æ¨¡ææ¸æçå¯è§£éåæï¼æå©æ¼é«çä¿å¥ä¸­çå¯è§£éåå¯è§£é AIãééå¼·èª¿æ¸æå¤æ¨£æ§åæ¨¡æç¹å®è¡¨ç¤ºçéè¦æ§ï¼æåå¢å¼·äºå³çµ±çèåæè¡ï¼ä¸¦çºè­å¥è¤éçç¼çè¡çºè¨­å®äºæ°çæ¨æºãæåçç¼ç¾å°ä¿é²ä»¥æ£èçºä¸­å¿çé«çä¿å¥å¹²é åæ¯æå¯è§£éçè¨åºæ±ºç­å¶å®å·æéè¦æç¾©ã

##### **Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**
2403.17873v1 by Andrea Ferrario, Alberto Termine, Alessandro Facchini

Human-centered explainable AI (HCXAI) advocates for the integration of social
aspects into AI explanations. Central to the HCXAI discourse is the Social
Transparency (ST) framework, which aims to make the socio-organizational
context of AI systems accessible to their users. In this work, we suggest
extending the ST framework to address the risks of social misattributions in
Large Language Models (LLMs), particularly in sensitive areas like mental
health. In fact LLMs, which are remarkably capable of simulating roles and
personas, may lead to mismatches between designers' intentions and users'
perceptions of social attributes, risking to promote emotional manipulation and
dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To
address these issues, we propose enhancing the ST framework with a fifth
'W-question' to clarify the specific social attributions assigned to LLMs by
its designers and users. This addition aims to bridge the gap between LLM
capabilities and user perceptions, promoting the ethically responsible
development and use of LLM-based technology.

æè¦ï¼ä»¥äººä¸ºæ¬çå¯è§£é AI (HCXAI) å¡å¯¼å°ç¤¾ä¼å±é¢æ´åå° AI è§£éä¸­ãHCXAI è¯è¯­çæ ¸å¿æ¯ç¤¾ä¼éæåº¦ (ST) æ¡æ¶ï¼å¶ç®æ æ¯è®© AI ç³»ç»çç¤¾ä¼ç»ç»èæ¯å¯¹ç¨æ·æ¥è¯´æ¯å¯çè§£çãå¨è¿é¡¹å·¥ä½ä¸­ï¼æä»¬å»ºè®®æ©å± ST æ¡æ¶ä»¥è§£å³å¤§åè¯­è¨æ¨¡å (LLM) ä¸­ç¤¾ä¼éè¯¯å½å çé£é©ï¼å°¤å¶æ¯å¨å¿çå¥åº·ç­ææé¢åãäºå®ä¸ï¼LLM è½å¤åºè²å°æ¨¡æè§è²åäººæ ¼ï¼è¿å¯è½å¯¼è´è®¾è®¡èçæå¾åç¨æ·å¯¹ç¤¾ä¼å±æ§çè®¤ç¥ä¹é´åºç°ééï¼ä»èæé£é©ä¿è¿æç»ªæçºµåå±é©è¡ä¸ºãè®¤ç¥ä¸å¬æ­£åä¸åççä¿¡ä»»ãä¸ºäºè§£å³è¿äºé®é¢ï¼æä»¬å»ºè®®ç¨ç¬¬äºä¸ªâW é®é¢âæ¥å¢å¼º ST æ¡æ¶ï¼ä»¥æç¡®è®¾è®¡èåç¨æ·èµäº LLM çå·ä½ç¤¾ä¼å±æ§ãæ­¤è¡¥åæ¨å¨å¼¥å LLM è½ååç¨æ·è®¤ç¥ä¹é´çå·®è·ï¼ä¿è¿åºäº LLM çææ¯å¨éå¾·ä¸è´è´£ä»»å°å¼ååä½¿ç¨ã

##### **Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**
2403.18871v1 by Han Yuan, Chuan Hong, Pengtao Jiang, Gangming Zhao, Nguyen Tuan Anh Tran, Xinxing Xu, Yet Yen Yan, Nan Liu

Background: Pneumothorax is an acute thoracic disease caused by abnormal air
collection between the lungs and chest wall. To address the opaqueness often
associated with deep learning (DL) models, explainable artificial intelligence
(XAI) methods have been introduced to outline regions related to pneumothorax
diagnoses made by DL models. However, these explanations sometimes diverge from
actual lesion areas, highlighting the need for further improvement. Method: We
propose a template-guided approach to incorporate the clinical knowledge of
pneumothorax into model explanations generated by XAI methods, thereby
enhancing the quality of these explanations. Utilizing one lesion delineation
created by radiologists, our approach first generates a template that
represents potential areas of pneumothorax occurrence. This template is then
superimposed on model explanations to filter out extraneous explanations that
fall outside the template's boundaries. To validate its efficacy, we carried
out a comparative analysis of three XAI methods with and without our template
guidance when explaining two DL models in two real-world datasets. Results: The
proposed approach consistently improved baseline XAI methods across twelve
benchmark scenarios built on three XAI methods, two DL models, and two
datasets. The average incremental percentages, calculated by the performance
improvements over the baseline performance, were 97.8% in Intersection over
Union (IoU) and 94.1% in Dice Similarity Coefficient (DSC) when comparing model
explanations and ground-truth lesion areas. Conclusions: In the context of
pneumothorax diagnoses, we proposed a template-guided approach for improving AI
explanations. We anticipate that our template guidance will forge a fresh
approach to elucidating AI models by integrating clinical domain expertise.

æè¦ï¼<paragraph>èæ¯ï¼æ°£è¸æ¯ä¸ç¨®å èºé¨èè¸å£ä¹éç°å¸¸éæ°£æå¼èµ·çæ¥æ§è¸èç¾çãçºäºè§£æ±ºæ·±åº¦å­¸ç¿ï¼DLï¼æ¨¡åç¶å¸¸ä¼´é¨çä¸éææ§ï¼å¯è§£éäººå·¥æºæ§ï¼XAIï¼æ¹æ³å·²è¢«å¼å¥ï¼ç¨æ¼æ¦è¿°è DL æ¨¡åååºçæ°£è¸è¨ºæ·ç¸éçååãç¶èï¼éäºè§£éæææèå¯¦éçç¶ååææåºå¥ï¼çªé¡¯åºé²ä¸æ­¥æ¹é²çå¿è¦æ§ãæ¹æ³ï¼æåæåºäºä¸ç¨®æ¨¡æ¿å¼å°å¼æ¹æ³ï¼å°æ°£è¸çè¨åºç¥è­ç´å¥ XAI æ¹æ³ç¢ççæ¨¡åè§£éä¸­ï¼å¾èæåéäºè§£éçåè³ªãå©ç¨æ¾å°ç§é«å¸«å»ºç«ççç¶æç¹ªï¼æåçåæ³é¦åç¢çä¸åæ¨¡æ¿ï¼ç¨æ¼è¡¨ç¤ºæ°£è¸å¯è½ç¼ççååãç¶å¾å°æ­¤æ¨¡æ¿çå å¨æ¨¡åè§£éä¸ï¼ä»¥ç¯©é¸åºè¶åºæ¨¡æ¿éççç¡éè§£éãçºäºé©è­å¶æåï¼æåå°ä¸ç¨® XAI æ¹æ³é²è¡äºæ¯è¼åæï¼å¨å©åçå¯¦ä¸çè³æéä¸­è§£éå©å DL æ¨¡åæï¼åå¥æ¡ç¨åä¸æ¡ç¨æåçæ¨¡æ¿å¼å°ãçµæï¼ææåºçæ¹æ³å¨å»ºç«æ¼ä¸ç¨® XAI æ¹æ³ãå©å DL æ¨¡ååå©åè³æéçåäºç¨®åºæºæå¢ä¸­ï¼å§çµæ¹åäºåºæº XAI æ¹æ³ãå¨æ¯è¼æ¨¡åè§£éåçå¯¦çç¶ååæï¼ééåºæºæè½çæè½æ¹é²è¨ç®åºçå¹³åå¢éç¾åæ¯çºäº¤éæ¯ï¼IoUï¼ç 97.8% åéª°å­ç¸ä¼¼æ§ä¿æ¸ï¼DSCï¼ç 94.1%ãçµè«ï¼å¨æ°£è¸è¨ºæ·çèæ¯ä¸ï¼æåæåºäºä¸ç¨®æ¨¡æ¿å¼å°å¼æ¹æ³ï¼ç¨æ¼æ¹å AI è§£éãæåé ææåçæ¨¡æ¿å¼å°å°ééæ´åè¨åºé åå°æ¥­ç¥è­ï¼çºé¡æ AI æ¨¡åå»ºç«ä¸ç¨®æ°æ¹æ³ã</paragraph>

##### **Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**
2403.01580v1 by SÃ©amus Lankford

In the current machine translation (MT) landscape, the Transformer
architecture stands out as the gold standard, especially for high-resource
language pairs. This research delves into its efficacy for low-resource
language pairs including both the English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi language pairs. Notably, the study identifies
the optimal hyperparameters and subword model type to significantly improve the
translation quality of Transformer models for low-resource language pairs.
  The scarcity of parallel datasets for low-resource languages can hinder MT
development. To address this, gaHealth was developed, the first bilingual
corpus of health data for the Irish language. Focusing on the health domain,
models developed using this in-domain dataset exhibited very significant
improvements in BLEU score when compared with models from the LoResMT2021
Shared Task. A subsequent human evaluation using the multidimensional quality
metrics error taxonomy showcased the superior performance of the Transformer
system in reducing both accuracy and fluency errors compared to an RNN-based
counterpart.
  Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source
applications streamlined for the development, fine-tuning, and deployment of
neural machine translation models. These tools considerably simplify the setup
and evaluation process, making MT more accessible to both developers and
translators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes
eco-friendly natural language processing research by highlighting the
environmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM
demonstrated advancements in translation performance for two low-resource
language pairs: English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi, compared to baselines from the LoResMT2021
Shared Task.

æè¦ï¼<paragraph>å¨ç¶åæ©å¨ç¿»è­¯ (MT) é åä¸­ï¼Transformer æ¶æ§è«ç©èåºï¼æçºé»éæ¨æºï¼ç¹å¥æ¯å°æ¼é«è³æºèªè¨å°ãæ¬ç ç©¶æ¢è¨å¶å°ä½è³æºèªè¨å°çæè½ï¼åæ¬è±èªâæç¾è­èªåè±èªâé¦¬æå°èªèªè¨å°ãå¼å¾æ³¨æçæ¯ï¼æ¬ç ç©¶è­å¥åºæä½³è¶åæ¸åå­è©æ¨¡åé¡åï¼ä»¥é¡¯èæé« Transformer æ¨¡åå°ä½è³æºèªè¨å°çç¿»è­¯åè³ªã
ä½è³æºèªè¨çå¹³è¡è³æéçç¨ç¼ºæé»ç¤ MT çç¼å±ãçºäºè§£æ±ºéååé¡ï¼éç¼äº gaHealthï¼éæ¯æç¾è­èªçç¬¬ä¸åéèªå¥åº·è³æèªæåº«ãå°æ³¨æ¼å¥åº·é åï¼ä½¿ç¨æ­¤åå§è³æééç¼çæ¨¡åå¨ BLEU å¾åæ¹é¢è¡¨ç¾åºéå¸¸é¡¯èçé²æ­¥ï¼è LoResMT2021 å±äº«ä»»åä¸­çæ¨¡åç¸æ¯ãé¨å¾ä½¿ç¨å¤ç¶­åè³ªææ¨é¯èª¤åé¡æ³é²è¡çäººå·¥è©ä¼°é¡¯ç¤ºï¼èåºæ¼ RNN çå°ææ¨¡åç¸æ¯ï¼Transformer ç³»çµ±å¨æ¸å°æºç¢ºæ§åæµæ¢æ§é¯èª¤æ¹é¢è¡¨ç¾åºåªç°çæ§è½ã
æ­¤å¤ï¼æ¬è«æä»ç´¹äº adaptNMT å adaptMLLMï¼éå©åéæºæç¨ç¨å¼ç°¡åäºç¥ç¶æ©å¨ç¿»è­¯æ¨¡åçéç¼ãå¾®èª¿åé¨ç½²ãéäºå·¥å·å¤§å¹ç°¡åäºè¨­å®åè©ä¼°æµç¨ï¼è® MT æ´å®¹æè®éç¼äººå¡åç¿»è­¯äººå¡ä½¿ç¨ãå¼å¾æ³¨æçæ¯ï¼adaptNMT ä»¥ OpenNMT çæç³»çµ±çºåºç¤ï¼ééå¼·èª¿æ¨¡åéç¼çç°å¢è¶³è·¡ä¾ä¿é²çæåå¥½çèªç¶èªè¨èçç ç©¶ãè LoResMT2021 å±äº«ä»»åä¸­çåºæºç¸æ¯ï¼adaptMLLM å° MLLM çå¾®èª¿è­æäºè±èªâæç¾è­èªåè±èªâé¦¬æå°èªéå©åä½è³æºèªè¨å°çç¿»è­¯æ§è½é²æ­¥ã</paragraph>

##### **Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**
2402.18600v1 by Yasin Sadeghi Bazargani, Majid Mirzaei, Navid Sobhi, Mirsaeed Abdollahi, Ali Jafarizadeh, Siamak Pedrammehr, Roohallah Alizadehsani, Ru San Tan, Sheikh Mohammed Shariful Islam, U. Rajendra Acharya

Diabetes mellitus (DM) predisposes patients to vascular complications.
Retinal images and vasculature reflect the body's micro- and macrovascular
health. They can be used to diagnose DM complications, including diabetic
retinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular
disease, as well as forecast the risk of cardiovascular events. Artificial
intelligence (AI)-enabled systems developed for high-throughput detection of DR
using digitized retinal images have become clinically adopted. Beyond DR
screening, AI integration also holds immense potential to address challenges
associated with the holistic care of the patient with DM. In this work, we aim
to comprehensively review the literature for studies on AI applications based
on retinal images related to DM diagnosis, prognostication, and management. We
will describe the findings of holistic AI-assisted diabetes care, including but
not limited to DR screening, and discuss barriers to implementing such systems,
including issues concerning ethics, data privacy, equitable access, and
explainability. With the ability to evaluate the patient's health status vis a
vis DM complication as well as risk prognostication of future cardiovascular
complications, AI-assisted retinal image analysis has the potential to become a
central tool for modern personalized medicine in patients with DM.

æè¦ï¼ç³å°¿çï¼DMï¼ä½¿æ£èå®¹æåºç¾è¡ç®¡ä½µç¼çã
è¦ç¶²èå½±ååè¡ç®¡åæ èº«é«çå¾®è¡ç®¡åå·¨è¡ç®¡å¥åº·çæ³ãå®åå¯ç¨æ¼è¨ºæ·ç³å°¿çä½µç¼çï¼åæ¬ç³å°¿çè¦ç¶²èçè®ï¼DRï¼ãç¥ç¶çè®ãèçååèç²¥æ¨£ç¡¬åæ§å¿è¡ç®¡ç¾çï¼ä»¥åé æ¸¬å¿è¡ç®¡äºä»¶çé¢¨éªãçºä½¿ç¨æ¸ä½åè¦ç¶²èå½±åé²è¡é«éé DR æª¢æ¸¬èéç¼çäººå·¥æºæ§ï¼AIï¼åç¨ç³»çµ±å·²å¨è¨åºæ¡ç¨ãé¤äº DR ç¯©æª¢å¤ï¼AI æ´åä¹å·æå·¨å¤§çæ½åä¾æå°èç³å°¿çæ£èæ´é«ç§è­·ç¸éçææ°ãå¨éé å·¥ä½ä¸­ï¼æåæ¨å¨å¨é¢åé¡§åºæ¼è¦ç¶²èå½±åç AI æç¨ç¸éç ç©¶çæç»ï¼éäºç ç©¶èç³å°¿ççè¨ºæ·ãé å¾åç®¡çæéãæåå°æè¿°æ´é« AI è¼å©ç³å°¿çç§è­·çç¼ç¾ï¼åæ¬ä½ä¸éæ¼ DR ç¯©æª¢ï¼ä¸¦è¨è«å¯¦æ½æ­¤é¡ç³»çµ±çéç¤ï¼åæ¬èå«çãè³æé±ç§ãå¬å¹³å­ååå¯è§£éæ§æéçåé¡ãééè©ä¼°æ£èçå¥åº·çæ³ï¼åæèéç³å°¿çä½µç¼çä»¥åæªä¾å¿è¡ç®¡ä½µç¼ççé¢¨éªé å¾ï¼AI è¼å©è¦ç¶²èå½±ååæææ½åæçºç³å°¿çæ£èç¾ä»£ååäººåé«ççä¸­å¿å·¥å·ã

##### **Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**
2402.15027v2 by A. J. Karran, P. Charland, J-T. Martineau, A. Ortiz de Guinea Lopez de Arana, AM. Lesage, S. Senecal, P-M. Leger

This study investigates the acceptability of different artificial
intelligence (AI) applications in education from a multi-stakeholder
perspective, including students, teachers, and parents. Acknowledging the
transformative potential of AI in education, it addresses concerns related to
data privacy, AI agency, transparency, explainability and the ethical
deployment of AI. Through a vignette methodology, participants were presented
with four scenarios where AI's agency, transparency, explainability, and
privacy were manipulated. After each scenario, participants completed a survey
that captured their perceptions of AI's global utility, individual usefulness,
justice, confidence, risk, and intention to use each scenario's AI if
available. The data collection comprising a final sample of 1198
multi-stakeholder participants was distributed through a partner institution
and social media campaigns and focused on individual responses to four AI use
cases. A mediation analysis of the data indicated that acceptance and trust in
AI varies significantly across stakeholder groups. We found that the key
mediators between high and low levels of AI's agency, transparency, and
explainability, as well as the intention to use the different educational AI,
included perceived global utility, justice, and confidence. The study
highlights that the acceptance of AI in education is a nuanced and multifaceted
issue that requires careful consideration of specific AI applications and their
characteristics, in addition to the diverse stakeholders' perceptions.

æè¦ï¼éé ç ç©¶å¾å¤åå©å®³éä¿äººçè§åº¦æ¢è¨ä¸åçäººå·¥æºæ§ (AI) æç¨å¨æè²ä¸çå¯æ¥åæ§ï¼åæ¬å­¸çãèå¸«åå®¶é·ãæ¿èª AI å¨æè²ä¸çè½åæ½åï¼å®è§£æ±ºäºèè³æé±ç§ãAI ä»£çãéæåº¦ãå¯è§£éæ§å AI çéå¾·é¨ç½²ç¸éççæ®ãééå°ææ²æ¹æ³ï¼åèèè¢«åç¾äºåç¨®æå¢ï¼å¶ä¸­ AI çä»£çãéæåº¦ãå¯è§£éæ§åé±ç§åå°æç¸±ãå¨æ¯åæå¢å¾ï¼åèèå®æäºä¸é èª¿æ¥ï¼è©²èª¿æ¥ææäºä»åå° AI çæ´é«æç¨ãåäººæç¨ãæ­£ç¾©ãä¿¡å¿ãé¢¨éªåå¦æå¯ç¨ï¼ä½¿ç¨æ¯åæå¢ç AI çæåççæ³ãè³æèéåå«ä¾èªåä½æ©æ§åç¤¾ç¾¤åªé«æ´»åç 1198 ä½å¤å©å®³éä¿äººåèèçæçµæ¨£æ¬ï¼ä¸¦å°æ³¨æ¼å°åå AI ä½¿ç¨æ¡ä¾çåå¥åæãå°è³æçèª¿è§£åæè¡¨æï¼å° AI çæ¥ååº¦åä¿¡ä»»å¨å©å®³éä¿äººåé«ä¹éæé¡¯èå·®ç°ãæåç¼ç¾ï¼AI çä»£çãéæåº¦åå¯è§£éæ§é«ä½ç¨åº¦ä¹éçééµèª¿è§£èï¼ä»¥åä½¿ç¨ä¸åæè² AI çæåï¼åæ¬æç¥å°çæ´é«æç¨ãæ­£ç¾©åä¿¡å¿ãéé ç ç©¶å¼·èª¿ï¼æ¥å AI å¨æè²ä¸çæç¨æ¯ä¸åå¾®å¦ä¸å¤é¢åçåé¡ï¼é¤äºä¸åçå©å®³éä¿äººççæ³å¤ï¼ééè¦ä»ç´°èæ®å·é«ç AI æç¨åå¶ç¹å¾µã

##### **Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**
2402.09474v2 by Aruna Mohan, Danne Elbers, Or Zilbershot, Fatemeh Afghah, David Vorchheimer

Remote patient monitoring based on wearable single-lead electrocardiogram
(ECG) devices has significant potential for enabling the early detection of
heart disease, especially in combination with artificial intelligence (AI)
approaches for automated heart disease detection. There have been prior studies
applying AI approaches based on deep learning for heart disease detection.
However, these models are yet to be widely accepted as a reliable aid for
clinical diagnostics, in part due to the current black-box perception
surrounding many AI algorithms. In particular, there is a need to identify the
key features of the ECG signal that contribute toward making an accurate
diagnosis, thereby enhancing the interpretability of the model. In the present
study, we develop a vision transformer approach to identify atrial fibrillation
based on single-lead ECG data. A residual network (ResNet) approach is also
developed for comparison with the vision transformer approach. These models are
applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as
well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm
heartbeats. The models enable the identification of the key regions of the
heartbeat that determine the resulting classification, and highlight the
importance of P-waves and T-waves, as well as heartbeat duration and signal
amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and
sinus bradycardia.

æè¦ï¼<paragraph>åºæ¼å¯ç©¿æ´å¼å®å°ç¨å¿é»å (ECG) è£ç½®çé ç«¯çæ£ç£æ¸¬å¨æ©æåµæ¸¬å¿èç¾çæ¹é¢å·æé¡¯èçæ½åï¼ç¹å¥æ¯èç¨æ¼èªååå¿èç¾çåµæ¸¬çäººå·¥æºæ§ (AI) æ¹æ³çµåä½¿ç¨æãååå·²æç ç©¶æç¨åºæ¼æ·±åº¦å­¸ç¿ç AI æ¹æ³é²è¡å¿èç¾çåµæ¸¬ãç¶èï¼éäºæ¨¡åå°æªè¢«å»£æ³æ¥åçºè¨åºè¨ºæ·çå¯é è¼å©å·¥å·ï¼é¨ååå å¨æ¼åç¹è¨±å¤ AI æ¼ç®æ³çç¶åé»ç®±æç¥ãç¹å¥æ¯ï¼æå¿è¦æ¾åºæå©æ¼ååºæºç¢ºè¨ºæ·ç ECG è¨èééµç¹å¾µï¼å¾èå¢å¼·æ¨¡åçå¯è§£éæ§ãå¨æ¬ç ç©¶ä¸­ï¼æåéç¼äºä¸ç¨®è¦è¦ºè½æå¨æ¹æ³ï¼ä»¥æ ¹æå®å°ç¨ ECG è³ææ¾åºå¿æ¿é¡«åãæ®å·®ç¶²è·¯ (ResNet) æ¹æ³ä¹å·²éç¼åºä¾ï¼ä»¥ä¾¿èè¦è¦ºè½æå¨æ¹æ³é²è¡æ¯è¼ãéäºæ¨¡åæç¨æ¼ Chapman-Shaoxing è³æéï¼ä»¥åé¡å¿æ¿é¡«åï¼ä»¥åå¦ä¸ç¨®å¸¸è¦çå¿å¾ä¸æ´ï¼ç«æ§å¿åéç·©ï¼åæ­£å¸¸ç«æ§å¿å¾çå¿è·³ãéäºæ¨¡åè½å¤ æ¾åºæ±ºå®æçµåé¡çå¿è·³ééµååï¼ä¸¦å¼·èª¿ P æ³¢å T æ³¢ï¼ä»¥åå¿è·³æçºæéåè¨èæ¯å¹å¨ååæ­£å¸¸ç«æ§å¿å¾èå¿æ¿é¡«ååç«æ§å¿åéç·©æ¹é¢çéè¦æ§ã</paragraph>

##### **Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**
2402.05127v1 by Aryan Agrawal

This paper introduces a novel paradigm for depression detection and treatment
using advanced Large Language Models (LLMs): Generative Pre-trained Transformer
4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized
prompts to diagnose, explain, and suggest therapeutic interventions for
depression. A unique few-shot prompting method enhances the models' ability to
analyze and explain depressive symptoms based on the DSM-5 criteria. In the
interaction phase, the models engage in empathetic dialogue management, drawing
from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide,
fostering supportive interactions with individuals experiencing major
depressive disorders. Additionally, the research introduces the Illuminate
Database, enriched with various CBT modules, aiding in personalized therapy
recommendations. The study evaluates LLM performance using metrics such as F1
scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) across different test sets, demonstrating their
effectiveness. This comprehensive approach blends cutting-edge AI with
established psychological methods, offering new possibilities in mental health
care and showcasing the potential of LLMs in revolutionizing depression
diagnosis and treatment strategies.

æè¦ï¼æ¬æä»ç´¹äºä¸ç¨®ä½¿ç¨åé²å¤§åèªè¨æ¨¡å (LLM) é²è¡æé¬±çåµæ¸¬åæ²»ççæ°æ¨¡å¼ï¼çæå¼é è¨ç·´Transformer 4 (GPT-4)ãLlama 2 èå¤©æ©å¨äººå Geminiãéäº LLM ç¶éå¾®èª¿ï¼å·åå°æ¥­æç¤ºï¼å¯è¨ºæ·ãè§£éä¸¦å»ºè­°æé¬±ççæ²»çä»å¥æ¹æ³ãä¸ç¨®ç¨ç¹çå°æ¬¡æç¤ºæ¹æ³å¢å¼·äºæ¨¡åæ ¹æ DSM-5 æ¨æºåæåè§£éæé¬±çççè½åãå¨äºåéæ®µï¼éäºæ¨¡åæåèåçå¿å°è©±ç®¡çï¼å¾ PsychDB åèªç¥è¡çºçæ³ (CBT) æåç­è³æºä¸­æ±²åï¼ä¿é²èç¶æ­·éåº¦æé¬±ççäººåçæ¯ææ§äºåãæ­¤å¤ï¼éé ç ç©¶éä»ç´¹äº Illuminate è³æåº«ï¼å¶ä¸­åå«åç¨® CBT æ¨¡çµï¼æå©æ¼åæ§åæ²»çå»ºè­°ãéé ç ç©¶ä½¿ç¨ F1 åæ¸ãæºç¢ºçãå¬åçãé¤å¼¦ç¸ä¼¼åº¦åé¢åå¬åçç Gisting è©ä¼°æ¿èº« (ROUGE) ç­ææ¨ï¼å¨ä¸åçæ¸¬è©¦éä¸­è©ä¼° LLM çè¡¨ç¾ï¼è­æäºå®åçæææ§ãéç¨®ç¶åæ¹æ³çµåäºå°ç«¯ç AI èæ¢å®çå¿çæ¹æ³ï¼çºå¿çä¿å¥æä¾äºæ°çå¯è½æ§ï¼ä¸¦å±ç¤ºäº LLM å¨é©æ°æé¬±çè¨ºæ·åæ²»çç­ç¥æ¹é¢çæ½åã

##### **Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**
2401.13324v4 by TimothÃ©e Schmude, Laura Koesten, Torsten MÃ¶ller, Sebastian Tschiatschek

Explanations of AI systems rarely address the information needs of people
affected by algorithmic decision-making (ADM). This gap between conveyed
information and information that matters to affected stakeholders can impede
understanding and adherence to regulatory frameworks such as the AI Act. To
address this gap, we present the "XAI Novice Question Bank": A catalog of
affected stakeholders' information needs in two ADM use cases (employment
prediction and health monitoring), covering the categories data, system
context, system usage, and system specifications. Information needs were
gathered in an interview study where participants received explanations in
response to their inquiries. Participants further reported their understanding
and decision confidence, showing that while confidence tended to increase after
receiving explanations, participants also met understanding challenges, such as
being unable to tell why their understanding felt incomplete. Explanations
further influenced participants' perceptions of the systems' risks and
benefits, which they confirmed or changed depending on the use case. When risks
were perceived as high, participants expressed particular interest in
explanations about intention, such as why and to what end a system was put in
place. With this work, we aim to support the inclusion of affected stakeholders
into explainability by contributing an overview of information and challenges
relevant to them when deciding on the adoption of ADM systems. We close by
summarizing our findings in a list of six key implications that inform the
design of future explanations for affected stakeholder audiences.

æè¦ï¼<paragraph>äººå·¥æºæ§ç³»çµ±çèªªæå¾å°è½æ»¿è¶³åæ¼ç®æ³æ±ºç­ (ADM) å½±é¿çäººåçè³è¨éæ±ãå³éçè³è¨èå½±é¿å©å®³éä¿äººéè¦çè³è¨ä¹éçå·®è·ï¼å¯è½æé»ç¤äºè§£åéµå®æ³è¦æ¶æ§ï¼ä¾å¦äººå·¥æºæ§æ³æ¡ãçºäºè§£æ±ºéåå·®è·ï¼æåæåºäºãXAI åå­¸èåé¡åº«ãï¼åå½±é¿å©å®³éä¿äººè³è¨éæ±çç®éï¼æ¶µèå©å ADM ä½¿ç¨æ¡ä¾ï¼å°±æ¥­é æ¸¬åå¥åº·ç£æ¸¬ï¼ï¼æ¶µèè³æãç³»çµ±èçµ¡ãç³»çµ±ä½¿ç¨åç³»çµ±è¦æ ¼é¡å¥ãè³è¨éæ±æ¯ééè¨ªè«ç ç©¶æ¶éçï¼åèèå¨è©¢åå¾æ¶å°èªªæãåèèé²ä¸æ­¥åå ±ä»åççè§£åæ±ºç­ä¿¡å¿ï¼é¡¯ç¤ºéç¶å¨æ¶å°èªªæå¾ä¿¡å¿å¾åæ¼å¢å ï¼ä½åèèä¹éå°äºçè§£ææ°ï¼ä¾å¦ç¡æ³èªªæçºä»éº¼ä»åççè§£æè¦ºä¸å®æ´ãèªªæé²ä¸æ­¥å½±é¿åèèå°ç³»çµ±é¢¨éªåå¥½èççæ³ï¼ä»åææ ¹æä½¿ç¨æ¡ä¾ç¢ºèªææ¹è®éäºçæ³ãç¶é¢¨éªè¢«èªçºå¾é«æï¼åèèè¡¨ç¤ºç¹å¥æèè¶£äºè§£æåçèªªæï¼ä¾å¦çºä»éº¼ä»¥åçºäºä»éº¼ç®çèå»ºç«ç³»çµ±ãéééé å·¥ä½ï¼æåæ¨å¨ééå¨æ±ºç­æ¡ç¨ ADM ç³»çµ±ææä¾ç¸éè³è¨åææ°çæ¦è¦½ï¼ä¾æ¯æ´å°åå½±é¿çå©å®³éä¿äººç´å¥å¯è§£éæ§ãæåæå¾ç¸½çµæåçç¼ç¾ï¼ååºå­é ééµå½±é¿ï¼éäºå½±é¿æåç¥æªä¾éå°åå½±é¿å©å®³éä¿äººåç¾èªªæçè¨­è¨ã</paragraph>

##### **Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**
2401.02985v1 by Vahid Ashrafimoghari, Necdet GÃ¼rkan, Jordan W. Suchow

The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.

æè¦ï¼äººå·¥æºæ§ (AI) çå¿«éæ¼é²ï¼å°¤å¶æ¯å¨å¤§åèªè¨æ¨¡å (LLM) åçæå¼ AI çé åï¼çºååé åçæç¨éåäºæ°éå¾ï¼ä½å¶å¨åæ¥­æè²ä¸­çè§è²ä»æªè¢«ååæ¢è¨ãæ¬ç ç©¶é¦æ¬¡å¼å¥äºåºæºï¼ç¨ä»¥è©ä¼°ä¸åä¸»è¦ LLM çæè½ï¼åæ¬ OpenAI çæ¨¡å (GPT-3.5 TurboãGPT-4 å GPT-4 Turbo)ãGoogle çæ¨¡å (PaLM 2ãGemini 1.0 Pro) å Anthropic çæ¨¡å (Claude 2 å Claude 2.1)ï¼éäºæ¨¡åå°ç¨æ¼ç ç©¶çåæ¥­èª²ç¨å¥å­¸ç¨åºä¸­çééµèè©¦ GMATãæåçåæé¡¯ç¤ºï¼å¤§å¤æ¸ LLM çè¡¨ç¾é½åªæ¼äººé¡èçï¼å¶ä¸­ GPT-4 Turbo ä¸ååªæ¼å¶ä»æ¨¡åï¼æ´è¶è¶äºé å°åå­¸é¢çç ç©¶çå¹³ååæ¸ãééæ¡ä¾ç ç©¶ï¼æ¬ç ç©¶æ¢è¨äº GPT-4 Turbo å¨è§£éç­æ¡ãè©ä¼°åæãè¾¨è­é¯èª¤ãèª¿æ´èªªæåç¢çæ¿ä»£æå¢æ¹é¢çè½åãèåä¸ä»£çæ¬ç¸æ¯ï¼ææ°ç LLM çæ¬ GPT-4 TurboãClaude 2.1 å Gemini 1.0 Pro å¨æ¨çä»»åæ¹é¢æé¡¯èçé²æ­¥ï¼å¸é¡¯äºå¶å¨è§£æ±ºè¤éåé¡æ¹é¢çæ½åãåç®¡ AI å¨æè²ãè©éåè¼å°æ¹é¢çæ¿è«¾å¾æç¢ºï¼ä½ä»æææ°å­å¨ãæåçç ç©¶ä¸åé¡æäº LLM çå­¸è¡æ½åï¼ä¹å¼·èª¿äºå¨æè²ä¸­å¯©æéç¼åæç¨ AI çå¿è¦æ§ãé¨è AI æè¡çé²æ­¥ï¼å»ºç« AI äºåçæ¶æ§ååå®ãé©è­ AI çæçå§å®¹çæºç¢ºæ§ãç¢ºä¿å¨çåå°å¤åå­¸ç¿èçå­åæ¬ï¼ä»¥ååµé ä¸å AI æ¯æäººé¡å°æ¥­ç¥è­çæè²ç°å¢è³ééè¦ãæ¬ç ç©¶çºé²ä¸æ­¥æ¢ç´¢è² è²¬ä»»å°ä½¿ç¨ AI ä¾è±å¯æè²é«é©ä¸¦æ¹åèè©¦æºååè©éæ¹æ³å¥ å®äºåºç¤ã

##### **XAI for In-hospital Mortality Prediction via Multimodal ICU Data**
2312.17624v1 by Xingqiao Li, Jindong Gu, Zhiyong Wang, Yancheng Yuan, Bo Du, Fengxiang He

Predicting in-hospital mortality for intensive care unit (ICU) patients is
key to final clinical outcomes. AI has shown advantaged accuracy but suffers
from the lack of explainability. To address this issue, this paper proposes an
eXplainable Multimodal Mortality Predictor (X-MMP) approaching an efficient,
explainable AI solution for predicting in-hospital mortality via multimodal ICU
data. We employ multimodal learning in our framework, which can receive
heterogeneous inputs from clinical data and make decisions. Furthermore, we
introduce an explainable method, namely Layer-Wise Propagation to Transformer,
as a proper extension of the LRP method to Transformers, producing explanations
over multimodal inputs and revealing the salient features attributed to
prediction. Moreover, the contribution of each modality to clinical outcomes
can be visualized, assisting clinicians in understanding the reasoning behind
decision-making. We construct a multimodal dataset based on MIMIC-III and
MIMIC-III Waveform Database Matched Subset. Comprehensive experiments on
benchmark datasets demonstrate that our proposed framework can achieve
reasonable interpretation with competitive prediction accuracy. In particular,
our framework can be easily transferred to other clinical tasks, which
facilitates the discovery of crucial factors in healthcare research.

æè¦ï¼é æ¸¬å è­·çæ¿ (ICU) çæ£çé¢å§æ­»äº¡çæ¯æçµè¨åºçµæçééµãAI å·²å±ç¾åºåªç°çæºç¢ºåº¦ï¼ä½å»ç¼ºä¹å¯è§£éæ§ãçºäºè§£æ±ºéååé¡ï¼æ¬ææåºäºä¸åå¯è§£éçå¤æ¨¡å¼æ­»äº¡çé æ¸¬å¨ (X-MMP)ï¼æ¡ç¨ææä¸å¯è§£éç AI æ¹å¼ï¼èç±å¤æ¨¡å¼ ICU è³æä¾é æ¸¬é¢å§æ­»äº¡çãæåå¨æ¶æ§ä¸­æ¡ç¨å¤æ¨¡å¼å­¸ç¿ï¼å¯ä»¥æ¥æ¶ä¾èªè¨åºè³æçç°è³ªè¼¸å¥ä¸¦ååºæ±ºç­ãæ­¤å¤ï¼æåå¼å¥äºä¸åå¯è§£éçæ¹æ³ï¼ä¹å°±æ¯åå±¤å³æ­è³ Transformerï¼ä½çº LRP æ¹æ³é©ç¶å°å»¶ä¼¸è³ Transformerï¼å°å¤æ¨¡å¼è¼¸å¥ç¢çè§£éï¼ä¸¦æ­é²æ­¸å æ¼é æ¸¬çé¡¯èç¹å¾µãæ­¤å¤ï¼æ¯åæ¨¡å¼å°è¨åºçµæçè²¢ç»å¯ä»¥è¦è¦ºåï¼åå©è¨åºé«å¸«äºè§£æ±ºç­èå¾ççç±ãæåæ ¹æ MIMIC-III å MIMIC-III æ³¢å½¢è³æåº«æ¯å°å­éå»ºæ§äºä¸åå¤æ¨¡å¼è³æéãå¨åºæºè³æéä¸çå¨é¢å¯¦é©è­æï¼æåæåºçæ¶æ§å¯ä»¥éæåççè©®éï¼ä¸¦å·åç«¶ç­åçé æ¸¬æºç¢ºåº¦ãç¹å¥æ¯ï¼æåçæ¶æ§å¯ä»¥è¼é¬å°è½ç§»å°å¶ä»è¨åºä»»åï¼éæå©æ¼å¨é«çä¿å¥ç ç©¶ä¸­ç¼ç¾ééµå ç´ ã

##### **Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**
2401.09450v2 by Norman Zerbe, Lars Ole Schwen, Christian GeiÃler, Katja Wiesemann, Tom Bisson, Peter Boor, Rita Carvalho, Michael Franz, Christoph Jansen, Tim-Rasmus Kiehl, BjÃ¶rn Lindequist, Nora Charlotte Pohlan, Sarah Schmell, Klaus Strohmenger, Falk Zakrzewski, Markus Plass, Michael Takla, Tobias KÃ¼ster, AndrÃ© Homeyer, Peter Hufnagl

Over the past decade, artificial intelligence (AI) methods in pathology have
advanced substantially. However, integration into routine clinical practice has
been slow due to numerous challenges, including technical and regulatory
hurdles in translating research results into clinical diagnostic products and
the lack of standardized interfaces. The open and vendor-neutral EMPAIA
initiative addresses these challenges. Here, we provide an overview of EMPAIA's
achievements and lessons learned. EMPAIA integrates various stakeholders of the
pathology AI ecosystem, i.e., pathologists, computer scientists, and industry.
In close collaboration, we developed technical interoperability standards,
recommendations for AI testing and product development, and explainability
methods. We implemented the modular and open-source EMPAIA platform and
successfully integrated 14 AI-based image analysis apps from 8 different
vendors, demonstrating how different apps can use a single standardized
interface. We prioritized requirements and evaluated the use of AI in real
clinical settings with 14 different pathology laboratories in Europe and Asia.
In addition to technical developments, we created a forum for all stakeholders
to share information and experiences on digital pathology and AI. Commercial,
clinical, and academic stakeholders can now adopt EMPAIA's common open-source
interfaces, providing a unique opportunity for large-scale standardization and
streamlining of processes. Further efforts are needed to effectively and
broadly establish AI assistance in routine laboratory use. To this end, a
sustainable infrastructure, the non-profit association EMPAIA International,
has been established to continue standardization and support broad
implementation and advocacy for an AI-assisted digital pathology future.

æè¦ï¼å¨éå»çåå¹´ä¸­ï¼ççå­¸ä¸­çäººå·¥æºæ§ (AI) æ¹æ³å·²å¤§å¹é²æ­¥ãç¶èï¼ç±æ¼è¨±å¤ææ°ï¼åæ¬å°ç ç©¶çµæè½åçºè¨åºè¨ºæ·ç¢åå¨æè¡åæ³è¦æ¹é¢çéç¤ï¼ä»¥åç¼ºä¹æ¨æºåä»é¢ï¼å°è´æ´åå°å¸¸è¦è¨åºå¯¦åä¸­é²å±ç·©æ¢ãéæ¾ä¸èä¾æåç¡éç EMPAIA è¨ç«æå°äºéäºææ°ãå¨æ­¤ï¼æåæä¾ EMPAIA çæå°±åç¶é©æè¨çæ¦è¿°ãEMPAIA æ´åäºççå­¸ AI çæç³»çµ±çååå©å®³éä¿äººï¼å³ççå­¸å®¶ãé»è¦ç§å­¸å®¶åç¢æ¥­ãå¨å¯ååä½ä¸ï¼æåå¶å®äºæè¡äºéæ§æ¨æºãAI æ¸¬è©¦åç¢åéç¼å»ºè­°ï¼ä»¥åå¯è§£éæ§æ¹æ³ãæåå¯¦ä½äºæ¨¡çµåä¸éæ¾åå§ç¢¼ç EMPAIA å¹³èºï¼ä¸¦æåæ´åäºä¾èª 8 åä¸åä¾æåç 14 ååºæ¼ AI çå½±ååææç¨ç¨å¼ï¼å±ç¤ºäºä¸åçæç¨ç¨å¼å¦ä½ä½¿ç¨å®ä¸çæ¨æºåä»é¢ãæååªåèæ®éæ±ï¼ä¸¦è©ä¼°äº AI å¨æ­æ´²åäºæ´²ç 14 åä¸åççå¯¦é©å®¤ä¸­çå¯¦éè¨åºæç¨ãé¤äºæè¡éç¼å¤ï¼æåéçºææå©å®³éä¿äººå»ºç«äºä¸åè«å£ï¼ä»¥åäº«æ¸ä½ççå­¸å AI çè³è¨åç¶é©ãåæ¥­ãè¨åºåå­¸è¡å©å®³éä¿äººç¾å¨å¯ä»¥æ¡ç¨ EMPAIA çå¸¸è¦éæ¾åå§ç¢¼ä»é¢ï¼éçºå¤§è¦æ¨¡æ¨æºååç°¡åæµç¨æä¾äºç¨ç¹çæ©æãéè¦é²ä¸æ­¥çåªåæè½ææä¸å»£æ³å°å»ºç«ä¾è¡å¯¦é©å®¤ä½¿ç¨ä¸­ç AI è¼å©ãçºæ­¤ï¼å·²æç«éçå©åæ EMPAIA Internationalï¼ä»¥ä½çºæ°¸çºåºç¤æ¶æ§ï¼ç¹¼çºé²è¡æ¨æºåï¼ä¸¦æ¯æ´å»£æ³å¯¦ä½åå¡å° AI è¼å©æ¸ä½ççå­¸çæªä¾ã

##### **Robust Stochastic Graph Generator for Counterfactual Explanations**
2312.11747v2 by Mario Alfonso Prado-Romero, Bardh Prenkaj, Giovanni Stilo

Counterfactual Explanation (CE) techniques have garnered attention as a means
to provide insights to the users engaging with AI systems. While extensively
researched in domains such as medical imaging and autonomous vehicles, Graph
Counterfactual Explanation (GCE) methods have been comparatively
under-explored. GCEs generate a new graph similar to the original one, with a
different outcome grounded on the underlying predictive model. Among these GCE
techniques, those rooted in generative mechanisms have received relatively
limited investigation despite demonstrating impressive accomplishments in other
domains, such as artistic styles and natural language modelling. The preference
for generative explainers stems from their capacity to generate counterfactual
instances during inference, leveraging autonomously acquired perturbations of
the input graph. Motivated by the rationales above, our study introduces
RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual
Explanations able to produce counterfactual examples from the learned latent
space considering a partially ordered generation sequence. Furthermore, we
undertake quantitative and qualitative analyses to compare RSGG-CE's
performance against SoA generative explainers, highlighting its increased
ability to engendering plausible counterfactual candidates.

æè¦ï¼åäºå¯¦è§£é (CE) æè¡å·²å¼èµ·éæ³¨ï¼ä½çºä¸ç¨®çºè AI ç³»çµ±äºåçä½¿ç¨èæä¾è¦è§£çæ¹æ³ãéç¶å¨é«å­¸å½±ååèªåé§é§æ±½è»ç­é åå»£æ³ç ç©¶ï¼åå½¢åäºå¯¦è§£é (GCE) æ¹æ³ç¸å°è¼å°è¢«æ¢ç´¢ãGCE æç¢çä¸åé¡ä¼¼æ¼åå§åå½¢çæ°åå½¢ï¼ä¸¦æ ¹æåºç¤é æ¸¬æ¨¡åç¢çä¸åççµæãå¨éäº GCE æè¡ä¸­ï¼åç®¡å¨å¶ä»é åï¼ä¾å¦èè¡é¢¨æ ¼åèªç¶èªè¨å»ºæ¨¡ï¼ä¸­å±ç¾åºä»¤äººå°è±¡æ·±å»çæå°±ï¼ä½æ¤åºæ¼çææ©å¶çæè¡ç²å¾çéæ³¨ç¸å°æéãå°çæå¼è§£éå¨çåå¥½æºæ¼å®åå¨æ¨çæéç¢çåäºå¯¦å¯¦ä¾çè½åï¼å©ç¨è¼¸å¥åå½¢çèªä¸»ç²åæ¾åãåºæ¼ä¸è¿°çç±ï¼æåçç ç©¶å¼å¥äº RSGG-CEï¼ä¸ç¨®ç¨æ¼åäºå¯¦è§£éçæ°åç©©å¥é¨æ©åå½¢çæå¨ï¼è½å¤ å¾å­¸ç¿å°çæ½å¨ç©ºéä¸­ç¢çåäºå¯¦ç¯ä¾ï¼èæ®é¨åæåºççæåºåãæ­¤å¤ï¼æåé²è¡å®éåå®æ§åæï¼ä»¥æ¯è¼ RSGG-CE çæè½è SoA çæå¼è§£éå¨ï¼å¼·èª¿å¶å¢å¼·äºç¢çåçè§£éåé¸çè½åã

##### **Evaluating the Utility of Model Explanations for Model Development**
2312.06032v1 by Shawn Im, Jacob Andreas, Yilun Zhou

One of the motivations for explainable AI is to allow humans to make better
and more informed decisions regarding the use and deployment of AI models. But
careful evaluations are needed to assess whether this expectation has been
fulfilled. Current evaluations mainly focus on algorithmic properties of
explanations, and those that involve human subjects often employ subjective
questions to test human's perception of explanation usefulness, without being
grounded in objective metrics and measurements. In this work, we evaluate
whether explanations can improve human decision-making in practical scenarios
of machine learning model development. We conduct a mixed-methods user study
involving image data to evaluate saliency maps generated by SmoothGrad,
GradCAM, and an oracle explanation on two tasks: model selection and
counterfactual simulation. To our surprise, we did not find evidence of
significant improvement on these tasks when users were provided with any of the
saliency maps, even the synthetic oracle explanation designed to be simple to
understand and highly indicative of the answer. Nonetheless, explanations did
help users more accurately describe the models. These findings suggest caution
regarding the usefulness and potential for misunderstanding in saliency-based
explanations.

æè¦ï¼å¯è§£é AI çåæ©ä¹ä¸æ¯è®äººåå¨ä½¿ç¨åé¨ç½² AI æ¨¡åæååºæ´å¥½ãæ´ææºçæ±ºç­ãä½éè¦ä»ç´°è©ä¼°ä»¥è©ä¼°æ¯å¦å·²éå°æ­¤é æãç®åçè©ä¼°ä¸»è¦éä¸­å¨è§£éçæ¼ç®æ³ç¹æ§ï¼èæ¶åäººé¡åè©¦èçè©ä¼°éå¸¸æ¡ç¨ä¸»è§åé¡ä¾æ¸¬è©¦äººé¡å°è§£éæç¨æ§ççæ³ï¼èæ²æåºæ¼å®¢è§ææ¨åæ¸¬éãå¨éé å·¥ä½ä¸­ï¼æåè©ä¼°è§£éæ¯å¦å¯ä»¥å¨æ©å¨å­¸ç¿æ¨¡åéç¼çå¯¦éå ´æ¯ä¸­æ¹åäººé¡æ±ºç­å¶å®ãæåé²è¡äºä¸é æ¶åå½±åè³æçæ··åæ¹æ³ä½¿ç¨èç ç©¶ï¼ä»¥è©ä¼° SmoothGradãGradCAM åé è¨è§£éå¨å©åä»»åä¸­ç¢ççé¡¯èæ§åï¼æ¨¡åé¸æååäºå¯¦æ¨¡æ¬ãä»¤äººé©è¨çæ¯ï¼æåæ²æç¼ç¾ä»»ä½é¡¯èæ§åï¼å³ä½¿æ¯è¨­è¨çºææ¼çè§£ä¸é«åº¦æç¤ºç­æ¡çåæé è¨è§£éï¼è½è®ä½¿ç¨èå¨éäºä»»åä¸é¡¯èæ¹åçè­æãåç®¡å¦æ­¤ï¼è§£éç¢ºå¯¦æå©æ¼ä½¿ç¨èæ´æºç¢ºå°æè¿°æ¨¡åãéäºç¼ç¾æç¤ºæåè¦å°åºæ¼é¡¯èæ§çè§£éä¸­å¯è½å­å¨èª¤è§£çæç¨æ§ä¿æè¬¹æã

##### **Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**
2312.06798v1 by Manas Gaur, Amit Sheth

Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.

æè¦ï¼å¯è§£éæ§åå®å¨æ§å»ºç«ä¿¡ä»»ãéäºéè¦ä¸åæ¨¡åä¾å±ç¤ºä¸è´æ§åå¯é æ§ãçºäºå¯¦ç¾éäºï¼æå¿è¦ä½¿ç¨ååææ¸æåç¥è­ï¼ä¸¦ä½¿ç¨è AI æç¨ç¸éççµ±è¨åç¬¦è AI æ¹æ³ - å®ç¨ä½¿ç¨ä»»ä½ä¸ç¨®æ¹æ³é½ä¸æå¥æãå æ­¤ï¼æåä¸»å¼µä¸¦è©¦åè­æ NeuroSymbolic AI æ¹æ³æ´é©åæ¼ä½¿ AI æçºåä¿¡ä»»ç AI ç³»çµ±ãæåæåºäº CREST æ¡æ¶ï¼å±ç¤ºäºä¸è´æ§ãå¯é æ§ãä½¿ç¨èå±¤ç´çå¯è§£éæ§åå®å¨æ§æ¯å¦ä½å»ºç«å¨ NeuroSymbolic æ¹æ³ä¸çï¼è©²æ¹æ³ä½¿ç¨æ¸æåç¥è­ä¾æ¯æééµæç¨ï¼ä¾å¦å¥åº·åç¦ç¥ï¼çè¦æ±ãæ¬æéé»éæ³¨å¤§åèªè¨æ¨¡å (LLM)ï¼å çºå®æ¯ CREST æ¡æ¶ä¸­é¸æç AI ç³»çµ±ãLLM å å¶å¨èçå»£æ³çèªç¶èªè¨èç (NLP) å ´æ¯æ¹é¢çå¤åè½æ§èååç ç©¶äººå¡çéæ³¨ãä¾å¦ï¼ChatGPT å Google ç MedPaLM å·²æçºæä¾ä¸è¬åå¥åº·ç¸éæ¥è©¢ä¿¡æ¯çæ¥µæå¸æçå¹³å°ãåç®¡å¦æ­¤ï¼éäºæ¨¡åä»ç¶æ¯é»çå­ï¼åç®¡ç´å¥äºäººé¡åé¥åæä»¤å¼å°çèª¿æ´ãä¾å¦ï¼åç®¡å¶å®äºå®å¨é²è­·æªæ½ï¼ChatGPT ä»å¯è½ç¢çä¸å®å¨çåæãCREST æåºäºä¸ç¨®åççæ¹æ³ï¼å¨ NeuroSymbolic æ¡æ¶ä¸­å©ç¨ç¨åºååºæ¼åè¡¨çç¥è­ï¼ä»¥é¡æè LLM ç¸éçææ°ã

##### **Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**
2311.17133v1 by Jacob R. Epifano, Stephen Glass, Ravi P. Ramachandran, Sharad Patel, Aaron J. Masino, Ghulam Rasool

This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.

æè¦ï¼æ¬ç ç©¶è°æ¥äºå¨ COVID-19 ç«ææé´åä»¥åé¢æµæ­»äº¡çæ¶ï¼å·²é¨ç½²äººå·¥æºè½ (AI) æ¨¡åçæ§è½ãå¯è§£éæ§åç¨³å¥æ§ãä½ä¸ºåç±»ç ç©¶ä¸­çé¦ä¾ï¼æä»¬åç°è´å¶æ¯ç¥ç»ç½ç» (BNN) åæºè½è®­ç»ææ¯è®©æä»¬çæ¨¡åå¨æ°æ®åçéå¤§ååæ¶ä»è½ä¿ææ§è½ãæä»¬çç»æå¼ºè°äºå¼åç¨³å¥ç AI æ¨¡åçéè¦æ§ï¼å³ä½¿å¨å·ææææ§çæ¡ä»¶ä¸ï¼è¿äºæ¨¡åä¹è½å¹éæè¶è¶ä¸´åºå»ççé¢æµãæä»¬å¯¹æ¨¡åå¯è§£éæ§çæ¢ç´¢è¡¨æï¼éæºæ¨¡åä¼äº§çæ´å¤æ ·åä¸ä¸ªæ§åçè§£éï¼ä»èçªåºäºå¨ç°å®ä¸ççä¸´åºç¯å¢ä¸­æä¾è¯¦ç»ä¸ä¸ªæ§åè§è§£ç AI æ¨¡åçå¿è¦æ§ãæ­¤å¤ï¼æä»¬å¼ºè°äºéå AI æ¨¡åä¸­ä¸ç¡®å®æ§çéè¦æ§ï¼è¿ä½¿ä¸´åºå»çè½å¤æ ¹æ®å¯é çé¢æµååºæ´ææºçå³ç­ãæä»¬çç ç©¶æå¡å¨å»çä¿å¥ç AI ç ç©¶ä¸­ä¼åèèå®æ½ç§å­¦ï¼å¹¶ç¡®ä¿ AI è§£å³æ¹æ¡å¨ç°å®ä¸ççä¸´åºç¯å¢ä¸­å®ç¨ãæçä¸å¯æç»­ãéè¿è§£å³å»çä¿å¥ç¯å¢ä¸­çç¬ç¹ææåå¤ææ§ï¼ç ç©¶äººåå¯ä»¥å¼ååºæææ¹åä¸´åºå®è·µåæ£èé¢åç AI æ¨¡åã

##### **Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**
2311.15719v1 by Benjamin Keel, Aaron Quyn, David Jayne, Samuel D. Relton

Lung cancer is responsible for 21% of cancer deaths in the UK and five-year
survival rates are heavily influenced by the stage the cancer was identified
at. Recent studies have demonstrated the capability of AI methods for accurate
and early diagnosis of lung cancer from routine scans. However, this evidence
has not translated into clinical practice with one barrier being a lack of
interpretable models. This study investigates the application Variational
Autoencoders (VAEs), a type of generative AI model, to lung cancer lesions.
Proposed models were trained on lesions extracted from 3D CT scans in the
LIDC-IDRI public dataset. Latent vector representations of 2D slices produced
by the VAEs were explored through clustering to justify their quality and used
in an MLP classifier model for lung cancer diagnosis, the best model achieved
state-of-the-art metrics of AUC 0.98 and 93.1% accuracy. Cluster analysis shows
the VAE latent space separates the dataset of malignant and benign lesions
based on meaningful feature components including tumour size, shape, patient
and malignancy class. We also include a comparative analysis of the standard
Gaussian VAE (GVAE) and the more recent Dirichlet VAE (DirVAE), which replaces
the prior with a Dirichlet distribution to encourage a more explainable latent
space with disentangled feature representation. Finally, we demonstrate the
potential for latent space traversals corresponding to clinically meaningful
feature changes.

æè¦ï¼èºçå è±åççæ­»äº¡äººæ¸ç 21%ï¼äºå¹´å­æ´»çå¾å¤§ç¨åº¦åæ±ºæ¼ççè¢«ç¼ç¾çéæ®µãæè¿çç ç©¶å·²è­æäººå·¥æºè½æ¹æ³å·æå¾ä¾è¡ææä¸­æºç¢ºåæ©è¨ºæ·èºççè½åãç¶èï¼æ­¤è­æå°æªè½åçºè¨åºå¯¦åï¼å¶ä¸­ä¸åéç¤æ¯ç¼ºä¹å¯è§£éçæ¨¡åãæ¬ç ç©¶æ¢è¨äºæç¨è®åèªåç·¨ç¢¼å¨ (VAE)ï¼ä¸ç¨®çæå¼äººå·¥æºè½æ¨¡åï¼æ¼èºççç¶ãå°æåºçæ¨¡åè¨ç·´æ¼å¾ LIDC-IDRI å¬å±æ¸æéä¸­æåç 3D é»è¦æ·å±¤ææçç¶ãééèé¡æ¢ç´¢äº VAE çæç 2D åççæ½å¨åéè¡¨ç¤ºï¼ä»¥è­æå¶åè³ªï¼ä¸¦ç¨æ¼èºçè¨ºæ·ç MLP åé¡å¨æ¨¡åï¼æä½³æ¨¡åéå°äº AUC 0.98 å 93.1% æºç¢ºåº¦çæåé²ææ¨ãèé¡åæé¡¯ç¤ºï¼VAE æ½å¨ç©ºéæ ¹æææç¾©çç¹å¾µçµæï¼åæ¬è«ç¤å¤§å°ãå½¢çãæ£èåæ¡æ§é¡å¥ï¼å°æ¡æ§åè¯æ§çç¶çæ¸æéåéãæåéåæ¬æ¨æºé«æ¯ VAE (GVAE) åæ´æ°ççå©åé· VAE (DirVAE) çæ¯è¼åæï¼å¾èç¨çå©åé·åä½åä»£åé©ï¼ä»¥ä¿é²å·æè§£éç¹å¾µè¡¨ç¤ºçæ´å·å¯è§£éæ§çæ½å¨ç©ºéãæå¾ï¼æåå±ç¤ºäºèè¨åºææç¾©çç¹å¾µè®åç¸æçæ½å¨ç©ºéæ©«è¶çæ½åã

##### **MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**
2311.14471v1 by Nathan Blake, Hana Chockler, David A. Kelly, Santiago Calderon Pena, Akchunya Chanchal

Existing tools for explaining the output of image classifiers can be divided
into white-box, which rely on access to the model internals, and black-box,
agnostic to the model. As the usage of AI in the medical domain grows, so too
does the usage of explainability tools. Existing work on medical image
explanations focuses on white-box tools, such as gradcam. However, there are
clear advantages to switching to a black-box tool, including the ability to use
it with any classifier and the wide selection of black-box tools available. On
standard images, black-box tools are as precise as white-box. In this paper we
compare the performance of several black-box methods against gradcam on a brain
cancer MRI dataset. We demonstrate that most black-box tools are not suitable
for explaining medical image classifications and present a detailed analysis of
the reasons for their shortcomings. We also show that one black-box tool, a
causal explainability-based rex, performs as well as \gradcam.

æè¦ï¼ç¾æçåååé¡å¨è¼¸åºè§£éå·¥å·å¯åçºä¾è³´æ¼æ¨¡åå§é¨å­åæ¬éçç½çï¼ä»¥åèæ¨¡åç¡éçé»çãé¨è AI å¨é«çé åçä½¿ç¨å¢å ï¼å¯è§£éæ§å·¥å·çä½¿ç¨ä¹é¨ä¹å¢å ãç¾æé«å­¸å½±åè§£éçå·¥ä½éé»å¨æ¼ç½çå·¥å·ï¼ä¾å¦ gradcamãç¶èï¼åæå°é»çå·¥å·ææé¡¯çåªé»ï¼åæ¬è½å¤ èä»»ä½åé¡å¨ä¸èµ·ä½¿ç¨ï¼ä»¥åå»£æ³çé»çå·¥å·å¯ä¾é¸æãå¨æ¨æºå½±åä¸ï¼é»çå·¥å·èç½çä¸æ¨£ç²¾ç¢ºãå¨æ¬æä¸­ï¼æåæ¯è¼äºå¤ç¨®é»çæ¹æ³å¨è¦ç MRI è³æéä¸è gradcam çæè½ãæåè­æå¤§å¤æ¸é»çå·¥å·ä¸é©åè§£éé«å­¸å½±ååé¡ï¼ä¸¦è©³ç´°åæå¶ç¼ºé»çåå ãæåéè¡¨æä¸ç¨®é»çå·¥å·ï¼åºæ¼å æå¯è§£éæ§ç rexï¼è¡¨ç¾è \gradcam ä¸æ¨£å¥½ã

##### **Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**
2311.12573v3 by Robert Gorwa, Michael Veale

The AI development community is increasingly making use of hosting
intermediaries such as Hugging Face provide easy access to user-uploaded models
and training data. These model marketplaces lower technical deployment barriers
for hundreds of thousands of users, yet can be used in numerous potentially
harmful and illegal ways. In this article, we explain ways in which AI systems,
which can both `contain' content and be open-ended tools, present one of the
trickiest platform governance challenges seen to date. We provide case studies
of several incidents across three illustrative platforms -- Hugging Face,
GitHub and Civitai -- to examine how model marketplaces moderate models.
Building on this analysis, we outline important (and yet nevertheless limited)
practices that industry has been developing to respond to moderation demands:
licensing, access and use restrictions, automated content moderation, and open
policy development. While the policy challenge at hand is a considerable one,
we conclude with some ideas as to how platforms could better mobilize resources
to act as a careful, fair, and proportionate regulatory access point.

æè¦ï¼AI éç¼ç¤¾ç¾¤æ¥çå©ç¨ Hugging Face ç­è¨ç®¡ä¸­ä»æ©æ§æä¾ç¨æ¶ä¸å³çæ¨¡ååè¨ç·´è³æçç°¡æå­åæ¬éãéäºæ¨¡åå¸ééä½äºæ¸åè¬åç¨æ¶çæè¡é¨ç½²éç¤ï¼ä½å¯è½æè¢«ç¨æ¼è¨±å¤æ½å¨æå®³åéæ³çæ¹å¼ãå¨æ¬æä¸­ï¼æåèªªæ AI ç³»çµ±æ¢å¯ä»¥ãåå«ãå§å®¹ï¼åå¯ä»¥ä½çºéæ¾å¼å·¥å·ï¼éæåºäºè¿ä»çºæ­¢ææ£æçå¹³å°æ²»çææ°ä¹ä¸ãæåæä¾ Hugging FaceãGitHub å Civitai ç­ä¸åèªªææ§å¹³å°ä¸æ¸èµ·äºä»¶çæ¡ä¾ç ç©¶ï¼ä»¥æª¢è¦æ¨¡åå¸éå¦ä½å¯©æ ¸æ¨¡åãæ ¹ææ­¤åæï¼æåæ¦è¿°ç¢æ¥­çºåæå¯©æ ¸éæ±èéç¼çéè¦ï¼ä½ä»æéï¼å¯¦åï¼ææ¬ãå­ååä½¿ç¨éå¶ãèªååå§å®¹å¯©æ ¸åéæ¾æ¿ç­å¶å®ãéç¶ç¶åæ¿ç­ææ°ç¸ç¶å¯è§ï¼æåæå¾æåºä¸äºæ§æ³ï¼èªªæå¹³å°å¦ä½è½æ´å¥½å°åå¡è³æºï¼ä½çºè¬¹æãå¬å¹³ä¸é©åº¦çæ³è¦å­åé»ã

##### **Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**
2311.11932v1 by Muta Tah Hira, Mohammad A. Razzaque, Mosharraf Sarker

Background and objectives: By extracting this information, Machine or Deep
Learning (ML/DL)-based autonomous data analysis tools can assist clinicians and
cancer researchers in discovering patterns and relationships from complex data
sets. Many DL-based analyses on ovarian cancer (OC) data have recently been
published. These analyses are highly diverse in various aspects of cancer
(e.g., subdomain(s) and cancer type they address) and data analysis features.
However, a comprehensive understanding of these analyses in terms of these
features and AI assurance (AIA) is currently lacking. This systematic review
aims to fill this gap by examining the existing literature and identifying
important aspects of OC data analysis using DL, explicitly focusing on the key
features and AI assurance perspectives. Methods: The PRISMA framework was used
to conduct comprehensive searches in three journal databases. Only studies
published between 2015 and 2023 in peer-reviewed journals were included in the
analysis. Results: In the review, a total of 96 DL-driven analyses were
examined. The findings reveal several important insights regarding DL-driven
ovarian cancer data analysis: - Most studies 71% (68 out of 96) focused on
detection and diagnosis, while no study addressed the prediction and prevention
of OC. - The analyses were predominantly based on samples from a non-diverse
population (75% (72/96 studies)), limited to a geographic location or country.
- Only a small proportion of studies (only 33% (32/96)) performed integrated
analyses, most of which used homogeneous data (clinical or omics). - Notably, a
mere 8.3% (8/96) of the studies validated their models using external and
diverse data sets, highlighting the need for enhanced model validation, and -
The inclusion of AIA in cancer data analysis is in a very early stage; only
2.1% (2/96) explicitly addressed AIA through explainability.

æè¦ï¼<paragraph>èæ¯åç®æ¨ï¼ééæåéäºè³è¨ï¼æ©å¨ææ·±åº¦å­¸ç¿ (ML/DL) åºæ¼èªä¸»æ¸æåæå·¥å·å¯ä»¥åå©è¨åºé«çåççç ç©¶äººå¡å¾è¤éçæ¸æéä¸­ç¼ç¾æ¨¡å¼åéä¿ãæè¿å·²ç¼è¡¨è¨±å¤åºæ¼ DL çåµå·¢ç (OC) æ¸æåæãéäºåæå¨çççååæ¹é¢ï¼ä¾å¦ï¼å®åæ¶åçå­é ååççé¡åï¼åæ¸æåæåè½æ¹é¢é«åº¦å¤æ¨£åãç¶èï¼ç®åç¼ºä¹å°éäºåæå¨éäºç¹å¾µå AI ä¿è­ (AIA) æ¹é¢çå¨é¢çè§£ãéç¯ç³»çµ±æ§åé¡§æ¨å¨ééæª¢è¦ç¾ææç»ä¸¦æç¢ºéæ³¨ééµç¹å¾µå AI ä¿è­è§é»ï¼ä¾å¡«è£éåç©ºç½ãæ¹æ³ï¼ä½¿ç¨ PRISMA æ¶æ§å¨ä¸åæåè³æåº«ä¸­é²è¡å¨é¢æå°ãåæååæ¬ 2015 å¹´è³ 2023 å¹´éç¼è¡¨æ¼åè¡è©å¯©æåçç ç©¶ãçµæï¼å¨åé¡§ä¸­ï¼ç¸½å±æª¢è¦äº 96 é ç± DL é©åçåæãç ç©¶çµææ­ç¤ºäºå¹¾åéæ¼ç± DL é©åçåµå·¢çæ¸æåæçéè¦è¦è§£ï¼- å¤§å¤æ¸ç ç©¶ 71%ï¼96 é ä¸­æ 68 é ï¼å°æ³¨æ¼æª¢æ¸¬åè¨ºæ·ï¼èæ²æç ç©¶æ¢è¨ OC çé æ¸¬åé é²ã- éäºåæä¸»è¦åºæ¼ä¾èªéå¤åæç¾¤çæ¨£æ¬ï¼75%ï¼96 é ç ç©¶ä¸­ç 72 é ï¼ï¼ï¼åéæ¼æåå°çä½ç½®æåå®¶ã- åªæå°é¨åç ç©¶ï¼å 33%ï¼96 é ç ç©¶ä¸­ç 32 é ï¼å·è¡æ´ååæï¼å¶ä¸­å¤§å¤æ¸ä½¿ç¨åè³ªæ¸æï¼è¨åºæçµå­¸ï¼ã- å¼å¾æ³¨æçæ¯ï¼åªæ 8.3%ï¼96 é ç ç©¶ä¸­ç 8 é ï¼ä½¿ç¨å¤é¨åå¤åæ¸æéé©è­äºå¶æ¨¡åï¼å¼·èª¿äºå å¼·æ¨¡åé©è­çå¿è¦æ§ï¼ä»¥å- å° AIA ç´å¥ççæ¸æåæä»èæ¼éå¸¸æ©æçéæ®µï¼åªæ 2.1%ï¼96 é ç ç©¶ä¸­ç 2 é ï¼ééå¯è§£éæ§æç¢ºæ¢è¨äº AIAã</paragraph>

##### **Representing visual classification as a linear combination of words**
2311.10933v1 by Shobhit Agarwal, Yevgeniy R. Semenov, William Lotter

Explainability is a longstanding challenge in deep learning, especially in
high-stakes domains like healthcare. Common explainability methods highlight
image regions that drive an AI model's decision. Humans, however, heavily rely
on language to convey explanations of not only "where" but "what".
Additionally, most explainability approaches focus on explaining individual AI
predictions, rather than describing the features used by an AI model in
general. The latter would be especially useful for model and dataset auditing,
and potentially even knowledge generation as AI is increasingly being used in
novel tasks. Here, we present an explainability strategy that uses a
vision-language model to identify language-based descriptors of a visual
classification task. By leveraging a pre-trained joint embedding space between
images and text, our approach estimates a new classification task as a linear
combination of words, resulting in a weight for each word that indicates its
alignment with the vision-based classifier. We assess our approach using two
medical imaging classification tasks, where we find that the resulting
descriptors largely align with clinical knowledge despite a lack of
domain-specific language training. However, our approach also identifies the
potential for 'shortcut connections' in the public datasets used. Towards a
functional measure of explainability, we perform a pilot reader study where we
find that the AI-identified words can enable non-expert humans to perform a
specialized medical task at a non-trivial level. Altogether, our results
emphasize the potential of using multimodal foundational models to deliver
intuitive, language-based explanations of visual tasks.

æè¦ï¼<paragraph>è§£éæ§æ¯æ·±åº¦å­¸ç¿ä¸­é·æçææ°ï¼ç¹å¥æ¯å¨é«çä¿å¥ç­é«é¢¨éªé åãå¸¸è¦çè§£éæ§æ¹æ³æå¼·èª¿é©å AI æ¨¡åæ±ºç­çå½±åååãç¶èï¼äººé¡å¾å¤§ç¨åº¦ä¾è³´èªè¨ä¾å³éä¸åæ¯ãå¨åªè£¡ãï¼éæãæ¯ä»éº¼ãçè§£éãæ­¤å¤ï¼å¤§å¤æ¸è§£éæ§æ¹æ³é½å°æ³¨æ¼è§£éåå¥ AI é æ¸¬ï¼èä¸æ¯æè¿° AI æ¨¡åä¸è¬ä½¿ç¨çç¹å¾µãå¾èå°æ¼æ¨¡ååè³æéç¨½æ ¸ç¹å¥æç¨ï¼çè³å¯è½å¨ AI æä¾æç¨æ¼æ°ç©ä»»åæç¢çç¥è­ãå¨æ­¤ï¼æåæåºä¸åä½¿ç¨è¦è¦ºèªè¨æ¨¡åä¾è¾¨è­è¦è¦ºåé¡ä»»åçèªè¨æè¿°ç¬¦çè§£éæ§ç­ç¥ãééå©ç¨å½±ååæå­ä¹éé åè¨ç·´çè¯ååµå¥ç©ºéï¼æåçåæ³å°æ°çåé¡ä»»åä¼°è¨çºä¸åç·æ§æå­çµåï¼å°è´æ¯åæå­é½ææ¬éï¼è¡¨ç¤ºå®èåºæ¼è¦è¦ºçåé¡å¨å°é½ãæåä½¿ç¨å©åé«å­¸å½±ååé¡ä»»åä¾è©ä¼°æåçåæ³ï¼æåç¼ç¾ç¢ççæè¿°ç¬¦å¨å¾å¤§ç¨åº¦ä¸èè¨åºç¥è­ä¸è´ï¼åç®¡ç¼ºä¹ç¹å®é åçèªè¨è¨ç·´ãç¶èï¼æåçåæ³ä¹ç¼ç¾äºæç¨å¬éè³æéä¸­çãæ·å¾é£ç·ãçå¯è½æ§ãçºäºéå°è§£éæ§çåè½æ§è¡¡éï¼æåé²è¡äºä¸é è©¦é©è®èç ç©¶ï¼ç¼ç¾ AI è­å¥çæå­è½è®éå°å®¶äººé¡å¨éå¹³å¡çå±¤ç´å·è¡å°æ¥­çé«çä»»åãç¸½ä¹ï¼æåççµæå¼·èª¿äºä½¿ç¨å¤æ¨¡å¼åºç¤æ¨¡åä¾æä¾ç´è§çãåºæ¼èªè¨çè¦è¦ºä»»åè§£éçæ½åã</paragraph>

##### **Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging**
2311.02115v2 by Emma A. M. Stanley, Raissa Souza, Anthony Winder, Vedant Gulve, Kimberly Amador, Matthias Wilms, Nils D. Forkert

Artificial intelligence (AI) models trained using medical images for clinical
tasks often exhibit bias in the form of disparities in performance between
subgroups. Since not all sources of biases in real-world medical imaging data
are easily identifiable, it is challenging to comprehensively assess how those
biases are encoded in models, and how capable bias mitigation methods are at
ameliorating performance disparities. In this article, we introduce a novel
analysis framework for systematically and objectively investigating the impact
of biases in medical images on AI models. We developed and tested this
framework for conducting controlled in silico trials to assess bias in medical
imaging AI using a tool for generating synthetic magnetic resonance images with
known disease effects and sources of bias. The feasibility is showcased by
using three counterfactual bias scenarios to measure the impact of simulated
bias effects on a convolutional neural network (CNN) classifier and the
efficacy of three bias mitigation strategies. The analysis revealed that the
simulated biases resulted in expected subgroup performance disparities when the
CNN was trained on the synthetic datasets. Moreover, reweighing was identified
as the most successful bias mitigation strategy for this setup, and we
demonstrated how explainable AI methods can aid in investigating the
manifestation of bias in the model using this framework. Developing fair AI
models is a considerable challenge given that many and often unknown sources of
biases can be present in medical imaging datasets. In this work, we present a
novel methodology to objectively study the impact of biases and mitigation
strategies on deep learning pipelines, which can support the development of
clinical AI that is robust and responsible.

æè¦ï¼<paragraph>ä½¿ç¨é«çå½±åè¨ç·´çäººå·¥æºæ§ (AI) æ¨¡åï¼ç¨æ¼è¨åºä»»åæï¼å¸¸æå¨æè½ä¸å±ç¾åºæ¬¡ç¾¤é«ä¹éçå·®ç°ï¼å½¢æåè¦ãç±æ¼ä¸¦éææçå¯¦ä¸çé«çå½±åè³æä¸­çåè¦ä¾æºé½å®¹æè¾¨è­ï¼å æ­¤å¨é¢è©ä¼°éäºåè¦æ¯å¦ä½ç·¨ç¢¼å°æ¨¡åä¸­ï¼ä»¥ååè¦ç·©è§£æ¹æ³å¨æ¹åæè½å·®ç°æ¹é¢çè½åï¼æ¯ä¸é ææ°ãå¨æ¬æä¸­ï¼æåä»ç´¹äºä¸åæ°ç©çåææ¶æ§ï¼ç¨æ¼ç³»çµ±åä¸å®¢è§å°èª¿æ¥é«çå½±åä¸­çåè¦å° AI æ¨¡åçå½±é¿ãæåéç¼ä¸¦æ¸¬è©¦äºéåæ¶æ§ï¼ä»¥é²è¡åæ§çé»è¦æ¨¡æ¬è©¦é©ï¼ä½¿ç¨ä¸åå·¥å·ä¾è©ä¼°é«çå½±å AI ä¸­çåè¦ï¼è©²å·¥å·ç¨æ¼ç¢çå·æå·²ç¥ç¾çå½±é¿ååè¦ä¾æºçåæç£å±æ¯å½±åãå¯è¡æ§ééä½¿ç¨ä¸ååäºå¯¦åè¦æå¢ä¾è¡¡éæ¨¡æ¬åè¦ææå°å·ç©ç¥ç¶ç¶²è·¯ (CNN) åé¡å¨åä¸ååè¦ç·©è§£ç­ç¥çå½±é¿ï¼ä¸¦å±ç¤ºåºä¾ãåæé¡¯ç¤ºï¼ç¶ CNN å¨åæè³æéä¸åè¨æï¼æ¨¡æ¬åè¦æå°è´é æçæ¬¡ç¾¤é«æè½å·®ç°ãæ­¤å¤ï¼éæ°å æ¬è¢«èªçºæ¯æ­¤è¨­å®ä¸­ææåçåè¦ç·©è§£ç­ç¥ï¼æåå±ç¤ºäºè§£éæ§ AI æ¹æ³å¦ä½åå©ä½¿ç¨éåæ¶æ§èª¿æ¥æ¨¡åä¸­åè¦çè¡¨ç¾ãéç¼å¬å¹³ç AI æ¨¡åæ¯ä¸é éå¤§çææ°ï¼å çºé«çå½±åè³æéä¸­å¯è½å­å¨è¨±å¤ä¸ç¶å¸¸æªç¥çåè¦ä¾æºãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸ç¨®æ°ç©çæ¹æ³ï¼ç¨æ¼å®¢è§å°ç ç©¶åè¦åç·©è§£ç­ç¥å°æ·±åº¦å­¸ç¿ç®¡ç·çå½±é¿ï¼éå¯ä»¥æ¯æ´å¥å¨ä¸è² è²¬ä»»çè¨åº AI çéç¼ã</paragraph>

##### **Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**
2310.19174v1 by Adam White, Margarita Saranti, Artur d'Avila Garcez, Thomas M. H. Hope, Cathy J. Price, Howard Bowman

Machine learning offers great potential for automated prediction of
post-stroke symptoms and their response to rehabilitation. Major challenges for
this endeavour include the very high dimensionality of neuroimaging data, the
relatively small size of the datasets available for learning, and how to
effectively combine neuroimaging and tabular data (e.g. demographic information
and clinical characteristics). This paper evaluates several solutions based on
two strategies. The first is to use 2D images that summarise MRI scans. The
second is to select key features that improve classification accuracy.
Additionally, we introduce the novel approach of training a convolutional
neural network (CNN) on images that combine regions-of-interest extracted from
MRIs, with symbolic representations of tabular data. We evaluate a series of
CNN architectures (both 2D and a 3D) that are trained on different
representations of MRI and tabular data, to predict whether a composite measure
of post-stroke spoken picture description ability is in the aphasic or
non-aphasic range. MRI and tabular data were acquired from 758 English speaking
stroke survivors who participated in the PLORAS study. The classification
accuracy for a baseline logistic regression was 0.678 for lesion size alone,
rising to 0.757 and 0.813 when initial symptom severity and recovery time were
successively added. The highest classification accuracy 0.854 was observed when
8 regions-of-interest was extracted from each MRI scan and combined with lesion
size, initial severity and recovery time in a 2D Residual Neural Network.Our
findings demonstrate how imaging and tabular data can be combined for high
post-stroke classification accuracy, even when the dataset is small in machine
learning terms. We conclude by proposing how the current models could be
improved to achieve even higher levels of accuracy using images from hospital
scanners.

æè¦ï¼æ©å¨å­¸ç¿çºèªåé æ¸¬ä¸­é¢¨å¾ççåå¶å°å¾©å¥çåææä¾äºæ¥µå¤§çæ½åãéé å·¥ä½çéå¤§ææ°åæ¬ç¥ç¶å½±åè³æçç¶­åº¦éå¸¸é«ãå¯ç¨æ¼å­¸ç¿çè³æéè¦æ¨¡ç¸å°è¼å°ï¼ä»¥åå¦ä½ææçµåç¥ç¶å½±ååè¡¨æ ¼è³æï¼ä¾å¦äººå£çµ±è¨è³è¨åè¨åºç¹å¾µï¼ãæ¬ææ ¹æå©ç¨®ç­ç¥è©ä¼°äºå¤ç¨®è§£æ±ºæ¹æ¡ãç¬¬ä¸ç¨®æ¯ä½¿ç¨ç¸½çµ MRI ææç 2D å½±åãç¬¬äºç¨®æ¯é¸ææå©æ¼æé«åé¡ç²¾ç¢ºåº¦çééµç¹å¾µãæ­¤å¤ï¼æåå¼å¥äºå¨çµåå¾ MRI ä¸­æåçæèè¶£ååèè¡¨æ ¼è³æçç¬¦èè¡¨ç¤ºçå½±åä¸è¨ç·´å·ç©ç¥ç¶ç¶²è·¯ (CNN) çæ°ç©æ¹æ³ãæåè©ä¼°äºä¸ç³»å CNN æ¶æ§ï¼2D å 3Dï¼ï¼éäºæ¶æ§å¨ MRI åè¡¨æ ¼è³æçä¸åè¡¨ç¤ºä¸é²è¡è¨ç·´ï¼ä»¥é æ¸¬ä¸­é¢¨å¾å£è¿°åçæè¿°è½åçç¶åæ¸¬éæ¯å¦å¨å¤±èªçæéå¤±èªçç¯åå§ãMRI åè¡¨æ ¼è³æä¾èª 758 ååè PLORAS ç ç©¶çè±èªä¸­é¢¨åå­èãåéå°çç¶å¤§å°çåºç·éè¼¯è¿´æ­¸åé¡æºç¢ºåº¦çº 0.678ï¼ç¶ä¾åºå å¥åå§ççå´éç¨åº¦åæ¢å¾©æéæï¼ä¸åè³ 0.757 å 0.813ãå¨å¾æ¯å MRI ææä¸­æå 8 åæèè¶£ååä¸¦å¨ 2D æ®å·®ç¥ç¶ç¶²è·¯ä¸­èçç¶å¤§å°ãåå§å´éç¨åº¦åæ¢å¾©æéçµåæï¼è§å¯å°æé«çåé¡æºç¢ºåº¦ 0.854ãæåçç ç©¶çµæå±ç¤ºäºå¦ä½å°å½±ååè¡¨æ ¼è³æçµåèµ·ä¾ä»¥ç²å¾é«æ¼ä¸­é¢¨å¾åé¡æºç¢ºåº¦ï¼å³ä½¿å¨æ©å¨å­¸ç¿è¡èªä¸­è³æéå¾å°çææ³ä¸ä¹æ¯å¦æ­¤ãæå¾ï¼æåæåºå¦ä½æ¹é²ç®åçæ¨¡åï¼ä»¥ä½¿ç¨ä¾èªé«é¢ææåçå½±åä¾å¯¦ç¾æ´é«çæºç¢ºåº¦ã

##### **Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**
2310.01828v2 by Hossein Shreim, Abdul Karim Gizzini, Ali J. Ghandour

eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.

æè¦ï¼å¯è§£éäººå·¥æºæ§ (XAI) å·²æçºèçä»»åééµæç¨ç¨å¼æçä¸é åºæ¬éæ±ï¼ç¢ºä¿æ¡ç¨é»ç AI æ¨¡åçéæåº¦åå¯è§£éæ§ãXAI çéè¦æ§æ¶µèå¾é«çä¿å¥å°éèçåç¨®é åï¼å¨éäºé åä¸­ï¼äºè§£æ·±åº¦å­¸ç¿æ¼ç®æ³çæ±ºç­å¶å®éç¨è³ééè¦ãå¤§å¤æ¸åºæ¼ AI çé»è¦è¦è¦ºæ¨¡åéå¸¸æ¯é»çå­ï¼å æ­¤ï¼å¨å½±åèçä¸­æä¾æ·±åº¦ç¥ç¶ç¶²è·¯çå¯è§£éæ§å°æ¼å¶å¨é«å­¸å½±ååæãèªåé§é§åéæ¸¬æç¨ä¸­çå»£æ³æ¡ç¨åé¨ç½²è³ééè¦ãæè¿ï¼å·²éå°å½±ååé¡ä»»åå¼å¥äºå¤ç¨® XAI æ¹æ³ãç¸åå°ï¼å½±ååå²å¨å¯è§£éæ§çèæ¯ä¸åå°çéæ³¨ç¸å°è¼å°ï¼åç®¡å®æ¯é»è¦è¦è¦ºæç¨ä¸­çä¸é åºæ¬ä»»åï¼ç¹å¥æ¯å¨éæ¸¬ä¸­ãåªæé¨åç ç©¶æåºç¨æ¼å½±ååå²çåºæ¼æ¢¯åº¦ç XAI æ¼ç®æ³ãæ¬ææ¹ç·¨äºæè¿çç¡æ¢¯åº¦ Sobol XAI æ¹æ³ä»¥é²è¡èªæåå²ãçºäºè¡¡é Sobol æ¹æ³å¨åå²ä¸­çæè½ï¼æåæåºäºä¸ç¨®åºæ¼å¯å­¸ç¿éè¨æ¨¡åçå®é XAI è©ä¼°æ¹æ³ãæ­¤æ¨¡åçä¸»è¦ç®çæ¯å¨è§£éåä¸èªç¼éè¨ï¼å¶ä¸­è¼é«çèªç¼éè¨è¡¨ç¤ºè¼ä½çæºç¢ºåº¦ï¼åä¹äº¦ç¶ãé²è¡åºæºåæä»¥è©ä¼°åæ¯è¼ä¸ç¨® XAI æ¹æ³çæè½ï¼åæ¬ Seg-Grad-CAMãSeg-Grad-CAM++ å Seg-Sobolï¼ä¸¦ä½¿ç¨ææåºçåºæ¼éè¨çè©ä¼°æè¡ãéæ§æäºä½¿ç¨é«è§£æåº¦è¡æå½±åå·è¡åè©ä¼° XAI æ¹æ³çé¦æ¬¡åè©¦ã

##### **Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**
2311.01463v1 by Muhammad Aurangzeb Ahmad, Ilker Yaramis, Taposh Dutta Roy

Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.

æè¦ï¼å¤§åèªè¨æ¨¡åå¨ç­æéå§å·²å¨å¤åé åä¸­å¤§éæ¿å¢ãç¶èï¼ç±æ¼äºå¯¦æ§ãé£è²«æ§åå¹»è¦ºç­åé¡ï¼é«çåä¿å¥é åå°å¶æ¡ç¨ç¶è±«ä¸æ±ºãéæ¼é«çä¿å¥çé«é¢¨éªæ§è³ªï¼è¨±å¤ç ç©¶äººå¡çè³è­¦åä¸è¦ä½¿ç¨å®ï¼ç´å°éäºåé¡å¾å°è§£æ±ºãå¨é«çä¿å¥ä¸­å¯¦æ½åé¨ç½² LLM çééµæ¯ä½¿éäºæ¨¡åå¼å¾ä¿¡è³´ãéæï¼ç¡å¯è½å¤ï¼ä¸å¯è§£éãå¨æ¬æä¸­ï¼æåæè¿°äºå»ºç«å¯é ãå¼å¾ä¿¡è³´åç¡åè¦æ¨¡åçééµè¦ç´ ï¼ä½çºå®åå¨é«çä¿å¥ä¸­å¾å°æ¡ç¨çå¿è¦æ¢ä»¶ãå·é«ä¾èªªï¼æåå°æ³¨æ¼å¨é«çä¿å¥èæ¯ä¸å°å¹»è¦ºé²è¡éåãé©è­åç·©è§£ãæå¾ï¼æåè¨è«äº LLM å¨é«çä¿å¥ä¸­çæªä¾å¯è½æ¯ä»éº¼æ¨£å­ã

##### **When to Trust AI: Advances and Challenges for Certification of Neural Networks**
2309.11196v1 by Marta Kwiatkowska, Xiyue Zhang

Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.

æè¦ï¼äººå·¥æºæ§ï¼AIï¼å·²å¿«éé²æ­¥ï¼ç¾å·²æºåé¨ç½²æ¼å»£æ³çæç¨ç¨å¼ä¸­ï¼ä¾å¦èªä¸»ç³»çµ±ãé«çè¨ºæ·åèªç¶èªè¨èçãåæ©æ¡ç¨ AI æè¡æ¼å¯¦éæç¨ç¨å¼ä¸¦éæ²æåé¡ï¼ç¹å¥æ¯å°æ¼ç¥ç¶ç¶²è·¯ï¼å®å¯è½ä¸ç©©å®ä¸å®¹æåå°å°ææ§ç¯ä¾çå½±é¿ãå¾é·é ä¾çï¼éè¦éç¼é©ç¶çå®å¨ä¿è­æè¡ï¼ä»¥æ¸å°å å¯é¿åçç³»çµ±æéèé æçæ½å¨å·å®³ï¼ä¸¦ç¢ºä¿å¯ä¿¡è³´æ§ãæ¬æèéæ¼èªè­åå¯è§£éæ§ï¼æ¦è¿°äºå·²éç¼ç¨æ¼ç¢ºä¿ AI æ±ºç­å®å¨çæè¡ï¼ä¸¦è¨è«æªä¾çææ°ã

##### **Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**
2309.10424v1 by Juan M. GarcÃ­a-GÃ³mez, Vicent Blanes-Selva, JosÃ© Carlos de BartolomÃ© Cenzano, Jaime Cebolla-Cornejo, AscensiÃ³n DoÃ±ate-MartÃ­nez

The Directorate General for Parliamentary Research Services of the European
Parliament has prepared a report to the Members of the European Parliament
where they enumerate seven main risks of Artificial Intelligence (AI) in
medicine and healthcare: patient harm due to AI errors, misuse of medical AI
tools, bias in AI and the perpetuation of existing inequities, lack of
transparency, privacy and security issues, gaps in accountability, and
obstacles in implementation.
  In this study, we propose fourteen functional requirements that AI systems
may implement to reduce the risks associated with their medical purpose: AI
passport, User management, Regulation check, Academic use only disclaimer, data
quality assessment, Clinicians double check, Continuous performance evaluation,
Audit trail, Continuous usability test, Review of retrospective/simulated
cases, Bias check, eXplainable AI, Encryption and use of field-tested
libraries, and Semantic interoperability.
  Our intention here is to provide specific high-level specifications of
technical solutions to ensure continuous good performance and use of AI systems
to benefit patients in compliance with the future EU regulatory framework.

æè¦ï¼æ­æ´²è­°æè­°æç ç©¶æåç¸½å±å·²çºæ­æ´²è­°æè­°å¡æºåäºä¸ä»½å ±åï¼å¶ä¸­åèäºäººå·¥æºè½ (AI) å¨é«çä¿å¥é åçä¸é ä¸»è¦é¢¨éªï¼AI é¯èª¤å°è´æ£èåå°å·å®³ãé«ç AI å·¥å·è¢«æ¿«ç¨ãAI å­å¨åè¦ä¸¦å°è´ç¾æ inequities æçºå­å¨ãç¼ºä¹éæåº¦ãé±ç§åå®å¨åé¡ãåè²¬å·®è·ä»¥åå¯¦æ½éç¤ã
  å¨éé ç ç©¶ä¸­ï¼æåæåºäºååé åè½æ§è¦æ±ï¼AI ç³»çµ±å¯ä»¥å¯¦æ½éäºè¦æ±ä¾éä½èå¶é«çç®çç¸éçé¢¨éªï¼AI è­·ç§ãä½¿ç¨èç®¡çãæ³è¦æª¢æ¥ãåéå­¸è¡ç¨éåè²¬è²æãè³æåè³ªè©ä¼°ãè¨åºé«çééæª¢æ¥ãæçºæè½è©ä¼°ãç¨½æ ¸è¿½è¹¤ãæçºå¯ç¨æ§æ¸¬è©¦ãåé¡§åæº¯/æ¨¡æ¬æ¡ä¾ãåè¦æª¢æ¥ãå¯è§£é AIãå å¯åä½¿ç¨ç¶éå¯¦å°æ¸¬è©¦çç¨å¼åº«ï¼ä»¥åèªæäºéæ§ã
  æåå¨æ­¤çç®çæ¯æä¾æè¡è§£æ±ºæ¹æ¡çç¹å®é«éè¦æ ¼ï¼ä»¥ç¢ºä¿æçºè¯å¥½çæè½ï¼ä¸¦ä½¿ç¨ AI ç³»çµ±ï¼ä»¥ç¬¦åæªä¾çæ­çæ³è¦æ¶æ§ï¼å¾èä½¿æ£èåçã

##### **QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**
2309.10293v3 by Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Juan D. Velasquez, Niall Higgins

Artificial Intelligence techniques can be used to classify a patient's
physical activities and predict vital signs for remote patient monitoring.
Regression analysis based on non-linear models like deep learning models has
limited explainability due to its black-box nature. This can require
decision-makers to make blind leaps of faith based on non-linear model results,
especially in healthcare applications. In non-invasive monitoring, patient data
from tracking sensors and their predisposing clinical attributes act as input
features for predicting future vital signs. Explaining the contributions of
various features to the overall output of the monitoring application is
critical for a clinician's decision-making. In this study, an Explainable AI
for Quantitative analysis (QXAI) framework is proposed with post-hoc model
explainability and intrinsic explainability for regression and classification
tasks in a supervised learning approach. This was achieved by utilizing the
Shapley values concept and incorporating attention mechanisms in deep learning
models. We adopted the artificial neural networks (ANN) and attention-based
Bidirectional LSTM (BiLSTM) models for the prediction of heart rate and
classification of physical activities based on sensor data. The deep learning
models achieved state-of-the-art results in both prediction and classification
tasks. Global explanation and local explanation were conducted on input data to
understand the feature contribution of various patient data. The proposed QXAI
framework was evaluated using PPG-DaLiA data to predict heart rate and mobile
health (MHEALTH) data to classify physical activities based on sensor data.
Monte Carlo approximation was applied to the framework to overcome the time
complexity and high computation power requirements required for Shapley value
calculations.

æè¦ï¼äººå·¥æºæ§æè¡å¯ç¨æ¼åé¡çæ£çèº«é«æ´»åä¸¦é æ¸¬é è·çæ£ç£æ§çéè¦çå½å¾µè±¡ãåºæ¼æ·±åº¦å­¸ç¿æ¨¡åç­éç·æ§æ¨¡åçåæ­¸åæç±æ¼å¶é»çå­çæ§è³ªèå·ææéçå¯è§£éæ§ãéå¯è½éè¦æ±ºç­èæ ¹æéç·æ§æ¨¡åçµæååºç²ç®çä¿¡ä»°é£èºï¼ç¹å¥æ¯å¨é«çä¿å¥æç¨ä¸­ãå¨éä¾µå¥æ§ç£æ§ä¸­ï¼ä¾èªè¿½è¹¤ææ¸¬å¨åå¶ææè¨åºå±¬æ§ççæ£è³æåç¶é æ¸¬æªä¾çå½å¾µè±¡çè¼¸å¥ç¹å¾µãè§£éåç¨®ç¹å¾µå°ç£æ§æç¨ç¨å¼æ´é«è¼¸åºçè²¢ç»å°æ¼è¨åºé«ççæ±ºç­è³ééè¦ãå¨æ¬ç ç©¶ä¸­ï¼æåºäºä¸åç¨æ¼éååæçå¯è§£éäººå·¥æºæ§ (QXAI) æ¶æ§ï¼è©²æ¶æ§å·æç£ç£å¼å­¸ç¿æ¹æ³ä¸­åæ­¸ååé¡ä»»åçäºå¾æ¨¡åå¯è§£éæ§åå§å¨å¯è§£éæ§ãéééå©ç¨ Shapley å¼æ¦å¿µä¸¦å°æ³¨æåæ©å¶ç´å¥æ·±åº¦å­¸ç¿æ¨¡åä¾å¯¦ç¾ãæåæ¡ç¨äººå·¥ç¥ç¶ç¶²è·¯ (ANN) ååºæ¼æ³¨æåçéå LSTM (BiLSTM) æ¨¡åï¼æ ¹æææ¸¬å¨è³æé æ¸¬å¿çååé¡èº«é«æ´»åãæ·±åº¦å­¸ç¿æ¨¡åå¨é æ¸¬ååé¡ä»»åä¸­é½åå¾äºæåé²çææãå°è¼¸å¥è³æé²è¡å¨å±è§£éåå±é¨è§£éï¼ä»¥äºè§£åç¨®çæ£è³æçç¹å¾µè²¢ç»ãææåºç QXAI æ¶æ§ä½¿ç¨ PPG-DaLiA è³æè©ä¼°ï¼ä»¥é æ¸¬å¿çï¼ä¸¦ä½¿ç¨è¡åå¥åº· (MHEALTH) è³ææ ¹æææ¸¬å¨è³æå°èº«é«æ´»åé²è¡åé¡ãèå°å¡ç¾è¿ä¼¼æ³æç¨æ¼è©²æ¶æ§ï¼ä»¥åæ Shapley å¼è¨ç®æéçæéè¤éåº¦åé«éç®è½åéæ±ã

##### **Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**
2309.09917v1 by Adarsa Sivaprasad, Ehud Reiter, Nava Tintarev, Nir Oren

In explainable artificial intelligence (XAI) research, the predominant focus
has been on interpreting models for experts and practitioners. Model agnostic
and local explanation approaches are deemed interpretable and sufficient in
many applications. However, in domains like healthcare, where end users are
patients without AI or domain expertise, there is an urgent need for model
explanations that are more comprehensible and instil trust in the model's
operations. We hypothesise that generating model explanations that are
narrative, patient-specific and global(holistic of the model) would enable
better understandability and enable decision-making. We test this using a
decision tree model to generate both local and global explanations for patients
identified as having a high risk of coronary heart disease. These explanations
are presented to non-expert users. We find a strong individual preference for a
specific type of explanation. The majority of participants prefer global
explanations, while a smaller group prefers local explanations. A task based
evaluation of mental models of these participants provide valuable feedback to
enhance narrative global explanations. This, in turn, guides the design of
health informatics systems that are both trustworthy and actionable.

æè¦ï¼å¨å¯è§£éäººå·¥æºè½ (XAI) ç ç©¶ä¸­ï¼ä¸»è¦éç¹å¨äºä¸ºä¸å®¶åä»ä¸èè§£éæ¨¡åãæ¨¡åä¸å¯ç¥åå±é¨è§£éæ¹æ³å¨è®¸å¤åºç¨ä¸­è¢«è®¤ä¸ºæ¯å¯è§£éä¸è¶³å¤çãç¶èï¼å¨å»çä¿å¥ç­é¢åï¼æç»ç¨æ·æ¯ç¼ºä¹äººå·¥æºè½æé¢åä¸ä¸ç¥è¯çæ£èï¼å æ­¤è¿«åéè¦æ´æäºçè§£ä¸è½æ¿åå¯¹æ¨¡åæä½çä¿¡ä»»çæ¨¡åè§£éãæä»¬åè®¾çæåè¿°æ§ãæ£èç¹å®ä¸å¨å±ï¼æ¨¡åæ´ä½ï¼çæ¨¡åè§£éå°è½å¤æé«å¯çè§£æ§å¹¶æ¯æå³ç­å¶å®ãæä»¬ä½¿ç¨å³ç­æ æ¨¡åå¯¹æ­¤è¿è¡æµè¯ï¼ä¸ºè¢«è¯å«ä¸ºæ£æå å¿çé«é£é©çæ£èçæå±é¨åå¨å±è§£éãè¿äºè§£éä¼åç°ç»éä¸å®¶ç¨æ·ãæä»¬åç°ç¨æ·å¼ºçåå¥½ç¹å®ç±»åçè§£éãå¤§å¤æ°åä¸èåå¥½å¨å±è§£éï¼èè¾å°çä¸ç»åä¸èåå¥½å±é¨è§£éãåºäºä»»å¡çå¿çæ¨¡åè¯ä¼°ä¸ºè¿äºåä¸èæä¾äºæä»·å¼çåé¦ï¼ä»¥å¢å¼ºåè¿°æ§å¨å±è§£éãè¿åè¿æ¥åæå¯¼äºæ¢å¼å¾ä¿¡èµåå¯æä½çå¥åº·ä¿¡æ¯å­¦ç³»ç»çè®¾è®¡ã

##### **Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**
2308.14321v1 by Yanjun Gao, Ruizhe Li, John Caskey, Dmitriy Dligach, Timothy Miller, Matthew M. Churpek, Majid Afshar

Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.

æè¦ï¼é»å­å¥åº·ç´é (EHR) åä¾è¡æä»¶è¨éå¯¦åå¨çæ£çæ¥å¸¸ç§è­·ä¸­æ®æ¼èè³ééè¦çè§è²ï¼æä¾å¥åº·ãè¨ºæ·åæ²»ççæ´é«ç´éãç¶èï¼è¤éä¸åé·ç EHR æè¿°æè®é«çä¿å¥æä¾èè¶è¼ï¼æè¨ºæ·ä¸æºç¢ºçé¢¨éªãå¤§åèªè¨æ¨¡å (LLM) å·²å±ç¾å¶å¨åç¨®èªè¨ä»»åä¸çæ½åï¼ä½å¶å¨é«çä¿å¥é åçæç¨éè¦ç¢ºä¿å°è¨ºæ·é¯èª¤éå°æä½ï¼ä¸¦é²æ­¢çæ£åå°å·å®³ãå¨æ¬æä¸­ï¼æåæ¦è¿°ä¸ç¨®åµæ°çæ¹æ³ï¼ééæ´åé«å­¸ç¥è­åè­ (KG) åä¸ç¨®æ°ç©çåè­æ¨¡åï¼Dr.Knowsï¼éæä¾èªè¨åºè¨ºæ·æ¨çéç¨ï¼ï¼ä¾å¢å¼· LLM å¨èªååè¨ºæ·ç¢çé åçè½åãæåå¾ç¾ååå®¶é«å­¸åæ¸é¤¨ççµ±ä¸é«å­¸èªè¨ç³»çµ± (UMLS) ä¸­è¡çåº KGï¼éæ¯ä¸åå¼·å¤§ççç©é«å­¸ç¥è­å²å­åº«ãæåçåæ³å¦å®äºé åè¨ç·´çéè¦ï¼èæ¯å° KG ä½çºè¼å©å·¥å·ï¼åå©è§£éåç¸½çµè¤éçé«å­¸æ¦å¿µãä½¿ç¨çå¯¦ä¸ççé«é¢è³æéï¼æåçå¯¦é©çµæè­æï¼å° LLM è KG çµåçå»ºè­°æ¹æ³ææ½åæé«èªååè¨ºæ·ç¢ççæºç¢ºæ§ãæ´éè¦çæ¯ï¼æåçåæ³æä¾äºä¸æ¢å¯è§£éçè¨ºæ·éå¾ï¼è®æåæ´æ¥è¿å¯¦ç¾ AI å¢å¼·çè¨ºæ·æ±ºç­æ¯æ´ç³»çµ±ã

##### **Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**
2308.09380v1 by Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai

Existing artificial intelligence (AI) models for diagnosing knee
osteoarthritis (OA) have faced criticism for their lack of transparency and
interpretability, despite achieving medical-expert-like performance. This
opacity makes them challenging to trust in clinical practice. Recently,
explainable artificial intelligence (XAI) has emerged as a specialized
technique that can provide confidence in the model's prediction by revealing
how the prediction is derived, thus promoting the use of AI systems in
healthcare. This paper presents the first survey of XAI techniques used for
knee OA diagnosis. The XAI techniques are discussed from two perspectives: data
interpretability and model interpretability. The aim of this paper is to
provide valuable insights into XAI's potential towards a more reliable knee OA
diagnosis approach and encourage its adoption in clinical practice.

æè¦ï¼ç¾æçç¨æ¼è¨ºæ·èéª¨éç¯ç (OA) çäººå·¥æºæ§ (AI) æ¨¡åå å¶ç¼ºä¹éæåº¦åå¯è§£éæ§èåå°æ¹è©ï¼åç®¡å®åéå°äºé¡ä¼¼é«å­¸å°å®¶çè¡¨ç¾ãéç¨®ä¸éææ§ä½¿å¾å®åå¨è¨åºå¯¦åä¸­é£ä»¥è¢«ä¿¡ä»»ãæè¿ï¼å¯è§£éäººå·¥æºæ§ (XAI) å·²æçºä¸ç¨®å°éæè¡ï¼å®è½ééæ­ç¤ºé æ¸¬çæ¨å°æ¹å¼ä¾æä¾å°æ¨¡åé æ¸¬çä¿¡å¿ï¼å¾èä¿é²å¨é«çä¿å¥ä¸­ä½¿ç¨ AI ç³»çµ±ãæ¬ææä¾äºéå°èéª¨éç¯çè¨ºæ·æä½¿ç¨ç XAI æè¡çç¬¬ä¸ä»½èª¿æ¥ãXAI æè¡å¾å©åè§åº¦é²è¡è¨è«ï¼è³æå¯è§£éæ§åæ¨¡åå¯è§£éæ§ãæ¬æçç®çæ¯æä¾å° XAI å¨æ´å¯é çèéª¨éç¯çè¨ºæ·æ¹æ³ä¸­çæ½åçå¯¶è²´è¦è§£ï¼ä¸¦é¼åµå¨è¨åºå¯¦åä¸­æ¡ç¨å®ã

##### **Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**
2308.08407v1 by Munib Mesinovic, Peter Watkinson, Tingting Zhu

Recent advancements in AI applications to healthcare have shown incredible
promise in surpassing human performance in diagnosis and disease prognosis.
With the increasing complexity of AI models, however, concerns regarding their
opacity, potential biases, and the need for interpretability. To ensure trust
and reliability in AI systems, especially in clinical risk prediction models,
explainability becomes crucial. Explainability is usually referred to as an AI
system's ability to provide a robust interpretation of its decision-making
logic or the decisions themselves to human stakeholders. In clinical risk
prediction, other aspects of explainability like fairness, bias, trust, and
transparency also represent important concepts beyond just interpretability. In
this review, we address the relationship between these concepts as they are
often used together or interchangeably. This review also discusses recent
progress in developing explainable models for clinical risk prediction,
highlighting the importance of quantitative and clinical evaluation and
validation across multiple common modalities in clinical practice. It
emphasizes the need for external validation and the combination of diverse
interpretability methods to enhance trust and fairness. Adopting rigorous
testing, such as using synthetic datasets with known generative factors, can
further improve the reliability of explainability methods. Open access and
code-sharing resources are essential for transparency and reproducibility,
enabling the growth and trustworthiness of explainable research. While
challenges exist, an end-to-end approach to explainability in clinical risk
prediction, incorporating stakeholders from clinicians to developers, is
essential for success.

æè¦ï¼æè¿å¨é«çä¿å¥ä¸­çäººå·¥æºæ§æç¨é²å±é¡¯ç¤ºåºä»¤äººé£ä»¥ç½®ä¿¡çæ¿è«¾ï¼å¨è¨ºæ·åç¾çé å¾æ¹é¢è¶è¶äººé¡è¡¨ç¾ãç¶èï¼é¨èäººå·¥æºè½æ¨¡åçæ¥çè¤éï¼äººåå°å¶ä¸éææ§ãæ½å¨åå·®åå°å¯è§£éæ§çéæ±æå°ææãçºäºç¢ºä¿äººå·¥æºè½ç³»çµ±çä¿¡ä»»åå¯é æ§ï¼å°¤å¶æ¯å¨è¨åºé¢¨éªé æ¸¬æ¨¡åä¸­ï¼å¯è§£éæ§è®å¾è³ééè¦ãå¯è§£éæ§éå¸¸è¢«ç¨±çºäººå·¥æºè½ç³»çµ±æä¾å¶æ±ºç­éè¼¯ææ±ºç­æ¬èº«å°äººé¡å©çç¸éèçå¼·æåè§£éçè½åãå¨è¨åºé¢¨éªé æ¸¬ä¸­ï¼å¯è§£éæ§çå¶ä»æ¹é¢ï¼å¦å¬å¹³æ§ãåè¦ãä¿¡ä»»åéæåº¦ï¼ä¹ä»£è¡¨äºè¶è¶å¯è§£éæ§çéè¦æ¦å¿µãå¨æ¬æ¬¡å¯©æ¥ä¸­ï¼æåæ¢è¨äºéäºæ¦å¿µä¹éçéä¿ï¼å çºå®åç¶å¸¸ä¸èµ·æäºæä½¿ç¨ãæ¬å¯©æ¥éè¨è«äºçºè¨åºé¢¨éªé æ¸¬éç¼å¯è§£éæ¨¡åçææ°é²å±ï¼å¼·èª¿äºå¨è¨åºå¯¦è¸ä¸­å°å¤ç¨®å¸¸è¦æ¨¡å¼é²è¡å®éåè¨åºè©ä¼°åé©è­çéè¦æ§ãå®å¼·èª¿äºå¤é¨é©è­åå¤æ¨£åå¯è§£éæ§æ¹æ³ç¸çµåçå¿è¦æ§ï¼ä»¥å¢å¼·ä¿¡ä»»åå¬å¹³æ§ãæ¡ç¨å´æ ¼çæ¸¬è©¦ï¼ä¾å¦ä½¿ç¨å·æå·²ç¥çæå ç´ çåææ¸æéï¼å¯ä»¥é²ä¸æ­¥æé«å¯è§£éæ§æ¹æ³çå¯é æ§ãéæ¾ç²ååä»£ç¢¼å±äº«è³æºå°æ¼éæåº¦åå¯éè¤æ§è³ééè¦ï¼å¾èä¿é²å¯è§£éç ç©¶çå¢é·åå¯ä¿¡åº¦ãåç®¡å­å¨ææ°ï¼ä½å¾è¨åºé«çå°éç¼äººå¡ï¼æ¡ç¨ç«¯å°ç«¯çå¯è§£éæ§æ¹æ³å°æ¼è¨åºé¢¨éªé æ¸¬çæåè³ééè¦ã

##### **FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**
2309.12325v3 by Karim Lekadir, Aasa Feragen, Abdul Joseph Fofanah, Alejandro F Frangi, Alena Buyx, Anais Emelie, Andrea Lara, Antonio R Porras, An-Wen Chan, Arcadi Navarro, Ben Glocker, Benard O Botwe, Bishesh Khanal, Brigit Beger, Carol C Wu, Celia Cintas, Curtis P Langlotz, Daniel Rueckert, Deogratias Mzurikwao, Dimitrios I Fotiadis, Doszhan Zhussupov, Enzo Ferrante, Erik Meijering, Eva Weicken, Fabio A GonzÃ¡lez, Folkert W Asselbergs, Fred Prior, Gabriel P Krestin, Gary Collins, Geletaw S Tegenaw, Georgios Kaissis, Gianluca Misuraca, Gianna Tsakou, Girish Dwivedi, Haridimos Kondylakis, Harsha Jayakody, Henry C Woodruf, Horst Joachim Mayer, Hugo JWL Aerts, Ian Walsh, Ioanna Chouvarda, IrÃ¨ne Buvat, Isabell Tributsch, Islem Rekik, James Duncan, Jayashree Kalpathy-Cramer, Jihad Zahir, Jinah Park, John Mongan, Judy W Gichoya, Julia A Schnabel, Kaisar Kushibar, Katrine Riklund, Kensaku Mori, Kostas Marias, Lameck M Amugongo, Lauren A Fromont, Lena Maier-Hein, Leonor CerdÃ¡ Alberich, Leticia Rittner, Lighton Phiri, Linda Marrakchi-Kacem, LluÃ­s Donoso-Bach, Luis MartÃ­-BonmatÃ­, M Jorge Cardoso, Maciej Bobowicz, Mahsa Shabani, Manolis Tsiknakis, Maria A Zuluaga, Maria Bielikova, Marie-Christine Fritzsche, Marina Camacho, Marius George Linguraru, Markus Wenzel, Marleen De Bruijne, Martin G Tolsgaard, Marzyeh Ghassemi, Md Ashrafuzzaman, Melanie Goisauf, Mohammad Yaqub, MÃ³nica Cano AbadÃ­a, Mukhtar M E Mahmoud, Mustafa Elattar, Nicola Rieke, Nikolaos Papanikolaou, Noussair Lazrak, Oliver DÃ­az, Olivier Salvado, Oriol Pujol, Ousmane Sall, Pamela Guevara, Peter Gordebeke, Philippe Lambin, Pieta Brown, Purang Abolmaesumi, Qi Dou, Qinghua Lu, Richard Osuala, Rose Nakasi, S Kevin Zhou, Sandy Napel, Sara Colantonio, Shadi Albarqouni, Smriti Joshi, Stacy Carter, Stefan Klein, Steffen E Petersen, Susanna AussÃ³, Suyash Awate, Tammy Riklin Raviv, Tessa Cook, Tinashe E M Mutsvangwa, Wendy A Rogers, Wiro J Niessen, XÃ¨nia Puig-Bosch, Yi Zeng, Yunusa G Mohammed, Yves Saint James Aquino, Zohaib Salahuddin, Martijn P A Starmans

Despite major advances in artificial intelligence (AI) for medicine and
healthcare, the deployment and adoption of AI technologies remain limited in
real-world clinical practice. In recent years, concerns have been raised about
the technical, clinical, ethical and legal risks associated with medical AI. To
increase real world adoption, it is essential that medical AI tools are trusted
and accepted by patients, clinicians, health organisations and authorities.
This work describes the FUTURE-AI guideline as the first international
consensus framework for guiding the development and deployment of trustworthy
AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and
currently comprises 118 inter-disciplinary experts from 51 countries
representing all continents, including AI scientists, clinicians, ethicists,
and social scientists. Over a two-year period, the consortium defined guiding
principles and best practices for trustworthy AI through an iterative process
comprising an in-depth literature review, a modified Delphi survey, and online
consensus meetings. The FUTURE-AI framework was established based on 6 guiding
principles for trustworthy AI in healthcare, i.e. Fairness, Universality,
Traceability, Usability, Robustness and Explainability. Through consensus, a
set of 28 best practices were defined, addressing technical, clinical, legal
and socio-ethical dimensions. The recommendations cover the entire lifecycle of
medical AI, from design, development and validation to regulation, deployment,
and monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which
provides a structured approach for constructing medical AI tools that will be
trusted, deployed and adopted in real-world practice. Researchers are
encouraged to take the recommendations into account in proof-of-concept stages
to facilitate future translation towards clinical practice of medical AI.

æè¦ï¼åç®¡å¨é«å­¸åé«çä¿å¥æ¹é¢çäººå·¥æºæ§ (AI) æéå¤§çé²å±ï¼ä½ AI æè¡çé¨ç½²åæ¡ç¨å¨ç¾å¯¦ä¸ççè¨åºå¯¦åä¸­ä»ç¶æéãè¿å¹´ä¾ï¼äººåå°æ¼èé«ç AI ç¸éçæè¡ãè¨åºãå«çåæ³å¾é¢¨éªæåºäºçæ®ãçºäºå¢å ç¾å¯¦ä¸ççæ¡ç¨çï¼é«ç AI å·¥å·å¿é ç²å¾æ£èãè¨åºé«çãé«çæ©æ§åç¶å±çä¿¡ä»»åæ¥åãéé å·¥ä½å° FUTURE-AI æåæè¿°çºæå°é«çä¿å¥ä¸­å¯ä¿¡è³´ AI å·¥å·éç¼åé¨ç½²çç¬¬ä¸ååéå±è­æ¶æ§ãFUTURE-AI è¯çæç«æ¼ 2021 å¹´ï¼ç®åç±ä¾èª 51 ååå®¶ç 118 ä½è·¨é åå°å®¶çµæï¼ä»£è¡¨æææ´²ï¼åæ¬ AI ç§å­¸å®¶ãè¨åºé«çãå«çå­¸å®¶åç¤¾æç§å­¸å®¶ãå¨å©å¹´çæéè£¡ï¼è©²è¯çééä¸ååè¦éç®çéç¨å®ç¾©äºå¯ä¿¡è³´ AI çæå°åååæä½³å¯¦åï¼åæ¬æ·±å¥çæç»åé¡§ãä¿®æ¹å¾çå¾·ç¾è²èª¿æ¥åç·ä¸å±è­æè­°ãFUTURE-AI æ¶æ§æ¯åºæ¼é«çä¿å¥ä¸­å¯ä¿¡è³´ AI ç 6 é æå°ååå»ºç«çï¼å³å¬å¹³æ§ãæ®éæ§ãå¯è¿½æº¯æ§ãå¯ç¨æ§ãç©©å¥æ§åå¯è§£éæ§ãééå±è­ï¼å®ç¾©äºä¸çµ 28 é æä½³å¯¦åï¼æ¶µèæè¡ãè¨åºãæ³å¾åç¤¾æå«çå±¤é¢ãå»ºè­°æ¶µèäºé«ç AI çæ´åçå½é±æï¼å¾è¨­è¨ãéç¼åé©è­å°æ³è¦ãé¨ç½²åç£æ§ãFUTURE-AI æ¯ä¸ååºæ¼é¢¨éªãç¡åè¨­çæåï¼æä¾äºä¸åçµæ§åçæ¹æ³ï¼ç¨æ¼å»ºæ§å°å¨ç¾å¯¦ä¸çå¯¦åä¸­åå°ä¿¡ä»»ãé¨ç½²åæ¡ç¨çé«ç AI å·¥å·ãé¼åµç ç©¶äººå¡å¨æ¦å¿µé©è­éæ®µèæ®éäºå»ºè­°ï¼ä»¥ä¿é²æªä¾å°é«ç AI è½åçºè¨åºå¯¦åã

##### **Explainable AI applications in the Medical Domain: a systematic review**
2308.05411v1 by Nicoletta Prentzas, Antonis Kakas, Constantinos S. Pattichis

Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.

æè¦ï¼äººå·¥æºæ§å¨é«çé åä¸­å·²åå¾é¡¯èé²å±ï¼å¨é«å­¸å½±åãçäººç§è­·åå¶ä»é åä¸­åºç¾äºæ°èæç¨ãéç¶éäºæç¨å·²å¨åé¡§æ§ç ç©¶ä¸­è¢«è­å¯¦æ¯æåçï¼ä½å¯¦éä¸åªææ¥µå°æ¸æç¨æ¼å¯¦åãé«ç AI é åé¢è¨èåç¨®ææ°ï¼åæ¬å»ºç«ä½¿ç¨èä¿¡ä»»ãéµå®æ³è¦ãä½¿ç¨è³æç¬¦åå«çãå¯è§£é AI (XAI) çç®æ¨æ¯è®äººé¡äºè§£ AI ä¸¦ç¸ä¿¡å¶çµæãæ¬æéå°æè¿å¹¾å¹´ç¼è¡¨ç 198 ç¯æç« çå·ä»£è¡¨æ§æ¨£æ¬ï¼æåºæéé«çæ±ºç­æ¯æ´ç XAI è§£æ±ºæ¹æ¡çææ°ç¼å±çæç»åé¡§ãç¸éæç« çç³»çµ±æ§ç¶åæ´çç¢çäºå¤é ç¼ç¾ï¼(1) éäºè§£æ±ºæ¹æ¡å¤§å¤æ¡ç¨èæ¨¡åç¡éç XAI æè¡ï¼(2) æ·±åº¦å­¸ç¿æ¨¡åçä½¿ç¨çé«æ¼å¶ä»é¡åçæ©å¨å­¸ç¿æ¨¡åï¼(3) å¯è§£éæ§è¢«ç¨æ¼ä¿é²ä¿¡ä»»ï¼ä½å¾å°æç ç©¶å ±åé«å¸«åèè¿´åï¼(4) è¦è¦ºåäºåå¼ä½¿ç¨èä»é¢å°æ¼çè§£ç³»çµ±çè§£éåå»ºè­°æ´æç¨ãéè¦æ´å¤é«çå AI å°å®¶åä½é²è¡ç ç©¶ï¼éæå©æ¼çºé«çé åç XAI è§£æ±ºæ¹æ¡çè¨­è¨ãå¯¦ä½åè©ä¼°æä¾é©ç¶æ¶æ§ã

##### **Exploring the Role of Explainability in AI-Assisted Embryo Selection**
2308.02534v1 by Lucia Urcelay, Daniel Hinjos, Pablo A. Martin-Torres, Marta Gonzalez, Marta Mendez, Salva CÃ­vico, Sergio Ãlvarez-Napagao, Dario Garcia-Gasulla

In Vitro Fertilization is among the most widespread treatments for
infertility. One of its main challenges is the evaluation and selection of
embryo for implantation, a process with large inter- and intra-clinician
variability. Deep learning based methods are gaining attention, but their
opaque nature compromises their acceptance in the clinical context, where
transparency in the decision making is key. In this paper we analyze the
current work in the explainability of AI-assisted embryo analysis models,
identifying the limitations. We also discuss how these models could be
integrated in the clinical context as decision support systems, considering the
needs of clinicians and patients. Finally, we propose guidelines for the sake
of increasing interpretability and trustworthiness, pushing this technology
forward towards established clinical practice.

æè¦ï¼é«å¤åç²¾æ¯æ²»çä¸å­çæå»£æ³çæ¹æ³ä¹ä¸ãå¶ä¸»è¦ææ°ä¹ä¸æ¯è©ä¼°åé¸æèèé²è¡æ¤å¥ï¼æ­¤éç¨å·æå¾å¤§çè¨åºéåè¨åºå§è®ç°æ§ãåºæ¼æ·±åº¦å­¸ç¿çæ¹æ³æ­£åå°éæ³¨ï¼ä½å¶ä¸éæçæ§è³ªæå½±é¿å¶å¨è¨åºç°å¢ä¸­çæ¥ååº¦ï¼èéæåº¦å¨æ±ºç­å¶å®ä¸­è³ééè¦ãå¨æ¬æä¸­ï¼æååæäº AI è¼å©èèåææ¨¡åçå¯è§£éæ§æ¹é¢çç¾æå·¥ä½ï¼ä¸¦æ¾åºå¶å±éæ§ãæåéè¨è«äºå¦ä½å°éäºæ¨¡åä½çºæ±ºç­æ¯æç³»çµ±æ´åå°è¨åºç°å¢ä¸­ï¼åæèæ®è¨åºé«çåæ£èçéæ±ãæå¾ï¼æåæåºäºæé«å¯è§£éæ§åå¯ä¿¡åº¦çæºåï¼æ¨é²éé æè¡æèæ¢å®çè¨åºå¯¦åéé²ã

##### **A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**
2307.14246v1 by Timo Speith, Markus Langer

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

æè¦ï¼å¨éæ±å·¥ç¨ (RE) é åä¸­ï¼å¯è§£éäººå·¥æºæ§ (XAI) å¨å° AI æ¯æçç³»çµ±èä½¿ç¨èéæ±ãç¤¾æææåæ³è¦æ¨æºç¸ç¬¦æ¹é¢çéè¦æ§æ¥çé¡¯èï¼å·²ç²å¾èªå¯ãä¸è¬ä¾èªªï¼å¯è§£éæ§å·²æçºå½±é¿ç³»çµ±åè³ªçéè¦éåè½éæ±ãç¶èï¼å¯è§£éæ§èæè½ä¹éçåå®æ¬è¡¡ææ°äºå¯è§£éæ§çåå®æ­£é¢å½±é¿ãå¦ææ»¿è¶³å¯è§£éæ§çéæ±éè¦éä½ç³»çµ±æè½ï¼é£éº¼å¿é ä»ç´°èæ®éäºåè³ªé¢åä¸­åªä¸ååªåï¼ä»¥åå¦ä½å¨å®åä¹éé²è¡æè¡·ãå¨æ¬æä¸­ï¼æåæ¹å¤æ§å°æ¢è¨äºéç¨®åå®çæ¬è¡¡ãæåèªçºï¼æå¥½çæ¹æ³æ¯ä»¥ä¸ç¨®ç´°ç·»çæ¹å¼ä¾èçï¼éç¨®æ¹å¼åå«è³æºå¯ç¨æ§ãé åç¹æ§åé¢¨éªèéãééæä¾æªä¾ç ç©¶åæä½³å¯¦åçåºç¤ï¼éé å·¥ä½æ¨å¨æå AI ç RE é åã

##### **Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**
2307.14239v1 by Barnaby Crook, Maximilian SchlÃ¼ter, Timo Speith

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

æè¦ï¼å¨éæ±å·¥ç¨ï¼REï¼é¢åï¼å¯è§£éäººå·¥æºè½ï¼XAIï¼å¨å°äººå·¥æºè½æ¯æçç³»ç»ä¸ç¨æ·éæ±ãç¤¾ä¼ææåçç®¡æ åç¸ä¸è´æ¹é¢çéè¦æ§æ¥çå¸æ¾ï¼å¹¶è·å¾äºè®¤å¯ãä¸è¬æ¥è¯´ï¼å¯è§£éæ§å·²æä¸ºå½±åç³»ç»è´¨éçéè¦éåè½æ§éæ±ãç¶èï¼å¯è§£éæ§åæ§è½ä¹é´çæè¡¡ææäºå¯è§£éæ§çæ­£é¢å½±åãå¦ææ»¡è¶³å¯è§£éæ§çè¦æ±éè¦éä½ç³»ç»æ§è½ï¼é£ä¹å¿é¡»ä»ç»èèè¿äºè´¨éæ¹é¢ä¸­çåªä¸ä¸ªä¼åï¼ä»¥åå¦ä½å¨å®ä»¬ä¹é´è¿è¡æè¡¡ãå¨æ¬æä¸­ï¼æä»¬æ¹å¤æ§å°èå¯äºæè°çæè¡¡ãæä»¬è®¤ä¸ºï¼æå¥½ä»¥ä¸ç§ç»è´å¥å¾®çæ¹å¼æ¥å¤çå®ï¼è¿ç§æ¹å¼ç»åäºèµæºå¯ç¨æ§ãé¢åç¹å¾åé£é©èèãéè¿ä¸ºæªæ¥çç ç©¶åæä½³å®è·µæä¾åºç¡ï¼è¿é¡¹å·¥ä½æ¨å¨æ¨è¿äººå·¥æºè½ç RE é¢åã

##### **Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**
2308.02047v1 by Henry Fraser, Jose-Miguel Bello y Villarino

This paper critically evaluates the European Commission's proposed AI Act's
approach to risk management and risk acceptability for high-risk AI systems
that pose risks to fundamental rights and safety. The Act aims to promote
"trustworthy" AI with a proportionate regulatory burden. Its provisions on risk
acceptability require residual risks from high-risk systems to be reduced or
eliminated "as far as possible", having regard to the "state of the art". This
criterion, especially if interpreted narrowly, is unworkable and promotes
neither proportionate regulatory burden, nor trustworthiness. By contrast the
Parliament's most recent draft amendments to the risk management provisions
introduce "reasonableness", cost-benefit analysis, and are more transparent
about the value-laden and contextual nature of risk acceptability judgements.
This paper argues that the Parliament's approach is more workable, and better
balances the goals of proportionality and trustworthiness. It explains what
reasonableness in risk acceptability judgments would entail, drawing on
principles from negligence law and European medical devices regulation. And it
contends that the approach to risk acceptability judgments need a firm
foundation of civic legitimacy: including detailed guidance or involvement from
regulators, and meaningful input from affected stakeholders.

æè¦ï¼æ¬æå´æ ¼è©ä¼°æ­æ´²å§å¡ææåºç AI æ³æ¡å°é¢¨éªç®¡çåé¢¨éªå¯æ¥åæ§çæ¹æ³ï¼ç¨æ¼å°åºæ¬æ¬å©åå®å¨æ§æé¢¨éªçé«é¢¨éª AI ç³»çµ±ãè©²æ³æ¡æ¨å¨ä»¥ç¸ç¨±çç£ç®¡è² æä¿é²ãå¼å¾ä¿¡è³´ãç AIãå¶éæ¼é¢¨éªå¯æ¥åæ§çæ¢æ¬¾è¦æ±å°é«é¢¨éªç³»çµ±çæ®é¤é¢¨éªæ¸ä½ææ¶é¤ãç¡å¯è½ãï¼ä¸¦èæ®ãæè¡çæããæ­¤æºåï¼ç¹å¥æ¯å¦æç¹ç¾©è§£éï¼ç¡æ³å·è¡ï¼æ¢ä¸ä¿é²ç¸ç¨±çç£ç®¡è² æï¼ä¹ä¸ä¿é²å¯ä¿¡è³´æ§ãç¸æ¯ä¹ä¸ï¼è­°æå°é¢¨éªç®¡çæ¢æ¬¾çææ°ä¿®æ­£èæ¡å¼å¥äºãåçæ§ããææ¬æçåæï¼ä¸¦ä¸æ´éæå°èªªæäºé¢¨éªå¯æ¥åæ§å¤æ·çå¹å¼è§åèæ¯æ§è³ªãæ¬æè«è­è­°æçæ¹æ³æ´å¯è¡ï¼ä¸è½æ´å¥½å°å¹³è¡¡ç¸ç¨±æ§åå¯ä¿¡è³´æ§çç®æ¨ãæ¬æèªªæé¢¨éªå¯æ¥åæ§å¤æ·ä¸­çåçæ§æå¸¶ä¾ä»éº¼ï¼ä¸¦æ ¹æéå¤±æ³åæ­æ´²é«çå¨ææ³è¦ä¸­çååé²è¡èªªæãæ¬æä¸»å¼µé¢¨éªå¯æ¥åæ§å¤æ·çæ¹æ³éè¦ç©©åºçå¬æ°åæ³æ§åºç¤ï¼åæ¬ç£ç®¡æ©æ§çè©³ç´°æå°æåèï¼ä»¥ååå½±é¿å©å®³éä¿äººçææç¾©æå¥ã

##### **eXplainable Artificial Intelligence (XAI) in aging clock models**
2307.13704v3 by Alena Kalyakulina, Igor Yusipov, Alexey Moskalev, Claudio Franceschi, Mikhail Ivanchenko

eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the "aging clocks" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems.

æè¦ï¼å¯è§£éäººå·¥æºæ§ (XAI) æ¯æ©å¨å­¸ç¿ä¸­å¿«éé²å±çé åï¼æ¨å¨è§£éè¤éæ¨¡åçé æ¸¬ãXAI å¨æææç¨ä¸­ç¹å¥éè¦ï¼ä¾å¦å¨é«çä¿å¥ä¸­ï¼ç¶è¨ºæ·ãå»ºè­°åæ²»çé¸æå¯è½ä¾è³´æ¼äººå·¥æºæ§ç³»çµ±ååºçæ±ºç­æãäººå·¥æºæ§æ¹æ³ä¹å·²å»£æ³ç¨æ¼èåç ç©¶ï¼ç¹å¥æ¯å¨éç¼çç©æéæ¨¡ååè­å¥èååèå¹´é½¡ç¸éç¾çççç©æ¨èªç©æ¹é¢ãç¶èï¼éè£¡ XAI çæ½åæå¾ååèªè­ãæåè¨è«äº XAI å¨éç¼ãèåæéãæ¹é¢çæç¨ï¼ä¸¦å°æç¹å®ççç³»çµ±çéé»åé¡çæç»é²è¡äºå¨é¢çåæã

##### **Interpreting and Correcting Medical Image Classification with PIP-Net**
2307.10404v2 by Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, JÃ¶rg SchlÃ¶tterer, Maurice van Keulen, Christin Seifert

Part-prototype models are explainable-by-design image classifiers, and a
promising alternative to black box AI. This paper explores the applicability
and potential of interpretable machine learning, in particular PIP-Net, for
automated diagnosis support on real-world medical imaging data. PIP-Net learns
human-understandable prototypical image parts and we evaluate its accuracy and
interpretability for fracture detection and skin cancer diagnosis. We find that
PIP-Net's decision making process is in line with medical classification
standards, while only provided with image-level class labels. Because of
PIP-Net's unsupervised pretraining of prototypes, data quality problems such as
undesired text in an X-ray or labelling errors can be easily identified.
Additionally, we are the first to show that humans can manually correct the
reasoning of PIP-Net by directly disabling undesired prototypes. We conclude
that part-prototype models are promising for medical applications due to their
interpretability and potential for advanced model debugging.

æè¦ï¼é¨åååæ¨¡åæ¯å¯è§£éè¨­è¨çå½±ååé¡å¨ï¼ä¹æ¯é»ç®± AI çä¸åæåéçæ¿ä»£æ¹æ¡ãéç¯è«ææ¢è¨äºè§£éæ§æ©å¨å­¸ç¿ï¼ç¹å¥æ¯ PIP-Netï¼å¨çå¯¦ä¸çé«å­¸å½±åè³æä¸èªååè¨ºæ·æ¯æ´çé©ç¨æ§åæ½åãPIP-Net å­¸ç¿äººé¡å¯çè§£çååå½±åé¨åï¼æåè©ä¼°å¶å¨éª¨ææª¢æ¸¬åç®èçè¨ºæ·æ¹é¢çæºç¢ºæ§åå¯è§£éæ§ãæåç¼ç¾ PIP-Net çæ±ºç­å¶å®éç¨ç¬¦åé«å­¸åé¡æ¨æºï¼åæåæä¾å½±åå±¤ç´é¡å¥æ¨ç±¤ãç±æ¼ PIP-Net å°ååçç¡ç£ç£é è¨ç·´ï¼å æ­¤å¯ä»¥è¼é¬è­å¥è³æåè³ªåé¡ï¼ä¾å¦ X åä¸­çä¸éè¦æå­ææ¨ç±¤é¯èª¤ãæ­¤å¤ï¼æåé¦æ¬¡å±ç¤ºäººé¡å¯ä»¥ééç´æ¥åç¨ä¸éè¦çååä¾æåä¿®æ­£ PIP-Net çæ¨çãæåå¾åºçµè«ï¼é¨åååæ¨¡åç±æ¼å¶å¯è§£éæ§åé²éæ¨¡åé¤é¯çæ½åï¼å æ­¤æææç¨æ¼é«çã

##### **Explaining and visualizing black-box models through counterfactual paths**
2307.07764v3 by Bastian Pfeifer, Mateusz Krzyzinski, Hubert Baniecki, Anna Saranti, Andreas Holzinger, Przemyslaw Biecek

Explainable AI (XAI) is an increasingly important area of machine learning
research, which aims to make black-box models transparent and interpretable. In
this paper, we propose a novel approach to XAI that uses the so-called
counterfactual paths generated by conditional permutations of features. The
algorithm measures feature importance by identifying sequential permutations of
features that most influence changes in model predictions. It is particularly
suitable for generating explanations based on counterfactual paths in knowledge
graphs incorporating domain knowledge. Counterfactual paths introduce an
additional graph dimension to current XAI methods in both explaining and
visualizing black-box models. Experiments with synthetic and medical data
demonstrate the practical applicability of our approach.

æè¦ï¼å¯è§£é AI (XAI) æ¯æ©å¨å­¸ç¿ç ç©¶ä¸­æ¥çéè¦çé åï¼å¶ç®æ¨æ¯è®é»ç®±æ¨¡åéæä¸å¯è§£éãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°ç XAI æ¹æ³ï¼è©²æ¹æ³ä½¿ç¨ç±ç¹å¾µæ¢ä»¶ç½®æç¢ççæè¬åäºå¯¦è·¯å¾ãè©²æ¼ç®æ³ééè­å¥ç¹å¾µçé åºç½®æä¾è¡¡éç¹å¾µéè¦æ§ï¼éäºç½®ææè½å½±é¿æ¨¡åé æ¸¬çè®åãå®ç¹å¥é©åæ ¹æåå«é åç¥è­çç¥è­åè­ä¸­çåäºå¯¦è·¯å¾ä¾ç¢çè§£éãåäºå¯¦è·¯å¾å¨è§£éåè¦è¦ºåé»ç®±æ¨¡åæï¼çºç®åç XAI æ¹æ³å¼å¥äºé¡å¤çåå½¢ç¶­åº¦ãä½¿ç¨åæåé«çè³æé²è¡çå¯¦é©è­æäºæåæ¹æ³çå¯¦ç¨é©ç¨æ§ã

##### **Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**
2307.02131v5 by Toygar Tanyel, Serkan Ayvaz, Bilgin Keserci

The field of explainability in artificial intelligence (AI) has witnessed a
growing number of studies and increasing scholarly interest. However, the lack
of human-friendly and individual interpretations in explaining the outcomes of
machine learning algorithms has significantly hindered the acceptance of these
methods by clinicians in their research and clinical practice. To address this
issue, our study uses counterfactual explanations to explore the applicability
of "what if?" scenarios in medical research. Our aim is to expand our
understanding of magnetic resonance imaging (MRI) features used for diagnosing
pediatric posterior fossa brain tumors beyond existing boundaries. In our case
study, the proposed concept provides a novel way to examine alternative
decision-making scenarios that offer personalized and context-specific
insights, enabling the validation of predictions and clarification of
variations under diverse circumstances. Additionally, we explore the potential
use of counterfactuals for data augmentation and evaluate their feasibility as
an alternative approach in our medical research case. The results demonstrate
the promising potential of using counterfactual explanations to enhance
acceptance of AI-driven methods in clinical research.

æè¦ï¼å¨äººå·¥æºè½ (AI) çå¯è§£éæ§é åä¸­ï¼å·²ç¶çå°è¶ä¾è¶å¤çç ç©¶åå­¸è¡èè¶£ãç¶èï¼å¨è§£éæ©å¨å­¸ç¿æ¼ç®æ³ççµææç¼ºä¹äººæ§åååäººåçè©®éï¼éé¡¯èé»ç¤äºè¨åºé«çå¨ç ç©¶åè¨åºå¯¦åä¸­æ¥åéäºæ¹æ³ãçºäºè§£æ±ºéååé¡ï¼æåçç ç©¶ä½¿ç¨åäºå¯¦è§£éä¾æ¢è¨ãå¦æï¼ãæå¢å¨é«å­¸ç ç©¶ä¸­çé©ç¨æ§ãæåçç®æ¨æ¯æ´å±æåå°ç¨æ¼è¨ºæ·å°åå¾é¡±çª©è¦è«ç¤çç£å±æ¯æå (MRI) ç¹å¾µççè§£ï¼è¶è¶ç¾æççç·ãå¨æåçæ¡ä¾ç ç©¶ä¸­ï¼ææåºçæ¦å¿µæä¾äºä¸ç¨®æ°ç©çæ¹æ³ä¾æª¢è¦æ¿ä»£æ±ºç­æå¢ï¼æä¾åäººååç¹å®æ¼æå¢çè¦è§£ï¼å¾èè½å¤ é©è­é æ¸¬ä¸¦éæ¸å¨ä¸åææ³ä¸çå·®ç°ãæ­¤å¤ï¼æåæ¢è¨äºåäºå¯¦ç¨æ¼è³ææ´åçæ½å¨ç¨éï¼ä¸¦è©ä¼°å¶ä½çºæåé«å­¸ç ç©¶æ¡ä¾ä¸­æ¿ä»£æ¹æ³çå¯è¡æ§ãçµæè­æäºä½¿ç¨åäºå¯¦è§£éä¾å¢å¼·è¨åºç ç©¶ä¸­ AI é©åæ¹æ³çæ¥ååº¦çæ½åã

##### **AI and Non AI Assessments for Dementia**
2307.01210v1 by Mahboobeh Parsapoor, Hamed Ghodrati, Vincenzo Dentamaro, Christopher R. Madan, Ioulietta Lazarou, Spiros Nikolopoulos, Ioannis Kompatsiaris

Current progress in the artificial intelligence domain has led to the
development of various types of AI-powered dementia assessments, which can be
employed to identify patients at the early stage of dementia. It can
revolutionize the dementia care settings. It is essential that the medical
community be aware of various AI assessments and choose them considering their
degrees of validity, efficiency, practicality, reliability, and accuracy
concerning the early identification of patients with dementia (PwD). On the
other hand, AI developers should be informed about various non-AI assessments
as well as recently developed AI assessments. Thus, this paper, which can be
readable by both clinicians and AI engineers, fills the gap in the literature
in explaining the existing solutions for the recognition of dementia to
clinicians, as well as the techniques used and the most widespread dementia
datasets to AI engineers. It follows a review of papers on AI and non-AI
assessments for dementia to provide valuable information about various dementia
assessments for both the AI and medical communities. The discussion and
conclusion highlight the most prominent research directions and the maturity of
existing solutions.

æè¦ï¼ç®åäººå·¥æºè½é åçé²å±å°è´äºåç¨®é¡åçäººå·¥æºæ§é©åçå¤±æºçè©ä¼°çç¼å±ï¼å¯ç¨æ¼è­å¥èæ¼å¤±æºçæ©æéæ®µçæ£èãå®å¯ä»¥å¾¹åºæ¹è®å¤±æºçè­·çè¨­ç½®ãéè¦çæ¯ï¼é«ççè¦äºè§£åç¨®äººå·¥æºè½è©ä¼°ï¼ä¸¦æ ¹æå¶æææ§ãæçãå¯¦ç¨æ§ãå¯é æ§åæºç¢ºæ§ç¨åº¦ï¼èæ®é¸æå®åä¾æ©æè­å¥å¤±æºçæ£è (PwD)ãå¦ä¸æ¹é¢ï¼äººå·¥æºè½éç¼äººå¡ä¹æè©²äºè§£åç¨®éäººå·¥æºè½è©ä¼°ä»¥åæè¿éç¼çäººå·¥æºè½è©ä¼°ãå æ­¤ï¼éç¯è¨åºé«çåäººå·¥æºè½å·¥ç¨å¸«é½å¯ä»¥é±è®çè«æå¡«è£äºæç»ä¸­éæ¼åè¨åºé«çè§£éç¾æå¤±æºçè­å¥è§£æ±ºæ¹æ¡ä»¥ååäººå·¥æºè½å·¥ç¨å¸«è§£éæç¨æè¡åæå»£æ³çå¤±æºçæ¸æéçç©ºç½ãå®éµå¾ªå°äººå·¥æºè½åéäººå·¥æºè½å¤±æºçè©ä¼°è«æçåé¡§ï¼çºäººå·¥æºè½åé«ççæä¾æéåç¨®å¤±æºçè©ä¼°çå¯¶è²´ä¿¡æ¯ãè¨è«åçµè«éé»ä»ç´¹äºæçªåºçç ç©¶æ¹ååç¾æè§£æ±ºæ¹æ¡çæçåº¦ã

##### **Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**
2306.07306v1 by Ruitao Xie, Jingbang Chen, Limai Jiang, Rui Xiao, Yi Pan, Yunpeng Cai

Explainability poses a major challenge to artificial intelligence (AI)
techniques. Current studies on explainable AI (XAI) lack the efficiency of
extracting global knowledge about the learning task, thus suffer deficiencies
such as imprecise saliency, context-aware absence and vague meaning. In this
paper, we propose the class association embedding (CAE) approach to address
these issues. We employ an encoder-decoder architecture to embed sample
features and separate them into class-related and individual-related style
vectors simultaneously. Recombining the individual-style code of a given sample
with the class-style code of another leads to a synthetic sample with preserved
individual characters but changed class assignment, following a cyclic
adversarial learning strategy. Class association embedding distills the global
class-related features of all instances into a unified domain with well
separation between classes. The transition rules between different classes can
be then extracted and further employed to individual instances. We then propose
an active XAI framework which manipulates the class-style vector of a certain
sample along guided paths towards the counter-classes, resulting in a series of
counter-example synthetic samples with identical individual characters.
Comparing these counterfactual samples with the original ones provides a
global, intuitive illustration to the nature of the classification tasks. We
adopt the framework on medical image classification tasks, which show that more
precise saliency maps with powerful context-aware representation can be
achieved compared with existing methods. Moreover, the disease pathology can be
directly visualized via traversing the paths in the class-style space.

æè¦ï¼<paragraph>å¯è§£éæ§å°äººå·¥æºæ§ (AI) æè¡æ§æä¸é éå¤§ææ°ãç¶åå°å¯è§£é AI (XAI) çç ç©¶ç¼ºä¹æåå­¸ç¿ä»»åæ´é«ç¥è­çæçï¼å æ­¤å­å¨ä¸ç²¾ç¢ºçé¡¯èæ§ãèæå¢ç¡éçç¼ºå¤±åå«ç³æç¾©ç­ç¼ºé·ãå¨æ¬æä¸­ï¼æåæåºé¡å¥éè¯åµå¥ (CAE) æ¹æ³ä¾è§£æ±ºéäºåé¡ãæåæ¡ç¨ç·¨ç¢¼å¨-è§£ç¢¼å¨æ¶æ§ä¾åµå¥æ¨£æ¬ç¹å¾µï¼ä¸¦åæå°å®ååçºé¡å¥ç¸éååé«ç¸éçæ¨£å¼åéãå°çµ¦å®æ¨£æ¬çåé«æ¨£å¼ä»£ç¢¼èå¦ä¸åæ¨£æ¬çé¡å¥æ¨£å¼ä»£ç¢¼éæ°çµåï¼æç¢çä¸åå·æä¿çåé«ç¹å¾µä½æ¹è®é¡å¥åéçåææ¨£æ¬ï¼éµå¾ªå¾ªç°å°æå­¸ç¿ç­ç¥ãé¡å¥éè¯åµå¥å°ææå¯¦ä¾çå¨å±é¡å¥ç¸éç¹å¾µæçå°ä¸åçµ±ä¸çé åä¸­ï¼ä¸¦å¨é¡å¥ä¹éæè¯å¥½çååãç¶å¾å¯ä»¥æåä¸åé¡å¥ä¹éçè½æè¦åï¼ä¸¦é²ä¸æ­¥æç¨æ¼åå¥å¯¦ä¾ãç¶å¾ï¼æåæåºä¸åä¸»å XAI æ¡æ¶ï¼å®æ²¿èå¼å°è·¯å¾æä½ç¹å®æ¨£æ¬çé¡å¥æ¨£å¼åéï¼æèåé¡å¥ç§»åï¼å¾èç¢çä¸ç³»åå·æç¸ååé«ç¹å¾µçåä¾åææ¨£æ¬ãå°éäºåäºå¯¦æ¨£æ¬èåå§æ¨£æ¬é²è¡æ¯è¼ï¼å¯ä»¥å°åé¡ä»»åçæ§è³ªæä¾å¨å±ãç´è§çèªªæãæåæ¡ç¨è©²æ¡æ¶é²è¡é«å­¸å½±ååé¡ä»»åï¼çµæè¡¨æï¼èç¾ææ¹æ³ç¸æ¯ï¼å¯ä»¥ç²å¾æ´ç²¾ç¢ºçé¡¯èæ§åï¼ä¸¦å·æå¼·å¤§çèæå¢ç¡éçè¡¨ç¤ºãæ­¤å¤ï¼ç¾çççå­¸å¯ä»¥ç´æ¥ééå¨é¡å¥æ¨£å¼ç©ºéä¸­éæ­·è·¯å¾ä¾é²è¡å¯è¦åã</paragraph>

##### **HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**
2306.06029v1 by Rodrigo Agerri, IÃ±igo Alonso, Aitziber Atutxa, Ander Berrondo, Ainara Estarrona, Iker Garcia-Ferrero, Iakes Goenaga, Koldo Gojenola, Maite Oronoz, Igor Perez-Tejedor, German Rigau, Anar Yeginbergenova

Providing high quality explanations for AI predictions based on machine
learning is a challenging and complex task. To work well it requires, among
other factors: selecting a proper level of generality/specificity of the
explanation; considering assumptions about the familiarity of the explanation
beneficiary with the AI task under consideration; referring to specific
elements that have contributed to the decision; making use of additional
knowledge (e.g. expert evidence) which might not be part of the prediction
process; and providing evidence supporting negative hypothesis. Finally, the
system needs to formulate the explanation in a clearly interpretable, and
possibly convincing, way. Given these considerations, ANTIDOTE fosters an
integrated vision of explainable AI, where low-level characteristics of the
deep learning process are combined with higher level schemes proper of the
human argumentation capacity. ANTIDOTE will exploit cross-disciplinary
competences in deep learning and argumentation to support a broader and
innovative view of explainable AI, where the need for high-quality explanations
for clinical cases deliberation is critical. As a first result of the project,
we publish the Antidote CasiMedicos dataset to facilitate research on
explainable AI in general, and argumentation in the medical domain in
particular.

æè¦ï¼æä¾åºæ¼æ©å¨å­¸ç¿ç AI é æ¸¬çé«åè³ªèªªææ¯ä¸é å·æææ°æ§åè¤éæ§çä»»åãè¦é å©é²è¡ï¼å®éè¦å·åä¸åå ç´ ï¼é¸æé©ç¶çèªªææ®éæ§/ç¹æ®æ§å±¤ç´ï¼èéèªªæåçäººå°æèæ®ç AI ä»»åççæç¨åº¦åè¨­ï¼åç§ä¿ææ±ºç­çç¹å®åç´ ï¼å©ç¨å¯è½ä¸å±¬æ¼é æ¸¬ç¨åºçä¸é¨åçé¡å¤ç¥è­ï¼ä¾å¦å°å®¶è­æï¼ï¼ä¸¦æä¾æ¯æå¦å®åè¨­çè­æãæå¾ï¼ç³»çµ±éè¦ä»¥æ¸æ°å¯è§£éä¸å¯è½ä»¤äººä¿¡æçæ¹å¼å¶å®èªªæãåºæ¼éäºèéï¼ANTIDOTE ä¿æäºå¯è§£é AI çæ´åé¡æ¯ï¼å¶ä¸­æ·±åº¦å­¸ç¿ç¨åºçä½éç¹å¾µèäººé¡è«è­è½åçé«éæ¶æ§ç¸çµåãANTIDOTE å°å©ç¨æ·±åº¦å­¸ç¿èè«è­çè·¨é åè½åï¼ä¾æ¯æå¯è§£é AI æ´å»£æ³ä¸åµæ°çè§é»ï¼å¶ä¸­å°è¨åºæ¡ä¾å¯©è­°çé«åè³ªèªªæéæ±è³ééè¦ãä½çºè©²å°æ¡çç¬¬ä¸åææï¼æåç¼å¸äº Antidote CasiMedicos è³æéï¼ä»¥å©æ¼ä¸è¬å¯è§£é AI çç ç©¶ï¼ç¹å¥æ¯é«çé åçè«è­ã

##### **XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**
2306.04791v1 by Eli Laird, Ayesh Madushanka, Elfi Kraka, Corey Clark

Progress in graph neural networks has grown rapidly in recent years, with
many new developments in drug discovery, medical diagnosis, and recommender
systems. While this progress is significant, many networks are `black boxes'
with little understanding of the `what' exactly the network is learning. Many
high-stakes applications, such as drug discovery, require human-intelligible
explanations from the models so that users can recognize errors and discover
new knowledge. Therefore, the development of explainable AI algorithms is
essential for us to reap the benefits of AI.
  We propose an explainability algorithm for GNNs called eXplainable Insight
(XInsight) that generates a distribution of model explanations using GFlowNets.
Since GFlowNets generate objects with probabilities proportional to a reward,
XInsight can generate a diverse set of explanations, compared to previous
methods that only learn the maximum reward sample. We demonstrate XInsight by
generating explanations for GNNs trained on two graph classification tasks:
classifying mutagenic compounds with the MUTAG dataset and classifying acyclic
graphs with a synthetic dataset that we have open-sourced. We show the utility
of XInsight's explanations by analyzing the generated compounds using QSAR
modeling, and we find that XInsight generates compounds that cluster by
lipophilicity, a known correlate of mutagenicity. Our results show that
XInsight generates a distribution of explanations that uncovers the underlying
relationships demonstrated by the model. They also highlight the importance of
generating a diverse set of explanations, as it enables us to discover hidden
relationships in the model and provides valuable guidance for further analysis.

æè¦ï¼<paragraph>è¿å¹´ä¾ï¼åç¥ç¶ç¶²è·¯çé²å±è¿éï¼å¨è¥ç©ç¼ç¾ãé«çè¨ºæ·åæ¨è¦ç³»çµ±æ¹é¢é½æè¨±å¤æ°ç¼å±ãéç¶éäºé²å±å¾éè¦ï¼ä½è¨±å¤ç¶²è·¯é½æ¯ãé»çå­ãï¼å°æ¼ç¶²è·¯å°åºå¨å­¸ç¿ãä»éº¼ãäºè§£çå°ãè¨±å¤é«é¢¨éªæç¨ï¼ä¾å¦è¥ç©ç¼ç¾ï¼éè¦æ¨¡åæä¾äººé¡å¯ä»¥çè§£çè§£éï¼ä»¥ä¾¿ä½¿ç¨èå¯ä»¥è¾¨è­é¯èª¤ä¸¦ç¼ç¾æ°ç¥è­ãå æ­¤ï¼å¯è§£é AI æ¼ç®æ³çéç¼å°æ¼æåç²å AI çå¥½èè³ééè¦ã
æåæåºäºä¸ç¨®ç¨±çº eXplainable Insight (XInsight) ç GNN å¯è§£éæ§æ¼ç®æ³ï¼å®ä½¿ç¨ GFlowNets ç¢çæ¨¡åè§£éåä½ãç±æ¼ GFlowNets æç¢çæ©çèçåµææ­£æ¯çç©ä»¶ï¼å æ­¤èåååå­¸ç¿æå¤§çåµç¯ä¾çæ¹æ³ç¸æ¯ï¼XInsight å¯ä»¥ç¢çå¤æ¨£åçè§£ééåãæåééçºå¨å©ååå½¢åé¡ä»»åä¸­è¨ç·´ç GNN ç¢çè§£éä¾å±ç¤º XInsightï¼ä½¿ç¨ MUTAG è³æéå°è´çªè®ååç©é²è¡åé¡ï¼ä¸¦ä½¿ç¨æåå·²éæ¾åå§ç¢¼çåæè³æéå°éç°çåå½¢é²è¡åé¡ãæåééä½¿ç¨ QSAR å»ºæ¨¡åæç¢ççååç©ä¾å±ç¤º XInsight è§£éçæç¨ï¼æåç¼ç¾ XInsight æç¢çæè¦ªèæ§ï¼å·²ç¥çè´çªè®ç¸éæ§ï¼åç¾¤çååç©ãæåççµæé¡¯ç¤º XInsight æç¢çä¸åè§£éåä½ï¼æ­ç¤ºæ¨¡åæå±ç¤ºçåºå±¤éä¿ãå®åä¹å¼·èª¿ç¢çå¤æ¨£åè§£ééåçéè¦æ§ï¼å çºå®ä½¿æåè½å¤ ç¼ç¾æ¨¡åä¸­çé±èéä¿ï¼ä¸¦çºé²ä¸æ­¥åææä¾æå¹å¼çæå°ã</paragraph>


### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-12**|**What Makes a Maze Look Like a Maze?**|Joy Hsu et.al.|[2409.08202v1](http://arxiv.org/abs/2409.08202v1)|null|
|**2024-09-12**|**Towards a graph-based foundation model for network traffic analysis**|Louis Van Langendonck et.al.|[2409.08111v1](http://arxiv.org/abs/2409.08111v1)|null|
|**2024-09-12**|**Learning Rules from KGs Guided by Language Models**|Zihang Peng et.al.|[2409.07869v1](http://arxiv.org/abs/2409.07869v1)|[link](https://github.com/pzh97/learning-rules-from-kgs-guided-by-language-models)|
|**2024-09-12**|**Multi-object event graph representation learning for Video Question Answering**|Yanan Wang et.al.|[2409.07747v1](http://arxiv.org/abs/2409.07747v1)|null|
|**2024-09-11**|**Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**|Khiem Ton et.al.|[2409.07368v1](http://arxiv.org/abs/2409.07368v1)|null|
|**2024-09-11**|**Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model**|Daehee Kim et.al.|[2409.07088v1](http://arxiv.org/abs/2409.07088v1)|[link](https://github.com/daehuikim/WikiOFGraph)|
|**2024-09-11**|**Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence**|Jiun-Ting Li et.al.|[2409.07064v1](http://arxiv.org/abs/2409.07064v1)|null|
|**2024-09-11**|**FreeRide: Harvesting Bubbles in Pipeline Parallelism**|Jiashu Zhang et.al.|[2409.06941v1](http://arxiv.org/abs/2409.06941v1)|null|
|**2024-09-10**|**Generative Hierarchical Materials Search**|Sherry Yang et.al.|[2409.06762v1](http://arxiv.org/abs/2409.06762v1)|null|
|**2024-09-10**|**Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization**|Gollam Rabby et.al.|[2409.06433v1](http://arxiv.org/abs/2409.06433v1)|null|
|**2024-09-09**|**Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity**|Dongyue Li et.al.|[2409.06091v1](http://arxiv.org/abs/2409.06091v1)|[link](https://github.com/virtuosoresearch/scalablemtl)|
|**2024-09-09**|**OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System**|Ningyu Zhang et.al.|[2409.07497v1](http://arxiv.org/abs/2409.07497v1)|[link](https://github.com/zjunlp/oneedit)|
|**2024-09-09**|**SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning**|Alireza Ghafarollahi et.al.|[2409.05556v1](http://arxiv.org/abs/2409.05556v1)|[link](https://github.com/lamm-mit/SciAgentsDiscovery)|
|**2024-09-09**|**Assessing SPARQL capabilities of Large Language Models**|Lars-Peter Meyer et.al.|[2409.05925v1](http://arxiv.org/abs/2409.05925v1)|[link](https://github.com/aksw/llm-kg-bench-results)|
|**2024-09-09**|**KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**|Yingshu Li et.al.|[2409.05370v1](http://arxiv.org/abs/2409.05370v1)|null|
|**2024-09-07**|**Action is the primary key: a categorical framework for episode description and logical reasoning**|Yoshiki Fukada et.al.|[2409.04793v1](http://arxiv.org/abs/2409.04793v1)|null|
|**2024-09-06**|**Accelerating Training with Neuron Interaction and Nowcasting Networks**|Boris Knyazev et.al.|[2409.04434v1](http://arxiv.org/abs/2409.04434v1)|[link](https://github.com/samsungsailmontreal/nino)|
|**2024-09-06**|**Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**|Desiree Heim et.al.|[2409.04286v1](http://arxiv.org/abs/2409.04286v1)|null|
|**2024-09-06**|**GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**|Ziyin Zhang et.al.|[2409.04183v1](http://arxiv.org/abs/2409.04183v1)|null|
|**2024-09-06**|**Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**|Larissa Pusch et.al.|[2409.04181v1](http://arxiv.org/abs/2409.04181v1)|null|
|**2024-09-06**|**Refining Wikidata Taxonomy using Large Language Models**|Yiwen Peng et.al.|[2409.04056v1](http://arxiv.org/abs/2409.04056v1)|[link](https://github.com/peng-yiwen/WiKC)|
|**2024-09-06**|**Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features**|Miao Fan et.al.|[2409.04009v1](http://arxiv.org/abs/2409.04009v1)|null|
|**2024-09-05**|**Rx Strategist: Prescription Verification using LLM Agents System**|Phuc Phan Van et.al.|[2409.03440v1](http://arxiv.org/abs/2409.03440v1)|null|
|**2024-09-05**|**iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models**|Yassir Lairgi et.al.|[2409.03284v1](http://arxiv.org/abs/2409.03284v1)|[link](https://github.com/AuvaLab/itext2kg)|
|**2024-09-05**|**GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**|Yukun Cao et.al.|[2409.03258v1](http://arxiv.org/abs/2409.03258v1)|null|
|**2024-09-05**|**Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models**|Jie Ma et.al.|[2409.03155v1](http://arxiv.org/abs/2409.03155v1)|[link](https://github.com/reml-group/dog)|
|**2024-09-04**|**Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification**|Junyoung Lee et.al.|[2409.02481v1](http://arxiv.org/abs/2409.02481v1)|null|
|**2024-09-04**|**Multi-modal Situated Reasoning in 3D Scenes**|Xiongkun Linghu et.al.|[2409.02389v1](http://arxiv.org/abs/2409.02389v1)|null|
|**2024-09-02**|**Grounding Language Models in Autonomous Loco-manipulation Tasks**|Jin Wang et.al.|[2409.01326v1](http://arxiv.org/abs/2409.01326v1)|null|
|**2024-09-02**|**LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning**|Haoran Yang et.al.|[2409.01145v1](http://arxiv.org/abs/2409.01145v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph**|Yuxiang Wang et.al.|[2409.00727v1](http://arxiv.org/abs/2409.00727v1)|null|
|**2024-08-31**|**WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction**|Oktie Hassanzadeh et.al.|[2409.00331v1](http://arxiv.org/abs/2409.00331v1)|[link](https://github.com/IBM/wikicausal)|
|**2024-08-29**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|Jingyi Wang et.al.|[2408.16224v2](http://arxiv.org/abs/2408.16224v2)|null|
|**2024-08-28**|**LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**|Ruirui Chen et.al.|[2408.15903v1](http://arxiv.org/abs/2408.15903v1)|null|
|**2024-08-27**|**VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**|Shusaku Egami et.al.|[2408.14895v2](http://arxiv.org/abs/2408.14895v2)|[link](https://github.com/aistairc/virtualhome_aist)|
|**2024-08-27**|**XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model**|Yasir Ali Farrukh et.al.|[2408.16021v1](http://arxiv.org/abs/2408.16021v1)|[link](https://github.com/yasir-ali-farrukh/gnn4id)|
|**2024-08-26**|**Process Trace Querying using Knowledge Graphs and Notation3**|William Van Woensel et.al.|[2409.04452v1](http://arxiv.org/abs/2409.04452v1)|null|
|**2024-08-26**|**PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method**|Runtao Ren et.al.|[2409.00092v1](http://arxiv.org/abs/2409.00092v1)|null|
|**2024-08-26**|**DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**|Ziai Zhou et.al.|[2408.14185v1](http://arxiv.org/abs/2408.14185v1)|null|
|**2024-08-26**|**Exploring the Potential of Large Language Models for Heterophilic Graphs**|Yuxia Wu et.al.|[2408.14134v1](http://arxiv.org/abs/2408.14134v1)|null|
|**2024-08-26**|**Towards Graph Prompt Learning: A Survey and Beyond**|Qingqing Long et.al.|[2408.14520v2](http://arxiv.org/abs/2408.14520v2)|null|
|**2024-08-25**|**CodeGraph: Enhancing Graph Reasoning of LLMs with Code**|Qiaolong Cai et.al.|[2408.13863v1](http://arxiv.org/abs/2408.13863v1)|null|
|**2024-08-25**|**LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings**|Duo Wang et.al.|[2408.14512v1](http://arxiv.org/abs/2408.14512v1)|null|
|**2024-08-24**|**Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models**|Sakhinana Sagar Srinivas et.al.|[2408.13661v1](http://arxiv.org/abs/2408.13661v1)|null|
|**2024-08-24**|**GNN: Graph Neural Network and Large Language Model for Data Discovery**|Thomas Hoang et.al.|[2408.13609v2](http://arxiv.org/abs/2408.13609v2)|null|
|**2024-08-24**|**HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation**|Azmine Toushik Wasi et.al.|[2408.13521v1](http://arxiv.org/abs/2408.13521v1)|[link](https://github.com/azminewasi/hrgraph)|
|**2024-08-24**|**Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation**|Yi-Hui Chen et.al.|[2408.13432v1](http://arxiv.org/abs/2408.13432v1)|null|
|**2024-08-23**|**CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers**|Ekaterina Trofimova et.al.|[2408.13366v1](http://arxiv.org/abs/2408.13366v1)|null|
|**2024-08-23**|**Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving**|Sakhinana Sagar Srinivas et.al.|[2408.14494v1](http://arxiv.org/abs/2408.14494v1)|null|
|**2024-08-22**|**A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language**|Ekdeep Singh Lubana et.al.|[2408.12578v2](http://arxiv.org/abs/2408.12578v2)|[link](https://github.com/ekdeepslubana/conceptpercolation)|
|**2024-08-22**|**Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language**|Arief Purnama Muharram et.al.|[2409.00061v1](http://arxiv.org/abs/2409.00061v1)|null|
|**2024-08-22**|**Cell-ontology guided transcriptome foundation model**|Xinyu Yuan et.al.|[2408.12373v1](http://arxiv.org/abs/2408.12373v1)|null|
|**2024-08-22**|**Graph Retrieval Augmented Trustworthiness Reasoning**|Ying Zhu et.al.|[2408.12333v2](http://arxiv.org/abs/2408.12333v2)|[link](https://github.com/EvoNexusX/Graph-Retrieval-Augmented-Trustworthiness-Reasoning)|
|**2024-08-22**|**MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**|Yanzeng Li et.al.|[2408.12236v1](http://arxiv.org/abs/2408.12236v1)|null|
|**2024-08-22**|**Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning**|Junlin He et.al.|[2408.12116v1](http://arxiv.org/abs/2408.12116v1)|null|
|**2024-08-21**|**Enabling Small Models for Zero-Shot Classification through Model Label Learning**|Jia Zhang et.al.|[2408.11449v1](http://arxiv.org/abs/2408.11449v1)|null|
|**2024-08-20**|**Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles**|Zhilong Wang et.al.|[2408.11182v1](http://arxiv.org/abs/2408.11182v1)|null|
|**2024-08-20**|**Public Health in Disaster: Emotional Health and Life Incidents Extraction during Hurricane Harvey**|Thomas Hoang et.al.|[2408.11133v1](http://arxiv.org/abs/2408.11133v1)|null|
|**2024-08-20**|**Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains**|Rui Yang et.al.|[2408.10819v1](http://arxiv.org/abs/2408.10819v1)|null|
|**2024-08-20**|**Hologram Reasoning for Solving Algebra Problems with Geometry Diagrams**|Litian Huang et.al.|[2408.10592v1](http://arxiv.org/abs/2408.10592v1)|[link](https://github.com/ferretdoll/hgr)|
|**2024-08-19**|**Query languages for neural networks**|Martin Grohe et.al.|[2408.10362v2](http://arxiv.org/abs/2408.10362v2)|null|
|**2024-08-19**|**Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**|Tianyu Zhang et.al.|[2408.10124v1](http://arxiv.org/abs/2408.10124v1)|[link](https://github.com/zhangtia16/molgraph-lardo)|
|**2024-08-19**|**Geometry Informed Tokenization of Molecules for Language Model Generation**|Xiner Li et.al.|[2408.10120v1](http://arxiv.org/abs/2408.10120v1)|null|
|**2024-08-19**|**GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization**|Ran Liu et.al.|[2408.10115v1](http://arxiv.org/abs/2408.10115v1)|[link](https://github.com/oswald1997/glimmer)|
|**2024-08-19**|**SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction with Legal Clue Tracing**|Pengjie Liu et.al.|[2408.09717v1](http://arxiv.org/abs/2408.09717v1)|null|
|**2024-08-18**|**Revisiting the Graph Reasoning Ability of Large Language Models: Case Studies in Translation, Connectivity and Shortest Path**|Xinnan Dai et.al.|[2408.09529v1](http://arxiv.org/abs/2408.09529v1)|null|
|**2024-08-18**|**Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting**|Geethan Sannidhi et.al.|[2408.13273v1](http://arxiv.org/abs/2408.13273v1)|null|
|**2024-08-18**|**Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models**|Kening Zheng et.al.|[2408.09429v1](http://arxiv.org/abs/2408.09429v1)|null|
|**2024-08-16**|**ASGM-KG: Unveiling Alluvial Gold Mining Through Knowledge Graphs**|Debashis Gupta et.al.|[2408.08972v1](http://arxiv.org/abs/2408.08972v1)|null|
|**2024-08-16**|**EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics**|Chenwei Wan et.al.|[2408.08782v1](http://arxiv.org/abs/2408.08782v1)|[link](https://github.com/cw-wan/EmoDynamiX-v2)|
|**2024-08-16**|**Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?**|Zhongjian Zhang et.al.|[2408.08685v1](http://arxiv.org/abs/2408.08685v1)|null|
|**2024-08-16**|**RoarGraph: A Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search**|Meng Chen et.al.|[2408.08933v1](http://arxiv.org/abs/2408.08933v1)|[link](https://github.com/matchyc/RoarGraph)|
|**2024-08-16**|**Handling abort commands for household kitchen robots**|Darius Has et.al.|[2408.14480v1](http://arxiv.org/abs/2408.14480v1)|null|
|**2024-08-16**|**CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking**|Rong-Ching Chang et.al.|[2408.08535v1](http://arxiv.org/abs/2408.08535v1)|null|
|**2024-08-15**|**VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning and Abstract Syntax Tree (AST)-based Waveform Tracing Tool**|Chia-Tung Ho et.al.|[2408.08927v1](http://arxiv.org/abs/2408.08927v1)|null|
|**2024-08-15**|**Graph Retrieval-Augmented Generation: A Survey**|Boci Peng et.al.|[2408.08921v2](http://arxiv.org/abs/2408.08921v2)|[link](https://github.com/pengboci/graphrag-survey)|
|**2024-08-14**|**Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability**|Jiri Hron et.al.|[2408.07852v1](http://arxiv.org/abs/2408.07852v1)|null|
|**2024-08-14**|**ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction Based on Large Language Model**|Xuanqing Yu et.al.|[2408.07840v1](http://arxiv.org/abs/2408.07840v1)|null|
|**2024-08-14**|**WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs**|Weijian Xie et.al.|[2408.07611v2](http://arxiv.org/abs/2408.07611v2)|null|
|**2024-08-14**|**Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals**|Tobias A. Opsahl et.al.|[2408.07453v1](http://arxiv.org/abs/2408.07453v1)|[link](https://github.com/tobias-opsahl/fact-or-fiction)|
|**2024-08-13**|**LLMs can Schedule**|Henrik Abgaryan et.al.|[2408.06993v1](http://arxiv.org/abs/2408.06993v1)|[link](https://github.com/starjob42/datasetjsp)|
|**2024-08-13**|**Causal Agent based on Large Language Model**|Kairong Han et.al.|[2408.06849v1](http://arxiv.org/abs/2408.06849v1)|[link](https://github.com/kairong-han/causal_agent)|
|**2024-08-13**|**Unlock the Power of Frozen LLMs in Knowledge Graph Completion**|Bo Xue et.al.|[2408.06787v1](http://arxiv.org/abs/2408.06787v1)|null|
|**2024-08-13**|**Computation-friendly Graph Neural Network Design by Accumulating Knowledge on Large Language Models**|Jialiang Wang et.al.|[2408.06717v1](http://arxiv.org/abs/2408.06717v1)|null|
|**2024-08-12**|**Body Transformer: Leveraging Robot Embodiment for Policy Learning**|Carmelo Sferrazza et.al.|[2408.06316v1](http://arxiv.org/abs/2408.06316v1)|null|
|**2024-08-12**|**ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation Using Large Language Models and Transformers**|Aristi Papastavrou et.al.|[2408.06040v1](http://arxiv.org/abs/2408.06040v1)|null|
|**2024-08-12**|**ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models**|Ronak Pradeep et.al.|[2408.05948v1](http://arxiv.org/abs/2408.05948v1)|null|
|**2024-08-11**|**The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms**|Adam Davies et.al.|[2408.05859v1](http://arxiv.org/abs/2408.05859v1)|null|
|**2024-08-10**|**Investigating Instruction Tuning Large Language Models on Graphs**|Kerui Zhu et.al.|[2408.05457v1](http://arxiv.org/abs/2408.05457v1)|[link](https://github.com/zhukerui/graph-instruction-tuning)|
|**2024-08-10**|**Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation**|Wenbo Shang et.al.|[2408.05456v1](http://arxiv.org/abs/2408.05456v1)|null|
|**2024-08-10**|**LaiDA: Linguistics-aware In-context Learning with Data Augmentation for Metaphor Components Identification**|Hongde Liu et.al.|[2408.05404v1](http://arxiv.org/abs/2408.05404v1)|[link](https://github.com/wxljz/laida)|
|**2024-08-09**|**Text classification optimization algorithm based on graph neural network**|Erdi Gao et.al.|[2408.15257v1](http://arxiv.org/abs/2408.15257v1)|null|
|**2024-08-09**|**SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV Battery Supply Chain Disruptions**|Zhi-Qi Cheng et.al.|[2408.05357v1](http://arxiv.org/abs/2408.05357v1)|null|
|**2024-08-09**|**A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning**|Ye Yuan et.al.|[2408.05141v3](http://arxiv.org/abs/2408.05141v3)|null|
|**2024-08-09**|**Node Level Graph Autoencoder: Unified Pretraining for Textual Graph Learning**|Wenbin Hu et.al.|[2408.07091v2](http://arxiv.org/abs/2408.07091v2)|null|
|**2024-08-09**|**HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction**|Bhaskarjit Sarmah et.al.|[2408.04948v1](http://arxiv.org/abs/2408.04948v1)|null|
|**2024-08-08**|**DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization**|Xin Sun et.al.|[2408.04400v1](http://arxiv.org/abs/2408.04400v1)|null|
|**2024-08-08**|**MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models**|Haoxuan Li et.al.|[2408.04388v1](http://arxiv.org/abs/2408.04388v1)|[link](https://github.com/luminosityx/mm-forecast)|

#### Abstracts
##### **What Makes a Maze Look Like a Maze?**
2409.08202v1 by Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu

A unique aspect of human visual understanding is the ability to flexibly
interpret abstract concepts: acquiring lifted rules explaining what they
symbolize, grounding them across familiar and unfamiliar contexts, and making
predictions or reasoning about them. While off-the-shelf vision-language models
excel at making literal interpretations of images (e.g., recognizing object
categories such as tree branches), they still struggle to make sense of such
visual abstractions (e.g., how an arrangement of tree branches may form the
walls of a maze). To address this challenge, we introduce Deep Schema Grounding
(DSG), a framework that leverages explicit structured representations of visual
abstractions for grounding and reasoning. At the core of DSG are
schemas--dependency graph descriptions of abstract concepts that decompose them
into more primitive-level symbols. DSG uses large language models to extract
schemas, then hierarchically grounds concrete to abstract components of the
schema onto images with vision-language models. The grounded schema is used to
augment visual abstraction understanding. We systematically evaluate DSG and
different methods in reasoning on our new Visual Abstractions Dataset, which
consists of diverse, real-world images of abstract concepts and corresponding
question-answer pairs labeled by humans. We show that DSG significantly
improves the abstract visual reasoning performance of vision-language models,
and is a step toward human-aligned understanding of visual abstractions.

æè¦ï¼äººé¡è¦è¦ºçè§£çç¨ç¹é¢åå¨æ¼éæ´»è©®éæ½è±¡æ¦å¿µçè½åï¼ç²åè§£éå¶è±¡å¾µæç¾©çæåè¦åï¼å¨çæåä¸çæçèæ¯ä¸å¥ å®å¶åºç¤ï¼ä¸¦å°å¶é²è¡é æ¸¬ææ¨çãéç¶ç¾æçè¦è¦ºèªè¨æ¨¡åæé·å°å½±åé²è¡å­é¢è©®éï¼ä¾å¦è¾¨è­æ¨¹æç­ç©é«é¡å¥ï¼ï¼ä½å®åå¨çè§£æ­¤é¡è¦è¦ºæ½è±¡æ¦å¿µæä»æå°é£ï¼ä¾å¦æ¨¹æçæåå¦ä½å½¢æè¿·å®®ççå£ï¼ãçºäºæå°æ­¤ææ°ï¼æåå¼å¥äºæ·±åº¦æ¨¡å¼åºç¤ï¼DSGï¼ï¼éæ¯ä¸åæ¡æ¶ï¼å©ç¨è¦è¦ºæ½è±¡æ¦å¿µçæç¢ºçµæ§åè¡¨ç¤ºä¾é²è¡åºç¤åæ¨çãDSG çæ ¸å¿æ¯æ¨¡å¼ââæ½è±¡æ¦å¿µçä¾è³´åæè¿°ï¼å°å¶åè§£çºæ´åå§å±¤ç´çç¬¦èãDSG ä½¿ç¨å¤§åèªè¨æ¨¡åä¾æåæ¨¡å¼ï¼ç¶å¾å°æ¨¡å¼çå·é«çµæé¨ååå±¤åºç¤å°å½±åä¸ï¼ä¸¦ä½¿ç¨è¦è¦ºèªè¨æ¨¡åãåºç¤æ¨¡å¼ç¨æ¼æ´åè¦è¦ºæ½è±¡çè§£ãæåç³»çµ±æ§å°è©ä¼°äº DSG åæåçæ°è¦è¦ºæ½è±¡è³æéä¸çä¸åæ¨çæ¹æ³ï¼è©²è³æéåå«åç¨®çå¯¦ä¸ççæ½è±¡æ¦å¿µå½±åï¼ä»¥åç±äººé¡æ¨è¨çå°æåé¡è§£ç­å°ãæåè­æ DSG å¤§å¹æåäºè¦è¦ºèªè¨æ¨¡åçæ½è±¡è¦è¦ºæ¨çæè½ï¼ä¸¦ä¸æèèäººé¡ä¸è´çè¦è¦ºæ½è±¡çè§£éé²ä¸æ­¥ã

##### **Towards a graph-based foundation model for network traffic analysis**
2409.08111v1 by Louis Van Langendonck, Ismael Castell-Uroz, Pere Barlet-Ros

Foundation models have shown great promise in various fields of study. A
potential application of such models is in computer network traffic analysis,
where these models can grasp the complexities of network traffic dynamics and
adapt to any specific task or network environment with minimal fine-tuning.
Previous approaches have used tokenized hex-level packet data and the model
architecture of large language transformer models. We propose a new, efficient
graph-based alternative at the flow-level. Our approach represents network
traffic as a dynamic spatio-temporal graph, employing a self-supervised link
prediction pretraining task to capture the spatial and temporal dynamics in
this network graph framework. To evaluate the effectiveness of our approach, we
conduct a few-shot learning experiment for three distinct downstream network
tasks: intrusion detection, traffic classification, and botnet classification.
Models finetuned from our pretrained base achieve an average performance
increase of 6.87\% over training from scratch, demonstrating their ability to
effectively learn general network traffic dynamics during pretraining. This
success suggests the potential for a large-scale version to serve as an
operational foundational model.

æè¦ï¼åºç¤æ¨¡åå·²å¨ååç ç©¶é åä¸­å±ç¾åºæ¥µå¤§çåæ¯ãæ­¤é¡æ¨¡åçæ½å¨æç¨ä¹ä¸å¨æ¼é»è¦ç¶²è·¯æµéåæï¼å¶ä¸­éäºæ¨¡åå¯ä»¥ææ¡ç¶²è·¯æµéåæçè¤éæ§ï¼ä¸¦ä»¥æå°çå¾®èª¿é©æä»»ä½ç¹å®ä»»åæç¶²è·¯ç°å¢ãååçåæ³å·²ä½¿ç¨æ¨è¨ååå­é²ä½å±¤ç´å°åè³æåå¤§åèªè¨è½æå¨æ¨¡åçæ¨¡åæ¶æ§ãæåæåºä¸åæ°çãææçæµç¨å±¤ç´åå½¢åæ¿ä»£æ¹æ¡ãæåçåæ³å°ç¶²è·¯æµéè¡¨ç¤ºçºåææç©ºåå½¢ï¼æ¡ç¨èªæç£ç£é£çµé æ¸¬é è¨ç·´ä»»åä¾æææ­¤ç¶²è·¯åå½¢æ¶æ§ä¸­çç©ºéåæéåæãçºäºè©ä¼°æååæ³çæææ§ï¼æåå°ä¸åä¸åçä¸æ¸¸ç¶²è·¯ä»»åï¼å¥ä¾µåµæ¸¬ãæµéåé¡åæ®­å±ç¶²è·¯åé¡ï¼é²è¡å°éå­¸ç¿å¯¦é©ãå¾æåçé è¨ç·´åºç¤å¾®èª¿çæ¨¡åï¼å¶å¹³åæè½æå 6.87%ï¼é«æ¼å¾é ­è¨ç·´ï¼éè­æäºå®åå¨é è¨ç·´æéææå­¸ç¿ä¸è¬ç¶²è·¯æµéåæçè½åãéé æåé¡¯ç¤ºåºå¤§è¦æ¨¡çæ¬ææ½åä½çºéä½åºç¤æ¨¡åã

##### **Learning Rules from KGs Guided by Language Models**
2409.07869v1 by Zihang Peng, Daria Stepanova, Vinh Thinh Ho, Heike Adel, Alessandra Russo, Simon Ott

Advances in information extraction have enabled the automatic construction of
large knowledge graphs (e.g., Yago, Wikidata or Google KG), which are widely
used in many applications like semantic search or data analytics. However, due
to their semi-automatic construction, KGs are often incomplete. Rule learning
methods, concerned with the extraction of frequent patterns from KGs and
casting them into rules, can be applied to predict potentially missing facts. A
crucial step in this process is rule ranking. Ranking of rules is especially
challenging over highly incomplete or biased KGs (e.g., KGs predominantly
storing facts about famous people), as in this case biased rules might fit the
data best and be ranked at the top based on standard statistical metrics like
rule confidence. To address this issue, prior works proposed to rank rules not
only relying on the original KG but also facts predicted by a KG embedding
model. At the same time, with the recent rise of Language Models (LMs), several
works have claimed that LMs can be used as alternative means for KG completion.
In this work, our goal is to verify to which extent the exploitation of LMs is
helpful for improving the quality of rule learning systems.

æè¦ï¼è³è¨èåçé²å±å·²è½èªåå»ºæ§å¤§åç¥è­åè­ï¼ä¾å¦ YagoãWikidata æ Google KGï¼ï¼éäºç¥è­åè­å»£æ³ç¨æ¼è¨±å¤æç¨ç¨å¼ï¼ä¾å¦èªææå°æè³æåæãç¶èï¼ç±æ¼éäºç¥è­åè­æ¯åèªåå»ºæ§çï¼å æ­¤éå¸¸ä¸¦ä¸å®æ´ãè¦åå­¸ç¿æ¹æ³èéæ¼å¾ç¥è­åè­ä¸­èåé »ç¹æ¨¡å¼ï¼ä¸¦å°å®åè½æçºè¦åï¼å¯æç¨æ¼é æ¸¬æ½å¨éºå¤±çäºå¯¦ãæ­¤éç¨ä¸­çä¸åééµæ­¥é©æ¯è¦åæåºãè¦åæåºå¨é«åº¦ä¸å®æ´ææåå·®çç¥è­åè­ï¼ä¾å¦ï¼ä¸»è¦å²å­åäººäºå¯¦çç¥è­åè­ï¼ä¸­ç¹å¥å·æææ°æ§ï¼å çºå¨éç¨®ææ³ä¸ï¼æåå·®çè¦åå¯è½æç¬¦åè³æï¼ä¸¦æ ¹ææ¨æºçµ±è¨éåº¦ï¼ä¾å¦è¦åä¿¡å¿ï¼æå¨æåé¢ãçºäºè§£æ±ºéååé¡ï¼ååçç ç©¶æåºä¸åªä¾è³´åå§ç¥è­åè­ï¼éè¦ä¾è³´ç¥è­åè­åµå¥æ¨¡åé æ¸¬çäºå¯¦ä¾å°è¦åé²è¡æåºãåæï¼é¨èèªè¨æ¨¡å (LM) çèèµ·ï¼ä¸äºç ç©¶è²ç¨± LM å¯ç¨ä½ç¥è­åè­å®æçæ¿ä»£æ¹æ³ãå¨éé ç ç©¶ä¸­ï¼æåçç®æ¨æ¯é©è­å©ç¨ LM å¨å¤å¤§ç¨åº¦ä¸æå©æ¼æåè¦åå­¸ç¿ç³»çµ±çåè³ªã

##### **Multi-object event graph representation learning for Video Question Answering**
2409.07747v1 by Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Vizcarra, Mori Kurokawa

Video question answering (VideoQA) is a task to predict the correct answer to
questions posed about a given video. The system must comprehend spatial and
temporal relationships among objects extracted from videos to perform causal
and temporal reasoning. While prior works have focused on modeling individual
object movements using transformer-based methods, they falter when capturing
complex scenarios involving multiple objects (e.g., "a boy is throwing a ball
in a hoop"). We propose a contrastive language event graph representation
learning method called CLanG to address this limitation. Aiming to capture
event representations associated with multiple objects, our method employs a
multi-layer GNN-cluster module for adversarial graph representation learning,
enabling contrastive learning between the question text and its relevant
multi-object event graph. Our method outperforms a strong baseline, achieving
up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and
TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal
and temporal questions, highlighting its strength in reasoning multiple
object-based events.

æè¦ï¼å½±çåç­ (VideoQA) æ¯ä¸é ä»»åï¼ç¨æ¼é æ¸¬éå°çµ¦å®å½±çæåºçåé¡çæ­£ç¢ºç­æ¡ãç³»çµ±å¿é äºè§£å¾å½±çä¸­æåçç©ä»¶ä¹éçç©ºéåæééä¿ï¼æè½å·è¡å æéä¿åæéæ¨çãéç¶ååçç ç©¶éä¸­æ¼ä½¿ç¨åºæ¼Transformerçæ¨¡åä¾å»ºæ¨¡åå¥ç©ä»¶çåä½ï¼ä½å¨æææ¶åå¤åç©ä»¶çè¤éå ´æ¯ï¼ä¾å¦ãä¸åç·å­©æ­£å¨å°çæé²ç±æ¡ãï¼æï¼å®åæåºç¾åé¡ãæåæåºäºä¸åå°æ¯å¼èªè¨äºä»¶åè¡¨è¡¨ç¤ºå­¸ç¿æ¹æ³ï¼ç¨±çº CLanGï¼ä»¥è§£æ±ºæ­¤éå¶ãçºäºææèå¤åç©ä»¶ç¸éçäºä»¶è¡¨ç¤ºï¼æåçæ¨¡åæ¡ç¨å¤å±¤ GNN éç¾¤æ¨¡çµé²è¡å°æå¼åè¡¨è¡¨ç¤ºå­¸ç¿ï¼ä½¿åé¡æå­åå¶ç¸éçå¤ç©ä»¶äºä»¶åè¡¨ä¹éè½å¤ é²è¡å°æ¯å¼å­¸ç¿ãæåçæ¨¡ååªæ¼å¼·å¤§çåºæºï¼å¨å©åå·æææ°æ§ç VideoQA è³æé NExT-QA å TGIF-QA-R ä¸éå°äºé«é 2.2% çæ´é«æºç¢ºåº¦ãç¹å¥æ¯ï¼å¨èçå æéä¿åæéåé¡æ¹é¢æ¯åºæºé«åº 2.8%ï¼çªé¡¯äºå®å¨æ¨çå¤ååºæ¼ç©ä»¶çäºä»¶æ¹é¢çåªå¢ã

##### **Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**
2409.07368v1 by Khiem Ton, Nhi Nguyen, Mahmoud Nazzal, Abdallah Khreishah, Cristian Borcea, NhatHai Phan, Ruoming Jin, Issa Khalil, Yelong Shen

This paper introduces SGCode, a flexible prompt-optimizing system to generate
secure code with large language models (LLMs). SGCode integrates recent
prompt-optimization approaches with LLMs in a unified system accessible through
front-end and back-end APIs, enabling users to 1) generate secure code, which
is free of vulnerabilities, 2) review and share security analysis, and 3)
easily switch from one prompt optimization approach to another, while providing
insights on model and system performance. We populated SGCode on an AWS server
with PromSec, an approach that optimizes prompts by combining an LLM and
security tools with a lightweight generative adversarial graph neural network
to detect and fix security vulnerabilities in the generated code. Extensive
experiments show that SGCode is practical as a public tool to gain insights
into the trade-offs between model utility, secure code generation, and system
cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is
available at: http://3.131.141.63:8501/.

æè¦ï¼æ¬æä»ç´¹ SGCodeï¼éæ¯ä¸åå½æ§çæç¤ºæä½³åç³»çµ±ï¼ç¨æ¼ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) ç¢çå®å¨çç¨å¼ç¢¼ãSGCode å°æè¿çæç¤ºæä½³åæ¹æ³è LLM æ´åå¨ä¸åçµ±ä¸çç³»çµ±ä¸­ï¼å¯ééåç«¯åå¾ç«¯ API å­åï¼ä½¿ç¨æ¶è½å¤  1) ç¢çå®å¨çç¨å¼ç¢¼ï¼æ²ææ¼æ´ï¼2) æª¢é±ä¸¦åäº«å®å¨æ§åæï¼ä»¥å 3) è¼é¬å¾ä¸ç¨®æç¤ºæä½³åæ¹æ³åæå°å¦ä¸ç¨®æ¹æ³ï¼åææä¾æ¨¡ååç³»çµ±æè½çè¦è§£ãæåå¨ AWS ä¼ºæå¨ä¸ä½¿ç¨ PromSec å¡«å SGCodeï¼éæ¯ä¸ç¨®ééçµå LLM åå®å¨æ§å·¥å·èè¼éç´çæå°æåç¥ç¶ç¶²è·¯ä¾æä½³åæç¤ºçæ¹æ³ï¼ç¨æ¼åµæ¸¬ä¸¦ä¿®æ­£ç¢çç¨å¼ç¢¼ä¸­çå®å¨æ§æ¼æ´ãå»£æ³çå¯¦é©é¡¯ç¤ºï¼SGCode æ¯ä¸åå¯¦ç¨çå¬éå·¥å·ï¼å¯ç¨æ¼æ·±å¥äºè§£æ¨¡åæç¨ãå®å¨ç¨å¼ç¢¼ç¢çåç³»çµ±ææ¬ä¹éçæ¬è¡¡ãèæç¤º LLM ç¸æ¯ï¼SGCode åæå¾®å°çææ¬ãSGCode å¯å¨ä»¥ä¸ç¶²ååå¾ï¼http://3.131.141.63:8501/ã

##### **Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model**
2409.07088v1 by Daehee Kim, Deokhyung Kang, Sangwon Ryu, Gary Geunbae Lee

Knowledge Graph-to-Text (G2T) generation involves verbalizing structured
knowledge graphs into natural language text. Recent advancements in Pretrained
Language Models (PLMs) have improved G2T performance, but their effectiveness
depends on datasets with precise graph-text alignment. However, the scarcity of
high-quality, general-domain G2T generation datasets restricts progress in the
general-domain G2T generation research. To address this issue, we introduce
Wikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T
dataset generated using a novel method that leverages Large Language Model
(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain
graph-text pairs, offers high graph-text consistency without relying on
external ontologies. Experimental results demonstrate that PLM fine-tuned on
WikiOFGraph outperforms those trained on other datasets across various
evaluation metrics. Our method proves to be a scalable and effective solution
for generating high-quality G2T data, significantly advancing the field of G2T
generation.

æè¦ï¼ç¥è­åè­å°æå­ (G2T) çææ¶åå°çµæ§åç¥è­åè­è¡¨éçºèªç¶èªè¨æå­ãé è¨ç·´èªè¨æ¨¡å (PLM) çææ°é²å±æ¹åäº G2T çæè½ï¼ä½å¶æææ§åæ±ºæ¼å·æç²¾ç¢ºåå½¢æå­å°é½çè³æéãç¶èï¼é«åè³ªãä¸è¬é å G2T çæè³æéçç¨å°æ§éå¶äºä¸è¬é å G2T çæç ç©¶çé²å±ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äºç¶­åºç¾ç§æ¬ä½åè²»åå½¢æå­è³æé (WikiOFGraph)ï¼éæ¯ä¸åä½¿ç¨å©ç¨å¤§åèªè¨æ¨¡å (LLM) å Data-QuestEval çæ°æ¹æ³çæçæ°å¤§å G2T è³æéãæåçéåæ°è³æéåå« 585 è¬åä¸è¬é åçåå½¢æå­å°ï¼æä¾é«åå½¢æå­ä¸è´æ§ï¼èä¸ä¾è³´æ¼å¤é¨æ¬ä½ãå¯¦é©çµæè¡¨æï¼å¨ WikiOFGraph ä¸å¾®èª¿ç PLM å¨åç¨®è©ä¼°ææ¨ä¸åªæ¼å¨å¶ä»è³æéä¸è¨ç·´ç PLMãæåçéåæ¹æ³è¢«è­ææ¯ä¸åå¯æ´åä¸ææçè§£æ±ºæ¹æ¡ï¼ç¨æ¼çæé«åè³ªç G2T è³æï¼é¡¯èæ¨åäº G2T çæé åçç¼å±ã

##### **Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence**
2409.07064v1 by Jiun-Ting Li, Bi-Cheng Yan, Tien-Hong Lo, Yi-Cheng Wang, Yung-Chang Hsu, Berlin Chen

Automated speaking assessment in conversation tests (ASAC) aims to evaluate
the overall speaking proficiency of an L2 (second-language) speaker in a
setting where an interlocutor interacts with one or more candidates. Although
prior ASAC approaches have shown promising performance on their respective
datasets, there is still a dearth of research specifically focused on
incorporating the coherence of the logical flow within a conversation into the
grading model. To address this critical challenge, we propose a hierarchical
graph model that aptly incorporates both broad inter-response interactions
(e.g., discourse relations) and nuanced semantic information (e.g., semantic
words and speaker intents), which is subsequently fused with contextual
information for the final prediction. Extensive experimental results on the
NICT-JLE benchmark dataset suggest that our proposed modeling approach can
yield considerable improvements in prediction accuracy with respect to various
assessment metrics, as compared to some strong baselines. This also sheds light
on the importance of investigating coherence-related facets of spoken responses
in ASAC.

æè¦ï¼èªåå°è©±è©éä¸­çèªååå£èªªè©éï¼ASACï¼æ¨å¨è©ä¼° L2ï¼ç¬¬äºèªè¨ï¼è©±èå¨èä¸ä½æå¤ä½æè©¦èäºåçç°å¢ä¸­ï¼æ´é«çå£èªªè½åãåç®¡ååç ASAC æ¹æ³å¨å¶åèªçè³æéä¸å±ç¾åºæåéçè¡¨ç¾ï¼ä½ä»ç¼ºä¹å°æ³¨æ¼å°å°è©±ä¸­éè¼¯æµç¨çé£è²«æ§ç´å¥è©åæ¨¡åçç ç©¶ãçºäºæå°éé ééµææ°ï¼æåæåºäºä¸åéå±¤å¼åå½¢æ¨¡åï¼å®é©ç¶å°çµåäºå»£æ³çåæéäºåï¼ä¾å¦ï¼èªç¯éä¿ï¼åç´°å¾®çèªç¾©è³è¨ï¼ä¾å¦ï¼èªç¾©å­è©åèªªè©±èæåï¼ï¼é¨å¾èèçµ¡è³è¨èåï¼ä»¥é²è¡æçµé æ¸¬ãå¨ NICT-JLE åºæºè³æéä¸é²è¡çå»£æ³å¯¦é©çµæè¡¨æï¼èä¸äºå¼·å¤§çåºæºç·ç¸æ¯ï¼æåæåºçå»ºæ¨¡æ¹æ³å¯ä»¥é¡¯èæåé æ¸¬æºç¢ºåº¦ï¼ç¹å¥æ¯å¨åç¨®è©éææ¨æ¹é¢ãéä¹é¡æäºå¨ ASAC ä¸­æ¢è¨å£èªåæçé£è²«æ§ç¸éé¢åçéè¦æ§ã

##### **FreeRide: Harvesting Bubbles in Pipeline Parallelism**
2409.06941v1 by Jiashu Zhang, Zihan Pan, Molly, Xu, Khuzaima Daudjee, Sihang Liu

The occurrence of bubbles in pipeline parallelism is an inherent limitation
that can account for more than 40% of the large language model (LLM) training
time and is one of the main reasons for the underutilization of GPU resources
in LLM training. Harvesting these bubbles for GPU side tasks can increase
resource utilization and reduce training costs but comes with challenges.
First, because bubbles are discontinuous with various shapes, programming side
tasks becomes difficult while requiring excessive engineering effort. Second, a
side task can compete with pipeline training for GPU resources and incur
significant overhead. To address these challenges, we propose FreeRide, a
system designed to harvest bubbles in pipeline parallelism for side tasks.
FreeRide provides programmers with interfaces to implement side tasks easily,
manages bubbles and side tasks during pipeline training, and controls access to
GPU resources by side tasks to reduce overhead. We demonstrate that FreeRide
achieves 7.8% average cost savings with a negligible overhead of about 1% in
training LLMs while serving model training, graph analytics, and image
processing side tasks.

æè¦ï¼ç®¡ç·å¹³è¡èçä¸­ç¼çæ°£æ³¡æ¯ä¸ååºæéå¶ï¼å¯è½ä½å¤§åèªè¨æ¨¡å (LLM) è¨ç·´æéç 40% ä»¥ä¸ï¼ä¸¦ä¸æ¯ LLM è¨ç·´ä¸­ GPU è³æºå©ç¨ä¸è¶³çä¸»è¦åå ä¹ä¸ãæ¶ééäºæ°£æ³¡ä»¥é²è¡ GPU å´é¢ä»»åå¯ä»¥æé«è³æºå©ç¨çä¸¦éä½è¨ç·´ææ¬ï¼ä½æå¸¶ä¾ææ°ãé¦åï¼ç±æ¼æ°£æ³¡æ¯ä¸é£çºçä¸å½¢çåç°ï¼å æ­¤ç·¨å¯«ç¨å¼å´é¢ä»»åè®å¾å°é£ï¼åæéè¦éå¤çå·¥ç¨å·¥ä½ãå¶æ¬¡ï¼å´é¢ä»»åå¯è½æèç®¡ç·è¨ç·´ç«¶ç­ GPU è³æºï¼ä¸¦é æé¡¯èçéé·ãçºäºæå°éäºææ°ï¼æåæåºäº FreeRideï¼éæ¯ä¸åæ¨å¨æ¶éç®¡ç·å¹³è¡èçä¸­çæ°£æ³¡ä»¥é²è¡å´é¢ä»»åçç³»çµ±ãFreeRide çºç¨å¼è¨­è¨å¸«æä¾äºè¼é¬å¯¦ä½å´é¢ä»»åçä»é¢ï¼å¨ç®¡ç·è¨ç·´æéç®¡çæ°£æ³¡åå´é¢ä»»åï¼ä¸¦æ§å¶å´é¢ä»»åå° GPU è³æºçå­åä»¥æ¸å°éé·ãæåè­æ FreeRide å¨è¨ç·´ LLM æå¯ç¯ç 7.8% çå¹³åææ¬ï¼åæå¨å·è¡æ¨¡åè¨ç·´ãåå½¢åæåå½±åèçå´é¢ä»»åæï¼éé·å¯å¿½ç¥ä¸è¨ï¼ç´çº 1%ã

##### **Generative Hierarchical Materials Search**
2409.06762v1 by Sherry Yang, Simon Batzner, Ruiqi Gao, Muratahan Aykol, Alexander L. Gaunt, Brendan McMorrow, Danilo J. Rezende, Dale Schuurmans, Igor Mordatch, Ekin D. Cubuk

Generative models trained at scale can now produce text, video, and more
recently, scientific data such as crystal structures. In applications of
generative approaches to materials science, and in particular to crystal
structures, the guidance from the domain expert in the form of high-level
instructions can be essential for an automated system to output candidate
crystals that are viable for downstream research. In this work, we formulate
end-to-end language-to-structure generation as a multi-objective optimization
problem, and propose Generative Hierarchical Materials Search (GenMS) for
controllable generation of crystal structures. GenMS consists of (1) a language
model that takes high-level natural language as input and generates
intermediate textual information about a crystal (e.g., chemical formulae), and
(2) a diffusion model that takes intermediate information as input and
generates low-level continuous value crystal structures. GenMS additionally
uses a graph neural network to predict properties (e.g., formation energy) from
the generated crystal structures. During inference, GenMS leverages all three
components to conduct a forward tree search over the space of possible
structures. Experiments show that GenMS outperforms other alternatives of
directly using language models to generate structures both in satisfying user
request and in generating low-energy structures. We confirm that GenMS is able
to generate common crystal structures such as double perovskites, or spinels,
solely from natural language input, and hence can form the foundation for more
complex structure generation in near future.

æè¦ï¼<paragraph>å¤§è¦æ¨¡è¨ç·´ççææ¨¡åç¾å¨å¯ä»¥ç¢çæå­ãå½±çï¼ä»¥åæè¿çç§å­¸è³æï¼ä¾å¦æ¶é«çµæ§ãå¨çææ¹æ³æç¨æ¼ææç§å­¸ï¼å°¤å¶æ¯æ¶é«çµæ§æï¼é åå°å®¶çæå°ï¼ä»¥é«éæä»¤çå½¢å¼ï¼å°æ¼èªååç³»çµ±è¼¸åºå¯è¡æ¼ä¸æ¸¸ç ç©¶çåé¸æ¶é«è³ééè¦ãå¨éé å·¥ä½ä¸­ï¼æåå°ç«¯å°ç«¯èªè¨å°çµæ§çæå¶å®çºå¤ç®æ¨æä½³ååé¡ï¼ä¸¦æåºçæåå±¤æææå° (GenMS) ä»¥æ§å¶æ¶é«çµæ§ççæãGenMS åå« (1) ä¸åèªè¨æ¨¡åï¼å®å°é«éèªç¶èªè¨ä½çºè¼¸å¥ï¼ä¸¦çææéæ¶é«çä¸­éæå­è³è¨ï¼ä¾å¦åå­¸å¬å¼ï¼ï¼ä»¥å (2) ä¸åæ´æ£æ¨¡åï¼å®å°ä¸­éè³è¨ä½çºè¼¸å¥ï¼ä¸¦çæä½éé£çºå¼æ¶é«çµæ§ãGenMS æ­¤å¤ä½¿ç¨åå½¢ç¥ç¶ç¶²è·¯å¾çæçæ¶é«çµæ§é æ¸¬å±¬æ§ï¼ä¾å¦å½¢æè½ï¼ãå¨æ¨çæéï¼GenMS å©ç¨ææä¸åçµä»¶å°å¯è½ççµæ§ç©ºéé²è¡ååæ¨¹çæå°ãå¯¦é©é¡¯ç¤ºï¼GenMS åªæ¼ç´æ¥ä½¿ç¨èªè¨æ¨¡åä¾çæçµæ§çå¶ä»æ¿ä»£æ¹æ¡ï¼ç¡è«æ¯å¨æ»¿è¶³ä½¿ç¨èè¦æ±æçæä½è½çµæ§æ¹é¢ãæåç¢ºèª GenMS è½å¤ åå¾èªç¶èªè¨è¼¸å¥çæå¸¸è¦çæ¶é«çµæ§ï¼ä¾å¦éé£é¦ç¤¦æå°æ¶ç³ï¼å æ­¤å¯ä»¥å¨ä¸ä¹çå°ä¾å½¢ææ´è¤éçµæ§çæçåºç¤ã</paragraph>

##### **Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization**
2409.06433v1 by Gollam Rabby, SÃ¶ren Auer, Jennifer D'Souza, Allard Oelen

The increasing amount of published scholarly articles, exceeding 2.5 million
yearly, raises the challenge for researchers in following scientific progress.
Integrating the contributions from scholarly articles into a novel type of
cognitive knowledge graph (CKG) will be a crucial element for accessing and
organizing scholarly knowledge, surpassing the insights provided by titles and
abstracts. This research focuses on effectively conveying structured scholarly
knowledge by utilizing large language models (LLMs) to categorize scholarly
articles and describe their contributions in a structured and comparable
manner. While previous studies explored language models within specific
research domains, the extensive domain-independent knowledge captured by LLMs
offers a substantial opportunity for generating structured contribution
descriptions as CKGs. Additionally, LLMs offer customizable pathways through
prompt engineering or fine-tuning, thus facilitating to leveraging of smaller
LLMs known for their efficiency, cost-effectiveness, and environmental
considerations. Our methodology involves harnessing LLM knowledge, and
complementing it with domain expert-verified scholarly data sourced from a CKG.
This strategic fusion significantly enhances LLM performance, especially in
tasks like scholarly article categorization and predicate recommendation. Our
method involves fine-tuning LLMs with CKG knowledge and additionally injecting
knowledge from a CKG with a novel prompting technique significantly increasing
the accuracy of scholarly knowledge extraction. We integrated our approach in
the Open Research Knowledge Graph (ORKG), thus enabling precise access to
organized scholarly knowledge, crucially benefiting domain-independent
scholarly knowledge exchange and dissemination among policymakers, industrial
practitioners, and the general public.

æè¦ï¼<paragraph>æ¯å¹´è¶é 250 è¬ç¯çå­¸è¡æç« ç¼è¡¨æ¸éæçºå¢å ï¼å°ç ç©¶äººå¡è¿½è¹¤ç§å­¸é²å±å¸¶ä¾ææ°ãå°å­¸è¡æç« çè²¢ç»æ´åå°æ°åæçèªç¥ç¥è­åè­ (CKG) ä¸­ï¼å°æçºå­ååçµç¹å­¸è¡ç¥è­çééµè¦ç´ ï¼è¶è¶æ¨é¡åæè¦æä¾çè¦è§£ãæ¬ç ç©¶å°æ³¨æ¼ææå³éçµæ§åçå­¸è¡ç¥è­ï¼å©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾åé¡å­¸è¡æç« ï¼ä¸¦ä»¥çµæ§åä¸å¯æ¯è¼çå½¢å¼æè¿°å¶è²¢ç»ãéç¶ååçç ç©¶å¨ç¹å®ç ç©¶é åä¸­æ¢ç´¢èªè¨æ¨¡åï¼ä½ LLM ææå°çå»£æ³é åç¡éç¥è­ï¼çºç¢ççµæ§åçè²¢ç»æè¿°æä¾äºå¯¦è³ªæ©æï¼ä¾å¦ CKGãæ­¤å¤ï¼LLM ééæç¤ºå·¥ç¨æå¾®èª¿æä¾å¯èªè¨è·¯å¾ï¼å¾èä¿é²å©ç¨ä»¥æçãææ¬æçåç°å¢èéèåçè¼å°å LLMãæåçåæ³åæ¬å©ç¨ LLM ç¥è­ï¼ä¸¦éé CKG ä¾æºçé åå°å®¶é©è­å­¸è¡è³æä¾è£åãéç¨®ç­ç¥èåé¡¯èæå LLM çæè½ï¼ç¹å¥æ¯å¨å­¸è¡æç« åé¡åè¬è©æ¨è¦ç­ä»»åä¸­ãæåçæ¹æ³åæ¬ä»¥ CKG ç¥è­å¾®èª¿ LLMï¼ä¸¦ééæ°çæç¤ºæè¡æ³¨å¥ CKG çç¥è­ï¼é¡¯èæåå­¸è¡ç¥è­èåçæºç¢ºåº¦ãæåå°æåçåæ³æ´åå°éæ¾ç ç©¶ç¥è­åè­ (ORKG) ä¸­ï¼å¾èè½ç²¾æºå­åå·²çµç¹çå­¸è¡ç¥è­ï¼éå°æ¿ç­å¶å®èãç¢æ¥­å¾æ¥­äººå¡åä¸è¬å¤§ç¾ä¹éçé åç¡éå­¸è¡ç¥è­äº¤æµåå³æ­è³ééè¦ã</paragraph>

##### **Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity**
2409.06091v1 by Dongyue Li, Aneesh Sharma, Hongyang R. Zhang

Multitask learning is a widely used paradigm for training models on diverse
tasks, with applications ranging from graph neural networks to language model
fine-tuning. Since tasks may interfere with each other, a key notion for
modeling their relationships is task affinity. This includes pairwise task
affinity, computed among pairs of tasks, and higher-order affinity, computed
among subsets of tasks. Naively computing either of them requires repeatedly
training on data from various task combinations, which is computationally
intensive. We present a new algorithm Grad-TAG that can estimate task
affinities without this repeated training.
  The key idea of Grad-TAG is to train a "base" model for all tasks and then
use a linearization technique to estimate the loss of the model for a specific
task combination. The linearization works by computing a gradient-based
approximation of the loss, using low-dimensional projections of gradients as
features in a logistic regression to predict labels for the task combination.
We show that the linearized model can provably approximate the loss when the
gradient-based approximation is accurate, and also empirically verify that on
several large models. Then, given the estimated task affinity, we design a
semi-definite program for clustering similar tasks by maximizing the average
density of clusters.
  We evaluate Grad-TAG's performance across seven datasets, including
multi-label classification on graphs, and instruction fine-tuning of language
models. Our task affinity estimates are within 2.7% distance to the true
affinities while needing only 3% of FLOPs in full training. On our largest
graph with 21M edges and 500 labeling tasks, our algorithm delivers estimates
within 5% distance to the true affinities, using only 112 GPU hours. Our
results show that Grad-TAG achieves excellent performance and runtime tradeoffs
compared to existing approaches.

æè¦ï¼å¤ä»»åå­¸ç¿æ¯ä¸ç¨®å»£æ³ä½¿ç¨çç¯ä¾ï¼ç¨æ¼å¨ä¸åçä»»åä¸è¨ç·´æ¨¡åï¼å¶æç¨ç¯åå¾åç¥ç¶ç¶²è·¯å°èªè¨æ¨¡åå¾®èª¿ãç±æ¼ä»»åå¯è½æç¸äºå¹²æ¾ï¼å æ­¤å»ºæ¨¡å®åéä¿çä¸åééµæ¦å¿µæ¯ä»»åè¦ªåæ§ãéåæ¬æå°ä»»åè¦ªåæ§ï¼å¨æå°ä»»åä¹éè¨ç®ï¼ä»¥åé«éè¦ªåæ§ï¼å¨ä»»åå­éä¹éè¨ç®ãå¤©çå°è¨ç®å¶ä¸­ä»»ä½ä¸åé½éè¦éè¤è¨ç·´ä¾èªåç¨®ä»»åçµåçè³æï¼éå¨è¨ç®ä¸å¾å¯éãæåæåºäºä¸ç¨®æ°çæ¼ç®æ³ Grad-TAGï¼å®å¯ä»¥å¨æ²æéè¤è¨ç·´çææ³ä¸ä¼°è¨ä»»åè¦ªåæ§ã
Grad-TAG çééµææ³æ¯çºææä»»åè¨ç·´ä¸åãåºç¤ãæ¨¡åï¼ç¶å¾ä½¿ç¨ç·æ§åæè¡ä¾ä¼°è¨æ¨¡åå°ç¹å®ä»»åçµåçæå¤±ãç·æ§åééè¨ç®æå¤±çåºæ¼æ¢¯åº¦çè¿ä¼¼å¼ä¾å·¥ä½ï¼ä½¿ç¨æ¢¯åº¦çä½ç¶­æå½±ä½çºç¹å¾µï¼å¨éè¼¯è¿´æ­¸ä¸­é æ¸¬ä»»åçµåçæ¨ç±¤ãæåè­æäºç¶åºæ¼æ¢¯åº¦çè¿ä¼¼å¼æºç¢ºæï¼ç·æ§åæ¨¡åå¯ä»¥è­æå°è¿ä¼¼æå¤±ï¼ä¸¦ä¸å¨å¹¾åå¤§åæ¨¡åä¸ç¶é©é©è­äºéä¸é»ãç¶å¾ï¼çµ¦å®ä¼°è¨çä»»åè¦ªåæ§ï¼æåè¨­è¨äºä¸ååå®ç¨å¼ï¼ééæå¤§åå¢éçå¹³åå¯åº¦ä¾å°é¡ä¼¼çä»»åé²è¡å¢éã
æåè©ä¼°äº Grad-TAG å¨ä¸åè³æéä¸çæè½ï¼åæ¬åå½¢ä¸çå¤æ¨ç±¤åé¡ï¼ä»¥åèªè¨æ¨¡åçæä»¤å¾®èª¿ãæåçä»»åè¦ªåæ§ä¼°è¨èçå¯¦è¦ªåæ§è·é¢å¨ 2.7% ä»¥å§ï¼åæåªéè¦ 3% ç FLOP é²è¡å®æ´è¨ç·´ãå¨æåæå¤§çåå½¢ï¼æ 2100 è¬æ¢éå 500 åæ¨ç±¤ä»»åï¼ä¸ï¼æåçæ¼ç®æ³æä¾çä¼°è¨èçå¯¦è¦ªåæ§è·é¢å¨ 5% ä»¥å§ï¼åªä½¿ç¨ 112 å GPU å°æãæåççµæè¡¨æï¼èç¾ææ¹æ³ç¸æ¯ï¼Grad-TAG å¨æè½åå·è¡æéæ¬è¡¡æ¹é¢åå¾äºåªç°çè¡¨ç¾ã

##### **OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System**
2409.07497v1 by Ningyu Zhang, Zekun Xi, Yujie Luo, Peng Wang, Bozhong Tian, Yunzhi Yao, Jintian Zhang, Shumin Deng, Mengshu Sun, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen

Knowledge representation has been a central aim of AI since its inception.
Symbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can
both represent knowledge. KGs provide highly accurate and explicit knowledge
representation, but face scalability issue; while LLMs offer expansive coverage
of knowledge, but incur significant training costs and struggle with precise
and reliable knowledge manipulation. To this end, we introduce OneEdit, a
neural-symbolic prototype system for collaborative knowledge editing using
natural language, which facilitates easy-to-use knowledge management with KG
and LLM. OneEdit consists of three modules: 1) The Interpreter serves for user
interaction with natural language; 2) The Controller manages editing requests
from various users, leveraging the KG with rollbacks to handle knowledge
conflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the
knowledge from the Controller to edit KG and LLM. We conduct experiments on two
new datasets with KGs which demonstrate that OneEdit can achieve superior
performance.

æè¦ï¼ç¥è­è¡¨å¾µèªäººå·¥æºæ§èªçä»¥ä¾ä¸ç´æ¯å¶æ ¸å¿ç®æ¨ã
ç¬¦èç¥è­åè­ (KG) åç¥ç¶èªè¨å¤§æ¨¡å (LLM) é½å¯ä»¥è¡¨å¾µç¥è­ãKG æä¾é«åº¦æºç¢ºä¸æç¢ºçç¥è­è¡¨å¾µï¼ä½é¢è¨å¯æ´åæ§çåé¡ï¼è LLM æä¾å»£æ³çç¥è­æ¶µèç¯åï¼ä½æç¢çå¤§éçè¨ç·´ææ¬ï¼ä¸¦ä¸å¨ç²¾ç¢ºä¸å¯é çç¥è­æä½æ¹é¢éå°å°é£ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº OneEditï¼éæ¯ä¸åä½¿ç¨èªç¶èªè¨é²è¡åä½ç¥è­ç·¨è¼¯çç¥ç¶ç¬¦èååç³»çµ±ï¼å®ä¿é²äºä½¿ç¨ KG å LLM é²è¡ææ¼ä½¿ç¨çç¥è­ç®¡çãOneEdit åå«ä¸åæ¨¡çµï¼1) è§£è­¯å¨ç¨æ¼ä½¿ç¨èééèªç¶èªè¨é²è¡äºåï¼2) æ§å¶å¨ç®¡çä¾èªä¸åä½¿ç¨èçç·¨è¼¯è«æ±ï¼å©ç¨ KG ååæ»¾ä¾èçç¥è­è¡çªä¸¦é²æ­¢ææ¯çç¥è­æ»æï¼3) ç·¨è¼¯å¨å©ç¨ä¾èªæ§å¶å¨çç¥è­ä¾ç·¨è¼¯ KG å LLMãæåå°å©åå·æ KG çæ°è³æéé²è¡äºå¯¦é©ï¼è­æ OneEdit å¯ä»¥å¯¦ç¾åªç°çæè½ã

##### **SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning**
2409.05556v1 by Alireza Ghafarollahi, Markus J. Buehler

A key challenge in artificial intelligence is the creation of systems capable
of autonomously advancing scientific understanding by exploring novel domains,
identifying complex patterns, and uncovering previously unseen connections in
vast scientific data. In this work, we present SciAgents, an approach that
leverages three core concepts: (1) the use of large-scale ontological knowledge
graphs to organize and interconnect diverse scientific concepts, (2) a suite of
large language models (LLMs) and data retrieval tools, and (3) multi-agent
systems with in-situ learning capabilities. Applied to biologically inspired
materials, SciAgents reveals hidden interdisciplinary relationships that were
previously considered unrelated, achieving a scale, precision, and exploratory
power that surpasses traditional human-driven research methods. The framework
autonomously generates and refines research hypotheses, elucidating underlying
mechanisms, design principles, and unexpected material properties. By
integrating these capabilities in a modular fashion, the intelligent system
yields material discoveries, critique and improve existing hypotheses, retrieve
up-to-date data about existing research, and highlights their strengths and
limitations. Our case studies demonstrate scalable capabilities to combine
generative AI, ontological representations, and multi-agent modeling,
harnessing a `swarm of intelligence' similar to biological systems. This
provides new avenues for materials discovery and accelerates the development of
advanced materials by unlocking Nature's design principles.

æè¦ï¼å¨äººå·¥æºè½ä¸­ï¼ä¸åééµçææ°æ¯åµé åºæè½åééæ¢ç´¢æ°é åãè­å¥è¤éæ¨¡å¼ï¼ä»¥åå¨å¤§éçç§å­¸æ¸æä¸­ç¼ç¾åææªè¦çéè¯ï¼ä¾èªä¸»æ¨é²ç§å­¸çè§£çç³»çµ±ãå¨éé å·¥ä½ä¸­ï¼æåæåºäº SciAgentsï¼ä¸ç¨®å©ç¨ä¸åæ ¸å¿æ¦å¿µçæ¹æ³ï¼(1) ä½¿ç¨å¤§è¦æ¨¡çæ¬ä½ç¥è­åè­ä¾æ´çåé£çµä¸åçç§å­¸æ¦å¿µï¼(2) ä¸å¥å¤§åèªè¨æ¨¡å (LLM) åæ¸ææª¢ç´¢å·¥å·ï¼ä»¥å (3) å·æåä½å­¸ç¿è½åçå¤ä»£çç³»çµ±ãæç¨æ¼çç©åç¼ææï¼SciAgents æ­ç¤ºäºä»¥åè¢«èªçºç¡éçé±èè·¨å­¸ç§éä¿ï¼éå°äºè¶è¶å³çµ±äººçºç ç©¶æ¹æ³çè¦æ¨¡ãç²¾ç¢ºåº¦åæ¢ç´¢è½åãè©²æ¡æ¶èªä¸»çæååªåç ç©¶åè¨­ï¼é¡æåºç¤æ©å¶ãè¨­è¨åçåæå¤çææç¹æ§ãééä»¥æ¨¡çµåæ¹å¼æ´åéäºè½åï¼æºè½ç³»çµ±ç¢çææç¼ç¾ãæ¹å¤åæ¹é²ç¾æåè¨­ãæª¢ç´¢éæ¼ç¾æç ç©¶çææ°æ¸æï¼ä¸¦å¼·èª¿å®åçåªé»åéå¶ãæåçæ¡ä¾ç ç©¶å±ç¤ºäºçµåçæå¼ AIãæ¬ä½è¡¨ç¤ºåå¤ä»£çå»ºæ¨¡çå¯æ´åè½åï¼å©ç¨é¡ä¼¼æ¼çç©ç³»çµ±çãæºæ§ç¾¤é«ããéçºææç¼ç¾æä¾äºæ°éå¾ï¼ä¸¦ééè§£éå¤§èªç¶çè¨­è¨åçä¾å éåé²ææçéç¼ã

##### **Assessing SPARQL capabilities of Large Language Models**
2409.05925v1 by Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt

The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)
offers significant synergistic potential for knowledge-driven applications. One
possible integration is the interpretation and generation of formal languages,
such as those used in the Semantic Web, with SPARQL being a core technology for
accessing KGs. In this paper, we focus on measuring out-of-the box capabilities
of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries
applying a quantitative approach.
  We implemented various benchmarking tasks in the LLM-KG-Bench framework for
automated execution and evaluation with several LLMs. The tasks assess
capabilities along the dimensions of syntax, semantic read, semantic create,
and the role of knowledge graph prompt inclusion.
  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,
and Claude models. Our findings indicate that working with SPARQL SELECT
queries is still challenging for LLMs and heavily depends on the specific LLM
as well as the complexity of the task. While fixing basic syntax errors seems
to pose no problems for the best of the current LLMs evaluated, creating
semantically correct SPARQL SELECT queries is difficult in several cases.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) èç¥è­åè­ (KG) çæ´åçºç¥è­é©åæç¨ç¨å¼æä¾äºé¡¯èçç¶ææ½åãä¸ç¨®å¯è½çæ´åæ¯è§£éåç¢çå½¢å¼åèªè¨ï¼ä¾å¦èªç¾©ç¶²è·¯ä¸­ä½¿ç¨çèªè¨ï¼è SPARQL æ¯å­å KG çæ ¸å¿æè¡ãå¨æ¬æä¸­ï¼æåå°æ³¨æ¼è¡¡é LLM éç®±å³ç¨çè½åï¼ä»¥ä½¿ç¨ SPARQLï¼æ´å·é«å°èªªï¼ä½¿ç¨ SPARQL SELECT æ¥è©¢æç¨éåæ¹æ³ã
  æåå¨ LLM-KG-Bench æ¶æ§ä¸­å¯¦ä½äºåç¨®åºæºæ¸¬è©¦ä»»åï¼ä»¥èªåå·è¡åè©ä¼°å¤å LLMãéäºä»»åè©ä¼°äºèªæ³ãèªç¾©è®åãèªç¾©å»ºç«åç¥è­åè­æç¤ºåå«çè§è²ç­é¢åçè½åã
  æäºéäºæ°çåºæºæ¸¬è©¦ä»»åï¼æåè©ä¼°äº GPTãGemini å Claude æ¨¡åçé¸é ãæåçç ç©¶çµæè¡¨æï¼ä½¿ç¨ SPARQL SELECT æ¥è©¢å°æ¼ LLM ä¾èªªä»ç¶å·æææ°æ§ï¼ä¸¦ä¸å¨å¾å¤§ç¨åº¦ä¸åæ±ºæ¼å·é«ç LLM ä»¥åä»»åçè¤éæ§ãåç®¡ä¿®å¾©åºæ¬çèªæ³é¯èª¤ä¼¼ä¹å°ç®åè©ä¼°çæä½³ LLM ä¾èªªä¸æåé¡ï¼ä½å¨æäºææ³ä¸å»ºç«èªç¾©æ­£ç¢ºç SPARQL SELECT æ¥è©¢å¾å°é£ã

##### **KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**
2409.05370v1 by Yingshu Li, Zhanyu Wang, Yunyi Liu, Lei Wang, Lingqiao Liu, Luping Zhou

Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

æè¦ï¼<paragraph>å©ç¨å¤§åèªè¨æ¨¡å (LLM) å¼·å¤§çåè½ï¼é²è¡æäºçæãéè¼¯æ¨çåå¸¸è­ç¥è­æ´åï¼æ¬ç ç©¶æ·±å¥æ¢è¨å©ç¨ LLM ä¾å¢å¼·èªååæ¾å°å ±åçæ (R2Gen)ãåç®¡ LLM ææè±å¯çç¥è­ï¼ä½è¦ææè§¸ç¼éäºå¤§åæ¨¡åä¸­èç¹å®ä»»åï¼å¦ R2Genï¼ç¸éçç¥è­ï¼æ¯ä¸åéè¦çç ç©¶ææ°ãæ¬ææåºäº KARGENï¼ä¸ååºæ¼ LLM çç¥è­å¢å¼·èªååæ¾å°å ±åçææ¡æ¶ãå©ç¨åçµç LLM ä¾çæå ±åï¼è©²æ¡æ¶æ´åäºä¸åç¥è­åè­ï¼ä»¥è§£é LLM ä¸­èè¸é¨ç¾çç¸éçç¥è­ï¼ä»¥å¢å¼·çæå ±åçè¨åºæç¨ãéæ¯ééå©ç¨ç¥è­åè­ä»¥è¨­è¨çæ¹å¼æåèç¾çç¸éçç¹å¾µä¾å¯¦ç¾çãç±æ¼æ¾å°å ±ååå«æ­£å¸¸åç¾çç¸éçç¼ç¾ï¼å æ­¤æåçåå½¢å¢å¼·ç¾çç¸éç¹å¾µèååå½±åç¹å¾µæ´åï¼å¼é¡§å©åæ¹é¢ãæåæ¢ç´¢äºå©ç¨®èåæ¹æ³ï¼ä»¥èªååªåæåºåé¸ææç¸éçç¹å¾µãèåçç¹å¾µç± LLM ä½¿ç¨ï¼ä»¥çæå°ç¾çæ´ææä¸åè³ªæ´é«çå ±åãæåçåæ³å¨ MIMIC-CXR å IU-Xray è³æéä¸å±ç¤ºäºæå¸æççµæã</paragraph>

##### **Action is the primary key: a categorical framework for episode description and logical reasoning**
2409.04793v1 by Yoshiki Fukada

This research presents a computational framework for describing and
recognizing episodes and for logical reasoning. This framework, named
cognitive-logs, consists of a set of relational and graph databases.
Cognitive-logs record knowledge, particularly in episodes that consist of
"actions" represented by verbs in natural languages and "participants" who
perform the actions. These objects are connected by arrows (morphisms) that
link each action to its participant and link cause to effect. Operations based
on category theory enable comparisons between episodes and deductive
inferences, including abstractions of stories. One of the goals of this study
is to develop a database-driven artificial intelligence. This artificial
intelligence thinks like a human but possesses the accuracy and rigour of a
machine. The vast capacities of databases (up to petabyte scales in current
technologies) enable the artificial intelligence to store a greater volume of
knowledge than neural-network based artificial intelligences. Cognitive-logs
serve as a model of human cognition and designed with references to cognitive
linguistics. Cognitive-logs also have the potential to model various human mind
activities.

æè¦ï¼æ¬ç ç©¶æåºä¸åè¨ç®æ¡æ¶ï¼ç¨ä¾æè¿°åè¾¨è­äºä»¶ä»¥åé²è¡éè¼¯æ¨çãéåæ¡æ¶åçºèªç¥æ¥èªï¼åå«ä¸çµéè¯å¼ååå½¢è³æåº«ãèªç¥æ¥èªè¨éç¥è­ï¼ç¹å¥æ¯åå«ç±èªç¶èªè¨ä¸­çåè©è¡¨ç¤ºçãåä½ãåå·è¡åä½çãåèèãçäºä»¶ãéäºç©ä»¶ç±ç®­é ­ï¼æå°ï¼é£æ¥ï¼å°æ¯ååä½é£çµå°å¶åèèï¼ä¸¦å°åå é£çµå°çµæãåºæ¼ç¯çè«çéç®å¯æ¯è¼äºä»¶åæ¼ç¹¹æ¨è«ï¼åæ¬æäºçæ½è±¡åãæ¬ç ç©¶çç®æ¨ä¹ä¸æ¯éç¼ä¸åè³æåº«é©åçäººå·¥æºæ§ãéåäººå·¥æºæ§æèæ¹å¼åäººé¡ï¼ä½æææ©å¨è¬çæºç¢ºæ§åå´è¬¹æ§ãè³æåº«çé¾å¤§å®¹éï¼å¨ç®åçæè¡ä¸­å¯éç®ä½åçµç­ç´ï¼ä½¿äººå·¥æºæ§è½å¤ å²å­æ¯åºæ¼ç¥ç¶ç¶²è·¯çäººå·¥æºæ§æ´å¤§çç¥è­éãèªç¥æ¥èªä½çºäººé¡èªç¥çæ¨¡åï¼ä¸¦åèèªç¥èªè¨å­¸é²è¡è¨­è¨ãèªç¥æ¥èªä¹ææ½åæ¨¡æ¬åç¨®äººé¡å¿æºæ´»åã

##### **Accelerating Training with Neuron Interaction and Nowcasting Networks**
2409.04434v1 by Boris Knyazev, Abhinav Moudgil, Guillaume Lajoie, Eugene Belilovsky, Simon Lacoste-Julien

Neural network training can be accelerated when a learnable update rule is
used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable
update rules can be costly and unstable to train and use. A simpler recently
proposed approach to accelerate training is to use Adam for most of the
optimization steps and periodically, only every few steps, nowcast (predict
future) parameters. We improve this approach by Neuron interaction and
Nowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural
networks to more accurately nowcast parameters by learning in a supervised way
from a set of training trajectories over multiple tasks. We show that in some
networks, such as Transformers, neuron connectivity is non-trivial. By
accurately modeling neuron connectivity, we allow NiNo to accelerate Adam
training by up to 50\% in vision and language tasks.

æè¦ï¼ç¥ç»ç½ç»è®­ç»å¯ä»¥å éï¼å½ä¸ä¸ªå¯å­¦ä¹ çæ´æ°è§åè¢«ç¨æ¥ä»£æ¿ç»å¸çèªéåºä¼åå¨ï¼ä¾å¦ Adamï¼ãç¶èï¼å¯å­¦ä¹ çæ´æ°è§åå¯è½æ¯æè´µä¸ä¸ç¨³å®çï¼éè¦è®­ç»åä½¿ç¨ãä¸ç§æè¿æåºçæ´ç®åçå éè®­ç»çæ¹æ³æ¯ï¼å¯¹äºå¤§å¤æ°çä¼åæ­¥éª¤ä½¿ç¨ Adamï¼å¹¶ä¸å®æå°ï¼ä»æ¯éå æ­¥ï¼é¢æµï¼é¢æµæªæ¥ï¼åæ°ãæä»¬éè¿ç¥ç»åäº¤äºåé¢æµï¼NiNoï¼ç½ç»æ¥æ¹è¿è¿ç§æ¹æ³ãNiNo å©ç¨ç¥ç»åè¿æ¥åå¾ç¥ç»ç½ç»ï¼éè¿ä»å¤ä¸ªä»»å¡ä¸­çä¸ç»è®­ç»è½¨è¿¹ä¸­ä»¥çç£æ¹å¼å­¦ä¹ ï¼æ´åç¡®å°é¢æµåæ°ãæä»¬è¡¨æï¼å¨ä¸äºç½ç»ä¸­ï¼ä¾å¦ Transformerï¼ç¥ç»åè¿æ¥æ¯éå¹³å¡çãéè¿åç¡®å°å»ºæ¨¡ç¥ç»åè¿æ¥ï¼æä»¬åè®¸ NiNo å° Adam è®­ç»å éé«è¾¾ 50%ï¼ç¨äºè§è§åè¯­è¨ä»»å¡ã

##### **Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**
2409.04286v1 by Desiree Heim, Christian Jilek, Adrian Ulges, Andreas Dengel

Current publicly available knowledge work data collections lack diversity,
extensive annotations, and contextual information about the users and their
documents. These issues hinder objective and comparable data-driven evaluations
and optimizations of knowledge work assistance systems. Due to the considerable
resources needed to collect such data in real-life settings and the necessity
of data censorship, collecting such a dataset appears nearly impossible. For
this reason, we propose a configurable, multi-agent knowledge work dataset
generator. This system simulates collaborative knowledge work among agents
producing Large Language Model-generated documents and accompanying data
traces. Additionally, the generator captures all background information, given
in its configuration or created during the simulation process, in a knowledge
graph. Finally, the resulting dataset can be utilized and shared without
privacy or confidentiality concerns.
  This paper introduces our approach's design and vision and focuses on
generating authentic knowledge work documents using Large Language Models. Our
study involving human raters who assessed 53% of the generated and 74% of the
real documents as realistic demonstrates the potential of our approach.
Furthermore, we analyze the authenticity criteria mentioned in the
participants' comments and elaborate on potential improvements for identified
common issues.

æè¦ï¼<paragraph>ç®åå¬éå¯ç¨çç¥è­å·¥ä½è³æèéç¼ºä¹å¤åæ§ãå»£æ³è¨»è§£åä½¿ç¨èåå¶æä»¶èæ¯è³è¨ãéäºåé¡é»ç¤äºå®¢è§ä¸å¯æ¯è¼çè³æé©åè©ä¼°ï¼ä»¥åç¥è­å·¥ä½åå©ç³»çµ±çæä½³åãç±æ¼å¨ç¾å¯¦çæ´»ä¸­èéæ­¤é¡è³æéè¦å¤§éè³æºï¼èä¸å¿é å¯©æ¥è³æï¼èéæ­¤é¡è³æçµé¡¯ç¶å¹¾ä¹ä¸å¯è½ãå æ­¤ï¼æåæåºä¸åå¯è¨­å®çå¤éä»£çç¥è­å·¥ä½è³æçµç¢çå¨ãæ­¤ç³»çµ±æ¨¡æ¬ä»£çä¹éçåä½ç¥è­å·¥ä½ï¼ç¢çå¤§åèªè¨æ¨¡åç¢ççæä»¶åé¨éçè³æè¿½è¹¤ãæ­¤å¤ï¼ç¢çå¨ææ·åææèæ¯è³è¨ï¼å¨çµæä¸­æä¾æå¨æ¨¡æ¬éç¨ä¸­å»ºç«ï¼ä¸¦å°å¶å²å­å¨ç¥è­åè­ä¸­ãæå¾ï¼ç¢ççè³æçµå¯ä»¥ä½¿ç¨ååäº«ï¼ç¡é æå¿é±ç§ææ©å¯æ§ã
æ¬æä»ç´¹æåæ¹æ³çè¨­è¨åé¡æ¯ï¼ä¸¦å°æ³¨æ¼ä½¿ç¨å¤§åèªè¨æ¨¡åç¢ççå¯¦çç¥è­å·¥ä½æä»¶ãæåçç ç©¶æ¶åäººé¡è©åå¡ï¼ä»åè©ä¼°äº 53% çç¢çæä»¶å 74% ççå¯¦æä»¶çºçå¯¦ï¼éè­æäºæåæ¹æ³çæ½åãæ­¤å¤ï¼æååæåèèè©è«ä¸­æå°ççå¯¦æ§æ¨æºï¼ä¸¦è©³ç´°èªªæå·²è­å¥å¸¸è¦åé¡çæ½å¨æ¹åæ¹æ³ã</paragraph>

##### **GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**
2409.04183v1 by Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang

Programming languages possess rich semantic information such as data flow
that is represented by graphs and not available from the surface form of source
code. Recent code language models have scaled to billions of parameters, but
model source code solely as text tokens while ignoring any other structural
information. Conversely, models that do encode structural information of code
make modifications to the Transformer architecture, limiting their scale and
compatibility with pretrained LLMs. In this work, we take the best of both
worlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph
neural networks and cross-modal alignment technologies to inject the structural
information of code into LLMs as an auxiliary task during finetuning. This
framework is both model-agnostic and task-agnostic, as it can be applied to any
code LLM for any code downstream task, and requires the structural graph data
only at training time from a corpus unrelated to the finetuning data, while
incurring no cost at inference time over the baseline LLM. Experiments on five
code tasks with four different baseline LLMs ranging in size from 350M to 8B
validate the effectiveness of GALLa, demonstrating consistent improvement over
the baseline, even for powerful models such as LLaMA3.

æè¦ï¼ç¨å¼èªè¨ææè±å¯çèªæè³è¨ï¼ä¾å¦ç±åå½¢è¡¨ç¤ºä¸ç¡æ³å¾åå§ç¢¼è¡¨é¢å½¢å¼åå¾çè³ææµç¨ãæè¿çç¨å¼ç¢¼èªè¨æ¨¡åå·²æ´åè³æ¸ååååæ¸ï¼ä½æ¨¡ååå§ç¢¼åä½çºæå­ç¬¦èï¼èå¿½ç¥ä»»ä½å¶ä»çµæ§è³è¨ãåä¹ï¼ç·¨ç¢¼ç¨å¼ç¢¼çµæ§è³è¨çæ¨¡åæä¿®æ¹ Transformer æ¶æ§ï¼éå¶å¶è¦æ¨¡åèé åè¨ç·´ç LLM çç¸å®¹æ§ãå¨éé å·¥ä½ä¸­ï¼æåæ¡ç¨ GALLaï¼åå½¢å°é½å¤§åèªè¨æ¨¡åï¼æ·åå©å¨å¶ç¾çåªé»ãGALLa å©ç¨åå½¢ç¥ç¶ç¶²è·¯åè·¨æ¨¡æå°é½æè¡ï¼å¨å¾®èª¿æéå°ç¨å¼ç¢¼ççµæ§è³è¨æ³¨å¥ LLM ä½çºè¼å©ä»»åãæ­¤æ¶æ§åæä¸ä¾è³´æ¨¡ååä»»åï¼å çºå®å¯ä»¥æç¨æ¼ä»»ä½ç¨å¼ç¢¼ LLM çä»»ä½ç¨å¼ç¢¼ä¸æ¸¸ä»»åï¼ä¸¦ä¸åå¨è¨ç·´æéå¾èå¾®èª¿è³æç¡éçèªæåº«åå¾çµæ§åå½¢è³æï¼åæå¨æ¨è«æéä¸ç¢çæ¯åºæº LLM æ´é«çææ¬ãå¨äºåç¨å¼ç¢¼ä»»åä¸­é²è¡å¯¦é©ï¼ä½¿ç¨ååä¸åçåºæº LLMï¼è¦æ¨¡å¾ 350M å° 8Bï¼é©è­ GALLa çæææ§ï¼è­æå³ä½¿å°æ¼ LLaMA3 ç­å¼·å¤§æ¨¡åï¼ä¹è½æçºåªæ¼åºæºã

##### **Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**
2409.04181v1 by Larissa Pusch, Tim O. F. Conrad

Advancements in natural language processing have revolutionized the way we
can interact with digital information systems, such as databases, making them
more accessible. However, challenges persist, especially when accuracy is
critical, as in the biomedical domain. A key issue is the hallucination
problem, where models generate information unsupported by the underlying data,
potentially leading to dangerous misinformation. This paper presents a novel
approach designed to bridge this gap by combining Large Language Models (LLM)
and Knowledge Graphs (KG) to improve the accuracy and reliability of
question-answering systems, on the example of a biomedical KG. Built on the
LangChain framework, our method incorporates a query checker that ensures the
syntactical and semantic validity of LLM-generated queries, which are then used
to extract information from a Knowledge Graph, substantially reducing errors
like hallucinations. We evaluated the overall performance using a new benchmark
dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo
and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other
models in generating accurate queries, open-source models like llama3:70b show
promise with appropriate prompt engineering. To make this approach accessible,
a user-friendly web-based interface has been developed, allowing users to input
natural language queries, view generated and corrected Cypher queries, and
verify the resulting paths for accuracy. Overall, this hybrid approach
effectively addresses common issues such as data gaps and hallucinations,
offering a reliable and intuitive solution for question answering systems. The
source code for generating the results of this paper and for the user-interface
can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui

æè¦ï¼èªç¶èªè¨èççé²å±å¾¹åºæ¹è®äºæåèæ¸ä½è³è¨ç³»çµ±ï¼ä¾å¦è³æåº«ï¼äºåçæ¹å¼ï¼è®éäºç³»çµ±è®å¾æ´ææ¼å­åãç¶èï¼ææ°ä»ç¶å­å¨ï¼å°¤å¶æ¯å¨æºç¢ºæ§è³ééè¦çææ³ä¸ï¼ä¾å¦å¨çç©é«å­¸é åãä¸åééµåé¡æ¯å¹»è¦ºåé¡ï¼å¶ä¸­æ¨¡åæç¢çæªç¶åºç¤è³æé©è­çè³è¨ï¼å¯è½å°è´å±éªçé¯èª¤è³è¨ãæ¬ææåºäºä¸ç¨®æ°ç©çæ¹æ³ï¼æ¨å¨ééçµåå¤§åèªè¨æ¨¡å (LLM) åç¥è­åè­ (KG) ä¾å½è£éåå·®è·ï¼ä»¥æé«çç©é«å­¸ KG ä¸­åç­ç³»çµ±çæºç¢ºæ§åå¯é æ§ãæåçæè¡å»ºç«å¨ LangChain æ¡æ¶ä¸ï¼çµåäºä¸åæ¥è©¢æª¢æ¥å¨ï¼å¯ç¢ºä¿ LLM çæçæ¥è©¢å¨èªæ³åèªæä¸ææï¼ç¶å¾ç¨æ¼å¾ç¥è­åè­ä¸­èåè³è¨ï¼å¤§å¹æ¸å°å¹»è¦ºç­é¯èª¤ãæåä½¿ç¨ä¸åæ°ç 50 åçç©é«å­¸åé¡åºæºè³æéè©ä¼°äºæ´é«æè½ï¼æ¸¬è©¦äºåæ¬ GPT-4 Turbo å llama3:70b å¨å§çå¹¾å LLMãæåççµæé¡¯ç¤ºï¼éç¶ GPT-4 Turbo å¨ç¢çæºç¢ºæ¥è©¢æ¹é¢åªæ¼å¶ä»æ¨¡åï¼ä½å llama3:70b éæ¨£çéæºæ¨¡åå¨é©ç¶çæç¤ºå·¥ç¨ä¸é¡¯ç¤ºåºåæ¯ãçºäºè®éç¨®æ¹æ³ææ¼ä½¿ç¨ï¼æåéç¼äºä¸åä½¿ç¨èååçç¶²è·¯ä»é¢ï¼è®ä½¿ç¨èå¯ä»¥è¼¸å¥èªç¶èªè¨æ¥è©¢ãæª¢è¦ç¢çåæ´æ­£ç Cypher æ¥è©¢ï¼ä¸¦é©è­çµæè·¯å¾çæºç¢ºæ§ãç¸½é«èè¨ï¼éç¨®æ··åæ¹æ³ææå°è§£æ±ºäºè³æå·®è·åå¹»è¦ºç­å¸¸è¦åé¡ï¼çºåç­ç³»çµ±æä¾äºä¸åå¯é ä¸ç´è§çè§£æ±ºæ¹æ¡ãæ¬æçµæç¢ççåå§ç¢¼åä½¿ç¨èä»é¢çåå§ç¢¼å¯ä»¥å¨æåç Git å²å­åº«ä¸­æ¾å°ï¼https://git.zib.de/lpusch/cyphergenkg-gui

##### **Refining Wikidata Taxonomy using Large Language Models**
2409.04056v1 by Yiwen Peng, Thomas Bonald, Mehwish Alam

Due to its collaborative nature, Wikidata is known to have a complex
taxonomy, with recurrent issues like the ambiguity between instances and
classes, the inaccuracy of some taxonomic paths, the presence of cycles, and
the high level of redundancy across classes. Manual efforts to clean up this
taxonomy are time-consuming and prone to errors or subjective decisions. We
present WiKC, a new version of Wikidata taxonomy cleaned automatically using a
combination of Large Language Models (LLMs) and graph mining techniques.
Operations on the taxonomy, such as cutting links or merging classes, are
performed with the help of zero-shot prompting on an open-source LLM. The
quality of the refined taxonomy is evaluated from both intrinsic and extrinsic
perspectives, on a task of entity typing for the latter, showing the practical
interest of WiKC.

æè¦ï¼ç±æ¼å¶åä½æ§è³ªï¼Wikidata å·²ç¥å·æè¤éçåé¡æ³ï¼ä¸¦æéè¤ç¼ççåé¡ï¼ä¾å¦å¯¦ä¾åé¡å¥ä¹éçæ­§ç¾©ãæäºåé¡è·¯å¾çä¸æºç¢ºæ§ãå¾ªç°çå­å¨ï¼ä»¥åé¡å¥ä¹éçé«åé¤ãæåæ¸çæ­¤åé¡æ³çå·¥ä½æ¢èæåå®¹æåºç¾é¯èª¤æä¸»è§å¤æ·ãæåæåº WiKCï¼éæ¯ Wikidata åé¡æ³çæ°çæ¬ï¼ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) ååå½¢æææè¡èªåæ¸çãåé¡æ³ä¸çæä½ï¼ä¾å¦åªåéæ¥æåä½µé¡å¥ï¼æ¯å¨éæº LLM ä¸åå©é¶æ¬¡æç¤ºçå¹«å©ä¸å·è¡çãç²¾çåé¡æ³çåè³ªå¾å§å¨åå¤å¨çè§é»é²è¡è©ä¼°ï¼å¨å¾èçå¯¦é«ååä»»åä¸ï¼é¡¯ç¤ºäº WiKC çå¯¦éèè¶£ã

##### **Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features**
2409.04009v1 by Miao Fan, Yeqi Bai, Mingming Sun, Ping Li

Relation classification (RC) plays a pivotal role in both natural language
understanding and knowledge graph completion. It is generally formulated as a
task to recognize the relationship between two entities of interest appearing
in a free-text sentence. Conventional approaches on RC, regardless of feature
engineering or deep learning based, can obtain promising performance on
categorizing common types of relation leaving a large proportion of
unrecognizable long-tail relations due to insufficient labeled instances for
training. In this paper, we consider few-shot learning is of great practical
significance to RC and thus improve a modern framework of metric learning for
few-shot RC. Specifically, we adopt the large-margin ProtoNet with fine-grained
features, expecting they can generalize well on long-tail relations. Extensive
experiments were conducted by FewRel, a large-scale supervised few-shot RC
dataset, to evaluate our framework: LM-ProtoNet (FGF). The results demonstrate
that it can achieve substantial improvements over many baseline approaches.

æè¦ï¼éä¿åé¡ (RC) å¨èªç¶èªè¨çè§£åç¥è­åè­å®æä¸­æ®æ¼èééµè§è²ãå®éå¸¸è¢«è¡¨è¿°çºä¸åä»»åï¼ç¨æ¼è¾¨è­åºç¾å¨èªç±æå­å¥å­ä¸­çå©åæèè¶£å¯¦é«ä¹éçéä¿ãç¡è«æ¯åºæ¼ç¹å¾µå·¥ç¨éæ¯æ·±åº¦å­¸ç¿çå³çµ± RC æ¹æ³ï¼é½å¯ä»¥å°å¸¸è¦çéä¿é¡åé²è¡åé¡ï¼å¾èç²å¾æå¸æçæè½ï¼ä½ç±æ¼è¨ç·´æ¨ç±¤å¯¦ä¾ä¸è¶³ï¼å æ­¤ç¡æ³è¾¨è­åºå¤§éçé·å°¾éä¿ãå¨æ¬æä¸­ï¼æåèªçºå°æ¨£æ¬å­¸ç¿å° RC å·æéè¦çå¯¦ç¨æç¾©ï¼å æ­¤æ¹é²äºåº¦éå­¸ç¿çç¾ä»£æ¡æ¶ï¼ä»¥é²è¡å°æ¨£æ¬ RCãå·é«ä¾èªªï¼æåæ¡ç¨å·æç´°ç²åº¦ç¹å¾µçå¤§éè· ProtoNetï¼ææå®åè½å¨é·å°¾éä¿ä¸å¾å¥½å°æ¦æ¬ãæåä½¿ç¨å¤§åç£ç£å°æ¨£æ¬ RC è³æé FewRel é²è¡äºå»£æ³çå¯¦é©ï¼ä»¥è©ä¼°æåçæ¡æ¶ï¼LM-ProtoNet (FGF)ãçµæè¡¨æï¼å®å¯ä»¥æ¯è¨±å¤åºç·æ¹æ³ç²å¾é¡¯èæ¹é²ã

##### **Rx Strategist: Prescription Verification using LLM Agents System**
2409.03440v1 by Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, Huy Phan Thanh

To protect patient safety, modern pharmaceutical complexity demands strict
prescription verification. We offer a new approach - Rx Strategist - that makes
use of knowledge graphs and different search strategies to enhance the power of
Large Language Models (LLMs) inside an agentic framework. This multifaceted
technique allows for a multi-stage LLM pipeline and reliable information
retrieval from a custom-built active ingredient database. Different facets of
prescription verification, such as indication, dose, and possible drug
interactions, are covered in each stage of the pipeline. We alleviate the
drawbacks of monolithic LLM techniques by spreading reasoning over these
stages, improving correctness and reliability while reducing memory demands.
Our findings demonstrate that Rx Strategist surpasses many current LLMs,
achieving performance comparable to that of a highly experienced clinical
pharmacist. In the complicated world of modern medications, this combination of
LLMs with organized knowledge and sophisticated search methods presents a
viable avenue for reducing prescription errors and enhancing patient outcomes.

æè¦ï¼çºäºä¿è­·æ£èå®å¨ï¼ç¾ä»£è¥åè¤éæ§è¦æ±å´æ ¼çèæ¹é©è­ãæåæä¾ä¸ç¨®æ°æ¹æ³ - Rx Strategist - å®å©ç¨ç¥è­åè­åä¸åçæå°ç­ç¥ä¾å¢å¼·ä»£çæ¶æ§å§å¤§åèªè¨æ¨¡å (LLM) çåè½ãéç¨®å¤æ¹é¢çæè¡åè¨±å¤éæ®µç LLM ç®¡ç·åå¾èªè¨ä¸»åæåè³æåº«ä¸­å¯é å°æ·åè³è¨ãèæ¹é©è­çä¸åé¢åï¼ä¾å¦é©æçãåéåå¯è½çè¥ç©äº¤äºä½ç¨ï¼é½å¨ç®¡ç·çæ¯åéæ®µä¸­æ¶µèãæåééå°æ¨çåæ£å¨éäºéæ®µä¾æ¸è¼å®ä¸ LLM æè¡çç¼ºé»ï¼åææé«æ­£ç¢ºæ§åå¯é æ§ï¼ä¸¦æ¸å°è¨æ¶é«éæ±ãæåçç ç©¶çµæè¡¨æï¼Rx Strategist è¶è¶è¨±å¤ç¾æç LLMï¼éå°èç¶é©è±å¯çè¨åºè¥åå¸«ç¸ç¶çè¡¨ç¾ãå¨ç¾ä»£è¥ç©è¤éçä¸çä¸­ï¼éç¨®å° LLM èæçµç¹çç¥è­ååé²æå°æ¹æ³ç¸çµåï¼çºæ¸å°èæ¹é¯èª¤åæ¹åæ£èé å¾æä¾äºå¯è¡çéå¾ã

##### **iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models**
2409.03284v1 by Yassir Lairgi, Ludovic Moncla, RÃ©my Cazabet, Khalid Benabdeslem, Pierre ClÃ©au

Most available data is unstructured, making it challenging to access valuable
information. Automatically building Knowledge Graphs (KGs) is crucial for
structuring data and making it accessible, allowing users to search for
information effectively. KGs also facilitate insights, inference, and
reasoning. Traditional NLP methods, such as named entity recognition and
relation extraction, are key in information retrieval but face limitations,
including the use of predefined entity types and the need for supervised
learning. Current research leverages large language models' capabilities, such
as zero- or few-shot learning. However, unresolved and semantically duplicated
entities and relations still pose challenges, leading to inconsistent graphs
and requiring extensive post-processing. Additionally, most approaches are
topic-dependent. In this paper, we propose iText2KG, a method for incremental,
topic-independent KG construction without post-processing. This plug-and-play,
zero-shot method is applicable across a wide range of KG construction scenarios
and comprises four modules: Document Distiller, Incremental Entity Extractor,
Incremental Relation Extractor, and Graph Integrator and Visualization. Our
method demonstrates superior performance compared to baseline methods across
three scenarios: converting scientific papers to graphs, websites to graphs,
and CVs to graphs.

æè¦ï¼å¤§é¨åå¯ç¨è³æçºéçµæ§åï¼éä½¿å¾å­åæå¹å¼çè³è¨è®å¾å·æææ°æ§ãèªåå»ºç«ç¥è­åè­ (KG) å°æ¼çµæ§åè³æåè®è³æææ¼å­åè³ééè¦ï¼è®ä½¿ç¨èè½å¤ ææå°æå°è³è¨ãKG ä¹ä¿é²è¦è§£ãæ¨è«åæ¨çãå³çµ±ç NLP æ¹æ³ï¼ä¾å¦å½åå¯¦é«è¾¨è­åéä¿èåï¼å¨è³è¨æª¢ç´¢ä¸­æ¯ééµï¼ä½é¢è¨éå¶ï¼åæ¬ä½¿ç¨é å®ç¾©çå¯¦é«é¡ååéè¦ç£ç£å¼å­¸ç¿ãç®åçç ç©¶æå©ç¨å¤§åèªè¨æ¨¡åçè½åï¼ä¾å¦é¶æ¬¡æå°æ¬¡å­¸ç¿ãç¶èï¼æªè§£æ±ºåèªç¾©éè¤çå¯¦é«åéä¿ä»ç¶æ§æææ°ï¼å°è´åå½¢ä¸ä¸è´ï¼éè¦å»£æ³çå¾èçãæ­¤å¤ï¼å¤§å¤æ¸æ¹æ³é½ä¾è³´æ¼ä¸»é¡ãå¨æ¬æä¸­ï¼æåæåº iText2KGï¼ä¸ç¨®ç¨æ¼æ¼¸é²å¼ãèä¸»é¡ç¡éç KG å»ºæ§æ¹æ³ï¼ç¡éå¾èçãéç¨®å³æå³ç¨ãé¶æ¬¡çæ¹æ³é©ç¨æ¼å»£æ³ç KG å»ºæ§å ´æ¯ï¼ä¸¦åå«ååæ¨¡çµï¼æä»¶ç²¾é¤¾å¨ãæ¼¸é²å¼å¯¦é«èåå¨ãæ¼¸é²å¼éä¿èåå¨ï¼ä»¥ååå½¢æ´åå¨åè¦è¦ºåå¨ãèåºç·æ¹æ³ç¸æ¯ï¼æåçæ¨¡åå¨ä¸ç¨®å ´æ¯ä¸­å±ç¾åºåè¶çæè½ï¼å°ç§å­¸è«æè½æçºåå½¢ãç¶²ç«è½æçºåå½¢ï¼ä»¥åå±¥æ­·è½æçºåå½¢ã

##### **GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**
2409.03258v1 by Yukun Cao, Shuo Han, Zengyi Gao, Zezhong Ding, Xike Xie, S. Kevin Zhou

Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.

æè¦ï¼åç®¡å¤§åèªè¨æ¨¡å (LLM) å·²å±ç¾åºèçåå½¢çè½åï¼ä½å®åå¨ééåå½¢æè¿°åºåæç¤ºçè§£åå½¢çµæ§è³è¨ææéå°å°é£ï¼ç¹å¥æ¯å¨åå½¢å¤§å°å¢å æãæåå°æ­¤ææ°æ­¸å æ¼ LLM å¨åå½¢æè¿°åºåä¸­ä¸åä½ç½®çè¨æ¶åè¡¨ç¾ä¸åï¼ç¨±çºãä½ç½®åèª¤ããçºäºè§£æ±ºéååé¡ï¼æåæåºäº GraphInsightï¼ä¸åæ¨å¨æ¹å LLM å°å·¨è§åå¾®è§å±¤ç´åå½¢è³è¨çè§£çæ°æ¡æ¶ãGraphInsight ä»¥å©åééµç­ç¥çºåºç¤ï¼1) å°ééµåå½¢è³è¨æ¾ç½®å¨ LLM å±ç¾è¼å¼·è¨æ¶åè¡¨ç¾çä½ç½®ï¼ä»¥å 2) èª¿æ¥ä¸ååå°æª¢ç´¢å¢å¼·çæ (RAG) åç¼çãéå°è¨æ¶åè¡¨ç¾è¼å¼±ååçè¼éç´å¤é¨ç¥è­åº«ãæ­¤å¤ï¼GraphInsight æ¢ç´¢å°éå©åç­ç¥æ´åå° LLM ä»£çç¨åºä¸­ï¼ä»¥èçéè¦å¤æ­¥é©æ¨ççè¤ååå½¢ä»»åãå¨å·æå»£æ³è©éä»»åçåºæºä¸é²è¡çå»£æ³å¯¦è­ç ç©¶é¡¯ç¤ºï¼GraphInsight å¨çè§£åç¨®å¤§å°çåå½¢çµæ§æ¹é¢ï¼æé¡¯åªæ¼ææå¶ä»åå½¢æè¿°æ¹æ³ï¼ä¾å¦æç¤ºæå·§åéæ°æåºç­ç¥ï¼ã

##### **Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models**
2409.03155v1 by Jie Ma, Zhitao Gao, Qi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei, Jing Tao, Lingyun Song, Jun Liu, Chen Zhang, Lizhen Cui

Large Language Models (LLMs) may suffer from hallucinations in real-world
applications due to the lack of relevant knowledge. In contrast, knowledge
graphs encompass extensive, multi-relational structures that store a vast array
of symbolic facts. Consequently, integrating LLMs with knowledge graphs has
been extensively explored, with Knowledge Graph Question Answering (KGQA)
serving as a critical touchstone for the integration. This task requires LLMs
to answer natural language questions by retrieving relevant triples from
knowledge graphs. However, existing methods face two significant challenges:
\textit{excessively long reasoning paths distracting from the answer
generation}, and \textit{false-positive relations hindering the path
refinement}. In this paper, we propose an iterative interactive KGQA framework
that leverages the interactive learning capabilities of LLMs to perform
reasoning and Debating over Graphs (DoG). Specifically, DoG employs a
subgraph-focusing mechanism, allowing LLMs to perform answer trying after each
reasoning step, thereby mitigating the impact of lengthy reasoning paths. On
the other hand, DoG utilizes a multi-role debate team to gradually simplify
complex questions, reducing the influence of false-positive relations. This
debate mechanism ensures the reliability of the reasoning process. Experimental
results on five public datasets demonstrate the effectiveness and superiority
of our architecture. Notably, DoG outperforms the state-of-the-art method ToG
by 23.7\% and 9.1\% in accuracy on WebQuestions and GrailQA, respectively.
Furthermore, the integration experiments with various LLMs on the mentioned
datasets highlight the flexibility of DoG. Code is available at
\url{https://github.com/reml-group/DoG}.

æè¦ï¼<paragraph>å¤§åèªè¨æ¨¡å (LLM) ç±æ¼ç¼ºä¹ç¸éç¥è­ï¼å¨å¯¦éæç¨ä¸­å¯è½æç¢çå¹»è¦ºãç¸è¼ä¹ä¸ï¼ç¥è­åè­åå«å»£æ³çå¤ééä¿çµæ§ï¼å²å­å¤§éç¬¦èäºå¯¦ãå æ­¤ï¼å° LLM èç¥è­åè­æ´åå·²å»£æ³æ¢è¨ï¼å¶ä¸­ç¥è­åè­åé¡è§£ç­ (KGQA) æçºæ´åçéè¦è©¦éç³ãæ­¤ä»»åè¦æ± LLM ééå¾ç¥è­åè­ä¸­æ·åç¸éä¸åçµä¾åç­èªç¶èªè¨åé¡ãç¶èï¼ç¾ææ¹æ³é¢è¨å©é éå¤§ææ°ï¼\textit{éé·çæ¨çè·¯å¾æåæ£åç­ç¢ç}ï¼ä»¥å\textit{é¯èª¤æ­£åéä¿é»ç¤è·¯å¾ç²¾ç}ãå¨æ¬æä¸­ï¼æåæåºä¸ååè¦äºåç KGQA æ¡æ¶ï¼å®å©ç¨ LLM çäºåå­¸ç¿è½åä¾å·è¡æ¨çååå½¢è¾¯è« (DoG)ãå·é«ä¾èªªï¼DoG æ¡ç¨å­åèç¦æ©å¶ï¼åè¨± LLM å¨æ¯åæ¨çæ­¥é©å¾å·è¡ç­æ¡åè©¦ï¼å¾èæ¸è¼åé·æ¨çè·¯å¾çå½±é¿ãå¦ä¸æ¹é¢ï¼DoG å©ç¨å¤è§è²è¾¯è«å°çµéæ¼¸ç°¡åè¤éåé¡ï¼æ¸å°é¯èª¤æ­£åéä¿çå½±é¿ãéç¨®è¾¯è«æ©å¶ç¢ºä¿äºæ¨çéç¨çå¯é æ§ãå¨äºåå¬å±æ¸æéä¸çå¯¦é©çµæè­æäºæåæ¶æ§çæææ§ååªè¶æ§ãå¼å¾æ³¨æçæ¯ï¼DoG å¨ WebQuestions å GrailQA ä¸çæºç¢ºåº¦åå¥æ¯æåé²çæ¹æ³ ToG é«åº 23.7% å 9.1%ãæ­¤å¤ï¼å¨ä¸è¿°æ¸æéä¸èåç¨® LLM çæ´åå¯¦é©çªé¡¯äº DoG çéæ´»æ§ãç¨å¼ç¢¼å¯å¨\url{https://github.com/reml-group/DoG}åå¾ã</paragraph>

##### **Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification**
2409.02481v1 by Junyoung Lee, Ninad Dixit, Kaustav Chakrabarti, S. Supraja

Effective question classification is crucial for AI-driven educational tools,
enabling adaptive learning systems to categorize questions by skill area,
difficulty level, and competence. This classification not only supports
educational diagnostics and analytics but also enhances complex tasks like
information retrieval and question answering by associating questions with
relevant categories. Traditional methods, often based on word embeddings and
conventional classifiers, struggle to capture the nuanced relationships in
natural language, leading to suboptimal performance. To address this, we
propose a novel approach leveraging graph convolutional networks (GCNs), named
Phrase Question-Graph Convolutional Network (PQ-GCN) to better model the
inherent structure of questions. By representing questions as graphs -- where
nodes signify words or phrases and edges denote syntactic or semantic
relationships -- our method allows GCNs to learn from the interconnected nature
of language more effectively. Additionally, we explore the incorporation of
phrase-based features to enhance classification accuracy, especially in
low-resource settings. Our findings demonstrate that GCNs, augmented with these
features, offer a promising solution for more accurate and context-aware
question classification, bridging the gap between graph neural network research
and practical educational applications.

æè¦ï¼ææçåé¡åé¡å°æ¼ AI é©åçæè²å·¥å·è³ééè¦ï¼
è®é©ææ§å­¸ç¿ç³»çµ±è½ä¾ææè½é åã
é£åº¦ç­ç´åè½åå°åé¡é²è¡åé¡ãéç¨®åé¡ä¸åæ¯æ´
æè²è¨ºæ·ååæï¼éè½ééå°åé¡è
ç¸éé¡å¥éè¯èµ·ä¾ï¼å¢å¼·è³è¨æª¢ç´¢ååé¡è§£ç­ç­è¤éä»»åãå³çµ±æ¹æ³éå¸¸å»ºç«å¨è©åµå¥å
å³çµ±åé¡å¨ä¸ï¼é£ä»¥ææèªç¶èªè¨ä¸­çç´°å¾®éä¿ï¼å°è´æ¬¡ä½³æè½ãçºäºè§£æ±ºéååé¡ï¼æå
æåºäºä¸ç¨®åµæ°çæ¹æ³ï¼å©ç¨åå½¢å·ç©ç¶²è·¯ (GCN)ï¼ç¨±çº
Phrase Question-Graph Convolutional Network (PQ-GCN) ä¾æ´å¥½å°å»ºæ¨¡åé¡çå§å¨çµæ§ãééå°åé¡è¡¨ç¤ºçºåå½¢ââå¶ä¸­
ç¯é»è¡¨ç¤ºè©æè©çµï¼éç·£è¡¨ç¤ºèªæ³æèªç¾©éä¿ââæåçæ¨¡ååè¨± GCN æ´ææå°å¾èªè¨çç¸äºé£çµæ§è³ªä¸­å­¸ç¿ãæ­¤å¤ï¼æåæ¢ç´¢äºæ´å
åºæ¼è©çµçç¹å¾µä»¥å¢å¼·åé¡æºç¢ºåº¦ï¼ç¹å¥æ¯å¨
ä½è³æºè¨­å®ä¸­ãæåçç ç©¶çµæè¡¨æï¼GCN å¨éäº
ç¹å¾µçå¢å¼·ä¸ï¼çºæ´æºç¢ºä¸å·åæå¢æç¥è½åçåé¡åé¡æä¾äºä¸åæåéçè§£æ±ºæ¹æ¡ï¼ç¸®å°äºåå½¢ç¥ç¶ç¶²è·¯ç ç©¶
èå¯¦éæè²æç¨ä¹éçå·®è·ã

##### **Multi-modal Situated Reasoning in 3D Scenes**
2409.02389v1 by Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, Siyuan Huang

Situation awareness is essential for understanding and reasoning about 3D
scenes in embodied AI agents. However, existing datasets and benchmarks for
situated understanding are limited in data modality, diversity, scale, and task
scope. To address these limitations, we propose Multi-modal Situated Question
Answering (MSQA), a large-scale multi-modal situated reasoning dataset,
scalably collected leveraging 3D scene graphs and vision-language models (VLMs)
across a diverse range of real-world 3D scenes. MSQA includes 251K situated
question-answering pairs across 9 distinct question categories, covering
complex scenarios within 3D scenes. We introduce a novel interleaved
multi-modal input setting in our benchmark to provide text, image, and point
cloud for situation and question description, resolving ambiguity in previous
single-modality convention (e.g., text). Additionally, we devise the
Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models'
situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN
highlight the limitations of existing vision-language models and underscore the
importance of handling multi-modal interleaved inputs and situation modeling.
Experiments on data scaling and cross-domain transfer further demonstrate the
efficacy of leveraging MSQA as a pre-training dataset for developing more
powerful situated reasoning models.

æè¦ï¼æå¢æç¥å°æ¼çè§£åæ¨çå·èº« AI ä»£çä¸­ç 3D å ´æ¯è³ééè¦ãç¶èï¼ç¾æçè³æéååºæºå¨è³ææ¨¡æãå¤æ¨£æ§ãè¦æ¨¡åä»»åç¯åæ¹é¢å°æ¼æå¢çè§£ä¾èªªæ¯æéçãçºäºè§£æ±ºéäºéå¶ï¼æåæåºäºå¤æ¨¡ææå¢åç­ (MSQA)ï¼éæ¯ä¸åå¤§åå¤æ¨¡ææå¢æ¨çè³æéï¼å¯ééå©ç¨ 3D å ´æ¯ååè¦è¦ºèªè¨æ¨¡å (VLM) å¨åç¨®çå¯¦ä¸ç 3D å ´æ¯ä¸­é²è¡å¯æ´åæ¶éãMSQA åå« 251K åæå¢åç­å°ï¼æ¶µè 9 åä¸åçåé¡é¡å¥ï¼æ¶µè 3D å ´æ¯ä¸­çè¤éå ´æ¯ãæåå¨åºæºä¸­å¼å¥äºä¸ç¨®æ°ç©çäº¤é¯å¤æ¨¡æè¼¸å¥è¨­å®ï¼ä»¥æä¾æå­ãå½±ååé»é²ï¼ç¨æ¼æå¢ååé¡æè¿°ï¼è§£æ±ºä»¥åå®ä¸æ¨¡ææ£ä¾ï¼ä¾å¦æå­ï¼ä¸­çæ­§ç¾©ãæ­¤å¤ï¼æåè¨­è¨äºå¤æ¨¡ææå¢ä¸ä¸æ­¥å°èª (MSNN) åºæºï¼ä»¥è©ä¼°æ¨¡åçå°èªæå¢æ¨çãMSQA å MSNN çç¶åè©ä¼°çªé¡¯äºç¾æè¦è¦ºèªè¨æ¨¡åçéå¶ï¼ä¸¦å¼·èª¿äºèçå¤æ¨¡æäº¤é¯è¼¸å¥åæå¢å»ºæ¨¡çéè¦æ§ãè³ææ´ååè·¨é åè½ç§»çå¯¦é©é²ä¸æ­¥è­æäºå©ç¨ MSQA ä½çºé è¨ç·´è³æéä¾éç¼æ´å¼·å¤§çæå¢æ¨çæ¨¡åçæææ§ã

##### **Grounding Language Models in Autonomous Loco-manipulation Tasks**
2409.01326v1 by Jin Wang, Nikos Tsagarakis

Humanoid robots with behavioral autonomy have consistently been regarded as
ideal collaborators in our daily lives and promising representations of
embodied intelligence. Compared to fixed-based robotic arms, humanoid robots
offer a larger operational space while significantly increasing the difficulty
of control and planning. Despite the rapid progress towards general-purpose
humanoid robots, most studies remain focused on locomotion ability with few
investigations into whole-body coordination and tasks planning, thus limiting
the potential to demonstrate long-horizon tasks involving both mobility and
manipulation under open-ended verbal instructions. In this work, we propose a
novel framework that learns, selects, and plans behaviors based on tasks in
different scenarios. We combine reinforcement learning (RL) with whole-body
optimization to generate robot motions and store them into a motion library. We
further leverage the planning and reasoning features of the large language
model (LLM), constructing a hierarchical task graph that comprises a series of
motion primitives to bridge lower-level execution with higher-level planning.
Experiments in simulation and real-world using the CENTAURO robot show that the
language model based planner can efficiently adapt to new loco-manipulation
tasks, demonstrating high autonomy from free-text commands in unstructured
scenes.

æè¦ï¼å·æè¡çºèªä¸»æ¬çäººå½¢æ©å¨äººä¸ç´è¢«è¦çºæåæ¥å¸¸çæ´»ä¸­çæ³çåä½èï¼ä¹æ¯å·é«æºè½çæå¸æçä»£è¡¨ãèåºå®å¼æ©å¨æèç¸æ¯ï¼äººå½¢æ©å¨äººæä¾äºæ´å¤§çæä½ç©ºéï¼åæé¡¯èå¢å äºæ§å¶åè¦åçé£åº¦ãåç®¡æèéç¨äººå½¢æ©å¨äººå¿«éç¼å±ï¼ä½å¤§å¤æ¸ç ç©¶ä»ç¶éä¸­å¨éåè½åä¸ï¼å¾å°ç ç©¶å¨èº«åèª¿åä»»åè¦åï¼å¾èéå¶äºå±ç¤ºæ¶åç§»ååæä½çé·æä»»åçæ½åï¼åæéè½æ¥åéæ¾å¼å£é ­æä»¤ãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸åæ°çæ¡æ¶ï¼è©²æ¡æ¶å¯ä»¥æ ¹æä¸åå ´æ¯ä¸­çä»»åå­¸ç¿ãé¸æåè¦åè¡çºãæåå°å¼·åå­¸ç¿ (RL) èå¨èº«åªåç¸çµåï¼ä»¥çææ©å¨äººåä½ä¸¦å°å¶å­å²å°åä½åº«ä¸­ãæåé²ä¸æ­¥å©ç¨å¤§åèªè¨æ¨¡å (LLM) çè¦ååæ¨çåè½ï¼æ§å»ºäºä¸ååå±¤ä»»ååï¼å¶ä¸­åå«ä¸ç³»åéååèªï¼ä»¥æ©æ¥ä½ç´å·è¡åé«ç´è¦åãå¨æ¨¡æ¬åä½¿ç¨ CENTAURO æ©å¨äººçç¾å¯¦ä¸çä¸­çå¯¦é©è¡¨æï¼åºæ¼èªè¨æ¨¡åçè¦åå¨å¯ä»¥ææé©ææ°çéåæä½ä»»åï¼è­æäºå¨éçµæ§åå ´æ¯ä¸­å¾èªç±ææ¬å½ä»¤ä¸­ç²å¾çé«åº¦èªä¸»æ§ã

##### **LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning**
2409.01145v1 by Haoran Yang, Xiangyu Zhao, Sirui Huang, Qing Li, Guandong Xu

Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised
graph learning that has attracted attention across various application
scenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet
to be explored. Because conventional augmentation techniques like feature
embedding masking cannot directly process textual attributes on TAGs. A naive
strategy for applying GCL to TAGs is to encode the textual attributes into
feature embeddings via a language model and then feed the embeddings into the
following GCL module for processing. Such a strategy faces three key
challenges: I) failure to avoid information loss, II) semantic loss during the
text encoding phase, and III) implicit augmentation constraints that lead to
uncontrollable and incomprehensible results. In this paper, we propose a novel
GCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to
produce textual augmentations and LLMs' powerful natural language processing
(NLP) abilities to address the three limitations aforementioned to pave the way
for applying GCL to TAG tasks. Extensive experiments on four high-quality TAG
datasets illustrate the superiority of the proposed LATEX-GCL method. The
source codes and datasets are released to ease the reproducibility, which can
be accessed via this link: https://anonymous.4open.science/r/LATEX-GCL-0712.

æè¦ï¼åå½¢å°æ¯å­¸ç¿ (GCL) æ¯èªç£ç£åå½¢å­¸ç¿çå¼·å¤§ç¯ä¾ï¼å·²å¨åç¨®æç¨å ´æ¯ä¸­å¼èµ·éæ³¨ãç¶èï¼GCL å°æ¼å¨ææ¬è¨»è§£åå½¢ (TAG) ä¸å­¸ç¿å°æªè¢«æ¢è¨ãå çºç¹å¾µåµå¥é®ç½©ç­å³çµ±æ´åæè¡ç¡æ³ç´æ¥èç TAG ä¸çææ¬å±¬æ§ãå° GCL æç¨æ¼ TAG çä¸ç¨®å¤©çç­ç¥æ¯ééèªè¨æ¨¡åå°ææ¬å±¬æ§ç·¨ç¢¼å°ç¹å¾µåµå¥ä¸­ï¼ç¶å¾å°åµå¥è¼¸å¥å¾çºç GCL æ¨¡çµé²è¡èçãéç¨®ç­ç¥é¢è¨ä¸åééµææ°ï¼I) ç¡æ³é¿åè³è¨éºå¤±ï¼II) å¨ææ¬ç·¨ç¢¼éæ®µç¼çèªç¾©éºå¤±ï¼ä»¥å III) å°è´ç¡æ³æ§å¶ä¸é£ä»¥çè§£çµæçé±å¼æ´åç´æãå¨æ¬æä¸­ï¼æåæåºä¸ååçº LATEX-GCL çæ°ç© GCL æ¡æ¶ï¼å©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾ç¢çææ¬æ´åï¼ä»¥å LLM å¼·å¤§çèªç¶èªè¨èç (NLP) è½åä¾è§£æ±ºä¸è¿°ä¸åéå¶ï¼çºå° GCL æç¨æ¼ TAG ä»»åéªå¹³éè·¯ãå¨ååé«åè³ª TAG è³æéä¸çå¤§éå¯¦é©èªªæäºææåºç LATEX-GCL æ¹æ³çåªè¶æ§ãåå§ç¢¼åè³æéå·²ç¼å¸ä»¥ç°¡åå¯éè£½æ§ï¼å¯ééæ­¤é£çµå­åï¼https://anonymous.4open.science/r/LATEX-GCL-0712ã

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ç¶å¸¸ç¼ºä¹ç¹å®é åçç¥è­ï¼å³ä½¿ç¶éå¾®èª¿çæ¨¡åä¹å®¹æç¢çå¹»è¦ºãå æ­¤ï¼éè¦æ´å¤å¯é çæ¨¡åä¾ç´å¥å¤é¨ç¥è­ãæåæåºäºä¸åæµç¨ 4StepFocusï¼ç¹å¥æ¯é èçæ­¥é©ï¼å¯ä»¥å¤§å¹æ¹å LLM çç­æ¡ãéæ¯ééæä¾åå¼å°çå¤é¨ç¥è­å­åï¼å©ç¨æ¨¡åèªè¡æ·åéè¯æ§èçµ¡åé²è¡åºæ¬æ¨ççè½åä¾å¯¦ç¾çãæ­¤æ¹æ³ééå¨åçµæ§åç¥è­åº«ä¸­é²è¡åºæ¼ä¸åçµçæå°ï¼ä»¥ç´æ¥ä¸å¯è¿½è¹¤çæ¹å¼ç¸®å°æ½å¨æ­£ç¢ºç­æ¡çç¯åï¼ç¶å¾ååæå°æ½å¨è¡¨å¾µï¼æ ¹æéçµæ§åè³æå°éäºåé¸ç­æ¡é²è¡æåãéèç´ç²¹åºæ¼æ½å¨è¡¨å¾µçç¸éæ¹æ³ææåå¥ã4StepFocus åå«ä»¥ä¸æ­¥é©ï¼1) ç± LLM é²è¡ä¸åçµç¢çä»¥æ·åéè¯è³æï¼2) å¨éäºä¸åçµä¸­æ¿æè®æ¸ï¼ä»¥æ¡ç¨ç¥è­åè¡¨ç¸®å°ç­æ¡åé¸ç¯åï¼3) ä½¿ç¨æ¶åéè¯éçµæ§åè³æçåéç¸ä¼¼æ§æå°å°å©é¤åé¸ç­æ¡é²è¡æåºï¼4) ç± LLM éæ°å°æä½³åé¸ç­æ¡é²è¡æåï¼ä¸¦æä¾èæ¯è³æãå¨é«çãç¢åæ¨è¦åå­¸è¡è«ææå°æ¸¬è©¦éä¸­é²è¡çå¯¦é©è­æï¼éç¨®æ¹æ³ç¢ºå¯¦æ¯ä¸ç¨®å¼·å¤§çæ´åãå®ä¸åå¢å äºä¾èªè³è¨æª¢ç´¢çç¸å³å¯è¿½è¹¤èæ¯è³è¨ï¼èä¸èæåé²çæ¹æ³ç¸æ¯ï¼ä¹å¤§å¹æåäºæè½ãæ¬ææåºäºä¸åæ°ç©ä¸é®®å°æ¢ç´¢çæ¹åï¼å æ­¤æä¾äºå»£æ³çæªä¾å·¥ä½æ©æãä½¿ç¨çåå§ç¢¼å¯å¨ https://github.com/kramerlab/4StepFocus åå¾ã

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

æè¦ï¼æ¬ææåºäºä¸åç¥è­å·¥ç¨åå¤èªè¨èªç¾©æ¨çæè¡çæ¬ä½è¨­è¨ï¼ç¨æ¼å»ºç«ä¸åèªååç³»çµ±ï¼ä»¥ç¥è­åè­çå½¢å¼å¸æ¶å°åº¦æççç¹é£ªè³è¨ãéé»å¨æ¼è¨­è¨æºæ§æ¹æ³ï¼ä»¥æ¨å°æ¬ä½è¨­è¨ï¼ä¸¦å¨é¢æ·åéæ¼é£ç©ãé£è­ãé£æãç¹é£ªç¹æ§ï¼ä»¥åæéè¦ççé¤çç¥è­ï¼ä¸¦æ´å¤§è¦æ¨¡ãæåå¨éåç è¨æè«æä¸­ä»ç´¹äºæåæ­£å¨é²è¡çå·¥ä½ï¼è©³ç´°æè¿°äºæ´çå°åº¦æçç¥è­ç¸éçææ°ï¼ä¸¦æåºäºæåçé«éæ¬ä½è¨­è¨ãæåä¹æåºäºä¸ç¨®æ°çå·¥ä½æµç¨ï¼å®ä½¿ç¨ AIãLLM åèªè¨æè¡ï¼å¾å¬å±é åçé£è­é¨è½æ ¼ç¶²ç«ä¸­æ´çè³è¨ï¼ä»¥å»ºç«å°åº¦æççç¥è­åè­ãæ¬ææåºçç¥è­æ´çæ¹æ³æ¯éç¨çï¼å¯ä»¥è¤è£½å°ä»»ä½é åãè¨­è¨èæç¨ç¡éï¼å¯ç¨æ¼ AI é©åçæºæ§åæãå»ºç«åäººåæ¸ä½å¥åº·æ¨è¦ç³»çµ±ï¼ä»¥åä½¿ç¨ä½¿ç¨èè³è¨ãé£ç©çç©åå­¸ãå°çè³è¨ãè¾²æ¥­è³è¨ç­èçµ¡è³è¨ï¼ä¾è£åå°åº¦æççç¥è­åè­ã

##### **Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph**
2409.00727v1 by Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuanhui Yang, Yuanyuan Zhu, Chuang Hu, Bo Du, Jiawei Jiang

Text-attributed graph (TAG) is an important type of graph structured data
with text descriptions for each node. Few- and zero-shot node classification on
TAGs have many applications in fields such as academia and social networks.
However, the two tasks are challenging due to the lack of supervision signals,
and existing methods only use the contrastive loss to align graph-based node
embedding and language-based text embedding. In this paper, we propose Hound to
improve accuracy by introducing more supervision signals, and the core idea is
to go beyond the node-text pairs that come with data. Specifically, we design
three augmentation techniques, i.e., node perturbation, text matching, and
semantics negation to provide more reference nodes for each text and vice
versa. Node perturbation adds/drops edges to produce diversified node
embeddings that can be matched with a text. Text matching retrieves texts with
similar embeddings to match with a node. Semantics negation uses a negative
prompt to construct a negative text with the opposite semantics, which is
contrasted with the original node and text. We evaluate Hound on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that Hound
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.

æè¦ï¼æå­å±æ§å (TAG) æ¯ä¸ç¨®éè¦çåå½¢çµæ§åè³æé¡åï¼å¶ä¸­æ¯åç¯é»é½ææå­æè¿°ãTAG ä¸çå°æ¨£æ¬åé¶æ¨£æ¬ç¯é»åé¡å¨å­¸è¡çåç¤¾äº¤ç¶²è·¯ç­é åæè¨±å¤æç¨ãç¶èï¼ç±æ¼ç¼ºä¹ç£ç£è¨èï¼éå©åä»»åå·æææ°æ§ï¼ç¾ææ¹æ³åä½¿ç¨å°æ¯æå¤±ä¾å°é½åºæ¼åå½¢ç¯é»çåµå¥ååºæ¼èªè¨çæå­åµå¥ãå¨æ¬æä¸­ï¼æåæåº Hound ä¾ééå¼å¥æ´å¤ç£ç£è¨èä¾æ¹åæºç¢ºåº¦ï¼å¶æ ¸å¿ææ³æ¯è¶è¶è³æä¸­éå¸¶çç¯é»æå­å°ãå·é«ä¾èªªï¼æåè¨­è¨äºä¸ç¨®æ´åæè¡ï¼å³ç¯é»æ¾åãæå­éå°åèªç¾©å¦å®ï¼çºæ¯åæå­æä¾æ´å¤åèç¯é»ï¼åä¹äº¦ç¶ãç¯é»æ¾åæ°å¢/åªé¤éç·£ä»¥ç¢çå¯ä»¥èæå­éå°çå¤æ¨£åç¯é»åµå¥ãæå­éå°æ·åå·æé¡ä¼¼åµå¥çæå­ä»¥èç¯é»éå°ãèªç¾©å¦å®ä½¿ç¨è² é¢æç¤ºä¾å»ºæ§å·æç¸åèªç¾©çè² é¢æå­ï¼èåå§ç¯é»åæå­å½¢æå°æ¯ãæåå¨ 5 åè³æéä¸è©ä¼° Houndï¼ä¸¦è 13 åæåé²çåºç·é²è¡æ¯è¼ãçµæè¡¨æï¼Hound å¨ææåºç·ä¸å§çµè¡¨ç¾åªç°ï¼å¶æºç¢ºåº¦éå¸¸æ¯æè½æä½³çåºç·æé«äº 5% ä»¥ä¸ã

##### **WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction**
2409.00331v1 by Oktie Hassanzadeh

Recently, there has been an increasing interest in the construction of
general-domain and domain-specific causal knowledge graphs. Such knowledge
graphs enable reasoning for causal analysis and event prediction, and so have a
range of applications across different domains. While great progress has been
made toward automated construction of causal knowledge graphs, the evaluation
of such solutions has either focused on low-level tasks (e.g., cause-effect
phrase extraction) or on ad hoc evaluation data and small manual evaluations.
In this paper, we present a corpus, task, and evaluation framework for causal
knowledge graph construction. Our corpus consists of Wikipedia articles for a
collection of event-related concepts in Wikidata. The task is to extract causal
relations between event concepts from the corpus. The evaluation is performed
in part using existing causal relations in Wikidata to measure recall, and in
part using Large Language Models to avoid the need for manual or crowd-sourced
evaluation. We evaluate a pipeline for causal knowledge graph construction that
relies on neural models for question answering and concept linking, and show
how the corpus and the evaluation framework allow us to effectively find the
right model for each task. The corpus and the evaluation framework are publicly
available.

æè¦ï¼<paragraph>æè¿ï¼äººåå°éç¨é ååç¹å®é åå æç¥è­åè­çå»ºæ§è¶ä¾è¶æèè¶£ãæ­¤é¡ç¥è­åè­è½å¤ æ¨çå æåæåäºä»¶é æ¸¬ï¼å æ­¤å¨ä¸åé åä¸­æå»£æ³çæç¨ãéç¶å¨å æç¥è­åè­çèªåå»ºæ§æ¹é¢åå¾äºéå¤§é²å±ï¼ä½æ­¤é¡è§£æ±ºæ¹æ¡çè©ä¼°è¦åèéæ¼ä½éä»»åï¼ä¾å¦å æéä¿ç­èªæ·åï¼ï¼è¦åèéæ¼è¨æè©ä¼°è³æåå°åæåè©ä¼°ãå¨æ¬æä¸­ï¼æåæåºäºä¸åèªæåº«ãä»»ååå æç¥è­åè­å»ºæ§è©ä¼°æ¶æ§ãæåçèªæåº«åå«ç¶­åºç¾ç§æç« ï¼å¶ä¸­åå« Wikidata ä¸­ä¸ç³»åäºä»¶ç¸éæ¦å¿µãä»»åæ¯å¾èªæåº«ä¸­æ·åäºä»¶æ¦å¿µä¹éçå æéä¿ãè©ä¼°é¨åä½¿ç¨ Wikidata ä¸­ç¾æçå æéä¿ä¾è¡¡éå¬åçï¼é¨åä½¿ç¨å¤§åèªè¨æ¨¡åä¾é¿åæåæç¾¤ç¾å¤åè©ä¼°çéè¦ãæåè©ä¼°äºä¸åå æç¥è­åè­å»ºæ§ç®¡éï¼è©²ç®¡éä¾è³´æ¼ç¨æ¼åç­åæ¦å¿µé£çµçç¥ç¶æ¨¡åï¼ä¸¦å±ç¤ºäºèªæåº«åè©ä¼°æ¶æ§å¦ä½è®æåææå°çºæ¯åä»»åæ¾å°åé©çæ¨¡åãèªæåº«åè©ä¼°æ¶æ§å¬éæä¾ã</paragraph>

##### **LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**
2408.16224v2 by Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng

Recent advances in large vision-language models (VLMs) typically employ
vision encoders based on the Vision Transformer (ViT) architecture. The
division of the images into patches by ViT results in a fragmented perception,
thereby hindering the visual understanding capabilities of VLMs. In this paper,
we propose an innovative enhancement to address this limitation by introducing
a Scene Graph Expression (SGE) module in VLMs. This module extracts and
structurally expresses the complex semantic information within images, thereby
improving the foundational perception and understanding abilities of VLMs.
Extensive experiments demonstrate that integrating our SGE module significantly
enhances the VLM's performance in vision-language tasks, indicating its
effectiveness in preserving intricate semantic details and facilitating better
visual understanding.

æè¦ï¼è¿ä¾å¤§åè¦è¦ºèªè¨æ¨¡å (VLM) çé²å±éå¸¸æ¡ç¨åºæ¼è¦è¦ºè½æå¨ (ViT) æ¶æ§çè¦è¦ºç·¨ç¢¼å¨ãViT å°å½±ååå²æåå¡æé æç ´ç¢çæç¥ï¼å¾èé»ç¤ VLM çè¦è¦ºçè§£è½åãå¨æ¬æä¸­ï¼æåæåºäºä¸é åµæ°çå¢å¼·åè½ï¼ééå¨ VLM ä¸­å¼å¥å ´æ¯åè¡¨é (SGE) æ¨¡çµä¾è§£æ±ºæ­¤éå¶ãæ­¤æ¨¡çµæèåå½±åä¸­çè¤éèªæè³è¨ä¸¦ä»¥çµæ§åçæ¹å¼è¡¨éï¼å¾èæ¹å VLM çåºç¤æç¥åçè§£è½åãå»£æ³çå¯¦é©è­æï¼æ´åæåç SGE æ¨¡çµè½é¡¯èæå VLM å¨è¦è¦ºèªè¨ä»»åä¸­çæè½ï¼è¡¨ç¤ºå®å¨ä¿çè¤éçèªæç´°ç¯åä¿é²æ´å¥½çè¦è¦ºçè§£æ¹é¢å¾ææã

##### **LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**
2408.15903v1 by Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai

The rapid obsolescence of information in Large Language Models (LLMs) has
driven the development of various techniques to incorporate new facts. However,
existing methods for knowledge editing still face difficulties with multi-hop
questions that require accurate fact identification and sequential logical
reasoning, particularly among numerous fact updates. To tackle these
challenges, this paper introduces Graph Memory-based Editing for Large Language
Models (GMeLLo), a straitforward and effective method that merges the explicit
knowledge representation of Knowledge Graphs (KGs) with the linguistic
flexibility of LLMs. Beyond merely leveraging LLMs for question answering,
GMeLLo employs these models to convert free-form language into structured
queries and fact triples, facilitating seamless interaction with KGs for rapid
updates and precise multi-hop reasoning. Our results show that GMeLLo
significantly surpasses current state-of-the-art knowledge editing methods in
the multi-hop question answering benchmark, MQuAKE, especially in scenarios
with extensive knowledge edits.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ä¸­è³è¨å¿«ééæï¼ä¿ä½¿åç¨®æè¡ç¼å±ä»¥ç´å¥æ°äºå¯¦ãç¶èï¼ç¾æçç¥è­ç·¨è¼¯æ¹æ³å¨éè¦æºç¢ºäºå¯¦è¾¨è­åé åºéè¼¯æ¨ççå¤è·³åé¡ä¸ä»é¢è¨å°é£ï¼ç¹å¥æ¯å¨ç¾å¤äºå¯¦æ´æ°ä¸­ãçºäºæå°éäºææ°ï¼æ¬æä»ç´¹äºå¤§åèªè¨æ¨¡åçåè¨æ¶ç·¨è¼¯ (GMeLLo)ï¼éæ¯ä¸ç¨®ç´æ¥ä¸ææçæ¹æ³ï¼çµåäºç¥è­åè­ (KG) çæç¢ºç¥è­è¡¨ç¤ºè LLM çèªè¨éæ´»æ§ãGMeLLo ä¸åå©ç¨ LLM ä¾åç­åé¡ï¼éä½¿ç¨éäºæ¨¡åå°èªç±å½¢å¼çèªè¨è½æçºçµæ§åæ¥è©¢åäºå¯¦ä¸åçµï¼ä¿é²è KG çç¡ç¸«äºåï¼ä»¥ä¾¿å¿«éæ´æ°åç²¾ç¢ºçå¤è·³æ¨çãæåççµæè¡¨æï¼å¨å¤è·³åé¡åç­åºæº MQuAKE ä¸­ï¼GMeLLo æé¡¯è¶è¶äºç¶åæåé²çç¥è­ç·¨è¼¯æ¹æ³ï¼ç¹å¥æ¯å¨å»£æ³ç¥è­ç·¨è¼¯çå ´æ¯ä¸­ã

##### **VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**
2408.14895v2 by Shusaku Egami, Takahiro Ugai, Swe Nwe Nwe Htun, Ken Fukuda

Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data
(e.g., images and videos) into symbols, have attracted attention as resources
enabling knowledge processing and machine learning across modalities. However,
the construction of MMKGs for videos consisting of multiple events, such as
daily activities, is still in the early stages. In this paper, we construct an
MMKG based on synchronized multi-view simulated videos of daily activities.
Besides representing the content of daily life videos as event-centric
knowledge, our MMKG also includes frame-by-frame fine-grained changes, such as
bounding boxes within video frames. In addition, we provide support tools for
querying our MMKG. As an application example, we demonstrate that our MMKG
facilitates benchmarking vision-language models by providing the necessary
vision-language datasets for a tailored task.

æè¦ï¼å¤æ¨¡æç¥è­åï¼MMKGï¼å°åç¨®éç¬¦èæ¸æï¼ä¾å¦ï¼å½±ååå½±çï¼è½æçºç¬¦èï¼æçºä¸ç¨®è³æºï¼è½è®è·¨æ¨¡æçç¥è­èçåæ©å¨å­¸ç¿æçºå¯è½ãç¶èï¼å°æ¼åå«å¤åäºä»¶ï¼ä¾å¦æ¥å¸¸çæ´»æ´»åï¼çå½±çï¼å¶ MMKG çå»ºæ§ä»èæ¼æ©æéæ®µãå¨æ¬æä¸­ï¼æååºæ¼æ¯æ¥æ´»åçåæ­¥å¤è¦è§æ¨¡æ¬å½±çï¼å»ºæ§äºä¸å MMKGãé¤äºå°æ¥å¸¸çæ´»å½±ççå§å®¹è¡¨ç¤ºçºä»¥äºä»¶çºä¸­å¿çç¥è­å¤ï¼æåç MMKG ä¹åå«éå¹çç´°å¾®è®åï¼ä¾å¦å½±çå¹ä¸­çéçæ¡ãæ­¤å¤ï¼æåéæä¾äºç¨æ¼æ¥è©¢ MMKG çæ¯æ´å·¥å·ãä½çºæç¨ç¯ä¾ï¼æåå±ç¤ºäºæåç MMKG å¦ä½ééæä¾ç¹å®ä»»åæéçè¦è¦ºèªè¨è³æéï¼ä¾ä¿é²è¦è¦ºèªè¨æ¨¡åçåºæºæ¸¬è©¦ã

##### **XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model**
2408.16021v1 by Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian

In the rapidly evolving field of cybersecurity, the integration of flow-level
and packet-level information for real-time intrusion detection remains a
largely untapped area of research. This paper introduces "XG-NID," a novel
framework that, to the best of our knowledge, is the first to fuse flow-level
and packet-level data within a heterogeneous graph structure, offering a
comprehensive analysis of network traffic. Leveraging a heterogeneous graph
neural network (GNN) with graph-level classification, XG-NID uniquely enables
real-time inference while effectively capturing the intricate relationships
between flow and packet payload data. Unlike traditional GNN-based
methodologies that predominantly analyze historical data, XG-NID is designed to
accommodate the heterogeneous nature of network traffic, providing a robust and
real-time defense mechanism. Our framework extends beyond mere classification;
it integrates Large Language Models (LLMs) to generate detailed, human-readable
explanations and suggest potential remedial actions, ensuring that the insights
produced are both actionable and comprehensible. Additionally, we introduce a
new set of flow features based on temporal information, further enhancing the
contextual and explainable inferences provided by our model. To facilitate
practical application and accessibility, we developed "GNN4ID," an open-source
tool that enables the extraction and transformation of raw network traffic into
the proposed heterogeneous graph structure, seamlessly integrating flow and
packet-level data. Our comprehensive quantitative comparative analysis
demonstrates that XG-NID achieves an F1 score of 97\% in multi-class
classification, outperforming existing baseline and state-of-the-art methods.
This sets a new standard in Network Intrusion Detection Systems by combining
innovative data fusion with enhanced interpretability and real-time
capabilities.

æè¦ï¼<paragraph>å¨å¿«éç¼å±çç¶²è·¯å®å¨é åä¸­ï¼æ´åæµå±¤ç´åå°åå±¤ç´è³è¨ä»¥é²è¡å³æå¥ä¾µåµæ¸¬ï¼ä»ç¶æ¯ä¸åå°æªéç¼çç ç©¶é åãæ¬æä»ç´¹ãXG-NIDãï¼ä¸ååµæ°çæ¶æ§ï¼ææåæç¥ï¼éæ¯ç¬¬ä¸åå¨ç°è³ªåå½¢çµæ§ä¸­èåæµå±¤ç´åå°åå±¤ç´è³æçæ¶æ§ï¼æä¾å°ç¶²è·¯æµéçå¨é¢åæãééå©ç¨ç°è³ªåå½¢ç¥ç¶ç¶²è·¯ (GNN) ååå½¢å±¤ç´åé¡ï¼XG-NID ç¨ç¹å°å¯¦ç¾å³ææ¨è«ï¼åææææ·åæµåå°åé¬è¼è³æä¹éçè¤ééä¿ãèå³çµ±åºæ¼ GNN çæ¹æ³ï¼ä¸»è¦åææ­·å²è³æï¼ä¸åï¼XG-NID è¢«è¨­è¨æé©æç¶²è·¯æµéçç°è³ªæ§ï¼æä¾å¼·å¤§ä¸å³æçé²ç¦¦æ©å¶ãæåçæ¶æ§ä¸åéæ¼åé¡ï¼å®æ´åå¤§åèªè¨æ¨¡å (LLM) ä»¥ç¢çè©³ç´°ãäººé¡å¯è®çè§£éä¸¦å»ºè­°æ½å¨çè£ææªæ½ï¼ç¢ºä¿ç¢ççè¦è§£æ¢å¯æä½åææ¼çè§£ãæ­¤å¤ï¼æåæ ¹ææéè³è¨å¼å¥ä¸çµæ°çæµç¹å¾µï¼é²ä¸æ­¥å¢å¼·æ¨¡åæä¾çèçµ¡åå¯è§£éæ¨è«ãçºäºä¿é²å¯¦éæç¨åå¯åæ§ï¼æåéç¼äºãGNN4IDãï¼ä¸åéæ¾åå§ç¢¼å·¥å·ï¼å¯ä»¥å°åå§ç¶²è·¯æµéæåä¸¦è½æçºå»ºè­°çç°è³ªåå½¢çµæ§ï¼ç¡ç¸«æ´åæµåå°åå±¤ç´è³æãæåå¨é¢çå®éæ¯è¼åæè¡¨æï¼XG-NID å¨å¤é¡å¥åé¡ä¸­éå° 97% ç F1 åæ¸ï¼åªæ¼ç¾æçåºæºåæåé²çæ¹æ³ãéééçµååµæ°çè³æèåãå¢å¼·çå¯è§£éæ§åå³æåè½ï¼å¨ç¶²è·¯å¥ä¾µåµæ¸¬ç³»çµ±ä¸­æ¨¹ç«äºæ°çæ¨æºã</paragraph>

##### **Process Trace Querying using Knowledge Graphs and Notation3**
2409.04452v1 by William Van Woensel

In process mining, a log exploration step allows making sense of the event
traces; e.g., identifying event patterns and illogical traces, and gaining
insight into their variability. To support expressive log exploration, the
event log can be converted into a Knowledge Graph (KG), which can then be
queried using general-purpose languages. We explore the creation of semantic KG
using the Resource Description Framework (RDF) as a data model, combined with
the general-purpose Notation3 (N3) rule language for querying. We show how
typical trace querying constraints, inspired by the state of the art, can be
implemented in N3. We convert case- and object-centric event logs into a
trace-based semantic KG; OCEL2 logs are hereby "flattened" into traces based on
object paths through the KG. This solution offers (a) expressivity, as queries
can instantiate constraints in multiple ways and arbitrarily constrain
attributes and relations (e.g., actors, resources); (b) flexibility, as OCEL2
event logs can be serialized as traces in arbitrary ways based on the KG; and
(c) extensibility, as others can extend our library by leveraging the same
implementation patterns.

æè¦ï¼å¨æµç¨ææä¸­ï¼æ¥å¿æ¢ç´¢æ­¥éª¤å¯ä»¥çè§£äºä»¶è½¨è¿¹ï¼ä¾å¦ï¼è¯å«äºä»¶æ¨¡å¼åéé»è¾è½¨è¿¹ï¼å¹¶æ·±å¥äºè§£å¶å¯åæ§ãä¸ºäºæ¯æè¡¨è¾¾æ§æ¥å¿æ¢ç´¢ï¼äºä»¶æ¥å¿å¯ä»¥è½¬æ¢ä¸ºç¥è¯å¾ (KG)ï¼ç¶åå¯ä»¥ä½¿ç¨éç¨è¯­è¨å¯¹å¶è¿è¡æ¥è¯¢ãæä»¬æ¢ç´¢ä½¿ç¨èµæºæè¿°æ¡æ¶ (RDF) ä½ä¸ºæ°æ®æ¨¡ååå»ºè¯­ä¹ KGï¼å¹¶ç»åéç¨ Notation3 (N3) è§åè¯­è¨è¿è¡æ¥è¯¢ãæä»¬å±ç¤ºäºå¦ä½ä½¿ç¨ N3 å®ç°åç°æææ¯å¯åçå¸åè½¨è¿¹æ¥è¯¢çº¦æãæä»¬å°æ¡ä¾åå¯¹è±¡ä¸­å¿äºä»¶æ¥å¿è½¬æ¢ä¸ºåºäºè½¨è¿¹çè¯­ä¹ KGï¼OCEL2 æ¥å¿å¨æ­¤è¢«âæå¹³åâä¸ºåºäºéè¿ KG çå¯¹è±¡è·¯å¾çè½¨è¿¹ãæ­¤è§£å³æ¹æ¡æä¾ (a) è¡¨è¾¾åï¼å ä¸ºæ¥è¯¢å¯ä»¥ä»¥å¤ç§æ¹å¼å®ä¾åçº¦æå¹¶ä»»æçº¦æå±æ§åå³ç³»ï¼ä¾å¦ï¼åä¸èãèµæºï¼ï¼(b) çµæ´»ï¼å ä¸º OCEL2 äºä»¶æ¥å¿å¯ä»¥åºäº KG ä»¥ä»»ææ¹å¼åºååä¸ºè½¨è¿¹ï¼ä»¥å (c) å¯æ©å±æ§ï¼å ä¸ºå¶ä»äººå¯ä»¥éè¿å©ç¨ç¸åçå®ç°æ¨¡å¼æ¥æ©å±æä»¬çåºã

##### **PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method**
2409.00092v1 by Runtao Ren, Jian Ma

As humanity stands on the brink of a new era of technological innovation, the
ability to rapidly transform creative ideas into protected intellectual
property (IP) is more crucial than ever. However, the conventional processes
for patent drafting are fraught with challenges, demanding a nuanced
understanding of advanced field knowledge and technical concepts. Existing
large language models (LLMs), while powerful, often fall short in this IP
creation domain due to their lack of specialized knowledge and
context-awareness necessary for generating technically accurate patent
documents. To bridge this critical gap, we propose a groundbreaking framework
for Knowledge Fine-Tuning (KFT) of LLMs, designed to endow AI with the ability
to autonomously mine, understand, and apply domain-specific knowledge. Our
model, PatentGPT leverages a unique combination of knowledge graph-based
pre-training, domain-specific supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF). Through extensive evaluation, PatentGPT
has demonstrated outstanding performance, scoring up to approximately 400%
higher in patent related benchmark tests compared to state-of-the-art models.
By KFT method the model's capability to not only assist but also augment human
creativity and innovation, our approach sets a new standard for AI-driven
intellectual property generation, paving the way for more efficient and
effective invention processes.

æè¦ï¼<paragraph>é¨èäººé¡éå¥ç§æåµæ°çæ°ç´åï¼è¿éå°åµæé»å­è½åçºåä¿è­·çæºæ§è²¡ç¢ï¼IPï¼çè½åæ¯ä»¥å¾ä»»ä½æåé½æ´å éè¦ãç¶èï¼å³çµ±çå°å©èµ·èç¨åºåæ»¿ææ°ï¼éè¦å°åé²é åç¥è­åæè¡æ¦å¿µæç´°ç·»å¥å¾®çäºè§£ãç¾æçå¤§åèªè¨æ¨¡åï¼LLMï¼éç¶å¼·å¤§ï¼ä½ç±æ¼ç¼ºä¹ç¢çæè¡ä¸æºç¢ºçå°å©æä»¶çå°æ¥­ç¥è­åæå¢æè­ï¼å æ­¤å¸¸å¸¸ç¡æ³æ»¿è¶³æ­¤ IP åµä½é åçéæ±ãçºäºå½è£éåééµå·®è·ï¼æåæåºäºä¸ååµæ°ç LLM ç¥è­å¾®èª¿ (KFT) æ¶æ§ï¼æ¨å¨è³¦äº AI èªä¸»ææãçè§£åæç¨ç¹å®é åç¥è­çè½åãæåçæ¨¡å PatentGPT ååå©ç¨äºåºæ¼ç¥è­åè¡¨çé è¨ç·´ãç¹å®é åçç£ç£å¼å¾®èª¿ (SFT) åäººé¡åé¥çå¼·åå­¸ç¿ (RLHF) çç¨ç¹çµåãééå»£æ³çè©ä¼°ï¼PatentGPT å·²å±ç¾åºååºçè¡¨ç¾ï¼å¨èæåé²æ¨¡åç¸æ¯çå°å©ç¸éåºæºæ¸¬è©¦ä¸­ï¼å¾åé«åºç´ 400%ãéé KFT æ¹æ³ï¼æ­¤æ¨¡åä¸åè½å¤ åå©ï¼éè½æ´å¢äººé¡çåµé åååµæ°åï¼æåçåæ³çº AI é©åçæºæ§è²¡ç¢çææ¨¹ç«äºæ°æ¨æºï¼çºæ´ææçä¸æ´ææçç¼ææµç¨éªè·¯ã</paragraph>

##### **DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**
2408.14185v1 by Ziai Zhou, Bin Zhou, Hao Liu

Real-time dynamic path planning in complex traffic environments presents
challenges, such as varying traffic volumes and signal wait times. Traditional
static routing algorithms like Dijkstra and A* compute shortest paths but often
fail under dynamic conditions. Recent Reinforcement Learning (RL) approaches
offer improvements but tend to focus on local optima, risking dead-ends or
boundary issues. This paper proposes a novel approach based on causal inference
for real-time dynamic path planning, balancing global and local optimality. We
first use the static Dijkstra algorithm to compute a globally optimal baseline
path. A distributed control strategy then guides vehicles along this path. At
intersections, DynamicRouteGPT performs real-time decision-making for local
path selection, considering real-time traffic, driving preferences, and
unexpected events. DynamicRouteGPT integrates Markov chains, Bayesian
inference, and large-scale pretrained language models like Llama3 8B to provide
an efficient path planning solution. It dynamically adjusts to traffic
scenarios and driver preferences and requires no pre-training, offering broad
applicability across road networks. A key innovation is the construction of
causal graphs for counterfactual reasoning, optimizing path decisions.
Experimental results show that our method achieves state-of-the-art performance
in real-time dynamic path planning for multiple vehicles while providing
explainable path selections, offering a novel and efficient solution for
complex traffic environments.

æè¦ï¼å¨è¤éäº¤éç°å¢ä¸­é²è¡å¯¦æåæè·¯å¾è¦åæé¢è¨ææ°ï¼ä¾å¦äº¤éæµéè®ååä¿¡èç­å¾æéãå³çµ±çéæè·¯ç±æ¼ç®æ³ï¼ä¾å¦ Dijkstra å A*ï¼æè¨ç®æç­è·¯å¾ï¼ä½éå¸¸å¨åææ¢ä»¶ä¸æå¤±æãæè¿çå¼·åå­¸ç¿ (RL) æ¹æ³æä¾äºæ¹é²ï¼ä½å¾åæ¼éæ³¨å±é¨æåªï¼åèé·å¥æ­»è¡åæéçåé¡çé¢¨éªãæ¬ææåºäºä¸ç¨®åºæ¼å ææ¨è«çæ°ç©æ¹æ³ï¼ç¨æ¼å¯¦æåæè·¯å¾è¦åï¼å¹³è¡¡å¨å±åå±é¨æåªæ§ãæåé¦åä½¿ç¨éæ Dijkstra æ¼ç®æ³è¨ç®å¨å±æåªåºç·è·¯å¾ãç¶å¾ï¼ä¸ååå¸å¼æ§å¶ç­ç¥æ²¿èéæ¢è·¯å¾å¼å°è»è¼ãå¨äº¤åè·¯å£ï¼DynamicRouteGPT éå°å±é¨è·¯å¾é¸æå·è¡å¯¦ææ±ºç­ï¼èéå¯¦æäº¤éãé§é§åå¥½åæå¤äºä»¶ãDynamicRouteGPT æ´åäºé¦¬å¯å¤«éãè²æ°æ¨è«å Llama3 8B ç­å¤§è¦æ¨¡é åè¨ç·´çèªè¨æ¨¡åï¼ä»¥æä¾ææçè·¯å¾è¦åè§£æ±ºæ¹æ¡ãå®æåæèª¿æ´å°äº¤éçæ³åé§é§åå¥½ï¼ä¸¦ä¸ä¸éè¦é åè¨ç·´ï¼å¨éè·¯ç¶²è·¯ä¸æä¾å»£æ³çé©ç¨æ§ãä¸åééµåµæ°æ¯å»ºç«åäºå¯¦æ¨ççå æåï¼ä»¥æä½³åè·¯å¾æ±ºç­ãå¯¦é©çµæé¡¯ç¤ºï¼æåçæ¨¡åå¨å¤è¼è»è¼çå¯¦æåæè·¯å¾è¦åä¸­éå°æåé²çæè½ï¼åææä¾å¯è§£éçè·¯å¾é¸æï¼çºè¤éçäº¤éç°å¢æä¾ä¸ç¨®æ°ç©ä¸ææçè§£æ±ºæ¹æ¡ã

##### **Exploring the Potential of Large Language Models for Heterophilic Graphs**
2408.14134v1 by Yuxia Wu, Shujie Li, Yuan Fang, Chuan Shi

Graph Neural Networks (GNNs) are essential for various graph-based learning
tasks. Notably, classical GNN architectures operate under the assumption of
homophily, which posits that connected nodes are likely to share similar
features. However, this assumption limits the effectiveness of GNNs in handling
heterophilic graphs where connected nodes often exhibit dissimilar
characteristics. Existing approaches for homophily graphs such as non-local
neighbor extension and architectural refinement overlook the rich textual data
associated with nodes, which could unlock deeper insights into these
heterophilic contexts. With advancements in Large Language Models (LLMs), there
is significant promise to enhance GNNs by leveraging the extensive open-world
knowledge within LLMs to more effectively interpret and utilize textual data
for characterizing heterophilic graphs. In this work, we explore the potential
of LLMs for modeling heterophilic graphs and propose a novel two-stage
framework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.
Specifically, in the first stage, we fine-tune the LLM to better identify
homophilic and heterophilic edges based on the textual information of their
nodes. In the second stage, we adaptively manage message propagation in GNNs
for different edge types based on node features, structures, and heterophilic
or homophilic characteristics. To cope with the computational demands when
deploying LLMs in practical scenarios, we further explore model distillation
techniques to fine-tune smaller, more efficient models that maintain
competitive performance. Extensive experiments validate the effectiveness of
our framework, demonstrating the feasibility of using LLMs to enhance GNNs for
node classification on heterophilic graphs.

æè¦ï¼åç¥ç¶ç¶²è·¯ (GNN) å°æ¼åç¨®åºæ¼åå½¢çå­¸ç¿ä»»åè³ééè¦ãå¼å¾æ³¨æçæ¯ï¼å³çµ±ç GNN æ¶æ§å¨åè³ªæ§çåè¨­ä¸éä½ï¼è©²åè¨­èªçºé£æ¥çç¯é»å¯è½å±äº«é¡ä¼¼çç¹å¾µãç¶èï¼æ­¤åè¨­éå¶äº GNN å¨èçç°è³ªæ§åå½¢ä¸­çæè½ï¼å¶ä¸­é£æ¥çç¯é»éå¸¸è¡¨ç¾åºä¸åçç¹å¾µãç¾æçåè³ªæ§åå½¢æ¹æ³ï¼ä¾å¦éå±é¨é°åå»¶ä¼¸åæ¶æ§æ¹é²ï¼å¿½ç¥äºèç¯é»ç¸éçè±å¯ææ¬è³æï¼éå¯ä»¥æ·±å¥äºè§£éäºç°è³ªæ§èçµ¡ãé¨èå¤§åèªè¨æ¨¡å (LLM) çé²æ­¥ï¼ééå©ç¨ LLM ä¸­å»£æ³çéæ¾ä¸çç¥è­ä¾å¢å¼· GNNï¼å°æ¼æ´ææå°è©®éåå©ç¨ææ¬è³æä¾è¡¨å¾µç°è³ªæ§åå½¢æå¾å¤§çå¸æãå¨éé å·¥ä½ä¸­ï¼æåæ¢è¨äº LLM å¨ç°è³ªæ§åå½¢å»ºæ¨¡ä¸­çæ½åï¼ä¸¦æåºäºä¸åæ°ç©çå©éæ®µæ¶æ§ï¼LLM å¢å¼·éç·£å¤å¥å¨å LLM å¼å°éç·£éæ°å æ¬ãå·é«ä¾èªªï¼å¨ç¬¬ä¸éæ®µï¼æåå¾®èª¿ LLM ä»¥æ ¹æå¶ç¯é»çææ¬è³è¨ï¼æ´å¥½å°è­å¥åè³ªæ§åç°è³ªæ§éç·£ãå¨ç¬¬äºéæ®µï¼æåæ ¹æç¯é»ç¹å¾µãçµæ§åç°è³ªæ§æåè³ªæ§ç¹å¾µï¼èªé©æå°ç®¡ç GNN ä¸­ä¸åéç·£é¡åçè¨æ¯å³éãçºäºæå°å¨å¯¦éå ´æ¯ä¸­é¨ç½² LLM æçè¨ç®éæ±ï¼æåé²ä¸æ­¥æ¢è¨æ¨¡åèåæè¡ï¼ä»¥å¾®èª¿è¼å°ãæ´ææççæ¨¡åï¼ä»¥ç¶­æç«¶ç­åãå»£æ³çå¯¦é©é©è­äºæåæ¶æ§çæææ§ï¼è­æäºä½¿ç¨ LLM ä¾å¢å¼· GNN ä»¥é²è¡ç°è³ªæ§åå½¢ä¸çç¯é»åé¡çå¯è¡æ§ã

##### **Towards Graph Prompt Learning: A Survey and Beyond**
2408.14520v2 by Qingqing Long, Yuchen Yan, Peiyan Zhang, Chen Fang, Wentao Cui, Zhiyuan Ning, Meng Xiao, Ning Cao, Xiao Luo, Lingjun Xu, Shiyue Jiang, Zheng Fang, Chong Chen, Xian-Sheng Hua, Yuanchun Zhou

Large-scale "pre-train and prompt learning" paradigms have demonstrated
remarkable adaptability, enabling broad applications across diverse domains
such as question answering, image recognition, and multimodal retrieval. This
approach fully leverages the potential of large-scale pre-trained models,
reducing downstream data requirements and computational costs while enhancing
model applicability across various tasks. Graphs, as versatile data structures
that capture relationships between entities, play pivotal roles in fields such
as social network analysis, recommender systems, and biological graphs. Despite
the success of pre-train and prompt learning paradigms in Natural Language
Processing (NLP) and Computer Vision (CV), their application in graph domains
remains nascent. In graph-structured data, not only do the node and edge
features often have disparate distributions, but the topological structures
also differ significantly. This diversity in graph data can lead to
incompatible patterns or gaps between pre-training and fine-tuning on
downstream graphs. We aim to bridge this gap by summarizing methods for
alleviating these disparities. This includes exploring prompt design
methodologies, comparing related techniques, assessing application scenarios
and datasets, and identifying unresolved problems and challenges. This survey
categorizes over 100 relevant works in this field, summarizing general design
principles and the latest applications, including text-attributed graphs,
molecules, proteins, and recommendation systems. Through this extensive review,
we provide a foundational understanding of graph prompt learning, aiming to
impact not only the graph mining community but also the broader Artificial
General Intelligence (AGI) community.

æè¦ï¼<paragraph>å¤§è¦æ¨¡ãé è¨ç·´åæç¤ºå­¸ç¿ãç¯ä¾å·²å±ç¾åºéå¡çé©æåï¼è½å»£æ³æç¨æ¼åç¨®é åï¼ä¾å¦åç­ãå½±åè¾¨è­åå¤æ¨¡ææª¢ç´¢ãæ­¤æ¹æ³ååç¼æ®å¤§åé è¨ç·´æ¨¡åçæ½åï¼æ¸å°ä¸æ¸¸è³æéæ±åéç®ææ¬ï¼åææåæ¨¡åå¨åç¨®ä»»åä¸­çé©ç¨æ§ãåå½¢ä½çºè½ææå¯¦é«ä¹ééä¿çå¤åè½è³æçµæ§ï¼å¨ç¤¾ç¾¤ç¶²è·¯åæãæ¨è¦ç³»çµ±åçç©åå½¢ç­é åæ®æ¼èééµè§è²ãåç®¡é è¨ç·´åæç¤ºå­¸ç¿ç¯ä¾å¨èªç¶èªè¨èç (NLP) åé»è¦è¦è¦º (CV) ä¸­ç²å¾æåï¼ä½å®åå¨åå½¢é åçæç¨ä»èæ¼èµ·æ­¥éæ®µãå¨åå½¢çµæ§åè³æä¸­ï¼ç¯é»åéç·£ç¹å¾µä¸åå¸¸æä¸åçåä½ï¼ææ²çµæ§ä¹å·®ç°å¾å¤§ãåå½¢è³æä¸­çéç¨®å¤æ¨£æ§å¯è½å°è´é è¨ç·´åå¾®èª¿ä¹éåºç¾ä¸ç¸å®¹çæ¨¡å¼æå·®è·ãæåæ¨å¨ééç¸½çµæ¸è¼éäºå·®ç°çæ¹æ³ä¾å½è£æ­¤å·®è·ãéåæ¬æ¢ç´¢æç¤ºè¨­è¨æ¹æ³ãæ¯è¼ç¸éæè¡ãè©ä¼°æç¨å ´æ¯åè³æéï¼ä»¥åæ¾åºæªè§£æ±ºçåé¡åææ°ãæ¬èª¿æ¥æ­¸é¡äºæ­¤é åä¸­è¶é 100 ç¯ç¸éä½åï¼ç¸½çµäºä¸è¬è¨­è¨åååææ°æç¨ï¼åæ¬æå­å±¬æ§åå½¢ãåå­ãèç½è³ªåæ¨è¦ç³»çµ±ãéééé å»£æ³çåé¡§ï¼æåæä¾äºåå½¢æç¤ºå­¸ç¿çåºæ¬çè§£ï¼æ¨å¨ä¸åå½±é¿åå½¢ææç¤¾ç¾¤ï¼ä¹å½±é¿æ´å»£æ³çäººå·¥éç¨æºæ§ (AGI) ç¤¾ç¾¤ã</paragraph>

##### **CodeGraph: Enhancing Graph Reasoning of LLMs with Code**
2408.13863v1 by Qiaolong Cai, Zhaowei Wang, Shizhe Diao, James Kwok, Yangqiu Song

With the increasing popularity of large language models (LLMs), reasoning on
basic graph algorithm problems is an essential intermediate step in assessing
their abilities to process and infer complex graph reasoning tasks. Existing
methods usually convert graph-structured data to textual descriptions and then
use LLMs for reasoning and computation. However, LLMs often produce computation
errors on arithmetic parts in basic graph algorithm problems, such as counting
number of edges. In addition, they struggle to control or understand the output
of the reasoning process, raising concerns about whether LLMs are simply
guessing. In this paper, we introduce CodeGraph, a method that encodes graph
problem solutions as code. The methods solve new graph problems by learning
from exemplars, generating programs, and executing them via a program
interpreter. Using the few-shot setting, we evaluate CodeGraph with the base
LLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and
Mixtral-8x7B Instruct. Experimental results on six tasks with six graph
encoding methods in the GraphQA dataset demonstrate that CodeGraph can boost
performance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on
the task. Compared to the existing methods, CodeGraph demonstrates strong
performance on arithmetic problems in graph tasks and offers a more
controllable and interpretable approach to the reasoning process.

æè¦ï¼é¨èå¤§åèªè¨æ¨¡å (LLM) çæ¥æ¼¸æ®åï¼å°åºæ¬åå½¢æ¼ç®æ³åé¡é²è¡æ¨çæ¯è©ä¼°å®åèçåæ¨è«è¤éåå½¢æ¨çä»»åçè½åä¸­ä¸åéè¦çä¸­éæ­¥é©ãç¾æçæ¹æ³éå¸¸æå°åå½¢çµæ§åçè³æè½æææå­æè¿°ï¼ç¶å¾ä½¿ç¨ LLM é²è¡æ¨çåéç®ãç¶èï¼LLM éå¸¸æå¨åºæ¬åå½¢æ¼ç®æ³åé¡ä¸­ï¼ä¾å¦è¨ç®éç·£æ¸éï¼å°ç®è¡é¨åç¢çéç®é¯èª¤ãæ­¤å¤ï¼å®åé£ä»¥æ§å¶æçè§£æ¨çéç¨çè¼¸åºï¼éå¼ç¼äº LLM æ¯å¦åªæ¯å¨çæ¸¬ççæ®ãå¨æ¬æä¸­ï¼æåä»ç´¹äº CodeGraphï¼éæ¯ä¸ç¨®å°åå½¢åé¡è§£æ±ºæ¹æ¡ç·¨ç¢¼çºç¨å¼ç¢¼çæ¹æ³ãéäºæ¹æ³ééå­¸ç¿ç¯ä¾ãç¢çç¨å¼ï¼ä¸¦ééç¨å¼ç¢¼ç´è­¯å¨å·è¡å®åä¾è§£æ±ºæ°çåå½¢åé¡ãä½¿ç¨å°æ¬¡åè©¦è¨­å®ï¼æåä½¿ç¨åºç¤ LLM çº GPT-3.5 TurboãLlama3-70B InstructãMixtral-8x22B Instruct å Mixtral-8x7B Instruct ä¾è©ä¼° CodeGraphãå¨ GraphQA è³æéä¸­ä½¿ç¨å­ç¨®åå½¢ç·¨ç¢¼æ¹æ³å°å­é ä»»åé²è¡çå¯¦é©çµæè¡¨æï¼CodeGraph å¯ä»¥å° LLM ä¸­çåå½¢æ¨çä»»åçæè½æå 1.3% å° 58.6%ï¼å·é«åæ±ºæ¼ä»»åãèç¾ææ¹æ³ç¸æ¯ï¼CodeGraph å¨åå½¢ä»»åä¸­çç®è¡åé¡ä¸è¡¨ç¾åºå¼·åçæè½ï¼ä¸¦çºæ¨çéç¨æä¾æ´å·å¯æ§æ§åå¯è§£éæ§çæ¹æ³ã

##### **LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings**
2408.14512v1 by Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu

Zero-shot graph machine learning, especially with graph neural networks
(GNNs), has garnered significant interest due to the challenge of scarce
labeled data. While methods like self-supervised learning and graph prompt
learning have been extensively explored, they often rely on fine-tuning with
task-specific labels, limiting their effectiveness in zero-shot scenarios.
Inspired by the zero-shot capabilities of instruction-fine-tuned large language
models (LLMs), we introduce a novel framework named Token Embedding-Aligned
Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and
cross-task zero-shot learners for graph machine learning. Concretely, we
pretrain a GNN, aligning its representations with token embeddings of an LLM.
We then train a linear projector that transforms the GNN's representations into
a fixed number of graph token embeddings without tuning the LLM. A unified
instruction is designed for various graph tasks at different levels, such as
node classification (node-level) and link prediction (edge-level). These design
choices collectively enhance our method's effectiveness in zero-shot learning,
setting it apart from existing methods. Experiments show that our graph token
embeddings help the LLM predictor achieve state-of-the-art performance on
unseen datasets and tasks compared to other methods using LLMs as predictors.

æè¦ï¼é¶ç¯ä¾åå½¢æ©å¨å­¸ç¿ï¼ç¹å¥æ¯åå½¢ç¥ç¶ç¶²è·¯ (GNN)ï¼ç±æ¼ç¨ææ¨ç±¤è³æçææ°èååéæ³¨ãéç¶èªç£ç£å¼å­¸ç¿ååå½¢æç¤ºå­¸ç¿ç­æ¹æ³å·²è¢«å»£æ³æ¢ç´¢ï¼ä½å®åéå¸¸ä¾è³´æ¼ä»»åç¹å®æ¨ç±¤çå¾®èª¿ï¼ééå¶äºå®åå¨é¶ç¯ä¾å ´æ¯ä¸­çæææ§ãåå°æä»¤å¾®èª¿å¤§åèªè¨æ¨¡å (LLM) çé¶ç¯ä¾åè½çåç¼ï¼æåå¼å¥äºä¸ååçº Token Embedding-Aligned Graph Language Model (TEA-GLM) çæ°æ¡æ¶ï¼å®å©ç¨ LLM ä½çºè·¨è³æéåè·¨ä»»åçé¶ç¯ä¾å­¸ç¿å¨ï¼ç¨æ¼åå½¢æ©å¨å­¸ç¿ãå·é«ä¾èªªï¼æåé è¨ç·´ä¸å GNNï¼å°å¶è¡¨ç¤ºè LLM ç token embedding å°é½ãç¶å¾ï¼æåè¨ç·´ä¸åç·æ§æå½±æ©ï¼å° GNN çè¡¨ç¤ºè½æçºåºå®æ¸éçåå½¢ token embeddingï¼èç¡éèª¿æ´ LLMãçµ±ä¸çæä»¤æ¯çºä¸åå±¤ç´çåç¨®åå½¢ä»»åè¨­è¨çï¼ä¾å¦ç¯é»åé¡ï¼ç¯é»å±¤ç´ï¼åé£çµé æ¸¬ï¼éç·£å±¤ç´ï¼ãéäºè¨­è¨é¸æå±åå¢å¼·äºæåçæ¹æ³å¨é¶ç¯ä¾å­¸ç¿ä¸­çæææ§ï¼ä½¿å¶æå¥æ¼ç¾ææ¹æ³ãå¯¦é©è¡¨æï¼èä½¿ç¨ LLM ä½çºé æ¸¬å¨çå¶ä»æ¹æ³ç¸æ¯ï¼æåçåå½¢ token embedding å¹«å© LLM é æ¸¬å¨å¨æªè¦éçè³æéåä»»åä¸å¯¦ç¾äºæåé²çæè½ã

##### **Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models**
2408.13661v1 by Sakhinana Sagar Srinivas, Geethan Sannidhi, Venkataramana Runkana

Characterizing materials with electron micrographs is a crucial task in
fields such as semiconductors and quantum materials. The complex hierarchical
structure of micrographs often poses challenges for traditional classification
methods. In this study, we propose an innovative backbone architecture for
analyzing electron micrographs. We create multi-modal representations of the
micrographs by tokenizing them into patch sequences and, additionally,
representing them as vision graphs, commonly referred to as patch attributed
graphs. We introduce the Hierarchical Network Fusion (HNF), a multi-layered
network structure architecture that facilitates information exchange between
the multi-modal representations and knowledge integration across different
patch resolutions. Furthermore, we leverage large language models (LLMs) to
generate detailed technical descriptions of nanomaterials as auxiliary
information to assist in the downstream task. We utilize a cross-modal
attention mechanism for knowledge fusion across cross-domain
representations(both image-based and linguistic insights) to predict the
nanomaterial category. This multi-faceted approach promises a more
comprehensive and accurate representation and classification of micrographs for
nanomaterial identification. Our framework outperforms traditional methods,
overcoming challenges posed by distributional shifts, and facilitating
high-throughput screening.

æè¦ï¼å©ç¨é»å­é¡¯å¾®ç§çä¾è¡¨å¾µææï¼å¨åå°é«åéå­ææç­é åä¸­æ¯ä¸é è³ééè¦çä»»åãé¡¯å¾®ç§çè¤éçåå±¤çµæ§éå¸¸æå°å³çµ±åé¡æ¹æ³å¸¶ä¾ææ°ãå¨éé ç ç©¶ä¸­ï¼æåæåºäºä¸ç¨®åµæ°çä¸»å¹¹æ¶æ§ï¼ç¨æ¼åæé»å­é¡¯å¾®ç§çãæåééå°é¡¯å¾®ç§çä»£ææåå¡åºåä¾å»ºç«å¶å¤æ¨¡æè¡¨ç¤ºï¼æ­¤å¤ï¼æåéå°å¶è¡¨ç¤ºçºè¦è¦ºåå½¢ï¼éå¸¸ç¨±çºåå¡å±¬æ§åå½¢ãæåå¼å¥äºåå±¤ç¶²è·¯èå (HNF)ï¼éæ¯ä¸ç¨®å¤å±¤ç¶²è·¯çµæ§æ¶æ§ï¼æå©æ¼å¤æ¨¡æè¡¨ç¤ºä¹éçè³è¨äº¤æï¼ä»¥åä¸ååå¡è§£æåº¦ä¹éçç¥è­æ´åãæ­¤å¤ï¼æåå©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾ç¢çå¥ç±³ææçè©³ç´°æè¡èªªæï¼ä½çºè¼å©è³è¨ï¼ä»¥åå©ä¸æ¸¸ä»»åãæåå©ç¨è·¨æ¨¡ææ³¨æåæ©å¶ï¼å¨è·¨é åè¡¨ç¤ºï¼åºæ¼å½±ååèªè¨æ´å¯åï¼ä¸­é²è¡ç¥è­èåï¼ä»¥é æ¸¬å¥ç±³ææé¡å¥ãéç¨®å¤æ¹é¢çåæ³ææçºå¥ç±³ææè­å¥æä¾æ´å¨é¢ä¸æºç¢ºçè¡¨ç¤ºååé¡ãæåçæ¶æ§åªæ¼å³çµ±æ¹æ³ï¼åæäºåä½è½ç§»å¸¶ä¾çææ°ï¼ä¸¦ä¿é²äºé«ééç¯©é¸ã

##### **GNN: Graph Neural Network and Large Language Model for Data Discovery**
2408.13609v2 by Thomas Hoang

Our algorithm GNN: Graph Neural Network and Large Language Model for Data
Discovery inherit the benefits of \cite{hoang2024plod} (PLOD: Predictive
Learning Optimal Data Discovery), \cite{Hoang2024BODBO} (BOD: Blindly Optimal
Data Discovery) in terms of overcoming the challenges of having to predefine
utility function and the human input for attribute ranking, which helps prevent
the time-consuming loop process. In addition to these previous works, our
algorithm GNN leverages the advantages of graph neural networks and large
language models to understand text type values that cannot be understood by
PLOD and MOD, thus making the task of predicting outcomes more reliable. GNN
could be seen as an extension of PLOD in terms of understanding the text type
value and the user's preferences, not only numerical values but also text
values, making the promise of data science and analytics purposes.

æè¦ï¼æåçæ¼ç®æ³ GNNï¼åç¥ç¶ç¶²è·¯åå¤§èªè¨æ¨¡åï¼ç¨æ¼è³ææ¢ç´¢ï¼ç¹¼æ¿äº \cite{hoang2024plod}ï¼PLODï¼é æ¸¬æ§æä½³è³ææ¢ç´¢ï¼ã\cite{Hoang2024BODBO}ï¼BODï¼ç²ç®æä½³è³ææ¢ç´¢ï¼çåªé»ï¼å¨æ¼åæå¿é é åå®ç¾©æç¨å½æ¸åäººé¡è¼¸å¥å±¬æ§æåçææ°ï¼éæå©æ¼é²æ­¢èæçè¿´åèçãé¤äºéäºååçä½åï¼æåçæ¼ç®æ³ GNN å©ç¨åç¥ç¶ç¶²è·¯åå¤§èªè¨æ¨¡åçåªé»ï¼ä¾çè§£ PLOD å MOD ç¡æ³çè§£çæå­é¡åå¼ï¼å¾èä½¿é æ¸¬çµæçä»»åæ´å¯é ãGNN å¯ä»¥è¦çº PLOD å¨çè§£æå­é¡åå¼åä½¿ç¨èåå¥½æ¹é¢çå»¶ä¼¸ï¼ä¸åæ¯æ¸å¼ï¼éææå­å¼ï¼éå¯¦ç¾äºè³æç§å­¸ååæç®ççæ¿è«¾ã

##### **HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation**
2408.13521v1 by Azmine Toushik Wasi

Knowledge Graphs (KGs) serving as semantic networks, prove highly effective
in managing complex interconnected data in different domains, by offering a
unified, contextualized, and structured representation with flexibility that
allows for easy adaptation to evolving knowledge. Processing complex Human
Resources (HR) data, KGs can help in different HR functions like recruitment,
job matching, identifying learning gaps, and enhancing employee retention.
Despite their potential, limited efforts have been made to implement practical
HR knowledge graphs. This study addresses this gap by presenting a framework
for effectively developing HR knowledge graphs from documents using Large
Language Models. The resulting KG can be used for a variety of downstream
tasks, including job matching, identifying employee skill gaps, and many more.
In this work, we showcase instances where HR KGs prove instrumental in precise
job matching, yielding advantages for both employers and employees. Empirical
evidence from experiments with information propagation in KGs and Graph Neural
Nets, along with case studies underscores the effectiveness of KGs in tasks
such as job and employee recommendations and job area classification. Code and
data are available at : https://github.com/azminewasi/HRGraph

æè¦ï¼ç¥è­åè­ (KG) ä½çºèªç¾©ç¶²è·¯ï¼è­æå¨ç®¡çä¸åé åä¸­è¤éçäºé£è³ææ¹é¢éå¸¸ææï¼ééæä¾çµ±ä¸ãèçµ¡åä¸çµæ§åçè¡¨ç¤ºï¼ä¸¦å·åéæ´»æ§ï¼å¯è¼é¬é©æä¸æ·è®åçç¥è­ãKG èçè¤éçäººåè³æº (HR) è³æï¼æå©æ¼ä¸åç HR åè½ï¼ä¾å¦æåãå·¥ä½å¹éãæ¾åºå­¸ç¿å·®è·åæåå¡å·¥çå­çãåç®¡æå¶æ½åï¼ä½å¯¦ä½å¯¦ç¨ç HR ç¥è­åè­çåªåæéãæ¬ç ç©¶ééæåºä¸åæ¶æ§ï¼å¾æä»¶ä¸­ä½¿ç¨å¤§åèªè¨æ¨¡åææéç¼ HR ç¥è­åè­ï¼ä¾è§£æ±ºéåå·®è·ãç¢çç KG å¯ç¨æ¼åç¨®ä¸æ¸¸ä»»åï¼åæ¬å·¥ä½å¹éãæ¾åºå¡å·¥æè½å·®è·ç­ãå¨éé å·¥ä½ä¸­ï¼æåå±ç¤ºäº HR KG å¨ç²¾ç¢ºå·¥ä½å¹éä¸­è­ææç¨çç¯ä¾ï¼çºéä¸»åå¡å·¥å¸¶ä¾åªå¢ãéé KG ååå½¢ç¥ç¶ç¶²è·¯ä¸­è³è¨å³æ­çå¯¦é©æå¾çå¯¦è­ï¼ä»¥åæ¡ä¾ç ç©¶ï¼å¼·èª¿äº KG å¨å·¥ä½åå¡å·¥æ¨è¦ä»¥åå·¥ä½é ååé¡ç­ä»»åä¸­çæææ§ãç¨å¼ç¢¼åè³æå¯å¨ä»¥ä¸ä½ç½®åå¾ï¼https://github.com/azminewasi/HRGraph

##### **Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation**
2408.13432v1 by Yi-Hui Chen, Eric Jui-Lin Lu, Kwan-Ho Cheng

The main task of the KGQA system (Knowledge Graph Question Answering) is to
convert user input questions into query syntax (such as SPARQL). With the rise
of modern popular encoders and decoders like Transformer and ConvS2S, many
scholars have shifted the research direction of SPARQL generation to the Neural
Machine Translation (NMT) architecture or the generative AI field of
Text-to-SPARQL. In NMT-based QA systems, the system treats knowledge base query
syntax as a language. It uses NMT-based translation models to translate natural
language questions into query syntax. Scholars use popular architectures
equipped with cross-attention, such as Transformer, ConvS2S, and BiLSTM, to
train translation models for query syntax. To achieve better query results,
this paper improved the ConvS2S encoder and added multi-head attention from the
Transformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the
n-gram language model. The principle is to use convolutional layers to capture
local hidden features in the input sequence with different receptive fields,
using multi-head attention to calculate dependencies between them. Ultimately,
we found that the translation model based on the Multi-Head Conv encoder
achieved better performance than other encoders, obtaining 76.52\% and 83.37\%
BLEU-1 (BiLingual Evaluation Understudy) on the QALD-9 and LC-QuAD-1.0
datasets, respectively. Additionally, in the end-to-end system experiments on
the QALD-9 and LC-QuAD-1.0 datasets, we achieved leading results over other
KGQA systems, with Macro F1-measures reaching 52\% and 66\%, respectively.
Moreover, the experimental results show that with limited computational
resources, if one possesses an excellent encoder-decoder architecture and
cross-attention, experts and scholars can achieve outstanding performance
equivalent to large pre-trained models using only general embeddings.

æè¦ï¼ç¥è­åè¡¨åç­ç³»çµ± (KGQA) çä¸»è¦ä»»åæ¯å°ä½¿ç¨èè¼¸å¥çåé¡è½æææ¥è©¢èªæ³ (ä¾å¦ SPARQL)ãé¨è Transformer å ConvS2S ç­ç¾ä»£æµè¡ç·¨ç¢¼å¨åè§£ç¢¼å¨çå´èµ·ï¼è¨±å¤å­¸èå·²å° SPARQL çæçç ç©¶æ¹åè½ç§»å°ç¥ç¶æ©å¨ç¿»è­¯ (NMT) æ¶æ§ææå­è½ SPARQL ççæå¼äººå·¥æºæ§é åãå¨åºæ¼ NMT çåç­ç³»çµ±ä¸­ï¼ç³»çµ±å°ç¥è­åº«æ¥è©¢èªæ³è¦çºä¸ç¨®èªè¨ãå®ä½¿ç¨åºæ¼ NMT çç¿»è­¯æ¨¡åå°èªç¶èªè¨åé¡è½æææ¥è©¢èªæ³ãå­¸èä½¿ç¨éåè·¨æ³¨æåæ©å¶çç±éæ¶æ§ï¼ä¾å¦ TransformerãConvS2S å BiLSTMï¼ä¾è¨ç·´æ¥è©¢èªæ³çç¿»è­¯æ¨¡åãçºäºç²å¾æ´å¥½çæ¥è©¢çµæï¼æ¬ææ¹é²äº ConvS2S ç·¨ç¢¼å¨ï¼ä¸¦å¾ Transformer ä¸­å å¥å¤é ­æ³¨æåæ©å¶ï¼æåºäºä¸ååºæ¼ n-gram èªè¨æ¨¡åçå¤é ­å·ç©ç·¨ç¢¼å¨ (MHC ç·¨ç¢¼å¨)ãå¶åçæ¯ä½¿ç¨å·ç©å±¤ä»¥ä¸åçæåéæ·åè¼¸å¥åºåä¸­çå±é¨é±èç¹å¾µï¼ä¸¦ä½¿ç¨å¤é ­æ³¨æåæ©å¶è¨ç®å®åä¹éçä¾è³´éä¿ãæçµï¼æåç¼ç¾åºæ¼å¤é ­å·ç©ç·¨ç¢¼å¨çç¿»è­¯æ¨¡åæ¯å¶ä»ç·¨ç¢¼å¨ç²å¾äºæ´å¥½çæè½ï¼åå¥å¨ QALD-9 å LC-QuAD-1.0 è³æéä¸ç²å¾ 76.52% å 83.37% ç BLEU-1ï¼éèªè©ä¼°ç ç©¶ï¼åæ¸ãæ­¤å¤ï¼å¨ QALD-9 å LC-QuAD-1.0 è³æéçç«¯å°ç«¯ç³»çµ±å¯¦é©ä¸­ï¼æåå¨å¶ä» KGQA ç³»çµ±ä¸­åå¾äºé åççµæï¼å·¨è§ F1 æ¸¬éå¼åå¥éå° 52% å 66%ãæ­¤å¤ï¼å¯¦é©çµæè¡¨æï¼å¦æææåºè²çç·¨ç¢¼å¨-è§£ç¢¼å¨æ¶æ§åè·¨æ³¨æåæ©å¶ï¼å³ä½¿å¨éç®è³æºæéçææ³ä¸ï¼å°å®¶åå­¸èä»å¯ä»¥ä½¿ç¨ä¸è¬çåµå¥ä¾ç²å¾ç­åæ¼å¤§åé è¨ç·´æ¨¡åçååºæè½ã

##### **CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers**
2408.13366v1 by Ekaterina Trofimova, Emil Sataev, Abhijit Singh Jowhari

This paper presents CodeRefine, a novel framework for automatically
transforming research paper methodologies into functional code using Large
Language Models (LLMs). Our multi-step approach first extracts and summarizes
key text chunks from papers, analyzes their code relevance, and creates a
knowledge graph using a predefined ontology. Code is then generated from this
structured representation and enhanced through a proposed retrospective
retrieval-augmented generation approach. CodeRefine addresses the challenge of
bridging theoretical research and practical implementation, offering a more
accurate alternative to LLM zero-shot prompting. Evaluations on diverse
scientific papers demonstrate CodeRefine's ability to improve code
implementation from the paper, potentially accelerating the adoption of
cutting-edge algorithms in real-world applications.

æè¦ï¼æ¬ç¯è«ææåº CodeRefineï¼ä¸åå©ç¨å¤§åèªè¨æ¨¡å (LLM) å°ç ç©¶è«ææ¹æ³èªåè½æçºåè½ç¨å¼ç¢¼çæ°ç©æ¶æ§ãæåçå¤æ­¥é©æ¹æ³é¦åå¾è«æä¸­èåä¸¦æè¦åºééµæå­åå¡ï¼åæå¶ç¨å¼ç¢¼ç¸éæ§ï¼ä¸¦ä½¿ç¨é å®ç¾©çæ¬ä½å»ºç«ç¥è­åè­ãæ¥èå¾éåçµæ§åè¡¨ç¤ºç¢çç¨å¼ç¢¼ï¼ä¸¦ééæåºçåæº¯å¼æª¢ç´¢å¢å¼·ç¢çæ¹æ³é²è¡å¼·åãCodeRefine è§£æ±ºäºçè«ç ç©¶èå¯¦éå¯¦ä½ä¹éçé´»æºï¼æä¾æ¯ LLM é¶æ¬¡æç¤ºæ´ç²¾ç¢ºçæ¿ä»£æ¹æ¡ãå¨åç¨®ç§å­¸è«æä¸çè©ä¼°è­æäº CodeRefine å¾è«ææ¹åç¨å¼ç¢¼å¯¦ä½çè½åï¼éææ½åå éå°ç«¯æ¼ç®æ³å¨å¯¦éæç¨ä¸­çæ¡ç¨ã

##### **Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving**
2408.14494v1 by Sakhinana Sagar Srinivas, Vijay Sri Vaikunth, Venkataramana Runkana

We present the Process Engineering Operations Assistant (PEOA), an AI-driven
framework designed to solve complex problems in the chemical and process
industries. The framework employs a modular architecture orchestrated by a
meta-agent, which serves as the central coordinator, managing an action
generator and instruction-tuned small-scale language models (expert models).
The action generator decomposes complex problems into sub-tasks and identifies
suitable expert models to execute each, delivering precise solutions for
multi-step problem-solving. Key techniques include advanced knowledge modeling
using property graphs for improved information retrieval, facilitating more
accurate and contextually relevant solutions. Additionally, the framework
utilizes a teacher-student transfer-learning approach with GPT-4 (Omni) to
fine-tune the action generator and expert models for domain adaptation,
alongside an iterative problem-solving mechanism with sophisticated error
handling. Custom datasets were developed to evaluate the framework against
leading proprietary language models on various engineering tasks. The results
demonstrate the framework effectiveness in automating calculations,
accelerating prototyping, and providing AI-augmented decision support for
industrial processes, marking a significant advancement in process engineering
capabilities.

æè¦ï¼æåæåºäºè£½ç¨å·¥ç¨ä½æ¥­å©ç (PEOA)ï¼éæ¯ä¸åç± AI é©åçæ¶æ§ï¼æ¨å¨è§£æ±ºåå­¸åè£½ç¨ç¢æ¥­ä¸­çè¤éåé¡ãè©²æ¶æ§æ¡ç¨æ¨¡çµåæ¶æ§ï¼ç±ä¸ååä»£çç¨å¼åèª¿ï¼è©²ä»£çç¨å¼ä½çºä¸­å¤®åèª¿å¨ï¼ç®¡çåä½ç¢çå¨åæä»¤èª¿æ´çå°è¦æ¨¡èªè¨æ¨¡å (å°å®¶æ¨¡å)ãåä½ç¢çå¨å°è¤éçåé¡åè§£çºå­ä»»åï¼ä¸¦è­å¥åé©çå°å®¶æ¨¡åä¾å·è¡æ¯åä»»åï¼çºå¤æ­¥é©åé¡è§£æ±ºæä¾ç²¾ç¢ºçè§£æ±ºæ¹æ¡ãééµæè¡åæ¬ä½¿ç¨å±¬æ§åé²è¡é²éç¥è­å»ºæ¨¡ï¼ä»¥æ¹åè³è¨æª¢ç´¢ï¼æä¾æ´æºç¢ºä¸èèçµ¡ç¸éçè§£æ±ºæ¹æ¡ãæ­¤å¤ï¼è©²æ¶æ§æ¡ç¨æå¸«-å­¸çå³è¼¸å­¸ç¿æ¹æ³ï¼ä½¿ç¨ GPT-4 (Omni) ä¾å¾®èª¿åä½ç¢çå¨åå°å®¶æ¨¡åï¼ä»¥é²è¡é åé©æï¼ä»¥åå·åç²¾ç·»é¯èª¤èçåè½çè¿­ä»£åé¡è§£æ±ºæ©å¶ãéç¼äºèªè¨è³æéï¼ä»¥éå°åç¨®å·¥ç¨ä»»åè©ä¼°è©²æ¶æ§èé åçå°æèªè¨æ¨¡åãçµæè­æäºè©²æ¶æ§å¨èªååè¨ç®ãå éå»ºæ¨¡åæä¾ AI å¢å¼·æ±ºç­æ¯æ´æ¹é¢çæææ§ï¼æ¨èªèè£½ç¨å·¥ç¨è½åçéå¤§é²å±ã

##### **A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language**
2408.12578v2 by Ekdeep Singh Lubana, Kyogo Kawaguchi, Robert P. Dick, Hidenori Tanaka

Increase in data, size, or compute can lead to sudden learning of specific
capabilities by a neural network -- a phenomenon often called "emergence''.
Beyond scientific understanding, establishing the causal factors underlying
such emergent capabilities is crucial to enable risk regulation frameworks for
AI. In this work, we seek inspiration from study of emergent properties in
other fields and propose a phenomenological definition for the concept in the
context of neural networks. Our definition implicates the acquisition of
general structures underlying the data-generating process as a cause of sudden
performance growth for specific, narrower tasks. We empirically investigate
this definition by proposing an experimental system grounded in a
context-sensitive formal language and find that Transformers trained to perform
tasks on top of strings from this language indeed exhibit emergent
capabilities. Specifically, we show that once the language's underlying grammar
and context-sensitivity inducing structures are learned by the model,
performance on narrower tasks suddenly begins to improve. We then analogize our
network's learning dynamics with the process of percolation on a bipartite
graph, establishing a formal phase transition model that predicts the shift in
the point of emergence observed in our experiments when changing the data
structure. Overall, our experimental and theoretical frameworks yield a step
towards better defining, characterizing, and predicting emergence in neural
networks.

æè¦ï¼<paragraph>è³æãè¦æ¨¡æéç®çå¢å ï¼å¯è½æå°è´ç¥ç¶ç¶²è·¯çªç¶å­¸æç¹å®è½åââéç¨®ç¾è±¡å¸¸ç¨±çºãæ¹§ç¾ããé¤äºç§å­¸çè§£ä¹å¤ï¼ç¢ºç«éç¨®æ¹§ç¾è½åèå¾çåºæ¬åå ï¼å°æ¼çº AI å»ºç«é¢¨éªæ³è¦æ¡æ¶è³ééè¦ãå¨éé å·¥ä½ä¸­ï¼æåå¾å¶ä»é åä¸­å°æ¹§ç¾ç¹æ§çç ç©¶ä¸­å°æ±éæï¼ä¸¦éå°ç¥ç¶ç¶²è·¯ä¸­çæ¦å¿µæåºç¾è±¡å­¸å®ç¾©ãæåçå®ç¾©æç¤ºï¼åå¾è³æç¢çç¨åºèå¾çéç¨çµæ§ï¼æ¯ç¹å®ãè¼ç¹éä»»åçªç¶æè½æåçåå ãæåééæåºä¸åä»¥æå¢ææå½¢å¼èªè¨çºåºç¤çå¯¦é©ç³»çµ±ï¼å°éåå®ç¾©é²è¡å¯¦è­ç ç©¶ï¼ç¼ç¾ç¶éè¨ç·´ä»¥å·è¡éåèªè¨ä¸­å­ä¸²é é¨ä»»åç Transformerï¼ç¢ºå¯¦å±ç¾åºæ¹§ç¾è½åãå·é«ä¾èªªï¼æåå±ç¤ºåºæ¨¡åä¸æ¦å­¸æèªè¨çåºå±¤ææ³åæå¢ææèªå°çµæ§ï¼å°è¼ç¹éä»»åçæè½å°±æçªç¶éå§æåãæ¥èæåå°ç¶²è·¯çå­¸ç¿åæé¡æ¯çºäºé¨åä¸çæ»²æµéç¨ï¼å»ºç«ä¸åæ­£å¼çç¸è®æ¨¡åï¼ç¨æ¼é æ¸¬å¨æ¹è®è³æçµæ§æï¼æåå¨å¯¦é©ä¸­è§å¯å°çæ¹§ç¾é»ä½ç§»ãæ´é«èè¨ï¼æåçå¯¦é©åçè«æ¡æ¶æèæ´å®åå°å®ç¾©ãæè¿°åé æ¸¬ç¥ç¶ç¶²è·¯ä¸­çæ¹§ç¾éé²äºä¸æ­¥ã</paragraph>

##### **Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language**
2409.00061v1 by Arief Purnama Muharram, Ayu Purwarianti

Automated fact-checking is a key strategy to overcome the spread of COVID-19
misinformation on the internet. These systems typically leverage deep learning
approaches through Natural Language Inference (NLI) to verify the truthfulness
of information based on supporting evidence. However, one challenge that arises
in deep learning is performance stagnation due to a lack of knowledge during
training. This study proposes using a Knowledge Graph (KG) as external
knowledge to enhance NLI performance for automated COVID-19 fact-checking in
the Indonesian language. The proposed model architecture comprises three
modules: a fact module, an NLI module, and a classifier module. The fact module
processes information from the KG, while the NLI module handles semantic
relationships between the given premise and hypothesis. The representation
vectors from both modules are concatenated and fed into the classifier module
to produce the final result. The model was trained using the generated
Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.
Our study demonstrates that incorporating KGs can significantly improve NLI
performance in fact-checking, achieving the best accuracy of 0,8616. This
suggests that KGs are a valuable component for enhancing NLI performance in
automated fact-checking.

æè¦ï¼èªåäºå¯¦æ¥æ ¸æ¯åæç¶²è·¯ä¸ COVID-19 é¯èª¤è³è¨æ£æ­çä¸é ééµç­ç¥ãéäºç³»çµ±éå¸¸ééèªç¶èªè¨æ¨è« (NLI) ä¾å©ç¨æ·±åº¦å­¸ç¿æ¹æ³ï¼æ ¹ææ¯æ´è­æé©è­è³è¨ççå¯¦æ§ãç¶èï¼å¨æ·±åº¦å­¸ç¿ä¸­æåºç¾ä¸åææ°ï¼é£å°±æ¯å¨è¨ç·´æéå ç¼ºä¹ç¥è­èå°è´æè½åæ»¯ãéé ç ç©¶æåºä½¿ç¨ç¥è­åè­ (KG) ä½çºå¤é¨ç¥è­ï¼ä»¥å¢å¼·èªåå COVID-19 äºå¯¦æ¥æ ¸ç NLI æè½ï¼ä¸¦ä»¥å°å°¼èªé²è¡ãææåºçæ¨¡åæ¶æ§åå«ä¸åæ¨¡çµï¼äºå¯¦æ¨¡çµãNLI æ¨¡çµååé¡å¨æ¨¡çµãäºå¯¦æ¨¡çµèçä¾èª KG çè³è¨ï¼è NLI æ¨¡çµåèççµ¦å®åæååè¨­ä¹éçèªç¾©éä¿ãä¾èªå©åæ¨¡çµçè¡¨ç¤ºåéæä¸²æ¥èµ·ä¾ï¼ä¸¦è¼¸å¥åé¡å¨æ¨¡çµä»¥ç¢çæçµçµæãæ­¤æ¨¡åä½¿ç¨ç¢ççå°å°¼èª COVID-19 äºå¯¦æ¥æ ¸è³æéå COVID-19 KG Bahasa Indonesia é²è¡è¨ç·´ãæåçç ç©¶è­æï¼ç´å¥ KG å¯ä»¥é¡¯èæ¹åäºå¯¦æ¥æ ¸ä¸­ç NLI æè½ï¼éå° 0.8616 çæä½³æºç¢ºåº¦ãéè¡¨ç¤º KG æ¯å¢å¼·èªååäºå¯¦æ¥æ ¸ä¸­ NLI æè½çå¯¶è²´çµæé¨åã

##### **Cell-ontology guided transcriptome foundation model**
2408.12373v1 by Xinyu Yuan, Zhihao Zhan, Zuobai Zhang, Manqi Zhou, Jianan Zhao, Boyu Han, Yue Li, Jian Tang

Transcriptome foundation models TFMs hold great promises of deciphering the
transcriptomic language that dictate diverse cell functions by self-supervised
learning on large-scale single-cell gene expression data, and ultimately
unraveling the complex mechanisms of human diseases. However, current TFMs
treat cells as independent samples and ignore the taxonomic relationships
between cell types, which are available in cell ontology graphs. We argue that
effectively leveraging this ontology information during the TFM pre-training
can improve learning biologically meaningful gene co-expression patterns while
preserving TFM as a general purpose foundation model for downstream zero-shot
and fine-tuning tasks. To this end, we present \textbf{s}ingle \textbf{c}ell,
\textbf{Cell}-\textbf{o}ntology guided TFM scCello. We introduce cell-type
coherence loss and ontology alignment loss, which are minimized along with the
masked gene expression prediction loss during the pre-training. The novel loss
component guide scCello to learn the cell-type-specific representation and the
structural relation between cell types from the cell ontology graph,
respectively. We pre-trained scCello on 22 million cells from CellxGene
database leveraging their cell-type labels mapped to the cell ontology graph
from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates
competitive generalization and transferability performance over the existing
TFMs on biologically important tasks including identifying novel cell types of
unseen cells, prediction of cell-type-specific marker genes, and cancer drug
responses.

æè¦ï¼<paragraph>è½éçµåºç¤æ¨¡å TFM æ¿è«¾è§£ç¢¼è½éçµèªè¨ï¼å®ééå¨å¤§åå®ç´°èåºå è¡¨ç¾è³æä¸é²è¡èªæç£ç£å­¸ç¿ï¼ä¾æ±ºå®ä¸åçç´°èåè½ï¼ä¸¦æçµè§£éäººé¡ç¾ççè¤éæ©å¶ãç¶èï¼ç®åç TFM å°ç´°èè¦çºç¨ç«æ¨£æ¬ï¼ä¸¦å¿½ç¥ç´°èé¡åä¹éçåé¡éä¿ï¼èéå¨ç´°èæ¬é«è«åè¡¨ä¸­æ¯å¯ç¨çãæåèªçºå¨ TFM é è¨ç·´æéææå©ç¨æ­¤æ¬é«è«è³è¨ï¼å¯ä»¥æ¹åå­¸ç¿çç©å­¸ä¸ææç¾©çåºå å±è¡¨ç¾æ¨¡å¼ï¼åæä¿ç TFM ä½çºä¸æ¸¸é¶æ¬¡å­¸ç¿åå¾®èª¿ä»»åçä¸è¬ç¨éåºç¤æ¨¡åãçºæ­¤ï¼æåæåºå®ç´°èãç´°èæ¬é«è«å¼å°ç TFM scCelloãæåå¼å¥ç´°èé¡åä¸è´æ§æå¤±åæ¬é«è«å°é½æå¤±ï¼å¨é è¨ç·´æéæå°å¶èé®ç½©åºå è¡¨ç¾é æ¸¬æå¤±ä¸èµ·æå°åãéåæ°ç©çæå¤±çµä»¶å¼å° scCello åå¥å¾ç´°èæ¬é«è«åè¡¨ä¸­å­¸ç¿ç´°èé¡åç¹å®è¡¨ç¤ºåç´°èé¡åä¹éççµæ§éä¿ãæåå¨ CellxGene è³æåº«ä¸­å° 2200 è¬åç´°èé²è¡ scCello é è¨ç·´ï¼å©ç¨å¶ç´°èé¡åæ¨ç±¤å°æå°éæ¾çç©åçç©é«å­¸æ¬é«éé å» çç´°èæ¬é«è«åè¡¨ãæåç TFM å¨çç©å­¸ä¸éè¦çä»»åä¸å±ç¤ºäºæ¯ç¾æ TFM æ´å·ç«¶ç­åçæ³ååå¯è½ç§»æ§ï¼åæ¬è­å¥æªè¦ç´°èçæ°ç´°èé¡åãé æ¸¬ç´°èé¡åç¹å®æ¨è¨åºå åççè¥ç©åæã</paragraph>

##### **Graph Retrieval Augmented Trustworthiness Reasoning**
2408.12333v2 by Ying Zhu, Shengchang Li, Ziqian Kong, Peilan Xu

Trustworthiness reasoning is crucial in multiplayer games with incomplete
information, enabling agents to identify potential allies and adversaries,
thereby enhancing reasoning and decision-making processes. Traditional
approaches relying on pre-trained models necessitate extensive domain-specific
data and considerable reward feedback, with their lack of real-time
adaptability hindering their effectiveness in dynamic environments. In this
paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,
leveraging the Retrieval-Augmented Generation (RAG) technique to bolster
trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness
graph, updating it in real-time with evidential information, and retrieves
relevant trust data to augment the reasoning capabilities of Large Language
Models (LLMs). We validate our approach through experiments on the multiplayer
game "Werewolf," comparing GRATR against baseline LLM and LLM enhanced with
Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the
baseline methods by over 30\% in winning rate, with superior reasoning
performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as
identity and objective amnesia, and crucially, it renders the reasoning process
more transparent and traceable through the use of the trustworthiness graph.

æè¦ï¼<paragraph>å¨ä¿¡æ¯ä¸å®æ´çå¤äººéæ²ä¸­ï¼å¯ä¿¡åº¦æ¨çè³ééè¦ï¼è®ä»£çäººè½å¤ è­å¥æ½å¨ççååæµäººï¼å¾èå¢å¼·æ¨çåæ±ºç­å¶å®éç¨ãä¾è³´é åè¨ç·´æ¨¡åçå³çµ±æ¹æ³éè¦å¤§éçç¹å®é åæ¸æåå¤§éççåµåé¥ï¼èå®åç¼ºä¹å¯¦æé©ææ§æé»ç¤å®åå¨åæç°å¢ä¸­çæææ§ãå¨æ¬æä¸­ï¼æåä»ç´¹äºåå½¢æª¢ç´¢å¢å¼·æ¨ç (GRATR) æ¡æ¶ï¼å©ç¨æª¢ç´¢å¢å¼·çæ (RAG) æè¡ä¾å å¼·ä»£çäººçå¯ä¿¡åº¦æ¨çãGRATR æ§å»ºäºä¸ååæå¯ä¿¡åº¦åå½¢ï¼ä¸¦ä½¿ç¨è­æä¿¡æ¯å¯¦ææ´æ°å®ï¼ä¸¦æª¢ç´¢ç¸éçä¿¡ä»»æ¸æä»¥å¢å¼·å¤§åèªè¨æ¨¡å (LLM) çæ¨çè½åãæåééå¤äººéæ²ãç¼äººãçå¯¦é©é©è­äºæåçåæ³ï¼å° GRATR èåºæº LLM åä½¿ç¨ Native RAG å Rerank RAG å¢å¼·ç LLM é²è¡äºæ¯è¼ãæåççµæè¡¨æï¼GRATR å¨ç²åçä¸æ¯åºæºæ¹æ³é«åº 30%ï¼å·æåè¶çæ¨çæ§è½ãæ­¤å¤ï¼GRATR ææå°æ¸è¼äº LLM çå¹»è¦ºï¼ä¾å¦èº«ä»½åç®æ¨å¥å¿çï¼æéè¦çæ¯ï¼å®ééä½¿ç¨å¯ä¿¡åº¦åå½¢ä½¿æ¨çéç¨æ´éæä¸å¯è¿½è¹¤ã</paragraph>

##### **MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**
2408.12236v1 by Yanzeng Li, Cheng Zeng, Jinchao Zhang, Jie Zhou, Lei Zou

Medical education relies heavily on Simulated Patients (SPs) to provide a
safe environment for students to practice clinical skills, including medical
image analysis. However, the high cost of recruiting qualified SPs and the lack
of diverse medical imaging datasets have presented significant challenges. To
address these issues, this paper introduces MedDiT, a novel
knowledge-controlled conversational framework that can dynamically generate
plausible medical images aligned with simulated patient symptoms, enabling
diverse diagnostic skill training. Specifically, MedDiT integrates various
patient Knowledge Graphs (KGs), which describe the attributes and symptoms of
patients, to dynamically prompt Large Language Models' (LLMs) behavior and
control the patient characteristics, mitigating hallucination during medical
conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is
incorporated to generate medical images according to the specified patient
attributes in the KG. In this paper, we present the capabilities of MedDiT
through a practical demonstration, showcasing its ability to act in diverse
simulated patient cases and generate the corresponding medical images. This can
provide an abundant and interactive learning experience for students, advancing
medical education by offering an immersive simulation platform for future
healthcare professionals. The work sheds light on the feasibility of
incorporating advanced technologies like LLM, KG, and DiT in education
applications, highlighting their potential to address the challenges faced in
simulated patient-based medical education.

æè¦ï¼é«å­¸æè²é«åº¦ä¾è³´æ¨¡æ¬çäºº (SP) æä¾ä¸åå®å¨çç°å¢ï¼è®å­¸çç·´ç¿è¨åºæè½ï¼åæ¬é«å­¸å½±ååæãç¶èï¼æååæ ¼ SP çé«ææ¬åç¼ºä¹å¤æ¨£çé«å­¸å½±åè³æéå·²é æé¡¯èçææ°ãçºäºè§£æ±ºéäºåé¡ï¼æ¬æä»ç´¹ MedDiTï¼ä¸åæ°ç©çç¥è­æ§å¶å°è©±æ¶æ§ï¼å®å¯ä»¥åæç¢çç¬¦åæ¨¡æ¬çäººçççåçé«å­¸å½±åï¼å¯¦ç¾å¤æ¨£çè¨ºæ·æè½è¨ç·´ãå·é«ä¾èªªï¼MedDiT æ´åäºåç¨®çäººç¥è­åè­ (KG)ï¼æè¿°çäººçå±¬æ§åççï¼ä»¥åææç¤ºå¤§åèªè¨æ¨¡å (LLM) çè¡çºï¼ä¸¦æ§å¶çäººç¹å¾µï¼æ¸è¼é«å­¸å°è©±ä¸­çå¹»è¦ºãæ­¤å¤ï¼éç´å¥ä¸åç¶éå¾®èª¿çæ´æ£Transformer (DiT) æ¨¡åï¼æ ¹æ KG ä¸­æå®ççäººå±¬æ§ç¢çé«å­¸å½±åãå¨æ¬æä¸­ï¼æåééå¯¦éç¤ºç¯å±ç¤º MedDiT çåè½ï¼å±ç¤ºå®å¨ä¸åæ¨¡æ¬çäººæ¡ä¾ä¸­ä½ç¨ä¸¦ç¢çç¸æé«å­¸å½±åçè½åãéå¯ä»¥çºå­¸çæä¾è±å¯ä¸äºåçå­¸ç¿é«é©ï¼ééæä¾èº«æ­·å¶å¢çæ¨¡æ¬å¹³å°ï¼æåé«å­¸æè²ï¼é ç¦æªä¾çé«çä¿å¥å°æ¥­äººå¡ãéé å·¥ä½é¡æäºå¨æè²æç¨ä¸­æ´å LLMãKG å DiT ç­åé²æè¡çå¯è¡æ§ï¼çªé¡¯å®åå¨è§£æ±ºæ¨¡æ¬çäººçºåºç¤çé«å­¸æè²æé¢è¨ææ°çæ½åã

##### **Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning**
2408.12116v1 by Junlin He, Tong Nie, Wei Ma

In the geospatial domain, universal representation models are significantly
less prevalent than their extensive use in natural language processing and
computer vision. This discrepancy arises primarily from the high costs
associated with the input of existing representation models, which often
require street views and mobility data. To address this, we develop a novel,
training-free method that leverages large language models (LLMs) and auxiliary
map data from OpenStreetMap to derive geolocation representations (LLMGeovec).
LLMGeovec can represent the geographic semantics of city, country, and global
scales, which acts as a generic enhancer for spatio-temporal learning.
Specifically, by direct feature concatenation, we introduce a simple yet
effective paradigm for enhancing multiple spatio-temporal tasks including
geographic prediction (GP), long-term time series forecasting (LTSF), and
graph-based spatio-temporal forecasting (GSTF). LLMGeovec can seamlessly
integrate into a wide spectrum of spatio-temporal learning models, providing
immediate enhancements. Experimental results demonstrate that LLMGeovec
achieves global coverage and significantly boosts the performance of leading
GP, LTSF, and GSTF models.

æè¦ï¼å¨ç©ºéå°çé åï¼éç¨è¡¨ç¤ºæ¨¡åé¡¯èå°æ¼å®åå¨èªç¶èªè¨èçåé»è¦è¦è¦ºä¸­çå»£æ³ä½¿ç¨ãéç¨®å·®ç°ä¸»è¦æºæ¼ç¾æè¡¨ç¤ºæ¨¡åçè¼¸å¥ææ¬é«ï¼ééå¸¸éè¦è¡æ¯åæµåæ§è³æãçºäºè§£æ±ºéååé¡ï¼æåéç¼ä¸ç¨®æ°ç©çåè¨ç·´æ¹æ³ï¼å©ç¨å¤§åèªè¨æ¨¡å (LLM) å OpenStreetMap çè¼å©å°åè³æä¾æ¨å°å°çä½ç½®è¡¨ç¤º (LLMGeovec)ãLLMGeovec å¯ä»¥è¡¨ç¤ºåå¸ãåå®¶åå¨çè¦æ¨¡çå°çèªç¾©ï¼ä½çºæç©ºå­¸ç¿çéç¨å¢å¼·å¨ãå·é«ä¾èªªï¼ééç´æ¥ç¹å¾µä¸²æ¥ï¼æåå¼å¥äºä¸åç°¡å®ä½ææçç¯ä¾ï¼ç¨æ¼å¢å¼·å¤åæç©ºä»»åï¼åæ¬å°çé æ¸¬ (GP)ãé·ææéåºåé æ¸¬ (LTSF) ååºæ¼åå½¢çæç©ºé æ¸¬ (GSTF)ãLLMGeovec å¯ä»¥ç¡ç¸«æ´åå°å»£æ³çæç©ºå­¸ç¿æ¨¡åä¸­ï¼æä¾ç«å³çå¢å¼·ãå¯¦é©çµæè¡¨æï¼LLMGeovec éå°äºå¨çè¦èçï¼ä¸¦é¡¯èæåäºé åç GPãLTSF å GSTF æ¨¡åçæè½ã

##### **Enabling Small Models for Zero-Shot Classification through Model Label Learning**
2408.11449v1 by Jia Zhang, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li

Vision-language models (VLMs) like CLIP have demonstrated impressive
zero-shot ability in image classification tasks by aligning text and images but
suffer inferior performance compared with task-specific expert models. On the
contrary, expert models excel in their specialized domains but lack zero-shot
ability for new tasks. How to obtain both the high performance of expert models
and zero-shot ability is an important research direction. In this paper, we
attempt to demonstrate that by constructing a model hub and aligning models
with their functionalities using model labels, new tasks can be solved in a
zero-shot manner by effectively selecting and reusing models in the hub. We
introduce a novel paradigm, Model Label Learning (MLL), which bridges the gap
between models and their functionalities through a Semantic Directed Acyclic
Graph (SDAG) and leverages an algorithm, Classification Head Combination
Optimization (CHCO), to select capable models for new tasks. Compared with the
foundation model paradigm, it is less costly and more scalable, i.e., the
zero-shot ability grows with the sizes of the model hub. Experiments on seven
real-world datasets validate the effectiveness and efficiency of MLL,
demonstrating that expert models can be effectively reused for zero-shot tasks.
Our code will be released publicly.

æè¦ï¼è¦è¦ºèªè¨æ¨¡åï¼VLMï¼ï¼ä¾å¦ CLIPï¼å·²å¨å½±ååé¡ä»»åä¸­å±ç¾ä»¤äººå°è±¡æ·±å»çé¶æ¬¡å­¸ç¿è½åï¼æ¹æ³æ¯å°é½æå­åå½±åï¼ä½èç¹å®ä»»åçå°å®¶æ¨¡åç¸æ¯ï¼å¶æè½è¼å·®ãç¸åå°ï¼å°å®¶æ¨¡åå¨å¶å°æ¥­é åä¸­è¡¨ç¾åºè²ï¼ä½å°æ¼æ°ä»»åç¼ºä¹é¶æ¬¡å­¸ç¿è½åãå¦ä½åæç²å¾å°å®¶æ¨¡åçé«æè½åé¶æ¬¡å­¸ç¿è½åï¼æ¯ä¸åéè¦çç ç©¶æ¹åãå¨æ¬æä¸­ï¼æååè©¦ééå»ºç«æ¨¡åä¸­å¿ï¼ä¸¦ä½¿ç¨æ¨¡åæ¨ç±¤å°æ¨¡åèå¶åè½å°é½ï¼è­æå¯ä»¥ééææé¸æåéè¤ä½¿ç¨ä¸­å¿ä¸­çæ¨¡åï¼ä»¥é¶æ¬¡å­¸ç¿çæ¹å¼è§£æ±ºæ°ä»»åãæåæåºäºä¸ç¨®æ°çç¯ä¾ï¼å³æ¨¡åæ¨ç±¤å­¸ç¿ï¼MLLï¼ï¼å®ééèªç¾©å°åéå¾ªç°åï¼SDAGï¼å½åæ¨¡ååå¶åè½ä¹éçå·®è·ï¼ä¸¦å©ç¨ä¸ç¨®æ¼ç®æ³ï¼å³åé¡é ­çµåæä½³åï¼CHCOï¼ï¼çºæ°ä»»åé¸ææè½åçæ¨¡åãèåºç¤æ¨¡åç¯ä¾ç¸æ¯ï¼å®çææ¬è¼ä½ä¸æ´å·å¯æ´åæ§ï¼ä¹å°±æ¯èªªï¼é¶æ¬¡å­¸ç¿è½åæé¨èæ¨¡åä¸­å¿è¦æ¨¡çæ´å¤§èå¢é·ãå¨ä¸åçå¯¦ä¸çè³æéä¸çå¯¦é©é©è­äº MLL çæææ§åæçï¼è­æäºå°å®¶æ¨¡åå¯ä»¥ææå°éè¤ç¨æ¼é¶æ¬¡å­¸ç¿ä»»åãæåçç¨å¼ç¢¼å°å¬éç¼å¸ã

##### **Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles**
2408.11182v1 by Zhilong Wang, Haizhou Wang, Nanqing Luo, Lan Zhang, Xiaoyan Sun, Yebo Cao, Peng Liu

Jailbreak attacks on Language Model Models (LLMs) entail crafting prompts
aimed at exploiting the models to generate malicious content. This paper
proposes a new type of jailbreak attacks which shift the attention of the LLM
by inserting a prohibited query into a carrier article. The proposed attack
leverage the knowledge graph and a composer LLM to automatically generating a
carrier article that is similar to the topic of the prohibited query but does
not violate LLM's safeguards. By inserting the malicious query to the carrier
article, the assembled attack payload can successfully jailbreak LLM. To
evaluate the effectiveness of our method, we leverage 4 popular categories of
``harmful behaviors'' adopted by related researches to attack 6 popular LLMs.
Our experiment results show that the proposed attacking method can successfully
jailbreak all the target LLMs which high success rate, except for Claude-3.

æè¦ï¼èªè¨æ¨¡åæ¨¡åï¼LLMï¼çè¶çæ»ææ¶åè£½ä½æç¤ºï¼æ¨å¨å©ç¨æ¨¡åä¾ç¢çæ¡æå§å®¹ãæ¬ææåºäºä¸ç¨®æ°åçè¶çæ»æï¼å®ééå¨è¼é«æç« ä¸­æå¥ç¦æ­¢æ¥è©¢ä¾è½ç§» LLM çæ³¨æåãæè­°çæ»æå©ç¨ç¥è­åè­åä½æ²å®¶ LLM èªåçæèç¦æ­¢æ¥è©¢çä¸»é¡ç¸ä¼¼ä½ä¸æéå LLM ä¿éæªæ½çè¼é«æç« ãééå°æ¡ææ¥è©¢æå¥è¼é«æç« ä¸­ï¼çµè£çæ»æææè¼è·å¯ä»¥æåè¶ç LLMãçºäºè©ä¼°æåæ¹æ³çæææ§ï¼æåå©ç¨ç¸éç ç©¶æ¡ç¨ç 4 é¡æµè¡çãæå®³è¡çºãä¾æ»æ 6 åæµè¡ç LLMãæåçå¯¦é©çµæè¡¨æï¼ææåºçæ»ææ¹æ³å¯ä»¥æåè¶çææç®æ¨ LLMï¼æåçå¾é«ï¼é¤äº Claude-3ã

##### **Public Health in Disaster: Emotional Health and Life Incidents Extraction during Hurricane Harvey**
2408.11133v1 by Thomas Hoang, Quynh Anh Nguyen, Long Nguyen

Countless disasters have resulted from climate change, causing severe damage
to infrastructure and the economy. These disasters have significant societal
impacts, necessitating mental health services for the millions affected. To
prepare for and respond effectively to such events, it is important to
understand people's emotions and the life incidents they experience before and
after a disaster strikes. In this case study, we collected a dataset of
approximately 400,000 public tweets related to the storm. Using a BERT-based
model, we predicted the emotions associated with each tweet. To efficiently
identify these topics, we utilized the Latent Dirichlet Allocation (LDA)
technique for topic modeling, which allowed us to bypass manual content
analysis and extract meaningful patterns from the data. However, rather than
stopping at topic identification like previous methods \cite{math11244910}, we
further refined our analysis by integrating Graph Neural Networks (GNN) and
Large Language Models (LLM). The GNN was employed to generate embeddings and
construct a similarity graph of the tweets, which was then used to optimize
clustering. Subsequently, we used an LLM to automatically generate descriptive
names for each event cluster, offering critical insights for disaster
preparedness and response strategies.

æè¦ï¼ç¡æ¸çç½é£æ¯ç±æ¼æ°£åè®é·æé æçï¼å°åºç¤å»ºè¨­åç¶æ¿é æå´éçæå®³ãéäºç½é£å°ç¤¾æé æéå¤§çå½±é¿ï¼éè¦çºæ¸ç¾è¬åç½æ°ç¾æä¾å¿çå¥åº·æåãçºäºææå°çºæ­¤é¡äºä»¶åå¥½æºåä¸¦ä½åºåæï¼äºè§£äººåçæç·ä»¥åä»åå¨ç½é£ç¼çåå¾æç¶æ­·ççæ´»äºä»¶éå¸¸éè¦ãå¨æ¬æ¡ä¾ç ç©¶ä¸­ï¼æåæ¶éäºä¸ååå«ç´ 400,000 åèé¢¨æ´ç¸éçå¬éæ¨æçè³æéãä½¿ç¨åºæ¼ BERT çæ¨¡åï¼æåé æ¸¬äºèæ¯åæ¨æç¸éçæç·ãçºäºææçå°æ¾åºéäºä¸»é¡ï¼æåå©ç¨äºæ½å¨çå©åé·éç½® (LDA) æè¡é²è¡ä¸»é¡å»ºæ¨¡ï¼éè®æåè½å¤ ç¹éæåå§å®¹åæï¼å¾è³æä¸­èååºææç¾©çæ¨¡å¼ãç¶èï¼æåä¸¦æªåååçç ç©¶æ¹æ³ \cite{math11244910} é£æ¨£åæ­¢æ¼ä¸»é¡è¾¨è­ï¼èæ¯é²ä¸æ­¥æ´ååç¥ç¶ç¶²è·¯ (GNN) åå¤§åèªè¨æ¨¡å (LLM) ä¾åªåæåçåæãGNN è¢«ç¨æ¼ç¢çåµå¥åå»ºæ§æ¨æçç¸ä¼¼æ§åï¼ç¶å¾ç¨æ¼æä½³ååç¾¤ãé¨å¾ï¼æåä½¿ç¨ LLM çºæ¯åäºä»¶ç¾¤éèªåç¢çæè¿°æ§åç¨±ï¼çºç½å®³é²ç¯åæè®ç­ç¥æä¾éè¦çè¦è§£ã

##### **Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains**
2408.10819v1 by Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou

Knowledge graph completion (KGC) aims to identify missing triples in a
knowledge graph (KG). This is typically achieved through tasks such as link
prediction and instance completion. However, these methods often focus on
either static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs),
addressing only within-scope triples. This paper introduces a new generative
completion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC
employs a question-answering format to directly generate target entities,
addressing the challenge of questions having multiple possible answers. We
propose a strategy that extracts subgraphs centered on entities and
relationships within the KG, from which negative samples and neighborhood
information are separately obtained to address the one-to-many problem. Our
method generates negative samples using known facts to facilitate the discovery
of new information. Furthermore, we collect and refine neighborhood path data
of known entities, providing contextual information to enhance reasoning in
large language models (LLMs). Our experiments evaluated the proposed method on
four SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five
datasets. Analysis of the results shows that GS-KGC can discover new triples
within existing KGs and generate new facts beyond the closed KG, effectively
bridging the gap between closed-world and open-world KGC.

æè¦ï¼ç¥è­åè­è£å¨ (KGC) çç®æ¨æ¯è­å¥ç¥è­åè­ (KG) ä¸­éºå¤±çä¸åçµãééå¸¸ééé£çµé æ¸¬åå¯¦ä¾è£å¨ç­ä»»åéæãç¶èï¼éäºæ¹æ³éå¸¸å°æ³¨æ¼éæç¥è­åè­ (SKG) ææåºç¥è­åè­ (TKG)ï¼åèçç¯åå§çä¸åçµãæ¬æä»ç´¹ä¸ååçºçæå­åçºåºç¤ç KGC (GS-KGC) çæ°çæè£å¨æ¶æ§ãGS-KGC ä½¿ç¨åç­æ ¼å¼ç´æ¥çæç®æ¨å¯¦é«ï¼ä»¥è§£æ±ºåé¡æå¤åå¯è½ç­æ¡çææ°ãæåæåºä¸åç­ç¥ï¼å¾ç¥è­åè­ä¸­ä»¥å¯¦é«åéä¿çºä¸­å¿çå­åï¼å¾ä¸­åå¥åå¾è² é¢æ¨£æ¬åé°åè³è¨ï¼ä»¥è§£æ±ºä¸å°å¤åé¡ãæåçæ¨¡åä½¿ç¨å·²ç¥äºå¯¦çæè² é¢æ¨£æ¬ï¼ä»¥å©ç¼ç¾æ°è³è¨ãæ­¤å¤ï¼æåæ¶éä¸¦ç²¾çå·²ç¥å¯¦é«çé°åè·¯å¾è³æï¼æä¾èæ¯è³è¨ä»¥å¢å¼·å¤§åèªè¨æ¨¡å (LLM) ä¸­çæ¨çãæåçå¯¦é©å¨åå SKG åå©å TKG ä¸è©ä¼°ææåºçæ¹æ³ï¼å¨äºåè³æéä¸éææåé²ç Hits@1 ææ¨ãçµæåæé¡¯ç¤ºï¼GS-KGC è½å¤ å¨ç¾æç KG ä¸­ç¼ç¾æ°çä¸åçµï¼ä¸¦çæå°é KG ä»¥å¤çæ°äºå¯¦ï¼ææå°ç¸®å°å°éä¸çåéæ¾ä¸ç KGC ä¹éçå·®è·ã

##### **Hologram Reasoning for Solving Algebra Problems with Geometry Diagrams**
2408.10592v1 by Litian Huang, Xinguo Yu, Feng Xiong, Bin He, Shengbing Tang, Jiawen Fu

Solving Algebra Problems with Geometry Diagrams (APGDs) is still a
challenging problem because diagram processing is not studied as intensively as
language processing. To work against this challenge, this paper proposes a
hologram reasoning scheme and develops a high-performance method for solving
APGDs by using this scheme. To reach this goal, it first defines a hologram,
being a kind of graph, and proposes a hologram generator to convert a given
APGD into a hologram, which represents the entire information of APGD and the
relations for solving the problem can be acquired from it by a uniform way.
Then HGR, a hologram reasoning method employs a pool of prepared graph models
to derive algebraic equations, which is consistent with the geometric theorems.
This method is able to be updated by adding new graph models into the pool.
Lastly, it employs deep reinforcement learning to enhance the efficiency of
model selection from the pool. The entire HGR not only ensures high solution
accuracy with fewer reasoning steps but also significantly enhances the
interpretability of the solution process by providing descriptions of all
reasoning steps. Experimental results demonstrate the effectiveness of HGR in
improving both accuracy and interpretability in solving APGDs.

æè¦ï¼å©ç¨å¹¾ä½åå½¢åï¼APGDï¼è§£æ±ºä»£æ¸åé¡ä»ç¶æ¯ä¸åå·æææ°æ§çåé¡ï¼å çºåå½¢èççç ç©¶ä¸å¦èªè¨èçé£éº¼æ·±å¥ãçºäºæå°éä¸ææ°ï¼æ¬ææåºäºä¸ç¨®å¨æ¯æ¨çæ¹æ¡ï¼ä¸¦éç¼äºä¸ç¨®ä½¿ç¨è©²æ¹æ¡è§£æ±º APGD çé«æ§è½æ¹æ³ãçºäºéå°éåç®æ¨ï¼å®é¦åå®ç¾©äºä¸åå¨æ¯åï¼ä½çºä¸ç¨®åå½¢ï¼ä¸¦æåºäºä¸åå¨æ¯åçæå¨ï¼å°çµ¦å®ç APGD è½æçºä¸åå¨æ¯åï¼å®è¡¨ç¤º APGD çå¨é¨ä¿¡æ¯ï¼ä¸¦ä¸å¯ä»¥ééçµ±ä¸çæ¹å¼å¾ä¸­ç²åè§£æ±ºåé¡çéä¿ãç¶å¾ï¼HGRï¼ä¸ç¨®å¨æ¯æ¨çæ¹æ³ï¼æ¡ç¨ä¸çµæºåå¥½çåå½¢æ¨¡åä¾æ¨å°ä»£æ¸æ¹ç¨å¼ï¼éèå¹¾ä½å®çæ¯ä¸è´çãéç¨®æ¹æ³å¯ä»¥ééåæ± ä¸­æ·»å æ°çåå½¢æ¨¡åä¾æ´æ°ãæå¾ï¼å®æ¡ç¨æ·±åº¦å¼·åå­¸ç¿ä¾æé«å¾æ± ä¸­é¸ææ¨¡åçæçãæ´å HGR ä¸åç¢ºä¿äºè¼å°çæ¨çæ­¥é©å³å¯ç²å¾è¼é«çæ±è§£ç²¾åº¦ï¼èä¸éééæä¾æææ¨çæ­¥é©çæè¿°ä¾é¡¯èå¢å¼·äºè§£æ±ºéç¨çå¯è§£éæ§ãå¯¦é©çµæè­æäº HGR å¨æé«æ±è§£ APGD çæºç¢ºæ§åå¯è§£éæ§æ¹é¢çæææ§ã

##### **Query languages for neural networks**
2408.10362v2 by Martin Grohe, Christoph Standke, Juno Steegmans, Jan Van den Bussche

We lay the foundations for a database-inspired approach to interpreting and
understanding neural network models by querying them using declarative
languages. Towards this end we study different query languages, based on
first-order logic, that mainly differ in their access to the neural network
model. First-order logic over the reals naturally yields a language which views
the network as a black box; only the input--output function defined by the
network can be queried. This is essentially the approach of constraint query
languages. On the other hand, a white-box language can be obtained by viewing
the network as a weighted graph, and extending first-order logic with summation
over weight terms. The latter approach is essentially an abstraction of SQL. In
general, the two approaches are incomparable in expressive power, as we will
show. Under natural circumstances, however, the white-box approach can subsume
the black-box approach; this is our main result. We prove the result concretely
for linear constraint queries over real functions definable by feedforward
neural networks with a fixed number of hidden layers and piecewise linear
activation functions.

æè¦ï¼<paragraph>æåå¥ å®äºä¸ååè³æåº«åç¼çåºç¤ï¼ç¨æ¼ééä½¿ç¨å®£åå¼èªè¨å°ç¥ç¶ç¶²è·¯æ¨¡åé²è¡è©®éåçè§£ãçºäºéå°éåç®çï¼æåç ç©¶äºåºæ¼ä¸ééè¼¯çä¸åæ¥è©¢èªè¨ï¼å®åä¸»è¦å¨æ¼å°ç¥ç¶ç¶²è·¯æ¨¡åçå­åæ¹å¼ä¸åãä¸éå¯¦æ¸éè¼¯èªç¶æç¢çä¸ç¨®èªè¨ï¼å°ç¶²è·¯è¦çºä¸åé»çå­ï¼åªè½æ¥è©¢ç¶²è·¯å®ç¾©çè¼¸å¥è¼¸åºå½æ¸ãéåºæ¬ä¸æ¯ç´ææ¥è©¢èªè¨çæ¹æ³ãå¦ä¸æ¹é¢ï¼å¯ä»¥ééå°ç¶²è·¯è¦çºä¸åå æ¬åï¼ä¸¦å°ä¸ééè¼¯å»¶ä¼¸å°æ¬éé ä¸çç¸½åï¼ä¾åå¾ä¸åç½çèªè¨ãå¾èæ¹æ³åºæ¬ä¸æ¯ SQL çæ½è±¡ãä¸è¬ä¾èªªï¼éå©ç¨®æ¹æ³å¨è¡¨éè½åä¸ç¡æ³ç¸æä¸¦è«ï¼æåå°æè­æéä¸é»ãç¶èï¼å¨èªç¶ææ³ä¸ï¼ç½çæ¹æ³å¯ä»¥åå«é»çæ¹æ³ï¼éæ¯æåçéé»ãæåå·é«è­æäºç·æ§ç´ææ¥è©¢å°æ¼ç±å·æåºå®æ¸éé±èå±¤ååæ®µç·æ§æ¿æ´»å½æ¸çåé¥ç¥ç¶ç¶²è·¯å¯å®ç¾©çå¯¦å½æ¸ã</paragraph>

##### **Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**
2408.10124v1 by Tianyu Zhang, Yuxiang Ren, Chengbin Hou, Hairong Lv, Xuegong Zhang

Molecular property prediction is a crucial foundation for drug discovery. In
recent years, pre-trained deep learning models have been widely applied to this
task. Some approaches that incorporate prior biological domain knowledge into
the pre-training framework have achieved impressive results. However, these
methods heavily rely on biochemical experts, and retrieving and summarizing
vast amounts of domain knowledge literature is both time-consuming and
expensive. Large Language Models (LLMs) have demonstrated remarkable
performance in understanding and efficiently providing general knowledge.
Nevertheless, they occasionally exhibit hallucinations and lack precision in
generating domain-specific knowledge. Conversely, Domain-specific Small Models
(DSMs) possess rich domain knowledge and can accurately calculate molecular
domain-related metrics. However, due to their limited model size and singular
functionality, they lack the breadth of knowledge necessary for comprehensive
representation learning. To leverage the advantages of both approaches in
molecular property prediction, we propose a novel Molecular Graph
representation learning framework that integrates Large language models and
Domain-specific small models (MolGraph-LarDo). Technically, we design a
two-stage prompt strategy where DSMs are introduced to calibrate the knowledge
provided by LLMs, enhancing the accuracy of domain-specific information and
thus enabling LLMs to generate more precise textual descriptions for molecular
samples. Subsequently, we employ a multi-modal alignment method to coordinate
various modalities, including molecular graphs and their corresponding
descriptive texts, to guide the pre-training of molecular representations.
Extensive experiments demonstrate the effectiveness of the proposed method.

æè¦ï¼åå­ç¹æ§é æ¸¬æ¯è¥ç©ç¼ç¾çééµåºç¤ãè¿å¹´ä¾ï¼é è¨ç·´æ·±åº¦å­¸ç¿æ¨¡åå·²å»£æ³æç¨æ¼æ­¤ä»»åãä¸äºå°åé©çç©é åç¥è­ç´å¥é è¨ç·´æ¶æ§çæ¹æ³å·²åå¾ä»¤äººå°è±¡æ·±å»çææãç¶èï¼éäºæ¹æ³å´éä¾è³´æ¼çç©åå­¸å°å®¶ï¼ä¸¦ä¸æª¢ç´¢åç¸½çµå¤§éçé åç¥è­æç»æ¢èæåæè²´ãå¤§åèªè¨æ¨¡å (LLM) å¨çè§£åæææä¾ä¸è¬ç¥è­æ¹é¢å±ç¤ºäºåè¶çæ§è½ãåç®¡å¦æ­¤ï¼å®åå¶ç¾æåºç¾å¹»è¦ºï¼ä¸¦ä¸å¨çæç¹å®é åçç¥è­æç¼ºä¹ç²¾ç¢ºæ§ãç¸åï¼ç¹å®é åçå°æ¨¡å (DSM) ææè±å¯çé åç¥è­ï¼ä¸¦ä¸å¯ä»¥æºç¢ºè¨ç®èåå­é åç¸éçææ¨ãç¶èï¼ç±æ¼å®åæéçæ¨¡åå¤§å°åå®ä¸åè½ï¼å®åç¼ºä¹å¨é¢è¡¨ç¤ºå­¸ç¿æéçç¥è­å»£åº¦ãçºäºå¨åå­ç¹æ§é æ¸¬ä¸­å©ç¨éå©ç¨®æ¹æ³çåªé»ï¼æåæåºäºä¸åæ°ç©çåå­åè¡¨ç¤ºå­¸ç¿æ¡æ¶ï¼å®éæäºå¤§åèªè¨æ¨¡ååç¹å®é åçå°æ¨¡å (MolGraph-LarDo)ãå¨æè¡ä¸ï¼æåè¨­è¨äºä¸åå©éæ®µæç¤ºç­ç¥ï¼å¶ä¸­å¼å¥ DSM ä¾æ ¡æº LLM æä¾çç¥è­ï¼æé«ç¹å®é åä¿¡æ¯çæºç¢ºæ§ï¼å¾èä½¿ LLM è½å¤ çºåå­æ¨£æ¬çææ´ç²¾ç¢ºçææ¬æè¿°ãé¨å¾ï¼æåæ¡ç¨å¤æ¨¡æå°é½æ¹æ³ä¾åèª¿åç¨®æ¨¡æï¼åæ¬åå­ååå¶å°æçæè¿°æ§ææ¬ï¼ä»¥æå°åå­è¡¨ç¤ºçé è¨ç·´ãå»£æ³çå¯¦é©è­æäºææåºæ¹æ³çæææ§ã

##### **Geometry Informed Tokenization of Molecules for Language Model Generation**
2408.10120v1 by Xiner Li, Limei Wang, Youzhi Luo, Carl Edwards, Shurui Gui, Yuchao Lin, Heng Ji, Shuiwang Ji

We consider molecule generation in 3D space using language models (LMs),
which requires discrete tokenization of 3D molecular geometries. Although
tokenization of molecular graphs exists, that for 3D geometries is largely
unexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which
converts molecular geometries into $SE(3)$-invariant 1D discrete sequences.
Geo2Seq consists of canonical labeling and invariant spherical representation
steps, which together maintain geometric and atomic fidelity in a format
conducive to LMs. Our experiments show that, when coupled with Geo2Seq, various
LMs excel in molecular geometry generation, especially in controlled generation
tasks.

æè¦ï¼æåèæ®ä½¿ç¨èªè¨æ¨¡å (LM) å¨ 3D ç©ºéä¸­çæåå­ï¼ééè¦å° 3D åå­å¹¾ä½çµæ§é²è¡é¢æ£çæ¨è¨åãåç®¡å­å¨åå­åçæ¨è¨åï¼ä½å° 3D å¹¾ä½çµæ§çæ¨è¨åå¨å¾å¤§ç¨åº¦ä¸å°æªè¢«æ¢ç´¢ãå¨æ­¤ï¼æååè©¦ééæåº Geo2Seq ä¾å½åéä¸å·®è·ï¼è©²æ¹æ³å°åå­å¹¾ä½çµæ§è½æçº $SE(3)$ ä¸è®ç 1D é¢æ£åºåãGeo2Seq åå«è¦ç¯æ¨ç±¤åä¸è®çé¢è¡¨ç¤ºæ­¥é©ï¼å®åå±åä»¥æå©æ¼ LM çæ ¼å¼ä¿æå¹¾ä½ååå­ä¿çåº¦ãæåçå¯¦é©è¡¨æï¼ç¶è Geo2Seq çµåä½¿ç¨æï¼åç¨® LM å¨åå­å¹¾ä½çææ¹é¢è¡¨ç¾åºè²ï¼ç¹å¥æ¯å¨åæ§çæä»»åä¸­ã

##### **GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization**
2408.10115v1 by Ran Liu, Ming Liu, Min Yu, Jianguo Jiang, Gang Li, Dan Zhang, Jingyuan Li, Xiang Meng, Weiqing Huang

Pre-trained language models are increasingly being used in multi-document
summarization tasks. However, these models need large-scale corpora for
pre-training and are domain-dependent. Other non-neural unsupervised
summarization approaches mostly rely on key sentence extraction, which can lead
to information loss. To address these challenges, we propose a lightweight yet
effective unsupervised approach called GLIMMER: a Graph and LexIcal features
based unsupervised Multi-docuMEnt summaRization approach. It first constructs a
sentence graph from the source documents, then automatically identifies
semantic clusters by mining low-level features from raw texts, thereby
improving intra-cluster correlation and the fluency of generated sentences.
Finally, it summarizes clusters into natural sentences. Experiments conducted
on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach
outperforms existing unsupervised approaches. Furthermore, it surpasses
state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS
and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally,
human evaluations indicate that summaries generated by GLIMMER achieve high
readability and informativeness scores. Our code is available at
https://github.com/Oswald1997/GLIMMER.

æè¦ï¼é¢è®­ç»è¯­è¨æ¨¡åå¨å¤æä»¶æè¦ä»»å¡ä¸­è¢«è¶æ¥è¶å¤å°ä½¿ç¨ãç¶èï¼è¿äºæ¨¡åéè¦å¤§è§æ¨¡è¯­æåºè¿è¡é¢è®­ç»ï¼å¹¶ä¸ä¾èµäºé¢åãå¶ä»éç¥ç»æ çç£æè¦æ¹æ³ä¸»è¦ä¾èµäºå³é®å¥å­æåï¼è¿å¯è½å¯¼è´ä¿¡æ¯ä¸¢å¤±ãä¸ºäºåºå¯¹è¿äºææï¼æä»¬æåºäºä¸ç§è½»éçº§ä½ææçæ çç£æ¹æ³ï¼ç§°ä¸º GLIMMERï¼ä¸ç§åºäºå¾åè¯æ±ç¹å¾çæ çç£å¤ææ¡£æè¦æ¹æ³ãå®é¦åä»æºææ¡£æå»ºä¸ä¸ªå¥å­å¾ï¼ç¶åéè¿ä»åå§ææ¬ä¸­ææä½çº§ç¹å¾èªå¨è¯å«è¯­ä¹ç°ï¼ä»èæé«ç°åç¸å³æ§åçæå¥å­çæµçæ§ãæåï¼å®å°ç°æ»ç»ä¸ºèªç¶å¥å­ãå¨ Multi-NewsãMulti-XScience å DUC-2004 ä¸è¿è¡çå®éªè¡¨æï¼æä»¬çæ¹æ³ä¼äºç°æçæ çç£æ¹æ³ãæ­¤å¤ï¼å¨é¶æ ·æ¬è®¾ç½®ä¸ï¼å®å¨ ROUGE å¾åæ¹é¢è¶è¶äºæåè¿çé¢è®­ç»å¤ææ¡£æè¦æ¨¡åï¼ä¾å¦ PEGASUS å PRIMERAï¼ãæ­¤å¤ï¼äººç±»è¯ä¼°è¡¨æï¼GLIMMER çæçæè¦è·å¾äºå¾é«çå¯è¯»æ§åä¿¡æ¯æ§å¾åãæä»¬çä»£ç å¯å¨ https://github.com/Oswald1997/GLIMMER è·å¾ã

##### **SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction with Legal Clue Tracing**
2408.09717v1 by Pengjie Liu, Wang Zhang, Yulong Ding, Xuefeng Zhang, Shuang-Hua Yang

Legal Judgment Prediction (LJP) aims to form legal judgments based on the
criminal fact description. However, researchers struggle to classify confusing
criminal cases, such as robbery and theft, which requires LJP models to
distinguish the nuances between similar crimes. Existing methods usually design
handcrafted features to pick up necessary semantic legal clues to make more
accurate legal judgment predictions. In this paper, we propose a Semantic-Aware
Dual Encoder Model (SEMDR), which designs a novel legal clue tracing mechanism
to conduct fine-grained semantic reasoning between criminal facts and
instruments. Our legal clue tracing mechanism is built from three reasoning
levels: 1) Lexicon-Tracing, which aims to extract criminal facts from criminal
descriptions; 2) Sentence Representation Learning, which contrastively trains
language models to better represent confusing criminal facts; 3) Multi-Fact
Reasoning, which builds a reasons graph to propagate semantic clues among fact
nodes to capture the subtle difference among criminal facts. Our legal clue
tracing mechanism helps SEMDR achieve state-of-the-art on the CAIL2018 dataset
and shows its advance in few-shot scenarios. Our experiments show that SEMDR
has a strong ability to learn more uniform and distinguished representations
for criminal facts, which helps to make more accurate predictions on confusing
criminal cases and reduces the model uncertainty during making judgments. All
codes will be released via GitHub.

æè¦ï¼æ³å¾å¤æ±ºé æ¸¬ (LJP) æ¨å¨æ ¹æç¯ç½ªäºå¯¦æè¿°å½¢ææ³å¾å¤æ±ºãç¶èï¼ç ç©¶äººå¡é£ä»¥å°æ¶å«åçç«ç­ä»¤äººå°æçåäºæ¡ä»¶é²è¡åé¡ï¼ééè¦ LJP æ¨¡åååé¡ä¼¼ç¯ç½ªä¹éçç´°å¾®å·®å¥ãç¾ææ¹æ³éå¸¸è¨­è¨æå·¥ç¹å¾µä»¥ç²åå¿è¦çèªç¾©æ³å¾ç·ç´¢ï¼ä»¥ååºæ´æºç¢ºçæ³å¾å¤æ±ºé æ¸¬ãå¨æ¬æä¸­ï¼æåæåºäºä¸åèªç¾©æç¥éç·¨ç¢¼å¨æ¨¡å (SEMDR)ï¼å®è¨­è¨äºä¸ç¨®æ°ç©çæ³å¾ç·ç´¢è¿½è¹¤æ©å¶ï¼ä»¥å¨ç¯ç½ªäºå¯¦åå·¥å·ä¹éé²è¡ç´°ç²åº¦çèªç¾©æ¨çãæåçæ³å¾ç·ç´¢è¿½è¹¤æ©å¶å»ºç«å¨ä¸åæ¨çå±¤ç´ä¹ä¸ï¼1) è©å½è¿½è¹¤ï¼æ¨å¨å¾ç¯ç½ªæè¿°ä¸­æåç¯ç½ªäºå¯¦ï¼2) å¥å­è¡¨ç¤ºå­¸ç¿ï¼å°æ¯è¨ç·´èªè¨æ¨¡åä»¥æ´å¥½å°è¡¨ç¤ºä»¤äººå°æçç¯ç½ªäºå¯¦ï¼3) å¤äºå¯¦æ¨çï¼æ§å»ºä¸ååå åï¼å¨äºå¯¦ç¯é»ä¹éå³æ­èªç¾©ç·ç´¢ï¼ä»¥ææç¯ç½ªäºå¯¦ä¹éçç´°å¾®å·®å¥ãæåçæ³å¾ç·ç´¢è¿½è¹¤æ©å¶å¹«å© SEMDR å¨ CAIL2018 è³æéä¸å¯¦ç¾äºæåé²çæè¡ï¼ä¸¦å±ç¤ºäºå¶å¨å°é¡é ­å ´æ¯ä¸­çé²æ­¥ãæåçå¯¦é©è¡¨æï¼SEMDR å·æå­¸ç¿æ´çµ±ä¸ååå¥çç¯ç½ªäºå¯¦è¡¨ç¤ºçå¼·å¤§è½åï¼éæå©æ¼å°ä»¤äººå°æçåäºæ¡ä»¶ååºæ´æºç¢ºçé æ¸¬ï¼ä¸¦å¨ååºå¤æ±ºææ¸å°æ¨¡åçä¸ç¢ºå®æ§ãææä»£ç¢¼é½å°éé GitHub ç¼å¸ã

##### **Revisiting the Graph Reasoning Ability of Large Language Models: Case Studies in Translation, Connectivity and Shortest Path**
2408.09529v1 by Xinnan Dai, Qihao Wen, Yifei Shen, Hongzhi Wen, Dongsheng Li, Jiliang Tang, Caihua Shan

Large Language Models (LLMs) have achieved great success in various reasoning
tasks. In this work, we focus on the graph reasoning ability of LLMs. Although
theoretical studies proved that LLMs are capable of handling graph reasoning
tasks, empirical evaluations reveal numerous failures. To deepen our
understanding on this discrepancy, we revisit the ability of LLMs on three
fundamental graph tasks: graph description translation, graph connectivity, and
the shortest-path problem. Our findings suggest that LLMs can fail to
understand graph structures through text descriptions and exhibit varying
performance for all these three fundamental tasks. Meanwhile, we perform a
real-world investigation on knowledge graphs and make consistent observations
with our findings. The codes and datasets are available.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨åç¨®æ¨çä»»åä¸­å·²åå¾å·¨å¤§çæåãå¨éé å·¥ä½ä¸­ï¼æåå°æ³¨æ¼ LLM çåå½¢æ¨çè½åãåç®¡çè«ç ç©¶è­æ LLM æè½åèçåå½¢æ¨çä»»åï¼ä½ç¶é©è©ä¼°é¡¯ç¤ºåºè¨±å¤å¤±æãçºäºå æ·±æåå°éç¨®å·®ç°ççè§£ï¼æåéæ°æ¢è¨ LLM å¨ä¸ååºæ¬åå½¢ä»»åä¸çè½åï¼åå½¢æè¿°ç¿»è­¯ãåå½¢é£éæ§åæç­è·¯å¾åé¡ãæåçç ç©¶çµæè¡¨æï¼LLM å¯è½ç¡æ³ééææ¬æè¿°çè§£åå½¢çµæ§ï¼ä¸¦ä¸å¨ææéä¸ååºæ¬ä»»åä¸­è¡¨ç¾åºä¸åçæ§è½ãåæï¼æåå°ç¥è­åè­é²è¡äºç¾å¯¦ä¸ççèª¿æ¥ï¼ä¸¦å°æåçç¼ç¾é²è¡äºä¸è´çè§å¯ãä»£ç¢¼åæ¸æéå¯ç¨ã

##### **Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting**
2408.13273v1 by Geethan Sannidhi, Sagar Srinivas Sakhinana, Venkataramana Runkana

Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google
Gemini face challenges such as inaccurate factual recall, hallucinations,
biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting.
To address these issues, we introduce sLA-tKGF (small-scale language assistant
for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG)
aided, custom-trained small-scale language models through a tabula rasa
approach from scratch for effective tKG forecasting. Our framework constructs
knowledge-infused prompts with relevant historical data from tKGs, web search
results, and PLLMs-generated textual descriptions to understand historical
entity relationships prior to the target time. It leverages these external
knowledge-infused prompts for deeper understanding and reasoning of
context-specific semantic and temporal information to zero-shot prompt
small-scale language models for more accurate predictions of future events
within tKGs. It reduces hallucinations and mitigates distributional shift
challenges through comprehending changing trends over time. As a result, it
enables more accurate and contextually grounded forecasts of future events
while minimizing computational demands. Rigorous empirical studies demonstrate
our framework robustness, scalability, and state-of-the-art (SOTA) performance
on benchmark datasets with interpretable and trustworthy tKG forecasting.

æè¦ï¼é è¨ç·´å¤§åèªè¨æ¨¡åï¼PLLMï¼ï¼ä¾å¦ OpenAI ChatGPT å Google
Gemini é¢è¨ææ°ï¼ä¾å¦ä¸æºç¢ºçäºå¯¦åæ¶ãå¹»è¦ºã
åè¦åæéç¥è­åï¼tKGï¼é æ¸¬çæªä¾æ¸ææ´©æ¼ã
çºäºè§£æ±ºéäºåé¡ï¼æåå¼å¥äº sLA-tKGFï¼tKG é æ¸¬çå°è¦æ¨¡èªè¨å©çï¼ï¼å®å©ç¨æª¢ç´¢å¢å¼·çæï¼RAGï¼
è¼å©ï¼å¾é ­éå§ééç½æ¿æ³èªè¨è¨ç·´çå°è¦æ¨¡èªè¨æ¨¡åï¼ä»¥é²è¡ææç tKG é æ¸¬ãæåçæ¶æ§å»ºæ§
æ³¨å¥ç¥è­çæç¤ºï¼å¶ä¸­åå«ä¾èª tKGãç¶²è·¯æå°
çµæå PLLM çæçæå­æè¿°ï¼ä»¥äºè§£ç®æ¨æéä¹åçæ­·å²å¯¦é«éä¿ãå®å©ç¨éäºå¤é¨
æ³¨å¥ç¥è­çæç¤ºï¼ä»¥æ´æ·±å¥å°çè§£åæ¨ç
ç¹å®æ¼èçµ¡çèªç¾©åæéè³è¨ï¼ä»¥é¶æ¬¡æç¤ºå°è¦æ¨¡èªè¨æ¨¡åï¼ä»¥æ´æºç¢ºå°é æ¸¬ tKG ä¸­çæªä¾äºä»¶ãå®æ¸å°å¹»è¦ºä¸¦ééäºè§£é¨æéè®åçè¶¨å¢ä¾æ¸è¼åä½è½ç§»ææ°ãå æ­¤ï¼å®
è½æ´æºç¢ºä¸æèçµ¡å°é æ¸¬æªä¾äºä»¶ï¼åæå°éç®éæ±éè³æä½ãå´è¬¹çå¯¦è­ç ç©¶è­æ
æåçæ¶æ§å·æç©©å¥æ§ãå¯æ´åæ§åæåé²ï¼SOTAï¼æè½
å¨åºæºè³æéä¸é²è¡å¯è§£éä¸å¼å¾ä¿¡è³´ç tKG é æ¸¬ã

##### **Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models**
2408.09429v1 by Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, Xuming Hu

Hallucination issues persistently plagued current multimodal large language
models (MLLMs). While existing research primarily focuses on object-level or
attribute-level hallucinations, sidelining the more sophisticated relation
hallucinations that necessitate advanced reasoning abilities from MLLMs.
Besides, recent benchmarks regarding relation hallucinations lack in-depth
evaluation and effective mitigation. Moreover, their datasets are typically
derived from a systematic annotation process, which could introduce inherent
biases due to the predefined process. To handle the aforementioned challenges,
we introduce Reefknot, a comprehensive benchmark specifically targeting
relation hallucinations, consisting of over 20,000 samples derived from
real-world scenarios. Specifically, we first provide a systematic definition of
relation hallucinations, integrating perspectives from perceptive and cognitive
domains. Furthermore, we construct the relation-based corpus utilizing the
representative scene graph dataset Visual Genome (VG), from which semantic
triplets follow real-world distributions. Our comparative evaluation across
three distinct tasks revealed a substantial shortcoming in the capabilities of
current MLLMs to mitigate relation hallucinations. Finally, we advance a novel
confidence-based mitigation strategy tailored to tackle the relation
hallucinations problem. Across three datasets, including Reefknot, we observed
an average reduction of 9.75% in the hallucination rate. We believe our paper
sheds valuable insights into achieving trustworthy multimodal intelligence. Our
dataset and code will be released upon paper acceptance.

æè¦ï¼å¹»è¦ºåé¡æçºå°æ¾èç¶åçå¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM)ãéç¶ç¾æç ç©¶ä¸»è¦éæ³¨ç©ä»¶å±¤ç´æå±¬æ§å±¤ç´çå¹»è¦ºï¼ä½å»å¿½è¦äºéè¦ MLLM å·åé²éæ¨çè½åçæ´è¤ééä¿å¹»è¦ºãæ­¤å¤ï¼éæ¼éä¿å¹»è¦ºçææ°åºæºç¼ºä¹æ·±å¥è©ä¼°åææçç·©è§£æªæ½ãèä¸ï¼ä»åçè³æééå¸¸ä¾èªç³»çµ±åçè¨»ééç¨ï¼éå¯è½æå çºé åå®ç¾©çéç¨èå¼å¥åºæçåå·®ãçºäºæå°ä¸è¿°ææ°ï¼æåå¼å¥äº Reefknotï¼éæ¯ä¸åå°ééå°éä¿å¹»è¦ºçç¶ååºæºï¼åå«è¶é 20,000 åä¾èªçå¯¦ä¸çå ´æ¯çç¯ä¾ãå·é«ä¾èªªï¼æåé¦åæä¾éä¿å¹»è¦ºçç³»çµ±æ§å®ç¾©ï¼æ´åä¾èªç¥è¦ºåèªç¥é åçè§é»ãæ­¤å¤ï¼æåå©ç¨å·æä»£è¡¨æ§çå ´æ¯åå½¢è³æé Visual Genome (VG) å»ºæ§åºæ¼éä¿çèªæåº«ï¼èªç¾©ä¸åçµéµå¾ªçå¯¦ä¸ççåä½ãæåå¨ä¸åä¸åçä»»åä¸­é²è¡æ¯è¼è©ä¼°ï¼æ­ç¤ºäºç¶å MLLM å¨æ¸è¼éä¿å¹»è¦ºæ¹é¢çè½åå­å¨éå¤§ç¼ºé·ãæå¾ï¼æåæåºäºä¸ç¨®æ°çåºæ¼ä¿¡å¿çç·©è§£ç­ç¥ï¼å°éç¨æ¼è§£æ±ºéä¿å¹»è¦ºåé¡ãå¨åæ¬ Reefknot å¨å§çä¸åè³æéä¸­ï¼æåè§å¯å°å¹»è¦ºçå¹³åéä½äº 9.75%ãæåç¸ä¿¡æåçè«æå°å¯¦ç¾å¼å¾ä¿¡è³´çå¤æ¨¡ææºæ§æä¾äºå¯¶è²´çè¦è§£ãæåçè³æéåç¨å¼ç¢¼å°å¨è«æè¢«æ¥åå¾ç¼å¸ã

##### **ASGM-KG: Unveiling Alluvial Gold Mining Through Knowledge Graphs**
2408.08972v1 by Debashis Gupta, Aditi Golder, Luis Fernendez, Miles Silman, Greg Lersen, Fan Yang, Bob Plemmons, Sarra Alqahtani, Paul Victor Pauca

Artisanal and Small-Scale Gold Mining (ASGM) is a low-cost yet highly
destructive mining practice, leading to environmental disasters across the
world's tropical watersheds. The topic of ASGM spans multiple domains of
research and information, including natural and social systems, and knowledge
is often atomized across a diversity of media and documents. We therefore
introduce a knowledge graph (ASGM-KG) that consolidates and provides crucial
information about ASGM practices and their environmental effects. The current
version of ASGM-KG consists of 1,899 triples extracted using a large language
model (LLM) from documents and reports published by both non-governmental and
governmental organizations. These documents were carefully selected by a group
of tropical ecologists with expertise in ASGM. This knowledge graph was
validated using two methods. First, a small team of ASGM experts reviewed and
labeled triples as factual or non-factual. Second, we devised and applied an
automated factual reduction framework that relies on a search engine and an LLM
for labeling triples. Our framework performs as well as five baselines on a
publicly available knowledge graph and achieves over 90 accuracy on our ASGM-KG
validated by domain experts. ASGM-KG demonstrates an advancement in knowledge
aggregation and representation for complex, interdisciplinary environmental
crises such as ASGM.

æè¦ï¼æå·¥åå°åæ¡éï¼ASGMï¼æ¯ä¸ç¨®ä½ææ¬ä½é«åº¦ç ´å£æ§çæ¡ç¤¦å¯¦åï¼å°è´å¨çç±å¸¶æµåç¼çç°å¢ç½é£ãASGM çä¸»é¡æ¶µèå¤åç ç©¶åè³è¨é åï¼åæ¬èªç¶åç¤¾æç³»çµ±ï¼èç¥è­éå¸¸åæ£å¨åç¨®åªé«åæä»¶ä¸­ãå æ­¤ï¼æåå¼å¥ç¥è­åè­ (ASGM-KG)ï¼å®æ´åä¸¦æä¾æé ASGM å¯¦ååå¶ç°å¢å½±é¿çéè¦è³è¨ãç®åçæ¬ç ASGM-KG åå« 1,899 åä¸åçµï¼éäºä¸åçµæ¯ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) å¾éæ¿åºçµç¹åæ¿åºçµç¹ç¼å¸çæä»¶åå ±åä¸­æååºä¾çãéäºæä»¶æ¯ç±ä¸ç¾¤å·æ ASGM å°æ¥­ç¥è­çç±å¸¶çæå­¸å®¶ä»ç´°æé¸çãéåç¥è­åè­ä½¿ç¨å©ç¨®æ¹æ³é©è­ãé¦åï¼ä¸å°çµ ASGM å°å®¶å¯©æ¥ä¸¦å°ä¸åçµæ¨è¨çºäºå¯¦æéäºå¯¦ãå¶æ¬¡ï¼æåè¨­è¨ä¸¦æç¨äºä¸åèªååçäºå¯¦ç°¡åæ¶æ§ï¼è©²æ¶æ§ä¾è³´æ¼æå°å¼æå LLM ä¾æ¨è¨ä¸åçµãæåçæ¶æ§å¨å¬éçç¥è­åè­ä¸å·è¡å¾èäºååºæºä¸æ¨£å¥½ï¼ä¸¦å¨æåç±é åå°å®¶é©è­ç ASGM-KG ä¸éå°è¶é 90 çæºç¢ºåº¦ãASGM-KG å±ç¤ºäºè¤éçè·¨å­¸ç§ç°å¢å±æ©ï¼ä¾å¦ ASGMï¼çç¥è­å½æ´åè¡¨ç¤ºæ¹é¢çä¸é é²å±ã

##### **EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics**
2408.08782v1 by Chenwei Wan, Matthieu Labeau, ChloÃ© Clavel

Designing emotionally intelligent conversational systems to provide comfort
and advice to people experiencing distress is a compelling area of research.
Previous efforts have focused on developing modular dialogue systems that treat
socio-emotional strategy prediction as an auxiliary task and generate
strategy-conditioned responses with customized decoders. Recently, with
advancements in large language models (LLMs), end-to-end dialogue agents
without explicit socio-emotional strategy prediction steps have become
prevalent. However, despite their excellence in language generation, recent
studies show that LLMs' inherent preference bias towards certain
socio-emotional strategies hinders the delivery of high-quality emotional
support. To address this challenge, we propose decoupling strategy prediction
from language generation, and introduce a novel dialogue strategy predictor,
EmoDynamiX, which models the discourse dynamics between user emotions and
system strategies using a heterogeneous graph. Additionally, we make use of the
Emotion Recognition in Conversations (ERC) task and design a flexible
mixed-emotion module to capture fine-grained emotional states of the user.
Experimental results on two ESC datasets show EmoDynamiX outperforms previous
state-of-the-art methods with a significant margin.

æè¦ï¼è¨­è¨æç·æºè½å°è©±ç³»çµ±ä»¥æä¾å®æ°åå»ºè­°çµ¦ç¶æ­·çè¦çäººæ¯ä¸åå¼äººå¥åçç ç©¶é åã
ååçåªåéä¸­æ¼éç¼æ¨¡çµåå°è©±ç³»çµ±ï¼å°ç¤¾ææç·ç­ç¥é æ¸¬è¦çºè¼å©ä»»åï¼ä¸¦ä½¿ç¨èªè¨è§£ç¢¼å¨ç¢çç­ç¥æ¢ä»¶åçåæãæè¿ï¼é¨èå¤§åèªè¨æ¨¡å (LLM) çé²æ­¥ï¼æ²ææç¢ºç¤¾ææç·ç­ç¥é æ¸¬æ­¥é©çç«¯å°ç«¯å°è©±ä»£çå·²è®å¾æ®éãç¶èï¼åç®¡å®åå¨èªè¨çææ¹é¢è¡¨ç¾åºè²ï¼ä½æè¿çç ç©¶è¡¨æï¼LLM å°æäºç¤¾ææç·ç­ç¥çåºæåå¥½æé»ç¤æä¾é«åè³ªçæç·æ¯æãçºäºæå°éä¸ææ°ï¼æåå»ºè­°å°ç­ç¥é æ¸¬èèªè¨çæè§£è¦ï¼ä¸¦å¼å¥ä¸ç¨®æ°ç©çå°è©±ç­ç¥é æ¸¬å¨ EmoDynamiXï¼å®ä½¿ç¨ç°è³ªåå½¢å°ä½¿ç¨èæç·åç³»çµ±ç­ç¥ä¹éçè©±èªåæé²è¡å»ºæ¨¡ãæ­¤å¤ï¼æåå©ç¨å°è©±ä¸­çæç·è¾¨è­ (ERC) ä»»åä¸¦è¨­è¨äºä¸åéæ´»çæ··åæç·æ¨¡çµä¾ææä½¿ç¨èçç´°ç·»æç·çæãå¨å©å ESC è³æéä¸çå¯¦é©çµæé¡¯ç¤ºï¼EmoDynamiX ä»¥é¡¯èçå¹åº¦åªæ¼ååçææ°æ¹æ³ã

##### **Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?**
2408.08685v1 by Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi

Graph neural networks (GNNs) are vulnerable to adversarial perturbations,
especially for topology attacks, and many methods that improve the robustness
of GNNs have received considerable attention. Recently, we have witnessed the
significant success of large language models (LLMs), leading many to explore
the great potential of LLMs on GNNs. However, they mainly focus on improving
the performance of GNNs by utilizing LLMs to enhance the node features.
Therefore, we ask: Will the robustness of GNNs also be enhanced with the
powerful understanding and inference capabilities of LLMs? By presenting the
empirical results, we find that despite that LLMs can improve the robustness of
GNNs, there is still an average decrease of 23.1% in accuracy, implying that
the GNNs remain extremely vulnerable against topology attack. Therefore,
another question is how to extend the capabilities of LLMs on graph adversarial
robustness. In this paper, we propose an LLM-based robust graph structure
inference framework, LLM4RGNN, which distills the inference capabilities of
GPT-4 into a local LLM for identifying malicious edges and an LM-based edge
predictor for finding missing important edges, so as to recover a robust graph
structure. Extensive experiments demonstrate that LLM4RGNN consistently
improves the robustness across various GNNs. Even in some cases where the
perturbation ratio increases to 40%, the accuracy of GNNs is still better than
that on the clean graph.

æè¦ï¼åå½¢ç¥ç¶ç¶²è·¯ (GNN) å®¹æåå°å°ææ§æ¾åçå½±é¿ï¼
ç¹å¥æ¯ææ²æ»æï¼è¨±å¤æ¹å GNN é­¯æ£æ§çæ¹æ³é½ååéæ³¨ãæè¿ï¼æåè¦è­äºå¤§åèªè¨æ¨¡å (LLM) çé¡¯èæåï¼å°è´è¨±å¤äººæ¢ç´¢ LLM å¨ GNN ä¸çå·¨å¤§æ½åãç¶èï¼ä»åä¸»è¦å°æ³¨æ¼å©ç¨ LLM å¢å¼·ç¯é»ç¹å¾µä¾æ¹å GNN çæè½ã
å æ­¤ï¼æååï¼LLM å¼·å¤§ççè§£åæ¨çè½åæ¯å¦ä¹æå¢å¼· GNN çé­¯æ£æ§ï¼ééåç¾å¯¦è­çµæï¼æåç¼ç¾åç®¡ LLM å¯ä»¥æ¹å GNN çé­¯æ£æ§ï¼ä½æºç¢ºåº¦ä»å¹³åä¸é 23.1%ï¼éè¡¨ç¤º GNN ä»ç¶æ¥µå®¹æåå°ææ²æ»æãå æ­¤ï¼å¦ä¸ååé¡æ¯å¦ä½æ´å± LLM å¨åå½¢å°æé­¯æ£æ§ä¸çè½åãå¨æ¬æä¸­ï¼æåæåºä¸ååºæ¼ LLM çé­¯æ£åå½¢çµæ§æ¨çæ¡æ¶ LLM4RGNNï¼å®å° GPT-4 çæ¨çè½åæçæä¸åç¨æ¼è­å¥æ¡æéç·£çæ¬å° LLMï¼ä»¥åä¸åç¨æ¼å°æ¾éºå¤±éè¦éç·£çåºæ¼ LM çéç·£é æ¸¬å¨ï¼ä»¥ä¾¿æ¢å¾©ä¸åé­¯æ£çåå½¢çµæ§ãå»£æ³çå¯¦é©è­æï¼LLM4RGNN æçºæ¹ååç¨® GNN çé­¯æ£æ§ãå³ä½¿å¨æäºæ¾åçå¢å å° 40% çææ³ä¸ï¼GNN çæºç¢ºåº¦ä»ç¶åªæ¼ä¹¾æ·¨åå½¢ã

##### **RoarGraph: A Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search**
2408.08933v1 by Meng Chen, Kai Zhang, Zhenying He, Yinan Jing, X. Sean Wang

Approximate Nearest Neighbor Search (ANNS) is a fundamental and critical
component in many applications, including recommendation systems and large
language model-based applications. With the advancement of multimodal neural
models, which transform data from different modalities into a shared
high-dimensional space as feature vectors, cross-modal ANNS aims to use the
data vector from one modality (e.g., texts) as the query to retrieve the most
similar items from another (e.g., images or videos). However, there is an
inherent distribution gap between embeddings from different modalities, and
cross-modal queries become Out-of-Distribution (OOD) to the base data.
Consequently, state-of-the-art ANNS approaches suffer poor performance for OOD
workloads. In this paper, we quantitatively analyze the properties of the OOD
workloads to gain an understanding of their ANNS efficiency. Unlike
single-modal workloads, we reveal OOD queries spatially deviate from base data,
and the k-nearest neighbors of an OOD query are distant from each other in the
embedding space. The property breaks the assumptions of existing ANNS
approaches and mismatches their design for efficient search. With insights from
the OOD workloads, we propose pRojected bipartite Graph (RoarGraph), an
efficient ANNS graph index built under the guidance of query distribution.
Extensive experiments show that RoarGraph significantly outperforms
state-of-the-art approaches on modern cross-modal datasets, achieving up to
3.56x faster search speed at a 90% recall rate for OOD queries.

æè¦ï¼è¿ä¼¼æè¿é»æç´¢ (ANNS) æ¯è®¸å¤åºç¨ç¨åºä¸­çåºæ¬å³é®ç»ä»¶ï¼åæ¬æ¨èç³»ç»ååºäºå¤§è¯­è¨æ¨¡åçåºç¨ç¨åºãéçå¤æ¨¡æç¥ç»æ¨¡åçåå±ï¼å®å°æ¥èªä¸åæ¨¡æçæ°æ®è½¬æ¢ä¸ºå±äº«çé«ç»´ç©ºé´ä½ä¸ºç¹å¾åéï¼è·¨æ¨¡æ ANNS æ¨å¨ä½¿ç¨æ¥èªä¸ä¸ªæ¨¡æï¼ä¾å¦ææ¬ï¼çæ°æ®åéä½ä¸ºæ¥è¯¢ï¼ä»¥æ£ç´¢æ¥èªå¦ä¸ä¸ªæ¨¡æï¼ä¾å¦å¾åæè§é¢ï¼æç¸ä¼¼çé¡¹ç®ãä½æ¯ï¼ä¸åæ¨¡æçåµå¥ä¹é´å­å¨åºæçåå¸å·®è·ï¼å¹¶ä¸è·¨æ¨¡ææ¥è¯¢å¯¹äºåºç¡æ°æ®èè¨æä¸ºåå¸å¤ (OOD)ãå æ­¤ï¼æåè¿ç ANNS æ¹æ³å¯¹äº OOD å·¥ä½è´è½½çæ§è½å¾å·®ãå¨æ¬æä¸­ï¼æä»¬å®éåæäº OOD å·¥ä½è´è½½çå±æ§ï¼ä»¥äºè§£å¶ ANNS æçãä¸åæ¨¡æå·¥ä½è´è½½ä¸åï¼æä»¬æ­ç¤ºäº OOD æ¥è¯¢å¨ç©ºé´ä¸åç¦»åºç¡æ°æ®ï¼å¹¶ä¸ OOD æ¥è¯¢ç k ä¸ªæè¿é»å¨åµå¥ç©ºé´ä¸­å½¼æ­¤ç¸è·çè¿ãè¯¥å±æ§æç ´äºç°æ ANNS æ¹æ³çåè®¾ï¼å¹¶ä¸ä¸å¹éå®ä»¬ä¸ºé«ææç´¢èè®¾è®¡çåè®¾ãéè¿å¯¹ OOD å·¥ä½è´è½½çè§è§£ï¼æä»¬æåºäº pRojected äºåå¾ (RoarGraph)ï¼è¿æ¯ä¸ç§å¨æ¥è¯¢åå¸æå¯¼ä¸æå»ºçé«æ ANNS å¾å½¢ç´¢å¼ãå¤§éçå®éªè¡¨æï¼RoarGraph å¨ç°ä»£è·¨æ¨¡ææ°æ®éä¸ææ¾ä¼äºæåè¿çæ¹æ³ï¼å¨ OOD æ¥è¯¢ç 90% å¬åçä¸å®ç°äºé«è¾¾ 3.56 åçæ´å¿«æç´¢éåº¦ã

##### **Handling abort commands for household kitchen robots**
2408.14480v1 by Darius Has, Adrian Groza, Mihai Pomarlan

We propose a solution for handling abort commands given to robots. The
solution is exemplified with a running scenario with household kitchen robots.
The robot uses planning to find sequences of actions that must be performed in
order to gracefully cancel a previously received command. The Planning Domain
Definition Language (PDDL) is used to write a domain to model kitchen
activities and behaviours, and this domain is enriched with knowledge from
online ontologies and knowledge graphs, like DBPedia. We discuss the results
obtained in different scenarios.

æè¦ï¼æåæåºäºä¸åèçç¼éçµ¦æ©å¨äººçä¸­æ­¢å½ä»¤çè§£æ±ºæ¹æ¡ã
éåè§£æ±ºæ¹æ¡ä»¥å®¶ç¨å»æ¿æ©å¨äººçå·è¡æå¢çºä¾ã
æ©å¨äººä½¿ç¨è¦åä¾å°æ¾å¿é å·è¡çåä½åºåï¼ä»¥ä¾¿åªéå°åæ¶ååæ¥æ¶çå½ä»¤ãè¦åé åå®ç¾©èªè¨ (PDDL) ç¨æ¼æ°å¯«ä¸åç¶²åä¾å»ºæ¨¡å»æ¿æ´»ååè¡çºï¼èéåç¶²ååééç·ä¸æ¬ä½åç¥è­åè¡¨ï¼ä¾å¦ DBPediaï¼çç¥è­ä¾è±å¯ãæåè¨è«å¨ä¸åæå¢ä¸­ç²å¾ççµæã

##### **CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking**
2408.08535v1 by Rong-Ching Chang, Jiawei Zhang

Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems, their effectiveness is often hindered by a lack of
integration with entity relationships and community structures, limiting their
ability to provide contextually rich and accurate information retrieval for
fact-checking. We introduce CommunityKG-RAG (Community Knowledge
Graph-Retrieval Augmented Generation), a novel zero-shot framework that
integrates community structures within Knowledge Graphs (KGs) with RAG systems
to enhance the fact-checking process. Capable of adapting to new domains and
queries without additional training, CommunityKG-RAG utilizes the multi-hop
nature of community structures within KGs to significantly improve the accuracy
and relevance of information retrieval. Our experimental results demonstrate
that CommunityKG-RAG outperforms traditional methods, representing a
significant advancement in fact-checking by offering a robust, scalable, and
efficient solution.

æè¦ï¼åç®¡å¤§åèªè¨æ¨¡å (LLM) åæª¢ç´¢å¢å¼·çæ (RAG) ç³»çµ±æé²æ­¥ï¼ä½å®åçæææ§ç¶å¸¸åå°ç¼ºä¹èå¯¦é«éä¿åç¤¾ç¾¤çµæ§æ´åçé»ç¤ï¼éå¶äºå®åæä¾èçµ¡è±å¯ä¸æºç¢ºçè³è¨æª¢ç´¢ä»¥é²è¡äºå¯¦æ¥æ ¸çè½åãæåä»ç´¹ CommunityKG-RAGï¼ç¤¾ç¾¤ç¥è­åè­æª¢ç´¢å¢å¼·çæï¼ï¼éæ¯ä¸åæ°ç©çé¶æ¬¡å­¸ç¿æ¶æ§ï¼å®å°ç¥è­åè­ (KG) å§çç¤¾ç¾¤çµæ§è RAG ç³»çµ±æ´åï¼ä»¥å¢å¼·äºå¯¦æ¥æ ¸æµç¨ãCommunityKG-RAG ç¡éé¡å¤è¨ç·´å°±è½é©ææ°çé ååæ¥è©¢ï¼å®å©ç¨ KG å§ç¤¾ç¾¤çµæ§çå¤è·³ç¹æ§ï¼å¤§å¹æåè³è¨æª¢ç´¢çæºç¢ºæ§åç¸éæ§ãæåçå¯¦é©çµæè­æ CommunityKG-RAG åªæ¼å³çµ±æ¹æ³ï¼ä»£è¡¨èäºå¯¦æ¥æ ¸çéå¤§é²æ­¥ï¼æä¾äºä¸åå¼·å¥ãå¯æ´åä¸ææççè§£æ±ºæ¹æ¡ã

##### **VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning and Abstract Syntax Tree (AST)-based Waveform Tracing Tool**
2408.08927v1 by Chia-Tung Ho, Haoxing Ren, Brucek Khailany

Due to the growing complexity of modern Integrated Circuits (ICs), automating
hardware design can prevent a significant amount of human error from the
engineering process and result in less errors. Verilog is a popular hardware
description language for designing and modeling digital systems; thus, Verilog
generation is one of the emerging areas of research to facilitate the design
process. In this work, we propose VerilogCoder, a system of multiple Artificial
Intelligence (AI) agents for Verilog code generation, to autonomously write
Verilog code and fix syntax and functional errors using collaborative Verilog
tools (i.e., syntax checker, simulator, and waveform tracer). Firstly, we
propose a task planner that utilizes a novel Task and Circuit Relation Graph
retrieval method to construct a holistic plan based on module descriptions. To
debug and fix functional errors, we develop a novel and efficient abstract
syntax tree (AST)-based waveform tracing tool, which is integrated within the
autonomous Verilog completion flow. The proposed methodology successfully
generates 94.2% syntactically and functionally correct Verilog code, surpassing
the state-of-the-art methods by 33.9% on the VerilogEval-Human v2 benchmark.

æè¦ï¼ç±æ¼ç¾ä»£æ´åé»è·¯ (IC) çè¤éæ§æ¥çå¢å ï¼èªååç¡¬é«è¨­è¨å¯ä»¥é²æ­¢å·¥ç¨éç¨ä¸­åºç¾å¤§éçäººçºé¯èª¤ï¼ä¸¦æ¸å°é¯èª¤ãVerilog æ¯ä¸ç¨®æµè¡çç¡¬é«æè¿°èªè¨ï¼ç¨æ¼è¨­è¨åå»ºæ¨¡æ¸ä½ç³»çµ±ï¼å æ­¤ï¼Verilog ç¢çæ¯æ°èçç ç©¶é åä¹ä¸ï¼æ¨å¨ä¿é²è¨­è¨éç¨ãå¨éé å·¥ä½ä¸­ï¼æåæåº VerilogCoderï¼ä¸åç±å¤åäººå·¥æºæ§ (AI) ä»£ççµæçç³»çµ±ï¼ç¨æ¼ Verilog ç¨å¼ç¢¼ç¢çï¼ä»¥èªä¸»æ°å¯« Verilog ç¨å¼ç¢¼ä¸¦ä½¿ç¨åä½å¼ Verilog å·¥å·ï¼ä¾å¦ï¼èªæ³æª¢æ¥å¨ãæ¨¡æ¬å¨åæ³¢å½¢è¿½è¹¤å¨ï¼ä¿®å¾©èªæ³ååè½é¯èª¤ãé¦åï¼æåæåºä¸åä»»åè¦åå¨ï¼å®å©ç¨æ°ç©çä»»ååé»è·¯éä¿åæ·åæ¹æ³ï¼æ ¹ææ¨¡çµæè¿°å»ºæ§ä¸åæ´é«è¨ç«ãçºäºé¤é¯åä¿®å¾©åè½é¯èª¤ï¼æåéç¼äºä¸åæ°ç©ä¸é«æçåºæ¼æ½è±¡èªæ³æ¨¹ (AST) çæ³¢å½¢è¿½è¹¤å·¥å·ï¼å®æ´åå¨èªä¸» Verilog å®ææµç¨ä¸­ãææåºçæ¹æ³æåç¢çäº 94.2% èªæ³ååè½æ­£ç¢ºç Verilog ç¨å¼ç¢¼ï¼å¨ VerilogEval-Human v2 åºæºä¸æ¯æåé²çæ¹æ³é«åº 33.9%ã

##### **Graph Retrieval-Augmented Generation: A Survey**
2408.08921v2 by Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang

Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable
success in addressing the challenges of Large Language Models (LLMs) without
necessitating retraining. By referencing an external knowledge base, RAG
refines LLM outputs, effectively mitigating issues such as ``hallucination'',
lack of domain-specific knowledge, and outdated information. However, the
complex structure of relationships among different entities in databases
presents challenges for RAG systems. In response, GraphRAG leverages structural
information across entities to enable more precise and comprehensive retrieval,
capturing relational knowledge and facilitating more accurate, context-aware
responses. Given the novelty and potential of GraphRAG, a systematic review of
current technologies is imperative. This paper provides the first comprehensive
overview of GraphRAG methodologies. We formalize the GraphRAG workflow,
encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced
Generation. We then outline the core technologies and training methods at each
stage. Additionally, we examine downstream tasks, application domains,
evaluation methodologies, and industrial use cases of GraphRAG. Finally, we
explore future research directions to inspire further inquiries and advance
progress in the field. In order to track recent progress in this field, we set
up a repository at \url{https://github.com/pengboci/GraphRAG-Survey}.

æè¦ï¼æè¿ï¼æ£ç´¢å¢å¼ºçæ (RAG) å¨è§£å³å¤§åè¯­è¨æ¨¡å (LLM) çæææ¹é¢åå¾äºæ¾çæåï¼èæ ééæ°è®­ç»ãéè¿åèå¤é¨ç¥è¯åºï¼RAG æ¹è¿äº LLM çè¾åºï¼ææå°åè½»äºè¯¸å¦ãå¹»è§ããç¼ºä¹ç¹å®é¢åç¥è¯åä¿¡æ¯è¿æ¶ç­é®é¢ãç¶èï¼æ°æ®åºä¸­ä¸åå®ä½ä¹é´å³ç³»çå¤æç»æç» RAG ç³»ç»å¸¦æ¥äºææãä½ä¸ºååºï¼GraphRAG å©ç¨å®ä½ä¹é´çç»æä¿¡æ¯æ¥å®ç°æ´ç²¾ç¡®åå¨é¢çæ£ç´¢ï¼æè·å³ç³»ç¥è¯å¹¶ä¿è¿æ´åç¡®ãæ´å·ä¸ä¸ææç¥çååºãé´äº GraphRAG çæ°é¢æ§åæ½åï¼å¯¹å½åææ¯è¿è¡ç³»ç»å®¡æ¥å¿å¨å¿è¡ãæ¬ææä¾äº GraphRAG æ¹æ³çç¬¬ä¸ä¸ªå¨é¢æ¦è¿°ãæä»¬å½¢å¼åäº GraphRAG å·¥ä½æµï¼åæ¬åºäºå¾çç´¢å¼ãå¾å¼å¯¼çæ£ç´¢åå¾å¢å¼ºççæãç¶åï¼æä»¬å¨æ¯ä¸ªé¶æ®µæ¦è¿°äºæ ¸å¿ææ¯åè®­ç»æ¹æ³ãæ­¤å¤ï¼æä»¬è¿ç ç©¶äº GraphRAG çä¸æ¸¸ä»»å¡ãåºç¨é¢åãè¯ä¼°æ¹æ³åå·¥ä¸ç¨ä¾ãæåï¼æä»¬æ¢è®¨äºæªæ¥çç ç©¶æ¹åï¼ä»¥æ¿åè¿ä¸æ­¥çæ¢ç©¶å¹¶æ¨è¿è¯¥é¢åçè¿å±ãä¸ºäºè¿½è¸ªè¯¥é¢åçææ°è¿å±ï¼æä»¬å¨ \url{https://github.com/pengboci/GraphRAG-Survey} ä¸å»ºç«äºä¸ä¸ªå­å¨åºã

##### **Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability**
2408.07852v1 by Jiri Hron, Laura Culp, Gamaleldin Elsayed, Rosanne Liu, Ben Adlam, Maxwell Bileschi, Bernd Bohnet, JD Co-Reyes, Noah Fiedel, C. Daniel Freeman, Izzeddin Gur, Kathleen Kenealy, Jaehoon Lee, Peter J. Liu, Gaurav Mishra, Igor Mordatch, Azade Nova, Roman Novak, Aaron Parisi, Jeffrey Pennington, Alex Rizkowsky, Isabelle Simpson, Hanie Sedghi, Jascha Sohl-dickstein, Kevin Swersky, Sharad Vikram, Tris Warkentin, Lechao Xiao, Kelvin Xu, Jasper Snoek, Simon Kornblith

While many capabilities of language models (LMs) improve with increased
training budget, the influence of scale on hallucinations is not yet fully
understood. Hallucinations come in many forms, and there is no universally
accepted definition. We thus focus on studying only those hallucinations where
a correct answer appears verbatim in the training set. To fully control the
training data content, we construct a knowledge graph (KG)-based dataset, and
use it to train a set of increasingly large LMs. We find that for a fixed
dataset, larger and longer-trained LMs hallucinate less. However, hallucinating
on $\leq5$% of the training data requires an order of magnitude larger model,
and thus an order of magnitude more compute, than Hoffmann et al. (2022)
reported was optimal. Given this costliness, we study how hallucination
detectors depend on scale. While we see detector size improves performance on
fixed LM's outputs, we find an inverse relationship between the scale of the LM
and the detectability of its hallucinations.

æè¦ï¼éç¶èªè¨æ¨¡å (LM) çè¨±å¤è½åæé¨èè¨ç·´é ç®çå¢å èæææåï¼ä½è¦æ¨¡å°å¹»è¦ºçå½±é¿å°æªå®å¨äºè§£ãå¹»è¦ºæè¨±å¤å½¢å¼ï¼ä¸æ²ææ®éæ¥åçå®ç¾©ãå æ­¤ï¼æååªå°æ³¨æ¼ç ç©¶è¨ç·´éä¸­åºç¾æ­£ç¢ºç­æ¡çå¹»è¦ºãçºäºå®å¨æ§å¶è¨ç·´è³æå§å®¹ï¼æåå»ºæ§äºä¸ååºæ¼ç¥è­åè­ (KG) çè³æéï¼ä¸¦ä½¿ç¨å®ä¾è¨ç·´ä¸çµè¶ä¾è¶å¤§ç LMãæåç¼ç¾å°æ¼åºå®çè³æéï¼è¦æ¨¡è¼å¤§ä¸è¨ç·´æéè¼é·ç LM ç¢ççå¹»è¦ºè¼å°ãç¶èï¼å¨ $\leq5$% çè¨ç·´è³æä¸ç¢çå¹»è¦ºéè¦è¦æ¨¡å¤§ä¸åæ¸éç´çæ¨¡åï¼å æ­¤æ¯ Hoffmann ç­äºº (2022) æå ±åçæä½³è¦æ¨¡å¤ä¸åæ¸éç´çéç®ææ¬ãèéå°éç¨®ææ¬ï¼æåç ç©¶å¹»è¦ºåµæ¸¬å¨å¦ä½åæ±ºæ¼è¦æ¨¡ãéç¶æåçå°åµæ¸¬å¨è¦æ¨¡ææåå°åºå® LM è¼¸åºçæè½ï¼ä½æåç¼ç¾ LM çè¦æ¨¡èå¶å¹»è¦ºçå¯åµæ¸¬æ§ä¹éå­å¨åæ¯éä¿ã

##### **ONSEP: A Novel Online Neural-Symbolic Framework for Event Prediction Based on Large Language Model**
2408.07840v1 by Xuanqing Yu, Wangtao Sun, Jingwei Li, Kang Liu, Chengbao Liu, Jie Tan

In the realm of event prediction, temporal knowledge graph forecasting (TKGF)
stands as a pivotal technique. Previous approaches face the challenges of not
utilizing experience during testing and relying on a single short-term history,
which limits adaptation to evolving data. In this paper, we introduce the
Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by
integrating dynamic causal rule mining (DCRM) and dual history augmented
generation (DHAG). DCRM dynamically constructs causal rules from real-time
data, allowing for swift adaptation to new causal relationships. In parallel,
DHAG merges short-term and long-term historical contexts, leveraging a
bi-branch approach to enrich event prediction. Our framework demonstrates
notable performance enhancements across diverse datasets, with significant
Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language
models (LLMs) for event prediction without necessitating extensive retraining.
The ONSEP framework not only advances the field of TKGF but also underscores
the potential of neural-symbolic approaches in adapting to dynamic data
environments.

æè¦ï¼å¨äºä»¶é æ¸¬é åä¸­ï¼æåºç¥è­åè­é æ¸¬ (TKGF) æ¯ä¸åééµæè¡ãååçåæ³é¢è¨å¨æ¸¬è©¦æéä¸å©ç¨ç¶é©ä»¥åä¾è³´å®ä¸ç­ææ­·å²çææ°ï¼ééå¶äºå°æ¼åè³æçé©ææ§ãå¨æ¬æä¸­ï¼æåä»ç´¹äºç·ä¸ç¥ç¶ç¬¦èäºä»¶é æ¸¬ (ONSEP) æ¶æ§ï¼å®ééæ´ååæå æè¦åææ (DCRM) åééæ­·å²æ´åçæ (DHAG) ä¾åµæ°ãDCRM å¾å³æè³æä¸­åæå»ºæ§å æè¦åï¼åè¨±å¿«éé©ææ°çå æéä¿ãåæï¼DHAG åä½µç­æåé·ææ­·å²èçµ¡ï¼å©ç¨éåæ¯æ¹æ³ä¾è±å¯äºä»¶é æ¸¬ãæåçæ¶æ§å¨åç¨®è³æéä¸å±ç¤ºåºé¡¯èçæè½æåï¼Hit@k (k=1,3,10) æé¡¯èçæ¹åï¼å±ç¤ºäºå®å¨ç¡éå»£æ³éæ°è¨ç·´çææ³ä¸æ´åå¤§åèªè¨æ¨¡å (LLM) ä»¥é²è¡äºä»¶é æ¸¬çè½åãONSEP æ¶æ§ä¸åæ¨åäº TKGF é åï¼ä¹å¼·èª¿äºç¥ç¶ç¬¦èæ¹æ³å¨é©æåæè³æç°å¢ä¸­çæ½åã

##### **WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs**
2408.07611v2 by Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu

Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce "phantom" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a "Retrieval-Augmented
Generation (RAG)" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) æ¥µå¤§å°ä¿è¿äºèªé©ææºè½ä»£ççéç¼ï¼ä¸¦è¢«å®ä½çºå¯¦ç¾äººå·¥éç¨æºæ§ (AGI) çéè¦éå¾ãç¶èï¼LLM å®¹æç¢çäºå¯¦ä¸ä¸æ­£ç¢ºçè³è¨ï¼èä¸å¸¸å¸¸ç¢çãå¹»å½±ãå§å®¹ï¼éæç ´å£å¶å¯é æ§ï¼å°å¶å¨ç¾å¯¦ä¸çå ´æ¯ä¸­çé¨ç½²æ§æå´å³»ææ°ãçµåå¤é¨è³æåº«åè³è¨æª¢ç´¢æ©å¶ä¾å¢å¼· LLM æ¯ä¸ç¨®ææçæ¹æ³ãçºäºæå°ä¸è¿°ææ°ï¼æåæåºäºä¸ç¨®ç¨±çº WeKnow-RAG çæ°æ¹æ³ï¼å®å°ç¶²è·¯æå°åç¥è­åè­æ´åå°ãæª¢ç´¢å¢å¼·çæ (RAG)ãç³»çµ±ä¸­ãé¦åï¼ééçµåç¥è­åè­ççµæ§åè¡¨ç¤ºåç¨ å¯åéæª¢ç´¢çéæ´»æ§ï¼ä¾æå LLM åæçæºç¢ºæ§åå¯é æ§ãWeKnow-RAG æ¥èå©ç¨ç¹å®é åçç¥è­åè­ä¾æ»¿è¶³åç¨®æ¥è©¢åé åï¼å¾èééä½¿ç¨ç¨çåç¨ å¯æª¢ç´¢æ¹æ³çå¤éæ®µç¶²é æª¢ç´¢æè¡ï¼ä¾æåäºå¯¦è³è¨åè¤éæ¨çä»»åçæè½ãæåçåæ³ææå°å¹³è¡¡äºè³è¨æª¢ç´¢çæçåæºç¢ºæ§ï¼é²èæ¹åæ´é«æª¢ç´¢æµç¨ãæå¾ï¼æåéæ´åäºä¸å LLM èªæè©ä¼°æ©å¶ï¼ä»¥è©ä¼°å¶æç¢çç­æ¡çå¯ä¿¡åº¦ãæåçåæ³å¨å»£æ³çé¢ç·å¯¦é©åç·ä¸æäº¤ä¸­è­æäºå¶ååºçæææ§ã

##### **Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals**
2408.07453v1 by Tobias A. Opsahl

Despite recent success in natural language processing (NLP), fact
verification still remains a difficult task. Due to misinformation spreading
increasingly fast, attention has been directed towards automatically verifying
the correctness of claims. In the domain of NLP, this is usually done by
training supervised machine learning models to verify claims by utilizing
evidence from trustworthy corpora. We present efficient methods for verifying
claims on a dataset where the evidence is in the form of structured knowledge
graphs. We use the FactKG dataset, which is constructed from the DBpedia
knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval
process, from fine-tuned language models to simple logical retrievals, we are
able to construct models that both require less computational resources and
achieve better test-set accuracy.

æè¦ï¼åç®¡å¨èªç¶èªè¨èç (NLP) ä¸­ç²å¾è¿ææåï¼äºå¯¦é©è­ä»ç¶æ¯ä¸é è±é£çä»»åãç±æ¼é¯èª¤è³è¨å³æ­å¾è¶ä¾è¶å¿«ï¼æ³¨æåå·²è½åèªåé©è­è²æçæ­£ç¢ºæ§ãå¨ NLP é åä¸­ï¼ééå¸¸ééè¨ç·´ç£ç£å¼æ©å¨å­¸ç¿æ¨¡åä¾å®æï¼éäºæ¨¡åå©ç¨ä¾èªå¯ä¿¡è³´èªæåº«çè­æä¾é©è­è²æãæåæåºææçæ¹æ³ä¾é©è­è³æéä¸­çè²æï¼å¶ä¸­è­ææ¯ä»¥çµæ§åç¥è­åè¡¨çå½¢å¼åç¾ãæåä½¿ç¨ FactKG è³æéï¼å®æ¯ç±å¾ç¶­åºç¾ç§ä¸­èåç DBpedia ç¥è­åè¡¨æå»ºæ§ãééç°¡åè­ææ·åæµç¨ï¼å¾å¾®èª¿èªè¨æ¨¡åå°ç°¡å®çéè¼¯æ·åï¼æåè½å¤ å»ºæ§æ¢éè¦è¼å°è¨ç®è³æºï¼åè½éå°è¼ä½³æ¸¬è©¦éæºç¢ºåº¦çæ¨¡åã

##### **LLMs can Schedule**
2408.06993v1 by Henrik Abgaryan, Ararat Harutyunyan, Tristan Cazenave

The job shop scheduling problem (JSSP) remains a significant hurdle in
optimizing production processes. This challenge involves efficiently allocating
jobs to a limited number of machines while minimizing factors like total
processing time or job delays. While recent advancements in artificial
intelligence have yielded promising solutions, such as reinforcement learning
and graph neural networks, this paper explores the potential of Large Language
Models (LLMs) for JSSP. We introduce the very first supervised 120k dataset
specifically designed to train LLMs for JSSP. Surprisingly, our findings
demonstrate that LLM-based scheduling can achieve performance comparable to
other neural approaches. Furthermore, we propose a sampling method that
enhances the effectiveness of LLMs in tackling JSSP.

æè¦ï¼ä½æ¥­è»éæç¨åé¡ (JSSP) ä»ç¶æ¯æä½³åçç¢æµç¨ä¸­çä¸å¤§éç¤ãéé ææ°æ¶åå°ä½æ¥­ææåéå°æ¸éæéçæ©å¨ï¼åæå°ç¸½èçæéæä½æ¥­å»¶é²ç­å ç´ éè³æä½ãåç®¡äººå·¥æºæ§çææ°é²å±å·²ç¢çæå¸æçè§£æ±ºæ¹æ¡ï¼ä¾å¦å¼·åå­¸ç¿ååå½¢ç¥ç¶ç¶²è·¯ï¼ä½æ¬ææ¢è¨äºå¤§åèªè¨æ¨¡å (LLM) å¨ JSSP ä¸­çæ½åãæåå¼å¥äºç¬¬ä¸åç£ç£å¼ 120k è³æéï¼å°éç¨æ¼è¨ç·´ JSSP ç LLMãä»¤äººé©è¨çæ¯ï¼æåçç ç©¶çµæè¡¨æï¼åºæ¼ LLM çæç¨å¯ä»¥éå°èå¶ä»ç¥ç¶æ¹æ³ç¸ç¶çæè½ãæ­¤å¤ï¼æåæåºäºä¸ç¨®æ½æ¨£æ¹æ³ï¼å¯å¢å¼· LLM å¨èç JSSP ä¸­çæææ§ã

##### **Causal Agent based on Large Language Model**
2408.06849v1 by Kairong Han, Kun Kuang, Ziyu Zhao, Junjian Ye, Fei Wu

Large language models (LLMs) have achieved significant success across various
domains. However, the inherent complexity of causal problems and causal theory
poses challenges in accurately describing them in natural language, making it
difficult for LLMs to comprehend and use them effectively. Causal methods are
not easily conveyed through natural language, which hinders LLMs' ability to
apply them accurately. Additionally, causal datasets are typically tabular,
while LLMs excel in handling natural language data, creating a structural
mismatch that impedes effective reasoning with tabular data. This lack of
causal reasoning capability limits the development of LLMs. To address these
challenges, we have equipped the LLM with causal tools within an agent
framework, named the Causal Agent, enabling it to tackle causal problems. The
causal agent comprises tools, memory, and reasoning modules. In the tools
module, the causal agent applies causal methods to align tabular data with
natural language. In the reasoning module, the causal agent employs the ReAct
framework to perform reasoning through multiple iterations with the tools. In
the memory module, the causal agent maintains a dictionary instance where the
keys are unique names and the values are causal graphs. To verify the causal
ability of the causal agent, we established a benchmark consisting of four
levels of causal problems: variable level, edge level, causal graph level, and
causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for
these four levels of issues and tested the causal agent on the datasets. Our
methodology demonstrates remarkable efficacy on the four-level causal problems,
with accuracy rates all above 80%. For further insights and implementation
details, our code is accessible via the GitHub repository
https://github.com/Kairong-Han/Causal_Agent.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å¨ååé ååå¾éå¤§æåãç¶èï¼å æåé¡åå æçè«çå§å¨è¤éæ§ï¼å¨èªç¶èªè¨ä¸­æºç¢ºæè¿°å®åææ§æææ°ï¼éä½¿å¾ LLM é£ä»¥çè§£ä¸¦ææä½¿ç¨å®åãå ææ¹æ³ä¸æééèªç¶èªè¨å³éï¼éé»ç¤äº LLM æºç¢ºæç¨å®åçè½åãæ­¤å¤ï¼å æè³æééå¸¸æ¯è¡¨æ ¼åçï¼è LLM æé·èçèªç¶èªè¨è³æï¼éé æäºçµæ§ä¸çä¸å¹éï¼é»ç¤äºå°è¡¨æ ¼è³æé²è¡ææçæ¨çãéç¨®ç¼ºä¹å ææ¨çè½åéå¶äº LLM çç¼å±ãçºäºæå°éäºææ°ï¼æåå¨ä¸åä»£çæ¡æ¶ä¸­çº LLM éåäºå æå·¥å·ï¼ç¨±çºå æä»£çï¼ä½¿å®è½å¤ è§£æ±ºå æåé¡ãå æä»£çåå«å·¥å·ãè¨æ¶é«åæ¨çæ¨¡çµãå¨å·¥å·æ¨¡çµä¸­ï¼å æä»£çæç¨å ææ¹æ³å°è¡¨æ ¼è³æèèªç¶èªè¨å°é½ãå¨æ¨çæ¨¡çµä¸­ï¼å æä»£çæ¡ç¨ ReAct æ¡æ¶ï¼ééèå·¥å·é²è¡å¤æ¬¡åè¦éç®ä¾å·è¡æ¨çãå¨è¨æ¶é«æ¨¡çµä¸­ï¼å æä»£çç¶­è­·ä¸åå­å¸å¯¦ä¾ï¼å¶ä¸­éµæ¯å¯ä¸åç¨±ï¼èå¼æ¯å æåãçºäºé©è­å æä»£ççå æè½åï¼æåå»ºç«äºä¸ååºæºï¼å¶ä¸­åå«ååå±¤ç´çå æåé¡ï¼è®æ¸å±¤ç´ãéå±¤ç´ãå æåå±¤ç´åå æææå±¤ç´ãæåä½¿ç¨ ChatGPT-3.5 çºéååå±¤ç´çåé¡ç¢çäº 1.3K çæ¸¬è©¦è³æéï¼ä¸¦å¨è³æéä¸æ¸¬è©¦äºå æä»£çãæåçéå¥æ¹æ³å¨ååå±¤ç´çå æåé¡ä¸å±ç¾äºé¡¯èçåæï¼æºç¢ºçé½é«æ¼ 80%ãæéé²ä¸æ­¥çè¦è§£åå¯¦ä½ç´°ç¯ï¼æåçç¨å¼ç¢¼å¯éé GitHub å²å­åº« https://github.com/Kairong-Han/Causal_Agent åå¾ã

##### **Unlock the Power of Frozen LLMs in Knowledge Graph Completion**
2408.06787v1 by Bo Xue, Yi Xu, Yunchong Song, Yiming Pang, Yuyang Ren, Jiaxin Ding, Luoyi Fu, Xinbing Wang

Classical knowledge graph completion (KGC) methods rely solely on structural
information, struggling with the inherent sparsity of knowledge graphs (KGs).
Large Language Models (LLMs) learn extensive knowledge from large corpora with
powerful context modeling, which is ideal for mitigating the limitations of
previous methods. Directly fine-tuning LLMs offers great capability but comes
at the cost of huge time and memory consumption, while utilizing frozen LLMs
yields suboptimal results. In this work, we aim to leverage LLMs for KGC
effectively and efficiently. We capture the context-aware hidden states of
knowledge triples by employing prompts to stimulate the intermediate layers of
LLMs. We then train a data-efficient classifier on these hidden states to
harness the inherent capabilities of frozen LLMs in KGC. We also generate
entity descriptions with subgraph sampling on KGs, reducing the ambiguity of
triplets and enriching the knowledge representation. Extensive experiments on
standard benchmarks showcase the efficiency and effectiveness of our approach.
We outperform classical KGC methods on most datasets and match the performance
of fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU
memory efficiency by \textbf{$188\times$} and speed up training+inference by
\textbf{$13.48\times$}.

æè¦ï¼å³çµ±ç¥è­åè­å®æ (KGC) æ¹æ³åä¾è³´çµæ§åè³è¨ï¼é£ä»¥æå°ç¥è­åè­ (KG) å§å¨çç¨çæ§ãå¤§åèªè¨æ¨¡å (LLM) å¾å¤§åèªæåº«ä¸­å­¸ç¿å»£æ³çç¥è­ï¼ä¸¦å·åå¼·å¤§çæå¢å»ºæ¨¡è½åï¼éå°æ¼ç·©è§£ååæ¹æ³çéå¶éå¸¸çæ³ãç´æ¥å¾®èª¿ LLM å¯æä¾å¼·å¤§çè½åï¼ä½ä»£å¹æ¯èè²»å¤§éæéåè¨æ¶é«ï¼èå©ç¨åçµç LLM åæç¢çæ¬¡ä½³çµæãå¨éé å·¥ä½ä¸­ï¼æåæ¨å¨ææä¸é«æå°å©ç¨ LLM ä¾é²è¡ KGCãæåééä½¿ç¨æç¤ºä¾åºæ¿ LLM çä¸­éå±¤ï¼ææå°ç¥è­ä¸åçµçæå¢æç¥é±èçæãç¶å¾ï¼æåå¨éäºé±èçæä¸è¨ç·´ä¸åè³æææççåé¡å¨ï¼ä»¥å©ç¨åçµ LLM å¨ KGC ä¸­çå§å¨è½åãæåéééå¨ KG ä¸é²è¡å­åæ½æ¨£ä¾ç¢çå¯¦é«æè¿°ï¼æ¸å°ä¸åçµçæ¨¡ç³æ§ä¸¦è±å¯ç¥è­è¡¨ç¤ºãæ¨æºåºæºä¸çå»£æ³å¯¦é©å±ç¤ºäºæåæ¹æ³çæçåæææ§ãæåå¨å¤æ¸è³æéä¸åªæ¼å³çµ±ç KGC æ¹æ³ï¼ä¸¦èå¾®èª¿å¾ç LLM éå°ç¸åçæè½ãæ­¤å¤ï¼èå¾®èª¿å¾ç LLM ç¸æ¯ï¼æåå° GPU è¨æ¶é«æçæåäº **$188\times$**ï¼ä¸¦å°è¨ç·´ + æ¨è«éåº¦æåäº **$13.48\times$**ã

##### **Computation-friendly Graph Neural Network Design by Accumulating Knowledge on Large Language Models**
2408.06717v1 by Jialiang Wang, Shimin Di, Hanmo Liu, Zhili Wang, Jiachuan Wang, Lei Chen, Xiaofang Zhou

Graph Neural Networks (GNNs), like other neural networks, have shown
remarkable success but are hampered by the complexity of their architecture
designs, which heavily depend on specific data and tasks. Traditionally,
designing proper architectures involves trial and error, which requires
intensive manual effort to optimize various components. To reduce human
workload, researchers try to develop automated algorithms to design GNNs.
However, both experts and automated algorithms suffer from two major issues in
designing GNNs: 1) the substantial computational resources expended in
repeatedly trying candidate GNN architectures until a feasible design is
achieved, and 2) the intricate and prolonged processes required for humans or
algorithms to accumulate knowledge of the interrelationship between graphs,
GNNs, and performance.
  To further enhance the automation of GNN architecture design, we propose a
computation-friendly way to empower Large Language Models (LLMs) with
specialized knowledge in designing GNNs, thereby drastically shortening the
computational overhead and development cycle of designing GNN architectures.
Our framework begins by establishing a knowledge retrieval pipeline that
comprehends the intercorrelations between graphs, GNNs, and performance. This
pipeline converts past model design experiences into structured knowledge for
LLM reference, allowing it to quickly suggest initial model proposals.
Subsequently, we introduce a knowledge-driven search strategy that emulates the
exploration-exploitation process of human experts, enabling quick refinement of
initial proposals within a promising scope. Extensive experiments demonstrate
that our framework can efficiently deliver promising (e.g., Top-5.77%) initial
model proposals for unseen datasets within seconds and without any prior
training and achieve outstanding search performance in a few iterations.

æè¦ï¼åå½¢ç¥ç¶ç¶²è·¯ (GNN) èå¶ä»ç¥ç¶ç¶²è·¯ä¸æ¨£ï¼å·²å±ç¾åºé¡¯èçæåï¼ä½å¶æ¶æ§è¨­è¨çè¤éæ§å»é»ç¤äºé²ä¸æ­¥çç¼å±ï¼èéç¨®è¤éæ§å¨å¾å¤§ç¨åº¦ä¸åæ±ºæ¼å·é«çè³æåä»»åãå³çµ±ä¸ï¼è¨­è¨é©ç¶çæ¶æ§éè¦åè¦åè©¦ï¼ééè¦å¤§éçäººå·¥å·¥ä½æè½æä½³ååç¨®åä»¶ãçºäºæ¸å°äººåçè² æï¼ç ç©¶äººå¡åè©¦éç¼èªååæ¼ç®æ³ä¾è¨­è¨ GNNãç¶èï¼å°å®¶åèªååæ¼ç®æ³å¨è¨­è¨ GNN æé½æéå°å©åä¸»è¦åé¡ï¼1) å¨åè¦åè©¦åé¸ GNN æ¶æ§ä»¥éæå¯è¡çè¨­è¨ä¹åï¼æèè²»å¤§éçéç®è³æºï¼ä»¥å 2) äººé¡ææ¼ç®æ³éè¦è±è²»å¤§éè¤éèæ¼«é·çç¨åºæè½ç´¯ç©æéåå½¢ãGNN åæè½ä¹éç¸äºéä¿çç¥è­ã
çºäºé²ä¸æ­¥æå GNN æ¶æ§è¨­è¨çèªååï¼æåæåºäºä¸ç¨®éç®ååçæ¹å¼ï¼è®å¤§åèªè¨æ¨¡å (LLM) å·åè¨­è¨ GNN çå°æ¥­ç¥è­ï¼å¾èå¤§å¹ç¸®ç­è¨­è¨ GNN æ¶æ§çéç®è² æåéç¼é±æãæåçæ¶æ§é¦åå»ºç«ä¸åç¥è­æ·åç®¡éï¼äºè§£åå½¢ãGNN åæè½ä¹éçç¸äºéè¯æ§ãéåç®¡éå°éå»çæ¨¡åè¨­è¨ç¶é©è½ææçµæ§åçç¥è­ï¼ä¾ LLM åèï¼è® LLM è½å¤ å¿«éæåºåæ­¥çæ¨¡åå»ºè­°ãé¨å¾ï¼æåå¼å¥ä¸ç¨®ç¥è­é©åçæå°ç­ç¥ï¼æ¨¡æ¬äººé¡å°å®¶çæ¢ç´¢èéç¼ç¨åºï¼è® LLM è½å¨æå¸æçç¯åå§å¿«éæ¹ååæ­¥å»ºè­°ãå»£æ³çå¯¦é©è­æï¼æåçæ¶æ§å¯ä»¥å¨å¹¾ç§éå§ææå°éå°æªè¦éçè³æéæä¾æå¸æç (ä¾å¦ï¼å 5.77%) åæ­¥æ¨¡åå»ºè­°ï¼èä¸ç¡éä»»ä½ååçè¨ç·´ï¼ä¸¦å¨å¹¾æ¬¡åè¦éç®ä¸­å°±è½éæååºçæå°æè½ã

##### **Body Transformer: Leveraging Robot Embodiment for Policy Learning**
2408.06316v1 by Carmelo Sferrazza, Dun-Ming Huang, Fangchen Liu, Jongmin Lee, Pieter Abbeel

In recent years, the transformer architecture has become the de facto
standard for machine learning algorithms applied to natural language processing
and computer vision. Despite notable evidence of successful deployment of this
architecture in the context of robot learning, we claim that vanilla
transformers do not fully exploit the structure of the robot learning problem.
Therefore, we propose Body Transformer (BoT), an architecture that leverages
the robot embodiment by providing an inductive bias that guides the learning
process. We represent the robot body as a graph of sensors and actuators, and
rely on masked attention to pool information throughout the architecture. The
resulting architecture outperforms the vanilla transformer, as well as the
classical multilayer perceptron, in terms of task completion, scaling
properties, and computational efficiency when representing either imitation or
reinforcement learning policies. Additional material including the open-source
code is available at https://sferrazza.cc/bot_site.

æè¦ï¼è¿å¹´æ¥ï¼ååå¨æ¶æå·²æä¸ºåºç¨äºèªç¶è¯­è¨å¤çåè®¡ç®æºè§è§çæºå¨å­¦ä¹ ç®æ³çå®éæ åãå°½ç®¡ææ¾çè¯æ®è¡¨æå¨æºå¨äººå­¦ä¹ çèæ¯ä¸æåé¨ç½²äºæ­¤æ¶æï¼ä½æä»¬å£°ç§°åå§ååå¨å¹¶æªååå©ç¨æºå¨äººå­¦ä¹ é®é¢çç»æãå æ­¤ï¼æä»¬æåºäº Body Transformer (BoT)ï¼ä¸ç§éè¿æä¾æå¯¼å­¦ä¹ è¿ç¨çå½çº³åå·®æ¥å©ç¨æºå¨äººä½ç°çæ¶æãæä»¬å°æºå¨äººä¸»ä½è¡¨ç¤ºä¸ºä¼ æå¨åæ§è¡å¨çå¾å½¢ï¼å¹¶ä¾é æ©ç æ³¨æåæ¥æ±éæ´ä¸ªæ¶æä¸­çä¿¡æ¯ãå¨å®æä»»å¡ãç¼©æ¾å±æ§åè®¡ç®æçæ¹é¢ï¼æ è®ºæ¯è¡¨ç¤ºæ¨¡ä»¿è¿æ¯å¼ºåå­¦ä¹ ç­ç¥ï¼ç±æ­¤äº§ççæ¶æé½ä¼äºåå§ååå¨ä»¥åç»å¸çå¤å±æç¥å¨ãåæ¬å¼æºä»£ç å¨åçå¶ä»ææå¯ä» https://sferrazza.cc/bot_site è·å¾ã

##### **ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation Using Large Language Models and Transformers**
2408.06040v1 by Aristi Papastavrou, Maria Lymperaiou, Giorgos Stamou

In the rapidly evolving fields of natural language processing and computer
vision, Visual Word Sense Disambiguation (VWSD) stands as a critical, yet
challenging task. The quest for models that can seamlessly integrate and
interpret multimodal data is more pressing than ever. Imagine a system that can
understand language with the depth and nuance of human cognition, while
simultaneously interpreting the rich visual context of the world around it.
  We present ARPA, an architecture that fuses the unparalleled contextual
understanding of large language models with the advanced feature extraction
capabilities of transformers, which then pass through a custom Graph Neural
Network (GNN) layer to learn intricate relationships and subtle nuances within
the data. This innovative architecture not only sets a new benchmark in visual
word disambiguation but also introduces a versatile framework poised to
transform how linguistic and visual data interact by harnessing the synergistic
strengths of its components, ensuring robust performance even in the most
complex disambiguation scenarios. Through a series of experiments and
comparative analysis, we reveal the substantial advantages of our model,
underscoring its potential to redefine standards in the field. Beyond its
architectural prowess, our architecture excels through experimental
enrichments, including sophisticated data augmentation and multi-modal training
techniques.
  ARPA's introduction marks a significant milestone in visual word
disambiguation, offering a compelling solution that bridges the gap between
linguistic and visual modalities. We invite researchers and practitioners to
explore the capabilities of our model, envisioning a future where such hybrid
models drive unprecedented advancements in artificial intelligence.

æè¦ï¼å¨èªç¶èªè¨èçåé»è¦è¦è¦ºå¿«éæ¼é²çé åä¸­ï¼è¦è¦ºè©å½æ¶æ­§ (VWSD) æ¯ä¸åééµä¸å·æææ°æ§çä»»åãå°æ¾è½å¤ ç¡ç¸«æ´ååè©®éå¤æ¨¡æè³æçæ¨¡åæ¯ä»¥å¾ä»»ä½æåé½æ´å è¿«åãæ³åä¸åç³»çµ±ï¼å®å¯ä»¥åäººé¡èªç¥ä¸æ¨£æ·±å¥ä¸ç´°ç·»å°çè§£èªè¨ï¼åæéè½è©®éå¨åä¸ççè±å¯è¦è¦ºèçµ¡ã
æåæåº ARPAï¼ä¸ç¨®æ¶æ§ï¼å®èåäºå¤§åèªè¨æ¨¡åç¡èå«æ¯çèçµ¡çè§£è½åå Transformer çé²éç¹å¾µèåè½åï¼ç¶å¾ééä¸åèªè¨åå½¢ç¥ç¶ç¶²è·¯ (GNN) å±¤ä¾å­¸ç¿è³æä¸­çè¤ééä¿åç´°å¾®å·®ç°ãéç¨®åµæ°çæ¶æ§ä¸åå¨è¦è¦ºè©å½æ¶æ­§ä¸­è¨­å®äºæ°çåºæºï¼éå¼å¥äºä¸åå¤åè½çæ¡æ¶ï¼æºåééå©ç¨å¶çµæé¨åçåååªå¢ä¾è½è®èªè¨åè¦è¦ºè³æçäºåæ¹å¼ï¼ç¢ºä¿å³ä½¿å¨æè¤éçæ¶æ­§å ´æ¯ä¸­ä¹è½æå¼·å¥çæè½ãééä¸ç³»åçå¯¦é©åæ¯è¼åæï¼æåæ­ç¤ºäºæåæ¨¡åçé¡¯èåªå¢ï¼å¼·èª¿äºå®å¨éæ°å®ç¾©è©²é åæ¨æºçæ½åãé¤äºå¶æ¶æ§åªå¢ä¹å¤ï¼æåçæ¶æ§éééå¯¦é©è±å¯åèè¡¨ç¾åºè²ï¼åæ¬ç²¾å¯çè³ææ´ååå¤æ¨¡æè¨ç·´æè¡ã
ARPA çæ¨åºæ¨èªèè¦è¦ºè©å½æ¶æ­§çä¸åéè¦éç¨ç¢ï¼æä¾äºä¸åå¼äººæ³¨ç®çè§£æ±ºæ¹æ¡ï¼å½åäºèªè¨åè¦è¦ºæ¨¡æä¹éçå·®è·ãæåéè«ç ç©¶äººå¡åå¾æ¥­äººå¡æ¢ç´¢æåæ¨¡åçè½åï¼å±æä¸åç±éç¨®æ··åæ¨¡åæ¨åäººå·¥æºæ§åææªæçé²æ­¥çæªä¾ã

##### **ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models**
2408.05948v1 by Ronak Pradeep, Daniel Lee, Ali Mousavi, Jeff Pound, Yisi Sang, Jimmy Lin, Ihab Ilyas, Saloni Potdar, Mostafa Arefiyan, Yunyao Li

The rapid advancement of Large Language Models (LLMs) and conversational
assistants necessitates dynamic, scalable, and configurable conversational
datasets for training and evaluation. These datasets must accommodate diverse
user interaction modes, including text and voice, each presenting unique
modeling challenges. Knowledge Graphs (KGs), with their structured and evolving
nature, offer an ideal foundation for current and precise knowledge. Although
human-curated KG-based conversational datasets exist, they struggle to keep
pace with the rapidly changing user information needs. We present ConvKGYarn, a
scalable method for generating up-to-date and configurable conversational KGQA
datasets. Qualitative psychometric analyses confirm our method can generate
high-quality datasets rivaling a popular conversational KGQA dataset while
offering it at scale and covering a wide range of human-interaction
configurations. We showcase its utility by testing LLMs on diverse
conversations - exploring model behavior on conversational KGQA sets with
different configurations grounded in the same KG fact set. Our results
highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate
parametric knowledge of LLMs, thus offering a robust solution to the constantly
evolving landscape of conversational assistants.

æè¦ï¼é¨èå¤§åèªè¨æ¨¡å (LLM) åå°è©±å¼å©ççå¿«éé²æ­¥ï¼éè¦åæãå¯æ´åä¸å¯è¨­å®çå°è©±å¼è³æéä¾é²è¡è¨ç·´åè©ä¼°ãéäºè³æéå¿é å®¹ç´ä¸åçä½¿ç¨èäºåæ¨¡å¼ï¼åæ¬æå­åèªé³ï¼æ¯ç¨®æ¨¡å¼é½åç¾ç¨ç¹çå»ºæ¨¡ææ°ãç¥è­åè­ (KG) å·æçµæ§åä¸ä¸æ·æ¼é²çç¹æ§ï¼çºç¶ååç²¾ç¢ºçç¥è­æä¾äºçæ³çåºç¤ãåç®¡å­å¨äººå·¥ç­å±çåºæ¼ç¥è­åè­çå°è©±å¼è³æéï¼ä½å®åé£ä»¥è·ä¸å¿«éè®åçä½¿ç¨èè³è¨éæ±ãæåæåº ConvKGYarnï¼éæ¯ä¸ç¨®å¯æ´åçæ¹æ³ï¼ç¨æ¼ç¢çææ°çä¸å¯è¨­å®çå°è©±å¼ KGQA è³æéãå®æ§çå¿çæ¸¬éåæè­å¯¦ï¼æåçæ¨¡åå¯ä»¥ç¢çèæµè¡çå°è©±å¼ KGQA è³æéç¸åª²ç¾çåªè³ªè³æéï¼åæå¤§è¦æ¨¡æä¾è³æéï¼ä¸¦æ¶µèå»£æ³çäººæ©äºåè¨­å®ãæåééå¨ä¸åçå°è©±ä¸­æ¸¬è©¦ LLM ä¾å±ç¤ºå¶æç¨ï¼æ¢ç´¢æ¨¡åå¨å°è©±å¼ KGQA è¨­å®ä¸çè¡çºï¼éäºè¨­å®åºæ¼ç¸åçç¥è­åè­äºå¯¦éãæåççµæçªé¡¯äº ConvKGYarn æ¹å KGQA åºç¤åè©ä¼° LLM åæ¸åç¥è­çè½åï¼å¾èçºä¸æ·æ¼é²çå°è©±å¼å©çé åæä¾äºä¸åå¼·å¤§çè§£æ±ºæ¹æ¡ã

##### **The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms**
2408.05859v1 by Adam Davies, Ashkan Khakzar

Artificial neural networks have long been understood as "black boxes": though
we know their computation graphs and learned parameters, the knowledge encoded
by these weights and functions they perform are not inherently interpretable.
As such, from the early days of deep learning, there have been efforts to
explain these models' behavior and understand them internally; and recently,
mechanistic interpretability (MI) has emerged as a distinct research area
studying the features and implicit algorithms learned by foundation models such
as large language models. In this work, we aim to ground MI in the context of
cognitive science, which has long struggled with analogous questions in
studying and explaining the behavior of "black box" intelligent systems like
the human brain. We leverage several important ideas and developments in the
history of cognitive science to disentangle divergent objectives in MI and
indicate a clear path forward. First, we argue that current methods are ripe to
facilitate a transition in deep learning interpretation echoing the "cognitive
revolution" in 20th-century psychology that shifted the study of human
psychology from pure behaviorism toward mental representations and processing.
Second, we propose a taxonomy mirroring key parallels in computational
neuroscience to describe two broad categories of MI research, semantic
interpretation (what latent representations are learned and used) and
algorithmic interpretation (what operations are performed over representations)
to elucidate their divergent goals and objects of study. Finally, we elaborate
the parallels and distinctions between various approaches in both categories,
analyze the respective strengths and weaknesses of representative works,
clarify underlying assumptions, outline key challenges, and discuss the
possibility of unifying these modes of interpretation under a common framework.

æè¦ï¼äººå·¥ç¥ç¶ç¶²è·¯é·æä»¥ä¾é½è¢«è¦çºãé»çå­ãï¼åç®¡æåç¥éå®åçéç®åè¡¨åå­¸ç¿åæ¸ï¼ä½éäºæ¬éåå®åå·è¡çå½æ¸æç·¨ç¢¼çç¥è­ä¸¦éå¤©çå°±å¯è§£éãå æ­¤ï¼å¾æ·±åº¦å­¸ç¿çæ©æéå§ï¼å°±æè¨±å¤äººè´åæ¼è§£ééäºæ¨¡åçè¡çºä¸¦å¨å§é¨çè§£å®åï¼æè¿ï¼æ©å¶å¯è§£éæ§ (MI) å·²æçºä¸åç¨ç¹ççç ç©¶é åï¼æ¢è¨åºç¤æ¨¡åï¼ä¾å¦å¤§åèªè¨æ¨¡åï¼å­¸ç¿å°çç¹å¾µåé±å¼æ¼ç®æ³ãå¨éé å·¥ä½ä¸­ï¼æåæ¨å¨å° MI åºæ¼èªç¥ç§å­¸çèæ¯ï¼èªç¥ç§å­¸é·æä»¥ä¾ä¸ç´å¨ç ç©¶åè§£éãé»çå­ãæºè½ç³»çµ±ï¼ä¾å¦äººè¦ï¼çè¡çºæï¼åªåè§£æ±ºé¡ä¼¼çåé¡ãæåå©ç¨èªç¥ç§å­¸å²ä¸å¹¾åéè¦çæ³æ³åç¼å±ï¼ä¾è§£é MI ä¸­ä¸åçç®æ¨ï¼ä¸¦æåºæç¢ºçåé²éè·¯ãé¦åï¼æåèªçºç¶åçåç¨®æ¹æ³å·²æºåå¥½ä¿é²æ·±åº¦å­¸ç¿è§£éçè½è®ï¼éå¼æäº 20 ä¸ç´å¿çå­¸ä¸­çãèªç¥é©å½ãï¼å°äººé¡å¿çå­¸çç ç©¶å¾ç´ç²¹çè¡çºä¸»ç¾©è½åå¿æºè¡¨å¾µåèçãå¶æ¬¡ï¼æåæåºä¸ååé¡æ³ï¼åæ è¨ç®ç¥ç¶ç§å­¸ä¸­çééµç¸ä¼¼ä¹èï¼ä»¥æè¿° MI ç ç©¶çå©åå»£æ³é¡å¥ï¼èªç¾©è§£éï¼å­¸ç¿åä½¿ç¨çæ½å¨è¡¨å¾µæ¯ä»éº¼ï¼åæ¼ç®æ³è§£éï¼å¨è¡¨å¾µä¸å·è¡çéç®æ¯ä»éº¼ï¼ï¼ä»¥é¡æå®åä¸åçç®æ¨åç ç©¶å°è±¡ãæå¾ï¼æåé¡è¿°äºéå©åé¡å¥ä¸­åç¨®æ¹æ³ä¹éçç¸ä¼¼ä¹èååå¥ï¼åæä»£è¡¨æ§ä½åçåªç¼ºé»ï¼éæ¸åºæ¬åè¨­ï¼æ¦è¿°ééµææ°ï¼ä¸¦è¨è«å¨ä¸åå±åæ¶æ§ä¸çµ±ä¸éäºè§£éæ¨¡å¼çå¯è½æ§ã

##### **Investigating Instruction Tuning Large Language Models on Graphs**
2408.05457v1 by Kerui Zhu, Bo-Wei Huang, Bowen Jin, Yizhu Jiao, Ming Zhong, Kevin Chang, Shou-De Lin, Jiawei Han

Inspired by the recent advancements of Large Language Models (LLMs) in NLP
tasks, there's growing interest in applying LLMs to graph-related tasks. This
study delves into the capabilities of instruction-following LLMs for engaging
with real-world graphs, aiming to offer empirical insights into how LLMs can
effectively interact with graphs and generalize across graph tasks. We begin by
constructing a dataset designed for instruction tuning, which comprises a
diverse collection of 79 graph-related tasks from academic and e-commerce
domains, featuring 44,240 training instances and 18,960 test samples. Utilizing
this benchmark, our initial investigation focuses on identifying the optimal
graph representation that serves as a conduit for LLMs to understand complex
graph structures. Our findings indicate that JSON format for graph
representation consistently outperforms natural language and code formats
across various LLMs and graph types. Furthermore, we examine the key factors
that influence the generalization abilities of instruction-tuned LLMs by
evaluating their performance on both in-domain and out-of-domain graph tasks.

æè¦ï¼åå°èªç¶èªè¨èç (NLP) ä¸­å¤§åèªè¨æ¨¡å (LLM) è¿æé²å±çåç¼ï¼å° LLM æç¨æ¼èåè¡¨ç¸éä»»åçèè¶£æ¥çæ¿åãæ¬ç ç©¶æ¢è¨äºéµå¾ªæä»¤ç LLM çåè½ï¼ä»¥å¾äºçå¯¦ä¸ççåè¡¨ï¼æ¨å¨æä¾ LLM å¦ä½ææå°èåè¡¨äºåä¸¦å¨åè¡¨ä»»åä¸­é²è¡æ¦æ¬çç¶é©è¦è§£ãæåå¾æ§å»ºä¸åå°çºæä»¤èª¿æ´èè¨­è¨çè³æééå§ï¼å¶ä¸­åå«ä¾èªå­¸è¡åé»å­ååé åç 79 ååè¡¨ç¸éä»»åçå¤ååéåï¼åå« 44,240 åè¨ç·´å¯¦ä¾å 18,960 åæ¸¬è©¦æ¨£æ¬ãå©ç¨æ­¤åºæºï¼æåçåæ­¥èª¿æ¥éé»å¨æ¼è­å¥æä½³åè¡¨è¡¨ç¤ºï¼ä½çº LLM çè§£è¤éåè¡¨çµæ§çç®¡éãæåçç ç©¶çµæè¡¨æï¼JSON æ ¼å¼çåè¡¨è¡¨ç¤ºå¨åç¨® LLM ååè¡¨é¡åä¸­å§çµåªæ¼èªç¶èªè¨åç¨å¼ç¢¼æ ¼å¼ãæ­¤å¤ï¼æåæ¢è¨äºå½±é¿æä»¤èª¿æ´ LLM æ¦æ¬è½åçä¸»è¦å ç´ ï¼æ¹æ³æ¯è©ä¼°å®åå¨é åå§åé åå¤åè¡¨ä»»åä¸çè¡¨ç¾ã

##### **Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation**
2408.05456v1 by Wenbo Shang, Xuliang Zhu, Xin Huang

Unified graph representation learning aims to produce node embeddings, which
can be applied to multiple downstream applications. However, existing studies
based on graph neural networks and language models either suffer from the
limitations of numerous training needed toward specific downstream predictions
or have shallow semantic features. In this work, we propose a novel Path-LLM
model to learn unified graph representation, which leverages a powerful large
language model (LLM) to incorporate our proposed path features. Our Path-LLM
framework consists of several well-designed techniques. First, we develop a new
mechanism of long-to-short shortest path (L2SP) selection, which covers
essential connections between different dense groups. An in-depth comparison of
different path selection plans is offered to illustrate the strength of our
designed L2SP. Then, we design path textualization to obtain L2SP-based
training texts. Next, we feed the texts into a self-supervised LLM training
process to learn embeddings. Extensive experiments on benchmarks validate the
superiority of Path-LLM against the state-of-the-art WalkLM method on two
classical graph learning tasks (node classification and link prediction) and
one NP-hard graph query processing task (keyword search), meanwhile saving more
than 90% of training paths.

æè¦ï¼çµ±ä¸åå½¢è¡¨å¾µå­¸ç¿æ¨å¨ç¢çç¯é»åµå¥ï¼å¯ç¨æ¼å¤åä¸æ¸¸æç¨ç¨å¼ãç¶èï¼ç¾æçåºæ¼åå½¢ç¥ç¶ç¶²è·¯åèªè¨æ¨¡åçç ç©¶ï¼ä¸æ¯å éè¦éå°ç¹å®ä¸æ¸¸é æ¸¬èé²è¡å¤§éè¨ç·´èåå°éå¶ï¼å°±æ¯èªæç¹å¾µæ·ºèãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸åæ°ç©ç Path-LLM æ¨¡åä¾å­¸ç¿çµ±ä¸çåå½¢è¡¨å¾µï¼å®å©ç¨å¼·å¤§çå¤§åèªè¨æ¨¡å (LLM) ä¾ç´å¥æåæåºçè·¯å¾ç¹å¾µãæåç Path-LLM æ¡æ¶åå«äºå¤é è¨­è¨è¯å¥½çæè¡ãé¦åï¼æåéç¼äºä¸ç¨®æ°çé·å°ç­æç­è·¯å¾ (L2SP) é¸ææ©å¶ï¼å®æ¶µèäºä¸åå¯éç¾¤çµä¹éçå¿è¦é£æ¥ãæä¾äºä¸åè·¯å¾é¸ææ¹æ¡çæ·±å¥æ¯è¼ï¼ä»¥èªªææåè¨­è¨ç L2SP çåªå¢ãç¶å¾ï¼æåè¨­è¨è·¯å¾æå­åä»¥ç²å¾åºæ¼ L2SP çè¨ç·´ææ¬ãæ¥ä¸ä¾ï¼æåå°ææ¬è¼¸å¥å°èªç£ç£ LLM è¨ç·´éç¨ä¸­ä»¥å­¸ç¿åµå¥ãå¨åºæºä¸çå¤§éå¯¦é©é©è­äº Path-LLM å¨å©åç¶å¸åå½¢å­¸ç¿ä»»åï¼ç¯é»åé¡åé£çµé æ¸¬ï¼åä¸å NP é£åå½¢æ¥è©¢èçä»»åï¼ééµå­æå°ï¼ä¸åªæ¼æåé²ç WalkLM æ¹æ³ï¼åæç¯çäºè¶é 90% çè¨ç·´è·¯å¾ã

##### **LaiDA: Linguistics-aware In-context Learning with Data Augmentation for Metaphor Components Identification**
2408.05404v1 by Hongde Liu, Chenyuan He, Feiyang Meng, Changyong Niu, Yuxiang Jia

Metaphor Components Identification (MCI) contributes to enhancing machine
understanding of metaphors, thereby advancing downstream natural language
processing tasks. However, the complexity, diversity, and dependency on context
and background knowledge pose significant challenges for MCI. Large language
models (LLMs) offer new avenues for accurate comprehension of complex natural
language texts due to their strong semantic analysis and extensive commonsense
knowledge. In this research, a new LLM-based framework is proposed, named
Linguistics-aware In-context Learning with Data Augmentation (LaiDA).
Specifically, ChatGPT and supervised fine-tuning are utilized to tailor a
high-quality dataset. LaiDA incorporates a simile dataset for pre-training. A
graph attention network encoder generates linguistically rich feature
representations to retrieve similar examples. Subsequently, LLM is fine-tuned
with prompts that integrate linguistically similar examples. LaiDA ranked 2nd
in Subtask 2 of NLPCC2024 Shared Task 9, demonstrating its effectiveness. Code
and data are available at https://github.com/WXLJZ/LaiDA.

æè¦ï¼é±å»çµæè¾¨è­ (MCI) æå©æ¼æåæ©å¨å°é±å»ççè§£ï¼é²èæ¨åä¸æ¸¸çèªç¶èªè¨èçä»»åãä¸éï¼è¤éæ§ãå¤æ¨£æ§ï¼ä»¥åå°èçµ¡åèæ¯ç¥è­çä¾è³´æ§ï¼å° MCI èè¨æ¯éå¤§çææ°ãå¤§åèªè¨æ¨¡å (LLM) ç±æ¼å¶å¼·å¤§çèªæåæåå»£æ³çå¸¸è­ç¥è­ï¼çºæºç¢ºçè§£è¤éçèªç¶èªè¨ææ¬æä¾äºæ°éå¾ãæ¬ç ç©¶æåºäºä¸åæ°çåºæ¼ LLM çæ¶æ§ï¼ç¨±çºå·åè³ææ´ååè½çèªè¨æç¥æå¢å­¸ç¿ (LaiDA)ãå·é«ä¾èªªï¼ChatGPT åç£ç£å¾®èª¿ç¨æ¼èª¿æ´ä¸åé«åè³ªçè³æéãLaiDA çµåäºä¸åæ¯å»è³æéé²è¡é è¨ç·´ãä¸ååå½¢æ³¨æåç¶²è·¯ç·¨ç¢¼å¨ç¢çèªè¨è±å¯çç¹å¾µè¡¨ç¤ºï¼ä»¥æ·åé¡ä¼¼çç¯ä¾ãé¨å¾ï¼LLM ä½¿ç¨æ´åäºèªè¨ç¸ä¼¼ç¯ä¾çæç¤ºé²è¡å¾®èª¿ãLaiDA å¨ NLPCC2024 å±äº«ä»»å 9 çå­ä»»å 2 ä¸­æåç¬¬ 2ï¼è­æäºå¶æææ§ãç¨å¼ç¢¼åè³æå¯å¨ https://github.com/WXLJZ/LaiDA åå¾ã

##### **Text classification optimization algorithm based on graph neural network**
2408.15257v1 by Erdi Gao, Haowei Yang, Dan Sun, Haohao Xia, Yuhan Ma, Yuanjing Zhu

In the field of natural language processing, text classification, as a basic
task, has important research value and application prospects. Traditional text
classification methods usually rely on feature representations such as the bag
of words model or TF-IDF, which overlook the semantic connections between words
and make it challenging to grasp the deep structural details of the text.
Recently, GNNs have proven to be a valuable asset for text classification
tasks, thanks to their capability to handle non-Euclidean data efficiently.
However, the existing text classification methods based on GNN still face
challenges such as complex graph structure construction and high cost of model
training. This paper introduces a text classification optimization algorithm
utilizing graph neural networks. By introducing adaptive graph construction
strategy and efficient graph convolution operation, the accuracy and efficiency
of text classification are effectively improved. The experimental results
demonstrate that the proposed method surpasses traditional approaches and
existing GNN models across multiple public datasets, highlighting its superior
performance and feasibility for text classification tasks.

æè¦ï¼å¨èªç¶èªè¨èçé åï¼ææ¬åé¡ä½çºä¸é åºç¤ä»»åï¼å·æéè¦çç ç©¶å¹å¼åæç¨åæ¯ãå³çµ±çææ¬åé¡æ¹æ³éå¸¸ä¾è³´æ¼è©è¢æ¨¡åæ TF-IDF ç­ç¹å¾µè¡¨ç¤ºï¼éå¿½è¦äºè©å½ä¹éçèªç¾©è¯ç¹«ï¼ä½¿å¾é£ä»¥ææ¡ææ¬çæ·±å±¤çµæ§ç´°ç¯ãè¿å¹´ä¾ï¼GNN å·²è¢«è­ææ¯ææ¬åé¡ä»»åçå¯¶è²´è³ç¢ï¼éæ­¸åæ¼å®åææèçéæ­å¹¾éå¾æ¸æçè½åãç¶èï¼åºæ¼ GNN çç¾æææ¬åé¡æ¹æ³ä»ç¶é¢è¨èåçµæ§æ§å»ºè¤éãæ¨¡åè¨ç·´ææ¬é«ç­ææ°ãæ¬ææåºäºä¸ç¨®å©ç¨åç¥ç¶ç¶²çµ¡çææ¬åé¡åªåç®æ³ãééå¼å¥èªé©æåæ§å»ºç­ç¥åé«æçåå·ç©éç®ï¼ææå°æé«äºææ¬åé¡çæºç¢ºçåæçãå¯¦é©çµæè¡¨æï¼è©²æ¹æ³å¨å¤åå¬éæ¸æéä¸è¶è¶äºå³çµ±æ¹æ³åç¾æç GNN æ¨¡åï¼å¸é¡¯äºå¶å¨ææ¬åé¡ä»»åä¸­çåªè¶æ§è½åå¯è¡æ§ã

##### **SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV Battery Supply Chain Disruptions**
2408.05357v1 by Zhi-Qi Cheng, Yifei Dong, Aike Shi, Wei Liu, Yuzhi Hu, Jason O'Connor, Alexander Hauptmann, Kate Whitefoot

The electric vehicle (EV) battery supply chain's vulnerability to disruptions
necessitates advanced predictive analytics. We present SHIELD (Schema-based
Hierarchical Induction for EV supply chain Disruption), a system integrating
Large Language Models (LLMs) with domain expertise for EV battery supply chain
risk assessment. SHIELD combines: (1) LLM-driven schema learning to construct a
comprehensive knowledge library, (2) a disruption analysis system utilizing
fine-tuned language models for event extraction, multi-dimensional similarity
matching for schema matching, and Graph Convolutional Networks (GCNs) with
logical constraints for prediction, and (3) an interactive interface for
visualizing results and incorporating expert feedback to enhance
decision-making. Evaluated on 12,070 paragraphs from 365 sources (2022-2023),
SHIELD outperforms baseline GCNs and LLM+prompt methods (e.g., GPT-4o) in
disruption prediction. These results demonstrate SHIELD's effectiveness in
combining LLM capabilities with domain expertise for enhanced supply chain risk
assessment.

æè¦ï¼é»åè» (EV) é»æ± ä¾æéå®¹æåå°å¹²æ¾ï¼å æ­¤éè¦é²éçé æ¸¬åæãæåæåº SHIELDï¼åºæ¼æ¶æ§ç EV ä¾æéä¸­æ·éå±¤å¼æ­¸ç´ï¼ï¼éæ¯ä¸åæ´åå¤§åèªè¨æ¨¡å (LLM) è EV é»æ± ä¾æéé¢¨éªè©ä¼°é åå°æ¥­ç¥è­çç³»çµ±ãSHIELD çµåï¼(1) LLM é©åçæ¶æ§å­¸ç¿ï¼ç¨æ¼å»ºç½®ä¸åå¨é¢çç¥è­åº«ï¼(2) ä¸åä¸­æ·åæç³»çµ±ï¼å©ç¨å¾®èª¿èªè¨æ¨¡åé²è¡äºä»¶èåãå¤ç¶­åº¦ç¸ä¼¼æ§æ¯å°ç¨æ¼æ¶æ§æ¯å°ï¼ä»¥åå¸¶æéè¼¯ç´æçåå½¢å·ç©ç¶²è·¯ (GCN) ç¨æ¼é æ¸¬ï¼ä»¥å (3) ä¸åäºåä»é¢ï¼ç¨æ¼è¦è¦ºåçµæåç´å¥å°å®¶åé¥ä»¥å¢å¼·æ±ºç­å¶å®ãå¨ä¾èª 365 åä¾æºç 12,070 æ®µè½ï¼2022-2023 å¹´ï¼ä¸é²è¡è©ä¼°ï¼SHIELD å¨ä¸­æ·é æ¸¬æ¹é¢åªæ¼åºæº GCN å LLM+æç¤ºæ¹æ³ï¼ä¾å¦ï¼GPT-4oï¼ãéäºçµæè­æäº SHIELD å¨çµå LLM åè½èé åå°æ¥­ç¥è­ä»¥å¢å¼·ä¾æéé¢¨éªè©ä¼°æ¹é¢çæææ§ã

##### **A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning**
2408.05141v3 by Ye Yuan, Chengwu Liu, Jingyang Yuan, Gongbo Sun, Siqi Li, Ming Zhang

Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.

æè¦ï¼æª¢ç´¢å¢å¼·çæ (RAG) æ¯ä¸åæ¡æ¶ï¼ä½¿å¤§åèªè¨æ¨¡å (LLM) è½å¤ ééæ´åå¤é¨ç¥è­åº«ä¾å¢å¼·å¶æºç¢ºåº¦ä¸¦æ¸å°å¹»è¦ºãå¨æ¬æä¸­ï¼æåä»ç´¹äºä¸åæ··åç RAG ç³»çµ±ï¼ééä¸çµå¨é¢çæä½³åä¾å¢å¼·ï¼éäºæä½³åé¡¯èå°æ¹åäºæª¢ç´¢åè³ªãå¢å¼·æ¨çè½åï¼ä¸¦æ¹åæ¸å¼è¨ç®è½åãæåæ¹é²äºç¶²é ä¸­çæå­åå¡åè¡¨æ ¼ï¼å å¥å±¬æ§é æ¸¬å¨ä»¥æ¸å°å¹»è¦ºï¼å·è¡äº LLM ç¥è­èåå¨åç¥è­åè¡¨èåå¨ï¼ä¸¦æçµå»ºæ§äºä¸ååå«ææåèçæ¨çç­ç¥ãæåå¨ Meta CRAG KDD Cup 2024 ç«¶è³½ä¸­éé CRAG è³æéè©ä¼°æåçç³»çµ±ãå¨å°ç«¯åç·ä¸è©ä¼°é½è­ææåçç³»çµ±é¡¯èå°å¢å¼·äºè¤éçæ¨çè½åãå¨å°ç«¯è©ä¼°ä¸­ï¼èåºç·æ¨¡åç¸æ¯ï¼æåé¡¯èå°æ¹åäºæºç¢ºåº¦ä¸¦éä½äºé¯èª¤çï¼éå°äºé¡¯èçè©åæåãåæï¼æåå¨ç·ä¸è©ä¼°ä¸­ç²å¾äºååºççµæï¼è­æäºææåºçç³»çµ±çæè½åæ³åè½åãæåç³»çµ±çåå§ç¢¼å·²å¨ \url{https://gitlab.aicrowd.com/shizueyy/crag-new} ä¸­ç¼å¸ã

##### **Node Level Graph Autoencoder: Unified Pretraining for Textual Graph Learning**
2408.07091v2 by Wenbin Hu, Huihao Jing, Qi Hu, Haoran Li, Yangqiu Song

Textual graphs are ubiquitous in real-world applications, featuring rich text
information with complex relationships, which enables advanced research across
various fields. Textual graph representation learning aims to generate
low-dimensional feature embeddings from textual graphs that can improve the
performance of downstream tasks. A high-quality feature embedding should
effectively capture both the structural and the textual information in a
textual graph. However, most textual graph dataset benchmarks rely on word2vec
techniques to generate feature embeddings, which inherently limits their
capabilities. Recent works on textual graph representation learning can be
categorized into two folds: supervised and unsupervised methods. Supervised
methods finetune a language model on labeled nodes, which have limited
capabilities when labeled data is scarce. Unsupervised methods, on the other
hand, extract feature embeddings by developing complex training pipelines. To
address these limitations, we propose a novel unified unsupervised learning
autoencoder framework, named Node Level Graph AutoEncoder (NodeGAE). We employ
language models as the backbone of the autoencoder, with pretraining on text
reconstruction. Additionally, we add an auxiliary loss term to make the feature
embeddings aware of the local graph structure. Our method maintains simplicity
in the training process and demonstrates generalizability across diverse
textual graphs and downstream tasks. We evaluate our method on two core graph
representation learning downstream tasks: node classification and link
prediction. Comprehensive experiments demonstrate that our approach
substantially enhances the performance of diverse graph neural networks (GNNs)
across multiple textual graph datasets.

æè¦ï¼<paragraph>ææ¬åè¡¨å¨ç¾å¯¦ä¸ççæç¨ä¸­ç¡èä¸å¨ï¼å®å·æè±å¯çæå­è³è¨åè¤éçéä¿ï¼éä½¿å¾å®è½å¤ å¨åç¨®é åé²è¡åé²çç ç©¶ãææ¬åè¡¨è¡¨ç¤ºå­¸ç¿æ¨å¨å¾ææ¬åè¡¨ä¸­ç¢çä½ç¶­ç¹å¾µåµå¥ï¼éå¯ä»¥æ¹åä¸æ¸¸ä»»åçæè½ãä¸åé«åè³ªçç¹å¾µåµå¥æè©²ææå°æ·åææ¬åè¡¨ä¸­ççµæ§åæå­è³è¨ãç¶èï¼å¤§å¤æ¸ææ¬åè¡¨è³æéåºæºä¾è³´ word2vec æè¡ä¾ç¢çç¹å¾µåµå¥ï¼éå¨æ ¹æ¬ä¸éå¶äºå®åçè½åãæè¿éæ¼ææ¬åè¡¨è¡¨ç¤ºå­¸ç¿çç ç©¶å¯ä»¥åçºå©é¡ï¼ç£ç£å¼åéç£ç£å¼æ¹æ³ãç£ç£å¼æ¹æ³å¨æ¨ç±¤ç¯é»ä¸å¾®èª¿èªè¨æ¨¡åï¼ç¶æ¨ç±¤è³æç¨å°æï¼å®åçè½åæéãå¦ä¸æ¹é¢ï¼éç£ç£å¼æ¹æ³éééç¼è¤éçè¨ç·´ç®¡ç·ä¾æåç¹å¾µåµå¥ãçºäºè§£æ±ºéäºéå¶ï¼æåæåºäºä¸åæ°ç©ççµ±ä¸éç£ç£å¼å­¸ç¿èªåç·¨ç¢¼å¨æ¡æ¶ï¼ç¨±çºç¯é»å±¤ç´åå½¢èªåç·¨ç¢¼å¨ï¼NodeGAEï¼ãæåæ¡ç¨èªè¨æ¨¡åä½çºèªåç·¨ç¢¼å¨çéª¨å¹¹ï¼ä¸¦å¨æå­éå»ºä¸é²è¡é è¨ç·´ãæ­¤å¤ï¼æåéæ°å¢äºä¸åè¼å©æå¤±é ï¼ä»¥ä½¿ç¹å¾µåµå¥èªè­å°å±é¨åå½¢çµæ§ãæåçæ¨¡åå¨è¨ç·´éç¨ä¸­ä¿æäºç°¡æ½æ§ï¼ä¸¦å±ç¤ºäºå¨ä¸åçææ¬åè¡¨åä¸æ¸¸ä»»åä¸­çæ³åæ§ãæåå¨å©åæ ¸å¿åå½¢è¡¨ç¤ºå­¸ç¿ä¸æ¸¸ä»»åä¸è©ä¼°äºæåçæ¨¡åï¼ç¯é»åé¡åé£çµé æ¸¬ãç¶åå¯¦é©è¡¨æï¼æåçæ¨¡åæ¹æ³å¤§å¹æåäºå¤åææ¬åå½¢è³æéä¸­çåç¨®åå½¢ç¥ç¶ç¶²è·¯ï¼GNNï¼çæè½ã</paragraph>

##### **HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction**
2408.04948v1 by Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, Dhagash Mehta

Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain

æè¦ï¼å¾éçµæ§åææ¬è³æä¸­æ·ååè©®éè¤éè³è¨ï¼ä¾å¦è²¡åæç¨ä¸­ç¢ççæ¶çé»è©±æè­°è¨éï¼å³ä½¿ä½¿ç¨ç¶åä½¿ç¨æª¢ç´¢æ´åçæ (RAG) çæä½³å¯¦åï¼ç¨±çº VectorRAG æè¡ï¼å®ä½¿ç¨åéè³æåº«ä¾é²è¡è³è¨æª¢ç´¢ï¼ï¼ç±æ¼é åç¹å®è¡èªåæä»¶æ ¼å¼è¤éç­ææ°ï¼å°å¤§åèªè¨æ¨¡å (LLM) èè¨ä»æ§æéå¤§ææ°ãæåæåºä¸ååºæ¼ç¨±çº HybridRAG ççµåçæ°æ¹æ³ï¼å¶çµåäºåºæ¼ç¥è­åè­ (KG) ç RAG æè¡ï¼ç¨±çº GraphRAGï¼å VectorRAG æè¡ï¼ä»¥å¢å¼·è²¡åæä»¶è³è¨æ·åçåç­ (Q&A) ç³»çµ±ï¼è­æå®è½å¤ ç¢çæºç¢ºä¸èèçµ¡ç¸éçç­æ¡ãä½¿ç¨ä¸çµä»¥åç­æ ¼å¼åç¾çè²¡åæ¶çé»è©±æè­°è¨éæä»¶é²è¡å¯¦é©ï¼å æ­¤æä¾äºèªç¶çä¸çµçå¯¦åç­å°ï¼æåè¡¨æ HybridRAG å¾åéè³æåº«å KG ä¸­æ·åèçµ¡ï¼å¨æª¢ç´¢åçæéæ®µçæª¢ç´¢æºç¢ºåº¦åç­æ¡çææ¹é¢ï¼é½åªæ¼å³çµ±ç VectorRAG å GraphRAGãææåºçæè¡å·æè¶åºè²¡åé åçæç¨

##### **DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization**
2408.04400v1 by Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, Liang Wang

This paper addresses the challenge of out-of-distribution (OOD)
generalization in graph machine learning, a field rapidly advancing yet
grappling with the discrepancy between source and target data distributions.
Traditional graph learning algorithms, based on the assumption of uniform
distribution between training and test data, falter in real-world scenarios
where this assumption fails, resulting in suboptimal performance. A principal
factor contributing to this suboptimal performance is the inherent simplicity
bias of neural networks trained through Stochastic Gradient Descent (SGD),
which prefer simpler features over more complex yet equally or more predictive
ones. This bias leads to a reliance on spurious correlations, adversely
affecting OOD performance in various tasks such as image recognition, natural
language understanding, and graph classification. Current methodologies,
including subgraph-mixup and information bottleneck approaches, have achieved
partial success but struggle to overcome simplicity bias, often reinforcing
spurious correlations. To tackle this, we propose DIVE, training a collection
of models to focus on all label-predictive subgraphs by encouraging the models
to foster divergence on the subgraph mask, which circumvents the limitation of
a model solely focusing on the subgraph corresponding to simple structural
patterns. Specifically, we employs a regularizer to punish overlap in extracted
subgraphs across models, thereby encouraging different models to concentrate on
distinct structural patterns. Model selection for robust OOD performance is
achieved through validation accuracy. Tested across four datasets from GOOD
benchmark and one dataset from DrugOOD benchmark, our approach demonstrates
significant improvement over existing methods, effectively addressing the
simplicity bias and enhancing generalization in graph machine learning.

æè¦ï¼<paragraph>éç¯è«ææ¢è¨äºåå½¢æ©å¨å­¸ç¿ä¸­éåä½ (OOD) æ¦åçææ°ï¼éæ¯ä¸åå¿«éç¼å±çé åï¼ä½å»å¨æå°ä¾æºåç®æ¨è³æåä½ä¹éçå·®ç°ä¸éå°å°é£ãå³çµ±çåå½¢å­¸ç¿æ¼ç®æ³åºæ¼è¨ç·´è³æåæ¸¬è©¦è³æä¹éåå»åä½çåè¨­ï¼ä½å¨éååè¨­å¤±æçå¯¦éææ³ä¸­æåºç¾åé¡ï¼å°è´æ¬¡ä½³æè½ãé æéç¨®æ¬¡ä½³æè½çä¸»è¦å ç´ æ¯ééé¨æ©æ¢¯åº¦ä¸é (SGD) è¨ç·´çç¥ç¶ç¶²è·¯åºæçç°¡ååå·®ï¼å®åå¥½è¼ç°¡å®çç¹å¾µï¼èéæ´è¤éä½é æ¸¬è½åç¸åææ´é«çç¹å¾µãéç¨®åå·®æå°è´ä¾è³´èåç¸éæ§ï¼å°åç¨®ä»»åï¼ä¾å¦å½±åè¾¨è­ãèªç¶èªè¨çè§£ååå½¢åé¡ï¼ç OOD æè½ç¢çè² é¢å½±é¿ãç®åçæè¡æ¹æ³ï¼åæ¬å­åæ··ååè³è¨ç¶é ¸æ¹æ³ï¼å·²åå¾é¨åæåï¼ä½ä»é£ä»¥åæç°¡ååå·®ï¼èä¸å¸¸å¸¸æå¼·åèåç¸éæ§ãçºäºè§£æ±ºéååé¡ï¼æåæåºäº DIVEï¼è¨ç·´ä¸çµæ¨¡åä»¥éæ³¨æææ¨ç±¤é æ¸¬å­åï¼æ¹æ³æ¯é¼åµæ¨¡åå¨å­åé®ç½©ä¸ä¿é²å·®ç°ï¼éé¿éäºæ¨¡ååéæ³¨å°ææ¼ç°¡å®çµæ§æ¨¡å¼çå­åçéå¶ãå·é«ä¾èªªï¼æåæ¡ç¨ä¸åæ­£è¦åå¨ä¾æ²ç½°æ¨¡åä¹éæåçå­åä¸­çéçï¼å¾èé¼åµä¸åçæ¨¡åå°æ³¨æ¼ä¸åççµæ§æ¨¡å¼ãééé©è­æºç¢ºåº¦ï¼å¯ä»¥é¸ææ¨¡åä»¥ç²å¾ç©©å¥ç OOD æè½ãæåçåæ³å¨ GOOD åºæºä¸­çååè³æéå DrugOOD åºæºä¸­çå¶ä¸­ä¸åè³æéä¸é²è¡äºæ¸¬è©¦ï¼çµæé¡¯ç¤ºåºæ¯ç¾ææ¹æ³æé¡¯èçé²æ­¥ï¼ææå°è§£æ±ºäºç°¡ååå·®ï¼ä¸¦å¢å¼·äºåå½¢æ©å¨å­¸ç¿ä¸­çæ¦åè½åã</paragraph>

##### **MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models**
2408.04388v1 by Haoxuan Li, Zhengmao Yang, Yunshan Ma, Yi Bin, Yang Yang, Tat-Seng Chua

We study an emerging and intriguing problem of multimodal temporal event
forecasting with large language models. Compared to using text or graph
modalities, the investigation of utilizing images for temporal event
forecasting has not been fully explored, especially in the era of large
language models (LLMs). To bridge this gap, we are particularly interested in
two key questions of: 1) why images will help in temporal event forecasting,
and 2) how to integrate images into the LLM-based forecasting framework. To
answer these research questions, we propose to identify two essential functions
that images play in the scenario of temporal event forecasting, i.e.,
highlighting and complementary. Then, we develop a novel framework, named
MM-Forecast. It employs an Image Function Identification module to recognize
these functions as verbal descriptions using multimodal large language models
(MLLMs), and subsequently incorporates these function descriptions into
LLM-based forecasting models. To evaluate our approach, we construct a new
multimodal dataset, MidEast-TE-mm, by extending an existing event dataset
MidEast-TE-mini with images. Empirical studies demonstrate that our MM-Forecast
can correctly identify the image functions, and further more, incorporating
these verbal function descriptions significantly improves the forecasting
performance. The dataset, code, and prompts are available at
https://github.com/LuminosityX/MM-Forecast.

æè¦ï¼æåç ç©¶å¤æ¨¡ææéäºä»¶é æ¸¬ä¸­ä¸åæ°èä¸æè¶£çèªè¨æ¨¡ååé¡ãç¸è¼æ¼ä½¿ç¨æå­æåè¡¨æ¨¡æï¼å©ç¨å½±åé²è¡æéäºä»¶é æ¸¬çç ç©¶å°æªè¢«ååæ¢ç´¢ï¼ç¹å¥æ¯å¨å¤§åèªè¨æ¨¡å (LLM) çæä»£ãçºäºå¡«è£éåç©ºç½ï¼æåç¹å¥æèè¶£çå©åééµåé¡æ¯ï¼1) çºä»éº¼å½±åæå©æ¼æéäºä»¶é æ¸¬ï¼ä»¥å 2) å¦ä½å°å½±åæ´åå°åºæ¼ LLM çé æ¸¬æ¡æ¶ä¸­ãçºäºåç­éäºç ç©¶åé¡ï¼æåæè­°æ¾åºå½±åå¨æéäºä»¶é æ¸¬å ´æ¯ä¸­æ®æ¼çå©ååºæ¬åè½ï¼å³çªé¡¯åè£åãç¶å¾ï¼æåéç¼ä¸ååçº MM-Forecast çæ°æ¡æ¶ãå®ä½¿ç¨å½±ååè½è­å¥æ¨¡çµï¼ä½¿ç¨å¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM) å°éäºåè½è­å¥çºæå­æè¿°ï¼ä¸¦é¨å¾å°éäºåè½æè¿°ç´å¥åºæ¼ LLM çé æ¸¬æ¨¡åä¸­ãçºäºè©ä¼°æåçæ¹æ³ï¼æåééä½¿ç¨å½±åæ´åç¾æçäºä»¶è³æé MidEast-TE-miniï¼å»ºæ§äºä¸åæ°çå¤æ¨¡æè³æé MidEast-TE-mmãå¯¦è­ç ç©¶è¡¨æï¼æåç MM-Forecast å¯ä»¥æ­£ç¢ºè­å¥å½±ååè½ï¼æ­¤å¤ï¼ç´å¥éäºæå­åè½æè¿°å¯ä»¥é¡¯èæ¹åé æ¸¬æè½ãè³æéãç¨å¼ç¢¼åæç¤ºå¯å¨ https://github.com/LuminosityX/MM-Forecast åå¾ã

