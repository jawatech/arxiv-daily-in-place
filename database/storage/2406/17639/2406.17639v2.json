{"2406.17639": {"publish_time": "2024-06-25", "title": "Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP", "paper_summary": "Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering two\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? We design AlignCLIP, in\norder to answer these questions and show that answers to both questions are\npositive. Through extensive experiments, we show that AlignCLIP achieves\nnoticeable enhancements in the cross-modal alignment of the embeddings, and\nthereby, reduces the modality gap, while maintaining the performance across\nseveral downstream evaluations, such as zero-shot image classification,\nzero-shot multi-modal retrieval and zero-shot semantic text similarity.", "paper_summary_zh": "\u5c0d\u6bd4\u8a9e\u8a00\u5f71\u50cf\u9810\u8a13\u7df4 (CLIP) \u5df2\u5c55\u73fe\u51fa\u5728\u96f6\u6b21\u5206\u985e\u548c\u8de8\u6a21\u614b\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e2d\u986f\u8457\u7684\u6539\u9032\u3002\u7136\u800c\uff0c\u5f9e\u5e7e\u4f55\u89c0\u9ede\u4f86\u770b\uff0c\u767c\u73fe CLIP \u5d4c\u5165\u7a7a\u9593\u5177\u6709\u660e\u986f\u7684\u6a21\u614b\u5dee\u8ddd\u3002\u6b64\u5dee\u8ddd\u4f7f\u5d4c\u5165\u7a7a\u9593\u904e\u65bc\u7a00\u758f\u4e14\u4e0d\u9023\u7e8c\uff0c\u4e14\u4e0d\u540c\u6a21\u614b\u5bc6\u96c6\u5206\u4f48\u5728\u8d85\u7403\u9ad4\u7684\u4e0d\u540c\u5b50\u5340\u57df\u4e2d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u56de\u7b54\u5169\u500b\u4e3b\u8981\u554f\u984c\uff1a1. \u5728\u591a\u6a21\u614b\u7de8\u78bc\u5668\u4e4b\u9593\u5171\u4eab\u53c3\u6578\u7a7a\u9593\u662f\u5426\u6703\u7e2e\u5c0f\u6a21\u614b\u5dee\u8ddd\uff1f2. \u662f\u5426\u53ef\u4ee5\u900f\u904e\u900f\u904e\u6a21\u614b\u5167\u5206\u9694\u5c07\u55ae\u6a21\u614b\u5d4c\u5165\u5206\u958b\u4f86\u7e2e\u5c0f\u5dee\u8ddd\uff1f\u6211\u5011\u8a2d\u8a08\u4e86 AlignCLIP\uff0c\u4ee5\u56de\u7b54\u9019\u4e9b\u554f\u984c\uff0c\u4e26\u8868\u660e\u5c0d\u9019\u5169\u500b\u554f\u984c\u7684\u7b54\u6848\u90fd\u662f\u80af\u5b9a\u7684\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8868\u660e AlignCLIP \u5728\u5d4c\u5165\u7684\u8de8\u6a21\u614b\u5c0d\u9f4a\u4e2d\u7372\u5f97\u986f\u8457\u7684\u589e\u5f37\uff0c\u5f9e\u800c\u7e2e\u5c0f\u4e86\u6a21\u614b\u5dee\u8ddd\uff0c\u540c\u6642\u7dad\u6301\u4e86\u591a\u500b\u4e0b\u6e38\u8a55\u4f30\u7684\u6548\u80fd\uff0c\u4f8b\u5982\u96f6\u6b21\u5f71\u50cf\u5206\u985e\u3001\u96f6\u6b21\u591a\u6a21\u614b\u6aa2\u7d22\u548c\u96f6\u6b21\u8a9e\u610f\u6587\u5b57\u76f8\u4f3c\u6027\u3002", "author": "Sedigheh Eslami et.al.", "authors": "Sedigheh Eslami, Gerard de Melo", "id": "2406.17639v2", "paper_url": "http://arxiv.org/abs/2406.17639v2", "repo": "null"}}