{"2407.10839": {"publish_time": "2024-07-15", "title": "Offline Reinforcement Learning with Imputed Rewards", "paper_summary": "Offline Reinforcement Learning (ORL) offers a robust solution to training\nagents in applications where interactions with the environment must be strictly\nlimited due to cost, safety, or lack of accurate simulation environments.\nDespite its potential to facilitate deployment of artificial agents in the real\nworld, Offline Reinforcement Learning typically requires very many\ndemonstrations annotated with ground-truth rewards. Consequently,\nstate-of-the-art ORL algorithms can be difficult or impossible to apply in\ndata-scarce scenarios. In this paper we propose a simple but effective Reward\nModel that can estimate the reward signal from a very limited sample of\nenvironment transitions annotated with rewards. Once the reward signal is\nmodeled, we use the Reward Model to impute rewards for a large sample of\nreward-free transitions, thus enabling the application of ORL techniques. We\ndemonstrate the potential of our approach on several D4RL continuous locomotion\ntasks. Our results show that, using only 1\\% of reward-labeled transitions from\nthe original datasets, our learned reward model is able to impute rewards for\nthe remaining 99\\% of the transitions, from which performant agents can be\nlearned using Offline Reinforcement Learning.", "paper_summary_zh": "\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2 (ORL) \u63d0\u4f9b\u4e00\u500b\u5f37\u5065\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u5728\u4e92\u52d5\u5fc5\u9808\u56b4\u683c\u53d7\u5230\u9650\u5236\u7684\u61c9\u7528\u7a0b\u5f0f\u4e2d\u8a13\u7df4\u4ee3\u7406\uff0c\u539f\u56e0\u53ef\u80fd\u662f\u6210\u672c\u3001\u5b89\u5168\u6027\u6216\u7f3a\u4e4f\u6e96\u78ba\u7684\u6a21\u64ec\u74b0\u5883\u3002\u5118\u7ba1\u6709\u6f5b\u529b\u4fc3\u9032\u5728\u771f\u5be6\u4e16\u754c\u4e2d\u90e8\u7f72\u4eba\u5de5\u4ee3\u7406\uff0c\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2\u901a\u5e38\u9700\u8981\u8a31\u591a\u6a19\u793a\u6709\u771f\u5be6\u734e\u52f5\u7684\u793a\u7bc4\u3002\u56e0\u6b64\uff0c\u6700\u5148\u9032\u7684 ORL \u6f14\u7b97\u6cd5\u53ef\u80fd\u96e3\u4ee5\u6216\u7121\u6cd5\u61c9\u7528\u5728\u8cc7\u6599\u7a00\u5c11\u7684\u5834\u666f\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u734e\u52f5\u6a21\u578b\uff0c\u53ef\u4ee5\u6839\u64da\u975e\u5e38\u6709\u9650\u7684\u74b0\u5883\u8f49\u63db\u7bc4\u4f8b\uff08\u6a19\u793a\u6709\u734e\u52f5\uff09\u4f86\u4f30\u8a08\u734e\u52f5\u8a0a\u865f\u3002\u4e00\u65e6\u734e\u52f5\u8a0a\u865f\u5efa\u6a21\u5b8c\u6210\uff0c\u6211\u5011\u4f7f\u7528\u734e\u52f5\u6a21\u578b\u4f86\u63a8\u7b97\u5927\u91cf\u7121\u734e\u52f5\u8f49\u63db\u7684\u734e\u52f5\uff0c\u5f9e\u800c\u80fd\u5920\u61c9\u7528 ORL \u6280\u8853\u3002\u6211\u5011\u5728\u5e7e\u500b D4RL \u9023\u7e8c\u904b\u52d5\u4efb\u52d9\u4e2d\u5c55\u793a\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6f5b\u529b\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u50c5\u4f7f\u7528\u539f\u59cb\u8cc7\u6599\u96c6\u4e2d 1% \u7684\u6a19\u7c64\u734e\u52f5\u8f49\u63db\uff0c\u6211\u5011\u5b78\u7fd2\u5230\u7684\u734e\u52f5\u6a21\u578b\u5c31\u80fd\u5920\u63a8\u7b97\u51fa\u5269\u9918 99% \u8f49\u63db\u7684\u734e\u52f5\uff0c\u800c\u6211\u5011\u53ef\u4ee5\u4f7f\u7528\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2\u5f9e\u4e2d\u5b78\u7fd2\u5230\u9ad8\u6027\u80fd\u7684\u4ee3\u7406\u3002", "author": "Carlo Romeo et.al.", "authors": "Carlo Romeo, Andrew D. Bagdanov", "id": "2407.10839v1", "paper_url": "http://arxiv.org/abs/2407.10839v1", "repo": "null"}}