{"2412.15127": {"publish_time": "2024-12-19", "title": "Adaptive Pruning for Large Language Models with Structural Importance Awareness", "paper_summary": "The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u986f\u8457\u5730\n\u6539\u9032\u4e86\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u65bc LLM \u5c0d\u8a08\u7b97\u548c\u5132\u5b58\u8cc7\u6e90\u9700\u6c42\u9ad8\uff0c\u56e0\u6b64\u96e3\u4ee5\u5c07\u5176\u90e8\u7f72\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u908a\u7de3\u88dd\u7f6e\u4e0a\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684 LLM \u6a21\u578b\u526a\u679d\u65b9\u6cd5\uff0c\u5373\u7d50\u69cb\u611f\u77e5\u81ea\u9069\u61c9\u526a\u679d (SAAP)\uff0c\u4ee5\u5728\u7dad\u6301\u6a21\u578b\u6548\u80fd\u7684\u540c\u6642\u986f\u8457\u964d\u4f4e\u8a08\u7b97\u548c\u8a18\u61b6\u9ad4\u6210\u672c\u3002\u6211\u5011\u9996\u5148\u5b9a\u7fa9\u4e00\u500b\u81ea\u9069\u61c9\u91cd\u8981\u6027\u878d\u5408\u6307\u6a19\uff0c\u900f\u904e\u8003\u616e LLM \u4e2d\u6240\u6709\u8026\u5408\u7d50\u69cb\u7684\u540c\u8cea\u7570\u65b9\u5dee\u4e0d\u78ba\u5b9a\u6027\u4f86\u8a55\u4f30\u5176\u91cd\u8981\u6027\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c0d\u6240\u6709\u6a21\u7d44\u7684\u91cd\u8981\u6027\u9032\u884c\u6392\u5e8f\uff0c\u4ee5\u78ba\u5b9a\u61c9\u526a\u679d\u7684\u7279\u5b9a\u5c64\uff0c\u4ee5\u6eff\u8db3\u7279\u5b9a\u7684\u6548\u80fd\u9700\u6c42\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u65b0\u7684\u7fa4\u7d44\u5fae\u8abf\u7b56\u7565\uff0c\u4ee5\u6539\u5584 LLM \u7684\u63a8\u8ad6\u6548\u7387\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728\u5169\u500b\u5e38\u898b\u4efb\u52d9\u4e2d\u5c0d\u591a\u500b LLM \u8a55\u4f30\u6240\u63d0\u51fa\u7684 SAAP \u65b9\u6cd5\uff0c\u5373\u96f6\u6b21\u5206\u985e\u548c\u6587\u5b57\u751f\u6210\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684 SAAP \u65b9\u6cd5\u512a\u65bc\u591a\u7a2e\u6700\u5148\u9032\u7684\u57fa\u7dda\u65b9\u6cd5\uff0c\u5728 LLaMA-7B\u3001Vicuna-7B \u548c LLaMA-13B \u4e0a\u5206\u5225\u7372\u5f97 2.17%\u30012.37% \u548c 2.39% \u7684\u6e96\u78ba\u5ea6\u63d0\u5347\u3002\u6b64\u5916\uff0cSAAP \u5c07\u4ee3\u78bc\u751f\u6210\u901f\u5ea6\u63d0\u9ad8\u4e86 5%\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8cc7\u6e90\u53d7\u9650\u5834\u666f\u4e2d\u7684\u5be6\u969b\u512a\u52e2\u3002", "author": "Haotian Zheng et.al.", "authors": "Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han", "id": "2412.15127v1", "paper_url": "http://arxiv.org/abs/2412.15127v1", "repo": "null"}}