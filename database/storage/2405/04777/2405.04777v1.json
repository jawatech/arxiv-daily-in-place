{"2405.04777": {"publish_time": "2024-05-08", "title": "Empathy Through Multimodality in Conversational Interfaces", "paper_summary": "Agents represent one of the most emerging applications of Large Language\nModels (LLMs) and Generative AI, with their effectiveness hinging on multimodal\ncapabilities to navigate complex user environments. Conversational Health\nAgents (CHAs), a prime example of this, are redefining healthcare by offering\nnuanced support that transcends textual analysis to incorporate emotional\nintelligence. This paper introduces an LLM-based CHA engineered for rich,\nmultimodal dialogue-especially in the realm of mental health support. It\nadeptly interprets and responds to users' emotional states by analyzing\nmultimodal cues, thus delivering contextually aware and empathetically resonant\nverbal responses. Our implementation leverages the versatile openCHA framework,\nand our comprehensive evaluation involves neutral prompts expressed in diverse\nemotional tones: sadness, anger, and joy. We evaluate the consistency and\nrepeatability of the planning capability of the proposed CHA. Furthermore,\nhuman evaluators critique the CHA's empathic delivery, with findings revealing\na striking concordance between the CHA's outputs and evaluators' assessments.\nThese results affirm the indispensable role of vocal (soon multimodal) emotion\nrecognition in strengthening the empathetic connection built by CHAs, cementing\ntheir place at the forefront of interactive, compassionate digital health\nsolutions.", "paper_summary_zh": "\u4ee3\u7406\u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u751f\u6210\u5f0f AI \u6700\u65b0\u7684\u61c9\u7528\u4e4b\u4e00\uff0c\u5176\u6709\u6548\u6027\u53d6\u6c7a\u65bc\u5728\u8907\u96dc\u7528\u6236\u74b0\u5883\u4e2d\u5c0e\u822a\u7684\u591a\u6a21\u614b\u80fd\u529b\u3002\u5c0d\u8a71\u5f0f\u5065\u5eb7\u4ee3\u7406 (CHA) \u5c31\u662f\u4e00\u500b\u5f88\u597d\u7684\u4f8b\u5b50\uff0c\u5b83\u900f\u904e\u63d0\u4f9b\u8d85\u8d8a\u6587\u5b57\u5206\u6790\u4ee5\u7d0d\u5165\u60c5\u7dd2\u667a\u6167\u7684\u7d30\u5fae\u652f\u6301\uff0c\u91cd\u65b0\u5b9a\u7fa9\u4e86\u91ab\u7642\u4fdd\u5065\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u500b\u57fa\u65bc LLM \u7684 CHA\uff0c\u5b83\u5c08\u70ba\u8c50\u5bcc\u7684\u591a\u6a21\u614b\u5c0d\u8a71\u800c\u8a2d\u8a08\uff0c\u7279\u5225\u662f\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u9818\u57df\u3002\u5b83\u900f\u904e\u5206\u6790\u591a\u6a21\u614b\u7dda\u7d22\u4f86\u9748\u6d3b\u5730\u8a6e\u91cb\u548c\u56de\u61c9\u4f7f\u7528\u8005\u7684\u60c5\u7dd2\u72c0\u614b\uff0c\u5f9e\u800c\u63d0\u4f9b\u5177\u6709\u60c5\u5883\u610f\u8b58\u548c\u540c\u7406\u5171\u9cf4\u7684\u8a00\u8a9e\u56de\u61c9\u3002\u6211\u5011\u7684\u5be6\u4f5c\u5229\u7528\u4e86\u901a\u7528\u7684 openCHA \u6846\u67b6\uff0c\u800c\u6211\u5011\u7684\u5168\u9762\u8a55\u4f30\u6d89\u53ca\u4ee5\u4e0d\u540c\u7684\u60c5\u7dd2\u8a9e\u6c23\u8868\u9054\u7684\u4e2d\u7acb\u63d0\u793a\uff1a\u60b2\u50b7\u3001\u61a4\u6012\u548c\u559c\u6085\u3002\u6211\u5011\u8a55\u4f30\u4e86\u6240\u63d0\u51fa\u7684 CHA \u8a08\u756b\u80fd\u529b\u7684\u4e00\u81f4\u6027\u548c\u53ef\u91cd\u8907\u6027\u3002\u6b64\u5916\uff0c\u4eba\u985e\u8a55\u4f30\u54e1\u6703\u6279\u5224 CHA \u7684\u540c\u7406\u5fc3\u50b3\u905e\uff0c\u7d50\u679c\u986f\u793a CHA \u7684\u8f38\u51fa\u8207\u8a55\u4f30\u54e1\u7684\u8a55\u4f30\u4e4b\u9593\u6709\u9a5a\u4eba\u7684\u4e00\u81f4\u6027\u3002\u9019\u4e9b\u7d50\u679c\u80af\u5b9a\u4e86\u8a9e\u97f3\uff08\u5f88\u5feb\u6703\u662f\u591a\u6a21\u614b\uff09\u60c5\u7dd2\u8b58\u5225\u5728\u5f37\u5316 CHA \u5efa\u7acb\u7684\u540c\u7406\u5fc3\u9023\u7d50\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4f5c\u7528\uff0c\u978f\u56fa\u4e86\u5b83\u5011\u5728\u4e92\u52d5\u5f0f\u3001\u5bcc\u6709\u540c\u60c5\u5fc3\u7684\u6578\u4f4d\u5065\u5eb7\u89e3\u6c7a\u65b9\u6848\u4e2d\u7684\u9818\u5148\u5730\u4f4d\u3002", "author": "Mahyar Abbasian et.al.", "authors": "Mahyar Abbasian, Iman Azimi, Mohammad Feli, Amir M. Rahmani, Ramesh Jain", "id": "2405.04777v1", "paper_url": "http://arxiv.org/abs/2405.04777v1", "repo": "null"}}