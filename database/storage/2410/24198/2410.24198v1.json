{"2410.24198": {"publish_time": "2024-10-31", "title": "SelfCodeAlign: Self-Alignment for Code Generation", "paper_summary": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.", "paper_summary_zh": "\u6307\u4ee4\u8abf\u6574\u662f\u4e00\u7a2e\u76e3\u7763\u5fae\u8abf\u65b9\u6cd5\uff0c\u5b83\u986f\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9075\u5faa\u4eba\u985e\u6307\u4ee4\u7684\u80fd\u529b\u3002\u6211\u5011\u63d0\u51fa SelfCodeAlign\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u5b8c\u5168\u900f\u660e\u4e14\u5141\u8a31\u7684\u7ba1\u9053\uff0c\u7528\u65bc\u81ea\u6211\u5c0d\u9f4a\u7a0b\u5f0f\u78bc LLM\uff0c\u7121\u9700\u5927\u91cf\u4eba\u5de5\u8a3b\u89e3\u6216\u84b8\u993e\u3002SelfCodeAlign \u5728\u6574\u500b\u6578\u64da\u751f\u6210\u904e\u7a0b\u4e2d\u63a1\u7528\u76f8\u540c\u7684\u57fa\u790e\u6a21\u578b\u9032\u884c\u63a8\u8ad6\u3002\u5b83\u9996\u5148\u5f9e\u9ad8\u54c1\u8cea\u7a2e\u5b50\u7247\u6bb5\u4e2d\u63d0\u53d6\u4e0d\u540c\u7684\u7de8\u78bc\u6982\u5ff5\u4f86\u751f\u6210\u65b0\u4efb\u52d9\u3002\u7136\u5f8c\u5b83\u5c0d\u6bcf\u500b\u4efb\u52d9\u63a1\u6a23\u591a\u500b\u97ff\u61c9\uff0c\u5c07\u6bcf\u500b\u97ff\u61c9\u8207\u6e2c\u8a66\u7528\u4f8b\u914d\u5c0d\uff0c\u4e26\u5728\u6c99\u7bb1\u74b0\u5883\u4e2d\u9a57\u8b49\u5b83\u5011\u3002\u6700\u5f8c\uff0c\u9078\u64c7\u901a\u904e\u7bc4\u4f8b\u9032\u884c\u6307\u4ee4\u8abf\u6574\u3002\u5728\u6211\u5011\u7684\u57fa\u790e\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u4f7f\u7528 SelfCodeAlign \u8207 CodeQwen1.5-7B \u751f\u6210\u4e00\u500b\u5305\u542b 74k \u6307\u4ee4\u97ff\u61c9\u5c0d\u61c9\u7d44\u7684\u6578\u64da\u96c6\u3002\u5728\u8a72\u6578\u64da\u96c6\u4e0a\u7684\u5fae\u8abf\u5c0e\u81f4\u4e00\u500b\u6a21\u578b\uff0c\u8a72\u6a21\u578b\u5728 HumanEval+ \u4e0a\u5be6\u73fe\u4e86 67.1 \u7684 pass@1\uff0c\u5118\u7ba1\u5b83\u6bd4 CodeLlama-70B-Instruct \u5c0f\u5341\u500d\u3002\u5728\u6240\u6709\u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0c\u9019\u500b\u5fae\u8abf\u6a21\u578b\u59cb\u7d42\u512a\u65bc\u4f7f\u7528 OctoPack \u8a13\u7df4\u7684\u539f\u59cb\u7248\u672c\uff0cOctoPack \u662f\u4e4b\u524d\u5728\u6c92\u6709\u4eba\u5de5\u8a3b\u89e3\u6216\u84b8\u993e\u7684\u60c5\u6cc1\u4e0b\u9032\u884c\u6307\u4ee4\u8abf\u6574\u7684\u6700\u65b0\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u5011\u8868\u660e SelfCodeAlign \u5c0d\u5404\u7a2e\u898f\u6a21\u7684 LLM \u6709\u6548\uff0c\u5f9e 3B \u5230 33B\uff0c\u4e26\u4e14\u57fa\u790e\u6a21\u578b\u53ef\u4ee5\u66f4\u591a\u5730\u53d7\u76ca\u65bc\u8207\u5b83\u5011\u81ea\u5df1\u7684\u6578\u64da\u5206\u4f48\u5c0d\u9f4a\u3002\u6211\u5011\u9032\u4e00\u6b65\u9a57\u8b49\u4e86\u7ba1\u9053\u4e2d\u6bcf\u500b\u7d44\u4ef6\u7684\u6709\u6548\u6027\uff0c\u8868\u660e SelfCodeAlign \u512a\u65bc\u5f9e GPT-4o \u76f4\u63a5\u84b8\u993e\u548c\u57fa\u65bc GPT-3.5 \u7684\u9818\u5148\u84b8\u993e\u65b9\u6cd5\uff0c\u4f8b\u5982 OSS-Instruct \u548c Evol-Instruct\u3002SelfCodeAlign \u9084\u5c0e\u81f4\u4e86 StarCoder2-Instruct \u7684\u5275\u5efa\uff0cStarCoder2-Instruct \u662f\u7b2c\u4e00\u500b\u5b8c\u5168\u900f\u660e\u3001\u8a31\u53ef\u5bec\u9b06\u4e14\u81ea\u6211\u5c0d\u9f4a\u7684\u7a0b\u5f0f\u78bc LLM\uff0c\u5b83\u5be6\u73fe\u4e86\u6700\u5148\u9032\u7684\u7de8\u78bc\u6027\u80fd\u3002", "author": "Yuxiang Wei et.al.", "authors": "Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang", "id": "2410.24198v1", "paper_url": "http://arxiv.org/abs/2410.24198v1", "repo": "null"}}