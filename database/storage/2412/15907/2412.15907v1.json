{"2412.15907": {"publish_time": "2024-12-20", "title": "Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model", "paper_summary": "Background: Recent advances in large language models highlight the need for\nhigh-quality multilingual medical datasets. While Japan leads globally in CT\nscanner deployment and utilization, the lack of large-scale Japanese radiology\ndatasets has hindered the development of specialized language models for\nmedical imaging analysis. Objective: To develop a comprehensive Japanese CT\nreport dataset through machine translation and establish a specialized language\nmodel for structured finding classification. Additionally, to create a\nrigorously validated evaluation dataset through expert radiologist review.\nMethods: We translated the CT-RATE dataset (24,283 CT reports from 21,304\npatients) into Japanese using GPT-4o mini. The training dataset consisted of\n22,778 machine-translated reports, while the validation dataset included 150\nradiologist-revised reports. We developed CT-BERT-JPN based on\n\"tohoku-nlp/bert-base-japanese-v3\" architecture for extracting 18 structured\nfindings from Japanese radiology reports. Results: Translation metrics showed\nstrong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores\nranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression\nsections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in\n11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular\nseptal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1\nscores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in\nfour conditions. Conclusions: Our study establishes a robust Japanese CT report\ndataset and demonstrates the effectiveness of a specialized language model for\nstructured finding classification. The hybrid approach of machine translation\nand expert validation enables the creation of large-scale medical datasets\nwhile maintaining high quality.", "paper_summary_zh": "\u80cc\u666f\uff1a\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u6700\u65b0\u9032\u5c55\u51f8\u986f\u4e86\u5c0d\u9ad8\u54c1\u8cea\u591a\u8a9e\u8a00\u91ab\u7642\u8cc7\u6599\u96c6\u7684\u9700\u6c42\u3002\u65e5\u672c\u5728 CT \u6383\u63cf\u5100\u7684\u90e8\u7f72\u548c\u4f7f\u7528\u65b9\u9762\u8655\u65bc\u5168\u7403\u9818\u5148\u5730\u4f4d\uff0c\u4f46\u7f3a\u4e4f\u5927\u898f\u6a21\u7684\u65e5\u8a9e\u653e\u5c04\u79d1\u8cc7\u6599\u96c6\u963b\u7919\u4e86\u91dd\u5c0d\u91ab\u5b78\u5f71\u50cf\u5206\u6790\u7684\u5c08\u9580\u8a9e\u8a00\u6a21\u578b\u7684\u958b\u767c\u3002\u76ee\u6a19\uff1a\u900f\u904e\u6a5f\u5668\u7ffb\u8b6f\u958b\u767c\u4e00\u500b\u5168\u9762\u7684\u65e5\u8a9e CT \u5831\u544a\u8cc7\u6599\u96c6\uff0c\u4e26\u5efa\u7acb\u4e00\u500b\u5c08\u9580\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u7528\u65bc\u7d50\u69cb\u5316\u7d50\u679c\u5206\u985e\u3002\u6b64\u5916\uff0c\u900f\u904e\u5c08\u5bb6\u653e\u5c04\u79d1\u91ab\u5e2b\u7684\u5be9\u67e5\uff0c\u5efa\u7acb\u4e00\u500b\u56b4\u683c\u9a57\u8b49\u7684\u8a55\u4f30\u8cc7\u6599\u96c6\u3002\u65b9\u6cd5\uff1a\u6211\u5011\u4f7f\u7528 GPT-4o mini \u5c07 CT-RATE \u8cc7\u6599\u96c6\uff08\u4f86\u81ea 21,304 \u540d\u60a3\u8005\u7684 24,283 \u4efd CT \u5831\u544a\uff09\u7ffb\u8b6f\u6210\u65e5\u8a9e\u3002\u8a13\u7df4\u8cc7\u6599\u96c6\u5305\u542b 22,778 \u4efd\u6a5f\u5668\u7ffb\u8b6f\u5831\u544a\uff0c\u800c\u9a57\u8b49\u8cc7\u6599\u96c6\u5305\u542b 150 \u4efd\u653e\u5c04\u79d1\u91ab\u5e2b\u4fee\u6539\u904e\u7684\u5831\u544a\u3002\u6211\u5011\u57fa\u65bc\u300ctohoku-nlp/bert-base-japanese-v3\u300d\u67b6\u69cb\u958b\u767c\u4e86 CT-BERT-JPN\uff0c\u7528\u65bc\u5f9e\u65e5\u8a9e\u653e\u5c04\u79d1\u5831\u544a\u4e2d\u63d0\u53d6 18 \u9805\u7d50\u69cb\u5316\u7d50\u679c\u3002\u7d50\u679c\uff1a\u7ffb\u8b6f\u6307\u6a19\u986f\u793a\u5f37\u52c1\u7684\u8868\u73fe\uff0cBLEU \u5206\u6578\u70ba 0.731 \u548c 0.690\uff0c\u800c ROUGE \u5206\u6578\u5f9e\u7d50\u679c\u7684 0.770 \u5230 0.876\uff0c\u5f9e\u5370\u8c61\u90e8\u5206\u7684 0.748 \u5230 0.857 \u4e0d\u7b49\u3002\u8207 GPT-4o \u76f8\u6bd4\uff0cCT-BERT-JPN \u5728 18 \u7a2e\u60c5\u6cc1\u4e2d\u7684 11 \u7a2e\u60c5\u6cc1\u4e0b\u8868\u73fe\u51fa\u512a\u7570\u7684\u8868\u73fe\uff0c\u5305\u62ec\u6dcb\u5df4\u817a\u75c5\u8b8a\uff08+14.2%\uff09\u3001\u5c0f\u8449\u9593\u9694\u589e\u539a\uff08+10.9%\uff09\u548c\u80ba\u4e0d\u5f35\uff08+7.4%\uff09\u3002\u8a72\u6a21\u578b\u5728 18 \u7a2e\u60c5\u6cc1\u4e2d\u7684 14 \u7a2e\u60c5\u6cc1\u4e0b\u7dad\u6301 F1 \u5206\u6578\u8d85\u904e 0.95\uff0c\u4e26\u5728\u56db\u7a2e\u60c5\u6cc1\u4e0b\u9054\u5230\u5b8c\u7f8e\u5206\u6578\u3002\u7d50\u8ad6\uff1a\u6211\u5011\u7684\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u500b\u5f37\u5927\u7684\u65e5\u8a9e CT \u5831\u544a\u8cc7\u6599\u96c6\uff0c\u4e26\u5c55\u793a\u4e86\u4e00\u500b\u5c08\u9580\u7684\u8a9e\u8a00\u6a21\u578b\u5728\u7d50\u69cb\u5316\u7d50\u679c\u5206\u985e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6a5f\u5668\u7ffb\u8b6f\u548c\u5c08\u5bb6\u9a57\u8b49\u7684\u6df7\u5408\u65b9\u6cd5\u80fd\u5920\u5efa\u7acb\u5927\u898f\u6a21\u7684\u91ab\u7642\u8cc7\u6599\u96c6\uff0c\u540c\u6642\u4fdd\u6301\u9ad8\u54c1\u8cea\u3002", "author": "Yosuke Yamagishi et.al.", "authors": "Yosuke Yamagishi, Yuta Nakamura, Tomohiro Kikuchi, Yuki Sonoda, Hiroshi Hirakawa, Shintaro Kano, Satoshi Nakamura, Shouhei Hanaoka, Takeharu Yoshikawa, Osamu Abe", "id": "2412.15907v1", "paper_url": "http://arxiv.org/abs/2412.15907v1", "repo": "null"}}