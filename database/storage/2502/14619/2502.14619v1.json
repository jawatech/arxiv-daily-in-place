{"2502.14619": {"publish_time": "2025-02-20", "title": "Reward Models Identify Consistency, Not Causality", "paper_summary": "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.", "paper_summary_zh": "\u734e\u52f5\u6a21\u578b (RM) \u5728\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u4e26\u63d0\u5347\u63a8\u7406\u54c1\u8cea\u65b9\u9762\u626e\u6f14\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u50b3\u7d71\u4e0a\uff0cRM \u6703\u8a13\u7df4\u4f86\u6839\u64da\u5019\u9078\u8f38\u51fa\u7684\u6b63\u78ba\u6027\u548c\u4e00\u81f4\u6027\u9032\u884c\u6392\u540d\u3002\u7136\u800c\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u5e7e\u500b\u4ee4\u4eba\u9a5a\u8a1d\u7684\u767c\u73fe\uff0c\u6311\u6230\u4e86\u95dc\u65bc RM \u884c\u70ba\u7684\u5e38\u898b\u5047\u8a2d\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u6700\u5148\u9032\u7684\u734e\u52f5\u6a21\u578b\u512a\u5148\u8003\u616e\u7d50\u69cb\u4e00\u81f4\u6027\uff0c\u800c\u4e0d\u662f\u56e0\u679c\u6b63\u78ba\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u79fb\u9664\u554f\u984c\u9673\u8ff0\u5c0d\u734e\u52f5\u5206\u6578\u7684\u5f71\u97ff\u5f88\u5c0f\uff0c\u800c\u6539\u8b8a\u6578\u503c\u6216\u4e2d\u65b7\u63a8\u7406\u6d41\u7a0b\u5247\u6703\u986f\u8457\u5f71\u97ff RM \u8f38\u51fa\u3002\u6b64\u5916\uff0cRM \u8868\u73fe\u51fa\u5c0d\u5b8c\u6574\u63a8\u7406\u8ecc\u8de1\u7684\u5f37\u70c8\u4f9d\u8cf4\u6027\uff0c\u622a\u65b7\u6216\u4e0d\u5b8c\u6574\u7684\u6b65\u9a5f\u6703\u5c0e\u81f4\u734e\u52f5\u5206\u914d\u7522\u751f\u91cd\u5927\u8b8a\u5316\uff0c\u9019\u8868\u793a RM \u4e3b\u8981\u4f9d\u8cf4\u65bc\u5b78\u7fd2\u5230\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u800c\u4e0d\u662f\u660e\u78ba\u7684\u554f\u984c\u7406\u89e3\u3002\u9019\u4e9b\u767c\u73fe\u9069\u7528\u65bc\u591a\u7a2e\u67b6\u69cb\u3001\u8cc7\u6599\u96c6\u548c\u4efb\u52d9\uff0c\u5f97\u51fa\u4e09\u500b\u95dc\u9375\u898b\u89e3\uff1a(1) RM \u4e3b\u8981\u8a55\u4f30\u4e00\u81f4\u6027\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u63a8\u7406\u54c1\u8cea\uff1b(2) \u5728\u734e\u52f5\u5206\u914d\u4e2d\uff0c\u660e\u78ba\u554f\u984c\u7406\u89e3\u7684\u89d2\u8272\u88ab\u8a87\u5927\u4e86\uff1b(3) \u76ee\u524d\u7684 RM \u5728\u6392\u540d\u56de\u61c9\u65b9\u9762\u53ef\u80fd\u6bd4\u9a57\u8b49\u908f\u8f2f\u6709\u6548\u6027\u66f4\u6709\u6548\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\u73fe\u6709\u734e\u52f5\u5efa\u6a21\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u9650\u5236\uff0c\u5f37\u8abf\u9700\u8981\u8f49\u5411\u56e0\u679c\u611f\u77e5\u734e\u52f5\u6a21\u578b\uff0c\u8d85\u8d8a\u4ee5\u4e00\u81f4\u6027\u70ba\u5c0e\u5411\u7684\u8a55\u4f30\u3002", "author": "Yuhui Xu et.al.", "authors": "Yuhui Xu, Hanze Dong, Lei Wang, Caiming Xiong, Junnan Li", "id": "2502.14619v1", "paper_url": "http://arxiv.org/abs/2502.14619v1", "repo": "null"}}