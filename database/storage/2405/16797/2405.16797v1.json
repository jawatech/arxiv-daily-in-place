{"2405.16797": {"publish_time": "2024-05-27", "title": "A Real-Time Voice Activity Detection Based On Lightweight Neural", "paper_summary": "Voice activity detection (VAD) is the task of detecting speech in an audio\nstream, which is challenging due to numerous unseen noises and low\nsignal-to-noise ratios in real environments. Recently, neural network-based\nVADs have alleviated the degradation of performance to some extent. However,\nthe majority of existing studies have employed excessively large models and\nincorporated future context, while neglecting to evaluate the operational\nefficiency and latency of the models. In this paper, we propose a lightweight\nand real-time neural network called MagicNet, which utilizes casual and depth\nseparable 1-D convolutions and GRU. Without relying on future features as\ninput, our proposed model is compared with two state-of-the-art algorithms on\nsynthesized in-domain and out-domain test datasets. The evaluation results\ndemonstrate that MagicNet can achieve improved performance and robustness with\nfewer parameter costs.", "paper_summary_zh": "\u8a9e\u97f3\u6d3b\u52d5\u5075\u6e2c (VAD) \u7684\u4efb\u52d9\u662f\u5075\u6e2c\u97f3\u8a0a\u4e32\u6d41\u4e2d\u7684\u8a9e\u97f3\uff0c\u7531\u65bc\u5728\u771f\u5be6\u74b0\u5883\u4e2d\u5b58\u5728\u8a31\u591a\u672a\u898b\u904e\u7684\u96dc\u8a0a\u548c\u4f4e\u4fe1\u865f\u96dc\u8a0a\u6bd4\uff0c\u9019\u662f\u4e00\u9805\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\u3002\u8fd1\u671f\uff0c\u57fa\u65bc\u795e\u7d93\u7db2\u8def\u7684 VAD \u5728\u67d0\u7a2e\u7a0b\u5ea6\u4e0a\u6e1b\u8f15\u4e86\u6548\u80fd\u7684\u964d\u4f4e\u3002\u7136\u800c\uff0c\u73fe\u6709\u7814\u7a76\u5927\u591a\u63a1\u7528\u904e\u5927\u7684\u6a21\u578b\u4e26\u7d0d\u5165\u672a\u4f86\u7684\u8108\u7d61\uff0c\u537b\u5ffd\u7565\u4e86\u8a55\u4f30\u6a21\u578b\u7684\u64cd\u4f5c\u6548\u7387\u8207\u5ef6\u9072\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba MagicNet \u7684\u8f15\u91cf\u7d1a\u5373\u6642\u795e\u7d93\u7db2\u8def\uff0c\u5b83\u5229\u7528\u96a8\u6a5f\u4e14\u6df1\u5ea6\u53ef\u5206\u96e2\u7684 1D \u5377\u7a4d\u548c GRU\u3002\u6211\u5011\u7684\u5efa\u8b70\u6a21\u578b\u4e26\u672a\u4f9d\u8cf4\u672a\u4f86\u7279\u5fb5\u4f5c\u70ba\u8f38\u5165\uff0c\u4e26\u5728\u5408\u6210\u9818\u57df\u5167\u548c\u9818\u57df\u5916\u6e2c\u8a66\u8cc7\u6599\u96c6\u4e0a\u8207\u5169\u7a2e\u6700\u5148\u9032\u7684\u6f14\u7b97\u6cd5\u9032\u884c\u6bd4\u8f03\u3002\u8a55\u4f30\u7d50\u679c\u986f\u793a\uff0cMagicNet \u80fd\u4ee5\u8f03\u5c11\u7684\u53c3\u6578\u6210\u672c\uff0c\u9054\u5230\u66f4\u597d\u7684\u6548\u80fd\u548c\u7a69\u5065\u6027\u3002", "author": "Jidong Jia et.al.", "authors": "Jidong Jia, Pei Zhao, Di Wang", "id": "2405.16797v1", "paper_url": "http://arxiv.org/abs/2405.16797v1", "repo": "null"}}