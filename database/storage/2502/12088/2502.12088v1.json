{"2502.12088": {"publish_time": "2025-02-17", "title": "Meta-Statistical Learning: Supervised Learning of Statistical Inference", "paper_summary": "This work demonstrates that the tools and principles driving the success of\nlarge language models (LLMs) can be repurposed to tackle distribution-level\ntasks, where the goal is to predict properties of the data-generating\ndistribution rather than labels for individual datapoints. These tasks\nencompass statistical inference problems such as parameter estimation,\nhypothesis testing, or mutual information estimation. Framing these tasks\nwithin traditional machine learning pipelines is challenging, as supervision is\ntypically tied to individual datapoint. We propose meta-statistical learning, a\nframework inspired by multi-instance learning that reformulates statistical\ninference tasks as supervised learning problems. In this approach, entire\ndatasets are treated as single inputs to neural networks, which predict\ndistribution-level parameters. Transformer-based architectures, without\npositional encoding, provide a natural fit due to their permutation-invariance\nproperties. By training on large-scale synthetic datasets, meta-statistical\nmodels can leverage the scalability and optimization infrastructure of\nTransformer-based LLMs. We demonstrate the framework's versatility with\napplications in hypothesis testing and mutual information estimation, showing\nstrong performance, particularly for small datasets where traditional neural\nmethods struggle.", "paper_summary_zh": "\u8fd9\u9879\u5de5\u4f5c\u8868\u660e\uff0c\u63a8\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6210\u529f\u53d1\u5c55\u7684\u5de5\u5177\u548c\u539f\u5219\u53ef\u4ee5\u91cd\u65b0\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u7ea7\u522b\u4efb\u52a1\uff0c\u5176\u4e2d\u76ee\u6807\u662f\u9884\u6d4b\u6570\u636e\u751f\u6210\u5206\u5e03\u7684\u5c5e\u6027\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u6570\u636e\u70b9\u7684\u6807\u7b7e\u3002\u8fd9\u4e9b\u4efb\u52a1\u5305\u62ec\u7edf\u8ba1\u63a8\u65ad\u95ee\u9898\uff0c\u4f8b\u5982\u53c2\u6570\u4f30\u8ba1\u3001\u5047\u8bbe\u68c0\u9a8c\u6216\u4e92\u4fe1\u606f\u4f30\u8ba1\u3002\u5728\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u4e2d\u6784\u5efa\u8fd9\u4e9b\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u76d1\u7763\u901a\u5e38\u4e0e\u5355\u4e2a\u6570\u636e\u70b9\u76f8\u5173\u8054\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u5143\u7edf\u8ba1\u5b66\u4e60\uff0c\u8fd9\u662f\u4e00\u4e2a\u53d7\u591a\u5b9e\u4f8b\u5b66\u4e60\u542f\u53d1\u7684\u6846\u67b6\uff0c\u5b83\u5c06\u7edf\u8ba1\u63a8\u65ad\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\u3002\u5728\u6b64\u65b9\u6cd5\u4e2d\uff0c\u6574\u4e2a\u6570\u636e\u96c6\u88ab\u89c6\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u5355\u4e2a\u8f93\u5165\uff0c\u8be5\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u5206\u5e03\u7ea7\u522b\u53c2\u6570\u3002\u57fa\u4e8e Transformer \u7684\u67b6\u6784\u5728\u6ca1\u6709\u4f4d\u7f6e\u7f16\u7801\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u81ea\u7136\u62df\u5408\uff0c\u56e0\u4e3a\u5b83\u4eec\u5177\u6709\u7f6e\u6362\u4e0d\u53d8\u6027\u3002\u901a\u8fc7\u5728\u5927\u578b\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5143\u7edf\u8ba1\u6a21\u578b\u53ef\u4ee5\u5229\u7528\u57fa\u4e8e Transformer \u7684 LLM \u7684\u53ef\u6269\u5c55\u6027\u548c\u4f18\u5316\u57fa\u7840\u8bbe\u65bd\u3002\u6211\u4eec\u901a\u8fc7\u5728\u5047\u8bbe\u68c0\u9a8c\u548c\u4e92\u4fe1\u606f\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u591a\u529f\u80fd\u6027\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f20\u7edf\u795e\u7ecf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u5c0f\u578b\u6570\u636e\u96c6\u3002", "author": "Maxime Peyrard et.al.", "authors": "Maxime Peyrard, Kyunghyun Cho", "id": "2502.12088v1", "paper_url": "http://arxiv.org/abs/2502.12088v1", "repo": "null"}}