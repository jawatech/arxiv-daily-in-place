{"2410.17485": {"publish_time": "2024-10-23", "title": "VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning", "paper_summary": "Recent studies have augmented large language models (LLMs) with speech\ncapabilities, leading to the development of speech language models (SpeechLMs).\nEarlier SpeechLMs focused on single-turn speech-based question answering (QA),\nwhere user input comprised a speech context and a text question. More recent\nstudies have extended this to multi-turn conversations, though they often\nrequire complex, multi-stage supervised fine-tuning (SFT) with diverse data.\nAnother critical challenge with SpeechLMs is catastrophic forgetting-where\nmodels optimized for speech tasks suffer significant degradation in text-only\nperformance. To mitigate these issues, we propose a novel single-stage joint\nspeech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone.\nOur joint SFT combines text-only SFT data with three types of speech-related\ndata: speech recognition and translation, speech-based QA, and mixed-modal SFT.\nCompared to previous SpeechLMs with 7B or 13B parameters, our 3B model\ndemonstrates superior performance across various speech benchmarks while\npreserving the original capabilities on text-only tasks. Furthermore, our model\nshows emergent abilities of effectively handling previously unseen prompts and\ntasks, including multi-turn, mixed-modal inputs.", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u64f4\u589e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8a9e\u97f3\u80fd\u529b\uff0c\u9032\u800c\u958b\u767c\u51fa\u8a9e\u97f3\u8a9e\u8a00\u6a21\u578b (SpeechLM)\u3002\n\u65e9\u671f\u7684 SpeechLM \u5c08\u6ce8\u65bc\u55ae\u8f2a\u6b21\u57fa\u65bc\u8a9e\u97f3\u7684\u554f\u7b54 (QA)\uff0c\u5176\u4e2d\u4f7f\u7528\u8005\u8f38\u5165\u5305\u542b\u8a9e\u97f3\u8108\u7d61\u548c\u6587\u5b57\u554f\u984c\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u5c07\u6b64\u64f4\u5c55\u5230\u591a\u8f2a\u6b21\u5c0d\u8a71\uff0c\u5118\u7ba1\u5b83\u5011\u901a\u5e38\u9700\u8981\u8907\u96dc\u7684\u591a\u968e\u6bb5\u76e3\u7763\u5fae\u8abf (SFT) \u8207\u591a\u6a23\u5316\u7684\u8cc7\u6599\u3002\nSpeechLM \u7684\u53e6\u4e00\u500b\u95dc\u9375\u6311\u6230\u662f\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u5176\u4e2d\u91dd\u5c0d\u8a9e\u97f3\u4efb\u52d9\u6700\u4f73\u5316\u7684\u6a21\u578b\u5728\u7d14\u6587\u5b57\u6548\u80fd\u4e0a\u6703\u5927\u5e45\u4e0b\u964d\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u5728 LLM \u4e3b\u5e79\u7684\u4f4e\u79e9\u9069\u61c9 (LoRA) \u4e0a\u63a1\u7528\u65b0\u7a4e\u7684\u55ae\u968e\u6bb5\u806f\u5408\u8a9e\u97f3\u6587\u5b57 SFT \u65b9\u6cd5\u3002\n\u6211\u5011\u7684\u806f\u5408 SFT \u5c07\u7d14\u6587\u5b57 SFT \u8cc7\u6599\u8207\u4e09\u7a2e\u985e\u578b\u7684\u8a9e\u97f3\u76f8\u95dc\u8cc7\u6599\u7d50\u5408\uff1a\u8a9e\u97f3\u8fa8\u8b58\u8207\u7ffb\u8b6f\u3001\u57fa\u65bc\u8a9e\u97f3\u7684 QA \u4ee5\u53ca\u6df7\u5408\u6a21\u5f0f SFT\u3002\n\u8207\u5148\u524d\u5177\u6709 7B \u6216 13B \u53c3\u6578\u7684 SpeechLM \u76f8\u6bd4\uff0c\u6211\u5011\u7684 3B \u6a21\u578b\u5728\u5404\u7a2e\u8a9e\u97f3\u57fa\u6e96\u4e0a\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u540c\u6642\u4fdd\u7559\u7d14\u6587\u5b57\u4efb\u52d9\u7684\u539f\u59cb\u529f\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6a21\u578b\u5c55\u73fe\u51fa\u6709\u6548\u8655\u7406\u5148\u524d\u672a\u898b\u63d0\u793a\u548c\u4efb\u52d9\u7684\u65b0\u8208\u80fd\u529b\uff0c\u5305\u62ec\u591a\u8f2a\u6b21\u3001\u6df7\u5408\u6a21\u5f0f\u8f38\u5165\u3002", "author": "Yifan Peng et.al.", "authors": "Yifan Peng, Krishna C. Puvvada, Zhehuai Chen, Piotr Zelasko, He Huang, Kunal Dhawan, Ke Hu, Shinji Watanabe, Jagadeesh Balam, Boris Ginsburg", "id": "2410.17485v1", "paper_url": "http://arxiv.org/abs/2410.17485v1", "repo": "null"}}