{"2409.17565": {"publish_time": "2024-09-26", "title": "Pixel-Space Post-Training of Latent Diffusion Models", "paper_summary": "Latent diffusion models (LDMs) have made significant advancements in the\nfield of image generation in recent years. One major advantage of LDMs is their\nability to operate in a compressed latent space, allowing for more efficient\ntraining and deployment. However, despite these advantages, challenges with\nLDMs still remain. For example, it has been observed that LDMs often generate\nhigh-frequency details and complex compositions imperfectly. We hypothesize\nthat one reason for these flaws is due to the fact that all pre- and\npost-training of LDMs are done in latent space, which is typically $8 \\times 8$\nlower spatial-resolution than the output images. To address this issue, we\npropose adding pixel-space supervision in the post-training process to better\npreserve high-frequency details. Experimentally, we show that adding a\npixel-space objective significantly improves both supervised quality\nfine-tuning and preference-based post-training by a large margin on a\nstate-of-the-art DiT transformer and U-Net diffusion models in both visual\nquality and visual flaw metrics, while maintaining the same text alignment\nquality.", "paper_summary_zh": "\u6f5b\u5728\u64f4\u6563\u6a21\u578b (LDM) \u8fd1\u5e74\u4f86\u5728\u5f71\u50cf\u751f\u6210\u9818\u57df\u53d6\u5f97\u986f\u8457\u9032\u5c55\u3002LDM \u7684\u4e00\u5927\u512a\u52e2\u662f\u5b83\u5011\u80fd\u5920\u5728\u58d3\u7e2e\u6f5b\u5728\u7a7a\u9593\u4e2d\u904b\u4f5c\uff0c\u5f9e\u800c\u5be6\u73fe\u66f4\u6709\u6548\u7387\u7684\u8a13\u7df4\u548c\u90e8\u7f72\u3002\u7136\u800c\uff0c\u5118\u7ba1\u6709\u9019\u4e9b\u512a\u52e2\uff0cLDM \u4ecd\u5b58\u5728\u6311\u6230\u3002\u4f8b\u5982\uff0c\u4eba\u5011\u89c0\u5bdf\u5230 LDM \u7d93\u5e38\u4e0d\u5b8c\u7f8e\u5730\u751f\u6210\u9ad8\u983b\u7387\u7d30\u7bc0\u548c\u8907\u96dc\u7684\u7d44\u5408\u3002\u6211\u5011\u5047\u8a2d\u9019\u4e9b\u7f3a\u9677\u7684\u4e00\u500b\u539f\u56e0\u662f\uff0cLDM \u7684\u6240\u6709\u8a13\u7df4\u524d\u548c\u8a13\u7df4\u5f8c\u90fd\u662f\u5728\u6f5b\u5728\u7a7a\u9593\u4e2d\u5b8c\u6210\u7684\uff0c\u800c\u6f5b\u5728\u7a7a\u9593\u901a\u5e38\u6bd4\u8f38\u51fa\u5f71\u50cf\u7684\u7a7a\u9593\u89e3\u6790\u5ea6\u4f4e $8 \\times 8$\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u5728\u8a13\u7df4\u5f8c\u904e\u7a0b\u4e2d\u52a0\u5165\u50cf\u7d20\u7a7a\u9593\u76e3\u7763\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u9ad8\u983b\u7387\u7d30\u7bc0\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u8868\u660e\u52a0\u5165\u50cf\u7d20\u7a7a\u9593\u76ee\u6a19\u986f\u8457\u6539\u5584\u4e86\u76e3\u7763\u5f0f\u54c1\u8cea\u5fae\u8abf\u548c\u57fa\u65bc\u504f\u597d\u7684\u8a13\u7df4\u5f8c\uff0c\u5728\u6700\u5148\u9032\u7684 DiT Transformer \u548c U-Net \u64f4\u6563\u6a21\u578b\u4e2d\uff0c\u5728\u8996\u89ba\u54c1\u8cea\u548c\u8996\u89ba\u7f3a\u9677\u6307\u6a19\u4e0a\u90fd\u6709\u5927\u5e45\u9032\u6b65\uff0c\u540c\u6642\u7dad\u6301\u76f8\u540c\u7684\u6587\u5b57\u5c0d\u9f4a\u54c1\u8cea\u3002", "author": "Christina Zhang et.al.", "authors": "Christina Zhang, Simran Motwani, Matthew Yu, Ji Hou, Felix Juefei-Xu, Sam Tsai, Peter Vajda, Zijian He, Jialiang Wang", "id": "2409.17565v1", "paper_url": "http://arxiv.org/abs/2409.17565v1", "repo": "null"}}