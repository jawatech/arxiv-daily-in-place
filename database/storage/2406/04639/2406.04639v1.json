{"2406.04639": {"publish_time": "2024-06-07", "title": "Cooperative Meta-Learning with Gradient Augmentation", "paper_summary": "Model agnostic meta-learning (MAML) is one of the most widely used\ngradient-based meta-learning, consisting of two optimization loops: an inner\nloop and outer loop. MAML learns the new task from meta-initialization\nparameters with an inner update and finds the meta-initialization parameters in\nthe outer loop. In general, the injection of noise into the gradient of the\nmodel for augmenting the gradient is one of the widely used regularization\nmethods. In this work, we propose a novel cooperative meta-learning framework\ndubbed CML which leverages gradient-level regularization with gradient\naugmentation. We inject learnable noise into the gradient of the model for the\nmodel generalization. The key idea of CML is introducing the co-learner which\nhas no inner update but the outer loop update to augment gradients for finding\nbetter meta-initialization parameters. Since the co-learner does not update in\nthe inner loop, it can be easily deleted after meta-training. Therefore, CML\ninfers with only meta-learner without additional cost and performance\ndegradation. We demonstrate that CML is easily applicable to gradient-based\nmeta-learning methods and CML leads to increased performance in few-shot\nregression, few-shot image classification and few-shot node classification\ntasks. Our codes are at https://github.com/JJongyn/CML.", "paper_summary_zh": "\u6a21\u578b\u4e0d\u53ef\u77e5\u5143\u5b78\u7fd2 (MAML) \u662f\u4e00\u7a2e\u6700\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u65bc\u68af\u5ea6\u7684\u5143\u5b78\u7fd2\uff0c\u5305\u542b\u5169\u500b\u512a\u5316\u8ff4\u5708\uff1a\u4e00\u500b\u5167\u90e8\u8ff4\u5708\u548c\u4e00\u500b\u5916\u90e8\u8ff4\u5708\u3002MAML \u5f9e\u5143\u521d\u59cb\u5316\u53c3\u6578\u4e2d\u5b78\u7fd2\u65b0\u4efb\u52d9\uff0c\u4e26\u4f7f\u7528\u5167\u90e8\u66f4\u65b0\uff0c\u4e26\u5728\u5916\u90e8\u8ff4\u5708\u4e2d\u627e\u5230\u5143\u521d\u59cb\u5316\u53c3\u6578\u3002\u4e00\u822c\u4f86\u8aaa\uff0c\u5c07\u96dc\u8a0a\u6ce8\u5165\u6a21\u578b\u68af\u5ea6\u4ee5\u64f4\u5145\u68af\u5ea6\u662f\u5ee3\u6cdb\u4f7f\u7528\u7684\u6b63\u5247\u5316\u65b9\u6cd5\u4e4b\u4e00\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u5354\u4f5c\u5143\u5b78\u7fd2\u6846\u67b6\uff0c\u7a31\u70ba CML\uff0c\u5b83\u5229\u7528\u68af\u5ea6\u5c64\u7d1a\u6b63\u5247\u5316\u548c\u68af\u5ea6\u64f4\u5145\u3002\u6211\u5011\u5c07\u53ef\u5b78\u7fd2\u7684\u96dc\u8a0a\u6ce8\u5165\u6a21\u578b\u68af\u5ea6\u4e2d\uff0c\u4ee5\u9032\u884c\u6a21\u578b\u6cdb\u5316\u3002CML \u7684\u95dc\u9375\u601d\u60f3\u662f\u5f15\u5165\u5171\u540c\u5b78\u7fd2\u5668\uff0c\u5b83\u6c92\u6709\u5167\u90e8\u66f4\u65b0\uff0c\u4f46\u5916\u90e8\u8ff4\u5708\u66f4\u65b0\u53ef\u4ee5\u64f4\u5145\u68af\u5ea6\uff0c\u4ee5\u627e\u5230\u66f4\u597d\u7684\u5143\u521d\u59cb\u5316\u53c3\u6578\u3002\u7531\u65bc\u5171\u540c\u5b78\u7fd2\u5668\u4e0d\u6703\u5728\u5167\u90e8\u8ff4\u5708\u4e2d\u66f4\u65b0\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728\u5143\u8a13\u7df4\u5f8c\u8f15\u9b06\u522a\u9664\u5b83\u3002\u56e0\u6b64\uff0cCML \u63a8\u65b7\u50c5\u4f7f\u7528\u5143\u5b78\u7fd2\u5668\uff0c\u800c\u6c92\u6709\u984d\u5916\u7684\u6210\u672c\u548c\u6548\u80fd\u4e0b\u964d\u3002\u6211\u5011\u8b49\u660e CML \u5bb9\u6613\u61c9\u7528\u65bc\u57fa\u65bc\u68af\u5ea6\u7684\u5143\u5b78\u7fd2\u65b9\u6cd5\uff0c\u4e26\u4e14 CML \u53ef\u4ee5\u5728\u5c11\u6b21\u6578\u8ff4\u6b78\u3001\u5c11\u6b21\u6578\u5f71\u50cf\u5206\u985e\u548c\u5c11\u6b21\u6578\u7bc0\u9ede\u5206\u985e\u4efb\u52d9\u4e2d\u63d0\u5347\u6548\u80fd\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5728 https://github.com/JJongyn/CML\u3002", "author": "Jongyun Shin et.al.", "authors": "Jongyun Shin, Seunjin Han, Jangho Kim", "id": "2406.04639v1", "paper_url": "http://arxiv.org/abs/2406.04639v1", "repo": "https://github.com/jjongyn/cml"}}