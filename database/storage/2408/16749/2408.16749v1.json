{"2408.16749": {"publish_time": "2024-08-29", "title": "Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge", "paper_summary": "The United States has experienced a significant increase in violent\nextremism, prompting the need for automated tools to detect and limit the\nspread of extremist ideology online. This study evaluates the performance of\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformers (GPT) in detecting and classifying online domestic\nextremist posts. We collected social media posts containing \"far-right\" and\n\"far-left\" ideological keywords and manually labeled them as extremist or\nnon-extremist. Extremist posts were further classified into one or more of five\ncontributing elements of extremism based on a working definitional framework.\nThe BERT model's performance was evaluated based on training data size and\nknowledge transfer between categories. We also compared the performance of GPT\n3.5 and GPT 4 models using different prompts: na\\\"ive, layperson-definition,\nrole-playing, and professional-definition. Results showed that the best\nperforming GPT models outperformed the best performing BERT models, with more\ndetailed prompts generally yielding better results. However, overly complex\nprompts may impair performance. Different versions of GPT have unique\nsensitives to what they consider extremist. GPT 3.5 performed better at\nclassifying far-left extremist posts, while GPT 4 performed better at\nclassifying far-right extremist posts. Large language models, represented by\nGPT models, hold significant potential for online extremism classification\ntasks, surpassing traditional BERT models in a zero-shot setting. Future\nresearch should explore human-computer interactions in optimizing GPT models\nfor extremist detection and classification tasks to develop more efficient\n(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)\nmethods for identifying extremist content.", "paper_summary_zh": "<paragraph>\u7f8e\u570b\u7d93\u6b77\u4e86\u66b4\u529b\u6975\u7aef\u4e3b\u7fa9\u7684\u986f\u8457\u589e\u52a0\uff0c\u4fc3\u4f7f\u9700\u8981\u81ea\u52d5\u5316\u5de5\u5177\u4f86\u5075\u6e2c\u548c\u9650\u5236\u6975\u7aef\u4e3b\u7fa9\u610f\u8b58\u5f62\u614b\u5728\u7db2\u8def\u4e0a\u6563\u5e03\u3002\u672c\u7814\u7a76\u8a55\u4f30\u4e86\u4f86\u81ea Transformer \u7684\u96d9\u5411\u7de8\u78bc\u5668\u8868\u5fb5 (BERT) \u548c\u751f\u6210\u5f0f\u9810\u8a13\u7df4 Transformer (GPT) \u5728\u5075\u6e2c\u548c\u5206\u985e\u7dda\u4e0a\u570b\u5167\u6975\u7aef\u4e3b\u7fa9\u8cbc\u6587\u4e2d\u7684\u8868\u73fe\u3002\u6211\u5011\u6536\u96c6\u4e86\u5305\u542b\u300c\u6975\u53f3\u6d3e\u300d\u548c\u300c\u6975\u5de6\u6d3e\u300d\u610f\u8b58\u5f62\u614b\u95dc\u9375\u5b57\u7684\u793e\u7fa4\u5a92\u9ad4\u8cbc\u6587\uff0c\u4e26\u624b\u52d5\u6a19\u8a18\u5b83\u5011\u70ba\u6975\u7aef\u4e3b\u7fa9\u6216\u975e\u6975\u7aef\u4e3b\u7fa9\u3002\u6975\u7aef\u4e3b\u7fa9\u8cbc\u6587\u9032\u4e00\u6b65\u6839\u64da\u4e00\u500b\u5de5\u4f5c\u5b9a\u7fa9\u67b6\u69cb\u5206\u985e\u70ba\u6975\u7aef\u4e3b\u7fa9\u7684\u4e94\u500b\u4fc3\u6210\u5143\u7d20\u4e2d\u7684\u5176\u4e2d\u4e00\u500b\u6216\u591a\u500b\u3002BERT \u6a21\u578b\u7684\u8868\u73fe\u6839\u64da\u8a13\u7df4\u8cc7\u6599\u5927\u5c0f\u548c\u985e\u5225\u4e4b\u9593\u7684\u77e5\u8b58\u8f49\u79fb\u9032\u884c\u8a55\u4f30\u3002\u6211\u5011\u4e5f\u6bd4\u8f03\u4e86\u4f7f\u7528\u4e0d\u540c\u63d0\u793a\u7684 GPT 3.5 \u548c GPT 4 \u6a21\u578b\u7684\u8868\u73fe\uff1a\u5929\u771f\u7684\u3001\u5916\u884c\u4eba\u7684\u5b9a\u7fa9\u3001\u89d2\u8272\u626e\u6f14\u548c\u5c08\u696d\u5b9a\u7fa9\u3002\u7d50\u679c\u986f\u793a\u8868\u73fe\u6700\u597d\u7684 GPT \u6a21\u578b\u512a\u65bc\u8868\u73fe\u6700\u597d\u7684 BERT \u6a21\u578b\uff0c\u800c\u66f4\u8a73\u7d30\u7684\u63d0\u793a\u901a\u5e38\u6703\u7522\u751f\u66f4\u597d\u7684\u7d50\u679c\u3002\u7136\u800c\uff0c\u904e\u65bc\u8907\u96dc\u7684\u63d0\u793a\u53ef\u80fd\u6703\u640d\u5bb3\u8868\u73fe\u3002\u4e0d\u540c\u7248\u672c\u7684 GPT \u5c0d\u5b83\u5011\u8a8d\u70ba\u6975\u7aef\u7684\u5167\u5bb9\u6709\u7368\u7279\u7684\u654f\u611f\u6027\u3002GPT 3.5 \u5728\u5206\u985e\u6975\u5de6\u6d3e\u6975\u7aef\u4e3b\u7fa9\u8cbc\u6587\u65b9\u9762\u8868\u73fe\u5f97\u66f4\u597d\uff0c\u800c GPT 4 \u5728\u5206\u985e\u6975\u53f3\u6d3e\u6975\u7aef\u4e3b\u7fa9\u8cbc\u6587\u65b9\u9762\u8868\u73fe\u5f97\u66f4\u597d\u3002\u4ee5 GPT \u6a21\u578b\u70ba\u4ee3\u8868\u7684\u5927\u8a9e\u8a00\u6a21\u578b\u5728\u7dda\u4e0a\u6975\u7aef\u4e3b\u7fa9\u5206\u985e\u4efb\u52d9\u4e2d\u5177\u6709\u986f\u8457\u7684\u6f5b\u529b\uff0c\u5728\u96f6\u6b21\u5b78\u7fd2\u8a2d\u7f6e\u4e2d\u8d85\u8d8a\u4e86\u50b3\u7d71\u7684 BERT \u6a21\u578b\u3002\u672a\u4f86\u7684\u7814\u7a76\u61c9\u63a2\u7d22\u4eba\u6a5f\u4e92\u52d5\uff0c\u4ee5\u6700\u4f73\u5316 GPT \u6a21\u578b\uff0c\u7528\u65bc\u6975\u7aef\u4e3b\u7fa9\u5075\u6e2c\u548c\u5206\u985e\u4efb\u52d9\uff0c\u4ee5\u958b\u767c\u66f4\u6709\u6548\u7387\uff08\u4f8b\u5982\uff0c\u66f4\u5feb\u3001\u66f4\u7701\u529b\uff09\u548c\u6709\u6548\uff08\u4f8b\u5982\uff0c\u66f4\u5c11\u7684\u932f\u8aa4\u6216\u5931\u8aa4\uff09\u7684\u65b9\u6cd5\u4f86\u8b58\u5225\u6975\u7aef\u4e3b\u7fa9\u5167\u5bb9\u3002</paragraph>", "author": "Beidi Dong et.al.", "authors": "Beidi Dong, Jin R. Lee, Ziwei Zhu, Balassubramanian Srinivasan", "id": "2408.16749v1", "paper_url": "http://arxiv.org/abs/2408.16749v1", "repo": "null"}}