{"2407.05975": {"publish_time": "2024-07-08", "title": "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "paper_summary": "Large Language Models~(LLMs) demonstrate remarkable translation capabilities\nin high-resource language tasks, yet their performance in low-resource\nlanguages is hindered by insufficient multilingual data during pre-training. To\naddress this, we dedicate 35,000 A100-SXM4-80GB GPU hours in conducting\nextensive multilingual continual pre-training on the LLaMA series models,\nenabling translation support across more than 100 languages. Through a\ncomprehensive analysis of training strategies, such as vocabulary expansion and\ndata augmentation, we develop LLaMAX. Remarkably, without sacrificing its\ngeneralization ability, LLaMAX achieves significantly higher translation\nperformance compared to existing open-source LLMs~(by more than 10 spBLEU\npoints) and performs on-par with specialized translation model~(M2M-100-12B) on\nthe Flores-101 benchmark. Extensive experiments indicate that LLaMAX can serve\nas a robust multilingual foundation model. The\ncode~\\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} and\nmodels~\\footnote{\\url{https://huggingface.co/LLaMAX/.}} are publicly available.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b~(LLM) \u5728\u9ad8\u8cc7\u6e90\u8a9e\u8a00\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u5353\u8d8a\u7684\u7ffb\u8b6f\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u5728\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u4e2d\u7684\u8868\u73fe\u53d7\u5230\u9810\u8a13\u7df4\u671f\u9593\u591a\u8a9e\u8a00\u6578\u64da\u4e0d\u8db3\u7684\u963b\u7919\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u6295\u5165 35,000 \u5c0f\u6642\u7684 A100-SXM4-80GB GPU \u6642\u9593\uff0c\u5c0d LLaMA \u7cfb\u5217\u6a21\u578b\u9032\u884c\u5ee3\u6cdb\u7684\u591a\u8a9e\u8a00\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u652f\u63f4\u8d85\u904e 100 \u7a2e\u8a9e\u8a00\u7684\u7ffb\u8b6f\u3002\u900f\u904e\u5c0d\u8a13\u7df4\u7b56\u7565\u9032\u884c\u5168\u9762\u5206\u6790\uff0c\u4f8b\u5982\u8a5e\u5f59\u64f4\u5145\u548c\u8cc7\u6599\u64f4\u5145\uff0c\u6211\u5011\u958b\u767c\u4e86 LLaMAX\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cLLaMAX \u5728\u4e0d\u72a7\u7272\u5176\u6cdb\u5316\u80fd\u529b\u7684\u60c5\u6cc1\u4e0b\uff0c\u8207\u73fe\u6709\u7684\u958b\u653e\u539f\u59cb\u78bc LLM \u76f8\u6bd4\uff0c\u5be6\u73fe\u4e86\u986f\u8457\u66f4\u9ad8\u7684\u7ffb\u8b6f\u6027\u80fd~(\u9ad8\u51fa 10 \u500b spBLEU \u9ede)\uff0c\u4e26\u5728 Flores-101 \u57fa\u6e96\u4e0a\u8207\u5c08\u696d\u7ffb\u8b6f\u6a21\u578b~(M2M-100-12B) \u76f8\u7576\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8868\u660e\uff0cLLaMAX \u53ef\u4ee5\u4f5c\u70ba\u4e00\u500b\u5f37\u5927\u7684\u591a\u8a9e\u8a00\u57fa\u790e\u6a21\u578b\u3002\u7a0b\u5f0f\u78bc~\\footnote{\\url{https://github.com/CONE-MT/LLaMAX/.}} \u548c\u6a21\u578b~\\footnote{\\url{https://huggingface.co/LLaMAX/.}} \u7686\u5df2\u516c\u958b\u3002", "author": "Yinquan Lu et.al.", "authors": "Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, Fei Yuan", "id": "2407.05975v1", "paper_url": "http://arxiv.org/abs/2407.05975v1", "repo": "https://github.com/cone-mt/llamax"}}