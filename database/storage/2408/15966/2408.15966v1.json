{"2408.15966": {"publish_time": "2024-08-28", "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding", "paper_summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.", "paper_summary_zh": "<paragraph>\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7406\u89e3 3D \u7269\u7406\u4e16\u754c\u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u7531\u65bc\u7f3a\u4e4f\u5927\u898f\u6a21\u7684 3D-\u6587\u5b57\u5c0d\u8cc7\u6599\u96c6\uff0cLLM \u7684\u6210\u529f\u5c1a\u672a\u5728 3D \u7406\u89e3\u4e2d\u8907\u88fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u91cd\u65b0\u601d\u8003\u9019\u500b\u554f\u984c\u4e26\u63d0\u51fa\u4e00\u500b\u65b0\u4efb\u52d9\uff1a3D \u8cc7\u6599\u6709\u6548\u9ede\u8a9e\u8a00\u7406\u89e3\u3002\u76ee\u6a19\u662f\u8b93 LLM \u80fd\u5920\u5728\u6700\u5c11\u7684 3D \u9ede\u96f2\u548c\u6587\u5b57\u8cc7\u6599\u5c0d\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u7a69\u5065\u7684 3D \u7269\u4ef6\u7406\u89e3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u4efb\u52d9\uff0c\u6211\u5011\u5f15\u5165\u4e86 GreenPLM\uff0c\u5b83\u5229\u7528\u66f4\u591a\u6587\u5b57\u8cc7\u6599\u4f86\u5f4c\u88dc 3D \u8cc7\u6599\u7684\u4e0d\u8db3\u3002\u9996\u5148\uff0c\u53d7\u4f7f\u7528 CLIP \u5c07\u5f71\u50cf\u548c\u6587\u5b57\u5c0d\u9f4a\u7684\u555f\u767c\uff0c\u6211\u5011\u5229\u7528\u9810\u5148\u8a13\u7df4\u597d\u7684\u9ede\u96f2\u6587\u5b57\u7de8\u78bc\u5668\u5c07 3D \u9ede\u96f2\u7a7a\u9593\u6620\u5c04\u5230\u6587\u5b57\u7a7a\u9593\u3002\u9019\u500b\u5c0d\u61c9\u8b93\u6211\u5011\u53ef\u4ee5\u7121\u7e2b\u5730\u5c07\u6587\u5b57\u7a7a\u9593\u8207 LLM \u9023\u63a5\u8d77\u4f86\u3002\u4e00\u65e6\u5efa\u7acb\u4e86\u9ede\u6587\u5b57 LLM \u9023\u63a5\uff0c\u6211\u5011\u9032\u4e00\u6b65\u900f\u904e\u64f4\u5c55\u4e2d\u9593\u6587\u5b57\u7a7a\u9593\u4f86\u589e\u5f37\u6587\u5b57 LLM \u5c0d\u9f4a\uff0c\u5f9e\u800c\u6e1b\u5c11\u5c0d 3D \u9ede\u96f2\u8cc7\u6599\u7684\u4f9d\u8cf4\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u751f\u6210\u4e86 600 \u842c\u500b 3D \u7269\u4ef6\u7684\u81ea\u7531\u6587\u5b57\u63cf\u8ff0\uff0c\u4e26\u8a2d\u8a08\u4e86\u4e00\u500b\u4e09\u968e\u6bb5\u8a13\u7df4\u7b56\u7565\u4f86\u5e6b\u52a9 LLM \u66f4\u6df1\u5165\u5730\u63a2\u7d22\u4e0d\u540c\u6a21\u614b\u4e4b\u9593\u7684\u5167\u5728\u806f\u7e6b\u3002\u70ba\u4e86\u5be6\u73fe\u6709\u6548\u7684\u6a21\u614b\u5c0d\u9f4a\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u96f6\u53c3\u6578\u8de8\u6ce8\u610f\u529b\u6a21\u7d44\u9032\u884c\u4ee3\u78bc\u6c60\u5316\u3002\u5927\u91cf\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cGreenPLM \u53ea\u9700\u8981\u73fe\u6709\u6700\u5148\u9032\u6a21\u578b\u6240\u4f7f\u7528 3D \u8a13\u7df4\u8cc7\u6599\u7684 12%\uff0c\u5c31\u80fd\u5be6\u73fe\u512a\u7570\u7684 3D \u7406\u89e3\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cGreenPLM \u9084\u4f7f\u7528\u7d14\u6587\u5b57\u8cc7\u6599\u9054\u5230\u4e86\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u548c\u6b0a\u91cd\u53ef\u5728\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1ahttps://github.com/TangYuan96/GreenPLM\u3002</paragraph>", "author": "Yuan Tang et.al.", "authors": "Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Jinfeng Xu, Yixue Hao, Long Hu, Min Chen", "id": "2408.15966v1", "paper_url": "http://arxiv.org/abs/2408.15966v1", "repo": "null"}}