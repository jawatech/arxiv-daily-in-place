{"2406.17654": {"publish_time": "2024-06-25", "title": "MDHA: Multi-Scale Deformable Transformer with Hybrid Anchors for Multi-View 3D Object Detection", "paper_summary": "Multi-view 3D object detection is a crucial component of autonomous driving\nsystems. Contemporary query-based methods primarily depend either on\ndataset-specific initialization of 3D anchors, introducing bias, or utilize\ndense attention mechanisms, which are computationally inefficient and\nunscalable. To overcome these issues, we present MDHA, a novel sparse\nquery-based framework, which constructs adaptive 3D output proposals using\nhybrid anchors from multi-view, multi-scale input. Fixed 2D anchors are\ncombined with depth predictions to form 2.5D anchors, which are projected to\nobtain 3D proposals. To ensure high efficiency, our proposed Anchor Encoder\nperforms sparse refinement and selects the top-k anchors and features.\nMoreover, while existing multi-view attention mechanisms rely on projecting\nreference points to multiple images, our novel Circular Deformable Attention\nmechanism only projects to a single image but allows reference points to\nseamlessly attend to adjacent images, improving efficiency without compromising\non performance. On the nuScenes val set, it achieves 46.4% mAP and 55.0% NDS\nwith a ResNet101 backbone. MDHA significantly outperforms the baseline, where\nanchor proposals are modelled as learnable embeddings.", "paper_summary_zh": "\u591a\u8996\u5716 3D \u7269\u4ef6\u5075\u6e2c\u662f\u81ea\u52d5\u99d5\u99db\u7cfb\u7d71\u7684\u95dc\u9375\u7d44\u6210\u90e8\u5206\u3002\u7576\u4ee3\u57fa\u65bc\u67e5\u8a62\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8cf4\u65bc 3D \u9328\u9ede\u7684\u7279\u5b9a\u8cc7\u6599\u96c6\u521d\u59cb\u5316\uff0c\u5f15\u5165\u504f\u5dee\uff0c\u6216\u5229\u7528\u5bc6\u96c6\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u9019\u5728\u8a08\u7b97\u4e0a\u6548\u7387\u4f4e\u4e0b\u4e14\u7121\u6cd5\u64f4\u5c55\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 MDHA\uff0c\u4e00\u500b\u5275\u65b0\u7684\u7a00\u758f\u57fa\u65bc\u67e5\u8a62\u7684\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u4f86\u81ea\u591a\u8996\u5716\u3001\u591a\u5c3a\u5ea6\u8f38\u5165\u7684\u6df7\u5408\u9328\u9ede\u69cb\u5efa\u81ea\u9069\u61c9 3D \u8f38\u51fa\u5efa\u8b70\u3002\u56fa\u5b9a\u7684 2D \u9328\u9ede\u8207\u6df1\u5ea6\u9810\u6e2c\u76f8\u7d50\u5408\uff0c\u5f62\u6210 2.5D \u9328\u9ede\uff0c\u4e26\u6295\u5f71\u4ee5\u7372\u5f97 3D \u5efa\u8b70\u3002\u70ba\u4e86\u78ba\u4fdd\u9ad8\u6548\u7387\uff0c\u6211\u5011\u63d0\u51fa\u7684 Anchor Encoder \u57f7\u884c\u7a00\u758f\u8abf\u6574\uff0c\u4e26\u9078\u64c7\u524d k \u500b\u9328\u9ede\u548c\u7279\u5fb5\u3002\u6b64\u5916\uff0c\u96d6\u7136\u73fe\u6709\u7684\u591a\u8996\u5716\u6ce8\u610f\u529b\u6a5f\u5236\u4f9d\u8cf4\u65bc\u5c07\u53c3\u8003\u9ede\u6295\u5f71\u5230\u591a\u500b\u5f71\u50cf\uff0c\u4f46\u6211\u5011\u5275\u65b0\u7684 Circular Deformable Attention \u6a5f\u5236\u50c5\u6295\u5f71\u5230\u55ae\u4e00\u5f71\u50cf\uff0c\u4f46\u5141\u8a31\u53c3\u8003\u9ede\u7121\u7e2b\u5730\u95dc\u6ce8\u76f8\u9130\u5f71\u50cf\uff0c\u5f9e\u800c\u5728\u4e0d\u5f71\u97ff\u6548\u80fd\u7684\u60c5\u6cc1\u4e0b\u63d0\u9ad8\u6548\u7387\u3002\u5728 nuScenes val \u8a2d\u5b9a\u4e2d\uff0c\u5b83\u4f7f\u7528 ResNet101 \u4e3b\u5e79\u9054\u5230\u4e86 46.4% mAP \u548c 55.0% NDS\u3002MDHA \u660e\u986f\u512a\u65bc\u57fa\u7dda\uff0c\u5176\u4e2d\u9328\u9ede\u5efa\u8b70\u88ab\u5efa\u6a21\u70ba\u53ef\u5b78\u7fd2\u7684\u5d4c\u5165\u3002", "author": "Michelle Adeline et.al.", "authors": "Michelle Adeline, Junn Yong Loo, Vishnu Monn Baskaran", "id": "2406.17654v1", "paper_url": "http://arxiv.org/abs/2406.17654v1", "repo": "null"}}