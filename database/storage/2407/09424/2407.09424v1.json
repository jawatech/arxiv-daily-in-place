{"2407.09424": {"publish_time": "2024-07-12", "title": "TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models", "paper_summary": "Large Language Models (LLMs) have the potential to revolutionize the Sixth\nGeneration (6G) communication networks. However, current mainstream LLMs\ngenerally lack the specialized knowledge in telecom domain. In this paper, for\nthe first time, we propose a pipeline to adapt any general purpose LLMs to a\ntelecom-specific LLMs. We collect and build telecom-specific pre-train dataset,\ninstruction dataset, preference dataset to perform continual pre-training,\ninstruct tuning and alignment tuning respectively. Besides, due to the lack of\nwidely accepted evaluation benchmarks in telecom domain, we extend existing\nevaluation benchmarks and proposed three new benchmarks, namely, Telecom Math\nModeling, Telecom Open QnA and Telecom Code Tasks. These new benchmarks provide\na holistic evaluation of the capabilities of LLMs including math modeling,\nOpen-Ended question answering, code generation, infilling, summarization and\nanalysis in telecom domain. Our fine-tuned LLM TelecomGPT outperforms state of\nthe art (SOTA) LLMs including GPT-4, Llama-3 and Mistral in Telecom Math\nModeling benchmark significantly and achieve comparable performance in various\nevaluation benchmarks such as TeleQnA, 3GPP technical documents classification,\ntelecom code summary and generation and infilling.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6709\u53ef\u80fd\u9769\u65b0\u7b2c\u516d\u4ee3 (6G) \u901a\u8a0a\u7db2\u8def\u3002\u7136\u800c\uff0c\u76ee\u524d\u4e3b\u6d41\u7684 LLM \u901a\u5e38\u7f3a\u4e4f\u96fb\u4fe1\u9818\u57df\u7684\u5c08\u696d\u77e5\u8b58\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u6b21\u63d0\u51fa\u4e00\u500b\u7ba1\u9053\uff0c\u4ee5\u5c07\u4efb\u4f55\u901a\u7528 LLM \u9069\u61c9\u5230\u96fb\u4fe1\u5c08\u7528 LLM\u3002\u6211\u5011\u6536\u96c6\u4e26\u5efa\u7acb\u96fb\u4fe1\u5c08\u7528\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u3001\u6307\u4ee4\u8cc7\u6599\u96c6\u3001\u504f\u597d\u8cc7\u6599\u96c6\uff0c\u5206\u5225\u57f7\u884c\u6301\u7e8c\u9810\u8a13\u7df4\u3001\u6307\u4ee4\u5fae\u8abf\u548c\u5c0d\u9f4a\u5fae\u8abf\u3002\u6b64\u5916\uff0c\u7531\u65bc\u7f3a\u4e4f\u5ee3\u6cdb\u63a5\u53d7\u7684\u96fb\u4fe1\u9818\u57df\u8a55\u4f30\u57fa\u6e96\uff0c\u6211\u5011\u64f4\u5145\u73fe\u6709\u7684\u8a55\u4f30\u57fa\u6e96\uff0c\u4e26\u63d0\u51fa\u4e09\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u5373\u96fb\u4fe1\u6578\u5b78\u5efa\u6a21\u3001\u96fb\u4fe1\u958b\u653e\u554f\u7b54\u548c\u96fb\u4fe1\u7a0b\u5f0f\u78bc\u4efb\u52d9\u3002\u9019\u4e9b\u65b0\u57fa\u6e96\u63d0\u4f9b\u5c0d LLM \u80fd\u529b\u7684\u6574\u9ad4\u8a55\u4f30\uff0c\u5305\u62ec\u96fb\u4fe1\u9818\u57df\u7684\u6578\u5b78\u5efa\u6a21\u3001\u958b\u653e\u5f0f\u554f\u984c\u89e3\u7b54\u3001\u7a0b\u5f0f\u78bc\u751f\u6210\u3001\u586b\u7a7a\u3001\u6458\u8981\u548c\u5206\u6790\u3002\u6211\u5011\u5fae\u8abf\u5f8c\u7684 LLM TelecomGPT \u5728\u96fb\u4fe1\u6578\u5b78\u5efa\u6a21\u57fa\u6e96\u4e0a\u986f\u8457\u512a\u65bc\u6700\u5148\u9032 (SOTA) \u7684 LLM\uff0c\u5305\u62ec GPT-4\u3001Llama-3 \u548c Mistral\uff0c\u4e26\u5728\u5404\u7a2e\u8a55\u4f30\u57fa\u6e96\uff08\u4f8b\u5982 TeleQnA\u30013GPP \u6280\u8853\u6587\u4ef6\u5206\u985e\u3001\u96fb\u4fe1\u7a0b\u5f0f\u78bc\u6458\u8981\u548c\u751f\u6210\uff0c\u4ee5\u53ca\u586b\u7a7a\uff09\u4e2d\u9054\u5230\u76f8\u7576\u7684\u6548\u80fd\u3002", "author": "Hang Zou et.al.", "authors": "Hang Zou, Qiyang Zhao, Yu Tian, Lina Bariah, Faouzi Bader, Thierry Lestable, Merouane Debbah", "id": "2407.09424v1", "paper_url": "http://arxiv.org/abs/2407.09424v1", "repo": "null"}}