{"2407.17827": {"publish_time": "2024-07-25", "title": "Unified Lexical Representation for Interpretable Visual-Language Alignment", "paper_summary": "Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's\ngroundbreaking work. Although CLIP performs well, the typical direct latent\nfeature alignment lacks clarity in its representation and similarity scores. On\nthe other hand, lexical representation, a vector whose element represents the\nsimilarity between the sample and a word from the vocabulary, is a natural\nsparse representation and interpretable, providing exact matches for individual\nwords. However, lexical representations is difficult to learn due to no\nground-truth supervision and false-discovery issues, and thus requires complex\ndesign to train effectively. In this paper, we introduce LexVLA, a more\ninterpretable VLA framework by learning a unified lexical representation for\nboth modalities without complex design. We use DINOv2 as our visual model for\nits local-inclined features and Llama 2, a generative language model, to\nleverage its in-context lexical prediction ability. To avoid the false\ndiscovery, we propose an overuse penalty to refrain the lexical representation\nfrom falsely frequently activating meaningless words. We demonstrate that these\ntwo pre-trained uni-modal models can be well-aligned by fine-tuning on modest\nmulti-modal dataset and avoid intricate training configurations. On cross-modal\nretrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,\noutperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those\ntrained from scratch on even bigger datasets (e.g., 1.1B data, including\nCC-12M). We conduct extensive experiments to analyze LexVLA.", "paper_summary_zh": "\u81ea CLIP \u7684\u958b\u5275\u6027\u5de5\u4f5c\u4ee5\u4f86\uff0c\u8996\u89ba\u8a9e\u8a00\u5c0d\u9f4a (VLA) \u7372\u5f97\u4e86\u8a31\u591a\u95dc\u6ce8\u3002\u5118\u7ba1 CLIP \u8868\u73fe\u826f\u597d\uff0c\u4f46\u5178\u578b\u7684\u76f4\u63a5\u6f5b\u5728\u7279\u5fb5\u5c0d\u9f4a\u7f3a\u4e4f\u5176\u8868\u793a\u548c\u76f8\u4f3c\u6027\u8a55\u5206\u7684\u6e05\u6670\u5ea6\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u8a5e\u5f59\u8868\u793a\u662f\u4e00\u500b\u5411\u91cf\uff0c\u5176\u5143\u7d20\u8868\u793a\u6a23\u672c\u8207\u8a5e\u5f59\u4e2d\u4e00\u500b\u55ae\u8a5e\u4e4b\u9593\u7684\u76f8\u4f3c\u6027\uff0c\u662f\u4e00\u7a2e\u81ea\u7136\u7684\u7a00\u758f\u8868\u793a\uff0c\u4e26\u4e14\u53ef\u4ee5\u89e3\u91cb\uff0c\u70ba\u500b\u5225\u55ae\u8a5e\u63d0\u4f9b\u6e96\u78ba\u7684\u5339\u914d\u3002\u7136\u800c\uff0c\u7531\u65bc\u6c92\u6709\u771f\u5be6\u7684\u76e3\u7763\u548c\u865b\u5047\u767c\u73fe\u554f\u984c\uff0c\u8a5e\u5f59\u8868\u793a\u96e3\u4ee5\u5b78\u7fd2\uff0c\u56e0\u6b64\u9700\u8981\u8907\u96dc\u7684\u8a2d\u8a08\u624d\u80fd\u6709\u6548\u8a13\u7df4\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 LexVLA\uff0c\u9019\u662f\u4e00\u500b\u66f4\u5177\u53ef\u89e3\u91cb\u6027\u7684 VLA \u6846\u67b6\uff0c\u901a\u904e\u5b78\u7fd2\u4e00\u500b\u7d71\u4e00\u7684\u8a5e\u5f59\u8868\u793a\u4f86\u8868\u793a\u5169\u7a2e\u6a21\u5f0f\uff0c\u800c\u7121\u9700\u8907\u96dc\u7684\u8a2d\u8a08\u3002\u6211\u5011\u4f7f\u7528 DINOv2 \u4f5c\u70ba\u6211\u5011\u7684\u8996\u89ba\u6a21\u578b\uff0c\u56e0\u70ba\u5b83\u5177\u6709\u5c40\u90e8\u50be\u659c\u7684\u7279\u5fb5\uff0c\u4ee5\u53ca Llama 2\uff0c\u4e00\u500b\u751f\u6210\u8a9e\u8a00\u6a21\u578b\uff0c\u4ee5\u5229\u7528\u5176\u4e0a\u4e0b\u6587\u8a5e\u5f59\u9810\u6e2c\u80fd\u529b\u3002\u70ba\u4e86\u907f\u514d\u865b\u5047\u767c\u73fe\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u904e\u5ea6\u4f7f\u7528\u61f2\u7f70\uff0c\u4ee5\u9632\u6b62\u8a5e\u5f59\u8868\u793a\u932f\u8aa4\u5730\u983b\u7e41\u6fc0\u6d3b\u7121\u610f\u7fa9\u7684\u8a5e\u3002\u6211\u5011\u8b49\u660e\u4e86\u9019\u5169\u500b\u9810\u5148\u8a13\u7df4\u7684\u55ae\u6a21\u614b\u6a21\u578b\u53ef\u4ee5\u901a\u904e\u5fae\u8abf\u9069\u5ea6\u7684\u591a\u6a21\u614b\u6578\u64da\u96c6\u4e26\u907f\u514d\u8907\u96dc\u7684\u8a13\u7df4\u914d\u7f6e\u4f86\u5f88\u597d\u5730\u5c0d\u9f4a\u3002\u5728\u8de8\u6a21\u614b\u6aa2\u7d22\u57fa\u6e96\u4e0a\uff0c\u5728 CC-12M \u591a\u6a21\u614b\u6578\u64da\u96c6\u4e0a\u8a13\u7df4\u7684 LexVLA \u512a\u65bc\u5728\u8f03\u5927\u6578\u64da\u96c6\uff08\u4f8b\u5982 YFCC15M\uff09\u4e0a\u9032\u884c\u5fae\u8abf\u7684\u57fa\u7dda\uff0c\u4ee5\u53ca\u5f9e\u66f4\u5927\u7684\u6578\u64da\u96c6\uff08\u4f8b\u5982 1.1B \u6578\u64da\uff0c\u5305\u62ec CC-12M\uff09\u4e2d\u5f9e\u982d\u8a13\u7df4\u7684\u57fa\u7dda\u3002\u6211\u5011\u9032\u884c\u4e86\u5927\u91cf\u7684\u5be6\u9a57\u4f86\u5206\u6790 LexVLA\u3002", "author": "Yifan Li et.al.", "authors": "Yifan Li, Yikai Wang, Yanwei Fu, Dongyu Ru, Zheng Zhang, Tong He", "id": "2407.17827v1", "paper_url": "http://arxiv.org/abs/2407.17827v1", "repo": "null"}}