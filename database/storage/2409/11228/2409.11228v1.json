{"2409.11228": {"publish_time": "2024-09-17", "title": "Learning Source Disentanglement in Neural Audio Codec", "paper_summary": "Neural audio codecs have significantly advanced audio compression by\nefficiently converting continuous audio signals into discrete tokens. These\ncodecs preserve high-quality sound and enable sophisticated sound generation\nthrough generative models trained on these tokens. However, existing neural\ncodec models are typically trained on large, undifferentiated audio datasets,\nneglecting the essential discrepancies between sound domains like speech,\nmusic, and environmental sound effects. This oversight complicates data\nmodeling and poses additional challenges to the controllability of sound\ngeneration. To tackle these issues, we introduce the Source-Disentangled Neural\nAudio Codec (SD-Codec), a novel approach that combines audio coding and source\nseparation. By jointly learning audio resynthesis and separation, SD-Codec\nexplicitly assigns audio signals from different domains to distinct codebooks,\nsets of discrete representations. Experimental results indicate that SD-Codec\nnot only maintains competitive resynthesis quality but also, supported by the\nseparation results, demonstrates successful disentanglement of different\nsources in the latent space, thereby enhancing interpretability in audio codec\nand providing potential finer control over the audio generation process.", "paper_summary_zh": "\u795e\u7d93\u97f3\u8a0a\u7de8\u89e3\u78bc\u5668\u900f\u904e\u6709\u6548\u7387\u5730\u5c07\u9023\u7e8c\u97f3\u8a0a\u8a0a\u865f\u8f49\u63db\u70ba\u96e2\u6563\u4ee3\u5e63\uff0c\u986f\u8457\u63d0\u5347\u97f3\u8a0a\u58d3\u7e2e\u80fd\u529b\u3002\u9019\u4e9b\u7de8\u89e3\u78bc\u5668\u4fdd\u7559\u9ad8\u54c1\u8cea\u97f3\u8a0a\uff0c\u4e26\u80fd\u900f\u904e\u8a13\u7df4\u9019\u4e9b\u4ee3\u5e63\u7684\u751f\u6210\u6a21\u578b\u4f86\u9032\u884c\u7cbe\u5bc6\u7684\u97f3\u8a0a\u7522\u751f\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u795e\u7d93\u7de8\u89e3\u78bc\u5668\u6a21\u578b\u901a\u5e38\u8a13\u7df4\u65bc\u5927\u578b\u3001\u672a\u5340\u5206\u7684\u97f3\u8a0a\u8cc7\u6599\u96c6\u4e0a\uff0c\u5ffd\u7565\u4e86\u8a9e\u97f3\u3001\u97f3\u6a02\u548c\u74b0\u5883\u97f3\u6548\u7b49\u97f3\u8a0a\u9818\u57df\u9593\u7684\u672c\u8cea\u5dee\u7570\u3002\u9019\u7a2e\u758f\u5ffd\u4f7f\u5f97\u8cc7\u6599\u5efa\u6a21\u8b8a\u5f97\u8907\u96dc\uff0c\u4e26\u5c0d\u97f3\u8a0a\u7522\u751f\u7684\u53ef\u63a7\u6027\u9020\u6210\u984d\u5916\u7684\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4f86\u6e90\u5206\u96e2\u795e\u7d93\u97f3\u8a0a\u7de8\u89e3\u78bc\u5668 (SD-Codec)\uff0c\u9019\u662f\u4e00\u7a2e\u7d50\u5408\u97f3\u8a0a\u7de8\u78bc\u548c\u4f86\u6e90\u5206\u96e2\u7684\u65b0\u7a4e\u65b9\u6cd5\u3002\u900f\u904e\u5171\u540c\u5b78\u7fd2\u97f3\u8a0a\u91cd\u65b0\u5408\u6210\u548c\u5206\u96e2\uff0cSD-Codec \u660e\u78ba\u5730\u5c07\u4e0d\u540c\u9818\u57df\u7684\u97f3\u8a0a\u8a0a\u865f\u5206\u914d\u7d66\u4e0d\u540c\u7684\u4ee3\u78bc\u7c3f\uff0c\u5373\u96e2\u6563\u8868\u793a\u96c6\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cSD-Codec \u4e0d\u50c5\u7dad\u6301\u4e86\u5177\u7af6\u722d\u529b\u7684\u91cd\u65b0\u5408\u6210\u54c1\u8cea\uff0c\u800c\u4e14\u5728\u5206\u96e2\u7d50\u679c\u7684\u652f\u63f4\u4e0b\uff0c\u8b49\u660e\u6210\u529f\u5730\u5c07\u6f5b\u5728\u7a7a\u9593\u4e2d\u4e0d\u540c\u7684\u4f86\u6e90\u5206\u96e2\u51fa\u4f86\uff0c\u5f9e\u800c\u589e\u5f37\u4e86\u97f3\u8a0a\u7de8\u89e3\u78bc\u5668\u7684\u53ef\u89e3\u91cb\u6027\uff0c\u4e26\u63d0\u4f9b\u4e86\u5c0d\u97f3\u8a0a\u7522\u751f\u904e\u7a0b\u66f4\u7cbe\u7d30\u7684\u63a7\u5236\u3002", "author": "Xiaoyu Bie et.al.", "authors": "Xiaoyu Bie, Xubo Liu, Ga\u00ebl Richard", "id": "2409.11228v1", "paper_url": "http://arxiv.org/abs/2409.11228v1", "repo": "null"}}