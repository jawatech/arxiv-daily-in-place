{"2407.18562": {"publish_time": "2024-07-26", "title": "Learning Robust Named Entity Recognizers From Noisy Data With Retrieval Augmentation", "paper_summary": "Named entity recognition (NER) models often struggle with noisy inputs, such\nas those with spelling mistakes or errors generated by Optical Character\nRecognition processes, and learning a robust NER model is challenging. Existing\nrobust NER models utilize both noisy text and its corresponding gold text for\ntraining, which is infeasible in many real-world applications in which gold\ntext is not available. In this paper, we consider a more realistic setting in\nwhich only noisy text and its NER labels are available. We propose to retrieve\nrelevant text of the noisy text from a knowledge corpus and use it to enhance\nthe representation of the original noisy input. We design three retrieval\nmethods: sparse retrieval based on lexicon similarity, dense retrieval based on\nsemantic similarity, and self-retrieval based on task-specific text. After\nretrieving relevant text, we concatenate the retrieved text with the original\nnoisy text and encode them with a transformer network, utilizing self-attention\nto enhance the contextual token representations of the noisy text using the\nretrieved text. We further employ a multi-view training framework that improves\nrobust NER without retrieving text during inference. Experiments show that our\nretrieval-augmented model achieves significant improvements in various noisy\nNER settings.", "paper_summary_zh": "\u547d\u540d\u5be6\u9ad4\u8fa8\u8b58 (NER) \u6a21\u578b\u901a\u5e38\u96e3\u4ee5\u8655\u7406\u6709\u96dc\u8a0a\u7684\u8f38\u5165\uff0c\u4f8b\u5982\u62fc\u5beb\u932f\u8aa4\u6216\u5149\u5b78\u5b57\u5143\u8fa8\u8b58\u7a0b\u5e8f\u7522\u751f\u7684\u932f\u8aa4\uff0c\u800c\u5b78\u7fd2\u7a69\u5065\u7684 NER \u6a21\u578b\u5177\u6709\u6311\u6230\u6027\u3002\u73fe\u6709\u7684\u7a69\u5065 NER \u6a21\u578b\u540c\u6642\u5229\u7528\u6709\u96dc\u8a0a\u7684\u6587\u5b57\u53ca\u5176\u5c0d\u61c9\u7684\u9ec3\u91d1\u6587\u5b57\u9032\u884c\u8a13\u7df4\uff0c\u9019\u5728\u8a31\u591a\u73fe\u5be6\u4e16\u754c\u7684\u61c9\u7528\u4e2d\u662f\u4e0d\u53ef\u884c\u7684\uff0c\u56e0\u70ba\u9ec3\u91d1\u6587\u5b57\u4e0d\u53ef\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8003\u616e\u4e00\u500b\u66f4\u5be6\u969b\u7684\u8a2d\u5b9a\uff0c\u5176\u4e2d\u53ea\u6709\u6709\u96dc\u8a0a\u7684\u6587\u5b57\u53ca\u5176 NER \u6a19\u7c64\u53ef\u7528\u3002\u6211\u5011\u63d0\u8b70\u5f9e\u77e5\u8b58\u8a9e\u6599\u5eab\u4e2d\u64f7\u53d6\u6709\u96dc\u8a0a\u6587\u5b57\u7684\u76f8\u5173\u6587\u5b57\uff0c\u4e26\u4f7f\u7528\u5b83\u4f86\u589e\u5f37\u539f\u59cb\u6709\u96dc\u8a0a\u8f38\u5165\u7684\u8868\u793a\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u4e09\u7a2e\u64f7\u53d6\u65b9\u6cd5\uff1a\u57fa\u65bc\u8a5e\u5f59\u76f8\u4f3c\u6027\u7684\u7a00\u758f\u64f7\u53d6\u3001\u57fa\u65bc\u8a9e\u7fa9\u76f8\u4f3c\u6027\u7684\u7a20\u5bc6\u64f7\u53d6\uff0c\u4ee5\u53ca\u57fa\u65bc\u7279\u5b9a\u4efb\u52d9\u6587\u5b57\u7684\u81ea\u64f7\u53d6\u3002\u5728\u64f7\u53d6\u76f8\u95dc\u6587\u5b57\u5f8c\uff0c\u6211\u5011\u5c07\u64f7\u53d6\u7684\u6587\u5b57\u8207\u539f\u59cb\u6709\u96dc\u8a0a\u6587\u5b57\u4e32\u63a5\uff0c\u4e26\u4f7f\u7528Transformer\u7db2\u8def\u5c0d\u5b83\u5011\u9032\u884c\u7de8\u78bc\uff0c\u5229\u7528\u81ea\u6211\u6ce8\u610f\u4f86\u4f7f\u7528\u64f7\u53d6\u7684\u6587\u5b57\u589e\u5f37\u6709\u96dc\u8a0a\u6587\u5b57\u7684\u8a9e\u5883\u6a19\u8a18\u8868\u793a\u3002\u6211\u5011\u9032\u4e00\u6b65\u63a1\u7528\u591a\u8996\u89d2\u8a13\u7df4\u67b6\u69cb\uff0c\u5b83\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u4e0d\u64f7\u53d6\u6587\u5b57\u5c31\u80fd\u6539\u5584\u7a69\u5065\u7684 NER\u3002\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u64f7\u53d6\u589e\u5f37\u6a21\u578b\u5728\u5404\u7a2e\u6709\u96dc\u8a0a\u7684 NER \u8a2d\u5b9a\u4e2d\u53d6\u5f97\u986f\u8457\u7684\u6539\u9032\u3002", "author": "Chaoyi Ai et.al.", "authors": "Chaoyi Ai, Yong Jiang, Shen Huang, Pengjun Xie, Kewei Tu", "id": "2407.18562v1", "paper_url": "http://arxiv.org/abs/2407.18562v1", "repo": "null"}}