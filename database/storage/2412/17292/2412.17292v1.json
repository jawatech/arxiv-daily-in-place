{"2412.17292": {"publish_time": "2024-12-23", "title": "AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues", "paper_summary": "In human communication, both verbal and non-verbal cues play a crucial role\nin conveying emotions, intentions, and meaning beyond words alone. These\nnon-linguistic information, such as facial expressions, eye contact, voice\ntone, and pitch, are fundamental elements of effective interactions, enriching\nconversations by adding emotional and contextual depth. Recognizing the\nimportance of non-linguistic content in communication, we present AV-EmoDialog,\na dialogue system designed to exploit verbal and non-verbal information from\nusers' audio-visual inputs to generate more responsive and empathetic\ninteractions. AV-EmoDialog systematically exploits the emotional cues in\naudio-visual dialogues; extracting speech content and emotional tones from\nspeech, analyzing fine-grained facial expressions from visuals, and integrating\nthese cues to generate emotionally aware responses in an end-to-end manner.\nThrough extensive experiments, we validate that the proposed AV-EmoDialog\noutperforms existing multimodal LLMs in generating not only emotionally\nappropriate but also contextually appropriate responses.", "paper_summary_zh": "\u5728\u4eba\u985e\u6e9d\u901a\u4e2d\uff0c\u8a00\u8a9e\u548c\u975e\u8a00\u8a9e\u7dda\u7d22\u5728\u50b3\u9054\u60c5\u7dd2\u3001\u610f\u5716\u548c\u610f\u7fa9\u65b9\u9762\u90fd\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\uff0c\u800c\u9019\u4e9b\u610f\u7fa9\u4e26\u975e\u8a00\u8a9e\u672c\u8eab\u6240\u80fd\u50b3\u9054\u7684\u3002\u9019\u4e9b\u975e\u8a9e\u8a00\u8cc7\u8a0a\uff0c\u4f8b\u5982\u9762\u90e8\u8868\u60c5\u3001\u773c\u795e\u63a5\u89f8\u3001\u8a9e\u8abf\u548c\u97f3\u9ad8\uff0c\u662f\u6709\u6548\u4e92\u52d5\u7684\u57fa\u672c\u8981\u7d20\uff0c\u5b83\u5011\u901a\u904e\u589e\u52a0\u60c5\u7dd2\u548c\u8a9e\u5883\u6df1\u5ea6\u4f86\u8c50\u5bcc\u5c0d\u8a71\u3002\u8a8d\u8b58\u5230\u975e\u8a9e\u8a00\u5167\u5bb9\u5728\u6e9d\u901a\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4e86 AV-EmoDialog\uff0c\u9019\u662f\u4e00\u500b\u5c0d\u8a71\u7cfb\u7d71\uff0c\u65e8\u5728\u5229\u7528\u4f7f\u7528\u8005\u97f3\u8a0a\u8996\u8a0a\u8f38\u5165\u4e2d\u7684\u8a00\u8a9e\u548c\u975e\u8a00\u8a9e\u8cc7\u8a0a\uff0c\u4ee5\u7522\u751f\u66f4\u5177\u56de\u61c9\u6027\u548c\u540c\u7406\u5fc3\u7684\u4e92\u52d5\u3002AV-EmoDialog \u7cfb\u7d71\u6027\u5730\u5229\u7528\u97f3\u8a0a\u8996\u8a0a\u5c0d\u8a71\u4e2d\u7684\u60c5\u7dd2\u7dda\u7d22\uff1b\u5f9e\u8a9e\u97f3\u4e2d\u63d0\u53d6\u8a9e\u97f3\u5167\u5bb9\u548c\u60c5\u7dd2\u8a9e\u8abf\uff0c\u5f9e\u8996\u89ba\u4e2d\u5206\u6790\u7d30\u7dfb\u7684\u9762\u90e8\u8868\u60c5\uff0c\u4e26\u5c07\u9019\u4e9b\u7dda\u7d22\u6574\u5408\u8d77\u4f86\uff0c\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u7522\u751f\u5177\u6709\u60c5\u7dd2\u611f\u77e5\u7684\u56de\u61c9\u3002\u901a\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u9a57\u8b49\u4e86\u6240\u63d0\u51fa\u7684 AV-EmoDialog \u5728\u7522\u751f\u4e0d\u50c5\u5728\u60c5\u7dd2\u4e0a\u9069\u7576\uff0c\u800c\u4e14\u5728\u8a9e\u5883\u4e0a\u9069\u7576\u7684\u56de\u61c9\u65b9\u9762\uff0c\u512a\u65bc\u73fe\u6709\u7684\u591a\u6a21\u614b LLM\u3002", "author": "Se Jin Park et.al.", "authors": "Se Jin Park, Yeonju Kim, Hyeongseop Rha, Bella Godiva, Yong Man Ro", "id": "2412.17292v1", "paper_url": "http://arxiv.org/abs/2412.17292v1", "repo": "null"}}