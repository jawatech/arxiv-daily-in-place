{"2406.18790": {"publish_time": "2024-06-26", "title": "MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data", "paper_summary": "We train a model to generate images from multimodal prompts of interleaved\ntext and images such as \"a <picture of a man> man and his <picture of a dog>\ndog in an <picture of a cartoon> animated style.\" We bootstrap a multimodal\ndataset by extracting semantically meaningful image crops corresponding to\nwords in the image captions of synthetically generated and publicly available\ntext-image data. Our model, MUMU, is composed of a vision-language model\nencoder with a diffusion decoder and is trained on a single 8xH100 GPU node.\nDespite being only trained on crops from the same image, MUMU learns to compose\ninputs from different images into a coherent output. For example, an input of a\nrealistic person and a cartoon will output the same person in the cartoon\nstyle, and an input of a standing subject and a scooter will output the subject\nriding the scooter. As a result, our model generalizes to tasks such as style\ntransfer and character consistency. Our results show the promise of using\nmultimodal models as general purpose controllers for image generation.", "paper_summary_zh": "\u6211\u5011\u8a13\u7df4\u4e00\u500b\u6a21\u578b\uff0c\u5f9e\u7a7f\u63d2\u6587\u5b57\u548c\u5716\u7247\u7684\u591a\u6a21\u614b\u63d0\u793a\u4e2d\u7522\u751f\u5716\u7247\uff0c\u4f8b\u5982\u300c<picture of a man> \u4e00\u500b\u7537\u4eba\u548c\u4ed6\u7684 <picture of a dog> \u72d7\u4ee5 <picture of a cartoon> \u52d5\u756b\u98a8\u683c\u5448\u73fe\u3002\u300d\u6211\u5011\u900f\u904e\u8403\u53d6\u8a9e\u7fa9\u4e0a\u6709\u610f\u7fa9\u7684\u5716\u7247\u88c1\u5207\uff0c\u5c0d\u61c9\u5230\u5408\u6210\u7522\u751f\u548c\u516c\u958b\u63d0\u4f9b\u7684\u6587\u5b57\u5716\u7247\u8cc7\u6599\u4e2d\u7684\u5716\u7247\u6a19\u984c\u4e2d\u7684\u6587\u5b57\uff0c\u4f86\u5efa\u7acb\u4e00\u500b\u591a\u6a21\u614b\u8cc7\u6599\u96c6\u3002\u6211\u5011\u7684\u6a21\u578b MUMU \u7531\u4e00\u500b\u5177\u6709\u64f4\u6563\u89e3\u78bc\u5668\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7de8\u78bc\u5668\u7d44\u6210\uff0c\u4e26\u5728\u4e00\u500b 8xH100 GPU \u7bc0\u9ede\u4e0a\u8a13\u7df4\u3002\u5118\u7ba1\u53ea\u5728\u540c\u4e00\u5f35\u5716\u7247\u7684\u88c1\u5207\u4e2d\u8a13\u7df4\uff0cMUMU \u5b78\u6703\u5c07\u4f86\u81ea\u4e0d\u540c\u5716\u7247\u7684\u8f38\u5165\u7d44\u6210\u4e00\u500b\u9023\u8cab\u7684\u8f38\u51fa\u3002\u4f8b\u5982\uff0c\u4e00\u500b\u771f\u5be6\u4eba\u7269\u548c\u4e00\u500b\u5361\u901a\u7684\u8f38\u5165\uff0c\u5c07\u8f38\u51fa\u4e00\u500b\u4ee5\u5361\u901a\u98a8\u683c\u5448\u73fe\u7684\u4eba\u7269\uff0c\u800c\u4e00\u500b\u7ad9\u7acb\u7684\u4e3b\u9ad4\u548c\u4e00\u500b\u6ed1\u677f\u8eca\u7684\u8f38\u5165\uff0c\u5c07\u8f38\u51fa\u4e00\u500b\u9a0e\u8457\u6ed1\u677f\u8eca\u7684\u4e3b\u9ad4\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u5ee3\u6cdb\u61c9\u7528\u65bc\u6a23\u5f0f\u8f49\u79fb\u548c\u89d2\u8272\u4e00\u81f4\u6027\u7b49\u4efb\u52d9\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\u51fa\u4f7f\u7528\u591a\u6a21\u614b\u6a21\u578b\u4f5c\u70ba\u5716\u7247\u7522\u751f\u7684\u4e00\u822c\u7528\u9014\u63a7\u5236\u5668\u7684\u524d\u666f\u3002", "author": "William Berman et.al.", "authors": "William Berman, Alexander Peysakhovich", "id": "2406.18790v1", "paper_url": "http://arxiv.org/abs/2406.18790v1", "repo": "null"}}