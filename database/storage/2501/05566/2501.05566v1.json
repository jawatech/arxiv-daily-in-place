{"2501.05566": {"publish_time": "2025-01-09", "title": "Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene Understanding", "paper_summary": "Scene understanding is essential for enhancing driver safety, generating\nhuman-centric explanations for Automated Vehicle (AV) decisions, and leveraging\nArtificial Intelligence (AI) for retrospective driving video analysis. This\nstudy developed a dynamic scene retrieval system using Contrastive\nLanguage-Image Pretraining (CLIP) models, which can be optimized for real-time\ndeployment on edge devices. The proposed system outperforms state-of-the-art\nin-context learning methods, including the zero-shot capabilities of GPT-4o,\nparticularly in complex scenarios. By conducting frame-level analysis on the\nHonda Scenes Dataset, which contains a collection of about 80 hours of\nannotated driving videos capturing diverse real-world road and weather\nconditions, our study highlights the robustness of CLIP models in learning\nvisual concepts from natural language supervision. Results also showed that\nfine-tuning the CLIP models, such as ViT-L/14 and ViT-B/32, significantly\nimproved scene classification, achieving a top F1 score of 91.1%. These results\ndemonstrate the ability of the system to deliver rapid and precise scene\nrecognition, which can be used to meet the critical requirements of Advanced\nDriver Assistance Systems (ADAS). This study shows the potential of CLIP models\nto provide scalable and efficient frameworks for dynamic scene understanding\nand classification. Furthermore, this work lays the groundwork for advanced\nautonomous vehicle technologies by fostering a deeper understanding of driver\nbehavior, road conditions, and safety-critical scenarios, marking a significant\nstep toward smarter, safer, and more context-aware autonomous driving systems.", "paper_summary_zh": "\u5834\u666f\u7406\u89e3\u5c0d\u65bc\u63d0\u5347\u99d5\u99db\u5b89\u5168\u3001\u70ba\u81ea\u52d5\u99d5\u99db\u8eca\u8f1b (AV) \u6c7a\u7b56\u7522\u751f\u4ee5\u4eba\u70ba\u4e2d\u5fc3\u7684\u89e3\u91cb\uff0c\u4ee5\u53ca\u5229\u7528\u4eba\u5de5\u667a\u6167 (AI) \u9032\u884c\u56de\u9867\u6027\u99d5\u99db\u5f71\u7247\u5206\u6790\u81f3\u95dc\u91cd\u8981\u3002\u672c\u7814\u7a76\u4f7f\u7528\u5c0d\u6bd4\u8a9e\u8a00\u5f71\u50cf\u9810\u8a13\u7df4 (CLIP) \u6a21\u578b\u958b\u767c\u4e86\u4e00\u500b\u52d5\u614b\u5834\u666f\u6aa2\u7d22\u7cfb\u7d71\uff0c\u8a72\u7cfb\u7d71\u53ef\u4ee5\u91dd\u5c0d\u908a\u7de3\u88dd\u7f6e\u4e0a\u7684\u5373\u6642\u90e8\u7f72\u9032\u884c\u6700\u4f73\u5316\u3002\u6240\u63d0\u51fa\u7684\u7cfb\u7d71\u512a\u65bc\u6700\u5148\u9032\u7684\u8108\u7d61\u5b78\u7fd2\u65b9\u6cd5\uff0c\u5305\u62ec GPT-4o \u7684\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\uff0c\u7279\u5225\u662f\u5728\u8907\u96dc\u5834\u666f\u4e2d\u3002\u900f\u904e\u5c0d Honda \u5834\u666f\u8cc7\u6599\u96c6\u9032\u884c\u9010\u5e40\u5206\u6790\uff0c\u5176\u4e2d\u5305\u542b\u7d04 80 \u5c0f\u6642\u7684\u6a19\u8a3b\u99d5\u99db\u5f71\u7247\uff0c\u6355\u6349\u5230\u591a\u6a23\u5316\u7684\u771f\u5be6\u4e16\u754c\u9053\u8def\u548c\u5929\u6c23\u72c0\u6cc1\uff0c\u6211\u5011\u7684\u7814\u7a76\u7a81\u986f\u4e86 CLIP \u6a21\u578b\u5728\u5f9e\u81ea\u7136\u8a9e\u8a00\u76e3\u7763\u4e2d\u5b78\u7fd2\u8996\u89ba\u6982\u5ff5\u7684\u7a69\u5065\u6027\u3002\u7d50\u679c\u4e5f\u986f\u793a\uff0c\u5fae\u8abf CLIP \u6a21\u578b\uff08\u4f8b\u5982 ViT-L/14 \u548c ViT-B/32\uff09\u986f\u8457\u6539\u5584\u4e86\u5834\u666f\u5206\u985e\uff0c\u9054\u5230\u4e86 91.1% \u7684\u6700\u9ad8 F1 \u5206\u6578\u3002\u9019\u4e9b\u7d50\u679c\u8b49\u660e\u4e86\u7cfb\u7d71\u63d0\u4f9b\u5feb\u901f\u4e14\u7cbe\u78ba\u5834\u666f\u8fa8\u8b58\u7684\u80fd\u529b\uff0c\u53ef\u6eff\u8db3\u5148\u9032\u99d5\u99db\u8f14\u52a9\u7cfb\u7d71 (ADAS) \u7684\u95dc\u9375\u9700\u6c42\u3002\u672c\u7814\u7a76\u986f\u793a\u4e86 CLIP \u6a21\u578b\u63d0\u4f9b\u53ef\u64f4\u5145\u4e14\u6709\u6548\u7387\u7684\u67b6\u69cb\uff0c\u7528\u65bc\u52d5\u614b\u5834\u666f\u7406\u89e3\u548c\u5206\u985e\u7684\u6f5b\u529b\u3002\u6b64\u5916\uff0c\u9019\u9805\u5de5\u4f5c\u900f\u904e\u4fc3\u9032\u5c0d\u99d5\u99db\u884c\u70ba\u3001\u9053\u8def\u72c0\u6cc1\u548c\u5b89\u5168\u95dc\u9375\u5834\u666f\u7684\u66f4\u6df1\u5165\u4e86\u89e3\uff0c\u70ba\u5148\u9032\u7684\u81ea\u52d5\u99d5\u99db\u8eca\u8f1b\u6280\u8853\u5960\u5b9a\u4e86\u57fa\u790e\uff0c\u6a19\u8a8c\u8457\u671d\u5411\u66f4\u667a\u6167\u3001\u66f4\u5b89\u5168\u4e14\u66f4\u5177\u8108\u7d61\u611f\u77e5\u7684\u81ea\u52d5\u99d5\u99db\u7cfb\u7d71\u9081\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002", "author": "Mohammed Elhenawy et.al.", "authors": "Mohammed Elhenawy, Huthaifa I. Ashqar, Andry Rakotonirainy, Taqwa I. Alhadidi, Ahmed Jaber, Mohammad Abu Tami", "id": "2501.05566v1", "paper_url": "http://arxiv.org/abs/2501.05566v1", "repo": "null"}}