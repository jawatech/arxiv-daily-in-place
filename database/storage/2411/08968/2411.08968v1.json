{"2411.08968": {"publish_time": "2024-11-13", "title": "Sparse Upcycling: Inference Inefficient Finetuning", "paper_summary": "Small, highly trained, open-source large language models are widely used due\nto their inference efficiency, but further improving their quality remains a\nchallenge. Sparse upcycling is a promising approach that transforms a\npretrained dense model into a Mixture-of-Experts (MoE) architecture, increasing\nthe model's parameter count and quality. In this work, we compare the\neffectiveness of sparse upcycling against continued pretraining (CPT) across\ndifferent model sizes, compute budgets, and pretraining durations. Our\nexperiments show that sparse upcycling can achieve better quality, with\nimprovements of over 20% relative to CPT in certain scenarios. However, this\ncomes with a significant inference cost, leading to 40% slowdowns in\nhigh-demand inference settings for larger models. Our findings highlight the\ntrade-off between model quality and inference efficiency, offering insights for\npractitioners seeking to balance model quality and deployment constraints.", "paper_summary_zh": "\u5c0f\u578b\u7684\u3001\u7ecf\u8fc7\u9ad8\u5ea6\u8bad\u7ec3\u7684\u3001\u5f00\u6e90\u7684\u5927\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u63a8\u7406\u6548\u7387\u800c\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5176\u8d28\u91cf\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u7a00\u758f\u5347\u7ea7\u662f\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u5b83\u5c06\u9884\u5148\u8bad\u7ec3\u7684\u7a20\u5bc6\u6a21\u578b\u8f6c\u6362\u4e3a\u4e13\u5bb6\u6df7\u5408 (MoE) \u67b6\u6784\uff0c\u4ece\u800c\u589e\u52a0\u4e86\u6a21\u578b\u7684\u53c2\u6570\u6570\u91cf\u548c\u8d28\u91cf\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86\u7a00\u758f\u5347\u7ea7\u4e0e\u6301\u7eed\u9884\u8bad\u7ec3 (CPT) \u5728\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u3001\u8ba1\u7b97\u9884\u7b97\u548c\u9884\u8bad\u7ec3\u6301\u7eed\u65f6\u95f4\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7a00\u758f\u5347\u7ea7\u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u8d28\u91cf\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u76f8\u5bf9\u4e8e CPT \u63d0\u9ad8\u4e86 20% \u4ee5\u4e0a\u3002\u7136\u800c\uff0c\u8fd9\u4f1a\u5e26\u6765\u663e\u7740\u7684\u63a8\u7406\u6210\u672c\uff0c\u5bfc\u81f4\u5728\u5927\u578b\u6a21\u578b\u7684\u9ad8\u9700\u6c42\u63a8\u7406\u8bbe\u7f6e\u4e2d\u901f\u5ea6\u964d\u4f4e 40%\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u6a21\u578b\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u5bfb\u6c42\u5e73\u8861\u6a21\u578b\u8d28\u91cf\u548c\u90e8\u7f72\u7ea6\u675f\u7684\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "author": "Sasha Doubov et.al.", "authors": "Sasha Doubov, Nikhil Sardana, Vitaliy Chiley", "id": "2411.08968v1", "paper_url": "http://arxiv.org/abs/2411.08968v1", "repo": "null"}}