{"2408.07990": {"publish_time": "2024-08-15", "title": "FuseChat: Knowledge Fusion of Chat Models", "paper_summary": "While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, it incurs substantial costs\nand may lead to redundancy in competencies. Knowledge fusion aims to integrate\nexisting LLMs of diverse architectures and capabilities into a more potent LLM\nthrough lightweight continual training, thereby reducing the need for costly\nLLM development. In this work, we propose a new framework for the knowledge\nfusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we\nconduct pairwise knowledge fusion on source chat LLMs of varying structures and\nscales to create multiple target LLMs with identical structure and size via\nlightweight fine-tuning. During this process, a statistics-based token\nalignment approach is introduced as the cornerstone for fusing LLMs with\ndifferent structures. Secondly, we merge these target LLMs within the parameter\nspace, where we propose a novel method for determining the merging coefficients\nbased on the magnitude of parameter updates before and after fine-tuning. We\nimplement and validate FuseChat using six prominent chat LLMs with diverse\narchitectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,\nNH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and\nQwen-1.5-Chat-72B. Experimental results on two instruction-following\nbenchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of\nFuseChat-7B over baselines of various sizes. Our model is even comparable to\nthe larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/FuseAI}.", "paper_summary_zh": "<paragraph>\u5f9e\u982d\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u78ba\u5be6\u53ef\u4ee5\u7522\u751f\u5177\u6709\u4e0d\u540c\u529f\u80fd\u548c\u512a\u52e2\u7684\u6a21\u578b\uff0c\u4f46\u6703\u7522\u751f\u5927\u91cf\u6210\u672c\uff0c\u4e26\u53ef\u80fd\u5c0e\u81f4\u80fd\u529b\u5197\u9918\u3002\u77e5\u8b58\u878d\u5408\u65e8\u5728\u900f\u904e\u8f15\u91cf\u7d1a\u7684\u6301\u7e8c\u8a13\u7df4\uff0c\u5c07\u4e0d\u540c\u67b6\u69cb\u548c\u529f\u80fd\u7684\u73fe\u6709 LLM \u6574\u5408\u5230\u66f4\u5f37\u5927\u7684 LLM \u4e2d\uff0c\u5f9e\u800c\u6e1b\u5c11\u5c0d\u6602\u8cb4 LLM \u958b\u767c\u7684\u9700\u6c42\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u804a\u5929 LLM \u77e5\u8b58\u878d\u5408\u67b6\u69cb\uff0c\u5206\u70ba\u5169\u500b\u4e3b\u8981\u968e\u6bb5\uff0c\u7522\u751f FuseChat\u3002\u9996\u5148\uff0c\u6211\u5011\u5c0d\u4e0d\u540c\u7d50\u69cb\u548c\u898f\u6a21\u7684\u539f\u59cb\u804a\u5929 LLM \u9032\u884c\u6210\u5c0d\u77e5\u8b58\u878d\u5408\uff0c\u900f\u904e\u8f15\u91cf\u7d1a\u5fae\u8abf\uff0c\u5efa\u7acb\u591a\u500b\u5177\u6709\u76f8\u540c\u7d50\u69cb\u548c\u5927\u5c0f\u7684\u76ee\u6a19 LLM\u3002\u5728\u6b64\u904e\u7a0b\u4e2d\uff0c\u5f15\u5165\u4e86\u57fa\u65bc\u7d71\u8a08\u7684\u4ee3\u78bc\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u4f5c\u70ba\u878d\u5408\u5177\u6709\u4e0d\u540c\u7d50\u69cb\u7684 LLM \u7684\u57fa\u77f3\u3002\u5176\u6b21\uff0c\u6211\u5011\u5728\u53c3\u6578\u7a7a\u9593\u4e2d\u5408\u4f75\u9019\u4e9b\u76ee\u6a19 LLM\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u6839\u64da\u5fae\u8abf\u524d\u5f8c\u53c3\u6578\u66f4\u65b0\u7684\u5e45\u5ea6\u4f86\u78ba\u5b9a\u5408\u4f75\u4fc2\u6578\u3002\u6211\u5011\u4f7f\u7528\u516d\u500b\u5177\u6709\u4e0d\u540c\u67b6\u69cb\u548c\u898f\u6a21\u7684\u8457\u540d\u804a\u5929 LLM \u5be6\u4f5c\u4e26\u9a57\u8b49\u4e86 FuseChat\uff0c\u5305\u62ec OpenChat-3.5-7B\u3001Starling-LM-7B-alpha\u3001NH2-SOLAR-10.7B\u3001InternLM2-Chat-20B\u3001Mixtral-8x7B-Instruct \u548c Qwen-1.5-Chat-72B\u3002\u5728\u5169\u500b\u6307\u4ee4\u9075\u5faa\u57fa\u6e96\u6e2c\u8a66 AlpacaEval 2.0 \u548c MT-Bench \u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86 FuseChat-7B \u512a\u65bc\u5404\u7a2e\u898f\u6a21\u7684\u57fa\u6e96\u3002\u6211\u5011\u7684\u6a21\u578b\u751a\u81f3\u53ef\u4ee5\u8207\u8f03\u5927\u7684 Mixtral-8x7B-Instruct \u76f8\u5ab2\u7f8e\uff0c\u4e26\u5728 MT-Bench \u4e0a\u63a5\u8fd1 GPT-3.5-Turbo-1106\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u3001\u6a21\u578b\u6b0a\u91cd\u548c\u8cc7\u6599\u5df2\u516c\u958b\u65bc\\url{https://github.com/fanqiwan/FuseAI}\u3002</paragraph>", "author": "Fanqi Wan et.al.", "authors": "Fanqi Wan, Longguang Zhong, Ziyi Yang, Ruijun Chen, Xiaojun Quan", "id": "2408.07990v1", "paper_url": "http://arxiv.org/abs/2408.07990v1", "repo": "https://github.com/fanqiwan/fuseai"}}