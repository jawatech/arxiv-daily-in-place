{"2407.08348": {"publish_time": "2024-07-11", "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On", "paper_summary": "In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u6f5b\u5728\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6578\u5b78\u63a8\u7406\u80fd\u529b\u7684\u57fa\u790e\u56e0\u7d20\u3002\u6211\u5011\u4e3b\u5f35\uff0c\u73fe\u4ee3 LLM \u4e2d\u6578\u5b78\u63a8\u7406\u80fd\u529b\u7684\u8cc7\u6599\u64f4\u5145\u6cd5\u5247\u9060\u672a\u9054\u5230\u98fd\u548c\uff0c\u5f37\u8abf\u6a21\u578b\u7684\u54c1\u8cea\u6703\u96a8\u8457\u8cc7\u6599\u91cf\u7684\u589e\u52a0\u800c\u63d0\u5347\u3002\u70ba\u4e86\u652f\u6301\u6b64\u8ad6\u9ede\uff0c\u6211\u5011\u5f15\u5165\u4e86 Skywork-Math \u6a21\u578b\u7cfb\u5217\uff0c\u4f7f\u7528\u6211\u5011\u63d0\u51fa\u7684 250 \u842c\u500b\u4f8b\u5b50\u7684 Skywork-MathQA \u8cc7\u6599\u96c6\uff0c\u5c0d\u5e38\u898b\u7684 7B LLM \u9032\u884c\u76e3\u7763\u5f0f\u5fae\u8abf (SFT)\u3002Skywork-Math 7B \u50c5\u4f7f\u7528 SFT \u8cc7\u6599\uff0c\u5c31\u5728\u7af6\u8cfd\u7d1a\u5225\u7684 MATH \u57fa\u6e96\u6e2c\u8a66\u4e2d\u9054\u5230\u4e86 51.2% \u7684\u9a5a\u4eba\u6e96\u78ba\u5ea6\uff0c\u5728 GSM8K \u57fa\u6e96\u6e2c\u8a66\u4e2d\u9054\u5230\u4e86 83.9%\uff0c\u8868\u73fe\u512a\u65bc GPT-4 \u7684\u65e9\u671f\u7248\u672c\u3002Skywork-Math \u6a21\u578b\u7684\u512a\u7570\u6027\u80fd\u6b78\u529f\u65bc\u6211\u5011\u65b0\u7a4e\u7684\u5169\u968e\u6bb5\u8cc7\u6599\u5408\u6210\u548c\u6a21\u578b SFT \u7ba1\u7dda\uff0c\u5176\u4e2d\u5305\u62ec\u4e09\u7a2e\u4e0d\u540c\u7684\u64f4\u5145\u65b9\u6cd5\u548c\u4e00\u500b\u591a\u6a23\u5316\u7684\u7a2e\u5b50\u554f\u984c\u96c6\uff0c\u78ba\u4fdd\u4e86 Skywork-MathQA \u8cc7\u6599\u96c6\u5728\u4e0d\u540c\u96e3\u5ea6\u7d1a\u5225\u4e0a\u7684\u6578\u91cf\u548c\u54c1\u8cea\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u5e7e\u500b\u5be6\u7528\u7684\u5916\u5e36\u4e8b\u9805\uff0c\u4ee5\u589e\u5f37 LLM \u5728\u7814\u7a76\u548c\u7522\u696d\u61c9\u7528\u4e2d\u7684\u6578\u5b78\u63a8\u7406\u80fd\u529b\u3002</paragraph>", "author": "Liang Zeng et.al.", "authors": "Liang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu, Yang Liu, Shuicheng Yan, Han Fang, Yahui Zhou", "id": "2407.08348v1", "paper_url": "http://arxiv.org/abs/2407.08348v1", "repo": "null"}}