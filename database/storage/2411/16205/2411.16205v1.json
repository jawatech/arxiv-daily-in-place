{"2411.16205": {"publish_time": "2024-11-25", "title": "MH-MoE:Multi-Head Mixture-of-Experts", "paper_summary": "Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.", "paper_summary_zh": "\u591a\u982d\u5c08\u5bb6\u6df7\u5408\uff08MH-MoE\uff09\u900f\u904e\u4f7f\u7528\u591a\u982d\u6a5f\u5236\u4f86\u5171\u540c\u95dc\u6ce8\u4f86\u81ea\u4e0d\u540c\u5c08\u5bb6\u5167\u90e8\u5404\u7a2e\u8868\u793a\u7a7a\u9593\u7684\u8cc7\u8a0a\uff0c\u5c55\u793a\u51fa\u512a\u8d8a\u7684\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e MH-MoE \u7684\u65b0\u7a4e\u5be6\u4f5c\uff0c\u5b83\u540c\u6642\u7dad\u6301\u4e86\u7a00\u758f\u5c08\u5bb6\u6df7\u5408\u6a21\u578b\u7684 FLOP \u548c\u53c3\u6578\u5e73\u50f9\u3002\u8a9e\u8a00\u6a21\u578b\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u65b0\u7684\u5be6\u4f5c\u5728\u50b3\u7d71 MoE \u548c\u7d30\u7c92\u5ea6 MoE \u6a21\u578b\u4e0a\u90fd\u7522\u751f\u4e86\u54c1\u8cea\u7684\u63d0\u5347\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e MH-MoE \u8207 1 \u4f4d\u5143\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f8b\u5982 BitNet \u76f8\u5bb9\u3002", "author": "Shaohan Huang et.al.", "authors": "Shaohan Huang, Xun Wu, Shuming Ma, Furu Wei", "id": "2411.16205v1", "paper_url": "http://arxiv.org/abs/2411.16205v1", "repo": "null"}}