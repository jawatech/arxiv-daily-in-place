{"2409.07748": {"publish_time": "2024-09-12", "title": "Top-down Activity Representation Learning for Video Question Answering", "paper_summary": "Capturing complex hierarchical human activities, from atomic actions (e.g.,\npicking up one present, moving to the sofa, unwrapping the present) to\ncontextual events (e.g., celebrating Christmas) is crucial for achieving\nhigh-performance video question answering (VideoQA). Recent works have expanded\nmultimodal models (e.g., CLIP, LLaVA) to process continuous video sequences,\nenhancing the model's temporal reasoning capabilities. However, these\napproaches often fail to capture contextual events that can be decomposed into\nmultiple atomic actions non-continuously distributed over relatively long-term\nsequences. In this paper, to leverage the spatial visual context representation\ncapability of the CLIP model for obtaining non-continuous visual\nrepresentations in terms of contextual events in videos, we convert long-term\nvideo sequences into a spatial image domain and finetune the multimodal model\nLLaVA for the VideoQA task. Our approach achieves competitive performance on\nthe STAR task, in particular, with a 78.4% accuracy score, exceeding the\ncurrent state-of-the-art score by 2.8 points on the NExTQA task.", "paper_summary_zh": "\u6355\u6349\u8907\u96dc\u7684\u5206\u5c64\u4eba\u985e\u6d3b\u52d5\uff0c\u5f9e\u539f\u5b50\u52d5\u4f5c\uff08\u4f8b\u5982\uff0c\u62ff\u8d77\u4e00\u500b\u79ae\u7269\u3001\u79fb\u52d5\u5230\u6c99\u767c\u4e0a\u3001\u62c6\u958b\u79ae\u7269\uff09\u5230\u60c5\u5883\u4e8b\u4ef6\uff08\u4f8b\u5982\uff0c\u6176\u795d\u8056\u8a95\u7bc0\uff09\uff0c\u5c0d\u65bc\u5be6\u73fe\u9ad8\u6027\u80fd\u5f71\u7247\u554f\u7b54 (VideoQA) \u81f3\u95dc\u91cd\u8981\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u64f4\u5c55\u591a\u6a21\u614b\u6a21\u578b\uff08\u4f8b\u5982\uff0cCLIP\u3001LLaVA\uff09\u4ee5\u8655\u7406\u9023\u7e8c\u5f71\u7247\u5e8f\u5217\uff0c\u589e\u5f37\u6a21\u578b\u7684\u6642\u9593\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u7121\u6cd5\u6355\u6349\u60c5\u5883\u4e8b\u4ef6\uff0c\u9019\u4e9b\u4e8b\u4ef6\u53ef\u4ee5\u5206\u89e3\u70ba\u5728\u76f8\u5c0d\u9577\u671f\u5e8f\u5217\u4e2d\u975e\u9023\u7e8c\u5206\u4f48\u7684\u591a\u500b\u539f\u5b50\u52d5\u4f5c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u70ba\u4e86\u5229\u7528 CLIP \u6a21\u578b\u7684\u7a7a\u9593\u8996\u89ba\u4e0a\u4e0b\u6587\u8868\u793a\u529f\u80fd\uff0c\u4ee5\u5728\u5f71\u7247\u4e2d\u4ee5\u60c5\u5883\u4e8b\u4ef6\u7684\u5f62\u5f0f\u7372\u5f97\u975e\u9023\u7e8c\u8996\u89ba\u8868\u793a\uff0c\u6211\u5011\u5c07\u9577\u671f\u5f71\u7247\u5e8f\u5217\u8f49\u63db\u70ba\u7a7a\u9593\u5f71\u50cf\u7db2\u57df\uff0c\u4e26\u5fae\u8abf\u591a\u6a21\u614b\u6a21\u578b LLaVA \u4ee5\u7528\u65bc VideoQA \u4efb\u52d9\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728 STAR \u4efb\u52d9\u4e2d\u53d6\u5f97\u4e86\u5177\u6709\u7af6\u722d\u529b\u7684\u8868\u73fe\uff0c\u7279\u5225\u662f\u6e96\u78ba\u5ea6\u5f97\u5206\u70ba 78.4%\uff0c\u5728 NExTQA \u4efb\u52d9\u4e2d\u6bd4\u76ee\u524d\u7684\u6700\u65b0\u6280\u8853\u5f97\u5206\u9ad8\u51fa 2.8 \u5206\u3002", "author": "Yanan Wang et.al.", "authors": "Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Vizcarra, Mori Kurokawa", "id": "2409.07748v1", "paper_url": "http://arxiv.org/abs/2409.07748v1", "repo": "null"}}