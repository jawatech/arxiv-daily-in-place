{"2405.21075": {"publish_time": "2024-05-31", "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis", "paper_summary": "In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 256 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io", "paper_summary_zh": "\u5728\u8ffd\u6c42\u4eba\u5de5\u901a\u7528\u667a\u80fd\u7684\u904e\u7a0b\u4e2d\uff0c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5df2\u6210\u70ba\u8fd1\u671f\u9032\u5c55\u4e2d\u7684\u7126\u9ede\u3002\u7136\u800c\uff0c\u4e3b\u8981\u7684\u91cd\u9ede\u4ecd\u653e\u5728\u767c\u5c55\u5b83\u5011\u5728\u975c\u614b\u5f71\u50cf\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u3002MLLM \u5728\u8655\u7406\u9806\u5e8f\u8996\u89ba\u8cc7\u6599\u7684\u6f5b\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8a0e\uff0c\u7a81\u986f\u4e86\u7f3a\u4e4f\u5c0d\u5176\u6548\u80fd\u9032\u884c\u5168\u9762\u3001\u9ad8\u54c1\u8cea\u8a55\u4f30\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 Video-MME\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u91dd\u5c0d MLLM \u5728\u5f71\u7247\u5206\u6790\u4e2d\u7684\u5168\u5149\u8b5c\u591a\u6a21\u614b\u8a55\u4f30\u57fa\u6e96\u3002\u6211\u5011\u7684\u7814\u7a76\u900f\u904e\u56db\u9805\u95dc\u9375\u7279\u5fb5\u5340\u5225\u65bc\u73fe\u6709\u57fa\u6e96\uff1a1) \u5f71\u7247\u985e\u578b\u7684\u591a\u6a23\u6027\uff0c\u6db5\u84cb 6 \u500b\u4e3b\u8981\u8996\u89ba\u9818\u57df\uff0c\u5305\u542b 30 \u500b\u5b50\u9818\u57df\uff0c\u4ee5\u78ba\u4fdd\u5ee3\u6cdb\u7684\u5834\u666f\u901a\u7528\u6027\uff1b2) \u6642\u9593\u7dad\u5ea6\u7684\u6301\u7e8c\u6642\u9593\uff0c\u5305\u542b\u77ed\u3001\u4e2d\u3001\u9577\u671f\u5f71\u7247\uff0c\u7bc4\u570d\u5f9e 11 \u79d2\u5230 1 \u5c0f\u6642\uff0c\u4ee5\u78ba\u4fdd\u7a69\u5065\u7684\u8108\u7d61\u52d5\u614b\uff1b3) \u8cc7\u6599\u6a21\u614b\u7684\u5ee3\u5ea6\uff0c\u9664\u4e86\u5f71\u7247\u5e40\u4e4b\u5916\uff0c\u9084\u6574\u5408\u4e86\u591a\u6a21\u614b\u8f38\u5165\uff0c\u5305\u62ec\u5b57\u5e55\u548c\u97f3\u8a0a\uff0c\u4ee5\u63ed\u793a MLLM \u7684\u5168\u9762\u80fd\u529b\uff1b4) \u6a19\u8a3b\u7684\u54c1\u8cea\uff0c\u5229\u7528\u5c08\u5bb6\u6a19\u8a3b\u54e1\u56b4\u8b39\u7684\u624b\u52d5\u6a19\u8a3b\uff0c\u4ee5\u5229\u7cbe\u78ba\u4e14\u53ef\u9760\u7684\u6a21\u578b\u8a55\u4f30\u3002900 \u500b\u5f71\u7247\uff0c\u7e3d\u9577\u70ba 256 \u5c0f\u6642\uff0c\u900f\u904e\u91cd\u8907\u89c0\u770b\u6240\u6709\u5f71\u7247\u5167\u5bb9\u9032\u884c\u624b\u52d5\u6311\u9078\u548c\u6a19\u8a3b\uff0c\u7522\u751f\u4e86 2,700 \u500b\u554f\u7b54\u914d\u5c0d\u3002\u900f\u904e Video-MME\uff0c\u6211\u5011\u5ee3\u6cdb\u8a55\u4f30\u4e86\u5404\u7a2e\u6700\u5148\u9032\u7684 MLLM\uff0c\u5305\u62ec GPT-4 \u7cfb\u5217\u548c Gemini 1.5 Pro\uff0c\u4ee5\u53ca\u50cf InternVL-Chat-V1.5 \u7684\u958b\u6e90\u5f71\u50cf\u6a21\u578b\u548c LLaVA-NeXT-Video \u7684\u5f71\u7247\u6a21\u578b\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0cGemini 1.5 Pro \u662f\u6548\u80fd\u6700\u597d\u7684\u5546\u696d\u6a21\u578b\uff0c\u986f\u8457\u512a\u65bc\u958b\u6e90\u6a21\u578b\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u548c\u9019\u4e9b\u767c\u73fe\u5f37\u8abf\u4e86\u9032\u4e00\u6b65\u6539\u9032\u8655\u7406\u8f03\u9577\u9806\u5e8f\u548c\u591a\u6a21\u614b\u8cc7\u6599\u7684\u5fc5\u8981\u6027\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://video-mme.github.io", "author": "Chaoyou Fu et.al.", "authors": "Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, Xing Sun", "id": "2405.21075v1", "paper_url": "http://arxiv.org/abs/2405.21075v1", "repo": "null"}}