{"2409.16099": {"publish_time": "2024-09-24", "title": "Neuromorphic Drone Detection: an Event-RGB Multimodal Approach", "paper_summary": "In recent years, drone detection has quickly become a subject of extreme\ninterest: the potential for fast-moving objects of contained dimensions to be\nused for malicious intents or even terrorist attacks has posed attention to the\nnecessity for precise and resilient systems for detecting and identifying such\nelements. While extensive literature and works exist on object detection based\non RGB data, it is also critical to recognize the limits of such modality when\napplied to UAVs detection. Detecting drones indeed poses several challenges\nsuch as fast-moving objects and scenes with a high dynamic range or, even\nworse, scarce illumination levels. Neuromorphic cameras, on the other hand, can\nretain precise and rich spatio-temporal information in situations that are\nchallenging for RGB cameras. They are resilient to both high-speed moving\nobjects and scarce illumination settings, while prone to suffer a rapid loss of\ninformation when the objects in the scene are static. In this context, we\npresent a novel model for integrating both domains together, leveraging\nmultimodal data to take advantage of the best of both worlds. To this end, we\nalso release NeRDD (Neuromorphic-RGB Drone Detection), a novel\nspatio-temporally synchronized Event-RGB Drone detection dataset of more than\n3.5 hours of multimodal annotated recordings.", "paper_summary_zh": "\u8fd1\u5e74\u6765\uff0c\u65e0\u4eba\u673a\u68c0\u6d4b\u5df2\u8fc5\u901f\u6210\u4e3a\u6781\u5ea6\u5173\u6ce8\u7684\u4e3b\u9898\uff1a\u5c3a\u5bf8\u53d7\u9650\u7684\u5feb\u901f\u79fb\u52a8\u7269\u4f53\u53ef\u80fd\u88ab\u7528\u4e8e\u6076\u610f\u76ee\u7684\u751a\u81f3\u6050\u6016\u88ad\u51fb\uff0c\u8fd9\u5f15\u8d77\u4e86\u4eba\u4eec\u5bf9\u7528\u4e8e\u68c0\u6d4b\u548c\u8bc6\u522b\u6b64\u7c7b\u5143\u7d20\u7684\u7cbe\u786e\u4e14\u6709\u5f39\u6027\u7684\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\u7684\u5173\u6ce8\u3002\u867d\u7136\u57fa\u4e8e RGB \u6570\u636e\u7684\u5bf9\u8c61\u68c0\u6d4b\u5b58\u5728\u5927\u91cf\u6587\u732e\u548c\u8457\u4f5c\uff0c\u4f46\u5728\u5c06\u5176\u5e94\u7528\u4e8e\u65e0\u4eba\u673a\u68c0\u6d4b\u65f6\uff0c\u8ba4\u8bc6\u5230\u8fd9\u79cd\u65b9\u5f0f\u7684\u5c40\u9650\u6027\u4e5f\u81f3\u5173\u91cd\u8981\u3002\u68c0\u6d4b\u65e0\u4eba\u673a\u786e\u5b9e\u4f1a\u5e26\u6765\u4e00\u4e9b\u6311\u6218\uff0c\u4f8b\u5982\u5feb\u901f\u79fb\u52a8\u7684\u7269\u4f53\u548c\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u7684\u573a\u666f\uff0c\u6216\u8005\u66f4\u7cdf\u7684\u662f\uff0c\u5149\u7167\u6c34\u5e73\u4e0d\u8db3\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u795e\u7ecf\u5f62\u6001\u76f8\u673a\u53ef\u4ee5\u5728\u5bf9\u4e8e RGB \u76f8\u673a\u800c\u8a00\u5177\u6709\u6311\u6218\u6027\u7684\u60c5\u51b5\u4e0b\u4fdd\u7559\u7cbe\u786e\u4e14\u4e30\u5bcc\u7684\u65f6\u7a7a\u4fe1\u606f\u3002\u5b83\u4eec\u65e2\u80fd\u62b5\u5fa1\u9ad8\u901f\u79fb\u52a8\u7684\u7269\u4f53\uff0c\u53c8\u80fd\u62b5\u5fa1\u5149\u7167\u4e0d\u8db3\u7684\u60c5\u51b5\uff0c\u4f46\u5f53\u573a\u666f\u4e2d\u7684\u7269\u4f53\u9759\u6b62\u65f6\uff0c\u5b83\u4eec\u5bb9\u6613\u8fc5\u901f\u4e22\u5931\u4fe1\u606f\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8fd9\u4e24\u4e2a\u57df\u96c6\u6210\u5728\u4e00\u8d77\u7684\u65b0\u9896\u6a21\u578b\uff0c\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u6765\u5145\u5206\u5229\u7528\u4e24\u5168\u5176\u7f8e\u7684\u4f18\u52bf\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8fd8\u53d1\u5e03\u4e86 NeRDD\uff08\u795e\u7ecf\u5f62\u6001 RGB \u65e0\u4eba\u673a\u68c0\u6d4b\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u65f6\u7a7a\u540c\u6b65\u4e8b\u4ef6 RGB \u65e0\u4eba\u673a\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u8d85\u8fc7 3.5 \u5c0f\u65f6\u7684\u591a\u6a21\u6001\u6807\u6ce8\u8bb0\u5f55\u3002", "author": "Gabriele Magrini et.al.", "authors": "Gabriele Magrini, Federico Becattini, Pietro Pala, Alberto Del Bimbo, Antonio Porta", "id": "2409.16099v1", "paper_url": "http://arxiv.org/abs/2409.16099v1", "repo": "null"}}