{"2410.23123": {"publish_time": "2024-10-30", "title": "On Memorization of Large Language Models in Logical Reasoning", "paper_summary": "Large language models (LLMs) achieve good performance on challenging\nreasoning benchmarks, yet could also make basic reasoning mistakes. This\ncontrasting behavior is puzzling when it comes to understanding the mechanisms\nbehind LLMs' reasoning capabilities. One hypothesis is that the increasingly\nhigh and nearly saturated performance on common reasoning benchmarks could be\ndue to the memorization of similar problems. In this paper, we systematically\ninvestigate this hypothesis with a quantitative measurement of memorization in\nreasoning tasks, using a dynamically generated logical reasoning benchmark\nbased on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate\nthe training puzzles (achieving near-perfect accuracy) after fine-tuning, yet\nfail when those puzzles are slightly perturbed, suggesting that the models\nheavily rely on memorization to solve those training puzzles. On the other\nhand, we show that while fine-tuning leads to heavy memorization, it also\nconsistently improves generalization performance. In-depth analyses with\nperturbation tests, cross difficulty-level transferability, probing model\ninternals, and fine-tuning with wrong answers suggest that the LLMs learn to\nreason on K&K puzzles despite training data memorization. This phenomenon\nindicates that LLMs exhibit a complex interplay between memorization and\ngenuine reasoning abilities. Finally, our analysis with per-sample memorization\nscore sheds light on how LLMs switch between reasoning and memorization in\nsolving logical puzzles. Our code and data are available at\nhttps://memkklogic.github.io.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5177\u6311\u6230\u6027\u7684\u63a8\u7406\u57fa\u6e96\u4e0a\u53d6\u5f97\u826f\u597d\u8868\u73fe\uff0c\u4f46\u4e5f\u6703\u72af\u4e0b\u57fa\u672c\u7684\u63a8\u7406\u932f\u8aa4\u3002\u9019\u7a2e\u5c0d\u6bd4\u884c\u70ba\u4ee4\u4eba\u8cbb\u89e3\uff0c\u56e0\u70ba\u5b83\u6d89\u53ca\u7406\u89e3 LLM \u63a8\u7406\u80fd\u529b\u80cc\u5f8c\u7684\u6a5f\u5236\u3002\u4e00\u500b\u5047\u8a2d\u662f\uff0c\u5728\u5e38\u898b\u63a8\u7406\u57fa\u6e96\u4e0a\u8d8a\u4f86\u8d8a\u9ad8\u4e14\u63a5\u8fd1\u98fd\u548c\u7684\u8868\u73fe\u53ef\u80fd\u662f\u7531\u65bc\u8a18\u4f4f\u4e86\u985e\u4f3c\u7684\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u57fa\u65bc\u9a0e\u58eb\u8207\u60e1\u68cd (K&K) \u8b0e\u984c\u7684\u52d5\u614b\u751f\u6210\u908f\u8f2f\u63a8\u7406\u57fa\u6e96\uff0c\u5c0d\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u8a18\u61b6\u9032\u884c\u5b9a\u91cf\u6e2c\u91cf\uff0c\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u9019\u500b\u5047\u8a2d\u3002\u6211\u5011\u767c\u73fe\uff0cLLM \u5728\u5fae\u8abf\u5f8c\u53ef\u4ee5\u5167\u63d2\u8a13\u7df4\u8b0e\u984c\uff08\u9054\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6e96\u78ba\u5ea6\uff09\uff0c\u4f46\u5728\u9019\u4e9b\u8b0e\u984c\u7a0d\u6709\u64fe\u52d5\u6642\u537b\u6703\u5931\u6557\uff0c\u9019\u8868\u660e\u6a21\u578b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8cf4\u8a18\u61b6\u4f86\u89e3\u6c7a\u9019\u4e9b\u8a13\u7df4\u8b0e\u984c\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u6211\u5011\u8868\u660e\uff0c\u96d6\u7136\u5fae\u8abf\u6703\u5c0e\u81f4\u5927\u91cf\u7684\u8a18\u61b6\uff0c\u4f46\u5b83\u4e5f\u6703\u6301\u7e8c\u6539\u5584\u6cdb\u5316\u8868\u73fe\u3002\u4f7f\u7528\u64fe\u52d5\u6e2c\u8a66\u3001\u8de8\u96e3\u5ea6\u7d1a\u5225\u7684\u53ef\u50b3\u905e\u6027\u3001\u63a2\u6e2c\u6a21\u578b\u5167\u90e8\u7d50\u69cb\u4ee5\u53ca\u4f7f\u7528\u932f\u8aa4\u7b54\u6848\u9032\u884c\u5fae\u8abf\u7684\u6df1\u5165\u5206\u6790\u8868\u660e\uff0cLLM \u5b78\u6703\u4e86\u5c0d K&K \u8b0e\u984c\u9032\u884c\u63a8\u7406\uff0c\u5118\u7ba1\u8a13\u7df4\u6578\u64da\u8a18\u61b6\u3002\u9019\u7a2e\u73fe\u8c61\u8868\u660e\uff0cLLM \u5728\u8a18\u61b6\u548c\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u4e4b\u9593\u8868\u73fe\u51fa\u8907\u96dc\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c0d\u6bcf\u500b\u6a23\u672c\u8a18\u61b6\u5206\u6578\u7684\u5206\u6790\u95e1\u660e\u4e86 LLM \u5728\u89e3\u6c7a\u908f\u8f2f\u8b0e\u984c\u6642\u5982\u4f55\u5728\u63a8\u7406\u548c\u8a18\u61b6\u4e4b\u9593\u5207\u63db\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6578\u64da\u53ef\u5728 https://memkklogic.github.io/ \u7372\u5f97\u3002</paragraph>", "author": "Chulin Xie et.al.", "authors": "Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, Ravi Kumar", "id": "2410.23123v1", "paper_url": "http://arxiv.org/abs/2410.23123v1", "repo": "null"}}