{"2407.18003": {"publish_time": "2024-07-25", "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption", "paper_summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4ee5 2022 \u5e74\u5e95\u767c\u5e03\u7684 ChatGPT \u70ba\u4ee3\u8868\uff0c\n\u4ee5\u5176\u5148\u9032\u7684\u8a9e\u8a00\u7406\u89e3\u529b\u5fb9\u5e95\u6539\u8b8a\u4e86\u5404\u500b\u7522\u696d\u3002\u7136\u800c\uff0c\u5176\u6548\u7387\u53d7\u5230 Transformer\n\u67b6\u69cb\u5728\u8655\u7406\u9577\u6587\u5b57\u6642\u6240\u906d\u9047\u7684\u56f0\u96e3\u6240\u6311\u6230\u3002KV \u5feb\u53d6\u5df2\u6210\u70ba\u89e3\u6c7a\u6b64\u554f\u984c\u7684\u95dc\u9375\u89e3\u6c7a\u65b9\u6848\uff0c\u5c07 token\n\u7522\u751f\u7684\u6642\u9593\u8907\u96dc\u5ea6\u5f9e\u4e8c\u6b21\u8f49\u63db\u70ba\u7dda\u6027\uff0c\u5118\u7ba1 GPU \u8a18\u61b6\u9ad4\u958b\u92b7\u6703\u96a8\u8457\u5c0d\u8a71\u9577\u5ea6\u6210\u6bd4\u4f8b\u5730\u589e\u52a0\u3002\u96a8\u8457 LLM \u793e\u7fa4\n\u548c\u5b78\u8853\u754c\u7684\u767c\u5c55\uff0c\u5df2\u63d0\u51fa\u5404\u7a2e KV \u5feb\u53d6\u58d3\u7e2e\u65b9\u6cd5\u3002\u5728\u6b64\n\u56de\u9867\u4e2d\uff0c\u6211\u5011\u5256\u6790\u4e86 KV \u5feb\u53d6\u7684\u5404\u7a2e\u7279\u6027\uff0c\u4e26\u95e1\u8ff0\u4e86\u76ee\u524d\u7528\u65bc\u6700\u4f73\u5316 LLM \u7684 KV \u5feb\u53d6\u7a7a\u9593\u4f7f\u7528\u7387\u7684\u5404\u7a2e\n\u65b9\u6cd5\u3002\u9019\u4e9b\n\u65b9\u6cd5\u6db5\u84cb\u4e86\u9810\u8a13\u7df4\u968e\u6bb5\u3001\u90e8\u7f72\u968e\u6bb5\u548c\u63a8\u8ad6\u968e\u6bb5\uff0c\u6211\u5011\u7e3d\u7d50\u4e86\u9019\u4e9b\u65b9\u6cd5\u4e4b\u9593\u7684\u5171\u6027\u548c\u5dee\u7570\u3002\n\u6b64\u5916\uff0c\u6211\u5011\u5217\u51fa\u4e86\u4e00\u4e9b\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u9577\u6587\u5b57\u80fd\u529b\u7684\u6307\u6a19\uff0c\u5f9e\u6548\u7387\u548c\u80fd\u529b\u7684\u89d2\u5ea6\u4f86\u770b\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7684\n\u56de\u9867\u95e1\u660e\u4e86 LLM \u6700\u4f73\u5316\u7684\u6f14\u8b8a\u8da8\u52e2\uff0c\u63d0\u4f9b\u4e86\u5c0d\u6b64\u52d5\u614b\u9818\u57df\u672a\u4f86\u9032\u5c55\u7684\u898b\u89e3\u3002", "author": "Shi Luohe et.al.", "authors": "Shi Luohe, Zhang Hongyi, Yao Yao, Li Zuchao, Zhao Hai", "id": "2407.18003v1", "paper_url": "http://arxiv.org/abs/2407.18003v1", "repo": "null"}}