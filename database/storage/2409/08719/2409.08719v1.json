{"2409.08719": {"publish_time": "2024-09-13", "title": "Distilling Monolingual and Crosslingual Word-in-Context Representations", "paper_summary": "In this study, we propose a method that distils representations of word\nmeaning in context from a pre-trained masked language model in both monolingual\nand crosslingual settings. Word representations are the basis for context-aware\nlexical semantics and unsupervised semantic textual similarity (STS)\nestimation. Different from existing approaches, our method does not require\nhuman-annotated corpora nor updates of the parameters of the pre-trained model.\nThe latter feature is appealing for practical scenarios where the off-the-shelf\npre-trained model is a common asset among different applications. Specifically,\nour method learns to combine the outputs of different hidden layers of the\npre-trained model using self-attention. Our auto-encoder based training only\nrequires an automatically generated corpus. To evaluate the performance of the\nproposed approach, we performed extensive experiments using various benchmark\ntasks. The results on the monolingual tasks confirmed that our representations\nexhibited a competitive performance compared to that of the previous study for\nthe context-aware lexical semantic tasks and outperformed it for STS\nestimation. The results of the crosslingual tasks revealed that the proposed\nmethod largely improved crosslingual word representations of multilingual\npre-trained models.", "paper_summary_zh": "<paragraph>\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5f9e\u9810\u5148\u8a13\u7df4\u597d\u7684\u906e\u853d\u8a9e\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u55ae\u8a5e\u5728\u4e0a\u4e0b\u6587\u4e2d\u542b\u7fa9\u7684\u8868\u5fb5\uff0c\u540c\u6642\u9069\u7528\u65bc\u55ae\u8a9e\u548c\u8de8\u8a9e\u8a00\u8a2d\u7f6e\u3002\u55ae\u8a5e\u8868\u5fb5\u662f\u57fa\u65bc\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8a5e\u5f59\u8a9e\u7fa9\u548c\u7121\u76e3\u7763\u8a9e\u7fa9\u6587\u672c\u76f8\u4f3c\u6027 (STS) \u4f30\u8a08\u3002\u8207\u73fe\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u4e0d\u9700\u8981\u4eba\u5de5\u8a3b\u89e3\u7684\u8a9e\u6599\u5eab\uff0c\u4e5f\u4e0d\u9700\u8981\u66f4\u65b0\u9810\u5148\u8a13\u7df4\u597d\u7684\u6a21\u578b\u7684\u53c3\u6578\u3002\u5f8c\u8005\u5c0d\u65bc\u5be6\u969b\u5834\u666f\u5f88\u6709\u5438\u5f15\u529b\uff0c\u5728\u9019\u7a2e\u5834\u666f\u4e2d\uff0c\u73fe\u6210\u7684\u9810\u5148\u8a13\u7df4\u597d\u7684\u6a21\u578b\u662f\u4e0d\u540c\u61c9\u7528\u7a0b\u5e8f\u4e4b\u9593\u7684\u5171\u540c\u8cc7\u7522\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5b78\u6703\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u7d50\u5408\u9810\u5148\u8a13\u7df4\u597d\u7684\u6a21\u578b\u7684\u4e0d\u540c\u96b1\u85cf\u5c64\u7684\u8f38\u51fa\u3002\u6211\u5011\u57fa\u65bc\u81ea\u52d5\u7de8\u78bc\u5668\u7684\u8a13\u7df4\u53ea\u9700\u8981\u4e00\u500b\u81ea\u52d5\u751f\u6210\u7684\u8a9e\u6599\u5eab\u3002\u70ba\u4e86\u8a55\u4f30\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u6211\u5011\u4f7f\u7528\u5404\u7a2e\u57fa\u6e96\u4efb\u52d9\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u55ae\u8a9e\u4efb\u52d9\u7684\u7d50\u679c\u8b49\u5be6\uff0c\u8207\u5148\u524d\u7814\u7a76\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u8868\u5fb5\u5728\u57fa\u65bc\u4e0a\u4e0b\u6587\u7684\u8a5e\u5f59\u8a9e\u7fa9\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u4e86\u7af6\u722d\u529b\uff0c\u4e26\u4e14\u5728 STS \u4f30\u8a08\u4e2d\u512a\u65bc\u5b83\u3002\u8de8\u8a9e\u8a00\u4efb\u52d9\u7684\u7d50\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6975\u5927\u5730\u6539\u9032\u4e86\u591a\u8a9e\u8a00\u9810\u5148\u8a13\u7df4\u6a21\u578b\u7684\u8de8\u8a9e\u8a00\u55ae\u8a5e\u8868\u5fb5\u3002</paragraph>", "author": "Yuki Arase et.al.", "authors": "Yuki Arase, Tomoyuki Kajiwara", "id": "2409.08719v1", "paper_url": "http://arxiv.org/abs/2409.08719v1", "repo": "https://github.com/yukiar/distil_wic"}}