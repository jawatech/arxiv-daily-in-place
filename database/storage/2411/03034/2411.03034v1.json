{"2411.03034": {"publish_time": "2024-11-05", "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model", "paper_summary": "Human-scene vision-language tasks are increasingly prevalent in diverse\nsocial applications, yet recent advancements predominantly rely on models\nspecifically tailored to individual tasks. Emerging research indicates that\nlarge vision-language models (VLMs) can enhance performance across various\ndownstream vision-language understanding tasks. However, general-domain models\noften underperform in specialized fields. This study introduces a\ndomain-specific Large Vision-Language Model, Human-Scene Vision-Language Model\n(HumanVLM), designed to provide a foundation for human-scene Vision-Language\ntasks. Specifically, (1) we create a large-scale human-scene multimodal\nimage-text dataset (HumanCaption-10M) sourced from the Internet to facilitate\ndomain-specific alignment; (2) develop a captioning approach for human-centered\nimages, capturing human faces, bodies, and backgrounds, and construct a\nhigh-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs)\nthat contain as much detailed information as possible about human; (3) Using\nHumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments,\nwe then evaluate our HumanVLM across varous downstream tasks, where it\ndemonstrates superior overall performance among multimodal models of comparable\nscale, particularly excelling in human-related tasks and significantly\noutperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM,\nalongside the data introduced, will stimulate the research in human-around\nfields.", "paper_summary_zh": "<paragraph>\u4eba\u666f\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u5728\u591a\u5143\u7684\u793e\u4ea4\u61c9\u7528\u4e2d\u8d8a\u4f86\u8d8a\u666e\u904d\uff0c\u4f46\u8fd1\u671f\u7684\u9032\u5c55\u4e3b\u8981\u4f9d\u8cf4\u65bc\u5c08\u9580\u91dd\u5c0d\u500b\u5225\u4efb\u52d9\u91cf\u8eab\u6253\u9020\u7684\u6a21\u578b\u3002\u65b0\u8208\u7684\u7814\u7a76\u6307\u51fa\uff0c\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u53ef\u4ee5\u63d0\u5347\u5404\u7a2e\u4e0b\u6e38\u8996\u89ba\u8a9e\u8a00\u7406\u89e3\u4efb\u52d9\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u4e00\u822c\u9818\u57df\u6a21\u578b\u5728\u5c08\u696d\u9818\u57df\u4e2d\u5e38\u5e38\u8868\u73fe\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u4ecb\u7d39\u4e00\u7a2e\u7279\u5b9a\u9818\u57df\u7684\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u4eba\u666f\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (HumanVLM)\uff0c\u65e8\u5728\u70ba\u4eba\u666f\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u63d0\u4f9b\u57fa\u790e\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c(1) \u6211\u5011\u5275\u5efa\u4e00\u500b\u5f9e\u7db2\u969b\u7db2\u8def\u53d6\u5f97\u7684\u5927\u578b\u4eba\u666f\u591a\u6a21\u614b\u5f71\u50cf\u6587\u5b57\u8cc7\u6599\u96c6 (HumanCaption-10M)\uff0c\u4ee5\u5229\u7279\u5b9a\u9818\u57df\u7684\u5c0d\u9f4a\uff1b(2) \u70ba\u4ee5\u4eba\u70ba\u4e2d\u5fc3\u7684\u5f71\u50cf\u958b\u767c\u4e00\u7a2e\u6a19\u984c\u65b9\u6cd5\uff0c\u64f7\u53d6\u4eba\u81c9\u3001\u8eab\u9ad4\u548c\u80cc\u666f\uff0c\u4e26\u5efa\u69cb\u4e00\u500b\u9ad8\u54c1\u8cea\u7684\u4eba\u666f\u5f71\u50cf\u6587\u5b57\u8cc7\u6599\u96c6 (HumanCaptionHQ\uff0c\u7d04 311k \u5c0d)\uff0c\u5176\u4e2d\u5305\u542b\u76e1\u53ef\u80fd\u591a\u7684\u4eba\u985e\u8a73\u7d30\u8cc7\u8a0a\uff1b(3) \u4f7f\u7528 HumanCaption-10M \u548c HumanCaptionHQ\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b HumanVLM\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u63a5\u8457\u8a55\u4f30\u6211\u5011\u7684 HumanVLM \u6a6b\u8de8\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\uff0c\u5b83\u5728\u540c\u7b49\u898f\u6a21\u7684\u591a\u6a21\u614b\u6a21\u578b\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6574\u9ad4\u8868\u73fe\uff0c\u7279\u5225\u662f\u5728\u8207\u4eba\u985e\u76f8\u95dc\u7684\u4efb\u52d9\u4e2d\u8868\u73fe\u5091\u51fa\uff0c\u4e14\u5927\u5e45\u512a\u65bc\u985e\u4f3c\u7684\u6a21\u578b\uff0c\u5305\u62ec Qwen2VL \u548c ChatGPT-4o\u3002HumanVLM \u9023\u540c\u6240\u5f15\u5165\u7684\u8cc7\u6599\uff0c\u5c07\u6703\u6fc0\u52f5\u4eba\u666f\u9818\u57df\u7684\u7814\u7a76\u3002</paragraph>", "author": "Dawei Dai et.al.", "authors": "Dawei Dai, Xu Long, Li Yutang, Zhang Yuanhui, Shuyin Xia", "id": "2411.03034v1", "paper_url": "http://arxiv.org/abs/2411.03034v1", "repo": "null"}}