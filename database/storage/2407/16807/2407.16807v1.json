{"2407.16807": {"publish_time": "2024-07-23", "title": "In Search for Architectures and Loss Functions in Multi-Objective Reinforcement Learning", "paper_summary": "Multi-objective reinforcement learning (MORL) is essential for addressing the\nintricacies of real-world RL problems, which often require trade-offs between\nmultiple utility functions. However, MORL is challenging due to unstable\nlearning dynamics with deep learning-based function approximators. The research\npath most taken has been to explore different value-based loss functions for\nMORL to overcome this issue. Our work empirically explores model-free policy\nlearning loss functions and the impact of different architectural choices. We\nintroduce two different approaches: Multi-objective Proximal Policy\nOptimization (MOPPO), which extends PPO to MORL, and Multi-objective Advantage\nActor Critic (MOA2C), which acts as a simple baseline in our ablations. Our\nproposed approach is straightforward to implement, requiring only small\nmodifications at the level of function approximator. We conduct comprehensive\nevaluations on the MORL Deep Sea Treasure, Minecart, and Reacher environments\nand show that MOPPO effectively captures the Pareto front. Our extensive\nablation studies and empirical analyses reveal the impact of different\narchitectural choices, underscoring the robustness and versatility of MOPPO\ncompared to popular MORL approaches like Pareto Conditioned Networks (PCN) and\nEnvelope Q-learning in terms of MORL metrics, including hypervolume and\nexpected utility.", "paper_summary_zh": "\u591a\u76ee\u6a19\u5f37\u5316\u5b78\u7fd2 (MORL) \u5c0d\u65bc\u89e3\u6c7a\u771f\u5be6\u4e16\u754c RL \u554f\u984c\u7684\u8907\u96dc\u6027\u81f3\u95dc\u91cd\u8981\uff0c\u800c\u9019\u4e9b\u554f\u984c\u901a\u5e38\u9700\u8981\u5728\u591a\u500b\u6548\u7528\u51fd\u6578\u4e4b\u9593\u9032\u884c\u6b0a\u8861\u3002\u7136\u800c\uff0c\u7531\u65bc\u57fa\u65bc\u6df1\u5ea6\u5b78\u7fd2\u7684\u51fd\u6578\u903c\u8fd1\u5668\u7684\u4e0d\u7a69\u5b9a\u5b78\u7fd2\u52d5\u614b\uff0cMORL \u5177\u6709\u6311\u6230\u6027\u3002\u7814\u7a76\u6700\u5e38\u63a1\u53d6\u7684\u8def\u5f91\u662f\u63a2\u7d22\u4e0d\u540c\u7684\u57fa\u65bc\u50f9\u503c\u7684\u640d\u5931\u51fd\u6578\uff0c\u4ee5\u514b\u670d MORL \u7684\u9019\u500b\u554f\u984c\u3002\u6211\u5011\u7684\u7814\u7a76\u7d93\u9a57\u6027\u5730\u63a2\u8a0e\u4e86\u7121\u6a21\u578b\u7b56\u7565\u5b78\u7fd2\u640d\u5931\u51fd\u6578\u548c\u4e0d\u540c\u67b6\u69cb\u9078\u64c7\u7684\u5f71\u97ff\u3002\u6211\u5011\u5f15\u5165\u4e86\u5169\u7a2e\u4e0d\u540c\u7684\u65b9\u6cd5\uff1a\u591a\u76ee\u6a19\u8fd1\u7aef\u7b56\u7565\u512a\u5316 (MOPPO)\uff0c\u5b83\u5c07 PPO \u64f4\u5c55\u5230 MORL\uff0c\u4ee5\u53ca\u591a\u76ee\u6a19\u512a\u52e2 Actor Critic (MOA2C)\uff0c\u5b83\u5728\u6211\u5011\u7684\u6d88\u878d\u4e2d\u4f5c\u70ba\u4e00\u500b\u7c21\u55ae\u7684\u57fa\u6e96\u3002\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u6613\u65bc\u5be6\u4f5c\uff0c\u53ea\u9700\u8981\u5728\u51fd\u6578\u903c\u8fd1\u5668\u7684\u5c64\u7d1a\u9032\u884c\u5fae\u5c0f\u7684\u4fee\u6539\u3002\u6211\u5011\u5c0d MORL \u6df1\u6d77\u5bf6\u85cf\u3001\u7926\u8eca\u548c Reacher \u74b0\u5883\u9032\u884c\u4e86\u5168\u9762\u7684\u8a55\u4f30\uff0c\u4e26\u8868\u660e MOPPO \u6709\u6548\u5730\u6355\u6349\u4e86 Pareto \u524d\u7de3\u3002\u6211\u5011\u5ee3\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u548c\u7d93\u9a57\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u67b6\u69cb\u9078\u64c7\u7684\u5f71\u97ff\uff0c\u5f37\u8abf\u4e86 MOPPO \u8207\u6d41\u884c\u7684 MORL \u65b9\u6cd5\uff08\u5982 Pareto \u689d\u4ef6\u7db2\u8def (PCN) \u548c\u4fe1\u5c01 Q \u5b78\u7fd2\uff09\u76f8\u6bd4\u5728 MORL \u6307\u6a19\uff08\u5305\u62ec\u8d85\u9ad4\u7a4d\u548c\u9810\u671f\u6548\u7528\uff09\u65b9\u9762\u7684\u7a69\u5065\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "author": "Mikhail Terekhov et.al.", "authors": "Mikhail Terekhov, Caglar Gulcehre", "id": "2407.16807v1", "paper_url": "http://arxiv.org/abs/2407.16807v1", "repo": "null"}}