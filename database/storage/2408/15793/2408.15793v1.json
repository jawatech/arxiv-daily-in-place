{"2408.15793": {"publish_time": "2024-08-28", "title": "Language Adaptation on a Tight Academic Compute Budget: Tokenizer Swapping Works and Pure bfloat16 Is Enough", "paper_summary": "We investigate continued pretraining of LLMs for language adaptation on a\ntight academic budget: a setting in which only a few GPUs can be used in\nparallel, for a heavily constrained duration. We focus on adapting Mistral-7B\nto German or Arabic and evaluate several techniques to improve efficiency and\neffectiveness in this setting. Our German models adapted on this tight compute\nbudget underperform compared to the base Mistral-7B, while our Arabic models\noutperform several baselines, showing that for sufficiently well-represented\nlanguages, continued pretraining for specialization is not always helpful. Our\nmain findings focus on training precision and tokenizer swapping. Our results\nshow that pure bfloat16 training is a viable alternative to mixed-precision\ntraining, while being much faster when only using a few GPUs. Swapping the\ntokenizer for a specialized one yields more efficient tokenization and is\ncompetitive with the original tokenizer, which already contains some German\ntokens, but did not significantly increase performance for German. Code and\nmodel weights are available at on GitHub.", "paper_summary_zh": "\u6211\u5011\u7814\u7a76\u4e86\u5728\u56b4\u683c\u7684\u5b78\u8853\u9810\u7b97\u4e2d\uff0c\u6301\u7e8c\u9810\u8a13\u7df4 LLM \u4ee5\u9069\u61c9\u8a9e\u8a00\uff1a\u4e00\u7a2e\u53ea\u80fd\u4e26\u884c\u4f7f\u7528\u5c11\u6578 GPU\uff0c\u4e14\u6642\u9593\u53d7\u5230\u56b4\u683c\u9650\u5236\u7684\u8a2d\u5b9a\u3002\u6211\u5011\u5c08\u6ce8\u65bc\u5c07 Mistral-7B \u9069\u61c9\u70ba\u5fb7\u8a9e\u6216\u963f\u62c9\u4f2f\u8a9e\uff0c\u4e26\u8a55\u4f30\u4e86\u5e7e\u7a2e\u6280\u8853\uff0c\u4ee5\u63d0\u9ad8\u6b64\u8a2d\u5b9a\u4e2d\u7684\u6548\u7387\u548c\u6548\u80fd\u3002\u6211\u5011\u5728\u56b4\u683c\u7684\u904b\u7b97\u9810\u7b97\u4e0b\u9069\u61c9\u7684\u5fb7\u8a9e\u6a21\u578b\uff0c\u8207\u57fa\u790e\u7684 Mistral-7B \u76f8\u6bd4\u8868\u73fe\u4e0d\u4f73\uff0c\u800c\u6211\u5011\u7684\u963f\u62c9\u4f2f\u8a9e\u6a21\u578b\u5247\u512a\u65bc\u5e7e\u500b\u57fa\u6e96\uff0c\u986f\u793a\u5c0d\u65bc\u5145\u5206\u4ee3\u8868\u7684\u8a9e\u8a00\uff0c\u6301\u7e8c\u9810\u8a13\u7df4\u4ee5\u9032\u884c\u5c08\u696d\u5316\u4e26\u975e\u7e3d\u662f\u6703\u6709\u5e6b\u52a9\u3002\u6211\u5011\u7684\u91cd\u9ede\u767c\u73fe\u96c6\u4e2d\u65bc\u8a13\u7df4\u7cbe\u5ea6\u548c tokenizer \u4ea4\u63db\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u7d14 bfloat16 \u8a13\u7df4\u662f\u6df7\u5408\u7cbe\u5ea6\u8a13\u7df4\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u540c\u6642\u5728\u50c5\u4f7f\u7528\u5c11\u6578 GPU \u6642\u901f\u5ea6\u5feb\u5f88\u591a\u3002\u5c07 tokenizer \u4ea4\u63db\u70ba\u5c08\u9580\u7684 tokenizer \u53ef\u7522\u751f\u66f4\u6709\u6548\u7387\u7684 tokenization\uff0c\u4e26\u4e14\u8207\u539f\u59cb tokenizer\uff08\u5176\u4e2d\u5df2\u5305\u542b\u4e00\u4e9b\u5fb7\u8a9e token\uff09\u5177\u6709\u7af6\u722d\u529b\uff0c\u4f46\u4e26\u672a\u986f\u8457\u63d0\u5347\u5fb7\u8a9e\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u6b0a\u91cd\u53ef\u5728 GitHub \u4e0a\u53d6\u5f97\u3002", "author": "Konstantin Dobler et.al.", "authors": "Konstantin Dobler, Gerard de Melo", "id": "2408.15793v1", "paper_url": "http://arxiv.org/abs/2408.15793v1", "repo": "https://github.com/konstantinjdobler/tight-budget-llm-adaptation"}}