{"2407.16664": {"publish_time": "2024-07-23", "title": "Towards scalable efficient on-device ASR with transfer learning", "paper_summary": "Multilingual pretraining for transfer learning significantly boosts the\nrobustness of low-resource monolingual ASR models. This study systematically\ninvestigates three main aspects: (a) the impact of transfer learning on model\nperformance during initial training or fine-tuning, (b) the influence of\ntransfer learning across dataset domains and languages, and (c) the effect on\nrare-word recognition compared to non-rare words. Our finding suggests that\nRNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word\nError Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across\nlanguages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8%\ncompared to monolingual baselines for MLS and in-house datasets. Out-of-domain\npretraining leads to 28% higher WERR than in-domain pretraining. Both rare and\nnon-rare words benefit, with rare words showing greater improvements with\nout-of-domain pretraining, and non-rare words with in-domain pretraining.", "paper_summary_zh": "\u591a\u8a9e\u8a00\u9810\u8a13\u7df4\u7528\u65bc\u9077\u79fb\u5b78\u7fd2\uff0c\u53ef\u986f\u8457\u63d0\u5347\u4f4e\u8cc7\u6e90\u55ae\u8a9e ASR \u6a21\u578b\u7684\u7a69\u5065\u6027\u3002\u672c\u7814\u7a76\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u4e86\u4e09\u500b\u4e3b\u8981\u9762\u5411\uff1a(a) \u9077\u79fb\u5b78\u7fd2\u5c0d\u6a21\u578b\u6548\u80fd\u7684\u5f71\u97ff\uff0c\u7121\u8ad6\u662f\u5728\u521d\u59cb\u8a13\u7df4\u6216\u5fae\u8abf\u671f\u9593\uff0c(b) \u9077\u79fb\u5b78\u7fd2\u5c0d\u8cc7\u6599\u96c6\u7db2\u57df\u548c\u8a9e\u8a00\u7684\u5f71\u97ff\uff0c\u4ee5\u53ca (c) \u5c0d\u7f55\u898b\u5b57\u8fa8\u8b58\u8207\u975e\u7f55\u898b\u5b57\u8fa8\u8b58\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u767c\u73fe\u986f\u793a\uff0cRNNT \u640d\u5931\u9810\u8a13\u7df4\uff0c\u63a5\u8457\u9032\u884c\u4f7f\u7528\u6700\u5c0f\u5b57\u5143\u932f\u8aa4\u7387 (MinWER) \u640d\u5931\u7684\u55ae\u8a9e\u5fae\u8abf\uff0c\u6703\u6301\u7e8c\u964d\u4f4e\u7fa9\u5927\u5229\u8a9e\u548c\u6cd5\u8a9e\u7b49\u8a9e\u8a00\u7684\u5b57\u5143\u932f\u8aa4\u7387 (WER)\u3002\u8207 MLS \u548c\u5167\u90e8\u8cc7\u6599\u96c6\u7684\u55ae\u8a9e\u57fa\u7dda\u76f8\u6bd4\uff0cWER \u964d\u4f4e\u5e45\u5ea6 (WERR) \u9054\u5230 36.2% \u548c 42.8%\u3002\u9818\u57df\u5916\u9810\u8a13\u7df4\u5c0e\u81f4 WERR \u6bd4\u9818\u57df\u5167\u9810\u8a13\u7df4\u9ad8\u51fa 28%\u3002\u7f55\u898b\u5b57\u548c\u975e\u7f55\u898b\u5b57\u5747\u53d7\u60e0\uff0c\u7f55\u898b\u5b57\u5728\u9818\u57df\u5916\u9810\u8a13\u7df4\u4e2d\u8868\u73fe\u51fa\u8f03\u5927\u7684\u9032\u6b65\uff0c\u800c\u975e\u7f55\u898b\u5b57\u5247\u5728\u9818\u57df\u5167\u9810\u8a13\u7df4\u4e2d\u8868\u73fe\u51fa\u8f03\u5927\u7684\u9032\u6b65\u3002", "author": "Laxmi Pandey et.al.", "authors": "Laxmi Pandey, Ke Li, Jinxi Guo, Debjyoti Paul, Arthur Guo, Jay Mahadeokar, Xuedong Zhang", "id": "2407.16664v1", "paper_url": "http://arxiv.org/abs/2407.16664v1", "repo": "null"}}