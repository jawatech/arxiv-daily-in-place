{"2501.13344": {"publish_time": "2025-01-23", "title": "Full-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation", "paper_summary": "In this paper, we address the lifelong sequential behavior incomprehension\nproblem in large language models (LLMs) for recommendation, where LLMs struggle\nto extract useful information from long user behavior sequences, even within\ntheir context limits. To tackle this, we propose ReLLaX (Retrieval-enhanced\nLarge Language models Plus), a framework offering optimization across data,\nprompt, and parameter levels. At the data level, we introduce Semantic User\nBehavior Retrieval (SUBR) to reduce sequence heterogeneity, making it easier\nfor LLMs to extract key information. For prompt-level enhancement, we employ\nSoft Prompt Augmentation (SPA) to inject collaborative knowledge, aligning item\nrepresentations with recommendation tasks and improving LLMs's exploration of\nitem relationships. Finally, at the parameter level, we propose Component\nFully-interactive LoRA (CFLoRA), which enhances LoRA's expressiveness by\nenabling interactions between its components, allowing better capture of\nsequential information. Moreover, we present new perspectives to compare\ncurrent LoRA-based LLM4Rec methods, i.e. from both a composite and a decomposed\nview. We theoretically demonstrate that the ways they employ LoRA for\nrecommendation are degraded versions of our CFLoRA, with different constraints\non atom component interactions. Extensive experiments on three public datasets\ndemonstrate ReLLaX's superiority over existing baselines and its ability to\nmitigate lifelong sequential behavior incomprehension effectively.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u63a8\u85a6\u4e2d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u7d42\u751f\u9806\u5e8f\u884c\u70ba\u96e3\u4ee5\u7406\u89e3\u7684\u554f\u984c\uff0c\u5176\u4e2d LLM \u96e3\u4ee5\u5f9e\u9577\u7528\u6236\u884c\u70ba\u5e8f\u5217\u4e2d\u63d0\u53d6\u6709\u7528\u7684\u8cc7\u8a0a\uff0c\u5373\u4f7f\u5728\u5b83\u5011\u7684\u5167\u5bb9\u9650\u5236\u5167\u4e5f\u662f\u5982\u6b64\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 ReLLaX\uff08\u6aa2\u7d22\u589e\u5f37\u578b\u5927\u578b\u8a9e\u8a00\u6a21\u578b Plus\uff09\uff0c\u4e00\u500b\u63d0\u4f9b\u8de8\u8cc7\u6599\u3001\u63d0\u793a\u548c\u53c3\u6578\u5c64\u7d1a\u6700\u4f73\u5316\u7684\u67b6\u69cb\u3002\u5728\u8cc7\u6599\u5c64\u7d1a\uff0c\u6211\u5011\u5f15\u5165\u4e86\u8a9e\u7fa9\u4f7f\u7528\u8005\u884c\u70ba\u6aa2\u7d22 (SUBR) \u4ee5\u6e1b\u5c11\u5e8f\u5217\u7570\u8cea\u6027\uff0c\u8b93 LLM \u66f4\u5bb9\u6613\u63d0\u53d6\u95dc\u9375\u8cc7\u8a0a\u3002\u5c0d\u65bc\u63d0\u793a\u5c64\u7d1a\u7684\u589e\u5f37\uff0c\u6211\u5011\u63a1\u7528\u4e86\u8edf\u63d0\u793a\u64f4\u5145 (SPA) \u4f86\u6ce8\u5165\u5354\u4f5c\u77e5\u8b58\uff0c\u5c07\u9805\u76ee\u8868\u793a\u8207\u63a8\u85a6\u4efb\u52d9\u5c0d\u9f4a\uff0c\u4e26\u6539\u5584 LLM \u5c0d\u9805\u76ee\u95dc\u4fc2\u7684\u63a2\u7d22\u3002\u6700\u5f8c\uff0c\u5728\u53c3\u6578\u5c64\u7d1a\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7d44\u4ef6\u5b8c\u5168\u4e92\u52d5 LoRA (CFLoRA)\uff0c\u5b83\u900f\u904e\u555f\u7528\u5176\u7d44\u4ef6\u4e4b\u9593\u7684\u4e92\u52d5\u4f86\u589e\u5f37 LoRA \u7684\u8868\u9054\u529b\uff0c\u5f9e\u800c\u53ef\u4ee5\u66f4\u597d\u5730\u64f7\u53d6\u9806\u5e8f\u8cc7\u8a0a\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u65b0\u7684\u89c0\u9ede\u4f86\u6bd4\u8f03\u76ee\u524d\u7684\u57fa\u65bc LoRA \u7684 LLM4Rec \u65b9\u6cd5\uff0c\u5373\u5f9e\u8907\u5408\u548c\u5206\u89e3\u7684\u89c0\u9ede\u4f86\u770b\u3002\u6211\u5011\u5728\u7406\u8ad6\u4e0a\u8b49\u660e\u4e86\u5b83\u5011\u4f7f\u7528 LoRA \u9032\u884c\u63a8\u85a6\u7684\u65b9\u5f0f\u662f\u6211\u5011 CFLoRA \u7684\u7c21\u5316\u7248\u672c\uff0c\u5c0d\u539f\u5b50\u7d44\u4ef6\u4e92\u52d5\u6709\u4e0d\u540c\u7684\u9650\u5236\u3002\u5728\u4e09\u500b\u516c\u958b\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5927\u91cf\u5be6\u9a57\u8b49\u660e\u4e86 ReLLaX \u512a\u65bc\u73fe\u6709\u7684\u57fa\u6e96\uff0c\u4e26\u4e14\u80fd\u5920\u6709\u6548\u5730\u7de9\u89e3\u7d42\u751f\u7684\u9806\u5e8f\u884c\u70ba\u96e3\u4ee5\u7406\u89e3\u7684\u554f\u984c\u3002</paragraph>", "author": "Rong Shan et.al.", "authors": "Rong Shan, Jiachen Zhu, Jianghao Lin, Chenxu Zhu, Bo Chen, Ruiming Tang, Yong Yu, Weinan Zhang", "id": "2501.13344v1", "paper_url": "http://arxiv.org/abs/2501.13344v1", "repo": "null"}}