{"2502.06742": {"publish_time": "2025-02-10", "title": "Gradient Multi-Normalization for Stateless and Scalable LLM Training", "paper_summary": "Training large language models (LLMs) typically relies on adaptive optimizers\nlike Adam (Kingma & Ba, 2015) which store additional state information to\naccelerate convergence but incur significant memory overhead. Recent efforts,\nsuch as SWAN (Ma et al., 2024) address this by eliminating the need for\noptimizer states while achieving performance comparable to Adam via a\nmulti-step preprocessing procedure applied to instantaneous gradients.\nMotivated by the success of SWAN, we introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gradients according to multiple\nnorms. To achieve this, we propose a simple alternating scheme to enforce the\nnormalization of gradients w.r.t these norms. We show that our procedure can\nproduce, up to an arbitrary precision, a fixed-point of the problem, and that\nSWAN is a particular instance of our approach with carefully chosen norms,\nproviding a deeper understanding of its design. However, SWAN's computationally\nexpensive whitening/orthogonalization step limit its practicality for large\nLMs. Using our principled perspective, we develop of a more efficient,\nscalable, and practical stateless optimizer. Our algorithm relaxes the\nproperties of SWAN, significantly reducing its computational cost while\nretaining its memory efficiency, making it applicable to training large-scale\nmodels. Experiments on pre-training LLaMA models with up to 1 billion\nparameters demonstrate a 3X speedup over Adam with significantly reduced memory\nrequirements, outperforming other memory-efficient baselines.", "paper_summary_zh": "\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u901a\u5e38\u4f9d\u8cf4\u81ea\u9069\u61c9\u6700\u4f73\u5316\u5668\uff0c\u4f8b\u5982 Adam (Kingma & Ba, 2015)\uff0c\u5b83\u5132\u5b58\u984d\u5916\u7684\u72c0\u614b\u8cc7\u8a0a\uff0c\u4ee5\u52a0\u901f\u6536\u6582\uff0c\u4f46\u6703\u7522\u751f\u986f\u8457\u7684\u8a18\u61b6\u9ad4\u958b\u92b7\u3002\u6700\u8fd1\u7684\u52aa\u529b\uff0c\u4f8b\u5982 SWAN (Ma \u7b49\u4eba\uff0c2024)\uff0c\u900f\u904e\u6d88\u9664\u6700\u4f73\u5316\u5668\u72c0\u614b\u7684\u9700\u6c42\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u540c\u6642\u900f\u904e\u61c9\u7528\u65bc\u77ac\u6642\u68af\u5ea6\u7684\u591a\u6b65\u9a5f\u9810\u8655\u7406\u7a0b\u5e8f\uff0c\u9054\u5230\u8207 Adam \u76f8\u7576\u7684\u6548\u80fd\u3002\u53d7 SWAN \u6210\u529f\u555f\u767c\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u7528\u65bc\u8a2d\u8a08\u7121\u72c0\u614b\u6700\u4f73\u5316\u5668\uff0c\u6839\u64da\u591a\u500b\u5e38\u614b\u5316\u96a8\u6a5f\u68af\u5ea6\u3002\u70ba\u4e86\u9054\u6210\u9019\u500b\u76ee\u7684\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u7684\u4ea4\u66ff\u65b9\u6848\uff0c\u4ee5\u5f37\u5236\u57f7\u884c\u91dd\u5c0d\u9019\u4e9b\u5e38\u614b\u7684\u68af\u5ea6\u6b63\u898f\u5316\u3002\u6211\u5011\u5c55\u793a\u6211\u5011\u7684\u7a0b\u5e8f\u53ef\u4ee5\u7522\u751f\u4e00\u500b\u554f\u984c\u7684\u5b9a\u9ede\uff0c\u7cbe\u78ba\u5ea6\u53ef\u9054\u4efb\u610f\u503c\uff0c\u800c SWAN \u662f\u6211\u5011\u7684\u505a\u6cd5\u7684\u4e00\u500b\u7279\u5b9a\u7bc4\u4f8b\uff0c\u5176\u4e2d\u5e38\u614b\u7d93\u904e\u4ed4\u7d30\u6311\u9078\uff0c\u63d0\u4f9b\u4e86\u5c0d\u5176\u8a2d\u8a08\u66f4\u6df1\u5165\u7684\u7406\u89e3\u3002\u7136\u800c\uff0cSWAN \u5728\u8a08\u7b97\u4e0a\u6602\u8cb4\u7684\u6f02\u767d/\u6b63\u4ea4\u5316\u6b65\u9a5f\u9650\u5236\u4e86\u5b83\u5c0d\u5927\u578b LLM \u7684\u5be6\u7528\u6027\u3002\u5229\u7528\u6211\u5011\u6709\u539f\u5247\u7684\u89c0\u9ede\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u66f4\u6709\u6548\u7387\u3001\u53ef\u64f4\u5145\u4e14\u5be6\u7528\u7684\u7121\u72c0\u614b\u6700\u4f73\u5316\u5668\u3002\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u653e\u5bec\u4e86 SWAN \u7684\u5c6c\u6027\uff0c\u5927\u5e45\u964d\u4f4e\u5176\u8a08\u7b97\u6210\u672c\uff0c\u540c\u6642\u4fdd\u7559\u5176\u8a18\u61b6\u9ad4\u6548\u7387\uff0c\u4f7f\u5176\u9069\u7528\u65bc\u8a13\u7df4\u5927\u578b\u6a21\u578b\u3002\u4f7f\u7528\u9ad8\u9054 10 \u5104\u500b\u53c3\u6578\u7684\u9810\u8a13\u7df4 LLaMA \u6a21\u578b\u9032\u884c\u7684\u5be6\u9a57\uff0c\u8b49\u660e\u5176\u901f\u5ea6\u6bd4 Adam \u5feb 3 \u500d\uff0c\u540c\u6642\u5927\u5e45\u964d\u4f4e\u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u512a\u65bc\u5176\u4ed6\u8a18\u61b6\u9ad4\u6548\u7387\u57fa\u6e96\u3002", "author": "Meyer Scetbon et.al.", "authors": "Meyer Scetbon, Chao Ma, Wenbo Gong, Edward Meeds", "id": "2502.06742v1", "paper_url": "http://arxiv.org/abs/2502.06742v1", "repo": "null"}}