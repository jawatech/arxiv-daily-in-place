{"2406.03997": {"publish_time": "2024-06-06", "title": "HackAtari: Atari Learning Environments for Robust and Continual Reinforcement Learning", "paper_summary": "Artificial agents' adaptability to novelty and alignment with intended\nbehavior is crucial for their effective deployment. Reinforcement learning (RL)\nleverages novelty as a means of exploration, yet agents often struggle to\nhandle novel situations, hindering generalization. To address these issues, we\npropose HackAtari, a framework introducing controlled novelty to the most\ncommon RL benchmark, the Atari Learning Environment. HackAtari allows us to\ncreate novel game scenarios (including simplification for curriculum learning),\nto swap the game elements' colors, as well as to introduce different reward\nsignals for the agent. We demonstrate that current agents trained on the\noriginal environments include robustness failures, and evaluate HackAtari's\nefficacy in enhancing RL agents' robustness and aligning behavior through\nexperiments using C51 and PPO. Overall, HackAtari can be used to improve the\nrobustness of current and future RL algorithms, allowing Neuro-Symbolic RL,\ncurriculum RL, causal RL, as well as LLM-driven RL. Our work underscores the\nsignificance of developing interpretable in RL agents.", "paper_summary_zh": "\u4eba\u5de5\u4ee3\u7406\u7684\u9002\u5e94\u65b0\u7a4e\u6027\u548c\u8207\u9810\u671f\u884c\u70ba\u4e00\u81f4\u5c0d\u5176\u6709\u6548\u90e8\u7f72\u81f3\u95dc\u91cd\u8981\u3002\u5f37\u5316\u5b78\u7fd2 (RL) \u5229\u7528\u65b0\u7a4e\u6027\u4f5c\u70ba\u63a2\u7d22\u7684\u624b\u6bb5\uff0c\u4f46\u4ee3\u7406\u901a\u5e38\u96e3\u4ee5\u61c9\u5c0d\u65b0\u7a4e\u60c5\u6cc1\uff0c\u963b\u7919\u6cdb\u5316\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 HackAtari\uff0c\u4e00\u500b\u5728\u6700\u5e38\u898b\u7684 RL \u57fa\u6e96 Atari \u5b78\u7fd2\u74b0\u5883\u4e2d\u5f15\u5165\u53d7\u63a7\u65b0\u7a4e\u6027\u7684\u6846\u67b6\u3002HackAtari \u5141\u8a31\u6211\u5011\u5275\u5efa\u65b0\u7a4e\u7684\u904a\u6232\u5834\u666f\uff08\u5305\u62ec\u7c21\u5316\u8ab2\u7a0b\u5b78\u7fd2\uff09\u3001\u4ea4\u63db\u904a\u6232\u5143\u7d20\u7684\u984f\u8272\uff0c\u4ee5\u53ca\u70ba\u4ee3\u7406\u5f15\u5165\u4e0d\u540c\u7684\u734e\u52f5\u4fe1\u865f\u3002\u6211\u5011\u8b49\u660e\u4e86\u5728\u539f\u59cb\u74b0\u5883\u4e2d\u8a13\u7df4\u7684\u7576\u524d\u4ee3\u7406\u5305\u62ec\u7a69\u5065\u6027\u6545\u969c\uff0c\u4e26\u901a\u904e\u4f7f\u7528 C51 \u548c PPO \u7684\u5be6\u9a57\u8a55\u4f30\u4e86 HackAtari \u5728\u589e\u5f37 RL \u4ee3\u7406\u7684\u7a69\u5065\u6027\u548c\u8abf\u6574\u884c\u70ba\u65b9\u9762\u7684\u529f\u6548\u3002\u7e3d\u7684\u4f86\u8aaa\uff0cHackAtari \u53ef\u7528\u65bc\u63d0\u9ad8\u7576\u524d\u548c\u672a\u4f86 RL \u6f14\u7b97\u6cd5\u7684\u7a69\u5065\u6027\uff0c\u5141\u8a31\u795e\u7d93\u7b26\u865f RL\u3001\u8ab2\u7a0b RL\u3001\u56e0\u679c RL\uff0c\u4ee5\u53ca LLM \u9a45\u52d5\u7684 RL\u3002\u6211\u5011\u7684\u7814\u7a76\u5f37\u8abf\u4e86\u5728 RL \u4ee3\u7406\u4e2d\u958b\u767c\u53ef\u89e3\u91cb\u6027\u7684\u91cd\u8981\u6027\u3002", "author": "Quentin Delfosse et.al.", "authors": "Quentin Delfosse, Jannis Bl\u00fcml, Bjarne Gregori, Kristian Kersting", "id": "2406.03997v1", "paper_url": "http://arxiv.org/abs/2406.03997v1", "repo": "null"}}