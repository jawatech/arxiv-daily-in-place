{"2408.15980": {"publish_time": "2024-08-28", "title": "In-Context Imitation Learning via Next-Token Prediction", "paper_summary": "We explore how to enhance next-token prediction models to perform in-context\nimitation learning on a real robot, where the robot executes new tasks by\ninterpreting contextual information provided during the input phase, without\nupdating its underlying policy parameters. We propose In-Context Robot\nTransformer (ICRT), a causal transformer that performs autoregressive\nprediction on sensorimotor trajectories without relying on any linguistic data\nor reward function. This formulation enables flexible and training-free\nexecution of new tasks at test time, achieved by prompting the model with\nsensorimotor trajectories of the new task composing of image observations,\nactions and states tuples, collected through human teleoperation. Experiments\nwith a Franka Emika robot demonstrate that the ICRT can adapt to new tasks\nspecified by prompts, even in environment configurations that differ from both\nthe prompt and the training data. In a multitask environment setup, ICRT\nsignificantly outperforms current state-of-the-art next-token prediction models\nin robotics on generalizing to unseen tasks. Code, checkpoints and data are\navailable on https://icrt.dev/", "paper_summary_zh": "\u6211\u5011\u63a2\u8a0e\u5982\u4f55\u589e\u5f37\u4e0b\u4e00\u500b\u4ee3\u5e63\u9810\u6e2c\u6a21\u578b\uff0c\u4ee5\u4fbf\u5728\u771f\u5be6\u6a5f\u5668\u4eba\u4e0a\u57f7\u884c\u60c5\u5883\u5167\u6a21\u4eff\u5b78\u7fd2\uff0c\u5176\u4e2d\u6a5f\u5668\u4eba\u5728\u8f38\u5165\u968e\u6bb5\u89e3\u91cb\u63d0\u4f9b\u7684\u80cc\u666f\u8cc7\u8a0a\u4f86\u57f7\u884c\u65b0\u4efb\u52d9\uff0c\u800c\u4e0d\u6703\u66f4\u65b0\u5176\u57fa\u790e\u7b56\u7565\u53c3\u6578\u3002\u6211\u5011\u63d0\u51fa\u60c5\u5883\u5167\u6a5f\u5668\u4eba\u8f49\u63db\u5668 (ICRT)\uff0c\u9019\u662f\u4e00\u500b\u56e0\u679c\u8f49\u63db\u5668\uff0c\u5c0d\u611f\u6e2c\u904b\u52d5\u8ecc\u8de1\u57f7\u884c\u81ea\u8ff4\u6b78\u9810\u6e2c\uff0c\u800c\u4e0d\u4f9d\u8cf4\u4efb\u4f55\u8a9e\u8a00\u8cc7\u6599\u6216\u734e\u52f5\u51fd\u6578\u3002\u6b64\u516c\u5f0f\u5316\u80fd\u9748\u6d3b\u4e14\u7121\u9700\u8a13\u7df4\u5730\u57f7\u884c\u65b0\u4efb\u52d9\u65bc\u6e2c\u8a66\u6642\u9593\uff0c\u9019\u662f\u900f\u904e\u63d0\u793a\u6a21\u578b\u5305\u542b\u5f71\u50cf\u89c0\u5bdf\u3001\u52d5\u4f5c\u548c\u72c0\u614b\u5143\u7d44\u7684\u65b0\u4efb\u52d9\u611f\u6e2c\u904b\u52d5\u8ecc\u8de1\uff0c\u7d93\u7531\u4eba\u70ba\u9059\u63a7\u6536\u96c6\u800c\u9054\u6210\u3002\u4f7f\u7528 Franka Emika \u6a5f\u5668\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u5373\u4f7f\u5728\u63d0\u793a\u548c\u8a13\u7df4\u8cc7\u6599\u4e0d\u540c\u7684\u74b0\u5883\u7d44\u614b\u4e2d\uff0cICRT \u4e5f\u80fd\u9069\u61c9\u7531\u63d0\u793a\u6307\u5b9a\u7684\u4efb\u52d9\u3002\u5728\u591a\u4efb\u52d9\u74b0\u5883\u8a2d\u5b9a\u4e2d\uff0cICRT \u5728\u6a5f\u5668\u4eba\u5b78\u4e0a\u986f\u8457\u512a\u65bc\u76ee\u524d\u6700\u5148\u9032\u7684\u4e0b\u4e00\u500b\u4ee3\u5e63\u9810\u6e2c\u6a21\u578b\uff0c\u4e26\u63a8\u5ee3\u5230\u672a\u898b\u4efb\u52d9\u3002\u7a0b\u5f0f\u78bc\u3001\u6aa2\u67e5\u9ede\u548c\u8cc7\u6599\u53ef\u65bc https://icrt.dev/ \u53d6\u5f97", "author": "Letian Fu et.al.", "authors": "Letian Fu, Huang Huang, Gaurav Datta, Lawrence Yunliang Chen, William Chung-Ho Panitch, Fangchen Liu, Hui Li, Ken Goldberg", "id": "2408.15980v1", "paper_url": "http://arxiv.org/abs/2408.15980v1", "repo": "null"}}