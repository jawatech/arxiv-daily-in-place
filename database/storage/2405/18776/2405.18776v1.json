{"2405.18776": {"publish_time": "2024-05-29", "title": "LMO-DP: Optimizing the Randomization Mechanism for Differentially Private Fine-Tuning (Large) Language Models", "paper_summary": "Differentially Private Stochastic Gradient Descent (DP-SGD) and its variants\nhave been proposed to ensure rigorous privacy for fine-tuning large-scale\npre-trained language models. However, they rely heavily on the Gaussian\nmechanism, which may overly perturb the gradients and degrade the accuracy,\nespecially in stronger privacy regimes (e.g., the privacy budget $\\epsilon <\n3$). To address such limitations, we propose a novel Language Model-based\nOptimal Differential Privacy (LMO-DP) mechanism, which takes the first step to\nenable the tight composition of accurately fine-tuning (large) language models\nwith a sub-optimal DP mechanism, even in strong privacy regimes (e.g., $0.1\\leq\n\\epsilon<3$). Furthermore, we propose a novel offline optimal noise search\nmethod to efficiently derive the sub-optimal DP that significantly reduces the\nnoise magnitude. For instance, fine-tuning RoBERTa-large (with 300M parameters)\non the SST-2 dataset can achieve an accuracy of 92.20% (given $\\epsilon=0.3$,\n$\\delta=10^{-10}$) by drastically outperforming the Gaussian mechanism (e.g.,\n$\\sim 50\\%$ for small $\\epsilon$ and $\\delta$). We also draw similar findings\non the text generation tasks on GPT-2. Finally, to our best knowledge, LMO-DP\nis also the first solution to accurately fine-tune Llama-2 with strong\ndifferential privacy guarantees. The code will be released soon and available\nupon request.", "paper_summary_zh": "\u5dee\u7570\u5316\u79c1\u4eba\u96a8\u6a5f\u68af\u5ea6\u4e0b\u964d (DP-SGD) \u53ca\u5176\u8b8a\u9ad4\u5df2\u88ab\u63d0\u51fa\uff0c\u4ee5\u78ba\u4fdd\u5c0d\u5927\u578b\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u9032\u884c\u5fae\u8abf\u7684\u56b4\u683c\u96b1\u79c1\u3002\u7136\u800c\uff0c\u5b83\u5011\u56b4\u91cd\u4f9d\u8cf4\u65bc\u9ad8\u65af\u6a5f\u5236\uff0c\u9019\u53ef\u80fd\u6703\u904e\u5ea6\u64fe\u52d5\u68af\u5ea6\u4e26\u964d\u4f4e\u6e96\u78ba\u6027\uff0c\u7279\u5225\u662f\u5728\u66f4\u5f37\u7684\u96b1\u79c1\u5236\u5ea6\u4e2d\uff08\u4f8b\u5982\uff0c\u96b1\u79c1\u9810\u7b97 $\\epsilon < 3$\uff09\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u57fa\u65bc\u8a9e\u8a00\u6a21\u578b\u7684\u6700\u4f73\u5dee\u7570\u96b1\u79c1 (LMO-DP) \u6a5f\u5236\uff0c\u5b83\u9081\u51fa\u4e86\u7b2c\u4e00\u6b65\uff0c\u5373\u4f7f\u5728\u5f37\u96b1\u79c1\u5236\u5ea6\uff08\u4f8b\u5982\uff0c$0.1\\leq \\epsilon<3$\uff09\u4e0b\uff0c\u4e5f\u80fd\u5920\u5c0d\uff08\u5927\u578b\uff09\u8a9e\u8a00\u6a21\u578b\u9032\u884c\u6e96\u78ba\u5fae\u8abf\u7684\u56b4\u5bc6\u7d44\u5408\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u96e2\u7dda\u6700\u4f73\u96dc\u8a0a\u641c\u5c0b\u65b9\u6cd5\uff0c\u4ee5\u6709\u6548\u5730\u63a8\u5c0e\u51fa\u986f\u8457\u964d\u4f4e\u96dc\u8a0a\u5e45\u5ea6\u7684\u6b21\u6700\u4f73 DP\u3002\u4f8b\u5982\uff0c\u5728 SST-2 \u8cc7\u6599\u96c6\u4e0a\u5c0d RoBERTa-large\uff08\u5177\u6709 300M \u53c3\u6578\uff09\u9032\u884c\u5fae\u8abf\uff0c\u53ef\u4ee5\u9054\u5230 92.20% \u7684\u6e96\u78ba\u5ea6\uff08\u7d66\u5b9a $\\epsilon=0.3$\uff0c$\\delta=10^{-10}$\uff09\uff0c\u5927\u5e45\u512a\u65bc\u9ad8\u65af\u6a5f\u5236\uff08\u4f8b\u5982\uff0c\u5c0d\u65bc\u8f03\u5c0f\u7684 $\\epsilon$ \u548c $\\delta$\uff0c\u7d04\u70ba 50%\uff09\u3002\u6211\u5011\u9084\u5728 GPT-2 \u4e0a\u7684\u6587\u672c\u751f\u6210\u4efb\u52d9\u4e2d\u5f97\u51fa\u4e86\u985e\u4f3c\u7684\u767c\u73fe\u3002\u6700\u5f8c\uff0c\u64da\u6211\u5011\u6240\u77e5\uff0cLMO-DP \u4e5f\u662f\u7b2c\u4e00\u500b\u6e96\u78ba\u5fae\u8abf Llama-2 \u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u4e26\u5177\u6709\u5f37\u5927\u7684\u5dee\u7570\u96b1\u79c1\u4fdd\u8b49\u3002\u8a72\u7a0b\u5f0f\u78bc\u5c07\u5f88\u5feb\u767c\u5e03\uff0c\u4e26\u53ef\u61c9\u8981\u6c42\u63d0\u4f9b\u3002", "author": "Qin Yang et.al.", "authors": "Qin Yang, Meisam Mohammad, Han Wang, Ali Payani, Ashish Kundu, Kai Shu, Yan Yan, Yuan Hong", "id": "2405.18776v1", "paper_url": "http://arxiv.org/abs/2405.18776v1", "repo": "null"}}