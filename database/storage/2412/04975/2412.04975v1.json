{"2412.04975": {"publish_time": "2024-12-06", "title": "PETapter: Leveraging PET-style classification heads for modular few-shot parameter-efficient fine-tuning", "paper_summary": "Few-shot learning and parameter-efficient fine-tuning (PEFT) are crucial to\novercome the challenges of data scarcity and ever growing language model sizes.\nThis applies in particular to specialized scientific domains, where researchers\nmight lack expertise and resources to fine-tune high-performing language models\nto nuanced tasks. We propose PETapter, a novel method that effectively combines\nPEFT methods with PET-style classification heads to boost few-shot learning\ncapabilities without the significant computational overhead typically\nassociated with full model training. We validate our approach on three\nestablished NLP benchmark datasets and one real-world dataset from\ncommunication research. We show that PETapter not only achieves comparable\nperformance to full few-shot fine-tuning using pattern-exploiting training\n(PET), but also provides greater reliability and higher parameter efficiency\nwhile enabling higher modularity and easy sharing of the trained modules, which\nenables more researchers to utilize high-performing NLP-methods in their\nresearch.", "paper_summary_zh": "\u5c0f\u6a23\u672c\u5b78\u7fd2\u548c\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u5c0d\u65bc\u514b\u670d\u8cc7\u6599\u7a00\u5c11\u548c\u8a9e\u8a00\u6a21\u578b\u5c3a\u5bf8\u6301\u7e8c\u589e\u9577\u7684\u6311\u6230\u81f3\u95dc\u91cd\u8981\u3002\u9019\u7279\u5225\u9069\u7528\u65bc\u5c08\u696d\u79d1\u5b78\u9818\u57df\uff0c\u7814\u7a76\u4eba\u54e1\u53ef\u80fd\u7f3a\u4e4f\u5c08\u696d\u77e5\u8b58\u548c\u8cc7\u6e90\u4f86\u5fae\u8abf\u9ad8\u6027\u80fd\u8a9e\u8a00\u6a21\u578b\u4ee5\u57f7\u884c\u7d30\u5fae\u4efb\u52d9\u3002\u6211\u5011\u63d0\u51fa PETapter\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u5730\u5c07 PEFT \u65b9\u6cd5\u8207 PET \u98a8\u683c\u5206\u985e\u6a19\u982d\u76f8\u7d50\u5408\uff0c\u4ee5\u63d0\u5347\u5c0f\u6a23\u672c\u5b78\u7fd2\u80fd\u529b\uff0c\u800c\u4e0d\u6703\u7522\u751f\u901a\u5e38\u8207\u5b8c\u6574\u6a21\u578b\u8a13\u7df4\u76f8\u95dc\u7684\u986f\u8457\u8a08\u7b97\u958b\u92b7\u3002\u6211\u5011\u5728\u4e09\u500b\u65e2\u5b9a\u7684 NLP \u57fa\u6e96\u8cc7\u6599\u96c6\u548c\u4e00\u500b\u4f86\u81ea\u6e9d\u901a\u7814\u7a76\u7684\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u4e0a\u9a57\u8b49\u4e86\u6211\u5011\u7684\u505a\u6cd5\u3002\u6211\u5011\u8868\u660e\uff0cPETapter \u4e0d\u50c5\u4f7f\u7528\u6a21\u5f0f\u5229\u7528\u8a13\u7df4 (PET) \u9054\u5230\u4e86\u8207\u5b8c\u6574\u5c0f\u6a23\u672c\u5fae\u8abf\u76f8\u7576\u7684\u6548\u80fd\uff0c\u800c\u4e14\u9084\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u53ef\u9760\u6027\u548c\u66f4\u9ad8\u7684\u53c3\u6578\u6548\u7387\uff0c\u540c\u6642\u5be6\u73fe\u4e86\u66f4\u9ad8\u7684\u6a21\u7d44\u5316\u548c\u8a13\u7df4\u6a21\u7d44\u7684\u8f15\u9b06\u5171\u4eab\uff0c\u9019\u4f7f\u66f4\u591a\u7814\u7a76\u4eba\u54e1\u80fd\u5920\u5728\u7814\u7a76\u4e2d\u4f7f\u7528\u9ad8\u6027\u80fd NLP \u65b9\u6cd5\u3002", "author": "Jonas Rieger et.al.", "authors": "Jonas Rieger, Mattes Ruckdeschel, Gregor Wiedemann", "id": "2412.04975v1", "paper_url": "http://arxiv.org/abs/2412.04975v1", "repo": "null"}}