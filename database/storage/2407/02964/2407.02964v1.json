{"2407.02964": {"publish_time": "2024-07-03", "title": "FSM: A Finite State Machine Based Zero-Shot Prompting Paradigm for Multi-Hop Question Answering", "paper_summary": "Large Language Models (LLMs) with chain-of-thought (COT) prompting have\ndemonstrated impressive abilities on simple nature language inference tasks.\nHowever, they tend to perform poorly on Multi-hop Question Answering (MHQA)\ntasks due to several challenges, including hallucination, error propagation and\nlimited context length. We propose a prompting method, Finite State Machine\n(FSM) to enhance the reasoning capabilities of LLM for complex tasks in\naddition to improved effectiveness and trustworthiness. Different from COT\nmethods, FSM addresses MHQA by iteratively decomposing a question into\nmulti-turn sub-questions, and self-correcting in time, improving the accuracy\nof answers in each step. Specifically, FSM addresses one sub-question at a time\nand decides on the next step based on its current result and state, in an\nautomaton-like format. Experiments on benchmarks show the effectiveness of our\nmethod. Although our method performs on par with the baseline on relatively\nsimpler datasets, it excels on challenging datasets like Musique. Moreover,\nthis approach mitigates the hallucination phenomenon, wherein the correct final\nanswer can be recovered despite errors in intermediate reasoning. Furthermore,\nour method improves LLMs' ability to follow specified output format\nrequirements, significantly reducing the difficulty of answer interpretation\nand the need for reformatting.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u601d\u8003\u93c8 (COT) \u63d0\u793a\u5728\u7c21\u55ae\u7684\u81ea\u7136\u8a9e\u8a00\u63a8\u8ad6\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\n\u7136\u800c\uff0c\u7531\u65bc\u5305\u62ec\u5e7b\u89ba\u3001\u932f\u8aa4\u50b3\u64ad\u548c\u6709\u9650\u7684\u8108\u7d61\u9577\u5ea6\u5728\u5167\u7684\u82e5\u5e72\u6311\u6230\uff0c\u5b83\u5011\u5728\u591a\u8df3\u5f0f\u554f\u7b54 (MHQA) \u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u5f80\u5f80\u4e0d\u4f73\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u63d0\u793a\u65b9\u6cd5\uff0c\u6709\u9650\u72c0\u614b\u6a5f (FSM)\uff0c\u4ee5\u589e\u5f37 LLM \u5c0d\u8907\u96dc\u4efb\u52d9\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e26\u63d0\u9ad8\u5176\u6709\u6548\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u8207 COT \u65b9\u6cd5\u4e0d\u540c\uff0cFSM \u901a\u904e\u53cd\u8986\u5c07\u554f\u984c\u5206\u89e3\u6210\u591a\u8f2a\u5b50\u554f\u984c\uff0c\u4e26\u53ca\u6642\u81ea\u6211\u7cfe\u6b63\uff0c\u4ee5\u89e3\u6c7a MHQA\uff0c\u5f9e\u800c\u63d0\u9ad8\u6bcf\u4e00\u6b65\u7b54\u6848\u7684\u6e96\u78ba\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cFSM \u4e00\u6b21\u8655\u7406\u4e00\u500b\u5b50\u554f\u984c\uff0c\u4e26\u6839\u64da\u5176\u7576\u524d\u7d50\u679c\u548c\u72c0\u614b\uff0c\u4ee5\u985e\u4f3c\u81ea\u52d5\u6a5f\u7684\u683c\u5f0f\u6c7a\u5b9a\u4e0b\u4e00\u6b65\u3002\u57fa\u6e96\u6e2c\u8a66\u7684\u5be6\u9a57\u986f\u793a\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5118\u7ba1\u6211\u5011\u7684\u6a21\u578b\u5728\u76f8\u5c0d\u7c21\u55ae\u7684\u6578\u64da\u96c6\u4e0a\u8207\u57fa\u6e96\u8868\u73fe\u76f8\u7576\uff0c\u4f46\u5b83\u5728\u50cf Musique \u9019\u6a23\u7684\u5177\u6709\u6311\u6230\u6027\u7684\u6578\u64da\u96c6\u4e0a\u8868\u73fe\u51fa\u8272\u3002\u6b64\u5916\uff0c\u9019\u7a2e\u65b9\u6cd5\u6e1b\u8f15\u4e86\u5e7b\u89ba\u73fe\u8c61\uff0c\u5176\u4e2d\u6b63\u78ba\u7684\u6700\u7d42\u7b54\u6848\u53ef\u4ee5\u5728\u4e2d\u9593\u63a8\u7406\u51fa\u73fe\u932f\u8aa4\u7684\u60c5\u6cc1\u4e0b\u6062\u5fa9\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6a21\u578b\u63d0\u9ad8\u4e86 LLM \u9075\u5faa\u6307\u5b9a\u8f38\u51fa\u683c\u5f0f\u8981\u6c42\u7684\u80fd\u529b\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u7b54\u6848\u89e3\u91cb\u7684\u96e3\u5ea6\u548c\u91cd\u65b0\u683c\u5f0f\u5316\u7684\u9700\u6c42\u3002", "author": "Xiaochen Wang et.al.", "authors": "Xiaochen Wang, Junqing He, Zhe yang, Yiru Wang, Xiangdi Meng, Kunhao Pan, Zhifang Sui", "id": "2407.02964v1", "paper_url": "http://arxiv.org/abs/2407.02964v1", "repo": "null"}}