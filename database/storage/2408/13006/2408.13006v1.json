{"2408.13006": {"publish_time": "2024-08-23", "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates", "paper_summary": "Alignment approaches such as RLHF and DPO are actively investigated to align\nlarge language models (LLMs) with human preferences. Commercial large language\nmodels (LLMs) like GPT-4 have been recently employed to evaluate and compare\ndifferent LLM alignment approaches. These models act as surrogates for human\nevaluators due to their promising abilities to approximate human preferences\nwith remarkably faster feedback and lower costs. This methodology is referred\nto as LLM-as-a-judge. However, concerns regarding its reliability have emerged,\nattributed to LLM judges' biases and inconsistent decision-making. Previous\nresearch has sought to develop robust evaluation frameworks for assessing the\nreliability of LLM judges and their alignment with human preferences. However,\nthe employed evaluation metrics often lack adequate explainability and fail to\naddress the internal inconsistency of LLMs. Additionally, existing studies\ninadequately explore the impact of various prompt templates when applying\nLLM-as-a-judge methods, which leads to potentially inconsistent comparisons\nbetween different alignment algorithms. In this work, we systematically\nevaluate LLM judges on alignment tasks (e.g. summarization) by defining\nevaluation metrics with improved theoretical interpretability and disentangling\nreliability metrics with LLM internal inconsistency. We develop a framework to\nevaluate, compare, and visualize the reliability and alignment of LLM judges to\nprovide informative observations that help choose LLM judges for alignment\ntasks. Our results indicate a significant impact of prompt templates on LLM\njudge performance, as well as a mediocre alignment level between the tested LLM\njudges and human evaluators.", "paper_summary_zh": "\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u4f8b\u5982 RLHF \u548c DPO\uff0c\u6b63\u7a4d\u6975\u5730\u7528\u65bc\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u3002\u5546\u696d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4f8b\u5982 GPT-4\uff0c\u6700\u8fd1\u5df2\u88ab\u7528\u65bc\u8a55\u4f30\u548c\u6bd4\u8f03\u4e0d\u540c\u7684 LLM \u5c0d\u9f4a\u65b9\u6cd5\u3002\u9019\u4e9b\u6a21\u578b\u7531\u65bc\u8fd1\u4f3c\u4eba\u985e\u504f\u597d\u7684\u80fd\u529b\u5f37\u5927\uff0c\u4e14\u53cd\u994b\u901f\u5ea6\u986f\u8457\u52a0\u5feb\uff0c\u6210\u672c\u66f4\u4f4e\uff0c\u56e0\u6b64\u5145\u7576\u4eba\u985e\u8a55\u4f30\u8005\u7684\u66ff\u4ee3\u8005\u3002\u9019\u7a2e\u65b9\u6cd5\u7a31\u70ba LLM \u4f5c\u70ba\u8a55\u5224\u8005\u3002\u7136\u800c\uff0c\u7531\u65bc LLM \u8a55\u5224\u8005\u7684\u504f\u898b\u548c\u4e0d\u4e00\u81f4\u7684\u6c7a\u7b56\uff0c\u5c0d\u5176\u53ef\u9760\u6027\u7684\u64d4\u6182\u5df2\u7d93\u6d6e\u73fe\u3002\u5148\u524d\u7684\u7814\u7a76\u8a66\u5716\u958b\u767c\u5065\u5168\u7684\u8a55\u4f30\u67b6\u69cb\uff0c\u7528\u65bc\u8a55\u4f30 LLM \u8a55\u5224\u8005\u7684\u53ef\u9760\u6027\u53ca\u5176\u8207\u4eba\u985e\u504f\u597d\u7684\u5c0d\u9f4a\u7a0b\u5ea6\u3002\u7136\u800c\uff0c\u6240\u63a1\u7528\u7684\u8a55\u4f30\u6307\u6a19\u901a\u5e38\u7f3a\u4e4f\u5145\u5206\u7684\u53ef\u89e3\u91cb\u6027\uff0c\u4e14\u7121\u6cd5\u89e3\u6c7a LLM \u7684\u5167\u90e8\u4e0d\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u73fe\u6709\u7814\u7a76\u5728\u61c9\u7528 LLM \u4f5c\u70ba\u8a55\u5224\u8005\u65b9\u6cd5\u6642\uff0c\u4e26\u672a\u5145\u5206\u63a2\u8a0e\u5404\u7a2e\u63d0\u793a\u7bc4\u672c\u7684\u5f71\u97ff\uff0c\u9019\u5c0e\u81f4\u5728\u4e0d\u540c\u7684\u5c0d\u9f4a\u6f14\u7b97\u6cd5\u4e4b\u9593\u9032\u884c\u6bd4\u8f03\u6642\u53ef\u80fd\u6703\u4e0d\u4e00\u81f4\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5b9a\u7fa9\u5177\u6709\u6539\u9032\u7684\u7406\u8ad6\u53ef\u89e3\u91cb\u6027\uff0c\u4e26\u5c07\u53ef\u9760\u6027\u6307\u6a19\u8207 LLM \u5167\u90e8\u4e0d\u4e00\u81f4\u6027\u5340\u5206\u958b\u4f86\uff0c\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u5c0d\u9f4a\u4efb\u52d9\uff08\u4f8b\u5982\u6458\u8981\uff09\u4e2d\u7684 LLM \u8a55\u5224\u8005\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u67b6\u69cb\u4f86\u8a55\u4f30\u3001\u6bd4\u8f03\u548c\u8996\u89ba\u5316 LLM \u8a55\u5224\u8005\u7684\u53ef\u9760\u6027\u548c\u5c0d\u9f4a\u7a0b\u5ea6\uff0c\u4ee5\u63d0\u4f9b\u6709\u52a9\u65bc\u9078\u64c7 LLM \u8a55\u5224\u8005\u9032\u884c\u5c0d\u9f4a\u4efb\u52d9\u7684\u898b\u89e3\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u63d0\u793a\u7bc4\u672c\u5c0d LLM \u8a55\u5224\u8005\u8868\u73fe\u6709\u986f\u8457\u5f71\u97ff\uff0c\u4e14\u5728\u6e2c\u8a66\u7684 LLM \u8a55\u5224\u8005\u8207\u4eba\u985e\u8a55\u4f30\u8005\u4e4b\u9593\u7684\u5c0d\u9f4a\u7a0b\u5ea6\u666e\u901a\u3002", "author": "Hui Wei et.al.", "authors": "Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, Mei Han", "id": "2408.13006v1", "paper_url": "http://arxiv.org/abs/2408.13006v1", "repo": "null"}}