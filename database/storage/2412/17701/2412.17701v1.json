{"2412.17701": {"publish_time": "2024-12-23", "title": "From Models to Microtheories: Distilling a Model's Topical Knowledge for Grounded Question Answering", "paper_summary": "Recent reasoning methods (e.g., chain-of-thought, entailment reasoning) help\nusers understand how language models (LMs) answer a single question, but they\ndo little to reveal the LM's overall understanding, or \"theory,\" about the\nquestion's $\\textit{topic}$, making it still hard to trust the model. Our goal\nis to materialize such theories - here called $\\textit{microtheories}$ (a\nlinguistic analog of logical microtheories) - as a set of sentences\nencapsulating an LM's core knowledge about a topic. These statements\nsystematically work together to entail answers to a $\\textit{set}$ of questions\nto both engender trust and improve performance. Our approach is to first\npopulate a knowledge store with (model-generated) sentences that entail answers\nto training questions and then distill those down to a core microtheory that is\nconcise, general, and non-redundant. We show that, when added to a general\ncorpus (e.g., Wikipedia), microtheories can supply critical, topical\ninformation not necessarily present in the corpus, improving both a model's\nability to ground its answers to verifiable knowledge (i.e., show how answers\nare systematically entailed by documents in the corpus, fully grounding up to\n+8% more answers), and the accuracy of those grounded answers (up to +8%\nabsolute). We also show that, in a human evaluation in the medical domain, our\ndistilled microtheories contain a significantly higher concentration of\ntopically critical facts than the non-distilled knowledge store. Finally, we\nshow we can quantify the coverage of a microtheory for a topic (characterized\nby a dataset) using a notion of $p$-relevance. Together, these suggest that\nmicrotheories are an efficient distillation of an LM's topic-relevant\nknowledge, that they can usefully augment existing corpora, and can provide\nboth performance gains and an interpretable, verifiable window into the model's\nknowledge of a topic.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u7684\u63a8\u7406\u65b9\u6cd5\uff08\u4f8b\u5982\uff0c\u601d\u7ef4\u94fe\u3001\u8574\u542b\u63a8\u7406\uff09\u6709\u52a9\u4e8e\n\u7528\u6237\u4e86\u89e3\u8bed\u8a00\u6a21\u578b (LM) \u5982\u4f55\u56de\u7b54\u5355\u4e2a\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\n\u51e0\u4e4e\u6ca1\u6709\u63ed\u793a LM \u5bf9\u95ee\u9898\u7684\u6574\u4f53\u7406\u89e3\u6216\u201c\u7406\u8bba\u201d\uff0c$\\textit{\u4e3b\u9898}$\uff0c\u8fd9\u4f7f\u5f97\u4ecd\u7136\u96be\u4ee5\u4fe1\u4efb\u8be5\u6a21\u578b\u3002\u6211\u4eec\u7684\u76ee\u6807\n\u662f\u5c06\u8fd9\u4e9b\u7406\u8bba\u5177\u4f53\u5316 - \u8fd9\u91cc\u79f0\u4e3a $\\textit{\u5fae\u7406\u8bba}$\uff08\u903b\u8f91\u5fae\u7406\u8bba\u7684\u8bed\u8a00\u7c7b\u6bd4\uff09 - \u4f5c\u4e3a\u4e00\u7ec4\u53e5\u5b50\n\u5c01\u88c5 LM \u5173\u4e8e\u67d0\u4e2a\u4e3b\u9898\u7684\u6838\u5fc3\u77e5\u8bc6\u3002\u8fd9\u4e9b\u9648\u8ff0\n\u7cfb\u7edf\u5730\u534f\u540c\u5de5\u4f5c\uff0c\u4ee5\u5f97\u51fa\u5bf9 $\\textit{\u4e00\u7ec4}$ \u95ee\u9898\u7684\u7b54\u6848\uff0c\u65e2\u80fd\u57f9\u517b\u4fe1\u4efb\u53c8\u80fd\u63d0\u9ad8\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9996\u5148\n\u4f7f\u7528\uff08\u6a21\u578b\u751f\u6210\u7684\uff09\u53e5\u5b50\u586b\u5145\u77e5\u8bc6\u5e93\uff0c\u8fd9\u4e9b\u53e5\u5b50\u8574\u542b\u5bf9\u8bad\u7ec3\u95ee\u9898\u7684\u7b54\u6848\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u53e5\u5b50\u63d0\u70bc\u6210\u4e00\u4e2a\u6838\u5fc3\u5fae\u7406\u8bba\uff0c\u8be5\u7406\u8bba\u7b80\u6d01\u3001\u901a\u7528\u4e14\u4e0d\u5197\u4f59\u3002\u6211\u4eec\u8868\u660e\uff0c\u5f53\u6dfb\u52a0\u5230\u901a\u7528\n\u8bed\u6599\u5e93\uff08\u4f8b\u5982\uff0c\u7ef4\u57fa\u767e\u79d1\uff09\u4e2d\u65f6\uff0c\u5fae\u7406\u8bba\u53ef\u4ee5\u63d0\u4f9b\u8bed\u6599\u5e93\u4e2d\u4e0d\u4e00\u5b9a\u5b58\u5728\u7684\u5173\u952e\u4e3b\u9898\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\n\u6839\u636e\u53ef\u9a8c\u8bc1\u77e5\u8bc6\u56de\u7b54\u95ee\u9898\u7684\u80fd\u529b\uff08\u5373\uff0c\u5c55\u793a\u7b54\u6848\u5982\u4f55\u88ab\u8bed\u6599\u5e93\u4e2d\u7684\u6587\u6863\u7cfb\u7edf\u5730\u8574\u542b\uff0c\u5b8c\u5168\u652f\u6301\u9ad8\u8fbe\n+8% \u7684\u7b54\u6848\uff09\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u6709\u6839\u636e\u7684\u7b54\u6848\u7684\u51c6\u786e\u6027\uff08\u9ad8\u8fbe +8%\n\u7edd\u5bf9\uff09\u3002\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u5728\u533b\u5b66\u9886\u57df\u7684\u8bc4\u4f30\u4e2d\uff0c\u6211\u4eec\u7684\n\u63d0\u70bc\u5fae\u7406\u8bba\u5305\u542b\u7684\u4e3b\u9898\u5173\u952e\u4e8b\u5b9e\u6d53\u5ea6\u663e\u7740\u9ad8\u4e8e\u672a\u63d0\u70bc\u7684\u77e5\u8bc6\u5e93\u3002\u6700\u540e\uff0c\u6211\u4eec\n\u8868\u660e\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 $p$-\u76f8\u5173\u6027\u7684\u6982\u5ff5\u91cf\u5316\u5fae\u7406\u8bba\u5bf9\u67d0\u4e2a\u4e3b\u9898\uff08\u7531\u6570\u636e\u96c6\u8868\u5f81\uff09\u7684\u8986\u76d6\u8303\u56f4\u3002\u603b\u4e4b\uff0c\u8fd9\u4e9b\u8868\u660e\n\u5fae\u7406\u8bba\u662f LM \u4e0e\u4e3b\u9898\u76f8\u5173\u7684\u77e5\u8bc6\u7684\u6709\u6548\u63d0\u70bc\uff0c\u5b83\u4eec\u53ef\u4ee5\u6709\u7528\u5730\u6269\u5145\u73b0\u6709\u8bed\u6599\u5e93\uff0c\u5e76\u4e14\u53ef\u4ee5\u63d0\u4f9b\n\u6027\u80fd\u63d0\u5347\u548c\u53ef\u89e3\u91ca\u7684\u3001\u53ef\u9a8c\u8bc1\u7684\u7a97\u53e3\uff0c\u4ee5\u4fbf\u4e86\u89e3\u6a21\u578b\u5bf9\u67d0\u4e2a\u4e3b\u9898\u7684\u77e5\u8bc6\u3002</paragraph>", "author": "Nathaniel Weir et.al.", "authors": "Nathaniel Weir, Bhavana Dalvi Mishra, Orion Weller, Oyvind Tafjord, Sam Hornstein, Alexander Sabol, Peter Jansen, Benjamin Van Durme, Peter Clark", "id": "2412.17701v1", "paper_url": "http://arxiv.org/abs/2412.17701v1", "repo": "https://github.com/nweir127/microtheories"}}