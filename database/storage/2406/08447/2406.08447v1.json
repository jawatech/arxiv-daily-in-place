{"2406.08447": {"publish_time": "2024-06-12", "title": "The Impact of Initialization on LoRA Finetuning Dynamics", "paper_summary": "In this paper, we study the role of initialization in Low Rank Adaptation\n(LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from\nthe pretrained model as initialization for finetuning, one can either\ninitialize B to zero and A to random (default initialization in PEFT package),\nor vice-versa. In both cases, the product BA is equal to zero at\ninitialization, which makes finetuning starts from the pretrained model. These\ntwo initialization schemes are seemingly similar. They should in-principle\nyield the same performance and share the same optimal learning rate. We\ndemonstrate that this is an incorrect intuition and that the first scheme\n(initializing B to zero and A to random) on average yields better performance\ncompared to the other scheme. Our theoretical analysis shows that the reason\nbehind this might be that the first initialization allows the use of larger\nlearning rates (without causing output instability) compared to the second\ninitialization, resulting in more efficient learning of the first scheme. We\nvalidate our results with extensive experiments on LLMs.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u521d\u59cb\u5316\u5728\u4f4e\u79e9\u9002\u5e94 (LoRA) \u4e2d\u7684\u89d2\u8272\uff0c\u6b63\u5982 Hu \u7b49\u4eba (2021) \u6700\u521d\u63d0\u51fa\u7684\u90a3\u6837\u3002\u4ece\u672c\u8d28\u4e0a\u8bb2\uff0c\u4e3a\u4e86\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u5f00\u59cb\u4f5c\u4e3a\u5fae\u8c03\u7684\u521d\u59cb\u5316\uff0c\u53ef\u4ee5\u5c06 B \u521d\u59cb\u5316\u4e3a\u96f6\uff0c\u5c06 A \u521d\u59cb\u5316\u4e3a\u968f\u673a\u503c\uff08PEFT \u5305\u4e2d\u7684\u9ed8\u8ba4\u521d\u59cb\u5316\uff09\uff0c\u6216\u8005\u53cd\u4e4b\u4ea6\u7136\u3002\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u4ea7\u54c1 BA \u5728\u521d\u59cb\u5316\u65f6\u7b49\u4e8e\u96f6\uff0c\u8fd9\u4f7f\u5f97\u5fae\u8c03\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u5f00\u59cb\u3002\u8fd9\u4e24\u4e2a\u521d\u59cb\u5316\u65b9\u6848\u770b\u4f3c\u76f8\u4f3c\u3002\u5b83\u4eec\u5728\u539f\u5219\u4e0a\u5e94\u8be5\u4ea7\u751f\u76f8\u540c\u7684\u6027\u80fd\u5e76\u5171\u4eab\u76f8\u540c\u7684\u6700\u4f73\u5b66\u4e60\u7387\u3002\u6211\u4eec\u8bc1\u660e\u8fd9\u662f\u4e00\u4e2a\u4e0d\u6b63\u786e\u7684\u76f4\u89c9\uff0c\u5e76\u4e14\u7b2c\u4e00\u4e2a\u65b9\u6848\uff08\u5c06 B \u521d\u59cb\u5316\u4e3a\u96f6\uff0c\u5c06 A \u521d\u59cb\u5316\u4e3a\u968f\u673a\u503c\uff09\u5e73\u5747\u4ea7\u751f\u6bd4\u53e6\u4e00\u4e2a\u65b9\u6848\u66f4\u597d\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5176\u80cc\u540e\u7684\u539f\u56e0\u53ef\u80fd\u662f\u7b2c\u4e00\u4e2a\u521d\u59cb\u5316\u5141\u8bb8\u4f7f\u7528\u66f4\u5927\u7684\u5b66\u4e60\u7387\uff08\u4e0d\u4f1a\u5bfc\u81f4\u8f93\u51fa\u4e0d\u7a33\u5b9a\uff09\uff0c\u4e0e\u7b2c\u4e8c\u4e2a\u521d\u59cb\u5316\u76f8\u6bd4\uff0c\u8fd9\u5bfc\u81f4\u7b2c\u4e00\u4e2a\u65b9\u6848\u7684\u5b66\u4e60\u6548\u7387\u66f4\u9ad8\u3002\u6211\u4eec\u901a\u8fc7\u5bf9 LLM \u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u7ed3\u679c\u3002", "author": "Soufiane Hayou et.al.", "authors": "Soufiane Hayou, Nikhil Ghosh, Bin Yu", "id": "2406.08447v1", "paper_url": "http://arxiv.org/abs/2406.08447v1", "repo": "null"}}