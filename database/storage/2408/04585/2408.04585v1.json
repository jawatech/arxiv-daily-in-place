{"2408.04585": {"publish_time": "2024-08-08", "title": "Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness", "paper_summary": "With the increasing demand for practical applications of Large Language\nModels (LLMs), many attention-efficient models have been developed to balance\nperformance and computational cost. However, the adversarial robustness of\nthese models remains under-explored. In this work, we design a framework to\ninvestigate the trade-off between efficiency, performance, and adversarial\nrobustness of LLMs by comparing three prominent models with varying levels of\ncomplexity and efficiency -- Transformer++, Gated Linear Attention (GLA)\nTransformer, and MatMul-Free LM -- utilizing the GLUE and AdvGLUE datasets. The\nAdvGLUE dataset extends the GLUE dataset with adversarial samples designed to\nchallenge model robustness. Our results show that while the GLA Transformer and\nMatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate\nhigher efficiency and either superior or comparative robustness on AdvGLUE\ntasks compared to Transformer++ across different attack levels. These findings\nhighlight the potential of simplified architectures to achieve a compelling\nbalance between efficiency, performance, and adversarial robustness, offering\nvaluable insights for applications where resource constraints and resilience to\nadversarial attacks are critical.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5be6\u7528\u61c9\u7528\u9700\u6c42\u7684\u589e\u52a0\uff0c\u8a31\u591a\u6ce8\u91cd\u6548\u7387\u7684\u6a21\u578b\u5df2\u88ab\u958b\u767c\u51fa\u4f86\uff0c\u4ee5\u5e73\u8861\u6548\u80fd\u548c\u904b\u7b97\u6210\u672c\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u7684\u5c0d\u6297\u6027\u7a69\u5065\u6027\u4ecd\u672a\u53d7\u5230\u5145\u5206\u63a2\u8a0e\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u67b6\u69cb\u4f86\u63a2\u8a0e LLM \u7684\u6548\u7387\u3001\u6548\u80fd\u548c\u5c0d\u6297\u6027\u7a69\u5065\u6027\u4e4b\u9593\u7684\u53d6\u6368\uff0c\u65b9\u6cd5\u662f\u6bd4\u8f03\u4e09\u500b\u5177\u6709\u4e0d\u540c\u7a0b\u5ea6\u8907\u96dc\u6027\u548c\u6548\u7387\u7684\u8457\u540d\u6a21\u578b\u2014\u2014Transformer++\u3001\u9580\u63a7\u7dda\u6027\u6ce8\u610f\u529b (GLA) Transformer \u548c\u7121 MatMul LM\u2014\u2014\u4e26\u5229\u7528 GLUE \u548c AdvGLUE \u8cc7\u6599\u96c6\u3002AdvGLUE \u8cc7\u6599\u96c6\u64f4\u5145\u4e86 GLUE \u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u65e8\u5728\u6311\u6230\u6a21\u578b\u7a69\u5065\u6027\u7684\u5c0d\u6297\u6027\u6a23\u672c\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u96d6\u7136 GLA Transformer \u548c\u7121 MatMul LM \u5728 GLUE \u4efb\u52d9\u4e0a\u7684\u6e96\u78ba\u5ea6\u7565\u4f4e\uff0c\u4f46\u5b83\u5011\u5c55\u73fe\u51fa\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u4e26\u4e14\u5728\u4e0d\u540c\u653b\u64ca\u5c64\u7d1a\u4e0b\uff0c\u5728 AdvGLUE \u4efb\u52d9\u4e0a\u5c55\u73fe\u51fa\u512a\u65bc\u6216\u76f8\u7576\u7684\u7a69\u5065\u6027\uff0c\u512a\u65bc Transformer++\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u7c21\u5316\u67b6\u69cb\u5728\u6548\u7387\u3001\u6548\u80fd\u548c\u5c0d\u6297\u6027\u7a69\u5065\u6027\u4e4b\u9593\u53d6\u5f97\u4ee4\u4eba\u4fe1\u670d\u7684\u5e73\u8861\u7684\u6f5b\u529b\uff0c\u70ba\u8cc7\u6e90\u53d7\u9650\u4e14\u5c0d\u6297\u653b\u64ca\u5fa9\u539f\u529b\u81f3\u95dc\u91cd\u8981\u7684\u61c9\u7528\u7a0b\u5f0f\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002", "author": "Xiaojing Fan et.al.", "authors": "Xiaojing Fan, Chunliang Tao", "id": "2408.04585v1", "paper_url": "http://arxiv.org/abs/2408.04585v1", "repo": "null"}}