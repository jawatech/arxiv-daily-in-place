{"2501.02669": {"publish_time": "2025-01-05", "title": "Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?", "paper_summary": "While Vision Language Models (VLMs) are impressive in tasks such as visual\nquestion answering (VQA) and image captioning, their ability to apply\nmulti-step reasoning to images has lagged, giving rise to perceptions of\nmodality imbalance or brittleness. Towards systematic study of such issues, we\nintroduce a synthetic framework for assessing the ability of VLMs to perform\nalgorithmic visual reasoning (AVR), comprising three tasks: Table Readout, Grid\nNavigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and\nHARD, and even the SIMPLE versions are difficult for frontier VLMs. We seek\nstrategies for training on the SIMPLE version of the tasks that improve\nperformance on the corresponding HARD task, i.e., S2H generalization. This\nsynthetic framework, where each task also has a text-only version, allows a\nquantification of the modality imbalance, and how it is impacted by training\nstrategy. Ablations highlight the importance of explicit image-to-text\nconversion in promoting S2H generalization when using auto-regressive training.\nWe also report results of mechanistic study of this phenomenon, including a\nmeasure of gradient alignment that seems to identify training strategies that\npromote better S2H generalization.", "paper_summary_zh": "\u96d6\u7136\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u8996\u89ba\u554f\u7b54 (VQA) \u548c\u5f71\u50cf\u5b57\u5e55\u7b49\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5b83\u5011\u5c07\u591a\u6b65\u9a5f\u63a8\u7406\u61c9\u7528\u65bc\u5f71\u50cf\u7684\u80fd\u529b\u537b\u843d\u5f8c\uff0c\u5c0e\u81f4\u4eba\u5011\u8a8d\u70ba\u5b83\u5011\u5728\u6a21\u614b\u4e0a\u5931\u8861\u6216\u8106\u5f31\u3002\u70ba\u4e86\u7cfb\u7d71\u6027\u5730\u7814\u7a76\u6b64\u985e\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u5408\u6210\u6846\u67b6\uff0c\u7528\u65bc\u8a55\u4f30 VLM \u57f7\u884c\u6f14\u7b97\u6cd5\u8996\u89ba\u63a8\u7406 (AVR) \u7684\u80fd\u529b\uff0c\u5176\u4e2d\u5305\u542b\u4e09\u9805\u4efb\u52d9\uff1a\u8868\u683c\u8b80\u53d6\u3001\u7db2\u683c\u5c0e\u822a\u548c\u8996\u89ba\u985e\u6bd4\u3002\u6bcf\u500b\u4efb\u52d9\u90fd\u6709\u5169\u500b\u96e3\u5ea6\u7b49\u7d1a\uff0c\u7c21\u55ae\u548c\u56f0\u96e3\uff0c\u5373\u4f7f\u662f\u7c21\u55ae\u7248\u672c\u5c0d\u524d\u6cbf VLM \u4f86\u8aaa\u4e5f\u5f88\u56f0\u96e3\u3002\u6211\u5011\u5c0b\u6c42\u5728\u4efb\u52d9\u7684\u7c21\u55ae\u7248\u672c\u4e0a\u9032\u884c\u8a13\u7df4\u7684\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u5c0d\u61c9\u56f0\u96e3\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u5373 S2H \u5ee3\u7fa9\u5316\u3002\u6b64\u5408\u6210\u6846\u67b6\uff08\u5176\u4e2d\u6bcf\u500b\u4efb\u52d9\u4e5f\u6709\u7d14\u6587\u5b57\u7248\u672c\uff09\u5141\u8a31\u91cf\u5316\u6a21\u614b\u5931\u8861\uff0c\u4ee5\u53ca\u8a13\u7df4\u7b56\u7565\u5982\u4f55\u5f71\u97ff\u5b83\u3002\u6d88\u878d\u5be6\u9a57\u7a81\u986f\u4e86\u5728\u4f7f\u7528\u81ea\u8ff4\u6b78\u8a13\u7df4\u6642\uff0c\u660e\u78ba\u7684\u5f71\u50cf\u8f49\u6587\u5b57\u8f49\u63db\u5728\u4fc3\u9032 S2H \u5ee3\u7fa9\u5316\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6211\u5011\u4e5f\u5831\u544a\u4e86\u5c0d\u6b64\u73fe\u8c61\u7684\u6a5f\u5236\u7814\u7a76\u7d50\u679c\uff0c\u5305\u62ec\u4e00\u7a2e\u68af\u5ea6\u5c0d\u9f4a\u6e2c\u91cf\uff0c\u5b83\u4f3c\u4e4e\u53ef\u4ee5\u627e\u51fa\u4fc3\u9032 S2H \u5ee3\u7fa9\u5316\u7684\u8a13\u7df4\u7b56\u7565\u3002", "author": "Simon Park et.al.", "authors": "Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, Sanjeev Arora", "id": "2501.02669v1", "paper_url": "http://arxiv.org/abs/2501.02669v1", "repo": "https://github.com/princeton-pli/vlm_s2h"}}