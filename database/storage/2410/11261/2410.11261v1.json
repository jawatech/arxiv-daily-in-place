{"2410.11261": {"publish_time": "2024-10-15", "title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix", "paper_summary": "Large Language Models (LLMs) have shown immense potential in enhancing\nvarious aspects of our daily lives, from conversational AI to search and AI\nassistants. However, their growing capabilities come at the cost of extremely\nlarge model sizes, making deployment on edge devices challenging due to memory\nand computational constraints. This paper introduces a novel approach to LLM\nweight pruning that directly optimizes for approximating the attention matrix,\na core component of transformer architectures. Unlike existing methods that\nfocus on linear approximations, our approach accounts for the non-linear nature\nof the Softmax attention mechanism. We provide theoretical guarantees for the\nconvergence of our Gradient Descent-based optimization method to a near-optimal\npruning mask solution. Our preliminary empirical results demonstrate the\neffectiveness of this approach in maintaining model performance while\nsignificantly reducing computational costs. This work establishes a new\ntheoretical foundation for pruning algorithm design in LLMs, potentially paving\nthe way for more efficient LLM inference on resource-constrained devices.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u6975\u5927\u7684\u6f5b\u529b\uff0c\u53ef\u589e\u5f37\u6211\u5011\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5404\u500b\u9762\u5411\uff0c\u5f9e\u5c0d\u8a71\u5f0f AI \u5230\u641c\u5c0b\u548c AI \u52a9\u7406\u3002\u7136\u800c\uff0c\u5b83\u5011\u65e5\u76ca\u589e\u9577\u7684\u6548\u80fd\u662f\u4ee5\u6975\u5927\u7684\u6a21\u578b\u898f\u6a21\u70ba\u4ee3\u50f9\uff0c\u9019\u4f7f\u5f97\u5728\u908a\u7de3\u88dd\u7f6e\u4e0a\u90e8\u7f72\u6642\u9762\u81e8\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u9650\u5236\u7684\u6311\u6230\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u5275\u65b0\u7684 LLM \u6b0a\u91cd\u526a\u679d\u65b9\u6cd5\uff0c\u53ef\u76f4\u63a5\u91dd\u5c0d\u8fd1\u4f3c\u6ce8\u610f\u77e9\u9663\u9032\u884c\u6700\u4f73\u5316\uff0c\u800c\u6ce8\u610f\u77e9\u9663\u662f Transformer \u67b6\u69cb\u7684\u6838\u5fc3\u5143\u4ef6\u3002\u8207\u5c08\u6ce8\u65bc\u7dda\u6027\u8fd1\u4f3c\u7684\u73fe\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u5011\u7684\u505a\u6cd5\u8003\u91cf\u4e86 Softmax \u6ce8\u610f\u529b\u6a5f\u5236\u7684\u975e\u7dda\u6027\u6027\u8cea\u3002\u6211\u5011\u70ba\u57fa\u65bc\u68af\u5ea6\u4e0b\u964d\u7684\u6700\u4f73\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8ad6\u4fdd\u8b49\uff0c\u4ee5\u6536\u6582\u5230\u8fd1\u4e4e\u6700\u4f73\u7684\u526a\u679d\u906e\u7f69\u89e3\u3002\u6211\u5011\u7684\u521d\u6b65\u5be6\u8b49\u7d50\u679c\u8b49\u660e\u4e86\u6b64\u65b9\u6cd5\u5728\u7dad\u6301\u6a21\u578b\u6548\u80fd\u7684\u540c\u6642\uff0c\u5927\u5e45\u964d\u4f4e\u904b\u7b97\u6210\u672c\u7684\u6709\u6548\u6027\u3002\u9019\u9805\u5de5\u4f5c\u70ba LLM \u4e2d\u7684\u526a\u679d\u6f14\u7b97\u6cd5\u8a2d\u8a08\u5efa\u7acb\u4e86\u65b0\u7684\u7406\u8ad6\u57fa\u790e\uff0c\u53ef\u80fd\u70ba\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u88dd\u7f6e\u4e0a\u9032\u884c\u66f4\u6709\u6548\u7387\u7684 LLM \u63a8\u8ad6\u92ea\u8def\u3002", "author": "Yingyu Liang et.al.", "authors": "Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Yufa Zhou", "id": "2410.11261v1", "paper_url": "http://arxiv.org/abs/2410.11261v1", "repo": "null"}}