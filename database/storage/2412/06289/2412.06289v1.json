{"2412.06289": {"publish_time": "2024-12-09", "title": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity", "paper_summary": "Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents overfitting and\nforgetting, delivers SOTA performance on both commonsense and arithmetic\nreasoning with 4.6% and 1.3% average improvements compared to LoRA, and\nsurpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models.", "paper_summary_zh": "\u76ee\u524d\u7528\u65bc LLM \u7684 PEFT \u65b9\u6cd5\u53ef\u5be6\u73fe\u9ad8\u54c1\u8cea\u3001\u9ad8\u6548\u8a13\u7df4\u6216\u53ef\u64f4\u5145\u670d\u52d9\uff0c\u4f46\u7121\u6cd5\u540c\u6642\u5be6\u73fe\u9019\u4e09\u9805\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u63a2\u8a0e\u7a00\u758f\u5fae\u8abf\uff0c\u4e26\u89c0\u5bdf\u5230\u6cdb\u5316\u80fd\u529b\u986f\u8457\u63d0\u5347\u3002\u5229\u7528\u9019\u500b\u95dc\u9375\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217 LLM \u7684\u7d50\u69cb\u5316\u7a00\u758f\u5fae\u8abf (S$^{2}$FT) \u65b9\u6cd5\uff0c\u540c\u6642\u5be6\u73fe\u4e86\u6700\u5148\u9032\u7684\u5fae\u8abf\u6548\u80fd\u3001\u8a13\u7df4\u6548\u7387\u548c\u63a8\u8ad6\u53ef\u64f4\u5145\u6027\u3002S$^{2}$FT \u900f\u904e\u300c\u7a00\u758f\u9078\u64c7\u548c\u5bc6\u96c6\u904b\u7b97\u300d\u4f86\u9054\u6210\u6b64\u76ee\u6a19\u3002\u5b83\u5206\u5225\u70ba\u6bcf\u500b Transformer \u5340\u584a\u7684 MHA \u548c FFN \u6a21\u7d44\u9078\u64c7\u4e00\u4e9b\u982d\u90e8\u548c\u901a\u9053\u3002\u63a5\u8457\uff0c\u5b83\u5728 LLM \u4e2d\u8026\u5408\u7d50\u69cb\u7684\u5169\u5074\u5c0d\u6b0a\u91cd\u77e9\u9663\u9032\u884c\u5171\u7f6e\u63db\uff0c\u5c07\u6bcf\u4e00\u5c64\u4e2d\u9078\u5b9a\u7684\u7d44\u4ef6\u9023\u63a5\u5230\u4e00\u500b\u5bc6\u96c6\u5b50\u77e9\u9663\u4e2d\u3002\u6700\u5f8c\uff0cS$^{2}$FT \u5c0d\u6240\u6709\u5b50\u77e9\u9663\u57f7\u884c\u5c31\u5730\u68af\u5ea6\u66f4\u65b0\u3002\u900f\u904e\u7406\u8ad6\u5206\u6790\u548c\u5be6\u8b49\u7d50\u679c\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u9632\u6b62\u904e\u5ea6\u64ec\u5408\u548c\u907a\u5fd8\uff0c\u5728\u5e38\u8b58\u548c\u7b97\u8853\u63a8\u7406\u4e0a\u63d0\u4f9b SOTA \u6548\u80fd\uff0c\u8207 LoRA \u76f8\u6bd4\uff0c\u5e73\u5747\u6539\u5584\u4e86 4.6% \u548c 1.3%\uff0c\u4e26\u4e14\u5728\u7d93\u904e\u6307\u4ee4\u5fae\u8abf\u5f8c\uff0c\u5c0d\u5404\u7a2e\u9818\u57df\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u4e86 11.5%\uff0c\u8d85\u8d8a\u4e86\u5b8c\u6574\u7684 FT\u3002\u4f7f\u7528\u6211\u5011\u7684\u90e8\u5206\u53cd\u5411\u50b3\u64ad\u6f14\u7b97\u6cd5\uff0c\u8207\u5b8c\u6574\u7684 FT \u76f8\u6bd4\uff0cS$^{2}$FT \u53ef\u7bc0\u7701\u9ad8\u9054 3 \u500d\u7684\u8a13\u7df4\u8a18\u61b6\u9ad4\uff0c\u4e26\u5c07\u5ef6\u9072\u6539\u5584 1.5-2.7 \u500d\uff0c\u540c\u6642\u5728\u5169\u500b\u6307\u6a19\u4e0a\u6bd4 LoRA \u5e73\u5747\u6539\u5584 10%\u3002\u6211\u5011\u9032\u4e00\u6b65\u8b49\u660e\uff0cS$^{2}$FT \u4e2d\u7684\u6b0a\u91cd\u66f4\u65b0\u53ef\u4ee5\u89e3\u8026\u6210\u9069\u914d\u5668\uff0c\u5f9e\u800c\u5be6\u73fe\u6709\u6548\u7684\u878d\u5408\u3001\u5feb\u901f\u5207\u63db\u548c\u9ad8\u6548\u4e26\u884c\u904b\u7b97\uff0c\u4ee5\u670d\u52d9\u591a\u500b\u5fae\u8abf\u6a21\u578b\u3002", "author": "Xinyu Yang et.al.", "authors": "Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen", "id": "2412.06289v1", "paper_url": "http://arxiv.org/abs/2412.06289v1", "repo": "null"}}