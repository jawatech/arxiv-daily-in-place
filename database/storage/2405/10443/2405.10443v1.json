{"2405.10443": {"publish_time": "2024-05-16", "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation", "paper_summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as an unnecessarily expanded training set,\ncomputational inefficiency from dumping the KV cache, increased prompt sizes,\nor restriction to a single decision policy. To eliminate these issues, we\npropose a new paradigm in fine-tuning LLMs for simultaneous translation, called\nSimulMask. It utilizes a novel attention mask technique that models\nsimultaneous translation during fine-tuning by masking attention connections\nunder a desired decision policy. Applying the proposed SimulMask on a Falcon\nLLM for the IWSLT 2017 dataset, we have observed a significant translation\nquality improvement compared to state-of-the-art prompting optimization\nstrategies on three language pairs when averaged across four different latency\nregimes while reducing the computational cost.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u90fd\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u4fc3\u4f7f\u5b83\u5011\u88ab\u63a1\u7528\u5728\u5373\u6642\u7ffb\u8b6f\u4e2d\u3002\u76ee\u524d\u7528\u65bc\u8abf\u6574 LLM \u4ee5\u9069\u61c9\u5373\u6642\u7ffb\u8b6f\u7684\u5fae\u8abf\u65b9\u6cd5\uff0c\u8457\u91cd\u65bc\u4f7f\u7528\u8cc7\u6599\u64f4\u5145\u6216\u63d0\u793a\u7d50\u69cb\u4fee\u6539\u7684\u63d0\u793a\u6700\u4f73\u5316\u7b56\u7565\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u6703\u7522\u751f\u5e7e\u500b\u554f\u984c\uff0c\u4f8b\u5982\u8a13\u7df4\u96c6\u4e0d\u5fc5\u8981\u5730\u64f4\u5145\u3001\u6e05\u9664 KV \u5feb\u53d6\u7684\u904b\u7b97\u6548\u7387\u4f4e\u3001\u63d0\u793a\u5927\u5c0f\u589e\u52a0\uff0c\u6216\u9650\u5236\u65bc\u55ae\u4e00\u6c7a\u7b56\u653f\u7b56\u3002\u70ba\u4e86\u6d88\u9664\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5fae\u8abf LLM \u4ee5\u9069\u61c9\u5373\u6642\u7ffb\u8b6f\u7684\u65b0\u6a21\u5f0f\uff0c\u7a31\u70ba SimulMask\u3002\u5b83\u5229\u7528\u4e00\u7a2e\u65b0\u7a4e\u7684\u6ce8\u610f\u529b\u906e\u7f69\u6280\u8853\uff0c\u900f\u904e\u5728\u6240\u9700\u6c7a\u7b56\u653f\u7b56\u4e0b\u906e\u7f69\u6ce8\u610f\u529b\u9023\u63a5\uff0c\u5728\u5fae\u8abf\u671f\u9593\u5c0d\u5373\u6642\u7ffb\u8b6f\u9032\u884c\u5efa\u6a21\u3002\u5c07\u5efa\u8b70\u7684 SimulMask \u61c9\u7528\u65bc IWSLT 2017 \u8cc7\u6599\u96c6\u7684 Falcon LLM\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u8207\u6700\u5148\u9032\u7684\u63d0\u793a\u6700\u4f73\u5316\u7b56\u7565\u76f8\u6bd4\uff0c\u5728\u56db\u7a2e\u4e0d\u540c\u7684\u5ef6\u9072\u6a21\u5f0f\u4e0b\u53d6\u5e73\u5747\u503c\u6642\uff0c\u4e09\u7a2e\u8a9e\u8a00\u5c0d\u7684\u7ffb\u8b6f\u54c1\u8cea\u6709\u986f\u8457\u7684\u63d0\u5347\uff0c\u540c\u6642\u964d\u4f4e\u4e86\u904b\u7b97\u6210\u672c\u3002", "author": "Matthew Raffel et.al.", "authors": "Matthew Raffel, Victor Agostinelli, Lizhong Chen", "id": "2405.10443v1", "paper_url": "http://arxiv.org/abs/2405.10443v1", "repo": "null"}}