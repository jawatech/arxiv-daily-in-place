{"2406.14898": {"publish_time": "2024-06-21", "title": "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "paper_summary": "Private data, being larger and quality-higher than public data, can greatly\nimprove large language models (LLM). However, due to privacy concerns, this\ndata is often dispersed in multiple silos, making its secure utilization for\nLLM training a challenge. Federated learning (FL) is an ideal solution for\ntraining models with distributed private data, but traditional frameworks like\nFedAvg are unsuitable for LLM due to their high computational demands on\nclients. An alternative, split learning, offloads most training parameters to\nthe server while training embedding and output layers locally, making it more\nsuitable for LLM. Nonetheless, it faces significant challenges in security and\nefficiency. Firstly, the gradients of embeddings are prone to attacks, leading\nto potential reverse engineering of private data. Furthermore, the server's\nlimitation of handle only one client's training request at a time hinders\nparallel training, severely impacting training efficiency. In this paper, we\npropose a Federated Learning framework for LLM, named FL-GLM, which prevents\ndata leakage caused by both server-side and peer-client attacks while improving\ntraining efficiency. Specifically, we first place the input block and output\nblock on local client to prevent embedding gradient attacks from server.\nSecondly, we employ key-encryption during client-server communication to\nprevent reverse engineering attacks from peer-clients. Lastly, we employ\noptimization methods like client-batching or server-hierarchical, adopting\ndifferent acceleration methods based on the actual computational capabilities\nof the server. Experimental results on NLU and generation tasks demonstrate\nthat FL-GLM achieves comparable metrics to centralized chatGLM model,\nvalidating the effectiveness of our federated learning framework.", "paper_summary_zh": "<paragraph>\u79c1\u4eba\u6578\u64da\u6bd4\u516c\u958b\u6578\u64da\u66f4\u5927\u4e14\u54c1\u8cea\u66f4\u9ad8\uff0c\u53ef\u4ee5\u5927\u5e45\u6539\u5584\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u7136\u800c\uff0c\u7531\u65bc\u96b1\u79c1\u554f\u984c\uff0c\u9019\u4e9b\u6578\u64da\u901a\u5e38\u5206\u6563\u5728\u591a\u500b\u8cc7\u6599\u5b64\u5cf6\u4e2d\uff0c\u4f7f\u5f97\u5176\u5b89\u5168\u5229\u7528\u6210\u70ba LLM \u8a13\u7df4\u7684\u4e00\u9805\u6311\u6230\u3002\u806f\u5408\u5b78\u7fd2 (FL) \u662f\u4f7f\u7528\u5206\u6563\u7684\u79c1\u4eba\u6578\u64da\u8a13\u7df4\u6a21\u578b\u7684\u7406\u60f3\u89e3\u6c7a\u65b9\u6848\uff0c\u4f46 FedAvg \u7b49\u50b3\u7d71\u6846\u67b6\u7531\u65bc\u5c0d\u7528\u6236\u7aef\u7684\u9ad8\u904b\u7b97\u9700\u6c42\u800c\u4e0d\u9069\u5408 LLM\u3002\u4e00\u7a2e\u66ff\u4ee3\u65b9\u6848\uff0c\u5206\u5272\u5f0f\u5b78\u7fd2\uff0c\u5c07\u5927\u591a\u6578\u8a13\u7df4\u53c3\u6578\u5378\u8f09\u5230\u4f3a\u670d\u5668\uff0c\u540c\u6642\u5728\u672c\u5730\u8a13\u7df4\u5d4c\u5165\u548c\u8f38\u51fa\u5c64\uff0c\u4f7f\u5176\u66f4\u9069\u5408 LLM\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u5b83\u5728\u5b89\u5168\u6027\u548c\u6548\u7387\u65b9\u9762\u9762\u81e8\u8457\u91cd\u5927\u6311\u6230\u3002\u9996\u5148\uff0c\u5d4c\u5165\u7684\u68af\u5ea6\u5bb9\u6613\u53d7\u5230\u653b\u64ca\uff0c\u5c0e\u81f4\u6f5b\u5728\u7684\u79c1\u4eba\u6578\u64da\u9006\u5411\u5de5\u7a0b\u3002\u6b64\u5916\uff0c\u4f3a\u670d\u5668\u4e00\u6b21\u53ea\u80fd\u8655\u7406\u4e00\u500b\u7528\u6236\u7aef\u7684\u8a13\u7df4\u8acb\u6c42\u7684\u9650\u5236\u963b\u7919\u4e86\u4e26\u884c\u8a13\u7df4\uff0c\u56b4\u91cd\u5f71\u97ff\u4e86\u8a13\u7df4\u6548\u7387\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba FL-GLM \u7684 LLM \u806f\u90a6\u5b78\u7fd2\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u9632\u6b62\u4f3a\u670d\u5668\u7aef\u548c\u5c0d\u7b49\u7528\u6236\u7aef\u653b\u64ca\u9020\u6210\u7684\u6578\u64da\u6d29\u6f0f\uff0c\u540c\u6642\u63d0\u9ad8\u8a13\u7df4\u6548\u7387\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u5c07\u8f38\u5165\u5340\u584a\u548c\u8f38\u51fa\u5340\u584a\u653e\u7f6e\u5728\u672c\u5730\u7528\u6236\u7aef\uff0c\u4ee5\u9632\u6b62\u4f3a\u670d\u5668\u767c\u8d77\u7684\u5d4c\u5165\u68af\u5ea6\u653b\u64ca\u3002\u5176\u6b21\uff0c\u6211\u5011\u5728\u7528\u6236\u7aef-\u4f3a\u670d\u5668\u901a\u4fe1\u671f\u9593\u63a1\u7528\u5bc6\u9470\u52a0\u5bc6\uff0c\u4ee5\u9632\u6b62\u5c0d\u7b49\u7528\u6236\u7aef\u767c\u8d77\u7684\u9006\u5411\u5de5\u7a0b\u653b\u64ca\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63a1\u7528\u7528\u6236\u7aef\u6279\u6b21\u8655\u7406\u6216\u4f3a\u670d\u5668\u5206\u5c64\u7b49\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u6839\u64da\u4f3a\u670d\u5668\u7684\u5be6\u969b\u904b\u7b97\u80fd\u529b\u63a1\u7528\u4e0d\u540c\u7684\u52a0\u901f\u65b9\u6cd5\u3002NLU \u548c\u751f\u6210\u4efb\u52d9\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cFL-GLM \u9054\u5230\u4e86\u8207\u96c6\u4e2d\u5f0f chatGLM \u6a21\u578b\u76f8\u7576\u7684\u6307\u6a19\uff0c\u9a57\u8b49\u4e86\u6211\u5011\u806f\u5408\u5b78\u7fd2\u6846\u67b6\u7684\u6709\u6548\u6027\u3002</paragraph>", "author": "JiaYing Zheng et.al.", "authors": "JiaYing Zheng, HaiNan Zhang, LingXiang Wang, WangJie Qiu, HongWei Zheng, ZhiMing Zheng", "id": "2406.14898v1", "paper_url": "http://arxiv.org/abs/2406.14898v1", "repo": "null"}}