{"2501.08313": {"publish_time": "2025-01-14", "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention", "paper_summary": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\nwhich are comparable to top-tier models while offering superior capabilities in\nprocessing longer contexts. The core lies in lightning attention and its\nefficient scaling. To maximize computational capacity, we integrate it with\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\ntotal parameters, of which 45.9 billion are activated for each token. We\ndevelop an optimized parallel strategy and highly efficient\ncomputation-communication overlap techniques for MoE and lightning attention.\nThis approach enables us to conduct efficient training and inference on models\nwith hundreds of billions of parameters across contexts spanning millions of\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\nduring training and extrapolate to 4 million tokens during inference at an\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\ncontinued training with 512 billion vision-language tokens. Experiments on both\nstandard and in-house benchmarks show that our models match the performance of\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\ntimes longer context window. We publicly release MiniMax-01 at\nhttps://github.com/MiniMax-AI.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a8\u51fa MiniMax-01 \u7cfb\u5217\uff0c\u5305\u62ec MiniMax-Text-01 \u548c MiniMax-VL-01\uff0c\n\u5b83\u5011\u8207\u9802\u7d1a\u6a21\u578b\u76f8\u7576\uff0c\u540c\u6642\u5728\u8655\u7406\u8f03\u9577\u8a9e\u5883\u65b9\u9762\u63d0\u4f9b\u5353\u8d8a\u7684\u529f\u80fd\u3002\u6838\u5fc3\u5728\u65bc\u9583\u96fb\u6ce8\u610f\u529b\u53ca\u5176\u9ad8\u6548\u7e2e\u653e\u3002\u70ba\u4e86\u6700\u5927\u5316\u8a08\u7b97\u80fd\u529b\uff0c\u6211\u5011\u5c07\u5176\u8207\u5c08\u5bb6\u6df7\u5408 (MoE) \u6574\u5408\uff0c\u5efa\u7acb\u4e00\u500b\u64c1\u6709 32 \u500b\u5c08\u5bb6\u548c 4560 \u5104\u500b\u7e3d\u53c3\u6578\u7684\u6a21\u578b\uff0c\u5176\u4e2d 459 \u5104\u500b\u53c3\u6578\u88ab\u555f\u7528\u65bc\u6bcf\u500b\u4ee3\u5e63\u3002\u6211\u5011\u70ba MoE \u548c\u9583\u96fb\u6ce8\u610f\u529b\u958b\u767c\u4e86\u4e00\u500b\u512a\u5316\u7684\u4e26\u884c\u7b56\u7565\u548c\u9ad8\u6548\u7684\u8a08\u7b97\u901a\u8a0a\u91cd\u758a\u6280\u8853\u3002\u9019\u7a2e\u65b9\u6cd5\u4f7f\u6211\u5011\u80fd\u5920\u5c0d\u64c1\u6709\u6578\u767e\u5104\u500b\u53c3\u6578\u7684\u6a21\u578b\u9032\u884c\u9ad8\u6548\u7684\u8a13\u7df4\u548c\u63a8\u7406\uff0c\u800c\u9019\u4e9b\u53c3\u6578\u8de8\u8d8a\u6578\u767e\u842c\u500b\u4ee3\u5e63\u7684\u8a9e\u5883\u3002MiniMax-Text-01 \u7684\u8a9e\u5883\u7a97\u53e3\u5728\u8a13\u7df4\u671f\u9593\u53ef\u4ee5\u9054\u5230 100 \u842c\u500b\u4ee3\u5e63\uff0c\u4e26\u5728\u63a8\u7406\u671f\u9593\u4ee5\u5408\u7406\u6210\u672c\u63a8\u65b7\u5230 400 \u842c\u500b\u4ee3\u5e63\u3002\u6211\u5011\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b MiniMax-VL-01 \u662f\u900f\u904e\u6301\u7e8c\u8a13\u7df4 5120 \u5104\u500b\u8996\u89ba\u8a9e\u8a00\u4ee3\u5e63\u5efa\u7acb\u7684\u3002\u5728\u6a19\u6e96\u57fa\u6e96\u548c\u5167\u90e8\u57fa\u6e96\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u8207 GPT-4o \u548c Claude-3.5-Sonnet \u7b49\u6700\u5148\u9032\u6a21\u578b\u7684\u6027\u80fd\u76f8\u5339\u914d\uff0c\u540c\u6642\u63d0\u4f9b 20-32 \u500d\u66f4\u9577\u7684\u8a9e\u5883\u7a97\u53e3\u3002\u6211\u5011\u5728 https://github.com/MiniMax-AI \u4e0a\u516c\u958b\u767c\u5e03 MiniMax-01\u3002</paragraph>", "author": "MiniMax et.al.", "authors": "MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu", "id": "2501.08313v1", "paper_url": "http://arxiv.org/abs/2501.08313v1", "repo": "null"}}