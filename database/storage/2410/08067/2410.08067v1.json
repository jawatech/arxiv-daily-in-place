{"2410.08067": {"publish_time": "2024-10-10", "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs", "paper_summary": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u504f\u597d\u5c0d\u9f4a\u986f\u8457\u63d0\u5347\u4e86\u5b83\u5011\u9075\u5b88\u4eba\u985e\u6307\u4ee4\u548c\u610f\u5716\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u76f4\u63a5\u5c0d\u9f4a\u6f14\u7b97\u6cd5\u4e3b\u8981\u95dc\u6ce8\u76f8\u5c0d\u504f\u597d\uff0c\u4e14\u5e38\u5e38\u5ffd\u7565\u56de\u61c9\u7684\u8cea\u5316\u9762\u5411\u3002\u52aa\u529b\u6700\u5927\u5316\u6240\u9078\u56de\u61c9\u548c\u7565\u905c\u4e00\u7c4c\u7684\u88ab\u62d2\u7d55\u56de\u61c9\u4e4b\u9593\u7684\u96b1\u542b\u734e\u52f5\u5dee\u8ddd\uff0c\u53ef\u80fd\u5c0e\u81f4\u904e\u5ea6\u64ec\u5408\u548c\u4e0d\u5fc5\u8981\u5730\u907a\u5fd8\u9ad8\u54c1\u8cea\u7684\u88ab\u62d2\u7d55\u56de\u61c9\u3002\u5c0d\u734e\u52f5\u5206\u6578\u7684\u7121\u77e5\u4e5f\u9a45\u4f7f LLM \u6beb\u7121\u5340\u5225\u5730\u504f\u597d\u4f4e\u54c1\u8cea\u7684\u6240\u9078\u56de\u61c9\uff0c\u4e14\u7121\u6cd5\u6982\u62ec\u5230\u734e\u52f5\u6700\u9ad8\u7684\u56de\u61c9\uff0c\u800c\u9019\u4e9b\u56de\u61c9\u5728\u8cc7\u6599\u4e2d\u662f\u7a00\u758f\u7684\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u7f3a\u9ede\uff0c\u6211\u5011\u7684\u7814\u7a76\u5f15\u5165\u4e86\u734e\u52f5\u689d\u4ef6\u7684 LLM \u653f\u7b56\uff0c\u5b83\u80fd\u8fa8\u5225\u4e26\u5f9e\u8cc7\u6599\u96c6\u5167\u56de\u61c9\u54c1\u8cea\u7684\u6574\u500b\u7bc4\u570d\u4e2d\u5b78\u7fd2\uff0c\u6709\u52a9\u65bc\u5916\u63a8\u5230\u66f4\u4f73\u7684\u5340\u57df\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u6709\u6548\u4e14\u7c21\u55ae\u7684\u8cc7\u6599\u91cd\u65b0\u6a19\u7c64\u65b9\u6cd5\uff0c\u5b83\u4ee5\u54c1\u8cea\u5206\u6578\u70ba\u689d\u4ef6\u7684\u504f\u597d\u5c0d\u4f86\u5efa\u69cb\u4e00\u500b\u734e\u52f5\u589e\u5f37\u7684\u8cc7\u6599\u96c6\u3002\u9019\u500b\u8cc7\u6599\u96c6\u5f88\u5bb9\u6613\u8207\u73fe\u6709\u7684\u76f4\u63a5\u5c0d\u9f4a\u6f14\u7b97\u6cd5\u6574\u5408\uff0c\u4e14\u9069\u7528\u65bc\u4efb\u4f55\u504f\u597d\u8cc7\u6599\u96c6\u3002\u5728\u5305\u62ec AlpacaEval\u3001MT-Bench \u548c Arena-Hard-Auto \u5728\u5167\u7684\u6307\u4ee4\u9075\u5faa\u57fa\u6e96\u4e2d\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u505a\u6cd5\u4e00\u81f4\u5730\u4ee5\u76f8\u7576\u5927\u7684\u5e45\u5ea6\u63d0\u5347\u4e86\u4e0d\u540c\u6a21\u578b\u7684 DPO \u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u505a\u6cd5\u4e5f\u63d0\u5347\u4e86\u5404\u7a2e\u5b78\u8853\u57fa\u6e96\u7684\u5e73\u5747\u6e96\u78ba\u5ea6\u3002\u7576\u5c07\u6211\u5011\u7684\u505a\u6cd5\u61c9\u7528\u65bc\u7b56\u7565\u8cc7\u6599\u6642\uff0c\u7522\u751f\u7684 DPO \u6a21\u578b\u5728 AlpacaEval \u4e0a\u9054\u5230\u4e86 SOTA \u7d50\u679c\u3002\u900f\u904e\u6d88\u878d\u7814\u7a76\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u505a\u6cd5\u4e0d\u50c5\u6700\u5927\u5316\u4e86\u504f\u597d\u8cc7\u6599\u7684\u6548\u7528\uff0c\u4e5f\u6e1b\u8f15\u4e86\u907a\u5fd8\u7684\u554f\u984c\uff0c\u8b49\u660e\u4e86\u5b83\u8d85\u8d8a\u55ae\u7d14\u7684\u8cc7\u6599\u96c6\u64f4\u5145\u7684\u5ee3\u6cdb\u6548\u529b\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u4ee5\u5728 https://github.com/shenao-zhang/reward-augmented-preference \u53d6\u5f97\u3002", "author": "Shenao Zhang et.al.", "authors": "Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang", "id": "2410.08067v1", "paper_url": "http://arxiv.org/abs/2410.08067v1", "repo": "https://github.com/shenao-zhang/reward-augmented-preference"}}