{"2501.09749": {"publish_time": "2025-01-16", "title": "Enhancing Lexicon-Based Text Embeddings with Large Language Models", "paper_summary": "Recent large language models (LLMs) have demonstrated exceptional performance\non general-purpose text embedding tasks. While dense embeddings have dominated\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\nthe inherent tokenization redundancy issue and unidirectional attention\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\nthrough token embedding clustering, and investigates bidirectional attention\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\nby assigning each dimension to a specific token cluster, where semantically\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\ndelivering compact feature representations that match the sizes of dense\ncounterparts. Notably, combining LENSE with dense embeddings achieves\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).", "paper_summary_zh": "\u6700\u8fd1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u901a\u7528\u6587\u5b57\u5d4c\u5165\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u975e\u51e1\u7684\u6027\u80fd\u3002\u867d\u7136\u7a20\u5bc6\u5d4c\u5165\u4e3b\u5bfc\u76f8\u5173\u7814\u7a76\uff0c\u4f46\u6211\u4eec\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u8bcd\u5178\u7684\u5d4c\u5165 (LENS)\uff0c\u5229\u7528 LLM \u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u5173\u4e8e\u4f20\u7edf\u56e0\u679c LLM \u4e2d\u56fa\u6709\u7684\u6807\u8bb0\u5316\u5197\u4f59\u95ee\u9898\u548c\u5355\u5411\u6ce8\u610f\u9650\u5236\uff0cLENS \u901a\u8fc7\u6807\u8bb0\u5d4c\u5165\u805a\u7c7b\u5de9\u56fa\u8bcd\u6c47\u7a7a\u95f4\uff0c\u5e76\u7814\u7a76\u53cc\u5411\u6ce8\u610f\u548c\u5404\u79cd\u6c60\u5316\u7b56\u7565\u3002\u5177\u4f53\u6765\u8bf4\uff0cLENS \u901a\u8fc7\u5c06\u6bcf\u4e2a\u7ef4\u5ea6\u5206\u914d\u7ed9\u4e00\u4e2a\u7279\u5b9a\u7684\u6807\u8bb0\u7fa4\u96c6\u6765\u7b80\u5316\u8bcd\u5178\u5339\u914d\uff0c\u5176\u4e2d\u8bed\u4e49\u76f8\u4f3c\u7684\u6807\u8bb0\u88ab\u5206\u7ec4\u5728\u4e00\u8d77\uff0c\u5e76\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u91ca\u653e LLM \u7684\u5168\u90e8\u6f5c\u529b\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLENS \u5728\u6d77\u91cf\u6587\u672c\u5d4c\u5165\u57fa\u51c6 (MTEB) \u4e0a\u4f18\u4e8e\u7a20\u5bc6\u5d4c\u5165\uff0c\u63d0\u4f9b\u4e86\u4e0e\u7a20\u5bc6\u5bf9\u5e94\u7269\u5927\u5c0f\u5339\u914d\u7684\u7d27\u51d1\u7279\u5f81\u8868\u793a\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5c06 LENSE \u4e0e\u7a20\u5bc6\u5d4c\u5165\u76f8\u7ed3\u5408\u5728 MTEB \u7684\u68c0\u7d22\u5b50\u96c6\uff08\u5373 BEIR\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "author": "Yibin Lei et.al.", "authors": "Yibin Lei, Tao Shen, Yu Cao, Andrew Yates", "id": "2501.09749v1", "paper_url": "http://arxiv.org/abs/2501.09749v1", "repo": "null"}}