{"2409.18082": {"publish_time": "2024-09-26", "title": "SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation", "paper_summary": "Automating garment manipulation poses a significant challenge for assistive\nrobotics due to the diverse and deformable nature of garments. Traditional\napproaches typically require separate models for each garment type, which\nlimits scalability and adaptability. In contrast, this paper presents a unified\napproach using vision-language models (VLMs) to improve keypoint prediction\nacross various garment categories. By interpreting both visual and semantic\ninformation, our model enables robots to manage different garment states with a\nsingle model. We created a large-scale synthetic dataset using advanced\nsimulation techniques, allowing scalable training without extensive real-world\ndata. Experimental results indicate that the VLM-based method significantly\nenhances keypoint detection accuracy and task success rates, providing a more\nflexible and general solution for robotic garment manipulation. In addition,\nthis research also underscores the potential of VLMs to unify various garment\nmanipulation tasks within a single framework, paving the way for broader\napplications in home automation and assistive robotics for future.", "paper_summary_zh": "\u81ea\u52d5\u5316\u670d\u98fe\u64cd\u4f5c\u5c0d\u8f14\u52a9\u6a5f\u5668\u4eba\u6280\u8853\u69cb\u6210\u91cd\u5927\u6311\u6230\uff0c\u56e0\u70ba\u670d\u98fe\u5177\u6709\u591a\u6a23\u4e14\u53ef\u8b8a\u5f62\u7684\u7279\u6027\u3002\u50b3\u7d71\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u91dd\u5c0d\u6bcf\u7a2e\u670d\u98fe\u985e\u578b\u5efa\u7acb\u7368\u7acb\u7684\u6a21\u578b\uff0c\u9019\u9650\u5236\u4e86\u53ef\u64f4\u5145\u6027\u548c\u9069\u61c9\u6027\u3002\u76f8\u53cd\u5730\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u500b\u7d71\u4e00\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4f86\u6539\u5584\u5404\u7a2e\u670d\u98fe\u985e\u5225\u7684\u95dc\u9375\u9ede\u9810\u6e2c\u3002\u900f\u904e\u89e3\u8b80\u8996\u89ba\u548c\u8a9e\u7fa9\u8cc7\u8a0a\uff0c\u6211\u5011\u7684\u6a21\u578b\u8b93\u6a5f\u5668\u4eba\u80fd\u5920\u4f7f\u7528\u55ae\u4e00\u6a21\u578b\u4f86\u7ba1\u7406\u4e0d\u540c\u7684\u670d\u98fe\u72c0\u614b\u3002\u6211\u5011\u4f7f\u7528\u9032\u968e\u6a21\u64ec\u6280\u8853\u5efa\u7acb\u4e86\u4e00\u500b\u5927\u578b\u5408\u6210\u8cc7\u6599\u96c6\uff0c\u5141\u8a31\u9032\u884c\u53ef\u64f4\u5145\u7684\u8a13\u7df4\uff0c\u800c\u4e0d\u9700\u8981\u5927\u91cf\u7684\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u57fa\u65bc VLM \u7684\u65b9\u6cd5\u986f\u8457\u63d0\u5347\u4e86\u95dc\u9375\u9ede\u5075\u6e2c\u7684\u6e96\u78ba\u5ea6\u548c\u4efb\u52d9\u6210\u529f\u7387\uff0c\u70ba\u6a5f\u5668\u4eba\u670d\u98fe\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u9748\u6d3b\u4e14\u901a\u7528\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u6b64\u5916\uff0c\u9019\u9805\u7814\u7a76\u4e5f\u5f37\u8abf\u4e86 VLM \u5c07\u5404\u7a2e\u670d\u98fe\u64cd\u4f5c\u4efb\u52d9\u7d71\u4e00\u5728\u55ae\u4e00\u67b6\u69cb\u4e2d\u7684\u6f5b\u529b\uff0c\u70ba\u672a\u4f86\u5c45\u5bb6\u81ea\u52d5\u5316\u548c\u8f14\u52a9\u6a5f\u5668\u4eba\u6280\u8853\u7684\u5ee3\u6cdb\u61c9\u7528\u92ea\u8def\u3002", "author": "Xin Li et.al.", "authors": "Xin Li, Siyuan Huang, Qiaojun Yu, Zhengkai Jiang, Ce Hao, Yimeng Zhu, Hongsheng Li, Peng Gao, Cewu Lu", "id": "2409.18082v1", "paper_url": "http://arxiv.org/abs/2409.18082v1", "repo": "null"}}