{"2502.15264": {"publish_time": "2025-02-21", "title": "Retrieval-Augmented Speech Recognition Approach for Domain Challenges", "paper_summary": "Speech recognition systems often face challenges due to domain mismatch,\nparticularly in real-world applications where domain-specific data is\nunavailable because of data accessibility and confidentiality constraints.\nInspired by Retrieval-Augmented Generation (RAG) techniques for large language\nmodels (LLMs), this paper introduces a LLM-based retrieval-augmented speech\nrecognition method that incorporates domain-specific textual data at the\ninference stage to enhance recognition performance. Rather than relying on\ndomain-specific textual data during the training phase, our model is trained to\nlearn how to utilize textual information provided in prompts for LLM decoder to\nimprove speech recognition performance. Benefiting from the advantages of the\nRAG retrieval mechanism, our approach efficiently accesses locally available\ndomain-specific documents, ensuring a convenient and effective process for\nsolving domain mismatch problems. Experiments conducted on the CSJ database\ndemonstrate that the proposed method significantly improves speech recognition\naccuracy and achieves state-of-the-art results on the CSJ dataset, even without\nrelying on the full training data.", "paper_summary_zh": "\u8a9e\u97f3\u8fa8\u8b58\u7cfb\u7d71\u901a\u5e38\u6703\u56e0\u70ba\u9818\u57df\u4e0d\u7b26\u800c\u9762\u81e8\u6311\u6230\uff0c\u7279\u5225\u662f\u5728\u5be6\u969b\u7684\u61c9\u7528\u4e2d\uff0c\u56e0\u70ba\u8cc7\u6599\u53d6\u5f97\u548c\u6a5f\u5bc6\u9650\u5236\uff0c\u7121\u6cd5\u53d6\u5f97\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u6599\u3002\u672c\u8ad6\u6587\u53d7\u5230\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6aa2\u7d22\u589e\u5f37\u751f\u6210\uff08RAG\uff09\u6280\u8853\u555f\u767c\uff0c\u4ecb\u7d39\u4e86\u4e00\u7a2e\u57fa\u65bc LLM \u7684\u6aa2\u7d22\u589e\u5f37\u8a9e\u97f3\u8fa8\u8b58\u65b9\u6cd5\uff0c\u5b83\u5728\u63a8\u8ad6\u968e\u6bb5\u7d0d\u5165\u4e86\u7279\u5b9a\u9818\u57df\u7684\u6587\u5b57\u8cc7\u6599\uff0c\u4ee5\u589e\u5f37\u8fa8\u8b58\u6548\u80fd\u3002\u6211\u5011\u7684\u6a21\u578b\u4e26\u975e\u5728\u8a13\u7df4\u968e\u6bb5\u4f9d\u8cf4\u7279\u5b9a\u9818\u57df\u7684\u6587\u5b57\u8cc7\u6599\uff0c\u800c\u662f\u8a13\u7df4\u6a21\u578b\u5b78\u7fd2\u5982\u4f55\u5229\u7528\u63d0\u793a\u4e2d\u63d0\u4f9b\u7684\u6587\u5b57\u8cc7\u8a0a\uff0c\u4ee5\u6539\u5584 LLM \u89e3\u78bc\u5668\u7684\u8a9e\u97f3\u8fa8\u8b58\u6548\u80fd\u3002\u6211\u5011\u7684\u505a\u6cd5\u53d7\u76ca\u65bc RAG \u6aa2\u7d22\u6a5f\u5236\u7684\u512a\u9ede\uff0c\u80fd\u6709\u6548\u5b58\u53d6\u7576\u5730\u53ef\u7528\u7684\u7279\u5b9a\u9818\u57df\u6587\u4ef6\uff0c\u78ba\u4fdd\u89e3\u6c7a\u9818\u57df\u4e0d\u7b26\u554f\u984c\u7684\u904e\u7a0b\u65e2\u4fbf\u5229\u53c8\u6709\u6548\u3002\u5728 CSJ \u8cc7\u6599\u5eab\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5927\u5e45\u6539\u5584\u4e86\u8a9e\u97f3\u8fa8\u8b58\u7684\u6e96\u78ba\u5ea6\uff0c\u5373\u4f7f\u4e0d\u4f9d\u8cf4\u5b8c\u6574\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u4e5f\u80fd\u5728 CSJ \u8cc7\u6599\u96c6\u4e0a\u9054\u6210\u6700\u5148\u9032\u7684\u7d50\u679c\u3002", "author": "Peng Shen et.al.", "authors": "Peng Shen, Xugang Lu, Hisashi Kawai", "id": "2502.15264v1", "paper_url": "http://arxiv.org/abs/2502.15264v1", "repo": "null"}}