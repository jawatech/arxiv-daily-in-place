{"2410.18952": {"publish_time": "2024-10-24", "title": "Dynamic Vocabulary Pruning in Early-Exit LLMs", "paper_summary": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u898f\u6a21\u8d8a\u5927\uff0c\u6548\u80fd\u8868\u73fe\u8d8a\u597d\uff0c\u9019\u9ede\u5df2\u7372\u5f97\u8b49\u5be6\u3002\u7136\u800c\uff0c\u4ee3\u50f9\u662f\u63a8\u8ad6\u901f\u5ea6\u8b8a\u6162\u4e14\u6210\u672c\u8b8a\u9ad8\u3002\u65e9\u671f\u9000\u51fa\u662f\u4e00\u7a2e\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u900f\u904e\u5728\u4e2d\u9593\u5c64\u9032\u884c\u4e0b\u4e00\u500b\u4ee3\u5e63\u9810\u6e2c\uff0c\u4f86\u63d0\u5347 LLM \u63a8\u8ad6\u7684\u6548\u7387\u3002\u7136\u800c\uff0c\u73fe\u4ee3 LLM \u4e2d\u9f90\u5927\u7684\u8a5e\u5f59\u91cf\uff0c\u4f7f\u5f97\u505a\u51fa\u9000\u51fa\u6c7a\u7b56\u6240\u9700\u7684\u4fe1\u5fc3\u4f30\u8a08\u5728\u904b\u7b97\u4e0a\u5341\u5206\u6602\u8cb4\uff0c\u9019\u964d\u4f4e\u4e86\u6548\u7387\u7684\u63d0\u5347\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u5728\u6e2c\u8a66\u6642\u52d5\u614b\u4fee\u526a\u6bcf\u500b\u4ee3\u5e63\u7684\u8a5e\u5f59\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u8a5e\u5f59\u6703\u5728\u5176\u4e2d\u4e00\u5c64\u9032\u884c\u4fee\u526a\uff0c\u7136\u5f8c\u8f03\u5c0f\u7684\u8a5e\u5f59\u6703\u5728\u524d\u9032\u50b3\u905e\u7684\u5176\u9918\u90e8\u5206\u4e2d\u4f7f\u7528\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u9019\u7a2e\u4e8b\u5f8c\u52d5\u614b\u8a5e\u5f59\u4fee\u526a\u53ef\u4ee5\u63d0\u5347\u65e9\u671f\u9000\u51fa LLM \u4e2d\u4fe1\u5fc3\u4f30\u8a08\u7684\u6548\u7387\uff0c\u540c\u6642\u7dad\u6301\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002", "author": "Jort Vincenti et.al.", "authors": "Jort Vincenti, Karim Abdel Sadek, Joan Velja, Matteo Nulli, Metod Jazbec", "id": "2410.18952v1", "paper_url": "http://arxiv.org/abs/2410.18952v1", "repo": "https://github.com/matteonulli/vocabulary_pruning"}}