{"2501.03226": {"publish_time": "2025-01-06", "title": "BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning", "paper_summary": "Cutting-edge large language models (LLMs) demonstrate promising performance\nin solving complex math problems with a divide-and-conquer pipeline and the\nassistance of in-context learning (ICL) examples. However, their potential for\nimprovement is limited by two critical problems within their ICL examples:\ngranularity-mismatch and the ensuing negative-effect noise problem.\nSpecifically, the LLMs are capable of the dividing process yet mostly failed by\ninaccurate reasoning within a few conquer steps, while the ICL examples\nretrieved in question-grained sometimes lack relevant steps for a specific\nchallenging reasoning step. Further, this disconnect may hinder the correct\nreasoning due to its irrelevance. To this end, we focus on improving the\nreasoning quality within each step and present BoostStep. BoostStep aligns the\ngranularity between the retrieving and reasoning on step grained, and provides\nhighly related ICL examples for each reasoning step with a novel `first-try'\nstrategy. BoostStep provides more relevant examples than the coarse\nquestion-grained strategy, enhancing the model reasoning quality within each\nstep steadily. BoostStep is a general and robust reasoning-enhancing method\nthat not only improves standalone reasoning performance but also integrates\nseamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate\ngeneration and decision-making. Quantitatively, it improves GPT-4o and\nQwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical\nbenchmarks, and 7.5\\% gain combined with MCTS.", "paper_summary_zh": "\u5c16\u7aef\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u89e3\u6c7a\u8907\u96dc\u7684\u6578\u5b78\u554f\u984c\u4e0a\u5c55\u73fe\u4e86\u6709\u524d\u9014\u7684\u8868\u73fe\uff0c\u63a1\u7528\u5206\u800c\u6cbb\u4e4b\u7684\u7ba1\u9053\u548c\u60c5\u5883\u5b78\u7fd2 (ICL) \u7bc4\u4f8b\u7684\u5354\u52a9\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u9032\u6b65\u6f5b\u529b\u53d7\u5230 ICL \u7bc4\u4f8b\u4e2d\u5169\u500b\u95dc\u9375\u554f\u984c\u7684\u9650\u5236\uff1a\u7c92\u5ea6\u4e0d\u5339\u914d\u548c\u96a8\u4e4b\u800c\u4f86\u7684\u8ca0\u9762\u6548\u61c9\u566a\u97f3\u554f\u984c\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cLLM \u6709\u80fd\u529b\u9032\u884c\u5206\u9694\u8655\u7406\uff0c\u4f46\u5728\u5e7e\u500b\u5f81\u670d\u6b65\u9a5f\u4e2d\u7684\u4e0d\u6e96\u78ba\u63a8\u7406\u5927\u591a\u5931\u6557\uff0c\u800c\u4ee5\u554f\u984c\u7c92\u5ea6\u6aa2\u7d22\u7684 ICL \u7bc4\u4f8b\u6709\u6642\u7f3a\u4e4f\u7279\u5b9a\u6311\u6230\u6027\u63a8\u7406\u6b65\u9a5f\u7684\u76f8\u5173\u6b65\u9a5f\u3002\u6b64\u5916\uff0c\u9019\u7a2e\u812b\u7bc0\u53ef\u80fd\u6703\u56e0\u5176\u7121\u95dc\u6027\u800c\u963b\u7919\u6b63\u78ba\u7684\u63a8\u7406\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u63d0\u9ad8\u6bcf\u500b\u6b65\u9a5f\u4e2d\u7684\u63a8\u7406\u54c1\u8cea\uff0c\u4e26\u63d0\u51fa BoostStep\u3002BoostStep \u5728\u6b65\u9a5f\u7c92\u5ea6\u4e0a\u8abf\u6574\u6aa2\u7d22\u548c\u63a8\u7406\u4e4b\u9593\u7684\u7c92\u5ea6\uff0c\u4e26\u4f7f\u7528\u65b0\u7a4e\u7684\u300c\u9996\u6b21\u5617\u8a66\u300d\u7b56\u7565\u70ba\u6bcf\u500b\u63a8\u7406\u6b65\u9a5f\u63d0\u4f9b\u9ad8\u5ea6\u76f8\u95dc\u7684 ICL \u7bc4\u4f8b\u3002BoostStep \u63d0\u4f9b\u6bd4\u7c97\u7565\u7684\u554f\u984c\u7c92\u5ea6\u7b56\u7565\u66f4\u76f8\u95dc\u7684\u7bc4\u4f8b\uff0c\u7a69\u6b65\u63d0\u5347\u6bcf\u500b\u6b65\u9a5f\u4e2d\u7684\u6a21\u578b\u63a8\u7406\u54c1\u8cea\u3002BoostStep \u662f\u4e00\u7a2e\u901a\u7528\u4e14\u5f37\u5927\u7684\u63a8\u7406\u589e\u5f37\u65b9\u6cd5\uff0c\u4e0d\u50c5\u6539\u5584\u7368\u7acb\u63a8\u7406\u8868\u73fe\uff0c\u9084\u80fd\u8207\u8499\u5730\u5361\u7f85\u6a39\u72c0\u641c\u5c0b\u65b9\u6cd5 (MCTS) \u7121\u7e2b\u6574\u5408\uff0c\u4ee5\u6539\u5584\u5019\u9078\u7522\u751f\u548c\u6c7a\u7b56\u5236\u5b9a\u3002\u5728\u91cf\u5316\u65b9\u9762\uff0c\u5b83\u5206\u5225\u5728\u5404\u7a2e\u6578\u5b78\u57fa\u6e96\u4e0a\u5c07 GPT-4o \u548c Qwen2.5-Math-72B \u63d0\u5347\u4e86 3.6% \u548c 2.0%\uff0c\u4e26\u7d50\u5408 MCTS \u7372\u5f97\u4e86 7.5% \u7684\u589e\u76ca\u3002", "author": "Beichen Zhang et.al.", "authors": "Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao, Dahua Lin, Jiaqi Wang", "id": "2501.03226v1", "paper_url": "http://arxiv.org/abs/2501.03226v1", "repo": "https://github.com/beichenzbc/booststep"}}