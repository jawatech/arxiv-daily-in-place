{"2412.04277": {"publish_time": "2024-12-05", "title": "Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic", "paper_summary": "Large Language Models (LLMs) have shown impressive results in multiple\ndomains of natural language processing (NLP) but are mainly focused on the\nEnglish language. Recently, more LLMs have incorporated a larger proportion of\nmultilingual text to represent low-resource languages. In Arabic NLP, several\nArabic-centric LLMs have shown remarkable results on multiple benchmarks in the\npast two years. However, most Arabic LLMs have more than 7 billion parameters,\nwhich increases their hardware requirements and inference latency, when\ncompared to smaller LLMs. This paper introduces Arabic Stable LM 1.6B in a base\nand chat version as a small but powerful Arabic-centric LLM. Our Arabic Stable\nLM 1.6B chat model achieves impressive results on several benchmarks beating\nmultiple models with up to 8x the parameters. In addition, we show the benefit\nof mixing in synthetic instruction tuning data by augmenting our fine-tuning\ndata with a large synthetic dialogue dataset.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u7684\u591a\u500b\u9818\u57df\u4e2d\u5c55\u73fe\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\uff0c\u4f46\u4e3b\u8981\u8457\u91cd\u65bc\u82f1\u8a9e\u3002\u6700\u8fd1\uff0c\u66f4\u591a LLM \u878d\u5165\u4e86\u66f4\u5927\u6bd4\u4f8b\u7684\u591a\u8a9e\u8a00\u6587\u5b57\uff0c\u4ee5\u5448\u73fe\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u3002\u5728\u963f\u62c9\u4f2f\u8a9e NLP \u4e2d\uff0c\u5e7e\u500b\u4ee5\u963f\u62c9\u4f2f\u8a9e\u70ba\u4e2d\u5fc3\u7684 LLM \u5728\u904e\u53bb\u5169\u5e74\u4e2d\uff0c\u5728\u591a\u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u5c55\u73fe\u5353\u8d8a\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u963f\u62c9\u4f2f\u8a9e LLM \u64c1\u6709\u8d85\u904e 70 \u5104\u500b\u53c3\u6578\uff0c\u8207\u8f03\u5c0f\u7684 LLM \u76f8\u6bd4\uff0c\u9019\u6703\u589e\u52a0\u5176\u786c\u9ad4\u9700\u6c42\u548c\u63a8\u8ad6\u5ef6\u9072\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u57fa\u790e\u7248\u548c\u804a\u5929\u7248\u7684\u963f\u62c9\u4f2f\u8a9e Stable LM 1.6B\uff0c\u4f5c\u70ba\u4e00\u500b\u5c0f\u5de7\u4f46\u5f37\u5927\u7684\u4ee5\u963f\u62c9\u4f2f\u8a9e\u70ba\u4e2d\u5fc3\u7684 LLM\u3002\u6211\u5011\u7684\u963f\u62c9\u4f2f\u8a9e Stable LM 1.6B \u804a\u5929\u6a21\u578b\u5728\u5e7e\u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7372\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\uff0c\u64ca\u6557\u591a\u500b\u53c3\u6578\u591a\u9054 8 \u500d\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6df7\u5408\u5408\u6210\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u7684\u597d\u8655\uff0c\u65b9\u6cd5\u662f\u7528\u5927\u578b\u5408\u6210\u5c0d\u8a71\u8cc7\u6599\u96c6\u64f4\u5145\u6211\u5011\u7684\u5fae\u8abf\u8cc7\u6599\u3002", "author": "Zaid Alyafeai et.al.", "authors": "Zaid Alyafeai, Michael Pieler, Hannah Teufel, Jonathan Tow, Marco Bellagente, Duy Phung, Nikhil Pinnaparaju, Reshinth Adithyan, Paulo Rocha, Maksym Zhuravinskyi, Carlos Riquelme", "id": "2412.04277v1", "paper_url": "http://arxiv.org/abs/2412.04277v1", "repo": "null"}}