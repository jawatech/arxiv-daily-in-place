{"2409.04082": {"publish_time": "2024-09-06", "title": "SDformerFlow: Spatiotemporal swin spikeformer for event-based optical flow estimation", "paper_summary": "Event cameras generate asynchronous and sparse event streams capturing\nchanges in light intensity. They offer significant advantages over conventional\nframe-based cameras, such as a higher dynamic range and an extremely faster\ndata rate, making them particularly useful in scenarios involving fast motion\nor challenging lighting conditions. Spiking neural networks (SNNs) share\nsimilar asynchronous and sparse characteristics and are well-suited for\nprocessing data from event cameras. Inspired by the potential of transformers\nand spike-driven transformers (spikeformers) in other computer vision tasks, we\npropose two solutions for fast and robust optical flow estimation for event\ncameras: STTFlowNet and SDformerFlow. STTFlowNet adopts a U-shaped artificial\nneural network (ANN) architecture with spatiotemporal shifted window\nself-attention (swin) transformer encoders, while SDformerFlow presents its\nfully spiking counterpart, incorporating swin spikeformer encoders.\nFurthermore, we present two variants of the spiking version with different\nneuron models. Our work is the first to make use of spikeformers for dense\noptical flow estimation. We conduct end-to-end training for all models using\nsupervised learning. Our results yield state-of-the-art performance among\nSNN-based event optical flow methods on both the DSEC and MVSEC datasets, and\nshow significant reduction in power consumption compared to the equivalent\nANNs.", "paper_summary_zh": "\u4e8b\u4ef6\u651d\u5f71\u6a5f\u6703\u7522\u751f\u975e\u540c\u6b65\u4e14\u7a00\u758f\u7684\u4e8b\u4ef6\u4e32\u6d41\uff0c\u7528\u4ee5\u6355\u6349\u5149\u7dda\u5f37\u5ea6\u8b8a\u5316\u3002\u8207\u50b3\u7d71\u7684\u57fa\u65bc\u5e40\u7684\u651d\u5f71\u6a5f\u76f8\u6bd4\uff0c\u5b83\u5011\u63d0\u4f9b\u986f\u8457\u7684\u512a\u52e2\uff0c\u4f8b\u5982\u66f4\u9ad8\u7684\u52d5\u614b\u7bc4\u570d\u548c\u6975\u5feb\u7684\u8cc7\u6599\u901f\u7387\uff0c\u9019\u4f7f\u5b83\u5011\u5728\u6d89\u53ca\u5feb\u901f\u904b\u52d5\u6216\u5177\u6709\u6311\u6230\u6027\u7684\u5149\u7dda\u689d\u4ef6\u7684\u5834\u666f\u4e2d\u7279\u5225\u6709\u7528\u3002\u8108\u885d\u795e\u7d93\u7db2\u8def (SNN) \u5171\u4eab\u985e\u4f3c\u7684\u975e\u540c\u6b65\u548c\u7a00\u758f\u7279\u6027\uff0c\u975e\u5e38\u9069\u5408\u8655\u7406\u4f86\u81ea\u4e8b\u4ef6\u651d\u5f71\u6a5f\u7684\u8cc7\u6599\u3002\u53d7\u5230Transformer\u548c\u8108\u885d\u9a45\u52d5Transformer (spikeformer) \u5728\u5176\u4ed6\u96fb\u8166\u8996\u89ba\u4efb\u52d9\u4e2d\u7684\u6f5b\u529b\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5169\u7a2e\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u4e8b\u4ef6\u651d\u5f71\u6a5f\u7684\u5feb\u901f\u4e14\u7a69\u5065\u7684\u5149\u6d41\u4f30\u8a08\uff1aSTTFlowNet \u548c SDformerFlow\u3002STTFlowNet \u63a1\u7528 U \u5f62\u4eba\u5de5\u795e\u7d93\u7db2\u8def (ANN) \u67b6\u69cb\uff0c\u5e36\u6709\u6642\u7a7a\u4f4d\u79fb\u8996\u7a97\u81ea\u6ce8\u610f\u529b (swin) Transformer\u7de8\u78bc\u5668\uff0c\u800c SDformerFlow \u5448\u73fe\u5176\u5b8c\u5168\u8108\u885d\u5c0d\u61c9\u7269\uff0c\u7d50\u5408 swin spikeformer \u7de8\u78bc\u5668\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u8108\u885d\u7248\u672c\u7684\u5169\u500b\u8b8a\u9ad4\uff0c\u5b83\u5011\u5177\u6709\u4e0d\u540c\u7684\u795e\u7d93\u5143\u6a21\u578b\u3002\u6211\u5011\u7684\u7814\u7a76\u9996\u6b21\u4f7f\u7528 spikeformer \u9032\u884c\u5bc6\u96c6\u5149\u6d41\u4f30\u8a08\u3002\u6211\u5011\u4f7f\u7528\u76e3\u7763\u5f0f\u5b78\u7fd2\u5c0d\u6240\u6709\u6a21\u578b\u9032\u884c\u7aef\u5230\u7aef\u8a13\u7df4\u3002\u6211\u5011\u7684\u7d50\u679c\u5728 DSEC \u548c MVSEC \u8cc7\u6599\u96c6\u4e0a\uff0c\u5728\u57fa\u65bc SNN \u7684\u4e8b\u4ef6\u5149\u6d41\u65b9\u6cd5\u4e2d\u7522\u751f\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u4e26\u4e14\u8207\u7b49\u6548\u7684 ANN \u76f8\u6bd4\uff0c\u986f\u8457\u964d\u4f4e\u4e86\u529f\u8017\u3002", "author": "Yi Tian et.al.", "authors": "Yi Tian, Juan Andrade-Cetto", "id": "2409.04082v1", "paper_url": "http://arxiv.org/abs/2409.04082v1", "repo": "https://github.com/yitian97/SDformerFlow"}}