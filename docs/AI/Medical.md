
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-01-13**|**RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment**|Difei Gu et.al.|[2501.07525v1](http://arxiv.org/abs/2501.07525v1)|[link](https://github.com/difeigu/radalign)|
|**2025-01-13**|**A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities**|Yihao Liu et.al.|[2501.07468v1](http://arxiv.org/abs/2501.07468v1)|null|
|**2025-01-13**|**Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for Volume-to-Volume Medical Image Translation**|Xiyue Zhu et.al.|[2501.07430v1](http://arxiv.org/abs/2501.07430v1)|null|
|**2025-01-13**|**Natural Language-Assisted Multi-modal Medication Recommendation**|Jie Tan et.al.|[2501.07166v1](http://arxiv.org/abs/2501.07166v1)|[link](https://github.com/jtan1102/nla-mmr_cikm_2024)|
|**2025-01-13**|**CureGraph: Contrastive Multi-Modal Graph Representation Learning for Urban Living Circle Health Profiling and Prediction**|Jinlin Li et.al.|[2501.07157v1](http://arxiv.org/abs/2501.07157v1)|[link](https://github.com/jinlin2021/curegraph)|
|**2025-01-13**|**UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN Powered Vision-LSTM**|Xuhui Guo et.al.|[2501.07017v1](http://arxiv.org/abs/2501.07017v1)|null|
|**2025-01-13**|**Combining LLM decision and RL action selection to improve RL policy for adaptive interventions**|Karine Karine et.al.|[2501.06980v1](http://arxiv.org/abs/2501.06980v1)|null|
|**2025-01-12**|**Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient Perspectives**|Xinyao Ma et.al.|[2501.06964v1](http://arxiv.org/abs/2501.06964v1)|null|
|**2025-01-12**|**MedGrad E-CLIP: Enhancing Trust and Transparency in AI-Driven Skin Lesion Diagnosis**|Sadia Kamal et.al.|[2501.06887v1](http://arxiv.org/abs/2501.06887v1)|null|
|**2025-01-12**|**A Foundational Generative Model for Breast Ultrasound Image Analysis**|Haojun Yu et.al.|[2501.06869v1](http://arxiv.org/abs/2501.06869v1)|null|
|**2025-01-12**|**A Comprehensive Evaluation of Large Language Models on Mental Illnesses in Arabic Context**|Noureldin Zahran et.al.|[2501.06859v1](http://arxiv.org/abs/2501.06859v1)|null|
|**2025-01-12**|**MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome Prediction**|Yiqing Zhang et.al.|[2501.06823v1](http://arxiv.org/abs/2501.06823v1)|null|
|**2025-01-12**|**PGP-SAM: Prototype-Guided Prompt Learning for Efficient Few-Shot Medical Image Segmentation**|Zhonghao Yan et.al.|[2501.06692v1](http://arxiv.org/abs/2501.06692v1)|null|
|**2025-01-12**|**Imbalanced Medical Image Segmentation with Pixel-dependent Noisy Labels**|Erjian Guo et.al.|[2501.06678v1](http://arxiv.org/abs/2501.06678v1)|[link](https://github.com/erjian96/clcs)|
|**2025-01-11**|**MedCT: A Clinical Terminology Graph for Generative AI Applications in Healthcare**|Ye Chen et.al.|[2501.06465v1](http://arxiv.org/abs/2501.06465v1)|null|
|**2025-01-11**|**Deep Learning on Hester Davis Scores for Inpatient Fall Prediction**|Hojjat Salehinejad et.al.|[2501.06432v1](http://arxiv.org/abs/2501.06432v1)|null|
|**2025-01-10**|**Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts**|Elizabeth Schaefer et.al.|[2501.06365v1](http://arxiv.org/abs/2501.06365v1)|null|
|**2025-01-10**|**Scale-up Unlearnable Examples Learning with High-Performance Computing**|Yanfan Zhu et.al.|[2501.06080v1](http://arxiv.org/abs/2501.06080v1)|null|
|**2025-01-10**|**AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery**|Johann Wenckstern et.al.|[2501.06039v1](http://arxiv.org/abs/2501.06039v1)|null|
|**2025-01-10**|**DiffuSETS: 12-lead ECG Generation Conditioned on Clinical Text Reports and Patient-Specific Information**|Yongfan Lai et.al.|[2501.05932v1](http://arxiv.org/abs/2501.05932v1)|null|
|**2025-01-10**|**AI-Driven Diabetic Retinopathy Screening: Multicentric Validation of AIDRSS in India**|Amit Kr Dey et.al.|[2501.05826v2](http://arxiv.org/abs/2501.05826v2)|null|
|**2025-01-10**|**Large Language Models for Bioinformatics**|Wei Ruan et.al.|[2501.06271v1](http://arxiv.org/abs/2501.06271v1)|null|
|**2025-01-09**|**From Simple to Complex Skills: The Case of In-Hand Object Reorientation**|Haozhi Qi et.al.|[2501.05439v1](http://arxiv.org/abs/2501.05439v1)|null|
|**2025-01-09**|**Strategy Masking: A Method for Guardrails in Value-based Reinforcement Learning Agents**|Jonathan Keane et.al.|[2501.05501v1](http://arxiv.org/abs/2501.05501v1)|null|
|**2025-01-09**|**Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charit√©, and Aignostics**|Maximilian Alber et.al.|[2501.05409v2](http://arxiv.org/abs/2501.05409v2)|null|
|**2025-01-09**|**An Algorithmic Approach for Causal Health Equity: A Look at Race Differentials in Intensive Care Unit (ICU) Outcomes**|Drago Plecko et.al.|[2501.05197v1](http://arxiv.org/abs/2501.05197v1)|null|
|**2025-01-09**|**Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo Development Assessment**|Lei Li et.al.|[2501.04958v1](http://arxiv.org/abs/2501.04958v1)|[link](https://github.com/yinghemedical/imbalance-aware_domain_adaptation)|
|**2025-01-09**|**Quantifying Itch and its Impact on Sleep Using Machine Learning and Radio Signals**|Michail Ouroutzoglou et.al.|[2501.04896v1](http://arxiv.org/abs/2501.04896v1)|null|
|**2025-01-08**|**MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data Generation**|Daniele Molino et.al.|[2501.04614v2](http://arxiv.org/abs/2501.04614v2)|null|
|**2025-01-08**|**A 65 nm Bayesian Neural Network Accelerator with 360 fJ/Sample In-Word GRNG for AI Uncertainty Estimation**|Zephan M. Enciso et.al.|[2501.04577v1](http://arxiv.org/abs/2501.04577v1)|null|
|**2025-01-08**|**Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images**|Ren Tasai et.al.|[2501.04217v1](http://arxiv.org/abs/2501.04217v1)|null|
|**2025-01-07**|**Generative Style Transfer for MRI Image Segmentation: A Case of Glioma Segmentation in Sub-Saharan Africa**|Rancy Chepchirchir et.al.|[2501.04734v1](http://arxiv.org/abs/2501.04734v1)|[link](https://github.com/CAMERA-MRI/SPARK2023/tree/main/SPARK_BTS_KIFARU)|
|**2025-01-07**|**Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study**|Ramya Jonnala et.al.|[2501.03904v1](http://arxiv.org/abs/2501.03904v1)|null|
|**2025-01-07**|**SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor Diagnosis**|Runci Bai et.al.|[2501.03836v2](http://arxiv.org/abs/2501.03836v2)|null|
|**2025-01-07**|**SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging through Selective Domain Alignment**|Siyuan Zhao et.al.|[2501.03764v1](http://arxiv.org/abs/2501.03764v1)|null|
|**2025-01-07**|**Self-adaptive vision-language model for 3D segmentation of pulmonary artery and vein**|Xiaotong Guo et.al.|[2501.03722v1](http://arxiv.org/abs/2501.03722v1)|null|
|**2025-01-07**|**Can Deep Learning Trigger Alerts from Mobile-Captured Images?**|Pritisha Sarkar et.al.|[2501.03499v1](http://arxiv.org/abs/2501.03499v1)|null|
|**2025-01-07**|**Activating Associative Disease-Aware Vision Token Memory for LLM-Based X-ray Report Generation**|Xiao Wang et.al.|[2501.03458v1](http://arxiv.org/abs/2501.03458v1)|[link](https://github.com/event-ahu/medical_image_analysis)|
|**2025-01-06**|**Existential Crisis: A Social Robot's Reason for Being**|Dora Medgyesy et.al.|[2501.03376v1](http://arxiv.org/abs/2501.03376v1)|null|
|**2025-01-06**|**Label-free Concept Based Multiple Instance Learning for Gigapixel Histopathology**|Susu Sun et.al.|[2501.02922v1](http://arxiv.org/abs/2501.02922v1)|null|
|**2025-01-06**|**Explaining Humour Style Classifications: An XAI Approach to Understanding Computational Humour Analysis**|Mary Ogbuka Kenneth et.al.|[2501.02891v1](http://arxiv.org/abs/2501.02891v1)|null|
|**2025-01-06**|**IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment**|Yiming Zhang et.al.|[2501.02869v1](http://arxiv.org/abs/2501.02869v1)|null|
|**2025-01-06**|**Multi-Modal One-Shot Federated Ensemble Learning for Medical Data with Vision Large Language Model**|Naibo Wang et.al.|[2501.03292v1](http://arxiv.org/abs/2501.03292v1)|null|
|**2025-01-06**|**GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation**|Niloufar Eghbali et.al.|[2501.02788v2](http://arxiv.org/abs/2501.02788v2)|[link](https://github.com/haail/glog-csunet)|
|**2025-01-06**|**Hybrid deep convolution model for lung cancer detection with transfer learning**|Sugandha Saxena et.al.|[2501.02785v1](http://arxiv.org/abs/2501.02785v1)|null|
|**2025-01-06**|**ICFNet: Integrated Cross-modal Fusion Network for Survival Prediction**|Binyu Zhang et.al.|[2501.02778v1](http://arxiv.org/abs/2501.02778v1)|[link](https://github.com/binging512/icfnet)|
|**2025-01-06**|**Tree-based RAG-Agent Recommendation System: A Case Study in Medical Test Data**|Yahe Yang et.al.|[2501.02727v1](http://arxiv.org/abs/2501.02727v1)|null|
|**2025-01-05**|**Representation Learning of Lab Values via Masked AutoEncoder**|David Restrepo et.al.|[2501.02648v2](http://arxiv.org/abs/2501.02648v2)|[link](https://github.com/dsrestrepo/lab-mae-foundation-tabular)|
|**2025-01-05**|**Trust and Dependability in Blockchain & AI Based MedIoT Applications: Research Challenges and Future Directions**|Ellis Solaiman et.al.|[2501.02647v1](http://arxiv.org/abs/2501.02647v1)|null|
|**2025-01-05**|**KM-UNet KAN Mamba UNet for medical image segmentation**|Yibo Zhang et.al.|[2501.02559v1](http://arxiv.org/abs/2501.02559v1)|[link](https://github.com/2760613195/km_unet)|
|**2025-01-05**|**Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine**|Yishen Liu et.al.|[2501.02471v1](http://arxiv.org/abs/2501.02471v1)|null|
|**2025-01-05**|**Enhancing Contrastive Learning for Retinal Imaging via Adjusted Augmentation Scales**|Zijie Cheng et.al.|[2501.02451v1](http://arxiv.org/abs/2501.02451v1)|null|
|**2025-01-04**|**Enhancing Workplace Productivity and Well-being Using AI Agent**|Ravirajan K et.al.|[2501.02368v1](http://arxiv.org/abs/2501.02368v1)|null|
|**2025-01-04**|**Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support**|Florian Putz et.al.|[2501.02346v1](http://arxiv.org/abs/2501.02346v1)|null|
|**2025-01-04**|**Deep Learning-Driven Segmentation of Ischemic Stroke Lesions Using Multi-Channel MRI**|Ashiqur Rahman et.al.|[2501.02287v1](http://arxiv.org/abs/2501.02287v1)|null|
|**2025-01-04**|**The Integration of Blockchain and Artificial Intelligence for Secure Healthcare Systems**|Umar Safdar et.al.|[2501.02169v1](http://arxiv.org/abs/2501.02169v1)|null|
|**2025-01-03**|**Online Detection of Water Contamination Under Concept Drift**|Jin Li et.al.|[2501.02107v1](http://arxiv.org/abs/2501.02107v1)|null|
|**2025-01-03**|**METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring**|Ollie Liu et.al.|[2501.02045v1](http://arxiv.org/abs/2501.02045v1)|null|
|**2025-01-03**|**Advancing Pancreatic Cancer Prediction with a Next Visit Token Prediction Head on top of Med-BERT**|Jianping He et.al.|[2501.02044v1](http://arxiv.org/abs/2501.02044v1)|null|
|**2025-01-03**|**Combined Hyper-Extensible Extremely-Secured Zero-Trust CIAM-PAM architecture**|Shivom Aggarwal et.al.|[2501.01732v1](http://arxiv.org/abs/2501.01732v1)|null|
|**2025-01-03**|**EAUWSeg: Eliminating annotation uncertainty in weakly-supervised medical image segmentation**|Wang Lituan et.al.|[2501.01658v1](http://arxiv.org/abs/2501.01658v1)|null|
|**2025-01-03**|**Implications of Artificial Intelligence on Health Data Privacy and Confidentiality**|Ahmad Momani et.al.|[2501.01639v2](http://arxiv.org/abs/2501.01639v2)|null|
|**2025-01-03**|**Merging Context Clustering with Visual State Space Models for Medical Image Segmentation**|Yun Zhu et.al.|[2501.01618v1](http://arxiv.org/abs/2501.01618v1)|[link](https://github.com/zymissy/ccvim)|
|**2025-01-03**|**PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents**|Jingoo Lee et.al.|[2501.01594v1](http://arxiv.org/abs/2501.01594v1)|null|
|**2025-01-02**|**Model Checking in Medical Imaging for Tumor Detection and Segmentation**|Elhoucine Elfatimi et.al.|[2501.02024v2](http://arxiv.org/abs/2501.02024v2)|null|
|**2025-01-02**|**Training Medical Large Vision-Language Models with Abnormal-Aware Feedback**|Yucheng Zhou et.al.|[2501.01377v1](http://arxiv.org/abs/2501.01377v1)|null|
|**2025-01-02**|**ScarNet: A Novel Foundation Model for Automated Myocardial Scar Quantification from LGE in Cardiac MRI**|Neda Tavakoli et.al.|[2501.01372v1](http://arxiv.org/abs/2501.01372v1)|[link](https://github.com/nedatavakoli/scarnet)|
|**2025-01-02**|**Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation**|Nathaniel Dennler et.al.|[2501.01367v1](http://arxiv.org/abs/2501.01367v1)|null|
|**2025-01-02**|**Multi-Head Explainer: A General Framework to Improve Explainability in CNNs and Transformers**|Bohang Sun et.al.|[2501.01311v2](http://arxiv.org/abs/2501.01311v2)|null|
|**2025-01-02**|**Machine Learning-Based Differential Diagnosis of Parkinson's Disease Using Kinematic Feature Extraction and Selection**|Masahiro Matsumoto et.al.|[2501.02014v1](http://arxiv.org/abs/2501.02014v1)|null|
|**2025-01-02**|**Data Augmentation Techniques for Chinese Disease Name Normalization**|Wenqian Cui et.al.|[2501.01195v1](http://arxiv.org/abs/2501.01195v1)|[link](https://github.com/dreamtheater123/disease_name_dataset)|
|**2025-01-02**|**Reasoning based on symbolic and parametric knowledge bases: a survey**|Mayi Xu et.al.|[2501.01030v1](http://arxiv.org/abs/2501.01030v1)|null|
|**2025-01-02**|**Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice**|Federico Ravenda et.al.|[2501.00982v1](http://arxiv.org/abs/2501.00982v1)|[link](https://github.com/fede-stack/adaptive-rag-for-psychological-assessment)|
|**2025-01-01**|**Enhancing Early Diabetic Retinopathy Detection through Synthetic DR1 Image Generation: A StyleGAN3 Approach**|Sagarnil Das et.al.|[2501.00954v1](http://arxiv.org/abs/2501.00954v1)|null|
|**2025-01-01**|**Multi-Center Study on Deep Learning-Assisted Detection and Classification of Fetal Central Nervous System Anomalies Using Ultrasound Imaging**|Yang Qi et.al.|[2501.02000v1](http://arxiv.org/abs/2501.02000v1)|null|
|**2024-12-31**|**Efficient Standardization of Clinical Notes using Large Language Models**|Daniel B. Hier et.al.|[2501.00644v1](http://arxiv.org/abs/2501.00644v1)|null|
|**2024-12-31**|**LLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models**|Hang Yang et.al.|[2501.05464v1](http://arxiv.org/abs/2501.05464v1)|null|
|**2024-12-31**|**Pan-infection Foundation Framework Enables Multiple Pathogen Prediction**|Lingrui Zhang et.al.|[2501.01462v1](http://arxiv.org/abs/2501.01462v1)|null|
|**2024-12-31**|**A Hybrid Deep Learning and Model-Checking Framework for Accurate Brain Tumor Detection and Validation**|Lahcen El Fatimi et.al.|[2501.01991v1](http://arxiv.org/abs/2501.01991v1)|null|
|**2024-12-31**|**GAN-TAT: A Novel Framework Using Protein Interaction Networks in Druggable Gene Identification**|George Yuanji Wang et.al.|[2501.01458v1](http://arxiv.org/abs/2501.01458v1)|[link](https://github.com/george-yuanji-wang/gan-tat)|
|**2024-12-31**|**Autonomous Alignment with Human Value on Altruism through Considerate Self-imagination and Theory of Mind**|Haibo Tong et.al.|[2501.00320v2](http://arxiv.org/abs/2501.00320v2)|[link](https://github.com/braincog-x/brain-cog)|
|**2024-12-31**|**A Fourfold Pathogen Reference Ontology Suite**|Shane Babcock et.al.|[2501.01454v1](http://arxiv.org/abs/2501.01454v1)|null|
|**2024-12-31**|**CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM Hybrid for Assisting with Optimal Cancer Treatment and Care**|Michael Gubanov et.al.|[2501.00223v1](http://arxiv.org/abs/2501.00223v1)|null|
|**2024-12-31**|**An Empirical Evaluation of Large Language Models on Consumer Health Questions**|Moaiz Abrar et.al.|[2501.00208v1](http://arxiv.org/abs/2501.00208v1)|null|
|**2024-12-31**|**GPT-4 on Clinic Depression Assessment: An LLM-Based Pilot Study**|Giuliano Lorenzoni et.al.|[2501.00199v1](http://arxiv.org/abs/2501.00199v1)|null|
|**2024-12-31**|**SepsisCalc: Integrating Clinical Calculators into Early Sepsis Prediction via Dynamic Temporal Graph Construction**|Changchang Yin et.al.|[2501.00190v2](http://arxiv.org/abs/2501.00190v2)|[link](https://github.com/yinchangchang/sepsiscalc)|
|**2024-12-30**|**DeepLL: Considering Linear Logic for the Analysis of Deep Learning Experiments**|Nick Papoulias et.al.|[2501.00169v1](http://arxiv.org/abs/2501.00169v1)|null|
|**2024-12-30**|**Temporal reasoning for timeline summarisation in social media**|Jiayu Song et.al.|[2501.00152v1](http://arxiv.org/abs/2501.00152v1)|null|
|**2024-12-30**|**A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection**|Julia Ive et.al.|[2501.00129v1](http://arxiv.org/abs/2501.00129v1)|null|
|**2024-12-30**|**Leveraging AI for Automatic Classification of PCOS Using Ultrasound Imaging**|Atharva Divekar et.al.|[2501.01984v1](http://arxiv.org/abs/2501.01984v1)|[link](https://github.com/ATHdevs/Auto-PCOS)|
|**2024-12-30**|**Advancing Parkinson's Disease Progression Prediction: Comparing Long Short-Term Memory Networks and Kolmogorov-Arnold Networks**|Abhinav Roy et.al.|[2412.20744v1](http://arxiv.org/abs/2412.20744v1)|null|
|**2024-12-30**|**Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis**|Yousef Yeganeh et.al.|[2412.20651v1](http://arxiv.org/abs/2412.20651v1)|null|
|**2024-12-29**|**HALLUCINOGEN: A Benchmark for Evaluating Object Hallucination in Large Visual-Language Models**|Ashish Seth et.al.|[2412.20622v1](http://arxiv.org/abs/2412.20622v1)|[link](https://github.com/AikyamLab/hallucinogen)|
|**2024-12-29**|**Dive into Time-Series Anomaly Detection: A Decade Review**|Paul Boniol et.al.|[2412.20512v1](http://arxiv.org/abs/2412.20512v1)|null|
|**2024-12-29**|**A Deep Subgrouping Framework for Precision Drug Repurposing via Emulating Clinical Trials on Real-world Patient Data**|Seungyeon Lee et.al.|[2412.20373v1](http://arxiv.org/abs/2412.20373v1)|null|
|**2024-12-28**|**Transforming CCTV cameras into NO$_2$ sensors at city scale for adaptive policymaking**|Mohamed R. Ibrahim et.al.|[2501.00056v1](http://arxiv.org/abs/2501.00056v1)|null|
|**2024-12-28**|**On the Compositional Generalization of Multimodal LLMs for Medical Imaging**|Zhenyang Cai et.al.|[2412.20070v1](http://arxiv.org/abs/2412.20070v1)|[link](https://github.com/freedomintelligence/med-mat)|
|**2024-12-28**|**The Emotional Spectrum of LLMs: Leveraging Empathy and Emotion-Based Markers for Mental Health Support**|Alessandro De Grandi et.al.|[2412.20068v1](http://arxiv.org/abs/2412.20068v1)|null|
|**2024-12-28**|**MobileNetV2: A lightweight classification model for home-based sleep apnea screening**|Hui Pan et.al.|[2412.19967v2](http://arxiv.org/abs/2412.19967v2)|[link](https://github.com/mindspore-lab/models/tree/master/research/arxiv_papers/Easy-MobileNetV2)|
|**2024-12-27**|**ErgoChat: a Visual Query System for the Ergonomic Risk Assessment of Construction Workers**|Chao Fan et.al.|[2412.19954v1](http://arxiv.org/abs/2412.19954v1)|null|

#### Abstracts
##### **RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment**
2501.07525v1 by Difei Gu, Yunhe Gao, Yang Zhou, Mu Zhou, Dimitris Metaxas

Automated chest radiographs interpretation requires both accurate disease
classification and detailed radiology report generation, presenting a
significant challenge in the clinical workflow. Current approaches either focus
on classification accuracy at the expense of interpretability or generate
detailed but potentially unreliable reports through image captioning
techniques. In this study, we present RadAlign, a novel framework that combines
the predictive accuracy of vision-language models (VLMs) with the reasoning
capabilities of large language models (LLMs). Inspired by the radiologist's
workflow, RadAlign first employs a specialized VLM to align visual features
with key medical concepts, achieving superior disease classification with an
average AUC of 0.885 across multiple diseases. These recognized medical
conditions, represented as text-based concepts in the aligned visual-language
space, are then used to prompt LLM-based report generation. Enhanced by a
retrieval-augmented generation mechanism that grounds outputs in similar
historical cases, RadAlign delivers superior report quality with a GREEN score
of 0.678, outperforming state-of-the-art methods' 0.634. Our framework
maintains strong clinical interpretability while reducing hallucinations,
advancing automated medical imaging and report analysis through integrated
predictive and generative AI. Code is available at
https://github.com/difeigu/RadAlign.

ÊëòË¶ÅÔºöËá™ÂãïÂåñËÉ∏ÈÉ® X ÂÖâÁâáËß£ËÆÄÈúÄË¶ÅÁ≤æÊ∫ñÁöÑÁñæÁóÖÂàÜÈ°ûÂíåË©≥Á¥∞ÁöÑÊîæÂ∞ÑÁßëÂ†±ÂëäÁîüÊàêÔºåÈÄôÂ∞çËá®Â∫äÂ∑•‰ΩúÊµÅÁ®ãÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÁõÆÂâçÁöÑÂÅöÊ≥ïË¶Å‰∏çÂ∞±ÊòØ‰ª•ÁäßÁâ≤ÂèØËß£ËÆÄÊÄßÁÇ∫‰ª£ÂÉπÂ∞àÊ≥®ÊñºÂàÜÈ°ûÊ∫ñÁ¢∫ÊÄßÔºåË¶Å‰∏çÂ∞±ÊòØÈÄèÈÅéÂΩ±ÂÉèÊ®ôÈ°åÊäÄË°ìÁî¢ÁîüË©≥Á¥∞‰ΩÜÂèØËÉΩ‰∏çÂèØÈù†ÁöÑÂ†±Âëä„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ RadAlignÔºå‰∏ÄÂÄãÁµêÂêà‰∫ÜË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Á©éÊû∂Êßã„ÄÇÂèóÂà∞ÊîæÂ∞ÑÁßëÈÜ´Â∏´Â∑•‰ΩúÊµÅÁ®ãÁöÑÂïüÁôºÔºåRadAlign È¶ñÂÖàÊé°Áî®Â∞àÈñÄÁöÑ VLM Â∞áË¶ñË¶∫ÁâπÂæµËàáÈóúÈçµÈÜ´ÁôÇÊ¶ÇÂøµÂ∞çÈΩäÔºåÂú®Â§öÁ®ÆÁñæÁóÖ‰∏≠ÈÅîÊàêÂÑ™Áï∞ÁöÑÁñæÁóÖÂàÜÈ°ûÔºåÂπ≥Âùá AUC ÁÇ∫ 0.885„ÄÇÈÄô‰∫õË≠òÂà•Âá∫ÁöÑÈÜ´ÁôÇÁãÄÊ≥ÅÊúÉÂú®Â∞çÈΩäÁöÑË¶ñË¶∫Ë™ûË®ÄÁ©∫Èñì‰∏≠Ë°®Á§∫ÁÇ∫Âü∫ÊñºÊñáÂ≠óÁöÑÊ¶ÇÂøµÔºåÁÑ∂ÂæåÁî®‰æÜÊèêÁ§∫Âü∫Êñº LLM ÁöÑÂ†±ÂëäÁîüÊàê„ÄÇÈÄèÈÅé‰∏ÄÁ®ÆÂ∞áËº∏Âá∫ÁµêÊûúÂª∫Á´ãÂú®È°û‰ººÈÅéÂæÄÊ°à‰æã‰∏≠ÁöÑÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÊ©üÂà∂ÔºåRadAlign Êèê‰æõÂÑ™Áï∞ÁöÑÂ†±ÂëäÂìÅË≥™ÔºåGREEN ÂàÜÊï∏ÁÇ∫ 0.678ÔºåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÊñπÊ≥ïÁöÑ 0.634„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÁ∂≠ÊåÅÂº∑Â§ßÁöÑËá®Â∫äÂèØËß£ËÆÄÊÄßÔºåÂêåÊôÇÊ∏õÂ∞ëÂπªË¶∫ÔºåÈÄèÈÅéÊï¥ÂêàÈ†êÊ∏¨ÂíåÁîüÊàêÂºè AIÔºåÊé®ÈÄ≤Ëá™ÂãïÂåñÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÂ†±ÂëäÂàÜÊûê„ÄÇÁ®ãÂºèÁ¢ºÂèØÊñº https://github.com/difeigu/RadAlign ÂèñÂæó„ÄÇ

##### **A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities**
2501.07468v1 by Yihao Liu, Xu Cao, Tingting Chen, Yankai Jiang, Junjie You, Minghua Wu, Xiaosong Wang, Mengling Feng, Yaochu Jin, Jintai Chen

Healthcare systems worldwide face persistent challenges in efficiency,
accessibility, and personalization. Powered by modern AI technologies such as
multimodal large language models and world models, Embodied AI (EmAI)
represents a transformative frontier, offering enhanced autonomy and the
ability to interact with the physical world to address these challenges. As an
interdisciplinary and rapidly evolving research domain, "EmAI in healthcare"
spans diverse fields such as algorithms, robotics, and biomedicine. This
complexity underscores the importance of timely reviews and analyses to track
advancements, address challenges, and foster cross-disciplinary collaboration.
In this paper, we provide a comprehensive overview of the "brain" of EmAI for
healthcare, wherein we introduce foundational AI algorithms for perception,
actuation, planning, and memory, and focus on presenting the healthcare
applications spanning clinical interventions, daily care & companionship,
infrastructure support, and biomedical research. Despite its promise, the
development of EmAI for healthcare is hindered by critical challenges such as
safety concerns, gaps between simulation platforms and real-world applications,
the absence of standardized benchmarks, and uneven progress across
interdisciplinary domains. We discuss the technical barriers and explore
ethical considerations, offering a forward-looking perspective on the future of
EmAI in healthcare. A hierarchical framework of intelligent levels for EmAI
systems is also introduced to guide further development. By providing
systematic insights, this work aims to inspire innovation and practical
applications, paving the way for a new era of intelligent, patient-centered
healthcare.

ÊëòË¶ÅÔºö<paragraph>ÂÖ®ÁêÉÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±Âú®ÊïàÁéá„ÄÅÂèØÂèäÊÄßÂíåÂÄã‰∫∫ÂåñÊñπÈù¢ÊåÅÁ∫åÈù¢Ëá®ÊåëÊà∞„ÄÇÈ´îÁèæÂºè AI (EmAI) Áî±Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂíå‰∏ñÁïåÊ®°ÂûãÁ≠âÁèæ‰ª£ AI ÊäÄË°ìÊèê‰æõÊîØÊåÅÔºå‰ª£Ë°®‰∫Ü‰∏ÄÂÄãËΩâÂûãÂâçÊ≤øÔºåÊèê‰æõÂ¢ûÂº∑ÁöÑËá™‰∏ªÊÄßÔºå‰ª•ÂèäËàáÁâ©ÁêÜ‰∏ñÁïå‰∫íÂãï‰ª•ÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÁöÑËÉΩÂäõ„ÄÇ‰ΩúÁÇ∫‰∏ÄÂÄãË∑®Â≠∏Áßë‰∏îÂø´ÈÄüÁôºÂ±ïÁöÑÁ†îÁ©∂È†òÂüüÔºå„ÄåÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ EmAI„ÄçÊ∂µËìã‰∫ÜÊºîÁÆóÊ≥ï„ÄÅÊ©üÂô®‰∫∫ÂíåÁîüÁâ©ÈÜ´Â≠∏Á≠âÂ§öÂÖÉÈ†òÂüü„ÄÇÈÄôÁ®ÆË§áÈõúÊÄßÁ™ÅÈ°Ø‰∫ÜÂèäÊôÇÂØ©Êü•ÂíåÂàÜÊûêÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ËøΩËπ§ÈÄ≤Â±ï„ÄÅÊáâÂ∞çÊåëÊà∞‰∏¶‰øÉÈÄ≤Ë∑®Â≠∏ÁßëÂêà‰Ωú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèê‰æõ‰∫Ü EmAI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ„ÄåÂ§ßËÖ¶„ÄçÁöÑÂÖ®Èù¢Ê¶ÇËø∞ÔºåÊàëÂÄëÂú®ÂÖ∂‰∏≠‰ªãÁ¥π‰∫ÜÊÑüÁü•„ÄÅÂü∑Ë°å„ÄÅË¶èÂäÉÂíåË®òÊÜ∂ÁöÑÂü∫Êú¨ AI ÊºîÁÆóÊ≥ïÔºå‰∏¶Â∞àÊ≥®ÊñºÂëàÁèæÊ∂µËìãËá®Â∫äÂπ≤È†ê„ÄÅÊó•Â∏∏ÁÖßË≠∑ÂíåÈô™‰º¥„ÄÅÂü∫Á§éË®≠ÊñΩÊîØÊè¥ÂíåÁîüÁâ©ÈÜ´Â≠∏Á†îÁ©∂ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®„ÄÇÂÑòÁÆ°ÂâçÊôØÁúãÂ•ΩÔºå‰ΩÜ EmAI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÁôºÂ±ïÂèóÂà∞ÈóúÈçµÊåëÊà∞ÁöÑÈòªÁ§ôÔºå‰æãÂ¶ÇÂÆâÂÖ®ÂïèÈ°å„ÄÅÊ®°Êì¨Âπ≥Âè∞ÂíåÂØ¶ÈöõÊáâÁî®‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÅÁº∫‰πèÊ®ôÊ∫ñÂåñÂü∫Ê∫ñÔºå‰ª•ÂèäË∑®Â≠∏ÁßëÈ†òÂüüÈÄ≤Â±ï‰∏çÂùá„ÄÇÊàëÂÄëË®éË´ñ‰∫ÜÊäÄË°ìÈöúÁ§ô‰∏¶Êé¢Ë®é‰∫ÜÈÅìÂæ∑ËÄÉÈáèÔºåÂ∞ç EmAI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊú™‰æÜÊèê‰æõ‰∫ÜÂâçÁûªÊÄßÁöÑËßÄÈªû„ÄÇÈÇÑÂºïÂÖ•‰∫Ü EmAI Á≥ªÁµ±ÁöÑÊô∫ÊÖßÂ±§Á¥öÊû∂ÊßãÔºå‰ª•ÊåáÂ∞éÈÄ≤‰∏ÄÊ≠•ÁöÑÁôºÂ±ï„ÄÇÈÄèÈÅéÊèê‰æõÁ≥ªÁµ±ÊÄßÁöÑË¶ãËß£ÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊó®Âú®ÊøÄÁôºÂâµÊñ∞ÂíåÂØ¶Áî®ÊáâÁî®ÔºåÁÇ∫Êô∫ÊÖß‰∏î‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÈÜ´ÁôÇ‰øùÂÅ•Êñ∞ÊôÇ‰ª£Èã™Ë∑Ø„ÄÇ</paragraph>

##### **Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for Volume-to-Volume Medical Image Translation**
2501.07430v1 by Xiyue Zhu, Dou Hoon Kwark, Ruike Zhu, Kaiwen Hong, Yiqi Tao, Shirui Luo, Yudu Li, Zhi-Pei Liang, Volodymyr Kindratenko

Despite success in volume-to-volume translations in medical images, most
existing models struggle to effectively capture the inherent volumetric
distribution using 3D representations. The current state-of-the-art approach
combines multiple 2D-based networks through weighted averaging, thereby
neglecting the 3D spatial structures. Directly training 3D models in medical
imaging presents significant challenges due to high computational demands and
the need for large-scale datasets. To address these challenges, we introduce
Diff-Ensembler, a novel hybrid 2D-3D model for efficient and effective
volumetric translations by ensembling perpendicularly trained 2D diffusion
models with a 3D network in each diffusion step. Moreover, our model can
naturally be used to ensemble diffusion models conditioned on different
modalities, allowing flexible and accurate fusion of input conditions.
Extensive experiments demonstrate that Diff-Ensembler attains superior accuracy
and volumetric realism in 3D medical image super-resolution and modality
translation. We further demonstrate the strength of our model's volumetric
realism using tumor segmentation as a downstream task.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠È´îÁ©çÂà∞È´îÁ©çÁöÑÁøªË≠ØÂèñÂæóÊàêÂäüÔºå‰ΩÜÁèæÊúâÁöÑÊ®°ÂûãÂ§ßÂ§öÈõ£‰ª•ÊúâÊïàÂú∞‰ΩøÁî® 3D ÂëàÁèæ‰æÜÊì∑ÂèñÂõ∫ÊúâÁöÑÈ´îÁ©çÂàÜ‰Ωà„ÄÇÁõÆÂâçÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÊòØÈÄèÈÅéÂä†Ê¨äÂπ≥Âùá‰æÜÁµêÂêàÂ§öÂÄãÂü∫Êñº 2D ÁöÑÁ∂≤Ë∑ØÔºåÂõ†Ê≠§ÂøΩÁï•‰∫Ü 3D Á©∫ÈñìÁµêÊßã„ÄÇÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠Áõ¥Êé•Ë®ìÁ∑¥ 3D Ê®°ÂûãÊúÉÁî¢ÁîüÈ°ØËëóÁöÑÊåëÊà∞ÔºåÂéüÂõ†Âú®ÊñºÈ´òÈÅãÁÆóÈúÄÊ±ÇÂíåÂ§ßË¶èÊ®°Ë≥áÊñôÈõÜÁöÑÈúÄÊ±Ç„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü Diff-EnsemblerÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ∑∑Âêà 2D-3D Ê®°ÂûãÔºåÂèØÈÄèÈÅéÂú®ÊØèÂÄãÊì¥Êï£Ê≠•È©ü‰∏≠Â∞áÂûÇÁõ¥Ë®ìÁ∑¥ÁöÑ 2D Êì¥Êï£Ê®°ÂûãËàá 3D Á∂≤Ë∑ØÁµêÂêàÔºå‰æÜÊúâÊïàÁéá‰∏îÊúâÊïàÂú∞ÈÄ≤Ë°åÈ´îÁ©çËΩâÊèõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•Ëá™ÁÑ∂Âú∞Áî®ÊñºÁµêÂêàÂü∫Êñº‰∏çÂêåÂΩ¢ÂºèÁöÑÊì¥Êï£Ê®°ÂûãÔºåÂæûËÄåÈùàÊ¥ª‰∏îÊ∫ñÁ¢∫Âú∞ËûçÂêàËº∏ÂÖ•Ê¢ù‰ª∂„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåDiff-Ensembler Âú® 3D ÈÜ´Â≠∏ÂΩ±ÂÉèË∂ÖËß£ÊûêÂ∫¶ÂíåÂΩ¢ÂºèËΩâÊèõ‰∏≠ÈÅîÂà∞‰∫ÜÊõ¥È´òÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÈ´îÁ©çÁúüÂØ¶ÊÑü„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®ËÖ´Áò§ÂàÜÂâ≤‰ΩúÁÇ∫‰∏ãÊ∏∏‰ªªÂãôÔºå‰æÜË≠âÊòéÊàëÂÄëÊ®°ÂûãÁöÑÈ´îÁ©çÁúüÂØ¶ÊÑü„ÄÇ

##### **Natural Language-Assisted Multi-modal Medication Recommendation**
2501.07166v1 by Jie Tan, Yu Rong, Kangfei Zhao, Tian Bian, Tingyang Xu, Junzhou Huang, Hong Cheng, Helen Meng

Combinatorial medication recommendation(CMR) is a fundamental task of
healthcare, which offers opportunities for clinical physicians to provide more
precise prescriptions for patients with intricate health conditions,
particularly in the scenarios of long-term medical care. Previous research
efforts have sought to extract meaningful information from electronic health
records (EHRs) to facilitate combinatorial medication recommendations. Existing
learning-based approaches further consider the chemical structures of
medications, but ignore the textual medication descriptions in which the
functionalities are clearly described. Furthermore, the textual knowledge
derived from the EHRs of patients remains largely underutilized. To address
these issues, we introduce the Natural Language-Assisted Multi-modal Medication
Recommendation(NLA-MMR), a multi-modal alignment framework designed to learn
knowledge from the patient view and medication view jointly. Specifically,
NLA-MMR formulates CMR as an alignment problem from patient and medication
modalities. In this vein, we employ pretrained language models(PLMs) to extract
in-domain knowledge regarding patients and medications, serving as the
foundational representation for both modalities. In the medication modality, we
exploit both chemical structures and textual descriptions to create medication
representations. In the patient modality, we generate the patient
representations based on textual descriptions of diagnosis, procedure, and
symptom. Extensive experiments conducted on three publicly accessible datasets
demonstrate that NLA-MMR achieves new state-of-the-art performance, with a
notable average improvement of 4.72% in Jaccard score. Our source code is
publicly available on https://github.com/jtan1102/NLA-MMR_CIKM_2024.

ÊëòË¶ÅÔºöÁµÑÂêàÂºèËó•Áâ©Êé®Ëñ¶ (CMR) ÊòØÈÜ´ÁôÇ‰øùÂÅ•ÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨‰ªªÂãôÔºåÂÆÉÁÇ∫Ëá®Â∫äÈÜ´ÁîüÊèê‰æõ‰∫ÜÈáùÂ∞çÂÖ∑ÊúâË§áÈõúÂÅ•Â∫∑ÁãÄÊ≥ÅÁöÑÊÇ£ËÄÖÊèê‰æõÊõ¥Á≤æÁ¢∫ËôïÊñπÁöÑÊ©üÊúÉÔºåÁâπÂà•ÊòØÂú®Èï∑ÊúüÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∑•‰ΩúË©¶ÂúñÂæûÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑ (EHR) ‰∏≠ÊèêÂèñÊúâÊÑèÁæ©ÁöÑË≥áË®äÔºå‰ª•‰øÉÈÄ≤ÁµÑÂêàÂºèËó•Áâ©Êé®Ëñ¶„ÄÇÁèæÊúâÁöÑÂü∫ÊñºÂ≠∏ÁøíÁöÑÊñπÊ≥ïÈÄ≤‰∏ÄÊ≠•ËÄÉÊÖÆ‰∫ÜËó•Áâ©ÁöÑÂåñÂ≠∏ÁµêÊßãÔºå‰ΩÜÂøΩÁï•‰∫ÜÂäüËÉΩÊ∏ÖÊ•öÊèèËø∞ÊñºÂÖ∂‰∏≠ÁöÑÊñáÊú¨Ëó•Áâ©Ë™™Êòé„ÄÇÊ≠§Â§ñÔºåÂæûÊÇ£ËÄÖÁöÑ EHR ‰∏≠Ë°çÁîüÁöÑÊñáÊú¨Áü•Ë≠òÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÂà©Áî®„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜËá™ÁÑ∂Ë™ûË®ÄËºîÂä©Â§öÊ®°ÂºèËó•Áâ©Êé®Ëñ¶ (NLA-MMR)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§öÊ®°ÂºèÂ∞çÈΩäÊ°ÜÊû∂ÔºåÊó®Âú®ÂæûÊÇ£ËÄÖË¶ñËßíÂíåËó•Áâ©Ë¶ñËßíÂÖ±ÂêåÂ≠∏ÁøíÁü•Ë≠ò„ÄÇÂÖ∑È´î‰æÜË™™ÔºåNLA-MMR Â∞á CMR ÊßãÂª∫ÁÇ∫ÊÇ£ËÄÖÂíåËó•Áâ©Ê®°ÂºèÁöÑÂ∞çÈΩäÂïèÈ°å„ÄÇÂú®Ê≠§ËÑàÁµ°‰∏≠ÔºåÊàëÂÄëÊé°Áî®È†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) ‰æÜÊèêÂèñÊúâÈóúÊÇ£ËÄÖÂíåËó•Áâ©ÁöÑÈ†òÂüüÂÖßÁü•Ë≠òÔºå‰ΩúÁÇ∫ÈÄôÂÖ©Á®ÆÊ®°ÂºèÁöÑÂü∫Êú¨Ë°®Á§∫„ÄÇÂú®Ëó•Áâ©Ê®°Âºè‰∏≠ÔºåÊàëÂÄëÂà©Áî®ÂåñÂ≠∏ÁµêÊßãÂíåÊñáÊú¨Ë™™Êòé‰æÜÂª∫Á´ãËó•Áâ©Ë°®Á§∫„ÄÇÂú®ÊÇ£ËÄÖÊ®°Âºè‰∏≠ÔºåÊàëÂÄëÊ†πÊìöË®∫Êñ∑„ÄÅÁ®ãÂ∫èÂíåÁóáÁãÄÁöÑÊñáÂ≠óË™™Êòé‰æÜÁîüÊàêÊÇ£ËÄÖË°®Á§∫„ÄÇÂú®‰∏âÂÄãÂÖ¨ÈñãÂ≠òÂèñÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË°®ÊòéÔºåNLA-MMR ÈÅîÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàÈÄ≤ÊïàËÉΩÔºåÂÇëÂç°Âæ∑ÊåáÊï∏Âπ≥ÂùáÊîπÈÄ≤‰∫Ü 4.72%„ÄÇÊàëÂÄëÁöÑÂéüÂßãÁ¢ºÂÖ¨ÈñãÊñº https://github.com/jtan1102/NLA-MMR_CIKM_2024„ÄÇ

##### **CureGraph: Contrastive Multi-Modal Graph Representation Learning for Urban Living Circle Health Profiling and Prediction**
2501.07157v1 by Jinlin Li, Xiao Zhou

The early detection and prediction of health status decline among the elderly
at the neighborhood level are of great significance for urban planning and
public health policymaking. While existing studies affirm the connection
between living environments and health outcomes, most rely on single data
modalities or simplistic feature concatenation of multi-modal information,
limiting their ability to comprehensively profile the health-oriented urban
environments. To fill this gap, we propose CureGraph, a contrastive multi-modal
representation learning framework for urban health prediction that employs
graph-based techniques to infer the prevalence of common chronic diseases among
the elderly within the urban living circles of each neighborhood. CureGraph
leverages rich multi-modal information, including photos and textual reviews of
residential areas and their surrounding points of interest, to generate urban
neighborhood embeddings. By integrating pre-trained visual and textual encoders
with graph modeling techniques, CureGraph captures cross-modal spatial
dependencies, offering a comprehensive understanding of urban environments
tailored to elderly health considerations. Extensive experiments on real-world
datasets demonstrate that CureGraph improves the best baseline by $28\%$ on
average in terms of $R^2$ across elderly disease risk prediction tasks.
Moreover, the model enables the identification of stage-wise chronic disease
progression and supports comparative public health analysis across
neighborhoods, offering actionable insights for sustainable urban development
and enhanced quality of life. The code is publicly available at
https://github.com/jinlin2021/CureGraph.

ÊëòË¶ÅÔºöÂú®ÈÑ∞ÈáåÂ±§Á¥öÊó©ÊúüÂÅµÊ∏¨ÂíåÈ†êÊ∏¨ËÄÅÂπ¥‰∫∫ÁöÑÂÅ•Â∫∑ÁãÄÊ≥Å‰∏ãÈôçÂ∞çÂüéÂ∏ÇË¶èÂäÉÂíåÂÖ¨ÂÖ±Ë°õÁîüÊîøÁ≠ñÂà∂ÂÆöÂÖ∑ÊúâÈáçÂ§ßÊÑèÁæ©„ÄÇÂÑòÁÆ°ÁèæÊúâÁ†îÁ©∂ËÇØÂÆö‰∫ÜÁîüÊ¥ªÁí∞Â¢ÉËàáÂÅ•Â∫∑ÁµêÊûú‰πãÈñìÁöÑÈóúËÅØÊÄßÔºå‰ΩÜÂ§ßÂ§ö‰æùË≥¥ÂñÆ‰∏ÄË≥áÊñôÊ®°ÂºèÊàñÂ§öÊ®°ÂºèË≥áË®äÁöÑÁ∞°ÂåñÁâπÂæµ‰∏≤Êé•ÔºåÈôêÂà∂‰∫Ü‰ªñÂÄëÂÖ®Èù¢ÊèèÁπ™‰ª•ÂÅ•Â∫∑ÁÇ∫Â∞éÂêëÁöÑÂüéÂ∏ÇÁí∞Â¢ÉÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü CureGraphÔºå‰∏ÄÂÄãÁî®ÊñºÂüéÂ∏ÇÂÅ•Â∫∑È†êÊ∏¨ÁöÑÂ∞çÊØîÂºèÂ§öÊ®°ÂºèË°®Á§∫Â≠∏ÁøíÊû∂ÊßãÔºåÂÆÉÊé°Áî®Âü∫ÊñºÂúñÂΩ¢ÊäÄË°ì‰æÜÊé®Ë´ñÊØèÂÄãÈÑ∞ÈáåÂüéÂ∏ÇÁîüÊ¥ªÂúà‰∏≠ËÄÅÂπ¥‰∫∫Â∏∏Ë¶ãÊÖ¢ÊÄßÁñæÁóÖÁöÑÊµÅË°åÁéá„ÄÇCureGraph Âà©Áî®Ë±êÂØåÁöÑÂ§öÊ®°ÂºèË≥áË®äÔºåÂåÖÊã¨‰ΩèÂÆÖÂçÄÂèäÂÖ∂Âë®ÂúçÊôØÈªûÁöÑÁÖßÁâáÂíåÊñáÂ≠óË©ïË´ñÔºå‰æÜÁî¢ÁîüÂüéÂ∏ÇÈÑ∞ÈáåÂµåÂÖ•„ÄÇÈÄèÈÅéÊï¥ÂêàÈ†êÂÖàË®ìÁ∑¥ÁöÑË¶ñË¶∫ÂíåÊñáÂ≠óÁ∑®Á¢ºÂô®ËàáÂúñÂΩ¢Âª∫Ê®°ÊäÄË°ìÔºåCureGraph ÊçïÊçâË∑®Ê®°ÂºèÁ©∫Èñì‰æùË≥¥ÊÄßÔºåÊèê‰æõÂ∞çÂüéÂ∏ÇÁí∞Â¢ÉÁöÑÂÖ®Èù¢ÁêÜËß£ÔºåÂ∞àÈñÄÈáùÂ∞çËÄÅÂπ¥‰∫∫ÁöÑÂÅ•Â∫∑ËÄÉÈáè„ÄÇÂú®ÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåCureGraph Âú®ËÄÅÂπ¥‰∫∫ÁñæÁóÖÈ¢®Èö™È†êÊ∏¨‰ªªÂãô‰∏≠ÔºåÂπ≥ÂùáÂú® R2 ÊñπÈù¢Â∞áÊúÄ‰Ω≥Âü∫Ê∫ñÁ∑öÊèêÈ´ò‰∫Ü 28%„ÄÇÊ≠§Â§ñÔºåË©≤Ê®°ÂûãËÉΩÂ§†Ë≠òÂà•ÈöéÊÆµÊÄßÁöÑÊÖ¢ÊÄßÁñæÁóÖÈÄ≤Á®ãÔºå‰∏¶ÊîØÊè¥Ë∑®ÈÑ∞ÈáåÁöÑÊØîËºÉÂÖ¨ÂÖ±Ë°õÁîüÂàÜÊûêÔºåÁÇ∫Ê∞∏Á∫åÁöÑÂüéÂ∏ÇÁôºÂ±ïÂíåÊèêÂçáÁîüÊ¥ªÂìÅË≥™Êèê‰æõÂèØË°åÁöÑË¶ãËß£„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº https://github.com/jinlin2021/CureGraph„ÄÇ

##### **UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN Powered Vision-LSTM**
2501.07017v1 by Xuhui Guo, Tanmoy Dam, Rohan Dhamdhere, Gourav Modanwal, Anant Madabhushi

3D medical image segmentation has progressed considerably due to
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these
methods struggle to balance long-range dependency acquisition with
computational efficiency. To address this challenge, we propose UNETVL (U-Net
Vision-LSTM), a novel architecture that leverages recent advancements in
temporal information processing. UNETVL incorporates Vision-LSTM (ViL) for
improved scalability and memory functions, alongside an efficient Chebyshev
Kolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency
patterns more effectively. We validated our method on the ACDC and AMOS2022
(post challenge Task 2) benchmark datasets, showing a significant improvement
in mean Dice score compared to recent state-of-the-art approaches, especially
over its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS,
respectively. Extensive ablation studies were conducted to demonstrate the
impact of each component in UNETVL, providing a comprehensive understanding of
its architecture. Our code is available at https://github.com/tgrex6/UNETVL,
facilitating further research and applications in this domain.

ÊëòË¶ÅÔºö3D ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Â∑≤Âõ†Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåË¶ñË¶∫ËΩâÊèõÂô® (ViT) ËÄåÂ§ßÂπÖÈÄ≤Â±ïÔºå‰ΩÜÈÄô‰∫õÊñπÊ≥ïÈõ£‰ª•Âú®ÈÅ†Á®ã‰æùË≥¥ÂèñÂæóËàáÈÅãÁÆóÊïàÁéá‰πãÈñìÂèñÂæóÂπ≥Ë°°„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫ UNETVL (U-Net Vision-LSTM)Ôºå‰∏ÄÁ®ÆÂà©Áî®ÊôÇÈñìË≥áË®äËôïÁêÜÊúÄÊñ∞ÈÄ≤Â±ïÁöÑÊñ∞Á©éÊû∂Êßã„ÄÇUNETVL ÁµêÂêàË¶ñË¶∫ LSTM (ViL) ‰ª•ÊîπÂñÑÂèØÊì¥ÂÖÖÊÄßÂíåË®òÊÜ∂È´îÂäüËÉΩÔºå‰∏¶ÁµêÂêàÈ´òÊïàÁöÑ Chebyshev Kolmogorov-Arnold Á∂≤Ë∑Ø (KAN) ‰ª•Êõ¥ÊúâÊïàÂú∞ËôïÁêÜË§áÈõú‰∏îÈÅ†Á®ãÁöÑ‰æùË≥¥Ê®°Âºè„ÄÇÊàëÂÄëÂú® ACDC Âíå AMOS2022ÔºàÊåëÊà∞‰ªªÂãô 2ÔºâÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÔºåËàáÊúÄËøëÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÈ°ØÁ§∫Âπ≥Âùá Dice ÂàÜÊï∏ÊúâÈ°ØËëóÊîπÂñÑÔºåÁâπÂà•ÊòØËàáÂÖ∂ÂâçË∫´ UNETR Áõ∏ÊØîÔºåÂú® ACDC ‰∏äÂ¢ûÂä†‰∫Ü 7.3%ÔºåÂú® AMOS ‰∏äÂ¢ûÂä†‰∫Ü 15.6%„ÄÇÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÊ∂àËûçÁ†îÁ©∂Ôºå‰ª•Â±ïÁ§∫ UNETVL ‰∏≠ÊØèÂÄãÁµÑÊàêÁöÑÂΩ±ÈüøÔºåÊèê‰æõÂ∞çÂÖ∂Êû∂ÊßãÁöÑÂÖ®Èù¢ÁêÜËß£„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/tgrex6/UNETVL ÂèñÂæóÔºå‰øÉÈÄ≤Ê≠§È†òÂüüÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÂíåÊáâÁî®„ÄÇ

##### **Combining LLM decision and RL action selection to improve RL policy for adaptive interventions**
2501.06980v1 by Karine Karine, Benjamin M. Marlin

Reinforcement learning (RL) is increasingly being used in the healthcare
domain, particularly for the development of personalized health adaptive
interventions. Inspired by the success of Large Language Models (LLMs), we are
interested in using LLMs to update the RL policy in real time, with the goal of
accelerating personalization. We use the text-based user preference to
influence the action selection on the fly, in order to immediately incorporate
the user preference. We use the term "user preference" as a broad term to refer
to a user personal preference, constraint, health status, or a statement
expressing like or dislike, etc. Our novel approach is a hybrid method that
combines the LLM response and the RL action selection to improve the RL policy.
Given an LLM prompt that incorporates the user preference, the LLM acts as a
filter in the typical RL action selection. We investigate different prompting
strategies and action selection strategies. To evaluate our approach, we
implement a simulation environment that generates the text-based user
preferences and models the constraints that impact behavioral dynamics. We show
that our approach is able to take into account the text-based user preferences,
while improving the RL policy, thus improving personalization in adaptive
intervention.

ÊëòË¶ÅÔºöÂº∑ÂåñÂ≠∏ÁøíÔºàRLÔºâÂú®ÈÜ´ÁôÇÈ†òÂüüÁöÑÊáâÁî®Êó•ÁõäÂª£Ê≥õÔºåÁâπÂà•ÊòØÁî®ÊñºÈñãÁôºÂÄã‰∫∫ÂåñÂÅ•Â∫∑ÈÅ©ÊáâÊÄßÂπ≤È†êÊé™ÊñΩ„ÄÇÂèóÂà∞Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÊàêÂäüÁöÑÂïüÁôºÔºåÊàëÂÄëÊúâËààË∂£‰ΩøÁî® LLM Âç≥ÊôÇÊõ¥Êñ∞ RL ÊîøÁ≠ñÔºåÁõÆÊ®ôÊòØÂä†ÈÄüÂÄã‰∫∫Âåñ„ÄÇÊàëÂÄë‰ΩøÁî®Âü∫ÊñºÊñáÂ≠óÁöÑ‰ΩøÁî®ËÄÖÂÅèÂ•Ω‰æÜÂΩ±ÈüøË°åÂãïÈÅ∏ÊìáÔºå‰ª•‰æøÁ´ãÂç≥Á¥çÂÖ•‰ΩøÁî®ËÄÖÂÅèÂ•Ω„ÄÇÊàëÂÄë‰ΩøÁî®„Äå‰ΩøÁî®ËÄÖÂÅèÂ•Ω„Äç‰∏ÄË©û‰ΩúÁÇ∫Âª£Áæ©Ë©ûÔºåÁî®‰æÜÊåá‰ΩøÁî®ËÄÖÁöÑÂÄã‰∫∫ÂÅèÂ•Ω„ÄÅÈôêÂà∂„ÄÅÂÅ•Â∫∑ÁãÄÊ≥ÅÊàñË°®ÈÅîÂ•ΩÊÉ°ÁöÑÈô≥Ëø∞Á≠â„ÄÇÊàëÂÄëÁöÑÊñ∞Á©éÊñπÊ≥ïÊòØ‰∏ÄÁ®ÆÊ∑∑ÂêàÊñπÊ≥ïÔºåÁµêÂêà‰∫Ü LLM ÂõûÊáâÂíå RL Ë°åÂãïÈÅ∏Êìá‰ª•ÊîπÂñÑ RL ÊîøÁ≠ñ„ÄÇÁµ¶ÂÆöÂåÖÂê´‰ΩøÁî®ËÄÖÂÅèÂ•ΩÁöÑ LLM ÊèêÁ§∫ÔºåLLM Âú®ÂÖ∏ÂûãÁöÑ RL Ë°åÂãïÈÅ∏Êìá‰∏≠ÂÖÖÁï∂ÈÅéÊøæÂô®„ÄÇÊàëÂÄëÁ†îÁ©∂‰∫Ü‰∏çÂêåÁöÑÊèêÁ§∫Á≠ñÁï•ÂíåË°åÂãïÈÅ∏ÊìáÁ≠ñÁï•„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÊàëÂÄëÂØ¶‰Ωú‰∫Ü‰∏ÄÂÄãÊ®°Êì¨Áí∞Â¢ÉÔºåÁî®ÊñºÁî¢ÁîüÂü∫ÊñºÊñáÂ≠óÁöÑ‰ΩøÁî®ËÄÖÂÅèÂ•ΩÔºå‰∏¶Â∞çÂΩ±ÈüøË°åÁÇ∫ÂãïÊÖãÁöÑÈôêÂà∂ÈÄ≤Ë°åÂª∫Ê®°„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïËÉΩÂ§†ËÄÉÈáèÂü∫ÊñºÊñáÂ≠óÁöÑ‰ΩøÁî®ËÄÖÂÅèÂ•ΩÔºåÂêåÊôÇÊîπÂñÑ RL ÊîøÁ≠ñÔºåÂæûËÄåÊîπÂñÑÈÅ©ÊáâÊÄßÂπ≤È†ê‰∏≠ÁöÑÂÄã‰∫∫Âåñ„ÄÇ

##### **Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient Perspectives**
2501.06964v1 by Xinyao Ma, Rui Zhu, Zihao Wang, Jingwei Xiong, Qingyu Chen, Haixu Tang, L. Jean Camp, Lucila Ohno-Machado

Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing scenarios, particularly in simulating domain-specific experts
using tailored prompts. This ability enables LLMs to adopt the persona of
individuals with specific backgrounds, offering a cost-effective and efficient
alternative to traditional, resource-intensive user studies. By mimicking human
behavior, LLMs can anticipate responses based on concrete demographic or
professional profiles. In this paper, we evaluate the effectiveness of LLMs in
simulating individuals with diverse backgrounds and analyze the consistency of
these simulated behaviors compared to real-world outcomes. In particular, we
explore the potential of LLMs to interpret and respond to discharge summaries
provided to patients leaving the Intensive Care Unit (ICU). We evaluate and
compare with human responses the comprehensibility of discharge summaries among
individuals with varying educational backgrounds, using this analysis to assess
the strengths and limitations of LLM-driven simulations. Notably, when LLMs are
primed with educational background information, they deliver accurate and
actionable medical guidance 88% of the time. However, when other information is
provided, performance significantly drops, falling below random chance levels.
This preliminary study shows the potential benefits and pitfalls of
automatically generating patient-specific health information from diverse
populations. While LLMs show promise in simulating health personas, our results
highlight critical gaps that must be addressed before they can be reliably used
in clinical settings. Our findings suggest that a straightforward
query-response model could outperform a more tailored approach in delivering
health information. This is a crucial first step in understanding how LLMs can
be optimized for personalized health communication while maintaining accuracy.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ËßíËâ≤ÊâÆÊºîÂ†¥ÊôØ‰∏≠Â±ïÁèæ‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåÁâπÂà•ÊòØÂú®Ê®°Êì¨ÁâπÂÆöÈ†òÂüüÁöÑÂ∞àÂÆ∂ÊôÇÔºåÊúÉ‰ΩøÁî®ÈáèË∫´ÊâìÈÄ†ÁöÑÊèêÁ§∫„ÄÇÈÄôÁ®ÆËÉΩÂäõ‰Ωø LLM ËÉΩÂ§†Êé°Áî®ÂÖ∑ÊúâÁâπÂÆöËÉåÊôØÁöÑÂÄã‰∫∫ËßíËâ≤ÔºåÊèê‰æõ‰∏ÄÁ®ÆÁ∂ìÊøüÂØ¶ÊÉ†‰∏îÊúâÊïàÁéáÁöÑÊõø‰ª£ÊñπÊ°àÔºåÁî®ÊñºÂÇ≥Áµ±‰∏îË≥áÊ∫êÂØÜÈõÜÁöÑ‰ΩøÁî®ËÄÖÁ†îÁ©∂„ÄÇÈÄèÈÅéÊ®°Êì¨‰∫∫È°ûË°åÁÇ∫ÔºåLLM ËÉΩÂ§†Ê†πÊìöÂÖ∑È´îÁöÑ‰∫∫Âè£Áµ±Ë®àÊàñÂ∞àÊ•≠ÁâπÂæµÈ†êÊ∏¨ÂèçÊáâ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË©ï‰º∞‰∫Ü LLM Âú®Ê®°Êì¨ÂÖ∑Êúâ‰∏çÂêåËÉåÊôØÁöÑÂÄã‰∫∫ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºå‰∏¶ÂàÜÊûê‰∫ÜÈÄô‰∫õÊ®°Êì¨Ë°åÁÇ∫ËàáÂØ¶ÈöõÁµêÊûúÁõ∏ÊØîÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Ëß£ÈáãÂíåÂõûÊáâÊèê‰æõÁµ¶Èõ¢ÈñãÂä†Ë≠∑ÁóÖÊàø (ICU) ÊÇ£ËÄÖÁöÑÂá∫Èô¢ÊëòË¶ÅÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëË©ï‰º∞‰∏¶Ëàá‰∫∫È°ûÁöÑÂèçÊáâÊØîËºÉ‰∫Ü‰∏çÂêåÊïôËÇ≤ËÉåÊôØÁöÑÂÄã‰∫∫Â∞çÂá∫Èô¢ÊëòË¶ÅÁöÑÂèØÁêÜËß£ÊÄßÔºå‰∏¶‰ΩøÁî®Ê≠§ÂàÜÊûê‰æÜË©ï‰º∞ LLM È©ÖÂãïÊ®°Êì¨ÁöÑÂÑ™ÈªûÂíåÈôêÂà∂„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÁï∂ LLM Ë¢´Ê§çÂÖ•ÊïôËÇ≤ËÉåÊôØË≥áË®äÊôÇÔºå‰ªñÂÄëÂú® 88% ÁöÑÊôÇÈñìÂÖßÈÉΩËÉΩÊèê‰æõÊ∫ñÁ¢∫‰∏îÂèØË°åÁöÑÈÜ´ÁôÇÊåáÂ∞é„ÄÇ‰ΩÜÊòØÔºåÁï∂Êèê‰æõÂÖ∂‰ªñË≥áË®äÊôÇÔºåÊïàËÉΩÊúÉÈ°ØËëó‰∏ãÈôçÔºå‰ΩéÊñºÈö®Ê©üÊ©üÊúÉÁöÑÁ≠âÁ¥ö„ÄÇÈÄôÈ†ÖÂàùÊ≠•Á†îÁ©∂È°ØÁ§∫‰∫ÜËá™ÂãïÁî¢Áîü‰æÜËá™‰∏çÂêåÁæ§È´îÁöÑÁâπÂÆöÊñºÊÇ£ËÄÖÁöÑÂÅ•Â∫∑Ë≥áË®äÁöÑÊΩõÂú®Â•ΩËôïÂíåÁº∫Èªû„ÄÇÂÑòÁÆ° LLM Âú®Ê®°Êì¨ÂÅ•Â∫∑ËßíËâ≤ÊñπÈù¢È°ØÁ§∫Âá∫ÂâçÊôØÔºå‰ΩÜÊàëÂÄëÁöÑÁµêÊûúÁ™ÅÂá∫‰∫ÜÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÂèØÈù†‰ΩøÁî®‰πãÂâçÂøÖÈ†àËß£Ê±∫ÁöÑÈóúÈçµÂ∑ÆË∑ù„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂú®Êèê‰æõÂÅ•Â∫∑Ë≥áË®äÊñπÈù¢Ôºå‰∏ÄÂÄãÁõ¥Êé•ÁöÑÊü•Ë©¢ÂõûÊáâÊ®°ÂûãÂèØ‰ª•ÂÑ™Êñº‰∏ÄÂÄãÊõ¥ÈáèË∫´ÊâìÈÄ†ÁöÑÊñπÊ≥ï„ÄÇÈÄôÊòØ‰∫ÜËß£Â¶Ç‰ΩïÈáùÂ∞çÂÄã‰∫∫ÂåñÂÅ•Â∫∑Ê∫ùÈÄöÂÑ™Âåñ LLM ÂêåÊôÇÁ∂≠ÊåÅÊ∫ñÁ¢∫ÊÄßÁöÑÁ¨¨‰∏ÄÊ≠•„ÄÇ

##### **MedGrad E-CLIP: Enhancing Trust and Transparency in AI-Driven Skin Lesion Diagnosis**
2501.06887v1 by Sadia Kamal, Tim Oates

As deep learning models gain attraction in medical data, ensuring transparent
and trustworthy decision-making is essential. In skin cancer diagnosis, while
advancements in lesion detection and classification have improved accuracy, the
black-box nature of these methods poses challenges in understanding their
decision processes, leading to trust issues among physicians. This study
leverages the CLIP (Contrastive Language-Image Pretraining) model, trained on
different skin lesion datasets, to capture meaningful relationships between
visual features and diagnostic criteria terms. To further enhance transparency,
we propose a method called MedGrad E-CLIP, which builds on gradient-based
E-CLIP by incorporating a weighted entropy mechanism designed for complex
medical imaging like skin lesions. This approach highlights critical image
regions linked to specific diagnostic descriptions. The developed integrated
pipeline not only classifies skin lesions by matching corresponding
descriptions but also adds an essential layer of explainability developed
especially for medical data. By visually explaining how different features in
an image relates to diagnostic criteria, this approach demonstrates the
potential of advanced vision-language models in medical image analysis,
ultimately improving transparency, robustness, and trust in AI-driven
diagnostic systems.

ÊëòË¶ÅÔºöÈöèÁùÄÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÂú®ÂåªÂ≠¶Êï∞ÊçÆ‰∏≠Ëé∑ÂæóÂÖ≥Ê≥®ÔºåÁ°Æ‰øùÈÄèÊòé‰∏îÂÄºÂæó‰ø°ËµñÁöÑÂÜ≥Á≠ñËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®ÁöÆËÇ§ÁôåËØäÊñ≠‰∏≠ÔºåËôΩÁÑ∂ÁóÖÁÅ∂Ê£ÄÊµãÂíåÂàÜÁ±ªÁöÑËøõÊ≠•ÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÔºå‰ΩÜËøô‰∫õÊñπÊ≥ïÁöÑÈªëÁõíÊÄßË¥®ÂØπÁêÜËß£ÂÖ∂ÂÜ≥Á≠ñËøáÁ®ãÊûÑÊàê‰∫ÜÊåëÊàòÔºåÂØºËá¥ÂåªÁîü‰πãÈó¥ÁöÑ‰ø°‰ªªÈóÆÈ¢ò„ÄÇÊú¨Á†îÁ©∂Âà©Áî®Âú®‰∏çÂêåÁöÆËÇ§ÁóÖÂèòÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁöÑ CLIPÔºàÂØπÊØîËØ≠Ë®ÄÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÔºâÊ®°ÂûãÔºå‰ª•ÊçïÊçâËßÜËßâÁâπÂæÅÂíåËØäÊñ≠Ê†áÂáÜÊúØËØ≠‰πãÈó¥ÁöÑÊúâÊÑè‰πâÂÖ≥Á≥ª„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÊèêÈ´òÈÄèÊòéÂ∫¶ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ MedGrad E-CLIP ÁöÑÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÈÄöËøáÁªìÂêà‰∏ì‰∏∫ÁöÆËÇ§ÁóÖÂèòÁ≠âÂ§çÊùÇÂåªÂ≠¶ÂΩ±ÂÉèËÆæËÆ°ÁöÑÂä†ÊùÉÁÜµÊú∫Âà∂ÔºåÂª∫Á´ãÂú®Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑ E-CLIP ‰πã‰∏ä„ÄÇÊ≠§ÊñπÊ≥ïÁ™ÅÂá∫‰∫Ü‰∏éÁâπÂÆöËØäÊñ≠ÊèèËø∞Áõ∏ÂÖ≥ËÅîÁöÑÂÖ≥ÈîÆÂõæÂÉèÂå∫Âüü„ÄÇÂºÄÂèëÁöÑÈõÜÊàêÁÆ°ÈÅì‰∏ç‰ªÖÈÄöËøáÂåπÈÖçÁõ∏Â∫îÁöÑÊèèËø∞ÂØπÁöÆËÇ§ÁóÖÂèòËøõË°åÂàÜÁ±ªÔºåËøòÊ∑ªÂä†‰∫Ü‰∏ÄÂ±Ç‰∏ìÈó®‰∏∫ÂåªÂ≠¶Êï∞ÊçÆÂºÄÂèëÁöÑÂü∫Êú¨ÂèØËß£ÈáäÊÄß„ÄÇÈÄöËøáÁõ¥ËßÇÂú∞Ëß£ÈáäÂõæÂÉè‰∏≠‰∏çÂêåÁâπÂæÅ‰∏éËØäÊñ≠Ê†áÂáÜÁöÑÂÖ≥Á≥ªÔºåËøôÁßçÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÈ´òÁ∫ßËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÂ≠¶ÂõæÂÉèÂàÜÊûê‰∏≠ÁöÑÊΩúÂäõÔºåÊúÄÁªàÊèêÈ´ò‰∫ÜÈÄèÊòéÂ∫¶„ÄÅÁ®≥ÂÅ•ÊÄßÂíåÂØπ‰∫∫Â∑•Êô∫ËÉΩÈ©±Âä®ÁöÑËØäÊñ≠Á≥ªÁªüÁöÑ‰ø°‰ªª„ÄÇ

##### **A Foundational Generative Model for Breast Ultrasound Image Analysis**
2501.06869v1 by Haojun Yu, Youcheng Li, Nan Zhang, Zihan Niu, Xuantong Gong, Yanwen Luo, Haotian Ye, Siyu He, Quanlin Wu, Wangyan Qin, Mengyuan Zhou, Jie Han, Jia Tao, Ziwei Zhao, Di Dai, Di He, Dong Wang, Binghui Tang, Ling Huo, James Zou, Qingli Zhu, Yong Wang, Liwei Wang

Foundational models have emerged as powerful tools for addressing various
tasks in clinical settings. However, their potential development to breast
ultrasound analysis remains untapped. In this paper, we present BUSGen, the
first foundational generative model specifically designed for breast ultrasound
image analysis. Pretrained on over 3.5 million breast ultrasound images, BUSGen
has acquired extensive knowledge of breast structures, pathological features,
and clinical variations. With few-shot adaptation, BUSGen can generate
repositories of realistic and informative task-specific data, facilitating the
development of models for a wide range of downstream tasks. Extensive
experiments highlight BUSGen's exceptional adaptability, significantly
exceeding real-data-trained foundational models in breast cancer screening,
diagnosis, and prognosis. In breast cancer early diagnosis, our approach
outperformed all board-certified radiologists (n=9), achieving an average
sensitivity improvement of 16.5% (P-value<0.0001). Additionally, we
characterized the scaling effect of using generated data which was as effective
as the collected real-world data for training diagnostic models. Moreover,
extensive experiments demonstrated that our approach improved the
generalization ability of downstream models. Importantly, BUSGen protected
patient privacy by enabling fully de-identified data sharing, making progress
forward in secure medical data utilization. An online demo of BUSGen is
available at https://aibus.bio.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂ∑≤ÊàêÁÇ∫Ëß£Ê±∫Ëá®Â∫äÁí∞Â¢É‰∏≠ÂêÑÁ®Æ‰ªªÂãôÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®‰π≥ÊàøË∂ÖÈü≥Ê≥¢ÂàÜÊûêÁöÑÊΩõÂú®ÁôºÂ±ï‰ªçÊú™ÈñãÁôº„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ BUSGenÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞àÈñÄË®≠Ë®àÁî®Êñº‰π≥ÊàøË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÂàÜÊûêÁöÑÂü∫Á§éÁîüÊàêÊ®°Âûã„ÄÇBUSGen Âú®Ë∂ÖÈÅé 350 Ëê¨Âºµ‰π≥ÊàøË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉè‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥ÔºåÂ∑≤Áç≤Âæó‰π≥ÊàøÁµêÊßã„ÄÅÁóÖÁêÜÁâπÂæµÂíåËá®Â∫äËÆäÁï∞ÁöÑÂª£Ê≥õÁü•Ë≠ò„ÄÇÈÄèÈÅéÂ∞ëÈáèÈÅ©ÊáâÔºåBUSGen ÂèØ‰ª•Áî¢ÁîüÈÄºÁúü‰∏îÂÖ∑ÊúâË≥áË®äÊÄßÁöÑÁâπÂÆö‰ªªÂãôË≥áÊñôÂÑ≤Â≠òÂ∫´Ôºå‰øÉÈÄ≤ÈñãÁôºÂª£Ê≥õÁöÑ‰∏ãÊ∏∏‰ªªÂãôÊ®°Âûã„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÁ™ÅÈ°Ø‰∫Ü BUSGen ÁöÑÂá∫Ëâ≤ÈÅ©ÊáâÊÄßÔºåÂú®‰π≥ÁôåÁØ©Ê™¢„ÄÅË®∫Êñ∑ÂíåÈ†êÂæåÊñπÈù¢È°ØËëóË∂ÖË∂ä‰ª•ÁúüÂØ¶Ë≥áÊñôË®ìÁ∑¥ÁöÑÂü∫Á§éÊ®°Âûã„ÄÇÂú®‰π≥ÁôåÊó©ÊúüË®∫Êñ∑‰∏≠ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂÑ™ÊñºÊâÄÊúâÈÄöÈÅéË™çË≠âÁöÑÊîæÂ∞ÑÁßëÈÜ´Â∏´ (n=9)ÔºåÂπ≥ÂùáÊïèÊÑüÂ∫¶ÊèêÈ´ò‰∫Ü 16.5%ÔºàP ÂÄº <0.0001Ôºâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèèËø∞‰∫Ü‰ΩøÁî®ÁîüÊàêË≥áÊñôÁöÑË¶èÊ®°ÊïàÊáâÔºåÂÖ∂ËàáÊî∂ÈõÜÁöÑÁúüÂØ¶‰∏ñÁïåË≥áÊñô‰∏ÄÊ®£ÊúâÊïàÔºåÂèØÁî®ÊñºË®ìÁ∑¥Ë®∫Êñ∑Ê®°Âûã„ÄÇÊ≠§Â§ñÔºåÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊîπÂñÑ‰∫Ü‰∏ãÊ∏∏Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈáçË¶ÅÁöÑÊòØÔºåBUSGen ‰øùË≠∑‰∫ÜÊÇ£ËÄÖÈö±ÁßÅÔºåÂõ†ÁÇ∫ÂÆÉËÉΩÂ§†ÂÆåÂÖ®ÂéªË≠òÂà•Ë≥áÊñôÂÖ±‰∫´ÔºåÂú®ÂÆâÂÖ®ÈÜ´ÁôÇË≥áÊñôÂà©Áî®ÊñπÈù¢ÂèñÂæóÈÄ≤Â±ï„ÄÇBUSGen ÁöÑÁ∑ö‰∏äÁ§∫ÁØÑÂèØÂú® https://aibus.bio ÂèñÂæó„ÄÇ

##### **A Comprehensive Evaluation of Large Language Models on Mental Illnesses in Arabic Context**
2501.06859v1 by Noureldin Zahran, Aya E. Fouda, Radwa J. Hanafy, Mohammed E. Fouda

Mental health disorders pose a growing public health concern in the Arab
world, emphasizing the need for accessible diagnostic and intervention tools.
Large language models (LLMs) offer a promising approach, but their application
in Arabic contexts faces challenges including limited labeled datasets,
linguistic complexity, and translation biases. This study comprehensively
evaluates 8 LLMs, including general multi-lingual models, as well as bi-lingual
ones, on diverse mental health datasets (such as AraDepSu, Dreaddit, MedMCQA),
investigating the impact of prompt design, language configuration (native
Arabic vs. translated English, and vice versa), and few-shot prompting on
diagnostic performance. We find that prompt engineering significantly
influences LLM scores mainly due to reduced instruction following, with our
structured prompt outperforming a less structured variant on multi-class
datasets, with an average difference of 14.5\%. While language influence on
performance was modest, model selection proved crucial: Phi-3.5 MoE excelled in
balanced accuracy, particularly for binary classification, while Mistral NeMo
showed superior performance in mean absolute error for severity prediction
tasks. Few-shot prompting consistently improved performance, with particularly
substantial gains observed for GPT-4o Mini on multi-class classification,
boosting accuracy by an average factor of 1.58. These findings underscore the
importance of prompt optimization, multilingual analysis, and few-shot learning
for developing culturally sensitive and effective LLM-based mental health tools
for Arabic-speaking populations.

ÊëòË¶ÅÔºö<paragraph>ÂøÉÁêÜÂÅ•Â∫∑ÈöúÁ§ôÂú®ÈòøÊãâ‰ºØ‰∏ñÁïå‰∏≠ÊßãÊàêÊó•ÁõäÂö¥ÈáçÁöÑÂÖ¨ÂÖ±Ë°õÁîüÂïèÈ°åÔºåÂº∑Ë™ø‰∫ÜÂ∞çÂèØÂèäÁöÑË®∫Êñ∑ÂíåÂπ≤È†êÂ∑•ÂÖ∑ÁöÑÈúÄÊ±Ç„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÔºå‰ΩÜÂÆÉÂÄëÂú®ÈòøÊãâ‰ºØË™ûÁí∞Â¢É‰∏≠ÁöÑÊáâÁî®Èù¢Ëá®ËëóÊåëÊà∞ÔºåÂåÖÊã¨Ê®ôË®òË≥áÊñôÈõÜÊúâÈôê„ÄÅË™ûË®ÄË§áÈõúÊÄßÂíåÁøªË≠ØÂÅèÂ∑Æ„ÄÇÊú¨Á†îÁ©∂ÂÖ®Èù¢Ë©ï‰º∞‰∫Ü 8 ÂÄã LLMÔºåÂåÖÊã¨‰∏ÄËà¨Â§öË™ûË®ÄÊ®°ÂûãÂíåÈõôË™ûÊ®°ÂûãÔºåÂú®‰∏çÂêåÁöÑÂøÉÁêÜÂÅ•Â∫∑Ë≥áÊñôÈõÜÔºà‰æãÂ¶Ç AraDepSu„ÄÅDreaddit„ÄÅMedMCQAÔºâ‰∏äÔºåÊé¢Ë®éÊèêÁ§∫Ë®≠Ë®à„ÄÅË™ûË®ÄÈÖçÁΩÆÔºàÈòøÊãâ‰ºØË™ûÂéüÊñáËàáÁøªË≠ØÂæåÁöÑËã±Ë™ûÔºåÂèç‰πã‰∫¶ÁÑ∂ÔºâÂíåÂ∞ëÊ¨°ÊèêÁ§∫Â∞çË®∫Êñ∑Ë°®ÁèæÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁôºÁèæÊèêÁ§∫Â∑•Á®ãÈ°ØËëóÂΩ±Èüø LLM ÂàÜÊï∏Ôºå‰∏ªË¶ÅÊòØÁî±ÊñºÊ∏õÂ∞ë‰∫ÜË™™ÊòéÈÅµÂæ™ÔºåÊàëÂÄëÁöÑÁµêÊßãÂåñÊèêÁ§∫Âú®Â§öÈ°ûË≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÁµêÊßãËºÉ‰∏çÂö¥Ë¨πÁöÑËÆäÈ´îÔºåÂπ≥ÂùáÂ∑ÆÁï∞ÁÇ∫ 14.5%„ÄÇÈõñÁÑ∂Ë™ûË®ÄÂ∞çË°®ÁèæÁöÑÂΩ±Èüø‰∏çÂ§ßÔºå‰ΩÜÊ®°ÂûãÈÅ∏ÊìáË¢´Ë≠âÊòéËá≥ÈóúÈáçË¶ÅÔºöPhi-3.5 MoE Âú®Âπ≥Ë°°Ê∫ñÁ¢∫Â∫¶ÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÔºåÁâπÂà•ÊòØÂú®‰∫åÂÖÉÂàÜÈ°ûÊñπÈù¢ÔºåËÄå Mistral NeMo Âú®Âö¥ÈáçÊÄßÈ†êÊ∏¨‰ªªÂãôÁöÑÂπ≥ÂùáÁµïÂ∞çË™§Â∑ÆÊñπÈù¢Ë°®ÁèæÂá∫ÂÑ™Áï∞ÁöÑË°®Áèæ„ÄÇÂ∞ëÊ¨°ÊèêÁ§∫ÂßãÁµÇÊîπÂñÑË°®ÁèæÔºåÁâπÂà•ÊòØÂú® GPT-4o Mini ‰∏äËßÄÂØüÂà∞Â§öÈ°ûÂàÜÈ°ûÁöÑÈ°ØËëóÂ¢ûÁõäÔºåÂ∞áÊ∫ñÁ¢∫Â∫¶ÊèêÈ´ò‰∫ÜÂπ≥Âùá 1.58 ÂÄç„ÄÇÈÄô‰∫õÁôºÁèæÂº∑Ë™ø‰∫ÜÊèêÁ§∫ÊúÄ‰Ω≥Âåñ„ÄÅÂ§öË™ûË®ÄÂàÜÊûêÂíåÂ∞ëÊ¨°Â≠∏ÁøíÂ∞çÊñºÈñãÁôºÈÅ©ÂêàÊñáÂåñ‰∏îÊúâÊïàÁöÑÂü∫Êñº LLM ÁöÑÂøÉÁêÜÂÅ•Â∫∑Â∑•ÂÖ∑‰ª•ÊúçÂãôÈòøÊãâ‰ºØË™û‰∫∫Âè£ÁöÑÈáçË¶ÅÊÄß„ÄÇ</paragraph>

##### **MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome Prediction**
2501.06823v1 by Yiqing Zhang, Xiaozhong Liu, Fabricio Murai

Clinical trials are the gold standard for assessing the effectiveness and
safety of drugs for treating diseases. Given the vast design space of drug
molecules, elevated financial cost, and multi-year timeline of these trials,
research on clinical trial outcome prediction has gained immense traction.
Accurate predictions must leverage data of diverse modes such as drug
molecules, target diseases, and eligibility criteria to infer successes and
failures. Previous Deep Learning approaches for this task, such as HINT, often
require wet lab data from synthesized molecules and/or rely on prior knowledge
to encode interactions as part of the model architecture. To address these
limitations, we propose a light-weight attention-based model, MEXA-CTP, to
integrate readily-available multi-modal data and generate effective
representations via specialized modules dubbed "mode experts", while avoiding
human biases in model design. We optimize MEXA-CTP with the Cauchy loss to
capture relevant interactions across modes. Our experiments on the Trial
Outcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon
existing approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,
and 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to
quantify the effectiveness of each component in our proposed method.

ÊëòË¶ÅÔºöËá®Â∫äË©¶È©óÊòØË©ï‰º∞Ê≤ªÁôÇÁñæÁóÖÁöÑËó•Áâ©ÊúâÊïàÊÄßÂíåÂÆâÂÖ®ÊÄßÁöÑÈªÉÈáëÊ®ôÊ∫ñ„ÄÇÈëëÊñºËó•Áâ©ÂàÜÂ≠êÁöÑÂª£Ê≥õË®≠Ë®àÁ©∫Èñì„ÄÅÈ´òÊòÇÁöÑË≤°ÂãôÊàêÊú¨ÂíåÈÄô‰∫õË©¶È©óÂ§öÂπ¥ÁöÑÊôÇÈñìË°®ÔºåËá®Â∫äË©¶È©óÁµêÊûúÈ†êÊ∏¨ÁöÑÁ†îÁ©∂Áç≤Âæó‰∫ÜÂ∑®Â§ßÁöÑÈóúÊ≥®„ÄÇÊ∫ñÁ¢∫ÁöÑÈ†êÊ∏¨ÂøÖÈ†àÂà©Áî®Ëó•Áâ©ÂàÜÂ≠ê„ÄÅÁõÆÊ®ôÁñæÁóÖÂíåÁ¨¶ÂêàË≥áÊ†ºÊ®ôÊ∫ñÁ≠âÂ§öÁ®ÆÊ®°ÂºèÁöÑÊï∏Êìö‰æÜÊé®Êñ∑ÊàêÂäüÂíåÂ§±Êïó„ÄÇÊ≠§‰ªªÂãôÁöÑÂÖàÂâçÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºà‰æãÂ¶Ç HINTÔºâÈÄöÂ∏∏ÈúÄË¶ÅÂêàÊàêÂàÜÂ≠êÁöÑÊøïÂØ¶È©óÂÆ§Êï∏ÊìöÂíå/Êàñ‰æùË≥¥ÊñºÂÖàÈ©óÁü•Ë≠òÂ∞á‰∫§‰∫íÁ∑®Á¢ºÁÇ∫Ê®°ÂûãÊû∂ÊßãÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËºïÈáèÁ¥öÁöÑÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÊ®°Âûã MEXA-CTPÔºå‰ª•Êï¥ÂêàÁèæÊàêÁöÑÂ§öÊ®°ÂºèÊï∏Êìö‰∏¶ÈÄöÈÅéÁ®±ÁÇ∫„ÄåÊ®°ÂºèÂ∞àÂÆ∂„ÄçÁöÑÂ∞àÁî®Ê®°ÁµÑÁî¢ÁîüÊúâÊïàÁöÑË°®Á§∫ÔºåÂêåÊôÇÈÅøÂÖçÊ®°ÂûãË®≠Ë®à‰∏≠ÁöÑ‰∫∫ÁÇ∫ÂÅèÂ∑Æ„ÄÇÊàëÂÄë‰ΩøÁî®ÊüØË•øÊêçÂ§±ÂáΩÊï∏ÊúÄ‰Ω≥Âåñ MEXA-CTPÔºå‰ª•ÊçïÊçâË∑®Ê®°ÂºèÁõ∏ÈóúÁöÑ‰∫§‰∫í„ÄÇÊàëÂÄëÂú®Ë©¶È©óÁµêÊûúÈ†êÊ∏¨ (TOP) Âü∫Ê∫ñ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåËàá HINT Áõ∏ÊØîÔºåMEXA-CTP ÂàÜÂà•Âú® F1 ÂàÜÊï∏‰∏äÊèêÈ´ò‰∫Ü 11.3%„ÄÅPR-AUC ‰∏äÊèêÈ´ò‰∫Ü 12.2%„ÄÅROC-AUC ‰∏äÊèêÈ´ò‰∫Ü 2.5%„ÄÇÊèê‰æõ‰∫ÜÊ∂àËûçÁ†îÁ©∂‰æÜÈáèÂåñÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ï‰∏≠ÊØèÂÄãÁµÑ‰ª∂ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **PGP-SAM: Prototype-Guided Prompt Learning for Efficient Few-Shot Medical Image Segmentation**
2501.06692v1 by Zhonghao Yan, Zijin Yin, Tianyu Lin, Xiangzhu Zeng, Kongming Liang, Zhanyu Ma

The Segment Anything Model (SAM) has demonstrated strong and versatile
segmentation capabilities, along with intuitive prompt-based interactions.
However, customizing SAM for medical image segmentation requires massive
amounts of pixel-level annotations and precise point- or box-based prompt
designs. To address these challenges, we introduce PGP-SAM, a novel
prototype-based few-shot tuning approach that uses limited samples to replace
tedious manual prompts. Our key idea is to leverage inter- and intra-class
prototypes to capture class-specific knowledge and relationships. We propose
two main components: (1) a plug-and-play contextual modulation module that
integrates multi-scale information, and (2) a class-guided cross-attention
mechanism that fuses prototypes and features for automatic prompt generation.
Experiments on a public multi-organ dataset and a private ventricle dataset
demonstrate that PGP-SAM achieves superior mean Dice scores compared with
existing prompt-free SAM variants, while using only 10\% of the 2D slices.

ÊëòË¶ÅÔºöÂàÜÊÆµ‰ªª‰ΩïÊ®°Âûã (SAM) Â∑≤Â±ïÁ§∫Âá∫Âº∫Â§ß‰∏îÂ§öÂäüËÉΩÁöÑÂàÜÊÆµËÉΩÂäõÔºå‰ª•ÂèäÁõ¥ËßÇÁöÑÂü∫‰∫éÊèêÁ§∫ÁöÑ‰∫§‰∫í„ÄÇ
ÁÑ∂ËÄåÔºåÈíàÂØπÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÊÆµÂÆöÂà∂ SAM ÈúÄË¶ÅÂ§ßÈáèÂÉèÁ¥†Á∫ßÊ≥®ÈáäÂíåÁ≤æÁ°ÆÁöÑÂü∫‰∫éÁÇπÊàñÊ°ÜÁöÑÊèêÁ§∫ËÆæËÆ°„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü PGP-SAMÔºå‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÂéüÂûãÁöÑÂ∞ëÈáèÈïúÂ§¥ÂæÆË∞ÉÊñπÊ≥ïÔºåÂÆÉ‰ΩøÁî®ÊúâÈôêÁöÑÊ†∑Êú¨Êù•ÊõøÊç¢ÁπÅÁêêÁöÑÊâãÂä®ÊèêÁ§∫„ÄÇÊàë‰ª¨ÁöÑÂÖ≥ÈîÆÊÄùÊÉ≥ÊòØÂà©Áî®Á±ªÈó¥ÂíåÁ±ªÂÜÖÂéüÂûãÊù•ÊçïÊçâÁâπÂÆö‰∫éÁ±ªÁöÑÁü•ËØÜÂíåÂÖ≥Á≥ª„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§‰∏™‰∏ªË¶ÅÁªÑ‰ª∂Ôºö(1) ‰∏Ä‰∏™Âç≥ÊèíÂç≥Áî®ÁöÑ‰∏ä‰∏ãÊñáË∞ÉÂà∂Ê®°ÂùóÔºåÂÆÉÈõÜÊàê‰∫ÜÂ§öÂ∞∫Â∫¶‰ø°ÊÅØÔºå‰ª•Âèä (2) ‰∏Ä‰∏™Á±ªÊåáÂØºÁöÑ‰∫§ÂèâÊ≥®ÊÑèÊú∫Âà∂ÔºåÂÆÉËûçÂêà‰∫ÜÂéüÂûãÂíåÁâπÂæÅ‰ª•ËøõË°åËá™Âä®ÊèêÁ§∫ÁîüÊàê„ÄÇÂú®ÂÖ¨ÂÖ±Â§öÂô®ÂÆòÊï∞ÊçÆÈõÜÂíåÁßÅ‰∫∫ÂøÉÂÆ§Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåPGP-SAM ‰∏éÁé∞ÊúâÁöÑÊó†ÊèêÁ§∫ SAM Âèò‰ΩìÁõ∏ÊØîÂÆûÁé∞‰∫ÜÂçìË∂äÁöÑÂπ≥Âùá Dice ÂàÜÊï∞ÔºåÂêåÊó∂‰ªÖ‰ΩøÁî®‰∫Ü 2D ÂàáÁâáÁöÑ 10%„ÄÇ

##### **Imbalanced Medical Image Segmentation with Pixel-dependent Noisy Labels**
2501.06678v1 by Erjian Guo, Zicheng Wang, Zhen Zhao, Luping Zhou

Accurate medical image segmentation is often hindered by noisy labels in
training data, due to the challenges of annotating medical images. Prior
research works addressing noisy labels tend to make class-dependent
assumptions, overlooking the pixel-dependent nature of most noisy labels.
Furthermore, existing methods typically apply fixed thresholds to filter out
noisy labels, risking the removal of minority classes and consequently
degrading segmentation performance. To bridge these gaps, our proposed
framework, Collaborative Learning with Curriculum Selection (CLCS), addresses
pixel-dependent noisy labels with class imbalance. CLCS advances the existing
works by i) treating noisy labels as pixel-dependent and addressing them
through a collaborative learning framework, and ii) employing a curriculum
dynamic thresholding approach adapting to model learning progress to select
clean data samples to mitigate the class imbalance issue, and iii) applying a
noise balance loss to noisy data samples to improve data utilization instead of
discarding them outright. Specifically, our CLCS contains two modules:
Curriculum Noisy Label Sample Selection (CNS) and Noise Balance Loss (NBL). In
the CNS module, we designed a two-branch network with discrepancy loss for
collaborative learning so that different feature representations of the same
instance could be extracted from distinct views and used to vote the class
probabilities of pixels. Besides, a curriculum dynamic threshold is adopted to
select clean-label samples through probability voting. In the NBL module,
instead of directly dropping the suspiciously noisy labels, we further adopt a
robust loss to leverage such instances to boost the performance.

ÊëòË¶ÅÔºö<paragraph>ÂáÜÁ°ÆÁöÑÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÂâ≤ÈÄöÂ∏∏‰ºöÂèóÂà∞ËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Ê†áÁ≠æÊúâÂô™Â£∞ÁöÑÈòªÁ¢çÔºåËøôÊòØÂõ†‰∏∫ÂØπÂåªÂ≠¶ÂΩ±ÂÉèËøõË°åÊ≥®ÈáäÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇËß£ÂÜ≥Ê†áÁ≠æÊúâÂô™Â£∞ÁöÑÂÖàÂâçÁ†îÁ©∂Â∑•‰ΩúÂÄæÂêë‰∫éÂÅöÂá∫Á±ªÁõ∏ÂÖ≥ÁöÑÂÅáËÆæÔºåËÄåÂøΩÁï•‰∫ÜÂ§ßÂ§öÊï∞Ê†áÁ≠æÊúâÂô™Â£∞ÁöÑÂÉèÁ¥†Áõ∏ÂÖ≥ÊÄßË¥®„ÄÇÊ≠§Â§ñÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Â∫îÁî®Âõ∫ÂÆöÈòàÂÄºÊù•ËøáÊª§ÊéâÊ†áÁ≠æÊúâÂô™Â£∞ÔºåËøôÊúâÂ∞ÜÂ∞ëÊï∞Á±ªÂà´ÁöÑÊ†áÁ≠æÁßªÈô§ÁöÑÈ£éÈô©Ôºå‰ªéËÄåÈôç‰ΩéÂàÜÂâ≤ÊÄßËÉΩ„ÄÇ‰∏∫‰∫ÜÂº•ÂêàËøô‰∫õÂ∑ÆË∑ùÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊ°ÜÊû∂ÔºåÂç≥ÂÖ∑ÊúâËØæÁ®ãÈÄâÊã©ÁöÑÂçè‰ΩúÂ≠¶‰π† (CLCS)ÔºåËß£ÂÜ≥‰∫ÜÁ±ªÂà´‰∏çÂπ≥Ë°°ÁöÑÂÉèÁ¥†Áõ∏ÂÖ≥Ê†áÁ≠æÊúâÂô™Â£∞ÁöÑÈóÆÈ¢ò„ÄÇCLCS ÈÄöËøá‰ª•‰∏ãÊñπÂºèÊèêÂçá‰∫ÜÁé∞ÊúâÂ∑•‰ΩúÔºöi) Â∞ÜÊ†áÁ≠æÊúâÂô™Â£∞ËßÜ‰∏∫ÂÉèÁ¥†Áõ∏ÂÖ≥ÔºåÂπ∂ÈÄöËøáÂçè‰ΩúÂ≠¶‰π†Ê°ÜÊû∂Ëß£ÂÜ≥ÂÆÉ‰ª¨Ôºåii) ÈááÁî®ËØæÁ®ãÂä®ÊÄÅÈòàÂÄºÂåñÊñπÊ≥ïÔºåÈÄÇÂ∫îÊ®°ÂûãÂ≠¶‰π†ËøõÂ∫¶ÔºåÈÄâÊã©Âπ≤ÂáÄÁöÑÊï∞ÊçÆÊ†∑Êú¨‰ª•ÂáèËΩªÁ±ªÂà´‰∏çÂπ≥Ë°°ÈóÆÈ¢òÔºå‰ª•Âèä iii) ÂØπÊ†áÁ≠æÊúâÂô™Â£∞ÁöÑÊï∞ÊçÆÊ†∑Êú¨Â∫îÁî®Âô™Â£∞Âπ≥Ë°°ÊçüÂ§±Ôºå‰ª•ÊèêÈ´òÊï∞ÊçÆÂà©Áî®ÁéáÔºåËÄå‰∏çÊòØÁõ¥Êé•‰∏¢ÂºÉÂÆÉ‰ª¨„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ÁöÑ CLCS ÂåÖÂê´‰∏§‰∏™Ê®°ÂùóÔºöËØæÁ®ãÊ†áÁ≠æÊúâÂô™Â£∞Ê†∑Êú¨ÈÄâÊã© (CNS) ÂíåÂô™Â£∞Âπ≥Ë°°ÊçüÂ§± (NBL)„ÄÇÂú® CNS Ê®°Âùó‰∏≠ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÂÖ∑ÊúâÂ∑ÆÂºÇÊçüÂ§±ÁöÑ‰∏§ÂàÜÊîØÁΩëÁªúÔºåÁî®‰∫éÂçè‰ΩúÂ≠¶‰π†Ôºå‰ª•‰æøÂèØ‰ª•‰ªé‰∏çÂêåÁöÑËßÜÂõæ‰∏≠ÊèêÂèñÂêå‰∏ÄÂÆû‰æãÁöÑ‰∏çÂêåÁâπÂæÅË°®Á§∫ÔºåÂπ∂Áî®‰∫éÊäïÁ•®ÂÉèÁ¥†ÁöÑÁ±ªÂà´Ê¶ÇÁéá„ÄÇÊ≠§Â§ñÔºåÈááÁî®ËØæÁ®ãÂä®ÊÄÅÈòàÂÄºÈÄöËøáÊ¶ÇÁéáÊäïÁ•®ÈÄâÊã©Âπ≤ÂáÄÊ†áÁ≠æÊ†∑Êú¨„ÄÇÂú® NBL Ê®°Âùó‰∏≠ÔºåÊàë‰ª¨Ê≤°ÊúâÁõ¥Êé•‰∏¢ÂºÉÂèØÁñëÁöÑÊ†áÁ≠æÊúâÂô™Â£∞ÔºåËÄåÊòØËøõ‰∏ÄÊ≠•ÈááÁî®È≤ÅÊ£íÊçüÂ§±Êù•Âà©Áî®Ê≠§Á±ªÂÆû‰æã‰ª•ÊèêÂçáÊÄßËÉΩ„ÄÇ</paragraph>

##### **MedCT: A Clinical Terminology Graph for Generative AI Applications in Healthcare**
2501.06465v1 by Ye Chen, Dongdong Huang, Haoyun Xu, Cong Fu, Lin Sheng, Qingli Zhou, Yuqiang Shen, Kai Wang

We introduce the world's first clinical terminology for the Chinese
healthcare community, namely MedCT, accompanied by a clinical foundation model
MedBERT and an entity linking model MedLink. The MedCT system enables
standardized and programmable representation of Chinese clinical data,
successively stimulating the development of new medicines, treatment pathways,
and better patient outcomes for the populous Chinese community. Moreover, the
MedCT knowledge graph provides a principled mechanism to minimize the
hallucination problem of large language models (LLMs), therefore achieving
significant levels of accuracy and safety in LLM-based clinical applications.
By leveraging the LLMs' emergent capabilities of generativeness and
expressiveness, we were able to rapidly built a production-quality terminology
system and deployed to real-world clinical field within three months, while
classical terminologies like SNOMED CT have gone through more than twenty years
development. Our experiments show that the MedCT system achieves
state-of-the-art (SOTA) performance in semantic matching and entity linking
tasks, not only for Chinese but also for English. We also conducted a
longitudinal field experiment by applying MedCT and LLMs in a representative
spectrum of clinical tasks, including electronic health record (EHR)
auto-generation and medical document search for diagnostic decision making. Our
study shows a multitude of values of MedCT for clinical workflows and patient
outcomes, especially in the new genre of clinical LLM applications. We present
our approach in sufficient engineering detail, such that implementing a
clinical terminology for other non-English societies should be readily
reproducible. We openly release our terminology, models and algorithms, along
with real-world clinical datasets for the development.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÁÇ∫‰∏≠ÊñáÈÜ´ÁôÇÁ§æÁæ§ÂºïÈÄ≤ÂÖ®ÁêÉÈ¶ñÂÄãËá®Â∫äË°ìË™ûÔºåÂç≥ MedCTÔºå‰∏¶ÈôÑÊúâËá®Â∫äÂü∫Á§éÊ®°Âûã MedBERT ÂíåÂØ¶È´îÈÄ£ÁµêÊ®°Âûã MedLink„ÄÇMedCT Á≥ªÁµ±ËÉΩÊ®ôÊ∫ñÂåñ‰∏¶‰ª•Á®ãÂºèÂåñÊñπÂºèÂëàÁèæ‰∏≠ÊñáËá®Â∫äË≥áÊñôÔºåÈÄ≤ËÄåÂà∫ÊøÄÊñ∞Ëó•Áâ©„ÄÅÊ≤ªÁôÇÈÄîÂæëÁöÑÈñãÁôºÔºå‰∏¶ÁÇ∫‰∫∫Âè£ÁúæÂ§öÁöÑËèØ‰∫∫Á§æÁæ§Â∏∂‰æÜÊõ¥Â•ΩÁöÑÈÜ´ÁôÇÁµêÊûú„ÄÇÊ≠§Â§ñÔºåMedCT Áü•Ë≠òÂúñË≠úÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂéüÂâáÊÄßÁöÑÊ©üÂà∂ÔºåÁî®‰ª•ÊúÄÂ∞èÂåñÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂπªË¶∫ÂïèÈ°åÔºåÂõ†Ê≠§Âú®Âü∫Êñº LLM ÁöÑËá®Â∫äÊáâÁî®‰∏≠ÂØ¶Áèæ‰∫ÜÈ°ØËëóÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÈÄèÈÅéÂà©Áî® LLM Âú®ÁîüÊàêÊÄßÂíåË°®ÈÅîÊÄßÊñπÈù¢ÁöÑÈ°ØËëóËÉΩÂäõÔºåÊàëÂÄëÂæó‰ª•Âø´ÈÄüÂª∫Êßã‰∏ÄÂÄãÁîüÁî¢ÂìÅË≥™ÁöÑË°ìË™ûÁ≥ªÁµ±Ôºå‰∏¶Âú®‰∏âÂÄãÊúàÂÖßÈÉ®ÁΩ≤Âà∞ÁèæÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÈ†òÂüüÔºåËÄåÂÉè SNOMED CT Á≠âÂÇ≥Áµ±Ë°ìË™ûÂ∑≤Ê≠∑Á∂ì‰∫åÂçÅÂ§öÂπ¥ÁöÑÁôºÂ±ï„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåMedCT Á≥ªÁµ±Âú®Ë™ûÊÑèÈÖçÂ∞çÂíåÂØ¶È´îÈÄ£Áµê‰ªªÂãô‰∏≠ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ (SOTA) ÁöÑÊïàËÉΩÔºå‰∏çÂÉÖÈÅ©Áî®Êñº‰∏≠ÊñáÔºå‰πüÈÅ©Áî®ÊñºËã±Êñá„ÄÇÊàëÂÄëÈÇÑÈÄèÈÅéÂú®ÂÖ∑‰ª£Ë°®ÊÄßÁöÑËá®Â∫ä‰ªªÂãôÁØÑÂúç‰∏≠ÊáâÁî® MedCT Âíå LLM ÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÁ∏±ÂêëÂØ¶Âú∞ÂØ¶È©óÔºåÂåÖÊã¨ÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) Ëá™ÂãïÁîüÊàêÂíåÁî®ÊñºË®∫Êñ∑Ê±∫Á≠ñÁöÑÈÜ´ÁôÇÊñá‰ª∂ÊêúÂ∞ã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂È°ØÁ§∫ MedCT Â∞çËá®Â∫äÂ∑•‰ΩúÊµÅÁ®ãÂíåÊÇ£ËÄÖÁµêÊûúÂÖ∑ÊúâÂ§öÈáçÂÉπÂÄºÔºåÁâπÂà•ÊòØÂú®Êñ∞È°ûÂûãÁöÑËá®Â∫ä LLM ÊáâÁî®‰∏≠„ÄÇÊàëÂÄë‰ª•ÂÖÖÂàÜÁöÑÂ∑•Á®ãÁ¥∞ÁØÄÂëàÁèæÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÂõ†Ê≠§ÂØ¶‰ΩúÂÖ∂‰ªñÈùûËã±Ë™ûÁ§æÊúÉÁöÑËá®Â∫äË°ìË™ûÊáâÊòìÊñºË§áË£Ω„ÄÇÊàëÂÄëÈñãÊîæÁôºÂ∏ÉÊàëÂÄëÁöÑË°ìË™û„ÄÅÊ®°ÂûãÂíåÊºîÁÆóÊ≥ïÔºå‰ª•ÂèäÁî®ÊñºÈñãÁôºÁöÑÁèæÂØ¶‰∏ñÁïåËá®Â∫äË≥áÊñôÈõÜ„ÄÇ</paragraph>

##### **Deep Learning on Hester Davis Scores for Inpatient Fall Prediction**
2501.06432v1 by Hojjat Salehinejad, Ricky Rojas, Kingsley Iheasirim, Mohammed Yousufuddin, Bijan Borah

Fall risk prediction among hospitalized patients is a critical aspect of
patient safety in clinical settings, and accurate models can help prevent
adverse events. The Hester Davis Score (HDS) is commonly used to assess fall
risk, with current clinical practice relying on a threshold-based approach. In
this method, a patient is classified as high-risk when their HDS exceeds a
predefined threshold. However, this approach may fail to capture dynamic
patterns in fall risk over time. In this study, we model the threshold-based
approach and propose two machine learning approaches for enhanced fall
prediction: One-step ahead fall prediction and sequence-to-point fall
prediction. The one-step ahead model uses the HDS at the current timestamp to
predict the risk at the next timestamp, while the sequence-to-point model
leverages all preceding HDS values to predict fall risk using deep learning. We
compare these approaches to assess their accuracy in fall risk prediction,
demonstrating that deep learning can outperform the traditional threshold-based
method by capturing temporal patterns and improving prediction reliability.
These findings highlight the potential for data-driven approaches to enhance
patient safety through more reliable fall prevention strategies.

ÊëòË¶ÅÔºö‰ΩèÈô¢ÊÇ£ËÄÖÁöÑË∑åÂÄíÈ¢®Èö™È†êÊ∏¨ÊòØËá®Â∫äÁí∞Â¢É‰∏≠ÊÇ£ËÄÖÂÆâÂÖ®ÁöÑÈáçË¶ÅÈù¢ÂêëÔºåÁ≤æÁ¢∫ÁöÑÊ®°ÂûãÊúâÂä©ÊñºÈ†êÈò≤‰∏çËâØ‰∫ã‰ª∂„ÄÇHester Davis Ë©ïÂàÜ (HDS) Â∏∏Áî®ÊñºË©ï‰º∞Ë∑åÂÄíÈ¢®Èö™ÔºåÁõÆÂâçÁöÑËá®Â∫äÂØ¶Âãô‰æùË≥¥ÊñºÂü∫ÊñºÈñæÂÄºÁöÑË©ï‰º∞ÊñπÂºè„ÄÇÂú®Ê≠§ÊñπÊ≥ï‰∏≠ÔºåÁï∂ÊÇ£ËÄÖÁöÑ HDS Ë∂ÖÈÅéÈ†êÂÖàÂÆöÁæ©ÁöÑÈñæÂÄºÊôÇÔºåÊúÉÂ∞áÂÖ∂Ê≠∏È°ûÁÇ∫È´òÈ¢®Èö™„ÄÇÁÑ∂ËÄåÔºåÊ≠§ÊñπÊ≥ïÂèØËÉΩÁÑ°Ê≥ïÊçïÊçâÈö®ËëóÊôÇÈñìÊé®ÁßªËÄåÁî¢ÁîüÁöÑÂãïÊÖãË∑åÂÄíÈ¢®Èö™Ê®°Âºè„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞çÂü∫ÊñºÈñæÂÄºÁöÑË©ï‰º∞ÊñπÂºèÈÄ≤Ë°åÂª∫Ê®°Ôºå‰∏¶ÊèêÂá∫ÂÖ©Á®ÆÊ©üÂô®Â≠∏ÁøíË©ï‰º∞ÊñπÂºè‰ª•Âä†Âº∑Ë∑åÂÄíÈ†êÊ∏¨ÔºöÂñÆÊ≠•ÂâçÁûªË∑åÂÄíÈ†êÊ∏¨ÂíåÂ∫èÂàóÂ∞çÈªûË∑åÂÄíÈ†êÊ∏¨„ÄÇÂñÆÊ≠•ÂâçÁûªÊ®°Âûã‰ΩøÁî®Áï∂ÂâçÊôÇÈñìÊà≥Ë®òÁöÑ HDS È†êÊ∏¨‰∏ã‰∏ÄÂÄãÊôÇÈñìÊà≥Ë®òÁöÑÈ¢®Èö™ÔºåËÄåÂ∫èÂàóÂ∞çÈªûÊ®°ÂûãÂâáÂà©Áî®ÊâÄÊúâÂâç‰∏ÄÂÄã HDS ÂÄº‰ΩøÁî®Ê∑±Â∫¶Â≠∏Áøí‰æÜÈ†êÊ∏¨Ë∑åÂÄíÈ¢®Èö™„ÄÇÊàëÂÄëÊØîËºÉÈÄô‰∫õË©ï‰º∞ÊñπÂºè‰ª•Ë©ï‰º∞ÂÖ∂Âú®Ë∑åÂÄíÈ¢®Èö™È†êÊ∏¨‰∏≠ÁöÑÊ∫ñÁ¢∫ÊÄßÔºåË≠âÊòéÊ∑±Â∫¶Â≠∏ÁøíÂèØ‰ª•ÈÄèÈÅéÊçïÊçâÊôÇÈñìÊ®°ÂºèÂíåÊèêÂçáÈ†êÊ∏¨ÂèØÈù†ÊÄßÔºåÂÑ™ÊñºÂÇ≥Áµ±ÁöÑÂü∫ÊñºÈñæÂÄºÁöÑË©ï‰º∞ÊñπÂºè„ÄÇÈÄô‰∫õÁôºÁèæÂº∑Ë™ø‰∫ÜË≥áÊñôÈ©ÖÂãïË©ï‰º∞ÊñπÂºèÂú®ÈÄèÈÅéÊõ¥ÂèØÈù†ÁöÑË∑åÂÄíÈ†êÈò≤Á≠ñÁï•‰æÜÂä†Âº∑ÊÇ£ËÄÖÂÆâÂÖ®ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts**
2501.06365v1 by Elizabeth Schaefer, Kirk Roberts

This paper presents a pipeline for mitigating gender bias in large language
models (LLMs) used in medical literature by neutralizing gendered occupational
pronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to
identify and modify pronouns tied to professions. We developed a BERT-based
model, ``Modern Occupational Bias Elimination with Refined Training,'' or
``MOBERT,'' trained on these neutralized abstracts, and compared its
performance with ``1965Bert,'' trained on the original dataset. MOBERT achieved
a 70\% inclusive replacement rate, while 1965Bert reached only 4\%. A further
analysis of MOBERT revealed that pronoun replacement accuracy correlated with
the frequency of occupational terms in the training data. We propose expanding
the dataset and refining the pipeline to improve performance and ensure more
equitable language modeling in medical applications.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁÆ°ÈÅìÔºåÁî®ÊñºÁ∑©Ëß£Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑÊÄßÂà•ÂÅèË¶ãÔºåÈÄô‰∫õÊ®°ÂûãÈÄèÈÅé‰∏≠ÂíåÊÄßÂà•ËÅ∑Ê•≠‰ª£ÂêçË©ûÔºåÁî®ÊñºÈÜ´Â≠∏ÊñáÁçª‰∏≠„ÄÇÊàëÂÄëËôïÁêÜ‰∫Ü 1965 Âπ¥Ëá≥ 1980 Âπ¥Èñì 379,000 ÁØá PubMed ÊñáÊëòÁöÑË≥áÊñôÈõÜÔºå‰ª•Ë≠òÂà•Âíå‰øÆÊîπËàáËÅ∑Ê•≠Áõ∏ÈóúÁöÑ‰ª£ÂêçË©û„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã BERT-based Ê®°ÂûãÔºåÁ®±ÁÇ∫„ÄåÊé°Áî®Á≤æÁ∑ªË®ìÁ∑¥ÁöÑÁèæ‰ª£ËÅ∑Ê•≠ÂÅèË¶ãÊ∂àÈô§„ÄçÊàñ„ÄåMOBERT„ÄçÔºå‰∏¶Âú®ÈÄô‰∫õ‰∏≠ÂíåÁöÑÊñáÊëò‰∏äÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰∏¶Â∞áÂÖ∂ÊïàËÉΩËàáÂú®ÂéüÂßãË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑ„Äå1965Bert„ÄçÈÄ≤Ë°åÊØîËºÉ„ÄÇMOBERT ÈÅîÂà∞‰∫Ü 70% ÁöÑÂåÖÂÆπÊÄßÊõøÊèõÁéáÔºåËÄå 1965Bert ÂÉÖÈÅîÂà∞ 4%„ÄÇÈÄ≤‰∏ÄÊ≠•ÂàÜÊûê MOBERT È°ØÁ§∫Ôºå‰ª£ÂêçË©ûÊõøÊèõÁöÑÊ∫ñÁ¢∫ÊÄßËàáË®ìÁ∑¥Ë≥áÊñô‰∏≠ËÅ∑Ê•≠Ë°ìË™ûÁöÑÈ†ªÁéáÁõ∏Èóú„ÄÇÊàëÂÄëÂª∫Ë≠∞Êì¥ÂÖÖË≥áÊñôÈõÜ‰∏¶Á≤æÁ∑ªÂåñÁÆ°ÈÅìÔºå‰ª•ÊèêÂçáÊïàËÉΩ‰∏¶Á¢∫‰øùÂú®ÈÜ´Â≠∏ÊáâÁî®‰∏≠ÈÄ≤Ë°åÊõ¥ÂÖ¨Âπ≥ÁöÑË™ûË®ÄÂª∫Ê®°„ÄÇ

##### **Scale-up Unlearnable Examples Learning with High-Performance Computing**
2501.06080v1 by Yanfan Zhu, Issac Lyngaas, Murali Gopalakrishnan Meena, Mary Ellen I. Koran, Bradley Malin, Daniel Moyer, Shunxing Bao, Anuj Kapadia, Xiao Wang, Bennett Landman, Yuankai Huo

Recent advancements in AI models are structured to retain user interactions,
which could inadvertently include sensitive healthcare data. In the healthcare
field, particularly when radiologists use AI-driven diagnostic tools hosted on
online platforms, there is a risk that medical imaging data may be repurposed
for future AI training without explicit consent, spotlighting critical privacy
and intellectual property concerns around healthcare data usage. Addressing
these privacy challenges, a novel approach known as Unlearnable Examples (UEs)
has been introduced, aiming to make data unlearnable to deep learning models. A
prominent method within this area, called Unlearnable Clustering (UC), has
shown improved UE performance with larger batch sizes but was previously
limited by computational resources. To push the boundaries of UE performance
with theoretically unlimited resources, we scaled up UC learning across various
datasets using Distributed Data Parallel (DDP) training on the Summit
supercomputer. Our goal was to examine UE efficacy at high-performance
computing (HPC) levels to prevent unauthorized learning and enhance data
security, particularly exploring the impact of batch size on UE's
unlearnability. Utilizing the robust computational capabilities of the Summit,
extensive experiments were conducted on diverse datasets such as Pets,
MedMNist, Flowers, and Flowers102. Our findings reveal that both overly large
and overly small batch sizes can lead to performance instability and affect
accuracy. However, the relationship between batch size and unlearnability
varied across datasets, highlighting the necessity for tailored batch size
strategies to achieve optimal data protection. Our results underscore the
critical role of selecting appropriate batch sizes based on the specific
characteristics of each dataset to prevent learning and ensure data security in
deep learning applications.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÂú® AI Ê®°Âûã‰∏≠ÁöÑÈÄ≤Â±ïË¢´Âª∫ÊßãÁÇ∫‰øùÁïô‰ΩøÁî®ËÄÖ‰∫íÂãïÔºå
ÈÄôÂèØËÉΩÁÑ°ÊÑèÈñìÂåÖÂê´ÊïèÊÑüÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ë≥áÊñô„ÄÇÂú®ÈÜ´ÁôÇ‰øùÂÅ•
È†òÂüüÔºåÁâπÂà•ÊòØÁï∂ÊîæÂ∞ÑÁßëÈÜ´Â∏´‰ΩøÁî®Á∑ö‰∏äÂπ≥Âè∞‰∏äÊèê‰æõÁöÑ AI È©ÖÂãïË®∫Êñ∑Â∑•ÂÖ∑ÊôÇÔºåÊúâÈ¢®Èö™ÊòØÈÜ´ÁôÇÂΩ±ÂÉèË≥áÊñôÂèØËÉΩÊúÉË¢´ÈáçÊñ∞Áî®ÊñºÊú™‰æÜÁöÑ AI Ë®ìÁ∑¥ÔºåËÄåÊú™Á∂ìÊòéÁ¢∫ÂêåÊÑèÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜËàáÈÜ´ÁôÇ‰øùÂÅ•Ë≥áÊñô‰ΩøÁî®Áõ∏ÈóúÁöÑÈö±ÁßÅÂíåÊô∫ÊÖßË≤°Áî¢Ê¨äÂïèÈ°å„ÄÇÁÇ∫‰∫ÜÊáâÂ∞ç
ÈÄô‰∫õÈö±ÁßÅÊåëÊà∞ÔºåÂ∑≤Â∞éÂÖ•‰∏ÄÁ®ÆÁ®±ÁÇ∫‰∏çÂèØÂ≠∏ÁøíÁØÑ‰æã (UE) ÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®ËÆìË≥áÊñôÂ∞çÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰∏çÂèØÂ≠∏Áøí„ÄÇÈÄôÂÄãÈ†òÂüüÂÖß‰∏ÄÁ®ÆËëóÂêçÁöÑÁ®±ÁÇ∫‰∏çÂèØÂ≠∏ÁøíËÅöÈ°û (UC) ÁöÑÊñπÊ≥ïÔºåÂ∑≤È°ØÁ§∫Âá∫Âú®ËºÉÂ§ßÁöÑÊâπÊ¨°Â§ßÂ∞è‰∏ãÊúâÊîπÂñÑÁöÑ UE ÊïàËÉΩÔºå‰ΩÜÂÖàÂâçÂèóÂà∞ÈÅãÁÆóË≥áÊ∫êÁöÑÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÂú®ÁêÜË´ñ‰∏äÁÑ°ÈôêÂà∂Ë≥áÊ∫êÁöÑÊÉÖÊ≥Å‰∏ãÊé®Âãï UE ÊïàËÉΩÁöÑÁïåÁ∑öÔºåÊàëÂÄëÂú® Summit Ë∂ÖÁ¥öÈõªËÖ¶‰∏ä‰ΩøÁî®ÂàÜÊï£ÂºèË≥áÊñôÂπ≥Ë°å (DDP) Ë®ìÁ∑¥ÔºåÊì¥Â§ß‰∫ÜÂêÑÁ®ÆË≥áÊñôÈõÜÁöÑ UC Â≠∏Áøí„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÊ™¢Êü• UE Âú®È´òÊïàËÉΩÈÅãÁÆó (HPC) Â±§Á¥öÁöÑÊïàËÉΩÔºå‰ª•Èò≤Ê≠¢Êú™Á∂ìÊéàÊ¨äÁöÑÂ≠∏ÁøíÔºå‰∏¶Â¢ûÂº∑Ë≥áÊñôÂÆâÂÖ®ÊÄßÔºåÁâπÂà•ÊòØÊé¢Ë®éÊâπÊ¨°Â§ßÂ∞èÂ∞ç UE ‰∏çÂèØÂ≠∏ÁøíÊÄßÁöÑÂΩ±Èüø„ÄÇÂà©Áî® Summit Âº∑Â§ßÁöÑÈÅãÁÆóËÉΩÂäõÔºåÂú®ÂêÑÁ®ÆË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰æãÂ¶Ç Pets„ÄÅMedMNist„ÄÅFlowers Âíå Flowers102„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÈÅéÂ§ßÊàñÈÅéÂ∞èÁöÑÊâπÊ¨°Â§ßÂ∞èÈÉΩÂèØËÉΩÂ∞éËá¥ÊïàËÉΩ‰∏çÁ©©ÂÆöÔºå‰∏¶ÂΩ±ÈüøÊ∫ñÁ¢∫Â∫¶„ÄÇÁÑ∂ËÄåÔºåÊâπÊ¨°Â§ßÂ∞èËàá‰∏çÂèØÂ≠∏ÁøíÊÄß‰πãÈñìÁöÑÈóú‰øÇÂú®ÂêÑÂÄãË≥áÊñôÈõÜ‰πãÈñìÊúâÊâÄ‰∏çÂêåÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜÊ†πÊìöÁâπÂÆöË≥áÊñôÈõÜÁöÑÁâπÂæµÈáèË∫´ÊâìÈÄ†ÊâπÊ¨°Â§ßÂ∞èÁ≠ñÁï•‰ª•ÈÅîÊàêÊúÄ‰Ω≥Ë≥áÊñô‰øùË≠∑ÁöÑÂøÖË¶ÅÊÄß„ÄÇÊàëÂÄëÁöÑÁµêÊûúÂº∑Ë™ø‰∫ÜÊ†πÊìöÊØèÂÄãË≥áÊñôÈõÜÁöÑÁâπÂÆöÁâπÂæµÈÅ∏ÊìáÈÅ©Áï∂ÊâπÊ¨°Â§ßÂ∞è‰ª•Èò≤Ê≠¢Â≠∏ÁøíÔºå‰∏¶Á¢∫‰øùÊ∑±Â∫¶Â≠∏ÁøíÊáâÁî®Á®ãÂºè‰∏≠Ë≥áÊñôÂÆâÂÖ®ÊÄßÁöÑÈóúÈçµ‰ΩúÁî®„ÄÇ</paragraph>

##### **AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery**
2501.06039v1 by Johann Wenckstern, Eeshaan Jain, Kiril Vasilev, Matteo Pariset, Andreas Wicki, Gabriele Gut, Charlotte Bunne

Spatial proteomics technologies have transformed our understanding of complex
tissue architectures by enabling simultaneous analysis of multiple molecular
markers and their spatial organization. The high dimensionality of these data,
varying marker combinations across experiments and heterogeneous study designs
pose unique challenges for computational analysis. Here, we present Virtual
Tissues (VirTues), a foundation model framework for biological tissues that
operates across the molecular, cellular and tissue scale. VirTues introduces
innovations in transformer architecture design, including a novel tokenization
scheme that captures both spatial and marker dimensions, and attention
mechanisms that scale to high-dimensional multiplex data while maintaining
interpretability. Trained on diverse cancer and non-cancer tissue datasets,
VirTues demonstrates strong generalization capabilities without task-specific
fine-tuning, enabling cross-study analysis and novel marker integration. As a
generalist model, VirTues outperforms existing approaches across clinical
diagnostics, biological discovery and patient case retrieval tasks, while
providing insights into tissue function and disease mechanisms.

ÊëòË¶ÅÔºöÁ©∫ÈñìËõãÁôΩË≥™ÁµÑÂ≠∏ÊäÄË°ìÈÄèÈÅéÂêåÊôÇÂàÜÊûêÂ§öÂÄãÂàÜÂ≠êÊ®ôË®òÂèäÂÖ∂Á©∫ÈñìÁµÑÁπîÔºåËΩâËÆä‰∫ÜÊàëÂÄëÂ∞çË§áÈõúÁµÑÁπîÁµêÊßãÁöÑÁêÜËß£„ÄÇÈÄô‰∫õÊï∏ÊìöÁöÑÈ´òÁ∂≠Â∫¶„ÄÅÂØ¶È©ó‰∏≠‰∏çÂêåÁöÑÊ®ôË®òÁµÑÂêàÂíåÁï∞Ë≥™ÁöÑÁ†îÁ©∂Ë®≠Ë®àÔºåÂ∞çË®àÁÆóÂàÜÊûêÊßãÊàê‰∫ÜÁç®ÁâπÁöÑÊåëÊà∞„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ËôõÊì¨ÁµÑÁπî (VirTues)Ôºå‰∏ÄÂÄãÈÅ©Áî®ÊñºÂàÜÂ≠ê„ÄÅÁ¥∞ËÉûÂíåÁµÑÁπîÂ±§Á¥öÁöÑÁîüÁâ©ÁµÑÁπîÂü∫Á§éÊ®°ÂûãÊû∂Êßã„ÄÇVirTues Âú®TransformerÊû∂ÊßãË®≠Ë®à‰∏≠ÂºïÈÄ≤ÂâµÊñ∞ÔºåÂåÖÊã¨‰∏ÄÁ®ÆÊñ∞ÁöÑÊ®ôË®òÂåñÊû∂ÊßãÔºåÂÆÉÊçïÊçâÁ©∫ÈñìÂíåÊ®ôË®òÁ∂≠Â∫¶Ôºå‰ª•ÂèäÂú®Á∂≠ÊåÅÂèØËß£ÈáãÊÄßÁöÑÂêåÊôÇÊì¥Â±ïÂà∞È´òÁ∂≠Â∫¶Â§öÈáçÊï∏ÊìöÁöÑÊ≥®ÊÑèÂäõÊ©üÂà∂„ÄÇVirTues Âú®Â§öÊ®£ÂåñÁöÑÁôåÁóáÂíåÈùûÁôåÁóáÁµÑÁπîÊï∏ÊìöÈõÜ‰∏äË®ìÁ∑¥ÔºåÂ±ïÁ§∫‰∫ÜÂº∑Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºåÁÑ°ÈúÄÁâπÂÆö‰ªªÂãôÂæÆË™øÔºåÂæûËÄåÂØ¶ÁèæË∑®Á†îÁ©∂ÂàÜÊûêÂíåÊñ∞ÁöÑÊ®ôË®òÊï¥Âêà„ÄÇ‰ΩúÁÇ∫‰∏ÄÂÄãÈÄöÊâçÊ®°ÂûãÔºåVirTues Âú®Ëá®Â∫äË®∫Êñ∑„ÄÅÁîüÁâ©ÁôºÁèæÂíåÊÇ£ËÄÖÁóÖ‰æãÊ™¢Á¥¢‰ªªÂãô‰∏≠ÂÑ™ÊñºÁèæÊúâÊñπÊ≥ïÔºåÂêåÊôÇÊèê‰æõ‰∫ÜÁµÑÁπîÂäüËÉΩÂíåÁñæÁóÖÊ©üÂà∂ÁöÑË¶ãËß£„ÄÇ

##### **DiffuSETS: 12-lead ECG Generation Conditioned on Clinical Text Reports and Patient-Specific Information**
2501.05932v1 by Yongfan Lai, Jiabo Chen, Deyun Zhang, Yue Wang, Shijia Geng, Hongyan Li, Shenda Hong

Heart disease remains a significant threat to human health. As a non-invasive
diagnostic tool, the electrocardiogram (ECG) is one of the most widely used
methods for cardiac screening. However, the scarcity of high-quality ECG data,
driven by privacy concerns and limited medical resources, creates a pressing
need for effective ECG signal generation. Existing approaches for generating
ECG signals typically rely on small training datasets, lack comprehensive
evaluation frameworks, and overlook potential applications beyond data
augmentation. To address these challenges, we propose DiffuSETS, a novel
framework capable of generating ECG signals with high semantic alignment and
fidelity. DiffuSETS accepts various modalities of clinical text reports and
patient-specific information as inputs, enabling the creation of clinically
meaningful ECG signals. Additionally, to address the lack of standardized
evaluation in ECG generation, we introduce a comprehensive benchmarking
methodology to assess the effectiveness of generative models in this domain.
Our model achieve excellent results in tests, proving its superiority in the
task of ECG generation. Furthermore, we showcase its potential to mitigate data
scarcity while exploring novel applications in cardiology education and medical
knowledge discovery, highlighting the broader impact of our work.

ÊëòË¶ÅÔºöÂøÉËÑèÁóÖ‰ªçÁÑ∂ÊòØ‰∫∫Á±ªÂÅ•Â∫∑ÁöÑ‰∏ÄÂ§ßÂ®ÅËÉÅ„ÄÇ‰Ωú‰∏∫‰∏ÄÁßçÈùû‰æµÂÖ•ÊÄßËØäÊñ≠Â∑•ÂÖ∑ÔºåÂøÉÁîµÂõæ (ECG) ÊòØÂøÉËÑèÁ≠õÊü•ÊúÄÂπøÊ≥õ‰ΩøÁî®ÁöÑÊñπÊ≥ï‰πã‰∏Ä„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÈöêÁßÅÈóÆÈ¢òÂíåÂåªÁñóËµÑÊ∫êÊúâÈôêÔºåÈ´òË¥®Èáè ECG Êï∞ÊçÆÁöÑÁ®ÄÁº∫ÂØπÊúâÊïàÁöÑ ECG ‰ø°Âè∑ÁîüÊàêÊèêÂá∫‰∫ÜËø´ÂàáÈúÄÊ±Ç„ÄÇÁé∞ÊúâÁî®‰∫éÁîüÊàê ECG ‰ø°Âè∑ÁöÑÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂ∞èÂûãËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÁº∫‰πèÂÖ®Èù¢ÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÂπ∂‰∏îÂøΩËßÜ‰∫ÜÊï∞ÊçÆÂ¢ûÂº∫‰πãÂ§ñÁöÑÊΩúÂú®Â∫îÁî®„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü DiffuSETSÔºåËøôÊòØ‰∏Ä‰∏™ËÉΩÂ§üÁîüÊàêÂÖ∑ÊúâÈ´òÂ∫¶ËØ≠‰πâÂØπÈΩêÂíå‰øùÁúüÂ∫¶ÁöÑ ECG ‰ø°Âè∑ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇDiffuSETS Êé•ÂèóÂêÑÁßçÂΩ¢ÂºèÁöÑ‰∏¥Â∫äÊñáÊú¨Êä•ÂëäÂíåÊÇ£ËÄÖÁâπÂÆö‰ø°ÊÅØ‰Ωú‰∏∫ËæìÂÖ•Ôºå‰ªéËÄåËÉΩÂ§üÂàõÂª∫ÂÖ∑Êúâ‰∏¥Â∫äÊÑè‰πâÁöÑ ECG ‰ø°Âè∑„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜËß£ÂÜ≥ ECG ÁîüÊàê‰∏≠Áº∫‰πèÊ†áÂáÜÂåñËØÑ‰º∞ÁöÑÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØïÊñπÊ≥ïÊù•ËØÑ‰º∞ÁîüÊàêÊ®°ÂûãÂú®Ê≠§È¢ÜÂüüÁöÑÊúâÊïàÊÄß„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®ÊµãËØï‰∏≠ÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊàêÊûúÔºåËØÅÊòé‰∫ÜÂÖ∂Âú® ECG ÁîüÊàê‰ªªÂä°‰∏≠ÁöÑ‰ºòË∂äÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂÖ∂Âú®ÁºìËß£Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÂêåÊó∂Âú®ÂøÉËÑèÁóÖÂ≠¶ÊïôËÇ≤ÂíåÂåªÂ≠¶Áü•ËØÜÂèëÁé∞‰∏≠Êé¢Á¥¢Êñ∞Â∫îÁî®ÁöÑÊΩúÂäõÔºåÁ™ÅÂá∫‰∫ÜÊàë‰ª¨Â∑•‰ΩúÁöÑÊõ¥ÂπøÊ≥õÂΩ±Âìç„ÄÇ

##### **AI-Driven Diabetic Retinopathy Screening: Multicentric Validation of AIDRSS in India**
2501.05826v2 by Amit Kr Dey, Pradeep Walia, Girish Somvanshi, Abrar Ali, Sagarnil Das, Pallabi Paul, Minakhi Ghosh

Purpose: Diabetic retinopathy (DR) is a major cause of vision loss,
particularly in India, where access to retina specialists is limited in rural
areas. This study aims to evaluate the Artificial Intelligence-based Diabetic
Retinopathy Screening System (AIDRSS) for DR detection and prevalence
assessment, addressing the growing need for scalable, automated screening
solutions in resource-limited settings.
  Approach: A multicentric, cross-sectional study was conducted in Kolkata,
India, involving 5,029 participants and 10,058 macula-centric retinal fundus
images. The AIDRSS employed a deep learning algorithm with 50 million trainable
parameters, integrated with Contrast Limited Adaptive Histogram Equalization
(CLAHE) preprocessing for enhanced image quality. DR was graded using the
International Clinical Diabetic Retinopathy (ICDR) Scale, categorizing disease
into five stages (DR0 to DR4). Statistical metrics including sensitivity,
specificity, and prevalence rates were evaluated against expert retina
specialist assessments.
  Results: The prevalence of DR in the general population was 13.7%, rising to
38.2% among individuals with elevated random blood glucose levels. The AIDRSS
achieved an overall sensitivity of 92%, specificity of 88%, and 100%
sensitivity for detecting referable DR (DR3 and DR4). These results demonstrate
the system's robust performance in accurately identifying and grading DR in a
diverse population.
  Conclusions: AIDRSS provides a reliable, scalable solution for early DR
detection in resource-constrained environments. Its integration of advanced AI
techniques ensures high diagnostic accuracy, with potential to significantly
reduce the burden of diabetes-related vision loss in underserved regions.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÁöÑÔºöÁ≥ñÂ∞øÁóÖËßÜÁΩëËÜúÁóÖÂèò (DR) ÊòØËßÜÂäõ‰∏ßÂ§±ÁöÑ‰∏ªË¶ÅÂéüÂõ†ÔºåÁâπÂà´ÊòØÂú®Âç∞Â∫¶ÔºåÈÇ£ÈáåÁöÑÂÜúÊùëÂú∞Âå∫ÁúºÁßë‰∏ìÂÆ∂Êï∞ÈáèÊúâÈôê„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ËØÑ‰º∞Âü∫‰∫é‰∫∫Â∑•Êô∫ËÉΩÁöÑÁ≥ñÂ∞øÁóÖËßÜÁΩëËÜúÁóÖÂèòÁ≠õÊü•Á≥ªÁªü (AIDRSS) ÁöÑ DR Ê£ÄÊµãÂíåÊµÅË°åÊÉÖÂÜµËØÑ‰º∞Ôºå‰ª•Êª°Ë∂≥ËµÑÊ∫êÊúâÈôêÁöÑÁéØÂ¢É‰∏≠ÂØπÂèØÊâ©Â±ïËá™Âä®ÂåñÁ≠õÊü•Ëß£ÂÜ≥ÊñπÊ°à‰∏çÊñ≠Â¢ûÈïøÁöÑÈúÄÊ±Ç„ÄÇ
ÊñπÊ≥ïÔºöÂú®Âç∞Â∫¶Âä†Â∞îÂêÑÁ≠îËøõË°å‰∫Ü‰∏ÄÈ°πÂ§ö‰∏≠ÂøÉÊ®™Êñ≠Èù¢Á†îÁ©∂ÔºåÊ∂âÂèä 5,029 ÂêçÂèÇ‰∏éËÄÖÂíå 10,058 Âº†ÈªÑÊñë‰∏≠ÂøÉËßÜÁΩëËÜúÁúºÂ∫ïÂõæÂÉè„ÄÇAIDRSS ÈááÁî®‰∫Ü‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†ÁÆóÊ≥ïÔºåÂÖ∑Êúâ 5000 ‰∏á‰∏™ÂèØËÆ≠ÁªÉÂèÇÊï∞ÔºåÂπ∂ÈõÜÊàê‰∫ÜÂØπÊØîÂ∫¶ÈôêÂà∂Ëá™ÈÄÇÂ∫îÁõ¥ÊñπÂõæÂùáË°°Âåñ (CLAHE) È¢ÑÂ§ÑÁêÜÔºå‰ª•ÊèêÈ´òÂõæÂÉèË¥®Èáè„ÄÇDR ‰ΩøÁî®ÂõΩÈôÖ‰∏¥Â∫äÁ≥ñÂ∞øÁóÖËßÜÁΩëËÜúÁóÖÂèò (ICDR) ÈáèË°®ËøõË°åÂàÜÁ∫ßÔºåÂ∞ÜÁñæÁóÖÂàÜ‰∏∫‰∫î‰∏™Èò∂ÊÆµÔºàDR0 Ëá≥ DR4Ôºâ„ÄÇÈíàÂØπ‰∏ìÂÆ∂ËßÜÁΩëËÜú‰∏ìÂÆ∂ËØÑ‰º∞ÔºåËØÑ‰º∞‰∫ÜÂåÖÊã¨ÊïèÊÑüÊÄß„ÄÅÁâπÂºÇÊÄßÂíåÊÇ£ÁóÖÁéáÂú®ÂÜÖÁöÑÁªüËÆ°ÊåáÊ†á„ÄÇ
ÁªìÊûúÔºö‰∏ÄËà¨‰∫∫Áæ§‰∏≠ DR ÁöÑÊÇ£ÁóÖÁéá‰∏∫ 13.7%ÔºåÂú®ÈöèÊú∫Ë°ÄÁ≥ñÊ∞¥Âπ≥ÂçáÈ´òÁöÑ‰∏™‰Ωì‰∏≠‰∏äÂçáËá≥ 38.2%„ÄÇAIDRSS ÁöÑÊÄª‰ΩìÊïèÊÑüÊÄßËææÂà∞ 92%ÔºåÁâπÂºÇÊÄßËææÂà∞ 88%ÔºåÊ£ÄÊµãÂèØËΩ¨ËØä DRÔºàDR3 Âíå DR4ÔºâÁöÑÊïèÊÑüÊÄßËææÂà∞ 100%„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéËØ•Á≥ªÁªüÂú®ÂáÜÁ°ÆËØÜÂà´ÂíåÂàÜÁ∫ß‰∏çÂêå‰∫∫Áæ§‰∏≠ÁöÑ DR ÊñπÈù¢ÂÖ∑ÊúâÂº∫Â§ßÁöÑÊÄßËÉΩ„ÄÇ
ÁªìËÆ∫ÔºöAIDRSS ‰∏∫ËµÑÊ∫êÂèóÈôêÁéØÂ¢É‰∏≠ÁöÑÊó©Êúü DR Ê£ÄÊµãÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÈù†‰∏îÂèØÊâ©Â±ïÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÂÆÉÈõÜÊàê‰∫ÜÂÖàËøõÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÔºåÁ°Æ‰øù‰∫ÜËæÉÈ´òÁöÑËØäÊñ≠ÂáÜÁ°ÆÊÄßÔºåÊúâÂèØËÉΩÊòæËëóÂáèÂ∞ëÊúçÂä°‰∏çË∂≥Âú∞Âå∫‰∏éÁ≥ñÂ∞øÁóÖÁõ∏ÂÖ≥ÁöÑËßÜÂäõ‰∏ßÂ§±Ë¥üÊãÖ„ÄÇ</paragraph>

##### **Large Language Models for Bioinformatics**
2501.06271v1 by Wei Ruan, Yanjun Lyu, Jing Zhang, Jiazhang Cai, Peng Shu, Yang Ge, Yao Lu, Shang Gao, Yue Wang, Peilong Wang, Lin Zhao, Tao Wang, Yufang Liu, Luyang Fang, Ziyu Liu, Zhengliang Liu, Yiwei Li, Zihao Wu, Junhao Chen, Hanqi Jiang, Yi Pan, Zhenyuan Yang, Jingyuan Chen, Shizhe Liang, Wei Zhang, Terry Ma, Yuan Dou, Jianli Zhang, Xinyu Gong, Qi Gan, Yusong Zou, Zebang Chen, Yuanxin Qian, Shuo Yu, Jin Lu, Kenan Song, Xianqiao Wang, Andrea Sikora, Gang Li, Xiang Li, Quanzheng Li, Yingfeng Wang, Lu Zhang, Yohannes Abate, Lifang He, Wenxuan Zhong, Rongjie Liu, Chao Huang, Wei Liu, Ye Shen, Ping Ma, Hongtu Zhu, Yajun Yan, Dajiang Zhu, Tianming Liu

With the rapid advancements in large language model (LLM) technology and the
emergence of bioinformatics-specific language models (BioLMs), there is a
growing need for a comprehensive analysis of the current landscape,
computational characteristics, and diverse applications. This survey aims to
address this need by providing a thorough review of BioLMs, focusing on their
evolution, classification, and distinguishing features, alongside a detailed
examination of training methodologies, datasets, and evaluation frameworks. We
explore the wide-ranging applications of BioLMs in critical areas such as
disease diagnosis, drug discovery, and vaccine development, highlighting their
impact and transformative potential in bioinformatics. We identify key
challenges and limitations inherent in BioLMs, including data privacy and
security concerns, interpretability issues, biases in training data and model
outputs, and domain adaptation complexities. Finally, we highlight emerging
trends and future directions, offering valuable insights to guide researchers
and clinicians toward advancing BioLMs for increasingly sophisticated
biological and clinical applications.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÊäÄË°ìÁöÑÂø´ÈÄüÈÄ≤Â±ïÂíåÁîüÁâ©Ë≥áË®äÂ≠∏ÁâπÂÆöË™ûË®ÄÊ®°ÂûãÔºàBioLMÔºâÁöÑÂá∫ÁèæÔºåÂ∞çÊñºÁï∂ÂâçÊÉÖÂã¢„ÄÅË®àÁÆóÁâπÂæµÂíåÂ§öÂÖÉÊáâÁî®ÈÄ≤Ë°åÂÖ®Èù¢ÂàÜÊûêÁöÑÈúÄÊ±ÇÊó•ÁõäÂ¢ûÂä†„ÄÇÊú¨Ë™øÊü•Êó®Âú®ÈÄèÈÅéÊèê‰æõÂ∞ç BioLM ÁöÑÂÖ®Èù¢Ê™¢Ë¶ñÔºåËëóÈáçÊñºÂÖ∂ÊºîÈÄ≤„ÄÅÂàÜÈ°ûÂíåÂçÄÂà•ÁâπÂæµÔºå‰ª•ÂèäÂ∞çË®ìÁ∑¥ÊñπÊ≥ï„ÄÅË≥áÊñôÈõÜÂíåË©ï‰º∞Êû∂ÊßãÁöÑË©≥Á¥∞Êé¢Ë®éÔºå‰æÜÊªøË∂≥Ê≠§ÈúÄÊ±Ç„ÄÇÊàëÂÄëÊé¢Ë®é BioLM Âú®ÁñæÁóÖË®∫Êñ∑„ÄÅËó•Áâ©ÁôºÁèæÂíåÁñ´ËãóÈñãÁôºÁ≠âÈóúÈçµÈ†òÂüüÁöÑÂª£Ê≥õÊáâÁî®ÔºåÂº∑Ë™øÂÖ∂Âú®ÁîüÁâ©Ë≥áË®äÂ≠∏‰∏≠ÁöÑÂΩ±ÈüøÂíåËΩâÂûãÊΩõÂäõ„ÄÇÊàëÂÄëÊâæÂá∫ BioLM Âõ∫ÊúâÁöÑÈóúÈçµÊåëÊà∞ÂíåÈôêÂà∂ÔºåÂåÖÊã¨Ë≥áÊñôÈö±ÁßÅÂíåÂÆâÂÖ®ÊÄßÂïèÈ°å„ÄÅÂèØËß£ÈáãÊÄßÂïèÈ°å„ÄÅË®ìÁ∑¥Ë≥áÊñôÂíåÊ®°ÂûãËº∏Âá∫ÁöÑÂÅèÂ∑ÆÔºå‰ª•ÂèäÈ†òÂüüÈÅ©ÊáâÁöÑË§áÈõúÊÄß„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈáçÈªû‰ªãÁ¥πÊñ∞ËààË∂®Âã¢ÂíåÊú™‰æÜÊñπÂêëÔºåÊèê‰æõÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰ª•ÊåáÂ∞éÁ†îÁ©∂‰∫∫Âì°ÂíåËá®Â∫äÈÜ´ÁîüÂ∞á BioLM ÊáâÁî®ÊñºÊó•ÁõäË§áÈõúÁöÑÁîüÁâ©ÂíåËá®Â∫äÊáâÁî®„ÄÇ

##### **From Simple to Complex Skills: The Case of In-Hand Object Reorientation**
2501.05439v1 by Haozhi Qi, Brent Yi, Mike Lambeta, Yi Ma, Roberto Calandra, Jitendra Malik

Learning policies in simulation and transferring them to the real world has
become a promising approach in dexterous manipulation. However, bridging the
sim-to-real gap for each new task requires substantial human effort, such as
careful reward engineering, hyperparameter tuning, and system identification.
In this work, we present a system that leverages low-level skills to address
these challenges for more complex tasks. Specifically, we introduce a
hierarchical policy for in-hand object reorientation based on previously
acquired rotation skills. This hierarchical policy learns to select which
low-level skill to execute based on feedback from both the environment and the
low-level skill policies themselves. Compared to learning from scratch, the
hierarchical policy is more robust to out-of-distribution changes and transfers
easily from simulation to real-world environments. Additionally, we propose a
generalizable object pose estimator that uses proprioceptive information,
low-level skill predictions, and control errors as inputs to estimate the
object pose over time. We demonstrate that our system can reorient objects,
including symmetrical and textureless ones, to a desired pose.

ÊëòË¶ÅÔºöÂú®Ê®°Êì¨‰∏≠Â≠∏ÁøíÁ≠ñÁï•‰∏¶Â∞áÂÖ∂ËΩâÁßªÂà∞ÁèæÂØ¶‰∏ñÁïåÂ∑≤ÊàêÁÇ∫ÈùàÂ∑ßÊìç‰Ωú‰∏≠‰∏ÄÁ®ÆÊúâÂâçÊôØÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÊØèÈ†ÖÊñ∞‰ªªÂãô‰æÜË™™ÔºåÂΩåÂêàÊ®°Êì¨Âà∞ÁèæÂØ¶ÁöÑÂ∑ÆË∑ùÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫ÂäõÔºå‰æãÂ¶Ç‰ªîÁ¥∞ÁöÑÁçéÂãµÂ∑•Á®ã„ÄÅË∂ÖÂèÉÊï∏Ë™øÊï¥ÂíåÁ≥ªÁµ±Ë≠òÂà•„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂà©Áî®‰ΩéÂ±§ÊäÄËÉΩ‰æÜÊáâÂ∞çÊõ¥Ë§áÈõú‰ªªÂãôÁöÑÊåëÊà∞ÁöÑÁ≥ªÁµ±„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÂÖàÂâçÁç≤ÂæóÁöÑÊóãËΩâÊäÄËÉΩÁöÑÊâã‰∏≠Áâ©È´îÈáçÊñ∞ÂÆöÂêëÁöÑÂàÜÂ±§Á≠ñÁï•„ÄÇÈÄôÁ®ÆÂàÜÂ±§Á≠ñÁï•Â≠∏ÁøíÊ†πÊìöÁí∞Â¢ÉÂíå‰ΩéÂ±§ÊäÄËÉΩÁ≠ñÁï•Êú¨Ë∫´ÁöÑÂõûÈ•ãÈÅ∏ÊìáÂü∑Ë°åÂì™Á®Æ‰ΩéÂ±§ÊäÄËÉΩ„ÄÇËàáÂæûÈ†≠ÈñãÂßãÂ≠∏ÁøíÁõ∏ÊØîÔºåÂàÜÂ±§Á≠ñÁï•Â∞çÂàÜ‰ΩàÂ§ñËÆäÂåñÊõ¥Âº∑ÂÅ•Ôºå‰∏¶‰∏îÂèØ‰ª•ËºïÈ¨ÜÂú∞ÂæûÊ®°Êì¨ËΩâÁßªÂà∞ÁèæÂØ¶‰∏ñÁïåÁí∞Â¢É„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØÊ≥õÂåñÁöÑÁâ©È´îÂßøÂã¢‰º∞Ë®àÂô®ÔºåÂÆÉ‰ΩøÁî® proprioceptive ‰ø°ÊÅØ„ÄÅ‰ΩéÂ±§ÊäÄËÉΩÈ†êÊ∏¨ÂíåÊéßÂà∂Ë™§Â∑Æ‰ΩúÁÇ∫Ëº∏ÂÖ•‰æÜ‰º∞Ë®àÁâ©È´îÂßøÂã¢„ÄÇÊàëÂÄëË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÁ≥ªÁµ±ÂèØ‰ª•Â∞áÁâ©È´îÔºàÂåÖÊã¨Â∞çÁ®±ÂíåÁÑ°Á¥ãÁêÜÁöÑÁâ©È´îÔºâÈáçÊñ∞ÂÆöÂêëÂà∞ÊâÄÈúÄÁöÑÂßøÂã¢„ÄÇ

##### **Strategy Masking: A Method for Guardrails in Value-based Reinforcement Learning Agents**
2501.05501v1 by Jonathan Keane, Sam Keyser, Jeremy Kedziora

The use of reward functions to structure AI learning and decision making is
core to the current reinforcement learning paradigm; however, without careful
design of reward functions, agents can learn to solve problems in ways that may
be considered ``undesirable" or ``unethical. Without thorough understanding of
the incentives a reward function creates, it can be difficult to impose
principled yet general control mechanisms over its behavior. In this paper, we
study methods for constructing guardrails for AI agents that use reward
functions to learn decision making. We introduce a novel approach, which we
call strategy masking, to explicitly learn and then suppress undesirable AI
agent behavior. We apply our method to study lying in AI agents and show that
strategy masking can effectively modify agent behavior by suppressing, or
actively penalizing, the reward dimension for lying such that agents act more
honestly while not compromising their ability to perform effectively.

ÊëòË¶ÅÔºö‰ΩøÁî®ÁçéÂãµÂáΩÊï∏‰æÜÂª∫Êßã AI Â≠∏ÁøíÂíåÊ±∫Á≠ñÂà∂ÂÆöÊòØÁï∂ÂâçÂº∑ÂåñÂ≠∏ÁøíÁØÑ‰æãÁöÑÊ†∏ÂøÉÔºõÁÑ∂ËÄåÔºåËã•ÁçéÂãµÂáΩÊï∏Ë®≠Ë®à‰∏çÂë®Ôºå‰ª£ÁêÜÁ®ãÂºèÂèØËÉΩÊúÉÂ≠∏ÊúÉ‰ª•„Äå‰∏çÂèØÂèñ„ÄçÊàñ„Äå‰∏çÈÅìÂæ∑„ÄçÁöÑÊñπÂºèËß£Ê±∫ÂïèÈ°å„ÄÇËã•‰∏çÂæπÂ∫ï‰∫ÜËß£ÁçéÂãµÂáΩÊï∏ÊâÄÂâµÈÄ†ÁöÑË™òÂõ†ÔºåÂ∞±Èõ£‰ª•Â∞çÂÖ∂Ë°åÁÇ∫ÊñΩÂä†ÊúâÂéüÂâá‰∏îÈÄöÁî®ÁöÑÊéßÂà∂Ê©üÂà∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÂª∫ÊßãÈò≤Ë≠∑Êé™ÊñΩÁöÑÊñπÊ≥ïÔºå‰ª•‰æõ‰ΩøÁî®ÁçéÂãµÂáΩÊï∏Â≠∏ÁøíÊ±∫Á≠ñÂà∂ÂÆöÁöÑ AI ‰ª£ÁêÜÁ®ãÂºè‰ΩøÁî®„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÁ®±ÁÇ∫Á≠ñÁï•ÈÅÆÁΩ©ÔºåÁî®ÊñºÊòéÁ¢∫Â≠∏Áøí‰∏¶ÊäëÂà∂‰∏çËâØÁöÑ AI ‰ª£ÁêÜÁ®ãÂºèË°åÁÇ∫„ÄÇÊàëÂÄëÂ∞áÊñπÊ≥ïÊáâÁî®ÊñºÁ†îÁ©∂ AI ‰ª£ÁêÜÁ®ãÂºè‰∏≠ÁöÑË™™Ë¨äË°åÁÇ∫Ôºå‰∏¶Ë≠âÊòéÁ≠ñÁï•ÈÅÆÁΩ©ÂèØ‰ª•ÊúâÊïà‰øÆÊîπ‰ª£ÁêÜÁ®ãÂºèË°åÁÇ∫ÔºåÊñπÊ≥ïÊòØÊäëÂà∂Êàñ‰∏ªÂãïÊá≤ÁΩ∞Ë™™Ë¨äÁöÑÁçéÂãµÁ∂≠Â∫¶ÔºåËÆì‰ª£ÁêÜÁ®ãÂºèÊõ¥Ë™†ÂØ¶ÔºåÂêåÊôÇ‰∏çÊêçÂÆ≥ÂÖ∂ÊúâÊïàÂü∑Ë°å‰ªªÂãôÁöÑËÉΩÂäõ„ÄÇ

##### **Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charit√©, and Aignostics**
2501.05409v2 by Maximilian Alber, Stephan Tietz, Jonas Dippel, Timo Milbich, Timoth√©e Lesort, Panos Korfiatis, Moritz Kr√ºgener, Beatriz Perez Cancer, Neelay Shah, Alexander M√∂llers, Philipp Seegerer, Alexandra Carpen-Amarie, Kai Standvoss, Gabriel Dernbach, Edwin de Jong, Simon Schallenberg, Andreas Kunft, Helmut Hoffer von Ankershoffen, Gavin Schaeferle, Patrick Duffy, Matt Redlon, Philipp Jurmeister, David Horst, Lukas Ruff, Klaus-Robert M√ºller, Frederick Klauschen, Andrew Norgan

Recent advances in digital pathology have demonstrated the effectiveness of
foundation models across diverse applications. In this report, we present
Atlas, a novel vision foundation model based on the RudolfV approach. Our model
was trained on a dataset comprising 1.2 million histopathology whole slide
images, collected from two medical institutions: Mayo Clinic and Charit\'e -
Universt\"atsmedizin Berlin. Comprehensive evaluations show that Atlas achieves
state-of-the-art performance across twenty-one public benchmark datasets, even
though it is neither the largest model by parameter count nor by training
dataset size.

ÊëòË¶ÅÔºöÊúÄËøëÂú®Êï∏‰ΩçÁóÖÁêÜÂ≠∏ÁöÑÈÄ≤Â±ïÂ∑≤Â±ïÁèæÂü∫Á§éÊ®°ÂûãÂú®ÂêÑÁ®ÆÊáâÁî®‰∏äÁöÑÊúâÊïàÊÄß„ÄÇÂú®Ê≠§Â†±Âëä‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ AtlasÔºå‰∏ÄÁ®ÆÂü∫Êñº RudolfV ÊñπÊ≥ïÁöÑÊñ∞Á©éË¶ñË¶∫Âü∫Á§éÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãË®ìÁ∑¥Êñº‰∏ÄÂÄãÂåÖÂê´ 120 Ëê¨ÂºµÁµÑÁπîÁóÖÁêÜÂÖ®ÁéªÁâáÂΩ±ÂÉèÁöÑË≥áÊñôÈõÜÔºåÈÄô‰∫õÂΩ±ÂÉèÊî∂ÈõÜËá™ÂÖ©ÂÄãÈÜ´ÁôÇÊ©üÊßãÔºöÊ¢ÖÁ¥ÑË®∫ÊâÄÂíåÊüèÊûóÂ§èÈáåÁâπÂ§ßÂ≠∏ÈÜ´Â≠∏‰∏≠ÂøÉ„ÄÇÂÖ®Èù¢ÁöÑË©ï‰º∞È°ØÁ§∫ÔºåAtlas Âú® 21 ÂÄãÂÖ¨ÈñãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂç≥‰ΩøÂÆÉÊó¢‰∏çÊòØÂèÉÊï∏Êï∏ÈáèÊúÄÂ§ßÁöÑÊ®°ÂûãÔºå‰πü‰∏çÊòØË®ìÁ∑¥Ë≥áÊñôÈõÜË¶èÊ®°ÊúÄÂ§ßÁöÑÊ®°Âûã„ÄÇ

##### **An Algorithmic Approach for Causal Health Equity: A Look at Race Differentials in Intensive Care Unit (ICU) Outcomes**
2501.05197v1 by Drago Plecko, Paul Secombe, Andrea Clarke, Amelia Fiske, Samarra Toby, Donisha Duff, David Pilcher, Leo Anthony Celi, Rinaldo Bellomo, Elias Bareinboim

The new era of large-scale data collection and analysis presents an
opportunity for diagnosing and understanding the causes of health inequities.
In this study, we describe a framework for systematically analyzing health
disparities using causal inference. The framework is illustrated by
investigating racial and ethnic disparities in intensive care unit (ICU)
outcome between majority and minority groups in Australia (Indigenous vs.
Non-Indigenous) and the United States (African-American vs. White). We
demonstrate that commonly used statistical measures for quantifying inequity
are insufficient, and focus on attributing the observed disparity to the causal
mechanisms that generate it. We find that minority patients are younger at
admission, have worse chronic health, are more likely to be admitted for urgent
and non-elective reasons, and have higher illness severity. At the same time,
however, we find a protective direct effect of belonging to a minority group,
with minority patients showing improved survival compared to their majority
counterparts, with all other variables kept equal. We demonstrate that this
protective effect is related to the increased probability of being admitted to
ICU, with minority patients having an increased risk of ICU admission. We also
find that minority patients, while showing improved survival, are more likely
to be readmitted to ICU. Thus, due to worse access to primary health care,
minority patients are more likely to end up in ICU for preventable conditions,
causing a reduction in the mortality rates and creating an effect that appears
to be protective. Since the baseline risk of ICU admission may serve as proxy
for lack of access to primary care, we developed the Indigenous Intensive Care
Equity (IICE) Radar, a monitoring system for tracking the over-utilization of
ICU resources by the Indigenous population of Australia across geographical
areas.

ÊëòË¶ÅÔºöÂ§ßÂûãË≥áÊñôÊî∂ÈõÜÂíåÂàÜÊûêÁöÑÊñ∞ÊôÇ‰ª£ÔºåÊèê‰æõ‰∫ÜË®∫Êñ∑Âíå‰∫ÜËß£ÂÅ•Â∫∑‰∏çÂπ≥Á≠âÊàêÂõ†ÁöÑÊ©üÊúÉ„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèèËø∞‰∫Ü‰∏ÄÂÄã‰ΩøÁî®Âõ†ÊûúÊé®Ë´ñÁ≥ªÁµ±ÂàÜÊûêÂÅ•Â∫∑Â∑ÆË∑ùÁöÑÊû∂Êßã„ÄÇÈÄôÂÄãÊû∂ÊßãÈÄèÈÅéË™øÊü•Êæ≥Ê¥≤ÔºàÂéü‰ΩèÊ∞ëÂ∞çÈùûÂéü‰ΩèÊ∞ëÔºâÂíåÁæéÂúãÔºàÈùûË£îÁæéÂúã‰∫∫Â∞çÁôΩ‰∫∫Ôºâ‰∏≠ÔºåÈáçÁóáÂä†Ë≠∑ÁóÖÊàøÔºàICUÔºâÁµêÊûúÂú®Á®ÆÊóèÂíåÊóèÁæ§‰∏äÁöÑÂ∑ÆÁï∞‰æÜÂä†‰ª•Ë™™Êòé„ÄÇÊàëÂÄëË≠âÊòé‰∫ÜÈÄöÂ∏∏Áî®ÊñºÈáèÂåñ‰∏çÂπ≥Á≠âÁöÑÁµ±Ë®àÊ∏¨ÈáèÊòØ‰∏çÂ§†ÁöÑÔºå‰∏¶Â∞àÊ≥®ÊñºÂ∞áËßÄÂØüÂà∞ÁöÑÂ∑ÆÁï∞Ê≠∏Âõ†ÊñºÁî¢ÁîüÂÆÉÁöÑÂõ†ÊûúÊ©üÂà∂„ÄÇÊàëÂÄëÁôºÁèæÔºåÂ∞ëÊï∏ÊóèË£îÊÇ£ËÄÖÂú®ÂÖ•Èô¢ÊôÇËºÉÂπ¥ËºïÔºåÊÖ¢ÊÄßÂÅ•Â∫∑ÁãÄÊ≥ÅËºÉÂ∑ÆÔºåÊõ¥ÊúâÂèØËÉΩÂõ†Á∑äÊÄ•ÂíåÈùûÈÅ∏ÊìáÊÄßÂéüÂõ†ËÄåÂÖ•Èô¢Ôºå‰∏îÁñæÁóÖÂö¥ÈáçÁ®ãÂ∫¶ËºÉÈ´ò„ÄÇÁÑ∂ËÄåÔºåÂêåÊôÇÊàëÂÄëÁôºÁèæÂ±¨ÊñºÂ∞ëÊï∏ÊóèË£îÁæ§È´îÂÖ∑Êúâ‰øùË≠∑ÊÄßÁöÑÁõ¥Êé•ÂΩ±ÈüøÔºåËàáÂ§öÊï∏ÊóèË£îÁöÑÂ∞çÁÖßÁµÑÁõ∏ÊØîÔºåÂ∞ëÊï∏ÊóèË£îÊÇ£ËÄÖÂú®ÂÖ∂‰ªñÊâÄÊúâËÆäÊï∏‰øùÊåÅÁõ∏ÂêåÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ≠òÊ¥ªÁéáÊúâÊâÄÊîπÂñÑ„ÄÇÊàëÂÄëË≠âÊòéÈÄôÁ®Æ‰øùË≠∑ÊïàÊáâËàáË¢´ÈÄÅÈÄ≤ ICU ÁöÑÊ©üÁéáÂ¢ûÂä†ÊúâÈóúÔºåÂ∞ëÊï∏ÊóèË£îÊÇ£ËÄÖÁöÑ ICU ÂÖ•Èô¢È¢®Èö™Â¢ûÂä†„ÄÇÊàëÂÄë‰πüÁôºÁèæÔºåÂ∞ëÊï∏ÊóèË£îÊÇ£ËÄÖÈõñÁÑ∂Â≠òÊ¥ªÁéáÊúâÊâÄÊîπÂñÑÔºå‰ΩÜÊõ¥ÊúâÂèØËÉΩÂÜçÊ¨°ÂÖ•Èô¢Âà∞ ICU„ÄÇÂõ†Ê≠§ÔºåÁî±ÊñºËºÉÈõ£Áç≤ÂæóÂàùÁ¥öÈÜ´ÁôÇ‰øùÂÅ•ÔºåÂ∞ëÊï∏ÊóèË£îÊÇ£ËÄÖÊõ¥ÊúâÂèØËÉΩÂõ†ÂèØÈ†êÈò≤ÁöÑÁñæÁóÖËÄåÈÄ≤ÂÖ• ICUÔºåÂ∞éËá¥Ê≠ª‰∫°ÁéáÈôç‰Ωé‰∏¶Áî¢ÁîüÁúã‰ººÂÖ∑Êúâ‰øùË≠∑‰ΩúÁî®ÁöÑÊïàÊáâ„ÄÇÁî±Êñº ICU ÂÖ•Èô¢ÁöÑÂü∫Êú¨È¢®Èö™ÂèØËÉΩ‰ΩúÁÇ∫Áº∫‰πèÂàùÁ¥öÁÖßË≠∑ÁöÑÊåáÊ®ôÔºåÂõ†Ê≠§ÊàëÂÄëÈñãÁôº‰∫ÜÂéü‰ΩèÊ∞ëÈáçÁóáÁõ£Ë≠∑ÂÖ¨Âπ≥ÊÄßÔºàIICEÔºâÈõ∑ÈÅîÔºåÈÄôÊòØ‰∏ÄÂÄãÁõ£ÊéßÁ≥ªÁµ±ÔºåÁî®ÊñºËøΩËπ§Êæ≥Ê¥≤Âéü‰ΩèÊ∞ë‰∫∫Âè£Âú®‰∏çÂêåÂú∞ÁêÜÂçÄÂüüÈÅéÂ∫¶‰ΩøÁî® ICU Ë≥áÊ∫êÁöÑÊÉÖÊ≥Å„ÄÇ

##### **Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo Development Assessment**
2501.04958v1 by Lei Li, Xinglin Zhang, Jun Liang, Tao Chen

Deep learning models in medical imaging face dual challenges: domain shift,
where models perform poorly when deployed in settings different from their
training environment, and class imbalance, where certain disease conditions are
naturally underrepresented. We present Imbalance-Aware Domain Adaptation
(IADA), a novel framework that simultaneously tackles both challenges through
three key components: (1) adaptive feature learning with class-specific
attention mechanisms, (2) balanced domain alignment with dynamic weighting, and
(3) adaptive threshold optimization. Our theoretical analysis establishes
convergence guarantees and complexity bounds. Through extensive experiments on
embryo development assessment across four imaging modalities, IADA demonstrates
significant improvements over existing methods, achieving up to 25.19\% higher
accuracy while maintaining balanced performance across classes. In challenging
scenarios with low-quality imaging systems, IADA shows robust generalization
with AUC improvements of up to 12.56\%. These results demonstrate IADA's
potential for developing reliable and equitable medical imaging systems for
diverse clinical settings. The code is made public available at
\url{https://github.com/yinghemedical/imbalance-aware_domain_adaptation}

ÊëòË¶ÅÔºö<paragraph>ÈÜ´ÁôÇÂΩ±ÂÉè‰∏≠ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÈù¢Ëá®ÈõôÈáçÊåëÊà∞ÔºöÈ†òÂüüËΩâÁßªÔºåÊ®°ÂûãÂú®ËàáÂÖ∂Ë®ìÁ∑¥Áí∞Â¢É‰∏çÂêåÁöÑË®≠ÂÆö‰∏≠ÈÉ®ÁΩ≤ÊôÇË°®Áèæ‰∏ç‰Ω≥Ôºå‰ª•ÂèäÈ°ûÂà•‰∏çÂπ≥Ë°°ÔºåÊüê‰∫õÁñæÁóÖÁãÄÊ≥ÅÂú®Ëá™ÁÑ∂Áïå‰∏≠‰ª£Ë°®ÊÄß‰∏çË∂≥„ÄÇÊàëÂÄëÊèêÂá∫‰∏çÂπ≥Ë°°ÊÑüÁü•ÂüüÈÅ©Êáâ (IADA)ÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂ÔºåÈÄèÈÅé‰∏âÂÄãÈóúÈçµÁµÑÊàêÈÉ®ÂàÜÂêåÊôÇÊáâÂ∞çÈÄôÂÖ©ÂÄãÊåëÊà∞Ôºö(1) ÂÖ∑ÊúâÈ°ûÂà•ÁâπÂÆöÊ≥®ÊÑèÂäõÊ©üÂà∂ÁöÑËá™ÈÅ©ÊáâÁâπÂæµÂ≠∏ÁøíÔºå(2) ÂÖ∑ÊúâÂãïÊÖãÂä†Ê¨äÁöÑÂπ≥Ë°°ÂüüÂ∞çÈΩäÔºå‰ª•Âèä (3) Ëá™ÈÅ©ÊáâÈñæÂÄºÊúÄ‰Ω≥Âåñ„ÄÇÊàëÂÄëÁöÑÁêÜË´ñÂàÜÊûêÂª∫Á´ã‰∫ÜÊî∂ÊñÇ‰øùË≠âÂíåË§áÈõúÂ∫¶ÁïåÈôê„ÄÇÈÄèÈÅéÂ∞çÂõõÁ®ÆÂΩ±ÂÉèÊ®°ÂºèÁöÑËÉöËÉéÁôºËÇ≤Ë©ï‰º∞ÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºåIADA Ë≠âÊòé‰∫ÜÂ∞çÁèæÊúâÊñπÊ≥ïÁöÑÈ°ØËëóÊîπÈÄ≤ÔºåÂú®Á∂≠ÊåÅÈ°ûÂà•ÈñìÂπ≥Ë°°ÊÄßËÉΩÁöÑÂêåÊôÇÔºåÊ∫ñÁ¢∫Â∫¶ÊèêÈ´ò‰∫Ü 25.19%„ÄÇÂú®‰ΩéÂìÅË≥™ÂΩ±ÂÉèÁ≥ªÁµ±ÁöÑÊåëÊà∞ÊÄßÂ†¥ÊôØ‰∏≠ÔºåIADA ‰ª•È´òÈÅî 12.56% ÁöÑ AUC ÊîπÈÄ≤È°ØÁ§∫Âá∫Âº∑Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈÄô‰∫õÁµêÊûúË≠âÊòé‰∫Ü IADA Âú®ÁÇ∫‰∏çÂêåÁöÑËá®Â∫äË®≠ÂÆöÈñãÁôºÂèØÈù†‰∏îÂÖ¨Âπ≥ÁöÑÈÜ´ÁôÇÂΩ±ÂÉèÁ≥ªÁµ±ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº \url{https://github.com/yinghemedical/imbalance-aware_domain_adaptation}</paragraph>

##### **Quantifying Itch and its Impact on Sleep Using Machine Learning and Radio Signals**
2501.04896v1 by Michail Ouroutzoglou, Mingmin Zhao, Joshua Hellerstein, Hariharan Rahul, Asima Badic, Brian S. Kim, Dina Katabi

Chronic itch affects 13% of the US population, is highly debilitating, and
underlies many medical conditions. A major challenge in clinical care and new
therapeutics development is the lack of an objective measure for quantifying
itch, leading to reliance on subjective measures like patients' self-assessment
of itch severity. In this paper, we show that a home radio device paired with
artificial intelligence (AI) can concurrently capture scratching and evaluate
its impact on sleep quality by analyzing radio signals bouncing in the
environment. The device eliminates the need for wearable sensors or skin
contact, enabling monitoring of chronic itch over extended periods at home
without burdening patients or interfering with their skin condition. To
validate the technology, we conducted an observational clinical study of
chronic pruritus patients, monitored at home for one month using both the radio
device and an infrared camera. Comparing the output of the device to ground
truth data from the camera demonstrates its feasibility and accuracy (ROC AUC =
0.997, sensitivity = 0.825, specificity = 0.997). The results reveal a
significant correlation between scratching and low sleep quality, manifested as
a reduction in sleep efficiency (R = 0.6, p < 0.001) and an increase in sleep
latency (R = 0.68, p < 0.001). Our study underscores the potential of passive,
long-term, at-home monitoring of chronic scratching and its sleep implications,
offering a valuable tool for both clinical care of chronic itch patients and
pharmaceutical clinical trials.

ÊëòË¶ÅÔºöÊÖ¢ÊÄßÊêîÁô¢ÂΩ±ÈüøÁæéÂúã 13% ÁöÑ‰∫∫Âè£ÔºåÊúÉÂö¥ÈáçË°∞Âº±Ôºå‰∏îÊòØË®±Â§öÁñæÁóÖÁöÑÊ†πÊú¨ÂéüÂõ†„ÄÇËá®Â∫äË≠∑ÁêÜÂíåÊñ∞ÁôÇÊ≥ïÈñãÁôºÁöÑ‰∏ÄÂ§ßÊåëÊà∞ÊòØÁº∫‰πèÂÆ¢ËßÄÁöÑÊåáÊ®ô‰æÜÈáèÂåñÊêîÁô¢ÔºåÂ∞éËá¥‰æùË≥¥ÊñºÊÇ£ËÄÖËá™ÊàëË©ï‰º∞ÊêîÁô¢Âö¥ÈáçÁ®ãÂ∫¶Á≠â‰∏ªËßÄÊåáÊ®ô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏ÄÁ®ÆËàá‰∫∫Â∑•Êô∫ÊÖß (AI) ÈÖçÂ∞çÁöÑÂÆ∂Áî®ÁÑ°Á∑öÈõªË£ùÁΩÆÔºåÂèØÈÄèÈÅéÂàÜÊûêÂú®Áí∞Â¢É‰∏≠ÂΩàË∑≥ÁöÑÁÑ°Á∑öÈõªË®äËôüÔºåÂêåÊôÇÊì∑ÂèñÊäìÊíì‰∏¶Ë©ï‰º∞ÂÖ∂Â∞çÁù°Áú†ÂìÅË≥™ÁöÑÂΩ±Èüø„ÄÇÊ≠§Ë£ùÁΩÆÊ∂àÈô§‰∫ÜÂ∞çÁ©øÊà¥ÂºèÊÑüÊ∏¨Âô®ÊàñÁöÆËÜöÊé•Ëß∏ÁöÑÈúÄÊ±ÇÔºåËÆìÊÇ£ËÄÖÂú®ÂÆ∂‰∏≠Èï∑ÊôÇÈñìÁõ£ÊéßÊÖ¢ÊÄßÊêîÁô¢ÔºåËÄå‰∏çÊúÉÈÄ†ÊàêË≤†ÊìîÊàñÂπ≤ÊìæÂÖ∂ÁöÆËÜöÁãÄÊ≥Å„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÈÄôÈ†ÖÊäÄË°ìÔºåÊàëÂÄëÂ∞çÊÖ¢ÊÄßÊêîÁô¢ÁóáÊÇ£ËÄÖÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖËßÄÂØüÊÄßËá®Â∫äÁ†îÁ©∂Ôºå‰ΩøÁî®ÁÑ°Á∑öÈõªË£ùÁΩÆÂíåÁ¥ÖÂ§ñÁ∑öÊîùÂΩ±Ê©üÂú®ÂÆ∂‰∏≠Áõ£Êéß‰∏ÄÂÄãÊúà„ÄÇÂ∞áË£ùÁΩÆÁöÑËº∏Âá∫ËàáÊîùÂΩ±Ê©üÁöÑÁúüÂØ¶Êï∏ÊìöÈÄ≤Ë°åÊØîËºÉÔºåË≠âÊòé‰∫ÜÂÖ∂ÂèØË°åÊÄßÂíåÊ∫ñÁ¢∫ÊÄß (ROC AUC = 0.997ÔºåÈùàÊïèÂ∫¶ = 0.825ÔºåÁâπÁï∞Â∫¶ = 0.997)„ÄÇÁµêÊûúÈ°ØÁ§∫ÊäìÊíìËàáÁù°Áú†ÂìÅË≥™‰Ωé‰∏ã‰πãÈñìÂ≠òÂú®È°ØËëóÁõ∏ÈóúÊÄßÔºåË°®ÁèæÁÇ∫Áù°Áú†ÊïàÁéáÈôç‰Ωé (R = 0.6Ôºåp < 0.001) ÂíåÁù°Áú†ÊΩõ‰ºèÊúüÂ¢ûÂä† (R = 0.68Ôºåp < 0.001)„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫ÜË¢´Âãï„ÄÅÈï∑Êúü„ÄÅÂú®ÂÆ∂‰∏≠Áõ£ÊéßÊÖ¢ÊÄßÊäìÊíìÂèäÂÖ∂Â∞çÁù°Áú†ÁöÑÂΩ±ÈüøÁöÑÊΩõÂäõÔºåÁÇ∫ÊÖ¢ÊÄßÊêîÁô¢ÁóáÊÇ£ËÄÖÁöÑËá®Â∫äË≠∑ÁêÜÂíåËó•Âª†Ëá®Â∫äË©¶È©óÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data Generation**
2501.04614v2 by Daniele Molino, Francesco Di Feola, Eliodoro Faiella, Deborah Fazzini, Domiziana Santucci, Linlin Shen, Valerio Guarrasi, Paolo Soda

Artificial Intelligence is revolutionizing medical practice, enhancing
diagnostic accuracy and healthcare delivery. However, its adaptation in medical
settings still faces significant challenges, related to data availability and
privacy constraints. Synthetic data has emerged as a promising solution to
mitigate these issues, addressing data scarcity while preserving privacy.
Recently, Latent Diffusion Models have emerged as a powerful tool for
generating high-quality synthetic data. Meanwhile, the integration of different
modalities has gained interest, emphasizing the need of models capable of
handle multimodal medical data. Existing approaches struggle to integrate
complementary information and lack the ability to generate modalities
simultaneously. To address this challenge, we present MedCoDi-M, a
6.77-billion-parameter model, designed for multimodal medical data generation,
that, following Foundation Model paradigm, exploits contrastive learning and
large quantity of data to build a shared latent space which capture the
relationships between different data modalities. Further, we introduce the
Multi-Prompt training technique, which significantly boosts MedCoDi-M's
generation under different settings. We extensively validate MedCoDi-M: first
we benchmark it against five competitors on the MIMIC-CXR dataset, a
state-of-the-art dataset for Chest X-ray and radiological report generation.
Secondly, we perform a Visual Turing Test with expert radiologists to assess
the realism and clinical relevance of the generated data, ensuring alignment
with real-world scenarios. Finally, we assess the utility of MedCoDi-M in
addressing key challenges in the medical field, such as anonymization, data
scarcity and imbalance learning. The results are promising, demonstrating the
applicability of MedCoDi-M in medical contexts. Project page is at
https://cosbidev.github.io/MedCoDi-M/.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ËÉΩÊ≠£Âú®Èù©Êñ∞ÈÜ´ÁôÇÂØ¶ÂãôÔºåÊèêÂçáË®∫Êñ∑Ê∫ñÁ¢∫Â∫¶ÂíåÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãô„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂú®ÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠ÁöÑÊáâÁî®‰ªçÈù¢Ëá®ËëóÈáçÂ§ßÊåëÊà∞ÔºåÈÄôËàáË≥áÊñôÂèØÁî®ÊÄßÂíåÈö±ÁßÅÈôêÂà∂ÊúâÈóú„ÄÇÂêàÊàêË≥áÊñôÂ∑≤ÊàêÁÇ∫Á∑©Ëß£ÈÄô‰∫õÂïèÈ°åÁöÑÊΩõÂú®Ëß£Ê±∫ÊñπÊ°àÔºåÂÆÉÂú®‰øùË≠∑Èö±ÁßÅÁöÑÂêåÊôÇËß£Ê±∫‰∫ÜË≥áÊñôÁü≠Áº∫ÁöÑÂïèÈ°å„ÄÇÊúÄËøëÔºåÊΩõÂú®Êì¥Êï£Ê®°ÂûãÂ∑≤ÊàêÁÇ∫Áî¢ÁîüÈ´òÂìÅË≥™ÂêàÊàêË≥áÊñôÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÂêåÊôÇÔºåÊï¥Âêà‰∏çÂêåÊ®°ÊÖãÂ∑≤ÂºïËµ∑ËààË∂£ÔºåÂº∑Ë™ø‰∫ÜÈúÄË¶ÅËÉΩÂ§†ËôïÁêÜÂ§öÊ®°ÊÖãÈÜ´ÁôÇË≥áÊñôÁöÑÊ®°Âûã„ÄÇÁèæÊúâÊñπÊ≥ïÈõ£‰ª•Êï¥ÂêàË£úÂÖÖË≥áË®äÔºå‰∏¶‰∏îÁº∫‰πèÂêåÊôÇÁî¢ÁîüÊ®°ÊÖãÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü MedCoDi-MÔºåÈÄôÊòØ‰∏ÄÂÄã 67.7 ÂÑÑÂèÉÊï∏ÁöÑÊ®°ÂûãÔºåÂ∞àÁÇ∫Â§öÊ®°ÊÖãÈÜ´ÁôÇË≥áÊñôÁî¢ÁîüËÄåË®≠Ë®àÔºåÂÆÉÈÅµÂæ™Âü∫Á§éÊ®°ÂûãÁØÑ‰æãÔºåÂà©Áî®Â∞çÊØîÂ≠∏ÁøíÂíåÂ§ßÈáèÁöÑË≥áÊñô‰æÜÂª∫Á´ã‰∏ÄÂÄãÂÖ±‰∫´ÊΩõÂú®Á©∫ÈñìÔºå‰ª•ÊçïÊçâ‰∏çÂêåË≥áÊñôÊ®°ÊÖã‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ§öÊèêÁ§∫Ë®ìÁ∑¥ÊäÄË°ìÔºåÂÆÉÈ°ØËëóÊèêÂçá‰∫Ü MedCoDi-M Âú®‰∏çÂêåË®≠ÂÆö‰∏ãÁöÑÁî¢Áîü„ÄÇÊàëÂÄëÂª£Ê≥õÈ©óË≠â‰∫Ü MedCoDi-MÔºöÈ¶ñÂÖàÔºåÊàëÂÄëÂú® MIMIC-CXR Ë≥áÊñôÈõÜ‰∏äÂ∞çÂÆÉËàá‰∫îÂÄãÁ´∂Áà≠ËÄÖÈÄ≤Ë°å‰∫ÜÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÈÄôÊòØËÉ∏ÈÉ® X ÂÖâÂíåÊîæÂ∞ÑÂ†±ÂëäÁî¢ÁîüÈ†òÂüüÁöÑÊúÄÊñ∞Ë≥áÊñôÈõÜ„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëËàáÊîæÂ∞ÑÁßëÂ∞àÂÆ∂ÈÄ≤Ë°å‰∫ÜË¶ñË¶∫ÂúñÈùàÊ∏¨Ë©¶Ôºå‰ª•Ë©ï‰º∞Áî¢ÁîüË≥áÊñôÁöÑÁúüÂØ¶ÊÄßÂíåËá®Â∫äÁõ∏ÈóúÊÄßÔºåÁ¢∫‰øùËàáÁúüÂØ¶Â†¥ÊôØ‰øùÊåÅ‰∏ÄËá¥„ÄÇÊúÄÂæåÔºåÊàëÂÄëË©ï‰º∞‰∫Ü MedCoDi-M Âú®Ëß£Ê±∫ÈÜ´ÁôÇÈ†òÂüüÈóúÈçµÊåëÊà∞‰∏≠ÁöÑÊïàÁî®Ôºå‰æãÂ¶ÇÂåøÂêçÂåñ„ÄÅË≥áÊñôÁü≠Áº∫Âíå‰∏çÂπ≥Ë°°Â≠∏Áøí„ÄÇÁµêÊûú‰ª§‰∫∫ÊªøÊÑèÔºåË≠âÊòé‰∫Ü MedCoDi-M Âú®ÈÜ´ÁôÇÁí∞Â¢É‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÂ∞àÊ°àÈ†ÅÈù¢‰ΩçÊñº https://cosbidev.github.io/MedCoDi-M/„ÄÇ

##### **A 65 nm Bayesian Neural Network Accelerator with 360 fJ/Sample In-Word GRNG for AI Uncertainty Estimation**
2501.04577v1 by Zephan M. Enciso, Boyang Cheng, Likai Pei, Jianbo Liu, Steven Davis, Ningyuan Cao, Michael Niemier

Uncertainty estimation is an indispensable capability for AI-enabled,
safety-critical applications, e.g. autonomous vehicles or medical diagnosis.
Bayesian neural networks (BNNs) use Bayesian statistics to provide both
classification predictions and uncertainty estimation, but they suffer from
high computational overhead associated with random number generation and
repeated sample iterations. Furthermore, BNNs are not immediately amenable to
acceleration through compute-in-memory architectures due to the frequent memory
writes necessary after each RNG operation. To address these challenges, we
present an ASIC that integrates 360 fJ/Sample Gaussian RNG directly into the
SRAM memory words. This integration reduces RNG overhead and enables
fully-parallel compute-in-memory operations for BNNs. The prototype chip
achieves 5.12 GSa/s RNG throughput and 102 GOp/s neural network throughput
while occupying 0.45 mm2, bringing AI uncertainty estimation to edge
computation.

ÊëòË¶ÅÔºö‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÂ∞çÊñº AI È©ÖÂãï„ÄÅÂÆâÂÖ®ÈóúÈçµÁöÑÊáâÁî®Á®ãÂºè‰æÜË™™ÊòØ‰∏çÂèØÊàñÁº∫ÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇËá™ÂãïÈßïÈßõËªäËºõÊàñÈÜ´ÁôÇË®∫Êñ∑„ÄÇË≤ùÊ∞èÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑Ø (BNN) ‰ΩøÁî®Ë≤ùÊ∞èÁµ±Ë®à‰æÜÊèê‰æõÂàÜÈ°ûÈ†êÊ∏¨Âíå‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÔºå‰ΩÜÂÆÉÂÄëÊúÉÂõ†Èö®Ê©üÊï∏ÁîüÊàêÂíåÈáçË§áÊ®£Êú¨Ëø≠‰ª£ËÄåÁî¢ÁîüÈ´òÈÅãÁÆóË≤†Êìî„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºÊØèÊ¨° RNG Êìç‰ΩúÂæåÈÉΩÈúÄË¶ÅÈ†ªÁπÅÁöÑË®òÊÜ∂È´îÂØ´ÂÖ•ÔºåÂõ†Ê≠§ BNN ÁÑ°Ê≥ïÁ´ãÂç≥ÈÅ©Áî®ÊñºÈÄèÈÅéË®òÊÜ∂È´îÈÅãÁÆóÊû∂ÊßãÈÄ≤Ë°åÂä†ÈÄü„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÊ¨æ ASICÔºåÂ∞á 360 fJ/Sample Gaussian RNG Áõ¥Êé•Êï¥ÂêàÂà∞ SRAM Ë®òÊÜ∂È´îÂ≠óÂÖÉ‰∏≠„ÄÇÊ≠§Êï¥ÂêàÂèØÊ∏õÂ∞ë RNG Ë≤†ÊìîÔºå‰∏¶ÁÇ∫ BNN ÂïüÁî®ÂÆåÂÖ®‰∏¶Ë°åÁöÑË®òÊÜ∂È´îÈÅãÁÆóÊìç‰Ωú„ÄÇÂéüÂûãÊô∂ÁâáÂèØÈÅîÊàê 5.12 GSa/s RNG ËôïÁêÜÈáèÂíå 102 GOp/s Á•ûÁ∂ìÁ∂≤Ë∑ØËôïÁêÜÈáèÔºåÂêåÊôÇ‰ΩîÁî® 0.45 mm2ÔºåÂ∞á AI ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÂ∏∂Âà∞ÈÇäÁ∑£ÈÅãÁÆó„ÄÇ

##### **Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images**
2501.04217v1 by Ren Tasai, Guang Li, Ren Togo, Minghui Tang, Takaaki Yoshimura, Hiroyuki Sugimori, Kenji Hirata, Takahiro Ogawa, Kohsuke Kudo, Miki Haseyama

We propose a novel continual self-supervised learning method (CSSL)
considering medical domain knowledge in chest CT images. Our approach addresses
the challenge of sequential learning by effectively capturing the relationship
between previously learned knowledge and new information at different stages.
By incorporating an enhanced DER into CSSL and maintaining both diversity and
representativeness within the rehearsal buffer of DER, the risk of data
interference during pretraining is reduced, enabling the model to learn more
richer and robust feature representations. In addition, we incorporate a mixup
strategy and feature distillation to further enhance the model's ability to
learn meaningful representations. We validate our method using chest CT images
obtained under two different imaging conditions, demonstrating superior
performance compared to state-of-the-art methods.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊåÅÁ∫åËá™ÊàëÁõ£Áù£Â≠∏ÁøíÊñπÊ≥ï (CSSL)ÔºåËÄÉÈáè‰∫ÜËÉ∏ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÂΩ±ÂÉè‰∏≠ÁöÑÈÜ´Â≠∏È†òÂüüÁü•Ë≠ò„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈÄèÈÅéÊúâÊïàÊçïÊçâÂÖàÂâçÂ≠∏ÁøíÁöÑÁü•Ë≠òËàá‰∏çÂêåÈöéÊÆµÁöÑÊñ∞Ë≥áË®ä‰πãÈñìÁöÑÈóú‰øÇÔºå‰æÜËß£Ê±∫Âæ™Â∫èÂ≠∏ÁøíÁöÑÊåëÊà∞„ÄÇÈÄèÈÅéÂ∞áÂ¢ûÂº∑ÁöÑ DER Á¥çÂÖ• CSSLÔºå‰∏¶Âú® DER ÁöÑÊéíÁ∑¥Á∑©Ë°ùÂçÄÂÖßÁ∂≠ÊåÅÂ§öÊ®£ÊÄßÂíå‰ª£Ë°®ÊÄßÔºåÈ†êË®ìÁ∑¥ÊúüÈñìË≥áÊñôÂπ≤ÊìæÁöÑÈ¢®Èö™Èôç‰ΩéÔºå‰ΩøÊ®°ÂûãËÉΩÂ§†Â≠∏ÁøíÊõ¥Ë±êÂØå‰∏îÂº∑ÂÅ•ÁöÑÁâπÂæµË°®Âæµ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁ¥çÂÖ•Ê∑∑Ê∑ÜÁ≠ñÁï•ÂíåÁâπÂæµËêÉÂèñÔºåÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑Ê®°ÂûãÂ≠∏ÁøíÊúâÊÑèÁæ©Ë°®ÂæµÁöÑËÉΩÂäõ„ÄÇÊàëÂÄë‰ΩøÁî®Âú®ÂÖ©Á®Æ‰∏çÂêåÂΩ±ÂÉèÊ¢ù‰ª∂‰∏ãÂèñÂæóÁöÑËÉ∏ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÂΩ±ÂÉèÈ©óË≠âÊàëÂÄëÁöÑÊ®°ÂûãÔºåË≠âÊòéËàáÁèæÊúâÊäÄË°ìÁõ∏ÊØîÂÖ∑ÊúâÂÑ™Áï∞ÁöÑÊïàËÉΩ„ÄÇ

##### **Generative Style Transfer for MRI Image Segmentation: A Case of Glioma Segmentation in Sub-Saharan Africa**
2501.04734v1 by Rancy Chepchirchir, Jill Sunday, Raymond Confidence, Dong Zhang, Talha Chaudhry, Udunna C. Anazodo, Kendi Muchungi, Yujing Zou

In Sub-Saharan Africa (SSA), the utilization of lower-quality Magnetic
Resonance Imaging (MRI) technology raises questions about the applicability of
machine learning methods for clinical tasks. This study aims to provide a
robust deep learning-based brain tumor segmentation (BraTS) method tailored for
the SSA population using a threefold approach. Firstly, the impact of domain
shift from the SSA training data on model efficacy was examined, revealing no
significant effect. Secondly, a comparative analysis of 3D and 2D
full-resolution models using the nnU-Net framework indicates similar
performance of both the models trained for 300 epochs achieving a five-fold
cross-validation score of 0.93. Lastly, addressing the performance gap observed
in SSA validation as opposed to the relatively larger BraTS glioma (GLI)
validation set, two strategies are proposed: fine-tuning SSA cases using the
GLI+SSA best-pretrained 2D fullres model at 300 epochs, and introducing a novel
neural style transfer-based data augmentation technique for the SSA cases. This
investigation underscores the potential of enhancing brain tumor prediction
within SSA's unique healthcare landscape.

ÊëòË¶ÅÔºöÂú®ÊííÂìàÊãâ‰ª•ÂçóÈùûÊ¥≤ (SSA)Ôºå‰ΩéË¥®ÈáèÁ£ÅÂÖ±ÊåØÊàêÂÉè (MRI) ÊäÄÊúØÁöÑ‰ΩøÁî®ÂºïÂèë‰∫ÜÊúâÂÖ≥Êú∫Âô®Â≠¶‰π†ÊñπÊ≥ïÂú®‰∏¥Â∫ä‰ªªÂä°‰∏≠ÈÄÇÁî®ÊÄßÁöÑÈóÆÈ¢ò„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Êèê‰æõ‰∏ÄÁßçÈíàÂØπ SSA ‰∫∫Áæ§ÈáèË∫´ÂÆöÂà∂ÁöÑÈ≤ÅÊ£íÊ∑±Â∫¶Â≠¶‰π†ËÑëËÇøÁò§ÂàÜÂâ≤ (BraTS) ÊñπÊ≥ïÔºåÈááÁî®‰∏âÈáçÊñπÊ≥ï„ÄÇÈ¶ñÂÖàÔºåÊ£ÄÊü•‰∫Ü SSA ËÆ≠ÁªÉÊï∞ÊçÆÂØπÊ®°ÂûãÊïàËÉΩÁöÑÂüüÂÅèÁßªÂΩ±ÂìçÔºåÁªìÊûúÊòæÁ§∫Ê≤°ÊúâÊòæÁùÄÂΩ±Âìç„ÄÇÂÖ∂Ê¨°Ôºå‰ΩøÁî® nnU-Net Ê°ÜÊû∂ÂØπ 3D Âíå 2D ÂÖ®ÂàÜËæ®ÁéáÊ®°ÂûãËøõË°åÊØîËæÉÂàÜÊûêÔºåË°®ÊòéÈíàÂØπ 300 ‰∏™ epoch ËÆ≠ÁªÉÁöÑ‰∏§‰∏™Ê®°ÂûãÁöÑÊÄßËÉΩÁõ∏‰ººÔºåÂÆûÁé∞‰∫Ü 0.93 ÁöÑ‰∫îÈáç‰∫§ÂèâÈ™åËØÅÂàÜÊï∞„ÄÇÊúÄÂêéÔºåÈíàÂØπ SSA È™åËØÅ‰∏≠ËßÇÂØüÂà∞ÁöÑÊÄßËÉΩÂ∑ÆË∑ùÔºåËÄå‰∏çÊòØÁõ∏ÂØπËæÉÂ§ßÁöÑ BraTS Á•ûÁªèËÉ∂Ë¥®Áò§ (GLI) È™åËØÅÈõÜÔºåÊèêÂá∫‰∫Ü‰∏§ÁßçÁ≠ñÁï•Ôºö‰ΩøÁî® GLI+SSA ÊúÄ‰Ω≥È¢ÑËÆ≠ÁªÉÁöÑ 2D ÂÖ®ÂàÜËæ®ÁéáÊ®°ÂûãÂú® 300 ‰∏™ epoch ÂØπ SSA ÁóÖ‰æãËøõË°åÂæÆË∞ÉÔºåÂπ∂‰∏∫ SSA ÁóÖ‰æãÂºïÂÖ•‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ•ûÁªèÈ£éÊ†ºËøÅÁßªÊï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØ„ÄÇËøôÈ°πË∞ÉÊü•Âº∫Ë∞É‰∫ÜÂú® SSA Áã¨ÁâπÁöÑÂåªÁñó‰øùÂÅ•ÁéØÂ¢É‰∏≠ÊèêÈ´òËÑëËÇøÁò§È¢ÑÊµãÊΩúÂäõÁöÑÂèØËÉΩÊÄß„ÄÇ

##### **Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study**
2501.03904v1 by Ramya Jonnala, Gongbo Liang, Jeong Yang, Izzat Alsmadi

The integration of large language models (LLMs) into public transit systems
presents a transformative opportunity to enhance urban mobility. This study
explores the potential of LLMs to revolutionize public transportation
management within the context of San Antonio's transit system. Leveraging the
capabilities of LLMs in natural language processing and data analysis, we
investigate their capabilities to optimize route planning, reduce wait times,
and provide personalized travel assistance. By utilizing the General Transit
Feed Specification (GTFS) and other relevant data, this research aims to
demonstrate how LLMs can potentially improve resource allocation, elevate
passenger satisfaction, and inform data-driven decision-making in transit
operations. A comparative analysis of different ChatGPT models was conducted to
assess their ability to understand transportation information, retrieve
relevant data, and provide comprehensive responses. Findings from this study
suggest that while LLMs hold immense promise for public transit, careful
engineering and fine-tuning are essential to realizing their full potential.
San Antonio serves as a case study to inform the development of LLM-powered
transit systems in other urban environments.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êï¥ÂêàÂà∞ÂÖ¨ÂÖ±‰∫§ÈÄöÁ≥ªÁµ±‰∏≠ÔºåÁÇ∫ÊèêÂçáÂüéÂ∏ÇÊµÅÂãïÊÄßÂ∏∂‰æÜËΩâÂûãÂ•ëÊ©ü„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é LLM Âú®ËÅñÂÆâÊù±Â∞ºÂ•ß‰∫§ÈÄöÁ≥ªÁµ±ËÑàÁµ°‰∏ãÔºåÈù©Êñ∞Â§ßÁúæÈÅãËº∏ÁÆ°ÁêÜÁöÑÊΩõÂäõ„ÄÇÂà©Áî® LLM Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåË≥áÊñôÂàÜÊûêÊñπÈù¢ÁöÑËÉΩÂäõÔºåÊàëÂÄëÊé¢Ë®éÂÖ∂Âú®ÂÑ™ÂåñË∑ØÁ∑öË¶èÂäÉ„ÄÅÁ∏ÆÁü≠Á≠âÂÄôÊôÇÈñìÔºå‰ª•ÂèäÊèê‰æõÂÄã‰∫∫ÂåñÊóÖÈÅäÂçîÂä©ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÈÄèÈÅéÂà©Áî®ÈÄöÁî®Â§ßÁúæÈÅãËº∏Ë≥áÊñôË¶èÁØÑ (GTFS) ÂíåÂÖ∂‰ªñÁõ∏ÈóúË≥áÊñôÔºåÊú¨Á†îÁ©∂Êó®Âú®Ë≠âÊòé LLM Â¶Ç‰ΩïÊΩõÂú®ÊèêÂçáË≥áÊ∫êÈÖçÁΩÆ„ÄÅÊèêÂçá‰πòÂÆ¢ÊªøÊÑèÂ∫¶Ôºå‰ª•ÂèäÂú®‰∫§ÈÄöÁáüÈÅã‰∏≠Êèê‰æõË≥áÊñôÈ©ÖÂãïÁöÑÊ±∫Á≠ñ„ÄÇÈáùÂ∞ç‰∏çÂêåÁöÑ ChatGPT Ê®°ÂûãÈÄ≤Ë°åÊØîËºÉÂàÜÊûêÔºå‰ª•Ë©ï‰º∞ÂÖ∂ÁêÜËß£‰∫§ÈÄöË≥áË®ä„ÄÅÊì∑ÂèñÁõ∏ÈóúË≥áÊñôÔºå‰ª•ÂèäÊèê‰æõÂÖ®Èù¢ÂõûÊáâÁöÑËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁôºÁèæÈ°ØÁ§∫ÔºåÂÑòÁÆ° LLM Â∞çÂ§ßÁúæÈÅãËº∏Ê•µÂÖ∑ÂâçÊôØÔºå‰ΩÜÁ≤æÂØÜÁöÑÂ∑•Á®ãÂíåÂæÆË™øÂ∞çÊñºÂØ¶ÁèæÂÖ∂ÂÖ®ÈÉ®ÊΩõÂäõËá≥ÈóúÈáçË¶Å„ÄÇËÅñÂÆâÊù±Â∞ºÂ•ß‰ΩúÁÇ∫‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂ÔºåÁÇ∫Âú®ÂÖ∂‰ªñÈÉΩÂ∏ÇÁí∞Â¢É‰∏≠ÈñãÁôºÁî± LLM È©ÖÂãïÁöÑ‰∫§ÈÄöÁ≥ªÁµ±Êèê‰æõÂèÉËÄÉ„ÄÇ

##### **SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor Diagnosis**
2501.03836v2 by Runci Bai

Brain tumors can result in neurological dysfunction, alterations in cognitive
and psychological states, increased intracranial pressure, and the occurrence
of seizures, thereby presenting a substantial risk to human life and health.
The You Only Look Once(YOLO) series models have demonstrated superior accuracy
in object detection for medical imaging. In this paper, we develop a novel
SCC-YOLO architecture by integrating the SCConv attention mechanism into
YOLOv9. The SCConv module reconstructs an efficient convolutional module by
reducing spatial and channel redundancy among features, thereby enhancing the
learning of image features. We investigate the impact of intergrating different
attention mechanisms with the YOLOv9 model on brain tumor image detection using
both the Br35H dataset and our self-made dataset(Brain_Tumor_Dataset).
Experimental results show that on the Br35H dataset, SCC-YOLO achieved a 0.3%
improvement in mAp50 compared to YOLOv9, while on our self-made dataset,
SCC-YOLO exhibited a 0.5% improvement over YOLOv9. SCC-YOLO has reached
state-of-the-art performance in brain tumor detection. Source code is available
at : https://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master

ÊëòË¶ÅÔºöËÖ¶Áò§ÂèØËÉΩÂ∞éËá¥Á•ûÁ∂ìÂäüËÉΩÈöúÁ§ô„ÄÅË™çÁü•ÂíåÂøÉÁêÜÁãÄÊÖãÊîπËÆä„ÄÅÈ°±ÂÖßÂ£ìÂçáÈ´òÂíåÁô≤ÁôáÁôº‰ΩúÔºåÂæûËÄåÂ∞ç‰∫∫È°ûÁîüÂëΩÂíåÂÅ•Â∫∑ÊßãÊàêÈáçÂ§ßÈ¢®Èö™„ÄÇYou Only Look Once (YOLO) Á≥ªÂàóÊ®°ÂûãÂ∑≤Ë≠âÊòéÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÁõÆÊ®ôÊ™¢Ê∏¨‰∏≠ÂÖ∑ÊúâÂÑ™Áï∞ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄöÈÅéÂ∞á SCConv Ê≥®ÊÑèÂäõÊ©üÂà∂Êï¥ÂêàÂà∞ YOLOv9 ‰∏≠ÔºåÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ SCC-YOLO Êû∂Êßã„ÄÇSCConv Ê®°ÁµÑÈÄöÈÅéÊ∏õÂ∞ëÁâπÂæµ‰πãÈñìÁöÑÁ©∫ÈñìÂíåÈÄöÈÅìÂÜóÈ§ò‰æÜÈáçÂª∫‰∏ÄÂÄãÈ´òÊïàÁöÑÂç∑Á©çÊ®°ÁµÑÔºåÂæûËÄåÂ¢ûÂº∑ÂΩ±ÂÉèÁâπÂæµÁöÑÂ≠∏Áøí„ÄÇÊàëÂÄë‰ΩøÁî® Br35H Ë≥áÊñôÈõÜÂíåÊàëÂÄëËá™Ë£ΩÁöÑË≥áÊñôÈõÜ (Brain_Tumor_Dataset) Ë™øÊü•‰∫ÜÂ∞á‰∏çÂêåÁöÑÊ≥®ÊÑèÂäõÊ©üÂà∂Ëàá YOLOv9 Ê®°ÂûãÊï¥ÂêàÂ∞çËÖ¶Áò§ÂΩ±ÂÉèÊ™¢Ê∏¨ÁöÑÂΩ±Èüø„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂú® Br35H Ë≥áÊñôÈõÜ‰∏äÔºåËàá YOLOv9 Áõ∏ÊØîÔºåSCC-YOLO Âú® mAP50 ‰∏äÊèêÈ´ò‰∫Ü 0.3%ÔºåËÄåÂú®ÊàëÂÄëËá™Ë£ΩÁöÑË≥áÊñôÈõÜ‰∏äÔºåSCC-YOLO ÊØî YOLOv9 ÊèêÈ´ò‰∫Ü 0.5%„ÄÇSCC-YOLO Â∑≤ÈÅîÂà∞ËÖ¶Áò§Ê™¢Ê∏¨ÁöÑÊúÄÊñ∞ÊïàËÉΩ„ÄÇÂéüÂßãÁ¢ºÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºöhttps://jihulab.com/healthcare-information-studio/SCC-YOLO/-/tree/master

##### **SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging through Selective Domain Alignment**
2501.03764v1 by Siyuan Zhao, Chenyu Liu, Yi Ding, Xinliang Zhou

In practical sleep stage classification, a key challenge is the variability
of EEG data across different subjects and environments. Differences in
physiology, age, health status, and recording conditions can lead to domain
shifts between data. These domain shifts often result in decreased model
accuracy and reliability, particularly when the model is applied to new data
with characteristics different from those it was originally trained on, which
is a typical manifestation of negative transfer. To address this, we propose
SelectiveFinetuning in this paper. Our method utilizes a pretrained Multi
Resolution Convolutional Neural Network (MRCNN) to extract EEG features,
capturing the distinctive characteristics of different sleep stages. To
mitigate the effect of domain shifts, we introduce a domain aligning mechanism
that employs Earth Mover Distance (EMD) to evaluate and select source domain
data closely matching the target domain. By finetuning the model with selective
source data, our SelectiveFinetuning enhances the model's performance on target
domain that exhibits domain shifts compared to the data used for training.
Experimental results show that our method outperforms existing baselines,
offering greater robustness and adaptability in practical scenarios where data
distributions are often unpredictable.

ÊëòË¶ÅÔºöÂú®ÂØ¶ÈöõÁöÑÁù°Áú†ÈöéÊÆµÂàÜÈ°û‰∏≠Ôºå‰∏ÄÂÄãÈóúÈçµÁöÑÊåëÊà∞ÊòØËÖ¶ÈõªÂúñÊï∏ÊìöÂú®‰∏çÂêåÂèóË©¶ËÄÖÂíåÁí∞Â¢É‰∏≠ÁöÑËÆäÁï∞ÊÄß„ÄÇÁîüÁêÜ„ÄÅÂπ¥ÈΩ°„ÄÅÂÅ•Â∫∑ÁãÄÊ≥ÅÂíåË®òÈåÑÊ¢ù‰ª∂ÁöÑÂ∑ÆÁï∞ÂèØËÉΩÂ∞éËá¥Êï∏Êìö‰πãÈñìÁöÑÈ†òÂüüÂÅèÁßª„ÄÇÈÄô‰∫õÈ†òÂüüÂÅèÁßªÈÄöÂ∏∏ÊúÉÂ∞éËá¥Ê®°ÂûãÊ∫ñÁ¢∫Â∫¶ÂíåÂèØÈù†ÊÄß‰∏ãÈôçÔºåÁâπÂà•ÊòØÁï∂Ê®°ÂûãÊáâÁî®ÊñºËàáÂÖ∂ÊúÄÂàùË®ìÁ∑¥ÊôÇ‰∏çÂêåÁöÑÁâπÂæµÁöÑÊñ∞Êï∏ÊìöÊôÇÔºåÈÄôÊòØË≤†ÈÅ∑ÁßªÁöÑÂÖ∏ÂûãË°®Áèæ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂú®Êú¨Êñá‰∏≠ÊèêÂá∫ÈÅ∏ÊìáÊÄßÂæÆË™ø„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂà©Áî®È†êË®ìÁ∑¥ÁöÑÂ§öËß£ÊûêÂ∫¶Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (MRCNN) ‰æÜÊèêÂèñËÖ¶ÈõªÂúñÁâπÂæµÔºåÊçïÊçâ‰∏çÂêåÁù°Áú†ÈöéÊÆµÁöÑÁç®ÁâπÁâπÂæµ„ÄÇÁÇ∫‰∫ÜÊ∏õËºïÈ†òÂüüÂÅèÁßªÁöÑÂΩ±ÈüøÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÈ†òÂüüÂ∞çÈΩäÊ©üÂà∂ÔºåÂÆÉÊé°Áî®Âú∞ÁêÉÁßªÂãïË∑ùÈõ¢ (EMD) ‰æÜË©ï‰º∞ÂíåÈÅ∏ÊìáËàáÁõÆÊ®ôÈ†òÂüüÁ∑äÂØÜÂåπÈÖçÁöÑÊ∫êÈ†òÂüüÊï∏Êìö„ÄÇÈÄöÈÅé‰ΩøÁî®ÈÅ∏ÊìáÊÄßÊ∫êÊï∏ÊìöÂæÆË™øÊ®°ÂûãÔºåÊàëÂÄëÁöÑÈÅ∏ÊìáÊÄßÂæÆË™øÂ¢ûÂº∑‰∫ÜÊ®°ÂûãÂú®ËàáÁî®ÊñºË®ìÁ∑¥ÁöÑÊï∏ÊìöÁõ∏ÊØîË°®ÁèæÂá∫È†òÂüüÂÅèÁßªÁöÑÁõÆÊ®ôÈ†òÂüü‰∏äÁöÑÊÄßËÉΩ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂÑ™ÊñºÁèæÊúâÁöÑÂü∫Ê∫ñÔºåÂú®Êï∏ÊìöÂàÜ‰ΩàÈÄöÂ∏∏‰∏çÂèØÈ†êÊ∏¨ÁöÑÂØ¶ÈöõÂ†¥ÊôØ‰∏≠Êèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁ©©ÂÅ•ÊÄßÂíåÈÅ©ÊáâÊÄß„ÄÇ

##### **Self-adaptive vision-language model for 3D segmentation of pulmonary artery and vein**
2501.03722v1 by Xiaotong Guo, Deqian Yang, Dan Wang, Haochen Zhao, Yuan Li, Zhilin Sui, Tao Zhou, Lijun Zhang, Yanda Meng

Accurate segmentation of pulmonary structures iscrucial in clinical
diagnosis, disease study, and treatment planning. Significant progress has been
made in deep learning-based segmentation techniques, but most require much
labeled data for training. Consequently, developing precise segmentation
methods that demand fewer labeled datasets is paramount in medical image
analysis. The emergence of pre-trained vision-language foundation models, such
as CLIP, recently opened the door for universal computer vision tasks.
Exploiting the generalization ability of these pre-trained foundation models on
downstream tasks, such as segmentation, leads to unexpected performance with a
relatively small amount of labeled data. However, exploring these models for
pulmonary artery-vein segmentation is still limited. This paper proposes a
novel framework called Language-guided self-adaptive Cross-Attention Fusion
Framework. Our method adopts pre-trained CLIP as a strong feature extractor for
generating the segmentation of 3D CT scans, while adaptively aggregating the
cross-modality of text and image representations. We propose a s pecially
designed adapter module to fine-tune pre-trained CLIP with a self-adaptive
learning strategy to effectively fuse the two modalities of embeddings. We
extensively validate our method on a local dataset, which is the largest
pulmonary artery-vein CT dataset to date and consists of 718 labeled data in
total. The experiments show that our method outperformed other state-of-the-art
methods by a large margin. Our data and code will be made publicly available
upon acceptance.

ÊëòË¶ÅÔºöÁ≤æÁ¢∫ÂàÜÂâ≤ËÇ∫ÈÉ®ÁµêÊßãÂú®Ëá®Â∫äË®∫Êñ∑„ÄÅÁñæÁóÖÁ†îÁ©∂ÂíåÊ≤ªÁôÇË®àÁï´‰∏≠Ëá≥ÈóúÈáçË¶Å„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂàÜÂâ≤ÊäÄË°ìÂ∑≤ÂèñÂæóÈáçÂ§ßÈÄ≤Â±ïÔºå‰ΩÜÂ§ßÂ§öÊï∏ÊäÄË°ìÂú®Ë®ìÁ∑¥ÊôÇÈúÄË¶ÅÂ§ßÈáèÁöÑÊ®ôË®òË≥áÊñô„ÄÇÂõ†Ê≠§ÔºåÈñãÁôºÁ≤æÁ¢∫ÁöÑÂàÜÂâ≤ÊñπÊ≥ïÔºå‰ª•Ê∏õÂ∞ëÊ®ôË®òË≥áÊñôÈõÜÁöÑÈúÄÊ±ÇÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰∏≠Ëá≥ÈóúÈáçË¶Å„ÄÇÈ†êË®ìÁ∑¥ÁöÑË¶ñË¶∫Ë™ûË®ÄÂü∫Á§éÊ®°ÂûãÔºà‰æãÂ¶Ç CLIPÔºâÁöÑÂá∫ÁèæÔºåÊúÄËøëÁÇ∫ÈÄöÁî®ÈõªËÖ¶Ë¶ñË¶∫‰ªªÂãôÈñãÂïü‰∫ÜÂ§ßÈñÄ„ÄÇÂà©Áî®ÈÄô‰∫õÈ†êË®ìÁ∑¥Âü∫Á§éÊ®°ÂûãÂú®ÂàÜÂâ≤Á≠â‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂç≥‰ΩøÊ®ôË®òË≥áÊñôÈáèÁõ∏Â∞çËºÉÂ∞ëÔºå‰πüËÉΩÁî¢ÁîüÊÑèÊÉ≥‰∏çÂà∞ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÊé¢Á¥¢ÈÄô‰∫õÊ®°ÂûãÂú®ËÇ∫ÂãïËÑàÈùúËÑàÂàÜÂâ≤‰∏≠ÁöÑÊáâÁî®‰ªçÁÑ∂ÊúâÈôê„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫Ë™ûË®ÄÂºïÂ∞éËá™ÈÅ©Êáâ‰∫§ÂèâÊ≥®ÊÑèÂäõËûçÂêàÊ°ÜÊû∂ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÊé°Áî®È†êË®ìÁ∑¥ÁöÑ CLIP ‰ΩúÁÇ∫Âº∑Â§ßÁöÑÁâπÂæµËêÉÂèñÂô®ÔºåÁî®ÊñºÁî¢Áîü 3D ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÁöÑÂàÜÂâ≤ÔºåÂêåÊôÇËá™ÈÅ©ÊáâÂú∞ËÅöÂêàÊñáÊú¨ÂíåÂΩ±ÂÉèË°®ÂæµÁöÑË∑®Ê®°ÊÖã„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁâπÂà•Ë®≠Ë®àÁöÑÈÅ©ÈÖçÂô®Ê®°ÁµÑÔºå‰ª•Ëá™ÈÅ©ÊáâÂ≠∏ÁøíÁ≠ñÁï•ÂæÆË™øÈ†êË®ìÁ∑¥ÁöÑ CLIPÔºå‰ª•ÊúâÊïàËûçÂêàÂÖ©Á®ÆÂµåÂÖ•Ê®°ÊÖã„ÄÇÊàëÂÄëÂú®‰∏ÄÂÄãÊú¨Âú∞Ë≥áÊñôÈõÜ‰∏äÂª£Ê≥õÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÔºåÈÄôÊòØËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÂ§ßÁöÑËÇ∫ÂãïËÑàÈùúËÑàÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèË≥áÊñôÈõÜÔºåÁ∏ΩÂÖ±ÂåÖÂê´ 718 ÂÄãÊ®ôË®òË≥áÊñô„ÄÇÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°Âûã‰ª•Â§ßÂπÖÂÑ™ÊñºÂÖ∂‰ªñÊúÄÂÖàÈÄ≤Ê®°Âûã„ÄÇÊàëÂÄëÁöÑË≥áÊñôÂíåÁ®ãÂºèÁ¢ºÂ∞áÂú®Áç≤ÂæóÊé•ÂèóÂæåÂÖ¨Èñã„ÄÇ

##### **Can Deep Learning Trigger Alerts from Mobile-Captured Images?**
2501.03499v1 by Pritisha Sarkar, Duranta Durbaar Vishal Saha, Mousumi Saha

Our research presents a comprehensive approach to leveraging mobile camera
image data for real-time air quality assessment and recommendation. We develop
a regression-based Convolutional Neural Network model and tailor it explicitly
for air quality prediction by exploiting the inherent relationship between
output parameters. As a result, the Mean Squared Error of 0.0077 and 0.0112
obtained for 2 and 5 pollutants respectively outperforms existing models.
Furthermore, we aim to verify the common practice of augmenting the original
dataset with a view to introducing more variation in the training phase. It is
one of our most significant contributions that our experimental results
demonstrate minimal accuracy differences between the original and augmented
datasets. Finally, a real-time, user-friendly dashboard is implemented which
dynamically displays the Air Quality Index and pollutant values derived from
captured mobile camera images. Users' health conditions are considered to
recommend whether a location is suitable based on current air quality metrics.
Overall, this research contributes to verification of data augmentation
techniques, CNN-based regression modelling for air quality prediction, and
user-centric air quality monitoring through mobile technology. The proposed
system offers practical solutions for individuals to make informed
environmental health and well-being decisions.

ÊëòË¶ÅÔºöÊàëÂÄëÁöÑÁ†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂà©Áî®Ë°åÂãïË£ùÁΩÆÁõ∏Ê©üÂΩ±ÂÉèË≥áÊñôÈÄ≤Ë°åÂç≥ÊôÇÁ©∫Ê∞£ÂìÅË≥™Ë©ï‰º∞ÂíåÂª∫Ë≠∞ÁöÑÂÖ®Èù¢ÊÄßÊñπÊ≥ï„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºËø¥Ê≠∏ÁöÑÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÔºå‰∏¶ÈÄèÈÅéÂà©Áî®Ëº∏Âá∫ÂèÉÊï∏‰πãÈñìÁöÑÂÖßÂú®Èóú‰øÇÔºåÈáùÂ∞çÁ©∫Ê∞£ÂìÅË≥™È†êÊ∏¨ÈáèË∫´ÊâìÈÄ†„ÄÇÂõ†Ê≠§ÔºåÂàÜÂà•ÈáùÂ∞ç 2 Âíå 5 Á®ÆÊ±°ÊüìÁâ©ÂèñÂæóÁöÑÂπ≥ÂùáÂπ≥ÊñπË™§Â∑ÆÁÇ∫ 0.0077 Âíå 0.0112ÔºåÂÑ™ÊñºÁèæÊúâÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊó®Âú®È©óË≠âÊì¥ÂÖÖÂéüÂßãË≥áÊñôÈõÜÁöÑÂ∏∏Ë¶ãÂÅöÊ≥ïÔºå‰ª•ÊúüÂú®Ë®ìÁ∑¥ÈöéÊÆµÂºïÂÖ•Êõ¥Â§öËÆäÁï∞„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÂéüÂßãË≥áÊñôÈõÜÂíåÊì¥ÂÖÖË≥áÊñôÈõÜ‰πãÈñìÁöÑÊ∫ñÁ¢∫Â∫¶Â∑ÆÁï∞Ê•µÂ∞èÔºåÈÄôÊòØÊàëÂÄëÊúÄÈáçË¶ÅÁöÑË≤¢Áçª‰πã‰∏Ä„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂØ¶‰Ωú‰∫Ü‰∏ÄÂÄãÂç≥ÊôÇ„ÄÅ‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑÂÑÄË°®ÊùøÔºåÂèØÂãïÊÖãÈ°ØÁ§∫ÂæûÊì∑ÂèñÁöÑË°åÂãïË£ùÁΩÆÁõ∏Ê©üÂΩ±ÂÉè‰∏≠Ë°çÁîüÁöÑÁ©∫Ê∞£ÂìÅË≥™ÊåáÊï∏ÂíåÊ±°ÊüìÁâ©Êï∏ÂÄº„ÄÇËÄÉÈáè‰ΩøÁî®ËÄÖÁöÑÂÅ•Â∫∑ÁãÄÊ≥ÅÔºåÂª∫Ë≠∞ÊòØÂê¶Ê†πÊìöÁõÆÂâçÁöÑÁ©∫Ê∞£ÂìÅË≥™ÊåáÊ®ôÈÅ∏ÊìáÈÅ©ÂêàÁöÑÂú∞Èªû„ÄÇÊï¥È´îËÄåË®ÄÔºåÈÄôÈ†ÖÁ†îÁ©∂ÊúâÂä©ÊñºÈ©óË≠âË≥áÊñôÊì¥ÂÖÖÊäÄË°ì„ÄÅÂü∫Êñº CNN ÁöÑËø¥Ê≠∏Ê®°ÂûãÔºàÁî®ÊñºÁ©∫Ê∞£ÂìÅË≥™È†êÊ∏¨Ôºâ‰ª•ÂèäÈÄèÈÅéË°åÂãïÊäÄË°ìÈÄ≤Ë°å‰ª•‰ΩøÁî®ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÁ©∫Ê∞£ÂìÅË≥™Áõ£Êéß„ÄÇÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±ÁÇ∫ÂÄã‰∫∫Êèê‰æõÂØ¶ÈöõÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰ª•‰æøÂÅöÂá∫ÊòéÊô∫ÁöÑÁí∞Â¢ÉÂÅ•Â∫∑ÂíåÁ¶èÁ•âÊ±∫Á≠ñ„ÄÇ

##### **Activating Associative Disease-Aware Vision Token Memory for LLM-Based X-ray Report Generation**
2501.03458v1 by Xiao Wang, Fuling Wang, Haowen Wang, Bo Jiang, Chuanfu Li, Yaowei Wang, Yonghong Tian, Jin Tang

X-ray image based medical report generation achieves significant progress in
recent years with the help of the large language model, however, these models
have not fully exploited the effective information in visual image regions,
resulting in reports that are linguistically sound but insufficient in
describing key diseases. In this paper, we propose a novel associative
memory-enhanced X-ray report generation model that effectively mimics the
process of professional doctors writing medical reports. It considers both the
mining of global and local visual information and associates historical report
information to better complete the writing of the current report. Specifically,
given an X-ray image, we first utilize a classification model along with its
activation maps to accomplish the mining of visual regions highly associated
with diseases and the learning of disease query tokens. Then, we employ a
visual Hopfield network to establish memory associations for disease-related
tokens, and a report Hopfield network to retrieve report memory information.
This process facilitates the generation of high-quality reports based on a
large language model and achieves state-of-the-art performance on multiple
benchmark datasets, including the IU X-ray, MIMIC-CXR, and Chexpert Plus. The
source code of this work is released on
\url{https://github.com/Event-AHU/Medical_Image_Analysis}.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÂú®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂπ´Âä©‰∏ãÔºåÂü∫Êñº X ÂÖâÂΩ±ÂÉèÁöÑÈÜ´ÁôÇÂ†±ÂëäÁîüÊàêÂèñÂæó‰∫ÜÈ°ØËëóÈÄ≤Â±ïÔºåÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®°Âûã‰∏¶Êú™ÂÖÖÂàÜÂà©Áî®Ë¶ñË¶∫ÂΩ±ÂÉèÂçÄÂüü‰∏≠ÁöÑÊúâÊïàË≥áË®äÔºåÂ∞éËá¥Â†±ÂëäÂú®Ë™ûË®Ä‰∏äÈõñÁÑ∂ÊµÅÊö¢Ôºå‰ΩÜÂú®ÊèèËø∞ÈóúÈçµÁñæÁóÖÊñπÈù¢Âçª‰∏çË∂≥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑËÅØÊÉ≥ÂºèË®òÊÜ∂Â¢ûÂº∑ X ÂÖâÂ†±ÂëäÁîüÊàêÊ®°ÂûãÔºåÊúâÊïàÂú∞Ê®°Êì¨Â∞àÊ•≠ÈÜ´ÁîüÊí∞ÂØ´ÈÜ´ÁôÇÂ†±ÂëäÁöÑÈÅéÁ®ã„ÄÇÂÆÉÂêåÊôÇËÄÉÊÖÆ‰∫ÜÂ∞çÂÖ®Â±ÄÂíåÂ±ÄÈÉ®Ë¶ñË¶∫Ë≥áË®äÁöÑÊåñÊéòÔºå‰∏¶ËÅØÁπ´Ê≠∑Âè≤Â†±ÂëäË≥áË®äÔºå‰ª•Êõ¥Â•ΩÂú∞ÂÆåÊàêÁï∂ÂâçÂ†±ÂëäÁöÑÊí∞ÂØ´„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÁµ¶ÂÆö‰∏ÄÂºµ X ÂÖâÂΩ±ÂÉèÔºåÊàëÂÄëÈ¶ñÂÖàÂà©Áî®ÂàÜÈ°ûÊ®°ÂûãÂèäÂÖ∂ÊøÄÊ¥ªÊò†Â∞Ñ‰æÜÂÆåÊàêËàáÁñæÁóÖÈ´òÂ∫¶Áõ∏ÈóúÁöÑË¶ñË¶∫ÂçÄÂüüÁöÑÊåñÊéòÂíåÁñæÁóÖÊü•Ë©¢‰ª§ÁâåÁöÑÂ≠∏Áøí„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊé°Áî®Ë¶ñË¶∫ÈúçÊôÆËè≤ÁàæÂæ∑Á∂≤Ë∑Ø‰æÜÂª∫Á´ãËàáÁñæÁóÖÁõ∏ÈóúÁöÑ‰ª§ÁâåÁöÑË®òÊÜ∂ËÅØÁπ´Ôºå‰∏¶Êé°Áî®Â†±ÂëäÈúçÊôÆËè≤ÁàæÂæ∑Á∂≤Ë∑Ø‰æÜÊ™¢Á¥¢Â†±ÂëäË®òÊÜ∂Ë≥áË®ä„ÄÇÈÄôÂÄãÈÅéÁ®ãÊúâÂä©ÊñºÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁîüÊàêÈ´òÂìÅË≥™ÁöÑÂ†±ÂëäÔºå‰∏¶Âú®ÂåÖÊã¨ IU X Â∞ÑÁ∑ö„ÄÅMIMIC-CXR Âíå Chexpert Plus Âú®ÂÖßÁöÑÂ§öÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÊ≠§È†ÖÂ∑•‰ΩúÁöÑÂéüÂßãÁ¢ºÂ∑≤Áôº‰ΩàÂú®\url{https://github.com/Event-AHU/Medical_Image_Analysis}„ÄÇ

##### **Existential Crisis: A Social Robot's Reason for Being**
2501.03376v1 by Dora Medgyesy, Joella Galas, Julian van Pol, Rustam Eynaliyev, Thijs Vollebregt

As Robots become ever more important in our daily lives there's growing need
for understanding how they're perceived by people. This study aims to
investigate how the user perception of robots is influenced by displays of
personality. Using LLMs and speech to text technology, we designed a
within-subject study to compare two conditions: a personality-driven robot and
a purely task-oriented, personality-neutral robot. Twelve participants,
recruited from Socially Intelligent Robotics course at Vrije Universiteit
Amsterdam, interacted with a robot Nao tasked with asking them a set of medical
questions under both conditions. After completing both interactions, the
participants completed a user experience questionnaire measuring their
emotional states and robot perception using standardized questionnaires from
the SRI and Psychology literature.

ÊëòË¶ÅÔºöÈö®ËëóÊ©üÂô®‰∫∫Âú®ÊàëÂÄëÊó•Â∏∏ÁîüÊ¥ª‰∏≠ÁöÑÈáçË¶ÅÊÄßÊó•ÁõäÊèêÂçáÔºåÂ∞çÊñº‰∫ÜËß£‰∫∫ÂÄëÂ¶Ç‰ΩïÊÑüÁü•Ê©üÂô®‰∫∫ÁöÑÈúÄÊ±Ç‰πüÊó•ÁõäÂ¢ûÂä†„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Êé¢Ë®éÊ©üÂô®‰∫∫ÁöÑ‰ΩøÁî®ËÄÖÊÑüÁü•Â¶Ç‰ΩïÂèóÂà∞‰∫∫Ê†ºË°®ÁèæÁöÑÂΩ±Èüø„ÄÇÊàëÂÄë‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåË™ûÈü≥ËΩâÊñáÂ≠óÊäÄË°ìÔºåË®≠Ë®à‰∫Ü‰∏ÄÈ†ÖÂèóË©¶ËÄÖÂÖßÁ†îÁ©∂Ôºå‰ª•ÊØîËºÉÂÖ©Á®ÆÊÉÖÊ≥ÅÔºö‰∏ÄÁ®ÆÊòØ‰∫∫Ê†ºÈ©ÖÂãïÁöÑÊ©üÂô®‰∫∫ÔºåÂè¶‰∏ÄÁ®ÆÊòØÁ¥îÁ≤π‰ª•‰ªªÂãôÁÇ∫Â∞éÂêë„ÄÅ‰∫∫Ê†º‰∏≠Á´ãÁöÑÊ©üÂô®‰∫∫„ÄÇÊàëÂÄëÂæûÈòøÂßÜÊñØÁâπ‰∏πËá™Áî±Â§ßÂ≠∏ÁöÑÁ§æ‰∫§Êô∫ËÉΩÊ©üÂô®‰∫∫Ë™≤Á®ã‰∏≠ÊãõÂãü‰∫Ü 12 ÂêçÂèÉËàáËÄÖÔºå‰ªñÂÄëËàáÊ©üÂô®‰∫∫ Nao ‰∫íÂãïÔºåÂú®ÂÖ©Á®ÆÊÉÖÊ≥Å‰∏ãÈÉΩÂêë‰ªñÂÄëË©¢Âïè‰∏ÄÁ≥ªÂàóÈÜ´ÁôÇÂïèÈ°å„ÄÇÂú®ÂÆåÊàêÈÄôÂÖ©Á®Æ‰∫íÂãïÂæåÔºåÂèÉËàáËÄÖÂÆåÊàê‰∫Ü‰∏Ä‰ªΩ‰ΩøÁî®ËÄÖÈ´îÈ©óÂïèÂç∑Ôºå‰ΩøÁî®‰æÜËá™ SRI ÂíåÂøÉÁêÜÂ≠∏ÊñáÁçªÁöÑÊ®ôÊ∫ñÂåñÂïèÂç∑Ê∏¨Èáè‰ªñÂÄëÁöÑÊÉÖÁ∑íÁãÄÊÖãÂíåÊ©üÂô®‰∫∫ÊÑüÁü•„ÄÇ

##### **Label-free Concept Based Multiple Instance Learning for Gigapixel Histopathology**
2501.02922v1 by Susu Sun, Leslie Tessier, Fr√©d√©rique Meeuwsen, Cl√©ment Grisi, Dominique van Midden, Geert Litjens, Christian F. Baumgartner

Multiple Instance Learning (MIL) methods allow for gigapixel Whole-Slide
Image (WSI) analysis with only slide-level annotations. Interpretability is
crucial for safely deploying such algorithms in high-stakes medical domains.
Traditional MIL methods offer explanations by highlighting salient regions.
However, such spatial heatmaps provide limited insights for end users. To
address this, we propose a novel inherently interpretable WSI-classification
approach that uses human-understandable pathology concepts to generate
explanations. Our proposed Concept MIL model leverages recent advances in
vision-language models to directly predict pathology concepts based on image
features. The model's predictions are obtained through a linear combination of
the concepts identified on the top-K patches of a WSI, enabling inherent
explanations by tracing each concept's influence on the prediction. In contrast
to traditional concept-based interpretable models, our approach eliminates the
need for costly human annotations by leveraging the vision-language model. We
validate our method on two widely used pathology datasets: Camelyon16 and
PANDA. On both datasets, Concept MIL achieves AUC and accuracy scores over 0.9,
putting it on par with state-of-the-art models. We further find that 87.1\%
(Camelyon16) and 85.3\% (PANDA) of the top 20 patches fall within the tumor
region. A user study shows that the concepts identified by our model align with
the concepts used by pathologists, making it a promising strategy for
human-interpretable WSI classification.

ÊëòË¶ÅÔºöÂ§öÂØ¶‰æãÂ≠∏Áøí (MIL) ÊñπÊ≥ïÂÉÖ‰ΩøÁî®ÁéªÁâáÂ±§Á¥öË®ªËß£ÔºåÂç≥ÂèØÈÄ≤Ë°åÂêâÂÉèÁ¥†ÂÖ®ÁéªÁâáÂΩ±ÂÉè (WSI) ÂàÜÊûê„ÄÇÂèØËß£ÈáãÊÄßÂ∞çÊñºÂú®È´òÈ¢®Èö™ÈÜ´ÁôÇÈ†òÂüüÂÆâÂÖ®ÈÉ®ÁΩ≤Ê≠§È°ûÊºîÁÆóÊ≥ïËá≥ÈóúÈáçË¶Å„ÄÇÂÇ≥Áµ±ÁöÑ MIL ÊñπÊ≥ïÈÄèÈÅéÂº∑Ë™øÈ°ØËëóÂçÄÂüü‰æÜÊèê‰æõË™™Êòé„ÄÇÁÑ∂ËÄåÔºåÊ≠§È°ûÁ©∫ÈñìÁÜ±ÂúñÁÇ∫ÊúÄÁµÇ‰ΩøÁî®ËÄÖÊèê‰æõÁöÑË¶ãËß£ÊúâÈôê„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©é‰∏îÊú¨Ë≥™‰∏äÂèØËß£ÈáãÁöÑ WSI ÂàÜÈ°ûÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®‰∫∫È°ûÂèØÁêÜËß£ÁöÑÁóÖÁêÜÊ¶ÇÂøµ‰æÜÁî¢ÁîüË™™Êòé„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊ¶ÇÂøµ MIL Ê®°ÂûãÂà©Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºåÊ†πÊìöÂΩ±ÂÉèÁâπÂæµÁõ¥Êé•È†êÊ∏¨ÁóÖÁêÜÊ¶ÇÂøµ„ÄÇË©≤Ê®°ÂûãÁöÑÈ†êÊ∏¨ÊòØÈÄèÈÅéÁ∑öÊÄßÁµÑÂêà WSI È†ÇÈÉ® K ÂÄãÂçÄÂ°ä‰∏äË≠òÂà•ÁöÑÊ¶ÇÂøµËÄåÁç≤ÂæóÁöÑÔºåÈÄèÈÅéËøΩËπ§ÊØèÂÄãÊ¶ÇÂøµÂ∞çÈ†êÊ∏¨ÁöÑÂΩ±ÈüøÔºåÂèØ‰ª•Êèê‰æõÂÖßÂú®Ë™™Êòé„ÄÇËàáÂÇ≥Áµ±Âü∫ÊñºÊ¶ÇÂøµÁöÑÂèØËß£ÈáãÊ®°ÂûãÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈÄèÈÅéÂà©Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÔºåÊ∂àÈô§‰∫ÜÂ∞çÊòÇË≤¥ÁöÑ‰∫∫Â∑•Ë®ªËß£ÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑÁóÖÁêÜË≥áÊñôÈõÜÔºöCamelyon16 Âíå PANDA ‰∏äÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊ®°Âûã„ÄÇÂú®ÂÖ©ÂÄãË≥áÊñôÈõÜ‰∏äÔºåÊ¶ÇÂøµ MIL ÁöÑ AUC ÂíåÊ∫ñÁ¢∫ÁéáÈÉΩË∂ÖÈÅé 0.9ÔºåËàáÊúÄÂÖàÈÄ≤ÁöÑÊ®°Âûã‰∏çÁõ∏‰∏ä‰∏ã„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÁôºÁèæÔºåÂâç 20 ÂÄãÂçÄÂ°ä‰∏≠Êúâ 87.1%ÔºàCamelyon16ÔºâÂíå 85.3%ÔºàPANDAÔºâËêΩÂú®ËÖ´Áò§ÂçÄÂüüÂÖß„ÄÇ‰∏ÄÈ†Ö‰ΩøÁî®ËÄÖÁ†îÁ©∂Ë°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãË≠òÂà•ÁöÑÊ¶ÇÂøµËàáÁóÖÁêÜÂ≠∏ÂÆ∂‰ΩøÁî®ÁöÑÊ¶ÇÂøµ‰∏ÄËá¥Ôºå‰ΩøÂÖ∂ÊàêÁÇ∫‰∫∫È°ûÂèØËß£Èáã WSI ÂàÜÈ°ûÁöÑ‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÁ≠ñÁï•„ÄÇ

##### **Explaining Humour Style Classifications: An XAI Approach to Understanding Computational Humour Analysis**
2501.02891v1 by Mary Ogbuka Kenneth, Foaad Khosmood, Abbas Edalat

Humour styles can have either a negative or a positive impact on well-being.
Given the importance of these styles to mental health, significant research has
been conducted on their automatic identification. However, the automated
machine learning models used for this purpose are black boxes, making their
prediction decisions opaque. Clarity and transparency are vital in the field of
mental health. This paper presents an explainable AI (XAI) framework for
understanding humour style classification, building upon previous work in
computational humour analysis. Using the best-performing single model
(ALI+XGBoost) from prior research, we apply comprehensive XAI techniques to
analyse how linguistic, emotional, and semantic features contribute to humour
style classification decisions. Our analysis reveals distinct patterns in how
different humour styles are characterised and misclassified, with particular
emphasis on the challenges in distinguishing affiliative humour from other
styles. Through detailed examination of feature importance, error patterns, and
misclassification cases, we identify key factors influencing model decisions,
including emotional ambiguity, context misinterpretation, and target
identification. The framework demonstrates significant utility in understanding
model behaviour, achieving interpretable insights into the complex interplay of
features that define different humour styles. Our findings contribute to both
the theoretical understanding of computational humour analysis and practical
applications in mental health, content moderation, and digital humanities
research.

ÊëòË¶ÅÔºöÂπΩÈªòÈ¢®Ê†ºÂ∞çÂπ∏Á¶èÊÑüÂèØËÉΩÁî¢ÁîüË≤†Èù¢ÊàñÊ≠£Èù¢ÁöÑÂΩ±Èüø„ÄÇ
ÈëëÊñºÈÄô‰∫õÈ¢®Ê†ºÂ∞çÂøÉÁêÜÂÅ•Â∫∑ÁöÑÈáçË¶ÅÊÄßÔºåÂ∑≤Á∂ìÂ∞çÂÖ∂Ëá™ÂãïË≠òÂà•ÈÄ≤Ë°å‰∫ÜÂ§ßÈáèÁ†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåÁî®ÊñºÊ≠§ÁõÆÁöÑÁöÑËá™ÂãïÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÊòØÈªëÁõíÂ≠êÔºå‰ΩøÂæóÂÖ∂È†êÊ∏¨Ê±∫Á≠ñ‰∏çÈÄèÊòé„ÄÇÊ∏ÖÊô∞Â∫¶ÂíåÈÄèÊòéÂ∫¶Âú®ÂøÉÁêÜÂÅ•Â∫∑È†òÂüüËá≥ÈóúÈáçË¶Å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑ AI (XAI) Ê°ÜÊû∂ÔºåÁî®ÊñºÁêÜËß£ÂπΩÈªòÈ¢®Ê†ºÂàÜÈ°ûÔºåÂª∫Á´ãÂú®Ë®àÁÆóÂπΩÈªòÂàÜÊûêÁöÑÂÖàÂâçÂ∑•‰Ωú‰πã‰∏ä„ÄÇ‰ΩøÁî®ÂÖàÂâçÁ†îÁ©∂‰∏≠Ë°®ÁèæÊúÄÂ•ΩÁöÑÂñÆ‰∏ÄÊ®°Âûã (ALI+XGBoost)ÔºåÊàëÂÄëÊáâÁî®ÂÖ®Èù¢ÁöÑ XAI ÊäÄË°ì‰æÜÂàÜÊûêË™ûË®Ä„ÄÅÊÉÖÁ∑íÂíåË™ûÁæ©ÁâπÂæµÂ¶Ç‰ΩïÂΩ±ÈüøÂπΩÈªòÈ¢®Ê†ºÂàÜÈ°ûÊ±∫Á≠ñ„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÊè≠Á§∫‰∫Ü‰∏çÂêåÂπΩÈªòÈ¢®Ê†ºÂ¶Ç‰ΩïË¢´Ë°®ÂæµÂíåÈåØË™§ÂàÜÈ°ûÁöÑ‰∏çÂêåÊ®°ÂºèÔºåÁâπÂà•Âº∑Ë™ø‰∫ÜÂçÄÂàÜËÅØÂ±¨ÂπΩÈªòËàáÂÖ∂‰ªñÈ¢®Ê†ºÁöÑÊåëÊà∞„ÄÇÈÄöÈÅé‰ªîÁ¥∞Ê™¢Êü•ÁâπÂæµÈáçË¶ÅÊÄß„ÄÅÈåØË™§Ê®°ÂºèÂíåÈåØË™§ÂàÜÈ°ûÊ°à‰æãÔºåÊàëÂÄëÁ¢∫ÂÆö‰∫ÜÂΩ±ÈüøÊ®°ÂûãÊ±∫Á≠ñÁöÑÈóúÈçµÂõ†Á¥†ÔºåÂåÖÊã¨ÊÉÖÁ∑íÊ®°Á≥ä„ÄÅÊÉÖÂ¢ÉË™§Ëß£ÂíåÁõÆÊ®ôË≠òÂà•„ÄÇË©≤Ê°ÜÊû∂Â±ïÁ§∫‰∫ÜÂú®ÁêÜËß£Ê®°ÂûãË°åÁÇ∫ÊñπÈù¢ÁöÑÈ°ØËëóÊïàÁî®ÔºåÂØ¶Áèæ‰∫ÜÂ∞çÂÆöÁæ©‰∏çÂêåÂπΩÈªòÈ¢®Ê†ºÁöÑÁâπÂæµ‰πãÈñìË§áÈõúÁõ∏‰∫í‰ΩúÁî®ÁöÑÂèØËß£ÈáãË¶ãËß£„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊúâÂä©ÊñºË®àÁÆóÂπΩÈªòÂàÜÊûêÁöÑÁêÜË´ñÁêÜËß£ÂíåÂøÉÁêÜÂÅ•Â∫∑„ÄÅÂÖßÂÆπÂØ©Ê†∏ÂíåÊï∏Â≠ó‰∫∫ÊñáÁ†îÁ©∂‰∏≠ÁöÑÂØ¶ÈöõÊáâÁî®„ÄÇ

##### **IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment**
2501.02869v1 by Yiming Zhang, Zheng Chang, Wentao Cai, MengXing Ren, Kang Yuan, Yining Sun, Zenghui Ding

Recent researches of large language models(LLM), which is pre-trained on
massive general-purpose corpora, have achieved breakthroughs in responding
human queries. However, these methods face challenges including limited data
insufficiency to support extensive pre-training and can not align responses
with users' instructions. To address these issues, we introduce a medical
instruction dataset, CMedINS, containing six medical instructions derived from
actual medical tasks, which effectively fine-tunes LLM in conjunction with
other data. Subsequently, We launch our medical model, IIMedGPT, employing an
efficient preference alignment method, Direct preference Optimization(DPO). The
results show that our final model outperforms existing medical models in
medical dialogue.Datsets, Code and model checkpoints will be released upon
acceptance.

ÊëòË¶ÅÔºöÊúÄËøëÈáùÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÁ†îÁ©∂ÔºåË©≤Ê®°ÂûãÈ†êÂÖàË®ìÁ∑¥ÊñºÈæêÂ§ßÁöÑÈÄöÁî®Ë™ûÊñôÂ∫´‰∏≠ÔºåÂ∑≤Âú®ÂõûÊáâ‰∫∫È°ûÊü•Ë©¢ÊñπÈù¢ÂèñÂæóÁ™ÅÁ†¥„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈù¢Ëá®ÁöÑÊåëÊà∞ÂåÖÊã¨Ë≥áÊñô‰∏çË∂≥‰ª•ÊîØÊè¥Âª£Ê≥õÁöÑÈ†êË®ìÁ∑¥Ôºå‰∏îÁÑ°Ê≥ïÂ∞áÂõûÊáâËàá‰ΩøÁî®ËÄÖÁöÑÊåáÁ§∫‰øùÊåÅ‰∏ÄËá¥„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂºïÈÄ≤‰∏ÄÂÄãÈÜ´ÁôÇÊåáÁ§∫Ë≥áÊñôÈõÜ CMedINSÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂÖ≠È†ÖÂæûÂØ¶ÈöõÈÜ´ÁôÇ‰ªªÂãô‰∏≠Ë°çÁîüÁöÑÈÜ´ÁôÇÊåáÁ§∫ÔºåËàáÂÖ∂‰ªñË≥áÊñôÁµêÂêàÂæåËÉΩÊúâÊïàÂæÆË™ø LLM„ÄÇÈö®ÂæåÔºåÊàëÂÄëÊé®Âá∫ÊàëÂÄëÁöÑÈÜ´ÁôÇÊ®°Âûã IIMedGPTÔºåÊé°Áî®‰∏ÄÁ®ÆÊúâÊïàÁéáÁöÑÂÅèÂ•ΩÂ∞çÈΩäÊñπÊ≥ïÔºåÁõ¥Êé•ÂÅèÂ•ΩÊúÄ‰Ω≥Âåñ (DPO)„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊúÄÁµÇÊ®°ÂûãÂú®ÈÜ´ÁôÇÂ∞çË©±‰∏≠ÂÑ™ÊñºÁèæÊúâÁöÑÈÜ´ÁôÇÊ®°Âûã„ÄÇË≥áÊñôÈõÜ„ÄÅÁ®ãÂºèÁ¢ºÂíåÊ®°ÂûãÊ™¢Êü•ÈªûÂ∞áÂú®ÈÄöÈÅéÈ©óË≠âÂæåÈáãÂá∫„ÄÇ

##### **Multi-Modal One-Shot Federated Ensemble Learning for Medical Data with Vision Large Language Model**
2501.03292v1 by Naibo Wang, Yuchen Deng, Shichen Fan, Jianwei Yin, See-Kiong Ng

Federated learning (FL) has attracted considerable interest in the medical
domain due to its capacity to facilitate collaborative model training while
maintaining data privacy. However, conventional FL methods typically
necessitate multiple communication rounds, leading to significant communication
overhead and delays, especially in environments with limited bandwidth.
One-shot federated learning addresses these issues by conducting model training
and aggregation in a single communication round, thereby reducing communication
costs while preserving privacy. Among these, one-shot federated ensemble
learning combines independently trained client models using ensemble techniques
such as voting, further boosting performance in non-IID data scenarios. On the
other hand, existing machine learning methods in healthcare predominantly use
unimodal data (e.g., medical images or textual reports), which restricts their
diagnostic accuracy and comprehensiveness. Therefore, the integration of
multi-modal data is proposed to address these shortcomings. In this paper, we
introduce FedMME, an innovative one-shot multi-modal federated ensemble
learning framework that utilizes multi-modal data for medical image analysis.
Specifically, FedMME capitalizes on vision large language models to produce
textual reports from medical images, employs a BERT model to extract textual
features from these reports, and amalgamates these features with visual
features to improve diagnostic accuracy. Experimental results show that our
method demonstrated superior performance compared to existing one-shot
federated learning methods in healthcare scenarios across four datasets with
various data distributions. For instance, it surpasses existing one-shot
federated learning approaches by more than 17.5% in accuracy on the RSNA
dataset when applying a Dirichlet distribution with ($\alpha$ = 0.3).

ÊëòË¶ÅÔºöËÅîÈÇ¶Â≠¶‰π† (FL) Áî±‰∫éÂÖ∂Âú®Áª¥Êä§Êï∞ÊçÆÈöêÁßÅÁöÑÂêåÊó∂‰øÉËøõÂçè‰ΩúÊ®°ÂûãËÆ≠ÁªÉÁöÑËÉΩÂäõÔºåÂú®ÂåªÂ≠¶È¢ÜÂüüÂºïËµ∑‰∫ÜÊûÅÂ§ßÁöÑÂÖ¥Ë∂£„ÄÇÁÑ∂ËÄåÔºå‰º†ÁªüÁöÑ FL ÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§öËΩÆÈÄö‰ø°ÔºåËøô‰ºöÂØºËá¥‰∏•ÈáçÁöÑÈÄö‰ø°ÂºÄÈîÄÂíåÂª∂ËøüÔºåÂ∞§ÂÖ∂ÊòØÂú®Â∏¶ÂÆΩÂèóÈôêÁöÑÁéØÂ¢É‰∏≠„ÄÇÂçïÊ¨°ËÅîÈÇ¶Â≠¶‰π†ÈÄöËøáÂú®ÂçïÊ¨°ÈÄö‰ø°ËΩÆ‰∏≠ËøõË°åÊ®°ÂûãËÆ≠ÁªÉÂíåËÅöÂêàÊù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰ªéËÄåÂú®‰øùÊä§ÈöêÁßÅÁöÑÂêåÊó∂Èôç‰ΩéÈÄö‰ø°ÊàêÊú¨„ÄÇÂÖ∂‰∏≠ÔºåÂçïÊ¨°ËÅîÈÇ¶ÈõÜÊàêÂ≠¶‰π†‰ΩøÁî®ÈõÜÊàêÊäÄÊúØÔºàÂ¶ÇÊäïÁ•®ÔºâÂ∞ÜÁã¨Á´ãËÆ≠ÁªÉÁöÑÂÆ¢Êà∑Á´ØÊ®°ÂûãÁªÑÂêàËµ∑Êù•ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÂú®Èùû IID Êï∞ÊçÆÂú∫ÊôØ‰∏≠ÁöÑÊÄßËÉΩ„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÁé∞ÊúâÁöÑÂåªÁñó‰øùÂÅ•Êú∫Âô®Â≠¶‰π†ÊñπÊ≥ï‰∏ªË¶Å‰ΩøÁî®ÂçïÊ®°ÊÄÅÊï∞ÊçÆÔºà‰æãÂ¶ÇÂåªÂ≠¶ÂõæÂÉèÊàñÊñáÊú¨Êä•ÂëäÔºâÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑËØäÊñ≠ÂáÜÁ°ÆÊÄßÂíåÂÖ®Èù¢ÊÄß„ÄÇÂõ†Ê≠§ÔºåÊèêÂá∫‰∫ÜÂ§öÊ®°ÊÄÅÊï∞ÊçÆÁöÑÈõÜÊàêÊù•Ëß£ÂÜ≥Ëøô‰∫õÁº∫ÁÇπ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨‰ªãÁªç‰∫Ü FedMMEÔºå‰∏ÄÁßçÂàõÊñ∞ÁöÑÂçïÊ¨°Â§öÊ®°ÊÄÅËÅîÈÇ¶ÈõÜÊàêÂ≠¶‰π†Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî®Â§öÊ®°ÊÄÅÊï∞ÊçÆËøõË°åÂåªÂ≠¶ÂõæÂÉèÂàÜÊûê„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåFedMME Âà©Áî®ËßÜËßâÂ§ßËØ≠Ë®ÄÊ®°Âûã‰ªéÂåªÂ≠¶ÂõæÂÉè‰∏≠ÁîüÊàêÊñáÊú¨Êä•ÂëäÔºåÈááÁî® BERT Ê®°Âûã‰ªéËøô‰∫õÊä•Âëä‰∏≠ÊèêÂèñÊñáÊú¨ÁâπÂæÅÔºåÂπ∂Â∞ÜËøô‰∫õÁâπÂæÅ‰∏éËßÜËßâÁâπÂæÅÁõ∏ÁªìÂêà‰ª•ÊèêÈ´òËØäÊñ≠ÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÁé∞ÊúâÁöÑÂçïÊ¨°ËÅîÈÇ¶Â≠¶‰π†ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Âõõ‰∏™ÂÖ∑Êúâ‰∏çÂêåÊï∞ÊçÆÂàÜÂ∏ÉÁöÑÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÂåªÁñó‰øùÂÅ•Âú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫‰ºòË∂äÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåÂΩìÂ∫îÁî®ÂÖ∑Êúâ ($\alpha$ = 0.3) ÁöÑ Dirichlet ÂàÜÂ∏ÉÊó∂ÔºåÂÆÉÂú® RSNA Êï∞ÊçÆÈõÜ‰∏äÁöÑÂáÜÁ°ÆÁéáÊØîÁé∞ÊúâÁöÑÂçïÊ¨°ËÅîÈÇ¶Â≠¶‰π†ÊñπÊ≥ïÈ´òÂá∫ 17.5% ‰ª•‰∏ä„ÄÇ

##### **GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation**
2501.02788v2 by Niloufar Eghbali, Hassan Bagher-Ebadian, Tuka Alhanai, Mohammad M. Ghassemi

Vision Transformers (ViTs) have shown promise in medical image semantic
segmentation (MISS) by capturing long-range correlations. However, ViTs often
struggle to model local spatial information effectively, which is essential for
accurately segmenting fine anatomical details, particularly when applied to
small datasets without extensive pre-training. We introduce Gabor and Laplacian
of Gaussian Convolutional Swin Network (GLoG-CSUnet), a novel architecture
enhancing Transformer-based models by incorporating learnable radiomic
features. This approach integrates dynamically adaptive Gabor and Laplacian of
Gaussian (LoG) filters to capture texture, edge, and boundary information,
enhancing the feature representation processed by the Transformer model. Our
method uniquely combines the long-range dependency modeling of Transformers
with the texture analysis capabilities of Gabor and LoG features. Evaluated on
the Synapse multi-organ and ACDC cardiac segmentation datasets, GLoG-CSUnet
demonstrates significant improvements over state-of-the-art models, achieving a
1.14% increase in Dice score for Synapse and 0.99% for ACDC, with minimal
computational overhead (only 15 and 30 additional parameters, respectively).
GLoG-CSUnet's flexible design allows integration with various base models,
offering a promising approach for incorporating radiomics-inspired feature
extraction in Transformer architectures for medical image analysis. The code
implementation is available on GitHub at: https://github.com/HAAIL/GLoG-CSUnet.

ÊëòË¶ÅÔºö<paragraph>Ë¶ñË¶∫ËΩâÊèõÂô® (ViT) Â∑≤Âú®ÈÜ´ÁôÇÂΩ±ÂÉèË™ûÊÑèÂàÜÂâ≤ (MISS) ‰∏≠Â±ïÁèæÂâçÊôØÔºåËóâÁî±Êì∑ÂèñÈï∑Á®ãÈóúËÅØÊÄß„ÄÇÁÑ∂ËÄåÔºåViT Á∂ìÂ∏∏Èõ£‰ª•ÊúâÊïàÂú∞Âª∫Ê®°Â±ÄÈÉ®Á©∫ÈñìË≥áË®äÔºåÈÄôÂ∞çÊñºÁ≤æÁ¢∫ÂàÜÂâ≤Á≤æÁ¥∞Ëß£ÂâñÁ¥∞ÁØÄËá≥ÈóúÈáçË¶ÅÔºåÁâπÂà•ÊòØÂú®ÊáâÁî®ÊñºÊ≤íÊúâÂª£Ê≥õÈ†êÂÖàË®ìÁ∑¥ÁöÑÂ∞èÂûãË≥áÊñôÈõÜÊôÇ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÈ´òÊñØÂç∑Á©ç Swin Á∂≤Ë∑Ø (GLoG-CSUnet) ÁöÑ Gabor Âíå LaplacianÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊû∂ÊßãÔºåÈÄèÈÅéÊï¥ÂêàÂèØÂ≠∏ÁøíÁöÑÊîæÂ∞ÑÁâπÂæµ‰æÜÂ¢ûÂº∑Âü∫ÊñºËΩâÊèõÂô®ÁöÑÊ®°Âûã„ÄÇÊ≠§ÊñπÊ≥ïÊï¥Âêà‰∫ÜÂãïÊÖãËá™ÈÅ©Êáâ Gabor ÂíåÈ´òÊñØ Laplacian (LoG) ÊøæÊ≥¢Âô®‰æÜÊì∑ÂèñÁ¥ãÁêÜ„ÄÅÈÇäÁ∑£ÂíåÈÇäÁïåË≥áË®äÔºåÂ¢ûÂº∑ËΩâÊèõÂô®Ê®°ÂûãËôïÁêÜÁöÑÁâπÂæµË°®Á§∫„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÁç®ÁâπÂú∞ÁµêÂêà‰∫ÜËΩâÊèõÂô®ÁöÑÈï∑Á®ã‰æùË≥¥ÊÄßÂª∫Ê®°Ëàá Gabor Âíå LoG ÁâπÂæµÁöÑÁ¥ãÁêÜÂàÜÊûêÂäüËÉΩ„ÄÇÂú® Synapse Â§öÂô®ÂÆòÂíå ACDC ÂøÉËáüÂàÜÂâ≤Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåGLoG-CSUnet Â±ïÁ§∫Âá∫ÊØîÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÊúâÈ°ØËëóÁöÑÊîπÈÄ≤ÔºåSynapse ÁöÑ Dice ÂàÜÊï∏Â¢ûÂä†‰∫Ü 1.14%ÔºåACDC ÁöÑ Dice ÂàÜÊï∏Â¢ûÂä†‰∫Ü 0.99%ÔºåË®àÁÆóË≤†ÊìîÊ•µÂ∞èÔºàÂàÜÂà•Âè™Êúâ 15 Âíå 30 ÂÄãÈ°çÂ§ñÁöÑÂèÉÊï∏Ôºâ„ÄÇGLoG-CSUnet ÁöÑÂΩàÊÄßË®≠Ë®àÂÖÅË®±ËàáÂêÑÁ®ÆÂü∫Á§éÊ®°ÂûãÊï¥ÂêàÔºåÁÇ∫Âú®ËΩâÊèõÂô®Êû∂Êßã‰∏≠Êï¥ÂêàÊîæÂ∞ÑÁµÑÂ≠∏ÂïüÁôºÁöÑÁâπÂæµËêÉÂèñÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºå‰ª•ÈÄ≤Ë°åÈÜ´ÁôÇÂΩ±ÂÉèÂàÜÊûê„ÄÇÁ®ãÂºèÁ¢ºÂØ¶‰ΩúÂèØÂú® GitHub ‰∏äÂèñÂæóÔºöhttps://github.com/HAAIL/GLoG-CSUnet„ÄÇ</paragraph>

##### **Hybrid deep convolution model for lung cancer detection with transfer learning**
2501.02785v1 by Sugandha Saxena, S. N. Prasad, Ashwin M Polnaya, Shweta Agarwala

Advances in healthcare research have significantly enhanced our understanding
of disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung
cancer remains one of the leading causes of cancer-related mortality worldwide
due to challenges in early and accurate diagnosis. While current lung cancer
detection models show promise, there is considerable potential for further
improving the accuracy for timely intervention. To address this challenge, we
introduce a hybrid deep convolution model leveraging transfer learning, named
the Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the
precision of lung cancer detection by refining sensitivity and specificity.
This model has surpassed existing deep learning approaches through experimental
validation, achieving an accuracy of 98% and a sensitivity of 97%. By
overlaying sensitivity maps onto lung Computed Tomography (CT) scans, it
enables the visualization of regions most indicative of malignant or benign
classifications. This innovative method demonstrates exceptional performance in
distinguishing lung cancer with minimal false positives, thereby enhancing the
accuracy of medical diagnoses.

ÊëòË¶ÅÔºöÈÜ´ÁôÇ‰øùÂÅ•Á†îÁ©∂ÁöÑÈÄ≤Ê≠•È°ØËëóÂ¢ûÈÄ≤‰∫ÜÊàëÂÄëÂ∞çÁñæÁóÖÊ©üÂà∂„ÄÅË®∫Êñ∑Á≤æÊ∫ñÂ∫¶ÂíåÊ≤ªÁôÇÈÅ∏ÊìáÁöÑ‰∫ÜËß£„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÊó©ÊúüÂíåÊ∫ñÁ¢∫Ë®∫Êñ∑ÁöÑÊåëÊà∞ÔºåËÇ∫Áôå‰ªçÁÑ∂ÊòØÂÖ®ÁêÉÁôåÁóáÁõ∏ÈóúÊ≠ª‰∫°ÁöÑ‰∏ªË¶ÅÂéüÂõ†‰πã‰∏Ä„ÄÇÈõñÁÑ∂ÁõÆÂâçÁöÑËÇ∫ÁôåÊ™¢Ê∏¨Ê®°ÂûãÈ°ØÁ§∫Âá∫ÂâçÊôØÔºå‰ΩÜ‰ªçÊúâÁõ∏Áï∂Â§ßÁöÑÊΩõÂäõÂèØ‰ª•ÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÊ∫ñÁ¢∫ÊÄßÔºå‰ª•‰æøÂèäÊôÇ‰ªãÂÖ•„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂà©Áî®ÈÅ∑ÁßªÂ≠∏ÁøíÁöÑÊ∑∑ÂêàÊ∑±Â∫¶Âç∑Á©çÊ®°ÂûãÔºåÂêçÁÇ∫ÊúÄÂ§ßÊïèÊÑüÂ∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø (MSNN)„ÄÇMSNN Êó®Âú®ÈÄèÈÅéË™øÊï¥ÊïèÊÑüÂ∫¶ÂíåÁâπÁï∞ÊÄß‰æÜÊèêÈ´òËÇ∫ÁôåÊ™¢Ê∏¨ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÊ≠§Ê®°ÂûãÂ∑≤ÈÄèÈÅéÂØ¶È©óÈ©óË≠âË∂ÖË∂äÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºåÈÅîÂà∞ 98% ÁöÑÊ∫ñÁ¢∫Â∫¶Âíå 97% ÁöÑÊïèÊÑüÂ∫¶„ÄÇÈÄèÈÅéÂ∞áÊïèÊÑüÂ∫¶ÂúñÁñäÂä†Âà∞ËÇ∫ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (CT) ‰∏äÔºåÂÆÉÂèØ‰ª•Ë¶ñË¶∫ÂåñÂá∫ÊúÄËÉΩ‰ª£Ë°®ÊÉ°ÊÄßÊàñËâØÊÄßÂàÜÈ°ûÁöÑÂçÄÂüü„ÄÇÈÄôÁ®ÆÂâµÊñ∞ÊñπÊ≥ïÂú®ÂçÄÂàÜËÇ∫ÁôåÊôÇË°®ÁèæÂá∫Ê•µ‰Ω≥ÁöÑÊïàËÉΩÔºå‰∏îË™§Âà§ÁÇ∫ÈôΩÊÄßÁöÑÊÉÖÊ≥ÅÊúÄÂ∞ëÔºåÂæûËÄåÊèêÈ´ò‰∫ÜÈÜ´ÁôÇË®∫Êñ∑ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇ

##### **ICFNet: Integrated Cross-modal Fusion Network for Survival Prediction**
2501.02778v1 by Binyu Zhang, Zhu Meng, Junhao Dong, Fei Su, Zhicheng Zhao

Survival prediction is a crucial task in the medical field and is essential
for optimizing treatment options and resource allocation. However, current
methods often rely on limited data modalities, resulting in suboptimal
performance. In this paper, we propose an Integrated Cross-modal Fusion Network
(ICFNet) that integrates histopathology whole slide images, genomic expression
profiles, patient demographics, and treatment protocols. Specifically, three
types of encoders, a residual orthogonal decomposition module and a unification
fusion module are employed to merge multi-modal features to enhance prediction
accuracy. Additionally, a balanced negative log-likelihood loss function is
designed to ensure fair training across different patients. Extensive
experiments demonstrate that our ICFNet outperforms state-of-the-art algorithms
on five public TCGA datasets, including BLCA, BRCA, GBMLGG, LUAD, and UCEC, and
shows its potential to support clinical decision-making and advance precision
medicine. The codes are available at: https://github.com/binging512/ICFNet.

ÊëòË¶ÅÔºöÂ≠òÊ¥ªÈ†êÊ∏¨ÊòØÈÜ´Â≠∏È†òÂüüÁöÑ‰∏ÄÈ†ÖÈóúÈçµ‰ªªÂãôÔºåÂ∞çÊñºÂÑ™ÂåñÊ≤ªÁôÇÈÅ∏È†ÖÂíåË≥áÊ∫êÂàÜÈÖçËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÊäÄË°ìÈÄöÂ∏∏‰ª∞Ë≥¥ÊúâÈôêÁöÑÊï∏ÊìöÂΩ¢ÂºèÔºåÂ∞éËá¥Ê¨°‰Ω≥ÁöÑË°®Áèæ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊï¥ÂêàÂºèË∑®ÂΩ¢ÂºèËûçÂêàÁ∂≤Ë∑Ø (ICFNet)ÔºåÂÆÉÊï¥Âêà‰∫ÜÁµÑÁπîÁóÖÁêÜÂ≠∏ÂÖ®ÂπªÁáàÁâáÂΩ±ÂÉè„ÄÅÂü∫Âõ†È´îË°®ÁèæÁâπÂæµ„ÄÅÁóÖÊÇ£‰∫∫Âè£Áµ±Ë®àË≥áÊñôÂíåÊ≤ªÁôÇÂçîÂÆö„ÄÇÂÖ∑È´î‰æÜË™™Ôºå‰ΩøÁî®‰∏âÁ®ÆÈ°ûÂûãÁöÑÁ∑®Á¢ºÂô®„ÄÅ‰∏ÄÂÄãÊÆòÂ∑ÆÊ≠£‰∫§ÂàÜËß£Ê®°ÁµÑÂíå‰∏ÄÂÄãÁµ±‰∏ÄËûçÂêàÊ®°ÁµÑÔºå‰ª•Âêà‰ΩµÂ§öÂΩ¢ÂºèÁâπÂæµÔºå‰ª•Â¢ûÂº∑È†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÊ≠§Â§ñÔºåË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂπ≥Ë°°ÁöÑË≤†Â∞çÊï∏‰ººÁÑ∂ÊêçÂ§±ÂáΩÊï∏Ôºå‰ª•Á¢∫‰øù‰∏çÂêåÁóÖÊÇ£‰πãÈñìÁöÑÂÖ¨Âπ≥Ë®ìÁ∑¥„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÁöÑ ICFNet Âú®‰∫îÂÄãÂÖ¨ÈñãÁöÑ TCGA Ë≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊºîÁÆóÊ≥ïÔºåÂåÖÊã¨ BLCA„ÄÅBRCA„ÄÅGBMLGG„ÄÅLUAD Âíå UCECÔºå‰∏¶Â±ïÁ§∫ÂÖ∂ÊîØÊè¥Ëá®Â∫äÊ±∫Á≠ñÂíåÊé®ÂãïÁ≤æÊ∫ñÈÜ´ÁôÇÁöÑÊΩõÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºöhttps://github.com/binging512/ICFNet„ÄÇ

##### **Tree-based RAG-Agent Recommendation System: A Case Study in Medical Test Data**
2501.02727v1 by Yahe Yang, Chengyue Huang

We present HiRMed (Hierarchical RAG-enhanced Medical Test Recommendation), a
novel tree-structured recommendation system that leverages Retrieval-Augmented
Generation (RAG) for intelligent medical test recommendations. Unlike
traditional vector similarity-based approaches, our system performs medical
reasoning at each tree node through a specialized RAG process. Starting from
the root node with initial symptoms, the system conducts step-wise medical
analysis to identify potential underlying conditions and their corresponding
diagnostic requirements. At each level, instead of simple matching, our
RAG-enhanced nodes analyze retrieved medical knowledge to understand
symptom-disease relationships and determine the most appropriate diagnostic
path. The system dynamically adjusts its recommendation strategy based on
medical reasoning results, considering factors such as urgency levels and
diagnostic uncertainty. Experimental results demonstrate that our approach
achieves superior performance in terms of coverage rate, accuracy, and miss
rate compared to conventional retrieval-based methods. This work represents a
significant advance in medical test recommendation by introducing medical
reasoning capabilities into the traditional tree-based retrieval structure.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫ HiRMedÔºàÂàÜÂ±§ RAG Â¢ûÂº∑ÂûãÈÜ´ÁôÇÊ™¢Ê∏¨Âª∫Ë≠∞ÔºâÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ®πÁãÄÁµêÊßãÂª∫Ë≠∞Á≥ªÁµ±ÔºåÂÆÉÂà©Áî®Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ‰æÜÈÄ≤Ë°åÊô∫ËÉΩÈÜ´ÁôÇÊ™¢Ê∏¨Âª∫Ë≠∞„ÄÇËàáÂÇ≥Áµ±ÁöÑÂü∫ÊñºÂêëÈáèÁõ∏‰ººÊÄßÁöÑÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÁöÑÁ≥ªÁµ±ÈÄöÈÅé‰∏ÄÂÄãÂ∞àÈñÄÁöÑ RAG Á®ãÂ∫èÂú®ÊØèÂÄãÊ®πÁØÄÈªûÂü∑Ë°åÈÜ´ÁôÇÊé®ÁêÜ„ÄÇÂæûÂÖ∑ÊúâÂàùÂßãÁóáÁãÄÁöÑÊ†πÁØÄÈªûÈñãÂßãÔºåÁ≥ªÁµ±Âü∑Ë°åÈÄêÊ≠•ÈÜ´ÁôÇÂàÜÊûê‰ª•Ë≠òÂà•ÊΩõÂú®ÁöÑÊΩõÂú®ÁñæÁóÖÂèäÂÖ∂Â∞çÊáâÁöÑË®∫Êñ∑Ë¶ÅÊ±Ç„ÄÇÂú®ÊØèÂÄãÂ±§Á¥öÔºåÊàëÂÄëÁöÑ RAG Â¢ûÂº∑ÁØÄÈªûÊúÉÂàÜÊûêÊ™¢Á¥¢Âà∞ÁöÑÈÜ´ÁôÇÁü•Ë≠òÔºå‰ª•‰∫ÜËß£ÁóáÁãÄËàáÁñæÁóÖÁöÑÈóú‰øÇÔºå‰∏¶Á¢∫ÂÆöÊúÄÂêàÈÅ©ÁöÑË®∫Êñ∑Ë∑ØÂæëÔºåËÄå‰∏çÊòØÈÄ≤Ë°åÁ∞°ÂñÆÁöÑÂåπÈÖç„ÄÇÁ≥ªÁµ±Ê†πÊìöÈÜ´ÁôÇÊé®ÁêÜÁµêÊûúÂãïÊÖãË™øÊï¥ÂÖ∂Âª∫Ë≠∞Á≠ñÁï•ÔºåËÄÉÊÖÆÁ∑äÊÄ•Á®ãÂ∫¶ÂíåË®∫Êñ∑‰∏çÁ¢∫ÂÆöÊÄßÁ≠âÂõ†Á¥†„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàáÂÇ≥Áµ±ÁöÑÂü∫ÊñºÊ™¢Á¥¢ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Ë¶ÜËìãÁéá„ÄÅÊ∫ñÁ¢∫ÊÄßÂíåÈÅ∫ÊºèÁéáÊñπÈù¢ÂèñÂæó‰∫ÜÂÑ™Áï∞ÁöÑË°®Áèæ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈÄöÈÅéÂ∞áÈÜ´ÁôÇÊé®ÁêÜËÉΩÂäõÂºïÂÖ•ÂÇ≥Áµ±ÁöÑÂü∫ÊñºÊ®πÁöÑÊ™¢Á¥¢ÁµêÊßãÔºå‰ª£Ë°®‰∫ÜÈÜ´ÁôÇÊ™¢Ê∏¨Âª∫Ë≠∞ÁöÑÈáçÂ§ßÈÄ≤Â±ï„ÄÇ

##### **Representation Learning of Lab Values via Masked AutoEncoder**
2501.02648v2 by David Restrepo, Chenwei Wu, Yueran Jia, Jaden K. Sun, Jack Gallifant, Catherine G. Bielick, Yugang Jia, Leo A. Celi

Accurate imputation of missing laboratory values in electronic health records
(EHRs) is critical to enable robust clinical predictions and reduce biases in
AI systems in healthcare. Existing methods, such as variational autoencoders
(VAEs) and decision tree-based approaches such as XGBoost, struggle to model
the complex temporal and contextual dependencies in EHR data, mainly in
underrepresented groups. In this work, we propose Lab-MAE, a novel
transformer-based masked autoencoder framework that leverages self-supervised
learning for the imputation of continuous sequential lab values. Lab-MAE
introduces a structured encoding scheme that jointly models laboratory test
values and their corresponding timestamps, enabling explicit capturing temporal
dependencies. Empirical evaluation on the MIMIC-IV dataset demonstrates that
Lab-MAE significantly outperforms the state-of-the-art baselines such as
XGBoost across multiple metrics, including root mean square error (RMSE),
R-squared (R2), and Wasserstein distance (WD). Notably, Lab-MAE achieves
equitable performance across demographic groups of patients, advancing fairness
in clinical predictions. We further investigate the role of follow-up
laboratory values as potential shortcut features, revealing Lab-MAE's
robustness in scenarios where such data is unavailable. The findings suggest
that our transformer-based architecture, adapted to the characteristics of the
EHR data, offers a foundation model for more accurate and fair clinical
imputation models. In addition, we measure and compare the carbon footprint of
Lab-MAE with the baseline XGBoost model, highlighting its environmental
requirements.

ÊëòË¶ÅÔºö<paragraph>Ê∫ñÁ¢∫‰º∞ÁÆóÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑ (EHR) ‰∏≠ÈÅ∫Â§±ÁöÑÂØ¶È©óÂÆ§ÂÄºÂ∞çÊñºÂïüÁî®Á©©ÂÅ•ÁöÑËá®Â∫äÈ†êÊ∏¨ÂíåÊ∏õÂ∞ëÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ AI Á≥ªÁµ±ÁöÑÂÅèÂ∑ÆËá≥ÈóúÈáçË¶Å„ÄÇÁèæÊúâÊñπÊ≥ïÔºà‰æãÂ¶ÇËÆäÁï∞Ëá™ÂãïÁ∑®Á¢ºÂô® (VAE) ÂíåÂü∫ÊñºÊ±∫Á≠ñÊ®πÁöÑÊñπÊ≥ïÔºå‰æãÂ¶Ç XGBoostÔºâÈõ£‰ª•Âª∫Ê®° EHR Ë≥áÊñô‰∏≠Ë§áÈõúÁöÑÊôÇÈñìÂíå‰∏ä‰∏ãÊñá‰æùË≥¥ÊÄßÔºåÁâπÂà•ÊòØÂú®‰ª£Ë°®ÊÄß‰∏çË∂≥ÁöÑÁæ§ÁµÑ‰∏≠„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ Lab-MAEÔºå‰∏ÄÂÄãÊñ∞Á©éÁöÑÂü∫Êñº Transformer ÁöÑÈÅÆÁΩ©Ëá™ÂãïÁ∑®Á¢ºÂô®Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî®Ëá™ÊàëÁõ£Áù£Â≠∏Áøí‰æÜ‰º∞ÁÆóÈÄ£Á∫åÈ†ÜÂ∫èÂØ¶È©óÂÆ§ÂÄº„ÄÇLab-MAE ÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÁµêÊßãÂåñÁ∑®Á¢ºÊñπÊ°àÔºåÂÆÉËÅØÂêàÂª∫Ê®°ÂØ¶È©óÂÆ§Ê∏¨Ë©¶ÂÄºÂèäÂÖ∂Â∞çÊáâÁöÑÊôÇÈñìÊà≥ÔºåÂæûËÄåËÉΩÂ§†ÊòéÁ¢∫ÊçïÊçâÊôÇÈñì‰æùË≥¥ÊÄß„ÄÇÂú® MIMIC-IV Ë≥áÊñôÈõÜ‰∏äÁöÑÁ∂ìÈ©óË©ï‰º∞Ë°®ÊòéÔºåLab-MAE Âú®ÂåÖÊã¨ÂùáÊñπÊ†πË™§Â∑Æ (RMSE)„ÄÅR Âπ≥Êñπ (R2) Âíå Wasserstein Ë∑ùÈõ¢ (WD) Âú®ÂÖßÁöÑÂ§öÈ†ÖÊåáÊ®ô‰∏äÈ°ØËëóÂÑ™Êñº XGBoost Á≠âÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåLab-MAE Âú®ÊÇ£ËÄÖÁöÑ‰∫∫Âè£Áµ±Ë®àÁæ§ÁµÑ‰∏≠ÂèñÂæó‰∫ÜÂÖ¨Âπ≥ÁöÑË°®ÁèæÔºåÂæûËÄåÊèêÂçá‰∫ÜËá®Â∫äÈ†êÊ∏¨‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄß„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂‰∫ÜÂæåÁ∫åÂØ¶È©óÂÆ§ÂÄº‰ΩúÁÇ∫ÊΩõÂú®Êç∑ÂæëÁâπÂæµÁöÑ‰ΩúÁî®ÔºåÊè≠Á§∫‰∫Ü Lab-MAE Âú®Ê≠§È°ûË≥áÊñô‰∏çÂèØÁî®ÁöÑÊÉÖÊ≥Å‰∏ãÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÂü∫Êñº Transformer ÁöÑÊû∂ÊßãÔºàË™øÊï¥ÁÇ∫ EHR Ë≥áÊñôÁöÑÁâπÂæµÔºâÁÇ∫Êõ¥Ê∫ñÁ¢∫ÂíåÂÖ¨Âπ≥ÁöÑËá®Â∫ä‰º∞ÁÆóÊ®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂü∫Á§éÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊ∏¨Èáè‰∏¶ÊØîËºÉ‰∫Ü Lab-MAE ËàáÂü∫Ê∫ñ XGBoost Ê®°ÂûãÁöÑÁ¢≥Ë∂≥Ë∑°ÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂Áí∞Â¢ÉÈúÄÊ±Ç„ÄÇ</paragraph>

##### **Trust and Dependability in Blockchain & AI Based MedIoT Applications: Research Challenges and Future Directions**
2501.02647v1 by Ellis Solaiman, Christa Awad

This paper critically reviews the integration of Artificial Intelligence (AI)
and blockchain technologies in the context of Medical Internet of Things
(MedIoT) applications, where they collectively promise to revolutionize
healthcare delivery. By examining current research, we underscore AI's
potential in advancing diagnostics and patient care, alongside blockchain's
capacity to bolster data security and patient privacy. We focus particularly on
the imperative to cultivate trust and ensure reliability within these systems.
Our review highlights innovative solutions for managing healthcare data and
challenges such as ensuring scalability, maintaining privacy, and promoting
ethical practices within the MedIoT domain. We present a vision for integrating
AI-driven insights with blockchain security in healthcare, offering a
comprehensive review of current research and future directions. We conclude
with a set of identified research gaps and propose that addressing these is
crucial for achieving the dependable, secure, and patient -centric MedIoT
applications of tomorrow.

ÊëòË¶ÅÔºöÊú¨ÊñáÊâπÂà§ÊÄßÂú∞ÂõûÈ°ß‰∫Ü‰∫∫Â∑•Êô∫ÊÖß (AI) ÂíåÂçÄÂ°äÈèàÊäÄË°ìÂú®ÈÜ´ÁôÇÁâ©ËÅØÁ∂≤ (MedIoT) ÊáâÁî®‰∏≠ÁöÑÊï¥ÂêàÔºåÈÄôÂÖ©ËÄÖÂÖ±ÂêåÊâøË´æÂ∞áÂæπÂ∫ïÊîπËÆäÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãô„ÄÇÈÄèÈÅéÊ™¢Ë¶ñÁõÆÂâçÁöÑÁ†îÁ©∂ÊâÄÔºåÊàëÂÄëÂº∑Ë™ø AI Âú®Êé®ÈÄ≤Ë®∫Êñ∑ÂíåÊÇ£ËÄÖÁÖßË≠∑ÊñπÈù¢ÁöÑÊΩõÂäõÔºå‰ª•ÂèäÂçÄÂ°äÈèàÂº∑ÂåñË≥áÊñôÂÆâÂÖ®ÂíåÊÇ£ËÄÖÈö±ÁßÅÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁâπÂà•ÈóúÊ≥®Âú®ÈÄô‰∫õÁ≥ªÁµ±ÂÖßÂüπÈ§ä‰ø°‰ªªÂíåÁ¢∫‰øùÂèØÈù†ÊÄßÁöÑÂøÖË¶ÅÊÄß„ÄÇÊàëÂÄëÁöÑÂõûÈ°ßÈáçÈªûÂú®ÊñºÁÆ°ÁêÜÈÜ´ÁôÇ‰øùÂÅ•Ë≥áÊñôÁöÑÂâµÊñ∞Ëß£Ê±∫ÊñπÊ°àÔºå‰ª•ÂèäÁ¢∫‰øùÂèØÊì¥ÂÖÖÊÄß„ÄÅÁ∂≠Ë≠∑Èö±ÁßÅÂíåÂú® MedIoT È†òÂüüÂÖßÊé®Âª£ÈÅìÂæ∑ÂØ¶ÂãôÁ≠âÊåëÊà∞„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ∞á AI È©ÖÂãïÁöÑË¶ãËß£ËàáÂçÄÂ°äÈèàÂÆâÂÖ®Êï¥ÂêàÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÈ°òÊôØÔºåÊèê‰æõÁõÆÂâçÁöÑÁ†îÁ©∂ÊâÄÂíåÊú™‰æÜÊñπÂêëÁöÑÂÖ®Èù¢ÂõûÈ°ß„ÄÇÊàëÂÄë‰ª•‰∏ÄÁµÑÂ∑≤Ë≠òÂà•ÁöÑÁ†îÁ©∂Â∑ÆË∑ù‰ΩúÁÇ∫ÁµêË´ñÔºå‰∏¶ÊèêÂá∫Ëß£Ê±∫ÈÄô‰∫õÂ∑ÆË∑ùÂ∞çÊñºÈÅîÊàêÊú™‰æÜÂèØ‰ø°Ë≥¥„ÄÅÂÆâÂÖ®‰∏î‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑ MedIoT ÊáâÁî®Ëá≥ÈóúÈáçË¶Å„ÄÇ

##### **KM-UNet KAN Mamba UNet for medical image segmentation**
2501.02559v1 by Yibo Zhang

Medical image segmentation is a critical task in medical imaging analysis.
Traditional CNN-based methods struggle with modeling long-range dependencies,
while Transformer-based models, despite their success, suffer from quadratic
computational complexity. To address these limitations, we propose KM-UNet, a
novel U-shaped network architecture that combines the strengths of
Kolmogorov-Arnold Networks (KANs) and state-space models (SSMs). KM-UNet
leverages the Kolmogorov-Arnold representation theorem for efficient feature
representation and SSMs for scalable long-range modeling, achieving a balance
between accuracy and computational efficiency. We evaluate KM-UNet on five
benchmark datasets: ISIC17, ISIC18, CVC, BUSI, and GLAS. Experimental results
demonstrate that KM-UNet achieves competitive performance compared to
state-of-the-art methods in medical image segmentation tasks. To the best of
our knowledge, KM-UNet is the first medical image segmentation framework
integrating KANs and SSMs. This work provides a valuable baseline and new
insights for the development of more efficient and interpretable medical image
segmentation systems. The code is open source at
https://github.com/2760613195/KM_UNet
  Keywords:KAN,Manba, state-space models,UNet, Medical image segmentation, Deep
learning

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰∏≠ÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇ
ÂÇ≥Áµ±Âü∫Êñº CNN ÁöÑÊñπÊ≥ïÈõ£‰ª•Ê®°Êì¨Èï∑Ë∑ùÈõ¢‰æùË≥¥ÊÄßÔºå
ËÄåÂü∫Êñº Transformer ÁöÑÊ®°ÂûãÂÑòÁÆ°ÊàêÂäüÔºåÂçªÊúâ‰∫åÊ¨°Ë®àÁÆóË§áÈõúÂ∫¶ÁöÑÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü KM-UNetÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ U ÂΩ¢Á∂≤Ë∑ØÊû∂ÊßãÔºåÁµêÂêà‰∫Ü Kolmogorov-Arnold Á∂≤Ë∑Ø (KANs) ÂíåÁãÄÊÖãÁ©∫ÈñìÊ®°Âûã (SSM) ÁöÑÂÑ™Èªû„ÄÇKM-UNet Âà©Áî® Kolmogorov-Arnold Ë°®Á§∫ÂÆöÁêÜÈÄ≤Ë°åÈ´òÊïàÁâπÂæµË°®Á§∫ÔºåÂà©Áî® SSM ÈÄ≤Ë°åÂèØÊì¥ÂÖÖÈï∑Ë∑ùÈõ¢Ê®°Êì¨ÔºåÂú®Ê∫ñÁ¢∫Â∫¶ÂíåË®àÁÆóÊïàÁéá‰πãÈñìÂèñÂæóÂπ≥Ë°°„ÄÇÊàëÂÄëÂú®‰∫îÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äË©ï‰º∞ KM-UNetÔºöISIC17„ÄÅISIC18„ÄÅCVC„ÄÅBUSI Âíå GLAS„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàáÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤‰ªªÂãô‰∏≠ÁöÑÊúÄÂÖàÈÄ≤ÊñπÊ≥ïÁõ∏ÊØîÔºåKM-UNet ÈÅîÂà∞‰∫ÜÊúâÁ´∂Áà≠ÂäõÁöÑÊïàËÉΩ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåKM-UNet ÊòØÁ¨¨‰∏ÄÂÄãÊï¥Âêà KAN Âíå SSM ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Ê°ÜÊû∂„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫ÈñãÁôºÊõ¥ÊúâÊïàÁéá‰∏îÂèØËß£ÈáãÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Á≥ªÁµ±Êèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑÂü∫Á∑öÂíåÊñ∞Ë¶ãËß£„ÄÇÁ®ãÂºèÁ¢ºÂú® https://github.com/2760613195/KM_UNet ÈñãÊ∫ê
ÈóúÈçµÂ≠óÔºöKAN„ÄÅManba„ÄÅÁãÄÊÖãÁ©∫ÈñìÊ®°Âûã„ÄÅUNet„ÄÅÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤„ÄÅÊ∑±Â∫¶Â≠∏Áøí

##### **Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine**
2501.02471v1 by Yishen Liu, Shengda Luo, Zishao Zhong, Tongtong Wu, Jianguo Zhang, Peiyao Ou, Yong Liang, Liang Liu, Hudan Pan

Large language models (LLMs) primarily trained on English texts, often face
biases and inaccuracies in Chinese contexts. Their limitations are pronounced
in fields like Traditional Chinese Medicine (TCM), where cultural and clinical
subtleties are vital, further hindered by a lack of domain-specific data, such
as rheumatoid arthritis (RA). To address these issues, this paper introduces
Hengqin-RA-v1, the first large language model specifically tailored for TCM
with a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a
comprehensive RA-specific dataset curated from ancient Chinese medical
literature, classical texts, and modern clinical studies. This dataset empowers
Hengqin-RA-v1 to deliver accurate and culturally informed responses,
effectively bridging the gaps left by general-purpose models. Extensive
experiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,
even surpassing the diagnostic accuracy of TCM practitioners in certain cases.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏ªË¶Å‰ª•Ëã±ÊñáÊñáÊú¨ÈÄ≤Ë°åË®ìÁ∑¥ÔºåÂú®‰∏≠ÊñáË™ûÂ¢É‰∏≠Á∂ìÂ∏∏Èù¢Ëá®ÂÅèË¶ãÂíå‰∏çÊ∫ñÁ¢∫ÁöÑÂïèÈ°å„ÄÇÂÆÉÂÄëÁöÑÂ±ÄÈôêÊÄßÂú®‰∏≠ÈÜ´Á≠âÈ†òÂüüÂ∞§ÁÇ∫ÊòéÈ°ØÔºåÂõ†ÁÇ∫‰∏≠ÈÜ´Ê∂âÂèäÊñáÂåñÂíåËá®Â∫ä‰∏äÁöÑÂæÆÂ¶ô‰πãËôïÔºåËÄå‰∏îÈÇÑÁº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑÊï∏ÊìöÔºå‰æãÂ¶ÇÈ°ûÈ¢®ÊøïÈóúÁØÄÁÇéÔºàRAÔºâ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü Hengqin-RA-v1ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞àÈñÄÈáùÂ∞ç‰∏≠ÈÜ´ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåÈáçÈªûÊòØË®∫Êñ∑ÂíåÊ≤ªÁôÇ RA„ÄÇÊàëÂÄëÈÇÑÊèê‰æõ‰∫Ü HQ-GCM-RA-C1ÔºåÈÄôÊòØ‰∏ÄÂÄãÂæûÂè§‰ª£‰∏≠ÈÜ´ÊñáÁçª„ÄÅÂè§ÂÖ∏ÊñáÊú¨ÂíåÁèæ‰ª£Ëá®Â∫äÁ†îÁ©∂‰∏≠Êï¥ÁêÜÂá∫‰æÜÁöÑ„ÄÅÂÖ®Èù¢ÁöÑ RA ÁâπÂÆöÊï∏ÊìöÈõÜ„ÄÇÈÄôÂÄãÊï∏ÊìöÈõÜËÆì Hengqin-RA-v1 ËÉΩÂ§†Êèê‰æõÊ∫ñÁ¢∫‰∏îÁ¨¶ÂêàÊñáÂåñËÉåÊôØÁöÑÂõûÊáâÔºåÊúâÊïàÂú∞ÂΩåË£ú‰∫ÜÈÄöÁî®Ê®°ÂûãÁïô‰∏ãÁöÑÁ©∫ÁôΩ„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË°®ÊòéÔºåHengqin-RA-v1 ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÁîöËá≥Ë∂ÖÈÅé‰∫Ü‰∏≠ÈÜ´ÂæûÊ•≠ËÄÖÁöÑË®∫Êñ∑Ê∫ñÁ¢∫ÊÄß„ÄÇ

##### **Enhancing Contrastive Learning for Retinal Imaging via Adjusted Augmentation Scales**
2501.02451v1 by Zijie Cheng, Boxuan Li, Andr√© Altmann, Pearse A Keane, Yukun Zhou

Contrastive learning, a prominent approach within self-supervised learning,
has demonstrated significant effectiveness in developing generalizable models
for various applications involving natural images. However, recent research
indicates that these successes do not necessarily extend to the medical imaging
domain. In this paper, we investigate the reasons for this suboptimal
performance and hypothesize that the dense distribution of medical images poses
challenges to the pretext tasks in contrastive learning, particularly in
constructing positive and negative pairs. We explore model performance under
different augmentation strategies and compare the results to those achieved
with strong augmentations. Our study includes six publicly available datasets
covering multiple clinically relevant tasks. We further assess the model's
generalizability through external evaluations. The model pre-trained with weak
augmentation outperforms those with strong augmentation, improving AUROC from
0.838 to 0.848 and AUPR from 0.523 to 0.597 on MESSIDOR2, and showing similar
enhancements across other datasets. Our findings suggest that optimizing the
scale of augmentation is critical for enhancing the efficacy of contrastive
learning in medical imaging.

ÊëòË¶ÅÔºöÂ∞çÊØîÂ≠∏ÁøíÊòØËá™Áõ£Áù£Â≠∏Áøí‰∏≠‰∏ÄÁ®ÆÈáçË¶ÅÁöÑÊñπÊ≥ïÔºåÂú®Ê∂âÂèäËá™ÁÑ∂ÂΩ±ÂÉèÁöÑÂêÑÁ®ÆÊáâÁî®‰∏≠ÔºåÂ∑≤Â±ïÁèæÂá∫È°ØËëóÁöÑÊúâÊïàÊÄßÔºåÂèØÈñãÁôºÂá∫ÂèØÊ¶ÇÂåñÁöÑÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåÊúÄËøëÁöÑÁ†îÁ©∂ÊåáÂá∫ÔºåÈÄô‰∫õÊàêÂäü‰∏¶Êú™ÂøÖÁÑ∂Âª∂‰º∏Ëá≥ÈÜ´Â≠∏ÂΩ±ÂÉèÈ†òÂüü„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÈÄ†ÊàêÈÄôÁ®ÆÊ¨°‰Ω≥ÊïàËÉΩÁöÑÂéüÂõ†Ôºå‰∏¶ÂÅáË®≠ÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÂØÜÈõÜÂàÜ‰ΩàÂ∞çÊØîÂ≠∏Áøí‰∏≠ÁöÑËóâÂè£‰ªªÂãôÈÄ†ÊàêÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®Âª∫ÊßãÊ≠£Ë≤†Â∞çÊôÇ„ÄÇÊàëÂÄëÂú®‰∏çÂêåÁöÑÊì¥ÂÖÖÁ≠ñÁï•‰∏ãÊé¢Ë®éÊ®°ÂûãÊïàËÉΩÔºå‰∏¶Â∞áÁµêÊûúËàáÂº∑Êì¥ÂÖÖÊâÄÈÅîÊàêÁöÑÁµêÊûúÈÄ≤Ë°åÊØîËºÉ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ê∂µËìãÂÖ≠ÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑË≥áÊñôÈõÜÔºåÊ∂µËìãÂ§öÈ†ÖËá®Â∫äÁõ∏Èóú‰ªªÂãô„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄèÈÅéÂ§ñÈÉ®Ë©ï‰º∞‰æÜË©ï‰º∞Ê®°ÂûãÁöÑÂèØÊ¶ÇÂåñÊÄß„ÄÇ‰ΩøÁî®Âº±Êì¥ÂÖÖÈÄ≤Ë°åÈ†êË®ìÁ∑¥ÁöÑÊ®°ÂûãÂÑ™Êñº‰ΩøÁî®Âº∑Êì¥ÂÖÖÁöÑÊ®°ÂûãÔºåÂú® MESSIDOR2 ‰∏äÂ∞á AUROC Âæû 0.838 ÊèêÂçáËá≥ 0.848ÔºåÂ∞á AUPR Âæû 0.523 ÊèêÂçáËá≥ 0.597Ôºå‰∏¶Âú®ÂÖ∂‰ªñË≥áÊñôÈõÜ‰∏äÂ±ïÁèæÈ°û‰ººÁöÑÊèêÂçá„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÊúÄ‰Ω≥ÂåñÊì¥ÂÖÖË¶èÊ®°Â∞çÊñºÊèêÂçáÂ∞çÊØîÂ≠∏ÁøíÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÊïàËÉΩËá≥ÈóúÈáçË¶Å„ÄÇ

##### **Enhancing Workplace Productivity and Well-being Using AI Agent**
2501.02368v1 by Ravirajan K, Arvind Sundarajan

This paper discusses the use of Artificial Intelligence (AI) to enhance
workplace productivity and employee well-being. By integrating machine learning
(ML) techniques with neurobiological data, the proposed approaches ensure
alignment with human ethical standards through value alignment models and
Hierarchical Reinforcement Learning (HRL) for autonomous task management. The
system utilizes biometric feedback from employees to generate personalized
health prompts, fostering a supportive work environment that encourages
physical activity. Additionally, we explore decentralized multi-agent systems
for improved collaboration and decision-making frameworks that enhance
transparency. Various approaches using ML techniques in conjunction with AI
implementations are discussed. Together, these innovations aim to create a more
productive and health-conscious workplace. These outcomes assist HR management
and organizations in launching more rational career progression streams for
employees and facilitating organizational transformation.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®éÂà©Áî®‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâ‰æÜÊèêÂçáËÅ∑Â†¥ÁîüÁî¢ÂäõÂíåÂì°Â∑•Á¶èÁ•â„ÄÇÈÄèÈÅéÂ∞áÊ©üÂô®Â≠∏ÁøíÔºàMLÔºâÊäÄË°ìËàáÁ•ûÁ∂ìÁîüÁâ©Â≠∏Ë≥áÊñôÊï¥ÂêàÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÁ¢∫‰øùÈÄèÈÅéÂÉπÂÄºÂ∞çÈΩäÊ®°ÂûãÂíåÁî®ÊñºËá™‰∏ª‰ªªÂãôÁÆ°ÁêÜÁöÑÂàÜÂ±§Âº∑ÂåñÂ≠∏ÁøíÔºàHRLÔºâËàá‰∫∫È°ûÂÄ´ÁêÜÊ®ôÊ∫ñ‰øùÊåÅ‰∏ÄËá¥„ÄÇË©≤Á≥ªÁµ±Âà©Áî®Âì°Â∑•ÁöÑÁîüÁâ©ÁâπÂæµÂõûÈ•ã‰æÜÁî¢ÁîüÂÄã‰∫∫ÂåñÂÅ•Â∫∑ÊèêÁ§∫ÔºåÁáüÈÄ†ÊîØÊåÅÊÄßÁöÑÂ∑•‰ΩúÁí∞Â¢ÉÔºåÈºìÂãµË∫´È´îÊ¥ªÂãï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®éÂàÜÊï£ÂºèÂ§öÊô∫ËÉΩÈ´îÁ≥ªÁµ±Ôºå‰ª•ÊîπÂñÑÂçî‰ΩúÂíåÊ±∫Á≠ñÂà∂ÂÆöÊû∂ÊßãÔºåÈÄ≤ËÄåÊèêÂçáÈÄèÊòéÂ∫¶„ÄÇÊú¨ÊñáË®éË´ñ‰∫ÜÂêÑÁ®ÆÁµêÂêà ML ÊäÄË°ìËàá AI ÂØ¶‰ΩúÁöÑÊñπÊ≥ï„ÄÇÁ∏ΩËÄåË®Ä‰πãÔºåÈÄô‰∫õÂâµÊñ∞Êó®Âú®ÂâµÈÄ†‰∏ÄÂÄãÊõ¥ÂÖ∑ÁîüÁî¢Âäõ‰∏îÊ≥®ÈáçÂÅ•Â∫∑ÁöÑËÅ∑Â†¥„ÄÇÈÄô‰∫õÊàêÊûúÊúâÂä©Êñº‰∫∫ÂäõË≥áÊ∫êÁÆ°ÁêÜÂíåÁµÑÁπîÊ©üÊßãÁÇ∫Âì°Â∑•ÂïüÂãïÊõ¥ÂêàÁêÜÁöÑËÅ∑Ê∂ØÁôºÂ±ïÁÆ°ÈÅìÔºå‰∏¶‰øÉÈÄ≤ÁµÑÁπîËΩâÂûã„ÄÇ

##### **Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support**
2501.02346v1 by Florian Putz, Marlen Haderleina, Sebastian Lettmaier, Sabine Semrau, Rainer Fietkau, Yixing Huang

Thanks to the rapidly evolving integration of LLMs into decision-support
tools, a significant transformation is happening across large-scale systems.
Like other medical fields, the use of LLMs such as GPT-4 is gaining increasing
interest in radiation oncology as well. An attempt to assess GPT-4's
performance in radiation oncology was made via a dedicated 100-question
examination on the highly specialized topic of radiation oncology physics,
revealing GPT-4's superiority over other LLMs. GPT-4's performance on a broader
field of clinical radiation oncology is further benchmarked by the ACR
Radiation Oncology In-Training (TXIT) exam where GPT-4 achieved a high accuracy
of 74.57%. Its performance on re-labelling structure names in accordance with
the AAPM TG-263 report has also been benchmarked, achieving above 96%
accuracies. Such studies shed light on the potential of LLMs in radiation
oncology. As interest in the potential and constraints of LLMs in general
healthcare applications continues to rise5, the capabilities and limitations of
LLMs in radiation oncology decision support have not yet been fully explored.

ÊëòË¶ÅÔºöÈö®Ëëó LLM Âø´ÈÄüÊºîÈÄ≤Êï¥ÂêàÂà∞Ê±∫Á≠ñÊîØÊè¥Â∑•ÂÖ∑‰∏≠ÔºåÂ§ßË¶èÊ®°Á≥ªÁµ±Ê≠£Âú®ÁôºÁîüÈáçÂ§ßËΩâËÆä„ÄÇ
ËàáÂÖ∂‰ªñÈÜ´ÁôÇÈ†òÂüü‰∏ÄÊ®£ÔºåLLMÔºà‰æãÂ¶Ç GPT-4ÔºâÁöÑ‰ΩøÁî®‰πüÂú®ÊîæÂ∞ÑËÖ´Áò§Â≠∏‰∏≠Áç≤ÂæóË∂ä‰æÜË∂äÂ§öÁöÑËààË∂£„ÄÇÈÄèÈÅéÈáùÂ∞çÊîæÂ∞ÑËÖ´Áò§Â≠∏Áâ©ÁêÜÂ≠∏ÈÄôÂÄãÈ´òÂ∫¶Â∞àÊ•≠ÁöÑ‰∏ªÈ°åÈÄ≤Ë°å 100 È°åÂ∞àÈñÄËÄÉË©¶ÔºåË©¶ÂúñË©ï‰º∞ GPT-4 Âú®ÊîæÂ∞ÑËÖ´Áò§Â≠∏‰∏≠ÁöÑË°®ÁèæÔºåÊè≠Á§∫‰∫Ü GPT-4 ÂÑ™ÊñºÂÖ∂‰ªñ LLM„ÄÇGPT-4 Âú®Êõ¥Âª£Ê≥õÁöÑËá®Â∫äÊîæÂ∞ÑËÖ´Áò§Â≠∏È†òÂüüÁöÑË°®ÁèæÈÄ≤‰∏ÄÊ≠•Áî± ACR ÊîæÂ∞ÑËÖ´Áò§Â≠∏Âú®ËÅ∑Ë®ìÁ∑¥ (TXIT) ËÄÉË©¶ÈÄ≤Ë°åË©ïÈáèÔºåGPT-4 Âú®ÂÖ∂‰∏≠ÂèñÂæó 74.57% ÁöÑÈ´òÊ∫ñÁ¢∫Â∫¶„ÄÇÂÆÉÊ†πÊìö AAPM TG-263 Â†±ÂëäÈáçÊñ∞Ê®ôË®òÁµêÊßãÂêçÁ®±ÁöÑË°®Áèæ‰πüÂ∑≤ÈÄ≤Ë°åË©ïÈáèÔºåÊ∫ñÁ¢∫Â∫¶ÈÅîÂà∞ 96% ‰ª•‰∏ä„ÄÇÈÄô‰∫õÁ†îÁ©∂Êè≠Á§∫‰∫Ü LLM Âú®ÊîæÂ∞ÑËÖ´Áò§Â≠∏‰∏≠ÁöÑÊΩõÂäõ„ÄÇÁî±Êñº‰∫∫ÂÄëÊåÅÁ∫åÂ∞ç LLM Âú®‰∏ÄËà¨ÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰∏≠ÁöÑÊΩõÂäõÂíåÈôêÂà∂ÊÑüËààË∂£5ÔºåLLM Âú®ÊîæÂ∞ÑËÖ´Áò§Â≠∏Ê±∫Á≠ñÊîØÊè¥‰∏≠ÁöÑÂäüËÉΩÂíåÈôêÂà∂Â∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ

##### **Deep Learning-Driven Segmentation of Ischemic Stroke Lesions Using Multi-Channel MRI**
2501.02287v1 by Ashiqur Rahman, Muhammad E. H. Chowdhury, Md Sharjis Ibne Wadud, Rusab Sarmun, Adam Mushtak, Sohaib Bassam Zoghoul, Israa Al-Hashimi

Ischemic stroke, caused by cerebral vessel occlusion, presents substantial
challenges in medical imaging due to the variability and subtlety of stroke
lesions. Magnetic Resonance Imaging (MRI) plays a crucial role in diagnosing
and managing ischemic stroke, yet existing segmentation techniques often fail
to accurately delineate lesions. This study introduces a novel deep
learning-based method for segmenting ischemic stroke lesions using
multi-channel MRI modalities, including Diffusion Weighted Imaging (DWI),
Apparent Diffusion Coefficient (ADC), and enhanced Diffusion Weighted Imaging
(eDWI). The proposed architecture integrates DenseNet121 as the encoder with
Self-Organized Operational Neural Networks (SelfONN) in the decoder, enhanced
by Channel and Space Compound Attention (CSCA) and Double
Squeeze-and-Excitation (DSE) blocks. Additionally, a custom loss function
combining Dice Loss and Jaccard Loss with weighted averages is introduced to
improve model performance. Trained and evaluated on the ISLES 2022 dataset, the
model achieved Dice Similarity Coefficients (DSC) of 83.88% using DWI alone,
85.86% with DWI and ADC, and 87.49% with the integration of DWI, ADC, and eDWI.
This approach not only outperforms existing methods but also addresses key
limitations in current segmentation practices. These advancements significantly
enhance diagnostic precision and treatment planning for ischemic stroke,
providing valuable support for clinical decision-making.

ÊëòË¶ÅÔºöÁº∫Ë°ÄÊÄß‰∏≠È¢®ÊòØÁî±ËÖ¶Ë°ÄÁÆ°ÈòªÂ°ûÊâÄÂºïËµ∑ÔºåÁî±Êñº‰∏≠È¢®ÁóÖÁÅ∂ÁöÑÂèØËÆäÊÄßÂíåÈö±ËîΩÊÄßÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÈÄ†ÊàêÁõ∏Áï∂Â§ßÁöÑÊåëÊà∞„ÄÇÁ£ÅÊåØÈÄ†ÂΩ± (MRI) Âú®Ë®∫Êñ∑ÂíåÊ≤ªÁôÇÁº∫Ë°ÄÊÄß‰∏≠È¢®‰∏≠ÊâÆÊºîËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤Ôºå‰ΩÜÁèæÊúâÁöÑÂàÜÂâ≤ÊäÄË°ìÂ∏∏Â∏∏ÁÑ°Ê≥ïÊ∫ñÁ¢∫Âú∞ÊèèÁπ™ÁóÖÁÅ∂„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºå‰ΩøÁî®Â§öÈÄöÈÅì MRI Ê®°ÂºèÂ∞çÁº∫Ë°ÄÊÄß‰∏≠È¢®ÁóÖÁÅ∂ÈÄ≤Ë°åÂàÜÂâ≤ÔºåÂåÖÊã¨Êì¥Êï£Âä†Ê¨äÂΩ±ÂÉè (DWI)„ÄÅË°®ËßÄÊì¥Êï£‰øÇÊï∏ (ADC) ÂíåÂ¢ûÂº∑ÂûãÊì¥Êï£Âä†Ê¨äÂΩ±ÂÉè (eDWI)„ÄÇÊâÄÊèêÂá∫ÁöÑÊû∂ÊßãÂ∞á DenseNet121 Êï¥ÂêàÁÇ∫Á∑®Á¢ºÂô®Ôºå‰∏¶Âú®Ëß£Á¢ºÂô®‰∏≠‰ΩøÁî®Ëá™ÁµÑÁπîÈÅãÁÆóÁ•ûÁ∂ìÁ∂≤Ë∑Ø (SelfONN)Ôºå‰∏¶Áî±ÈÄöÈÅìÂíåÁ©∫ÈñìË§áÂêàÊ≥®ÊÑèÂäõ (CSCA) ÂíåÈõôÈáçÊì†Â£ìÊøÄÂãµ (DSE) ÂçÄÂ°äÈÄ≤Ë°åÂä†Âº∑„ÄÇÊ≠§Â§ñÔºåÈÇÑÂºïÈÄ≤‰∫Ü‰∏ÄÂÄãËá™Ë®ÇÁöÑÊêçÂ§±ÂáΩÊï∏ÔºåÁµêÂêà‰∫Ü Dice ÊêçÂ§±Âíå Jaccard ÊêçÂ§±‰ª•ÂèäÂä†Ê¨äÂπ≥ÂùáÔºå‰ª•ÊèêÂçáÊ®°ÂûãÊïàËÉΩ„ÄÇÂú® ISLES 2022 Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÂíåË©ï‰º∞ÔºåË©≤Ê®°Âûã‰ΩøÁî® DWI ÂñÆÁç®ÊôÇÈÅîÂà∞ 83.88% ÁöÑ Dice Áõ∏‰ººÊÄß‰øÇÊï∏ (DSC)Ôºå‰ΩøÁî® DWI Âíå ADC ÊôÇÈÅîÂà∞ 85.86%Ôºå‰ΩøÁî® DWI„ÄÅADC Âíå eDWI Êï¥ÂêàÊôÇÈÅîÂà∞ 87.49%„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰∏çÂÉÖÂÑ™ÊñºÁèæÊúâÊñπÊ≥ïÔºåÈÇÑËÉΩËß£Ê±∫Áï∂ÂâçÂàÜÂâ≤ÂØ¶Âãô‰∏≠ÁöÑ‰∏ªË¶ÅÈôêÂà∂„ÄÇÈÄô‰∫õÈÄ≤Â±ïÈ°ØËëóÊèêÂçá‰∫ÜÁº∫Ë°ÄÊÄß‰∏≠È¢®ÁöÑË®∫Êñ∑Á≤æÊ∫ñÂ∫¶ÂíåÊ≤ªÁôÇË¶èÂäÉÔºåÁÇ∫Ëá®Â∫äÊ±∫Á≠ñÊèê‰æõÊúâÂÉπÂÄºÁöÑÊîØÊè¥„ÄÇ

##### **The Integration of Blockchain and Artificial Intelligence for Secure Healthcare Systems**
2501.02169v1 by Umar Safdar, Simon Gabrael

Verisign reported a 125 percent increase in data breaches within the
healthcare sector in the United States during 2022, with 18.2 million patient
records being impacted. Growing healthcare data volumes and diversification
mean that medical information is becoming more valuable. Many Health Centers
use various technologies to ease the classification, storage, and exchange of
big data. This use can also make the health data of the users at risk and
vulnerable. AI and blockchain are among the leading technologies at hand. With
AI, data-driven operations and big data efficiency have been improved with
respect to traditional techniques. Due to its potential to bring about
improvements in health services and lower medical costs, this AI technology is
regularly used in healthcare. Blockchain helps protect transactions on sharing
information and private privacy as long as the exchange of knowledge is that of
the standard. The objective of this analysis is to investigate the research and
unique contributions since 2008 regarding blockchain-integrated AI and
healthcare systems. The work sheds light on applied AI-based healthcare schemes
with machine, ballistic, and acrylic learning and disparate blockchain
structures. The use of technology in order to ensure patient data security and
manage medical information effectively in healthcare settings offers a highly
successful position for both healthcare providers and patients. From 2018 to
2021, the best year was 2021 to grow, enhancing everything to examine the
download of the device and the counting of Google Academies, for which the
joining perspective was borrowed; local research experts were asked, identified
articles in recent years, and read reviews of large research grants.

ÊëòË¶ÅÔºöVerisign Â†±Âëä 2022 Âπ¥ÁæéÂúãÈÜ´ÁôÇ‰øùÂÅ•ÈÉ®ÈñÄÁöÑË≥áÊñôÂ§ñÊ¥©‰∫ã‰ª∂Â¢ûÂä†‰∫Ü 125%ÔºåÂΩ±Èüø‰∫Ü 1,820 Ëê¨Á≠ÜÁóÖÊ≠∑„ÄÇÈÜ´ÁôÇ‰øùÂÅ•Ë≥áÊñôÈáè‰∏çÊñ∑Â¢ûÂä†‰∏îÂ§öÂÖÉÂåñÔºåÈÄôË°®Á§∫ÈÜ´ÁôÇË≥áË®äËÆäÂæóÊõ¥ÊúâÂÉπÂÄº„ÄÇË®±Â§öÈÜ´ÁôÇ‰∏≠ÂøÉ‰ΩøÁî®ÂêÑÁ®ÆÊäÄË°ìÔºå‰ª•Á∞°ÂåñÂ§ßÊï∏ÊìöÁöÑÂàÜÈ°û„ÄÅÂÑ≤Â≠òÂíå‰∫§Êèõ„ÄÇÈÄôÁ®Æ‰ΩøÁî®ÊñπÂºè‰πüÂèØËÉΩ‰Ωø‰ΩøÁî®ËÄÖÁöÑÂÅ•Â∫∑Ë≥áÊñôÈù¢Ëá®È¢®Èö™ÂíåËÑÜÂº±ÊÄß„ÄÇ‰∫∫Â∑•Êô∫ÊÖßÂíåÂçÄÂ°äÈèàÊòØÁèæÊúâÁöÑÈ†òÂÖàÊäÄË°ì„ÄÇÈÄèÈÅé‰∫∫Â∑•Êô∫ÊÖßÔºåË≥áÊñôÈ©ÖÂãïÁöÑÈÅã‰ΩúÂíåÂ§ßÊï∏ÊìöÊïàÁéáÂ∑≤Áõ∏ËºÉÊñºÂÇ≥Áµ±ÊäÄË°ìÁç≤ÂæóÊîπÂñÑ„ÄÇÁî±Êñº‰∫∫Â∑•Êô∫ÊÖßÊäÄË°ìÊúâÊΩõÂäõÊîπÂñÑÈÜ´ÁôÇÊúçÂãô‰∏¶Èôç‰ΩéÈÜ´ÁôÇÊàêÊú¨ÔºåÂõ†Ê≠§Á∂ìÂ∏∏Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠‰ΩøÁî®„ÄÇÂçÄÂ°äÈèàÊúâÂä©Êñº‰øùË≠∑‰∫§ÊòìÔºåÂú®Ë≥áË®äÂÖ±‰∫´ÂíåÈö±ÁßÅÊñπÈù¢ÔºåÂè™Ë¶ÅÁü•Ë≠òÁöÑ‰∫§ÊèõÊòØÊ®ôÊ∫ñÁöÑ„ÄÇÊú¨ÂàÜÊûêÁöÑÁõÆÊ®ôÊòØË™øÊü•Ëá™ 2008 Âπ¥‰ª•‰æÜËàáÂçÄÂ°äÈèàÊï¥Âêà‰∫∫Â∑•Êô∫ÊÖßÂíåÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±Áõ∏ÈóúÁöÑÁ†îÁ©∂ÂíåÁç®ÁâπË≤¢Áçª„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈó°Êòé‰∫ÜÊáâÁî®‰∫∫Â∑•Êô∫ÊÖßÁÇ∫Âü∫Á§éÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ë®àÁï´ÔºåÂåÖÊã¨Ê©üÂô®„ÄÅÂΩàÈÅìÂíå‰∏ôÁÉØÈÖ∏Â≠∏Áøí‰ª•Âèä‰∏çÂêåÁöÑÂçÄÂ°äÈèàÁµêÊßã„ÄÇÁÇ∫‰∫ÜÁ¢∫‰øùÁóÖÊÇ£Ë≥áÊñôÂÆâÂÖ®‰∏¶Âú®ÈÜ´ÁôÇ‰øùÂÅ•Áí∞Â¢É‰∏≠ÊúâÊïàÁÆ°ÁêÜÈÜ´ÁôÇË≥áË®äÔºå‰ΩøÁî®ÊäÄË°ìÁÇ∫ÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖÂíåÁóÖÊÇ£Êèê‰æõ‰∫ÜÊ•µÁÇ∫ÊàêÂäüÁöÑÂÆö‰Ωç„ÄÇÂæû 2018 Âπ¥Âà∞ 2021 Âπ¥ÔºåÊúÄÈÅ©ÂêàÊàêÈï∑ÁöÑÊòØ 2021 Âπ¥ÔºåÂä†Âº∑ÊâÄÊúâ‰∏ÄÂàáÔºå‰ª•Ê™¢Êü•Ë£ùÁΩÆÁöÑ‰∏ãËºâÂíå Google Â≠∏Ë°ìÁöÑË®àÊï∏ÔºåÂÄüÁî®‰∫ÜÂä†ÂÖ•ÁöÑËßÄÈªûÔºõË©¢Âïè‰∫ÜÁï∂Âú∞Á†îÁ©∂Â∞àÂÆ∂ÔºåÊâæÂá∫ËøëÂπ¥‰æÜÁöÑÊñáÁ´†Ôºå‰∏¶Èñ±ËÆÄÂ§ßÂûãÁ†îÁ©∂Ë£úÂä©ÈáëÁöÑË©ïË´ñ„ÄÇ

##### **Online Detection of Water Contamination Under Concept Drift**
2501.02107v1 by Jin Li, Kleanthis Malialis, Stelios G. Vrachimis, Marios M. Polycarpou

Water Distribution Networks (WDNs) are vital infrastructures, and
contamination poses serious public health risks. Harmful substances can
interact with disinfectants like chlorine, making chlorine monitoring essential
for detecting contaminants. However, chlorine sensors often become unreliable
and require frequent calibration. This study introduces the Dual-Threshold
Anomaly and Drift Detection (AD&DD) method, an unsupervised approach combining
a dual-threshold drift detection mechanism with an LSTM-based Variational
Autoencoder(LSTM-VAE) for real-time contamination detection. Tested on two
realistic WDNs, AD&DD effectively identifies anomalies with sensor offsets as
concept drift, and outperforms other methods. A proposed decentralized
architecture enables accurate contamination detection and localization by
deploying AD&DD on selected nodes.

ÊëòË¶ÅÔºöÈÖçÊ∞¥Á∂≤Ë∑Ø (WDN) ÊòØÈáçË¶ÅÁöÑÂü∫Á§éË®≠ÊñΩÔºåËÄåÊ±°ÊüìÊúÉÈÄ†ÊàêÂö¥ÈáçÁöÑÂÖ¨ÂÖ±Ë°õÁîüÈ¢®Èö™„ÄÇÊúâÂÆ≥Áâ©Ë≥™ÂèØËÉΩÊúÉËàáÊ∂àÊØíÂäëÔºàÂ¶ÇÊ∞ØÊ∞£Ôºâ‰∫§‰∫í‰ΩúÁî®ÔºåÂõ†Ê≠§Áõ£Ê∏¨Ê∞ØÊ∞£Â∞çÊñºÂÅµÊ∏¨Ê±°ÊüìÁâ©Ëá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÊ∞ØÊ∞£ÊÑüÊ∏¨Âô®Â∏∏Â∏∏ËÆäÂæó‰∏çÂèØÈù†ÔºåÈúÄË¶ÅÈ†ªÁπÅÊ†°Ê≠£„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫ÈõôÈñæÂÄºÁï∞Â∏∏ËàáÊºÇÁßªÂÅµÊ∏¨ (AD&DD) ÊñπÊ≥ïÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈùûÁõ£Áù£ÂºèÊñπÊ≥ïÔºåÁµêÂêàÈõôÈñæÂÄºÊºÇÁßªÂÅµÊ∏¨Ê©üÂà∂ËàáÂü∫Êñº LSTM ÁöÑËÆäÁï∞Ëá™ÂãïÁ∑®Á¢ºÂô® (LSTM-VAE)ÔºåÁî®ÊñºÂç≥ÊôÇÊ±°ÊüìÂÅµÊ∏¨„ÄÇÂú®ÂÖ©ÂÄãÂØ¶ÈöõÁöÑ WDN ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåAD&DD ËÉΩÊúâÊïàÂú∞Â∞áÊÑüÊ∏¨Âô®ÂÅèÁßªË¶ñÁÇ∫Ê¶ÇÂøµÊºÇÁßªÔºå‰∏¶ÂÑ™ÊñºÂÖ∂‰ªñÊñπÊ≥ï„ÄÇÊâÄÊèêÂá∫ÁöÑÂàÜÊï£ÂºèÊû∂ÊßãËÉΩÈÄèÈÅéÂú®ÈÅ∏ÂÆöÁöÑÁØÄÈªûÈÉ®ÁΩ≤ AD&DDÔºåÂØ¶ÁèæÁ≤æÁ¢∫ÁöÑÊ±°ÊüìÂÅµÊ∏¨ËàáÂÆö‰Ωç„ÄÇ

##### **METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring**
2501.02045v1 by Ollie Liu, Sami Jaghouar, Johannes Hagemann, Shangshang Wang, Jason Wiemels, Jeff Kaufman, Willie Neiswanger

We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer
model, which we refer to as a metagenomic foundation model, on a novel corpus
of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base
pairs. This dataset is sourced from a large collection of human wastewater
samples, processed and sequenced using deep metagenomic (next-generation)
sequencing methods. Unlike genomic models that focus on individual genomes or
curated sets of specific species, the aim of METAGENE-1 is to capture the full
distribution of genomic information present within this wastewater, to aid in
tasks relevant to pandemic monitoring and pathogen detection. We carry out
byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic
sequences, and then pretrain our model. In this paper, we first detail the
pretraining dataset, tokenization strategy, and model architecture,
highlighting the considerations and design choices that enable the effective
modeling of metagenomic data. We then show results of pretraining this model on
our metagenomic dataset, providing details about our losses, system metrics,
and training stability over the course of pretraining. Finally, we demonstrate
the performance of METAGENE-1, which achieves state-of-the-art results on a set
of genomic benchmarks and new evaluations focused on human-pathogen detection
and genomic sequence embedding, showcasing its potential for public health
applications in pandemic monitoring, biosurveillance, and early detection of
emerging health threats.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÈ†êË®ìÁ∑¥‰∫Ü‰∏ÄÂÄãÂÖ∑Êúâ 70 ÂÑÑÂÄãÂèÉÊï∏ÁöÑËá™Ëø¥Ê≠∏ËΩâÊèõÂô®Ê®°Âûã METAGENE-1ÔºåÊàëÂÄëÁ®±‰πãÁÇ∫ÂÆèÂü∫Âõ†ÁµÑÂü∫Á§éÊ®°ÂûãÔºåÂÆÉÂª∫Á´ãÂú®‰∏ÄÂÄãÊñ∞Á©éÁöÑË™ûÊñôÂ∫´‰∏äÔºåÂÖ∂‰∏≠ÂåÖÂê´Ë∂ÖÈÅé 1.5 ÂÖÜÂÄãÈπºÂü∫Â∞çÁöÑÂ§öÊ®£ÂåñÂÆèÂü∫Âõ†ÁµÑ DNA Âíå RNA Â∫èÂàó„ÄÇÊ≠§Êï∏ÊìöÈõÜ‰æÜËá™Â§ßÈáè‰∫∫È°ûÂª¢Ê∞¥Ê®£Êú¨Ôºå‰ΩøÁî®Ê∑±Â∫¶ÂÆèÂü∫Âõ†ÁµÑÔºà‰∏ã‰∏Ä‰ª£ÔºâÂÆöÂ∫èÊñπÊ≥ïÈÄ≤Ë°åËôïÁêÜÂíåÂÆöÂ∫è„ÄÇËàáÂ∞àÊ≥®ÊñºÂÄãÂà•Âü∫Âõ†ÁµÑÊàñÁâπÂÆöÁâ©Á®ÆÁ≠ñÂäÉÈõÜÂêàÁöÑÂü∫Âõ†ÁµÑÊ®°Âûã‰∏çÂêåÔºåMETAGENE-1 ÁöÑÁõÆÊ®ôÊòØÊì∑ÂèñÊ≠§Âª¢Ê∞¥‰∏≠Â≠òÂú®ÁöÑÂü∫Âõ†ÁµÑË≥áË®äÁöÑÂÆåÊï¥ÂàÜ‰ΩàÔºå‰ª•ÂçîÂä©ËàáÂ§ßÊµÅË°åÁõ£Ê∏¨ÂíåÁóÖÂéüÈ´îÊ™¢Ê∏¨Áõ∏ÈóúÁöÑ‰ªªÂãô„ÄÇÊàëÂÄëÂ∞çÊï∏ÊìöÈõÜÂü∑Ë°åÈáùÂ∞çÂÆèÂü∫Âõ†ÁµÑÂ∫èÂàóÈáèË∫´ÊâìÈÄ†ÁöÑ‰ΩçÂÖÉÁµÑÂ∞çÁ∑®Á¢º (BPE) Ê®ôË®òÂåñÔºåÁÑ∂ÂæåÈ†êË®ìÁ∑¥ÊàëÂÄëÁöÑÊ®°Âûã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàË©≥Á¥∞Ë™™ÊòéÈ†êË®ìÁ∑¥Êï∏ÊìöÈõÜ„ÄÅÊ®ôË®òÂåñÁ≠ñÁï•ÂíåÊ®°ÂûãÊû∂ÊßãÔºåÈáçÈªûË™™ÊòéËÉΩÊúâÊïàÂª∫Ê®°ÂÆèÂü∫Âõ†ÁµÑÊï∏ÊìöÁöÑËÄÉÈáèÂíåË®≠Ë®àÈÅ∏Êìá„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂú®ÊàëÂÄëÁöÑÂÆèÂü∫Âõ†ÁµÑÊï∏ÊìöÈõÜ‰∏äÈ†êË®ìÁ∑¥Ê≠§Ê®°ÂûãÁöÑÁµêÊûúÔºåÊèê‰æõÊúâÈóúÊàëÂÄëÁöÑÊêçÂ§±„ÄÅÁ≥ªÁµ±ÊåáÊ®ôÂíåÂú®È†êË®ìÁ∑¥ÈÅéÁ®ã‰∏≠Ë®ìÁ∑¥Á©©ÂÆöÊÄßÁöÑË©≥Á¥∞Ë≥áË®ä„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü METAGENE-1 ÁöÑÊïàËÉΩÔºåÂÆÉÂú®ÈáùÂ∞ç‰∫∫È°ûÁóÖÂéüÈ´îÊ™¢Ê∏¨ÂíåÂü∫Âõ†ÁµÑÂ∫èÂàóÂµåÂÖ•ÁöÑ‰∏ÄÁµÑÂü∫Âõ†ÁµÑÂü∫Ê∫ñÂíåÊñ∞Ë©ï‰º∞‰∏≠ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûúÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂÖ¨ÂÖ±Ë°õÁîüÊáâÁî®‰∏≠ÁöÑÊΩõÂäõÔºåÂåÖÊã¨Â§ßÊµÅË°åÁõ£Ê∏¨„ÄÅÁîüÁâ©Áõ£ÊéßÂíåÊñ∞ËààÂÅ•Â∫∑Â®ÅËÑÖÁöÑÊó©ÊúüÊ™¢Ê∏¨„ÄÇ</paragraph>

##### **Advancing Pancreatic Cancer Prediction with a Next Visit Token Prediction Head on top of Med-BERT**
2501.02044v1 by Jianping He, Laila Rasmy, Degui Zhi, Cui Tao

Background: Recently, numerous foundation models pretrained on extensive data
have demonstrated efficacy in disease prediction using Electronic Health
Records (EHRs). However, there remains some unanswered questions on how to best
utilize such models especially with very small fine-tuning cohorts. Methods: We
utilized Med-BERT, an EHR-specific foundation model, and reformulated the
disease binary prediction task into a token prediction task and a next visit
mask token prediction task to align with Med-BERT's pretraining task format in
order to improve the accuracy of pancreatic cancer (PaCa) prediction in both
few-shot and fully supervised settings. Results: The reformulation of the task
into a token prediction task, referred to as Med-BERT-Sum, demonstrates
slightly superior performance in both few-shot scenarios and larger data
samples. Furthermore, reformulating the prediction task as a Next Visit Mask
Token Prediction task (Med-BERT-Mask) significantly outperforms the
conventional Binary Classification (BC) prediction task (Med-BERT-BC) by 3% to
7% in few-shot scenarios with data sizes ranging from 10 to 500 samples. These
findings highlight that aligning the downstream task with Med-BERT's
pretraining objectives substantially enhances the model's predictive
capabilities, thereby improving its effectiveness in predicting both rare and
common diseases. Conclusion: Reformatting disease prediction tasks to align
with the pretraining of foundation models enhances prediction accuracy, leading
to earlier detection and timely intervention. This approach improves treatment
effectiveness, survival rates, and overall patient outcomes for PaCa and
potentially other cancers.

ÊëòË¶ÅÔºöËÉåÊôØÔºöÊúÄËøëÔºåÂ§ßÈáèÂü∫‰∫éÂπøÊ≥õÊï∞ÊçÆËøõË°åÈ¢ÑËÆ≠ÁªÉÁöÑÂü∫Á°ÄÊ®°ÂûãÂ∑≤ËØÅÊòéÂú®‰ΩøÁî®ÁîµÂ≠êÂÅ•Â∫∑ËÆ∞ÂΩï (EHR) È¢ÑÊµãÁñæÁóÖÊñπÈù¢ÊúâÊïà„ÄÇÁÑ∂ËÄåÔºåÂÖ≥‰∫éÂ¶Ç‰ΩïÊúÄÂ•ΩÂú∞Âà©Áî®Ê≠§Á±ªÊ®°ÂûãÔºàÂ∞§ÂÖ∂ÊòØÂú®ÊûÅÂ∞èÂæÆË∞ÉÈòüÂàó‰∏≠Ôºâ‰ªçÊúâ‰∏Ä‰∫õÊú™Ëß£ÂÜ≥ÁöÑÈóÆÈ¢ò„ÄÇÊñπÊ≥ïÔºöÊàë‰ª¨Âà©Áî®‰∫Ü EHR ÁâπÂÆöÁöÑÂü∫Á°ÄÊ®°Âûã Med-BERTÔºåÂπ∂Â∞ÜÁñæÁóÖ‰∫åÂÖÉÈ¢ÑÊµã‰ªªÂä°ÈáçÊñ∞Ë°®Ëø∞‰∏∫Ê†áËÆ∞È¢ÑÊµã‰ªªÂä°Âíå‰∏ãÊ¨°ËÆøÈóÆÊé©Á†ÅÊ†áËÆ∞È¢ÑÊµã‰ªªÂä°Ôºå‰ª•‰∏é Med-BERT ÁöÑÈ¢ÑËÆ≠ÁªÉ‰ªªÂä°Ê†ºÂºè‰øùÊåÅ‰∏ÄËá¥Ôºå‰ªéËÄåÊèêÈ´òËÉ∞ËÖ∫Áôå (PaCa) È¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÔºåÊó†ËÆ∫ÊòØÂú®Â∞èÊ†∑Êú¨ËøòÊòØÂÆåÂÖ®ÁõëÁù£ÁöÑËÆæÁΩÆ‰∏≠„ÄÇÁªìÊûúÔºöÂ∞Ü‰ªªÂä°ÈáçÊñ∞Ë°®Ëø∞‰∏∫Ê†áËÆ∞È¢ÑÊµã‰ªªÂä°ÔºàÁß∞‰∏∫ Med-BERT-SumÔºâÔºåÂú®Â∞èÊ†∑Êú¨Âú∫ÊôØÂíåËæÉÂ§ßÊï∞ÊçÆÊ†∑Êú¨‰∏≠ÂùáË°®Áé∞Âá∫Áï•ÂæÆ‰ºòË∂äÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂ∞ÜÈ¢ÑÊµã‰ªªÂä°ÈáçÊñ∞Ë°®Ëø∞‰∏∫‰∏ã‰∏ÄÊ¨°ËÆøÈóÆÊé©Á†ÅÊ†áËÆ∞È¢ÑÊµã‰ªªÂä°ÔºàMed-BERT-MaskÔºâÂú®Â∞èÊ†∑Êú¨Âú∫ÊôØ‰∏≠ÊòéÊòæ‰ºò‰∫é‰º†ÁªüÁöÑ‰∫åÂÖÉÂàÜÁ±ª (BC) È¢ÑÊµã‰ªªÂä°ÔºàMed-BERT-BCÔºâÔºåÊï∞ÊçÆÂ§ßÂ∞è‰ªé 10 Âà∞ 500 ‰∏™Ê†∑Êú¨‰∏çÁ≠âÔºå‰ºòË∂äÂπÖÂ∫¶‰∏∫ 3% Âà∞ 7%„ÄÇËøô‰∫õÂèëÁé∞Âº∫Ë∞ÉÔºåÂ∞Ü‰∏ãÊ∏∏‰ªªÂä°‰∏é Med-BERT ÁöÑÈ¢ÑËÆ≠ÁªÉÁõÆÊ†á‰øùÊåÅ‰∏ÄËá¥ÔºåÂèØ‰ª•ÊòæÁùÄÂ¢ûÂº∫Ê®°ÂûãÁöÑÈ¢ÑÊµãËÉΩÂäõÔºå‰ªéËÄåÊèêÈ´òÂÖ∂È¢ÑÊµãÁΩïËßÅÁñæÁóÖÂíåÂ∏∏ËßÅÁñæÁóÖÁöÑÊúâÊïàÊÄß„ÄÇÁªìËÆ∫ÔºöÈáçÊñ∞Ê†ºÂºèÂåñÁñæÁóÖÈ¢ÑÊµã‰ªªÂä°‰ª•‰∏éÂü∫Á°ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉ‰øùÊåÅ‰∏ÄËá¥ÔºåÂèØÊèêÈ´òÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÔºå‰ªéËÄåÂÆûÁé∞Êó©ÊúüÊ£ÄÊµãÂíåÂèäÊó∂Âπ≤È¢Ñ„ÄÇËøôÁßçÊñπÊ≥ïÊèêÈ´ò‰∫Ü PaCa ÂíåÂÖ∂‰ªñÁôåÁóáÁöÑÊ≤ªÁñóÊïàÊûú„ÄÅÂ≠òÊ¥ªÁéáÂíåÊÇ£ËÄÖÊÄª‰ΩìÈ¢ÑÂêé„ÄÇ

##### **Combined Hyper-Extensible Extremely-Secured Zero-Trust CIAM-PAM architecture**
2501.01732v1 by Shivom Aggarwal, Shourya Mehra, Safeer Sathar

Customer Identity and Access Management (CIAM) systems play a pivotal role in
securing enterprise infrastructures. However, the complexity of implementing
these systems requires careful architectural planning to ensure positive Return
on Investment (RoI) and avoid costly delays. The proliferation of Active
Persistent cyber threats, coupled with advancements in AI, cloud computing, and
geographically distributed customer populations, necessitates a paradigm shift
towards adaptive and zero-trust security frameworks. This paper introduces the
Combined Hyper-Extensible Extremely-Secured Zero-Trust (CHEZ) CIAM-PAM
architecture, designed specifically for large-scale enterprises. The CHEZ PL
CIAM-PAM framework addresses critical security gaps by integrating federated
identity management (private and public identities), password-less
authentication, adaptive multi-factor authentication (MFA), microservice-based
PEP (Policy Entitlement Point), multi-layer RBAC (Role Based Access Control)
and multi-level trust systems. This future-proof design also includes
end-to-end data encryption, and seamless integration with state-of-the-art
AI-based threat detection systems, while ensuring compliance with stringent
regulatory standards.

ÊëòË¶ÅÔºöÂÆ¢Êà∂Ë∫´ÂàÜËàáÂ≠òÂèñÁÆ°ÁêÜ (CIAM) Á≥ªÁµ±Âú®Á¢∫‰øù‰ºÅÊ•≠Âü∫Á§éË®≠ÊñΩÂÆâÂÖ®ÊñπÈù¢ÊâÆÊºîËëóÈóúÈçµËßíËâ≤„ÄÇÁÑ∂ËÄåÔºåÂØ¶‰ΩúÈÄô‰∫õÁ≥ªÁµ±ÁöÑË§áÈõúÊÄßÈúÄË¶Å‰ªîÁ¥∞ÁöÑÊû∂ÊßãË¶èÂäÉÔºå‰ª•Á¢∫‰øùÊäïË≥áÂ†±ÈÖ¨Áéá (RoI) ÁÇ∫Ê≠£Ôºå‰∏¶ÈÅøÂÖçÊàêÊú¨È´òÊòÇÁöÑÂª∂Ë™§„ÄÇ‰∏ªÂãïÊåÅÁ∫åÁöÑÁ∂≤Ë∑ØÂ®ÅËÑÖÁöÑÊì¥Êï£ÔºåÂä†‰∏ä‰∫∫Â∑•Êô∫ÊÖß„ÄÅÈõ≤Á´ØÈÅãÁÆóÂíåÂú∞ÁêÜÂàÜÂ∏ÉÁöÑÂÆ¢Êà∂Áæ§ÁöÑÈÄ≤Ê≠•ÔºåÈúÄË¶ÅÊúùÂêëÈÅ©ÊáâÊÄßÂíåÈõ∂‰ø°‰ªªÂÆâÂÖ®Êû∂ÊßãËΩâËÆä„ÄÇÊú¨Êñá‰ªãÁ¥πÂ∞àÁÇ∫Â§ßÂûã‰ºÅÊ•≠Ë®≠Ë®àÁöÑ Combined Hyper-Extensible Extremely-Secured Zero-Trust (CHEZ) CIAM-PAM Êû∂Êßã„ÄÇCHEZ PL CIAM-PAM Êû∂ÊßãÈÄèÈÅéÊï¥ÂêàËÅØÂêàË∫´ÂàÜÁÆ°ÁêÜÔºàÁßÅ‰∫∫ÂíåÂÖ¨Áî®Ë∫´ÂàÜÔºâ„ÄÅÁÑ°ÂØÜÁ¢ºÈ©óË≠â„ÄÅÈÅ©ÊáâÊÄßÂ§öÈáçË∫´ÂàÜÈ©óË≠â (MFA)„ÄÅÂü∫ÊñºÂæÆÊúçÂãôÁöÑ PEPÔºàÊîøÁ≠ñÊéàÊ¨äÈªûÔºâ„ÄÅÂ§öÂ±§ RBACÔºàÂü∫ÊñºËßíËâ≤ÁöÑÂ≠òÂèñÊéßÂà∂ÔºâÂíåÂ§öÂ±§Á¥ö‰ø°‰ªªÁ≥ªÁµ±‰æÜËß£Ê±∫ÈóúÈçµÁöÑÂÆâÂÖ®ÊºèÊ¥û„ÄÇÈÄôÁ®ÆÂÖ∑ÂÇôÊú™‰æÜÊÄßÁöÑË®≠Ë®à‰πüÂåÖÂê´Á´ØÂ∞çÁ´ØË≥áÊñôÂä†ÂØÜÔºå‰∏¶ËàáÊúÄÂÖàÈÄ≤ÁöÑÂü∫Êñº‰∫∫Â∑•Êô∫ÊÖßÁöÑÂ®ÅËÑÖÂÅµÊ∏¨Á≥ªÁµ±ÁÑ°Á∏´Êï¥ÂêàÔºåÂêåÊôÇÁ¢∫‰øùÁ¨¶ÂêàÂö¥Ê†ºÁöÑÊ≥ïË¶èÊ®ôÊ∫ñ„ÄÇ

##### **EAUWSeg: Eliminating annotation uncertainty in weakly-supervised medical image segmentation**
2501.01658v1 by Wang Lituan, Zhang Lei, Wang Yan, Wang Zhenbin, Zhang Zhenwei, Zhang Yi

Weakly-supervised medical image segmentation is gaining traction as it
requires only rough annotations rather than accurate pixel-to-pixel labels,
thereby reducing the workload for specialists. Although some progress has been
made, there is still a considerable performance gap between the label-efficient
methods and fully-supervised one, which can be attributed to the uncertainty
nature of these weak labels. To address this issue, we propose a novel weak
annotation method coupled with its learning framework EAUWSeg to eliminate the
annotation uncertainty. Specifically, we first propose the Bounded Polygon
Annotation (BPAnno) by simply labeling two polygons for a lesion. Then, the
tailored learning mechanism that explicitly treat bounded polygons as two
separated annotations is proposed to learn invariant feature by providing
adversarial supervision signal for model training. Subsequently, a
confidence-auxiliary consistency learner incorporates with a
classification-guided confidence generator is designed to provide reliable
supervision signal for pixels in uncertain region by leveraging the feature
presentation consistency across pixels within the same category as well as
class-specific information encapsulated in bounded polygons annotation.
Experimental results demonstrate that EAUWSeg outperforms existing
weakly-supervised segmentation methods. Furthermore, compared to
fully-supervised counterparts, the proposed method not only delivers superior
performance but also costs much less annotation workload. This underscores the
superiority and effectiveness of our approach.

ÊëòË¶ÅÔºöÂº±ÁõëÁù£ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÂâ≤Ê≠£Ëé∑ÂæóÂÖ≥Ê≥®ÔºåÂõ†‰∏∫ÂÆÉÂè™ÈúÄË¶ÅÁ≤óÁï•ÁöÑÊ≥®ÈáäÔºåËÄå‰∏çÊòØÁ≤æÁ°ÆÁöÑÂÉèÁ¥†Âà∞ÂÉèÁ¥†Ê†áÁ≠æÔºå‰ªéËÄåÂáèÂ∞ë‰∫Ü‰∏ìÂÆ∂ÁöÑÂ∑•‰ΩúÈáè„ÄÇÂ∞ΩÁÆ°ÂèñÂæó‰∫Ü‰∏Ä‰∫õËøõÂ±ïÔºå‰ΩÜÂú®Ê†áÁ≠æÈ´òÊïàÊñπÊ≥ïÂíåÂÆåÂÖ®ÁõëÁù£ÊñπÊ≥ï‰πãÈó¥‰ªçÁÑ∂Â≠òÂú®Áõ∏ÂΩìÂ§ßÁöÑÊÄßËÉΩÂ∑ÆË∑ùÔºåËøôÂèØÂΩíÂõ†‰∫éËøô‰∫õÂº±Ê†áÁ≠æÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº±Ê≥®ÈáäÊñπÊ≥ïÔºåÂπ∂ÁªìÂêàÂÖ∂Â≠¶‰π†Ê°ÜÊû∂ EAUWSeg Êù•Ê∂àÈô§Ê≥®ÈáäÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨È¶ñÂÖàÈÄöËøáÁÆÄÂçïÂú∞‰∏∫ÁóÖÁÅ∂Ê†áËÆ∞‰∏§‰∏™Â§öËæπÂΩ¢Êù•ÊèêÂá∫ÊúâÁïåÂ§öËæπÂΩ¢Ê≥®Èáä (BPAnno)„ÄÇÁÑ∂ÂêéÔºåÊèêÂá∫‰∫ÜÂ∞ÜÊúâÁïåÂ§öËæπÂΩ¢ÊòéÁ°ÆÂú∞ËßÜ‰∏∫‰∏§‰∏™ÂàÜÁ¶ªÊ≥®ÈáäÁöÑÂÆöÂà∂Â≠¶‰π†Êú∫Âà∂Ôºå‰ª•ÈÄöËøá‰∏∫Ê®°ÂûãËÆ≠ÁªÉÊèê‰æõÂØπÊäóÊÄßÁõëÁù£‰ø°Âè∑Êù•Â≠¶‰π†‰∏çÂèòÁâπÂæÅ„ÄÇÈöèÂêéÔºåÁΩÆ‰ø°ËæÖÂä©‰∏ÄËá¥ÊÄßÂ≠¶‰π†Âô®‰∏éÂàÜÁ±ªÂºïÂØºÁΩÆ‰ø°Â∫¶ÁîüÊàêÂô®ÁªìÂêàËÆæËÆ°Ôºå‰ª•ÈÄöËøáÂà©Áî®Âêå‰∏ÄÁ±ªÂà´ÂÜÖÂÉèÁ¥†ÁöÑÁâπÂæÅË°®Á§∫‰∏ÄËá¥ÊÄß‰ª•ÂèäÊúâÁïåÂ§öËæπÂΩ¢Ê≥®Èáä‰∏≠Â∞ÅË£ÖÁöÑÁâπÂÆö‰∫éÁ±ªÁöÑ‰ø°ÊÅØÔºå‰∏∫‰∏çÁ°ÆÂÆöÂå∫Âüü‰∏≠ÁöÑÂÉèÁ¥†Êèê‰æõÂèØÈù†ÁöÑÁõëÁù£‰ø°Âè∑„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåEAUWSeg ‰ºò‰∫éÁé∞ÊúâÁöÑÂº±ÁõëÁù£ÂàÜÂâ≤ÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºå‰∏éÂÆåÂÖ®ÁõëÁù£ÁöÑÂØπÂ∫îÊñπÊ≥ïÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï‰∏ç‰ªÖÊèê‰æõ‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩÔºåËÄå‰∏îÊ≥®ÈáäÂ∑•‰ΩúÈáè‰πüÂ§ßÂ§ßÂáèÂ∞ë„ÄÇËøôÁ™ÅÂá∫‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑ‰ºòË∂äÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

##### **Implications of Artificial Intelligence on Health Data Privacy and Confidentiality**
2501.01639v2 by Ahmad Momani

The rapid integration of artificial intelligence (AI) in healthcare is
revolutionizing medical diagnostics, personalized medicine, and operational
efficiency. However, alongside these advancements, significant challenges arise
concerning patient data privacy, ethical considerations, and regulatory
compliance. This paper examines the dual impact of AI on healthcare,
highlighting its transformative potential and the critical need for
safeguarding sensitive health information. It explores the role of the Health
Insurance Portability and Accountability Act (HIPAA) as a regulatory framework
for ensuring data privacy and security, emphasizing the importance of robust
safeguards and ethical standards in AI-driven healthcare. Through case studies,
including AI applications in diabetic retinopathy, oncology, and the
controversies surrounding data sharing, this study underscores the ethical and
legal complexities of AI implementation. A balanced approach that fosters
innovation while maintaining patient trust and privacy is imperative. The
findings emphasize the importance of continuous education, transparency, and
adherence to regulatory frameworks to harness AI's full potential responsibly
and ethically in healthcare.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑÂø´ÈÄüÊï¥ÂêàÔºåÊ≠£Âú®ÂæπÂ∫ïËÆäÈù©ÈÜ´ÁôÇË®∫Êñ∑„ÄÅÂÄã‰∫∫ÂåñÈÜ´ÁôÇÂíåÁáüÈÅãÊïàÁéá„ÄÇÁÑ∂ËÄåÔºåÈö®ËëóÈÄô‰∫õÈÄ≤Ê≠•Ôºå‰πüÂá∫Áèæ‰∫ÜÈóúÊñºÊÇ£ËÄÖË≥áÊñôÈö±ÁßÅ„ÄÅÂÄ´ÁêÜËÄÉÈáèÂíåÊ≥ïË¶èÈÅµÂæ™ÁöÑÈáçÂ§ßÊåëÊà∞„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫Ü AI Â∞çÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÈõôÈáçÂΩ±ÈüøÔºåÂº∑Ë™øÂÖ∂ËΩâÂûãÊΩõÂäõ‰ª•Âèä‰øùË≠∑ÊïèÊÑüÂÅ•Â∫∑Ë≥áË®äÁöÑÈóúÈçµÈúÄÊ±Ç„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÂÅ•Â∫∑‰øùÈö™ÂèØÊîúÊÄßÂíåË≤¨‰ªªÊ≥ïÊ°à (HIPAA) ‰ΩúÁÇ∫Á¢∫‰øùË≥áÊñôÈö±ÁßÅÂíåÂÆâÂÖ®ÁöÑÊ≥ïË¶èÊû∂ÊßãÁöÑËßíËâ≤ÔºåÂº∑Ë™øÂú® AI È©ÖÂãïÁöÑÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂÅ•ÂÖ®‰øùÈöúÊé™ÊñΩÂíåÈÅìÂæ∑Ê®ôÊ∫ñÁöÑÈáçË¶ÅÊÄß„ÄÇÊú¨Á†îÁ©∂ÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÔºåÂåÖÊã¨ AI Âú®Á≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆä„ÄÅËÖ´Áò§Â≠∏‰∏≠ÁöÑÊáâÁî®Ôºå‰ª•ÂèäÂúçÁπûË≥áÊñôÂÖ±‰∫´ÁöÑÁà≠Ë≠∞ÔºåÂº∑Ë™ø‰∫Ü AI ÂØ¶ÊñΩÁöÑÂÄ´ÁêÜÂíåÊ≥ïÂæãË§áÈõúÊÄß„ÄÇ‰∏ÄÁ®ÆÂπ≥Ë°°ÁöÑÊñπÊ≥ïÔºåÂú®‰øÉÈÄ≤ÂâµÊñ∞ÁöÑÂêåÊôÇÔºåÁ∂≠Ë≠∑ÊÇ£ËÄÖÁöÑ‰ø°‰ªªÂíåÈö±ÁßÅÔºåËá≥ÈóúÈáçË¶Å„ÄÇÁ†îÁ©∂ÁµêÊûúÂº∑Ë™ø‰∫ÜÊåÅÁ∫åÊïôËÇ≤„ÄÅÈÄèÊòéÂ∫¶ÂíåÈÅµÂÆàÊ≥ïË¶èÊ°ÜÊû∂ÁöÑÈáçË¶ÅÊÄßÔºå‰ª•Ë≤†Ë≤¨‰ªª‰∏îÂêà‰πéÈÅìÂæ∑ÁöÑÊñπÂºèÂà©Áî® AI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÂÖ®ÈÉ®ÊΩõÂäõ„ÄÇ

##### **Merging Context Clustering with Visual State Space Models for Medical Image Segmentation**
2501.01618v1 by Yun Zhu, Dong Zhang, Yi Lin, Yifei Feng, Jinhui Tang

Medical image segmentation demands the aggregation of global and local
feature representations, posing a challenge for current methodologies in
handling both long-range and short-range feature interactions. Recently, vision
mamba (ViM) models have emerged as promising solutions for addressing model
complexities by excelling in long-range feature iterations with linear
complexity. However, existing ViM approaches overlook the importance of
preserving short-range local dependencies by directly flattening spatial tokens
and are constrained by fixed scanning patterns that limit the capture of
dynamic spatial context information. To address these challenges, we introduce
a simple yet effective method named context clustering ViM (CCViM), which
incorporates a context clustering module within the existing ViM models to
segment image tokens into distinct windows for adaptable local clustering. Our
method effectively combines long-range and short-range feature interactions,
thereby enhancing spatial contextual representations for medical image
segmentation tasks. Extensive experimental evaluations on diverse public
datasets, i.e., Kumar, CPM17, ISIC17, ISIC18, and Synapse demonstrate the
superior performance of our method compared to current state-of-the-art
methods. Our code can be found at https://github.com/zymissy/CCViM.

ÊëòË¶ÅÔºöÈÜ´ÁôÇÂΩ±ÂÉèÂàÜÂâ≤ÈúÄË¶ÅËÅöÂêàÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÁâπÂæµË°®Á§∫ÔºåÂ∞çÁï∂ÂâçÊñπÊ≥ïËôïÁêÜÈï∑Á®ãÂíåÁü≠Á®ãÁâπÂæµ‰∫§‰∫íÊßãÊàêÊåëÊà∞„ÄÇÊúÄËøëÔºåË¶ñË¶∫ÊõºÂ∑¥ (ViM) Ê®°ÂûãÂ∑≤ÊàêÁÇ∫Ëß£Ê±∫Ê®°ÂûãË§áÈõúÊÄßÁöÑÊúâÂâçÈÄîÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂÆÉÂú®Á∑öÊÄßË§áÈõúÂ∫¶‰∏ãÊìÖÈï∑Èï∑Á®ãÁâπÂæµËø≠‰ª£„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑ ViM ÊñπÊ≥ïÂøΩË¶ñ‰∫ÜÈÄèÈÅéÁõ¥Êé•Â£ìÂπ≥Á©∫ÈñìÊ®ôË®ò‰æÜ‰øùÁïôÁü≠Á®ãÂ±ÄÈÉ®‰æùË≥¥ÊÄßÁöÑÈáçË¶ÅÊÄßÔºå‰∏¶‰∏îÂèóÂà∞ÈôêÂà∂ÁöÑÊéÉÊèèÊ®°ÂºèÁöÑÁ¥ÑÊùüÔºåÈÄôÊúÉÈôêÂà∂ÂãïÊÖãÁ©∫ÈñìËÉåÊôØË≥áË®äÁöÑÊì∑Âèñ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂêçÁÇ∫ËÉåÊôØËÅöÈ°û ViM (CCViM) ÁöÑÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂÆÉÂú®ÁèæÊúâÁöÑ ViM Ê®°Âûã‰∏≠Âä†ÂÖ•‰∫Ü‰∏ÄÂÄãËÉåÊôØËÅöÈ°ûÊ®°ÁµÑÔºåÂ∞áÂΩ±ÂÉèÊ®ôË®òÂàÜÂâ≤Êàê‰∏çÂêåÁöÑË¶ñÁ™óÔºå‰ª•ÈÄ≤Ë°åÈÅ©ÊáâÊÄßÂ±ÄÈÉ®ËÅöÈ°û„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÊúâÊïàÂú∞ÁµêÂêà‰∫ÜÈï∑Á®ãÂíåÁü≠Á®ãÁâπÂæµ‰∫§‰∫íÔºåÂæûËÄåÂ¢ûÂº∑‰∫ÜÁî®ÊñºÈÜ´ÁôÇÂΩ±ÂÉèÂàÜÂâ≤‰ªªÂãôÁöÑÁ©∫ÈñìËÉåÊôØË°®Á§∫„ÄÇÂú®ÂêÑÁ®ÆÂÖ¨ÈñãË≥áÊñôÈõÜÔºàÂç≥ Kumar„ÄÅCPM17„ÄÅISIC17„ÄÅISIC18 Âíå SynapseÔºâ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË©ï‰º∞Ë≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïËàáÁï∂ÂâçÊúÄÂÖàÈÄ≤ÊñπÊ≥ïÁõ∏ÊØîÂÖ∑ÊúâÂçìË∂äÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØ‰ª•Âú® https://github.com/zymissy/CCViM ÊâæÂà∞„ÄÇ

##### **PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents**
2501.01594v1 by Jingoo Lee, Kyungho Lim, Young-Chul Jung, Byung-Hoon Kim

Recent advances in large language models (LLMs) have accelerated the
development of conversational agents capable of generating human-like
responses. Since psychiatric assessments typically involve complex
conversational interactions between psychiatrists and patients, there is
growing interest in developing LLM-based psychiatric assessment conversational
agents (PACAs) that aim to simulate the role of psychiatrists in clinical
evaluations. However, standardized methods for benchmarking the clinical
appropriateness of PACAs' interaction with patients still remain underexplored.
Here, we propose PSYCHE, a novel framework designed to enable the 1) clinically
relevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation
of PACAs. This is achieved by simulating psychiatric patients based on a
multi-faceted psychiatric construct that defines the simulated patients'
profiles, histories, and behaviors, which PACAs are expected to assess. We
validate the effectiveness of PSYCHE through a study with 10 board-certified
psychiatrists, supported by an in-depth analysis of the simulated patient
utterances.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂä†ÈÄü‰∫ÜÊúÉË©±‰ª£ÁêÜÁöÑÈñãÁôºÔºåÈÄô‰∫õ‰ª£ÁêÜËÉΩÂ§†Áî¢ÁîüÈ°û‰ºº‰∫∫È°ûÁöÑÂõûÊáâ„ÄÇÁî±ÊñºÁ≤æÁ•ûÁßëË©ï‰º∞ÈÄöÂ∏∏Ê∂âÂèäÁ≤æÁ•ûÁßëÈÜ´Â∏´ÂíåÊÇ£ËÄÖ‰πãÈñìË§áÈõúÁöÑÊúÉË©±‰∫íÂãïÔºåÂõ†Ê≠§Â∞çÊñºÈñãÁôºÂü∫Êñº LLM ÁöÑÁ≤æÁ•ûÁßëË©ï‰º∞ÊúÉË©±‰ª£ÁêÜ (PACA) ÁöÑËààË∂£ËàáÊó•‰ø±Â¢ûÔºåÈÄô‰∫õ‰ª£ÁêÜÊó®Âú®Ê®°Êì¨Á≤æÁ•ûÁßëÈÜ´Â∏´Âú®Ëá®Â∫äË©ï‰º∞‰∏≠ÁöÑËßíËâ≤„ÄÇÁÑ∂ËÄåÔºåÁî®ÊñºË©ïÈáè PACA ËàáÊÇ£ËÄÖ‰∫íÂãïÁöÑËá®Â∫äÈÅ©Áï∂ÊÄßÁöÑÊ®ôÊ∫ñÂåñÊñπÊ≥ï‰ªçÊú™Ë¢´ÂÖÖÂàÜÊé¢Ë®é„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ PSYCHEÔºå‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÂØ¶Áèæ 1) Ëá®Â∫äÁõ∏Èóú„ÄÅ2) ÈÅìÂæ∑ÂÆâÂÖ®„ÄÅ3) ÊàêÊú¨ÊïàÁõäÔºå‰ª•Âèä 4) PACA ÁöÑÂÆöÈáèË©ï‰º∞„ÄÇÈÄôÊòØÈÄèÈÅéÊ®°Êì¨Âü∫ÊñºÂ§öÈù¢ÂêëÁ≤æÁ•ûÁßëÂª∫ÊßãÁöÑÁ≤æÁ•ûÁßëÊÇ£ËÄÖ‰æÜÂØ¶ÁèæÁöÑÔºåË©≤Âª∫ÊßãÂÆöÁæ©‰∫ÜÊ®°Êì¨ÊÇ£ËÄÖÁöÑÂÄã‰∫∫Ë≥áÊñô„ÄÅÁóÖÂè≤ÂíåË°åÁÇ∫ÔºåËÄå PACA È†êË®àÊúÉË©ï‰º∞ÈÄô‰∫õÂÖßÂÆπ„ÄÇÊàëÂÄëÈÄèÈÅé‰∏ÄÈ†ÖÊúâ 10 ‰ΩçÁ∂ìË™çË≠âÁöÑÁ≤æÁ•ûÁßëÈÜ´Â∏´ÂèÉËàáÁöÑÁ†îÁ©∂È©óË≠â‰∫Ü PSYCHE ÁöÑÊúâÊïàÊÄßÔºå‰∏¶Ëºî‰ª•Â∞çÊ®°Êì¨ÊÇ£ËÄÖË©±Ë™ûÁöÑÊ∑±ÂÖ•ÂàÜÊûê„ÄÇ

##### **Model Checking in Medical Imaging for Tumor Detection and Segmentation**
2501.02024v2 by Elhoucine Elfatimi, Lahcen El fatimi

Recent advancements in model checking have demonstrated significant potential
across diverse applications, particularly in signal and image analysis. Medical
imaging stands out as a critical domain where model checking can be effectively
applied to design and evaluate robust frameworks. These frameworks facilitate
automatic and semi-automatic delineation of regions of interest within images,
aiding in accurate segmentation. This paper provides a comprehensive analysis
of recent works leveraging spatial logic to develop operators and tools for
identifying regions of interest, including tumorous and non-tumorous areas.
Additionally, we examine the challenges inherent to spatial model-checking
techniques, such as variability in ground truth data and the need for
streamlined procedures suitable for routine clinical practice.

ÊëòË¶ÅÔºöËøë‰æÜÊ®°ÂûãÊ™¢ÂÆöÁöÑÈÄ≤Â±ïÈ°ØÁ§∫Âá∫Âú®ÂêÑÁ®ÆÊáâÁî®‰∏≠ÂÖ∑ÊúâÈ°ØËëóÁöÑÊΩõÂäõÔºåÁâπÂà•ÊòØÂú®Ë®äËôüÂíåÂΩ±ÂÉèÂàÜÊûê‰∏≠„ÄÇÈÜ´ÁôÇÊàêÂÉè‰ΩúÁÇ∫‰∏ÄÂÄãÈóúÈçµÈ†òÂüüÔºåÊ®°ÂûãÊ™¢ÂÆöÂèØ‰ª•ÊúâÊïàÂú∞ÊáâÁî®ÊñºË®≠Ë®àÂíåË©ï‰º∞Á©©ÂÅ•ÁöÑÊû∂Êßã„ÄÇÈÄô‰∫õÊû∂ÊßãÊúâÂä©ÊñºËá™ÂãïÂíåÂçäËá™ÂãïÂú∞ÊèèÁπ™ÂΩ±ÂÉè‰∏≠ÁöÑÊÑüËààË∂£ÂçÄÂüüÔºåÊúâÂä©ÊñºÊ∫ñÁ¢∫ÁöÑÂàÜÂâ≤„ÄÇÊú¨ÊñáÂ∞çËøë‰æÜÂà©Áî®Á©∫ÈñìÈÇèËºØÈñãÁôºÈÅãÁÆóÂ≠êÂíåÂ∑•ÂÖ∑‰ª•Ë≠òÂà•ÊÑüËààË∂£ÂçÄÂüüÔºàÂåÖÊã¨ËÖ´Áò§ÂíåÈùûËÖ´Áò§ÂçÄÂüüÔºâÁöÑÁõ∏ÈóúÁ†îÁ©∂ÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂàÜÊûê„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÁ©∫ÈñìÊ®°ÂûãÊ™¢ÂÆöÊäÄË°ìÂõ∫ÊúâÁöÑÊåëÊà∞Ôºå‰æãÂ¶ÇÂü∫Êú¨‰∫ãÂØ¶Ë≥áÊñôÁöÑÂèØËÆäÊÄß‰ª•ÂèäÂ∞çÈÅ©ÂêàÂ∏∏Ë¶èËá®Â∫äÂØ¶ÂãôÁöÑÁ∞°ÂåñÁ®ãÂ∫èÁöÑÈúÄÊ±Ç„ÄÇ

##### **Training Medical Large Vision-Language Models with Abnormal-Aware Feedback**
2501.01377v1 by Yucheng Zhou, Lingran Song, Jianbing Shen

Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate
extensive medical knowledge, demonstrate excellent capabilities in
understanding medical images and responding to human queries based on these
images. However, there remain challenges in visual localization in medical
images, which is crucial for abnormality detection and interpretation. To
address these issues, we propose a novel UMed-LVLM designed with Unveiling
Medical abnormalities. Specifically, we collect a Medical Abnormalities
Unveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM
training. To collect MAU dataset, we propose a prompt method utilizing the
GPT-4V to generate diagnoses based on identified abnormal areas in medical
images. Moreover, the two-stage training method includes Abnormal-Aware
Instruction Tuning and Abnormal-Aware Rewarding, comprising Abnormal
Localization Rewarding and Vision Relevance Rewarding. Experimental results
demonstrate that our UMed-LVLM surpasses existing Med-LVLMs in identifying and
understanding medical abnormality. In addition, this work shows that enhancing
the abnormality detection capabilities of Med-LVLMs significantly improves
their understanding of medical images and generalization capability.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÈÜ´ÁôÇÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (Med-LVLMs) Â∞ÅË£ù‰∫ÜÂª£Ê≥õÁöÑÈÜ´ÁôÇÁü•Ë≠òÔºåÂú®ÁêÜËß£ÈÜ´ÁôÇÂΩ±ÂÉèÂíåÊ†πÊìöÈÄô‰∫õÂΩ±ÂÉèÂõûÊáâ‰∫∫È°ûÊü•Ë©¢ÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÜ´ÁôÇÂΩ±ÂÉè‰∏≠ÈÄ≤Ë°åË¶ñË¶∫ÂÆö‰Ωç‰ªçÂ≠òÂú®ÊåëÊà∞ÔºåÈÄôÂ∞çÊñºÁï∞Â∏∏ÂÅµÊ∏¨ÂíåËß£ËÆÄËá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ UMed-LVLMÔºåÂÖ∂Ë®≠Ë®àÁî®ÊñºÊè≠Á§∫ÈÜ´ÁôÇÁï∞Â∏∏„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊî∂ÈõÜ‰∫Ü‰∏ÄÂÄãÈÜ´ÁôÇÁï∞Â∏∏Êè≠Á§∫ (MAU) Ë≥áÊñôÈõÜÔºå‰∏¶ÁÇ∫ UMed-LVLM Ë®ìÁ∑¥ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ©ÈöéÊÆµË®ìÁ∑¥ÊñπÊ≥ï„ÄÇÁÇ∫‰∫ÜÊî∂ÈõÜ MAU Ë≥áÊñôÈõÜÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊèêÁ§∫ÊñπÊ≥ïÔºåÂà©Áî® GPT-4V Ê†πÊìöÈÜ´ÁôÇÂΩ±ÂÉè‰∏≠Ë≠òÂà•Âá∫ÁöÑÁï∞Â∏∏ÂçÄÂüüÁîüÊàêË®∫Êñ∑„ÄÇÊ≠§Â§ñÔºåÂÖ©ÈöéÊÆµË®ìÁ∑¥ÊñπÊ≥ïÂåÖÊã¨Áï∞Â∏∏ÊÑüÁü•ÊåáÂ∞éË™øÊï¥ÂíåÁï∞Â∏∏ÊÑüÁü•ÁçéÂãµÔºåÂåÖÊã¨Áï∞Â∏∏ÂÆö‰ΩçÁçéÂãµÂíåË¶ñË¶∫Áõ∏ÈóúÊÄßÁçéÂãµ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑ UMed-LVLM Âú®Ë≠òÂà•ÂíåÁêÜËß£ÈÜ´ÁôÇÁï∞Â∏∏ÊñπÈù¢ÂÑ™ÊñºÁèæÊúâÁöÑ Med-LVLMs„ÄÇÊ≠§Â§ñÔºåÈÄôÈ†ÖÂ∑•‰ΩúË°®ÊòéÔºåÂ¢ûÂº∑ Med-LVLMs ÁöÑÁï∞Â∏∏ÂÅµÊ∏¨ËÉΩÂäõÂèØ‰ª•È°ØËëóÊèêÂçáÂÆÉÂÄëÂ∞çÈÜ´ÁôÇÂΩ±ÂÉèÁöÑÁêÜËß£ÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ

##### **ScarNet: A Novel Foundation Model for Automated Myocardial Scar Quantification from LGE in Cardiac MRI**
2501.01372v1 by Neda Tavakoli, Amir Ali Rahsepar, Brandon C. Benefield, Daming Shen, Santiago L√≥pez-Tapia, Florian Schiffers, Jeffrey J. Goldberger, Christine M. Albert, Edwin Wu, Aggelos K. Katsaggelos, Daniel C. Lee, Daniel Kim

Background: Late Gadolinium Enhancement (LGE) imaging is the gold standard
for assessing myocardial fibrosis and scarring, with left ventricular (LV) LGE
extent predicting major adverse cardiac events (MACE). Despite its importance,
routine LGE-based LV scar quantification is hindered by labor-intensive manual
segmentation and inter-observer variability. Methods: We propose ScarNet, a
hybrid model combining a transformer-based encoder from the Medical Segment
Anything Model (MedSAM) with a convolution-based U-Net decoder, enhanced by
tailored attention blocks. ScarNet was trained on 552 ischemic cardiomyopathy
patients with expert segmentations of myocardial and scar boundaries and tested
on 184 separate patients. Results: ScarNet achieved robust scar segmentation in
184 test patients, yielding a median Dice score of 0.912 (IQR: 0.863--0.944),
significantly outperforming MedSAM (median Dice = 0.046, IQR: 0.043--0.047) and
nnU-Net (median Dice = 0.638, IQR: 0.604--0.661). ScarNet demonstrated lower
bias (-0.63%) and coefficient of variation (4.3%) compared to MedSAM (bias:
-13.31%, CoV: 130.3%) and nnU-Net (bias: -2.46%, CoV: 20.3%). In Monte Carlo
simulations with noise perturbations, ScarNet achieved significantly higher
scar Dice (0.892 \pm 0.053, CoV = 5.9%) than MedSAM (0.048 \pm 0.112, CoV =
233.3%) and nnU-Net (0.615 \pm 0.537, CoV = 28.7%). Conclusion: ScarNet
outperformed MedSAM and nnU-Net in accurately segmenting myocardial and scar
boundaries in LGE images. The model exhibited robust performance across diverse
image qualities and scar patterns.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöÂª∂ËøüÈíÜÂ¢ûÂº∫ÔºàLGEÔºâÊàêÂÉèÁî®‰∫éËØÑ‰º∞ÂøÉËÇåÁ∫§Áª¥ÂåñÂíåÁò¢ÁóïÁöÑÈªÑÈáëÊ†áÂáÜÔºåÂ∑¶ÂøÉÂÆ§ (LV) LGE ËåÉÂõ¥È¢ÑÊµãÈáçÂ§ßÁöÑÂøÉËÑè‰∏çËâØ‰∫ã‰ª∂ (MACE)„ÄÇÂ∞ΩÁÆ°ÂÖ∂ÈáçË¶ÅÊÄßÔºå‰ΩÜÂü∫‰∫é LGE ÁöÑÂ∏∏ËßÑ LV Áò¢ÁóïÈáèÂåñÂèóÂà∞Âä≥Âä®ÂØÜÈõÜÂûãÊâãÂä®ÂàÜÂâ≤ÂíåËßÇÂØüËÄÖÈó¥Â∑ÆÂºÇÁöÑÈòªÁ¢ç„ÄÇÊñπÊ≥ïÔºöÊàë‰ª¨ÊèêÂá∫ ScarNetÔºå‰∏ÄÁßçÊ∑∑ÂêàÊ®°ÂûãÔºåÂÆÉÂ∞ÜÊù•Ëá™ÂåªÂ≠¶ÂàÜÂâ≤‰ªª‰ΩïÊ®°Âûã (MedSAM) ÁöÑÂü∫‰∫é Transformer ÁöÑÁºñÁ†ÅÂô®‰∏éÂü∫‰∫éÂç∑ÁßØÁöÑ U-Net Ëß£Á†ÅÂô®Áõ∏ÁªìÂêàÔºåÂπ∂ÈÄöËøáÂÆöÂà∂ÁöÑÊ≥®ÊÑèÂäõÂùóËøõË°åÂ¢ûÂº∫„ÄÇScarNet Âú® 552 ‰æãÁº∫Ë°ÄÊÄßÂøÉËÇåÁóÖÊÇ£ËÄÖ‰∏äÊé•ÂèóËÆ≠ÁªÉÔºåËøô‰∫õÊÇ£ËÄÖÁöÑÂøÉËÇåÂíåÁò¢ÁóïËæπÁïåÁî±‰∏ìÂÆ∂ÂàÜÂâ≤ÔºåÂπ∂Âú® 184 ‰æãÂçïÁã¨ÊÇ£ËÄÖ‰∏äËøõË°åÊµãËØï„ÄÇÁªìÊûúÔºöScarNet Âú® 184 ‰æãÊµãËØïÊÇ£ËÄÖ‰∏≠ÂÆûÁé∞‰∫ÜÁ®≥ÂÅ•ÁöÑÁò¢ÁóïÂàÜÂâ≤Ôºå‰∫ßÁîü 0.912 ÁöÑ‰∏≠ÂÄº Dice ÂæóÂàÜÔºàIQRÔºö0.863--0.944ÔºâÔºåÊòéÊòæ‰ºò‰∫é MedSAMÔºà‰∏≠ÂÄº Dice = 0.046ÔºåIQRÔºö0.043--0.047ÔºâÂíå nnU-NetÔºà‰∏≠ÂÄº Dice = 0.638ÔºåIQRÔºö0.604--0.661Ôºâ„ÄÇ‰∏é MedSAMÔºàÂÅèÂ∑ÆÔºö-13.31%ÔºåCoVÔºö130.3%ÔºâÂíå nnU-NetÔºàÂÅèÂ∑ÆÔºö-2.46%ÔºåCoVÔºö20.3%ÔºâÁõ∏ÊØîÔºåScarNet Ë°®Áé∞Âá∫ËæÉ‰ΩéÁöÑÂÅèÂ∑ÆÔºà-0.63%ÔºâÂíåÂèòÂºÇÁ≥ªÊï∞Ôºà4.3%Ôºâ„ÄÇÂú®Â∏¶ÊúâÂô™Â£∞Êâ∞Âä®ÁöÑËíôÁâπÂç°ÁΩóÊ®°Êãü‰∏≠ÔºåScarNet ÂÆûÁé∞‰∫ÜÊòéÊòæÈ´ò‰∫é MedSAMÔºà0.048 ¬± 0.112ÔºåCoV = 233.3%ÔºâÂíå nnU-NetÔºà0.615 ¬± 0.537ÔºåCoV = 28.7%ÔºâÁöÑÁò¢Áóï DiceÔºà0.892 ¬± 0.053ÔºåCoV = 5.9%Ôºâ„ÄÇÁªìËÆ∫ÔºöScarNet Âú®ÂáÜÁ°ÆÂàÜÂâ≤ LGE ÂõæÂÉè‰∏≠ÁöÑÂøÉËÇåÂíåÁò¢ÁóïËæπÁïåÊñπÈù¢‰ºò‰∫é MedSAM Âíå nnU-Net„ÄÇËØ•Ê®°ÂûãÂú®‰∏çÂêåÁöÑÂõæÂÉèË¥®ÈáèÂíåÁò¢ÁóïÊ®°Âºè‰∏ãË°®Áé∞Âá∫Á®≥ÂÅ•ÁöÑÊÄßËÉΩ„ÄÇ</paragraph>

##### **Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation**
2501.01367v1 by Nathaniel Dennler, Stefanos Nikolaidis, Maja Matariƒá

People have a variety of preferences for how robots behave. To understand and
reason about these preferences, robots aim to learn a reward function that
describes how aligned robot behaviors are with a user's preferences. Good
representations of a robot's behavior can significantly reduce the time and
effort required for a user to teach the robot their preferences. Specifying
these representations -- what "features" of the robot's behavior matter to
users -- remains a difficult problem; Features learned from raw data lack
semantic meaning and features learned from user data require users to engage in
tedious labeling processes. Our key insight is that users tasked with
customizing a robot are intrinsically motivated to produce labels through
exploratory search; they explore behaviors that they find interesting and
ignore behaviors that are irrelevant. To harness this novel data source of
exploratory actions, we propose contrastive learning from exploratory actions
(CLEA) to learn trajectory features that are aligned with features that users
care about. We learned CLEA features from exploratory actions users performed
in an open-ended signal design activity (N=25) with a Kuri robot, and evaluated
CLEA features through a second user study with a different set of users (N=42).
CLEA features outperformed self-supervised features when eliciting user
preferences over four metrics: completeness, simplicity, minimality, and
explainability.

ÊëòË¶ÅÔºö‰∫∫ÂÄëÂ∞çÊñºÊ©üÂô®‰∫∫ÁöÑË°åÁÇ∫ÊñπÂºèÊúâÂêÑÁ®ÆÂÅèÂ•Ω„ÄÇÁÇ∫‰∫ÜÁêÜËß£ÂíåÊé®Ë´ñÈÄô‰∫õÂÅèÂ•ΩÔºåÊ©üÂô®‰∫∫Êó®Âú®Â≠∏Áøí‰∏ÄÂÄãÁçéÂãµÂáΩÊï∏ÔºåË™™ÊòéÊ©üÂô®‰∫∫ÁöÑË°åÁÇ∫Ëàá‰ΩøÁî®ËÄÖÁöÑÂÅèÂ•ΩÊúâÂ§öÈ∫º‰∏ÄËá¥„ÄÇËâØÂ•ΩÁöÑÊ©üÂô®‰∫∫Ë°åÁÇ∫Ë°®Á§∫ÂèØ‰ª•Â§ßÂπÖÊ∏õÂ∞ë‰ΩøÁî®ËÄÖÊïôÂ∞éÊ©üÂô®‰∫∫ÂÖ∂ÂÅèÂ•ΩÊâÄÈúÄÁöÑÊôÇÈñìÂíåÁ≤æÂäõ„ÄÇË™™ÊòéÈÄô‰∫õË°®Á§∫‚Äî‚ÄîÊ©üÂô®‰∫∫Ë°åÁÇ∫ÁöÑÂì™‰∫õ„ÄåÁâπÂæµ„ÄçÂ∞ç‰ΩøÁî®ËÄÖ‰æÜË™™ÂæàÈáçË¶Å‚Äî‚Äî‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÂõ∞Èõ£ÁöÑÂïèÈ°åÔºõÂæûÂéüÂßãË≥áÊñôÂ≠∏ÁøíÂà∞ÁöÑÁâπÂæµÁº∫‰πèË™ûÊÑèÊÑèÁæ©ÔºåËÄåÂæû‰ΩøÁî®ËÄÖË≥áÊñôÂ≠∏ÁøíÂà∞ÁöÑÁâπÂæµÈúÄË¶Å‰ΩøÁî®ËÄÖÂèÉËàáÁπÅÁë£ÁöÑÊ®ôÁ±§ËôïÁêÜÁ®ãÂ∫è„ÄÇÊàëÂÄëÁöÑÈóúÈçµË¶ãËß£ÊòØÔºåË≤†Ë≤¨Ëá™Ë®ÇÊ©üÂô®‰∫∫ÁöÑ‰ΩøÁî®ËÄÖÊú¨Ë≥™‰∏äÊúÉÈÄèÈÅéÊé¢Á¥¢ÊÄßÊêúÂ∞ãÁî¢ÁîüÊ®ôÁ±§Ôºõ‰ªñÂÄëÊúÉÊé¢Á¥¢‰ªñÂÄëË¶∫ÂæóÊúâË∂£ÁöÑË°åÁÇ∫Ôºå‰∏¶ÂøΩÁï•‰∏çÁõ∏ÈóúÁöÑË°åÁÇ∫„ÄÇÁÇ∫‰∫ÜÂà©Áî®ÈÄôÂÄãÊé¢Á¥¢ÊÄßÂãï‰ΩúÁöÑÊñ∞Á©éË≥áÊñô‰æÜÊ∫êÔºåÊàëÂÄëÊèêÂá∫ÂæûÊé¢Á¥¢ÊÄßÂãï‰Ωú‰∏≠ÈÄ≤Ë°åÂ∞çÊØîÂ≠∏Áøí (CLEA)Ôºå‰ª•Â≠∏ÁøíËàá‰ΩøÁî®ËÄÖÈóúÂøÉÁöÑÁâπÂæµ‰∏ÄËá¥ÁöÑËªåË∑°ÁâπÂæµ„ÄÇÊàëÂÄëÂæû‰ΩøÁî®ËÄÖÂú®Ëàá Kuri Ê©üÂô®‰∫∫ÁöÑÈñãÊîæÂºèË®äËôüË®≠Ë®àÊ¥ªÂãï (N=25) ‰∏≠Âü∑Ë°åÁöÑÊé¢Á¥¢ÊÄßÂãï‰Ωú‰∏≠Â≠∏Áøí‰∫Ü CLEA ÁâπÂæµÔºå‰∏¶ÈÄèÈÅéÁ¨¨‰∫åÂÄã‰ΩøÁî®ËÄÖÁ†îÁ©∂Â∞ç CLEA ÁâπÂæµÈÄ≤Ë°åË©ï‰º∞ÔºåË©≤Á†îÁ©∂‰ΩøÁî®‰∫Ü‰∏ÄÁµÑ‰∏çÂêåÁöÑ‰ΩøÁî®ËÄÖ (N=42)„ÄÇÂú®ÂºïÂá∫‰ΩøÁî®ËÄÖÂÅèÂ•ΩÊôÇÔºåCLEA ÁâπÂæµÂú®ÂõõÂÄãÊåáÊ®ô‰∏äÂÑ™ÊñºËá™Áõ£Áù£ÁâπÂæµÔºöÂÆåÊï¥ÊÄß„ÄÅÁ∞°ÊΩîÊÄß„ÄÅÊúÄÂ∞èÊÄß„ÄÅÂèØËß£ÈáãÊÄß„ÄÇ

##### **Multi-Head Explainer: A General Framework to Improve Explainability in CNNs and Transformers**
2501.01311v2 by Bohang Sun, Pietro Li√≤

In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and
modular framework that enhances both the explainability and accuracy of
Convolutional Neural Networks (CNNs) and Transformer-based models. MHEX
consists of three core components: an Attention Gate that dynamically
highlights task-relevant features, Deep Supervision that guides early layers to
capture fine-grained details pertinent to the target class, and an Equivalent
Matrix that unifies refined local and global representations to generate
comprehensive saliency maps. Our approach demonstrates superior compatibility,
enabling effortless integration into existing residual networks like ResNet and
Transformer architectures such as BERT with minimal modifications. Extensive
experiments on benchmark datasets in medical imaging and text classification
show that MHEX not only improves classification accuracy but also produces
highly interpretable and detailed saliency scores.

ÊëòË¶ÅÔºöÂú®Êú¨Ê¨°Á†îÁ©∂‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂ§öÈ†≠Ëß£ÈáãÂô® (MHEX)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§öÂäüËÉΩ‰∏îÊ®°ÁµÑÂåñÁöÑÊû∂ÊßãÔºåÂèØÂ¢ûÂº∑Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) Âíå Transformer Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÂíåÊ∫ñÁ¢∫ÊÄß„ÄÇMHEX ÂåÖÂê´‰∏âÂÄãÊ†∏ÂøÉÁµÑÊàêÈÉ®ÂàÜÔºöÂãïÊÖãÁ™ÅÈ°ØËàá‰ªªÂãôÁõ∏ÈóúÁâπÂæµÁöÑÊ≥®ÊÑèÂäõÈñòÈñÄ„ÄÅÂºïÂ∞éÊó©ÊúüÂ±§ÊçïÊçâËàáÁõÆÊ®ôÈ°ûÂà•Áõ∏ÈóúÁöÑÁ¥∞ÂæÆÁ¥∞ÁØÄÁöÑÊ∑±Â∫¶Áõ£Áù£Ôºå‰ª•ÂèäÁµ±‰∏ÄÁ≤æÁ∑ªÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄË°®Á§∫‰ª•Áî¢ÁîüÂÖ®Èù¢È°ØËëóÊÄßÂúñÁöÑÁ≠âÊïàÁü©Èô£„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂ±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÁõ∏ÂÆπÊÄßÔºåËÉΩËºïÈ¨ÜÊï¥ÂêàÂà∞ÁèæÊúâÁöÑÊÆòÂ∑ÆÁ∂≤Ë∑ØÔºàÂ¶Ç ResNetÔºâÂíå Transformer Êû∂ÊßãÔºàÂ¶Ç BERTÔºâÔºå‰∏îÂè™ÈúÄÈÄ≤Ë°åÊúÄÂ∞èÁöÑ‰øÆÊîπ„ÄÇÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÊñáÂ≠óÂàÜÈ°ûÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óÈ°ØÁ§∫ÔºåMHEX ‰∏çÂÉÖËÉΩÊèêÂçáÂàÜÈ°ûÊ∫ñÁ¢∫ÊÄßÔºåÈÇÑËÉΩÁî¢ÁîüÈ´òÂ∫¶ÂèØËß£Èáã‰∏îË©≥Á¥∞ÁöÑÈ°ØËëóÊÄßÂàÜÊï∏„ÄÇ

##### **Machine Learning-Based Differential Diagnosis of Parkinson's Disease Using Kinematic Feature Extraction and Selection**
2501.02014v1 by Masahiro Matsumoto, Abu Saleh Musa Miah, Nobuyoshi Asai, Jungpil Shin

Parkinson's disease (PD), the second most common neurodegenerative disorder,
is characterized by dopaminergic neuron loss and the accumulation of abnormal
synuclein. PD presents both motor and non-motor symptoms that progressively
impair daily functioning. The severity of these symptoms is typically assessed
using the MDS-UPDRS rating scale, which is subjective and dependent on the
physician's experience. Additionally, PD shares symptoms with other
neurodegenerative diseases, such as progressive supranuclear palsy (PSP) and
multiple system atrophy (MSA), complicating accurate diagnosis. To address
these diagnostic challenges, we propose a machine learning-based system for
differential diagnosis of PD, PSP, MSA, and healthy controls (HC). This system
utilizes a kinematic feature-based hierarchical feature extraction and
selection approach. Initially, 18 kinematic features are extracted, including
two newly proposed features: Thumb-to-index vector velocity and acceleration,
which provide insights into motor control patterns. In addition, 41 statistical
features were extracted here from each kinematic feature, including some new
approaches such as Average Absolute Change, Rhythm, Amplitude, Frequency,
Standard Deviation of Frequency, and Slope. Feature selection is performed
using One-way ANOVA to rank features, followed by Sequential Forward Floating
Selection (SFFS) to identify the most relevant ones, aiming to reduce the
computational complexity. The final feature set is used for classification,
achieving a classification accuracy of 66.67% for each dataset and 88.89% for
each patient, with particularly high performance for the MSA and HC groups
using the SVM algorithm. This system shows potential as a rapid and accurate
diagnostic tool in clinical practice, though further data collection and
refinement are needed to enhance its reliability.

ÊëòË¶ÅÔºöÂ∏ïÈáëÊ£ÆÊ∞èÁóáÔºàPDÔºâÊòØÁ¨¨‰∫åÂ∏∏ËßÅÁöÑËÑëÁ•ûÁªèÈÄÄÂåñÊÄßÁñæÁóÖÔºå
ÂÖ∂ÁâπÂæÅÊòØÂ§öÂ∑¥ËÉ∫ËÉΩÁ•ûÁªèÂÖÉ‰∏ßÂ§±ÂíåÂºÇÂ∏∏Œ±-Á™ÅËß¶Ê†∏ËõãÁôΩÁöÑÁßØÁ¥Ø„ÄÇPD ÂêåÊó∂Âá∫Áé∞ËøêÂä®ÂíåÈùûËøêÂä®ÁóáÁä∂ÔºåËøô‰∫õÁóáÁä∂‰ºöÈÄêÊ∏êÊçüÂÆ≥Êó•Â∏∏ÂäüËÉΩ„ÄÇËøô‰∫õÁóáÁä∂ÁöÑ‰∏•ÈáçÁ®ãÂ∫¶ÈÄöÂ∏∏‰ΩøÁî® MDS-UPDRS ËØÑÂÆöÈáèË°®ËøõË°åËØÑ‰º∞ÔºåËØ•ÈáèË°®ÊòØ‰∏ªËßÇÁöÑÔºåÂπ∂‰∏î‰æùËµñ‰∫éÂåªÁîüÁöÑÁªèÈ™å„ÄÇÊ≠§Â§ñÔºåPD ‰∏éÂÖ∂‰ªñÁ•ûÁªèÈÄÄÂåñÊÄßÁñæÁóÖÔºà‰æãÂ¶ÇËøõË°åÊÄßÊ†∏‰∏äÊÄßÈ∫ªÁóπ (PSP) ÂíåÂ§öÁ≥ªÁªüËêéÁº© (MSA)ÔºâÊúâÁõ∏ÂêåÁöÑÁóáÁä∂ÔºåËøô‰ΩøÂæóÂáÜÁ°ÆËØäÊñ≠ÂèòÂæóÂ§çÊùÇ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∫õËØäÊñ≠ÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊú∫Âô®Â≠¶‰π†ÁöÑÁ≥ªÁªüÔºåÁî®‰∫é PD„ÄÅPSP„ÄÅMSA ÂíåÂÅ•Â∫∑ÂØπÁÖß (HC) ÁöÑÈâ¥Âà´ËØäÊñ≠„ÄÇËØ•Á≥ªÁªüÂà©Áî®Âü∫‰∫éËøêÂä®Â≠¶ÁâπÂæÅÁöÑÂàÜÂ±ÇÁâπÂæÅÊèêÂèñÂíåÈÄâÊã©ÊñπÊ≥ï„ÄÇÊúÄÂàùÔºåÊèêÂèñ‰∫Ü 18 ‰∏™ËøêÂä®Â≠¶ÁâπÂæÅÔºåÂåÖÊã¨‰∏§‰∏™Êñ∞ÊèêÂá∫ÁöÑÁâπÂæÅÔºöÊãáÊåáÂà∞È£üÊåáÁöÑÁü¢ÈáèÈÄüÂ∫¶ÂíåÂä†ÈÄüÂ∫¶ÔºåÂÆÉ‰ª¨Êèê‰æõ‰∫ÜÂØπËøêÂä®ÊéßÂà∂Ê®°ÂºèÁöÑËßÅËß£„ÄÇÊ≠§Â§ñÔºåÊ≠§Â§Ñ‰ªéÊØè‰∏™ËøêÂä®Â≠¶ÁâπÂæÅ‰∏≠ÊèêÂèñ‰∫Ü 41 ‰∏™ÁªüËÆ°ÁâπÂæÅÔºåÂåÖÊã¨‰∏Ä‰∫õÊñ∞ÊñπÊ≥ïÔºå‰æãÂ¶ÇÂπ≥ÂùáÁªùÂØπÂèòÂåñ„ÄÅËäÇÂ•è„ÄÅÊåØÂπÖ„ÄÅÈ¢ëÁéá„ÄÅÈ¢ëÁéáÊ†áÂáÜÂ∑ÆÂíåÊñúÁéá„ÄÇ‰ΩøÁî®ÂçïÂêëÊñπÂ∑ÆÂàÜÊûêÂØπÁâπÂæÅËøõË°åÊéíÂêçÔºåÁÑ∂Âêé‰ΩøÁî®È°∫Â∫èÂâçÂêëÊµÆÂä®ÈÄâÊã© (SFFS) ËØÜÂà´ÊúÄÁõ∏ÂÖ≥ÁöÑÁâπÂæÅÔºå‰ª•Èôç‰ΩéËÆ°ÁÆóÂ§çÊùÇÂ∫¶„ÄÇÊúÄÁªàÁâπÂæÅÈõÜÁî®‰∫éÂàÜÁ±ªÔºåÂØπ‰∫éÊØè‰∏™Êï∞ÊçÆÈõÜÔºåÂàÜÁ±ªÂáÜÁ°ÆÁéáËææÂà∞ 66.67%ÔºåÂØπ‰∫éÊØè‰∏™ÊÇ£ËÄÖÔºåÂáÜÁ°ÆÁéáËææÂà∞ 88.89%Ôºå‰ΩøÁî® SVM ÁÆóÊ≥ïÊó∂ÔºåMSA Âíå HC ÁªÑÁöÑÊÄßËÉΩÂ∞§ÂÖ∂È´ò„ÄÇËØ•Á≥ªÁªüÊòæÁ§∫Âá∫‰Ωú‰∏∫‰∏¥Â∫äÂÆûË∑µ‰∏≠Âø´ÈÄü‰∏îÂáÜÁ°ÆÁöÑËØäÊñ≠Â∑•ÂÖ∑ÁöÑÊΩúÂäõÔºåÂ∞ΩÁÆ°ÈúÄË¶ÅËøõ‰∏ÄÊ≠•Êî∂ÈõÜÊï∞ÊçÆÂíåÊîπËøõ‰ª•Â¢ûÂº∫ÂÖ∂ÂèØÈù†ÊÄß„ÄÇ

##### **Data Augmentation Techniques for Chinese Disease Name Normalization**
2501.01195v1 by Wenqian Cui, Xiangling Fu, Shaohui Liu, Mingjun Gu, Xien Liu, Ji Wu, Irwin King

Disease name normalization is an important task in the medical domain. It
classifies disease names written in various formats into standardized names,
serving as a fundamental component in smart healthcare systems for various
disease-related functions. Nevertheless, the most significant obstacle to
existing disease name normalization systems is the severe shortage of training
data. Consequently, we present a novel data augmentation approach that includes
a series of data augmentation techniques and some supporting modules to help
mitigate the problem. Through extensive experimentation, we illustrate that our
proposed approach exhibits significant performance improvements across various
baseline models and training objectives, particularly in scenarios with limited
training data

ÊëòË¶ÅÔºöÁñæÁóÖÂêçÁ®±Ê≠£Ë¶èÂåñÊòØÈÜ´Â≠∏È†òÂüü‰∏≠‰∏ÄÈ†ÖÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇÂÆÉÂ∞á‰ª•ÂêÑÁ®ÆÊ†ºÂºèÊõ∏ÂØ´ÁöÑÁñæÁóÖÂêçÁ®±ÂàÜÈ°ûÁÇ∫Ê®ôÊ∫ñÂåñÂêçÁ®±Ôºå‰ΩúÁÇ∫Êô∫ÊÖßÈÜ´ÁôÇÁ≥ªÁµ±‰∏≠ÂêÑÁ®ÆÁñæÁóÖÁõ∏ÈóúÂäüËÉΩÁöÑÂü∫Êú¨ÁµÑÊàêÈÉ®ÂàÜ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁñæÁóÖÂêçÁ®±Ê≠£Ë¶èÂåñÁ≥ªÁµ±ÊúÄÈ°ØËëóÁöÑÈöúÁ§ôÊòØË®ìÁ∑¥Ë≥áÊñôÂö¥ÈáçÁü≠Áº∫„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑË≥áÊñôÊì¥ÂÖÖÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÂåÖÊã¨‰∏ÄÁ≥ªÂàóË≥áÊñôÊì¥ÂÖÖÊäÄË°ìÂíå‰∏Ä‰∫õËºîÂä©Ê®°ÁµÑÔºå‰ª•Âπ´Âä©Ê∏õËºïÈÄôÂÄãÂïèÈ°å„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óÔºåÊàëÂÄëË™™ÊòéÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÂêÑÁ®ÆÂü∫Á∑öÊ®°ÂûãÂíåË®ìÁ∑¥ÁõÆÊ®ô‰∏≠Â±ïÁèæÂá∫È°ØËëóÁöÑÊïàËÉΩÊèêÂçáÔºåÁâπÂà•ÊòØÂú®Ë®ìÁ∑¥Ë≥áÊñôÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ã

##### **Reasoning based on symbolic and parametric knowledge bases: a survey**
2501.01030v1 by Mayi Xu, Yunfeng Ning, Yongqi Li, Jianhao Chen, Jintao Wen, Yao Xiao, Shen Zhou, Birong Pan, Zepeng Bao, Xin Miao, Hankun Kang, Ke Sun, Tieyun Qian

Reasoning is fundamental to human intelligence, and critical for
problem-solving, decision-making, and critical thinking. Reasoning refers to
drawing new conclusions based on existing knowledge, which can support various
applications like clinical diagnosis, basic education, and financial analysis.
Though a good number of surveys have been proposed for reviewing
reasoning-related methods, none of them has systematically investigated these
methods from the viewpoint of their dependent knowledge base. Both the
scenarios to which the knowledge bases are applied and their storage formats
are significantly different. Hence, investigating reasoning methods from the
knowledge base perspective helps us better understand the challenges and future
directions. To fill this gap, this paper first classifies the knowledge base
into symbolic and parametric ones. The former explicitly stores information in
human-readable symbols, and the latter implicitly encodes knowledge within
parameters. Then, we provide a comprehensive overview of reasoning methods
using symbolic knowledge bases, parametric knowledge bases, and both of them.
Finally, we identify the future direction toward enhancing reasoning
capabilities to bridge the gap between human and machine intelligence.

ÊëòË¶ÅÔºöÊé®ÁêÜÊòØ‰∫∫Á±ªÊô∫ËÉΩÁöÑÂü∫Á°ÄÔºåÂØπ‰∫éËß£ÂÜ≥ÈóÆÈ¢ò„ÄÅÂÜ≥Á≠ñÂíåÊâπÂà§ÊÄßÊÄùÁª¥Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊé®ÁêÜÊòØÊåáÊ†πÊçÆÁé∞ÊúâÁü•ËØÜÂæóÂá∫Êñ∞ÁöÑÁªìËÆ∫ÔºåËøôÂèØ‰ª•ÊîØÊåÅÂêÑÁßçÂ∫îÁî®Á®ãÂ∫èÔºåÂ¶Ç‰∏¥Â∫äËØäÊñ≠„ÄÅÂü∫Á°ÄÊïôËÇ≤ÂíåË¥¢Âä°ÂàÜÊûê„ÄÇÂ∞ΩÁÆ°Â∑≤ÁªèÊèêÂá∫‰∫ÜÂ§ßÈáèË∞ÉÊü•Êù•ÂÆ°Êü•‰∏éÊé®ÁêÜÁõ∏ÂÖ≥ÁöÑÂêÑÁßçÊñπÊ≥ïÔºå‰ΩÜÊ≤°Êúâ‰∏ÄÁßçÊñπÊ≥ï‰ªéÂÖ∂‰æùËµñÁü•ËØÜÂ∫ìÁöÑËßíÂ∫¶Á≥ªÁªüÂú∞Á†îÁ©∂Ëøô‰∫õÊñπÊ≥ï„ÄÇÁü•ËØÜÂ∫ìË¢´Â∫îÁî®Âà∞ÁöÑÂú∫ÊôØÂèäÂÖ∂Â≠òÂÇ®Ê†ºÂºèÈÉΩÊúâÊòæÁùÄÂ∑ÆÂºÇ„ÄÇÂõ†Ê≠§Ôºå‰ªéÁü•ËØÜÂ∫ìÁöÑËßíÂ∫¶Á†îÁ©∂Êé®ÁêÜÊñπÊ≥ïÊúâÂä©‰∫éÊàë‰ª¨Êõ¥Â•ΩÂú∞ÁêÜËß£ÊåëÊàòÂíåÊú™Êù•ÁöÑÊñπÂêë„ÄÇ‰∏∫‰∫ÜÂ°´Ë°•Ëøô‰∏ÄÁ©∫ÁôΩÔºåÊú¨ÊñáÈ¶ñÂÖàÂ∞ÜÁü•ËØÜÂ∫ìÂàÜ‰∏∫Á¨¶Âè∑Áü•ËØÜÂ∫ìÂíåÂèÇÊï∞Áü•ËØÜÂ∫ì„ÄÇÂâçËÄÖ‰ª•‰∫∫Á±ªÂèØËØªÁöÑÁ¨¶Âè∑ÊòéÁ°ÆÂ≠òÂÇ®‰ø°ÊÅØÔºåËÄåÂêéËÄÖÂàôÂú®ÂèÇÊï∞‰∏≠ÈöêÂºèÁºñÁ†ÅÁü•ËØÜ„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÂØπ‰ΩøÁî®Á¨¶Âè∑Áü•ËØÜÂ∫ì„ÄÅÂèÇÊï∞Áü•ËØÜÂ∫ì‰ª•Âèä‰∏§ËÄÖÁªìÂêàÁöÑÊé®ÁêÜÊñπÊ≥ïËøõË°å‰∫ÜÂÖ®Èù¢Ê¶ÇËø∞„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Á°ÆÂÆö‰∫ÜÂ¢ûÂº∫Êé®ÁêÜËÉΩÂäõ‰ª•Áº©Â∞è‰∫∫ÂíåÊú∫Âô®Êô∫ËÉΩ‰πãÈó¥Â∑ÆË∑ùÁöÑÊú™Êù•ÊñπÂêë„ÄÇ

##### **Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice**
2501.00982v1 by Federico Ravenda, Seyed Ali Bahrainian, Andrea Raballo, Antonietta Mira, Noriko Kando

In psychological practice, standardized questionnaires serve as essential
tools for assessing mental constructs (e.g., attitudes, traits, and emotions)
through structured questions (aka items). With the increasing prevalence of
social media platforms where users share personal experiences and emotions,
researchers are exploring computational methods to leverage this data for rapid
mental health screening. In this study, we propose a novel adaptive
Retrieval-Augmented Generation (RAG) approach that completes psychological
questionnaires by analyzing social media posts. Our method retrieves the most
relevant user posts for each question in a psychological survey and uses Large
Language Models (LLMs) to predict questionnaire scores in a zero-shot setting.
Our findings are twofold. First we demonstrate that this approach can
effectively predict users' responses to psychological questionnaires, such as
the Beck Depression Inventory II (BDI-II), achieving performance comparable to
or surpassing state-of-the-art models on Reddit-based benchmark datasets
without relying on training data. Second, we show how this methodology can be
generalized as a scalable screening tool, as the final assessment is
systematically derived by completing standardized questionnaires and tracking
how individual item responses contribute to the diagnosis, aligning with
established psychometric practices.

ÊëòË¶ÅÔºö<paragraph>Âú®ÂøÉÁêÜÂ≠∏ÂØ¶Âãô‰∏≠ÔºåÊ®ôÊ∫ñÂåñÂïèÂç∑‰ΩúÁÇ∫Ë©ïÈáèÂøÉÁêÜÂª∫ÊßãÔºà‰æãÂ¶ÇÊÖãÂ∫¶„ÄÅÁâπË≥™ÂíåÊÉÖÁ∑íÔºâÁöÑÂøÖË¶ÅÂ∑•ÂÖ∑ÔºåÈÄèÈÅéÁµêÊßãÂåñÂïèÈ°åÔºàÂèàÁ®±È†ÖÁõÆÔºâ‰æÜÈÄ≤Ë°åË©ïÈáè„ÄÇÈö®ËëóÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞ÁöÑÊôÆÂèäÔºå‰ΩøÁî®ËÄÖÊúÉÂú®‰∏äÈù¢ÂàÜ‰∫´ÂÄã‰∫∫Á∂ìÈ©óÂíåÊÉÖÁ∑íÔºåÁ†îÁ©∂‰∫∫Âì°Ê≠£Âú®Êé¢Ë®éÈÅãÁÆóÊñπÊ≥ïÔºå‰ª•Âà©Áî®ÈÄô‰∫õË≥áÊñôÈÄ≤Ë°åÂø´ÈÄüÁöÑÁöÑÂøÉÁêÜÂÅ•Â∫∑ÁØ©Ê™¢„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÈÅ©ÊáâÊÄßÊì∑ÂèñÂ¢ûÂº∑ÁîüÊàêÔºàRAGÔºâÊñπÊ≥ïÔºåÈÄèÈÅéÂàÜÊûêÁ§æÁæ§Â™íÈ´îË≤ºÊñá‰æÜÂÆåÊàêÂøÉÁêÜÂïèÂç∑„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊòØÈáùÂ∞çÂøÉÁêÜË™øÊü•‰∏≠ÁöÑÊØèÂÄãÂïèÈ°åÔºåÊì∑ÂèñËàá‰πãÊúÄÁõ∏ÈóúÁöÑ‰ΩøÁî®ËÄÖË≤ºÊñáÔºå‰∏¶‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Èõ∂Ê¨°Â≠∏ÁøíÁöÑË®≠ÂÆö‰∏ãÈ†êÊ∏¨ÂïèÂç∑ÂàÜÊï∏„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊúâÂÖ©ÊñπÈù¢„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëË≠âÊòé‰∫ÜÈÄôÁ®ÆÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÈ†êÊ∏¨‰ΩøÁî®ËÄÖÂ∞çÂøÉÁêÜÂïèÂç∑ÁöÑÂõûÁ≠îÔºå‰æãÂ¶ÇË≤ùÂÖãÊÜÇÈ¨±ÈáèË°®Á¨¨‰∫åÁâàÔºàBDI-IIÔºâÔºåÂú®Âü∫Êñº Reddit ÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫ÜËàáÊúÄÂÖàÈÄ≤Ê®°ÂûãÁõ∏Áï∂ÊàñË∂ÖË∂äÁöÑË°®ÁèæÔºåËÄå‰∏î‰∏¶Êú™‰æùË≥¥Ë®ìÁ∑¥Ë≥áÊñô„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÈÄôÂÄãÊñπÊ≥ïÂ¶Ç‰ΩïËÉΩË¢´Ê¶ÇÊã¨ÁÇ∫‰∏ÄÁ®ÆÂèØÊì¥ÂÖÖÁöÑÁØ©Ê™¢Â∑•ÂÖ∑ÔºåÂõ†ÁÇ∫ÊúÄÁµÇË©ïÈáèÊòØÈÄèÈÅéÂÆåÊàêÊ®ôÊ∫ñÂåñÂïèÂç∑‰∏¶ËøΩËπ§ÂÄãÂà•È†ÖÁõÆÂõûÁ≠îÂ¶Ç‰Ωï‰øÉÊàêË®∫Êñ∑ËÄåÁ≥ªÁµ±ÊÄßÂú∞ÂæóÂá∫ÁöÑÔºåÈÄôËàáÊó¢ÂÆöÁöÑÂøÉÁêÜÊ∏¨ÈáèÂØ¶ÂãôÁõ∏Á¨¶„ÄÇ</paragraph>

##### **Enhancing Early Diabetic Retinopathy Detection through Synthetic DR1 Image Generation: A StyleGAN3 Approach**
2501.00954v1 by Sagarnil Das, Pradeep Walia

Diabetic Retinopathy (DR) is a leading cause of preventable blindness. Early
detection at the DR1 stage is critical but is hindered by a scarcity of
high-quality fundus images. This study uses StyleGAN3 to generate synthetic DR1
images characterized by microaneurysms with high fidelity and diversity. The
aim is to address data scarcity and enhance the performance of supervised
classifiers. A dataset of 2,602 DR1 images was used to train the model,
followed by a comprehensive evaluation using quantitative metrics, including
Frechet Inception Distance (FID), Kernel Inception Distance (KID), and
Equivariance with respect to translation (EQ-T) and rotation (EQ-R).
Qualitative assessments included Human Turing tests, where trained
ophthalmologists evaluated the realism of synthetic images. Spectral analysis
further validated image quality. The model achieved a final FID score of 17.29,
outperforming the mean FID of 21.18 (95 percent confidence interval - 20.83 to
21.56) derived from bootstrap resampling. Human Turing tests demonstrated the
model's ability to produce highly realistic images, though minor artifacts near
the borders were noted. These findings suggest that StyleGAN3-generated
synthetic DR1 images hold significant promise for augmenting training datasets,
enabling more accurate early detection of Diabetic Retinopathy. This
methodology highlights the potential of synthetic data in advancing medical
imaging and AI-driven diagnostics.

ÊëòË¶ÅÔºöÁ≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆä (DR) ÊòØÂèØÈ†êÈò≤Â§±ÊòéÁöÑ‰∏ªË¶ÅÂéüÂõ†„ÄÇÂú® DR1 ÈöéÊÆµÊó©ÊúüÁôºÁèæËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÁî±ÊñºÁº∫‰πèÈ´òÂìÅË≥™ÁúºÂ∫ïÂúñÂÉèËÄåÂèóÂà∞ÈòªÁ§ô„ÄÇÊú¨Á†îÁ©∂‰ΩøÁî® StyleGAN3 ÁîüÊàêÂêàÊàê DR1 ÂúñÂÉèÔºåÂÖ∂ÁâπÂæµÊòØÂÖ∑ÊúâÈ´ò‰øùÁúüÂ∫¶ÂíåÂ§öÊ®£ÊÄßÁöÑÂæÆÂãïËÑàÁò§„ÄÇÁõÆÁöÑÊòØËß£Ê±∫Ë≥áÊñôÁ®ÄÂ∞ëÁöÑÂïèÈ°åÔºå‰∏¶ÊèêÂçáÁõ£Áù£ÂàÜÈ°ûÂô®ÁöÑÊïàËÉΩ„ÄÇ‰ΩøÁî® 2,602 Âºµ DR1 ÂúñÂÉèÁöÑË≥áÊñôÈõÜ‰æÜË®ìÁ∑¥Ê®°ÂûãÔºåÁÑ∂Âæå‰ΩøÁî®ÈáèÂåñÊåáÊ®ôÈÄ≤Ë°åÂÖ®Èù¢Ë©ï‰º∞ÔºåÂåÖÊã¨ Fr√©chet Inception Distance (FID)„ÄÅKernel Inception Distance (KID) ‰ª•ÂèäÁõ∏Â∞çÊñºÂπ≥Áßª (EQ-T) ÂíåÊóãËΩâ (EQ-R) ÁöÑÁ≠âËÆäÁï∞ÊÄß„ÄÇÂÆöÊÄßË©ï‰º∞ÂåÖÊã¨‰∫∫È°ûÂúñÈùàÊ∏¨Ë©¶ÔºåÂÖ∂‰∏≠Ë®ìÁ∑¥ÊúâÁ¥†ÁöÑÁúºÁßëÈÜ´ÁîüË©ï‰º∞ÂêàÊàêÂúñÂÉèÁöÑÁúüÂØ¶ÊÄß„ÄÇÂÖâË≠úÂàÜÊûêÈÄ≤‰∏ÄÊ≠•È©óË≠â‰∫ÜÂΩ±ÂÉèÂìÅË≥™„ÄÇË©≤Ê®°ÂûãÈÅîÂà∞‰∫Ü 17.29 ÁöÑÊúÄÁµÇ FID ÂàÜÊï∏ÔºåÂÑ™ÊñºÂæû bootstrap ÈáçÊäΩÊ®£ÂæóÂá∫ÁöÑ 21.18 ÁöÑÂπ≥Âùá FIDÔºà95% ‰ø°Ë≥¥ÂçÄÈñì - 20.83 Âà∞ 21.56Ôºâ„ÄÇ‰∫∫È°ûÂúñÈùàÊ∏¨Ë©¶Ë≠âÊòé‰∫ÜË©≤Ê®°ÂûãÁî¢ÁîüÈ´òÂ∫¶ÈÄºÁúüÂúñÂÉèÁöÑËÉΩÂäõÔºåÂÑòÁÆ°Ê≥®ÊÑèÂà∞ÈÇäÁ∑£ÈôÑËøëÊúâËºïÂæÆÁöÑ‰∫∫Â∑•Ë£ΩÂìÅ„ÄÇÈÄô‰∫õÁôºÁèæË°®ÊòéÔºåStyleGAN3 ÁîüÊàêÁöÑÂêàÊàê DR1 ÂúñÂÉèÂ∞çÊñºÊì¥ÂÖÖË®ìÁ∑¥Ë≥áÊñôÈõÜÂÖ∑ÊúâÈ°ØËëóÁöÑÂ∏åÊúõÔºåËÉΩÂ§†Êõ¥Ê∫ñÁ¢∫Âú∞Êó©ÊúüÁôºÁèæÁ≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆä„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÁ™ÅÈ°Ø‰∫ÜÂêàÊàêË≥áÊñôÂú®Êé®ÈÄ≤ÈÜ´Â≠∏ÂΩ±ÂÉèÂíå AI È©ÖÂãïË®∫Êñ∑ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **Multi-Center Study on Deep Learning-Assisted Detection and Classification of Fetal Central Nervous System Anomalies Using Ultrasound Imaging**
2501.02000v1 by Yang Qi, Jiaxin Cai, Jing Lu, Runqing Xiong, Rongshang Chen, Liping Zheng, Duo Ma

Prenatal ultrasound evaluates fetal growth and detects congenital
abnormalities during pregnancy, but the examination of ultrasound images by
radiologists requires expertise and sophisticated equipment, which would
otherwise fail to improve the rate of identifying specific types of fetal
central nervous system (CNS) abnormalities and result in unnecessary patient
examinations. We construct a deep learning model to improve the overall
accuracy of the diagnosis of fetal cranial anomalies to aid prenatal diagnosis.
In our collected multi-center dataset of fetal craniocerebral anomalies
covering four typical anomalies of the fetal central nervous system (CNS):
anencephaly, encephalocele (including meningocele), holoprosencephaly, and
rachischisis, patient-level prediction accuracy reaches 94.5%, with an AUROC
value of 99.3%. In the subgroup analyzes, our model is applicable to the entire
gestational period, with good identification of fetal anomaly types for any
gestational period. Heatmaps superimposed on the ultrasound images not only
provide a visual interpretation for the algorithm but also provide an intuitive
visual aid to the physician by highlighting key areas that need to be reviewed,
helping the physician to quickly identify and validate key areas. Finally, the
retrospective reader study demonstrates that by combining the automatic
prediction of the DL system with the professional judgment of the radiologist,
the diagnostic accuracy and efficiency can be effectively improved and the
misdiagnosis rate can be reduced, which has an important clinical application
prospect.

ÊëòË¶ÅÔºöÁî¢ÂâçË∂ÖÈü≥Ê≥¢Ë©ï‰º∞ËÉéÂÖíÁîüÈï∑‰∏¶Âú®Êá∑Â≠ïÊúüÈñìÂÅµÊ∏¨ÂÖàÂ§©Áï∞Â∏∏Ôºå‰ΩÜË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÁöÑÊ™¢Êü•ÈúÄË¶ÅÊîæÂ∞ÑÁßëÈÜ´Â∏´ÁöÑÂ∞àÊ•≠Áü•Ë≠òÂíåÁ≤æÂØÜÂÑÄÂô®ÔºåÂê¶ÂâáÁÑ°Ê≥ïÊîπÂñÑÁâπÂÆöÈ°ûÂûãËÉéÂÖí‰∏≠Ê®ûÁ•ûÁ∂ìÁ≥ªÁµ± (CNS) Áï∞Â∏∏ÁöÑËæ®Ë≠òÁéáÔºå‰∏¶Â∞éËá¥‰∏çÂøÖË¶ÅÁöÑÁóÖ‰∫∫Ê™¢Êü•„ÄÇÊàëÂÄëÂª∫Êßã‰∏ÄÂÄãÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰ª•ÊîπÂñÑËÉéÂÖíÈ°±È™®Áï∞Â∏∏Ë®∫Êñ∑ÁöÑÊï¥È´îÊ∫ñÁ¢∫Â∫¶Ôºå‰ª•ÂçîÂä©Áî¢ÂâçË®∫Êñ∑„ÄÇÂú®ÊàëÂÄëÊî∂ÈõÜÁöÑÂ§ö‰∏≠ÂøÉËÉéÂÖíÈ°±ËÖ¶Áï∞Â∏∏Ë≥áÊñôÈõÜ‰∏≠ÔºåÊ∂µËìãËÉéÂÖí‰∏≠Ê®ûÁ•ûÁ∂ìÁ≥ªÁµ± (CNS) ÁöÑÂõõÁ®ÆÂÖ∏ÂûãÁï∞Â∏∏ÔºöÁÑ°ËÖ¶Áóá„ÄÅËÖ¶ËÜ®Âá∫ÔºàÂåÖÊã¨ËÖ¶ËÜúËÜ®Âá∫Ôºâ„ÄÅÂÖ®ÂâçËÖ¶ÁóáÂíåËÑäË£ÇÔºåÁóÖ‰∫∫Â±§Á¥öÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÈÅîÂà∞ 94.5%ÔºåAUROC ÂÄºÁÇ∫ 99.3%„ÄÇÂú®Â≠êÁæ§ÂàÜÊûê‰∏≠ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅ©Áî®ÊñºÊï¥ÂÄãÂ¶äÂ®†ÊúüÔºå‰∏îËÉΩËâØÂ•ΩËæ®Ë≠ò‰ªª‰ΩïÂ¶äÂ®†ÊúüÁöÑËÉéÂÖíÁï∞Â∏∏È°ûÂûã„ÄÇÁñäÂä†Âú®Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉè‰∏äÁöÑÁÜ±Âúñ‰∏çÂÉÖÊèê‰æõÊºîÁÆóÊ≥ïÁöÑË¶ñË¶∫Ë©ÆÈáãÔºå‰πüÈÄèÈÅéÁ™ÅÈ°ØÈúÄË¶ÅÊ™¢Ë¶ñÁöÑÈóúÈçµÂçÄÂüüÔºåÊèê‰æõÁõ¥Ë¶∫ÁöÑË¶ñË¶∫ËºîÂä©Â∑•ÂÖ∑Áµ¶ÈÜ´Â∏´ÔºåÂçîÂä©ÈÜ´Â∏´Âø´ÈÄüËæ®Ë≠òÂíåÈ©óË≠âÈóúÈçµÂçÄÂüü„ÄÇÊúÄÂæåÔºåÂõûÊ∫ØÊÄßÈñ±ËÆÄÁ†îÁ©∂È°ØÁ§∫ÔºåÁµêÂêà DL Á≥ªÁµ±ÁöÑËá™ÂãïÈ†êÊ∏¨ÂíåÊîæÂ∞ÑÁßëÈÜ´Â∏´ÁöÑÂ∞àÊ•≠Âà§Êñ∑ÔºåÂèØ‰ª•ÊúâÊïàÊîπÂñÑË®∫Êñ∑Ê∫ñÁ¢∫Â∫¶ÂíåÊïàÁéáÔºå‰∏¶Èôç‰ΩéË™§Ë®∫ÁéáÔºåÈÄôÂÖ∑ÊúâÈáçË¶ÅÁöÑËá®Â∫äÊáâÁî®ÂâçÊôØ„ÄÇ

##### **Efficient Standardization of Clinical Notes using Large Language Models**
2501.00644v1 by Daniel B. Hier, Michael D. Carrithers, Thanh Son Do, Tayo Obafemi-Ajayi

Clinician notes are a rich source of patient information but often contain
inconsistencies due to varied writing styles, colloquialisms, abbreviations,
medical jargon, grammatical errors, and non-standard formatting. These
inconsistencies hinder the extraction of meaningful data from electronic health
records (EHRs), posing challenges for quality improvement, population health,
precision medicine, decision support, and research.
  We present a large language model approach to standardizing a corpus of 1,618
clinical notes. Standardization corrected an average of $4.9 +/- 1.8$
grammatical errors, $3.3 +/- 5.2$ spelling errors, converted $3.1 +/- 3.0$
non-standard terms to standard terminology, and expanded $15.8 +/- 9.1$
abbreviations and acronyms per note. Additionally, notes were re-organized into
canonical sections with standardized headings. This process prepared notes for
key concept extraction, mapping to medical ontologies, and conversion to
interoperable data formats such as FHIR.
  Expert review of randomly sampled notes found no significant data loss after
standardization. This proof-of-concept study demonstrates that standardization
of clinical notes can improve their readability, consistency, and usability,
while also facilitating their conversion into interoperable data formats.

ÊëòË¶ÅÔºöËá®Â∫äÈÜ´Â∏´ÁöÑÁ≠ÜË®òÊòØË±êÂØåÁöÑÁóÖ‰∫∫Ë≥áË®ä‰æÜÊ∫êÔºå‰ΩÜÂ∏∏Â∏∏Âõ†ÁÇ∫Êõ∏ÂØ´È¢®Ê†º‰∏çÂêå„ÄÅÊÖ£Áî®Ë™û„ÄÅÁ∏ÆÂØ´„ÄÅÈÜ´Â≠∏Ë°ìË™û„ÄÅÊñáÊ≥ïÈåØË™§ÂíåÈùûÊ®ôÊ∫ñÊ†ºÂºèËÄåÂåÖÂê´‰∏ç‰∏ÄËá¥ÁöÑÂú∞Êñπ„ÄÇÈÄô‰∫õ‰∏ç‰∏ÄËá¥ÊúÉÈòªÁ§ôÂæûÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) ‰∏≠ËêÉÂèñÊúâÊÑèÁæ©ÁöÑË≥áÊñôÔºåÂ∞çÂìÅË≥™ÊîπÂñÑ„ÄÅ‰∫∫Âè£ÂÅ•Â∫∑„ÄÅÁ≤æÊ∫ñÈÜ´ÁôÇ„ÄÅÊ±∫Á≠ñÊîØÊè¥ÂíåÁ†îÁ©∂ÊßãÊàêÊåëÊà∞„ÄÇ
ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÊñπÊ≥ï‰æÜÊ®ôÊ∫ñÂåñ 1,618 ‰ªΩËá®Â∫äÁ≠ÜË®òÁöÑË™ûÊñôÂ∫´„ÄÇÊ®ôÊ∫ñÂåñÂπ≥ÂùáÊõ¥Ê≠£‰∫Ü $4.9 +/- 1.8$ ÂÄãÊñáÊ≥ïÈåØË™§„ÄÅ$3.3 +/- 5.2$ ÂÄãÊãºÂ≠óÈåØË™§ÔºåÂ∞á $3.1 +/- 3.0$ ÂÄãÈùûÊ®ôÊ∫ñË°ìË™ûËΩâÊèõÁÇ∫Ê®ôÊ∫ñË°ìË™ûÔºå‰∏¶Êì¥ÂÖÖ‰∫ÜÊØè‰ªΩÁ≠ÜË®ò‰∏≠ $15.8 +/- 9.1$ ÂÄãÁ∏ÆÂØ´ÂíåÈ¶ñÂ≠óÊØçÁ∏ÆÁï•Â≠ó„ÄÇÊ≠§Â§ñÔºåÁ≠ÜË®òË¢´ÈáçÊñ∞ÁµÑÁπîÊàêÂÖ∑ÊúâÊ®ôÊ∫ñÊ®ôÈ°åÁöÑÊ≠£Ë¶èÁ´†ÁØÄ„ÄÇÈÄôÂÄãÈÅéÁ®ãÊ∫ñÂÇô‰∫ÜÁ≠ÜË®òÔºåÁî®ÊñºÈóúÈçµÊ¶ÇÂøµËêÉÂèñ„ÄÅÂ∞çÊáâÂà∞ÈÜ´Â≠∏Êú¨‰ΩìÔºå‰ª•ÂèäËΩâÊèõÁÇ∫ÂèØ‰∫íÊìç‰ΩúÁöÑË≥áÊñôÊ†ºÂºèÔºå‰æãÂ¶Ç FHIR„ÄÇ
Â∞çÈö®Ê©üÊäΩÊ®£ÁöÑÁ≠ÜË®òÈÄ≤Ë°åÂ∞àÂÆ∂ÂØ©Êü•ÂæåÁôºÁèæÔºåÂú®Ê®ôÊ∫ñÂåñÂæåÊ≤íÊúâÈ°ØËëóÁöÑË≥áÊñôÈÅ∫Â§±„ÄÇÈÄôÂÄãÊ¶ÇÂøµÈ©óË≠âÁ†îÁ©∂Ë≠âÊòé‰∫ÜËá®Â∫äÁ≠ÜË®òÁöÑÊ®ôÊ∫ñÂåñÂèØ‰ª•ÊîπÂñÑÂÖ∂ÂèØËÆÄÊÄß„ÄÅ‰∏ÄËá¥ÊÄßÂíåÂèØÁî®ÊÄßÔºåÂêåÊôÇ‰πü‰øÉÈÄ≤ÂÖ∂ËΩâÊèõÁÇ∫ÂèØ‰∫íÊìç‰ΩúÁöÑË≥áÊñôÊ†ºÂºè„ÄÇ

##### **LLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models**
2501.05464v1 by Hang Yang, Hao Chen, Hui Guo, Yineng Chen, Ching-Sheng Lin, Shu Hu, Jinrong Hu, Xi Wu, Xin Wang

Accurate and efficient question-answering systems are essential for
delivering high-quality patient care in the medical field. While Large Language
Models (LLMs) have made remarkable strides across various domains, they
continue to face significant challenges in medical question answering,
particularly in understanding domain-specific terminologies and performing
complex reasoning. These limitations undermine their effectiveness in critical
medical applications. To address these issues, we propose a novel approach
incorporating similar case generation within a multi-agent medical
question-answering (MedQA) system. Specifically, we leverage the Llama3.1:70B
model, a state-of-the-art LLM, in a multi-agent architecture to enhance
performance on the MedQA dataset using zero-shot learning. Our method
capitalizes on the model's inherent medical knowledge and reasoning
capabilities, eliminating the need for additional training data. Experimental
results show substantial performance gains over existing benchmark models, with
improvements of 7% in both accuracy and F1-score across various medical QA
tasks. Furthermore, we examine the model's interpretability and reliability in
addressing complex medical queries. This research not only offers a robust
solution for medical question answering but also establishes a foundation for
broader applications of LLMs in the medical domain.

ÊëòË¶ÅÔºöÁ≤æÊ∫ñÈ´òÊïàÁöÑÂïèÈ°åËß£Á≠îÁ≥ªÁµ±Â∞çÊñºÊèê‰æõÈÜ´ÁôÇÈ†òÂüüÁöÑÈ´òÂìÅË≥™ÁóÖ‰∫∫ÁÖßË≠∑Ëá≥ÈóúÈáçË¶Å„ÄÇÈõñÁÑ∂Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÂÄãÈ†òÂüüÈÉΩÊúâÈ°ØËëóÈÄ≤Â±ïÔºå‰ΩÜÂÆÉÂÄëÂú®ÈÜ´ÁôÇÂïèÈ°åËß£Á≠î‰∏≠‰ªçÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÁêÜËß£ÁâπÂÆöÈ†òÂüüÁöÑË°ìË™ûÂíåÂü∑Ë°åË§áÈõúÊé®ÁêÜÊñπÈù¢„ÄÇÈÄô‰∫õÈôêÂà∂ÂΩ±Èüø‰∫ÜÂÆÉÂÄëÂú®ÈóúÈçµÈÜ´ÁôÇÊáâÁî®‰∏≠ÁöÑÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÂ∞áÈ°û‰ººÊ°à‰æãÁîüÊàêÊï¥ÂêàÂà∞Â§ö‰∏ªÈ´îÈÜ´ÁôÇÂïèÈ°åËß£Á≠î (MedQA) Á≥ªÁµ±‰∏≠„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂú®Â§ö‰∏ªÈ´îÊû∂Êßã‰∏≠Âà©Áî®ÊúÄÂÖàÈÄ≤ÁöÑ LLM Llama3.1:70B Ê®°ÂûãÔºå‰ª•‰ΩøÁî®Èõ∂Ê¨°Â≠∏Áøí‰æÜÂ¢ûÂº∑ MedQA Ë≥áÊñôÈõÜÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂà©Áî®‰∫ÜË©≤Ê®°ÂûãÂÖßÂª∫ÁöÑÈÜ´ÁôÇÁü•Ë≠òÂíåÊé®ÁêÜËÉΩÂäõÔºåÊ∂àÈô§‰∫ÜÂ∞çÈ°çÂ§ñË®ìÁ∑¥Ë≥áÊñôÁöÑÈúÄÊ±Ç„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåËàáÁèæÊúâÁöÑÂü∫Ê∫ñÊ®°ÂûãÁõ∏ÊØîÔºåÊïàËÉΩÊúâÈ°ØËëóÊèêÂçáÔºåÂú®ÂêÑÁ®ÆÈÜ´ÁôÇÂïèÁ≠î‰ªªÂãô‰∏≠ÔºåÊ∫ñÁ¢∫Â∫¶Âíå F1 ÂàÜÊï∏ÈÉΩÊèêÂçá‰∫Ü 7%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜË©≤Ê®°ÂûãÂú®ÂõûÁ≠îË§áÈõúÈÜ´ÁôÇÂïèÈ°åÊôÇÁöÑË©ÆÈáãÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÈÄôÈ†ÖÁ†îÁ©∂‰∏çÂÉÖÁÇ∫ÈÜ´ÁôÇÂïèÈ°åËß£Á≠îÊèê‰æõ‰∫ÜÂº∑ÂÅ•ÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰πüÁÇ∫ LLM Âú®ÈÜ´ÁôÇÈ†òÂüüÁöÑÊõ¥Âª£Ê≥õÊáâÁî®Â•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ

##### **Pan-infection Foundation Framework Enables Multiple Pathogen Prediction**
2501.01462v1 by Lingrui Zhang, Haonan Wu, Nana Jin, Chenqing Zheng, Jize Xie, Qitai Cai, Jun Wang, Qin Cao, Xubin Zheng, Jiankun Wang, Lixin Cheng

Host-response-based diagnostics can improve the accuracy of diagnosing
bacterial and viral infections, thereby reducing inappropriate antibiotic
prescriptions. However, the existing cohorts with limited sample size and
coarse infections types are unable to support the exploration of an accurate
and generalizable diagnostic model. Here, we curate the largest infection
host-response transcriptome data, including 11,247 samples across 89 blood
transcriptome datasets from 13 countries and 21 platforms. We build a
diagnostic model for pathogen prediction starting from a pan-infection model as
foundation (AUC = 0.97) based on the pan-infection dataset. Then, we utilize
knowledge distillation to efficiently transfer the insights from this "teacher"
model to four lightweight pathogen "student" models, i.e., staphylococcal
infection (AUC = 0.99), streptococcal infection (AUC = 0.94), HIV infection
(AUC = 0.93), and RSV infection (AUC = 0.94), as well as a sepsis "student"
model (AUC = 0.99). The proposed knowledge distillation framework not only
facilitates the diagnosis of pathogens using pan-infection data, but also
enables an across-disease study from pan-infection to sepsis. Moreover, the
framework enables high-degree lightweight design of diagnostic models, which is
expected to be adaptively deployed in clinical settings.

ÊëòË¶ÅÔºöÂü∫ÊñºÂÆø‰∏ªÂèçÊáâÁöÑË®∫Êñ∑ÂèØ‰ª•ÊèêÈ´òÁ¥∞ËèåÂíåÁóÖÊØíÊÑüÊüìÁöÑË®∫Êñ∑Ê∫ñÁ¢∫Â∫¶ÔºåÂæûËÄåÊ∏õÂ∞ë‰∏çÈÅ©Áï∂ÁöÑÊäóÁîüÁ¥†ËôïÊñπ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊ®£Êú¨ÈáèÊúâÈôê„ÄÅÊÑüÊüìÈ°ûÂûãÁ≤óÁ≥ôÁöÑÁæ§ÁµÑÁÑ°Ê≥ïÊîØÊåÅÊ∫ñÁ¢∫‰∏îÂèØÊ¶ÇÂåñÁöÑË®∫Êñ∑Ê®°ÂûãÁöÑÊé¢Á¥¢„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊï¥ÁêÜ‰∫ÜÊúÄÂ§ßÁöÑÊÑüÊüìÂÆø‰∏ªÂèçÊáâËΩâÈåÑÁµÑÊï∏ÊìöÔºåÂåÖÊã¨‰æÜËá™ 13 ÂÄãÂúãÂÆ∂Âíå 21 ÂÄãÂπ≥Âè∞ÁöÑ 89 ÂÄãË°ÄÊ∂≤ËΩâÈåÑÁµÑÊï∏ÊìöÈõÜ‰∏≠ÁöÑ 11,247 ÂÄãÊ®£Êú¨„ÄÇÊàëÂÄëÂæûÊ≥õÊÑüÊüìÊ®°ÂûãÈñãÂßãÂª∫Á´ã‰∏ÄÂÄãÁî®ÊñºÁóÖÂéüÈ´îÈ†êÊ∏¨ÁöÑË®∫Êñ∑Ê®°ÂûãÔºå‰ΩúÁÇ∫Âü∫Á§é (AUC = 0.97)ÔºåË©≤Ê®°ÂûãÂü∫ÊñºÊ≥õÊÑüÊüìÊï∏ÊìöÈõÜ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂà©Áî®Áü•Ë≠òËí∏È§æÊúâÊïàÂú∞Â∞áÈÄôÂÄã„ÄåÊïôÂ∏´„ÄçÊ®°Âûã‰∏≠ÁöÑË¶ãËß£ËΩâÁßªÂà∞ÂõõÂÄãËºïÈáèÁ¥öÁóÖÂéüÈ´î„ÄåÂ≠∏Áîü„ÄçÊ®°ÂûãÔºåÂç≥Ëë°ËêÑÁêÉËèåÊÑüÊüì (AUC = 0.99)„ÄÅÈèàÁêÉËèåÊÑüÊüì (AUC = 0.94)„ÄÅHIV ÊÑüÊüì (AUC = 0.93) Âíå RSV ÊÑüÊüì (AUC = 0.94)Ôºå‰ª•Âèä‰∏ÄÂÄãÊïóË°ÄÁóá„ÄåÂ≠∏Áîü„ÄçÊ®°Âûã (AUC = 0.99)„ÄÇÊâÄÊèêÂá∫ÁöÑÁü•Ë≠òËí∏È§æÊ°ÜÊû∂‰∏çÂÉÖ‰øÉÈÄ≤‰∫Ü‰ΩøÁî®Ê≥õÊÑüÊüìÊï∏ÊìöË®∫Êñ∑ÁóÖÂéüÈ´îÔºåÈÇÑÂØ¶Áèæ‰∫ÜÂæûÊ≥õÊÑüÊüìÂà∞ÊïóË°ÄÁóáÁöÑË∑®ÁñæÁóÖÁ†îÁ©∂„ÄÇÊ≠§Â§ñÔºåË©≤Ê°ÜÊû∂‰ΩøË®∫Êñ∑Ê®°ÂûãËÉΩÂ§†ÈÄ≤Ë°åÈ´òÂ∫¶ËºïÈáèÁ¥öË®≠Ë®àÔºåÈ†êË®àÂ∞áÈÅ©ÊáâÊÄßÂú∞ÈÉ®ÁΩ≤Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠„ÄÇ

##### **A Hybrid Deep Learning and Model-Checking Framework for Accurate Brain Tumor Detection and Validation**
2501.01991v1 by Lahcen El Fatimi, Elhoucine Elfatimi, Hanifa Bouchaneb

Model checking, a formal verification technique, ensures systems meet
predefined requirements, playing a crucial role in minimizing errors and
enhancing quality during development. This paper introduces a novel hybrid
framework integrating model checking with deep learning for brain tumor
detection and validation in medical imaging. By combining model-checking
principles with CNN-based feature extraction and K-FCM clustering for
segmentation, the proposed approach enhances the reliability of tumor detection
and segmentation. Experimental results highlight the framework's effectiveness,
achieving 98\% accuracy, 96.15\% precision, and 100\% recall, demonstrating its
potential as a robust tool for advanced medical image analysis.

ÊëòË¶ÅÔºöÊ®°ÂûãÊ™¢Êü•ÊòØ‰∏ÄÁ®ÆÊ≠£ÂºèÈ©óË≠âÊäÄË°ìÔºåÁî®ÊñºÁ¢∫‰øùÁ≥ªÁµ±Á¨¶ÂêàÈ†êÂÖàÂÆöÁæ©ÁöÑË¶ÅÊ±ÇÔºåÂú®ÈñãÁôºÈÅéÁ®ã‰∏≠ÊâÆÊºîËëóÊ•µÂÖ∂ÈáçË¶ÅÁöÑËßíËâ≤ÔºåÁî®ÊñºÊúÄÂ∞èÂåñÈåØË™§‰∏¶ÊèêÂçáÂìÅË≥™„ÄÇÈÄôÁØáË´ñÊñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊï¥ÂêàÊ®°ÂûãÊ™¢Êü•ËàáÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂâµÊñ∞Ê∑∑ÂêàÊ°ÜÊû∂ÔºåÁî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑËÖ¶Áò§ÂÅµÊ∏¨ËàáÈ©óË≠â„ÄÇÈÄèÈÅéÁµêÂêàÊ®°ÂûãÊ™¢Êü•ÂéüÂâáËàáÂü∫Êñº CNN ÁöÑÁâπÂæµËêÉÂèñ‰ª•ÂèäÁî®ÊñºÂàÜÂâ≤ÁöÑ K-FCM ËÅöÈ°ûÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊèêÂçá‰∫ÜËÖ´Áò§ÂÅµÊ∏¨ËàáÂàÜÂâ≤ÁöÑÂèØÈù†Â∫¶„ÄÇÂØ¶È©óÁµêÊûúÁ™ÅÈ°Ø‰∫ÜÈÄôÂÄãÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄßÔºåÈÅîÂà∞‰∫Ü 98% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÅ96.15% ÁöÑÁ≤æÁ¢∫Â∫¶Ôºå‰ª•Âèä 100% ÁöÑÂè¨ÂõûÁéáÔºåÈ°ØÁ§∫Âá∫ÂÖ∂‰ΩúÁÇ∫ÈÄ≤ÈöéÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÁöÑÂº∑ÂÅ•Â∑•ÂÖ∑ÁöÑÊΩõÂäõ„ÄÇ

##### **GAN-TAT: A Novel Framework Using Protein Interaction Networks in Druggable Gene Identification**
2501.01458v1 by George Yuanji Wang, Srisharan Murugesan, Aditya Prince Rohatgi

Identifying druggable genes is essential for developing effective
pharmaceuticals. With the availability of extensive, high-quality data,
computational methods have become a significant asset. Protein Interaction
Network (PIN) is valuable but challenging to implement due to its high
dimensionality and sparsity. Previous methods relied on indirect integration,
leading to resolution loss. This study proposes GAN-TAT, a framework utilizing
an advanced graph embedding technology, ImGAGN, to directly integrate PIN for
druggable gene inference work. Tested on three Pharos datasets, GAN-TAT
achieved the highest AUC-ROC score of 0.951 on Tclin. Further evaluation shows
that GAN-TAT's predictions are supported by clinical evidence, highlighting its
potential practical applications in pharmacogenomics. This research represents
a methodological attempt with the direct utilization of PIN, expanding
potential new solutions for developing drug targets. The source code of GAN-TAT
is available at (https://github.com/george-yuanji-wang/GAN-TAT).

ÊëòË¶ÅÔºöË≠òÂà•ÂèØËó•Áâ©ÂåñÂü∫Âõ†Â∞çÊñºÈñãÁôºÊúâÊïàÁöÑËó•Áâ©Ëá≥ÈóúÈáçË¶Å„ÄÇÈö®ËëóÂ§ßÈáèÈ´òÂìÅË≥™Êï∏ÊìöÁöÑÂá∫ÁèæÔºåË®àÁÆóÊñπÊ≥ïÂ∑≤ÊàêÁÇ∫‰∏ÄÈ†ÖÈáçË¶ÅÁöÑË≥áÁî¢„ÄÇËõãÁôΩË≥™‰∫§‰∫íÁ∂≤Áµ° (PIN) ÂæàÊúâÂÉπÂÄºÔºå‰ΩÜÁî±ÊñºÂÖ∂È´òÁ∂≠Â∫¶ÂíåÁ®ÄÁñèÊÄßÔºåÂØ¶‰ΩúËµ∑‰æÜÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂÖàÂâçÁöÑËæ¶Ê≥ï‰æùË≥¥ÊñºÈñìÊé•Êï¥ÂêàÔºåÂ∞éËá¥Ëß£ÊûêÂ∫¶Èôç‰Ωé„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫ GAN-TATÔºå‰∏ÄÂÄãÂà©Áî®ÂÖàÈÄ≤ÂúñÂΩ¢ÂµåÂÖ•ÊäÄË°ì ImGAGN ÁöÑÊû∂ÊßãÔºåÁõ¥Êé•Êï¥Âêà PIN ‰ª•ÈÄ≤Ë°åÂèØËó•Áâ©ÂåñÂü∫Âõ†Êé®Ë´ñÂ∑•‰Ωú„ÄÇÂú®‰∏âÂÄã Pharos Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåGAN-TAT Âú® Tclin ‰∏äÈÅîÂà∞‰∫ÜÊúÄÈ´òÁöÑ AUC-ROC ÂàÜÊï∏ 0.951„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑË©ï‰º∞È°ØÁ§∫ÔºåGAN-TAT ÁöÑÈ†êÊ∏¨Áç≤Âæó‰∫ÜËá®Â∫äË≠âÊìöÁöÑÊîØÊåÅÔºåÁ™ÅÈ°Ø‰∫ÜÂÖ∂Âú®Ëó•Áâ©Âü∫Âõ†ÁµÑÂ≠∏‰∏≠ÁöÑÊΩõÂú®ÂØ¶ÈöõÊáâÁî®„ÄÇÊú¨Á†îÁ©∂‰ª£Ë°®‰∫Ü‰∏ÄÁ®ÆÁõ¥Êé•Âà©Áî® PIN ÁöÑÊñπÊ≥ïË´ñÂòóË©¶ÔºåÊì¥Â±ï‰∫ÜÈñãÁôºËó•Áâ©Èù∂Ê®ôÁöÑÊΩõÂú®Êñ∞Ëß£Ê±∫ÊñπÊ°à„ÄÇGAN-TAT ÁöÑÂéüÂßãÁ¢ºÂèØÂú® (https://github.com/george-yuanji-wang/GAN-TAT) ÂèñÂæó„ÄÇ

##### **Autonomous Alignment with Human Value on Altruism through Considerate Self-imagination and Theory of Mind**
2501.00320v2 by Haibo Tong, Enmeng Lu, Yinqian Sun, Zhengqiang Han, Chao Liu, Feifei Zhao, Yi Zeng

With the widespread application of Artificial Intelligence (AI) in human
society, enabling AI to autonomously align with human values has become a
pressing issue to ensure its sustainable development and benefit to humanity.
One of the most important aspects of aligning with human values is the
necessity for agents to autonomously make altruistic, safe, and ethical
decisions, considering and caring for human well-being. Current AI extremely
pursues absolute superiority in certain tasks, remaining indifferent to the
surrounding environment and other agents, which has led to numerous safety
risks. Altruistic behavior in human society originates from humans' capacity
for empathizing others, known as Theory of Mind (ToM), combined with predictive
imaginative interactions before taking action to produce thoughtful and
altruistic behaviors. Inspired by this, we are committed to endow agents with
considerate self-imagination and ToM capabilities, driving them through
implicit intrinsic motivations to autonomously align with human altruistic
values. By integrating ToM within the imaginative space, agents keep an eye on
the well-being of other agents in real time, proactively anticipate potential
risks to themselves and others, and make thoughtful altruistic decisions that
balance negative effects on the environment. The ancient Chinese story of Sima
Guang Smashes the Vat illustrates the moral behavior of the young Sima Guang
smashed a vat to save a child who had accidentally fallen into it, which is an
excellent reference scenario for this paper. We design an experimental scenario
similar to Sima Guang Smashes the Vat and its variants with different
complexities, which reflects the trade-offs and comprehensive considerations
between self-goals, altruistic rescue, and avoiding negative side effects.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂú®‰∫∫È°ûÁ§æÊúÉ‰∏≠ÁöÑÂª£Ê≥õÊáâÁî®ÔºåËÆì AI Ëá™‰∏ªËàá‰∫∫È°ûÂÉπÂÄºËßÄ‰∏ÄËá¥Â∑≤ÊàêÁÇ∫Á¢∫‰øùÂÖ∂Ê∞∏Á∫åÁôºÂ±ïÂíåÈÄ†Á¶è‰∫∫È°ûÁöÑÁï∂Âãô‰πãÊÄ•„ÄÇËàá‰∫∫È°ûÂÉπÂÄºËßÄ‰∏ÄËá¥ÊúÄÈáçË¶ÅÁöÑÈù¢Âêë‰πã‰∏ÄÔºåÂú®Êñº‰ª£ÁêÜ‰∫∫ÂøÖÈ†àËá™‰∏ªÂÅöÂá∫Âà©‰ªñ„ÄÅÂÆâÂÖ®„ÄÅ‰∏îÂêà‰πéÈÅìÂæ∑ÁöÑÊ±∫Á≠ñÔºåËÄÉÈáè‰∏¶ÈóúÊá∑‰∫∫È°ûÁ¶èÁ•â„ÄÇÁõÆÂâçÁöÑ AI Âú®ÁâπÂÆö‰ªªÂãô‰∏≠Ê•µÂäõËøΩÊ±ÇÁµïÂ∞çÂÑ™Ë∂äÊÄßÔºåÂ∞çÊñºÂë®ÈÅ≠Áí∞Â¢ÉÂíåÂÖ∂ÂÆÉ‰ª£ÁêÜ‰∫∫Êº†‰∏çÈóúÂøÉÔºåÈÄôÂ∑≤Â∞éËá¥Ë®±Â§öÂÆâÂÖ®È¢®Èö™„ÄÇ‰∫∫È°ûÁ§æÊúÉ‰∏≠ÁöÑÂà©‰ªñË°åÁÇ∫Ê∫êËá™Êñº‰∫∫È°ûÂêåÁêÜ‰ªñ‰∫∫ÁöÑËÉΩÂäõÔºåÁ®±ÁÇ∫ÂøÉÊô∫ÁêÜË´ñÔºàToMÔºâÔºåÁµêÂêàÂú®Êé°ÂèñË°åÂãïÂâçÈÄ≤Ë°åÈ†êÊ∏¨ÊÄßÁöÑÊÉ≥ÂÉè‰∫íÂãïÔºå‰ª•Áî¢ÁîüÂë®Âà∞‰∏îÂà©‰ªñÁöÑË°åÁÇ∫„ÄÇÂèóÂà∞Ê≠§ÂïüÁôºÔºåÊàëÂÄëËá¥ÂäõÊñºË≥¶‰∫à‰ª£ÁêÜ‰∫∫È´îË≤ºÁöÑËá™ÊàëÊÉ≥ÂÉèÂíå ToM ËÉΩÂäõÔºåÈÄèÈÅéÈö±Âê´ÁöÑÂÖßÂú®ÂãïÊ©üÈ©Ö‰Ωø‰ªñÂÄëËá™‰∏ªËàá‰∫∫È°ûÂà©‰ªñÂÉπÂÄºËßÄ‰∏ÄËá¥„ÄÇÈÄèÈÅéÂ∞á ToM Êï¥ÂêàÂú®ÊÉ≥ÂÉèÁ©∫Èñì‰∏≠Ôºå‰ª£ÁêÜ‰∫∫ËÉΩÂç≥ÊôÇÈóúÊ≥®ÂÖ∂‰ªñ‰ª£ÁêÜ‰∫∫ÁöÑÁ¶èÁ•âÔºå‰∏ªÂãïÈ†êÊ∏¨Â∞çËá™Ë∫´Âíå‰ªñ‰∫∫ÊΩõÂú®ÁöÑÈ¢®Èö™Ôºå‰∏¶ÂÅöÂá∫Âë®Âà∞‰∏îÂà©‰ªñÁöÑÊ±∫Á≠ñÔºåÂπ≥Ë°°Â∞çÁí∞Â¢ÉÁöÑË≤†Èù¢ÂΩ±Èüø„ÄÇ‰∏≠ÂúãÂè§‰ª£ÊïÖ‰∫ã„ÄåÂè∏È¶¨ÂÖâÁ†∏Áº∏„ÄçË™™Êòé‰∫ÜÂπ¥ÂπºÁöÑÂè∏È¶¨ÂÖâÁÇ∫‰∫ÜÊïë‰∏ÄÂÄã‰∏çÂ∞èÂøÉÊéâÈÄ≤Ê∞¥Áº∏‰∏≠ÁöÑÂ≠©Â≠êËÄåÁ†∏Á†¥Ê∞¥Áº∏ÁöÑÈÅìÂæ∑Ë°åÁÇ∫ÔºåÊòØÊú¨ÊñáÁöÑÁµï‰Ω≥ÂèÉËÄÉÊÉÖÂ¢É„ÄÇÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãËàá„ÄåÂè∏È¶¨ÂÖâÁ†∏Áº∏„ÄçÁõ∏‰ººÁöÑÂØ¶È©óÊÉÖÂ¢ÉÔºå‰ª•ÂèäÂÖ∑Êúâ‰∏çÂêåË§áÈõúÊÄßÁöÑËÆäÈ´îÔºåÂèçÊò†‰∫ÜËá™ÊàëÁõÆÊ®ô„ÄÅÂà©‰ªñÊïëÊè¥ÂíåÈÅøÂÖçË≤†Èù¢ÂâØ‰ΩúÁî®‰πãÈñìÁöÑÊ¨äË°°ÂíåÁ∂úÂêàËÄÉÈáè„ÄÇ

##### **A Fourfold Pathogen Reference Ontology Suite**
2501.01454v1 by Shane Babcock, Carter Benson, Giacomo De Colle, Sydney Cohen, Alexander D. Diehl, Ram A. N. R. Challa, Anthony Huffman, Yongqun He, John Beverley

Infectious diseases remain a critical global health challenge, and the
integration of standardized ontologies plays a vital role in managing related
data. The Infectious Disease Ontology (IDO) and its extensions, such as the
Coronavirus Infectious Disease Ontology (CIDO), are essential for organizing
and disseminating information related to infectious diseases. The COVID-19
pandemic highlighted the need for updating IDO and its virus-specific
extensions. There is an additional need to update IDO extensions specific to
bacteria, fungus, and parasite infectious diseases. We adopt the "hub and
spoke" methodology to generate pathogen-specific extensions of IDO: Virus
Infectious Disease Ontology (VIDO), Bacteria Infectious Disease Ontology
(BIDO), Mycosis Infectious Disease Ontology (MIDO), and Parasite Infectious
Disease Ontology (PIDO). The creation of pathogen-specific reference ontologies
advances modularization and reusability of infectious disease data within the
IDO ecosystem. Future work will focus on further refining these ontologies,
creating new extensions, and developing application ontologies based on them,
in line with ongoing efforts to standardize biological and biomedical
terminologies for improved data sharing and analysis.

ÊëòË¶ÅÔºöÂÇ≥ÊüìÁóÖ‰ªçÊòØ‰∏ÄÈ†ÖÂÖ®ÁêÉÊÄßÁöÑÂÅ•Â∫∑ÊåëÊà∞ÔºåËÄåÊ®ôÊ∫ñÂåñÊú¨È´îÁöÑÊï¥ÂêàÂú®ÁÆ°ÁêÜÁõ∏ÈóúÊï∏ÊìöÊñπÈù¢ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÂÇ≥ÊüìÁóÖÊú¨È´î (IDO) ÂèäÂÖ∂Êì¥ÂÖÖÔºå‰æãÂ¶ÇÂÜ†ÁãÄÁóÖÊØíÂÇ≥ÊüìÁóÖÊú¨È´î (CIDO)ÔºåÂ∞çÊñºÁµÑÁπîÂíåÂÇ≥Êí≠ËàáÂÇ≥ÊüìÁóÖÁõ∏ÈóúÁöÑË≥áË®äËá≥ÈóúÈáçË¶Å„ÄÇCOVID-19 Â§ßÊµÅË°åÂá∏È°Ø‰∫ÜÊõ¥Êñ∞ IDO ÂèäÂÖ∂ÁâπÂÆöÊñºÁóÖÊØíÁöÑÊì¥ÂÖÖÁöÑÈúÄÊ±Ç„ÄÇÊ≠§Â§ñÔºåÈÇÑÊúâÊõ¥Êñ∞ÁâπÂÆöÊñºÁ¥∞Ëèå„ÄÅÁúüËèåÂíåÂØÑÁîüËü≤ÂÇ≥ÊüìÁóÖÁöÑ IDO Êì¥ÂÖÖÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÊé°Áî®„ÄåÊ®ûÁ¥êËºªÊ¢ù„ÄçÊñπÊ≥ï‰æÜÁî¢Áîü IDO ÁöÑÁâπÂÆöÊñºÁóÖÂéüÈ´îÁöÑÊì¥ÂÖÖÔºöÁóÖÊØíÂÇ≥ÊüìÁóÖÊú¨È´î (VIDO)„ÄÅÁ¥∞ËèåÂÇ≥ÊüìÁóÖÊú¨È´î (BIDO)„ÄÅÁúüËèåÁóÖÂÇ≥ÊüìÁóÖÊú¨È´î (MIDO) ÂíåÂØÑÁîüËü≤ÂÇ≥ÊüìÁóÖÊú¨È´î (PIDO)„ÄÇÁâπÂÆöÊñºÁóÖÂéüÈ´îÁöÑÂèÉËÄÉÊú¨È´îÁöÑÂª∫Á´ãÔºå‰øÉËøõ‰∫Ü IDO ÁîüÊÖãÁ≥ªÁµ±ÂÖßÂÇ≥ÊüìÁóÖÊï∏ÊìöÁöÑÊ®°ÁµÑÂåñÂíåÂèØÈáçË§á‰ΩøÁî®ÊÄß„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂Â∑•‰ΩúÂ∞áÈáçÈªûÊîæÂú®ÈÄ≤‰∏ÄÊ≠•ÂÆåÂñÑÈÄô‰∫õÊú¨È´î„ÄÅÂª∫Á´ãÊñ∞ÁöÑÊì¥ÂÖÖÔºå‰ª•ÂèäÊ†πÊìöÈÄô‰∫õÊú¨È´îÈñãÁôºÊáâÁî®Êú¨È´îÔºåÈÄôËàáÊ®ôÊ∫ñÂåñÁîüÁâ©ÂíåÁîüÁâ©ÈÜ´Â≠∏Ë°ìË™û‰ª•ÊîπÂñÑÊï∏ÊìöÂÖ±‰∫´ÂíåÂàÜÊûêÁöÑÊåÅÁ∫åÂä™ÂäõÁõ∏‰∏ÄËá¥„ÄÇ

##### **CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM Hybrid for Assisting with Optimal Cancer Treatment and Care**
2501.00223v1 by Michael Gubanov, Anna Pyayt, Aleksandra Karolak

Here, we describe one of the first Web-scale hybrid Knowledge Graph
(KG)-Large Language Model (LLM), populated with the latest peer-reviewed
medical knowledge on colorectal Cancer. It is currently being evaluated to
assist with both medical research and clinical information retrieval tasks at
Moffitt Cancer Center, which is one of the top Cancer centers in the U.S. and
in the world. Our hybrid is remarkable as it serves the user needs better than
just an LLM, KG or a search-engine in isolation. LLMs as is are known to
exhibit hallucinations and catastrophic forgetting as well as are trained on
outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal,
ChEMBL, NCBI, and other require manual curation, hence are quickly getting
stale. CancerKG is unsupervised and is capable of automatically ingesting and
organizing the latest medical findings. To alleviate the LLMs shortcomings, the
verified KG serves as a Retrieval Augmented Generation (RAG) guardrail.
CancerKG exhibits 5 different advanced user interfaces, each tailored to serve
different data modalities better and more convenient for the user.

ÊëòË¶ÅÔºöÂú®Ê≠§ÔºåÊàë‰ª¨ÊèèËø∞‰∫ÜÁ¨¨‰∏Ä‰∏™ Web Á∫ßÊ∑∑ÂêàÁü•ËØÜÂõæË∞± (KG) - Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM)ÔºåÂÖ∂‰∏≠ÂÖÖÊñ•ÁùÄÊúâÂÖ≥ÁªìÁõ¥ËÇ†ÁôåÁöÑÊúÄÊñ∞ÂêåË°åËØÑÂÆ°ÂåªÂ≠¶Áü•ËØÜ„ÄÇÁõÆÂâçÊ≠£Âú®ËØÑ‰º∞ÂÆÉ‰ª•ÂçèÂä© Moffitt ÁôåÁóá‰∏≠ÂøÉËøõË°åÂåªÂ≠¶Á†îÁ©∂Âíå‰∏¥Â∫ä‰ø°ÊÅØÊ£ÄÁ¥¢‰ªªÂä°ÔºåËØ•‰∏≠ÂøÉÊòØÁæéÂõΩÂíå‰∏ñÁïåÈ°∂Á∫ßÁôåÁóá‰∏≠ÂøÉ‰πã‰∏Ä„ÄÇÊàë‰ª¨ÁöÑÊ∑∑Âêà‰ΩìÈùûÂ∏∏Âá∫Ëâ≤ÔºåÂõ†‰∏∫ÂÆÉÊØîÂ≠§Á´ãÁöÑ LLM„ÄÅKG ÊàñÊêúÁ¥¢ÂºïÊìéÊõ¥Â•ΩÂú∞Êª°Ë∂≥Áî®Êà∑ÈúÄÊ±Ç„ÄÇ‰ºóÊâÄÂë®Áü•ÔºåLLM ‰ºöÂá∫Áé∞ÂπªËßâÂíåÁÅæÈöæÊÄßÈÅóÂøòÔºåÂπ∂‰∏îÊòØÂú®ËøáÊó∂ÁöÑËØ≠ÊñôÂ∫ì‰∏äËøõË°åËÆ≠ÁªÉÁöÑ„ÄÇÊúÄÂÖàËøõÁöÑ KGÔºå‰æãÂ¶Ç PrimeKG„ÄÅcBioPortal„ÄÅChEMBL„ÄÅNCBI Á≠âÈúÄË¶Å‰∫∫Â∑•Êï¥ÁêÜÔºåÂõ†Ê≠§ÂæàÂø´Â∞±‰ºöËøáÊó∂„ÄÇCancerKG Êó†ÈúÄÁõëÁù£ÔºåËÉΩÂ§üËá™Âä®ÊëÑÂèñÂíåÁªÑÁªáÊúÄÊñ∞ÁöÑÂåªÂ≠¶ÂèëÁé∞„ÄÇ‰∏∫‰∫ÜÂáèËΩª LLM ÁöÑÁº∫ÁÇπÔºåÁªèËøáÈ™åËØÅÁöÑ KG ÂÖÖÂΩìÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) Êä§Ê†è„ÄÇCancerKG Â±ïÁ§∫‰∫Ü 5 Áßç‰∏çÂêåÁöÑÈ´òÁ∫ßÁî®Êà∑ÁïåÈù¢ÔºåÊØèÁßçÁïåÈù¢ÈÉΩÈíàÂØπÊúçÂä°‰∏çÂêåÁöÑÊï∞ÊçÆÊ®°ÂºèÔºå‰∏∫Áî®Êà∑Êèê‰æõÊõ¥Â•Ω„ÄÅÊõ¥Êñπ‰æøÁöÑÊúçÂä°„ÄÇ

##### **An Empirical Evaluation of Large Language Models on Consumer Health Questions**
2501.00208v1 by Moaiz Abrar, Yusuf Sermet, Ibrahim Demir

This study evaluates the performance of several Large Language Models (LLMs)
on MedRedQA, a dataset of consumer-based medical questions and answers by
verified experts extracted from the AskDocs subreddit. While LLMs have shown
proficiency in clinical question answering (QA) benchmarks, their effectiveness
on real-world, consumer-based, medical questions remains less understood.
MedRedQA presents unique challenges, such as informal language and the need for
precise responses suited to non-specialist queries. To assess model
performance, responses were generated using five LLMs: GPT-4o mini, Llama 3.1:
70B, Mistral-123B, Mistral-7B, and Gemini-Flash. A cross-evaluation method was
used, where each model evaluated its responses as well as those of others to
minimize bias. The results indicated that GPT-4o mini achieved the highest
alignment with expert responses according to four out of the five models'
judges, while Mistral-7B scored lowest according to three out of five models'
judges. This study highlights the potential and limitations of current LLMs for
consumer health medical question answering, indicating avenues for further
development.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Ë©ï‰º∞‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú® MedRedQA ‰∏äÁöÑÊïàËÉΩÔºåMedRedQA ÊòØ‰∏ÄÁµÑÊ∂àË≤ªËÄÖÈÜ´ÁôÇÂïèÈ°åËàáÁ≠îÊ°àÁöÑË≥áÊñôÈõÜÔºåÁî± AskDocs Â≠êÁâàÂ°ä‰∏≠Á∂ìÈÅéÈ©óË≠âÁöÑÂ∞àÂÆ∂ÊâÄÊèêÂá∫„ÄÇÂÑòÁÆ° LLM Â∑≤Âú®Ëá®Â∫äÂïèÈ°åËß£Á≠î (QA) Âü∫Ê∫ñ‰∏≠Â±ïÁèæÂá∫Â∞àÊ•≠Áü•Ë≠òÔºå‰ΩÜÂÆÉÂÄëÂú®ÁèæÂØ¶‰∏ñÁïå„ÄÅÊ∂àË≤ªËÄÖÁÇ∫Âü∫Á§éÁöÑÈÜ´ÁôÇÂïèÈ°å‰∏äÁöÑÊúâÊïàÊÄß‰ªçËºÉ‰∏çÊòéÁ¢∫„ÄÇMedRedQA ÊèêÂá∫Áç®ÁâπÁöÑÊåëÊà∞Ôºå‰æãÂ¶ÇÈùûÊ≠£ÂºèË™ûË®ÄÂíåÂ∞çÈùûÂ∞àÂÆ∂Êü•Ë©¢Êèê‰æõÁ≤æÁ¢∫ÂõûÊáâÁöÑÈúÄÊ±Ç„ÄÇÁÇ∫‰∫ÜË©ï‰º∞Ê®°ÂûãÊïàËÉΩÔºå‰ΩøÁî®‰∫îÂÄã LLM ÁîüÊàê‰∫ÜÂõûÊáâÔºöGPT-4o mini„ÄÅLlama 3.1Ôºö70B„ÄÅMistral-123B„ÄÅMistral-7B Âíå Gemini-Flash„ÄÇ‰ΩøÁî®‰∫Ü‰∫§ÂèâË©ï‰º∞ÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÊØèÂÄãÊ®°ÂûãË©ï‰º∞Ëá™Â∑±ÁöÑÂõûÊáâ‰ª•ÂèäÂÖ∂‰ªñÊ®°ÂûãÁöÑÂõûÊáâÔºå‰ª•ÊúÄÂ∞èÂåñÂÅèÂ∑Æ„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÊ†πÊìö‰∫îÂÄãÊ®°Âûã‰∏≠ÁöÑÂõõÂÄãÊ®°ÂûãË©ïÂØ©ÔºåGPT-4o mini ËàáÂ∞àÂÆ∂ÂõûÊáâÁöÑ‰∏ÄËá¥ÊÄßÊúÄÈ´òÔºåËÄåÊ†πÊìö‰∫îÂÄãÊ®°Âûã‰∏≠ÁöÑ‰∏âÂÄãÊ®°ÂûãË©ïÂØ©ÔºåMistral-7B ÁöÑÂàÜÊï∏ÊúÄ‰Ωé„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÁõÆÂâç LLM Âú®Ê∂àË≤ªËÄÖÂÅ•Â∫∑ÈÜ´ÁôÇÂïèÈ°åËß£Á≠îÊñπÈù¢ÁöÑÊΩõÂäõÂíåÈôêÂà∂Ôºå‰∏¶ÊåáÂá∫ÈÄ≤‰∏ÄÊ≠•ÁôºÂ±ïÁöÑÈÄîÂæë„ÄÇ

##### **GPT-4 on Clinic Depression Assessment: An LLM-Based Pilot Study**
2501.00199v1 by Giuliano Lorenzoni, Pedro Elkind Velmovitsky, Paulo Alencar, Donald Cowan

Depression has impacted millions of people worldwide and has become one of
the most prevalent mental disorders. Early mental disorder detection can lead
to cost savings for public health agencies and avoid the onset of other major
comorbidities. Additionally, the shortage of specialized personnel is a
critical issue because clinical depression diagnosis is highly dependent on
expert professionals and is time consuming.
  In this study, we explore the use of GPT-4 for clinical depression assessment
based on transcript analysis. We examine the model's ability to classify
patient interviews into binary categories: depressed and not depressed. A
comparative analysis is conducted considering prompt complexity (e.g., using
both simple and complex prompts) as well as varied temperature settings to
assess the impact of prompt complexity and randomness on the model's
performance.
  Results indicate that GPT-4 exhibits considerable variability in accuracy and
F1-Score across configurations, with optimal performance observed at lower
temperature values (0.0-0.2) for complex prompts. However, beyond a certain
threshold (temperature >= 0.3), the relationship between randomness and
performance becomes unpredictable, diminishing the gains from prompt
complexity.
  These findings suggest that, while GPT-4 shows promise for clinical
assessment, the configuration of the prompts and model parameters requires
careful calibration to ensure consistent results. This preliminary study
contributes to understanding the dynamics between prompt engineering and large
language models, offering insights for future development of AI-powered tools
in clinical settings.

ÊëòË¶ÅÔºöÊÜÇÈ¨±ÁóáÂΩ±ÈüøÂÖ®ÁêÉÊï∏ÁôæËê¨‰∫∫ÔºåÂ∑≤ÊàêÁÇ∫ÊúÄÊôÆÈÅçÁöÑÁ≤æÁ•ûÁñæÁóÖ‰πã‰∏Ä„ÄÇÊèêÊó©ÂÅµÊ∏¨Á≤æÁ•ûÁñæÁóÖÔºåËÉΩÁÇ∫ÂÖ¨ÂÖ±Ë°õÁîüÊ©üÊßãÁØÄÁúÅÊàêÊú¨Ôºå‰∏¶ÈÅøÂÖçÂÖ∂‰ªñ‰∏ªË¶ÅÂÖ±ÁóÖÁöÑÁôºÁîü„ÄÇÊ≠§Â§ñÔºåÂ∞àÊ•≠‰∫∫Âì°Áü≠Áº∫ÊòØ‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÔºåÂõ†ÁÇ∫Ëá®Â∫äÊÜÇÈ¨±ÁóáÁöÑË®∫Êñ∑È´òÂ∫¶‰æùË≥¥ÊñºÂ∞àÊ•≠‰∫∫Âì°Ôºå‰∏îËÄóÊôÇË≤ªÂäõ„ÄÇ
Âú®ÈÄôÂÄãÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰ΩøÁî® GPT-4 ÈÄ≤Ë°åËá®Â∫äÊÜÇÈ¨±ÁóáË©ï‰º∞ÔºåÂü∫Á§éÊòØË¨ÑÊú¨ÂàÜÊûê„ÄÇÊàëÂÄëÊ™¢Ë¶ñÊ®°ÂûãÂ∞áÁóÖ‰∫∫Ë®™Ë´áÂàÜÈ°ûÁÇ∫‰∫åÂÖÉÈ°ûÂà•ÔºàÊÜÇÈ¨±ÁóáÂíåÈùûÊÜÇÈ¨±ÁóáÔºâÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÈÄ≤Ë°åÊØîËºÉÂàÜÊûêÔºåËÄÉÈáèÊèêÁ§∫Ë§áÈõúÂ∫¶Ôºà‰æãÂ¶ÇÔºåÂêåÊôÇ‰ΩøÁî®Á∞°ÂñÆÂíåË§áÈõúÁöÑÊèêÁ§∫ÔºâÔºå‰ª•ÂèäÂêÑÁ®ÆÊ∫´Â∫¶Ë®≠ÂÆöÔºå‰ª•Ë©ï‰º∞ÊèêÁ§∫Ë§áÈõúÂ∫¶ÂíåÈö®Ê©üÊÄßÂ∞çÊ®°ÂûãÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇ
ÁµêÊûúÈ°ØÁ§∫ÔºåGPT-4 Âú®ÂêÑÁµÑÊÖã‰∏≠ÁöÑÊ∫ñÁ¢∫Â∫¶Âíå F1 ÂàÜÊï∏ËÆäÂåñÂæàÂ§ßÔºåÂú®Ë§áÈõúÊèêÁ§∫ÁöÑËºÉ‰ΩéÊ∫´Â∫¶ÂÄºÔºà0.0-0.2Ôºâ‰∏ãËßÄÂØüÂà∞ÊúÄ‰Ω≥ÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåË∂ÖÈÅéÊüêÂÄãÈñæÂÄºÔºàÊ∫´Â∫¶ >= 0.3ÔºâÂæåÔºåÈö®Ê©üÊÄßÂíåÊïàËÉΩ‰πãÈñìÁöÑÈóú‰øÇËÆäÂæóÈõ£‰ª•È†êÊ∏¨ÔºåÈôç‰Ωé‰∫ÜÊèêÁ§∫Ë§áÈõúÂ∫¶Â∏∂‰æÜÁöÑÊî∂Áõä„ÄÇ
ÈÄô‰∫õÁôºÁèæË°®ÊòéÔºåÈõñÁÑ∂ GPT-4 Âú®Ëá®Â∫äË©ï‰º∞ÊñπÈù¢È°ØÁ§∫Âá∫ÂâçÊôØÔºå‰ΩÜÊèêÁ§∫ÂíåÊ®°ÂûãÂèÉÊï∏ÁöÑÁµÑÊÖãÈúÄË¶Å‰ªîÁ¥∞Ê†°Ê∫ñÔºå‰ª•Á¢∫‰øùÁµêÊûúÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄôÈ†ÖÂàùÊ≠•Á†îÁ©∂ÊúâÂä©Êñº‰∫ÜËß£ÊèêÁ§∫Â∑•Á®ãÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰πãÈñìÁöÑÂãïÊÖãÔºåÁÇ∫Êú™‰æÜÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÈñãÁôº AI È©ÖÂãïÂ∑•ÂÖ∑Êèê‰æõË¶ãËß£„ÄÇ

##### **SepsisCalc: Integrating Clinical Calculators into Early Sepsis Prediction via Dynamic Temporal Graph Construction**
2501.00190v2 by Changchang Yin, Shihan Fu, Bingsheng Yao, Thai-Hoang Pham, Weidan Cao, Dakuo Wang, Jeffrey Caterino, Ping Zhang

Sepsis is an organ dysfunction caused by a deregulated immune response to an
infection. Early sepsis prediction and identification allow for timely
intervention, leading to improved clinical outcomes. Clinical calculators
(e.g., the six-organ dysfunction assessment of SOFA) play a vital role in
sepsis identification within clinicians' workflow, providing evidence-based
risk assessments essential for sepsis diagnosis. However, artificial
intelligence (AI) sepsis prediction models typically generate a single sepsis
risk score without incorporating clinical calculators for assessing organ
dysfunctions, making the models less convincing and transparent to clinicians.
To bridge the gap, we propose to mimic clinicians' workflow with a novel
framework SepsisCalc to integrate clinical calculators into the predictive
model, yielding a clinically transparent and precise model for utilization in
clinical settings. Practically, clinical calculators usually combine
information from multiple component variables in Electronic Health Records
(EHR), and might not be applicable when the variables are (partially) missing.
We mitigate this issue by representing EHRs as temporal graphs and integrating
a learning module to dynamically add the accurately estimated calculator to the
graphs. Experimental results on real-world datasets show that the proposed
model outperforms state-of-the-art methods on sepsis prediction tasks.
Moreover, we developed a system to identify organ dysfunctions and potential
sepsis risks, providing a human-AI interaction tool for deployment, which can
help clinicians understand the prediction outputs and prepare timely
interventions for the corresponding dysfunctions, paving the way for actionable
clinical decision-making support for early intervention.

ÊëòË¶ÅÔºöÊïóË°ÄÁóáÊòØÁî±Â∞çÊÑüÊüìÁöÑÂ§±Ë™øÂÖçÁñ´ÂèçÊáâÊâÄÈÄ†ÊàêÁöÑÂô®ÂÆòÂäüËÉΩÈöúÁ§ô„ÄÇÊó©ÊúüÊïóË°ÄÁóáÈ†êÊ∏¨ÂíåË≠òÂà•ÊúâÂä©ÊñºÂèäÊôÇ‰ªãÂÖ•ÔºåÈÄ≤ËÄåÊîπÂñÑËá®Â∫äÁµêÊûú„ÄÇËá®Â∫äË®àÁÆóÂô®Ôºà‰æãÂ¶ÇÔºåSOFA ÁöÑÂÖ≠Âô®ÂÆòÂäüËÉΩÈöúÁ§ôË©ï‰º∞ÔºâÂú®Ëá®Â∫äÈÜ´Â∏´ÁöÑÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÊâÆÊºîËëóÊïóË°ÄÁóáË≠òÂà•ÁöÑÈáçË¶ÅËßíËâ≤ÔºåÊèê‰æõÊïóË°ÄÁóáË®∫Êñ∑ÂøÖË¶ÅÁöÑË≠âÊìöÁÇ∫Âü∫Á§éÁöÑÈ¢®Èö™Ë©ï‰º∞„ÄÇÁÑ∂ËÄåÔºå‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÊïóË°ÄÁóáÈ†êÊ∏¨Ê®°ÂûãÈÄöÂ∏∏ÊúÉÁî¢ÁîüÂñÆ‰∏ÄÁöÑÊïóË°ÄÁóáÈ¢®Èö™Ë©ïÂàÜÔºåËÄåÊú™Á¥çÂÖ•Áî®ÊñºË©ï‰º∞Âô®ÂÆòÂäüËÉΩÈöúÁ§ôÁöÑËá®Â∫äË®àÁÆóÂô®Ôºå‰ΩøÂæóÊ®°ÂûãÂ∞çËá®Â∫äÈÜ´Â∏´‰æÜË™™È°ØÂæóËºÉ‰∏çÂÖ∑Ë™™ÊúçÂäõ‰∏îÈÄèÊòé„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫Ê®°Êì¨Ëá®Â∫äÈÜ´Â∏´ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºå‰ΩøÁî®ÂâµÊñ∞ÁöÑ SepsisCalc Êû∂ÊßãÂ∞áËá®Â∫äË®àÁÆóÂô®Êï¥ÂêàÂà∞È†êÊ∏¨Ê®°Âûã‰∏≠ÔºåÁî¢Áîü‰∏ÄÂÄãÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠‰ΩøÁî®ÊôÇÂÖ∑ÊúâËá®Â∫äÈÄèÊòéÂ∫¶‰∏îÁ≤æÁ¢∫ÁöÑÊ®°Âûã„ÄÇÂØ¶Èöõ‰∏äÔºåËá®Â∫äË®àÁÆóÂô®ÈÄöÂ∏∏ÊúÉÁµêÂêàÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑÔºàEHRÔºâ‰∏≠Â§öÂÄãÁµÑÊàêËÆäÊï∏ÁöÑË≥áË®äÔºå‰∏îÁï∂ËÆäÊï∏ÔºàÈÉ®ÂàÜÔºâÈÅ∫Â§±ÊôÇÂèØËÉΩ‰∏çÈÅ©Áî®„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞á EHR Ë°®Á§∫ÁÇ∫ÊôÇÈñìÂúñÂΩ¢‰∏¶Êï¥Âêà‰∏ÄÂÄãÂ≠∏ÁøíÊ®°ÁµÑÔºåÂãïÊÖãÂ∞áÊ∫ñÁ¢∫‰º∞Ë®àÁöÑË®àÁÆóÂô®Êñ∞Â¢ûÂà∞ÂúñÂΩ¢‰∏≠Ôºå‰æÜÊ∏õËºïÈÄôÂÄãÂïèÈ°å„ÄÇÂú®ÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂú®ÊïóË°ÄÁóáÈ†êÊ∏¨‰ªªÂãô‰∏äÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÁ≥ªÁµ±‰æÜË≠òÂà•Âô®ÂÆòÂäüËÉΩÈöúÁ§ôÂíåÊΩõÂú®ÁöÑÊïóË°ÄÁóáÈ¢®Èö™ÔºåÊèê‰æõ‰∏ÄÂÄãÂèØÁî®ÊñºÈÉ®ÁΩ≤ÁöÑ‰∫∫Â∑•Êô∫ÊÖß‰∫íÂãïÂ∑•ÂÖ∑ÔºåÈÄôÊúâÂä©ÊñºËá®Â∫äÈÜ´Â∏´‰∫ÜËß£È†êÊ∏¨Ëº∏Âá∫Ôºå‰∏¶ÈáùÂ∞çÁõ∏ÊáâÁöÑÂäüËÉΩÈöúÁ§ôÊ∫ñÂÇôÂèäÊôÇÁöÑ‰ªãÂÖ•Êé™ÊñΩÔºåÁÇ∫Êó©Êúü‰ªãÂÖ•ÁöÑË°åÂãïËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Èã™Ë∑Ø„ÄÇ

##### **DeepLL: Considering Linear Logic for the Analysis of Deep Learning Experiments**
2501.00169v1 by Nick Papoulias

Deep Learning experiments have critical requirements regarding the careful
handling of their datasets as well as the efficient and correct usage of APIs
that interact with hardware accelerators. On the one hand, software mistakes
during data handling can contaminate experiments and lead to incorrect results.
On the other hand, poorly coded APIs that interact with the hardware can lead
to sub-optimal usage and untrustworthy conclusions. In this work we investigate
the use of Linear Logic for the analysis of Deep Learning experiments. We show
that primitives and operators of Linear Logic can be used to express: (i) an
abstract representation of the control flow of an experiment, (ii) a set of
available experimental resources, such as API calls to the underlying
data-structures and hardware as well as (iii) reasoning rules about the correct
consumption of resources during experiments. Our proposed model is not only
lightweight but also easy to comprehend having both a symbolic and a visual
component. Finally, its artifacts are themselves proofs in Linear Logic that
can be readily verified by off-the-shelf reasoners.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÂØ¶È©óÂ∞çÊñºË≥áÊñôÈõÜÁöÑ‰ªîÁ¥∞ËôïÁêÜ‰ª•ÂèäËàáÁ°¨È´îÂä†ÈÄüÂô®‰∫íÂãïÁöÑ API ÁöÑÊúâÊïà‰∏îÊ≠£Á¢∫‰ΩøÁî®ÂÖ∑ÊúâÈáçË¶ÅÁöÑË¶ÅÊ±Ç„ÄÇ‰∏ÄÊñπÈù¢ÔºåË≥áÊñôËôïÁêÜÈÅéÁ®ã‰∏≠ÁöÑËªüÈ´îÈåØË™§ÂèØËÉΩÊúÉÊ±°ÊüìÂØ¶È©ó‰∏¶Â∞éËá¥‰∏çÊ≠£Á¢∫ÁöÑÁµêÊûú„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåËàáÁ°¨È´î‰∫íÂãïÁöÑÁ∑®Á¢º‰∏çËâØÁöÑ API ÂèØËÉΩÂ∞éËá¥Ê¨°‰Ω≥‰ΩøÁî®Âíå‰∏çÂèØÈù†ÁöÑÁµêË´ñ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü‰ΩøÁî®Á∑öÊÄßÈÇèËºØ‰æÜÂàÜÊûêÊ∑±Â∫¶Â≠∏ÁøíÂØ¶È©ó„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÁ∑öÊÄßÈÇèËºØÁöÑÂü∫Êú¨ÂéüÁêÜÂíåÈÅãÁÆóÁ¨¶ÂèØÁî®ÊñºË°®ÈÅîÔºö(i) ÂØ¶È©óÊéßÂà∂ÊµÅÁ®ãÁöÑÊäΩË±°Ë°®Á§∫Ôºå(ii) ‰∏ÄÁµÑÂèØÁî®ÁöÑÂØ¶È©óË≥áÊ∫êÔºå‰æãÂ¶ÇÂ∞çÂ∫ïÂ±§Ë≥áÊñôÁµêÊßãÂíåÁ°¨È´îÁöÑ API ÂëºÂè´‰ª•Âèä (iii) ÈóúÊñºÂú®ÂØ¶È©óÊúüÈñìÊ≠£Á¢∫Ê∂àËÄóË≥áÊ∫êÁöÑÊé®ÁêÜË¶èÂâá„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊ®°Âûã‰∏çÂÉÖËºïÈáèÁ¥öÔºåËÄå‰∏îÊòìÊñºÁêÜËß£ÔºåÊó¢ÊúâÁ¨¶ËôüÁµÑÊàêÔºå‰πüÊúâË¶ñË¶∫ÁµÑÊàê„ÄÇÊúÄÂæåÔºåÂÖ∂Â∑•‰ª∂Êú¨Ë∫´Â∞±ÊòØÁ∑öÊÄßÈÇèËºØ‰∏≠ÁöÑË≠âÊòéÔºåÂèØ‰ª•‰ΩøÁî®ÁèæÊàêÁöÑÊé®ÁêÜÂô®ËºïÈ¨ÜÈ©óË≠â„ÄÇ

##### **Temporal reasoning for timeline summarisation in social media**
2501.00152v1 by Jiayu Song, Mahmud Akhter, Dana Atzil Slonim, Maria Liakata

This paper explores whether enhancing temporal reasoning capabilities in
Large Language Models (LLMs) can improve the quality of timeline summarization,
the task of summarising long texts containing sequences of events, particularly
social media threads . We introduce \textit{NarrativeReason}, a novel dataset
focused on temporal relationships among sequential events within narratives,
distinguishing it from existing temporal reasoning datasets that primarily
address pair-wise event relationships. Our approach then combines temporal
reasoning with timeline summarization through a knowledge distillation
framework, where we first fine-tune a teacher model on temporal reasoning tasks
and then distill this knowledge into a student model while simultaneously
training it for the task of timeline summarization. Experimental results
demonstrate that our model achieves superior performance on mental
health-related timeline summarization tasks, which involve long social media
threads with repetitions of events and a mix of emotions, highlighting the
importance of leveraging temporal reasoning to improve timeline summarisation.

ÊëòË¶ÅÔºöÈÄôÁØáË´ñÊñáÊé¢Ë®éÂ¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑÊôÇÈñìÊé®ÁêÜËÉΩÂäõÊòØÂê¶ËÉΩÊèêÂçáÊôÇÈñìËª∏ÊëòË¶ÅÁöÑÂìÅË≥™ÔºåÊôÇÈñìËª∏ÊëòË¶ÅÊòØÈáùÂ∞çÂåÖÂê´‰∫ã‰ª∂È†ÜÂ∫èÁöÑÈï∑ÁØáÊñáÂ≠óÈÄ≤Ë°åÊëòË¶ÅÁöÑ‰ªªÂãôÔºåÂ∞§ÂÖ∂ÊòØÁ§æÁæ§Â™íÈ´î‰∏≤„ÄÇÊàëÂÄëÂºïÈÄ≤‰∫Ü\textit{NarrativeReason}ÔºåÈÄôÂÄãÊñ∞Á©éÁöÑË≥áÊñôÈõÜÂ∞àÊ≥®ÊñºÊïòËø∞‰∏≠È†ÜÂ∫è‰∫ã‰ª∂‰πãÈñìÁöÑÊôÇÈñìÈóú‰øÇÔºå‰∏¶Â∞áÂÖ∂ËàáÁèæÊúâÁöÑÊôÇÈñìÊé®ÁêÜË≥áÊñôÈõÜÂçÄÂàÜÈñã‰æÜÔºåÂæåËÄÖ‰∏ªË¶ÅËôïÁêÜÊàêÂ∞çÁöÑ‰∫ã‰ª∂Èóú‰øÇ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊé•ËëóÈÄèÈÅéÁü•Ë≠òËêÉÂèñÊû∂ÊßãÂ∞áÊôÇÈñìÊé®ÁêÜËàáÊôÇÈñìËª∏ÊëòË¶ÅÁµêÂêàÔºåÊàëÂÄëÈ¶ñÂÖàÈáùÂ∞çÊôÇÈñìÊé®ÁêÜ‰ªªÂãôÂæÆË™ø‰∏ÄÂÄãÊïôÂ∏´Ê®°ÂûãÔºåÁÑ∂ÂæåÂ∞áÊ≠§Áü•Ë≠òËêÉÂèñÂà∞‰∏ÄÂÄãÂ≠∏ÁîüÊ®°Âûã‰∏≠ÔºåÂêåÊôÇË®ìÁ∑¥ÂÆÉÈÄ≤Ë°åÊôÇÈñìËª∏ÊëòË¶ÅÁöÑ‰ªªÂãô„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÊàëÂÄëÁöÑÊ®°ÂûãÂú®ËàáÂøÉÁêÜÂÅ•Â∫∑Áõ∏ÈóúÁöÑÊôÇÈñìËª∏ÊëòË¶Å‰ªªÂãô‰∏≠Áç≤Âæó‰∫ÜÂçìË∂äÁöÑË°®ÁèæÔºåÈÄôÊ∂âÂèäÂà∞ÂåÖÂê´ÈáçË§á‰∫ã‰ª∂ÂíåÂêÑÁ®ÆÊÉÖÁ∑íÁöÑÈï∑ÁØáÁ§æÁæ§Â™íÈ´î‰∏≤ÔºåÂº∑Ë™ø‰∫ÜÂà©Áî®ÊôÇÈñìÊé®ÁêÜ‰æÜÊèêÂçáÊôÇÈñìËª∏ÊëòË¶ÅÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection**
2501.00129v1 by Julia Ive, Paulina Bondaronek, Vishal Yadav, Daniel Santel, Tracy Glauser, Tina Cheng, Jeffrey R. Strawn, Greeshma Agasthya, Jordan Tschida, Sanghyun Choo, Mayanka Chandrashekar, Anuj J. Kapadia, John Pestian

Introduction: Healthcare AI models often inherit biases from their training
data. While efforts have primarily targeted bias in structured data, mental
health heavily depends on unstructured data. This study aims to detect and
mitigate linguistic differences related to non-biological differences in the
training data of AI models designed to assist in pediatric mental health
screening. Our objectives are: (1) to assess the presence of bias by evaluating
outcome parity across sex subgroups, (2) to identify bias sources through
textual distribution analysis, and (3) to develop a de-biasing method for
mental health text data. Methods: We examined classification parity across
demographic groups and assessed how gendered language influences model
predictions. A data-centric de-biasing method was applied, focusing on
neutralizing biased terms while retaining salient clinical information. This
methodology was tested on a model for automatic anxiety detection in pediatric
patients. Results: Our findings revealed a systematic under-diagnosis of female
adolescent patients, with a 4% lower accuracy and a 9% higher False Negative
Rate (FNR) compared to male patients, likely due to disparities in information
density and linguistic differences in patient notes. Notes for male patients
were on average 500 words longer, and linguistic similarity metrics indicated
distinct word distributions between genders. Implementing our de-biasing
approach reduced diagnostic bias by up to 27%, demonstrating its effectiveness
in enhancing equity across demographic groups. Discussion: We developed a
data-centric de-biasing framework to address gender-based content disparities
within clinical text. By neutralizing biased language and enhancing focus on
clinically essential information, our approach demonstrates an effective
strategy for mitigating bias in AI healthcare models trained on text.

ÊëòË¶ÅÔºö<paragraph>ÂºïË®ÄÔºöÈÜ´ÁôÇ‰øùÂÅ• AI Ê®°ÂûãÈÄöÂ∏∏ÊúÉÂæûÂÖ∂Ë®ìÁ∑¥Ë≥áÊñô‰∏≠ÁπºÊâøÂÅèË¶ã„ÄÇÈõñÁÑ∂Âä™Âäõ‰∏ªË¶ÅÈáùÂ∞çÁµêÊßãÂåñË≥áÊñô‰∏≠ÁöÑÂÅèË¶ãÔºå‰ΩÜÂøÉÁêÜÂÅ•Â∫∑Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÈùûÁµêÊßãÂåñË≥áÊñô„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Ê™¢Ê∏¨‰∏¶Ê∏õËºïËàáË®≠Ë®àÁî®ÊñºÂçîÂä©ÂÖíÁ´•ÂøÉÁêÜÂÅ•Â∫∑ÁØ©Ê™¢ÁöÑ AI Ê®°ÂûãË®ìÁ∑¥Ë≥áÊñô‰∏≠ÁöÑÈùûÁîüÁâ©Â∑ÆÁï∞Áõ∏ÈóúÁöÑË™ûË®ÄÂ∑ÆÁï∞„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÔºö(1) ÈÄèÈÅéË©ï‰º∞‰∏çÂêåÊÄßÂà•Â≠êÁæ§È´îÁöÑÁµêÊûúÂπ≥ÂÉπ‰æÜË©ï‰º∞ÂÅèË¶ãÁöÑÂ≠òÂú®Ôºå(2) ÈÄèÈÅéÊñáÊú¨ÂàÜ‰ΩàÂàÜÊûê‰æÜÊâæÂá∫ÂÅèË¶ã‰æÜÊ∫êÔºå‰ª•Âèä (3) ÈñãÁôº‰∏ÄÁ®ÆÂøÉÁêÜÂÅ•Â∫∑ÊñáÊú¨Ë≥áÊñôÁöÑÂéªÂÅèË¶ãÊñπÊ≥ï„ÄÇÊñπÊ≥ïÔºöÊàëÂÄëÊ™¢Êü•‰∫Ü‰∏çÂêå‰∫∫Âè£Áæ§È´îÁöÑÂàÜÈ°ûÂπ≥ÂÉπÔºå‰∏¶Ë©ï‰º∞‰∫ÜÊÄßÂà•Ë™ûË®ÄÂ¶Ç‰ΩïÂΩ±ÈüøÊ®°ÂûãÈ†êÊ∏¨„ÄÇÊáâÁî®‰∫Ü‰∏ÄÁ®Æ‰ª•Ë≥áÊñôÁÇ∫‰∏≠ÂøÉÁöÑÂéªÂÅèË¶ãÊñπÊ≥ïÔºåÂ∞àÊ≥®ÊñºÂú®‰øùÁïôÈ°ØËëóËá®Â∫äË≥áË®äÁöÑÂêåÊôÇ‰∏≠ÂíåÊúâÂÅèË¶ãÁöÑË°ìË™û„ÄÇÊ≠§ÊñπÊ≥ïÂú®‰∏ÄÂÄãÁî®ÊñºÂÖíÁ´•ÊÇ£ËÄÖËá™ÂãïÁÑ¶ÊÖÆÊ™¢Ê∏¨ÁöÑÊ®°Âûã‰∏äÈÄ≤Ë°å‰∫ÜÊ∏¨Ë©¶„ÄÇÁµêÊûúÔºöÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÂ∞çÂ•≥ÊÄßÈùíÂ∞ëÂπ¥ÊÇ£ËÄÖÁöÑÁ≥ªÁµ±ÊÄßË®∫Êñ∑‰∏çË∂≥ÔºåËàáÁî∑ÊÄßÊÇ£ËÄÖÁõ∏ÊØîÔºåÊ∫ñÁ¢∫Áéá‰Ωé‰∫Ü 4%ÔºåÂÅáÈô∞ÊÄßÁéá (FNR) È´ò‰∫Ü 9%ÔºåÈÄôÂèØËÉΩÊòØÁî±ÊñºÊÇ£ËÄÖÂÇôË®ª‰∏≠Ë≥áË®äÂØÜÂ∫¶ÂíåË™ûË®ÄÂ∑ÆÁï∞ÁöÑÂ∑ÆÁï∞„ÄÇÁî∑ÊÄßÊÇ£ËÄÖÁöÑÂÇôË®ªÂπ≥ÂùáÈï∑ 500 ÂÄãÂ≠óÔºåË™ûË®ÄÁõ∏‰ººÊÄßÊåáÊ®ôÈ°ØÁ§∫‰∏çÂêåÊÄßÂà•‰πãÈñìÁöÑÂ≠óË©ûÂàÜ‰ΩàÊà™ÁÑ∂‰∏çÂêå„ÄÇÂØ¶ÊñΩÊàëÂÄëÁöÑÂéªÂÅèË¶ãÊñπÊ≥ïÂ∞áË®∫Êñ∑ÂÅèË¶ãÈôç‰Ωé‰∫Ü 27%ÔºåË≠âÊòé‰∫ÜÂÖ∂Âú®ÊèêÂçá‰∏çÂêå‰∫∫Âè£Áæ§È´î‰πãÈñìÂÖ¨Âπ≥ÊÄßÁöÑÊúâÊïàÊÄß„ÄÇË®éË´ñÔºöÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ª•Ë≥áÊñôÁÇ∫‰∏≠ÂøÉÁöÑÂéªÂÅèË¶ãÊû∂ÊßãÔºåÁî®ÊñºËß£Ê±∫Ëá®Â∫äÊñáÊú¨‰∏≠ÁöÑÂü∫ÊñºÊÄßÂà•ÁöÑÂÖßÂÆπÂ∑ÆÁï∞„ÄÇÈÄèÈÅé‰∏≠ÂíåÊúâÂÅèË¶ãÁöÑË™ûË®ÄÂíåÂä†Âº∑Â∞çËá®Â∫äÂøÖË¶ÅË≥áË®äÁöÑÈóúÊ≥®ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ±ïÁ§∫‰∫Ü‰∏ÄÁ®ÆÊúâÊïàÁ≠ñÁï•ÔºåÁî®ÊñºÊ∏õËºïÂú®ÊñáÊú¨‰∏äË®ìÁ∑¥ÁöÑ AI ÈÜ´ÁôÇ‰øùÂÅ•Ê®°Âûã‰∏≠ÁöÑÂÅèË¶ã„ÄÇ</paragraph>

##### **Leveraging AI for Automatic Classification of PCOS Using Ultrasound Imaging**
2501.01984v1 by Atharva Divekar, Atharva Sonawane

The AUTO-PCOS Classification Challenge seeks to advance the diagnostic
capabilities of artificial intelligence (AI) in identifying Polycystic Ovary
Syndrome (PCOS) through automated classification of healthy and unhealthy
ultrasound frames. This report outlines our methodology for building a robust
AI pipeline utilizing transfer learning with the InceptionV3 architecture to
achieve high accuracy in binary classification. Preprocessing steps ensured the
dataset was optimized for training, validation, and testing, while
interpretability methods like LIME and saliency maps provided valuable insights
into the model's decision-making. Our approach achieved an accuracy of 90.52%,
with precision, recall, and F1-score metrics exceeding 90% on validation data,
demonstrating its efficacy. The project underscores the transformative
potential of AI in healthcare, particularly in addressing diagnostic challenges
like PCOS. Key findings, challenges, and recommendations for future
enhancements are discussed, highlighting the pathway for creating reliable,
interpretable, and scalable AI-driven medical diagnostic tools.

ÊëòË¶ÅÔºöAUTO-PCOS ÂàÜÈ°ûÊåëÊà∞Êó®Âú®ÈÄèÈÅéËá™ÂãïÂàÜÈ°ûÂÅ•Â∫∑Âíå‰∏çÂÅ•Â∫∑ÁöÑË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÔºåÊèêÂçá‰∫∫Â∑•Êô∫ÊÖß (AI) Âú®Ëæ®Ë≠òÂ§öÂõäÊÄßÂçµÂ∑¢ÁóáÂÄôÁæ§ (PCOS) ÁöÑË®∫Êñ∑ËÉΩÂäõ„ÄÇÈÄô‰ªΩÂ†±ÂëäÊ¶ÇËø∞‰∫ÜÊàëÂÄëÂª∫ÊßãÂº∑ÂÅ• AI ÁÆ°Á∑öÁöÑÊñπÊ≥ïÔºåÂà©Áî® InceptionV3 Êû∂ÊßãÈÄ≤Ë°åÈÅ∑ÁßªÂ≠∏ÁøíÔºå‰ª•Âú®‰∫åÂÖÉÂàÜÈ°û‰∏≠ÈÅîÊàêÈ´òÊ∫ñÁ¢∫Â∫¶„ÄÇÈ†êËôïÁêÜÊ≠•È©üÁ¢∫‰øùË≥áÊñôÈõÜÂ∑≤ÈáùÂ∞çË®ìÁ∑¥„ÄÅÈ©óË≠âÂíåÊ∏¨Ë©¶ÈÄ≤Ë°åÊúÄ‰Ω≥ÂåñÔºåËÄå LIME ÂíåÈ°ØËëóÊÄßÂúñÁ≠âÂèØËß£ÈáãÊÄßÊñπÊ≥ïÂâáÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£ÔºåË™™ÊòéÊ®°ÂûãÁöÑÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®È©óË≠âË≥áÊñô‰∏äÈÅîÂà∞‰∫Ü 90.52% ÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÁ≤æÁ¢∫Â∫¶„ÄÅÂè¨ÂõûÁéáÂíå F1 ÂàÜÊï∏ÈÉΩË∂ÖÈÅé 90%ÔºåË≠âÊòé‰∫ÜÂÆÉÁöÑÊïàÂäõ„ÄÇÈÄôÂÄãÂ∞àÊ°àÂº∑Ë™ø‰∫Ü AI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑËΩâÂûãÊΩõÂäõÔºåÁâπÂà•ÊòØÂú®Ëß£Ê±∫ PCOS Á≠âË®∫Êñ∑ÊåëÊà∞ÊñπÈù¢„ÄÇË®éË´ñ‰∫ÜÈóúÈçµÁôºÁèæ„ÄÅÊåëÊà∞ÂíåÊú™‰æÜÂ¢ûÂº∑Âª∫Ë≠∞ÔºåÁ™ÅÈ°Ø‰∫ÜÂª∫Á´ãÂèØÈù†„ÄÅÂèØËß£Èáã‰∏îÂèØÊì¥ÂÖÖÁöÑ AI È©ÖÂãïÈÜ´ÁôÇË®∫Êñ∑Â∑•ÂÖ∑ÁöÑÈÄîÂæë„ÄÇ

##### **Advancing Parkinson's Disease Progression Prediction: Comparing Long Short-Term Memory Networks and Kolmogorov-Arnold Networks**
2412.20744v1 by Abhinav Roy, Bhavesh Gyanchandani, Aditya Oza, Abhishek Sharma

Parkinson's Disease (PD) is a degenerative neurological disorder that impairs
motor and non-motor functions, significantly reducing quality of life and
increasing mortality risk. Early and accurate detection of PD progression is
vital for effective management and improved patient outcomes. Current
diagnostic methods, however, are often costly, time-consuming, and require
specialized equipment and expertise. This work proposes an innovative approach
to predicting PD progression using regression methods, Long Short-Term Memory
(LSTM) networks, and Kolmogorov Arnold Networks (KAN). KAN, utilizing
spline-parametrized univariate functions, allows for dynamic learning of
activation patterns, unlike traditional linear models.
  The Movement Disorder Society-Sponsored Revision of the Unified Parkinson's
Disease Rating Scale (MDS-UPDRS) is a comprehensive tool for evaluating PD
symptoms and is commonly used to measure disease progression. Additionally,
protein or peptide abnormalities are linked to PD onset and progression.
Identifying these associations can aid in predicting disease progression and
understanding molecular changes.
  Comparing multiple models, including LSTM and KAN, this study aims to
identify the method that delivers the highest metrics. The analysis reveals
that KAN, with its dynamic learning capabilities, outperforms other approaches
in predicting PD progression. This research highlights the potential of AI and
machine learning in healthcare, paving the way for advanced computational
models to enhance clinical predictions and improve patient care and treatment
strategies in PD management.

ÊëòË¶ÅÔºöÂ∏ïÈáëÊ£ÆÊ∞èÁóá (PD) ÊòØ‰∏ÄÁ®ÆÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖÔºåÊúÉÊêçÂÆ≥ÈÅãÂãïÂíåÈùûÈÅãÂãïÂäüËÉΩÔºåÂö¥ÈáçÈôç‰ΩéÁîüÊ¥ªÂìÅË≥™‰∏¶Â¢ûÂä†Ê≠ª‰∫°È¢®Èö™„ÄÇÊó©Êúü‰∏îÊ∫ñÁ¢∫Ê™¢Ê∏¨ PD ÈÄ≤Á®ãÂ∞çÊñºÊúâÊïàÁÆ°ÁêÜÂíåÊîπÂñÑÊÇ£ËÄÖÈ†êÂæåËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑË®∫Êñ∑ÊñπÊ≥ïÈÄöÂ∏∏ÊàêÊú¨È´òÊòÇ„ÄÅËÄóÊôÇ‰∏îÈúÄË¶ÅÂ∞àÊ•≠Ë®≠ÂÇôÂíåÂ∞àÊ•≠Áü•Ë≠ò„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºå‰ΩøÁî®Ëø¥Ê≠∏ÊñπÊ≥ï„ÄÅÈï∑Áü≠ÊúüË®òÊÜ∂ (LSTM) Á∂≤Ë∑ØÂíå Kolmogorov Arnold Á∂≤Ë∑Ø (KAN) ‰æÜÈ†êÊ∏¨ PD ÈÄ≤Á®ã„ÄÇKAN Âà©Áî®Ê®£Ê¢ùÂèÉÊï∏ÂåñÁöÑÂñÆËÆäÈáèÂáΩÊï∏ÔºåÂèØ‰ª•ÂãïÊÖãÂ≠∏ÁøíÊøÄÊ¥ªÊ®°ÂºèÔºåÈÄôËàáÂÇ≥Áµ±Á∑öÊÄßÊ®°Âûã‰∏çÂêå„ÄÇÈÅãÂãïÈöúÁ§ôÂçîÊúÉË¥äÂä©ÁöÑÁµ±‰∏ÄÂ∏ïÈáëÊ£ÆÊ∞èÁóáË©ïÂàÜÈáèË°® (MDS-UPDRS) ÊòØË©ï‰º∞ PD ÁóáÁãÄÁöÑÁ∂úÂêàÂ∑•ÂÖ∑ÔºåÈÄöÂ∏∏Áî®ÊñºÊ∏¨ÈáèÁñæÁóÖÈÄ≤Á®ã„ÄÇÊ≠§Â§ñÔºåËõãÁôΩË≥™ÊàñËÉúËÇΩÁï∞Â∏∏Ëàá PD Áôº‰ΩúÂíåÈÄ≤Á®ãÊúâÈóú„ÄÇÊâæÂá∫ÈÄô‰∫õÈóúËÅØÂèØ‰ª•Âπ´Âä©È†êÊ∏¨ÁñæÁóÖÈÄ≤Á®ã‰∏¶‰∫ÜËß£ÂàÜÂ≠êËÆäÂåñ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÊØîËºÉ‰∫ÜÂåÖÊã¨ LSTM Âíå KAN Âú®ÂÖßÁöÑÂ§öÁ®ÆÊ®°ÂûãÔºåÊó®Âú®ÊâæÂá∫Êèê‰æõÊúÄÈ´òÊåáÊ®ôÁöÑÊñπÊ≥ï„ÄÇÂàÜÊûêÈ°ØÁ§∫ÔºåÂÖ∑ÊúâÂãïÊÖãÂ≠∏ÁøíËÉΩÂäõÁöÑ KAN Âú®È†êÊ∏¨ PD ÈÄ≤Á®ãÊñπÈù¢ÂÑ™ÊñºÂÖ∂‰ªñÊñπÊ≥ï„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Á™ÅÈ°Ø‰∫Ü AI ÂíåÊ©üÂô®Â≠∏ÁøíÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊΩõÂäõÔºåÁÇ∫ÂÖàÈÄ≤ÁöÑË®àÁÆóÊ®°ÂûãÈã™Ë∑ØÔºå‰ª•Â¢ûÂº∑Ëá®Â∫äÈ†êÊ∏¨‰∏¶ÊîπÂñÑ PD ÁÆ°ÁêÜ‰∏≠ÁöÑÊÇ£ËÄÖÁÖßË≠∑ÂíåÊ≤ªÁôÇÁ≠ñÁï•„ÄÇ

##### **Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis**
2412.20651v1 by Yousef Yeganeh, Ioannis Charisiadis, Marta Hasny, Martin Hartenberger, Bj√∂rn Ommer, Nassir Navab, Azade Farshad, Ehsan Adeli

Scaling by training on large datasets has been shown to enhance the quality
and fidelity of image generation and manipulation with diffusion models;
however, such large datasets are not always accessible in medical imaging due
to cost and privacy issues, which contradicts one of the main applications of
such models to produce synthetic samples where real data is scarce. Also,
finetuning on pre-trained general models has been a challenge due to the
distribution shift between the medical domain and the pre-trained models. Here,
we propose Latent Drift (LD) for diffusion models that can be adopted for any
fine-tuning method to mitigate the issues faced by the distribution shift or
employed in inference time as a condition. Latent Drifting enables diffusion
models to be conditioned for medical images fitted for the complex task of
counterfactual image generation, which is crucial to investigate how parameters
such as gender, age, and adding or removing diseases in a patient would alter
the medical images. We evaluate our method on three public longitudinal
benchmark datasets of brain MRI and chest X-rays for counterfactual image
generation. Our results demonstrate significant performance gains in various
scenarios when combined with different fine-tuning schemes. The source code of
this work will be publicly released upon its acceptance.

ÊëòË¶ÅÔºö<paragraph>ÈÄèÈÅéË®ìÁ∑¥Â§ßÂûãË≥áÊñôÈõÜ‰æÜË™øÊï¥ÊØî‰æãÔºåÂ∑≤Ë¢´Ë≠âÊòéÂèØ‰ª•ÊèêÂçáÊì¥Êï£Ê®°ÂûãÂΩ±ÂÉèÁî¢ÁîüËàáÊìç‰ΩúÁöÑÂìÅË≥™Âíå‰øùÁúüÂ∫¶ÔºõÁÑ∂ËÄåÔºåÁî±ÊñºÊàêÊú¨ÂíåÈö±ÁßÅÂïèÈ°åÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠‰∏¶‰∏çÁ∏ΩÊòØËÉΩÂèñÂæóÈÄôÈ∫ºÂ§ßÂûãÁöÑË≥áÊñôÈõÜÔºåÈÄôËàáÈÄô‰∫õÊ®°ÂûãÁöÑ‰∏ªË¶ÅÊáâÁî®‰πã‰∏ÄÁõ∏ÁüõÁõæÔºå‰πüÂ∞±ÊòØÂú®ÁúüÂØ¶Ë≥áÊñôÁ®ÄÂ∞ëÁöÑÊÉÖÊ≥Å‰∏ãÁî¢ÁîüÂêàÊàêÊ®£Êú¨„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºÈÜ´Â≠∏È†òÂüüËàáÈ†êË®ìÁ∑¥Ê®°Âûã‰πãÈñìÁöÑÂàÜÂ∏ÉËΩâÁßªÔºåÂ∞çÈ†êË®ìÁ∑¥ÁöÑÈÄöÁî®Ê®°ÂûãÈÄ≤Ë°åÂæÆË™ø‰∏ÄÁõ¥ÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫Êì¥Êï£Ê®°ÂûãÁöÑÊΩõÂú®ÊºÇÁßª (LD)ÔºåÂèØ‰ª•Êé°Áî®‰ªª‰ΩïÂæÆË™øÊñπÊ≥ï‰æÜÊ∏õËºïÂàÜÂ∏ÉËΩâÁßªÊâÄÈù¢Ëá®ÁöÑÂïèÈ°åÔºåÊàñÂú®Êé®ÁêÜÊôÇÈñì‰ΩúÁÇ∫Ê¢ù‰ª∂‰ΩøÁî®„ÄÇÊΩõÂú®ÊºÇÁßª‰ΩøÊì¥Êï£Ê®°ÂûãËÉΩÂ§†ÈáùÂ∞çÈÅ©ÂêàÊñºÂèç‰∫ãÂØ¶ÂΩ±ÂÉèÁî¢ÁîüË§áÈõú‰ªªÂãôÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÈÄ≤Ë°åË™øÊï¥ÔºåÈÄôÂ∞çÊñºÊé¢Ë®éË´∏Â¶ÇÊÄßÂà•„ÄÅÂπ¥ÈΩ°‰ª•ÂèäÂú®ÊÇ£ËÄÖ‰∏≠Â¢ûÂä†ÊàñÁßªÈô§ÁñæÁóÖÁ≠âÂèÉÊï∏Â∞áÂ¶Ç‰ΩïÊîπËÆäÈÜ´Â≠∏ÂΩ±ÂÉèËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÂú®‰∏âÂÄãÂ§ßËÖ¶ MRI ÂíåËÉ∏ÈÉ® X ÂÖâÁöÑÂÖ¨ÈñãÁ∏±ÂêëÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äË©ï‰º∞‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÔºå‰ª•ÈÄ≤Ë°åÂèç‰∫ãÂØ¶ÂΩ±ÂÉèÁî¢Áîü„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåËàá‰∏çÂêåÁöÑÂæÆË™øÊñπÊ°àÁµêÂêà‰ΩøÁî®ÊôÇÔºåÂú®ÂêÑÁ®ÆÊÉÖÊ≥Å‰∏ãÈÉΩËÉΩÈ°ØËëóÊèêÂçáÊïàËÉΩ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁöÑÂéüÂßãÁ¢ºÂ∞áÂú®Áç≤ÂæóÊé•ÂèóÂæåÂÖ¨ÈñãÁôºÂ∏É„ÄÇ</paragraph>

##### **HALLUCINOGEN: A Benchmark for Evaluating Object Hallucination in Large Visual-Language Models**
2412.20622v1 by Ashish Seth, Dinesh Manocha, Chirag Agarwal

Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
in performing complex multimodal tasks. However, they are still plagued by
object hallucination: the misidentification or misclassification of objects
present in images. To this end, we propose HALLUCINOGEN, a novel visual
question answering (VQA) object hallucination attack benchmark that utilizes
diverse contextual reasoning prompts to evaluate object hallucination in
state-of-the-art LVLMs. We design a series of contextual reasoning
hallucination prompts to evaluate LVLMs' ability to accurately identify objects
in a target image while asking them to perform diverse visual-language tasks
such as identifying, locating or performing visual reasoning around specific
objects. Further, we extend our benchmark to high-stakes medical applications
and introduce MED-HALLUCINOGEN, hallucination attacks tailored to the
biomedical domain, and evaluate the hallucination performance of LVLMs on
medical images, a critical area where precision is crucial. Finally, we conduct
extensive evaluations of eight LVLMs and two hallucination mitigation
strategies across multiple datasets to show that current generic and medical
LVLMs remain susceptible to hallucination attacks.

ÊëòË¶ÅÔºöÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (LVLMs) Âú®Âü∑Ë°åË§áÈõúÁöÑÂ§öÊ®°ÊÖã‰ªªÂãôÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰ªçÁÑ∂ÂèóÂà∞Áâ©È´îÂπªË¶∫ÁöÑÂõ∞ÊìæÔºöÈåØË™§Ë≠òÂà•ÊàñÈåØË™§ÂàÜÈ°ûÂúñÂÉè‰∏≠Â≠òÂú®ÁöÑÁâ©È´î„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü HALLUCINOGENÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑË¶ñË¶∫ÂïèÁ≠î (VQA) Áâ©È´îÂπªË¶∫ÊîªÊìäÂü∫Ê∫ñÔºåÂÆÉÂà©Áî®Â§öÊ®£ÂåñÁöÑ‰∏ä‰∏ãÊñáÊé®ÁêÜÊèêÁ§∫‰æÜË©ï‰º∞ÊúÄÂÖàÈÄ≤ÁöÑ LVLMs ‰∏≠ÁöÑÁâ©È´îÂπªË¶∫„ÄÇÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÁ≥ªÂàó‰∏ä‰∏ãÊñáÊé®ÁêÜÂπªË¶∫ÊèêÁ§∫Ôºå‰ª•Ë©ï‰º∞ LVLMs Âú®Ë¶ÅÊ±ÇÂÆÉÂÄëÂü∑Ë°åÂ§öÊ®£ÂåñÁöÑË¶ñË¶∫Ë™ûË®Ä‰ªªÂãôÔºà‰æãÂ¶ÇË≠òÂà•„ÄÅÂÆö‰ΩçÊàñÂ∞çÁâπÂÆöÁâ©È´îÈÄ≤Ë°åË¶ñË¶∫Êé®ÁêÜÔºâÁöÑÂêåÊôÇÊ∫ñÁ¢∫Ë≠òÂà•ÁõÆÊ®ôÂúñÂÉè‰∏≠Áâ©È´îÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞áÂü∫Ê∫ñÊì¥Â±ïÂà∞È´òÈ¢®Èö™ÁöÑÈÜ´Â≠∏ÊáâÁî®Ôºå‰∏¶ÂºïÂÖ•‰∫ÜÂ∞àÈñÄÈáùÂ∞çÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÁöÑÂπªË¶∫ÊîªÊìä MED-HALLUCINOGENÔºå‰∏¶Ë©ï‰º∞‰∫Ü LVLMs Âú®ÈÜ´Â≠∏ÂúñÂÉèÔºà‰∏ÄÂÄãÁ≤æÁ¢∫Ëá≥ÈóúÈáçË¶ÅÁöÑÈóúÈçµÈ†òÂüüÔºâ‰∏äÁöÑÂπªË¶∫Ë°®Áèæ„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ∞çÂÖ´ÂÄã LVLMs ÂíåÂÖ©ÂÄãÂπªË¶∫Á∑©Ëß£Á≠ñÁï•ÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑË©ï‰º∞ÔºåË∑®Â§öÂÄãÊï∏ÊìöÈõÜÔºå‰ª•Ë°®ÊòéÁï∂ÂâçÁöÑÈÄöÁî®ÂíåÈÜ´Â≠∏ LVLMs ‰ªçÁÑ∂ÂÆπÊòìÂèóÂà∞ÂπªË¶∫ÊîªÊìä„ÄÇ

##### **Dive into Time-Series Anomaly Detection: A Decade Review**
2412.20512v1 by Paul Boniol, Qinghua Liu, Mingyi Huang, Themis Palpanas, John Paparrizos

Recent advances in data collection technology, accompanied by the ever-rising
volume and velocity of streaming data, underscore the vital need for time
series analytics. In this regard, time-series anomaly detection has been an
important activity, entailing various applications in fields such as cyber
security, financial markets, law enforcement, and health care. While
traditional literature on anomaly detection is centered on statistical
measures, the increasing number of machine learning algorithms in recent years
call for a structured, general characterization of the research methods for
time-series anomaly detection. This survey groups and summarizes anomaly
detection existing solutions under a process-centric taxonomy in the time
series context. In addition to giving an original categorization of anomaly
detection methods, we also perform a meta-analysis of the literature and
outline general trends in time-series anomaly detection research.

ÊëòË¶ÅÔºöÈö®ËëóË≥áÊñôÊî∂ÈõÜÊäÄË°ìÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºå‰ª•Âèä‰∏≤ÊµÅË≥áÊñôÁöÑÊï∏ÈáèÂíåÈÄüÂ∫¶ÊåÅÁ∫å‰∏äÂçáÔºåÂº∑Ë™ø‰∫ÜÊôÇÈñìÂ∫èÂàóÂàÜÊûêÁöÑËø´ÂàáÈúÄÊ±Ç„ÄÇÂú®ÈÄôÊñπÈù¢ÔºåÊôÇÈñìÂ∫èÂàóÁï∞Â∏∏ÂÅµÊ∏¨‰∏ÄÁõ¥ÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÁöÑÊ¥ªÂãïÔºåÂåÖÂê´Á∂≤Ë∑ØÂÆâÂÖ®„ÄÅÈáëËûçÂ∏ÇÂ†¥„ÄÅÂü∑Ê≥ïÂíåÈÜ´ÁôÇ‰øùÂÅ•Á≠âÈ†òÂüü‰∏≠ÁöÑÂêÑÁ®ÆÊáâÁî®„ÄÇÈõñÁÑ∂Áï∞Â∏∏ÂÅµÊ∏¨ÁöÑÂÇ≥Áµ±ÊñáÁçªÈõÜ‰∏≠ÊñºÁµ±Ë®àÊ∏¨ÈáèÔºå‰ΩÜËøëÂπ¥‰æÜÊ©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ïÁöÑÊï∏Èáè‰∏çÊñ∑Â¢ûÂä†ÔºåÂõ†Ê≠§ÈúÄË¶ÅÂ∞çÊôÇÈñìÂ∫èÂàóÁï∞Â∏∏ÂÅµÊ∏¨ÁöÑÁ†îÁ©∂ÊñπÊ≥ïÈÄ≤Ë°åÁµêÊßãÂåñ„ÄÅÈÄöÁî®ÁöÑÊèèËø∞„ÄÇÈÄôÈ†ÖË™øÊü•Âú®ÊôÇÈñìÂ∫èÂàóËÑàÁµ°‰∏≠Ôºå‰æùÊìö‰ª•ÊµÅÁ®ãÁÇ∫‰∏≠ÂøÉÁöÑÂàÜÈ°ûÊ≥ïÔºåÂ∞çÁï∞Â∏∏ÂÅµÊ∏¨ÁèæÊúâËß£Ê±∫ÊñπÊ°àÈÄ≤Ë°åÂàÜÁµÑÂíåÊëòË¶Å„ÄÇÈô§‰∫ÜÂ∞çÁï∞Â∏∏ÂÅµÊ∏¨ÊñπÊ≥ïÈÄ≤Ë°åÂéüÂßãÂàÜÈ°ûÂ§ñÔºåÊàëÂÄë‰πüÂ∞çÊñáÁçªÈÄ≤Ë°åÂÖÉÂàÜÊûêÔºå‰∏¶Ê¶ÇËø∞ÊôÇÈñìÂ∫èÂàóÁï∞Â∏∏ÂÅµÊ∏¨Á†îÁ©∂ÁöÑ‰∏ÄËà¨Ë∂®Âã¢„ÄÇ

##### **A Deep Subgrouping Framework for Precision Drug Repurposing via Emulating Clinical Trials on Real-world Patient Data**
2412.20373v1 by Seungyeon Lee, Ruoqi Liu, Feixiong Cheng, Ping Zhang

Drug repurposing identifies new therapeutic uses for existing drugs, reducing
the time and costs compared to traditional de novo drug discovery. Most
existing drug repurposing studies using real-world patient data often treat the
entire population as homogeneous, ignoring the heterogeneity of treatment
responses across patient subgroups. This approach may overlook promising drugs
that benefit specific subgroups but lack notable treatment effects across the
entire population, potentially limiting the number of repurposable candidates
identified. To address this, we introduce STEDR, a novel drug repurposing
framework that integrates subgroup analysis with treatment effect estimation.
Our approach first identifies repurposing candidates by emulating multiple
clinical trials on real-world patient data and then characterizes patient
subgroups by learning subgroup-specific treatment effects. We deploy \model to
Alzheimer's Disease (AD), a condition with few approved drugs and known
heterogeneity in treatment responses. We emulate trials for over one thousand
medications on a large-scale real-world database covering over 8 million
patients, identifying 14 drug candidates with beneficial effects to AD in
characterized subgroups. Experiments demonstrate STEDR's superior capability in
identifying repurposing candidates compared to existing approaches.
Additionally, our method can characterize clinically relevant patient subgroups
associated with important AD-related risk factors, paving the way for precision
drug repurposing.

ÊëòË¶ÅÔºöËçØÁâ©ÂÜçÂà©Áî®‰∏∫Áé∞ÊúâËçØÁâ©ÊâæÂá∫Êñ∞ÁöÑÊ≤ªÁñóÁî®ÈÄîÔºå‰∏é‰º†ÁªüÁöÑ‰ªéÂ§¥ËçØÁâ©ÂèëÁé∞Áõ∏ÊØîÔºåÂáèÂ∞ë‰∫ÜÊó∂Èó¥ÂíåÊàêÊú¨„ÄÇÂ§ßÂ§öÊï∞‰ΩøÁî®ÁúüÂÆû‰∏ñÁïåÊÇ£ËÄÖÊï∞ÊçÆÁöÑÁé∞ÊúâËçØÁâ©ÂÜçÂà©Áî®Á†îÁ©∂ÈÄöÂ∏∏Â∞ÜÊï¥‰∏™‰∫∫Áæ§ËßÜ‰∏∫ÂêåË¥®ÁöÑÔºåËÄåÂøΩÁï•‰∫Ü‰∏çÂêåÊÇ£ËÄÖ‰∫öÁªÑÊ≤ªÁñóÂèçÂ∫îÁöÑÂºÇË¥®ÊÄß„ÄÇËøôÁßçÊñπÊ≥ïÂèØËÉΩ‰ºöÂøΩËßÜÂØπÁâπÂÆö‰∫öÁªÑÊúâÁõä‰ΩÜÊï¥‰∏™Áæ§‰ΩìÁº∫‰πèÊòæÁùÄÊ≤ªÁñóÊïàÊûúÁöÑÊúâÂ∏åÊúõÁöÑËçØÁâ©Ôºå‰ªéËÄåÂèØËÉΩÈôêÂà∂Â∑≤ËØÜÂà´ÁöÑÂèØÂÜçÂà©Áî®ÂÄôÈÄâËçØÁâ©ÁöÑÊï∞Èáè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü STEDRÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑËçØÁâ©ÂÜçÂà©Áî®Ê°ÜÊû∂ÔºåÂÆÉÂ∞Ü‰∫öÁªÑÂàÜÊûê‰∏éÊ≤ªÁñóÊïàÊûú‰º∞ËÆ°Áõ∏ÁªìÂêà„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈ¶ñÂÖàÈÄöËøáÊ®°ÊãüÁúüÂÆû‰∏ñÁïåÊÇ£ËÄÖÊï∞ÊçÆÁöÑÂ§ö‰∏™‰∏¥Â∫äËØïÈ™åÊù•ËØÜÂà´ÂÜçÂà©Áî®ÂÄôÈÄâËçØÁâ©ÔºåÁÑ∂ÂêéÈÄöËøáÂ≠¶‰π†‰∫öÁªÑÁâπÂºÇÊÄßÊ≤ªÁñóÊïàÊûúÊù•Ë°®ÂæÅÊÇ£ËÄÖ‰∫öÁªÑ„ÄÇÊàë‰ª¨ÈÉ®ÁΩ≤\modelÂà∞ÈòøÂ∞îËå®Êµ∑ÈªòÁóÖ (AD)ÔºåËøôÊòØ‰∏ÄÁßçÂ∑≤Ëé∑ÊâπËçØÁâ©ËæÉÂ∞ë‰∏îÊ≤ªÁñóÂèçÂ∫îÂ∑≤Áü•ÂºÇË¥®ÊÄßÁöÑÁñæÁóÖ„ÄÇÊàë‰ª¨Âú®‰∏Ä‰∏™Ë¶ÜÁõñË∂ÖËøá 800 ‰∏áÊÇ£ËÄÖÁöÑÂ§ßËßÑÊ®°ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÂ∫ì‰∏äÊ®°Êãü‰∫ÜË∂ÖËøá‰∏ÄÂçÉÁßçËçØÁâ©ÁöÑËØïÈ™åÔºåÁ°ÆÂÆö‰∫Ü 14 ÁßçÂú®Ë°®ÂæÅÁöÑ‰∫öÁªÑ‰∏≠ÂØπ AD ÊúâÁõäÁöÑÂÄôÈÄâËçØÁâ©„ÄÇÂÆûÈ™åË°®ÊòéÔºå‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåSTEDR Âú®ËØÜÂà´ÂÜçÂà©Áî®ÂÄôÈÄâËçØÁâ©ÊñπÈù¢ÂÖ∑ÊúâÊõ¥Âº∫ÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂèØ‰ª•Ë°®ÂæÅ‰∏éÈáçË¶ÅÁöÑ AD Áõ∏ÂÖ≥Âç±Èô©Âõ†Á¥†Áõ∏ÂÖ≥ÁöÑ‰∏¥Â∫äÁõ∏ÂÖ≥ÊÇ£ËÄÖ‰∫öÁªÑÔºå‰∏∫Á≤æÂáÜËçØÁâ©ÂÜçÂà©Áî®Èì∫Âπ≥ÈÅìË∑Ø„ÄÇ

##### **Transforming CCTV cameras into NO$_2$ sensors at city scale for adaptive policymaking**
2501.00056v1 by Mohamed R. Ibrahim, Terry Lyons

Air pollution in cities, especially NO\textsubscript{2}, is linked to
numerous health problems, ranging from mortality to mental health challenges
and attention deficits in children. While cities globally have initiated
policies to curtail emissions, real-time monitoring remains challenging due to
limited environmental sensors and their inconsistent distribution. This gap
hinders the creation of adaptive urban policies that respond to the sequence of
events and daily activities affecting pollution in cities. Here, we demonstrate
how city CCTV cameras can act as a pseudo-NO\textsubscript{2} sensors. Using a
predictive graph deep model, we utilised traffic flow from London's cameras in
addition to environmental and spatial factors, generating NO\textsubscript{2}
predictions from over 133 million frames. Our analysis of London's mobility
patterns unveiled critical spatiotemporal connections, showing how specific
traffic patterns affect NO\textsubscript{2} levels, sometimes with temporal
lags of up to 6 hours. For instance, if trucks only drive at night, their
effects on NO\textsubscript{2} levels are most likely to be seen in the morning
when people commute. These findings cast doubt on the efficacy of some of the
urban policies currently being implemented to reduce pollution. By leveraging
existing camera infrastructure and our introduced methods, city planners and
policymakers could cost-effectively monitor and mitigate the impact of
NO\textsubscript{2} and other pollutants.

ÊëòË¶ÅÔºöÂüéÂ∏Ç‰∏≠ÁöÑÁ©∫Ê∞£Ê±°ÊüìÔºåÁâπÂà•ÊòØ‰∫åÊ∞ßÂåñÊ∞ÆÔºåËàáË®±Â§öÂÅ•Â∫∑ÂïèÈ°åÊúâÈóúÔºåÂæûÊ≠ª‰∫°ÁéáÂà∞ÂÖíÁ´•ÁöÑÁ≤æÁ•ûÂÅ•Â∫∑ÊåëÊà∞ÂíåÊ≥®ÊÑèÂäõÁº∫Èô∑„ÄÇÂÑòÁÆ°ÂÖ®ÁêÉÂüéÂ∏ÇÂ∑≤ÂïüÂãïÊîøÁ≠ñ‰æÜÊ∏õÂ∞ëÊéíÊîæÔºå‰ΩÜÁî±ÊñºÁí∞Â¢ÉÊÑüÊ∏¨Âô®ÊúâÈôê‰∏îÂàÜ‰Ωà‰∏çÂùáÔºåÂØ¶ÊôÇÁõ£Ê∏¨‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÈÄôÂÄãÂ∑ÆË∑ùÈòªÁ§ô‰∫ÜÈÅ©ÊáâÊÄßÂüéÂ∏ÇÊîøÁ≠ñÁöÑÂà∂ÂÆöÔºåÈÄô‰∫õÊîøÁ≠ñÂ∞çÂΩ±ÈüøÂüéÂ∏ÇÊ±°ÊüìÁöÑ‰∫ã‰ª∂È†ÜÂ∫èÂíåÊó•Â∏∏Ê¥ªÂãïÂÅöÂá∫ÂõûÊáâ„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂüéÂ∏ÇÈñâË∑ØÈõªË¶ñÊîùÂΩ±Ê©üÂ¶Ç‰ΩïÂÖÖÁï∂ÂÅΩ‰∫åÊ∞ßÂåñÊ∞ÆÊÑüÊ∏¨Âô®„ÄÇ‰ΩøÁî®È†êÊ∏¨ÂúñÊ∑±Â∫¶Ê®°ÂûãÔºåÊàëÂÄëÂà©Áî®‰∫ÜÂÄ´Êï¶ÊîùÂΩ±Ê©üÁöÑ‰∫§ÈÄöÊµÅÈáè‰ª•ÂèäÁí∞Â¢ÉÂíåÁ©∫ÈñìÂõ†Á¥†ÔºåÂæûË∂ÖÈÅé 1.33 ÂÑÑÂÄãÁï´Èù¢‰∏≠ÁîüÊàê‰∫Ü‰∫åÊ∞ßÂåñÊ∞ÆÈ†êÊ∏¨„ÄÇÊàëÂÄëÂ∞çÂÄ´Êï¶ÊµÅÂãïÊ®°ÂºèÁöÑÂàÜÊûêÊè≠Á§∫‰∫ÜÈóúÈçµÁöÑÊôÇÁ©∫ÈÄ£Êé•ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∑È´îÁöÑ‰∫§ÈÄöÊ®°ÂºèÂ¶Ç‰ΩïÂΩ±Èüø‰∫åÊ∞ßÂåñÊ∞ÆÊ∞¥Âπ≥ÔºåÊúâÊôÇÊôÇÈñìÊªØÂæåÈï∑ÈÅî 6 Â∞èÊôÇ„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÂç°ËªäÂè™Âú®Êôö‰∏äË°åÈßõÔºåÂÆÉÂÄëÂ∞ç‰∫åÊ∞ßÂåñÊ∞ÆÊ∞¥Âπ≥ÁöÑÂΩ±ÈüøÊúÄÊúâÂèØËÉΩÂú®‰∫∫ÂÄëÈÄöÂã§ÁöÑÊó©‰∏äÈ°ØÁèæ„ÄÇÈÄô‰∫õÁôºÁèæÂ∞çÁõÆÂâçÂØ¶ÊñΩÁöÑ‰∏Ä‰∫õÊó®Âú®Ê∏õÂ∞ëÊ±°ÊüìÁöÑÂüéÂ∏ÇÊîøÁ≠ñÁöÑÊúâÊïàÊÄßÊèêÂá∫‰∫ÜË≥™Áñë„ÄÇÈÄöÈÅéÂà©Áî®ÁèæÊúâÁöÑÊîùÂΩ±Ê©üÂü∫Á§éË®≠ÊñΩÂíåÊàëÂÄëÂºïÂÖ•ÁöÑÊñπÊ≥ïÔºåÂüéÂ∏ÇË¶èÂäÉËÄÖÂíåÊîøÁ≠ñÂà∂ÂÆöËÄÖÂèØ‰ª•Á∂ìÊøüÊúâÊïàÂú∞Áõ£ÊéßÂíåÊ∏õËºï‰∫åÊ∞ßÂåñÊ∞ÆÂíåÂÖ∂‰ªñÊ±°ÊüìÁâ©ÁöÑÂΩ±Èüø„ÄÇ

##### **On the Compositional Generalization of Multimodal LLMs for Medical Imaging**
2412.20070v1 by Zhenyang Cai, Junying Chen, Rongsheng Wang, Weihong Wang, Yonglin Deng, Dingjie Song, Yize Chen, Zixu Zhang, Benyou Wang

Multimodal large language models (MLLMs) hold significant potential in the
medical field, but their capabilities are often limited by insufficient data in
certain medical domains, highlighting the need for understanding what kinds of
images can be used by MLLMs for generalization. Current research suggests that
multi-task training outperforms single-task as different tasks can benefit each
other, but they often overlook the internal relationships within these tasks,
providing limited guidance on selecting datasets to enhance specific tasks. To
analyze this phenomenon, we attempted to employ compositional generalization
(CG)-the ability of models to understand novel combinations by recombining
learned elements-as a guiding framework. Since medical images can be precisely
defined by Modality, Anatomical area, and Task, naturally providing an
environment for exploring CG. Therefore, we assembled 106 medical datasets to
create Med-MAT for comprehensive experiments. The experiments confirmed that
MLLMs can use CG to understand unseen medical images and identified CG as one
of the main drivers of the generalization observed in multi-task training.
Additionally, further studies demonstrated that CG effectively supports
datasets with limited data and delivers consistent performance across different
backbones, highlighting its versatility and broad applicability. Med-MAT is
publicly available at https://github.com/FreedomIntelligence/Med-MAT.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (MLLM) Âú®ÂåªÁñóÈ¢ÜÂüüÊã•ÊúâÂ∑®Â§ßÊΩúÂäõÔºå‰ΩÜÂÖ∂ËÉΩÂäõÂæÄÂæÄÂèóÂà∞ÁâπÂÆöÂåªÁñóÈ¢ÜÂüüÊï∞ÊçÆ‰∏çË∂≥ÁöÑÈôêÂà∂ÔºåËøôÁ™ÅÂá∫‰∫ÜÁêÜËß£ MLLM ÂèØÁî®‰∫éÊ≥õÂåñÁöÑÂõæÂÉèÁ±ªÂûãÁöÑÂøÖË¶ÅÊÄß„ÄÇÂΩìÂâçÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ö‰ªªÂä°ËÆ≠ÁªÉ‰ºò‰∫éÂçï‰ªªÂä°ËÆ≠ÁªÉÔºåÂõ†‰∏∫‰∏çÂêåÁöÑ‰ªªÂä°ÂèØ‰ª•Áõ∏‰∫íÂèóÁõäÔºå‰ΩÜÂÆÉ‰ª¨Â∏∏Â∏∏ÂøΩÁï•Ëøô‰∫õ‰ªªÂä°‰∏≠ÁöÑÂÜÖÈÉ®ÂÖ≥Á≥ªÔºåÂú®ÈÄâÊã©Êï∞ÊçÆÈõÜ‰ª•Â¢ûÂº∫ÁâπÂÆö‰ªªÂä°ÊñπÈù¢Êèê‰æõÁöÑÊåáÂØºÊúâÈôê„ÄÇ‰∏∫‰∫ÜÂàÜÊûêËøôÁßçÁé∞Ë±°ÔºåÊàë‰ª¨Â∞ùËØïÈááÁî®ÁªÑÂêàÊ≥õÂåñ (CG)‚Äî‚ÄîÊ®°ÂûãÈÄöËøáÈáçÊñ∞ÁªÑÂêàÂ≠¶‰π†ÁöÑÂÖÉÁ¥†Êù•ÁêÜËß£Êñ∞ÁªÑÂêàÁöÑËÉΩÂäõ‚Äî‚Äî‰Ωú‰∏∫ÊåáÂØºÊ°ÜÊû∂„ÄÇÁî±‰∫éÂåªÂ≠¶ÂõæÂÉèÂèØ‰ª•ÈÄöËøáÊñπÂºè„ÄÅËß£ÂâñÂå∫ÂüüÂíå‰ªªÂä°Êù•Á≤æÁ°ÆÂÆö‰πâÔºåÂõ†Ê≠§Ëá™ÁÑ∂Âú∞‰∏∫Êé¢Á¥¢ CG Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÁéØÂ¢É„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÁªÑË£Ö‰∫Ü 106 ‰∏™ÂåªÂ≠¶Êï∞ÊçÆÈõÜÊù•ÂàõÂª∫ Med-MAT ‰ª•ËøõË°åÁªºÂêàÂÆûÈ™å„ÄÇÂÆûÈ™åËØÅÂÆûÔºåMLLM ÂèØ‰ª•‰ΩøÁî® CG Êù•ÁêÜËß£Áúã‰∏çËßÅÁöÑÂåªÂ≠¶ÂõæÂÉèÔºåÂπ∂Â∞Ü CG Á°ÆÂÆö‰∏∫Â§ö‰ªªÂä°ËÆ≠ÁªÉ‰∏≠ËßÇÂØüÂà∞ÁöÑÊ≥õÂåñÁöÑ‰∏ªË¶ÅÈ©±Âä®Âõ†Á¥†‰πã‰∏Ä„ÄÇÊ≠§Â§ñÔºåËøõ‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåCG ÊúâÊïàÂú∞ÊîØÊåÅ‰∫ÜÊï∞ÊçÆÊúâÈôêÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂Âú®‰∏çÂêåÁöÑ‰∏ªÂπ≤‰∏≠Êèê‰æõ‰∫ÜÊåÅÁª≠ÁöÑÊÄßËÉΩÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂Â§öÂäüËÉΩÊÄßÂíåÂπøÊ≥õÁöÑÈÄÇÁî®ÊÄß„ÄÇMed-MAT Âú® https://github.com/FreedomIntelligence/Med-MAT ÂÖ¨ÂºÄÂèØÁî®„ÄÇ

##### **The Emotional Spectrum of LLMs: Leveraging Empathy and Emotion-Based Markers for Mental Health Support**
2412.20068v1 by Alessandro De Grandi, Federico Ravenda, Andrea Raballo, Fabio Crestani

The increasing demand for mental health services has highlighted the need for
innovative solutions, particularly in the realm of psychological conversational
AI, where the availability of sensitive data is scarce. In this work, we
explored the development of a system tailored for mental health support with a
novel approach to psychological assessment based on explainable emotional
profiles in combination with empathetic conversational models, offering a
promising tool for augmenting traditional care, particularly where immediate
expertise is unavailable. Our work can be divided into two main parts,
intrinsecaly connected to each other. First, we present RACLETTE, a
conversational system that demonstrates superior emotional accuracy compared to
state-of-the-art benchmarks in both understanding users' emotional states and
generating empathetic responses during conversations, while progressively
building an emotional profile of the user through their interactions. Second,
we show how the emotional profiles of a user can be used as interpretable
markers for mental health assessment. These profiles can be compared with
characteristic emotional patterns associated with different mental disorders,
providing a novel approach to preliminary screening and support.

ÊëòË¶ÅÔºöÈö®ËëóÂ∞çÂøÉÁêÜÂÅ•Â∫∑ÊúçÂãôÈúÄÊ±ÇÁöÑÂ¢ûÂä†ÔºåÂá∏È°Ø‰∫ÜÂâµÊñ∞Ëß£Ê±∫ÊñπÊ°àÁöÑÈúÄÊ±ÇÔºåÁâπÂà•ÊòØÂú®ÂøÉÁêÜÂ∞çË©±Âºè‰∫∫Â∑•Êô∫ÊÖßÈ†òÂüüÔºåÈÇ£Ë£°Áº∫‰πèÊïèÊÑüË≥áÊñô„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Á¥¢‰∫ÜÈñãÁôº‰∏ÄÂÄãÈáùÂ∞çÂøÉÁêÜÂÅ•Â∫∑ÊîØÊåÅÁöÑÁ≥ªÁµ±ÔºåÊé°Áî®‰∏ÄÁ®ÆÂü∫ÊñºÂèØËß£ÈáãÁöÑÊÉÖÁ∑íÁâπÂæµÁöÑÊñ∞ÊñπÊ≥ïÈÄ≤Ë°åÂøÉÁêÜË©ï‰º∞ÔºåÁµêÂêàÂêåÁêÜÂøÉÂ∞çË©±Ê®°ÂºèÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÂ∑•ÂÖ∑ÔºåÁî®ÊñºÊì¥ÂÖÖÂÇ≥Áµ±ÁÖßË≠∑ÔºåÁâπÂà•ÊòØÂú®ÁÑ°Ê≥ïÁ´ãÂç≥Áç≤ÂæóÂ∞àÊ•≠Áü•Ë≠òÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÊàëÂÄëÁöÑÂ∑•‰ΩúÂèØ‰ª•ÂàÜÁÇ∫ÂÖ©ÂÄã‰∏ªË¶ÅÈÉ®ÂàÜÔºåÂΩºÊ≠§ÂÖßÂú®Áõ∏Èóú„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü RACLETTEÔºå‰∏ÄÂÄãÂ∞çË©±Á≥ªÁµ±ÔºåËàáÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÁõ∏ÊØîÔºåÂú®ÁêÜËß£‰ΩøÁî®ËÄÖÊÉÖÁ∑íÁãÄÊÖãÂíåÂú®Â∞çË©±‰∏≠Áî¢ÁîüÂêåÁêÜÂøÉÂõûÊáâÊñπÈù¢Ë°®ÁèæÂá∫ÂÑ™Ë∂äÁöÑÊÉÖÁ∑íÊ∫ñÁ¢∫ÊÄßÔºåÂêåÊôÇÈÄèÈÅé‰ªñÂÄëÁöÑ‰∫íÂãïÈÄêÊº∏Âª∫Á´ã‰ΩøÁî®ËÄÖÁöÑÊÉÖÁ∑íÁâπÂæµ„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰ΩøÁî®ËÄÖÁöÑÊÉÖÁ∑íÁâπÂæµÂ¶Ç‰ΩïÂèØÁî®‰ΩúÂøÉÁêÜÂÅ•Â∫∑Ë©ï‰º∞ÁöÑÂèØËß£ÈáãÊ®ôË®ò„ÄÇÈÄô‰∫õÁâπÂæµÂèØ‰ª•ËàáËàá‰∏çÂêåÂøÉÁêÜÁñæÁóÖÁõ∏ÈóúÁöÑÂÖ∏ÂûãÊÉÖÁ∑íÊ®°ÂºèÈÄ≤Ë°åÊØîËºÉÔºåÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂàùÊ≠•ÁØ©ÈÅ∏ÂíåÊîØÊåÅÁöÑÊñ∞ÊñπÊ≥ï„ÄÇ

##### **MobileNetV2: A lightweight classification model for home-based sleep apnea screening**
2412.19967v2 by Hui Pan, Yanxuan Yu, Jilun Ye, Xu Zhang

This study proposes a novel lightweight neural network model leveraging
features extracted from electrocardiogram (ECG) and respiratory signals for
early OSA screening. ECG signals are used to generate feature spectrograms to
predict sleep stages, while respiratory signals are employed to detect
sleep-related breathing abnormalities. By integrating these predictions, the
method calculates the apnea-hypopnea index (AHI) with enhanced accuracy,
facilitating precise OSA diagnosis.
  The method was validated on three publicly available sleep apnea databases:
the Apnea-ECG database, the UCDDB dataset, and the MIT-BIH Polysomnographic
database. Results showed an overall OSA detection accuracy of 0.978,
highlighting the model's robustness. Respiratory event classification achieved
an accuracy of 0.969 and an area under the receiver operating characteristic
curve (ROC-AUC) of 0.98. For sleep stage classification, in UCDDB dataset, the
ROC-AUC exceeded 0.85 across all stages, with recall for Sleep reaching 0.906
and specificity for REM and Wake states at 0.956 and 0.937, respectively.
  This study underscores the potential of integrating lightweight neural
networks with multi-signal analysis for accurate, portable, and cost-effective
OSA screening, paving the way for broader adoption in home-based and wearable
health monitoring systems.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑËºïÈáèÁ¥öÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÔºåÂà©Áî®ÂæûÂøÉÈõªÂúñ (ECG) ÂíåÂëºÂê∏‰ø°Ëôü‰∏≠ÊèêÂèñÁöÑÁâπÂæµÔºåÈÄ≤Ë°åÊó©Êúü OSA ÁØ©Ê™¢„ÄÇECG ‰ø°ËôüÁî®ÊñºÁî¢ÁîüÁâπÂæµÈ†ªË≠úÂúñÔºå‰ª•È†êÊ∏¨Áù°Áú†ÈöéÊÆµÔºåËÄåÂëºÂê∏‰ø°ËôüÂâáÁî®ÊñºÂÅµÊ∏¨ËàáÁù°Áú†Áõ∏ÈóúÁöÑÂëºÂê∏Áï∞Â∏∏„ÄÇÈÄèÈÅéÊï¥ÂêàÈÄô‰∫õÈ†êÊ∏¨ÔºåÊ≠§ÊñπÊ≥ïË®àÁÆóÂá∫ÂÖ∑ÊúâÊõ¥È´òÁ≤æÁ¢∫Â∫¶ÁöÑÂëºÂê∏‰∏≠Ê≠¢‰ΩéÈÄöÊ∞£ÊåáÊï∏ (AHI)Ôºå‰øÉÈÄ≤Á≤æÁ¢∫ÁöÑ OSA Ë®∫Êñ∑„ÄÇ
Ë©≤ÊñπÊ≥ïÂ∑≤Âú®‰∏âÂÄãÂÖ¨ÈñãÁöÑÁù°Áú†ÂëºÂê∏‰∏≠Ê≠¢ÁóáË≥áÊñôÂ∫´‰∏≠È©óË≠âÔºöÂëºÂê∏‰∏≠Ê≠¢Áóá-ECG Ë≥áÊñôÂ∫´„ÄÅUCDDB Ë≥áÊñôÈõÜÂíå MIT-BIH Â§öÈáçÁù°Áú†ÁîüÁêÜÊ™¢Êü•Ë≥áÊñôÂ∫´„ÄÇÁµêÊûúÈ°ØÁ§∫Êï¥È´î OSA Ê™¢Ê∏¨Ê∫ñÁ¢∫Â∫¶ÁÇ∫ 0.978ÔºåÁ™ÅÈ°Ø‰∫ÜË©≤Ê®°ÂûãÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÂëºÂê∏‰∫ã‰ª∂ÂàÜÈ°ûÁöÑÊ∫ñÁ¢∫Â∫¶ÈÅîÂà∞ 0.969Ôºå‰∏îÂú®ÂèóË©¶ËÄÖÊìç‰ΩúÁâπÂæµÊõ≤Á∑ö (ROC-AUC) ‰∏ãÊñπÁöÑÈù¢Á©çÁÇ∫ 0.98„ÄÇÂ∞çÊñºÁù°Áú†ÈöéÊÆµÂàÜÈ°ûÔºåÂú® UCDDB Ë≥áÊñôÈõÜ‰∏≠ÔºåÊâÄÊúâÈöéÊÆµÁöÑ ROC-AUC ÂùáË∂ÖÈÅé 0.85ÔºåÁù°Áú†ÁöÑÂè¨ÂõûÁéáÈÅîÂà∞ 0.906ÔºåËÄå REM ÂíåÊ∏ÖÈÜíÁãÄÊÖãÁöÑÁâπÁï∞ÊÄßÂàÜÂà•ÁÇ∫ 0.956 Âíå 0.937„ÄÇ
Êú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÂ∞áËºïÈáèÁ¥öÁ•ûÁ∂ìÁ∂≤Ë∑ØËàáÂ§ö‰ø°ËôüÂàÜÊûêÊï¥ÂêàÁöÑÊΩõÂäõÔºå‰ª•ÈÄ≤Ë°åÊ∫ñÁ¢∫„ÄÅÂèØÊîúÂºè‰∏îÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑ OSA ÁØ©Ê™¢ÔºåÁÇ∫Âú®Â±ÖÂÆ∂ÂíåÁ©øÊà¥ÂºèÂÅ•Â∫∑Áõ£Ê∏¨Á≥ªÁµ±‰∏≠Êõ¥Âª£Ê≥õÊé°Áî®Èã™Ë∑Ø„ÄÇ

##### **ErgoChat: a Visual Query System for the Ergonomic Risk Assessment of Construction Workers**
2412.19954v1 by Chao Fan, Qipei Mei, Xiaonan Wang, Xinming Li

In the construction sector, workers often endure prolonged periods of
high-intensity physical work and prolonged use of tools, resulting in injuries
and illnesses primarily linked to postural ergonomic risks, a longstanding
predominant health concern. To mitigate these risks, researchers have applied
various technological methods to identify the ergonomic risks that construction
workers face. However, traditional ergonomic risk assessment (ERA) techniques
do not offer interactive feedback. The rapidly developing vision-language
models (VLMs), capable of generating textual descriptions or answering
questions about ergonomic risks based on image inputs, have not yet received
widespread attention. This research introduces an interactive visual query
system tailored to assess the postural ergonomic risks of construction workers.
The system's capabilities include visual question answering (VQA), which
responds to visual queries regarding workers' exposure to postural ergonomic
risks, and image captioning (IC), which generates textual descriptions of these
risks from images. Additionally, this study proposes a dataset designed for
training and testing such methodologies. Systematic testing indicates that the
VQA functionality delivers an accuracy of 96.5%. Moreover, evaluations using
nine metrics for IC and assessments from human experts indicate that the
proposed approach surpasses the performance of a method using the same
architecture trained solely on generic datasets. This study sets a new
direction for future developments in interactive ERA using generative
artificial intelligence (AI) technologies.

ÊëòË¶ÅÔºöÂú®Âª∫ÁØâÊ•≠‰∏≠ÔºåÂ∑•‰∫∫Á∂ìÂ∏∏ÂøçÂèóÈï∑ÊôÇÈñìÈ´òÂº∑Â∫¶È´îÂäõÂãûÂãïÂíåÈï∑ÊôÇÈñì‰ΩøÁî®Â∑•ÂÖ∑ÔºåÂ∞éËá¥ÂèóÂÇ∑ÂíåÁñæÁóÖÔºåÈÄô‰∫õÂïèÈ°å‰∏ªË¶ÅËàáÂßøÂã¢‰∫∫È´îÂ∑•Â≠∏È¢®Èö™ÊúâÈóúÔºåÈÄôÊòØ‰∏ÄÂÄãÈï∑ÊúüÁöÑ‰∏ªË¶ÅÂÅ•Â∫∑ÂïèÈ°å„ÄÇÁÇ∫‰∫ÜÊ∏õËºïÈÄô‰∫õÈ¢®Èö™ÔºåÁ†îÁ©∂‰∫∫Âì°ÊáâÁî®ÂêÑÁ®ÆÊäÄË°ìÊñπÊ≥ï‰æÜË≠òÂà•Âª∫ÁØâÂ∑•‰∫∫Èù¢Ëá®ÁöÑ‰∫∫È´îÂ∑•Â≠∏È¢®Èö™„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑ‰∫∫È´îÂ∑•Â≠∏È¢®Èö™Ë©ï‰º∞ (ERA) ÊäÄË°ì‰∏¶‰∏çËÉΩÊèê‰æõ‰∫íÂãïÂºèÂõûÈ•ã„ÄÇÂø´ÈÄüÁôºÂ±ïÁöÑË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ËÉΩÂ§†Ê†πÊìöÂΩ±ÂÉèËº∏ÂÖ•Áî¢ÁîüÊñáÂ≠óÊèèËø∞ÊàñÂõûÁ≠îÊúâÈóú‰∫∫È´îÂ∑•Â≠∏È¢®Èö™ÁöÑÂïèÈ°åÔºå‰ΩÜÂ∞öÊú™ÂèóÂà∞Âª£Ê≥õÈóúÊ≥®„ÄÇÊú¨Á†îÁ©∂‰ªãÁ¥π‰∫Ü‰∏ÄÂÄã‰∫íÂãïÂºèË¶ñË¶∫Êü•Ë©¢Á≥ªÁµ±ÔºåÂ∞àÈñÄÁî®ÊñºË©ï‰º∞Âª∫ÁØâÂ∑•‰∫∫ÁöÑÂßøÂã¢‰∫∫È´îÂ∑•Â≠∏È¢®Èö™„ÄÇË©≤Á≥ªÁµ±ÁöÑÂäüËÉΩÂåÖÊã¨Ë¶ñË¶∫ÂïèÁ≠î (VQA)ÔºåÂÆÉÂèØ‰ª•ÂõûÁ≠îÊúâÈóúÂ∑•‰∫∫Êé•Ëß∏ÂßøÂã¢‰∫∫È´îÂ∑•Â≠∏È¢®Èö™ÁöÑË¶ñË¶∫Êü•Ë©¢Ôºå‰ª•ÂèäÂΩ±ÂÉèÊ®ôÈ°å (IC)ÔºåÂÆÉÂèØ‰ª•Ê†πÊìöÂΩ±ÂÉèÁî¢ÁîüÈÄô‰∫õÈ¢®Èö™ÁöÑÊñáÂ≠óÊèèËø∞„ÄÇÊ≠§Â§ñÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∞àÈñÄÁî®ÊñºË®ìÁ∑¥ÂíåÊ∏¨Ë©¶Ê≠§È°ûÊñπÊ≥ïÁöÑË≥áÊñôÈõÜ„ÄÇÁ≥ªÁµ±ÊÄßÊ∏¨Ë©¶Ë°®ÊòéÔºåVQA ÂäüËÉΩÁöÑÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 96.5%„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®‰πùÂÄã IC ÊåáÊ®ôÈÄ≤Ë°åÁöÑË©ï‰º∞Âíå‰æÜËá™‰∫∫È°ûÂ∞àÂÆ∂ÁöÑË©ï‰º∞Ë°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïË∂ÖË∂ä‰∫Ü‰ΩøÁî®Áõ∏ÂêåÊû∂ÊßãÂÉÖÂú®ÈÄöÁî®Ë≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÊñπÊ≥ïÁöÑÊïàËÉΩ„ÄÇÊú¨Á†îÁ©∂ÁÇ∫‰ΩøÁî®ÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖß (AI) ÊäÄË°ìÁöÑ‰∫íÂãïÂºè ERA Êú™‰æÜÁôºÂ±ïË®≠ÂÆö‰∫Ü‰∏ÄÂÄãÊñ∞ÊñπÂêë„ÄÇ

