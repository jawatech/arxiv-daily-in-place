{"2407.15811": {"publish_time": "2024-07-22", "title": "Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget", "paper_summary": "As scaling laws in generative AI push performance, they also simultaneously\nconcentrate the development of these models among actors with large\ncomputational resources. With a focus on text-to-image (T2I) generative models,\nwe aim to address this bottleneck by demonstrating very low-cost training of\nlarge-scale T2I diffusion transformer models. As the computational cost of\ntransformers increases with the number of patches in each image, we propose to\nrandomly mask up to 75% of the image patches during training. We propose a\ndeferred masking strategy that preprocesses all patches using a patch-mixer\nbefore masking, thus significantly reducing the performance degradation with\nmasking, making it superior to model downscaling in reducing computational\ncost. We also incorporate the latest improvements in transformer architecture,\nsuch as the use of mixture-of-experts layers, to improve performance and\nfurther identify the critical benefit of using synthetic images in micro-budget\ntraining. Finally, using only 37M publicly available real and synthetic images,\nwe train a 1.16 billion parameter sparse transformer with only \\$1,890\neconomical cost and achieve a 12.7 FID in zero-shot generation on the COCO\ndataset. Notably, our model achieves competitive FID and high-quality\ngenerations while incurring 118$\\times$ lower cost than stable diffusion models\nand 14$\\times$ lower cost than the current state-of-the-art approach that costs\n\\$28,400. We aim to release our end-to-end training pipeline to further\ndemocratize the training of large-scale diffusion models on micro-budgets.", "paper_summary_zh": "<paragraph>\u96a8\u8457\u751f\u6210\u5f0f AI \u4e2d\u7684\u7e2e\u653e\u5b9a\u5f8b\u63d0\u5347\u6548\u80fd\uff0c\u5b83\u5011\u4e5f\u540c\u6642\u5c07\u9019\u4e9b\u6a21\u578b\u7684\u958b\u767c\u96c6\u4e2d\u5728\u64c1\u6709\u9f90\u5927\u904b\u7b97\u8cc7\u6e90\u7684\u53c3\u8207\u8005\u4e4b\u9593\u3002\u6211\u5011\u5c08\u6ce8\u65bc\u6587\u5b57\u8f49\u5f71\u50cf (T2I) \u751f\u6210\u5f0f\u6a21\u578b\uff0c\u65e8\u5728\u900f\u904e\u5c55\u793a\u5927\u898f\u6a21 T2I \u64f4\u6563Transformer\u6a21\u578b\u7684\u6975\u4f4e\u6210\u672c\u8a13\u7df4\u4f86\u89e3\u6c7a\u9019\u500b\u74f6\u9838\u3002\u7531\u65bcTransformer\u7684\u904b\u7b97\u6210\u672c\u6703\u96a8\u8457\u6bcf\u500b\u5f71\u50cf\u4e2d\u7684\u8cbc\u7247\u6578\u91cf\u800c\u589e\u52a0\uff0c\u6211\u5011\u5efa\u8b70\u5728\u8a13\u7df4\u671f\u9593\u96a8\u6a5f\u906e\u853d\u6700\u591a 75% \u7684\u5f71\u50cf\u8cbc\u7247\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5ef6\u9072\u906e\u853d\u7b56\u7565\uff0c\u5728\u906e\u853d\u4e4b\u524d\u4f7f\u7528\u8cbc\u7247\u6df7\u5408\u5668\u9810\u8655\u7406\u6240\u6709\u8cbc\u7247\uff0c\u5f9e\u800c\u986f\u8457\u964d\u4f4e\u906e\u853d\u9020\u6210\u7684\u6548\u80fd\u4e0b\u964d\uff0c\u4f7f\u5176\u5728\u964d\u4f4e\u904b\u7b97\u6210\u672c\u65b9\u9762\u512a\u65bc\u6a21\u578b\u7e2e\u653e\u3002\u6211\u5011\u9084\u7d0d\u5165\u4e86Transformer\u67b6\u69cb\u4e2d\u7684\u6700\u65b0\u6539\u9032\uff0c\u4f8b\u5982\u4f7f\u7528\u6df7\u5408\u5c08\u5bb6\u5c64\uff0c\u4ee5\u63d0\u5347\u6548\u80fd\u4e26\u9032\u4e00\u6b65\u627e\u51fa\u5728\u5fae\u9810\u7b97\u8a13\u7df4\u4e2d\u4f7f\u7528\u5408\u6210\u5f71\u50cf\u7684\u91cd\u8981\u597d\u8655\u3002\u6700\u5f8c\uff0c\u6211\u5011\u50c5\u4f7f\u7528 3700 \u842c\u5f35\u516c\u958b\u7684\u771f\u5be6\u548c\u5408\u6210\u5f71\u50cf\uff0c\u8a13\u7df4\u4e86\u4e00\u500b\u5177\u6709 11.6 \u5104\u500b\u53c3\u6578\u7684\u7a00\u758fTransformer\uff0c\u7d93\u6fdf\u6210\u672c\u50c5\u70ba 1,890 \u7f8e\u5143\uff0c\u4e26\u5728 COCO \u8cc7\u6599\u96c6\u7684\u96f6\u6b21\u751f\u6210\u4e2d\u9054\u5230\u4e86 12.7 FID\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u6a21\u578b\u9054\u5230\u4e86\u6709\u7af6\u722d\u529b\u7684 FID \u548c\u9ad8\u54c1\u8cea\u7684\u751f\u6210\uff0c\u540c\u6642\u7522\u751f\u7684\u6210\u672c\u6bd4\u7a69\u5b9a\u7684\u64f4\u6563\u6a21\u578b\u4f4e 118 \u500d\uff0c\u6bd4\u76ee\u524d\u6700\u5148\u9032\u4e14\u82b1\u8cbb 28,400 \u7f8e\u5143\u7684\u6280\u8853\u4f4e 14 \u500d\u3002\u6211\u5011\u65e8\u5728\u91cb\u51fa\u6211\u5011\u7684\u7aef\u5c0d\u7aef\u8a13\u7df4\u7ba1\u9053\uff0c\u4ee5\u9032\u4e00\u6b65\u6c11\u4e3b\u5316\u5728\u5fae\u9810\u7b97\u4e0a\u8a13\u7df4\u5927\u898f\u6a21\u64f4\u6563\u6a21\u578b\u3002</paragraph>", "author": "Vikash Sehwag et.al.", "authors": "Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, Lingjuan Lyu", "id": "2407.15811v1", "paper_url": "http://arxiv.org/abs/2407.15811v1", "repo": "null"}}