{"2406.02069": {"publish_time": "2024-06-04", "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling", "paper_summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusin on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques achieving up to a 20.5 absolute accuracy improvement on\nTREC.", "paper_summary_zh": "\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u8abf\u67e5\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5167\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u8cc7\u8a0a\u6d41\u662f\u5426\u900f\u904e\u986f\u8457\u6a21\u5f0f\u805a\u5408\u4ee5\u9032\u884c\u9577\u8a9e\u5883\u8655\u7406\u3002\u6211\u5011\u7684\u89c0\u5bdf\u986f\u793a\uff0cLLM \u900f\u904e\u91d1\u5b57\u5854\u5f0f\u8cc7\u8a0a\u6f0f\u6597\u805a\u5408\u8cc7\u8a0a\uff0c\u5176\u4e2d\u6ce8\u610f\u529b\u5728\u8f03\u4f4e\u5c64\u5ee3\u6cdb\u5206\u6563\uff0c\u5728\u7279\u5b9a\u8a9e\u5883\u4e2d\u9010\u6f38\u6574\u5408\uff0c\u4e26\u6700\u7d42\u5728\u8f03\u9ad8\u5c64\u805a\u7126\u5728\u95dc\u9375\u4ee3\u5e63\uff08\u53c8\u7a31\u5927\u91cf\u6d3b\u5316\u6216\u6ce8\u610f\u529b\u63a5\u6536\u5668\uff09\u4e0a\u3002\u53d7\u5230\u9019\u4e9b\u898b\u89e3\u7684\u555f\u767c\uff0c\u6211\u5011\u958b\u767c\u4e86 PyramidKV\uff0c\u4e00\u7a2e\u65b0\u7a4e\u4e14\u6709\u6548\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u65b9\u6cd5\u3002\u6b64\u65b9\u6cd5\u6703\u52d5\u614b\u8abf\u6574\u4e0d\u540c\u5c64\u7684 KV \u5feb\u53d6\u5927\u5c0f\uff0c\u5728\u8f03\u4f4e\u5c64\u5206\u914d\u8f03\u591a\u5feb\u53d6\uff0c\u5728\u8f03\u9ad8\u5c64\u5206\u914d\u8f03\u5c11\u5feb\u53d6\uff0c\u8207\u7dad\u6301\u5747\u52fb KV \u5feb\u53d6\u5927\u5c0f\u7684\u50b3\u7d71\u65b9\u6cd5\u4e0d\u540c\u3002\u6211\u5011\u4f7f\u7528 LongBench \u57fa\u6e96\u9032\u884c\u5be6\u9a57\u8a55\u4f30\uff0c\u7d50\u679c\u986f\u793a PyramidKV \u7684\u6548\u80fd\u8207\u5177\u6709\u5b8c\u6574 KV \u5feb\u53d6\u7684\u6a21\u578b\u76f8\u7b26\uff0c\u540c\u6642\u50c5\u4fdd\u7559 12% \u7684 KV \u5feb\u53d6\uff0c\u56e0\u6b64\u5927\u5e45\u6e1b\u5c11\u4e86\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u5728\u5f37\u8abf\u8a18\u61b6\u9ad4\u6548\u7387\u7684\u60c5\u6cc1\u4e0b\uff0c\u50c5\u7dad\u6301 0.7% \u7684 KV \u5feb\u53d6\uff0cPyramidKV \u512a\u65bc\u5176\u4ed6 KV \u5feb\u53d6\u58d3\u7e2e\u6280\u8853\uff0c\u5728 TREC \u4e0a\u9054\u5230\u4e86\u9ad8\u9054 20.5 \u7684\u7d55\u5c0d\u6e96\u78ba\u5ea6\u63d0\u5347\u3002", "author": "Zefan Cai. et.al.", "authors": "Zefan Cai., Yichi Zhang, Bofei Gao, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, Wen Xiao", "id": "2406.02069v1", "paper_url": "http://arxiv.org/abs/2406.02069v1", "repo": "null"}}