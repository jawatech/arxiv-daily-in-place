{"2410.15926": {"publish_time": "2024-10-21", "title": "Mitigating Object Hallucination via Concentric Causal Attention", "paper_summary": "Recent Large Vision Language Models (LVLMs) present remarkable zero-shot\nconversational and reasoning capabilities given multimodal queries.\nNevertheless, they suffer from object hallucination, a phenomenon where LVLMs\nare prone to generate textual responses not factually aligned with image\ninputs. Our pilot study reveals that object hallucination is closely tied with\nRotary Position Encoding (RoPE), a widely adopted positional dependency\nmodeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs\ntend to hallucinate more when relevant visual cues are distant from instruction\ntokens in the multimodal input sequence. Additionally, we observe a similar\neffect when reversing the sequential order of visual tokens during multimodal\nalignment. Our tests indicate that long-term decay in RoPE poses challenges to\nLVLMs while capturing visual-instruction interactions across long distances. We\npropose Concentric Causal Attention (CCA), a simple yet effective positional\nalignment strategy that mitigates the impact of RoPE long-term decay in LVLMs\nby naturally reducing relative distance between visual and instruction tokens.\nWith CCA, visual tokens can better interact with instruction tokens, thereby\nenhancing model's perception capability and alleviating object hallucination.\nWithout bells and whistles, our positional alignment method surpasses existing\nhallucination mitigation strategies by large margins on multiple object\nhallucination benchmarks.", "paper_summary_zh": "\u6700\u8fd1\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u5728\u7ed9\u5b9a\u591a\u6a21\u6001\u67e5\u8be2\u65f6\u5448\u73b0\u51fa\u975e\u51e1\u7684\u96f6\u6b21\u5b66\u4e60\u5bf9\u8bdd\u548c\u63a8\u7406\u80fd\u529b\u3002\n\u5c3d\u7ba1\u5982\u6b64\uff0c\u5b83\u4eec\u8fd8\u662f\u4f1a\u4ea7\u751f\u5bf9\u8c61\u5e7b\u89c9\uff0c\u8fd9\u662f\u4e00\u4e2a\u73b0\u8c61\uff0c\u5373 LVLMs \u5bb9\u6613\u751f\u6210\u4e0e\u56fe\u50cf\u8f93\u5165\u4e8b\u5b9e\u4e0d\u7b26\u7684\u6587\u672c\u54cd\u5e94\u3002\u6211\u4eec\u7684\u8bd5\u70b9\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u8c61\u5e7b\u89c9\u4e0e\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 (RoPE) \u5bc6\u5207\u76f8\u5173\uff0c\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u662f\u73b0\u6709 LVLMs \u4e2d\u5e7f\u6cdb\u91c7\u7528\u7684\u4f4d\u7f6e\u4f9d\u8d56\u5efa\u6a21\u8bbe\u8ba1\u3002\u7531\u4e8e RoPE \u4e2d\u7684\u957f\u671f\u8870\u51cf\uff0c\u5f53\u76f8\u5173\u89c6\u89c9\u7ebf\u7d22\u4e0e\u591a\u6a21\u6001\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6307\u4ee4\u6807\u8bb0\u76f8\u8ddd\u8f83\u8fdc\u65f6\uff0cLVLMs \u5f80\u5f80\u4f1a\u4ea7\u751f\u66f4\u591a\u7684\u5e7b\u89c9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u591a\u6a21\u6001\u5bf9\u9f50\u671f\u95f4\u53cd\u8f6c\u89c6\u89c9\u6807\u8bb0\u7684\u987a\u5e8f\u65f6\u89c2\u5bdf\u5230\u4e86\u7c7b\u4f3c\u7684\u6548\u679c\u3002\u6211\u4eec\u7684\u6d4b\u8bd5\u8868\u660e\uff0cRoPE \u4e2d\u7684\u957f\u671f\u8870\u51cf\u5bf9 LVLMs \u5728\u8fdc\u8ddd\u79bb\u6355\u83b7\u89c6\u89c9\u6307\u4ee4\u4ea4\u4e92\u65f6\u6784\u6210\u4e86\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u540c\u5fc3\u56e0\u679c\u6ce8\u610f\u529b (CCA)\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5b9a\u4f4d\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u7136\u5730\u7f29\u77ed\u89c6\u89c9\u6807\u8bb0\u548c\u6307\u4ee4\u6807\u8bb0\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb\u6765\u51cf\u8f7b RoPE \u957f\u671f\u8870\u51cf\u5bf9 LVLMs \u7684\u5f71\u54cd\u3002\u501f\u52a9 CCA\uff0c\u89c6\u89c9\u6807\u8bb0\u53ef\u4ee5\u66f4\u597d\u5730\u4e0e\u6307\u4ee4\u6807\u8bb0\u4ea4\u4e92\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u5e76\u51cf\u8f7b\u5bf9\u8c61\u5e7b\u89c9\u3002\u5728\u6ca1\u6709\u82b1\u91cc\u80e1\u54e8\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u7684\u4f4d\u7f6e\u5bf9\u9f50\u65b9\u6cd5\u5728\u591a\u4e2a\u5bf9\u8c61\u5e7b\u89c9\u57fa\u51c6\u4e0a\u4ee5\u5927\u5e45\u4f18\u52bf\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5e7b\u89c9\u7f13\u89e3\u7b56\u7565\u3002", "author": "Yun Xing et.al.", "authors": "Yun Xing, Yiheng Li, Ivan Laptev, Shijian Lu", "id": "2410.15926v1", "paper_url": "http://arxiv.org/abs/2410.15926v1", "repo": "https://github.com/xing0047/cca-llava"}}