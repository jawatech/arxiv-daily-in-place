{"2405.03098": {"publish_time": "2024-05-06", "title": "FairMonitor: A Dual-framework for Detecting Stereotypes and Biases in Large Language Models", "paper_summary": "Detecting stereotypes and biases in Large Language Models (LLMs) is crucial\nfor enhancing fairness and reducing adverse impacts on individuals or groups\nwhen these models are applied. Traditional methods, which rely on embedding\nspaces or are based on probability metrics, fall short in revealing the nuanced\nand implicit biases present in various contexts. To address this challenge, we\npropose the FairMonitor framework and adopt a static-dynamic detection method\nfor a comprehensive evaluation of stereotypes and biases in LLMs. The static\ncomponent consists of a direct inquiry test, an implicit association test, and\nan unknown situation test, including 10,262 open-ended questions with 9\nsensitive factors and 26 educational scenarios. And it is effective for\nevaluating both explicit and implicit biases. Moreover, we utilize the\nmulti-agent system to construst the dynamic scenarios for detecting subtle\nbiases in more complex and realistic setting. This component detects the biases\nbased on the interaction behaviors of LLMs across 600 varied educational\nscenarios. The experimental results show that the cooperation of static and\ndynamic methods can detect more stereotypes and biased in LLMs.", "paper_summary_zh": "", "author": "Yanhong Bai et.al.", "authors": "Yanhong Bai,Jiabao Zhao,Jinxin Shi,Zhentao Xie,Xingjiao Wu,Liang He", "id": "2405.03098v1", "paper_url": "http://arxiv.org/abs/2405.03098v1", "repo": "null"}}