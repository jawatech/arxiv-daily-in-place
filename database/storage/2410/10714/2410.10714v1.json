{"2410.10714": {"publish_time": "2024-10-14", "title": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators", "paper_summary": "Large Language Models (LLMs) have transformed natural language processing,\nbut face significant challenges in widespread deployment due to their high\nruntime cost. In this paper, we introduce SeedLM, a novel post-training\ncompression method that uses seeds of pseudo-random generators to encode and\ncompress model weights. Specifically, for each block of weights, we find a seed\nthat is fed into a Linear Feedback Shift Register (LFSR) during inference to\nefficiently generate a random matrix. This matrix is then linearly combined\nwith compressed coefficients to reconstruct the weight block. SeedLM reduces\nmemory access and leverages idle compute cycles during inference, effectively\nspeeding up memory-bound tasks by trading compute for fewer memory accesses.\nUnlike state-of-the-art compression methods that rely on calibration data, our\napproach is data-free and generalizes well across diverse tasks. Our\nexperiments with Llama 3 70B, which is particularly challenging to compress,\nshow that SeedLM achieves significantly better zero-shot accuracy retention at\n4- and 3-bit than state-of-the-art techniques, while maintaining performance\ncomparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that\n4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an\nFP16 Llama 2/3 baseline.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u8f49\u8b8a\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u4f46\u7531\u65bc\u5176\u9ad8\u57f7\u884c\u6642\u9593\u6210\u672c\uff0c\u5728\u5ee3\u6cdb\u90e8\u7f72\u4e2d\u9762\u81e8\u91cd\u5927\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 SeedLM\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u8a13\u7df4\u5f8c\u58d3\u7e2e\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u507d\u96a8\u6a5f\u751f\u6210\u5668\u7684\u7a2e\u5b50\u5c0d\u6a21\u578b\u6b0a\u91cd\u9032\u884c\u7de8\u78bc\u548c\u58d3\u7e2e\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5c0d\u65bc\u6bcf\u500b\u6b0a\u91cd\u584a\uff0c\u6211\u5011\u627e\u5230\u4e00\u500b\u7a2e\u5b50\uff0c\u5728\u63a8\u7406\u671f\u9593\u5c07\u5176\u8f38\u5165\u7dda\u6027\u53cd\u994b\u79fb\u4f4d\u66ab\u5b58\u5668 (LFSR) \u4e2d\uff0c\u4ee5\u6709\u6548\u751f\u6210\u96a8\u6a5f\u77e9\u9663\u3002\u7136\u5f8c\u5c07\u6b64\u77e9\u9663\u8207\u58d3\u7e2e\u4fc2\u6578\u7dda\u6027\u7d44\u5408\u4ee5\u91cd\u5efa\u6b0a\u91cd\u584a\u3002SeedLM \u6e1b\u5c11\u4e86\u8a18\u61b6\u9ad4\u5b58\u53d6\uff0c\u4e26\u5728\u63a8\u7406\u671f\u9593\u5229\u7528\u9592\u7f6e\u7684\u904b\u7b97\u9031\u671f\uff0c\u6709\u6548\u5730\u901a\u904e\u7528\u904b\u7b97\u4ea4\u63db\u66f4\u5c11\u7684\u8a18\u61b6\u9ad4\u5b58\u53d6\u4f86\u52a0\u901f\u53d7\u8a18\u61b6\u9ad4\u9650\u5236\u7684\u4efb\u52d9\u3002\u8207\u4f9d\u8cf4\u6821\u6e96\u8cc7\u6599\u7684\u6700\u65b0\u58d3\u7e2e\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u5011\u7684\u505a\u6cd5\u662f\u7121\u8cc7\u6599\u7684\uff0c\u4e26\u4e14\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002\u6211\u5011\u4f7f\u7528\u7279\u5225\u96e3\u4ee5\u58d3\u7e2e\u7684 Llama 3 70B \u9032\u884c\u7684\u5be6\u9a57\u8868\u660e\uff0cSeedLM \u5728 4 \u4f4d\u548c 3 \u4f4d\u6642\u5be6\u73fe\u4e86\u986f\u8457\u66f4\u597d\u7684\u96f6\u6b21\u5b78\u7fd2\u6e96\u78ba\u7387\u4fdd\u7559\uff0c\u540c\u6642\u4fdd\u6301\u8207 FP16 \u57fa\u6e96\u76f8\u7576\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u57fa\u65bc FPGA \u7684\u6e2c\u8a66\u8868\u660e\uff0c\u96a8\u8457\u6a21\u578b\u5927\u5c0f\u589e\u52a0\u5230 70B\uff0c4 \u4f4d\u5143 SeedLM \u63a5\u8fd1 FP16 Llama 2/3 \u57fa\u6e96\u7684 4 \u500d\u52a0\u901f\u3002", "author": "Rasoul Shafipour et.al.", "authors": "Rasoul Shafipour, David Harrison, Maxwell Horton, Jeffrey Marker, Houman Bedayat, Sachin Mehta, Mohammad Rastegari, Mahyar Najibi, Saman Naderiparizi", "id": "2410.10714v1", "paper_url": "http://arxiv.org/abs/2410.10714v1", "repo": "null"}}