{"2410.11317": {"publish_time": "2024-10-15", "title": "Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation", "paper_summary": "Automatic adversarial prompt generation provides remarkable success in\njailbreaking safely-aligned large language models (LLMs). Existing\ngradient-based attacks, while demonstrating outstanding performance in\njailbreaking white-box LLMs, often generate garbled adversarial prompts with\nchaotic appearance. These adversarial prompts are difficult to transfer to\nother LLMs, hindering their performance in attacking unknown victim models. In\nthis paper, for the first time, we delve into the semantic meaning embedded in\ngarbled adversarial prompts and propose a novel method that \"translates\" them\ninto coherent and human-readable natural language adversarial prompts. In this\nway, we can effectively uncover the semantic information that triggers\nvulnerabilities of the model and unambiguously transfer it to the victim model,\nwithout overlooking the adversarial information hidden in the garbled text, to\nenhance jailbreak attacks. It also offers a new approach to discovering\neffective designs for jailbreak prompts, advancing the understanding of\njailbreak attacks. Experimental results demonstrate that our method\nsignificantly improves the success rate of jailbreak attacks against various\nsafety-aligned LLMs and outperforms state-of-the-arts by large margins. With at\nmost 10 queries, our method achieves an average attack success rate of 81.8% in\nattacking 7 commercial closed-source LLMs, including GPT and Claude-3 series,\non HarmBench. Our method also achieves over 90% attack success rates against\nLlama-2-Chat models on AdvBench, despite their outstanding resistance to\njailbreak attacks. Code at:\nhttps://github.com/qizhangli/Adversarial-Prompt-Translator.", "paper_summary_zh": "\u81ea\u52d5\u5c0d\u6297\u63d0\u793a\u751f\u6210\u5728\u5b89\u5168\u5c0d\u9f4a\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8d8a\u7344\u4e2d\u53d6\u5f97\u4e86\u986f\u8457\u6210\u529f\u3002\u73fe\u6709\u7684\u57fa\u65bc\u68af\u5ea6\u7684\u653b\u64ca\u5728\u8d8a\u7344\u767d\u76d2 LLM \u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u6703\u7522\u751f\u6df7\u4e82\u7684\u5916\u89c0\uff0c\u751f\u6210\u96dc\u4e82\u7684\u5c0d\u6297\u63d0\u793a\u3002\u9019\u4e9b\u5c0d\u6297\u63d0\u793a\u96e3\u4ee5\u8f49\u79fb\u5230\u5176\u4ed6 LLM\uff0c\u963b\u7919\u4e86\u5b83\u5011\u653b\u64ca\u672a\u77e5\u53d7\u5bb3\u8005\u6a21\u578b\u7684\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u6b21\u6df1\u5165\u63a2\u8a0e\u4e86\u6df7\u4e82\u7684\u5c0d\u6297\u63d0\u793a\u4e2d\u5d4c\u5165\u7684\u8a9e\u7fa9\u542b\u7fa9\uff0c\u4e26\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u5c07\u5b83\u5011\u300c\u7ffb\u8b6f\u300d\u6210\u9023\u8cab\u4e14\u4eba\u985e\u53ef\u8b80\u7684\u81ea\u7136\u8a9e\u8a00\u5c0d\u6297\u63d0\u793a\u3002\u901a\u904e\u9019\u7a2e\u65b9\u5f0f\uff0c\u6211\u5011\u53ef\u4ee5\u6709\u6548\u5730\u63ed\u793a\u89f8\u767c\u6a21\u578b\u6f0f\u6d1e\u7684\u8a9e\u7fa9\u4fe1\u606f\uff0c\u4e26\u5c07\u5176\u660e\u78ba\u5730\u8f49\u79fb\u5230\u53d7\u5bb3\u8005\u6a21\u578b\u4e2d\uff0c\u800c\u4e0d\u6703\u5ffd\u8996\u96b1\u85cf\u5728\u6df7\u4e82\u6587\u672c\u4e2d\u7684\u5c0d\u6297\u4fe1\u606f\uff0c\u4ee5\u589e\u5f37\u8d8a\u7344\u653b\u64ca\u3002\u5b83\u9084\u63d0\u4f9b\u4e86\u4e00\u7a2e\u767c\u73fe\u8d8a\u7344\u63d0\u793a\u7684\u6709\u6548\u8a2d\u8a08\u7684\u65b0\u65b9\u6cd5\uff0c\u63a8\u52d5\u4e86\u5c0d\u8d8a\u7344\u653b\u64ca\u7684\u7406\u89e3\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u986f\u8457\u63d0\u9ad8\u4e86\u91dd\u5c0d\u5404\u7a2e\u5b89\u5168\u5c0d\u9f4a LLM \u7684\u8d8a\u7344\u653b\u64ca\u7684\u6210\u529f\u7387\uff0c\u4e26\u5927\u5e45\u512a\u65bc\u6700\u5148\u9032\u7684\u6280\u8853\u3002\u4f7f\u7528\u6700\u591a 10 \u500b\u67e5\u8a62\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5728\u653b\u64ca 7 \u500b\u5546\u696d\u9589\u6e90 LLM\uff08\u5305\u62ec GPT \u548c Claude-3 \u7cfb\u5217\uff09\u6642\uff0c\u5728 HarmBench \u4e0a\u5be6\u73fe\u4e86 81.8% \u7684\u5e73\u5747\u653b\u64ca\u6210\u529f\u7387\u3002\u5118\u7ba1 Llama-2-Chat \u6a21\u578b\u5c0d\u8d8a\u7344\u653b\u64ca\u5177\u6709\u51fa\u8272\u7684\u62b5\u6297\u529b\uff0c\u4f46\u6211\u5011\u7684\u65b9\u6cd5\u5728 AdvBench \u4e0a\u4ecd\u5be6\u73fe\u4e86\u8d85\u904e 90% \u7684\u653b\u64ca\u6210\u529f\u7387\u3002\u4ee3\u78bc\u5728\uff1a\nhttps://github.com/qizhangli/Adversarial-Prompt-Translator\u3002", "author": "Qizhang Li et.al.", "authors": "Qizhang Li, Xiaochen Yang, Wangmeng Zuo, Yiwen Guo", "id": "2410.11317v1", "paper_url": "http://arxiv.org/abs/2410.11317v1", "repo": "null"}}