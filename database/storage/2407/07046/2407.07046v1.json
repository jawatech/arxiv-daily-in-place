{"2407.07046": {"publish_time": "2024-07-09", "title": "CorMulT: A Semi-supervised Modality Correlation-aware Multimodal Transformer for Sentiment Analysis", "paper_summary": "Multimodal sentiment analysis is an active research area that combines\nmultiple data modalities, e.g., text, image and audio, to analyze human\nemotions and benefits a variety of applications. Existing multimodal sentiment\nanalysis methods can be classified as modality interaction-based methods,\nmodality transformation-based methods and modality similarity-based methods.\nHowever, most of these methods highly rely on the strong correlations between\nmodalities, and cannot fully uncover and utilize the correlations between\nmodalities to enhance sentiment analysis. Therefore, these methods usually\nachieve bad performance for identifying the sentiment of multimodal data with\nweak correlations. To address this issue, we proposed a two-stage\nsemi-supervised model termed Correlation-aware Multimodal Transformer (CorMulT)\nwhich consists pre-training stage and prediction stage. At the pre-training\nstage, a modality correlation contrastive learning module is designed to\nefficiently learn modality correlation coefficients between different\nmodalities. At the prediction stage, the learned correlation coefficients are\nfused with modality representations to make the sentiment prediction. According\nto the experiments on the popular multimodal dataset CMU-MOSEI, CorMulT\nobviously surpasses state-of-the-art multimodal sentiment analysis methods.", "paper_summary_zh": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u662f\u4e00\u4e2a\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\uff0c\u5b83\u7ed3\u5408\u4e86\u591a\u79cd\u6570\u636e\u6a21\u5f0f\uff0c\u4f8b\u5982\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\uff0c\u6765\u5206\u6790\u4eba\u7c7b\u60c5\u7eea\uff0c\u5e76\u4f7f\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\u53d7\u76ca\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u53ef\u4ee5\u5206\u4e3a\u57fa\u4e8e\u6a21\u6001\u4ea4\u4e92\u7684\u65b9\u6cd5\u3001\u57fa\u4e8e\u6a21\u6001\u8f6c\u6362\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u6a21\u6001\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5927\u591a\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6a21\u6001\u4e4b\u95f4\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u65e0\u6cd5\u5145\u5206\u53d1\u73b0\u548c\u5229\u7528\u6a21\u6001\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6765\u589e\u5f3a\u60c5\u611f\u5206\u6790\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5728\u8bc6\u522b\u5f31\u76f8\u5173\u6027\u591a\u6a21\u6001\u6570\u636e\u7684\u8bed\u4e49\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u534a\u76d1\u7763\u6a21\u578b\uff0c\u79f0\u4e3a\u76f8\u5173\u611f\u77e5\u591a\u6a21\u6001\u8f6c\u6362\u5668 (CorMulT)\uff0c\u5b83\u7531\u9884\u8bad\u7ec3\u9636\u6bb5\u548c\u9884\u6d4b\u9636\u6bb5\u7ec4\u6210\u3002\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u6001\u76f8\u5173\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u4ee5\u6709\u6548\u5730\u5b66\u4e60\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u6a21\u6001\u76f8\u5173\u7cfb\u6570\u3002\u5728\u9884\u6d4b\u9636\u6bb5\uff0c\u5b66\u4e60\u5230\u7684\u76f8\u5173\u7cfb\u6570\u4e0e\u6a21\u6001\u8868\u793a\u878d\u5408\uff0c\u4ee5\u8fdb\u884c\u60c5\u611f\u9884\u6d4b\u3002\u6839\u636e\u6d41\u884c\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6 CMU-MOSEI \u4e0a\u7684\u5b9e\u9a8c\uff0cCorMulT \u660e\u663e\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u3002", "author": "Yangmin Li et.al.", "authors": "Yangmin Li, Ruiqi Zhu, Wengen Li", "id": "2407.07046v1", "paper_url": "http://arxiv.org/abs/2407.07046v1", "repo": "null"}}