{"2405.02937": {"publish_time": "2024-05-05", "title": "Unraveling the Dominance of Large Language Models Over Transformer Models for Bangla Natural Language Inference: A Comprehensive Study", "paper_summary": "Natural Language Inference (NLI) is a cornerstone of Natural Language\nProcessing (NLP), providing insights into the entailment relationships between\ntext pairings. It is a critical component of Natural Language Understanding\n(NLU), demonstrating the ability to extract information from spoken or written\ninteractions. NLI is mainly concerned with determining the entailment\nrelationship between two statements, known as the premise and hypothesis. When\nthe premise logically implies the hypothesis, the pair is labeled \"entailment\".\nIf the hypothesis contradicts the premise, the pair receives the\n\"contradiction\" label. When there is insufficient evidence to establish a\nconnection, the pair is described as \"neutral\". Despite the success of Large\nLanguage Models (LLMs) in various tasks, their effectiveness in NLI remains\nconstrained by issues like low-resource domain accuracy, model overconfidence,\nand difficulty in capturing human judgment disagreements. This study addresses\nthe underexplored area of evaluating LLMs in low-resourced languages such as\nBengali. Through a comprehensive evaluation, we assess the performance of\nprominent LLMs and state-of-the-art (SOTA) models in Bengali NLP tasks,\nfocusing on natural language inference. Utilizing the XNLI dataset, we conduct\nzero-shot and few-shot evaluations, comparing LLMs like GPT-3.5 Turbo and\nGemini 1.5 Pro with models such as BanglaBERT, Bangla BERT Base, DistilBERT,\nmBERT, and sahajBERT. Our findings reveal that while LLMs can achieve\ncomparable or superior performance to fine-tuned SOTA models in few-shot\nscenarios, further research is necessary to enhance our understanding of LLMs\nin languages with modest resources like Bengali. This study underscores the\nimportance of continued efforts in exploring LLM capabilities across diverse\nlinguistic contexts.", "paper_summary_zh": "", "author": "Fatema Tuj Johora Faria et.al.", "authors": "Fatema Tuj Johora Faria,Mukaffi Bin Moin,Asif Iftekher Fahim,Pronay Debnath,Faisal Muhammad Shah", "id": "2405.02937v2", "paper_url": "http://arxiv.org/abs/2405.02937v2", "repo": "https://github.com/fatemafaria142/Large-Language-Models-Over-Transformer-Models-for-Bangla-NLI"}}