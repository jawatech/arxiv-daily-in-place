{"2406.12806": {"publish_time": "2024-06-18", "title": "Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents", "paper_summary": "Configuration settings are essential for tailoring software behavior to meet\nspecific performance requirements. However, incorrect configurations are\nwidespread, and identifying those that impact system performance is challenging\ndue to the vast number and complexity of possible settings. In this work, we\npresent PerfSense, a lightweight framework that leverages Large Language Models\n(LLMs) to efficiently identify performance-sensitive configurations with\nminimal overhead. PerfSense employs LLM agents to simulate interactions between\ndevelopers and performance engineers using advanced prompting techniques such\nas prompt chaining and retrieval-augmented generation (RAG). Our evaluation of\nseven open-source Java systems demonstrates that PerfSense achieves an average\naccuracy of 64.77% in classifying performance-sensitive configurations,\noutperforming both our LLM baseline (50.36%) and the previous state-of-the-art\nmethod (61.75%). Notably, our prompt chaining technique improves recall by 10%\nto 30% while maintaining similar precision levels. Additionally, a manual\nanalysis of 362 misclassifications reveals common issues, including LLMs'\nmisunderstandings of requirements (26.8%). In summary, PerfSense significantly\nreduces manual effort in classifying performance-sensitive configurations and\noffers valuable insights for future LLM-based code analysis research.", "paper_summary_zh": "\u8a2d\u5b9a\u7d44\u614b\u5c0d\u65bc\u8abf\u6574\u8edf\u9ad4\u884c\u70ba\u4ee5\u7b26\u5408\u7279\u5b9a\u6548\u80fd\u9700\u6c42\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u4e0d\u6b63\u78ba\u7684\u7d44\u614b\u5f88\u666e\u904d\uff0c\u4e14\u7531\u65bc\u53ef\u80fd\u7684\u8a2d\u5b9a\u6578\u91cf\u9f90\u5927\u4e14\u8907\u96dc\uff0c\u56e0\u6b64\u627e\u51fa\u5f71\u97ff\u7cfb\u7d71\u6548\u80fd\u7684\u8a2d\u5b9a\u5177\u6709\u6311\u6230\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa PerfSense\uff0c\u4e00\u500b\u8f15\u91cf\u7d1a\u7684\u67b6\u69cb\uff0c\u5b83\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u6700\u5c0f\u7684\u958b\u92b7\u6709\u6548\u5730\u627e\u51fa\u6548\u80fd\u654f\u611f\u7684\u7d44\u614b\u3002PerfSense \u4f7f\u7528 LLM \u4ee3\u7406\u7a0b\u5f0f\u4f86\u6a21\u64ec\u958b\u767c\u4eba\u54e1\u548c\u6548\u80fd\u5de5\u7a0b\u5e2b\u4e4b\u9593\u7684\u4e92\u52d5\uff0c\u4f7f\u7528\u9032\u968e\u63d0\u793a\u6280\u8853\uff0c\u4f8b\u5982\u63d0\u793a\u93c8\u63a5\u548c\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG)\u3002\u6211\u5011\u5c0d\u4e03\u500b\u958b\u6e90 Java \u7cfb\u7d71\u7684\u8a55\u4f30\u986f\u793a\uff0cPerfSense \u5728\u5206\u985e\u6548\u80fd\u654f\u611f\u7d44\u614b\u65b9\u9762\u9054\u5230\u4e86 64.77% \u7684\u5e73\u5747\u6e96\u78ba\u5ea6\uff0c\u512a\u65bc\u6211\u5011\u7684 LLM \u57fa\u6e96 (50.36%) \u548c\u5148\u524d\u7684\u6700\u5148\u9032\u65b9\u6cd5 (61.75%)\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u63d0\u793a\u93c8\u63a5\u6280\u8853\u5c07\u53ec\u56de\u7387\u63d0\u9ad8\u4e86 10% \u81f3 30%\uff0c\u540c\u6642\u7dad\u6301\u985e\u4f3c\u7684\u7cbe\u78ba\u5ea6\u6c34\u6e96\u3002\u6b64\u5916\uff0c\u5c0d 362 \u500b\u932f\u8aa4\u5206\u985e\u7684\u624b\u52d5\u5206\u6790\u63ed\u9732\u4e86\u5e38\u898b\u7684\u554f\u984c\uff0c\u5305\u62ec LLM \u5c0d\u9700\u6c42\u7684\u8aa4\u89e3 (26.8%)\u3002\u7e3d\u4e4b\uff0cPerfSense \u5927\u5e45\u6e1b\u5c11\u4e86\u5206\u985e\u6548\u80fd\u654f\u611f\u7d44\u614b\u7684\u624b\u52d5\u5de5\u4f5c\uff0c\u4e26\u70ba\u672a\u4f86\u7684\u57fa\u65bc LLM \u7684\u7a0b\u5f0f\u78bc\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002", "author": "Zehao Wang et.al.", "authors": "Zehao Wang, Dong Jae Kim, Tse-Hsun Chen", "id": "2406.12806v1", "paper_url": "http://arxiv.org/abs/2406.12806v1", "repo": "null"}}