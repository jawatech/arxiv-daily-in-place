{"2406.14129": {"publish_time": "2024-06-20", "title": "Towards Event-oriented Long Video Understanding", "paper_summary": "With the rapid development of video Multimodal Large Language Models (MLLMs),\nnumerous benchmarks have been proposed to assess their video understanding\ncapability. However, due to the lack of rich events in the videos, these\ndatasets may suffer from the short-cut bias that the answers can be deduced\nfrom a few frames, without the need to watch the entire video. To address this\nissue, we introduce Event-Bench, an event-oriented long video understanding\nbenchmark built on existing datasets and human annotations. Event-Bench\nincludes six event-related tasks and 2,190 test instances to comprehensively\nevaluate video event understanding ability. Additionally, we propose Video\nInstruction Merging~(VIM), a cost-effective method that enhances video MLLMs\nusing merged, event-intensive video instructions, addressing the scarcity of\nhuman-annotated, event-intensive data. Extensive experiments show that the\nbest-performing model, GPT-4o, achieves an overall accuracy of 53.33,\nsignificantly outperforming the best open-source model by 41.42%. Leveraging an\neffective instruction synthesis method and an adaptive model architecture, VIM\nsurpasses both state-of-the-art open-source models and GPT-4V on the\nEvent-Bench. All code, data, and models are publicly available at\nhttps://github.com/RUCAIBox/Event-Bench.", "paper_summary_zh": "\u96a8\u8457\u8996\u8a0a\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u5feb\u901f\u767c\u5c55\uff0c\n\u5df2\u7d93\u63d0\u51fa\u8a31\u591a\u57fa\u6e96\u4f86\u8a55\u4f30\u5b83\u5011\u7684\u8996\u8a0a\u7406\u89e3\u80fd\u529b\u3002\n\u7136\u800c\uff0c\u7531\u65bc\u5f71\u7247\u4e2d\u8c50\u5bcc\u4e8b\u4ef6\u7684\u7f3a\u4e4f\uff0c\u9019\u4e9b\n\u8cc7\u6599\u96c6\u53ef\u80fd\u56e0\u6377\u5f91\u504f\u8aa4\u800c\u53d7\u5f71\u97ff\uff0c\u5373\u7b54\u6848\u53ef\u4ee5\u5f9e\u5e7e\u5e40\u4e2d\u63a8\u8ad6\u51fa\u4f86\uff0c\u800c\u7121\u9700\u89c0\u770b\u6574\u500b\u5f71\u7247\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\n\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 Event-Bench\uff0c\u9019\u662f\u4e00\u500b\u5efa\u7acb\u5728\u73fe\u6709\u8cc7\u6599\u96c6\u548c\u4eba\u5de5\u8a3b\u89e3\u4e0a\u7684\u4e8b\u4ef6\u5c0e\u5411\u9577\u5f71\u7247\u7406\u89e3\u57fa\u6e96\u3002Event-Bench\n\u5305\u542b\u516d\u500b\u8207\u4e8b\u4ef6\u76f8\u95dc\u7684\u4efb\u52d9\u548c 2,190 \u500b\u6e2c\u8a66\u5be6\u4f8b\uff0c\u4ee5\u5168\u9762\n\u8a55\u4f30\u5f71\u7247\u4e8b\u4ef6\u7406\u89e3\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5f71\u7247\n\u6307\u4ee4\u5408\u4f75~(VIM)\uff0c\u9019\u662f\u4e00\u7a2e\u4f7f\u7528\u5408\u4f75\u7684\u3001\u4e8b\u4ef6\u5bc6\u96c6\u578b\u5f71\u7247\u6307\u4ee4\u4f86\u589e\u5f37\u5f71\u7247 MLLM \u7684\u7d93\u6fdf\u6709\u6548\u65b9\u6cd5\uff0c\u89e3\u6c7a\u4e86\u4eba\u5de5\u8a3b\u89e3\u3001\u4e8b\u4ef6\u5bc6\u96c6\u578b\u8cc7\u6599\u7684\u7a00\u7f3a\u6027\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8868\u660e\uff0c\n\u8868\u73fe\u6700\u4f73\u7684\u6a21\u578b GPT-4o\uff0c\u9054\u5230\u4e86 53.33 \u7684\u6574\u9ad4\u6e96\u78ba\u5ea6\uff0c\n\u986f\u8457\u512a\u65bc\u6700\u4f73\u958b\u6e90\u6a21\u578b 41.42%\u3002\u5229\u7528\u6709\u6548\u7684\u6307\u4ee4\u5408\u6210\u65b9\u6cd5\u548c\u81ea\u9069\u61c9\u6a21\u578b\u67b6\u69cb\uff0cVIM\n\u8d85\u8d8a\u4e86 Event-Bench \u4e0a\u7684\u6700\u65b0\u958b\u6e90\u6a21\u578b\u548c GPT-4V\u3002\u6240\u6709\u7a0b\u5f0f\u78bc\u3001\u8cc7\u6599\u548c\u6a21\u578b\u90fd\u53ef\u4ee5\u5728\nhttps://github.com/RUCAIBox/Event-Bench \u516c\u958b\u53d6\u5f97\u3002", "author": "Yifan Du et.al.", "authors": "Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen", "id": "2406.14129v1", "paper_url": "http://arxiv.org/abs/2406.14129v1", "repo": "https://github.com/rucaibox/event-bench"}}