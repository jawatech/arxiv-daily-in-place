{"2406.03792": {"publish_time": "2024-06-06", "title": "Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning", "paper_summary": "Parameter-efficient fine-tuning (PEFT) has emerged as the predominant\ntechnique for fine-tuning in the era of large language models. However,\nexisting PEFT methods still have inadequate training efficiency. Firstly, the\nutilization of large-scale foundation models during the training process is\nexcessively redundant for certain fine-tuning tasks. Secondly, as the model\nsize increases, the growth in trainable parameters of empirically added PEFT\nmodules becomes non-negligible and redundant, leading to inefficiency. To\nachieve task-specific efficient fine-tuning, we propose the Light-PEFT\nframework, which includes two methods: Masked Early Pruning of the Foundation\nModel and Multi-Granularity Early Pruning of PEFT. The Light-PEFT framework\nallows for the simultaneous estimation of redundant parameters in both the\nfoundation model and PEFT modules during the early stage of training. These\nparameters can then be pruned for more efficient fine-tuning. We validate our\napproach on GLUE, SuperGLUE, QA tasks, and various models. With Light-PEFT,\nparameters of the foundation model can be pruned by up to over 40%, while still\ncontrolling trainable parameters to be only 25% of the original PEFT method.\nCompared to utilizing the PEFT method directly, Light-PEFT achieves training\nand inference speedup, reduces memory usage, and maintains comparable\nperformance and the plug-and-play feature of PEFT.", "paper_summary_zh": "\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u5df2\u6210\u70ba\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u6642\u4ee3\u5fae\u8abf\u7684\u4e3b\u8981\u6280\u8853\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 PEFT \u65b9\u6cd5\u4ecd\u6709\u8a13\u7df4\u6548\u7387\u4e0d\u8db3\u7684\u554f\u984c\u3002\u9996\u5148\uff0c\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u4f7f\u7528\u5927\u898f\u6a21\u57fa\u790e\u6a21\u578b\u5c0d\u65bc\u67d0\u4e9b\u5fae\u8abf\u4efb\u52d9\u4f86\u8aaa\u904e\u65bc\u5197\u9918\u3002\u5176\u6b21\uff0c\u96a8\u8457\u6a21\u578b\u898f\u6a21\u7684\u589e\u52a0\uff0c\u7d93\u9a57\u6027\u6dfb\u52a0\u7684 PEFT \u6a21\u7d44\u4e2d\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u589e\u9577\u8b8a\u5f97\u4e0d\u53ef\u5ffd\u8996\u4e14\u5197\u9918\uff0c\u5c0e\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u70ba\u4e86\u5be6\u73fe\u7279\u5b9a\u4efb\u52d9\u7684\u9ad8\u6548\u5fae\u8abf\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Light-PEFT \u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u5169\u7a2e\u65b9\u6cd5\uff1a\u57fa\u790e\u6a21\u578b\u7684\u906e\u7f69\u65e9\u671f\u526a\u679d\u548c PEFT \u7684\u591a\u7c92\u5ea6\u65e9\u671f\u526a\u679d\u3002Light-PEFT \u6846\u67b6\u5141\u8a31\u5728\u8a13\u7df4\u7684\u65e9\u671f\u968e\u6bb5\u540c\u6642\u4f30\u8a08\u57fa\u790e\u6a21\u578b\u548c PEFT \u6a21\u7d44\u4e2d\u7684\u5197\u9918\u53c3\u6578\u3002\u7136\u5f8c\u53ef\u4ee5\u526a\u679d\u9019\u4e9b\u53c3\u6578\u4ee5\u5be6\u73fe\u66f4\u6709\u6548\u7684\u5fae\u8abf\u3002\u6211\u5011\u5728 GLUE\u3001SuperGLUE\u3001QA \u4efb\u52d9\u548c\u5404\u7a2e\u6a21\u578b\u4e0a\u9a57\u8b49\u4e86\u6211\u5011\u7684\u505a\u6cd5\u3002\u4f7f\u7528 Light-PEFT\uff0c\u57fa\u790e\u6a21\u578b\u7684\u53c3\u6578\u53ef\u4ee5\u526a\u679d\u591a\u9054 40%\uff0c\u540c\u6642\u4ecd\u5c07\u53ef\u8a13\u7df4\u53c3\u6578\u63a7\u5236\u5728\u539f\u59cb PEFT \u65b9\u6cd5\u7684 25%\u3002\u8207\u76f4\u63a5\u4f7f\u7528 PEFT \u65b9\u6cd5\u76f8\u6bd4\uff0cLight-PEFT \u53ef\u52a0\u5feb\u8a13\u7df4\u548c\u63a8\u8ad6\u901f\u5ea6\u3001\u6e1b\u5c11\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\uff0c\u4e26\u7dad\u6301 PEFT \u7684\u53ef\u6bd4\u6548\u80fd\u548c\u5373\u63d2\u5373\u7528\u529f\u80fd\u3002", "author": "Naibin Gu et.al.", "authors": "Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, Weiping Wang", "id": "2406.03792v1", "paper_url": "http://arxiv.org/abs/2406.03792v1", "repo": "null"}}