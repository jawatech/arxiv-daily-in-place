{"2406.18925": {"publish_time": "2024-06-27", "title": "Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding", "paper_summary": "Visual arguments, often used in advertising or social causes, rely on images\nto persuade viewers to do or believe something. Understanding these arguments\nrequires selective vision: only specific visual stimuli within an image are\nrelevant to the argument, and relevance can only be understood within the\ncontext of a broader argumentative structure. While visual arguments are\nreadily appreciated by human audiences, we ask: are today's AI capable of\nsimilar understanding?\n  We collect and release VisArgs, an annotated corpus designed to make explicit\nthe (usually implicit) structures underlying visual arguments. VisArgs includes\n1,611 images accompanied by three types of textual annotations: 5,112 visual\npremises (with region annotations), 5,574 commonsense premises, and reasoning\ntrees connecting them to a broader argument. We propose three tasks over\nVisArgs to probe machine capacity for visual argument understanding:\nlocalization of premises, identification of premises, and deduction of\nconclusions. Experiments demonstrate that 1) machines cannot fully identify the\nrelevant visual cues. The top-performing model, GPT-4-O, achieved an accuracy\nof only 78.5%, whereas humans reached 98.0%. All models showed a performance\ndrop, with an average decrease in accuracy of 19.5%, when the comparison set\nwas changed from objects outside the image to irrelevant objects within the\nimage. Furthermore, 2) this limitation is the greatest factor impacting their\nperformance in understanding visual arguments. Most models improved the most\nwhen given relevant visual premises as additional inputs, compared to other\ninputs, for deducing the conclusion of the visual argument.", "paper_summary_zh": "\u8996\u89ba\u8ad6\u8b49\u7d93\u5e38\u4f7f\u7528\u5728\u5ee3\u544a\u6216\u793e\u6703\u8b70\u984c\u4e2d\uff0c\u5229\u7528\u5716\u7247\u8aaa\u670d\u89c0\u773e\u63a1\u53d6\u884c\u52d5\u6216\u76f8\u4fe1\u67d0\u4ef6\u4e8b\u3002\u4e86\u89e3\u9019\u4e9b\u8ad6\u8b49\u9700\u8981\u9078\u64c7\u6027\u8996\u89ba\uff1a\u5716\u50cf\u4e2d\u53ea\u6709\u7279\u5b9a\u7684\u8996\u89ba\u523a\u6fc0\u8207\u8ad6\u8b49\u76f8\u95dc\uff0c\u800c\u76f8\u95dc\u6027\u53ea\u80fd\u5728\u66f4\u5ee3\u6cdb\u7684\u8ad6\u8b49\u7d50\u69cb\u4e2d\u7406\u89e3\u3002\u96d6\u7136\u8996\u89ba\u8ad6\u8b49\u5f88\u5bb9\u6613\u88ab\u4eba\u985e\u53d7\u773e\u7406\u89e3\uff0c\u4f46\u6211\u5011\u60f3\u77e5\u9053\uff1a\u7576\u4eca\u7684\u4eba\u5de5\u667a\u6167\u662f\u5426\u80fd\u6709\u985e\u4f3c\u7684\u7406\u89e3\uff1f\n\u6211\u5011\u6536\u96c6\u4e26\u767c\u5e03 VisArgs\uff0c\u9019\u662f\u4e00\u500b\u5e36\u6709\u8a3b\u91cb\u7684\u8a9e\u6599\u5eab\uff0c\u65e8\u5728\u660e\u78ba\u8996\u89ba\u8ad6\u8b49\u80cc\u5f8c\uff08\u901a\u5e38\u662f\u96b1\u542b\u7684\uff09\u7d50\u69cb\u3002VisArgs \u5305\u542b 1,611 \u5f35\u5716\u7247\uff0c\u4e26\u9644\u6709\u4e09\u7a2e\u985e\u578b\u7684\u6587\u672c\u8a3b\u91cb\uff1a5,112 \u500b\u8996\u89ba\u524d\u63d0\uff08\u5e36\u6709\u5340\u57df\u8a3b\u91cb\uff09\u30015,574 \u500b\u5e38\u8b58\u524d\u63d0\uff0c\u4ee5\u53ca\u5c07\u5b83\u5011\u8207\u66f4\u5ee3\u6cdb\u7684\u8ad6\u8b49\u806f\u7e6b\u8d77\u4f86\u7684\u63a8\u7406\u6a39\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e09\u9805\u95dc\u65bc VisArgs \u7684\u4efb\u52d9\uff0c\u4ee5\u63a2\u8a0e\u6a5f\u5668\u5728\u8996\u89ba\u8ad6\u8b49\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff1a\u524d\u63d0\u5b9a\u4f4d\u3001\u524d\u63d0\u8b58\u5225\u548c\u7d50\u8ad6\u63a8\u8ad6\u3002\u5be6\u9a57\u8868\u660e 1) \u6a5f\u5668\u7121\u6cd5\u5b8c\u5168\u8b58\u5225\u76f8\u95dc\u7684\u8996\u89ba\u7dda\u7d22\u3002\u8868\u73fe\u6700\u597d\u7684\u6a21\u578b GPT-4-O \u50c5\u9054\u5230 78.5% \u7684\u6e96\u78ba\u5ea6\uff0c\u800c\u4eba\u985e\u9054\u5230 98.0%\u3002\u7576\u6bd4\u8f03\u96c6\u5f9e\u5716\u50cf\u5916\u90e8\u5c0d\u8c61\u8b8a\u70ba\u5716\u50cf\u5167\u90e8\u7121\u95dc\u5c0d\u8c61\u6642\uff0c\u6240\u6709\u6a21\u578b\u90fd\u8868\u73fe\u51fa\u6027\u80fd\u4e0b\u964d\uff0c\u6e96\u78ba\u5ea6\u5e73\u5747\u4e0b\u964d 19.5%\u3002\u6b64\u5916\uff0c2) \u9019\u7a2e\u9650\u5236\u662f\u5f71\u97ff\u5b83\u5011\u7406\u89e3\u8996\u89ba\u8ad6\u8b49\u8868\u73fe\u7684\u6700\u5927\u56e0\u7d20\u3002\u8207\u5176\u4ed6\u8f38\u5165\u76f8\u6bd4\uff0c\u5927\u591a\u6578\u6a21\u578b\u5728\u7372\u5f97\u76f8\u95dc\u8996\u89ba\u524d\u63d0\u4f5c\u70ba\u9644\u52a0\u8f38\u5165\u6642\uff0c\u5728\u63a8\u8ad6\u8996\u89ba\u8ad6\u8b49\u7d50\u8ad6\u65b9\u9762\u7684\u9032\u6b65\u6700\u5927\u3002", "author": "Jiwan Chung et.al.", "authors": "Jiwan Chung, Sungjae Lee, Minseo Kim, Seungju Han, Ashkan Yousefpour, Jack Hessel, Youngjae Yu", "id": "2406.18925v1", "paper_url": "http://arxiv.org/abs/2406.18925v1", "repo": "null"}}