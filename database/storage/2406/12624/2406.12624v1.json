{"2406.12624": {"publish_time": "2024-06-18", "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges", "paper_summary": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges. We leverage TriviaQA\nas a benchmark for assessing objective knowledge reasoning of LLMs and evaluate\nthem alongside human annotations which we found to have a high inter-annotator\nagreement. Our study includes 9 judge models and 9 exam taker models -- both\nbase and instruction-tuned. We assess the judge model's alignment across\ndifferent model sizes, families, and judge prompts. Among other results, our\nresearch rediscovers the importance of using Cohen's kappa as a metric of\nalignment as opposed to simple percent agreement, showing that judges with high\npercent agreement can still assign vastly different scores. We find that both\nLlama-3 70B and GPT-4 Turbo have an excellent alignment with humans, but in\nterms of ranking exam taker models, they are outperformed by both JudgeLM-7B\nand the lexical judge Contains, which have up to 34 points lower human\nalignment. Through error analysis and various other studies, including the\neffects of instruction length and leniency bias, we hope to provide valuable\nlessons for using LLMs as judges in the future.", "paper_summary_zh": "\u91dd\u5c0d\u8207\u4eba\u5de5\u8a55\u91cf\u76f8\u95dc\u7684\u53ef\u64f4\u5145\u6027\u6311\u6230\u63d0\u4f9b\u6709\u524d\u666f\u7684\u89e3\u6c7a\u65b9\u6848\uff0cLLM \u4f5c\u70ba\u8a55\u5be9\u5178\u7bc4\u6b63\u8fc5\u901f\u7372\u5f97\u95dc\u6ce8\uff0c\u4f5c\u70ba\u8a55\u91cf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u95dc\u65bc\u6b64\u5178\u7bc4\u7684\u512a\u7f3a\u9ede\u4ee5\u53ca\u6f5b\u5728\u504f\u8aa4\uff0c\u4ecd\u6709\u8a31\u591a\u554f\u984c\u5c1a\u672a\u89e3\u6c7a\u3002\u5728\u6b64\u8ad6\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u9805\u91dd\u5c0d\u5404\u7a2e\u64d4\u4efb\u8a55\u5be9\u89d2\u8272\u7684 LLM \u7684\u6548\u80fd\u9032\u884c\u5168\u9762\u7814\u7a76\u3002\u6211\u5011\u5229\u7528 TriviaQA \u4f5c\u70ba\u8a55\u91cf LLM \u5ba2\u89c0\u77e5\u8b58\u63a8\u7406\u7684\u57fa\u6e96\uff0c\u4e26\u6839\u64da\u6211\u5011\u767c\u73fe\u5177\u6709\u9ad8\u5ea6\u6a19\u8a3b\u8005\u9593\u4e00\u81f4\u6027\u7684\u6a19\u8a3b\uff0c\u5c0d\u5176\u9032\u884c\u8a55\u91cf\u3002\u6211\u5011\u7684\u7814\u7a76\u5305\u542b 9 \u500b\u8a55\u5be9\u6a21\u578b\u548c 9 \u500b\u53d7\u8a66\u8005\u6a21\u578b\uff0c\u5305\u542b\u57fa\u790e\u6a21\u578b\u548c\u7d93\u904e\u6307\u4ee4\u5fae\u8abf\u7684\u6a21\u578b\u3002\u6211\u5011\u8a55\u4f30\u8a55\u5be9\u6a21\u578b\u5728\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u3001\u6a21\u578b\u985e\u578b\u548c\u8a55\u5be9\u63d0\u793a\u4e0b\u7684\u5c0d\u9f4a\u5ea6\u3002\u5728\u5176\u4ed6\u7d50\u679c\u4e2d\uff0c\u6211\u5011\u7684\u7814\u7a76\u91cd\u65b0\u767c\u73fe\u4f7f\u7528 Cohen's kappa \u4f5c\u70ba\u5c0d\u9f4a\u5ea6\u91cf\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u7c21\u55ae\u7684\u767e\u5206\u6bd4\u4e00\u81f4\u6027\uff0c\u986f\u793a\u8a55\u5be9\u5177\u6709\u9ad8\u767e\u5206\u6bd4\u4e00\u81f4\u6027\uff0c\u4f46\u4ecd\u53ef\u80fd\u5206\u914d\u51fa\u5dee\u7570\u6975\u5927\u7684\u5206\u6578\u3002\u6211\u5011\u767c\u73fe Llama-3 70B \u548c GPT-4 Turbo \u8207\u4eba\u985e\u7684\u5c0d\u9f4a\u5ea6\u90fd\u5f88\u512a\u79c0\uff0c\u4f46\u5c31\u5c0d\u53d7\u8a66\u8005\u6a21\u578b\u9032\u884c\u6392\u540d\u800c\u8a00\uff0cJudgeLM-7B \u548c\u8a5e\u5f59\u8a55\u5be9 Contains \u7684\u8868\u73fe\u90fd\u6bd4\u5b83\u5011\u597d\uff0c\u5176\u4eba\u985e\u5c0d\u9f4a\u5ea6\u4f4e\u9054 34 \u500b\u767e\u5206\u9ede\u3002\u900f\u904e\u932f\u8aa4\u5206\u6790\u548c\u5404\u7a2e\u5176\u4ed6\u7814\u7a76\uff0c\u5305\u62ec\u6307\u4ee4\u9577\u5ea6\u548c\u5bec\u5bb9\u5ea6\u504f\u5dee\u7684\u5f71\u97ff\uff0c\u6211\u5011\u5e0c\u671b\u70ba\u672a\u4f86\u4f7f\u7528 LLM \u4f5c\u70ba\u8a55\u5be9\u63d0\u4f9b\u5bf6\u8cb4\u7684\u7d93\u9a57\u3002", "author": "Aman Singh Thakur et.al.", "authors": "Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes", "id": "2406.12624v1", "paper_url": "http://arxiv.org/abs/2406.12624v1", "repo": "null"}}