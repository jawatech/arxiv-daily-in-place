{"2504.06196": {"publish_time": "2025-04-08", "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics", "paper_summary": "Therapeutic development is a costly and high-risk endeavor that is often\nplagued by high failure rates. To address this, we introduce TxGemma, a suite\nof efficient, generalist large language models (LLMs) capable of therapeutic\nproperty prediction as well as interactive reasoning and explainability. Unlike\ntask-specific models, TxGemma synthesizes information from diverse sources,\nenabling broad application across the therapeutic development pipeline. The\nsuite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a\ncomprehensive dataset of small molecules, proteins, nucleic acids, diseases,\nand cell lines. Across 66 therapeutic development tasks, TxGemma achieved\nsuperior or comparable performance to the state-of-the-art generalist model on\n64 (superior on 45), and against state-of-the-art specialist models on 50\n(superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks,\nsuch as clinical trial adverse event prediction, requires less training data\nthan fine-tuning base LLMs, making TxGemma suitable for data-limited\napplications. Beyond these predictive capabilities, TxGemma features\nconversational models that bridge the gap between general LLMs and specialized\nproperty predictors. These allow scientists to interact in natural language,\nprovide mechanistic reasoning for predictions based on molecular structure, and\nengage in scientific discussions. Building on this, we further introduce\nAgentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that\nreasons, acts, manages diverse workflows, and acquires external domain\nknowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last\nExam benchmark (Chemistry & Biology) with 52.3% relative improvement over\no3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels\nwith improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over\no3-mini (high).", "paper_summary_zh": "<paragraph>\u6cbb\u7642\u85e5\u7269\u958b\u767c\u662f\u4e00\u9805\u6210\u672c\u9ad8\u6602\u4e14\u98a8\u96aa\u6975\u5927\u7684\u5de5\u4f5c\uff0c\u5931\u6557\u7387\u901a\u5e38\u5f88\u9ad8\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63a8\u51fa\u4e86 TxGemma\uff0c\u4e00\u5957\u9ad8\u6548\u3001\u901a\u7528\u7684\u5de8\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u80fd\u5920\u9810\u6e2c\u6cbb\u7642\u7279\u6027\uff0c\u4e26\u5177\u5099\u4e92\u52d5\u63a8\u7406\u548c\u53ef\u89e3\u91cb\u6027\u3002 \u8207\u7279\u5b9a\u4efb\u52d9\u6a21\u578b\u4e0d\u540c\uff0cTxGemma \u80fd\u5920\u7d9c\u5408\u4f86\u81ea\u4e0d\u540c\u4f86\u6e90\u7684\u8cc7\u8a0a\uff0c\u4f7f\u5176\u53ef\u5728\u6574\u500b\u6cbb\u7642\u85e5\u7269\u958b\u767c\u6d41\u7a0b\u4e2d\u5ee3\u6cdb\u61c9\u7528\u3002\u8a72\u5957\u4ef6\u5305\u542b 2B\u30019B \u548c 27B \u53c3\u6578\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u662f\u6839\u64da Gemma-2 \u5728\u5305\u542b\u5c0f\u5206\u5b50\u3001\u86cb\u767d\u8cea\u3001\u6838\u9178\u3001\u75be\u75c5\u548c\u7d30\u80de\u7cfb\u7684\u7d9c\u5408\u6578\u64da\u96c6\u4e0a\u9032\u884c\u5fae\u8abf\u7684\u3002\u5728 66 \u9805\u6cbb\u7642\u85e5\u7269\u958b\u767c\u4efb\u52d9\u4e2d\uff0cTxGemma \u5728 64 \u9805\u4efb\u52d9\u4e2d\u53d6\u5f97\u4e86\u512a\u65bc\u6216\u8207\u6700\u5148\u9032\u7684\u901a\u7528\u6a21\u578b\u76f8\u7576\u7684\u6027\u80fd\uff08\u5728 45 \u9805\u4efb\u52d9\u4e2d\u8868\u73fe\u66f4\u4f73\uff09\uff0c\u4e26\u4e14\u5728 50 \u9805\u4efb\u52d9\u4e2d\u512a\u65bc\u6216\u8207\u6700\u5148\u9032\u7684\u5c08\u696d\u6a21\u578b\u76f8\u7576\uff08\u5728 26 \u9805\u4efb\u52d9\u4e2d\u8868\u73fe\u66f4\u4f73\uff09\u3002\u5728\u6cbb\u7642\u6027\u4e0b\u6e38\u4efb\u52d9\uff08\u4f8b\u5982\u81e8\u5e8a\u8a66\u9a57\u4e0d\u826f\u4e8b\u4ef6\u9810\u6e2c\uff09\u4e0a\u5fae\u8abf TxGemma \u6a21\u578b\u6240\u9700\u7684\u8a13\u7df4\u6578\u64da\u6bd4\u5fae\u8abf\u57fa\u790e LLM \u5c11\uff0c\u9019\u4f7f\u5f97 TxGemma \u9069\u5408\u6578\u64da\u6709\u9650\u7684\u61c9\u7528\u3002\u9664\u4e86\u9019\u4e9b\u9810\u6e2c\u80fd\u529b\u4e4b\u5916\uff0cTxGemma \u9084\u5177\u6709\u5c0d\u8a71\u6a21\u578b\uff0c\u5f4c\u5408\u4e86\u901a\u7528 LLM \u548c\u5c08\u696d\u7279\u6027\u9810\u6e2c\u5668\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u9019\u4e9b\u6a21\u578b\u5141\u8a31\u79d1\u5b78\u5bb6\u4ee5\u81ea\u7136\u8a9e\u8a00\u9032\u884c\u4e92\u52d5\uff0c\u6839\u64da\u5206\u5b50\u7d50\u69cb\u70ba\u9810\u6e2c\u63d0\u4f9b\u6a5f\u7406\u89e3\u91cb\uff0c\u4e26\u53c3\u8207\u79d1\u5b78\u8a0e\u8ad6\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63a8\u51fa\u4e86 Agentic-Tx\uff0c\u9019\u662f\u4e00\u500b\u7531 Gemini 2.5 \u652f\u63f4\u7684\u901a\u7528\u6cbb\u7642\u4ee3\u7406\u7cfb\u7d71\uff0c\u53ef\u4ee5\u63a8\u7406\u3001\u884c\u52d5\u3001\u7ba1\u7406\u5404\u7a2e\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e26\u7372\u53d6\u5916\u90e8\u9818\u57df\u77e5\u8b58\u3002Agentic-Tx \u5728 Humanity's Last Exam \u57fa\u6e96\u6e2c\u8a66\uff08\u5316\u5b78\u8207\u751f\u7269\u5b78\uff09\u4e2d\u8d85\u8d8a\u4e86\u5148\u524d\u9818\u5148\u7684\u6a21\u578b\uff0c\u76f8\u8f03\u65bc o3-mini (high) \u6709 52.3% \u7684\u76f8\u5c0d\u6539\u9032\uff0c\u5728 GPQA\uff08\u5316\u5b78\uff09\u4e0a\u76f8\u8f03\u65bc o3-mini (high) \u6709 26.7% \u7684\u6539\u9032\uff0c\u4e26\u4e14\u5728 ChemBench-Preference \u4e0a\u6bd4 o3-mini (high) \u6539\u9032\u4e86 6.3%\uff0c\u5728 ChemBench-Mini \u4e0a\u6bd4 o3-mini (high) \u6539\u9032\u4e86 2.4%\u3002</paragraph>\n", "author": "Eric Wang et.al.", "authors": "Eric Wang, Samuel Schmidgall, Paul F. Jaeger, Fan Zhang, Rory Pilgrim, Yossi Matias, Joelle Barral, David Fleet, Shekoofeh Azizi", "id": "2504.06196v1", "paper_url": "http://arxiv.org/abs/2504.06196v1", "repo": "null"}}