{"2501.04982": {"publish_time": "2025-01-09", "title": "CuRLA: Curriculum Learning Based Deep Reinforcement Learning for Autonomous Driving", "paper_summary": "In autonomous driving, traditional Computer Vision (CV) agents often struggle\nin unfamiliar situations due to biases in the training data. Deep Reinforcement\nLearning (DRL) agents address this by learning from experience and maximizing\nrewards, which helps them adapt to dynamic environments. However, ensuring\ntheir generalization remains challenging, especially with static training\nenvironments. Additionally, DRL models lack transparency, making it difficult\nto guarantee safety in all scenarios, particularly those not seen during\ntraining. To tackle these issues, we propose a method that combines DRL with\nCurriculum Learning for autonomous driving. Our approach uses a Proximal Policy\nOptimization (PPO) agent and a Variational Autoencoder (VAE) to learn safe\ndriving in the CARLA simulator. The agent is trained using two-fold curriculum\nlearning, progressively increasing environment difficulty and incorporating a\ncollision penalty in the reward function to promote safety. This method\nimproves the agent's adaptability and reliability in complex environments, and\nunderstand the nuances of balancing multiple reward components from different\nfeedback signals in a single scalar reward function. Keywords: Computer Vision,\nDeep Reinforcement Learning, Variational Autoencoder, Proximal Policy\nOptimization, Curriculum Learning, Autonomous Driving.", "paper_summary_zh": "\u5728\u81ea\u52d5\u99d5\u99db\u4e2d\uff0c\u50b3\u7d71\u7684\u96fb\u8166\u8996\u89ba (CV) \u4ee3\u7406\u5728\u8a13\u7df4\u8cc7\u6599\u7684\u504f\u5dee\u4e0b\uff0c\u7d93\u5e38\u5728\u4e0d\u719f\u6089\u7684\u74b0\u5883\u4e2d\u6399\u624e\u3002\u6df1\u5ea6\u5f37\u5316\u5b78\u7fd2 (DRL) \u4ee3\u7406\u901a\u904e\u5f9e\u7d93\u9a57\u4e2d\u5b78\u7fd2\u548c\u6700\u5927\u5316\u734e\u52f5\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u9019\u6709\u52a9\u65bc\u5b83\u5011\u9069\u61c9\u52d5\u614b\u74b0\u5883\u3002\u7136\u800c\uff0c\u78ba\u4fdd\u5b83\u5011\u7684\u6cdb\u5316\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\uff0c\u7279\u5225\u662f\u5728\u975c\u614b\u8a13\u7df4\u74b0\u5883\u4e2d\u3002\u6b64\u5916\uff0cDRL \u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u9019\u4f7f\u5f97\u96e3\u4ee5\u4fdd\u8b49\u5728\u6240\u6709\u5834\u666f\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u7279\u5225\u662f\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u6c92\u6709\u898b\u904e\u7684\u5834\u666f\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5c07 DRL \u8207\u8ab2\u7a0b\u5b78\u7fd2\u76f8\u7d50\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u81ea\u52d5\u99d5\u99db\u3002\u6211\u5011\u7684\u505a\u6cd5\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u512a\u5316 (PPO) \u4ee3\u7406\u548c\u8b8a\u5206\u81ea\u52d5\u7de8\u78bc\u5668 (VAE) \u4f86\u5b78\u7fd2\u5728 CARLA \u6a21\u64ec\u5668\u4e2d\u5b89\u5168\u99d5\u99db\u3002\u8a72\u4ee3\u7406\u4f7f\u7528\u5169\u500d\u8ab2\u7a0b\u5b78\u7fd2\u9032\u884c\u8a13\u7df4\uff0c\u9010\u6b65\u589e\u52a0\u74b0\u5883\u96e3\u5ea6\uff0c\u4e26\u5728\u734e\u52f5\u51fd\u6578\u4e2d\u52a0\u5165\u78b0\u649e\u61f2\u7f70\u4ee5\u4fc3\u9032\u5b89\u5168\u6027\u3002\u9019\u7a2e\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4ee3\u7406\u5728\u8907\u96dc\u74b0\u5883\u4e2d\u7684\u9069\u61c9\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e26\u4e14\u4e86\u89e3\u4e86\u5728\u55ae\u500b\u6a19\u91cf\u734e\u52f5\u51fd\u6578\u4e2d\u5e73\u8861\u4f86\u81ea\u4e0d\u540c\u56de\u994b\u4fe1\u865f\u7684 multiple reward \u7d44\u4ef6\u7684\u7d30\u5fae\u5dee\u5225\u3002\u95dc\u9375\u5b57\uff1a\u96fb\u8166\u8996\u89ba\u3001\u6df1\u5ea6\u5f37\u5316\u5b78\u7fd2\u3001\u8b8a\u5206\u81ea\u52d5\u7de8\u78bc\u5668\u3001\u8fd1\u7aef\u7b56\u7565\u512a\u5316\u3001\u8ab2\u7a0b\u5b78\u7fd2\u3001\u81ea\u52d5\u99d5\u99db\u3002", "author": "Bhargava Uppuluri et.al.", "authors": "Bhargava Uppuluri, Anjel Patel, Neil Mehta, Sridhar Kamath, Pratyush Chakraborty", "id": "2501.04982v1", "paper_url": "http://arxiv.org/abs/2501.04982v1", "repo": "null"}}