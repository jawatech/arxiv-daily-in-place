{"2410.07771": {"publish_time": "2024-10-10", "title": "Full-Rank No More: Low-Rank Weight Training for Modern Speech Recognition Models", "paper_summary": "This paper investigates the under-explored area of low-rank weight training\nfor large-scale Conformer-based speech recognition models from scratch. Our\nstudy demonstrates the viability of this training paradigm for such models,\nyielding several notable findings. Firstly, we discover that applying a\nlow-rank structure exclusively to the attention modules can unexpectedly\nenhance performance, even with a significant rank reduction of 12%. In\ncontrast, feed-forward layers present greater challenges, as they begin to\nexhibit performance degradation with a moderate 50% rank reduction.\nFurthermore, we find that both initialization and layer-wise rank assignment\nplay critical roles in successful low-rank training. Specifically, employing\nSVD initialization and linear layer-wise rank mapping significantly boosts the\nefficacy of low-rank weight training. Building on these insights, we introduce\nthe Low-Rank Speech Model from Scratch (LR-SMS), an approach that achieves\nperformance parity with full-rank training while delivering substantial\nreductions in parameters count (by at least 2x), and training time speedups (by\n1.3x for ASR and 1.15x for AVSR).", "paper_summary_zh": "\u672c\u6587\u63a2\u8a0e\u4e86\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u5927\u578b\u57fa\u65bc Conformer \u7684\u8a9e\u97f3\u8b58\u5225\u6a21\u578b\u7684\u4f4e\u79e9\u6b0a\u91cd\u8a13\u7df4\u9019\u500b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u9818\u57df\u3002\u6211\u5011\u7684\u7814\u7a76\u8b49\u660e\u4e86\u9019\u7a2e\u8a13\u7df4\u7bc4\u4f8b\u5c0d\u6b64\u985e\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e26\u7522\u751f\u4e86\u5e7e\u9805\u503c\u5f97\u6ce8\u610f\u7684\u767c\u73fe\u3002\u9996\u5148\uff0c\u6211\u5011\u767c\u73fe\u50c5\u5c07\u4f4e\u79e9\u7d50\u69cb\u61c9\u7528\u65bc\u6ce8\u610f\u529b\u6a21\u7d44\u5c31\u80fd\u610f\u5916\u5730\u63d0\u5347\u6548\u80fd\uff0c\u5373\u4f7f\u79e9\u5927\u5e45\u964d\u4f4e\u4e86 12%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u524d\u994b\u5c64\u6703\u5e36\u4f86\u66f4\u5927\u7684\u6311\u6230\uff0c\u56e0\u70ba\u5b83\u5011\u6703\u5728\u79e9\u9069\u5ea6\u964d\u4f4e 50% \u7684\u60c5\u6cc1\u4e0b\u958b\u59cb\u8868\u73fe\u51fa\u6548\u80fd\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u521d\u59cb\u5316\u548c\u9010\u5c64\u79e9\u6307\u6d3e\u5728\u4f4e\u79e9\u8a13\u7df4\u7684\u6210\u529f\u4e2d\u90fd\u626e\u6f14\u8457\u95dc\u9375\u89d2\u8272\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u63a1\u7528 SVD \u521d\u59cb\u5316\u548c\u7dda\u6027\u9010\u5c64\u79e9\u5c0d\u61c9\u6703\u986f\u8457\u63d0\u5347\u4f4e\u79e9\u6b0a\u91cd\u8a13\u7df4\u7684\u6548\u80fd\u3002\u57fa\u65bc\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u7684\u4f4e\u79e9\u8a9e\u97f3\u6a21\u578b (LR-SMS)\uff0c\u9019\u662f\u4e00\u7a2e\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u53c3\u6578\u6578\u91cf\u5927\u5e45\u6e1b\u5c11\uff08\u81f3\u5c11 2 \u500d\uff09\u548c\u8a13\u7df4\u6642\u9593\u52a0\u5feb\uff08ASR \u70ba 1.3 \u500d\uff0cAVSR \u70ba 1.15 \u500d\uff09\u7684\u60c5\u6cc1\u4e0b\uff0c\u9054\u5230\u8207\u5168\u79e9\u8a13\u7df4\u76f8\u7576\u7684\u6548\u80fd\u3002", "author": "Adriana Fernandez-Lopez et.al.", "authors": "Adriana Fernandez-Lopez, Shiwei Liu, Lu Yin, Stavros Petridis, Maja Pantic", "id": "2410.07771v1", "paper_url": "http://arxiv.org/abs/2410.07771v1", "repo": "null"}}