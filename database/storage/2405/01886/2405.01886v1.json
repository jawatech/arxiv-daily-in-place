{"2405.01886": {"publish_time": "2024-05-03", "title": "Aloe: A Family of Fine-tuned Open Healthcare LLMs", "paper_summary": "As the capabilities of Large Language Models (LLMs) in healthcare and\nmedicine continue to advance, there is a growing need for competitive\nopen-source models that can safeguard public interest. With the increasing\navailability of highly competitive open base models, the impact of continued\npre-training is increasingly uncertain. In this work, we explore the role of\ninstruct tuning, model merging, alignment, red teaming and advanced inference\nschemes, as means to improve current open models. To that end, we introduce the\nAloe family, a set of open medical LLMs highly competitive within its scale\nrange. Aloe models are trained on the current best base models (Mistral, LLaMA\n3), using a new custom dataset which combines public data sources improved with\nsynthetic Chain of Thought (CoT). Aloe models undergo an alignment phase,\nbecoming one of the first few policy-aligned open healthcare LLM using Direct\nPreference Optimization, setting a new standard for ethical performance in\nhealthcare LLMs. Model evaluation expands to include various bias and toxicity\ndatasets, a dedicated red teaming effort, and a much-needed risk assessment for\nhealthcare LLMs. Finally, to explore the limits of current LLMs in inference,\nwe study several advanced prompt engineering strategies to boost performance\nacross benchmarks, yielding state-of-the-art results for open healthcare 7B\nLLMs, unprecedented at this scale.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u91ab\u7642\u4fdd\u5065\u548c\u91ab\u5b78\u9818\u57df\u7684\u80fd\u529b\u6301\u7e8c\u9032\u6b65\uff0c\u5c0d\u65bc\u80fd\u4fdd\u969c\u516c\u5171\u5229\u76ca\u7684\u7af6\u722d\u6027\u958b\u6e90\u6a21\u578b\u7684\u9700\u6c42\u4e5f\u8207\u65e5\u4ff1\u589e\u3002\u96a8\u8457\u6975\u5177\u7af6\u722d\u529b\u7684\u958b\u653e\u57fa\u790e\u6a21\u578b\u8d8a\u4f86\u8d8a\u666e\u53ca\uff0c\u6301\u7e8c\u9810\u8a13\u7df4\u7684\u5f71\u97ff\u8d8a\u4f86\u8d8a\u4e0d\u78ba\u5b9a\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u6307\u4ee4\u5fae\u8abf\u3001\u6a21\u578b\u5408\u4f75\u3001\u6821\u6e96\u3001\u7d05\u968a\u548c\u5148\u9032\u63a8\u8ad6\u67b6\u69cb\u5728\u6539\u5584\u7576\u524d\u958b\u653e\u6a21\u578b\u4e2d\u7684\u89d2\u8272\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63a8\u51fa\u4e86 Aloe \u7cfb\u5217\uff0c\u9019\u662f\u4e00\u7d44\u5728\u898f\u6a21\u7bc4\u570d\u5167\u6975\u5177\u7af6\u722d\u529b\u7684\u958b\u653e\u5f0f\u91ab\u7642 LLM\u3002Aloe \u6a21\u578b\u4f7f\u7528\u65b0\u7684\u81ea\u8a02\u8cc7\u6599\u96c6\u8a13\u7df4\u65bc\u7576\u524d\u6700\u4f73\u57fa\u790e\u6a21\u578b\uff08Mistral\u3001LLaMA 3\uff09\uff0c\u8a72\u8cc7\u6599\u96c6\u7d50\u5408\u4e86\u4f7f\u7528\u5408\u6210\u601d\u7dad\u93c8 (CoT) \u6539\u9032\u7684\u516c\u958b\u8cc7\u6599\u4f86\u6e90\u3002Aloe \u6a21\u578b\u6703\u7d93\u6b77\u4e00\u500b\u6821\u6e96\u968e\u6bb5\uff0c\u6210\u70ba\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\u7684\u7b2c\u4e00\u6279\u653f\u7b56\u6821\u6e96\u958b\u653e\u5f0f\u91ab\u7642\u4fdd\u5065 LLM \u4e4b\u4e00\uff0c\u70ba\u91ab\u7642\u4fdd\u5065 LLM \u7684\u9053\u5fb7\u8868\u73fe\u6a39\u7acb\u4e86\u65b0\u7684\u6a19\u6e96\u3002\u6a21\u578b\u8a55\u4f30\u64f4\u5c55\u5230\u5305\u62ec\u5404\u7a2e\u504f\u898b\u548c\u6bd2\u6027\u8cc7\u6599\u96c6\u3001\u5c08\u9580\u7684\u7d05\u968a\u5de5\u4f5c\uff0c\u4ee5\u53ca\u91ab\u7642\u4fdd\u5065 LLM \u6025\u9700\u7684\u98a8\u96aa\u8a55\u4f30\u3002\u6700\u5f8c\uff0c\u70ba\u4e86\u63a2\u7d22\u7576\u524d LLM \u5728\u63a8\u8ad6\u4e2d\u7684\u6975\u9650\uff0c\u6211\u5011\u7814\u7a76\u4e86\u5e7e\u7a2e\u5148\u9032\u7684\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u57fa\u6e96\u6e2c\u8a66\u7684\u6548\u80fd\uff0c\u7522\u751f\u958b\u653e\u5f0f\u91ab\u7642\u4fdd\u5065 7B LLM \u7684\u6700\u65b0\u7d50\u679c\uff0c\u5728\u9019\u500b\u898f\u6a21\u4e2d\u524d\u6240\u672a\u898b\u3002", "author": "Ashwin Kumar Gururajan et.al.", "authors": "Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Lucia Urcelay-Ganzabal, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguad\u00e9-Parra, Ulises Cort\u00e9s Dario Garcia-Gasulla", "id": "2405.01886v1", "paper_url": "http://arxiv.org/abs/2405.01886v1", "repo": "null"}}