{"2405.03146": {"publish_time": "2024-05-06", "title": "Quantifying the Capabilities of LLMs across Scale and Precision", "paper_summary": "Scale is often attributed as one of the factors that cause an increase in the\nperformance of LLMs, resulting in models with billion and trillion parameters.\nOne of the limitations of such large models is the high computational\nrequirements that limit their usage, deployment, and debugging in\nresource-constrained scenarios. Two commonly used alternatives to bypass these\nlimitations are to use the smaller versions of LLMs (e.g. Llama 7B instead of\nLlama 70B) and lower the memory requirements by using quantization. While these\napproaches effectively address the limitation of resources, their impact on\nmodel performance needs thorough examination. In this study, we perform a\ncomprehensive evaluation to investigate the effect of model scale and\nquantization on the performance. We experiment with two major families of\nopen-source instruct models ranging from 7 billion to 70 billion parameters.\nOur extensive zero-shot experiments across various tasks including natural\nlanguage understanding, reasoning, misinformation detection, and hallucination\nreveal that larger models generally outperform their smaller counterparts,\nsuggesting that scale remains an important factor in enhancing performance. We\nfound that larger models show exceptional resilience to precision reduction and\ncan maintain high accuracy even at 4-bit quantization for numerous tasks and\nthey serve as a better solution than using smaller models at high precision\nunder similar memory requirements.", "paper_summary_zh": "", "author": "Sher Badshah et.al.", "authors": "Sher Badshah,Hassan Sajjad", "id": "2405.03146v1", "paper_url": "http://arxiv.org/abs/2405.03146v1", "repo": "null"}}