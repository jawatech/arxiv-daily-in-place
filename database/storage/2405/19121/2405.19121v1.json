{"2405.19121": {"publish_time": "2024-05-29", "title": "Spatio-Spectral Graph Neural Networks", "paper_summary": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for\nlearning on graph-structured data. However, key limitations of l-step MPGNNs\nare that their \"receptive field\" is typically limited to the l-hop neighborhood\nof a node and that information exchange between distant nodes is limited by\nover-squashing. Motivated by these limitations, we propose Spatio-Spectral\nGraph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural\nNetworks (GNNs) that synergistically combines spatially and spectrally\nparametrized graph filters. Parameterizing filters partially in the frequency\ndomain enables global yet efficient information propagation. We show that\nS$^2$GNNs vanquish over-squashing and yield strictly tighter\napproximation-theoretic error bounds than MPGNNs. Further, rethinking graph\nconvolutions at a fundamental level unlocks new design spaces. For example,\nS$^2$GNNs allow for free positional encodings that make them strictly more\nexpressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain\ngeneral-purpose S$^2$GNNs, we propose spectrally parametrized filters for\ndirected graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and\ngraph rewirings, e.g., on the peptide long-range benchmark tasks, and are\ncompetitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs\nscale to millions of nodes.", "paper_summary_zh": "\u7a7a\u9593\u8a0a\u606f\u50b3\u905e\u5716\u795e\u7d93\u7db2\u8def (MPGNN) \u5ee3\u6cdb\u7528\u65bc\u5b78\u7fd2\u5716\u5f62\u7d50\u69cb\u8cc7\u6599\u3002\u7136\u800c\uff0cl \u6b65 MPGNN \u7684\u4e3b\u8981\u9650\u5236\u5728\u65bc\u5176\u300c\u611f\u53d7\u91ce\u300d\u901a\u5e38\u50c5\u9650\u65bc\u7bc0\u9ede\u7684 l \u8df3\u9130\u57df\uff0c\u800c\u9060\u8ddd\u96e2\u7bc0\u9ede\u4e4b\u9593\u7684\u8cc7\u8a0a\u4ea4\u63db\u53d7\u5230\u904e\u5ea6\u58d3\u7e2e\u7684\u9650\u5236\u3002\u57fa\u65bc\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u6642\u7a7a\u8b5c\u5716\u795e\u7d93\u7db2\u8def (S$^2$GNN) -- \u4e00\u7a2e\u5716\u795e\u7d93\u7db2\u8def (GNN) \u7684\u65b0\u5efa\u6a21\u7bc4\u5f0f\uff0c\u5b83\u5354\u540c\u7d50\u5408\u7a7a\u9593\u548c\u983b\u8b5c\u53c3\u6578\u5316\u7684\u5716\u5f62\u6ffe\u6ce2\u5668\u3002\u5728\u983b\u7387\u57df\u4e2d\u90e8\u5206\u53c3\u6578\u5316\u6ffe\u6ce2\u5668\u53ef\u5be6\u73fe\u5168\u5c40\u4e14\u6709\u6548\u7684\u8cc7\u8a0a\u50b3\u64ad\u3002\u6211\u5011\u8b49\u660e S$^2$GNN \u6d88\u9664\u904e\u5ea6\u58d3\u7e2e\uff0c\u4e26\u7522\u751f\u6bd4 MPGNN \u66f4\u56b4\u683c\u7684\u8fd1\u4f3c\u7406\u8ad6\u8aa4\u5dee\u754c\u9650\u3002\u6b64\u5916\uff0c\u5728\u57fa\u672c\u5c64\u9762\u4e0a\u91cd\u65b0\u601d\u8003\u5716\u5f62\u5377\u7a4d\uff0c\u958b\u555f\u4e86\u65b0\u7684\u8a2d\u8a08\u7a7a\u9593\u3002\u4f8b\u5982\uff0cS$^2$GNN \u5141\u8a31\u81ea\u7531\u4f4d\u7f6e\u7de8\u78bc\uff0c\u4f7f\u5176\u6bd4 1-Weisfeiler-Lehman (WL) \u6e2c\u8a66\u66f4\u5177\u8868\u9054\u529b\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u7372\u5f97\u901a\u7528 S$^2$GNN\uff0c\u6211\u5011\u63d0\u51fa\u91dd\u5c0d\u6709\u5411\u5716\u7684\u983b\u8b5c\u53c3\u6578\u5316\u6ffe\u6ce2\u5668\u3002S$^2$GNN \u512a\u65bc\u7a7a\u9593 MPGNN\u3001\u5716\u5f62Transformer\u548c\u5716\u5f62\u91cd\u65b0\u9023\u7dda\uff0c\u4f8b\u5982\u5728\u80dc\u80bd\u9577\u7a0b\u57fa\u6e96\u4efb\u52d9\u4e0a\uff0c\u4e26\u4e14\u8207\u6700\u5148\u9032\u7684\u5e8f\u5217\u5efa\u6a21\u5177\u6709\u7af6\u722d\u529b\u3002\u5728 40 GB GPU \u4e0a\uff0cS$^2$GNN \u53ef\u64f4\u5145\u81f3\u6578\u767e\u842c\u500b\u7bc0\u9ede\u3002", "author": "Simon Geisler et.al.", "authors": "Simon Geisler, Arthur Kosmala, Daniel Herbst, Stephan G\u00fcnnemann", "id": "2405.19121v1", "paper_url": "http://arxiv.org/abs/2405.19121v1", "repo": "null"}}