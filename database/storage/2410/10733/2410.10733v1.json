{"2410.10733": {"publish_time": "2024-10-14", "title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models", "paper_summary": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa\u6df1\u5ea6\u58d3\u7e2e\u81ea\u7de8\u78bc\u5668 (DC-AE)\uff0c\u4e00\u7a2e\u7528\u65bc\u52a0\u901f\u9ad8\u89e3\u6790\u5ea6\u64f4\u6563\u6a21\u578b\u7684\u81ea\u7de8\u78bc\u5668\u6a21\u578b\u7684\u65b0\u7cfb\u5217\u3002\u73fe\u6709\u7684\u81ea\u7de8\u78bc\u5668\u6a21\u578b\u5728\u4e2d\u7b49\u7a7a\u9593\u58d3\u7e2e\u6bd4 (\u4f8b\u5982 8 \u500d) \u4e0b\u5df2\u5c55\u73fe\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7d50\u679c\uff0c\u4f46\u7121\u6cd5\u7dad\u6301\u9ad8\u7a7a\u9593\u58d3\u7e2e\u6bd4 (\u4f8b\u5982 64 \u500d) \u7684\u6eff\u610f\u91cd\u5efa\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u900f\u904e\u5f15\u5165\u5169\u9805\u95dc\u9375\u6280\u8853\u4f86\u89e3\u6c7a\u6b64\u6311\u6230\uff1a(1) \u6b98\u5dee\u81ea\u7de8\u78bc\uff0c\u6211\u5011\u8a2d\u8a08\u6211\u5011\u7684\u6a21\u578b\u4ee5\u5b78\u7fd2\u57fa\u65bc\u7a7a\u9593\u5230\u901a\u9053\u8f49\u63db\u7279\u5fb5\u7684\u6b98\u5dee\uff0c\u4ee5\u6e1b\u8f15\u9ad8\u7a7a\u9593\u58d3\u7e2e\u81ea\u7de8\u78bc\u5668\u7684\u6700\u4f73\u5316\u96e3\u5ea6\uff1b(2) \u89e3\u8026\u7684\u9ad8\u89e3\u6790\u5ea6\u9069\u61c9\uff0c\u4e00\u7a2e\u7528\u65bc\u6e1b\u8f15\u9ad8\u7a7a\u9593\u58d3\u7e2e\u81ea\u7de8\u78bc\u5668\u7684\u6cdb\u5316\u61f2\u7f70\u7684\u6709\u6548\u89e3\u8026\u4e09\u968e\u6bb5\u8a13\u7df4\u7b56\u7565\u3002\u900f\u904e\u9019\u4e9b\u8a2d\u8a08\uff0c\u6211\u5011\u5c07\u81ea\u7de8\u78bc\u5668\u7684\u7a7a\u9593\u58d3\u7e2e\u6bd4\u63d0\u5347\u81f3 128\uff0c\u540c\u6642\u7dad\u6301\u91cd\u5efa\u54c1\u8cea\u3002\u5c07\u6211\u5011\u7684 DC-AE \u5957\u7528\u65bc\u6f5b\u5728\u64f4\u6563\u6a21\u578b\uff0c\u6211\u5011\u5728\u4e0d\u964d\u4f4e\u6e96\u78ba\u5ea6\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u986f\u8457\u7684\u52a0\u901f\u3002\u4f8b\u5982\uff0c\u5728 ImageNet 512x512 \u4e0a\uff0c\u6211\u5011\u7684 DC-AE \u5728 H100 GPU \u4e0a\u70ba UViT-H \u63d0\u4f9b 19.1 \u500d\u7684\u63a8\u8ad6\u52a0\u901f\u548c 17.9 \u500d\u7684\u8a13\u7df4\u52a0\u901f\uff0c\u540c\u6642\u5be6\u73fe\u6bd4\u5ee3\u6cdb\u4f7f\u7528\u7684 SD-VAE-f8 \u81ea\u7de8\u78bc\u5668\u66f4\u597d\u7684 FID\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/mit-han-lab/efficientvit \u53d6\u5f97\u3002</paragraph>", "author": "Junyu Chen et.al.", "authors": "Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han", "id": "2410.10733v1", "paper_url": "http://arxiv.org/abs/2410.10733v1", "repo": "https://github.com/mit-han-lab/efficientvit"}}