{"2502.02945": {"publish_time": "2025-02-05", "title": "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a Plug-and-Play Instruction", "paper_summary": "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines.", "paper_summary_zh": "\u77e5\u8b58\u8ffd\u8e64 (KT) \u554f\u984c\u662f\u500b\u4eba\u5316\u6559\u80b2\u4e2d\u4e00\u500b\u975e\u5e38\u91cd\u8981\u7684\u4e3b\u984c\uff0c\u5176\u76ee\u6a19\u662f\u6839\u64da\u5b78\u751f\u7684\u904e\u53bb\u554f\u984c\u56de\u7b54\u8a18\u9304\u9810\u6e2c\u5b78\u751f\u662f\u5426\u80fd\u6b63\u78ba\u56de\u7b54\u4e0b\u4e00\u500b\u554f\u984c\u3002\u5148\u524d\u91dd\u5c0d\u6b64\u4efb\u52d9\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u65bc\u6839\u64da ID \u6216\u6587\u5b57\u8cc7\u8a0a\u5b78\u7fd2\u884c\u70ba\u5e8f\u5217\u3002\u7136\u800c\uff0c\u9019\u4e9b\u7814\u7a76\u901a\u5e38\u7121\u6cd5\u5728\u4e0d\u63a8\u8ad6\u51fa\u95dc\u65bc\u554f\u984c\u7684\u8c50\u5bcc\u4e16\u754c\u77e5\u8b58\u7684\u60c5\u6cc1\u4e0b\u6355\u6349\u5230\u5b78\u751f\u7684\u8db3\u5920\u884c\u70ba\u6a21\u5f0f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684 KT \u6846\u67b6\uff0c\u7a31\u70ba \\texttt{\\textbf{LLM-KT}}\uff0c\u4ee5\u6574\u5408 LLM \u548c\u50b3\u7d71\u5e8f\u5217\u4e92\u52d5\u6a21\u578b\u7684\u512a\u52e2\u3002\u5c0d\u65bc\u4efb\u52d9\u5c64\u7d1a\u5c0d\u9f4a\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u5373\u63d2\u5373\u7528\u7684\u6307\u4ee4\uff0c\u4ee5\u5c07 LLM \u8207 KT \u5c0d\u9f4a\uff0c\u5229\u7528 LLM \u8c50\u5bcc\u7684\u77e5\u8b58\u548c\u5f37\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002\u5c0d\u65bc\u6a21\u614b\u5c64\u7d1a\u5c0d\u9f4a\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u5916\u639b\u5f0f\u5167\u5bb9\u548c\u5e8f\u5217\uff0c\u4ee5\u6574\u5408\u50b3\u7d71\u65b9\u6cd5\u5b78\u7fd2\u5230\u7684\u591a\u7a2e\u6a21\u614b\u3002\u70ba\u4e86\u6355\u6349\u6b77\u53f2\u8a18\u9304\u7684\u9577\u5167\u5bb9\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5916\u639b\u5f0f\u5167\u5bb9\uff0c\u4ee5\u4f7f\u7528\u7279\u5b9a\u65bc\u554f\u984c\u548c\u7279\u5b9a\u65bc\u6982\u5ff5\u7684\u6a19\u8a18\u5f48\u6027\u5730\u5c07\u58d3\u7e2e\u5167\u5bb9\u5d4c\u5165\u63d2\u5165 LLM \u4e2d\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5916\u639b\u5f0f\u5e8f\u5217\uff0c\u4ee5\u4f7f\u7528\u5e8f\u5217\u9069\u914d\u5668\u589e\u5f37 LLM \u7684\u5e8f\u5217\u4e92\u52d5\u884c\u70ba\u8868\u793a\uff0c\u8a72\u8868\u793a\u662f\u7531\u50b3\u7d71\u5e8f\u5217\u6a21\u578b\u5b78\u7fd2\u7684\u3002\u5927\u91cf\u5be6\u9a57\u8868\u660e\uff0c\u8207\u5927\u7d04 20 \u500b\u5f37\u5927\u7684\u57fa\u7dda\u9032\u884c\u6bd4\u8f03\u6642\uff0c\\texttt{\\textbf{LLM-KT}} \u5728\u56db\u500b\u5178\u578b\u6578\u64da\u96c6\u4e0a\u7372\u5f97\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002", "author": "Ziwei Wang et.al.", "authors": "Ziwei Wang, Jie Zhou, Qin Chen, Min Zhang, Bo Jiang, Aimin Zhou, Qinchun Bai, Liang He", "id": "2502.02945v1", "paper_url": "http://arxiv.org/abs/2502.02945v1", "repo": "null"}}