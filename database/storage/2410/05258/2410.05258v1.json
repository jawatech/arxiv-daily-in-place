{"2410.05258": {"publish_time": "2024-10-07", "title": "Differential Transformer", "paper_summary": "Transformer tends to overallocate attention to irrelevant context. In this\nwork, we introduce Diff Transformer, which amplifies attention to the relevant\ncontext while canceling noise. Specifically, the differential attention\nmechanism calculates attention scores as the difference between two separate\nsoftmax attention maps. The subtraction cancels noise, promoting the emergence\nof sparse attention patterns. Experimental results on language modeling show\nthat Diff Transformer outperforms Transformer in various settings of scaling up\nmodel size and training tokens. More intriguingly, it offers notable advantages\nin practical applications, such as long-context modeling, key information\nretrieval, hallucination mitigation, in-context learning, and reduction of\nactivation outliers. By being less distracted by irrelevant context, Diff\nTransformer can mitigate hallucination in question answering and text\nsummarization. For in-context learning, Diff Transformer not only enhances\naccuracy but is also more robust to order permutation, which was considered as\na chronic robustness issue. The results position Diff Transformer as a highly\neffective and promising architecture to advance large language models.", "paper_summary_zh": "Transformer \u50be\u5411\u65bc\u904e\u5ea6\u5c07\u6ce8\u610f\u529b\u5206\u914d\u7d66\u7121\u95dc\u7684\u5167\u5bb9\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 Diff Transformer\uff0c\u5b83\u6703\u653e\u5927\u5c0d\u76f8\u95dc\u5167\u5bb9\u7684\u6ce8\u610f\u529b\uff0c\u540c\u6642\u6d88\u9664\u96dc\u8a0a\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5dee\u5206\u6ce8\u610f\u529b\u6a5f\u5236\u5c07\u6ce8\u610f\u529b\u5206\u6578\u8a08\u7b97\u70ba\u5169\u500b\u7368\u7acb softmax \u6ce8\u610f\u529b\u5716\u4e4b\u9593\u7684\u5dee\u7570\u3002\u6e1b\u6cd5\u6d88\u9664\u4e86\u96dc\u8a0a\uff0c\u4fc3\u8fdb\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u51fa\u73fe\u3002\u8a9e\u8a00\u5efa\u6a21\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cDiff Transformer \u5728\u64f4\u5c55\u6a21\u578b\u5927\u5c0f\u548c\u8a13\u7df4\u6a19\u8a18\u7684\u5404\u7a2e\u8a2d\u5b9a\u4e2d\u90fd\u512a\u65bc Transformer\u3002\u66f4\u6709\u8da3\u7684\u662f\uff0c\u5b83\u5728\u5be6\u7528\u61c9\u7528\u4e2d\u63d0\u4f9b\u4e86\u986f\u8457\u7684\u512a\u52e2\uff0c\u4f8b\u5982\u9577\u5167\u5bb9\u5efa\u6a21\u3001\u95dc\u9375\u8cc7\u8a0a\u6aa2\u7d22\u3001\u5e7b\u89ba\u6e1b\u7de9\u3001\u60c5\u5883\u5b78\u7fd2\u548c\u6e1b\u5c11\u6fc0\u6d3b\u7570\u5e38\u503c\u3002\u900f\u904e\u6e1b\u5c11\u5c0d\u7121\u95dc\u5167\u5bb9\u7684\u5e72\u64fe\uff0cDiff Transformer \u53ef\u4ee5\u6e1b\u8f15\u554f\u7b54\u548c\u6587\u5b57\u6458\u8981\u4e2d\u7684\u5e7b\u89ba\u3002\u5c0d\u65bc\u60c5\u5883\u5b78\u7fd2\uff0cDiff Transformer \u4e0d\u50c5\u63d0\u9ad8\u4e86\u6e96\u78ba\u6027\uff0c\u800c\u4e14\u5c0d\u9806\u5e8f\u6392\u5217\u4e5f\u66f4\u7a69\u5065\uff0c\u9019\u88ab\u8a8d\u70ba\u662f\u4e00\u500b\u9577\u671f\u7684\u7a69\u5065\u6027\u554f\u984c\u3002\u9019\u4e9b\u7d50\u679c\u5c07 Diff Transformer \u5b9a\u4f4d\u70ba\u4e00\u7a2e\u9ad8\u6548\u4e14\u6709\u524d\u9014\u7684\u67b6\u69cb\uff0c\u4ee5\u63a8\u9032\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u3002", "author": "Tianzhu Ye et.al.", "authors": "Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei", "id": "2410.05258v1", "paper_url": "http://arxiv.org/abs/2410.05258v1", "repo": "null"}}