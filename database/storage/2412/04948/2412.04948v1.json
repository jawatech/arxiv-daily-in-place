{"2412.04948": {"publish_time": "2024-12-06", "title": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning", "paper_summary": "Autoregressive large language models (LLMs) pre-trained by next token\nprediction are inherently proficient in generative tasks. However, their\nperformance on knowledge-driven tasks such as factual knowledge querying\nremains unsatisfactory. Knowledge graphs (KGs), as high-quality structured\nknowledge bases, can provide reliable knowledge for LLMs, potentially\ncompensating for their knowledge deficiencies. Aligning LLMs with explicit,\nstructured knowledge from KGs has been a challenge; previous attempts either\nfailed to effectively align knowledge representations or compromised the\ngenerative capabilities of LLMs, leading to less-than-optimal outcomes. This\npaper proposes \\textbf{KaLM}, a \\textit{Knowledge-aligned Language Modeling}\napproach, which fine-tunes autoregressive LLMs to align with KG knowledge via\nthe joint objective of explicit knowledge alignment and implicit knowledge\nalignment. The explicit knowledge alignment objective aims to directly optimize\nthe knowledge representation of LLMs through dual-view knowledge graph\ncontrastive learning. The implicit knowledge alignment objective focuses on\nincorporating textual patterns of knowledge into LLMs through triple completion\nlanguage modeling. Notably, our method achieves a significant performance boost\nin evaluations of knowledge-driven tasks, specifically embedding-based\nknowledge graph completion and generation-based knowledge graph question\nanswering.", "paper_summary_zh": "<paragraph>\u81ea\u52d5\u56de\u6b78\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7d93\u7531\u4e0b\u4e00\u500b\u7b26\u865f\u9810\u6e2c\u9810\u5148\u8a13\u7df4\uff0c\u672c\u8cea\u4e0a\u64c5\u9577\u751f\u6210\u5f0f\u4efb\u52d9\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u77e5\u8b58\u9a45\u52d5\u4efb\u52d9\uff08\u4f8b\u5982\u4e8b\u5be6\u77e5\u8b58\u67e5\u8a62\uff09\u4e0a\u7684\u8868\u73fe\u4ecd\u4e0d\u76e1\u4eba\u610f\u3002\u77e5\u8b58\u5716\u8b5c (KG) \u4f5c\u70ba\u9ad8\u54c1\u8cea\u7684\u7d50\u69cb\u5316\u77e5\u8b58\u5eab\uff0c\u53ef\u4ee5\u70ba LLM \u63d0\u4f9b\u53ef\u9760\u7684\u77e5\u8b58\uff0c\u6f5b\u5728\u5730\u5f4c\u88dc\u5176\u77e5\u8b58\u4e0d\u8db3\u3002\u5c07 LLM \u8207\u4f86\u81ea KG \u7684\u660e\u78ba\u7d50\u69cb\u5316\u77e5\u8b58\u5c0d\u9f4a\u4e00\u76f4\u662f\u4e00\u9805\u6311\u6230\uff1b\u5148\u524d\u7684\u5617\u8a66\u8981\u4e48\u7121\u6cd5\u6709\u6548\u5c0d\u9f4a\u77e5\u8b58\u8868\u793a\uff0c\u8981\u4e48\u640d\u5bb3 LLM \u7684\u751f\u6210\u80fd\u529b\uff0c\u5c0e\u81f4\u7d50\u679c\u4e0d\u76e1\u7406\u60f3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u500b**KaLM**\uff0c\u4e00\u7a2e**\u77e5\u8b58\u5c0d\u9f4a\u8a9e\u8a00\u5efa\u6a21**\u65b9\u6cd5\uff0c\u5b83\u5fae\u8abf\u81ea\u52d5\u56de\u6b78 LLM \u4ee5\u900f\u904e\u660e\u78ba\u77e5\u8b58\u5c0d\u9f4a\u548c\u96b1\u5f0f\u77e5\u8b58\u5c0d\u9f4a\u7684\u806f\u5408\u76ee\u6a19\u8207 KG \u77e5\u8b58\u5c0d\u9f4a\u3002\u660e\u78ba\u77e5\u8b58\u5c0d\u9f4a\u76ee\u6a19\u65e8\u5728\u900f\u904e\u96d9\u8996\u5716\u77e5\u8b58\u5716\u8b5c\u5c0d\u6bd4\u5b78\u7fd2\u76f4\u63a5\u6700\u4f73\u5316 LLM \u7684\u77e5\u8b58\u8868\u793a\u3002\u96b1\u5f0f\u77e5\u8b58\u5c0d\u9f4a\u76ee\u6a19\u5c08\u6ce8\u65bc\u900f\u904e\u4e09\u5143\u7d44\u5b8c\u6210\u8a9e\u8a00\u5efa\u6a21\u5c07\u77e5\u8b58\u7684\u6587\u5b57\u6a21\u5f0f\u7d0d\u5165 LLM\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u77e5\u8b58\u9a45\u52d5\u4efb\u52d9\u7684\u8a55\u4f30\u4e2d\u7372\u5f97\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u7279\u5225\u662f\u57fa\u65bc\u5d4c\u5165\u7684\u77e5\u8b58\u5716\u8b5c\u5b8c\u6210\u548c\u57fa\u65bc\u751f\u6210\u7684\u77e5\u8b58\u5716\u8b5c\u554f\u984c\u89e3\u7b54\u3002</paragraph>", "author": "Peng Yu et.al.", "authors": "Peng Yu, Cheng Deng, Beiya Dai, Xinbing Wang, Ying Wen", "id": "2412.04948v1", "paper_url": "http://arxiv.org/abs/2412.04948v1", "repo": "null"}}