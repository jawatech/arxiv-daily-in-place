{"2408.10174": {"publish_time": "2024-08-19", "title": "SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models", "paper_summary": "Deep model training on extensive datasets is increasingly becoming\ncost-prohibitive, prompting the widespread adoption of deep model fusion\ntechniques to leverage knowledge from pre-existing models. From simple weight\naveraging to more sophisticated methods like AdaMerging, model fusion\neffectively improves model performance and accelerates the development of new\nmodels. However, potential interference between parameters of individual models\nand the lack of interpretability in the fusion progress remain significant\nchallenges. Existing methods often try to resolve the parameter interference\nissue by evaluating attributes of parameters, such as their magnitude or sign,\nor by parameter pruning. In this study, we begin by examining the fine-tuning\nof linear layers through the lens of subspace analysis and explicitly define\nparameter interference as an optimization problem to shed light on this\nsubject. Subsequently, we introduce an innovative approach to model fusion\ncalled zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which\nallows for the upscaling of source models into an MoE model without extra data\nor further training. Our approach relies on the observation that fine-tuning\nmostly keeps the important parts from the pre-training, but it uses less\nsignificant or unused areas to adapt to new tasks. Also, the issue of parameter\ninterference, which is intrinsically intractable in the original parameter\nspace, can be managed by expanding the dimensions. We conduct extensive\nexperiments across diverse scenarios, such as image classification and text\ngeneralization tasks, using full fine-tuning and LoRA fine-tuning, and we apply\nour method to large language models (CLIP models, Flan-T5 models, and\nMistral-7B models), highlighting the adaptability and scalability of SMILE.\nCode is available at https://github.com/tanganke/fusion_bench", "paper_summary_zh": "<paragraph>\u5728\u6d77\u91cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6df1\u5ea6\u6a21\u578b\u8bad\u7ec3\u7684\u6210\u672c\u8d8a\u6765\u8d8a\u9ad8\uff0c\u4fc3\u4f7f\u6df1\u5ea6\u6a21\u578b\u878d\u5408\u6280\u672f\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4ee5\u5229\u7528\u73b0\u6709\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u3002\u4ece\u7b80\u5355\u7684\u6743\u91cd\u5e73\u5747\u5230\u66f4\u590d\u6742\u7684\u65b9\u6cd5\uff08\u5982 AdaMerging\uff09\uff0c\u6a21\u578b\u878d\u5408\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u52a0\u901f\u4e86\u65b0\u6a21\u578b\u7684\u5f00\u53d1\u3002\u7136\u800c\uff0c\u5404\u4e2a\u6a21\u578b\u53c2\u6570\u4e4b\u95f4\u7684\u6f5c\u5728\u5e72\u6270\u548c\u878d\u5408\u8fc7\u7a0b\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u7f3a\u4e4f\u4ecd\u7136\u662f\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u8bd5\u56fe\u901a\u8fc7\u8bc4\u4f30\u53c2\u6570\u7684\u5c5e\u6027\uff08\u4f8b\u5982\u5b83\u4eec\u7684\u5e45\u5ea6\u6216\u7b26\u53f7\uff09\u6216\u901a\u8fc7\u53c2\u6570\u526a\u679d\u6765\u89e3\u51b3\u53c2\u6570\u5e72\u6270\u95ee\u9898\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5b50\u7a7a\u95f4\u5206\u6790\u7684\u89c6\u89d2\u6765\u68c0\u67e5\u7ebf\u6027\u5c42\u7684\u5fae\u8c03\uff0c\u5e76\u660e\u786e\u5730\u5c06\u53c2\u6570\u5e72\u6270\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u9610\u660e\u8fd9\u4e00\u4e3b\u9898\u3002\u968f\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u79f0\u4e3a\u96f6\u6837\u672c\u7a00\u758f\u4f4e\u79e9\u4e13\u5bb6\u6df7\u5408\uff08SMILE\uff09\u6784\u5efa\uff0c\u5b83\u5141\u8bb8\u5c06\u6e90\u6a21\u578b\u63d0\u5347\u5230 MoE \u6a21\u578b\u4e2d\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u6570\u636e\u6216\u8fdb\u4e00\u6b65\u7684\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8fd9\u6837\u4e00\u4e2a\u89c2\u5bdf\uff1a\u5fae\u8c03\u4e3b\u8981\u4fdd\u7559\u4e86\u9884\u8bad\u7ec3\u4e2d\u7684\u91cd\u8981\u90e8\u5206\uff0c\u4f46\u5b83\u4f7f\u7528\u4e0d\u592a\u91cd\u8981\u6216\u672a\u4f7f\u7528\u7684\u533a\u57df\u6765\u9002\u5e94\u65b0\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u53c2\u6570\u5e72\u6270\u95ee\u9898\u5728\u539f\u59cb\u53c2\u6570\u7a7a\u95f4\u4e2d\u672c\u8d28\u4e0a\u662f\u96be\u4ee5\u5904\u7406\u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u6269\u5c55\u7ef4\u5ea6\u6765\u7ba1\u7406\u3002\u6211\u4eec\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u6587\u672c\u6cdb\u5316\u4efb\u52a1\u7b49\u4e0d\u540c\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4f7f\u7528\u5b8c\u5168\u5fae\u8c03\u548c LoRA \u5fae\u8c03\uff0c\u5e76\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08CLIP \u6a21\u578b\u3001Flan-T5 \u6a21\u578b\u548c Mistral-7B \u6a21\u578b\uff09\uff0c\u7a81\u51fa\u4e86 SMILE \u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/tanganke/fusion_bench \u83b7\u5f97</paragraph>", "author": "Anke Tang et.al.", "authors": "Anke Tang, Li Shen, Yong Luo, Shuai Xie, Han Hu, Lefei Zhang, Bo Du, Dacheng Tao", "id": "2408.10174v1", "paper_url": "http://arxiv.org/abs/2408.10174v1", "repo": "https://github.com/tanganke/fusion_bench"}}