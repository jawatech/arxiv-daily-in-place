{"2410.02694": {"publish_time": "2024-10-03", "title": "HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly", "paper_summary": "There have been many benchmarks for evaluating long-context language models\n(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack\n(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate\nto the diverse downstream applications of LCLMs, and the inconsistency further\ncomplicates model comparison. We investigate the underlying reasons behind\ncurrent practices and find that existing benchmarks often provide noisy signals\ndue to low coverage of applications, insufficient lengths, unreliable metrics,\nand incompatibility with base models. In this work, we present HELMET (How to\nEvaluate Long-context Models Effectively and Thoroughly), a comprehensive\nbenchmark encompassing seven diverse, application-centric categories. We also\naddress many issues in previous benchmarks by adding controllable lengths up to\n128k tokens, model-based evaluation for reliable metrics, and few-shot\nprompting for robustly evaluating base models. Consequently, we demonstrate\nthat HELMET offers more reliable and consistent rankings of frontier LCLMs.\nThrough a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks\nlike NIAH are not good predictors of downstream performance; (2) the diverse\ncategories in HELMET exhibit distinct trends and low correlation with each\nother; and (3) while most LCLMs achieve perfect NIAH scores, open-source models\nsignificantly lag behind closed ones when the task requires full-context\nreasoning or following complex instructions -- the gap widens with increased\nlengths. Finally, we recommend using our RAG tasks for fast model development,\nas they are easy to run and more predictive of other downstream performance;\nultimately, we advocate for a holistic evaluation across diverse tasks.", "paper_summary_zh": "<paragraph>\u5df2\u7d93\u6709\u8a31\u591a\u7528\u65bc\u8a55\u4f30\u9577\u8a9e\u5883\u8a9e\u8a00\u6a21\u578b (LCLM) \u7684\u57fa\u6e96\uff0c\u4f46\u958b\u767c\u4eba\u54e1\u7d93\u5e38\u4f9d\u8cf4\u65bc\u5408\u6210\u4efb\u52d9\uff0c\u4f8b\u5982\u5927\u6d77\u6488\u91dd (NIAH) \u6216\u4efb\u52d9\u7684\u4efb\u610f\u5b50\u96c6\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5b83\u5011\u662f\u5426\u8f49\u63db\u70ba LCLM \u7684\u5404\u7a2e\u4e0b\u6e38\u61c9\u7528\uff0c\u800c\u9019\u7a2e\u4e0d\u4e00\u81f4\u6027\u9032\u4e00\u6b65\u8907\u96dc\u5316\u4e86\u6a21\u578b\u6bd4\u8f03\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u7576\u524d\u505a\u6cd5\u80cc\u5f8c\u7684\u57fa\u672c\u539f\u56e0\uff0c\u4e26\u767c\u73fe\u73fe\u6709\u7684\u57fa\u6e96\u7d93\u5e38\u7531\u65bc\u61c9\u7528\u8986\u84cb\u7387\u4f4e\u3001\u9577\u5ea6\u4e0d\u8db3\u3001\u6307\u6a19\u4e0d\u53ef\u9760\u4ee5\u53ca\u8207\u57fa\u790e\u6a21\u578b\u4e0d\u76f8\u5bb9\u800c\u63d0\u4f9b\u96dc\u8a0a\u8a0a\u865f\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 HELMET\uff08\u5982\u4f55\u6709\u6548\u4e14\u5fb9\u5e95\u5730\u8a55\u4f30\u9577\u8a9e\u5883\u6a21\u578b\uff09\uff0c\u9019\u662f\u4e00\u500b\u5305\u542b\u4e03\u500b\u4e0d\u540c\u7684\u4ee5\u61c9\u7528\u70ba\u4e2d\u5fc3\u7684\u985e\u5225\u7684\u7d9c\u5408\u57fa\u6e96\u3002\u6211\u5011\u9084\u901a\u904e\u589e\u52a0\u9577\u5ea6\u53ef\u63a7\u6027\uff08\u6700\u591a 128k \u500b\u7b26\u865f\uff09\u3001\u57fa\u65bc\u6a21\u578b\u7684\u8a55\u4f30\u4ee5\u7372\u5f97\u53ef\u9760\u7684\u6307\u6a19\uff0c\u4ee5\u53ca\u5c11\u91cf\u63d0\u793a\u4ee5\u7a69\u5065\u5730\u8a55\u4f30\u57fa\u790e\u6a21\u578b\u4f86\u89e3\u6c7a\u5148\u524d\u57fa\u6e96\u4e2d\u7684\u8a31\u591a\u554f\u984c\u3002\u56e0\u6b64\uff0c\u6211\u5011\u8b49\u660e HELMET \u70ba\u524d\u6cbf LCLM \u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u4e14\u4e00\u81f4\u7684\u6392\u540d\u3002\u901a\u904e\u5c0d 51 \u500b LCLM \u7684\u5168\u9762\u7814\u7a76\uff0c\u6211\u5011\u767c\u73fe (1) NIAH \u7b49\u5408\u6210\u4efb\u52d9\u4e26\u4e0d\u80fd\u5f88\u597d\u5730\u9810\u6e2c\u4e0b\u6e38\u6027\u80fd\uff1b(2) HELMET \u4e2d\u4e0d\u540c\u7684\u985e\u5225\u8868\u73fe\u51fa\u4e0d\u540c\u7684\u8da8\u52e2\uff0c\u4e26\u4e14\u5f7c\u6b64\u4e4b\u9593\u7684\u76f8\u95dc\u6027\u5f88\u4f4e\uff1b\u4ee5\u53ca (3) \u5118\u7ba1\u5927\u591a\u6578 LCLM \u90fd\u9054\u5230\u4e86\u5b8c\u7f8e\u7684 NIAH \u5206\u6578\uff0c\u4f46\u7576\u4efb\u52d9\u9700\u8981\u5168\u8a9e\u5883\u63a8\u7406\u6216\u9075\u5faa\u8907\u96dc\u6307\u4ee4\u6642\uff0c\u958b\u6e90\u6a21\u578b\u986f\u8457\u843d\u5f8c\u65bc\u5c01\u9589\u6a21\u578b - \u5dee\u8ddd\u96a8\u8457\u9577\u5ea6\u7684\u589e\u52a0\u800c\u64f4\u5927\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u6211\u5011\u7684 RAG \u4efb\u52d9\u9032\u884c\u5feb\u901f\u6a21\u578b\u958b\u767c\uff0c\u56e0\u70ba\u5b83\u5011\u6613\u65bc\u57f7\u884c\u4e14\u66f4\u80fd\u9810\u6e2c\u5176\u4ed6\u4e0b\u6e38\u6027\u80fd\uff1b\u6700\u7d42\uff0c\u6211\u5011\u63d0\u5021\u5c0d\u5404\u7a2e\u4efb\u52d9\u9032\u884c\u6574\u9ad4\u8a55\u4f30\u3002</paragraph>", "author": "Howard Yen et.al.", "authors": "Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izasak, Moshe Wasserblat, Danqi Chen", "id": "2410.02694v1", "paper_url": "http://arxiv.org/abs/2410.02694v1", "repo": "null"}}