{"2406.16777": {"publish_time": "2024-06-24", "title": "Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech Translation System for IWSLT 2024", "paper_summary": "Large Language Models (LLMs) are currently under exploration for various\ntasks, including Automatic Speech Recognition (ASR), Machine Translation (MT),\nand even End-to-End Speech Translation (ST). In this paper, we present KIT's\noffline submission in the constrained + LLM track by incorporating recently\nproposed techniques that can be added to any cascaded speech translation.\nSpecifically, we integrate\nMistral-7B\\footnote{mistralai/Mistral-7B-Instruct-v0.1} into our system to\nenhance it in two ways. Firstly, we refine the ASR outputs by utilizing the\nN-best lists generated by our system and fine-tuning the LLM to predict the\ntranscript accurately. Secondly, we refine the MT outputs at the document level\nby fine-tuning the LLM, leveraging both ASR and MT predictions to improve\ntranslation quality. We find that integrating the LLM into the ASR and MT\nsystems results in an absolute improvement of $0.3\\%$ in Word Error Rate and\n$0.65\\%$ in COMET for tst2019 test set. In challenging test sets with\noverlapping speakers and background noise, we find that integrating LLM is not\nbeneficial due to poor ASR performance. Here, we use ASR with chunked long-form\ndecoding to improve context usage that may be unavailable when transcribing\nwith Voice Activity Detection segmentation alone.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u76ee\u524d\u6b63\u5728\u63a2\u7d22\u5404\u7a2e\u4efb\u52d9\uff0c\u5305\u62ec\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR)\u3001\u6a5f\u5668\u7ffb\u8b6f (MT)\uff0c\u751a\u81f3\u7aef\u5230\u7aef\u8a9e\u97f3\u7ffb\u8b6f (ST)\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 KIT \u5728\u53d7\u9650 + LLM \u8ecc\u9053\u4e2d\u7684\u96e2\u7dda\u63d0\u4ea4\uff0c\u65b9\u6cd5\u662f\u63a1\u7528\u6700\u8fd1\u63d0\u51fa\u7684\u6280\u8853\uff0c\u9019\u4e9b\u6280\u8853\u53ef\u4ee5\u65b0\u589e\u81f3\u4efb\u4f55\u4e32\u806f\u8a9e\u97f3\u7ffb\u8b6f\u3002\u5177\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u5c07 Mistral-7B\\footnote{mistralai/Mistral-7B-Instruct-v0.1} \u6574\u5408\u5230\u6211\u5011\u7684\u7cfb\u7d71\u4e2d\uff0c\u4ee5\u5169\u7a2e\u65b9\u5f0f\u589e\u5f37\u5b83\u3002\u9996\u5148\uff0c\u6211\u5011\u5229\u7528\u7cfb\u7d71\u7522\u751f\u7684 N-best \u6e05\u55ae\uff0c\u4e26\u5fae\u8abf LLM \u4ee5\u6e96\u78ba\u9810\u6e2c\u8f49\u9304\uff0c\u5f9e\u800c\u6539\u5584 ASR \u8f38\u51fa\u3002\u5176\u6b21\uff0c\u6211\u5011\u900f\u904e\u5fae\u8abf LLM\uff0c\u540c\u6642\u5229\u7528 ASR \u548c MT \u9810\u6e2c\u4f86\u6539\u5584\u7ffb\u8b6f\u54c1\u8cea\uff0c\u5f9e\u800c\u6539\u5584\u6587\u4ef6\u5c64\u7d1a\u7684 MT \u8f38\u51fa\u3002\u6211\u5011\u767c\u73fe\u5c07 LLM \u6574\u5408\u5230 ASR \u548c MT \u7cfb\u7d71\u4e2d\uff0c\u6703\u8b93 tst2019 \u6e2c\u8a66\u96c6\u7684\u5b57\u5143\u932f\u8aa4\u7387\u7d55\u5c0d\u6539\u5584 $0.3\\%$\uff0cCOMET \u6539\u5584 $0.65\\%$\u3002\u5728\u6709\u91cd\u758a\u8aaa\u8a71\u8005\u548c\u80cc\u666f\u566a\u97f3\u7684\u5177\u6311\u6230\u6027\u6e2c\u8a66\u96c6\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u7531\u65bc ASR \u6548\u80fd\u4e0d\u4f73\uff0c\u6574\u5408 LLM \u4e26\u6c92\u6709\u5e6b\u52a9\u3002\u5728\u6b64\uff0c\u6211\u5011\u4f7f\u7528\u5206\u584a\u9577\u683c\u5f0f\u89e3\u78bc\u7684 ASR \u4f86\u6539\u5584\u53ef\u80fd\u5728\u55ae\u7368\u4f7f\u7528\u8a9e\u97f3\u6d3b\u52d5\u5075\u6e2c\u5206\u6bb5\u9032\u884c\u8f49\u9304\u6642\u7121\u6cd5\u4f7f\u7528\u7684\u5167\u5bb9\u4f7f\u7528\u3002", "author": "Sai Koneru et.al.", "authors": "Sai Koneru, Thai-Binh Nguyen, Ngoc-Quan Pham, Danni Liu, Zhaolin Li, Alexander Waibel, Jan Niehues", "id": "2406.16777v1", "paper_url": "http://arxiv.org/abs/2406.16777v1", "repo": "null"}}