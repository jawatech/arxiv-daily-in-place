{"2406.17245": {"publish_time": "2024-06-25", "title": "Unlocking Continual Learning Abilities in Language Models", "paper_summary": "Language models (LMs) exhibit impressive performance and generalization\ncapabilities. However, LMs struggle with the persistent challenge of\ncatastrophic forgetting, which undermines their long-term sustainability in\ncontinual learning (CL). Existing approaches usually address the issue by\nincorporating old task data or task-wise inductive bias into LMs. However, old\ndata and accurate task information are often unavailable or costly to collect,\nhindering the availability of current CL approaches for LMs. To address this\nlimitation, we introduce $\\textbf{MIGU}$ ($\\textbf{M}$agn$\\textbf{I}$tude-based\n$\\textbf{G}$radient $\\textbf{U}$pdating for continual learning), a\nrehearsal-free and task-label-free method that only updates the model\nparameters with large magnitudes of output in LMs' linear layers. MIGU is based\non our observation that the L1-normalized magnitude distribution of the output\nin LMs' linear layers is different when the LM models deal with different task\ndata. By imposing this simple constraint on the gradient update process, we can\nleverage the inherent behaviors of LMs, thereby unlocking their innate CL\nabilities. Our experiments demonstrate that MIGU is universally applicable to\nall three LM architectures (T5, RoBERTa, and Llama2), delivering\nstate-of-the-art or on-par performance across continual finetuning and\ncontinual pre-training settings on four CL benchmarks. For example, MIGU brings\na 15.2% average accuracy improvement over conventional parameter-efficient\nfinetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly\nintegrate with all three existing CL types to further enhance performance. Code\nis available at \\href{https://github.com/wenyudu/MIGU}{this https URL}.", "paper_summary_zh": "<paragraph>\u8a9e\u8a00\u6a21\u578b (LM) \u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\u548c\u6982\u5316\u80fd\u529b\u3002\u7136\u800c\uff0cLM \u9762\u81e8\u6301\u7e8c\u7684\u707d\u96e3\u6027\u907a\u5fd8\u6311\u6230\uff0c\u9019\u6703\u640d\u5bb3\u5176\u5728\u6301\u7e8c\u5b78\u7fd2 (CL) \u4e2d\u7684\u9577\u671f\u6c38\u7e8c\u6027\u3002\u73fe\u6709\u65b9\u6cd5\u901a\u5e38\u900f\u904e\u5c07\u820a\u4efb\u52d9\u8cc7\u6599\u6216\u4efb\u52d9\u660e\u667a\u7684\u6b78\u7d0d\u504f\u5dee\u7d0d\u5165 LM \u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002\u7136\u800c\uff0c\u820a\u8cc7\u6599\u548c\u6e96\u78ba\u7684\u4efb\u52d9\u8cc7\u8a0a\u901a\u5e38\u7121\u6cd5\u53d6\u5f97\u6216\u6536\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u9019\u963b\u7919\u4e86\u76ee\u524d LM \u7684 CL \u65b9\u6cd5\u7684\u53ef\u7528\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 $\\textbf{MIGU}$\uff08$\\textbf{M}$agn$\\textbf{I}$tude-based $\\textbf{G}$radient $\\textbf{U}$pdating for continual learning\uff09\uff0c\u4e00\u7a2e\u7121\u9700\u8907\u7fd2\u548c\u4efb\u52d9\u6a19\u7c64\u7684\u65b9\u6cd5\uff0c\u5b83\u53ea\u66f4\u65b0 LM \u7dda\u6027\u5c64\u4e2d\u8f38\u51fa\u5e45\u5ea6\u5927\u7684\u6a21\u578b\u53c3\u6578\u3002MIGU \u57fa\u65bc\u6211\u5011\u7684\u89c0\u5bdf\uff0c\u5373\u7576 LM \u6a21\u578b\u8655\u7406\u4e0d\u540c\u7684\u4efb\u52d9\u8cc7\u6599\u6642\uff0cLM \u7dda\u6027\u5c64\u4e2d\u8f38\u51fa\u7684 L1 \u6a19\u6e96\u5316\u5e45\u5ea6\u5206\u4f48\u662f\u4e0d\u540c\u7684\u3002\u900f\u904e\u5c0d\u68af\u5ea6\u66f4\u65b0\u904e\u7a0b\u65bd\u52a0\u9019\u500b\u7c21\u55ae\u7684\u7d04\u675f\uff0c\u6211\u5011\u53ef\u4ee5\u5229\u7528 LM \u7684\u56fa\u6709\u884c\u70ba\uff0c\u5f9e\u800c\u89e3\u9396\u5176\u5929\u751f\u7684 CL \u80fd\u529b\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0cMIGU \u666e\u904d\u9069\u7528\u65bc\u6240\u6709\u4e09\u7a2e LM \u67b6\u69cb\uff08T5\u3001RoBERTa \u548c Llama2\uff09\uff0c\u5728\u56db\u500b CL \u57fa\u6e96\u4e0a\u63d0\u4f9b\u6700\u5148\u9032\u6216\u540c\u7b49\u7684\u6548\u80fd\uff0c\u5305\u62ec\u6301\u7e8c\u5fae\u8abf\u548c\u6301\u7e8c\u9810\u8a13\u7df4\u8a2d\u5b9a\u3002\u4f8b\u5982\uff0c\u5728 15 \u500b\u4efb\u52d9\u7684 CL \u57fa\u6e96\u4e2d\uff0cMIGU \u6bd4\u50b3\u7d71\u7684\u53c3\u6578\u6709\u6548\u5fae\u8abf\u57fa\u6e96\u63d0\u9ad8\u4e86 15.2% \u7684\u5e73\u5747\u6e96\u78ba\u5ea6\u3002MIGU \u4e5f\u53ef\u4ee5\u7121\u7e2b\u6574\u5408\u6240\u6709\u4e09\u7a2e\u985e\u578b\u7684\u73fe\u6709 CL\uff0c\u4ee5\u9032\u4e00\u6b65\u63d0\u5347\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 \\href{https://github.com/wenyudu/MIGU}{\u9019\u500b https URL} \u53d6\u5f97\u3002</paragraph>", "author": "Wenyu Du et.al.", "authors": "Wenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold Cheng, Jie Fu", "id": "2406.17245v1", "paper_url": "http://arxiv.org/abs/2406.17245v1", "repo": "https://github.com/wenyudu/migu"}}