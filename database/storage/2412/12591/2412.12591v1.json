{"2412.12591": {"publish_time": "2024-12-17", "title": "LLMs are Also Effective Embedding Models: An In-depth Overview", "paper_summary": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods, such as handling longer texts, and multilingual and cross-modal data.\nFurthermore, we discuss factors affecting choices of embedding models, such as\nperformance/efficiency comparisons, dense vs sparse embeddings, pooling\nstrategies, and scaling law. Lastly, the survey highlights the limitations and\nchallenges in adapting LLMs for embeddings, including cross-task embedding\nquality, trade-offs between efficiency and accuracy, low-resource,\nlong-context, data bias, robustness, etc. This survey serves as a valuable\nresource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u9054\u6210\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u5fb9\u5e95\u9769\u65b0\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3002\u6700\u8fd1\uff0c\u5b83\u5011\u4f5c\u70ba\u5d4c\u5165\u6a21\u578b\u7684\u6548\u80fd\u5099\u53d7\u77da\u76ee\uff0c\u6a19\u8a8c\u8457\u5f9e\u50b3\u7d71\u7684\u50c5\u7de8\u78bc\u5668\u6a21\u578b\uff08\u4f8b\u5982 ELMo \u548c BERT\uff09\u8f49\u8b8a\u70ba\u50c5\u89e3\u78bc\u5668\u3001\u5927\u898f\u6a21\u7684 LLM\uff08\u4f8b\u5982 GPT\u3001LLaMA \u548c Mistral\uff09\u3002\u9019\u9805\u8abf\u67e5\u6df1\u5165\u63a2\u8a0e\u4e86\u9019\u9805\u8f49\u8b8a\uff0c\u5f9e LLM \u6642\u4ee3\u4e4b\u524d\u7684\u57fa\u790e\u6280\u8853\u958b\u59cb\uff0c\u63a5\u8457\u662f\u900f\u904e\u5f9e LLM \u884d\u751f\u5d4c\u5165\u7684\u5169\u7a2e\u4e3b\u8981\u7b56\u7565\uff0c\u63a2\u8a0e\u57fa\u65bc LLM \u7684\u5d4c\u5165\u6a21\u578b\u30021) \u76f4\u63a5\u63d0\u793a\uff1a\u6211\u5011\u4e3b\u8981\u8a0e\u8ad6\u63d0\u793a\u8a2d\u8a08\u548c\u884d\u751f\u7af6\u722d\u529b\u5d4c\u5165\u7684\u57fa\u790e\u539f\u7406\u30022) \u4ee5\u8cc7\u6599\u70ba\u4e2d\u5fc3\u7684\u8abf\u6574\uff1a\u6211\u5011\u6db5\u84cb\u5f71\u97ff\u8abf\u6574\u5d4c\u5165\u6a21\u578b\u7684\u5ee3\u6cdb\u9762\u5411\uff0c\u5305\u62ec\u6a21\u578b\u67b6\u69cb\u3001\u8a13\u7df4\u76ee\u6a19\u3001\u8cc7\u6599\u5efa\u69cb\u7b49\u3002\u5728\u4e0a\u8ff0\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u4e5f\u6db5\u84cb\u9032\u968e\u65b9\u6cd5\uff0c\u4f8b\u5982\u8655\u7406\u8f03\u9577\u7684\u6587\u5b57\uff0c\u4ee5\u53ca\u591a\u8a9e\u8a00\u548c\u8de8\u6a21\u614b\u8cc7\u6599\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a0e\u8ad6\u5f71\u97ff\u5d4c\u5165\u6a21\u578b\u9078\u64c7\u7684\u56e0\u7d20\uff0c\u4f8b\u5982\u6548\u80fd/\u6548\u7387\u6bd4\u8f03\u3001\u7a20\u5bc6\u8207\u7a00\u758f\u5d4c\u5165\u3001\u532f\u96c6\u7b56\u7565\u548c\u898f\u6a21\u5b9a\u5f8b\u3002\u6700\u5f8c\uff0c\u9019\u9805\u8abf\u67e5\u91cd\u9ede\u8aaa\u660e\u4e86\u5c07 LLM \u8abf\u6574\u70ba\u5d4c\u5165\u7684\u9650\u5236\u548c\u6311\u6230\uff0c\u5305\u62ec\u8de8\u4efb\u52d9\u5d4c\u5165\u54c1\u8cea\u3001\u6548\u7387\u548c\u6e96\u78ba\u6027\u4e4b\u9593\u7684\u6b0a\u8861\u3001\u4f4e\u8cc7\u6e90\u3001\u9577\u8a9e\u5883\u3001\u8cc7\u6599\u504f\u5dee\u3001\u7a69\u5065\u6027\u7b49\u3002\u9019\u9805\u8abf\u67e5\u900f\u904e\u7d9c\u5408\u76ee\u524d\u7684\u9032\u5c55\u3001\u5f37\u8abf\u95dc\u9375\u6311\u6230\uff0c\u4e26\u63d0\u4f9b\u4e00\u500b\u5168\u9762\u7684\u67b6\u69cb\uff0c\u4ee5\u589e\u5f37 LLM \u4f5c\u70ba\u5d4c\u5165\u6a21\u578b\u7684\u6548\u80fd\u548c\u6548\u7387\uff0c\u9032\u800c\u6210\u70ba\u7814\u7a76\u4eba\u54e1\u548c\u5be6\u52d9\u5de5\u4f5c\u8005\u7684\u5bf6\u8cb4\u8cc7\u6e90\u3002", "author": "Chongyang Tao et.al.", "authors": "Chongyang Tao, Tao Shen, Shen Gao, Junshuo Zhang, Zhen Li, Zhengwei Tao, Shuai Ma", "id": "2412.12591v1", "paper_url": "http://arxiv.org/abs/2412.12591v1", "repo": "null"}}