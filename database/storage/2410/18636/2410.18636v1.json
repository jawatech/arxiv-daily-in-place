{"2410.18636": {"publish_time": "2024-10-24", "title": "Multi-agent cooperation through learning-aware policy gradients", "paper_summary": "Self-interested individuals often fail to cooperate, posing a fundamental\nchallenge for multi-agent learning. How can we achieve cooperation among\nself-interested, independent learning agents? Promising recent work has shown\nthat in certain tasks cooperation can be established between learning-aware\nagents who model the learning dynamics of each other. Here, we present the\nfirst unbiased, higher-derivative-free policy gradient algorithm for\nlearning-aware reinforcement learning, which takes into account that other\nagents are themselves learning through trial and error based on multiple noisy\ntrials. We then leverage efficient sequence models to condition behavior on\nlong observation histories that contain traces of the learning dynamics of\nother agents. Training long-context policies with our algorithm leads to\ncooperative behavior and high returns on standard social dilemmas, including a\nchallenging environment where temporally-extended action coordination is\nrequired. Finally, we derive from the iterated prisoner's dilemma a novel\nexplanation for how and when cooperation arises among self-interested\nlearning-aware agents.", "paper_summary_zh": "\u81ea\u79c1\u7684\u4eba\u5e38\u5e38\u65e0\u6cd5\u5408\u4f5c\uff0c\u5bf9\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u6784\u6210\u4e86\u6839\u672c\u6311\u6218\u3002\u6211\u4eec\u5982\u4f55\u5728\u81ea\u79c1\u3001\u72ec\u7acb\u7684\u5b66\u4e60\u667a\u80fd\u4f53\u4e4b\u95f4\u5b9e\u73b0\u5408\u4f5c\uff1f\u6700\u8fd1\u6709\u5e0c\u671b\u7684\u7814\u7a76\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\uff0c\u53ef\u4ee5\u5efa\u7acb\u5b66\u4e60\u611f\u77e5\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5408\u4f5c\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u5bf9\u5f7c\u6b64\u7684\u5b66\u4e60\u52a8\u6001\u8fdb\u884c\u5efa\u6a21\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u65e0\u504f\u3001\u65e0\u9ad8\u9636\u5bfc\u6570\u7684\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u5b83\u8003\u8651\u5230\u5176\u4ed6\u667a\u80fd\u4f53\u672c\u8eab\u6b63\u5728\u901a\u8fc7\u591a\u6b21\u5608\u6742\u8bd5\u9a8c\u8fdb\u884c\u8bd5\u9519\u5b66\u4e60\u3002\u7136\u540e\uff0c\u6211\u4eec\u5229\u7528\u9ad8\u6548\u7684\u5e8f\u5217\u6a21\u578b\uff0c\u6839\u636e\u5305\u542b\u5176\u4ed6\u667a\u80fd\u4f53\u5b66\u4e60\u52a8\u6001\u75d5\u8ff9\u7684\u957f\u89c2\u5bdf\u5386\u53f2\u8bb0\u5f55\u6765\u8c03\u8282\u884c\u4e3a\u3002\u4f7f\u7528\u6211\u4eec\u7684\u7b97\u6cd5\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u7b56\u7565\u4f1a\u5bfc\u81f4\u5408\u4f5c\u884c\u4e3a\uff0c\u5e76\u5728\u6807\u51c6\u793e\u4f1a\u56f0\u5883\u4e2d\u83b7\u5f97\u9ad8\u56de\u62a5\uff0c\u5305\u62ec\u9700\u8981\u65f6\u95f4\u6269\u5c55\u52a8\u4f5c\u534f\u8c03\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u3002\u6700\u540e\uff0c\u6211\u4eec\u4ece\u91cd\u590d\u56da\u5f92\u56f0\u5883\u4e2d\u63a8\u5bfc\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u89e3\u91ca\uff0c\u8bf4\u660e\u81ea\u79c1\u7684\u5b66\u4e60\u611f\u77e5\u667a\u80fd\u4f53\u5982\u4f55\u4ee5\u53ca\u4f55\u65f6\u4ea7\u751f\u5408\u4f5c\u3002", "author": "Alexander Meulemans et.al.", "authors": "Alexander Meulemans, Seijin Kobayashi, Johannes von Oswald, Nino Scherrer, Eric Elmoznino, Blake Richards, Guillaume Lajoie, Blaise Ag\u00fcera y Arcas, Jo\u00e3o Sacramento", "id": "2410.18636v1", "paper_url": "http://arxiv.org/abs/2410.18636v1", "repo": "null"}}