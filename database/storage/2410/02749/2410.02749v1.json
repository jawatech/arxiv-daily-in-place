{"2410.02749": {"publish_time": "2024-10-03", "title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis", "paper_summary": "Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode.", "paper_summary_zh": "\u8edf\u9ad4\u5de5\u7a0b\u5e2b\u4e3b\u8981\u900f\u904e\u7de8\u8f2f\u73fe\u6709\u7a0b\u5f0f\u4f86\u64b0\u5beb\u7a0b\u5f0f\u78bc\u3002\u76f8\u5c0d\u800c\u8a00\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6703\u5728\u55ae\u6b21\u50b3\u905e\u4e2d\u81ea\u52d5\u56de\u6b78\u5730\u5408\u6210\u7a0b\u5f0f\u78bc\u3002\u5176\u4e2d\u4e00\u500b\u89e3\u91cb\u662f\u958b\u653e\u539f\u59cb\u78bc\u7de8\u8f2f\u8cc7\u6599\u7684\u7a00\u5c11\u6027\u3002\u96d6\u7136\u7528\u65bc\u7a0b\u5f0f\u78bc\u5408\u6210\u7684\u512a\u8cea\u6307\u4ee4\u8cc7\u6599\u5df2\u7d93\u5f88\u7a00\u5c11\uff0c\u4f46\u512a\u8cea\u7684\u7de8\u8f2f\u8cc7\u6599\u66f4\u70ba\u7a00\u5c11\u3002\u70ba\u4e86\u586b\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u540d\u70ba LintSeq \u7684\u5408\u6210\u8cc7\u6599\u7522\u751f\u6f14\u7b97\u6cd5\u3002\u6b64\u6f14\u7b97\u6cd5\u6703\u900f\u904e\u4f7f\u7528 linter \u5faa\u5e8f\u6f38\u9032\u5730\u5c0d\u7121\u932f\u8aa4\u63d2\u5165\u9032\u884c\u62bd\u6a23\uff0c\u5c07\u73fe\u6709\u7a0b\u5f0f\u78bc\u91cd\u69cb\u70ba\u4e00\u7cfb\u5217\u7a0b\u5f0f\u78bc\u7de8\u8f2f\u3002\u5b83\u6703\u4ee5\u7531\u9023\u7e8c\u7a0b\u5f0f\u78bc\u5dee\u7570\u7d44\u6210\u7684\u6587\u5b57\u5b57\u4e32\u4f5c\u70ba\u8f38\u51fa\u7de8\u8f2f\u5e8f\u5217\u3002\u70ba\u4e86\u6e2c\u8a66 LintSeq\uff0c\u6211\u5011\u4f7f\u7528\u5b83\u5c07\u6307\u4ee4 + \u7a0b\u5f0f\u78bc\u914d\u5c0d\u8cc7\u6599\u96c6\u91cd\u69cb\u70ba\u6307\u4ee4 + \u7a0b\u5f0f\u78bc\u5dee\u7570\u5e8f\u5217\u7d44\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c0d\u5f9e 2.6B \u5230 14B \u53c3\u6578\u7bc4\u570d\u5167\u7684\u4e00\u7cfb\u5217\u8f03\u5c0f\u7684 LLM \u9032\u884c\u6307\u4ee4\u5fae\u8abf\uff0c\u540c\u6642\u91dd\u5c0d\u6b64\u8cc7\u6599\u96c6\u7684\u91cd\u69cb\u7248\u672c\u548c\u539f\u59cb\u7248\u672c\u9032\u884c\u5fae\u8abf\uff0c\u4e26\u6bd4\u8f03\u7a0b\u5f0f\u78bc\u5408\u6210\u57fa\u6e96\u4e0a\u7684\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\u3002\u6211\u5011\u5c55\u793a\u4e86\u5728\u91cd\u8907\u62bd\u6a23\u671f\u9593\uff0c\u7de8\u8f2f\u5e8f\u5217\u5fae\u8abf\u6a21\u578b\u6703\u7522\u751f\u6bd4\u57fa\u6e96\u66f4\u70ba\u591a\u6a23\u5316\u7684\u7a0b\u5f0f\u78bc\u3002\u9019\u6703\u5c0e\u81f4\u57fa\u6e96\u6db5\u84cb\u7bc4\u570d\u5728\u6a23\u672c\u51fd\u6578\u4e2d\u7684\u63a8\u8ad6\u6642\u9593\u7e2e\u653e\u6548\u679c\u66f4\u597d\uff0c\u4ea6\u5373\u5728\u7d66\u5b9a\u300ck\u300d\u6b21\u5617\u8a66\u7684\u4efb\u4f55\u5617\u8a66\u4e2d\u89e3\u6c7a\u300cpass@k\u300d\u554f\u984c\u7684\u6bd4\u4f8b\u3002\u4f8b\u5982\uff0c\u5728 HumanEval pass@50 \u4e2d\uff0c\u91dd\u5c0d\u5408\u6210\u7de8\u8f2f\u5e8f\u5217\u9032\u884c\u5fae\u8abf\u7684\u5c0f\u578b LLM \u8207 GPT-4 \u5177\u6709\u7af6\u722d\u529b\uff0c\u4e14\u5728\u7d55\u5c0d\u5206\u6578\u4e0a\u6bd4\u91dd\u5c0d\u57fa\u6e96\u8cc7\u6599\u96c6\u9032\u884c\u5fae\u8abf\u7684\u6a21\u578b\u9ad8\u51fa +20% (+/-3%)\u3002\u6700\u5f8c\uff0c\u6211\u5011\u4e5f\u91dd\u5c0d\u7a0b\u5f0f\u78bc\u7406\u89e3\u9810\u8a13\u7df4\u6211\u5011\u81ea\u5df1\u7684\u5c0f\u578b LM\u3002\u6211\u5011\u5c55\u793a\u4e86\u91dd\u5c0d\u5408\u6210\u7a0b\u5f0f\u78bc\u7de8\u8f2f\u9032\u884c\u5fae\u8abf\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u6703\u7522\u751f\u88dd\u7f6e\u4e0a\u6a21\u578b\u985e\u5225\u7684\u6700\u65b0\u7a0b\u5f0f\u78bc\u5408\u6210\u3002\u6211\u5011\u7684 150M \u53c3\u6578\u7de8\u8f2f\u5e8f\u5217 LM \u5339\u914d\u6216\u8d85\u8d8a\u53c3\u6578\u591a\u4e00\u500d\u7684\u7a0b\u5f0f\u78bc\u6a21\u578b\uff0c\u7121\u8ad6\u662f\u5426\u91cd\u8907\u62bd\u6a23\uff0c\u5176\u4e2d\u5305\u62ec Codex \u548c AlphaCode\u3002", "author": "Ulyana Piterbarg et.al.", "authors": "Ulyana Piterbarg, Lerrel Pinto, Rob Fergus", "id": "2410.02749v1", "paper_url": "http://arxiv.org/abs/2410.02749v1", "repo": "null"}}