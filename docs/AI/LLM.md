
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-31**|**Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey**|Atsuyuki Miyai et.al.|[2407.21794v1](http://arxiv.org/abs/2407.21794v1)|null|
|**2024-07-31**|**Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?**|Richard Ren et.al.|[2407.21792v1](http://arxiv.org/abs/2407.21792v1)|null|
|**2024-07-31**|**Vision-Language Model Based Handwriting Verification**|Mihir Chauhan et.al.|[2407.21788v1](http://arxiv.org/abs/2407.21788v1)|null|
|**2024-07-31**|**Large Language Monkeys: Scaling Inference Compute with Repeated Sampling**|Bradley Brown et.al.|[2407.21787v1](http://arxiv.org/abs/2407.21787v1)|null|
|**2024-07-31**|**The Llama 3 Herd of Models**|Abhimanyu Dubey et.al.|[2407.21783v1](http://arxiv.org/abs/2407.21783v1)|null|
|**2024-07-31**|**Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries**|Felix Ocker et.al.|[2407.21778v1](http://arxiv.org/abs/2407.21778v1)|null|
|**2024-07-31**|**ShieldGemma: Generative AI Content Moderation Based on Gemma**|Wenjun Zeng et.al.|[2407.21772v1](http://arxiv.org/abs/2407.21772v1)|null|
|**2024-07-31**|**MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts**|Xi Victoria Lin et.al.|[2407.21770v1](http://arxiv.org/abs/2407.21770v1)|null|
|**2024-07-31**|**HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection**|Junwei He et.al.|[2407.21742v1](http://arxiv.org/abs/2407.21742v1)|null|
|**2024-07-31**|**Contrastive Factor Analysis**|Zhibin Duan et.al.|[2407.21740v2](http://arxiv.org/abs/2407.21740v2)|null|
|**2024-07-31**|**A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation**|Mothilal Asokan et.al.|[2407.21739v1](http://arxiv.org/abs/2407.21739v1)|null|
|**2024-07-31**|**Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos**|Joseph Geo Benjamin et.al.|[2407.21738v1](http://arxiv.org/abs/2407.21738v1)|null|
|**2024-07-31**|**Open-Vocabulary Audio-Visual Semantic Segmentation**|Ruohao Guo et.al.|[2407.21721v1](http://arxiv.org/abs/2407.21721v1)|null|
|**2024-07-31**|**Social Learning through Interactions with Other Agents: A Survey**|Dylan hillier et.al.|[2407.21713v1](http://arxiv.org/abs/2407.21713v1)|null|
|**2024-07-31**|**Adaptive Retrieval-Augmented Generation for Conversational Systems**|Xi Wang et.al.|[2407.21712v1](http://arxiv.org/abs/2407.21712v1)|null|
|**2024-07-31**|**CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**|Stefan Langer et.al.|[2407.21708v1](http://arxiv.org/abs/2407.21708v1)|null|
|**2024-07-31**|**TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities**|Ming Zhang et.al.|[2407.21693v1](http://arxiv.org/abs/2407.21693v1)|null|
|**2024-07-31**|**Dynamic Object Queries for Transformer-based Incremental Object Detection**|Jichuan Zhang et.al.|[2407.21687v1](http://arxiv.org/abs/2407.21687v1)|null|
|**2024-07-31**|**Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation**|Krishan Agyakari Raja Babu et.al.|[2407.21674v1](http://arxiv.org/abs/2407.21674v1)|null|
|**2024-07-31**|**Universal Approximation Theory: Foundations for Parallelism in Neural Networks**|Wei Wang et.al.|[2407.21670v1](http://arxiv.org/abs/2407.21670v1)|null|
|**2024-07-31**|**Synth-Empathy: Towards High-Quality Synthetic Empathy Data**|Hao Liang et.al.|[2407.21669v1](http://arxiv.org/abs/2407.21669v1)|null|
|**2024-07-31**|**An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification**|Aswini Kumar Patra et.al.|[2407.21666v1](http://arxiv.org/abs/2407.21666v1)|null|
|**2024-07-31**|**Defending Jailbreak Attack in VLMs via Cross-modality Information Detector**|Yue Xu et.al.|[2407.21659v2](http://arxiv.org/abs/2407.21659v2)|null|
|**2024-07-31**|**Spatial Transformer Network YOLO Model for Agricultural Object Detection**|Yash Zambre et.al.|[2407.21652v1](http://arxiv.org/abs/2407.21652v1)|null|
|**2024-07-31**|**Human interaction classifier for LLM based chatbot**|Diego Martín et.al.|[2407.21647v1](http://arxiv.org/abs/2407.21647v1)|null|
|**2024-07-31**|**Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent**|Shanbo Cheng et.al.|[2407.21646v1](http://arxiv.org/abs/2407.21646v1)|null|
|**2024-07-31**|**Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components**|Hermione Warr et.al.|[2407.21638v1](http://arxiv.org/abs/2407.21638v1)|null|
|**2024-07-31**|**Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation**|Xiang Luo et.al.|[2407.21633v1](http://arxiv.org/abs/2407.21633v1)|null|
|**2024-07-31**|**TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods**|Gabriel Loiseau et.al.|[2407.21630v1](http://arxiv.org/abs/2407.21630v1)|null|
|**2024-07-31**|**Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music**|Pedro Sarmento et.al.|[2407.21615v1](http://arxiv.org/abs/2407.21615v1)|null|
|**2024-07-31**|**Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism**|Jiafeng Zhong et.al.|[2407.21611v1](http://arxiv.org/abs/2407.21611v1)|null|
|**2024-07-31**|**Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors**|Shoujin Huang et.al.|[2407.21600v1](http://arxiv.org/abs/2407.21600v1)|null|
|**2024-07-31**|**Voxel Scene Graph for Intracranial Hemorrhage**|Antoine P. Sanner et.al.|[2407.21580v1](http://arxiv.org/abs/2407.21580v1)|null|
|**2024-07-31**|**A Performance Study of LLM-Generated Code on Leetcode**|Tristan Coignion et.al.|[2407.21579v1](http://arxiv.org/abs/2407.21579v1)|null|
|**2024-07-31**|**Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography**|Kit M. Bransby et.al.|[2407.21577v1](http://arxiv.org/abs/2407.21577v1)|null|
|**2024-07-31**|**PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning**|Min Jae Jung et.al.|[2407.21571v1](http://arxiv.org/abs/2407.21571v1)|null|
|**2024-07-31**|**Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding**|Jun Zhou et.al.|[2407.21560v1](http://arxiv.org/abs/2407.21560v1)|null|
|**2024-07-31**|**Operator-based semantics for choice programs: is choosing losing? (full version)**|Jesse Heyninck et.al.|[2407.21556v1](http://arxiv.org/abs/2407.21556v1)|null|
|**2024-07-31**|**Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition**|Jiang Li et.al.|[2407.21536v1](http://arxiv.org/abs/2407.21536v1)|null|
|**2024-07-31**|**Can LLMs "Reason" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation**|Ziya Zhou et.al.|[2407.21531v1](http://arxiv.org/abs/2407.21531v1)|null|
|**2024-07-31**|**Data Contamination Report from the 2024 CONDA Shared Task**|Oscar Sainz et.al.|[2407.21530v1](http://arxiv.org/abs/2407.21530v1)|null|
|**2024-07-31**|**Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI**|Lingxi Cui et.al.|[2407.21523v1](http://arxiv.org/abs/2407.21523v1)|null|
|**2024-07-31**|**Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images**|I. M. Chernenkiy et.al.|[2407.21516v1](http://arxiv.org/abs/2407.21516v1)|null|
|**2024-07-31**|**Interpreting and learning voice commands with a Large Language Model for a robot system**|Stanislau Stankevich et.al.|[2407.21512v1](http://arxiv.org/abs/2407.21512v1)|null|
|**2024-07-31**|**FSSC: Federated Learning of Transformer Neural Networks for Semantic Image Communication**|Yuna Yan et.al.|[2407.21507v1](http://arxiv.org/abs/2407.21507v1)|null|
|**2024-07-31**|**MaskUno: Switch-Split Block For Enhancing Instance Segmentation**|Jawad Haidar et.al.|[2407.21498v1](http://arxiv.org/abs/2407.21498v1)|null|
|**2024-07-31**|**Generative Expressive Conversational Speech Synthesis**|Rui Liu et.al.|[2407.21491v2](http://arxiv.org/abs/2407.21491v2)|null|
|**2024-07-31**|**Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation**|Junxuan Yu et.al.|[2407.21490v1](http://arxiv.org/abs/2407.21490v1)|null|
|**2024-07-31**|**Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends**|Giuliano Martinelli et.al.|[2407.21489v1](http://arxiv.org/abs/2407.21489v1)|null|
|**2024-07-31**|**eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**|Xiny Pan et.al.|[2407.21483v2](http://arxiv.org/abs/2407.21483v2)|null|
|**2024-07-31**|**On the Problem of Text-To-Speech Model Selection for Synthetic Data Generation in Automatic Speech Recognition**|Nick Rossenbach et.al.|[2407.21476v1](http://arxiv.org/abs/2407.21476v1)|null|
|**2024-07-31**|**Fine-gained Zero-shot Video Sampling**|Dengsheng Chen et.al.|[2407.21475v1](http://arxiv.org/abs/2407.21475v1)|null|
|**2024-07-31**|**An Invertible State Space for Process Trees**|Gero Kolhof et.al.|[2407.21468v1](http://arxiv.org/abs/2407.21468v1)|null|
|**2024-07-31**|**Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data**|Mengtian Kang et.al.|[2407.21467v1](http://arxiv.org/abs/2407.21467v1)|null|
|**2024-07-31**|**KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making**|Gilang Fajar Febrian et.al.|[2407.21459v1](http://arxiv.org/abs/2407.21459v1)|null|
|**2024-07-31**|**TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors**|Zhaolan Huang et.al.|[2407.21453v1](http://arxiv.org/abs/2407.21453v1)|null|
|**2024-07-31**|**Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**|Haodong Hong et.al.|[2407.21452v1](http://arxiv.org/abs/2407.21452v1)|null|
|**2024-07-31**|**Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency**|Taiji Li et.al.|[2407.21443v1](http://arxiv.org/abs/2407.21443v1)|null|
|**2024-07-31**|**QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications**|Ritvik Setty et.al.|[2407.21441v2](http://arxiv.org/abs/2407.21441v2)|null|
|**2024-07-31**|**MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training**|Zhanpeng Chen et.al.|[2407.21439v1](http://arxiv.org/abs/2407.21439v1)|null|
|**2024-07-31**|**Deformable 3D Shape Diffusion Model**|Dengsheng Chen et.al.|[2407.21428v1](http://arxiv.org/abs/2407.21428v1)|null|
|**2024-07-31**|**Cost-Effective Hallucination Detection for LLMs**|Simon Valentin et.al.|[2407.21424v1](http://arxiv.org/abs/2407.21424v1)|null|
|**2024-07-31**|**Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models**|Zhengxuan Wu et.al.|[2407.21417v1](http://arxiv.org/abs/2407.21417v1)|null|
|**2024-07-31**|**Towards interfacing large language models with ASR systems using confidence measures and prompting**|Maryam Naderi et.al.|[2407.21414v1](http://arxiv.org/abs/2407.21414v1)|null|
|**2024-07-31**|**GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction**|Yanxu Mao et.al.|[2407.21384v1](http://arxiv.org/abs/2407.21384v1)|null|
|**2024-07-31**|**An Extended Kalman Filter Integrated Latent Feature Model on Dynamic Weighted Directed Graphs**|Hongxun Zhou et.al.|[2407.21376v1](http://arxiv.org/abs/2407.21376v1)|null|
|**2024-07-31**|**Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering**|Danfeng Guo et.al.|[2407.21368v1](http://arxiv.org/abs/2407.21368v1)|null|
|**2024-07-31**|**ProSpec RL: Plan Ahead, then Execute**|Liangliang Liu et.al.|[2407.21359v1](http://arxiv.org/abs/2407.21359v1)|null|
|**2024-07-31**|**Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**|Elan Markowitz et.al.|[2407.21358v1](http://arxiv.org/abs/2407.21358v1)|null|
|**2024-07-31**|**Small Object Few-shot Segmentation for Vision-based Industrial Inspection**|Zilong Zhang et.al.|[2407.21351v1](http://arxiv.org/abs/2407.21351v1)|null|
|**2024-07-31**|**Differentially Private Block-wise Gradient Shuffle for Deep Learning**|David Zagardo et.al.|[2407.21347v1](http://arxiv.org/abs/2407.21347v1)|null|
|**2024-07-31**|**Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction**|Jingyao Wu et.al.|[2407.21344v1](http://arxiv.org/abs/2407.21344v1)|null|
|**2024-07-31**|**Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks**|David Valencia et.al.|[2407.21338v1](http://arxiv.org/abs/2407.21338v1)|null|
|**2024-07-31**|**Performance of Recent Large Language Models for a Low-Resourced Language**|Ravindu Jayakody et.al.|[2407.21330v1](http://arxiv.org/abs/2407.21330v1)|null|
|**2024-07-31**|**MetaOpenFOAM: an LLM-based multi-agent framework for CFD**|Yuxuan Chena et.al.|[2407.21320v1](http://arxiv.org/abs/2407.21320v1)|null|
|**2024-07-31**|**Big Cooperative Learning**|Yulai Cong et.al.|[2407.21319v1](http://arxiv.org/abs/2407.21319v1)|null|
|**2024-07-31**|**Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances**|Zehui Wu et.al.|[2407.21315v2](http://arxiv.org/abs/2407.21315v2)|null|
|**2024-07-31**|**EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer**|Ali Abedi et.al.|[2407.21311v1](http://arxiv.org/abs/2407.21311v1)|null|
|**2024-07-31**|**Implementing Streaming algorithm and k-means clusters to RAG**|Haoyu Kang et.al.|[2407.21300v1](http://arxiv.org/abs/2407.21300v1)|null|
|**2024-07-31**|**Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models**|Kaustav Bhattacharjee et.al.|[2407.21299v1](http://arxiv.org/abs/2407.21299v1)|null|
|**2024-07-31**|**SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**|Peiru Zheng et.al.|[2407.21293v1](http://arxiv.org/abs/2407.21293v1)|null|
|**2024-07-31**|**Robust Box Prompt based SAM for Medical Image Segmentation**|Yuhao Huang et.al.|[2407.21284v1](http://arxiv.org/abs/2407.21284v1)|null|
|**2024-07-31**|**Multi-Level Querying using A Knowledge Pyramid**|Rubing Chen et.al.|[2407.21276v1](http://arxiv.org/abs/2407.21276v1)|null|
|**2024-07-31**|**Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net**|Rohini Banerjee et.al.|[2407.21273v1](http://arxiv.org/abs/2407.21273v1)|null|
|**2024-07-31**|**DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations**|Dongwon Son et.al.|[2407.21267v1](http://arxiv.org/abs/2407.21267v1)|null|
|**2024-07-31**|**Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning**|Alimohammad Beigi et.al.|[2407.21264v1](http://arxiv.org/abs/2407.21264v1)|null|
|**2024-07-31**|**Lifelong Person Search**|Jae-Won Yang et.al.|[2407.21252v1](http://arxiv.org/abs/2407.21252v1)|null|
|**2024-07-30**|**Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens**|Anqi Zhang et.al.|[2407.21248v1](http://arxiv.org/abs/2407.21248v1)|null|
|**2024-07-30**|**Informed Correctors for Discrete Diffusion Models**|Yixiu Zhao et.al.|[2407.21243v1](http://arxiv.org/abs/2407.21243v1)|null|
|**2024-07-30**|**Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration**|Ngoc Son Nguyen et.al.|[2407.21229v1](http://arxiv.org/abs/2407.21229v1)|null|
|**2024-07-30**|**Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models**|Florian Tambon et.al.|[2407.21227v1](http://arxiv.org/abs/2407.21227v1)|null|
|**2024-07-30**|**AI methods for approximate compiling of unitaries**|David Kremer et.al.|[2407.21225v1](http://arxiv.org/abs/2407.21225v1)|null|
|**2024-07-30**|**LoRaWAN Based Dynamic Noise Mapping with Machine Learning for Urban Noise Enforcement**|H. Emre Erdem et.al.|[2407.21204v1](http://arxiv.org/abs/2407.21204v1)|null|
|**2024-07-30**|**GenRec: Generative Personalized Sequential Recommendation**|Panfeng Cao et.al.|[2407.21191v1](http://arxiv.org/abs/2407.21191v1)|null|
|**2024-07-30**|**AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning**|Maisha Binte Rashid et.al.|[2407.21174v1](http://arxiv.org/abs/2407.21174v1)|null|
|**2024-07-30**|**Decomposed Prompting to Answer Questions on a Course Discussion Board**|Brandon Jaipersaud et.al.|[2407.21170v1](http://arxiv.org/abs/2407.21170v1)|null|
|**2024-07-30**|**Understanding Public Safety Trends in Calgary through data mining**|Zack Dewis et.al.|[2407.21163v1](http://arxiv.org/abs/2407.21163v1)|null|
|**2024-07-30**|**Event-Arguments Extraction Corpus and Modeling using BERT for Arabic**|Alaa Aljabari et.al.|[2407.21153v1](http://arxiv.org/abs/2407.21153v1)|null|
|**2024-07-30**|**Private Collaborative Edge Inference via Over-the-Air Computation**|Selim F. Yilmaz et.al.|[2407.21151v1](http://arxiv.org/abs/2407.21151v1)|null|
|**2024-07-30**|**Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population**|Mayanka Chandrashekar et.al.|[2407.21149v1](http://arxiv.org/abs/2407.21149v1)|null|

#### Abstracts
##### **Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey**
2407.21794v1 by Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin, Qing Yu, Go Irie, Shafiq Joty, Yixuan Li, Hai Li, Ziwei Liu, Toshihiko Yamasaki, Kiyoharu Aizawa

Detecting out-of-distribution (OOD) samples is crucial for ensuring the
safety of machine learning systems and has shaped the field of OOD detection.
Meanwhile, several other problems are closely related to OOD detection,
including anomaly detection (AD), novelty detection (ND), open set recognition
(OSR), and outlier detection (OD). To unify these problems, a generalized OOD
detection framework was proposed, taxonomically categorizing these five
problems. However, Vision Language Models (VLMs) such as CLIP have
significantly changed the paradigm and blurred the boundaries between these
fields, again confusing researchers. In this survey, we first present a
generalized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD
detection, and OD in the VLM era. Our framework reveals that, with some field
inactivity and integration, the demanding challenges have become OOD detection
and AD. In addition, we also highlight the significant shift in the definition,
problem settings, and benchmarks; we thus feature a comprehensive review of the
methodology for OOD detection, including the discussion over other related
tasks to clarify their relationship to OOD detection. Finally, we explore the
advancements in the emerging Large Vision Language Model (LVLM) era, such as
GPT-4V. We conclude this survey with open challenges and future directions.

摘要：偵測異常樣本 (OOD) 對於確保機器學習系統的安全性至關重要，並形塑了 OOD 偵測領域。同時，還有許多其他問題與 OOD 偵測息息相關，包括異常偵測 (AD)、新穎性偵測 (ND)、開放集識別 (OSR) 和離群值偵測 (OD)。為了統一這些問題，提出了廣義的 OOD 偵測架構，將這五個問題分類。然而，像 CLIP 等視覺語言模型 (VLM) 已大幅改變典範，並模糊了這些領域之間的界線，再次讓研究人員感到困惑。在這項調查中，我們首先提出廣義的 OOD 偵測 v2，概括了 AD、ND、OSR、OOD 偵測和 OD 在 VLM 時代的演進。我們的架構揭示，由於某些領域的不活躍和整合，具有挑戰性的問題已成為 OOD 偵測和 AD。此外，我們也重點說明定義、問題設定和基準的重大轉變；因此，我們對 OOD 偵測的方法論進行全面檢視，包括討論其他相關任務以釐清它們與 OOD 偵測的關係。最後，我們探討新興的大型視覺語言模型 (LVLM) 時代的進展，例如 GPT-4V。我們以開放挑戰和未來方向作為這項調查的結論。

##### **Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?**
2407.21792v1 by Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks

As artificial intelligence systems grow more powerful, there has been
increasing interest in "AI safety" research to address emerging and future
risks. However, the field of AI safety remains poorly defined and
inconsistently measured, leading to confusion about how researchers can
contribute. This lack of clarity is compounded by the unclear relationship
between AI safety benchmarks and upstream general capabilities (e.g., general
knowledge and reasoning). To address these issues, we conduct a comprehensive
meta-analysis of AI safety benchmarks, empirically analyzing their correlation
with general capabilities across dozens of models and providing a survey of
existing directions in AI safety. Our findings reveal that many safety
benchmarks highly correlate with upstream model capabilities, potentially
enabling "safetywashing" -- where capability improvements are misrepresented as
safety advancements. Based on these findings, we propose an empirical
foundation for developing more meaningful safety metrics and define AI safety
in a machine learning research context as a set of clearly delineated research
goals that are empirically separable from generic capabilities advancements. In
doing so, we aim to provide a more rigorous framework for AI safety research,
advancing the science of safety evaluations and clarifying the path towards
measurable progress.

摘要：隨著人工智慧系統變得越來越強大，對於「AI 安全」的研究興趣也與日俱增，以因應新興和未來的風險。然而，AI 安全領域的定義仍然很模糊，衡量標準也不一致，導致研究人員如何做出貢獻感到困惑。AI 安全基準與上游一般能力（例如一般知識和推理）之間關係不明確，進一步加劇了這種不確定性。為了解決這些問題，我們對 AI 安全基準進行了全面的後設分析，根據數十個模型實證分析它們與一般能力的相關性，並對 AI 安全中的現有方向進行調查。我們的研究結果顯示，許多安全基準與上游模型能力高度相關，這可能會導致「安全漂白」——將能力的提升誤認為是安全性的進步。根據這些研究結果，我們提出了一個實證基礎，用於開發更有意義的安全指標，並在機器學習研究背景下將 AI 安全定義為一組明確界定的研究目標，這些目標在實證上可以與一般能力的進步區分開來。透過這麼做，我們旨在為 AI 安全研究提供一個更嚴謹的架構，推進安全評估的科學，並釐清邁向可衡量進展的道路。

##### **Vision-Language Model Based Handwriting Verification**
2407.21788v1 by Mihir Chauhan, Abhishek Satbhai, Mohammad Abuzar Hashemi, Mir Basheer Ali, Bina Ramamurthy, Mingchen Gao, Siwei Lyu, Sargur Srihari

Handwriting Verification is a critical in document forensics. Deep learning
based approaches often face skepticism from forensic document examiners due to
their lack of explainability and reliance on extensive training data and
handcrafted features. This paper explores using Vision Language Models (VLMs),
such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By
leveraging their Visual Question Answering capabilities and 0-shot
Chain-of-Thought (CoT) reasoning, our goal is to provide clear,
human-understandable explanations for model decisions. Our experiments on the
CEDAR handwriting dataset demonstrate that VLMs offer enhanced
interpretability, reduce the need for large training datasets, and adapt better
to diverse handwriting styles. However, results show that the CNN-based
ResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach
with GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:
71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings
highlight the potential of VLMs in generating human-interpretable decisions
while underscoring the need for further advancements to match the performance
of specialized deep learning models.

摘要：手寫驗證在文件鑑識中至關重要。基於深度學習的方法通常會受到文件鑑識專家的懷疑，原因在於它們缺乏可解釋性，並且依賴於大量的訓練資料和手工特徵。本文探討使用視覺語言模型 (VLM)，例如 OpenAI 的 GPT-4o 和 Google 的 PaliGemma，來解決這些挑戰。通過利用它們的視覺問答能力和 0-shot 思想鏈 (CoT) 推理，我們的目標是為模型決策提供清晰、人類可以理解的解釋。我們在 CEDAR 手寫資料集上的實驗表明，VLM 提供了增強的可解釋性，減少了對大型訓練資料集的需求，並且可以更好地適應不同的手寫風格。然而，結果表明，基於 CNN 的 ResNet-18 架構優於使用 GPT-4o（準確率：70%）和監督微調 PaliGemma（準確率：71%）的 0-shot CoT 提示工程方法，在 CEDAR AND 資料集上達到了 84% 的準確率。這些發現突顯了 VLM 在產生人類可以理解的決策方面的潛力，同時也強調了進一步提升與專業深度學習模型相匹配的效能的必要性。

##### **Large Language Monkeys: Scaling Inference Compute with Repeated Sampling**
2407.21787v1 by Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, Azalia Mirhoseini

Scaling the amount of compute used to train language models has dramatically
improved their capabilities. However, when it comes to inference, we often
limit the amount of compute to only one attempt per problem. Here, we explore
inference compute as another axis for scaling by increasing the number of
generated samples. Across multiple tasks and models, we observe that coverage -
the fraction of problems solved by any attempt - scales with the number of
samples over four orders of magnitude. In domains like coding and formal
proofs, where all answers can be automatically verified, these increases in
coverage directly translate into improved performance. When we apply repeated
sampling to SWE-bench Lite, the fraction of issues solved with
DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250
samples, outperforming the single-attempt state-of-the-art of 43% which uses
more capable frontier models. Moreover, using current API pricing, amplifying
the cheaper DeepSeek model with five samples is more cost-effective and solves
more issues than paying a premium for one sample from GPT-4o or Claude 3.5
Sonnet. Interestingly, the relationship between coverage and the number of
samples is often log-linear and can be modelled with an exponentiated power
law, suggesting the existence of inference-time scaling laws. Finally, we find
that identifying correct samples out of many generations remains an important
direction for future research in domains without automatic verifiers. When
solving math word problems from GSM8K and MATH, coverage with Llama-3 models
grows to over 95% with 10,000 samples. However, common methods to pick correct
solutions from a sample collection, such as majority voting or reward models,
plateau beyond several hundred samples and fail to fully scale with the sample
budget.

摘要：<paragraph>擴大用於訓練語言模型的運算量已大幅提升其功能。然而，在進行推論時，我們通常將運算量限制在每個問題僅嘗試一次。在此，我們將推論運算視為另一種擴展軸，藉由增加生成範例的數量來進行擴展。在多個任務和模型中，我們觀察到覆蓋率（任何嘗試解決問題的分數）會隨著範例數量而擴展，超過四個數量級。在編碼和形式化證明等領域中，所有答案都可以自動驗證，這些覆蓋率的增加會直接轉化為效能的提升。當我們將重複抽樣套用於 SWE-bench Lite 時，使用 DeepSeek-V2-Coder-Instruct 解決問題的分數從一個範例的 15.9% 提升到 250 個範例的 56%，優於使用功能更強大的前沿模型而達到的 43% 單次嘗試最先進水準。此外，使用目前的 API 定價，以五個範例擴充較便宜的 DeepSeek 模型比支付溢價取得 GPT-4o 或 Claude 3.5 Sonnet 的一個範例更具成本效益，且能解決更多問題。有趣的是，覆蓋率與範例數量之間的關係通常是對數線性的，且可用指數冪律建模，這表示存在推論時間擴展律。最後，我們發現從許多世代中找出正確範例仍然是沒有自動驗證器的領域中未來研究的重要方向。在解決 GSM8K 和 MATH 的數學文字題時，使用 Llama-3 模型的覆蓋率會在 10,000 個範例中成長到超過 95%。然而，從範例集合中挑選正確解答的常見方法（例如多數決或獎勵模型）會在數百個範例後達到平穩期，且無法完全隨著範例預算而擴展。</paragraph>

##### **The Llama 3 Herd of Models**
2407.21783v1 by Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao

Modern artificial intelligence (AI) systems are powered by foundation models.
This paper presents a new set of foundation models, called Llama 3. It is a
herd of language models that natively support multilinguality, coding,
reasoning, and tool usage. Our largest model is a dense Transformer with 405B
parameters and a context window of up to 128K tokens. This paper presents an
extensive empirical evaluation of Llama 3. We find that Llama 3 delivers
comparable quality to leading language models such as GPT-4 on a plethora of
tasks. We publicly release Llama 3, including pre-trained and post-trained
versions of the 405B parameter language model and our Llama Guard 3 model for
input and output safety. The paper also presents the results of experiments in
which we integrate image, video, and speech capabilities into Llama 3 via a
compositional approach. We observe this approach performs competitively with
the state-of-the-art on image, video, and speech recognition tasks. The
resulting models are not yet being broadly released as they are still under
development.

摘要：現代人工智慧 (AI) 系統由基礎模型提供動力。
本文提出了一組新的基礎模型，稱為 Llama 3。它是一群語言模型，本機支援多語言、編碼、推理和工具使用。我們最大的模型是一個具有 405B 參數和最多 128K 令牌的上下文視窗的密集Transformer。本文提供了對 Llama 3 的廣泛實證評估。我們發現 Llama 3 在大量任務上提供了與 GPT-4 等領先語言模型相當的品質。我們公開發布 Llama 3，包括 405B 參數語言模型的預訓練和後訓練版本，以及我們的 Llama Guard 3 模型，用於輸入和輸出安全性。本文還提供了將影像、影片和語音功能整合到 Llama 3 中的實驗結果，方法是採用組合式方法。我們觀察到這種方法在影像、影片和語音辨識任務上表現出與最先進技術相當的競爭力。由於這些模型仍處於開發階段，因此尚未廣泛發布。

##### **Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries**
2407.21778v1 by Felix Ocker, Daniel Tanneberg, Julian Eggert, Michael Gienger

We introduce tulip agent, an architecture for autonomous LLM-based agents
with Create, Read, Update, and Delete access to a tool library containing a
potentially large number of tools. In contrast to state-of-the-art
implementations, tulip agent does not encode the descriptions of all available
tools in the system prompt, which counts against the model's context window, or
embed the entire prompt for retrieving suitable tools. Instead, the tulip agent
can recursively search for suitable tools in its extensible tool library,
implemented exemplarily as a vector store. The tulip agent architecture
significantly reduces inference costs, allows using even large tool libraries,
and enables the agent to adapt and extend its set of tools. We evaluate the
architecture with several ablation studies in a mathematics context and
demonstrate its generalizability with an application to robotics. A reference
implementation and the benchmark are available at
github.com/HRI-EU/tulip_agent.

摘要：我們介紹了 Tulip 代理，一種基於自主 LLM 的代理架構，可以對包含大量工具的工具庫進行建立、讀取、更新和刪除存取。與最先進的實作相反，Tulip 代理不會將所有可用工具的描述編碼在系統提示中，這會計入模型的內容視窗，或嵌入整個提示以擷取合適的工具。相反地，Tulip 代理可以在其可延伸工具庫中遞迴搜尋合適的工具，範例實作為向量儲存。Tulip 代理架構大幅降低了推論成本，允許使用甚至大型工具庫，並讓代理調整和延伸其工具組。我們在數學背景中使用多項消融研究評估架構，並透過機器人應用程式展示其概括性。可以在 github.com/HRI-EU/tulip_agent 取得參考實作和基準。

##### **ShieldGemma: Generative AI Content Moderation Based on Gemma**
2407.21772v1 by Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, Olivia Sturman, Oscar Wahltinez

We present ShieldGemma, a comprehensive suite of LLM-based safety content
moderation models built upon Gemma2. These models provide robust,
state-of-the-art predictions of safety risks across key harm types (sexually
explicit, dangerous content, harassment, hate speech) in both user input and
LLM-generated output. By evaluating on both public and internal benchmarks, we
demonstrate superior performance compared to existing models, such as Llama
Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%).
Additionally, we present a novel LLM-based data curation pipeline, adaptable to
a variety of safety-related tasks and beyond. We have shown strong
generalization performance for model trained mainly on synthetic data. By
releasing ShieldGemma, we provide a valuable resource to the research
community, advancing LLM safety and enabling the creation of more effective
content moderation solutions for developers.

摘要：我們展示 ShieldGemma，這是一套建構於 Gemma2 的全面 LLM 安全內容審核模型套件。這些模型提供健全、最先進的安全性風險預測，涵蓋使用者輸入和 LLM 產生的輸出中的主要危害類型（露骨性內容、危險內容、騷擾、仇恨言論）。透過在公開和內部基準上進行評估，我們展示出優於現有模型的卓越效能，例如 Llama Guard（在公開基準上 +10.8% AU-PRC）和 WildCard（+4.3%）。此外，我們提出一個新穎的 LLM 資料策管流程，適用於各種安全相關任務及其他任務。我們已展示出對於主要訓練在合成資料上的模型的強大泛化效能。透過釋出 ShieldGemma，我們為研究社群提供了寶貴的資源，推進 LLM 安全性並協助開發人員建立更有效的內容審核解決方案。

##### **MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts**
2407.21770v1 by Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, Armen Aghajanyan

We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)
architecture designed for pre-training mixed-modal, early-fusion language
models. MoMa processes images and text in arbitrary sequences by dividing
expert modules into modality-specific groups. These groups exclusively process
designated tokens while employing learned routing within each group to maintain
semantically informed adaptivity. Our empirical results reveal substantial
pre-training efficiency gains through this modality-specific parameter
allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,
featuring 4 text experts and 4 image experts, achieves impressive FLOPs
savings: 3.7x overall, with 2.6x for text and 5.2x for image processing
compared to a compute-equivalent dense baseline, measured by pre-training loss.
This outperforms the standard expert-choice MoE with 8 mixed-modal experts,
which achieves 3x overall FLOPs savings (3x for text, 2.8x for image).
Combining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs
savings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination
hurts performance in causal inference due to increased sensitivity to router
accuracy. These results demonstrate MoMa's potential to significantly advance
the efficiency of mixed-modal, early-fusion language model pre-training, paving
the way for more resource-efficient and capable multimodal AI systems.

摘要：<paragraph>我們介紹 MoMa，一種新穎的模態感知混合專家 (MoE) 架構，專為混合模態、早期融合語言模型的預訓練而設計。MoMa 透過將專家模組分為模態特定群組，以任意順序處理影像和文字。這些群組會獨自處理指定的代碼，同時在每個群組內使用已學習的路由，以維持語義適應性。我們的實證結果顯示，透過這種模態特定參數配置，可大幅提升預訓練效率。在 1 兆個代碼的訓練預算下，MoMa 1.4B 模型配備 4 個文字專家和 4 個影像專家，可節省令人驚豔的 FLOP：整體而言為 3.7 倍，文字處理為 2.6 倍，影像處理為 5.2 倍，這是以預訓練損失測量，並與運算等效的密集基準線相比較。這優於標準的專家選擇 MoE，後者配備 8 個混合模態專家，可節省整體 FLOP 3 倍（文字為 3 倍，影像為 2.8 倍）。將 MoMa 與混合深度 (MoD) 結合，可進一步將預訓練 FLOP 節省提升至整體 4.2 倍（文字：3.4 倍，影像：5.3 倍），儘管這種組合會因路由器精確度敏感度增加而損害因果推理的效能。這些結果證明了 MoMa 在提升混合模態、早期融合語言模型預訓練效率方面的潛力，為更具資源效率且功能強大的多模態 AI 系統鋪路。</paragraph>

##### **HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection**
2407.21742v1 by Junwei He, Qianqian Xu, Yangbangyan Jiang, Zitai Wang, Yuchen Sun, Qingming Huang

With the progressive advancements in deep graph learning, out-of-distribution
(OOD) detection for graph data has emerged as a critical challenge. While the
efficacy of auxiliary datasets in enhancing OOD detection has been extensively
studied for image and text data, such approaches have not yet been explored for
graph data. Unlike Euclidean data, graph data exhibits greater diversity but
lower robustness to perturbations, complicating the integration of outliers. To
tackle these challenges, we propose the introduction of \textbf{H}ybrid
External and Internal \textbf{G}raph \textbf{O}utlier \textbf{E}xposure (HGOE)
to improve graph OOD detection performance. Our framework involves using
realistic external graph data from various domains and synthesizing internal
outliers within ID subgroups to address the poor robustness and presence of OOD
samples within the ID class. Furthermore, we develop a boundary-aware OE loss
that adaptively assigns weights to outliers, maximizing the use of high-quality
OOD samples while minimizing the impact of low-quality ones. Our proposed HGOE
framework is model-agnostic and designed to enhance the effectiveness of
existing graph OOD detection models. Experimental results demonstrate that our
HGOE framework can significantly improve the performance of existing OOD
detection models across all 8 real datasets.

摘要：隨著深度圖形學習的進步，圖形資料的分布外 (OOD) 偵測已成為一項關鍵挑戰。雖然輔助資料集在增強影像和文字資料的 OOD 偵測方面已獲得廣泛研究，但此類方法尚未探索圖形資料。與歐幾里得資料不同，圖形資料展現出更大的多樣性，但對擾動的魯棒性較低，使離群值的整合複雜化。為了應對這些挑戰，我們提出引入\textbf{H}ybrid\textbf{E}xternal and \textbf{I}nternal \textbf{G}raph \textbf{O}utlier \textbf{E}xposure (HGOE) 以提升圖形 OOD 偵測效能。我們的架構涉及使用來自不同領域的真實外部圖形資料，並在 ID 子群中合成內部離群值，以解決 ID 類別中 OOD 樣本的魯棒性差和存在性問題。此外，我們開發了一種邊界感知 OE 損失，可適應性地將權重分配給離群值，最大化高品質 OOD 樣本的使用，同時最小化低品質樣本的影響。我們提出的 HGOE 架構與模型無關，旨在提升現有圖形 OOD 偵測模型的效能。實驗結果證明，我們的 HGOE 架構可以顯著提升現有 OOD 偵測模型在所有 8 個真實資料集中的效能。

##### **Contrastive Factor Analysis**
2407.21740v2 by Zhibin Duan, Tiansheng Wen, Yifei Wang, Chen Zhu, Bo Chen, Mingyuan Zhou

Factor analysis, often regarded as a Bayesian variant of matrix
factorization, offers superior capabilities in capturing uncertainty, modeling
complex dependencies, and ensuring robustness. As the deep learning era
arrives, factor analysis is receiving less and less attention due to their
limited expressive ability. On the contrary, contrastive learning has emerged
as a potent technique with demonstrated efficacy in unsupervised
representational learning. While the two methods are different paradigms,
recent theoretical analysis has revealed the mathematical equivalence between
contrastive learning and matrix factorization, providing a potential
possibility for factor analysis combined with contrastive learning. Motivated
by the interconnectedness of contrastive learning, matrix factorization, and
factor analysis, this paper introduces a novel Contrastive Factor Analysis
framework, aiming to leverage factor analysis's advantageous properties within
the realm of contrastive learning. To further leverage the interpretability
properties of non-negative factor analysis, which can learn disentangled
representations, contrastive factor analysis is extended to a non-negative
version. Finally, extensive experimental validation showcases the efficacy of
the proposed contrastive (non-negative) factor analysis methodology across
multiple key properties, including expressiveness, robustness,
interpretability, and accurate uncertainty estimation.

摘要：因子分析通常被视为矩阵分解的贝叶斯变体，在捕获不确定性、建模复杂依赖关系和确保稳健性方面具有卓越的能力。随着深度学习时代的到来，因子分析由于其表达能力有限而受到越来越少的关注。相反，对比学习已经成为一种有效的技术，在无监督表征学习中显示出功效。虽然这两种方法是不同的范式，但最近的理论分析揭示了对比学习和矩阵分解之间的数学等价性，为因子分析结合对比学习提供了潜在的可能性。受对比学习、矩阵分解和因子分析的相互联系的启发，本文介绍了一种新颖的对比因子分析框架，旨在利用因子分析在对比学习领域中的优势特性。为了进一步利用非负因子分析的可解释性，它可以学习解耦表征，将对比因子分析扩展到非负版本。最后，广泛的实验验证展示了所提出的对比（非负）因子分析方法在多个关键属性（包括表达性、鲁棒性、可解释性和准确的不确定性估计）上的有效性。

##### **A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation**
2407.21739v1 by Mothilal Asokan, Joseph Geo Benjamin, Mohammad Yaqub, Karthik Nandakumar

Adapting foundation models for medical image analysis requires finetuning
them on a considerable amount of data because of extreme distribution shifts
between natural (source) data used for pretraining and medical (target) data.
However, collecting task-specific medical data for such finetuning at a central
location raises many privacy concerns. Although Federated learning (FL)
provides an effective means for training on private decentralized data,
communication costs in federating large foundation models can quickly become a
significant bottleneck, impacting the solution's scalability. In this work, we
address this problem of efficient communication while ensuring effective
learning in FL by combining the strengths of Parameter-Efficient Fine-tuning
(PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA)
in a federated manner to adapt the Segment Anything Model (SAM) for 3D medical
image segmentation. Unlike prior works that utilize LoRA and finetune the
entire decoder, we critically analyze the contribution of each granular
component of SAM on finetuning performance. Thus, we identify specific layers
to be federated that are very efficient in terms of communication cost while
producing on-par accuracy. Our experiments show that retaining the parameters
of the SAM model (including most of the decoder) in their original state during
adaptation is beneficial because fine-tuning on small datasets tends to distort
the inherent capabilities of the underlying foundation model. On Fed-KiTS, our
approach decreases communication cost (~48x) compared to full fine-tuning while
increasing performance (~6% Dice score) in 3D segmentation tasks. Our approach
performs similar to SAMed while achieving ~2.8x reduction in communication and
parameters to be finetuned. We further validate our approach with experiments
on Fed-IXI and Prostate MRI datasets.

摘要：<paragraph>由於預訓練所用的自然（來源）資料和醫療（目標）資料之間的極端分佈轉移，因此將基礎模型調整用於醫學影像分析需要在大量資料上對其進行微調。
然而，在中心位置收集此類微調的特定任務醫療資料會引發許多隱私問題。儘管聯合學習 (FL) 提供了一種在私有分散式資料上進行訓練的有效方法，但在聯合大型基礎模型時，通訊成本可能會迅速成為一個重大瓶頸，影響解決方案的可擴充性。在這項工作中，我們通過結合參數高效微調 (PEFT) 和 FL 的優勢，解決了在確保 FL 中有效學習的同時進行高效通訊的問題。具體來說，我們以聯合的方式研究即插即用低秩適配器 (LoRA)，以調整區段任何模型 (SAM) 以進行 3D 醫學影像分割。與利用 LoRA 和微調整個解碼器的先前工作不同，我們批判性地分析了 SAM 的每個粒狀組成部分對微調效能的貢獻。因此，我們確定了在通訊成本方面非常高效的特定層，同時產生了同等的準確度。我們的實驗表明，在調整過程中將 SAM 模型的參數（包括大部分解碼器）保留在其原始狀態是有益的，因為在小型資料集上進行微調往往會扭曲基礎模型的內在能力。在 Fed-KiTS 上，與完全微調相比，我們的做法降低了通訊成本（約 48 倍），同時提高了 3D 分割任務中的效能（約 6% 的骰子分數）。我們的做法與 SAMed 類似，同時將通訊和待微調參數減少了約 2.8 倍。我們進一步通過在 Fed-IXI 和 Prostate MRI 資料集上進行實驗驗證了我們的做法。</paragraph>

##### **Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos**
2407.21738v1 by Joseph Geo Benjamin, Mothilal Asokan, Amna Alhosani, Hussain Alasmawi, Werner Gerhard Diehl, Leanne Bricker, Karthik Nandakumar, Mohammad Yaqub

Self-supervised learning (SSL) methods are popular since they can address
situations with limited annotated data by directly utilising the underlying
data distribution. However, the adoption of such methods is not explored enough
in ultrasound (US) imaging, especially for fetal assessment. We investigate the
potential of dual-encoder SSL in utilizing unlabelled US video data to improve
the performance of challenging downstream Standard Fetal Cardiac Planes (SFCP)
classification using limited labelled 2D US images. We study 7 SSL approaches
based on reconstruction, contrastive loss, distillation, and information theory
and evaluate them extensively on a large private US dataset. Our observations
and findings are consolidated from more than 500 downstream training
experiments under different settings. Our primary observation shows that for
SSL training, the variance of the dataset is more crucial than its size because
it allows the model to learn generalisable representations, which improve the
performance of downstream tasks. Overall, the BarlowTwins method shows robust
performance, irrespective of the training settings and data variations, when
used as an initialisation for downstream tasks. Notably, full fine-tuning with
1% of labelled data outperforms ImageNet initialisation by 12% in F1-score and
outperforms other SSL initialisations by at least 4% in F1-score, thus making
it a promising candidate for transfer learning from US video to image data.

摘要：自监督学习 (SSL) 方法很受欢迎，因为它们可以通过直接利用底层数据分布来解决带注释数据有限的情况。然而，在超声 (US) 成像中，特别是对于胎儿评估，对这种方法的采用尚未得到充分探索。我们研究了双编码器 SSL 在利用未标记的 US 视频数据以使用有限标记的 2D US 图像提高具有挑战性的下游标准胎儿心脏平面 (SFCP) 分类性能方面的潜力。我们研究了 7 种基于重建、对比损失、蒸馏和信息论的 SSL 方法，并在一个大型私有 US 数据集上对它们进行了广泛的评估。我们的观察和发现来自 500 多个不同设置下的下游训练实验。我们的主要观察结果表明，对于 SSL 训练，数据集的方差比其大小更重要，因为它允许模型学习可推广的表示，从而提高下游任务的性能。总体而言，BarlowTwins 方法表现出稳健的性能，无论训练设置和数据变化如何，当用作下游任务的初始化时。值得注意的是，使用 1% 的标记数据进行完全微调在 F1 分数上优于 ImageNet 初始化 12%，并且在 F1 分数上优于其他 SSL 初始化至少 4%，因此使其成为从 US 视频到图像数据的迁移学习的有希望的候选者。

##### **Open-Vocabulary Audio-Visual Semantic Segmentation**
2407.21721v1 by Ruohao Guo, Liao Qu, Dantong Niu, Yanyu Qi, Wenzhen Yue, Ji Shi, Bowei Xing, Xianghua Ying

Audio-visual semantic segmentation (AVSS) aims to segment and classify
sounding objects in videos with acoustic cues. However, most approaches operate
on the close-set assumption and only identify pre-defined categories from
training data, lacking the generalization ability to detect novel categories in
practical applications. In this paper, we introduce a new task: open-vocabulary
audio-visual semantic segmentation, extending AVSS task to open-world scenarios
beyond the annotated label space. This is a more challenging task that requires
recognizing all categories, even those that have never been seen nor heard
during training. Moreover, we propose the first open-vocabulary AVSS framework,
OV-AVSS, which mainly consists of two parts: 1) a universal sound source
localization module to perform audio-visual fusion and locate all potential
sounding objects and 2) an open-vocabulary classification module to predict
categories with the help of the prior knowledge from large-scale pre-trained
vision-language models. To properly evaluate the open-vocabulary AVSS, we split
zero-shot training and testing subsets based on the AVSBench-semantic
benchmark, namely AVSBench-OV. Extensive experiments demonstrate the strong
segmentation and zero-shot generalization ability of our model on all
categories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on base
categories and 29.14% mIoU on novel categories, exceeding the state-of-the-art
zero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%.
The code is available at https://github.com/ruohaoguo/ovavss.

摘要：<paragraph>音視語義分割（AVSS）旨在使用聽覺線索對影片中的發聲物體進行分割和分類。然而，大多數方法都運作在封閉集合假設上，且僅從訓練資料中識別預先定義的類別，缺乏在實際應用中偵測新類別的概化能力。在本文中，我們引入了新任務：開放詞彙音視語義分割，將 AVSS 任務擴展到標註標籤空間之外的開放世界場景。這是一項更具挑戰性的任務，需要辨識所有類別，即使是那些在訓練期間從未見過或聽過的類別。此外，我們提出了第一個開放詞彙 AVSS 架構 OV-AVSS，它主要包含兩個部分：1) 一個通用音源定位模組，用於執行音視融合並定位所有潛在發聲物體，以及 2) 一個開放詞彙分類模組，在來自大型預訓練視覺語言模型的先驗知識的幫助下預測類別。為了適當地評估開放詞彙 AVSS，我們根據 AVSBench-semantic 基準（即 AVSBench-OV）拆分了零次學習訓練和測試子集。大量的實驗證明了我們的模型在所有類別上具有強大的分割和零次學習概化能力。在 AVSBench-OV 資料集上，OV-AVSS 在基礎類別上達到 55.43% mIoU，在新穎類別上達到 29.14% mIoU，比最先進的零次學習方法高出 41.88%/20.61%，比開放詞彙方法高出 10.2%/11.6%。程式碼可在 https://github.com/ruohaoguo/ovavss 取得。</paragraph>

##### **Social Learning through Interactions with Other Agents: A Survey**
2407.21713v1 by Dylan hillier, Cheston Tan, Jing Jiang

Social learning plays an important role in the development of human
intelligence. As children, we imitate our parents' speech patterns until we are
able to produce sounds; we learn from them praising us and scolding us; and as
adults, we learn by working with others. In this work, we survey the degree to
which this paradigm -- social learning -- has been mirrored in machine
learning. In particular, since learning socially requires interacting with
others, we are interested in how embodied agents can and have utilised these
techniques. This is especially in light of the degree to which recent advances
in natural language processing (NLP) enable us to perform new forms of social
learning. We look at how behavioural cloning and next-token prediction mirror
human imitation, how learning from human feedback mirrors human education, and
how we can go further to enable fully communicative agents that learn from each
other. We find that while individual social learning techniques have been used
successfully, there has been little unifying work showing how to bring them
together into socially embodied agents.

摘要：社會學習在人類智能發展中扮演著重要的角色。身為孩童時，我們模仿父母的說話模式，直到我們能夠發出聲音；我們從他們的讚美與責罵中學習；而身為成年人時，我們從與他人合作中學習。在這項工作中，我們調查了這個典範（社會學習）在機器學習中被反映的程度。特別地，由於社會學習需要與他人互動，我們有興趣了解具身代理如何能夠並已經利用這些技術。這特別是鑑於自然語言處理 (NLP) 的最新進展使我們能夠執行新的社會學習形式。我們探討了行為複製和下一個代幣預測如何反映人類模仿、從人類回饋中學習如何反映人類教育，以及我們如何進一步實現能夠彼此學習的完全溝通代理。我們發現，儘管個別的社會學習技術已被成功使用，但鮮少有統一的工作展示如何將它們帶入社會具身代理中。

##### **Adaptive Retrieval-Augmented Generation for Conversational Systems**
2407.21712v1 by Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz

Despite the success of integrating large language models into the development
of conversational systems, many studies have shown the effectiveness of
retrieving and augmenting external knowledge for informative responses. Hence,
many existing studies commonly assume the always need for Retrieval Augmented
Generation (RAG) in a conversational system without explicit control. This
raises a research question about such a necessity. In this study, we propose to
investigate the need for each turn of system response to be augmented with
external knowledge. In particular, by leveraging human judgements on the binary
choice of adaptive augmentation, we develop RAGate, a gating model, which
models conversation context and relevant inputs to predict if a conversational
system requires RAG for improved responses. We conduct extensive experiments on
devising and applying RAGate to conversational models and well-rounded analyses
of different conversational scenarios. Our experimental results and analysis
indicate the effective application of RAGate in RAG-based conversational
systems in identifying system responses for appropriate RAG with high-quality
responses and a high generation confidence. This study also identifies the
correlation between the generation's confidence level and the relevance of the
augmented knowledge.

摘要：儘管大型語言模型整合到對話系統開發中獲得成功，許多研究已顯示擷取和擴充外部知識對於提供資訊性回應的有效性。因此，許多現有研究通常假設對話系統中總是需要檢索擴充生成 (RAG)，而沒有明確的控制。這引發了關於這種必要性的研究問題。在本研究中，我們建議探討系統回應的每個轉折是否都需要擴充外部知識。特別是，透過利用人類對適應性擴充的二元選擇的判斷，我們開發了 RAGate，一種閘控模型，該模型對話語境和相關輸入進行建模，以預測對話系統是否需要 RAG 來改善回應。我們對設計和將 RAGate 應用於對話模型進行廣泛的實驗，並對不同的對話場景進行全面的分析。我們的實驗結果和分析表明 RAGate 在基於 RAG 的對話系統中有效應用於識別系統回應，以適當的 RAG 獲得高品質的回應和高生成信心。本研究還確定了生成信心水準與擴充知識相關性之間的關聯性。

##### **CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**
2407.21708v1 by Stefan Langer, Fabian Neuhaus, Andreas Nürnberger

Ontologies are formal representations of knowledge in specific domains that
provide a structured framework for organizing and understanding complex
information. Creating ontologies, however, is a complex and time-consuming
endeavor. ChEBI is a well-known ontology in the field of chemistry, which
provides a comprehensive resource for defining chemical entities and their
properties. However, it covers only a small fraction of the rapidly growing
knowledge in chemistry and does not provide references to the scientific
literature. To address this, we propose a methodology that involves augmenting
existing annotated text corpora with knowledge from Chebi and fine-tuning a
large language model (LLM) to recognize chemical entities and their roles in
scientific text. Our experiments demonstrate the effectiveness of our approach.
By combining ontological knowledge and the language understanding capabilities
of LLMs, we achieve high precision and recall rates in identifying both the
chemical entities and roles in scientific literature. Furthermore, we extract
them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a
knowledge graph (KG) of chemical entities and roles (CEAR), which provides
complementary information to ChEBI, and can help to extend it.

摘要：本体是特定領域中知識的形式化表示，它提供了一個結構化的框架，用於組織和理解複雜的資訊。然而，建立本体是一項複雜且耗時的努力。ChEBI 是化學領域中一個著名的本体，它提供了一個全面的資源，用於定義化學實體及其屬性。然而，它僅涵蓋了化學領域快速增長的知識中的一小部分，並且沒有提供科學文獻的參考。為了解決這個問題，我們提出了一種方法，它涉及使用來自 Chebi 的知識擴充現有的註釋文本語料庫，並微調大型語言模型 (LLM)，以識別化學實體及其在科學文本中的作用。我們的實驗證明了我們方法的有效性。透過結合本体知識和 LLM 的語言理解能力，我們在識別科學文獻中的化學實體和作用方面達到了很高的準確度和召回率。此外，我們從一組 8,000 篇 ChemRxiv 文章中提取它們，並應用第二個 LLM 來建立一個化學實體和作用 (CEAR) 的知識圖譜 (KG)，它提供補充 ChEBI 的資訊，並有助於擴充它。

##### **TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities**
2407.21693v1 by Ming Zhang, Caishuang Huang, Yilong Wu, Shichun Liu, Huiyuan Zheng, Yurui Dong, Yujiong Shen, Shihan Dou, Jun Zhao, Junjie Ye, Qi Zhang, Tao Gui, Xuanjing Huang

Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented
conversations, including information gathering. How to utilize ToD accurately,
efficiently and effectively for information gathering has always been a
critical and challenging task. Recent studies have demonstrated that Large
Language Models (LLMs) excel in dialogue, instruction generation, and
reasoning, and can significantly enhance the performance of TOD through
fine-tuning. However, current datasets primarily cater to user-led systems and
are limited to predefined specific scenarios and slots, thereby necessitating
improvements in the proactiveness, diversity, and capabilities of TOD. In this
study, we present a detailed multi-domain task-oriented data construction
process for conversations, and a Chinese dialogue dataset generated based on
this process, \textbf{TransferTOD}, which authentically simulates human-machine
dialogues in 30 popular life service scenarios. Leveraging this dataset, we
trained a \textbf{TransferTOD-7B} model using full-parameter fine-tuning,
showcasing notable abilities in slot filling and questioning. Our work has
demonstrated its strong generalization capabilities in various downstream
scenarios, significantly enhancing both data utilization efficiency and system
performance. The data is released in
https://github.com/KongLongGeFDU/TransferTOD.

摘要：<paragraph>面向任務的對話 (TOD) 系統旨在有效地處理面向任務的對話，包括資訊收集。如何準確、有效地利用 TOD 進行資訊收集一直是一項關鍵且具有挑戰性的任務。最近的研究表明，大型語言模型 (LLM) 在對話、指令生成和推理方面表現出色，並且可以透過微調顯著提升 TOD 的效能。然而，目前的資料集主要迎合使用者主導的系統，並且僅限於預定義的特定場景和時段，因此有必要提升 TOD 的主動性、多樣性和能力。在本研究中，我們提出了一個詳細的多領域面向任務資料建構流程，以及一個基於此流程生成的中文對話資料集，\textbf{TransferTOD}，它真實地模擬了 30 個流行生活服務場景中的人機對話。利用此資料集，我們使用全參數微調訓練了一個 \textbf{TransferTOD-7B} 模型，展示了在時段填補和提問方面的顯著能力。我們的研究展示了其在各種下游場景中的強大泛化能力，顯著提升了資料利用率和系統效能。資料已發佈在 https://github.com/KongLongGeFDU/TransferTOD。</paragraph>

##### **Dynamic Object Queries for Transformer-based Incremental Object Detection**
2407.21687v1 by Jichuan Zhang, Wei Li, Shuang Cheng, Ya-Li Li, Shengjin Wang

Incremental object detection (IOD) aims to sequentially learn new classes,
while maintaining the capability to locate and identify old ones. As the
training data arrives with annotations only with new classes, IOD suffers from
catastrophic forgetting. Prior methodologies mainly tackle the forgetting issue
through knowledge distillation and exemplar replay, ignoring the conflict
between limited model capacity and increasing knowledge. In this paper, we
explore \textit{dynamic object queries} for incremental object detection built
on Transformer architecture. We propose the \textbf{Dy}namic object
\textbf{Q}uery-based \textbf{DE}tection \textbf{TR}ansformer (DyQ-DETR), which
incrementally expands the model representation ability to achieve
stability-plasticity tradeoff. First, a new set of learnable object queries are
fed into the decoder to represent new classes. These new object queries are
aggregated with those from previous phases to adapt both old and new knowledge
well. Second, we propose the isolated bipartite matching for object queries in
different phases, based on disentangled self-attention. The interaction among
the object queries at different phases is eliminated to reduce inter-class
confusion. Thanks to the separate supervision and computation over object
queries, we further present the risk-balanced partial calibration for effective
exemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly
surpasses the state-of-the-art methods, with limited parameter overhead. Code
will be made publicly available.

摘要：增量式物件偵測 (IOD) 旨在循序漸進地學習新類別，同時維持定位和辨識舊類別的能力。由於訓練資料僅附有新類別的註解，IOD 會發生災難性遺忘。先前的研究方法主要透過知識萃取和範例重播來解決遺忘問題，卻忽略了模型容量有限和知識增加之間的衝突。在本文中，我們探索建立在 Transformer 架構上的增量式物件偵測的「動態物件查詢」。我們提出以動態物件查詢為基礎的偵測變形金剛 (DyQ-DETR)，它會逐步擴展模型表示能力，以達成穩定性和可塑性的折衷。首先，將一組新的可學習物件查詢輸入解碼器，以表示新類別。這些新的物件查詢會與前一階段的查詢合併，以適當地調整舊有和新的知識。其次，我們提出基於解開自我注意力的、用於不同階段物件查詢的孤立二部匹配。消除不同階段物件查詢之間的互動，以減少類別間的混淆。由於物件查詢有獨立的監督和運算，我們進一步提出風險平衡的部分校正，以有效地重播範例。大量的實驗證明，DyQ-DETR 大幅超越最先進的方法，且參數開銷有限。程式碼將公開提供。

##### **Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation**
2407.21674v1 by Krishan Agyakari Raja Babu, Rachana Sathish, Mrunal Pattanaik, Rahul Venkataramani

Synthetic data is becoming increasingly integral in data-scarce fields such
as medical imaging, serving as a substitute for real data. However, its
inherent statistical characteristics can significantly impact downstream tasks,
potentially compromising deployment performance. In this study, we empirically
investigate this issue and uncover a critical phenomenon: downstream neural
networks often exploit spurious distinctions between real and synthetic data
when there is a strong correlation between the data source and the task label.
This exploitation manifests as \textit{simplicity bias}, where models overly
rely on superficial features rather than genuine task-related complexities.
Through principled experiments, we demonstrate that the source of data (real
vs.\ synthetic) can introduce spurious correlating factors leading to poor
performance during deployment when the correlation is absent. We first
demonstrate this vulnerability on a digit classification task, where the model
spuriously utilizes the source of data instead of the digit to provide an
inference. We provide further evidence of this phenomenon in a medical imaging
problem related to cardiac view classification in echocardiograms, particularly
distinguishing between 2-chamber and 4-chamber views. Given the increasing role
of utilizing synthetic datasets, we hope that our experiments serve as
effective guidelines for the utilization of synthetic datasets in model
training.

摘要：合成資料在資料稀少的領域中變得越來越不可或缺，例如醫學影像，用作真實資料的替代品。然而，其內在的統計特性會顯著影響下游任務，可能損害部署效能。在本研究中，我們實證調查此問題，並揭露一個關鍵現象：當資料來源與任務標籤之間有很強的相關性時，下游神經網路通常會利用真實資料與合成資料之間的虛假區別。這種利用表現為「簡化偏差」，其中模型過度依賴表面特徵，而不是真正的與任務相關的複雜性。透過有原則的實驗，我們證明資料來源（真實資料與合成資料）可能會引入虛假的相關因素，導致在相關性不存在時部署期間效能不佳。我們首先在數字分類任務中證明此漏洞，其中模型虛假地利用資料來源而非數字來提供推論。我們在與超音波心臟視野分類相關的醫學影像問題中進一步提供此現象的證據，特別是區分二腔和四腔視野。鑑於合成資料集的使用角色日益增加，我們希望我們的實驗能作為在模型訓練中利用合成資料集的有效指南。

##### **Universal Approximation Theory: Foundations for Parallelism in Neural Networks**
2407.21670v1 by Wei Wang, Qing Li

Neural networks are increasingly evolving towards training large models with
big data, a method that has demonstrated superior performance across many
tasks. However, this approach introduces an urgent problem: current deep
learning models are predominantly serial, meaning that as the number of network
layers increases, so do the training and inference times. This is unacceptable
if deep learning is to continue advancing. Therefore, this paper proposes a
deep learning parallelization strategy based on the Universal Approximation
Theorem (UAT). From this foundation, we designed a parallel network called
Para-Former to test our theory. Unlike traditional serial models, the inference
time of Para-Former does not increase with the number of layers, significantly
accelerating the inference speed of multi-layer networks. Experimental results
validate the effectiveness of this network.

摘要：神經網路正朝訓練大型模型與巨量資料的方向發展，這是一種在許多任務中展現出優異效能的方法。然而，這種方法帶來了一個迫切的問題：目前深度學習模型主要是串列的，這表示網路層數越多，訓練和推論時間也會越長。如果深度學習要繼續進步，這是無法接受的。因此，本文提出一個基於泛函逼近定理 (UAT) 的深度學習並行化策略。基於此基礎，我們設計了一個稱為 Para-Former 的平行網路來測試我們的理論。與傳統串列模型不同，Para-Former 的推論時間不會隨著層數增加而增加，大幅加速多層網路的推論速度。實驗結果驗證了此網路的有效性。

##### **Synth-Empathy: Towards High-Quality Synthetic Empathy Data**
2407.21669v1 by Hao Liang, Linzhuang Sun, Jingxuan Wei, Xijie Huang, Linkun Sun, Bihui Yu, Conghui He, Wentao Zhang

In recent years, with the rapid advancements in large language models (LLMs),
achieving excellent empathetic response capabilities has become a crucial
prerequisite. Consequently, managing and understanding empathetic datasets have
gained increasing significance. However, empathetic data are typically
human-labeled, leading to insufficient datasets and wasted human labor. In this
work, we present Synth-Empathy, an LLM-based data generation and quality and
diversity selection pipeline that automatically generates high-quality
empathetic data while discarding low-quality data. With the data generated from
a low empathetic model, we are able to further improve empathetic response
performance and achieve state-of-the-art (SoTA) results across multiple
benchmarks. Moreover, our model achieves SoTA performance on various human
evaluation benchmarks, demonstrating its effectiveness and robustness in
real-world applications. Furthermore, we show the trade-off between data
quantity and quality, providing insights into empathetic data generation and
selection.

摘要：近年來，隨著大型語言模型 (LLM) 的快速進展，實現優異的同理心回應能力已成為一項關鍵先決條件。因此，管理和理解同理心資料集變得越來越重要。然而，同理心資料通常是由人類標記，導致資料集不足且浪費人力。在這項工作中，我們提出了 Synth-Empathy，這是一個基於 LLM 的資料生成、品質和多樣性選擇管道，可以自動生成高品質的同理心資料，同時捨棄低品質資料。透過從低同理心模型生成的資料，我們能夠進一步改善同理心回應表現，並在多個基準測試中達到最先進 (SoTA) 的結果。此外，我們的模型在各種人類評估基準測試中都達到了 SoTA 的表現，證明了其在實際應用中的有效性和穩健性。此外，我們展示了資料數量和品質之間的權衡，提供了對同理心資料生成和選擇的見解。

##### **An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification**
2407.21666v1 by Aswini Kumar Patra, Ankit Varshney, Lingaraj Sahoo

Early detection of drought stress is critical for taking timely measures for
reducing crop loss before the drought impact becomes irreversible. The subtle
phenotypical and physiological changes in response to drought stress are
captured by non-invasive imaging techniques and these imaging data serve as
valuable resource for machine learning methods to identify drought stress.
While convolutional neural networks (CNNs) are in wide use, vision transformers
(ViTs) present a promising alternative in capturing long-range dependencies and
intricate spatial relationships, thereby enhancing the detection of subtle
indicators of drought stress. We propose an explainable deep learning pipeline
that leverages the power of ViTs for drought stress detection in potato crops
using aerial imagery. We applied two distinct approaches: a synergistic
combination of ViT and support vector machine (SVM), where ViT extracts
intricate spatial features from aerial images, and SVM classifies the crops as
stressed or healthy and an end-to-end approach using a dedicated classification
layer within ViT to directly detect drought stress. Our key findings explain
the ViT model's decision-making process by visualizing attention maps. These
maps highlight the specific spatial features within the aerial images that the
ViT model focuses as the drought stress signature. Our findings demonstrate
that the proposed methods not only achieve high accuracy in drought stress
identification but also shedding light on the diverse subtle plant features
associated with drought stress. This offers a robust and interpretable solution
for drought stress monitoring for farmers to undertake informed decisions for
improved crop management.

摘要：及早偵測乾旱壓力，對於在乾旱影響變得不可逆轉之前採取適時措施以減少作物損失至關重要。非侵入式影像技術可捕捉到對乾旱壓力產生的細微表型和生理變化，而這些影像資料可作為機器學習方法識別乾旱壓力的寶貴資源。雖然卷積神經網路 (CNN) 被廣泛使用，但視覺Transformer (ViT) 在捕捉長程依賴關係和複雜空間關係方面提供了有前景的替代方案，從而增強了對乾旱壓力的細微指標的偵測。我們提出了一個可解釋的深度學習管道，它利用 ViT 的功能來偵測馬鈴薯作物中的乾旱壓力，並使用航拍影像。我們應用兩種不同的方法：ViT 和支援向量機 (SVM) 的協同組合，其中 ViT 從航拍影像中萃取複雜的空間特徵，而 SVM 將作物分類為受壓或健康，以及使用 ViT 內的專用分類層的端到端方法來直接偵測乾旱壓力。我們的關鍵發現透過視覺化注意力圖來解釋 ViT 模型的決策過程。這些圖突顯了 ViT 模型關注的航拍影像中的特定空間特徵，作為乾旱壓力的特徵。我們的發現表明，所提出的方法不僅在乾旱壓力識別中實現了高準確度，而且還闡明了與乾旱壓力相關的各種細微植物特徵。這為乾旱壓力監控提供了一個強大且可解釋的解決方案，讓農民可以做出明智的決策，以改善作物管理。

##### **Defending Jailbreak Attack in VLMs via Cross-modality Information Detector**
2407.21659v2 by Yue Xu, Xiuyuan Qi, Zhan Qin, Wenjie Wang

Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively
understand vision information, achieving remarkable performance in many
vision-centric tasks. Despite that, recent studies have shown that these models
are susceptible to jailbreak attacks, which refer to an exploitative technique
where malicious users can break the safety alignment of the target model and
generate misleading and harmful answers. This potential threat is caused by
both the inherent vulnerabilities of LLM and the larger attack scope introduced
by vision input. To enhance the security of VLMs against jailbreak attacks,
researchers have developed various defense techniques. However, these methods
either require modifications to the model's internal structure or demand
significant computational resources during the inference phase. Multimodal
information is a double-edged sword. While it increases the risk of attacks, it
also provides additional data that can enhance safeguards. Inspired by this, we
propose $\underline{\textbf{C}}$ross-modality
$\underline{\textbf{I}}$nformation
$\underline{\textbf{DE}}$tecto$\underline{\textbf{R}}$ ($\textit{CIDER})$, a
plug-and-play jailbreaking detector designed to identify maliciously perturbed
image inputs, utilizing the cross-modal similarity between harmful queries and
adversarial images. This simple yet effective cross-modality information
detector, $\textit{CIDER}$, is independent of the target VLMs and requires less
computation cost. Extensive experimental results demonstrate the effectiveness
and efficiency of $\textit{CIDER}$, as well as its transferability to both
white-box and black-box VLMs.

摘要：<paragraph>視覺語言模型 (VLM) 擴展了 LLM 全面理解視覺資訊的能力，在許多以視覺為中心的任务中取得顯著的表現。儘管如此，最近的研究顯示，這些模型容易受到越獄攻擊，這是一種剝削技術，惡意使用者可以破壞目標模型的安全對齊，並產生具有誤導性和危害性的答案。這種潛在威脅是由 LLM 的固有漏洞和視覺輸入引入的更大攻擊範圍所造成的。為了增強 VLM 對抗越獄攻擊的安全性，研究人員開發了各種防禦技術。然而，這些方法需要修改模型的內部結構，或在推理階段需要大量的計算資源。多模態資訊是一把雙面刃。雖然它增加了攻擊風險，但它也提供了可以增強防護措施的額外資料。受此啟發，我們提出跨模態資訊偵測器 ($\textit{CIDER}$)，這是一個即插即用的越獄偵測器，旨在識別惡意擾動的影像輸入，利用有害查詢和對抗性影像之間的跨模態相似性。這個簡單但有效的跨模態資訊偵測器 $\textit{CIDER}$ 獨立於目標 VLM，並且需要較少的計算成本。廣泛的實驗結果證明了 $\textit{CIDER}$ 的有效性和效率，以及它對白盒和黑盒 VLM 的可移植性。</paragraph>

##### **Spatial Transformer Network YOLO Model for Agricultural Object Detection**
2407.21652v1 by Yash Zambre, Ekdev Rajkitkul, Akshatha Mohan, Joshua Peeples

Object detection plays a crucial role in the field of computer vision by
autonomously identifying and locating objects of interest. The You Only Look
Once (YOLO) model is an effective single-shot detector. However, YOLO faces
challenges in cluttered or partially occluded scenes and can struggle with
small, low-contrast objects. We propose a new method that integrates spatial
transformer networks (STNs) into YOLO to improve performance. The proposed
STN-YOLO aims to enhance the model's effectiveness by focusing on important
areas of the image and improving the spatial invariance of the model before the
detection process. Our proposed method improved object detection performance
both qualitatively and quantitatively. We explore the impact of different
localization networks within the STN module as well as the robustness of the
model across different spatial transformations. We apply the STN-YOLO on
benchmark datasets for Agricultural object detection as well as a new dataset
from a state-of-the-art plant phenotyping greenhouse facility. Our code and
dataset are publicly available.

摘要：目標偵測在電腦視覺領域中扮演著關鍵角色，它能自主辨識並定位感興趣的目標。You Only Look Once (YOLO) 模型是一個有效的單次偵測器。然而，YOLO 在雜亂或部分遮擋的場景中會面臨挑戰，並且在處理小而對比度低的目標時會遇到困難。我們提出了一種新的方法，將空間轉換網路 (STN) 整合到 YOLO 中以提升效能。所提出的 STN-YOLO 旨在透過在偵測過程前專注於影像的重要區域並改善模型的空間不變性，來提升模型的效能。我們提出的方法在質量和數量上都改善了目標偵測效能。我們探討了 STN 模組中不同定位網路的影響，以及模型在不同空間轉換中的穩健性。我們將 STN-YOLO 應用於農業目標偵測的基準資料集，以及來自最先進的植物表型溫室設施的新資料集。我們的程式碼和資料集已公開。

##### **Human interaction classifier for LLM based chatbot**
2407.21647v1 by Diego Martín, Jordi Sanchez, Xavier Vizcaíno

This study investigates different approaches to classify human interactions
in an artificial intelligence-based environment, specifically for Applus+
IDIADA's intelligent agent AIDA. The main objective is to develop a classifier
that accurately identifies the type of interaction received (Conversation,
Services, or Document Translation) to direct requests to the appropriate
channel and provide a more specialized and efficient service. Various models
are compared, including LLM-based classifiers, KNN using Titan and Cohere
embeddings, SVM, and artificial neural networks. Results show that SVM and ANN
models with Cohere embeddings achieve the best overall performance, with
superior F1 scores and faster execution times compared to LLM-based approaches.
The study concludes that the SVM model with Cohere embeddings is the most
suitable option for classifying human interactions in the AIDA environment,
offering an optimal balance between accuracy and computational efficiency.

摘要：本研究探討了在基於人工智慧的環境中對人類互動進行分類的不同方法，特別是針對 Applus+ IDIADA 的智慧代理人 AIDA。主要目標是開發一個分類器，能精確識別接收到的互動類型（對話、服務或文件翻譯），以將請求導向適當的管道，並提供更專業且有效率的服務。比較了各種模型，包括基於 LLM 的分類器、使用 Titan 和 Cohere 內嵌的 KNN、SVM 和人工神經網路。結果顯示，採用 Cohere 內嵌的 SVM 和 ANN 模型可達成最佳整體效能，與基於 LLM 的方法相比，具有優異的 F1 分數和更快的執行時間。研究結論是，採用 Cohere 內嵌的 SVM 模型最適合在 AIDA 環境中對人類互動進行分類，在準確度和運算效率之間取得最佳平衡。

##### **Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent**
2407.21646v1 by Shanbo Cheng, Zhichao Huang, Tom Ko, Hang Li, Ningxin Peng, Lu Xu, Qini Zhang

In this paper, we present Cross Language Agent -- Simultaneous
Interpretation, CLASI, a high-quality and human-like Simultaneous Speech
Translation (SiST) System. Inspired by professional human interpreters, we
utilize a novel data-driven read-write strategy to balance the translation
quality and latency. To address the challenge of translating in-domain
terminologies, CLASI employs a multi-modal retrieving module to obtain relevant
information to augment the translation. Supported by LLMs, our approach can
generate error-tolerated translation by considering the input audio, historical
context, and retrieved information. Experimental results show that our system
outperforms other systems by significant margins. Aligned with professional
human interpreters, we evaluate CLASI with a better human evaluation metric,
valid information proportion (VIP), which measures the amount of information
that can be successfully conveyed to the listeners. In the real-world
scenarios, where the speeches are often disfluent, informal, and unclear, CLASI
achieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese
translation directions, respectively. In contrast, state-of-the-art commercial
or open-source systems only achieve 35.4% and 41.6%. On the extremely hard
dataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%
VIP.

摘要：<paragraph>在本文中，我們提出跨語言代理——同聲傳譯 (CLAS)，一個高品質且類似人類的同聲傳譯 (SiST) 系統。受到專業人類口譯員的啟發，我們利用一種新穎的數據驅動讀寫策略來平衡翻譯品質和延遲。為了應對翻譯領域內術語的挑戰，CLAS 採用多模式檢索模組來獲取相關資訊以擴充翻譯。在大型語言模型 (LLM) 的支援下，我們的做法可以透過考量輸入音訊、歷史脈絡和檢索資訊來產生容錯翻譯。實驗結果顯示，我們的系統以顯著的幅度優於其他系統。與專業人類口譯員保持一致，我們使用更好的人類評量指標——有效資訊比例 (VIP) 來評量 CLAS，該指標衡量可以成功傳達給聽眾的資訊量。在現實世界場景中，演講通常不流暢、非正式且不清不楚，CLAS 在中譯英和英譯中翻譯方向分別達到 81.3% 和 78.0% 的 VIP。相比之下，最先進的商用或開源系統僅達到 35.4% 和 41.6%。在極困難的資料集上，其他系統的 VIP 低於 13%，CLAS 仍能達到 70% 的 VIP。</paragraph>

##### **Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components**
2407.21638v1 by Hermione Warr, Yasin Ibrahim, Daniel R. McGowan, Konstantinos Kamnitsas

Automation of medical image interpretation could alleviate bottlenecks in
diagnostic workflows, and has become of particular interest in recent years due
to advancements in natural language processing. Great strides have been made
towards automated radiology report generation via AI, yet ensuring clinical
accuracy in generated reports is a significant challenge, hindering deployment
of such methods in clinical practice. In this work we propose a quality control
framework for assessing the reliability of AI-generated radiology reports with
respect to semantics of diagnostic importance using modular auxiliary auditing
components (AC). Evaluating our pipeline on the MIMIC-CXR dataset, our findings
show that incorporating ACs in the form of disease-classifiers can enable
auditing that identifies more reliable reports, resulting in higher F1 scores
compared to unfiltered generated reports. Additionally, leveraging the
confidence of the AC labels further improves the audit's effectiveness.

摘要：醫療影像判讀的自動化可以減輕診斷工作流程中的瓶頸，並且由於自然語言處理的進步，在近年來特別受到重視。在透過 AI 自動生成放射線報告方面已經取得了長足的進展，然而確保生成報告的臨床準確性是一項重大的挑戰，阻礙了此類方法在臨床實務中的部署。在這項工作中，我們提出了一個品質控制架構，用於評估 AI 生成的放射線報告的可靠性，並使用模組化輔助稽核元件 (AC) 針對診斷重要性的語義進行評估。在 MIMIC-CXR 資料集上評估我們的管道，我們的發現顯示，以疾病分類器的形式納入 AC 可以啟用稽核，以識別更可靠的報告，與未經篩選的生成報告相比，會產生更高的 F1 分數。此外，進一步利用 AC 標籤的信心可以提高稽核的有效性。

##### **Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation**
2407.21633v1 by Xiang Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang

Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to
transition to unfamiliar domains without manual annotation or extensive
retraining. Prior research has approached this objective by embedding prompts
into language models (LMs). Common methodologies include integrating prompts at
the input layer or introducing learnable variables at each transformer layer.
Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at
the input layer risk underutilization, with their impact potentially
diminishing across successive transformer layers. Conversely, the addition of
learnable variables to each layer can complicate the training process and
increase inference latency. To tackle the issues mentioned above, this paper
proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture
designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank
Adaptation (LoRA) components, targeting both dialogue context processing and
prompt optimization, to ensure the comprehensive influence of prompts
throughout the transformer model layers. This is achieved without incurring
additional inference latency, showcasing an efficient integration into existing
architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets,
DualLoRA demonstrates notable improvements across multiple domains,
outperforming traditional baseline methods in zero-shot settings. Our code is
accessible at: \url{https://github.com/suntea233/DualLoRA}.

摘要：零次發話狀態追蹤 (DST) 旨在讓對話系統能夠在沒有手動註解或廣泛重新訓練的情況下轉換到不熟悉的領域。先前的研究透過將提示嵌入語言模型 (LM) 來達成此目標。常見的方法包括在輸入層整合提示或在每個Transformer層中引入可學習變數。儘管如此，每種策略都展現出固有的限制。整合在輸入層的提示有未充分利用的風險，它們的影響可能會隨著後續的Transformer層而遞減。相反地，在每個層中加入可學習變數會使訓練過程複雜化並增加推論延遲。為了解決上述問題，本文提出雙重低秩適應 (DualLoRA)，這是一種專為零次 DST 設計的即插即用架構。DualLoRA 結合了兩個不同的低秩適應 (LoRA) 元件，針對對話內容處理和提示最佳化，以確保提示在Transformer模型層中全面發揮影響力。這是在不產生額外推論延遲的情況下實現的，展示了與現有架構的有效整合。透過在 MultiWOZ 和 SGD 資料集上的嚴格評估，DualLoRA 在多個領域展現出顯著的改進，在零次設定中優於傳統的基準方法。我們的程式碼可於此處取得：\url{https://github.com/suntea233/DualLoRA}。

##### **TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods**
2407.21630v1 by Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi

Authorship obfuscation aims to disguise the identity of an author within a
text by altering the writing style, vocabulary, syntax, and other linguistic
features associated with the text author. This alteration needs to balance
privacy and utility. While strong obfuscation techniques can effectively hide
the author's identity, they often degrade the quality and usefulness of the
text for its intended purpose. Conversely, maintaining high utility tends to
provide insufficient privacy, making it easier for an adversary to de-anonymize
the author. Thus, achieving an optimal trade-off between these two conflicting
objectives is crucial. In this paper, we propose TAROT: Task-Oriented
Authorship Obfuscation Using Policy Optimization, a new unsupervised authorship
obfuscation method whose goal is to optimize the privacy-utility trade-off by
regenerating the entire text considering its downstream utility. Our approach
leverages policy optimization as a fine-tuning paradigm over small language
models in order to rewrite texts by preserving author identity and downstream
task utility. We show that our approach largely reduce the accuracy of
attackers while preserving utility. We make our code and models publicly
available.

摘要：作者混淆旨在通过改变与文本作者相关的写作风格、词汇、语法和其他语言特征来隐藏作者在文本中的身份。这种改变需要平衡隐私和实用性。虽然强大的混淆技术可以有效地隐藏作者的身份，但它们通常会降低文本的质量和实用性，使其无法达到预期目的。相反，维持高实用性往往会提供不足的隐私，使得对手更容易对作者进行去匿名化。因此，在两个相互冲突的目标之间取得最佳平衡至关重要。在本文中，我们提出了 TAROT：面向任务的作者混淆，使用策略优化，这是一种新的无监督作者混淆方法，其目标是通过重新生成考虑其下游实用性的整个文本来优化隐私实用性权衡。我们的方法利用策略优化作为小型语言模型上的微调范式，以便通过保留作者身份和下游任务实用性来重写文本。我们表明，我们的方法在保留实用性的同时大大降低了攻击者的准确性。我们将我们的代码和模型公开提供。

##### **Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music**
2407.21615v1 by Pedro Sarmento, Jackson Loth, Mathieu Barthet

Generative AI models have recently blossomed, significantly impacting
artistic and musical traditions. Research investigating how humans interact
with and deem these models is therefore crucial. Through a listening and
reflection study, we explore participants' perspectives on AI- vs
human-generated progressive metal, in symbolic format, using rock music as a
control group. AI-generated examples were produced by ProgGP, a
Transformer-based model. We propose a mixed methods approach to assess the
effects of generation type (human vs. AI), genre (progressive metal vs. rock),
and curation process (random vs. cherry-picked). This combines quantitative
feedback on genre congruence, preference, creativity, consistency, playability,
humanness, and repeatability, and qualitative feedback to provide insights into
listeners' experiences. A total of 32 progressive metal fans completed the
study. Our findings validate the use of fine-tuning to achieve genre-specific
specialization in AI music generation, as listeners could distinguish between
AI-generated rock and progressive metal. Despite some AI-generated excerpts
receiving similar ratings to human music, listeners exhibited a preference for
human compositions. Thematic analysis identified key features for genre and AI
vs. human distinctions. Finally, we consider the ethical implications of our
work in promoting musical data diversity within MIR research by focusing on an
under-explored genre.

摘要：生成式 AI 模型近期蓬勃發展，對藝術和音樂傳統產生重大影響。因此，探討人類如何與這些模型互動並評斷它們的研究至關重要。透過聆聽與反思研究，我們以符號格式探討參與者對 AI 與人類產生的前衛金屬音樂的觀點，並以搖滾音樂作為對照組。AI 生成的範例由 ProgGP，一個基於 Transformer 的模型製作。我們提出一個混合方法來評估生成類型（人類與 AI）、類型（前衛金屬與搖滾）和策展流程（隨機與精選）的影響。這結合了對類型一致性、偏好、創造力、一致性、可演奏性、人性化和可重複性的量化回饋，以及定性回饋，以提供對聽眾體驗的見解。共有 32 位前衛金屬樂迷完成這項研究。我們的發現驗證了微調在 AI 音樂生成中實現特定類型專業化的用途，因為聽眾可以區分 AI 生成的搖滾和前衛金屬。儘管一些 AI 生成的片段獲得與人類音樂類似的評分，但聽眾表現出對人類作品的偏好。主題分析識別出類型和 AI 與人類區別的主要特徵。最後，我們考慮了我們的工作在促進 MIR 研究中的音樂資料多樣性方面的倫理影響，重點放在一個探索不足的類型上。

##### **Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism**
2407.21611v1 by Jiafeng Zhong, Bin Li, Jiangyan Yi

The task of partially spoofed audio localization aims to accurately determine
audio authenticity at a frame level. Although some works have achieved
encouraging results, utilizing boundary information within a single model
remains an unexplored research topic. In this work, we propose a novel method
called Boundary-aware Attention Mechanism (BAM). Specifically, it consists of
two core modules: Boundary Enhancement and Boundary Frame-wise Attention. The
former assembles the intra-frame and inter-frame information to extract
discriminative boundary features that are subsequently used for boundary
position detection and authenticity decision, while the latter leverages
boundary prediction results to explicitly control the feature interaction
between frames, which achieves effective discrimination between real and fake
frames. Experimental results on PartialSpoof database demonstrate our proposed
method achieves the best performance. The code is available at
https://github.com/media-sec-lab/BAM.

摘要：部分欺骗音频定位的任务旨在准确确定帧级别的音频真实性。尽管一些作品取得了令人鼓舞的结果，但在单个模型中利用边界信息仍然是一个尚未探索的研究课题。在这项工作中，我们提出了一种名为边界感知注意力机制 (BAM) 的新方法。具体来说，它包含两个核心模块：边界增强和边界逐帧注意力。前者汇集帧内和帧间信息以提取判别边界特征，随后用于边界位置检测和真实性决策，而后者利用边界预测结果显式控制帧之间的特征交互，从而实现了对真实帧和虚假帧的有效区分。PartialSpoof 数据库上的实验结果表明，我们提出的方法取得了最佳性能。代码可在 https://github.com/media-sec-lab/BAM 获得。

##### **Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors**
2407.21600v1 by Shoujin Huang, Guanxiong Luo, Yuwan Wang, Kexin Yang, Lingyan Zhang, Jingzhe Liu, Hua Guo, Min Wang, Mengye Lyu

Simultaneous multislice (SMS) imaging is a powerful technique for
accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS
reconstruction remains challenging due to the complex signal interactions
between and within the excited slices. This study presents a robust SMS MRI
reconstruction method using deep generative priors. Starting from Gaussian
noise, we leverage denoising diffusion probabilistic models (DDPM) to gradually
recover the individual slices through reverse diffusion iterations while
imposing data consistency from the measured k-space under readout concatenation
framework. The posterior sampling procedure is designed such that the DDPM
training can be performed on single-slice images without special adjustments
for SMS tasks. Additionally, our method integrates a low-frequency enhancement
(LFE) module to address a practical issue that SMS-accelerated fast spin echo
(FSE) and echo-planar imaging (EPI) sequences cannot easily embed
autocalibration signals. Extensive experiments demonstrate that our approach
consistently outperforms existing methods and generalizes well to unseen
datasets. The code is available at https://github.com/Solor-pikachu/ROGER after
the review process.

摘要：同時多切片 (SMS) 影像是一種強大的技術，用於加速磁振造影 (MRI) 的擷取。然而，由於激發切片之間和之內的複雜訊號交互作用，SMS 重建仍然具有挑戰性。本研究提出了一種使用深度生成先驗的強健 SMS MRI 重建方法。從高斯雜訊開始，我們利用去噪擴散機率模型 (DDPM) 透過反向擴散反覆運算來逐漸恢復個別切片，同時在讀取串接架構下施加測量 k 空間的資料一致性。後驗抽樣程序的設計使得 DDPM 訓練可以在單切片影像上執行，無需針對 SMS 任務進行特殊調整。此外，我們的模型整合了一個低頻增強 (LFE) 模組，以解決一個實際問題，即 SMS 加速的快速自旋回波 (FSE) 和回波平面影像 (EPI) 序列無法輕鬆嵌入自動校正訊號。大量的實驗證明，我們的模型始終優於現有方法，並且可以很好地推廣到未見的資料集。程式碼在審查程序後可於 https://github.com/Solor-pikachu/ROGER 取得。

##### **Voxel Scene Graph for Intracranial Hemorrhage**
2407.21580v1 by Antoine P. Sanner, Nils F. Grauhan, Marc A. Brockmann, Ahmed E. Othman, Anirban Mukhopadhyay

Patients with Intracranial Hemorrhage (ICH) face a potentially
life-threatening condition, and patient-centered individualized treatment
remains challenging due to possible clinical complications. Deep-Learning-based
methods can efficiently analyze the routinely acquired head CTs to support the
clinical decision-making. The majority of early work focuses on the detection
and segmentation of ICH, but do not model the complex relations between ICH and
adjacent brain structures. In this work, we design a tailored object detection
method for ICH, which we unite with segmentation-grounded Scene Graph
Generation (SGG) methods to learn a holistic representation of the clinical
cerebral scene. To the best of our knowledge, this is the first application of
SGG for 3D voxel images. We evaluate our method on two head-CT datasets and
demonstrate that our model can recall up to 74% of clinically relevant
relations. This work lays the foundation towards SGG for 3D voxel data. The
generated Scene Graphs can already provide insights for the clinician, but are
also valuable for all downstream tasks as a compact and interpretable
representation.

摘要：腦出血 (ICH) 患者面臨可能危及生命的狀況，由於可能的臨床併發症，以患者為中心的個人化治療仍然具有挑戰性。基於深度學習的方法可以有效分析常規獲得的頭部電腦斷層掃描，以支持臨床決策制定。大多數早期工作都集中在 ICH 的檢測和分割，但沒有對 ICH 和相鄰大腦結構之間的複雜關係進行建模。在這項工作中，我們設計了一種針對 ICH 的客製化目標檢測方法，我們將其與基於分割的場景圖生成 (SGG) 方法結合，以學習臨床腦部場景的整體表徵。據我們所知，這是 SGG 第一次應用於 3D 體素影像。我們在兩個頭部電腦斷層掃描數據集上評估我們的模型，並證明我們的模型可以召回高達 74% 的臨床相關關係。這項工作為 3D 體素數據的 SGG 奠定了基礎。生成的場景圖已經可以為臨床醫生提供見解，但對於所有下游任務而言，它也是一種精簡且可解釋的表徵，因此非常有價值。

##### **A Performance Study of LLM-Generated Code on Leetcode**
2407.21579v1 by Tristan Coignion, Clément Quinton, Romain Rouvoy

This study evaluates the efficiency of code generation by Large Language
Models (LLMs) and measures their performance against human-crafted solutions
using a dataset from Leetcode. We compare 18 LLMs, considering factors such as
model temperature and success rate, and their impact on code performance. This
research introduces a novel method for measuring and comparing the speed of
LLM-generated code, revealing that LLMs produce code with comparable
performance, irrespective of the adopted LLM. We also find that LLMs are
capable of generating code that is, on average, more efficient than the code
written by humans. The paper further discusses the use of Leetcode as a
benchmarking dataset, the limitations imposed by potential data contamination,
and the platform's measurement reliability. We believe that our findings
contribute to a better understanding of LLM capabilities in code generation and
set the stage for future optimizations in the field.

摘要：本研究評估大型語言模型 (LLM) 的程式碼產生效率，並使用來自 Leetcode 的資料集衡量它們與人工製作的解決方案的效能。我們比較了 18 個 LLM，考量了模型溫度和成功率等因素，以及它們對程式碼效能的影響。本研究提出了一種新的方法來衡量和比較 LLM 生成的程式碼的速度，揭示了 LLM 產生的程式碼具有可比較的效能，而與採用的 LLM 無關。我們還發現，LLM 能夠產生的程式碼平均而言比人工編寫的程式碼更有效率。本文進一步討論了使用 Leetcode 作為基準資料集、潛在資料污染所造成的限制，以及該平台的測量可靠性。我們相信我們的研究結果有助於更了解 LLM 在程式碼產生方面的能力，並為該領域的未來最佳化奠定基礎。

##### **Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography**
2407.21577v1 by Kit M. Bransby, Woo-jin Cho Kim, Jorge Oliveira, Alex Thorley, Arian Beqiri, Alberto Gomez, Agisilaos Chartsias

Building an echocardiography view classifier that maintains performance in
real-life cases requires diverse multi-site data, and frequent updates with
newly available data to mitigate model drift. Simply fine-tuning on new
datasets results in "catastrophic forgetting", and cannot adapt to variations
of view labels between sites. Alternatively, collecting all data on a single
server and re-training may not be feasible as data sharing agreements may
restrict image transfer, or datasets may only become available at different
times. Furthermore, time and cost associated with re-training grows with every
new dataset. We propose a class-incremental learning method which learns an
expert network for each dataset, and combines all expert networks with a score
fusion model. The influence of ``unqualified experts'' is minimised by
weighting each contribution with a learnt in-distribution score. These weights
promote transparency as the contribution of each expert is known during
inference. Instead of using the original images, we use learned features from
each dataset, which are easier to share and raise fewer licensing and privacy
concerns. We validate our work on six datasets from multiple sites,
demonstrating significant reductions in training time while improving view
classification performance.

摘要：建立一個在實際情況中能維持效能的心臟超音波檢視分類器，需要多樣化的多站點資料，以及頻繁使用新取得的資料進行更新，以減輕模型漂移。單純針對新的資料集進行微調會導致「災難性遺忘」，且無法適應站點之間視圖標籤的差異。或者，在單一伺服器上收集所有資料並重新訓練可能不可行，因為資料分享協議可能會限制影像傳輸，或資料集可能只在不同的時間點取得。此外，每次使用新的資料集進行重新訓練所花費的時間和成本都會增加。我們提出了一種類別遞增學習方法，會為每個資料集學習一個專家網路，並將所有專家網路與一個分數融合模型結合。透過使用學習到的分佈內分數對每個貢獻進行加權，可以將「不合格專家」的影響降到最低。這些權重會提升透明度，因為在推論過程中會知道每個專家的貢獻。我們不使用原始影像，而是使用從每個資料集中學習到的特徵，這些特徵更容易分享，且會引發較少的授權和隱私問題。我們在多個站點的六個資料集上驗證我們的成果，證明在改善視圖分類效能的同時，大幅減少了訓練時間。

##### **PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning**
2407.21571v1 by Min Jae Jung, JooHee Kim

Large Language Models (LLMs) encounter significant challenges in continual
learning due to catastrophic forgetting, where new information overwrites
previously acquired knowledge. This limitation leads to substantial
environmental and economic waste. In this study, we introduce the PMoE,
Progressive Mixture of Experts with Asymmetric Transformer, which aims to
minimize forgetting by utilizing an asymmetric design with shallow layers
dedicated to general knowledge and deep layers for new knowledge. PMoE
incorporates progressively added experts in deep layers and a router that
allocates new knowledge to the appropriate experts efficiently. The router,
positioned adjacent to the deep layers, utilizes deep features aggregating
consolidated information. This enables the router to perform efficiently,
allocating new knowledge to the appropriate experts, which progressively
increase in the deep layers. Extensive experiments on TRACE datasets and
general language understanding datasets demonstrate that the proposed PMoE
outperforms previous state-of-the-art approaches.

摘要：大型語言模型 (LLM) 在持續學習中會遭遇重大挑戰，因為災難性遺忘會導致新資訊覆蓋先前獲得的知識。此限制會造成大量的環境和經濟浪費。本研究中，我們引入了 PMoE，一種非對稱Transformer的專家漸進混合，其目標是透過使用非對稱設計來最小化遺忘，其中淺層專門用於一般知識，而深層則用於新知識。PMoE 在深層中加入漸進新增的專家，以及一個路由器，可有效地將新知識分配給適當的專家。路由器與深層相鄰，使用整合後的資訊彙總深度特徵。這讓路由器得以有效執行，將新知識分配給適當的專家，而這些專家會在深層中逐漸增加。在 TRACE 資料集和一般語言理解資料集上的廣泛實驗證明，所提出的 PMoE 優於先前的最先進方法。

##### **Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding**
2407.21560v1 by Jun Zhou, Dongyang Yu, Kamran Aziz, Fangfang Su, Qing Zhang, Fei Li, Donghong Ji

Fine-grained sentiment analysis involves extracting and organizing sentiment
elements from textual data. However, existing approaches often overlook issues
of category semantic inclusion and overlap, as well as inherent structural
patterns within the target sequence. This study introduces a generative
sentiment analysis model. To address the challenges related to category
semantic inclusion and overlap, a latent category distribution variable is
introduced. By reconstructing the input of a variational autoencoder, the model
learns the intensity of the relationship between categories and text, thereby
improving sequence generation. Additionally, a trie data structure and
constrained decoding strategy are utilized to exploit structural patterns,
which in turn reduces the search space and regularizes the generation process.
Experimental results on the Restaurant-ACOS and Laptop-ACOS datasets
demonstrate a significant performance improvement compared to baseline models.
Ablation experiments further confirm the effectiveness of latent category
distribution and constrained decoding strategy.

摘要：細粒度情緒分析涉及從文本資料中萃取和組織情緒元素。然而，現有方法經常忽略類別語義包含和重疊的問題，以及目標序列內在的結構模式。本研究引入一個生成式情緒分析模型。為了應對與類別語義包含和重疊相關的挑戰，引入了潛在類別分佈變數。透過重建變異自動編碼器的輸入，模型學習類別與文字之間關係的強度，從而改善序列生成。此外，利用樹狀結構和約束式解碼策略來利用結構模式，進而減少搜尋空間並規範生成過程。在 Restaurant-ACOS 和 Laptop-ACOS 資料集上的實驗結果證明與基線模型相比有顯著的效能提升。消融實驗進一步確認了潛在類別分佈和約束式解碼策略的有效性。

##### **Operator-based semantics for choice programs: is choosing losing? (full version)**
2407.21556v1 by Jesse Heyninck

Choice constructs are an important part of the language of logic programming,
yet the study of their semantics has been a challenging task. So far, only
two-valued semantics have been studied, and the different proposals for such
semantics have not been compared in a principled way. In this paper, an
operator-based framework allow for the definition and comparison of different
semantics in a principled way is proposed.

摘要：選擇建構是邏輯程式設計語言中重要的部分，
然而研究其語意學一直是一項具有挑戰性的任務。目前為止，
僅研究了二值語意學，且對於此類語意學的不同提案尚未以有原則的方式進行比較。本文中，
提出了基於運算子的架構，允許以有原則的方式定義和比較不同的語意學。

##### **Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition**
2407.21536v1 by Jiang Li, Xiaoping Wang, Zhigang Zeng

Multimodal emotion recognition in conversation (MERC) has garnered
substantial research attention recently. Existing MERC methods face several
challenges: (1) they fail to fully harness direct inter-modal cues, possibly
leading to less-than-thorough cross-modal modeling; (2) they concurrently
extract information from the same and different modalities at each network
layer, potentially triggering conflicts from the fusion of multi-source data;
(3) they lack the agility required to detect dynamic sentimental changes,
perhaps resulting in inaccurate classification of utterances with abrupt
sentiment shifts. To address these issues, a novel approach named GraphSmile is
proposed for tracking intricate emotional cues in multimodal dialogues.
GraphSmile comprises two key components, i.e., GSF and SDP modules. GSF
ingeniously leverages graph structures to alternately assimilate inter-modal
and intra-modal emotional dependencies layer by layer, adequately capturing
cross-modal cues while effectively circumventing fusion conflicts. SDP is an
auxiliary task to explicitly delineate the sentiment dynamics between
utterances, promoting the model's ability to distinguish sentimental
discrepancies. Furthermore, GraphSmile is effortlessly applied to multimodal
sentiment analysis in conversation (MSAC), forging a unified multimodal
affective model capable of executing MERC and MSAC tasks. Empirical results on
multiple benchmarks demonstrate that GraphSmile can handle complex emotional
and sentimental patterns, significantly outperforming baseline models.

摘要：多模态对话情感识别（MERC）最近引起了大量研究关注。现有的 MERC 方法面临着一些挑战：（1）它们未能充分利用直接的模态间线索，可能导致不太彻底的跨模态建模；（2）它们在每个网络层同时从相同和不同的模态中提取信息，可能引发来自多源数据融合的冲突；（3）它们缺乏检测动态情感变化所需的敏捷性，可能导致对情感急剧转变的话语进行不准确的分类。为了解决这些问题，提出了一种名为 GraphSmile 的新方法，用于追踪多模态对话中的复杂情感线索。GraphSmile 包含两个关键组件，即 GSF 和 SDP 模块。GSF 巧妙地利用图结构逐层交替同化模态间和模态内情感依赖关系，充分捕捉跨模态线索，同时有效规避融合冲突。SDP 是一项辅助任务，用于明确描绘话语之间的情感动态，提升模型区分情感差异的能力。此外，GraphSmile 可以毫不费力地应用于对话中的多模态情感分析（MSAC），打造一个统一的多模态情感模型，能够执行 MERC 和 MSAC 任务。在多个基准上的实证结果表明，GraphSmile 可以处理复杂的情感和情感模式，明显优于基线模型。

##### **Can LLMs "Reason" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation**
2407.21531v1 by Ziya Zhou, Yuhang Wu, Zhiyue Wu, Xinyue Zhang, Ruibin Yuan, Yinghao Ma, Lu Wang, Emmanouil Benetos, Wei Xue, Yike Guo

Symbolic Music, akin to language, can be encoded in discrete symbols. Recent
research has extended the application of large language models (LLMs) such as
GPT-4 and Llama2 to the symbolic music domain including understanding and
generation. Yet scant research explores the details of how these LLMs perform
on advanced music understanding and conditioned generation, especially from the
multi-step reasoning perspective, which is a critical aspect in the
conditioned, editable, and interactive human-computer co-creation process. This
study conducts a thorough investigation of LLMs' capability and limitations in
symbolic music processing. We identify that current LLMs exhibit poor
performance in song-level multi-step music reasoning, and typically fail to
leverage learned music knowledge when addressing complex musical tasks. An
analysis of LLMs' responses highlights distinctly their pros and cons. Our
findings suggest achieving advanced musical capability is not intrinsically
obtained by LLMs, and future research should focus more on bridging the gap
between music knowledge and reasoning, to improve the co-creation experience
for musicians.

摘要：符號音樂類似於語言，可以用離散符號編碼。最近的研究將大型語言模型 (LLM)，例如 GPT-4 和 Llama2，的應用擴展到符號音樂領域，包括理解和生成。然而，很少有研究探討這些 LLM 如何執行進階音樂理解和條件生成，特別是從多步驟推理的角度來看，這是條件式、可編輯和互動式人機共同創作過程中的一個關鍵面向。本研究對 LLM 在符號音樂處理中的能力和限制進行了徹底的調查。我們發現，目前的 LLM 在歌曲層級的多步驟音樂推理中表現不佳，並且在處理複雜的音樂任務時通常無法利用所學的音樂知識。對 LLM 回應的分析突出了它們的優缺點。我們的研究結果表明，LLM 本質上並未獲得進階的音樂能力，未來的研究應更專注於彌合音樂知識和推理之間的差距，以改善音樂家的共同創作體驗。

##### **Data Contamination Report from the 2024 CONDA Shared Task**
2407.21530v1 by Oscar Sainz, Iker García-Ferrero, Alon Jacovi, Jon Ander Campos, Yanai Elazar, Eneko Agirre, Yoav Goldberg, Wei-Lin Chen, Jenny Chim, Leshem Choshen, Luca D'Amico-Wong, Melissa Dell, Run-Ze Fan, Shahriar Golchin, Yucheng Li, Pengfei Liu, Bhavish Pahwa, Ameya Prabhu, Suryansh Sharma, Emily Silcock, Kateryna Solonko, David Stap, Mihai Surdeanu, Yu-Min Tseng, Vishaal Udandarao, Zengzhi Wang, Ruijie Xu, Jinglin Yang

The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant
aspects of data contamination in natural language processing, where data
contamination is understood as situations where evaluation data is included in
pre-training corpora used to train large scale models, compromising evaluation
results. The workshop fostered a shared task to collect evidence on data
contamination in current available datasets and models. The goal of the shared
task and associated database is to assist the community in understanding the
extent of the problem and to assist researchers in avoiding reporting
evaluation results on known contaminated resources. The shared task provides a
structured, centralized public database for the collection of contamination
evidence, open to contributions from the community via GitHub pool requests.
This first compilation paper is based on 566 reported entries over 91
contaminated sources from a total of 23 contributors. The details of the
individual contamination events are available in the platform. The platform
continues to be online, open to contributions from the community.

摘要：第一屆資料污染工作坊（CONDA 2024）專注於自然語言處理中資料污染的所有相關面向，其中資料污染被理解為評量資料包含在用於訓練大型模型的預訓練語料庫中，進而影響評量結果的情況。工作坊促進了一項共同任務，以收集證據，了解當前可用資料集和模型中的資料污染。共同任務和相關資料庫的目標是協助社群了解問題的程度，並協助研究人員避免回報已知受污染資源的評量結果。共同任務提供一個結構化、集中的公開資料庫，用於收集污染證據，並開放社群透過 GitHub 池請求提供貢獻。這份第一份彙編論文是根據來自 23 位貢獻者的 91 個受污染來源所回報的 566 個條目。個別污染事件的詳細資訊可在平台上取得。平台持續上線，開放社群提供貢獻。

##### **Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI**
2407.21523v1 by Lingxi Cui, Huan Li, Ke Chen, Lidan Shou, Gang Chen

Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant
high-quality tabular data for model training remains a significant obstacle.
Numerous works have focused on tabular data augmentation (TDA) to enhance the
original table with additional data, thereby improving downstream ML tasks.
Recently, there has been a growing interest in leveraging the capabilities of
generative AI for TDA. Therefore, we believe it is time to provide a
comprehensive review of the progress and future prospects of TDA, with a
particular emphasis on the trending generative AI. Specifically, we present an
architectural view of the TDA pipeline, comprising three main procedures:
pre-augmentation, augmentation, and post-augmentation. Pre-augmentation
encompasses preparation tasks that facilitate subsequent TDA, including error
handling, table annotation, table simplification, table representation, table
indexing, table navigation, schema matching, and entity matching. Augmentation
systematically analyzes current TDA methods, categorized into retrieval-based
methods, which retrieve external data, and generation-based methods, which
generate synthetic data. We further subdivide these methods based on the
granularity of the augmentation process at the row, column, cell, and table
levels. Post-augmentation focuses on the datasets, evaluation and optimization
aspects of TDA. We also summarize current trends and future directions for TDA,
highlighting promising opportunities in the era of generative AI. In addition,
the accompanying papers and related resources are continuously updated and
maintained in the GitHub repository at
https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation to reflect
ongoing advancements in the field.

摘要：表格資料的機器學習 (ML) 無所不在，但取得大量的優質表格資料以進行模型訓練仍然是一項重大的障礙。許多研究專注於表格資料擴充 (TDA)，以使用額外的資料來增強原始表格，從而改善下游 ML 任務。最近，人們對利用生成式 AI 的功能進行 TDA 產生越來越大的興趣。因此，我們相信現在是時候對 TDA 的進展和未來前景進行全面回顧，特別強調趨勢性的生成式 AI。具體來說，我們提出了 TDA 管線的架構視圖，包含三個主要程序：預擴充、擴充和後擴充。預擴充包含有助於後續 TDA 的準備任務，包括錯誤處理、表格註解、表格簡化、表格表示、表格索引、表格導覽、架構比對和實體比對。擴充系統性地分析目前的 TDA 方法，分類為基於檢索的方法（檢索外部資料）和基於生成的的方法（生成合成資料）。我們進一步根據列、欄、儲存格和表格層級的擴充程序粒度細分這些方法。後擴充專注於 TDA 的資料集、評估和最佳化面向。我們也總結了 TDA 目前的趨勢和未來方向，強調了生成式 AI 時代充滿希望的機會。此外，隨附的論文和相關資源會持續更新並保存在 GitHub 儲存庫中，網址為 https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation，以反映該領域的持續進展。

##### **Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images**
2407.21516v1 by I. M. Chernenkiy, Y. A. Drach, S. R. Mustakimova, V. V. Kazantseva, N. A. Ushakov, S. K. Efetov, M. V. Feldsherov

Colorectal cancer is the third-most common cancer in the Western Hemisphere.
The segmentation of colorectal and colorectal cancer by computed tomography is
an urgent problem in medicine. Indeed, a system capable of solving this problem
will enable the detection of colorectal cancer at early stages of the disease,
facilitate the search for pathology by the radiologist, and significantly
accelerate the process of diagnosing the disease. However, scientific
publications on medical image processing mostly use closed, non-public data.
This paper presents an extension of the Medical Decathlon dataset with
colorectal markups in order to improve the quality of segmentation algorithms.
An experienced radiologist validated the data, categorized it into subsets by
quality, and published it in the public domain. Based on the obtained results,
we trained neural network models of the UNet architecture with 5-part
cross-validation and achieved a Dice metric quality of $0.6988 \pm 0.3$. The
published markups will improve the quality of colorectal cancer detection and
simplify the radiologist's job for study description.

摘要：大腸癌是西半球第三常見的癌症。
利用電腦斷層掃描對大腸癌與大腸癌進行分段是醫學上的緊急問題。事實上，一個能夠解決這個問題的系統將能夠在疾病的早期階段偵測大腸癌，協助放射科醫師尋找病理，並顯著加速診斷疾病的過程。然而，關於醫學影像處理的科學刊物大多使用封閉、非公開的資料。這篇論文提出了一個帶有大腸標記的醫學十項全能資料集的延伸，以提高分段演算法的品質。一位經驗豐富的放射科醫師驗證了資料，將其依品質分類成子集，並將其發布在公共領域。根據獲得的結果，我們訓練了具有 5 部分交叉驗證的 UNet 架構的神經網路模型，並達到了 $0.6988 \pm 0.3$ 的 Dice 指標品質。發布的標記將提高大腸癌偵測的品質，並簡化放射科醫師研究描述的工作。

##### **Interpreting and learning voice commands with a Large Language Model for a robot system**
2407.21512v1 by Stanislau Stankevich, Wojciech Dudek

Robots are increasingly common in industry and daily life, such as in nursing
homes where they can assist staff. A key challenge is developing intuitive
interfaces for easy communication. The use of Large Language Models (LLMs) like
GPT-4 has enhanced robot capabilities, allowing for real-time interaction and
decision-making. This integration improves robots' adaptability and
functionality. This project focuses on merging LLMs with databases to improve
decision-making and enable knowledge acquisition for request interpretation
problems.

摘要：機器人越來越普遍於工業和日常生活中，例如在護理之家，它們可以協助員工。一項關鍵挑戰是開發直覺式介面，以便輕鬆溝通。使用大型語言模型 (LLM)，例如 GPT-4，增強了機器人的能力，允許即時互動和決策制定。此整合改善了機器人的適應性和功能性。此專案專注於將 LLM 與資料庫合併，以改善決策制定，並讓知識擷取可用於請求詮釋問題。

##### **FSSC: Federated Learning of Transformer Neural Networks for Semantic Image Communication**
2407.21507v1 by Yuna Yan, Xin Zhang, Lixin Li, Wensheng Lin, Rui Li, Wenchi Cheng, Zhu Han

In this paper, we address the problem of image semantic communication in a
multi-user deployment scenario and propose a federated learning (FL) strategy
for a Swin Transformer-based semantic communication system (FSSC). Firstly, we
demonstrate that the adoption of a Swin Transformer for joint source-channel
coding (JSCC) effectively extracts semantic information in the communication
system. Next, the FL framework is introduced to collaboratively learn a global
model by aggregating local model parameters, rather than directly sharing
clients' data. This approach enhances user privacy protection and reduces the
workload on the server or mobile edge. Simulation evaluations indicate that our
method outperforms the typical JSCC algorithm and traditional separate-based
communication algorithms. Particularly after integrating local semantics, the
global aggregation model has further increased the Peak Signal-to-Noise Ratio
(PSNR) by more than 2dB, thoroughly proving the effectiveness of our algorithm.

摘要：在本文中，我们探讨了多用户部署场景中的图像语义通信问题，并针对基于 Swin Transformer 的语义通信系统 (FSSC) 提出了一种联邦学习 (FL) 策略。首先，我们证明了采用 Swin Transformer 进行联合源信道编码 (JSCC) 可以有效地提取通信系统中的语义信息。接下来，引入 FL 框架，通过聚合本地模型参数来协作学习全局模型，而不是直接共享客户端数据。这种方法增强了用户隐私保护，并减少了服务器或移动边缘的工作负载。仿真评估表明，我们的方法优于典型的 JSCC 算法和传统的基于分离的通信算法。特别是在整合了局部语义之后，全局聚合模型进一步将峰值信噪比 (PSNR) 提高了 2dB 以上，充分证明了我们算法的有效性。

##### **MaskUno: Switch-Split Block For Enhancing Instance Segmentation**
2407.21498v1 by Jawad Haidar, Marc Mouawad, Imad Elhajj, Daniel Asmar

Instance segmentation is an advanced form of image segmentation which, beyond
traditional segmentation, requires identifying individual instances of
repeating objects in a scene. Mask R-CNN is the most common architecture for
instance segmentation, and improvements to this architecture include steps such
as benefiting from bounding box refinements, adding semantics, or backbone
enhancements. In all the proposed variations to date, the problem of competing
kernels (each class aims to maximize its own accuracy) persists when models try
to synchronously learn numerous classes. In this paper, we propose mitigating
this problem by replacing mask prediction with a Switch-Split block that
processes refined ROIs, classifies them, and assigns them to specialized mask
predictors. We name the method MaskUno and test it on various models from the
literature, which are then trained on multiple classes using the benchmark COCO
dataset. An increase in the mean Average Precision (mAP) of 2.03% was observed
for the high-performing DetectoRS when trained on 80 classes. MaskUno proved to
enhance the mAP of instance segmentation models regardless of the number and
typ

摘要：實例分割是影像分割的進階形式，除了傳統分割之外，還需要識別場景中重複物體的個別實例。Mask R-CNN 是實例分割最常見的架構，而對此架構的改進包括從邊界框優化、加入語意或主幹增強等步驟。在迄今為止提出的所有變體中，當模型嘗試同步學習多個類別時，競爭核心的問題（每個類別都旨在最大化其自身準確度）仍然存在。在本文中，我們提出透過將遮罩預測替換為處理精緻 ROI、對其分類並將其分配給專門遮罩預測器的開關分割區塊來減輕此問題。我們將此方法命名為 MaskUno，並在來自文獻的各種模型上對其進行測試，然後使用基準 COCO 資料集對其進行多個類別的訓練。在 80 個類別上進行訓練時，高性能 DetectoRS 的平均準確度 (mAP) 觀察到增加了 2.03%。無論類別數量和

##### **Generative Expressive Conversational Speech Synthesis**
2407.21491v2 by Rui Liu, Yifan Hu, Yi Ren, Xiang Yin, Haizhou Li

Conversational Speech Synthesis (CSS) aims to express a target utterance with
the proper speaking style in a user-agent conversation setting. Existing CSS
methods employ effective multi-modal context modeling techniques to achieve
empathy understanding and expression. However, they often need to design
complex network architectures and meticulously optimize the modules within
them. In addition, due to the limitations of small-scale datasets containing
scripted recording styles, they often fail to simulate real natural
conversational styles. To address the above issues, we propose a novel
generative expressive CSS system, termed GPT-Talker.We transform the multimodal
information of the multi-turn dialogue history into discrete token sequences
and seamlessly integrate them to form a comprehensive user-agent dialogue
context. Leveraging the power of GPT, we predict the token sequence, that
includes both semantic and style knowledge, of response for the agent. After
that, the expressive conversational speech is synthesized by the
conversation-enriched VITS to deliver feedback to the user.Furthermore, we
propose a large-scale Natural CSS Dataset called NCSSD, that includes both
naturally recorded conversational speech in improvised styles and dialogues
extracted from TV shows. It encompasses both Chinese and English languages,
with a total duration of 236 hours.We conducted comprehensive experiments on
the reliability of the NCSSD and the effectiveness of our GPT-Talker. Both
subjective and objective evaluations demonstrate that our model outperforms
other state-of-the-art CSS systems significantly in terms of naturalness and
expressiveness. The Code, Dataset, and Pre-trained Model are available at:
https://github.com/AI-S2-Lab/GPT-Talker.

摘要：對話式語音合成 (CSS) 旨在以適當的說話風格在使用者代理對話設定中表達目標話語。現有的 CSS 方法採用有效的多模式語境建模技術來達成同理理解和表達。然而，他們經常需要設計複雜的網路架構並精心最佳化其中的模組。此外，由於包含腳本記錄風格的小規模資料集的限制，他們經常無法模擬真實自然的對話風格。為了解決上述問題，我們提出了一個創新的生成式表達性 CSS 系統，稱為 GPT-Talker。我們將多輪對話歷史的多模式資訊轉換為離散的符號序列，並將它們無縫整合以形成全面的使用者代理對話語境。利用 GPT 的功能，我們預測回應的符號序列，其中包含語義和風格知識，以供代理使用。在那之後，由對話豐富的 VITS 合成富有表現力的對話式語音，以向使用者提供回饋。此外，我們提出了一個稱為 NCSSD 的大型自然 CSS 資料集，其中包含以即興風格自然記錄的對話式語音和從電視節目中擷取的對話。它包含中文和英文，總時長為 236 小時。我們對 NCSSD 的可靠性和我們 GPT-Talker 的有效性進行了全面的實驗。主觀和客觀評估都證明，我們的模型在自然性和表現力方面顯著優於其他最先進的 CSS 系統。程式碼、資料集和預先訓練的模型可在以下位置取得：https://github.com/AI-S2-Lab/GPT-Talker。

##### **Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation**
2407.21490v1 by Junxuan Yu, Rusi Chen, Yongsong Zhou, Yanlin Chen, Yaofei Duan, Yuhao Huang, Han Zhou, Tan Tao, Xin Yang, Dong Ni

Echocardiography video is a primary modality for diagnosing heart diseases,
but the limited data poses challenges for both clinical teaching and machine
learning training. Recently, video generative models have emerged as a
promising strategy to alleviate this issue. However, previous methods often
relied on holistic conditions during generation, hindering the flexible
movement control over specific cardiac structures. In this context, we propose
an explainable and controllable method for echocardiography video generation,
taking an initial frame and a motion curve as guidance. Our contributions are
three-fold. First, we extract motion information from each heart substructure
to construct motion curves, enabling the diffusion model to synthesize
customized echocardiography videos by modifying these curves. Second, we
propose the structure-to-motion alignment module, which can map semantic
features onto motion curves across cardiac structures. Third, The
position-aware attention mechanism is designed to enhance video consistency
utilizing Gaussian masks with structural position information. Extensive
experiments on three echocardiography datasets show that our method outperforms
others regarding fidelity and consistency. The full code will be released at
https://github.com/mlmi-2024-72/ECM.

摘要：超音波心動圖影片是診斷心臟疾病的主要方式，
但有限的數據對臨床教學和機器學習訓練都構成挑戰。最近，影片生成模型已成為緩解此問題的一種有前途的策略。然而，先前的辦法在生成過程中通常依賴整體條件，阻礙了對特定心臟結構的靈活運動控制。在此背景下，我們提出了一種可解釋且可控的超音波心動圖影片生成方法，以初始幀和運動曲線作為指導。我們的貢獻有三方面。首先，我們從每個心臟子結構中提取運動資訊以建構運動曲線，讓擴散模型能夠透過修改這些曲線來合成客製化的超音波心動圖影片。其次，我們提出了結構到運動對齊模組，它可以將語義特徵對應到心臟結構中的運動曲線。第三，位置感知注意力機制旨在利用具有結構位置資訊的高斯遮罩來增強影片的一致性。在三個超音波心動圖資料集上的廣泛實驗顯示，我們的辦法在保真度和一致性方面優於其他辦法。完整程式碼將在 https://github.com/mlmi-2024-72/ECM 上釋出。

##### **Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends**
2407.21489v1 by Giuliano Martinelli, Edoardo Barba, Roberto Navigli

Large autoregressive generative models have emerged as the cornerstone for
achieving the highest performance across several Natural Language Processing
tasks. However, the urge to attain superior results has, at times, led to the
premature replacement of carefully designed task-specific approaches without
exhaustive experimentation. The Coreference Resolution task is no exception;
all recent state-of-the-art solutions adopt large generative autoregressive
models that outperform encoder-based discriminative systems. In this work,we
challenge this recent trend by introducing Maverick, a carefully designed - yet
simple - pipeline, which enables running a state-of-the-art Coreference
Resolution system within the constraints of an academic budget, outperforming
models with up to 13 billion parameters with as few as 500 million parameters.
Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,
training with up to 0.006x the memory resources and obtaining a 170x faster
inference compared to previous state-of-the-art systems. We extensively
validate the robustness of the Maverick framework with an array of diverse
experiments, reporting improvements over prior systems in data-scarce,
long-document, and out-of-domain settings. We release our code and models for
research purposes at https://github.com/SapienzaNLP/maverick-coref.

摘要：大型自回归生成模型已成为在多项自然语言处理任务中实现最高性能的基石。然而，追求卓越成果的冲动有时会导致在没有详尽实验的情况下过早替换精心设计的特定任务方法。共指消解任务也不例外；所有最近的最新解决方案都采用大型生成自回归模型，其性能优于基于编码器的判别系统。在这项工作中，我们通过引入 Maverick 来挑战这一最新趋势，Maverick 是一款精心设计但简单的管道，它可以在学术预算的限制内运行最先进的共指消解系统，其性能优于多达 130 亿参数的模型，而参数却少至 5 亿。Maverick 在 CoNLL-2012 基准测试中实现了最先进的性能，训练时使用的内存资源最多为 0.006 倍，并且与之前的最先进系统相比，推理速度提高了 170 倍。我们通过一系列不同的实验广泛验证了 Maverick 框架的鲁棒性，报告了在数据稀缺、长文档和域外设置中对先前系统的改进。我们在 https://github.com/SapienzaNLP/maverick-coref 上发布我们的代码和模型以用于研究目的。

##### **eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**
2407.21483v2 by Xiny Pan, Daniel Hernández, Philipp Seifer, Ralf Lämmel, Steffen Staab

Over the past few years, we have seen the emergence of large knowledge graphs
combining information from multiple sources. Sometimes, this information is
provided in the form of assertions about other assertions, defining contexts
where assertions are valid. A recent extension to RDF which admits statements
over statements, called RDF-star, is in revision to become a W3C standard.
However, there is no proposal for a semantics of these RDF-star statements nor
a built-in facility to operate over them. In this paper, we propose a query
language for epistemic RDF-star metadata based on a four-valued logic, called
eSPARQL. Our proposed query language extends SPARQL-star, the query language
for RDF-star, with a new type of FROM clause to facilitate operating with
multiple and sometimes conflicting beliefs. We show that the proposed query
language can express four use case queries, including the following features:
(i) querying the belief of an individual, (ii) the aggregating of beliefs,
(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs
(i.e., nesting of beliefs).

摘要：在過去幾年，我們已經看到大型知識圖譜的出現，結合來自多個來源的資訊。有時，此資訊以對其他斷言的斷言形式提供，定義斷言有效的脈絡。RDF 的最新擴充允許對斷言進行陳述，稱為 RDF-star，目前正在修訂為 W3C 標準。然而，沒有針對這些 RDF-star 陳述的語義提出建議，也沒有內建的運作工具。在本文中，我們提出一個基於四值邏輯的知識 RDF-star 元資料查詢語言，稱為 eSPARQL。我們提出的查詢語言擴充了 RDF-star 的查詢語言 SPARQL-star，並使用新的 FROM 子句類型來促進運作，包括多重且有時衝突的信念。我們展示了所提出的查詢語言可以表達四個用例查詢，包括下列功能：(i) 查詢個人的信念，(ii) 信念的彙總，(iii) 查詢誰與某人衝突，以及 (iv) 關於信念的信念（即信念的巢狀）。

##### **On the Problem of Text-To-Speech Model Selection for Synthetic Data Generation in Automatic Speech Recognition**
2407.21476v1 by Nick Rossenbach, Ralf Schlüter, Sakriani Sakti

The rapid development of neural text-to-speech (TTS) systems enabled its
usage in other areas of natural language processing such as automatic speech
recognition (ASR) or spoken language translation (SLT). Due to the large number
of different TTS architectures and their extensions, selecting which TTS
systems to use for synthetic data creation is not an easy task. We use the
comparison of five different TTS decoder architectures in the scope of
synthetic data generation to show the impact on CTC-based speech recognition
training. We compare the recognition results to computable metrics like NISQA
MOS and intelligibility, finding that there are no clear relations to the ASR
performance. We also observe that for data generation auto-regressive decoding
performs better than non-autoregressive decoding, and propose an approach to
quantify TTS generalization capabilities.

摘要：神經文字轉語音 (TTS) 系統的快速發展，使其得以應用於自然語言處理的其他領域，例如自動語音辨識 (ASR) 或口語翻譯 (SLT)。由於 TTS 架構及其擴充功能種類繁多，因此選擇哪個 TTS 系統來進行合成資料建立並非易事。我們在合成資料產生範圍內比較五種不同的 TTS 解碼器架構，以顯示對基於 CTC 的語音辨識訓練的影響。我們將辨識結果與可計算的指標（例如 NISQA MOS 和可理解度）進行比較，發現與 ASR 效能並無明確的關係。我們還觀察到，對於資料產生，自迴歸解碼的表現優於非自迴歸解碼，並提出了一種量化 TTS 泛化能力的方法。

##### **Fine-gained Zero-shot Video Sampling**
2407.21475v1 by Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu

Incorporating a temporal dimension into pretrained image diffusion models for
video generation is a prevalent approach. However, this method is
computationally demanding and necessitates large-scale video datasets. More
critically, the heterogeneity between image and video datasets often results in
catastrophic forgetting of the image expertise. Recent attempts to directly
extract video snippets from image diffusion models have somewhat mitigated
these problems. Nevertheless, these methods can only generate brief video clips
with simple movements and fail to capture fine-grained motion or non-grid
deformation. In this paper, we propose a novel Zero-Shot video Sampling
algorithm, denoted as $\mathcal{ZS}^2$, capable of directly sampling
high-quality video clips from existing image synthesis methods, such as Stable
Diffusion, without any training or optimization. Specifically, $\mathcal{ZS}^2$
utilizes the dependency noise model and temporal momentum attention to ensure
content consistency and animation coherence, respectively. This ability enables
it to excel in related tasks, such as conditional and context-specialized video
generation and instruction-guided video editing. Experimental results
demonstrate that $\mathcal{ZS}^2$ achieves state-of-the-art performance in
zero-shot video generation, occasionally outperforming recent supervised
methods.
  Homepage: \url{https://densechen.github.io/zss/}.

摘要：將時間維度整合到預訓練影像擴散模型中，以進行影片生成，是一種普遍的方法。然而，這種方法在運算上要求很高，且需要大規模的影片資料集。更重要的是，影像資料集和影片資料集之間的異質性，通常會導致影像專業知識的災難性遺忘。近期嘗試直接從影像擴散模型中擷取影片片段，在某種程度上緩解了這些問題。儘管如此，這些方法只能生成具有簡單動作的簡短影片片段，而且無法捕捉細粒度的動作或非網格變形。在本文中，我們提出了一種新穎的零次學習影片採樣演算法，表示為 $\mathcal{ZS}^2$，它能夠直接從現有的影像合成方法（例如 Stable Diffusion）中採樣高品質的影片片段，而無需任何訓練或最佳化。具體來說，$\mathcal{ZS}^2$ 分別利用依賴雜訊模型和時間動量注意力，以確保內容一致性和動畫連貫性。這種能力使它能夠在相關任務中表現出色，例如條件式和特定於內容的影片生成，以及指令引導的影片編輯。實驗結果表明，$\mathcal{ZS}^2$ 在零次學習影片生成中實現了最先進的效能，偶爾優於最近的監督式方法。主頁：\url{https://densechen.github.io/zss/}。

##### **An Invertible State Space for Process Trees**
2407.21468v1 by Gero Kolhof, Sebastiaan J. van Zelst

Process models are, like event data, first-class citizens in most process
mining approaches. Several process modeling formalisms have been proposed and
used, e.g., Petri nets, BPMN, and process trees. Despite their frequent use,
little research addresses the formal properties of process trees and the
corresponding potential to improve the efficiency of solving common
computational problems. Therefore, in this paper, we propose an invertible
state space definition for process trees and demonstrate that the corresponding
state space graph is isomorphic to the state space graph of the tree's inverse.
Our result supports the development of novel, time-efficient, decomposition
strategies for applications of process trees. Our experiments confirm that our
state space definition allows for the adoption of bidirectional state space
search, which significantly improves the overall performance of state space
searches.

摘要：在大多数流程挖掘方法中，流程模型与事件数据类似，都是一等公民。已经提出并使用了多种流程建模形式主义，例如，Petri 网、BPMN 和流程树。尽管它们经常使用，但很少有研究涉及流程树的形式属性以及提高解决常见计算问题的效率的相应潜力。因此，在本文中，我们提出了流程树的可逆状态空间定义，并证明了相应的状态空间图与树的逆状态空间图同构。我们的结果支持为流程树的应用程序开发新颖、省时的分解策略。我们的实验证实，我们的状态空间定义允许采用双向状态空间搜索，这显著提高了状态空间搜索的整体性能。

##### **Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data**
2407.21467v1 by Mengtian Kang, Yansong Hu, Shuo Gao, Yuanyuan Liu, Hongbei Meng, Xuemeng Li, Xuhang Chen, Hubin Zhao, Jing Fu, Guohua Hu, Wei Wang, Yanning Dai, Arokia Nathan, Peter Smielewski, Ningli Wang, Shiming Li

Childhood myopia constitutes a significant global health concern. It exhibits
an escalating prevalence and has the potential to evolve into severe,
irreversible conditions that detrimentally impact familial well-being and
create substantial economic costs. Contemporary research underscores the
importance of precisely predicting myopia progression to enable timely and
effective interventions, thereby averting severe visual impairment in children.
Such predictions predominantly rely on subjective clinical assessments, which
are inherently biased and resource-intensive, thus hindering their widespread
application. In this study, we introduce a novel, high-accuracy method for
quantitatively predicting the myopic trajectory and myopia risk in children
using only fundus images and baseline refraction data. This approach was
validated through a six-year longitudinal study of 3,408 children in Henan,
utilizing 16,211 fundus images and corresponding refractive data. Our method
based on deep learning demonstrated predictive accuracy with an error margin of
0.311D per year and AUC scores of 0.944 and 0.995 for forecasting the risks of
developing myopia and high myopia, respectively. These findings confirm the
utility of our model in supporting early intervention strategies and in
significantly reducing healthcare costs, particularly by obviating the need for
additional metadata and repeated consultations. Furthermore, our method was
designed to rely only on fundus images and refractive error data, without the
need for meta data or multiple inquiries from doctors, strongly reducing the
associated medical costs and facilitating large-scale screening. Our model can
even provide good predictions based on only a single time measurement.
Consequently, the proposed method is an important means to reduce medical
inequities caused by economic disparities.

摘要：兒童近視構成全球重要的健康問題。它顯示出日益增加的盛行率，並可能演變成嚴重、不可逆轉的狀況，對家庭福祉造成不利影響，並產生大量的經濟成本。現代研究強調精準預測近視進展的重要性，以實現及時有效的干預，從而避免兒童出現嚴重的視力損害。此類預測主要依賴主觀的臨床評估，其本身具有偏見且資源密集，從而阻礙了它們的廣泛應用。在本研究中，我們引入了一種新穎、高精確度的方法，僅使用眼底圖像和基線屈光數據，就能定量預測兒童的近視軌跡和近視風險。這種方法通過對河南省 3,408 名兒童進行為期六年的縱向研究，利用 16,211 張眼底圖像和相應的屈光數據進行了驗證。我們基於深度學習的方法展示了預測準確度，年誤差範圍為 0.311D，預測發生近視和高度近視的風險的 AUC 分數分別為 0.944 和 0.995。這些發現證實了我們的模型在支持早期干預策略和顯著降低醫療保健成本方面的效用，特別是通過消除對額外元數據和重複諮詢的需要。此外，我們的方法被設計為僅依賴眼底圖像和屈光不正數據，而無需元數據或醫生的多次詢問，從而大大降低了相關的醫療成本，並促進了大規模篩查。我們的模型甚至可以僅根據單次時間測量提供良好的預測。因此，所提出的方法是減少由經濟差距造成的醫療不平等的重要手段。

##### **KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making**
2407.21459v1 by Gilang Fajar Febrian, Grazziela Figueredo

Data is crucial for evidence-based policymaking and enhancing public
services, including those at the Ministry of Finance of the Republic of
Indonesia. However, the complexity and dynamic nature of governmental financial
data and regulations can hinder decision-making. This study investigates the
potential of Large Language Models (LLMs) to address these challenges, focusing
on Indonesia's financial data and regulations. While LLMs are effective in the
financial sector, their use in the public sector in Indonesia is unexplored.
This study undertakes an iterative process to develop KemenkeuGPT using the
LangChain with Retrieval-Augmented Generation (RAG), prompt engineering and
fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of
Finance, Statistics Indonesia and the International Monetary Fund (IMF).
Surveys and interviews with Ministry officials informed, enhanced and
fine-tuned the model. We evaluated the model using human feedback, LLM-based
evaluation and benchmarking. The model's accuracy improved from 35% to 61%,
with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation
Assessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness
with 73% faithfulness, 40% precision and 60% recall, outperforming several
other base models. An interview with an expert from the Ministry of Finance
indicated that KemenkeuGPT has the potential to become an essential tool for
decision-making. These results are expected to improve with continuous human
feedback.

摘要：<paragraph>資料對於證據基礎的政策制定和改善公共服務至關重要，包括印尼共和國財政部。然而，政府財務資料和法規的複雜性和動態性可能會阻礙決策。本研究探討了大型語言模型 (LLM) 應對這些挑戰的潛力，重點關注印尼的財務資料和法規。雖然 LLM 在金融領域很有效，但它們在印尼公共部門的使用尚未得到探索。本研究採用迭代流程，使用帶有檢索增強生成 (RAG)、提示工程和微調的 LangChain 來開發 KemenkeuGPT。資料集從 2003 年到 2023 年收集自印尼財政部、印尼統計局和國際貨幣基金組織 (IMF)。對財政部官員的調查和訪談告知、增強和微調了模型。我們使用人類回饋、基於 LLM 的評估和基準對模型進行了評估。該模型的準確率從 35% 提高到 61%，正確率從 48% 提高到 64%。檢索增強生成評估 (RAGAS) 框架表明，KemenkeuGPT 達到了 44% 的正確率，73% 的忠實度，40% 的精確度和 60% 的召回率，優於其他幾個基礎模型。對財政部專家的採訪表明，KemenkeuGPT 有可能成為決策的重要工具。預計這些結果會隨著持續的人類回饋而得到改善。</paragraph>

##### **TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors**
2407.21453v1 by Zhaolan Huang, Adrien Tousnakhoff, Polina Kozyr, Roman Rehausen, Felix Bießmann, Robert Lachlan, Cedric Adjih, Emmanuel Baccelli

Monitoring biodiversity at scale is challenging. Detecting and identifying
species in fine grained taxonomies requires highly accurate machine learning
(ML) methods. Training such models requires large high quality data sets. And
deploying these models to low power devices requires novel compression
techniques and model architectures. While species classification methods have
profited from novel data sets and advances in ML methods, in particular neural
networks, deploying these state of the art models to low power devices remains
difficult. Here we present a comprehensive empirical comparison of various
tinyML neural network architectures and compression techniques for species
classification. We focus on the example of bird song detection, more concretely
a data set curated for studying the corn bunting bird species. The data set is
released along with all code and experiments of this study. In our experiments
we compare predictive performance, memory and time complexity of classical
spectrogram based methods and recent approaches operating on raw audio signal.
Our results indicate that individual bird species can be robustly detected with
relatively simple architectures that can be readily deployed to low power
devices.

摘要：大規模監控生物多樣性是一項挑戰。在細粒度分類法中檢測和識別物種需要高度準確的機器學習 (ML) 方法。訓練此類模型需要大量高品質的資料集。而將這些模型部署到低功耗裝置需要創新的壓縮技術和模型架構。雖然物種分類方法受益於新穎的資料集和機器學習方法的進步，特別是神經網路，但將這些最先進的模型部署到低功耗裝置仍然很困難。在此，我們提出了各種 tinyML 神經網路架構和物種分類壓縮技術的全面實證比較。我們專注於鳥鳴聲檢測的範例，更具體地說，是為研究玉米鵐鳥類物種而策展的資料集。該資料集與本研究的所有程式碼和實驗一起發布。在我們的實驗中，我們比較了基於經典聲譜圖的方法和處理原始音訊訊號的最新方法的預測效能、記憶體和時間複雜度。我們的結果表明，可以透過相對簡單的架構穩健地檢測個別鳥類物種，這些架構可以輕易地部署到低功耗裝置。

##### **Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**
2407.21452v1 by Haodong Hong, Sen Wang, Zi Huang, Qi Wu, Jiajun Liu

Real-world navigation often involves dealing with unexpected obstructions
such as closed doors, moved objects, and unpredictable entities. However,
mainstream Vision-and-Language Navigation (VLN) tasks typically assume
instructions perfectly align with the fixed and predefined navigation graphs
without any obstructions. This assumption overlooks potential discrepancies in
actual navigation graphs and given instructions, which can cause major failures
for both indoor and outdoor agents. To address this issue, we integrate diverse
obstructions into the R2R dataset by modifying both the navigation graphs and
visual observations, introducing an innovative dataset and task, R2R with
UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers
of path obstructions to generate instruction-reality mismatches for VLN
research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods
inevitably encounter significant challenges when facing such mismatches,
indicating that they rigidly follow instructions rather than navigate
adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),
which includes a curriculum training strategy and virtual graph construction to
help agents effectively adapt to obstructed environments. Empirical results
show that ObVLN not only maintains robust performance in unobstructed scenarios
but also achieves a substantial performance advantage with unexpected
obstructions.

摘要：现实世界的导航通常涉及处理意外的障碍，例如关着的门、移动的物体和不可预测的实体。然而，主流的视觉和语言导航 (VLN) 任务通常假设指令与固定的和预定义的导航图完全一致，没有任何障碍。这种假设忽略了实际导航图和给定指令中潜在的差异，这可能会导致室内和室外代理出现重大故障。为了解决这个问题，我们通过修改导航图和视觉观察，将各种障碍整合到 R2R 数据集中，引入了创新数据集和任务，即带有意外障碍的 R2R (R2R-UNO)。R2R-UNO 包含各种类型和数量的路径障碍，以生成 VLN 研究的指令-现实不匹配。在 R2R-UNO 上的实验表明，最先进的 VLN 方法在面对此类不匹配时不可避免地会遇到重大挑战，这表明它们严格遵循指令，而不是自适应地导航。因此，我们提出了一种称为 ObVLN（受阻 VLN）的新方法，其中包括课程训练策略和虚拟图构建，以帮助代理有效地适应受阻环境。经验结果表明，ObVLN 不仅在无障碍场景中保持了稳健的性能，而且在意外障碍中也获得了实质性的性能优势。

##### **Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency**
2407.21443v1 by Taiji Li, Zhi Li, Yin Zhang

Despite large language models (LLMs) have demonstrated impressive performance
in various tasks, they are still suffering from the factual inconsistency
problem called hallucinations. For instance, LLMs occasionally generate content
that diverges from source article, and prefer to extract information that
appears at the beginning and end of the context, especially in long document
summarization. Inspired by these findings, we propose to improve the
faithfulness of LLMs in summarization by impelling them to process the entire
article more fairly and faithfully. We present a novel summary generation
strategy, namely SliSum, which exploits the ideas of sliding windows and
self-consistency. Specifically, SliSum divides the source article into
overlapping windows, and utilizes LLM to generate local summaries for the
content in the windows. Finally, SliSum aggregates all local summaries using
clustering and majority voting algorithm to produce more faithful summary of
entire article. Extensive experiments demonstrate that SliSum significantly
improves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and
GPT-3.5 in both short and long text summarization, while maintaining their
fluency and informativeness and without additional fine-tuning and resources.
We further conduct qualitative and quantitative studies to investigate why
SliSum works and impacts of hyperparameters in SliSum on performance.

摘要：儘管大型語言模型 (LLM) 已在各種任務中展現出令人印象深刻的效能，但它們仍飽受稱為幻覺的虛假不一致問題所苦。例如，LLM 有時會產生與原始文章不同的內容，且偏好擷取出現在內容開頭和結尾的資訊，特別是在長篇文件摘要中。受到這些發現的啟發，我們建議透過促使 LLM 更公平且忠實地處理整篇文章，來改善其在摘要中的真實性。我們提出了一種新的摘要產生策略，稱為 SliSum，它利用滑動視窗和自我一致性的概念。具體來說，SliSum 將原始文章分成重疊的視窗，並利用 LLM 為視窗中的內容產生局部摘要。最後，SliSum 使用群集和多數決演算法彙總所有局部摘要，以產生更忠實的整篇文章摘要。廣泛的實驗證明，SliSum 在簡短和長篇文字摘要中，顯著提升了 LLaMA-2、Claude-2 和 GPT-3.5 等不同 LLM 的真實性，同時維持其流暢度和資訊量，且無需額外的微調和資源。我們進一步進行定性和定量研究，以探討 SliSum 的運作原理，以及 SliSum 中的超參數對效能的影響。

##### **QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications**
2407.21441v2 by Ritvik Setty, Vinay Setty

Verifying fact-checking claims poses a significant challenge, even for
humans. Recent approaches have demonstrated that decomposing claims into
relevant questions to gather evidence enhances the efficiency of the
fact-checking process. In this paper, we provide empirical evidence showing
that this question decomposition can be effectively automated. We demonstrate
that smaller generative models, fine-tuned for the question generation task
using data augmentation from various datasets, outperform large language models
by up to 8%. Surprisingly, in some cases, the evidence retrieved using
machine-generated questions proves to be significantly more effective for
fact-checking than that obtained from human-written questions. We also perform
manual evaluation of the decomposed questions to assess the quality of the
questions generated.

摘要：驗證查核事實的聲明，即使對人類來說，也是一項重大的挑戰。最近的方法已證明，將聲明分解成相關問題以收集證據，可以提高查核事實程序的效率。在本文中，我們提供了經驗證據，說明這種問題分解可以有效自動化。我們證明了較小的生成模型，針對問題生成任務進行微調，使用來自各種資料集的資料擴充，比大型語言模型的表現高出 8%。令人驚訝的是，在某些情況下，使用機器產生的問題所檢索到的證據，被證明比從人工撰寫的問題中獲得的證據，對查核事實更有效。我們也對分解的問題進行手動評估，以評估所產生問題的品質。

##### **MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training**
2407.21439v1 by Zhanpeng Chen, Chengjin Xu, Yiyan Qi, Jian Guo

Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities, including text, images, audio, and video. However, a significant
drawback of MLLMs is their reliance on static training data, leading to
outdated information and limited contextual awareness. This static nature
hampers their ability to provide accurate, up-to-date responses, particularly
in dynamic or rapidly evolving contexts. Integrating Multimodal
Retrieval-augmented Generation (Multimodal RAG) offers a promising solution,
but the system would inevitably encounter the multi-granularity noisy
correspondence (MNC) problem, which involves two types of noise: coarse-grained
(query-caption) and fine-grained (query-image). This noise hinders accurate
retrieval and generation. In this work, we propose \textbf{RagLLaVA}, a novel
framework with knowledge-enhanced reranking and noise-injected training, to
address these limitations. We instruction-tune the MLLM with a simple yet
effective instruction template to induce its ranking ability and serve it as a
reranker to precisely filter the top-k retrieved images. For generation, we
inject visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments are conducted on the subsets of
two datasets that require retrieving and reasoning over images to answer a
given query. Our results demonstrate the superiority of RagLLaVA in retrieving
accurately and generating robustly. Code and models are available at
https://github.com/IDEA-FinAI/RagLLaVA.

摘要：多模态大语言模型 (MLLM) 已在处理和生成跨多个数据模态（包括文本、图像、音频和视频）的内容方面展示出非凡的能力。然而，MLLM 的一个重大缺点是它们依赖于静态训练数据，导致信息过时和上下文感知有限。这种静态特性阻碍了它们提供准确、最新的响应的能力，尤其是在动态或快速变化的上下文中。集成多模态检索增强生成 (Multimodal RAG) 提供了一个有希望的解决方案，但该系统不可避免地会遇到多粒度噪声对应 (MNC) 问题，其中涉及两种类型的噪声：粗粒度（查询标题）和细粒度（查询图像）。这种噪声阻碍了准确的检索和生成。在这项工作中，我们提出了 RagLLaVA，这是一个新颖的框架，具有知识增强重新排序和噪声注入训练，以解决这些限制。我们使用一个简单但有效的指令模板对 MLLM 进行指令微调，以诱导其排序能力，并将其用作重新排序器来精确过滤前 k 个检索到的图像。对于生成，我们在数据和标记级别在训练期间注入视觉噪声，以增强生成器的鲁棒性。在两个数据集的子集上进行了广泛的实验，这些数据集需要检索和推理图像才能回答给定的查询。我们的结果证明了 RagLLaVA 在准确检索和稳健生成方面的优越性。代码和模型可在 https://github.com/IDEA-FinAI/RagLLaVA 获得。

##### **Deformable 3D Shape Diffusion Model**
2407.21428v1 by Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu

The Gaussian diffusion model, initially designed for image generation, has
recently been adapted for 3D point cloud generation. However, these adaptations
have not fully considered the intrinsic geometric characteristics of 3D shapes,
thereby constraining the diffusion model's potential for 3D shape manipulation.
To address this limitation, we introduce a novel deformable 3D shape diffusion
model that facilitates comprehensive 3D shape manipulation, including point
cloud generation, mesh deformation, and facial animation. Our approach
innovatively incorporates a differential deformation kernel, which deconstructs
the generation of geometric structures into successive non-rigid deformation
stages. By leveraging a probabilistic diffusion model to simulate this
step-by-step process, our method provides a versatile and efficient solution
for a wide range of applications, spanning from graphics rendering to facial
expression animation. Empirical evidence highlights the effectiveness of our
approach, demonstrating state-of-the-art performance in point cloud generation
and competitive results in mesh deformation. Additionally, extensive visual
demonstrations reveal the significant potential of our approach for practical
applications. Our method presents a unique pathway for advancing 3D shape
manipulation and unlocking new opportunities in the realm of virtual reality.

摘要：高斯擴散模型最初是為影像生成而設計，最近已改用於 3D 點雲生成。然而，這些改編並未充分考慮 3D 形狀的內在幾何特徵，從而限制了擴散模型在 3D 形狀操作方面的潛力。為了解決此限制，我們引進了一種新穎的可變形 3D 形狀擴散模型，該模型有助於進行全面的 3D 形狀操作，包括點雲生成、網格變形和面部動畫。我們的做法創新地納入了微分變形核，將幾何結構的生成解構為連續的非剛性變形階段。透過利用機率擴散模型來模擬此逐步程序，我們的做法提供了一種多功能且有效率的解決方案，適用於從圖形渲染到面部表情動畫的廣泛應用。經驗證據突顯了我們做法的有效性，證明了在點雲生成中達到最先進的效能，以及在網格變形中獲得競爭力的結果。此外，廣泛的視覺示範揭示了我們做法在實際應用中具有顯著的潛力。我們的做法為推進 3D 形狀操作和在虛擬實境領域開啟新機會提供了獨特的途徑。

##### **Cost-Effective Hallucination Detection for LLMs**
2407.21424v1 by Simon Valentin, Jinmiao Fu, Gianluca Detommaso, Shaoyuan Xu, Giovanni Zappella, Bryan Wang

Large language models (LLMs) can be prone to hallucinations - generating
unreliable outputs that are unfaithful to their inputs, external facts or
internally inconsistent. In this work, we address several challenges for
post-hoc hallucination detection in production settings. Our pipeline for
hallucination detection entails: first, producing a confidence score
representing the likelihood that a generated answer is a hallucination; second,
calibrating the score conditional on attributes of the inputs and candidate
response; finally, performing detection by thresholding the calibrated score.
We benchmark a variety of state-of-the-art scoring methods on different
datasets, encompassing question answering, fact checking, and summarization
tasks. We employ diverse LLMs to ensure a comprehensive assessment of
performance. We show that calibrating individual scoring methods is critical
for ensuring risk-aware downstream decision making. Based on findings that no
individual score performs best in all situations, we propose a multi-scoring
framework, which combines different scores and achieves top performance across
all datasets. We further introduce cost-effective multi-scoring, which can
match or even outperform more expensive detection methods, while significantly
reducing computational overhead.

摘要：大型語言模型 (LLM) 容易出現幻覺 - 生成不忠於其輸入、外部事實或內部不一致的不可靠輸出。在這項工作中，我們解決了生產環境中事後幻覺檢測的幾個挑戰。我們的幻覺檢測管道包括：首先，產生一個置信度分數，表示生成的答案是幻覺的可能性；其次，根據輸入和候選響應的屬性校準分數；最後，通過對校準分數進行閾值處理來執行檢測。我們對不同的資料集進行了各種最先進的評分方法的基準測試，包括問答、事實查核和摘要任務。我們採用不同的 LLM 來確保對性能進行全面評估。我們表明，校準個別評分方法對於確保風險感知的下游決策制定至關重要。基於沒有任何個別分數在所有情況下表現最佳的發現，我們提出了一個多評分框架，它結合了不同的分數，並在所有資料集上實現了最佳性能。我們進一步引入了具有成本效益的多評分，它可以匹配甚至優於更昂貴的檢測方法，同時顯著降低計算開銷。

##### **Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models**
2407.21417v1 by Zhengxuan Wu, Yuhao Zhang, Peng Qi, Yumo Xu, Rujun Han, Yian Zhang, Jifan Chen, Bonan Min, Zhiheng Huang

Modern language models (LMs) need to follow human instructions while being
faithful; yet, they often fail to achieve both. Here, we provide concrete
evidence of a trade-off between instruction following (i.e., follow open-ended
instructions) and faithfulness (i.e., ground responses in given context) when
training LMs with these objectives. For instance, fine-tuning LLaMA-7B on
instruction following datasets renders it less faithful. Conversely,
instruction-tuned Vicuna-7B shows degraded performance at following
instructions when further optimized on tasks that require contextual grounding.
One common remedy is multi-task learning (MTL) with data mixing, yet it remains
far from achieving a synergic outcome. We propose a simple yet effective method
that relies on Rejection Sampling for Continued Self-instruction Tuning
(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find
that less is more, as training ReSet with high-quality, yet substantially
smaller data (three-fold less) yields superior results. Our findings offer a
better understanding of objective discrepancies in alignment training of LMs.

摘要：現代語言模型 (LM) 在忠實的同時需要遵循人類的指示；然而，它們常常無法同時實現這兩點。在這裡，我們提供了具體證據，說明在使用這些目標訓練 LM 時，在遵循指示（即遵循開放式指示）和忠實度（即在給定的背景中建立回應）之間存在權衡。例如，在遵循指示的數據集上微調 LLaMA-7B 會降低其忠實度。相反，在進一步針對需要上下文依據的任務進行優化時，經過指示調整的 Vicuna-7B 在遵循指示方面的表現會下降。一種常見的補救措施是使用數據混合的多任務學習 (MTL)，但它仍然遠未達到協同效應。我們提出了一種簡單但有效的方法，它依賴於拒絕採樣，用於持續自指導調整 (ReSet)，它明顯優於香草 MTL。令人驚訝的是，我們發現少即是多，因為使用高品質但數量顯著較少（少三倍）的數據訓練 ReSet 會產生更好的結果。我們的發現有助於更好地理解 LM 對齊訓練中的目標差異。

##### **Towards interfacing large language models with ASR systems using confidence measures and prompting**
2407.21414v1 by Maryam Naderi, Enno Hermann, Alexandre Nanchen, Sevada Hovsepyan, Mathew Magimai. -Doss

As large language models (LLMs) grow in parameter size and capabilities, such
as interaction through prompting, they open up new ways of interfacing with
automatic speech recognition (ASR) systems beyond rescoring n-best lists. This
work investigates post-hoc correction of ASR transcripts with LLMs. To avoid
introducing errors into likely accurate transcripts, we propose a range of
confidence-based filtering methods. Our results indicate that this can improve
the performance of less competitive ASR systems.

摘要：隨著大型語言模型 (LLM) 在參數規模和功能上不斷增長，例如透過提示進行互動，它們開啟了與自動語音辨識 (ASR) 系統互動的新方式，而不再僅限於重新評分 n 個最佳清單。本研究探討了使用 LLM 對 ASR 轉錄進行事後校正。為了避免在可能準確的轉錄中引入錯誤，我們提出了一系列基於信心的過濾方法。我們的結果表明，這可以提升競爭力較低的 ASR 系統的效能。

##### **GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction**
2407.21384v1 by Yanxu Mao, Peipei Liu, Tiehan Cui

Document-level relation extraction (DocRE) aims to extract relations between
entities from unstructured document text. Compared to sentence-level relation
extraction, it requires more complex semantic understanding from a broader text
context. Currently, some studies are utilizing logical rules within evidence
sentences to enhance the performance of DocRE. However, in the data without
provided evidence sentences, researchers often obtain a list of evidence
sentences for the entire document through evidence retrieval (ER). Therefore,
DocRE suffers from two challenges: firstly, the relevance between evidence and
entity pairs is weak; secondly, there is insufficient extraction of complex
cross-relations between long-distance multi-entities. To overcome these
challenges, we propose GEGA, a novel model for DocRE. The model leverages graph
neural networks to construct multiple weight matrices, guiding attention
allocation to evidence sentences. It also employs multi-scale representation
aggregation to enhance ER. Subsequently, we integrate the most efficient
evidence information to implement both fully supervised and weakly supervised
training processes for the model. We evaluate the GEGA model on three widely
used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The
experimental results indicate that our model has achieved comprehensive
improvements compared to the existing SOTA model.

摘要：<paragraph>文件級關係萃取 (DocRE) 旨在從非結構文件文字中萃取實體間的關係。與句子級關係萃取相比，它需要從更廣泛的文字脈絡中進行更複雜的語義理解。目前，一些研究利用證據句子中的邏輯規則來增強 DocRE 的效能。然而，在沒有提供證據句子的資料中，研究人員通常會透過證據擷取 (ER) 為整個文件取得一份證據句子清單。因此，DocRE 面臨兩項挑戰：首先，證據和實體對之間的關聯性較弱；其次，無法充分萃取遠距離多實體之間的複雜交叉關係。為了克服這些挑戰，我們提出 GEGA，這是一種用於 DocRE 的新型模型。該模型利用圖神經網路來建構多個權重矩陣，引導注意力分配到證據句子。它還採用多尺度表示聚合來增強 ER。隨後，我們整合最有效的證據資訊，為模型實作完全監督式和弱監督式訓練流程。我們在三個廣泛使用的基準資料集：DocRED、Re-DocRED 和 Revisit-DocRED 上評估 GEGA 模型。實驗結果表明，與現有的 SOTA 模型相比，我們的模型已獲得全面的改進。</paragraph>

##### **An Extended Kalman Filter Integrated Latent Feature Model on Dynamic Weighted Directed Graphs**
2407.21376v1 by Hongxun Zhou, Xiangyu Chen, Ye Yuan

A dynamic weighted directed graph (DWDG) is commonly encountered in various
application scenarios. It involves extensive dynamic interactions among
numerous nodes. Most existing approaches explore the intricate temporal
patterns hidden in a DWDG from the purely data-driven perspective, which
suffers from accuracy loss when a DWDG exhibits strong fluctuations over time.
To address this issue, this study proposes a novel
Extended-Kalman-Filter-Incorporated Latent Feature (EKLF) model to represent a
DWDG from the model-driven perspective. Its main idea is divided into the
following two-fold ideas: a) adopting a control model, i.e., the Extended
Kalman Filter (EKF), to track the complex temporal patterns precisely with its
nonlinear state-transition and observation functions; and b) introducing an
alternating least squares (ALS) algorithm to train the latent features (LFs)
alternatively for precisely representing a DWDG. Empirical studies on DWDG
datasets demonstrate that the proposed EKLF model outperforms state-of-the-art
models in prediction accuracy and computational efficiency for missing edge
weights of a DWDG. It unveils the potential for precisely representing a DWDG
by incorporating a control model.

摘要：動態加權有向圖 (DWDG) 常見於各種應用情境中。它涉及眾多節點之間廣泛的動態交互。現有方法大多從純粹的數據驅動觀點探索隱藏在 DWDG 中的複雜時間模式，當 DWDG 隨時間推移出現劇烈波動時，這種方法會導致準確度下降。為了解決這個問題，本研究提出了一個新的擴展卡爾曼濾波器結合潛在特徵 (EKLF) 模型，從模型驅動的角度表示 DWDG。其主要思想可分為以下兩個方面：a) 採用控制模型，即擴展卡爾曼濾波器 (EKF)，以其非線性狀態轉移和觀測函數精確追蹤複雜的時間模式；b) 導入交替最小二乘 (ALS) 演算法，交替訓練潛在特徵 (LF)，以精確表示 DWDG。針對 DWDG 資料集的實證研究表明，所提出的 EKLF 模型在預測準確度和計算效率方面優於最先進的模型，用於預測 DWDG 的缺失邊權重。它揭示了透過結合控制模型精確表示 DWDG 的潛力。

##### **Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering**
2407.21368v1 by Danfeng Guo, Demetri Terzopoulos

Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.

摘要：大型視覺語言模型 (LVLMs) 在近年來取得顯著的成功，並已擴展到醫療領域。儘管在醫學視覺問答 (VQA) 任務中表現令人滿意，但醫學 LVLMs (MLVLMs) 仍存在幻覺問題，導致它們無法診斷出複雜的病理。此外，由於訓練資料不平衡，它們很容易無法學習少數病理。我們提出兩種針對 MLVLMs 的提示策略，以減少幻覺並改善 VQA 效能。在第一個策略中，我們提供查詢病理的詳細說明。在第二個策略中，我們微調一個便宜、效能不佳的學習器，以在特定指標上獲得高效能，並以文字方式向 MLVLM 提供其判斷。在 MIMIC-CXR-JPG 和 Chexpert 資料集上進行測試後，我們的模型顯著改善了診斷 F1 分數，最高提升幅度為 0.27。我們還展示了我們的提示策略可以擴展到一般的 LVLM 領域。根據 POPE 指標，它有效地抑制了現有 LVLMs 的假陰性預測，並將召回率提高了約 0.07。

##### **ProSpec RL: Plan Ahead, then Execute**
2407.21359v1 by Liangliang Liu, Yi Guan, BoRan Wang, Rujia Shen, Yi Lin, Chaoran Kong, Lian Yan, Jingchi Jiang

Imagining potential outcomes of actions before execution helps agents make
more informed decisions, a prospective thinking ability fundamental to human
cognition. However, mainstream model-free Reinforcement Learning (RL) methods
lack the ability to proactively envision future scenarios, plan, and guide
strategies. These methods typically rely on trial and error to adjust policy
functions, aiming to maximize cumulative rewards or long-term value, even if
such high-reward decisions place the environment in extremely dangerous states.
To address this, we propose the Prospective (ProSpec) RL method, which makes
higher-value, lower-risk optimal decisions by imagining future n-stream
trajectories. Specifically, ProSpec employs a dynamic model to predict future
states (termed "imagined states") based on the current state and a series of
sampled actions. Furthermore, we integrate the concept of Model Predictive
Control and introduce a cycle consistency constraint that allows the agent to
evaluate and select the optimal actions from these trajectories. Moreover,
ProSpec employs cycle consistency to mitigate two fundamental issues in RL:
augmenting state reversibility to avoid irreversible events (low risk) and
augmenting actions to generate numerous virtual trajectories, thereby improving
data efficiency. We validated the effectiveness of our method on the DMControl
benchmarks, where our approach achieved significant performance improvements.
Code will be open-sourced upon acceptance.

摘要：在執行動作前想像其潛在結果，有助於代理人做出更明智的決策，這是一種對人類認知至關重要的前瞻性思考能力。然而，主流的無模型強化學習 (RL) 方法缺乏主動設想未來場景、規劃和指導策略的能力。這些方法通常依賴於試錯來調整策略函數，旨在最大化累積獎勵或長期價值，即使這樣的獲取高獎勵決策會讓環境處於極度危險的狀態。為了解決這個問題，我們提出了前瞻性 (ProSpec) RL 方法，它通過想像未來的 n 串流軌跡來做出價值更高、風險更低的最佳決策。具體來說，ProSpec 使用動態模型根據當前狀態和一系列採樣動作來預測未來的狀態（稱為「想像狀態」）。此外，我們整合了模型預測控制的概念，並引入了一個週期一致性約束，允許代理人從這些軌跡中評估和選擇最佳動作。此外，ProSpec 使用週期一致性來緩解 RL 中的兩個基本問題：增加狀態可逆性以避免不可逆事件（低風險）和增加動作以產生大量的虛擬軌跡，從而提高資料效率。我們在 DMControl 基準上驗證了我們方法的有效性，我們的做法取得了顯著的效能提升。程式碼將在被接受後開源。

##### **Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**
2407.21358v1 by Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan

Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing
reliable, structured, domain-specific, and up-to-date external knowledge.
However, KGs and LLMs are often developed separately and must be integrated
after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning
algorithm that enables augmentation of black-box LLMs with one or more KGs. The
algorithm equips a LLM with actions for interfacing a KG and enables the LLM to
perform tree search over possible thoughts and actions to find high confidence
reasoning paths. We evaluate on two popular benchmark datasets. Our results
show that Tree-of-Traversals significantly improves performance on question
answering and KG question answering tasks. Code is available at
\url{https://github.com/amazon-science/tree-of-traversals}

摘要：知識圖譜 (KG) 透過提供可靠、結構化、特定於領域且最新的外部知識，來補充大型語言模型 (LLM)。
然而，KG 和 LLM 通常是分開開發，並且必須在訓練後整合。我們介紹了 Tree-of-Traversals，一種新穎的零次推理演算法，它能讓黑盒 LLM 使用一個或多個 KG。該演算法為 LLM 提供與 KG 介面的動作，並讓 LLM 能在可能的思考和動作上執行樹狀搜尋，以找出高度信心的推理路徑。我們在兩個熱門的基準資料集上進行評估。我們的結果顯示，Tree-of-Traversals 大幅提升了問題解答和 KG 問題解答任務的效能。程式碼可在 \url{https://github.com/amazon-science/tree-of-traversals} 取得

##### **Small Object Few-shot Segmentation for Vision-based Industrial Inspection**
2407.21351v1 by Zilong Zhang, Chang Niu, Zhibin Zhao, Xingwu Zhang, Xuefeng Chen

Vision-based industrial inspection (VII) aims to locate defects quickly and
accurately. Supervised learning under a close-set setting and industrial
anomaly detection, as two common paradigms in VII, face different problems in
practical applications. The former is that various and sufficient defects are
difficult to obtain, while the latter is that specific defects cannot be
located. To solve these problems, in this paper, we focus on the few-shot
semantic segmentation (FSS) method, which can locate unseen defects conditioned
on a few annotations without retraining. Compared to common objects in natural
images, the defects in VII are small. This brings two problems to current FSS
methods: 1 distortion of target semantics and 2 many false positives for
backgrounds. To alleviate these problems, we propose a small object few-shot
segmentation (SOFS) model. The key idea for alleviating 1 is to avoid the
resizing of the original image and correctly indicate the intensity of target
semantics. SOFS achieves this idea via the non-resizing procedure and the
prototype intensity downsampling of support annotations. To alleviate 2, we
design an abnormal prior map in SOFS to guide the model to reduce false
positives and propose a mixed normal Dice loss to preferentially prevent the
model from predicting false positives. SOFS can achieve FSS and few-shot
anomaly detection determined by support masks. Diverse experiments substantiate
the superior performance of SOFS. Code is available at
https://github.com/zhangzilongc/SOFS.

摘要：<paragraph>基於視覺的工業檢測 (VII) 旨在快速且準確地找出缺陷。有監督學習在封閉式設定下和工業異常偵測，作為 VII 中兩個常見的範例，在實際應用中面臨不同的問題。前者是難以取得各種且足夠的缺陷，而後者則是無法找出特定的缺陷。為了解決這些問題，在本文中，我們專注於少樣本語意分割 (FSS) 方法，它可以在不重新訓練的情況下，根據少數註解找出未見過的缺陷。與自然影像中的常見物件相比，VII 中的缺陷很小。這為目前的 FSS 方法帶來兩個問題：1 目標語意的失真和 2 背景的許多假陽性。為了減輕這些問題，我們提出一個小型物件少樣本分割 (SOFS) 模型。減輕 1 的關鍵想法是避免調整原始影像的大小，並正確指出目標語意的強度。SOFS 透過非調整大小的程序和支援註解的原型強度下採樣來達成這個想法。為了減輕 2，我們在 SOFS 中設計一個異常先驗地圖，引導模型減少假陽性，並提出一個混合正規 Dice 損失，以優先防止模型預測假陽性。SOFS 可以達成由支援遮罩決定的 FSS 和少樣本異常偵測。各種實驗證實了 SOFS 的優異效能。程式碼可在 https://github.com/zhangzilongc/SOFS 取得。</paragraph>

##### **Differentially Private Block-wise Gradient Shuffle for Deep Learning**
2407.21347v1 by David Zagardo

Traditional Differentially Private Stochastic Gradient Descent (DP-SGD)
introduces statistical noise on top of gradients drawn from a Gaussian
distribution to ensure privacy. This paper introduces the novel Differentially
Private Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning.
BloGS builds off of existing private deep learning literature, but makes a
definitive shift by taking a probabilistic approach to gradient noise
introduction through shuffling modeled after information theoretic privacy
analyses. The theoretical results presented in this paper show that the
combination of shuffling, parameter-specific block size selection, batch layer
clipping, and gradient accumulation allows DP-BloGS to achieve training times
close to that of non-private training while maintaining similar privacy and
utility guarantees to DP-SGD. DP-BloGS is found to be significantly more
resistant to data extraction attempts than DP-SGD. The theoretical results are
validated by the experimental findings.

摘要：傳統差分私人隨機梯度下降（DP-SGD）
在從高斯分佈中繪製的梯度上引入統計噪聲以確保隱私。本文介紹了用於深度學習的全新差分私人區塊梯度混洗（DP-BloGS）演算法。
BloGS 建構於現有的私人深度學習文獻之上，但透過採用機率方法來引入梯度噪聲，並透過模擬資訊理論隱私分析中的洗牌進行建模，從而做出明確的轉變。本文中提出的理論結果表明，洗牌、特定於參數的區塊大小選擇、批次層裁剪和梯度累積的組合使 DP-BloGS 能夠實現接近於非私人訓練的訓練時間，同時保持與 DP-SGD 類似的隱私和效用保證。發現 DP-BloGS 比 DP-SGD 對資料提取嘗試具有顯著更高的抵抗力。實驗結果驗證了理論結果。

##### **Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction**
2407.21344v1 by Jingyao Wu, Ting Dang, Vidhyasaharan Sethu, Eliathamby Ambikairajah

There has been a significant focus on modelling emotion ambiguity in recent
years, with advancements made in representing emotions as distributions to
capture ambiguity. However, there has been comparatively less effort devoted to
the consideration of temporal dependencies in emotion distributions which
encodes ambiguity in perceived emotions that evolve smoothly over time.
Recognizing the benefits of using constrained dynamical neural ordinary
differential equations (CD-NODE) to model time series as dynamic processes, we
propose an ambiguity-aware dual-constrained Neural ODE approach to model the
dynamics of emotion distributions on arousal and valence. In our approach, we
utilize ODEs parameterised by neural networks to estimate the distribution
parameters, and we integrate additional constraints to restrict the range of
the system outputs to ensure the validity of predicted distributions. We
evaluated our proposed system on the publicly available RECOLA dataset and
observed very promising performance across a range of evaluation metrics.

摘要：近年來，情緒模糊建模備受關注，在表示情緒為分佈以捕捉模糊性方面取得進展。然而，相對而言，較少關注情緒分佈中的時間依賴性，而時間依賴性編碼了隨著時間推移而平穩演變的感知情緒中的模糊性。認識到使用約束動態神經常微分方程 (CD-NODE) 將時間序列建模為動態過程的好處，我們提出了一種模糊感知雙約束神經 ODE 方法，以建模喚醒和效價的情緒分佈動態。在我們的做法中，我們利用神經網路參數化的 ODE 來估計分佈參數，並整合額外的約束條件來限制系統輸出的範圍，以確保預測分佈的有效性。我們在公開的 RECOLA 資料集上評估了我們提出的系統，並在各種評估指標中觀察到非常有希望的表現。

##### **Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks**
2407.21338v1 by David Valencia, Henry Williams, Yuning Xing, Trevor Gee, Minas Liarokapis, Bruce A. MacDonald

Reinforcement Learning (RL) has been widely used to solve tasks where the
environment consistently provides a dense reward value. However, in real-world
scenarios, rewards can often be poorly defined or sparse. Auxiliary signals are
indispensable for discovering efficient exploration strategies and aiding the
learning process. In this work, inspired by intrinsic motivation theory, we
postulate that the intrinsic stimuli of novelty and surprise can assist in
improving exploration in complex, sparsely rewarded environments. We introduce
a novel sample-efficient method able to learn directly from pixels, an
image-based extension of TD3 with an autoencoder called \textit{NaSA-TD3}. The
experiments demonstrate that NaSA-TD3 is easy to train and an efficient method
for tackling complex continuous-control robotic tasks, both in simulated
environments and real-world settings. NaSA-TD3 outperforms existing
state-of-the-art RL image-based methods in terms of final performance without
requiring pre-trained models or human demonstrations.

摘要：強化學習 (RL) 已廣泛用於解決環境持續提供密集獎勵值的任務。然而，在真實世界的場景中，獎勵通常可能定義不佳或稀疏。輔助訊號對於發現有效的探索策略和協助學習過程是不可或缺的。在這項工作中，受到內在動機理論的啟發，我們假設新奇和驚奇的內在刺激有助於改善在複雜、獎勵稀疏的環境中探索。我們介紹一種新穎的樣本有效率方法，能夠直接從像素學習，一種 TD3 的基於影像的延伸，帶有稱為 \textit{NaSA-TD3} 的自動編碼器。實驗證明，NaSA-TD3 易於訓練，且是一種有效的方法，用於解決複雜的連續控制機器人任務，無論是在模擬環境或真實世界設定中。NaSA-TD3 在最終效能方面優於現有的最先進 RL 基於影像的方法，而不需要預先訓練的模型或人類示範。

##### **Performance of Recent Large Language Models for a Low-Resourced Language**
2407.21330v1 by Ravindu Jayakody, Gihan Dias

Large Language Models (LLMs) have shown significant advances in the past
year. In addition to new versions of GPT and Llama, several other LLMs have
been introduced recently. Some of these are open models available for download
and modification.
  Although multilingual large language models have been available for some
time, their performance on low-resourced languages such as Sinhala has been
poor. We evaluated four recent LLMs on their performance directly in the
Sinhala language, and by translation to and from English. We also evaluated
their fine-tunability with a small amount of fine-tuning data. Claude and GPT
4o perform well out-of-the-box and do significantly better than previous
versions. Llama and Mistral perform poorly but show some promise of improvement
with fine tuning.

摘要：大型語言模型 (LLM) 在過去一年中展現出顯著的進步。除了 GPT 和 Llama 的新版本之外，最近還推出了其他幾種 LLM。其中一些是開放模型，可供下載和修改。
儘管多語言大型語言模型已經存在一段時間，但它們在僧伽羅語等低資源語言上的表現一直很差。我們直接在僧伽羅語和通過翻譯成英語和從英語翻譯的方式評估了四種最新的 LLM 的表現。我們還評估了它們在少量微調數據下的微調能力。Claude 和 GPT 4o 開箱即用表現良好，並且比之前的版本有顯著的進步。Llama 和 Mistral 表現不佳，但微調後表現出了一些改進的希望。

##### **MetaOpenFOAM: an LLM-based multi-agent framework for CFD**
2407.21320v1 by Yuxuan Chena, Xu Zhua, Hua Zhoua, Zhuyin Rena

Remarkable progress has been made in automated problem solving through
societies of agents based on large language models (LLMs). Computational fluid
dynamics (CFD), as a complex problem, presents unique challenges in automated
simulations that require sophisticated solutions. MetaOpenFOAM, as a novel
multi-agent collaborations framework, aims to complete CFD simulation tasks
with only natural language as input. These simulation tasks include mesh
pre-processing, simulation and post-processing, etc. MetaOpenFOAM harnesses the
power of MetaGPT's assembly line paradigm, which assigns diverse roles to
various agents, efficiently breaking down complex CFD tasks into manageable
subtasks. Langchain further complements MetaOpenFOAM by integrating
Retrieval-Augmented Generation (RAG) technology, which enhances the framework's
ability by integrating a searchable database of OpenFOAM tutorials for LLMs.
Tests on a benchmark for natural language-based CFD solver, consisting of 8 CFD
simulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per
test (85%), with each test case costing only $0.22 on average. The 8 CFD
simulation tasks include compressible and incompressible flows, 2D and 3D
flows, heat transfer, and combustion, demonstrating the ability to automate CFD
simulations using only natural language input and iteratively correct errors to
achieve the desired simulation at a low cost. An ablation study was conducted
to verify the necessity of each component in the multi-agent system and the RAG
technology. A sensitivity study on the randomness of LLM showed that LLM with
low randomness can obtain more stable and accurate results. Additionally,
MetaOpenFOAM own the ability to identify and modify key parameters in user
requirements and excels in correcting bugs when failures occur, with or without
human participation, which demonstrates the generalization of MetaOpenFOAM.

摘要：<paragraph>利用基於大型語言模型 (LLM) 的代理社群，在自動化問題解決方面取得了顯著的進展。計算流體力學 (CFD) 作為一個複雜的問題，在需要精密解決方案的自動化模擬中提出了獨特的挑戰。MetaOpenFOAM 作為一個新穎的多重代理協作框架，旨在僅以自然語言作為輸入來完成 CFD 模擬任務。這些模擬任務包括網格前處理、模擬和後處理等。MetaOpenFOAM 利用了 MetaGPT 組裝線範例的力量，將不同的角色分配給不同的代理，有效地將複雜的 CFD 任務分解為可管理的子任務。Langchain 進一步透過整合檢索增強生成 (RAG) 技術來補充 MetaOpenFOAM，這透過整合可搜尋的 OpenFOAM 教學資料庫來增強 LLM 的框架能力。在一個基於自然語言的 CFD 求解器的基準測試中，包含 8 個 CFD 模擬任務，結果顯示 MetaOpenFOAM 在每個測試中都達到了很高的通過率 (85%)，每個測試案例平均只花費 0.22 美元。這 8 個 CFD 模擬任務包括可壓縮和不可壓縮流、2D 和 3D 流、熱傳遞和燃燒，展示了僅使用自然語言輸入和反覆更正錯誤就能自動執行 CFD 模擬的能力，以低成本實現所需的模擬。進行了消融研究以驗證多重代理系統和 RAG 技術中每個組成的必要性。對 LLM 隨機性的敏感性研究表明，隨機性低的 LLM 可以獲得更穩定和準確的結果。此外，MetaOpenFOAM 擁有識別和修改使用者需求中關鍵參數的能力，並且在發生故障時擅長修正錯誤，無論是否有人參與，這證明了 MetaOpenFOAM 的概括性。</paragraph>

##### **Big Cooperative Learning**
2407.21319v1 by Yulai Cong

Cooperation plays a pivotal role in the evolution of human intelligence;
moreover, it also underlies the recent revolutionary advancement of artificial
intelligence (AI) that is driven by foundation models. Specifically, we reveal
that the training of foundation models can be interpreted as a form of big
cooperative learning (\textit{abbr.} big learning), where massive learning
individuals/tasks \emph{cooperate} to approach the unique essence of data from
diverse perspectives of data prediction, leveraging a universal model. The
presented big learning therefore unifies most training objectives of foundation
models within a consistent framework, where their underlying assumptions are
exposed simultaneously. We design tailored simulations to demonstrate the
principle of big learning, based on which we provide learning-perspective
justifications for the successes of foundation models, with interesting
side-products. Furthermore, we reveal that big learning is a new dimension for
upgrading conventional machine learning paradigms, valuable for endowing
reinvigorations to associated applications; as an illustrative example, we
propose the BigLearn-GAN, which is a novel adversarially-trained foundation
model with versatile data sampling capabilities. Code is available at
\texttt{https://github.com/YulaiCong/BigCooperativeLearning}.

摘要：合作在人類智能的演化中扮演著舉足輕重的角色；
此外，它也支撐著近期由基礎模型推動的人工智慧 (AI) 的革命性進展。具體而言，我們揭示了基礎模型的訓練可以被解釋為一種大型合作學習（簡稱大學習）的形式，其中大量的學習個體/任務「合作」從資料預測的不同觀點來接近資料的獨特本質，利用一個通用模型。因此，所提出的「大學習」在一個一致的架構中統一了基礎模型的大部分訓練目標，同時揭露了其背後的假設。我們設計了客製化的模擬來展示大學習的原理，並根據此原理，為基礎模型的成功提供學習觀點的合理化，並產生有趣的副產品。此外，我們揭示了大學習是升級傳統機器學習範例的新面向，對於賦予相關應用程式新的活力非常有價值；作為一個說明性的範例，我們提出了 BigLearn-GAN，這是一個新穎的對抗式訓練基礎模型，具備多功能的資料取樣能力。程式碼可在 \texttt{https://github.com/YulaiCong/BigCooperativeLearning} 取得。

##### **Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances**
2407.21315v2 by Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan Donbekci, Julia Hirschberg

This paper introduces a novel approach to emotion detection in speech using
Large Language Models (LLMs). We address the limitation of LLMs in processing
audio inputs by translating speech characteristics into natural language
descriptions. Our method integrates these descriptions into text prompts,
enabling LLMs to perform multimodal emotion analysis without architectural
modifications. We evaluate our approach on two datasets: IEMOCAP and MELD,
demonstrating significant improvements in emotion recognition accuracy,
particularly for high-quality audio data. Our experiments show that
incorporating speech descriptions yields a 2 percentage point increase in
weighted F1 score on IEMOCAP (from 70.111\% to 72.596\%). We also compare
various LLM architectures and explore the effectiveness of different feature
representations. Our findings highlight the potential of this approach in
enhancing emotion detection capabilities of LLMs and underscore the importance
of audio quality in speech-based emotion recognition tasks. We'll release the
source code on Github.

摘要：本文提出了一個新穎的方法，使用大型語言模型 (LLM) 進行語音的情緒偵測。我們透過將語音特徵轉換為自然語言描述，來解決 LLM 在處理音訊輸入時的限制。我們的技術將這些描述整合到文字提示中，讓 LLM 能夠在不修改架構的情況下執行多模態的情緒分析。我們在兩個資料集：IEMOCAP 和 MELD 上評估我們的技術，顯示出在情緒辨識準確度上大幅提升，特別是對於高品質的音訊資料。我們的實驗顯示，加入語音描述讓 IEMOCAP 上的加權 F1 分數提升了 2 個百分點（從 70.111% 到 72.596%）。我們也比較了各種 LLM 架構，並探索了不同特徵表徵的有效性。我們的發現突顯了這種技術在提升 LLM 情緒偵測能力的潛力，並強調了音訊品質在基於語音的情緒辨識任務中的重要性。我們將在 Github 上釋出原始碼。

##### **EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer**
2407.21311v1 by Ali Abedi, Q. M. Jonathan Wu, Ning Zhang, Farhad Pourpanah

Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue,
where the distribution of training (source) data differs from that of testing
(target) data. Many models have been developed to tackle this problem, and
recently vision transformers (ViTs) have shown promising results. However, the
complexity and large number of trainable parameters of ViTs restrict their
deployment in practical applications. This underscores the need for an
efficient model that not only reduces trainable parameters but also allows for
adjustable complexity based on specific needs while delivering comparable
performance. To achieve this, in this paper we introduce an Efficient
Unsupervised Domain Adaptation (EUDA) framework. EUDA employs the DINOv2, which
is a self-supervised ViT, as a feature extractor followed by a simplified
bottleneck of fully connected layers to refine features for enhanced domain
adaptation. Additionally, EUDA employs the synergistic domain alignment loss
(SDAL), which integrates cross-entropy (CE) and maximum mean discrepancy (MMD)
losses, to balance adaptation by minimizing classification errors in the source
domain while aligning the source and target domain distributions. The
experimental results indicate the effectiveness of EUDA in producing comparable
results as compared with other state-of-the-art methods in domain adaptation
with significantly fewer trainable parameters, between 42% to 99.7% fewer. This
showcases the ability to train the model in a resource-limited environment. The
code of the model is available at: https://github.com/A-Abedi/EUDA.

摘要：無監督域適應 (UDA) 旨在減輕域偏移問題，其中訓練 (來源) 資料的分配與測試 (目標) 資料的分配不同。已開發許多模型來解決此問題，而最近的視覺轉換器 (ViT) 已顯示出有希望的結果。然而，ViT 的複雜性和大量的可訓練參數限制了它們在實際應用中的部署。這強調了對一種有效模型的需求，該模型不僅可以減少可訓練參數，還可以根據特定需求調整複雜性，同時提供可比較的效能。為了實現這一點，我們在本文中介紹了一個有效的無監督域適應 (EUDA) 框架。EUDA 使用 DINOv2，這是一個自監督的 ViT，作為一個特徵提取器，後面是一個簡化的全連接層瓶頸，以優化特徵以增強域適應。此外，EUDA 採用協同域對齊損失 (SDAL)，它整合了交叉熵 (CE) 和最大平均差異 (MMD) 損失，通過最小化來源域中的分類錯誤同時對齊來源和目標域分配來平衡適應。實驗結果表明，與其他最先進的域適應方法相比，EUDA 在產生可比較的結果方面是有效的，可訓練參數顯著減少，減少了 42% 到 99.7%。這展示了在資源受限的環境中訓練模型的能力。模型的程式碼可在 https://github.com/A-Abedi/EUDA 中取得。

##### **Implementing Streaming algorithm and k-means clusters to RAG**
2407.21300v1 by Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang

Retrieval-augmented generation (RAG) has achieved great success in
information retrieval to assist large models because it builds an external
knowledge database. However, it also has many problems: it consumes a lot of
memory because of the huge database. When faced with massive streaming data, it
is unable to update the established index database in time. To save the memory
of building the database and maintain accuracy simultaneously, we proposed a
new approach combining a streaming algorithm and k-means cluster with RAG. Our
approach applies a streaming algorithm to update the index and reduce memory
consumption. Then use the k-means algorithm to cluster documents with high
similarities together, the query time will be shortened by doing this. We
conducted comparative experiments on four methods, and the results show that
RAG with streaming algorithm and k-means cluster performs well in accuracy and
memory. For massive streaming data, we find that our method behaves better than
traditional RAG

摘要：檢索增強式生成（RAG）在資訊檢索方面已取得巨大成功，可協助大型模型建置外部知識資料庫。然而，它也存在許多問題：由於資料庫龐大，它會消耗大量記憶體。當面對大量串流資料時，它無法即時更新已建立的索引資料庫。為了同時節省建置資料庫的記憶體並維持準確性，我們提出了一種結合串流演算法和 k 平均值群集與 RAG 的新方法。我們的做法是應用串流演算法來更新索引並減少記憶體消耗。然後使用 k 平均值演算法將相似度高的文件分群，這樣可以縮短查詢時間。我們對四種方法進行了比較實驗，結果顯示，採用串流演算法和 k 平均值群集的 RAG 在準確性和記憶體方面表現良好。對於大量串流資料，我們發現我們的做法優於傳統 RAG

##### **Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models**
2407.21299v1 by Kaustav Bhattacharjee, Soumya Kundu, Indrasis Chakraborty, Aritra Dasgupta

Net load forecasting is crucial for energy planning and facilitating informed
decision-making regarding trade and load distributions. However, evaluating
forecasting models' performance against benchmark models remains challenging,
thereby impeding experts' trust in the model's performance. In this context,
there is a demand for technological interventions that allow scientists to
compare models across various timeframes and solar penetration levels. This
paper introduces a visual analytics-based application designed to compare the
performance of deep-learning-based net load forecasting models with other
models for probabilistic net load forecasting. This application employs
carefully selected visual analytic interventions, enabling users to discern
differences in model performance across different solar penetration levels,
dataset resolutions, and hours of the day over multiple months. We also present
observations made using our application through a case study, demonstrating the
effectiveness of visualizations in aiding scientists in making informed
decisions and enhancing trust in net load forecasting models.

摘要：淨載荷預測對於能源規劃和促進有關貿易和負載分配的明智決策至關重要。然而，評估預測模型相對於基準模型的效能仍然具有挑戰性，從而阻礙專家對模型效能的信任。在此背景下，需要技術干預，允許科學家在各種時間範圍和太陽能滲透率層級中比較模型。本文介紹了一個基於視覺分析的應用程式，旨在比較基於深度學習的淨載荷預測模型與其他機率淨載荷預測模型的效能。此應用程式採用仔細挑選的視覺分析干預措施，使用戶能夠辨別模型效能的差異，這些差異存在於不同的太陽能滲透率層級、資料集解析度和多個月的白天時段中。我們也透過案例研究提出使用我們的應用程式所做的觀察，證明視覺化在協助科學家做出明智決策和增強對淨載荷預測模型的信任方面是有效的。

##### **SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**
2407.21293v1 by Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, Shaohua Wu

Many fields could benefit from the rapid development of the large language
models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the
typically fields facing new opportunities as the LLMs have supported more and
more modalities. Here, by utilizing vision-language model (VLM), we proposed an
e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided
into four stages, which are perception, prediction, planning, and behavior.
Each stage consists of several visual question answering (VQA) pairs and VQA
pairs interconnect with each other constructing a graph called Graph VQA
(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our
method could achieve e2e driving with language. In our method, vision
transformers (ViT) models are employed to process nuScenes visual data, while
VLM are utilized to interpret and reason about the information extracted from
the visual inputs. In the perception stage, the system identifies and
classifies objects from the driving environment. The prediction stage involves
forecasting the potential movements of these objects. The planning stage
utilizes the gathered information to develop a driving strategy, ensuring the
safety and efficiency of the autonomous vehicle. Finally, the behavior stage
translates the planned actions into executable commands for the vehicle. Our
experiments demonstrate that SimpleLLM4AD achieves competitive performance in
complex driving scenarios.

摘要：大型語言模型 (LLM) 的快速發展可能使許多領域受益。端到端自動駕駛 (e2eAD) 是典型領域之一，因為 LLM 支援越來越多的模式，因此面臨新的機會。在此，透過利用視覺語言模型 (VLM)，我們提出了一個稱為 SimpleLLM4AD 的 e2eAD 方法。在我們的模型中，e2eAD 任務分為四個階段，分別是感知、預測、規劃和行為。每個階段包含多個視覺問答 (VQA) 配對，且 VQA 配對相互連接，構建一個稱為圖形 VQA (GVQA) 的圖形。透過 VLM 分階段推理 GVQA 中的每個 VQA 配對，我們的模型可以透過語言實現端到端駕駛。在我們的模型中，採用視覺Transformer (ViT) 模型來處理 nuScenes 視覺資料，同時利用 VLM 來詮釋和推理從視覺輸入中提取的資訊。在感知階段，系統識別和分類駕駛環境中的物件。預測階段涉及預測這些物件的潛在移動。規劃階段利用收集的資訊來制定駕駛策略，確保自動駕駛汽車的安全性和效率。最後，行為階段將規劃的動作轉換為車輛可執行的命令。我們的實驗證明，SimpleLLM4AD 在複雜的駕駛場景中實現了競爭力。

##### **Robust Box Prompt based SAM for Medical Image Segmentation**
2407.21284v1 by Yuhao Huang, Xin Yang, Han Zhou, Yan Cao, Haoran Dou, Fajin Dong, Dong Ni

The Segment Anything Model (SAM) can achieve satisfactory segmentation
performance under high-quality box prompts. However, SAM's robustness is
compromised by the decline in box quality, limiting its practicality in
clinical reality. In this study, we propose a novel Robust Box prompt based SAM
(\textbf{RoBox-SAM}) to ensure SAM's segmentation performance under prompts
with different qualities. Our contribution is three-fold. First, we propose a
prompt refinement module to implicitly perceive the potential targets, and
output the offsets to directly transform the low-quality box prompt into a
high-quality one. We then provide an online iterative strategy for further
prompt refinement. Second, we introduce a prompt enhancement module to
automatically generate point prompts to assist the box-promptable segmentation
effectively. Last, we build a self-information extractor to encode the prior
information from the input image. These features can optimize the image
embeddings and attention calculation, thus, the robustness of SAM can be
further enhanced. Extensive experiments on the large medical segmentation
dataset including 99,299 images, 5 modalities, and 25 organs/targets validated
the efficacy of our proposed RoBox-SAM.

摘要：分段任何模型 (SAM) 可以在高质量框提示下实现令人满意的分段性能。然而，SAM 的鲁棒性因框质量的下降而受到损害，限制了其在临床现实中的实用性。在这项研究中，我们提出了一个基于 SAM 的新型鲁棒框提示（**RoBox-SAM**），以确保 SAM 在具有不同质量的提示下的分段性能。我们的贡献是三方面的。首先，我们提出一个提示优化模块，以隐式感知潜在目标，并输出偏移量，以直接将低质量框提示转换为高质量提示。然后，我们提供了一个在线迭代策略，以便进一步优化提示。其次，我们引入了一个提示增强模块，以自动生成点提示，以有效地辅助框提示分段。最后，我们构建了一个自信息提取器，以对来自输入图像的先验信息进行编码。这些特征可以优化图像嵌入和注意力计算，因此，可以进一步增强 SAM 的鲁棒性。在包括 99,299 张图像、5 种方式和 25 个器官/目标的大型医学分段数据集上进行的广泛实验验证了我们提出的 RoBox-SAM 的功效。

##### **Multi-Level Querying using A Knowledge Pyramid**
2407.21276v1 by Rubing Chen, Xulu Zhang, Jiaxin Wu, Wenqi Fan, Xiao-Yong Wei, Qing Li

This paper addresses the need for improved precision in existing
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395\% F1 gain by improving its performance from 0.1636 to 0.8109.

摘要：本文探討了現有檢索增強生成 (RAG) 方法中，對於精準度的提升需求，這些方法主要專注於增強召回率。我們在 RAG 架構中提出一個多層知識金字塔方法，以在精準度和召回率之間取得更好的平衡。知識金字塔包含三個層級：本体、知識圖譜 (KG) 和基於區塊的原始文字。我們採用跨層增強技術，以實現全面的知識涵蓋範圍和本体架構及實例的動態更新。為了確保緊湊性，我們利用跨層過濾方法，以進行 KG 中的知識濃縮。我們的方法稱為 PolyRAG，它遵循瀑布模型進行檢索，從金字塔頂端開始，並向下進行，直到獲得確定的答案。我們引入了兩個特定領域知識檢索基準，一個在學術領域，另一個在金融領域。這些方法的有效性已通過全面實驗驗證，其表現優於 19 種 SOTA 方法。一個令人振奮的觀察結果是，所提出的方法已經增強了 GPT-4，通過將其效能從 0.1636 提升至 0.8109，提供了 395% 的 F1 增益。

##### **Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net**
2407.21273v1 by Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski

Efficient intravascular access in trauma and critical care significantly
impacts patient outcomes. However, the availability of skilled medical
personnel in austere environments is often limited. Autonomous robotic
ultrasound systems can aid in needle insertion for medication delivery and
support non-experts in such tasks. Despite advances in autonomous needle
insertion, inaccuracies in vessel segmentation predictions pose risks.
Understanding the uncertainty of predictive models in ultrasound imaging is
crucial for assessing their reliability. We introduce MSU-Net, a novel
multistage approach for training an ensemble of U-Nets to yield accurate
ultrasound image segmentation maps. We demonstrate substantial improvements,
18.1% over a single Monte Carlo U-Net, enhancing uncertainty evaluations, model
transparency, and trustworthiness. By highlighting areas of model certainty,
MSU-Net can guide safe needle insertions, empowering non-experts to accomplish
such tasks.

摘要：在創傷和重症照護中，有效的血管內通路會顯著影響病患的治療結果。然而，在惡劣的環境中，熟練的醫療人員往往不足。自主機器人超音波系統可以協助針頭插入，以提供藥物並支援非專家執行此類任務。儘管自主針頭插入技術進步，但血管分割預測的不準確性會造成風險。了解超音波影像中預測模型的不確定性，對於評估其可靠性至關重要。我們引進 MSU-Net，這是一種新穎的多階段方法，用於訓練一組 U-Net 以產生準確的超音波影像分割圖。我們展示了大幅改善，比單一的蒙地卡羅 U-Net 改善了 18.1%，增強了不確定性評估、模型透明度和可信度。透過強調模型確定性的區域，MSU-Net 可以引導安全的針頭插入，讓非專家也能執行此類任務。

##### **DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations**
2407.21267v1 by Dongwon Son, Sanghyeon Son, Jaehyung Kim, Beomjoon Kim

We present DEF-oriCORN, a framework for language-directed manipulation tasks.
By leveraging a novel object-based scene representation and
diffusion-model-based state estimation algorithm, our framework enables
efficient and robust manipulation planning in response to verbal commands, even
in tightly packed environments with sparse camera views without any
demonstrations. Unlike traditional representations, our representation affords
efficient collision checking and language grounding. Compared to
state-of-the-art baselines, our framework achieves superior estimation and
motion planning performance from sparse RGB images and zero-shot generalizes to
real-world scenarios with diverse materials, including transparent and
reflective objects, despite being trained exclusively in simulation. Our code
for data generation, training, inference, and pre-trained weights are publicly
available at: https://sites.google.com/view/def-oricorn/home.

摘要：我們提出 DEF-oriCORN，一個用於語言導向操作任務的框架。
透過利用新穎的基於物件的場景表示和
基於擴散模型的狀態估計演算法，我們的框架能夠
有效且穩健地進行操作規劃，以回應口頭指令，即使
在緊密封閉的環境中，使用稀疏相機視角，且無任何
示範。與傳統表示法不同，我們的表示法提供
有效的碰撞檢查和語言基礎。與
最先進的基準相比，我們的框架從稀疏 RGB 影像中獲得優異的估計和
動作規劃效能，並在零次學習的情況下概括到
真實世界場景，其中包含各種材料，包括透明和
反射物體，儘管僅在模擬中進行訓練。我們的資料產生、訓練、推論和預先訓練權重的程式碼已公開於：https://sites.google.com/view/def-oricorn/home。

##### **Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning**
2407.21264v1 by Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu, Huan Liu

Model attribution for machine-generated disinformation poses a significant
challenge in understanding its origins and mitigating its spread. This task is
especially challenging because modern large language models (LLMs) produce
disinformation with human-like quality. Additionally, the diversity in
prompting methods used to generate disinformation complicates accurate source
attribution. These methods introduce domain-specific features that can mask the
fundamental characteristics of the models. In this paper, we introduce the
concept of model attribution as a domain generalization problem, where each
prompting method represents a unique domain. We argue that an effective
attribution model must be invariant to these domain-specific features. It
should also be proficient in identifying the originating models across all
scenarios, reflecting real-world detection challenges. To address this, we
introduce a novel approach based on Supervised Contrastive Learning. This
method is designed to enhance the model's robustness to variations in prompts
and focuses on distinguishing between different source LLMs. We evaluate our
model through rigorous experiments involving three common prompting methods:
``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:
``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the
effectiveness of our approach in model attribution tasks, achieving
state-of-the-art performance across diverse and unseen datasets.

摘要：機器產生的錯誤訊息的模型歸因對了解其起源和減輕其傳播構成重大挑戰。這項任務特別具有挑戰性，因為現代大型語言模型 (LLM) 會產生具有類人品質的錯誤訊息。此外，用於產生錯誤訊息的提示方法的多樣性使得準確的來源歸因變得複雜。這些方法引入了特定於領域的功能，這些功能可能會掩蓋模型的基本特徵。在本文中，我們將模型歸因的概念引入為領域概化問題，其中每個提示方法代表一個獨特的領域。我們認為，一個有效的歸因模型必須不變於這些特定於領域的功能。它還應該能夠識別所有場景中的原始模型，反映現實世界的偵測挑戰。為了解決這個問題，我們引入了一個基於監督對比學習的新方法。此方法旨在增強模型對提示變化的魯棒性，並專注於區分不同的來源 LLM。我們透過涉及三種常見提示方法的嚴格實驗來評估我們的模型：``開放式''、``重寫''和``改寫''，以及三種進階 LLM：``llama 2''、``chatgpt''和``vicuna''。我們的結果證明了我們的方法在模型歸因任務中的有效性，在多樣化且未見的資料集上實現了最先進的效能。

##### **Lifelong Person Search**
2407.21252v1 by Jae-Won Yang, Seungbin Hong, Jae-Young Sim

Person search is the task to localize a query person in gallery datasets of
scene images. Existing methods have been mainly developed to handle a single
target dataset only, however diverse datasets are continuously given in
practical applications of person search. In such cases, they suffer from the
catastrophic knowledge forgetting in the old datasets when trained on new
datasets. In this paper, we first introduce a novel problem of lifelong person
search (LPS) where the model is incrementally trained on the new datasets while
preserving the knowledge learned in the old datasets. We propose an end-to-end
LPS framework that facilitates the knowledge distillation to enforce the
consistency learning between the old and new models by utilizing the prototype
features of the foreground persons as well as the hard background proposals in
the old domains. Moreover, we also devise the rehearsal-based instance matching
to further improve the discrimination ability in the old domains by using the
unlabeled person instances additionally. Experimental results demonstrate that
the proposed method achieves significantly superior performance of both the
detection and re-identification to preserve the knowledge learned in the old
domains compared with the existing methods.

摘要：人物搜尋是將查詢人物定位在場景影像的資料庫資料集中的任務。現有方法主要開發為僅處理單一目標資料集，然而在人物搜尋的實際應用中，會持續提供多樣化的資料集。在這種情況下，當在新的資料集上訓練時，舊資料集會遭受災難性的知識遺忘。在本文中，我們首先介紹終身人物搜尋 (LPS) 的新問題，其中模型會逐步在新的資料集上訓練，同時保留在舊資料集中学到的知識。我們提出一個端對端的 LPS 架構，透過利用前景人物的原型特徵和舊網域中的硬背景提案，促進知識萃取，以強制舊模型和新模型之間的一致性學習。此外，我們還設計了基於彩排的實例匹配，以透過另外使用未標記的人物實例，進一步提升舊網域中的辨別能力。實驗結果證明，與現有方法相比，所提出的方法在偵測和再辨識方面都獲得顯著的優異效能，以保留在舊網域中學到的知識。

##### **Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens**
2407.21248v1 by Anqi Zhang, Chaofeng Wu

While large language models (LLMs) are extensively used, there are raising
concerns regarding privacy, security, and copyright due to their opaque
training data, which brings the problem of detecting pre-training data on the
table. Current solutions to this problem leverage techniques explored in
machine learning privacy such as Membership Inference Attacks (MIAs), which
heavily depend on LLMs' capability of verbatim memorization. However, this
reliance presents challenges, especially given the vast amount of training data
and the restricted number of effective training epochs. In this paper, we
propose an adaptive pre-training data detection method which alleviates this
reliance and effectively amplify the identification. Our method adaptively
locates \textit{surprising tokens} of the input. A token is surprising to a LLM
if the prediction on the token is "certain but wrong", which refers to low
Shannon entropy of the probability distribution and low probability of the
ground truth token at the same time. By using the prediction probability of
surprising tokens to measure \textit{surprising}, the detection method is
achieved based on the simple hypothesis that seeing seen data is less
surprising for the model compared with seeing unseen data. The method can be
applied without any access to the the pre-training data corpus or additional
training like reference models. Our approach exhibits a consistent enhancement
compared to existing methods in diverse experiments conducted on various
benchmarks and models, achieving a maximum improvement of 29.5\%. We also
introduce a new benchmark Dolma-Book developed upon a novel framework, which
employs book data collected both before and after model training to provide
further evaluation.

摘要：<paragraph>儘管大型語言模型 (LLM) 被廣泛使用，但由於它們不透明的訓練資料，對於隱私、安全性，以及版權的問題正逐漸浮現，這也帶來了在表格中偵測預訓練資料的問題。目前解決這個問題的方法，利用了機器學習隱私中所探討的技術，例如成員身分推論攻擊 (MIA)，這在很大程度上依賴於 LLM 的逐字記憶能力。然而，這種依賴性帶來了挑戰，特別是考量到大量的訓練資料和受限的有效訓練輪次。在本文中，我們提出了一種適應性預訓練資料偵測方法，可以減輕這種依賴性，並有效地擴大識別範圍。我們的適應性方法定位輸入的「令人驚訝的符號」。如果符號的預測是「確定但錯誤」，則該符號對 LLM 來說是令人驚訝的，這指的是機率分佈的香農熵低，且同時具有低於真實符號的機率。透過使用令人驚訝的符號的預測機率來衡量「令人驚訝」，偵測方法是基於一個簡單的假設，與看到未見過的資料相比，看到已見過的資料對於模型來說較不令人驚訝。此方法可以應用，而無需存取預訓練資料語料庫或額外的訓練，例如參考模型。與在各種基準和模型上進行的不同實驗中現有的方法相比，我們的做法展現了一致的進步，達到了 29.5% 的最大改進。我們還引入了基於新架構開發的新基準 Dolma-Book，它採用在模型訓練前後收集的書籍資料來提供進一步的評估。</paragraph>

##### **Informed Correctors for Discrete Diffusion Models**
2407.21243v1 by Yixiu Zhao, Jiaxin Shi, Lester Mackey, Scott Linderman

Discrete diffusion modeling is a promising framework for modeling and
generating data in discrete spaces. To sample from these models, different
strategies present trade-offs between computation and sample quality. A
predominant sampling strategy is predictor-corrector $\tau$-leaping, which
simulates the continuous time generative process with discretized predictor
steps and counteracts the accumulation of discretization error via corrector
steps. However, for absorbing state diffusion, an important class of discrete
diffusion models, the standard forward-backward corrector can be ineffective in
fixing such errors, resulting in subpar sample quality. To remedy this problem,
we propose a family of informed correctors that more reliably counteracts
discretization error by leveraging information learned by the model. For
further efficiency gains, we also propose $k$-Gillespie's, a sampling algorithm
that better utilizes each model evaluation, while still enjoying the speed and
flexibility of $\tau$-leaping. Across several real and synthetic datasets, we
show that $k$-Gillespie's with informed correctors reliably produces higher
quality samples at lower computational cost.

摘要：離散擴散模型是一種用於對離散空間中的資料進行建模和生成的很有前景的框架。為了從這些模型中取樣，不同的策略在運算和樣本品質之間進行權衡。一種主要的取樣策略是預測校正 $\tau$-跳躍，它使用離散化預測步驟模擬連續時間生成過程，並透過校正步驟抵消離散化誤差的累積。然而，對於吸收狀態擴散（離散擴散模型中的一個重要類別），標準的前向後向校正器在修正此類誤差時可能無效，導致次佳的樣本品質。為了解決這個問題，我們提出了一系列的知情校正器，這些校正器透過利用模型學習到的資訊，更可靠地抵消離散化誤差。為了進一步提高效率，我們還提出了 $k$-Gillespie，這是一種取樣演算法，它能更好地利用每個模型評估，同時仍享有 $\tau$-跳躍的速度和靈活性。在幾個真實和合成資料集上，我們展示了使用知情校正器的 $k$-Gillespie 能可靠地以較低的運算成本產生較高品質的樣本。

##### **Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration**
2407.21229v1 by Ngoc Son Nguyen, Van Son Nguyen, Tung Le

Visual Question Answering (VQA) has recently emerged as a potential research
domain, captivating the interest of many in the field of artificial
intelligence and computer vision. Despite the prevalence of approaches in
English, there is a notable lack of systems specifically developed for certain
languages, particularly Vietnamese. This study aims to bridge this gap by
conducting comprehensive experiments on the Vietnamese Visual Question
Answering (ViVQA) dataset, demonstrating the effectiveness of our proposed
model. In response to community interest, we have developed a model that
enhances image representation capabilities, thereby improving overall
performance in the ViVQA system. Specifically, our model integrates the
Bootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2)
and the convolutional neural network EfficientNet to extract and process both
local and global features from images. This integration leverages the strengths
of transformer-based architectures for capturing comprehensive contextual
information and convolutional networks for detailed local features. By freezing
the parameters of these pre-trained models, we significantly reduce the
computational cost and training time, while maintaining high performance. This
approach significantly improves image representation and enhances the
performance of existing VQA systems. We then leverage a multi-modal fusion
module based on a general-purpose multi-modal foundation model (BEiT-3) to fuse
the information between visual and textual features. Our experimental findings
demonstrate that our model surpasses competing baselines, achieving promising
performance. This is particularly evident in its accuracy of $71.04\%$ on the
test set of the ViVQA dataset, marking a significant advancement in our
research area. The code is available at https://github.com/nngocson2002/ViVQA.

摘要：視覺問答 (VQA) 最近已成為潛在的研究領域，吸引了許多人工智慧和電腦視覺領域的研究人員。儘管英語中有許多方法，但顯著缺乏專門為特定語言（尤其是越南語）開發的系統。本研究旨在透過對越南視覺問答 (ViVQA) 資料集進行全面實驗，展示我們提出的模型的有效性，以彌補這一差距。為了回應社群的興趣，我們開發了一個模型，增強了影像表徵能力，從而改善了 ViVQA 系統的整體效能。具體來說，我們的模型整合了凍結單模態模型 (BLIP-2) 的引導語言影像預訓練和卷積神經網路 EfficientNet，以從影像中提取和處理局部和全域特徵。這種整合利用了基於 Transformer 的架構來擷取全面的脈絡資訊，以及卷積網路來擷取詳細的局部特徵。透過凍結這些預訓練模型的參數，我們大幅降低了運算成本和訓練時間，同時維持高效能。這種方法顯著改善了影像表徵，並增強了現有 VQA 系統的效能。然後，我們利用基於通用多模態基礎模型 (BEiT-3) 的多模態融合模組，融合視覺和文字特徵之間的資訊。我們的實驗結果表明，我們的模型超越了競爭基準，達到了令人滿意的效能。這在 ViVQA 資料集的測試集中準確率達 71.04% 中特別明顯，標誌著我們研究領域的重大進展。程式碼可在 https://github.com/nngocson2002/ViVQA 取得。

##### **Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models**
2407.21227v1 by Florian Tambon, Amin Nikanjam, Foutse Khomh, Giuliano Antoniol

Large Language Models (LLMs) show promising potential in Software
Engineering, especially for code-related tasks like code completion and code
generation. LLMs' evaluation is generally centred around general metrics
computed over benchmarks. While painting a macroscopic view of the benchmarks
and of the LLMs' capacity, it is unclear how each programming task in these
benchmarks assesses the capabilities of the LLMs. In particular, the difficulty
level of the tasks in the benchmarks is not reflected in the score used to
report the performance of the model. Yet, a model achieving a 90% score on a
benchmark of predominantly easy tasks is likely less capable than a model
achieving a 90% score on a benchmark containing predominantly difficult tasks.
This paper devises a framework, HardEval, for assessing task difficulty for
LLMs and crafting new tasks based on identified hard tasks. The framework uses
a diverse array of prompts for a single task across multiple LLMs to obtain a
difficulty score for each task of a benchmark. Using two code generation
benchmarks, HumanEval+ and ClassEval, we show that HardEval can reliably
identify the hard tasks within those benchmarks, highlighting that only 21% of
HumanEval+ and 27% of ClassEval tasks are hard for LLMs. Through our analysis
of task difficulty, we also characterize 6 practical hard task topics which we
used to generate new hard tasks. Orthogonal to current benchmarking evaluation
efforts, HardEval can assist researchers and practitioners in fostering better
assessments of LLMs. The difficulty score can be used to identify hard tasks
within existing benchmarks. This, in turn, can be leveraged to generate more
hard tasks centred around specific topics either for evaluation or improvement
of LLMs. HardEval generalistic approach can be applied to other domains such as
code completion or Q/A.

摘要：大型語言模型 (LLM) 在軟體工程中展現出令人期待的潛力，特別是在與程式碼相關的任務中，例如程式碼完成和程式碼生成。LLM 的評估通常集中在基準上計算出的通用指標。雖然描繪出基準和 LLM 能力的巨觀觀點，但目前尚不清楚這些基準中的每個程式設計任務如何評估 LLM 的能力。特別是，基準中任務的難度等級並未反映在用於報告模型效能的分數中。然而，在主要由容易任務組成的基準上達到 90% 分數的模型，其能力可能低於在主要由困難任務組成的基準上達到 90% 分數的模型。本文設計了一個架構 HardEval，用於評估 LLM 的任務難度，並根據識別出的困難任務建立新任務。該架構針對單一任務使用多個 LLM 的各種提示，以取得基準中每個任務的難度分數。使用兩個程式碼生成基準 HumanEval+ 和 ClassEval，我們展示 HardEval 可以可靠地識別這些基準中的困難任務，並強調只有 21% 的 HumanEval+ 和 27% 的 ClassEval 任務對 LLM 來說是困難的。透過對任務難度的分析，我們還描述了 6 個實際的困難任務主題，我們用這些主題來產生新的困難任務。與目前的基準評估工作正交，HardEval 可以協助研究人員和從業人員促進對 LLM 的更佳評估。難度分數可用於識別現有基準中的困難任務。反過來，這可以用於產生更多圍繞特定主題的困難任務，以用於評估或改進 LLM。HardEval 的通用方法可以應用於其他領域，例如程式碼完成或問答。

##### **AI methods for approximate compiling of unitaries**
2407.21225v1 by David Kremer, Victor Villar, Sanjay Vishwakarma, Ismael Faro, Juan Cruz-Benito

This paper explores artificial intelligence (AI) methods for the approximate
compiling of unitaries, focusing on the use of fixed two-qubit gates and
arbitrary single-qubit rotations typical in superconducting hardware. Our
approach involves three main stages: identifying an initial template that
approximates the target unitary, predicting initial parameters for this
template, and refining these parameters to maximize the fidelity of the
circuit. We propose AI-driven approaches for the first two stages, with a deep
learning model that suggests initial templates and an autoencoder-like model
that suggests parameter values, which are refined through gradient descent to
achieve the desired fidelity. We demonstrate the method on 2 and 3-qubit
unitaries, showcasing promising improvements over exhaustive search and random
parameter initialization. The results highlight the potential of AI to enhance
the transpiling process, supporting more efficient quantum computations on
current and future quantum hardware.

摘要：本文探討了人工智慧 (AI) 方法，用於近似編譯酉算子，重點在於使用超導硬體中常見的固定雙位元閘和任意單位元旋轉。我們的做法涉及三個主要階段：找出近似目標酉算子的初始範本、預測此範本的初始參數，以及微調這些參數以最大化電路的保真度。我們提出針對前兩個階段的 AI 驅動方法，使用深度學習模型建議初始範本，以及類似自動編碼器的模型建議參數值，透過梯度下降微調這些參數以達到所需的保真度。我們在 2 和 3 位元酉算子上展示此方法，展示出比窮舉搜尋和隨機參數初始化更好的改進。結果突顯了 AI 增強轉譯過程的潛力，支援在現有和未來的量子硬體上進行更有效率的量子運算。

##### **LoRaWAN Based Dynamic Noise Mapping with Machine Learning for Urban Noise Enforcement**
2407.21204v1 by H. Emre Erdem, Henry Leung

Static noise maps depicting long-term noise levels over wide areas are
valuable urban planning assets for municipalities in decreasing noise exposure
of residents. However, non-traffic noise sources with transient behavior, which
people complain frequently, are usually ignored by static maps. We propose here
a dynamic noise mapping approach using the data collected via low-power
wide-area network (LPWAN, specifically LoRaWAN) based internet of things (IoT)
infrastructure, which is one of the most common communication backbones for
smart cities. Noise mapping based on LPWAN is challenging due to the low data
rates of these protocols. The proposed dynamic noise mapping approach
diminishes the negative implications of data rate limitations using machine
learning (ML) for event and location prediction of non-traffic sources based on
the scarce data. The strength of these models lies in their consideration of
the spatial variance in acoustic behavior caused by the buildings in urban
settings. The effectiveness of the proposed method and the accuracy of the
resulting dynamic maps are evaluated in field tests. The results show that the
proposed system can decrease the map error caused by non-traffic sources up to
51% and can stay effective under significant packet losses.

摘要：靜態噪音地圖描繪了廣闊區域的長期噪音水平，對於降低居民的噪音曝露，是市政單位有價值的都市規劃資產。然而，人們經常抱怨的非交通噪音來源具有暫態行為，通常會被靜態地圖忽略。我們在此提出使用透過低功率廣域網路（LPWAN，特別是 LoRaWAN）所收集的資料，以及基於物聯網（IoT）基礎設施的動態噪音繪製方法，這是智慧城市最常見的通訊主幹之一。由於這些協定的資料速率低，因此基於 LPWAN 的噪音繪製具有挑戰性。所提出的動態噪音繪製方法透過機器學習（ML）來預測非交通來源的事件和位置，以減少資料速率限制的負面影響，而資料非常稀少。這些模型的優點在於考量了都市環境中建築物所造成的聲學行為的空間變異。在實地測試中評估了所提出方法的有效性以及產生的動態地圖的準確性。結果顯示，所提出的系統可以將非交通來源造成的映射誤差降低多達 51%，且在封包遺失率顯著的情況下仍能保持有效性。

##### **GenRec: Generative Personalized Sequential Recommendation**
2407.21191v1 by Panfeng Cao, Pietro Lio

Sequential recommendation is a task to capture hidden user preferences from
historical user item interaction data. Significant progress has been made in
this domain by leveraging classification based learning methods. Inspired by
the recent paradigm of 'pretrain, prompt and predict' in NLP, we consider
sequential recommendation as a sequence to sequence generation task and propose
a novel model named Generative Recommendation (GenRec). Unlike classification
based models that learn explicit user and item representations, GenRec utilizes
the sequence modeling capability of Transformer and adopts the masked item
prediction objective to effectively learn the hidden bidirectional sequential
patterns. Different from existing generative sequential recommendation models,
GenRec does not rely on manually designed hard prompts. The input to GenRec is
textual user item sequence and the output is top ranked next items. Moreover,
GenRec is lightweight and requires only a few hours to train effectively in
low-resource settings, making it highly applicable to real-world scenarios and
helping to democratize large language models in the sequential recommendation
domain. Our extensive experiments have demonstrated that GenRec generalizes on
various public real-world datasets and achieves state-of-the-art results. Our
experiments also validate the effectiveness of the the proposed masked item
prediction objective that improves the model performance by a large margin.

摘要：序列推薦任務是從歷史使用者項目互動資料中擷取隱藏使用者偏好。透過利用基於分類的學習方法，已在這個領域取得顯著進展。受到自然語言處理中「預訓練、提示和預測」的新興典範啟發，我們將序列推薦視為序列到序列產生任務，並提出一個名為生成式推薦 (GenRec) 的新穎模型。與學習明確使用者和項目表徵的基於分類的模型不同，GenRec 利用 Transformer 的序列建模能力，並採用遮蔽項目預測目標，以有效學習隱藏的雙向序列模式。與現有的生成式序列推薦模型不同，GenRec 不依賴手動設計的硬提示。GenRec 的輸入是文字使用者項目序列，而輸出是排名最前面的下一個項目。此外，GenRec 非常精簡，只需幾個小時即可在低資源設定中有效訓練，使其非常適用於真實世界場景，並有助於在序列推薦領域民主化大型語言模型。我們的廣泛實驗證明 GenRec 可以概括到各種公開的真實世界資料集，並取得最先進的結果。我們的實驗也驗證了所提出的遮蔽項目預測目標的有效性，它將模型效能大幅提升。

##### **AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning**
2407.21174v1 by Maisha Binte Rashid, Pablo Rivas

Multimodal machine learning models that combine visual and textual data are
increasingly being deployed in critical applications, raising significant
safety and security concerns due to their vulnerability to adversarial attacks.
This paper presents an effective strategy to enhance the robustness of
multimodal image captioning models against such attacks. By leveraging the Fast
Gradient Sign Method (FGSM) to generate adversarial examples and incorporating
adversarial training techniques, we demonstrate improved model robustness on
two benchmark datasets: Flickr8k and COCO. Our findings indicate that
selectively training only the text decoder of the multimodal architecture shows
performance comparable to full adversarial training while offering increased
computational efficiency. This targeted approach suggests a balance between
robustness and training costs, facilitating the ethical deployment of
multimodal AI systems across various domains.

摘要：多模态机器学习模型结合了视觉和文本数据，
越来越多地用于关键应用程序中，由于其容易受到对抗性攻击，因此引发了重大的
安全问题。本文提出了一种有效的策略来增强
多模态图像字幕模型对这种攻击的鲁棒性。通过利用快速
梯度符号方法 (FGSM) 生成对抗性示例并结合
对抗性训练技术，我们在两个基准数据集：Flickr8k 和 COCO 上展示了改进的模型鲁棒性。我们的研究结果表明
选择性地仅训练多模态架构的文本解码器显示
性能与完全对抗性训练相当，同时提供更高的
计算效率。这种有针对性的方法表明了
鲁棒性和训练成本之间的平衡，促进了
多模态人工智能系统在各个领域的道德部署。

##### **Decomposed Prompting to Answer Questions on a Course Discussion Board**
2407.21170v1 by Brandon Jaipersaud, Paul Zhang, Jimmy Ba, Andrew Petersen, Lisa Zhang, Michael R. Zhang

We propose and evaluate a question-answering system that uses decomposed
prompting to classify and answer student questions on a course discussion
board. Our system uses a large language model (LLM) to classify questions into
one of four types: conceptual, homework, logistics, and not answerable. This
enables us to employ a different strategy for answering questions that fall
under different types. Using a variant of GPT-3, we achieve $81\%$
classification accuracy. We discuss our system's performance on answering
conceptual questions from a machine learning course and various failure modes.

摘要：我們提出並評估一個問題回答系統，該系統使用分解提示來分類和回答學生在課程討論區上的問題。我們的系統使用大型語言模型 (LLM) 將問題分類為四種類型之一：概念、作業、後勤和無法回答。這使我們能夠針對不同類型的問題採用不同的回答策略。使用 GPT-3 的變體，我們達到了 81% 的分類準確度。我們討論了我們的系統在回答機器學習課程中的概念性問題時的表現和各種失敗模式。

##### **Understanding Public Safety Trends in Calgary through data mining**
2407.21163v1 by Zack Dewis, Apratim Sen, Jeffrey Wong, Yujia Zhang

This paper utilizes statistical data from various open datasets in Calgary to
to uncover patterns and insights for community crimes, disorders, and traffic
incidents. Community attributes like demographics, housing, and pet
registration were collected and analyzed through geospatial visualization and
correlation analysis. Strongly correlated features were identified using the
chi-square test, and predictive models were built using association rule mining
and machine learning algorithms. The findings suggest that crime rates are
closely linked to factors such as population density, while pet registration
has a smaller impact. This study offers valuable insights for city managers to
enhance community safety strategies.

摘要：本論文利用卡加利各種開放資料集的統計數據，揭示社區犯罪、失序和交通事故的模式和見解。通過地理空間可視化和相關性分析收集和分析了社區屬性，例如人口統計、住房和寵物註冊。使用卡方檢驗識別出相關性很強的特徵，並使用關聯規則挖掘和機器學習算法構建預測模型。研究結果表明，犯罪率與人口密度等因素密切相關，而寵物註冊的影響較小。本研究為城市管理者提供了有價值的見解，以增強社區安全策略。

##### **Event-Arguments Extraction Corpus and Modeling using BERT for Arabic**
2407.21153v1 by Alaa Aljabari, Lina Duaibes, Mustafa Jarrar, Mohammed Khalilia

Event-argument extraction is a challenging task, particularly in Arabic due
to sparse linguistic resources. To fill this gap, we introduce the \hadath
corpus ($550$k tokens) as an extension of Wojood, enriched with event-argument
annotations. We used three types of event arguments: $agent$, $location$, and
$date$, which we annotated as relation types. Our inter-annotator agreement
evaluation resulted in $82.23\%$ $Kappa$ score and $87.2\%$ $F_1$-score.
Additionally, we propose a novel method for event relation extraction using
BERT, in which we treat the task as text entailment. This method achieves an
$F_1$-score of $94.01\%$. To further evaluate the generalization of our
proposed method, we collected and annotated another out-of-domain corpus (about
$80$k tokens) called \testNLI and used it as a second test set, on which our
approach achieved promising results ($83.59\%$ $F_1$-score). Last but not
least, we propose an end-to-end system for event-arguments extraction. This
system is implemented as part of SinaTools, and both corpora are publicly
available at {\small \url{https://sina.birzeit.edu/wojood}}

摘要：事件論元萃取是一項具有挑戰性的任務，特別是對於阿拉伯語，因為語言資源稀少。為了填補這個空白，我們引入了\hadath語料庫（55 萬個詞彙），作為Wojood的延伸，並加入了事件論元註解。我們使用了三種類型的事件論元：$agent$、$location$和$date$，我們將它們註解為關係類型。我們的標註者間一致性評估得出的Kappa評分為82.23%，F1評分為87.2%。此外，我們提出了一種使用BERT進行事件關係萃取的新方法，在該方法中，我們將任務視為文本蘊涵。此方法達到了94.01%的F1評分。為了進一步評估我們提出的方法的泛化性，我們收集並註解了另一個領域外的語料庫（約8萬個詞彙），稱為\testNLI，並將其用作第二個測試集，我們的做法在該測試集上取得了有希望的結果（F1評分為83.59%）。最後但並非最不重要的一點是，我們提出了一個事件論元萃取的端到端系統。此系統作為SinaTools的一部分實作，並且兩個語料庫都可以在{\small \url{https://sina.birzeit.edu/wojood}}公開取得。

##### **Private Collaborative Edge Inference via Over-the-Air Computation**
2407.21151v1 by Selim F. Yilmaz, Burak Hasircioglu, Li Qiao, Deniz Gunduz

We consider collaborative inference at the wireless edge, where each client's
model is trained independently on their local datasets. Clients are queried in
parallel to make an accurate decision collaboratively. In addition to
maximizing the inference accuracy, we also want to ensure the privacy of local
models. To this end, we leverage the superposition property of the multiple
access channel to implement bandwidth-efficient multi-user inference methods.
Specifically, we propose different methods for ensemble and multi-view
classification that exploit over-the-air computation. We show that these
schemes perform better than their orthogonal counterparts with statistically
significant differences while using fewer resources and providing privacy
guarantees. We also provide experimental results verifying the benefits of the
proposed over-the-air multi-user inference approach and perform an ablation
study to demonstrate the effectiveness of our design choices. We share the
source code of the framework publicly on Github to facilitate further research
and reproducibility.

摘要：我們在無線邊緣考慮協作推論，其中每個客戶端的模型都是獨立在他們的本地資料集上訓練的。客戶端會被平行查詢以協作做出準確的決策。除了最大化推論準確性之外，我們也想要確保本地模型的隱私性。為此，我們利用多重存取通道的疊加屬性來實作頻寬效率高的多使用者推論方法。具體來說，我們提出不同的方法用於整體和多視角分類，利用空中運算。我們展示這些方案比它們的正交對應方案表現得更好，具有統計上的顯著差異，同時使用較少的資源並提供隱私保證。我們還提供實驗結果，驗證所提出的空中多使用者推論方法的優點，並進行消融研究以證明我們的設計選擇的有效性。我們在 Github 上公開分享框架的原始碼，以利於進一步的研究和重現性。

##### **Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population**
2407.21149v1 by Mayanka Chandrashekar, Ian Goethert, Md Inzamam Ul Haque, Benjamin McMahon, Sayera Dhaubhadel, Kathryn Knight, Joseph Erdos, Donna Reagan, Caroline Taylor, Peter Kuzmak, John Michael Gaziano, Eileen McAllister, Lauren Costa, Yuk-Lam Ho, Kelly Cho, Suzanne Tamang, Samah Fodeh-Jarad, Olga S. Ovchinnikova, Amy C. Justice, Jacob Hinkle, Ioana Danciu

Objectives: This study aims to assess the impact of domain shift on chest
X-ray classification accuracy and to analyze the influence of ground truth
label quality and demographic factors such as age group, sex, and study year.
Materials and Methods: We used a DenseNet121 model pretrained MIMIC-CXR dataset
for deep learning-based multilabel classification using ground truth labels
from radiology reports extracted using the CheXpert and CheXbert Labeler. We
compared the performance of the 14 chest X-ray labels on the MIMIC-CXR and
Veterans Healthcare Administration chest X-ray dataset (VA-CXR). The VA-CXR
dataset comprises over 259k chest X-ray images spanning between the years 2010
and 2022. Results: The validation of ground truth and the assessment of
multi-label classification performance across various NLP extraction tools
revealed that the VA-CXR dataset exhibited lower disagreement rates than the
MIMIC-CXR datasets. Additionally, there were notable differences in AUC scores
between models utilizing CheXpert and CheXbert. When evaluating multi-label
classification performance across different datasets, minimal domain shift was
observed in unseen datasets, except for the label "Enlarged Cardiomediastinum."
The study year's subgroup analyses exhibited the most significant variations in
multi-label classification model performance. These findings underscore the
importance of considering domain shifts in chest X-ray classification tasks,
particularly concerning study years. Conclusion: Our study reveals the
significant impact of domain shift and demographic factors on chest X-ray
classification, emphasizing the need for improved transfer learning and
equitable model development. Addressing these challenges is crucial for
advancing medical imaging and enhancing patient care.

摘要：<paragraph>目標：本研究旨在評估領域轉移對胸部 X 光分類精度的影響，並分析基本事實標籤品質和年齡組、性別和研究年份等人口因素的影響。
材料和方法：我們使用 DenseNet121 模型預訓練 MIMIC-CXR 資料集，使用從使用 CheXpert 和 CheXbert 標籤器從放射科報告中提取的基本事實標籤進行基於深度學習的多標籤分類。我們比較了 MIMIC-CXR 和退伍軍人健康管理局胸部 X 光資料集 (VA-CXR) 上 14 個胸部 X 光標籤的性能。VA-CXR 資料集包含超過 259k 張胸部 X 光影像，時間跨度為 2010 年至 2022 年。結果：基本事實的驗證和對各種 NLP 提取工具的多標籤分類性能的評估顯示，VA-CXR 資料集表現出的分歧率低於 MIMIC-CXR 資料集。此外，使用 CheXpert 和 CheXbert 的模型之間的 AUC 得分存在顯著差異。在評估不同資料集上的多標籤分類性能時，除了標籤「心縱隔增大」之外，在未見資料集中觀察到的領域轉移很小。研究年份的子群分析顯示，多標籤分類模型性能變化最大。這些發現強調了在胸部 X 光分類任務中考慮領域轉移的重要性，特別是關於研究年份。結論：我們的研究揭示了領域轉移和人口因素對胸部 X 光分類的顯著影響，強調了改進遷移學習和公平模型開發的必要性。應對這些挑戰對於推進醫學影像和加強患者護理至關重要。</paragraph>

