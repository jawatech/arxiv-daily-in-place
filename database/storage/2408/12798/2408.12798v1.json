{"2408.12798": {"publish_time": "2024-08-23", "title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models", "paper_summary": "Generative Large Language Models (LLMs) have made significant strides across\nvarious tasks, but they remain vulnerable to backdoor attacks, where specific\ntriggers in the prompt cause the LLM to generate adversary-desired responses.\nWhile most backdoor research has focused on vision or text classification\ntasks, backdoor attacks in text generation have been largely overlooked. In\nthis work, we introduce \\textit{BackdoorLLM}, the first comprehensive benchmark\nfor studying backdoor attacks on LLMs. \\textit{BackdoorLLM} features: 1) a\nrepository of backdoor benchmarks with a standardized training pipeline, 2)\ndiverse attack strategies, including data poisoning, weight poisoning, hidden\nstate attacks, and chain-of-thought attacks, 3) extensive evaluations with over\n200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and\n4) key insights into the effectiveness and limitations of backdoors in LLMs. We\nhope \\textit{BackdoorLLM} will raise awareness of backdoor threats and\ncontribute to advancing AI safety. The code is available at\n\\url{https://github.com/bboylyg/BackdoorLLM}.", "paper_summary_zh": "\u751f\u6210\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u9805\u4efb\u52d9\u4e2d\u5df2\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4f46\u5b83\u5011\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5f8c\u9580\u653b\u64ca\uff0c\u800c\u5f8c\u9580\u653b\u64ca\u4e2d\u63d0\u793a\u4e2d\u7684\u7279\u5b9a\u89f8\u767c\u56e0\u7d20\u6703\u5c0e\u81f4 LLM \u7522\u751f\u5c0d\u624b\u60f3\u8981\u7684\u56de\u61c9\u3002\u96d6\u7136\u5927\u591a\u6578\u5f8c\u9580\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u8996\u89ba\u6216\u6587\u672c\u5206\u985e\u4efb\u52d9\u4e0a\uff0c\u4f46\u6587\u672c\u751f\u6210\u4e2d\u7684\u5f8c\u9580\u653b\u64ca\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u88ab\u5ffd\u8996\u4e86\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 \\textit{BackdoorLLM}\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u91dd\u5c0d LLM \u4e0a\u5f8c\u9580\u653b\u64ca\u9032\u884c\u7814\u7a76\u7684\u7d9c\u5408\u57fa\u6e96\u6e2c\u8a66\u3002\\textit{BackdoorLLM} \u7684\u7279\u9ede\uff1a1) \u5177\u6709\u6a19\u6e96\u5316\u8a13\u7df4\u7ba1\u7dda\u7684\u5f8c\u9580\u57fa\u6e96\u6e2c\u8a66\u5132\u5b58\u5eab\uff0c2) \u591a\u6a23\u5316\u7684\u653b\u64ca\u7b56\u7565\uff0c\u5305\u62ec\u8cc7\u6599\u4e2d\u6bd2\u3001\u6b0a\u91cd\u4e2d\u6bd2\u3001\u96b1\u85cf\u72c0\u614b\u653b\u64ca\u548c\u601d\u7dad\u93c8\u653b\u64ca\uff0c3) \u5728 7 \u500b\u5834\u666f\u548c 6 \u500b\u6a21\u578b\u67b6\u69cb\u4e2d\u5c0d 8 \u6b21\u653b\u64ca\u9032\u884c\u4e86 200 \u591a\u6b21\u5be6\u9a57\u7684\u5ee3\u6cdb\u8a55\u4f30\uff0c\u4ee5\u53ca 4) \u5c0d LLM \u4e2d\u5f8c\u9580\u7684\u6709\u6548\u6027\u548c\u9650\u5236\u7684\u4e3b\u8981\u898b\u89e3\u3002\u6211\u5011\u5e0c\u671b \\textit{BackdoorLLM} \u80fd\u63d0\u9ad8\u4eba\u5011\u5c0d\u5f8c\u9580\u5a01\u8105\u7684\u8a8d\u8b58\uff0c\u4e26\u6709\u52a9\u65bc\u63a8\u9032 AI \u5b89\u5168\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 \\url{https://github.com/bboylyg/BackdoorLLM} \u53d6\u5f97\u3002", "author": "Yige Li et.al.", "authors": "Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, Jun Sun", "id": "2408.12798v1", "paper_url": "http://arxiv.org/abs/2408.12798v1", "repo": "null"}}