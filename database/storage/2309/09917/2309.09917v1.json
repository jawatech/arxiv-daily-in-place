{"2309.09917": {"publish_time": "2023-09-18", "title": "Evaluation of Human-Understandability of Global Model Explanations using Decision Tree", "paper_summary": "In explainable artificial intelligence (XAI) research, the predominant focus\nhas been on interpreting models for experts and practitioners. Model agnostic\nand local explanation approaches are deemed interpretable and sufficient in\nmany applications. However, in domains like healthcare, where end users are\npatients without AI or domain expertise, there is an urgent need for model\nexplanations that are more comprehensible and instil trust in the model's\noperations. We hypothesise that generating model explanations that are\nnarrative, patient-specific and global(holistic of the model) would enable\nbetter understandability and enable decision-making. We test this using a\ndecision tree model to generate both local and global explanations for patients\nidentified as having a high risk of coronary heart disease. These explanations\nare presented to non-expert users. We find a strong individual preference for a\nspecific type of explanation. The majority of participants prefer global\nexplanations, while a smaller group prefers local explanations. A task based\nevaluation of mental models of these participants provide valuable feedback to\nenhance narrative global explanations. This, in turn, guides the design of\nhealth informatics systems that are both trustworthy and actionable.", "paper_summary_zh": "", "author": "Adarsa Sivaprasad et.al.", "authors": "Adarsa Sivaprasad,Ehud Reiter,Nava Tintarev,Nir Oren", "id": "2309.09917v1", "paper_url": "http://arxiv.org/abs/2309.09917v1", "repo": "null"}}