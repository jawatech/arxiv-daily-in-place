{"2405.14622": {"publish_time": "2024-05-23", "title": "Calibrated Self-Rewarding Vision Language Models", "paper_summary": "Large Vision-Language Models (LVLMs) have made substantial progress by\nintegrating pre-trained large language models (LLMs) and vision models through\ninstruction tuning. Despite these advancements, LVLMs often exhibit the\nhallucination phenomenon, where generated text responses appear linguistically\nplausible but contradict the input image, indicating a misalignment between\nimage and text pairs. This misalignment arises because the model tends to\nprioritize textual information over visual input, even when both the language\nmodel and visual representations are of high quality. Existing methods leverage\nadditional models or human annotations to curate preference data and enhance\nmodality alignment through preference optimization. These approaches may not\neffectively reflect the target LVLM's preferences, making the curated\npreferences easily distinguishable. Our work addresses these challenges by\nproposing the Calibrated Self-Rewarding (CSR) approach, which enables the model\nto self-improve by iteratively generating candidate responses, evaluating the\nreward for each response, and curating preference data for fine-tuning. In the\nreward modeling, we employ a step-wise strategy and incorporate visual\nconstraints into the self-rewarding process to place greater emphasis on visual\ninput. Empirical results demonstrate that CSR enhances performance and reduces\nhallucinations across ten benchmarks and tasks, achieving substantial\nimprovements over existing methods by 7.62%. Our empirical results are further\nsupported by rigorous theoretical analysis, under mild assumptions, verifying\nthe effectiveness of introducing visual constraints into the self-rewarding\nparadigm. Additionally, CSR shows compatibility with different vision-language\nmodels and the ability to incrementally improve performance through iterative\nfine-tuning. Our data and code are available at\nhttps://github.com/YiyangZhou/CSR.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u900f\u904e\u6307\u4ee4\u8abf\u6574\u6574\u5408\u9810\u5148\u8a13\u7df4\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLMs) \u548c\u8996\u89ba\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u986f\u8457\u9032\u5c55\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u9032\u5c55\uff0cLVLMs \u7d93\u5e38\u5c55\u73fe\u51fa\u5e7b\u89ba\u73fe\u8c61\uff0c\u5176\u4e2d\u7522\u751f\u7684\u6587\u5b57\u56de\u61c9\u5728\u8a9e\u8a00\u4e0a\u770b\u4f3c\u5408\u7406\uff0c\u4f46\u8207\u8f38\u5165\u5f71\u50cf\u76f8\u77db\u76fe\uff0c\u9019\u8868\u793a\u5f71\u50cf\u548c\u6587\u5b57\u914d\u5c0d\u4e4b\u9593\u7684\u932f\u4f4d\u3002\u9019\u7a2e\u932f\u4f4d\u7522\u751f\uff0c\u662f\u56e0\u70ba\u6a21\u578b\u50be\u5411\u65bc\u512a\u5148\u8003\u616e\u6587\u5b57\u8cc7\u8a0a\u800c\u975e\u8996\u89ba\u8f38\u5165\uff0c\u5373\u4f7f\u8a9e\u8a00\u6a21\u578b\u548c\u8996\u89ba\u8868\u5fb5\u90fd\u662f\u9ad8\u54c1\u8cea\u7684\u3002\u73fe\u6709\u65b9\u6cd5\u5229\u7528\u984d\u5916\u7684\u6a21\u578b\u6216\u4eba\u5de5\u8a3b\u89e3\u4f86\u7b56\u5283\u504f\u597d\u8cc7\u6599\uff0c\u4e26\u900f\u904e\u504f\u597d\u6700\u4f73\u5316\u589e\u5f37\u6a21\u614b\u5c0d\u9f4a\u3002\u9019\u4e9b\u65b9\u6cd5\u53ef\u80fd\u7121\u6cd5\u6709\u6548\u53cd\u6620\u76ee\u6a19 LVLM \u7684\u504f\u597d\uff0c\u4f7f\u5f97\u7b56\u5283\u7684\u504f\u597d\u5bb9\u6613\u5340\u5206\u3002\u6211\u5011\u7684\u7814\u7a76\u900f\u904e\u63d0\u8b70\u6821\u6e96\u81ea\u6211\u734e\u52f5 (CSR) \u65b9\u6cd5\u4f86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff0c\u9019\u4f7f\u6a21\u578b\u80fd\u5920\u900f\u904e\u53cd\u8986\u7522\u751f\u5019\u9078\u56de\u61c9\u3001\u8a55\u4f30\u6bcf\u500b\u56de\u61c9\u7684\u734e\u52f5\uff0c\u4ee5\u53ca\u7b56\u5283\u7528\u65bc\u5fae\u8abf\u7684\u504f\u597d\u8cc7\u6599\u4f86\u81ea\u6211\u6539\u5584\u3002\u5728\u734e\u52f5\u5efa\u6a21\u4e2d\uff0c\u6211\u5011\u63a1\u7528\u9010\u6b65\u7b56\u7565\uff0c\u4e26\u5c07\u8996\u89ba\u7d04\u675f\u7d0d\u5165\u81ea\u6211\u734e\u52f5\u904e\u7a0b\u4e2d\uff0c\u4ee5\u66f4\u5f37\u8abf\u8996\u89ba\u8f38\u5165\u3002\u5be6\u8b49\u7d50\u679c\u986f\u793a\uff0cCSR \u63d0\u5347\u4e86\u6548\u80fd\uff0c\u4e26\u6e1b\u5c11\u4e86\u5341\u500b\u57fa\u6e96\u548c\u4efb\u52d9\u4e2d\u7684\u5e7b\u89ba\uff0c\u6bd4\u73fe\u6709\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\u4e86 7.62%\u3002\u6211\u5011\u7684\u5be6\u8b49\u7d50\u679c\u9032\u4e00\u6b65\u7372\u5f97\u56b4\u8b39\u7406\u8ad6\u5206\u6790\u7684\u652f\u6301\uff0c\u5728\u6eab\u548c\u7684\u5047\u8a2d\u4e0b\uff0c\u9a57\u8b49\u4e86\u5728\u81ea\u6211\u734e\u52f5\u7bc4\u4f8b\u4e2d\u5f15\u5165\u8996\u89ba\u7d04\u675f\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0cCSR \u986f\u793a\u51fa\u8207\u4e0d\u540c\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u76f8\u5bb9\u6027\uff0c\u4ee5\u53ca\u900f\u904e\u53cd\u8986\u5fae\u8abf\u9010\u6b65\u63d0\u5347\u6548\u80fd\u7684\u80fd\u529b\u3002\u6211\u5011\u7684\u8cc7\u6599\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/YiyangZhou/CSR \u53d6\u5f97\u3002", "author": "Yiyang Zhou et.al.", "authors": "Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, Huaxiu Yao", "id": "2405.14622v1", "paper_url": "http://arxiv.org/abs/2405.14622v1", "repo": "null"}}