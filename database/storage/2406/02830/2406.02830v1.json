{"2406.02830": {"publish_time": "2024-06-05", "title": "Too Big to Fail: Larger Language Models are Disproportionately Resilient to Induction of Dementia-Related Linguistic Anomalies", "paper_summary": "As artificial neural networks grow in complexity, understanding their inner\nworkings becomes increasingly challenging, which is particularly important in\nhealthcare applications. The intrinsic evaluation metrics of autoregressive\nneural language models (NLMs), perplexity (PPL), can reflect how \"surprised\" an\nNLM model is at novel input. PPL has been widely used to understand the\nbehavior of NLMs. Previous findings show that changes in PPL when masking\nattention layers in pre-trained transformer-based NLMs reflect linguistic\nanomalies associated with Alzheimer's disease dementia. Building upon this, we\nexplore a novel bidirectional attention head ablation method that exhibits\nproperties attributed to the concepts of cognitive and brain reserve in human\nbrain studies, which postulate that people with more neurons in the brain and\nmore efficient processing are more resilient to neurodegeneration. Our results\nshow that larger GPT-2 models require a disproportionately larger share of\nattention heads to be masked/ablated to display degradation of similar\nmagnitude to masking in smaller models. These results suggest that the\nattention mechanism in transformer models may present an analogue to the\nnotions of cognitive and brain reserve and could potentially be used to model\ncertain aspects of the progression of neurodegenerative disorders and aging.", "paper_summary_zh": "\u96a8\u8457\u4eba\u5de5\u795e\u7d93\u7db2\u8def\u7684\u8907\u96dc\u6027\u63d0\u5347\uff0c\u7406\u89e3\u5176\u5167\u90e8\u904b\u4f5c\u4e5f\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u56f0\u96e3\uff0c\u9019\u5728\u91ab\u7642\u4fdd\u5065\u61c9\u7528\u4e0a\u7279\u5225\u91cd\u8981\u3002\u81ea\u8ff4\u6b78\u795e\u7d93\u8a9e\u8a00\u6a21\u578b (NLM) \u7684\u5167\u5728\u8a55\u4f30\u6307\u6a19\uff0c\u56f0\u60d1\u5ea6 (PPL)\uff0c\u53ef\u4ee5\u53cd\u6620 NLM \u6a21\u578b\u5c0d\u65b0\u8f38\u5165\u611f\u5230\u6709\u591a\u300c\u9a5a\u8a1d\u300d\u3002PPL \u5df2\u88ab\u5ee3\u6cdb\u7528\u65bc\u4e86\u89e3 NLM \u7684\u884c\u70ba\u3002\u5148\u524d\u7684\u7814\u7a76\u767c\u73fe\uff0c\u5728\u9810\u5148\u8a13\u7df4\u7684\u57fa\u65bc Transformer \u7684 NLM \u4e2d\u906e\u853d\u6ce8\u610f\u529b\u5c64\u6642\uff0cPPL \u7684\u8b8a\u5316\u53cd\u6620\u4e86\u8207\u963f\u8332\u6d77\u9ed8\u75c7\u75f4\u5446\u75c7\u76f8\u95dc\u7684\u8a9e\u8a00\u7570\u5e38\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u63a2\u7d22\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u96d9\u5411\u6ce8\u610f\u529b\u982d\u90e8\u6d88\u878d\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u8868\u73fe\u51fa\u6b78\u56e0\u65bc\u4eba\u985e\u5927\u8166\u7814\u7a76\u4e2d\u8a8d\u77e5\u548c\u8166\u529b\u5132\u5099\u6982\u5ff5\u7684\u7279\u6027\uff0c\u9019\u4e9b\u6982\u5ff5\u5047\u8a2d\u5927\u8166\u4e2d\u795e\u7d93\u5143\u8d8a\u591a\u3001\u8655\u7406\u6548\u7387\u8d8a\u9ad8\u7684\u4eba\u5c0d\u795e\u7d93\u9000\u5316\u66f4\u5177\u62b5\u6297\u529b\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u8f03\u5927\u7684 GPT-2 \u6a21\u578b\u9700\u8981\u4e0d\u6210\u6bd4\u4f8b\u5730\u66f4\u5927\u6bd4\u4f8b\u7684\u6ce8\u610f\u529b\u982d\u90e8\u88ab\u906e\u853d/\u6d88\u878d\uff0c\u624d\u80fd\u8868\u73fe\u51fa\u8207\u8f03\u5c0f\u6a21\u578b\u4e2d\u906e\u853d\u76f8\u4f3c\u7684\u7a0b\u5ea6\u7684\u9000\u5316\u3002\u9019\u4e9b\u7d50\u679c\u8868\u660e\uff0cTransformer \u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u53ef\u80fd\u5448\u73fe\u51fa\u8207\u8a8d\u77e5\u548c\u8166\u529b\u5132\u5099\u6982\u5ff5\u985e\u4f3c\u7684\u73fe\u8c61\uff0c\u4e26\u4e14\u6709\u53ef\u80fd\u7528\u65bc\u6a21\u64ec\u795e\u7d93\u9000\u5316\u6027\u75be\u75c5\u548c\u8001\u5316\u7684\u67d0\u4e9b\u65b9\u9762\u9032\u5c55\u3002", "author": "Changye Li et.al.", "authors": "Changye Li, Zhecheng Sheng, Trevor Cohen, Serguei Pakhomov", "id": "2406.02830v1", "paper_url": "http://arxiv.org/abs/2406.02830v1", "repo": "null"}}