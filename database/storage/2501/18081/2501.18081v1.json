{"2501.18081": {"publish_time": "2025-01-30", "title": "Normative Evaluation of Large Language Models with Everyday Moral Dilemmas", "paper_summary": "The rapid adoption of large language models (LLMs) has spurred extensive\nresearch into their encoded moral norms and decision-making processes. Much of\nthis research relies on prompting LLMs with survey-style questions to assess\nhow well models are aligned with certain demographic groups, moral beliefs, or\npolitical ideologies. While informative, the adherence of these approaches to\nrelatively superficial constructs tends to oversimplify the complexity and\nnuance underlying everyday moral dilemmas. We argue that auditing LLMs along\nmore detailed axes of human interaction is of paramount importance to better\nassess the degree to which they may impact human beliefs and actions. To this\nend, we evaluate LLMs on complex, everyday moral dilemmas sourced from the \"Am\nI the Asshole\" (AITA) community on Reddit, where users seek moral judgments on\neveryday conflicts from other community members. We prompted seven LLMs to\nassign blame and provide explanations for over 10,000 AITA moral dilemmas. We\nthen compared the LLMs' judgments and explanations to those of Redditors and to\neach other, aiming to uncover patterns in their moral reasoning. Our results\ndemonstrate that large language models exhibit distinct patterns of moral\njudgment, varying substantially from human evaluations on the AITA subreddit.\nLLMs demonstrate moderate to high self-consistency but low inter-model\nagreement. Further analysis of model explanations reveals distinct patterns in\nhow models invoke various moral principles. These findings highlight the\ncomplexity of implementing consistent moral reasoning in artificial systems and\nthe need for careful evaluation of how different models approach ethical\njudgment. As LLMs continue to be used in roles requiring ethical\ndecision-making such as therapists and companions, careful evaluation is\ncrucial to mitigate potential biases and limitations.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u63a1\u7528\u5df2\u4fc3\u4f7f\u4eba\u5011\u6df1\u5165\u7814\u7a76\u5176\u7de8\u78bc\u7684\u9053\u5fb7\u898f\u7bc4\u548c\u6c7a\u7b56\u904e\u7a0b\u3002\u8a31\u591a\u9019\u985e\u7814\u7a76\u4f9d\u8cf4\u65bc\u4ee5\u8abf\u67e5\u5f0f\u554f\u984c\u63d0\u793a LLM\uff0c\u4ee5\u8a55\u4f30\u6a21\u578b\u8207\u7279\u5b9a\u4eba\u53e3\u7fa4\u9ad4\u3001\u9053\u5fb7\u4fe1\u5ff5\u6216\u653f\u6cbb\u610f\u8b58\u5f62\u614b\u7684\u5951\u5408\u7a0b\u5ea6\u3002\u5118\u7ba1\u6709\u63d0\u4f9b\u8cc7\u8a0a\uff0c\u4f46\u9019\u4e9b\u65b9\u6cd5\u5c0d\u76f8\u5c0d\u819a\u6dfa\u7684\u7d50\u69cb\u7684\u5805\u6301\u50be\u5411\u65bc\u904e\u5ea6\u7c21\u5316\u65e5\u5e38\u9053\u5fb7\u56f0\u5883\u80cc\u5f8c\u7684\u8907\u96dc\u6027\u548c\u7d30\u5fae\u5dee\u5225\u3002\u6211\u5011\u8a8d\u70ba\uff0c\u6cbf\u8457\u66f4\u8a73\u7d30\u7684\u4eba\u985e\u4e92\u52d5\u8ef8\u7dda\u5be9\u67e5 LLM \u5c0d\u65bc\u66f4\u597d\u5730\u8a55\u4f30\u5b83\u5011\u53ef\u80fd\u5f71\u97ff\u4eba\u985e\u4fe1\u5ff5\u548c\u884c\u70ba\u7684\u7a0b\u5ea6\u81f3\u95dc\u91cd\u8981\u3002\u70ba\u6b64\uff0c\u6211\u5011\u6839\u64da Reddit \u4e0a\u300c\u6211\u662f\u6df7\u86cb\u55ce\u300d(AITA) \u793e\u7fa4\u8a55\u4f30 LLM \u5728\u8907\u96dc\u7684\u65e5\u5e38\u9053\u5fb7\u56f0\u5883\u4e2d\uff0c\u4f7f\u7528\u8005\u5728\u5176\u4e2d\u5c0b\u6c42\u5176\u4ed6\u793e\u7fa4\u6210\u54e1\u5c0d\u65e5\u5e38\u885d\u7a81\u7684\u9053\u5fb7\u5224\u65b7\u3002\u6211\u5011\u63d0\u793a\u4e03\u500b LLM \u5c0d\u8d85\u904e 10,000 \u500b AITA \u9053\u5fb7\u56f0\u5883\u5206\u914d\u8cac\u4efb\u4e26\u63d0\u4f9b\u89e3\u91cb\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07 LLM \u7684\u5224\u65b7\u548c\u89e3\u91cb\u8207 Reddit \u4f7f\u7528\u8005\u7684\u5224\u65b7\u548c\u89e3\u91cb\u4ee5\u53ca\u5f7c\u6b64\u9032\u884c\u6bd4\u8f03\uff0c\u65e8\u5728\u63ed\u793a\u5176\u9053\u5fb7\u63a8\u7406\u4e2d\u7684\u6a21\u5f0f\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5c55\u73fe\u51fa\u4e0d\u540c\u7684\u9053\u5fb7\u5224\u65b7\u6a21\u5f0f\uff0c\u8207 AITA \u5b50\u7248\u584a\u4e0a\u7684\u4eba\u985e\u8a55\u4f30\u6709\u5f88\u5927\u5dee\u7570\u3002LLM \u8868\u73fe\u51fa\u4e2d\u5ea6\u5230\u9ad8\u5ea6\u7684\u81ea\u6211\u4e00\u81f4\u6027\uff0c\u4f46\u6a21\u578b\u9593\u5354\u8b70\u4f4e\u3002\u9032\u4e00\u6b65\u5206\u6790\u6a21\u578b\u89e3\u91cb\u63ed\u793a\u4e86\u6a21\u578b\u5982\u4f55\u63f4\u5f15\u5404\u7a2e\u9053\u5fb7\u539f\u5247\u7684\u4e0d\u540c\u6a21\u5f0f\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u5728\u4eba\u5de5\u7cfb\u7d71\u4e2d\u5be6\u65bd\u4e00\u81f4\u7684\u9053\u5fb7\u63a8\u7406\u7684\u8907\u96dc\u6027\uff0c\u4ee5\u53ca\u4ed4\u7d30\u8a55\u4f30\u4e0d\u540c\u6a21\u578b\u5982\u4f55\u9032\u884c\u9053\u5fb7\u5224\u65b7\u7684\u5fc5\u8981\u6027\u3002\u96a8\u8457 LLM \u6301\u7e8c\u7528\u65bc\u9700\u8981\u9053\u5fb7\u6c7a\u7b56\u7684\u89d2\u8272\uff0c\u4f8b\u5982\u6cbb\u7642\u5e2b\u548c\u4f34\u4fb6\uff0c\u4ed4\u7d30\u8a55\u4f30\u5c0d\u65bc\u6e1b\u8f15\u6f5b\u5728\u504f\u898b\u548c\u9650\u5236\u81f3\u95dc\u91cd\u8981\u3002", "author": "Pratik S. Sachdeva et.al.", "authors": "Pratik S. Sachdeva, Tom van Nuenen", "id": "2501.18081v1", "paper_url": "http://arxiv.org/abs/2501.18081v1", "repo": "null"}}