{"2502.13923": {"publish_time": "2025-02-19", "title": "Qwen2.5-VL Technical Report", "paper_summary": "We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a8\u51fa Qwen2.5-VL\uff0c\u9019\u662f Qwen \u8996\u89ba\u8a9e\u8a00\u7cfb\u5217\u7684\u6700\u65b0\u65d7\u8266\u6a5f\u578b\uff0c\u5b83\u5728\u57fa\u790e\u80fd\u529b\u548c\u5275\u65b0\u529f\u80fd\u65b9\u9762\u90fd\u5c55\u73fe\u51fa\u986f\u8457\u7684\u9032\u6b65\u3002Qwen2.5-VL \u5728\u7406\u89e3\u548c\u8207\u4e16\u754c\u4e92\u52d5\u65b9\u9762\u53d6\u5f97\u91cd\u5927\u9032\u5c55\uff0c\u900f\u904e\u589e\u5f37\u7684\u8996\u89ba\u8fa8\u8b58\u3001\u7cbe\u78ba\u7684\u7269\u4ef6\u5b9a\u4f4d\u3001\u7a69\u5065\u7684\u6587\u4ef6\u89e3\u6790\u548c\u9577\u5f71\u7247\u7406\u89e3\u4f86\u5be6\u73fe\u3002Qwen2.5-VL \u7684\u4e00\u500b\u986f\u8457\u7279\u9ede\u662f\u5b83\u80fd\u5920\u4f7f\u7528\u908a\u754c\u6846\u6216\u9ede\u6e96\u78ba\u5b9a\u4f4d\u7269\u4ef6\u3002\u5b83\u63d0\u4f9b\u5f9e\u767c\u7968\u3001\u8868\u683c\u548c\u8868\u683c\u4e2d\u63d0\u53d6\u7a69\u5065\u7684\u7d50\u69cb\u5316\u8cc7\u6599\uff0c\u4ee5\u53ca\u5c0d\u5716\u8868\u3001\u5716\u89e3\u548c\u7248\u9762\u7684\u8a73\u7d30\u5206\u6790\u3002\u70ba\u4e86\u8655\u7406\u8907\u96dc\u7684\u8f38\u5165\uff0cQwen2.5-VL \u5f15\u5165\u4e86\u52d5\u614b\u89e3\u6790\u8655\u7406\u548c\u7d55\u5c0d\u6642\u9593\u7de8\u78bc\uff0c\u4f7f\u5176\u80fd\u5920\u8655\u7406\u4e0d\u540c\u5927\u5c0f\u7684\u5f71\u50cf\u548c\u9577\u6642\u5f71\u7247\uff08\u9577\u9054\u6578\u5c0f\u6642\uff09\uff0c\u4e26\u5177\u6709\u79d2\u7d1a\u4e8b\u4ef6\u5b9a\u4f4d\u3002\u9019\u5141\u8a31\u6a21\u578b\u5728\u4e0d\u4f9d\u8cf4\u50b3\u7d71\u6b63\u898f\u5316\u6280\u8853\u7684\u60c5\u6cc1\u4e0b\uff0c\u539f\u751f\u611f\u77e5\u7a7a\u9593\u5c3a\u5ea6\u548c\u6642\u9593\u52d5\u614b\u3002\u900f\u904e\u5f9e\u982d\u8a13\u7df4\u539f\u751f\u52d5\u614b\u89e3\u6790\u8996\u89ba\u8f49\u63db\u5668 (ViT) \u4e26\u6574\u5408\u8996\u7a97\u6ce8\u610f\u529b\uff0c\u6211\u5011\u5728\u7dad\u6301\u539f\u751f\u89e3\u6790\u5ea6\u7684\u540c\u6642\u6e1b\u5c11\u4e86\u904b\u7b97\u958b\u92b7\u3002\u56e0\u6b64\uff0cQwen2.5-VL \u4e0d\u50c5\u5728\u975c\u614b\u5f71\u50cf\u548c\u6587\u4ef6\u7406\u89e3\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u9084\u80fd\u5920\u4f5c\u70ba\u4e00\u500b\u4e92\u52d5\u5f0f\u8996\u89ba\u4ee3\u7406\uff0c\u5728\u64cd\u4f5c\u96fb\u8166\u548c\u884c\u52d5\u88dd\u7f6e\u7b49\u771f\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u9032\u884c\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548c\u4efb\u52d9\u57f7\u884c\u3002Qwen2.5-VL \u6709\u4e09\u7a2e\u5c3a\u5bf8\uff0c\u53ef\u6eff\u8db3\u5f9e\u908a\u7de3 AI \u5230\u9ad8\u6027\u80fd\u904b\u7b97\u7684\u5404\u7a2e\u4f7f\u7528\u6848\u4f8b\u3002\u65d7\u8266 Qwen2.5-VL-72B \u6a5f\u578b\u8207 GPT-4o \u548c Claude 3.5 Sonnet \u7b49\u6700\u5148\u9032\u7684\u6a5f\u578b\u76f8\u5339\u914d\uff0c\u7279\u5225\u662f\u5728\u6587\u4ef6\u548c\u5716\u89e3\u7406\u89e3\u65b9\u9762\u8868\u73fe\u51fa\u8272\u3002\u6b64\u5916\uff0cQwen2.5-VL \u7dad\u6301\u7a69\u5065\u7684\u8a9e\u8a00\u6548\u80fd\uff0c\u4fdd\u7559\u4e86 Qwen2.5 LLM \u7684\u6838\u5fc3\u8a9e\u8a00\u80fd\u529b\u3002</paragraph>", "author": "Shuai Bai et.al.", "authors": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin", "id": "2502.13923v1", "paper_url": "http://arxiv.org/abs/2502.13923v1", "repo": "null"}}