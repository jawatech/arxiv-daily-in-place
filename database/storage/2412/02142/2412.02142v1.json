{"2412.02142": {"publish_time": "2024-12-03", "title": "Personalized Multimodal Large Language Models: A Survey", "paper_summary": "Multimodal Large Language Models (MLLMs) have become increasingly important\ndue to their state-of-the-art performance and ability to integrate multiple\ndata modalities, such as text, images, and audio, to perform complex tasks with\nhigh accuracy. This paper presents a comprehensive survey on personalized\nmultimodal large language models, focusing on their architecture, training\nmethods, and applications. We propose an intuitive taxonomy for categorizing\nthe techniques used to personalize MLLMs to individual users, and discuss the\ntechniques accordingly. Furthermore, we discuss how such techniques can be\ncombined or adapted when appropriate, highlighting their advantages and\nunderlying rationale. We also provide a succinct summary of personalization\ntasks investigated in existing research, along with the evaluation metrics\ncommonly used. Additionally, we summarize the datasets that are useful for\nbenchmarking personalized MLLMs. Finally, we outline critical open challenges.\nThis survey aims to serve as a valuable resource for researchers and\npractitioners seeking to understand and advance the development of personalized\nmultimodal large language models.", "paper_summary_zh": "\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7531\u65bc\u5176\u6700\u5148\u9032\u7684\u6548\u80fd\u548c\u6574\u5408\u591a\u7a2e\u8cc7\u6599\u6a21\u5f0f\uff08\u4f8b\u5982\u6587\u5b57\u3001\u5f71\u50cf\u548c\u97f3\u8a0a\uff09\u4ee5\u9ad8\u6e96\u78ba\u5ea6\u57f7\u884c\u8907\u96dc\u4efb\u52d9\u7684\u80fd\u529b\uff0c\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u91cd\u8981\u3002\u672c\u6587\u91dd\u5c0d\u500b\u4eba\u5316\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u63d0\u51fa\u5168\u9762\u7684\u8abf\u67e5\uff0c\u91cd\u9ede\u5728\u65bc\u5176\u67b6\u69cb\u3001\u8a13\u7df4\u65b9\u6cd5\u548c\u61c9\u7528\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u76f4\u89ba\u7684\u5206\u985e\u6cd5\uff0c\u7528\u65bc\u5206\u985e\u7528\u65bc\u5c07 MLLM \u500b\u4eba\u5316\u70ba\u500b\u5225\u4f7f\u7528\u8005\u7684\u6280\u8853\uff0c\u4e26\u64da\u6b64\u8a0e\u8ad6\u9019\u4e9b\u6280\u8853\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a0e\u8ad6\u5982\u4f55\u9069\u7576\u5730\u7d50\u5408\u6216\u8abf\u6574\u9019\u4e9b\u6280\u8853\uff0c\u5f37\u8abf\u5176\u512a\u9ede\u548c\u80cc\u5f8c\u7684\u539f\u7406\u3002\u6211\u5011\u4e5f\u7c21\u8981\u7e3d\u7d50\u73fe\u6709\u7814\u7a76\u4e2d\u8abf\u67e5\u7684\u500b\u4eba\u5316\u4efb\u52d9\uff0c\u4ee5\u53ca\u5e38\u7528\u7684\u8a55\u91cf\u6307\u6a19\u3002\u6b64\u5916\uff0c\u6211\u5011\u7e3d\u7d50\u4e86\u7528\u65bc\u5c0d\u500b\u4eba\u5316 MLLM \u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u7684\u6709\u7528\u8cc7\u6599\u96c6\u3002\u6700\u5f8c\uff0c\u6211\u5011\u6982\u8ff0\u91cd\u8981\u7684\u958b\u653e\u6027\u6311\u6230\u3002\u672c\u8abf\u67e5\u65e8\u5728\u70ba\u5c0b\u6c42\u4e86\u89e3\u548c\u63a8\u9032\u500b\u4eba\u5316\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u767c\u5c55\u7684\u7814\u7a76\u4eba\u54e1\u548c\u5f9e\u696d\u4eba\u54e1\u63d0\u4f9b\u6709\u50f9\u503c\u7684\u8cc7\u6e90\u3002", "author": "Junda Wu et.al.", "authors": "Junda Wu, Hanjia Lyu, Yu Xia, Zhehao Zhang, Joe Barrow, Ishita Kumar, Mehrnoosh Mirtaheri, Hongjie Chen, Ryan A. Rossi, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Namyong Park, Sungchul Kim, Huanrui Yang, Subrata Mitra, Zhengmian Hu, Nedim Lipka, Dang Nguyen, Yue Zhao, Jiebo Luo, Julian McAuley", "id": "2412.02142v1", "paper_url": "http://arxiv.org/abs/2412.02142v1", "repo": "null"}}