{"2501.17391": {"publish_time": "2025-01-29", "title": "Learning Free Token Reduction for Multi-Modal LLM", "paper_summary": "Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5df2\u5728\u5404\u7a2e\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\uff1b\u7136\u800c\uff0c\u5176\u5be6\u969b\u90e8\u7f72\u901a\u5e38\u53d7\u5230\u9ad8\u904b\u7b97\u6210\u672c\u548c\u5ef6\u9577\u7684\u63a8\u8ad6\u6642\u9593\u7684\u9650\u5236\u3002\u7531\u65bc\u8996\u89ba\u6a21\u614b\u901a\u5e38\u6bd4\u6587\u5b57\u6a21\u614b\u627f\u8f09\u66f4\u591a\u8cc7\u8a0a\uff0c\u56e0\u6b64\u58d3\u7e2e\u8996\u89ba\u63d0\u793a\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\u4f86\u7de9\u89e3\u9019\u4e9b\u6311\u6230\u3002\u73fe\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u65bc\u512a\u5316\u6a21\u578b\u67b6\u69cb\u6216\u76f4\u63a5\u6e1b\u5c11\u8996\u89ba\u4ee3\u78bc\u7684\u6578\u91cf\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u7531\u65bc\u6c92\u6709\u8003\u616e\u8996\u89ba\u8cc7\u6599\u7684\u7368\u7279\u7a7a\u9593\u548c\u6642\u9593\u7279\u5fb5\uff0c\u56e0\u6b64\u5e38\u5e38\u6703\u5f71\u97ff\u63a8\u8ad6\u6548\u80fd\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5728\u7a7a\u9593\u548c\u6642\u9593\u7dad\u5ea6\u4e0a\u904b\u4f5c\u7684\u4ee3\u78bc\u58d3\u7e2e\u7bc4\u4f8b\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u62ec\u4e00\u500b\u514d\u5b78\u7fd2\u3001\u5373\u63d2\u5373\u7528\u7684\u58d3\u7e2e\u7ba1\u7dda\uff0c\u53ef\u4ee5\u7121\u7e2b\u6574\u5408\u5230\u5927\u591a\u6578\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u6846\u67b6\u4e2d\u3002\u900f\u904e\u5229\u7528\u9019\u7a2e\u65b9\u6cd5\uff0c\u6211\u5011\u589e\u5f37\u4e86\u6a21\u578b\u63a8\u8ad6\u80fd\u529b\uff0c\u540c\u6642\u964d\u4f4e\u4e86\u5176\u904b\u7b97\u6210\u672c\u3002\u8996\u8a0a\u554f\u7b54\u4efb\u52d9\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u72a7\u7272\u6548\u80fd\u7684\u60c5\u6cc1\u4e0b\u986f\u8457\u63d0\u5347\u6548\u7387\u3002", "author": "Zihui Zhao et.al.", "authors": "Zihui Zhao, Yingxin Li, Yang Li", "id": "2501.17391v1", "paper_url": "http://arxiv.org/abs/2501.17391v1", "repo": "null"}}