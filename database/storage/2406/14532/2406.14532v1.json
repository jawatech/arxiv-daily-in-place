{"2406.14532": {"publish_time": "2024-06-20", "title": "RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold", "paper_summary": "Training on model-generated synthetic data is a promising approach for\nfinetuning LLMs, but it remains unclear when it helps or hurts. In this paper,\nwe investigate this question for math reasoning via an empirical study,\nfollowed by building a conceptual understanding of our observations. First, we\nfind that while the typical approach of finetuning a model on synthetic correct\nor positive problem-solution pairs generated by capable models offers modest\nperformance gains, sampling more correct solutions from the finetuned learner\nitself followed by subsequent fine-tuning on this self-generated data\n$\\textbf{doubles}$ the efficiency of the same synthetic problems. At the same\ntime, training on model-generated positives can amplify various spurious\ncorrelations, resulting in flat or even inverse scaling trends as the amount of\ndata increases. Surprisingly, we find that several of these issues can be\naddressed if we also utilize negative responses, i.e., model-generated\nresponses that are deemed incorrect by a final answer verifier. Crucially,\nthese negatives must be constructed such that the training can appropriately\nrecover the utility or advantage of each intermediate step in the negative\nresponse. With this per-step scheme, we are able to attain consistent gains\nover only positive data, attaining performance similar to amplifying the amount\nof synthetic data by $\\mathbf{8 \\times}$. We show that training on per-step\nnegatives can help to unlearn spurious correlations in the positive data, and\nis equivalent to advantage-weighted reinforcement learning (RL), implying that\nit inherits robustness benefits of RL over imitating positive data alone.", "paper_summary_zh": "<paragraph>\u5229\u7528\u6a21\u578b\u751f\u6210\u5408\u6210\u8cc7\u6599\u9032\u884c\u8a13\u7df4\u5c0d\u65bc\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4f86\u8aaa\u662f\u4e00\u7a2e\u5f88\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u5728\u4ec0\u9ebc\u6642\u5019\u6709\u52a9\u76ca\u6216\u6709\u5bb3\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5be6\u8b49\u7814\u7a76\u63a2\u8a0e\u9019\u500b\u554f\u984c\uff0c\u4e26\u9032\u4e00\u6b65\u5efa\u7acb\u5c0d\u6211\u5011\u89c0\u5bdf\u7d50\u679c\u7684\u6982\u5ff5\u6027\u7406\u89e3\u3002\u9996\u5148\uff0c\u6211\u5011\u767c\u73fe\uff0c\u5118\u7ba1\u5728\u6709\u80fd\u529b\u7684\u6a21\u578b\u6240\u7522\u751f\u7684\u5408\u6210\u6b63\u78ba\u6216\u6b63\u5411\u554f\u984c\u89e3\u6c7a\u914d\u5c0d\u4e0a\u5fae\u8abf\u6a21\u578b\u7684\u5178\u578b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u9069\u5ea6\u7684\u6548\u80fd\u63d0\u5347\uff0c\u4f46\u5f9e\u5fae\u8abf\u5f8c\u7684\u5b78\u7fd2\u8005\u672c\u8eab\u53d6\u6a23\u66f4\u591a\u6b63\u78ba\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u4e26\u5728\u6b64\u81ea\u751f\u8cc7\u6599\u4e0a\u9032\u884c\u5f8c\u7e8c\u5fae\u8abf\uff0c\u6703\u5c07\u76f8\u540c\u5408\u6210\u554f\u984c\u7684\u6548\u7387\u63d0\u5347\u4e86\u4e00\u500d\u3002\u540c\u6642\uff0c\u5728\u6a21\u578b\u7522\u751f\u7684\u6b63\u5411\u8cc7\u6599\u4e0a\u9032\u884c\u8a13\u7df4\u53ef\u80fd\u6703\u653e\u5927\u5404\u7a2e\u865b\u5047\u76f8\u95dc\u6027\uff0c\u5c0e\u81f4\u8cc7\u6599\u91cf\u589e\u52a0\u6642\u51fa\u73fe\u5e73\u5766\u751a\u81f3\u53cd\u5411\u7684\u64f4\u5145\u8da8\u52e2\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe\uff0c\u5982\u679c\u6211\u5011\u4e5f\u5229\u7528\u8ca0\u9762\u56de\u61c9\uff08\u5373\u6a21\u578b\u7522\u751f\u7684\u3001\u88ab\u6700\u7d42\u7b54\u6848\u9a57\u8b49\u5668\u8996\u70ba\u4e0d\u6b63\u78ba\u7684\u56de\u61c9\uff09\uff0c\u5c31\u80fd\u89e3\u6c7a\u5176\u4e2d\u5e7e\u500b\u554f\u984c\u3002\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u9019\u4e9b\u8ca0\u9762\u56de\u61c9\u5fc5\u9808\u5efa\u69cb\u5f97\u8b93\u8a13\u7df4\u80fd\u9069\u7576\u5730\u6062\u5fa9\u8ca0\u9762\u56de\u61c9\u4e2d\u6bcf\u500b\u4e2d\u9593\u6b65\u9a5f\u7684\u6548\u7528\u6216\u512a\u52e2\u3002\u900f\u904e\u9019\u500b\u9010\u6b65\u9a5f\u7684\u67b6\u69cb\uff0c\u6211\u5011\u80fd\u5920\u5728\u50c5\u6709\u6b63\u5411\u8cc7\u6599\u7684\u57fa\u790e\u4e0a\u6301\u7e8c\u7372\u5f97\u63d0\u5347\uff0c\u9054\u5230\u985e\u4f3c\u65bc\u5c07\u5408\u6210\u8cc7\u6599\u91cf\u64f4\u5927 8 \u500d\u7684\u6548\u80fd\u3002\u6211\u5011\u8b49\u660e\uff0c\u5728\u9010\u6b65\u9a5f\u7684\u8ca0\u9762\u8cc7\u6599\u4e0a\u9032\u884c\u8a13\u7df4\u6709\u52a9\u65bc\u53d6\u6d88\u6b63\u5411\u8cc7\u6599\u4e2d\u7684\u865b\u5047\u76f8\u95dc\u6027\uff0c\u4e26\u4e14\u7b49\u540c\u65bc\u512a\u52e2\u52a0\u6b0a\u5f37\u5316\u5b78\u7fd2 (RL)\uff0c\u9019\u8868\u793a\u5b83\u7e7c\u627f\u4e86 RL \u512a\u65bc\u50c5\u6a21\u4eff\u6b63\u5411\u8cc7\u6599\u7684\u7a69\u5065\u6027\u512a\u52e2\u3002</paragraph>", "author": "Amrith Setlur et.al.", "authors": "Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, Aviral Kumar", "id": "2406.14532v1", "paper_url": "http://arxiv.org/abs/2406.14532v1", "repo": "null"}}