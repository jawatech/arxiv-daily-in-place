{"2502.09503": {"publish_time": "2025-02-13", "title": "AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization", "paper_summary": "Transformer architectures have transformed AI applications but remain complex\nto customize for domain experts lacking low-level implementation expertise. We\nintroduce AttentionSmithy, a modular software package that simplifies\ntransformer innovation by breaking down key components into reusable building\nblocks: attention modules, feed-forward networks, normalization layers, and\npositional encodings. Users can rapidly prototype and evaluate transformer\nvariants without extensive coding. Our framework supports four positional\nencoding strategies and integrates with neural architecture search for\nautomated design. We validate AttentionSmithy by replicating the original\ntransformer under resource constraints and optimizing translation performance\nby combining positional encodings. Additionally, we demonstrate its\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\ntype classification. These case studies highlight AttentionSmithy's potential\nto accelerate research across diverse fields by removing framework\nimplementation barriers.", "paper_summary_zh": "Transformer \u67b6\u69cb\u5df2\u8f49\u8b8a AI \u61c9\u7528\uff0c\u4f46\u5c0d\u65bc\u7f3a\u4e4f\u4f4e\u968e\u5be6\u4f5c\u5c08\u696d\u77e5\u8b58\u7684\u9818\u57df\u5c08\u5bb6\u800c\u8a00\uff0c\u81ea\u8a02\u4ecd\u5f88\u8907\u96dc\u3002\u6211\u5011\u63a8\u51fa AttentionSmithy\uff0c\u9019\u662f\u4e00\u500b\u6a21\u7d44\u5316\u8edf\u9ad4\u5957\u4ef6\uff0c\u900f\u904e\u5c07\u95dc\u9375\u5143\u4ef6\u5206\u89e3\u6210\u53ef\u91cd\u8907\u4f7f\u7528\u7684\u5efa\u69cb\u5340\u584a\uff08\u6ce8\u610f\u529b\u6a21\u7d44\u3001\u524d\u994b\u7db2\u8def\u3001\u6b63\u898f\u5316\u5c64\u548c\u4f4d\u7f6e\u7de8\u78bc\uff09\u4f86\u7c21\u5316 Transformer \u5275\u65b0\u3002\u4f7f\u7528\u8005\u53ef\u4ee5\u5feb\u901f\u5efa\u7f6e\u539f\u578b\u548c\u8a55\u4f30 Transformer \u8b8a\u9ad4\uff0c\u800c\u7121\u9700\u5927\u91cf\u7de8\u78bc\u3002\u6211\u5011\u7684\u67b6\u69cb\u652f\u63f4\u56db\u7a2e\u4f4d\u7f6e\u7de8\u78bc\u7b56\u7565\uff0c\u4e26\u6574\u5408\u795e\u7d93\u67b6\u69cb\u641c\u5c0b\u4ee5\u9032\u884c\u81ea\u52d5\u5316\u8a2d\u8a08\u3002\u6211\u5011\u900f\u904e\u5728\u8cc7\u6e90\u9650\u5236\u4e0b\u8907\u88fd\u539f\u59cb Transformer \u548c\u7d50\u5408\u4f4d\u7f6e\u7de8\u78bc\u4f86\u6700\u4f73\u5316\u7ffb\u8b6f\u6548\u80fd\uff0c\u9a57\u8b49 AttentionSmithy\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u5176\u5728\u57fa\u56e0\u7279\u5b9a\u5efa\u6a21\u4e2d\u7684\u9069\u61c9\u6027\uff0c\u5728\u7d30\u80de\u985e\u578b\u5206\u985e\u4e2d\u9054\u5230\u8d85\u904e 95% \u7684\u6e96\u78ba\u5ea6\u3002\u9019\u4e9b\u6848\u4f8b\u7814\u7a76\u7a81\u986f AttentionSmithy \u5728\u79fb\u9664\u67b6\u69cb\u5be6\u4f5c\u969c\u7919\u5f8c\uff0c\u52a0\u901f\u5404\u500b\u9818\u57df\u7814\u7a76\u7684\u6f5b\u529b\u3002", "author": "Caleb Cranney et.al.", "authors": "Caleb Cranney, Jesse G. Meyer", "id": "2502.09503v1", "paper_url": "http://arxiv.org/abs/2502.09503v1", "repo": "null"}}