{"2504.12422": {"publish_time": "2025-04-16", "title": "Mitigating LLM Hallucinations with Knowledge Graphs: A Case Study", "paper_summary": "High-stakes domains like cyber operations need responsible and trustworthy AI\nmethods. While large language models (LLMs) are becoming increasingly popular\nin these domains, they still suffer from hallucinations. This research paper\nprovides learning outcomes from a case study with LinkQ, an open-source natural\nlanguage interface that was developed to combat hallucinations by forcing an\nLLM to query a knowledge graph (KG) for ground-truth data during\nquestion-answering (QA). We conduct a quantitative evaluation of LinkQ using a\nwell-known KGQA dataset, showing that the system outperforms GPT-4 but still\nstruggles with certain question categories - suggesting that alternative query\nconstruction strategies will need to be investigated in future LLM querying\nsystems. We discuss a qualitative study of LinkQ with two domain experts using\na real-world cybersecurity KG, outlining these experts' feedback, suggestions,\nperceived limitations, and future opportunities for systems like LinkQ.", "paper_summary_zh": "<paragraph>\u9ad8\u98a8\u96aa\u9818\u57df\uff0c\u4f8b\u5982\u7db2\u8def\u4f5c\u6230\uff0c\u9700\u8981\u8ca0\u8cac\u4efb\u4e14\u503c\u5f97\u4fe1\u8cf4\u7684 AI \u65b9\u6cd5\u3002\u96d6\u7136\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9019\u4e9b\u9818\u57df\u8d8a\u4f86\u8d8a\u6d41\u884c\uff0c\u4f46\u5b83\u5011\u4ecd\u7136\u6703\u7522\u751f\u5e7b\u89ba\uff08\u932f\u8aa4\u8cc7\u8a0a\uff09\u3002\u672c\u7814\u7a76\u8ad6\u6587\u63d0\u4f9b\u4e86\u4e00\u500b\u95dc\u65bc LinkQ \u7684\u6848\u4f8b\u7814\u7a76\u4e4b\u5b78\u7fd2\u6210\u679c\uff0cLinkQ \u662f\u4e00\u500b\u958b\u6e90\u81ea\u7136\u8a9e\u8a00\u4ecb\u9762\uff0c\u65e8\u5728\u900f\u904e\u5f37\u5236 LLM \u5728\u554f\u7b54 (QA) \u904e\u7a0b\u4e2d\u67e5\u8a62\u77e5\u8b58\u5716\u8b5c (KG) \u4ee5\u7372\u53d6\u771f\u5be6\u6578\u64da\u4f86\u5c0d\u6297\u5e7b\u89ba\u3002\u6211\u5011\u4f7f\u7528\u4e00\u500b\u77e5\u540d\u7684 KGQA \u8cc7\u6599\u96c6\u5c0d LinkQ \u9032\u884c\u4e86\u5b9a\u91cf\u8a55\u4f30\uff0c\u7d50\u679c\u986f\u793a\u8a72\u7cfb\u7d71\u7684\u6027\u80fd\u512a\u65bc GPT-4\uff0c\u4f46\u5728\u67d0\u4e9b\u554f\u984c\u985e\u5225\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96e3\uff0c\u9019\u8868\u660e\u5728\u672a\u4f86\u7684 LLM \u67e5\u8a62\u7cfb\u7d71\u4e2d\u9700\u8981\u7814\u7a76\u66ff\u4ee3\u67e5\u8a62\u5efa\u69cb\u7b56\u7565\u3002\u6211\u5011\u8207\u5169\u4f4d\u9818\u57df\u5c08\u5bb6\u4f7f\u7528\u771f\u5be6\u4e16\u754c\u7684\u7db2\u8def\u5b89\u5168 KG \u5c0d LinkQ \u9032\u884c\u4e86\u5b9a\u6027\u7814\u7a76\uff0c\u6982\u8ff0\u4e86\u9019\u4e9b\u5c08\u5bb6\u7684\u56de\u994b\u3001\u5efa\u8b70\u3001\u611f\u77e5\u5230\u7684\u9650\u5236\u4ee5\u53ca LinkQ \u7b49\u7cfb\u7d71\u7684\u672a\u4f86\u767c\u5c55\u6a5f\u6703\u3002</paragraph>\n", "author": "Harry Li et.al.", "authors": "Harry Li, Gabriel Appleby, Kenneth Alperin, Steven R Gomez, Ashley Suh", "id": "2504.12422v1", "paper_url": "http://arxiv.org/abs/2504.12422v1", "repo": "null"}}