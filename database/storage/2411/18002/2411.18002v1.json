{"2411.18002": {"publish_time": "2024-11-27", "title": "An End-to-End Two-Stream Network Based on RGB Flow and Representation Flow for Human Action Recognition", "paper_summary": "With the rapid advancements in deep learning, computer vision tasks have seen\nsignificant improvements, making two-stream neural networks a popular focus for\nvideo based action recognition. Traditional models using RGB and optical flow\nstreams achieve strong performance but at a high computational cost. To address\nthis, we introduce a representation flow algorithm to replace the optical flow\nbranch in the egocentric action recognition model, enabling end-to-end training\nwhile reducing computational cost and prediction time. Our model, designed for\negocentric action recognition, uses class activation maps (CAMs) to improve\naccuracy and ConvLSTM for spatio temporal encoding with spatial attention. When\nevaluated on the GTEA61, EGTEA GAZE+, and HMDB datasets, our model matches the\naccuracy of the original model on GTEA61 and exceeds it by 0.65% and 0.84% on\nEGTEA GAZE+ and HMDB, respectively. Prediction runtimes are significantly\nreduced to 0.1881s, 0.1503s, and 0.1459s, compared to the original model's\n101.6795s, 25.3799s, and 203.9958s. Ablation studies were also conducted to\nstudy the impact of different parameters on model performance.\n  Keywords: two-stream, egocentric, action recognition, CAM, representation\nflow, CAM, ConvLSTM", "paper_summary_zh": "<paragraph>\u96a8\u8457\u6df1\u5ea6\u5b78\u7fd2\u7684\u5feb\u901f\u9032\u5c55\uff0c\u96fb\u8166\u8996\u89ba\u4efb\u52d9\u5df2\u898b\u986f\u8457\u9032\u6b65\uff0c\u4f7f\u96d9\u6d41\u795e\u7d93\u7db2\u8def\u6210\u70ba\u57fa\u65bc\u5f71\u7247\u52d5\u4f5c\u8fa8\u8b58\u7684\u71b1\u9580\u7126\u9ede\u3002\u4f7f\u7528 RGB \u548c\u5149\u6d41\u4e32\u6d41\u7684\u50b3\u7d71\u6a21\u578b\u53ef\u9054\u6210\u5f37\u52c1\u7684\u6548\u80fd\uff0c\u4f46\u8a08\u7b97\u6210\u672c\u9ad8\u6602\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u8868\u793a\u6d41\u6f14\u7b97\u6cd5\u4f86\u53d6\u4ee3\u81ea\u6211\u4e2d\u5fc3\u52d5\u4f5c\u8fa8\u8b58\u6a21\u578b\u4e2d\u7684\u5149\u6d41\u5206\u652f\uff0c\u540c\u6642\u964d\u4f4e\u8a08\u7b97\u6210\u672c\u548c\u9810\u6e2c\u6642\u9593\uff0c\u5be6\u73fe\u7aef\u5230\u7aef\u8a13\u7df4\u3002\u6211\u5011\u7684\u6a21\u578b\u5c08\u70ba\u81ea\u6211\u4e2d\u5fc3\u52d5\u4f5c\u8fa8\u8b58\u800c\u8a2d\u8a08\uff0c\u4f7f\u7528\u985e\u5225\u6fc0\u6d3b\u5716 (CAM) \u4f86\u63d0\u9ad8\u6e96\u78ba\u5ea6\uff0c\u4e26\u4f7f\u7528 ConvLSTM \u9032\u884c\u6642\u7a7a\u7de8\u78bc\uff0c\u4e26\u5177\u5099\u7a7a\u9593\u6ce8\u610f\u529b\u3002\u5728 GTEA61\u3001EGTEA GAZE+ \u548c HMDB \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a55\u4f30\u6642\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728 GTEA61 \u4e0a\u8207\u539f\u59cb\u6a21\u578b\u7684\u6e96\u78ba\u5ea6\u76f8\u7b26\uff0c\u800c\u5728 EGTEA GAZE+ \u548c HMDB \u4e0a\u5206\u5225\u8d85\u904e\u4e86 0.65% \u548c 0.84%\u3002\u8207\u539f\u59cb\u6a21\u578b\u7684 101.6795 \u79d2\u300125.3799 \u79d2\u548c 203.9958 \u79d2\u76f8\u6bd4\uff0c\u9810\u6e2c\u57f7\u884c\u6642\u9593\u986f\u8457\u964d\u4f4e\u81f3 0.1881 \u79d2\u30010.1503 \u79d2\u548c 0.1459 \u79d2\u3002\u9084\u9032\u884c\u4e86\u6d88\u878d\u7814\u7a76\uff0c\u4ee5\u7814\u7a76\u4e0d\u540c\u53c3\u6578\u5c0d\u6a21\u578b\u6548\u80fd\u7684\u5f71\u97ff\u3002\n\u95dc\u9375\u5b57\uff1a\u96d9\u6d41\u3001\u81ea\u6211\u4e2d\u5fc3\u3001\u52d5\u4f5c\u8fa8\u8b58\u3001CAM\u3001\u8868\u793a\u6d41\u3001CAM\u3001ConvLSTM</paragraph>", "author": "Song-Jiang Lai et.al.", "authors": "Song-Jiang Lai, Tsun-Hin Cheung, Ka-Chun Fung, Tian-Shan Liu, Kin-Man Lam", "id": "2411.18002v1", "paper_url": "http://arxiv.org/abs/2411.18002v1", "repo": "null"}}