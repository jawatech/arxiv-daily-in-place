{"2408.17198": {"publish_time": "2024-08-30", "title": "Towards Symbolic XAI -- Explanation Through Human Understandable Logical Relationships Between Features", "paper_summary": "Explainable Artificial Intelligence (XAI) plays a crucial role in fostering\ntransparency and trust in AI systems, where traditional XAI approaches\ntypically offer one level of abstraction for explanations, often in the form of\nheatmaps highlighting single or multiple input features. However, we ask\nwhether abstract reasoning or problem-solving strategies of a model may also be\nrelevant, as these align more closely with how humans approach solutions to\nproblems. We propose a framework, called Symbolic XAI, that attributes\nrelevance to symbolic queries expressing logical relationships between input\nfeatures, thereby capturing the abstract reasoning behind a model's\npredictions. The methodology is built upon a simple yet general multi-order\ndecomposition of model predictions. This decomposition can be specified using\nhigher-order propagation-based relevance methods, such as GNN-LRP, or\nperturbation-based explanation methods commonly used in XAI. The effectiveness\nof our framework is demonstrated in the domains of natural language processing\n(NLP), vision, and quantum chemistry (QC), where abstract symbolic domain\nknowledge is abundant and of significant interest to users. The Symbolic XAI\nframework provides an understanding of the model's decision-making process that\nis both flexible for customization by the user and human-readable through\nlogical formulas.", "paper_summary_zh": "\u53ef\u89e3\u91cb\u4eba\u5de5\u667a\u6167 (XAI) \u5728\u4fc3\u9032\u4eba\u5de5\u667a\u6167\u7cfb\u7d71\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u65b9\u9762\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\uff0c\u50b3\u7d71\u7684 XAI \u65b9\u6cd5\u901a\u5e38\u63d0\u4f9b\u4e00\u500b\u62bd\u8c61\u5c64\u7d1a\u7684\u89e3\u91cb\uff0c\u901a\u5e38\u4ee5\u71b1\u9ede\u5716\u7684\u5f62\u5f0f\u7a81\u986f\u55ae\u4e00\u6216\u591a\u500b\u8f38\u5165\u7279\u5fb5\u3002\u7136\u800c\uff0c\u6211\u5011\u63a2\u8a0e\u4e00\u500b\u6a21\u578b\u7684\u62bd\u8c61\u63a8\u7406\u6216\u554f\u984c\u89e3\u6c7a\u7b56\u7565\u662f\u5426\u4e5f\u53ef\u80fd\u76f8\u95dc\uff0c\u56e0\u70ba\u9019\u4e9b\u7b56\u7565\u8207\u4eba\u985e\u89e3\u6c7a\u554f\u984c\u7684\u65b9\u5f0f\u66f4\u70ba\u63a5\u8fd1\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7a31\u70ba\u7b26\u865f XAI \u7684\u67b6\u69cb\uff0c\u5b83\u5c07\u76f8\u95dc\u6027\u6b78\u56e0\u65bc\u8868\u9054\u8f38\u5165\u7279\u5fb5\u4e4b\u9593\u908f\u8f2f\u95dc\u4fc2\u7684\u7b26\u865f\u67e5\u8a62\uff0c\u5f9e\u800c\u6355\u6349\u6a21\u578b\u9810\u6e2c\u80cc\u5f8c\u7684\u62bd\u8c61\u63a8\u7406\u3002\u6b64\u65b9\u6cd5\u5efa\u7acb\u5728\u6a21\u578b\u9810\u6e2c\u7684\u7c21\u55ae\u4f46\u901a\u7528\u7684\u591a\u968e\u5206\u89e3\u4e4b\u4e0a\u3002\u6b64\u5206\u89e3\u53ef\u4ee5\u4f7f\u7528\u57fa\u65bc\u9ad8\u968e\u50b3\u64ad\u76f8\u95dc\u6027\u7684\u65b9\u6cd5\uff08\u4f8b\u5982 GNN-LRP\uff09\u6216 XAI \u4e2d\u5e38\u7528\u7684\u57fa\u65bc\u64fe\u52d5\u7684\u89e3\u91cb\u65b9\u6cd5\u4f86\u6307\u5b9a\u3002\u6211\u5011\u67b6\u69cb\u7684\u6709\u6548\u6027\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP)\u3001\u8996\u89ba\u548c\u91cf\u5b50\u5316\u5b78 (QC) \u9818\u57df\u5f97\u5230\u8b49\u660e\uff0c\u9019\u4e9b\u9818\u57df\u4e2d\u62bd\u8c61\u7b26\u865f\u9818\u57df\u77e5\u8b58\u8c50\u5bcc\u4e14\u5c0d\u4f7f\u7528\u8005\u800c\u8a00\u975e\u5e38\u91cd\u8981\u3002\u7b26\u865f XAI \u67b6\u69cb\u63d0\u4f9b\u4e86\u5c0d\u6a21\u578b\u6c7a\u7b56\u904e\u7a0b\u7684\u7406\u89e3\uff0c\u5b83\u65e2\u53ef\u4ee5\u7531\u4f7f\u7528\u8005\u9748\u6d3b\u81ea\u8a02\uff0c\u53c8\u53ef\u4ee5\u900f\u904e\u908f\u8f2f\u516c\u5f0f\u8b93\u4eba\u8b80\u61c2\u3002", "author": "Thomas Schnake et.al.", "authors": "Thomas Schnake, Farnoush Rezaei Jafaria, Jonas Lederer, Ping Xiong, Shinichi Nakajima, Stefan Gugler, Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller", "id": "2408.17198v1", "paper_url": "http://arxiv.org/abs/2408.17198v1", "repo": "null"}}