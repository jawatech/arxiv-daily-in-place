{"2407.15516": {"publish_time": "2024-07-22", "title": "Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models", "paper_summary": "The inference demand for LLMs has skyrocketed in recent months, and serving\nmodels with low latencies remains challenging due to the quadratic input length\ncomplexity of the attention layers. In this work, we investigate the effect of\ndropping MLP and attention layers at inference time on the performance of\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\ndecreases performance but leads to the best speedups alongside dropping entire\nlayers. For example, removing 33\\% of attention layers in a 13B Llama2 model\nresults in a 1.8\\% drop in average performance over the OpenLLM benchmark. We\nalso observe that skipping layers except the latter layers reduces performances\nfor more layers skipped, except for skipping the attention layers.", "paper_summary_zh": "\u8fd1\u51e0\u4e2a\u6708\u6765\uff0c\u5bf9 LLM \u7684\u63a8\u7406\u9700\u6c42\u6fc0\u589e\uff0c\u7531\u4e8e\u6ce8\u610f\u529b\u5c42\u7684\u4e8c\u6b21\u8f93\u5165\u957f\u5ea6\u590d\u6742\u5ea6\uff0c\u4ee5\u4f4e\u5ef6\u8fdf\u63d0\u4f9b\u670d\u52a1\u6a21\u578b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5728\u63a8\u7406\u65f6\u4e22\u5f03 MLP \u548c\u6ce8\u610f\u529b\u5c42\u5bf9 Llama-v2 \u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e22\u5f03\u66f4\u6df1\u7684\u6ce8\u610f\u529b\u5c42\u53ea\u4f1a\u8f7b\u5fae\u964d\u4f4e\u6027\u80fd\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6700\u4f73\u52a0\u901f\uff0c\u540c\u65f6\u4e22\u5f03\u6574\u4e2a\u5c42\u3002\u4f8b\u5982\uff0c\u5728 13B Llama2 \u6a21\u578b\u4e2d\u79fb\u9664 33% \u7684\u6ce8\u610f\u529b\u5c42\u4f1a\u5bfc\u81f4 OpenLLM \u57fa\u51c6\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u4e0b\u964d 1.8%\u3002\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\uff0c\u9664\u4e86\u540e\u4e00\u5c42\u4e4b\u5916\uff0c\u8df3\u8fc7\u5c42\u4f1a\u964d\u4f4e\u6027\u80fd\u4ee5\u8df3\u8fc7\u66f4\u591a\u5c42\uff0c\u9664\u4e86\u8df3\u8fc7\u6ce8\u610f\u529b\u5c42\u3002", "author": "Georgy Tyukin et.al.", "authors": "Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini", "id": "2407.15516v1", "paper_url": "http://arxiv.org/abs/2407.15516v1", "repo": "null"}}