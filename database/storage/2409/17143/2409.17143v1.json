{"2409.17143": {"publish_time": "2024-09-25", "title": "Attention Prompting on Image for Large Vision-Language Models", "paper_summary": "Compared with Large Language Models (LLMs), Large Vision-Language Models\n(LVLMs) can also accept images as input, thus showcasing more interesting\nemergent capabilities and demonstrating impressive performance on various\nvision-language tasks. Motivated by text prompting in LLMs, visual prompting\nhas been explored to enhance LVLMs' capabilities of perceiving visual\ninformation. However, previous visual prompting techniques solely process\nvisual inputs without considering text queries, limiting the models' ability to\nfollow text instructions to complete tasks. To fill this gap, in this work, we\npropose a new prompting technique named Attention Prompting on Image, which\njust simply overlays a text-query-guided attention heatmap on the original\ninput image and effectively enhances LVLM on various tasks. Specifically, we\ngenerate an attention heatmap for the input image dependent on the text query\nwith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel\nvalues of the original image to obtain the actual input image for the LVLM.\nExtensive experiments on various vison-language benchmarks verify the\neffectiveness of our technique. For example, Attention Prompting on Image\nimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,\nrespectively.", "paper_summary_zh": "\u8207\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u76f8\u6bd4\uff0c\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u4e5f\u53ef\u4ee5\u63a5\u53d7\u5f71\u50cf\u4f5c\u70ba\u8f38\u5165\uff0c\u56e0\u6b64\u5c55\u793a\u51fa\u66f4\u6709\u8da3\u7684\u6d6e\u73fe\u80fd\u529b\uff0c\u4e26\u5728\u5404\u7a2e\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e0a\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\u3002\u53d7 LLM \u4e2d\u6587\u5b57\u63d0\u793a\u7684\u555f\u767c\uff0c\u8996\u89ba\u63d0\u793a\u5df2\u5ee3\u6cdb\u63a2\u7d22\uff0c\u4ee5\u589e\u5f37 LVLMs \u611f\u77e5\u8996\u89ba\u8cc7\u8a0a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u8996\u89ba\u63d0\u793a\u6280\u8853\u50c5\u8655\u7406\u8996\u89ba\u8f38\u5165\uff0c\u800c\u4e0d\u8003\u616e\u6587\u5b57\u67e5\u8a62\uff0c\u9019\u9650\u5236\u4e86\u6a21\u578b\u6309\u7167\u6587\u5b57\u6307\u793a\u5b8c\u6210\u4efb\u52d9\u7684\u80fd\u529b\u3002\u70ba\u4e86\u586b\u88dc\u9019\u500b\u7a7a\u767d\uff0c\u6211\u5011\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u63d0\u793a\u6280\u8853\uff0c\u7a31\u70ba\u5f71\u50cf\u4e0a\u7684\u6ce8\u610f\u529b\u63d0\u793a\uff0c\u5b83\u53ea\u662f\u7c21\u55ae\u5730\u5c07\u6587\u5b57\u67e5\u8a62\u5f15\u5c0e\u7684\u6ce8\u610f\u529b\u71b1\u5716\u758a\u52a0\u5728\u539f\u59cb\u8f38\u5165\u5f71\u50cf\u4e0a\uff0c\u4e26\u6709\u6548\u5730\u589e\u5f37\u4e86 LVLM \u5728\u5404\u7a2e\u4efb\u52d9\u4e0a\u7684\u80fd\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u70ba\u8f38\u5165\u5f71\u50cf\u7522\u751f\u4e00\u500b\u6ce8\u610f\u529b\u71b1\u5716\uff0c\u8a72\u71b1\u5716\u53d6\u6c7a\u65bc\u6587\u5b57\u67e5\u8a62\u548c CLIP \u7b49\u8f14\u52a9\u6a21\u578b\u3002\u7136\u5f8c\uff0c\u71b1\u5716\u7c21\u55ae\u5730\u5c07\u539f\u59cb\u5f71\u50cf\u7684\u50cf\u7d20\u503c\u76f8\u4e58\uff0c\u4ee5\u53d6\u5f97 LVLM \u7684\u5be6\u969b\u8f38\u5165\u5f71\u50cf\u3002\u5728\u5404\u7a2e\u8996\u89ba\u8a9e\u8a00\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u9a57\u8b49\u4e86\u6211\u5011\u6280\u8853\u7684\u6709\u6548\u6027\u3002\u4f8b\u5982\uff0c\u5f71\u50cf\u4e0a\u7684\u6ce8\u610f\u529b\u63d0\u793a\u5206\u5225\u5728 MM-Vet \u548c LLaVA-Wild \u57fa\u6e96\u4e0a\u5c07 LLaVA-1.5 \u63d0\u5347\u4e86 3.8% \u548c 2.9%\u3002", "author": "Runpeng Yu et.al.", "authors": "Runpeng Yu, Weihao Yu, Xinchao Wang", "id": "2409.17143v1", "paper_url": "http://arxiv.org/abs/2409.17143v1", "repo": "https://github.com/yu-rp/apiprompting"}}