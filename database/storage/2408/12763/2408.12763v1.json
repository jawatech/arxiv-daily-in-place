{"2408.12763": {"publish_time": "2024-08-22", "title": "Assessing Modality Bias in Video Question Answering Benchmarks with Multimodal Large Language Models", "paper_summary": "Multimodal large language models (MLLMs) can simultaneously process visual,\ntextual, and auditory data, capturing insights that complement human analysis.\nHowever, existing video question-answering (VidQA) benchmarks and datasets\noften exhibit a bias toward a single modality, despite the goal of requiring\nadvanced reasoning skills that integrate diverse modalities to answer the\nqueries. In this work, we introduce the modality importance score (MIS) to\nidentify such bias. It is designed to assess which modality embeds the\nnecessary information to answer the question. Additionally, we propose an\ninnovative method using state-of-the-art MLLMs to estimate the modality\nimportance, which can serve as a proxy for human judgments of modality\nperception. With this MIS, we demonstrate the presence of unimodal bias and the\nscarcity of genuinely multimodal questions in existing datasets. We further\nvalidate the modality importance score with multiple ablation studies to\nevaluate the performance of MLLMs on permuted feature sets. Our results\nindicate that current models do not effectively integrate information due to\nmodality imbalance in existing datasets. Our proposed MLLM-derived MIS can\nguide the curation of modality-balanced datasets that advance multimodal\nlearning and enhance MLLMs' capabilities to understand and utilize synergistic\nrelations across modalities.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u53ef\u4ee5\u540c\u65f6\u5904\u7406\u89c6\u89c9\u3001\u6587\u672c\u548c\u542c\u89c9\u6570\u636e\uff0c\u6355\u6349\u8865\u5145\u4eba\u7c7b\u5206\u6790\u7684\u89c1\u89e3\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u89c6\u9891\u95ee\u7b54 (VidQA) \u57fa\u51c6\u548c\u6570\u636e\u96c6\u901a\u5e38\u8868\u73b0\u51fa\u5bf9\u5355\u4e00\u6a21\u6001\u7684\u504f\u89c1\uff0c\u5c3d\u7ba1\u76ee\u6807\u662f\u9700\u8981\u6574\u5408\u4e0d\u540c\u6a21\u6001\u7684\u9ad8\u7ea7\u63a8\u7406\u6280\u80fd\u6765\u56de\u7b54\u67e5\u8be2\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6a21\u6001\u91cd\u8981\u6027\u5206\u6570 (MIS) \u6765\u8bc6\u522b\u8fd9\u79cd\u504f\u89c1\u3002\u5b83\u65e8\u5728\u8bc4\u4f30\u54ea\u4e2a\u6a21\u6001\u5d4c\u5165\u4e86\u56de\u7b54\u95ee\u9898\u6240\u9700\u7684\u5fc5\u8981\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6700\u5148\u8fdb\u7684 MLLM \u6765\u4f30\u8ba1\u6a21\u6001\u91cd\u8981\u6027\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u4eba\u7c7b\u5bf9\u6a21\u6001\u611f\u77e5\u5224\u65ad\u7684\u4ee3\u7406\u3002\u901a\u8fc7\u8fd9\u4e2a MIS\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5355\u6a21\u6001\u504f\u89c1\u7684\u51fa\u73b0\u548c\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u771f\u6b63\u591a\u6a21\u6001\u95ee\u9898\u7684\u7a00\u7f3a\u6027\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u901a\u8fc7\u591a\u9879\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6a21\u6001\u91cd\u8981\u6027\u5206\u6570\uff0c\u4ee5\u8bc4\u4f30 MLLM \u5728\u7f6e\u6362\u7279\u5f81\u96c6\u4e0a\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u7531\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u6a21\u6001\u4e0d\u5e73\u8861\uff0c\u5f53\u524d\u6a21\u578b\u4e0d\u80fd\u6709\u6548\u5730\u6574\u5408\u4fe1\u606f\u3002\u6211\u4eec\u63d0\u51fa\u7684 MLLM \u884d\u751f\u7684 MIS \u53ef\u4ee5\u6307\u5bfc\u5bf9\u6a21\u6001\u5e73\u8861\u6570\u636e\u96c6\u7684\u6574\u7406\uff0c\u4ece\u800c\u63a8\u8fdb\u591a\u6a21\u6001\u5b66\u4e60\u5e76\u589e\u5f3a MLLM \u8de8\u6a21\u6001\u7406\u89e3\u548c\u5229\u7528\u534f\u540c\u5173\u7cfb\u7684\u80fd\u529b\u3002", "author": "Jean Park et.al.", "authors": "Jean Park, Kuk Jin Jang, Basam Alasaly, Sriharsha Mopidevi, Andrew Zolensky, Eric Eaton, Insup Lee, Kevin Johnson", "id": "2408.12763v1", "paper_url": "http://arxiv.org/abs/2408.12763v1", "repo": "null"}}