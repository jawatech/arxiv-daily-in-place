{"2410.03440": {"publish_time": "2024-10-04", "title": "Exploring the Benefit of Activation Sparsity in Pre-training", "paper_summary": "Pre-trained Transformers inherently possess the characteristic of sparse\nactivation, where only a small fraction of the neurons are activated for each\ntoken. While sparse activation has been explored through post-training methods,\nits potential in pre-training remains untapped. In this work, we first study\nhow activation properties change during pre-training. Our examination reveals\nthat Transformers exhibit sparse activation throughout the majority of the\npre-training process while the activation correlation keeps evolving as\ntraining progresses. Leveraging this observation, we propose Switchable\nSparse-Dense Learning (SSD). SSD adaptively switches between the\nMixtures-of-Experts (MoE) based sparse training and the conventional dense\ntraining during the pre-training process, leveraging the efficiency of sparse\ntraining and avoiding the static activation correlation of sparse training.\nCompared to dense training, SSD achieves comparable performance with identical\nmodel size and reduces pre-training costs. Moreover, the models trained with\nSSD can be directly used as MoE models for sparse inference and achieve the\nsame performance as dense models with up to $2\\times$ faster inference speed.\nCodes are available at https://github.com/thunlp/moefication.", "paper_summary_zh": "\u9810\u8a13\u7df4\u8f49\u63db\u5668\u672c\u8cea\u4e0a\u5177\u6709\u7a00\u758f\u6fc0\u6d3b\u7684\u7279\u5fb5\uff0c\u5176\u4e2d\u53ea\u6703\u70ba\u6bcf\u500b\u6a19\u8a18\u555f\u7528\u4e00\u5c0f\u90e8\u5206\u795e\u7d93\u5143\u3002\u5118\u7ba1\u5df2\u900f\u904e\u5f8c\u8a13\u7df4\u65b9\u6cd5\u63a2\u7d22\u7a00\u758f\u6fc0\u6d3b\uff0c\u4f46\u5176\u5728\u9810\u8a13\u7df4\u4e2d\u7684\u6f5b\u529b\u4ecd\u672a\u958b\u767c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u5148\u7814\u7a76\u5728\u9810\u8a13\u7df4\u671f\u9593\u6fc0\u6d3b\u5c6c\u6027\u5982\u4f55\u6539\u8b8a\u3002\u6211\u5011\u7684\u6aa2\u67e5\u986f\u793a\uff0c\u8f49\u63db\u5668\u5728\u9810\u8a13\u7df4\u904e\u7a0b\u7684\u5927\u90e8\u5206\u6642\u9593\u4e2d\u90fd\u8868\u73fe\u51fa\u7a00\u758f\u6fc0\u6d3b\uff0c\u800c\u6fc0\u6d3b\u95dc\u806f\u6703\u96a8\u8457\u8a13\u7df4\u9032\u5ea6\u4e0d\u65b7\u6f14\u8b8a\u3002\u5229\u7528\u9019\u4e00\u89c0\u5bdf\u7d50\u679c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u53ef\u5207\u63db\u7a00\u758f\u5bc6\u96c6\u5b78\u7fd2 (SSD)\u3002SSD \u5728\u9810\u8a13\u7df4\u904e\u7a0b\u4e2d\u81ea\u9069\u61c9\u5730\u5728\u57fa\u65bc\u5c08\u5bb6\u6df7\u5408 (MoE) \u7684\u7a00\u758f\u8a13\u7df4\u548c\u50b3\u7d71\u5bc6\u96c6\u8a13\u7df4\u4e4b\u9593\u5207\u63db\uff0c\u5229\u7528\u7a00\u758f\u8a13\u7df4\u7684\u6548\u7387\u4e26\u907f\u514d\u7a00\u758f\u8a13\u7df4\u7684\u975c\u614b\u6fc0\u6d3b\u95dc\u806f\u3002\u8207\u5bc6\u96c6\u8a13\u7df4\u76f8\u6bd4\uff0cSSD \u5728\u76f8\u540c\u7684\u6a21\u578b\u5927\u5c0f\u4e0b\u5be6\u73fe\u4e86\u76f8\u7576\u7684\u6548\u80fd\uff0c\u4e26\u964d\u4f4e\u4e86\u9810\u8a13\u7df4\u6210\u672c\u3002\u6b64\u5916\uff0c\u4f7f\u7528 SSD \u8a13\u7df4\u7684\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u7528\u4f5c MoE \u6a21\u578b\u9032\u884c\u7a00\u758f\u63a8\u8ad6\uff0c\u4e26\u4ee5\u9ad8\u9054 2 \u500d\u7684\u63a8\u8ad6\u901f\u5ea6\u5be6\u73fe\u8207\u5bc6\u96c6\u6a21\u578b\u76f8\u540c\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/thunlp/moefication \u53d6\u5f97\u3002", "author": "Zhengyan Zhang et.al.", "authors": "Zhengyan Zhang, Chaojun Xiao, Qiujieli Qin, Yankai Lin, Zhiyuan Zeng, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Jie Zhou", "id": "2410.03440v1", "paper_url": "http://arxiv.org/abs/2410.03440v1", "repo": "https://github.com/thunlp/moefication"}}