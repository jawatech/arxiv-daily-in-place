{"2405.07309": {"publish_time": "2024-05-12", "title": "DiffGen: Robot Demonstration Generation via Differentiable Physics Simulation, Differentiable Rendering, and Vision-Language Model", "paper_summary": "Generating robot demonstrations through simulation is widely recognized as an\neffective way to scale up robot data. Previous work often trained reinforcement\nlearning agents to generate expert policies, but this approach lacks sample\nefficiency. Recently, a line of work has attempted to generate robot\ndemonstrations via differentiable simulation, which is promising but heavily\nrelies on reward design, a labor-intensive process. In this paper, we propose\nDiffGen, a novel framework that integrates differentiable physics simulation,\ndifferentiable rendering, and a vision-language model to enable automatic and\nefficient generation of robot demonstrations. Given a simulated robot\nmanipulation scenario and a natural language instruction, DiffGen can generate\nrealistic robot demonstrations by minimizing the distance between the embedding\nof the language instruction and the embedding of the simulated observation\nafter manipulation. The embeddings are obtained from the vision-language model,\nand the optimization is achieved by calculating and descending gradients\nthrough the differentiable simulation, differentiable rendering, and\nvision-language model components, thereby accomplishing the specified task.\nExperiments demonstrate that with DiffGen, we could efficiently and effectively\ngenerate robot data with minimal human effort or training time.", "paper_summary_zh": "\u900f\u904e\u6a21\u64ec\u7522\u751f\u6a5f\u5668\u4eba\u793a\u7bc4\u64cd\u4f5c\uff0c\u88ab\u5ee3\u6cdb\u8a8d\u70ba\u662f\u64f4\u5145\u6a5f\u5668\u4eba\u8cc7\u6599\u7684\u6709\u6548\u65b9\u6cd5\u3002\u5148\u524d\u7684\u7814\u7a76\uff0c\u901a\u5e38\u8a13\u7df4\u5f37\u5316\u5b78\u7fd2\u4ee3\u7406\u7522\u751f\u5c08\u5bb6\u7b56\u7565\uff0c\u4f46\u9019\u7a2e\u65b9\u6cd5\u7f3a\u4e4f\u6a23\u672c\u6548\u7387\u3002\u6700\u8fd1\uff0c\u6709\u689d\u7814\u7a76\u8def\u7dda\u8a66\u5716\u900f\u904e\u53ef\u5fae\u5206\u6a21\u64ec\u7522\u751f\u6a5f\u5668\u4eba\u793a\u7bc4\u64cd\u4f5c\uff0c\u9019\u5f88\u6709\u524d\u666f\uff0c\u4f46\u6975\u5ea6\u4f9d\u8cf4\u65bc\u56de\u5831\u8a2d\u8a08\uff0c\u9019\u662f\u4e00\u500b\u52de\u529b\u5bc6\u96c6\u7684\u904e\u7a0b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa DiffGen\uff0c\u4e00\u500b\u6574\u5408\u53ef\u5fae\u5206\u7269\u7406\u6a21\u64ec\u3001\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u65bc\u81ea\u52d5\u4e14\u6709\u6548\u5730\u7522\u751f\u6a5f\u5668\u4eba\u793a\u7bc4\u64cd\u4f5c\u3002\u7d66\u5b9a\u4e00\u500b\u6a21\u64ec\u6a5f\u5668\u4eba\u64cd\u4f5c\u5834\u666f\u548c\u4e00\u500b\u81ea\u7136\u8a9e\u8a00\u6307\u4ee4\uff0cDiffGen \u80fd\u5920\u900f\u904e\u6700\u5c0f\u5316\u64cd\u4f5c\u5f8c\u8a9e\u8a00\u6307\u4ee4\u7684\u5d4c\u5165\u5f0f\u8207\u6a21\u64ec\u89c0\u5bdf\u7684\u5d4c\u5165\u5f0f\u4e4b\u9593\u7684\u8ddd\u96e2\uff0c\u7522\u751f\u903c\u771f\u7684\u6a5f\u5668\u4eba\u793a\u7bc4\u64cd\u4f5c\u3002\u9019\u4e9b\u5d4c\u5165\u5f0f\u5f9e\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u53d6\u5f97\uff0c\u800c\u6700\u4f73\u5316\u5247\u900f\u904e\u8a08\u7b97\u548c\u4e0b\u964d\u53ef\u5fae\u5206\u6a21\u64ec\u3001\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u5143\u4ef6\u7684\u68af\u5ea6\u4f86\u9054\u6210\uff0c\u5f9e\u800c\u5b8c\u6210\u6307\u5b9a\u4efb\u52d9\u3002\u5be6\u9a57\u8b49\u660e\uff0c\u900f\u904e DiffGen\uff0c\u6211\u5011\u80fd\u5920\u6709\u6548\u7387\u4e14\u6709\u6548\u5730\u7522\u751f\u6a5f\u5668\u4eba\u8cc7\u6599\uff0c\u4e26\u5c07\u4eba\u529b\u6216\u8a13\u7df4\u6642\u9593\u964d\u81f3\u6700\u4f4e\u3002", "author": "Yang Jin et.al.", "authors": "Yang Jin, Jun Lv, Shuqiang Jiang, Cewu Lu", "id": "2405.07309v1", "paper_url": "http://arxiv.org/abs/2405.07309v1", "repo": "null"}}