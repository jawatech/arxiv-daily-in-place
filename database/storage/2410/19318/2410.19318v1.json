{"2410.19318": {"publish_time": "2024-10-25", "title": "Two are better than one: Context window extension with multi-grained self-injection", "paper_summary": "The limited context window of contemporary large language models (LLMs)\nremains a huge barrier to their broader application across various domains.\nWhile continual pre-training on long-context data is a straightforward and\neffective solution, it incurs substantial costs in terms of data acquisition\nand computational resources. To alleviate this issue, we propose SharedLLM, a\nnovel approach grounded in the design philosophy of multi-grained context\ncompression and query-aware information retrieval. SharedLLM is composed of two\nshort-context LLMs such as LLaMA-2, termed upper model and lower model. The\nlower model functions as a compressor while the upper model acts as a decoder.\nThe upper model receives compressed, multi-grained context information from the\nlower model and performs context-aware modeling on the running text.\nInformation transfer between the compressor and decoder occurs only at the\nlowest layers to refrain from long forward paths in the lower model and\nredundant cross-attention modules in the upper model. Based on this\narchitecture, we introduce a specialized tree-style data structure to\nefficiently encode, store and retrieve multi-grained contextual information for\ntext chunks. This structure, combined with a search algorithm, enables rapid\nencoding and retrieval of relevant information from various levels of the tree\nbased on the input query. This entire process, wherein the sender and receiver\nare derived from the same LLM layer, is referred to as self-injection.", "paper_summary_zh": "\u7576\u4ee3\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53d7\u9650\u7684\u4e0a\u4e0b\u6587\u8996\u7a97\uff0c\u4ecd\u7136\u662f\u5176\u5728\u5404\u7a2e\u9818\u57df\u5ee3\u6cdb\u61c9\u7528\u7684\u5de8\u5927\u969c\u7919\u3002\u96d6\u7136\u6301\u7e8c\u5728\u9577\u6587\u672c\u8cc7\u6599\u4e0a\u9032\u884c\u9810\u8a13\u7df4\u662f\u4e00\u500b\u76f4\u63a5\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u5728\u8cc7\u6599\u53d6\u5f97\u548c\u904b\u7b97\u8cc7\u6e90\u65b9\u9762\u6703\u7522\u751f\u5927\u91cf\u7684\u6210\u672c\u3002\u70ba\u4e86\u7de9\u89e3\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa SharedLLM\uff0c\u4e00\u500b\u5efa\u57fa\u65bc\u591a\u7c92\u5ea6\u4e0a\u4e0b\u6587\u58d3\u7e2e\u548c\u67e5\u8a62\u611f\u77e5\u8cc7\u8a0a\u6aa2\u7d22\u7684\u8a2d\u8a08\u7406\u5ff5\u7684\u65b0\u65b9\u6cd5\u3002SharedLLM \u7531\u5169\u500b\u77ed\u6587\u672c LLM \u7d44\u6210\uff0c\u4f8b\u5982 LLaMA-2\uff0c\u7a31\u70ba\u4e0a\u5c64\u6a21\u578b\u548c\u4e0b\u5c64\u6a21\u578b\u3002\u4e0b\u5c64\u6a21\u578b\u4f5c\u70ba\u58d3\u7e2e\u5668\uff0c\u800c\u4e0a\u5c64\u6a21\u578b\u4f5c\u70ba\u89e3\u78bc\u5668\u3002\u4e0a\u5c64\u6a21\u578b\u5f9e\u4e0b\u5c64\u6a21\u578b\u63a5\u6536\u58d3\u7e2e\u7684\u591a\u7c92\u5ea6\u4e0a\u4e0b\u6587\u8cc7\u8a0a\uff0c\u4e26\u5c0d\u57f7\u884c\u4e2d\u7684\u6587\u5b57\u9032\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u5efa\u6a21\u3002\u58d3\u7e2e\u5668\u548c\u89e3\u78bc\u5668\u4e4b\u9593\u7684\u8cc7\u8a0a\u50b3\u905e\u53ea\u767c\u751f\u5728\u6700\u5e95\u5c64\uff0c\u4ee5\u907f\u514d\u4e0b\u5c64\u6a21\u578b\u4e2d\u904e\u9577\u7684\u6b63\u5411\u8def\u5f91\u548c\u4e0a\u5c64\u6a21\u578b\u4e2d\u5197\u9918\u7684\u4ea4\u53c9\u6ce8\u610f\u6a21\u7d44\u3002\u57fa\u65bc\u9019\u500b\u67b6\u69cb\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u5c08\u9580\u7684\u6a39\u72c0\u8cc7\u6599\u7d50\u69cb\uff0c\u4ee5\u6709\u6548\u7de8\u78bc\u3001\u5132\u5b58\u548c\u6aa2\u7d22\u6587\u5b57\u5340\u584a\u7684\u591a\u7c92\u5ea6\u4e0a\u4e0b\u6587\u8cc7\u8a0a\u3002\u9019\u500b\u7d50\u69cb\u8207\u641c\u5c0b\u6f14\u7b97\u6cd5\u76f8\u7d50\u5408\uff0c\u53ef\u4ee5\u6839\u64da\u8f38\u5165\u67e5\u8a62\u5feb\u901f\u7de8\u78bc\u548c\u6aa2\u7d22\u6a39\u4e2d\u4e0d\u540c\u5c64\u7d1a\u7684\u76f8\u95dc\u8cc7\u8a0a\u3002\u9019\u500b\u6574\u500b\u904e\u7a0b\uff0c\u5176\u4e2d\u767c\u9001\u8005\u548c\u63a5\u6536\u8005\u4f86\u81ea\u540c\u4e00\u500b LLM \u5c64\uff0c\u7a31\u70ba\u81ea\u6ce8\u5165\u3002", "author": "Wei Han et.al.", "authors": "Wei Han, Pan Zhou, Soujanya Poria, Shuicheng Yan", "id": "2410.19318v1", "paper_url": "http://arxiv.org/abs/2410.19318v1", "repo": "https://github.com/clement25/sharedllm"}}