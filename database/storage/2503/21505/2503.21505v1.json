{"2503.21505": {"publish_time": "2025-03-27", "title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "paper_summary": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems.", "paper_summary_zh": "\u73fe\u6709\u7684\u81ea\u52d5\u99d5\u99db (AD) \u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u57fa\u6e96\u4e3b\u8981\u900f\u904e\u5728\u7c97\u7565\u4efb\u52d9\u4e2d\u958b\u653e\u5f0f\u8996\u89ba\u554f\u7b54 (QA) \u4f86\u8a55\u4f30\u5176\u53ef\u89e3\u91cb\u6027\uff0c\u9019\u4ecd\u7136\u4e0d\u8db3\u4ee5\u8a55\u4f30\u5728\u8907\u96dc\u99d5\u99db\u5834\u666f\u4e2d\u7684\u80fd\u529b\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 $\\textbf{VLADBench}$\uff0c\u9019\u662f\u4e00\u500b\u5177\u6709\u6311\u6230\u6027\u4e14\u7d30\u7c92\u5ea6\u7684\u6578\u64da\u96c6\uff0c\u5176\u7279\u9ede\u662f\u5c01\u9589\u5f0f\u554f\u7b54\uff0c\u5f9e\u975c\u614b\u57fa\u790e\u77e5\u8b58\u548c\u5143\u7d20\u5230\u52d5\u614b\u9053\u8def\u60c5\u6cc1\u7684\u9ad8\u7d1a\u63a8\u7406\u9010\u6b65\u63a8\u9032\u3002\u7cbe\u5fc3\u8a2d\u8a08\u7684 $\\textbf{VLADBench}$ \u6db5\u84cb 5 \u500b\u95dc\u9375\u9818\u57df\uff1a\u4ea4\u901a\u77e5\u8b58\u7406\u89e3\u3001\u4e00\u822c\u5143\u7d20\u8b58\u5225\u3001\u4ea4\u901a\u5716\u751f\u6210\u3001\u76ee\u6a19\u5c6c\u6027\u7406\u89e3\u4ee5\u53ca\u81ea\u6211\u6c7a\u7b56\u548c\u898f\u5283\u3002\u9019\u4e9b\u9818\u57df\u9032\u4e00\u6b65\u7d30\u5206\u70ba 11 \u500b\u6b21\u8981\u65b9\u9762\u548c 29 \u500b\u4e09\u7d1a\u4efb\u52d9\uff0c\u4ee5\u4fbf\u9032\u884c\u66f4\u7cbe\u7d30\u7684\u8a55\u4f30\u3002\u5c0d\u6b64\u57fa\u6e96\u4e2d\u901a\u7528\u548c\u7279\u5b9a\u9818\u57df (DS) VLM \u7684\u5168\u9762\u8a55\u4f30\u63ed\u793a\u4e86\u5b83\u5011\u5728\u81ea\u52d5\u99d5\u99db\u74b0\u5883\u4e2d\u7684\u512a\u52e2\u548c\u95dc\u9375\u5c40\u9650\u6027\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u5229\u7528 5 \u500b\u9818\u57df\u4e4b\u9593\u7684\u8a8d\u77e5\u548c\u63a8\u7406\u4ea4\u4e92\u4f5c\u7528\u4f86\u7406\u89e3\u81ea\u52d5\u99d5\u99db\uff0c\u6211\u5011\u5f9e\u4e00\u500b\u5c0f\u578b VLM \u958b\u59cb\uff0c\u4e26\u5728\u5404\u500b\u9818\u57df\u6578\u64da\u96c6\uff08\u5f9e\u516c\u5171\u8cc7\u6e90\u4e2d\u6536\u96c6\u7684 140 \u842c\u500b DS \u554f\u7b54\uff09\u4e0a\u8a13\u7df4 DS \u6a21\u578b\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u6e96\u70ba\u66f4\u5168\u9762\u5730\u8a55\u4f30\u81ea\u52d5\u99d5\u99db\u4e2d\u7684 VLM \u9081\u51fa\u4e86\u95dc\u9375\u4e00\u6b65\uff0c\u70ba\u958b\u767c\u66f4\u5177\u8a8d\u77e5\u8907\u96dc\u6027\u548c\u63a8\u7406\u80fd\u529b\u7684\u81ea\u52d5\u99d5\u99db\u7cfb\u7d71\u92ea\u5e73\u4e86\u9053\u8def\u3002\n", "author": "Yue Li et.al.", "authors": "Yue Li, Meng Tian, Zhenyu Lin, Jiangtong Zhu, Dechang Zhu, Haiqiang Liu, Zining Wang, Yueyi Zhang, Zhiwei Xiong, Xinhai Zhao", "id": "2503.21505v1", "paper_url": "http://arxiv.org/abs/2503.21505v1", "repo": "null"}}