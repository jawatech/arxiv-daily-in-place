{"2502.13516": {"publish_time": "2025-02-19", "title": "SPPD: Self-training with Process Preference Learning Using Dynamic Value Margin", "paper_summary": "Recently, enhancing the numerical and logical reasoning capability of Large\nLanguage Models (LLMs) has emerged as a research hotspot. Existing methods face\nseveral limitations: inference-phase techniques (e.g., Chain of Thoughts) rely\non prompt selection and the pretrained knowledge; sentence-level Supervised\nFine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with\nstep-wise mathematical correctness and depend on stronger models distillation\nor human annotations; while Reinforcement Learning (RL) approaches incur high\nGPU memory costs and unstable training. To address these, we propose\n\\textbf{S}elf-training framework integrating \\textbf{P}rocess\n\\textbf{P}reference learning using \\textbf{D}ynamic value margin (SPPD). SPPD\nleverages a process-based Markov Decision Process (MDP) and Bellman optimality\nequation to derive \\textbf{dynamic value margin} on step-level preference\noptimization, which employs tree-based self-sampling on model responses\n\\textbf{without any distillation} from other models. Furthermore, we\ntheoretically prove that SPPD is \\textbf{equivalent to on-policy policy\ngradient methods} under reward constraints. Experiments on 7B-scale models\ndemonstrate superior performance across in-domain and out-domain mathematical\nbenchmarks. We open-source our code at\n\\href{https://anonymous.4open.science/r/SSDPO-D-DCDD}{https://anonymous.4open.science/r/SPPD-DCDD}.", "paper_summary_zh": "<paragraph>\u8fd1\u671f\uff0c\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6570\u5b57\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u5df2\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7740\u4e00\u4e9b\u9650\u5236\uff1a\u63a8\u7406\u9636\u6bb5\u6280\u672f\uff08\u4f8b\u5982\u601d\u60f3\u94fe\uff09\u4f9d\u8d56\u4e8e\u63d0\u793a\u9009\u62e9\u548c\u9884\u8bad\u7ec3\u77e5\u8bc6\uff1b\u53e5\u5b50\u7ea7\u522b\u7684\u76d1\u7763\u5fae\u8c03 (SFT) \u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO) \u96be\u4ee5\u5b9e\u73b0\u9010\u6b65\u7684\u6570\u5b66\u6b63\u786e\u6027\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u66f4\u5f3a\u7684\u6a21\u578b\u84b8\u998f\u6216\u4eba\u5de5\u6ce8\u91ca\uff1b\u800c\u5f3a\u5316\u5b66\u4e60 (RL) \u65b9\u6cd5\u4f1a\u4ea7\u751f\u9ad8 GPU \u5185\u5b58\u6210\u672c\u548c\u4e0d\u7a33\u5b9a\u7684\u8bad\u7ec3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5c06\u57fa\u4e8e\u8fc7\u7a0b\u7684\u504f\u597d\u5b66\u4e60\u4e0e\u52a8\u6001\u503c\u8fb9\u9645 (SPPD) \u76f8\u7ed3\u5408\u7684\u81ea\u6211\u8bad\u7ec3\u6846\u67b6\u3002SPPD \u5229\u7528\u57fa\u4e8e\u8fc7\u7a0b\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b (MDP) \u548c\u8d1d\u5c14\u66fc\u6700\u4f18\u6027\u65b9\u7a0b\uff0c\u5728\u6b65\u9aa4\u7ea7\u504f\u597d\u4f18\u5316\u4e2d\u63a8\u5bfc\u51fa\u52a8\u6001\u503c\u8fb9\u9645\uff0c\u8be5\u8fb9\u9645\u5728\u6a21\u578b\u54cd\u5e94\u4e0a\u91c7\u7528\u57fa\u4e8e\u6811\u7684\u81ea\u91c7\u6837\uff0c\u800c\u65e0\u9700\u4ece\u5176\u4ed6\u6a21\u578b\u4e2d\u8fdb\u884c\u4efb\u4f55\u84b8\u998f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86 SPPD \u5728\u5956\u52b1\u7ea6\u675f\u4e0b\u7b49\u6548\u4e8e\u7b56\u7565\u68af\u5ea6\u6cd5\u3002\u5728 7B \u7ea7\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u57df\u5185\u548c\u57df\u5916\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6211\u4eec\u5728\n\\href{https://anonymous.4open.science/r/SSDPO-D-DCDD}{https://anonymous.4open.science/r/SPPD-DCDD} \u4e0a\u5f00\u6e90\u4e86\u6211\u4eec\u7684\u4ee3\u7801\u3002</paragraph>", "author": "Hao Yi et.al.", "authors": "Hao Yi, Qingyang Li, Yulan Hu, Fuzheng Zhang, Di Zhang, Yong Liu", "id": "2502.13516v1", "paper_url": "http://arxiv.org/abs/2502.13516v1", "repo": "null"}}