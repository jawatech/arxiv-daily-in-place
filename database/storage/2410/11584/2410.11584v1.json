{"2410.11584": {"publish_time": "2024-10-15", "title": "DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment", "paper_summary": "In recent years, imitation learning has made progress in the field of robotic\nmanipulation. However, it still faces challenges when dealing with complex\nlong-horizon deformable object tasks, such as high-dimensional state spaces,\ncomplex dynamics, and multimodal action distributions. Traditional imitation\nlearning methods often require a large amount of data and encounter\ndistributional shifts and accumulative errors in these tasks. To address these\nissues, we propose a data-efficient general learning framework (DeformPAM)\nbased on preference learning and reward-guided action selection. DeformPAM\ndecomposes long-horizon tasks into multiple action primitives, utilizes 3D\npoint cloud inputs and diffusion models to model action distributions, and\ntrains an implicit reward model using human preference data. During the\ninference phase, the reward model scores multiple candidate actions, selecting\nthe optimal action for execution, thereby reducing the occurrence of anomalous\nactions and improving task completion quality. Experiments conducted on three\nchallenging real-world long-horizon deformable object manipulation tasks\ndemonstrate the effectiveness of this method. Results show that DeformPAM\nimproves both task completion quality and efficiency compared to baseline\nmethods even with limited data. Code and data will be available at\nhttps://deform-pam.robotflow.ai.", "paper_summary_zh": "\u8fd1\u5e74\u6765\uff0c\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5728\u5904\u7406\u590d\u6742\u7684\u957f\u671f\u53ef\u53d8\u5f62\u5bf9\u8c61\u4efb\u52a1\uff08\u4f8b\u5982\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u3001\u590d\u6742\u52a8\u6001\u548c\u591a\u5cf0\u52a8\u4f5c\u5206\u5e03\uff09\u65f6\uff0c\u5b83\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u5e76\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u9047\u5230\u5206\u5e03\u5f0f\u8f6c\u79fb\u548c\u7d2f\u79ef\u8bef\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u5b66\u4e60\u548c\u5956\u52b1\u5f15\u5bfc\u52a8\u4f5c\u9009\u62e9\u7684\u3001\u6570\u636e\u9ad8\u6548\u7684\u901a\u7528\u5b66\u4e60\u6846\u67b6\uff08DeformPAM\uff09\u3002DeformPAM \u5c06\u957f\u671f\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u52a8\u4f5c\u57fa\u5143\uff0c\u5229\u7528 3D \u70b9\u4e91\u8f93\u5165\u548c\u6269\u6563\u6a21\u578b\u5bf9\u52a8\u4f5c\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u4f7f\u7528\u4eba\u7c7b\u504f\u597d\u6570\u636e\u8bad\u7ec3\u9690\u5f0f\u5956\u52b1\u6a21\u578b\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u5956\u52b1\u6a21\u578b\u5bf9\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\u8fdb\u884c\u8bc4\u5206\uff0c\u9009\u62e9\u6700\u4f73\u52a8\u4f5c\u6267\u884c\uff0c\u4ece\u800c\u51cf\u5c11\u5f02\u5e38\u52a8\u4f5c\u7684\u53d1\u751f\u5e76\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u8d28\u91cf\u3002\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u957f\u671f\u53ef\u53d8\u5f62\u5bf9\u8c61\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u6570\u636e\u6709\u9650\uff0cDeformPAM \u4e5f\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002\u4ee3\u7801\u548c\u6570\u636e\u5c06\u53ef\u5728 https://deform-pam.robotflow.ai/ \u83b7\u5f97\u3002", "author": "Wendi Chen et.al.", "authors": "Wendi Chen, Han Xue, Fangyuan Zhou, Yuan Fang, Cewu Lu", "id": "2410.11584v1", "paper_url": "http://arxiv.org/abs/2410.11584v1", "repo": "null"}}