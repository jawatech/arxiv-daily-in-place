{"2404.17854": {"publish_time": "2024-04-27", "title": "GLIMS: Attention-Guided Lightweight Multi-Scale Hybrid Network for Volumetric Semantic Segmentation", "paper_summary": "Convolutional Neural Networks (CNNs) have become widely adopted for medical\nimage segmentation tasks, demonstrating promising performance. However, the\ninherent inductive biases in convolutional architectures limit their ability to\nmodel long-range dependencies and spatial correlations. While recent\ntransformer-based architectures address these limitations by leveraging\nself-attention mechanisms to encode long-range dependencies and learn\nexpressive representations, they often struggle to extract low-level features\nand are highly dependent on data availability. This motivated us for the\ndevelopment of GLIMS, a data-efficient attention-guided hybrid volumetric\nsegmentation network. GLIMS utilizes Dilated Feature Aggregator Convolutional\nBlocks (DACB) to capture local-global feature correlations efficiently.\nFurthermore, the incorporated Swin Transformer-based bottleneck bridges the\nlocal and global features to improve the robustness of the model. Additionally,\nGLIMS employs an attention-guided segmentation approach through Channel and\nSpatial-Wise Attention Blocks (CSAB) to localize expressive features for\nfine-grained border segmentation. Quantitative and qualitative results on\nglioblastoma and multi-organ CT segmentation tasks demonstrate GLIMS'\neffectiveness in terms of complexity and accuracy. GLIMS demonstrated\noutstanding performance on BraTS2021 and BTCV datasets, surpassing the\nperformance of Swin UNETR. Notably, GLIMS achieved this high performance with a\nsignificantly reduced number of trainable parameters. Specifically, GLIMS has\n47.16M trainable parameters and 72.30G FLOPs, while Swin UNETR has 61.98M\ntrainable parameters and 394.84G FLOPs. The code is publicly available on\nhttps://github.com/yaziciz/GLIMS.", "paper_summary_zh": "\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN) \u5df2\u5ee3\u6cdb\u7528\u65bc\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u4efb\u52d9\uff0c\u5c55\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u5377\u7a4d\u67b6\u69cb\u4e2d\u56fa\u6709\u7684\u6b78\u7d0d\u504f\u5dee\u9650\u5236\u4e86\u5176\u5efa\u6a21\u9577\u7a0b\u4f9d\u8cf4\u6027\u548c\u7a7a\u9593\u95dc\u806f\u6027\u7684\u80fd\u529b\u3002\u96d6\u7136\u6700\u8fd1\u7684\u57fa\u65bc Transformer \u7684\u67b6\u69cb\u900f\u904e\u5229\u7528\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\u7de8\u78bc\u9577\u7a0b\u4f9d\u8cf4\u6027\u548c\u5b78\u7fd2\u8868\u9054\u5f0f\u8868\u793a\u4f86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u96e3\u4ee5\u63d0\u53d6\u4f4e\u968e\u7279\u5fb5\uff0c\u4e14\u9ad8\u5ea6\u4f9d\u8cf4\u8cc7\u6599\u53ef\u7528\u6027\u3002\u9019\u4fc3\u4f7f\u6211\u5011\u958b\u767c GLIMS\uff0c\u4e00\u7a2e\u8cc7\u6599\u6709\u6548\u7387\u7684\u6ce8\u610f\u529b\u5f15\u5c0e\u5f0f\u6df7\u5408\u9ad4\u7a4d\u5206\u5272\u7db2\u8def\u3002GLIMS \u5229\u7528\u64f4\u5f35\u7279\u5fb5\u532f\u805a\u5668\u5377\u7a4d\u5340\u584a (DACB) \u4f86\u6709\u6548\u64f7\u53d6\u5c40\u90e8\u548c\u6574\u9ad4\u7279\u5fb5\u95dc\u806f\u6027\u3002\u6b64\u5916\uff0c\u6574\u5408\u7684 Swin Transformer \u70ba\u57fa\u790e\u7684\u74f6\u9838\u6a4b\u63a5\u4e86\u5c40\u90e8\u548c\u6574\u9ad4\u7279\u5fb5\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u7a69\u5065\u6027\u3002\u6b64\u5916\uff0cGLIMS \u900f\u904e\u901a\u9053\u548c\u7a7a\u9593\u660e\u667a\u6ce8\u610f\u529b\u5340\u584a (CSAB) \u63a1\u7528\u6ce8\u610f\u529b\u5f15\u5c0e\u5f0f\u5206\u5272\u65b9\u6cd5\uff0c\u4ee5\u5b9a\u4f4d\u8868\u9054\u5f0f\u7279\u5fb5\u9032\u884c\u7d30\u7dfb\u7684\u908a\u754c\u5206\u5272\u3002\u5728\u795e\u7d93\u81a0\u8cea\u6bcd\u7d30\u80de\u7624\u548c\u591a\u5668\u5b98\u96fb\u8166\u65b7\u5c64\u6383\u63cf\u5206\u5272\u4efb\u52d9\u4e0a\u7684\u91cf\u5316\u548c\u5b9a\u6027\u7d50\u679c\uff0c\u8b49\u660e\u4e86 GLIMS \u5728\u8907\u96dc\u6027\u548c\u6e96\u78ba\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002GLIMS \u5728 BraTS2021 \u548c BTCV \u8cc7\u6599\u96c6\u4e0a\u5c55\u73fe\u51fa\u5091\u51fa\u7684\u6548\u80fd\uff0c\u8d85\u8d8a\u4e86 Swin UNETR \u7684\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cGLIMS \u4ee5\u5927\u5e45\u6e1b\u5c11\u7684\u53ef\u8a13\u7df4\u53c3\u6578\u6578\u91cf\u9054\u5230\u4e86\u6b64\u9ad8\u6548\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cGLIMS \u6709 47.16M \u500b\u53ef\u8a13\u7df4\u53c3\u6578\u548c 72.30G FLOP\uff0c\u800c Swin UNETR \u5247\u6709 61.98M \u500b\u53ef\u8a13\u7df4\u53c3\u6578\u548c 394.84G FLOP\u3002\u7a0b\u5f0f\u78bc\u5df2\u65bc https://github.com/yaziciz/GLIMS \u516c\u958b\u3002", "author": "Ziya Ata Yaz\u0131c\u0131 et.al.", "authors": "Ziya Ata Yaz\u0131c\u0131, \u0130lkay \u00d6ks\u00fcz, Haz\u0131m Kemal Ekenel", "id": "2404.17854v1", "paper_url": "http://arxiv.org/abs/2404.17854v1", "repo": "https://github.com/yaziciz/GLIMS"}}