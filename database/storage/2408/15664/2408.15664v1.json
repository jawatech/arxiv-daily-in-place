{"2408.15664": {"publish_time": "2024-08-28", "title": "Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts", "paper_summary": "For Mixture-of-Experts (MoE) models, an unbalanced expert load will lead to\nrouting collapse or increased computational overhead. Existing methods commonly\nemploy an auxiliary loss to encourage load balance, but a large auxiliary loss\nwill introduce non-negligible interference gradients into training and thus\nimpair the model performance. In order to control load balance while not\nproducing undesired gradients during training, we propose Loss-Free Balancing,\nfeatured by an auxiliary-loss-free load balancing strategy. To be specific,\nbefore the top-K routing decision, Loss-Free Balancing will first apply an\nexpert-wise bias to the routing scores of each expert. By dynamically updating\nthe bias of each expert according to its recent load, Loss-Free Balancing can\nconsistently maintain a balanced distribution of expert load. In addition,\nsince Loss-Free Balancing does not produce any interference gradients, it also\nelevates the upper bound of model performance gained from MoE training. We\nvalidate the performance of Loss-Free Balancing on MoE models with up to 3B\nparameters trained on up to 200B tokens. Experimental results show that\nLoss-Free Balancing achieves both better performance and better load balance\ncompared with traditional auxiliary-loss-controlled load balancing strategies.", "paper_summary_zh": "\u5c0d\u65bc Mixture-of-Experts (MoE) \u6a21\u578b\uff0c\u4e0d\u5e73\u8861\u7684\u5c08\u5bb6\u8ca0\u8f09\u5c07\u5c0e\u81f4\u8def\u7531\u5d29\u6f70\u6216\u589e\u52a0\u8a08\u7b97\u958b\u92b7\u3002\u73fe\u6709\u65b9\u6cd5\u901a\u5e38\u63a1\u7528\u8f14\u52a9\u640d\u5931\u4f86\u9f13\u52f5\u8ca0\u8f09\u5e73\u8861\uff0c\u4f46\u5927\u7684\u8f14\u52a9\u640d\u5931\u6703\u5728\u8a13\u7df4\u4e2d\u5f15\u5165\u4e0d\u53ef\u5ffd\u7565\u7684\u5e72\u64fe\u68af\u5ea6\uff0c\u5f9e\u800c\u640d\u5bb3\u6a21\u578b\u6548\u80fd\u3002\u70ba\u4e86\u5728\u8a13\u7df4\u671f\u9593\u63a7\u5236\u8ca0\u8f09\u5e73\u8861\uff0c\u540c\u6642\u4e0d\u7522\u751f\u4e0d\u5fc5\u8981\u7684\u68af\u5ea6\uff0c\u6211\u5011\u63d0\u51fa\u7121\u640d\u5931\u5e73\u8861\uff0c\u5176\u7279\u9ede\u662f\u7121\u8f14\u52a9\u640d\u5931\u8ca0\u8f09\u5e73\u8861\u7b56\u7565\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5728\u9802\u90e8 K \u8def\u7531\u6c7a\u7b56\u4e4b\u524d\uff0c\u7121\u640d\u5931\u5e73\u8861\u5c07\u9996\u5148\u5c0d\u6bcf\u500b\u5c08\u5bb6\u7684\u8def\u7531\u5206\u6578\u61c9\u7528\u5c08\u5bb6\u504f\u5dee\u3002\u900f\u904e\u6839\u64da\u6bcf\u500b\u5c08\u5bb6\u7684\u6700\u65b0\u8ca0\u8f09\u52d5\u614b\u66f4\u65b0\u504f\u5dee\uff0c\u7121\u640d\u5931\u5e73\u8861\u53ef\u4ee5\u6301\u7e8c\u7dad\u6301\u5c08\u5bb6\u8ca0\u8f09\u7684\u5e73\u8861\u5206\u4f48\u3002\u6b64\u5916\uff0c\u7531\u65bc\u7121\u640d\u5931\u5e73\u8861\u4e0d\u6703\u7522\u751f\u4efb\u4f55\u5e72\u64fe\u68af\u5ea6\uff0c\u56e0\u6b64\u5b83\u4e5f\u63d0\u5347\u4e86\u5f9e MoE \u8a13\u7df4\u4e2d\u7372\u5f97\u7684\u6a21\u578b\u6548\u80fd\u7684\u4e0a\u9650\u3002\u6211\u5011\u5728\u8a13\u7df4\u591a\u9054 200B \u500b\u4ee3\u5e63\u7684\u591a\u9054 3B \u500b\u53c3\u6578\u7684 MoE \u6a21\u578b\u4e0a\u9a57\u8b49\u4e86\u7121\u640d\u5931\u5e73\u8861\u7684\u6548\u80fd\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207\u50b3\u7d71\u7684\u8f14\u52a9\u640d\u5931\u63a7\u5236\u8ca0\u8f09\u5e73\u8861\u7b56\u7565\u76f8\u6bd4\uff0c\u7121\u640d\u5931\u5e73\u8861\u5be6\u73fe\u4e86\u66f4\u597d\u7684\u6548\u80fd\u548c\u8ca0\u8f09\u5e73\u8861\u3002", "author": "Lean Wang et.al.", "authors": "Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, Damai Dai", "id": "2408.15664v1", "paper_url": "http://arxiv.org/abs/2408.15664v1", "repo": "null"}}