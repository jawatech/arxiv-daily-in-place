{"2501.04568": {"publish_time": "2025-01-08", "title": "Supervision-free Vision-Language Alignment", "paper_summary": "Vision-language models (VLMs) have demonstrated remarkable potential in\nintegrating visual and linguistic information, but their performance is often\nconstrained by the need for extensive, high-quality image-text training data.\nCuration of these image-text pairs is both time-consuming and computationally\nexpensive. To address this challenge, we introduce SVP (Supervision-free Visual\nProjection), a novel framework that enhances vision-language alignment without\nrelying on curated data or preference annotation. SVP leverages self-captioning\nand a pre-trained grounding model as a feedback mechanism to elicit latent\ninformation in VLMs. We evaluate our approach across six key areas: captioning,\nreferring, visual question answering, multitasking, hallucination control, and\nobject recall. Results demonstrate significant improvements, including a 14%\naverage improvement in captioning tasks, up to 12% increase in object recall,\nand substantial reduction in hallucination rates. Notably, a small VLM using\nSVP achieves hallucination reductions comparable to a model five times larger,\nwhile a VLM with initially poor referring capabilities more than doubles its\nperformance, approaching parity with a model twice its size.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u6574\u5408\u8996\u89ba\u548c\u8a9e\u8a00\u8cc7\u8a0a\u65b9\u9762\u5c55\u73fe\u4e86\u975e\u51e1\u7684\u6f5b\u529b\uff0c\u4f46\u5176\u6548\u80fd\u5f80\u5f80\u53d7\u5230\u5ee3\u6cdb\u3001\u9ad8\u54c1\u8cea\u7684\u5f71\u50cf\u6587\u5b57\u8a13\u7df4\u8cc7\u6599\u9700\u6c42\u7684\u9650\u5236\u3002\u6574\u7406\u9019\u4e9b\u5f71\u50cf\u6587\u5b57\u5c0d\u65e2\u8017\u6642\u53c8\u8017\u8cbb\u904b\u7b97\u8cc7\u6e90\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u9805\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86 SVP\uff08\u7121\u76e3\u7763\u8996\u89ba\u6295\u5f71\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u5b83\u589e\u5f37\u4e86\u8996\u89ba\u8a9e\u8a00\u5c0d\u9f4a\uff0c\u800c\u7121\u9700\u4f9d\u8cf4\u6574\u7406\u8cc7\u6599\u6216\u504f\u597d\u8a3b\u89e3\u3002SVP \u5229\u7528\u81ea\u6211\u6a19\u984c\u548c\u9810\u8a13\u7df4\u7684\u57fa\u790e\u6a21\u578b\u4f5c\u70ba\u4e00\u7a2e\u56de\u994b\u6a5f\u5236\uff0c\u4ee5\u5f15\u51fa VLM \u4e2d\u7684\u6f5b\u5728\u8cc7\u8a0a\u3002\u6211\u5011\u5728\u516d\u500b\u95dc\u9375\u9818\u57df\u8a55\u4f30\u4e86\u6211\u5011\u7684\u65b9\u6cd5\uff1a\u6a19\u984c\u3001\u6307\u6d89\u3001\u8996\u89ba\u554f\u984c\u89e3\u7b54\u3001\u591a\u5de5\u8655\u7406\u3001\u5e7b\u89ba\u63a7\u5236\u548c\u7269\u4ef6\u56de\u61b6\u3002\u7d50\u679c\u986f\u793a\u51fa\u986f\u8457\u7684\u9032\u6b65\uff0c\u5305\u62ec\u6a19\u984c\u4efb\u52d9\u5e73\u5747\u9032\u6b65 14%\uff0c\u7269\u4ef6\u56de\u61b6\u589e\u52a0\u591a\u9054 12%\uff0c\u4ee5\u53ca\u5e7b\u89ba\u7387\u5927\u5e45\u964d\u4f4e\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4f7f\u7528 SVP \u7684\u5c0f\u578b VLM \u9054\u5230\u7684\u5e7b\u89ba\u6e1b\u5c11\u91cf\u53ef\u8207\u5927\u4e94\u500d\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u800c\u6700\u521d\u6307\u6d89\u80fd\u529b\u8f03\u5dee\u7684 VLM \u5247\u5c07\u5176\u6548\u80fd\u63d0\u9ad8\u4e86\u4e00\u500d\u4ee5\u4e0a\uff0c\u63a5\u8fd1\u65bc\u5176\u5169\u500d\u5927\u5c0f\u7684\u6a21\u578b\u3002", "author": "Giorgio Giannone et.al.", "authors": "Giorgio Giannone, Ruoteng Li, Qianli Feng, Evgeny Perevodchikov, Rui Chen, Aleix Martinez", "id": "2501.04568v1", "paper_url": "http://arxiv.org/abs/2501.04568v1", "repo": "null"}}