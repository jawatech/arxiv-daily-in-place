{"2501.04155": {"publish_time": "2025-01-07", "title": "MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation", "paper_summary": "Vision-language models (VLMs) are highly effective but often underperform on\nspecialized tasks; for example, Llava-1.5 struggles with chart and diagram\nunderstanding due to scarce task-specific training data. Existing training\ndata, sourced from general-purpose datasets, fails to capture the nuanced\ndetails needed for these tasks. We introduce MM-Gen, a scalable method that\ngenerates task-specific, high-quality synthetic text for candidate images by\nleveraging stronger models. MM-Gen employs a three-stage targeted process:\npartitioning data into subgroups, generating targeted text based on task\ndescriptions, and filtering out redundant and outlier data. Fine-tuning VLMs\nwith data generated by MM-Gen leads to significant performance gains, including\n29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B).\nCompared to human-curated caption data, MM-Gen achieves up to 1.6x better\nimprovements for the original models, proving its effectiveness in enhancing\ntask-specific VLM performance and bridging the gap between general-purpose\ndatasets and specialized requirements. Code available at\nhttps://github.com/sjoshi804/MM-Gen.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u975e\u5e38\u6709\u6548\uff0c\u4f46\u901a\u5e38\u5728\u5c08\u9580\u4efb\u52d9\u4e0a\u8868\u73fe\u4e0d\u4f73\uff1b\u4f8b\u5982\uff0c\u7531\u65bc\u7f3a\u4e4f\u7279\u5b9a\u4efb\u52d9\u7684\u8a13\u7df4\u8cc7\u6599\uff0cLlava-1.5 \u5728\u7406\u89e3\u5716\u8868\u548c\u5716\u89e3\u6642\u6703\u9047\u5230\u56f0\u96e3\u3002\u73fe\u6709\u7684\u8a13\u7df4\u8cc7\u6599\u4f86\u81ea\u4e00\u822c\u7528\u9014\u7684\u8cc7\u6599\u96c6\uff0c\u7121\u6cd5\u64f7\u53d6\u9019\u4e9b\u4efb\u52d9\u6240\u9700\u7684\u7d30\u5fae\u7d30\u7bc0\u3002\u6211\u5011\u5f15\u5165\u4e86 MM-Gen\uff0c\u9019\u662f\u4e00\u7a2e\u53ef\u64f4\u5145\u7684\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u66f4\u5f37\u5927\u7684\u6a21\u578b\u70ba\u5019\u9078\u5f71\u50cf\u7522\u751f\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u9ad8\u54c1\u8cea\u5408\u6210\u6587\u5b57\u3002MM-Gen \u63a1\u7528\u4e00\u500b\u4e09\u968e\u6bb5\u76ee\u6a19\u6d41\u7a0b\uff1a\u5c07\u8cc7\u6599\u5206\u5272\u6210\u5b50\u7fa4\u7d44\u3001\u6839\u64da\u4efb\u52d9\u8aaa\u660e\u7522\u751f\u76ee\u6a19\u6587\u5b57\uff0c\u4e26\u904e\u6ffe\u6389\u591a\u9918\u548c\u7570\u5e38\u8cc7\u6599\u3002\u4f7f\u7528 MM-Gen \u7522\u751f\u7684\u8cc7\u6599\u5fae\u8abf VLM \u6703\u5e36\u4f86\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u5305\u62ec Llava-1.5 (7B) \u7684\u7a7a\u9593\u63a8\u7406\u63d0\u5347 29%\uff0c\u5716\u89e3\u7406\u89e3\u63d0\u5347 15%\u3002\u8207\u4eba\u5de5\u7b56\u5c55\u7684\u6a19\u984c\u8cc7\u6599\u76f8\u6bd4\uff0cMM-Gen \u70ba\u539f\u59cb\u6a21\u578b\u5be6\u73fe\u4e86\u9ad8\u9054 1.6 \u500d\u7684\u6539\u9032\uff0c\u8b49\u660e\u4e86\u5176\u5728\u589e\u5f37\u7279\u5b9a\u4efb\u52d9\u7684 VLM \u6548\u80fd\u548c\u5f4c\u5408\u4e00\u822c\u7528\u9014\u8cc7\u6599\u96c6\u8207\u7279\u5b9a\u9700\u6c42\u4e4b\u9593\u7684\u5dee\u8ddd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/sjoshi804/MM-Gen \u53d6\u5f97\u3002", "author": "Siddharth Joshi et.al.", "authors": "Siddharth Joshi, Besmira Nushi, Vidhisha Balachandran, Varun Chandrasekaran, Vibhav Vineet, Neel Joshi, Baharan Mirzasoleiman", "id": "2501.04155v1", "paper_url": "http://arxiv.org/abs/2501.04155v1", "repo": "https://github.com/sjoshi804/mm-gen"}}