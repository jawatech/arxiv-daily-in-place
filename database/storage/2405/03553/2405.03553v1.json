{"2405.03553": {"publish_time": "2024-05-06", "title": "AlphaMath Almost Zero: process Supervision without process", "paper_summary": "Recent advancements in large language models (LLMs) have substantially\nenhanced their mathematical reasoning abilities. However, these models still\nstruggle with complex problems that require multiple reasoning steps,\nfrequently leading to logical or numerical errors. While numerical mistakes can\nlargely be addressed by integrating a code interpreter, identifying logical\nerrors within intermediate steps is more challenging. Moreover, manually\nannotating these steps for training is not only expensive but also demands\nspecialized expertise. In this study, we introduce an innovative approach that\neliminates the need for manual annotation by leveraging the Monte Carlo Tree\nSearch (MCTS) framework to generate both the process supervision and evaluation\nsignals automatically. Essentially, when a LLM is well pre-trained, only the\nmathematical questions and their final answers are required to generate our\ntraining data, without requiring the solutions. We proceed to train a\nstep-level value model designed to improve the LLM's inference process in\nmathematical domains. Our experiments indicate that using automatically\ngenerated solutions by LLMs enhanced with MCTS significantly improves the\nmodel's proficiency in dealing with intricate mathematical reasoning tasks.", "paper_summary_zh": "", "author": "Guoxin Chen et.al.", "authors": "Guoxin Chen,Minpeng Liao,Chengxi Li,Kai Fan", "id": "2405.03553v1", "paper_url": "http://arxiv.org/abs/2405.03553v1", "repo": "null"}}