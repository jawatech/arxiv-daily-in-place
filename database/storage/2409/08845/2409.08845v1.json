{"2409.08845": {"publish_time": "2024-09-13", "title": "AIPO: Improving Training Objective for Iterative Preference Optimization", "paper_summary": "Preference Optimization (PO), is gaining popularity as an alternative choice\nof Proximal Policy Optimization (PPO) for aligning Large Language Models\n(LLMs). Recent research on aligning LLMs iteratively with synthetic or\npartially synthetic data shows promising results in scaling up PO training for\nboth academic settings and proprietary trained models such as Llama3. Despite\nits success, our study shows that the length exploitation issue present in PO\nis even more severe in Iterative Preference Optimization (IPO) due to the\niterative nature of the process. In this work, we study iterative preference\noptimization with synthetic data. We share the findings and analysis along the\nway of building the iterative preference optimization pipeline. More\nspecifically, we discuss the length exploitation issue during iterative\npreference optimization and propose our training objective for iterative\npreference optimization, namely Agreement-aware Iterative Preference\nOptimization (AIPO). To demonstrate the effectiveness of our method, we conduct\ncomprehensive experiments and achieve state-of-the-art performance on MT-Bench,\nAlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints will\nbe made available at https://github.com/bytedance/AIPO.", "paper_summary_zh": "\u504f\u597d\u512a\u5316 (PO) \u6b63\u5728\u4f5c\u70ba\u8fd1\u7aef\u7b56\u7565\u512a\u5316\u7684\u66ff\u4ee3\u9078\u64c7\u800c\u7372\u5f97\u666e\u53ca\uff0c\u7528\u65bc\u5c0d\u9f4a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u6700\u8fd1\u5c0d\u4f7f\u7528\u5408\u6210\u6216\u90e8\u5206\u5408\u6210\u6578\u64da\u8fed\u4ee3\u5c0d\u9f4a LLM \u7684\u7814\u7a76\u986f\u793a\uff0c\u5728\u64f4\u5c55 PO \u8a13\u7df4\u65b9\u9762\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6210\u679c\uff0c\u9069\u7528\u65bc\u5b78\u8853\u74b0\u5883\u548c\u5c08\u6709\u8a13\u7df4\u6a21\u578b\uff0c\u4f8b\u5982 Llama3\u3002\u5118\u7ba1\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u6211\u5011\u7684\u7814\u7a76\u8868\u660e\uff0c\u7531\u65bc\u904e\u7a0b\u7684\u8fed\u4ee3\u6027\u8cea\uff0c\u5728\u8fed\u4ee3\u504f\u597d\u512a\u5316 (IPO) \u4e2d\uff0cPO \u4e2d\u5b58\u5728\u7684\u9577\u5ea6\u5229\u7528\u554f\u984c\u751a\u81f3\u66f4\u52a0\u56b4\u91cd\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86\u4f7f\u7528\u5408\u6210\u6578\u64da\u9032\u884c\u8fed\u4ee3\u504f\u597d\u512a\u5316\u3002\u6211\u5011\u5728\u5efa\u7acb\u8fed\u4ee3\u504f\u597d\u512a\u5316\u7ba1\u9053\u7684\u904e\u7a0b\u4e2d\u5206\u4eab\u4e86\u767c\u73fe\u548c\u5206\u6790\u3002\u66f4\u5177\u9ad4\u5730\u8aaa\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86\u5728\u8fed\u4ee3\u504f\u597d\u512a\u5316\u671f\u9593\u7684\u9577\u5ea6\u5229\u7528\u554f\u984c\uff0c\u4e26\u63d0\u51fa\u4e86\u6211\u5011\u7528\u65bc\u8fed\u4ee3\u504f\u597d\u512a\u5316\u7684\u8a13\u7df4\u76ee\u6a19\uff0c\u5373\u57fa\u65bc\u5354\u8b70\u7684\u8fed\u4ee3\u504f\u597d\u512a\u5316 (AIPO)\u3002\u70ba\u4e86\u8b49\u660e\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u9032\u884c\u4e86\u5168\u9762\u7684\u5be6\u9a57\uff0c\u4e26\u5728 MT-Bench\u3001AlpacaEval 2.0 \u548c Arena-Hard \u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u6027\u80fd\u3002\u6211\u5011\u7684\u5be6\u4f5c\u548c\u6a21\u578b\u6aa2\u67e5\u9ede\u5c07\u5728 https://github.com/bytedance/AIPO \u4e0a\u63d0\u4f9b\u3002", "author": "Yaojie Shen et.al.", "authors": "Yaojie Shen, Xinyao Wang, Yulei Niu, Ying Zhou, Lexin Tang, Libo Zhang, Fan Chen, Longyin Wen", "id": "2409.08845v1", "paper_url": "http://arxiv.org/abs/2409.08845v1", "repo": "null"}}