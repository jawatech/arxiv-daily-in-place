{"2411.18207": {"publish_time": "2024-11-27", "title": "From Open Vocabulary to Open World: Teaching Vision Language Models to Detect Novel Objects", "paper_summary": "Traditional object detection methods operate under the closed-set assumption,\nwhere models can only detect a fixed number of objects predefined in the\ntraining set. Recent works on open vocabulary object detection (OVD) enable the\ndetection of objects defined by an unbounded vocabulary, which reduces the cost\nof training models for specific tasks. However, OVD heavily relies on accurate\nprompts provided by an ''oracle'', which limits their use in critical\napplications such as driving scene perception. OVD models tend to misclassify\nnear-out-of-distribution (NOOD) objects that have similar semantics to known\nclasses, and ignore far-out-of-distribution (FOOD) objects. To address theses\nlimitations, we propose a framework that enables OVD models to operate in open\nworld settings, by identifying and incrementally learning novel objects. To\ndetect FOOD objects, we propose Open World Embedding Learning (OWEL) and\nintroduce the concept of Pseudo Unknown Embedding which infers the location of\nunknown classes in a continuous semantic space based on the information of\nknown classes. We also propose Multi-Scale Contrastive Anchor Learning (MSCAL),\nwhich enables the identification of misclassified unknown objects by promoting\nthe intra-class consistency of object embeddings at different scales. The\nproposed method achieves state-of-the-art performance in common open world\nobject detection and autonomous driving benchmarks.", "paper_summary_zh": "\u50b3\u7d71\u7684\u7269\u4ef6\u5075\u6e2c\u65b9\u6cd5\u5728\u5c01\u9589\u96c6\u5408\u5047\u8a2d\u4e0b\u904b\u4f5c\uff0c\u5176\u4e2d\u6a21\u578b\u53ea\u80fd\u5075\u6e2c\u8a13\u7df4\u96c6\u4e2d\u9810\u5148\u5b9a\u7fa9\u7684\u56fa\u5b9a\u6578\u91cf\u7269\u4ef6\u3002\u8fd1\u671f\u95dc\u65bc\u958b\u653e\u8a5e\u5f59\u7269\u4ef6\u5075\u6e2c (OVD) \u7684\u7814\u7a76\u80fd\u5075\u6e2c\u7531\u4e0d\u53d7\u9650\u8a5e\u5f59\u5b9a\u7fa9\u7684\u7269\u4ef6\uff0c\u9019\u6e1b\u5c11\u4e86\u8a13\u7df4\u7279\u5b9a\u4efb\u52d9\u6a21\u578b\u7684\u6210\u672c\u3002\u7136\u800c\uff0cOVD \u56b4\u91cd\u4f9d\u8cf4\u7531\u300c\u795e\u8aed\u300d\u63d0\u4f9b\u7684\u6e96\u78ba\u63d0\u793a\uff0c\u9019\u9650\u5236\u4e86\u5b83\u5011\u5728\u95dc\u9375\u61c9\u7528\u4e2d\u7684\u4f7f\u7528\uff0c\u4f8b\u5982\u99d5\u99db\u5834\u666f\u611f\u77e5\u3002OVD \u6a21\u578b\u50be\u5411\u65bc\u5c07\u8fd1\u4e4e\u8d85\u51fa\u5206\u4f48 (NOOD) \u7684\u7269\u4ef6\u932f\u8aa4\u5206\u985e\u70ba\u8207\u5df2\u77e5\u985e\u5225\u5177\u6709\u985e\u4f3c\u8a9e\u610f\u7684\u7269\u4ef6\uff0c\u4e26\u5ffd\u7565\u9060\u8d85\u51fa\u5206\u4f48 (FOOD) \u7684\u7269\u4ef6\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u6846\u67b6\uff0c\u8b93 OVD \u6a21\u578b\u80fd\u5920\u5728\u958b\u653e\u4e16\u754c\u8a2d\u5b9a\u4e2d\u904b\u4f5c\uff0c\u65b9\u6cd5\u662f\u8b58\u5225\u4e26\u9010\u6b65\u5b78\u7fd2\u65b0\u7a4e\u7269\u4ef6\u3002\u70ba\u4e86\u5075\u6e2c FOOD \u7269\u4ef6\uff0c\u6211\u5011\u63d0\u51fa\u958b\u653e\u4e16\u754c\u5d4c\u5165\u5f0f\u5b78\u7fd2 (OWEL)\uff0c\u4e26\u5f15\u5165\u507d\u672a\u77e5\u5d4c\u5165\u7684\u6982\u5ff5\uff0c\u8a72\u6982\u5ff5\u6839\u64da\u5df2\u77e5\u985e\u5225\u7684\u8cc7\u8a0a\uff0c\u63a8\u8ad6\u51fa\u672a\u77e5\u985e\u5225\u5728\u9023\u7e8c\u8a9e\u7fa9\u7a7a\u9593\u4e2d\u7684\u4f4d\u7f6e\u3002\u6211\u5011\u4e5f\u63d0\u51fa\u591a\u5c3a\u5ea6\u5c0d\u6bd4\u9328\u5b78\u7fd2 (MSCAL)\uff0c\u5b83\u80fd\u900f\u904e\u63d0\u5347\u4e0d\u540c\u5c3a\u5ea6\u7269\u4ef6\u5d4c\u5165\u7684\u985e\u5167\u4e00\u81f4\u6027\uff0c\u4f86\u8b58\u5225\u932f\u8aa4\u5206\u985e\u7684\u672a\u77e5\u7269\u4ef6\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e38\u898b\u7684\u958b\u653e\u4e16\u754c\u7269\u4ef6\u5075\u6e2c\u548c\u81ea\u52d5\u99d5\u99db\u57fa\u6e96\u4e2d\uff0c\u9054\u6210\u6700\u5148\u9032\u7684\u6548\u80fd\u3002", "author": "Zizhao Li et.al.", "authors": "Zizhao Li, Zhengkang Xiang, Joseph West, Kourosh Khoshelham", "id": "2411.18207v1", "paper_url": "http://arxiv.org/abs/2411.18207v1", "repo": "https://github.com/343gltysprk/ovow"}}