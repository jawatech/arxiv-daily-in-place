{"2408.13678": {"publish_time": "2024-08-24", "title": "A layer-wise analysis of Mandarin and English suprasegmentals in SSL speech models", "paper_summary": "This study asks how self-supervised speech models represent suprasegmental\ncategories like Mandarin lexical tone, English lexical stress, and English\nphrasal accents. Through a series of probing tasks, we make layer-wise\ncomparisons of English and Mandarin 12 layer monolingual models. Our findings\nsuggest that 1) English and Mandarin wav2vec 2.0 models learn contextual\nrepresentations of abstract suprasegmental categories which are strongest in\nthe middle third of the network. 2) Models are better at representing features\nthat exist in the language of their training data, and this difference is\ndriven by enriched context in transformer blocks, not local acoustic\nrepresentation. 3) Fine-tuned wav2vec 2.0 improves performance in later layers\ncompared to pre-trained models mainly for lexically contrastive features like\ntone and stress, 4) HuBERT and WavLM learn similar representations to wav2vec\n2.0, differing mainly in later layer performance. Our results extend previous\nunderstanding of how models represent suprasegmentals and offer new insights\ninto the language-specificity and contextual nature of these representations.", "paper_summary_zh": "\u672c\u7814\u7a76\u63a2\u8a0e\u81ea\u76e3\u7763\u8a9e\u97f3\u6a21\u578b\u5982\u4f55\u5448\u73fe\u8d85\u97f3\u6bb5\u7bc4\u7587\uff0c\u4f8b\u5982\u6f22\u8a9e\u8a5e\u8abf\u3001\u82f1\u8a9e\u8a5e\u5f59\u91cd\u97f3\u548c\u82f1\u8a9e\u7247\u8a9e\u91cd\u97f3\u3002\u900f\u904e\u4e00\u7cfb\u5217\u63a2\u6e2c\u4efb\u52d9\uff0c\u6211\u5011\u5c0d\u82f1\u8a9e\u548c\u6f22\u8a9e 12 \u5c64\u55ae\u8a9e\u6a21\u578b\u9032\u884c\u9010\u5c64\u6bd4\u8f03\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff1a1) \u82f1\u8a9e\u548c\u6f22\u8a9e wav2vec 2.0 \u6a21\u578b\u5b78\u7fd2\u62bd\u8c61\u8d85\u97f3\u6bb5\u7bc4\u7587\u7684\u8a9e\u5883\u8868\u793a\uff0c\u9019\u4e9b\u7bc4\u7587\u5728\u7db2\u8def\u7684\u4e2d\u9593\u4e09\u5206\u4e4b\u4e00\u8655\u6700\u5f37\u30022) \u6a21\u578b\u66f4\u64c5\u9577\u8868\u793a\u5176\u8a13\u7df4\u8cc7\u6599\u8a9e\u8a00\u4e2d\u5b58\u5728\u7684\u7279\u5fb5\uff0c\u800c\u9019\u7a2e\u5dee\u7570\u662f\u7531Transformer\u5340\u584a\u4e2d\u7684\u8c50\u5bcc\u8a9e\u5883\u63a8\u52d5\u7684\uff0c\u800c\u4e0d\u662f\u7531\u5c40\u90e8\u97f3\u8a0a\u8868\u793a\u63a8\u52d5\u30023) \u5fae\u8abf\u7684 wav2vec 2.0 \u6539\u5584\u4e86\u5f8c\u5c64\u7684\u6548\u80fd\uff0c\u8207\u9810\u5148\u8a13\u7df4\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u4e3b\u8981\u662f\u91dd\u5c0d\u97f3\u8abf\u548c\u91cd\u97f3\u7b49\u8a5e\u5f59\u5c0d\u6bd4\u7279\u5fb5\u30024) HuBERT \u548c WavLM \u5b78\u7fd2\u5230\u8207 wav2vec 2.0 \u985e\u4f3c\u7684\u8868\u793a\uff0c\u4e3b\u8981\u5dee\u7570\u5728\u65bc\u5f8c\u5c64\u6548\u80fd\u3002\u6211\u5011\u7684\u7d50\u679c\u64f4\u5c55\u4e86\u5148\u524d\u5c0d\u6a21\u578b\u5982\u4f55\u8868\u793a\u8d85\u97f3\u6bb5\u7684\u4e86\u89e3\uff0c\u4e26\u5c0d\u9019\u4e9b\u8868\u793a\u7684\u8a9e\u8a00\u7279\u7570\u6027\u548c\u8a9e\u5883\u6027\u8cea\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\u3002", "author": "Ant\u00f3n de la Fuente et.al.", "authors": "Ant\u00f3n de la Fuente, Dan Jurafsky", "id": "2408.13678v1", "paper_url": "http://arxiv.org/abs/2408.13678v1", "repo": "null"}}