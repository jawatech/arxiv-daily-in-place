{"2408.04093": {"publish_time": "2024-08-07", "title": "Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters", "paper_summary": "Self-attention is the core mathematical operation of modern transformer\narchitectures and is also a significant computational bottleneck due to its\nquadratic complexity in the sequence length. In this work, we derive the scalar\nenergy function whose gradient computes the self-attention block, thus\nelucidating the theoretical underpinnings of self-attention, providing a\nBayesian interpretation of the operation and linking it closely with\nenergy-based models such as Hopfield Networks. Moreover, due to this\nformulation, we discover that we can use efficient and optimized\nautomatic-differentiation techniques to derive a highly efficient Tree\nAttention algorithm to compute the gradient of the energy and hence\nself-attention. Our formulation reveals that the reduction across the sequence\naxis can be efficiently computed in parallel through a tree reduction. Our\nalgorithm, for parallelizing attention computation across multiple GPUs,\nenables cross-device decoding to be performed asymptotically faster (up to 8x\nfaster) than alternative approaches such as Ring Attention, while also\nrequiring significantly less communication volume and incurring 2x less peak\nmemory. Our code is publicly available here:\n\\url{https://github.com/Zyphra/tree_attention}", "paper_summary_zh": "\u81ea\u6211\u6ce8\u610f\u529b\u662f\u73fe\u4ee3Transformer\u67b6\u69cb\u7684\u6838\u5fc3\u6578\u5b78\u904b\u7b97\uff0c\u800c\u4e14\u7531\u65bc\u5176\u5728\u5e8f\u5217\u9577\u5ea6\u4e2d\u7684\u4e8c\u6b21\u8907\u96dc\u6027\uff0c\u5b83\u4e5f\u662f\u4e00\u500b\u91cd\u8981\u7684\u8a08\u7b97\u74f6\u9838\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a8\u5c0e\u51fa\u6a19\u91cf\u80fd\u91cf\u51fd\u6578\uff0c\u5176\u68af\u5ea6\u8a08\u7b97\u81ea\u6211\u6ce8\u610f\u529b\u5340\u584a\uff0c\u5f9e\u800c\u95e1\u660e\u81ea\u6211\u6ce8\u610f\u529b\u7684\u7406\u8ad6\u57fa\u790e\uff0c\u63d0\u4f9b\u8a72\u904b\u7b97\u7684\u8c9d\u8449\u65af\u8a6e\u91cb\uff0c\u4e26\u5c07\u5176\u8207\u57fa\u65bc\u80fd\u91cf\u7684\u6a21\u578b\uff08\u4f8b\u5982\u970d\u666e\u83f2\u723e\u5fb7\u7db2\u8def\uff09\u7dca\u5bc6\u9023\u7d50\u3002\u6b64\u5916\uff0c\u7531\u65bc\u9019\u500b\u516c\u5f0f\uff0c\u6211\u5011\u767c\u73fe\u6211\u5011\u53ef\u4ee5\u4f7f\u7528\u9ad8\u6548\u4e14\u6700\u4f73\u5316\u7684\u81ea\u52d5\u5fae\u5206\u6280\u8853\u4f86\u63a8\u5c0e\u4e00\u500b\u9ad8\u6548\u7684\u6a39\u6ce8\u610f\u529b\u6f14\u7b97\u6cd5\uff0c\u4ee5\u8a08\u7b97\u80fd\u91cf\u7684\u68af\u5ea6\uff0c\u5f9e\u800c\u81ea\u6211\u6ce8\u610f\u3002\u6211\u5011\u7684\u516c\u5f0f\u63ed\u793a\u4e86\u5e8f\u5217\u8ef8\u4e0a\u7684\u7d04\u7c21\u53ef\u4ee5\u900f\u904e\u6a39\u7d04\u7c21\u6709\u6548\u5730\u4e26\u884c\u8a08\u7b97\u3002\u6211\u5011\u7684\u6f14\u7b97\u6cd5\uff0c\u7528\u65bc\u5728\u591a\u500b GPU \u4e0a\u4e26\u884c\u5316\u6ce8\u610f\u529b\u8a08\u7b97\uff0c\u4f7f\u8de8\u88dd\u7f6e\u89e3\u78bc\u80fd\u5920\u6bd4\u66ff\u4ee3\u65b9\u6cd5\uff08\u4f8b\u5982\u74b0\u5f62\u6ce8\u610f\u529b\uff09\u57f7\u884c\u5f97\u66f4\u5feb\uff08\u5feb 8 \u500d\uff09\uff0c\u540c\u6642\u4e5f\u9700\u8981\u986f\u8457\u6e1b\u5c11\u901a\u8a0a\u91cf\uff0c\u4e26\u6e1b\u5c11 2 \u500d\u7684\u5cf0\u503c\u8a18\u61b6\u9ad4\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5728\u6b64\u516c\u958b\uff1a\n\\url{https://github.com/Zyphra/tree_attention}", "author": "Vasudev Shyam et.al.", "authors": "Vasudev Shyam, Jonathan Pilault, Emily Shepperd, Quentin Anthony, Beren Millidge", "id": "2408.04093v1", "paper_url": "http://arxiv.org/abs/2408.04093v1", "repo": "null"}}