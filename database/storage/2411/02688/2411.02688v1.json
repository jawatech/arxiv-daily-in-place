{"2411.02688": {"publish_time": "2024-11-05", "title": "On the loss of context-awareness in general instruction fine-tuning", "paper_summary": "Pretrained Large Language Models (LLMs) require post-training methods such as\nsupervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pretraining. In this paper, we investigate the loss\nof context awareness after SFT, defined as the capability to extract and\nunderstand information from the user-provided context and respond accordingly.\nWe are the first to identify and show that the loss of context-awareness\nappears on instruction-finetuned LLMs when the chat template is applied to the\ninput prompts. We identify the performance decline is partially caused by the\nbias embedded into the chat template to focus less on the user-provided\ncontext. Based on these observations, we propose two methods to mitigate the\nloss of context awareness in instruct models: post-hoc attention steering on\nuser prompts and conditional instruction fine-tuning with a context-dependency\nindicator. Empirical experiments on 4 context-dependent downstream tasks and 3\npretrained LLMs of different sizes show that our methods effectively mitigates\nthe loss of context awareness without compromising the general ability to\nfollow instructions. Our findings also strongly advocate the necessity to\ncarefully benchmark context awareness after instruction fine-tuning.", "paper_summary_zh": "\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9700\u8981\u5f8c\u8a13\u7df4\u65b9\u6cd5\uff0c\u4f8b\u5982\u5c0d\u6307\u4ee4\u56de\u61c9\u914d\u5c0d\u9032\u884c\u76e3\u7763\u5fae\u8abf (SFT)\uff0c\u4ee5\u5be6\u73fe\u6307\u4ee4\u9075\u5faa\u3002\u7136\u800c\uff0c\u6b64\u7a0b\u5e8f\u53ef\u80fd\u6703\u5c0d\u9810\u8a13\u7df4\u671f\u9593\u5b78\u7fd2\u5230\u7684\u73fe\u6709\u80fd\u529b\u9020\u6210\u6f5b\u5728\u50b7\u5bb3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86 SFT \u5f8c\u7684\u80cc\u666f\u8a8d\u77e5\u640d\u5931\uff0c\u5b9a\u7fa9\u70ba\u5f9e\u4f7f\u7528\u8005\u63d0\u4f9b\u7684\u80cc\u666f\u4e2d\u63d0\u53d6\u548c\u7406\u89e3\u8cc7\u8a0a\u4e26\u505a\u51fa\u76f8\u61c9\u56de\u61c9\u7684\u80fd\u529b\u3002\u6211\u5011\u7387\u5148\u767c\u73fe\u4e26\u5c55\u793a\u4e86\u7576\u804a\u5929\u7bc4\u672c\u5957\u7528\u65bc\u8f38\u5165\u63d0\u793a\u6642\uff0c\u80cc\u666f\u8a8d\u77e5\u640d\u5931\u51fa\u73fe\u5728\u6307\u4ee4\u5fae\u8abf LLM \u4e0a\u3002\u6211\u5011\u767c\u73fe\u6548\u80fd\u4e0b\u964d\u90e8\u5206\u662f\u7531\u65bc\u804a\u5929\u7bc4\u672c\u4e2d\u5167\u5d4c\u7684\u504f\u5dee\uff0c\u8f03\u5c11\u95dc\u6ce8\u4f7f\u7528\u8005\u63d0\u4f9b\u7684\u80cc\u666f\u3002\u6839\u64da\u9019\u4e9b\u89c0\u5bdf\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5169\u7a2e\u65b9\u6cd5\u4f86\u6e1b\u8f15\u6307\u4ee4\u6a21\u578b\u4e2d\u80cc\u666f\u8a8d\u77e5\u7684\u640d\u5931\uff1a\u4f7f\u7528\u8005\u63d0\u793a\u7684\u5f8c\u8a2d\u6ce8\u610f\u5c0e\u5411\u4ee5\u53ca\u5177\u6709\u80cc\u666f\u4f9d\u8cf4\u6027\u6307\u6a19\u7684\u689d\u4ef6\u6307\u4ee4\u5fae\u8abf\u3002\u5728 4 \u500b\u80cc\u666f\u4f9d\u8cf4\u7684\u4e0b\u6e38\u4efb\u52d9\u548c 3 \u500b\u4e0d\u540c\u5927\u5c0f\u7684\u9810\u8a13\u7df4 LLM \u4e0a\u9032\u884c\u7684\u7d93\u9a57\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u6709\u6548\u5730\u6e1b\u8f15\u4e86\u80cc\u666f\u8a8d\u77e5\u7684\u640d\u5931\uff0c\u540c\u6642\u4e0d\u640d\u5bb3\u9075\u5faa\u6307\u4ee4\u7684\u4e00\u822c\u80fd\u529b\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u4e5f\u5f37\u70c8\u4e3b\u5f35\u5728\u6307\u4ee4\u5fae\u8abf\u5f8c\u4ed4\u7d30\u8a55\u91cf\u80cc\u666f\u8a8d\u77e5\u7684\u5fc5\u8981\u6027\u3002", "author": "Yihan Wang et.al.", "authors": "Yihan Wang, Andrew Bai, Nanyun Peng, Cho-Jui Hsieh", "id": "2411.02688v1", "paper_url": "http://arxiv.org/abs/2411.02688v1", "repo": "https://github.com/YihanWang617/context_awareness"}}