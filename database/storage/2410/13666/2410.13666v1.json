{"2410.13666": {"publish_time": "2024-10-17", "title": "VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic Reasoning Tasks", "paper_summary": "Deriving inference from heterogeneous inputs (such as images, text, and\naudio) is an important skill for humans to perform day-to-day tasks. A similar\nability is desirable for the development of advanced Artificial Intelligence\n(AI) systems. While state-of-the-art models are rapidly closing the gap with\nhuman-level performance on diverse computer vision and NLP tasks separately,\nthey struggle to solve tasks that require joint reasoning over visual and\ntextual modalities. Inspired by GLUE (Wang et. al., 2018)- a multitask\nbenchmark for natural language understanding, we propose VL-GLUE in this paper.\nVL-GLUE consists of over 100k samples spanned across seven different tasks,\nwhich at their core require visuo-linguistic reasoning. Moreover, our benchmark\ncomprises of diverse image types (from synthetically rendered figures, and\nday-to-day scenes to charts and complex diagrams) and includes a broad variety\nof domain-specific text (from cooking, politics, and sports to high-school\ncurricula), demonstrating the need for multi-modal understanding in the\nreal-world. We show that this benchmark is quite challenging for existing\nlarge-scale vision-language models and encourage development of systems that\npossess robust visuo-linguistic reasoning capabilities.", "paper_summary_zh": "\u5f9e\u7570\u8cea\u8f38\u5165\uff08\u4f8b\u5982\u5f71\u50cf\u3001\u6587\u5b57\u548c\u97f3\u8a0a\uff09\u4e2d\u63a8\u8ad6\u662f\u4eba\u985e\u57f7\u884c\u65e5\u5e38\u4efb\u52d9\u7684\u4e00\u9805\u91cd\u8981\u6280\u80fd\u3002\u5148\u9032\u7684\u4eba\u5de5\u667a\u6167 (AI) \u7cfb\u7d71\u7684\u958b\u767c\u4e5f\u9700\u8981\u985e\u4f3c\u7684\u80fd\u529b\u3002\u96d6\u7136\u6700\u5148\u9032\u7684\u6a21\u578b\u5728\u4e0d\u540c\u7684\u96fb\u8166\u8996\u89ba\u548c NLP \u4efb\u52d9\u4e2d\u8fc5\u901f\u7e2e\u5c0f\u8207\u4eba\u985e\u8868\u73fe\u7684\u5dee\u8ddd\uff0c\u4f46\u5b83\u5011\u96e3\u4ee5\u89e3\u6c7a\u9700\u8981\u5c0d\u8996\u89ba\u548c\u6587\u5b57\u6a21\u5f0f\u9032\u884c\u806f\u5408\u63a8\u7406\u7684\u4efb\u52d9\u3002\u53d7\u5230 GLUE (Wang et. al., 2018) \u7684\u555f\u767c\uff0c\u9019\u662f\u4e00\u500b\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u7684\u591a\u4efb\u52d9\u57fa\u6e96\uff0c\u6211\u5011\u5728\u9019\u7bc7\u8ad6\u6587\u4e2d\u63d0\u51fa VL-GLUE\u3002VL-GLUE \u7531\u8d85\u904e 100k \u500b\u6a23\u672c\u7d44\u6210\uff0c\u6db5\u84cb\u4e03\u9805\u4e0d\u540c\u7684\u4efb\u52d9\uff0c\u9019\u4e9b\u4efb\u52d9\u7684\u6838\u5fc3\u9700\u8981\u8996\u89ba\u8a9e\u8a00\u63a8\u7406\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u57fa\u6e96\u5305\u542b\u5404\u7a2e\u5f71\u50cf\u985e\u578b\uff08\u5f9e\u5408\u6210\u6e32\u67d3\u7684\u4eba\u7269\u3001\u65e5\u5e38\u5834\u666f\u5230\u5716\u8868\u548c\u8907\u96dc\u7684\u5716\u8868\uff09\uff0c\u4e26\u5305\u542b\u5ee3\u6cdb\u7684\u9818\u57df\u7279\u5b9a\u6587\u5b57\uff08\u5f9e\u70f9\u98ea\u3001\u653f\u6cbb\u548c\u9ad4\u80b2\u5230\u9ad8\u4e2d\u8ab2\u7a0b\uff09\uff0c\u8b49\u660e\u4e86\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u5c0d\u591a\u6a21\u5f0f\u7406\u89e3\u7684\u9700\u6c42\u3002\u6211\u5011\u8b49\u660e\u9019\u500b\u57fa\u6e96\u5c0d\u65bc\u73fe\u6709\u7684\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u4f86\u8aaa\u975e\u5e38\u5177\u6709\u6311\u6230\u6027\uff0c\u4e26\u9f13\u52f5\u958b\u767c\u5177\u5099\u5f37\u5065\u8996\u89ba\u8a9e\u8a00\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7d71\u3002", "author": "Shailaja Keyur Sampat et.al.", "authors": "Shailaja Keyur Sampat, Mutsumi Nakamura, Shankar Kailas, Kartik Aggarwal, Mandy Zhou, Yezhou Yang, Chitta Baral", "id": "2410.13666v1", "paper_url": "http://arxiv.org/abs/2410.13666v1", "repo": "null"}}