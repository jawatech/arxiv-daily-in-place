{"2410.16662": {"publish_time": "2024-10-22", "title": "Visual Question Answering in Ophthalmology: A Progressive and Practical Perspective", "paper_summary": "Accurate diagnosis of ophthalmic diseases relies heavily on the\ninterpretation of multimodal ophthalmic images, a process often time-consuming\nand expertise-dependent. Visual Question Answering (VQA) presents a potential\ninterdisciplinary solution by merging computer vision and natural language\nprocessing to comprehend and respond to queries about medical images. This\nreview article explores the recent advancements and future prospects of VQA in\nophthalmology from both theoretical and practical perspectives, aiming to\nprovide eye care professionals with a deeper understanding and tools for\nleveraging the underlying models. Additionally, we discuss the promising trend\nof large language models (LLM) in enhancing various components of the VQA\nframework to adapt to multimodal ophthalmic tasks. Despite the promising\noutlook, ophthalmic VQA still faces several challenges, including the scarcity\nof annotated multimodal image datasets, the necessity of comprehensive and\nunified evaluation methods, and the obstacles to achieving effective real-world\napplications. This article highlights these challenges and clarifies future\ndirections for advancing ophthalmic VQA with LLMs. The development of LLM-based\nophthalmic VQA systems calls for collaborative efforts between medical\nprofessionals and AI experts to overcome existing obstacles and advance the\ndiagnosis and care of eye diseases.", "paper_summary_zh": "\u6e96\u78ba\u8a3a\u65b7\u773c\u79d1\u75be\u75c5\u4ef0\u8cf4\u5c0d\u591a\u6a21\u614b\u773c\u79d1\u5f71\u50cf\u7684\u89e3\u8b80\uff0c\u9019\u500b\u904e\u7a0b\u901a\u5e38\u8017\u6642\u4e14\u4f9d\u8cf4\u5c08\u696d\u77e5\u8b58\u3002\u8996\u89ba\u554f\u7b54\uff08VQA\uff09\u7d50\u5408\u96fb\u8166\u8996\u89ba\u548c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u6f5b\u5728\u7684\u8de8\u9818\u57df\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u7406\u89e3\u4e26\u56de\u7b54\u6709\u95dc\u91ab\u5b78\u5f71\u50cf\u7684\u7591\u554f\u3002\u9019\u7bc7\u8a55\u8ad6\u6587\u7ae0\u63a2\u8a0e\u4e86 VQA \u5728\u773c\u79d1\u9818\u57df\u7684\u6700\u65b0\u9032\u5c55\u548c\u672a\u4f86\u524d\u666f\uff0c\u5f9e\u7406\u8ad6\u548c\u5be6\u52d9\u7684\u89d2\u5ea6\u51fa\u767c\uff0c\u65e8\u5728\u70ba\u773c\u79d1\u4fdd\u5065\u5c08\u696d\u4eba\u54e1\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u7406\u89e3\u548c\u5de5\u5177\uff0c\u4ee5\u5229\u7528\u57fa\u790e\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u589e\u5f37 VQA \u67b6\u69cb\u5404\u7a2e\u7d44\u6210\u90e8\u5206\u4ee5\u9069\u61c9\u591a\u6a21\u614b\u773c\u79d1\u4efb\u52d9\u4e2d\u7684\u6709\u671b\u8da8\u52e2\u3002\u5118\u7ba1\u524d\u666f\u770b\u597d\uff0c\u773c\u79d1 VQA \u4ecd\u9762\u81e8\u6578\u9805\u6311\u6230\uff0c\u5305\u62ec\u6a19\u8a3b\u591a\u6a21\u614b\u5f71\u50cf\u8cc7\u6599\u96c6\u7684\u7a00\u5c11\u6027\u3001\u5168\u9762\u4e14\u7d71\u4e00\u7684\u8a55\u4f30\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u53ca\u5be6\u73fe\u6709\u6548\u5be6\u969b\u61c9\u7528\u6240\u9047\u5230\u7684\u969c\u7919\u3002\u672c\u6587\u91cd\u9ede\u8aaa\u660e\u4e86\u9019\u4e9b\u6311\u6230\uff0c\u4e26\u91d0\u6e05\u4e86\u5229\u7528 LLM \u63a8\u52d5\u773c\u79d1 VQA \u7684\u672a\u4f86\u65b9\u5411\u3002\u57fa\u65bc LLM \u7684\u773c\u79d1 VQA \u7cfb\u7d71\u7684\u958b\u767c\u9700\u8981\u91ab\u5b78\u5c08\u696d\u4eba\u54e1\u548c AI \u5c08\u5bb6\u5171\u540c\u52aa\u529b\uff0c\u4ee5\u514b\u670d\u73fe\u6709\u969c\u7919\u4e26\u63a8\u52d5\u773c\u79d1\u75be\u75c5\u7684\u8a3a\u65b7\u548c\u7167\u8b77\u3002", "author": "Xiaolan Chen et.al.", "authors": "Xiaolan Chen, Ruoyu Chen, Pusheng Xu, Weiyi Zhang, Xianwen Shang, Mingguang He, Danli Shi", "id": "2410.16662v1", "paper_url": "http://arxiv.org/abs/2410.16662v1", "repo": "null"}}