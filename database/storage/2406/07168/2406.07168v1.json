{"2406.07168": {"publish_time": "2024-06-11", "title": "Teaching Language Models to Self-Improve by Learning from Language Feedback", "paper_summary": "Aligning Large Language Models (LLMs) with human intentions and values is\ncrucial yet challenging. Current methods primarily rely on human preferences,\nwhich are costly and insufficient in capturing nuanced feedback expressed in\nnatural language. In this paper, we present Self-Refinement Tuning (SRT), a\nmethod that leverages model feedback for alignment, thereby reducing reliance\non human annotations. SRT uses a base language model (e.g., Tulu2) to generate\ninitial responses, which are critiqued and refined by a more advanced model\n(e.g., GPT-4-Turbo). This process enables the base model to self-evaluate and\nimprove its outputs, facilitating continuous learning. SRT further optimizes\nthe model by learning from its self-generated feedback and refinements,\ncreating a feedback loop that promotes model improvement. Our empirical\nevaluations demonstrate that SRT significantly outperforms strong baselines\nacross diverse tasks and model sizes. When applied to a 70B parameter model,\nSRT increases the win rate from 9.6\\% to 25.8\\% on the AlpacaEval 2.0\nbenchmark, surpassing well-established systems such as GPT-4-0314, Claude 2,\nand Gemini. Our analysis highlights the crucial role of language feedback in\nthe success of SRT, suggesting potential for further exploration in this\ndirection.", "paper_summary_zh": "\u8abf\u6574\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u7b26\u5408\u4eba\u985e\u610f\u5716\u548c\u50f9\u503c\u89c0\u81f3\u95dc\u91cd\u8981\u4e14\u5177\u6709\u6311\u6230\u6027\u3002\u76ee\u524d\u7684\u6280\u8853\u4e3b\u8981\u4f9d\u8cf4\u65bc\u4eba\u985e\u504f\u597d\uff0c\u800c\u9019\u5728\u6355\u6349\u81ea\u7136\u8a9e\u8a00\u4e2d\u8868\u9054\u7684\u7d30\u5fae\u53cd\u994b\u65b9\u9762\u65e2\u6602\u8cb4\u53c8\u4e0d\u8db3\u5920\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u81ea\u6211\u7cbe\u9032\u8abf\u6574 (SRT)\uff0c\u9019\u662f\u4e00\u7a2e\u5229\u7528\u6a21\u578b\u53cd\u994b\u9032\u884c\u8abf\u6574\u7684\u65b9\u6cd5\uff0c\u5f9e\u800c\u6e1b\u5c11\u5c0d\u4eba\u5de5\u8a3b\u89e3\u7684\u4f9d\u8cf4\u3002SRT \u4f7f\u7528\u57fa\u790e\u8a9e\u8a00\u6a21\u578b\uff08\u4f8b\u5982 Tulu2\uff09\u751f\u6210\u521d\u59cb\u56de\u61c9\uff0c\u9019\u4e9b\u56de\u61c9\u6703\u53d7\u5230\u66f4\u5148\u9032\u7684\u6a21\u578b\uff08\u4f8b\u5982 GPT-4-Turbo\uff09\u7684\u6279\u8a55\u548c\u6539\u9032\u3002\u6b64\u904e\u7a0b\u4f7f\u57fa\u790e\u6a21\u578b\u80fd\u5920\u81ea\u6211\u8a55\u4f30\u548c\u6539\u9032\u5176\u8f38\u51fa\uff0c\u4fc3\u9032\u6301\u7e8c\u5b78\u7fd2\u3002SRT \u9032\u4e00\u6b65\u901a\u904e\u5f9e\u5176\u81ea\u6211\u751f\u6210\u7684\u56de\u994b\u548c\u6539\u9032\u4e2d\u5b78\u7fd2\u4f86\u512a\u5316\u6a21\u578b\uff0c\u5f9e\u800c\u5efa\u7acb\u4e00\u500b\u4fc3\u9032\u6a21\u578b\u6539\u9032\u7684\u53cd\u994b\u8ff4\u8def\u3002\u6211\u5011\u7684\u7d93\u9a57\u8a55\u4f30\u8868\u660e\uff0cSRT \u5728\u4e0d\u540c\u7684\u4efb\u52d9\u548c\u6a21\u578b\u898f\u6a21\u4e0a\u90fd\u660e\u986f\u512a\u65bc\u5f37\u5927\u7684\u57fa\u6e96\u3002\u7576\u61c9\u7528\u65bc 70B \u53c3\u6578\u6a21\u578b\u6642\uff0cSRT \u5c07 AlpacaEval 2.0 \u57fa\u6e96\u4e0a\u7684\u7372\u52dd\u7387\u5f9e 9.6% \u63d0\u9ad8\u5230 25.8%\uff0c\u8d85\u8d8a\u4e86 GPT-4-0314\u3001Claude 2 \u548c Gemini \u7b49\u5b8c\u5584\u7684\u7cfb\u7d71\u3002\u6211\u5011\u7684\u5206\u6790\u5f37\u8abf\u4e86\u8a9e\u8a00\u53cd\u994b\u5728 SRT \u6210\u529f\u4e2d\u7684\u95dc\u9375\u4f5c\u7528\uff0c\u8868\u660e\u4e86\u9032\u4e00\u6b65\u63a2\u7d22\u6b64\u65b9\u5411\u7684\u6f5b\u529b\u3002", "author": "Chi Hu et.al.", "authors": "Chi Hu, Yimin Hu, Hang Cao, Tong Xiao, Jingbo Zhu", "id": "2406.07168v1", "paper_url": "http://arxiv.org/abs/2406.07168v1", "repo": "null"}}