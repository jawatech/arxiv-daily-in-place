{"2412.07171": {"publish_time": "2024-12-10", "title": "Breaking the Stage Barrier: A Novel Single-Stage Approach to Long Context Extension for Large Language Models", "paper_summary": "Recently, Large language models (LLMs) have revolutionized Natural Language\nProcessing (NLP). Pretrained LLMs, due to limited training context size,\nstruggle with handling long token sequences, limiting their performance on\nvarious downstream tasks. Current solutions toward long context modeling often\nemploy multi-stage continual pertaining, which progressively increases the\neffective context length through several continual pretraining stages. However,\nthose approaches require extensive manual tuning and human expertise. In this\npaper, we introduce a novel single-stage continual pretraining method,\nHead-Adaptive Rotary Position Encoding (HARPE), to equip LLMs with long context\nmodeling capabilities while simplifying the training process. Our HARPE\nleverages different Rotary Position Encoding (RoPE) base frequency values\nacross different attention heads and directly trains LLMs on the target context\nlength. Extensive experiments on 4 language modeling benchmarks, including the\nlatest RULER benchmark, demonstrate that HARPE excels in understanding and\nintegrating long-context tasks with single-stage training, matching and even\noutperforming existing multi-stage methods. Our results highlight that HARPE\nsuccessfully breaks the stage barrier for training LLMs with long context\nmodeling capabilities.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5f7b\u5e95\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP)\u3002\u7531\u4e8e\u8bad\u7ec3\u8bed\u5883\u5927\u5c0f\u6709\u9650\uff0c\u9884\u8bad\u7ec3 LLM \u96be\u4ee5\u5904\u7406\u957f\u6807\u8bb0\u5e8f\u5217\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u5f53\u524d\u9488\u5bf9\u957f\u8bed\u5883\u5efa\u6a21\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u91c7\u7528\u591a\u9636\u6bb5\u6301\u7eed\u8bad\u7ec3\uff0c\u901a\u8fc7\u51e0\u4e2a\u6301\u7eed\u9884\u8bad\u7ec3\u9636\u6bb5\u9010\u6b65\u589e\u52a0\u6709\u6548\u8bed\u5883\u957f\u5ea6\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u624b\u52a8\u8c03\u6574\u548c\u4eba\u529b\u4e13\u4e1a\u77e5\u8bc6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5355\u9636\u6bb5\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5373\u81ea\u9002\u5e94\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 (HARPE)\uff0c\u4ee5\u5728\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b\u7684\u540c\u65f6\u4e3a LLM \u63d0\u4f9b\u957f\u8bed\u5883\u5efa\u6a21\u80fd\u529b\u3002\u6211\u4eec\u7684 HARPE \u5229\u7528\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u7684\u4e0d\u540c\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 (RoPE) \u57fa\u672c\u9891\u7387\u503c\uff0c\u5e76\u5728\u76ee\u6807\u8bed\u5883\u957f\u5ea6\u4e0a\u76f4\u63a5\u8bad\u7ec3 LLM\u3002\u5728\u5305\u62ec\u6700\u65b0 RULER \u57fa\u51c6\u5728\u5185\u7684 4 \u4e2a\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHARPE \u5728\u7406\u89e3\u548c\u6574\u5408\u957f\u8bed\u5883\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u53ea\u9700\u5355\u9636\u6bb5\u8bad\u7ec3\uff0c\u5373\u53ef\u5339\u914d\u751a\u81f3\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u9636\u6bb5\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7ed3\u679c\u7a81\u51fa\u8868\u660e\uff0cHARPE \u6210\u529f\u6253\u7834\u4e86\u8bad\u7ec3\u5177\u6709\u957f\u8bed\u5883\u5efa\u6a21\u80fd\u529b\u7684 LLM \u7684\u9636\u6bb5\u969c\u788d\u3002", "author": "Haoran Lian et.al.", "authors": "Haoran Lian, Junmin Chen, Wei Huang, Yizhe Xiong, Wenping Hu, Guiguang Ding, Hui Chen, Jianwei Niu, Zijia Lin, Fuzheng Zhang, Di Zhang", "id": "2412.07171v1", "paper_url": "http://arxiv.org/abs/2412.07171v1", "repo": "null"}}