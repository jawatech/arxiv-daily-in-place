{"2409.16722": {"publish_time": "2024-09-25", "title": "PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning", "paper_summary": "Low-rank adaptation (LoRA) and its variants have recently gained much\ninterest due to their ability to avoid excessive inference costs. However, LoRA\nstill encounters the following challenges: (1) Limitation of low-rank\nassumption; and (2) Its initialization method may be suboptimal. To this end,\nwe propose PMSS(Pre-trained Matrices Skeleton Selection), which enables\nhigh-rank updates with low costs while leveraging semantic and linguistic\ninformation inherent in pre-trained weight. It achieves this by selecting\nskeletons from the pre-trained weight matrix and only learning a small matrix\ninstead. Experiments demonstrate that PMSS outperforms LoRA and other\nfine-tuning methods across tasks with much less trainable parameters. We\ndemonstrate its effectiveness, especially in handling complex tasks such as\nDROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math\nreasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of\nGSM8K). The code and model will be released soon.", "paper_summary_zh": "\u4f4e\u79e9\u9069\u61c9 (LoRA) \u53ca\u5176\u8b8a\u9ad4\u6700\u8fd1\u56e0\u5176\u907f\u514d\u904e\u5ea6\u63a8\u7406\u6210\u672c\u7684\u80fd\u529b\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u7136\u800c\uff0cLoRA \u4ecd\u9762\u81e8\u4ee5\u4e0b\u6311\u6230\uff1a(1) \u4f4e\u79e9\u5047\u8a2d\u7684\u9650\u5236\uff1b\u4ee5\u53ca (2) \u5176\u521d\u59cb\u5316\u65b9\u6cd5\u53ef\u80fd\u6b21\u512a\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa PMSS\uff08\u9810\u8a13\u7df4\u77e9\u9663\u9aa8\u67b6\u9078\u64c7\uff09\uff0c\u5b83\u80fd\u5920\u4ee5\u4f4e\u6210\u672c\u9032\u884c\u9ad8\u79e9\u66f4\u65b0\uff0c\u540c\u6642\u5229\u7528\u9810\u8a13\u7df4\u6b0a\u91cd\u4e2d\u56fa\u6709\u7684\u8a9e\u7fa9\u548c\u8a9e\u8a00\u4fe1\u606f\u3002\u5b83\u901a\u904e\u5f9e\u9810\u8a13\u7df4\u6b0a\u91cd\u77e9\u9663\u4e2d\u9078\u64c7\u9aa8\u67b6\u4e26\u53ea\u5b78\u7fd2\u4e00\u500b\u5c0f\u77e9\u9663\u4f86\u5be6\u73fe\u9019\u4e00\u9ede\u3002\u5be6\u9a57\u8868\u660e\uff0cPMSS \u5728\u5177\u6709\u66f4\u5c11\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u4efb\u52d9\u4e2d\u512a\u65bc LoRA \u548c\u5176\u4ed6\u5fae\u8abf\u65b9\u6cd5\u3002\u6211\u5011\u8b49\u660e\u4e86\u5b83\u7684\u6709\u6548\u6027\uff0c\u7279\u5225\u662f\u5728\u8655\u7406\u8907\u96dc\u4efb\u52d9\u65b9\u9762\uff0c\u4f8b\u5982 DROP \u57fa\u6e96\uff08\u5728 LLaMA2-7B/13B \u4e0a+3.4%/+5.9%\uff09\u548c\u6578\u5b78\u63a8\u7406\uff08\u5728 GSM8K \u7684 LLaMA2-7B\u3001Mistral-7B \u548c Gemma-7B \u4e0a+12.89%/+5.61%/+3.11%\uff09\u3002\u4ee3\u78bc\u548c\u6a21\u578b\u5c07\u5f88\u5feb\u767c\u5e03\u3002", "author": "Qibin Wang et.al.", "authors": "Qibin Wang, Xiaolin Hu, Weikai Xu, Wei Liu, Jian Luan, Bin Wang", "id": "2409.16722v1", "paper_url": "http://arxiv.org/abs/2409.16722v1", "repo": "null"}}