{"2502.09780": {"publish_time": "2025-02-13", "title": "Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games", "paper_summary": "Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of\napplications involving the interaction of a group of agents in a shared unknown\nenvironment. A prominent framework for studying MARL is Markov games, with the\ngoal of finding various notions of equilibria in a sample-efficient manner,\nsuch as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE).\nHowever, existing sample-efficient approaches either require tailored\nuncertainty estimation under function approximation, or careful coordination of\nthe players. In this paper, we propose a novel model-based algorithm, called\nVMG, that incentivizes exploration via biasing the empirical estimate of the\nmodel parameters towards those with a higher collective best-response values of\nall the players when fixing the other players' policies, thus encouraging the\npolicy to deviate from its current equilibrium for more exploration. VMG is\noblivious to different forms of function approximation, and permits\nsimultaneous and uncoupled policy updates of all players. Theoretically, we\nalso establish that VMG achieves a near-optimal regret for finding both the NEs\nof two-player zero-sum Markov games and CCEs of multi-player general-sum Markov\ngames under linear function approximation in an online environment, which\nnearly match their counterparts with sophisticated uncertainty quantification.", "paper_summary_zh": "\u591a\u667a\u80fd\u9ad4\u5f37\u5316\u5b78\u7fd2 (MARL) \u662f\u4e00\u7cfb\u5217\u61c9\u7528\u7a0b\u5f0f\u7684\u5fc3\u81df\uff0c\u9019\u4e9b\u61c9\u7528\u7a0b\u5f0f\u6d89\u53ca\u4e00\u7fa4\u667a\u80fd\u9ad4\u5728\u4e00\u500b\u5171\u7528\u672a\u77e5\u74b0\u5883\u4e2d\u7684\u4e92\u52d5\u3002\u7814\u7a76 MARL \u7684\u4e00\u500b\u8457\u540d\u6846\u67b6\u662f\u99ac\u53ef\u592b\u535a\u5f08\uff0c\u5176\u76ee\u6a19\u662f\u7528\u6a23\u672c\u6709\u6548\u7387\u7684\u65b9\u5f0f\u627e\u51fa\u5404\u7a2e\u5747\u8861\u6982\u5ff5\uff0c\u4f8b\u5982\u7d0d\u8a31\u5747\u8861 (NE) \u548c\u7c97\u76f8\u95dc\u5747\u8861 (CCE)\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u6a23\u672c\u6709\u6548\u7387\u65b9\u6cd5\u9700\u8981\u5728\u51fd\u6578\u903c\u8fd1\u4e0b\u9032\u884c\u91cf\u8eab\u6253\u9020\u7684\u4e0d\u78ba\u5b9a\u6027\u4f30\u8a08\uff0c\u6216\u8b39\u614e\u5354\u8abf\u53c3\u8207\u8005\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u57fa\u65bc\u6a21\u578b\u7684\u6f14\u7b97\u6cd5\uff0c\u7a31\u70ba VMG\uff0c\u5b83\u900f\u904e\u5c07\u6a21\u578b\u53c3\u6578\u7684\u7d93\u9a57\u4f30\u8a08\u503c\u504f\u5411\u65bc\u5728\u56fa\u5b9a\u5176\u4ed6\u53c3\u8207\u8005\u653f\u7b56\u6642\u6240\u6709\u53c3\u8207\u8005\u7684\u96c6\u9ad4\u6700\u4f73\u53cd\u61c9\u503c\uff0c\u5f9e\u800c\u6fc0\u52f5\u63a2\u7d22\uff0c\u9032\u800c\u9f13\u52f5\u653f\u7b56\u504f\u96e2\u5176\u7576\u524d\u5747\u8861\u4ee5\u9032\u884c\u66f4\u591a\u63a2\u7d22\u3002VMG \u4e0d\u6703\u5ffd\u7565\u51fd\u6578\u903c\u8fd1\u7684\u4e0d\u540c\u5f62\u5f0f\uff0c\u4e26\u5141\u8a31\u6240\u6709\u53c3\u8207\u8005\u540c\u6642\u9032\u884c\u975e\u8026\u5408\u7684\u653f\u7b56\u66f4\u65b0\u3002\u5728\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u4e5f\u5efa\u7acb\u4e86 VMG \u5728\u7dda\u4e0a\u74b0\u5883\u4e2d\u4f7f\u7528\u7dda\u6027\u51fd\u6578\u903c\u8fd1\u4f86\u5c0b\u627e\u96d9\u4eba\u96f6\u548c\u99ac\u53ef\u592b\u535a\u5f08\u7684 NE \u548c\u591a\u4eba\u4e00\u822c\u548c\u99ac\u53ef\u592b\u535a\u5f08\u7684 CCE \u6642\uff0c\u6703\u7372\u5f97\u63a5\u8fd1\u6700\u4f73\u7684\u5f8c\u6094\uff0c\u9019\u5e7e\u4e4e\u8207\u5176\u5728\u4e0d\u78ba\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u66f4\u70ba\u8907\u96dc\u7684\u5c0d\u61c9\u7269\u76f8\u5339\u914d\u3002", "author": "Tong Yang et.al.", "authors": "Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi", "id": "2502.09780v1", "paper_url": "http://arxiv.org/abs/2502.09780v1", "repo": "null"}}