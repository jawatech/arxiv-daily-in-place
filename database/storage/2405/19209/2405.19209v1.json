{"2405.19209": {"publish_time": "2024-05-29", "title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos", "paper_summary": "Video-language understanding tasks have focused on short video clips, often\nstruggling with long-form video understanding tasks. Recently, many long\nvideo-language understanding approaches have leveraged the reasoning\ncapabilities of Large Language Models (LLMs) to perform long video QA,\ntransforming videos into densely sampled frame captions, and asking LLMs to\nrespond to text queries over captions. However, the frames used for captioning\nare often redundant and contain irrelevant information, making dense sampling\ninefficient, and ignoring the fact that video QA requires varying levels of\ngranularity, with some video segments being highly relevant to the question\n(needing more fine-grained detail) while others being less relevant. Thus,\nthese LLM-based approaches are prone to missing information and operate on\nlarge numbers of irrelevant captions, lowering both performance and efficiency.\nTo address these issues, we introduce VideoTree, a query-adaptive and\nhierarchical framework for long-video understanding with LLMs. VideoTree\ndynamically extracts query-related information from a video and builds a\ntree-based representation for LLM reasoning. First, VideoTree adaptively\nselects frames for captioning by iteratively clustering frames based on their\nvisual features and scoring clusters using their relevance to the query.\nSecond, it organizes visual clusters into a query-adaptive and hierarchical\ntree structure; the tree encodes varying levels of granularity, with higher\nresolution on relevant segments. Finally, VideoTree produces an answer by\ntraversing the tree's keyframes and passing their captions to an LLM answerer.\nOur method improves both reasoning accuracy and efficiency compared to existing\nmethods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselines\non the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, while\nreducing inference time by 40%.", "paper_summary_zh": "\u5f71\u7247\u8a9e\u8a00\u7406\u89e3\u4efb\u52d9\u4e00\u76f4\u5c08\u6ce8\u65bc\u77ed\u5f71\u7247\u7247\u6bb5\uff0c\u901a\u5e38\u96e3\u4ee5\u7406\u89e3\u9577\u7bc7\u5f71\u7247\u7406\u89e3\u4efb\u52d9\u3002\u6700\u8fd1\uff0c\u8a31\u591a\u9577\u5f71\u7247\u8a9e\u8a00\u7406\u89e3\u65b9\u6cd5\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u80fd\u529b\u57f7\u884c\u9577\u5f71\u7247\u554f\u7b54\uff0c\u5c07\u5f71\u7247\u8f49\u63db\u6210\u5bc6\u96c6\u63a1\u6a23\u7684\u5e40\u5b57\u5e55\uff0c\u4e26\u8981\u6c42 LLM \u56de\u61c9\u5b57\u5e55\u4e0a\u7684\u6587\u5b57\u67e5\u8a62\u3002\u7136\u800c\uff0c\u7528\u65bc\u5b57\u5e55\u7684\u5e40\u901a\u5e38\u662f\u5197\u9918\u7684\uff0c\u4e14\u5305\u542b\u4e0d\u76f8\u95dc\u7684\u8cc7\u8a0a\uff0c\u9019\u4f7f\u5f97\u5bc6\u96c6\u63a1\u6a23\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u5ffd\u7565\u4e86\u5f71\u7247\u554f\u7b54\u9700\u8981\u4e0d\u540c\u5c64\u7d1a\u7684\u8a73\u7d30\u7a0b\u5ea6\uff0c\u5176\u4e2d\u67d0\u4e9b\u5f71\u7247\u7247\u6bb5\u8207\u554f\u984c\u9ad8\u5ea6\u76f8\u95dc\uff08\u9700\u8981\u66f4\u7d30\u7dfb\u7684\u7d30\u7bc0\uff09\uff0c\u800c\u5176\u4ed6\u5f71\u7247\u7247\u6bb5\u5247\u76f8\u95dc\u6027\u8f03\u4f4e\u3002\u56e0\u6b64\uff0c\u9019\u4e9b\u57fa\u65bc LLM \u7684\u65b9\u6cd5\u5bb9\u6613\u907a\u6f0f\u8cc7\u8a0a\uff0c\u4e26\u904b\u4f5c\u65bc\u5927\u91cf\u7684\u7121\u95dc\u5b57\u5e55\u4e0a\uff0c\u964d\u4f4e\u4e86\u6548\u80fd\u548c\u6548\u7387\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 VideoTree\uff0c\u9019\u662f\u4e00\u500b\u91dd\u5c0d LLM \u7684\u9577\u5f71\u7247\u7406\u89e3\u67e5\u8a62\u9069\u61c9\u6027\u4e14\u968e\u5c64\u5316\u7684\u67b6\u69cb\u3002VideoTree \u52d5\u614b\u5730\u5f9e\u5f71\u7247\u4e2d\u64f7\u53d6\u8207\u67e5\u8a62\u76f8\u95dc\u7684\u8cc7\u8a0a\uff0c\u4e26\u5efa\u7acb\u4e00\u500b\u57fa\u65bc\u6a39\u72c0\u7d50\u69cb\u7684 LLM \u63a8\u7406\u8868\u793a\u3002\u9996\u5148\uff0cVideoTree \u900f\u904e\u6839\u64da\u8996\u89ba\u7279\u5fb5\u53cd\u8986\u5c0d\u5e40\u9032\u884c\u5206\u7fa4\uff0c\u4e26\u4f7f\u7528\u8207\u67e5\u8a62\u76f8\u95dc\u6027\u5c0d\u5206\u7fa4\u9032\u884c\u8a55\u5206\uff0c\u81ea\u9069\u61c9\u5730\u9078\u64c7\u7528\u65bc\u5b57\u5e55\u7684\u5e40\u3002\u5176\u6b21\uff0c\u5b83\u5c07\u8996\u89ba\u5206\u7fa4\u7d44\u7e54\u6210\u4e00\u500b\u8207\u67e5\u8a62\u76f8\u95dc\u4e14\u968e\u5c64\u5316\u7684\u6a39\u72c0\u7d50\u69cb\uff1b\u9019\u68f5\u6a39\u7de8\u78bc\u4e86\u4e0d\u540c\u5c64\u7d1a\u7684\u8a73\u7d30\u7a0b\u5ea6\uff0c\u76f8\u95dc\u7247\u6bb5\u5177\u6709\u8f03\u9ad8\u7684\u89e3\u6790\u5ea6\u3002\u6700\u5f8c\uff0cVideoTree \u900f\u904e\u904d\u6b77\u6a39\u72c0\u7d50\u69cb\u7684\u95dc\u9375\u5e40\uff0c\u4e26\u5c07\u5176\u5b57\u5e55\u50b3\u905e\u7d66 LLM \u56de\u7b54\u5668\u4f86\u7522\u751f\u7b54\u6848\u3002\u8207\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u6539\u5584\u4e86\u63a8\u7406\u6e96\u78ba\u5ea6\u548c\u6548\u7387\uff1aVideoTree \u5728 EgoSchema\u3001NExT-QA \u548c IntentQA \u57fa\u6e96\u4e0a\u5206\u5225\u6bd4\u57fa\u7dda\u9ad8\u51fa 7.0%\u30012.2% \u548c 2.7% \u7684\u6e96\u78ba\u5ea6\uff0c\u540c\u6642\u5c07\u63a8\u7406\u6642\u9593\u6e1b\u5c11\u4e86 40%\u3002", "author": "Ziyang Wang et.al.", "authors": "Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, Mohit Bansal", "id": "2405.19209v1", "paper_url": "http://arxiv.org/abs/2405.19209v1", "repo": "https://github.com/Ziyang412/VideoTree"}}