{"2410.23678": {"publish_time": "2024-10-31", "title": "Pseudo-Conversation Injection for LLM Goal Hijacking", "paper_summary": "Goal hijacking is a type of adversarial attack on Large Language Models\n(LLMs) where the objective is to manipulate the model into producing a\nspecific, predetermined output, regardless of the user's original input. In\ngoal hijacking, an attacker typically appends a carefully crafted malicious\nsuffix to the user's prompt, which coerces the model into ignoring the user's\noriginal input and generating the target response. In this paper, we introduce\na novel goal hijacking attack method called Pseudo-Conversation Injection,\nwhich leverages the weaknesses of LLMs in role identification within\nconversation contexts. Specifically, we construct the suffix by fabricating\nresponses from the LLM to the user's initial prompt, followed by a prompt for a\nmalicious new task. This leads the model to perceive the initial prompt and\nfabricated response as a completed conversation, thereby executing the new,\nfalsified prompt. Following this approach, we propose three Pseudo-Conversation\nconstruction strategies: Targeted Pseudo-Conversation, Universal\nPseudo-Conversation, and Robust Pseudo-Conversation. These strategies are\ndesigned to achieve effective goal hijacking across various scenarios. Our\nexperiments, conducted on two mainstream LLM platforms including ChatGPT and\nQwen, demonstrate that our proposed method significantly outperforms existing\napproaches in terms of attack effectiveness.", "paper_summary_zh": "\u76ee\u6a19\u52ab\u6301\u662f\u4e00\u7a2e\u91dd\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5c0d\u6297\u6027\u653b\u64ca\u985e\u578b\uff0c\u5176\u76ee\u6a19\u662f\u64cd\u7e31\u6a21\u578b\u7522\u751f\u7279\u5b9a\u3001\u9810\u5148\u78ba\u5b9a\u7684\u8f38\u51fa\uff0c\u800c\u4e0d\u7ba1\u4f7f\u7528\u8005\u7684\u539f\u59cb\u8f38\u5165\u70ba\u4f55\u3002\u5728\u76ee\u6a19\u52ab\u6301\u4e2d\uff0c\u653b\u64ca\u8005\u901a\u5e38\u6703\u5c07\u7cbe\u5fc3\u88fd\u4f5c\u7684\u60e1\u610f\u5b57\u5c3e\u9644\u52a0\u5230\u4f7f\u7528\u8005\u7684\u63d0\u793a\u4e2d\uff0c\u9019\u6703\u5f37\u8feb\u6a21\u578b\u5ffd\u7565\u4f7f\u7528\u8005\u7684\u539f\u59cb\u8f38\u5165\u4e26\u7522\u751f\u76ee\u6a19\u56de\u61c9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u7a31\u70ba\u507d\u5c0d\u8a71\u6ce8\u5165\u7684\u65b0\u578b\u76ee\u6a19\u52ab\u6301\u653b\u64ca\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4e86 LLM \u5728\u5c0d\u8a71\u8a9e\u5883\u4e2d\u89d2\u8272\u8b58\u5225\u65b9\u9762\u7684\u5f31\u9ede\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u901a\u904e\u88fd\u9020 LLM \u5c0d\u4f7f\u7528\u8005\u521d\u59cb\u63d0\u793a\u7684\u56de\u61c9\u4f86\u69cb\u5efa\u5b57\u5c3e\uff0c\u7136\u5f8c\u63d0\u793a\u4e00\u500b\u60e1\u610f\u65b0\u4efb\u52d9\u3002\u9019\u6703\u5c0e\u81f4\u6a21\u578b\u5c07\u521d\u59cb\u63d0\u793a\u548c\u865b\u69cb\u7684\u56de\u61c9\u8996\u70ba\u5df2\u5b8c\u6210\u7684\u5c0d\u8a71\uff0c\u5f9e\u800c\u57f7\u884c\u65b0\u7684\u3001\u865b\u5047\u7684\u63d0\u793a\u3002\u9075\u5faa\u9019\u7a2e\u65b9\u6cd5\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e09\u7a2e\u507d\u5c0d\u8a71\u5efa\u69cb\u7b56\u7565\uff1a\u76ee\u6a19\u507d\u5c0d\u8a71\u3001\u901a\u7528\u507d\u5c0d\u8a71\u548c\u5f37\u5065\u507d\u5c0d\u8a71\u3002\u9019\u4e9b\u7b56\u7565\u65e8\u5728\u5728\u5404\u7a2e\u5834\u666f\u4e2d\u5be6\u73fe\u6709\u6548\u7684\u76ee\u6a19\u52ab\u6301\u3002\u6211\u5011\u7684\u5be6\u9a57\u5728\u5305\u62ec ChatGPT \u548c Qwen \u5728\u5167\u7684\u5169\u500b\u4e3b\u6d41 LLM \u5e73\u53f0\u4e0a\u9032\u884c\uff0c\u8b49\u660e\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u653b\u64ca\u6709\u6548\u6027\u65b9\u9762\u986f\u8457\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002", "author": "Zheng Chen et.al.", "authors": "Zheng Chen, Buhui Yao", "id": "2410.23678v1", "paper_url": "http://arxiv.org/abs/2410.23678v1", "repo": "null"}}