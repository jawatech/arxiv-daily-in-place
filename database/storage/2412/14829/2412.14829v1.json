{"2412.14829": {"publish_time": "2024-12-19", "title": "Mention Attention for Pronoun Translation", "paper_summary": "Most pronouns are referring expressions, computers need to resolve what do\nthe pronouns refer to, and there are divergences on pronoun usage across\nlanguages. Thus, dealing with these divergences and translating pronouns is a\nchallenge in machine translation. Mentions are referring candidates of pronouns\nand have closer relations with pronouns compared to general tokens. We assume\nthat extracting additional mention features can help pronoun translation.\nTherefore, we introduce an additional mention attention module in the decoder\nto pay extra attention to source mentions but not non-mention tokens. Our\nmention attention module not only extracts features from source mentions, but\nalso considers target-side context which benefits pronoun translation. In\naddition, we also introduce two mention classifiers to train models to\nrecognize mentions, whose outputs guide the mention attention. We conduct\nexperiments on the WMT17 English-German translation task, and evaluate our\nmodels on general translation and pronoun translation, using BLEU, APT, and\ncontrastive evaluation metrics. Our proposed model outperforms the baseline\nTransformer model in terms of APT and BLEU scores, this confirms our hypothesis\nthat we can improve pronoun translation by paying additional attention to\nsource mentions, and shows that our introduced additional modules do not have\nnegative effect on the general translation quality.", "paper_summary_zh": "\u5927\u90e8\u5206\u4ee3\u540d\u8a5e\u90fd\u662f\u6307\u6d89\u8868\u9054\uff0c\u96fb\u8166\u9700\u8981\u89e3\u6790\u4ee3\u540d\u8a5e\u6240\u6307\u6d89\u7684\u5167\u5bb9\uff0c\u800c\u4e0d\u540c\u8a9e\u8a00\u5728\u4ee3\u540d\u8a5e\u7684\u4f7f\u7528\u4e0a\u5b58\u5728\u5dee\u7570\u3002\u56e0\u6b64\uff0c\u8655\u7406\u9019\u4e9b\u5dee\u7570\u4e26\u7ffb\u8b6f\u4ee3\u540d\u8a5e\u662f\u6a5f\u5668\u7ffb\u8b6f\u4e2d\u7684\u4e00\u9805\u6311\u6230\u3002\u63d0\u53ca\u662f\u4ee3\u540d\u8a5e\u7684\u6307\u6d89\u5019\u9078\uff0c\u8207\u4e00\u822c\u7b26\u865f\u76f8\u6bd4\uff0c\u8207\u4ee3\u540d\u8a5e\u6709\u66f4\u7dca\u5bc6\u7684\u95dc\u4fc2\u3002\u6211\u5011\u5047\u8a2d\u8403\u53d6\u984d\u5916\u7684\u63d0\u53ca\u7279\u5fb5\u6709\u52a9\u65bc\u4ee3\u540d\u8a5e\u7ffb\u8b6f\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5728\u89e3\u78bc\u5668\u4e2d\u5f15\u5165\u4e00\u500b\u984d\u5916\u7684\u63d0\u53ca\u6ce8\u610f\u529b\u6a21\u7d44\uff0c\u4ee5\u7279\u5225\u6ce8\u610f\u4f86\u6e90\u63d0\u53ca\uff0c\u4f46\u4e0d\u6ce8\u610f\u975e\u63d0\u53ca\u7b26\u865f\u3002\u6211\u5011\u7684\u63d0\u53ca\u6ce8\u610f\u529b\u6a21\u7d44\u4e0d\u50c5\u5f9e\u4f86\u6e90\u63d0\u53ca\u4e2d\u8403\u53d6\u7279\u5fb5\uff0c\u9084\u8003\u616e\u4e86\u5c0d\u4ee3\u540d\u8a5e\u7ffb\u8b6f\u6709\u76ca\u7684\u76ee\u6a19\u5074\u60c5\u5883\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86\u5169\u500b\u63d0\u53ca\u5206\u985e\u5668\uff0c\u4ee5\u8a13\u7df4\u6a21\u578b\u8b58\u5225\u63d0\u53ca\uff0c\u5176\u8f38\u51fa\u6307\u5c0e\u63d0\u53ca\u6ce8\u610f\u529b\u3002\u6211\u5011\u5c0d WMT17 \u82f1\u5fb7\u7ffb\u8b6f\u4efb\u52d9\u9032\u884c\u4e86\u5be6\u9a57\uff0c\u4e26\u4f7f\u7528 BLEU\u3001APT \u548c\u5c0d\u6bd4\u8a55\u4f30\u6307\u6a19\u5c0d\u6211\u5011\u7684\u6a21\u578b\u9032\u884c\u4e86\u901a\u7528\u7ffb\u8b6f\u548c\u4ee3\u540d\u8a5e\u7ffb\u8b6f\u8a55\u4f30\u3002\u6211\u5011\u63d0\u51fa\u7684\u6a21\u578b\u5728 APT \u548c BLEU \u5206\u6578\u65b9\u9762\u512a\u65bc\u57fa\u6e96 Transformer \u6a21\u578b\uff0c\u9019\u8b49\u5be6\u4e86\u6211\u5011\u7684\u5047\u8a2d\uff0c\u5373\u6211\u5011\u53ef\u4ee5\u900f\u904e\u7279\u5225\u6ce8\u610f\u4f86\u6e90\u63d0\u53ca\u4f86\u6539\u9032\u4ee3\u540d\u8a5e\u7ffb\u8b6f\uff0c\u4e26\u8868\u660e\u6211\u5011\u5f15\u5165\u7684\u984d\u5916\u6a21\u7d44\u5c0d\u4e00\u822c\u7ffb\u8b6f\u54c1\u8cea\u6c92\u6709\u8ca0\u9762\u5f71\u97ff\u3002", "author": "Gongbo Tang et.al.", "authors": "Gongbo Tang, Christian Hardmeier", "id": "2412.14829v1", "paper_url": "http://arxiv.org/abs/2412.14829v1", "repo": "null"}}