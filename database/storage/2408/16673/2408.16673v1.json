{"2408.16673": {"publish_time": "2024-08-29", "title": "Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity", "paper_summary": "Large language models rely on Supervised Fine-Tuning (SFT) to specialize in\ndownstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it\noften leads to overfitting and limited output diversity due to its aggressive\nupdates to the data distribution. This paper aim to address these issues by\nintroducing the maximum entropy principle, which favors models with flatter\ndistributions that still effectively capture the data. Specifically, we develop\na new distribution matching method called GEM, which solves reverse\nKullback-Leibler divergence minimization with an entropy regularizer.\n  For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects.\nFirst, when applied to the UltraFeedback dataset to develop general\ninstruction-following abilities, GEM exhibits reduced overfitting, evidenced by\nlower perplexity and better performance on the IFEval benchmark. Furthermore,\nGEM enhances output diversity, leading to performance gains of up to 7 points\non math reasoning and code generation tasks using best-of-n sampling, even\nwithout domain-specific data. Second, when fine-tuning with domain-specific\ndatasets for math reasoning and code generation, GEM also shows less\noverfitting and improvements of up to 10 points compared with CE.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4f9d\u8cf4\u53d7\u63a7\u5fae\u8abf (SFT) \u4f86\u5c08\u7cbe\u65bc\u4e0b\u6e38\u4efb\u52d9\u3002\u4ea4\u53c9\u71b5 (CE) \u640d\u5931\u662f SFT \u4e2d\u7684\u5be6\u969b\u9078\u64c7\uff0c\u4f46\u7531\u65bc\u5176\u5c0d\u8cc7\u6599\u5206\u4f48\u7684\u6fc0\u9032\u66f4\u65b0\uff0c\u5b83\u7d93\u5e38\u5c0e\u81f4\u904e\u5ea6\u64ec\u5408\u548c\u8f38\u51fa\u591a\u6a23\u6027\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u900f\u904e\u5f15\u5165\u6700\u5927\u71b5\u539f\u7406\u4f86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u8a72\u539f\u7406\u6709\u5229\u65bc\u5177\u6709\u8f03\u5e73\u5766\u5206\u4f48\u4e14\u4ecd\u80fd\u6709\u6548\u64f7\u53d6\u8cc7\u6599\u7684\u6a21\u578b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u7a31\u70ba GEM \u7684\u65b0\u5206\u4f48\u5339\u914d\u65b9\u6cd5\uff0c\u5b83\u4ee5\u71b5\u6b63\u5247\u5316\u6c42\u89e3\u53cd\u5411 Kullback-Leibler \u6563\u5ea6\u6700\u5c0f\u5316\u3002\u5c0d\u65bc Llama-3-8B \u6a21\u578b\u7684 SFT\uff0cGEM \u5728\u5e7e\u500b\u65b9\u9762\u512a\u65bc CE\u3002\u9996\u5148\uff0c\u7576\u61c9\u7528\u65bc UltraFeedback \u8cc7\u6599\u96c6\u4ee5\u958b\u767c\u4e00\u822c\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u6642\uff0cGEM \u5c55\u73fe\u51fa\u6e1b\u5c11\u7684\u904e\u5ea6\u64ec\u5408\uff0c\u9019\u7531\u8f03\u4f4e\u7684\u56f0\u60d1\u5ea6\u548c\u5728 IFEval \u57fa\u6e96\u4e0a\u7684\u66f4\u597d\u6548\u80fd\u6240\u8b49\u660e\u3002\u6b64\u5916\uff0cGEM \u589e\u5f37\u4e86\u8f38\u51fa\u591a\u6a23\u6027\uff0c\u5373\u4f7f\u6c92\u6709\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u6599\uff0c\u4e5f\u80fd\u5728\u6578\u5b78\u63a8\u7406\u548c\u7a0b\u5f0f\u78bc\u7522\u751f\u4efb\u52d9\u4e2d\u4f7f\u7528\u6700\u4f73 n \u63a1\u6a23\u7372\u5f97\u9ad8\u9054 7 \u5206\u7684\u6548\u80fd\u63d0\u5347\u3002\u5176\u6b21\uff0c\u7576\u4f7f\u7528\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u6599\u96c6\u9032\u884c\u5fae\u8abf\u4ee5\u9032\u884c\u6578\u5b78\u63a8\u7406\u548c\u7a0b\u5f0f\u78bc\u7522\u751f\u6642\uff0c\u8207 CE \u76f8\u6bd4\uff0cGEM \u4e5f\u986f\u793a\u51fa\u8f03\u5c11\u7684\u904e\u5ea6\u64ec\u5408\u548c\u9ad8\u9054 10 \u5206\u7684\u6539\u9032\u3002", "author": "Ziniu Li et.al.", "authors": "Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Ruoyu Sun, Zhi-Quan Luo", "id": "2408.16673v1", "paper_url": "http://arxiv.org/abs/2408.16673v1", "repo": "null"}}