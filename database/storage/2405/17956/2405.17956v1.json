{"2405.17956": {"publish_time": "2024-05-28", "title": "Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives", "paper_summary": "For aligning large language models (LLMs), prior work has leveraged\nreinforcement learning via human feedback (RLHF) or variations of direct\npreference optimization (DPO). While DPO offers a simpler framework based on\nmaximum likelihood estimation, it compromises on the ability to tune language\nmodels to easily maximize non-differentiable and non-binary objectives\naccording to the LLM designer's preferences (e.g., using simpler language or\nminimizing specific kinds of harmful content). These may neither align with\nuser preferences nor even be able to be captured tractably by binary preference\ndata. To leverage the simplicity and performance of DPO with the\ngeneralizability of RL, we propose a hybrid approach between DPO and RLHF. With\na simple augmentation to the implicit reward decomposition of DPO, we allow for\ntuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL.\nThe proposed method, Hybrid Preference Optimization (HPO), shows the ability to\neffectively generalize to both user preferences and auxiliary designer\nobjectives, while preserving alignment performance across a range of\nchallenging benchmarks and model sizes.", "paper_summary_zh": "\u5c0d\u65bc\u5c0d\u9f4a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u5148\u524d\u7684\u7814\u7a76\u5229\u7528\u4e86\u900f\u904e\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF) \u6216\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u7684\u8b8a\u7570\u3002\u96d6\u7136 DPO \u63d0\u4f9b\u4e86\u4e00\u500b\u57fa\u65bc\u6700\u5927\u4f3c\u7136\u4f30\u8a08\u7684\u66f4\u7c21\u55ae\u67b6\u69cb\uff0c\u4f46\u5b83\u5728\u6839\u64da LLM \u8a2d\u8a08\u8005\u7684\u504f\u597d\u8abf\u6574\u8a9e\u8a00\u6a21\u578b\u4ee5\u8f15\u9b06\u6700\u5927\u5316\u4e0d\u53ef\u5fae\u5206\u548c\u975e\u4e8c\u5143\u76ee\u6a19\u7684\u80fd\u529b\u4e0a\u6709\u6240\u59a5\u5354\uff08\u4f8b\u5982\uff0c\u4f7f\u7528\u66f4\u7c21\u55ae\u7684\u8a9e\u8a00\u6216\u6700\u5c0f\u5316\u7279\u5b9a\u985e\u578b\u7684\u6709\u5bb3\u5167\u5bb9\uff09\u3002\u9019\u4e9b\u53ef\u80fd\u65e2\u4e0d\u7b26\u5408\u4f7f\u7528\u8005\u7684\u504f\u597d\uff0c\u751a\u81f3\u7121\u6cd5\u900f\u904e\u4e8c\u5143\u504f\u597d\u8cc7\u6599\u9032\u884c\u6709\u6548\u64f7\u53d6\u3002\u70ba\u4e86\u5229\u7528 DPO \u7684\u7c21\u55ae\u6027\u548c\u6548\u80fd\u4ee5\u53ca RL \u7684\u53ef\u6982\u5316\u6027\uff0c\u6211\u5011\u63d0\u51fa DPO \u548c RLHF \u4e4b\u9593\u7684\u6df7\u5408\u65b9\u6cd5\u3002\u900f\u904e\u5c0d DPO \u7684\u96b1\u542b\u734e\u52f5\u5206\u89e3\u9032\u884c\u7c21\u55ae\u7684\u64f4\u5145\uff0c\u6211\u5011\u5141\u8a31\u8abf\u6574 LLM \u4ee5\u4f7f\u7528\u96e2\u7dda RL \u6700\u5927\u5316\u4e00\u7d44\u4efb\u610f\u7684\u8f14\u52a9\u734e\u52f5\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u6df7\u5408\u504f\u597d\u6700\u4f73\u5316 (HPO)\uff0c\u986f\u793a\u4e86\u6709\u6548\u6982\u5316\u5230\u4f7f\u7528\u8005\u504f\u597d\u548c\u8f14\u52a9\u8a2d\u8a08\u8005\u76ee\u6a19\u7684\u80fd\u529b\uff0c\u540c\u6642\u5728\u5404\u7a2e\u5177\u6709\u6311\u6230\u6027\u7684\u57fa\u6e96\u548c\u6a21\u578b\u5927\u5c0f\u4e2d\u4fdd\u7559\u5c0d\u9f4a\u6548\u80fd\u3002", "author": "Anirudhan Badrinath et.al.", "authors": "Anirudhan Badrinath, Prabhat Agarwal, Jiajing Xu", "id": "2405.17956v1", "paper_url": "http://arxiv.org/abs/2405.17956v1", "repo": "null"}}