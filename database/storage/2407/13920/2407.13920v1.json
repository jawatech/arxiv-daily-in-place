{"2407.13920": {"publish_time": "2024-07-18", "title": "DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention", "paper_summary": "We here propose a novel hierarchical transformer model that adeptly\nintegrates the feature extraction capabilities of Convolutional Neural Networks\n(CNNs) with the advanced representational potential of Vision Transformers\n(ViTs). Addressing the lack of inductive biases and dependence on extensive\ntraining datasets in ViTs, our model employs a CNN backbone to generate\nhierarchical visual representations. These representations are then adapted for\ntransformer input through an innovative patch tokenization. We also introduce a\n'scale attention' mechanism that captures cross-scale dependencies,\ncomplementing patch attention to enhance spatial understanding and preserve\nglobal perception. Our approach significantly outperforms baseline models on\nsmall and medium-sized medical datasets, demonstrating its efficiency and\ngeneralizability. The components are designed as plug-and-play for different\nCNN architectures and can be adapted for multiple applications. The code is\navailable at https://github.com/xiaoyatang/DuoFormer.git.", "paper_summary_zh": "\u6211\u5011\u5728\u6b64\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u5206\u5c64Transformer\u6a21\u578b\uff0c\u5b83\u5de7\u5999\u5730\u6574\u5408\u4e86\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN) \u7684\u7279\u5fb5\u64f7\u53d6\u80fd\u529b\uff0c\u4ee5\u53ca\u8996\u89baTransformer (ViT) \u7684\u5148\u9032\u8868\u793a\u6f5b\u529b\u3002\u91dd\u5c0d ViT \u4e2d\u7f3a\u4e4f\u6b78\u7d0d\u504f\u8aa4\u548c\u4f9d\u8cf4\u65bc\u5ee3\u6cdb\u8a13\u7df4\u8cc7\u6599\u96c6\u7684\u554f\u984c\uff0c\u6211\u5011\u7684\u6a21\u578b\u63a1\u7528 CNN \u4e3b\u5e79\u4f86\u7522\u751f\u5206\u5c64\u8996\u89ba\u8868\u793a\u3002\u9019\u4e9b\u8868\u793a\u63a5\u8457\u900f\u904e\u5275\u65b0\u7684\u5340\u584a\u6a19\u8a18\u5316\uff0c\u8abf\u6574\u70baTransformer\u8f38\u5165\u3002\u6211\u5011\u4e5f\u5f15\u5165\u300c\u5c3a\u5ea6\u6ce8\u610f\u529b\u300d\u6a5f\u5236\uff0c\u5b83\u6355\u6349\u8de8\u5c3a\u5ea6\u4f9d\u8cf4\u6027\uff0c\u88dc\u5145\u5340\u584a\u6ce8\u610f\u529b\u4ee5\u589e\u5f37\u7a7a\u9593\u7406\u89e3\u4e26\u4fdd\u7559\u5168\u5c40\u611f\u77e5\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728\u5c0f\u578b\u548c\u4e2d\u578b\u7684\u91ab\u5b78\u8cc7\u6599\u96c6\u4e0a\uff0c\u660e\u986f\u512a\u65bc\u57fa\u7dda\u6a21\u578b\uff0c\u8b49\u660e\u4e86\u5b83\u7684\u6548\u7387\u548c\u53ef\u6982\u5316\u6027\u3002\u9019\u4e9b\u7d44\u4ef6\u88ab\u8a2d\u8a08\u6210\u5373\u63d2\u5373\u7528\uff0c\u9069\u7528\u65bc\u4e0d\u540c\u7684 CNN \u67b6\u69cb\uff0c\u4e26\u4e14\u53ef\u4ee5\u8abf\u6574\u70ba\u591a\u7a2e\u61c9\u7528\u7a0b\u5f0f\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/xiaoyatang/DuoFormer.git \u53d6\u5f97\u3002", "author": "Xiaoya Tang et.al.", "authors": "Xiaoya Tang, Bodong Zhang, Beatrice S. Knudsen, Tolga Tasdizen", "id": "2407.13920v1", "paper_url": "http://arxiv.org/abs/2407.13920v1", "repo": "null"}}