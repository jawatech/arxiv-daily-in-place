{"2407.11948": {"publish_time": "2024-07-16", "title": "Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation", "paper_summary": "The utilization of Transformer-based models prospers the growth of\nmulti-document summarization (MDS). Given the huge impact and widespread\nadoption of Transformer-based models in various natural language processing\ntasks, investigating their performance and behaviors in the context of MDS\nbecomes crucial for advancing the field and enhancing the quality of summary.\nTo thoroughly examine the behaviours of Transformer-based MDS models, this\npaper presents five empirical studies on (1) measuring the impact of document\nboundary separators quantitatively; (2) exploring the effectiveness of\ndifferent mainstream Transformer structures; (3) examining the sensitivity of\nthe encoder and decoder; (4) discussing different training strategies; and (5)\ndiscovering the repetition in a summary generation. The experimental results on\nprevalent MDS datasets and eleven evaluation metrics show the influence of\ndocument boundary separators, the granularity of different level features and\ndifferent model training strategies. The results also reveal that the decoder\nexhibits greater sensitivity to noises compared to the encoder. This\nunderscores the important role played by the decoder, suggesting a potential\ndirection for future research in MDS. Furthermore, the experimental results\nindicate that the repetition problem in the generated summaries has\ncorrelations with the high uncertainty scores.", "paper_summary_zh": "<paragraph>\u57fa\u65bc Transformer \u7684\u6a21\u578b\u904b\u7528\u84ec\u52c3\u767c\u5c55\u4e86\u591a\u6587\u4ef6\u6458\u8981 (MDS) \u7684\u6210\u9577\u3002\u7531\u65bc\u57fa\u65bc Transformer \u7684\u6a21\u578b\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5177\u6709\u5de8\u5927\u7684\u5f71\u97ff\u529b\u548c\u5ee3\u6cdb\u7684\u63a1\u7528\uff0c\u56e0\u6b64\u7814\u7a76\u5b83\u5011\u5728 MDS \u80cc\u666f\u4e0b\u7684\u6548\u80fd\u548c\u884c\u70ba\u5c0d\u65bc\u63a8\u52d5\u8a72\u9818\u57df\u548c\u63d0\u5347\u6458\u8981\u54c1\u8cea\u81f3\u95dc\u91cd\u8981\u3002\u70ba\u4e86\u5fb9\u5e95\u6aa2\u9a57\u57fa\u65bc Transformer \u7684 MDS \u6a21\u578b\u7684\u884c\u70ba\uff0c\u672c\u6587\u91dd\u5c0d (1) \u5b9a\u91cf\u6e2c\u91cf\u6587\u4ef6\u908a\u754c\u5206\u9694\u7b26\u7684\u5f71\u97ff\uff1b(2) \u63a2\u8a0e\u4e0d\u540c\u4e3b\u6d41 Transformer \u7d50\u69cb\u7684\u6709\u6548\u6027\uff1b(3) \u6aa2\u9a57\u7de8\u78bc\u5668\u548c\u89e3\u78bc\u5668\u7684\u654f\u611f\u6027\uff1b(4) \u8a0e\u8ad6\u4e0d\u540c\u7684\u8a13\u7df4\u7b56\u7565\uff1b\u4ee5\u53ca (5) \u767c\u73fe\u6458\u8981\u751f\u6210\u4e2d\u7684\u91cd\u8907\u6027\uff0c\u63d0\u51fa\u4e86\u4e94\u9805\u5be6\u8b49\u7814\u7a76\u3002\u5728\u6d41\u884c\u7684 MDS \u8cc7\u6599\u96c6\u548c 11 \u9805\u8a55\u4f30\u6307\u6a19\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\u4e86\u6587\u4ef6\u908a\u754c\u5206\u9694\u7b26\u3001\u4e0d\u540c\u5c64\u7d1a\u7279\u5fb5\u7684\u7c92\u5ea6\u548c\u4e0d\u540c\u6a21\u578b\u8a13\u7df4\u7b56\u7565\u7684\u5f71\u97ff\u3002\u7d50\u679c\u9084\u63ed\u793a\uff0c\u8207\u7de8\u78bc\u5668\u76f8\u6bd4\uff0c\u89e3\u78bc\u5668\u5c0d\u96dc\u8a0a\u8868\u73fe\u51fa\u66f4\u5927\u7684\u654f\u611f\u6027\u3002\u9019\u5f37\u8abf\u4e86\u89e3\u78bc\u5668\u6240\u626e\u6f14\u7684\u91cd\u8981\u89d2\u8272\uff0c\u70ba MDS \u672a\u4f86\u7684\u7814\u7a76\u6307\u51fa\u4e86\u6f5b\u5728\u7684\u65b9\u5411\u3002\u6b64\u5916\uff0c\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u6458\u8981\u4e2d\u91cd\u8907\u51fa\u73fe\u7684\u554f\u984c\u8207\u9ad8\u4e0d\u78ba\u5b9a\u6027\u5206\u6578\u6709\u95dc\u3002</paragraph>", "author": "Congbo Ma et.al.", "authors": "Congbo Ma, Wei Emma Zhang, Dileepa Pitawela, Haojie Zhuang, Yanfeng Shu", "id": "2407.11948v1", "paper_url": "http://arxiv.org/abs/2407.11948v1", "repo": "null"}}