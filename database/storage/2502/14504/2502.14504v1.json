{"2502.14504": {"publish_time": "2025-02-20", "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models", "paper_summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5df2\u5728\u5404\u7a2e\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5176\u63a8\u7406\u6548\u7387\u53d7\u5230\u89e3\u78bc\u904e\u7a0b\u4e2d\u8655\u7406\u7684\u5927\u91cf\u8996\u89ba\u7b26\u865f\u7684\u9650\u5236\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u9010\u5c64\u9010\u982d\u8996\u89ba\u7b26\u865f\u526a\u679d (PLPHP)\uff0c\u9019\u662f\u4e00\u7a2e\u5305\u62ec\u5c64\u7d1a\u4fdd\u7559\u7387\u5206\u914d\u548c\u982d\u7d1a\u8996\u89ba\u7b26\u865f\u526a\u679d\u7684\u5169\u7d1a\u7d30\u7c92\u5ea6\u526a\u679d\u65b9\u6cd5\u3002\u53d7\u89e3\u78bc\u5668\u5c64\u4e2d\u8996\u89ba\u7b26\u865f\u91cd\u65b0\u95dc\u6ce8\u73fe\u8c61\u7684\u555f\u767c\uff0c\u6211\u5011\u52d5\u614b\u5730\u9010\u5c64\u8abf\u6574\u7b26\u865f\u4fdd\u7559\u7387\u3002\u5c0d\u8996\u89ba\u8cc7\u8a0a\u8868\u73fe\u51fa\u66f4\u5f37\u95dc\u6ce8\u529b\u7684\u5c64\u4fdd\u7559\u66f4\u591a\u8996\u89ba\u7b26\u865f\uff0c\u800c\u8996\u89ba\u95dc\u6ce8\u529b\u8f03\u4f4e\u7684\u5c64\u5247\u88ab\u7a4d\u6975\u526a\u679d\u3002\u6b64\u5916\uff0cPLPHP \u5728\u95dc\u6ce8\u982d\u7d1a\u5225\u61c9\u7528\u526a\u679d\uff0c\u4f7f\u540c\u4e00\u5c64\u4e2d\u7684\u4e0d\u540c\u982d\u90e8\u53ef\u4ee5\u7368\u7acb\u4fdd\u7559\u95dc\u9375\u4e0a\u4e0b\u6587\u3002\u5728\u591a\u500b\u57fa\u6e96\u6e2c\u8a66\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0cPLPHP \u7684\u89e3\u78bc\u901f\u5ea6\u63d0\u9ad8\u4e86 18%\uff0c\u4e14\u5c07\u9375\u503c\u5feb\u53d6 (KV \u5feb\u53d6) \u5927\u5c0f\u6e1b\u5c11\u4e86 50% \u4ee5\u4e0a\uff0c\u800c\u4ee3\u50f9\u50c5\u70ba\u5e73\u5747\u6548\u80fd\u4e0b\u964d 0.46%\uff0c\u540c\u6642\u9084\u5728\u591a\u5f71\u50cf\u4efb\u52d9\u4e2d\u5be6\u73fe\u4e86\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86\u7d30\u7c92\u5ea6\u7b26\u865f\u526a\u679d\u7684\u6709\u6548\u6027\uff0c\u4e26\u6709\u52a9\u65bc\u63d0\u5347 LVLMs \u7684\u6548\u7387\u548c\u53ef\u64f4\u5145\u6027\u3002\u6211\u5011\u7684\u539f\u59cb\u78bc\u5c07\u516c\u958b\u63d0\u4f9b\u3002", "author": "Yu Meng et.al.", "authors": "Yu Meng, Kaiyuan Li, Chenran Huang, Chen Gao, Xinlei Chen, Yong Li, Xiaoping Zhang", "id": "2502.14504v1", "paper_url": "http://arxiv.org/abs/2502.14504v1", "repo": "null"}}