{"2410.19144": {"publish_time": "2024-10-24", "title": "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "paper_summary": "We revisit knowledge-aware text-based visual question answering, also known\nas Text-KVQA, in the light of modern advancements in large multimodal models\n(LMMs), and make the following contributions: (i) We propose VisTEL - a\nprincipled approach to perform visual text entity linking. The proposed VisTEL\nmodule harnesses a state-of-the-art visual text recognition engine and the\npower of a large multimodal model to jointly reason using textual and visual\ncontext obtained using surrounding cues in the image to link the visual text\nentity to the correct knowledge base entity. (ii) We present KaLMA - a\nknowledge-aware large multimodal assistant that augments an LMM with knowledge\nassociated with visual text entity in the image to arrive at an accurate\nanswer. Further, we provide a comprehensive experimental analysis and\ncomparison of our approach with traditional visual question answering,\npre-large multimodal models, and large multimodal models, as well as prior\ntop-performing approaches. Averaging over three splits of Text-KVQA, our\nproposed approach surpasses the previous best approach by a substantial 23.3%\non an absolute scale and establishes a new state of the art. We make our\nimplementation publicly available.", "paper_summary_zh": "<paragraph>\u6211\u5011\u5728\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u7684\u73fe\u4ee3\u9032\u6b65\u7684\u57fa\u790e\u4e0a\uff0c\u91cd\u65b0\u63a2\u8a0e\u4e86\u77e5\u8b58\u611f\u77e5\u7684\u57fa\u65bc\u6587\u5b57\u7684\u8996\u89ba\u554f\u984c\u89e3\u7b54\uff0c\u4e5f\u7a31\u70ba Text-KVQA\uff0c\u4e26\u505a\u51fa\u4ee5\u4e0b\u8ca2\u737b\uff1a(i) \u6211\u5011\u63d0\u51fa VisTEL - \u4e00\u7a2e\u57f7\u884c\u8996\u89ba\u6587\u5b57\u5be6\u9ad4\u9023\u7d50\u7684\u539f\u5247\u6027\u65b9\u6cd5\u3002\u63d0\u51fa\u7684 VisTEL \u6a21\u7d44\u5229\u7528\u6700\u5148\u9032\u7684\u8996\u89ba\u6587\u5b57\u8fa8\u8b58\u5f15\u64ce\u548c\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\u7684\u529b\u91cf\uff0c\u4f7f\u7528\u5716\u50cf\u4e2d\u5468\u570d\u7dda\u7d22\u53d6\u5f97\u7684\u6587\u5b57\u548c\u8996\u89ba\u8108\u7d61\uff0c\u5171\u540c\u63a8\u8ad6\uff0c\u5c07\u8996\u89ba\u6587\u5b57\u5be6\u9ad4\u9023\u7d50\u5230\u6b63\u78ba\u7684\u77e5\u8b58\u5eab\u5be6\u9ad4\u3002(ii) \u6211\u5011\u63d0\u51fa KaLMA - \u4e00\u7a2e\u77e5\u8b58\u611f\u77e5\u7684\u5927\u578b\u591a\u6a21\u614b\u52a9\u7406\uff0c\u5b83\u64f4\u5145\u4e86\u4e00\u500b LMM\uff0c\u7d50\u5408\u4e86\u5716\u50cf\u4e2d\u8996\u89ba\u6587\u5b57\u5be6\u9ad4\u76f8\u95dc\u7684\u77e5\u8b58\uff0c\u4ee5\u5f97\u51fa\u6e96\u78ba\u7684\u7b54\u6848\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u5168\u9762\u7684\u5be6\u9a57\u5206\u6790\uff0c\u4e26\u5c07\u6211\u5011\u7684\u65b9\u6cd5\u8207\u50b3\u7d71\u7684\u8996\u89ba\u554f\u984c\u89e3\u7b54\u3001\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\u4e4b\u524d\u548c\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\u4ee5\u53ca\u4e4b\u524d\u7684\u9802\u5c16\u65b9\u6cd5\u9032\u884c\u6bd4\u8f03\u3002\u5728 Text-KVQA \u7684\u4e09\u500b\u5206\u5272\u5e73\u5747\u503c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7d55\u5c0d\u898f\u6a21\u4e0a\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u9054\u5230\u4e86 23.3%\uff0c\u4e26\u5efa\u7acb\u4e86\u65b0\u7684\u6280\u8853\u6c34\u6e96\u3002\u6211\u5011\u516c\u958b\u4e86\u6211\u5011\u7684\u5be6\u4f5c\u3002</paragraph>", "author": "Abhirama Subramanyam Penamakuri et.al.", "authors": "Abhirama Subramanyam Penamakuri, Anand Mishra", "id": "2410.19144v1", "paper_url": "http://arxiv.org/abs/2410.19144v1", "repo": "https://github.com/vl2g/KaLMA"}}