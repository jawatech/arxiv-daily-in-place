{"2501.07783": {"publish_time": "2025-01-14", "title": "Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding", "paper_summary": "Image pyramids are widely adopted in top-performing methods to obtain\nmulti-scale features for precise visual perception and understanding. However,\ncurrent image pyramids use the same large-scale model to process multiple\nresolutions of images, leading to significant computational cost. To address\nthis challenge, we propose a novel network architecture, called\nParameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses\npretrained models (ViTs or CNNs) as branches to process multi-scale images,\nwhere images of higher resolutions are processed by smaller network branches to\nbalance computational cost and performance. To integrate information from\ndifferent spatial scales, we further propose a novel cross-branch feature\ninteraction mechanism. To validate PIIP, we apply it to various perception\nmodels and a representative multimodal large language model called LLaVA, and\nconduct extensive experiments on various tasks such as object detection,\nsegmentation, image classification and multimodal understanding. PIIP achieves\nsuperior performance compared to single-branch and existing multi-resolution\napproaches with lower computational cost. When applied to InternViT-6B, a\nlarge-scale vision foundation model, PIIP can improve its performance by 1%-2%\non detection and segmentation with only 40%-60% of the original computation,\nfinally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For\nmultimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and\n74.5% on MMBench with only 2.8M training data. Our code is released at\nhttps://github.com/OpenGVLab/PIIP.", "paper_summary_zh": "\u56fe\u50cf\u91d1\u5b57\u5854\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u9ad8\u6027\u80fd\u65b9\u6cd5\u4e2d\uff0c\u4ee5\u83b7\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u89c6\u89c9\u611f\u77e5\u548c\u7406\u89e3\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u56fe\u50cf\u91d1\u5b57\u5854\u4f7f\u7528\u76f8\u540c\u7684\u5927\u89c4\u6a21\u6a21\u578b\u6765\u5904\u7406\u56fe\u50cf\u7684\u591a\u4e2a\u5206\u8fa8\u7387\uff0c\u5bfc\u81f4\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u79f0\u4e3a\u53c2\u6570\u53cd\u8f6c\u56fe\u50cf\u91d1\u5b57\u5854\u7f51\u7edc (PIIP)\u3002\u5177\u4f53\u6765\u8bf4\uff0cPIIP \u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff08ViT \u6216 CNN\uff09\u4f5c\u4e3a\u5206\u652f\u6765\u5904\u7406\u591a\u5c3a\u5ea6\u56fe\u50cf\uff0c\u5176\u4e2d\u8f83\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u7531\u8f83\u5c0f\u7684\u7f51\u7edc\u5206\u652f\u5904\u7406\uff0c\u4ee5\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u3002\u4e3a\u4e86\u6574\u5408\u6765\u81ea\u4e0d\u540c\u7a7a\u95f4\u5c3a\u5ea6\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u5206\u652f\u7279\u5f81\u4ea4\u4e92\u673a\u5236\u3002\u4e3a\u4e86\u9a8c\u8bc1 PIIP\uff0c\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e\u5404\u79cd\u611f\u77e5\u6a21\u578b\u548c\u4e00\u4e2a\u540d\u4e3a LLaVA \u7684\u4ee3\u8868\u6027\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u5bf9\u8c61\u68c0\u6d4b\u3001\u5206\u5272\u3001\u56fe\u50cf\u5206\u7c7b\u548c\u591a\u6a21\u6001\u7406\u89e3\u7b49\u5404\u79cd\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u4e0e\u5355\u5206\u652f\u548c\u73b0\u6709\u7684\u591a\u5206\u8fa8\u7387\u65b9\u6cd5\u76f8\u6bd4\uff0cPIIP \u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u5f53\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u89c6\u89c9\u57fa\u7840\u6a21\u578b InternViT-6B \u65f6\uff0cPIIP \u53ef\u4ee5\u5c06\u5176\u5728\u68c0\u6d4b\u548c\u5206\u5272\u4e0a\u7684\u6027\u80fd\u63d0\u9ad8 1%-2%\uff0c\u800c\u539f\u59cb\u8ba1\u7b97\u91cf\u4ec5\u4e3a 40%-60%\uff0c\u6700\u7ec8\u5728 MS COCO \u4e0a\u5b9e\u73b0\u4e86 60.0 \u6846 AP\uff0c\u5728 ADE20K \u4e0a\u5b9e\u73b0\u4e86 59.7 mIoU\u3002\u5bf9\u4e8e\u591a\u6a21\u6001\u7406\u89e3\uff0c\u6211\u4eec\u7684 PIIP-LLaVA \u5728 TextVQA \u4e0a\u5b9e\u73b0\u4e86 73.0% \u7684\u51c6\u786e\u7387\uff0c\u5728 MMBench \u4e0a\u5b9e\u73b0\u4e86 74.5%\uff0c\u800c\u8bad\u7ec3\u6570\u636e\u4ec5\u4e3a 2.8M\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728 https://github.com/OpenGVLab/PIIP \u4e2d\u53d1\u5e03\u3002", "author": "Zhaokai Wang et.al.", "authors": "Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai", "id": "2501.07783v1", "paper_url": "http://arxiv.org/abs/2501.07783v1", "repo": "https://github.com/opengvlab/piip"}}