{"2502.15618": {"publish_time": "2025-02-21", "title": "Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing", "paper_summary": "We introduce Probe Pruning (PP), a novel framework for online, dynamic,\nstructured pruning of Large Language Models (LLMs) applied in a batch-wise\nmanner. PP leverages the insight that not all samples and tokens contribute\nequally to the model's output, and probing a small portion of each batch\neffectively identifies crucial weights, enabling tailored dynamic pruning for\ndifferent batches. It comprises three main stages: probing, history-informed\npruning, and full inference. In the probing stage, PP selects a small yet\ncrucial set of hidden states, based on residual importance, to run a few model\nlayers ahead. During the history-informed pruning stage, PP strategically\nintegrates the probing states with historical states. Subsequently, it\nstructurally prunes weights based on the integrated states and the PP\nimportance score, a metric developed specifically to assess the importance of\neach weight channel in maintaining performance. In the final stage, full\ninference is conducted on the remaining weights. A major advantage of PP is its\ncompatibility with existing models, as it operates without requiring additional\nneural network modules or fine-tuning. Comprehensive evaluations of PP on\nLLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of\nFLOPs-can substantially enhance the efficiency of structured pruning of LLMs.\nFor instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56\ntimes lower ratio of performance degradation per unit of runtime reduction\ncompared to the state-of-the-art method at a 40% pruning ratio. Our code is\navailable at https://github.com/Qi-Le1/Probe_Pruning.", "paper_summary_zh": "<paragraph>\u6211\u5011\u5f15\u5165\u4e86\u63a2\u6e2c\u526a\u679d\uff08Probe Pruning\uff0cPP\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u7528\u65bc\u5927\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7dda\u4e0a\u3001\u52d5\u614b\u3001\u7d50\u69cb\u5316\u526a\u679d\uff0c\u4ee5\u6279\u6b21\u65b9\u5f0f\u61c9\u7528\u3002PP \u5229\u7528\u4e86\u9019\u6a23\u4e00\u500b\u89c0\u9ede\uff1a\u4e26\u975e\u6240\u6709\u6a23\u672c\u548c\u7b26\u865f\u90fd\u5c0d\u6a21\u578b\u7684\u8f38\u51fa\u6709\u540c\u7b49\u7684\u8ca2\u737b\uff0c\u4e26\u4e14\u63a2\u6e2c\u6bcf\u500b\u6279\u6b21\u7684\u4e00\u5c0f\u90e8\u5206\u53ef\u4ee5\u6709\u6548\u5730\u8b58\u5225\u95dc\u9375\u6b0a\u91cd\uff0c\u5f9e\u800c\u91dd\u5c0d\u4e0d\u540c\u7684\u6279\u6b21\u555f\u7528\u91cf\u8eab\u5b9a\u5236\u7684\u52d5\u614b\u526a\u679d\u3002\u5b83\u5305\u542b\u4e09\u500b\u4e3b\u8981\u968e\u6bb5\uff1a\u63a2\u6e2c\u3001\u6b77\u53f2\u8a0a\u606f\u526a\u679d\u548c\u5b8c\u6574\u63a8\u8ad6\u3002\u5728\u63a2\u6e2c\u968e\u6bb5\uff0cPP \u57fa\u65bc\u6b98\u5dee\u91cd\u8981\u6027\u9078\u64c7\u4e00\u5c0f\u7d44\u4f46\u95dc\u9375\u7684\u96b1\u85cf\u72c0\u614b\uff0c\u4ee5\u904b\u884c\u4e00\u4e9b\u6a21\u578b\u5c64\u3002\u5728\u6b77\u53f2\u8a0a\u606f\u526a\u679d\u968e\u6bb5\u671f\u9593\uff0cPP \u5c07\u63a2\u6e2c\u72c0\u614b\u8207\u6b77\u53f2\u72c0\u614b\u7b56\u7565\u6027\u5730\u6574\u5408\u5728\u4e00\u8d77\u3002\u96a8\u5f8c\uff0c\u5b83\u6839\u64da\u6574\u5408\u72c0\u614b\u548c PP \u91cd\u8981\u6027\u5206\u6578\u5c0d\u6b0a\u91cd\u9032\u884c\u7d50\u69cb\u5316\u526a\u679d\uff0c\u9019\u662f\u4e00\u500b\u5c08\u9580\u958b\u767c\u7684\u6307\u6a19\uff0c\u7528\u65bc\u8a55\u4f30\u6bcf\u500b\u6b0a\u91cd\u901a\u9053\u5728\u7dad\u6301\u6548\u80fd\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002\u5728\u6700\u5f8c\u968e\u6bb5\uff0c\u5c0d\u5269\u9918\u6b0a\u91cd\u9032\u884c\u5b8c\u6574\u63a8\u8ad6\u3002PP \u7684\u4e00\u500b\u4e3b\u8981\u512a\u9ede\u662f\u5b83\u8207\u73fe\u6709\u6a21\u578b\u76f8\u5bb9\uff0c\u56e0\u70ba\u5b83\u5728\u904b\u884c\u6642\u4e0d\u9700\u8981\u984d\u5916\u7684\u795e\u7d93\u7db2\u8def\u6a21\u7d44\u6216\u5fae\u8abf\u3002\u5c0d LLaMA-2/3 \u548c OPT \u6a21\u578b\u7684 PP \u7d9c\u5408\u8a55\u4f30\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5c0f\u7684\u63a2\u6e2c\u2014\u2014\u50c5\u4f7f\u7528 1.5% \u7684 FLOP\u2014\u2014\u4e5f\u53ef\u4ee5\u5927\u5e45\u63d0\u9ad8 LLM \u7684\u7d50\u69cb\u5316\u526a\u679d\u6548\u7387\u3002\u4f8b\u5982\uff0c\u7576\u5728 LLaMA-2-7B \u4e0a\u4f7f\u7528 WikiText2 \u9032\u884c\u8a55\u4f30\u6642\uff0cPP \u5728 40% \u7684\u526a\u679d\u7387\u4e0b\uff0c\u6bcf\u55ae\u4f4d\u57f7\u884c\u6642\u9593\u6e1b\u5c11\u7684\u6548\u80fd\u4e0b\u964d\u7387\u6bd4\u6700\u5148\u9032\u7684\u65b9\u6cd5\u4f4e 2.56 \u500d\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/Qi-Le1/Probe_Pruning \u53d6\u5f97\u3002</paragraph>", "author": "Qi Le et.al.", "authors": "Qi Le, Enmao Diao, Ziyan Wang, Xinran Wang, Jie Ding, Li Yang, Ali Anwar", "id": "2502.15618v1", "paper_url": "http://arxiv.org/abs/2502.15618v1", "repo": "null"}}