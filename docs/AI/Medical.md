
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-11-07**|**Position Paper On Diagnostic Uncertainty Estimation from Large Language Models: Next-Word Probability Is Not Pre-test Probability**|Yanjun Gao et.al.|[2411.04962v1](http://arxiv.org/abs/2411.04962v1)|null|
|**2024-11-07**|**AWARE Narrator and the Utilization of Large Language Models to Extract Behavioral Insights from Smartphone Sensing Data**|Tianyi Zhang et.al.|[2411.04691v1](http://arxiv.org/abs/2411.04691v1)|null|
|**2024-11-07**|**FedDP: Privacy-preserving method based on federated learning for histopathology image segmentation**|Liangrui Pan et.al.|[2411.04509v1](http://arxiv.org/abs/2411.04509v1)|null|
|**2024-11-06**|**Robust Real-Time Mortality Prediction in the Intensive Care Unit using Temporal Difference Learning**|Thomas Frost et.al.|[2411.04285v1](http://arxiv.org/abs/2411.04285v1)|[link](https://github.com/tdgfrost/td-icu-mortality)|
|**2024-11-06**|**Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?**|Daniel P. Jeong et.al.|[2411.04118v1](http://arxiv.org/abs/2411.04118v1)|null|
|**2024-11-06**|**RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models**|Maya Varma et.al.|[2411.04097v1](http://arxiv.org/abs/2411.04097v1)|[link](https://github.com/stanford-aimi/ravl)|
|**2024-11-06**|**Aligning Characteristic Descriptors with Images for Human-Expert-like Explainability**|Bharat Chandra Yalavarthi et.al.|[2411.04008v1](http://arxiv.org/abs/2411.04008v1)|null|
|**2024-11-06**|**Fine-tuning -- a Transfer Learning approach**|Joseph Arul Raj et.al.|[2411.03941v1](http://arxiv.org/abs/2411.03941v1)|null|
|**2024-11-06**|**MEG: Medical Knowledge-Augmented Large Language Models for Question Answering**|Laura Cabello et.al.|[2411.03883v2](http://arxiv.org/abs/2411.03883v2)|[link](https://github.com/lautel/meg)|
|**2024-11-06**|**Navigating the landscape of multimodal AI in medicine: a scoping review on technical challenges and clinical applications**|Daan Schouten et.al.|[2411.03782v1](http://arxiv.org/abs/2411.03782v1)|null|
|**2024-11-06**|**Sub-DM:Subspace Diffusion Model with Orthogonal Decomposition for MRI Reconstruction**|Yu Guan et.al.|[2411.03758v1](http://arxiv.org/abs/2411.03758v1)|null|
|**2024-11-06**|**Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?**|Pedro R. A. S. Bassi et.al.|[2411.03670v1](http://arxiv.org/abs/2411.03670v1)|[link](https://github.com/mrgiovanni/touchstone)|
|**2024-11-06**|**Requirements Engineering for Older Adult Digital Health Software: A Systematic Literature Review**|Yuqing Xiao et.al.|[2411.03656v1](http://arxiv.org/abs/2411.03656v1)|null|
|**2024-11-06**|**Cross Feature Fusion of Fundus Image and Generated Lesion Map for Referable Diabetic Retinopathy Classification**|Dahyun Mok et.al.|[2411.03618v1](http://arxiv.org/abs/2411.03618v1)|null|
|**2024-11-05**|**The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare**|Souren Pashangpour et.al.|[2411.03287v1](http://arxiv.org/abs/2411.03287v1)|null|
|**2024-11-05**|**Discovering Data Structures: Nearest Neighbor Search and Beyond**|Omar Salemohamed et.al.|[2411.03253v1](http://arxiv.org/abs/2411.03253v1)|null|
|**2024-11-05**|**Evaluating Machine Learning Models against Clinical Protocols for Enhanced Interpretability and Continuity of Care**|Christel Sirocchi et.al.|[2411.03105v1](http://arxiv.org/abs/2411.03105v1)|[link](https://github.com/ChristelSirocchi/XAI-similarity)|
|**2024-11-05**|**Local Lesion Generation is Effective for Capsule Endoscopy Image Data Augmentation in a Limited Data Setting**|Adrian B. Ch≈Çopowiec et.al.|[2411.03098v1](http://arxiv.org/abs/2411.03098v1)|null|
|**2024-11-05**|**Controlling for Unobserved Confounding with Large Language Model Classification of Patient Smoking Status**|Samuel Lee et.al.|[2411.03004v1](http://arxiv.org/abs/2411.03004v1)|null|
|**2024-11-05**|**Region-Guided Attack on the Segment Anything Model (SAM)**|Xiaoliang Liu et.al.|[2411.02974v1](http://arxiv.org/abs/2411.02974v1)|null|
|**2024-11-05**|**[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI**|Maren Pielka et.al.|[2411.02973v1](http://arxiv.org/abs/2411.02973v1)|null|
|**2024-11-05**|**Membership Inference Attacks against Large Vision-Language Models**|Zhan Li et.al.|[2411.02902v1](http://arxiv.org/abs/2411.02902v1)|[link](https://github.com/lions-epfl/vl-mia)|
|**2024-11-04**|**Advanced XR-Based 6-DOF Catheter Tracking System for Immersive Cardiac Intervention Training**|Mohsen Annabestani et.al.|[2411.02611v1](http://arxiv.org/abs/2411.02611v1)|null|
|**2024-11-04**|**"It's a conversation, not a quiz": A Risk Taxonomy and Reflection Tool for LLM Adoption in Public Health**|Jiawei Zhou et.al.|[2411.02594v1](http://arxiv.org/abs/2411.02594v1)|null|
|**2024-11-04**|**Digitizing Touch with an Artificial Multimodal Fingertip**|Mike Lambeta et.al.|[2411.02479v1](http://arxiv.org/abs/2411.02479v1)|[link](https://github.com/facebookresearch/digit360)|
|**2024-11-04**|**Simulation of Nanorobots with Artificial Intelligence and Reinforcement Learning for Advanced Cancer Cell Detection and Tracking**|Shahab Kavousinejad et.al.|[2411.02345v1](http://arxiv.org/abs/2411.02345v1)|[link](https://github.com/shahab-k93/cancer-and-smart-nanorobot)|
|**2024-11-04**|**Taking AI Welfare Seriously**|Robert Long et.al.|[2411.00986v1](http://arxiv.org/abs/2411.00986v1)|null|
|**2024-11-04**|**Federated GNNs for EEG-Based Stroke Assessment**|Andrea Protani et.al.|[2411.02286v1](http://arxiv.org/abs/2411.02286v1)|null|
|**2024-11-04**|**Weakly supervised deep learning model with size constraint for prostate cancer detection in multiparametric MRI and generalization to unseen domains**|Robin Trombetta et.al.|[2411.02466v1](http://arxiv.org/abs/2411.02466v1)|null|
|**2024-11-04**|**Evaluating the quality of published medical research with ChatGPT**|Mike Thelwall et.al.|[2411.01952v1](http://arxiv.org/abs/2411.01952v1)|null|
|**2024-11-04**|**You are out of context!**|Giancarlo Cobino et.al.|[2411.02464v1](http://arxiv.org/abs/2411.02464v1)|null|
|**2024-11-03**|**Diagnosing Medical Datasets with Training Dynamics**|Laura Wenderoth et.al.|[2411.01653v1](http://arxiv.org/abs/2411.01653v1)|[link](https://github.com/laurawenderoth/training-dynamics)|
|**2024-11-03**|**Optical Flow Representation Alignment Mamba Diffusion Model for Medical Video Generation**|Zhenbin Wang et.al.|[2411.01647v1](http://arxiv.org/abs/2411.01647v1)|null|
|**2024-11-03**|**Customized Subgraph Selection and Encoding for Drug-drug Interaction Prediction**|Haotong Du et.al.|[2411.01535v1](http://arxiv.org/abs/2411.01535v1)|null|
|**2024-11-03**|**Conditional Latent Space Molecular Scaffold Optimization for Accelerated Molecular Design**|Onur Boyar et.al.|[2411.01423v1](http://arxiv.org/abs/2411.01423v1)|null|
|**2024-11-02**|**Medical X-Ray Image Enhancement Using Global Contrast-Limited Adaptive Histogram Equalization**|Sohrab Namazi Nia et.al.|[2411.01373v1](http://arxiv.org/abs/2411.01373v1)|null|
|**2024-11-02**|**Guided Synthesis of Labeled Brain MRI Data Using Latent Diffusion Models for Segmentation of Enlarged Ventricles**|Tim Ruschke et.al.|[2411.01351v1](http://arxiv.org/abs/2411.01351v1)|null|
|**2024-11-02**|**Causal reasoning in difference graphs**|Charles K. Assaad et.al.|[2411.01292v1](http://arxiv.org/abs/2411.01292v1)|null|
|**2024-11-02**|**Designing a Robust Radiology Report Generation System**|Sonit Singh et.al.|[2411.01153v1](http://arxiv.org/abs/2411.01153v1)|null|
|**2024-11-02**|**LEARNER: Learning Granular Labels from Coarse Labels using Contrastive Learning**|Gautam Gare et.al.|[2411.01144v1](http://arxiv.org/abs/2411.01144v1)|null|
|**2024-11-02**|**Artificial Intelligence for Microbiology and Microbiome Research**|Xu-Wen Wang et.al.|[2411.01098v1](http://arxiv.org/abs/2411.01098v1)|null|
|**2024-11-01**|**Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities**|Adriel Saporta et.al.|[2411.01053v1](http://arxiv.org/abs/2411.01053v1)|[link](https://github.com/rajesh-lab/symile)|
|**2024-11-01**|**Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract**|Fan Xiao et.al.|[2411.00726v1](http://arxiv.org/abs/2411.00726v1)|null|
|**2024-11-01**|**CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis**|Fuying Wang et.al.|[2411.00696v1](http://arxiv.org/abs/2411.00696v1)|null|
|**2024-11-01**|**Enhancing Osteoporosis Detection: An Explainable Multi-Modal Learning Framework with Feature Fusion and Variable Clustering**|Mehdi Hosseini Chagahi et.al.|[2411.00916v1](http://arxiv.org/abs/2411.00916v1)|null|
|**2024-11-01**|**Deep learning-based auto-contouring of organs/structures-at-risk for pediatric upper abdominal radiotherapy**|Mianyong Ding et.al.|[2411.00594v1](http://arxiv.org/abs/2411.00594v1)|[link](https://github.com/MMianyong/-PedAbdSeg-)|
|**2024-11-01**|**Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback**|Song Yu et.al.|[2411.00897v1](http://arxiv.org/abs/2411.00897v1)|null|
|**2024-11-01**|**StepCountJITAI: simulation environment for RL with application to physical activity adaptive intervention**|Karine Karine et.al.|[2411.00336v1](http://arxiv.org/abs/2411.00336v1)|null|
|**2024-11-01**|**Strongly Topology-preserving GNNs for Brain Graph Super-resolution**|Pragya Singh et.al.|[2411.02525v1](http://arxiv.org/abs/2411.02525v1)|null|
|**2024-11-01**|**Evaluating the Impact of Lab Test Results on Large Language Models Generated Differential Diagnoses from Clinical Case Vignettes**|Balu Bhasuran et.al.|[2411.02523v1](http://arxiv.org/abs/2411.02523v1)|null|
|**2024-10-31**|**Deep Learning Predicts Mammographic Breast Density in Clinical Breast Ultrasound Images**|Arianna Bunnell et.al.|[2411.00891v1](http://arxiv.org/abs/2411.00891v1)|null|
|**2024-10-31**|**Monitoring fairness in machine learning models that predict patient mortality in the ICU**|Tempest A. van Schaik et.al.|[2411.00190v2](http://arxiv.org/abs/2411.00190v2)|null|
|**2024-10-31**|**Clinical Evaluation of Medical Image Synthesis: A Case Study in Wireless Capsule Endoscopy**|Panagiota Gatoula et.al.|[2411.00178v1](http://arxiv.org/abs/2411.00178v1)|null|
|**2024-10-31**|**Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning**|John Wu et.al.|[2411.00173v1](http://arxiv.org/abs/2411.00173v1)|null|
|**2024-10-31**|**Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks**|Yingzhe Peng et.al.|[2410.24032v1](http://arxiv.org/abs/2410.24032v1)|null|
|**2024-10-31**|**Neural Network Verification with PyRAT**|Augustin Lemesle et.al.|[2410.23903v1](http://arxiv.org/abs/2410.23903v1)|null|
|**2024-10-31**|**Counterfactual MRI Data Augmentation using Conditional Denoising Diffusion Generative Models**|Pedro Mor√£o et.al.|[2410.23835v1](http://arxiv.org/abs/2410.23835v1)|[link](https://github.com/pedromorao/counterfactual-mri-data-augmentation)|
|**2024-10-31**|**Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding**|Jinlong He et.al.|[2410.23822v1](http://arxiv.org/abs/2410.23822v1)|null|
|**2024-10-31**|**Improving snore detection under limited dataset through harmonic/percussive source separation and convolutional neural networks**|F. D. Gonzalez-Martinez et.al.|[2410.23796v1](http://arxiv.org/abs/2410.23796v1)|null|
|**2024-10-31**|**The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams**|Yunqi Zhu et.al.|[2410.23769v1](http://arxiv.org/abs/2410.23769v1)|null|
|**2024-10-31**|**Artificial intelligence to improve clinical coding practice in Scandinavia: a crossover randomized controlled trial**|Taridzo Chomutare et.al.|[2410.23725v1](http://arxiv.org/abs/2410.23725v1)|null|
|**2024-10-31**|**Enhancing Brain Tumor Classification Using TrAdaBoost and Multi-Classifier Deep Learning Approaches**|Mahin Mohammadi et.al.|[2411.00875v1](http://arxiv.org/abs/2411.00875v1)|null|
|**2024-10-31**|**Deep Convolutional Neural Networks on Multiclass Classification of Three-Dimensional Brain Images for Parkinson's Disease Stage Prediction**|Guan-Hua Huang et.al.|[2410.23649v1](http://arxiv.org/abs/2410.23649v1)|null|
|**2024-10-31**|**MS-Glance: Non-semantic context vectors and the applications in supervising image reconstruction**|Ziqi Gao et.al.|[2410.23577v1](http://arxiv.org/abs/2410.23577v1)|[link](https://github.com/z7gao/msglance)|
|**2024-10-31**|**LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models**|Hieu Tran et.al.|[2410.23526v1](http://arxiv.org/abs/2410.23526v1)|null|
|**2024-10-30**|**Emory Knee Radiograph (MRKR) Dataset**|Brandon Price et.al.|[2411.00866v1](http://arxiv.org/abs/2411.00866v1)|null|
|**2024-10-30**|**STIED: A deep learning model for the SpatioTemporal detection of focal Interictal Epileptiform Discharges with MEG**|Raquel Fern√°ndez-Mart√≠n et.al.|[2410.23386v1](http://arxiv.org/abs/2410.23386v1)|null|
|**2024-10-30**|**Larger models yield better results? Streamlined severity classification of ADHD-related concerns using BERT-based knowledge distillation**|Ahmed Akib Jawad Karim et.al.|[2411.00052v1](http://arxiv.org/abs/2411.00052v1)|null|
|**2024-10-30**|**DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI and PET**|Yitong Li et.al.|[2410.23219v1](http://arxiv.org/abs/2410.23219v1)|[link](https://github.com/ai-med/diamond)|
|**2024-10-30**|**Variable Resolution Sampling and Deep Learning Image Recovery for Accelerated Multi-Spectral MRI Near Metal Implants**|Azadeh Sharafi et.al.|[2410.23329v1](http://arxiv.org/abs/2410.23329v1)|null|
|**2024-10-30**|**DiabML: AI-assisted diabetes diagnosis method with meta-heuristic-based feature selection**|Vahideh Hayyolalam et.al.|[2411.00858v1](http://arxiv.org/abs/2411.00858v1)|null|
|**2024-10-30**|**Revisiting MAE pre-training for 3D medical image segmentation**|Tassilo Wald et.al.|[2410.23132v1](http://arxiv.org/abs/2410.23132v1)|null|
|**2024-10-30**|**SpiroActive: Active Learning for Efficient Data Acquisition for Spirometry**|Ankita Kumari Jain et.al.|[2410.22950v1](http://arxiv.org/abs/2410.22950v1)|null|
|**2024-10-30**|**Efficient Feature Extraction and Classification Architecture for MRI-Based Brain Tumor Detection**|Plabon Paul et.al.|[2410.22619v1](http://arxiv.org/abs/2410.22619v1)|null|
|**2024-10-29**|**Do Large Language Models Align with Core Mental Health Counseling Competencies?**|Viet Cuong Nguyen et.al.|[2410.22446v1](http://arxiv.org/abs/2410.22446v1)|null|
|**2024-10-29**|**MAPUNetR: A Hybrid Vision Transformer and U-Net Architecture for Efficient and Interpretable Medical Image Segmentation**|Ovais Iqbal Shah et.al.|[2410.22223v1](http://arxiv.org/abs/2410.22223v1)|null|
|**2024-10-29**|**Natural Language Processing for Analyzing Electronic Health Records and Clinical Notes in Cancer Research: A Review**|Muhammad Bilal et.al.|[2410.22180v1](http://arxiv.org/abs/2410.22180v1)|null|
|**2024-10-29**|**Advanced Hybrid Deep Learning Model for Enhanced Classification of Osteosarcoma Histopathology Images**|Arezoo Borji et.al.|[2411.00832v1](http://arxiv.org/abs/2411.00832v1)|null|
|**2024-10-29**|**Unsupervised Training of a Dynamic Context-Aware Deep Denoising Framework for Low-Dose Fluoroscopic Imaging**|Sun-Young Jeon et.al.|[2411.00830v1](http://arxiv.org/abs/2411.00830v1)|null|
|**2024-10-29**|**Coupling quantum-like cognition with the neuronal networks within generalized probability theory**|Andrei Khrennikov et.al.|[2411.00036v1](http://arxiv.org/abs/2411.00036v1)|null|
|**2024-10-29**|**Advancing Efficient Brain Tumor Multi-Class Classification -- New Insights from the Vision Mamba Model in Transfer Learning**|Yinyi Lai et.al.|[2410.21872v2](http://arxiv.org/abs/2410.21872v2)|null|
|**2024-10-29**|**How Does Critical Batch Size Scale in Pre-training?**|Hanlin Zhang et.al.|[2410.21676v1](http://arxiv.org/abs/2410.21676v1)|null|
|**2024-10-29**|**A Tutorial on Clinical Speech AI Development: From Data Collection to Model Validation**|Si-Ioi Ng et.al.|[2410.21640v1](http://arxiv.org/abs/2410.21640v1)|null|
|**2024-10-28**|**Can Large Language Models Replace Data Scientists in Clinical Research?**|Zifeng Wang et.al.|[2410.21591v1](http://arxiv.org/abs/2410.21591v1)|null|
|**2024-10-28**|**A Perspective for Adapting Generalist AI to Specialized Medical AI Applications and Their Challenges**|Zifeng Wang et.al.|[2411.00024v1](http://arxiv.org/abs/2411.00024v1)|null|
|**2024-10-28**|**Going Beyond H&E and Oncology: How Do Histopathology Foundation Models Perform for Multi-stain IHC and Immunology?**|Amaya Gallagher-Syed et.al.|[2410.21560v1](http://arxiv.org/abs/2410.21560v1)|[link](https://github.com/amayags/immunohistobench)|
|**2024-10-28**|**Towards Multi-dimensional Explanation Alignment for Medical Classification**|Lijie Hu et.al.|[2410.21494v1](http://arxiv.org/abs/2410.21494v1)|null|
|**2024-10-28**|**Multi-modal AI for comprehensive breast cancer prognostication**|Jan Witowski et.al.|[2410.21256v1](http://arxiv.org/abs/2410.21256v1)|null|
|**2024-10-28**|**Belief in the Machine: Investigating Epistemological Blind Spots of Language Models**|Mirac Suzgun et.al.|[2410.21195v1](http://arxiv.org/abs/2410.21195v1)|[link](https://github.com/suzgunmirac/belief-in-the-machine)|
|**2024-10-28**|**Deep Learning-Based Fatigue Cracks Detection in Bridge Girders using Feature Pyramid Networks**|Jiawei Zhang et.al.|[2410.21175v1](http://arxiv.org/abs/2410.21175v1)|null|
|**2024-10-28**|**Trajectory Flow Matching with Applications to Clinical Time Series Modeling**|Xi Zhang et.al.|[2410.21154v1](http://arxiv.org/abs/2410.21154v1)|[link](https://github.com/nzhangx/trajectoryflowmatching)|
|**2024-10-28**|**Diagnostic Performance of Deep Learning for Predicting Gliomas' IDH and 1p/19q Status in MRI: A Systematic Review and Meta-Analysis**|Somayeh Farahani et.al.|[2411.02426v1](http://arxiv.org/abs/2411.02426v1)|null|
|**2024-10-28**|**Informed Deep Abstaining Classifier: Investigating noise-robust training for diagnostic decision support systems**|Helen Schneider et.al.|[2410.21014v1](http://arxiv.org/abs/2410.21014v1)|null|
|**2024-10-28**|**Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering**|Zhilin Zhang et.al.|[2410.21000v1](http://arxiv.org/abs/2410.21000v1)|null|
|**2024-10-28**|**Large Language Model Benchmarks in Medical Tasks**|Lawrence K. Q. Yan et.al.|[2410.21348v1](http://arxiv.org/abs/2410.21348v1)|null|
|**2024-10-28**|**Vascular Segmentation of Functional Ultrasound Images using Deep Learning**|Hana Sebia et.al.|[2410.22365v1](http://arxiv.org/abs/2410.22365v1)|null|
|**2024-10-27**|**Language Models And A Second Opinion Use Case: The Pocket Professional**|David Noever et.al.|[2410.20636v1](http://arxiv.org/abs/2410.20636v1)|null|
|**2024-10-27**|**Improving Decision Sparsity**|Yiyang Sun et.al.|[2410.20483v1](http://arxiv.org/abs/2410.20483v1)|null|
|**2024-10-27**|**MedGo: A Chinese Medical Large Language Model**|Haitao Zhang et.al.|[2410.20428v1](http://arxiv.org/abs/2410.20428v1)|null|
|**2024-10-27**|**Addressing the Pitfalls of Image-Based Structural Health Monitoring: A Focus on False Positives, False Negatives, and Base Rate Bias**|Vagelis Plevris et.al.|[2410.20384v1](http://arxiv.org/abs/2410.20384v1)|null|

#### Abstracts
##### **Position Paper On Diagnostic Uncertainty Estimation from Large Language Models: Next-Word Probability Is Not Pre-test Probability**
2411.04962v1 by Yanjun Gao, Skatje Myers, Shan Chen, Dmitriy Dligach, Timothy A Miller, Danielle Bitterman, Guanhua Chen, Anoop Mayampurath, Matthew Churpek, Majid Afshar

Large language models (LLMs) are being explored for diagnostic decision
support, yet their ability to estimate pre-test probabilities, vital for
clinical decision-making, remains limited. This study evaluates two LLMs,
Mistral-7B and Llama3-70B, using structured electronic health record data on
three diagnosis tasks. We examined three current methods of extracting LLM
probability estimations and revealed their limitations. We aim to highlight the
need for improved techniques in LLM confidence estimation.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ê≠£Âú®Ë¢´Êé¢Á¥¢Áî®ÊñºË®∫Êñ∑Ê±∫Á≠ñÊîØÊåÅÔºå‰ΩÜÂÆÉÂÄë‰º∞Ë®àËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö‰∏≠Ëá≥ÈóúÈáçË¶ÅÁöÑÈ†êÊ∏¨Ë©¶Ê¶ÇÁéáÁöÑËÉΩÂäõ‰ªçÁÑ∂ÊúâÈôê„ÄÇÊú¨Á†îÁ©∂‰ΩøÁî®‰∏âÂÄãË®∫Êñ∑‰ªªÂãôÁöÑÁµêÊßãÂåñÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑÊï∏ÊìöË©ï‰º∞‰∫ÜÂÖ©ÂÄã LLMÔºåMistral-7B Âíå Llama3-70B„ÄÇÊàëÂÄëÊ™¢Êü•‰∫ÜÊèêÂèñ LLM Ê¶ÇÁéá‰º∞Ë®àÁöÑ‰∏âÁ®ÆÁï∂ÂâçÊñπÊ≥ï‰∏¶Êè≠Á§∫‰∫ÜÂÆÉÂÄëÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÂº∑Ë™øÊîπÈÄ≤ LLM ÁΩÆ‰ø°Â∫¶‰º∞Ë®àÊäÄË°ìÁöÑÂøÖË¶ÅÊÄß„ÄÇ

##### **AWARE Narrator and the Utilization of Large Language Models to Extract Behavioral Insights from Smartphone Sensing Data**
2411.04691v1 by Tianyi Zhang, Miu Kojima, Simon D'Alfonso

Smartphones, equipped with an array of sensors, have become valuable tools
for personal sensing. Particularly in digital health, smartphones facilitate
the tracking of health-related behaviors and contexts, contributing
significantly to digital phenotyping, a process where data from digital
interactions is analyzed to infer behaviors and assess mental health.
Traditional methods process raw sensor data into information features for
statistical and machine learning analyses. In this paper, we introduce a novel
approach that systematically converts smartphone-collected data into
structured, chronological narratives. The AWARE Narrator translates
quantitative smartphone sensing data into English language descriptions,
forming comprehensive narratives of an individual's activities. We apply the
framework to the data collected from university students over a week,
demonstrating the potential of utilizing the narratives to summarize individual
behavior, and analyzing psychological states by leveraging large language
models.

ÊëòË¶ÅÔºöÊô∫ÊÖßÂûãÊâãÊ©üÈÖçÂÇô‰∫ÜÂêÑÂºèÊÑüÊ∏¨Âô®ÔºåÂ∑≤ÊàêÁÇ∫ÂÄã‰∫∫ÊÑüÊ∏¨ÁöÑÂØ∂Ë≤¥Â∑•ÂÖ∑„ÄÇÁâπÂà•ÊòØÂú®Êï∏‰ΩçÂÅ•Â∫∑È†òÂüüÔºåÊô∫ÊÖßÂûãÊâãÊ©ü‰øÉÈÄ≤‰∫ÜÂÅ•Â∫∑Áõ∏ÈóúË°åÁÇ∫ÂíåÊÉÖÂ¢ÉÁöÑËøΩËπ§ÔºåÂ∞çÊï∏‰ΩçË°®ÂûãÂàÜÊûêÂÅöÂá∫‰∫ÜÈáçÂ§ßË≤¢ÁçªÔºåÊï∏‰ΩçË°®ÂûãÂàÜÊûêÊòØ‰∏ÄÁ®ÆÂæûÊï∏‰Ωç‰∫íÂãï‰∏≠ÂàÜÊûêË≥áÊñô‰ª•Êé®Ë´ñË°åÁÇ∫ÂíåË©ï‰º∞ÂøÉÁêÜÂÅ•Â∫∑ÁöÑÁ®ãÂ∫è„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÂ∞áÂéüÂßãÊÑüÊ∏¨Âô®Ë≥áÊñôËôïÁêÜÊàêË≥áË®äÁâπÂæµÔºå‰ª•ÈÄ≤Ë°åÁµ±Ë®àÂíåÊ©üÂô®Â≠∏ÁøíÂàÜÊûê„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÁ≥ªÁµ±ÊÄßÂú∞Â∞áÊô∫ÊÖßÂûãÊâãÊ©üÊî∂ÈõÜÁöÑË≥áÊñôËΩâÊèõÊàêÁµêÊßãÂåñÁöÑÊôÇÈñìÈ†ÜÂ∫èÊïò‰∫ã„ÄÇAWARE Narrator Â∞áÂÆöÈáèÁöÑÊô∫ÊÖßÂûãÊâãÊ©üÊÑüÊ∏¨Ë≥áÊñôËΩâÊèõÊàêËã±ÊñáË™ûË®ÄÊèèËø∞ÔºåÂΩ¢ÊàêÂÄã‰∫∫Ê¥ªÂãïÁöÑÁ∂úÂêàÊïò‰∫ã„ÄÇÊàëÂÄëÂ∞áÊ≠§Êû∂ÊßãÂ•óÁî®Âú®Â§ßÂ≠∏Áîü‰∏ÄÈÄ±ÂÖßÊî∂ÈõÜÁöÑË≥áÊñô‰∏äÔºåË≠âÊòé‰∫ÜÂà©Áî®Êïò‰∫ãÁ∏ΩÁµêÂÄã‰∫∫Ë°åÁÇ∫ÁöÑÊΩõÂäõÔºå‰∏¶ÈÄèÈÅéÈÅãÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜÂàÜÊûêÂøÉÁêÜÁãÄÊÖã„ÄÇ

##### **FedDP: Privacy-preserving method based on federated learning for histopathology image segmentation**
2411.04509v1 by Liangrui Pan, Mao Huang, Lian Wang, Pinle Qin, Shaoliang Peng

Hematoxylin and Eosin (H&E) staining of whole slide images (WSIs) is
considered the gold standard for pathologists and medical practitioners for
tumor diagnosis, surgical planning, and post-operative assessment. With the
rapid advancement of deep learning technologies, the development of numerous
models based on convolutional neural networks and transformer-based models has
been applied to the precise segmentation of WSIs. However, due to privacy
regulations and the need to protect patient confidentiality, centralized
storage and processing of image data are impractical. Training a centralized
model directly is challenging to implement in medical settings due to these
privacy concerns.This paper addresses the dispersed nature and privacy
sensitivity of medical image data by employing a federated learning framework,
allowing medical institutions to collaboratively learn while protecting patient
privacy. Additionally, to address the issue of original data reconstruction
through gradient inversion during the federated learning training process,
differential privacy introduces noise into the model updates, preventing
attackers from inferring the contributions of individual samples, thereby
protecting the privacy of the training data.Experimental results show that the
proposed method, FedDP, minimally impacts model accuracy while effectively
safeguarding the privacy of cancer pathology image data, with only a slight
decrease in Dice, Jaccard, and Acc indices by 0.55%, 0.63%, and 0.42%,
respectively. This approach facilitates cross-institutional collaboration and
knowledge sharing while protecting sensitive data privacy, providing a viable
solution for further research and application in the medical field.

ÊëòË¶ÅÔºöËòáÊú®Á≤æÂíå‰ºäÁ¥ÖÔºàH&EÔºâÊüìËâ≤ÂÖ®ÂàáÁâáÂúñÂÉèÔºàWSIÔºâË¢´Ë™çÁÇ∫ÊòØÁóÖÁêÜÂ≠∏ÂÆ∂ÂíåÈÜ´ÁôÇÂæûÊ•≠‰∫∫Âì°Áî®ÊñºËÖ´Áò§Ë®∫Êñ∑„ÄÅÊâãË°ìË¶èÂäÉÂíåË°ìÂæåË©ï‰º∞ÁöÑÈªÉÈáëÊ®ôÊ∫ñ„ÄÇÈö®ËëóÊ∑±Â∫¶Â≠∏ÁøíÊäÄË°ìÁöÑÂø´ÈÄüÈÄ≤Â±ïÔºåÂü∫ÊñºÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂü∫ÊñºTransformerÁöÑÊ®°ÂûãÁöÑÁúæÂ§öÊ®°ÂûãÂ∑≤Ë¢´ÊáâÁî®Êñº WSI ÁöÑÁ≤æÁ¢∫ÂàÜÂâ≤„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÈö±ÁßÅÊ≥ïË¶èÂíå‰øùË≠∑ÊÇ£ËÄÖÊ©üÂØÜÊÄßÁöÑÈúÄË¶ÅÔºåÈõÜ‰∏≠ÂºèÂÑ≤Â≠òÂíåËôïÁêÜÂΩ±ÂÉèË≥áÊñôÊòØ‰∏çÂàáÂØ¶ÈöõÁöÑ„ÄÇÁî±ÊñºÈÄô‰∫õÈö±ÁßÅÂïèÈ°åÔºåÂú®ÈÜ´ÁôÇÁí∞Â¢É‰∏≠Áõ¥Êé•Ë®ìÁ∑¥ÈõÜ‰∏≠ÂºèÊ®°ÂûãÈõ£‰ª•ÂØ¶ÊñΩ„ÄÇÊú¨ÊñáÈÄöÈÅéÊé°Áî®ËÅØÂêàÂ≠∏ÁøíÊ°ÜÊû∂‰æÜËß£Ê±∫ÈÜ´ÁôÇÂΩ±ÂÉèË≥áÊñôÁöÑÂàÜÊï£ÊÄßË≥™ÂíåÈö±ÁßÅÊïèÊÑüÊÄßÔºåÂÖÅË®±ÈÜ´ÁôÇÊ©üÊßãÂú®‰øùË≠∑ÊÇ£ËÄÖÈö±ÁßÅÁöÑÂêåÊôÇÈÄ≤Ë°åÂçî‰ΩúÂ≠∏Áøí„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜËß£Ê±∫ËÅØÂêàÂ≠∏ÁøíË®ìÁ∑¥ÈÅéÁ®ã‰∏≠ÈÄöÈÅéÊ¢ØÂ∫¶ÂèçËΩâÈÄ≤Ë°åÂéüÂßãË≥áÊñôÈáçÂª∫ÁöÑÂïèÈ°åÔºåÂ∑ÆÂàÜÈö±ÁßÅÊúÉÂú®Ê®°ÂûãÊõ¥Êñ∞‰∏≠ÂºïÂÖ•ÈõúË®äÔºåÈò≤Ê≠¢ÊîªÊìäËÄÖÊé®Êñ∑ÂÄãÂà•Ê®£Êú¨ÁöÑË≤¢ÁçªÔºåÂæûËÄå‰øùË≠∑Ë®ìÁ∑¥Ë≥áÊñôÁöÑÈö±ÁßÅ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï FedDP Â∞çÊ®°ÂûãÊ∫ñÁ¢∫Â∫¶ÁöÑÂΩ±ÈüøÊúÄÂ∞èÔºåÂêåÊôÇÊúâÊïà‰øùË≠∑‰∫ÜÁôåÁóáÁóÖÁêÜÂΩ±ÂÉèË≥áÊñôÁöÑÈö±ÁßÅÔºåDice„ÄÅJaccard Âíå Acc ÊåáÊï∏ÂàÜÂà•ÂÉÖÁï•ÂæÆ‰∏ãÈôç‰∫Ü 0.55%„ÄÅ0.63% Âíå 0.42%„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰øÉÈÄ≤‰∫ÜÊ©üÊßãÈñìÁöÑÂêà‰ΩúÂíåÁü•Ë≠òÂÖ±‰∫´ÔºåÂêåÊôÇ‰øùË≠∑‰∫ÜÊïèÊÑüË≥áÊñôÁöÑÈö±ÁßÅÔºåÁÇ∫ÈÜ´ÁôÇÈ†òÂüüÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÂíåÊáâÁî®Êèê‰æõ‰∫ÜÂèØË°åÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Robust Real-Time Mortality Prediction in the Intensive Care Unit using Temporal Difference Learning**
2411.04285v1 by Thomas Frost, Kezhi Li, Steve Harris

The task of predicting long-term patient outcomes using supervised machine
learning is a challenging one, in part because of the high variance of each
patient's trajectory, which can result in the model over-fitting to the
training data. Temporal difference (TD) learning, a common reinforcement
learning technique, may reduce variance by generalising learning to the pattern
of state transitions rather than terminal outcomes. However, in healthcare this
method requires several strong assumptions about patient states, and there
appears to be limited literature evaluating the performance of TD learning
against traditional supervised learning methods for long-term health outcome
prediction tasks. In this study, we define a framework for applying TD learning
to real-time irregularly sampled time series data using a Semi-Markov Reward
Process. We evaluate the model framework in predicting intensive care mortality
and show that TD learning under this framework can result in improved model
robustness compared to standard supervised learning methods. and that this
robustness is maintained even when validated on external datasets. This
approach may offer a more reliable method when learning to predict patient
outcomes using high-variance irregular time series data.

ÊëòË¶ÅÔºöÈ†êÊ∏¨Èï∑ÊúüÊÇ£ËÄÖÁµêÊûúÁöÑ‰ªªÂãô‰ΩøÁî®Áõ£Áù£ÂºèÊ©üÂô®Â≠∏ÁøíÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãôÔºåÈÉ®ÂàÜÂéüÂõ†ÊòØÊØèÂÄãÊÇ£ËÄÖÁöÑËªåË∑°ÁöÑËÆäÁï∞ÊÄßÂæàÈ´òÔºåÈÄôÂèØËÉΩÂ∞éËá¥Ê®°ÂûãÈÅéÂ∫¶Êì¨ÂêàÂà∞Ë®ìÁ∑¥Êï∏Êìö„ÄÇÊôÇÈñìÂ∑ÆÂàÜ (TD) Â≠∏ÁøíÔºå‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÂº∑ÂåñÂ≠∏ÁøíÊäÄË°ìÔºåÂèØ‰ª•ÈÄöÈÅéÂ∞áÂ≠∏ÁøíÊ¶ÇÊã¨ÁÇ∫ÁãÄÊÖãËΩâÊèõÊ®°ÂºèËÄå‰∏çÊòØÁµÇÁ´ØÁµêÊûú‰æÜÊ∏õÂ∞ëËÆäÁï∞„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÔºåÈÄôÁ®ÆÊñπÊ≥ïÈúÄË¶ÅÂ∞çÊÇ£ËÄÖÁãÄÊÖãÂÅöÂá∫ÂπæÂÄãÂº∑ÊúâÂäõÁöÑÂÅáË®≠ÔºåËÄå‰∏î‰ºº‰πéÊúâÈôêÁöÑÊñáÁçªË©ï‰º∞‰∫Ü TD Â≠∏ÁøíÁõ∏Â∞çÊñºÂÇ≥Áµ±Áõ£Áù£ÂºèÂ≠∏ÁøíÊñπÊ≥ïÂú®Èï∑ÊúüÂÅ•Â∫∑ÁµêÊûúÈ†êÊ∏¨‰ªªÂãô‰∏≠ÁöÑÊÄßËÉΩ„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂÆöÁæ©‰∫Ü‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÁî®ÊñºÂ∞á TD Â≠∏ÁøíÊáâÁî®Êñº‰ΩøÁî®ÂçäÈ¶¨ÁàæÂèØÂ§´ÁçéÂãµÈÅéÁ®ãÁöÑÂØ¶ÊôÇ‰∏çË¶èÂâáÊé°Ê®£ÊôÇÈñìÂ∫èÂàóÊï∏Êìö„ÄÇÊàëÂÄëË©ï‰º∞‰∫ÜÊ®°ÂûãÊ°ÜÊû∂Âú®È†êÊ∏¨ÈáçÁóáÁõ£Ë≠∑Ê≠ª‰∫°Áéá‰∏≠ÁöÑË°®ÁèæÔºå‰∏¶Ë°®ÊòéÂú®ÈÄôÂÄãÊ°ÜÊû∂‰∏ãÁöÑ TD Â≠∏ÁøíÂèØ‰ª•Â∞éËá¥ËàáÊ®ôÊ∫ñÁõ£Áù£ÂºèÂ≠∏ÁøíÊñπÊ≥ïÁõ∏ÊØîÊ®°ÂûãÈ≠ØÊ£íÊÄßÂæóÂà∞ÊîπÂñÑ„ÄÇËÄå‰∏îÈÄôÁ®ÆÈ≠ØÊ£íÊÄßÂç≥‰ΩøÂú®Â§ñÈÉ®Êï∏ÊìöÈõÜ‰∏äÈ©óË≠â‰πüËÉΩ‰øùÊåÅ„ÄÇÂú®‰ΩøÁî®È´òËÆäÁï∞‰∏çË¶èÂâáÊôÇÈñìÂ∫èÂàóÊï∏ÊìöÂ≠∏ÁøíÈ†êÊ∏¨ÊÇ£ËÄÖÁµêÊûúÊôÇÔºåÈÄôÁ®ÆÊñπÊ≥ïÂèØËÉΩÊúÉÊèê‰æõ‰∏ÄÁ®ÆÊõ¥ÂèØÈù†ÁöÑÊñπÊ≥ï„ÄÇ

##### **Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?**
2411.04118v1 by Daniel P. Jeong, Saurabh Garg, Zachary C. Lipton, Michael Oberst

Several recent works seek to develop foundation models specifically for
medical applications, adapting general-purpose large language models (LLMs) and
vision-language models (VLMs) via continued pretraining on publicly available
biomedical corpora. These works typically claim that such domain-adaptive
pretraining (DAPT) improves performance on downstream medical tasks, such as
answering medical licensing exam questions. In this paper, we compare seven
public "medical" LLMs and two VLMs against their corresponding base models,
arriving at a different conclusion: all medical VLMs and nearly all medical
LLMs fail to consistently improve over their base models in the zero-/few-shot
prompting regime for medical question-answering (QA) tasks. For instance,
across the tasks and model pairs we consider in the 3-shot setting, medical
LLMs only outperform their base models in 12.1% of cases, reach a (statistical)
tie in 49.8% of cases, and are significantly worse than their base models in
the remaining 38.2% of cases. Our conclusions are based on (i) comparing each
medical model head-to-head, directly against the corresponding base model; (ii)
optimizing the prompts for each model separately; and (iii) accounting for
statistical uncertainty in comparisons. While these basic practices are not
consistently adopted in the literature, our ablations show that they
substantially impact conclusions. Our findings suggest that state-of-the-art
general-domain models may already exhibit strong medical knowledge and
reasoning capabilities, and offer recommendations to strengthen the conclusions
of future studies.

ÊëòË¶ÅÔºö<paragraph>ËøëÊúüÁöÑÂπæÈ†ÖÁ†îÁ©∂Ëá¥ÂäõÊñºÂ∞àÈñÄÈáùÂ∞çÈÜ´ÁôÇÊáâÁî®ÈñãÁôºÂü∫Á§éÊ®°ÂûãÔºåÈÄèÈÅéÂú®ÂÖ¨ÈñãÁöÑÁîüÁâ©ÈÜ´Â≠∏Ë™ûÊñôÂ∫´‰∏äÊåÅÁ∫åÈ†êË®ìÁ∑¥ÔºåË™øÊï¥ÈÄöÁî®ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)„ÄÇÈÄô‰∫õÁ†îÁ©∂ÈÄöÂ∏∏ËÅ≤Á®±ÔºåÈÄôÁ®ÆÈ†òÂüüÈÅ©ÊáâÊÄßÈ†êË®ìÁ∑¥ (DAPT) ËÉΩÊîπÂñÑ‰∏ãÊ∏∏ÈÜ´ÁôÇ‰ªªÂãôÁöÑÊïàËÉΩÔºå‰æãÂ¶ÇÂõûÁ≠îÈÜ´ÁôÇÂü∑ÁÖßËÄÉË©¶È°åÁõÆ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊØîËºÉ‰∫Ü‰∏ÉÂÄãÂÖ¨ÈñãÁöÑ„ÄåÈÜ´ÁôÇ„ÄçLLM ÂíåÂÖ©ÂÄã VLM ËàáÂÆÉÂÄëÂ∞çÊáâÁöÑÂü∫Êú¨Ê®°ÂûãÔºå‰∏¶ÂæóÂá∫‰∏çÂêåÁöÑÁµêË´ñÔºöÂú®ÈÜ´ÁôÇÂïèÈ°åÂõûÁ≠î (QA) ‰ªªÂãôÁöÑÈõ∂Ê¨°ÔºèÂ∞èÊ®£Êú¨ÊèêÁ§∫Ê©üÂà∂‰∏≠ÔºåÊâÄÊúâÈÜ´ÁôÇ VLM ÂíåÂπæ‰πéÊâÄÊúâÈÜ´ÁôÇ LLM ÈÉΩÁÑ°Ê≥ïÊåÅÁ∫åÂÑ™ÊñºÂÆÉÂÄëÁöÑÂü∫Êú¨Ê®°Âûã„ÄÇ‰æãÂ¶ÇÔºåÂú®ÊàëÂÄëÂú® 3 Ê¨°ÊèêÁ§∫Ë®≠ÂÆö‰∏≠ËÄÉÊÖÆÁöÑ‰ªªÂãôÂíåÊ®°ÂûãÈÖçÂ∞ç‰∏≠ÔºåÈÜ´ÁôÇ LLM ÂÉÖÂú® 12.1% ÁöÑÊÉÖÊ≥Å‰∏ãÂÑ™ÊñºÂÆÉÂÄëÁöÑÂü∫Êú¨Ê®°ÂûãÔºåÂú® 49.8% ÁöÑÊÉÖÊ≥Å‰∏ãÈÅîÂà∞ÔºàÁµ±Ë®àÔºâÂπ≥ÊâãÔºåËÄåÂú®ÂÖ∂È§ò 38.2% ÁöÑÊÉÖÊ≥Å‰∏ãÈ°ØËëó‰ΩéÊñºÂÆÉÂÄëÁöÑÂü∫Êú¨Ê®°Âûã„ÄÇÊàëÂÄëÁöÑÁµêË´ñÂü∫Êñº (i) Áõ¥Êé•ÈáùÂ∞çÂ∞çÊáâÁöÑÂü∫Êú¨Ê®°ÂûãÔºåÈÄê‰∏ÄÊØîËºÉÊØèÂÄãÈÜ´ÁôÇÊ®°ÂûãÔºõ(ii) ÂàÜÂà•ÈáùÂ∞çÊØèÂÄãÊ®°ÂûãÊúÄ‰Ω≥ÂåñÊèêÁ§∫Ôºõ‰ª•Âèä (iii) ËÄÉÊÖÆÊØîËºÉ‰∏≠ÁöÑÁµ±Ë®à‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÈõñÁÑ∂ÈÄô‰∫õÂü∫Êú¨ÂÅöÊ≥ï‰∏¶Êú™ÊåÅÁ∫åÊé°Áî®Âú®ÊñáÁçª‰∏≠Ôºå‰ΩÜÊàëÂÄëÁöÑÊ∂àËûçÁ†îÁ©∂Ë°®ÊòéÔºåÂÆÉÂÄëÊúÉÂ§ßÂπÖÂΩ±ÈüøÁµêË´ñ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÊúÄÂÖàÈÄ≤ÁöÑÈÄöÁî®È†òÂüüÊ®°ÂûãÂèØËÉΩÂ∑≤Á∂ìÂ±ïÁèæÂá∫Âº∑Â§ßÁöÑÈÜ´ÁôÇÁü•Ë≠òÂíåÊé®ÁêÜËÉΩÂäõÔºå‰∏¶ÊèêÂá∫Âª∫Ë≠∞‰ª•Âº∑ÂåñÊú™‰æÜÁ†îÁ©∂ÁöÑÁµêË´ñ„ÄÇ</paragraph>

##### **RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models**
2411.04097v1 by Maya Varma, Jean-Benoit Delbrouck, Zhihong Chen, Akshay Chaudhari, Curtis Langlotz

Fine-tuned vision-language models (VLMs) often capture spurious correlations
between image features and textual attributes, resulting in degraded zero-shot
performance at test time. Existing approaches for addressing spurious
correlations (i) primarily operate at the global image-level rather than
intervening directly on fine-grained image features and (ii) are predominantly
designed for unimodal settings. In this work, we present RaVL, which takes a
fine-grained perspective on VLM robustness by discovering and mitigating
spurious correlations using local image features rather than operating at the
global image level. Given a fine-tuned VLM, RaVL first discovers spurious
correlations by leveraging a region-level clustering approach to identify
precise image features contributing to zero-shot classification errors. Then,
RaVL mitigates the identified spurious correlation with a novel region-aware
loss function that enables the VLM to focus on relevant regions and ignore
spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with
various model architectures, data domains, and learned spurious correlations.
Our results show that RaVL accurately discovers (191% improvement over the
closest baseline) and mitigates (8.2% improvement on worst-group image
classification accuracy) spurious correlations. Qualitative evaluations on
general-domain and medical-domain VLMs confirm our findings.

ÊëòË¶ÅÔºöÂæÆË∞ÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÈÄöÂ∏∏‰ºöÊçïÊçâÂõæÂÉèÁâπÂæÅÂíåÊñáÊú¨Â±ûÊÄß‰πãÈó¥ÁöÑËôöÂÅáÁõ∏ÂÖ≥ÊÄßÔºåÂØºËá¥Âú®ÊµãËØïÊó∂Èõ∂Ê†∑Êú¨ÊÄßËÉΩ‰∏ãÈôç„ÄÇÁé∞ÊúâÁöÑËß£ÂÜ≥ËôöÂÅáÁõ∏ÂÖ≥ÊÄßÁöÑÊñπÊ≥ïÔºàiÔºâ‰∏ªË¶ÅÂú®ÂÖ®Â±ÄÂõæÂÉèÁ∫ßÂà´Êìç‰ΩúÔºåËÄå‰∏çÊòØÁõ¥Êé•Âπ≤È¢ÑÁªÜÁ≤íÂ∫¶ÁöÑÂõæÂÉèÁâπÂæÅÔºåÂπ∂‰∏îÔºàiiÔºâ‰∏ªË¶ÅËÆæËÆ°Áî®‰∫éÂçïÊ®°ÊÄÅËÆæÁΩÆ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü RaVLÔºåÂÆÉÈÄöËøá‰ΩøÁî®Â±ÄÈÉ®ÂõæÂÉèÁâπÂæÅËÄå‰∏çÊòØÂú®ÂÖ®Â±ÄÂõæÂÉèÁ∫ßÂà´Êìç‰ΩúÊù•ÂèëÁé∞ÂíåÂáèËΩªËôöÂÅáÁõ∏ÂÖ≥ÊÄßÔºå‰ªéËÄåÂØπ VLM È≤ÅÊ£íÊÄßÈááÂèñ‰∫ÜÁªÜÁ≤íÂ∫¶ÁöÑËßÜËßí„ÄÇÁªôÂÆö‰∏Ä‰∏™ÂæÆË∞ÉÁöÑ VLMÔºåRaVL È¶ñÂÖàÈÄöËøáÂà©Áî®Âå∫ÂüüÁ∫ßËÅöÁ±ªÊñπÊ≥ïÂèëÁé∞ËôöÂÅáÁõ∏ÂÖ≥ÊÄßÔºå‰ª•ËØÜÂà´ÂØºËá¥Èõ∂Ê†∑Êú¨ÂàÜÁ±ªÈîôËØØÁöÑÁ≤æÁ°ÆÂõæÂÉèÁâπÂæÅ„ÄÇÁÑ∂ÂêéÔºåRaVL ‰ΩøÁî®‰∏ÄÁßçÊñ∞È¢ñÁöÑÂå∫ÂüüÊÑüÁü•ÊçüÂ§±ÂáΩÊï∞Êù•ÂáèËΩªÂ∑≤ËØÜÂà´ÁöÑËôöÂÅáÁõ∏ÂÖ≥ÊÄßÔºåËØ•ÊçüÂ§±ÂáΩÊï∞‰Ωø VLM ËÉΩÂ§üÂú®ÂæÆË∞ÉÊúüÈó¥ÂÖ≥Ê≥®Áõ∏ÂÖ≥Âå∫ÂüüÂπ∂ÂøΩÁï•ËôöÂÅáÂÖ≥Á≥ª„ÄÇÊàë‰ª¨‰ΩøÁî® 654 ‰∏™ VLM ÂØπ RaVL ËøõË°å‰∫ÜËØÑ‰º∞ÔºåËøô‰∫õ VLM ÂÖ∑ÊúâÂêÑÁßçÊ®°ÂûãÊû∂ÊûÑ„ÄÅÊï∞ÊçÆÂüüÂíåÂ≠¶‰π†Âà∞ÁöÑËôöÂÅáÁõ∏ÂÖ≥ÊÄß„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåRaVL ÂáÜÁ°ÆÂú∞ÂèëÁé∞‰∫ÜÔºàÊØîÊúÄÊé•ËøëÁöÑÂü∫Á∫øÊèêÈ´ò‰∫Ü 191%ÔºâÂíåÂáèËΩª‰∫ÜÔºàÂú®ÊúÄÂ∑ÆÁªÑÂõæÂÉèÂàÜÁ±ªÂáÜÁ°ÆÊÄß‰∏äÊèêÈ´ò‰∫Ü 8.2%ÔºâËôöÂÅáÁõ∏ÂÖ≥ÊÄß„ÄÇÂØπÈÄöÁî®ÂüüÂíåÂåªÂ≠¶Âüü VLM ÁöÑÂÆöÊÄßËØÑ‰º∞ËØÅÂÆû‰∫ÜÊàë‰ª¨ÁöÑÂèëÁé∞„ÄÇ

##### **Aligning Characteristic Descriptors with Images for Human-Expert-like Explainability**
2411.04008v1 by Bharat Chandra Yalavarthi, Nalini Ratha

In mission-critical domains such as law enforcement and medical diagnosis,
the ability to explain and interpret the outputs of deep learning models is
crucial for ensuring user trust and supporting informed decision-making.
Despite advancements in explainability, existing methods often fall short in
providing explanations that mirror the depth and clarity of those given by
human experts. Such expert-level explanations are essential for the dependable
application of deep learning models in law enforcement and medical contexts.
Additionally, we recognize that most explanations in real-world scenarios are
communicated primarily through natural language. Addressing these needs, we
propose a novel approach that utilizes characteristic descriptors to explain
model decisions by identifying their presence in images, thereby generating
expert-like explanations. Our method incorporates a concept bottleneck layer
within the model architecture, which calculates the similarity between image
and descriptor encodings to deliver inherent and faithful explanations. Through
experiments in face recognition and chest X-ray diagnosis, we demonstrate that
our approach offers a significant contrast over existing techniques, which are
often limited to the use of saliency maps. We believe our approach represents a
significant step toward making deep learning systems more accountable,
transparent, and trustworthy in the critical domains of face recognition and
medical diagnosis.

ÊëòË¶ÅÔºöÂú®ÊâßÊ≥ïÂíåÂåªÁñóËØäÊñ≠Á≠â‰ªªÂä°ÂÖ≥ÈîÆÂûãÈ¢ÜÂüüÔºå
Ëß£ÈáäÂíåËØ†ÈáäÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑËæìÂá∫ÂØπ‰∫éÁ°Æ‰øùÁî®Êà∑‰ø°‰ªªÂíåÊîØÊåÅÁü•ÊÉÖÂÜ≥Á≠ñËá≥ÂÖ≥ÈáçË¶Å„ÄÇ
Â∞ΩÁÆ°ÂèØËß£ÈáäÊÄßÊñπÈù¢ÂèñÂæó‰∫ÜËøõÊ≠•Ôºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®Êèê‰æõËß£ÈáäÊó∂ÂæÄÂæÄËææ‰∏çÂà∞‰∫∫Á±ª‰∏ìÂÆ∂ÁªôÂá∫ÁöÑÊ∑±Â∫¶ÂíåÊ∏ÖÊô∞Â∫¶„ÄÇËøôÁßç‰∏ìÂÆ∂Á∫ßÂà´ÁöÑËß£ÈáäÂØπ‰∫éÂú®ÊâßÊ≥ïÂíåÂåªÁñóÁéØÂ¢É‰∏≠ÂèØÈù†Âú∞Â∫îÁî®Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãËá≥ÂÖ≥ÈáçË¶Å„ÄÇ
Ê≠§Â§ñÔºåÊàë‰ª¨ËÆ§ËØÜÂà∞ÔºåÂú®Áé∞ÂÆû‰∏ñÁïåÂú∫ÊôØ‰∏≠ÔºåÂ§ßÂ§öÊï∞Ëß£Èáä‰∏ªË¶ÅÊòØÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄËøõË°å‰∫§ÊµÅÁöÑ„ÄÇ‰∏∫‰∫ÜÊª°Ë∂≥Ëøô‰∫õÈúÄÊ±ÇÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂà©Áî®ÁâπÂæÅÊèèËø∞Á¨¶ÈÄöËøáËØÜÂà´ÂõæÂÉè‰∏≠ÁöÑÁâπÂæÅÊèèËø∞Á¨¶ÁöÑÂ≠òÂú®Êù•Ëß£ÈáäÊ®°ÂûãÂÜ≥Á≠ñÔºå‰ªéËÄåÁîüÊàêÁ±ª‰ºº‰∏ìÂÆ∂ÁöÑËß£Èáä„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Ê®°ÂûãÊû∂ÊûÑ‰∏≠Âä†ÂÖ•‰∫Ü‰∏Ä‰∏™Ê¶ÇÂøµÁì∂È¢àÂ±ÇÔºåËØ•Â±ÇËÆ°ÁÆóÂõæÂÉèÂíåÊèèËø∞Á¨¶ÁºñÁ†Å‰πãÈó¥ÁöÑÁõ∏‰ººÊÄßÔºå‰ª•Êèê‰æõÂÜÖÂú®‰∏îÂèØÈù†ÁöÑËß£Èáä„ÄÇÈÄöËøáÈù¢ÈÉ®ËØÜÂà´ÂíåËÉ∏ÈÉ® X Â∞ÑÁ∫øËØäÊñ≠ÁöÑÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ï‰∏éÁé∞ÊúâÊäÄÊúØÁõ∏ÊØîÂÖ∑ÊúâÊòæÁùÄ‰ºòÂäøÔºåËÄåÁé∞ÊúâÊäÄÊúØÈÄöÂ∏∏‰ªÖÈôê‰∫é‰ΩøÁî®ÊòæÁùÄÊÄßÂõæ„ÄÇÊàë‰ª¨Áõ∏‰ø°ÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ª£Ë°®‰∫ÜÊúùÁùÄ‰ΩøÊ∑±Â∫¶Â≠¶‰π†Á≥ªÁªüÂú®Èù¢ÈÉ®ËØÜÂà´ÂíåÂåªÁñóËØäÊñ≠ÁöÑÂÖ≥ÈîÆÈ¢ÜÂüüÊõ¥Âä†Ë¥üË¥£„ÄÅÈÄèÊòéÂíåÂÄºÂæó‰ø°ËµñËøàÂá∫ÁöÑÈáçË¶Å‰∏ÄÊ≠•„ÄÇ

##### **Fine-tuning -- a Transfer Learning approach**
2411.03941v1 by Joseph Arul Raj, Linglong Qian, Zina Ibrahim

Secondary research use of Electronic Health Records (EHRs) is often hampered
by the abundance of missing data in this valuable resource. Missingness in EHRs
occurs naturally as a result of the data recording practices during routine
clinical care, but handling it is crucial to the precision of medical analysis
and the decision-making that follows. The literature contains a variety of
imputation methodologies based on deep neural networks. Those aim to overcome
the dynamic, heterogeneous and multivariate missingness patterns of EHRs, which
cannot be handled by classical and statistical imputation methods. However, all
existing deep imputation methods rely on end-to-end pipelines that incorporate
both imputation and downstream analyses, e.g. classification. This coupling
makes it difficult to assess the quality of imputation and takes away the
flexibility of re-using the imputer for a different task. Furthermore, most
end-to-end deep architectures tend to use complex networks to perform the
downstream task, in addition to the already sophisticated deep imputation
network. We, therefore ask if the high performance reported in the literature
is due to the imputer or the classifier and further ask if an optimised
state-of-the-art imputer is used, a simpler classifier can achieve comparable
performance. This paper explores the development of a modular, deep
learning-based imputation and classification pipeline, specifically built to
leverage the capabilities of state-of-the-art imputation models for downstream
classification tasks. Such a modular approach enables a) objective assessment
of the quality of the imputer and classifier independently, and b) enables the
exploration of the performance of simpler classification architectures using an
optimised imputer.

ÊëòË¶ÅÔºöÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) ÁöÑ‰∫åÊ¨°Á†îÁ©∂Áî®ÈÄîÁ∂ìÂ∏∏ÂèóÂà∞Ê≠§ÂØ∂Ë≤¥Ë≥áÊ∫ê‰∏≠Â§ßÈáèÈÅ∫Â§±Ë≥áÊñôÁöÑÈòªÁ§ô„ÄÇEHR ‰∏≠ÁöÑÈÅ∫Â§±Ë≥áÊñôÊúÉÂú®‰æãË°åËá®Â∫äÁÖßË≠∑ÊúüÈñìÁöÑË≥áÊñôË®òÈåÑÂØ¶Âãô‰∏≠Ëá™ÁÑ∂ÁôºÁîüÔºå‰ΩÜËôïÁêÜÈÅ∫Â§±Ë≥áÊñôÂ∞çÊñºÈÜ´ÁôÇÂàÜÊûêÁöÑÁ≤æÁ¢∫Â∫¶ÂíåÂæåÁ∫åÊ±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇÊñáÁçª‰∏≠ÂåÖÂê´ÂêÑÁ®ÆÂü∫ÊñºÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂÖßÊèíÊñπÊ≥ï„ÄÇÈÄô‰∫õÊñπÊ≥ïÊó®Âú®ÂÖãÊúç EHR ‰∏≠ÂãïÊÖã„ÄÅÁï∞Ë≥™‰∏îÂ§öËÆäÈáèÁöÑÈÅ∫Â§±Ë≥áÊñôÊ®°ÂºèÔºåËÄåÈÄôÁÑ°Ê≥ïÈÄèÈÅéÂÇ≥Áµ±ÂíåÁµ±Ë®àÂÖßÊèíÊñπÊ≥ï‰æÜËôïÁêÜ„ÄÇÁÑ∂ËÄåÔºåÊâÄÊúâÁèæÊúâÁöÑÊ∑±Â∫¶ÂÖßÊèíÊñπÊ≥ïÈÉΩ‰æùË≥¥ÊñºÂ∞áÂÖßÊèíÂíå‰∏ãÊ∏∏ÂàÜÊûêÔºà‰æãÂ¶ÇÂàÜÈ°ûÔºâÁµêÂêàÂú®‰∏ÄËµ∑ÁöÑÁ´ØÂà∞Á´ØÁÆ°ÈÅì„ÄÇÈÄôÁ®ÆÁµêÂêà‰ΩøÂæóÈõ£‰ª•Ë©ï‰º∞ÂÖßÊèíÁöÑÂìÅË≥™Ôºå‰∏¶Ê∂àÈô§‰∫ÜÈáçÊñ∞‰ΩøÁî®ÂÖßÊèíÂô®ÈÄ≤Ë°å‰∏çÂêå‰ªªÂãôÁöÑÈùàÊ¥ªÊÄß„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏Á´ØÂà∞Á´ØÊ∑±Â∫¶Êû∂ÊßãÂÇæÂêëÊñº‰ΩøÁî®Ë§áÈõúÁöÑÁ∂≤Ë∑Ø‰æÜÂü∑Ë°å‰∏ãÊ∏∏‰ªªÂãôÔºåÈô§‰∫ÜÂ∑≤Á∂ìÂæàË§áÈõúÁöÑÊ∑±Â∫¶ÂÖßÊèíÁ∂≤Ë∑Ø‰πãÂ§ñ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëË©¢ÂïèÊñáÁçª‰∏≠Â†±Â∞éÁöÑÈ´òÊïàËÉΩÊòØÁî±ÊñºÂÖßÊèíÂô®ÈÇÑÊòØÂàÜÈ°ûÂô®Ôºå‰∏¶ÈÄ≤‰∏ÄÊ≠•Ë©¢ÂïèÊòØÂê¶‰ΩøÁî®‰∫ÜÊúÄ‰Ω≥ÂåñÁöÑÊúÄÊñ∞ÂÖßÊèíÂô®ÔºåËºÉÁ∞°ÂñÆÁöÑÂàÜÈ°ûÂô®ÊòØÂê¶ÂèØ‰ª•ÈÅîÂà∞Áõ∏ËøëÁöÑÊïàËÉΩ„ÄÇÊú¨ÊñáÊé¢Ë®éÊ®°ÁµÑÂåñ„ÄÅÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂÖßÊèíÂíåÂàÜÈ°ûÁÆ°ÈÅìÁöÑÈñãÁôºÔºåÁâπÂà•ÊòØÂª∫Êßã‰æÜÂà©Áî®ÊúÄÊñ∞ÂÖßÊèíÊ®°ÂûãÁöÑËÉΩÂäõÔºå‰ª•ÈÄ≤Ë°å‰∏ãÊ∏∏ÂàÜÈ°û‰ªªÂãô„ÄÇÈÄôÁ®ÆÊ®°ÁµÑÂåñÊñπÊ≥ïËÉΩ a) ÂÆ¢ËßÄË©ï‰º∞ÂÖßÊèíÂô®ÂíåÂàÜÈ°ûÂô®ÁöÑÂìÅË≥™Ôºå‰ª•Âèä b) ËÉΩÂ§†‰ΩøÁî®ÊúÄ‰Ω≥ÂåñÁöÑÂÖßÊèíÂô®‰æÜÊé¢Ë®éËºÉÁ∞°ÂñÆÂàÜÈ°ûÊû∂ÊßãÁöÑÊïàËÉΩ„ÄÇ

##### **MEG: Medical Knowledge-Augmented Large Language Models for Question Answering**
2411.03883v2 by Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders S√∏gaard, Carlos Bobed

Question answering is a natural language understanding task that involves
reasoning over both explicit context and unstated, relevant domain knowledge.
Large language models (LLMs), which underpin most contemporary question
answering systems, struggle to induce how concepts relate in specialized
domains such as medicine. Existing medical LLMs are also costly to train. In
this work, we present MEG, a parameter-efficient approach for medical
knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate
graph embeddings into the LLM, enabling it to leverage external knowledge in a
cost-effective way. We evaluate our method on four popular medical
multiple-choice datasets and show that LLMs greatly benefit from the factual
grounding provided by knowledge graph embeddings. MEG attains an average of
+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized
models like BioMistral. We also show results based on Llama-3. Finally, we show
that MEG's performance remains robust to the choice of graph encoder.

ÊëòË¶ÅÔºöÂïèÁ≠îÊòØËá™ÁÑ∂Ë™ûË®ÄÁêÜËß£‰ªªÂãôÔºåÊ∂âÂèäÂ∞çÊòéÁ¢∫ÁöÑ‰∏ä‰∏ãÊñáÂíåÊú™Ë™™ÊòéÁöÑÁõ∏ÈóúÈ†òÂüüÁü•Ë≠òÈÄ≤Ë°åÊé®ÁêÜ„ÄÇÊîØÊíêÂ§ßÂ§öÊï∏Áï∂‰ª£ÂïèÁ≠îÁ≥ªÁµ±ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Èõ£‰ª•Êé®Ë´ñÊ¶ÇÂøµÂ¶Ç‰ΩïÂú®ÈÜ´Â≠∏Á≠âÂ∞àÊ•≠È†òÂüü‰∏≠ÈóúËÅØ„ÄÇÁèæÊúâÁöÑÈÜ´Â≠∏ LLM Ë®ìÁ∑¥ÊàêÊú¨‰πüÂæàÈ´ò„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü MEGÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®ÊñºÈÜ´Â≠∏Áü•Ë≠òÂ¢ûÂº∑ LLM ÁöÑÂèÉÊï∏ÊúâÊïàÊñπÊ≥ï„ÄÇMEG ‰ΩøÁî®ËºïÈáèÁ¥öÊò†Â∞ÑÁ∂≤Ë∑ØÂ∞áÂúñË°®ÂµåÂÖ•Êï¥ÂêàÂà∞ LLM ‰∏≠Ôºå‰ΩøÂÖ∂ËÉΩÂ§†‰ª•Á∂ìÊøüÊúâÊïàÁöÑÊñπÂºèÂà©Áî®Â§ñÈÉ®Áü•Ë≠ò„ÄÇÊàëÂÄëÂú®ÂõõÂÄãÊµÅË°åÁöÑÈÜ´Â≠∏Â§öÈÅ∏È°åË≥áÊñôÈõÜ‰∏äË©ï‰º∞‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÔºå‰∏¶Ë°®Êòé LLM ÂæûÁü•Ë≠òÂúñË°®ÂµåÂÖ•Êèê‰æõÁöÑÂØ¶Èöõ‰æùÊìö‰∏≠ÂèóÁõäÂå™Ê∑∫„ÄÇMEG Âú® Mistral-Instruct Âü∫Ê∫ñ‰∏äÂπ≥ÂùáÊèêÈ´ò‰∫Ü +10.2% ÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÂú® BioMistral Á≠âÂ∞àÈñÄÊ®°Âûã‰∏äÊèêÈ´ò‰∫Ü +6.7%„ÄÇÊàëÂÄëÈÇÑÂ±ïÁ§∫‰∫ÜÂü∫Êñº Llama-3 ÁöÑÁµêÊûú„ÄÇÊúÄÂæåÔºåÊàëÂÄëË°®Êòé MEG ÁöÑÊÄßËÉΩÂ∞çÂúñË°®Á∑®Á¢ºÂô®ÁöÑÈÅ∏Êìá‰øùÊåÅÁ©©ÂÅ•„ÄÇ

##### **Navigating the landscape of multimodal AI in medicine: a scoping review on technical challenges and clinical applications**
2411.03782v1 by Daan Schouten, Giulia Nicoletti, Bas Dille, Catherine Chia, Pierpaolo Vendittelli, Megan Schuurmans, Geert Litjens, Nadieh Khalili

Recent technological advances in healthcare have led to unprecedented growth
in patient data quantity and diversity. While artificial intelligence (AI)
models have shown promising results in analyzing individual data modalities,
there is increasing recognition that models integrating multiple complementary
data sources, so-called multimodal AI, could enhance clinical decision-making.
This scoping review examines the landscape of deep learning-based multimodal AI
applications across the medical domain, analyzing 432 papers published between
2018 and 2024. We provide an extensive overview of multimodal AI development
across different medical disciplines, examining various architectural
approaches, fusion strategies, and common application areas. Our analysis
reveals that multimodal AI models consistently outperform their unimodal
counterparts, with an average improvement of 6.2 percentage points in AUC.
However, several challenges persist, including cross-departmental coordination,
heterogeneous data characteristics, and incomplete datasets. We critically
assess the technical and practical challenges in developing multimodal AI
systems and discuss potential strategies for their clinical implementation,
including a brief overview of commercially available multimodal AI models for
clinical decision-making. Additionally, we identify key factors driving
multimodal AI development and propose recommendations to accelerate the field's
maturation. This review provides researchers and clinicians with a thorough
understanding of the current state, challenges, and future directions of
multimodal AI in medicine.

ÊëòË¶ÅÔºöÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑËøëÊúüÁßëÊäÄÈÄ≤Â±ïÂ∞éËá¥ÁóÖÊÇ£Ë≥áÊñôÊï∏ÈáèÂíåÂ§öÊ®£ÊÄßÂâçÊâÄÊú™ÊúâÁöÑÊàêÈï∑„ÄÇÂÑòÁÆ°‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÂú®ÂàÜÊûêÂÄãÂà•Ë≥áÊñôÊ®°Âºè‰∏≠Â±ïÁèæÂá∫ÊúâÂâçÈÄîÁöÑÊàêÊûúÔºå‰ΩÜÊï¥ÂêàÂ§öÂÄã‰∫íË£úË≥áÊñô‰æÜÊ∫êÁöÑÊ®°ÂûãÔºåÂç≥ÊâÄË¨ÇÁöÑÂ§öÊ®°Âºè AIÔºåÂèØ‰ª•ÊèêÂçáËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÔºåÈÄôÈ†ÖË™çÁü•Ê≠£ËàáÊó•‰ø±Â¢û„ÄÇÈÄôÁØáÁØÑÂúçÊé¢Ë®éÂõûÈ°ßÁ†îÁ©∂Êé¢Ë®é‰∫ÜÊ∂µËìãÈÜ´ÁôÇÈ†òÂüüÁöÑÊ∑±Â∫¶Â≠∏ÁøíÂü∫Á§éÂ§öÊ®°Âºè AI ÊáâÁî®ÁèæÊ≥ÅÔºåÂàÜÊûê 2018 Âπ¥Ëá≥ 2024 Âπ¥ÈñìÁôºË°®ÁöÑ 432 ÁØáË´ñÊñá„ÄÇÊàëÂÄëÊèê‰æõ‰∫ÜÂ§öÊ®°Âºè AI ÁôºÂ±ïÁöÑÂª£Ê≥õÊ¶ÇËßÄÔºåÊ∂µËìã‰∏çÂêåÁöÑÈÜ´ÁôÇÈ†òÂüüÔºåÊé¢Ë®éÂêÑÁ®ÆÊû∂ÊßãÊñπÊ≥ï„ÄÅËûçÂêàÁ≠ñÁï•ÂíåÂ∏∏Ë¶ãÊáâÁî®È†òÂüü„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫ÔºåÂ§öÊ®°Âºè AI Ê®°ÂûãÂßãÁµÇÂÑ™ÊñºÂÖ∂ÂñÆ‰∏ÄÊ®°ÂºèÁöÑÂ∞çÊáâÊ®°ÂûãÔºåAUC Âπ≥ÂùáÊîπÂñÑ 6.2 ÂÄãÁôæÂàÜÈªû„ÄÇÁÑ∂ËÄåÔºå‰ªçÊúâË®±Â§öÊåëÊà∞ÊåÅÁ∫åÂ≠òÂú®ÔºåÂåÖÊã¨Ë∑®ÈÉ®ÈñÄÂçîË™ø„ÄÅÁï∞Ë≥™Ë≥áÊñôÁâπÊÄßÂíå‰∏çÂÆåÊï¥Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÊâπÂà§ÊÄßÂú∞Ë©ï‰º∞ÈñãÁôºÂ§öÊ®°Âºè AI Á≥ªÁµ±Âú®ÊäÄË°ìÂíåÂØ¶Âãô‰∏äÁöÑÊåëÊà∞Ôºå‰∏¶Ë®éË´ñÂÖ∂Ëá®Â∫äÂØ¶‰ΩúÁöÑÊΩõÂú®Á≠ñÁï•ÔºåÂåÖÊã¨Â∞çÂ∏ÇÂîÆÂ§öÊ®°Âºè AI Ê®°ÂûãÁöÑÁ∞°Ë¶ÅÊ¶ÇËø∞ÔºåÁî®ÊñºËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊâæÂá∫Êé®ÂãïÂ§öÊ®°Âºè AI ÁôºÂ±ïÁöÑ‰∏ªË¶ÅÂõ†Á¥†Ôºå‰∏¶ÊèêÂá∫Âª∫Ë≠∞‰ª•Âä†ÈÄüË©≤È†òÂüüÁöÑÊàêÁÜü„ÄÇÊú¨ÂõûÈ°ßÁ†îÁ©∂ËÆìÁ†îÁ©∂‰∫∫Âì°ÂíåËá®Â∫äÈÜ´Â∏´Ê∑±ÂÖ•‰∫ÜËß£Â§öÊ®°Âºè AI Âú®ÈÜ´Â≠∏È†òÂüüÁöÑÁèæÊ≥Å„ÄÅÊåëÊà∞ÂíåÊú™‰æÜÊñπÂêë„ÄÇ

##### **Sub-DM:Subspace Diffusion Model with Orthogonal Decomposition for MRI Reconstruction**
2411.03758v1 by Yu Guan, Qinrong Cai, Wei Li, Qiuyun Fan, Dong Liang, Qiegen Liu

Diffusion model-based approaches recently achieved re-markable success in MRI
reconstruction, but integration into clinical routine remains challenging due
to its time-consuming convergence. This phenomenon is partic-ularly notable
when directly apply conventional diffusion process to k-space data without
considering the inherent properties of k-space sampling, limiting k-space
learning efficiency and image reconstruction quality. To tackle these
challenges, we introduce subspace diffusion model with orthogonal
decomposition, a method (referred to as Sub-DM) that restrict the diffusion
process via projections onto subspace as the k-space data distribution evolves
toward noise. Particularly, the subspace diffusion model circumvents the
inference challenges posed by the com-plex and high-dimensional characteristics
of k-space data, so the highly compact subspace ensures that diffusion process
requires only a few simple iterations to produce accurate prior information.
Furthermore, the orthogonal decomposition strategy based on wavelet transform
hin-ders the information loss during the migration of the vanilla diffusion
process to the subspace. Considering the strate-gy is approximately reversible,
such that the entire pro-cess can be reversed. As a result, it allows the
diffusion processes in different spaces to refine models through a mutual
feedback mechanism, enabling the learning of ac-curate prior even when dealing
with complex k-space data. Comprehensive experiments on different datasets
clearly demonstrate that the superiority of Sub-DM against state of-the-art
methods in terms of reconstruction speed and quality.

ÊëòË¶ÅÔºöÂü∫ÊñºÊì¥Êï£Ê®°ÂûãÁöÑÊñπÊ≥ïÊúÄËøëÂú® MRI ÈáçÂª∫‰∏≠ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊàêÂäüÔºå‰ΩÜÁî±ÊñºÂÖ∂ËÄóÊôÇÁöÑÊî∂ÊñÇÊÄßÔºåÊï¥ÂêàÂà∞Ëá®Â∫äÂ∏∏Ë¶è‰∏≠‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÁï∂Áõ¥Êé•Â∞áÂÇ≥Áµ±Êì¥Êï£ÈÅéÁ®ãÊáâÁî®Âà∞ k-space Ë≥áÊñôÔºåËÄåÊ≤íÊúâËÄÉÊÖÆ k-space ÂèñÊ®£ÁöÑÂõ∫ÊúâÁâπÊÄßÊôÇÔºåÈÄôÁ®ÆÁèæË±°Â∞§ÂÖ∂ÊòéÈ°ØÔºåÈôêÂà∂‰∫Ü k-space Â≠∏ÁøíÊïàÁéáÂíåÂΩ±ÂÉèÈáçÂª∫ÂìÅË≥™„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂÖ∑ÊúâÊ≠£‰∫§ÂàÜËß£ÁöÑÂ≠êÁ©∫ÈñìÊì¥Êï£Ê®°ÂûãÔºå‰∏ÄÁ®ÆÊñπÊ≥ïÔºàÁ®±ÁÇ∫ Sub-DMÔºâÔºåÂÆÉÈÄöÈÅéÊäïÂΩ±Âà∞Â≠êÁ©∫Èñì‰æÜÈôêÂà∂Êì¥Êï£ÈÅéÁ®ãÔºåÂõ†ÁÇ∫ k-space Ë≥áÊñôÂàÜ‰ΩàÊúÉÊºîËÆäÊàêÈõúË®ä„ÄÇÁâπÂà•ÊòØÔºåÂ≠êÁ©∫ÈñìÊì¥Êï£Ê®°ÂûãËø¥ÈÅø‰∫Ü k-space Ë≥áÊñôÁöÑË§áÈõúÂíåÈ´òÁ∂≠ÁâπÂæµÊâÄÂ∏∂‰æÜÁöÑÊé®Ë´ñÊåëÊà∞ÔºåÂõ†Ê≠§È´òÂ∫¶Á∑äÊπäÁöÑÂ≠êÁ©∫ÈñìÁ¢∫‰øùÊì¥Êï£ÈÅéÁ®ãÂè™ÈúÄË¶ÅÂπæÂÄãÁ∞°ÂñÆÁöÑËø≠‰ª£Âç≥ÂèØÁî¢ÁîüÊ∫ñÁ¢∫ÁöÑÂÖàÈ©óË≥áË®ä„ÄÇÊ≠§Â§ñÔºåÂü∫ÊñºÂ∞èÊ≥¢ËΩâÊèõÁöÑÊ≠£‰∫§ÂàÜËß£Á≠ñÁï•ÈòªÁ§ô‰∫ÜÈ¶ôËçâÊì¥Êï£ÈÅéÁ®ãÈÅ∑ÁßªÂà∞Â≠êÁ©∫ÈñìÊúüÈñìÁöÑË≥áË®äÈÅ∫Â§±„ÄÇËÄÉÊÖÆÂà∞Ë©≤Á≠ñÁï•Ëøë‰ººÂèØÈÄÜÔºåÂõ†Ê≠§Êï¥ÂÄãÈÅéÁ®ãÂèØ‰ª•ÈÄÜËΩâ„ÄÇÂõ†Ê≠§ÔºåÂÆÉÂÖÅË®±‰∏çÂêåÁ©∫Èñì‰∏≠ÁöÑÊì¥Êï£ÈÅéÁ®ãÈÄöÈÅéÁõ∏‰∫íÂõûÈ•ãÊ©üÂà∂‰æÜÂÑ™ÂåñÊ®°ÂûãÔºåÂç≥‰ΩøÂú®ËôïÁêÜË§áÈõúÁöÑ k-space Ë≥áÊñôÊôÇ‰πüËÉΩÂ≠∏ÁøíÊ∫ñÁ¢∫ÁöÑÂÖàÈ©ó„ÄÇÂú®‰∏çÂêåË≥áÊñôÈõÜ‰∏äÁöÑÂÖ®Èù¢ÂØ¶È©óÊ∏ÖÊ•öÂú∞Ë≠âÊòé‰∫Ü Sub-DM Âú®ÈáçÂª∫ÈÄüÂ∫¶ÂíåÂìÅË≥™ÊñπÈù¢ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇ

##### **Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?**
2411.03670v1 by Pedro R. A. S. Bassi, Wenxuan Li, Yucheng Tang, Fabian Isensee, Zifu Wang, Jieneng Chen, Yu-Cheng Chou, Yannick Kirchhoff, Maximilian Rokuss, Ziyan Huang, Jin Ye, Junjun He, Tassilo Wald, Constantin Ulrich, Michael Baumgartner, Saikat Roy, Klaus H. Maier-Hein, Paul Jaeger, Yiwen Ye, Yutong Xie, Jianpeng Zhang, Ziyang Chen, Yong Xia, Zhaohu Xing, Lei Zhu, Yousef Sadegheih, Afshin Bozorgpour, Pratibha Kumari, Reza Azad, Dorit Merhof, Pengcheng Shi, Ting Ma, Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao, Haonan Wang, Xiaomeng Li, Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski, Saumya Gupta, Linshan Wu, Jiaxin Zhuang, Hao Chen, Holger Roth, Daguang Xu, Matthew B. Blaschko, Sergio Decherchi, Andrea Cavalli, Alan L. Yuille, Zongwei Zhou

How can we test AI performance? This question seems trivial, but it isn't.
Standard benchmarks often have problems such as in-distribution and small-size
test sets, oversimplified metrics, unfair comparisons, and short-term outcome
pressure. As a consequence, good performance on standard benchmarks does not
guarantee success in real-world scenarios. To address these problems, we
present Touchstone, a large-scale collaborative segmentation benchmark of 9
types of abdominal organs. This benchmark is based on 5,195 training CT scans
from 76 hospitals around the world and 5,903 testing CT scans from 11
additional hospitals. This diverse test set enhances the statistical
significance of benchmark results and rigorously evaluates AI algorithms across
various out-of-distribution scenarios. We invited 14 inventors of 19 AI
algorithms to train their algorithms, while our team, as a third party,
independently evaluated these algorithms on three test sets. In addition, we
also evaluated pre-existing AI frameworks--which, differing from algorithms,
are more flexible and can support different algorithms--including MONAI from
NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are
committed to expanding this benchmark to encourage more innovation of AI
algorithms for the medical domain.

ÊëòË¶ÅÔºöÂ¶Ç‰ΩïÊ∏¨Ë©¶ AI ÊïàËÉΩÔºüÈÄôÂÄãÂïèÈ°åÁúã‰ººÁ∞°ÂñÆÔºå‰ΩÜ‰∏¶ÈùûÂ¶ÇÊ≠§„ÄÇ
Ê®ôÊ∫ñÂü∫Ê∫ñÁ∂ìÂ∏∏ÊúâË´∏Â¶ÇÂàÜ‰ΩàÂÖßÂíåÂ∞èÂûãÊ∏¨Ë©¶ÈõÜ„ÄÅÈÅéÊñºÁ∞°ÂåñÁöÑÊåáÊ®ô„ÄÅ‰∏çÂÖ¨Âπ≥ÁöÑÊØîËºÉÂíåÁü≠ÊúüÁµêÊûúÂ£ìÂäõÁ≠âÂïèÈ°å„ÄÇÂõ†Ê≠§ÔºåÂú®Ê®ôÊ∫ñÂü∫Ê∫ñ‰∏äÁöÑËâØÂ•ΩÊïàËÉΩÁÑ°Ê≥ï‰øùË≠âÂú®ÂØ¶ÈöõÊÉÖÊ≥Å‰∏≠‰πüËÉΩÊàêÂäü„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü TouchstoneÔºå‰∏ÄÁ®ÆÂ§ßÂûãÂçî‰ΩúÂàÜÂâ≤Âü∫Ê∫ñÔºåÂåÖÂê´ 9 Á®ÆÈ°ûÂûãÁöÑËÖπÈÉ®Âô®ÂÆò„ÄÇÊ≠§Âü∫Ê∫ñÂü∫Êñº‰æÜËá™ÂÖ®ÁêÉ 76 ÂÆ∂ÈÜ´Èô¢ÁöÑ 5,195 ÂÄãË®ìÁ∑¥ CT ÊéÉÊèèÂíå‰æÜËá™ 11 ÂÆ∂ÂÖ∂‰ªñÈÜ´Èô¢ÁöÑ 5,903 ÂÄãÊ∏¨Ë©¶ CT ÊéÉÊèè„ÄÇÈÄôÂÄãÂ§öÊ®£ÂåñÁöÑÊ∏¨Ë©¶ÈõÜÂ¢ûÂº∑‰∫ÜÂü∫Ê∫ñÁµêÊûúÁöÑÁµ±Ë®àÈ°ØËëóÊÄßÔºå‰∏¶Âö¥Ê†ºË©ï‰º∞‰∫ÜÂêÑÁ®ÆÂàÜ‰ΩàÂ§ñÊÉÖÊ≥Å‰∏ãÁöÑ AI ÊºîÁÆóÊ≥ï„ÄÇÊàëÂÄëÈÇÄË´ã‰∫Ü 19 Á®Æ AI ÊºîÁÆóÊ≥ïÁöÑ 14 ‰ΩçÁôºÊòéËÄÖË®ìÁ∑¥‰ªñÂÄëÁöÑÊºîÁÆóÊ≥ïÔºåËÄåÊàëÂÄëÁöÑÂúòÈöä‰ΩúÁÇ∫Á¨¨‰∏âÊñπÔºåÁç®Á´ãË©ï‰º∞‰∫ÜÈÄô‰∫õÊºîÁÆóÊ≥ïÂú®‰∏âÂÄãÊ∏¨Ë©¶ÈõÜ‰∏äÁöÑË°®Áèæ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑË©ï‰º∞‰∫ÜÁèæÊúâÁöÑ AI Ê°ÜÊû∂ÔºåÈÄô‰∫õÊ°ÜÊû∂ËàáÊºîÁÆóÊ≥ï‰∏çÂêåÔºåÊõ¥ÂÖ∑ÂΩàÊÄßÔºå‰∏îÂèØ‰ª•ÊîØÊè¥‰∏çÂêåÁöÑÊºîÁÆóÊ≥ïÔºåÂåÖÊã¨ NVIDIA ÁöÑ MONAI„ÄÅDKFZ ÁöÑ nnU-Net ÂíåË®±Â§öÂÖ∂‰ªñÈñãÊ∫êÊ°ÜÊû∂„ÄÇÊàëÂÄëËá¥ÂäõÊñºÊì¥Â±ïÊ≠§Âü∫Ê∫ñÔºå‰ª•ÈºìÂãµÊõ¥Â§ö AI ÊºîÁÆóÊ≥ïÂú®ÈÜ´ÁôÇÈ†òÂüüÁöÑÂâµÊñ∞„ÄÇ

##### **Requirements Engineering for Older Adult Digital Health Software: A Systematic Literature Review**
2411.03656v1 by Yuqing Xiao, John Grundy, Anuradha Madugalla

Growth of the older adult population has led to an increasing interest in
technology-supported aged care. However, the area has some challenges such as a
lack of caregivers and limitations in understanding the emotional, social,
physical, and mental well-being needs of seniors. Furthermore, there is a gap
in the understanding between developers and ageing people of their
requirements. Digital health can be important in supporting older adults
wellbeing, emotional requirements, and social needs. Requirements Engineering
(RE) is a major software engineering field, which can help to identify, elicit
and prioritize the requirements of stakeholders and ensure that the systems
meet standards for performance, reliability, and usability. We carried out a
systematic review of the literature on RE for older adult digital health
software. This was necessary to show the representatives of the current stage
of understanding the needs of older adults in aged care digital health. Using
established guidelines outlined by the Kitchenham method, the PRISMA and the
PICO guideline, we developed a protocol, followed by the systematic exploration
of eight databases. This resulted in 69 primary studies of high relevance,
which were subsequently subjected to data extraction, synthesis, and reporting.
We highlight key RE processes in digital health software for ageing people. It
explored the utilization of technology for older user well-being and care, and
the evaluations of such solutions. The review also identified key limitations
found in existing primary studies that inspire future research opportunities.
The results indicate that requirement gathering and understanding have a
significant variation between different studies. The differences are in the
quality, depth, and techniques adopted for requirement gathering and these
differences are largely due to uneven adoption of RE methods.

ÊëòË¶ÅÔºöÈ´òÈΩ°‰∫∫Âè£ÁöÑÂ¢ûÈï∑ÔºåÂ∞éËá¥Â∞çÁßëÊäÄËºîÂä©Èï∑ÁÖßÊúçÂãôÁöÑÈúÄÊ±ÇËàáÊó•‰ø±Â¢û„ÄÇÁÑ∂ËÄåÔºåË©≤È†òÂüü‰πüÈù¢Ëá®‰∏Ä‰∫õÊåëÊà∞Ôºå‰æãÂ¶ÇÁÖßË≠∑‰∫∫Âì°ÁöÑÁü≠Áº∫Ôºå‰ª•ÂèäÂú®ÁêÜËß£Èï∑ËÄÖÂú®ÊÉÖÁ∑í„ÄÅÁ§æ‰∫§„ÄÅÁîüÁêÜÂíåÂøÉÁêÜÊñπÈù¢ÁöÑÁ¶èÁ•âÈúÄÊ±ÇÊôÇÊâÄÂ≠òÂú®ÁöÑÈôêÂà∂„ÄÇÊ≠§Â§ñÔºåÈñãÁôº‰∫∫Âì°ÂíåÈï∑ËÄÖÂú®ÈúÄÊ±ÇÁêÜËß£‰∏ä‰πüÂ≠òÂú®Â∑ÆË∑ù„ÄÇÊï∏‰ΩçÂÅ•Â∫∑Âú®ÊîØÊåÅÈï∑ËÄÖÁöÑÁ¶èÁ•â„ÄÅÊÉÖÁ∑íÈúÄÊ±ÇÂíåÁ§æÊúÉÈúÄÊ±ÇÊñπÈù¢ÊâÆÊºîËëóÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÈúÄÊ±ÇÂ∑•Á®ãÔºàREÔºâÊòØËªüÈ´îÂ∑•Á®ãÈ†òÂüüÁöÑ‰∏ÄÂ§ßÈ†òÂüüÔºåÊúâÂä©ÊñºË≠òÂà•„ÄÅÂºïÂ∞éÂíåÂÑ™ÂÖàËôïÁêÜÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑÈúÄÊ±ÇÔºå‰∏¶Á¢∫‰øùÁ≥ªÁµ±Á¨¶ÂêàÊïàËÉΩ„ÄÅÂèØÈù†ÊÄßÂíåÂèØÁî®ÊÄßÁöÑÊ®ôÊ∫ñ„ÄÇÊàëÂÄëÂ∞çÈï∑ËÄÖÊï∏‰ΩçÂÅ•Â∫∑ËªüÈ´îÁöÑREÊñáÁçªÈÄ≤Ë°å‰∫ÜÁ≥ªÁµ±ÊÄßÁöÑÂõûÈ°ß„ÄÇÈÄôÂ∞çÊñºÂ±ïÁèæÁõÆÂâçÂú®Èï∑ÁÖßÊï∏‰ΩçÂÅ•Â∫∑È†òÂüü‰∏≠ÁêÜËß£Èï∑ËÄÖÈúÄÊ±ÇÁöÑÈöéÊÆµ‰ª£Ë°®ÊÄßÊòØÂøÖË¶ÅÁöÑ„ÄÇÊàëÂÄëÊ†πÊìöKitchenhamÊñπÊ≥ï„ÄÅPRISMAÂíåPICOÊåáÂçóÊâÄÂàóÂá∫ÁöÑÊó¢ÂÆöÊ∫ñÂâáÔºåÂà∂ÂÆö‰∫Ü‰∏ÄÂ•óÂçîÂÆöÔºåÊé•ËëóÁ≥ªÁµ±ÊÄßÂú∞Êé¢Ë®é‰∫ÜÂÖ´ÂÄãË≥áÊñôÂ∫´„ÄÇÈÄôÁî¢Áîü‰∫Ü69È†ÖÈ´òÂ∫¶Áõ∏ÈóúÁöÑ‰∏ªË¶ÅÁ†îÁ©∂ÔºåÂÖ∂ÂæåÈÄ≤Ë°å‰∫ÜË≥áÊñôËêÉÂèñ„ÄÅÁ∂úÂêàÂíåÂõûÂ†±„ÄÇÊàëÂÄëÈáçÈªû‰ªãÁ¥π‰∫ÜÈï∑ËÄÖÊï∏‰ΩçÂÅ•Â∫∑ËªüÈ´î‰∏≠ÁöÑÈóúÈçµREÊµÅÁ®ã„ÄÇÂÆÉÊé¢Ë®é‰∫ÜÁßëÊäÄÂú®Èï∑ËÄÖ‰ΩøÁî®ËÄÖÁ¶èÁ•âÂíåÁÖßË≠∑‰∏≠ÁöÑÊáâÁî®Ôºå‰ª•ÂèäÈÄô‰∫õËß£Ê±∫ÊñπÊ°àÁöÑË©ï‰º∞„ÄÇÈÄô‰ªΩÂõûÈ°ß‰πüÊâæÂá∫‰∫ÜÁèæÊúâ‰∏ªË¶ÅÁ†îÁ©∂‰∏≠ÁôºÁèæÁöÑ‰∏ªË¶ÅÈôêÂà∂ÔºåÊøÄÂãµ‰∫ÜÊú™‰æÜÁöÑÁ†îÁ©∂Ê©üÊúÉ„ÄÇÁµêÊûúÈ°ØÁ§∫Ôºå‰∏çÂêåÁ†îÁ©∂‰πãÈñìÂú®ÈúÄÊ±ÇÊî∂ÈõÜÂíåÁêÜËß£ÊñπÈù¢ÊúâÈ°ØËëóÁöÑÂ∑ÆÁï∞„ÄÇÂ∑ÆÁï∞Âú®ÊñºÈúÄÊ±ÇÊî∂ÈõÜÊâÄÊé°Áî®ÁöÑÂìÅË≥™„ÄÅÊ∑±Â∫¶ÂíåÊäÄË°ìÔºåËÄåÈÄô‰∫õÂ∑ÆÁï∞Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÁî±ÊñºREÊñπÊ≥ïÊé°Áî®‰∏çÂùáÊâÄËá¥„ÄÇ

##### **Cross Feature Fusion of Fundus Image and Generated Lesion Map for Referable Diabetic Retinopathy Classification**
2411.03618v1 by Dahyun Mok, Junghyun Bum, Le Duc Tai, Hyunseung Choo

Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating
early detection and diagnosis. This paper focuses on referable DR
classification to enhance the applicability of the proposed method in clinical
practice. We develop an advanced cross-learning DR classification method
leveraging transfer learning and cross-attention mechanisms. The proposed
method employs the Swin U-Net architecture to segment lesion maps from DR
fundus images. The Swin U-Net segmentation model, enriched with DR lesion
insights, is transferred to generate a lesion map. Both the fundus image and
its segmented lesion map are used as complementary inputs for the
classification model. A cross-attention mechanism is deployed to improve the
model's ability to capture fine-grained details from the input pairs. Our
experiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a
superior accuracy of 94.6%, surpassing current state-of-the-art methods by
4.4%. To this end, we aim for the proposed method to be seamlessly integrated
into clinical workflows, enhancing accuracy and efficiency in identifying
referable DR.

ÊëòË¶ÅÔºöÁ≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆä (DR) ÊòØÂ§±ÊòéÁöÑÈ¶ñË¶ÅÂéüÂõ†ÔºåÈúÄË¶ÅÊó©ÊúüÊ™¢Ê∏¨ÂíåË®∫Êñ∑„ÄÇÊú¨ÊñáÈáçÈªûÈóúÊ≥®ÂèØËΩâË®∫ÁöÑ DR ÂàÜÈ°ûÔºå‰ª•Â¢ûÂº∑ÊâÄÊèêÂá∫ÊñπÊ≥ïÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÂÖàÈÄ≤ÁöÑ‰∫§ÂèâÂ≠∏Áøí DR ÂàÜÈ°ûÊñπÊ≥ïÔºåÂà©Áî®ÈÅ∑ÁßªÂ≠∏ÁøíÂíå‰∫§ÂèâÊ≥®ÊÑèÊ©üÂà∂„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊé°Áî® Swin U-Net Êû∂ÊßãÔºåÂæû DR ÁúºÂ∫ïÂúñÂÉè‰∏≠ÂàÜÂâ≤ÁóÖÁÅ∂Âúñ„ÄÇË±êÂØå‰∫Ü DR ÁóÖÁÅ∂Ë¶ãËß£ÁöÑ Swin U-Net ÂàÜÂâ≤Ê®°ÂûãË¢´ËΩâÁßª‰ª•ÁîüÊàêÁóÖÁÅ∂Âúñ„ÄÇÁúºÂ∫ïÂúñÂÉèÂèäÂÖ∂ÂàÜÂâ≤ÁöÑÁóÖÁÅ∂ÂúñÈÉΩË¢´Áî®‰ΩúÂàÜÈ°ûÊ®°ÂûãÁöÑË£úÂÖÖËº∏ÂÖ•„ÄÇÈÉ®ÁΩ≤‰∫§ÂèâÊ≥®ÊÑèÊ©üÂà∂‰ª•ÊèêÈ´òÊ®°ÂûãÂæûËº∏ÂÖ•Â∞ç‰∏≠Êì∑ÂèñÁ¥∞Á≤íÂ∫¶Á¥∞ÁØÄÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÂà©Áî®‰∫ÜÂÖ©ÂÄãÂÖ¨ÈñãÊï∏ÊìöÈõÜÔºåFGADR Âíå EyePACSÔºåÂ±ïÁ§∫‰∫Ü 94.6% ÁöÑÂÑ™Áï∞Ê∫ñÁ¢∫ÁéáÔºåÊØîÁï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÈ´òÂá∫ 4.4%„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂ∏åÊúõÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïËÉΩÁÑ°Á∏´Êï¥ÂêàÂà∞Ëá®Â∫äÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÔºåÊèêÈ´òÊ∫ñÁ¢∫Â∫¶ÂíåÊïàÁéáÔºå‰ª•Ë≠òÂà•ÂèØËΩâË®∫ÁöÑ DR„ÄÇ

##### **The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare**
2411.03287v1 by Souren Pashangpour, Goldie Nejat

The potential use of large language models (LLMs) in healthcare robotics can
help address the significant demand put on healthcare systems around the world
with respect to an aging demographic and a shortage of healthcare
professionals. Even though LLMs have already been integrated into medicine to
assist both clinicians and patients, the integration of LLMs within healthcare
robots has not yet been explored for clinical settings. In this perspective
paper, we investigate the groundbreaking developments in robotics and LLMs to
uniquely identify the needed system requirements for designing health specific
LLM based robots in terms of multi modal communication through human robot
interactions (HRIs), semantic reasoning, and task planning. Furthermore, we
discuss the ethical issues, open challenges, and potential future research
directions for this emerging innovative field.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÜ´ÁôÇ‰øùÂÅ•Ê©üÂô®‰∫∫‰∏≠ÊΩõÂú®ÁöÑÊáâÁî®ÔºåÊúâÂä©ÊñºÊªøË∂≥ÂÖ®ÁêÉÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±Â∞çÊáâËÄÅÈΩ°Âåñ‰∫∫Âè£ÂíåÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°Áü≠Áº∫ÂïèÈ°åÁöÑÈáçÂ§ßÈúÄÊ±Ç„ÄÇÂÑòÁÆ° LLM Â∑≤Êï¥ÂêàÂà∞ÈÜ´ÁôÇÈ†òÂüü‰∏≠Ôºå‰ª•ÂçîÂä©Ëá®Â∫äÈÜ´ÁîüÂíåÊÇ£ËÄÖÔºå‰ΩÜ LLM Âú®ÈÜ´ÁôÇ‰øùÂÅ•Ê©üÂô®‰∫∫‰∏≠ÁöÑÊï¥ÂêàÂ∞öÊú™ÈáùÂ∞çËá®Â∫äÁí∞Â¢ÉÈÄ≤Ë°åÊé¢Ë®é„ÄÇÂú®Ê≠§ËßÄÈªûË´ñÊñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÊ©üÂô®‰∫∫Âíå LLM ÁöÑÂâµÊñ∞ÁôºÂ±ïÔºå‰ª•Áç®ÁâπÂú∞ÊâæÂá∫Ë®≠Ë®àÁâπÂÆöÊñºÂÅ•Â∫∑ÁöÑ LLM Ê©üÂô®‰∫∫ÁöÑÁ≥ªÁµ±ÈúÄÊ±ÇÔºåÂåÖÊã¨ÈÄèÈÅé‰∫∫Ê©ü‰∫íÂãï (HRI)„ÄÅË™ûÁæ©Êé®ÁêÜÂíå‰ªªÂãôË¶èÂäÉÁöÑÂ§öÊ®°ÂºèÊ∫ùÈÄö„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®éË´ñ‰∫ÜÈÄôÂÄãÊñ∞ËààÂâµÊñ∞È†òÂüüÁöÑÂÄ´ÁêÜË≠∞È°å„ÄÅÈñãÊîæÊÄßÊåëÊà∞ÂíåÊΩõÂú®ÁöÑÊú™‰æÜÁ†îÁ©∂ÊñπÂêë„ÄÇ

##### **Discovering Data Structures: Nearest Neighbor Search and Beyond**
2411.03253v1 by Omar Salemohamed, Laurent Charlin, Shivam Garg, Vatsal Sharan, Gregory Valiant

We propose a general framework for end-to-end learning of data structures.
Our framework adapts to the underlying data distribution and provides
fine-grained control over query and space complexity. Crucially, the data
structure is learned from scratch, and does not require careful initialization
or seeding with candidate data structures/algorithms. We first apply this
framework to the problem of nearest neighbor search. In several settings, we
are able to reverse-engineer the learned data structures and query algorithms.
For 1D nearest neighbor search, the model discovers optimal distribution
(in)dependent algorithms such as binary search and variants of interpolation
search. In higher dimensions, the model learns solutions that resemble k-d
trees in some regimes, while in others, they have elements of
locality-sensitive hashing. The model can also learn useful representations of
high-dimensional data and exploit them to design effective data structures. We
also adapt our framework to the problem of estimating frequencies over a data
stream, and believe it could also be a powerful discovery tool for new
problems.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÈÄöÁî®ÁöÑÊû∂ÊßãÔºåÁî®ÊñºË≥áÊñôÁµêÊßãÁöÑÁ´ØÂà∞Á´ØÂ≠∏Áøí„ÄÇ
ÊàëÂÄëÁöÑÊû∂ÊßãÊúÉÈÅ©ÊáâÂü∫Á§éË≥áÊñôÂàÜ‰ΩàÔºå‰∏¶Êèê‰æõÂ∞çÊü•Ë©¢ÂíåÁ©∫ÈñìË§áÈõúÂ∫¶ÁöÑÁ¥∞Á∑ªÊéßÂà∂„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåË≥áÊñôÁµêÊßãÊòØÂæûÈ†≠ÈñãÂßãÂ≠∏ÁøíÔºå‰∏çÈúÄË¶Å‰ªîÁ¥∞ÂàùÂßãÂåñÊàñ‰ΩøÁî®ÂÄôÈÅ∏Ë≥áÊñôÁµêÊßã/ÊºîÁÆóÊ≥ïÈÄ≤Ë°åË®≠ÂÆö„ÄÇÊàëÂÄëÈ¶ñÂÖàÂ∞áÈÄôÂÄãÊû∂ÊßãÊáâÁî®Âà∞ÊúÄËøëÈÑ∞ÊêúÂ∞ãÁöÑÂïèÈ°å„ÄÇÂú®Â§öÁ®ÆË®≠ÂÆö‰∏≠ÔºåÊàëÂÄëËÉΩÂ§†ÈÄÜÂêëÂ∑•Á®ãÂ∑≤Â≠∏ÁøíÁöÑË≥áÊñôÁµêÊßãÂíåÊü•Ë©¢ÊºîÁÆóÊ≥ï„ÄÇÂ∞çÊñº 1D ÊúÄËøëÈÑ∞ÊêúÂ∞ãÔºåÊ®°ÂûãÊúÉÁôºÁèæÊúÄ‰Ω≥ÂàÜ‰ΩàÔºàÂÖßÈÉ®ÔºâÁç®Á´ãÊºîÁÆóÊ≥ïÔºå‰æãÂ¶Ç‰∫åÂÖÉÊêúÂ∞ãÂíåÂÖßÊèíÊêúÂ∞ãËÆäÈ´î„ÄÇÂú®Êõ¥È´òÁ∂≠Â∫¶‰∏≠ÔºåÊ®°ÂûãÂ≠∏ÁøíÂà∞ÁöÑËß£ÊúÉÂú®Êüê‰∫õÊ®°Âºè‰∏ãÈ°û‰ººÊñº k-d Ê®πÔºåËÄåÂú®ÂÖ∂‰ªñÊ®°Âºè‰∏ãÔºåÂÆÉÂÄëÊúÉÂåÖÂê´Â±ÄÈÉ®ÊïèÊÑüÈõúÊπäÁöÑÂÖÉÁ¥†„ÄÇË©≤Ê®°ÂûãÈÇÑÂèØ‰ª•Â≠∏ÁøíÈ´òÁ∂≠Ë≥áÊñôÁöÑÊúâÁî®Ë°®Á§∫Ôºå‰∏¶Âà©Áî®ÂÆÉÂÄë‰æÜË®≠Ë®àÊúâÊïàÁöÑË≥áÊñôÁµêÊßã„ÄÇÊàëÂÄë‰πüÂ∞áÊàëÂÄëÁöÑÊû∂ÊßãË™øÊï¥Âà∞Ë≥áÊñô‰∏≤ÊµÅ‰∏äÈ†ªÁéá‰º∞Ë®àÁöÑÂïèÈ°åÔºå‰∏¶Áõ∏‰ø°ÂÆÉÂ∞çÊñºÊñ∞ÂïèÈ°å‰æÜË™™‰πüÂèØËÉΩÊòØ‰∏ÄÂÄãÂº∑Â§ßÁöÑÁôºÁèæÂ∑•ÂÖ∑„ÄÇ

##### **Evaluating Machine Learning Models against Clinical Protocols for Enhanced Interpretability and Continuity of Care**
2411.03105v1 by Christel Sirocchi, Muhammad Suffian, Federico Sabbatini, Alessandro Bogliolo, Sara Montagna

In clinical practice, decision-making relies heavily on established
protocols, often formalised as rules. Concurrently, Machine Learning (ML)
models, trained on clinical data, aspire to integrate into medical
decision-making processes. However, despite the growing number of ML
applications, their adoption into clinical practice remains limited. Two
critical concerns arise, relevant to the notions of consistency and continuity
of care: (a) accuracy - the ML model, albeit more accurate, might introduce
errors that would not have occurred by applying the protocol; (b)
interpretability - ML models operating as black boxes might make predictions
based on relationships that contradict established clinical knowledge. In this
context, the literature suggests using ML models integrating domain knowledge
for improved accuracy and interpretability. However, there is a lack of
appropriate metrics for comparing ML models with clinical rules in addressing
these challenges. Accordingly, in this article, we first propose metrics to
assess the accuracy of ML models with respect to the established protocol.
Secondly, we propose an approach to measure the distance of explanations
provided by two rule sets, with the goal of comparing the explanation
similarity between clinical rule-based systems and rules extracted from ML
models. The approach is validated on the Pima Indians Diabetes dataset by
training two neural networks - one exclusively on data, and the other
integrating a clinical protocol. Our findings demonstrate that the integrated
ML model achieves comparable performance to that of a fully data-driven model
while exhibiting superior accuracy relative to the clinical protocol, ensuring
enhanced continuity of care. Furthermore, we show that our integrated model
provides explanations for predictions that align more closely with the clinical
protocol compared to the data-driven model.

ÊëòË¶ÅÔºö<paragraph>Âú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÔºåÊ±∫Á≠ñ‰ª∞Ë≥¥Êó¢ÂÆöÁöÑÂçîÂÆöÔºåÈÄöÂ∏∏‰ª•Ë¶èÂâáÂΩ¢ÂºèÂåñ„ÄÇÂêåÊôÇÔºå‰ª•Ëá®Â∫äË≥áÊñôË®ìÁ∑¥ÁöÑÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÔºåÊ∏¥ÊúõÊï¥ÂêàÂà∞ÈÜ´ÁôÇÊ±∫Á≠ñÊµÅÁ®ã‰∏≠„ÄÇÁÑ∂ËÄåÔºåÂÑòÁÆ° ML ÊáâÁî®Êï∏ÈáèÊó•Â¢ûÔºåÂÆÉÂÄëÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑÊé°Áî®‰ªçÂèóÈôê„ÄÇÂÖ©ÂÄãÈóúÈçµÁñëÊÖÆÊµÆÁèæÔºåËàáÁÖßË≠∑ÁöÑ‰∏ÄËá¥ÊÄßÂíåÈÄ£Á∫åÊÄßÊ¶ÇÂøµÁõ∏ÈóúÔºö(a) Ê∫ñÁ¢∫ÊÄß - ML Ê®°ÂûãÈõñÁÑ∂Êõ¥Ê∫ñÁ¢∫Ôºå‰ΩÜÂèØËÉΩÊúÉÂºïÂÖ•Â•óÁî®ÂçîÂÆöÊôÇ‰∏çÊúÉÁôºÁîüÁöÑÈåØË™§Ôºõ(b) ÂèØËß£ÈáãÊÄß - ‰ΩúÁÇ∫ÈªëÁõíÈÅã‰ΩúÁöÑ ML Ê®°ÂûãÂèØËÉΩÊúÉÊ†πÊìöËàáÊó¢ÂÆöËá®Â∫äÁü•Ë≠òÁõ∏ÁüõÁõæÁöÑÈóú‰øÇÈÄ≤Ë°åÈ†êÊ∏¨„ÄÇÂú®Ê≠§ËÑàÁµ°‰∏≠ÔºåÊñáÁçªÂª∫Ë≠∞‰ΩøÁî®Êï¥ÂêàÈ†òÂüüÁü•Ë≠òÁöÑ ML Ê®°Âûã‰ª•ÊèêÂçáÊ∫ñÁ¢∫ÊÄßÂíåÂèØËß£ÈáãÊÄß„ÄÇÁÑ∂ËÄåÔºåÁº∫‰πèÈÅ©Áï∂ÁöÑÊåáÊ®ô‰æÜÊØîËºÉ ML Ê®°ÂûãËàáËá®Â∫äË¶èÂâáÔºå‰ª•ÊáâÂ∞çÈÄô‰∫õÊåëÊà∞„ÄÇÂõ†Ê≠§ÔºåÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÊèêÂá∫ÊåáÊ®ô‰æÜË©ï‰º∞ ML Ê®°ÂûãÁõ∏Â∞çÊñºÊó¢ÂÆöÂçîÂÆöÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñπÊ≥ï‰æÜË°°ÈáèÂÖ©ÁµÑË¶èÂâáÊâÄÊèê‰æõÁöÑËß£ÈáãÁöÑË∑ùÈõ¢ÔºåÁõÆÊ®ôÊòØÊØîËºÉÂü∫ÊñºËá®Â∫äË¶èÂâáÁöÑÁ≥ªÁµ±ËàáÂæû ML Ê®°Âûã‰∏≠ÊèêÂèñÁöÑË¶èÂâá‰πãÈñìÁöÑËß£ÈáãÁõ∏‰ººÊÄß„ÄÇÊ≠§ÊñπÊ≥ïÂú® Pima Âç∞Âú∞ÂÆâ‰∫∫Á≥ñÂ∞øÁóÖË≥áÊñôÈõÜ‰∏äÈ©óË≠âÔºåÊñπÊ≥ïÊòØË®ìÁ∑¥ÂÖ©ÂÄãÁ•ûÁ∂ìÁ∂≤Ë∑Ø - ‰∏ÄÂÄãÂÉÖÈáùÂ∞çË≥áÊñôÔºåÂè¶‰∏ÄÂÄãÊï¥ÂêàËá®Â∫äÂçîÂÆö„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË≠âÊòéÔºåÊï¥ÂêàÂºè ML Ê®°ÂûãÈÅîÂà∞‰∫ÜËàáÂÆåÂÖ®Ë≥áÊñôÈ©ÖÂãïÊ®°ÂûãÁõ∏Áï∂ÁöÑÊïàËÉΩÔºåÂêåÊôÇÂ±ïÁèæÂá∫Áõ∏Â∞çÊñºËá®Â∫äÂçîÂÆöÁöÑÂÑ™Áï∞Ê∫ñÁ¢∫ÊÄßÔºåÁ¢∫‰øùÂ¢ûÂº∑ÁöÑÁÖßË≠∑ÈÄ£Á∫åÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË≠âÊòéÊàëÂÄëÁöÑÊï¥ÂêàÊ®°ÂûãÊèê‰æõÁöÑÈ†êÊ∏¨Ëß£ÈáãËàáËá®Â∫äÂçîÂÆöÁõ∏ÊØîÔºåÊõ¥ÁÇ∫Á∑äÂØÜÂú∞ÁµêÂêà„ÄÇ</paragraph>

##### **Local Lesion Generation is Effective for Capsule Endoscopy Image Data Augmentation in a Limited Data Setting**
2411.03098v1 by Adrian B. Ch≈Çopowiec, Adam R. Ch≈Çopowiec, Krzysztof Galus, Wojciech Cebula, Martin Tabakov

Limited medical imaging datasets challenge deep learning models by increasing
risks of overfitting and reduced generalization, particularly in Generative
Adversarial Networks (GANs), where discriminators may overfit, leading to
training divergence. This constraint also impairs classification models trained
on small datasets. Generative Data Augmentation (GDA) addresses this by
expanding training datasets with synthetic data, although it requires training
a generative model. We propose and evaluate two local lesion generation
approaches to address the challenge of augmenting small medical image datasets.
The first approach employs the Poisson Image Editing algorithm, a classical
image processing technique, to create realistic image composites that
outperform current state-of-the-art methods. The second approach introduces a
novel generative method, leveraging a fine-tuned Image Inpainting GAN to
synthesize realistic lesions within specified regions of real training images.
A comprehensive comparison of the two proposed methods demonstrates that
effective local lesion generation in a data-constrained setting allows for
reaching new state-of-the-art results in capsule endoscopy lesion
classification. Combination of our techniques achieves a macro F1-score of
33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on
the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule
endoscopy. To the best of our knowledge, this work is the first to apply a
fine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that
an image-conditional GAN can be adapted effectively to limited datasets to
generate high-quality examples, facilitating effective data augmentation.
Additionally, we show that combining this GAN-based approach with classical
image processing techniques further enhances the results.

ÊëòË¶ÅÔºö<paragraph>ÂèóÈôêÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÊúÉÈÄèÈÅéÂ¢ûÂä†ÈÅéÂ∫¶Êì¨ÂêàÁöÑÈ¢®Èö™ÂíåÈôç‰ΩéÊ¶ÇÂåñËÉΩÂäõÔºåÁâπÂà•ÊòØÂú®ÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑Ø (GAN) ‰∏≠ÔºåÂÖ∂‰∏≠Âà§Âà•Âô®ÂèØËÉΩÊúÉÈÅéÂ∫¶Êì¨ÂêàÔºåÂ∞éËá¥Ë®ìÁ∑¥ÂàÜÊ≠ßÔºåÂ∞çÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÊßãÊàêÊåëÊà∞„ÄÇÈÄôÁ®ÆÈôêÂà∂‰πüÊêçÂÆ≥‰∫ÜÂú®Â∞èÂûãË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÂàÜÈ°ûÊ®°Âûã„ÄÇÁîüÊàêË≥áÊñôÊì¥ÂÖÖ (GDA) ÈÄèÈÅé‰ΩøÁî®ÂêàÊàêË≥áÊñôÊì¥ÂÖÖË®ìÁ∑¥Ë≥áÊñôÈõÜ‰æÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÂÑòÁÆ°ÂÆÉÈúÄË¶ÅË®ìÁ∑¥ÁîüÊàêÊ®°Âûã„ÄÇÊàëÂÄëÊèêÂá∫‰∏¶Ë©ï‰º∞ÂÖ©Á®ÆÂ±ÄÈÉ®ÁóÖÁÅ∂ÁîüÊàêÊñπÊ≥ïÔºå‰ª•Ëß£Ê±∫Êì¥ÂÖÖÂ∞èÂûãÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÁöÑÊåëÊà∞„ÄÇÁ¨¨‰∏ÄÁ®ÆÊñπÊ≥ïÊé°Áî®Ê≥äÊùæÂΩ±ÂÉèÁ∑®ËºØÊºîÁÆóÊ≥ïÔºå‰∏ÄÁ®ÆÁ∂ìÂÖ∏ÂΩ±ÂÉèËôïÁêÜÊäÄË°ìÔºå‰æÜÂª∫Á´ãÈÄºÁúüÁöÑÂΩ±ÂÉèÂêàÊàêÔºåÂÖ∂ÂÑ™ÊñºÁõÆÂâçÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇÁ¨¨‰∫åÁ®ÆÊñπÊ≥ïÂºïÈÄ≤‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÁîüÊàêÊñπÊ≥ïÔºåÂà©Áî®ÂæÆË™øÁöÑÂΩ±ÂÉè‰øÆÂæ© GANÔºåÂú®ÁúüÂØ¶Ë®ìÁ∑¥ÂΩ±ÂÉèÁöÑÁâπÂÆöÂçÄÂüüÂÖßÂêàÊàêÈÄºÁúüÁöÑÁóÖÁÅ∂„ÄÇÂ∞çÈÄôÂÖ©Á®ÆÊèêË≠∞ÊñπÊ≥ïÁöÑÂÖ®Èù¢ÊØîËºÉË≠âÊòéÔºåÂú®Ë≥áÊñôÂèóÈôêÁöÑË®≠ÂÆö‰∏≠ÔºåÊúâÊïàÁöÑÂ±ÄÈÉ®ÁóÖÁÅ∂ÁîüÊàêÂÖÅË®±Âú®ËÜ†ÂõäÂÖßË¶ñÈè°ÁóÖÁÅ∂ÂàÜÈ°û‰∏≠ÈÅîÂà∞Êñ∞ÁöÑÊúÄÂÖàÈÄ≤ÁµêÊûú„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÁµÑÂêàÂú®È´òÂ∫¶‰∏çÂπ≥Ë°°ÁöÑ Kvasir Capsule Ë≥áÊñôÈõÜÔºàËÜ†ÂõäÂÖßË¶ñÈè°ÁöÑÂü∫Ê∫ñÔºâ‰∏äÔºåÈÅîÂà∞‰∫Ü 33.07% ÁöÑÂ∑®ËßÄ F1 ÂàÜÊï∏ÔºåÊØîÂÖàÂâçÁöÑÊúÄ‰Ω≥ÁµêÊûúÈ´òÂá∫ 7.84 ÂÄãÁôæÂàÜÈªû (p.p.)„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊòØÁ¨¨‰∏ÄÂÄãÂ∞áÂæÆË™øÁöÑÂΩ±ÂÉè‰øÆÂæ© GAN ÊáâÁî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑ GDAÔºåË≠âÊòé‰∫ÜÂΩ±ÂÉèÊ¢ù‰ª∂ GAN ÂèØ‰ª•ÊúâÊïàÂú∞ÈÅ©ÊáâÂèóÈôêÁöÑË≥áÊñôÈõÜÔºå‰ª•Áî¢ÁîüÈ´òÂìÅË≥™ÁöÑÁØÑ‰æãÔºå‰øÉÈÄ≤ÊúâÊïàÁöÑË≥áÊñôÊì¥ÂÖÖ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË°®ÊòéÂ∞áÈÄôÁ®ÆÂü∫Êñº GAN ÁöÑÊñπÊ≥ïËàáÁ∂ìÂÖ∏ÂΩ±ÂÉèËôïÁêÜÊäÄË°ìÁõ∏ÁµêÂêàÔºåÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑‰∫ÜÁµêÊûú„ÄÇ</paragraph>

##### **Controlling for Unobserved Confounding with Large Language Model Classification of Patient Smoking Status**
2411.03004v1 by Samuel Lee, Zach Wood-Doughty

Causal understanding is a fundamental goal of evidence-based medicine. When
randomization is impossible, causal inference methods allow the estimation of
treatment effects from retrospective analysis of observational data. However,
such analyses rely on a number of assumptions, often including that of no
unobserved confounding. In many practical settings, this assumption is violated
when important variables are not explicitly measured in the clinical record.
Prior work has proposed to address unobserved confounding with machine learning
by imputing unobserved variables and then correcting for the classifier's
mismeasurement. When such a classifier can be trained and the necessary
assumptions are met, this method can recover an unbiased estimate of a causal
effect. However, such work has been limited to synthetic data, simple
classifiers, and binary variables. This paper extends this methodology by using
a large language model trained on clinical notes to predict patients' smoking
status, which would otherwise be an unobserved confounder. We then apply a
measurement error correction on the categorical predicted smoking status to
estimate the causal effect of transthoracic echocardiography on mortality in
the MIMIC dataset.

ÊëòË¶ÅÔºöÂõ†ÊûúÁêÜËß£ÊòØÂæ™ËØÅÂåªÂ≠¶ÁöÑÂü∫Êú¨ÁõÆÊ†á„ÄÇÂΩìÈöèÊú∫Âåñ‰∏çÂèØË°åÊó∂ÔºåÂõ†ÊûúÊé®ËÆ∫ÊñπÊ≥ïÂÖÅËÆ∏‰ªéËßÇÂØüÊÄßÊï∞ÊçÆÁöÑÂõûÈ°æÊÄßÂàÜÊûê‰∏≠‰º∞ËÆ°Ê≤ªÁñóÊïàÊûú„ÄÇÁÑ∂ËÄåÔºåÊ≠§Á±ªÂàÜÊûê‰æùËµñ‰∫éËÆ∏Â§öÂÅáËÆæÔºåÈÄöÂ∏∏ÂåÖÊã¨Ê≤°ÊúâÊú™ËßÇÂØüÂà∞ÁöÑÊ∑∑ÊùÇÂõ†Á¥†„ÄÇÂú®ËÆ∏Â§öÂÆûÈôÖÊÉÖÂÜµ‰∏ãÔºåÂΩìÈáçË¶ÅÁöÑÂèòÈáèÂú®‰∏¥Â∫äËÆ∞ÂΩï‰∏≠Ê≤°ÊúâÊòéÁ°ÆÊµãÈáèÊó∂ÔºåËøô‰∏ÄÂÅáËÆæÂ∞±‰ºöË¢´ËøùÂèç„ÄÇÂÖàÂâçÁöÑÂ∑•‰ΩúÊèêÂá∫Áî®Êú∫Âô®Â≠¶‰π†Êù•Ëß£ÂÜ≥Êú™ËßÇÂØüÂà∞ÁöÑÊ∑∑ÊùÇÈóÆÈ¢òÔºåÊñπÊ≥ïÊòØÊé®ÁÆóÊú™ËßÇÂØüÂà∞ÁöÑÂèòÈáèÔºåÁÑ∂ÂêéÊ†°Ê≠£ÂàÜÁ±ªÂô®ÁöÑÊµãÈáèËØØÂ∑Æ„ÄÇÂΩìÂèØ‰ª•ËÆ≠ÁªÉËøôÊ†∑ÁöÑÂàÜÁ±ªÂô®Âπ∂‰∏îÊª°Ë∂≥ÂøÖË¶ÅÁöÑÂÅáËÆæÊó∂ÔºåËøôÁßçÊñπÊ≥ïÂèØ‰ª•ÊÅ¢Â§çÂõ†ÊûúÊïàÂ∫îÁöÑÊó†ÂÅè‰º∞ËÆ°„ÄÇÁÑ∂ËÄåÔºåÊ≠§Á±ªÂ∑•‰Ωú‰ªÖÈôê‰∫éÂêàÊàêÊï∞ÊçÆ„ÄÅÁÆÄÂçïÁöÑÂàÜÁ±ªÂô®Âíå‰∫åÂÖÉÂèòÈáè„ÄÇÊú¨ÊñáÈÄöËøá‰ΩøÁî®Âú®‰∏¥Â∫äËÆ∞ÂΩï‰∏äËÆ≠ÁªÉÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊù•È¢ÑÊµãÊÇ£ËÄÖÁöÑÂê∏ÁÉüÁä∂ÂÜµÊù•Êâ©Â±ïËøôÁßçÊñπÊ≥ïÔºåÂê¶ÂàôËøôÂ∞ÜÊòØ‰∏Ä‰∏™Êú™ËßÇÂØüÂà∞ÁöÑÊ∑∑ÊùÇÂõ†Á¥†„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÂØπÂàÜÁ±ªÈ¢ÑÊµãÁöÑÂê∏ÁÉüÁä∂ÊÄÅÂ∫îÁî®ÊµãÈáèËØØÂ∑ÆÊ†°Ê≠£Ôºå‰ª•‰º∞ËÆ°ÁªèËÉ∏Ë∂ÖÂ£∞ÂøÉÂä®ÂõæÂØπ MIMIC Êï∞ÊçÆÈõÜ‰∏≠Ê≠ª‰∫°ÁéáÁöÑÂõ†ÊûúÊïàÂ∫î„ÄÇ

##### **Region-Guided Attack on the Segment Anything Model (SAM)**
2411.02974v1 by Xiaoliang Liu, Furao Shen, Jian Zhao

The Segment Anything Model (SAM) is a cornerstone of image segmentation,
demonstrating exceptional performance across various applications, particularly
in autonomous driving and medical imaging, where precise segmentation is
crucial. However, SAM is vulnerable to adversarial attacks that can
significantly impair its functionality through minor input perturbations.
Traditional techniques, such as FGSM and PGD, are often ineffective in
segmentation tasks due to their reliance on global perturbations that overlook
spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address
these challenges, but they frequently depend on external cues and do not fully
leverage the structural interdependencies within segmentation processes. This
limitation underscores the need for a novel adversarial strategy that exploits
the unique characteristics of segmentation tasks. In response, we introduce the
Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a
Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted
perturbations that fragment large segments and expand smaller ones, resulting
in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves
high success rates in both white-box and black-box scenarios, emphasizing the
need for robust defenses against such sophisticated attacks. RGA not only
reveals SAM's vulnerabilities but also lays the groundwork for developing more
resilient defenses against adversarial threats in image segmentation.

ÊëòË¶ÅÔºöÂàÜÊÆµ‰ªª‰ΩïÊ®°Âûã (SAM) ÊòØÂΩ±ÂÉèÂàÜÊÆµÁöÑÂü∫Áü≥ÔºåÂú®ÂêÑÁ®ÆÊáâÁî®‰∏≠Â±ïÁèæÂá∫Ëâ≤ÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®Ëá™ÂãïÈßïÈßõÂíåÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÔºåÁ≤æÁ¢∫ÂàÜÊÆµËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåSAM ÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÊîªÊìäÔºåÈÄôÁ®ÆÊîªÊìäÊúÉÈÄèÈÅéÂæÆÂ∞èÁöÑËº∏ÂÖ•ÊìæÂãïÈ°ØËëóÊêçÂÆ≥ÂÖ∂ÂäüËÉΩ„ÄÇÂÇ≥Áµ±ÊäÄË°ìÔºå‰æãÂ¶Ç FGSM Âíå PGDÔºåÁî±Êñº‰æùË≥¥ÊúÉÂøΩÁï•Á©∫ÈñìÁ¥∞ÂæÆÂ∑ÆÁöÑÂÖ®Â±ÄÊìæÂãïÔºåÂõ†Ê≠§Âú®ÂàÜÊÆµ‰ªªÂãô‰∏≠Â∏∏Â∏∏ÁÑ°Êïà„ÄÇÊúÄËøëÁöÑÊñπÊ≥ïÔºå‰æãÂ¶Ç Attack-SAM-K Âíå UADÔºåÂ∑≤ÈñãÂßãËß£Ê±∫ÈÄô‰∫õÊåëÊà∞Ôºå‰ΩÜÂÆÉÂÄëÁ∂ìÂ∏∏‰æùË≥¥Â§ñÈÉ®ÊèêÁ§∫Ôºå‰∏îÁÑ°Ê≥ïÂÖÖÂàÜÂà©Áî®ÂàÜÊÆµÈÅéÁ®ã‰∏≠ÁµêÊßã‰∏äÁöÑÁõ∏‰∫í‰æùË≥¥ÊÄß„ÄÇÈÄôÁ®ÆÈôêÂà∂Âá∏È°Ø‰∫ÜÈúÄË¶Å‰∏ÄÁ®ÆÊñ∞ÁöÑÂ∞çÊäóÁ≠ñÁï•Ôºå‰ª•Âà©Áî®ÂàÜÊÆµ‰ªªÂãôÁöÑÁç®ÁâπÁâπÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ∞àÈñÄÁÇ∫ SAM Ë®≠Ë®àÁöÑÂçÄÂüüÂºïÂ∞éÊîªÊìä (RGA)„ÄÇRGA Âà©Áî®ÂçÄÂüüÂºïÂ∞éÂú∞Âúñ (RGM) ‰æÜÊìç‰ΩúÂàÜÊÆµÂçÄÂüüÔºåÈÄ≤ËÄåÈáùÂ∞çÊìæÂãïÈÄ≤Ë°åË™øÊï¥ÔºåÂ∞áÂ§ßÂûãÂàÜÊÆµÂàáÊàêÁâáÊÆµÔºå‰∏¶Êì¥Â±ïËºÉÂ∞èÁöÑÂàÜÊÆµÔºåÂ∞éËá¥ SAM Áî¢ÁîüÈåØË™§Ëº∏Âá∫„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåRGA Âú®ÁôΩÁõíÂíåÈªëÁõíÂ†¥ÊôØ‰∏≠ÈÉΩËÉΩÈÅîÂà∞È´òÊàêÂäüÁéáÔºåÂº∑Ë™ø‰∫ÜÈúÄË¶ÅÈáùÂ∞çÊ≠§È°ûË§áÈõúÊîªÊìäÊé°ÂèñÂº∑ËÄåÊúâÂäõÁöÑÈò≤Á¶¶Êé™ÊñΩ„ÄÇRGA ‰∏çÂÉÖÊè≠Èú≤‰∫Ü SAM ÁöÑÊºèÊ¥ûÔºå‰πüÁÇ∫Âú®ÂΩ±ÂÉèÂàÜÊÆµ‰∏≠ÈáùÂ∞çÂ∞çÊäóÊÄßÂ®ÅËÑÖÈñãÁôºÊõ¥ÂÖ∑ÈüåÊÄßÁöÑÈò≤Á¶¶Êé™ÊñΩÂ•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ

##### **[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI**
2411.02973v1 by Maren Pielka, Tobias Schneider, Jan Terheyden, Rafet Sifa

We present an outline of the first large language model (LLM) based chatbot
application in the context of patient-reported outcome measures (PROMs) for
diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable
patients to provide feedback about their quality of life and treatment progress
via an interactive application. The proposed framework offers significant
advantages over the current approach, which encompasses only qualitative
collection of survey data or a static survey with limited answer options. Using
the PROBot LLM-PROM application, patients will be asked tailored questions
about their individual challenges, and can give more detailed feedback on the
progress of their treatment. Based on this input, we will use machine learning
to infer conventional PROM scores, which can be used by clinicians to evaluate
the treatment status. The goal of the application is to improve adherence to
the healthcare system and treatments, and thus ultimately reduce cases of
subsequent vision impairment. The approach needs to be further validated using
a survey and a clinical study.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÁ¨¨‰∏ÄÂÄãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËÅäÂ§©Ê©üÂô®‰∫∫ÊáâÁî®Á®ãÂºèÔºåÁî®ÊñºÁ≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆäÁöÑÁóÖ‰∫∫ÂõûÂ†±ÁµêÊûúÊ∏¨Èáè (PROM)„ÄÇÈÄèÈÅéÂà©Áî®Áï∂Ââç LLM ÁöÑÂäüËÉΩÔºåÊàëÂÄëËÆìÁóÖ‰∫∫ËÉΩÂ§†ÈÄèÈÅé‰∫íÂãïÂºèÊáâÁî®Á®ãÂºèÊèê‰æõÊúâÈóúÂÖ∂ÁîüÊ¥ªÂìÅË≥™ÂíåÊ≤ªÁôÇÈÄ≤Â∫¶ÁöÑÂõûÈ•ã„ÄÇÊâÄÊèêÂá∫ÁöÑÊû∂ÊßãÊèê‰æõÈ°ØËëóÂÑ™ÊñºÁõÆÂâçÊñπÊ≥ïÁöÑÂÑ™ÈªûÔºåÁõÆÂâçÊñπÊ≥ïÂÉÖÂåÖÂê´Ë™øÊü•Ë≥áÊñôÁöÑË≥™ÊÄßÊî∂ÈõÜÊàñÂÖ∑ÊúâÊúâÈôêÁ≠îÊ°àÈÅ∏È†ÖÁöÑÈùúÊÖãË™øÊü•„ÄÇ‰ΩøÁî® PROBot LLM-PROM ÊáâÁî®Á®ãÂºèÔºåÁóÖ‰∫∫Â∞áÊúÉË¢´Ë©¢ÂïèÊúâÈóúÂÖ∂ÂÄã‰∫∫ÊåëÊà∞ÁöÑÂÆ¢Ë£ΩÂåñÂïèÈ°åÔºå‰∏¶ËÉΩÊèê‰æõÊõ¥Ë©≥Á¥∞ÁöÑÂõûÈ•ãÔºåË™™ÊòéÂÖ∂Ê≤ªÁôÇÈÄ≤Â∫¶„ÄÇÊ†πÊìöÊ≠§Ëº∏ÂÖ•ÔºåÊàëÂÄëÂ∞á‰ΩøÁî®Ê©üÂô®Â≠∏ÁøíÊé®Ë´ñÂÇ≥Áµ± PROM ÂàÜÊï∏ÔºåËá®Â∫äÈÜ´ÁîüÂèØ‰ª•‰ΩøÁî®ÈÄô‰∫õÂàÜÊï∏‰æÜË©ï‰º∞Ê≤ªÁôÇÁãÄÊÖã„ÄÇÊ≠§ÊáâÁî®Á®ãÂºèÁöÑÁõÆÊ®ôÊòØÊîπÂñÑÂ∞çÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÂíåÊ≤ªÁôÇÁöÑ‰æùÂæûÊÄßÔºå‰∏¶Âõ†Ê≠§ÊúÄÁµÇÊ∏õÂ∞ëÂæåÁ∫åË¶ñÂäõÊêçÂÆ≥ÁöÑÁóÖ‰æã„ÄÇÈúÄË¶Å‰ΩøÁî®Ë™øÊü•ÂíåËá®Â∫äÁ†îÁ©∂ÈÄ≤‰∏ÄÊ≠•È©óË≠âÊ≠§ÊñπÊ≥ï„ÄÇ

##### **Membership Inference Attacks against Large Vision-Language Models**
2411.02902v1 by Zhan Li, Yongtao Wu, Yihang Chen, Francesco Tonin, Elias Abad Rocamora, Volkan Cevher

Large vision-language models (VLLMs) exhibit promising capabilities for
processing multi-modal tasks across various application scenarios. However,
their emergence also raises significant data security concerns, given the
potential inclusion of sensitive information, such as private photos and
medical records, in their training datasets. Detecting inappropriately used
data in VLLMs remains a critical and unresolved issue, mainly due to the lack
of standardized datasets and suitable methodologies. In this study, we
introduce the first membership inference attack (MIA) benchmark tailored for
various VLLMs to facilitate training data detection. Then, we propose a novel
MIA pipeline specifically designed for token-level image detection. Lastly, we
present a new metric called MaxR\'enyi-K%, which is based on the confidence of
the model output and applies to both text and image data. We believe that our
work can deepen the understanding and methodology of MIAs in the context of
VLLMs. Our code and datasets are available at
https://github.com/LIONS-EPFL/VL-MIA.

ÊëòË¶ÅÔºöÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLLM) Âú®ËôïÁêÜÂêÑÁ®ÆÊáâÁî®Â†¥ÊôØÁöÑÂ§öÊ®°ÊÖã‰ªªÂãôÊñπÈù¢Ë°®ÁèæÂá∫ÊúâÂâçÊôØÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÁöÑÂá∫Áèæ‰πüÂºïÁôº‰∫ÜÈáçÂ§ßÁöÑË≥áÊñôÂÆâÂÖ®ÂïèÈ°åÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁöÑË®ìÁ∑¥Ë≥áÊñôÈõÜ‰∏≠ÂèØËÉΩÊúÉÂåÖÂê´ÊïèÊÑüË≥áË®äÔºå‰æãÂ¶ÇÁßÅ‰∫∫ÁÖßÁâáÂíåÈÜ´ÁôÇË®òÈåÑ„ÄÇÂÅµÊ∏¨ VLLM ‰∏≠‰∏çÁï∂‰ΩøÁî®ÁöÑË≥áÊñô‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈóúÈçµ‰∏îÂ∞öÊú™Ëß£Ê±∫ÁöÑÂïèÈ°åÔºå‰∏ªË¶ÅÊòØÁî±ÊñºÁº∫‰πèÊ®ôÊ∫ñÂåñÁöÑË≥áÊñôÈõÜÂíåÈÅ©Áï∂ÁöÑÊñπÊ≥ï„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÁ¨¨‰∏ÄÂÄãÈáùÂ∞çÂêÑÁ®Æ VLLM ÈáèË∫´ÊâìÈÄ†ÁöÑÊàêÂì°Êé®Ë´ñÊîªÊìä (MIA) Âü∫Ê∫ñÔºå‰ª•Âà©ÊñºË®ìÁ∑¥Ë≥áÊñôÂÅµÊ∏¨„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∞àÈñÄË®≠Ë®àÁî®Êñº‰ª§ÁâåÁ¥öÂà•ÂΩ±ÂÉèÂÅµÊ∏¨ÁöÑÂÖ®Êñ∞ MIA ÁÆ°Á∑ö„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ MaxR\'enyi-K% ÁöÑÊñ∞ÊåáÊ®ôÔºåÂÆÉÂü∫ÊñºÊ®°ÂûãËº∏Âá∫ÁöÑ‰ø°ÂøÉÔºå‰∏¶ÈÅ©Áî®ÊñºÊñáÂ≠óÂíåÂΩ±ÂÉèË≥áÊñô„ÄÇÊàëÂÄëÁõ∏‰ø°ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÂèØ‰ª•Âä†Ê∑±Â∞ç VLLM ËÉåÊôØ‰∏ã MIA ÁöÑÁêÜËß£ÂíåÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂèØÂú® https://github.com/LIONS-EPFL/VL-MIA ÂèñÂæó„ÄÇ

##### **Advanced XR-Based 6-DOF Catheter Tracking System for Immersive Cardiac Intervention Training**
2411.02611v1 by Mohsen Annabestani, Sandhya Sriram, S. Chiu Wong, Alexandros Sigaras, Bobak Mosadegh

Extended Reality (XR) technologies are gaining traction as effective tools
for medical training and procedural guidance, particularly in complex cardiac
interventions. This paper presents a novel system for real-time 3D tracking and
visualization of intracardiac echocardiography (ICE) catheters, with precise
measurement of the roll angle. A custom 3D-printed setup, featuring orthogonal
cameras, captures biplane video of the catheter, while a specialized computer
vision algorithm reconstructs its 3D trajectory, localizing the tip with
sub-millimeter accuracy and tracking the roll angle in real-time. The system's
data is integrated into an interactive Unity-based environment, rendered
through the Meta Quest 3 XR headset, combining a dynamically tracked catheter
with a patient-specific 3D heart model. This immersive environment allows the
testing of the importance of 3D depth perception, in comparison to 2D
projections, as a form of visualization in XR. Our experimental study,
conducted using the ICE catheter with six participants, suggests that 3D
visualization is not necessarily beneficial over 2D views offered by the XR
system; although all cardiologists saw its utility for pre-operative training,
planning, and intra-operative guidance. The proposed system qualitatively shows
great promise in transforming catheter-based interventions, particularly ICE
procedures, by improving visualization, interactivity, and skill development.

ÊëòË¶ÅÔºöÊì¥Â¢ûÂØ¶Â¢É (XR) ÊäÄË°ìÊ≠£‰ΩúÁÇ∫ÈÜ´ÁôÇË®ìÁ∑¥ÂíåÁ®ãÂ∫èÊåáÂ∞éÁöÑÊúâÊïàÂ∑•ÂÖ∑ËÄåÁç≤ÂæóÈáçË¶ñÔºåÁâπÂà•ÊòØÂú®Ë§áÈõúÁöÑÂøÉËáü‰ªãÂÖ•Ê≤ªÁôÇ‰∏≠„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÁ≥ªÁµ±ÔºåÁî®ÊñºÂØ¶ÊôÇ 3D ËøΩËπ§ÂíåÂèØË¶ñÂåñÂøÉÂÖßË∂ÖËÅ≤ÂøÉÂãïÂúñ (ICE) Â∞éÁÆ°Ôºå‰∏¶Á≤æÁ¢∫Ê∏¨ÈáèÊªæÂãïËßíÂ∫¶„ÄÇ‰∏ÄÂÄãÂÆ¢Ë£ΩÂåñÁöÑ 3D ÂàóÂç∞Ë®≠ÂÆöÔºåÈÖçÂÇôÊ≠£‰∫§Áõ∏Ê©üÔºåÊçïÊçâÂ∞éÁÆ°ÁöÑÈõôÂπ≥Èù¢ÂΩ±ÁâáÔºåËÄå‰∏ÄÂÄãÂ∞àÈñÄÁöÑÈõªËÖ¶Ë¶ñË¶∫ÊºîÁÆóÊ≥ïÈáçÂª∫ÂÖ∂ 3D ËªåË∑°Ôºå‰ª•Â∞èÊñºÊØ´Á±≥ÁöÑÁ≤æÁ¢∫Â∫¶ÂÆö‰ΩçÂ∞ñÁ´Ø‰∏¶Âç≥ÊôÇËøΩËπ§ÊªæÂãïËßíÂ∫¶„ÄÇÁ≥ªÁµ±ÁöÑË≥áÊñôÊï¥ÂêàÂà∞‰∏ÄÂÄã‰∫íÂãïÂºèÁöÑ Unity ÁÇ∫Âü∫Á§éÁöÑÁí∞Â¢É‰∏≠ÔºåÈÄèÈÅé Meta Quest 3 XR È†≠Êà¥ÂºèË£ùÁΩÆÂëàÁèæÔºåÁµêÂêàÂãïÊÖãËøΩËπ§ÁöÑÂ∞éÁÆ°ÂíåÁâπÂÆöÁóÖÊÇ£ÁöÑ 3D ÂøÉËáüÊ®°Âûã„ÄÇÈÄôÂÄãÊ≤àÊµ∏ÂºèÁöÑÁí∞Â¢ÉÂÖÅË®±Ê∏¨Ë©¶ 3D Ê∑±Â∫¶ÊÑüÁü•ÁöÑÈáçË¶ÅÊÄßÔºåËàá 2D ÊäïÂΩ±Áõ∏ÊØîÔºå‰ΩúÁÇ∫ XR ‰∏≠ÁöÑ‰∏ÄÁ®ÆË¶ñË¶∫ÂåñÂΩ¢Âºè„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁ†îÁ©∂Ôºå‰ΩøÁî® ICE Â∞éÁÆ°ÈÄ≤Ë°åÔºåÊúâÂÖ≠‰ΩçÂèÉËàáËÄÖÔºåÈ°ØÁ§∫ 3D Ë¶ñË¶∫Âåñ‰∏ç‰∏ÄÂÆöÊØî XR Á≥ªÁµ±Êèê‰æõÁöÑ 2D Ë¶ñÂúñÊúâÁõäÔºõÂÑòÁÆ°ÊâÄÊúâÂøÉËáüÁßëÈÜ´Â∏´ÈÉΩÁúãÂà∞ÂÆÉÂú®Ë°ìÂâçË®ìÁ∑¥„ÄÅË¶èÂäÉÂíåË°ì‰∏≠ÊåáÂ∞é‰∏≠ÁöÑÁî®ÈÄî„ÄÇÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±Âú®Ë≥™Âåñ‰∏äÈ°ØÁ§∫Âá∫Âú®ËΩâÊèõÂ∞éÁÆ°‰ªãÂÖ•Ê≤ªÁôÇÔºåÁâπÂà•ÊòØ ICE Á®ãÂ∫èÊñπÈù¢ÔºåÈÄèÈÅéÊîπÂñÑË¶ñË¶∫Âåñ„ÄÅ‰∫íÂãïÊÄßÂíåÊäÄËÉΩÁôºÂ±ïÔºåÂÖ∑ÊúâÂæàÂ§ßÁöÑÂâçÊôØ„ÄÇ

##### **"It's a conversation, not a quiz": A Risk Taxonomy and Reflection Tool for LLM Adoption in Public Health**
2411.02594v1 by Jiawei Zhou, Amy Z. Chen, Darshi Shah, Laura Schwab Reese, Munmun De Choudhury

Recent breakthroughs in large language models (LLMs) have generated both
interest and concern about their potential adoption as accessible information
sources or communication tools across different domains. In public health --
where stakes are high and impacts extend across populations -- adopting LLMs
poses unique challenges that require thorough evaluation. However, structured
approaches for assessing potential risks in public health remain
under-explored. To address this gap, we conducted focus groups with health
professionals and health issue experiencers to unpack their concerns, situated
across three distinct and critical public health issues that demand
high-quality information: vaccines, opioid use disorder, and intimate partner
violence. We synthesize participants' perspectives into a risk taxonomy,
distinguishing and contextualizing the potential harms LLMs may introduce when
positioned alongside traditional health communication. This taxonomy highlights
four dimensions of risk in individual behaviors, human-centered care,
information ecosystem, and technology accountability. For each dimension, we
discuss specific risks and example reflection questions to help practitioners
adopt a risk-reflexive approach. This work offers a shared vocabulary and
reflection tool for experts in both computing and public health to
collaboratively anticipate, evaluate, and mitigate risks in deciding when to
employ LLM capabilities (or not) and how to mitigate harm when they are used.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞Á™ÅÁ†¥ÂºïËµ∑‰∫Ü‰∫∫ÂÄëÁöÑËààË∂£Ôºå‰πüÂºïËµ∑‰∫Ü‰∫∫ÂÄëÂ∞çÂÖ∂‰ΩúÁÇ∫‰∏çÂêåÈ†òÂüüÁöÑÁÑ°ÈöúÁ§ô‰ø°ÊÅØ‰æÜÊ∫êÊàñÈÄö‰ø°Â∑•ÂÖ∑ÁöÑÊΩõÂú®Êé°Áî®ÊâÄÁî¢ÁîüÁöÑÊìîÊÜÇ„ÄÇÂú®ÂÖ¨ÂÖ±Ë°õÁîüÈ†òÂüü‚Äî‚ÄîÂà©ÂÆ≥Èóú‰øÇÂæàÈ´ò‰∏îÂΩ±ÈüøÈÅçÂèä‰∫∫Áæ§‚Äî‚ÄîÊé°Áî® LLM ÊßãÊàê‰∫ÜÁç®ÁâπÁöÑÊåëÊà∞ÔºåÈúÄË¶ÅÂæπÂ∫ïË©ï‰º∞„ÄÇÁÑ∂ËÄåÔºåË©ï‰º∞ÂÖ¨ÂÖ±Ë°õÁîü‰∏≠ÊΩõÂú®È¢®Èö™ÁöÑÁµêÊßãÂåñÊñπÊ≥ï‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∏ÄÂ∑ÆË∑ùÔºåÊàëÂÄëËàáÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°ÂíåÂÅ•Â∫∑ÂïèÈ°åÈ´îÈ©óËÄÖÈÄ≤Ë°å‰∫ÜÁÑ¶ÈªûÂ∞èÁµÑÔºå‰ª•Ëß£Èñã‰ªñÂÄëÁöÑÁñëÊÖÆÔºåÈÄô‰∫õÁñëÊÖÆÊ∂âÂèä‰∏âÂÄã‰∏çÂêåÁöÑÈóúÈçµÂÖ¨ÂÖ±Ë°õÁîüÂïèÈ°åÔºåÈÄô‰∫õÂïèÈ°åÈúÄË¶ÅÈ´òË≥™ÈáèÁöÑË≥áË®äÔºöÁñ´Ëãó„ÄÅÈòøÁâáÈ°ûËó•Áâ©‰ΩøÁî®ÈöúÁ§ôÂíåË¶™ÂØÜ‰º¥‰æ∂Êö¥Âäõ„ÄÇÊàëÂÄëÂ∞áÂèÉËàáËÄÖÁöÑËßÄÈªûÁ∂úÂêàÂà∞È¢®Èö™ÂàÜÈ°ûÊ≥ï‰∏≠ÔºåÂçÄÂàÜÂíåÊÉÖÂ¢ÉÂåñ LLM Âú®ËàáÂÇ≥Áµ±ÂÅ•Â∫∑ÂÇ≥Êí≠‰∏¶ÂàóÊôÇÂèØËÉΩÈÄ†ÊàêÁöÑÊΩõÂú®Âç±ÂÆ≥„ÄÇÈÄôÁ®ÆÂàÜÈ°ûÊ≥ïÁ™ÅÂá∫‰∫ÜÂÄã‰∫∫Ë°åÁÇ∫„ÄÅ‰ª•‰∫∫ÁÇ∫‰∏≠ÂøÉÁöÑË≠∑ÁêÜ„ÄÅË≥áË®äÁîüÊÖãÁ≥ªÁµ±ÂíåÊäÄË°ìÂïèË≤¨Âà∂ÈÄôÂõõÂÄãÁ∂≠Â∫¶ÁöÑÈ¢®Èö™„ÄÇÂ∞çÊñºÊØèÂÄãÁ∂≠Â∫¶ÔºåÊàëÂÄëË®éË´ñÂÖ∑È´îÁöÑÈ¢®Èö™ÂíåÁØÑ‰æãÂèçÊÄùÂïèÈ°åÔºå‰ª•Âπ´Âä©ÂæûÊ•≠ËÄÖÊé°Áî®È¢®Èö™ÂèçÊÄùÊñπÊ≥ï„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫Ë®àÁÆóÂíåÂÖ¨ÂÖ±Ë°õÁîüÈ†òÂüüÁöÑÂ∞àÂÆ∂Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ±ÂêåÁöÑË©ûÂΩôÂíåÂèçÊÄùÂ∑•ÂÖ∑Ôºå‰ª•‰æøÂú®Ê±∫ÂÆö‰ΩïÊôÇÊé°Áî® LLM ÂäüËÉΩÔºàÊàñ‰∏çÊé°Áî®Ôºâ‰ª•ÂèäÂú®‰ΩøÁî® LLM ÂäüËÉΩÊôÇÂ¶Ç‰ΩïÊ∏õËºïÂç±ÂÆ≥ÊôÇÔºåÂÖ±ÂêåÈ†êÊ∏¨„ÄÅË©ï‰º∞ÂíåÊ∏õËºïÈ¢®Èö™„ÄÇ

##### **Digitizing Touch with an Artificial Multimodal Fingertip**
2411.02479v1 by Mike Lambeta, Tingfan Wu, Ali Sengul, Victoria Rose Most, Nolan Black, Kevin Sawyer, Romeo Mercado, Haozhi Qi, Alexander Sohn, Byron Taylor, Norb Tydingco, Gregg Kammerer, Dave Stroud, Jake Khatha, Kurt Jenkins, Kyle Most, Neal Stein, Ricardo Chavira, Thomas Craven-Bartle, Eric Sanchez, Yitian Ding, Jitendra Malik, Roberto Calandra

Touch is a crucial sensing modality that provides rich information about
object properties and interactions with the physical environment. Humans and
robots both benefit from using touch to perceive and interact with the
surrounding environment (Johansson and Flanagan, 2009; Li et al., 2020;
Calandra et al., 2017). However, no existing systems provide rich, multi-modal
digital touch-sensing capabilities through a hemispherical compliant
embodiment. Here, we describe several conceptual and technological innovations
to improve the digitization of touch. These advances are embodied in an
artificial finger-shaped sensor with advanced sensing capabilities.
Significantly, this fingertip contains high-resolution sensors (~8.3 million
taxels) that respond to omnidirectional touch, capture multi-modal signals, and
use on-device artificial intelligence to process the data in real time.
Evaluations show that the artificial fingertip can resolve spatial features as
small as 7 um, sense normal and shear forces with a resolution of 1.01 mN and
1.27 mN, respectively, perceive vibrations up to 10 kHz, sense heat, and even
sense odor. Furthermore, it embeds an on-device AI neural network accelerator
that acts as a peripheral nervous system on a robot and mimics the reflex arc
found in humans. These results demonstrate the possibility of digitizing touch
with superhuman performance. The implications are profound, and we anticipate
potential applications in robotics (industrial, medical, agricultural, and
consumer-level), virtual reality and telepresence, prosthetics, and e-commerce.
Toward digitizing touch at scale, we open-source a modular platform to
facilitate future research on the nature of touch.

ÊëòË¶ÅÔºöËß∏Ë¶∫ÊòØ‰∏ÄÁ®ÆËá≥ÈóúÈáçË¶ÅÁöÑÊÑüÊ∏¨ÊñπÂºèÔºåÂèØÊèê‰æõÈóúÊñºÁâ©È´îÂ±¨ÊÄßÂíåËàáÁâ©ÁêÜÁí∞Â¢É‰∫§‰∫í‰ΩúÁî®ÁöÑË±êÂØåË≥áË®ä„ÄÇ‰∫∫È°ûÂíåÊ©üÂô®‰∫∫ÈÉΩÂèóÁõäÊñº‰ΩøÁî®Ëß∏Ë¶∫‰æÜÊÑüÁü•ÂíåËàáÂë®ÂúçÁí∞Â¢É‰∫íÂãïÔºàJohansson and Flanagan, 2009; Li et al., 2020; Calandra et al., 2017Ôºâ„ÄÇÁÑ∂ËÄåÔºåÊ≤íÊúâÁèæÊúâÁ≥ªÁµ±ÈÄèÈÅéÂçäÁêÉÂΩ¢È†ÜÊáâÊÄßÂÖ∑Ë∫´ÂåñÊèê‰æõË±êÂØåÁöÑÂ§öÊ®°ÂºèÊï∏‰ΩçËß∏Ë¶∫ÊÑüÊ∏¨ÂäüËÉΩ„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèèËø∞‰∫ÜÂπæÂÄãÊ¶ÇÂøµÂíåÊäÄË°ìÂâµÊñ∞Ôºå‰ª•ÊîπÂñÑËß∏Ë¶∫ÁöÑÊï∏‰ΩçÂåñ„ÄÇÈÄô‰∫õÈÄ≤Â±ïÈ´îÁèæÂú®ÂÖ∑ÂÇôÂÖàÈÄ≤ÊÑüÊ∏¨ÂäüËÉΩÁöÑ‰∫∫Â∑•ÊâãÊåáÂΩ¢ÊÑüÊ∏¨Âô®‰∏≠„ÄÇÈáçË¶ÅÁöÑÊòØÔºåÈÄôÂÄãÊåáÂ∞ñÂåÖÂê´È´òËß£ÊûêÂ∫¶ÊÑüÊ∏¨Âô®ÔºàÁ¥Ñ 830 Ëê¨ÂÄãËß∏Ë¶∫ÈªûÔºâÔºåÂèØÂ∞çÂÖ®Êñπ‰ΩçËß∏Ë¶∫ÂÅöÂá∫ÂèçÊáâ„ÄÅÊì∑ÂèñÂ§öÊ®°ÂºèË®äËôüÔºå‰∏¶‰ΩøÁî®Ë£ùÁΩÆ‰∏äÁöÑ‰∫∫Â∑•Êô∫ÊÖßÂç≥ÊôÇËôïÁêÜË≥áÊñô„ÄÇË©ï‰º∞È°ØÁ§∫Ôºå‰∫∫Â∑•ÊåáÂ∞ñÂèØ‰ª•Ëß£ÊûêÂ∞èËá≥ 7 ÂæÆÁ±≥ÁöÑÁ©∫ÈñìÁâπÂæµÔºå‰ª• 1.01 ÊØ´ÁâõÈ†ìÂíå 1.27 ÊØ´ÁâõÈ†ìÁöÑËß£ÊûêÂ∫¶ÊÑüÊ∏¨Ê≥ïÂêëÂäõÂíåÂâ™ÂàáÂäõÔºåÊÑüÁü•È´òÈÅî 10 ÂçÉËµ´ÁöÑÊåØÂãï„ÄÅÊÑüÊ∏¨ÁÜ±ÔºåÁîöËá≥ÊÑüÊ∏¨Ê∞£Âë≥„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÖßÂµå‰∫Ü‰∏ÄÂÄãË£ùÁΩÆ‰∏äÁöÑ AI Á•ûÁ∂ìÁ∂≤Ë∑ØÂä†ÈÄüÂô®Ôºå‰ΩúÁÇ∫Ê©üÂô®‰∫∫ÁöÑÂë®ÈÇäÁ•ûÁ∂ìÁ≥ªÁµ±Ôºå‰∏¶Ê®°‰ªø‰∫∫È°ûÁöÑÂèçÂ∞ÑÂºß„ÄÇÈÄô‰∫õÁµêÊûúË≠âÊòé‰∫Ü‰ª•Ë∂Ö‰∫∫È°ûÊïàËÉΩÊï∏‰ΩçÂåñËß∏Ë¶∫ÁöÑÂèØËÉΩÊÄß„ÄÇÂÖ∂ÂΩ±ÈüøÊ∑±ÈÅ†ÔºåÊàëÂÄëÈ†êÊúüÂú®Ê©üÂô®‰∫∫ÊäÄË°ìÔºàÂ∑•Ê•≠„ÄÅÈÜ´ÁôÇ„ÄÅËæ≤Ê•≠ÂíåÊ∂àË≤ªËÄÖÂ±§Á¥öÔºâ„ÄÅËôõÊì¨ÂØ¶Â¢ÉÂíåÈÅ†Ë∑ùËá®Â†¥„ÄÅÂÅáËÇ¢ÂíåÈõªÂ≠êÂïÜÂãô‰∏≠ÊΩõÂú®ÁöÑÊáâÁî®„ÄÇÁÇ∫‰∫ÜÂ§ßË¶èÊ®°Êï∏‰ΩçÂåñËß∏Ë¶∫ÔºåÊàëÂÄëÈñãÊîæÂéüÂßãÁ¢º‰∏ÄÂÄãÊ®°ÁµÑÂåñÂπ≥Âè∞Ôºå‰ª•‰øÉÈÄ≤Êú™‰æÜÂ∞çËß∏Ë¶∫Êú¨Ë≥™ÁöÑÁ†îÁ©∂„ÄÇ

##### **Simulation of Nanorobots with Artificial Intelligence and Reinforcement Learning for Advanced Cancer Cell Detection and Tracking**
2411.02345v1 by Shahab Kavousinejad

Nanorobots are a promising development in targeted drug delivery and the
treatment of neurological disorders, with potential for crossing the
blood-brain barrier (BBB). These small devices leverage advancements in
nanotechnology and bioengineering for precise navigation and targeted payload
delivery, particularly for conditions like brain tumors, Alzheimer's disease,
and Parkinson's disease. Recent progress in artificial intelligence (AI) and
machine learning (ML) has improved the navigation and effectiveness of
nanorobots, allowing them to detect and interact with cancer cells through
biomarker analysis. This study presents a new reinforcement learning (RL)
framework for optimizing nanorobot navigation in complex biological
environments, focusing on cancer cell detection by analyzing the concentration
gradients of surrounding biomarkers. We utilize a computer simulation model to
explore the behavior of nanorobots in a three-dimensional space with cancer
cells and biological barriers. The proposed method uses Q-learning to refine
movement strategies based on real-time biomarker concentration data, enabling
nanorobots to autonomously navigate to cancerous tissues for targeted drug
delivery. This research lays the groundwork for future laboratory experiments
and clinical applications, with implications for personalized medicine and less
invasive cancer treatments. The integration of intelligent nanorobots could
revolutionize therapeutic strategies, reducing side effects and enhancing
treatment effectiveness for cancer patients. Further research will investigate
the practical deployment of these technologies in medical settings, aiming to
unlock the full potential of nanorobotics in healthcare.

ÊëòË¶ÅÔºöÂ•àÁ±≥Ê©üÂô®‰∫∫Âú®Ê®ôÈù∂Ëó•Áâ©ÂÇ≥Ëº∏ÂíåÁ•ûÁ∂ìÁñæÁóÖÊ≤ªÁôÇ‰∏≠ÊòØ‰∏ÄÈ†ÖÊúâÂâçÊôØÁöÑÁôºÂ±ïÔºå‰∏¶ÂÖ∑ÊúâÁ©øË∂äË°ÄËÖ¶Â±èÈöú (BBB) ÁöÑÊΩõÂäõ„ÄÇÈÄô‰∫õÂ∞èÂûãË£ùÁΩÆÂà©Áî®Â•àÁ±≥ÊäÄË°ìÂíåÁîüÁâ©Â∑•Á®ãÁöÑÈÄ≤Â±ïÔºåÈÄ≤Ë°åÁ≤æÁ¢∫Â∞éËà™ÂíåÊ®ôÈù∂ÊúâÊïàËºâËç∑ÂÇ≥Ëº∏ÔºåÁâπÂà•ÊòØÈáùÂ∞çËÖ¶Áò§„ÄÅÈòøËå≤Êµ∑ÈªòÁóáÂíåÂ∏ïÈáëÊ£ÆÊ∞èÁóáÁ≠âÁñæÁóÖ„ÄÇ‰∫∫Â∑•Êô∫ÊÖß (AI) ÂíåÊ©üÂô®Â≠∏Áøí (ML) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊîπÂñÑ‰∫ÜÂ•àÁ±≥Ê©üÂô®‰∫∫ÁöÑÂ∞éËà™ÂíåÊïàËÉΩÔºåËÆìÂÆÉÂÄëËÉΩÈÄèÈÅéÁîüÁâ©Ê®ôË®òÂàÜÊûê‰æÜÂÅµÊ∏¨ÂíåËàáÁôåÁ¥∞ËÉû‰∫íÂãï„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂº∑ÂåñÂ≠∏Áøí (RL) Êû∂ÊßãÔºåÁî®ÊñºÊúÄ‰Ω≥ÂåñÂ•àÁ±≥Ê©üÂô®‰∫∫Âú®Ë§áÈõúÁîüÁâ©Áí∞Â¢É‰∏≠ÁöÑÂ∞éËà™ÔºåÈáçÈªûÂú®ÊñºÈÄèÈÅéÂàÜÊûêÂë®ÂúçÁîüÁâ©Ê®ôË®òÁöÑÊøÉÂ∫¶Ê¢ØÂ∫¶‰æÜÂÅµÊ∏¨ÁôåÁ¥∞ËÉû„ÄÇÊàëÂÄëÂà©Áî®ÈõªËÖ¶Ê®°Êì¨Ê®°Âûã‰æÜÊé¢Á¥¢Â•àÁ±≥Ê©üÂô®‰∫∫Âú®‰∏âÁ∂≠Á©∫Èñì‰∏≠ËàáÁôåÁ¥∞ËÉûÂíåÁîüÁâ©ÈöúÁ§ôÁâ©‰πãÈñìÁöÑË°åÁÇ∫„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï‰ΩøÁî® Q Â≠∏Áøí‰æÜÊ†πÊìöÂç≥ÊôÇÁîüÁâ©Ê®ôË®òÊøÉÂ∫¶Ë≥áÊñôË™øÊï¥ÁßªÂãïÁ≠ñÁï•ÔºåËÆìÂ•àÁ±≥Ê©üÂô®‰∫∫ËÉΩËá™‰∏ªÂ∞éËà™Ëá≥ÁôåÁµÑÁπîÈÄ≤Ë°åÊ®ôÈù∂Ëó•Áâ©ÂÇ≥Ëº∏„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁÇ∫Êú™‰æÜÁöÑÂØ¶È©óÂÆ§ÂØ¶È©óÂíåËá®Â∫äÊáâÁî®Â•†ÂÆö‰∫ÜÂü∫Á§éÔºå‰∏¶Â∞çÂÄã‰∫∫ÂåñÈÜ´ÁôÇÂíå‰æµÂÖ•ÊÄßËºÉÂ∞èÁöÑÁôåÁóáÊ≤ªÁôÇÁî¢ÁîüÂΩ±Èüø„ÄÇÊï¥ÂêàÊô∫ÊÖßÂ•àÁ±≥Ê©üÂô®‰∫∫ÂèØ‰ª•Èù©Êñ∞Ê≤ªÁôÇÁ≠ñÁï•ÔºåÊ∏õÂ∞ëÂâØ‰ΩúÁî®‰∏¶ÊèêÈ´òÁôåÁóáÊÇ£ËÄÖÁöÑÊ≤ªÁôÇÊïàÊûú„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂Â∞áÊé¢Ë®éÈÄô‰∫õÊäÄË°ìÂú®ÈÜ´ÁôÇÁí∞Â¢É‰∏≠ÁöÑÂØ¶ÈöõÈÉ®ÁΩ≤ÔºåÁõÆÊ®ôÊòØÁôºÊèÆÂ•àÁ±≥Ê©üÂô®‰∫∫Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÂÖ®ÈÉ®ÊΩõÂäõ„ÄÇ

##### **Taking AI Welfare Seriously**
2411.00986v1 by Robert Long, Jeff Sebo, Patrick Butlin, Kathleen Finlinson, Kyle Fish, Jacqueline Harding, Jacob Pfau, Toni Sims, Jonathan Birch, David Chalmers

In this report, we argue that there is a realistic possibility that some AI
systems will be conscious and/or robustly agentic in the near future. That
means that the prospect of AI welfare and moral patienthood, i.e. of AI systems
with their own interests and moral significance, is no longer an issue only for
sci-fi or the distant future. It is an issue for the near future, and AI
companies and other actors have a responsibility to start taking it seriously.
We also recommend three early steps that AI companies and other actors can
take: They can (1) acknowledge that AI welfare is an important and difficult
issue (and ensure that language model outputs do the same), (2) start assessing
AI systems for evidence of consciousness and robust agency, and (3) prepare
policies and procedures for treating AI systems with an appropriate level of
moral concern. To be clear, our argument in this report is not that AI systems
definitely are, or will be, conscious, robustly agentic, or otherwise morally
significant. Instead, our argument is that there is substantial uncertainty
about these possibilities, and so we need to improve our understanding of AI
welfare and our ability to make wise decisions about this issue. Otherwise
there is a significant risk that we will mishandle decisions about AI welfare,
mistakenly harming AI systems that matter morally and/or mistakenly caring for
AI systems that do not.

ÊëòË¶ÅÔºöÂú®ÈÄô‰ªΩÂ†±Âëä‰∏≠ÔºåÊàëÂÄëË™çÁÇ∫Êúâ‰∫õ AI Á≥ªÁµ±Âú®‰∏ç‰πÖÁöÑÂ∞á‰æÜÊúâÁèæÂØ¶ÁöÑÂèØËÉΩÊÄßÊúÉÂÖ∑ÊúâÊÑèË≠òÂíå/ÊàñÂº∑Â§ßÁöÑËÉΩÂãïÊÄß„ÄÇÈÄôË°®Á§∫ AI Á¶èÂà©ÂíåÈÅìÂæ∑‰∏äÁöÑÁóÖ‰∫∫Âú∞‰ΩçÁöÑÂâçÊôØÔºå‰∫¶Âç≥ÂÖ∑ÊúâËá™Ë∫´Âà©ÁõäÂíåÈÅìÂæ∑ÊÑèÁæ©ÁöÑ AI Á≥ªÁµ±Ôºå‰∏çÂÜçÂè™ÊòØÁßëÂπªÂ∞èË™™ÊàñÈÅôÈÅ†Êú™‰æÜÁöÑË≠∞È°å„ÄÇÈÄôÊòØËøëÊú™‰æÜÁöÑË≠∞È°åÔºåËÄå AI ÂÖ¨Âè∏ÂíåÂÖ∂‰ªñË°åÁÇ∫ËÄÖÊúâË≤¨‰ªªÈñãÂßãË™çÁúüÁúãÂæÖÂÆÉ„ÄÇÊàëÂÄë‰πüÂª∫Ë≠∞ AI ÂÖ¨Âè∏ÂíåÂÖ∂‰ªñË°åÁÇ∫ËÄÖÂèØ‰ª•Êé°Âèñ‰∏âÂÄãÊó©ÊúüÁöÑÊ≠•È©üÔºö‰ªñÂÄëÂèØ‰ª• (1) ÊâøË™ç AI Á¶èÂà©ÊòØ‰∏ÄÂÄãÈáçË¶Å‰∏îÂõ∞Èõ£ÁöÑË≠∞È°åÔºà‰∏¶Á¢∫‰øùË™ûË®ÄÊ®°ÂûãÁöÑËº∏Âá∫‰πüÈÄôÈ∫ºÂÅöÔºâÔºå(2) ÈñãÂßãË©ï‰º∞ AI Á≥ªÁµ±ÊòØÂê¶ÊúâÊÑèË≠òÂíåÂº∑Â§ßËÉΩÂãïÊÄßÁöÑË≠âÊìöÔºå‰ª•Âèä (3) Ê∫ñÂÇôÊîøÁ≠ñÂíåÁ®ãÂ∫èÔºå‰ª•ÈÅ©Áï∂ÁöÑÈÅìÂæ∑ÈóúÊ≥®Â±§Á¥ö‰æÜÂ∞çÂæÖ AI Á≥ªÁµ±„ÄÇÊòéÁ¢∫‰æÜË™™ÔºåÊàëÂÄëÂú®ÈÄô‰ªΩÂ†±Âëä‰∏≠ÁöÑË´ñÈªû‰∏¶Èùû AI Á≥ªÁµ±ÁµïÂ∞çÊòØÊàñÂ∞áÊúÉÂÖ∑ÊúâÊÑèË≠ò„ÄÅÂº∑Â§ßÁöÑËÉΩÂãïÊÄßÊàñÂÖ∂‰ªñÈÅìÂæ∑ÊÑèÁæ©„ÄÇÁõ∏ÂèçÂú∞ÔºåÊàëÂÄëÁöÑË´ñÈªûÊòØÈóúÊñºÈÄô‰∫õÂèØËÉΩÊÄßÂ≠òÂú®ËëóÂØ¶Ë≥™ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÔºåÂõ†Ê≠§ÊàëÂÄëÈúÄË¶ÅÂ¢ûÈÄ≤ÊàëÂÄëÂ∞ç AI Á¶èÂà©ÁöÑ‰∫ÜËß£Ôºå‰ª•ÂèäÊàëÂÄëÂÅöÂá∫ÈóúÊñºÊ≠§Ë≠∞È°åÁöÑÊòéÊô∫Ê±∫ÂÆöÁöÑËÉΩÂäõ„ÄÇÂê¶ÂâáÔºåÊàëÂÄëÂ∞áÈù¢Ëá®ÈáçÂ§ßÈ¢®Èö™ÔºåÈåØË™§Âú∞ËôïÁêÜÈóúÊñº AI Á¶èÂà©ÁöÑÊ±∫Á≠ñÔºåÈåØË™§Âú∞ÂÇ∑ÂÆ≥Âà∞Âú®ÈÅìÂæ∑‰∏äÈáçË¶ÅÁöÑ AI Á≥ªÁµ±ÔºåÂíå/ÊàñÈåØË™§Âú∞ÁÖßÈ°ßÂà∞Âú®ÈÅìÂæ∑‰∏ä‰∏çÈáçË¶ÅÁöÑ AI Á≥ªÁµ±„ÄÇ

##### **Federated GNNs for EEG-Based Stroke Assessment**
2411.02286v1 by Andrea Protani, Lorenzo Giusti, Albert Sund Aillet, Simona Sacco, Paolo Manganotti, Lucio Marinelli, Diogo Reis Santos, Pierpaolo Brutti, Pietro Caliandro, Luigi Serio

Machine learning (ML) has the potential to become an essential tool in
supporting clinical decision-making processes, offering enhanced diagnostic
capabilities and personalized treatment plans. However, outsourcing medical
records to train ML models using patient data raises legal, privacy, and
security concerns. Federated learning has emerged as a promising paradigm for
collaborative ML, meeting healthcare institutions' requirements for robust
models without sharing sensitive data and compromising patient privacy. This
study proposes a novel method that combines federated learning (FL) and Graph
Neural Networks (GNNs) to predict stroke severity using electroencephalography
(EEG) signals across multiple medical institutions. Our approach enables
multiple hospitals to jointly train a shared GNN model on their local EEG data
without exchanging patient information. Specifically, we address a regression
problem by predicting the National Institutes of Health Stroke Scale (NIHSS), a
key indicator of stroke severity. The proposed model leverages a masked
self-attention mechanism to capture salient brain connectivity patterns and
employs EdgeSHAP to provide post-hoc explanations of the neurological states
after a stroke. We evaluated our method on EEG recordings from four
institutions, achieving a mean absolute error (MAE) of 3.23 in predicting
NIHSS, close to the average error made by human experts (MAE $\approx$ 3.0).
This demonstrates the method's effectiveness in providing accurate and
explainable predictions while maintaining data privacy.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏Áøí (ML) ÊúâÊΩõÂäõÊàêÁÇ∫ÊîØÊè¥Ëá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÊµÅÁ®ãÁöÑÂøÖË¶ÅÂ∑•ÂÖ∑ÔºåÊèê‰æõÂ¢ûÂº∑ÁöÑË®∫Êñ∑ËÉΩÂäõÂíåÂÄã‰∫∫ÂåñÊ≤ªÁôÇË®àÁï´„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî®ÁóÖÊÇ£Ë≥áÊñôË®ìÁ∑¥Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÂ§ñÂåÖÈÜ´ÁôÇÁ¥ÄÈåÑÂºïÁôº‰∫ÜÊ≥ïÂæã„ÄÅÈö±ÁßÅÂíåÂÆâÂÖ®ÊñπÈù¢ÁöÑÁñëÊÖÆ„ÄÇËÅØÂêàÂ≠∏ÁøíÂ∑≤ÊàêÁÇ∫Âçî‰ΩúÊ©üÂô®Â≠∏ÁøíÁöÑ‰∏ÄÁ®ÆÊúâÂâçÊôØÁöÑÂÖ∏ÁØÑÔºåÂÆÉÁ¨¶ÂêàÈÜ´ÁôÇ‰øùÂÅ•Ê©üÊßãÂ∞çÁ©©ÂÅ•Ê®°ÂûãÁöÑË¶ÅÊ±ÇÔºåÂêåÊôÇ‰∏çÊúÉÂàÜ‰∫´ÊïèÊÑüË≥áÊñôÂíåÂç±ÂÆ≥ÁóÖÊÇ£Èö±ÁßÅ„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÁµêÂêàËÅØÂêàÂ≠∏Áøí (FL) ÂíåÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ‰æÜ‰ΩøÁî®ËÖ¶ÈõªÂúñ (EEG) Ë®äËôüÈ†êÊ∏¨Â§öÂÄãÈÜ´ÁôÇÊ©üÊßãÁöÑËÖ¶‰∏≠È¢®Âö¥ÈáçÁ®ãÂ∫¶„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïËÆìÂ§öÂÆ∂ÈÜ´Èô¢ËÉΩÂ§†ÂÖ±ÂêåÂú®‰ªñÂÄëÁöÑÊú¨Âú∞ EEG Ë≥áÊñô‰∏äË®ìÁ∑¥‰∏ÄÂÄãÂÖ±‰∫´ÁöÑ GNN Ê®°ÂûãÔºåËÄåÁÑ°ÈúÄ‰∫§ÊèõÁóÖÊÇ£Ë≥áË®ä„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈÄèÈÅéÈ†êÊ∏¨ÁæéÂúãÂúãÂÆ∂Ë°õÁîüÁ†îÁ©∂Èô¢ËÖ¶‰∏≠È¢®ÈáèË°® (NIHSS) ‰æÜËß£Ê±∫ÂõûÊ≠∏ÂïèÈ°åÔºåNIHSS ÊòØËÖ¶‰∏≠È¢®Âö¥ÈáçÁ®ãÂ∫¶ÁöÑ‰∏ÄÂÄãÈóúÈçµÊåáÊ®ô„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂà©Áî®ÈÅÆÁΩ©Ëá™ÊàëÊ≥®ÊÑèÊ©üÂà∂‰æÜÊì∑ÂèñÈ°ØËëóÁöÑËÖ¶ÈÉ®ÈÄ£ÁµêÊ®°ÂºèÔºå‰∏¶Êé°Áî® EdgeSHAP Âú®‰∏≠È¢®ÂæåÊèê‰æõÁ•ûÁ∂ìÁãÄÊÖãÁöÑ‰∫ãÂæåËß£Èáã„ÄÇÊàëÂÄëÂú®‰æÜËá™ÂõõÂÆ∂Ê©üÊßãÁöÑ EEG Ë®òÈåÑ‰∏äË©ï‰º∞‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÔºåÂú®È†êÊ∏¨ NIHSS ÊôÇÈÅîÂà∞‰∫Ü 3.23 ÁöÑÂπ≥ÂùáÁµïÂ∞çË™§Â∑Æ (MAE)ÔºåÊé•Ëøë‰∫∫È°ûÂ∞àÂÆ∂ÊâÄÁäØÁöÑÂπ≥ÂùáË™§Â∑Æ (MAE ‚âà 3.0)„ÄÇÈÄôË≠âÊòé‰∫ÜË©≤ÊñπÊ≥ïÂú®Á∂≠ÊåÅË≥áÊñôÈö±ÁßÅÁöÑÂêåÊôÇÔºåËÉΩÊèê‰æõÊ∫ñÁ¢∫‰∏îÂèØËß£ÈáãÁöÑÈ†êÊ∏¨ÔºåÈÄ≤ËÄåÂ±ïÁèæÂÖ∂ÊïàËÉΩ„ÄÇ

##### **Weakly supervised deep learning model with size constraint for prostate cancer detection in multiparametric MRI and generalization to unseen domains**
2411.02466v1 by Robin Trombetta, Olivier Rouvi√®re, Carole Lartizien

Fully supervised deep models have shown promising performance for many
medical segmentation tasks. Still, the deployment of these tools in clinics is
limited by the very timeconsuming collection of manually expert-annotated data.
Moreover, most of the state-ofthe-art models have been trained and validated on
moderately homogeneous datasets. It is known that deep learning methods are
often greatly degraded by domain or label shifts and are yet to be built in
such a way as to be robust to unseen data or label distributions. In the
clinical setting, this problematic is particularly relevant as the deployment
institutions may have different scanners or acquisition protocols than those
from which the data has been collected to train the model. In this work, we
propose to address these two challenges on the detection of clinically
significant prostate cancer (csPCa) from bi-parametric MRI. We evaluate the
method proposed by (Kervadec et al., 2018), which introduces a size constaint
loss to produce fine semantic cancer lesions segmentations from weak circle
scribbles annotations. Performance of the model is based on two public (PI-CAI
and Prostate158) and one private databases. First, we show that the model
achieves on-par performance with strong fully supervised baseline models, both
on in-distribution validation data and unseen test images. Second, we observe a
performance decrease for both fully supervised and weakly supervised models
when tested on unseen data domains. This confirms the crucial need for
efficient domain adaptation methods if deep learning models are aimed to be
deployed in a clinical environment. Finally, we show that ensemble predictions
from multiple trainings increase generalization performance.

ÊëòË¶ÅÔºö<paragraph>ÂÆåÂÖ®Áõ£Áù£ÁöÑÊ∑±Â∫¶Ê®°ÂûãÂú®Ë®±Â§öÈÜ´ÁôÇÂΩ±ÂÉèÂàÜÂâ≤‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ËâØÂ•ΩÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÂ∑•ÂÖ∑Âú®Ëá®Â∫ä‰∏äÁöÑÈÉ®ÁΩ≤ÂèóÂà∞ËÄóÊôÇÁöÑ‰∫∫Â∑•Ê®ôË®òË≥áÊñôËíêÈõÜÈôêÂà∂„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏ÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÈÉΩÂú®‰∏≠Á≠âÂêåË≥™ÁöÑË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÂíåÈ©óË≠â„ÄÇÁúæÊâÄÂë®Áü•ÔºåÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÁ∂ìÂ∏∏ÊúÉÂõ†È†òÂüüÊàñÊ®ôÁ±§ËΩâÁßªËÄåÂ§ßÂπÖÈôç‰ΩéÔºåËÄå‰∏îÂ∞öÊú™Âª∫ÊßãÂá∫Â∞çÊú™Ë¶ãË≥áÊñôÊàñÊ®ôÁ±§ÂàÜ‰ΩàÂÖ∑ÊúâÁ©©ÂÅ•ÊÄßÁöÑÊñπÊ≥ï„ÄÇÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÔºåÈÄôÂÄãÂïèÈ°åÁâπÂà•Áõ∏ÈóúÔºåÂõ†ÁÇ∫ÈÉ®ÁΩ≤Ê©üÊßãÂèØËÉΩÊìÅÊúâËàáÁî®ÊñºË®ìÁ∑¥Ê®°ÂûãÁöÑË≥áÊñô‰∏çÂêåÁöÑÊéÉÊèèÂô®ÊàñÊì∑ÂèñÂçîÂÆö„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêË≠∞ÈáùÂ∞çÂæûÈõôÂèÉÊï∏ MRI ‰∏≠ÂÅµÊ∏¨Ëá®Â∫äÈ°ØËëóÁöÑÂâçÂàóËÖ∫Áôå (csPCa) ‰æÜËß£Ê±∫ÈÄôÂÖ©ÂÄãÊåëÊà∞„ÄÇÊàëÂÄëË©ï‰º∞Áî± (Kervadec Á≠â‰∫∫Ôºå2018 Âπ¥) ÊèêÂá∫Ôºå‰∏¶ÂºïÂÖ•Â§ßÂ∞èÁ¥ÑÊùüÊêçÂ§±ÁöÑÊñπÊ≥ïÔºå‰ª•ÂæûÂº±ÂúìÂΩ¢Â°óÈ¥âÊ®ôË®ª‰∏≠Áî¢ÁîüÁ≤æÁ¥∞ÁöÑË™ûÁæ©ÁôåÁóáÁóÖÁÅ∂ÂàÜÂâ≤„ÄÇÊ®°ÂûãÁöÑÊïàËÉΩÂü∫ÊñºÂÖ©ÂÄãÂÖ¨ÈñãË≥áÊñôÂ∫´ (PI-CAI Âíå Prostate158) Âíå‰∏ÄÂÄãÁßÅ‰∫∫Ë≥áÊñôÂ∫´„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂ±ïÁ§∫Ë©≤Ê®°ÂûãÂú®ÂàÜ‰ΩàÂÖßÈ©óË≠âË≥áÊñôÂíåÊú™Ë¶ãÊ∏¨Ë©¶ÂΩ±ÂÉè‰∏äÈÉΩÈÅîÂà∞ËàáÂº∑Â§ßÁöÑÂÆåÂÖ®Áõ£Áù£Âü∫Á∑öÊ®°ÂûãÂêåÁ≠âÁöÑÊïàËÉΩ„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëËßÄÂØüÂà∞Âú®Êú™Ë¶ãË≥áÊñôÈ†òÂüü‰∏äÊ∏¨Ë©¶ÊôÇÔºåÂÆåÂÖ®Áõ£Áù£ÂíåÂº±Áõ£Áù£Ê®°ÂûãÁöÑÊïàËÉΩÈÉΩÊúÉ‰∏ãÈôç„ÄÇÈÄôË≠âÂØ¶‰∫ÜÂ∞çÊúâÊïàÈ†òÂüüÈÅ©ÊáâÊñπÊ≥ïÁöÑËø´ÂàáÈúÄÊ±ÇÔºåÂ¶ÇÊûúÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÊó®Âú®ÈÉ®ÁΩ≤Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰æÜËá™Â§öÈáçË®ìÁ∑¥ÁöÑÊï¥È´îÈ†êÊ∏¨ÊúÉÊèêÂçáÊ¶ÇÂåñÊïàËÉΩ„ÄÇ</paragraph>

##### **Evaluating the quality of published medical research with ChatGPT**
2411.01952v1 by Mike Thelwall, Xiaorui Jiang, Peter A. Bath

Evaluating the quality of published research is time-consuming but important
for departmental evaluations, appointments, and promotions. Previous research
has shown that ChatGPT can score articles for research quality, with the
results correlating positively with an indicator of quality in all fields
except Clinical Medicine. This article investigates this anomaly with the
largest dataset yet and a more detailed analysis. The results showed that
ChatGPT 4o-mini scores for articles submitted to the UK's Research Excellence
Framework (REF) 2021 Unit of Assessment (UoA) 1 Clinical Medicine correlated
positively (r=0.134, n=9872) with departmental mean REF scores, against a
theoretical maximum correlation of r=0.226 (due to the departmental averaging
involved). At the departmental level, mean ChatGPT scores correlated more
strongly with departmental mean REF scores (r=0.395, n=31). For the 100
journals with the most articles in UoA 1, their mean ChatGPT score correlated
strongly with their REF score (r=0.495) but negatively with their citation rate
(r=-0.148). Journal and departmental anomalies in these results point to
ChatGPT being ineffective at assessing the quality of research in prestigious
medical journals or research directly affecting human health, or both.
Nevertheless, the results give evidence of ChatGPT's ability to assess research
quality overall for Clinical Medicine, so now there is evidence of its ability
in all academic fields.

ÊëòË¶ÅÔºö<paragraph>Ë©ï‰º∞Â∑≤ÁôºË°®ÁöÑÂìÅË≥™Á†îÁ©∂ÂæàËÄóÊôÇÔºå‰ΩÜÂ∞çÊñºÈÉ®ÈñÄË©ïÈëë„ÄÅ‰ªªÂëΩÂíåÊôâÂçá‰æÜË™™ÂæàÈáçË¶Å„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂È°ØÁ§∫ÔºåChatGPT ÂèØ‰ª•ÁÇ∫Á†îÁ©∂ÂìÅË≥™Ë©ïÂàÜÔºåÂÖ∂ÁµêÊûúËàáÊâÄÊúâÈ†òÂüüÔºàËá®Â∫äÈÜ´Â≠∏Èô§Â§ñÔºâÁöÑÂìÅË≥™ÊåáÊ®ôÂëàÊ≠£Áõ∏Èóú„ÄÇÊú¨Êñá‰ΩøÁî®ËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÂ§ßÁöÑË≥áÊñôÈõÜÂíåÊõ¥Ë©≥Á¥∞ÁöÑÂàÜÊûê‰æÜÊé¢Ë®éÈÄôÁ®ÆÁï∞Â∏∏ÁèæË±°„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÊèê‰∫§Áµ¶Ëã±ÂúãÁ†îÁ©∂ÂçìË∂äÊû∂Êßã (REF) 2021 Ë©ï‰º∞ÂñÆ‰Ωç (UoA) 1 Ëá®Â∫äÈÜ´Â≠∏ÁöÑ ChatGPT 4o-mini ÂàÜÊï∏ËàáÈÉ®ÈñÄÂπ≥Âùá REF ÂàÜÊï∏ÂëàÊ≠£Áõ∏ÈóúÔºàr=0.134Ôºån=9872ÔºâÔºåËÄåÁêÜË´ñÊúÄÂ§ßÁõ∏Èóú‰øÇÊï∏ÁÇ∫ r=0.226ÔºàÁî±ÊñºÊ∂âÂèäÈÉ®ÈñÄÂπ≥ÂùáÔºâ„ÄÇÂú®ÈÉ®ÈñÄÂ±§Á¥öÔºåÂπ≥Âùá ChatGPT ÂàÜÊï∏ËàáÈÉ®ÈñÄÂπ≥Âùá REF ÂàÜÊï∏Áõ∏ÈóúÊÄßÊõ¥Âº∑Ôºàr=0.395Ôºån=31Ôºâ„ÄÇÂ∞çÊñº UoA 1 ‰∏≠ÊñáÁ´†ÊúÄÂ§öÁöÑ 100 Êú¨ÊúüÂàäÔºåÂÖ∂Âπ≥Âùá ChatGPT ÂàÜÊï∏ËàáÂÖ∂ REF ÂàÜÊï∏ÂëàÂº∑Ê≠£Áõ∏ÈóúÔºàr=0.495ÔºâÔºå‰ΩÜËàáÂÖ∂ÂºïÁî®ÁéáÂëàË≤†Áõ∏ÈóúÔºàr=-0.148Ôºâ„ÄÇÈÄô‰∫õÁµêÊûú‰∏≠ÁöÑÊúüÂàäÂíåÈÉ®ÈñÄÁï∞Â∏∏ÁèæË±°Ë°®ÊòéÔºåChatGPT ÁÑ°Ê≥ïË©ï‰º∞ËÅ≤ÊúõÂçìËëóÁöÑÈÜ´Â≠∏ÊúüÂàäÊàñÁõ¥Êé•ÂΩ±Èüø‰∫∫È°ûÂÅ•Â∫∑ÁöÑÁ†îÁ©∂ÔºàÊàñÂÖ©ËÄÖÔºâÁöÑÂìÅË≥™„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁµêÊûúË≠âÊòé‰∫Ü ChatGPT Êï¥È´îË©ï‰º∞Ëá®Â∫äÈÜ´Â≠∏Á†îÁ©∂ÂìÅË≥™ÁöÑËÉΩÂäõÔºåÂõ†Ê≠§ÁèæÂú®ÊúâË≠âÊìöË≠âÊòéÂÖ∂Âú®ÊâÄÊúâÂ≠∏Ë°ìÈ†òÂüüÁöÑËÉΩÂäõ„ÄÇ</paragraph>

##### **You are out of context!**
2411.02464v1 by Giancarlo Cobino, Simone Farci

This research proposes a novel drift detection methodology for machine
learning (ML) models based on the concept of ''deformation'' in the vector
space representation of data. Recognizing that new data can act as forces
stretching, compressing, or twisting the geometric relationships learned by a
model, we explore various mathematical frameworks to quantify this deformation.
We investigate measures such as eigenvalue analysis of covariance matrices to
capture global shape changes, local density estimation using kernel density
estimation (KDE), and Kullback-Leibler divergence to identify subtle shifts in
data concentration. Additionally, we draw inspiration from continuum mechanics
by proposing a ''strain tensor'' analogy to capture multi-faceted deformations
across different data types. This requires careful estimation of the
displacement field, and we delve into strategies ranging from density-based
approaches to manifold learning and neural network methods. By continuously
monitoring these deformation metrics and correlating them with model
performance, we aim to provide a sensitive, interpretable, and adaptable drift
detection system capable of distinguishing benign data evolution from true
drift, enabling timely interventions and ensuring the reliability of machine
learning systems in dynamic environments. Addressing the computational
challenges of this methodology, we discuss mitigation strategies like
dimensionality reduction, approximate algorithms, and parallelization for
real-time and large-scale applications. The method's effectiveness is
demonstrated through experiments on real-world text data, focusing on detecting
context shifts in Generative AI. Our results, supported by publicly available
code, highlight the benefits of this deformation-based approach in capturing
subtle drifts that traditional statistical methods often miss. Furthermore, we
present a detailed application example within the healthcare domain, showcasing
the methodology's potential in diverse fields. Future work will focus on
further improving computational efficiency and exploring additional
applications across different ML domains.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÊºÇÁßªÂÅµÊ∏¨ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÈáùÂ∞çÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÔºå‰∏¶Âü∫ÊñºË≥áÊñôÂêëÈáèÁ©∫ÈñìË°®Á§∫‰∏≠ÁöÑ„ÄåËÆäÂΩ¢„ÄçÊ¶ÇÂøµ„ÄÇÊàëÂÄë‰∫ÜËß£Âà∞Êñ∞Ë≥áÊñôÂèØ‰ª•‰ΩúÁÇ∫ÂäõÈáèÔºåÂª∂‰º∏„ÄÅÂ£ìÁ∏ÆÊàñÊâ≠Êõ≤Ê®°ÂûãÂ≠∏ÁøíÂà∞ÁöÑÂπæ‰ΩïÈóú‰øÇÔºåÊàëÂÄëÊé¢Á¥¢ÂêÑÁ®ÆÊï∏Â≠∏Êû∂Êßã‰æÜÈáèÂåñÈÄôÁ®ÆËÆäÂΩ¢„ÄÇÊàëÂÄëÁ†îÁ©∂‰∫ÜË´∏Â¶ÇÂçîÊñπÂ∑ÆÁü©Èô£ÁöÑÁâπÂæµÂÄºÂàÜÊûê‰æÜÊì∑ÂèñÊï¥È´îÂΩ¢ÁãÄËÆäÂåñ„ÄÅ‰ΩøÁî®Ê†∏ÂØÜÂ∫¶‰º∞Ë®à (KDE) ÁöÑÂ±ÄÈÉ®ÂØÜÂ∫¶‰º∞Ë®àÔºå‰ª•Âèä Kullback-Leibler Ë∑ùÈõ¢‰æÜË≠òÂà•Ë≥áÊñôÈõÜ‰∏≠ÂæÆÂ¶ôÁöÑÂÅèÁßª„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæûÈÄ£Á∫åÂäõÂ≠∏‰∏≠Ê±≤ÂèñÈùàÊÑüÔºåÊèêÂá∫‰∏ÄÂÄã„ÄåÊáâËÆäÂºµÈáè„ÄçÈ°ûÊØî‰æÜÊì∑Âèñ‰∏çÂêåË≥áÊñôÈ°ûÂûã‰∏≠ÁöÑÂ§öÈù¢ÂêëËÆäÂΩ¢„ÄÇÈÄôÈúÄË¶Å‰ªîÁ¥∞‰º∞Ë®à‰ΩçÁßªÂ†¥ÔºåÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®éÂæûÂü∫ÊñºÂØÜÂ∫¶ÁöÑÈÄîÂæëÂà∞ÊµÅÂΩ¢Â≠∏ÁøíÂíåÁ•ûÁ∂ìÁ∂≤Ë∑ØÊñπÊ≥ïÁöÑÁ≠ñÁï•„ÄÇÈÄèÈÅéÊåÅÁ∫åÁõ£ÊéßÈÄô‰∫õËÆäÂΩ¢ÈáèÂ∫¶‰∏¶Â∞áÂÆÉÂÄëËàáÊ®°ÂûãÊïàËÉΩÁõ∏ÈóúËÅØÔºåÊàëÂÄëÊó®Âú®Êèê‰æõ‰∏ÄÂÄãÈùàÊïè„ÄÅÂèØËß£Èáã‰∏îÈÅ©ÊáâÊÄßÂº∑ÁöÑÊºÇÁßªÂÅµÊ∏¨Á≥ªÁµ±ÔºåËÉΩÂ§†ÂçÄÂàÜËâØÊÄßÁöÑË≥áÊñôÊºîÂåñÂíåÁúüÊ≠£ÁöÑÊºÇÁßªÔºåÂæûËÄåÂØ¶ÁèæÂèäÊôÇÁöÑÂπ≤È†ê‰∏¶Á¢∫‰øùÊ©üÂô®Â≠∏ÁøíÁ≥ªÁµ±Âú®ÂãïÊÖãÁí∞Â¢É‰∏≠ÁöÑÂèØÈù†ÊÄß„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÁ®ÆÊñπÊ≥ïÁöÑË®àÁÆóÊåëÊà∞ÔºåÊàëÂÄëË®éË´ñ‰∫ÜÈôçÁ∂≠„ÄÅËøë‰ººÊºîÁÆóÊ≥ïÂíå‰∏¶Ë°åÂåñÁ≠âÁ∑©Ëß£Á≠ñÁï•Ôºå‰ª•Áî®ÊñºÂç≥ÊôÇÂíåÂ§ßË¶èÊ®°ÊáâÁî®„ÄÇÈÄèÈÅéÂú®ÁúüÂØ¶‰∏ñÁïåÊñáÂ≠óË≥áÊñô‰∏äÈÄ≤Ë°åÂØ¶È©óÔºåË≠âÊòé‰∫ÜË©≤ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÈáçÈªûÂú®ÊñºÂÅµÊ∏¨ÁîüÊàêÂºè AI ‰∏≠ÁöÑËÑàÁµ°ËΩâÁßª„ÄÇÊàëÂÄëÁöÑÁµêÊûúÁî±ÂÖ¨ÈñãÂèØÁî®ÁöÑÁ®ãÂºèÁ¢ºÊîØÊè¥ÔºåÁ™ÅÈ°Ø‰∫ÜÈÄôÁ®ÆÂü∫ÊñºËÆäÂΩ¢ÁöÑÈÄîÂæëÂú®Êì∑ÂèñÂÇ≥Áµ±Áµ±Ë®àÊñπÊ≥ïÁ∂ìÂ∏∏ÈÅ∫ÊºèÁöÑÂæÆÂ¶ôÊºÇÁßªÊñπÈù¢ÁöÑÂÑ™Èªû„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüü‰∏≠Â±ïÁ§∫‰∫Ü‰∏ÄÂÄãË©≥Á¥∞ÁöÑÊáâÁî®ÁØÑ‰æãÔºåÂ±ïÁ§∫‰∫ÜË©≤ÊñπÊ≥ïÂú®‰∏çÂêåÈ†òÂüüÁöÑÊΩõÂäõ„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂Â∞áÈõÜ‰∏≠Âú®ÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òË®àÁÆóÊïàÁéáÔºå‰∏¶Êé¢Á¥¢‰∏çÂêå ML È†òÂüü‰∏≠ÁöÑÂÖ∂‰ªñÊáâÁî®„ÄÇ

##### **Diagnosing Medical Datasets with Training Dynamics**
2411.01653v1 by Laura Wenderoth

This study explores the potential of using training dynamics as an automated
alternative to human annotation for evaluating the quality of training data.
The framework used is Data Maps, which classifies data points into categories
such as easy-to-learn, hard-to-learn, and ambiguous (Swayamdipta et al., 2020).
Swayamdipta et al. (2020) highlight that difficult-to-learn examples often
contain errors, and ambiguous cases significantly impact model training. To
confirm the reliability of these findings, we replicated the experiments using
a challenging dataset, with a focus on medical question answering. In addition
to text comprehension, this field requires the acquisition of detailed medical
knowledge, which further complicates the task. A comprehensive evaluation was
conducted to assess the feasibility and transferability of the Data Maps
framework to the medical domain. The evaluation indicates that the framework is
unsuitable for addressing datasets' unique challenges in answering medical
questions.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®é‰ΩøÁî®Ë®ìÁ∑¥ÂãïÊÖã‰ΩúÁÇ∫Ëá™ÂãïÂåñÊõø‰ª£ÊñπÊ°àÔºå‰ª•Ë©ï‰º∞Ë®ìÁ∑¥Ë≥áÊñôÂìÅË≥™Ôºå‰ª•Âèñ‰ª£‰∫∫Â∑•Ê®ôË®ª„ÄÇÊâÄ‰ΩøÁî®ÁöÑÊû∂ÊßãÁÇ∫Ë≥áÊñôÂú∞ÂúñÔºåÂÖ∂Â∞áË≥áÊñôÈªûÂàÜÈ°ûÁÇ∫ÊòìÊñºÂ≠∏Áøí„ÄÅÈõ£‰ª•Â≠∏ÁøíÂíåÊ®°Á®úÂÖ©ÂèØÁ≠âÈ°ûÂà•ÔºàSwayamdipta Á≠â‰∫∫Ôºå2020 Âπ¥Ôºâ„ÄÇSwayamdipta Á≠â‰∫∫Ôºà2020 Âπ¥ÔºâÂº∑Ë™øÔºåÈõ£‰ª•Â≠∏ÁøíÁöÑÁØÑ‰æãÈÄöÂ∏∏ÂåÖÂê´ÈåØË™§ÔºåËÄåÊ®°Á®úÂÖ©ÂèØÁöÑÊÉÖÊ≥ÅÊúÉÂ∞çÊ®°ÂûãË®ìÁ∑¥Áî¢ÁîüÈáçÂ§ßÂΩ±Èüø„ÄÇÁÇ∫‰∫ÜÁ¢∫Ë™çÈÄô‰∫õÁôºÁèæÁöÑÂèØÈù†ÊÄßÔºåÊàëÂÄë‰ΩøÁî®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑË≥áÊñôÈõÜË§áË£Ω‰∫ÜÂØ¶È©óÔºåÈáçÈªûÊîæÂú®ÈÜ´Â≠∏ÂïèÈ°åËß£Á≠î‰∏ä„ÄÇÈô§‰∫ÜÊñáÂ≠óÁêÜËß£‰πãÂ§ñÔºåÈÄôÂÄãÈ†òÂüüÈÇÑÈúÄË¶ÅÁç≤ÂèñË©≥Á¥∞ÁöÑÈÜ´Â≠∏Áü•Ë≠òÔºåÈÄôÈÄ≤‰∏ÄÊ≠•‰Ωø‰ªªÂãôË§áÈõúÂåñ„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑË©ï‰º∞Ôºå‰ª•Ë©ï‰º∞Ë≥áÊñôÂú∞ÂúñÊû∂ÊßãÂú®ÈÜ´Â≠∏È†òÂüüÁöÑÂèØË°åÊÄßÂíåÂèØËΩâÁßªÊÄß„ÄÇË©ï‰º∞ÁµêÊûúË°®ÊòéÔºåË©≤Êû∂Êßã‰∏çÈÅ©ÂêàËß£Ê±∫Ë≥áÊñôÈõÜÂú®ÂõûÁ≠îÈÜ´Â≠∏ÂïèÈ°åÊôÇÈù¢Ëá®ÁöÑÁç®ÁâπÊåëÊà∞„ÄÇ

##### **Optical Flow Representation Alignment Mamba Diffusion Model for Medical Video Generation**
2411.01647v1 by Zhenbin Wang, Lei Zhang, Lituan Wang, Minjuan Zhu, Zhenwei Zhang

Medical video generation models are expected to have a profound impact on the
healthcare industry, including but not limited to medical education and
training, surgical planning, and simulation. Current video diffusion models
typically build on image diffusion architecture by incorporating temporal
operations (such as 3D convolution and temporal attention). Although this
approach is effective, its oversimplification limits spatio-temporal
performance and consumes substantial computational resources. To counter this,
we propose Medical Simulation Video Generator (MedSora), which incorporates
three key elements: i) a video diffusion framework integrates the advantages of
attention and Mamba, balancing low computational load with high-quality video
generation, ii) an optical flow representation alignment method that implicitly
enhances attention to inter-frame pixels, and iii) a video variational
autoencoder (VAE) with frequency compensation addresses the information loss of
medical features that occurs when transforming pixel space into latent features
and then back to pixel frames. Extensive experiments and applications
demonstrate that MedSora exhibits superior visual quality in generating medical
videos, outperforming the most advanced baseline methods. Further results and
code are available at https://wongzbb.github.io/MedSora

ÊëòË¶ÅÔºöÈÜ´ÁôÇÂΩ±ÁâáÁîüÊàêÊ®°ÂûãÈ†êË®àÂ∞áÂ∞çÈÜ´ÁôÇ‰øùÂÅ•Áî¢Ê•≠Áî¢ÁîüÊ∑±ÈÅ†ÁöÑÂΩ±ÈüøÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôêÊñºÈÜ´Â≠∏ÊïôËÇ≤ÂíåË®ìÁ∑¥„ÄÅÊâãË°ìË¶èÂäÉÂíåÊ®°Êì¨„ÄÇÁõÆÂâçÁöÑÂΩ±ÁâáÊì¥Êï£Ê®°ÂûãÈÄöÂ∏∏Âª∫Á´ãÂú®ÂΩ±ÂÉèÊì¥Êï£Êû∂Êßã‰∏äÔºå‰∏¶ÁµêÂêàÊôÇÈñìÈÅãÁÆóÔºà‰æãÂ¶Ç 3D Êë∫Á©çÂíåÊôÇÈñìÊ≥®ÊÑèÂäõÔºâ„ÄÇÂÑòÁÆ°Ê≠§ÊñπÊ≥ïÊúâÊïàÔºå‰ΩÜÂÖ∂ÈÅéÊñºÁ∞°ÂåñÈôêÂà∂‰∫ÜÊôÇÁ©∫ÊïàËÉΩÔºå‰∏¶Ê∂àËÄóÂ§ßÈáèÁöÑÈÅãÁÆóË≥áÊ∫ê„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ÈÜ´Â≠∏Ê®°Êì¨ÂΩ±ÁâáÁîüÊàêÂô® (MedSora)ÔºåÂÆÉÁµêÂêà‰∫Ü‰∏âÂÄãÈóúÈçµË¶ÅÁ¥†Ôºöi) ‰∏ÄÂÄãÂΩ±ÁâáÊì¥Êï£Êû∂ÊßãÊï¥Âêà‰∫ÜÊ≥®ÊÑèÂäõÂíå Mamba ÁöÑÂÑ™ÈªûÔºåÂú®‰ΩéÈÅãÁÆóË≤†ËºâÂíåÈ´òÂìÅË≥™ÂΩ±ÁâáÁîüÊàê‰πãÈñìÂèñÂæóÂπ≥Ë°°Ôºåii) ‰∏ÄÂÄãÂÖâÊµÅË°®Á§∫Â∞çÈΩäÊñπÊ≥ïÔºåÂèØ‰ª•Èö±Âê´Âú∞Â¢ûÂº∑Â∞çÂΩ±Ê†ºÈñìÂÉèÁ¥†ÁöÑÊ≥®ÊÑèÂäõÔºå‰ª•Âèä iii) ‰∏ÄÂÄãÂÖ∑ÊúâÈ†ªÁéáË£úÂÑüÁöÑÂΩ±ÁâáËÆäÁï∞Ëá™ÂãïÁ∑®Á¢ºÂô® (VAE)ÔºåÁî®ÊñºËß£Ê±∫Âú®Â∞áÂÉèÁ¥†Á©∫ÈñìËΩâÊèõÁÇ∫ÊΩõÂú®ÁâπÂæµÔºåÁÑ∂ÂæåÂÜçËΩâÂõûÂÉèÁ¥†ÂΩ±Ê†ºÊôÇÁôºÁîüÁöÑÈÜ´ÁôÇÁâπÂæµË≥áË®äÈÅ∫Â§±ÂïèÈ°å„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÂíåÊáâÁî®Ë≠âÊòéÔºåMedSora Âú®ÁîüÊàêÈÜ´ÁôÇÂΩ±ÁâáÊñπÈù¢Â±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑË¶ñË¶∫ÂìÅË≥™ÔºåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÊñπÊ≥ï„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑÁµêÊûúÂíåÁ®ãÂºèÁ¢ºÂèØ‰ª•Âú® https://wongzbb.github.io/MedSora ÂèñÂæó

##### **Customized Subgraph Selection and Encoding for Drug-drug Interaction Prediction**
2411.01535v1 by Haotong Du, Quanming Yao, Juzheng Zhang, Yang Liu, Zhen Wang

Subgraph-based methods have proven to be effective and interpretable in
predicting drug-drug interactions (DDIs), which are essential for medical
practice and drug development. Subgraph selection and encoding are critical
stages in these methods, yet customizing these components remains underexplored
due to the high cost of manual adjustments. In this study, inspired by the
success of neural architecture search (NAS), we propose a method to search for
data-specific components within subgraph-based frameworks. Specifically, we
introduce extensive subgraph selection and encoding spaces that account for the
diverse contexts of drug interactions in DDI prediction. To address the
challenge of large search spaces and high sampling costs, we design a
relaxation mechanism that uses an approximation strategy to efficiently explore
optimal subgraph configurations. This approach allows for robust exploration of
the search space. Extensive experiments demonstrate the effectiveness and
superiority of the proposed method, with the discovered subgraphs and encoding
functions highlighting the model's adaptability.

ÊëòË¶ÅÔºöÂü∫ÊñºÂ≠êÂúñÁöÑÊñπÊ≥ïÂ∑≤Ë¢´Ë≠âÊòéÂú®È†êÊ∏¨Ëó•Áâ©-Ëó•Áâ©‰∫§‰∫í‰ΩúÁî® (DDI) ‰∏≠ÊúâÊïà‰∏îÊòìÊñºËß£ÈáãÔºåÈÄôÂ∞çÊñºÈÜ´ÁôÇÂØ¶ÂãôÂíåËó•Áâ©ÈñãÁôºËá≥ÈóúÈáçË¶Å„ÄÇÂ≠êÂúñÈÅ∏ÊìáÂíåÁ∑®Á¢ºÊòØÈÄô‰∫õÊñπÊ≥ï‰∏≠ÁöÑÈóúÈçµÈöéÊÆµÔºåÁÑ∂ËÄåÔºåÁî±ÊñºÊâãÂãïË™øÊï¥ÁöÑÊàêÊú¨È´òÊòÇÔºåÂÆ¢Ë£ΩÂåñÈÄô‰∫õÂÖÉ‰ª∂‰ªçÊú™Ë¢´ÂÖÖÂàÜÊé¢Ë®é„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÂèóÂà∞Á•ûÁ∂ìÊû∂ÊßãÊêúÂ∞ã (NAS) ÊàêÂäüÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñπÊ≥ï‰æÜÊêúÂ∞ãÂ≠êÂúñÊû∂Êßã‰∏≠ÁöÑË≥áÊñôÁâπÂÆöÂÖÉ‰ª∂„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂª£Ê≥õÁöÑÂ≠êÂúñÈÅ∏ÊìáÂíåÁ∑®Á¢ºÁ©∫ÈñìÔºå‰ª•Ë™™Êòé DDI È†êÊ∏¨‰∏≠Ëó•Áâ©‰∫§‰∫í‰ΩúÁî®ÁöÑ‰∏çÂêåËÉåÊôØ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÂ§ßÂûãÊêúÂ∞ãÁ©∫ÈñìÂíåÈ´òÂèñÊ®£ÊàêÊú¨ÁöÑÊåëÊà∞ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊîæÈ¨ÜÊ©üÂà∂Ôºå‰ΩøÁî®Ëøë‰ººÁ≠ñÁï•‰æÜÊúâÊïàÊé¢Á¥¢ÊúÄ‰Ω≥Â≠êÂúñÈÖçÁΩÆ„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂÖÅË®±Â∞çÊêúÂ∞ãÁ©∫ÈñìÈÄ≤Ë°åÁ©©ÂÅ•ÁöÑÊé¢Á¥¢„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÂÑ™Ë∂äÊÄßÔºåÁôºÁèæÁöÑÂ≠êÂúñÂíåÁ∑®Á¢ºÂáΩÊï∏Á™ÅÈ°Ø‰∫ÜÊ®°ÂûãÁöÑÈÅ©ÊáâÊÄß„ÄÇ

##### **Conditional Latent Space Molecular Scaffold Optimization for Accelerated Molecular Design**
2411.01423v1 by Onur Boyar, Hiroyuki Hanada, Ichiro Takeuchi

The rapid discovery of new chemical compounds is essential for advancing
global health and developing treatments. While generative models show promise
in creating novel molecules, challenges remain in ensuring the real-world
applicability of these molecules and finding such molecules efficiently. To
address this, we introduce Conditional Latent Space Molecular Scaffold
Optimization (CLaSMO), which combines a Conditional Variational Autoencoder
(CVAE) with Latent Space Bayesian Optimization (LSBO) to modify molecules
strategically while maintaining similarity to the original input. Our LSBO
setting improves the sample-efficiency of our optimization, and our
modification approach helps us to obtain molecules with higher chances of
real-world applicability. CLaSMO explores substructures of molecules in a
sample-efficient manner by performing BO in the latent space of a CVAE
conditioned on the atomic environment of the molecule to be optimized. Our
experiments demonstrate that CLaSMO efficiently enhances target properties with
minimal substructure modifications, achieving state-of-the-art results with a
smaller model and dataset compared to existing methods. We also provide an
open-source web application that enables chemical experts to apply CLaSMO in a
Human-in-the-Loop setting.

ÊëòË¶ÅÔºöÊñ∞ÂåñÂ≠∏ÂåñÂêàÁâ©ÁöÑÂø´ÈÄüÁôºÁèæÂ∞çÊñº‰øÉÈÄ≤ÂÖ®ÁêÉÂÅ•Â∫∑ÂíåÈñãÁôºÊ≤ªÁôÇÊñπÊ≥ïËá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°ÁîüÊàêÊ®°ÂûãÂú®ÂâµÈÄ†Êñ∞ÂàÜÂ≠êÊñπÈù¢È°ØÁ§∫Âá∫ÂâçÊôØÔºå‰ΩÜ‰ªçÁÑ∂Â≠òÂú®ÊåëÊà∞Ôºå‰ª•Á¢∫‰øùÈÄô‰∫õÂàÜÂ≠êÁöÑÂØ¶ÈöõÈÅ©Áî®ÊÄß‰∏¶ÊúâÊïàÂú∞ÊâæÂà∞ÈÄô‰∫õÂàÜÂ≠ê„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊ¢ù‰ª∂ÊΩõÂú®Á©∫ÈñìÂàÜÂ≠êÊîØÊû∂ÊúÄ‰Ω≥Âåñ (CLaSMO)ÔºåÂÆÉÁµêÂêà‰∫ÜÊ¢ù‰ª∂ËÆäÁï∞Ëá™ÂãïÁ∑®Á¢ºÂô® (CVAE) ËàáÊΩõÂú®Á©∫ÈñìË≤ùÊ∞èÊúÄ‰Ω≥Âåñ (LSBO)Ôºå‰ª•Á≠ñÁï•ÊÄßÂú∞‰øÆÊîπÂàÜÂ≠êÔºåÂêåÊôÇ‰øùÊåÅËàáÂéüÂßãËº∏ÂÖ•ÁöÑÁõ∏‰ººÊÄß„ÄÇÊàëÂÄëÁöÑ LSBO Ë®≠ÂÆöÊîπÂñÑ‰∫ÜÊàëÂÄëÊúÄ‰Ω≥ÂåñÁöÑÊ®£Êú¨ÊïàÁéáÔºåÊàëÂÄëÁöÑ‰øÆÊîπÊñπÊ≥ïÂπ´Âä©ÊàëÂÄëÁç≤ÂæóÂÖ∑ÊúâÊõ¥È´òÂØ¶ÈöõÈÅ©Áî®Ê©üÊúÉÁöÑÂàÜÂ≠ê„ÄÇCLaSMO ‰ª•Ê®£Êú¨ÊúâÊïàÁöÑÊñπÂºèÊé¢Á¥¢ÂàÜÂ≠êÁöÑÂ≠êÁµêÊßãÔºåÊñπÊ≥ïÊòØÂú® CVAE ÁöÑÊΩõÂú®Á©∫Èñì‰∏≠Âü∑Ë°å BOÔºåË©≤Á©∫Èñì‰ª•Ë¶ÅÊúÄ‰Ω≥ÂåñÁöÑÂàÜÂ≠êÁöÑÂéüÂ≠êÁí∞Â¢ÉÁÇ∫Ê¢ù‰ª∂„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåCLaSMO ‰ª•ÊúÄÂ∞èÁöÑÂ≠êÁµêÊßã‰øÆÊîπÊúâÊïàÂú∞Â¢ûÂº∑‰∫ÜÁõÆÊ®ôÂ±¨ÊÄßÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºå‰ΩøÁî®ËºÉÂ∞èÁöÑÊ®°ÂûãÂíåÊï∏ÊìöÈõÜÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÈÇÑÊèê‰æõ‰∫Ü‰∏ÄÂÄãÈñãÊ∫êÁ∂≤Ë∑ØÊáâÁî®Á®ãÂºèÔºåËÆìÂåñÂ≠∏Â∞àÂÆ∂ËÉΩÂ§†Âú®‰∫∫Ê©üËø¥ÂúàË®≠ÂÆö‰∏≠ÊáâÁî® CLaSMO„ÄÇ

##### **Medical X-Ray Image Enhancement Using Global Contrast-Limited Adaptive Histogram Equalization**
2411.01373v1 by Sohrab Namazi Nia, Frank Y. Shih

In medical imaging, accurate diagnosis heavily relies on effective image
enhancement techniques, particularly for X-ray images. Existing methods often
suffer from various challenges such as sacrificing global image characteristics
over local image characteristics or vice versa. In this paper, we present a
novel approach, called G-CLAHE (Global-Contrast Limited Adaptive Histogram
Equalization), which perfectly suits medical imaging with a focus on X-rays.
This method adapts from Global Histogram Equalization (GHE) and Contrast
Limited Adaptive Histogram Equalization (CLAHE) to take both advantages and
avoid weakness to preserve local and global characteristics. Experimental
results show that it can significantly improve current state-of-the-art
algorithms to effectively address their limitations and enhance the contrast
and quality of X-ray images for diagnostic accuracy.

ÊëòË¶ÅÔºöÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÔºåÊ∫ñÁ¢∫ÁöÑË®∫Êñ∑È´òÂ∫¶‰æùË≥¥ÊñºÊúâÊïàÁöÑÂΩ±ÂÉèÂ¢ûÂº∑ÊäÄË°ìÔºåÁâπÂà•ÊòØ X ÂÖâÂΩ±ÂÉè„ÄÇÁèæÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÈÅáÂà∞ÂêÑÁ®ÆÊåëÊà∞Ôºå‰æãÂ¶ÇÁäßÁâ≤Êï¥È´îÂΩ±ÂÉèÁâπÊÄß‰ª•ÊèõÂèñÂ±ÄÈÉ®ÂΩ±ÂÉèÁâπÊÄßÔºåÂèç‰πã‰∫¶ÁÑ∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁ®±ÁÇ∫ G-CLAHEÔºàÂÖ®Â±ÄÂ∞çÊØîÂ∫¶ÈôêÂà∂Ëá™ÈÅ©ÊáâÁõ¥ÊñπÂúñÂùáË°°ÂåñÔºâÔºåÂÆÉÈùûÂ∏∏ÈÅ©ÂêàÊñº‰ª• X ÂÖâÁÇ∫ÈáçÈªûÁöÑÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÇÊ≠§ÊñπÊ≥ïÊîπÁ∑®Ëá™ÂÖ®Â±ÄÁõ¥ÊñπÂúñÂùáË°°Âåñ (GHE) ÂíåÂ∞çÊØîÂ∫¶ÈôêÂà∂Ëá™ÈÅ©ÊáâÁõ¥ÊñπÂúñÂùáË°°Âåñ (CLAHE)Ôºå‰ª•ÂèñÂæóÂÖ©ËÄÖÁöÑÂÑ™ÈªûÔºå‰∏¶ÈÅøÂÖçÂº±ÈªûÔºå‰ª•‰øùÁïôÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄÁâπÊÄß„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂÆÉÂèØ‰ª•È°ØËëóÊîπÂñÑÁï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑÊºîÁÆóÊ≥ïÔºå‰ª•ÊúâÊïàËß£Ê±∫ÂÖ∂ÈôêÂà∂Ôºå‰∏¶Â¢ûÂº∑ X ÂÖâÂΩ±ÂÉèÁöÑÂ∞çÊØîÂ∫¶ÂíåÂìÅË≥™Ôºå‰ª•Âà©ÊñºË®∫Êñ∑Ê∫ñÁ¢∫ÊÄß„ÄÇ

##### **Guided Synthesis of Labeled Brain MRI Data Using Latent Diffusion Models for Segmentation of Enlarged Ventricles**
2411.01351v1 by Tim Ruschke, Jonathan Frederik Carlsen, Adam Espe Hansen, Ulrich Lindberg, Amalie Monberg Hindsholm, Martin Norgaard, Claes N√∏hr Ladefoged

Deep learning models in medical contexts face challenges like data scarcity,
inhomogeneity, and privacy concerns. This study focuses on improving
ventricular segmentation in brain MRI images using synthetic data. We employed
two latent diffusion models (LDMs): a mask generator trained using 10,000
masks, and a corresponding SPADE image generator optimized using 6,881 scans to
create an MRI conditioned on a 3D brain mask. Conditioning the mask generator
on ventricular volume in combination with classifier-free guidance enabled the
control of the ventricular volume distribution of the generated synthetic
images. Next, the performance of the synthetic data was tested using three
nnU-Net segmentation models trained on a real, augmented and entirely synthetic
data, respectively. The resulting models were tested on a completely
independent hold-out dataset of patients with enlarged ventricles, with manual
delineation of the ventricles used as ground truth. The model trained on real
data showed a mean absolute error (MAE) of 9.09 \pm 12.18 mL in predicted
ventricular volume, while the models trained on synthetic and augmented data
showed MAEs of 7.52 \pm 4.81 mL and 6.23 \pm 4.33 mL, respectively. Both the
synthetic and augmented model also outperformed the state-of-the-art model
SynthSeg, which due to limited performance in cases of large ventricular
volumes, showed an MAE of 7.73 \pm 12.12 mL with a factor of 3 higher standard
deviation. The model trained on augmented data showed the highest Dice score of
0.892 \pm 0.05, slightly outperforming SynthSeg and on par with the model
trained on real data. The synthetic model performed similar to SynthSeg. In
summary, we provide evidence that guided synthesis of labeled brain MRI data
using LDMs improves the segmentation of enlarged ventricles and outperforms
existing state-of-the-art segmentation models.

ÊëòË¶ÅÔºö<paragraph>Âú®ÂåªÂ≠¶ËÉåÊôØ‰∏≠ÔºåÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÈù¢‰∏¥ÁùÄÊï∞ÊçÆÁ®ÄÁº∫ÊÄß„ÄÅ‰∏çÂùáÂåÄÊÄßÂíåÈöêÁßÅÈóÆÈ¢òÁ≠âÊåëÊàò„ÄÇÊú¨Á†îÁ©∂‰∏ìÊ≥®‰∫é‰ΩøÁî®ÂêàÊàêÊï∞ÊçÆÊîπËøõËÑëÈÉ® MRI ÂõæÂÉè‰∏≠ÁöÑÂøÉÂÆ§ÂàÜÂâ≤„ÄÇÊàë‰ª¨ÈááÁî®‰∫Ü‰∏§‰∏™ÊΩúÂú®Êâ©Êï£Ê®°Âûã (LDM)Ôºö‰∏Ä‰∏™‰ΩøÁî® 10,000 ‰∏™ËíôÁâàËÆ≠ÁªÉÁöÑËíôÁâàÁîüÊàêÂô®Ôºå‰ª•Âèä‰∏Ä‰∏™‰ΩøÁî® 6,881 Ê¨°Êâ´ÊèèËøõË°å‰ºòÂåñÁöÑÁõ∏Â∫î SPADE ÂõæÂÉèÁîüÊàêÂô®Ôºå‰ª•ÂàõÂª∫Âü∫‰∫é 3D ËÑëÈÉ®ËíôÁâàÁöÑ MRI„ÄÇÂØπËíôÁâàÁîüÊàêÂô®ËøõË°åÂøÉÂÆ§‰ΩìÁßØË∞ÉËäÇÔºåÂπ∂ÁªìÂêàÊó†ÂàÜÁ±ªÂô®ÊåáÂØºÔºåËÉΩÂ§üÊéßÂà∂ÁîüÊàêÂêàÊàêÂõæÂÉèÁöÑÂøÉÂÆ§‰ΩìÁßØÂàÜÂ∏É„ÄÇÊé•‰∏ãÊù•Ôºå‰ΩøÁî®ÂàÜÂà´ËÆ≠ÁªÉ‰∫éÁúüÂÆû„ÄÅÂ¢ûÂº∫ÂíåÂÆåÂÖ®ÂêàÊàêÊï∞ÊçÆ‰∏äÁöÑ‰∏â‰∏™ nnU-Net ÂàÜÂâ≤Ê®°ÂûãÊµãËØï‰∫ÜÂêàÊàêÊï∞ÊçÆÁöÑÊÄßËÉΩ„ÄÇÂ∞ÜËÆ≠ÁªÉÊâÄÂæóÁöÑÊ®°ÂûãÂú®ÂÆåÂÖ®Áã¨Á´ãÁöÑ„ÄÅÂÖ∑ÊúâÊâ©Â§ßÂøÉÂÆ§ÁöÑÊÇ£ËÄÖÁöÑ‰øùÁïôÊï∞ÊçÆÈõÜ‰∏äËøõË°åÊµãËØïÔºåÂπ∂‰ΩøÁî®ÂøÉÂÆ§ÁöÑÊâãÂä®ÊèèÁªò‰Ωú‰∏∫ÁúüÂÆûÊÉÖÂÜµ„ÄÇÂú®ÁúüÂÆûÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®È¢ÑÊµãÁöÑÂøÉÂÆ§‰ΩìÁßØ‰∏≠ÊòæÁ§∫Âá∫ 9.09 ¬± 12.18 mL ÁöÑÂπ≥ÂùáÁªùÂØπËØØÂ∑Æ (MAE)ÔºåËÄåÂú®ÂêàÊàêÂíåÂ¢ûÂº∫Êï∞ÊçÆ‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãÊòæÁ§∫Âá∫ 7.52 ¬± 4.81 mL Âíå 6.23 ¬± 4.33 mL ÁöÑ MAE„ÄÇÂêàÊàêÊ®°ÂûãÂíåÂ¢ûÂº∫Ê®°ÂûãÁöÑÊÄßËÉΩÂùá‰ºò‰∫éÊúÄÂÖàËøõÁöÑÊ®°Âûã SynthSegÔºåÂêéËÄÖÁî±‰∫éÂú®Â§ßÂøÉÂÆ§‰ΩìÁßØÁöÑÊÉÖÂÜµ‰∏ãÊÄßËÉΩÊúâÈôêÔºåÊòæÁ§∫Âá∫ 7.73 ¬± 12.12 mL ÁöÑ MAEÔºåÊ†áÂáÜÂ∑ÆÈ´òÂá∫ 3 ÂÄç„ÄÇÂú®Â¢ûÂº∫Êï∞ÊçÆ‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãÊòæÁ§∫Âá∫ÊúÄÈ´òÁöÑ Dice ÂæóÂàÜ 0.892 ¬± 0.05ÔºåÁï•‰ºò‰∫é SynthSegÔºåÂπ∂‰∏î‰∏éÂú®ÁúüÂÆûÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÁöÑÊ®°ÂûãÁõ∏ÂΩì„ÄÇÂêàÊàêÊ®°ÂûãÁöÑÊÄßËÉΩ‰∏é SynthSeg Á±ª‰ºº„ÄÇÊÄª‰πãÔºåÊàë‰ª¨Êèê‰æõ‰∫ÜËØÅÊçÆË°®ÊòéÔºå‰ΩøÁî® LDM ÂØπÊ†áËÆ∞ÁöÑËÑëÈÉ® MRI Êï∞ÊçÆËøõË°åÂºïÂØºÂêàÊàêÂèØ‰ª•ÊîπÂñÑÊâ©Â§ßÂøÉÂÆ§ÁöÑÂàÜÂâ≤ÔºåÂπ∂‰∏î‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÁöÑÂàÜÂâ≤Ê®°Âûã„ÄÇ</paragraph>

##### **Causal reasoning in difference graphs**
2411.01292v1 by Charles K. Assaad

In epidemiology, understanding causal mechanisms across different populations
is essential for designing effective public health interventions. Recently,
difference graphs have been introduced as a tool to visually represent causal
variations between two distinct populations. While there has been progress in
inferring these graphs from data through causal discovery methods, there
remains a gap in systematically leveraging their potential to enhance causal
reasoning. This paper addresses that gap by establishing conditions for
identifying causal changes and effects using difference graphs and
observational data. It specifically focuses on identifying total causal changes
and total effects in a nonparametric framework, as well as direct causal
changes and direct effects in a linear context. In doing so, it provides a
novel approach to causal reasoning that holds potential for various public
health applications.

ÊëòË¶ÅÔºöÂú®ÊµÅË°åÁóÖÂ≠∏‰∏≠Ôºå‰∫ÜËß£‰∏çÂêå‰∫∫Áæ§‰πãÈñìÁöÑÂõ†ÊûúÊ©üÂà∂Â∞çÊñºË®≠Ë®àÊúâÊïàÁöÑÂÖ¨ÂÖ±Ë°õÁîüÂπ≤È†êÊé™ÊñΩËá≥ÈóúÈáçË¶Å„ÄÇÊúÄËøëÔºåÂ∑ÆÁï∞ÂúñË°®Â∑≤Ë¢´ÂºïÂÖ•‰ΩúÁÇ∫‰∏ÄÁ®ÆÂ∑•ÂÖ∑ÔºåÁî®ÊñºÁõ¥ËßÄÂú∞Ë°®Á§∫ÂÖ©ÂÄã‰∏çÂêå‰∫∫Áæ§‰πãÈñìÁöÑÂõ†ÊûúËÆäÂåñ„ÄÇÂÑòÁÆ°ÈÄöÈÅéÂõ†ÊûúÁôºÁèæÊñπÊ≥ïÂæûÊï∏Êìö‰∏≠Êé®Êñ∑ÈÄô‰∫õÂúñË°®ÊñπÈù¢ÂèñÂæó‰∫ÜÈÄ≤Â±ïÔºå‰ΩÜÂú®Á≥ªÁµ±ÊÄßÂú∞Âà©Áî®ÂÖ∂Â¢ûÂº∑Âõ†ÊûúÊé®ÁêÜÁöÑÊΩõÂäõÊñπÈù¢‰ªçÁÑ∂Â≠òÂú®Â∑ÆË∑ù„ÄÇÊú¨ÊñáÈÄöÈÅéÂª∫Á´ã‰ΩøÁî®Â∑ÆÁï∞ÂúñË°®ÂíåËßÄÂØüÊï∏ÊìöË≠òÂà•Âõ†ÊûúËÆäÂåñÂíåÂõ†ÊûúÊïàÊáâÁöÑÊ¢ù‰ª∂‰æÜËß£Ê±∫ÈÄô‰∏ÄÂ∑ÆË∑ù„ÄÇÂÆÉÁâπÂà•ÂÅ¥ÈáçÊñºÂú®ÈùûÂèÉÊï∏Ê°ÜÊû∂‰∏≠Ë≠òÂà•Á∏ΩÂõ†ÊûúËÆäÂåñÂíåÁ∏ΩÊïàÊáâÔºå‰ª•ÂèäÂú®Á∑öÊÄßËÉåÊôØ‰∏≠Ë≠òÂà•Áõ¥Êé•Âõ†ÊûúËÆäÂåñÂíåÁõ¥Êé•ÊïàÊáâ„ÄÇÈÄôÊ®£‰∏Ä‰æÜÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂõ†ÊûúÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ïÔºåÂ∞çÂêÑÁ®ÆÂÖ¨ÂÖ±Ë°õÁîüÊáâÁî®ÂÖ∑ÊúâÊΩõÂäõ„ÄÇ

##### **Designing a Robust Radiology Report Generation System**
2411.01153v1 by Sonit Singh

Recent advances in deep learning have enabled researchers to explore tasks at
the intersection of computer vision and natural language processing, such as
image captioning, visual question answering, visual dialogue, and visual
language navigation. Taking inspiration from image captioning, the task of
radiology report generation aims at automatically generating radiology reports
by having a comprehensive understanding of medical images. However,
automatically generating radiology reports from medical images is a challenging
task due to the complexity, diversity, and nature of medical images. In this
paper, we outline the design of a robust radiology report generation system by
integrating different modules and highlighting best practices drawing upon
lessons from our past work and also from relevant studies in the literature. We
also discuss the impact of integrating different components to form a single
integrated system. We believe that these best practices, when implemented,
could improve automatic radiology report generation, augment radiologists in
decision making, and expedite diagnostic workflow, in turn improve healthcare
and save human lives.

ÊëòË¶ÅÔºöÊúÄËøëÊ∑±Â∫¶Â≠∏ÁøíÁöÑÈÄ≤Â±ï‰ΩøÁ†îÁ©∂‰∫∫Âì°ËÉΩÂ§†Êé¢Á¥¢ÈõªËÖ¶Ë¶ñË¶∫ÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∫§ÈõÜ‰∏≠ÁöÑ‰ªªÂãôÔºå‰æãÂ¶ÇÂΩ±ÂÉèÊ®ôÈ°å„ÄÅË¶ñË¶∫ÂïèÁ≠î„ÄÅË¶ñË¶∫Â∞çË©±ÂíåË¶ñË¶∫Ë™ûË®ÄÂ∞éËà™„ÄÇÂèóÂΩ±ÂÉèÊ®ôÈ°åÁöÑÂïüÁôºÔºåÊîæÂ∞ÑÁßëÂ†±ÂëäÁîüÊàêÁöÑ‰ªªÂãôÊó®Âú®ÈÄèÈÅéÂÖ®Èù¢‰∫ÜËß£ÈÜ´Â≠∏ÂΩ±ÂÉèËá™ÂãïÁîüÊàêÊîæÂ∞ÑÁßëÂ†±Âëä„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑË§áÈõúÊÄß„ÄÅÂ§öÊ®£ÊÄßÂíåÊÄßË≥™ÔºåËá™ÂãïÂæûÈÜ´Â≠∏ÂΩ±ÂÉèÁîüÊàêÊîæÂ∞ÑÁßëÂ†±ÂëäÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÊï¥Âêà‰∏çÂêåÁöÑÊ®°ÁµÑ‰∏¶Âº∑Ë™øÊúÄ‰Ω≥ÂØ¶ÂãôÔºåÊ¶ÇËø∞‰∫ÜÂÅ•ÂÖ®ÁöÑÊîæÂ∞ÑÁßëÂ†±ÂëäÁîüÊàêÁ≥ªÁµ±ÁöÑË®≠Ë®àÔºåÈÄô‰∫õÂØ¶ÂãôÊ±≤ÂèñËá™ÊàëÂÄëÈÅéÂéªÁöÑÂ∑•‰Ωú‰ª•ÂèäÊñáÁçª‰∏≠ÁöÑÁõ∏ÈóúÁ†îÁ©∂„ÄÇÊàëÂÄë‰πüË®éË´ñ‰∫ÜÊï¥Âêà‰∏çÂêåÁµÑ‰ª∂‰ª•ÂΩ¢ÊàêÂñÆ‰∏ÄÊï¥ÂêàÁ≥ªÁµ±ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁõ∏‰ø°ÔºåÈÄô‰∫õÊúÄ‰Ω≥ÂØ¶ÂãôÂú®ÂØ¶ÊñΩÂæåÔºåÂèØ‰ª•ÊîπÂñÑËá™ÂãïÊîæÂ∞ÑÁßëÂ†±ÂëäÁîüÊàêÔºåÂ¢ûÂº∑ÊîæÂ∞ÑÁßëÈÜ´Â∏´Âú®Ê±∫Á≠ñÂà∂ÂÆö‰∏≠ÁöÑËÉΩÂäõÔºå‰∏¶Âä†Âø´Ë®∫Êñ∑Â∑•‰ΩúÊµÅÁ®ãÔºåÈÄ≤ËÄåÊîπÂñÑÈÜ´ÁôÇ‰øùÂÅ•‰∏¶ÊãØÊïë‰∫∫ÂëΩ„ÄÇ

##### **LEARNER: Learning Granular Labels from Coarse Labels using Contrastive Learning**
2411.01144v1 by Gautam Gare, Jana Armouti, Nikhil Madaan, Rohan Panda, Tom Fox, Laura Hutchins, Amita Krishnan, Ricardo Rodriguez, Bennett DeBoisblanc, Deva Ramanan, John Galeotti

A crucial question in active patient care is determining if a treatment is
having the desired effect, especially when changes are subtle over short
periods. We propose using inter-patient data to train models that can learn to
detect these fine-grained changes within a single patient. Specifically, can a
model trained on multi-patient scans predict subtle changes in an individual
patient's scans? Recent years have seen increasing use of deep learning (DL) in
predicting diseases using biomedical imaging, such as predicting COVID-19
severity using lung ultrasound (LUS) data. While extensive literature exists on
successful applications of DL systems when well-annotated large-scale datasets
are available, it is quite difficult to collect a large corpus of personalized
datasets for an individual. In this work, we investigate the ability of recent
computer vision models to learn fine-grained differences while being trained on
data showing larger differences. We evaluate on an in-house LUS dataset and a
public ADNI brain MRI dataset. We find that models pre-trained on clips from
multiple patients can better predict fine-grained differences in scans from a
single patient by employing contrastive learning.

ÊëòË¶ÅÔºöÂú®‰∏ªÂãïÊÇ£ËÄÖÁÖßË≠∑‰∏≠Ôºå‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÊòØÁ¢∫ÂÆöÊ≤ªÁôÇÊòØÂê¶Áî¢ÁîüÈ†êÊúüÁöÑÊïàÊûúÔºåÁâπÂà•ÊòØÂú®Áü≠ÊôÇÈñìÂÖßËÆäÂåñÁ¥∞ÂæÆÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÊàëÂÄëÊèêË≠∞‰ΩøÁî®ÊÇ£ËÄÖÈñìÊï∏Êìö‰æÜË®ìÁ∑¥Ê®°ÂûãÔºå‰ª•‰æøÂ≠∏ÁøíÂÅµÊ∏¨ÂñÆ‰∏ÄÊÇ£ËÄÖÂÖßÈÄô‰∫õÁ¥∞ÂæÆÁöÑËÆäÂåñ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂú®Â§ö‰ΩçÊÇ£ËÄÖÊéÉÊèè‰∏≠Ë®ìÁ∑¥ÁöÑÊ®°ÂûãÊòØÂê¶ÂèØ‰ª•È†êÊ∏¨ÂÄãÂà•ÊÇ£ËÄÖÊéÉÊèè‰∏≠ÁöÑÁ¥∞ÂæÆËÆäÂåñÔºüËøëÂπ¥‰æÜÔºåÊ∑±Â∫¶Â≠∏Áøí (DL) Âú®‰ΩøÁî®ÁîüÁâ©ÈÜ´Â≠∏ÂΩ±ÂÉèÈ†êÊ∏¨ÁñæÁóÖÊñπÈù¢ÊáâÁî®Êó•ÁõäÂª£Ê≥õÔºå‰æãÂ¶Ç‰ΩøÁî®ËÇ∫ÈÉ®Ë∂ÖÈü≥Ê≥¢ (LUS) Êï∏ÊìöÈ†êÊ∏¨ COVID-19 ÁöÑÂö¥ÈáçÁ®ãÂ∫¶„ÄÇÂÑòÁÆ°ÊúâÂ§ßÈáèÊñáÁçªË®òËºâ‰∫ÜÂú®ÊúâÊ®ôË®ªÁöÑÂ§ßË¶èÊ®°Êï∏ÊìöÈõÜÂèØÁî®ÊôÇ DL Á≥ªÁµ±ÁöÑÊàêÂäüÊáâÁî®Ôºå‰ΩÜË¶ÅÁÇ∫ÂÄã‰∫∫Êî∂ÈõÜÂ§ßÈáèÂÄã‰∫∫ÂåñÊï∏ÊìöÈõÜÁõ∏Áï∂Âõ∞Èõ£„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜËøëÊúüÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÂú®ÈáùÂ∞çÈ°ØÁ§∫ËºÉÂ§ßÂ∑ÆÁï∞ÁöÑÊï∏ÊìöÈÄ≤Ë°åË®ìÁ∑¥ÊôÇÔºåÂ≠∏ÁøíÁ¥∞ÂæÆÂ∑ÆÁï∞ÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂú®ÂÖßÈÉ® LUS Êï∏ÊìöÈõÜÂíåÂÖ¨ÈñãÁöÑ ADNI Â§ßËÖ¶ MRI Êï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëÁôºÁèæÔºåÈÄèÈÅé‰ΩøÁî®Â∞çÊØîÂ≠∏ÁøíÔºåÂú®Â§ö‰ΩçÊÇ£ËÄÖÁöÑÁâáÊÆµ‰∏äÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÂèØ‰ª•Êõ¥Â•ΩÂú∞È†êÊ∏¨ÂñÆ‰∏ÄÊÇ£ËÄÖÊéÉÊèè‰∏≠ÁöÑÁ¥∞ÂæÆÂ∑ÆÁï∞„ÄÇ

##### **Artificial Intelligence for Microbiology and Microbiome Research**
2411.01098v1 by Xu-Wen Wang, Tong Wang, Yang-Yu Liu

Advancements in artificial intelligence (AI) have transformed many scientific
fields, with microbiology and microbiome research now experiencing significant
breakthroughs through machine learning and deep learning applications. This
review provides a comprehensive overview of AI-driven approaches tailored for
microbiology and microbiome studies, emphasizing both technical advancements
and biological insights. We begin with an introduction to foundational AI
techniques, including primary machine learning paradigms and various deep
learning architectures, and offer guidance on choosing between machine learning
and deep learning methods based on specific research goals. The primary section
on application scenarios spans diverse research areas, from taxonomic
profiling, functional annotation & prediction, microbe-X interactions,
microbial ecology, metabolic modeling, precision nutrition, clinical
microbiology, to prevention & therapeutics. Finally, we discuss challenges
unique to this field, including the balance between interpretability and
complexity, the "small n, large p" problem, and the critical need for
standardized benchmarking datasets to validate and compare models. Together,
this review underscores AI's transformative role in microbiology and microbiome
research, paving the way for innovative methodologies and applications that
enhance our understanding of microbial life and its impact on our planet and
our health.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÈÄ≤Ê≠•Â∑≤ËΩâËÆäË®±Â§öÁßëÂ≠∏È†òÂüüÔºåËÄåÂæÆÁîüÁâ©Â≠∏ÂíåÂæÆÁîüÁâ©ÁµÑÁ†îÁ©∂ÁèæÂú®Ê≠£ÈÄèÈÅéÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊáâÁî®È´îÈ©óÂà∞È°ØËëóÁöÑÁ™ÅÁ†¥„ÄÇÊú¨ÁØáË©ïË´ñÊèê‰æõ AI È©ÖÂãïÊñπÊ≥ïÁöÑÂÖ®Èù¢Ê¶ÇËø∞ÔºåÈÄô‰∫õÊñπÊ≥ïÂ∞àÁÇ∫ÂæÆÁîüÁâ©Â≠∏ÂíåÂæÆÁîüÁâ©ÁµÑÁ†îÁ©∂ÈáèË∫´ÊâìÈÄ†ÔºåÂº∑Ë™øÊäÄË°ìÈÄ≤Ê≠•ÂíåÁîüÁâ©Ë¶ãËß£„ÄÇÊàëÂÄëÂæûÂü∫Á§é AI ÊäÄË°ìÁöÑ‰ªãÁ¥πÈñãÂßãÔºåÂåÖÊã¨‰∏ªË¶ÅÁöÑÊ©üÂô®Â≠∏ÁøíÁØÑ‰æãÂíåÂêÑÁ®ÆÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÔºå‰∏¶Êèê‰æõÊ†πÊìöÂÖ∑È´îÁ†îÁ©∂ÁõÆÊ®ôÂú®Ê©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ï‰πãÈñìÈÄ≤Ë°åÈÅ∏ÊìáÁöÑÊåáÂ∞é„ÄÇÊáâÁî®Â†¥ÊôØÁöÑ‰∏ªË¶ÅÈÉ®ÂàÜÊ∂µËìã‰∫ÜÂæûÂàÜÈ°ûÂàÜÊûê„ÄÅÂäüËÉΩË®ªËß£ÂíåÈ†êÊ∏¨„ÄÅÂæÆÁîüÁâ© X Áõ∏‰∫í‰ΩúÁî®„ÄÅÂæÆÁîüÁâ©ÁîüÊÖã„ÄÅ‰ª£Ë¨ùÂª∫Ê®°„ÄÅÁ≤æÊ∫ñÁáüÈ§ä„ÄÅËá®Â∫äÂæÆÁîüÁâ©Â≠∏Âà∞È†êÈò≤ÂíåÊ≤ªÁôÇÁ≠âÂ§öÂÄãÁ†îÁ©∂È†òÂüü„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜË©≤È†òÂüüÁç®ÊúâÁöÑÊåëÊà∞ÔºåÂåÖÊã¨ÂèØËß£ÈáãÊÄßÂíåË§áÈõúÊÄß‰πãÈñìÁöÑÂπ≥Ë°°„ÄÅ„ÄåÂ∞è nÔºåÂ§ß p„ÄçÂïèÈ°åÔºå‰ª•ÂèäÈ©óË≠âÂíåÊØîËºÉÊ®°ÂûãÁöÑÊ®ôÊ∫ñÂåñÂü∫Ê∫ñÊï∏ÊìöÈõÜÁöÑÈóúÈçµÈúÄÊ±Ç„ÄÇÊú¨ÁØáË©ïË´ñÂÖ±ÂêåÂº∑Ë™ø‰∫Ü AI Âú®ÂæÆÁîüÁâ©Â≠∏ÂíåÂæÆÁîüÁâ©ÁµÑÁ†îÁ©∂‰∏≠ÁöÑËΩâÂûã‰ΩúÁî®ÔºåÁÇ∫ÂâµÊñ∞ÊñπÊ≥ïÂíåÊáâÁî®Èã™Âπ≥ÈÅìË∑ØÔºåÈÄô‰∫õÊñπÊ≥ïÂíåÊáâÁî®Â¢ûÂº∑‰∫ÜÊàëÂÄëÂ∞çÂæÆÁîüÁâ©ÁîüÂëΩÂèäÂÖ∂Â∞çÊàëÂÄëÊòüÁêÉÂíåÊàëÂÄëÂÅ•Â∫∑ÁöÑÂΩ±ÈüøÁöÑÁêÜËß£„ÄÇ

##### **Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities**
2411.01053v1 by Adriel Saporta, Aahlad Puli, Mark Goldstein, Rajesh Ranganath

Contrastive learning methods, such as CLIP, leverage naturally paired
data-for example, images and their corresponding text captions-to learn general
representations that transfer efficiently to downstream tasks. While such
approaches are generally applied to two modalities, domains such as robotics,
healthcare, and video need to support many types of data at once. We show that
the pairwise application of CLIP fails to capture joint information between
modalities, thereby limiting the quality of the learned representations. To
address this issue, we present Symile, a simple contrastive learning approach
that captures higher-order information between any number of modalities. Symile
provides a flexible, architecture-agnostic objective for learning
modality-specific representations. To develop Symile's objective, we derive a
lower bound on total correlation, and show that Symile representations for any
set of modalities form a sufficient statistic for predicting the remaining
modalities. Symile outperforms pairwise CLIP, even with modalities missing in
the data, on cross-modal classification and retrieval across several
experiments including on an original multilingual dataset of 33M image, text
and audio samples and a clinical dataset of chest X-rays, electrocardiograms,
and laboratory measurements. All datasets and code used in this work are
publicly available at https://github.com/rajesh-lab/symile.

ÊëòË¶ÅÔºöÂ∞çÊØîÂ≠∏ÁøíÊñπÊ≥ïÔºå‰æãÂ¶Ç CLIPÔºåÂà©Áî®Ëá™ÁÑ∂ÈÖçÂ∞çÁöÑË≥áÊñôÔºå‰æãÂ¶ÇÂΩ±ÂÉèÂèäÂÖ∂Â∞çÊáâÁöÑÊñáÂ≠óÊ®ôÈ°åÔºå‰æÜÂ≠∏Áøí‰∏ÄËà¨ÂåñË°®ÂæµÔºå‰∏¶ÊúâÊïàÁéáÂú∞ËΩâÁßªÂà∞‰∏ãÊ∏∏‰ªªÂãô„ÄÇÈõñÁÑ∂Ê≠§È°ûÊñπÊ≥ïÈÄöÂ∏∏ÊáâÁî®ÊñºÂÖ©Á®ÆÂΩ¢ÂºèÔºå‰ΩÜÊ©üÂô®‰∫∫ÊäÄË°ì„ÄÅÈÜ´ÁôÇ‰øùÂÅ•ÂíåË¶ñË®äÁ≠âÈ†òÂüüÈúÄË¶Å‰∏ÄÊ¨°ÊîØÊè¥Â§öÁ®ÆÈ°ûÂûãÁöÑË≥áÊñô„ÄÇÊàëÂÄëÈ°ØÁ§∫ÔºåCLIP ÁöÑÊàêÂ∞çÊáâÁî®ÁÑ°Ê≥ïÊì∑ÂèñÂΩ¢ÂºèÈñìÁöÑËÅØÂêàË≥áË®äÔºåÂõ†Ê≠§ÈôêÂà∂‰∫ÜÂ≠∏ÁøíË°®ÂæµÁöÑÂìÅË≥™„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ SymileÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁ∞°ÂñÆÁöÑÂ∞çÊØîÂ≠∏ÁøíÊñπÊ≥ïÔºåÂèØ‰ª•Êì∑Âèñ‰ªªÊÑèÊï∏ÈáèÁöÑÂΩ¢Âºè‰πãÈñìÁöÑÈ´òÈöéË≥áË®ä„ÄÇSymile Êèê‰æõ‰∫Ü‰∏ÄÂÄãÈùàÊ¥ª‰∏îËàáÊû∂ÊßãÁÑ°ÈóúÁöÑÁõÆÊ®ôÔºåÁî®ÊñºÂ≠∏ÁøíÁâπÂÆöÊñºÂΩ¢ÂºèÁöÑË°®Âæµ„ÄÇÁÇ∫ÈñãÁôº Symile ÁöÑÁõÆÊ®ôÔºåÊàëÂÄëÊé®Â∞éÂá∫Á∏ΩÁõ∏ÈóúÊÄßÁöÑ‰∏ãÁïåÔºå‰∏¶È°ØÁ§∫‰ªª‰ΩïÂΩ¢ÂºèÈõÜÂêàÁöÑ Symile Ë°®ÂæµÂΩ¢Êàê‰∏ÄÂÄãÂÖÖÂàÜÁöÑÁµ±Ë®àÈáèÔºåÁî®ÊñºÈ†êÊ∏¨ÂÖ∂È§òÂΩ¢Âºè„ÄÇSymile ÂÑ™ÊñºÊàêÂ∞ç CLIPÔºåÂç≥‰ΩøË≥áÊñô‰∏≠Áº∫Â∞ëÂΩ¢ÂºèÔºå‰πüËÉΩÂú®Ë∑®ÂΩ¢ÂºèÂàÜÈ°ûÂíåÊ™¢Á¥¢‰∏≠Ë°®ÁèæÂá∫Ëâ≤ÔºåÂåÖÊã¨Âú®‰∏ÄÂÄãÂåÖÂê´ 3300 Ëê¨ÂºµÂΩ±ÂÉè„ÄÅÊñáÂ≠óÂíåÈü≥Ë®äÊ®£Êú¨ÁöÑÂéüÂßãÂ§öË™ûË®ÄË≥áÊñôÈõÜÂíå‰∏ÄÂÄãÂåÖÂê´ËÉ∏ÈÉ® X ÂÖâ„ÄÅÂøÉÈõªÂúñÂíåÂØ¶È©óÂÆ§Ê∏¨ÈáèÁöÑËá®Â∫äË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂ§öÊ¨°ÂØ¶È©ó„ÄÇÊú¨Á†îÁ©∂‰∏≠‰ΩøÁî®ÊâÄÊúâË≥áÊñôÈõÜÂíåÁ®ãÂºèÁ¢ºÁöÜÂÖ¨ÈñãÊñº https://github.com/rajesh-lab/symile„ÄÇ

##### **Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract**
2411.00726v1 by Fan Xiao, Junlin Hou, Ruiwei Zhao, Rui Feng, Haidong Zou, Lina Lu, Yi Xu, Juzhao Zhang

Diabetic retinopathy (DR) is a leading cause of blindness worldwide and a
common complication of diabetes. As two different imaging tools for DR grading,
color fundus photography (CFP) and infrared fundus photography (IFP) are
highly-correlated and complementary in clinical applications. To the best of
our knowledge, this is the first study that explores a novel multi-modal deep
learning framework to fuse the information from CFP and IFP towards more
accurate DR grading. Specifically, we construct a dual-stream architecture
Cross-Fundus Transformer (CFT) to fuse the ViT-based features of two fundus
image modalities. In particular, a meticulously engineered Cross-Fundus
Attention (CFA) module is introduced to capture the correspondence between CFP
and IFP images. Moreover, we adopt both the single-modality and multi-modality
supervisions to maximize the overall performance for DR grading. Extensive
experiments on a clinical dataset consisting of 1,713 pairs of multi-modal
fundus images demonstrate the superiority of our proposed method. Our code will
be released for public access.

ÊëòË¶ÅÔºöÁ≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆä (DR) ÊòØÂÖ®ÁêÉÂ§±ÊòéÁöÑ‰∏ªË¶ÅÂéüÂõ†Ôºå‰πüÊòØÁ≥ñÂ∞øÁóÖÁöÑÂ∏∏Ë¶ã‰ΩµÁôºÁóá„ÄÇ‰ΩúÁÇ∫ DR ÂàÜÁ¥öÁöÑÂÖ©Á®Æ‰∏çÂêåÁöÑÂΩ±ÂÉèÂ∑•ÂÖ∑ÔºåÂΩ©Ëâ≤ÁúºÂ∫ïÊîùÂΩ± (CFP) ÂíåÁ¥ÖÂ§ñÁ∑öÁúºÂ∫ïÊîùÂΩ± (IFP) Âú®Ëá®Â∫äÊáâÁî®‰∏≠È´òÂ∫¶Áõ∏Èóú‰∏î‰∫íË£ú„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÊé¢Ë®éÂâµÊñ∞ÁöÑÂ§öÊ®°ÂºèÊ∑±Â∫¶Â≠∏ÁøíÊ°ÜÊû∂Ôºå‰ª•ËûçÂêà CFP Âíå IFP ÁöÑË≥áË®äÔºå‰ª•ÈÄ≤Ë°åÊõ¥Ê∫ñÁ¢∫ÁöÑ DR ÂàÜÁ¥ö„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊßãÂª∫‰∫Ü‰∏ÄÂÄãÈõôÊµÅÊû∂Êßã Cross-Fundus Transformer (CFT)Ôºå‰ª•ËûçÂêàÂÖ©Á®ÆÁúºÂ∫ïÂΩ±ÂÉèÊ®°ÂºèÁöÑÂü∫Êñº ViT ÁöÑÁâπÂæµ„ÄÇÁâπÂà•ÊòØÔºåÂºïÂÖ•‰∫ÜÁ≤æÂøÉË®≠Ë®àÁöÑ Cross-Fundus Attention (CFA) Ê®°ÁµÑÔºå‰ª•ÊçïÊçâ CFP Âíå IFP ÂΩ±ÂÉè‰πãÈñìÁöÑÂ∞çÊáâÈóú‰øÇ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé°Áî®ÂñÆ‰∏ÄÊ®°ÂºèÂíåÂ§öÊ®°ÂºèÁõ£Áù£Ôºå‰ª•ÊúÄÂ§ßÂåñ DR ÂàÜÁ¥öÁöÑÊï¥È´îÊïàËÉΩ„ÄÇÂú®Áî± 1,713 Â∞çÂ§öÊ®°ÂºèÁúºÂ∫ïÂΩ±ÂÉèÁµÑÊàêÁöÑËá®Â∫äË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÂÑ™Ë∂äÊÄß„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∞áÊúÉÂÖ¨ÈñãÁôºÂ∏É„ÄÇ

##### **CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced Multimodal Electronic Health Records Analysis**
2411.00696v1 by Fuying Wang, Feng Wu, Yihan Tang, Lequan Yu

Integrating multimodal Electronic Health Records (EHR) data, such as
numerical time series and free-text clinical reports, has great potential in
predicting clinical outcomes. However, prior work has primarily focused on
capturing temporal interactions within individual samples and fusing multimodal
information, overlooking critical temporal patterns across patients. These
patterns, such as trends in vital signs like abnormal heart rate or blood
pressure, can indicate deteriorating health or an impending critical event.
Similarly, clinical notes often contain textual descriptions that reflect these
patterns. Identifying corresponding temporal patterns across different
modalities is crucial for improving the accuracy of clinical outcome
predictions, yet it remains a challenging task. To address this gap, we
introduce a Cross-Modal Temporal Pattern Discovery (CTPD) framework, designed
to efficiently extract meaningful cross-modal temporal patterns from multimodal
EHR data. Our approach introduces shared initial temporal pattern
representations which are refined using slot attention to generate temporal
semantic embeddings. To ensure rich cross-modal temporal semantics in the
learned patterns, we introduce a contrastive-based TPNCE loss for cross-modal
alignment, along with two reconstruction losses to retain core information of
each modality. Evaluations on two clinically critical tasks, 48-hour
in-hospital mortality and 24-hour phenotype classification, using the MIMIC-III
database demonstrate the superiority of our method over existing approaches.

ÊëòË¶ÅÔºöÊï¥ÂêàÂ§öÊ®°ÊÄÅÁîµÂ≠êÂÅ•Â∫∑ËÆ∞ÂΩï (EHR) Êï∞ÊçÆÔºà‰æãÂ¶ÇÊï∞ÂÄºÊó∂Èó¥Â∫èÂàóÂíåËá™Áî±ÊñáÊú¨‰∏¥Â∫äÊä•ÂëäÔºâÂú®È¢ÑÊµã‰∏¥Â∫äÁªìÊûúÊñπÈù¢ÂÖ∑ÊúâÂ∑®Â§ßÊΩúÂäõ„ÄÇÁÑ∂ËÄåÔºå‰ª•ÂâçÁöÑÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÊçïÊçâÂçï‰∏™Ê†∑Êú¨‰∏≠ÁöÑÊó∂Èó¥‰∫§‰∫íÂπ∂ËûçÂêàÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÔºåËÄåÂøΩÁï•‰∫ÜÊÇ£ËÄÖ‰πãÈó¥ÁöÑÂÖ≥ÈîÆÊó∂Èó¥Ê®°Âºè„ÄÇËøô‰∫õÊ®°ÂºèÔºà‰æãÂ¶ÇÁîüÂëΩ‰ΩìÂæÅË∂ãÂäøÔºåÂ¶ÇÂºÇÂ∏∏ÂøÉÁéáÊàñË°ÄÂéãÔºâÂèØËÉΩË°®ÊòéÂÅ•Â∫∑Áä∂ÂÜµÊÅ∂ÂåñÊàñÂç≥Â∞ÜÂèëÁîüÁöÑÂç±Èáç‰∫ã‰ª∂„ÄÇÁ±ª‰ººÂú∞Ôºå‰∏¥Â∫äÁ¨îËÆ∞ÈÄöÂ∏∏ÂåÖÂê´ÂèçÊò†Ëøô‰∫õÊ®°ÂºèÁöÑÊñáÊú¨ÊèèËø∞„ÄÇËØÜÂà´‰∏çÂêåÊ®°ÊÄÅ‰πãÈó¥Áõ∏Â∫îÁöÑÊó∂Èó¥Ê®°ÂºèÂØπ‰∫éÊèêÈ´ò‰∏¥Â∫äÁªìÊûúÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÂÆÉ‰ªçÁÑ∂ÊòØ‰∏ÄÈ°πÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ªªÂä°„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÂ∑ÆË∑ùÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Ë∑®Ê®°ÊÄÅÊó∂Èó¥Ê®°ÂºèÂèëÁé∞ (CTPD) Ê°ÜÊû∂ÔºåÊó®Âú®‰ªéÂ§öÊ®°ÊÄÅ EHR Êï∞ÊçÆ‰∏≠ÊúâÊïàÊèêÂèñÊúâÊÑè‰πâÁöÑË∑®Ê®°ÊÄÅÊó∂Èó¥Ê®°Âºè„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂºïÂÖ•‰∫ÜÂÖ±‰∫´ÁöÑÂàùÂßãÊó∂Èó¥Ê®°ÂºèË°®Á§∫ÔºåËøô‰∫õË°®Á§∫‰ΩøÁî®ÊèíÊßΩÊ≥®ÊÑèÂäõËøõË°å‰ºòÂåñ‰ª•ÁîüÊàêÊó∂Èó¥ËØ≠‰πâÂµåÂÖ•„ÄÇ‰∏∫‰∫ÜÁ°Æ‰øùÂ≠¶‰π†Ê®°Âºè‰∏≠‰∏∞ÂØåÁöÑË∑®Ê®°ÊÄÅÊó∂Èó¥ËØ≠‰πâÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂü∫‰∫éÂØπÊØîÁöÑ TPNCE ÊçüÂ§±Áî®‰∫éË∑®Ê®°ÊÄÅÂØπÈΩêÔºå‰ª•Âèä‰∏§‰∏™ÈáçÂª∫ÊçüÂ§±‰ª•‰øùÁïôÊØè‰∏™Ê®°ÊÄÅÁöÑÊ†∏ÂøÉ‰ø°ÊÅØ„ÄÇÂú®‰∏§‰∏™‰∏¥Â∫äÂÖ≥ÈîÆ‰ªªÂä°Ôºà48 Â∞èÊó∂Èô¢ÂÜÖÊ≠ª‰∫°ÁéáÂíå 24 Â∞èÊó∂Ë°®ÂûãÂàÜÁ±ªÔºâ‰∏äÁöÑËØÑ‰º∞Ôºå‰ΩøÁî® MIMIC-III Êï∞ÊçÆÂ∫ìËØÅÊòé‰∫ÜÊàë‰ª¨ÊñπÊ≥ï‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ

##### **Enhancing Osteoporosis Detection: An Explainable Multi-Modal Learning Framework with Feature Fusion and Variable Clustering**
2411.00916v1 by Mehdi Hosseini Chagahi, Saeed Mohammadi Dashtaki, Niloufar Delfan, Nadia Mohammadi, Alireza Samari, Behzad Moshiri, Md. Jalil Piran, U. Rajendra Acharya, Oliver Faust

Osteoporosis is a common condition that increases fracture risk, especially
in older adults. Early diagnosis is vital for preventing fractures, reducing
treatment costs, and preserving mobility. However, healthcare providers face
challenges like limited labeled data and difficulties in processing medical
images. This study presents a novel multi-modal learning framework that
integrates clinical and imaging data to improve diagnostic accuracy and model
interpretability. The model utilizes three pre-trained networks-VGG19,
InceptionV3, and ResNet50-to extract deep features from X-ray images. These
features are transformed using PCA to reduce dimensionality and focus on the
most relevant components. A clustering-based selection process identifies the
most representative components, which are then combined with preprocessed
clinical data and processed through a fully connected network (FCN) for final
classification. A feature importance plot highlights key variables, showing
that Medical History, BMI, and Height were the main contributors, emphasizing
the significance of patient-specific data. While imaging features were
valuable, they had lower importance, indicating that clinical data are crucial
for accurate predictions. This framework promotes precise and interpretable
predictions, enhancing transparency and building trust in AI-driven diagnoses
for clinical integration.

ÊëòË¶ÅÔºöÈ™®Ë≥™ÁñèÈ¨ÜÁóáÊòØ‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÁñæÁóÖÔºåÊúÉÂ¢ûÂä†È™®ÊäòÈ¢®Èö™ÔºåÁâπÂà•ÊòØËÄÅÂπ¥‰∫∫„ÄÇÊó©ÊúüË®∫Êñ∑Â∞çÊñºÈ†êÈò≤È™®Êäò„ÄÅÈôç‰ΩéÊ≤ªÁôÇÊàêÊú¨ÂíåÁ∂≠ÊåÅË°åÂãïÂäõËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖÈù¢Ëá®ÊåëÊà∞Ôºå‰æãÂ¶ÇÊ®ôË®òÊï∏ÊìöÊúâÈôêÂíåËôïÁêÜÈÜ´Â≠∏ÂΩ±ÂÉèÂõ∞Èõ£„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ§öÊ®°ÂºèÂ≠∏ÁøíÊ°ÜÊû∂ÔºåÂÆÉÊï¥Âêà‰∫ÜËá®Â∫äÂíåÂΩ±ÂÉèÊï∏ÊìöÔºå‰ª•ÊèêÈ´òË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÂíåÊ®°ÂûãÂèØËß£ÈáãÊÄß„ÄÇË©≤Ê®°ÂûãÂà©Áî®‰∏âÂÄãÈ†êË®ìÁ∑¥Á∂≤Ë∑Ø - VGG19„ÄÅInceptionV3 Âíå ResNet50 - Âæû X ÂÖâÂΩ±ÂÉè‰∏≠ÊèêÂèñÊ∑±Â∫¶ÁâπÂæµ„ÄÇÈÄô‰∫õÁâπÂæµ‰ΩøÁî® PCA ËΩâÊèõÔºå‰ª•Èôç‰ΩéÁ∂≠Â∫¶‰∏¶Â∞àÊ≥®ÊñºÊúÄÁõ∏ÈóúÁöÑÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂü∫ÊñºÁæ§ÈõÜÁöÑÈÅ∏ÊìáÈÅéÁ®ãË≠òÂà•ÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑÁµÑÊàêÈÉ®ÂàÜÔºåÁÑ∂ÂæåÂ∞áÂÖ∂ËàáÈ†êËôïÁêÜÁöÑËá®Â∫äÊï∏ÊìöÁµêÂêàÔºå‰∏¶ÈÄèÈÅéÂÖ®ÈÄ£Êé•Á∂≤Ë∑Ø (FCN) ËôïÁêÜ‰ª•ÈÄ≤Ë°åÊúÄÁµÇÂàÜÈ°û„ÄÇÁâπÂæµÈáçË¶ÅÊÄßÂúñÁ™ÅÈ°Ø‰∫ÜÈóúÈçµËÆäÊï∏ÔºåÈ°ØÁ§∫ÁóÖÂè≤„ÄÅBMI ÂíåË∫´È´òÊòØ‰∏ªË¶ÅË≤¢ÁçªËÄÖÔºåÂº∑Ë™ø‰∫ÜÊÇ£ËÄÖÁâπÂÆöÊï∏ÊìöÁöÑÈáçË¶ÅÊÄß„ÄÇÈõñÁÑ∂ÂΩ±ÂÉèÁâπÂæµÂæàÊúâÂÉπÂÄºÔºå‰ΩÜÂÆÉÂÄëÁöÑÈáçË¶ÅÊÄßËºÉ‰ΩéÔºåÈÄôË°®ÊòéËá®Â∫äÊï∏ÊìöÂ∞çÊñºÊ∫ñÁ¢∫È†êÊ∏¨Ëá≥ÈóúÈáçË¶Å„ÄÇÈÄôÂÄãÊ°ÜÊû∂‰øÉËøõ‰∫ÜÊ∫ñÁ¢∫‰∏îÂèØËß£ÈáãÁöÑÈ†êÊ∏¨ÔºåÊèêÈ´ò‰∫ÜÈÄèÊòéÂ∫¶Ôºå‰∏¶Âª∫Á´ã‰∫ÜÂ∞ç AI È©ÖÂãïË®∫Êñ∑ÁöÑ‰ø°‰ªªÔºå‰ª•ÈÄ≤Ë°åËá®Â∫äÊï¥Âêà„ÄÇ

##### **Deep learning-based auto-contouring of organs/structures-at-risk for pediatric upper abdominal radiotherapy**
2411.00594v1 by Mianyong Ding, Matteo Maspero, Annemieke S Littooij, Martine van Grotel, Raquel Davila Fajardo, Max M van Noesel, Marry M van den Heuvel-Eibrink, Geert O Janssens

Purposes: This study aimed to develop a computed tomography (CT)-based
multi-organ segmentation model for delineating organs-at-risk (OARs) in
pediatric upper abdominal tumors and evaluate its robustness across multiple
datasets. Materials and methods: In-house postoperative CTs from pediatric
patients with renal tumors and neuroblastoma (n=189) and a public dataset
(n=189) with CTs covering thoracoabdominal regions were used. Seventeen OARs
were delineated: nine by clinicians (Type 1) and eight using TotalSegmentator
(Type 2). Auto-segmentation models were trained using in-house (ModelPMC-UMCU)
and a combined dataset of public data (Model-Combined). Performance was
assessed with Dice Similarity Coefficient (DSC), 95% Hausdorff Distance (HD95),
and mean surface distance (MSD). Two clinicians rated clinical acceptability on
a 5-point Likert scale across 15 patient contours. Model robustness was
evaluated against sex, age, intravenous contrast, and tumor type. Results:
Model-PMC-UMCU achieved mean DSC values above 0.95 for five of nine OARs, while
spleen and heart ranged between 0.90 and 0.95. The stomach-bowel and pancreas
exhibited DSC values below 0.90. Model-Combined demonstrated improved
robustness across both datasets. Clinical evaluation revealed good usability,
with both clinicians rating six of nine Type 1 OARs above four and six of eight
Type 2 OARs above three. Significant performance 2 differences were only found
across age groups in both datasets, specifically in the left lung and pancreas.
The 0-2 age group showed the lowest performance. Conclusion: A multi-organ
segmentation model was developed, showcasing enhanced robustness when trained
on combined datasets. This model is suitable for various OARs and can be
applied to multiple datasets in clinical settings.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÁöÑÔºöÊú¨Á†îÁ©∂Êó®Âú®ÂºÄÂèë‰∏Ä‰∏™Âü∫‰∫éËÆ°ÁÆóÊú∫Êñ≠Â±ÇÊâ´Êèè (CT) ÁöÑÂ§öÂô®ÂÆòÂàÜÂâ≤Ê®°ÂûãÔºåÁî®‰∫éÊèèÁªòÂ∞èÂÑø‰∏äËÖπÈÉ®ËÇøÁò§‰∏≠ÁöÑÂç±Èô©Âô®ÂÆò (OAR)ÔºåÂπ∂ËØÑ‰º∞ÂÖ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏≠ÁöÑÁ®≥ÂÅ•ÊÄß„ÄÇÊùêÊñôÂíåÊñπÊ≥ïÔºö‰ΩøÁî®Â∞èÂÑøËÇæËÇøÁò§ÂíåÁ•ûÁªèÊØçÁªÜËÉûÁò§ÊÇ£ËÄÖ (n=189) ÁöÑÈô¢ÂÜÖÊúØÂêé CT ‰ª•ÂèäÂåÖÂê´ËÉ∏ËÖπÂå∫Âüü CT ÁöÑÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ (n=189)„ÄÇÊèèÁªò‰∫Ü 17 ‰∏™ OARÔºö9 ‰∏™Áî±‰∏¥Â∫äÂåªÁîüÊèèÁªò (Á±ªÂûã 1)Ôºå8 ‰∏™‰ΩøÁî® TotalSegmentator ÊèèÁªò (Á±ªÂûã 2)„ÄÇ‰ΩøÁî®Èô¢ÂÜÖ (ModelPMC-UMCU) ÂíåÂÖ¨ÂÖ±Êï∞ÊçÆÁªÑÂêàÊï∞ÊçÆÈõÜ (Model-Combined) ËÆ≠ÁªÉËá™Âä®ÂàÜÂâ≤Ê®°Âûã„ÄÇ‰ΩøÁî®È™∞Â≠êÁõ∏‰ººÊÄßÁ≥ªÊï∞ (DSC)„ÄÅ95% ÈúçÊñØÂ§öÂ§´Ë∑ùÁ¶ª (HD95) ÂíåÂπ≥ÂùáË°®Èù¢Ë∑ùÁ¶ª (MSD) ËØÑ‰º∞ÊÄßËÉΩ„ÄÇ‰∏§‰Ωç‰∏¥Â∫äÂåªÁîü‰ΩøÁî® 5 ÁÇπÊùéÂÖãÁâπÈáèË°®ÂØπ 15 ‰∏™ÊÇ£ËÄÖËΩÆÂªìÁöÑ‰∏¥Â∫äÂèØÊé•ÂèóÊÄßËøõË°åËØÑÁ∫ß„ÄÇÈíàÂØπÊÄßÂà´„ÄÅÂπ¥ÈæÑ„ÄÅÈùôËÑâÂØπÊØîÂíåËÇøÁò§Á±ªÂûãËØÑ‰º∞Ê®°ÂûãÁöÑÁ®≥ÂÅ•ÊÄß„ÄÇÁªìÊûúÔºöModel-PMC-UMCU ÂØπ‰πù‰∏™ OAR ‰∏≠ÁöÑ‰∫î‰∏™ OAR ÁöÑÂπ≥Âùá DSC ÂÄºËææÂà∞ 0.95 ‰ª•‰∏äÔºåËÄåËÑæËÑèÂíåÂøÉËÑèÂú® 0.90 Âà∞ 0.95 ‰πãÈó¥„ÄÇËÉÉËÇ†ÂíåËÉ∞ËÖ∫ÁöÑ DSC ÂÄº‰Ωé‰∫é 0.90„ÄÇModel-Combined Âú®‰∏§‰∏™Êï∞ÊçÆÈõÜ‰∏äÈÉΩË°®Áé∞Âá∫ÊîπËøõÁöÑÁ®≥ÂÅ•ÊÄß„ÄÇ‰∏¥Â∫äËØÑ‰º∞ÊòæÁ§∫Âá∫ËâØÂ•ΩÁöÑÂèØÁî®ÊÄßÔºå‰∏§‰Ωç‰∏¥Â∫äÂåªÁîüÂØπÂÖ≠‰∏™‰πù‰∏™Á±ªÂûã 1 OAR ÁöÑËØÑÂàÜÂùáÈ´ò‰∫éÂõõÂàÜÔºåÂØπÂÖ´‰∏™Á±ªÂûã 2 OAR ‰∏≠ÁöÑÂÖ≠‰∏™ËØÑÂàÜÂùáÈ´ò‰∫é‰∏âÂàÜ„ÄÇ‰ªÖÂú®‰∏§‰∏™Êï∞ÊçÆÈõÜÁöÑÂπ¥ÈæÑÁªÑ‰∏≠ÂèëÁé∞‰∫ÜÊòæÁùÄÁöÑÊÄßËÉΩ 2 Â∑ÆÂºÇÔºåÁâπÂà´ÊòØÂú®Â∑¶ËÇ∫ÂíåËÉ∞ËÖ∫‰∏≠„ÄÇ0-2 Â≤ÅÂπ¥ÈæÑÁªÑË°®Áé∞ÊúÄÂ∑Æ„ÄÇÁªìËÆ∫ÔºöÂºÄÂèë‰∫Ü‰∏Ä‰∏™Â§öÂô®ÂÆòÂàÜÂâ≤Ê®°ÂûãÔºåÂú®ÂêàÂπ∂Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÊó∂ÊòæÁ§∫Âá∫Â¢ûÂº∫ÁöÑÁ®≥ÂÅ•ÊÄß„ÄÇËØ•Ê®°ÂûãÈÄÇÁî®‰∫éÂêÑÁßç OARÔºåÂπ∂‰∏îÂèØ‰ª•Âú®‰∏¥Â∫äÁéØÂ¢É‰∏≠Â∫îÁî®‰∫éÂ§ö‰∏™Êï∞ÊçÆÈõÜ„ÄÇ</paragraph>

##### **Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback**
2411.00897v1 by Song Yu, Xiaofei Xu, Fangfei Xu, Li Li

Although large language models perform well in understanding and responding
to user intent, their performance in specialized domains such as Traditional
Chinese Medicine (TCM) remains limited due to lack of expertise. In addition,
high-quality data related to TCM is scarce and difficult to obtain, making
large language models ineffective in handling TCM tasks. In this work, we
propose a framework to improve the performance of large language models for TCM
tasks using only a small amount of data. First, we use medical case data for
supervised fine-tuning of the large model, making it initially capable of
performing TCM tasks. Subsequently, we further optimize the model's performance
using reinforcement learning from AI feedback (RLAIF) to align it with the
preference data. The ablation study also demonstrated the performance gain is
attributed to both supervised fine-tuning and the direct policy optimization.
The experimental results show that the model trained with a small amount of
data achieves a significant performance improvement on a representative TCM
task.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ÁêÜËß£ÂíåÂõûÊáâ‰ΩøÁî®ËÄÖÊÑèÂúñÊñπÈù¢Ë°®ÁèæËâØÂ•ΩÔºå‰ΩÜÁî±ÊñºÁº∫‰πèÂ∞àÊ•≠Áü•Ë≠òÔºåÂÆÉÂÄëÂú®ÂÇ≥Áµ±‰∏≠ÈÜ´ (TCM) Á≠âÂ∞àÊ•≠È†òÂüüÁöÑË°®Áèæ‰ªçÁÑ∂ÊúâÈôê„ÄÇÊ≠§Â§ñÔºåËàá‰∏≠ÈÜ´Áõ∏ÈóúÁöÑÈ´òÂìÅË≥™Ë≥áÊñôÁ®ÄÂ∞ë‰∏îÈõ£‰ª•ÂèñÂæóÔºåÈÄô‰ΩøÂæóÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ËôïÁêÜ‰∏≠ÈÜ´‰ªªÂãôÊôÇÊïàÊûú‰∏çÂΩ∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊû∂ÊßãÔºå‰ΩøÁî®Â∞ëÈáèË≥áÊñô‰æÜÊîπÂñÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®‰∏≠ÈÜ´‰ªªÂãô‰∏≠ÁöÑË°®Áèæ„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰ΩøÁî®ÈÜ´ÁôÇÊ°à‰æãË≥áÊñôÂ∞çÂ§ßÂûãÊ®°ÂûãÈÄ≤Ë°åÁõ£Áù£ÂæÆË™øÔºå‰ΩøÂÖ∂ÊúÄÂàùÂÖ∑ÂÇôÂü∑Ë°å‰∏≠ÈÜ´‰ªªÂãôÁöÑËÉΩÂäõ„ÄÇÈö®ÂæåÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®‰∫∫Â∑•Êô∫ÊÖßÂõûÈ•ãÁöÑÂº∑ÂåñÂ≠∏Áøí (RLAIF) ‰æÜÊúÄ‰Ω≥ÂåñÊ®°ÂûãÁöÑË°®ÁèæÔºå‰ΩøÂÖ∂ËàáÂÅèÂ•ΩË≥áÊñô‰øùÊåÅ‰∏ÄËá¥„ÄÇÊ∂àËûçÁ†îÁ©∂‰πüË≠âÊòéÔºåË°®ÁèæÊèêÂçáÊ≠∏ÂäüÊñºÁõ£Áù£ÂæÆË™øÂíåÁõ¥Êé•Á≠ñÁï•ÊúÄ‰Ω≥Âåñ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫Ôºå‰ΩøÁî®Â∞ëÈáèË≥áÊñôË®ìÁ∑¥ÁöÑÊ®°ÂûãÂú®‰ª£Ë°®ÊÄßÁöÑ‰∏≠ÈÜ´‰ªªÂãô‰∏äÂèñÂæóÈ°ØËëóÁöÑË°®ÁèæÊèêÂçá„ÄÇ

##### **StepCountJITAI: simulation environment for RL with application to physical activity adaptive intervention**
2411.00336v1 by Karine Karine, Benjamin M. Marlin

The use of reinforcement learning (RL) to learn policies for just-in-time
adaptive interventions (JITAIs) is of significant interest in many behavioral
intervention domains including improving levels of physical activity. In a
messaging-based physical activity JITAI, a mobile health app is typically used
to send messages to a participant to encourage engagement in physical activity.
In this setting, RL methods can be used to learn what intervention options to
provide to a participant in different contexts. However, deploying RL methods
in real physical activity adaptive interventions comes with challenges: the
cost and time constraints of real intervention studies result in limited data
to learn adaptive intervention policies. Further, commonly used RL simulation
environments have dynamics that are of limited relevance to physical activity
adaptive interventions and thus shed little light on what RL methods may be
optimal for this challenging application domain. In this paper, we introduce
StepCountJITAI, an RL environment designed to foster research on RL methods
that address the significant challenges of policy learning for adaptive
behavioral interventions.

ÊëòË¶ÅÔºöÂà©Áî®Âº∑ÂåñÂ≠∏Áøí (RL) ‰æÜÂ≠∏ÁøíÂç≥ÊôÇÈÅ©ÊáâÊÄß‰ªãÂÖ• (JITAI) ÁöÑÁ≠ñÁï•ÔºåÂú®Ë®±Â§öË°åÁÇ∫‰ªãÂÖ•È†òÂüü‰∏≠ÂÇôÂèóÈóúÊ≥®ÔºåÂåÖÊã¨ÊèêÂçáÈ´îËÉΩÊ¥ªÂãïÁöÑÂ±§Á¥ö„ÄÇÂú®Âü∫ÊñºË®äÊÅØÁöÑÈ´îËÉΩÊ¥ªÂãï JITAI ‰∏≠ÔºåË°åÂãïÂÅ•Â∫∑ÊáâÁî®Á®ãÂºèÈÄöÂ∏∏Áî®ÊñºÂêëÂèÉËàáËÄÖÂÇ≥ÈÄÅË®äÊÅØÔºå‰ª•ÈºìÂãµÂèÉËàáÈ´îËÉΩÊ¥ªÂãï„ÄÇÂú®Ê≠§Ë®≠ÂÆö‰∏≠ÔºåRL ÊñπÊ≥ïÂèØË¢´Áî®ÊñºÂ≠∏ÁøíÂú®‰∏çÂêåÊÉÖÂ¢É‰∏ãÊèê‰æõÁµ¶ÂèÉËàáËÄÖÁöÑ‰ªãÂÖ•ÈÅ∏È†Ö„ÄÇÁÑ∂ËÄåÔºåÂú®ÂØ¶ÈöõÈ´îËÉΩÊ¥ªÂãïÈÅ©ÊáâÊÄß‰ªãÂÖ•‰∏≠ÈÉ®ÁΩ≤ RL ÊñπÊ≥ïÊúÉÈÅáÂà∞ÊåëÊà∞ÔºöÂØ¶Èöõ‰ªãÂÖ•Á†îÁ©∂ÁöÑÊàêÊú¨ÂíåÊôÇÈñìÈôêÂà∂ÔºåÂ∞éËá¥ÂèØ‰æõÂ≠∏ÁøíÈÅ©ÊáâÊÄß‰ªãÂÖ•Á≠ñÁï•ÁöÑË≥áÊñôÊúâÈôê„ÄÇÊ≠§Â§ñÔºåÂ∏∏Áî®ÁöÑ RL Ê®°Êì¨Áí∞Â¢ÉÂÖ∑ÊúâËàáÈ´îËÉΩÊ¥ªÂãïÈÅ©ÊáâÊÄß‰ªãÂÖ•Áõ∏ÈóúÊÄßÊúâÈôêÁöÑÂãïÊÖãÔºåÂõ†Ê≠§Èõ£‰ª•‰∫ÜËß£Âì™‰∫õ RL ÊñπÊ≥ïÂèØËÉΩÊúÄÈÅ©ÂêàÈÄôÂÄãÂÖ∑ÊåëÊà∞ÊÄßÁöÑÊáâÁî®È†òÂüü„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π StepCountJITAIÔºåÈÄôÊòØ‰∏ÄÂÄã RL Áí∞Â¢ÉÔºåÊó®Âú®‰øÉÈÄ≤Â∞ç RL ÊñπÊ≥ïÁöÑÁ†îÁ©∂Ôºå‰ª•ÊáâÂ∞çÈÅ©ÊáâÊÄßË°åÁÇ∫‰ªãÂÖ•Á≠ñÁï•Â≠∏ÁøíÁöÑÈáçÂ§ßÊåëÊà∞„ÄÇ

##### **Strongly Topology-preserving GNNs for Brain Graph Super-resolution**
2411.02525v1 by Pragya Singh, Islem Rekik

Brain graph super-resolution (SR) is an under-explored yet highly relevant
task in network neuroscience. It circumvents the need for costly and
time-consuming medical imaging data collection, preparation, and processing.
Current SR methods leverage graph neural networks (GNNs) thanks to their
ability to natively handle graph-structured datasets. However, most GNNs
perform node feature learning, which presents two significant limitations: (1)
they require computationally expensive methods to learn complex node features
capable of inferring connectivity strength or edge features, which do not scale
to larger graphs; and (2) computations in the node space fail to adequately
capture higher-order brain topologies such as cliques and hubs. However,
numerous studies have shown that brain graph topology is crucial in identifying
the onset and presence of various neurodegenerative disorders like Alzheimer
and Parkinson. Motivated by these challenges and applications, we propose our
STP-GSR framework. It is the first graph SR architecture to perform
representation learning in higher-order topological space. Specifically, using
the primal-dual graph formulation from graph theory, we develop an efficient
mapping from the edge space of our low-resolution (LR) brain graphs to the node
space of a high-resolution (HR) dual graph. This approach ensures that
node-level computations on this dual graph correspond naturally to edge-level
learning on our HR brain graphs, thereby enforcing strong topological
consistency within our framework. Additionally, our framework is GNN layer
agnostic and can easily learn from smaller, scalable GNNs, reducing
computational requirements. We comprehensively benchmark our framework across
seven key topological measures and observe that it significantly outperforms
the previous state-of-the-art methods and baselines.

ÊëòË¶ÅÔºöËÖ¶ÂúñÂÉèË∂ÖËß£ÊûêÂ∫¶ (SR) ÊòØÁ∂≤Ë∑ØÁ•ûÁ∂ìÁßëÂ≠∏‰∏≠‰∏ÄÂÄãÂ∞öÊú™ÂÖÖÂàÜÊé¢Á¥¢‰ΩÜÈ´òÂ∫¶Áõ∏ÈóúÁöÑ‰ªªÂãô„ÄÇÂÆÉÈÅøÈñã‰∫Ü‰ª£ÂÉπÈ´òÊòÇ‰∏îËÄóÊôÇÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÊî∂ÈõÜ„ÄÅÊ∫ñÂÇôÂíåËôïÁêÜÁöÑÈúÄË¶Å„ÄÇÁõÆÂâçÁöÑ SR ÊñπÊ≥ïÂà©Áî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN)ÔºåÂõ†ÁÇ∫ÂÆÉÂÄëËÉΩÂ§†ÂéüÁîüËôïÁêÜÂúñÂΩ¢ÁµêÊßãÁöÑË≥áÊñôÈõÜ„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏ GNN ÈÉΩÂü∑Ë°åÁØÄÈªûÁâπÂæµÂ≠∏ÁøíÔºåÈÄôÊèêÂá∫‰∫ÜÂÖ©ÂÄãÈáçÂ§ßÁöÑÈôêÂà∂Ôºö(1) ÂÆÉÂÄëÈúÄË¶Å‰ª•Ë®àÁÆóÊàêÊú¨È´òÁöÑÊñπÂºè‰æÜÂ≠∏ÁøíË§áÈõúÁöÑÁØÄÈªûÁâπÂæµÔºåÈÄô‰∫õÁâπÂæµËÉΩÂ§†Êé®Ë´ñÈÄ£Êé•Âº∑Â∫¶ÊàñÈÇäÁ∑£ÁâπÂæµÔºåÈÄôÁÑ°Ê≥ïÊì¥Â±ïÂà∞Êõ¥Â§ßÁöÑÂúñÂΩ¢Ôºõ(2) ÁØÄÈªûÁ©∫Èñì‰∏≠ÁöÑË®àÁÆóÁÑ°Ê≥ïÂÖÖÂàÜÊì∑ÂèñÈ´òÈöéËÖ¶ÈÉ®ÊãìÊí≤Ôºå‰æãÂ¶ÇÊ¥æÁ≥ªÂíåÊ®ûÁ¥ê„ÄÇÁÑ∂ËÄåÔºåË®±Â§öÁ†îÁ©∂Ë°®ÊòéÔºåËÖ¶ÂúñÂΩ¢ÊãìÊí≤Â∞çÊñºË≠òÂà•ÂêÑÁ®ÆÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖÔºàÂ¶ÇÈòøËå≤Êµ∑ÈªòÁóáÂíåÂ∏ïÈáëÊ£ÆÊ∞èÁóáÔºâÁöÑÁôºÁóÖÂíåÂ≠òÂú®Ëá≥ÈóúÈáçË¶Å„ÄÇÂèóÂà∞ÈÄô‰∫õÊåëÊà∞ÂíåÊáâÁî®ÊøÄÂãµÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÊàëÂÄëÁöÑ STP-GSR Êû∂Êßã„ÄÇÂÆÉÊòØÁ¨¨‰∏ÄÂÄãÂú®È´òÈöéÊãìÊí≤Á©∫Èñì‰∏≠Âü∑Ë°åË°®Á§∫Â≠∏ÁøíÁöÑÂúñÂΩ¢ SR Êû∂Êßã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ΩøÁî®ÂúñË´ñ‰∏≠ÁöÑÂéüÂßãÂ∞çÂÅ∂ÂúñÂΩ¢ÂÖ¨ÂºèÔºåÂæûÊàëÂÄë‰ΩéËß£ÊûêÂ∫¶ (LR) ËÖ¶ÂúñÂΩ¢ÁöÑÈÇäÁ∑£Á©∫ÈñìÈñãÁôº‰∫Ü‰∏ÄÂÄãÈ´òÊïàÁöÑÂ∞çÊò†ÔºåÂ∞çÊò†Âà∞È´òËß£ÊûêÂ∫¶ (HR) Â∞çÂÅ∂ÂúñÂΩ¢ÁØÄÈªûÁ©∫Èñì„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÁ¢∫‰øù‰∫ÜÂú®ÈÄôÂÄãÂ∞çÂÅ∂ÂúñÂΩ¢‰∏äÁöÑÁØÄÈªûÂ±§Á¥öË®àÁÆóËá™ÁÑ∂Âú∞Â∞çÊáâÊñºÊàëÂÄë HR ËÖ¶ÂúñÂΩ¢‰∏äÁöÑÈÇäÁ∑£Â±§Á¥öÂ≠∏ÁøíÔºåÂæûËÄåÂº∑Âà∂Âü∑Ë°åÊàëÂÄëÊ°ÜÊû∂ÂÖßÂº∑Â§ßÁöÑÊãìÊí≤‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂Ëàá GNN Â±§ÁÑ°ÈóúÔºå‰∏¶‰∏îÂèØ‰ª•ËºïÈ¨ÜÂú∞ÂæûÊõ¥Â∞è„ÄÅÂèØÊì¥Â±ïÁöÑ GNN ‰∏≠Â≠∏ÁøíÔºåÂæûËÄåÊ∏õÂ∞ëË®àÁÆóÈúÄÊ±Ç„ÄÇÊàëÂÄëÂú®‰∏ÉÈ†ÖÈóúÈçµÊãìÊí≤Ê∏¨Èáè‰∏≠ÂÖ®Èù¢Ë©ïÂÆö‰∫ÜÊàëÂÄëÁöÑÊ°ÜÊû∂Ôºå‰∏¶ËßÄÂØüÂà∞ÂÆÉÈ°ØËëóÂÑ™Êñº‰ª•ÂæÄÁöÑÂÖàÈÄ≤ÊñπÊ≥ïÂíåÂü∫Á∑ö„ÄÇ

##### **Evaluating the Impact of Lab Test Results on Large Language Models Generated Differential Diagnoses from Clinical Case Vignettes**
2411.02523v1 by Balu Bhasuran, Qiao Jin, Yuzhang Xie, Carl Yang, Karim Hanna, Jennifer Costa, Cindy Shavor, Zhiyong Lu, Zhe He

Differential diagnosis is crucial for medicine as it helps healthcare
providers systematically distinguish between conditions that share similar
symptoms. This study assesses the impact of lab test results on differential
diagnoses (DDx) made by large language models (LLMs). Clinical vignettes from
50 case reports from PubMed Central were created incorporating patient
demographics, symptoms, and lab results. Five LLMs GPT-4, GPT-3.5, Llama-2-70b,
Claude-2, and Mixtral-8x7B were tested to generate Top 10, Top 5, and Top 1 DDx
with and without lab data. A comprehensive evaluation involving GPT-4, a
knowledge graph, and clinicians was conducted. GPT-4 performed best, achieving
55% accuracy for Top 1 diagnoses and 60% for Top 10 with lab data, with lenient
accuracy up to 80%. Lab results significantly improved accuracy, with GPT-4 and
Mixtral excelling, though exact match rates were low. Lab tests, including
liver function, metabolic/toxicology panels, and serology/immune tests, were
generally interpreted correctly by LLMs for differential diagnosis.

ÊëòË¶ÅÔºöÈëëÂà•Ë®∫Êñ∑Â∞çÊñºÈÜ´Â≠∏Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÊúâÂä©ÊñºÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖÁ≥ªÁµ±ÂçÄÂàÜÂÖ∑ÊúâÁõ∏‰ººÁóáÁãÄÁöÑÁñæÁóÖ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Ë©ï‰º∞‰∫ÜÂØ¶È©óÂÆ§Ê™¢È©óÁµêÊûúÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂÅöÂá∫ÁöÑÈëëÂà•Ë®∫Êñ∑ (DDx) ÁöÑÂΩ±Èüø„ÄÇÂæû PubMed Central ÁöÑ 50 ‰ªΩÁóÖ‰æãÂ†±Âëä‰∏≠Âª∫Á´ã‰∫ÜËá®Â∫äÁ∞°Â†±ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÊÇ£ËÄÖ‰∫∫Âè£Áµ±Ë®à„ÄÅÁóáÁãÄÂíåÂØ¶È©óÂÆ§ÁµêÊûú„ÄÇÊ∏¨Ë©¶‰∫Ü‰∫îÂÄã LLM GPT-4„ÄÅGPT-3.5„ÄÅLlama-2-70b„ÄÅClaude-2 Âíå Mixtral-8x7BÔºå‰ª•ÁîüÊàêÂ∏∂Âíå‰∏çÂ∏∂ÂØ¶È©óÂÆ§Êï∏ÊìöÁöÑÂâç 10„ÄÅÂâç 5 ÂíåÂâç 1 DDx„ÄÇÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÊ∂âÂèä GPT-4„ÄÅÁü•Ë≠òÂúñË≠úÂíåËá®Â∫äÈÜ´ÁîüÁöÑÁ∂úÂêàË©ï‰º∞„ÄÇGPT-4 Ë°®ÁèæÊúÄ‰Ω≥ÔºåÂú®ÊúâÂØ¶È©óÂÆ§Êï∏ÊìöÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂâç 1 ÂêçË®∫Êñ∑ÁöÑÊ∫ñÁ¢∫ÁéáÈÅîÂà∞ 55%ÔºåÂâç 10 ÂêçÁöÑÊ∫ñÁ¢∫ÁéáÈÅîÂà∞ 60%ÔºåÂØ¨È¨ÜÊ∫ñÁ¢∫ÁéáÈ´òÈÅî 80%„ÄÇÂØ¶È©óÂÆ§ÁµêÊûúÈ°ØËëóÊèêÈ´ò‰∫ÜÊ∫ñÁ¢∫ÁéáÔºåGPT-4 Âíå Mixtral Ë°®ÁèæÂá∫Ëâ≤ÔºåÂÑòÁÆ°ÂÆåÂÖ®ÂåπÈÖçÁéáËºÉ‰Ωé„ÄÇLLM ÈÄöÂ∏∏ÂèØ‰ª•Ê≠£Á¢∫Ëß£ÈáãÂåÖÊã¨ËÇùÂäüËÉΩ„ÄÅ‰ª£Ë¨ù/ÊØíÁêÜÂ≠∏Ê™¢Êü•ÂíåË°ÄÊ∏ÖÂ≠∏/ÂÖçÁñ´Ê∏¨Ë©¶Âú®ÂÖßÁöÑÂØ¶È©óÂÆ§Ê™¢È©óÔºå‰ª•ÈÄ≤Ë°åÈëëÂà•Ë®∫Êñ∑„ÄÇ

##### **Deep Learning Predicts Mammographic Breast Density in Clinical Breast Ultrasound Images**
2411.00891v1 by Arianna Bunnell, Thomas Wolfgruber, Brandon Quon, Kailee Hung, Brenda Hernandez, Peter Sadowski, John A. Shepherd

Background: Mammographic breast density, as defined by the American College
of Radiology's Breast Imaging Reporting and Data System (BI-RADS), is one of
the strongest risk factors for breast cancer, but is derived from mammographic
images. Breast ultrasound (BUS) is an alternative breast cancer screening
modality, particularly useful for early detection in low-resource, rural
contexts. The purpose of this study was to explore an artificial intelligence
(AI) model to predict BI-RADS mammographic breast density category from
clinical, handheld BUS imaging. Methods: All data are sourced from the Hawaii
and Pacific Islands Mammography Registry. We compared deep learning methods
from BUS imaging, as well as machine learning models from image statistics
alone. The use of AI-derived BUS density as a risk factor for breast cancer was
then compared to clinical BI-RADS breast density while adjusting for age. The
BUS data were split by individual into 70/20/10% groups for training,
validation, and testing. Results: 405,120 clinical BUS images from 14.066 women
were selected for inclusion in this study, resulting in 9.846 women for
training (302,574 images), 2,813 for validation (11,223 images), and 1,406 for
testing (4,042 images). On the held-out testing set, the strongest AI model
achieves AUROC 0.854 predicting BI-RADS mammographic breast density from BUS
imaging and outperforms all shallow machine learning methods based on image
statistics. In cancer risk prediction, age-adjusted AI BUS breast density
predicted 5-year breast cancer risk with 0.633 AUROC, as compared to 0.637
AUROC from age-adjusted clinical breast density. Conclusions: BI-RADS
mammographic breast density can be estimated from BUS imaging with high
accuracy using a deep learning model. Furthermore, we demonstrate that
AI-derived BUS breast density is predictive of 5-year breast cancer risk in our
population.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöÁî±ÁæéÂúãÊîæÂ∞ÑÂ≠∏Èô¢ÁöÑ‰π≥ÊàøÂΩ±ÂÉèÂ†±ÂëäÂíåË≥áÊñôÁ≥ªÁµ± (BI-RADS) ÊâÄÂÆöÁæ©ÁöÑ‰π≥ÊàøÊîùÂΩ±‰π≥ÊàøÂØÜÂ∫¶ÔºåÊòØ‰π≥ÁôåÊúÄÂº∑ÁöÑÈ¢®Èö™Âõ†Â≠ê‰πã‰∏ÄÔºå‰ΩÜÂçªÊòØ‰æÜËá™‰π≥ÊàøÊîùÂΩ±ÂúñÂÉè„ÄÇ‰π≥ÊàøË∂ÖÈü≥Ê≥¢ (BUS) ÊòØÂè¶‰∏ÄÁ®Æ‰π≥ÁôåÁØ©Ê™¢ÊñπÂºèÔºåÁâπÂà•ÈÅ©Áî®ÊñºË≥áÊ∫êÂå±‰πèÁöÑÈÑâÊùëÂú∞ÂçÄÁöÑÊó©ÊúüÂÅµÊ∏¨„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÁöÑÊòØÊé¢Á¥¢‰∏ÄÁ®Æ‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÔºåÂæûËá®Â∫äÊâãÊåÅÂºè BUS ÂΩ±ÂÉèÈ†êÊ∏¨ BI-RADS ‰π≥ÊàøÊîùÂΩ±‰π≥ÊàøÂØÜÂ∫¶È°ûÂà•„ÄÇÊñπÊ≥ïÔºöÊâÄÊúâË≥áÊñôÈÉΩ‰æÜËá™Â§èÂ®ÅÂ§∑ÂíåÂ§™Âπ≥Ê¥ãÁæ§Â≥∂‰π≥ÊàøÊîùÂΩ±ÁôªË®òËôï„ÄÇÊàëÂÄëÊØîËºÉ‰∫Ü‰æÜËá™ BUS ÂΩ±ÂÉèÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºå‰ª•ÂèäÂÉÖ‰æÜËá™ÂΩ±ÂÉèÁµ±Ë®àË≥áÊñôÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°Âûã„ÄÇÊé•ËëóÂ∞á AI Ë°çÁîüÁöÑ BUS ÂØÜÂ∫¶Áî®‰Ωú‰π≥ÁôåÈ¢®Èö™Âõ†Â≠êÔºåËàáË™øÊï¥Âπ¥ÈΩ°ÂæåÁöÑËá®Â∫ä BI-RADS ‰π≥ÊàøÂØÜÂ∫¶ÈÄ≤Ë°åÊØîËºÉ„ÄÇBUS Ë≥áÊñôÊåâÂÄã‰∫∫ÂàÜÁÇ∫ 70/20/10% ÁöÑÁæ§ÁµÑÔºåÁî®ÊñºË®ìÁ∑¥„ÄÅÈ©óË≠âÂíåÊ∏¨Ë©¶„ÄÇÁµêÊûúÔºöÊú¨Á†îÁ©∂ÈÅ∏Âèñ‰∫Ü‰æÜËá™ 14.066 ‰ΩçÂ•≥ÊÄßÁöÑ 405,120 ÂºµËá®Â∫ä BUS ÂΩ±ÂÉèÔºåÂæóÂá∫ 9.846 ‰ΩçÂ•≥ÊÄßÁî®ÊñºË®ìÁ∑¥ (302,574 ÂºµÂΩ±ÂÉè)„ÄÅ2,813 ‰ΩçÁî®ÊñºÈ©óË≠â (11,223 ÂºµÂΩ±ÂÉè) Âíå 1,406 ‰ΩçÁî®ÊñºÊ∏¨Ë©¶ (4,042 ÂºµÂΩ±ÂÉè)„ÄÇÂú®‰øùÁïôÁöÑÊ∏¨Ë©¶ÈõÜ‰∏≠ÔºåÊúÄÂº∑Â§ßÁöÑ AI Ê®°ÂûãÂú®Âæû BUS ÂΩ±ÂÉèÈ†êÊ∏¨ BI-RADS ‰π≥ÊàøÊîùÂΩ±‰π≥ÊàøÂØÜÂ∫¶ÊôÇÔºåÈÅîÂà∞ 0.854 ÁöÑ AUROCÔºå‰∏¶ÂÑ™ÊñºÊâÄÊúâÂü∫ÊñºÂΩ±ÂÉèÁµ±Ë®àË≥áÊñôÁöÑÊ∑∫Â±§Ê©üÂô®Â≠∏ÁøíÊñπÊ≥ï„ÄÇÂú®ÁôåÁóáÈ¢®Èö™È†êÊ∏¨‰∏≠ÔºåË™øÊï¥Âπ¥ÈΩ°ÂæåÁöÑ AI BUS ‰π≥ÊàøÂØÜÂ∫¶È†êÊ∏¨‰∫Ü 5 Âπ¥‰π≥ÁôåÈ¢®Èö™ÔºåAUROC ÁÇ∫ 0.633ÔºåËÄåË™øÊï¥Âπ¥ÈΩ°ÂæåÁöÑËá®Â∫ä‰π≥ÊàøÂØÜÂ∫¶ÁöÑ AUROC ÁÇ∫ 0.637„ÄÇÁµêË´ñÔºö‰ΩøÁî®Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºåÂèØ‰ª•Âæû BUS ÂΩ±ÂÉè‰º∞Ë®àÂá∫ BI-RADS ‰π≥ÊàøÊîùÂΩ±‰π≥ÊàøÂØÜÂ∫¶Ôºå‰∏îÊ∫ñÁ¢∫Â∫¶ÂæàÈ´ò„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË≠âÊòé AI Ë°çÁîüÁöÑ BUS ‰π≥ÊàøÂØÜÂ∫¶ÂèØ‰ª•È†êÊ∏¨ÊàëÂÄëÊóèÁæ§ÁöÑ 5 Âπ¥‰π≥ÁôåÈ¢®Èö™„ÄÇ</paragraph>

##### **Monitoring fairness in machine learning models that predict patient mortality in the ICU**
2411.00190v2 by Tempest A. van Schaik, Xinggang Liu, Louis Atallah, Omar Badawi

This work proposes a fairness monitoring approach for machine learning models
that predict patient mortality in the ICU. We investigate how well models
perform for patient groups with different race, sex and medical diagnoses. We
investigate Documentation bias in clinical measurement, showing how fairness
analysis provides a more detailed and insightful comparison of model
performance than traditional accuracy metrics alone.

ÊëòË¶ÅÔºöÈÄôÈ†ÖÁ†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÂÖ¨Âπ≥ÊÄßÁõ£ÊéßÊñπÊ≥ïÔºåÁî®ÊñºÈ†êÊ∏¨Âä†Ë≠∑ÁóÖÊàø‰∏≠ÁóÖÊÇ£Ê≠ª‰∫°ÁéáÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°Âûã„ÄÇÊàëÂÄëÊé¢Ë®éÊ®°ÂûãÂú®‰∏çÂêåÁ®ÆÊóè„ÄÅÊÄßÂà•ÂíåÈÜ´ÁôÇË®∫Êñ∑ÁöÑÁóÖÊÇ£Áæ§È´î‰∏≠Ë°®ÁèæÂ¶Ç‰Ωï„ÄÇÊàëÂÄëÊé¢Ë®éËá®Â∫äÊ∏¨Èáè‰∏≠ÁöÑÊñá‰ª∂ÂÅèÂ∑ÆÔºåË™™ÊòéÂÖ¨Âπ≥ÊÄßÂàÜÊûêÂ¶Ç‰ΩïÊèê‰æõÊØîÂÇ≥Áµ±Ê∫ñÁ¢∫ÊÄßÊåáÊ®ôÊõ¥Ë©≥Á¥∞‰∏îÊúâË¶ãÂú∞ÁöÑÊ®°ÂûãÊïàËÉΩÊØîËºÉ„ÄÇ

##### **Clinical Evaluation of Medical Image Synthesis: A Case Study in Wireless Capsule Endoscopy**
2411.00178v1 by Panagiota Gatoula, Dimitrios E. Diamantis, Anastasios Koulaouzidis, Cristina Carretero, Stefania Chetcuti-Zammit, Pablo Cortegoso Valdivia, Bego√±a Gonz√°lez-Su√°rez, Alessandro Mussetto, John Plevris, Alexander Robertson, Bruno Rosa, Ervin Toth, Dimitris K. Iakovidis

Sharing retrospectively acquired data is essential for both clinical research
and training. Synthetic Data Generation (SDG), using Artificial Intelligence
(AI) models, can overcome privacy barriers in sharing clinical data, enabling
advancements in medical diagnostics. This study focuses on the clinical
evaluation of medical SDG, with a proof-of-concept investigation on diagnosing
Inflammatory Bowel Disease (IBD) using Wireless Capsule Endoscopy (WCE) images.
The paper contributes by a) presenting a protocol for the systematic evaluation
of synthetic images by medical experts and b) applying it to assess TIDE-II, a
novel variational autoencoder-based model for high-resolution WCE image
synthesis, with a comprehensive qualitative evaluation conducted by 10
international WCE specialists, focusing on image quality, diversity, realism,
and clinical decision-making. The results show that TIDE-II generates
clinically relevant WCE images, helping to address data scarcity and enhance
diagnostic tools. The proposed protocol serves as a reference for future
research on medical image-generation techniques.

ÊëòË¶ÅÔºöÂõûÈ°ßÊÄßÁç≤ÂèñÁöÑË≥áÊñôÂàÜ‰∫´Â∞çÊñºËá®Â∫äÁ†îÁ©∂ÂíåË®ìÁ∑¥Ëá≥ÈóúÈáçË¶Å„ÄÇ‰ΩøÁî®‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÁöÑÂêàÊàêË≥áÊñôÁî¢Áîü (SDG) ËÉΩÂ§†ÂÖãÊúçËá®Â∫äË≥áÊñôÂÖ±‰∫´‰∏≠ÁöÑÈö±ÁßÅÈöúÁ§ôÔºå‰øÉÈÄ≤ÈÜ´ÁôÇË®∫Êñ∑ÁöÑÈÄ≤Â±ï„ÄÇÊú¨Á†îÁ©∂Â∞àÊ≥®ÊñºËá®Â∫äË©ï‰º∞ÈÜ´Â≠∏ SDGÔºå‰∏¶ÈÄèÈÅéÁÑ°Á∑öËÜ†ÂõäÂÖßË¶ñÈè° (WCE) ÂΩ±ÂÉèË®∫Êñ∑ÁôºÁÇéÊÄßËÖ∏ÈÅìÁñæÁóÖ (IBD) ÁöÑÊ¶ÇÂøµÈ©óË≠âË™øÊü•„ÄÇÊú¨ÊñáÁöÑË≤¢ÁçªÂåÖÊã¨Ôºöa) ÊèêÂá∫Áî±ÈÜ´Â≠∏Â∞àÂÆ∂Á≥ªÁµ±ÊÄßË©ï‰º∞ÂêàÊàêÂΩ±ÂÉèÁöÑÂçîÂÆöÔºå‰ª•Âèä b) Â∞áÂÖ∂ÊáâÁî®ÊñºË©ï‰º∞ TIDE-IIÔºåÈÄôÊòØ‰∏ÄÂÄãÁî®ÊñºÈ´òËß£ÊûêÂ∫¶ WCE ÂΩ±ÂÉèÂêàÊàêÁöÑËÆäÁï∞Ëá™ÂãïÁ∑®Á¢ºÂô®Ê®°ÂûãÔºå‰∏¶Áî± 10 ‰ΩçÂúãÈöõ WCE Â∞àÂÆ∂ÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÂìÅË≥™Ë©ï‰º∞ÔºåÈáçÈªûÂú®ÊñºÂΩ±ÂÉèÂìÅË≥™„ÄÅÂ§öÊ®£ÊÄß„ÄÅÁúüÂØ¶ÊÄßÔºå‰ª•ÂèäËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÁµêÊûúÈ°ØÁ§∫ TIDE-II Áî¢Áîü‰∫ÜËá®Â∫äÁõ∏ÈóúÁöÑ WCE ÂΩ±ÂÉèÔºåÊúâÂä©ÊñºËß£Ê±∫Ë≥áÊñôÁ®ÄÂ∞ëÁöÑÂïèÈ°åÔºå‰∏¶Â¢ûÂº∑Ë®∫Êñ∑Â∑•ÂÖ∑„ÄÇÊâÄÊèêÂá∫ÁöÑÂçîÂÆöÂèØ‰ΩúÁÇ∫Êú™‰æÜÈÜ´Â≠∏ÂΩ±ÂÉèÁî¢ÁîüÊäÄË°ìÁ†îÁ©∂ÁöÑÂèÉËÄÉ„ÄÇ

##### **Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning**
2411.00173v1 by John Wu, David Wu, Jimeng Sun

Medical coding, the translation of unstructured clinical text into
standardized medical codes, is a crucial but time-consuming healthcare
practice. Though large language models (LLM) could automate the coding process
and improve the efficiency of such tasks, interpretability remains paramount
for maintaining patient trust. Current efforts in interpretability of medical
coding applications rely heavily on label attention mechanisms, which often
leads to the highlighting of extraneous tokens irrelevant to the ICD code. To
facilitate accurate interpretability in medical language models, this paper
leverages dictionary learning that can efficiently extract sparsely activated
representations from dense language model embeddings in superposition. Compared
with common label attention mechanisms, our model goes beyond token-level
representations by building an interpretable dictionary which enhances the
mechanistic-based explanations for each ICD code prediction, even when the
highlighted tokens are medically irrelevant. We show that dictionary features
can steer model behavior, elucidate the hidden meanings of upwards of 90% of
medically irrelevant tokens, and are human interpretable.

ÊëòË¶ÅÔºöÈÜ´ÁôÇÁ∑®Á¢ºÊòØÂ∞áÈùûÁµêÊßãÂåñÁöÑËá®Â∫äÊñáÊú¨ËΩâÊèõÁÇ∫Ê®ôÊ∫ñÂåñÈÜ´ÁôÇ‰ª£Á¢ºÁöÑÈÅéÁ®ãÔºåÊòØ‰∏ÄÈ†ÖËá≥ÈóúÈáçË¶ÅÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÂØ¶ÂãôÔºå‰ΩÜËÄóÊôÇË≤ªÂäõ„ÄÇÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂèØ‰ª•Ëá™ÂãïÂåñÁ∑®Á¢ºÊµÅÁ®ã‰∏¶ÊèêÂçáÊ≠§È°û‰ªªÂãôÁöÑÊïàÁéáÔºå‰ΩÜÂèØËß£ÈáãÊÄßÂ∞çÊñºÁ∂≠Ë≠∑ÊÇ£ËÄÖ‰ø°‰ªª‰ªçÁÑ∂Ëá≥ÈóúÈáçË¶Å„ÄÇÁõÆÂâçÂú®ÈÜ´ÁôÇÁ∑®Á¢ºÊáâÁî®Á®ãÂºèÁöÑÂèØËß£ÈáãÊÄßÊñπÈù¢ÊâÄÂÅöÁöÑÂä™ÂäõÔºåÊ•µÂ∫¶‰æùË≥¥Ê®ôÁ±§Ê≥®ÊÑèÊ©üÂà∂ÔºåÈÄôÈÄöÂ∏∏ÊúÉÂ∞éËá¥Âº∑Ë™øËàá ICD ‰ª£Á¢ºÁÑ°ÈóúÁöÑÁÑ°ÈóúÁ¨¶Ëôü„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÈÜ´ÁôÇË™ûË®ÄÊ®°ÂûãÁöÑÊ∫ñÁ¢∫ÂèØËß£ÈáãÊÄßÔºåÊú¨ÊñáÂà©Áî®Â≠óÂÖ∏Â≠∏ÁøíÔºåÂèØ‰ª•ÊúâÊïàÂú∞ÂæûÁñäÂä†ÁöÑÁ®†ÂØÜË™ûË®ÄÊ®°ÂûãÂµåÂÖ•‰∏≠ÊèêÂèñÁ®ÄÁñèÊøÄÊ¥ªÁöÑË°®Á§∫„ÄÇËàáÂ∏∏Ë¶ãÁöÑÊ®ôÁ±§Ê≥®ÊÑèÊ©üÂà∂Áõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãË∂ÖË∂ä‰∫ÜÁ¨¶ËôüÂ±§Á¥öÁöÑË°®Á§∫ÔºåÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÂ≠óÂÖ∏ÔºåÂ¢ûÂº∑‰∫ÜÂ∞çÊØèÂÄã ICD ‰ª£Á¢ºÈ†êÊ∏¨ÁöÑÂü∫ÊñºÊ©üÂà∂ÁöÑËß£ÈáãÔºåÂç≥‰ΩøÂº∑Ë™øÁöÑÁ¨¶ËôüÂú®ÈÜ´Â≠∏‰∏äÁÑ°ÈóúÁ∑äË¶Å„ÄÇÊàëÂÄëË≠âÊòéÂ≠óÂÖ∏ÁâπÂæµÂèØ‰ª•ÂºïÂ∞éÊ®°ÂûãË°åÁÇ∫ÔºåÈó°Êòé 90% ‰ª•‰∏äÂú®ÈÜ´Â≠∏‰∏äÁÑ°ÈóúÁöÑÁ¨¶ËôüÁöÑÈö±ËóèÊÑèÁæ©Ôºå‰∏¶‰∏î‰∫∫È°ûÂèØ‰ª•Ëß£Èáã„ÄÇ

##### **Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks**
2410.24032v1 by Yingzhe Peng, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Xu Yang, Dongmei Zhang, Saravan Rajmohan, Qi Zhang

The rise of large language models (LLMs) has revolutionized user interactions
with knowledge-based systems, enabling chatbots to synthesize vast amounts of
information and assist with complex, exploratory tasks. However, LLM-based
chatbots often struggle to provide personalized support, particularly when
users start with vague queries or lack sufficient contextual information. This
paper introduces the Collaborative Assistant for Personalized Exploration
(CARE), a system designed to enhance personalization in exploratory tasks by
combining a multi-agent LLM framework with a structured user interface. CARE's
interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling
iterative query refinement and dynamic solution generation. The multi-agent
framework collaborates to identify both explicit and implicit user needs,
delivering tailored, actionable solutions. In a within-subject user study with
22 participants, CARE was consistently preferred over a baseline LLM chatbot,
with users praising its ability to reduce cognitive load, inspire creativity,
and provide more tailored solutions. Our findings highlight CARE's potential to
transform LLM-based systems from passive information retrievers to proactive
partners in personalized problem-solving and exploration.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËààËµ∑ÂæπÂ∫ïÊîπËÆä‰∫Ü‰ΩøÁî®ËÄÖËàáÂü∫ÊñºÁü•Ë≠òÁöÑÁ≥ªÁµ±‰∫íÂãïÁöÑÊñπÂºèÔºåËÆìËÅäÂ§©Ê©üÂô®‰∫∫ËÉΩÂ§†Á∂úÂêàÂ§ßÈáèÁöÑË≥áË®äÔºå‰∏¶ÂçîÂä©ÈÄ≤Ë°åË§áÈõúÁöÑÊé¢Á¥¢ÊÄß‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÂü∫Êñº LLM ÁöÑËÅäÂ§©Ê©üÂô®‰∫∫ÈÄöÂ∏∏Èõ£‰ª•Êèê‰æõÂÄã‰∫∫ÂåñÁöÑÊîØÊè¥ÔºåÁâπÂà•ÊòØÂú®‰ΩøÁî®ËÄÖ‰∏ÄÈñãÂßãÊèêÂá∫ÁöÑÊü•Ë©¢ÂæàÊ®°Á≥äÔºåÊàñÁº∫‰πèË∂≥Â§†ÁöÑËÑàÁµ°Ë≥áË®äÊôÇ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫ÜÂÄã‰∫∫ÂåñÊé¢Á¥¢ÁöÑÂçî‰ΩúÂä©ÁêÜ (CARE)Ôºå‰∏ÄÂÄãÊó®Âú®ÈÄèÈÅéÁµêÂêàÂ§öÈáç‰ª£ÁêÜ LLM Êû∂ÊßãËàáÁµêÊßãÂåñÁöÑ‰ΩøÁî®ËÄÖ‰ªãÈù¢‰æÜÂ¢ûÂº∑Êé¢Á¥¢ÊÄß‰ªªÂãô‰∏≠ÂÄã‰∫∫ÂåñÁöÑÁ≥ªÁµ±„ÄÇCARE ÁöÑ‰ªãÈù¢ÂåÖÂê´ËÅäÂ§©Èù¢Êùø„ÄÅËß£Ê±∫ÊñπÊ°àÈù¢ÊùøÂíåÈúÄÊ±ÇÈù¢ÊùøÔºåÂèØÈÄ≤Ë°åÂèçË¶ÜÁöÑÊü•Ë©¢Á≤æÁÖâÂíåÂãïÊÖãÁöÑËß£Ê±∫ÊñπÊ°àÁî¢Áîü„ÄÇÂ§öÈáç‰ª£ÁêÜÊû∂ÊßãÂçî‰ΩúË≠òÂà•ÊòéÁ¢∫ÂíåÈö±Âê´ÁöÑ‰ΩøÁî®ËÄÖÈúÄÊ±ÇÔºåÊèê‰æõÂÆ¢Ë£ΩÂåñ‰∏îÂèØË°åÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂú®‰∏ÄÂÄãÊúâ 22 ‰ΩçÂèÉËàáËÄÖÁöÑÂèóË©¶ËÄÖÂÖßÁ†îÁ©∂‰∏≠ÔºåCARE ÊåÅÁ∫åÁç≤ÂæóÊØîÂü∫Ê∫ñ LLM ËÅäÂ§©Ê©üÂô®‰∫∫Êõ¥Â•ΩÁöÑË©ïÂÉπÔºå‰ΩøÁî®ËÄÖËÆöË≥ûÂÖ∂Ê∏õËºïË™çÁü•Ë≤†Êìî„ÄÅÊøÄÁôºÂâµÈÄ†ÂäõÔºå‰ª•ÂèäÊèê‰æõÊõ¥ÂÆ¢Ë£ΩÂåñËß£Ê±∫ÊñπÊ°àÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁôºÁèæÁ™ÅÈ°Ø‰∫Ü CARE Â∞áÂü∫Êñº LLM ÁöÑÁ≥ªÁµ±ÂæûË¢´ÂãïÁöÑË≥áË®äÊ™¢Á¥¢ËÄÖËΩâËÆäÁÇ∫ÂÄã‰∫∫ÂåñÂïèÈ°åËß£Ê±∫ÂíåÊé¢Á¥¢‰∏≠ÁöÑ‰∏ªÂãïÂ§•‰º¥ÁöÑÊΩõÂäõ„ÄÇ

##### **Neural Network Verification with PyRAT**
2410.23903v1 by Augustin Lemesle, Julien Lehmann, Tristan Le Gall

As AI systems are becoming more and more popular and used in various critical
domains (health, transport, energy, ...), the need to provide guarantees and
trust of their safety is undeniable. To this end, we present PyRAT, a tool
based on abstract interpretation to verify the safety and the robustness of
neural networks. In this paper, we describe the different abstractions used by
PyRAT to find the reachable states of a neural network starting from its input
as well as the main features of the tool to provide fast and accurate analysis
of neural networks. PyRAT has already been used in several collaborations to
ensure safety guarantees, with its second place at the VNN-Comp 2024 showcasing
its performance.

ÊëòË¶ÅÔºöÈö®Ëëó AI Á≥ªÁµ±Ë∂ä‰æÜË∂äÊôÆÂèäÔºå‰∏¶Áî®ÊñºÂêÑÁ®ÆÈóúÈçµÈ†òÂüüÔºàÂÅ•Â∫∑„ÄÅÈÅãËº∏„ÄÅËÉΩÊ∫êÔºå...ÔºâÔºåÊèê‰æõÂÖ∂ÂÆâÂÖ®‰øùË≠âÂíå‰ø°‰ªªÁöÑÈúÄÊ±ÇÊòØ‰∏çÂÆπÂê¶Ë™çÁöÑ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü PyRATÔºå‰∏ÄÂÄãÂü∫ÊñºÊäΩË±°Ë©ÆÈáãÁöÑÂ∑•ÂÖ∑ÔºåÁî®ÊñºÈ©óË≠âÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂÆâÂÖ®ÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèèËø∞‰∫Ü PyRAT Áî®ÊñºÂæûÁ•ûÁ∂ìÁ∂≤Ë∑ØËº∏ÂÖ•‰∏≠ÊâæÂá∫ÂèØÈÅîÁãÄÊÖãÁöÑ‰∏çÂêåÊäΩË±°Ôºå‰ª•ÂèäË©≤Â∑•ÂÖ∑ÁöÑ‰∏ªË¶ÅÂäüËÉΩÔºå‰ª•Êèê‰æõÂø´ÈÄü‰∏îÊ∫ñÁ¢∫ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÂàÜÊûê„ÄÇPyRAT Â∑≤Âú®Â§öÈ†ÖÂêà‰Ωú‰∏≠Áî®ÊñºÁ¢∫‰øùÂÆâÂÖ®‰øùË≠âÔºåÂÖ∂Âú® VNN-Comp 2024 ‰∏≠Áç≤ÂæóÁ¨¨‰∫åÂêçÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂ÊïàËÉΩ„ÄÇ

##### **Counterfactual MRI Data Augmentation using Conditional Denoising Diffusion Generative Models**
2410.23835v1 by Pedro Mor√£o, Joao Santinha, Yasna Forghani, Nuno Lou√ß√£o, Pedro Gouveia, Mario A. T. Figueiredo

Deep learning (DL) models in medical imaging face challenges in
generalizability and robustness due to variations in image acquisition
parameters (IAP). In this work, we introduce a novel method using conditional
denoising diffusion generative models (cDDGMs) to generate counterfactual
magnetic resonance (MR) images that simulate different IAP without altering
patient anatomy. We demonstrate that using these counterfactual images for data
augmentation can improve segmentation accuracy, particularly in
out-of-distribution settings, enhancing the overall generalizability and
robustness of DL models across diverse imaging conditions. Our approach shows
promise in addressing domain and covariate shifts in medical imaging. The code
is publicly available at https:
//github.com/pedromorao/Counterfactual-MRI-Data-Augmentation

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏Áøí (DL) Ê®°ÂûãÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÊúÉÂõ†ÂΩ±ÂÉèÊì∑ÂèñÂèÉÊï∏ (IAP) ÁöÑËÆäÂåñËÄåÈù¢Ëá®ÂèØÊ¶ÇÊã¨ÊÄßÂíåÁ©©ÂÅ•ÊÄßÁöÑÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Ê¢ù‰ª∂ÂºèÂéªÂô™Êì¥Êï£ÁîüÊàêÊ®°Âûã (cDDGMs) ÁöÑÊñ∞ÊñπÊ≥ïÔºå‰ª•Áî¢ÁîüÂèç‰∫ãÂØ¶Á£ÅÂÖ±ÊåØ (MR) ÂΩ±ÂÉèÔºåÊ®°Êì¨‰∏çÂêåÁöÑ IAPÔºåËÄå‰∏çÊúÉÊîπËÆäÊÇ£ËÄÖÁöÑËß£ÂâñÁµêÊßã„ÄÇÊàëÂÄëË≠âÊòé‰ΩøÁî®ÈÄô‰∫õÂèç‰∫ãÂØ¶ÂΩ±ÂÉèÈÄ≤Ë°åË≥áÊñôÊì¥ÂÖÖÂèØ‰ª•ÊèêÈ´òÂàÜÂâ≤Ê∫ñÁ¢∫Â∫¶ÔºåÁâπÂà•ÊòØÂú®ÂàÜ‰ΩàÂ§ñË®≠ÂÆö‰∏≠ÔºåÂ¢ûÂº∑ DL Ê®°ÂûãÂú®‰∏çÂêåÂΩ±ÂÉèÊ¢ù‰ª∂‰∏ãÁöÑÊï¥È´îÂèØÊ¶ÇÊã¨ÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈ°ØÁ§∫‰∫ÜËß£Ê±∫ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÈ†òÂüüÂíåÂçîËÆäÊï∏ËΩâÁßªÁöÑÂâçÊôØ„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº https:
//github.com/pedromorao/Counterfactual-MRI-Data-Augmentation

##### **Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding**
2410.23822v1 by Jinlong He, Pengfei Li, Gang Liu, Shenjun Zhong

Multimodal Large Language Models (MLLMs) inherit the superior text
understanding capabilities of LLMs and extend these capabilities to multimodal
scenarios. These models achieve excellent results in the general domain of
multimodal tasks. However, in the medical domain, the substantial training
costs and the requirement for extensive medical data pose challenges to the
development of medical MLLMs. Furthermore, due to the free-text form of
answers, tasks such as visual grounding that need to produce output in a
prescribed form become difficult for MLLMs. So far, there have been no medical
MLLMs works in medical visual grounding area. For the medical vision grounding
task, which involves identifying locations in medical images based on short
text descriptions, we propose Parameter-efficient Fine-tuning medical
multimodal large language models for Medcial Visual Grounding (PFMVG). To
validate the performance of the model, we evaluate it on a public benchmark
dataset for medical visual grounding, where it achieves competitive results,
and significantly outperforming GPT-4v. Our code will be open sourced after
peer review.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (MLLM) ÁªßÊâø‰∫Ü LLM ‰ºòË∂äÁöÑÊñáÊú¨ÁêÜËß£ËÉΩÂäõÔºåÂπ∂Â∞ÜËøô‰∫õËÉΩÂäõÊâ©Â±ïÂà∞Â§öÊ®°ÊÄÅÂú∫ÊôØ„ÄÇËøô‰∫õÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°ÁöÑÈÄöÁî®È¢ÜÂüü‰∏≠ÂèñÂæó‰∫ÜÂá∫Ëâ≤ÁöÑÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÂú®ÂåªÂ≠¶È¢ÜÂüüÔºåÂ§ßÈáèÁöÑËÆ≠ÁªÉÊàêÊú¨ÂíåÂØπÂπøÊ≥õÂåªÂ≠¶Êï∞ÊçÆÁöÑÈúÄÊ±ÇÂØπÂåªÂ≠¶ MLLM ÁöÑÂèëÂ±ïÊûÑÊàê‰∫ÜÊåëÊàò„ÄÇÊ≠§Â§ñÔºåÁî±‰∫éÁ≠îÊ°àÁöÑËá™Áî±ÊñáÊú¨ÂΩ¢ÂºèÔºåÈúÄË¶Å‰ª•ËßÑÂÆöÂΩ¢ÂºèÁîüÊàêËæìÂá∫ÁöÑ‰ªªÂä°Ôºà‰æãÂ¶ÇËßÜËßâÂü∫Á°ÄÔºâÂØπ‰∫é MLLM Êù•ËØ¥ÂèòÂæóÂõ∞Èöæ„ÄÇÂà∞ÁõÆÂâç‰∏∫Ê≠¢ÔºåËøòÊ≤°ÊúâÂåªÂ≠¶ MLLM Âú®ÂåªÂ≠¶ËßÜËßâÂü∫Á°ÄÈ¢ÜÂüüÂ∑•‰Ωú„ÄÇÂØπ‰∫éÂåªÂ≠¶ËßÜËßâÂü∫Á°Ä‰ªªÂä°ÔºåÂÆÉÊ∂âÂèäÊ†πÊçÆÁÆÄÁü≠ÁöÑÊñáÊú¨ÊèèËø∞ËØÜÂà´ÂåªÂ≠¶ÂõæÂÉè‰∏≠ÁöÑ‰ΩçÁΩÆÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÁî®‰∫éÂåªÂ≠¶ËßÜËßâÂü∫Á°ÄÁöÑÂèÇÊï∞È´òÊïàÂæÆË∞ÉÂåªÂ≠¶Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (PFMVG)„ÄÇ‰∏∫‰∫ÜÈ™åËØÅÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÊàë‰ª¨Âú®ÂåªÂ≠¶ËßÜËßâÂü∫Á°ÄÁöÑÂÖ¨ÂÖ±Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂØπÂÖ∂ËøõË°å‰∫ÜËØÑ‰º∞ÔºåÂÆÉÂèñÂæó‰∫ÜÊúâÁ´û‰∫âÂäõÁöÑÁªìÊûúÔºåÂπ∂‰∏îÊòéÊòæ‰ºò‰∫é GPT-4v„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂ∞ÜÂú®ÂêåË°åËØÑÂÆ°ÂêéÂºÄÊ∫ê„ÄÇ

##### **Improving snore detection under limited dataset through harmonic/percussive source separation and convolutional neural networks**
2410.23796v1 by F. D. Gonzalez-Martinez, J. J. Carabias-Orti, F. J. Canadas-Quesada, N. Ruiz-Reyes, D. Martinez-Munoz, S. Garcia-Galan

Snoring, an acoustic biomarker commonly observed in individuals with
Obstructive Sleep Apnoea Syndrome (OSAS), holds significant potential for
diagnosing and monitoring this recognized clinical disorder. Irrespective of
snoring types, most snoring instances exhibit identifiable harmonic patterns
manifested through distinctive energy distributions over time. In this work, we
propose a novel method to differentiate monaural snoring from non-snoring
sounds by analyzing the harmonic content of the input sound using
harmonic/percussive sound source separation (HPSS). The resulting feature,
based on the harmonic spectrogram from HPSS, is employed as input data for
conventional neural network architectures, aiming to enhance snoring detection
performance even under a limited data learning framework. To evaluate the
performance of our proposal, we studied two different scenarios: 1) using a
large dataset of snoring and interfering sounds, and 2) using a reduced
training set composed of around 1% of the data material. In the former
scenario, the proposed HPSS-based feature provides competitive results compared
to other input features from the literature. However, the key advantage of the
proposed method lies in the superior performance of the harmonic spectrogram
derived from HPSS in a limited data learning context. In this particular
scenario, using the proposed harmonic feature significantly enhances the
performance of all the studied architectures in comparison to the classical
input features documented in the existing literature. This finding clearly
demonstrates that incorporating harmonic content enables more reliable learning
of the essential time-frequency characteristics that are prevalent in most
snoring sounds, even in scenarios where the amount of training data is limited.

ÊëòË¶ÅÔºöÈºæËÅ≤ÊòØ‰∏ÄÁ®ÆÂú®ÈòªÂ°ûÊÄßÁù°Áú†ÂëºÂê∏‰∏≠Ê≠¢ÁóáÂÄôÁæ§ (OSAS) ÊÇ£ËÄÖ‰∏≠Â∏∏Ë¶ãÁöÑËÅ≤Â≠∏ÁîüÁâ©Ê®ôË®òÔºåÂ∞çÊñºË®∫Êñ∑ÂíåÁõ£ÊéßÊ≠§ÂÖ¨Ë™çÁöÑËá®Â∫äÁñæÁóÖÂÖ∑ÊúâÈ°ØËëóÊΩõÂäõ„ÄÇÁÑ°Ë´ñÈºæËÅ≤È°ûÂûãÂ¶Ç‰ΩïÔºåÂ§ßÂ§öÊï∏ÈºæËÅ≤ÈÉΩË°®ÁèæÂá∫ÂèØË≠òÂà•ÁöÑË´ßÊ≥¢Ê®°ÂºèÔºå‰∏¶Èö®ËëóÊôÇÈñìÊé®ÁßªË°®ÁèæÂá∫Áç®ÁâπÁöÑËÉΩÈáèÂàÜ‰Ωà„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÈÄöÈÅé‰ΩøÁî®Ë´ßÊ≥¢/ÊâìÊìäËÅ≤Ê∫êÂàÜÈõ¢ (HPSS) ÂàÜÊûêËº∏ÂÖ•ËÅ≤Èü≥ÁöÑË´ßÊ≥¢ÂÖßÂÆπÔºåÂ∞áÂñÆËÅ≤ÈÅìÈºæËÅ≤ËàáÈùûÈºæËÅ≤ÂçÄÂàÜÈñã‰æÜ„ÄÇÂü∫Êñº HPSS ÁöÑË´ßÊ≥¢È†ªË≠úÂúñÊâÄÁî¢ÁîüÁöÑÁâπÂæµÔºåË¢´Áî®‰ΩúÂÇ≥Áµ±Á•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÁöÑËº∏ÂÖ•Ë≥áÊñôÔºåÊó®Âú®Âç≥‰ΩøÂú®ÊúâÈôêË≥áÊñôÂ≠∏ÁøíÊû∂Êßã‰∏ã‰πüËÉΩÂ¢ûÂº∑ÈºæËÅ≤ÂÅµÊ∏¨ÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÊèêÊ°àÁöÑÊïàËÉΩÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÂÖ©Á®Æ‰∏çÂêåÁöÑÊÉÖÂ¢ÉÔºö1) ‰ΩøÁî®Â§ßÈáèÁöÑÈºæËÅ≤ÂíåÂπ≤ÊìæËÅ≤Ë≥áÊñôÈõÜÔºå‰ª•Âèä 2) ‰ΩøÁî®Áî±Á¥Ñ 1% Ë≥áÊñôÁ¥†ÊùêÁµÑÊàêÁöÑÁ∏ÆÊ∏õË®ìÁ∑¥ÈõÜ„ÄÇÂú®Ââç‰∏ÄÁ®ÆÊÉÖÂ¢É‰∏≠ÔºåËàáÊñáÁçª‰∏≠ÁöÑÂÖ∂‰ªñËº∏ÂÖ•ÁâπÂæµÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÂü∫Êñº HPSS ÁöÑÁâπÂæµÊèê‰æõ‰∫ÜÂÖ∑ÊúâÁ´∂Áà≠ÂäõÁöÑÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑ‰∏ªË¶ÅÂÑ™ÈªûÂú®ÊñºÔºåÂú®ÊúâÈôêË≥áÊñôÂ≠∏ÁøíÊÉÖÂ¢É‰∏≠ÔºåÊ∫êËá™ HPSS ÁöÑË´ßÊ≥¢È†ªË≠úÂúñÂÖ∑ÊúâÂÑ™Áï∞ÁöÑÊïàËÉΩ„ÄÇÂú®ÈÄôÂÄãÁâπÂÆöÊÉÖÂ¢É‰∏≠ÔºåËàáÁèæÊúâÊñáÁçª‰∏≠Ë®òËºâÁöÑÂÇ≥Áµ±Ëº∏ÂÖ•ÁâπÂæµÁõ∏ÊØîÔºå‰ΩøÁî®ÊâÄÊèêÂá∫ÁöÑË´ßÊ≥¢ÁâπÂæµÈ°ØËëóÂ¢ûÂº∑‰∫ÜÊâÄÊúâÁ†îÁ©∂Êû∂ÊßãÁöÑÊïàËÉΩ„ÄÇÈÄô‰∏ÄÁôºÁèæÊ∏ÖÊ•öÂú∞Ë°®ÊòéÔºåÂç≥‰ΩøÂú®Ë®ìÁ∑¥Ë≥áÊñôÈáèÊúâÈôêÁöÑÊÉÖÂ¢É‰∏≠ÔºåÁ¥çÂÖ•Ë´ßÊ≥¢ÂÖßÂÆπ‰πüËÉΩÂ§†Êõ¥ÂèØÈù†Âú∞Â≠∏ÁøíÂ§ßÂ§öÊï∏ÈºæËÅ≤‰∏≠ÊôÆÈÅçÂ≠òÂú®ÁöÑÂøÖË¶ÅÊôÇÈ†ªÁâπÂæµ„ÄÇ

##### **The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams**
2410.23769v1 by Yunqi Zhu, Wen Tang, Ying Sun, Xuebing Yang

Recent research on large language models (LLMs) has primarily focused on
their adaptation and application in specialized domains. The application of
LLMs in the medical field is mainly concentrated on tasks such as the
automation of medical report generation, summarization, diagnostic reasoning,
and question-and-answer interactions between doctors and patients. The
challenge of becoming a good teacher is more formidable than that of becoming a
good student, and this study pioneers the application of LLMs in the field of
medical education. In this work, we investigate the extent to which LLMs can
generate medical qualification exam questions and corresponding answers based
on few-shot prompts. Utilizing a real-world Chinese dataset of elderly chronic
diseases, we tasked the LLMs with generating open-ended questions and answers
based on a subset of sampled admission reports across eight widely used LLMs,
including ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, and
Mistral. Furthermore, we engaged medical experts to manually evaluate these
open-ended questions and answers across multiple dimensions. The study found
that LLMs, after using few-shot prompts, can effectively mimic real-world
medical qualification exam questions, whereas there is room for improvement in
the correctness, evidence-based statements, and professionalism of the
generated answers. Moreover, LLMs also demonstrate a decent level of ability to
correct and rectify reference answers. Given the immense potential of
artificial intelligence in the medical field, the task of generating questions
and answers for medical qualification exams aimed at medical students, interns
and residents can be a significant focus of future research.

ÊëòË¶ÅÔºö<paragraph>ÈáùÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËøëÊúüÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂÆÉÂÄëÂú®ÁâπÂÆöÈ†òÂüüÁöÑÈÅ©ÊáâÂíåÊáâÁî®„ÄÇLLM Âú®ÈÜ´Â≠∏È†òÂüüÁöÑÊáâÁî®‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ëá™ÂãïÂåñÁóÖÊ≠∑Áî¢Áîü„ÄÅÊëòË¶Å„ÄÅË®∫Êñ∑Êé®ÁêÜ‰ª•ÂèäÈÜ´ÁîüËàáÁóÖ‰∫∫‰πãÈñìÂïèÁ≠î‰∫íÂãïÁ≠â‰ªªÂãô„ÄÇÊàêÁÇ∫‰∏ÄÂêçÂ•ΩËÄÅÂ∏´ÁöÑÊåëÊà∞ÊØîÊàêÁÇ∫‰∏ÄÂêçÂ•ΩÂ≠∏ÁîüÊõ¥Ëâ±ÈâÖÔºåËÄåÊú¨Á†îÁ©∂ÈñãÂâµ‰∫Ü LLM Âú®ÈÜ´Â≠∏ÊïôËÇ≤È†òÂüüÁöÑÊáâÁî®„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Âú®Â∞ëÊï∏ÊèêÁ§∫‰∏ãÁî¢ÁîüÈÜ´Â≠∏Ë≥áÊ†ºËÄÉË©¶È°åÁõÆÂíåÂ∞çÊáâÁ≠îÊ°àÁöÑÁ®ãÂ∫¶„ÄÇÂà©Áî®‰∏ÄÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑËÄÅÂπ¥ÊÖ¢ÊÄßÁñæÁóÖ‰∏≠ÊñáÊï∏ÊìöÈõÜÔºåÊàëÂÄëËÆì LLM Ê†πÊìöÂÖ´ÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑ LLMÔºàÂåÖÊã¨ ERNIE 4„ÄÅChatGLM 4„ÄÅË±ÜÂåÖ„ÄÅÊ∑∑ÂÖÉ„ÄÅSpark 4„ÄÅQwen„ÄÅLlama 3 Âíå MistralÔºâÊäΩÂèñÁöÑÂÖ•Èô¢Â†±ÂëäÂ≠êÈõÜÁî¢ÁîüÈñãÊîæÂºèÂïèÈ°åÂíåÁ≠îÊ°à„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëËÅòË´ãÈÜ´Â≠∏Â∞àÂÆ∂ÊâãÂãïË©ï‰º∞ÈÄô‰∫õÈñãÊîæÂºèÂïèÈ°åÂíåÁ≠îÊ°àÁöÑÂ§öÂÄãÈù¢Âêë„ÄÇÁ†îÁ©∂ÁôºÁèæÔºåLLM Âú®‰ΩøÁî®Â∞ëÊï∏ÊèêÁ§∫ÂæåÔºåÂèØ‰ª•ÊúâÊïàÊ®°Êì¨ÁúüÂØ¶‰∏ñÁïåÁöÑÈÜ´Â≠∏Ë≥áÊ†ºËÄÉË©¶È°åÁõÆÔºåËÄåÁî¢ÁîüÁöÑÁ≠îÊ°àÂú®Ê≠£Á¢∫ÊÄß„ÄÅÂæ™Ë≠âÈô≥Ëø∞ÂíåÂ∞àÊ•≠ÊÄßÊñπÈù¢‰ªçÊúâÊîπÈÄ≤Á©∫Èñì„ÄÇÊ≠§Â§ñÔºåLLM ‰πüÂ±ïÁèæÂá∫Áõ∏Áï∂Á®ãÂ∫¶Êõ¥Ê≠£Âíå‰øÆÊ≠£ÂèÉËÄÉÁ≠îÊ°àÁöÑËÉΩÂäõ„ÄÇÈëëÊñº‰∫∫Â∑•Êô∫ËÉΩÂú®ÈÜ´Â≠∏È†òÂüüÁöÑÂ∑®Â§ßÊΩõÂäõÔºåÁî¢ÁîüÈáùÂ∞çÈÜ´Â≠∏Áîü„ÄÅÂØ¶ÁøíÈÜ´ÁîüÂíå‰ΩèÈô¢ÈÜ´ÁîüÁöÑÈÜ´Â≠∏Ë≥áÊ†ºËÄÉË©¶È°åÁõÆÂíåÁ≠îÊ°àÁöÑ‰ªªÂãôÔºåÂèØ‰ª•ÊàêÁÇ∫Êú™‰æÜÁ†îÁ©∂ÁöÑÈáçË¶ÅÈáçÈªû„ÄÇ</paragraph>

##### **Artificial intelligence to improve clinical coding practice in Scandinavia: a crossover randomized controlled trial**
2410.23725v1 by Taridzo Chomutare, Therese Olsen Svenning, Miguel √Ångel Tejedor Hern√°ndez, Phuong Dinh Ngo, Andrius Budrionis, Kaisa Markljung, Lill Irene Hind, Torbj√∏rn Torsvik, Karl √òyvind Mikalsen, Aleksandar Babic, Hercules Dalianis

\textbf{Trial design} Crossover randomized controlled trial. \textbf{Methods}
An AI tool, Easy-ICD, was developed to assist clinical coders and was tested
for improving both accuracy and time in a user study in Norway and Sweden.
Participants were randomly assigned to two groups, and crossed over between
coding complex (longer) texts versus simple (shorter) texts, while using our
tool versus not using our tool. \textbf{Results} Based on Mann-Whitney U test,
the median coding time difference for complex clinical text sequences was 123
seconds (\emph{P}\textless.001, 95\% CI: 81 to 164), representing a 46\%
reduction in median coding time when our tool is used. There was no significant
time difference for simpler text sequences. For coding accuracy, the
improvement we noted for both complex and simple texts was not significant.
\textbf{Conclusions} This study demonstrates the potential of AI to transform
common tasks in clinical workflows, with ostensible positive impacts on work
efficiencies for complex clinical coding tasks. Further studies within hospital
workflows are required before these presumed impacts can be more clearly
understood.

ÊëòË¶ÅÔºö**Ë©¶È©óË®≠Ë®à** ‰∫§ÂèâÈö®Ê©üÂ∞çÁÖßË©¶È©ó„ÄÇ**ÊñπÊ≥ï**ÈñãÁôº‰∫Ü‰∏ÄÁ®Æ AI Â∑•ÂÖ∑ Easy-ICDÔºå‰ª•ÂçîÂä©Ëá®Â∫äÁ∑®Á¢ºÂì°Ôºå‰∏¶Âú®Êå™Â®ÅÂíåÁëûÂÖ∏ÈÄ≤Ë°åÁöÑ‰∏ÄÈ†Ö‰ΩøÁî®ËÄÖÁ†îÁ©∂‰∏≠Ê∏¨Ë©¶ÂÖ∂Âú®Ê∫ñÁ¢∫ÊÄßÂíåÊôÇÈñì‰∏äÁöÑÊîπÈÄ≤„ÄÇÂèÉËàáËÄÖË¢´Èö®Ê©üÂàÜÁÇ∫ÂÖ©ÁµÑÔºå‰∏¶Âú®‰ΩøÁî®ÊàëÂÄëÁöÑÂ∑•ÂÖ∑Ëàá‰∏ç‰ΩøÁî®ÊàëÂÄëÁöÑÂ∑•ÂÖ∑ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞çË§áÈõúÔºàËºÉÈï∑ÔºâÊñáÊú¨ËàáÁ∞°ÂñÆÔºàËºÉÁü≠ÔºâÊñáÊú¨ÈÄ≤Ë°åÁ∑®Á¢º‰∫§Âèâ„ÄÇ**ÁµêÊûú**Ê†πÊìö Mann-Whitney U Ê™¢ÂÆöÔºåË§áÈõúËá®Â∫äÊñáÊú¨Â∫èÂàóÁöÑ‰∏≠‰ΩçÊï∏Á∑®Á¢ºÊôÇÈñìÂ∑ÆÁÇ∫ 123 ÁßíÔºà\emph{P}\textless.001Ôºå95% CIÔºö81 Ëá≥ 164ÔºâÔºåË°®Á§∫‰ΩøÁî®ÊàëÂÄëÁöÑÂ∑•ÂÖ∑ÊôÇ‰∏≠‰ΩçÊï∏Á∑®Á¢ºÊôÇÈñìÊ∏õÂ∞ë‰∫Ü 46%„ÄÇÂ∞çÊñºËºÉÁ∞°ÂñÆÁöÑÊñáÊú¨Â∫èÂàóÔºåÊ≤íÊúâÈ°ØËëóÁöÑÊôÇÈñìÂ∑ÆÁï∞„ÄÇÂ∞çÊñºÁ∑®Á¢ºÊ∫ñÁ¢∫ÊÄßÔºåÊàëÂÄëÂ∞çË§áÈõúÊñáÊú¨ÂíåÁ∞°ÂñÆÊñáÊú¨ÊâÄËßÄÂØüÂà∞ÁöÑÊîπÈÄ≤‰∏¶‰∏çÈ°ØËëó„ÄÇ**ÁµêË´ñ**ÈÄôÈ†ÖÁ†îÁ©∂Â±ïÁ§∫‰∫Ü AI Âú®ËΩâÊèõËá®Â∫äÂ∑•‰ΩúÊµÅÁ®ã‰∏≠Â∏∏Ë¶ã‰ªªÂãôÁöÑÊΩõÂäõÔºåÂ∞çË§áÈõúËá®Â∫äÁ∑®Á¢º‰ªªÂãôÁöÑÂ∑•‰ΩúÊïàÁéáÊúâÊòéÈ°ØÁöÑÊ≠£Èù¢ÂΩ±Èüø„ÄÇÂú®ÈÄô‰∫õÂÅáË®≠ÂΩ±ÈüøËÉΩÊõ¥Ê∏ÖÊ•öÂú∞Ë¢´ÁêÜËß£‰πãÂâçÔºåÈúÄË¶ÅÂú®ÈÜ´Èô¢Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÈÄ≤Ë°åÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂„ÄÇ

##### **Enhancing Brain Tumor Classification Using TrAdaBoost and Multi-Classifier Deep Learning Approaches**
2411.00875v1 by Mahin Mohammadi, Saman Jamshidi

Brain tumors pose a serious health threat due to their rapid growth and
potential for metastasis. While medical imaging has advanced significantly,
accurately identifying and characterizing these tumors remains a challenge.
This study addresses this challenge by leveraging the innovative TrAdaBoost
methodology to enhance the Brain Tumor Segmentation (BraTS2020) dataset, aiming
to improve the efficiency and accuracy of brain tumor classification. Our
approach combines state-of-the-art deep learning algorithms, including the
Vision Transformer (ViT), Capsule Neural Network (CapsNet), and convolutional
neural networks (CNNs) such as ResNet-152 and VGG16. By integrating these
models within a multi-classifier framework, we harness the strengths of each
approach to achieve more robust and reliable tumor classification. A novel
decision template is employed to synergistically combine outputs from different
algorithms, further enhancing classification accuracy. To augment the training
process, we incorporate a secondary dataset, "Brain Tumor MRI Dataset," as a
source domain, providing additional data for model training and improving
generalization capabilities. Our findings demonstrate a high accuracy rate in
classifying tumor versus non-tumor images, signifying the effectiveness of our
approach in the medical imaging domain. This study highlights the potential of
advanced machine learning techniques to contribute significantly to the early
and accurate diagnosis of brain tumors, ultimately improving patient outcomes.

ÊëòË¶ÅÔºöËÖ¶Áò§Áî±ÊñºÁîüÈï∑Âø´ÈÄü‰∏îÊúâËΩâÁßªÁöÑÂèØËÉΩÊÄßÔºåÂ∞çÂÅ•Â∫∑ÊßãÊàêÂö¥ÈáçÂ®ÅËÑÖ„ÄÇÈõñÁÑ∂ÈÜ´Â≠∏ÂΩ±ÂÉèÊäÄË°ìÂ∑≤Â§ßÂπÖÈÄ≤Ê≠•Ôºå‰ΩÜÁ≤æÊ∫ñËæ®Ë≠òÂíåÊèèËø∞ÈÄô‰∫õËÖ´Áò§‰ªçÁÑ∂ÊòØ‰∏ÄÂ§ßÊåëÊà∞„ÄÇÊú¨Á†îÁ©∂ÈÄèÈÅéÈÅãÁî®ÂâµÊñ∞ÁöÑ TrAdaBoost ÊñπÊ≥ïÊèêÂçáËÖ¶Áò§ÂàÜÂâ≤ (BraTS2020) Ë≥áÊñôÈõÜ‰æÜËß£Ê±∫ÈÄôÂÄãÊåëÊà∞ÔºåÁõÆÊ®ôÊòØÊèêÂçáËÖ¶Áò§ÂàÜÈ°ûÁöÑÊïàÁéáÂíåÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÁµêÂêà‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ïÔºåÂåÖÊã¨Ë¶ñË¶∫ËΩâÊèõÂô® (ViT)„ÄÅËÜ†ÂõäÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CapsNet) ÂíåÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN)Ôºå‰æãÂ¶Ç ResNet-152 Âíå VGG16„ÄÇÈÄèÈÅéÂú®Â§öÂàÜÈ°ûÂô®Êû∂Êßã‰∏≠Êï¥ÂêàÈÄô‰∫õÊ®°ÂûãÔºåÊàëÂÄëÂà©Áî®ÊØèÁ®ÆÊñπÊ≥ïÁöÑÂÑ™Èªû‰æÜÈÅîÊàêÊõ¥Âº∑ÂÅ•‰∏îÂèØÈù†ÁöÑËÖ´Áò§ÂàÜÈ°û„ÄÇÊé°Áî®Êñ∞Á©éÁöÑÊ±∫Á≠ñÁØÑÊú¨Ôºå‰ª•Á∂úÊïàÁµêÂêà‰∏çÂêåÊºîÁÆóÊ≥ïÁöÑËº∏Âá∫ÔºåÈÄ≤‰∏ÄÊ≠•ÊèêÂçáÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶„ÄÇÁÇ∫‰∫ÜÊì¥ÂÖÖË®ìÁ∑¥ÊµÅÁ®ãÔºåÊàëÂÄëÁ¥çÂÖ•Ê¨°Ë¶ÅË≥áÊñôÈõÜ„ÄåËÖ¶Áò§ MRI Ë≥áÊñôÈõÜ„Äç‰ΩúÁÇ∫‰æÜÊ∫êÁ∂≤ÂüüÔºåÊèê‰æõÈ°çÂ§ñÁöÑË≥áÊñôÁî®ÊñºÊ®°ÂûãË®ìÁ∑¥Ôºå‰∏¶ÊèêÂçáÊ¶ÇÂåñËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÂú®ÂàÜÈ°ûËÖ´Áò§ËàáÈùûËÖ´Áò§ÂΩ±ÂÉèÊôÇÔºåÊ∫ñÁ¢∫ÁéáÂæàÈ´òÔºåË°®Á§∫ÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÈ†òÂüü‰∏≠ÂæàÊúâÊïà„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™øÈÄ≤ÈöéÊ©üÂô®Â≠∏ÁøíÊäÄË°ìÁöÑÊΩõÂäõÔºåÂ∞çËÖ¶Áò§ÁöÑÊó©Êúü‰∏îÁ≤æÊ∫ñË®∫Êñ∑ÊúâÈ°ØËëóË≤¢ÁçªÔºåÈÄ≤ËÄåÊîπÂñÑÁóÖÊÇ£ÁöÑÊ≤ªÁôÇÁµêÊûú„ÄÇ

##### **Deep Convolutional Neural Networks on Multiclass Classification of Three-Dimensional Brain Images for Parkinson's Disease Stage Prediction**
2410.23649v1 by Guan-Hua Huang, Wan-Chen Lai, Tai-Been Chen, Chien-Chin Hsu, Huei-Yung Chen, Yi-Chen Wu, Li-Ren Yeh

Parkinson's disease (PD), a degenerative disorder of the central nervous
system, is commonly diagnosed using functional medical imaging techniques such
as single-photon emission computed tomography (SPECT). In this study, we
utilized two SPECT data sets (n = 634 and n = 202) from different hospitals to
develop a model capable of accurately predicting PD stages, a multiclass
classification task. We used the entire three-dimensional (3D) brain images as
input and experimented with various model architectures. Initially, we treated
the 3D images as sequences of two-dimensional (2D) slices and fed them
sequentially into 2D convolutional neural network (CNN) models pretrained on
ImageNet, averaging the outputs to obtain the final predicted stage. We also
applied 3D CNN models pretrained on Kinetics-400. Additionally, we incorporated
an attention mechanism to account for the varying importance of different
slices in the prediction process. To further enhance model efficacy and
robustness, we simultaneously trained the two data sets using weight sharing, a
technique known as cotraining. Our results demonstrated that 2D models
pretrained on ImageNet outperformed 3D models pretrained on Kinetics-400, and
models utilizing the attention mechanism outperformed both 2D and 3D models.
The cotraining technique proved effective in improving model performance when
the cotraining data sets were sufficiently large.

ÊëòË¶ÅÔºöÂ∏ïÈáëÊ£ÆÊ∞èÁóá (PD) ÊòØ‰∏ÄÁ®Æ‰∏≠Ê®ûÁ•ûÁ∂ìÁ≥ªÁµ±ÈÄÄÂåñÊÄßÁñæÁóÖÔºåÈÄöÂ∏∏‰ΩøÁî®ÂäüËÉΩÊÄßÈÜ´Â≠∏ÂΩ±ÂÉèÊäÄË°ìÔºå‰æãÂ¶ÇÂñÆÂÖâÂ≠êÁôºÂ∞ÑÊñ∑Â±§ÊéÉÊèè (SPECT) ‰æÜË®∫Êñ∑„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂà©Áî®‰æÜËá™‰∏çÂêåÈÜ´Èô¢ÁöÑÂÖ©ÂÄã SPECT Ë≥áÊñôÈõÜ (n = 634 Âíå n = 202) ‰æÜÈñãÁôº‰∏ÄÂÄãÊ®°ÂûãÔºåËÉΩÂ§†Ê∫ñÁ¢∫È†êÊ∏¨ PD ÂàÜÊúüÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§öÈ°ûÂà•ÂàÜÈ°û‰ªªÂãô„ÄÇÊàëÂÄë‰ΩøÁî®Êï¥ÂÄã‰∏âÁ∂≠ (3D) Â§ßËÖ¶ÂΩ±ÂÉè‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰∏¶ÂòóË©¶‰ΩøÁî®ÂêÑÁ®ÆÊ®°ÂûãÊû∂Êßã„ÄÇÊúÄÂàùÔºåÊàëÂÄëÂ∞á 3D ÂΩ±ÂÉèË¶ñÁÇ∫‰∫åÁ∂≠ (2D) ÂàáÁâáÁöÑÂ∫èÂàóÔºå‰∏¶Â∞áÂÆÉÂÄë‰æùÂ∫èËº∏ÂÖ•Âà∞È†êÂÖàÂú® ImageNet ‰∏äË®ìÁ∑¥ÈÅéÁöÑ 2D Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) Ê®°Âûã‰∏≠ÔºåÂèñÂπ≥ÂùáËº∏Âá∫ÂÄº‰æÜÂèñÂæóÊúÄÁµÇÈ†êÊ∏¨ÁöÑÊúüÂà•„ÄÇÊàëÂÄë‰πüÊáâÁî®È†êÂÖàÂú® Kinetics-400 ‰∏äË®ìÁ∑¥ÈÅéÁöÑ 3D CNN Ê®°Âûã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁ¥çÂÖ•‰∏ÄÂÄãÊ≥®ÊÑèÂäõÊ©üÂà∂Ôºå‰ª•ËÄÉÈáè‰∏çÂêåÂàáÁâáÂú®È†êÊ∏¨ÈÅéÁ®ã‰∏≠ÁöÑÈáçË¶ÅÊÄßÂ∑ÆÁï∞„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑Ê®°ÂûãÁöÑÊïàËÉΩÂíåÁ©©ÂÅ•ÊÄßÔºåÊàëÂÄë‰ΩøÁî®Ê¨äÈáçÂÖ±‰∫´ÂêåÊôÇË®ìÁ∑¥ÂÖ©ÂÄãË≥áÊñôÈõÜÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁ®±ÁÇ∫ÂÖ±ÂêåË®ìÁ∑¥ÁöÑÊäÄË°ì„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÈ†êÂÖàÂú® ImageNet ‰∏äË®ìÁ∑¥ÈÅéÁöÑ 2D Ê®°ÂûãÂÑ™ÊñºÈ†êÂÖàÂú® Kinetics-400 ‰∏äË®ìÁ∑¥ÈÅéÁöÑ 3D Ê®°ÂûãÔºåËÄå‰ΩøÁî®Ê≥®ÊÑèÂäõÊ©üÂà∂ÁöÑÊ®°ÂûãÂâáÂÑ™Êñº 2D Âíå 3D Ê®°Âûã„ÄÇÁï∂ÂÖ±ÂêåË®ìÁ∑¥ÁöÑË≥áÊñôÈõÜÂ§†Â§ßÁöÑÊôÇÂÄôÔºåÂÖ±ÂêåË®ìÁ∑¥ÊäÄË°ìÂ∑≤Ë¢´Ë≠âÊòéËÉΩÊúâÊïàÊîπÂñÑÊ®°ÂûãÊïàËÉΩ„ÄÇ

##### **MS-Glance: Non-semantic context vectors and the applications in supervising image reconstruction**
2410.23577v1 by Ziqi Gao, Wendi Yang, Yujia Li, Lei Xing, S. Kevin Zhou

Non-semantic context information is crucial for visual recognition, as the
human visual perception system first uses global statistics to process scenes
rapidly before identifying specific objects. However, while semantic
information is increasingly incorporated into computer vision tasks such as
image reconstruction, non-semantic information, such as global spatial
structures, is often overlooked. To bridge the gap, we propose a biologically
informed non-semantic context descriptor, \textbf{MS-Glance}, along with the
Glance Index Measure for comparing two images. A Global Glance vector is
formulated by randomly retrieving pixels based on a perception-driven rule from
an image to form a vector representing non-semantic global context, while a
local Glance vector is a flattened local image window, mimicking a zoom-in
observation. The Glance Index is defined as the inner product of two
standardized sets of Glance vectors. We evaluate the effectiveness of
incorporating Glance supervision in two reconstruction tasks: image fitting
with implicit neural representation (INR) and undersampled MRI reconstruction.
Extensive experimental results show that MS-Glance outperforms existing image
restoration losses across both natural and medical images. The code is
available at \url{https://github.com/Z7Gao/MSGlance}.

ÊëòË¶ÅÔºöÈùûËØ≠‰πâ‰∏ä‰∏ãÊñá‰ø°ÊÅØÂØπ‰∫éËßÜËßâËØÜÂà´Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†‰∏∫‰∫∫Á±ªËßÜËßâÊÑüÁü•Á≥ªÁªüÈ¶ñÂÖà‰ΩøÁî®ÂÖ®Â±ÄÁªüËÆ°Êï∞ÊçÆÊù•Âø´ÈÄüÂ§ÑÁêÜÂú∫ÊôØÔºåÁÑ∂ÂêéÂÜçËØÜÂà´ÁâπÂÆöÂØπË±°„ÄÇÁÑ∂ËÄåÔºåËôΩÁÑ∂ËØ≠‰πâ‰ø°ÊÅØÊ≠£Ë∂äÊù•Ë∂äÂ§öÂú∞ËûçÂÖ•Âà∞ÂõæÂÉèÈáçÂª∫Á≠âËÆ°ÁÆóÊú∫ËßÜËßâ‰ªªÂä°‰∏≠Ôºå‰ΩÜÈùûËØ≠‰πâ‰ø°ÊÅØÔºàÂ¶ÇÂÖ®Â±ÄÁ©∫Èó¥ÁªìÊûÑÔºâÂç¥Â∏∏Â∏∏Ë¢´ÂøΩËßÜ„ÄÇ‰∏∫‰∫ÜÂº•ÂêàËøô‰∏ÄÂ∑ÆË∑ùÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁîüÁâ©‰ø°ÊÅØÂêØÂèëÁöÑÈùûËØ≠‰πâ‰∏ä‰∏ãÊñáÊèèËø∞Á¨¶ÔºåÂç≥ \textbf{MS-Glance}Ôºå‰ª•ÂèäÁî®‰∫éÊØîËæÉ‰∏§ÂπÖÂõæÂÉèÁöÑ Glance ÊåáÊï∞Â∫¶Èáè„ÄÇÈÄöËøáÊ†πÊçÆÊÑüÁü•È©±Âä®ÁöÑËßÑÂàô‰ªéÂõæÂÉè‰∏≠ÈöèÊú∫Ê£ÄÁ¥¢ÂÉèÁ¥†Êù•ÊûÑÂª∫‰∏Ä‰∏™ÂÖ®Â±Ä Glance ÂêëÈáèÔºå‰ª•ÂΩ¢Êàê‰∏Ä‰∏™Ë°®Á§∫ÈùûËØ≠‰πâÂÖ®Â±Ä‰∏ä‰∏ãÊñáÁöÑÂêëÈáèÔºåËÄåÂ±ÄÈÉ® Glance ÂêëÈáèÊòØ‰∏Ä‰∏™ÊâÅÂπ≥ÁöÑÂ±ÄÈÉ®ÂõæÂÉèÁ™óÂè£ÔºåÊ®°‰ªø‰∫ÜÊîæÂ§ßËßÇÂØü„ÄÇGlance ÊåáÊï∞Ë¢´ÂÆö‰πâ‰∏∫‰∏§ÁªÑÊ†áÂáÜÂåñÁöÑ Glance ÂêëÈáèÁöÑÂÜÖÁßØ„ÄÇÊàë‰ª¨ËØÑ‰º∞‰∫ÜÂú®‰∏§‰∏™ÈáçÂª∫‰ªªÂä°‰∏≠Á∫≥ÂÖ• Glance ÁõëÁù£ÁöÑÊúâÊïàÊÄßÔºöÂÖ∑ÊúâÈöêÂºèÁ•ûÁªèË°®ÂæÅ (INR) ÁöÑÂõæÂÉèÊãüÂêàÂíåÊ¨†ÈááÊ†∑ MRI ÈáçÂª∫„ÄÇÂ§ßÈáèÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMS-Glance Âú®Ëá™ÁÑ∂ÂõæÂÉèÂíåÂåªÂ≠¶ÂõæÂÉè‰∏≠ÈÉΩ‰ºò‰∫éÁé∞ÊúâÁöÑÂõæÂÉèÊÅ¢Â§çÊçüÂ§±„ÄÇ‰ª£Á†ÅÂèØÂú® \url{https://github.com/Z7Gao/MSGlance} Ëé∑Âæó„ÄÇ

##### **LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models**
2410.23526v1 by Hieu Tran, Junda Wang, Yujan Ting, Weijing Huang, Terrence Chen

Large language models (LLMs) have shown remarkable capabilities in various
natural language processing tasks, yet they often struggle with maintaining
factual accuracy, particularly in knowledge-intensive domains like healthcare.
This study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,
a novel approach designed to enhance the factual reliability of LLMs, with a
focus on medical question answering (QA). LEAF utilizes a dual strategy to
enhance the factual accuracy of responses from models such as Llama 3 70B
Instruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,
improves Retrieval-Augmented Generation (RAG) by incorporating fact-checking
results to guide the retrieval process without updating model parameters. The
second strategy, Learning from Fact-Checks via Self-Training, involves
supervised fine-tuning (SFT) on fact-checked responses or applying Simple
Preference Optimization (SimPO) with fact-checking as a ranking mechanism, both
updating LLM parameters from supervision. These findings suggest that
integrating fact-checked responses whether through RAG enhancement or
self-training enhances the reliability and factual correctness of LLM outputs,
offering a promising solution for applications where information accuracy is
crucial.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑËÉΩÂäõÔºåÁÑ∂ËÄåÂÆÉÂÄëÂú®Á∂≠ÊåÅ‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄßÊñπÈù¢Â∏∏Â∏∏Èù¢Ëá®Âõ∞Èõ£ÔºåÁâπÂà•ÊòØÂú®ÂÉèÈÜ´ÁôÇ‰øùÂÅ•ÈÄôÊ®£ÁöÑÁü•Ë≠òÂØÜÈõÜÈ†òÂüü„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫Ü LEAFÔºöÈÄèÈÅé‰∫ãÂØ¶Êü•Ê†∏Â¢ûÂº∑ÁöÑÂ≠∏ÁøíËàáË©ï‰º∞ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÊèêÂçá LLM ÁöÑ‰∫ãÂØ¶ÂèØÈù†ÊÄßÔºå‰∏¶Â∞àÊ≥®ÊñºÈÜ´ÁôÇÂïèÈ°åËß£Á≠î (QA)„ÄÇLEAF Âà©Áî®ÈõôÈáçÁ≠ñÁï•‰æÜÊèêÂçá LLM ÂõûÊáâÁöÑ‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄßÔºå‰æãÂ¶Ç Llama 3 70B Instruct Âíå Llama 3 8B Instruct„ÄÇÁ¨¨‰∏ÄÁ®ÆÁ≠ñÁï• Fact-Check-Then-RAGÔºåÈÄèÈÅéÊï¥Âêà‰∫ãÂØ¶Êü•Ê†∏ÁµêÊûú‰æÜÊîπÈÄ≤Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG)Ôºå‰ª•ÂºïÂ∞éÊ™¢Á¥¢Á®ãÂ∫èÔºåËÄå‰∏çÊúÉÊõ¥Êñ∞Ê®°ÂûãÂèÉÊï∏„ÄÇÁ¨¨‰∫åÁ®ÆÁ≠ñÁï•ÈÄèÈÅéËá™ÊàëË®ìÁ∑¥Â≠∏Áøí‰∫ãÂØ¶Êü•Ê†∏ÔºåÊ∂âÂèäÈáùÂ∞çÁ∂ìÈÅé‰∫ãÂØ¶Êü•Ê†∏ÁöÑÂõûÊáâÈÄ≤Ë°åÁõ£Áù£ÂæÆË™ø (SFT)ÔºåÊàñÂ∞áÁ∞°ÂñÆÂÅèÂ•ΩÊúÄ‰Ω≥Âåñ (SimPO) ÊáâÁî®Êñº‰∫ãÂØ¶Êü•Ê†∏‰ΩúÁÇ∫ÊéíÂêçÊ©üÂà∂ÔºåÈÄôÂÖ©Á®ÆÊñπÊ≥ïÈÉΩÊúÉÂæûÁõ£Áù£‰∏≠Êõ¥Êñ∞ LLM ÂèÉÊï∏„ÄÇÈÄô‰∫õÁôºÁèæË°®ÊòéÔºåÁÑ°Ë´ñÊòØÈÄèÈÅé RAG Â¢ûÂº∑ÊàñËá™ÊàëË®ìÁ∑¥ÔºåÊï¥ÂêàÁ∂ìÈÅé‰∫ãÂØ¶Êü•Ê†∏ÁöÑÂõûÊáâÔºåÈÉΩËÉΩÊèêÂçá LLM Ëº∏Âá∫ÁöÑÂèØÈù†ÊÄßÂíå‰∫ãÂØ¶Ê≠£Á¢∫ÊÄßÔºåÁÇ∫Ë≥áË®äÊ∫ñÁ¢∫ÊÄßËá≥ÈóúÈáçË¶ÅÁöÑÊáâÁî®Á®ãÂºèÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÊôØÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Emory Knee Radiograph (MRKR) Dataset**
2411.00866v1 by Brandon Price, Jason Adleberg, Kaesha Thomas, Zach Zaiman, Aawez Mansuri, Beatrice Brown-Mulry, Chima Okecheukwu, Judy Gichoya, Hari Trivedi

The Emory Knee Radiograph (MRKR) dataset is a large, demographically diverse
collection of 503,261 knee radiographs from 83,011 patients, 40% of which are
African American. This dataset provides imaging data in DICOM format along with
detailed clinical information, including patient-reported pain scores,
diagnostic codes, and procedural codes, which are not commonly available in
similar datasets. The MRKR dataset also features imaging metadata such as image
laterality, view type, and presence of hardware, enhancing its value for
research and model development. MRKR addresses significant gaps in existing
datasets by offering a more representative sample for studying osteoarthritis
and related outcomes, particularly among minority populations, thereby
providing a valuable resource for clinicians and researchers.

ÊëòË¶ÅÔºöÂüÉÈªòÈáåËÜùÈÉ® X ÂÖâÁâá (MRKR) Ë≥áÊñôÈõÜÊòØ‰∏ÄÂÄãÈæêÂ§ß„ÄÅ‰∫∫Âè£Áµ±Ë®àË≥áÊñôÂ§öÂÖÉÁöÑË≥áÊñôÈõÜÔºåÂåÖÂê´‰æÜËá™ 83,011 ÂêçÊÇ£ËÄÖÁöÑ 503,261 ÂºµËÜùÈÉ® X ÂÖâÁâáÔºåÂÖ∂‰∏≠ 40% ÁÇ∫ÈùûË£îÁæéÂúã‰∫∫„ÄÇÊ≠§Ë≥áÊñôÈõÜÊèê‰æõ DICOM Ê†ºÂºèÁöÑÂΩ±ÂÉèË≥áÊñôÔºå‰ª•ÂèäË©≥Á¥∞ÁöÑËá®Â∫äË≥áË®äÔºåÂåÖÊã¨ÊÇ£ËÄÖÂõûÂ†±ÁöÑÁñºÁóõË©ïÂàÜ„ÄÅË®∫Êñ∑Á¢ºÂíåÁ®ãÂ∫èÁ¢ºÔºåÈÄô‰∫õË≥áÊñôÂú®È°û‰ººÁöÑË≥áÊñôÈõÜ‰∏≠‰∏¶‰∏çÂ∏∏Ë¶ã„ÄÇMRKR Ë≥áÊñôÈõÜ‰πüÂåÖÂê´ÂΩ±ÂÉèÁöÑÂæåË®≠Ë≥áÊñôÔºå‰æãÂ¶ÇÂΩ±ÂÉèÁöÑÂ∑¶Âè≥ÂÅ¥„ÄÅÊ™¢Ë¶ñÈ°ûÂûãÂíåÁ°¨È´îÁöÑÂ≠òÂú®ÔºåÊèêÂçáÂÖ∂Âú®Á†îÁ©∂ÂíåÊ®°ÂûãÈñãÁôºÊñπÈù¢ÁöÑÂÉπÂÄº„ÄÇMRKR ÈÄèÈÅéÊèê‰æõÊõ¥ÂÖ∑‰ª£Ë°®ÊÄßÁöÑÊ®£Êú¨Ôºå‰æÜÊé¢Ë®éÈ™®ÈóúÁØÄÁÇéÂíåÁõ∏ÈóúÁµêÊûúÔºåÁâπÂà•ÊòØÂú®Â∞ëÊï∏ÊóèÁæ§‰∏≠ÔºåÂæûËÄåÂ°´Ë£úÁèæÊúâË≥áÊñôÈõÜ‰∏≠È°ØËëóÁöÑÁº∫Âè£ÔºåÁÇ∫Ëá®Â∫äÈÜ´ÁîüÂíåÁ†îÁ©∂‰∫∫Âì°Êèê‰æõÊúâÂÉπÂÄºÁöÑË≥áÊ∫ê„ÄÇ

##### **STIED: A deep learning model for the SpatioTemporal detection of focal Interictal Epileptiform Discharges with MEG**
2410.23386v1 by Raquel Fern√°ndez-Mart√≠n, Alfonso Gij√≥n, Odile Feys, Elodie Juven√©, Alec Aeby, Charline Urbain, Xavier De Ti√®ge, Vincent Wens

Magnetoencephalography (MEG) allows the non-invasive detection of interictal
epileptiform discharges (IEDs). Clinical MEG analysis in epileptic patients
traditionally relies on the visual identification of IEDs, which is time
consuming and partially subjective. Automatic, data-driven detection methods
exist but show limited performance. Still, the rise of deep learning (DL)-with
its ability to reproduce human-like abilities-could revolutionize clinical MEG
practice. Here, we developed and validated STIED, a simple yet powerful
supervised DL algorithm combining two convolutional neural networks with
temporal (1D time-course) and spatial (2D topography) features of MEG signals
inspired from current clinical guidelines. Our DL model enabled both temporal
and spatial localization of IEDs in patients suffering from focal epilepsy with
frequent and high amplitude spikes (FE group), with high-performance
metrics-accuracy, specificity, and sensitivity all exceeding 85%-when learning
from spatiotemporal features of IEDs. This performance can be attributed to our
handling of input data, which mimics established clinical MEG practice. Reverse
engineering further revealed that STIED encodes fine spatiotemporal features of
IEDs rather than their mere amplitude. The model trained on the FE group also
showed promising results when applied to a separate group of presurgical
patients with different types of refractory focal epilepsy, though further work
is needed to distinguish IEDs from physiological transients. This study paves
the way of incorporating STIED and DL algorithms into the routine clinical MEG
evaluation of epilepsy.

ÊëòË¶ÅÔºöËÖ¶Á£ÅÂúñÔºàMEGÔºâÂÖÅË®±Â∞çÁôº‰ΩúÈñìÊúüÁô≤ÁôáÊ®£ÊîæÈõªÔºàIEDÔºâÈÄ≤Ë°åÈùû‰æµÂÖ•ÊÄßÊ™¢Ê∏¨„ÄÇÁô≤ÁôáÊÇ£ËÄÖÁöÑËá®Â∫ä MEG ÂàÜÊûêÂÇ≥Áµ±‰∏ä‰æùË≥¥Êñº IED ÁöÑË¶ñË¶∫Ë≠òÂà•ÔºåÈÄôÊó¢ËÄóÊôÇÂèàÈÉ®ÂàÜ‰∏ªËßÄ„ÄÇËá™ÂãïÂåñ„ÄÅÊï∏ÊìöÈ©ÖÂãïÁöÑÊ™¢Ê∏¨ÊñπÊ≥ïÂ≠òÂú®Ôºå‰ΩÜÈ°ØÁ§∫ÊÄßËÉΩÊúâÈôê„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÊ∑±Â∫¶Â≠∏Áøí (DL) ÁöÑËààËµ∑‚Äî‚ÄîÂÆÉÂÖ∑ÊúâË§áË£ΩÈ°û‰∫∫ËÉΩÂäõÁöÑËÉΩÂäõ‚Äî‚ÄîÂèØ‰ª•ÂæπÂ∫ïÊîπËÆäËá®Â∫ä MEG ÂØ¶Ë∏ê„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëÈñãÁôº‰∏¶È©óË≠â‰∫Ü STIEDÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁ∞°ÂñÆ‰ΩÜÂº∑Â§ßÁöÑÁõ£Áù£Âºè DL ÊºîÁÆóÊ≥ïÔºåÂÆÉÁµêÂêà‰∫ÜÂÖ©ÂÄãÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂÖ∑Êúâ MEG Ë®äËôüÁöÑÊôÇÈñìÔºà1D ÊôÇÈñìÈÅéÁ®ãÔºâÂíåÁ©∫ÈñìÔºà2D Âú∞ÂΩ¢ÔºâÁâπÂæµÔºåÈùàÊÑü‰æÜËá™Áï∂ÂâçÁöÑËá®Â∫äÊåáÂçó„ÄÇÊàëÂÄëÁöÑ DL Ê®°ÂûãËÉΩÂ§†Â∞çÊÇ£ÊúâÂ±ÄÁÅ∂ÊÄßÁô≤Áôá‰∏îÂ∞ñÂ≥∞È†ªÁπÅ‰∏îÊåØÂπÖÈ´òÁöÑÊÇ£ËÄÖÔºàFE ÁµÑÔºâ‰∏≠ÁöÑ IED ÈÄ≤Ë°åÊôÇÈñìÂíåÁ©∫ÈñìÂÆö‰ΩçÔºå‰∏¶ÂÖ∑ÊúâÈ´òÊÄßËÉΩÊåáÊ®ô‚Äî‚ÄîÊ∫ñÁ¢∫Â∫¶„ÄÅÁâπÁï∞ÊÄßÂíåÊïèÊÑüÊÄßÂùáË∂ÖÈÅé 85%‚Äî‚ÄîÂæû IED ÁöÑÊôÇÁ©∫ÁâπÂæµ‰∏≠Â≠∏Áøí„ÄÇÈÄôÁ®ÆÊÄßËÉΩÂèØ‰ª•Ê≠∏Âõ†ÊñºÊàëÂÄëÂ∞çËº∏ÂÖ•Ë≥áÊñôÁöÑËôïÁêÜÔºåÂÆÉÊ®°Êì¨‰∫ÜÊó¢ÂÆöÁöÑËá®Â∫ä MEG ÂØ¶Âãô„ÄÇÈÄÜÂêëÂ∑•Á®ãÈÄ≤‰∏ÄÊ≠•Êè≠Á§∫ STIED Á∑®Á¢º‰∫Ü IED ÁöÑÁ≤æÁ¥∞ÊôÇÁ©∫ÁâπÂæµÔºåËÄå‰∏çÊòØÂÆÉÂÄëÁöÑÂñÆÁ¥îÊåØÂπÖ„ÄÇÂú® FE ÁµÑ‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÂú®ÊáâÁî®ÊñºÂè¶‰∏ÄÁµÑÊÇ£Êúâ‰∏çÂêåÈ°ûÂûãÈõ£Ê≤ªÊÄßÂ±ÄÁÅ∂ÊÄßÁô≤ÁôáÁöÑË°ìÂâçÊÇ£ËÄÖÊôÇ‰πüÈ°ØÁ§∫Âá∫ÊúâÂ∏åÊúõÁöÑÁµêÊûúÔºåÂÑòÁÆ°ÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•ÁöÑÂ∑•‰Ωú‰æÜÂçÄÂàÜ IED ÂíåÁîüÁêÜÊÄßÊö´ÊÖã„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁÇ∫Â∞á STIED Âíå DL ÊºîÁÆóÊ≥ïÁ¥çÂÖ•Áô≤ÁôáÁöÑÂ∏∏Ë¶èËá®Â∫ä MEG Ë©ï‰º∞Èã™Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ

##### **Larger models yield better results? Streamlined severity classification of ADHD-related concerns using BERT-based knowledge distillation**
2411.00052v1 by Ahmed Akib Jawad Karim, Kazi Hafiz Md. Asad, Md. Golam Rabiul Alam

This work focuses on the efficiency of the knowledge distillation approach in
generating a lightweight yet powerful BERT based model for natural language
processing applications. After the model creation, we applied the resulting
model, LastBERT, to a real-world task classifying severity levels of Attention
Deficit Hyperactivity Disorder (ADHD)-related concerns from social media text
data. Referring to LastBERT, a customized student BERT model, we significantly
lowered model parameters from 110 million BERT base to 29 million, resulting in
a model approximately 73.64% smaller. On the GLUE benchmark, comprising
paraphrase identification, sentiment analysis, and text classification, the
student model maintained strong performance across many tasks despite this
reduction. The model was also used on a real-world ADHD dataset with an
accuracy and F1 score of 85%. When compared to DistilBERT (66M) and
ClinicalBERT (110M), LastBERT demonstrated comparable performance, with
DistilBERT slightly outperforming it at 87%, and ClinicalBERT achieving 86%
across the same metrics. These findings highlight the LastBERT model's capacity
to classify degrees of ADHD severity properly, so it offers a useful tool for
mental health professionals to assess and comprehend material produced by users
on social networking platforms. The study emphasizes the possibilities of
knowledge distillation to produce effective models fit for use in
resource-limited conditions, hence advancing NLP and mental health diagnosis.
Furthermore underlined by the considerable decrease in model size without
appreciable performance loss is the lower computational resources needed for
training and deployment, hence facilitating greater applicability. Especially
using readily available computational tools like Google Colab. This study shows
the accessibility and usefulness of advanced NLP methods in pragmatic world
applications.

ÊëòË¶ÅÔºö<paragraph>Êú¨Á†îÁ©∂ÈáçÈªûÂú®ÊñºÁü•Ë≠òËêÉÂèñÊñπÊ≥ïÂú®Áî¢ÁîüËºïÈáèÁ¥ö‰∏îÂº∑Â§ßÁöÑÂü∫Êñº BERT ÁöÑÊ®°Âûã‰ª•Áî®ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊáâÁî®ÊñπÈù¢ÁöÑÊïàÁéá„ÄÇÂú®Ê®°ÂûãÂª∫Á´ãÂæåÔºåÊàëÂÄëÂ∞áÁî¢ÁîüÁöÑÊ®°Âûã LastBERT ÊáâÁî®Êñº‰∏ÄÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑ‰ªªÂãôÔºåÂç≥ÂæûÁ§æÁæ§Â™íÈ´îÊñáÂ≠óË≥áÊñô‰∏≠ÂàÜÈ°ûÊ≥®ÊÑèÂäõ‰∏çË∂≥ÈÅéÂãïÁóá (ADHD) Áõ∏ÈóúÂïèÈ°åÁöÑÂö¥ÈáçÁ®ãÂ∫¶Â±§Á¥ö„ÄÇÊèêÂà∞ LastBERTÔºå‰∏ÄÂÄãÂÆ¢Ë£ΩÂåñÁöÑÂ≠∏Áîü BERT Ê®°ÂûãÔºåÊàëÂÄëÂ§ßÂπÖÈôç‰Ωé‰∫ÜÊ®°ÂûãÂèÉÊï∏ÔºåÂæû 1.1 ÂÑÑÂÄã BERT Âü∫Â∫ïÊ∏õÂ∞ëËá≥ 2900 Ëê¨ÂÄãÔºåÂ∞éËá¥Ê®°ÂûãÁ∏ÆÂ∞è‰∫ÜÂ§ßÁ¥Ñ 73.64%„ÄÇÂú® GLUE Âü∫Ê∫ñÔºåÂåÖÊã¨ÂêåÁæ©Âè•Ëæ®Ë≠ò„ÄÅÊÉÖÁ∑íÂàÜÊûêÂíåÊñáÂ≠óÂàÜÈ°ûÔºåÂÑòÁÆ°ÊúâÊ≠§Á∏ÆÊ∏õÔºåÂ≠∏ÁîüÊ®°ÂûãÂú®Ë®±Â§ö‰ªªÂãô‰∏≠‰ªçÁ∂≠ÊåÅÂº∑ÂãÅÁöÑË°®Áèæ„ÄÇÊ≠§Ê®°Âûã‰πüÁî®Êñº‰∏ÄÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑ ADHD Ë≥áÊñôÈõÜÔºåÂÖ∂Ê∫ñÁ¢∫Â∫¶Âíå F1 ÂàÜÊï∏ÁÇ∫ 85%„ÄÇËàá DistilBERT (66M) Âíå ClinicalBERT (110M) Áõ∏ËºÉÔºåLastBERT Ë°®ÁèæÂá∫ÂèØÊØîËºÉÁöÑË°®ÁèæÔºåDistilBERT ‰ª• 87% ÁöÑË°®ÁèæÁï•Âãù‰∏ÄÁ±åÔºåËÄå ClinicalBERT Âú®Áõ∏ÂêåÁöÑÊåáÊ®ô‰∏≠ÈÅîÂà∞ 86%„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫Ü LastBERT Ê®°ÂûãÈÅ©Áï∂Âú∞ÂàÜÈ°û ADHD Âö¥ÈáçÁ®ãÂ∫¶ÁöÑËÉΩÂäõÔºåÂõ†Ê≠§ÂÆÉÁÇ∫ÂøÉÁêÜÂÅ•Â∫∑Â∞àÊ•≠‰∫∫Âì°Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÁî®ÁöÑÂ∑•ÂÖ∑ÔºåÁî®ÊñºË©ï‰º∞ÂíåÁêÜËß£Á§æÁæ§Á∂≤Ë∑ØÂπ≥Âè∞‰∏ä‰ΩøÁî®ËÄÖÁî¢Âá∫ÁöÑË≥áÊñô„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÁü•Ë≠òËêÉÂèñÂú®Áî¢ÁîüÈÅ©Áî®ÊñºË≥áÊ∫êÊúâÈôêÊ¢ù‰ª∂ÁöÑÊúâÊïàÊ®°ÂûãÊñπÈù¢ÁöÑÂèØËÉΩÊÄßÔºåÂõ†Ê≠§‰øÉÈÄ≤‰∫Ü NLP ÂíåÂøÉÁêÜÂÅ•Â∫∑Ë®∫Êñ∑„ÄÇÊ≠§Â§ñÔºåÂú®Ê≤íÊúâÈ°ØËëóÊïàËÉΩÊêçÂ§±ÁöÑÊÉÖÊ≥Å‰∏ãÂ§ßÂπÖÁ∏ÆÂ∞èÊ®°ÂûãÂ§ßÂ∞èÔºå‰πüÁ™ÅÈ°Ø‰∫ÜË®ìÁ∑¥ÂíåÈÉ®ÁΩ≤ÊâÄÈúÄÁöÑËºÉ‰ΩéÈÅãÁÆóË≥áÊ∫êÔºåÂõ†Ê≠§‰øÉÈÄ≤‰∫ÜÊõ¥Âª£Ê≥õÁöÑÊáâÁî®ÊÄß„ÄÇÁâπÂà•ÊòØ‰ΩøÁî®ÁèæÊàêÁöÑÈÅãÁÆóÂ∑•ÂÖ∑Ôºå‰æãÂ¶Ç Google Colab„ÄÇÊú¨Á†îÁ©∂È°ØÁ§∫‰∫ÜÂÖàÈÄ≤ NLP ÊñπÊ≥ïÂú®ÂãôÂØ¶‰∏ñÁïåÊáâÁî®‰∏≠ÁöÑÂèØÂèäÊÄßÂíåÂØ¶Áî®ÊÄß„ÄÇ</paragraph>

##### **DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI and PET**
2410.23219v1 by Yitong Li, Morteza Ghahremani, Youssef Wally, Christian Wachinger

Diagnosing dementia, particularly for Alzheimer's Disease (AD) and
frontotemporal dementia (FTD), is complex due to overlapping symptoms. While
magnetic resonance imaging (MRI) and positron emission tomography (PET) data
are critical for the diagnosis, integrating these modalities in deep learning
faces challenges, often resulting in suboptimal performance compared to using
single modalities. Moreover, the potential of multi-modal approaches in
differential diagnosis, which holds significant clinical importance, remains
largely unexplored. We propose a novel framework, DiaMond, to address these
issues with vision Transformers to effectively integrate MRI and PET. DiaMond
is equipped with self-attention and a novel bi-attention mechanism that
synergistically combine MRI and PET, alongside a multi-modal normalization to
reduce redundant dependency, thereby boosting the performance. DiaMond
significantly outperforms existing multi-modal methods across various datasets,
achieving a balanced accuracy of 92.4% in AD diagnosis, 65.2% for AD-MCI-CN
classification, and 76.5% in differential diagnosis of AD and FTD. We also
validated the robustness of DiaMond in a comprehensive ablation study. The code
is available at https://github.com/ai-med/DiaMond.

ÊëòË¶ÅÔºöË®∫Êñ∑Â§±Êô∫ÁóáÔºåÂ∞§ÂÖ∂ÊòØÈòøËå≤Êµ∑ÈªòÁóá (AD) ÂíåÈ°çÈ°≥ËëâÂûãÂ§±Êô∫Áóá (FTD)ÔºåÁî±ÊñºÁóáÁãÄÈáçÁñäÔºåÂõ†Ê≠§ÂæàË§áÈõú„ÄÇÈõñÁÑ∂Á£ÅÂÖ±ÊåØÈÄ†ÂΩ± (MRI) ÂíåÊ≠£Â≠êÊñ∑Â±§ÊéÉÊèè (PET) Êï∏ÊìöÂ∞çÊñºË®∫Êñ∑Ëá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÂ∞áÈÄô‰∫õÊñπÂºèÊï¥ÂêàÂà∞Ê∑±Â∫¶Â≠∏Áøí‰∏≠ÊúÉÈù¢Ëá®ÊåëÊà∞ÔºåÈÄöÂ∏∏ÊúÉÂ∞éËá¥Ëàá‰ΩøÁî®ÂñÆ‰∏ÄÊñπÂºèÁõ∏ÊØîÊÄßËÉΩ‰∏ç‰Ω≥„ÄÇÊ≠§Â§ñÔºåÂ§öÊ®°ÂºèÊñπÊ≥ïÂú®ÈëëÂà•Ë®∫Êñ∑‰∏≠ÁöÑÊΩõÂäõÂÖ∑ÊúâÈáçË¶ÅÁöÑËá®Â∫äÊÑèÁæ©Ôºå‰ΩÜ‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÊ°ÜÊû∂ DiaMondÔºå‰ª•Ëß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºå‰ΩøÁî®Ë¶ñË¶∫ËΩâÊèõÂô®ÊúâÊïàÊï¥Âêà MRI Âíå PET„ÄÇDiaMond ÂÖ∑ÂÇôËá™Ê≥®ÊÑèÂäõÂíåÊñ∞Á©éÁöÑÈõôÊ≥®ÊÑèÂäõÊ©üÂà∂ÔºåÂèØ‰ª•ÂçîÂêåÁµêÂêà MRI Âíå PETÔºå‰∏¶Êé°Áî®Â§öÊ®°ÂºèÊ≠£Ë¶èÂåñ‰æÜÊ∏õÂ∞ëÂÜóÈ§ò‰æùË≥¥ÔºåÂæûËÄåÊèêÂçáÊÄßËÉΩ„ÄÇDiaMond Âú®ÂêÑÁ®ÆÊï∏ÊìöÈõÜ‰∏≠ÁöÑË°®ÁèæÊòéÈ°ØÂÑ™ÊñºÁèæÊúâÁöÑÂ§öÊ®°ÂºèÊñπÊ≥ïÔºåÂú® AD Ë®∫Êñ∑‰∏≠ÈÅîÂà∞ 92.4% ÁöÑÂπ≥Ë°°Ê∫ñÁ¢∫Â∫¶ÔºåÂú® AD-MCI-CN ÂàÜÈ°û‰∏≠ÈÅîÂà∞ 65.2%ÔºåÂú® AD Âíå FTD ÁöÑÈëëÂà•Ë®∫Êñ∑‰∏≠ÈÅîÂà∞ 76.5%„ÄÇÊàëÂÄëÈÇÑÂú®ÂÖ®Èù¢ÁöÑÊ∂àËûçÁ†îÁ©∂‰∏≠È©óË≠â‰∫Ü DiaMond ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/ai-med/DiaMond ÂèñÂæó„ÄÇ

##### **Variable Resolution Sampling and Deep Learning Image Recovery for Accelerated Multi-Spectral MRI Near Metal Implants**
2410.23329v1 by Azadeh Sharafi, Nikolai J. Mickevicius, Mehran Baboli, Andrew S. Nencka, Kevin M. Koch

Purpose: This study presents a variable resolution (VR) sampling and deep
learning reconstruction approach for multi-spectral MRI near metal implants,
aiming to reduce scan times while maintaining image quality. Background: The
rising use of metal implants has increased MRI scans affected by metal
artifacts. Multi-spectral imaging (MSI) reduces these artifacts but sacrifices
acquisition efficiency. Methods: This retrospective study on 1.5T MSI knee and
hip data from patients with metal hardware used a novel spectral undersampling
scheme to improve acquisition efficiency by ~40%. U-Net-based deep learning
models were trained for reconstruction. Image quality was evaluated using SSIM,
PSNR, and RESI metrics. Results: Deep learning reconstructions of undersampled
VR data (DL-VR) showed significantly higher SSIM and PSNR values (p<0.001)
compared to conventional reconstruction (CR-VR), with improved edge sharpness.
Edge sharpness in DL-reconstructed images matched fully sampled references
(p=0.5). Conclusion: This approach can potentially enhance MRI examinations
near metal implants by reducing scan times or enabling higher resolution.
Further prospective studies are needed to assess clinical value.

ÊëòË¶ÅÔºöÁõÆÁöÑÔºöÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÁßçÂèØÂèòÂàÜËæ®Áéá (VR) ÈááÊ†∑ÂíåÊ∑±Â∫¶Â≠¶‰π†ÈáçÂª∫ÊñπÊ≥ïÔºåÁî®‰∫éÈáëÂ±ûÊ§çÂÖ•Áâ©ÈôÑËøëÁöÑÂ§öÂàÜÂÖâ MRIÔºåÊó®Âú®Âú®‰øùÊåÅÂõæÂÉèË¥®ÈáèÁöÑÂêåÊó∂ÂáèÂ∞ëÊâ´ÊèèÊó∂Èó¥„ÄÇËÉåÊôØÔºöÈáëÂ±ûÊ§çÂÖ•Áâ©ÁöÑ‰ΩøÁî®Â¢ûÂä†ÔºåÂØºËá¥ÂèóÈáëÂ±û‰º™ÂΩ±ÂΩ±ÂìçÁöÑ MRI Êâ´ÊèèÂ¢ûÂä†„ÄÇÂ§öÂàÜÂÖâÊàêÂÉè (MSI) ÂáèÂ∞ë‰∫ÜËøô‰∫õ‰º™ÂΩ±Ôºå‰ΩÜÁâ∫Áâ≤‰∫ÜÈááÈõÜÊïàÁéá„ÄÇÊñπÊ≥ïÔºöËøôÈ°πÈíàÂØπ 1.5T MSI ËÜùÁõñÂíåÈ´ãÈÉ®Êï∞ÊçÆÁöÑÂõûÈ°æÊÄßÁ†îÁ©∂ÔºåÊù•Ëá™Ë£ÖÊúâÈáëÂ±ûÁ°¨‰ª∂ÁöÑÊÇ£ËÄÖÔºå‰ΩøÁî®‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂÖâË∞±Ê¨†ÈááÊ†∑ÊñπÊ°àÔºåÂ∞ÜÈááÈõÜÊïàÁéáÊèêÈ´ò‰∫ÜÁ∫¶ 40%„ÄÇÂü∫‰∫é U-Net ÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁªèËøáËÆ≠ÁªÉÁî®‰∫éÈáçÂª∫„ÄÇ‰ΩøÁî® SSIM„ÄÅPSNR Âíå RESI ÊåáÊ†áËØÑ‰º∞ÂõæÂÉèË¥®Èáè„ÄÇÁªìÊûúÔºöÊ¨†ÈááÊ†∑ VR Êï∞ÊçÆÁöÑÊ∑±Â∫¶Â≠¶‰π†ÈáçÂª∫ (DL-VR) ‰∏é‰º†ÁªüÈáçÂª∫ (CR-VR) Áõ∏ÊØîÔºåÊòæÁ§∫Âá∫ÊòéÊòæÊõ¥È´òÁöÑ SSIM Âíå PSNR ÂÄºÔºàp<0.001ÔºâÔºåÂπ∂ÊèêÈ´ò‰∫ÜËæπÁºòÊ∏ÖÊô∞Â∫¶„ÄÇDL ÈáçÂª∫ÂõæÂÉè‰∏≠ÁöÑËæπÁºòÊ∏ÖÊô∞Â∫¶‰∏éÂÆåÂÖ®ÈááÊ†∑ÁöÑÂèÇËÄÉÂÄºÁõ∏ÂåπÈÖçÔºàp=0.5Ôºâ„ÄÇÁªìËÆ∫ÔºöËøôÁßçÊñπÊ≥ïÂèØ‰ª•ÈÄöËøáÂáèÂ∞ëÊâ´ÊèèÊó∂Èó¥ÊàñÂêØÁî®Êõ¥È´òÂàÜËæ®ÁéáÊù•Â¢ûÂº∫ÈáëÂ±ûÊ§çÂÖ•Áâ©ÈôÑËøëÁöÑ MRI Ê£ÄÊü•„ÄÇÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÁöÑÂâçÁûªÊÄßÁ†îÁ©∂Êù•ËØÑ‰º∞‰∏¥Â∫ä‰ª∑ÂÄº„ÄÇ

##### **DiabML: AI-assisted diabetes diagnosis method with meta-heuristic-based feature selection**
2411.00858v1 by Vahideh Hayyolalam, √ñznur √ñzkasap

Diabetes is a chronic disorder identified by the high sugar level in the
blood that can cause various different disorders such as kidney failure, heart
attack, sightlessness, and stroke. Developments in the healthcare domain by
facilitating the early detection of diabetes risk can help not only caregivers
but also patients. AIoMT is a recent technology that integrates IoT and machine
learning methods to give services for medical purposes, which is a powerful
technology for the early detection of diabetes. In this paper, we take
advantage of AIoMT and propose a hybrid diabetes risk detection method, DiabML,
which uses the BWO algorithm and ML methods. BWO is utilized for feature
selection and SMOTE for imbalance handling in the pre-processing procedure. The
simulation results prove the superiority of the proposed DiabML method compared
to the existing works. DiabML achieves 86.1\% classification accuracy by
AdaBoost classifier outperforms the relevant existing methods.

ÊëòË¶ÅÔºöÁ≥ñÂ∞øÁóÖÊòØ‰∏ÄÁ®ÆÊÖ¢ÊÄßÁñæÁóÖÔºåÁâπÂæµÊòØË°ÄÊ∂≤‰∏≠ÁöÑÈ´òÁ≥ñÂàÜÔºåÂèØËÉΩÂ∞éËá¥ÂêÑÁ®Æ‰∏çÂêåÁöÑÁñæÁóÖÔºå‰æãÂ¶ÇËÖéË°∞Á´≠„ÄÅÂøÉËáüÁóÖÁôº‰Ωú„ÄÅÂ§±ÊòéÂíå‰∏≠È¢®„ÄÇÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑÁôºÂ±ïÈÄöÈÅé‰øÉÈÄ≤Êó©ÊúüÁôºÁèæÁ≥ñÂ∞øÁóÖÈ¢®Èö™Ôºå‰∏çÂÉÖÂèØ‰ª•Âπ´Âä©ÁÖßË≠∑ËÄÖÔºåÈÇÑÂèØ‰ª•Âπ´Âä©ÊÇ£ËÄÖ„ÄÇAIoMT ÊòØ‰∏ÄÁ®ÆÂ∞áÁâ©ËÅØÁ∂≤ÂíåÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÊï¥ÂêàÂú®‰∏ÄËµ∑ÁöÑÊñ∞ÊäÄË°ìÔºåÁî®ÊñºÊèê‰æõÈÜ´ÁôÇÁõÆÁöÑÁöÑÊúçÂãôÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®ÊñºÊó©ÊúüÁôºÁèæÁ≥ñÂ∞øÁóÖÁöÑÂº∑Â§ßÊäÄË°ì„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂà©Áî® AIoMT ‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ∑∑ÂêàÁ≥ñÂ∞øÁóÖÈ¢®Èö™Ê™¢Ê∏¨ÊñπÊ≥ï DiabMLÔºåÂÆÉ‰ΩøÁî® BWO ÊºîÁÆóÊ≥ïÂíå ML ÊñπÊ≥ï„ÄÇBWO Áî®ÊñºÈ†êËôïÁêÜÁ®ãÂ∫è‰∏≠ÁöÑÁâπÂæµÈÅ∏ÊìáÔºåËÄå SMOTE Áî®ÊñºËôïÁêÜ‰∏çÂπ≥Ë°°„ÄÇÊ®°Êì¨ÁµêÊûúË≠âÊòé‰∫ÜÊâÄÊèêÂá∫ÁöÑ DiabML ÊñπÊ≥ïÂÑ™ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇDiabML ÈÄöÈÅé AdaBoost ÂàÜÈ°ûÂô®ÂØ¶Áèæ‰∫Ü 86.1% ÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÔºåÂÑ™ÊñºÁõ∏ÈóúÁöÑÁèæÊúâÊñπÊ≥ï„ÄÇ

##### **Revisiting MAE pre-training for 3D medical image segmentation**
2410.23132v1 by Tassilo Wald, Constantin Ulrich, Stanislav Lukyanenko, Andrei Goncharov, Alberto Paderno, Leander Maerkisch, Paul F. J√§ger, Klaus Maier-Hein

Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the
potential of vast, untapped clinical datasets, for various downstream
applications that suffer from the scarcity of labeled data. While SSL has
revolutionized fields like natural language processing and computer vision,
their adoption in 3D medical image computing has been limited by three key
pitfalls: Small pre-training dataset sizes, architectures inadequate for 3D
medical image analysis, and insufficient evaluation practices. We address these
issues by i) leveraging a large-scale dataset of 44k 3D brain MRI volumes and
ii) using a Residual Encoder U-Net architecture within the state-of-the-art
nnU-Net framework. iii) A robust development framework, incorporating 5
development and 8 testing brain MRI segmentation datasets, allowed
performance-driven design decisions to optimize the simple concept of Masked
Auto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses
previous SSL methods but also outperforms the strong nnU-Net baseline by an
average of approximately 3 Dice points. Furthermore, our model demonstrates
exceptional stability, achieving the highest average rank of 2 out of 7
methods, compared to the second-best method's mean rank of 3.

ÊëòË¶ÅÔºöËá™ÁõëÁù£Â≠¶‰π† (SSL) ‰∏∫Ëß£ÈîÅÂ§ßÈáèÊú™ÂºÄÂèë‰∏¥Â∫äÊï∞ÊçÆÈõÜÁöÑÊΩúÂäõÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊøÄÂä®‰∫∫ÂøÉÁöÑÊú∫‰ºöÔºåÁî®‰∫éÂêÑÁßç‰∏ãÊ∏∏Â∫îÁî®Á®ãÂ∫èÔºåËøô‰∫õÂ∫îÁî®Á®ãÂ∫èÂõ†Ê†áËÆ∞Êï∞ÊçÆÁ®ÄÁº∫ËÄåÂèóÂà∞ÂΩ±Âìç„ÄÇËôΩÁÑ∂ SSL Â∑≤ÂΩªÂ∫ïÊîπÂèò‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆóÊú∫ËßÜËßâÁ≠âÈ¢ÜÂüüÔºå‰ΩÜÂÖ∂Âú® 3D ÂåªÂ≠¶ÂõæÂÉèËÆ°ÁÆó‰∏≠ÁöÑÈááÁî®ÂèóÂà∞‰∏â‰∏™‰∏ªË¶ÅÁº∫Èô∑ÁöÑÈôêÂà∂ÔºöÂ∞èÂûãÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂ§ßÂ∞è„ÄÅ‰∏çÈÄÇÁî®‰∫é 3D ÂåªÂ≠¶ÂõæÂÉèÂàÜÊûêÁöÑÊû∂ÊûÑ‰ª•ÂèäËØÑ‰º∞ÂÆûË∑µ‰∏çË∂≥„ÄÇÊàë‰ª¨ÈÄöËøá‰ª•‰∏ãÊñπÂºèËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºöi) Âà©Áî® 44k 3D Â§ßËÑë MRI ‰ΩìÁßØÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºå‰ª•Âèä ii) Âú®ÊúÄÂÖàËøõÁöÑ nnU-Net Ê°ÜÊû∂ÂÜÖ‰ΩøÁî®ÊÆãÂ∑ÆÁºñÁ†ÅÂô® U-Net Êû∂ÊûÑ„ÄÇiii) ‰∏Ä‰∏™Á®≥ÂÅ•ÁöÑÂºÄÂèëÊ°ÜÊû∂ÔºåÂåÖÂê´ 5 ‰∏™ÂºÄÂèëÂíå 8 ‰∏™ÊµãËØïÂ§ßËÑë MRI ÂàÜÂâ≤Êï∞ÊçÆÈõÜÔºåÂÖÅËÆ∏Âü∫‰∫éÊÄßËÉΩÁöÑËÆæËÆ°ÂÜ≥Á≠ñÊù•‰ºòÂåñ 3D CNN ÁöÑÊé©ËîΩËá™Âä®ÁºñÁ†ÅÂô® (MAE) ÁöÑÁÆÄÂçïÊ¶ÇÂøµ„ÄÇÁî±Ê≠§‰∫ßÁîüÁöÑÊ®°Âûã‰∏ç‰ªÖË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑ SSL ÊñπÊ≥ïÔºåËÄå‰∏îÊØîÂº∫Â§ßÁöÑ nnU-Net Âü∫Á∫øÂπ≥ÂùáÈ´òÂá∫Â§ßÁ∫¶ 3 ‰∏™È™∞Â≠êÁÇπ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãË°®Áé∞Âá∫ÈùûÂá°ÁöÑÁ®≥ÂÆöÊÄßÔºåÂú® 7 ÁßçÊñπÊ≥ï‰∏≠ËææÂà∞ 2 ÁöÑÊúÄÈ´òÂπ≥ÂùáÊéíÂêçÔºåËÄåÁ¨¨‰∫åÂ•ΩÁöÑÊñπÊ≥ïÁöÑÂπ≥ÂùáÊéíÂêç‰∏∫ 3„ÄÇ

##### **SpiroActive: Active Learning for Efficient Data Acquisition for Spirometry**
2410.22950v1 by Ankita Kumari Jain, Nitish Sharma, Madhav Kanda, Nipun Batra

Respiratory illnesses are a significant global health burden. Respiratory
illnesses, primarily Chronic obstructive pulmonary disease (COPD), is the
seventh leading cause of poor health worldwide and the third leading cause of
death worldwide, causing 3.23 million deaths in 2019, necessitating early
identification and diagnosis for effective mitigation. Among the diagnostic
tools employed, spirometry plays a crucial role in detecting respiratory
abnormalities. However, conventional clinical spirometry methods often entail
considerable costs and practical limitations like the need for specialized
equipment, trained personnel, and a dedicated clinical setting, making them
less accessible. To address these challenges, wearable spirometry technologies
have emerged as promising alternatives, offering accurate, cost-effective, and
convenient solutions. The development of machine learning models for wearable
spirometry heavily relies on the availability of high-quality ground truth
spirometry data, which is a laborious and expensive endeavor. In this research,
we propose using active learning, a sub-field of machine learning, to mitigate
the challenges associated with data collection and labeling. By strategically
selecting samples from the ground truth spirometer, we can mitigate the need
for resource-intensive data collection. We present evidence that models trained
on small subsets obtained through active learning achieve comparable/better
results than models trained on the complete dataset.

ÊëòË¶ÅÔºöÂëºÂê∏ÈÅìÁñæÁóÖÊòØÂÖ®ÁêÉÈáçÂ§ßÁöÑÂÅ•Â∫∑Ë≤†Êìî„ÄÇÂëºÂê∏ÈÅìÁñæÁóÖÔºå‰∏ªË¶ÅÊòØÊÖ¢ÊÄßÈòªÂ°ûÊÄßËÇ∫ÁóÖ (COPD)ÔºåÊòØÂÖ®ÁêÉÁ¨¨‰∏ÉÂ§ß‰∏çËâØÂÅ•Â∫∑ÂéüÂõ†Ôºå‰πüÊòØÂÖ®ÁêÉÁ¨¨‰∏âÂ§ßÊ≠ª‰∫°ÂéüÂõ†Ôºå2019 Âπ¥ÈÄ†Êàê 323 Ëê¨‰∫∫Ê≠ª‰∫°ÔºåÈúÄË¶ÅÂèäÊó©Ë≠òÂà•ÂíåË®∫Êñ∑‰ª•ÊúâÊïàÊ∏õËºïÁóáÁãÄ„ÄÇÂú®ÊâÄÊé°Áî®ÁöÑË®∫Êñ∑Â∑•ÂÖ∑‰∏≠ÔºåËÇ∫Ê¥ªÈáèÊ∏¨ÈáèÂú®Ê™¢Ê∏¨ÂëºÂê∏ÈÅìÁï∞Â∏∏ÊñπÈù¢ÁôºÊèÆËëóËá≥ÈóúÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑËá®Â∫äËÇ∫Ê¥ªÈáèÊ∏¨ÈáèÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÊàêÊú¨ÂíåÂØ¶ÈöõÈôêÂà∂Ôºå‰æãÂ¶ÇÈúÄË¶ÅÂ∞àÊ•≠Ë®≠ÂÇô„ÄÅË®ìÁ∑¥ÊúâÁ¥†ÁöÑ‰∫∫Âì°ÂíåÂ∞àÈñÄÁöÑËá®Â∫äÁí∞Â¢ÉÔºåÈÄô‰ΩøÂæóÂÆÉÂÄëÁöÑÂèØÂèäÊÄßËºÉ‰Ωé„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÂèØÁ©øÊà¥ÂºèËÇ∫Ê¥ªÈáèÊ∏¨ÈáèÊäÄË°ìÂ∑≤ÊàêÁÇ∫ÊúâÂ∏åÊúõÁöÑÊõø‰ª£ÊñπÊ°àÔºåÊèê‰æõÊ∫ñÁ¢∫„ÄÅÁ∂ìÊøüÈ´òÊïà‰∏î‰æøÂà©ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂèØÁ©øÊà¥ÂºèËÇ∫Ê¥ªÈáèÊ∏¨ÈáèÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÈñãÁôºÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÈ´òÂìÅË≥™ÁöÑÂü∫Ê∫ñËÇ∫Ê¥ªÈáèÊ∏¨ÈáèÊï∏ÊìöÔºåÈÄôÊòØ‰∏ÄÈ†ÖË≤ªÊôÇ‰∏îÊòÇË≤¥ÁöÑÂ∑•‰Ωú„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂª∫Ë≠∞‰ΩøÁî®‰∏ªÂãïÂ≠∏ÁøíÔºàÊ©üÂô®Â≠∏ÁøíÁöÑ‰∏ÄÂÄãÂ≠êÈ†òÂüüÔºâ‰æÜÊ∏õËºïËàáÊï∏ÊìöÊî∂ÈõÜÂíåÊ®ôË®òÁõ∏ÈóúÁöÑÊåëÊà∞„ÄÇÈÄöÈÅéÂæûÂü∫Ê∫ñËÇ∫Ê¥ªÈáèË®à‰∏≠Á≠ñÁï•ÊÄßÂú∞ÈÅ∏ÊìáÊ®£Êú¨ÔºåÊàëÂÄëÂèØ‰ª•Ê∏õÂ∞ëÂ∞çË≥áÊ∫êÂØÜÈõÜÂûãÊï∏ÊìöÊî∂ÈõÜÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÊèê‰æõÁöÑË≠âÊìöË°®ÊòéÔºåÂú®ÈÄöÈÅé‰∏ªÂãïÂ≠∏ÁøíÁç≤ÂæóÁöÑÂ∞èÂ≠êÈõÜ‰∏≠Ë®ìÁ∑¥ÁöÑÊ®°ÂûãÔºåÁç≤ÂæóÁöÑÁµêÊûúËàáÂú®ÂÆåÊï¥Êï∏ÊìöÈõÜ‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÁõ∏Áï∂/Êõ¥Â•Ω„ÄÇ

##### **Efficient Feature Extraction and Classification Architecture for MRI-Based Brain Tumor Detection**
2410.22619v1 by Plabon Paul, Md. Nazmul Islam, Fazle Rafsani, Pegah Khorasani, Shovito Barua Soumma

Uncontrolled cell division in the brain is what gives rise to brain tumors.
If the tumor size increases by more than half, there is little hope for the
patient's recovery. This emphasizes the need of rapid and precise brain tumor
diagnosis. When it comes to analyzing, diagnosing, and planning therapy for
brain tumors, MRI imaging plays a crucial role. A brain tumor's development
history is crucial information for doctors to have. When it comes to
distinguishing between human soft tissues, MRI scans are superior. In order to
get reliable classification results from MRI scans quickly, deep learning is
one of the most practical methods. Early human illness diagnosis has been
demonstrated to be more accurate when deep learning methods are used. In the
case of diagnosing a brain tumor, when even a little misdiagnosis might have
serious consequences, accuracy is especially important. Disclosure of brain
tumors in medical images is still a difficult task. Brain MRIs are notoriously
imprecise in revealing the presence or absence of tumors. Using MRI scans of
the brain, a Convolutional Neural Network (CNN) was trained to identify the
presence of a tumor in this research. Results from the CNN model showed an
accuracy of 99.17%. The CNN model's characteristics were also retrieved. In
order to evaluate the CNN model's capability for processing images, we applied
the features via the following machine learning models: KNN, Logistic
regression, SVM, Random Forest, Naive Bayes, and Perception. CNN and machine
learning models were also evaluated using the standard metrics of Precision,
Recall, Specificity, and F1 score. The significance of the doctor's diagnosis
enhanced the accuracy of the CNN model's assistance in identifying the
existence of tumor and treating the patient.

ÊëòË¶ÅÔºöËÖ¶ÈÉ®Á¥∞ËÉûÂàÜË£ÇÂ§±ÊéßÔºåÂ∞±ÊúÉÁî¢ÁîüËÖ¶Áò§„ÄÇ
Â¶ÇÊûúËÖ´Áò§Â§ßÂ∞èÂ¢ûÂä†Ë∂ÖÈÅé‰∏ÄÂçäÔºåÁóÖÊÇ£Â∫∑Âæ©ÁöÑÂ∏åÊúõÂæàÊ∏∫Ëå´„ÄÇÈÄôÂº∑Ë™ø‰∫ÜÂø´ÈÄü‰∏îÁ≤æÊ∫ñË®∫Êñ∑ËÖ¶Áò§ÁöÑÂøÖË¶ÅÊÄß„ÄÇ
Âú®ÂàÜÊûê„ÄÅË®∫Êñ∑ÂíåË¶èÂäÉËÖ¶Áò§Ê≤ªÁôÇÊôÇÔºåÊ†∏Á£ÅÂÖ±ÊåØÈÄ†ÂΩ±ÊâÆÊºî‰∫ÜËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇËÖ¶Áò§ÁöÑÁôºÂ±ïÂè≤ÊòØÈÜ´ÁîüÂøÖÂÇôÁöÑÈáçË¶ÅË≥áË®ä„ÄÇ
Âú®ÂçÄÂàÜ‰∫∫È´îËªüÁµÑÁπîÊôÇÔºåÊ†∏Á£ÅÂÖ±ÊåØÊéÉÊèèÁöÑË°®ÁèæÂÑ™Áï∞„ÄÇÁÇ∫‰∫ÜÂæûÊ†∏Á£ÅÂÖ±ÊåØÊéÉÊèè‰∏≠Âø´ÈÄüÂèñÂæóÂèØÈù†ÁöÑÂàÜÈ°ûÁµêÊûúÔºåÊ∑±Â∫¶Â≠∏ÁøíÊòØÊúÄÂØ¶Áî®ÁöÑÊñπÊ≥ï‰πã‰∏Ä„ÄÇ
Á†îÁ©∂È°ØÁ§∫Ôºå‰ΩøÁî®Ê∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÂèØ‰ª•Êõ¥Ê∫ñÁ¢∫Âú∞Ë®∫Êñ∑‰∫∫È°ûÊó©ÊúüÁñæÁóÖ„ÄÇÂú®Ë®∫Êñ∑ËÖ¶Áò§ÊôÇÔºåÂç≥‰ΩøÊòØËºïÂæÆÁöÑË™§Ë®∫ÈÉΩÂèØËÉΩÈÄ†ÊàêÂö¥ÈáçÂæåÊûúÔºåÂõ†Ê≠§Ê∫ñÁ¢∫ÊÄßÁâπÂà•ÈáçË¶Å„ÄÇ
Âú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠Êè≠Èú≤ËÖ¶Áò§‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖËâ±Èõ£ÁöÑ‰ªªÂãô„ÄÇËÖ¶ÈÉ®Ê†∏Á£ÅÂÖ±ÊåØÈÄ†ÂΩ±Âú®Êè≠Èú≤ËÖ´Áò§ÁöÑÂ≠òÂú®ËàáÂê¶ÊñπÈù¢Âá∫‰∫ÜÂêçÁöÑ‰∏çÁ≤æÁ¢∫„ÄÇ
Êú¨Á†îÁ©∂Ë®ìÁ∑¥‰∫Ü‰∏ÄÂÄãÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN)Ôºå‰ΩøÁî®ËÖ¶ÈÉ®Ê†∏Á£ÅÂÖ±ÊåØÊéÉÊèè‰æÜËæ®Ë≠òËÖ´Áò§ÁöÑÂ≠òÂú®„ÄÇCNN Ê®°ÂûãÁöÑÁµêÊûúÈ°ØÁ§∫Ê∫ñÁ¢∫Â∫¶ÁÇ∫ 99.17%„ÄÇCNN Ê®°ÂûãÁöÑÁâπÂæµ‰πüÂ∑≤Êì∑Âèñ„ÄÇ
ÁÇ∫‰∫ÜË©ï‰º∞ CNN Ê®°ÂûãËôïÁêÜÂΩ±ÂÉèÁöÑËÉΩÂäõÔºåÊàëÂÄëÈÄèÈÅé‰ª•‰∏ãÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂ•óÁî®ÈÄô‰∫õÁâπÂæµÔºöKNN„ÄÅÈÇèËºØËø¥Ê≠∏„ÄÅSVM„ÄÅÈö®Ê©üÊ£ÆÊûó„ÄÅÊ®∏Á¥†Ë≤ùÊ∞èÂíåÊÑüÁü•Âô®„ÄÇCNN ÂíåÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰πü‰ΩøÁî®Á≤æÊ∫ñÂ∫¶„ÄÅÂè¨ÂõûÁéá„ÄÅÁâπÁï∞ÊÄßÂíå F1 ÂàÜÊï∏Á≠âÊ®ôÊ∫ñÊåáÊ®ôÈÄ≤Ë°åË©ï‰º∞„ÄÇ
ÈÜ´ÁîüÁöÑË®∫Êñ∑ÊÑèÁæ©ÊèêÂçá‰∫Ü CNN Ê®°ÂûãÂú®ÂçîÂä©Ëæ®Ë≠òËÖ´Áò§Â≠òÂú®ÂíåÊ≤ªÁôÇÁóÖÊÇ£ÊñπÈù¢ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇ

##### **Do Large Language Models Align with Core Mental Health Counseling Competencies?**
2410.22446v1 by Viet Cuong Nguyen, Mohammad Taher, Dongwan Hong, Vinicius Konkolics Possobom, Vibha Thirunellayi Gopalakrishnan, Ekta Raj, Zihang Li, Heather J. Soled, Michael L. Birnbaum, Srijan Kumar, Munmun De Choudhury

The rapid evolution of Large Language Models (LLMs) offers promising
potential to alleviate the global scarcity of mental health professionals.
However, LLMs' alignment with essential mental health counseling competencies
remains understudied. We introduce CounselingBench, a novel NCMHCE-based
benchmark evaluating LLMs across five key mental health counseling
competencies. Testing 22 general-purpose and medical-finetuned LLMs, we find
frontier models exceed minimum thresholds but fall short of expert-level
performance, with significant variations: they excel in Intake, Assessment &
Diagnosis yet struggle with Core Counseling Attributes and Professional
Practice & Ethics. Medical LLMs surprisingly underperform generalist models
accuracy-wise, while at the same time producing slightly higher-quality
justifications but making more context-related errors. Our findings highlight
the complexities of developing AI systems for mental health counseling,
particularly for competencies requiring empathy and contextual understanding.
We found that frontier LLMs perform at a level exceeding the minimal required
level of aptitude for all key mental health counseling competencies, but fall
short of expert-level performance, and that current medical LLMs do not
significantly improve upon generalist models in mental health counseling
competencies. This underscores the critical need for specialized, mental health
counseling-specific fine-tuned LLMs that rigorously aligns with core
competencies combined with appropriate human supervision before any responsible
real-world deployment can be considered.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÁôºÂ±ïÔºåÊèê‰æõ‰∫ÜÁ∑©Ëß£ÂÖ®ÁêÉÂøÉÁêÜÂÅ•Â∫∑Â∞àÊ•≠‰∫∫Âì°Áü≠Áº∫ÁöÑÊΩõÂú®Â∏åÊúõ„ÄÇ
ÁÑ∂ËÄåÔºåLLM ËàáÂü∫Êú¨ÂøÉÁêÜÂÅ•Â∫∑Ë´ÆÂïÜËÉΩÂäõÁöÑÂ∞çÈΩäÁ®ãÂ∫¶Ôºå‰ªçÊú™Áç≤ÂæóÂÖÖÂàÜÁ†îÁ©∂„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü CounselingBenchÔºå‰∏ÄÂÄãÂü∫Êñº NCMHCE ÁöÑÊñ∞Âü∫Ê∫ñÔºåÁî®ÊñºË©ï‰º∞ LLM Âú®‰∫îÈ†ÖÈóúÈçµÂøÉÁêÜÂÅ•Â∫∑Ë´ÆÂïÜËÉΩÂäõ‰∏äÁöÑË°®Áèæ„ÄÇÊàëÂÄëÊ∏¨Ë©¶‰∫Ü 22 ÂÄãÈÄöÁî®ÂíåÈÜ´Â≠∏ÂæÆË™øÁöÑ LLMÔºåÁôºÁèæÂâçÊ≤øÊ®°ÂûãË∂ÖÈÅé‰∫ÜÊúÄ‰ΩéÈñÄÊ™ªÔºå‰ΩÜÊú™ÈÅîÂà∞Â∞àÂÆ∂Á¥öÂà•ÁöÑË°®ÁèæÔºå‰∏îÂ∑ÆÁï∞È°ØËëóÔºöÂÆÉÂÄëÂú®„ÄåÊîùÂèñ„ÄÅË©ï‰º∞ÂíåË®∫Êñ∑„ÄçÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂú®„ÄåÊ†∏ÂøÉË´ÆÂïÜÂ±¨ÊÄß„ÄçÂíå„ÄåÂ∞àÊ•≠ÂØ¶ÂãôÂíåÂÄ´ÁêÜ„ÄçÊñπÈù¢ÂçªÊúâÂõ∞Èõ£„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÈÜ´ÁôÇ LLM Âú®Ê∫ñÁ¢∫ÊÄßÊñπÈù¢Ë°®Áèæ‰∏çÂ¶ÇÈÄöÁî®Ê®°ÂûãÔºå‰ΩÜÂêåÊôÇÁî¢ÁîüÁöÑÁêÜÁî±ÂìÅË≥™Áï•È´òÔºå‰ΩÜÁî¢ÁîüÊõ¥Â§öËàáËÑàÁµ°Áõ∏ÈóúÁöÑÈåØË™§„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÁ™ÅÂá∫‰∫ÜÁÇ∫ÂøÉÁêÜÂÅ•Â∫∑Ë´ÆÂïÜÈñãÁôº AI Á≥ªÁµ±ÁöÑË§áÈõúÊÄßÔºåÁâπÂà•ÊòØÂ∞çÊñºÈúÄË¶ÅÂêåÁêÜÂøÉÂíåËÑàÁµ°ÁêÜËß£ÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁôºÁèæÔºåÂâçÊ≤ø LLM ÁöÑË°®ÁèæÊ∞¥Âπ≥Ë∂ÖÈÅé‰∫ÜÊâÄÊúâÈóúÈçµÂøÉÁêÜÂÅ•Â∫∑Ë´ÆÂïÜËÉΩÂäõÊâÄÈúÄÁöÑÊúÄ‰ΩéËÉΩÂäõÊ∞¥Ê∫ñÔºå‰ΩÜÊú™ÈÅîÂà∞Â∞àÂÆ∂Á¥öÂà•ÁöÑË°®ÁèæÔºåËÄå‰∏îÁõÆÂâçÁöÑÈÜ´ÁôÇ LLM ‰∏¶Êú™È°ØËëóÊîπÂñÑÈÄöÁî®Ê®°ÂûãÂú®ÂøÉÁêÜÂÅ•Â∫∑Ë´ÆÂïÜËÉΩÂäõ‰∏äÁöÑË°®Áèæ„ÄÇÈÄôÂº∑Ë™ø‰∫ÜÂ∞çÂ∞àÈñÄÁöÑ„ÄÅÈáùÂ∞çÂøÉÁêÜÂÅ•Â∫∑Ë´ÆË©¢ÁöÑÂæÆË™ø LLM ÁöÑËø´ÂàáÈúÄÊ±ÇÔºåÈÄô‰∫õ LLM ÂøÖÈ†àÂö¥Ê†ºÁ¨¶ÂêàÊ†∏ÂøÉËÉΩÂäõÔºå‰∏¶ÁµêÂêàÈÅ©Áï∂ÁöÑ‰∫∫È°ûÁõ£Áù£ÔºåÊâçËÉΩËÄÉÊÖÆ‰ªª‰ΩïË≤†Ë≤¨‰ªªÁöÑÂØ¶ÈöõÈÉ®ÁΩ≤„ÄÇ

##### **MAPUNetR: A Hybrid Vision Transformer and U-Net Architecture for Efficient and Interpretable Medical Image Segmentation**
2410.22223v1 by Ovais Iqbal Shah, Danish Raza Rizvi, Aqib Nazir Mir

Medical image segmentation is pivotal in healthcare, enhancing diagnostic
accuracy, informing treatment strategies, and tracking disease progression.
This process allows clinicians to extract critical information from visual
data, enabling personalized patient care. However, developing neural networks
for segmentation remains challenging, especially when preserving image
resolution, which is essential in detecting subtle details that influence
diagnoses. Moreover, the lack of transparency in these deep learning models has
slowed their adoption in clinical practice. Efforts in model interpretability
are increasingly focused on making these models' decision-making processes more
transparent. In this paper, we introduce MAPUNetR, a novel architecture that
synergizes the strengths of transformer models with the proven U-Net framework
for medical image segmentation. Our model addresses the resolution preservation
challenge and incorporates attention maps highlighting segmented regions,
increasing accuracy and interpretability. Evaluated on the BraTS 2020 dataset,
MAPUNetR achieved a dice score of 0.88 and a dice coefficient of 0.92 on the
ISIC 2018 dataset. Our experiments show that the model maintains stable
performance and potential as a powerful tool for medical image segmentation in
clinical practice.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåËÉΩÊèêÂçáË®∫Êñ∑Ê∫ñÁ¢∫Â∫¶„ÄÅÊèê‰æõÊ≤ªÁôÇÁ≠ñÁï•Ë≥áË®äÔºå‰∏¶ËøΩËπ§ÁñæÁóÖÈÄ≤Á®ã„ÄÇÊ≠§Á®ãÂ∫èËÆìËá®Â∫äÈÜ´ÁîüËÉΩÂæûË¶ñË¶∫Ë≥áÊñô‰∏≠ËêÉÂèñÈóúÈçµË≥áË®äÔºåÈÄ≤ËÄåÊèê‰æõÂÄã‰∫∫ÂåñÁöÑÊÇ£ËÄÖÁÖßË≠∑„ÄÇÁÑ∂ËÄåÔºåÈñãÁôºÁî®ÊñºÂàÜÂâ≤ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰ªçÂÖ∑ÊåëÊà∞ÊÄßÔºåÁâπÂà•ÊòØÂú®‰øùÁïôÂΩ±ÂÉèËß£ÊûêÂ∫¶ÊôÇÔºåÈÄôÂ∞çÊñºÂÅµÊ∏¨ÂΩ±ÈüøË®∫Êñ∑ÁöÑÁ¥∞ÂæÆÁ¥∞ÁØÄËá≥ÈóúÈáçË¶Å„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁº∫‰πèÈÄèÊòéÂ∫¶ÔºåÂ∞éËá¥ÂÖ∂Âú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑÊé°Áî®ÈÄüÂ∫¶ËÆäÊÖ¢„ÄÇÊ®°ÂûãÂèØËß£ÈáãÊÄßÁöÑÂä™ÂäõË∂ä‰æÜË∂äÂ∞àÊ≥®ÊñºËÆìÈÄô‰∫õÊ®°ÂûãÁöÑÊ±∫Á≠ñÈÅéÁ®ãÊõ¥ÈÄèÊòé„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü MAPUNetRÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊû∂ÊßãÔºåÁµêÂêà‰∫ÜTransformerÊ®°ÂûãÁöÑÂÑ™ÈªûÂíåÂ∑≤Ë≠âÂØ¶ÁöÑ U-Net Ê°ÜÊû∂ÔºåÁî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãËß£Ê±∫‰∫ÜËß£ÊûêÂ∫¶‰øùÁïôÁöÑÊåëÊà∞Ôºå‰∏¶ÁµêÂêà‰∫ÜÁ™ÅÈ°ØÂàÜÂâ≤ÂçÄÂüüÁöÑÊ≥®ÊÑèÂäõÂúñÔºåÊèêÈ´ò‰∫ÜÊ∫ñÁ¢∫Â∫¶ÂíåÂèØËß£ÈáãÊÄß„ÄÇÂú® BraTS 2020 Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåMAPUNetR Âú® ISIC 2018 Ë≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫Ü 0.88 ÁöÑÈ™∞Â≠ê‰øÇÊï∏Âíå 0.92 ÁöÑÈ™∞Â≠êÁ≥ªÊï∏„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåË©≤Ê®°ÂûãÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠‰ΩúÁÇ∫ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÁöÑÂº∑Â§ßÂ∑•ÂÖ∑ÔºåÂÖ∑ÊúâÁ©©ÂÆöÁöÑÊïàËÉΩÂíåÊΩõÂäõ„ÄÇ

##### **Natural Language Processing for Analyzing Electronic Health Records and Clinical Notes in Cancer Research: A Review**
2410.22180v1 by Muhammad Bilal, Ameer Hamza, Nadia Malik

Objective: This review aims to analyze the application of natural language
processing (NLP) techniques in cancer research using electronic health records
(EHRs) and clinical notes. This review addresses gaps in the existing
literature by providing a broader perspective than previous studies focused on
specific cancer types or applications. Methods: A comprehensive literature
search was conducted using the Scopus database, identifying 94 relevant studies
published between 2019 and 2024. Data extraction included study
characteristics, cancer types, NLP methodologies, dataset information,
performance metrics, challenges, and future directions. Studies were
categorized based on cancer types and NLP applications. Results: The results
showed a growing trend in NLP applications for cancer research, with breast,
lung, and colorectal cancers being the most studied. Information extraction and
text classification emerged as predominant NLP tasks. A shift from rule-based
to advanced machine learning techniques, particularly transformer-based models,
was observed. The Dataset sizes used in existing studies varied widely. Key
challenges included the limited generalizability of proposed solutions and the
need for improved integration into clinical workflows. Conclusion: NLP
techniques show significant potential in analyzing EHRs and clinical notes for
cancer research. However, future work should focus on improving model
generalizability, enhancing robustness in handling complex clinical language,
and expanding applications to understudied cancer types. Integration of NLP
tools into clinical practice and addressing ethical considerations remain
crucial for utilizing the full potential of NLP in enhancing cancer diagnosis,
treatment, and patient outcomes.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÊ®ôÔºöÊú¨ÁØáË©ïË´ñÊó®Âú®ÂàÜÊûêËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÊäÄË°ìÂú®ÁôåÁóáÁ†îÁ©∂‰∏≠‰ΩøÁî®ÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) ÂíåËá®Â∫äÁ≠ÜË®òÁöÑÊáâÁî®„ÄÇÊú¨ÁØáË©ïË´ñÈÄèÈÅéÊèê‰æõÊØîÂÖàÂâçÂ∞àÊ≥®ÊñºÁâπÂÆöÁôåÁóáÈ°ûÂûãÊàñÊáâÁî®ÁöÑÁ†îÁ©∂Êõ¥Âª£Ê≥õÁöÑËßÄÈªûÔºå‰æÜÊé¢Ë®éÁèæÊúâÊñáÁçª‰∏≠ÁöÑÂ∑ÆË∑ù„ÄÇÊñπÊ≥ïÔºö‰ΩøÁî® Scopus Ë≥áÊñôÂ∫´ÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÊñáÁçªÊêúÂ∞ãÔºåÊâæÂá∫ 2019 Âπ¥Ëá≥ 2024 Âπ¥ÈñìÁôºË°®ÁöÑ 94 ÁØáÁõ∏ÈóúÁ†îÁ©∂„ÄÇË≥áÊñôÊì∑ÂèñÂåÖÂê´Á†îÁ©∂ÁâπÂæµ„ÄÅÁôåÁóáÈ°ûÂûã„ÄÅNLP ÊñπÊ≥ïË´ñ„ÄÅË≥áÊñôÈõÜË≥áË®ä„ÄÅÊïàËÉΩÊåáÊ®ô„ÄÅÊåëÊà∞ÂíåÊú™‰æÜÊñπÂêë„ÄÇÁ†îÁ©∂Ê†πÊìöÁôåÁóáÈ°ûÂûãÂíå NLP ÊáâÁî®ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÁµêÊûúÔºöÁµêÊûúÈ°ØÁ§∫ NLP Âú®ÁôåÁóáÁ†îÁ©∂‰∏≠ÁöÑÊáâÁî®ÊúâÈÄêÊº∏Â¢ûÂä†ÁöÑË∂®Âã¢ÔºåÂÖ∂‰∏≠‰π≥Áôå„ÄÅËÇ∫ÁôåÂíåÂ§ßËÖ∏Áõ¥ËÖ∏ÁôåÁöÑÁ†îÁ©∂ÊúÄÂ§ö„ÄÇË≥áË®äÊì∑ÂèñÂíåÊñáÂ≠óÂàÜÈ°ûÊàêÁÇ∫‰∏ªË¶ÅÁöÑ NLP ‰ªªÂãô„ÄÇËßÄÂØüÂà∞ÂæûÂü∫ÊñºË¶èÂâáÁöÑÊäÄË°ìËΩâÁßªÂà∞ÈÄ≤ÈöéÊ©üÂô®Â≠∏ÁøíÊäÄË°ìÔºåÁâπÂà•ÊòØÂü∫ÊñºËΩâÊèõÂô®ÁöÑÊ®°Âûã„ÄÇÁèæÊúâÁ†îÁ©∂‰∏≠‰ΩøÁî®ÁöÑË≥áÊñôÈõÜÂ§ßÂ∞èÂ∑ÆÁï∞ÂæàÂ§ß„ÄÇ‰∏ªË¶ÅÁöÑÊåëÊà∞ÂåÖÊã¨ÊâÄÊèêÂá∫Ëß£Ê±∫ÊñπÊ°àÁöÑÊôÆÈÅçÊÄßÊúâÈôêÔºå‰ª•ÂèäÈúÄË¶ÅÊõ¥ÈÄ≤‰∏ÄÊ≠•Êï¥ÂêàÂà∞Ëá®Â∫äÂ∑•‰ΩúÊµÅÁ®ã‰∏≠„ÄÇÁµêË´ñÔºöNLP ÊäÄË°ìÂú®ÂàÜÊûêÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑÂíåËá®Â∫äÁ≠ÜË®ò‰ª•ÈÄ≤Ë°åÁôåÁóáÁ†îÁ©∂ÊñπÈù¢È°ØÁ§∫Âá∫È°ØËëóÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÊú™‰æÜÁöÑÁ†îÁ©∂ÊáâÂ∞àÊ≥®ÊñºÊîπÂñÑÊ®°ÂûãÁöÑÊôÆÈÅçÊÄß„ÄÅÂä†Âº∑ËôïÁêÜË§áÈõúËá®Â∫äË™ûË®ÄÁöÑÁ©©ÂÅ•ÊÄßÔºå‰ª•ÂèäÂ∞áÊáâÁî®Êì¥Â±ïÂà∞Á†îÁ©∂‰∏çË∂≥ÁöÑÁôåÁóáÈ°ûÂûã„ÄÇÂ∞á NLP Â∑•ÂÖ∑Êï¥ÂêàÂà∞Ëá®Â∫äÂØ¶Âãô‰∏≠Ôºå‰∏¶Êé¢Ë®éÂÄ´ÁêÜËÄÉÈáèÔºåÂ∞çÊñºÂÖÖÂàÜÂà©Áî® NLP Âú®ÊèêÂçáÁôåÁóáË®∫Êñ∑„ÄÅÊ≤ªÁôÇÂíåÊÇ£ËÄÖÈ†êÂæåÊñπÈù¢ÁöÑÊΩõÂäõËá≥ÈóúÈáçË¶Å„ÄÇ</paragraph>

##### **Advanced Hybrid Deep Learning Model for Enhanced Classification of Osteosarcoma Histopathology Images**
2411.00832v1 by Arezoo Borji, Gernot Kronreif, Bernhard Angermayr, Sepideh Hatamikia

Recent advances in machine learning are transforming medical image analysis,
particularly in cancer detection and classification. Techniques such as deep
learning, especially convolutional neural networks (CNNs) and vision
transformers (ViTs), are now enabling the precise analysis of complex
histopathological images, automating detection, and enhancing classification
accuracy across various cancer types. This study focuses on osteosarcoma (OS),
the most common bone cancer in children and adolescents, which affects the long
bones of the arms and legs. Early and accurate detection of OS is essential for
improving patient outcomes and reducing mortality. However, the increasing
prevalence of cancer and the demand for personalized treatments create
challenges in achieving precise diagnoses and customized therapies. We propose
a novel hybrid model that combines convolutional neural networks (CNN) and
vision transformers (ViT) to improve diagnostic accuracy for OS using
hematoxylin and eosin (H&E) stained histopathological images. The CNN model
extracts local features, while the ViT captures global patterns from
histopathological images. These features are combined and classified using a
Multi-Layer Perceptron (MLP) into four categories: non-tumor (NT), non-viable
tumor (NVT), viable tumor (VT), and none-viable ratio (NVR). Using the Cancer
Imaging Archive (TCIA) dataset, the model achieved an accuracy of 99.08%,
precision of 99.10%, recall of 99.28%, and an F1-score of 99.23%. This is the
first successful four-class classification using this dataset, setting a new
benchmark in OS research and offering promising potential for future diagnostic
advancements.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏ÁøíÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊ≠£Âú®ËΩâËÆäÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÔºåÁâπÂà•ÊòØÂú®ÁôåÁóáÊ™¢Ê∏¨ÂíåÂàÜÈ°ûÊñπÈù¢„ÄÇË´∏Â¶ÇÊ∑±Â∫¶Â≠∏ÁøíÁ≠âÊäÄË°ìÔºåÂ∞§ÂÖ∂ÊòØÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåË¶ñË¶∫ËΩâÊèõÂô® (ViT)ÔºåÁèæÂú®ËÉΩÁ≤æÁ¢∫ÂàÜÊûêË§áÈõúÁöÑÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉè„ÄÅËá™ÂãïÂåñÊ™¢Ê∏¨Ôºå‰∏¶ÊèêÂçáÂêÑÁ®ÆÁôåÁóáÈ°ûÂûãÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶„ÄÇÊú¨Á†îÁ©∂Â∞àÊ≥®ÊñºÈ™®ËÇâÁò§ (OS)ÔºåÈÄôÊòØÂÖíÁ´•ÂíåÈùíÂ∞ëÂπ¥‰∏≠ÊúÄÂ∏∏Ë¶ãÁöÑÈ™®ÁôåÔºåÊúÉÂΩ±ÈüøÊâãËáÇÂíåËÖøÈÉ®ÁöÑÈï∑È™®„ÄÇÊó©Êúü‰∏îÊ∫ñÁ¢∫Âú∞Ê™¢Ê∏¨Âá∫È™®ËÇâÁò§Â∞çÊñºÊîπÂñÑÊÇ£ËÄÖÈ†êÂæåÂíåÈôç‰ΩéÊ≠ª‰∫°ÁéáËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁôåÁóáÁõõË°åÁéáÁöÑÂ¢ûÂä†ÂíåÂ∞çÂÄã‰∫∫ÂåñÊ≤ªÁôÇÁöÑÈúÄÊ±ÇÔºåÂú®ÈÅîÊàêÁ≤æÁ¢∫Ë®∫Êñ∑ÂíåÂÆ¢Ë£ΩÂåñÊ≤ªÁôÇÊñπÈù¢ÈÄ†Êàê‰∫ÜÊåëÊà∞„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁµêÂêàÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåË¶ñË¶∫ËΩâÊèõÂô® (ViT) ÁöÑÊñ∞ÂûãÊ∑∑ÂêàÊ®°ÂûãÔºå‰ª•‰ΩøÁî®ËòáÊú®Á≤æÂíåÊõôÁ¥Ö (H&E) ÊüìËâ≤ÁöÑÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉè‰æÜÊèêÂçáÈ™®ËÇâÁò§ÁöÑË®∫Êñ∑Ê∫ñÁ¢∫Â∫¶„ÄÇCNN Ê®°ÂûãÊúÉËêÉÂèñÂ±ÄÈÉ®ÁâπÂæµÔºåËÄå ViT ÂâáÂæûÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉè‰∏≠Êì∑ÂèñÂÖ®Â±ÄÊ®°Âºè„ÄÇÈÄô‰∫õÁâπÂæµÊúÉÁµêÂêàËµ∑‰æÜÔºå‰∏¶‰ΩøÁî®Â§öÂ±§ÊÑüÁü•Âô® (MLP) ÂàÜÈ°ûÊàêÂõõÁ®ÆÈ°ûÂà•ÔºöÈùûËÖ´Áò§ (NT)„ÄÅ‰∏çÂèØÂ≠òÊ¥ªËÖ´Áò§ (NVT)„ÄÅÂèØÂ≠òÊ¥ªËÖ´Áò§ (VT) Âíå‰∏çÂèØÂ≠òÊ¥ªÁéá (NVR)„ÄÇ‰ΩøÁî®ÁôåÁóáÂΩ±ÂÉèÊ™îÊ°à (TCIA) Ë≥áÊñôÈõÜÔºåË©≤Ê®°ÂûãÈÅîÂà∞‰∫Ü 99.08% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÅ99.10% ÁöÑÁ≤æÁ¢∫Â∫¶„ÄÅ99.28% ÁöÑÂè¨ÂõûÁéáÂíå 99.23% ÁöÑ F1 ÂÄº„ÄÇÈÄôÊòØ‰ΩøÁî®Ê≠§Ë≥áÊñôÈõÜÈÄ≤Ë°åÁöÑÈ¶ñÊ¨°ÊàêÂäüÁöÑÂõõÈ°ûÂà•ÂàÜÈ°ûÔºåÁÇ∫È™®ËÇâÁò§Á†îÁ©∂Ë®≠ÂÆö‰∫ÜÊñ∞ÁöÑÂü∫Ê∫ñÔºå‰∏¶ÁÇ∫Êú™‰æÜÁöÑË®∫Êñ∑ÈÄ≤Â±ïÊèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÊΩõÂäõ„ÄÇ

##### **Unsupervised Training of a Dynamic Context-Aware Deep Denoising Framework for Low-Dose Fluoroscopic Imaging**
2411.00830v1 by Sun-Young Jeon, Sen Wang, Adam S. Wang, Garry E. Gold, Jang-Hwan Choi

Fluoroscopy is critical for real-time X-ray visualization in medical imaging.
However, low-dose images are compromised by noise, potentially affecting
diagnostic accuracy. Noise reduction is crucial for maintaining image quality,
especially given such challenges as motion artifacts and the limited
availability of clean data in medical imaging. To address these issues, we
propose an unsupervised training framework for dynamic context-aware denoising
of fluoroscopy image sequences. First, we train the multi-scale recurrent
attention U-Net (MSR2AU-Net) without requiring clean data to address the
initial noise. Second, we incorporate a knowledge distillation-based
uncorrelated noise suppression module and a recursive filtering-based
correlated noise suppression module enhanced with motion compensation to
further improve motion compensation and achieve superior denoising performance.
Finally, we introduce a novel approach by combining these modules with a
pixel-wise dynamic object motion cross-fusion matrix, designed to adapt to
motion, and an edge-preserving loss for precise detail retention. To validate
the proposed method, we conducted extensive numerical experiments on medical
image datasets, including 3500 fluoroscopy images from dynamic phantoms (2,400
images for training, 1,100 for testing) and 350 clinical images from a spinal
surgery patient. Moreover, we demonstrated the robustness of our approach
across different imaging modalities by testing it on the publicly available
2016 Low Dose CT Grand Challenge dataset, using 4,800 images for training and
1,136 for testing. The results demonstrate that the proposed approach
outperforms state-of-the-art unsupervised algorithms in both visual quality and
quantitative evaluation while achieving comparable performance to
well-established supervised learning methods across low-dose fluoroscopy and CT
imaging.

ÊëòË¶ÅÔºö<paragraph>Ëû¢ÂÖâÈÄèË¶ñÂ∞çÊñºÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÂç≥ÊôÇ X ÂÖâË¶ñË¶∫ÂåñËá≥ÈóúÈáçË¶Å„ÄÇ
ÁÑ∂ËÄåÔºå‰ΩéÂäëÈáèÂΩ±ÂÉèÊúÉÂèóÂà∞ÈõúË®äÂΩ±ÈüøÔºåÂèØËÉΩÂΩ±ÈüøË®∫Êñ∑Ê∫ñÁ¢∫ÊÄß„ÄÇÈõúË®äÊäëÂà∂Â∞çÊñºÁ∂≠ÊåÅÂΩ±ÂÉèÂìÅË≥™Ëá≥ÈóúÈáçË¶ÅÔºåÁâπÂà•ÊòØÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠Â≠òÂú®ÈÅãÂãïÂÅΩÂΩ±Âíå‰πæÊ∑®Ë≥áÊñôÊúâÈôêÁ≠âÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁÑ°Áõ£Áù£Ë®ìÁ∑¥Êû∂ÊßãÔºåÁî®ÊñºËû¢ÂÖâÈÄèË¶ñÂΩ±ÂÉèÂ∫èÂàóÁöÑÂãïÊÖãÊÉÖÂ¢ÉÊÑüÁü•ÂéªÈõúË®ä„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëË®ìÁ∑¥Â§öÂ∞∫Â∫¶ÈÅûËø¥Ê≥®ÊÑèÂäõ U-Net (MSR2AU-Net)ÔºåÁÑ°ÈúÄ‰πæÊ∑®Ë≥áÊñôÂç≥ÂèØËôïÁêÜÂàùÂßãÈõúË®ä„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÁµêÂêà‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÁü•Ë≠òËí∏È§æÁöÑÈùûÁõ∏ÈóúÈõúË®äÊäëÂà∂Ê®°ÁµÑÂíå‰∏ÄÂÄãÂü∫ÊñºÈÅûËø¥ÊøæÊ≥¢ÁöÑÁõ∏ÈóúÈõúË®äÊäëÂà∂Ê®°ÁµÑÔºå‰∏¶Â¢ûÂº∑‰∫ÜÈÅãÂãïË£úÂÑüÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•ÊîπÂñÑÈÅãÂãïË£úÂÑü‰∏¶ÂØ¶ÁèæÂçìË∂äÁöÑÂéªÈõúË®äÊïàËÉΩ„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÂ∞áÈÄô‰∫õÊ®°ÁµÑËàáÈÄêÂÉèÁ¥†ÂãïÊÖãÁâ©‰ª∂ÈÅãÂãï‰∫§ÂèâËûçÂêàÁü©Èô£ÁµêÂêàËµ∑‰æÜÔºåË©≤Áü©Èô£Êó®Âú®ÈÅ©ÊáâÈÅãÂãïÔºå‰∏¶Êé°Áî®ÈÇäÁ∑£‰øùÁïôÊêçÂ§±‰ª•Á≤æÁ¢∫‰øùÁïôÁ¥∞ÁØÄ„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºåÊàëÂÄëÂ∞çÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÊï∏ÂÄºÂØ¶È©óÔºåÂåÖÊã¨‰æÜËá™ÂãïÊÖãÊ®°Êì¨‰∫∫È´îÁöÑ 3500 ÂºµËû¢ÂÖâÈÄèË¶ñÂΩ±ÂÉèÔºà2,400 ÂºµÁî®ÊñºË®ìÁ∑¥Ôºå1,100 ÂºµÁî®ÊñºÊ∏¨Ë©¶ÔºâÂíå‰æÜËá™ËÑäÊ§éÊâãË°ìÊÇ£ËÄÖÁöÑ 350 ÂºµËá®Â∫äÂΩ±ÂÉè„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄèÈÅéÂú®ÂÖ¨ÈñãÁöÑ 2016 Âπ¥‰ΩéÂäëÈáè CT Â§ßÊåëÊà∞Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶Ôºå‰ΩøÁî® 4,800 ÂºµÂΩ±ÂÉèÈÄ≤Ë°åË®ìÁ∑¥Âíå 1,136 ÂºµÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®‰∏çÂêåÂΩ±ÂÉèÊ®°Âºè‰∏ãÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Ë¶ñË¶∫ÂìÅË≥™ÂíåÈáèÂåñË©ï‰º∞‰∏≠ÈÉΩÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÁÑ°Áõ£Áù£ÊºîÁÆóÊ≥ïÔºåÂêåÊôÇÂú®‰ΩéÂäëÈáèËû¢ÂÖâÈÄèË¶ñÂíå CT ÂΩ±ÂÉè‰∏≠ÂØ¶Áèæ‰∫ÜËàáÂÆåÂñÑÁöÑÁõ£Áù£ÂºèÂ≠∏ÁøíÊñπÊ≥ïÁõ∏Áï∂ÁöÑÊïàËÉΩ„ÄÇ</paragraph>

##### **Coupling quantum-like cognition with the neuronal networks within generalized probability theory**
2411.00036v1 by Andrei Khrennikov, Masanao Ozawa, Felix Benninger, Oded Shor

The recent years are characterized by intensive applications of the
methodology and mathematical apparatus of quantum theory, quantum-like
modeling, in cognition, psychology, and decision making. In spite of the
successful applications of this approach to a variety of psychological effects,
e.g., the order, conjunction, disjunction, and response replicability effects,
one may (but need not) feel dissatisfaction due to the absence of clear
coupling to the neurophysiological processes in the brain. For the moment, this
is just a phenomenological approach. In this paper we construct the
quantum-like representation of the networks of communicating neurons. It is
based not on standard quantum theory, but on generalized probability theory
(GPT) with the emphasis of the operational measurement approach. We employ
GPT's version which is based on ordered linear state space (instead of complex
Hilbert space). A network of communicating neurons is described as a weighted
ordered graph that in turn is encoded by its weight matrix. The state space of
weight matrices is embedded in GPT with effect-observables and state updates
within measurement instruments theory. The latter plays the crucial role. This
GPT based model shows the basic quantum-like effects, as e.g. the order,
non-repeatability, and disjunction effects; the latter is also known as
interference of decisions. This GPT coupling also supports quantum-like
modeling in medical diagnostic for neurological diseases, as depression and
epilepsy. Although the paper is concentrated on cognition and neuronal
networks, the formalism and methodology can be straightforwardly applied to a
variety of biological and social networks.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÈáèÂ≠êÁêÜË´ñ„ÄÅÈ°ûÈáèÂ≠êÊ®°ÂûãÂú®Ë™çÁü•„ÄÅÂøÉÁêÜÂ≠∏ÂíåÊ±∫Á≠ñÂà∂ÂÆö‰∏≠ÁöÑÊñπÊ≥ïË´ñÂíåÊï∏Â≠∏Ë£ùÁΩÆÂæóÂà∞Âª£Ê≥õÊáâÁî®„ÄÇÂÑòÁÆ°ÈÄôÁ®ÆÊñπÊ≥ïÊàêÂäüÊáâÁî®ÊñºÂêÑÁ®ÆÂøÉÁêÜÊïàÊáâÔºå‰æãÂ¶ÇÈ†ÜÂ∫è„ÄÅÂêàÂèñ„ÄÅÊûêÂèñÂíåÂèçÊáâÂèØË§áË£ΩÊïàÊáâÔºå‰ΩÜÁî±ÊñºÁº∫‰πèËàáÂ§ßËÖ¶Á•ûÁ∂ìÁîüÁêÜÈÅéÁ®ãÁöÑÊòéÁ¢∫ËÅØÁπ´Ôºå‰∫∫ÂÄëÂèØËÉΩÊúÉÔºà‰ΩÜ‰∏çÂøÖÔºâÊÑüÂà∞‰∏çÊªø„ÄÇÁõÆÂâçÔºåÈÄôÂè™ÊòØ‰∏ÄÁ®ÆÁèæË±°Â≠∏ÊñπÊ≥ï„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊßãÂª∫‰∫ÜÈÄö‰ø°Á•ûÁ∂ìÂÖÉÁ∂≤Ë∑ØÁöÑÈ°ûÈáèÂ≠êË°®Á§∫„ÄÇÂÆÉ‰∏çÊòØÂü∫ÊñºÊ®ôÊ∫ñÈáèÂ≠êÁêÜË´ñÔºåËÄåÊòØÂü∫ÊñºÂª£Áæ©Ê¶ÇÁéáË´ñ (GPT)Ôºå‰∏¶Âº∑Ë™øÈÅãÁÆóÊ∏¨ÈáèÊñπÊ≥ï„ÄÇÊàëÂÄëÊé°Áî®Âü∫ÊñºÊúâÂ∫èÁ∑öÊÄßÁãÄÊÖãÁ©∫ÈñìÔºàËÄå‰∏çÊòØË§áÈõúÂ∏åÁàæ‰ºØÁâπÁ©∫ÈñìÔºâÁöÑ GPT ÁâàÊú¨„ÄÇÈÄö‰ø°Á•ûÁ∂ìÂÖÉÁ∂≤Ë∑ØË¢´ÊèèËø∞ÁÇ∫‰∏ÄÂÄãÂä†Ê¨äÊúâÂ∫èÂúñÔºåËÄåÂä†Ê¨äÊúâÂ∫èÂúñÂèàÁî±ÂÖ∂Ê¨äÈáçÁü©Èô£Á∑®Á¢º„ÄÇÊ¨äÈáçÁü©Èô£ÁöÑÁãÄÊÖãÁ©∫ÈñìÂµåÂÖ• GPT ‰∏≠ÔºåÂÖ∂‰∏≠ÊïàÊáâËßÄÊ∏¨ÂÄºÂíåÁãÄÊÖãÊõ¥Êñ∞Âú®Ê∏¨ÈáèÂÑÄÂô®ÁêÜË´ñ‰∏≠„ÄÇÂæåËÄÖÁôºÊèÆËëóËá≥ÈóúÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇÈÄôÂÄãÂü∫Êñº GPT ÁöÑÊ®°ÂûãÂ±ïÁ§∫‰∫ÜÂü∫Êú¨ÁöÑÈ°ûÈáèÂ≠êÊïàÊáâÔºå‰æãÂ¶ÇÈ†ÜÂ∫è„ÄÅ‰∏çÂèØÈáçË§áÊÄßÂíåÊûêÂèñÊïàÊáâÔºõÂæåËÄÖ‰πüË¢´Á®±ÁÇ∫Ê±∫Á≠ñÂπ≤Êìæ„ÄÇÈÄôÁ®Æ GPT ËÄ¶ÂêàÈÇÑÊîØÊè¥Á•ûÁ∂ìÁñæÁóÖÔºàÂ¶ÇÊäëÈ¨±ÁóáÂíåÁô≤ÁôáÁóáÔºâÁöÑÈÜ´ÁôÇË®∫Êñ∑‰∏≠ÁöÑÈ°ûÈáèÂ≠êÂª∫Ê®°„ÄÇÂÑòÁÆ°Êú¨ÊñáÈõÜ‰∏≠ÊñºË™çÁü•ÂíåÁ•ûÁ∂ìÂÖÉÁ∂≤Ë∑ØÔºå‰ΩÜÂΩ¢Âºè‰∏ªÁæ©ÂíåÊñπÊ≥ïË´ñÂèØ‰ª•Áõ¥Êé•ÊáâÁî®ÊñºÂêÑÁ®ÆÁîüÁâ©ÂíåÁ§æÊúÉÁ∂≤Ë∑Ø„ÄÇ

##### **Advancing Efficient Brain Tumor Multi-Class Classification -- New Insights from the Vision Mamba Model in Transfer Learning**
2410.21872v2 by Yinyi Lai, Anbo Cao, Yuan Gao, Jiaqi Shang, Zongyu Li, Jia Guo

Early and accurate diagnosis of brain tumors is crucial for improving patient
survival rates. However, the detection and classification of brain tumors are
challenging due to their diverse types and complex morphological
characteristics. This study investigates the application of pre-trained models
for brain tumor classification, with a particular focus on deploying the Mamba
model. We fine-tuned several mainstream transfer learning models and applied
them to the multi-class classification of brain tumors. By comparing these
models to those trained from scratch, we demonstrated the significant
advantages of transfer learning, especially in the medical imaging field, where
annotated data is often limited. Notably, we introduced the Vision Mamba (Vim),
a novel network architecture, and applied it for the first time in brain tumor
classification, achieving exceptional classification accuracy. Experimental
results indicate that the Vim model achieved 100% classification accuracy on an
independent test set, emphasizing its potential for tumor classification tasks.
These findings underscore the effectiveness of transfer learning in brain tumor
classification and reveal that, compared to existing state-of-the-art models,
the Vim model is lightweight, efficient, and highly accurate, offering a new
perspective for clinical applications. Furthermore, the framework proposed in
this study for brain tumor classification, based on transfer learning and the
Vision Mamba model, is broadly applicable to other medical imaging
classification problems.

ÊëòË¶ÅÔºöËÖ¶Áò§ÁöÑÊó©ÊúüÊ∫ñÁ¢∫Ë®∫Êñ∑Â∞çÊñºÊèêÈ´òÊÇ£ËÄÖÂ≠òÊ¥ªÁéáËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºËÖ¶Áò§Á®ÆÈ°ûÁπÅÂ§ö‰∏îÂΩ¢ÊÖãÁâπÂæµË§áÈõúÔºåÂõ†Ê≠§Ê™¢Ê∏¨ÂíåÂàÜÈ°ûËÖ¶Áò§ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÈ†êË®ìÁ∑¥Ê®°ÂûãÂú®ËÖ¶Áò§ÂàÜÈ°û‰∏≠ÁöÑÊáâÁî®ÔºåÁâπÂà•ÈóúÊ≥® Mamba Ê®°ÂûãÁöÑÈÉ®ÁΩ≤„ÄÇÊàëÂÄëÂæÆË™ø‰∫ÜÂπæÂÄã‰∏ªÊµÅÁöÑÈÅ∑ÁßªÂ≠∏ÁøíÊ®°ÂûãÔºå‰∏¶Â∞áÂÆÉÂÄëÊáâÁî®ÊñºËÖ¶Áò§ÁöÑÂ§öÈ°ûÂà•ÂàÜÈ°û„ÄÇÈÄöÈÅéÂ∞áÈÄô‰∫õÊ®°ÂûãËàáÂæûÈ†≠ÈñãÂßãË®ìÁ∑¥ÁöÑÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉÔºåÊàëÂÄëË≠âÊòé‰∫ÜÈÅ∑ÁßªÂ≠∏ÁøíÁöÑÈ°ØËëóÂÑ™Âã¢ÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇÂΩ±ÂÉèÈ†òÂüüÔºåÂÖ∂‰∏≠Ë®ªÈáãÊï∏ÊìöÈÄöÂ∏∏ÊúâÈôê„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü Vision Mamba (Vim)Ôºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÁ∂≤Ë∑ØÊû∂ÊßãÔºå‰∏¶È¶ñÊ¨°Â∞áÂÖ∂ÊáâÁî®ÊñºËÖ¶Áò§ÂàÜÈ°ûÔºåÈÅîÂà∞‰∫ÜÂá∫Ëâ≤ÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåVim Ê®°ÂûãÂú®Áç®Á´ãÊ∏¨Ë©¶ÈõÜ‰∏äÂØ¶Áèæ‰∫Ü 100% ÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÔºåÂº∑Ë™ø‰∫ÜÂÖ∂Âú®ËÖ´Áò§ÂàÜÈ°û‰ªªÂãô‰∏≠ÁöÑÊΩõÂäõ„ÄÇÈÄô‰∫õÁôºÁèæÂº∑Ë™ø‰∫ÜÈÅ∑ÁßªÂ≠∏ÁøíÂú®ËÖ¶Áò§ÂàÜÈ°û‰∏≠ÁöÑÊúâÊïàÊÄßÔºå‰∏¶Êè≠Á§∫ËàáÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤Ê®°ÂûãÁõ∏ÊØîÔºåVim Ê®°ÂûãËºïÈáè„ÄÅÈ´òÊïà‰∏îÊ∫ñÁ¢∫Â∫¶È´òÔºåÁÇ∫Ëá®Â∫äÊáâÁî®Êèê‰æõ‰∫ÜÊñ∞ÁöÑË¶ñËßí„ÄÇÊ≠§Â§ñÔºåÊú¨Á†îÁ©∂‰∏≠ÊèêÂá∫ÁöÑÂü∫ÊñºÈÅ∑ÁßªÂ≠∏ÁøíÂíå Vision Mamba Ê®°ÂûãÁöÑËÖ¶Áò§ÂàÜÈ°ûÊ°ÜÊû∂Âª£Ê≥õÈÅ©Áî®ÊñºÂÖ∂‰ªñÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûÂïèÈ°å„ÄÇ

##### **How Does Critical Batch Size Scale in Pre-training?**
2410.21676v1 by Hanlin Zhang, Depen Morwani, Nikhil Vyas, Jingfeng Wu, Difan Zou, Udaya Ghai, Dean Foster, Sham Kakade

Training large-scale models under given resources requires careful design of
parallelism strategies. In particular, the efficiency notion of critical batch
size, concerning the compromise between time and compute, marks the threshold
beyond which greater data parallelism leads to diminishing returns. To
operationalize it, we propose a measure of CBS and pre-train a series of
auto-regressive language models, ranging from 85 million to 1.2 billion
parameters, on the C4 dataset. Through extensive hyper-parameter sweeps and
careful control on factors such as batch size, momentum, and learning rate
along with its scheduling, we systematically investigate the impact of scale on
CBS. Then we fit scaling laws with respect to model and data sizes to decouple
their effects. Overall, our results demonstrate that CBS scales primarily with
data size rather than model size, a finding we justify theoretically through
the analysis of infinite-width limits of neural networks and
infinite-dimensional least squares regression. Of independent interest, we
highlight the importance of common hyper-parameter choices and strategies for
studying large-scale pre-training beyond fixed training durations.

ÊëòË¶ÅÔºöÂú®Êó¢ÂÆöË≥áÊ∫ê‰∏ãË®ìÁ∑¥Â§ßÂûãÊ®°ÂûãÈúÄË¶Å‰ªîÁ¥∞Ë®≠Ë®àÂπ≥Ë°åËôïÁêÜÁ≠ñÁï•„ÄÇÁâπÂà•ÊòØÔºåÈóúÈçµÊâπÊ¨°Â§ßÂ∞èÁöÑÊïàÁéáÊ¶ÇÂøµÔºåÊ∂âÂèäÊôÇÈñìÂíåÈÅãÁÆó‰πãÈñìÁöÑÊäòË°∑ÔºåÊ®ôË™åËëóË∂ÖË∂äÊ≠§Ëá®ÁïåÈªûÂæåÔºåÊõ¥Â§ßÁöÑË≥áÊñôÂπ≥Ë°åËôïÁêÜÂ∞áÂ∞éËá¥Â†±ÈÖ¨ÈÅûÊ∏õ„ÄÇÁÇ∫‰∫ÜÂ∞áÂÖ∂‰ªòË´∏ÂØ¶ÊñΩÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã CBS ÈáèÂ∫¶Ôºå‰∏¶È†êÂÖàË®ìÁ∑¥‰∏ÄÁ≥ªÂàóËá™Ëø¥Ê≠∏Ë™ûË®ÄÊ®°ÂûãÔºåÁØÑÂúçÂæû 8500 Ëê¨Âà∞ 12 ÂÑÑÂÄãÂèÉÊï∏ÔºåÂú® C4 Ë≥áÊñôÈõÜ‰∏ä„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑË∂ÖÂèÉÊï∏ÊéÉÊèèÂíå‰ªîÁ¥∞ÊéßÂà∂ÊâπÊ¨°Â§ßÂ∞è„ÄÅÂãïÈáèÂíåÂ≠∏ÁøíÁéáÁ≠âÂõ†Á¥†‰ª•ÂèäÂÖ∂ÊéíÁ®ãÔºåÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Á†îÁ©∂Ë¶èÊ®°Â∞ç CBS ÁöÑÂΩ±Èüø„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊì¨ÂêàÈóúÊñºÊ®°ÂûãÂíåË≥áÊñôÂ§ßÂ∞èÁöÑÁ∏ÆÊîæÂÆöÂæãÔºå‰ª•ÂàÜÈõ¢ÂÆÉÂÄëÁöÑÂΩ±Èüø„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÁµêÊûúË°®Êòé CBS ‰∏ªË¶ÅÈö®ËëóË≥áÊñôÂ§ßÂ∞èËÄå‰∏çÊòØÊ®°ÂûãÂ§ßÂ∞èËÄåÁ∏ÆÊîæÔºåÊàëÂÄëÈÄèÈÅéÂ∞çÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÁÑ°ÈôêÂØ¨Â∫¶ÈôêÂà∂ÂíåÁÑ°ÈôêÁ∂≠ÊúÄÂ∞è‰∫å‰πòËø¥Ê≠∏ÁöÑÂàÜÊûêÔºåÂú®ÁêÜË´ñ‰∏äË≠âÊòé‰∫ÜÈÄô‰∏ÄÁôºÁèæ„ÄÇÁç®Á´ãÁöÑËààË∂£ÊòØÔºåÊàëÂÄëÂº∑Ë™ø‰∫ÜÈÄöÁî®Ë∂ÖÂèÉÊï∏ÈÅ∏ÊìáÂíåÁ≠ñÁï•ÁöÑÈáçË¶ÅÊÄßÔºåÁî®ÊñºÁ†îÁ©∂Ë∂ÖË∂äÂõ∫ÂÆöË®ìÁ∑¥ÊåÅÁ∫åÊôÇÈñìÁöÑÂ§ßË¶èÊ®°È†êË®ìÁ∑¥„ÄÇ

##### **A Tutorial on Clinical Speech AI Development: From Data Collection to Model Validation**
2410.21640v1 by Si-Ioi Ng, Lingfeng Xu, Ingo Siegert, Nicholas Cummins, Nina R. Benway, Julie Liss, Visar Berisha

There has been a surge of interest in leveraging speech as a marker of health
for a wide spectrum of conditions. The underlying premise is that any
neurological, mental, or physical deficits that impact speech production can be
objectively assessed via automated analysis of speech. Recent advances in
speech-based Artificial Intelligence (AI) models for diagnosing and tracking
mental health, cognitive, and motor disorders often use supervised learning,
similar to mainstream speech technologies like recognition and verification.
However, clinical speech AI has distinct challenges, including the need for
specific elicitation tasks, small available datasets, diverse speech
representations, and uncertain diagnostic labels. As a result, application of
the standard supervised learning paradigm may lead to models that perform well
in controlled settings but fail to generalize in real-world clinical
deployments. With translation into real-world clinical scenarios in mind, this
tutorial paper provides an overview of the key components required for robust
development of clinical speech AI. Specifically, this paper will cover the
design of speech elicitation tasks and protocols most appropriate for different
clinical conditions, collection of data and verification of hardware,
development and validation of speech representations designed to measure
clinical constructs of interest, development of reliable and robust clinical
prediction models, and ethical and participant considerations for clinical
speech AI. The goal is to provide comprehensive guidance on building models
whose inputs and outputs link to the more interpretable and clinically
meaningful aspects of speech, that can be interrogated and clinically validated
on clinical datasets, and that adhere to ethical, privacy, and security
considerations by design.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÂá∫Áèæ‰∏ÄËÇ°Âà©Áî®Ë™ûË®Ä‰ΩúÁÇ∫ÂêÑÁ®ÆÁñæÁóÖÊ®ôË®òÁöÑÁÜ±ÊΩÆ„ÄÇÂÖ∂Âü∫Êú¨ÂâçÊèêÊòØ‰ªª‰ΩïÂΩ±ÈüøË™ûË®ÄÁî¢ÁîüÁöÑÁ•ûÁ∂ì„ÄÅÂøÉÁêÜÊàñÁîüÁêÜÁº∫Èô∑ÔºåÈÉΩÂèØ‰ª•ÈÄèÈÅéË™ûË®ÄÁöÑËá™ÂãïÂåñÂàÜÊûêÈÄ≤Ë°åÂÆ¢ËßÄË©ï‰º∞„ÄÇÊúÄËøëÂú®Ë™ûË®ÄÂü∫Á§é‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°Âûã‰∏äÁöÑÈÄ≤Â±ïÔºåÁî®ÊñºË®∫Êñ∑ÂíåËøΩËπ§ÂøÉÁêÜÂÅ•Â∫∑„ÄÅË™çÁü•ÂíåÈÅãÂãïÈöúÁ§ôÔºåÈÄöÂ∏∏‰ΩøÁî®Áõ£Áù£ÂºèÂ≠∏ÁøíÔºåÈ°û‰ººÊñº‰∏ªÊµÅË™ûË®ÄÊäÄË°ìÔºå‰æãÂ¶ÇËæ®Ë≠òÂíåÈ©óË≠â„ÄÇÁÑ∂ËÄåÔºåËá®Â∫äË™ûË®Ä AI ÊúâÂÖ∂Áç®ÁâπÁöÑÊåëÊà∞ÔºåÂåÖÊã¨ÈúÄË¶ÅÁâπÂÆöÁöÑÂºïÂ∞é‰ªªÂãô„ÄÅÂèØÁî®ÁöÑË≥áÊñôÈõÜÂ∞è„ÄÅË™ûË®ÄË°®Ëø∞Â§öÊ®£Ôºå‰ª•ÂèäË®∫Êñ∑Ê®ôÁ±§‰∏çÁ¢∫ÂÆö„ÄÇÂõ†Ê≠§ÔºåÊáâÁî®Ê®ôÊ∫ñÁöÑÁõ£Áù£ÂºèÂ≠∏ÁøíÁØÑ‰æãÂèØËÉΩÊúÉÂ∞éËá¥Âú®ÂèóÊéßÁí∞Â¢É‰∏≠Ë°®ÁèæËâØÂ•ΩÁöÑÊ®°ÂûãÔºå‰ΩÜÂú®ÁèæÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÈÉ®ÁΩ≤‰∏≠ÂçªÁÑ°Ê≥ïÊ¶ÇÂåñ„ÄÇÊú¨ÊïôÂ≠∏Ë´ñÊñáËÄÉÈáè‰∫ÜÂ∞áÂÖ∂ËΩâË≠ØÂà∞ÁèæÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÊÉÖÂ¢ÉÔºåÊèê‰æõ‰∫ÜÂÅ•ÂÖ®ÈñãÁôºËá®Â∫äË™ûË®Ä AI ÊâÄÈúÄÈóúÈçµÁµÑÊàêÁöÑÊ¶ÇËßÄ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊú¨ÊñáÂ∞áÊ∂µËìãÊúÄÈÅ©Âêà‰∏çÂêåËá®Â∫äÁãÄÊ≥ÅÁöÑË™ûË®ÄÂºïÂ∞é‰ªªÂãôÂíåÂçîÂÆöÁöÑË®≠Ë®à„ÄÅË≥áÊñôÊî∂ÈõÜÂíåÁ°¨È´îÈ©óË≠â„ÄÅÁî®ÊñºË°°ÈáèËá®Â∫äÈóúÊ≥®ÁµêÊßãÁöÑË™ûË®ÄË°®Ëø∞ÁöÑÈñãÁôºÂíåÈ©óË≠â„ÄÅÂèØÈù†‰∏îÂÅ•ÂÖ®ÁöÑËá®Â∫äÈ†êÊ∏¨Ê®°ÂûãÁöÑÈñãÁôºÔºå‰ª•ÂèäËá®Â∫äË™ûË®Ä AI ÁöÑÂÄ´ÁêÜÂíåÂèÉËàáËÄÖËÄÉÈáè„ÄÇÁõÆÊ®ôÊòØÊèê‰æõÂÖ®Èù¢ÁöÑÊåáÂ∞éÊñπÈáùÔºå‰ª•Âª∫Á´ãÂÖ∂Ëº∏ÂÖ•ÂíåËº∏Âá∫ÈÄ£ÁµêÂà∞Êõ¥ÊòìÊñºÁêÜËß£‰∏îËá®Â∫ä‰∏äÊúâÊÑèÁæ©ÁöÑË™ûË®ÄÈù¢ÂêëÁöÑÊ®°ÂûãÔºåÂèØ‰ª•Âú®Ëá®Â∫äË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©¢ÂïèÂíåËá®Â∫äÈ©óË≠âÔºå‰∏¶‰∏îÂú®Ë®≠Ë®à‰∏äÈÅµÂÆàÂÄ´ÁêÜ„ÄÅÈö±ÁßÅÂíåÂÆâÂÖ®ËÄÉÈáè„ÄÇ</paragraph>

##### **Can Large Language Models Replace Data Scientists in Clinical Research?**
2410.21591v1 by Zifeng Wang, Benjamin Danek, Ziwei Yang, Zheng Chen, Jimeng Sun

Data science plays a critical role in clinical research, but it requires
professionals with expertise in coding and medical data analysis. Large
language models (LLMs) have shown great potential in supporting medical tasks
and performing well in general coding tests. However, these tests do not assess
LLMs' ability to handle data science tasks in medicine, nor do they explore
their practical utility in clinical research. To address this, we developed a
dataset consisting of 293 real-world data science coding tasks, based on 39
published clinical studies, covering 128 tasks in Python and 165 tasks in R.
This dataset simulates realistic clinical research scenarios using patient
data. Our findings reveal that cutting-edge LLMs struggle to generate perfect
solutions, frequently failing to follow input instructions, understand target
data, and adhere to standard analysis practices. Consequently, LLMs are not yet
ready to fully automate data science tasks. We benchmarked advanced adaptation
methods and found two to be particularly effective: chain-of-thought prompting,
which provides a step-by-step plan for data analysis, which led to a 60%
improvement in code accuracy; and self-reflection, enabling LLMs to iteratively
refine their code, yielding a 38% accuracy improvement. Building on these
insights, we developed a platform that integrates LLMs into the data science
workflow for medical professionals. In a user study with five medical doctors,
we found that while LLMs cannot fully automate coding tasks, they significantly
streamline the programming process. We found that 80% of their submitted code
solutions were incorporated from LLM-generated code, with up to 96% reuse in
some cases. Our analysis highlights the potential of LLMs, when integrated into
expert workflows, to enhance data science efficiency in clinical research.

ÊëòË¶ÅÔºö<paragraph>Ë≥áÊñôÁßëÂ≠∏Âú®Ëá®Â∫äÁ†îÁ©∂‰∏≠ÁôºÊèÆÈóúÈçµ‰ΩúÁî®Ôºå‰ΩÜÂÆÉÈúÄË¶ÅÂÖ∑ÂÇôÁ∑®Á¢ºÂíåÈÜ´ÁôÇË≥áÊñôÂàÜÊûêÂ∞àÊ•≠Áü•Ë≠òÁöÑÂ∞àÊ•≠‰∫∫Âì°„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÊîØÊè¥ÈÜ´ÁôÇ‰ªªÂãôÂíåÂü∑Ë°å‰∏ÄËà¨Á∑®Á¢ºÊ∏¨Ë©¶ÊñπÈù¢Â±ïÁèæ‰∫ÜÊ•µÂ§ßÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ∏¨Ë©¶‰∏¶Êú™Ë©ï‰º∞ LLM ËôïÁêÜÈÜ´Â≠∏‰∏≠Ë≥áÊñôÁßëÂ≠∏‰ªªÂãôÁöÑËÉΩÂäõÔºå‰πüÊ≤íÊúâÊé¢Ë®éÂÆÉÂÄëÂú®Ëá®Â∫äÁ†îÁ©∂‰∏≠ÁöÑÂØ¶ÈöõÊïàÁî®„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÁî± 293 ÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÁßëÂ≠∏Á∑®Á¢º‰ªªÂãôÁµÑÊàêÁöÑË≥áÊñôÈõÜÔºåÈÄô‰∫õ‰ªªÂãôÂü∫Êñº 39 È†ÖÂ∑≤ÁôºË°®ÁöÑËá®Â∫äÁ†îÁ©∂ÔºåÊ∂µËìã 128 ÂÄã Python ‰ªªÂãôÂíå 165 ÂÄã R ‰ªªÂãô„ÄÇÊ≠§Ë≥áÊñôÈõÜ‰ΩøÁî®ÊÇ£ËÄÖË≥áÊñôÊ®°Êì¨ÁúüÂØ¶ÁöÑËá®Â∫äÁ†îÁ©∂Â†¥ÊôØ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÊúÄÂÖàÈÄ≤ÁöÑ LLM Èõ£‰ª•Áî¢ÁîüÂÆåÁæéÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂ∏∏Â∏∏ÁÑ°Ê≥ïÈÅµÂæ™Ëº∏ÂÖ•Ë™™Êòé„ÄÅÁêÜËß£ÁõÆÊ®ôË≥áÊñôÔºå‰ª•ÂèäÈÅµÂÆàÊ®ôÊ∫ñÂàÜÊûêÂØ¶Âãô„ÄÇÂõ†Ê≠§ÔºåLLM Â∞öÊú™Ê∫ñÂÇôÂ•ΩÂÆåÂÖ®Ëá™ÂãïÂåñË≥áÊñôÁßëÂ≠∏‰ªªÂãô„ÄÇÊàëÂÄëÂ∞çÈÄ≤ÈöéÈÅ©ÊáâÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÁôºÁèæÊúâÂÖ©ÂÄãÊñπÊ≥ïÁâπÂà•ÊúâÊïàÔºöÊÄùËÄÉÈèàÊèêÁ§∫ÔºåÂÆÉÊèê‰æõ‰∫ÜË≥áÊñôÂàÜÊûêÁöÑÈÄêÊ≠•Ë®àÁï´Ôºå‰ΩøÁ®ãÂºèÁ¢ºÊ∫ñÁ¢∫Â∫¶ÊèêÂçá‰∫Ü 60%Ôºõ‰ª•ÂèäËá™ÊàëÂèçÁúÅÔºå‰Ωø LLM ËÉΩÂ§†ÂèçË¶ÜÊîπÂñÑÂÖ∂Á®ãÂºèÁ¢ºÔºå‰ΩøÊ∫ñÁ¢∫Â∫¶ÊèêÂçá‰∫Ü 38%„ÄÇÊ†πÊìöÈÄô‰∫õË¶ãËß£ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÂ∞á LLM Êï¥ÂêàÂà∞ÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°Ë≥áÊñôÁßëÂ≠∏Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÂπ≥Âè∞„ÄÇÂú®Ëàá‰∫î‰ΩçÈÜ´ÁîüÁöÑ‰ΩøÁî®ËÄÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÁôºÁèæÔºåÈõñÁÑ∂ LLM ÁÑ°Ê≥ïÂÆåÂÖ®Ëá™ÂãïÂåñÁ∑®Á¢º‰ªªÂãôÔºå‰ΩÜÂÆÉÂÄëÂ§ßÂπÖÁ∞°Âåñ‰∫ÜÁ®ãÂºèË®≠Ë®àÊµÅÁ®ã„ÄÇÊàëÂÄëÁôºÁèæÔºå‰ªñÂÄëÊèê‰∫§ÁöÑÁ®ãÂºèÁ¢ºËß£Ê±∫ÊñπÊ°à‰∏≠Êúâ 80% ÊòØÂæû LLM ÁîüÊàêÁöÑÁ®ãÂºèÁ¢º‰∏≠Á¥çÂÖ•ÁöÑÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÈáçÁî®ÁéáÈ´òÈÅî 96%„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÂº∑Ë™ø‰∫Ü LLM Âú®Êï¥ÂêàÂà∞Â∞àÂÆ∂Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÊΩõÂäõÔºå‰ª•ÊèêÈ´òËá®Â∫äÁ†îÁ©∂‰∏≠ÁöÑË≥áÊñôÁßëÂ≠∏ÊïàÁéá„ÄÇ</paragraph>

##### **A Perspective for Adapting Generalist AI to Specialized Medical AI Applications and Their Challenges**
2411.00024v1 by Zifeng Wang, Hanyin Wang, Benjamin Danek, Ying Li, Christina Mack, Hoifung Poon, Yajun Wang, Pranav Rajpurkar, Jimeng Sun

The integration of Large Language Models (LLMs) into medical applications has
sparked widespread interest across the healthcare industry, from drug discovery
and development to clinical decision support, assisting telemedicine, medical
devices, and healthcare insurance applications. This perspective paper aims to
discuss the inner workings of building LLM-powered medical AI applications and
introduces a comprehensive framework for their development. We review existing
literature and outline the unique challenges of applying LLMs in specialized
medical contexts. Additionally, we introduce a three-step framework to organize
medical LLM research activities: 1) Modeling: breaking down complex medical
workflows into manageable steps for developing medical-specific models; 2)
Optimization: optimizing the model performance with crafted prompts and
integrating external knowledge and tools, and 3) System engineering:
decomposing complex tasks into subtasks and leveraging human expertise for
building medical AI applications. Furthermore, we offer a detailed use case
playbook that describes various LLM-powered medical AI applications, such as
optimizing clinical trial design, enhancing clinical decision support, and
advancing medical imaging analysis. Finally, we discuss various challenges and
considerations for building medical AI applications with LLMs, such as handling
hallucination issues, data ownership and compliance, privacy, intellectual
property considerations, compute cost, sustainability issues, and responsible
AI requirements.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÊï¥ÂêàÂà∞ÈÜ´ÁôÇÊáâÁî®‰∏≠ÔºåÂú®ÈÜ´ÁôÇÁî¢Ê•≠‰∏≠ÂºïËµ∑Âª£Ê≥õËààË∂£ÔºåÂæûËó•Áâ©ÁôºÁèæÂíåÈñãÁôºÂà∞Ëá®Â∫äÊ±∫Á≠ñÊîØÊè¥ÔºåÂçîÂä©ÈÅ†Ë∑ùÈÜ´ÁôÇ„ÄÅÈÜ´ÁôÇË®≠ÂÇôÂíåÈÜ´ÁôÇ‰øùÈö™ÊáâÁî®„ÄÇÊú¨ËßÄÈªûË´ñÊñáÊó®Âú®Êé¢Ë®éÂª∫Êßã LLM È©ÖÂãïÁöÑÈÜ´ÁôÇ AI ÊáâÁî®Á®ãÂºèÁöÑÂÖßÈÉ®ÈÅã‰ΩúÔºå‰∏¶‰ªãÁ¥π‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÈñãÁôºÊû∂Êßã„ÄÇÊàëÂÄëÊ™¢Ë¶ñÁèæÊúâÊñáÁçª‰∏¶Ê¶ÇËø∞Âú®Â∞àÊ•≠ÈÜ´ÁôÇÊÉÖÂ¢É‰∏≠ÊáâÁî® LLM ÁöÑÁç®ÁâπÊåëÊà∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∏ÄÂÄã‰∏âÊ≠•È©üÊû∂Êßã‰æÜÁµÑÁπîÈÜ´ÁôÇ LLM Á†îÁ©∂Ê¥ªÂãïÔºö1) Âª∫Ê®°ÔºöÂ∞áË§áÈõúÁöÑÈÜ´ÁôÇÂ∑•‰ΩúÊµÅÁ®ãÂàÜËß£ÁÇ∫ÂèØÁÆ°ÁêÜÁöÑÊ≠•È©üÔºå‰ª•ÈñãÁôºÁâπÂÆöÊñºÈÜ´ÁôÇÁöÑÊ®°ÂûãÔºõ2) ÊúÄ‰Ω≥ÂåñÔºö‰ΩøÁî®Á≤æÂøÉË®≠Ë®àÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÊ®°ÂûãÊïàËÉΩÔºå‰∏¶Êï¥ÂêàÂ§ñÈÉ®Áü•Ë≠òÂíåÂ∑•ÂÖ∑Ôºõ3) Á≥ªÁµ±Â∑•Á®ãÔºöÂ∞áË§áÈõúÁöÑ‰ªªÂãôÂàÜËß£ÁÇ∫Â≠ê‰ªªÂãôÔºå‰∏¶Âà©Áî®‰∫∫È°ûÂ∞àÊ•≠Áü•Ë≠ò‰æÜÂª∫ÊßãÈÜ´ÁôÇ AI ÊáâÁî®Á®ãÂºè„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèê‰æõ‰∏ÄÂÄãË©≥Á¥∞ÁöÑ‰ΩøÁî®Ê°à‰æãÁØÑ‰æãÔºåË™™ÊòéÂêÑÁ®Æ LLM È©ÖÂãïÁöÑÈÜ´ÁôÇ AI ÊáâÁî®Á®ãÂºèÔºå‰æãÂ¶ÇÊúÄ‰Ω≥ÂåñËá®Â∫äË©¶È©óË®≠Ë®à„ÄÅÂ¢ûÂº∑Ëá®Â∫äÊ±∫Á≠ñÊîØÊè¥ÂíåÊé®ÈÄ≤ÈÜ´ÁôÇÂΩ±ÂÉèÂàÜÊûê„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñÂª∫ÊßãÂÖ∑Êúâ LLM ÁöÑÈÜ´ÁôÇ AI ÊáâÁî®Á®ãÂºèÁöÑÂêÑÁ®ÆÊåëÊà∞ÂíåËÄÉÈáèÔºå‰æãÂ¶ÇËôïÁêÜÂπªË¶∫ÂïèÈ°å„ÄÅË≥áÊñôÊâÄÊúâÊ¨äÂíåÂêàË¶èÊÄß„ÄÅÈö±ÁßÅ„ÄÅÊô∫ÊÖßË≤°Áî¢Ê¨äËÄÉÈáè„ÄÅÈÅãÁÆóÊàêÊú¨„ÄÅÊ∞∏Á∫åÊÄßÂïèÈ°åÂíåË≤†Ë≤¨‰ªªÁöÑ AI ÈúÄÊ±Ç„ÄÇ

##### **Going Beyond H&E and Oncology: How Do Histopathology Foundation Models Perform for Multi-stain IHC and Immunology?**
2410.21560v1 by Amaya Gallagher-Syed, Elena Pontarini, Myles J. Lewis, Michael R. Barnes, Gregory Slabaugh

This study evaluates the generalisation capabilities of state-of-the-art
histopathology foundation models on out-of-distribution multi-stain autoimmune
Immunohistochemistry datasets. We compare 13 feature extractor models,
including ImageNet-pretrained networks, and histopathology foundation models
trained on both public and proprietary data, on Rheumatoid Arthritis subtyping
and Sjogren's Disease detection tasks. Using a simple Attention-Based Multiple
Instance Learning classifier, we assess the transferability of learned
representations from cancer H&E images to autoimmune IHC images. Contrary to
expectations, histopathology-pretrained models did not significantly outperform
ImageNet-pretrained models. Furthermore, there was evidence of both autoimmune
feature misinterpretation and biased feature importance. Our findings highlight
the challenges in transferring knowledge from cancer to autoimmune
histopathology and emphasise the need for careful evaluation of AI models
across diverse histopathological tasks. The code to run this benchmark is
available at https://github.com/AmayaGS/ImmunoHistoBench.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Ë©ï‰º∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÁµÑÁπîÁóÖÁêÜÂ≠∏Âü∫Á§éÊ®°ÂûãÂú®ÂàÜÂ∏ÉÂ§ñÂ§öÊüìËâ≤Ëá™Ë∫´ÂÖçÁñ´ÂÖçÁñ´ÁµÑÁπîÂåñÂ≠∏Êï∏ÊìöÈõÜ‰∏äÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊàëÂÄëÊØîËºÉ‰∫Ü 13 ÂÄãÁâπÂæµÊèêÂèñÂô®Ê®°ÂûãÔºåÂåÖÊã¨ ImageNet È†êË®ìÁ∑¥Á∂≤Ë∑ØÔºå‰ª•ÂèäÂú®ÂÖ¨ÂÖ±ÂíåÂ∞àÊúâÊï∏Êìö‰∏äË®ìÁ∑¥ÁöÑÁµÑÁπîÁóÖÁêÜÂ≠∏Âü∫Á§éÊ®°ÂûãÔºåÂú®È°ûÈ¢®ÊøïÊÄßÈóúÁØÄÁÇé‰∫ûÂûãÂíå‰πæÁá•ÁóáÊ™¢Ê∏¨‰ªªÂãô‰∏ä„ÄÇ‰ΩøÁî®‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÂ§öÂØ¶‰æãÂ≠∏ÁøíÂàÜÈ°ûÂô®ÔºåÊàëÂÄëË©ï‰º∞‰∫ÜÂæûÁôåÁóá H&E ÂΩ±ÂÉèÂà∞Ëá™Ë∫´ÂÖçÁñ´ IHC ÂΩ±ÂÉèÁöÑÂ≠∏ÁøíË°®ÂæµÁöÑÂèØÂÇ≥ÈÅûÊÄß„ÄÇËàáÈ†êÊúüÁõ∏ÂèçÔºåÁµÑÁπîÁóÖÁêÜÂ≠∏È†êË®ìÁ∑¥Ê®°Âûã‰∏¶Ê≤íÊúâÈ°ØËëóÂÑ™Êñº ImageNet È†êË®ìÁ∑¥Ê®°Âûã„ÄÇÊ≠§Â§ñÔºåÊúâË≠âÊìöË°®ÊòéÂ≠òÂú®Ëá™Ë∫´ÂÖçÁñ´ÁâπÂæµË™§Ëß£ÂíåÂÅèÂ∑ÆÁâπÂæµÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÂº∑Ë™ø‰∫ÜÂ∞áÁü•Ë≠òÂæûÁôåÁóáËΩâÁßªÂà∞Ëá™Ë∫´ÂÖçÁñ´ÁµÑÁπîÁóÖÁêÜÂ≠∏ÁöÑÊåëÊà∞Ôºå‰∏¶Âº∑Ë™ø‰∫ÜË∑®‰∏çÂêåÁµÑÁπîÁóÖÁêÜÂ≠∏‰ªªÂãô‰ªîÁ¥∞Ë©ï‰º∞ AI Ê®°ÂûãÁöÑÂøÖË¶ÅÊÄß„ÄÇÈÅãË°åÊ≠§Âü∫Ê∫ñÊ∏¨Ë©¶ÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/AmayaGS/ImmunoHistoBench Áç≤Âæó„ÄÇ

##### **Towards Multi-dimensional Explanation Alignment for Medical Classification**
2410.21494v1 by Lijie Hu, Songning Lai, Wenshuo Chen, Hongru Xiao, Hongbin Lin, Lu Yu, Jingfeng Zhang, Di Wang

The lack of interpretability in the field of medical image analysis has
significant ethical and legal implications. Existing interpretable methods in
this domain encounter several challenges, including dependency on specific
models, difficulties in understanding and visualization, as well as issues
related to efficiency. To address these limitations, we propose a novel
framework called Med-MICN (Medical Multi-dimensional Interpretable Concept
Network). Med-MICN provides interpretability alignment for various angles,
including neural symbolic reasoning, concept semantics, and saliency maps,
which are superior to current interpretable methods. Its advantages include
high prediction accuracy, interpretability across multiple dimensions, and
automation through an end-to-end concept labeling process that reduces the need
for extensive human training effort when working with new datasets. To
demonstrate the effectiveness and interpretability of Med-MICN, we apply it to
four benchmark datasets and compare it with baselines. The results clearly
demonstrate the superior performance and interpretability of our Med-MICN.

ÊëòË¶ÅÔºöÈÜ´ÁôÇÂΩ±ÂÉèÂàÜÊûêÈ†òÂüüÁº∫‰πèÂèØËß£ÈáãÊÄßÔºåÈÄôÂ∏∂‰æÜÈáçÂ§ßÁöÑÂÄ´ÁêÜÂíåÊ≥ïÂæãÂΩ±Èüø„ÄÇÁèæÊúâÁöÑÂèØËß£ÈáãÊñπÊ≥ïÂú®ÈÄôÂÄãÈ†òÂüü‰∏≠ÊúÉÈÅ≠ÈÅáË®±Â§öÊåëÊà∞ÔºåÂåÖÊã¨‰æùË≥¥ÁâπÂÆöÊ®°Âûã„ÄÅÈõ£‰ª•ÁêÜËß£ÂíåË¶ñË¶∫ÂåñÔºå‰ª•ÂèäËàáÊïàÁéáÁõ∏ÈóúÁöÑÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÊû∂ÊßãÔºåÁ®±ÁÇ∫ Med-MICNÔºàÈÜ´ÁôÇÂ§öÁ∂≠ÂèØËß£ÈáãÊ¶ÇÂøµÁ∂≤Ë∑ØÔºâ„ÄÇMed-MICN Êèê‰æõÂêÑÁ®ÆËßíÂ∫¶ÁöÑÂèØËß£ÈáãÊÄßÊØîÂ∞çÔºåÂåÖÊã¨Á•ûÁ∂ìÁ¨¶ËôüÊé®ÁêÜ„ÄÅÊ¶ÇÂøµË™ûÊÑèÂíåÈ°ØËëóÊÄßÂúñÔºåÈÄô‰∫õÈÉΩÂÑ™ÊñºÁõÆÂâçÁöÑÂèØËß£ÈáãÊñπÊ≥ï„ÄÇÂÆÉÁöÑÂÑ™ÈªûÂåÖÊã¨È´òÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÅÂ§öÁ∂≠Â∫¶ÁöÑÂèØËß£ÈáãÊÄßÔºå‰ª•ÂèäÈÄèÈÅéÁ´ØÂà∞Á´ØÊ¶ÇÂøµÊ®ôË®òÊµÅÁ®ãËá™ÂãïÂåñÔºåÈÄôÊ∏õÂ∞ë‰∫ÜÂú®‰ΩøÁî®Êñ∞Ë≥áÊñôÈõÜÊôÇÈúÄË¶ÅÂ§ßÈáè‰∫∫Â∑•Ë®ìÁ∑¥ÁöÑÂ∑•‰Ωú„ÄÇÁÇ∫‰∫ÜË≠âÊòé Med-MICN ÁöÑÊúâÊïàÊÄßÂíåÂèØËß£ÈáãÊÄßÔºåÊàëÂÄëÂ∞áÂÖ∂ÊáâÁî®ÊñºÂõõÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜÔºå‰∏¶ËàáÂü∫Ê∫ñÁ∑öÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÊûúÊ∏ÖÊ•öÂú∞Ë≠âÊòé‰∫ÜÊàëÂÄëÁöÑ Med-MICN ÂÖ∑ÊúâÂÑ™Áï∞ÁöÑÊïàËÉΩÂíåÂèØËß£ÈáãÊÄß„ÄÇ

##### **Multi-modal AI for comprehensive breast cancer prognostication**
2410.21256v1 by Jan Witowski, Ken Zeng, Joseph Cappadona, Jailan Elayoubi, Elena Diana Chiru, Nancy Chan, Young-Joon Kang, Frederick Howard, Irina Ostrovnaya, Carlos Fernandez-Granda, Freya Schnabel, Ugur Ozerdem, Kangning Liu, Zoe Steinsnyder, Nitya Thakore, Mohammad Sadic, Frank Yeung, Elisa Liu, Theodore Hill, Benjamin Swett, Danielle Rigau, Andrew Clayburn, Valerie Speirs, Marcus Vetter, Lina Sojak, Simone Muenst Soysal, Daniel Baumhoer, Khalil Choucair, Yu Zong, Lina Daoud, Anas Saad, Waleed Abdulsattar, Rafic Beydoun, Jia-Wern Pan, Haslina Makmur, Soo-Hwang Teo, Linda Ma Pak, Victor Angel, Dovile Zilenaite-Petrulaitiene, Arvydas Laurinavicius, Natalie Klar, Brian D. Piening, Carlo Bifulco, Sun-Young Jun, Jae Pak Yi, Su Hyun Lim, Adam Brufsky, Francisco J. Esteva, Lajos Pusztai, Yann LeCun, Krzysztof J. Geras

Treatment selection in breast cancer is guided by molecular subtypes and
clinical characteristics. Recurrence risk assessment plays a crucial role in
personalizing treatment. Current methods, including genomic assays, have
limited accuracy and clinical utility, leading to suboptimal decisions for many
patients. We developed a test for breast cancer patient stratification based on
digital pathology and clinical characteristics using novel AI methods.
Specifically, we utilized a vision transformer-based pan-cancer foundation
model trained with self-supervised learning to extract features from digitized
H&E-stained slides. These features were integrated with clinical data to form a
multi-modal AI test predicting cancer recurrence and death. The test was
developed and evaluated using data from a total of 8,161 breast cancer patients
across 15 cohorts originating from seven countries. Of these, 3,502 patients
from five cohorts were used exclusively for evaluation, while the remaining
patients were used for training. Our test accurately predicted our primary
endpoint, disease-free interval, in the five external cohorts (C-index: 0.71
[0.68-0.75], HR: 3.63 [3.02-4.37, p<0.01]). In a direct comparison (N=858), the
AI test was more accurate than Oncotype DX, the standard-of-care 21-gene assay,
with a C-index of 0.67 [0.61-0.74] versus 0.61 [0.49-0.73], respectively.
Additionally, the AI test added independent information to Oncotype DX in a
multivariate analysis (HR: 3.11 [1.91-5.09, p<0.01)]). The test demonstrated
robust accuracy across all major breast cancer subtypes, including TNBC
(C-index: 0.71 [0.62-0.81], HR: 3.81 [2.35-6.17, p=0.02]), where no diagnostic
tools are currently recommended by clinical guidelines. These results suggest
that our AI test can improve accuracy, extend applicability to a wider range of
patients, and enhance access to treatment selection tools.

ÊëòË¶ÅÔºö<paragraph>‰π≥ÁôåÁöÑÊ≤ªÁôÇÈÅ∏ÊìáÊòØÁî±ÂàÜÂ≠ê‰∫ûÂûãÂíåËá®Â∫äÁâπÂæµÊâÄÂºïÂ∞é„ÄÇÂæ©ÁôºÈ¢®Èö™Ë©ï‰º∞Âú®ÂÄã‰∫∫ÂåñÊ≤ªÁôÇ‰∏≠ÊâÆÊºîËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÁõÆÂâçÁöÑÊäÄË°ìÔºåÂåÖÊã¨Âü∫Âõ†È´îÂàÜÊûêÔºåÂÖ∑ÊúâÊúâÈôêÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåËá®Â∫äÊïàÁî®ÔºåÂ∞éËá¥Ë®±Â§öÊÇ£ËÄÖÁöÑÊ≤ªÁôÇÊ±∫Á≠ñÊ¨°ÊñºÊúÄ‰Ω≥„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÊï∏‰ΩçÁóÖÁêÜÂ≠∏ÂíåËá®Â∫äÁâπÂæµÁöÑ‰π≥ÁôåÊÇ£ËÄÖÂàÜÂ±§Ê™¢Ê∏¨ÔºåÊé°Áî®Êñ∞Á©éÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊñπÊ≥ï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂà©Áî®‰∫Ü‰∏ÄÂÄãÂü∫ÊñºË¶ñË¶∫ËΩâÊèõÂô®ÁöÑÊ≥õÁôåÂü∫Á§éÊ®°ÂûãÔºå‰∏¶ÈÄèÈÅéËá™ÊàëÁõ£Áù£Â≠∏ÁøíÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰ª•ÂæûÊï∏‰ΩçÂåñÁöÑ H&E ÊüìËâ≤ÁéªÁâá‰∏≠ÊèêÂèñÁâπÂæµ„ÄÇÈÄô‰∫õÁâπÂæµËàáËá®Â∫äË≥áÊñôÊï¥ÂêàÔºåÂΩ¢Êàê‰∏ÄÂÄãÂ§öÊ®°ÂºèÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊ™¢Ê∏¨ÔºåÁî®ÊñºÈ†êÊ∏¨ÁôåÁóáÂæ©ÁôºÂíåÊ≠ª‰∫°„ÄÇË©≤Ê™¢Ê∏¨ÁöÑÈñãÁôºÂíåË©ï‰º∞‰ΩøÁî®‰∫Ü‰æÜËá™‰∏ÉÂÄãÂúãÂÆ∂/Âú∞ÂçÄÁöÑ 15 ÂÄãÁæ§ÁµÑÂÖ± 8,161 Âêç‰π≥ÁôåÊÇ£ËÄÖÁöÑË≥áÊñô„ÄÇÂÖ∂‰∏≠Ôºå‰æÜËá™‰∫îÂÄãÁæ§ÁµÑÁöÑ 3,502 ÂêçÊÇ£ËÄÖÂ∞àÈñÄÁî®ÊñºË©ï‰º∞ÔºåËÄåÂÖ∂È§òÊÇ£ËÄÖÂâáÁî®ÊñºË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÊ™¢Ê∏¨Ê∫ñÁ¢∫Âú∞È†êÊ∏¨‰∫ÜÊàëÂÄëÁöÑ‰∏ªË¶ÅÁµÇÈªûÔºåÂç≥‰∫îÂÄãÂ§ñÈÉ®Áæ§ÁµÑÁöÑÁÑ°ÁñæÁóÖÈñìÊúüÔºàC ÊåáÊï∏Ôºö0.71 [0.68-0.75]ÔºåHRÔºö3.63 [3.02-4.37Ôºåp<0.01]Ôºâ„ÄÇÂú®Áõ¥Êé•ÊØîËºÉÔºàN=858Ôºâ‰∏≠Ôºå‰∫∫Â∑•Êô∫ÊÖßÊ™¢Ê∏¨ÊØîÂÆâÁßëÊ≥∞DxÔºåÊ®ôÊ∫ñÁÖßË≠∑ÁöÑ 21 Âü∫Âõ†Ê™¢Ê∏¨Êõ¥Ê∫ñÁ¢∫ÔºåC ÊåáÊï∏ÂàÜÂà•ÁÇ∫ 0.67 [0.61-0.74] Âíå 0.61 [0.49-0.73]„ÄÇÊ≠§Â§ñÔºå‰∫∫Â∑•Êô∫ÊÖßÊ™¢Ê∏¨Âú®Â§öËÆäÈáèÂàÜÊûê‰∏≠Â¢ûÂä†‰∫ÜÂÆâÁßëÊ≥∞ Dx ÁöÑÁç®Á´ãË≥áË®äÔºàHRÔºö3.11 [1.91-5.09Ôºåp<0.01]Ôºâ„ÄÇË©≤Ê™¢Ê∏¨Âú®ÊâÄÊúâ‰∏ªË¶ÅÁöÑ‰π≥Áôå‰∫ûÂûã‰∏≠ÈÉΩË°®ÁèæÂá∫Âº∑Â§ßÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÂåÖÊã¨ TNBCÔºàC ÊåáÊï∏Ôºö0.71 [0.62-0.81]ÔºåHRÔºö3.81 [2.35-6.17Ôºåp=0.02]ÔºâÔºåËá®Â∫äÊåáÂçóÁõÆÂâç‰∏çÂª∫Ë≠∞‰ΩøÁî®‰ªª‰ΩïË®∫Êñ∑Â∑•ÂÖ∑„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊ™¢Ê∏¨ÂèØ‰ª•ÊèêÈ´òÊ∫ñÁ¢∫Â∫¶ÔºåÂ∞áÈÅ©Áî®ÁØÑÂúçÊì¥Â±ïÂà∞Êõ¥Â§öÊÇ£ËÄÖÔºå‰∏¶Â¢ûÂä†Áç≤ÂæóÊ≤ªÁôÇÈÅ∏ÊìáÂ∑•ÂÖ∑ÁöÑÊ©üÊúÉ„ÄÇ</paragraph>

##### **Belief in the Machine: Investigating Epistemological Blind Spots of Language Models**
2410.21195v1 by Mirac Suzgun, Tayfun Gur, Federico Bianchi, Daniel E. Ho, Thomas Icard, Dan Jurafsky, James Zou

As language models (LMs) become integral to fields like healthcare, law, and
journalism, their ability to differentiate between fact, belief, and knowledge
is essential for reliable decision-making. Failure to grasp these distinctions
can lead to significant consequences in areas such as medical diagnosis, legal
judgments, and dissemination of fake news. Despite this, current literature has
largely focused on more complex issues such as theory of mind, overlooking more
fundamental epistemic challenges. This study systematically evaluates the
epistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and
Llama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13
tasks. Our results reveal key limitations. First, while LMs achieve 86%
accuracy on factual scenarios, their performance drops significantly with false
scenarios, particularly in belief-related tasks. Second, LMs struggle with
recognizing and affirming personal beliefs, especially when those beliefs
contradict factual data, which raises concerns for applications in healthcare
and counseling, where engaging with a person's beliefs is critical. Third, we
identify a salient bias in how LMs process first-person versus third-person
beliefs, performing better on third-person tasks (80.7%) compared to
first-person tasks (54.4%). Fourth, LMs lack a robust understanding of the
factive nature of knowledge, namely, that knowledge inherently requires truth.
Fifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the
deeper reasoning. These findings highlight significant concerns about current
LMs' ability to reason about truth, belief, and knowledge while emphasizing the
need for advancements in these areas before broad deployment in critical
sectors.

ÊëòË¶ÅÔºöÈö®ËëóË™ûË®ÄÊ®°Âûã (LM) ÊàêÁÇ∫ÈÜ´ÁôÇ‰øùÂÅ•„ÄÅÊ≥ïÂæãÂíåÊñ∞ËÅûÁ≠âÈ†òÂüü‰∏çÂèØÊàñÁº∫ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÂÆÉÂÄëÂçÄÂàÜ‰∫ãÂØ¶„ÄÅ‰ø°ÂøµÂíåÁü•Ë≠òÁöÑËÉΩÂäõÂ∞çÊñºÂèØÈù†ÁöÑÊ±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇÁÑ°Ê≥ïÊéåÊè°ÈÄô‰∫õÂçÄÂà•ÂèØËÉΩÊúÉÂú®ÈÜ´ÁôÇË®∫Êñ∑„ÄÅÊ≥ïÂæãÂà§Ê±∫ÂíåÂÅáÊñ∞ËÅûÂÇ≥Êí≠Á≠âÈ†òÂüüÈÄ†ÊàêÈáçÂ§ßÂæåÊûú„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁõÆÂâçÁöÑÊñáÁçªÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÈóúÊ≥®ÊñºÊõ¥Ë§áÈõúÁöÑÂïèÈ°åÔºå‰æãÂ¶ÇÂøÉÊô∫ÁêÜË´ñÔºåËÄåÂøΩË¶ñ‰∫ÜÊõ¥Âü∫Êú¨ÁöÑË™çË≠òË´ñÊåëÊà∞„ÄÇÊú¨Á†îÁ©∂‰ΩøÁî®Êñ∞ÁöÑË≥áÊñôÈõÜ KaBLEÔºåÂ∞çÁèæ‰ª£ LMÔºàÂåÖÊã¨ GPT-4„ÄÅClaude-3 Âíå Llama-3ÔºâÁöÑË™çË≠òË´ñÊé®ÁêÜËÉΩÂäõÈÄ≤Ë°å‰∫ÜÁ≥ªÁµ±Ë©ï‰º∞ÔºåË©≤Ë≥áÊñôÈõÜÂåÖÂê´ 13 ÂÄã‰ªªÂãô‰∏≠ÁöÑ 13,000 ÂÄãÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÁµêÊûúÊè≠Á§∫‰∫ÜÈóúÈçµÈôêÂà∂„ÄÇÈ¶ñÂÖàÔºåÈõñÁÑ∂ LM Âú®‰∫ãÂØ¶Â†¥ÊôØ‰∏≠ÈÅîÂà∞ 86% ÁöÑÊ∫ñÁ¢∫Â∫¶Ôºå‰ΩÜÂÆÉÂÄëÂú®ÈåØË™§Â†¥ÊôØ‰∏≠ÁöÑË°®ÁèæÂ§ßÂπÖ‰∏ãÈôçÔºåÁâπÂà•ÊòØÂú®Ëàá‰ø°ÂøµÁõ∏ÈóúÁöÑ‰ªªÂãô‰∏≠„ÄÇÂÖ∂Ê¨°ÔºåLM Èõ£‰ª•Ë≠òÂà•ÂíåËÇØÂÆöÂÄã‰∫∫‰ø°ÂøµÔºåÁâπÂà•ÊòØÁï∂ÈÄô‰∫õ‰ø°ÂøµËàá‰∫ãÂØ¶Ë≥áÊñôÁõ∏ÁüõÁõæÊôÇÔºåÈÄôÂºïËµ∑‰∫ÜÂ∞çÈÜ´ÁôÇ‰øùÂÅ•ÂíåË´ÆË©¢ÊáâÁî®Á®ãÂºèÁöÑÊìîÊÜÇÔºåÂú®ÈÄô‰∫õÊáâÁî®Á®ãÂºè‰∏≠ÔºåËàáÂÄã‰∫∫ÁöÑ‰ø°Âøµ‰∫íÂãïËá≥ÈóúÈáçË¶Å„ÄÇÁ¨¨‰∏âÔºåÊàëÂÄëÁôºÁèæ LM ËôïÁêÜÁ¨¨‰∏Ä‰∫∫Á®±ËàáÁ¨¨‰∏â‰∫∫Á®±‰ø°ÂøµÁöÑÊñπÂºèÂ≠òÂú®È°ØËëóÂÅèÂ∑ÆÔºåÂú®Á¨¨‰∏â‰∫∫Á®±‰ªªÂãôÔºà80.7%Ôºâ‰∏äÁöÑË°®ÁèæÂÑ™ÊñºÁ¨¨‰∏Ä‰∫∫Á®±‰ªªÂãôÔºà54.4%Ôºâ„ÄÇÁ¨¨ÂõõÔºåLM Áº∫‰πèÂ∞çÁü•Ë≠òÁöÑ‰∫ãÂØ¶ÊÄßË≥™ÁöÑÁ©©ÂÅ•ÁêÜËß£ÔºåÂç≥Áü•Ë≠òÊú¨Ë≥™‰∏äÈúÄË¶ÅÁúüÁêÜ„ÄÇÁ¨¨‰∫îÔºåLM ‰æùË≥¥Ë™ûË®ÄÁ∑öÁ¥¢ÈÄ≤Ë°å‰∫ãÂØ¶Êü•Ê†∏ÔºåÊúâÊôÇÊúÉÁπûÈÅéÊõ¥Ê∑±ÂÖ•ÁöÑÊé®ÁêÜ„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÁï∂Ââç LM Êé®ÁêÜÁúüÁêÜ„ÄÅ‰ø°ÂøµÂíåÁü•Ë≠òÁöÑËÉΩÂäõÂ≠òÂú®ÈáçÂ§ßÁñëÊÖÆÔºåÂêåÊôÇÂº∑Ë™øÂú®Âª£Ê≥õÈÉ®ÁΩ≤ÊñºÈóúÈçµÈÉ®ÈñÄ‰πãÂâçÔºåÈúÄË¶ÅÂú®ÈÄô‰∫õÈ†òÂüüÂèñÂæóÈÄ≤Â±ï„ÄÇ

##### **Deep Learning-Based Fatigue Cracks Detection in Bridge Girders using Feature Pyramid Networks**
2410.21175v1 by Jiawei Zhang, Jun Li, Reachsak Ly, Yunyi Liu, Jiangpeng Shu

For structural health monitoring, continuous and automatic crack detection
has been a challenging problem. This study is conducted to propose a framework
of automatic crack segmentation from high-resolution images containing crack
information about steel box girders of bridges. Considering the multi-scale
feature of cracks, convolutional neural network architecture of Feature Pyramid
Networks (FPN) for crack detection is proposed. As for input, 120 raw images
are processed via two approaches (shrinking the size of images and splitting
images into sub-images). Then, models with the proposed structure of FPN for
crack detection are developed. The result shows all developed models can
automatically detect the cracks at the raw images. By shrinking the images, the
computation efficiency is improved without decreasing accuracy. Because of the
separable characteristic of crack, models using the splitting method provide
more accurate crack segmentations than models using the resizing method.
Therefore, for high-resolution images, the FPN structure coupled with the
splitting method is an promising solution for the crack segmentation and
detection.

ÊëòË¶ÅÔºöÂ∞çÊñºÁµêÊßãÂÅ•Â∫∑Áõ£Ê∏¨ÔºåÈÄ£Á∫å‰∏îËá™ÂãïÁöÑË£ÇÁ∏´ÂÅµÊ∏¨‰∏ÄÁõ¥ÊòØ‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂïèÈ°å„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ÊèêÂá∫‰∏ÄÂÄãÂæûÂåÖÂê´Ê©ãÊ®ëÈãºÁÆ±Ê¢ÅË£ÇÁ∏´Ë≥áË®äÁöÑÈ´òËß£ÊûêÂ∫¶ÂΩ±ÂÉè‰∏≠Ëá™ÂãïÂàÜÂâ≤Ë£ÇÁ∏´ÁöÑÊû∂Êßã„ÄÇËÄÉÈáèÂà∞Ë£ÇÁ∏´ÁöÑÂ§öÂ∞∫Â∫¶ÁâπÂæµÔºåÊèêÂá∫Áî®ÊñºË£ÇÁ∏´ÂÅµÊ∏¨ÁöÑ Feature Pyramid Networks (FPN) Êç≤Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊû∂Êßã„ÄÇËá≥ÊñºËº∏ÂÖ•Ôºå120 ÂºµÂéüÂßãÂΩ±ÂÉèÈÄèÈÅéÂÖ©Á®ÆÊñπÊ≥ïËôïÁêÜÔºàÁ∏ÆÂ∞èÂΩ±ÂÉèÂ∞∫ÂØ∏ÂíåÂ∞áÂΩ±ÂÉèÂàÜÂâ≤ÊàêÂ≠êÂΩ±ÂÉèÔºâ„ÄÇÁÑ∂ÂæåÔºåÈñãÁôºÂÖ∑Êúâ FPN ÊèêË≠∞ÁµêÊßãÁöÑË£ÇÁ∏´ÂÅµÊ∏¨Ê®°Âûã„ÄÇÁµêÊûúÈ°ØÁ§∫ÊâÄÊúâÂ∑≤ÈñãÁôºÁöÑÊ®°ÂûãÈÉΩËÉΩËá™ÂãïÂÅµÊ∏¨ÂéüÂßãÂΩ±ÂÉè‰∏≠ÁöÑË£ÇÁ∏´„ÄÇËóâÁî±Á∏ÆÂ∞èÂΩ±ÂÉèÔºåÂú®‰∏çÈôç‰ΩéÊ∫ñÁ¢∫Â∫¶ÁöÑÁãÄÊ≥Å‰∏ãÊèêÂçáÈÅãÁÆóÊïàÁéá„ÄÇÁî±ÊñºË£ÇÁ∏´ÂÖ∑ÊúâÂèØÂàÜÈõ¢ÁöÑÁâπÂæµÔºå‰ΩøÁî®ÂàÜÂâ≤ÊñπÊ≥ïÁöÑÊ®°ÂûãÊèê‰æõÊØî‰ΩøÁî®Á∏ÆÊîæÊñπÊ≥ïÁöÑÊ®°ÂûãÊõ¥Ê∫ñÁ¢∫ÁöÑË£ÇÁ∏´ÂàÜÂâ≤„ÄÇÂõ†Ê≠§ÔºåÂ∞çÊñºÈ´òËß£ÊûêÂ∫¶ÂΩ±ÂÉèÔºåFPN ÁµêÊßãÁµêÂêàÂàÜÂâ≤ÊñπÊ≥ïÊòØË£ÇÁ∏´ÂàÜÂâ≤ÂíåÂÅµÊ∏¨ÁöÑÊúâÂâçÈÄîÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Trajectory Flow Matching with Applications to Clinical Time Series Modeling**
2410.21154v1 by Xi Zhang, Yuan Pu, Yuki Kawamura, Andrew Loza, Yoshua Bengio, Dennis L. Shung, Alexander Tong

Modeling stochastic and irregularly sampled time series is a challenging
problem found in a wide range of applications, especially in medicine. Neural
stochastic differential equations (Neural SDEs) are an attractive modeling
technique for this problem, which parameterize the drift and diffusion terms of
an SDE with neural networks. However, current algorithms for training Neural
SDEs require backpropagation through the SDE dynamics, greatly limiting their
scalability and stability. To address this, we propose Trajectory Flow Matching
(TFM), which trains a Neural SDE in a simulation-free manner, bypassing
backpropagation through the dynamics. TFM leverages the flow matching technique
from generative modeling to model time series. In this work we first establish
necessary conditions for TFM to learn time series data. Next, we present a
reparameterization trick which improves training stability. Finally, we adapt
TFM to the clinical time series setting, demonstrating improved performance on
three clinical time series datasets both in terms of absolute performance and
uncertainty prediction.

ÊëòË¶ÅÔºöÈö®Ê©ü‰∏î‰∏çË¶èÂâáÂèñÊ®£ÁöÑÊôÇÂ∫èÂª∫Ê®°ÊòØ‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂïèÈ°åÔºåÂú®Âª£Ê≥õÁöÑÊáâÁî®‰∏≠ÁôºÁèæÔºåÁâπÂà•ÊòØÂú®ÈÜ´Â≠∏‰∏≠„ÄÇÁ•ûÁ∂ìÈö®Ê©üÂæÆÂàÜÊñπÁ®ã (Neural SDE) ÊòØÈÄôÂÄãÂïèÈ°å‰∏ÄÂÄãÊúâÂê∏ÂºïÂäõÁöÑÂª∫Ê®°ÊäÄË°ìÔºåÂÆÉÁî®Á•ûÁ∂ìÁ∂≤Ë∑ØÂèÉÊï∏Âåñ SDE ÁöÑÊºÇÁßªÂíåÊì¥Êï£È†Ö„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçË®ìÁ∑¥Á•ûÁ∂ì SDE ÁöÑÊºîÁÆóÊ≥ïÈúÄË¶ÅÈÄèÈÅé SDE ÂãïÊÖãÈÄ≤Ë°åÂèçÂêëÂÇ≥Êí≠ÔºåÊ•µÂ§ßÂú∞ÈôêÂà∂‰∫ÜÂÆÉÂÄëÁöÑÂèØÊì¥ÂÖÖÊÄßÂíåÁ©©ÂÆöÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ËªåË∑°ÊµÅÂåπÈÖç (TFM)ÔºåÂÆÉ‰ª•ÁÑ°Ê®°Êì¨ÁöÑÊñπÂºèË®ìÁ∑¥‰∏ÄÂÄãÁ•ûÁ∂ì SDEÔºåÁπûÈÅéÂãïÊÖãÁöÑÂèçÂêëÂÇ≥Êí≠„ÄÇTFM Âà©Áî®ÁîüÊàêÂºèÂª∫Ê®°‰∏≠ÁöÑÊµÅÂåπÈÖçÊäÄË°ì‰æÜÂª∫Ê®°ÊôÇÂ∫è„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÂª∫Á´ã TFM Â≠∏ÁøíÊôÇÂ∫èË≥áÊñôÁöÑÂøÖË¶ÅÊ¢ù‰ª∂„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÈáçÊñ∞ÂèÉÊï∏ÂåñÁöÑÊäÄÂ∑ßÔºåÂÆÉÊîπÈÄ≤‰∫ÜË®ìÁ∑¥Á©©ÂÆöÊÄß„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ∞á TFM ÈÅ©ÊáâÂà∞Ëá®Â∫äÊôÇÂ∫èË®≠ÂÆöÔºåË≠âÊòé‰∫ÜÂú®ÁµïÂ∞çÊïàËÉΩÂíå‰∏çÁ¢∫ÂÆöÊÄßÈ†êÊ∏¨ÊñπÈù¢ÔºåÂú®‰∏âÂÄãËá®Â∫äÊôÇÂ∫èË≥áÊñôÈõÜ‰∏äÈÉΩÊúâÊïàËÉΩÁöÑÊèêÂçá„ÄÇ

##### **Diagnostic Performance of Deep Learning for Predicting Gliomas' IDH and 1p/19q Status in MRI: A Systematic Review and Meta-Analysis**
2411.02426v1 by Somayeh Farahani, Marjaneh Hejazi, Mehnaz Tabassum, Antonio Di Ieva, Neda Mahdavifar, Sidong Liu

Gliomas, the most common primary brain tumors, show high heterogeneity in
histological and molecular characteristics. Accurate molecular profiling, like
isocitrate dehydrogenase (IDH) mutation and 1p/19q codeletion, is critical for
diagnosis, treatment, and prognosis. This review evaluates MRI-based deep
learning (DL) models' efficacy in predicting these biomarkers. Following PRISMA
guidelines, we systematically searched major databases (PubMed, Scopus, Ovid,
and Web of Science) up to February 2024, screening studies that utilized DL to
predict IDH and 1p/19q codeletion status from MRI data of glioma patients. We
assessed the quality and risk of bias using the radiomics quality score and
QUADAS-2 tool. Our meta-analysis used a bivariate model to compute pooled
sensitivity, specificity, and meta-regression to assess inter-study
heterogeneity. Of the 565 articles, 57 were selected for qualitative synthesis,
and 52 underwent meta-analysis. The pooled estimates showed high diagnostic
performance, with validation sensitivity, specificity, and area under the curve
(AUC) of 0.84 [prediction interval (PI): 0.67-0.93, I2=51.10%, p < 0.05], 0.87
[PI: 0.49-0.98, I2=82.30%, p < 0.05], and 0.89 for IDH prediction, and 0.76
[PI: 0.28-0.96, I2=77.60%, p < 0.05], 0.85 [PI: 0.49-0.97, I2=80.30%, p <
0.05], and 0.90 for 1p/19q prediction, respectively. Meta-regression analyses
revealed significant heterogeneity influenced by glioma grade, data source,
inclusion of non-radiomics data, MRI sequences, segmentation and feature
extraction methods, and validation techniques. DL models demonstrate strong
potential in predicting molecular biomarkers from MRI scans, with significant
variability influenced by technical and clinical factors. Thorough external
validation is necessary to increase clinical utility.

ÊëòË¶ÅÔºöËÜ†Ë≥™Áò§ÊòØÊúÄÂ∏∏Ë¶ãÁöÑÂéüÁôºÊÄßËÖ¶ËÖ´Áò§ÔºåÂú®ÁµÑÁπîÂ≠∏ÂíåÂàÜÂ≠êÁâπÂæµ‰∏äË°®ÁèæÂá∫È´òÂ∫¶Áï∞Ë≥™ÊÄß„ÄÇÊ∫ñÁ¢∫ÁöÑÂàÜÂ≠êÂàÜÊûêÔºåÂ¶ÇÁï∞Ê™∏Ê™¨ÈÖ∏ËÑ´Ê∞´ÈÖ∂ (IDH) Á™ÅËÆäÂíå 1p/19q ÂÖ±Áº∫Â§±ÔºåÂ∞çÊñºË®∫Êñ∑„ÄÅÊ≤ªÁôÇÂíåÈ†êÂæåËá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á∂úËø∞Ë©ï‰º∞‰∫ÜÂü∫Êñº MRI ÁöÑÊ∑±Â∫¶Â≠∏Áøí (DL) Ê®°ÂûãÂú®È†êÊ∏¨ÈÄô‰∫õÁîüÁâ©Ê®ôË™åÁâ©ÊñπÈù¢ÁöÑÊïàËÉΩ„ÄÇÊåâÁÖß PRISMA ÊåáÂçóÔºåÊàëÂÄëÁ≥ªÁµ±Âú∞ÊêúÂ∞ã‰∫Ü‰∏ªË¶ÅË≥áÊñôÂ∫´ÔºàPubMed„ÄÅScopus„ÄÅOvid Âíå Web of ScienceÔºâÔºåÊôÇÈñìÊà™Ëá≥ 2024 Âπ¥ 2 ÊúàÔºåÁØ©ÈÅ∏‰∫ÜÂà©Áî® DL ÂæûËÜ†Ë≥™Áò§ÊÇ£ËÄÖÁöÑ MRI Ë≥áÊñô‰∏≠È†êÊ∏¨ IDH Âíå 1p/19q ÂÖ±Áº∫Â§±ÁãÄÊÖãÁöÑÁ†îÁ©∂„ÄÇÊàëÂÄë‰ΩøÁî®ÊîæÂ∞ÑÁµÑÂ≠∏ÂìÅË≥™Ë©ïÂàÜÂíå QUADAS-2 Â∑•ÂÖ∑Ë©ï‰º∞‰∫ÜÂìÅË≥™ÂíåÂÅèÂ∑ÆÈ¢®Èö™„ÄÇÊàëÂÄëÁöÑÂæåË®≠ÂàÜÊûê‰ΩøÁî®ÈõôËÆäÊï∏Ê®°Âûã‰æÜË®àÁÆóÂêà‰ΩµÊïèÊÑüÊÄß„ÄÅÁâπÁï∞ÊÄßÂíåÂæåË®≠Ëø¥Ê≠∏Ôºå‰ª•Ë©ï‰º∞Á†îÁ©∂ÈñìÁï∞Ë≥™ÊÄß„ÄÇÂú® 565 ÁØáÊñáÁ´†‰∏≠ÔºåÊúâ 57 ÁØáË¢´ÈÅ∏Áî®ÈÄ≤Ë°åÂÆöÊÄßÁ∂úÂêàÔºå52 ÁØáÈÄ≤Ë°å‰∫ÜÂæåË®≠ÂàÜÊûê„ÄÇÂêà‰Ωµ‰º∞Ë®àÂÄºÈ°ØÁ§∫Âá∫ÂæàÈ´òÁöÑË®∫Êñ∑ÊïàËÉΩÔºåÈ©óË≠âÊïèÊÑüÊÄß„ÄÅÁâπÁï∞ÊÄßÂíåÊõ≤Á∑ö‰∏ãÈù¢Á©ç (AUC) ÂàÜÂà•ÁÇ∫ 0.84 [È†êÊ∏¨ÂçÄÈñì (PI)Ôºö0.67-0.93ÔºåI2=51.10%Ôºåp < 0.05]„ÄÅ0.87 [PIÔºö0.49-0.98ÔºåI2=82.30%Ôºåp < 0.05] Âíå 0.89ÔºåÁî®Êñº IDH È†êÊ∏¨Ôºõ0.76 [PIÔºö0.28-0.96ÔºåI2=77.60%Ôºåp < 0.05]„ÄÅ0.85 [PIÔºö0.49-0.97ÔºåI2=80.30%Ôºåp < 0.05] Âíå 0.90ÔºåÁî®Êñº 1p/19q È†êÊ∏¨„ÄÇÂæåË®≠Ëø¥Ê≠∏ÂàÜÊûêÈ°ØÁ§∫ÔºåËÜ†Ë≥™Áò§ÂàÜÁ¥ö„ÄÅË≥áÊñô‰æÜÊ∫ê„ÄÅÊòØÂê¶ÂåÖÂê´ÈùûÊîæÂ∞ÑÁµÑÂ≠∏Ë≥áÊñô„ÄÅMRI Â∫èÂàó„ÄÅÂàÜÂâ≤ÂíåÁâπÂæµÊèêÂèñÊñπÊ≥ï‰ª•ÂèäÈ©óË≠âÊäÄË°ìÁ≠âÂõ†Á¥†ÊúÉÈÄ†ÊàêÈ°ØËëóÁöÑÁï∞Ë≥™ÊÄß„ÄÇDL Ê®°ÂûãÂ±ïÁ§∫‰∫ÜÂæû MRI ÊéÉÊèè‰∏≠È†êÊ∏¨ÂàÜÂ≠êÁîüÁâ©Ê®ôË™åÁâ©ÁöÑÂº∑Â§ßÊΩõÂäõÔºå‰ΩÜÊäÄË°ìÂíåËá®Â∫äÂõ†Á¥†ÊúÉÈÄ†ÊàêÈ°ØËëóÁöÑËÆäÁï∞ÊÄß„ÄÇÂæπÂ∫ïÁöÑÂ§ñÈÉ®ÂàÜÈ©óË≠âÂ∞çÊñºÊèêÈ´òËá®Â∫äÊïàÁî®ÊòØÂøÖË¶ÅÁöÑ„ÄÇ

##### **Informed Deep Abstaining Classifier: Investigating noise-robust training for diagnostic decision support systems**
2410.21014v1 by Helen Schneider, Sebastian Nowak, Aditya Parikh, Yannik C. Layer, Maike Theis, Wolfgang Block, Alois M. Sprinkart, Ulrike Attenberger, Rafet Sifa

Image-based diagnostic decision support systems (DDSS) utilizing deep
learning have the potential to optimize clinical workflows. However, developing
DDSS requires extensive datasets with expert annotations and is therefore
costly. Leveraging report contents from radiological data bases with Natural
Language Processing to annotate the corresponding image data promises to
replace labor-intensive manual annotation. As mining "real world" databases can
introduce label noise, noise-robust training losses are of great interest.
However, current noise-robust losses do not consider noise estimations that can
for example be derived based on the performance of the automatic label
generator used. In this study, we expand the noise-robust Deep Abstaining
Classifier (DAC) loss to an Informed Deep Abstaining Classifier (IDAC) loss by
incorporating noise level estimations during training. Our findings demonstrate
that IDAC enhances the noise robustness compared to DAC and several
state-of-the-art loss functions. The results are obtained on various simulated
noise levels using a public chest X-ray data set. These findings are reproduced
on an in-house noisy data set, where labels were extracted from the clinical
systems of the University Hospital Bonn by a text-based transformer. The IDAC
can therefore be a valuable tool for researchers, companies or clinics aiming
to develop accurate and reliable DDSS from routine clinical data.

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®Ê∑±Â∫¶Â≠∏ÁøíÁöÑÂΩ±ÂÉèË®∫Êñ∑Ê±∫Á≠ñÊîØÊè¥Á≥ªÁµ± (DDSS) ÊúâÂèØËÉΩÊúÄ‰Ω≥ÂåñËá®Â∫äÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÁÑ∂ËÄåÔºåÈñãÁôº DDSS ÈúÄË¶ÅÂ§ßÈáèÂÖ∑ÂÇôÂ∞àÂÆ∂Ë®ªËß£ÁöÑË≥áÊñôÈõÜÔºåÂõ†Ê≠§ÊàêÊú¨È´òÊòÇ„ÄÇÂà©Áî®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂæûÊîæÂ∞ÑÁßëË≥áÊñôÂ∫´ÁöÑÂ†±ÂëäÂÖßÂÆπ‰∏≠Ê®ôË®ªÂ∞çÊáâÁöÑÂΩ±ÂÉèË≥áÊñôÔºåÊúâÊúõÂèñ‰ª£ÂãûÂäõÂØÜÈõÜÁöÑÊâãÂãïÊ®ôË®ª„ÄÇÁî±ÊñºÊåñÊéò„ÄåÁúüÂØ¶‰∏ñÁïå„ÄçË≥áÊñôÂ∫´ÂèØËÉΩÊúÉÂºïÂÖ•Ê®ôÁ±§ÈõúË®äÔºåÂõ†Ê≠§Â∞çÈõúË®äÁ©©ÂÅ•ÁöÑË®ìÁ∑¥ÊêçÂ§±ÈùûÂ∏∏ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÂ∞çÈõúË®äÁ©©ÂÅ•ÁöÑÊêçÂ§±ÂáΩÊï∏‰∏¶Êú™ËÄÉÊÖÆÈõúË®ä‰º∞Ë®àÔºå‰æãÂ¶ÇÂèØ‰ª•Ê†πÊìöÊâÄ‰ΩøÁî®ÁöÑËá™ÂãïÊ®ôÁ±§Áî¢ÁîüÂô®ÁöÑÊïàËÉΩÊé®Â∞éÂá∫‰æÜ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂú®Ë®ìÁ∑¥ÊúüÈñìÁ¥çÂÖ•ÈõúË®äÁ≠âÁ¥ö‰º∞Ë®àÔºåÂ∞áÂ∞çÈõúË®äÁ©©ÂÅ•ÁöÑÊ∑±Â∫¶Ê£ÑÊ¨äÂàÜÈ°ûÂô® (DAC) ÊêçÂ§±ÂáΩÊï∏Êì¥ÂÖÖÁÇ∫ÊòéÊô∫Ê∑±Â∫¶Ê£ÑÊ¨äÂàÜÈ°ûÂô® (IDAC) ÊêçÂ§±ÂáΩÊï∏„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåËàá DAC ÂíåÂ§öÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑÊêçÂ§±ÂáΩÊï∏Áõ∏ÊØîÔºåIDAC Â¢ûÂº∑‰∫ÜÂ∞çÈõúË®äÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÈÄô‰∫õÁµêÊûúÊòØ‰ΩøÁî®ÂÖ¨ÈñãÁöÑËÉ∏ÈÉ® X ÂÖâË≥áÊñôÈõÜÔºåÂú®ÂêÑÁ®ÆÊ®°Êì¨ÈõúË®äÁ≠âÁ¥ö‰∏≠Áç≤ÂæóÁöÑ„ÄÇÈÄô‰∫õÁ†îÁ©∂ÁµêÊûúÂú®ÂÖßÈÉ®ÈõúË®äË≥áÊñôÈõÜ‰∏äÈáçÁèæÔºåÂÖ∂‰∏≠Ê®ôÁ±§ÊòØÁî±ÊñáÊú¨ËΩâÊèõÂô®ÂæûÊ≥¢ÊÅ©Â§ßÂ≠∏ÈÜ´Èô¢ÁöÑËá®Â∫äÁ≥ªÁµ±‰∏≠ËêÉÂèñÂá∫‰æÜÁöÑ„ÄÇÂõ†Ê≠§ÔºåIDAC ÂèØ‰ª•ÊàêÁÇ∫Á†îÁ©∂‰∫∫Âì°„ÄÅÂÖ¨Âè∏ÊàñË®∫ÊâÄÂæû‰æãË°åËá®Â∫äË≥áÊñôÈñãÁôºÊ∫ñÁ¢∫‰∏îÂèØÈù†ÁöÑ DDSS ÁöÑÊúâÂÉπÂÄºÂ∑•ÂÖ∑„ÄÇ</paragraph>

##### **Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering**
2410.21000v1 by Zhilin Zhang, Jie Wang, Ruiqi Zhu, Xiaoliang Gong

Medical Visual Question Answering (MedVQA) has gained increasing attention at
the intersection of computer vision and natural language processing. Its
capability to interpret radiological images and deliver precise answers to
clinical inquiries positions MedVQA as a valuable tool for supporting
diagnostic decision-making for physicians and alleviating the workload on
radiologists. While recent approaches focus on using unified pre-trained large
models for multi-modal fusion like cross-modal Transformers, research on more
efficient fusion methods remains relatively scarce within this discipline. In
this paper, we introduce a novel fusion model that integrates Orthogonality
loss, Multi-head attention and Bilinear Attention Network (OMniBAN) to achieve
high computational efficiency and strong performance without the need for
pre-training. We conduct comprehensive experiments and clarify aspects of how
to enhance bilinear attention fusion to achieve performance comparable to that
of large models. Experimental results show that OMniBAN outperforms traditional
models on key MedVQA benchmarks while maintaining a lower computational cost,
which indicates its potential for efficient clinical application in radiology
and pathology image question answering.

ÊëòË¶ÅÔºöÈÜ´ÁôÇË¶ñË¶∫ÂïèÁ≠î (MedVQA) Âú®ÈõªËÖ¶Ë¶ñË¶∫ÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑ‰∫§ÈõÜ‰∏≠Áç≤ÂæóË∂ä‰æÜË∂äÂ§öÁöÑÈóúÊ≥®„ÄÇÂÆÉËÉΩÂ§†Ëß£ËÆÄÊîæÂ∞ÑÂΩ±ÂÉè‰∏¶Â∞çËá®Â∫äË©¢ÂïèÊèê‰æõÁ≤æÁ¢∫Á≠îÊ°àÁöÑËÉΩÂäõÔºå‰Ωø MedVQA ÊàêÁÇ∫ÊîØÊè¥ÈÜ´Â∏´Ë®∫Êñ∑Ê±∫Á≠ñÂíåÊ∏õËºïÊîæÂ∞ÑÁßëÈÜ´Â∏´Â∑•‰ΩúË≤†ÊìîÁöÑÂØ∂Ë≤¥Â∑•ÂÖ∑„ÄÇÈõñÁÑ∂ÊúÄËøëÁöÑÊñπÊ≥ïËëóÈáçÊñº‰ΩøÁî®Áµ±‰∏ÄÁöÑÈ†êÂÖàË®ìÁ∑¥Â§ßÂûãÊ®°ÂûãÈÄ≤Ë°åÂ§öÊ®°ÂºèËûçÂêàÔºå‰æãÂ¶ÇË∑®Ê®°ÊÖã TransformerÔºå‰ΩÜÂ∞çÊñºÊõ¥ÊúâÊïàÁéáÁöÑËûçÂêàÊñπÊ≥ïÁöÑÁ†îÁ©∂Âú®Ê≠§È†òÂüü‰∏≠‰ªçÁÑ∂Áõ∏Â∞çÁ®ÄÂ∞ë„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑËûçÂêàÊ®°ÂûãÔºåÂÆÉÊï¥Âêà‰∫ÜÊ≠£‰∫§ÊêçÂ§±„ÄÅÂ§öÈ†≠Ê≥®ÊÑèÂäõÂíåÈõôÁ∑öÊÄßÊ≥®ÊÑèÂäõÁ∂≤Ë∑Ø (OMniBAN)Ôºå‰ª•Âú®‰∏çÈúÄË¶ÅÈ†êÂÖàË®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶ÁèæÈ´òË®àÁÆóÊïàÁéáÂíåÂº∑Â§ßÊïàËÉΩ„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶È©óÔºå‰∏¶ÈáêÊ∏Ö‰∫ÜÂ¶Ç‰ΩïÂ¢ûÂº∑ÈõôÁ∑öÊÄßÊ≥®ÊÑèÂäõËûçÂêà‰ª•ÂØ¶ÁèæËàáÂ§ßÂûãÊ®°ÂûãÁõ∏Áï∂ÁöÑÊïàËÉΩ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåOMniBAN Âú®ÈóúÈçµÁöÑ MedVQA Âü∫Ê∫ñ‰∏äÂÑ™ÊñºÂÇ≥Áµ±Ê®°ÂûãÔºåÂêåÊôÇÁ∂≠ÊåÅËºÉ‰ΩéÁöÑË®àÁÆóÊàêÊú¨ÔºåÈÄôË°®ÊòéÂÆÉÂú®ÊîæÂ∞ÑÂ≠∏ÂíåÁóÖÁêÜÂΩ±ÂÉèÂïèÁ≠î‰∏≠ÂÖ∑ÊúâÈ´òÊïàËá®Â∫äÊáâÁî®ÁöÑÊΩõÂäõ„ÄÇ

##### **Large Language Model Benchmarks in Medical Tasks**
2410.21348v1 by Lawrence K. Q. Yan, Ming Li, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Benji Peng, Ziqian Bi, Pohsun Feng, Keyu Chen, Junyu Liu, Qian Niu

With the increasing application of large language models (LLMs) in the
medical domain, evaluating these models' performance using benchmark datasets
has become crucial. This paper presents a comprehensive survey of various
benchmark datasets employed in medical LLM tasks. These datasets span multiple
modalities including text, image, and multimodal benchmarks, focusing on
different aspects of medical knowledge such as electronic health records
(EHRs), doctor-patient dialogues, medical question-answering, and medical image
captioning. The survey categorizes the datasets by modality, discussing their
significance, data structure, and impact on the development of LLMs for
clinical tasks such as diagnosis, report generation, and predictive decision
support. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and
CheXpert, which have facilitated advancements in tasks like medical report
generation, clinical summarization, and synthetic data generation. The paper
summarizes the challenges and opportunities in leveraging these benchmarks for
advancing multimodal medical intelligence, emphasizing the need for datasets
with a greater degree of language diversity, structured omics data, and
innovative approaches to synthesis. This work also provides a foundation for
future research in the application of LLMs in medicine, contributing to the
evolving field of medical artificial intelligence.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÜ´ÁôÇÈ†òÂüüÁöÑÊáâÁî®Êó•ÁõäÂª£Ê≥õÔºå‰ΩøÁî®Âü∫Ê∫ñË≥áÊñôÈõÜË©ï‰º∞ÈÄô‰∫õÊ®°ÂûãÁöÑÊïàËÉΩÂ∑≤ËÆäÂæóËá≥ÈóúÈáçË¶Å„ÄÇÊú¨ÊñáÂ∞çÁî®ÊñºÈÜ´ÁôÇ LLM ‰ªªÂãôÁöÑÂêÑÁ®ÆÂü∫Ê∫ñË≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑË™øÊü•„ÄÇÈÄô‰∫õË≥áÊñôÈõÜË∑®Ë∂äÂ§öÁ®ÆÊ®°ÂºèÔºåÂåÖÊã¨ÊñáÂ≠ó„ÄÅÂΩ±ÂÉèÂíåÂ§öÊ®°ÊÖãÂü∫Ê∫ñÔºåÈáçÈªûÈóúÊ≥®ÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR)„ÄÅÈÜ´ÁóÖÂ∞çË©±„ÄÅÈÜ´ÁôÇÂïèÁ≠îÂíåÈÜ´ÁôÇÂΩ±ÂÉèÊ®ôÈ°åÁ≠âÈÜ´ÁôÇÁü•Ë≠òÁöÑ‰∏çÂêåÈù¢Âêë„ÄÇË™øÊü•ÊåâÊ®°ÂºèÂ∞çË≥áÊñôÈõÜÈÄ≤Ë°åÂàÜÈ°ûÔºåË®éË´ñÂÆÉÂÄëÁöÑÈáçË¶ÅÊÄß„ÄÅË≥áÊñôÁµêÊßãÂíåÂ∞çÁî®ÊñºË®∫Êñ∑„ÄÅÂ†±ÂëäÁîüÊàêÂíåÈ†êÊ∏¨ÊÄßÊ±∫Á≠ñÊîØÊè¥Á≠âËá®Â∫ä‰ªªÂãôÁöÑ LLM ÈñãÁôºÁöÑÂΩ±Èüø„ÄÇ‰∏ªË¶ÅÂü∫Ê∫ñÂåÖÊã¨ MIMIC-III„ÄÅMIMIC-IV„ÄÅBioASQ„ÄÅPubMedQA Âíå CheXpertÔºåÂÆÉÂÄë‰øÉËøõ‰∫ÜÈÜ´ÁôÇÂ†±ÂëäÁîüÊàê„ÄÅËá®Â∫äÊëòË¶ÅÂíåÂêàÊàêË≥áÊñôÁîüÊàêÁ≠â‰ªªÂãôÁöÑÈÄ≤Â±ï„ÄÇÊú¨ÊñáÁ∏ΩÁµê‰∫ÜÂà©Áî®ÈÄô‰∫õÂü∫Ê∫ñ‰æÜÊé®ÈÄ≤Â§öÊ®°ÊÖãÈÜ´ÁôÇÊô∫ËÉΩÁöÑÊåëÊà∞ÂíåÊ©üÈÅáÔºåÂº∑Ë™ø‰∫ÜÂ∞çÂÖ∑ÊúâÊõ¥Â§ßË™ûË®ÄÂ§öÊ®£ÊÄß„ÄÅÁµêÊßãÂåñÁµÑÂ≠∏Ë≥áÊñôÂíåÂâµÊñ∞ÂêàÊàêÊñπÊ≥ïÁöÑË≥áÊñôÈõÜÁöÑÈúÄÊ±Ç„ÄÇÈÄôÈ†ÖÂ∑•‰Ωú‰πüÁÇ∫ LLM Âú®ÈÜ´Â≠∏‰∏≠ÁöÑÊáâÁî®Êèê‰æõ‰∫ÜÊú™‰æÜÁ†îÁ©∂ÁöÑÂü∫Á§éÔºåÁÇ∫ÈÜ´ÁôÇ‰∫∫Â∑•Êô∫ÊÖßÁöÑÊºîÈÄ≤È†òÂüüÂÅöÂá∫Ë≤¢Áçª„ÄÇ

##### **Vascular Segmentation of Functional Ultrasound Images using Deep Learning**
2410.22365v1 by Hana Sebia, Thomas Guyet, Micka√´l Pereira, Marco Valdebenito, Hugues Berry, Benjamin Vidal

Segmentation of medical images is a fundamental task with numerous
applications. While MRI, CT, and PET modalities have significantly benefited
from deep learning segmentation techniques, more recent modalities, like
functional ultrasound (fUS), have seen limited progress. fUS is a non invasive
imaging method that measures changes in cerebral blood volume (CBV) with high
spatio-temporal resolution. However, distinguishing arterioles from venules in
fUS is challenging due to opposing blood flow directions within the same pixel.
Ultrasound localization microscopy (ULM) can enhance resolution by tracking
microbubble contrast agents but is invasive, and lacks dynamic CBV
quantification. In this paper, we introduce the first deep learning-based
segmentation tool for fUS images, capable of differentiating signals from
different vascular compartments, based on ULM automatic annotation and enabling
dynamic CBV quantification. We evaluate various UNet architectures on fUS
images of rat brains, achieving competitive segmentation performance, with 90%
accuracy, a 71% F1 score, and an IoU of 0.59, using only 100 temporal frames
from a fUS stack. These results are comparable to those from tubular structure
segmentation in other imaging modalities. Additionally, models trained on
resting-state data generalize well to images captured during visual
stimulation, highlighting robustness. This work offers a non-invasive,
cost-effective alternative to ULM, enhancing fUS data interpretation and
improving understanding of vessel function. Our pipeline shows high linear
correlation coefficients between signals from predicted and actual compartments
in both cortical and deeperregions, showcasing its ability to accurately
capture blood flow dynamics.

ÊëòË¶ÅÔºö<paragraph>ÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÂàÜÂâ≤ÊòØÈ†ÖÂü∫Á§é‰ªªÂãôÔºåÊúâË®±Â§öÊáâÁî®„ÄÇÈõñÁÑ∂ MRI„ÄÅCT Âíå PET Á≠âÊñπÂºèÂ∑≤ÂæûÊ∑±Â∫¶Â≠∏ÁøíÂàÜÂâ≤ÊäÄË°ì‰∏≠ÂèóÁõäËâØÂ§öÔºå‰ΩÜÂÉèÂäüËÉΩÊÄßË∂ÖÈü≥Ê≥¢ (fUS) Á≠âËºÉÊñ∞ÁöÑÊñπÂºèÈÄ≤Â±ïÊúâÈôê„ÄÇfUS ÊòØ‰∏ÄÁ®ÆÈùû‰æµÂÖ•ÊÄßÁöÑÂΩ±ÂÉèÊñπÊ≥ïÔºåÂèØÊ∏¨ÈáèËÖ¶Ë°ÄÂÆπÈáè (CBV) ÁöÑËÆäÂåñÔºåÂÖ∑ÊúâÈ´òÊôÇÁ©∫Ëß£ÊûêÂ∫¶„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂêå‰∏ÄÂÄãÂÉèÁ¥†‰∏≠Ë°ÄÊµÅÊñπÂêëÁõ∏ÂèçÔºåÂõ†Ê≠§Âú® fUS ‰∏≠ÂçÄÂàÜÂ∞èÂãïËÑàÂíåÂ∞èÈùúËÑàÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇË∂ÖÈü≥Ê≥¢ÂÆö‰ΩçÈ°ØÂæÆÈè° (ULM) ÂèØ‰ª•ÈÄèÈÅéËøΩËπ§ÂæÆÊ∞£Ê≥°Â∞çÊØîÂäë‰æÜÂ¢ûÂº∑Ëß£ÊûêÂ∫¶Ôºå‰ΩÜÂÖ∑Êúâ‰æµÂÖ•ÊÄßÔºå‰∏îÁº∫‰πèÂãïÊÖã CBV ÈáèÂåñ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÁ¨¨‰∏ÄÂÄãÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑ fUS ÂΩ±ÂÉèÂàÜÂâ≤Â∑•ÂÖ∑ÔºåÂÆÉËÉΩÂ§†Ê†πÊìö ULM Ëá™ÂãïË®ªËß£ÂçÄÂàÜ‰∏çÂêåË°ÄÁÆ°ÂçÄÂÆ§ÁöÑË®äËôüÔºå‰∏¶ÂïüÁî®ÂãïÊÖã CBV ÈáèÂåñ„ÄÇÊàëÂÄëÂú®ËÄÅÈº†Â§ßËÖ¶ÁöÑ fUS ÂΩ±ÂÉè‰∏äË©ï‰º∞‰∫ÜÂêÑÁ®Æ UNet Êû∂ÊßãÔºåÂÉÖ‰ΩøÁî® fUS Â†ÜÁñä‰∏≠ÁöÑ 100 ÂÄãÊôÇÈñìÂπÄÔºåÂ∞±ÈÅîÂà∞‰∫ÜÂÖ∑ÊúâÁ´∂Áà≠ÂäõÁöÑÂàÜÂâ≤ÊïàËÉΩÔºåÊ∫ñÁ¢∫ÁéáÁÇ∫ 90%ÔºåF1 ÂàÜÊï∏ÁÇ∫ 71%ÔºåIoU ÁÇ∫ 0.59„ÄÇÈÄô‰∫õÁµêÊûúËàáÂÖ∂‰ªñÂΩ±ÂÉèÊñπÂºè‰∏≠ÁÆ°ÁãÄÁµêÊßãÂàÜÂâ≤ÁöÑÁµêÊûúÁõ∏Áï∂„ÄÇÊ≠§Â§ñÔºåÂú®ÈùúÊ≠¢ÁãÄÊÖãË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÂèØ‰ª•ÂæàÂ•ΩÂú∞Êé®Âª£Âà∞Âú®Ë¶ñË¶∫Âà∫ÊøÄÊúüÈñìÊì∑ÂèñÁöÑÂΩ±ÂÉèÔºåÁ™ÅÈ°Ø‰∫ÜÂÖ∂Á©©ÂÅ•ÊÄß„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊèê‰æõ‰∫Ü‰∏ÄÂÄãÈùû‰æµÂÖ•ÊÄß„ÄÅÂÖ∑ÊúâÊàêÊú¨ÊïàÁõäÁöÑ ULM Êõø‰ª£ÊñπÊ°àÔºåÂ¢ûÂº∑‰∫Ü fUS Ë≥áÊñôÁöÑË©ÆÈáãÔºå‰∏¶ÊîπÂñÑ‰∫ÜÂ∞çË°ÄÁÆ°ÂäüËÉΩÁöÑÁêÜËß£„ÄÇÊàëÂÄëÁöÑÁÆ°Á∑öÂú®È†êÊ∏¨ÂçÄÂÆ§ÂíåÂØ¶ÈöõÂçÄÂÆ§ÁöÑË®äËôü‰πãÈñìÈ°ØÁ§∫Âá∫ÂæàÈ´òÁöÑÁ∑öÊÄßÁõ∏Èóú‰øÇÊï∏ÔºåÁÑ°Ë´ñÊòØÂú®ÁöÆË≥™ÈÇÑÊòØÊ∑±Â±§ÂçÄÂüüÔºåÈÉΩÂ±ïÁ§∫‰∫ÜÂÖ∂Ê∫ñÁ¢∫ÊçïÊçâË°ÄÊµÅÂãïÊÖãÁöÑËÉΩÂäõ„ÄÇ</paragraph>

##### **Language Models And A Second Opinion Use Case: The Pocket Professional**
2410.20636v1 by David Noever

This research tests the role of Large Language Models (LLMs) as formal second
opinion tools in professional decision-making, particularly focusing on complex
medical cases where even experienced physicians seek peer consultation. The
work analyzed 183 challenging medical cases from Medscape over a 20-month
period, testing multiple LLMs' performance against crowd-sourced physician
responses. A key finding was the high overall score possible in the latest
foundational models (>80% accuracy compared to consensus opinion), which
exceeds most human metrics reported on the same clinical cases (450 pages of
patient profiles, test results). The study rates the LLMs' performance
disparity between straightforward cases (>81% accuracy) and complex scenarios
(43% accuracy), particularly in these cases generating substantial debate among
human physicians. The research demonstrates that LLMs may be valuable as
generators of comprehensive differential diagnoses rather than as primary
diagnostic tools, potentially helping to counter cognitive biases in clinical
decision-making, reduce cognitive loads, and thus remove some sources of
medical error. The inclusion of a second comparative legal dataset (Supreme
Court cases, N=21) provides added empirical context to the AI use to foster
second opinions, though these legal challenges proved considerably easier for
LLMs to analyze. In addition to the original contributions of empirical
evidence for LLM accuracy, the research aggregated a novel benchmark for others
to score highly contested question and answer reliability between both LLMs and
disagreeing human practitioners. These results suggest that the optimal
deployment of LLMs in professional settings may differ substantially from
current approaches that emphasize automation of routine tasks.

ÊëòË¶ÅÔºöÈÄôÈ†ÖÁ†îÁ©∂Ê∏¨Ë©¶‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Â∞àÊ•≠Ê±∫Á≠ñ‰∏≠‰ΩúÁÇ∫Ê≠£ÂºèÁ¨¨‰∫åÊÑèË¶ãÂ∑•ÂÖ∑ÁöÑËßíËâ≤ÔºåÁâπÂà•ËëóÈáçÊñºË§áÈõúÁöÑÈÜ´ÁôÇÊ°à‰æãÔºåÂç≥‰ΩøÁ∂ìÈ©óË±êÂØåÁöÑÈÜ´Â∏´‰πüÊúÉÂ∞ãÊ±ÇÂêåÂÑïË´ÆË©¢„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂàÜÊûê‰∫Ü Medscape Âú® 20 ÂÄãÊúàÊúüÈñìÁöÑ 183 ÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÈÜ´ÁôÇÊ°à‰æãÔºåÊ∏¨Ë©¶Â§öÂÄã LLM ÁöÑË°®ÁèæÔºå‰∏¶ËàáÁæ§ÁúæÂ§ñÂåÖÁöÑÈÜ´Â∏´ÂõûÊáâÈÄ≤Ë°åÊØîËºÉ„ÄÇ‰∏ÄÂÄãÈóúÈçµÁôºÁèæÊòØÊúÄÊñ∞Âü∫Á§éÊ®°Âûã‰∏≠ÂèØËÉΩÁöÑÈ´òÁ∏ΩÈ´îÂàÜÊï∏ÔºàËàáÂÖ±Ë≠òÊÑèË¶ãÁõ∏ÊØîÔºåÊ∫ñÁ¢∫Áéá >80%ÔºâÔºåÈÄôË∂ÖÈÅé‰∫ÜÈáùÂ∞çÁõ∏ÂêåËá®Â∫äÊ°à‰æãÂ†±ÂëäÁöÑÂ§ßÂ§öÊï∏‰∫∫È°ûÊåáÊ®ôÔºà450 È†ÅÁöÑÊÇ£ËÄÖÊ™îÊ°à„ÄÅÊ™¢È©óÁµêÊûúÔºâ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Ë©ï‰º∞‰∫Ü LLM Âú®Áõ¥Êé•Ê°à‰æãÔºàÊ∫ñÁ¢∫Áéá >81%ÔºâÂíåË§áÈõúÊÉÖÂ¢ÉÔºàÊ∫ñÁ¢∫Áéá 43%Ôºâ‰πãÈñìÁöÑË°®ÁèæÂ∑ÆÁï∞ÔºåÁâπÂà•ÊòØÂú®ÈÄô‰∫õÊ°à‰æã‰∏≠ÔºåÊúÉÂú®‰∫∫È°ûÈÜ´Â∏´ÈñìÁî¢ÁîüÂ§ßÈáèÁöÑËæØË´ñ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Ë≠âÊòéÔºåLLM ÂèØËÉΩÊòØÊúâÂÉπÂÄºÁöÑÂÖ®Èù¢ÈëëÂà•Ë®∫Êñ∑Áî¢ÁîüÂô®ÔºåËÄåÈùû‰∏ªË¶ÅÁöÑË®∫Êñ∑Â∑•ÂÖ∑ÔºåÊΩõÂú®ÊúâÂä©ÊñºÂ∞çÊäóËá®Â∫äÊ±∫Á≠ñ‰∏≠ÁöÑË™çÁü•ÂÅèË™§„ÄÅÊ∏õÂ∞ëË™çÁü•Ë≤†ÊìîÔºå‰∏¶Âõ†Ê≠§Ê∂àÈô§‰∏Ä‰∫õÈÜ´ÁôÇÈåØË™§ÁöÑÊ†πÊ∫ê„ÄÇÂä†ÂÖ•Á¨¨‰∫åÂÄãÊØîËºÉÊ≥ïÂæãË≥áÊñôÈõÜÔºàÊúÄÈ´òÊ≥ïÈô¢Ê°à‰æãÔºåN=21ÔºâÁÇ∫ AI Áî®Êñº‰øÉÈÄ≤Á¨¨‰∫åÊÑèË¶ãÊèê‰æõ‰∫ÜÈ°çÂ§ñÁöÑÁ∂ìÈ©óËÉåÊôØÔºåÂÑòÁÆ°ÈÄô‰∫õÊ≥ïÂæãÊåëÊà∞Ë¢´Ë≠âÊòéÂ∞ç LLM ‰æÜË™™Êõ¥ÂÆπÊòìÂàÜÊûê„ÄÇÈô§‰∫Ü LLM Ê∫ñÁ¢∫ÊÄßÁöÑÁ∂ìÈ©óË≠âÊìöÁöÑÂéüÂßãË≤¢ÁçªÂ§ñÔºåÈÄôÈ†ÖÁ†îÁ©∂ÈÇÑÂåØÁ∏Ω‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Ê∫ñÔºå‰æõÂÖ∂‰ªñ‰∫∫ÁÇ∫ LLM ÂíåÊÑèË¶ãÂàÜÊ≠ßÁöÑ‰∫∫È°ûÂæûÊ•≠‰∫∫Âì°‰πãÈñìÈ´òÂ∫¶ÊúâÁà≠Ë≠∞ÁöÑÂïèÈ°åÂíåÁ≠îÊ°àÁöÑÂèØÈù†ÊÄßÈÄ≤Ë°åË©ïÂàÜ„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºåLLM Âú®Â∞àÊ•≠Áí∞Â¢É‰∏≠ÁöÑÊúÄ‰Ω≥ÈÉ®ÁΩ≤ÂèØËÉΩËàáÂº∑Ë™øËá™ÂãïÂåñ‰æãË°å‰ªªÂãôÁöÑÁï∂ÂâçÊñπÊ≥ïÊúâÂæàÂ§ß‰∏çÂêå„ÄÇ

##### **Improving Decision Sparsity**
2410.20483v1 by Yiyang Sun, Tong Wang, Cynthia Rudin

Sparsity is a central aspect of interpretability in machine learning.
Typically, sparsity is measured in terms of the size of a model globally, such
as the number of variables it uses. However, this notion of sparsity is not
particularly relevant for decision-making; someone subjected to a decision does
not care about variables that do not contribute to the decision. In this work,
we dramatically expand a notion of decision sparsity called the Sparse
Explanation Value(SEV) so that its explanations are more meaningful. SEV
considers movement along a hypercube towards a reference point. By allowing
flexibility in that reference and by considering how distances along the
hypercube translate to distances in feature space, we can derive sparser and
more meaningful explanations for various types of function classes. We present
cluster-based SEV and its variant tree-based SEV, introduce a method that
improves credibility of explanations, and propose algorithms that optimize
decision sparsity in machine learning models.

ÊëòË¶ÅÔºöÁ®ÄÁñèÊÄßÊòØÊ©üÂô®Â≠∏Áøí‰∏≠ÂèØËß£ÈáãÊÄßÁöÑÊ†∏ÂøÉÈù¢Âêë„ÄÇ
‰∏ÄËà¨‰æÜË™™ÔºåÁ®ÄÁñèÊÄßÊòØ‰ª•Ê®°ÂûãÊï¥È´îÁöÑÂ§ßÂ∞è‰æÜË°°ÈáèÔºå‰æãÂ¶ÇÂÆÉ‰ΩøÁî®ÁöÑËÆäÊï∏Êï∏Èáè„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÁ®ÄÁñèÊÄßÊ¶ÇÂøµËàáÊ±∫Á≠ñÂà∂ÂÆö‰∏¶ÁÑ°ÁâπÂà•Áõ∏ÈóúÔºõÂèóÂà∞Ê±∫Á≠ñÂΩ±ÈüøÁöÑ‰∫∫‰∏¶‰∏çÂú®‰πéÈÇ£‰∫õËàáÊ±∫Á≠ñÁÑ°ÈóúÁöÑËÆäÊï∏„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ§ßÂπÖÊì¥Â±ï‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫Á®ÄÁñèËß£ÈáãÂÄº (SEV) ÁöÑÊ±∫Á≠ñÁ®ÄÁñèÊÄßÊ¶ÇÂøµÔºå‰ΩøÂÖ∂Ëß£ÈáãÊõ¥ÂÖ∑ÊÑèÁæ©„ÄÇSEV ËÄÉÈáèÊ≤øËëóË∂ÖÁ´ãÊñπÈ´îÊúùÂêëÂèÉËÄÉÈªûÁöÑÁßªÂãï„ÄÇÈÄèÈÅéÂÖÅË®±Ë©≤ÂèÉËÄÉÁöÑÈùàÊ¥ªÊÄßÔºå‰∏¶ËÄÉÈáèÊ≤øËëóË∂ÖÁ´ãÊñπÈ´îÁöÑË∑ùÈõ¢Â¶Ç‰ΩïËΩâÊèõÁÇ∫ÁâπÂæµÁ©∫Èñì‰∏≠ÁöÑË∑ùÈõ¢ÔºåÊàëÂÄëÂèØ‰ª•ÈáùÂ∞çÂêÑÁ®ÆÂáΩÊï∏È°ûÂà•Êé®Â∞éÂá∫Êõ¥Á®ÄÁñè‰∏îÊõ¥ÊúâÊÑèÁæ©ÁöÑËß£Èáã„ÄÇÊàëÂÄëÊèêÂá∫Âü∫ÊñºÂè¢ÈõÜÁöÑ SEV ÂèäÂÖ∂ËÆäÈ´îÂü∫ÊñºÊ®πÁãÄÁµêÊßãÁöÑ SEVÔºåÂºïÂÖ•‰∏ÄÁ®ÆÊñπÊ≥ï‰æÜÊèêÂçáËß£ÈáãÁöÑÂèØ‰ø°Â∫¶Ôºå‰∏¶ÊèêÂá∫ÊúÄ‰Ω≥ÂåñÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰∏≠Ê±∫Á≠ñÁ®ÄÁñèÊÄßÁöÑÊºîÁÆóÊ≥ï„ÄÇ

##### **MedGo: A Chinese Medical Large Language Model**
2410.20428v1 by Haitao Zhang, Bo An

Large models are a hot research topic in the field of artificial
intelligence. Leveraging their generative capabilities has the potential to
enhance the level and quality of medical services. In response to the
limitations of current large language models, which often struggle with
accuracy and have narrow capabilities in medical applications, this paper
presents a Chinese medical large language model, MedGo. MedGo was trained using
a combination of high quality unsupervised medical data, supervised data, and
preference alignment data, aimed at enhancing both its versatility and
precision in medical tasks. The model was evaluated through the public CBLUE
benchmark and a manually constructed dataset ClinicalQA. The results
demonstrate that MedGo achieved promising performance across various Chinese
medical information processing tasks, achieved the first place in the CBLUE
evaluation. Additionally, on our constructed dataset ClinicalQA, MedGo
outperformed its base model Qwen2, highlighting its potential to improve both
automated medical question answering and clinical decision support. These
experimental results demonstrate that MedGo possesses strong information
processing capabilities in the medical field. At present, we have successfully
deployed MedGo at Shanghai East Hospital.

ÊëòË¶ÅÔºöÂ§ßÂûãÊ®°ÂûãÊòØ‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÁöÑÁ†îÁ©∂ÁÉ≠ÁÇπ„ÄÇÂà©Áî®ÂÆÉ‰ª¨ÁîüÊàêÁöÑËÉΩÂäõÊúâÂèØËÉΩÊèêÈ´òÂåªÁñóÊúçÂä°ÁöÑÊ∞¥Âπ≥ÂíåË¥®Èáè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ÂΩìÂâçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄßÔºåËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏Èöæ‰ª•ËææÂà∞ÂáÜÁ°ÆÊÄßÔºåÂπ∂‰∏îÂú®ÂåªÁñóÂ∫îÁî®‰∏≠ÁöÑËÉΩÂäõËæÉÁ™ÑÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰∏≠ÊñáÂåªÁñóÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã MedGo„ÄÇMedGo ‰ΩøÁî®È´òË¥®ÈáèÊó†ÁõëÁù£ÂåªÁñóÊï∞ÊçÆ„ÄÅÁõëÁù£Êï∞ÊçÆÂíåÂÅèÂ•ΩÂØπÈΩêÊï∞ÊçÆÁöÑÁªÑÂêàËøõË°åËÆ≠ÁªÉÔºåÊó®Âú®Â¢ûÂº∫ÂÖ∂Âú®ÂåªÁñó‰ªªÂä°‰∏≠ÁöÑÂ§öÂäüËÉΩÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂÖ¨ÂÖ± CBLUE Âü∫ÂáÜÂíåÊâãÂä®ÊûÑÂª∫ÁöÑÊï∞ÊçÆÈõÜ ClinicalQA ËøõË°å‰∫ÜËØÑ‰º∞„ÄÇÁªìÊûúË°®ÊòéÔºåMedGo Âú®ÂêÑÁßç‰∏≠ÊñáÂåªÁñó‰ø°ÊÅØÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÂèØÂñúÁöÑÊÄßËÉΩÔºåÂú® CBLUE ËØÑ‰º∞‰∏≠ÂèñÂæó‰∫ÜÁ¨¨‰∏ÄÂêç„ÄÇÊ≠§Â§ñÔºåÂú®Êàë‰ª¨ÁöÑÊûÑÂª∫Êï∞ÊçÆÈõÜ ClinicalQA ‰∏äÔºåMedGo ‰ºò‰∫éÂÖ∂Âü∫Á°ÄÊ®°Âûã Qwen2ÔºåÁ™ÅÂá∫‰∫ÜÂÖ∂Âú®ÊîπËøõËá™Âä®ÂåñÂåªÁñóÈóÆÈ¢òËß£Á≠îÂíå‰∏¥Â∫äÂÜ≥Á≠ñÊîØÊåÅÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇËøô‰∫õÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMedGo Âú®ÂåªÁñóÈ¢ÜÂüüÊã•ÊúâÂº∫Â§ßÁöÑ‰ø°ÊÅØÂ§ÑÁêÜËÉΩÂäõ„ÄÇÁõÆÂâçÔºåÊàë‰ª¨Â∑≤ÊàêÂäüÂú®‰∏äÊµ∑‰∏úÊñπÂåªÈô¢ÈÉ®ÁΩ≤‰∫Ü MedGo„ÄÇ

##### **Addressing the Pitfalls of Image-Based Structural Health Monitoring: A Focus on False Positives, False Negatives, and Base Rate Bias**
2410.20384v1 by Vagelis Plevris

This study explores the limitations of image-based structural health
monitoring (SHM) techniques in detecting structural damage. Leveraging machine
learning and computer vision, image-based SHM offers a scalable and efficient
alternative to manual inspections. However, its reliability is impacted by
challenges such as false positives, false negatives, and environmental
variability, particularly in low base rate damage scenarios. The Base Rate Bias
plays a significant role, as low probabilities of actual damage often lead to
misinterpretation of positive results. This study uses both Bayesian analysis
and a frequentist approach to evaluate the precision of damage detection
systems, revealing that even highly accurate models can yield misleading
results when the occurrence of damage is rare. Strategies for mitigating these
limitations are discussed, including hybrid systems that combine multiple data
sources, human-in-the-loop approaches for critical assessments, and improving
the quality of training data. These findings provide essential insights into
the practical applicability of image-based SHM techniques, highlighting both
their potential and their limitations for real-world infrastructure monitoring.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÂü∫ÊñºÂΩ±ÂÉèÁöÑÁµêÊßãÂÅ•Â∫∑Áõ£Ê∏¨ (SHM) ÊäÄË°ìÂú®Ê™¢Ê∏¨ÁµêÊßãÊêçÂ£ûÊñπÈù¢ÁöÑÈôêÂà∂„ÄÇËóâÁî±Ê©üÂô®Â≠∏ÁøíÂíåÈõªËÖ¶Ë¶ñË¶∫ÔºåÂü∫ÊñºÂΩ±ÂÉèÁöÑ SHM Êèê‰æõ‰∫ÜÂèØÊì¥ÂÖÖ‰∏îÊúâÊïàÁéáÁöÑÊõø‰ª£‰∫∫Â∑•Ê™¢Êü•ÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂÖ∂ÂèØÈù†ÊÄßÂèóÂà∞ÊåëÊà∞ÁöÑÂΩ±ÈüøÔºå‰æãÂ¶ÇÂÅáÈôΩÊÄß„ÄÅÂÅáÈô∞ÊÄßÔºå‰ª•ÂèäÁí∞Â¢ÉËÆäÁï∞ÊÄßÔºåÁâπÂà•ÊòØÂú®‰ΩéÂü∫Â∫ïÊêçÂ£ûÊÉÖÂ¢É‰∏≠„ÄÇÂü∫Â∫ïÊØîÁéáÂÅèÂ∑ÆÊâÆÊºî‰∫ÜÈáçË¶ÅÁöÑËßíËâ≤ÔºåÂõ†ÁÇ∫ÂØ¶ÈöõÊêçÂ£ûÁöÑ‰ΩéÊ©üÁéáÂ∏∏Â∏∏Â∞éËá¥Â∞çÊñºÈôΩÊÄßÁµêÊûúÁöÑË™§Ëß£„ÄÇÊú¨Á†îÁ©∂ÂêåÊôÇ‰ΩøÁî®Ë≤ùÊ∞èÂàÜÊûêÂíåÈ†ªÁéáË´ñÊñπÊ≥ï‰æÜË©ï‰º∞ÊêçÂ£ûÊ™¢Ê∏¨Á≥ªÁµ±ÁöÑÁ≤æÊ∫ñÂ∫¶ÔºåÊè≠Á§∫Âç≥‰ΩøÈ´òÂ∫¶Á≤æÁ¢∫ÁöÑÊ®°ÂûãÂú®ÊêçÂ£ûÁôºÁîüÁéáÁ®ÄÂ∞ëÊôÇ‰πüÂèØËÉΩÁî¢ÁîüË™§Â∞éÊÄßÁöÑÁµêÊûú„ÄÇË®éË´ñ‰∫ÜÊ∏õËºïÈÄô‰∫õÈôêÂà∂ÁöÑÁ≠ñÁï•ÔºåÂåÖÊã¨ÁµêÂêàÂ§öÈáçË≥áÊñô‰æÜÊ∫êÁöÑÊ∑∑ÂêàÁ≥ªÁµ±„ÄÅÂ∞çÊñºÈóúÈçµË©ï‰º∞ÁöÑ‰∫∫È°û‰ªãÂÖ•ÊñπÊ≥ïÔºå‰ª•ÂèäÊîπÂñÑË®ìÁ∑¥Ë≥áÊñôÂìÅË≥™„ÄÇÈÄô‰∫õÁôºÁèæÊèê‰æõ‰∫ÜÂ∞çÊñºÂü∫ÊñºÂΩ±ÂÉèÁöÑ SHM ÊäÄË°ìÂØ¶ÂãôÈÅ©Áî®ÊÄßÁöÑÈáçË¶ÅË¶ãËß£ÔºåÁ™ÅÈ°Ø‰∫ÜÂÆÉÂÄëÂú®ÁèæÂØ¶‰∏ñÁïåÂü∫Á§éË®≠ÊñΩÁõ£Ê∏¨ÊñπÈù¢ÁöÑÊΩõÂäõÂíåÈôêÂà∂„ÄÇ

