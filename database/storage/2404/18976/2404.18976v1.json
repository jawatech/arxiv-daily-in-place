{"2404.18976": {"publish_time": "2024-04-29", "title": "Foundations of Multisensory Artificial Intelligence", "paper_summary": "Building multisensory AI systems that learn from multiple sensory inputs such\nas text, speech, video, real-world sensors, wearable devices, and medical data\nholds great promise for impact in many scientific areas with practical\nbenefits, such as in supporting human health and well-being, enabling\nmultimedia content processing, and enhancing real-world autonomous agents. By\nsynthesizing a range of theoretical frameworks and application domains, this\nthesis aims to advance the machine learning foundations of multisensory AI. In\nthe first part, we present a theoretical framework formalizing how modalities\ninteract with each other to give rise to new information for a task. These\ninteractions are the basic building blocks in all multimodal problems, and\ntheir quantification enables users to understand their multimodal datasets,\ndesign principled approaches to learn these interactions, and analyze whether\ntheir model has succeeded in learning. In the second part, we study the design\nof practical multimodal foundation models that generalize over many modalities\nand tasks, which presents a step toward grounding large language models to\nreal-world sensory modalities. We introduce MultiBench, a unified large-scale\nbenchmark across a wide range of modalities, tasks, and research areas,\nfollowed by the cross-modal attention and multimodal transformer architectures\nthat now underpin many of today's multimodal foundation models. Scaling these\narchitectures on MultiBench enables the creation of general-purpose\nmultisensory AI systems, and we discuss our collaborative efforts in applying\nthese models for real-world impact in affective computing, mental health,\ncancer prognosis, and robotics. Finally, we conclude this thesis by discussing\nhow future work can leverage these ideas toward more general, interactive, and\nsafe multisensory AI.", "paper_summary_zh": "<paragraph>\u5efa\u7acb\u591a\u611f\u77e5\u4eba\u5de5\u667a\u6167\u7cfb\u7d71\uff0c\u5f9e\u591a\u7a2e\u611f\u5b98\u8f38\u5165\u4e2d\u5b78\u7fd2\uff0c\u4f8b\u5982\u6587\u5b57\u3001\u8a9e\u97f3\u3001\u5f71\u7247\u3001\u771f\u5be6\u4e16\u754c\u7684\u611f\u6e2c\u5668\u3001\u7a7f\u6234\u5f0f\u88dd\u7f6e\u548c\u91ab\u7642\u8cc7\u6599\uff0c\u5c0d\u65bc\u5728\u8a31\u591a\u79d1\u5b78\u9818\u57df\u4e2d\u7522\u751f\u5f71\u97ff\u529b\u5177\u6709\u6975\u5927\u7684\u5e0c\u671b\uff0c\u4e26\u5177\u6709\u5be6\u969b\u6548\u76ca\uff0c\u4f8b\u5982\u5728\u652f\u63f4\u4eba\u985e\u5065\u5eb7\u548c\u798f\u7949\u3001\u555f\u7528\u591a\u5a92\u9ad4\u5167\u5bb9\u8655\u7406\u548c\u589e\u5f37\u771f\u5be6\u4e16\u754c\u7684\u81ea\u4e3b\u4ee3\u7406\u65b9\u9762\u3002\u900f\u904e\u7d9c\u5408\u4e00\u7cfb\u5217\u7684\u7406\u8ad6\u67b6\u69cb\u548c\u61c9\u7528\u9818\u57df\uff0c\u672c\u8ad6\u6587\u65e8\u5728\u63a8\u9032\u591a\u611f\u77e5\u4eba\u5de5\u667a\u6167\u7684\u6a5f\u5668\u5b78\u7fd2\u57fa\u790e\u3002\u5728\u7b2c\u4e00\u90e8\u5206\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7406\u8ad6\u67b6\u69cb\uff0c\u5c07\u6a21\u614b\u5982\u4f55\u5f7c\u6b64\u4e92\u52d5\u4ee5\u7522\u751f\u4efb\u52d9\u7684\u65b0\u8cc7\u8a0a\u5f62\u5f0f\u5316\u3002\u9019\u4e9b\u4e92\u52d5\u662f\u6240\u6709\u591a\u6a21\u614b\u554f\u984c\u4e2d\u7684\u57fa\u672c\u5efa\u69cb\u6a21\u7d44\uff0c\u800c\u5b83\u5011\u7684\u91cf\u5316\u4f7f\u7528\u6236\u80fd\u5920\u4e86\u89e3\u5176\u591a\u6a21\u614b\u8cc7\u6599\u96c6\uff0c\u8a2d\u8a08\u539f\u5247\u5316\u65b9\u6cd5\u4f86\u5b78\u7fd2\u9019\u4e9b\u4e92\u52d5\uff0c\u4e26\u5206\u6790\u5176\u6a21\u578b\u662f\u5426\u5df2\u6210\u529f\u5b78\u7fd2\u3002\u5728\u7b2c\u4e8c\u90e8\u5206\uff0c\u6211\u5011\u7814\u7a76\u5be6\u7528\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u7684\u8a2d\u8a08\uff0c\u9019\u4e9b\u6a21\u578b\u6982\u62ec\u4e86\u8a31\u591a\u6a21\u614b\u548c\u4efb\u52d9\uff0c\u9019\u8868\u793a\u671d\u5411\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5960\u57fa\u65bc\u771f\u5be6\u4e16\u754c\u7684\u611f\u5b98\u6a21\u614b\u9081\u9032\u4e00\u6b65\u3002\u6211\u5011\u4ecb\u7d39 MultiBench\uff0c\u4e00\u500b\u7d71\u4e00\u7684\u5927\u898f\u6a21\u57fa\u6e96\uff0c\u6db5\u84cb\u5404\u7a2e\u6a21\u614b\u3001\u4efb\u52d9\u548c\u7814\u7a76\u9818\u57df\uff0c\u63a5\u8457\u662f\u8de8\u6a21\u614b\u6ce8\u610f\u529b\u548c\u591a\u6a21\u614b\u8f49\u63db\u5668\u67b6\u69cb\uff0c\u9019\u4e9b\u67b6\u69cb\u73fe\u5728\u652f\u6490\u8457\u7576\u4eca\u8a31\u591a\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u3002\u5728 MultiBench \u4e0a\u64f4\u5145\u9019\u4e9b\u67b6\u69cb\u80fd\u5920\u5efa\u7acb\u901a\u7528\u591a\u611f\u5b98\u4eba\u5de5\u667a\u6167\u7cfb\u7d71\uff0c\u6211\u5011\u8a0e\u8ad6\u6211\u5011\u5728\u61c9\u7528\u9019\u4e9b\u6a21\u578b\u4ee5\u5c0d\u60c5\u611f\u904b\u7b97\u3001\u5fc3\u7406\u5065\u5eb7\u3001\u764c\u75c7\u9810\u5f8c\u548c\u6a5f\u5668\u4eba\u6280\u8853\u7522\u751f\u771f\u5be6\u4e16\u754c\u5f71\u97ff\u65b9\u9762\u7684\u5408\u4f5c\u52aa\u529b\u3002\u6700\u5f8c\uff0c\u6211\u5011\u900f\u904e\u8a0e\u8ad6\u672a\u4f86\u7684\u5de5\u4f5c\u5982\u4f55\u5229\u7528\u9019\u4e9b\u60f3\u6cd5\u671d\u5411\u66f4\u901a\u7528\u3001\u4e92\u52d5\u4e14\u5b89\u5168\u7684\u611f\u77e5\u4eba\u5de5\u667a\u6167\u4f86\u7e3d\u7d50\u672c\u8ad6\u6587\u3002</paragraph>", "author": "Paul Pu Liang et.al.", "authors": "Paul Pu Liang", "id": "2404.18976v1", "paper_url": "http://arxiv.org/abs/2404.18976v1", "repo": "null"}}