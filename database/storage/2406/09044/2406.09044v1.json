{"2406.09044": {"publish_time": "2024-06-13", "title": "MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning", "paper_summary": "Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computation and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with gaussian distribution and zero values,\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might have interference with the\nwell-learned subspace of the pretrained weight matrix. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprinciple singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principle matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principle matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nfinetuning dataset. Extensive experiments on commonsense reasoning, math\nreasoning and instruction following benchmarks present the superior performance\nof our method.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6709\u6548\u5fae\u8abf\u65e8\u5728\u4ee5\u964d\u4f4e\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u6210\u672c\u7684\u65b9\u5f0f\u8abf\u6574 LLM\u3002\u5148\u524d\u7684\u57fa\u65bc LoRA \u7684\u65b9\u6cd5\u4f7f\u7528\u9ad8\u65af\u5206\u4f48\u548c\u96f6\u503c\u521d\u59cb\u5316\u4f4e\u79e9\u77e9\u9663\uff0c\u540c\u6642\u4fdd\u6301\u539f\u59cb\u6b0a\u91cd\u77e9\u9663\u51cd\u7d50\u3002\u7136\u800c\uff0c\u5728\u4e0d\u53d7\u5f15\u5c0e\u7684\u5b50\u7a7a\u9593\u4e2d\u6700\u4f73\u5316\u7684\u53ef\u8a13\u7df4\u6a21\u578b\u53c3\u6578\u53ef\u80fd\u6703\u8207\u9810\u8a13\u7df4\u6b0a\u91cd\u77e9\u9663\u7684\u826f\u597d\u5b78\u7fd2\u5b50\u7a7a\u9593\u7522\u751f\u5e72\u64fe\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa MiLoRA\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684 LLM \u5fae\u8abf\u65b9\u6cd5\uff0c\u50c5\u66f4\u65b0\u6b0a\u91cd\u77e9\u9663\u7684\u6b21\u8981\u5947\u7570\u5206\u91cf\uff0c\u540c\u6642\u4fdd\u6301\u4e3b\u8981\u5947\u7570\u5206\u91cf\u51cd\u7d50\u3002\u53ef\u4ee5\u89c0\u5bdf\u5230\uff0c\u6b21\u8981\u77e9\u9663\u5c0d\u61c9\u65bc\u96dc\u8a0a\u6216\u9577\u5c3e\u8cc7\u8a0a\uff0c\u800c\u4e3b\u77e9\u9663\u5305\u542b\u91cd\u8981\u77e5\u8b58\u3002MiLoRA \u5728\u8207\u4e3b\u77e9\u9663\u6b63\u4ea4\u7684\u5b50\u7a7a\u9593\u5167\u521d\u59cb\u5316\u4f4e\u79e9\u77e9\u9663\uff0c\u56e0\u6b64\u9810\u8a08\u9810\u8a13\u7df4\u77e5\u8b58\u5c07\u5f97\u5230\u5f88\u597d\u7684\u4fdd\u7559\u3002\u5728\u5fae\u8abf\u904e\u7a0b\u4e2d\uff0cMiLoRA \u6700\u5145\u5206\u5730\u5229\u7528\u4e86\u6b21\u512a\u5316\u7684\u5b50\u7a7a\u9593\u4f86\u5b78\u7fd2\u5fae\u8abf\u8cc7\u6599\u96c6\u3002\u5728\u5e38\u8b58\u63a8\u7406\u3001\u6578\u5b78\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u5c55\u793a\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u512a\u7570\u6027\u80fd\u3002", "author": "Hanqing Wang et.al.", "authors": "Hanqing Wang, Zeguan Xiao, Yixia Li, Shuo Wang, Guanhua Chen, Yun Chen", "id": "2406.09044v1", "paper_url": "http://arxiv.org/abs/2406.09044v1", "repo": "null"}}