{"2409.12139": {"publish_time": "2024-09-18", "title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models", "paper_summary": "With the advent of the big data and large language model era, zero-shot\npersonalized rapid customization has emerged as a significant trend. In this\nreport, we introduce Takin AudioLLM, a series of techniques and models, mainly\nincluding Takin TTS, Takin VC, and Takin Morphing, specifically designed for\naudiobook production. These models are capable of zero-shot speech production,\ngenerating high-quality speech that is nearly indistinguishable from real human\nspeech and facilitating individuals to customize the speech content according\nto their own needs. Specifically, we first introduce Takin TTS, a neural codec\nlanguage model that builds upon an enhanced neural speech codec and a\nmulti-task training framework, capable of generating high-fidelity natural\nspeech in a zero-shot way. For Takin VC, we advocate an effective content and\ntimbre joint modeling approach to improve the speaker similarity, while\nadvocating for a conditional flow matching based decoder to further enhance its\nnaturalness and expressiveness. Last, we propose the Takin Morphing system with\nhighly decoupled and advanced timbre and prosody modeling approaches, which\nenables individuals to customize speech production with their preferred timbre\nand prosody in a precise and controllable manner. Extensive experiments\nvalidate the effectiveness and robustness of our Takin AudioLLM series models.\nFor detailed demos, please refer to https://takinaudiollm.github.io.", "paper_summary_zh": "\u96a8\u8457\u5927\u6578\u64da\u548c\u5927\u8a9e\u8a00\u6a21\u578b\u6642\u4ee3\u7684\u5230\u4f86\uff0c\u96f6\u6b21\u5b78\u7fd2\u500b\u4eba\u5316\u5feb\u901f\u5ba2\u88fd\u5316\u5df2\u6210\u70ba\u4e00\u80a1\u91cd\u8981\u7684\u8da8\u52e2\u3002\u5728\u6b64\u5831\u544a\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 Takin AudioLLM\uff0c\u9019\u662f\u4e00\u7cfb\u5217\u5c08\u9580\u70ba\u6709\u8072\u66f8\u88fd\u4f5c\u8a2d\u8a08\u7684\u6280\u8853\u548c\u6a21\u578b\uff0c\u4e3b\u8981\u5305\u62ec Takin TTS\u3001Takin VC \u548c Takin Morphing\u3002\u9019\u4e9b\u6a21\u578b\u5177\u5099\u96f6\u6b21\u5b78\u7fd2\u8a9e\u97f3\u7522\u751f\u529f\u80fd\uff0c\u53ef\u7522\u751f\u8fd1\u4e4e\u8207\u771f\u4eba\u8a9e\u97f3\u7121\u6cd5\u5340\u5225\u7684\u9ad8\u54c1\u8cea\u8a9e\u97f3\uff0c\u4e26\u5354\u52a9\u500b\u4eba\u6839\u64da\u81ea\u5df1\u7684\u9700\u6c42\u5ba2\u88fd\u5316\u8a9e\u97f3\u5167\u5bb9\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u4ecb\u7d39 Takin TTS\uff0c\u9019\u662f\u4e00\u7a2e\u795e\u7d93\u7de8\u89e3\u78bc\u5668\u8a9e\u8a00\u6a21\u578b\uff0c\u5efa\u7acb\u5728\u589e\u5f37\u7684\u795e\u7d93\u8a9e\u97f3\u7de8\u89e3\u78bc\u5668\u548c\u591a\u4efb\u52d9\u8a13\u7df4\u67b6\u69cb\u4e4b\u4e0a\uff0c\u80fd\u5920\u4ee5\u96f6\u6b21\u5b78\u7fd2\u7684\u65b9\u5f0f\u7522\u751f\u9ad8\u4fdd\u771f\u81ea\u7136\u8a9e\u97f3\u3002\u5c0d\u65bc Takin VC\uff0c\u6211\u5011\u63d0\u5021\u4e00\u7a2e\u6709\u6548\u5167\u5bb9\u548c\u97f3\u8272\u806f\u5408\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u8aaa\u8a71\u8005\u7684\u76f8\u4f3c\u6027\uff0c\u540c\u6642\u63d0\u5021\u57fa\u65bc\u689d\u4ef6\u6d41\u5339\u914d\u7684\u89e3\u78bc\u5668\uff0c\u4ee5\u9032\u4e00\u6b65\u589e\u5f37\u5176\u81ea\u7136\u6027\u548c\u8868\u73fe\u529b\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa Takin Morphing \u7cfb\u7d71\uff0c\u63a1\u7528\u9ad8\u5ea6\u89e3\u8026\u4e14\u5148\u9032\u7684\u97f3\u8272\u548c\u97fb\u5f8b\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f7f\u500b\u4eba\u80fd\u5920\u4ee5\u7cbe\u78ba\u4e14\u53ef\u63a7\u7684\u65b9\u5f0f\u5ba2\u88fd\u5316\u8a9e\u97f3\u88fd\u4f5c\uff0c\u4e26\u63a1\u7528\u4ed6\u5011\u504f\u597d\u7684\u97f3\u8272\u548c\u97fb\u5f8b\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u9a57\u8b49\u4e86\u6211\u5011\u7684 Takin AudioLLM \u7cfb\u5217\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u7a69\u5065\u6027\u3002\u6709\u95dc\u8a73\u7d30\u793a\u7bc4\uff0c\u8acb\u53c3\u95b1 https://takinaudiollm.github.io\u3002", "author": "EverestAI et.al.", "authors": "EverestAI, :, Sijin Chen, Yuan Feng, Laipeng He, Tianwei He, Wendi He, Yanni Hu, Bin Lin, Yiting Lin, Pengfei Tan, Chengwei Tian, Chen Wang, Zhicheng Wang, Ruoye Xie, Jingjing Yin, Jianhao Ye, Jixun Yao, Quanlei Yan, Yuguang Yang", "id": "2409.12139v1", "paper_url": "http://arxiv.org/abs/2409.12139v1", "repo": "null"}}