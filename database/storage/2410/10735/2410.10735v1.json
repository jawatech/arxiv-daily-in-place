{"2410.10735": {"publish_time": "2024-10-14", "title": "Embedding Self-Correction as an Inherent Ability in Large Language Models for Enhanced Mathematical Reasoning", "paper_summary": "Accurate mathematical reasoning with Large Language Models (LLMs) is crucial\nin revolutionizing domains that heavily rely on such reasoning. However, LLMs\noften encounter difficulties in certain aspects of mathematical reasoning,\nleading to flawed reasoning and erroneous results. To mitigate these issues, we\nintroduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically\ndesigned to embed self-correction as an inherent ability in LLMs, enabling them\nto validate and rectify their own results. The CoSC mechanism operates through\na sequence of self-correction stages. In each stage, the LLMs generate a\nprogram to address a given problem, execute this program using program-based\ntools to obtain an output, subsequently verify this output. Based on the\nverification, the LLMs either proceed to the next correction stage or finalize\nthe answer. This iterative self-correction process allows the LLMs to refine\ntheir reasoning steps and improve the accuracy of their mathematical reasoning.\nTo enable the CoSC mechanism at a low cost, we employ a two-phase finetuning\napproach. In the first phase, the LLMs are trained with a relatively small\nvolume of seeding data generated from GPT-4, establishing an initial CoSC\ncapability. In the second phase, the CoSC capability is further enhanced by\ntraining with a larger volume of self-generated data using the trained model in\nthe first phase, without relying on the paid GPT-4. Our comprehensive\nexperiments demonstrate that CoSC significantly improves performance on\ntraditional mathematical datasets among existing open-source LLMs. Notably, our\nCoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging\nmathematical reasoning dataset in the public domain, surpassing the performance\nof well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs\nlike GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6e96\u78ba\u6578\u5b78\u63a8\u7406\u5c0d\u65bc\u5fb9\u5e95\u9769\u65b0\u9ad8\u5ea6\u4f9d\u8cf4\u6b64\u985e\u63a8\u7406\u7684\u9818\u57df\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0cLLM \u5728\u6578\u5b78\u63a8\u7406\u7684\u67d0\u4e9b\u65b9\u9762\u5e38\u5e38\u6703\u9047\u5230\u56f0\u96e3\uff0c\u5c0e\u81f4\u63a8\u7406\u6709\u7f3a\u9677\u4e14\u7d50\u679c\u932f\u8aa4\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u6a5f\u5236\uff0c\u5373\u81ea\u6211\u4fee\u6b63\u93c8 (CoSC)\uff0c\u5c08\u9580\u8a2d\u8a08\u70ba\u5c07\u81ea\u6211\u4fee\u6b63\u4f5c\u70ba LLM \u7684\u56fa\u6709\u80fd\u529b\uff0c\u4f7f\u5b83\u5011\u80fd\u5920\u9a57\u8b49\u548c\u7cfe\u6b63\u81ea\u5df1\u7684\u7d50\u679c\u3002CoSC \u6a5f\u5236\u901a\u904e\u4e00\u7cfb\u5217\u81ea\u6211\u4fee\u6b63\u968e\u6bb5\u904b\u884c\u3002\u5728\u6bcf\u500b\u968e\u6bb5\uff0cLLM \u6703\u751f\u6210\u4e00\u500b\u7a0b\u5f0f\u4f86\u89e3\u6c7a\u7d66\u5b9a\u7684\u554f\u984c\uff0c\u4f7f\u7528\u57fa\u65bc\u7a0b\u5f0f\u7684\u5de5\u5177\u57f7\u884c\u6b64\u7a0b\u5f0f\u4ee5\u7372\u5f97\u8f38\u51fa\uff0c\u96a8\u5f8c\u9a57\u8b49\u6b64\u8f38\u51fa\u3002\u6839\u64da\u9a57\u8b49\uff0cLLM \u8981\u4e48\u7e7c\u7e8c\u9032\u884c\u4e0b\u4e00\u500b\u4fee\u6b63\u968e\u6bb5\uff0c\u8981\u4e48\u6700\u7d42\u78ba\u5b9a\u7b54\u6848\u3002\u9019\u500b\u53cd\u8986\u7684\u81ea\u6211\u4fee\u6b63\u904e\u7a0b\u4f7f LLM \u80fd\u5920\u512a\u5316\u5176\u63a8\u7406\u6b65\u9a5f\u4e26\u63d0\u9ad8\u5176\u6578\u5b78\u63a8\u7406\u7684\u6e96\u78ba\u6027\u3002\u70ba\u4e86\u4ee5\u4f4e\u6210\u672c\u555f\u7528 CoSC \u6a5f\u5236\uff0c\u6211\u5011\u63a1\u7528\u4e86\u5169\u968e\u6bb5\u5fae\u8abf\u65b9\u6cd5\u3002\u5728\u7b2c\u4e00\u968e\u6bb5\uff0cLLM \u4f7f\u7528\u5f9e GPT-4 \u751f\u6210\u7684\u76f8\u5c0d\u8f03\u5c0f\u91cf\u7684\u7a2e\u5b50\u8cc7\u6599\u9032\u884c\u8a13\u7df4\uff0c\u5efa\u7acb\u4e86\u521d\u59cb\u7684 CoSC \u80fd\u529b\u3002\u5728\u7b2c\u4e8c\u968e\u6bb5\uff0cCoSC \u80fd\u529b\u901a\u904e\u4f7f\u7528\u7b2c\u4e00\u968e\u6bb5\u4e2d\u8a13\u7df4\u7684\u6a21\u578b\u8a13\u7df4\u5927\u91cf\u81ea\u751f\u8cc7\u6599\u9032\u4e00\u6b65\u589e\u5f37\uff0c\u800c\u7121\u9700\u4f9d\u8cf4\u4ed8\u8cbb\u7684 GPT-4\u3002\u6211\u5011\u7684\u7d9c\u5408\u5be6\u9a57\u8868\u660e\uff0cCoSC \u5728\u73fe\u6709\u7684\u958b\u6e90 LLM \u4e2d\u986f\u8457\u63d0\u9ad8\u4e86\u50b3\u7d71\u6578\u5b78\u8cc7\u6599\u96c6\u7684\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684 CoSC-Code-34B \u6a21\u578b\u5728 MATH \u4e0a\u53d6\u5f97\u4e86 53.5% \u7684\u5206\u6578\uff0c\u9019\u662f\u516c\u6709\u9818\u57df\u4e2d\u6700\u5177\u6311\u6230\u6027\u7684\u6578\u5b78\u63a8\u7406\u8cc7\u6599\u96c6\uff0c\u8d85\u8d8a\u4e86 ChatGPT\u3001GPT-4 \u751a\u81f3\u591a\u6a21\u614b LLM \u7b49\u6210\u719f\u6a21\u578b\u7684\u6548\u80fd\uff0c\u4f8b\u5982 GPT-4V\u3001Gemini-1.0 Pro \u548c Gemini-1.0 Ultra\u3002", "author": "Kuofeng Gao et.al.", "authors": "Kuofeng Gao, Huanqia Cai, Qingyao Shuai, Dihong Gong, Zhifeng Li", "id": "2410.10735v1", "paper_url": "http://arxiv.org/abs/2410.10735v1", "repo": "null"}}