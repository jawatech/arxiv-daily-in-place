{"2411.19504": {"publish_time": "2024-11-29", "title": "TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with Scalable Context and Symbolic Extension", "paper_summary": "The advent of large language models (LLMs) has unlocked great opportunities\nin complex data management tasks, particularly in question answering (QA) over\ncomplicated multi-table relational data. Despite significant progress,\nsystematically evaluating LLMs on multi-table QA remains a critical challenge\ndue to the inherent complexity of analyzing heterogeneous table structures and\npotential large scale of serialized relational data. Existing benchmarks\nprimarily focus on single-table QA, failing to capture the intricacies of\nreasoning across multiple relational tables, as required in real-world domains\nsuch as finance, healthcare, and e-commerce. To address this gap, we present\nTQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities\nof LLMs in tackling complex QA tasks over relational data. Our benchmark\nincorporates diverse relational database instances sourced from real-world\npublic datasets and introduces a flexible sampling mechanism to create tasks\nwith varying multi-table context lengths, ranging from 8K to 64K tokens. To\nensure robustness and reliability, we integrate symbolic extensions into the\nevaluation framework, enabling the assessment of LLM reasoning capabilities\nbeyond simple data retrieval or probabilistic pattern matching. We\nsystematically evaluate a range of LLMs, both open-source and closed-source,\nspanning model scales from 7 billion to 70 billion parameters. Our extensive\nexperiments reveal critical insights into the performance of LLMs in\nmulti-table QA, highlighting both challenges and opportunities for advancing\ntheir application in complex, data-driven environments. Our benchmark\nimplementation and results are available at\nhttps://github.com/Relaxed-System-Lab/TQA-Bench.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u51fa\u73fe\u70ba\u8907\u96dc\u8cc7\u6599\u7ba1\u7406\u4efb\u52d9\u958b\u555f\u4e86\u7d55\u4f73\u7684\u6a5f\u9047\uff0c\u7279\u5225\u662f\u5728\u8907\u96dc\u7684\u591a\u8868\u95dc\u806f\u8cc7\u6599\u4e2d\u9032\u884c\u554f\u7b54\uff08QA\uff09\u3002\u5118\u7ba1\u9032\u5c55\u986f\u8457\uff0c\u4f46\u7531\u65bc\u5206\u6790\u7570\u8cea\u8868\u7d50\u69cb\u548c\u5e8f\u5217\u5316\u95dc\u806f\u8cc7\u6599\u7684\u6f5b\u5728\u898f\u6a21\u7684\u8907\u96dc\u6027\uff0c\u5728\u591a\u8868 QA \u4e0a\u7cfb\u7d71\u6027\u5730\u8a55\u4f30 LLM \u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u7684\u6311\u6230\u3002\u73fe\u6709\u7684\u57fa\u6e96\u4e3b\u8981\u95dc\u6ce8\u55ae\u8868 QA\uff0c\u672a\u80fd\u6355\u6349\u5230\u8de8\u591a\u500b\u95dc\u806f\u8868\u7684\u63a8\u7406\u8907\u96dc\u6027\uff0c\u9019\u5728\u91d1\u878d\u3001\u91ab\u7642\u4fdd\u5065\u548c\u96fb\u5b50\u5546\u52d9\u7b49\u73fe\u5be6\u4e16\u754c\u9818\u57df\u4e2d\u662f\u5fc5\u9700\u7684\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86 TQA-Bench\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u591a\u8868 QA \u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30 LLM \u5728\u8655\u7406\u95dc\u806f\u8cc7\u6599\u4e0a\u7684\u8907\u96dc QA \u4efb\u52d9\u7684\u80fd\u529b\u3002\u6211\u5011\u7684\u57fa\u6e96\u7d0d\u5165\u4e86\u5f9e\u73fe\u5be6\u4e16\u754c\u516c\u5171\u8cc7\u6599\u96c6\u63a1\u96c6\u7684\u4e0d\u540c\u95dc\u806f\u8cc7\u6599\u5eab\u5be6\u4f8b\uff0c\u4e26\u5f15\u5165\u4e86\u4e00\u500b\u9748\u6d3b\u7684\u62bd\u6a23\u6a5f\u5236\u4f86\u5efa\u7acb\u5177\u6709\u4e0d\u540c\u591a\u8868\u5167\u5bb9\u9577\u5ea6\u7684\u4efb\u52d9\uff0c\u7bc4\u570d\u5f9e 8K \u5230 64K \u500b\u7b26\u865f\u3002\u70ba\u4e86\u78ba\u4fdd\u7a69\u5065\u6027\u548c\u53ef\u9760\u6027\uff0c\u6211\u5011\u5c07\u7b26\u865f\u64f4\u5145\u6574\u5408\u5230\u8a55\u4f30\u6846\u67b6\u4e2d\uff0c\u4f7f LLM \u63a8\u7406\u80fd\u529b\u7684\u8a55\u4f30\u4e0d\u50c5\u9650\u65bc\u7c21\u55ae\u7684\u8cc7\u6599\u6aa2\u7d22\u6216\u6a5f\u7387\u6a21\u5f0f\u5339\u914d\u3002\u6211\u5011\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u4e86\u4e00\u7cfb\u5217 LLM\uff0c\u5305\u62ec\u958b\u6e90\u548c\u9589\u6e90\uff0c\u6a21\u578b\u898f\u6a21\u5f9e 70 \u5104\u5230 700 \u5104\u500b\u53c3\u6578\u3002\u6211\u5011\u5ee3\u6cdb\u7684\u5be6\u9a57\u63ed\u793a\u4e86 LLM \u5728\u591a\u8868 QA \u4e2d\u6548\u80fd\u7684\u91cd\u8981\u898b\u89e3\uff0c\u7a81\u51fa\u4e86\u5728\u8907\u96dc\u7684\u8cc7\u6599\u9a45\u52d5\u74b0\u5883\u4e2d\u63a8\u9032\u5176\u61c9\u7528\u6240\u9762\u81e8\u7684\u6311\u6230\u548c\u6a5f\u9047\u3002\u6211\u5011\u7684\u57fa\u6e96\u5be6\u4f5c\u548c\u7d50\u679c\u53ef\u5728 https://github.com/Relaxed-System-Lab/TQA-Bench \u53d6\u5f97\u3002", "author": "Zipeng Qiu et.al.", "authors": "Zipeng Qiu, You Peng, Guangxin He, Binhang Yuan, Chen Wang", "id": "2411.19504v1", "paper_url": "http://arxiv.org/abs/2411.19504v1", "repo": "null"}}