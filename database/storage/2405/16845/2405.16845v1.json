{"2405.16845": {"publish_time": "2024-05-27", "title": "On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability", "paper_summary": "Autoregressively trained transformers have brought a profound revolution to\nthe world, especially with their in-context learning (ICL) ability to address\ndownstream tasks. Recently, several studies suggest that transformers learn a\nmesa-optimizer during autoregressive (AR) pretraining to implement ICL. Namely,\nthe forward pass of the trained transformer is equivalent to optimizing an\ninner objective function in-context. However, whether the practical non-convex\ntraining dynamics will converge to the ideal mesa-optimizer is still unclear.\nTowards filling this gap, we investigate the non-convex dynamics of a one-layer\nlinear causal self-attention model autoregressively trained by gradient flow,\nwhere the sequences are generated by an AR process $x_{t+1} = W x_t$. First,\nunder a certain condition of data distribution, we prove that an\nautoregressively trained transformer learns $W$ by implementing one step of\ngradient descent to minimize an ordinary least squares (OLS) problem\nin-context. It then applies the learned $\\widehat{W}$ for next-token\nprediction, thereby verifying the mesa-optimization hypothesis. Next, under the\nsame data conditions, we explore the capability limitations of the obtained\nmesa-optimizer. We show that a stronger assumption related to the moments of\ndata is the sufficient and necessary condition that the learned mesa-optimizer\nrecovers the distribution. Besides, we conduct exploratory analyses beyond the\nfirst data condition and prove that generally, the trained transformer will not\nperform vanilla gradient descent for the OLS problem. Finally, our simulation\nresults verify the theoretical results.", "paper_summary_zh": "\u81ea\u56de\u5f52\u8bad\u7ec3\u7684 Transformer \u4e3a\u4e16\u754c\u5e26\u6765\u4e86\u6df1\u523b\u7684\u53d8\u9769\uff0c\u5c24\u5176\u662f\u5176\u4e0a\u4e0b\u6587\u5b66\u4e60 (ICL) \u80fd\u529b\uff0c\u53ef\u89e3\u51b3\u4e0b\u6e38\u4efb\u52a1\u3002\u6700\u8fd1\uff0c\u591a\u9879\u7814\u7a76\u8868\u660e Transformer \u5728\u81ea\u56de\u5f52 (AR) \u9884\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u4e86\u4e00\u4e2a mesa \u4f18\u5316\u5668\u6765\u5b9e\u73b0 ICL\u3002\u5373\uff0c\u8bad\u7ec3\u540e\u7684 Transformer \u7684\u524d\u5411\u4f20\u9012\u7b49\u540c\u4e8e\u4f18\u5316\u4e0a\u4e0b\u6587\u4e2d\u5185\u90e8\u7684\u76ee\u6807\u51fd\u6570\u3002\u7136\u800c\uff0c\u5b9e\u9645\u7684\u975e\u51f8\u8bad\u7ec3\u52a8\u6001\u662f\u5426\u4f1a\u6536\u655b\u5230\u7406\u60f3\u7684 mesa \u4f18\u5316\u5668\u4ecd\u4e0d\u6e05\u695a\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u7531\u68af\u5ea6\u6d41\u81ea\u56de\u5f52\u8bad\u7ec3\u7684\u5355\u5c42\u7ebf\u6027\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u7684\u975e\u51f8\u52a8\u6001\uff0c\u5176\u4e2d\u5e8f\u5217\u7531 AR \u8fc7\u7a0b\u751f\u6210 $x_{t+1} = W x_t$\u3002\u9996\u5148\uff0c\u5728\u6570\u636e\u5206\u5e03\u7684\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u81ea\u56de\u5f52\u8bad\u7ec3\u7684 Transformer \u901a\u8fc7\u5b9e\u65bd\u4e00\u6b65\u68af\u5ea6\u4e0b\u964d\u6765\u5b66\u4e60 $W$\uff0c\u4ee5\u6700\u5c0f\u5316\u4e0a\u4e0b\u6587\u4e2d\u7684\u666e\u901a\u6700\u5c0f\u4e8c\u4e58 (OLS) \u95ee\u9898\u3002\u7136\u540e\uff0c\u5b83\u5c06\u5b66\u4e60\u5230\u7684 $\\widehat{W}$ \u5e94\u7528\u4e8e\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff0c\u4ece\u800c\u9a8c\u8bc1\u4e86 mesa \u4f18\u5316\u5047\u8bbe\u3002\u63a5\u4e0b\u6765\uff0c\u5728\u76f8\u540c\u7684\u6570\u636e\u6761\u4ef6\u4e0b\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u83b7\u5f97\u7684 mesa \u4f18\u5316\u5668\u7684\u80fd\u529b\u9650\u5236\u3002\u6211\u4eec\u8868\u660e\uff0c\u4e0e\u6570\u636e\u77e9\u76f8\u5173\u7684\u66f4\u5f3a\u5047\u8bbe\u662f\u5b66\u4e60\u5230\u7684 mesa \u4f18\u5316\u5668\u6062\u590d\u5206\u5e03\u7684\u5145\u5206\u4e14\u5fc5\u8981\u6761\u4ef6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u8d85\u51fa\u7b2c\u4e00\u4e2a\u6570\u636e\u6761\u4ef6\u7684\u63a2\u7d22\u6027\u5206\u6790\uff0c\u5e76\u8bc1\u660e\u901a\u5e38\uff0c\u8bad\u7ec3\u540e\u7684 Transformer \u4e0d\u4f1a\u5bf9 OLS \u95ee\u9898\u6267\u884c\u9999\u8349\u68af\u5ea6\u4e0b\u964d\u3002\u6700\u540e\uff0c\u6211\u4eec\u7684\u6a21\u62df\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "author": "Chenyu Zheng et.al.", "authors": "Chenyu Zheng, Wei Huang, Rongzhen Wang, Guoqiang Wu, Jun Zhu, Chongxuan Li", "id": "2405.16845v1", "paper_url": "http://arxiv.org/abs/2405.16845v1", "repo": "null"}}