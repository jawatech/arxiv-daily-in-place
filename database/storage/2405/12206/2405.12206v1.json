{"2405.12206": {"publish_time": "2024-05-20", "title": "Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models", "paper_summary": "Scientist learn early on how to cite scientific sources to support their\nclaims. Sometimes, however, scientists have challenges determining where a\ncitation should be situated -- or, even worse, fail to cite a source\naltogether. Automatically detecting sentences that need a citation (i.e.,\ncitation worthiness) could solve both of these issues, leading to more robust\nand well-constructed scientific arguments. Previous researchers have applied\nmachine learning to this task but have used small datasets and models that do\nnot take advantage of recent algorithmic developments such as attention\nmechanisms in deep learning. We hypothesize that we can develop significantly\naccurate deep learning architectures that learn from large supervised datasets\nconstructed from open access publications. In this work, we propose a\nBidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism\nand contextual information to detect sentences that need citations. We also\nproduce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset,\nwhich is orders of magnitude larger than previous datasets. Our experiments\nshow that our architecture achieves state of the art performance on the\nstandard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance\n($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transfer\nlearning across these datasets. We further use interpretable models to\nilluminate how specific language is used to promote and inhibit citations. We\ndiscover that sections and surrounding sentences are crucial for our improved\npredictions. We further examined purported mispredictions of the model, and\nuncovered systematic human mistakes in citation behavior and source data. This\nopens the door for our model to check documents during pre-submission and\npre-archival procedures. We make this new dataset, the code, and a web-based\ntool available to the community.", "paper_summary_zh": "<paragraph>\u79d1\u5b78\u5bb6\u5f88\u65e9\u5c31\u5b78\u6703\u5982\u4f55\u5f15\u7528\u79d1\u5b78\u6587\u737b\u4f86\u652f\u6301\u4ed6\u5011\u7684\u8aaa\u6cd5\u3002\u7136\u800c\uff0c\u6709\u6642\u5019\uff0c\u79d1\u5b78\u5bb6\u5728\u6c7a\u5b9a\u5f15\u6587\u61c9\u7f6e\u65bc\u4f55\u8655\u6642\u6703\u9047\u5230\u6311\u6230\uff0c\u6216\u8005\u66f4\u7cdf\u7684\u662f\uff0c\u6839\u672c\u6c92\u6709\u5f15\u7528\u4f86\u6e90\u3002\u81ea\u52d5\u5075\u6e2c\u9700\u8981\u5f15\u7528\u7684\u53e5\u5b50\uff08\u5373\u5f15\u7528\u7684\u50f9\u503c\uff09\u53ef\u4ee5\u89e3\u6c7a\u9019\u5169\u500b\u554f\u984c\uff0c\u5f9e\u800c\u7522\u751f\u66f4\u5f37\u5927\u4e14\u7d50\u69cb\u826f\u597d\u7684\u79d1\u5b78\u8ad6\u8b49\u3002\u5148\u524d\u7684\u7814\u7a76\u4eba\u54e1\u5df2\u5c07\u6a5f\u5668\u5b78\u7fd2\u61c9\u7528\u65bc\u6b64\u4efb\u52d9\uff0c\u4f46\u4f7f\u7528\u4e86\u5c0f\u578b\u8cc7\u6599\u96c6\u548c\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u4e26\u672a\u5229\u7528\u6df1\u5ea6\u5b78\u7fd2\u4e2d\u7684\u6700\u65b0\u6f14\u7b97\u6cd5\u767c\u5c55\uff0c\u4f8b\u5982\u6ce8\u610f\u529b\u6a5f\u5236\u3002\u6211\u5011\u5047\u8a2d\u6211\u5011\u53ef\u4ee5\u958b\u767c\u51fa\u5f9e\u7531\u958b\u653e\u7372\u53d6\u51fa\u7248\u7269\u5efa\u69cb\u7684\u5927\u578b\u76e3\u7763\u5f0f\u8cc7\u6599\u96c6\u4e2d\u5b78\u7fd2\u7684\u986f\u8457\u6e96\u78ba\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5177\u6709\u6ce8\u610f\u529b\u6a5f\u5236\u548c\u4e0a\u4e0b\u6587\u8cc7\u8a0a\u7684\u96d9\u5411\u9577\u77ed\u671f\u8a18\u61b6 (BiLSTM) \u7db2\u8def\uff0c\u4ee5\u5075\u6e2c\u9700\u8981\u5f15\u7528\u7684\u53e5\u5b50\u3002\u6211\u5011\u9084\u6839\u64da PubMed \u958b\u653e\u7372\u53d6\u5b50\u96c6\u88fd\u4f5c\u4e86\u4e00\u500b\u65b0\u7684\u5927\u578b\u8cc7\u6599\u96c6 (PMOA-CITE)\uff0c\u5176\u6578\u91cf\u7d1a\u5927\u65bc\u5148\u524d\u7684\u8cc7\u6599\u96c6\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u67b6\u69cb\u5728\u6a19\u6e96 ACL-ARC \u8cc7\u6599\u96c6 ($F_{1}=0.507$) \u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u4e26\u5728\u65b0\u7684 PMOA-CITE \u4e0a\u8868\u73fe\u51fa\u9ad8\u6548\u80fd ($F_{1}=0.856$)\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5b83\u53ef\u4ee5\u5728\u9019\u4e9b\u8cc7\u6599\u96c6\u4e4b\u9593\u8f49\u79fb\u5b78\u7fd2\u3002\u6211\u5011\u9032\u4e00\u6b65\u4f7f\u7528\u53ef\u89e3\u91cb\u6a21\u578b\u4f86\u8aaa\u660e\u5982\u4f55\u4f7f\u7528\u7279\u5b9a\u8a9e\u8a00\u4f86\u4fc3\u9032\u548c\u6291\u5236\u5f15\u7528\u3002\u6211\u5011\u767c\u73fe\u7ae0\u7bc0\u548c\u5468\u570d\u7684\u53e5\u5b50\u5c0d\u65bc\u6211\u5011\u6539\u9032\u7684\u9810\u6e2c\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u9032\u4e00\u6b65\u6aa2\u67e5\u4e86\u6a21\u578b\u7684\u5047\u5b9a\u932f\u8aa4\u9810\u6e2c\uff0c\u4e26\u767c\u73fe\u4e86\u5f15\u7528\u884c\u70ba\u548c\u4f86\u6e90\u8cc7\u6599\u4e2d\u7684\u4eba\u70ba\u7cfb\u7d71\u6027\u932f\u8aa4\u3002\u9019\u70ba\u6211\u5011\u7684\u6a21\u578b\u5728\u63d0\u4ea4\u524d\u548c\u6b78\u6a94\u524d\u7a0b\u5e8f\u4e2d\u6aa2\u67e5\u6587\u4ef6\u958b\u555f\u4e86\u5927\u9580\u3002\u6211\u5011\u5c07\u9019\u500b\u65b0\u8cc7\u6599\u96c6\u3001\u7a0b\u5f0f\u78bc\u548c\u4e00\u500b\u57fa\u65bc\u7db2\u8def\u7684\u5de5\u5177\u63d0\u4f9b\u7d66\u793e\u7fa4\u4f7f\u7528\u3002</paragraph>", "author": "Tong Zeng et.al.", "authors": "Tong Zeng, Daniel E. Acuna", "id": "2405.12206v1", "paper_url": "http://arxiv.org/abs/2405.12206v1", "repo": "https://github.com/sciosci/cite-worthiness"}}