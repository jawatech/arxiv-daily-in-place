{"2408.15778": {"publish_time": "2024-08-28", "title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models", "paper_summary": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u986f\u8457\u7684\u80fd\u529b\uff0c\u5c55\u793a\u51fa\u8907\u96dc\u7684\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u3002\u7406\u89e3\u548c\u57f7\u884c\u8907\u96dc\u7684\u898f\u5247\uff0c\u4ee5\u53ca\u591a\u6b65\u9a5f\u898f\u5283\uff0c\u662f\u908f\u8f2f\u63a8\u7406\u7684\u57fa\u790e\uff0c\u5c0d\u65bc\u5be6\u7528\u7684 LLM \u4ee3\u7406\u548c\u6c7a\u7b56\u7cfb\u7d71\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u5c07 LLM \u8a55\u4f30\u70ba\u6709\u6548\u7684\u57fa\u65bc\u898f\u5247\u7684\u57f7\u884c\u8005\u548c\u898f\u5283\u8005\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 LogicGame\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30 LLM \u5168\u9762\u7684\u898f\u5247\u7406\u89e3\u3001\u57f7\u884c\u548c\u898f\u5283\u80fd\u529b\u3002\u8207\u50b3\u7d71\u57fa\u6e96\u4e0d\u540c\uff0cLogicGame \u63d0\u4f9b\u4e86\u5305\u542b\u4e00\u7cfb\u5217\u898f\u5247\u548c\u521d\u59cb\u72c0\u614b\u7684\u4e0d\u540c\u904a\u6232\uff0c\u8981\u6c42\u6a21\u578b\u7406\u89e3\u4e26\u61c9\u7528\u9810\u5b9a\u7fa9\u6cd5\u898f\u4f86\u89e3\u6c7a\u554f\u984c\u3002\u6211\u5011\u5275\u5efa\u4e86\u6a21\u64ec\u5834\u666f\uff0c\u5176\u4e2d\u6a21\u578b\u57f7\u884c\u6216\u898f\u5283\u64cd\u4f5c\u4ee5\u5be6\u73fe\u7279\u5b9a\u7d50\u679c\u3002\u9019\u4e9b\u904a\u6232\u5834\u666f\u7d93\u904e\u5c08\u9580\u8a2d\u8a08\uff0c\u901a\u904e\u5b8c\u5168\u4f9d\u8cf4\u9810\u5b9a\u7fa9\u898f\u5247\u4f86\u5340\u5206\u908f\u8f2f\u63a8\u7406\u548c\u50c5\u6709\u7684\u77e5\u8b58\u3002\u9019\u7a2e\u5206\u96e2\u5141\u8a31\u5c0d\u57fa\u65bc\u898f\u5247\u7684\u63a8\u7406\u80fd\u529b\u9032\u884c\u7d14\u7cb9\u7684\u8a55\u4f30\u3002\u8a55\u4f30\u4e0d\u50c5\u8003\u616e\u6700\u7d42\u7d50\u679c\uff0c\u9084\u8003\u616e\u4e2d\u9593\u6b65\u9a5f\uff0c\u5c0d\u6a21\u578b\u6027\u80fd\u9032\u884c\u5168\u9762\u8a55\u4f30\u3002\u6b64\u5916\uff0c\u9019\u4e9b\u4e2d\u9593\u6b65\u9a5f\u662f\u78ba\u5b9a\u6027\u7684\uff0c\u53ef\u4ee5\u81ea\u52d5\u9a57\u8b49\u3002LogicGame \u5b9a\u7fa9\u4e86\u5177\u6709\u4e0d\u540c\u96e3\u5ea6\u7d1a\u5225\u7684\u904a\u6232\u5834\u666f\uff0c\u5f9e\u7c21\u55ae\u7684\u898f\u5247\u61c9\u7528\u5230\u8907\u96dc\u7684\u63a8\u7406\u93c8\uff0c\u4ee5\u4fbf\u5c0d\u6a21\u578b\u5728\u898f\u5247\u7406\u89e3\u548c\u591a\u6b65\u9a5f\u57f7\u884c\u65b9\u9762\u7684\u6027\u80fd\u9032\u884c\u7cbe\u78ba\u8a55\u4f30\u3002\u5229\u7528 LogicGame\uff0c\u6211\u5011\u6e2c\u8a66\u4e86\u5404\u7a2e LLM\uff0c\u4e26\u767c\u73fe\u5b83\u5011\u5728\u57fa\u65bc\u898f\u5247\u7684\u908f\u8f2f\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u986f\u8457\u7684\u7f3a\u9677\u3002", "author": "Jiayi Gui et.al.", "authors": "Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang", "id": "2408.15778v1", "paper_url": "http://arxiv.org/abs/2408.15778v1", "repo": "null"}}