{"2409.18023": {"publish_time": "2024-09-26", "title": "DARE: Diverse Visual Question Answering with Robustness Evaluation", "paper_summary": "Vision Language Models (VLMs) extend remarkable capabilities of text-only\nlarge language models and vision-only models, and are able to learn from and\nprocess multi-modal vision-text input. While modern VLMs perform well on a\nnumber of standard image classification and image-text matching tasks, they\nstill struggle with a number of crucial vision-language (VL) reasoning\nabilities such as counting and spatial reasoning. Moreover, while they might be\nvery brittle to small variations in instructions and/or evaluation protocols,\nexisting benchmarks fail to evaluate their robustness (or rather the lack of\nit). In order to couple challenging VL scenarios with comprehensive robustness\nevaluation, we introduce DARE, Diverse Visual Question Answering with\nRobustness Evaluation, a carefully created and curated multiple-choice VQA\nbenchmark. DARE evaluates VLM performance on five diverse categories and\nincludes four robustness-oriented evaluations based on the variations of:\nprompts, the subsets of answer options, the output format and the number of\ncorrect answers. Among a spectrum of other findings, we report that\nstate-of-the-art VLMs still struggle with questions in most categories and are\nunable to consistently deliver their peak performance across the tested\nrobustness evaluations. The worst case performance across the subsets of\noptions is up to 34% below the performance in the standard case. The robustness\nof the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the\nclosed-source models such as GPT-4 and Gemini, but even the latter remain very\nbrittle to different variations.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u64f4\u5c55\u4e86\u7d14\u6587\u5b57\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u548c\u7d14\u8996\u89ba\u6a21\u578b\u7684\u986f\u8457\u80fd\u529b\uff0c\u4e26\u4e14\u80fd\u5920\u5f9e\u591a\u6a21\u614b\u8996\u89ba\u6587\u5b57\u8f38\u5165\u4e2d\u5b78\u7fd2\u548c\u8655\u7406\u3002\u96d6\u7136\u73fe\u4ee3 VLM \u5728\u8a31\u591a\u6a19\u6e96\u5716\u50cf\u5206\u985e\u548c\u5716\u50cf\u6587\u5b57\u5339\u914d\u4efb\u52d9\u4e0a\u8868\u73fe\u826f\u597d\uff0c\u4f46\u5b83\u5011\u5728\u8a31\u591a\u95dc\u9375\u7684\u8996\u89ba\u8a9e\u8a00 (VL) \u63a8\u7406\u80fd\u529b\u4e0a\u4ecd\u5b58\u5728\u56f0\u96e3\uff0c\u4f8b\u5982\u8a08\u6578\u548c\u7a7a\u9593\u63a8\u7406\u3002\u6b64\u5916\uff0c\u96d6\u7136\u5b83\u5011\u53ef\u80fd\u5c0d\u6307\u4ee4\u548c/\u6216\u8a55\u4f30\u5354\u8b70\u7684\u5fae\u5c0f\u8b8a\u5316\u975e\u5e38\u8106\u5f31\uff0c\u4f46\u73fe\u6709\u7684\u57fa\u6e96\u672a\u80fd\u8a55\u4f30\u5176\u5065\u58ef\u6027\uff08\u6216\u66f4\u78ba\u5207\u5730\u8aaa\uff0c\u7f3a\u4e4f\u5065\u58ef\u6027\uff09\u3002\u70ba\u4e86\u5c07\u5177\u6709\u6311\u6230\u6027\u7684 VL \u5834\u666f\u8207\u5168\u9762\u7684\u5065\u58ef\u6027\u8a55\u4f30\u76f8\u7d50\u5408\uff0c\u6211\u5011\u5f15\u5165\u4e86 DARE\uff0c\u4e00\u7a2e\u591a\u9078\u984c VQA \u57fa\u6e96\uff0c\u5177\u6709\u5065\u58ef\u6027\u8a55\u4f30\uff0c\u7d93\u904e\u4ed4\u7d30\u5275\u5efa\u548c\u7b56\u5283\u3002DARE \u8a55\u4f30\u4e86 VLM \u5728\u4e94\u500b\u4e0d\u540c\u985e\u5225\u4e0a\u7684\u8868\u73fe\uff0c\u4e26\u5305\u62ec\u56db\u9805\u57fa\u65bc\u4ee5\u4e0b\u8b8a\u5316\u7684\u5065\u58ef\u6027\u5c0e\u5411\u8a55\u4f30\uff1a\u63d0\u793a\u3001\u7b54\u6848\u9078\u9805\u7684\u5b50\u96c6\u3001\u8f38\u51fa\u683c\u5f0f\u548c\u6b63\u78ba\u7b54\u6848\u7684\u6578\u91cf\u3002\u5728\u5176\u4ed6\u4e00\u7cfb\u5217\u767c\u73fe\u4e2d\uff0c\u6211\u5011\u5831\u544a\u8aaa\uff0c\u6700\u5148\u9032\u7684 VLM \u4ecd\u7136\u96e3\u4ee5\u61c9\u5c0d\u5927\u591a\u6578\u985e\u5225\u4e2d\u7684\u554f\u984c\uff0c\u4e26\u4e14\u7121\u6cd5\u5728\u6e2c\u8a66\u7684\u5065\u58ef\u6027\u8a55\u4f30\u4e2d\u6301\u7e8c\u63d0\u4f9b\u5176\u5cf0\u503c\u6548\u80fd\u3002\u9078\u9805\u5b50\u96c6\u4e2d\u7684\u6700\u5dee\u60c5\u6cc1\u6548\u80fd\u6bd4\u6a19\u6e96\u60c5\u6cc1\u4e0b\u7684\u6548\u80fd\u4f4e 34%\u3002LLaVA 1.6 \u548c Idefics2 \u7b49\u958b\u6e90 VLM \u7684\u5065\u58ef\u6027\u7121\u6cd5\u8207 GPT-4 \u548c Gemini \u7b49\u9589\u6e90\u6a21\u578b\u76f8\u5339\u914d\uff0c\u4f46\u5373\u4f7f\u662f\u5f8c\u8005\u5c0d\u4e0d\u540c\u7684\u8b8a\u5316\u4ecd\u7136\u975e\u5e38\u8106\u5f31\u3002", "author": "Hannah Sterz et.al.", "authors": "Hannah Sterz, Jonas Pfeiffer, Ivan Vuli\u0107", "id": "2409.18023v1", "paper_url": "http://arxiv.org/abs/2409.18023v1", "repo": "null"}}