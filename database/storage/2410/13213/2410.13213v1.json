{"2410.13213": {"publish_time": "2024-10-17", "title": "LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch", "paper_summary": "Optimization problems are prevalent across various scenarios. Formulating and\nthen solving optimization problems described by natural language often requires\nhighly specialized human expertise, which could block the widespread\napplication of optimization-based decision making. To make problem formulating\nand solving automated, leveraging large language models (LLMs) has emerged as a\npotential way. However, this kind of way suffers from the issue of optimization\ngeneralization. Namely, the accuracy of most current LLM-based methods and the\ngenerality of optimization problem types that they can model are still limited.\nIn this paper, we propose a unified learning-based framework called LLMOPT to\nboost optimization generalization. Starting from the natural language\ndescriptions of optimization problems and a pre-trained LLM, LLMOPT constructs\nthe introduced five-element formulation as a universal model for learning to\ndefine diverse optimization problem types. Then, LLMOPT employs the\nmulti-instruction tuning to enhance both problem formalization and solver code\ngeneration accuracy and generality. After that, to prevent hallucinations in\nLLMs, such as sacrificing solving accuracy to avoid execution errors, model\nalignment and self-correction mechanism are adopted in LLMOPT. We evaluate the\noptimization generalization ability of LLMOPT and compared methods across six\nreal-world datasets covering roughly 20 fields such as health, environment,\nenergy and manufacturing, etc. Extensive experiment results show that LLMOPT is\nable to model various optimization problem types such as linear/nonlinear\nprogramming, mixed integer programming and combinatorial optimization, and\nachieves a notable 11.08% average solving accuracy improvement compared with\nthe state-of-the-art methods. The code is available at\nhttps://github.com/caigaojiang/LLMOPT.", "paper_summary_zh": "<paragraph>\u512a\u5316\u554f\u984c\u666e\u904d\u5b58\u5728\u65bc\u5404\u7a2e\u5834\u666f\u4e2d\u3002\u5236\u5b9a\u4e26\u89e3\u6c7a\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\u7684\u512a\u5316\u554f\u984c\u901a\u5e38\u9700\u8981\u9ad8\u5ea6\u5c08\u696d\u7684\u4eba\u985e\u5c08\u696d\u77e5\u8b58\uff0c\u9019\u53ef\u80fd\u6703\u963b\u7919\u57fa\u65bc\u512a\u5316\u7684\u6c7a\u7b56\u5236\u5b9a\u7684\u5ee3\u6cdb\u61c9\u7528\u3002\u70ba\u4e86\u4f7f\u554f\u984c\u5236\u5b9a\u548c\u6c42\u89e3\u81ea\u52d5\u5316\uff0c\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u4e00\u7a2e\u6f5b\u5728\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u5f0f\u5b58\u5728\u512a\u5316\u6cdb\u5316\u554f\u984c\u3002\u4e5f\u5c31\u662f\u8aaa\uff0c\u7576\u524d\u5927\u591a\u6578\u57fa\u65bc LLM \u7684\u65b9\u6cd5\u7684\u6e96\u78ba\u6027\u548c\u5b83\u5011\u53ef\u4ee5\u5efa\u6a21\u7684\u512a\u5316\u554f\u984c\u985e\u578b\u7684\u666e\u904d\u6027\u4ecd\u7136\u6709\u9650\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba LLMOPT \u7684\u7d71\u4e00\u57fa\u65bc\u5b78\u7fd2\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u512a\u5316\u6cdb\u5316\u80fd\u529b\u3002\u5f9e\u512a\u5316\u554f\u984c\u7684\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\u548c\u9810\u8a13\u7df4\u7684 LLM \u958b\u59cb\uff0cLLMOPT \u5c07\u5f15\u5165\u7684\u4e94\u8981\u7d20\u8868\u8ff0\u69cb\u5efa\u70ba\u5b78\u7fd2\u5b9a\u7fa9\u5404\u7a2e\u512a\u5316\u554f\u984c\u985e\u578b\u7684\u901a\u7528\u6a21\u578b\u3002\u7136\u5f8c\uff0cLLMOPT \u63a1\u7528\u591a\u6307\u4ee4\u8abf\u6574\u4f86\u589e\u5f37\u554f\u984c\u5f62\u5f0f\u5316\u548c\u6c42\u89e3\u5668\u4ee3\u78bc\u751f\u6210\u6e96\u78ba\u6027\u548c\u666e\u904d\u6027\u3002\u5728\u90a3\u4e4b\u5f8c\uff0c\u70ba\u4e86\u9632\u6b62 LLM \u4e2d\u7684\u5e7b\u89ba\uff0c\u4f8b\u5982\u72a7\u7272\u6c42\u89e3\u6e96\u78ba\u6027\u4ee5\u907f\u514d\u57f7\u884c\u932f\u8aa4\uff0c\u5728 LLMOPT \u4e2d\u63a1\u7528\u4e86\u6a21\u578b\u5c0d\u9f4a\u548c\u81ea\u6821\u6b63\u6a5f\u5236\u3002\u6211\u5011\u8a55\u4f30\u4e86 LLMOPT \u7684\u512a\u5316\u6cdb\u5316\u80fd\u529b\uff0c\u4e26\u6bd4\u8f03\u4e86\u516d\u500b\u6db5\u84cb\u5065\u5eb7\u3001\u74b0\u5883\u3001\u80fd\u6e90\u548c\u88fd\u9020\u7b49\u7d04 20 \u500b\u9818\u57df\u7684\u771f\u5be6\u4e16\u754c\u6578\u64da\u96c6\u4e2d\u7684\u65b9\u6cd5\u3002\u5927\u91cf\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cLLMOPT \u80fd\u5920\u5c0d\u5404\u7a2e\u512a\u5316\u554f\u984c\u985e\u578b\u5efa\u6a21\uff0c\u4f8b\u5982\u7dda\u6027/\u975e\u7dda\u6027\u898f\u5283\u3001\u6df7\u5408\u6574\u6578\u898f\u5283\u548c\u7d44\u5408\u512a\u5316\uff0c\u4e26\u8207\u6700\u5148\u9032\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53d6\u5f97\u4e86\u986f\u8457\u7684 11.08% \u5e73\u5747\u6c42\u89e3\u6e96\u78ba\u5ea6\u63d0\u5347\u3002\u4ee3\u78bc\u53ef\u5728 https://github.com/caigaojiang/LLMOPT \u7372\u5f97\u3002</paragraph>", "author": "Caigao Jiang et.al.", "authors": "Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun Zhou, Aimin Zhou, Yang Yu", "id": "2410.13213v1", "paper_url": "http://arxiv.org/abs/2410.13213v1", "repo": "https://github.com/caigaojiang/llmopt"}}