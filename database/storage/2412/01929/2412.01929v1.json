{"2412.01929": {"publish_time": "2024-12-02", "title": "ECG-SleepNet: Deep Learning-Based Comprehensive Sleep Stage Classification Using ECG Signals", "paper_summary": "Accurate sleep stage classification is essential for understanding sleep\ndisorders and improving overall health. This study proposes a novel three-stage\napproach for sleep stage classification using ECG signals, offering a more\naccessible alternative to traditional methods that often rely on complex\nmodalities like EEG. In Stages 1 and 2, we initialize the weights of two\nnetworks, which are then integrated in Stage 3 for comprehensive\nclassification. In the first phase, we estimate key features using Feature\nImitating Networks (FINs) to achieve higher accuracy and faster convergence.\nThe second phase focuses on identifying the N1 sleep stage through the\ntime-frequency representation of ECG signals. Finally, the third phase\nintegrates models from the previous stages and employs a Kolmogorov-Arnold\nNetwork (KAN) to classify five distinct sleep stages. Additionally, data\naugmentation techniques, particularly SMOTE, are used in enhancing\nclassification capabilities for underrepresented stages like N1. Our results\ndemonstrate significant improvements in the classification performance, with an\noverall accuracy of 80.79% an overall kappa of 0.73. The model achieves\nspecific accuracies of 86.70% for Wake, 60.36% for N1, 83.89% for N2, 84.85%\nfor N3, and 87.16% for REM. This study emphasizes the importance of weight\ninitialization and data augmentation in optimizing sleep stage classification\nwith ECG signals.", "paper_summary_zh": "\u7cbe\u6e96\u7684\u7761\u7720\u5206\u671f\u5206\u985e\u5c0d\u65bc\u4e86\u89e3\u7761\u7720\u969c\u7919\u548c\u6539\u5584\u6574\u9ad4\u5065\u5eb7\u81f3\u95dc\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u4e09\u968e\u6bb5\u65b9\u6cd5\uff0c\u4f7f\u7528 ECG \u8a0a\u865f\u9032\u884c\u7761\u7720\u5206\u671f\u5206\u985e\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u66f4\u6613\u65bc\u53d6\u5f97\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u50b3\u7d71\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u65bc EEG \u7b49\u8907\u96dc\u7684\u6a21\u5f0f\u3002\u5728\u7b2c 1 \u548c\u7b2c 2 \u968e\u6bb5\uff0c\u6211\u5011\u521d\u59cb\u5316\u5169\u500b\u7db2\u8def\u7684\u6b0a\u91cd\uff0c\u7136\u5f8c\u5728\u7b2c 3 \u968e\u6bb5\u6574\u5408\u5b83\u5011\u4ee5\u9032\u884c\u5168\u9762\u7684\u5206\u985e\u3002\u5728\u7b2c\u4e00\u968e\u6bb5\uff0c\u6211\u5011\u4f7f\u7528\u7279\u5fb5\u6a21\u4eff\u7db2\u8def (FIN) \u4f30\u8a08\u95dc\u9375\u7279\u5fb5\uff0c\u4ee5\u5be6\u73fe\u66f4\u9ad8\u7684\u6e96\u78ba\u5ea6\u548c\u66f4\u5feb\u7684\u6536\u6582\u3002\u7b2c\u4e8c\u968e\u6bb5\u5c08\u6ce8\u65bc\u900f\u904e ECG \u8a0a\u865f\u7684\u6642\u983b\u8868\u793a\u4f86\u8b58\u5225 N1 \u7761\u7720\u968e\u6bb5\u3002\u6700\u5f8c\uff0c\u7b2c\u4e09\u968e\u6bb5\u6574\u5408\u524d\u4e00\u968e\u6bb5\u7684\u6a21\u578b\uff0c\u4e26\u63a1\u7528 Kolmogorov-Arnold \u7db2\u8def (KAN) \u4f86\u5206\u985e\u4e94\u500b\u4e0d\u540c\u7684\u7761\u7720\u968e\u6bb5\u3002\u6b64\u5916\uff0c\u8cc7\u6599\u64f4\u5145\u6280\u8853\uff0c\u7279\u5225\u662f SMOTE\uff0c\u7528\u65bc\u589e\u5f37\u5c0d N1 \u7b49\u4ee3\u8868\u6027\u4e0d\u8db3\u968e\u6bb5\u7684\u5206\u985e\u80fd\u529b\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\u4e86\u5206\u985e\u6548\u80fd\u6709\u986f\u8457\u7684\u6539\u5584\uff0c\u6574\u9ad4\u6e96\u78ba\u5ea6\u70ba 80.79%\uff0c\u6574\u9ad4 kappa \u70ba 0.73\u3002\u8a72\u6a21\u578b\u5c0d\u6e05\u9192\u3001N1\u3001N2\u3001N3 \u548c REM \u7684\u7279\u5b9a\u6e96\u78ba\u5ea6\u5206\u5225\u70ba 86.70%\u300160.36%\u300183.89%\u300184.85% \u548c 87.16%\u3002\u672c\u7814\u7a76\u5f37\u8abf\u4e86\u6b0a\u91cd\u521d\u59cb\u5316\u548c\u8cc7\u6599\u64f4\u5145\u5728\u4f7f\u7528 ECG \u8a0a\u865f\u6700\u4f73\u5316\u7761\u7720\u5206\u671f\u5206\u985e\u4e2d\u7684\u91cd\u8981\u6027\u3002", "author": "Poorya Aghaomidi et.al.", "authors": "Poorya Aghaomidi, Ge Wang", "id": "2412.01929v1", "paper_url": "http://arxiv.org/abs/2412.01929v1", "repo": "null"}}