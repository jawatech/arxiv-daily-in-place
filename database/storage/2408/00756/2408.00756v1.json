{"2408.00756": {"publish_time": "2024-08-01", "title": "Segment anything model 2: an application to 2D and 3D medical images", "paper_summary": "Segment Anything Model (SAM) has gained significant attention because of its\nability to segment a variety of objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we provide an extensive evaluation of SAM\n2's ability to segment both 2D and 3D medical images. We collect 18 medical\nimaging datasets, including common 3D modalities such as computed tomography\n(CT), magnetic resonance imaging (MRI), and positron emission tomography (PET)\nas well as 2D modalities such as X-ray and ultrasound. We consider two\nevaluation pipelines of SAM 2: (1) multi-frame 3D segmentation, where prompts\nare provided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. We learn that SAM 2 exhibits similar performance as SAM\nunder single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.", "paper_summary_zh": "\u5206\u6bb5\u4efb\u4f55\u6a21\u578b (SAM) \u56e0\u5176\u5728\u7d66\u5b9a\u63d0\u793a\u7684\u60c5\u6cc1\u4e0b\u5206\u6bb5\u5716\u50cf\u4e2d\u5404\u7a2e\u7269\u9ad4\u7684\u80fd\u529b\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u6700\u8fd1\u958b\u767c\u7684 SAM 2 \u5df2\u5c07\u6b64\u80fd\u529b\u64f4\u5c55\u5230\u5f71\u7247\u8f38\u5165\u3002\u9019\u958b\u555f\u4e86\u4e00\u500b\u5c07 SAM \u61c9\u7528\u65bc 3D \u5f71\u50cf\u7684\u6a5f\u6703\uff0c\u9019\u662f\u91ab\u5b78\u5f71\u50cf\u9818\u57df\u7684\u57fa\u790e\u4efb\u52d9\u4e4b\u4e00\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c0d SAM 2 \u5206\u6bb5 2D \u548c 3D \u91ab\u5b78\u5f71\u50cf\u7684\u80fd\u529b\u9032\u884c\u4e86\u5ee3\u6cdb\u8a55\u4f30\u3002\u6211\u5011\u6536\u96c6\u4e86 18 \u500b\u91ab\u5b78\u5f71\u50cf\u8cc7\u6599\u96c6\uff0c\u5305\u62ec\u5e38\u898b\u7684 3D \u65b9\u5f0f\uff0c\u4f8b\u5982\u96fb\u8166\u65b7\u5c64\u6383\u63cf (CT)\u3001\u78c1\u632f\u9020\u5f71 (MRI) \u548c\u6b63\u5b50\u767c\u5c04\u65b7\u5c64\u6383\u63cf (PET)\uff0c\u4ee5\u53ca 2D \u65b9\u5f0f\uff0c\u4f8b\u5982 X \u5149\u548c\u8d85\u97f3\u6ce2\u3002\u6211\u5011\u8003\u616e\u4e86 SAM 2 \u7684\u5169\u500b\u8a55\u4f30\u7ba1\u9053\uff1a(1) \u591a\u5e40 3D \u5206\u6bb5\uff0c\u5176\u4e2d\u63d0\u793a\u63d0\u4f9b\u7d66\u5f9e\u9ad4\u7a4d\u4e2d\u9078\u53d6\u7684\u4e00\u500b\u6216\u591a\u500b\u5207\u7247\uff0c\u4ee5\u53ca (2) \u55ae\u5e40 2D \u5206\u6bb5\uff0c\u5176\u4e2d\u63d0\u793a\u63d0\u4f9b\u7d66\u6bcf\u500b\u5207\u7247\u3002\u524d\u8005\u50c5\u9069\u7528\u65bc 3D \u65b9\u5f0f\uff0c\u800c\u5f8c\u8005\u9069\u7528\u65bc 2D \u548c 3D \u65b9\u5f0f\u3002\u6211\u5011\u4e86\u89e3\u5230\uff0c\u5728\u55ae\u5e40 2D \u5206\u6bb5\u4e0b\uff0cSAM 2 \u8868\u73fe\u51fa\u8207 SAM \u76f8\u4f3c\u7684\u6548\u80fd\uff0c\u800c\u5728\u591a\u5e40 3D \u5206\u6bb5\u4e0b\u5247\u8868\u73fe\u51fa\u4e0d\u540c\u7684\u6548\u80fd\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u6a19\u8a18\u5207\u7247\u7684\u9078\u64c7\u3001\u50b3\u64ad\u65b9\u5411\u3001\u50b3\u64ad\u671f\u9593\u4f7f\u7528\u7684\u9810\u6e2c\u7b49\u3002", "author": "Haoyu Dong et.al.", "authors": "Haoyu Dong, Hanxue Gu, Yaqian Chen, Jichen Yang, Maciej A. Mazurowski", "id": "2408.00756v1", "paper_url": "http://arxiv.org/abs/2408.00756v1", "repo": "null"}}