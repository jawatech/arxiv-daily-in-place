{"2410.02596": {"publish_time": "2024-10-03", "title": "Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks", "paper_summary": "Generative Flow Networks (GFlowNets) are a novel class of generative models\ndesigned to sample from unnormalized distributions and have found applications\nin various important tasks, attracting great research interest in their\ntraining algorithms. In general, GFlowNets are trained by fitting the forward\nflow to the backward flow on sampled training objects. Prior work focused on\nthe choice of training objects, parameterizations, sampling and resampling\nstrategies, and backward policies, aiming to enhance credit assignment,\nexploration, or exploitation of the training process. However, the choice of\nregression loss, which can highly influence the exploration and exploitation\nbehavior of the under-training policy, has been overlooked. Due to the lack of\ntheoretical understanding for choosing an appropriate regression loss, most\nexisting algorithms train the flow network by minimizing the squared error of\nthe forward and backward flows in log-space, i.e., using the quadratic\nregression loss. In this work, we rigorously prove that distinct regression\nlosses correspond to specific divergence measures, enabling us to design and\nanalyze regression losses according to the desired properties of the\ncorresponding divergence measures. Specifically, we examine two key properties:\nzero-forcing and zero-avoiding, where the former promotes exploitation and\nhigher rewards, and the latter encourages exploration and enhances diversity.\nBased on our theoretical framework, we propose three novel regression losses,\nnamely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three\nbenchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our\nproposed losses are compatible with most existing training algorithms, and\nsignificantly improve the performances of the algorithms concerning convergence\nspeed, sample diversity, and robustness.", "paper_summary_zh": "\u751f\u6210\u6d41\u7db2\u8def (GFlowNets) \u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u751f\u6210\u6a21\u578b\u985e\u5225\uff0c\n\u65e8\u5728\u5f9e\u672a\u6b63\u898f\u5316\u7684\u5206\u4f48\u4e2d\u9032\u884c\u53d6\u6a23\uff0c\u4e26\u5df2\u5728\u5404\u7a2e\u91cd\u8981\u4efb\u52d9\u4e2d\u627e\u5230\u61c9\u7528\uff0c\n\u5728\u4ed6\u5011\u7684\u8a13\u7df4\u6f14\u7b97\u6cd5\u4e2d\u5438\u5f15\u4e86\u6975\u5927\u7684\u7814\u7a76\u8208\u8da3\u3002\u4e00\u822c\u4f86\u8aaa\uff0cGFlowNets \u662f\u900f\u904e\u5c07\u6b63\u5411\u6d41\u8207\u53d6\u6a23\u8a13\u7df4\u7269\u4ef6\u4e0a\u7684\u53cd\u5411\u6d41\u76f8\u64ec\u5408\u4f86\u8a13\u7df4\u7684\u3002\u5148\u524d\u7684\u7814\u7a76\u5c08\u6ce8\u65bc\u8a13\u7df4\u7269\u4ef6\u3001\u53c3\u6578\u5316\u3001\u53d6\u6a23\u548c\u518d\u53d6\u6a23\u7b56\u7565\u4ee5\u53ca\u53cd\u5411\u7b56\u7565\u7684\u9078\u64c7\uff0c\u65e8\u5728\u589e\u5f37\u8a13\u7df4\u904e\u7a0b\u7684\u4fe1\u7528\u5206\u914d\u3001\u63a2\u7d22\u6216\u5229\u7528\u3002\u7136\u800c\uff0c\u56de\u6b78\u640d\u5931\u7684\u9078\u64c7\u53ef\u80fd\u6703\u9ad8\u5ea6\u5f71\u97ff\u53d7\u8a13\u7df4\u7b56\u7565\u7684\u63a2\u7d22\u548c\u5229\u7528\u884c\u70ba\uff0c\u800c\u9019\u9ede\u5df2\u88ab\u5ffd\u7565\u3002\u7531\u65bc\u7f3a\u4e4f\u9078\u64c7\u9069\u7576\u56de\u6b78\u640d\u5931\u7684\u7406\u8ad6\u7406\u89e3\uff0c\u5927\u591a\u6578\u73fe\u6709\u6f14\u7b97\u6cd5\u900f\u904e\u6700\u5c0f\u5316\u5c0d\u6578\u7a7a\u9593\u4e2d\u6b63\u5411\u548c\u53cd\u5411\u6d41\u7684\u5e73\u65b9\u8aa4\u5dee\u4f86\u8a13\u7df4\u6d41\u7db2\u8def\uff0c\u5373\u4f7f\u7528\u4e8c\u6b21\u56de\u6b78\u640d\u5931\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u56b4\u8b39\u5730\u8b49\u660e\u4e86\u4e0d\u540c\u7684\u56de\u6b78\u640d\u5931\u5c0d\u61c9\u65bc\u7279\u5b9a\u7684\u8ddd\u96e2\u6e2c\u5ea6\uff0c\u4f7f\u6211\u5011\u80fd\u5920\u6839\u64da\u5c0d\u61c9\u8ddd\u96e2\u6e2c\u5ea6\u7684\u6240\u9700\u7279\u6027\u4f86\u8a2d\u8a08\u548c\u5206\u6790\u56de\u6b78\u640d\u5931\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u6aa2\u67e5\u4e86\u5169\u500b\u95dc\u9375\u7279\u6027\uff1a\u96f6\u5f37\u5236\u548c\u96f6\u907f\u514d\uff0c\u5176\u4e2d\u524d\u8005\u4fc3\u9032\u5229\u7528\u548c\u66f4\u9ad8\u7684\u56de\u5831\uff0c\u800c\u5f8c\u8005\u9f13\u52f5\u63a2\u7d22\u4e26\u589e\u5f37\u591a\u6a23\u6027\u3002\u57fa\u65bc\u6211\u5011\u7684\u7406\u8ad6\u6846\u67b6\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e09\u7a2e\u65b0\u7a4e\u7684\u56de\u6b78\u640d\u5931\uff0c\u5373 Shifted-Cosh\u3001Linex(1/2) \u548c Linex(1)\u3002\u6211\u5011\u5728\u4e09\u500b\u57fa\u6e96\u4e0a\u5c0d\u5b83\u5011\u9032\u884c\u8a55\u4f30\uff1a\u8d85\u7db2\u683c\u3001\u4f4d\u5143\u5e8f\u5217\u751f\u6210\u548c\u5206\u5b50\u751f\u6210\u3002\u6211\u5011\u63d0\u51fa\u7684\u640d\u5931\u8207\u5927\u591a\u6578\u73fe\u6709\u7684\u8a13\u7df4\u6f14\u7b97\u6cd5\u76f8\u5bb9\uff0c\u4e26\u4e14\u986f\u8457\u6539\u5584\u4e86\u6f14\u7b97\u6cd5\u5728\u6536\u6582\u901f\u5ea6\u3001\u6a23\u672c\u591a\u6a23\u6027\u548c\u7a69\u5065\u6027\u65b9\u9762\u7684\u6548\u80fd\u3002", "author": "Rui Hu et.al.", "authors": "Rui Hu, Yifan Zhang, Zhuoran Li, Longbo Huang", "id": "2410.02596v1", "paper_url": "http://arxiv.org/abs/2410.02596v1", "repo": "null"}}