{"2406.03439": {"publish_time": "2024-06-05", "title": "Text-to-Events: Synthetic Event Camera Streams from Conditional Text Input", "paper_summary": "Event cameras are advantageous for tasks that require vision sensors with\nlow-latency and sparse output responses. However, the development of deep\nnetwork algorithms using event cameras has been slow because of the lack of\nlarge labelled event camera datasets for network training. This paper reports a\nmethod for creating new labelled event datasets by using a text-to-X model,\nwhere X is one or multiple output modalities, in the case of this work, events.\nOur proposed text-to-events model produces synthetic event frames directly from\ntext prompts. It uses an autoencoder which is trained to produce sparse event\nframes representing event camera outputs. By combining the pretrained\nautoencoder with a diffusion model architecture, the new text-to-events model\nis able to generate smooth synthetic event streams of moving objects. The\nautoencoder was first trained on an event camera dataset of diverse scenes. In\nthe combined training with the diffusion model, the DVS gesture dataset was\nused. We demonstrate that the model can generate realistic event sequences of\nhuman gestures prompted by different text statements. The classification\naccuracy of the generated sequences, using a classifier trained on the real\ndataset, ranges between 42% to 92%, depending on the gesture group. The results\ndemonstrate the capability of this method in synthesizing event datasets.", "paper_summary_zh": "\u4e8b\u4ef6\u76f8\u6a5f\u5c0d\u65bc\u9700\u8981\u5177\u6709\u4f4e\u5ef6\u9072\u548c\u7a00\u758f\u8f38\u51fa\u56de\u61c9\u7684\u8996\u89ba\u611f\u6e2c\u5668\u7684\u4efb\u52d9\u4f86\u8aaa\u662f\u6709\u5229\u7684\u3002\u7136\u800c\uff0c\u7531\u65bc\u7f3a\u4e4f\u7528\u65bc\u7db2\u8def\u8a13\u7df4\u7684\u5927\u578b\u6a19\u7c64\u4e8b\u4ef6\u76f8\u6a5f\u8cc7\u6599\u96c6\uff0c\u4f7f\u7528\u4e8b\u4ef6\u76f8\u6a5f\u958b\u767c\u6df1\u5ea6\u7db2\u8def\u6f14\u7b97\u6cd5\u7684\u901f\u5ea6\u5f88\u6162\u3002\u672c\u6587\u5831\u544a\u4e86\u4e00\u7a2e\u4f7f\u7528\u6587\u672c\u5230 X \u6a21\u578b\u5efa\u7acb\u65b0\u7684\u6a19\u7c64\u4e8b\u4ef6\u8cc7\u6599\u96c6\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d X \u662f\u55ae\u4e00\u6216\u591a\u500b\u8f38\u51fa\u6a21\u5f0f\uff0c\u5728\u672c\u5de5\u4f5c\u4e2d\u70ba\u4e8b\u4ef6\u3002\u6211\u5011\u63d0\u51fa\u7684\u6587\u672c\u5230\u4e8b\u4ef6\u6a21\u578b\u76f4\u63a5\u5f9e\u6587\u672c\u63d0\u793a\u7522\u751f\u5408\u6210\u4e8b\u4ef6\u5e40\u3002\u5b83\u4f7f\u7528\u4e00\u500b\u81ea\u52d5\u7de8\u78bc\u5668\uff0c\u7d93\u904e\u8a13\u7df4\u53ef\u4ee5\u7522\u751f\u8868\u793a\u4e8b\u4ef6\u76f8\u6a5f\u8f38\u51fa\u7684\u7a00\u758f\u4e8b\u4ef6\u5e40\u3002\u901a\u904e\u5c07\u9810\u5148\u8a13\u7df4\u7684\u81ea\u52d5\u7de8\u78bc\u5668\u8207\u64f4\u6563\u6a21\u578b\u67b6\u69cb\u76f8\u7d50\u5408\uff0c\u65b0\u7684\u6587\u672c\u5230\u4e8b\u4ef6\u6a21\u578b\u80fd\u5920\u751f\u6210\u79fb\u52d5\u7269\u9ad4\u7684\u5e73\u6ed1\u5408\u6210\u4e8b\u4ef6\u4e32\u6d41\u3002\u81ea\u52d5\u7de8\u78bc\u5668\u9996\u5148\u5728\u5177\u6709\u4e0d\u540c\u5834\u666f\u7684\u4e8b\u4ef6\u76f8\u6a5f\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a13\u7df4\u3002\u5728\u8207\u64f4\u6563\u6a21\u578b\u7684\u7d44\u5408\u8a13\u7df4\u4e2d\uff0c\u4f7f\u7528\u4e86 DVS \u624b\u52e2\u8cc7\u6599\u96c6\u3002\u6211\u5011\u8b49\u660e\u4e86\u8a72\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u7531\u4e0d\u540c\u6587\u672c\u9673\u8ff0\u63d0\u793a\u7684\u4eba\u985e\u624b\u52e2\u7684\u903c\u771f\u4e8b\u4ef6\u5e8f\u5217\u3002\u4f7f\u7528\u5728\u771f\u5be6\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u5206\u985e\u5668\uff0c\u751f\u6210\u5e8f\u5217\u7684\u5206\u985e\u6e96\u78ba\u5ea6\u5728 42% \u5230 92% \u4e4b\u9593\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u624b\u52e2\u7d44\u3002\u7d50\u679c\u8b49\u660e\u4e86\u9019\u7a2e\u65b9\u6cd5\u5728\u5408\u6210\u4e8b\u4ef6\u8cc7\u6599\u96c6\u65b9\u9762\u7684\u80fd\u529b\u3002", "author": "Joachim Ott et.al.", "authors": "Joachim Ott, Zuowen Wang, Shih-Chii Liu", "id": "2406.03439v1", "paper_url": "http://arxiv.org/abs/2406.03439v1", "repo": "null"}}