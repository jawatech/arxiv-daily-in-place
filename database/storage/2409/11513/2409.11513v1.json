{"2409.11513": {"publish_time": "2024-09-17", "title": "Mamba Fusion: Learning Actions Through Questioning", "paper_summary": "Video Language Models (VLMs) are crucial for generalizing across diverse\ntasks and using language cues to enhance learning. While transformer-based\narchitectures have been the de facto in vision-language training, they face\nchallenges like quadratic computational complexity, high GPU memory usage, and\ndifficulty with long-term dependencies. To address these limitations, we\nintroduce MambaVL, a novel model that leverages recent advancements in\nselective state space modality fusion to efficiently capture long-range\ndependencies and learn joint representations for vision and language data.\nMambaVL utilizes a shared state transition matrix across both modalities,\nallowing the model to capture information about actions from multiple\nperspectives within the scene. Furthermore, we propose a question-answering\ntask that helps guide the model toward relevant cues. These questions provide\ncritical information about actions, objects, and environmental context, leading\nto enhanced performance. As a result, MambaVL achieves state-of-the-art\nperformance in action recognition on the Epic-Kitchens-100 dataset and\noutperforms baseline methods in action anticipation.", "paper_summary_zh": "\u5f71\u7247\u8a9e\u8a00\u6a21\u578b (VLM) \u5c0d\u65bc\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u9032\u884c\u6982\u62ec\u548c\u4f7f\u7528\u8a9e\u8a00\u63d0\u793a\u589e\u5f37\u5b78\u7fd2\u81f3\u95dc\u91cd\u8981\u3002\u96d6\u7136\u57fa\u65bc\u8b8a\u63db\u5668\u7684\u67b6\u69cb\u4e00\u76f4\u662f\u8996\u89ba\u8a9e\u8a00\u8a13\u7df4\u7684\u4e8b\u5be6\u6a19\u6e96\uff0c\u4f46\u5b83\u5011\u9762\u81e8\u4e8c\u6b21\u8a08\u7b97\u8907\u96dc\u5ea6\u3001\u9ad8 GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u7387\u548c\u9577\u671f\u4f9d\u8cf4\u6027\u7684\u56f0\u96e3\u7b49\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 MambaVL\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6a21\u578b\uff0c\u5229\u7528\u9078\u64c7\u6027\u72c0\u614b\u7a7a\u9593\u6a21\u614b\u878d\u5408\u7684\u6700\u65b0\u9032\u5c55\u4f86\u6709\u6548\u6355\u6349\u9060\u7a0b\u4f9d\u8cf4\u6027\u4e26\u5b78\u7fd2\u8996\u89ba\u548c\u8a9e\u8a00\u8cc7\u6599\u7684\u806f\u5408\u8868\u793a\u3002MambaVL \u5728\u5169\u7a2e\u6a21\u614b\u4e2d\u4f7f\u7528\u5171\u7528\u7684\u72c0\u614b\u8f49\u63db\u77e9\u9663\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u6355\u6349\u5834\u666f\u4e2d\u591a\u500b\u8996\u89d2\u7684\u52d5\u4f5c\u8cc7\u8a0a\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u6709\u52a9\u65bc\u5f15\u5c0e\u6a21\u578b\u671d\u5411\u76f8\u95dc\u63d0\u793a\u7684\u554f\u7b54\u4efb\u52d9\u3002\u9019\u4e9b\u554f\u984c\u63d0\u4f9b\u4e86\u6709\u95dc\u52d5\u4f5c\u3001\u7269\u4ef6\u548c\u74b0\u5883\u80cc\u666f\u7684\u91cd\u8981\u8cc7\u8a0a\uff0c\u5f9e\u800c\u589e\u5f37\u4e86\u6548\u80fd\u3002\u56e0\u6b64\uff0cMambaVL \u5728 Epic-Kitchens-100 \u8cc7\u6599\u96c6\u4e0a\u7684\u52d5\u4f5c\u8fa8\u8b58\u4e2d\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u4e26\u4e14\u5728\u52d5\u4f5c\u9810\u6e2c\u4e2d\u512a\u65bc\u57fa\u7dda\u65b9\u6cd5\u3002", "author": "Zhikang Dong et.al.", "authors": "Zhikang Dong, Apoorva Beedu, Jason Sheinkopf, Irfan Essa", "id": "2409.11513v1", "paper_url": "http://arxiv.org/abs/2409.11513v1", "repo": "https://github.com/dongzhikang/mambavl"}}