{"2406.07546": {"publish_time": "2024-06-11", "title": "Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?", "paper_summary": "We present a novel task and benchmark for evaluating the ability of\ntext-to-image(T2I) generation models to produce images that fit commonsense in\nreal life, which we call Commonsense-T2I. Given two adversarial text prompts\ncontaining an identical set of action words with minor differences, such as \"a\nlightbulb without electricity\" v.s. \"a lightbulb with electricity\", we evaluate\nwhether T2I models can conduct visual-commonsense reasoning, e.g. produce\nimages that fit \"the lightbulb is unlit\" vs. \"the lightbulb is lit\"\ncorrespondingly. Commonsense-T2I presents an adversarial challenge, providing\npairwise text prompts along with expected outputs. The dataset is carefully\nhand-curated by experts and annotated with fine-grained labels, such as\ncommonsense type and likelihood of the expected outputs, to assist analyzing\nmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I models\nand surprisingly find that, there is still a large gap between image synthesis\nand real life photos--even the DALL-E 3 model could only achieve 48.92% on\nCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%\naccuracy. Our experiments show that GPT-enriched prompts cannot solve this\nchallenge, and we include a detailed analysis about possible reasons for such\ndeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluation\nbenchmark for T2I commonsense checking, fostering advancements in real life\nimage generation.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u9805\u65b0\u4efb\u52d9\u548c\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30\u6587\u5b57\u8f49\u5716\u50cf (T2I) \u751f\u6210\u6a21\u578b\u7522\u751f\u7b26\u5408\u73fe\u5be6\u751f\u6d3b\u5e38\u8b58\u5716\u50cf\u7684\u80fd\u529b\uff0c\u6211\u5011\u7a31\u4e4b\u70ba\u5e38\u8b58 T2I\u3002\u7d66\u5b9a\u5169\u500b\u5c0d\u6297\u6027\u7684\u6587\u5b57\u63d0\u793a\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u7d44\u76f8\u540c\u7684\u52d5\u4f5c\u8a5e\uff0c\u4f46\u6709\u7d30\u5fae\u7684\u5dee\u7570\uff0c\u4f8b\u5982\u300c\u6c92\u6709\u96fb\u7684\u71c8\u6ce1\u300d\u8207\u300c\u6709\u96fb\u7684\u71c8\u6ce1\u300d\uff0c\u6211\u5011\u8a55\u4f30 T2I \u6a21\u578b\u662f\u5426\u80fd\u9032\u884c\u8996\u89ba\u5e38\u8b58\u63a8\u7406\uff0c\u4f8b\u5982\u7522\u751f\u7b26\u5408\u300c\u71c8\u6ce1\u672a\u9ede\u4eae\u300d\u8207\u300c\u71c8\u6ce1\u5df2\u9ede\u4eae\u300d\u7684\u5716\u50cf\u3002\u5e38\u8b58 T2I \u5448\u73fe\u4e86\u4e00\u500b\u5c0d\u6297\u6027\u7684\u6311\u6230\uff0c\u63d0\u4f9b\u4e86\u6210\u5c0d\u7684\u6587\u5b57\u63d0\u793a\u4ee5\u53ca\u9810\u671f\u7684\u8f38\u51fa\u3002\u8a72\u6578\u64da\u96c6\u7531\u5c08\u5bb6\u4ed4\u7d30\u624b\u5de5\u7b56\u5283\uff0c\u4e26\u8a3b\u89e3\u4e86\u7d30\u7c92\u5ea6\u7684\u6a19\u7c64\uff0c\u4f8b\u5982\u5e38\u8b58\u985e\u578b\u548c\u9810\u671f\u8f38\u51fa\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u5354\u52a9\u5206\u6790\u6a21\u578b\u884c\u70ba\u3002\u6211\u5011\u5c0d\u5404\u7a2e\u6700\u5148\u9032 (sota) T2I \u6a21\u578b\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u4ee4\u4eba\u9a5a\u8a1d\u5730\u767c\u73fe\uff0c\u5716\u50cf\u5408\u6210\u548c\u73fe\u5be6\u751f\u6d3b\u7167\u7247\u4e4b\u9593\u4ecd\u7136\u5b58\u5728\u5f88\u5927\u5dee\u8ddd\u2014\u2014\u5373\u4f7f\u662f DALL-E 3 \u6a21\u578b\u5728\u5e38\u8b58 T2I \u4e0a\u4e5f\u50c5\u80fd\u9054\u5230 48.92%\uff0c\u800c\u7a69\u5b9a\u64f4\u6563 XL \u6a21\u578b\u50c5\u9054\u5230 24.92% \u7684\u6e96\u78ba\u7387\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cGPT \u8c50\u5bcc\u7684\u63d0\u793a\u7121\u6cd5\u89e3\u6c7a\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u5c0d\u9019\u7a2e\u7f3a\u9677\u7684\u53ef\u80fd\u539f\u56e0\u9032\u884c\u4e86\u8a73\u7d30\u5206\u6790\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\u8b93\u5e38\u8b58 T2I \u6210\u70ba T2I \u5e38\u8b58\u6aa2\u67e5\u7684\u9ad8\u54c1\u8cea\u8a55\u4f30\u57fa\u6e96\uff0c\u4fc3\u9032\u73fe\u5be6\u751f\u6d3b\u5716\u50cf\u751f\u6210\u7684\u9032\u6b65\u3002</paragraph>", "author": "Xingyu Fu et.al.", "authors": "Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, Dan Roth", "id": "2406.07546v1", "paper_url": "http://arxiv.org/abs/2406.07546v1", "repo": "null"}}