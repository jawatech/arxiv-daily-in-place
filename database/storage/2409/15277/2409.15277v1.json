{"2409.15277": {"publish_time": "2024-09-23", "title": "A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?", "paper_summary": "Large language models (LLMs) have exhibited remarkable capabilities across\nvarious domains and tasks, pushing the boundaries of our knowledge in learning\nand cognition. The latest model, OpenAI's o1, stands out as the first LLM with\nan internalized chain-of-thought technique using reinforcement learning\nstrategies. While it has demonstrated surprisingly strong capabilities on\nvarious general language tasks, its performance in specialized fields such as\nmedicine remains unknown. To this end, this report provides a comprehensive\nexploration of o1 on different medical scenarios, examining 3 key aspects:\nunderstanding, reasoning, and multilinguality. Specifically, our evaluation\nencompasses 6 tasks using data from 37 medical datasets, including two newly\nconstructed and more challenging question-answering (QA) tasks based on\nprofessional medical quizzes from the New England Journal of Medicine (NEJM)\nand The Lancet. These datasets offer greater clinical relevance compared to\nstandard medical QA benchmarks such as MedQA, translating more effectively into\nreal-world clinical utility. Our analysis of o1 suggests that the enhanced\nreasoning ability of LLMs may (significantly) benefit their capability to\nunderstand various medical instructions and reason through complex clinical\nscenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average\nof 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios.\nBut meanwhile, we identify several weaknesses in both the model capability and\nthe existing evaluation protocols, including hallucination, inconsistent\nmultilingual ability, and discrepant metrics for evaluation. We release our raw\ndata and model outputs at https://ucsc-vlaa.github.io/o1_medicine/ for future\nresearch.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u9818\u57df\u548c\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u63a8\u52d5\u4e86\u6211\u5011\u5728\u5b78\u7fd2\u548c\u8a8d\u77e5\u65b9\u9762\u7684\u77e5\u8b58\u754c\u9650\u3002\u6700\u65b0\u7684\u6a21\u578b\uff0cOpenAI \u7684 o1\uff0c\u812b\u7a4e\u800c\u51fa\uff0c\u6210\u70ba\u7b2c\u4e00\u500b\u4f7f\u7528\u5f37\u5316\u5b78\u7fd2\u7b56\u7565\u5167\u5316\u601d\u60f3\u93c8\u6280\u8853\u7684 LLM\u3002\u96d6\u7136\u5b83\u5728\u5404\u7a2e\u4e00\u822c\u8a9e\u8a00\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u9a5a\u4eba\u5f37\u5927\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u5728\u91ab\u5b78\u7b49\u5c08\u696d\u9818\u57df\u7684\u8868\u73fe\u4ecd\u7136\u672a\u77e5\u3002\u70ba\u6b64\uff0c\u672c\u5831\u544a\u5168\u9762\u63a2\u8a0e\u4e86 o1 \u5728\u4e0d\u540c\u91ab\u7642\u60c5\u5883\u4e2d\u7684\u8868\u73fe\uff0c\u6aa2\u8996\u4e86 3 \u500b\u95dc\u9375\u9762\u5411\uff1a\u7406\u89e3\u3001\u63a8\u7406\u548c\u591a\u8a9e\u8a00\u80fd\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u8a55\u4f30\u6db5\u84cb\u4e86\u4f7f\u7528\u4f86\u81ea 37 \u500b\u91ab\u7642\u8cc7\u6599\u96c6\u7684\u8cc7\u6599\u7684 6 \u9805\u4efb\u52d9\uff0c\u5305\u62ec\u5169\u500b\u65b0\u5efa\u69cb\u4e14\u66f4\u5177\u6311\u6230\u6027\u7684\u554f\u7b54 (QA) \u4efb\u52d9\uff0c\u9019\u4e9b\u4efb\u52d9\u662f\u6839\u64da\u65b0\u82f1\u683c\u862d\u91ab\u5b78\u671f\u520a (NEJM) \u548c\u523a\u80f3\u91dd\u96dc\u8a8c\u7684\u5c08\u696d\u91ab\u7642\u6e2c\u9a57\u800c\u4f86\u3002\u8207 MedQA \u7b49\u6a19\u6e96\u91ab\u7642 QA \u57fa\u6e96\u76f8\u6bd4\uff0c\u9019\u4e9b\u8cc7\u6599\u96c6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u81e8\u5e8a\u76f8\u95dc\u6027\uff0c\u66f4\u6709\u6548\u5730\u8f49\u5316\u70ba\u5be6\u969b\u7684\u81e8\u5e8a\u6548\u7528\u3002\u6211\u5011\u5c0d o1 \u7684\u5206\u6790\u8868\u660e\uff0cLLM \u589e\u5f37\u7684\u63a8\u7406\u80fd\u529b\u53ef\u80fd\uff08\u986f\u8457\u5730\uff09\u63d0\u5347\u5b83\u5011\u7406\u89e3\u5404\u7a2e\u91ab\u7642\u6307\u793a\u548c\u63a8\u7406\u8907\u96dc\u81e8\u5e8a\u60c5\u5883\u7684\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0co1 \u5728 19 \u500b\u8cc7\u6599\u96c6\u548c\u5169\u500b\u65b0\u5efa\u7acb\u7684\u8907\u96dc QA \u60c5\u5883\u4e2d\u7684\u6e96\u78ba\u5ea6\u5e73\u5747\u6bd4\u5148\u524d\u7684 GPT-4 \u9ad8\u51fa 6.2% \u548c 6.6%\u3002\u4f46\u540c\u6642\uff0c\u6211\u5011\u767c\u73fe\u6a21\u578b\u80fd\u529b\u548c\u73fe\u6709\u8a55\u4f30\u5354\u5b9a\u4e2d\u5b58\u5728\u82e5\u5e72\u5f31\u9ede\uff0c\u5305\u62ec\u5e7b\u89ba\u3001\u4e0d\u4e00\u81f4\u7684\u591a\u8a9e\u8a00\u80fd\u529b\u548c\u8a55\u4f30\u7684\u5dee\u7570\u5316\u6307\u6a19\u3002\u6211\u5011\u5728 https://ucsc-vlaa.github.io/o1_medicine/ \u767c\u5e03\u6211\u5011\u7684\u539f\u59cb\u8cc7\u6599\u548c\u6a21\u578b\u8f38\u51fa\uff0c\u4ee5\u4f9b\u672a\u4f86\u7814\u7a76\u3002", "author": "Yunfei Xie et.al.", "authors": "Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, Yuyin Zhou", "id": "2409.15277v1", "paper_url": "http://arxiv.org/abs/2409.15277v1", "repo": "null"}}