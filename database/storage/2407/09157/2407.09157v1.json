{"2407.09157": {"publish_time": "2024-07-12", "title": "Movie Recommendation with Poster Attention via Multi-modal Transformer Feature Fusion", "paper_summary": "Pre-trained models learn general representations from large datsets which can\nbe fine-turned for specific tasks to significantly reduce training time.\nPre-trained models like generative pretrained transformers (GPT), bidirectional\nencoder representations from transformers (BERT), vision transfomers (ViT) have\nbecome a cornerstone of current research in machine learning. This study\nproposes a multi-modal movie recommendation system by extract features of the\nwell designed posters for each movie and the narrative text description of the\nmovie. This system uses the BERT model to extract the information of text\nmodality, the ViT model applied to extract the information of poster/image\nmodality, and the Transformer architecture for feature fusion of all modalities\nto predict users' preference. The integration of pre-trained foundational\nmodels with some smaller data sets in downstream applications capture\nmulti-modal content features in a more comprehensive manner, thereby providing\nmore accurate recommendations. The efficiency of the proof-of-concept model is\nverified by the standard benchmark problem the MovieLens 100K and 1M datasets.\nThe prediction accuracy of user ratings is enhanced in comparison to the\nbaseline algorithm, thereby demonstrating the potential of this cross-modal\nalgorithm to be applied for movie or video recommendation.", "paper_summary_zh": "\u9810\u8a13\u7df4\u6a21\u578b\u5f9e\u5927\u578b\u8cc7\u6599\u96c6\u4e2d\u5b78\u7fd2\u4e00\u822c\u8868\u793a\uff0c\u53ef\u4ee5\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u9032\u884c\u5fae\u8abf\uff0c\u4ee5\u986f\u8457\u6e1b\u5c11\u8a13\u7df4\u6642\u9593\u3002\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u4f8b\u5982\u751f\u6210\u5f0f\u9810\u8a13\u7df4Transformer (GPT)\u3001\u4f86\u81eaTransformer\u7684\u96d9\u5411\u7de8\u78bc\u8868\u793a (BERT)\u3001\u8996\u89baTransformer (ViT)\uff0c\u5df2\u6210\u70ba\u7576\u524d\u6a5f\u5668\u5b78\u7fd2\u7814\u7a76\u7684\u57fa\u77f3\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u500b\u591a\u6a21\u614b\u96fb\u5f71\u63a8\u85a6\u7cfb\u7d71\uff0c\u901a\u904e\u63d0\u53d6\u70ba\u6bcf\u90e8\u96fb\u5f71\u8a2d\u8a08\u7cbe\u7f8e\u7684\u6d77\u5831\u548c\u96fb\u5f71\u7684\u6558\u4e8b\u6587\u5b57\u63cf\u8ff0\u7684\u7279\u5fb5\u3002\u9019\u500b\u7cfb\u7d71\u4f7f\u7528 BERT \u6a21\u578b\u63d0\u53d6\u6587\u672c\u6a21\u614b\u7684\u4fe1\u606f\uff0c\u61c9\u7528 ViT \u6a21\u578b\u63d0\u53d6\u6d77\u5831/\u5716\u50cf\u6a21\u614b\u7684\u4fe1\u606f\uff0c\u4ee5\u53caTransformer\u67b6\u69cb\u878d\u5408\u6240\u6709\u6a21\u614b\u7684\u7279\u5fb5\u4f86\u9810\u6e2c\u7528\u6236\u7684\u504f\u597d\u3002\u9810\u8a13\u7df4\u57fa\u790e\u6a21\u578b\u8207\u4e0b\u6e38\u61c9\u7528\u4e2d\u4e00\u4e9b\u8f03\u5c0f\u7684\u6578\u64da\u96c6\u7684\u6574\u5408\u4ee5\u66f4\u5168\u9762\u7684\u65b9\u5f0f\u64f7\u53d6\u591a\u6a21\u614b\u5167\u5bb9\u7279\u5fb5\uff0c\u5f9e\u800c\u63d0\u4f9b\u66f4\u6e96\u78ba\u7684\u5efa\u8b70\u3002\u6982\u5ff5\u9a57\u8b49\u6a21\u578b\u7684\u6548\u7387\u901a\u904e\u6a19\u6e96\u57fa\u6e96\u554f\u984c MovieLens 100K \u548c 1M \u6578\u64da\u96c6\u5f97\u5230\u9a57\u8b49\u3002\u8207\u57fa\u7dda\u6f14\u7b97\u6cd5\u76f8\u6bd4\uff0c\u7528\u6236\u8a55\u5206\u7684\u9810\u6e2c\u6e96\u78ba\u5ea6\u5f97\u5230\u63d0\u5347\uff0c\u5f9e\u800c\u8b49\u660e\u4e86\u9019\u7a2e\u8de8\u6a21\u614b\u6f14\u7b97\u6cd5\u61c9\u7528\u65bc\u96fb\u5f71\u6216\u5f71\u7247\u63a8\u85a6\u7684\u6f5b\u529b\u3002", "author": "Linhan Xia et.al.", "authors": "Linhan Xia, Yicheng Yang, Ziou Chen, Zheng Yang, Shengxin Zhu", "id": "2407.09157v1", "paper_url": "http://arxiv.org/abs/2407.09157v1", "repo": "null"}}