{"2412.17744": {"publish_time": "2024-12-23", "title": "RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation", "paper_summary": "Repository-level code translation refers to translating an entire code\nrepository from one programming language to another while preserving the\nfunctionality of the source repository. Many benchmarks have been proposed to\nevaluate the performance of such code translators. However, previous benchmarks\nmostly provide fine-grained samples, focusing at either code snippet, function,\nor file-level code translation. Such benchmarks do not accurately reflect\nreal-world demands, where entire repositories often need to be translated,\ninvolving longer code length and more complex functionalities. To address this\ngap, we propose a new benchmark, named RepoTransBench, which is a real-world\nrepository-level code translation benchmark with an automatically executable\ntest suite. We conduct experiments on RepoTransBench to evaluate the\ntranslation performance of 11 advanced LLMs. We find that the Success@1 score\n(test success in one attempt) of the best-performing LLM is only 7.33%. To\nfurther explore the potential of LLMs for repository-level code translation, we\nprovide LLMs with error-related feedback to perform iterative debugging and\nobserve an average 7.09% improvement on Success@1. However, even with this\nimprovement, the Success@1 score of the best-performing LLM is only 21%, which\nmay not meet the need for reliable automatic repository-level code translation.\nFinally, we conduct a detailed error analysis and highlight current LLMs'\ndeficiencies in repository-level code translation, which could provide a\nreference for further improvements.", "paper_summary_zh": "<paragraph>\u5132\u5b58\u5eab\u5c64\u7d1a\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\u662f\u6307\u5c07\u6574\u500b\u7a0b\u5f0f\u78bc\u5132\u5b58\u5eab\u5f9e\u4e00\u7a2e\u7a0b\u5f0f\u8a9e\u8a00\u7ffb\u8b6f\u6210\u53e6\u4e00\u7a2e\uff0c\u540c\u6642\u4fdd\u7559\u4f86\u6e90\u5132\u5b58\u5eab\u7684\u529f\u80fd\u3002\u5df2\u7d93\u63d0\u51fa\u4e86\u8a31\u591a\u57fa\u6e96\u4f86\u8a55\u4f30\u6b64\u985e\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\u5668\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u4ee5\u524d\u7684\u57fa\u6e96\u5927\u591a\u63d0\u4f9b\u7d30\u7c92\u5ea6\u7684\u7bc4\u4f8b\uff0c\u5c08\u6ce8\u65bc\u7a0b\u5f0f\u78bc\u7247\u6bb5\u3001\u51fd\u5f0f\u6216\u6a94\u6848\u5c64\u7d1a\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\u3002\u6b64\u985e\u57fa\u6e96\u4e26\u672a\u6e96\u78ba\u53cd\u6620\u5be6\u969b\u9700\u6c42\uff0c\u5728\u5be6\u969b\u9700\u6c42\u4e2d\uff0c\u901a\u5e38\u9700\u8981\u7ffb\u8b6f\u6574\u500b\u5132\u5b58\u5eab\uff0c\u6d89\u53ca\u8f03\u9577\u7684\u7a0b\u5f0f\u78bc\u9577\u5ea6\u548c\u66f4\u8907\u96dc\u7684\u529f\u80fd\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u7a31\u70ba RepoTransBench\uff0c\u9019\u662f\u4e00\u500b\u771f\u5be6\u4e16\u754c\u7684\u5132\u5b58\u5eab\u5c64\u7d1a\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\u57fa\u6e96\uff0c\u4e26\u5177\u6709\u81ea\u52d5\u53ef\u57f7\u884c\u6e2c\u8a66\u5957\u4ef6\u3002\u6211\u5011\u5728 RepoTransBench \u4e0a\u9032\u884c\u5be6\u9a57\uff0c\u4ee5\u8a55\u4f30 11 \u500b\u5148\u9032 LLM \u7684\u7ffb\u8b6f\u6548\u80fd\u3002\u6211\u5011\u767c\u73fe\uff0c\u6548\u80fd\u6700\u4f73\u7684 LLM \u7684 Success@1 \u5206\u6578\uff08\u4e00\u6b21\u5617\u8a66\u4e2d\u7684\u6e2c\u8a66\u6210\u529f\u7387\uff09\u50c5\u70ba 7.33%\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u63a2\u7d22 LLM \u5728\u5132\u5b58\u5eab\u5c64\u7d1a\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\u65b9\u9762\u7684\u6f5b\u529b\uff0c\u6211\u5011\u70ba LLM \u63d0\u4f9b\u8207\u932f\u8aa4\u76f8\u95dc\u7684\u56de\u994b\uff0c\u4ee5\u57f7\u884c\u53cd\u8986\u9664\u932f\uff0c\u4e26\u89c0\u5bdf\u5230 Success@1 \u5e73\u5747\u63d0\u5347 7.09%\u3002\u7136\u800c\uff0c\u5373\u4f7f\u6709\u6b64\u63d0\u5347\uff0c\u6548\u80fd\u6700\u4f73\u7684 LLM \u7684 Success@1 \u5206\u6578\u4e5f\u50c5\u70ba 21%\uff0c\u53ef\u80fd\u7121\u6cd5\u6eff\u8db3\u53ef\u9760\u7684\u81ea\u52d5\u5132\u5b58\u5eab\u5c64\u7d1a\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\u9700\u6c42\u3002\u6700\u5f8c\uff0c\u6211\u5011\u9032\u884c\u8a73\u7d30\u7684\u932f\u8aa4\u5206\u6790\uff0c\u4e26\u5f37\u8abf\u7576\u524d LLM \u5728\u5132\u5b58\u5eab\u5c64\u7d1a\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\u4e2d\u7684\u7f3a\u9677\uff0c\u9019\u53ef\u4ee5\u4f5c\u70ba\u9032\u4e00\u6b65\u6539\u9032\u7684\u53c3\u8003\u3002</paragraph>", "author": "Yanli Wang et.al.", "authors": "Yanli Wang, Yanlin Wang, Suiquan Wang, Daya Guo, Jiachi Chen, John Grundy, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, Zibin Zheng", "id": "2412.17744v1", "paper_url": "http://arxiv.org/abs/2412.17744v1", "repo": "null"}}