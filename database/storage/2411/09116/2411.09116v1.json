{"2411.09116": {"publish_time": "2024-11-14", "title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs", "paper_summary": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we present a pipeline for selecting available and\nreasonable benchmarks from massive ones, addressing the oversight in previous\nwork regarding the utility of these benchmarks, i.e., their ability to\ndifferentiate between models being evaluated. Leveraging this pipeline, we\nintroduce P-MMEval, a large-scale benchmark covering effective fundamental and\ncapability-specialized datasets. Furthermore, P-MMEval delivers consistent\nlanguage coverage across various datasets and provides parallel samples.\nFinally, we conduct extensive experiments on representative multilingual model\nseries to compare performances across models, analyze dataset effectiveness,\nexamine prompt impacts on model performances, and explore the relationship\nbetween multilingual performances and factors such as tasks, model sizes, and\nlanguages. These insights offer valuable guidance for future research. The\ndataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5c55\u793a\u4e86\u5728\u7ffb\u8b6f\u3001\u7a0b\u5f0f\u78bc\u7522\u751f\u548c\u63a8\u7406\u7b49\u4efb\u52d9\u4e0a\u7684\u591a\u7a2e\u591a\u8a9e\u80fd\u529b\u3002\u5148\u524d\u7684\u8a55\u4f30\u901a\u5e38\u5c07\u5176\u7bc4\u570d\u9650\u5236\u5728\u57fa\u672c\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u6216\u5b64\u7acb\u7684\u80fd\u529b\u7279\u5b9a\u4efb\u52d9\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u7f3a\u9ede\uff0c\u6211\u5011\u65e8\u5728\u63d0\u51fa\u4e00\u500b\u5168\u9762\u7684\u591a\u8a9e\u8a00\u591a\u4efb\u52d9\u57fa\u6e96\u3002\u9996\u5148\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7ba1\u9053\uff0c\u7528\u65bc\u5f9e\u5927\u91cf\u7684\u57fa\u6e96\u4e2d\u9078\u64c7\u53ef\u7528\u4e14\u5408\u7406\u7684\u57fa\u6e96\uff0c\u89e3\u6c7a\u5148\u524d\u5de5\u4f5c\u4e2d\u95dc\u65bc\u9019\u4e9b\u57fa\u6e96\u7684\u6548\u7528\uff08\u5373\u5b83\u5011\u5340\u5206\u6b63\u5728\u8a55\u4f30\u7684\u6a21\u578b\u7684\u80fd\u529b\uff09\u7684\u758f\u5ffd\u3002\u5229\u7528\u9019\u500b\u7ba1\u9053\uff0c\u6211\u5011\u5f15\u5165\u4e86 P-MMEval\uff0c\u4e00\u500b\u6db5\u84cb\u6709\u6548\u7684\u57fa\u672c\u548c\u80fd\u529b\u5c08\u7528\u8cc7\u6599\u96c6\u7684\u5927\u898f\u6a21\u57fa\u6e96\u3002\u6b64\u5916\uff0cP-MMEval \u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u4e0a\u63d0\u4f9b\u4e86\u4e00\u81f4\u7684\u8a9e\u8a00\u8986\u84cb\u7bc4\u570d\uff0c\u4e26\u63d0\u4f9b\u4e26\u884c\u7bc4\u4f8b\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c0d\u5177\u4ee3\u8868\u6027\u7684\u591a\u8a9e\u8a00\u6a21\u578b\u7cfb\u5217\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4ee5\u6bd4\u8f03\u6a21\u578b\u4e4b\u9593\u7684\u6548\u80fd\u3001\u5206\u6790\u8cc7\u6599\u96c6\u7684\u6709\u6548\u6027\u3001\u6aa2\u67e5\u63d0\u793a\u5c0d\u6a21\u578b\u6548\u80fd\u7684\u5f71\u97ff\uff0c\u4e26\u63a2\u8a0e\u591a\u8a9e\u8a00\u6548\u80fd\u8207\u4efb\u52d9\u3001\u6a21\u578b\u5927\u5c0f\u548c\u8a9e\u8a00\u7b49\u56e0\u7d20\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\u9019\u4e9b\u898b\u89e3\u70ba\u672a\u4f86\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u6307\u5c0e\u3002\u8cc7\u6599\u96c6\u53ef\u5728 https://huggingface.co/datasets/Qwen/P-MMEval \u53d6\u5f97\u3002", "author": "Yidan Zhang et.al.", "authors": "Yidan Zhang, Boyi Deng, Yu Wan, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, Fei Huang, Jingren Zhou", "id": "2411.09116v1", "paper_url": "http://arxiv.org/abs/2411.09116v1", "repo": "null"}}