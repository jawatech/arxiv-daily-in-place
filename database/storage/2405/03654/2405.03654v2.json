{"2405.03654": {"publish_time": "2024-05-06", "title": "Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent", "paper_summary": "To demonstrate and address the underlying maliciousness, we propose a\ntheoretical hypothesis and analytical approach, and introduce a new black-box\njailbreak attack methodology named IntentObfuscator, exploiting this identified\nflaw by obfuscating the true intentions behind user prompts.This approach\ncompels LLMs to inadvertently generate restricted content, bypassing their\nbuilt-in content security measures. We detail two implementations under this\nframework: \"Obscure Intention\" and \"Create Ambiguity\", which manipulate query\ncomplexity and ambiguity to evade malicious intent detection effectively. We\nempirically validate the effectiveness of the IntentObfuscator method across\nseveral models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving\nan average jailbreak success rate of 69.21\\%. Notably, our tests on\nChatGPT-3.5, which claims 100 million weekly active users, achieved a\nremarkable success rate of 83.65\\%. We also extend our validation to diverse\ntypes of sensitive content like graphic violence, racism, sexism, political\nsensitivity, cybersecurity threats, and criminal skills, further proving the\nsubstantial impact of our findings on enhancing 'Red Team' strategies against\nLLM content security frameworks.", "paper_summary_zh": "", "author": "Shang Shang et.al.", "authors": "Shang Shang,Xinqiang Zhao,Zhongjiang Yao,Yepeng Yao,Liya Su,Zijing Fan,Xiaodan Zhang,Zhengwei Jiang", "id": "2405.03654v2", "paper_url": "http://arxiv.org/abs/2405.03654v2", "repo": "null"}}