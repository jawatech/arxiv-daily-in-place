{"2502.03843": {"publish_time": "2025-02-06", "title": "Improving Natural Language Understanding for LLMs via Large-Scale Instruction Synthesis", "paper_summary": "High-quality, large-scale instructions are crucial for aligning large\nlanguage models (LLMs), however, there is a severe shortage of instruction in\nthe field of natural language understanding (NLU). Previous works on\nconstructing NLU instructions mainly focus on information extraction (IE),\nneglecting tasks such as machine reading comprehension, question answering, and\ntext classification. Furthermore, the lack of diversity in the data has led to\na decreased generalization ability of trained LLMs in other NLU tasks and a\nnoticeable decline in the fundamental model's general capabilities. To address\nthis issue, we propose Hum, a large-scale, high-quality synthetic instruction\ncorpus for NLU tasks, designed to enhance the NLU capabilities of LLMs.\nSpecifically, Hum includes IE (either close IE or open IE), machine reading\ncomprehension, text classification, and instruction generalist tasks, thereby\nenriching task diversity. Additionally, we introduce a human-LLMs collaborative\nmechanism to synthesize instructions, which enriches instruction diversity by\nincorporating guidelines, preference rules, and format variants. We conduct\nextensive experiments on 5 NLU tasks and 28 general capability evaluation\ndatasets for LLMs. Experimental results show that Hum enhances the NLU\ncapabilities of six LLMs by an average of 3.1\\%, with no significant decline\nobserved in other general capabilities.", "paper_summary_zh": "\u9ad8\u54c1\u8cea\u3001\u5927\u898f\u6a21\u7684\u6307\u793a\u5c0d\u65bc\u8abf\u6574\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u81f3\u95dc\u91cd\u8981\uff0c\u7136\u800c\uff0c\u5728\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u9818\u57df\u4e2d\u56b4\u91cd\u7f3a\u4e4f\u6307\u793a\u3002\u5148\u524d\u95dc\u65bc\u5efa\u69cb NLU \u6307\u793a\u7684\u7814\u7a76\u4e3b\u8981\u5074\u91cd\u65bc\u8cc7\u8a0a\u8403\u53d6 (IE)\uff0c\u5ffd\u7565\u4e86\u6a5f\u5668\u95b1\u8b80\u7406\u89e3\u3001\u554f\u984c\u56de\u7b54\u548c\u6587\u5b57\u5206\u985e\u7b49\u4efb\u52d9\u3002\u6b64\u5916\uff0c\u8cc7\u6599\u7f3a\u4e4f\u591a\u6a23\u6027\u5c0e\u81f4\u8a13\u7df4\u5f8c\u7684 LLM \u5728\u5176\u4ed6 NLU \u4efb\u52d9\u4e2d\u7684\u6982\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u4ee5\u53ca\u57fa\u790e\u6a21\u578b\u7684\u6574\u9ad4\u80fd\u529b\u986f\u8457\u4e0b\u964d\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa Hum\uff0c\u9019\u662f\u4e00\u500b\u91dd\u5c0d NLU \u4efb\u52d9\u7684\u5927\u898f\u6a21\u3001\u9ad8\u54c1\u8cea\u5408\u6210\u6307\u793a\u8a9e\u6599\u5eab\uff0c\u65e8\u5728\u589e\u5f37 LLM \u7684 NLU \u80fd\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cHum \u5305\u542b IE\uff08\u5c01\u9589\u5f0f IE \u6216\u958b\u653e\u5f0f IE\uff09\u3001\u6a5f\u5668\u95b1\u8b80\u7406\u89e3\u3001\u6587\u5b57\u5206\u985e\u548c\u6307\u793a\u901a\u624d\u4efb\u52d9\uff0c\u5f9e\u800c\u8c50\u5bcc\u4efb\u52d9\u591a\u6a23\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u4eba\u6a5f\u5354\u4f5c\u6a5f\u5236\u4f86\u5408\u6210\u6307\u793a\uff0c\u900f\u904e\u7d0d\u5165\u6e96\u5247\u3001\u504f\u597d\u898f\u5247\u548c\u683c\u5f0f\u8b8a\u9ad4\uff0c\u8c50\u5bcc\u6307\u793a\u7684\u591a\u6a23\u6027\u3002\u6211\u5011\u91dd\u5c0d 5 \u500b NLU \u4efb\u52d9\u548c 28 \u500b LLM \u7684\u4e00\u822c\u80fd\u529b\u8a55\u4f30\u8cc7\u6599\u96c6\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cHum \u5c07\u516d\u500b LLM \u7684 NLU \u80fd\u529b\u5e73\u5747\u63d0\u5347\u4e86 3.1%\uff0c\u800c\u5176\u4ed6\u4e00\u822c\u80fd\u529b\u5247\u6c92\u6709\u986f\u8457\u4e0b\u964d\u3002", "author": "Lin Yuan et.al.", "authors": "Lin Yuan, Jun Xu, Honghao Gui, Mengshu Sun, Zhiqiang Zhang, Lei Liang, Jun Zhou", "id": "2502.03843v1", "paper_url": "http://arxiv.org/abs/2502.03843v1", "repo": "null"}}