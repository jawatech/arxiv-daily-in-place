{"2405.17428": {"publish_time": "2024-05-27", "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models", "paper_summary": "Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model with a variety of architectural designs and\ntraining procedures to significantly enhance the performance of LLM as a\nversatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For model training, we introduce a\ntwo-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ndatasets into instruction tuning, which not only enhances non-retrieval task\naccuracy but also improves retrieval performance. Combining these techniques,\nour NV-Embed model, using only publicly available data, has achieved a\nrecord-high score of 69.32, ranking No. 1 on the Massive Text Embedding\nBenchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,\nreranking, classification, clustering, and semantic textual similarity tasks.\nNotably, our model also attains the highest score of 59.36 on 15 retrieval\ntasks in the MTEB benchmark (also known as BEIR). We will open-source the model\nat: https://huggingface.co/nvidia/NV-Embed-v1.", "paper_summary_zh": "<paragraph>\u50c5\u89e3\u78bc\u5668\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u70ba\u57fa\u790e\u7684\u5d4c\u5165\u6a21\u578b\u6b63\u958b\u59cb\n\u5728\u4e00\u822c\u7528\u9014\u6587\u5b57\u5d4c\u5165\u4efb\u52d9\u4e2d\u8d85\u8d8a BERT \u6216 T5 \u70ba\u57fa\u790e\u7684\u5d4c\u5165\u6a21\u578b\uff0c\u5305\u62ec\u5bc6\u96c6\u5411\u91cf\u70ba\u57fa\u790e\u7684\u6aa2\u7d22\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\n\u5f15\u5165\u4e86 NV-Embed \u6a21\u578b\uff0c\u5176\u5177\u5099\u5404\u7a2e\u67b6\u69cb\u8a2d\u8a08\u548c\n\u8a13\u7df4\u7a0b\u5e8f\uff0c\u4ee5\u986f\u8457\u63d0\u5347 LLM \u7684\u6548\u80fd\uff0c\u4f7f\u5176\u6210\u70ba\u4e00\u500b\u591a\u529f\u80fd\u7684\u5d4c\u5165\u6a21\u578b\uff0c\u540c\u6642\u7dad\u6301\u5176\u7c21\u6f54\u6027\u548c\n\u53ef\u8907\u88fd\u6027\u3002\u5c0d\u65bc\u6a21\u578b\u67b6\u69cb\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u6f5b\u5728\u6ce8\u610f\u529b\u5c64\uff0c\u4ee5\u53d6\u5f97\u532f\u96c6\u5d4c\u5165\uff0c\u8207\u5e73\u5747\u532f\u96c6\u6216\u4f7f\u7528 LLM \u7684\u6700\u5f8c\u4e00\u500b <EOS> \u4ee3\u865f\u5d4c\u5165\u76f8\u6bd4\uff0c\u9019\u6301\u7e8c\u6539\u5584\u6aa2\u7d22\u548c\u4e0b\u6e38\n\u4efb\u52d9\u7684\u6e96\u78ba\u5ea6\u3002\u70ba\u4e86\u52a0\u5f37\u8868\u5fb5\u5b78\u7fd2\uff0c\u6211\u5011\u5728\u5c0d\u6bd4\u8a13\u7df4\u671f\u9593\u79fb\u9664\u4e86 LLM \u7684\u56e0\u679c\u6ce8\u610f\u529b\u906e\u7f69\u3002\u5c0d\u65bc\u6a21\u578b\u8a13\u7df4\uff0c\u6211\u5011\u5f15\u5165\u4e86\n\u4e00\u500b\u5169\u968e\u6bb5\u5c0d\u6bd4\u6307\u4ee4\u5fae\u8abf\u65b9\u6cd5\u3002\u5b83\u9996\u5148\u5c0d\u6aa2\u7d22\u8cc7\u6599\u96c6\u4e2d\u7684\u6307\u4ee4\u5957\u7528\u5c0d\u6bd4\u8a13\u7df4\uff0c\u5229\u7528\u6279\u6b21\u5167\u8ca0\u4f8b\u548c\u7b56\u5283\u7684\u56f0\u96e3\u8ca0\u4f8b\u3002\u5728\u7b2c 2 \u968e\u6bb5\uff0c\u5b83\u5c07\u5404\u7a2e\u975e\u6aa2\u7d22\n\u8cc7\u6599\u96c6\u6df7\u5408\u5230\u6307\u4ee4\u5fae\u8abf\u4e2d\uff0c\u9019\u4e0d\u50c5\u52a0\u5f37\u4e86\u975e\u6aa2\u7d22\u4efb\u52d9\u7684\u6e96\u78ba\u5ea6\uff0c\u4e5f\u6539\u5584\u4e86\u6aa2\u7d22\u6548\u80fd\u3002\u7d50\u5408\u9019\u4e9b\u6280\u8853\uff0c\u6211\u5011\u7684 NV-Embed \u6a21\u578b\u50c5\u4f7f\u7528\u516c\u958b\u53ef\u53d6\u5f97\u7684\u8cc7\u6599\uff0c\u5728 Massive Text Embedding Benchmark (MTEB)\uff08\u622a\u81f3 2024 \u5e74 5 \u6708 24 \u65e5\uff09\u4e2d\u7372\u5f97\u4e86 69.32 \u7684\u7834\u7d00\u9304\u9ad8\u5206\uff0c\u6392\u540d\u7b2c 1\uff0c\u6db5\u84cb\u4e86 56 \u500b\u4efb\u52d9\uff0c\u5305\u542b\u6aa2\u7d22\u3001\u91cd\u65b0\u6392\u540d\u3001\u5206\u985e\u3001\u5206\u7fa4\u548c\u8a9e\u7fa9\u6587\u672c\u76f8\u4f3c\u5ea6\u4efb\u52d9\u3002\n\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u6a21\u578b\u4e5f\u5728 MTEB \u57fa\u6e96\u6e2c\u8a66\uff08\u4e5f\u7a31\u70ba BEIR\uff09\u4e2d\u7684 15 \u500b\u6aa2\u7d22\u4efb\u52d9\u4e2d\u7372\u5f97\u4e86 59.36 \u7684\u6700\u9ad8\u5206\u3002\u6211\u5011\u5c07\u5728\u4ee5\u4e0b\u7db2\u5740\u958b\u653e\u6a21\u578b\u539f\u59cb\u78bc\uff1ahttps://huggingface.co/nvidia/NV-Embed-v1\u3002</paragraph>", "author": "Chankyu Lee et.al.", "authors": "Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping", "id": "2405.17428v1", "paper_url": "http://arxiv.org/abs/2405.17428v1", "repo": "null"}}