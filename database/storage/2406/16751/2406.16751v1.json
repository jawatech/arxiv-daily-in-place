{"2406.16751": {"publish_time": "2024-06-24", "title": "Towards Zero-Shot Text-To-Speech for Arabic Dialects", "paper_summary": "Zero-shot multi-speaker text-to-speech (ZS-TTS) systems have advanced for\nEnglish, however, it still lags behind due to insufficient resources. We\naddress this gap for Arabic, a language of more than 450 million native\nspeakers, by first adapting a sizeable existing dataset to suit the needs of\nspeech synthesis. Additionally, we employ a set of Arabic dialect\nidentification models to explore the impact of pre-defined dialect labels on\nimproving the ZS-TTS model in a multi-dialect setting. Subsequently, we\nfine-tune the\nXTTS\\footnote{https://docs.coqui.ai/en/latest/models/xtts.html}\\footnote{https://medium.com/machine-learns/xtts-v2-new-version-of-the-open-source-text-to-speech-model-af73914db81f}\\footnote{https://medium.com/@erogol/xtts-v1-techincal-notes-eb83ff05bdc}\nmodel, an open-source architecture. We then evaluate our models on a dataset\ncomprising 31 unseen speakers and an in-house dialectal dataset. Our automated\nand human evaluation results show convincing performance while capable of\ngenerating dialectal speech. Our study highlights significant potential for\nimprovements in this emerging area of research in Arabic.", "paper_summary_zh": "\u96f6\u7bc4\u4f8b\u591a\u8aaa\u8a71\u8005\u6587\u5b57\u8f49\u8a9e\u97f3 (ZS-TTS) \u7cfb\u7d71\u5df2\u91dd\u5c0d\u82f1\u8a9e\u9032\u6b65\uff0c\u4f46\u7531\u65bc\u8cc7\u6e90\u4e0d\u8db3\uff0c\u5b83\u4ecd\u843d\u5f8c\u65bc\u82f1\u8a9e\u3002\u6211\u5011\u900f\u904e\u9996\u5148\u8abf\u6574\u4e00\u500b\u9f90\u5927\u7684\u73fe\u6709\u8cc7\u6599\u96c6\u4f86\u6eff\u8db3\u8a9e\u97f3\u5408\u6210\u7684\u9700\u6c42\uff0c\u4f86\u89e3\u6c7a\u963f\u62c9\u4f2f\u8a9e\u7684\u9019\u500b\u5dee\u8ddd\uff0c\u963f\u62c9\u4f2f\u8a9e\u662f\u8d85\u904e 4.5 \u5104\u6bcd\u8a9e\u4eba\u58eb\u7684\u8a9e\u8a00\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a1\u7528\u4e00\u7d44\u963f\u62c9\u4f2f\u8a9e\u65b9\u8a00\u8b58\u5225\u6a21\u578b\u4f86\u63a2\u7d22\u9810\u5148\u5b9a\u7fa9\u7684\u65b9\u8a00\u6a19\u7c64\u5c0d\u6539\u5584\u591a\u65b9\u8a00\u8a2d\u5b9a\u4e2d\u7684 ZS-TTS \u6a21\u578b\u7684\u5f71\u97ff\u3002\u96a8\u5f8c\uff0c\u6211\u5011\u5fae\u8abf XTTS\\footnote{https://docs.coqui.ai/en/latest/models/xtts.html}\\footnote{https://medium.com/machine-learns/xtts-v2-new-version-of-the-open-source-text-to-speech-model-af73914db81f}\\footnote{https://medium.com/@erogol/xtts-v1-techincal-notes-eb83ff05bdc} \u6a21\u578b\uff0c\u9019\u662f\u4e00\u500b\u958b\u6e90\u67b6\u69cb\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5728\u4e00\u500b\u5305\u542b 31 \u4f4d\u672a\u898b\u8aaa\u8a71\u8005\u548c\u4e00\u500b\u5167\u90e8\u65b9\u8a00\u8cc7\u6599\u96c6\u7684\u8cc7\u6599\u96c6\u4e0a\u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\u3002\u6211\u5011\u7684\u81ea\u52d5\u5316\u548c\u4eba\u5de5\u8a55\u4f30\u7d50\u679c\u986f\u793a\u4ee4\u4eba\u4fe1\u670d\u7684\u6548\u80fd\uff0c\u540c\u6642\u80fd\u5920\u7522\u751f\u65b9\u8a00\u8a9e\u97f3\u3002\u6211\u5011\u7684\u7814\u7a76\u7a81\u986f\u4e86\u963f\u62c9\u4f2f\u8a9e\u9019\u500b\u65b0\u8208\u7814\u7a76\u9818\u57df\u7684\u986f\u8457\u6539\u5584\u6f5b\u529b\u3002", "author": "Khai Duy Doan et.al.", "authors": "Khai Duy Doan, Abdul Waheed, Muhammad Abdul-Mageed", "id": "2406.16751v1", "paper_url": "http://arxiv.org/abs/2406.16751v1", "repo": "null"}}