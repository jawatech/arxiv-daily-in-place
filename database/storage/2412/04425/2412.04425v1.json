{"2412.04425": {"publish_time": "2024-12-05", "title": "CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing", "paper_summary": "We introduce Condition-Aware Self-Supervised Learning Representation\n(CA-SSLR), a generalist conditioning model broadly applicable to various\nspeech-processing tasks. Compared to standard fine-tuning methods that optimize\nfor downstream models, CA-SSLR integrates language and speaker embeddings from\nearlier layers, making the SSL model aware of the current language and speaker\ncontext. This approach reduces the reliance on input audio features while\npreserving the integrity of the base SSLR. CA-SSLR improves the model's\ncapabilities and demonstrates its generality on unseen tasks with minimal\ntask-specific tuning. Our method employs linear modulation to dynamically\nadjust internal representations, enabling fine-grained adaptability without\nsignificantly altering the original model behavior. Experiments show that\nCA-SSLR reduces the number of trainable parameters, mitigates overfitting, and\nexcels in under-resourced and unseen tasks. Specifically, CA-SSLR achieves a\n10% relative reduction in LID errors, a 37% improvement in ASR CER on the\nML-SUPERB benchmark, and a 27% decrease in SV EER on VoxCeleb-1, demonstrating\nits effectiveness.", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86\u689d\u4ef6\u611f\u77e5\u81ea\u6211\u76e3\u7763\u5b78\u7fd2\u8868\u793a (CA-SSLR)\uff0c\u9019\u662f\u4e00\u500b\u901a\u624d\u689d\u4ef6\u6a21\u578b\uff0c\u5ee3\u6cdb\u9069\u7528\u65bc\u5404\u7a2e\u8a9e\u97f3\u8655\u7406\u4efb\u52d9\u3002\u8207\u91dd\u5c0d\u4e0b\u6e38\u6a21\u578b\u9032\u884c\u512a\u5316\u7684\u6a19\u6e96\u5fae\u8abf\u65b9\u6cd5\u76f8\u6bd4\uff0cCA-SSLR \u6574\u5408\u4e86\u4f86\u81ea\u65e9\u671f\u5c64\u7684\u8a9e\u8a00\u548c\u8aaa\u8a71\u8005\u5d4c\u5165\uff0c\u4f7f SSL \u6a21\u578b\u611f\u77e5\u7576\u524d\u7684\u8a9e\u8a00\u548c\u8aaa\u8a71\u8005\u80cc\u666f\u3002\u6b64\u65b9\u6cd5\u6e1b\u5c11\u4e86\u5c0d\u8f38\u5165\u97f3\u8a0a\u7279\u5fb5\u7684\u4f9d\u8cf4\uff0c\u540c\u6642\u4fdd\u7559\u4e86\u57fa\u790e SSLR \u7684\u5b8c\u6574\u6027\u3002CA-SSLR \u6539\u5584\u4e86\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4e26\u5728\u672a\u898b\u4efb\u52d9\u4e0a\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\uff0c\u4e14\u4efb\u52d9\u7279\u5b9a\u8abf\u6574\u6700\u5c11\u3002\u6211\u5011\u7684\u6a21\u578b\u63a1\u7528\u7dda\u6027\u8abf\u88fd\u4f86\u52d5\u614b\u8abf\u6574\u5167\u90e8\u8868\u793a\uff0c\u5be6\u73fe\u7d30\u7c92\u5ea6\u7684\u9069\u61c9\u6027\uff0c\u800c\u4e0d\u6703\u986f\u8457\u6539\u8b8a\u539f\u59cb\u6a21\u578b\u884c\u70ba\u3002\u5be6\u9a57\u8868\u660e\uff0cCA-SSLR \u6e1b\u5c11\u4e86\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u6578\u91cf\uff0c\u6e1b\u8f15\u4e86\u904e\u5ea6\u64ec\u5408\uff0c\u4e26\u5728\u8cc7\u6e90\u4e0d\u8db3\u548c\u672a\u898b\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cCA-SSLR \u5728 LID \u932f\u8aa4\u4e2d\u5be6\u73fe\u4e86 10% \u7684\u76f8\u5c0d\u6e1b\u5c11\uff0c\u5728 ML-SUPERB \u57fa\u6e96\u4e0a ASR CER \u63d0\u9ad8\u4e86 37%\uff0c\u5728 VoxCeleb-1 \u4e0a SV EER \u6e1b\u5c11\u4e86 27%\uff0c\u8b49\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "author": "Yen-Ju Lu et.al.", "authors": "Yen-Ju Lu, Jing Liu, Thomas Thebaud, Laureano Moro-Velazquez, Ariya Rastrow, Najim Dehak, Jesus Villalba", "id": "2412.04425v1", "paper_url": "http://arxiv.org/abs/2412.04425v1", "repo": "null"}}