{"2409.15869": {"publish_time": "2024-09-24", "title": "Whisper in Medusa's Ear: Multi-head Efficient Decoding for Transformer-based ASR", "paper_summary": "Large transformer-based models have significant potential for speech\ntranscription and translation. Their self-attention mechanisms and parallel\nprocessing enable them to capture complex patterns and dependencies in audio\nsequences. However, this potential comes with challenges, as these large and\ncomputationally intensive models lead to slow inference speeds. Various\noptimization strategies have been proposed to improve performance, including\nefficient hardware utilization and algorithmic enhancements. In this paper, we\nintroduce Whisper-Medusa, a novel approach designed to enhance processing speed\nwith minimal impact on Word Error Rate (WER). The proposed model extends the\nOpenAI's Whisper architecture by predicting multiple tokens per iteration,\nresulting in a 50% reduction in latency. We showcase the effectiveness of\nWhisper-Medusa across different learning setups and datasets.", "paper_summary_zh": "\u5927\u578bTransformer\u6a21\u578b\u5728\u8a9e\u97f3\u8f49\u9304\u548c\u7ffb\u8b6f\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5b\u529b\u3002\u5b83\u5011\u7684\u81ea\u6211\u6ce8\u610f\u6a5f\u5236\u548c\u4e26\u884c\u8655\u7406\u4f7f\u5b83\u5011\u80fd\u5920\u6355\u6349\u97f3\u8a0a\u5e8f\u5217\u4e2d\u7684\u8907\u96dc\u6a21\u5f0f\u548c\u4f9d\u8cf4\u95dc\u4fc2\u3002\u7136\u800c\uff0c\u9019\u7a2e\u6f5b\u529b\u4e5f\u5e36\u4f86\u6311\u6230\uff0c\u56e0\u70ba\u9019\u4e9b\u5927\u578b\u4e14\u8a08\u7b97\u5bc6\u96c6\u7684\u6a21\u578b\u6703\u5c0e\u81f4\u7de9\u6162\u7684\u63a8\u7406\u901f\u5ea6\u3002\u5df2\u7d93\u63d0\u51fa\u4e86\u5404\u7a2e\u6700\u4f73\u5316\u7b56\u7565\u4f86\u6539\u5584\u6548\u80fd\uff0c\u5305\u62ec\u6709\u6548\u7387\u7684\u786c\u9ad4\u5229\u7528\u548c\u6f14\u7b97\u6cd5\u5f37\u5316\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 Whisper-Medusa\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u8655\u7406\u901f\u5ea6\uff0c\u540c\u6642\u5c0d\u5b57\u5143\u932f\u8aa4\u7387 (WER) \u7684\u5f71\u97ff\u6700\u5c0f\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u900f\u904e\u6bcf\u6b21\u758a\u4ee3\u9810\u6e2c\u591a\u500b\u4ee3\u78bc\uff0c\u64f4\u5145\u4e86 OpenAI \u7684 Whisper \u67b6\u69cb\uff0c\u5f9e\u800c\u5c07\u5ef6\u9072\u6e1b\u5c11\u4e86 50%\u3002\u6211\u5011\u5c55\u793a\u4e86 Whisper-Medusa \u5728\u4e0d\u540c\u5b78\u7fd2\u8a2d\u5b9a\u548c\u8cc7\u6599\u96c6\u4e2d\u7684\u6709\u6548\u6027\u3002", "author": "Yael Segal-Feldman et.al.", "authors": "Yael Segal-Feldman, Aviv Shamsian, Aviv Navon, Gill Hetz, Joseph Keshet", "id": "2409.15869v1", "paper_url": "http://arxiv.org/abs/2409.15869v1", "repo": "null"}}