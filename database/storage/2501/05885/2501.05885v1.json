{"2501.05885": {"publish_time": "2025-01-10", "title": "EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster Context Attention, Better Feature Fusion, and Hardware Acceleration", "paper_summary": "Detecting small targets in drone imagery is challenging due to low\nresolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel\nedge-target detection framework built on an enhanced YOLOv10 architecture,\noptimized for real-time applications without post-processing. EDNet\nincorporates an XSmall detection head and a Cross Concat strategy to improve\nfeature fusion and multi-scale context awareness for detecting tiny targets in\ndiverse environments. Our unique C2f-FCA block employs Faster Context Attention\nto enhance feature extraction while reducing computational complexity. The WIoU\nloss function is employed for improved bounding box regression. With seven\nmodel sizes ranging from Tiny to XL, EDNet accommodates various deployment\nenvironments, enabling local real-time inference and ensuring data privacy.\nNotably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer\nparameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16\nto 55 FPS, providing a scalable and efficient solution for edge-based object\ndetection in challenging drone imagery. The source code and pre-trained models\nare available at: https://github.com/zsniko/EDNet.", "paper_summary_zh": "\u7531\u65bc\u89e3\u6790\u5ea6\u4f4e\u3001\u80cc\u666f\u8907\u96dc\u4e14\u5834\u666f\u52d5\u614b\uff0c\u5728\u7121\u4eba\u6a5f\u5f71\u50cf\u4e2d\u5075\u6e2c\u5c0f\u578b\u76ee\u6a19\u662f\u4e00\u9805\u6311\u6230\u3002\u6211\u5011\u63d0\u51fa EDNet\uff0c\u4e00\u500b\u5efa\u69cb\u5728\u589e\u5f37\u7684 YOLOv10 \u67b6\u69cb\u4e0a\u7684\u65b0\u578b\u908a\u7de3\u76ee\u6a19\u5075\u6e2c\u67b6\u69cb\uff0c\u91dd\u5c0d\u5373\u6642\u61c9\u7528\u9032\u884c\u6700\u4f73\u5316\uff0c\u7121\u9700\u5f8c\u8655\u7406\u3002EDNet \u7d50\u5408\u4e00\u500b XSmall \u5075\u6e2c\u982d\u548c\u4e00\u500b Cross Concat \u7b56\u7565\uff0c\u4ee5\u6539\u5584\u7279\u5fb5\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u80cc\u666f\u611f\u77e5\uff0c\u7528\u65bc\u5728\u5404\u7a2e\u74b0\u5883\u4e2d\u5075\u6e2c\u5fae\u5c0f\u76ee\u6a19\u3002\u6211\u5011\u7368\u7279\u7684 C2f-FCA \u5340\u584a\u63a1\u7528 Faster Context Attention \u4f86\u589e\u5f37\u7279\u5fb5\u8403\u53d6\uff0c\u540c\u6642\u964d\u4f4e\u904b\u7b97\u8907\u96dc\u5ea6\u3002WIoU \u640d\u5931\u51fd\u6578\u7528\u65bc\u6539\u5584\u908a\u754c\u6846\u56de\u6b78\u3002EDNet \u6709\u4e03\u7a2e\u578b\u865f\u5c3a\u5bf8\uff0c\u5f9e Tiny \u5230 XL\uff0c\u53ef\u9069\u61c9\u5404\u7a2e\u90e8\u7f72\u74b0\u5883\uff0c\u652f\u63f4\u672c\u5730\u5373\u6642\u63a8\u8ad6\u4e26\u78ba\u4fdd\u8cc7\u6599\u96b1\u79c1\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cEDNet \u5728 mAP@50 \u4e2d\u7372\u5f97\u9ad8\u9054 5.6% \u7684\u589e\u76ca\uff0c\u800c\u53c3\u6578\u537b\u660e\u986f\u6e1b\u5c11\u3002\u5728 iPhone 12 \u4e0a\uff0cEDNet \u8b8a\u9ad4\u904b\u4f5c\u901f\u5ea6\u7bc4\u570d\u5f9e 16 \u5230 55 FPS\uff0c\u70ba\u5177\u6311\u6230\u6027\u7684\u7121\u4eba\u6a5f\u5f71\u50cf\u4e2d\u7684\u908a\u7de3\u7269\u4ef6\u5075\u6e2c\u63d0\u4f9b\u53ef\u64f4\u5145\u4e14\u9ad8\u6548\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u539f\u59cb\u7a0b\u5f0f\u78bc\u548c\u9810\u5148\u8a13\u7df4\u7684\u6a21\u578b\u53ef\u5728 https://github.com/zsniko/EDNet \u53d6\u5f97\u3002", "author": "Zhifan Song et.al.", "authors": "Zhifan Song, Yuan Zhang, Abd Al Rahman M. Abu Ebayyeh", "id": "2501.05885v1", "paper_url": "http://arxiv.org/abs/2501.05885v1", "repo": "null"}}