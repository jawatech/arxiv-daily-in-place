{"2410.03524": {"publish_time": "2024-10-04", "title": "Steering Large Language Models between Code Execution and Textual Reasoning", "paper_summary": "While a lot of recent research focuses on enhancing the textual reasoning\ncapabilities of Large Language Models (LLMs) by optimizing the multi-agent\nframework or reasoning chains, several benchmark tasks can be solved with 100%\nsuccess through direct coding, which is more scalable and avoids the\ncomputational overhead associated with textual iterating and searching. Textual\nreasoning has inherent limitations in solving tasks with challenges in math,\nlogics, optimization, and searching, which is unlikely to be solved by simply\nscaling up the model and data size. The recently released OpenAI GPT Code\nInterpreter and multi-agent frameworks such as AutoGen have demonstrated\nremarkable proficiency of integrating code generation and execution to solve\ncomplex tasks using LLMs. However, based on our experiments on 7 existing\npopular methods for steering code/text generation in both single- and\nmulti-turn settings with 14 tasks and 6 types of LLMs (including the new\nO1-preview), currently there is no optimal method to correctly steer LLMs to\nwrite code when needed. We discover some interesting patterns on when models\nuse code vs. textual reasoning with the evolution to task complexity and model\nsizes, which even result in an astonishingly inverse scaling law. We also\ndiscover that results from LLM written code are not always better than using\ntextual reasoning, even if the task could be solved through code. To mitigate\nthe above issues, we propose three methods to better steer LLM code/text\ngeneration and achieve a notable improvement. The costs of token lengths and\nruntime are thoroughly discussed for all the methods. We believe the problem of\nsteering LLM code/text generation is critical for future research and has much\nspace for further improvement. Project Page, Datasets, and Codes are available\nat https://yongchao98.github.io/CodeSteer/.", "paper_summary_zh": "<paragraph>\u5118\u7ba1\u8a31\u591a\u8fd1\u671f\u7814\u7a76\u5c08\u6ce8\u65bc\u900f\u904e\u6700\u4f73\u5316\u591a\u91cd\u4ee3\u7406\u67b6\u69cb\u6216\u63a8\u7406\u93c8\u4f86\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6587\u5b57\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5e7e\u500b\u57fa\u6e96\u4efb\u52d9\u53ef\u900f\u904e\u76f4\u63a5\u7de8\u78bc\u89e3\u6c7a\uff0c\u4e26\u7372\u5f97 100% \u7684\u6210\u529f\u7387\uff0c\u9019\u66f4\u5177\u53ef\u64f4\u5145\u6027\uff0c\u4e26\u907f\u514d\u8207\u6587\u5b57\u53cd\u8986\u904b\u7b97\u53ca\u641c\u5c0b\u76f8\u95dc\u7684\u904b\u7b97\u958b\u92b7\u3002\u6587\u5b57\u63a8\u7406\u5728\u89e3\u6c7a\u6578\u5b78\u3001\u908f\u8f2f\u3001\u6700\u4f73\u5316\u548c\u641c\u5c0b\u7b49\u6311\u6230\u4efb\u52d9\u6642\u6709\u5176\u56fa\u6709\u9650\u5236\uff0c\u9019\u4e0d\u592a\u53ef\u80fd\u900f\u904e\u55ae\u7d14\u64f4\u5145\u6a21\u578b\u548c\u8cc7\u6599\u5927\u5c0f\u4f86\u89e3\u6c7a\u3002\u6700\u8fd1\u767c\u5e03\u7684 OpenAI GPT \u7a0b\u5f0f\u78bc\u89e3\u8b6f\u5668\u548c AutoGen \u7b49\u591a\u91cd\u4ee3\u7406\u67b6\u69cb\u5df2\u5c55\u793a\u51fa\u6574\u5408\u7a0b\u5f0f\u78bc\u7522\u751f\u548c\u57f7\u884c\u4ee5\u4f7f\u7528 LLM \u89e3\u6c7a\u8907\u96dc\u4efb\u52d9\u7684\u986f\u8457\u80fd\u529b\u3002\u7136\u800c\uff0c\u6839\u64da\u6211\u5011\u5c0d 7 \u7a2e\u73fe\u6709\u6d41\u884c\u65b9\u6cd5\u7684\u5be6\u9a57\uff0c\u9019\u4e9b\u65b9\u6cd5\u7528\u65bc\u5728\u55ae\u56de\u5408\u548c\u591a\u56de\u5408\u8a2d\u5b9a\u4e2d\u5f15\u5c0e\u7a0b\u5f0f\u78bc/\u6587\u5b57\u7522\u751f\uff0c\u4e26\u4f7f\u7528 14 \u9805\u4efb\u52d9\u548c 6 \u7a2e\u985e\u578b\u7684 LLM\uff08\u5305\u62ec\u65b0\u7684 O1-preview\uff09\uff0c\u76ee\u524d\u6c92\u6709\u6700\u4f73\u65b9\u6cd5\u53ef\u4ee5\u6b63\u78ba\u5f15\u5c0e LLM \u5728\u9700\u8981\u6642\u64b0\u5beb\u7a0b\u5f0f\u78bc\u3002\u6211\u5011\u767c\u73fe\u4e86\u4e00\u4e9b\u6709\u8da3\u7684\u6a21\u5f0f\uff0c\u8aaa\u660e\u6a21\u578b\u4f55\u6642\u4f7f\u7528\u7a0b\u5f0f\u78bc\u76f8\u5c0d\u65bc\u6587\u5b57\u63a8\u7406\uff0c\u4ee5\u53ca\u4efb\u52d9\u8907\u96dc\u5ea6\u548c\u6a21\u578b\u5927\u5c0f\u7684\u6f14\u8b8a\uff0c\u9019\u751a\u81f3\u5c0e\u81f4\u4e86\u4e00\u500b\u9a5a\u4eba\u7684\u53cd\u5411\u64f4\u5145\u6cd5\u5247\u3002\u6211\u5011\u9084\u767c\u73fe\uff0c\u5373\u4f7f\u4efb\u52d9\u53ef\u4ee5\u900f\u904e\u7a0b\u5f0f\u78bc\u89e3\u6c7a\uff0cLLM \u7de8\u5beb\u7a0b\u5f0f\u78bc\u7684\u7d50\u679c\u4e26\u4e0d\u7e3d\u662f\u512a\u65bc\u4f7f\u7528\u6587\u5b57\u63a8\u7406\u3002\u70ba\u4e86\u6e1b\u8f15\u4e0a\u8ff0\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e09\u7a2e\u65b9\u6cd5\u4f86\u66f4\u597d\u5730\u5f15\u5c0e LLM \u7a0b\u5f0f\u78bc/\u6587\u5b57\u7522\u751f\uff0c\u4e26\u7372\u5f97\u986f\u8457\u7684\u6539\u9032\u3002\u6240\u6709\u65b9\u6cd5\u7684\u6b0a\u6756\u9577\u5ea6\u548c\u57f7\u884c\u6642\u9593\u6210\u672c\u90fd\u7d93\u904e\u5fb9\u5e95\u8a0e\u8ad6\u3002\u6211\u5011\u76f8\u4fe1\u5f15\u5c0e LLM \u7a0b\u5f0f\u78bc/\u6587\u5b57\u7522\u751f\u7684\u554f\u984c\u5c0d\u65bc\u672a\u4f86\u7684\u7814\u7a76\u81f3\u95dc\u91cd\u8981\uff0c\u4e26\u4e14\u6709\u5f88\u5927\u7684\u6539\u9032\u7a7a\u9593\u3002\u5c08\u6848\u9801\u9762\u3001\u8cc7\u6599\u96c6\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://yongchao98.github.io/CodeSteer/ \u53d6\u5f97\u3002</paragraph>", "author": "Yongchao Chen et.al.", "authors": "Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, Chi Wang", "id": "2410.03524v1", "paper_url": "http://arxiv.org/abs/2410.03524v1", "repo": "null"}}