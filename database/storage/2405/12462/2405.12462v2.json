{"2405.12462": {"publish_time": "2024-05-21", "title": "Boosting X-formers with Structured Matrix for Long Sequence Time Series Forecasting", "paper_summary": "Transformer-based models for long sequence time series forecasting (LSTF)\nproblems have gained significant attention due to their exceptional forecasting\nprecision. As the cornerstone of these models, the self-attention mechanism\nposes a challenge to efficient training and inference due to its quadratic time\ncomplexity. In this article, we propose a novel architectural design for\nTransformer-based models in LSTF, leveraging a substitution framework that\nincorporates Surrogate Attention Blocks and Surrogate FFN Blocks. The framework\naims to boost any well-designed model's efficiency without sacrificing its\naccuracy. We further establish the equivalence of the Surrogate Attention Block\nto the self-attention mechanism in terms of both expressiveness and\ntrainability. Through extensive experiments encompassing nine Transformer-based\nmodels across five time series tasks, we observe an average performance\nimprovement of 9.45% while achieving a significant reduction in model size by\n46%", "paper_summary_zh": "\u57fa\u65bc Transformer \u7684\u9577\u5e8f\u5217\u6642\u9593\u5e8f\u5217\u9810\u6e2c (LSTF) \u554f\u984c\u6a21\u578b\u7531\u65bc\u5176\u5353\u8d8a\u7684\u9810\u6e2c\u7cbe\u6e96\u5ea6\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u4f5c\u70ba\u9019\u4e9b\u6a21\u578b\u7684\u57fa\u77f3\uff0c\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\u7531\u65bc\u5176\u4e8c\u6b21\u6642\u9593\u8907\u96dc\u5ea6\u800c\u5c0d\u9ad8\u6548\u8a13\u7df4\u548c\u63a8\u8ad6\u69cb\u6210\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u70ba LSTF \u4e2d\u57fa\u65bc Transformer \u7684\u6a21\u578b\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u67b6\u69cb\u8a2d\u8a08\uff0c\u5229\u7528\u4e86\u4e00\u500b\u5305\u542b\u4ee3\u7406\u6ce8\u610f\u529b\u5340\u584a\u548c\u4ee3\u7406 FFN \u5340\u584a\u7684\u66ff\u63db\u6846\u67b6\u3002\u8a72\u6846\u67b6\u65e8\u5728\u63d0\u5347\u4efb\u4f55\u7cbe\u5fc3\u8a2d\u8a08\u7684\u6a21\u578b\u7684\u6548\u7387\uff0c\u800c\u4e0d\u6703\u72a7\u7272\u5176\u6e96\u78ba\u6027\u3002\u6211\u5011\u9032\u4e00\u6b65\u5efa\u7acb\u4e86\u4ee3\u7406\u6ce8\u610f\u529b\u5340\u584a\u8207\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\u5728\u8868\u9054\u529b\u548c\u53ef\u8a13\u7df4\u6027\u65b9\u9762\u7684\u7b49\u50f9\u6027\u3002\u901a\u904e\u6db5\u84cb\u4e5d\u500b\u57fa\u65bc Transformer \u7684\u6a21\u578b\u7684\u5ee3\u6cdb\u5be6\u9a57\uff0c\u8de8\u8d8a\u4e94\u500b\u6642\u9593\u5e8f\u5217\u4efb\u52d9\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e86 9.45%\uff0c\u540c\u6642\u5c07\u6a21\u578b\u5927\u5c0f\u986f\u8457\u6e1b\u5c11\u4e86 46%", "author": "Zhicheng Zhang et.al.", "authors": "Zhicheng Zhang, Yong Wang, Shaoqi Tan, Bowei Xia, Yujie Luo", "id": "2405.12462v2", "paper_url": "http://arxiv.org/abs/2405.12462v2", "repo": "null"}}