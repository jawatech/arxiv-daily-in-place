{"2406.07151": {"publish_time": "2024-06-11", "title": "EEG-ImageNet: An Electroencephalogram Dataset and Benchmarks with Image Visual Stimuli of Multi-Granularity Labels", "paper_summary": "Identifying and reconstructing what we see from brain activity gives us a\nspecial insight into investigating how the biological visual system represents\nthe world. While recent efforts have achieved high-performance image\nclassification and high-quality image reconstruction from brain signals\ncollected by Functional Magnetic Resonance Imaging (fMRI) or\nmagnetoencephalogram (MEG), the expensiveness and bulkiness of these devices\nmake relevant applications difficult to generalize to practical applications.\nOn the other hand, Electroencephalography (EEG), despite its advantages of ease\nof use, cost-efficiency, high temporal resolution, and non-invasive nature, has\nnot been fully explored in relevant studies due to the lack of comprehensive\ndatasets. To address this gap, we introduce EEG-ImageNet, a novel EEG dataset\ncomprising recordings from 16 subjects exposed to 4000 images selected from the\nImageNet dataset. EEG-ImageNet consists of 5 times EEG-image pairs larger than\nexisting similar EEG benchmarks. EEG-ImageNet is collected with image stimuli\nof multi-granularity labels, i.e., 40 images with coarse-grained labels and 40\nwith fine-grained labels. Based on it, we establish benchmarks for object\nclassification and image reconstruction. Experiments with several commonly used\nmodels show that the best models can achieve object classification with\naccuracy around 60% and image reconstruction with two-way identification around\n64%. These results demonstrate the dataset's potential to advance EEG-based\nvisual brain-computer interfaces, understand the visual perception of\nbiological systems, and provide potential applications in improving machine\nvisual models.", "paper_summary_zh": "<paragraph>\u900f\u904e\u8fa8\u8b58\u548c\u91cd\u5efa\u6211\u5011\u5f9e\u8166\u90e8\u6d3b\u52d5\u6240\u770b\u5230\u7684\uff0c\u8b93\u6211\u5011\u5f97\u4ee5\u7279\u5225\u6df1\u5165\u63a2\u7a76\u751f\u7269\u8996\u89ba\u7cfb\u7d71\u5982\u4f55\u5448\u73fe\u4e16\u754c\u3002\u96d6\u7136\u8fd1\u671f\u52aa\u529b\u5df2\u5f9e\u529f\u80fd\u6027\u78c1\u632f\u9020\u5f71 (fMRI) \u6216\u8166\u78c1\u5716 (MEG) \u6536\u96c6\u7684\u8166\u90e8\u8a0a\u865f\u4e2d\uff0c\u9054\u6210\u9ad8\u6027\u80fd\u5f71\u50cf\u5206\u985e\u548c\u9ad8\u54c1\u8cea\u5f71\u50cf\u91cd\u5efa\uff0c\u4f46\u9019\u4e9b\u88dd\u7f6e\u7684\u6602\u8cb4\u548c\u7b28\u91cd\uff0c\u4f7f\u5f97\u76f8\u95dc\u61c9\u7528\u96e3\u4ee5\u63a8\u5ee3\u5230\u5be6\u969b\u61c9\u7528\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u8166\u6ce2\u5716 (EEG) \u96d6\u7136\u5177\u6709\u4f7f\u7528\u5bb9\u6613\u3001\u6210\u672c\u6548\u76ca\u9ad8\u3001\u6642\u9593\u89e3\u6790\u5ea6\u9ad8\u548c\u975e\u4fb5\u5165\u6027\u7b49\u512a\u9ede\uff0c\u4f46\u7531\u65bc\u7f3a\u4e4f\u5168\u9762\u7684\u8cc7\u6599\u96c6\uff0c\u5c1a\u672a\u5728\u76f8\u95dc\u7814\u7a76\u4e2d\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 EEG-ImageNet\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684 EEG \u8cc7\u6599\u96c6\uff0c\u5305\u542b\u4f86\u81ea 16 \u4f4d\u53d7\u8a66\u8005\u89c0\u770b\u5f9e ImageNet \u8cc7\u6599\u96c6\u4e2d\u6311\u9078\u7684 4000 \u5f35\u5f71\u50cf\u7684\u8a18\u9304\u3002EEG-ImageNet \u5305\u542b\u6bd4\u73fe\u6709\u985e\u4f3c EEG \u57fa\u6e96\u5927 5 \u500d\u7684 EEG \u5f71\u50cf\u5c0d\u3002EEG-ImageNet \u662f\u4f7f\u7528\u5177\u6709\u591a\u7c92\u5ea6\u6a19\u7c64\u7684\u5f71\u50cf\u523a\u6fc0\u6536\u96c6\u7684\uff0c\u5373 40 \u5f35\u5177\u6709\u7c97\u7c92\u5ea6\u6a19\u7c64\u7684\u5f71\u50cf\u548c 40 \u5f35\u5177\u6709\u7d30\u7c92\u5ea6\u6a19\u7c64\u7684\u5f71\u50cf\u3002\u57fa\u65bc\u6b64\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u7269\u4ef6\u5206\u985e\u548c\u5f71\u50cf\u91cd\u5efa\u7684\u57fa\u6e96\u3002\u4f7f\u7528\u5e7e\u500b\u5e38\u7528\u6a21\u578b\u7684\u5be6\u9a57\u986f\u793a\uff0c\u6700\u4f73\u6a21\u578b\u53ef\u4ee5\u5728\u7269\u4ef6\u5206\u985e\u4e2d\u9054\u5230\u7d04 60% \u7684\u6e96\u78ba\u5ea6\uff0c\u5728\u96d9\u5411\u8fa8\u8b58\u4e2d\u9054\u5230\u7d04 64% \u7684\u5f71\u50cf\u91cd\u5efa\u3002\u9019\u4e9b\u7d50\u679c\u8b49\u660e\u4e86\u8a72\u8cc7\u6599\u96c6\u5728\u63a8\u52d5\u57fa\u65bc EEG \u7684\u8996\u89ba\u8166\u96fb\u8166\u4ecb\u9762\u3001\u4e86\u89e3\u751f\u7269\u7cfb\u7d71\u7684\u8996\u89ba\u611f\u77e5\uff0c\u4ee5\u53ca\u63d0\u4f9b\u6539\u5584\u6a5f\u5668\u8996\u89ba\u6a21\u578b\u7684\u6f5b\u5728\u61c9\u7528\u65b9\u9762\u7684\u6f5b\u529b\u3002</paragraph>", "author": "Shuqi Zhu et.al.", "authors": "Shuqi Zhu, Ziyi Ye, Qingyao Ai, Yiqun Liu", "id": "2406.07151v1", "paper_url": "http://arxiv.org/abs/2406.07151v1", "repo": "https://github.com/promise-z5q2sq/eeg-imagenet-dataset"}}