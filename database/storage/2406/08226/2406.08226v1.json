{"2406.08226": {"publish_time": "2024-06-12", "title": "DistilDoc: Knowledge Distillation for Visually-Rich Document Applications", "paper_summary": "This work explores knowledge distillation (KD) for visually-rich document\n(VRD) applications such as document layout analysis (DLA) and document image\nclassification (DIC). While VRD research is dependent on increasingly\nsophisticated and cumbersome models, the field has neglected to study\nefficiency via model compression. Here, we design a KD experimentation\nmethodology for more lean, performant models on document understanding (DU)\ntasks that are integral within larger task pipelines. We carefully selected KD\nstrategies (response-based, feature-based) for distilling knowledge to and from\nbackbones with different architectures (ResNet, ViT, DiT) and capacities (base,\nsmall, tiny). We study what affects the teacher-student knowledge gap and find\nthat some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can\nconsistently outperform supervised student training. Furthermore, we design\ndownstream task setups to evaluate covariate shift and the robustness of\ndistilled DLA models on zero-shot layout-aware document visual question\nanswering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap,\nwhich unpredictably translates to downstream robustness, accentuating the need\nto further explore how to efficiently obtain more semantic document layout\nawareness.", "paper_summary_zh": "\u8fd9\u9879\u5de5\u4f5c\u63a2\u7d22\u4e86\u89c6\u89c9\u4e30\u5bcc\u7684\u6587\u6863 (VRD) \u5e94\u7528\uff08\u4f8b\u5982\u6587\u6863\u5e03\u5c40\u5206\u6790 (DLA) \u548c\u6587\u6863\u56fe\u50cf\u5206\u7c7b (DIC)\uff09\u7684\u77e5\u8bc6\u84b8\u998f (KD)\u3002\u867d\u7136 VRD \u7814\u7a76\u4f9d\u8d56\u4e8e\u8d8a\u6765\u8d8a\u590d\u6742\u4e14\u7e41\u7410\u7684\u6a21\u578b\uff0c\u4f46\u8be5\u9886\u57df\u5ffd\u7565\u4e86\u901a\u8fc7\u6a21\u578b\u538b\u7f29\u6765\u7814\u7a76\u6548\u7387\u3002\u5728\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd KD \u5b9e\u9a8c\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u66f4\u5927\u4efb\u52a1\u7ba1\u9053\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u6587\u6863\u7406\u89e3 (DU) \u4efb\u52a1\u4e0a\u83b7\u5f97\u66f4\u7cbe\u7b80\u3001\u6027\u80fd\u66f4\u9ad8\u7684\u6a21\u578b\u3002\u6211\u4eec\u4ed4\u7ec6\u9009\u62e9\u4e86 KD \u7b56\u7565\uff08\u57fa\u4e8e\u54cd\u5e94\u3001\u57fa\u4e8e\u7279\u5f81\uff09\uff0c\u7528\u4e8e\u5c06\u77e5\u8bc6\u4ece\u5177\u6709\u4e0d\u540c\u67b6\u6784\uff08ResNet\u3001ViT\u3001DiT\uff09\u548c\u5bb9\u91cf\uff08\u57fa\u7840\u3001\u5c0f\u578b\u3001\u5fae\u578b\uff09\u7684\u4e3b\u5e72\u7f51\u7edc\u84b8\u998f\u5230\u4e3b\u5e72\u7f51\u7edc\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u6211\u4eec\u7814\u7a76\u4e86\u5f71\u54cd\u5e08\u751f\u77e5\u8bc6\u5dee\u8ddd\u7684\u56e0\u7d20\uff0c\u53d1\u73b0\u4e00\u4e9b\u65b9\u6cd5\uff08\u8c03\u6574\u540e\u7684\u9999\u8349 KD\u3001MSE\u3001\u5e26\u6709\u9002\u5f53\u6295\u5f71\u4eea\u7684 SimKD\uff09\u53ef\u4ee5\u6301\u7eed\u4f18\u4e8e\u76d1\u7763\u5f0f\u5b66\u751f\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e0b\u6e38\u4efb\u52a1\u8bbe\u7f6e\uff0c\u4ee5\u8bc4\u4f30\u534f\u53d8\u91cf\u504f\u79fb\u548c\u84b8\u998f DLA \u6a21\u578b\u5728\u96f6\u6837\u672c\u5e03\u5c40\u611f\u77e5\u6587\u6863\u89c6\u89c9\u95ee\u9898\u89e3\u7b54 (DocVQA) \u4e2d\u7684\u9c81\u68d2\u6027\u3002DLA-KD \u5b9e\u9a8c\u5bfc\u81f4\u8f83\u5927\u7684 mAP \u77e5\u8bc6\u5dee\u8ddd\uff0c\u8be5\u5dee\u8ddd\u4e0d\u53ef\u9884\u6d4b\u5730\u8f6c\u5316\u4e3a\u4e0b\u6e38\u9c81\u68d2\u6027\uff0c\u7a81\u51fa\u4e86\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u6709\u6548\u83b7\u5f97\u66f4\u591a\u8bed\u4e49\u6587\u6863\u5e03\u5c40\u611f\u77e5\u7684\u5fc5\u8981\u6027\u3002", "author": "Jordy Van Landeghem et.al.", "authors": "Jordy Van Landeghem, Subhajit Maity, Ayan Banerjee, Matthew Blaschko, Marie-Francine Moens, Josep Llad\u00f3s, Sanket Biswas", "id": "2406.08226v1", "paper_url": "http://arxiv.org/abs/2406.08226v1", "repo": "null"}}