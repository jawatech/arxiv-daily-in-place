{"2406.05981": {"publish_time": "2024-06-10", "title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization", "paper_summary": "Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a9e\u8a00\u4efb\u52d9\u4e0a\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u7531\u65bc\u5176\u5ee3\u6cdb\u7684\u53c3\u6578\u548c\u5c0d\u5bc6\u96c6\u4e58\u6cd5\u7684\u4f9d\u8cf4\u6027\uff0c\u5728\u90e8\u7f72\u5230\u8cc7\u6e90\u53d7\u9650\u7684\u8a2d\u5099\u4e0a\u6642\u6703\u9762\u81e8\u6311\u6230\uff0c\u5c0e\u81f4\u9ad8\u8a18\u61b6\u9ad4\u9700\u6c42\u548c\u5ef6\u9072\u74f6\u9838\u3002\u79fb\u4f4d\u548c\u52a0\u6cd5\u91cd\u65b0\u53c3\u6578\u5316\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u5b83\u7528\u786c\u9ad4\u53cb\u5584\u7684\u539f\u8a9e\u53d6\u4ee3\u4e86\u6ce8\u610f\u529b\u548c\u591a\u5c64\u611f\u77e5\u5668 (MLP) \u5c64\u4e2d\u7684\u6602\u8cb4\u4e58\u6cd5\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u91cd\u65b0\u53c3\u6578\u5316\u6280\u8853\u9700\u8981\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u6216\u9032\u884c\u5b8c\u5168\u53c3\u6578\u5fae\u8abf\u624d\u80fd\u6062\u5fa9\u6e96\u78ba\u6027\uff0c\u9019\u5c0d\u65bc LLM \u4f86\u8aaa\u662f\u8cc7\u6e90\u5bc6\u96c6\u578b\u7684\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u901a\u904e\u8a13\u7df4\u5f8c\u79fb\u4f4d\u548c\u52a0\u6cd5\u91cd\u65b0\u53c3\u6578\u5316\u4f86\u52a0\u901f\u9810\u8a13\u7df4\u7684 LLM\uff0c\u5275\u9020\u51fa\u7a31\u70ba ShiftAddLLM \u7684\u9ad8\u6548\u7121\u4e58\u6cd5\u6a21\u578b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u6bcf\u500b\u6b0a\u91cd\u77e9\u9663\u91cf\u5316\u70ba\u4e8c\u9032\u5236\u77e9\u9663\uff0c\u4e26\u914d\u5c0d\u7d44\u7d1a\u7e2e\u653e\u56e0\u5b50\u3002\u76f8\u95dc\u7684\u4e58\u6cd5\u91cd\u65b0\u53c3\u6578\u5316\u70ba\uff1a(1) \u6fc0\u6d3b\u548c\u7e2e\u653e\u56e0\u5b50\u4e4b\u9593\u7684\u79fb\u4f4d\uff0c\u4ee5\u53ca (2) \u67e5\u8a62\u548c\u6839\u64da\u4e8c\u9032\u5236\u77e9\u9663\u9032\u884c\u52a0\u6cd5\u3002\u70ba\u4e86\u6e1b\u5c11\u6e96\u78ba\u6027\u640d\u5931\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u591a\u76ee\u6a19\u512a\u5316\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u6b0a\u91cd\u548c\u8f38\u51fa\u6fc0\u6d3b\u91cd\u65b0\u53c3\u6578\u5316\u8aa4\u5dee\u3002\u6b64\u5916\uff0c\u57fa\u65bc\u5404\u5c64\u5c0d\u91cd\u65b0\u53c3\u6578\u5316\u7684\u4e0d\u540c\u654f\u611f\u6027\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u81ea\u52d5\u5316\u4f4d\u5143\u5206\u914d\u7b56\u7565\uff0c\u4ee5\u9032\u4e00\u6b65\u6e1b\u5c11\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u548c\u5ef6\u9072\u3002\u5728\u4e94\u500b LLM \u7cfb\u5217\u548c\u516b\u500b\u4efb\u52d9\u4e0a\u7684\u5be6\u9a57\u6301\u7e8c\u9a57\u8b49\u4e86 ShiftAddLLM \u7684\u6709\u6548\u6027\uff0c\u5728\u8207\u6700\u5177\u7af6\u722d\u529b\u7684 3 \u4f4d\u548c 2 \u4f4d\u91cf\u5316 LLM \u76f8\u7576\u6216\u66f4\u4f4e\u7684\u5ef6\u9072\u4e0b\uff0c\u5206\u5225\u5be6\u73fe\u4e86\u5e73\u5747\u56f0\u60d1\u5ea6\u6539\u5584 5.6 \u9ede\u548c 22.7 \u9ede\uff0c\u4e26\u4e14\u6bd4\u539f\u59cb LLM \u7bc0\u7701\u4e86\u8d85\u904e 80% \u7684\u8a18\u61b6\u9ad4\u548c\u80fd\u6e90\u3002\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u5728 https://github.com/GATECH-EIC/ShiftAddLLM \u53d6\u5f97\u3002", "author": "Haoran You et.al.", "authors": "Haoran You, Yipin Guo, Yichao Fu, Wei Zhou, Huihong Shi, Xiaofan Zhang, Souvik Kundu, Amir Yazdanbakhsh, Yingyan Lin", "id": "2406.05981v1", "paper_url": "http://arxiv.org/abs/2406.05981v1", "repo": "null"}}