{"2410.02498": {"publish_time": "2024-10-03", "title": "Dynamic Gradient Alignment for Online Data Mixing", "paper_summary": "The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability.", "paper_summary_zh": "\u8a13\u7df4\u8cc7\u6599\u6df7\u5408\u7269\u7684\u7d44\u6210\u5c0d\u65bc\u6709\u6548\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u81f3\u95dc\u91cd\u8981\uff0c\u56e0\u70ba\u5b83\u76f4\u63a5\u5f71\u97ff\u5176\u5728\u4e0b\u6e38\u4efb\u52d9\u7684\u8868\u73fe\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\u627e\u51fa\u6700\u4f73\u8cc7\u6599\u6df7\u5408\u7269\uff0c\u4ee5\u4f7f\u7528\u7279\u5b9a\u4efb\u52d9\u7684\u5c11\u6578\u7bc4\u4f8b\uff0c\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u5c08\u9580\u5316 LLM\u3002\u89e3\u6c7a\u6b64\u554f\u984c\u7684\u50b3\u7d71\u65b9\u6cd5\u5305\u62ec\u81e8\u6642\u91cd\u65b0\u52a0\u6b0a\u65b9\u6cd5\u3001\u91cd\u8981\u6027\u53d6\u6a23\u548c\u68af\u5ea6\u5c0d\u9f4a\u6280\u8853\u3002\u672c\u6587\u91cd\u9ede\u5728\u68af\u5ea6\u5c0d\u9f4a\uff0c\u4e26\u4ecb\u7d39\u52d5\u614b\u68af\u5ea6\u5c0d\u9f4a (DGA)\uff0c\u4e00\u7a2e\u53ef\u64f4\u5145\u7684\u7dda\u4e0a\u68af\u5ea6\u5c0d\u9f4a\u6f14\u7b97\u6cd5\u3002DGA \u52d5\u614b\u4f30\u8a08\u9810\u8a13\u7df4\u8cc7\u6599\u6df7\u5408\u7269\uff0c\u6a21\u578b\u7684\u68af\u5ea6\u8207\u7279\u5b9a\u4efb\u52d9\u6a21\u578b\u7684\u68af\u5ea6\u5728\u6b64\u6df7\u5408\u7269\u4e0a\u5c0d\u9f4a\u5f97\u6700\u597d\u3002DGA \u662f\u7b2c\u4e00\u500b\u68af\u5ea6\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u8207\u6a19\u6e96\u9810\u8a13\u7df4\u76f8\u6bd4\uff0c\u7522\u751f\u7684\u958b\u92b7\u6700\u5c0f\uff0c\u4e14\u8f38\u51fa\u5177\u7af6\u722d\u529b\u7684\u6a21\u578b\uff0c\u6d88\u9664\u4e86\u91cd\u65b0\u8a13\u7df4\u6a21\u578b\u7684\u9700\u8981\u3002\u900f\u904e\u5be6\u9a57\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5728\u5169\u500b\u95dc\u9375\u60c5\u5883\u4e2d\uff0c\u76f8\u8f03\u65bc\u91cd\u8981\u6027\u53d6\u6a23\uff0c\u6709\u986f\u8457\u7684\u9032\u6b65\uff1a(i) \u7576\u9810\u8a13\u7df4\u96c6\u5f88\u5c0f\uff0c\u4e14\u91cd\u8981\u6027\u53d6\u6a23\u56e0\u8cc7\u6599\u6709\u9650\u800c\u904e\u5ea6\u64ec\u5408\u6642\uff1b(ii) \u7576\u6c92\u6709\u8db3\u5920\u7684\u5c08\u696d\u5316\u8cc7\u6599\u6642\uff0c\u5c07\u91cd\u8981\u6027\u53d6\u6a23\u9650\u5236\u5728\u72f9\u7a84\u7684\u8cc7\u6599\u5340\u584a\u4e2d\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u5f37\u8abf\u4e86\u68af\u5ea6\u5c0d\u9f4a\u65b9\u6cd5\u5728\u6700\u4f73\u5316\u8a13\u7df4\u8cc7\u6599\u6df7\u5408\u7269\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7279\u5225\u662f\u5728\u8cc7\u6599\u53d7\u9650\u7684\u74b0\u5883\u4e2d\uff0c\u4e26\u70ba\u5728\u8cc7\u6599\u53ef\u7528\u6027\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\uff0c\u63d0\u5347\u7279\u5b9a\u4efb\u52d9\u7684 LLM \u8868\u73fe\uff0c\u63d0\u4f9b\u4e86\u5be6\u7528\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Simin Fan et.al.", "authors": "Simin Fan, David Grangier, Pierre Ablin", "id": "2410.02498v1", "paper_url": "http://arxiv.org/abs/2410.02498v1", "repo": "null"}}