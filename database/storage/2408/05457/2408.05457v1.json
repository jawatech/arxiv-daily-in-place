{"2408.05457": {"publish_time": "2024-08-10", "title": "Investigating Instruction Tuning Large Language Models on Graphs", "paper_summary": "Inspired by the recent advancements of Large Language Models (LLMs) in NLP\ntasks, there's growing interest in applying LLMs to graph-related tasks. This\nstudy delves into the capabilities of instruction-following LLMs for engaging\nwith real-world graphs, aiming to offer empirical insights into how LLMs can\neffectively interact with graphs and generalize across graph tasks. We begin by\nconstructing a dataset designed for instruction tuning, which comprises a\ndiverse collection of 79 graph-related tasks from academic and e-commerce\ndomains, featuring 44,240 training instances and 18,960 test samples. Utilizing\nthis benchmark, our initial investigation focuses on identifying the optimal\ngraph representation that serves as a conduit for LLMs to understand complex\ngraph structures. Our findings indicate that JSON format for graph\nrepresentation consistently outperforms natural language and code formats\nacross various LLMs and graph types. Furthermore, we examine the key factors\nthat influence the generalization abilities of instruction-tuned LLMs by\nevaluating their performance on both in-domain and out-of-domain graph tasks.", "paper_summary_zh": "\u53d7\u5230\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8fd1\u671f\u9032\u5c55\u7684\u555f\u767c\uff0c\u5c07 LLM \u61c9\u7528\u65bc\u8207\u5716\u8868\u76f8\u95dc\u4efb\u52d9\u7684\u8208\u8da3\u65e5\u76ca\u6fc3\u539a\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u9075\u5faa\u6307\u4ee4\u7684 LLM \u7684\u529f\u80fd\uff0c\u4ee5\u5f9e\u4e8b\u771f\u5be6\u4e16\u754c\u7684\u5716\u8868\uff0c\u65e8\u5728\u63d0\u4f9b LLM \u5982\u4f55\u6709\u6548\u5730\u8207\u5716\u8868\u4e92\u52d5\u4e26\u5728\u5716\u8868\u4efb\u52d9\u4e2d\u9032\u884c\u6982\u62ec\u7684\u7d93\u9a57\u898b\u89e3\u3002\u6211\u5011\u5f9e\u69cb\u5efa\u4e00\u500b\u5c08\u70ba\u6307\u4ee4\u8abf\u6574\u800c\u8a2d\u8a08\u7684\u8cc7\u6599\u96c6\u958b\u59cb\uff0c\u5176\u4e2d\u5305\u542b\u4f86\u81ea\u5b78\u8853\u548c\u96fb\u5b50\u5546\u52d9\u9818\u57df\u7684 79 \u500b\u5716\u8868\u76f8\u95dc\u4efb\u52d9\u7684\u591a\u5143\u5316\u96c6\u5408\uff0c\u5305\u542b 44,240 \u500b\u8a13\u7df4\u5be6\u4f8b\u548c 18,960 \u500b\u6e2c\u8a66\u6a23\u672c\u3002\u5229\u7528\u6b64\u57fa\u6e96\uff0c\u6211\u5011\u7684\u521d\u6b65\u8abf\u67e5\u91cd\u9ede\u5728\u65bc\u8b58\u5225\u6700\u4f73\u5716\u8868\u8868\u793a\uff0c\u4f5c\u70ba LLM \u7406\u89e3\u8907\u96dc\u5716\u8868\u7d50\u69cb\u7684\u7ba1\u9053\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0cJSON \u683c\u5f0f\u7684\u5716\u8868\u8868\u793a\u5728\u5404\u7a2e LLM \u548c\u5716\u8868\u985e\u578b\u4e2d\u59cb\u7d42\u512a\u65bc\u81ea\u7136\u8a9e\u8a00\u548c\u7a0b\u5f0f\u78bc\u683c\u5f0f\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5f71\u97ff\u6307\u4ee4\u8abf\u6574 LLM \u6982\u62ec\u80fd\u529b\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u65b9\u6cd5\u662f\u8a55\u4f30\u5b83\u5011\u5728\u9818\u57df\u5167\u548c\u9818\u57df\u5916\u5716\u8868\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u3002", "author": "Kerui Zhu et.al.", "authors": "Kerui Zhu, Bo-Wei Huang, Bowen Jin, Yizhu Jiao, Ming Zhong, Kevin Chang, Shou-De Lin, Jiawei Han", "id": "2408.05457v1", "paper_url": "http://arxiv.org/abs/2408.05457v1", "repo": "null"}}