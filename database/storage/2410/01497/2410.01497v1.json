{"2410.01497": {"publish_time": "2024-10-02", "title": "DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models", "paper_summary": "Recent advancements in Large Language Models (LLMs) have achieved robust\nperformance across diverse tasks, but fine-tuning these models for specific\ndomains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT)\nmethods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a\nsmall subset of parameters. However, existing methods for fusing multiple LoRAs\nlack dynamic fusion based on contextual inputs and often increase inference\ntime due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight\nPlugin that employs a mini-MLP module with only 5M parameters to dynamically\nfuse multiple LoRAs at the sentence level using top-p sampling strategies. This\napproach reduces inference time to less than twice that of single LoRA\ninference by leveraging parallel computation. Evaluations across 26\ntasks-including multiple-choice questions and question answering-demonstrate\nthat DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice\ndatasets and significant improvements in BLEU and ROUGE scores on QA datasets,\noutperforming different LLMs backbones under composite task settings. DLP-LoRA\neffectively balances performance and efficiency, making it a practical solution\nfor dynamic multi-task adaptation in LLMs. Our code is available at\nhttps://github.com/MeCuping/DLP-LoRA.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u90fd\u53d6\u5f97\u4e86\u7a69\u5065\u7684\u8868\u73fe\uff0c\u4f46\u91dd\u5c0d\u7279\u5b9a\u9818\u57df\u5fae\u8abf\u9019\u4e9b\u6a21\u578b\u4ecd\u7136\u9700\u8981\u5927\u91cf\u8cc7\u6e90\u3002\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\uff08\u4f8b\u5982\u4f4e\u79e9\u9069\u61c9 (LoRA)\uff09\u900f\u904e\u5fae\u8abf\u4e00\u5c0f\u90e8\u5206\u53c3\u6578\u4f86\u89e3\u6c7a\u9019\u500b\u6311\u6230\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u878d\u5408\u591a\u500b LoRA \u7684\u65b9\u6cd5\u7f3a\u4e4f\u57fa\u65bc\u4e0a\u4e0b\u6587\u8f38\u5165\u7684\u52d5\u614b\u878d\u5408\uff0c\u4e26\u4e14\u7531\u65bc\u4ee3\u78bc\u7d1a\u5225\u64cd\u4f5c\uff0c\u901a\u5e38\u6703\u589e\u52a0\u63a8\u8ad6\u6642\u9593\u3002\u6211\u5011\u63d0\u51fa DLP-LoRA\uff0c\u9019\u662f\u4e00\u500b\u52d5\u614b\u8f15\u91cf\u7d1a\u5916\u639b\u7a0b\u5f0f\uff0c\u5b83\u4f7f\u7528\u50c5\u6709 5M \u53c3\u6578\u7684 mini-MLP \u6a21\u7d44\uff0c\u4ee5\u4f7f\u7528 top-p \u62bd\u6a23\u7b56\u7565\u5728\u53e5\u5b50\u5c64\u7d1a\u52d5\u614b\u878d\u5408\u591a\u500b LoRA\u3002\u6b64\u65b9\u6cd5\u900f\u904e\u5229\u7528\u4e26\u884c\u904b\u7b97\uff0c\u5c07\u63a8\u8ad6\u6642\u9593\u6e1b\u5c11\u5230\u4e0d\u5230\u55ae\u4e00 LoRA \u63a8\u8ad6\u6642\u9593\u7684\u5169\u500d\u3002\u5728 26 \u9805\u4efb\u52d9\uff08\u5305\u62ec\u591a\u9078\u984c\u548c\u554f\u7b54\uff09\u4e2d\u7684\u8a55\u4f30\u8b49\u660e\uff0cDLP-LoRA \u5728\u591a\u9078\u984c\u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u4e86 92.34% \u7684\u5e73\u5747\u6e96\u78ba\u5ea6\uff0c\u4e26\u4e14\u5728 QA \u8cc7\u6599\u96c6\u4e0a BLEU \u548c ROUGE \u5206\u6578\u6709\u986f\u8457\u63d0\u5347\uff0c\u5728\u8907\u5408\u4efb\u52d9\u8a2d\u5b9a\u4e0b\u512a\u65bc\u4e0d\u540c\u7684 LLM \u4e3b\u5e79\u3002DLP-LoRA \u6709\u6548\u5730\u5e73\u8861\u4e86\u6548\u80fd\u548c\u6548\u7387\uff0c\u4f7f\u5176\u6210\u70ba LLM \u4e2d\u52d5\u614b\u591a\u4efb\u52d9\u9069\u61c9\u7684\u5be6\u7528\u89e3\u6c7a\u65b9\u6848\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/MeCuping/DLP-LoRA \u53d6\u5f97\u3002", "author": "Yuxuan Zhang et.al.", "authors": "Yuxuan Zhang, Ruizhe Li", "id": "2410.01497v1", "paper_url": "http://arxiv.org/abs/2410.01497v1", "repo": "https://github.com/mecuping/dlp-lora"}}