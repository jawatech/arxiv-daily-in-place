{"2405.20892": {"publish_time": "2024-05-31", "title": "MALT: Multi-scale Action Learning Transformer for Online Action Detection", "paper_summary": "Online action detection (OAD) aims to identify ongoing actions from streaming\nvideo in real-time, without access to future frames. Since these actions\nmanifest at varying scales of granularity, ranging from coarse to fine,\nprojecting an entire set of action frames to a single latent encoding may\nresult in a lack of local information, necessitating the acquisition of action\nfeatures across multiple scales. In this paper, we propose a multi-scale action\nlearning transformer (MALT), which includes a novel recurrent decoder (used for\nfeature fusion) that includes fewer parameters and can be trained more\nefficiently. A hierarchical encoder with multiple encoding branches is further\nproposed to capture multi-scale action features. The output from the preceding\nbranch is then incrementally input to the subsequent branch as part of a\ncross-attention calculation. In this way, output features transition from\ncoarse to fine as the branches deepen. We also introduce an explicit frame\nscoring mechanism employing sparse attention, which filters irrelevant frames\nmore efficiently, without requiring an additional network. The proposed method\nachieved state-of-the-art performance on two benchmark datasets (THUMOS'14 and\nTVSeries), outperforming all existing models used for comparison, with an mAP\nof 0.2% for THUMOS'14 and an mcAP of 0.1% for TVseries.", "paper_summary_zh": "\u7dda\u4e0a\u52d5\u4f5c\u5075\u6e2c (OAD) \u65e8\u5728\u5f9e\u4e32\u6d41\u5f71\u7247\u4e2d\u8fa8\u8b58\u6b63\u5728\u9032\u884c\u7684\u52d5\u4f5c\uff0c\u4e14\u70ba\u5373\u6642\u5075\u6e2c\uff0c\u4e14\u7121\u6cd5\u5b58\u53d6\u5f8c\u7e8c\u7684\u5f71\u683c\u3002\u7531\u65bc\u9019\u4e9b\u52d5\u4f5c\u6703\u4ee5\u4e0d\u540c\u5c3a\u5ea6\u7684\u7c92\u5ea6\u986f\u73fe\uff0c\u5f9e\u7c97\u7565\u5230\u7cbe\u7d30\uff0c\u5c07\u4e00\u6574\u7d44\u52d5\u4f5c\u5f71\u683c\u6295\u5f71\u5230\u55ae\u4e00\u6f5b\u5728\u7de8\u78bc\u4e2d\u53ef\u80fd\u6703\u5c0e\u81f4\u7f3a\u4e4f\u5c40\u90e8\u8cc7\u8a0a\uff0c\u56e0\u6b64\u9700\u8981\u8de8\u591a\u500b\u5c3a\u5ea6\u53d6\u5f97\u52d5\u4f5c\u7279\u5fb5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u591a\u5c3a\u5ea6\u52d5\u4f5c\u5b78\u7fd2\u8f49\u63db\u5668 (MALT)\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u500b\u65b0\u7a4e\u7684\u905e\u8ff4\u89e3\u78bc\u5668 (\u7528\u65bc\u7279\u5fb5\u878d\u5408)\uff0c\u8a72\u89e3\u78bc\u5668\u5305\u542b\u8f03\u5c11\u7684\u53c3\u6578\uff0c\u4e14\u53ef\u4ee5\u66f4\u6709\u6548\u7387\u5730\u9032\u884c\u8a13\u7df4\u3002\u9032\u4e00\u6b65\u63d0\u51fa\u4e00\u500b\u5177\u6709\u591a\u500b\u7de8\u78bc\u5206\u652f\u7684\u968e\u5c64\u5f0f\u7de8\u78bc\u5668\uff0c\u4ee5\u64f7\u53d6\u591a\u5c3a\u5ea6\u52d5\u4f5c\u7279\u5fb5\u3002\u7136\u5f8c\u5c07\u524d\u4e00\u500b\u5206\u652f\u7684\u8f38\u51fa\u4f5c\u70ba\u8de8\u6ce8\u610f\u529b\u8a08\u7b97\u7684\u4e00\u90e8\u5206\uff0c\u905e\u589e\u8f38\u5165\u5230\u5f8c\u7e8c\u5206\u652f\u3002\u9019\u6a23\u4e00\u4f86\uff0c\u96a8\u8457\u5206\u652f\u7684\u52a0\u6df1\uff0c\u8f38\u51fa\u7279\u5fb5\u6703\u5f9e\u7c97\u7565\u8f49\u8b8a\u70ba\u7cbe\u7d30\u3002\u6211\u5011\u9084\u5f15\u5165\u4e86\u4e00\u500b\u660e\u78ba\u7684\u5f71\u683c\u8a55\u5206\u6a5f\u5236\uff0c\u63a1\u7528\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u7387\u5730\u904e\u6ffe\u4e0d\u76f8\u95dc\u7684\u5f71\u683c\uff0c\u800c\u4e0d\u9700\u8981\u984d\u5916\u7684\u7db2\u8def\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5169\u500b\u57fa\u6e96\u8cc7\u6599\u96c6 (THUMOS'14 \u548c TVSeries) \u4e0a\u9054\u5230\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u512a\u65bc\u6240\u6709\u73fe\u6709\u7684\u6bd4\u8f03\u6a21\u578b\uff0c\u5176\u4e2d THUMOS'14 \u7684 mAP \u70ba 0.2%\uff0cTVseries \u7684 mcAP \u70ba 0.1%\u3002", "author": "Zhipeng Yang et.al.", "authors": "Zhipeng Yang, Ruoyu Wang, Yang Tan, Liping Xie", "id": "2405.20892v1", "paper_url": "http://arxiv.org/abs/2405.20892v1", "repo": "null"}}