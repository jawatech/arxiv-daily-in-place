{"2409.05021": {"publish_time": "2024-09-08", "title": "Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text against Neural Machine Translation", "paper_summary": "While neural machine translation (NMT) models achieve success in our daily\nlives, they show vulnerability to adversarial attacks. Despite being harmful,\nthese attacks also offer benefits for interpreting and enhancing NMT models,\nthus drawing increased research attention. However, existing studies on\nadversarial attacks are insufficient in both attacking ability and human\nimperceptibility due to their sole focus on the scope of language. This paper\nproposes a novel vision-fused attack (VFA) framework to acquire powerful\nadversarial text, i.e., more aggressive and stealthy. Regarding the attacking\nability, we design the vision-merged solution space enhancement strategy to\nenlarge the limited semantic solution space, which enables us to search for\nadversarial candidates with higher attacking ability. For human\nimperceptibility, we propose the perception-retained adversarial text selection\nstrategy to align the human text-reading mechanism. Thus, the finally selected\nadversarial text could be more deceptive. Extensive experiments on various\nmodels, including large language models (LLMs) like LLaMA and GPT-3.5, strongly\nsupport that VFA outperforms the comparisons by large margins (up to 81%/14%\nimprovements on ASR/SSIM).", "paper_summary_zh": "\u5118\u7ba1\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f (NMT) \u6a21\u578b\u5728\u6211\u5011\u7684\u65e5\u5e38\u751f\u6d3b\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5b83\u5011\u5c0d\u5c0d\u6297\u653b\u64ca\u986f\u793a\u51fa\u8106\u5f31\u6027\u3002\u5118\u7ba1\u6709\u5bb3\uff0c\u4f46\u9019\u4e9b\u653b\u64ca\u4e5f\u70ba\u89e3\u91cb\u548c\u589e\u5f37 NMT \u6a21\u578b\u63d0\u4f9b\u4e86\u597d\u8655\uff0c\u5f9e\u800c\u5f15\u8d77\u8d8a\u4f86\u8d8a\u591a\u7684\u7814\u7a76\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u5c0d\u6297\u653b\u64ca\u7814\u7a76\u7531\u65bc\u50c5\u95dc\u6ce8\u8a9e\u8a00\u7bc4\u570d\uff0c\u56e0\u6b64\u5728\u653b\u64ca\u80fd\u529b\u548c\u4eba\u985e\u4e0d\u53ef\u611f\u77e5\u6027\u65b9\u9762\u5747\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u8996\u89ba\u878d\u5408\u653b\u64ca (VFA) \u6846\u67b6\uff0c\u4ee5\u7372\u53d6\u5f37\u5927\u7684\u5c0d\u6297\u6587\u672c\uff0c\u5373\u66f4\u5177\u653b\u64ca\u6027\u548c\u96b1\u853d\u6027\u3002\u95dc\u65bc\u653b\u64ca\u80fd\u529b\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u8996\u89ba\u5408\u4f75\u89e3\u6c7a\u65b9\u6848\u7a7a\u9593\u589e\u5f37\u7b56\u7565\uff0c\u4ee5\u64f4\u5927\u6709\u9650\u7684\u8a9e\u7fa9\u89e3\u6c7a\u65b9\u6848\u7a7a\u9593\uff0c\u9019\u4f7f\u6211\u5011\u80fd\u5920\u641c\u5c0b\u5177\u6709\u66f4\u9ad8\u653b\u64ca\u80fd\u529b\u7684\u5c0d\u6297\u5019\u9078\u8005\u3002\u5c0d\u65bc\u4eba\u985e\u7684\u4e0d\u53ef\u611f\u77e5\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u611f\u77e5\u4fdd\u7559\u5c0d\u6297\u6587\u672c\u9078\u64c7\u7b56\u7565\uff0c\u4ee5\u8abf\u6574\u4eba\u985e\u6587\u672c\u95b1\u8b80\u6a5f\u5236\u3002\u56e0\u6b64\uff0c\u6700\u7d42\u9078\u64c7\u7684\u5c0d\u6297\u6587\u672c\u53ef\u80fd\u6703\u66f4\u5177\u6b3a\u9a19\u6027\u3002\u5728\u5404\u7a2e\u6a21\u578b\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\uff0c\u5305\u62ec\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4f8b\u5982 LLaMA \u548c GPT-3.5\uff0c\u6709\u529b\u5730\u8b49\u660e VFA \u4ee5\u5f88\u5927\u7684\u5e45\u5ea6\u512a\u65bc\u6bd4\u8f03\uff08\u5728 ASR/SSIM \u4e0a\u7684\u6539\u9032\u5e45\u5ea6\u9ad8\u9054 81%/14%\uff09\u3002", "author": "Yanni Xue et.al.", "authors": "Yanni Xue, Haojie Hao, Jiakai Wang, Qiang Sheng, Renshuai Tao, Yu Liang, Pu Feng, Xianglong Liu", "id": "2409.05021v1", "paper_url": "http://arxiv.org/abs/2409.05021v1", "repo": "https://github.com/levelower/vfa"}}