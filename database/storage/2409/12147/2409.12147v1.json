{"2409.12147": {"publish_time": "2024-09-18", "title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning", "paper_summary": "Large Language Models' (LLM) reasoning can be improved using test-time\naggregation strategies, i.e., generating multiple samples and voting among\ngenerated samples. While these improve performance, they often reach a\nsaturation point. Refinement offers an alternative by using LLM-generated\nfeedback to improve solution quality. However, refinement introduces 3 key\nchallenges: (1) Excessive refinement: Uniformly refining all instances can\nover-correct and reduce the overall performance. (2) Inability to localize and\naddress errors: LLMs have a limited ability to self-correct and struggle to\nidentify and correct their own mistakes. (3) Insufficient refinement: Deciding\nhow many iterations of refinement are needed is non-trivial, and stopping too\nsoon could leave errors unaddressed. To tackle these issues, we propose\nMAgICoRe, which avoids excessive refinement by categorizing problem difficulty\nas easy or hard, solving easy problems with coarse-grained aggregation and hard\nones with fine-grained and iterative multi-agent refinement. To improve error\nlocalization, we incorporate external step-wise reward model (RM) scores.\nMoreover, to ensure effective refinement, we employ a multi-agent loop with\nthree agents: Solver, Reviewer (which generates targeted feedback based on\nstep-wise RM scores), and the Refiner (which incorporates feedback). To ensure\nsufficient refinement, we re-evaluate updated solutions, iteratively initiating\nfurther rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5\nand show its effectiveness across 5 math datasets. Even one iteration of\nMAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by\n4.0% while using less than half the samples. Unlike iterative refinement with\nbaselines, MAgICoRe continues to improve with more iterations. Finally, our\nablations highlight the importance of MAgICoRe's RMs and multi-agent\ncommunication.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u4f7f\u7528\u6e2c\u8a66\u6642\u9593\u805a\u5408\u7b56\u7565\u4f86\u6539\u5584\uff0c\u5373\u751f\u6210\u591a\u500b\u6a23\u672c\u4e26\u5728\u751f\u6210\u7684\u6a23\u672c\u4e2d\u9032\u884c\u6295\u7968\u3002\u96d6\u7136\u9019\u4e9b\u7b56\u7565\u53ef\u4ee5\u63d0\u5347\u6548\u80fd\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u6703\u9054\u5230\u98fd\u548c\u9ede\u3002\u7cbe\u7149\u63d0\u4f9b\u4e86\u4e00\u7a2e\u66ff\u4ee3\u65b9\u6848\uff0c\u85c9\u7531\u4f7f\u7528 LLM \u751f\u6210\u7684\u56de\u994b\u4f86\u6539\u5584\u89e3\u6c7a\u65b9\u6848\u7684\u54c1\u8cea\u3002\u7136\u800c\uff0c\u7cbe\u7149\u5f15\u5165\u4e86 3 \u500b\u4e3b\u8981\u6311\u6230\uff1a(1) \u904e\u5ea6\u7cbe\u7149\uff1a\u5747\u52fb\u5730\u7cbe\u7149\u6240\u6709\u5be6\u4f8b\u53ef\u80fd\u6703\u904e\u5ea6\u4fee\u6b63\u4e26\u964d\u4f4e\u6574\u9ad4\u6548\u80fd\u3002(2) \u7121\u6cd5\u5b9a\u4f4d\u548c\u89e3\u6c7a\u932f\u8aa4\uff1aLLM \u81ea\u6211\u4fee\u6b63\u7684\u80fd\u529b\u6709\u9650\uff0c\u4e14\u96e3\u4ee5\u8fa8\u8b58\u548c\u4fee\u6b63\u81ea\u5df1\u7684\u932f\u8aa4\u3002(3) \u7cbe\u7149\u4e0d\u8db3\uff1a\u6c7a\u5b9a\u9700\u8981\u591a\u5c11\u6b21\u7cbe\u7149\u4e26\u975e\u6613\u4e8b\uff0c\u800c\u4e14\u592a\u5feb\u505c\u6b62\u53ef\u80fd\u6703\u8b93\u932f\u8aa4\u672a\u7372\u89e3\u6c7a\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 MAgICoRe\uff0c\u5b83\u900f\u904e\u5c07\u554f\u984c\u96e3\u5ea6\u5206\u985e\u70ba\u5bb9\u6613\u6216\u56f0\u96e3\uff0c\u4f7f\u7528\u7c97\u7565\u805a\u5408\u89e3\u6c7a\u5bb9\u6613\u7684\u554f\u984c\uff0c\u4e26\u4f7f\u7528\u7d30\u7dfb\u4e14\u53cd\u8986\u7684\u591a\u91cd\u4ee3\u7406\u7cbe\u7149\u89e3\u6c7a\u56f0\u96e3\u7684\u554f\u984c\uff0c\u5f9e\u800c\u907f\u514d\u904e\u5ea6\u7cbe\u7149\u3002\u70ba\u4e86\u6539\u5584\u932f\u8aa4\u5b9a\u4f4d\uff0c\u6211\u5011\u7d0d\u5165\u4e86\u5916\u90e8\u9010\u6b65\u734e\u52f5\u6a21\u578b (RM) \u5206\u6578\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u78ba\u4fdd\u7cbe\u7149\u6709\u6548\uff0c\u6211\u5011\u63a1\u7528\u4e00\u500b\u5305\u542b\u4e09\u500b\u4ee3\u7406\u7684\u591a\u91cd\u4ee3\u7406\u8ff4\u5708\uff1a\u6c42\u89e3\u5668\u3001\u5be9\u67e5\u8005\uff08\u6839\u64da\u9010\u6b65 RM \u5206\u6578\u7522\u751f\u76ee\u6a19\u56de\u994b\uff09\uff0c\u4ee5\u53ca\u7cbe\u7149\u5668\uff08\u7d0d\u5165\u56de\u994b\uff09\u3002\u70ba\u4e86\u78ba\u4fdd\u7cbe\u7149\u5145\u5206\uff0c\u6211\u5011\u91cd\u65b0\u8a55\u4f30\u66f4\u65b0\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u53cd\u8986\u555f\u52d5\u9032\u4e00\u6b65\u7684\u7cbe\u7149\u56de\u5408\u3002\u6211\u5011\u5728 Llama-3-8B \u548c GPT-3.5 \u4e0a\u8a55\u4f30 MAgICoRe\uff0c\u4e26\u5c55\u793a\u4e86\u5b83\u5728 5 \u500b\u6578\u5b78\u8cc7\u6599\u96c6\u4e2d\u7684\u6709\u6548\u6027\u3002\u5373\u4f7f\u53ea\u9032\u884c\u4e00\u6b21 MAgICoRe \u53cd\u8986\u904b\u7b97\uff0c\u4e5f\u80fd\u6bd4\u81ea\u6211\u4e00\u81f4\u6027\u9ad8\u51fa 3.4%\uff0c\u6bd4\u6700\u4f73 k \u9ad8\u51fa 3.2%\uff0c\u6bd4\u81ea\u6211\u7cbe\u7149\u9ad8\u51fa 4.0%\uff0c\u540c\u6642\u4f7f\u7528\u7684\u6a23\u672c\u4e0d\u5230\u4e00\u534a\u3002\u8207\u4f7f\u7528\u57fa\u6e96\u9032\u884c\u53cd\u8986\u7cbe\u7149\u4e0d\u540c\uff0cMAgICoRe \u6703\u96a8\u8457\u53cd\u8986\u904b\u7b97\u6b21\u6578\u7684\u589e\u52a0\u800c\u6301\u7e8c\u6539\u5584\u3002\u6700\u5f8c\uff0c\u6211\u5011\u7684\u6d88\u878d\u5be6\u9a57\u7a81\u51fa\u4e86 MAgICoRe \u7684 RM \u548c\u591a\u91cd\u4ee3\u7406\u6e9d\u901a\u7684\u91cd\u8981\u6027\u3002", "author": "Justin Chih-Yao Chen et.al.", "authors": "Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal", "id": "2409.12147v1", "paper_url": "http://arxiv.org/abs/2409.12147v1", "repo": "https://github.com/dinobby/magicore"}}