{"2410.10818": {"publish_time": "2024-10-14", "title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models", "paper_summary": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable.", "paper_summary_zh": "<paragraph>\u4e86\u89e3\u7cbe\u7d30\u7684\u6642\u9593\u52d5\u614b\u5c0d\u65bc\u591a\u6a21\u614b\u5f71\u7247\u7406\u89e3\u548c\u751f\u6210\u81f3\u95dc\u91cd\u8981\u3002\u7531\u65bc\u7f3a\u4e4f\u7cbe\u7d30\u7684\u6642\u9593\u8a3b\u89e3\uff0c\u73fe\u6709\u7684\u5f71\u7247\u57fa\u6e96\u6e2c\u8a66\u5927\u591a\u985e\u4f3c\u65bc\u975c\u614b\u5f71\u50cf\u57fa\u6e96\u6e2c\u8a66\uff0c\u4e26\u4e14\u7121\u6cd5\u8a55\u4f30\u6642\u9593\u7406\u89e3\u6a21\u578b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 TemporalBench\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u57fa\u6e96\u6e2c\u8a66\uff0c\u5c08\u9580\u7528\u65bc\u8a55\u4f30\u5f71\u7247\u4e2d\u7684\u7cbe\u7d30\u6642\u9593\u7406\u89e3\u3002TemporalBench \u5305\u542b\u7d04 10K \u500b\u5f71\u7247\u554f\u7b54\u5c0d\uff0c\u9019\u4e9b\u5c0d\u4f86\u81ea\u7d04 2K \u500b\u4eba\u985e\u9ad8\u54c1\u8cea\u8a3b\u89e3\uff0c\u8a73\u7d30\u8aaa\u660e\u4e86\u5f71\u7247\u526a\u8f2f\u4e2d\u7684\u6642\u9593\u52d5\u614b\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7684\u57fa\u6e96\u6e2c\u8a66\u63d0\u4f9b\u4e86\u4e00\u500b\u7368\u7279\u7684\u6e2c\u8a66\u5e73\u53f0\uff0c\u7528\u65bc\u8a55\u4f30\u5404\u7a2e\u6642\u9593\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f8b\u5982\u52d5\u4f5c\u983b\u7387\u3001\u52d5\u4f5c\u5e45\u5ea6\u3001\u4e8b\u4ef6\u9806\u5e8f\u7b49\u3002\u6b64\u5916\uff0c\u5b83\u652f\u63f4\u5404\u7a2e\u4efb\u52d9\u7684\u8a55\u4f30\uff0c\u4f8b\u5982\u5f71\u7247\u554f\u7b54\u548c\u5b57\u5e55\u3001\u77ed\u5f71\u7247\u548c\u9577\u5f71\u7247\u7406\u89e3\uff0c\u4ee5\u53ca\u5404\u7a2e\u6a21\u578b\uff0c\u4f8b\u5982\u591a\u6a21\u614b\u5f71\u7247\u5d4c\u5165\u6a21\u578b\u548c\u6587\u5b57\u751f\u6210\u6a21\u578b\u3002\u7d50\u679c\u986f\u793a\uff0c\u50cf GPT-4o \u9019\u6a23\u7684\u6700\u5148\u9032\u6a21\u578b\u5728 TemporalBench \u4e0a\u50c5\u9054\u5230 38.5% \u7684\u554f\u7b54\u6e96\u78ba\u7387\uff0c\u9019\u8868\u660e\u4eba\u985e\u548c AI \u5728\u6642\u9593\u7406\u89e3\u4e0a\u5b58\u5728\u986f\u8457\u5dee\u8ddd\uff08\u7d04 30%\uff09\u3002\u6b64\u5916\uff0c\u6211\u5011\u6ce8\u610f\u5230\u591a\u9078\u984c QA \u7684\u4e00\u500b\u95dc\u9375\u7f3a\u9677\uff0c\u5176\u4e2d LLM \u53ef\u4ee5\u6aa2\u6e2c\u5230\u8ca0\u9762\u5b57\u5e55\u4e2d\u7684\u7d30\u5fae\u8b8a\u5316\uff0c\u4e26\u627e\u5230\u4e00\u500b\u96c6\u4e2d\u63cf\u8ff0\u4f5c\u70ba\u5176\u9810\u6e2c\u7684\u7dda\u7d22\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u591a\u91cd\u4e8c\u9032\u5236\u6e96\u78ba\u7387 (MBA) \u4f86\u7cfe\u6b63\u9019\u7a2e\u504f\u5dee\u3002\u6211\u5011\u5e0c\u671b TemporalBench \u80fd\u5920\u4fc3\u9032\u6539\u5584\u6a21\u578b\u6642\u9593\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u3002\u8cc7\u6599\u96c6\u548c\u8a55\u4f30\u7a0b\u5f0f\u78bc\u90fd\u5c07\u516c\u958b\u3002</paragraph>", "author": "Mu Cai et.al.", "authors": "Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, Yao Dou, Jaden Park, Jianfeng Gao, Yong Jae Lee, Jianwei Yang", "id": "2410.10818v1", "paper_url": "http://arxiv.org/abs/2410.10818v1", "repo": "null"}}