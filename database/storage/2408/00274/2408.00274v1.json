{"2408.00274": {"publish_time": "2024-08-01", "title": "QUITO: Accelerating Long-Context Reasoning through Query-Guided Context Compression", "paper_summary": "In-context learning (ICL) capabilities are foundational to the success of\nlarge language models (LLMs). Recently, context compression has attracted\ngrowing interest since it can largely reduce reasoning complexities and\ncomputation costs of LLMs. In this paper, we introduce a novel Query-gUIded\naTtention cOmpression (QUITO) method, which leverages attention of the question\nover the contexts to filter useless information. Specifically, we take a\ntrigger token to calculate the attention distribution of the context in\nresponse to the question. Based on the distribution, we propose three different\nfiltering methods to satisfy the budget constraints of the context length. We\nevaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and\nASQA. Experimental results demonstrate that QUITO significantly outperforms\nestablished baselines across various datasets and downstream LLMs, underscoring\nits effectiveness. Our code is available at\nhttps://github.com/Wenshansilvia/attention_compressor.", "paper_summary_zh": "\u8108\u7d61\u5b78\u7fd2 (ICL) \u80fd\u529b\u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6210\u529f\u767c\u5c55\u7684\u57fa\u790e\u3002\u6700\u8fd1\uff0c\u8108\u7d61\u58d3\u7e2e\u5f15\u8d77\u4e86\u8d8a\u4f86\u8d8a\u591a\u7684\u95dc\u6ce8\uff0c\u56e0\u70ba\u5b83\u53ef\u4ee5\u5927\u5e45\u964d\u4f4e LLM \u7684\u63a8\u7406\u8907\u96dc\u5ea6\u548c\u8a08\u7b97\u6210\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684 Query \u5f15\u5c0e\u5f0f\u6ce8\u610f\u529b\u58d3\u7e2e (QUITO) \u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u554f\u984c\u5c0d\u8108\u7d61\u7684\u95dc\u6ce8\u4f86\u904e\u6ffe\u7121\u7528\u7684\u8cc7\u8a0a\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63a1\u7528\u89f8\u767c\u8a5e\u5f59\u4f86\u8a08\u7b97\u8108\u7d61\u5728\u56de\u61c9\u554f\u984c\u6642\u7684\u6ce8\u610f\u529b\u5206\u4f48\u3002\u6839\u64da\u5206\u4f48\uff0c\u6211\u5011\u63d0\u51fa\u4e09\u7a2e\u4e0d\u540c\u7684\u904e\u6ffe\u65b9\u6cd5\u4f86\u6eff\u8db3\u8108\u7d61\u9577\u5ea6\u7684\u9810\u7b97\u9650\u5236\u3002\u6211\u5011\u4f7f\u7528\u5169\u500b\u5ee3\u6cdb\u4f7f\u7528\u7684\u8cc7\u6599\u96c6\uff0c\u5373 NaturalQuestions \u548c ASQA\uff0c\u4f86\u8a55\u4f30 QUITO\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cQUITO \u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u548c\u4e0b\u6e38 LLM \u4e2d\u90fd\u660e\u986f\u512a\u65bc\u5df2\u5efa\u7acb\u7684\u57fa\u6e96\uff0c\u7a81\u986f\u4e86\u5176\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/Wenshansilvia/attention_compressor \u53d6\u5f97\u3002", "author": "Wenshan Wang et.al.", "authors": "Wenshan Wang, Yihang Wang, Yixing Fan, Huaming Liao, Jiafeng Guo", "id": "2408.00274v1", "paper_url": "http://arxiv.org/abs/2408.00274v1", "repo": "null"}}