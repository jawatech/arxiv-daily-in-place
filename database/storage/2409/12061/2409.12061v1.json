{"2409.12061": {"publish_time": "2024-09-18", "title": "Generalized Robot Learning Framework", "paper_summary": "Imitation based robot learning has recently gained significant attention in\nthe robotics field due to its theoretical potential for transferability and\ngeneralizability. However, it remains notoriously costly, both in terms of\nhardware and data collection, and deploying it in real-world environments\ndemands meticulous setup of robots and precise experimental conditions. In this\npaper, we present a low-cost robot learning framework that is both easily\nreproducible and transferable to various robots and environments. We\ndemonstrate that deployable imitation learning can be successfully applied even\nto industrial-grade robots, not just expensive collaborative robotic arms.\nFurthermore, our results show that multi-task robot learning is achievable with\nsimple network architectures and fewer demonstrations than previously thought\nnecessary. As the current evaluating method is almost subjective when it comes\nto real-world manipulation tasks, we propose Voting Positive Rate (VPR) - a\nnovel evaluation strategy that provides a more objective assessment of\nperformance. We conduct an extensive comparison of success rates across various\nself-designed tasks to validate our approach. To foster collaboration and\nsupport the robot learning community, we have open-sourced all relevant\ndatasets and model checkpoints, available at huggingface.co/ZhiChengAI.", "paper_summary_zh": "\u6a21\u4eff\u5f0f\u6a5f\u5668\u4eba\u5b78\u7fd2\u6700\u8fd1\u5728\u6a5f\u5668\u4eba\u9818\u57df\u7372\u5f97\u4e86\u6975\u5927\u7684\u95dc\u6ce8\uff0c\u56e0\u70ba\u5b83\u5728\u53ef\u8f49\u79fb\u6027\u548c\u6982\u62ec\u6027\u65b9\u9762\u5177\u6709\u7406\u8ad6\u6f5b\u529b\u3002\u7136\u800c\uff0c\u5b83\u4ecd\u7136\u60e1\u540d\u662d\u5f70\u5730\u6602\u8cb4\uff0c\u7121\u8ad6\u662f\u5728\u786c\u9ad4\u9084\u662f\u8cc7\u6599\u6536\u96c6\u65b9\u9762\uff0c\u800c\u4e14\u5728\u73fe\u5be6\u4e16\u754c\u74b0\u5883\u4e2d\u90e8\u7f72\u5b83\u9700\u8981\u5c0d\u6a5f\u5668\u4eba\u9032\u884c\u7d30\u7dfb\u7684\u8a2d\u5b9a\u548c\u7cbe\u78ba\u7684\u5be6\u9a57\u689d\u4ef6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u4f4e\u6210\u672c\u7684\u6a5f\u5668\u4eba\u5b78\u7fd2\u6846\u67b6\uff0c\u5b83\u6613\u65bc\u8907\u88fd\uff0c\u4e26\u4e14\u53ef\u4ee5\u8f49\u79fb\u5230\u5404\u7a2e\u6a5f\u5668\u4eba\u548c\u74b0\u5883\u4e2d\u3002\u6211\u5011\u8b49\u660e\uff0c\u53ef\u90e8\u7f72\u7684\u6a21\u4eff\u5b78\u7fd2\u53ef\u4ee5\u6210\u529f\u61c9\u7528\u65bc\u5de5\u696d\u7d1a\u6a5f\u5668\u4eba\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6602\u8cb4\u7684\u5354\u4f5c\u6a5f\u5668\u4eba\u624b\u81c2\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u591a\u4efb\u52d9\u6a5f\u5668\u4eba\u5b78\u7fd2\u53ef\u4ee5\u4f7f\u7528\u7c21\u55ae\u7684\u7db2\u8def\u67b6\u69cb\u548c\u6bd4\u4ee5\u524d\u8a8d\u70ba\u5fc5\u8981\u7684\u66f4\u5c11\u7684\u6f14\u793a\u4f86\u5be6\u73fe\u3002\u7531\u65bc\u7576\u524d\u7684\u8a55\u4f30\u65b9\u6cd5\u5728\u6d89\u53ca\u5be6\u969b\u64cd\u4f5c\u4efb\u52d9\u6642\u5e7e\u4e4e\u662f\u4e3b\u89c0\u7684\uff0c\u56e0\u6b64\u6211\u5011\u63d0\u51fa\u4e86\u6295\u7968\u967d\u6027\u7387 (VPR) - \u4e00\u7a2e\u65b0\u7a4e\u7684\u8a55\u4f30\u7b56\u7565\uff0c\u5b83\u63d0\u4f9b\u4e86\u5c0d\u6027\u80fd\u66f4\u5ba2\u89c0\u7684\u8a55\u4f30\u3002\u6211\u5011\u5c0d\u5404\u7a2e\u81ea\u8a2d\u8a08\u4efb\u52d9\u7684\u6210\u529f\u7387\u9032\u884c\u4e86\u5ee3\u6cdb\u6bd4\u8f03\uff0c\u4ee5\u9a57\u8b49\u6211\u5011\u7684\u505a\u6cd5\u3002\u70ba\u4e86\u4fc3\u9032\u5408\u4f5c\u4e26\u652f\u6301\u6a5f\u5668\u4eba\u5b78\u7fd2\u793e\u7fa4\uff0c\u6211\u5011\u5df2\u958b\u653e\u539f\u59cb\u78bc\u6240\u6709\u76f8\u95dc\u8cc7\u6599\u96c6\u548c\u6a21\u578b\u6aa2\u67e5\u9ede\uff0c\u53ef\u5728 huggingface.co/ZhiChengAI \u7372\u5f97\u3002", "author": "Jiahuan Yan et.al.", "authors": "Jiahuan Yan, Zhouyang Hong, Yu Zhao, Yu Tian, Yunxin Liu, Travis Davies, Luhui Hu", "id": "2409.12061v1", "paper_url": "http://arxiv.org/abs/2409.12061v1", "repo": "null"}}