{"2406.17253": {"publish_time": "2024-06-25", "title": "How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?", "paper_summary": "As large language models (LLMs) are widely deployed, targeted editing of\ntheir knowledge has become a critical challenge. Recently, advancements in\nmodel editing techniques, such as Rank-One Model Editing (ROME), have paved the\nway for updating LLMs with new knowledge. However, the efficacy of these\nmethods varies across different types of knowledge. This study investigates the\ncapability of knowledge editing methods to incorporate new knowledge with\nvarying degrees of \"perplexingness\", a term we use to describe the initial\ndifficulty LLMs have in understanding new concepts. We begin by quantifying the\n\"perplexingness\" of target knowledge using pre-edit conditional probabilities,\nand assess the efficacy of edits through post-edit conditional probabilities.\nUtilizing the widely-used CounterFact dataset, we find significant negative\ncorrelations between the \"perplexingness\" of the new knowledge and the edit\nefficacy across all 12 scenarios. To dive deeper into this phenomenon, we\nintroduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym\npairs across diverse categories. Our analysis reveal that more abstract\nconcepts (hypernyms) tend to be more perplexing than their specific\ncounterparts (hyponyms). Further exploration into the influence of knowledge\nhierarchy on editing outcomes indicates that knowledge positioned at higher\nhierarchical levels is more challenging to modify in some scenarios. Our\nresearch highlights a previously overlooked aspect of LLM editing: the variable\nefficacy of editing methods in handling perplexing knowledge. By revealing how\nhierarchical relationships can influence editing outcomes, our findings offer\nnew insights into the challenges of updating LLMs and pave the way for more\nnuanced approaches to model editing in the future.", "paper_summary_zh": "<paragraph>\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5ee3\u6cdb\u90e8\u7f72\uff0c\u6709\u91dd\u5c0d\u6027\u5730\u7de8\u8f2f\u5176\u77e5\u8b58\u5df2\u6210\u70ba\u4e00\u9805\u95dc\u9375\u6311\u6230\u3002\u6700\u8fd1\uff0c\u6a21\u578b\u7de8\u8f2f\u6280\u8853\u7684\u9032\u5c55\uff0c\u4f8b\u5982\u79e9\u4e00\u6a21\u578b\u7de8\u8f2f (ROME)\uff0c\u70ba\u4f7f\u7528\u65b0\u77e5\u8b58\u66f4\u65b0 LLM \u92ea\u5e73\u4e86\u9053\u8def\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u7684\u529f\u6548\u56e0\u4e0d\u540c\u985e\u578b\u7684\u77e5\u8b58\u800c\u7570\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u77e5\u8b58\u7de8\u8f2f\u65b9\u6cd5\u5c07\u5177\u6709\u4e0d\u540c\u7a0b\u5ea6\u300c\u56f0\u60d1\u6027\u300d\u7684\u65b0\u77e5\u8b58\u7d0d\u5165\u5176\u4e2d\u7684\u80fd\u529b\uff0c\u6211\u5011\u4f7f\u7528\u9019\u500b\u8853\u8a9e\u4f86\u63cf\u8ff0 LLM \u5728\u7406\u89e3\u65b0\u6982\u5ff5\u6642\u7684\u521d\u59cb\u56f0\u96e3\u3002\u6211\u5011\u9996\u5148\u4f7f\u7528\u9810\u7de8\u8f2f\u689d\u4ef6\u6a5f\u7387\u5c0d\u76ee\u6a19\u77e5\u8b58\u7684\u300c\u56f0\u60d1\u6027\u300d\u9032\u884c\u91cf\u5316\uff0c\u4e26\u900f\u904e\u5f8c\u7de8\u8f2f\u689d\u4ef6\u6a5f\u7387\u8a55\u4f30\u7de8\u8f2f\u7684\u529f\u6548\u3002\u5229\u7528\u5ee3\u6cdb\u4f7f\u7528\u7684 CounterFact \u8cc7\u6599\u96c6\uff0c\u6211\u5011\u767c\u73fe\u65b0\u77e5\u8b58\u7684\u300c\u56f0\u60d1\u6027\u300d\u8207\u6240\u6709 12 \u7a2e\u60c5\u6cc1\u4e0b\u7684\u7de8\u8f2f\u529f\u6548\u4e4b\u9593\u5b58\u5728\u986f\u8457\u7684\u8ca0\u76f8\u95dc\u3002\u70ba\u4e86\u66f4\u6df1\u5165\u63a2\u8a0e\u9019\u500b\u73fe\u8c61\uff0c\u6211\u5011\u5f15\u5165\u4e86\u65b0\u7684\u8cc7\u6599\u96c6 HierarchyData\uff0c\u5176\u4e2d\u5305\u542b 99 \u500b\u8de8\u985e\u5225\u7684\u4f4e\u968e\u8a5e-\u9ad8\u968e\u8a5e\u5c0d\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u66f4\u62bd\u8c61\u7684\u6982\u5ff5\uff08\u9ad8\u968e\u8a5e\uff09\u5f80\u5f80\u6bd4\u5b83\u5011\u5177\u9ad4\u7684\u5c0d\u61c9\u6982\u5ff5\uff08\u4f4e\u968e\u8a5e\uff09\u66f4\u4ee4\u4eba\u56f0\u60d1\u3002\u9032\u4e00\u6b65\u63a2\u8a0e\u77e5\u8b58\u5c64\u7d1a\u5c0d\u7de8\u8f2f\u7d50\u679c\u7684\u5f71\u97ff\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\uff0c\u4f4d\u65bc\u8f03\u9ad8\u5c64\u7d1a\u7684\u77e5\u8b58\u66f4\u96e3\u4ee5\u4fee\u6539\u3002\u6211\u5011\u7684\u7814\u7a76\u5f37\u8abf\u4e86 LLM \u7de8\u8f2f\u4e2d\u5148\u524d\u88ab\u5ffd\u7565\u7684\u4e00\u500b\u9762\u5411\uff1a\u7de8\u8f2f\u65b9\u6cd5\u5728\u8655\u7406\u4ee4\u4eba\u56f0\u60d1\u7684\u77e5\u8b58\u6642\u529f\u6548\u7684\u53ef\u8b8a\u6027\u3002\u900f\u904e\u63ed\u793a\u5c64\u7d1a\u95dc\u4fc2\u5982\u4f55\u5f71\u97ff\u7de8\u8f2f\u7d50\u679c\uff0c\u6211\u5011\u7684\u767c\u73fe\u70ba\u66f4\u65b0 LLM \u7684\u6311\u6230\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\uff0c\u4e26\u70ba\u672a\u4f86\u66f4\u7d30\u7dfb\u7684\u6a21\u578b\u7de8\u8f2f\u65b9\u6cd5\u92ea\u5e73\u4e86\u9053\u8def\u3002</paragraph>", "author": "Huaizhi Ge et.al.", "authors": "Huaizhi Ge, Frank Rudzicz, Zining Zhu", "id": "2406.17253v1", "paper_url": "http://arxiv.org/abs/2406.17253v1", "repo": "null"}}