{"2412.04703": {"publish_time": "2024-12-06", "title": "Transformers Struggle to Learn to Search", "paper_summary": "Search is an ability foundational in many important tasks, and recent studies\nhave shown that large language models (LLMs) struggle to perform search\nrobustly. It is unknown whether this inability is due to a lack of data,\ninsufficient model parameters, or fundamental limitations of the transformer\narchitecture. In this work, we use the foundational graph connectivity problem\nas a testbed to generate effectively limitless high-coverage data to train\nsmall transformers and test whether they can learn to perform search. We find\nthat, when given the right training distribution, the transformer is able to\nlearn to search.\n  We analyze the algorithm that the transformer has learned through a novel\nmechanistic interpretability technique that enables us to extract the\ncomputation graph from the trained model. We find that for each vertex in the\ninput graph, transformers compute the set of vertices reachable from that\nvertex. Each layer then progressively expands these sets, allowing the model to\nsearch over a number of vertices exponential in the number of layers.\n  However, we find that as the input graph size increases, the transformer has\ngreater difficulty in learning the task. This difficulty is not resolved even\nas the number of parameters is increased, suggesting that increasing model\nscale will not lead to robust search abilities. We also find that performing\nsearch in-context (i.e., chain-of-thought) does not resolve this inability to\nlearn to search on larger graphs.", "paper_summary_zh": "\u641c\u5c0b\u662f\u8a31\u591a\u91cd\u8981\u4efb\u52d9\u4e2d\u7684\u4e00\u9805\u57fa\u790e\u80fd\u529b\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u96e3\u4ee5\u7a69\u5065\u5730\u57f7\u884c\u641c\u5c0b\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u9019\u7a2e\u7121\u80fd\u662f\u6e90\u65bc\u8cc7\u6599\u4e0d\u8db3\u3001\u6a21\u578b\u53c3\u6578\u4e0d\u8db3\uff0c\u9084\u662f Transformer \u67b6\u69cb\u7684\u57fa\u672c\u9650\u5236\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u57fa\u790e\u5716\u5f62\u9023\u901a\u6027\u554f\u984c\u4f5c\u70ba\u6e2c\u8a66\u5e73\u53f0\uff0c\u751f\u6210\u6709\u6548\u7121\u9650\u7684\u9ad8\u8986\u84cb\u7387\u8cc7\u6599\uff0c\u4ee5\u8a13\u7df4\u5c0f\u578b Transformer \u4e26\u6e2c\u8a66\u5b83\u5011\u662f\u5426\u80fd\u5b78\u6703\u57f7\u884c\u641c\u5c0b\u3002\u6211\u5011\u767c\u73fe\uff0c\u7576\u7d66\u4e88\u6b63\u78ba\u7684\u8a13\u7df4\u5206\u4f48\u6642\uff0cTransformer \u80fd\u5920\u5b78\u6703\u641c\u5c0b\u3002\n\u6211\u5011\u900f\u904e\u4e00\u7a2e\u65b0\u7a4e\u7684\u6a5f\u5236\u53ef\u89e3\u91cb\u6027\u6280\u8853\u5206\u6790 Transformer \u5b78\u5230\u7684\u6f14\u7b97\u6cd5\uff0c\u9019\u8b93\u6211\u5011\u80fd\u5920\u5f9e\u8a13\u7df4\u597d\u7684\u6a21\u578b\u4e2d\u63d0\u53d6\u904b\u7b97\u5716\u5f62\u3002\u6211\u5011\u767c\u73fe\uff0c\u5c0d\u65bc\u8f38\u5165\u5716\u5f62\u4e2d\u7684\u6bcf\u500b\u9802\u9ede\uff0cTransformer \u6703\u8a08\u7b97\u5f9e\u8a72\u9802\u9ede\u53ef\u5230\u9054\u7684\u9802\u9ede\u96c6\u5408\u3002\u7136\u5f8c\uff0c\u6bcf\u4e00\u5c64\u90fd\u6703\u9010\u6b65\u64f4\u5145\u9019\u4e9b\u96c6\u5408\uff0c\u8b93\u6a21\u578b\u80fd\u5920\u5728\u8207\u5c64\u6578\u5448\u6307\u6578\u95dc\u4fc2\u7684\u9802\u9ede\u6578\u76ee\u4e0a\u9032\u884c\u641c\u5c0b\u3002\n\u7136\u800c\uff0c\u6211\u5011\u767c\u73fe\uff0c\u96a8\u8457\u8f38\u5165\u5716\u5f62\u5927\u5c0f\u7684\u589e\u52a0\uff0cTransformer \u5728\u5b78\u7fd2\u4efb\u52d9\u6642\u6703\u9047\u5230\u66f4\u5927\u7684\u56f0\u96e3\u3002\u5373\u4f7f\u589e\u52a0\u53c3\u6578\u6578\u91cf\uff0c\u9019\u7a2e\u56f0\u96e3\u4e5f\u4e0d\u6703\u5f97\u5230\u89e3\u6c7a\uff0c\u9019\u8868\u660e\u589e\u52a0\u6a21\u578b\u898f\u6a21\u4e0d\u6703\u5e36\u4f86\u7a69\u5065\u7684\u641c\u5c0b\u80fd\u529b\u3002\u6211\u5011\u9084\u767c\u73fe\uff0c\u5728\u4e0a\u4e0b\u6587\u4e2d\u57f7\u884c\u641c\u5c0b\uff08\u5373\u601d\u8003\u93c8\uff09\u7121\u6cd5\u89e3\u6c7a\u9019\u7a2e\u7121\u6cd5\u5b78\u7fd2\u5728\u8f03\u5927\u5716\u5f62\u4e0a\u641c\u5c0b\u7684\u554f\u984c\u3002", "author": "Abulhair Saparov et.al.", "authors": "Abulhair Saparov, Srushti Pawar, Shreyas Pimpalgaonkar, Nitish Joshi, Richard Yuanzhe Pang, Vishakh Padmakumar, Seyed Mehran Kazemi, Najoung Kim, He He", "id": "2412.04703v1", "paper_url": "http://arxiv.org/abs/2412.04703v1", "repo": "null"}}