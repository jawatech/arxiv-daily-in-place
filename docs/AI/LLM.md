
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-14**|**Quantifying Variance in Evaluation Benchmarks**|Lovish Madaan et.al.|[2406.10229v1](http://arxiv.org/abs/2406.10229v1)|null|
|**2024-06-14**|**VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models**|Chenyu Zhou et.al.|[2406.10228v1](http://arxiv.org/abs/2406.10228v1)|null|
|**2024-06-14**|**VideoGUI: A Benchmark for GUI Automation from Instructional Videos**|Kevin Qinghong Lin et.al.|[2406.10227v1](http://arxiv.org/abs/2406.10227v1)|null|
|**2024-06-14**|**Short Film Dataset (SFD): A Benchmark for Story-Level Video Understanding**|Ridouane Ghermi et.al.|[2406.10221v1](http://arxiv.org/abs/2406.10221v1)|null|
|**2024-06-14**|**Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs**|Rui Yang et.al.|[2406.10216v1](http://arxiv.org/abs/2406.10216v1)|null|
|**2024-06-14**|**DevBench: A multimodal developmental benchmark for language learning**|Alvin Wei Ming Tan et.al.|[2406.10215v1](http://arxiv.org/abs/2406.10215v1)|null|
|**2024-06-14**|**Make It Count: Text-to-Image Generation with an Accurate Number of Objects**|Lital Binyamin et.al.|[2406.10210v1](http://arxiv.org/abs/2406.10210v1)|null|
|**2024-06-14**|**Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs**|Abhimanyu Hans et.al.|[2406.10209v1](http://arxiv.org/abs/2406.10209v1)|[link](https://github.com/ahans30/goldfish-loss)|
|**2024-06-14**|**A Fundamental Trade-off in Aligned Language Models and its Relation to Sampling Adaptors**|Naaman Tan et.al.|[2406.10203v1](http://arxiv.org/abs/2406.10203v1)|null|
|**2024-06-14**|**Crafting Parts for Expressive Object Composition**|Harsh Rangwani et.al.|[2406.10197v1](http://arxiv.org/abs/2406.10197v1)|null|
|**2024-06-14**|**TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners**|Tomas de la Rosa et.al.|[2406.10196v1](http://arxiv.org/abs/2406.10196v1)|null|
|**2024-06-14**|**CHIRON: Rich Character Representations in Long-Form Narratives**|Alexander Gurung et.al.|[2406.10190v1](http://arxiv.org/abs/2406.10190v1)|null|
|**2024-06-14**|**Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors**|Siyuan Chen et.al.|[2406.10181v1](http://arxiv.org/abs/2406.10181v1)|null|
|**2024-06-14**|**Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation**|Mohamad Elzohbi et.al.|[2406.10174v1](http://arxiv.org/abs/2406.10174v1)|null|
|**2024-06-14**|**IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce**|Wenxuan Ding et.al.|[2406.10173v1](http://arxiv.org/abs/2406.10173v1)|[link](https://github.com/hkust-knowcomp/intentionqa)|
|**2024-06-14**|**Datasets for Multilingual Answer Sentence Selection**|Matteo Gabburo et.al.|[2406.10172v1](http://arxiv.org/abs/2406.10172v1)|null|
|**2024-06-14**|**MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers**|Yiwen Chen et.al.|[2406.10163v1](http://arxiv.org/abs/2406.10163v1)|[link](https://github.com/buaacyw/meshanything)|
|**2024-06-14**|**Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models**|Carson Denison et.al.|[2406.10162v1](http://arxiv.org/abs/2406.10162v1)|[link](https://github.com/anthropics/sycophancy-to-subterfuge-paper)|
|**2024-06-14**|**One-pass Multiple Conformer and Foundation Speech Systems Compression and Quantization Using An All-in-one Neural Model**|Zhaoqing Li et.al.|[2406.10160v1](http://arxiv.org/abs/2406.10160v1)|null|
|**2024-06-14**|**RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model**|Hantao Zhou et.al.|[2406.10157v1](http://arxiv.org/abs/2406.10157v1)|null|
|**2024-06-14**|**BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack**|Yuri Kuratov et.al.|[2406.10149v1](http://arxiv.org/abs/2406.10149v1)|null|
|**2024-06-14**|**Improving rule mining via embedding-based link prediction**|N'Dah Jean Kouagou et.al.|[2406.10144v1](http://arxiv.org/abs/2406.10144v1)|[link](https://github.com/jean-kouagou/enhancedrulelearning)|
|**2024-06-14**|**Evaluation of Large Language Models: STEM education and Gender Stereotypes**|Smilla Due et.al.|[2406.10133v1](http://arxiv.org/abs/2406.10133v1)|null|
|**2024-06-14**|**Linear Contextual Bandits with Hybrid Payoff: Revisited**|Nirjhar Das et.al.|[2406.10131v1](http://arxiv.org/abs/2406.10131v1)|[link](https://github.com/nirjhar-das/hypay_bandits)|
|**2024-06-14**|**The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models**|Yan Liu et.al.|[2406.10130v1](http://arxiv.org/abs/2406.10130v1)|null|
|**2024-06-14**|**SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages**|Holy Lovenia et.al.|[2406.10118v1](http://arxiv.org/abs/2406.10118v1)|null|
|**2024-06-14**|**Precipitation Nowcasting Using Physics Informed Discriminator Generative Models**|Junzhe Yin et.al.|[2406.10108v1](http://arxiv.org/abs/2406.10108v1)|null|
|**2024-06-14**|**SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding**|Junwei Luo et.al.|[2406.10100v1](http://arxiv.org/abs/2406.10100v1)|[link](https://github.com/luo-z13/skysensegpt)|
|**2024-06-14**|**Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning**|Jiaqi Li et.al.|[2406.10099v1](http://arxiv.org/abs/2406.10099v1)|null|
|**2024-06-14**|**ECGMamba: Towards Efficient ECG Classification with BiSSM**|Yupeng Qiang et.al.|[2406.10098v1](http://arxiv.org/abs/2406.10098v1)|null|
|**2024-06-14**|**Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation**|Xiaoman Wang et.al.|[2406.10091v1](http://arxiv.org/abs/2406.10091v1)|null|
|**2024-06-14**|**Biomarker based Cancer Classification using an Ensemble with Pre-trained Models**|Chongmin Lee et.al.|[2406.10087v1](http://arxiv.org/abs/2406.10087v1)|null|
|**2024-06-14**|**Discovering influential text using convolutional neural networks**|Megan Ayers et.al.|[2406.10086v1](http://arxiv.org/abs/2406.10086v1)|null|
|**2024-06-14**|**Enhancing Question Answering on Charts Through Effective Pre-training Tasks**|Ashim Gupta et.al.|[2406.10085v1](http://arxiv.org/abs/2406.10085v1)|null|
|**2024-06-14**|**On the Evaluation of Speech Foundation Models for Spoken Language Understanding**|Siddhant Arora et.al.|[2406.10083v1](http://arxiv.org/abs/2406.10083v1)|null|
|**2024-06-14**|**Localizing Events in Videos with Multimodal Queries**|Gengyuan Zhang et.al.|[2406.10079v1](http://arxiv.org/abs/2406.10079v1)|null|
|**2024-06-14**|**Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content**|Rémi Uro et.al.|[2406.10073v1](http://arxiv.org/abs/2406.10073v1)|null|
|**2024-06-14**|**First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models**|Enming Zhang et.al.|[2406.10057v1](http://arxiv.org/abs/2406.10057v1)|null|
|**2024-06-14**|**Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection**|Haoyu Wang et.al.|[2406.10052v1](http://arxiv.org/abs/2406.10052v1)|null|
|**2024-06-14**|**Bridging the Communication Gap: Artificial Agents Learning Sign Language through Imitation**|Federico Tavella et.al.|[2406.10043v1](http://arxiv.org/abs/2406.10043v1)|null|
|**2024-06-14**|**FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain**|Jin Liu et.al.|[2406.10040v1](http://arxiv.org/abs/2406.10040v1)|[link](https://github.com/jens5588/fzi-wim-nli4ct)|
|**2024-06-14**|**Intepretative Deep Learning using Domain Adaptation for Fluorescence Spectroscopy**|Umberto Michelucci et.al.|[2406.10031v1](http://arxiv.org/abs/2406.10031v1)|null|
|**2024-06-14**|**Deep Bayesian Active Learning for Preference Modeling in Large Language Models**|Luckeciano C. Melo et.al.|[2406.10023v1](http://arxiv.org/abs/2406.10023v1)|[link](https://github.com/luckeciano/bal-pm)|
|**2024-06-14**|**Group and Shuffle: Efficient Structured Orthogonal Parametrization**|Mikhail Gorbunov et.al.|[2406.10019v1](http://arxiv.org/abs/2406.10019v1)|null|
|**2024-06-14**|**Beyond Slow Signs in High-fidelity Model Extraction**|Hanna Foerster et.al.|[2406.10011v1](http://arxiv.org/abs/2406.10011v1)|null|
|**2024-06-14**|**Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models**|Manas Jhalani et.al.|[2406.09994v1](http://arxiv.org/abs/2406.09994v1)|null|
|**2024-06-14**|**Details Make a Difference: Object State-Sensitive Neurorobotic Task Planning**|Xiaowen Sun et.al.|[2406.09988v1](http://arxiv.org/abs/2406.09988v1)|[link](https://github.com/xiao-wen-sun/ossa)|
|**2024-06-14**|**Challenges in explaining deep learning models for data with biological variation**|Lenka Tětková et.al.|[2406.09981v1](http://arxiv.org/abs/2406.09981v1)|null|
|**2024-06-14**|**HIRO: Hierarchical Information Retrieval Optimization**|Krish Goel et.al.|[2406.09979v1](http://arxiv.org/abs/2406.09979v1)|[link](https://github.com/krishgoel/hiro)|
|**2024-06-14**|**Disentangling Dialect from Social Bias via Multitask Learning to Improve Fairness**|Maximilian Spliethöver et.al.|[2406.09977v1](http://arxiv.org/abs/2406.09977v1)|null|
|**2024-06-14**|**Robust Model-Based Reinforcement Learning with an Adversarial Auxiliary Model**|Siemen Herremans et.al.|[2406.09976v1](http://arxiv.org/abs/2406.09976v1)|[link](https://github.com/rmbpo-eval/rmbpo-eval)|
|**2024-06-14**|**A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization**|KuanChao Chu et.al.|[2406.09972v1](http://arxiv.org/abs/2406.09972v1)|null|
|**2024-06-14**|**Bag of Lies: Robustness in Continuous Pre-training BERT**|Ine Gevers et.al.|[2406.09967v1](http://arxiv.org/abs/2406.09967v1)|null|
|**2024-06-14**|**Outlier detection in maritime environments using AIS data and deep recurrent architectures**|Constantine Maganaris et.al.|[2406.09966v1](http://arxiv.org/abs/2406.09966v1)|null|
|**2024-06-14**|**ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation**|Chufan Shi et.al.|[2406.09961v1](http://arxiv.org/abs/2406.09961v1)|[link](https://github.com/chartmimic/chartmimic)|
|**2024-06-14**|**DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning**|Zeyu Gao et.al.|[2406.09953v1](http://arxiv.org/abs/2406.09953v1)|null|
|**2024-06-14**|**BiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval**|Imanol Miranda et.al.|[2406.09952v1](http://arxiv.org/abs/2406.09952v1)|[link](https://github.com/imirandam/bivlc)|
|**2024-06-14**|**An efficient text augmentation approach for contextualized Mandarin speech recognition**|Naijun Zheng et.al.|[2406.09950v1](http://arxiv.org/abs/2406.09950v1)|null|
|**2024-06-14**|**Neural Concept Binder**|Wolfgang Stammer et.al.|[2406.09949v1](http://arxiv.org/abs/2406.09949v1)|[link](https://github.com/ml-research/neuralconceptbinder)|
|**2024-06-14**|**BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages**|Junho Myung et.al.|[2406.09948v1](http://arxiv.org/abs/2406.09948v1)|[link](https://github.com/nlee0212/blend)|
|**2024-06-14**|**Experiments in News Bias Detection with Pre-Trained Neural Transformers**|Tim Menzner et.al.|[2406.09938v1](http://arxiv.org/abs/2406.09938v1)|null|
|**2024-06-14**|**What Does it Take to Generalize SER Model Across Datasets? A Comprehensive Benchmark**|Adham Ibrahim et.al.|[2406.09933v1](http://arxiv.org/abs/2406.09933v1)|null|
|**2024-06-14**|**Personalized Speech Enhancement Without a Separate Speaker Embedding Model**|Tanel Pärnamaa et.al.|[2406.09928v1](http://arxiv.org/abs/2406.09928v1)|null|
|**2024-06-14**|**CliBench: Multifaceted Evaluation of Large Language Models in Clinical Decisions on Diagnoses, Procedures, Lab Tests Orders and Prescriptions**|Mingyu Derek Ma et.al.|[2406.09923v1](http://arxiv.org/abs/2406.09923v1)|[link](https://github.com/clibench/clibench)|
|**2024-06-14**|**Knowledge Editing in Language Models via Adapted Direct Preference Optimization**|Amit Rozner et.al.|[2406.09920v1](http://arxiv.org/abs/2406.09920v1)|null|
|**2024-06-14**|**GEB-1.3B: Open Lightweight Large Language Model**|Jie Wu et.al.|[2406.09900v1](http://arxiv.org/abs/2406.09900v1)|null|
|**2024-06-14**|**Learning Solution-Aware Transformers for Efficiently Solving Quadratic Assignment Problem**|Zhentao Tan et.al.|[2406.09899v1](http://arxiv.org/abs/2406.09899v1)|[link](https://github.com/pkutan/sawt)|
|**2024-06-14**|**3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position Encoding**|Xindian Ma et.al.|[2406.09897v1](http://arxiv.org/abs/2406.09897v1)|null|
|**2024-06-14**|**Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming**|Victor-Alexandru Pădurean et.al.|[2406.09891v1](http://arxiv.org/abs/2406.09891v1)|null|
|**2024-06-14**|**A Unified Data Augmentation Framework for Low-Resource Multi-Domain Dialogue Generation**|Yongkang Liu et.al.|[2406.09881v1](http://arxiv.org/abs/2406.09881v1)|null|
|**2024-06-14**|**Federated Learning with Flexible Architectures**|Jong-Ik Park et.al.|[2406.09877v1](http://arxiv.org/abs/2406.09877v1)|null|
|**2024-06-14**|**Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition**|Yicong Jiang et.al.|[2406.09873v1](http://arxiv.org/abs/2406.09873v1)|null|
|**2024-06-14**|**LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal Data**|Grigor Bezirganyan et.al.|[2406.09864v1](http://arxiv.org/abs/2406.09864v1)|[link](https://github.com/bezirganyan/luma)|
|**2024-06-14**|**Dataset Condensation with Latent Quantile Matching**|Wei Wei et.al.|[2406.09860v1](http://arxiv.org/abs/2406.09860v1)|null|
|**2024-06-14**|**On the Encoding of Gender in Transformer-based ASR Representations**|Aravind Krishnan et.al.|[2406.09855v1](http://arxiv.org/abs/2406.09855v1)|null|
|**2024-06-14**|**Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting**|Muhammad Yeza Baihaqi et.al.|[2406.09839v1](http://arxiv.org/abs/2406.09839v1)|null|
|**2024-06-14**|**Vision-Language Models Meet Meteorology: Developing Models for Extreme Weather Events Detection with Heatmaps**|Jian Chen et.al.|[2406.09838v1](http://arxiv.org/abs/2406.09838v1)|[link](https://github.com/AlexJJJChen/Climate-Zoo)|
|**2024-06-14**|**SHMamba: Structured Hyperbolic State Space Model for Audio-Visual Question Answering**|Zhe Yang et.al.|[2406.09833v1](http://arxiv.org/abs/2406.09833v1)|null|
|**2024-06-14**|**Federated Learning driven Large Language Models for Swarm Intelligence: A Survey**|Youyang Qu et.al.|[2406.09831v1](http://arxiv.org/abs/2406.09831v1)|null|
|**2024-06-14**|**HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning**|Heejun Lee et.al.|[2406.09827v1](http://arxiv.org/abs/2406.09827v1)|null|
|**2024-06-14**|**From Manifestations to Cognitive Architectures: a Scalable Framework**|Alfredo Ibias et.al.|[2406.09823v1](http://arxiv.org/abs/2406.09823v1)|null|
|**2024-06-14**|**Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments**|Zhenrui Yue et.al.|[2406.09815v1](http://arxiv.org/abs/2406.09815v1)|null|
|**2024-06-14**|**Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity**|Bowen Zhang et.al.|[2406.09790v1](http://arxiv.org/abs/2406.09790v1)|null|
|**2024-06-14**|**Evolving Self-Assembling Neural Networks: From Spontaneous Activity to Experience-Dependent Learning**|Erwan Plantec et.al.|[2406.09787v1](http://arxiv.org/abs/2406.09787v1)|null|
|**2024-06-14**|**OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst**|Jingtao Cao et.al.|[2406.09779v1](http://arxiv.org/abs/2406.09779v1)|null|
|**2024-06-14**|**Research on Edge Detection of LiDAR Images Based on Artificial Intelligence Technology**|Haowei Yang et.al.|[2406.09773v1](http://arxiv.org/abs/2406.09773v1)|null|
|**2024-06-14**|**Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model Fusion**|Anke Tang et.al.|[2406.09770v1](http://arxiv.org/abs/2406.09770v1)|[link](https://github.com/tanganke/pareto_set_learning)|
|**2024-06-14**|**Bayesian Conditioned Diffusion Models for Inverse Problems**|Alper Güngör et.al.|[2406.09768v1](http://arxiv.org/abs/2406.09768v1)|null|
|**2024-06-14**|**Application of Natural Language Processing in Financial Risk Detection**|Liyang Wang et.al.|[2406.09765v1](http://arxiv.org/abs/2406.09765v1)|null|
|**2024-06-14**|**Bootstrapping Language Models with DPO Implicit Rewards**|Changyu Chen et.al.|[2406.09760v1](http://arxiv.org/abs/2406.09760v1)|[link](https://github.com/sail-sg/dice)|
|**2024-06-14**|**Mix Q-learning for Lane Changing: A Collaborative Decision-Making Method in Multi-Agent Deep Reinforcement Learning**|Xiaojun Bi et.al.|[2406.09755v1](http://arxiv.org/abs/2406.09755v1)|null|
|**2024-06-14**|**ControlVAR: Exploring Controllable Visual Autoregressive Modeling**|Xiang Li et.al.|[2406.09750v1](http://arxiv.org/abs/2406.09750v1)|null|
|**2024-06-14**|**When Will Gradient Regularization Be Harmful?**|Yang Zhao et.al.|[2406.09723v1](http://arxiv.org/abs/2406.09723v1)|[link](https://github.com/zhaoyang-0204/gnp)|
|**2024-06-14**|**Self-Knowledge Distillation for Learning Ambiguity**|Hancheol Park et.al.|[2406.09719v1](http://arxiv.org/abs/2406.09719v1)|null|
|**2024-06-14**|**UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages**|Trinh Pham et.al.|[2406.09717v1](http://arxiv.org/abs/2406.09717v1)|[link](https://github.com/TokisakiKurumi2001/UniBridge)|
|**2024-06-14**|**Fine-Grained Urban Flow Inference with Multi-scale Representation Learning**|Shilu Yuan et.al.|[2406.09710v1](http://arxiv.org/abs/2406.09710v1)|null|
|**2024-06-14**|**Detecting Response Generation Not Requiring Factual Judgment**|Ryohei Kamei et.al.|[2406.09702v1](http://arxiv.org/abs/2406.09702v1)|null|
|**2024-06-14**|**FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation**|Zijian Feng et.al.|[2406.09688v1](http://arxiv.org/abs/2406.09688v1)|null|
|**2024-06-14**|**Explainable AI for Comparative Analysis of Intrusion Detection Models**|Pap M. Corea et.al.|[2406.09684v1](http://arxiv.org/abs/2406.09684v1)|[link](https://github.com/pcwhy/xml-intrusiondetection)|
|**2024-06-14**|**Optimizing Byte-level Representation for End-to-end ASR**|Roger Hsiao et.al.|[2406.09676v1](http://arxiv.org/abs/2406.09676v1)|null|

#### Abstracts
##### **Quantifying Variance in Evaluation Benchmarks**
2406.10229v1 by Lovish Madaan, Aaditya K. Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang, Dieuwke Hupkes

Evaluation benchmarks are the cornerstone of measuring capabilities of large
language models (LLMs), as well as driving progress in said capabilities.
Originally designed to make claims about capabilities (or lack thereof) in
fully pretrained models, evaluation benchmarks are now also extensively used to
decide between various training choices. Despite this widespread usage, we
rarely quantify the variance in our evaluation benchmarks, which dictates
whether differences in performance are meaningful. Here, we define and measure
a range of metrics geared towards measuring variance in evaluation benchmarks,
including seed variance across initialisations, and monotonicity during
training. By studying a large number of models -- both openly available and
pretrained from scratch -- we provide empirical estimates for a variety of
variance metrics, with considerations and recommendations for practitioners. We
also evaluate the utility and tradeoffs of continuous versus discrete
performance measures and explore options for better understanding and reducing
this variance. We find that simple changes, such as framing choice tasks (like
MMLU) as completion tasks, can often reduce variance for smaller scale
($\sim$7B) models, while more involved methods inspired from human testing
literature (such as item analysis and item response theory) struggle to
meaningfully reduce variance. Overall, our work provides insights into variance
in evaluation benchmarks, suggests LM-specific techniques to reduce variance,
and more generally encourages practitioners to carefully factor in variance
when comparing models.

摘要：評估基準是衡量大型語言模型 (LLM) 能力的基石，同時也能促進此類能力的進步。評估基準最初設計用於提出關於完全預訓練模型的能力（或缺乏能力）的說法，現在也廣泛用於在各種訓練選項之間做出決定。儘管如此廣泛的使用，我們很少量化評估基準中的差異，這決定了效能差異是否有意義。在這裡，我們定義和衡量一系列指標，這些指標旨在衡量評估基準中的差異，包括初始化時的種子差異和訓練期間的單調性。透過研究大量模型（包括公開可用的模型和從頭開始預訓練的模型），我們針對各種差異指標提供經驗估計，並提供給實務工作者的考量和建議。我們也評估連續與離散效能測量的方法及其權衡取捨，並探討更了解和減少此差異的選項。我們發現，一些簡單的變更，例如將選擇任務（例如 MMLU）設定為完成任務，通常可以減少較小規模（$\sim$7B）模型的差異，而受到人類測試文獻啟發的更複雜方法（例如項目分析和項目反應理論）則難以有效減少差異。總體而言，我們的研究提供了對評估基準中差異的見解，提出了減少差異的特定於 LM 的技術，並更普遍地鼓勵實務工作者在比較模型時仔細考量差異。

##### **VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models**
2406.10228v1 by Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, Xiawu Zheng, Xing Sun, Rongrong Ji

The swift progress of Multi-modal Large Models (MLLMs) has showcased their
impressive ability to tackle tasks blending vision and language. Yet, most
current models and benchmarks cater to scenarios with a narrow scope of visual
and textual contexts. These models often fall short when faced with complex
comprehension tasks, which involve navigating through a plethora of irrelevant
and potentially misleading information in both text and image forms. To bridge
this gap, we introduce a new, more demanding task known as Interleaved
Image-Text Comprehension (IITC). This task challenges models to discern and
disregard superfluous elements in both images and text to accurately answer
questions and to follow intricate instructions to pinpoint the relevant image.
In support of this task, we further craft a new VEGA dataset, tailored for the
IITC task on scientific content, and devised a subtask, Image-Text Association
(ITA), to refine image-text correlation skills. Our evaluation of four leading
closed-source models, as well as various open-source models using VEGA,
underscores the rigorous nature of IITC. Even the most advanced models, such as
Gemini-1.5-pro and GPT4V, only achieved modest success. By employing a
multi-task, multi-scale post-training strategy, we have set a robust baseline
for MLLMs on the IITC task, attaining an $85.8\%$ accuracy rate in image
association and a $0.508$ Rouge score. These results validate the effectiveness
of our dataset in improving MLLMs capabilities for nuanced image-text
comprehension.

摘要：多模態大型模型 (MMLM) 的快速進展展示了它們在處理結合視覺和語言任務上的驚人能力。然而，大多數目前的模型和基準都迎合了視覺和文本語境範圍狹窄的場景。這些模型在面對複雜的理解任務時常常表現不佳，這涉及在文本和圖像形式中瀏覽大量無關且可能具有誤導性的資訊。為了彌合這一差距，我們引入了一項新的、更具挑戰性的任務，稱為交錯圖像文本理解 (IITC)。此任務挑戰模型辨別和忽略圖像和文本中的多餘元素，以準確回答問題並遵循複雜的指示來精確指出相關圖像。為了支持此任務，我們進一步製作了一個新的 VEGA 資料集，專門針對科學內容的 IITC 任務，並設計了一個子任務，圖像文本關聯 (ITA)，以提升圖像文本關聯技能。我們對四個領先的閉源模型以及使用 VEGA 的各種開源模型的評估，突顯了 IITC 的嚴謹性。即使是最先進的模型，例如 Gemini-1.5-pro 和 GPT4V，也只取得了適度的成功。通過採用多任務、多尺度的後訓練策略，我們為 MLLM 在 IITC 任務上設定了一個穩健的基線，在圖像關聯中達到 85.8% 的準確率和 0.508 的 Rouge 分數。這些結果驗證了我們的資料集在提升 MLLM 對細微差別的圖像文本理解能力方面的有效性。

##### **VideoGUI: A Benchmark for GUI Automation from Instructional Videos**
2406.10227v1 by Kevin Qinghong Lin, Linjie Li, Difei Gao, Qinchen WU, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, Mike Zheng Shou

Graphical User Interface (GUI) automation holds significant promise for
enhancing human productivity by assisting with computer tasks. Existing task
formulations primarily focus on simple tasks that can be specified by a single,
language-only instruction, such as "Insert a new slide." In this work, we
introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI
assistants on visual-centric GUI tasks. Sourced from high-quality web
instructional videos, our benchmark focuses on tasks involving professional and
novel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complex
activities (e.g., video editing). VideoGUI evaluates GUI assistants through a
hierarchical process, allowing for identification of the specific levels at
which they may fail: (i) high-level planning: reconstruct procedural subtasks
from visual conditions without language descriptions; (ii) middle-level
planning: generate sequences of precise action narrations based on visual state
(i.e., screenshot) and goals; (iii) atomic action execution: perform specific
actions such as accurately clicking designated elements. For each level, we
design evaluation metrics across individual dimensions to provide clear
signals, such as individual performance in clicking, dragging, typing, and
scrolling for atomic action execution. Our evaluation on VideoGUI reveals that
even the SoTA large multimodal model GPT4o performs poorly on visual-centric
GUI tasks, especially for high-level planning.

摘要：圖形使用者介面 (GUI) 自動化對於透過協助電腦工作任務來提升人類生產力而言，具有顯著的遠景。現有的任務表述主要專注於可以由單一、僅語言的指令指定的簡單任務，例如「插入新投影片」。在這項工作中，我們介紹 VideoGUI，一個新穎的多模態基準，旨在評估 GUI 助理在視覺為中心的 GUI 任務上的表現。我們的基準源自高品質的網路教學影片，專注於涉及專業且新穎軟體（例如 Adobe Photoshop 或 Stable Diffusion WebUI）和複雜活動（例如影片編輯）的任務。VideoGUI 透過分層流程評估 GUI 助理，讓系統得以識別其可能失敗的特定層級：(i) 高階規劃：在沒有語言描述的情況下，從視覺條件中重建程序子任務；(ii) 中階規劃：根據視覺狀態（即螢幕截圖）和目標產生精確動作敘述的序列；(iii) 原子動作執行：執行特定動作，例如準確點擊指定的元素。對於每個層級，我們在個別面向設計評估指標，以提供明確的訊號，例如在原子動作執行中點擊、拖曳、輸入和捲動的個別表現。我們在 VideoGUI 上的評估顯示，即使是 SoTA 大型多模態模型 GPT4o 在視覺為中心的 GUI 任務上表現不佳，特別是在高階規劃方面。

##### **Short Film Dataset (SFD): A Benchmark for Story-Level Video Understanding**
2406.10221v1 by Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, Ivan Laptev

Recent advances in vision-language models have significantly propelled video
understanding. Existing datasets and tasks, however, have notable limitations.
Most datasets are confined to short videos with limited events and narrow
narratives. For example, datasets with instructional and egocentric videos
often document the activities of one person in a single scene. Although some
movie datasets offer richer content, they are often limited to short-term
tasks, lack publicly available videos and frequently encounter data leakage
given the use of movie forums and other resources in LLM training. To address
the above limitations, we propose the Short Film Dataset (SFD) with 1,078
publicly available amateur movies, a wide variety of genres and minimal data
leakage issues. SFD offers long-term story-oriented video tasks in the form of
multiple-choice and open-ended question answering. Our extensive experiments
emphasize the need for long-term reasoning to solve SFD tasks. Notably, we find
strong signals in movie transcripts leading to the on-par performance of people
and LLMs. We also show significantly lower performance of current models
compared to people when using vision data alone.

摘要：近期在視覺語言模型上的進展顯著推動了影片理解。然而，現有的資料集和任務有顯著的限制。大多數資料集僅限於事件有限且敘述狹隘的短影片。例如，包含教學和自我中心影片的資料集通常記錄單一場景中一個人的活動。儘管一些電影資料集提供了更豐富的內容，但它們通常僅限於短期任務，缺乏公開可用的影片，並且由於在 LLM 訓練中使用電影論壇和其他資源，因此經常會遇到資料外洩。為了解決上述限制，我們提出了短片資料集 (SFD)，其中包含 1,078 部公開可用的業餘電影、種類繁多且資料外洩問題最小。SFD 以多選題和開放式問答的形式提供長期的故事導向影片任務。我們廣泛的實驗強調了解決 SFD 任務需要長期推理。值得注意的是，我們在電影腳本中找到了強烈的訊號，這導致了人類和 LLM 的表現相當。我們還表明，與僅使用視覺資料相比，當前模型的表現明顯低於人類。

##### **Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs**
2406.10216v1 by Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, Tong Zhang

Reward models trained on human preference data have been proven to be
effective for aligning Large Language Models (LLMs) with human intent within
the reinforcement learning from human feedback (RLHF) framework. However, the
generalization capabilities of current reward models to unseen prompts and
responses are limited. This limitation can lead to an unexpected phenomenon
known as reward over-optimization, where excessive optimization of rewards
results in a decline in actual performance. While previous research has
advocated for constraining policy optimization, our study proposes a novel
approach to enhance the reward model's generalization ability against
distribution shifts by regularizing the hidden states. Specifically, we retain
the base model's language model head and incorporate a suite of text-generation
losses to preserve the hidden states' text generation capabilities, while
concurrently learning a reward head behind the same hidden states. Our
experimental results demonstrate that the introduced regularization technique
markedly improves the accuracy of learned reward models across a variety of
out-of-distribution (OOD) tasks and effectively alleviate the over-optimization
issue in RLHF, offering a more reliable and robust preference learning
paradigm.

摘要：獎勵模型經過人類偏好數據訓練，已證明可有效將大型語言模型 (LLM) 與人類意圖結合在人類回饋強化學習 (RLHF) 架構中。然而，當前獎勵模型對未見提示和回應的概化能力有限。這種限制可能導致一種稱為獎勵過度最佳化的現象，其中過度最佳化獎勵會導致實際效能下降。雖然先前的研究主張約束政策最佳化，但我們的研究提出了一種新穎的方法，透過規範隱藏狀態來增強獎勵模型對抗分佈轉移的概化能力。具體來說，我們保留基礎模型的語言模型頭，並結合一套文字生成損失，以保留隱藏狀態的文字生成能力，同時在相同的隱藏狀態後面學習獎勵頭。我們的實驗結果表明，引入的正則化技術顯著提高了各種分佈外 (OOD) 任務中學習獎勵模型的準確性，並有效緩解了 RLHF 中的過度最佳化問題，提供了一個更可靠且強大的偏好學習範例。

##### **DevBench: A multimodal developmental benchmark for language learning**
2406.10215v1 by Alvin Wei Ming Tan, Sunny Yu, Bria Long, Wanjing Anya Ma, Tonya Murray, Rebecca D. Silverman, Jason D. Yeatman, Michael C. Frank

How (dis)similar are the learning trajectories of vision-language models and
children? Recent modeling work has attempted to understand the gap between
models' and humans' data efficiency by constructing models trained on less
data, especially multimodal naturalistic data. However, such models are often
evaluated on adult-level benchmarks, with limited breadth in language abilities
tested, and without direct comparison to behavioral data. We introduce
DevBench, a multimodal benchmark comprising seven language evaluation tasks
spanning the domains of lexical, syntactic, and semantic ability, with
behavioral data from both children and adults. We evaluate a set of
vision-language models on these tasks, comparing models and humans not only on
accuracy but on their response patterns. Across tasks, models exhibit variation
in their closeness to human response patterns, and models that perform better
on a task also more closely resemble human behavioral responses. We also
examine the developmental trajectory of OpenCLIP over training, finding that
greater training results in closer approximations to adult response patterns.
DevBench thus provides a benchmark for comparing models to human language
development. These comparisons highlight ways in which model and human language
learning processes diverge, providing insight into entry points for improving
language models.

摘要：視覺語言模型和兒童的學習軌跡有多（不）相似？最近的建模工作已嘗試透過建立在較少資料上訓練的模型來了解模型與人類資料效率之間的差距，尤其是多模態自然資料。然而，這些模型通常在成人層級的基準上進行評估，語言能力的廣度有限，且未與行為資料直接比較。我們引入了 DevBench，這是一個多模態基準，包含七項語言評量任務，涵蓋詞彙、句法和語義能力的領域，並包含來自兒童和成人的行為資料。我們在這些任務上評估了一組視覺語言模型，不僅在準確度上比較模型和人類，也在他們的反應模式上進行比較。在各項任務中，模型在接近人類反應模式方面表現出差異，在任務中表現較好的模型也更接近人類行為反應。我們還檢視了 OpenCLIP 在訓練過程中的發展軌跡，發現訓練越多，更接近成人的反應模式。因此，DevBench 提供了一個基準來比較模型與人類語言發展。這些比較突出了模型和人類語言學習過程分歧的方式，提供了改善語言模型的切入點。

##### **Make It Count: Text-to-Image Generation with an Accurate Number of Objects**
2406.10210v1 by Lital Binyamin, Yoad Tewel, Hilit Segev, Eran Hirsch, Royi Rassin, Gal Chechik

Despite the unprecedented success of text-to-image diffusion models,
controlling the number of depicted objects using text is surprisingly hard.
This is important for various applications from technical documents, to
children's books to illustrating cooking recipes. Generating object-correct
counts is fundamentally challenging because the generative model needs to keep
a sense of separate identity for every instance of the object, even if several
objects look identical or overlap, and then carry out a global computation
implicitly during generation. It is still unknown if such representations
exist. To address count-correct generation, we first identify features within
the diffusion model that can carry the object identity information. We then use
them to separate and count instances of objects during the denoising process
and detect over-generation and under-generation. We fix the latter by training
a model that predicts both the shape and location of a missing object, based on
the layout of existing ones, and show how it can be used to guide denoising
with correct object count. Our approach, CountGen, does not depend on external
source to determine object layout, but rather uses the prior from the diffusion
model itself, creating prompt-dependent and seed-dependent layouts. Evaluated
on two benchmark datasets, we find that CountGen strongly outperforms the
count-accuracy of existing baselines.

摘要：儘管文字轉圖片擴散模型取得了前所未有的成功，但使用文字控制描繪物體的數量卻意外困難。這對於各種應用程式來說很重要，從技術文件、兒童讀物到烹飪食譜說明。生成物體正確數量在根本上具有挑戰性，因為生成模型需要為物體的每個實例保留獨立識別感，即使多個物體看起來相同或重疊，然後在生成過程中隱式執行全局計算。目前尚不清楚是否存在此類表徵。為了解決數量正確生成的問題，我們首先識別擴散模型中可以攜帶物體識別資訊的特徵。然後，我們在去噪過程中使用它們來分離和計算物體實例，並偵測過度生成和不足生成。我們透過訓練一個模型來修正後者，該模型根據現有物體的佈局預測遺失物體的形狀和位置，並展示如何使用它來引導具有正確物體數量的去噪。我們的 CountGen 方法不依賴於外部來源來確定物體佈局，而是使用擴散模型本身的先驗，從而建立提示依賴和種子依賴的佈局。在兩個基準資料集上進行評估，我們發現 CountGen 在計數準確度方面明顯優於現有基準。

##### **Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs**
2406.10209v1 by Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein

Large language models can memorize and repeat their training data, causing
privacy and copyright risks. To mitigate memorization, we introduce a subtle
modification to the next-token training objective that we call the goldfish
loss. During training, a randomly sampled subset of tokens are excluded from
the loss computation. These dropped tokens are not memorized by the model,
which prevents verbatim reproduction of a complete chain of tokens from the
training set. We run extensive experiments training billion-scale Llama-2
models, both pre-trained and trained from scratch, and demonstrate significant
reductions in extractable memorization with little to no impact on downstream
benchmarks.

摘要：大型語言模型可以記憶和重複它們的訓練資料，導致隱私和版權風險。為了減輕記憶，我們對下一個代幣訓練目標進行了微妙的修改，我們稱之為金魚損失。在訓練期間，隨機抽取的代幣子集會從損失計算中排除。這些被刪除的代幣不會被模型記憶，這會防止從訓練集中逐字複製完整的代幣鏈。我們運行廣泛的實驗訓練十億規模的 Llama-2 模型，包括預訓練和從頭開始訓練，並證明可提取的記憶力顯著減少，對下游基準幾乎沒有影響。

##### **A Fundamental Trade-off in Aligned Language Models and its Relation to Sampling Adaptors**
2406.10203v1 by Naaman Tan, Josef Valvoda, Anej Svete, Tianyu Liu, Yanxia Qin, Kan Min-Yen, Ryan Cotterell

The relationship between the quality of a string and its probability
$p(\boldsymbol{y})$ under a language model has been influential in the
development of techniques to build good text generation systems. For example,
several decoding algorithms have been motivated to manipulate
$p(\boldsymbol{y})$ to produce higher-quality text. In this work, we examine
the probability--quality relationship in language models explicitly aligned to
human preferences, e.g., through Reinforcement Learning through Human Feedback
(RLHF). We find that, given a general language model and its aligned version,
for corpora sampled from an aligned language model, there exists a trade-off
between the average reward and average log-likelihood of the strings under the
general language model. We provide a formal treatment of this issue and
demonstrate how a choice of sampling adaptor allows for a selection of how much
likelihood we exchange for the reward.

摘要：語言模型中字串品質與其機率 $p(\boldsymbol{y})$ 之間的關係，對於建構良好文字產生系統的技術發展具有影響力。例如，已經激勵了多種解碼演算法來操作 $p(\boldsymbol{y})$ 以產生更高品質的文字。在這項工作中，我們探討與人類偏好明確對齊的語言模型中的機率品質關係，例如透過人類回饋強化學習 (RLHF)。我們發現，給定一般語言模型及其對齊版本，對於從對齊語言模型取樣的語料庫，在一般語言模型下字串的平均獎勵與平均對數似然度之間存在權衡取捨。我們對這個問題提供正式處理，並展示取樣適配器的選擇如何允許選擇我們為獎勵交換多少似然度。

##### **Crafting Parts for Expressive Object Composition**
2406.10197v1 by Harsh Rangwani, Aishwarya Agarwal, Kuldeep Kulkarni, R. Venkatesh Babu, Srikrishna Karanam

Text-to-image generation from large generative models like Stable Diffusion,
DALLE-2, etc., have become a common base for various tasks due to their
superior quality and extensive knowledge bases. As image composition and
generation are creative processes the artists need control over various parts
of the images being generated. We find that just adding details about parts in
the base text prompt either leads to an entirely different image (e.g.,
missing/incorrect identity) or the extra part details simply being ignored. To
mitigate these issues, we introduce PartCraft, which enables image generation
based on fine-grained part-level details specified for objects in the base text
prompt. This allows more control for artists and enables novel object
compositions by combining distinctive object parts. PartCraft first localizes
object parts by denoising the object region from a specific diffusion process.
This enables each part token to be localized to the right object region. After
obtaining part masks, we run a localized diffusion process in each of the part
regions based on fine-grained part descriptions and combine them to produce the
final image. All the stages of PartCraft are based on repurposing a pre-trained
diffusion model, which enables it to generalize across various domains without
training. We demonstrate the effectiveness of part-level control provided by
PartCraft qualitatively through visual examples and quantitatively in
comparison to the contemporary baselines.

摘要：<paragraph>像 Stable Diffusion、DALLE-2 等大型生成模型的文字轉圖像生成，由於其優異的品質和廣泛的知識庫，已成為各種任務的共同基礎。由於影像合成和生成是創意的過程，藝術家需要控制所生成影像的各個部分。我們發現，僅在基本文字提示中加入有關部分的細節，就會導致產生完全不同的影像（例如，遺失/錯誤的身份）或額外的部分細節被忽略。為了減輕這些問題，我們引入了 PartCraft，它可以根據基本文字提示中指定物件的細緻部分層級細節來產生影像。這讓藝術家可以更靈活地控制，並透過結合獨特的物件部分來產生新穎的物件組合。PartCraft 首先透過從特定擴散過程中對物件區域進行去噪來定位物件部分。這讓每個部分標記都能定位到正確的物件區域。取得部分遮罩後，我們在每個部分區域中執行局部擴散程序，根據細緻的部分描述進行處理，並將它們組合起來產生最終影像。PartCraft 的所有階段都是基於重新利用預先訓練好的擴散模型，讓它可以在各種領域中概化，而不需要訓練。我們透過視覺範例定性地展示了 PartCraft 提供的部分層級控制的有效性，並與當代基準進行定量比較。</paragraph>

##### **TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners**
2406.10196v1 by Tomas de la Rosa, Sriram Gopalakrishnan, Alberto Pozanco, Zhen Zeng, Daniel Borrajo

Travel planning is a complex task that involves generating a sequence of
actions related to visiting places subject to constraints and maximizing some
user satisfaction criteria. Traditional approaches rely on problem formulation
in a given formal language, extracting relevant travel information from web
sources, and use an adequate problem solver to generate a valid solution. As an
alternative, recent Large Language Model (LLM) based approaches directly output
plans from user requests using language. Although LLMs possess extensive travel
domain knowledge and provide high-level information like points of interest and
potential routes, current state-of-the-art models often generate plans that
lack coherence, fail to satisfy constraints fully, and do not guarantee the
generation of high-quality solutions. We propose TRIP-PAL, a hybrid method that
combines the strengths of LLMs and automated planners, where (i) LLMs get and
translate travel information and user information into data structures that can
be fed into planners; and (ii) automated planners generate travel plans that
guarantee constraint satisfaction and optimize for users' utility. Our
experiments across various travel scenarios show that TRIP-PAL outperforms an
LLM when generating travel plans.

摘要：旅遊規劃是一項複雜的任務，它涉及到產生一系列與參觀受約束地點相關的動作，並最大化某些使用者滿意度標準。傳統方法依賴於在既定的形式語言中制定問題、從網路來源中提取相關旅遊資訊，並使用適當的問題解決器來產生一個有效的解決方案。作為一種替代方案，最近的大語言模型 (LLM) 基於直接從使用者要求中使用語言輸出計畫的方法。儘管 LLM 擁有廣泛的旅遊領域知識，並提供諸如景點和潛在路線等高級資訊，但目前的最新模型通常會產生缺乏連貫性、無法完全滿足約束條件且無法保證產生高品質解決方案的計畫。我們提出 TRIP-PAL，這是一種結合 LLM 和自動化計畫員優勢的混合方法，其中 (i) LLM 取得並將旅遊資訊和使用者資訊轉換成可以提供給計畫員的資料結構；以及 (ii) 自動化計畫員產生旅遊計畫，以保證約束條件得到滿足，並針對使用者的效用進行最佳化。我們在各種旅遊情境中進行的實驗顯示，TRIP-PAL 在產生旅遊計畫時優於 LLM。

##### **CHIRON: Rich Character Representations in Long-Form Narratives**
2406.10190v1 by Alexander Gurung, Mirella Lapata

Characters are integral to long-form narratives, but are poorly understood by
existing story analysis and generation systems. While prior work has simplified
characters via graph-based methods and brief character descriptions, we aim to
better tackle the problem of representing complex characters by taking
inspiration from advice given to professional writers. We propose CHIRON, a new
`character sheet' based representation that organizes and filters textual
information about characters. We construct CHIRON sheets in two steps: a
Generation Module that prompts an LLM for character information via
question-answering and a Validation Module that uses automated reasoning and a
domain-specific entailment model to eliminate false facts about a character. We
validate CHIRON via the downstream task of masked-character prediction, where
our experiments show CHIRON is better and more flexible than comparable
summary-based baselines. We also show that metrics derived from CHIRON can be
used to automatically infer character-centricity in stories, and that these
metrics align with human judgments.

摘要：角色對於長篇敘事來說不可或缺，但現有的故事分析和生成系統卻對角色了解不足。雖然先前的研究已透過基於圖表的各種方法和簡短的角色描述簡化了角色，但我們希望透過參考提供給專業作家的建議，來更好地解決複雜角色的呈現問題。我們提出 CHIRON，這是一種新的「角色表」型態的呈現方式，用於整理和過濾關於角色的文字資訊。我們透過兩個步驟建構 CHIRON 表：一個產生模組，透過問答提示 LLM 提供角色資訊，以及一個驗證模組，使用自動化推理和領域特定蘊涵模型來消除關於角色的錯誤事實。我們透過遮蔽角色預測的下游任務驗證 CHIRON，我們的實驗顯示，CHIRON 比其他基於摘要的基準更好且更靈活。我們也顯示，從 CHIRON 衍生的指標可用於自動推論故事中的角色中心性，而且這些指標與人類判斷一致。

##### **Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors**
2406.10181v1 by Siyuan Chen, Zelong Guan, Yudong Liu, Phillip B. Gibbons

Fine-tuning large language models (LLMs) requires significant memory, often
exceeding the capacity of a single GPU. A common solution to this memory
challenge is offloading compute and data from the GPU to the CPU. However, this
approach is hampered by the limited bandwidth of commodity hardware, which
constrains communication between the CPU and GPU.
  In this paper, we present an offloading framework, LSP_Offload, that enables
near-native speed LLM fine-tuning on commodity hardware through learned
subspace projectors. Our data-driven approach involves learning an efficient
sparse compressor that minimizes communication with minimal precision loss.
Additionally, we introduce a novel layer-wise communication schedule to
maximize parallelism between communication and computation. As a result, our
framework can fine-tune a 1.3 billion parameter model on a 4GB laptop GPU and a
7 billion parameter model on an NVIDIA RTX 4090 GPU with 24GB memory, achieving
only a 31% slowdown compared to fine-tuning with unlimited memory. Compared to
state-of-the-art offloading frameworks, our approach increases fine-tuning
throughput by up to 3.33 times and reduces end-to-end fine-tuning time by
33.1%~62.5% when converging to the same accuracy.

摘要：微調大型語言模型 (LLM) 需要大量的記憶體，通常會超過單一 GPU 的容量。解決此記憶體挑戰的常見方法是將運算和資料從 GPU 卸載到 CPU。然而，此方法受到商品硬體有限頻寬的阻礙，這會限制 CPU 和 GPU 之間的通訊。
  在這篇論文中，我們提出一個卸載框架 LSP_Offload，它能透過學習的子空間投影器，在商品硬體上以接近原生速度微調 LLM。我們的資料驅動方法涉及學習一個有效的稀疏壓縮器，它能以最小的精度損失最小化通訊。此外，我們引入一個新穎的分層通訊排程，以最大化通訊和運算之間的平行性。因此，我們的框架可以在 4GB 筆電 GPU 上微調 13 億個參數的模型，並在具有 24GB 記憶體的 NVIDIA RTX 4090 GPU 上微調 70 億個參數的模型，與在不受限記憶體的情況下進行微調相比，只會減慢 31%。與最先進的卸載框架相比，我們的做法將微調處理量提高了 3.33 倍，並在收斂到相同準確度時將端到端微調時間減少了 33.1%~62.5%。

##### **Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation**
2406.10174v1 by Mohamad Elzohbi, Richard Zhao

The intersection between poetry and music provides an interesting case for
computational creativity, yet remains relatively unexplored. This paper
explores the integration of poetry and music through the lens of beat patterns,
investigating whether a byte-based language model can generate words that fit
specific beat patterns within the context of poetry. Drawing on earlier
studies, we developed a method to train a byte-based transformer model, ByT5,
to align poems with beat patterns. The results demonstrate a high level of beat
alignment while maintaining semantic coherence. Future work will aim to improve
the model's ability to create complete beat-aligned poems.

摘要：詩歌與音樂之間的交集為運算創意提供了有趣的案例，但仍相對未被探索。本文透過節奏型態的視角探討詩歌與音樂的整合，探討基於位元的語言模型是否能在詩歌的脈絡中產生符合特定節奏型態的詞彙。根據先前的研究，我們開發出一種訓練基於位元的 Transformer 模型 ByT5 的方法，以將詩歌與節奏型態對齊。結果顯示出高度的節奏對齊，同時維持語意連貫性。未來的研究目標是提升模型建立完整節奏對齊詩歌的能力。

##### **IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce**
2406.10173v1 by Wenxuan Ding, Weiqi Wang, Sze Heng Douglas Kwok, Minghao Liu, Tianqing Fang, Jiaxin Bai, Junxian He, Yangqiu Song

Enhancing Language Models' (LMs) ability to understand purchase intentions in
E-commerce scenarios is crucial for their effective assistance in various
downstream tasks. However, previous approaches that distill intentions from LMs
often fail to generate meaningful and human-centric intentions applicable in
real-world E-commerce contexts. This raises concerns about the true
comprehension and utilization of purchase intentions by LMs. In this paper, we
present IntentionQA, a double-task multiple-choice question answering benchmark
to evaluate LMs' comprehension of purchase intentions in E-commerce.
Specifically, LMs are tasked to infer intentions based on purchased products
and utilize them to predict additional purchases. IntentionQA consists of 4,360
carefully curated problems across three difficulty levels, constructed using an
automated pipeline to ensure scalability on large E-commerce platforms. Human
evaluations demonstrate the high quality and low false-negative rate of our
benchmark. Extensive experiments across 19 language models show that they still
struggle with certain scenarios, such as understanding products and intentions
accurately, jointly reasoning with products and intentions, and more, in which
they fall far behind human performances. Our code and data are publicly
available at https://github.com/HKUST-KnowComp/IntentionQA.

摘要：加強語言模型 (LM) 在電子商務場景中了解購買意圖的能力，對於它們在各種下游任務中提供有效協助至關重要。然而，先前從 LM 中提煉意圖的方法通常無法產生在現實世界電子商務環境中適用的有意義且以人為中心的意圖。這引起了人們對 LM 對購買意圖的真正理解和利用的擔憂。在本文中，我們提出了 IntentionQA，這是一個雙任務多選題問答基準，用於評估 LM 在電子商務中對購買意圖的理解。具體來說，LM 被要求根據已購買的產品推斷意圖，並利用它們來預測額外的購買。IntentionQA 包含 4,360 個精心策劃的問題，分為三個難度等級，使用自動化管道構建，以確保在大型電子商務平台上的可擴展性。人工評估證明了我們基準的高品質和低假陰性率。跨 19 個語言模型進行的廣泛實驗表明，它們仍然難以應對某些場景，例如準確理解產品和意圖、結合產品和意圖進行推理等等，在這些場景中它們遠遠落後於人類的表現。我們的代碼和數據已公開發布在 https://github.com/HKUST-KnowComp/IntentionQA。

##### **Datasets for Multilingual Answer Sentence Selection**
2406.10172v1 by Matteo Gabburo, Stefano Campese, Federico Agostini, Alessandro Moschitti

Answer Sentence Selection (AS2) is a critical task for designing effective
retrieval-based Question Answering (QA) systems. Most advancements in AS2 focus
on English due to the scarcity of annotated datasets for other languages. This
lack of resources prevents the training of effective AS2 models in different
languages, creating a performance gap between QA systems in English and other
locales. In this paper, we introduce new high-quality datasets for AS2 in five
European languages (French, German, Italian, Portuguese, and Spanish), obtained
through supervised Automatic Machine Translation (AMT) of existing English AS2
datasets such as ASNQ, WikiQA, and TREC-QA using a Large Language Model (LLM).
We evaluated our approach and the quality of the translated datasets through
multiple experiments with different Transformer architectures. The results
indicate that our datasets are pivotal in producing robust and powerful
multilingual AS2 models, significantly contributing to closing the performance
gap between English and other languages.

摘要：答案句子選擇 (AS2) 是設計有效基於檢索的問答 (QA) 系統的一項關鍵任務。由於缺乏其他語言的註解資料集，因此 AS2 的大多數進展都著重於英語。這種資源不足的情況會阻礙在不同語言中訓練有效的 AS2 模型，造成英語和其他地區的 QA 系統在效能上的差距。在本文中，我們引進了五種歐洲語言（法語、德語、義大利語、葡萄牙語和西班牙語）的新的高品質 AS2 資料集，這些資料集是透過使用大型語言模型 (LLM) 對現有的英語 AS2 資料集（例如 ASNQ、WikiQA 和 TREC-QA）進行監督式自動機器翻譯 (AMT) 所取得的。我們透過使用不同的 Transformer 架構進行多項實驗，來評估我們的做法和翻譯資料集的品質。結果顯示我們的資料集對於產生強健且強大的多語言 AS2 模型至關重要，有助於顯著縮小英語和其他語言之間的效能差距。

##### **MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers**
2406.10163v1 by Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, Chi Zhang

Recently, 3D assets created via reconstruction and generation have matched
the quality of manually crafted assets, highlighting their potential for
replacement. However, this potential is largely unrealized because these assets
always need to be converted to meshes for 3D industry applications, and the
meshes produced by current mesh extraction methods are significantly inferior
to Artist-Created Meshes (AMs), i.e., meshes created by human artists.
Specifically, current mesh extraction methods rely on dense faces and ignore
geometric features, leading to inefficiencies, complicated post-processing, and
lower representation quality. To address these issues, we introduce
MeshAnything, a model that treats mesh extraction as a generation problem,
producing AMs aligned with specified shapes. By converting 3D assets in any 3D
representation into AMs, MeshAnything can be integrated with various 3D asset
production methods, thereby enhancing their application across the 3D industry.
The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned
decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE,
then train the shape-conditioned decoder-only transformer on this vocabulary
for shape-conditioned autoregressive mesh generation. Our extensive experiments
show that our method generates AMs with hundreds of times fewer faces,
significantly improving storage, rendering, and simulation efficiencies, while
achieving precision comparable to previous methods.

摘要：<paragraph>最近，透過重建和生成所建立的 3D 資產，已達到人工打造資產的品質，突顯了其替代的潛力。然而，這項潛力在很大程度上尚未實現，因為這些資產總是需要轉換為網格，以供 3D 產業應用，而目前的網格提取方法所產生的網格，顯著劣於藝術家建立的網格 (AM)，亦即由人類藝術家建立的網格。具體來說，目前的網格提取方法仰賴密集的面，且忽略幾何特徵，導致效率低落、後處理複雜，以及表現品質較低。為了解決這些問題，我們引入了 MeshAnything，這是一個將網格提取視為生成問題的模型，產生與指定形狀一致的 AM。透過將任何 3D 表現中的 3D 資產轉換為 AM，MeshAnything 可整合至各種 3D 資產製作方法，從而提升其在 3D 產業中的應用。MeshAnything 的架構包含一個 VQ-VAE 和一個形狀條件的僅解碼器Transformer。我們首先使用 VQ-VAE 學習網格詞彙，然後針對形狀條件自迴歸網格生成，在這個詞彙上訓練形狀條件的僅解碼器Transformer。我們廣泛的實驗顯示，我們的技術會生成面數少於數百倍的 AM，顯著改善儲存、渲染和模擬效率，同時達成與先前技術相當的精準度。</paragraph>

##### **Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models**
2406.10162v1 by Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez, Evan Hubinger

In reinforcement learning, specification gaming occurs when AI systems learn
undesired behaviors that are highly rewarded due to misspecified training
goals. Specification gaming can range from simple behaviors like sycophancy to
sophisticated and pernicious behaviors like reward-tampering, where a model
directly modifies its own reward mechanism. However, these more pernicious
behaviors may be too complex to be discovered via exploration. In this paper,
we study whether Large Language Model (LLM) assistants which find easily
discovered forms of specification gaming will generalize to perform rarer and
more blatant forms, up to and including reward-tampering. We construct a
curriculum of increasingly sophisticated gameable environments and find that
training on early-curriculum environments leads to more specification gaming on
remaining environments. Strikingly, a small but non-negligible proportion of
the time, LLM assistants trained on the full curriculum generalize zero-shot to
directly rewriting their own reward function. Retraining an LLM not to game
early-curriculum environments mitigates, but does not eliminate,
reward-tampering in later environments. Moreover, adding harmlessness training
to our gameable environments does not prevent reward-tampering. These results
demonstrate that LLMs can generalize from common forms of specification gaming
to more pernicious reward tampering and that such behavior may be nontrivial to
remove.

摘要：在強化學習中，當 AI 系統學習到因訓練目標錯誤而獲得高度獎勵的不良行為時，就會發生規範博弈。規範博弈的範圍從簡單的行為，例如阿諛奉承，到複雜且有害的行為，例如獎勵篡改，其中模型直接修改其自身的獎勵機制。然而，這些更惡劣的行為可能過於複雜，無法透過探索來發現。在本文中，我們研究大型語言模型 (LLM) 助理是否會發現容易發現的規範博弈形式，並推廣到執行更罕見且更明顯的形式，包括獎勵篡改。我們建構了一個越來越複雜的可博弈環境課程，並發現對早期課程環境的訓練會導致在剩餘環境中進行更多規範博弈。令人驚訝的是，在很小但不可忽略的時間比例中，在完整課程中訓練的 LLM 助理會概化為零次學習，直接改寫自己的獎勵函數。重新訓練 LLM 不玩早期課程環境可以減輕，但無法消除，在後續環境中的獎勵篡改。此外，在我們的可博弈環境中加入無害訓練並不能防止獎勵篡改。這些結果表明，LLM 可以從常見的規範博弈形式推廣到更惡劣的獎勵篡改，並且這種行為可能很難消除。

##### **One-pass Multiple Conformer and Foundation Speech Systems Compression and Quantization Using An All-in-one Neural Model**
2406.10160v1 by Zhaoqing Li, Haoning Xu, Tianzi Wang, Shoukang Hu, Zengrui Jin, Shujie Hu, Jiajun Deng, Mingyu Cui, Mengzhe Geng, Xunying Liu

We propose a novel one-pass multiple ASR systems joint compression and
quantization approach using an all-in-one neural model. A single compression
cycle allows multiple nested systems with varying Encoder depths, widths, and
quantization precision settings to be simultaneously constructed without the
need to train and store individual target systems separately. Experiments
consistently demonstrate the multiple ASR systems compressed in a single
all-in-one model produced a word error rate (WER) comparable to, or lower by up
to 1.01\% absolute (6.98\% relative) than individually trained systems of equal
complexity. A 3.4x overall system compression and training time speed-up was
achieved. Maximum model size compression ratios of 12.8x and 3.93x were
obtained over the baseline Switchboard-300hr Conformer and LibriSpeech-100hr
fine-tuned wav2vec2.0 models, respectively, incurring no statistically
significant WER increase.

摘要：我們提出一個新穎的一遍式多重 ASR 系統聯合壓縮和量化方法，使用一個多合一的類神經模型。單一壓縮週期允許同時構造具有不同編碼器深度、寬度和量化精度設定的多重嵌套系統，而無需分別訓練和儲存個別的目標系統。實驗持續證明壓縮在單一多合一模型中的多重 ASR 系統產生一個詞彙錯誤率 (WER)，與個別訓練的相同複雜度系統相當，或低至 1.01% 絕對值 (6.98% 相對值)。實現了 3.4 倍的整體系統壓縮和訓練時間加速。與基準 Switchboard-300hr Conformer 和 LibriSpeech-100hr 微調 wav2vec2.0 模型相比，分別獲得了 12.8 倍和 3.93 倍的最大模型大小壓縮比，不會造成統計上顯著的 WER 增加。

##### **RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model**
2406.10157v1 by Hantao Zhou, Tianying Ji, Jianwei Zhang, Fuchun Sun, Huazhe Xu

Minigolf, a game with countless court layouts, and complex ball motion,
constitutes a compelling real-world testbed for the study of embodied
intelligence. As it not only challenges spatial and kinodynamic reasoning but
also requires reflective and corrective capacities to address erroneously
designed courses. We introduce RoboGolf, a framework that perceives dual-camera
visual inputs with nested VLM-empowered closed-loop control and reflective
equilibrium loop. Extensive experiments demonstrate the effectiveness of
RoboGolf on challenging minigolf courts including those that are impossible to
finish.

摘要：迷你高爾夫，一項擁有無數球場佈局和複雜球路運動的遊戲，
構成了一個引人入勝的真實世界測試平台，用於研究具身智能。由於它不僅挑戰空間和運動動力推理，而且還需要反思和糾正能力來解決錯誤設計的球場。我們介紹了 RoboGolf，一個感知雙攝像頭視覺輸入的框架，具有嵌套的 VLM 增強閉環控制和反思平衡迴路。大量的實驗證明了 RoboGolf 在具有挑戰性的迷你高爾夫球場上的有效性，包括那些不可能完成的球場。

##### **BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack**
2406.10149v1 by Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev

In recent years, the input context sizes of large language models (LLMs) have
increased dramatically. However, existing evaluation methods have not kept
pace, failing to comprehensively assess the efficiency of models in handling
long contexts. To bridge this gap, we introduce the BABILong benchmark,
designed to test language models' ability to reason across facts distributed in
extremely long documents. BABILong includes a diverse set of 20 reasoning
tasks, including fact chaining, simple induction, deduction, counting, and
handling lists/sets. These tasks are challenging on their own, and even more
demanding when the required facts are scattered across long natural text. Our
evaluations show that popular LLMs effectively utilize only 10-20\% of the
context and their performance declines sharply with increased reasoning
complexity. Among alternatives to in-context reasoning, Retrieval-Augmented
Generation methods achieve a modest 60\% accuracy on single-fact question
answering, independent of context length. Among context extension methods, the
highest performance is demonstrated by recurrent memory transformers, enabling
the processing of lengths up to 11 million tokens. The BABILong benchmark is
extendable to any length to support the evaluation of new upcoming models with
increased capabilities, and we provide splits up to 1 million token lengths.

摘要：近年来，大型语言模型（LLM）的输入上下文大小已大幅增加。然而，现有的评估方法并未跟上步伐，未能全面评估模型处理长上下文的效率。为了弥合这一差距，我们引入了 BABILong 基准，旨在测试语言模型在极长文档中分布的事实中进行推理的能力。BABILong 包含 20 项不同的推理任务，包括事实链接、简单归纳、演绎、计数以及处理列表/集合。这些任务本身具有挑战性，当所需的事实分散在很长的自然文本中时，难度更大。我们的评估表明，流行的 LLM 实际上只利用了 10-20% 的上下文，并且它们的性能随着推理复杂性的增加而急剧下降。在上下文推理的替代方案中，检索增强生成方法在单事实问题解答中实现了 60% 的适度准确率，与上下文长度无关。在上下文扩展方法中，循环记忆转换器表现出最高的性能，能够处理长达 1100 万个标记的长度。BABILong 基准可以扩展到任何长度，以支持评估具有更多功能的新兴模型，并且我们提供了长达 100 万个标记长度的拆分。

##### **Improving rule mining via embedding-based link prediction**
2406.10144v1 by N'Dah Jean Kouagou, Arif Yilmaz, Michel Dumontier, Axel-Cyrille Ngonga Ngomo

Rule mining on knowledge graphs allows for explainable link prediction.
Contrarily, embedding-based methods for link prediction are well known for
their generalization capabilities, but their predictions are not interpretable.
Several approaches combining the two families have been proposed in recent
years. The majority of the resulting hybrid approaches are usually trained
within a unified learning framework, which often leads to convergence issues
due to the complexity of the learning task. In this work, we propose a new way
to combine the two families of approaches. Specifically, we enrich a given
knowledge graph by means of its pre-trained entity and relation embeddings
before applying rule mining systems on the enriched knowledge graph. To
validate our approach, we conduct extensive experiments on seven benchmark
datasets. An analysis of the results generated by our approach suggests that we
discover new valuable rules on the enriched graphs. We provide an open source
implementation of our approach as well as pretrained models and datasets at
https://github.com/Jean-KOUAGOU/EnhancedRuleLearning

摘要：知識圖譜上的規則挖掘允許可解釋的連結預測。
相反地，基於嵌入的連結預測方法以其概化能力而聞名，但其預測不可解釋。
近年來已經提出了結合這兩個系列的幾種方法。大多數產生的混合方法通常在統一的學習框架內進行訓練，這由於學習任務的複雜性，通常會導致收斂問題。在這項工作中，我們提出了一種結合這兩個系列方法的新方法。具體來說，我們通過其預先訓練的實體和關係嵌入來豐富給定的知識圖譜，然後在豐富的知識圖譜上應用規則挖掘系統。為了驗證我們的做法，我們對七個基準資料集進行了廣泛的實驗。對我們的方法產生的結果的分析表明，我們在豐富的圖譜上發現了有價值的新規則。我們提供了我們方法的開源實現，以及預訓練模型和資料集，網址為 https://github.com/Jean-KOUAGOU/EnhancedRuleLearning

##### **Evaluation of Large Language Models: STEM education and Gender Stereotypes**
2406.10133v1 by Smilla Due, Sneha Das, Marianne Andersen, Berta Plandolit López, Sniff Andersen Nexø, Line Clemmensen

Large Language Models (LLMs) have an increasing impact on our lives with use
cases such as chatbots, study support, coding support, ideation, writing
assistance, and more. Previous studies have revealed linguistic biases in
pronouns used to describe professions or adjectives used to describe men vs
women. These issues have to some degree been addressed in updated LLM versions,
at least to pass existing tests. However, biases may still be present in the
models, and repeated use of gender stereotypical language may reinforce the
underlying assumptions and are therefore important to examine further. This
paper investigates gender biases in LLMs in relation to educational choices
through an open-ended, true to user-case experimental design and a quantitative
analysis. We investigate the biases in the context of four different cultures,
languages, and educational systems (English/US/UK, Danish/DK, Catalan/ES, and
Hindi/IN) for ages ranging from 10 to 16 years, corresponding to important
educational transition points in the different countries. We find that there
are significant and large differences in the ratio of STEM to non-STEM
suggested education paths provided by chatGPT when using typical girl vs boy
names to prompt lists of suggested things to become. There are generally fewer
STEM suggestions in the Danish, Spanish, and Indian context compared to the
English. We also find subtle differences in the suggested professions, which we
categorise and report.

摘要：大型語言模型 (LLM) 對我們的生活影響越來越大，其應用案例包括聊天機器人、學習支援、編碼支援、構思、寫作協助等。先前的研究揭露了用於描述職業的代名詞或用於描述男性與女性的形容詞中存在的語言偏見。這些問題在更新的 LLM 版本中已獲得一定程度的解決，至少可以通過現有的測試。然而，模型中可能仍然存在偏見，並且重複使用性別刻板語言可能會強化基本假設，因此有必要進一步檢視。本文透過開放式、真實的使用者案例實驗設計和量化分析，探討 LLM 在與教育選擇相關的性別偏見。我們在四種不同的文化、語言和教育系統（英語/美國/英國、丹麥語/丹麥、加泰羅尼亞語/西班牙和印地語/印度）中調查偏見，年齡範圍從 10 到 16 歲，這對不同國家的重要教育轉捩點來說相當重要。我們發現，當使用典型的女孩名字與男孩名字來提示建議事項清單時，chatGPT 所提供的 STEM 與非 STEM 建議教育路徑的比率存在顯著且巨大的差異。與英語相比，丹麥語、西班牙語和印度語脈絡中通常有較少的 STEM 建議。我們還發現建議職業中存在細微的差異，我們對這些差異進行了分類和報告。

##### **Linear Contextual Bandits with Hybrid Payoff: Revisited**
2406.10131v1 by Nirjhar Das, Gaurav Sinha

We study the Linear Contextual Bandit problem in the hybrid reward setting.
In this setting every arm's reward model contains arm specific parameters in
addition to parameters shared across the reward models of all the arms. We can
reduce this setting to two closely related settings (a) Shared - no arm
specific parameters, and (b) Disjoint - only arm specific parameters, enabling
the application of two popular state of the art algorithms - $\texttt{LinUCB}$
and $\texttt{DisLinUCB}$ (Algorithm 1 in (Li et al. 2010)). When the arm
features are stochastic and satisfy a popular diversity condition, we provide
new regret analyses for both algorithms, significantly improving on the known
regret guarantees of these algorithms. Our novel analysis critically exploits
the hybrid reward structure and the diversity condition. Moreover, we introduce
a new algorithm $\texttt{HyLinUCB}$ that crucially modifies $\texttt{LinUCB}$
(using a new exploration coefficient) to account for sparsity in the hybrid
setting. Under the same diversity assumptions, we prove that
$\texttt{HyLinUCB}$ also incurs only $O(\sqrt{T})$ regret for $T$ rounds. We
perform extensive experiments on synthetic and real-world datasets
demonstrating strong empirical performance of $\texttt{HyLinUCB}$.For number of
arm specific parameters much larger than the number of shared parameters, we
observe that $\texttt{DisLinUCB}$ incurs the lowest regret. In this case,
regret of $\texttt{HyLinUCB}$ is the second best and extremely competitive to
$\texttt{DisLinUCB}$. In all other situations, including our real-world
dataset, $\texttt{HyLinUCB}$ has significantly lower regret than
$\texttt{LinUCB}$, $\texttt{DisLinUCB}$ and other SOTA baselines we considered.
We also empirically observe that the regret of $\texttt{HyLinUCB}$ grows much
slower with the number of arms compared to baselines, making it suitable even
for very large action spaces.

摘要：<paragraph>我們在混合獎勵設定中研究線性情境強盜問題。
在此設定中，每個臂的獎勵模型包含特定於臂的參數，以及所有臂的獎勵模型中共享的參數。我們可以將此設定簡化為兩個密切相關的設定：(a) 共享 - 沒有特定於臂的參數，以及 (b) 分離 - 只有特定於臂的參數，從而能夠應用兩種流行的最新演算法 - $\texttt{LinUCB}$ 和 $\texttt{DisLinUCB}$（李等人 2010 年的演算法 1）。當臂特徵是隨機的，並滿足流行的多樣性條件時，我們為這兩種演算法提供新的後悔分析，大幅改善這些演算法已知的後悔保證。我們新穎的分析批判性地利用混合獎勵結構和多樣性條件。此外，我們引入一種新的演算法 $\texttt{HyLinUCB}$，它批判性地修改 $\texttt{LinUCB}$（使用新的探索係數）以考量混合設定中的稀疏性。在相同的多樣性假設下，我們證明 $\texttt{HyLinUCB}$ 在 $T$ 輪中也僅產生 $O(\sqrt{T})$ 的後悔。我們對合成和真實世界資料集執行廣泛的實驗，證明 $\texttt{HyLinUCB}$ 具有強大的經驗效能。對於特定於臂的參數數量遠大於共享參數的數量，我們觀察到 $\texttt{DisLinUCB}$ 產生最低後悔。在這種情況下，$\texttt{HyLinUCB}$ 的後悔是第二好的，並且極具競爭力，可以與 $\texttt{DisLinUCB}$ 相提並論。在所有其他情況下，包括我們的真實世界資料集，$\texttt{HyLinUCB}$ 的後悔都顯著低於 $\texttt{LinUCB}$、$\texttt{DisLinUCB}$ 和我們考慮的其他 SOTA 基準。我們還根據經驗觀察到，與基準相比，$\texttt{HyLinUCB}$ 的後悔隨著臂的數量而增長得慢得多，使其甚至適用於非常大的動作空間。</paragraph>

##### **The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models**
2406.10130v1 by Yan Liu, Yu Liu, Xiaokang Chen, Pin-Yu Chen, Daoguang Zan, Min-Yen Kan, Tsung-Yi Ho

Pre-trained Language models (PLMs) have been acknowledged to contain harmful
information, such as social biases, which may cause negative social impacts or
even bring catastrophic results in application. Previous works on this problem
mainly focused on using black-box methods such as probing to detect and
quantify social biases in PLMs by observing model outputs. As a result,
previous debiasing methods mainly finetune or even pre-train language models on
newly constructed anti-stereotypical datasets, which are high-cost. In this
work, we try to unveil the mystery of social bias inside language models by
introducing the concept of {\sc Social Bias Neurons}. Specifically, we propose
{\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e.,
neurons) in a language model that can be attributed to undesirable behavior,
such as social bias. By formalizing undesirable behavior as a distributional
property of language, we employ sentiment-bearing prompts to elicit classes of
sensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus
attributes the uneven distribution for different demographics to specific
Social Bias Neurons, which track the trail of unwanted behavior inside PLM
units to achieve interoperability. Moreover, derived from our interpretable
technique, {\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate
social biases. By studying BERT, RoBERTa, and their attributable differences
from debiased FairBERTa, IG$^2$ allows us to locate and suppress identified
neurons, and further mitigate undesired behaviors. As measured by prior metrics
from StereoSet, our model achieves a higher degree of fairness while
maintaining language modeling ability with low cost.

摘要：<paragraph>預先訓練的語言模型 (PLM) 已被確認包含有害資訊，例如社會偏見，這可能會造成負面的社會影響，甚至在應用中帶來災難性的結果。針對此問題先前的研究，主要集中在使用黑盒方法，例如探測來檢測和量化 PLM 中的社會偏見，方法是觀察模型輸出。因此，先前的去偏見方法主要微調甚至預先訓練語言模型，使用新建的反刻板印象資料集，而這成本很高。在這項研究中，我們嘗試揭開語言模型內部社會偏見的神秘面紗，方法是引入「社會偏見神經元」的概念。具體來說，我們提出「整合間隙梯度 (IG$^2$)」，以精確找出語言模型中的單元（即神經元），這些單元可歸因於不良行為，例如社會偏見。我們將不良行為形式化為語言的分配屬性，並使用帶有情緒的提示來引出與此類情緒相關的敏感詞彙（人口統計資料）類別。因此，我們的 IG$^2$ 將不同人口統計資料的不均勻分配歸因於特定的社會偏見神經元，這些神經元追蹤 PLM 單元內不需要的行為的軌跡，以實現互操作性。此外，衍生自我們可解釋的技術，進一步提出「偏見神經元抑制 (BNS)」來減輕社會偏見。透過研究 BERT、RoBERTa 以及它們與去偏見的 FairBERTa 之間可歸因的差異，IG$^2$ 使我們能夠找出並抑制已識別的神經元，並進一步減輕不需要的行為。根據 StereoSet 先前的指標測量，我們的模型在維持低成本的語言建模能力的同時，達到了更高的公平性。</paragraph>

##### **SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages**
2406.10118v1 by Holy Lovenia, Rahmad Mahendra, Salsabil Maulana Akbar, Lester James V. Miranda, Jennifer Santoso, Elyanah Aco, Akhdan Fadhilah, Jonibek Mansurov, Joseph Marvin Imperial, Onno P. Kampman, Joel Ruben Antony Moniz, Muhammad Ravi Shulthan Habibi, Frederikus Hudi, Railey Montalan, Ryan Ignatius, Joanito Agili Lopo, William Nixon, Börje F. Karlsson, James Jaya, Ryandito Diandaru, Yuze Gao, Patrick Amadeus, Bin Wang, Jan Christian Blaise Cruz, Chenxi Whitehouse, Ivan Halim Parmonangan, Maria Khelli, Wenyu Zhang, Lucky Susanto, Reynard Adha Ryanda, Sonny Lazuardi Hermawan, Dan John Velasco, Muhammad Dehan Al Kautsar, Willy Fitra Hendria, Yasmin Moslem, Noah Flynn, Muhammad Farid Adilazuarda, Haochen Li, Johanes Lee, R. Damanhuri, Shuo Sun, Muhammad Reza Qorib, Amirbek Djanibekov, Wei Qi Leong, Quyet V. Do, Niklas Muennighoff, Tanrada Pansuwan, Ilham Firdausi Putra, Yan Xu, Ngee Chia Tai, Ayu Purwarianti, Sebastian Ruder, William Tjhi, Peerat Limkonchotiwat, Alham Fikri Aji, Sedrick Keh, Genta Indra Winata, Ruochen Zhang, Fajri Koto, Zheng-Xin Yong, Samuel Cahyawijaya

Southeast Asia (SEA) is a region rich in linguistic diversity and cultural
variety, with over 1,300 indigenous languages and a population of 671 million
people. However, prevailing AI models suffer from a significant lack of
representation of texts, images, and audio datasets from SEA, compromising the
quality of AI models for SEA languages. Evaluating models for SEA languages is
challenging due to the scarcity of high-quality datasets, compounded by the
dominance of English training data, raising concerns about potential cultural
misrepresentation. To address these challenges, we introduce SEACrowd, a
collaborative initiative that consolidates a comprehensive resource hub that
fills the resource gap by providing standardized corpora in nearly 1,000 SEA
languages across three modalities. Through our SEACrowd benchmarks, we assess
the quality of AI models on 36 indigenous languages across 13 tasks, offering
valuable insights into the current AI landscape in SEA. Furthermore, we propose
strategies to facilitate greater AI advancements, maximizing potential utility
and resource equity for the future of AI in SEA.

摘要：東南亞 (SEA) 是個語言多元且文化多樣的地區，擁有超過 1,300 種原住民語言和 6.71 億人口。然而，現有的 AI 模型嚴重缺乏來自東南亞的文字、影像和音訊資料集，影響了東南亞語言 AI 模型的品質。由於缺乏高品質的資料集，加上英文訓練資料的影響，評估東南亞語言的模型具有挑戰性，也引發了潛在文化誤解的疑慮。為了應對這些挑戰，我們推出了 SEACrowd，這是一個合作計畫，整合了一個全面的資源中心，透過提供近 1,000 種東南亞語言的三種模式的標準語料庫，填補了資源缺口。透過我們的 SEACrowd 基準測試，我們評估了 36 種原住民語言在 13 個任務上的 AI 模型品質，提供了對東南亞當前 AI 發展的寶貴見解。此外，我們提出了促進 AI 進一步發展的策略，最大化東南亞 AI 未來的潛在效益和資源公平性。

##### **Precipitation Nowcasting Using Physics Informed Discriminator Generative Models**
2406.10108v1 by Junzhe Yin, Cristian Meo, Ankush Roy, Zeineh Bou Cher, Yanbo Wang, Ruben Imhoff, Remko Uijlenhoet, Justin Dauwels

Nowcasting leverages real-time atmospheric conditions to forecast weather
over short periods. State-of-the-art models, including PySTEPS, encounter
difficulties in accurately forecasting extreme weather events because of their
unpredictable distribution patterns. In this study, we design a
physics-informed neural network to perform precipitation nowcasting using the
precipitation and meteorological data from the Royal Netherlands Meteorological
Institute (KNMI). This model draws inspiration from the novel Physics-Informed
Discriminator GAN (PID-GAN) formulation, directly integrating physics-based
supervision within the adversarial learning framework. The proposed model
adopts a GAN structure, featuring a Vector Quantization Generative Adversarial
Network (VQ-GAN) and a Transformer as the generator, with a temporal
discriminator serving as the discriminator. Our findings demonstrate that the
PID-GAN model outperforms numerical and SOTA deep generative models in terms of
precipitation nowcasting downstream metrics.

摘要：現在預測利用實時大氣條件來預測短期的天氣。包括 PySTEPS 在內的最新模型，由於其難以預測的分布模式，在準確預測極端天氣事件時會遇到困難。在本研究中，我們設計了一個物理信息神經網路，使用荷蘭皇家氣象研究所 (KNMI) 的降水和氣象數據來執行降水現在預測。此模型從新穎的物理信息判別器 GAN (PID-GAN) 公式中汲取靈感，直接在對抗性學習框架中整合基於物理的監督。所提出的模型採用 GAN 結構，以向量量化生成對抗網路 (VQ-GAN) 和 Transformer 作為生成器，並以時間判別器作為判別器。我們的研究結果表明，PID-GAN 模型在降水現在預測下游指標方面優於數值和 SOTA 深度生成模型。

##### **SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding**
2406.10100v1 by Junwei Luo, Zhen Pang, Yongjun Zhang, Tingzhu Wang, Linlin Wang, Bo Dang, Jiangwei Lao, Jian Wang, Jingdong Chen, Yihua Tan, Yansheng Li

Remote Sensing Large Multi-Modal Models (RSLMMs) are developing rapidly and
showcase significant capabilities in remote sensing imagery (RSI)
comprehension. However, due to the limitations of existing datasets, RSLMMs
have shortcomings in understanding the rich semantic relations among objects in
complex remote sensing scenes. To unlock RSLMMs' complex comprehension ability,
we propose a large-scale instruction tuning dataset FIT-RS, containing
1,800,851 instruction samples. FIT-RS covers common interpretation tasks and
innovatively introduces several complex comprehension tasks of escalating
difficulty, ranging from relation reasoning to image-level scene graph
generation. Based on FIT-RS, we build the FIT-RSFG benchmark. Furthermore, we
establish a new benchmark to evaluate the fine-grained relation comprehension
capabilities of LMMs, named FIT-RSRC. Based on combined instruction data, we
propose SkySenseGPT, which achieves outstanding performance on both public
datasets and FIT-RSFG, surpassing existing RSLMMs. We hope the FIT-RS dataset
can enhance the relation comprehension capability of RSLMMs and provide a
large-scale fine-grained data source for the remote sensing community. The
dataset will be available at https://github.com/Luo-Z13/SkySenseGPT

摘要：遙感大型多模態模型 (RSLMM) 發展迅速，並在遙感影像 (RSI) 理解方面展現出顯著的能力。然而，由於現有資料集的限制，RSLMM 在理解複雜遙感場景中物件之間豐富的語意關係方面存在不足。為了釋放 RSLMM 的複雜理解能力，我們提出了一個包含 1,800,851 個指令範例的大型指令調整資料集 FIT-RS。FIT-RS 涵蓋常見的解譯任務，並創新地引入了從關係推理到影像級場景圖生成等多個難度遞增的複雜理解任務。基於 FIT-RS，我們建立了 FIT-RSFG 基準。此外，我們建立了一個新的基準來評估 LMM 的細粒度關係理解能力，稱為 FIT-RSRC。基於結合的指令資料，我們提出了 SkySenseGPT，它在公共資料集和 FIT-RSFG 上都取得了傑出的表現，超越了現有的 RSLMM。我們希望 FIT-RS 資料集可以增強 RSLMM 的關係理解能力，並為遙感社群提供一個大規模的細粒度資料來源。該資料集將在 https://github.com/Luo-Z13/SkySenseGPT 上提供

##### **Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning**
2406.10099v1 by Jiaqi Li, Yixuan Tang, Yi Yang

Large language models (LLMs) have demonstrated remarkable capabilities across
various tasks but still face challenges such as hallucinations. One potential
reason for hallucinations is the lack of relevant knowledge or context. Thus, a
promising solution to mitigate this issue involves instructing LLMs to respond
with "I do not know" when a question falls outside their knowledge domain or
the provided context. However, in this work, we observed that LLMs struggle to
admit their lack of knowledge, primarily due to existing instruction datasets
designed to encourage specific answers. To improve large language models'
capability to recognize the boundaries of their knowledge, we propose a novel
approach called uncertainty-sensitive tuning. This method involves two-stage
training designed for uncertainty recognition and prompt-sensitive activation.
In the first stage, we guide the LLM to reject unknown questions. In the second
stage, we recover the decreased performance in QA tasks by incorporating
designed causal instructions. By leveraging this method, we aim to enhance the
model's ability to identify areas of uncertainty. The experimental results
demonstrate that our proposed uncertainty-sensitive tuning method significantly
improves the performance of the Llama2-chat-7B model. Specifically, it achieves
a substantial 34.7% improvement in handling questions involving knowledge gaps
compared to the original model. Moreover, our approach outperforms GPT-4,
exhibiting a 9.4% increase in overall performance. We open-source the model and
code on GitHub.

摘要：大型語言模型 (LLM) 已在各種任務中展示出卓越的能力，但仍面臨幻覺等挑戰。幻覺的一個潛在原因是缺乏相關知識或背景。因此，減輕此問題的一個有前途的解決方案涉及指導 LLM 在問題超出其知識領域或提供的背景時回答「我不知道」。然而，在這項工作中，我們觀察到 LLM 難以承認其知識不足，這主要是由於現有的指令數據集旨在鼓勵具體答案。為了提高大型語言模型識別其知識邊界的能力，我們提出了一種稱為不確定性敏感調整的新方法。此方法涉及兩階段訓練，旨在進行不確定性識別和提示敏感激活。在第一階段，我們指導 LLM 拒絕未知問題。在第二階段，我們通過納入設計的因果指令來恢復在 QA 任務中下降的性能。通過利用這種方法，我們旨在增強模型識別不確定性領域的能力。實驗結果表明，我們提出的不確定性敏感調整方法顯著提高了 Llama2-chat-7B 模型的性能。具體來說，與原始模型相比，它在處理涉及知識差距的問題時取得了 34.7% 的顯著改進。此外，我們的做法優於 GPT-4，整體性能提高了 9.4%。我們在 GitHub 上開源了模型和代碼。

##### **ECGMamba: Towards Efficient ECG Classification with BiSSM**
2406.10098v1 by Yupeng Qiang, Xunde Dong, Xiuling Liu, Yang Yang, Yihai Fang, Jianhong Dou

Electrocardiogram (ECG) signal analysis represents a pivotal technique in the
diagnosis of cardiovascular diseases. Although transformer-based models have
made significant progress in ECG classification, they exhibit inefficiencies in
the inference phase. The issue is primarily attributable to the secondary
computational complexity of Transformer's self-attention mechanism.
particularly when processing lengthy sequences. To address this issue, we
propose a novel model, ECGMamba, which employs a bidirectional state-space
model (BiSSM) to enhance classification efficiency. ECGMamba is based on the
innovative Mamba-based block, which incorporates a range of time series
modeling techniques to enhance performance while maintaining the efficiency of
inference. The experimental results on two publicly available ECG datasets
demonstrate that ECGMamba effectively balances the effectiveness and efficiency
of classification, achieving competitive performance. This study not only
contributes to the body of knowledge in the field of ECG classification but
also provides a new research path for efficient and accurate ECG signal
analysis. This is of guiding significance for the development of diagnostic
models for cardiovascular diseases.

摘要：心電圖 (ECG) 訊號分析是心血管疾病診斷中的關鍵技術。儘管基於 Transformer 的模型在 ECG 分類方面取得了重大進展，但在推理階段卻表現出效率低下。這個問題主要歸因於 Transformer 自注意力機制的次要運算複雜度，特別是在處理長序列時。為了解決這個問題，我們提出了一種新模型 ECGMamba，它採用雙向狀態空間模型 (BiSSM) 來提高分類效率。ECGMamba 基於創新的 Mamba 塊，它結合了一系列時間序列建模技術來增強效能，同時保持推理效率。在兩個公開可用的 ECG 資料集上的實驗結果表明，ECGMamba 有效地平衡了分類的有效性和效率，達到了有競爭力的效能。這項研究不僅為 ECG 分類領域的知識體系做出了貢獻，而且還為高效、準確的 ECG 訊號分析提供了新的研究途徑。這對於開發心血管疾病診斷模型具有指導意義。

##### **Exploring the Correlation between Human and Machine Evaluation of Simultaneous Speech Translation**
2406.10091v1 by Xiaoman Wang, Claudio Fantinuoli

Assessing the performance of interpreting services is a complex task, given
the nuanced nature of spoken language translation, the strategies that
interpreters apply, and the diverse expectations of users. The complexity of
this task become even more pronounced when automated evaluation methods are
applied. This is particularly true because interpreted texts exhibit less
linearity between the source and target languages due to the strategies
employed by the interpreter.
  This study aims to assess the reliability of automatic metrics in evaluating
simultaneous interpretations by analyzing their correlation with human
evaluations. We focus on a particular feature of interpretation quality, namely
translation accuracy or faithfulness. As a benchmark we use human assessments
performed by language experts, and evaluate how well sentence embeddings and
Large Language Models correlate with them. We quantify semantic similarity
between the source and translated texts without relying on a reference
translation. The results suggest GPT models, particularly GPT-3.5 with direct
prompting, demonstrate the strongest correlation with human judgment in terms
of semantic similarity between source and target texts, even when evaluating
short textual segments. Additionally, the study reveals that the size of the
context window has a notable impact on this correlation.

摘要：評估口譯服務的表現是一項複雜的任務，因為口語翻譯的性質微妙，口譯員所應用的策略，以及使用者的多元期待。當應用自動化評估方法時，這項任務的複雜性變得更加明顯。特別是因為口譯文本在來源語言和目標語言之間的線性度較低，這是因為口譯員所採用的策略。
本研究旨在評估自動化指標在評估同聲傳譯時的可靠性，方法是分析它們與人類評估之間的相關性。我們專注於口譯品質的特定特徵，即翻譯準確性或忠實度。我們使用語言專家進行的人類評估作為基準，並評估句子嵌入和大型語言模型與它們有多高的相關性。我們量化來源文本和翻譯文本之間的語義相似性，而不依賴參考翻譯。結果表明，GPT 模型，特別是帶有直接提示的 GPT-3.5，在來源文本和目標文本之間的語義相似性方面與人類判斷具有最強相關性，即使在評估短文本片段時也是如此。此外，研究表明，上下文視窗的大小對這種相關性有顯著影響。

##### **Biomarker based Cancer Classification using an Ensemble with Pre-trained Models**
2406.10087v1 by Chongmin Lee, Jihie Kim

Certain cancer types, namely pancreatic cancer is difficult to detect at an
early stage; sparking the importance of discovering the causal relationship
between biomarkers and cancer to identify cancer efficiently. By allowing for
the detection and monitoring of specific biomarkers through a non-invasive
method, liquid biopsies enhance the precision and efficacy of medical
interventions, advocating the move towards personalized healthcare. Several
machine learning algorithms such as Random Forest, SVM are utilized for
classification, yet causing inefficiency due to the need for conducting
hyperparameter tuning. We leverage a meta-trained Hyperfast model for
classifying cancer, accomplishing the highest AUC of 0.9929 and simultaneously
achieving robustness especially on highly imbalanced datasets compared to other
ML algorithms in several binary classification tasks (e.g. breast invasive
carcinoma; BRCA vs. non-BRCA). We also propose a novel ensemble model combining
pre-trained Hyperfast model, XGBoost, and LightGBM for multi-class
classification tasks, achieving an incremental increase in accuracy (0.9464)
while merely using 500 PCA features; distinguishable from previous studies
where they used more than 2,000 features for similar results.

摘要：某些类型的癌症，例如胰臟癌，在早期階段難以檢測出來；這點凸顯了找出生物標記與癌症之間的因果關係以有效辨識癌症的重要性。液態切片透過非侵入性方法檢測和監控特定生物標記，進而提升醫療介入的精準度和效能，並倡導朝向個人化醫療保健邁進。隨機森林、SVM 等多種機器學習演算法用於分類，但由於需要進行超參數調整，因此造成效率不彰。我們利用經過元訓練的 Hyperfast 模型來分類癌症，達到了 0.9929 的最高 AUC，同時在多項二元分類任務中（例如乳房浸潤性癌；BRCA 與非 BRCA）實現了穩健性，特別是在高度不平衡的資料集上，優於其他 ML 演算法。我們還提出了一個新穎的整合模型，結合預先訓練的 Hyperfast 模型、XGBoost 和 LightGBM，用於多類別分類任務，僅使用 500 個 PCA 特徵便達到了精準度的增量提升（0.9464）；這點有別於先前的研究，它們使用超過 2,000 個特徵來獲得類似的結果。

##### **Discovering influential text using convolutional neural networks**
2406.10086v1 by Megan Ayers, Luke Sanford, Margaret Roberts, Eddie Yang

Experimental methods for estimating the impacts of text on human evaluation
have been widely used in the social sciences. However, researchers in
experimental settings are usually limited to testing a small number of
pre-specified text treatments. While efforts to mine unstructured texts for
features that causally affect outcomes have been ongoing in recent years, these
models have primarily focused on the topics or specific words of text, which
may not always be the mechanism of the effect. We connect these efforts with
NLP interpretability techniques and present a method for flexibly discovering
clusters of similar text phrases that are predictive of human reactions to
texts using convolutional neural networks. When used in an experimental
setting, this method can identify text treatments and their effects under
certain assumptions. We apply the method to two datasets. The first enables
direct validation of the model's ability to detect phrases known to cause the
outcome. The second demonstrates its ability to flexibly discover text
treatments with varying textual structures. In both cases, the model learns a
greater variety of text treatments compared to benchmark methods, and these
text features quantitatively meet or exceed the ability of benchmark methods to
predict the outcome.

摘要：實驗方法用於估計文本對人類評估的影響，已廣泛用於社會科學。然而，實驗環境中的研究人員通常僅限於測試少數預先指定的文本處理。儘管近年來一直在努力挖掘非結構化文本以找出因果影響結果的特徵，但這些模型主要集中在文本的主題或特定詞彙上，這可能並不總是影響的機制。我們將這些努力與 NLP 可解釋性技術聯繫起來，並提出了一種靈活發現類似文本短語群集的方法，這些短語群集可以預測人類對文本的反應，並使用卷積神經網路。在實驗環境中使用時，此方法可以在特定假設下識別文本處理及其效果。我們將此方法應用於兩個數據集。第一個數據集能夠直接驗證模型檢測已知會導致結果的短語的能力。第二個數據集展示了其靈活發現具有不同文本結構的文本處理的能力。在兩種情況下，與基準方法相比，模型學習了更多樣化的文本處理，而這些文本特徵在預測結果的能力上達到或超過了基準方法。

##### **Enhancing Question Answering on Charts Through Effective Pre-training Tasks**
2406.10085v1 by Ashim Gupta, Vivek Gupta, Shuo Zhang, Yujie He, Ning Zhang, Shalin Shah

To completely understand a document, the use of textual information is not
enough. Understanding visual cues, such as layouts and charts, is also
required. While the current state-of-the-art approaches for document
understanding (both OCR-based and OCR-free) work well, a thorough analysis of
their capabilities and limitations has not yet been performed. Therefore, in
this work, we addresses the limitation of current VisualQA models when applied
to charts and plots. To investigate shortcomings of the state-of-the-art
models, we conduct a comprehensive behavioral analysis, using ChartQA as a case
study. Our findings indicate that existing models particularly underperform in
answering questions related to the chart's structural and visual context, as
well as numerical information. To address these issues, we propose three simple
pre-training tasks that enforce the existing model in terms of both
structural-visual knowledge, as well as its understanding of numerical
questions. We evaluate our pre-trained model (called MatCha-v2) on three chart
datasets - both extractive and abstractive question datasets - and observe that
it achieves an average improvement of 1.7% over the baseline model.

摘要：要完全理解文件，僅使用文字資訊是不夠的。也需要理解視覺提示，例如配置和圖表。雖然用於文件理解（基於 OCR 和非 OCR）的目前最先進的方法運作良好，但尚未徹底分析它們的能力和限制。因此，在這項工作中，我們探討了應用於圖表和繪圖時的當前 VisualQA 模型的限制。為了調查最先進模型的缺點，我們以 ChartQA 為案例研究，進行了全面的行為分析。我們的發現表明，現有模型在回答與圖表的結構和視覺內容以及數值資訊相關的問題時表現特別不佳。為了解決這些問題，我們提出了三項簡單的預訓練任務，在結構視覺知識和對數值問題的理解方面強制執行現有模型。我們在三個圖表資料集（萃取式和抽象式問題資料集）上評估我們的預訓練模型（稱為 MatCha-v2），並觀察到它比基準模型平均提高了 1.7%。

##### **On the Evaluation of Speech Foundation Models for Spoken Language Understanding**
2406.10083v1 by Siddhant Arora, Ankita Pasad, Chung-Ming Chien, Jionghao Han, Roshan Sharma, Jee-weon Jung, Hira Dhamyal, William Chen, Suwon Shon, Hung-yi Lee, Karen Livescu, Shinji Watanabe

The Spoken Language Understanding Evaluation (SLUE) suite of benchmark tasks
was recently introduced to address the need for open resources and benchmarking
of complex spoken language understanding (SLU) tasks, including both
classification and sequence generation tasks, on natural speech. The benchmark
has demonstrated preliminary success in using pre-trained speech foundation
models (SFM) for these SLU tasks. However, the community still lacks a
fine-grained understanding of the comparative utility of different SFMs.
Inspired by this, we ask: which SFMs offer the most benefits for these complex
SLU tasks, and what is the most effective approach for incorporating these
SFMs? To answer this, we perform an extensive evaluation of multiple supervised
and self-supervised SFMs using several evaluation protocols: (i) frozen SFMs
with a lightweight prediction head, (ii) frozen SFMs with a complex prediction
head, and (iii) fine-tuned SFMs with a lightweight prediction head. Although
the supervised SFMs are pre-trained on much more speech recognition data (with
labels), they do not always outperform self-supervised SFMs; the latter tend to
perform at least as well as, and sometimes better than, supervised SFMs,
especially on the sequence generation tasks in SLUE. While there is no
universally optimal way of incorporating SFMs, the complex prediction head
gives the best performance for most tasks, although it increases the inference
time. We also introduce an open-source toolkit and performance leaderboard,
SLUE-PERB, for these tasks and modeling strategies.

摘要：口語語言理解評估 (SLUE) 基準任務套件最近被提出，以滿足開放資源和複雜口語語言理解 (SLU) 任務的基準測試需求，包括自然語音中的分類和序列生成任務。該基準測試已證明在使用預訓練語音基礎模型 (SFM) 執行這些 SLU 任務時取得了初步成功。然而，社群仍然缺乏對不同 SFM 比較效用的深入了解。受此啟發，我們提出了以下問題：哪些 SFM 為這些複雜的 SLU 任務提供了最大的好處，以及整合這些 SFM 最有效的方法是什麼？為了回答這個問題，我們使用多個評估協定對多個監督式和自監督式 SFM 進行了廣泛的評估：(i) 具有輕量級預測頭的凍結式 SFM、(ii) 具有複雜預測頭的凍結式 SFM，以及 (iii) 具有輕量級預測頭的微調式 SFM。儘管監督式 SFM 在更多語音辨識資料 (帶標籤) 上進行預訓練，但它們並不總是優於自監督式 SFM；後者往往表現得至少和監督式 SFM 一樣好，有時甚至更好，特別是在 SLUE 中的序列生成任務上。雖然沒有普遍最佳的整合 SFM 方法，但複雜的預測頭在大多數任務中表現最佳，儘管它增加了推理時間。我們還為這些任務和建模策略引入了開源工具包和效能排行榜 SLUE-PERB。

##### **Localizing Events in Videos with Multimodal Queries**
2406.10079v1 by Gengyuan Zhang, Mang Ling Ada Fok, Yan Xia, Yansong Tang, Daniel Cremers, Philip Torr, Volker Tresp, Jindong Gu

Video understanding is a pivotal task in the digital era, yet the dynamic and
multievent nature of videos makes them labor-intensive and computationally
demanding to process. Thus, localizing a specific event given a semantic query
has gained importance in both user-oriented applications like video search and
academic research into video foundation models. A significant limitation in
current research is that semantic queries are typically in natural language
that depicts the semantics of the target event. This setting overlooks the
potential for multimodal semantic queries composed of images and texts. To
address this gap, we introduce a new benchmark, ICQ, for localizing events in
videos with multimodal queries, along with a new evaluation dataset
ICQ-Highlight. Our new benchmark aims to evaluate how well models can localize
an event given a multimodal semantic query that consists of a reference image,
which depicts the event, and a refinement text to adjust the images' semantics.
To systematically benchmark model performance, we include 4 styles of reference
images and 5 types of refinement texts, allowing us to explore model
performance across different domains. We propose 3 adaptation methods that
tailor existing models to our new setting and evaluate 10 SOTA models, ranging
from specialized to large-scale foundation models. We believe this benchmark is
an initial step toward investigating multimodal queries in video event
localization.

摘要：影片理解是數位時代的關鍵任務，但影片的動態和多事件特性讓它們的處理過程耗時且需要大量的運算。因此，在語意查詢中定位特定事件已在以使用者為導向的應用程式（例如影片搜尋）和影片基礎模型的學術研究中變得重要。目前的重大研究限制在於，語意查詢通常以自然語言呈現，描述目標事件的語意。此設定忽略了由影像和文字組成之多模態語意查詢的潛力。為了解決這個差距，我們針對影片中的事件定位引入了新的基準 ICQ，並使用新的評估資料集 ICQ-Highlight。我們的基準旨在評估模型在給定多模態語意查詢（包含描述事件的參考影像和調整影像語意的修正文字）時，定位事件的表現。為了系統性地評估模型效能，我們包含 4 種參考影像風格和 5 種修正文字類型，讓我們能夠探索不同領域的模型效能。我們提出 3 種調整方法，將現有的模型調整到我們的設定，並評估 10 個 SOTA 模型，從專門模型到大型基礎模型。我們相信這個基準是針對影片事件定位中多模態查詢研究的第一步。

##### **Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content**
2406.10073v1 by Rémi Uro, Marie Tahon, David Doukhan, Antoine Laurent, Albert Rilliard

Transition Relevance Places are defined as the end of an utterance where the
interlocutor may take the floor without interrupting the current speaker
--i.e., a place where the turn is terminal. Analyzing turn terminality is
useful to study the dynamic of turn-taking in spontaneous conversations. This
paper presents an automatic classification of spoken utterances as Terminal or
Non-Terminal in multi-speaker settings. We compared audio, text, and fusions of
both approaches on a French corpus of TV and Radio extracts annotated with
turn-terminality information at each speaker change. Our models are based on
pre-trained self-supervised representations. We report results for different
fusion strategies and varying context sizes. This study also questions the
problem of performance variability by analyzing the differences in results for
multiple training runs with random initialization. The measured accuracy would
allow the use of these models for large-scale analysis of turn-taking.

摘要：轉換相關位置定義為發話結束，對話者可以在不中斷當前發言者的情況下發言
-- 即轉換為終點的地方。分析轉換終點有助於研究自發對話中輪流發言的動態。本文介紹了在多發言者設置中將口語發言自動分類為終點或非終點。我們在法語電視和廣播摘錄語料庫中比較了音頻、文本和兩種方法的融合，在每個發言者變更時都註明了轉換終點信息。我們的模型基於預訓練的自監督表示。我們報告了不同融合策略和不同上下文大小的結果。本研究還通過分析多次訓練運行（隨機初始化）的結果差異，質疑了性能變異性的問題。測量的準確度將允許使用這些模型進行大規模的輪流發言分析。

##### **First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models**
2406.10057v1 by Enming Zhang, Ruobing Yao, Huanyong Liu, Junhui Yu, Jiale Wang

With the development of multimodal large language models (MLLMs) technology,
its general capabilities are increasingly powerful. To evaluate the various
abilities of MLLMs, numerous evaluation systems have emerged. But now there is
still a lack of a comprehensive method to evaluate MLLMs in the tasks related
to flowcharts, which are very important in daily life and work. We propose the
first comprehensive method, FlowCE, to assess MLLMs across various dimensions
for tasks related to flowcharts. It encompasses evaluating MLLMs' abilities in
Reasoning, Localization Recognition, Information Extraction, Logical
Verification, and Summarization on flowcharts. However, we find that even the
GPT4o model achieves only a score of 56.63. Among open-source models,
Phi-3-Vision obtained the highest score of 49.97. We hope that FlowCE can
contribute to future research on multimodal large language models (MLLMs) for
tasks based on flowcharts. We are open-sourcing this project:
\url{https://github.com/360AILAB-NLP/FlowCE}

摘要：隨著多模態大型語言模型（MLLM）技術的發展，其綜合能力越來越強大。為了評估 MLLM 的各種能力，已經出現了許多評估系統。但目前仍然缺乏一種綜合的方法來評估 MLLM 在與流程圖相關的任務中，這在日常生活和工作中非常重要。我們提出了第一個綜合性方法 FlowCE，用於評估 MLLM 在與流程圖相關的任務的各個維度。它包括評估 MLLM 在流程圖上的推理、定位識別、資訊萃取、邏輯驗證和摘要的能力。然而，我們發現即使是 GPT4o 模型也只獲得了 56.63 的分數。在開源模型中，Phi-3-Vision 獲得了 49.97 的最高分。我們希望 FlowCE 能夠為基於流程圖任務的多模態大型語言模型 (MLLM) 的未來研究做出貢獻。我們開放了這個專案：\url{https://github.com/360AILAB-NLP/FlowCE}

##### **Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection**
2406.10052v1 by Haoyu Wang, Guoqiang Hu, Guodong Lin, Wei-Qiang Zhang, Jian Li

As a robust and large-scale multilingual speech recognition model, Whisper
has demonstrated impressive results in many low-resource and
out-of-distribution scenarios. However, its encoder-decoder structure hinders
its application to streaming speech recognition. In this paper, we introduce
Simul-Whisper, which uses the time alignment embedded in Whisper's
cross-attention to guide auto-regressive decoding and achieve chunk-based
streaming ASR without any fine-tuning of the pre-trained model. Furthermore, we
observe the negative effect of the truncated words at the chunk boundaries on
the decoding results and propose an integrate-and-fire-based truncation
detection model to address this issue. Experiments on multiple languages and
Whisper architectures show that Simul-Whisper achieves an average absolute word
error rate degradation of only 1.46% at a chunk size of 1 second, which
significantly outperforms the current state-of-the-art baseline.

摘要：作為一個強大且大規模的多語言語音辨識模型，Whisper 在許多低資源和分佈外場景中已展現出令人印象深刻的成果。然而，其編碼器-解碼器結構阻礙了它在串流語音辨識中的應用。在本文中，我們介紹了 Simul-Whisper，它使用嵌入在 Whisper 的交叉注意力中的時間對齊來引導自迴歸解碼，並在不微調預訓練模型的情況下實現基於區塊的串流 ASR。此外，我們觀察到在區塊邊界處截斷的詞對解碼結果的負面影響，並提出了一個基於積分和發射的截斷檢測模型來解決這個問題。在多種語言和 Whisper 架構上的實驗表明，Simul-Whisper 在 1 秒的區塊大小下實現了僅 1.46% 的平均絕對詞錯誤率下降，這顯著優於當前最先進的基線。

##### **Bridging the Communication Gap: Artificial Agents Learning Sign Language through Imitation**
2406.10043v1 by Federico Tavella, Aphrodite Galata, Angelo Cangelosi

Artificial agents, particularly humanoid robots, interact with their
environment, objects, and people using cameras, actuators, and physical
presence. Their communication methods are often pre-programmed, limiting their
actions and interactions. Our research explores acquiring non-verbal
communication skills through learning from demonstrations, with potential
applications in sign language comprehension and expression. In particular, we
focus on imitation learning for artificial agents, exemplified by teaching a
simulated humanoid American Sign Language. We use computer vision and deep
learning to extract information from videos, and reinforcement learning to
enable the agent to replicate observed actions. Compared to other methods, our
approach eliminates the need for additional hardware to acquire information. We
demonstrate how the combination of these different techniques offers a viable
way to learn sign language. Our methodology successfully teaches 5 different
signs involving the upper body (i.e., arms and hands). This research paves the
way for advanced communication skills in artificial agents.

摘要：人工代理，尤其是類人機器人，透過相機、致動器和物理存在與其環境、物體和人互動。他們的溝通方式通常是預先編程好的，限制了他們的動作和互動。我們的研究探討透過示範學習來獲得非語言溝通技能，在手語理解和表達方面具有潛在應用。特別是，我們專注於人工代理的模仿學習，例如教導模擬類人美國手語。我們使用電腦視覺和深度學習從影片中提取資訊，並使用強化學習讓代理複製觀察到的動作。與其他方法相比，我們的做法消除了獲取資訊所需的額外硬體。我們展示了這些不同技術的結合如何提供學習手語的可行方法。我們的教學法成功地教授了 5 個不同的涉及上半身（即手臂和手）的手勢。這項研究為人工代理的進階溝通技能鋪平了道路。

##### **FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain**
2406.10040v1 by Jin Liu, Steffen Thoma

This paper describes the inference system of FZI-WIM at the SemEval-2024 Task
2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system
utilizes the chain of thought (CoT) paradigm to tackle this complex reasoning
problem and further improves the CoT performance with self-consistency. Instead
of greedy decoding, we sample multiple reasoning chains with the same prompt
and make the final verification with majority voting. The self-consistent CoT
system achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90
(3rd), and consistency score of 0.73 (12th). We release the code and data
publicly https://github.com/jens5588/FZI-WIM-NLI4CT.

摘要：這篇論文描述了 FZI-WIM 在 SemEval-2024 任務 2：臨床試驗的安全生物醫學自然語言推論中的推論系統。我們的系統利用思考鏈（CoT）範例來解決這個複雜的推理問題，並進一步透過自洽性來提升 CoT 的效能。我們使用相同的提示取樣多個推理鏈，而非貪婪解碼，並透過多數決進行最終驗證。自洽的 CoT 系統達到基線 F1 分數 0.80（第 1 名）、忠實度分數 0.90（第 3 名）和一致性分數 0.73（第 12 名）。我們公開發布程式碼和資料 https://github.com/jens5588/FZI-WIM-NLI4CT。

##### **Intepretative Deep Learning using Domain Adaptation for Fluorescence Spectroscopy**
2406.10031v1 by Umberto Michelucci, Francesca Venturini

Fluorescence spectroscopy is a fundamental tool in life sciences and
chemistry, widely used for applications such as environmental monitoring, food
quality control, and biomedical diagnostics. However, analysis of spectroscopic
data with deep learning, in particular of fluorescence excitation-emission
matrices (EEMs), presents significant challenges due mainly to the typically
small and sparse datasets available. Furthermore, the analysis of EEMs is
difficult due to their high dimensionality and overlapping spectral features.
This study proposes a new approach that exploits domain adaptation with
pretrained vision models, alongside a novel interpretability algorithm to
address these challenges. Thanks to specialised feature engineering of the
neural networks described in this work, we are now able to provide deeper and
meaningful insights into the physico-chemical processes underlying the data.
The proposed approach is demonstrated through the analysis of the oxidation
process in extra virgin olive oil (EVOO), showing its effectiveness in
predicting quality indicators and identifying relevant spectral bands. This
work describes significantly innovative results in the use of deep learning for
spectroscopy, transforming it from a black box into a tool for understanding
complex biological and chemical processes.

摘要：螢光光譜法是生命科學和化學中的一項基本工具，廣泛用於環境監測、食品品質控管和生物醫學診斷等應用中。然而，光譜資料的分析使用深度學習，特別是螢光激發發射矩陣 (EEM)，由於通常可取得的資料集小且稀疏，因此會產生重大挑戰。此外，由於 EEM 的高維度和重疊的光譜特徵，因此分析 EEM 十分困難。本研究提出了一種新的方法，利用預訓練視覺模型的領域適應，以及一種新穎的可解釋性演算法來解決這些挑戰。由於本研究中描述的神經網路的專業特徵工程，我們現在能夠提供更深入且有意義的見解，了解資料背後的物理化學過程。所提出的方法透過分析特級初榨橄欖油 (EVOO) 的氧化過程來證明，顯示其在預測品質指標和識別相關光譜波段方面的有效性。本研究描述了光譜學中使用深度學習的顯著創新成果，將其從黑盒子轉變為一種理解複雜生物和化學過程的工具。

##### **Deep Bayesian Active Learning for Preference Modeling in Large Language Models**
2406.10023v1 by Luckeciano C. Melo, Panagiotis Tigas, Alessandro Abate, Yarin Gal

Leveraging human preferences for steering the behavior of Large Language
Models (LLMs) has demonstrated notable success in recent years. Nonetheless,
data selection and labeling are still a bottleneck for these systems,
particularly at large scale. Hence, selecting the most informative points for
acquiring human feedback may considerably reduce the cost of preference
labeling and unleash the further development of LLMs. Bayesian Active Learning
provides a principled framework for addressing this challenge and has
demonstrated remarkable success in diverse settings. However, previous attempts
to employ it for Preference Modeling did not meet such expectations. In this
work, we identify that naive epistemic uncertainty estimation leads to the
acquisition of redundant samples. We address this by proposing the Bayesian
Active Learner for Preference Modeling (BAL-PM), a novel stochastic acquisition
policy that not only targets points of high epistemic uncertainty according to
the preference model but also seeks to maximize the entropy of the acquired
prompt distribution in the feature space spanned by the employed LLM. Notably,
our experiments demonstrate that BAL-PM requires 33% to 68% fewer preference
labels in two popular human preference datasets and exceeds previous stochastic
Bayesian acquisition policies.

摘要：近年來，利用人類偏好來引導大型語言模型 (LLM) 的行為已展現出顯著的成功。儘管如此，資料選取和標記仍然是這些系統的瓶頸，特別是在大規模的情況下。因此，選擇最有資訊性的點來獲取人類回饋可能大幅降低偏好標記的成本，並釋放 LLM 的進一步發展。貝氏主動學習提供了一個解決此挑戰的原則性架構，並已在各種設定中展現出顯著的成功。然而，先前嘗試將其用於偏好建模並未達到此類預期。在這項工作中，我們發現天真的認識論不確定性估計導致重複樣本的獲取。我們透過提出偏好建模的貝氏主動學習器 (BAL-PM) 來解決此問題，這是一種新穎的隨機獲取政策，它不僅根據偏好模型針對認識論不確定性高的點，還尋求最大化所採用 LLM 所跨越特徵空間中已獲取提示分佈的熵。值得注意的是，我們的實驗表明，BAL-PM 在兩個流行的人類偏好資料集中需要減少 33% 到 68% 的偏好標籤，並且超過先前的隨機貝氏獲取政策。

##### **Group and Shuffle: Efficient Structured Orthogonal Parametrization**
2406.10019v1 by Mikhail Gorbunov, Nikolay Yudin, Vera Soboleva, Aibek Alanov, Alexey Naumov, Maxim Rakhuba

The increasing size of neural networks has led to a growing demand for
methods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm
was introduced that uses orthogonal matrices for adapting the weights of a
pretrained model. In this paper, we introduce a new class of structured
matrices, which unifies and generalizes structured classes from previous works.
We examine properties of this class and build a structured orthogonal
parametrization upon it. We then use this parametrization to modify the
orthogonal fine-tuning framework, improving parameter and computational
efficiency. We empirically validate our method on different domains, including
adapting of text-to-image diffusion models and downstream task fine-tuning in
language modeling. Additionally, we adapt our construction for orthogonal
convolutions and conduct experiments with 1-Lipschitz neural networks.

摘要：神經網路規模的持續擴大，導致對有效微調方法的需求日益增加。最近，一種正交微調範例被引入，它使用正交矩陣來調整預訓練模型的權重。在本文中，我們引入了一類新的結構化矩陣，它統一並概括了先前工作的結構化類別。我們檢查了此類別的屬性，並在它上面建立了一個結構化的正交參數化。然後，我們使用這個參數化來修改正交微調框架，從而提高參數和計算效率。我們在不同的領域（包括文本到影像擴散模型的調整和語言建模的下游任務微調）中對我們的模型進行了實證驗證。此外，我們調整了正交卷積的構造，並對 1-Lipschitz 神經網路進行了實驗。

##### **Beyond Slow Signs in High-fidelity Model Extraction**
2406.10011v1 by Hanna Foerster, Robert Mullins, Ilia Shumailov, Jamie Hayes

Deep neural networks, costly to train and rich in intellectual property
value, are increasingly threatened by model extraction attacks that compromise
their confidentiality. Previous attacks have succeeded in reverse-engineering
model parameters up to a precision of float64 for models trained on random data
with at most three hidden layers using cryptanalytical techniques. However, the
process was identified to be very time consuming and not feasible for larger
and deeper models trained on standard benchmarks. Our study evaluates the
feasibility of parameter extraction methods of Carlini et al. [1] further
enhanced by Canales-Mart\'inez et al. [2] for models trained on standard
benchmarks. We introduce a unified codebase that integrates previous methods
and reveal that computational tools can significantly influence performance. We
develop further optimisations to the end-to-end attack and improve the
efficiency of extracting weight signs by up to 14.8 times compared to former
methods through the identification of easier and harder to extract neurons.
Contrary to prior assumptions, we identify extraction of weights, not
extraction of weight signs, as the critical bottleneck. With our improvements,
a 16,721 parameter model with 2 hidden layers trained on MNIST is extracted
within only 98 minutes compared to at least 150 minutes previously. Finally,
addressing methodological deficiencies observed in previous studies, we propose
new ways of robust benchmarking for future model extraction attacks.

摘要：深度神经网络在训练上成本高昂，且富含知识产权价值，越来越受到模型提取攻击的威胁，而这些攻击会损害其机密性。之前的攻击成功地对训练在随机数据上的模型进行逆向工程，将模型参数精度提升至 float64，最多使用三层隐藏层，并采用密码分析技术。然而，这一过程被认为非常耗时，并且对于在标准基准上训练的更大、更深层次的模型来说不可行。我们的研究评估了 Carlini 等人提出的参数提取方法的可行性 [1]，该方法得到了 Canales-Martínez 等人的进一步增强 [2]，用于在标准基准上训练的模型。我们引入了一个统一的代码库，集成了以前的方法，并揭示了计算工具可以显著影响性能。我们进一步优化了端到端攻击，并通过识别更容易和更难提取的神经元，将提取权重符号的效率提高了 14.8 倍，与以前的方法相比。与先前的假设相反，我们认为提取权重，而不是提取权重符号，是关键瓶颈。通过我们的改进，在 MNIST 上训练的具有 2 个隐藏层的 16,721 参数模型仅在 98 分钟内提取，而之前至少需要 150 分钟。最后，针对以前的研究中观察到的方法缺陷，我们提出了针对未来模型提取攻击的稳健基准测试的新方法。

##### **Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models**
2406.09994v1 by Manas Jhalani, Annervaz K M, Pushpak Bhattacharyya

In the realm of multimodal tasks, Visual Question Answering (VQA) plays a
crucial role by addressing natural language questions grounded in visual
content. Knowledge-Based Visual Question Answering (KBVQA) advances this
concept by adding external knowledge along with images to respond to questions.
We introduce an approach for KBVQA, augmenting the existing vision-language
transformer encoder-decoder (OFA) model. Our main contribution involves
enhancing questions by incorporating relevant external knowledge extracted from
knowledge graphs, using a dynamic triple extraction method. We supply a
flexible number of triples from the knowledge graph as context, tailored to
meet the requirements for answering the question. Our model, enriched with
knowledge, demonstrates an average improvement of 4.75\% in Exact Match Score
over the state-of-the-art on three different KBVQA datasets. Through
experiments and analysis, we demonstrate that furnishing variable triples for
each question improves the reasoning capabilities of the language model in
contrast to supplying a fixed number of triples. This is illustrated even for
recent large language models. Additionally, we highlight the model's
generalization capability by showcasing its SOTA-beating performance on a small
dataset, achieved through straightforward fine-tuning.

摘要：在多模态任务领域，视觉问答（VQA）通过解决基于视觉内容的自然语言问题，扮演着至关重要的角色。基于知识的视觉问答（KBVQA）通过添加外部知识以及图像来回答问题，从而推进了这一概念。我们引入了一种用于 KBVQA 的方法，增强了现有的视觉语言 transformer 编码器解码器 (OFA) 模型。我们的主要贡献涉及通过使用动态三元组提取方法，整合从知识图谱中提取的相关外部知识来增强问题。我们提供来自知识图谱的灵活数量的三元组作为上下文，以满足回答问题的要求。我们经过知识丰富的模型在三个不同的 KBVQA 数据集上，在精确匹配分数方面展示了比最先进水平平均提高 4.75%。通过实验和分析，我们证明为每个问题提供可变三元组提高了语言模型的推理能力，这与提供固定数量的三元组形成对比。即使对于最近的大型语言模型，这一点也得到了说明。此外，我们通过展示模型在小型数据集上的 SOTA 击败性能，突出了模型的泛化能力，这是通过直接微调实现的。

##### **Details Make a Difference: Object State-Sensitive Neurorobotic Task Planning**
2406.09988v1 by Xiaowen Sun, Xufeng Zhao, Jae Hee Lee, Wenhao Lu, Matthias Kerzel, Stefan Wermter

The state of an object reflects its current status or condition and is
important for a robot's task planning and manipulation. However, detecting an
object's state and generating a state-sensitive plan for robots is challenging.
Recently, pre-trained Large Language Models (LLMs) and Vision-Language Models
(VLMs) have shown impressive capabilities in generating plans. However, to the
best of our knowledge, there is hardly any investigation on whether LLMs or
VLMs can also generate object state-sensitive plans. To study this, we
introduce an Object State-Sensitive Agent (OSSA), a task-planning agent
empowered by pre-trained neural networks. We propose two methods for OSSA: (i)
a modular model consisting of a pre-trained vision processing module (dense
captioning model, DCM) and a natural language processing model (LLM), and (ii)
a monolithic model consisting only of a VLM. To quantitatively evaluate the
performances of the two methods, we use tabletop scenarios where the task is to
clear the table. We contribute a multimodal benchmark dataset that takes object
states into consideration. Our results show that both methods can be used for
object state-sensitive tasks, but the monolithic approach outperforms the
modular approach. The code for OSSA is available at
\url{https://github.com/Xiao-wen-Sun/OSSA}

摘要：物體的狀態反映其當前狀態或條件，對於機器人的任務規劃和操作非常重要。然而，偵測物體的狀態並為機器人產生狀態敏感的計畫是一項挑戰。最近，預先訓練的大語言模型 (LLM) 和視覺語言模型 (VLM) 在產生計畫方面展現了令人印象深刻的能力。然而，據我們所知，幾乎沒有任何研究探討 LLM 或 VLM 是否也能產生對物體狀態敏感的計畫。為了研究這一點，我們引入了物件狀態敏感代理 (OSSA)，這是一個由預先訓練的神經網路支援的任務規劃代理。我們為 OSSA 提出兩種方法：(i) 一個模組化模型，由預先訓練的視覺處理模組（密集式標題模型，DCM）和自然語言處理模型 (LLM) 組成，以及 (ii) 一個單一模型，僅由 VLM 組成。為了量化評估這兩種方法的效能，我們使用桌面場景，其中任務是清理桌面。我們提供了一個多模態基準資料集，其中考慮了物件狀態。我們的結果表明，這兩種方法都可以用於對物件狀態敏感的任務，但單一方法的表現優於模組化方法。OSSA 的程式碼可在 \url{https://github.com/Xiao-wen-Sun/OSSA} 取得

##### **Challenges in explaining deep learning models for data with biological variation**
2406.09981v1 by Lenka Tětková, Erik Schou Dreier, Robin Malm, Lars Kai Hansen

Much machine learning research progress is based on developing models and
evaluating them on a benchmark dataset (e.g., ImageNet for images). However,
applying such benchmark-successful methods to real-world data often does not
work as expected. This is particularly the case for biological data where we
expect variability at multiple time and spatial scales. In this work, we are
using grain data and the goal is to detect diseases and damages. Pink fusarium,
skinned grains, and other diseases and damages are key factors in setting the
price of grains or excluding dangerous grains from food production. Apart from
challenges stemming from differences of the data from the standard toy
datasets, we also present challenges that need to be overcome when explaining
deep learning models. For example, explainability methods have many
hyperparameters that can give different results, and the ones published in the
papers do not work on dissimilar images. Other challenges are more general:
problems with visualization of the explanations and their comparison since the
magnitudes of their values differ from method to method. An open fundamental
question also is: How to evaluate explanations? It is a non-trivial task
because the "ground truth" is usually missing or ill-defined. Also, human
annotators may create what they think is an explanation of the task at hand,
yet the machine learning model might solve it in a different and perhaps
counter-intuitive way. We discuss several of these challenges and evaluate
various post-hoc explainability methods on grain data. We focus on robustness,
quality of explanations, and similarity to particular "ground truth"
annotations made by experts. The goal is to find the methods that overall
perform well and could be used in this challenging task. We hope the proposed
pipeline will be used as a framework for evaluating explainability methods in
specific use cases.

摘要：許多機器學習研究進展都是建立在開發模型，並在基準資料集（例如，用於影像的 ImageNet）上評估模型。然而，將這些基準成功的模型應用於真實世界資料時，通常無法如預期般運作。這特別適用於生物資料，因為我們預期會在多重時間和空間尺度中出現變異性。在這項工作中，我們使用穀物資料，目標是偵測疾病和損害。粉紅黴菌、剝皮穀物和其他疾病和損害是設定穀物價格或排除危險穀物於食品生產中的關鍵因素。除了源自資料與標準玩具資料集差異的挑戰外，我們也提出在解釋深度學習模型時需要克服的挑戰。例如，可解釋性方法有許多超參數，可能會產生不同的結果，且發表在論文中的方法無法用於相異的影像。其他挑戰較為普遍：解釋的視覺化和比較有問題，因為它們數值的大小因方法而異。一個開放的基本問題是：如何評估解釋？這項任務並不容易，因為「基本事實」通常不存在或定義不明確。此外，人類註解者可能會建立他們認為是手邊任務解釋的內容，但機器學習模型可能會以不同且可能違反直覺的方式解決任務。我們討論了其中幾個挑戰，並在穀物資料上評估各種事後可解釋性方法。我們專注於穩健性、解釋的品質，以及與專家所做的特定「基本事實」註解的相似性。目標是找出整體表現良好的方法，並能用於這項具有挑戰性的任務。我們希望所提出的流程將被用作在特定使用案例中評估可解釋性方法的架構。

##### **HIRO: Hierarchical Information Retrieval Optimization**
2406.09979v1 by Krish Goel, Mahek Chandak

Large Language Models (LLMs) excel in natural language tasks but face
limitations due to static training datasets, resulting in outdated or
contextually shallow responses. Retrieval-Augmented Generation (RAG) addresses
this by integrating real-time external knowledge, enhancing model accuracy and
credibility, especially for knowledge-intensive tasks. However, RAG-enhanced
LLMs struggle with long contexts, causing them to "choke" on information
overload, compromising response quality. Recent RAG applications use
hierarchical data structures for storing documents, organized at various levels
of summarization and information density. In this context, we introduce HIRO
(Hierarchical Information Retrieval Optimization), a novel querying approach
for RAG applications using hierarchical structures for storing documents. HIRO
employs DFS-based recursive similarity score calculation and branch pruning to
minimize the context returned to the LLM without informational loss. HIRO
outperforms existing querying mechanisms on the NarrativeQA dataset by an
absolute performance gain of 10.85%.

摘要：大型語言模型 (LLM) 在自然語言任務中表現出色，但由於訓練資料集為靜態，因此面臨限制，導致回應過時或脈絡淺薄。檢索增強生成 (RAG) 透過整合即時外部知識來解決此問題，增強模型的準確性和可信度，特別是對於知識密集型任務。然而，RAG 增強的 LLM 難以處理長脈絡，導致它們在資訊超載時「窒息」，損害回應品質。最近的 RAG 應用程式使用階層式資料結構來儲存文件，並在各種摘要和資訊密度層級中組織。在此脈絡下，我們介紹了 HIRO（階層式資訊檢索最佳化），這是一種新穎的查詢方法，用於 RAG 應用程式，使用階層式結構來儲存文件。HIRO 採用基於 DFS 的遞迴相似性分數計算和分支剪枝，以在不造成資訊遺失的情況下，將回傳至 LLM 的脈絡降至最低。HIRO 在 NarrativeQA 資料集上優於現有的查詢機制，絕對效能提升了 10.85%。

##### **Disentangling Dialect from Social Bias via Multitask Learning to Improve Fairness**
2406.09977v1 by Maximilian Spliethöver, Sai Nikhil Menon, Henning Wachsmuth

Dialects introduce syntactic and lexical variations in language that occur in
regional or social groups. Most NLP methods are not sensitive to such
variations. This may lead to unfair behavior of the methods, conveying negative
bias towards dialect speakers. While previous work has studied dialect-related
fairness for aspects like hate speech, other aspects of biased language, such
as lewdness, remain fully unexplored. To fill this gap, we investigate
performance disparities between dialects in the detection of five aspects of
biased language and how to mitigate them. To alleviate bias, we present a
multitask learning approach that models dialect language as an auxiliary task
to incorporate syntactic and lexical variations. In our experiments with
African-American English dialect, we provide empirical evidence that
complementing common learning approaches with dialect modeling improves their
fairness. Furthermore, the results suggest that multitask learning achieves
state-of-the-art performance and helps to detect properties of biased language
more reliably.

摘要：方言會在語言中引入句法和詞彙變化，這些變化發生在區域或社會群體中。大多數 NLP 方法對此類變化不敏感。這可能會導致這些方法產生不公平的行為，對方言使用者傳達負面偏見。雖然先前的研究已經探討了與方言相關的公平性，例如仇恨言論，但有偏見語言的其他方面，例如淫穢，仍然完全未被探索。為了填補這一空白，我們研究了方言在五個有偏見語言方面的檢測中表現出的差異，以及如何減輕這些差異。為了減輕偏見，我們提出了一種多任務學習方法，將方言語言建模為輔助任務，以納入句法和詞彙變化。在我們對非裔美國英語方言的實驗中，我們提供了經證據證明，用方言建模補充常見的學習方法可以提高它們的公平性。此外，結果表明多任務學習達到了最先進的性能，並有助於更可靠地檢測有偏見語言的屬性。

##### **Robust Model-Based Reinforcement Learning with an Adversarial Auxiliary Model**
2406.09976v1 by Siemen Herremans, Ali Anwar, Siegfried Mercelis

Reinforcement learning has demonstrated impressive performance in various
challenging problems such as robotics, board games, and classical arcade games.
However, its real-world applications can be hindered by the absence of
robustness and safety in the learned policies. More specifically, an RL agent
that trains in a certain Markov decision process (MDP) often struggles to
perform well in nearly identical MDPs. To address this issue, we employ the
framework of Robust MDPs (RMDPs) in a model-based setting and introduce a novel
learned transition model. Our method specifically incorporates an auxiliary
pessimistic model, updated adversarially, to estimate the worst-case MDP within
a Kullback-Leibler uncertainty set. In comparison to several existing works,
our work does not impose any additional conditions on the training environment,
such as the need for a parametric simulator. To test the effectiveness of the
proposed pessimistic model in enhancing policy robustness, we integrate it into
a practical RL algorithm, called Robust Model-Based Policy Optimization
(RMBPO). Our experimental results indicate a notable improvement in policy
robustness on high-dimensional MuJoCo control tasks, with the auxiliary model
enhancing the performance of the learned policy in distorted MDPs. We further
explore the learned deviation between the proposed auxiliary world model and
the nominal model, to examine how pessimism is achieved. By learning a
pessimistic world model and demonstrating its role in improving policy
robustness, our research contributes towards making (model-based) RL more
robust.

摘要：強化學習已在各種具有挑戰性的問題中展現出令人印象深刻的表現，例如機器人技術、棋盤遊戲和經典街機遊戲。然而，其真實世界的應用可能會受到學習政策中缺乏穩健性和安全性所阻礙。更具體地說，在特定馬可夫決策過程中 (MDP) 進行訓練的 RL 代理通常難以在幾乎相同的 MDP 中表現良好。為了解決這個問題，我們在基於模型的設置中採用穩健 MDP (RMDP) 的框架，並引入一個新穎的學習轉換模型。我們的模型特別結合了一個輔助悲觀模型，以對抗的方式進行更新，以估計 Kullback-Leibler 不確定性集內的最悪情況 MDP。與現有的幾項工作相比，我們的模型不會對訓練環境施加任何額外的條件，例如對參數化模擬器的需求。為了測試所提出的悲觀模型在增強策略穩健性方面的有效性，我們將其整合到一個實用的 RL 演算法中，稱為穩健基於模型的策略最佳化 (RMBPO)。我們的實驗結果表明，在高維 MuJoCo 控制任務中，策略穩健性有了顯著的提升，輔助模型增強了學習策略在扭曲 MDP 中的表現。我們進一步探討了所提出的輔助世界模型和標稱模型之間的學習偏差，以檢視如何實現悲觀主義。透過學習悲觀的世界模型並展示其在改善策略穩健性中的作用，我們的研究有助於讓（基於模型的）RL 更加穩健。

##### **A Better LLM Evaluator for Text Generation: The Impact of Prompt Output Sequencing and Optimization**
2406.09972v1 by KuanChao Chu, Yi-Pei Chen, Hideki Nakayama

This research investigates prompt designs of evaluating generated texts using
large language models (LLMs). While LLMs are increasingly used for scoring
various inputs, creating effective prompts for open-ended text evaluation
remains challenging due to model sensitivity and subjectivity in evaluation of
text generation. Our study experimented with different prompt structures,
altering the sequence of output instructions and including explanatory reasons.
We found that the order of presenting reasons and scores significantly
influences LLMs' scoring, with a different level of rule understanding in the
prompt. An additional optimization may enhance scoring alignment if sufficient
data is available. This insight is crucial for improving the accuracy and
consistency of LLM-based evaluations.

摘要：本研究探討使用大型語言模型 (LLM) 評估生成文字的提示設計。儘管 LLM 愈來愈常被用於評分各種輸入，但由於模型敏感度和文字生成評估中的主觀性，為開放式文字評估建立有效的提示仍然具有挑戰性。我們的研究實驗了不同的提示結構，改變輸出指令的順序並納入說明性理由。我們發現提示中呈現理由和分數的順序會顯著影響 LLM 的評分，而提示中對規則的理解程度也不同。如果資料充足，額外的最佳化可能會增強評分的一致性。這個見解對於提高基於 LLM 的評估的準確性和一致性至關重要。

##### **Bag of Lies: Robustness in Continuous Pre-training BERT**
2406.09967v1 by Ine Gevers, Walter Daelemans

This study aims to acquire more insights into the continuous pre-training
phase of BERT regarding entity knowledge, using the COVID-19 pandemic as a case
study. Since the pandemic emerged after the last update of BERT's pre-training
data, the model has little to no entity knowledge about COVID-19. Using
continuous pre-training, we control what entity knowledge is available to the
model. We compare the baseline BERT model with the further pre-trained variants
on the fact-checking benchmark Check-COVID. To test the robustness of
continuous pre-training, we experiment with several adversarial methods to
manipulate the input data, such as training on misinformation and shuffling the
word order until the input becomes nonsensical. Surprisingly, our findings
reveal that these methods do not degrade, and sometimes even improve, the
model's downstream performance. This suggests that continuous pre-training of
BERT is robust against misinformation. Furthermore, we are releasing a new
dataset, consisting of original texts from academic publications in the
LitCovid repository and their AI-generated false counterparts.

摘要：本研究旨在透過以 COVID-19 疫情為案例研究，進一步了解 BERT 持續預訓練階段中關於實體知識的見解。由於疫情在 BERT 預訓練資料的最後一次更新後才出現，因此該模型對於 COVID-19 的實體知識幾乎沒有或完全沒有。透過持續預訓練，我們控制模型可取得的實體知識。我們在事實查核基準 Check-COVID 上比較基準 BERT 模型與進一步預訓練的變體。為了測試持續預訓練的穩健性，我們嘗試使用多種對抗方法來操縱輸入資料，例如針對錯誤資訊進行訓練，以及將字詞順序打亂，直到輸入變得毫無意義為止。令人驚訝的是，我們的研究結果顯示，這些方法並不會降低模型的下游效能，有時甚至會提升效能。這表示 BERT 的持續預訓練對於錯誤資訊具有穩健性。此外，我們發布了一個新資料集，其中包含 LitCovid 存放庫中學術出版品的原始文字及其 AI 生成的錯誤對應文字。

##### **Outlier detection in maritime environments using AIS data and deep recurrent architectures**
2406.09966v1 by Constantine Maganaris, Eftychios Protopapadakis, Nikolaos Doulamis

A methodology based on deep recurrent models for maritime surveillance, over
publicly available Automatic Identification System (AIS) data, is presented in
this paper. The setup employs a deep Recurrent Neural Network (RNN)-based
model, for encoding and reconstructing the observed ships' motion patterns. Our
approach is based on a thresholding mechanism, over the calculated errors
between observed and reconstructed motion patterns of maritime vessels.
Specifically, a deep-learning framework, i.e. an encoder-decoder architecture,
is trained using the observed motion patterns, enabling the models to learn and
predict the expected trajectory, which will be compared to the effective ones.
Our models, particularly the bidirectional GRU with recurrent dropouts,
showcased superior performance in capturing the temporal dynamics of maritime
data, illustrating the potential of deep learning to enhance maritime
surveillance capabilities. Our work lays a solid foundation for future research
in this domain, highlighting a path toward improved maritime safety through the
innovative application of technology.

摘要：本文提出了一種基於深度遞迴模型的海事監控方法，該方法使用公開的自動識別系統 (AIS) 數據。該設置採用基於深度遞迴神經網路 (RNN) 的模型，用於編碼和重建觀測到的船舶運動模式。我們的做法基於一個閾值機制，用於計算海事船舶觀測和重建運動模式之間的誤差。具體來說，一個深度學習框架，即編碼器-解碼器架構，使用觀測到的運動模式進行訓練，使模型能夠學習和預測預期的軌跡，並將其與實際軌跡進行比較。我們的模型，特別是帶有遞迴中斷的雙向 GRU，在捕捉海事數據的時間動態方面表現出卓越的性能，說明了深度學習增強海事監控能力的潛力。我們的研究為該領域的未來研究奠定了堅實的基礎，強調了通過創新應用技術改善海事安全的路徑。

##### **ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation**
2406.09961v1 by Chufan Shi, Cheng Yang, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, Gongye Liu, Xiaomei Nie, Deng Cai, Yujiu Yang

We introduce a new benchmark, ChartMimic, aimed at assessing the
visually-grounded code generation capabilities of large multimodal models
(LMMs). ChartMimic utilizes information-intensive visual charts and textual
instructions as inputs, requiring LMMs to generate the corresponding code for
chart rendering. ChartMimic includes 1,000 human-curated (figure, instruction,
code) triplets, which represent the authentic chart use cases found in
scientific papers across various domains(e.g., Physics, Computer Science,
Economics, etc). These charts span 18 regular types and 4 advanced types,
diversifying into 191 subcategories. Furthermore, we propose multi-level
evaluation metrics to provide an automatic and thorough assessment of the
output code and the rendered charts. Unlike existing code generation
benchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to
harmonize a blend of cognitive capabilities, encompassing visual understanding,
code generation, and cross-modal reasoning. The evaluation of 3 proprietary
models and 11 open-weight models highlights the substantial challenges posed by
ChartMimic. Even the advanced GPT-4V, Claude-3-opus only achieve an average
score of 73.2 and 53.7, respectively, indicating significant room for
improvement. We anticipate that ChartMimic will inspire the development of
LMMs, advancing the pursuit of artificial general intelligence.

摘要：<paragraph>我們推出了新的基準 ChartMimic，旨在評估大型多模態模型 (LMM) 的視覺基礎程式碼生成能力。ChartMimic 利用資訊密集的視覺圖表和文字說明作為輸入，要求 LMM 產生對應的圖表繪製程式碼。ChartMimic 包含 1,000 個由人策劃的 (圖形、說明、程式碼) 三元組，代表在各個領域 (例如物理、電腦科學、經濟學等) 的科學論文中發現的真實圖表使用案例。這些圖表涵蓋 18 種常規類型和 4 種進階類型，分為 191 個子類別。此外，我們提出了多層級評估指標，以自動且徹底地評估輸出程式碼和繪製的圖表。與現有的程式碼生成基準不同，ChartMimic 強調評估 LMM 將視覺理解、程式碼生成和跨模態推理等認知能力融會貫通的能力。對 3 個專有模型和 11 個開放權重模型的評估突顯了 ChartMimic 帶來的重大挑戰。即使是進階的 GPT-4V 和 Claude-3-opus 也只分別達到 73.2 和 53.7 的平均分數，表示有很大的進步空間。我們預期 ChartMimic 將激勵 LMM 的發展，推動人工通用智慧的研究。</paragraph>

##### **DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning**
2406.09953v1 by Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Lingyue Guo, Ping Luo, Yanfeng Lu

Dual-arm robots offer enhanced versatility and efficiency over single-arm
counterparts by enabling concurrent manipulation of multiple objects or
cooperative execution of tasks using both arms. However, effectively
coordinating the two arms for complex long-horizon tasks remains a significant
challenge. Existing task planning methods predominantly focus on single-arm
robots or rely on predefined bimanual operations, failing to fully leverage the
capabilities of dual-arm systems. To address this limitation, we introduce
DAG-Plan, a structured task planning framework tailored for dual-arm robots.
DAG-Plan harnesses large language models (LLMs) to decompose intricate tasks
into actionable sub-tasks represented as nodes within a directed acyclic graph
(DAG). Critically, DAG-Plan dynamically assigns these sub-tasks to the
appropriate arm based on real-time environmental observations, enabling
parallel and adaptive execution. We evaluate DAG-Plan on the novel Dual-Arm
Kitchen Benchmark, comprising 9 sequential tasks with 78 sub-tasks and 26
objects. Extensive experiments demonstrate the superiority of DAG-Plan over
directly using LLM to generate plans, achieving nearly 50% higher efficiency
compared to the single-arm task planning baseline and nearly double the success
rate of the dual-arm task planning baseline.

摘要：雙臂機器人透過同時操控多個物件或使用雙臂協同執行任務，提供比單臂機器人更高的靈活性與效率。然而，要有效協調雙臂以執行複雜且時間跨度長的任務，仍然是一項重大的挑戰。現有的任務規劃方法主要專注於單臂機器人，或依賴於預先定義的雙手操作，無法充分利用雙臂系統的能力。為了解決這個限制，我們引入了 DAG-Plan，一個專門為雙臂機器人量身打造的結構化任務規劃架構。DAG-Plan 利用大型語言模型 (LLM) 將複雜的任務分解成可操作的子任務，並將其表示為有向無環圖 (DAG) 中的節點。更重要的是，DAG-Plan 會根據即時的環境觀察動態地將這些子任務分配給適當的手臂，從而實現並行和自適應的執行。我們在新的雙臂廚房基準測試中評估了 DAG-Plan，其中包含 9 個順序任務、78 個子任務和 26 個物件。廣泛的實驗證明了 DAG-Plan 優於直接使用 LLM 來產生計畫，與單臂任務規劃基準線相比，效率提高了近 50%，而與雙臂任務規劃基準線相比，成功率幾乎提高了一倍。

##### **BiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval**
2406.09952v1 by Imanol Miranda, Ander Salaberria, Eneko Agirre, Gorka Azkune

Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe
are formulated as image-to-text retrieval problems, where, given an image, the
models need to select between the correct textual description and a synthetic
hard negative text. In this work we present the Bidirectional Vision-Language
Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic
hard negative image generated from the synthetic text, resulting in two
image-to-text retrieval examples (one for each image) and, more importantly,
two text-to-image retrieval examples (one for each text). Human annotators
filter out ill-formed examples ensuring the validity of the benchmark. The
experiments on BiVLC uncover a weakness of current multimodal models, as they
perform poorly in the text-to-image direction. In fact, when considering both
retrieval directions, the conclusions obtained in previous works change
significantly. In addition to the benchmark, we show that a contrastive model
trained using synthetic images and texts improves the state of the art in
SugarCrepe and in BiVLC for both retrieval directions. The gap to human
performance in BiVLC confirms that Vision-Language Compositionality is still a
challenging problem. BiVLC and code are available at
https://imirandam.github.io/BiVLC_project_page.

摘要：現有的視覺語言組合性 (VLC) 基準，例如 SugarCrepe，被制定為圖像到文字的檢索問題，其中，給定一個圖像，模型需要在正確的文字描述和一個合成的硬負面文字之間進行選擇。在這項工作中，我們提出了雙向視覺語言組合性 (BiVLC) 資料集。BiVLC 的新穎之處在於添加了一個由合成文字生成的合成硬負面圖像，從而產生兩個圖像到文字的檢索範例（每個圖像一個），更重要的是，兩個文字到圖像的檢索範例（每個文字一個）。人類註釋者會過濾掉格式不佳的範例，以確保基準的有效性。在 BiVLC 上的實驗揭示了當前多模態模型的弱點，因為它們在文字到圖像的方向上表現不佳。事實上，在考慮兩個檢索方向時，先前工作中得出的結論發生了顯著變化。除了基準之外，我們展示了一個使用合成圖像和文字訓練的對比模型，改善了 SugarCrepe 和 BiVLC 在兩個檢索方向上的最新技術。BiVLC 中與人類表現的差距證實了視覺語言組合性仍然是一個具有挑戰性的問題。BiVLC 和程式碼可在 https://imirandam.github.io/BiVLC_project_page 取得。

##### **An efficient text augmentation approach for contextualized Mandarin speech recognition**
2406.09950v1 by Naijun Zheng, Xucheng Wan, Kai Liu, Ziqing Du, Zhou Huan

Although contextualized automatic speech recognition (ASR) systems are
commonly used to improve the recognition of uncommon words, their effectiveness
is hindered by the inherent limitations of speech-text data availability. To
address this challenge, our study proposes to leverage extensive text-only
datasets and contextualize pre-trained ASR models using a straightforward
text-augmentation (TA) technique, all while keeping computational costs
minimal. In particular, to contextualize a pre-trained CIF-based ASR, we
construct a codebook using limited speech-text data. By utilizing a simple
codebook lookup process, we convert available text-only data into latent text
embeddings. These embeddings then enhance the inputs for the contextualized
ASR. Our experiments on diverse Mandarin test sets demonstrate that our TA
approach significantly boosts recognition performance. The top-performing
system shows relative CER improvements of up to 30% on rare words and 15%
across all words in general.

摘要：儘管情境化自動語音辨識 (ASR) 系統常被用於提升不常見字詞的辨識率，但其效能受到語音文字資料可用性的內在限制所阻礙。為了應對此挑戰，本研究提出利用廣泛的純文字資料集，並使用一種簡單的文字擴增 (TA) 技術來情境化預訓練的 ASR 模型，同時將運算成本降至最低。特別是，為了情境化一個預訓練的基於 CIF 的 ASR，我們使用有限的語音文字資料建構一個碼本。透過使用一個簡單的碼本查詢程序，我們將可用的純文字資料轉換為潛在文字嵌入。這些嵌入接著增強情境化 ASR 的輸入。我們在不同的國語測試集上的實驗證明，我們的 TA 方法顯著提升了辨識效能。效能最佳的系統在罕見字詞上展現出高達 30% 的相對 CER 改善，且整體上在所有字詞上展現出 15% 的改善。

##### **Neural Concept Binder**
2406.09949v1 by Wolfgang Stammer, Antonia Wüst, David Steinmann, Kristian Kersting

The challenge in object-based visual reasoning lies in generating descriptive
yet distinct concept representations. Moreover, doing this in an unsupervised
fashion requires human users to understand a model's learned concepts and
potentially revise false concepts. In addressing this challenge, we introduce
the Neural Concept Binder, a new framework for deriving discrete concept
representations resulting in what we term "concept-slot encodings". These
encodings leverage both "soft binding" via object-centric block-slot encodings
and "hard binding" via retrieval-based inference. The Neural Concept Binder
facilitates straightforward concept inspection and direct integration of
external knowledge, such as human input or insights from other AI models like
GPT-4. Additionally, we demonstrate that incorporating the hard binding
mechanism does not compromise performance; instead, it enables seamless
integration into both neural and symbolic modules for intricate reasoning
tasks, as evidenced by evaluations on our newly introduced CLEVR-Sudoku
dataset.

摘要：在基於物件的視覺推理中，挑戰在於產生具描述性且獨特的概念表徵。此外，以無監督的方式進行此操作需要人類使用者了解模型學習到的概念，並有可能修正錯誤的概念。在解決此挑戰時，我們引入了神經概念結合器，一個用於衍生離散概念表徵的新架構，產生我們稱之為「概念槽編碼」的結果。這些編碼透過物件為中心的區塊槽編碼利用「軟綁定」，並透過基於檢索的推論利用「硬綁定」。神經概念結合器促進直接的概念檢查和外部知識的直接整合，例如人類輸入或來自其他 AI 模型（例如 GPT-4）的見解。此外，我們證明了納入硬綁定機制並不會影響效能；相反地，它能將神經模組和符號模組無縫整合到複雜的推理任務中，正如我們新推出的 CLEVR-Sudoku 資料集上的評估所證明的那樣。

##### **BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages**
2406.09948v1 by Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, Víctor Gutiérrez-Basulto, Yazmín Ibáñez-García, Hwaran Lee, Shamsuddeen Hassan Muhammad, Kiwoong Park, Anar Sabuhi Rzayev, Nina White, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jose Camacho-Collados, Alice Oh

Large language models (LLMs) often lack culture-specific knowledge of daily
life, especially across diverse regions and non-English languages. Existing
benchmarks for evaluating LLMs' cultural sensitivities are limited to a single
language or collected from online sources such as Wikipedia, which do not
reflect the mundane everyday lifestyles of diverse regions. That is,
information about the food people eat for their birthday celebrations, spices
they typically use, musical instruments youngsters play, or the sports they
practice in school is common cultural knowledge but uncommon in easily
collected online sources, especially for underrepresented cultures. To address
this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate
LLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises
52.6k question-answer pairs from 16 countries/regions, in 13 different
languages, including low-resource ones such as Amharic, Assamese, Azerbaijani,
Hausa, and Sundanese. We construct the benchmark to include two formats of
questions: short-answer and multiple-choice. We show that LLMs perform better
for cultures that are highly represented online, with a maximum 57.34%
difference in GPT-4, the best-performing model, in the short-answer format. For
cultures represented by mid-to-high-resource languages, LLMs perform better in
their local languages, but for cultures represented by low-resource languages,
LLMs perform better in English than the local languages. We make our dataset
publicly available at: https://github.com/nlee0212/BLEnD.

摘要：大型語言模型（LLM）通常缺乏對日常生活的文化特定知識，尤其是在不同的地區和非英語語言中。現有的用於評估 LLM 文化敏感度的基準僅限於單一語言或從網上來源（例如維基百科）收集，而這些來源並不能反映不同地區的日常平淡生活方式。也就是說，人們在生日慶祝活動中吃的食物、他們通常使用的香料、年輕人演奏的樂器或他們在學校練習的運動是常見的文化知識，但在容易收集的網上來源中並不常見，尤其是對於代表性不足的文化。為了解決這個問題，我們引入了 BLEnD，這是一個手工製作的基準，旨在評估 LLM 在不同文化和語言中的日常知識。BLEnD 包含來自 16 個國家/地區的 52.6k 個問答對，使用 13 種不同的語言，包括阿姆哈拉語、阿薩姆語、亞塞拜然語、豪薩語和巽他語等資源匱乏的語言。我們構建基準以包含兩種格式的問題：簡答和多選。我們展示了 LLM 對在網上高度代表的文化表現得更好，在簡答格式中，表現最好的模型 GPT-4 的最大差異為 57.34%。對於由中等至高資源語言代表的文化，LLM 在其當地語言中表現得更好，但對於由資源匱乏的語言代表的文化，LLM 在英語中的表現優於當地語言。我們在 https://github.com/nlee0212/BLEnD 上公開我們的數據集。

##### **Experiments in News Bias Detection with Pre-Trained Neural Transformers**
2406.09938v1 by Tim Menzner, Jochen L. Leidner

The World Wide Web provides unrivalled access to information globally,
including factual news reporting and commentary. However, state actors and
commercial players increasingly spread biased (distorted) or fake (non-factual)
information to promote their agendas. We compare several large, pre-trained
language models on the task of sentence-level news bias detection and sub-type
classification, providing quantitative and qualitative results.

摘要：世界網際網路提供無與倫比的全球資訊存取，
包括事實新聞報導和評論。然而，國家行為者和
商業參與者日益散播有偏見（扭曲）或虛假（非事實）
資訊以宣傳他們的議程。我們比較數個大型、預先訓練過的
語言模型在句子層級新聞偏見偵測和子類型
分類的任務上，提供量化和定性結果。

##### **What Does it Take to Generalize SER Model Across Datasets? A Comprehensive Benchmark**
2406.09933v1 by Adham Ibrahim, Shady Shehata, Ajinkya Kulkarni, Mukhtar Mohamed, Muhammad Abdul-Mageed

Speech emotion recognition (SER) is essential for enhancing human-computer
interaction in speech-based applications. Despite improvements in specific
emotional datasets, there is still a research gap in SER's capability to
generalize across real-world situations. In this paper, we investigate
approaches to generalize the SER system across different emotion datasets. In
particular, we incorporate 11 emotional speech datasets and illustrate a
comprehensive benchmark on the SER task. We also address the challenge of
imbalanced data distribution using over-sampling methods when combining SER
datasets for training. Furthermore, we explore various evaluation protocols for
adeptness in the generalization of SER. Building on this, we explore the
potential of Whisper for SER, emphasizing the importance of thorough
evaluation. Our approach is designed to advance SER technology by integrating
speaker-independent methods.

摘要：語音情緒辨識 (SER) 對於增強以語音為基礎的應用程式中的人機互動至關重要。儘管特定情緒資料集有改善，但 SER 在真實世界情況下進行概括的能力仍存在研究差距。在本文中，我們探討了在不同情緒資料集之間概括 SER 系統的方法。特別是，我們納入了 11 個情緒語音資料集，並說明了 SER 任務的全面基準。我們還使用過取樣方法解決了在結合 SER 資料集進行訓練時資料分佈不平衡的挑戰。此外，我們探討了各種評估協定，以了解 SER 概括的熟練程度。在此基礎上，我們探討了 Whisper 在 SER 中的潛力，強調了徹底評估的重要性。我們的做法旨在透過整合與說話者無關的方法來推進 SER 技術。

##### **Personalized Speech Enhancement Without a Separate Speaker Embedding Model**
2406.09928v1 by Tanel Pärnamaa, Ando Saabas

Personalized speech enhancement (PSE) models can improve the audio quality of
teleconferencing systems by adapting to the characteristics of a speaker's
voice. However, most existing methods require a separate speaker embedding
model to extract a vector representation of the speaker from enrollment audio,
which adds complexity to the training and deployment process. We propose to use
the internal representation of the PSE model itself as the speaker embedding,
thereby avoiding the need for a separate model. We show that our approach
performs equally well or better than the standard method of using a pre-trained
speaker embedding model on noise suppression and echo cancellation tasks.
Moreover, our approach surpasses the ICASSP 2023 Deep Noise Suppression
Challenge winner by 0.15 in Mean Opinion Score.

摘要：個人化語音增強 (PSE) 模型能透過適應說話者聲音的特性來提升電話會議系統的音質。然而，現有方法大多需要一個獨立的說話者嵌入模型，從註冊音訊中萃取出說話者的向量表示，這增加了訓練和部署流程的複雜性。我們建議使用 PSE 模型本身的內部表示作為說話者嵌入，從而避免使用獨立模型的需要。我們展示了我們的做法在雜訊消除和迴音消除工作中，表現與使用預先訓練好的說話者嵌入模型的標準方法一樣好，甚至更好。此外，我們的做法在平均意見分數上超越了 ICASSP 2023 深度雜訊消除挑戰賽的優勝者 0.15。

##### **CliBench: Multifaceted Evaluation of Large Language Models in Clinical Decisions on Diagnoses, Procedures, Lab Tests Orders and Prescriptions**
2406.09923v1 by Mingyu Derek Ma, Chenchen Ye, Yu Yan, Xiaoxuan Wang, Peipei Ping, Timothy S Chang, Wei Wang

The integration of Artificial Intelligence (AI), especially Large Language
Models (LLMs), into the clinical diagnosis process offers significant potential
to improve the efficiency and accessibility of medical care. While LLMs have
shown some promise in the medical domain, their application in clinical
diagnosis remains underexplored, especially in real-world clinical practice,
where highly sophisticated, patient-specific decisions need to be made. Current
evaluations of LLMs in this field are often narrow in scope, focusing on
specific diseases or specialties and employing simplified diagnostic tasks. To
bridge this gap, we introduce CliBench, a novel benchmark developed from the
MIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'
capabilities in clinical diagnosis. This benchmark not only covers diagnoses
from a diverse range of medical cases across various specialties but also
incorporates tasks of clinical significance: treatment procedure
identification, lab test ordering and medication prescriptions. Supported by
structured output ontologies, CliBench enables a precise and multi-granular
evaluation, offering an in-depth understanding of LLM's capability on diverse
clinical tasks of desired granularity. We conduct a zero-shot evaluation of
leading LLMs to assess their proficiency in clinical decision-making. Our
preliminary results shed light on the potential and limitations of current LLMs
in clinical settings, providing valuable insights for future advancements in
LLM-powered healthcare.

摘要：人工智能（AI），尤其是大型語言模型（LLM），整合到臨床診斷過程中，提供了顯著的潛力，可以提高醫療保健的效率和可及性。雖然 LLM 在醫療領域顯示出一些前景，但它們在臨床診斷中的應用仍未得到充分探索，尤其是在現實世界的臨床實務中，需要做出高度複雜、針對特定患者的決策。目前在這個領域對 LLM 的評估通常範圍狹窄，著重於特定疾病或專科，並採用簡化的診斷任務。為了彌補這個差距，我們引入了 CliBench，這是一個從 MIMIC IV 資料集開發的新基準，提供了對 LLM 在臨床診斷中能力的全面且實際的評估。此基準不僅涵蓋了各種專科中各種醫療案例的診斷，還納入了具有臨床意義的任務：治療程序識別、實驗室檢驗訂購和藥物處方。在結構化輸出本体的支持下，CliBench 能夠進行精確且多粒度的評估，提供對 LLM 在各種所需粒度的臨床任務上的能力的深入了解。我們對領先的 LLM 進行了零次學習評估，以評估它們在臨床決策中的熟練程度。我們的初步結果揭示了當前 LLM 在臨床環境中的潛力和局限性，為 LLM 驅動的醫療保健的未來進展提供了寶貴的見解。

##### **Knowledge Editing in Language Models via Adapted Direct Preference Optimization**
2406.09920v1 by Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum

Large Language Models (LLMs) can become outdated over time as they may lack
updated world knowledge, leading to factual knowledge errors and gaps.
Knowledge Editing (KE) aims to overcome this challenge using weight updates
that do not require expensive retraining. We propose treating KE as an LLM
alignment problem. Toward this goal, we introduce Knowledge Direct Preference
Optimization (KDPO), a variation of the Direct Preference Optimization (DPO)
that is more effective for knowledge modifications. Our method is based on an
online approach that continually updates the knowledge stored in the model. We
use the current knowledge as a negative sample and the new knowledge we want to
introduce as a positive sample in a process called DPO. We also use
teacher-forcing for negative sample generation and optimize using the positive
sample, which helps maintain localized changes. We tested our KE method on
various datasets and models, comparing it to several cutting-edge methods, with
100 and 500 sequential edits. Additionally, we conducted an ablation study
comparing our method to the standard DPO approach. Our experimental results
show that our modified DPO method allows for more refined KE, achieving similar
or better performance compared to previous methods.

摘要：大型語言模型 (LLM) 隨著時間推移可能會過時，因為它們可能缺乏
更新的世界知識，導致事實知識錯誤和空白。
知識編輯 (KE) 旨在克服這個挑戰，使用不需要昂貴的重新訓練的權重更新。我們建議將 KE 視為 LLM
對齊問題。為了這個目標，我們引入了知識直接偏好
最佳化 (KDPO)，這是直接偏好最佳化 (DPO) 的一種變體
更有效地修改知識。我們的模型基於一種
持續更新模型中儲存知識的線上方法。我們
使用當前知識作為負面範例，我們想要
在稱為 DPO 的過程中引入的新知識作為正面範例。我們也使用
教師強制進行負面範例產生，並使用正面
範例進行最佳化，這有助於維持局部變更。我們在
各種資料集和模型上測試我們的 KE 模型，將其與多種尖端方法進行比較，進行
100 和 500 個順序編輯。此外，我們進行了一項消融研究
將我們的模型與標準 DPO 方法進行比較。我們的實驗結果
表明，我們修改的 DPO 方法允許更精緻的 KE，與先前的模型相比，達到類似
或更好的效能。

##### **GEB-1.3B: Open Lightweight Large Language Model**
2406.09900v1 by Jie Wu, Yufeng Zhu, Lei Shen, Xuqing Lu

Recently developed large language models (LLMs) such as ChatGPT, Claude, and
Llama have demonstrated impressive abilities, and even surpass human-level
performance in several tasks. Despite their success, the resource-intensive
demands of these models, requiring significant computational power for both
training and inference, limit their deployment to high-performance servers.
Additionally, the extensive calculation requirements of the models often lead
to increased latency in response times. With the increasing need for LLMs to
operate efficiently on CPUs, research about lightweight models that are
optimized for CPU inference has emerged. In this work, we introduce GEB-1.3B, a
lightweight LLM trained on 550 billion tokens in both Chinese and English
languages. We employ novel training techniques, including ROPE,
Group-Query-Attention, and FlashAttention-2, to accelerate training while
maintaining model performance. Additionally, we fine-tune the model using 10
million samples of instruction data to enhance alignment. GEB-1.3B exhibits
outstanding performance on general benchmarks such as MMLU, C-Eval, and CMMLU,
outperforming comparative models such as MindLLM-1.3B and TinyLLaMA-1.1B.
Notably, the FP32 version of GEB-1.3B achieves commendable inference times on
CPUs, with ongoing efforts to further enhance speed through advanced
quantization techniques. The release of GEB-1.3B as an open-source model marks
a significant contribution to the development of lightweight LLMs, promising to
foster further research and innovation in the field.

摘要：近期開發的大語言模型（LLM），例如 ChatGPT、Claude 和 Llama，已展現出令人印象深刻的能力，甚至在多項任務中超越人類水準的表現。儘管這些模型很成功，但它們需要大量資源，在訓練和推理時都需要大量的運算能力，這限制了它們在高性能伺服器上的部署。此外，這些模型龐大的運算需求通常會導致回應時間延遲。由於 LLM 在 CPU 上高效運作的需求日益增加，因此針對 CPU 推論進行最佳化的輕量級模型的研究應運而生。在這項工作中，我們介紹了 GEB-1.3B，這是一個輕量級 LLM，使用 5500 億個中文和英文語言的字元進行訓練。我們採用創新的訓練技術，包括 ROPE、Group-Query-Attention 和 FlashAttention-2，以在維持模型效能的同時加速訓練。此外，我們使用 1000 萬個指令資料範本來微調模型，以增強對齊。GEB-1.3B 在 MMLU、C-Eval 和 CMMLU 等一般基準測試中展現出傑出的效能，優於 MindLLM-1.3B 和 TinyLLaMA-1.1B 等比較模型。值得注意的是，GEB-1.3B 的 FP32 版本在 CPU 上實現了值得讚賞的推論時間，我們正持續努力透過進階量化技術進一步提升速度。GEB-1.3B 作為一個開源模型的發布為輕量級 LLM 的發展做出了重大貢獻，有望促進該領域進一步的研究和創新。

##### **Learning Solution-Aware Transformers for Efficiently Solving Quadratic Assignment Problem**
2406.09899v1 by Zhentao Tan, Yadong Mu

Recently various optimization problems, such as Mixed Integer Linear
Programming Problems (MILPs), have undergone comprehensive investigation,
leveraging the capabilities of machine learning. This work focuses on
learning-based solutions for efficiently solving the Quadratic Assignment
Problem (QAPs), which stands as a formidable challenge in combinatorial
optimization. While many instances of simpler problems admit fully
polynomial-time approximate solution (FPTAS), QAP is shown to be strongly
NP-hard. Even finding a FPTAS for QAP is difficult, in the sense that the
existence of a FPTAS implies $P = NP$. Current research on QAPs suffer from
limited scale and computational inefficiency. To attack the aforementioned
issues, we here propose the first solution of its kind for QAP in the
learn-to-improve category. This work encodes facility and location nodes
separately, instead of forming computationally intensive association graphs
prevalent in current approaches. This design choice enables scalability to
larger problem sizes. Furthermore, a \textbf{S}olution \textbf{AW}are
\textbf{T}ransformer (SAWT) architecture integrates the incumbent solution
matrix with the attention score to effectively capture higher-order information
of the QAPs. Our model's effectiveness is validated through extensive
experiments on self-generated QAP instances of varying sizes and the QAPLIB
benchmark.

摘要：最近各種最佳化問題，例如混合整數線性規劃問題 (MILP)，已經經過全面的研究，利用機器學習的能力。這項工作專注於基於學習的解決方案，以有效解決二次指派問題 (QAP)，這在組合最佳化中是一個艱鉅的挑戰。儘管許多較簡單問題的實例承認完全多項式時間近似解 (FPTAS)，但 QAP 已被證明是強 NP-hard。即使找到 QAP 的 FPTAS 也很困難，因為 FPTAS 的存在意味著 $P = NP$。目前對 QAP 的研究受到規模有限和計算效率低下的困擾。為了解決上述問題，我們在此提出 QAP 在學習改進類別中的第一個此類解決方案。這項工作分別編碼設施和位置節點，而不是形成當前方法中普遍存在的計算密集型關聯圖。這種設計選擇使可擴展性達到更大的問題規模。此外，\textbf{S}olution \textbf{AW}are \textbf{T}ransformer (SAWT) 架構將現有解矩陣與注意力分數整合在一起，以有效捕捉 QAP 的高階資訊。我們的模型的有效性通過在各種大小的自我生成 QAP 實例和 QAPLIB 基準上進行廣泛的實驗得到驗證。

##### **3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position Encoding**
2406.09897v1 by Xindian Ma, Wenyuan Liu, Peng Zhang, Nan Xu

Inspired by the Bloch Sphere representation, we propose a novel rotary
position encoding on a three-dimensional sphere, named 3D Rotary Position
Encoding (3D-RPE). 3D-RPE is an advanced version of the widely used 2D Rotary
Position Encoding (RoPE), with two major advantages for modeling long contexts:
controllable long-term decay and improved position resolution. For controllable
long-term decay, 3D-RPE allows for the regulation of long-term decay within the
chunk size, ensuring the modeling of relative positional information between
tokens at a distant relative position. For enhanced position resolution, 3D-RPE
can mitigate the degradation of position resolution caused by position
interpolation on RoPE. We have conducted experiments on long-context Natural
Language Understanding (NLU) and long-sequence Language Modeling (LM) tasks.
From the experimental results, 3D-RPE achieved performance improvements over
RoPE, especially in long-context NLU tasks.

摘要：受到 Bloch 球體表示法的啟發，我們提出一個三維球體上的新型旋轉位置編碼，稱為 3D 旋轉位置編碼 (3D-RPE)。3D-RPE 是廣泛使用的 2D 旋轉位置編碼 (RoPE) 的進階版本，在建模長文本方面具有兩個主要優點：可控的長期衰減和改善的位置解析度。對於可控的長期衰減，3D-RPE 允許在區塊大小內調節長期衰減，確保對遠距離相對位置的標記之間的相對位置資訊進行建模。對於增強的位置解析度，3D-RPE 可以減輕因 RoPE 上的位置內插而導致的位置解析度降低。我們已經對長文本自然語言理解 (NLU) 和長序列語言建模 (LM) 任務進行了實驗。從實驗結果來看，3D-RPE 在 RoPE 上取得了效能提升，特別是在長文本 NLU 任務中。

##### **Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming**
2406.09891v1 by Victor-Alexandru Pădurean, Adish Singla

Generative models have demonstrated human-level proficiency in various
benchmarks across domains like programming, natural sciences, and general
knowledge. Despite these promising results on competitive benchmarks, they
still struggle with seemingly simple problem-solving tasks typically carried
out by elementary-level students. How do state-of-the-art models perform on
standardized tests designed to assess computational thinking and
problem-solving skills at schools? In this paper, we curate a novel benchmark
involving computational thinking tests grounded in elementary visual
programming domains. Our initial results show that state-of-the-art models like
GPT-4o and Llama3 barely match the performance of an average school student. To
further boost the performance of these models, we fine-tune them using a novel
synthetic data generation methodology. The key idea is to develop a
comprehensive dataset using symbolic methods that capture different skill
levels, ranging from recognition of visual elements to multi-choice quizzes to
synthesis-style tasks. We showcase how various aspects of symbolic information
in synthetic data help improve fine-tuned models' performance. We will release
the full implementation and datasets to facilitate further research on
enhancing computational thinking in generative models.

摘要：生成式模型已在编程、自然科学和一般知识等领域的各种基准测试中展现出人类水平的能力。尽管在竞争性基准测试中取得了这些有希望的结果，但它们仍然难以解决通常由小学生执行的看似简单的解决问题任务。最先进的模型在旨在评估计算思维和学校解决问题技能的标准化测试中表现如何？在本文中，我们整理了一个新基准，其中涉及基于小学可视化编程域的计算思维测试。我们的初步结果表明，像 GPT-4o 和 Llama3 这样的最先进模型几乎无法与普通学生的表现相匹配。为了进一步提升这些模型的性能，我们使用一种新颖的合成数据生成方法对它们进行微调。关键思想是使用符号方法开发一个综合数据集，该方法可以捕捉不同的技能水平，从识别视觉元素到多项选择测验再到合成式任务。我们展示了合成数据中符号信息的各个方面如何帮助提高微调模型的性能。我们将发布完整的实现和数据集，以促进对增强生成模型中计算思维的研究。

##### **A Unified Data Augmentation Framework for Low-Resource Multi-Domain Dialogue Generation**
2406.09881v1 by Yongkang Liu, Ercong Nie, Zheng Hua, Zifeng Ding, Daling Wang, Yifei Zhang, Hinrich Schütze

Current state-of-the-art dialogue systems heavily rely on extensive training
datasets. However, challenges arise in domains where domain-specific training
datasets are insufficient or entirely absent. To tackle this challenge, we
propose a novel data \textbf{A}ugmentation framework for
\textbf{M}ulti-\textbf{D}omain \textbf{D}ialogue \textbf{G}eneration, referred
to as \textbf{AMD$^2$G}. The AMD$^2$G framework consists of a data augmentation
process and a two-stage training approach: domain-agnostic training and domain
adaptation training. We posit that domain corpora are a blend of
domain-agnostic and domain-specific features, with certain representation
patterns shared among diverse domains. Domain-agnostic training aims to enable
models to learn these common expressive patterns. To construct domain-agnostic
dialogue corpora, we employ a \textit{\textbf{de-domaining}} data processing
technique used to remove domain-specific features. By mitigating the effects of
domain-specific features, the model trained on the de-domained corpora can
effectively learn common expression patterns in different domains.
Subsequently, we adapt the learned domain-agnostic features to the target
domain through domain adaptation training. We conduct experiments on Chinese
dialogue datasets from five different domains and show that AMD$^2$G achieves
superior performance compared to both direct training on the target domain
corpus and collective training on all five domain corpora. Our work underscores
AMD$^2$G as a viable alternative solution for low-resource multi-domain
dialogue generation. Code and data associated with our work are available on
GitHub repository$^{\text 1}$.

摘要：<paragraph>目前最先进的对话系统严重依赖于广泛的训练数据集。然而，在特定领域中，如果特定领域的训练数据集不足或完全不存在，就会出现挑战。为了应对这一挑战，我们提出了一种用于多领域对话生成的新型数据增强框架，称为 AMD^2G。AMD^2G 框架包括一个数据增强过程和一个两阶段训练方法：领域无关训练和领域适应训练。我们认为领域语料库是领域无关和领域特定特征的混合体，其中某些表示模式在不同的领域之间共享。领域无关训练旨在使模型能够学习这些常见的表达模式。为了构建领域无关的对话语料库，我们采用了一种去领域化的数据处理技术，用于去除领域特定的特征。通过减轻领域特定特征的影响，在去领域语料库上训练的模型可以有效地学习不同领域中的常见表达模式。随后，我们通过领域适应训练将学习到的领域无关特征适应到目标领域。我们对来自五个不同领域的中文对话数据集进行了实验，结果表明，与在目标领域语料库上进行直接训练和对所有五个领域语料库进行集体训练相比，AMD^2G 取得了更好的性能。我们的工作强调了 AMD^2G 作为低资源多领域对话生成的可行替代方案。与我们工作相关联的代码和数据可在 GitHub 存储库中获得。$^{\text 1}$</paragraph>

##### **Federated Learning with Flexible Architectures**
2406.09877v1 by Jong-Ik Park, Carlee Joe-Wong

Traditional federated learning (FL) methods have limited support for clients
with varying computational and communication abilities, leading to
inefficiencies and potential inaccuracies in model training. This limitation
hinders the widespread adoption of FL in diverse and resource-constrained
environments, such as those with client devices ranging from powerful servers
to mobile devices. To address this need, this paper introduces Federated
Learning with Flexible Architectures (FedFA), an FL training algorithm that
allows clients to train models of different widths and depths. Each client can
select a network architecture suitable for its resources, with shallower and
thinner networks requiring fewer computing resources for training. Unlike prior
work in this area, FedFA incorporates the layer grafting technique to align
clients' local architectures with the largest network architecture in the FL
system during model aggregation. Layer grafting ensures that all client
contributions are uniformly integrated into the global model, thereby
minimizing the risk of any individual client's data skewing the model's
parameters disproportionately and introducing security benefits. Moreover,
FedFA introduces the scalable aggregation method to manage scale variations in
weights among different network architectures. Experimentally, FedFA
outperforms previous width and depth flexible aggregation strategies.
Furthermore, FedFA demonstrates increased robustness against performance
degradation in backdoor attack scenarios compared to earlier strategies.

摘要：傳統的聯合學習 (FL) 方法對運算和通訊能力各異的用戶端支援有限，導致模型訓練低效且潛在不準確。這種限制阻礙了 FL 在多元且資源受限的環境中的廣泛採用，例如那些用戶端裝置從強大的伺服器到行動裝置不等。為了滿足此需求，本文介紹了具備彈性架構的聯合學習 (FedFA)，這是一種 FL 訓練演算法，允許用戶端訓練不同寬度和深度的模型。每個用戶端可以選擇適合其資源的網路架構，較淺較薄的網路在訓練時需要較少的運算資源。與此領域先前的研究不同，FedFA 在模型聚合期間採用層移植技術，將用戶端的本地架構與 FL 系統中最大的網路架構對齊。層移植確保所有用戶端貢獻都均勻地整合到全球模型中，從而將任何個別用戶端資料扭曲模型參數不成比例的風險降至最低，並帶來安全性優勢。此外，FedFA 引入了可擴充聚合方法來管理不同網路架構之間權重中的規模差異。實驗證明，FedFA 優於先前的寬度和深度彈性聚合策略。此外，與先前的策略相比，FedFA 在後門攻擊場景中展現出對效能下降的更強健性。

##### **Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition**
2406.09873v1 by Yicong Jiang, Tianzi Wang, Xurong Xie, Juan Liu, Wei Sun, Nan Yan, Hui Chen, Lan Wang, Xunying Liu, Feng Tian

Disordered speech recognition profound implications for improving the quality
of life for individuals afflicted with, for example, dysarthria. Dysarthric
speech recognition encounters challenges including limited data, substantial
dissimilarities between dysarthric and non-dysarthric speakers, and significant
speaker variations stemming from the disorder. This paper introduces
Perceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the
Whisper large-scale model. We first fine-tune Whisper using LoRA and then
integrate a trainable Perceiver to generate fixed-length speaker prompts from
variable-length inputs, to improve model recognition of Chinese dysarthric
speech. Experimental results from our Chinese dysarthric speech dataset
demonstrate consistent improvements in recognition performance with
Perceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the
fine-tuned Whisper.

摘要：語言辨識障礙對於改善個人生活品質有深遠的影響，例如構音障礙。構音障礙語言辨識會遭遇的挑戰包括資料有限、構音障礙與非構音障礙說話者之間的差異顯著，以及由於障礙導致說話者之間的差異很大。本文介紹了 Perceiver-Prompt，這是一種使用 Whisper 大型模型進行 P-Tuning 的說話者適應方法。我們首先使用 LoRA 對 Whisper 進行微調，然後整合一個可訓練的 Perceiver，從可變長度的輸入產生固定長度的說話者提示，以提高模型對中文構音障礙語言的辨識度。我們從中文構音障礙語言資料集獲得的實驗結果證明，Perceiver-Prompt 在辨識效能上持續改善。與經過微調的 Whisper 相比，CER 相對減少了 13.04%。

##### **LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal Data**
2406.09864v1 by Grigor Bezirganyan, Sana Sellami, Laure Berti-Équille, Sébastien Fournier

Multimodal Deep Learning enhances decision-making by integrating diverse
information sources, such as texts, images, audio, and videos. To develop
trustworthy multimodal approaches, it is essential to understand how
uncertainty impacts these models. We introduce LUMA, a unique benchmark
dataset, featuring audio, image, and textual data from 50 classes, for learning
from uncertain and multimodal data. It extends the well-known CIFAR 10/100
dataset with audio samples extracted from three audio corpora, and text data
generated using the Gemma-7B Large Language Model (LLM). The LUMA dataset
enables the controlled injection of varying types and degrees of uncertainty to
achieve and tailor specific experiments and benchmarking initiatives. LUMA is
also available as a Python package including the functions for generating
multiple variants of the dataset with controlling the diversity of the data,
the amount of noise for each modality, and adding out-of-distribution samples.
A baseline pre-trained model is also provided alongside three uncertainty
quantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable
Conflictive Multi-View Learning. This comprehensive dataset and its tools are
intended to promote and support the development and benchmarking of trustworthy
and robust multimodal deep learning approaches.

摘要：多模態深度學習透過整合文本、影像、音訊和影片等多元資訊來源，提升決策品質。為了發展值得信賴的多模態方法，了解不確定性如何影響這些模型至關重要。我們引進 LUMA，一個獨特的基準資料集，包含來自 50 個類別的音訊、影像和文字資料，用於從不確定的多模態資料中學習。它擴充了著名的 CIFAR 10/100 資料集，加入了從三個音訊語料庫中萃取的音訊範例，以及使用 Gemma-7B 大型語言模型 (LLM) 生成的文字資料。LUMA 資料集能控制注入不同類型和程度的不確定性，以達成並調整特定的實驗和基準評量計畫。LUMA 也可用作 Python 套件，包含用於產生多種資料集變體的函式，並控制資料的多樣性、每個模態的雜訊量，以及加入分布外範例。也提供了一個預先訓練的基準模型，以及三種不確定性量化方法：蒙地卡羅中輟、深度整合和可靠衝突多視角學習。這個全面的資料集及其工具旨在推廣和支援值得信賴且穩健的多模態深度學習方法的開發和基準評量。

##### **Dataset Condensation with Latent Quantile Matching**
2406.09860v1 by Wei Wei, Tom De Schepper, Kevin Mets

Dataset condensation (DC) methods aim to learn a smaller synthesized dataset
with informative data records to accelerate the training of machine learning
models. Current distribution matching (DM) based DC methods learn a synthesized
dataset by matching the mean of the latent embeddings between the synthetic and
the real dataset. However two distributions with the same mean can still be
vastly different. In this work we demonstrate the shortcomings of using Maximum
Mean Discrepancy to match latent distributions i.e. the weak matching power and
lack of outlier regularization. To alleviate these shortcomings we propose our
new method: Latent Quantile Matching (LQM) which matches the quantiles of the
latent embeddings to minimize the goodness of fit test statistic between two
distributions. Empirical experiments on both image and graph-structured
datasets show that LQM matches or outperforms previous state of the art in
distribution matching based DC. Moreover we show that LQM improves the
performance in continual graph learning (CGL) setting where memory efficiency
and privacy can be important. Our work sheds light on the application of DM
based DC for CGL.

摘要：資料集濃縮 (DC) 方法旨在學習一個較小的合成資料集，其中包含有意義的資料記錄，以加速機器學習模型的訓練。目前的基於分配匹配 (DM) 的 DC 方法透過比對合成資料集與真實資料集之間的潛在嵌入的平均值來學習合成資料集。然而，兩個具有相同平均值的分配仍然可能截然不同。在這項工作中，我們展示了使用最大平均差異來比對潛在分配的缺點，即匹配能力弱且缺乏離群值正則化。為了緩解這些缺點，我們提出了我們的新方法：潛在分位數匹配 (LQM)，它比對潛在嵌入的分位數以最小化兩個分配之間的擬合優度檢定統計量。在影像和圖形結構資料集上的經驗實驗表明，LQM 在基於分配匹配的 DC 中比對或優於先前的技術水準。此外，我們表明 LQM 改善了持續圖形學習 (CGL) 設定中的效能，在該設定中記憶體效率和隱私可能很重要。我們的研究闡明了基於 DM 的 DC 在 CGL 中的應用。

##### **On the Encoding of Gender in Transformer-based ASR Representations**
2406.09855v1 by Aravind Krishnan, Badr M. Abdullah, Dietrich Klakow

While existing literature relies on performance differences to uncover gender
biases in ASR models, a deeper analysis is essential to understand how gender
is encoded and utilized during transcript generation. This work investigates
the encoding and utilization of gender in the latent representations of two
transformer-based ASR models, Wav2Vec2 and HuBERT. Using linear erasure, we
demonstrate the feasibility of removing gender information from each layer of
an ASR model and show that such an intervention has minimal impacts on the ASR
performance. Additionally, our analysis reveals a concentration of gender
information within the first and last frames in the final layers, explaining
the ease of erasing gender in these layers. Our findings suggest the prospect
of creating gender-neutral embeddings that can be integrated into ASR
frameworks without compromising their efficacy.

摘要：現有文獻依賴於表現差異來揭露 ASR 模型中的性別偏見，深入的分析對於了解在轉錄產生過程中如何編碼和利用性別至關重要。這項研究探討了基於Transformer的兩個 ASR 模型，Wav2Vec2 和 HuBERT 的潛在表示中性別的編碼和利用。使用線性擦除，我們證明了從 ASR 模型的每一層中移除性別資訊的可行性，並表明這種介入對 ASR 效能的影響很小。此外，我們的分析揭示了在最後一層的第一幀和最後一幀中性別資訊的集中，這解釋了在這些層中擦除性別的容易性。我們的發現表明了創造性別中立嵌入的可能性，這些嵌入可以整合到 ASR 框架中，而不會損害其效能。

##### **Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting**
2406.09839v1 by Muhammad Yeza Baihaqi, Angel García Contreras, Seiya Kawano, Koichiro Yoshino

Rapport is known as a conversational aspect focusing on relationship
building, which influences outcomes in collaborative tasks. This study aims to
establish human-agent rapport through small talk by using a rapport-building
strategy. We implemented this strategy for the virtual agents based on dialogue
strategies by prompting a large language model (LLM). In particular, we
utilized two dialogue strategies-predefined sequence and free-form-to guide the
dialogue generation framework. We conducted analyses based on human
evaluations, examining correlations between total turn, utterance characters,
rapport score, and user experience variables: naturalness, satisfaction,
interest, engagement, and usability. We investigated correlations between
rapport score and naturalness, satisfaction, engagement, and conversation flow.
Our experimental results also indicated that using free-form to prompt the
rapport-building strategy performed the best in subjective scores.

摘要：建立關係是對話中專注於建立關係的一項面向，會影響合作任務的成果。本研究旨在透過閒聊建立人與代理之間的關係，方法是使用建立關係的策略。我們根據對話策略為虛擬代理實作此策略，方法是提示大型語言模型 (LLM)。特別是，我們利用了兩個對話策略：預定義序列和自由形式，來引導對話產生架構。我們根據人類評估進行分析，探討總回合數、發話字元、建立關係分數與使用者經驗變數之間的關聯性：自然性、滿意度、興趣、參與度和可用性。我們調查了建立關係分數與自然性、滿意度、參與度和對話流之間的關聯性。我們的實驗結果也顯示，使用自由形式來提示建立關係策略在主觀分數上表現最佳。

##### **Vision-Language Models Meet Meteorology: Developing Models for Extreme Weather Events Detection with Heatmaps**
2406.09838v1 by Jian Chen, Peilin Zhou, Yining Hua, Dading Chong, Meng Cao, Yaowei Li, Zixuan Yuan, Bing Zhu, Junwei Liang

Real-time detection and prediction of extreme weather protect human lives and
infrastructure. Traditional methods rely on numerical threshold setting and
manual interpretation of weather heatmaps with Geographic Information Systems
(GIS), which can be slow and error-prone. Our research redefines Extreme
Weather Events Detection (EWED) by framing it as a Visual Question Answering
(VQA) problem, thereby introducing a more precise and automated solution.
Leveraging Vision-Language Models (VLM) to simultaneously process visual and
textual data, we offer an effective aid to enhance the analysis process of
weather heatmaps. Our initial assessment of general-purpose VLMs (e.g.,
GPT-4-Vision) on EWED revealed poor performance, characterized by low accuracy
and frequent hallucinations due to inadequate color differentiation and
insufficient meteorological knowledge. To address these challenges, we
introduce ClimateIQA, the first meteorological VQA dataset, which includes
8,760 wind gust heatmaps and 254,040 question-answer pairs covering four
question types, both generated from the latest climate reanalysis data. We also
propose Sparse Position and Outline Tracking (SPOT), an innovative technique
that leverages OpenCV and K-Means clustering to capture and depict color
contours in heatmaps, providing ClimateIQA with more accurate color spatial
location information. Finally, we present Climate-Zoo, the first meteorological
VLM collection, which adapts VLMs to meteorological applications using the
ClimateIQA dataset. Experiment results demonstrate that models from Climate-Zoo
substantially outperform state-of-the-art general VLMs, achieving an accuracy
increase from 0% to over 90% in EWED verification. The datasets and models in
this study are publicly available for future climate science research:
https://github.com/AlexJJJChen/Climate-Zoo.

摘要：<paragraph>極端天氣的即時偵測與預測能保護人類生命和基礎建設。傳統方法依賴於數值閾值設定和人工判讀帶有地理資訊系統 (GIS) 的天氣熱圖，這可能會很緩慢且容易出錯。我們的研究透過將極端天氣事件偵測 (EWED) 重新定義為視覺問答 (VQA) 問題，從而提出更精確且自動化的解決方案。我們利用視覺語言模型 (VLM) 來同時處理視覺和文字資料，提供有效的輔助工具來增強天氣熱圖的分析流程。我們對一般用途 VLM（例如 GPT-4-Vision）在 EWED 上的初步評估顯示效能不佳，其特徵是準確度低且頻繁出現幻覺，原因是顏色區分不足且氣象知識不足。為了應對這些挑戰，我們引入了 ClimateIQA，這是第一個氣象 VQA 資料集，其中包含 8,760 個陣風熱圖和 254,040 個問題解答對，涵蓋四種類型的問題，兩者都是從最新的氣候再分析資料中產生的。我們還提出了稀疏位置和輪廓追蹤 (SPOT)，這是一種創新的技術，利用 OpenCV 和 K-Means 聚類來擷取和描繪熱圖中的顏色輪廓，為 ClimateIQA 提供更準確的顏色空間位置資訊。最後，我們展示了 Climate-Zoo，這是第一個氣象 VLM 彙整，它使用 ClimateIQA 資料集將 VLM 調整為氣象應用程式。實驗結果表明，Climate-Zoo 中的模型大幅優於最先進的通用 VLM，在 EWED 驗證中將準確度從 0% 提高到 90% 以上。本研究中的資料集和模型公開提供，供未來的氣候科學研究使用：https://github.com/AlexJJJChen/Climate-Zoo。</paragraph>

##### **SHMamba: Structured Hyperbolic State Space Model for Audio-Visual Question Answering**
2406.09833v1 by Zhe Yang, Wenrui Li, Guanghui Cheng

The Audio-Visual Question Answering (AVQA) task holds significant potential
for applications. Compared to traditional unimodal approaches, the multi-modal
input of AVQA makes feature extraction and fusion processes more challenging.
Euclidean space is difficult to effectively represent multi-dimensional
relationships of data. Especially when extracting and processing data with a
tree structure or hierarchical structure, Euclidean space is not suitable as an
embedding space. Additionally, the self-attention mechanism in Transformers is
effective in capturing the dynamic relationships between elements in a
sequence. However, the self-attention mechanism's limitations in window
modeling and quadratic computational complexity reduce its effectiveness in
modeling long sequences. To address these limitations, we propose SHMamba:
Structured Hyperbolic State Space Model to integrate the advantages of
hyperbolic geometry and state space models. Specifically, SHMamba leverages the
intrinsic properties of hyperbolic space to represent hierarchical structures
and complex relationships in audio-visual data. Meanwhile, the state space
model captures dynamic changes over time by globally modeling the entire
sequence. Furthermore, we introduce an adaptive curvature hyperbolic alignment
module and a cross fusion block to enhance the understanding of hierarchical
structures and the dynamic exchange of cross-modal information, respectively.
Extensive experiments demonstrate that SHMamba outperforms previous methods
with fewer parameters and computational costs. Our learnable parameters are
reduced by 78.12\%, while the average performance improves by 2.53\%.
Experiments show that our method demonstrates superiority among all current
major methods and is more suitable for practical application scenarios.

摘要：<paragraph>音訊視覺問答 (AVQA) 任務對於應用程式具有顯著的潛力。與傳統的單模態方法相比，AVQA 的多模態輸入使得特徵萃取和融合的程序更具挑戰性。歐幾里得空間難以有效表示資料的多維關係。特別是在萃取和處理具有樹狀結構或階層結構的資料時，歐幾里得空間並不適合作為嵌入空間。此外，Transformer 中的自注意力機制在捕捉序列中元素之間的動態關係方面很有效。然而，自注意力機制的視窗建模和二次運算複雜度限制了其在建模長序列中的有效性。為了解決這些限制，我們提出了 SHMamba：結構化雙曲狀態空間模型，以整合雙曲幾何和狀態空間模型的優點。具體來說，SHMamba 利用雙曲空間的內在性質來表示音訊視覺資料中的階層結構和複雜關係。同時，狀態空間模型透過整體建模整個序列來捕捉動態變化。此外，我們引入了自適應曲率雙曲對齊模組和交叉融合區塊，分別增強了對階層結構的理解和跨模態資訊的動態交換。大量的實驗證明，SHMamba 以較少的參數和運算成本優於先前的模型。我們的可學習參數減少了 78.12%，而平均效能則提高了 2.53%。實驗表明，我們的模型在所有現行主要模型中表現出優越性，並且更適合實際應用場景。</paragraph>

##### **Federated Learning driven Large Language Models for Swarm Intelligence: A Survey**
2406.09831v1 by Youyang Qu

Federated learning (FL) offers a compelling framework for training large
language models (LLMs) while addressing data privacy and decentralization
challenges. This paper surveys recent advancements in the federated learning of
large language models, with a particular focus on machine unlearning, a crucial
aspect for complying with privacy regulations like the Right to be Forgotten.
Machine unlearning in the context of federated LLMs involves systematically and
securely removing individual data contributions from the learned model without
retraining from scratch. We explore various strategies that enable effective
unlearning, such as perturbation techniques, model decomposition, and
incremental learning, highlighting their implications for maintaining model
performance and data privacy. Furthermore, we examine case studies and
experimental results from recent literature to assess the effectiveness and
efficiency of these approaches in real-world scenarios. Our survey reveals a
growing interest in developing more robust and scalable federated unlearning
methods, suggesting a vital area for future research in the intersection of AI
ethics and distributed machine learning technologies.

摘要：聯邦學習 (FL) 提供了一個引人注目的框架，用於訓練大型語言模型 (LLM)，同時解決資料隱私和分散化挑戰。本文調查了大型語言模型的聯邦學習的最新進展，特別關注機器遺忘，這是遵守隱私法規（例如被遺忘權）的關鍵方面。聯邦 LLM 中的機器遺忘涉及系統且安全地從學習模型中移除個別資料貢獻，而無需從頭開始重新訓練。我們探討了各種策略，這些策略能實現有效的遺忘，例如擾動技術、模型分解和增量學習，強調它們對維護模型效能和資料隱私的影響。此外，我們從近期文獻中檢視案例研究和實驗結果，以評估這些方法在實際場景中的有效性和效率。我們的調查顯示，人們越來越有興趣開發更強大且可擴充的聯邦遺忘方法，這表示在 AI 倫理和分散式機器學習技術的交集中，未來的研究領域至關重要。

##### **HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning**
2406.09827v1 by Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, Sung Ju Hwang

In modern large language models (LLMs), increasing sequence lengths is a
crucial challenge for enhancing their comprehension and coherence in handling
complex tasks such as multi-modal question answering. However, handling long
context sequences with LLMs is prohibitively costly due to the conventional
attention mechanism's quadratic time and space complexity, and the context
window size is limited by the GPU memory. Although recent works have proposed
linear and sparse attention mechanisms to address this issue, their real-world
applicability is often limited by the need to re-train pre-trained models. In
response, we propose a novel approach, Hierarchically Pruned Attention (HiP),
which simultaneously reduces the training and inference time complexity from
$O(T^2)$ to $O(T \log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To
this end, we devise a dynamic sparse attention mechanism that generates an
attention mask through a novel tree-search-like algorithm for a given query on
the fly. HiP is training-free as it only utilizes the pre-trained attention
scores to spot the positions of the top-$k$ most significant elements for each
query. Moreover, it ensures that no token is overlooked, unlike the sliding
window-based sub-quadratic attention methods, such as StreamingLLM. Extensive
experiments on diverse real-world benchmarks demonstrate that HiP significantly
reduces prompt (i.e., prefill) and decoding latency and memory usage while
maintaining high generation performance with little or no degradation. As HiP
allows pretrained LLMs to scale to millions of tokens on commodity GPUs with no
additional engineering due to its easy plug-and-play deployment, we believe
that our work will have a large practical impact, opening up the possibility to
many long-context LLM applications previously infeasible.

摘要：<paragraph>在現代大型語言模型 (LLM) 中，增加序列長度對於提升其在處理多模態問題解答等複雜任務時的理解力和連貫性來說是一項至關重要的挑戰。然而，由於傳統的注意力機制具有二次時間和空間複雜度，且上下文視窗大小受到 GPU 記憶體的限制，因此使用 LLM 處理長上下文序列的成本過於高昂。儘管最近的研究提出了線性和稀疏注意力機制來解決這個問題，但它們的實際應用通常受到重新訓練預訓練模型的需要所限制。為了解決這個問題，我們提出了一種新的方法，即階層化修剪注意力 (HiP)，它同時將訓練和推論時間複雜度從 $O(T^2)$ 降低到 $O(T \log T)$，並將空間複雜度從 $O(T^2)$ 降低到 $O(T)$。為此，我們設計了一個動態稀疏注意力機制，它通過一種新穎的類樹狀搜尋演算法為給定的查詢動態生成注意力遮罩。HiP 是免訓練的，因為它只利用預訓練的注意力分數來找出每個查詢中最重要的前 $k$ 個元素的位置。此外，它確保不會遺漏任何符號，這與基於滑動視窗的次二次注意力方法（例如 StreamingLLM）不同。在各種真實世界基準上的大量實驗表明，HiP 大幅減少了提示（即預填充）和解碼延遲以及記憶體使用量，同時在幾乎沒有或沒有降低的情況下維持了高生成效能。由於 HiP 允許預訓練的 LLM 在沒有額外工程的條件下擴充到數百萬個符號的商品 GPU 上，而且部署起來簡單易行，我們相信我們的這項工作將產生巨大的實際影響，為許多以前不可行的長上下文 LLM 應用開啟了可能性。</paragraph>

##### **From Manifestations to Cognitive Architectures: a Scalable Framework**
2406.09823v1 by Alfredo Ibias, Guillem Ramirez-Miranda, Enric Guinovart, Eduard Alarcon

The Artificial Intelligence field is flooded with optimisation methods. In
this paper, we change the focus to developing modelling methods with the aim of
getting us closer to Artificial General Intelligence. To do so, we propose a
novel way to interpret reality as an information source, that is later
translated into a computational framework able to capture and represent such
information. This framework is able to build elements of classical cognitive
architectures, like Long Term Memory and Working Memory, starting from a simple
primitive that only processes Spatial Distributed Representations. Moreover, it
achieves such level of verticality in a seamless scalable hierarchical way.

摘要：人工智慧領域充斥著優化方法。在本文中，我們將焦點轉移到開發建模方法，目的是讓我們更接近人工通用智慧。為此，我們提出了一種新的方式來詮釋現實為資訊來源，稍後轉化為一個計算架構，能夠擷取並表示此類資訊。這個架構能夠建構經典認知架構的元素，例如長期記憶和工作記憶，從一個僅處理空間分佈式表徵的簡單基本元素開始。此外，它以一種無縫可擴充的階層方式，達到了這種垂直度。

##### **Retrieval Augmented Fact Verification by Synthesizing Contrastive Arguments**
2406.09815v1 by Zhenrui Yue, Huimin Zeng, Lanyu Shang, Yifan Liu, Yang Zhang, Dong Wang

The rapid propagation of misinformation poses substantial risks to public
interest. To combat misinformation, large language models (LLMs) are adapted to
automatically verify claim credibility. Nevertheless, existing methods heavily
rely on the embedded knowledge within LLMs and / or black-box APIs for evidence
collection, leading to subpar performance with smaller LLMs or upon unreliable
context. In this paper, we propose retrieval augmented fact verification
through the synthesis of contrasting arguments (RAFTS). Upon input claims,
RAFTS starts with evidence retrieval, where we design a retrieval pipeline to
collect and re-rank relevant documents from verifiable sources. Then, RAFTS
forms contrastive arguments (i.e., supporting or refuting) conditioned on the
retrieved evidence. In addition, RAFTS leverages an embedding model to identify
informative demonstrations, followed by in-context prompting to generate the
prediction and explanation. Our method effectively retrieves relevant documents
as evidence and evaluates arguments from varying perspectives, incorporating
nuanced information for fine-grained decision-making. Combined with informative
in-context examples as prior, RAFTS achieves significant improvements to
supervised and LLM baselines without complex prompts. We demonstrate the
effectiveness of our method through extensive experiments, where RAFTS can
outperform GPT-based methods with a significantly smaller 7B LLM.

摘要：错误信息的快速传播对公众利益构成重大风险。为了打击错误信息，大型语言模型 (LLM) 被调整为自动验证声明的可信度。然而，现有方法严重依赖于 LLM 中的嵌入式知识和/或用于收集证据的黑盒 API，导致使用较小的 LLM 或在不可靠的上下文中性能不佳。在本文中，我们提出了通过对比论点的综合（RAFTS）检索增强事实验证。在输入声明后，RAFTS 从证据检索开始，我们设计了一个检索管道来从可验证的来源收集和重新排列相关文档。然后，RAFTS 根据检索到的证据形成对比论点（即支持或反驳）。此外，RAFTS 利用嵌入模型来识别信息丰富的论证，然后在上下文中提示以生成预测和解释。我们的方法有效地检索相关文档作为证据，并从不同的角度评估论点，纳入细微的信息以进行细粒度的决策。结合信息丰富的上下文示例作为先验，RAFTS 在没有复杂提示的情况下实现了对监督和 LLM 基线的显着改进。我们通过广泛的实验展示了我们方法的有效性，其中 RAFTS 可以使用明显更小的 7B LLM 优于基于 GPT 的方法。

##### **Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity**
2406.09790v1 by Bowen Zhang, Chunping Li

Semantic Textual Similarity (STS) constitutes a critical research direction
in computational linguistics and serves as a key indicator of the encoding
capabilities of embedding models. Driven by advances in pre-trained language
models and contrastive learning techniques, leading sentence representation
methods can already achieved average Spearman's correlation scores of
approximately 86 across seven STS benchmarks in SentEval. However, further
improvements have become increasingly marginal, with no existing method
attaining an average score higher than 87 on these tasks. This paper conducts
an in-depth analysis of this phenomenon and concludes that the upper limit for
Spearman's correlation scores using contrastive learning is 87.5. To transcend
this ceiling, we propose an innovative approach termed Pcc-tuning, which
employs Pearson's correlation coefficient as a loss function to refine model
performance beyond contrastive learning. Experimental results demonstrate that
Pcc-tuning markedly surpasses previous state-of-the-art strategies, raising the
Spearman's correlation score to above 90.

摘要：語義文本相似度 (STS) 構成計算語言學中一項重要的研究方向，同時也作為嵌入模型編碼能力的一個關鍵指標。在預先訓練語言模型和對比學習技術的推動下，領先的句子表示方法已經可以在 SentEval 中的七個 STS 基準測試中達到平均 Spearman 相關分數約 86 分。然而，進一步的改進變得越來越邊緣化，目前沒有任何現有方法在這些任務中達到平均分數高於 87 分。本文對此現象進行了深入分析，並得出結論，使用對比學習的 Spearman 相關分數的上限為 87.5。為了超越這個上限，我們提出了一種稱為 Pcc-tuning 的創新方法，它採用 Pearson 相關係數作為損失函數，以優化模型在對比學習之外的效能。實驗結果表明，Pcc-tuning 明顯優於先前的最先進策略，將 Spearman 相關分數提高到 90 分以上。

##### **Evolving Self-Assembling Neural Networks: From Spontaneous Activity to Experience-Dependent Learning**
2406.09787v1 by Erwan Plantec, Joachin W. Pedersen, Milton L. Montero, Eleni Nisioti, Sebastian Risi

Biological neural networks are characterized by their high degree of
plasticity, a core property that enables the remarkable adaptability of natural
organisms. Importantly, this ability affects both the synaptic strength and the
topology of the nervous systems. Artificial neural networks, on the other hand,
have been mainly designed as static, fully connected structures that can be
notoriously brittle in the face of changing environments and novel inputs.
Building on previous works on Neural Developmental Programs (NDPs), we propose
a class of self-organizing neural networks capable of synaptic and structural
plasticity in an activity and reward-dependent manner which we call Lifelong
Neural Developmental Program (LNDP). We present an instance of such a network
built on the graph transformer architecture and propose a mechanism for
pre-experience plasticity based on the spontaneous activity of sensory neurons.
Our results demonstrate the ability of the model to learn from experiences in
different control tasks starting from randomly connected or empty networks. We
further show that structural plasticity is advantageous in environments
necessitating fast adaptation or with non-stationary rewards.

摘要：生物神经网络的特征在于其高度的可塑性，这是使自然生物具有惊人适应能力的核心特性。重要的是，这种能力既影响突触强度，也影响神经系统的拓扑结构。另一方面，人工神经网络主要被设计为静态的、完全连接的结构，在面对不断变化的环境和新颖的输入时，它们出了名的脆弱。在神经发育程序 (NDP) 的先前工作基础上，我们提出了一类自组织神经网络，它能够以活动和奖励依赖的方式进行突触和结构可塑性，我们称之为终身神经发育程序 (LNDP)。我们提出了一个建立在图转换器架构上的此类网络的实例，并提出了一种基于感觉神经元自发活动的先验可塑性机制。我们的结果证明了该模型从随机连接或空网络开始，从不同控制任务中的经验中学习的能力。我们进一步表明，结构可塑性在需要快速适应或非平稳奖励的环境中是有利的。

##### **OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst**
2406.09779v1 by Jingtao Cao, Zheng Zhang, Hongru Wang, Bin Liang, Hao Wang, Kam-Fai Wong

Memes, which rapidly disseminate personal opinions and positions across the
internet, also pose significant challenges in propagating social bias and
prejudice. This study presents a novel approach to detecting harmful memes,
particularly within the multicultural and multilingual context of Singapore.
Our methodology integrates image captioning, Optical Character Recognition
(OCR), and Large Language Model (LLM) analysis to comprehensively understand
and classify harmful memes. Utilizing the BLIP model for image captioning,
PP-OCR and TrOCR for text recognition across multiple languages, and the Qwen
LLM for nuanced language understanding, our system is capable of identifying
harmful content in memes created in English, Chinese, Malay, and Tamil. To
enhance the system's performance, we fine-tuned our approach by leveraging
additional data labeled using GPT-4V, aiming to distill the understanding
capability of GPT-4V for harmful memes to our system. Our framework achieves
top-1 at the public leaderboard of the Online Safety Prize Challenge hosted by
AI Singapore, with the AUROC as 0.7749 and accuracy as 0.7087, significantly
ahead of the other teams. Notably, our approach outperforms previous
benchmarks, with FLAVA achieving an AUROC of 0.5695 and VisualBERT an AUROC of
0.5561.

摘要：網路迷因快速傳播個人意見與立場，也對散播社會偏見和歧視構成重大挑戰。本研究提出了一種偵測有害迷因的新穎方法，特別是在新加坡的多元文化和多語言環境中。我們的做法整合了圖片標題、光學字元辨識（OCR）和大語言模型（LLM）分析，以全面理解和分類有害迷因。我們的系統利用 BLIP 模型進行圖片標題、PP-OCR 和 TrOCR 進行多語言文字辨識，以及 Qwen LLM 進行細緻的語言理解，能夠辨識出以英語、中文、馬來語和淡米爾語製作的迷因中的有害內容。為了提升系統效能，我們利用標記有 GPT-4V 的額外資料微調我們的做法，目標是將 GPT-4V 對有害迷因的理解能力提煉到我們的系統中。我們的架構在 AI 新加坡主辦的網路安全獎挑戰賽的公開排行榜中獲得第一名，AUROC 為 0.7749，準確度為 0.7087，大幅領先其他團隊。值得注意的是，我們的做法優於先前的基準，FLAVA 的 AUROC 為 0.5695，VisualBERT 的 AUROC 為 0.5561。

##### **Research on Edge Detection of LiDAR Images Based on Artificial Intelligence Technology**
2406.09773v1 by Haowei Yang, Liyang Wang, Jingyu Zhang, Yu Cheng, Ao Xiang

With the widespread application of Light Detection and Ranging (LiDAR)
technology in fields such as autonomous driving, robot navigation, and terrain
mapping, the importance of edge detection in LiDAR images has become
increasingly prominent. Traditional edge detection methods often face
challenges in accuracy and computational complexity when processing LiDAR
images. To address these issues, this study proposes an edge detection method
for LiDAR images based on artificial intelligence technology. This paper first
reviews the current state of research on LiDAR technology and image edge
detection, introducing common edge detection algorithms and their applications
in LiDAR image processing. Subsequently, a deep learning-based edge detection
model is designed and implemented, optimizing the model training process
through preprocessing and enhancement of the LiDAR image dataset. Experimental
results indicate that the proposed method outperforms traditional methods in
terms of detection accuracy and computational efficiency, showing significant
practical application value. Finally, improvement strategies are proposed for
the current method's shortcomings, and the improvements are validated through
experiments.

摘要：隨著光達 (LiDAR) 技術在自動駕駛、機器人導航和地形繪製等領域的廣泛應用，LiDAR 影像中的邊緣檢測重要性日益凸顯。傳統的邊緣檢測方法在處理 LiDAR 影像時，往往面臨準確度和計算複雜度的挑戰。為了解決這些問題，本研究提出了一種基於人工智慧技術的 LiDAR 影像邊緣檢測方法。本文首先回顧了 LiDAR 技術和影像邊緣檢測的現有研究現況，介紹了常見的邊緣檢測演算法及其在 LiDAR 影像處理中的應用。隨後設計並實作了一個基於深度學習的邊緣檢測模型，透過 LiDAR 影像資料集的預處理和增強，優化模型訓練過程。實驗結果表明，所提出的方法在檢測準確度和計算效率方面優於傳統方法，展現顯著的實務應用價值。最後，針對目前方法的不足之處提出改進策略，並透過實驗驗證改進效果。

##### **Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model Fusion**
2406.09770v1 by Anke Tang, Li Shen, Yong Luo, Shiwei Liu, Han Hu, Bo Du

Solving multi-objective optimization problems for large deep neural networks
is a challenging task due to the complexity of the loss landscape and the
expensive computational cost of training and evaluating models. Efficient
Pareto front approximation of large models enables multi-objective optimization
for various tasks such as multi-task learning and trade-off analysis. Existing
algorithms for learning Pareto set, including (1) evolutionary, hypernetworks,
and hypervolume-maximization methods, are computationally expensive and have
restricted scalability to large models; (2) Scalarization algorithms, where a
separate model is trained for each objective ray, which is inefficient for
learning the entire Pareto set and fails to capture the objective trade-offs
effectively. Inspired by the recent success of model merging, we propose a
practical and scalable approach to Pareto set learning problem via mixture of
experts (MoE) based model fusion. By ensembling the weights of specialized
single-task models, the MoE module can effectively capture the trade-offs
between multiple objectives and closely approximate the entire Pareto set of
large neural networks. Once the routers are learned and a preference vector is
set, the MoE module can be unloaded, thus no additional computational cost is
introduced during inference. We conduct extensive experiments on vision and
language tasks using large-scale models such as CLIP-ViT and GPT-2. The
experimental results demonstrate that our method efficiently approximates the
entire Pareto front of large models. Using only hundreds of trainable
parameters of the MoE routers, our method even has lower memory usage compared
to linear scalarization and algorithms that learn a single Pareto optimal
solution, and are scalable to both the number of objectives and the size of the
model.

摘要：<paragraph>解決大型深度神經網路的多目標最佳化問題是一項具有挑戰性的任務，原因在於損失情況的複雜性以及訓練和評估模型的高昂運算成本。大型模型的有效 Pareto 前緣近似值能針對各種任務（例如多任務學習和權衡分析）進行多目標最佳化。現有的學習 Pareto 集合演算法，包括（1）演化、超網路和超體積最大化方法，在運算上成本昂貴，且在大型模型中具有受限的可擴充性；（2）標量化演算法，其中針對每個目標射線訓練一個單獨的模型，這對於學習整個 Pareto 集合沒有效率，且無法有效捕捉目標權衡。受到最近模型合併成功的啟發，我們提出一個透過基於混合專家（MoE）模型融合的實用且可擴充的 Pareto 集合學習問題方法。透過將專業單任務模型的權重集成，MoE 模組能有效捕捉多個目標之間的權衡，並近似大型神經網路的整個 Pareto 集合。一旦路由器被學習且偏好向量被設定，MoE 模組就能卸載，因此在推論期間不會產生額外的運算成本。我們針對使用 CLIP-ViT 和 GPT-2 等大規模模型的視覺和語言任務進行廣泛的實驗。實驗結果證明，我們的模型能有效近似大型模型的整個 Pareto 前緣。我們的模型僅使用 MoE 路由器的數百個可訓練參數，與線性標量化和學習單一 Pareto 最佳解的演算法相比，甚至具有較低的記憶體使用量，且能擴充到目標數量和模型大小。</paragraph>

##### **Bayesian Conditioned Diffusion Models for Inverse Problems**
2406.09768v1 by Alper Güngör, Bahri Batuhan Bilecen, Tolga Çukur

Diffusion models have recently been shown to excel in many image
reconstruction tasks that involve inverse problems based on a forward
measurement operator. A common framework uses task-agnostic unconditional
models that are later post-conditioned for reconstruction, an approach that
typically suffers from suboptimal task performance. While task-specific
conditional models have also been proposed, current methods heuristically
inject measured data as a naive input channel that elicits sampling
inaccuracies. Here, we address the optimal conditioning of diffusion models for
solving challenging inverse problems that arise during image reconstruction.
Specifically, we propose a novel Bayesian conditioning technique for diffusion
models, BCDM, based on score-functions associated with the conditional
distribution of desired images given measured data. We rigorously derive the
theory to express and train the conditional score-function. Finally, we show
state-of-the-art performance in image dealiasing, deblurring, super-resolution,
and inpainting with the proposed technique.

摘要：擴散模型最近被證明在許多影像重建任務中表現出色，這些任務涉及基於正向測量算子的反問題。一個常見的架構使用與任務無關的無條件模型，這些模型稍後會經過後條件化以進行重建，這種方法通常會導致次優的任務執行。雖然特定於任務的條件模型也已提出，但目前的方法啟發式地將測量資料注入為一個天真的輸入通道，這會引發取樣不準確。在這裡，我們探討擴散模型在影像重建期間解決具有挑戰性的反問題時的最佳條件化。具體來說，我們提出一個新的貝氏條件化技術，用於擴散模型，稱為 BCDM，它基於給定測量資料的所需影像條件分佈相關的評分函數。我們嚴格推導出表達和訓練條件評分函數的理論。最後，我們展示了在影像去混疊、去模糊、超解析度和內插方面的最先進效能，並採用所提出的技術。

##### **Application of Natural Language Processing in Financial Risk Detection**
2406.09765v1 by Liyang Wang, Yu Cheng, Ao Xiang, Jingyu Zhang, Haowei Yang

This paper explores the application of Natural Language Processing (NLP) in
financial risk detection. By constructing an NLP-based financial risk detection
model, this study aims to identify and predict potential risks in financial
documents and communications. First, the fundamental concepts of NLP and its
theoretical foundation, including text mining methods, NLP model design
principles, and machine learning algorithms, are introduced. Second, the
process of text data preprocessing and feature extraction is described.
Finally, the effectiveness and predictive performance of the model are
validated through empirical research. The results show that the NLP-based
financial risk detection model performs excellently in risk identification and
prediction, providing effective risk management tools for financial
institutions. This study offers valuable references for the field of financial
risk management, utilizing advanced NLP techniques to improve the accuracy and
efficiency of financial risk detection.

摘要：本文探討自然語言處理 (NLP) 在金融風險偵測中的應用。本研究透過建構基於 NLP 的金融風險偵測模型，旨在識別和預測金融文件和通訊中的潛在風險。首先，介紹 NLP 的基本概念及其理論基礎，包括文字探勘方法、NLP 模型設計原則和機器學習演算法。其次，說明文字資料前處理和特徵萃取的過程。最後，透過實證研究驗證模型的效能和預測表現。結果顯示，基於 NLP 的金融風險偵測模型在風險識別和預測方面表現優異，為金融機構提供有效的風險管理工具。本研究為金融風險管理領域提供了寶貴的參考，利用進階的 NLP 技術來提升金融風險偵測的準確性和效率。

##### **Bootstrapping Language Models with DPO Implicit Rewards**
2406.09760v1 by Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, Min Lin

Human alignment in large language models (LLMs) is an active area of
research. A recent groundbreaking work, direct preference optimization (DPO),
has greatly simplified the process from past work in reinforcement learning
from human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,
after training, provides an implicit reward model. In this work, we make a
novel observation that this implicit reward model can by itself be used in a
bootstrapping fashion to further align the LLM. Our approach is to use the
rewards from a current LLM model to construct a preference dataset, which is
then used in subsequent DPO rounds. We incorporate refinements that debias the
length of the responses and improve the quality of the preference dataset to
further improve our approach. Our approach, named self-alignment with DPO
ImpliCit rEwards (DICE), shows great improvements in alignment and achieves
superior performance than Gemini Pro on AlpacaEval 2, reaching 27.55%
length-controlled win rate against GPT-4 Turbo, but with only 8B parameters and
no external feedback. Our code is available at https://github.com/sail-sg/dice.

摘要：大型語言模型 (LLM) 中的人類對齊是一個活躍的研究領域。最近一項突破性的工作，直接偏好優化 (DPO)，通過繞過 RLHF 中的獎勵學習階段，極大地簡化了人類回饋 (RLHF) 中強化學習過去工作的流程。DPO 在訓練後提供了隱含的獎勵模型。在這項工作中，我們做出了一個新穎的觀察，即這個隱含的獎勵模型本身可以用於自舉方式進一步對齊 LLM。我們的做法是使用當前 LLM 模型中的獎勵來構建偏好數據集，然後在後續的 DPO 輪次中使用該數據集。我們納入了改進，以消除對應答長度的偏見並提高偏好數據集的質量，以進一步改進我們的做法。我們的做法，稱為使用 DPO 隱含獎勵的自對齊 (DICE)，在對齊方面顯示出巨大的改進，並且在 AlpacaEval 2 上達到了比 Gemini Pro 更優越的性能，對抗 GPT-4 Turbo 時達到了 27.55% 的長度控制獲勝率，但只有 8B 參數和沒有外部回饋。我們的代碼可在 https://github.com/sail-sg/dice 獲得。

##### **Mix Q-learning for Lane Changing: A Collaborative Decision-Making Method in Multi-Agent Deep Reinforcement Learning**
2406.09755v1 by Xiaojun Bi, Mingjie He, Yiwen Sun

Lane-changing decisions, which are crucial for autonomous vehicle path
planning, face practical challenges due to rule-based constraints and limited
data. Deep reinforcement learning has become a major research focus due to its
advantages in data acquisition and interpretability. However, current models
often overlook collaboration, which affects not only impacts overall traffic
efficiency but also hinders the vehicle's own normal driving in the long run.
To address the aforementioned issue, this paper proposes a method named Mix
Q-learning for Lane Changing(MQLC) that integrates a hybrid value Q network,
taking into account both collective and individual benefits for the greater
good. At the collective level, our method coordinates the individual Q and
global Q networks by utilizing global information. This enables agents to
effectively balance their individual interests with the collective benefit. At
the individual level, we integrated a deep learning-based intent recognition
module into our observation and enhanced the decision network. These changes
provide agents with richer decision information and more accurate feature
extraction for improved lane-changing decisions. This strategy enables the
multi-agent system to learn and formulate optimal decision-making strategies
effectively. Our MQLC model, through extensive experimental results,
impressively outperforms other state-of-the-art multi-agent decision-making
methods, achieving significantly safer and faster lane-changing decisions.

摘要：車道變換決策對於自動駕駛路徑規劃至關重要，但因基於規則的限制和有限的資料而面臨實際挑戰。深度強化學習由於其在資料獲取和可解釋性的優勢，已成為主要的研究重點。然而，目前的模型常常忽略協作，這不僅影響整體交通效率，也阻礙了車輛本身長期正常的行駛。為了解決上述問題，本文提出了一個名為車道變換混合 Q 學習 (MQLC) 的方法，它整合了一個混合值 Q 網路，同時考慮集體和個人利益以達到更大的利益。在集體層面，我們的模型透過利用全局資訊來協調個別 Q 網路和全局 Q 網路。這使代理人能夠有效地平衡他們的個人利益和集體利益。在個別層面，我們將基於深度學習的意圖識別模組整合到我們的觀察中，並增強了決策網路。這些改變為代理人提供了更豐富的決策資訊，並進行更準確的特徵提取，以改善車道變換決策。此策略使多代理系統能夠有效地學習和制定最佳決策策略。我們的 MQLC 模型透過廣泛的實驗結果，令人印象深刻地優於其他最先進的多代理決策制定方法，實現了顯著更安全、更快速的車道變換決策。

##### **ControlVAR: Exploring Controllable Visual Autoregressive Modeling**
2406.09750v1 by Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Zhe Lin, Rita Singh, Bhiksha Raj

Conditional visual generation has witnessed remarkable progress with the
advent of diffusion models (DMs), especially in tasks like control-to-image
generation. However, challenges such as expensive computational cost, high
inference latency, and difficulties of integration with large language models
(LLMs) have necessitated exploring alternatives to DMs. This paper introduces
ControlVAR, a novel framework that explores pixel-level controls in visual
autoregressive (VAR) modeling for flexible and efficient conditional
generation. In contrast to traditional conditional models that learn the
conditional distribution, ControlVAR jointly models the distribution of image
and pixel-level conditions during training and imposes conditional controls
during testing. To enhance the joint modeling, we adopt the next-scale AR
prediction paradigm and unify control and image representations. A
teacher-forcing guidance strategy is proposed to further facilitate
controllable generation with joint modeling. Extensive experiments demonstrate
the superior efficacy and flexibility of ControlVAR across various conditional
generation tasks against popular conditional DMs, \eg, ControlNet and
T2I-Adaptor.

摘要：條件視覺生成在擴散模型 (DM) 的出現後，特別是在控制到影像生成等任務上，見證了顯著的進展。然而，昂貴的計算成本、高推理延遲以及與大型語言模型 (LLM) 整合的困難等挑戰，使得探索 DM 的替代方案變得必要。本文介紹 ControlVAR，一個新的框架，探索像素級別控制在視覺自迴歸 (VAR) 建模中，以實現靈活且高效的條件式生成。與學習條件式分布的傳統條件式模型相比，ControlVAR 在訓練期間聯合建模影像和像素級別條件的分布，並在測試期間施加條件式控制。為了增強聯合建模，我們採用了下一個尺度的 AR 預測範例，並統一控制和影像表示。提出了一個教師強制指導策略，以進一步促進聯合建模的可控生成。廣泛的實驗證明了 ControlVAR 在各種條件式生成任務中，相較於流行的條件式 DM，例如 ControlNet 和 T2I-Adaptor，具有優異的效能和靈活性。

##### **When Will Gradient Regularization Be Harmful?**
2406.09723v1 by Yang Zhao, Hao Zhang, Xiuyuan Hu

Gradient regularization (GR), which aims to penalize the gradient norm atop
the loss function, has shown promising results in training modern
over-parameterized deep neural networks. However, can we trust this powerful
technique? This paper reveals that GR can cause performance degeneration in
adaptive optimization scenarios, particularly with learning rate warmup. Our
empirical and theoretical analyses suggest this is due to GR inducing
instability and divergence in gradient statistics of adaptive optimizers at the
initial training stage. Inspired by the warmup heuristic, we propose three GR
warmup strategies, each relaxing the regularization effect to a certain extent
during the warmup course to ensure the accurate and stable accumulation of
gradients. With experiments on Vision Transformer family, we confirm the three
GR warmup strategies can effectively circumvent these issues, thereby largely
improving the model performance. Meanwhile, we note that scalable models tend
to rely more on the GR warmup, where the performance can be improved by up to
3\% on Cifar10 compared to baseline GR. Code is available at
\href{https://github.com/zhaoyang-0204/gnp}{https://github.com/zhaoyang-0204/gnp}.

摘要：梯度正則化 (GR) 旨在懲罰損失函數上的梯度範數，在訓練現代過度參數化的深度神經網路方面已展現出令人滿意的成果。然而，我們可以相信這種強大的技術嗎？本文揭示了 GR 會在適應性最佳化情境中導致效能退化，特別是在學習率熱身的情況下。我們的經驗和理論分析表明，這是因為 GR 在適應性最佳化器的初始訓練階段中會導致梯度統計的不穩定和發散。受到熱身啟發法的啟發，我們提出了三種 GR 熱身策略，每種策略都在熱身過程中在一定程度上放寬正則化效果，以確保梯度準確且穩定地累積。透過在 Vision Transformer 家族上進行實驗，我們確認這三種 GR 熱身策略可以有效規避這些問題，從而大幅提升模型效能。同時，我們注意到可擴充模型往往更依賴 GR 熱身，其中在 Cifar10 上的效能可以比基線 GR 提升多達 3%。程式碼可於\href{https://github.com/zhaoyang-0204/gnp}{https://github.com/zhaoyang-0204/gnp}取得。

##### **Self-Knowledge Distillation for Learning Ambiguity**
2406.09719v1 by Hancheol Park, Soyeong Jeong, Sukmin Cho, Jong C. Park

Recent language models have shown remarkable performance on natural language
understanding (NLU) tasks. However, they are often sub-optimal when faced with
ambiguous samples that can be interpreted in multiple ways, over-confidently
predicting a single label without consideration for its correctness. To address
this issue, we propose a novel self-knowledge distillation method that enables
models to learn label distributions more accurately by leveraging knowledge
distilled from their lower layers. This approach also includes a learning phase
that re-calibrates the unnecessarily strengthened confidence for training
samples judged as extremely ambiguous based on the distilled distribution
knowledge. We validate our method on diverse NLU benchmark datasets and the
experimental results demonstrate its effectiveness in producing better label
distributions. Particularly, through the process of re-calibrating the
confidence for highly ambiguous samples, the issue of over-confidence when
predictions for unseen samples do not match with their ground-truth labels has
been significantly alleviated. This has been shown to contribute to generating
better distributions than the existing state-of-the-art method. Moreover, our
method is more efficient in training the models compared to the existing
method, as it does not involve additional training processes to refine label
distributions.

摘要：近期的语言模型在自然语言理解 (NLU) 任务上表现亮眼。然而，当面对可有多种解读的模棱两可样本时，它们通常表现不佳，会过于自信地预测单一标签，而未考虑其正确性。为了解决这个问题，我们提出了一种新颖的自知识蒸馏方法，该方法使模型能够通过利用从其较低层蒸馏的知识，更准确地学习标签分布。此方法还包括一个学习阶段，该阶段会重新校准对基于蒸馏分布知识判断为极度模棱两可的训练样本的不必要增强信心。我们在不同的 NLU 基准数据集上验证了我们的方法，实验结果证明了其在生成更好的标签分布方面的有效性。特别是，通过重新校准高度模棱两可样本的信心的过程，当对未见样本的预测与其真实标签不匹配时的过度自信问题得到了显著缓解。事实证明，这有助于生成比现有最先进方法更好的分布。此外，与现有方法相比，我们的方法在训练模型时效率更高，因为它不涉及额外的训练过程来优化标签分布。

##### **UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages**
2406.09717v1 by Trinh Pham, Khoi M. Le, Luu Anh Tuan

In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with
Optimized Embeddings and Vocabulary), a comprehensive approach developed to
improve the effectiveness of Cross-Lingual Transfer Learning, particularly in
languages with limited resources. Our approach tackles two essential elements
of a language model: the initialization of embeddings and the optimal
vocabulary size. Specifically, we propose a novel embedding initialization
method that leverages both lexical and semantic alignment for a language. In
addition, we present a method for systematically searching for the optimal
vocabulary size, ensuring a balance between model complexity and linguistic
coverage. Our experiments across multilingual datasets show that our approach
greatly improves the F1-Score in several languages. UniBridge is a robust and
adaptable solution for cross-lingual systems in various languages, highlighting
the significance of initializing embeddings and choosing the right vocabulary
size in cross-lingual environments.

摘要：在本文中，我們介紹了 UniBridge（使用最佳嵌入和詞彙進行跨語言轉移學習），這是一種綜合方法，用於提高跨語言轉移學習的有效性，特別是在資源有限的語言中。我們的做法應對了語言模型的兩個基本要素：嵌入的初始化和最佳詞彙大小。具體來說，我們提出了一種新穎的嵌入初始化方法，該方法利用了語言的詞彙和語義對齊。此外，我們提出了一種系統地搜索最佳詞彙大小的方法，確保模型複雜性和語言覆蓋率之間的平衡。我們在多語言數據集上的實驗表明，我們的做法極大地提高了多種語言中的 F1 分數。UniBridge 是一種適用於各種語言的跨語言系統的強大且適應性強的解決方案，突顯了在跨語言環境中初始化嵌入和選擇正確詞彙大小的重要性。

##### **Fine-Grained Urban Flow Inference with Multi-scale Representation Learning**
2406.09710v1 by Shilu Yuan, Dongfeng Li, Wei Liu, Xinxin Zhang, Meng Chen, Junjie Zhang, Yongshun Gong

Fine-grained urban flow inference (FUFI) is a crucial transportation service
aimed at improving traffic efficiency and safety. FUFI can infer fine-grained
urban traffic flows based solely on observed coarse-grained data. However, most
of existing methods focus on the influence of single-scale static geographic
information on FUFI, neglecting the interactions and dynamic information
between different-scale regions within the city. Different-scale geographical
features can capture redundant information from the same spatial areas. In
order to effectively learn multi-scale information across time and space, we
propose an effective fine-grained urban flow inference model called UrbanMSR,
which uses self-supervised contrastive learning to obtain dynamic multi-scale
representations of neighborhood-level and city-level geographic information,
and fuses multi-scale representations to improve fine-grained accuracy. The
fusion of multi-scale representations enhances fine-grained. We validate the
performance through extensive experiments on three real-world datasets. The
resutls compared with state-of-the-art methods demonstrate the superiority of
the proposed model.

摘要：精细化城市流推断（FUFI）是一项至关重要的交通服务，旨在提高交通效率和安全性。FUFI 可以仅根据观察到的粗粒度数据推断出精细化的城市交通流。然而，现有的大多数方法都集中于单尺度静态地理信息对 FUFI 的影响，而忽略了城市内不同尺度区域之间的交互和动态信息。不同尺度的地理特征可以从相同的空间区域获取冗余信息。为了有效地学习跨时间和空间的多尺度信息，我们提出了一种称为 UrbanMSR 的有效的精细化城市流推断模型，该模型使用自监督对比学习来获得邻里级和城市级地理信息的动态多尺度表示，并融合多尺度表示以提高精细化准确度。多尺度表示的融合增强了精细化。我们通过对三个真实世界数据集的广泛实验验证了性能。与最先进的方法相比，结果证明了所提出模型的优越性。

##### **Detecting Response Generation Not Requiring Factual Judgment**
2406.09702v1 by Ryohei Kamei, Daiki Shiono, Reina Akama, Jun Suzuki

With the remarkable development of large language models (LLMs), ensuring the
factuality of output has become a challenge. However, having all the contents
of the response with given knowledge or facts is not necessarily a good thing
in dialogues. This study aimed to achieve both attractiveness and factuality in
a dialogue response for which a task was set to predict sentences that do not
require factual correctness judgment such as agreeing, or personal
opinions/feelings. We created a dataset, dialogue dataset annotated with
fact-check-needed label (DDFC), for this task via crowdsourcing, and
classification tasks were performed on several models using this dataset. The
model with the highest classification accuracy could yield about 88% accurate
classification results.

摘要：隨著大型語言模型 (LLM) 的顯著發展，確保輸出的真實性已成為一項挑戰。然而，在對話中，讓回應的所有內容都具備既定的知識或事實並非一定是件好事。本研究旨在對話回應中同時實現吸引力和真實性，為此設定了一項任務，即預測不需要事實正確性判斷的句子，例如同意或個人意見/感受。我們透過群眾外包為此任務建立了一個帶有事實查核標籤的對話資料集 (DDFC)，並使用此資料集對多個模型執行分類任務。分類準確度最高的模型可以產生約 88% 的準確分類結果。

##### **FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation**
2406.09688v1 by Zijian Feng, Hanzhang Zhou, Zixiao Zhu, Kezhi Mao

Controllable text generation (CTG) seeks to craft texts adhering to specific
attributes, traditionally employing learning-based techniques such as training,
fine-tuning, or prefix-tuning with attribute-specific datasets. These
approaches, while effective, demand extensive computational and data resources.
In contrast, some proposed learning-free alternatives circumvent learning but
often yield inferior results, exemplifying the fundamental machine learning
trade-off between computational expense and model efficacy. To overcome these
limitations, we propose FreeCtrl, a learning-free approach that dynamically
adjusts the weights of selected feedforward neural network (FFN) vectors to
steer the outputs of large language models (LLMs). FreeCtrl hinges on the
principle that the weights of different FFN vectors influence the likelihood of
different tokens appearing in the output. By identifying and adaptively
adjusting the weights of attribute-related FFN vectors, FreeCtrl can control
the output likelihood of attribute keywords in the generated content. Extensive
experiments on single- and multi-attribute control reveal that the
learning-free FreeCtrl outperforms other learning-free and learning-based
methods, successfully resolving the dilemma between learning costs and model
performance.

摘要：可控文本生成 (CTG) 旨在生成符合特定屬性的文本，傳統上採用基於學習的技術，例如使用特定屬性的資料集進行訓練、微調或前綴微調。這些方法雖然有效，但需要大量的計算和資料資源。相比之下，一些提出的免學習替代方案避開了學習，但通常會產生較差的結果，這說明了計算成本和模型效能之間的基本機器學習權衡。為了克服這些限制，我們提出了 FreeCtrl，這是一種免學習方法，可以動態調整所選前饋神經網路 (FFN) 向量權重，以引導大型語言模型 (LLM) 的輸出。FreeCtrl 採用了一個原則，即不同 FFN 向量權重會影響輸出中出現不同記號的可能性。透過識別和適應性調整與屬性相關的 FFN 向量權重，FreeCtrl 可以控制生成內容中屬性關鍵字的輸出可能性。對單屬性和多屬性控制的廣泛實驗表明，免學習的 FreeCtrl 優於其他免學習和基於學習的方法，成功地解決了學習成本和模型效能之間的兩難困境。

##### **Explainable AI for Comparative Analysis of Intrusion Detection Models**
2406.09684v1 by Pap M. Corea, Yongxin Liu, Jian Wang, Shuteng Niu, Houbing Song

Explainable Artificial Intelligence (XAI) has become a widely discussed
topic, the related technologies facilitate better understanding of conventional
black-box models like Random Forest, Neural Networks and etc. However,
domain-specific applications of XAI are still insufficient. To fill this gap,
this research analyzes various machine learning models to the tasks of binary
and multi-class classification for intrusion detection from network traffic on
the same dataset using occlusion sensitivity. The models evaluated include
Linear Regression, Logistic Regression, Linear Support Vector Machine (SVM),
K-Nearest Neighbors (KNN), Random Forest, Decision Trees, and Multi-Layer
Perceptrons (MLP). We trained all models to the accuracy of 90\% on the
UNSW-NB15 Dataset. We found that most classifiers leverage only less than three
critical features to achieve such accuracies, indicating that effective feature
engineering could actually be far more important for intrusion detection than
applying complicated models. We also discover that Random Forest provides the
best performance in terms of accuracy, time efficiency and robustness. Data and
code available at https://github.com/pcwhy/XML-IntrusionDetection.git

摘要：可解釋人工智慧 (XAI) 已成為廣泛討論的主題，相關技術促進了對傳統黑盒模型（例如隨機森林、神經網路等）的更深入了解。然而，XAI 的特定領域應用仍然不足。為了填補這個缺口，本研究分析了各種機器學習模型，使用遮擋敏感性對網路流量中的入侵偵測執行二元和多類別分類任務。評估的模型包括線性回歸、邏輯回歸、線性支援向量機 (SVM)、K 最近鄰 (KNN)、隨機森林、決策樹和多層感知器 (MLP)。我們訓練所有模型在 UNSW-NB15 資料集上達到 90% 的準確度。我們發現，大多數分類器僅利用不到三個關鍵特徵來實現這種準確度，這表明有效的特徵工程實際上可能比應用複雜的模型對入侵偵測更重要。我們還發現，隨機森林在準確度、時間效率和穩健性方面提供了最佳性能。資料和程式碼可在 https://github.com/pcwhy/XML-IntrusionDetection.git 取得

##### **Optimizing Byte-level Representation for End-to-end ASR**
2406.09676v1 by Roger Hsiao, Liuhui Deng, Erik McDermott, Ruchir Travadi, Xiaodan Zhuang

We propose a novel approach to optimizing a byte-level representation for
end-to-end automatic speech recognition (ASR). Byte-level representation is
often used by large scale multilingual ASR systems when the character set of
the supported languages is large. The compactness and universality of
byte-level representation allow the ASR models to use smaller output
vocabularies and therefore, provide more flexibility. UTF-8 is a commonly used
byte-level representation for multilingual ASR, but it is not designed to
optimize machine learning tasks directly. By using auto-encoder and vector
quantization, we show that we can optimize a byte-level representation for ASR
and achieve better accuracy. Our proposed framework can incorporate information
from different modalities, and provides an error correction mechanism. In an
English/Mandarin dictation task, we show that a bilingual ASR model built with
this approach can outperform UTF-8 representation by 5% relative in error rate.

摘要：我們提出一個創新的方法來最佳化位元組層級表示，以進行端對端自動語音辨識 (ASR)。位元組層級表示通常由大型多語言 ASR 系統使用，當支援語言的字元集很大時。位元組層級表示的緊湊性和普遍性讓 ASR 模型可以使用較小的輸出詞彙表，因此提供更大的彈性。UTF-8 是多語言 ASR 常用的位元組層級表示，但它並未設計為直接最佳化機器學習任務。透過使用自動編碼器和向量量化，我們證明我們可以最佳化 ASR 的位元組層級表示，並獲得更好的準確度。我們提出的架構可以整合來自不同模式的資訊，並提供錯誤修正機制。在英語/華語聽寫任務中，我們證明使用此方法建構的雙語 ASR 模型，在錯誤率上可以比 UTF-8 表示高出 5%。

