{"2501.17384": {"publish_time": "2025-01-29", "title": "A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning", "paper_summary": "Recently, empowered with the powerful capabilities of neural networks,\nreinforcement learning (RL) has successfully tackled numerous challenging\ntasks. However, while these models demonstrate enhanced decision-making\nabilities, they are increasingly prone to overfitting. For instance, a trained\nRL model often fails to generalize to even minor variations of the same task,\nsuch as a change in background color or other minor semantic differences. To\naddress this issue, we propose a dual-agent adversarial policy learning\nframework, which allows agents to spontaneously learn the underlying semantics\nwithout introducing any human prior knowledge. Specifically, our framework\ninvolves a game process between two agents: each agent seeks to maximize the\nimpact of perturbing on the opponent's policy by producing representation\ndifferences for the same state, while maintaining its own stability against\nsuch perturbations. This interaction encourages agents to learn generalizable\npolicies, capable of handling irrelevant features from the high-dimensional\nobservations. Extensive experimental results on the Procgen benchmark\ndemonstrate that the adversarial process significantly improves the\ngeneralization performance of both agents, while also being applied to various\nRL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial\nframework, the RL agent outperforms the baseline methods by a significant\nmargin, especially in hard-level tasks, marking a significant step forward in\nthe generalization capabilities of deep reinforcement learning.", "paper_summary_zh": "<paragraph>\u8fd1\u671f\uff0c\u5728\u795e\u7ecf\u7db2\u8def\u5f37\u5927\u529f\u80fd\u7684\u52a0\u6301\u4e0b\uff0c\u5f37\u5316\u5b78\u7fd2 (RL) \u5df2\u6210\u529f\u89e3\u6c7a\u8a31\u591a\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\u3002\u7136\u800c\uff0c\u5118\u7ba1\u9019\u4e9b\u6a21\u578b\u5c55\u73fe\u51fa\u589e\u5f37\u7684\u6c7a\u7b56\u80fd\u529b\uff0c\u5b83\u5011\u537b\u8d8a\u4f86\u8d8a\u5bb9\u6613\u904e\u5ea6\u64ec\u5408\u3002\u4f8b\u5982\uff0c\u8a13\u7df4\u904e\u7684 RL \u6a21\u578b\u901a\u5e38\u7121\u6cd5\u6982\u62ec\u5230\u540c\u4e00\u500b\u4efb\u52d9\u7684\u5fae\u5c0f\u8b8a\u5316\uff0c\u4f8b\u5982\u80cc\u666f\u984f\u8272\u7684\u6539\u8b8a\u6216\u5176\u4ed6\u7d30\u5fae\u7684\u8a9e\u7fa9\u5dee\u7570\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u96d9\u4ee3\u7406\u5c0d\u6297\u7b56\u7565\u5b78\u7fd2\u6846\u67b6\uff0c\u8a72\u6846\u67b6\u5141\u8a31\u4ee3\u7406\u81ea\u52d5\u5b78\u7fd2\u57fa\u790e\u8a9e\u7fa9\uff0c\u800c\u7121\u9700\u5f15\u5165\u4efb\u4f55\u4eba\u985e\u5148\u9a57\u77e5\u8b58\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u6846\u67b6\u6d89\u53ca\u5169\u500b\u4ee3\u7406\u4e4b\u9593\u7684\u904a\u6232\u904e\u7a0b\uff1a\u6bcf\u500b\u4ee3\u7406\u90fd\u8a66\u5716\u901a\u904e\u7522\u751f\u76f8\u540c\u72c0\u614b\u7684\u8868\u793a\u5dee\u7570\u4f86\u6700\u5927\u5316\u5c0d\u5c0d\u624b\u7b56\u7565\u7684\u64fe\u52d5\u5f71\u97ff\uff0c\u540c\u6642\u4fdd\u6301\u5176\u81ea\u8eab\u5c0d\u9019\u4e9b\u64fe\u52d5\u7684\u7a69\u5b9a\u6027\u3002\u9019\u7a2e\u4ea4\u4e92\u4f5c\u7528\u9f13\u52f5\u4ee3\u7406\u5b78\u7fd2\u53ef\u6982\u62ec\u7684\u7b56\u7565\uff0c\u80fd\u5920\u8655\u7406\u9ad8\u7dad\u5ea6\u89c0\u5bdf\u4e2d\u7684\u7121\u95dc\u7279\u5fb5\u3002\u5728 Procgen \u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5c0d\u6297\u904e\u7a0b\u986f\u8457\u63d0\u9ad8\u4e86\u5169\u500b\u4ee3\u7406\u7684\u6cdb\u5316\u6548\u80fd\uff0c\u540c\u6642\u4e5f\u61c9\u7528\u65bc\u5404\u7a2e RL \u6f14\u7b97\u6cd5\uff0c\u4f8b\u5982\u8fd1\u7aef\u7b56\u7565\u6700\u4f73\u5316 (PPO)\u3002\u6709\u4e86\u5c0d\u6297\u6846\u67b6\uff0cRL \u4ee3\u7406\u7684\u8868\u73fe\u986f\u8457\u512a\u65bc\u57fa\u6e96\u65b9\u6cd5\uff0c\u7279\u5225\u662f\u5728\u9ad8\u96e3\u5ea6\u4efb\u52d9\u4e2d\uff0c\u6a19\u8a8c\u8457\u6df1\u5ea6\u5f37\u5316\u5b78\u7fd2\u7684\u6cdb\u5316\u80fd\u529b\u5411\u524d\u9081\u9032\u4e86\u4e00\u5927\u6b65\u3002</paragraph>", "author": "Zhengpeng Xie et.al.", "authors": "Zhengpeng Xie, Jiahang Cao, Yulong Zhang, Qiang Zhang, Renjing Xu", "id": "2501.17384v1", "paper_url": "http://arxiv.org/abs/2501.17384v1", "repo": "null"}}