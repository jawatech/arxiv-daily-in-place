{"2406.09920": {"publish_time": "2024-06-14", "title": "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "paper_summary": "Large Language Models (LLMs) can become outdated over time as they may lack\nupdated world knowledge, leading to factual knowledge errors and gaps.\nKnowledge Editing (KE) aims to overcome this challenge using weight updates\nthat do not require expensive retraining. We propose treating KE as an LLM\nalignment problem. Toward this goal, we introduce Knowledge Direct Preference\nOptimization (KDPO), a variation of the Direct Preference Optimization (DPO)\nthat is more effective for knowledge modifications. Our method is based on an\nonline approach that continually updates the knowledge stored in the model. We\nuse the current knowledge as a negative sample and the new knowledge we want to\nintroduce as a positive sample in a process called DPO. We also use\nteacher-forcing for negative sample generation and optimize using the positive\nsample, which helps maintain localized changes. We tested our KE method on\nvarious datasets and models, comparing it to several cutting-edge methods, with\n100 and 500 sequential edits. Additionally, we conducted an ablation study\ncomparing our method to the standard DPO approach. Our experimental results\nshow that our modified DPO method allows for more refined KE, achieving similar\nor better performance compared to previous methods.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u96a8\u8457\u6642\u9593\u63a8\u79fb\u53ef\u80fd\u6703\u904e\u6642\uff0c\u56e0\u70ba\u5b83\u5011\u53ef\u80fd\u7f3a\u4e4f\n\u66f4\u65b0\u7684\u4e16\u754c\u77e5\u8b58\uff0c\u5c0e\u81f4\u4e8b\u5be6\u77e5\u8b58\u932f\u8aa4\u548c\u7a7a\u767d\u3002\n\u77e5\u8b58\u7de8\u8f2f (KE) \u65e8\u5728\u514b\u670d\u9019\u500b\u6311\u6230\uff0c\u4f7f\u7528\u4e0d\u9700\u8981\u6602\u8cb4\u7684\u91cd\u65b0\u8a13\u7df4\u7684\u6b0a\u91cd\u66f4\u65b0\u3002\u6211\u5011\u5efa\u8b70\u5c07 KE \u8996\u70ba LLM\n\u5c0d\u9f4a\u554f\u984c\u3002\u70ba\u4e86\u9019\u500b\u76ee\u6a19\uff0c\u6211\u5011\u5f15\u5165\u4e86\u77e5\u8b58\u76f4\u63a5\u504f\u597d\n\u6700\u4f73\u5316 (KDPO)\uff0c\u9019\u662f\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u7684\u4e00\u7a2e\u8b8a\u9ad4\n\u66f4\u6709\u6548\u5730\u4fee\u6539\u77e5\u8b58\u3002\u6211\u5011\u7684\u6a21\u578b\u57fa\u65bc\u4e00\u7a2e\n\u6301\u7e8c\u66f4\u65b0\u6a21\u578b\u4e2d\u5132\u5b58\u77e5\u8b58\u7684\u7dda\u4e0a\u65b9\u6cd5\u3002\u6211\u5011\n\u4f7f\u7528\u7576\u524d\u77e5\u8b58\u4f5c\u70ba\u8ca0\u9762\u7bc4\u4f8b\uff0c\u6211\u5011\u60f3\u8981\n\u5728\u7a31\u70ba DPO \u7684\u904e\u7a0b\u4e2d\u5f15\u5165\u7684\u65b0\u77e5\u8b58\u4f5c\u70ba\u6b63\u9762\u7bc4\u4f8b\u3002\u6211\u5011\u4e5f\u4f7f\u7528\n\u6559\u5e2b\u5f37\u5236\u9032\u884c\u8ca0\u9762\u7bc4\u4f8b\u7522\u751f\uff0c\u4e26\u4f7f\u7528\u6b63\u9762\n\u7bc4\u4f8b\u9032\u884c\u6700\u4f73\u5316\uff0c\u9019\u6709\u52a9\u65bc\u7dad\u6301\u5c40\u90e8\u8b8a\u66f4\u3002\u6211\u5011\u5728\n\u5404\u7a2e\u8cc7\u6599\u96c6\u548c\u6a21\u578b\u4e0a\u6e2c\u8a66\u6211\u5011\u7684 KE \u6a21\u578b\uff0c\u5c07\u5176\u8207\u591a\u7a2e\u5c16\u7aef\u65b9\u6cd5\u9032\u884c\u6bd4\u8f03\uff0c\u9032\u884c\n100 \u548c 500 \u500b\u9806\u5e8f\u7de8\u8f2f\u3002\u6b64\u5916\uff0c\u6211\u5011\u9032\u884c\u4e86\u4e00\u9805\u6d88\u878d\u7814\u7a76\n\u5c07\u6211\u5011\u7684\u6a21\u578b\u8207\u6a19\u6e96 DPO \u65b9\u6cd5\u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\n\u8868\u660e\uff0c\u6211\u5011\u4fee\u6539\u7684 DPO \u65b9\u6cd5\u5141\u8a31\u66f4\u7cbe\u7dfb\u7684 KE\uff0c\u8207\u5148\u524d\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u9054\u5230\u985e\u4f3c\n\u6216\u66f4\u597d\u7684\u6548\u80fd\u3002", "author": "Amit Rozner et.al.", "authors": "Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum", "id": "2406.09920v1", "paper_url": "http://arxiv.org/abs/2406.09920v1", "repo": "null"}}