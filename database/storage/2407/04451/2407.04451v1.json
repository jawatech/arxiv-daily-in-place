{"2407.04451": {"publish_time": "2024-07-05", "title": "Hindsight Preference Learning for Offline Preference-based Reinforcement Learning", "paper_summary": "Offline preference-based reinforcement learning (RL), which focuses on\noptimizing policies using human preferences between pairs of trajectory\nsegments selected from an offline dataset, has emerged as a practical avenue\nfor RL applications. Existing works rely on extracting step-wise reward signals\nfrom trajectory-wise preference annotations, assuming that preferences\ncorrelate with the cumulative Markovian rewards. However, such methods fail to\ncapture the holistic perspective of data annotation: Humans often assess the\ndesirability of a sequence of actions by considering the overall outcome rather\nthan the immediate rewards. To address this challenge, we propose to model\nhuman preferences using rewards conditioned on future outcomes of the\ntrajectory segments, i.e. the hindsight information. For downstream RL\noptimization, the reward of each step is calculated by marginalizing over\npossible future outcomes, the distribution of which is approximated by a\nvariational auto-encoder trained using the offline dataset. Our proposed\nmethod, Hindsight Preference Learning (HPL), can facilitate credit assignment\nby taking full advantage of vast trajectory data available in massive unlabeled\ndatasets. Comprehensive empirical studies demonstrate the benefits of HPL in\ndelivering robust and advantageous rewards across various domains. Our code is\npublicly released at https://github.com/typoverflow/WiseRL.", "paper_summary_zh": "\u96e2\u7dda\u57fa\u65bc\u504f\u597d\u7684\u5f37\u5316\u5b78\u7fd2 (RL)\uff0c\u5c08\u6ce8\u65bc\u4f7f\u7528\u4eba\u985e\u504f\u597d\u4f86\u6700\u4f73\u5316\u7b56\u7565\uff0c\u9019\u4e9b\u504f\u597d\u4f86\u81ea\u96e2\u7dda\u8cc7\u6599\u96c6\u4e2d\u9078\u64c7\u7684\u8ecc\u8de1\u5340\u6bb5\u5c0d\u4e4b\u9593\uff0c\u5df2\u6210\u70ba RL \u61c9\u7528\u7684\u4e00\u689d\u5be6\u7528\u9014\u5f91\u3002\u73fe\u6709\u5de5\u4f5c\u4f9d\u8cf4\u65bc\u5f9e\u8ecc\u8de1\u504f\u597d\u8a3b\u89e3\u4e2d\u63d0\u53d6\u9010\u6b65\u734e\u52f5\u8a0a\u865f\uff0c\u5047\u8a2d\u504f\u597d\u8207\u7d2f\u7a4d\u99ac\u53ef\u592b\u734e\u52f5\u76f8\u95dc\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u672a\u80fd\u6355\u6349\u5230\u8cc7\u6599\u8a3b\u89e3\u7684\u6574\u9ad4\u89c0\u9ede\uff1a\u4eba\u985e\u901a\u5e38\u900f\u904e\u8003\u91cf\u6574\u9ad4\u7d50\u679c\u800c\u975e\u7acb\u5373\u734e\u52f5\uff0c\u4f86\u8a55\u4f30\u4e00\u7cfb\u5217\u52d5\u4f5c\u7684\u53ef\u53d6\u6027\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u63d0\u8b70\u4f7f\u7528\u53d7\u8ecc\u8de1\u5340\u6bb5\u672a\u4f86\u7d50\u679c\u5236\u7d04\u7684\u734e\u52f5\uff0c\u4ea6\u5373\u5f8c\u898b\u4e4b\u660e\u8cc7\u8a0a\uff0c\u4f86\u5efa\u6a21\u4eba\u985e\u504f\u597d\u3002\u5c0d\u65bc\u4e0b\u6e38 RL \u6700\u4f73\u5316\uff0c\u6bcf\u500b\u6b65\u9a5f\u7684\u734e\u52f5\u662f\u900f\u904e\u5c0d\u53ef\u80fd\u7684\u672a\u4f86\u7d50\u679c\u9032\u884c\u908a\u969b\u5316\u4f86\u8a08\u7b97\uff0c\u5176\u5206\u4f48\u7531\u4f7f\u7528\u96e2\u7dda\u8cc7\u6599\u96c6\u8a13\u7df4\u7684\u8b8a\u7570\u81ea\u52d5\u7de8\u78bc\u5668\u4f86\u8fd1\u4f3c\u3002\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5f8c\u898b\u4e4b\u660e\u504f\u597d\u5b78\u7fd2 (HPL)\uff0c\u53ef\u4ee5\u900f\u904e\u5145\u5206\u5229\u7528\u5927\u91cf\u672a\u6a19\u8a18\u8cc7\u6599\u96c6\u4e2d\u53ef\u7528\u7684\u9f90\u5927\u8ecc\u8de1\u8cc7\u6599\uff0c\u4f86\u4fc3\u9032\u4fe1\u7528\u5206\u914d\u3002\u5168\u9762\u7684\u5be6\u8b49\u7814\u7a76\u8b49\u660e\u4e86 HPL \u5728\u63d0\u4f9b\u5404\u7a2e\u9818\u57df\u7684\u7a69\u5065\u4e14\u6709\u5229\u734e\u52f5\u65b9\u9762\u7684\u512a\u9ede\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u767c\u5e03\u5728 https://github.com/typoverflow/WiseRL\u3002", "author": "Chen-Xiao Gao et.al.", "authors": "Chen-Xiao Gao, Shengjun Fang, Chenjun Xiao, Yang Yu, Zongzhang Zhang", "id": "2407.04451v1", "paper_url": "http://arxiv.org/abs/2407.04451v1", "repo": "https://github.com/typoverflow/wiserl"}}