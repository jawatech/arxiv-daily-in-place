{"2409.10775": {"publish_time": "2024-09-16", "title": "Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?", "paper_summary": "Image classification models, including convolutional neural networks (CNNs),\nperform well on a variety of classification tasks but struggle under conditions\nof partial occlusion, i.e., conditions in which objects are partially covered\nfrom the view of a camera. Methods to improve performance under occlusion,\nincluding data augmentation, part-based clustering, and more inherently robust\narchitectures, including Vision Transformer (ViT) models, have, to some extent,\nbeen evaluated on their ability to classify objects under partial occlusion.\nHowever, evaluations of these methods have largely relied on images containing\nartificial occlusion, which are typically computer-generated and therefore\ninexpensive to label. Additionally, methods are rarely compared against each\nother, and many methods are compared against early, now outdated, deep learning\nmodels. We contribute the Image Recognition Under Occlusion (IRUO) dataset,\nbased on the recently developed Occluded Video Instance Segmentation (OVIS)\ndataset (arXiv:2102.01558). IRUO utilizes real-world and artificially occluded\nimages to test and benchmark leading methods' robustness to partial occlusion\nin visual recognition tasks. In addition, we contribute the design and results\nof a human study using images from IRUO that evaluates human classification\nperformance at multiple levels and types of occlusion. We find that modern\nCNN-based models show improved recognition accuracy on occluded images compared\nto earlier CNN-based models, and ViT-based models are more accurate than\nCNN-based models on occluded images, performing only modestly worse than human\naccuracy. We also find that certain types of occlusion, including diffuse\nocclusion, where relevant objects are seen through \"holes\" in occluders such as\nfences and leaves, can greatly reduce the accuracy of deep recognition models\nas compared to humans, especially those with CNN backbones.", "paper_summary_zh": "\u5f71\u50cf\u5206\u985e\u6a21\u578b\uff0c\u5305\u62ec\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN)\uff0c\u5728\u5404\u7a2e\u5206\u985e\u4efb\u52d9\u4e2d\u8868\u73fe\u826f\u597d\uff0c\u4f46\u5728\u90e8\u5206\u906e\u64cb\u7684\u60c5\u6cc1\u4e0b\u8868\u73fe\u4e0d\u4f73\uff0c\u4e5f\u5c31\u662f\u7269\u4ef6\u5728\u76f8\u6a5f\u8996\u89d2\u4e2d\u88ab\u90e8\u5206\u906e\u4f4f\u7684\u60c5\u6cc1\u3002\u6539\u5584\u906e\u64cb\u4e0b\u7684\u8868\u73fe\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u8cc7\u6599\u64f4\u5145\u3001\u57fa\u65bc\u90e8\u5206\u7684\u7fa4\u96c6\uff0c\u4ee5\u53ca\u672c\u8cea\u4e0a\u66f4\u7a69\u5065\u7684\u67b6\u69cb\uff0c\u5305\u62ec\u8996\u89baTransformer (ViT) \u6a21\u578b\uff0c\u5728\u67d0\u7a2e\u7a0b\u5ea6\u4e0a\u5df2\u6839\u64da\u5176\u5728\u90e8\u5206\u906e\u64cb\u4e0b\u5206\u985e\u7269\u4ef6\u7684\u80fd\u529b\u9032\u884c\u8a55\u4f30\u3002\u7136\u800c\uff0c\u5c0d\u9019\u4e9b\u65b9\u6cd5\u7684\u8a55\u4f30\u4e3b\u8981\u4f9d\u8cf4\u5305\u542b\u4eba\u5de5\u906e\u64cb\u7684\u5f71\u50cf\uff0c\u9019\u4e9b\u5f71\u50cf\u901a\u5e38\u662f\u96fb\u8166\u7522\u751f\u7684\uff0c\u56e0\u6b64\u6a19\u8a18\u6210\u672c\u4f4e\u5ec9\u3002\u6b64\u5916\uff0c\u5f88\u5c11\u5c07\u65b9\u6cd5\u76f8\u4e92\u6bd4\u8f03\uff0c\u800c\u4e14\u8a31\u591a\u65b9\u6cd5\u90fd\u662f\u8207\u73fe\u5728\u5df2\u904e\u6642\u7684\u65e9\u671f\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u8ca2\u737b\u4e86\u906e\u64cb\u4e0b\u7684\u5f71\u50cf\u8fa8\u8b58 (IRUO) \u8cc7\u6599\u96c6\uff0c\u5b83\u662f\u57fa\u65bc\u6700\u8fd1\u958b\u767c\u7684\u906e\u64cb\u8996\u8a0a\u5be6\u4f8b\u5206\u5272 (OVIS) \u8cc7\u6599\u96c6 (arXiv:2102.01558)\u3002IRUO \u5229\u7528\u771f\u5be6\u4e16\u754c\u548c\u4eba\u5de5\u906e\u64cb\u7684\u5f71\u50cf\u4f86\u6e2c\u8a66\u548c\u8a55\u91cf\u9818\u5148\u65b9\u6cd5\u5728\u8996\u89ba\u8fa8\u8b58\u4efb\u52d9\u4e2d\u5c0d\u90e8\u5206\u906e\u64cb\u7684\u7a69\u5065\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u8ca2\u737b\u4e86\u4f7f\u7528\u4f86\u81ea IRUO \u7684\u5f71\u50cf\u7684\u4eba\u985e\u7814\u7a76\u7684\u8a2d\u8a08\u548c\u7d50\u679c\uff0c\u8a72\u7814\u7a76\u8a55\u4f30\u4e86\u4eba\u985e\u5728\u591a\u500b\u5c64\u7d1a\u548c\u906e\u64cb\u985e\u578b\u4e0b\u7684\u5206\u985e\u8868\u73fe\u3002\u6211\u5011\u767c\u73fe\uff0c\u8207\u65e9\u671f\u7684\u57fa\u65bc CNN \u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u73fe\u4ee3\u7684\u57fa\u65bc CNN \u7684\u6a21\u578b\u5728\u906e\u64cb\u5f71\u50cf\u4e0a\u5c55\u73fe\u51fa\u66f4\u9ad8\u7684\u8fa8\u8b58\u6e96\u78ba\u7387\uff0c\u800c\u57fa\u65bc ViT \u7684\u6a21\u578b\u5728\u906e\u64cb\u5f71\u50cf\u4e0a\u7684\u6e96\u78ba\u7387\u9ad8\u65bc\u57fa\u65bc CNN \u7684\u6a21\u578b\uff0c\u50c5\u6bd4\u4eba\u985e\u6e96\u78ba\u7387\u4f4e\u4e00\u9ede\u3002\u6211\u5011\u9084\u767c\u73fe\uff0c\u67d0\u4e9b\u985e\u578b\u7684\u906e\u64cb\uff0c\u5305\u62ec\u6f2b\u5c04\u906e\u64cb\uff0c\u5176\u4e2d\u76f8\u95dc\u7269\u4ef6\u900f\u904e\u906e\u64cb\u7269\uff08\u4f8b\u5982\u570d\u6b04\u548c\u6a39\u8449\uff09\u7684\u300c\u5b54\u6d1e\u300d\u53ef\u898b\uff0c\u8207\u4eba\u985e\u76f8\u6bd4\uff0c\u6703\u5927\u5e45\u964d\u4f4e\u6df1\u5ea6\u8fa8\u8b58\u6a21\u578b\u7684\u6e96\u78ba\u7387\uff0c\u7279\u5225\u662f\u90a3\u4e9b\u5177\u6709 CNN \u4e3b\u5e79\u7684\u6a21\u578b\u3002", "author": "Kaleb Kassaw et.al.", "authors": "Kaleb Kassaw, Francesco Luzi, Leslie M. Collins, Jordan M. Malof", "id": "2409.10775v1", "paper_url": "http://arxiv.org/abs/2409.10775v1", "repo": "null"}}