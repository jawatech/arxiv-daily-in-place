{"2408.01343": {"publish_time": "2024-08-02", "title": "StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal Semantic Segmentation", "paper_summary": "Multimodal semantic segmentation shows significant potential for enhancing\nsegmentation accuracy in complex scenes. However, current methods often\nincorporate specialized feature fusion modules tailored to specific modalities,\nthereby restricting input flexibility and increasing the number of training\nparameters. To address these challenges, we propose StitchFusion, a\nstraightforward yet effective modal fusion framework that integrates\nlarge-scale pre-trained models directly as encoders and feature fusers. This\napproach facilitates comprehensive multi-modal and multi-scale feature fusion,\naccommodating any visual modal inputs. Specifically, Our framework achieves\nmodal integration during encoding by sharing multi-modal visual information. To\nenhance information exchange across modalities, we introduce a\nmulti-directional adapter module (MultiAdapter) to enable cross-modal\ninformation transfer during encoding. By leveraging MultiAdapter to propagate\nmulti-scale information across pre-trained encoders during the encoding\nprocess, StitchFusion achieves multi-modal visual information integration\nduring encoding. Extensive comparative experiments demonstrate that our model\nachieves state-of-the-art performance on four multi-modal segmentation datasets\nwith minimal additional parameters. Furthermore, the experimental integration\nof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their\ncomplementary nature. Our code is available at StitchFusion_repo.", "paper_summary_zh": "\u591a\u6a21\u614b\u8a9e\u610f\u5206\u5272\u5728\u589e\u5f37\u8907\u96dc\u5834\u666f\u4e2d\u7684\u5206\u5272\u6e96\u78ba\u5ea6\u65b9\u9762\u5c55\u73fe\u51fa\u986f\u8457\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u7576\u524d\u65b9\u6cd5\u901a\u5e38\u6703\u7d0d\u5165\u91dd\u5c0d\u7279\u5b9a\u6a21\u614b\u91cf\u8eab\u6253\u9020\u7684\u5c08\u696d\u7279\u5fb5\u878d\u5408\u6a21\u7d44\uff0c\u5f9e\u800c\u9650\u5236\u8f38\u5165\u5f48\u6027\u4e26\u589e\u52a0\u8a13\u7df4\u53c3\u6578\u6578\u91cf\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa StitchFusion\uff0c\u4e00\u500b\u76f4\u63a5\u5c07\u5927\u898f\u6a21\u9810\u8a13\u7df4\u6a21\u578b\u4f5c\u70ba\u7de8\u78bc\u5668\u548c\u7279\u5fb5\u878d\u5408\u5668\u7684\u7c21\u55ae\u537b\u6709\u6548\u7684\u6a21\u614b\u878d\u5408\u67b6\u69cb\u3002\u9019\u7a2e\u65b9\u6cd5\u4fc3\u9032\u4e86\u5168\u9762\u7684\u591a\u6a21\u614b\u548c\u591a\u5c3a\u5ea6\u7279\u5fb5\u878d\u5408\uff0c\u5bb9\u7d0d\u4efb\u4f55\u8996\u89ba\u6a21\u614b\u8f38\u5165\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u67b6\u69cb\u900f\u904e\u5171\u4eab\u591a\u6a21\u614b\u8996\u89ba\u8cc7\u8a0a\uff0c\u5728\u7de8\u78bc\u904e\u7a0b\u4e2d\u5be6\u73fe\u6a21\u614b\u6574\u5408\u3002\u70ba\u4e86\u589e\u5f37\u8de8\u6a21\u614b\u7684\u8cc7\u8a0a\u4ea4\u63db\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u591a\u5411\u9069\u914d\u5668\u6a21\u7d44 (MultiAdapter)\uff0c\u4ee5\u4fbf\u5728\u7de8\u78bc\u904e\u7a0b\u4e2d\u5be6\u73fe\u8de8\u6a21\u614b\u8cc7\u8a0a\u50b3\u8f38\u3002\u900f\u904e\u5229\u7528 MultiAdapter \u5728\u7de8\u78bc\u904e\u7a0b\u4e2d\u8de8\u9810\u8a13\u7df4\u7de8\u78bc\u5668\u50b3\u64ad\u591a\u5c3a\u5ea6\u8cc7\u8a0a\uff0cStitchFusion \u5728\u7de8\u78bc\u904e\u7a0b\u4e2d\u5be6\u73fe\u4e86\u591a\u6a21\u614b\u8996\u89ba\u8cc7\u8a0a\u6574\u5408\u3002\u5ee3\u6cdb\u7684\u6bd4\u8f03\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u56db\u500b\u591a\u6a21\u614b\u5206\u5272\u8cc7\u6599\u96c6\u4e0a\u5be6\u73fe\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u4e14\u984d\u5916\u53c3\u6578\u6700\u5c11\u3002\u6b64\u5916\uff0cMultiAdapter \u8207\u73fe\u6709\u7279\u5fb5\u878d\u5408\u6a21\u7d44 (FFM) \u7684\u5be6\u9a57\u6574\u5408\u7a81\u986f\u4e86\u5b83\u5011\u7684\u4e92\u88dc\u6027\u8cea\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 StitchFusion_repo \u53d6\u5f97\u3002", "author": "Bingyu Li et.al.", "authors": "Bingyu Li, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li", "id": "2408.01343v1", "paper_url": "http://arxiv.org/abs/2408.01343v1", "repo": "null"}}