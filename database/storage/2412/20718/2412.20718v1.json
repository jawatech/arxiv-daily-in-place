{"2412.20718": {"publish_time": "2024-12-30", "title": "M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs", "paper_summary": "Recently, large foundation models, including large language models (LLMs) and\nlarge vision-language models (LVLMs), have become essential tools in critical\nfields such as law, finance, and healthcare. As these models increasingly\nintegrate into our daily life, it is necessary to conduct moral evaluation to\nensure that their outputs align with human values and remain within moral\nboundaries. Previous works primarily focus on LLMs, proposing moral datasets\nand benchmarks limited to text modality. However, given the rapid development\nof LVLMs, there is still a lack of multimodal moral evaluation methods. To\nbridge this gap, we introduce M$^3$oralBench, the first MultiModal Moral\nBenchmark for LVLMs. M$^3$oralBench expands the everyday moral scenarios in\nMoral Foundations Vignettes (MFVs) and employs the text-to-image diffusion\nmodel, SD3.0, to create corresponding scenario images. It conducts moral\nevaluation across six moral foundations of Moral Foundations Theory (MFT) and\nencompasses tasks in moral judgement, moral classification, and moral response,\nproviding a comprehensive assessment of model performance in multimodal moral\nunderstanding and reasoning. Extensive experiments on 10 popular open-source\nand closed-source LVLMs demonstrate that M$^3$oralBench is a challenging\nbenchmark, exposing notable moral limitations in current models. Our benchmark\nis publicly available.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u5728\u5185\u7684\u5927\u578b\u57fa\u7840\u6a21\u578b\u5df2\u6210\u4e3a\u6cd5\u5f8b\u3001\u91d1\u878d\u548c\u533b\u7597\u4fdd\u5065\u7b49\u5173\u952e\u9886\u57df\u7684\u5fc5\u8981\u5de5\u5177\u3002\u968f\u7740\u8fd9\u4e9b\u6a21\u578b\u65e5\u76ca\u878d\u5165\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\uff0c\u6709\u5fc5\u8981\u8fdb\u884c\u9053\u5fb7\u8bc4\u4f30\uff0c\u4ee5\u786e\u4fdd\u5176\u8f93\u51fa\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e00\u81f4\u5e76\u4fdd\u6301\u5728\u9053\u5fb7\u754c\u9650\u5185\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728 LLM \u4e0a\uff0c\u63d0\u51fa\u4e86\u4ec5\u9650\u4e8e\u6587\u672c\u6a21\u5f0f\u7684\u9053\u5fb7\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002\u7136\u800c\uff0c\u9274\u4e8e LVLMs \u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4ecd\u7136\u7f3a\u4e4f\u591a\u6a21\u6001\u9053\u5fb7\u8bc4\u4f30\u65b9\u6cd5\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 M$^3$oralBench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u9488\u5bf9 LVLMs \u7684\u591a\u6a21\u6001\u9053\u5fb7\u57fa\u51c6\u3002M$^3$oralBench \u6269\u5c55\u4e86\u9053\u5fb7\u57fa\u7840\u5c0f\u63d2\u56fe (MFV) \u4e2d\u7684\u65e5\u5e38\u9053\u5fb7\u573a\u666f\uff0c\u5e76\u91c7\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b SD3.0 \u6765\u521b\u5efa\u76f8\u5e94\u7684\u573a\u666f\u56fe\u50cf\u3002\u5b83\u6839\u636e\u9053\u5fb7\u57fa\u7840\u7406\u8bba (MFT) \u7684\u516d\u4e2a\u9053\u5fb7\u57fa\u7840\u8fdb\u884c\u9053\u5fb7\u8bc4\u4f30\uff0c\u5e76\u5305\u542b\u9053\u5fb7\u5224\u65ad\u3001\u9053\u5fb7\u5206\u7c7b\u548c\u9053\u5fb7\u53cd\u5e94\u7684\u4efb\u52a1\uff0c\u5bf9\u6a21\u578b\u5728\u591a\u6a21\u6001\u9053\u5fb7\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002\u5bf9 10 \u4e2a\u6d41\u884c\u7684\u5f00\u6e90\u548c\u95ed\u6e90 LVLMs \u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cM$^3$oralBench \u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u4e2d\u660e\u663e\u7684\u9053\u5fb7\u9650\u5236\u3002\u6211\u4eec\u7684\u57fa\u51c6\u662f\u516c\u5f00\u7684\u3002", "author": "Bei Yan et.al.", "authors": "Bei Yan, Jie Zhang, Zhiyuan Chen, Shiguang Shan, Xilin Chen", "id": "2412.20718v1", "paper_url": "http://arxiv.org/abs/2412.20718v1", "repo": "null"}}