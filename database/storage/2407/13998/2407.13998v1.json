{"2407.13998": {"publish_time": "2024-07-19", "title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering", "paper_summary": "Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research.", "paper_summary_zh": "\u57fa\u65bc\u64f7\u53d6\u64f4\u5145\u751f\u6210\uff08RAG-QA\uff09\u7684\u554f\u984c\u89e3\u7b54\u662f NLP \u4e2d\u4e00\u500b\u91cd\u8981\u7684\u7814\u7a76\u4e3b\u984c\uff0c\u4e26\u6709\u5ee3\u6cdb\u7684\u771f\u5be6\u4e16\u754c\u61c9\u7528\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u73fe\u6709\u7684\u8cc7\u6599\u96c6\u4e0d\u662f\u4f7f\u7528\u55ae\u4e00\u4f86\u6e90\u8a9e\u6599\u5eab\u5efa\u69cb\uff0c\u5c31\u662f\u7531\u7c21\u77ed\u7684\u62bd\u53d6\u5f0f\u7b54\u6848\u7d44\u6210\uff0c\u9019\u7121\u6cd5\u8a55\u4f30\u57fa\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684 RAG-QA \u7cfb\u7d71\u5728\u8de8\u7db2\u57df\u6982\u62ec\u4e0a\u7684\u8868\u73fe\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u9577\u7bc7 RobustQA\uff08LFRQA\uff09\uff0c\u4e00\u500b\u65b0\u7684\u8cc7\u6599\u96c6\uff0c\u5305\u542b\u4eba\u5de5\u64b0\u5beb\u7684\u9577\u7bc7\u7b54\u6848\uff0c\u5c07\u4f86\u81ea\u591a\u500b\u6587\u4ef6\u7684\u7c21\u77ed\u62bd\u53d6\u5f0f\u7b54\u6848\u6574\u5408\u5230\u4e00\u500b\u55ae\u4e00\u7684\u9023\u8cab\u6558\u8ff0\u4e2d\uff0c\u6db5\u84cb 26K \u500b\u67e5\u8a62\u548c\u6a6b\u8de8\u4e03\u500b\u4e0d\u540c\u7db2\u57df\u7684\u5927\u578b\u8a9e\u6599\u5eab\u3002\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa RAG-QA Arena\uff0c\u900f\u904e\u4f7f\u7528 LLM \u4f5c\u70ba\u8a55\u4f30\u5668\uff0c\u76f4\u63a5\u6bd4\u8f03\u6a21\u578b\u7522\u751f\u7684\u7b54\u6848\u548c LFRQA \u7684\u7b54\u6848\u3002\u6211\u5011\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cRAG-QA Arena \u548c\u4eba\u985e\u5c0d\u7b54\u6848\u54c1\u8cea\u7684\u5224\u65b7\u9ad8\u5ea6\u76f8\u95dc\u3002\u6b64\u5916\uff0c\u53ea\u6709 41.3% \u6700\u5177\u7af6\u722d\u529b\u7684 LLM \u7b54\u6848\u512a\u65bc LFRQA \u7684\u7b54\u6848\uff0c\u9019\u8b49\u660e\u4e86 RAG-QA Arena \u662f\u672a\u4f86\u7814\u7a76\u5177\u6709\u6311\u6230\u6027\u7684\u8a55\u4f30\u5e73\u53f0\u3002", "author": "Rujun Han et.al.", "authors": "Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang, Bonan Min, Vittorio Castelli", "id": "2407.13998v1", "paper_url": "http://arxiv.org/abs/2407.13998v1", "repo": "https://github.com/awslabs/rag-qa-arena"}}