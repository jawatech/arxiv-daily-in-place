{"2412.15504": {"publish_time": "2024-12-20", "title": "Mitigating Social Bias in Large Language Models: A Multi-Objective Approach within a Multi-Agent Framework", "paper_summary": "Natural language processing (NLP) has seen remarkable advancements with the\ndevelopment of large language models (LLMs). Despite these advancements, LLMs\noften produce socially biased outputs. Recent studies have mainly addressed\nthis problem by prompting LLMs to behave ethically, but this approach results\nin unacceptable performance degradation. In this paper, we propose a\nmulti-objective approach within a multi-agent framework (MOMA) to mitigate\nsocial bias in LLMs without significantly compromising their performance. The\nkey idea of MOMA involves deploying multiple agents to perform causal\ninterventions on bias-related contents of the input questions, breaking the\nshortcut connection between these contents and the corresponding answers.\nUnlike traditional debiasing techniques leading to performance degradation,\nMOMA substantially reduces bias while maintaining accuracy in downstream tasks.\nOur experiments conducted on two datasets and two models demonstrate that MOMA\nreduces bias scores by up to 87.7%, with only a marginal performance\ndegradation of up to 6.8% in the BBQ dataset. Additionally, it significantly\nenhances the multi-objective metric icat in the StereoSet dataset by up to\n58.1%. Code will be made available at https://github.com/Cortantse/MOMA.", "paper_summary_zh": "\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u767c\u5c55\u800c\u6709\u4e86\u986f\u8457\u7684\u9032\u6b65\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u9032\u6b65\uff0cLLM \u7d93\u5e38\u7522\u751f\u793e\u6703\u504f\u898b\u7684\u8f38\u51fa\u3002\u6700\u8fd1\u7684\u7814\u7a76\u4e3b\u8981\u900f\u904e\u63d0\u793a LLM \u4ee5\u7b26\u5408\u9053\u5fb7\u7684\u65b9\u5f0f\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u4f46\u9019\u7a2e\u65b9\u6cd5\u5c0e\u81f4\u4e86\u7121\u6cd5\u63a5\u53d7\u7684\u6548\u80fd\u4e0b\u964d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u591a\u4e3b\u9ad4\u67b6\u69cb (MOMA) \u5167\u7684\u591a\u76ee\u6a19\u65b9\u6cd5\uff0c\u4ee5\u6e1b\u8f15 LLM \u4e2d\u7684\u793e\u6703\u504f\u898b\uff0c\u800c\u4e0d\u6703\u986f\u8457\u640d\u5bb3\u5176\u6548\u80fd\u3002MOMA \u7684\u95dc\u9375\u601d\u60f3\u6d89\u53ca\u90e8\u7f72\u591a\u500b\u4e3b\u9ad4\uff0c\u5c0d\u8f38\u5165\u554f\u984c\u4e2d\u8207\u504f\u898b\u76f8\u95dc\u7684\u5167\u5bb9\u57f7\u884c\u56e0\u679c\u5e72\u9810\uff0c\u6253\u7834\u9019\u4e9b\u5167\u5bb9\u8207\u5c0d\u61c9\u7b54\u6848\u4e4b\u9593\u7684\u6377\u5f91\u9023\u63a5\u3002\u8207\u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\u7684\u50b3\u7d71\u53bb\u504f\u898b\u6280\u8853\u4e0d\u540c\uff0cMOMA \u5728\u7dad\u6301\u4e0b\u6e38\u4efb\u52d9\u6e96\u78ba\u6027\u7684\u540c\u6642\uff0c\u5927\u5e45\u6e1b\u5c11\u4e86\u504f\u898b\u3002\u6211\u5011\u5728\u5169\u500b\u8cc7\u6599\u96c6\u548c\u5169\u500b\u6a21\u578b\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8868\u660e\uff0cMOMA \u5c07\u504f\u898b\u5206\u6578\u964d\u4f4e\u4e86 87.7%\uff0c\u5728 BBQ \u8cc7\u6599\u96c6\u4e2d\u7684\u6548\u80fd\u4e0b\u964d\u5e45\u5ea6\u50c5\u70ba 6.8%\u3002\u6b64\u5916\uff0c\u5b83\u5728 StereoSet \u8cc7\u6599\u96c6\u4e2d\u5c07\u591a\u76ee\u6a19\u6307\u6a19 icat \u5927\u5e45\u63d0\u5347\u4e86 58.1%\u3002\u7a0b\u5f0f\u78bc\u5c07\u5728 https://github.com/Cortantse/MOMA \u4e0a\u63d0\u4f9b\u3002", "author": "Zhenjie Xu et.al.", "authors": "Zhenjie Xu, Wenqing Chen, Yi Tang, Xuanying Li, Cheng Hu, Zhixuan Chu, Kui Ren, Zibin Zheng, Zhichao Lu", "id": "2412.15504v1", "paper_url": "http://arxiv.org/abs/2412.15504v1", "repo": "null"}}