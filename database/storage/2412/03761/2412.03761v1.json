{"2412.03761": {"publish_time": "2024-12-04", "title": "Language Model Meets Prototypes: Towards Interpretable Text Classification Models through Prototypical Networks", "paper_summary": "Pretrained transformer-based Language Models (LMs) are well-known for their\nability to achieve significant improvement on NLP tasks, but their black-box\nnature, which leads to a lack of interpretability, has been a major concern. My\ndissertation focuses on developing intrinsically interpretable models when\nusing LMs as encoders while maintaining their superior performance via\nprototypical networks. I initiated my research by investigating enhancements in\nperformance for interpretable models of sarcasm detection. My proposed approach\nfocuses on capturing sentiment incongruity to enhance accuracy while offering\ninstance-based explanations for the classification decisions. Later, I\ndeveloped a novel white-box multi-head graph attention-based prototype network\ndesigned to explain the decisions of text classification models without\nsacrificing the accuracy of the original black-box LMs. In addition, I am\nworking on extending the attention-based prototype network with contrastive\nlearning to redesign an interpretable graph neural network, aiming to enhance\nboth the interpretability and performance of the model in document\nclassification.", "paper_summary_zh": "\u9810\u5148\u8a13\u7df4\u597d\u7684\u57fa\u65bc Transformer \u7684\u8a9e\u8a00\u6a21\u578b (LM) \u4ee5\u5176\u5728 NLP \u4efb\u52d9\u4e2d\u53d6\u5f97\u986f\u8457\u9032\u6b65\u7684\u80fd\u529b\u800c\u805e\u540d\uff0c\u4f46\u5b83\u5011\u7684\u9ed1\u76d2\u6027\u8cea\u5c0e\u81f4\u7f3a\u4e4f\u53ef\u89e3\u91cb\u6027\uff0c\u4e00\u76f4\u662f\u4e00\u500b\u4e3b\u8981\u554f\u984c\u3002\u6211\u7684\u8ad6\u6587\u91cd\u9ede\u5728\u65bc\u5728\u4f7f\u7528 LM \u4f5c\u70ba\u7de8\u78bc\u5668\u6642\u958b\u767c\u5167\u5728\u53ef\u89e3\u91cb\u7684\u6a21\u578b\uff0c\u540c\u6642\u901a\u904e\u539f\u578b\u7db2\u8def\u7dad\u6301\u5176\u512a\u7570\u7684\u6548\u80fd\u3002\u6211\u900f\u904e\u7814\u7a76\u8af7\u523a\u5075\u6e2c\u7684\u53ef\u89e3\u91cb\u6a21\u578b\u7684\u6548\u80fd\u63d0\u5347\u4f86\u555f\u52d5\u6211\u7684\u7814\u7a76\u3002\u6211\u63d0\u51fa\u7684\u65b9\u6cd5\u5c08\u6ce8\u65bc\u6355\u6349\u60c5\u7dd2\u4e0d\u4e00\u81f4\u6027\uff0c\u4ee5\u63d0\u9ad8\u6e96\u78ba\u5ea6\uff0c\u540c\u6642\u70ba\u5206\u985e\u6c7a\u7b56\u63d0\u4f9b\u57fa\u65bc\u5be6\u4f8b\u7684\u89e3\u91cb\u3002\u5f8c\u4f86\uff0c\u6211\u958b\u767c\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u767d\u76d2\u591a\u982d\u5716\u5f62\u6ce8\u610f\u529b\u539f\u578b\u7db2\u8def\uff0c\u65e8\u5728\u89e3\u91cb\u6587\u5b57\u5206\u985e\u6a21\u578b\u7684\u6c7a\u7b56\uff0c\u800c\u4e0d\u6703\u72a7\u7272\u539f\u59cb\u9ed1\u76d2 LM \u7684\u6e96\u78ba\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u6b63\u5728\u52aa\u529b\u5c07\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u539f\u578b\u7db2\u8def\u8207\u5c0d\u6bd4\u5b78\u7fd2\u64f4\u5c55\uff0c\u4ee5\u91cd\u65b0\u8a2d\u8a08\u4e00\u500b\u53ef\u89e3\u91cb\u7684\u5716\u5f62\u795e\u7d93\u7db2\u8def\uff0c\u65e8\u5728\u589e\u5f37\u6a21\u578b\u5728\u6587\u4ef6\u5206\u985e\u4e2d\u7684\u53ef\u89e3\u91cb\u6027\u548c\u6548\u80fd\u3002", "author": "Ximing Wen et.al.", "authors": "Ximing Wen", "id": "2412.03761v1", "paper_url": "http://arxiv.org/abs/2412.03761v1", "repo": "null"}}