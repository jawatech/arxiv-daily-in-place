{"2411.13045": {"publish_time": "2024-11-20", "title": "Explainable LLM-driven Multi-dimensional Distillation for E-Commerce Relevance Learning", "paper_summary": "Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.", "paper_summary_zh": "<paragraph>\u6709\u6548\u7684\u67e5\u8a62\u9805\u76ee\u76f8\u95dc\u6027\u5efa\u6a21\u5c0d\u65bc\u63d0\u5347\u4f7f\u7528\u8005\u9ad4\u9a57\u548c\u4fdd\u969c\u96fb\u5b50\u5546\u52d9\u641c\u5c0b\u7cfb\u7d71\u4e2d\u7684\u4f7f\u7528\u8005\u6eff\u610f\u5ea6\u81f3\u95dc\u91cd\u8981\u3002\u6700\u8fd1\uff0c\u53d7\u76ca\u65bc\u5ee3\u6cdb\u7684\u5167\u5728\u77e5\u8b58\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u65b9\u6cd5\u5c55\u73fe\u4e86\u5f37\u5927\u7684\u6548\u80fd\u548c\u9577\u5c3e\u6982\u5316\u80fd\u529b\uff0c\u8207\u5148\u524d\u7684\u57fa\u65bc\u795e\u7d93\u7db2\u8def\u7684\u5c08\u696d\u76f8\u95dc\u6027\u5b78\u7fd2\u65b9\u6cd5\u76f8\u6bd4\u3002\u5118\u7ba1\u6709\u524d\u666f\uff0c\u76ee\u524d\u7684\u57fa\u65bc LLM \u7684\u65b9\u6cd5\u5728\u5be6\u52d9\u4e0a\u4ecd\u9047\u5230\u4ee5\u4e0b\u4e0d\u8db3\uff1a\u9996\u5148\uff0c\u9f90\u5927\u7684\u53c3\u6578\u548c\u904b\u7b97\u9700\u6c42\u4f7f\u5f97\u7dda\u4e0a\u90e8\u7f72\u56f0\u96e3\u3002\u5176\u6b21\uff0c\u5c07 LLM \u6a21\u578b\u7cbe\u7c21\u70ba\u7dda\u4e0a\u6a21\u578b\u662f\u4e00\u500b\u53ef\u884c\u7684\u65b9\u5411\uff0c\u4f46 LLM \u76f8\u95dc\u6027\u5efa\u6a21\u662f\u4e00\u500b\u9ed1\u76d2\u5b50\uff0c\u5176\u8c50\u5bcc\u7684\u5167\u5728\u77e5\u8b58\u96e3\u4ee5\u63d0\u53d6\u4e26\u7dda\u4e0a\u61c9\u7528\u3002\u70ba\u4e86\u63d0\u5347 LLM \u7684\u53ef\u89e3\u91cb\u6027\uff0c\u4e26\u900f\u904e LLM \u63d0\u5347\u7dda\u4e0a\u76f8\u95dc\u6027\u6a21\u578b\u7684\u6548\u80fd\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u53ef\u89e3\u91cb LLM \u9a45\u52d5\u7684\u591a\u7dad\u5ea6\u77e5\u8b58\u8403\u53d6\u67b6\u69cb\uff0c\u7528\u65bc\u96fb\u5b50\u5546\u52d9\u76f8\u95dc\u6027\u5b78\u7fd2\uff0c\u5176\u4e2d\u5305\u542b\u5169\u500b\u6838\u5fc3\u7d44\u6210\u90e8\u5206\uff1a(1) \u4e00\u500b\u7528\u65bc\u76f8\u95dc\u6027\u5efa\u6a21\u7684\u53ef\u89e3\u91cb LLM (ELLM-rele)\uff0c\u5b83\u5c07\u76f8\u95dc\u6027\u5b78\u7fd2\u5206\u89e3\u6210\u4e2d\u9593\u6b65\u9a5f\uff0c\u4e26\u5c07\u76f8\u95dc\u6027\u5b78\u7fd2\u5efa\u6a21\u70ba\u4e00\u500b\u601d\u8003\u93c8 (CoT) \u63a8\u7406\uff0c\u5f9e\u800c\u63d0\u5347 LLM \u7684\u53ef\u89e3\u91cb\u6027\u548c\u6548\u80fd\u3002(2) \u4e00\u500b\u591a\u7dad\u5ea6\u77e5\u8b58\u8403\u53d6 (MKD) \u67b6\u69cb\uff0c\u5b83\u5c07 ELLM-rele \u7684\u77e5\u8b58\u8f49\u79fb\u5230\u76ee\u524d\u53ef\u90e8\u7f72\u7684\u57fa\u65bc\u4e92\u52d5\u548c\u57fa\u65bc\u8868\u793a\u7684\u5b78\u751f\u6a21\u578b\uff0c\u5f9e\u76f8\u95dc\u6027\u5206\u6578\u5206\u4f48\u548c CoT \u63a8\u7406\u9762\u5411\u9032\u884c\u8f49\u79fb\u3002\u900f\u904e\u8403\u53d6\u6a5f\u7387\u548c CoT \u63a8\u7406\u77e5\u8b58\uff0cMKD \u63d0\u5347\u4e86\u5b78\u751f\u6a21\u578b\u7684\u8a9e\u610f\u4e92\u52d5\u548c\u9577\u5c3e\u6982\u5316\u80fd\u529b\u3002\u5728\u6dd8\u5bf6\u641c\u5c0b\u5ee3\u544a\u5834\u666f\u4e2d\u9032\u884c\u7684\u5927\u91cf\u96e2\u7dda\u8a55\u4f30\u548c\u7dda\u4e0a\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u63d0\u51fa\u7684\u67b6\u69cb\u986f\u8457\u63d0\u5347\u4e86\u96fb\u5b50\u5546\u52d9\u76f8\u95dc\u6027\u5b78\u7fd2\u6548\u80fd\u548c\u4f7f\u7528\u8005\u9ad4\u9a57\u3002</paragraph>", "author": "Gang Zhao et.al.", "authors": "Gang Zhao, Ximing Zhang, Chenji Lu, Hui Zhao, Tianshu Wu, Pengjie Wang, Jian Xu, Bo Zheng", "id": "2411.13045v1", "paper_url": "http://arxiv.org/abs/2411.13045v1", "repo": "null"}}