{"2405.19315": {"publish_time": "2024-05-29", "title": "Matryoshka Query Transformer for Large Vision-Language Models", "paper_summary": "Large Vision-Language Models (LVLMs) typically encode an image into a fixed\nnumber of visual tokens (e.g., 576) and process these tokens with a language\nmodel. Despite their strong performance, LVLMs face challenges in adapting to\nvarying computational constraints. This raises the question: can we achieve\nflexibility in the number of visual tokens to suit different tasks and\ncomputational resources? We answer this with an emphatic yes. Inspired by\nMatryoshka Representation Learning, we introduce the Matryoshka Query\nTransformer (MQT), capable of encoding an image into m visual tokens during\ninference, where m can be any number up to a predefined maximum. This is\nachieved by employing a query transformer with M latent query tokens to\ncompress the visual embeddings. During each training step, we randomly select m\n<= M latent query tokens and train the model using only these first m tokens,\ndiscarding the rest. Combining MQT with LLaVA, we train a single model once,\nand flexibly and drastically reduce the number of inference-time visual tokens\nwhile maintaining similar or better performance compared to training\nindependent models for each number of tokens. Our model, MQT-LLAVA, matches\nLLaVA-1.5 performance across 11 benchmarks using a maximum of 256 tokens\ninstead of LLaVA's fixed 576. Reducing to 16 tokens (8x less TFLOPs) only\nsacrifices the performance by 2.4 points on MMBench. On certain tasks such as\nScienceQA and MMMU, we can even go down to only 2 visual tokens with\nperformance drops of just 3% and 6% each. Our exploration of the trade-off\nbetween the accuracy and computational cost brought about by the number of\nvisual tokens facilitates future research to achieve the best of both worlds.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u901a\u5e38\u5c07\u5f71\u50cf\u7de8\u78bc\u6210\u56fa\u5b9a\u6578\u91cf\u7684\u8996\u89ba\u4ee3\u5e63\uff08\u4f8b\u5982 576 \u500b\uff09\uff0c\u4e26\u4f7f\u7528\u8a9e\u8a00\u6a21\u578b\u8655\u7406\u9019\u4e9b\u4ee3\u5e63\u3002\u5118\u7ba1 LVLMs \u6548\u80fd\u5f37\u5927\uff0c\u4f46\u5b83\u5011\u5728\u9069\u61c9\u4e0d\u540c\u7684\u904b\u7b97\u9650\u5236\u6642\u6703\u9047\u5230\u6311\u6230\u3002\u9019\u5f15\u767c\u4e86\u4e00\u500b\u554f\u984c\uff1a\u6211\u5011\u80fd\u5426\u8abf\u6574\u8996\u89ba\u4ee3\u5e63\u6578\u91cf\u4ee5\u9069\u61c9\u4e0d\u540c\u7684\u4efb\u52d9\u548c\u904b\u7b97\u8cc7\u6e90\uff1f\u6211\u5011\u7684\u7b54\u6848\u662f\u80af\u5b9a\u7684\u3002\u53d7\u5230 Matryoshka \u8868\u5fb5\u5b78\u7fd2\u7684\u555f\u767c\uff0c\u6211\u5011\u5f15\u5165\u4e86 Matryoshka \u67e5\u8a62\u8f49\u63db\u5668 (MQT)\uff0c\u5b83\u80fd\u5920\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u5c07\u5f71\u50cf\u7de8\u78bc\u6210 m \u500b\u8996\u89ba\u4ee3\u5e63\uff0c\u5176\u4e2d m \u53ef\u4ee5\u662f\u4efb\u4f55\u5c0f\u65bc\u9810\u5b9a\u7fa9\u6700\u5927\u503c\u7684\u6578\u5b57\u3002\u9019\u662f\u900f\u904e\u4f7f\u7528\u5177\u6709 M \u500b\u6f5b\u5728\u67e5\u8a62\u4ee3\u5e63\u7684\u67e5\u8a62\u8f49\u63db\u5668\u4f86\u58d3\u7e2e\u8996\u89ba\u5d4c\u5165\u4f86\u5be6\u73fe\u7684\u3002\u5728\u6bcf\u500b\u8a13\u7df4\u6b65\u9a5f\u4e2d\uff0c\u6211\u5011\u6703\u96a8\u6a5f\u9078\u64c7 m <= M \u500b\u6f5b\u5728\u67e5\u8a62\u4ee3\u5e63\uff0c\u4e26\u50c5\u4f7f\u7528\u9019\u4e9b\u524d m \u500b\u4ee3\u5e63\u4f86\u8a13\u7df4\u6a21\u578b\uff0c\u6368\u68c4\u5176\u9918\u7684\u4ee3\u5e63\u3002\u5c07 MQT \u8207 LLaVA \u7d50\u5408\uff0c\u6211\u5011\u4e00\u6b21\u8a13\u7df4\u4e00\u500b\u6a21\u578b\uff0c\u4e26\u9748\u6d3b\u4e14\u5927\u5e45\u6e1b\u5c11\u63a8\u7406\u6642\u9593\u8996\u89ba\u4ee3\u5e63\u7684\u6578\u91cf\uff0c\u540c\u6642\u7dad\u6301\u8207\u70ba\u6bcf\u500b\u4ee3\u5e63\u6578\u91cf\u8a13\u7df4\u7368\u7acb\u6a21\u578b\u76f8\u7576\u6216\u66f4\u597d\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u6a21\u578b MQT-LLAVA \u4f7f\u7528\u6700\u591a 256 \u500b\u4ee3\u5e63\uff0c\u800c\u4e0d\u662f LLaVA \u7684\u56fa\u5b9a 576 \u500b\u4ee3\u5e63\uff0c\u5728 11 \u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u8207 LLaVA-1.5 \u7684\u6548\u80fd\u76f8\u7b26\u3002\u6e1b\u5c11\u5230 16 \u500b\u4ee3\u5e63\uff088 \u500d TFLOPs\uff09\u53ea\u6703\u5728 MMBench \u4e0a\u72a7\u7272 2.4 \u500b\u6548\u80fd\u9ede\u6578\u3002\u5728\u67d0\u4e9b\u4efb\u52d9\u4e0a\uff0c\u4f8b\u5982 ScienceQA \u548c MMMU\uff0c\u6211\u5011\u751a\u81f3\u53ef\u4ee5\u4f7f\u7528\u53ea\u6709 2 \u500b\u8996\u89ba\u4ee3\u5e63\uff0c\u6548\u80fd\u53ea\u4e0b\u964d 3% \u548c 6%\u3002\u6211\u5011\u63a2\u7d22\u8996\u89ba\u4ee3\u5e63\u6578\u91cf\u5e36\u4f86\u7684\u6e96\u78ba\u6027\u548c\u904b\u7b97\u6210\u672c\u4e4b\u9593\u7684\u6b0a\u8861\uff0c\u6709\u52a9\u65bc\u672a\u4f86\u7684\u7814\u7a76\u5be6\u73fe\u5169\u5168\u5176\u7f8e\u3002", "author": "Wenbo Hu et.al.", "authors": "Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, Kai-Wei Chang", "id": "2405.19315v1", "paper_url": "http://arxiv.org/abs/2405.19315v1", "repo": "https://github.com/gordonhu608/mqt-llava"}}