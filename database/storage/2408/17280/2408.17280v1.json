{"2408.17280": {"publish_time": "2024-08-30", "title": "Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts", "paper_summary": "We present a toolkit for creating low-cost Mixture-of-Domain-Experts (MOE)\nfrom trained models. The toolkit can be used for creating a mixture from models\nor from adapters. We perform extensive tests and offer guidance on defining the\narchitecture of the resulting MOE using the toolkit. A public repository is\navailable.", "paper_summary_zh": "\u6211\u5011\u63d0\u4f9b\u4e00\u500b\u5de5\u5177\u7d44\uff0c\u7528\u65bc\u5f9e\u8a13\u7df4\u597d\u7684\u6a21\u578b\u5efa\u7acb\u4f4e\u6210\u672c\u7684 Mixture-of-Domain-Experts (MOE)\u3002\u6b64\u5de5\u5177\u7d44\u53ef\u7528\u65bc\u5f9e\u6a21\u578b\u6216\u9069\u914d\u5668\u5efa\u7acb\u6df7\u5408\u9ad4\u3002\u6211\u5011\u57f7\u884c\u5ee3\u6cdb\u7684\u6e2c\u8a66\uff0c\u4e26\u63d0\u4f9b\u6709\u95dc\u4f7f\u7528\u5de5\u5177\u7d44\u5b9a\u7fa9\u7d50\u679c MOE \u67b6\u69cb\u7684\u6307\u5c0e\u3002\u5df2\u63d0\u4f9b\u516c\u958b\u5b58\u653e\u5eab\u3002", "author": "Rhui Dih Lee et.al.", "authors": "Rhui Dih Lee, Laura Wynter, Raghu Kiran Ganti", "id": "2408.17280v1", "paper_url": "http://arxiv.org/abs/2408.17280v1", "repo": "null"}}