{"2501.07502": {"publish_time": "2025-01-13", "title": "RbRL2.0: Integrated Reward and Policy Learning for Rating-based Reinforcement Learning", "paper_summary": "Reinforcement learning (RL), a common tool in decision making, learns\npolicies from various experiences based on the associated cumulative\nreturn/rewards without treating them differently. On the contrary, humans often\nlearn to distinguish from different levels of performance and extract the\nunderlying trends towards improving their decision making for best performance.\nMotivated by this, this paper proposes a novel RL method that mimics humans'\ndecision making process by differentiating among collected experiences for\neffective policy learning. The main idea is to extract important directional\ninformation from experiences with different performance levels, named ratings,\nso that policies can be updated towards desired deviation from these\nexperiences with different ratings. Specifically, we propose a new policy loss\nfunction that penalizes distribution similarities between the current policy\nand failed experiences with different ratings, and assign different weights to\nthe penalty terms based on the rating classes. Meanwhile, reward learning from\nthese rated samples can be integrated with the new policy loss towards an\nintegrated reward and policy learning from rated samples. Optimizing the\nintegrated reward and policy loss function will lead to the discovery of\ndirections for policy improvement towards maximizing cumulative rewards and\npenalizing most from the lowest performance level while least from the highest\nperformance level. To evaluate the effectiveness of the proposed method, we\npresent results for experiments on a few typical environments that show\nimproved convergence and overall performance over the existing rating-based\nreinforcement learning method with only reward learning.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2 (RL) \u662f\u6c7a\u7b56\u5236\u5b9a\u4e2d\u5e38\u898b\u7684\u5de5\u5177\uff0c\u5b83\u6839\u64da\u76f8\u95dc\u7684\u7d2f\u7a4d\u56de\u5831/\u734e\u52f5\u5f9e\u5404\u7a2e\u7d93\u9a57\u4e2d\u5b78\u7fd2\u7b56\u7565\uff0c\u800c\u4e0d\u6703\u5c0d\u5b83\u5011\u9032\u884c\u4e0d\u540c\u7684\u8655\u7406\u3002\u76f8\u53cd\uff0c\u4eba\u985e\u901a\u5e38\u5b78\u6703\u5340\u5206\u4e0d\u540c\u7684\u8868\u73fe\u5c64\u7d1a\uff0c\u4e26\u63d0\u53d6\u6539\u5584\u6c7a\u7b56\u5236\u5b9a\u4ee5\u7372\u5f97\u6700\u4f73\u8868\u73fe\u7684\u6f5b\u5728\u8da8\u52e2\u3002\u53d7\u6b64\u555f\u767c\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684 RL \u65b9\u6cd5\uff0c\u5b83\u6a21\u64ec\u4eba\u985e\u7684\u6c7a\u7b56\u5236\u5b9a\u904e\u7a0b\uff0c\u901a\u904e\u5340\u5206\u6536\u96c6\u7684\u7d93\u9a57\u4f86\u9032\u884c\u6709\u6548\u7684\u7b56\u7565\u5b78\u7fd2\u3002\u5176\u4e3b\u8981\u601d\u60f3\u662f\u5f9e\u5177\u6709\u4e0d\u540c\u8868\u73fe\u5c64\u7d1a\u7684\u7d93\u9a57\u4e2d\u63d0\u53d6\u91cd\u8981\u7684\u65b9\u5411\u6027\u8cc7\u8a0a\uff0c\u7a31\u70ba\u8a55\u5206\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u91dd\u5c0d\u9019\u4e9b\u5177\u6709\u4e0d\u540c\u8a55\u5206\u7684\u7d93\u9a57\u66f4\u65b0\u7b56\u7565\uff0c\u671d\u8457\u6240\u9700\u7684\u504f\u5dee\u65b9\u5411\u767c\u5c55\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u7b56\u7565\u640d\u5931\u51fd\u6578\uff0c\u5b83\u61f2\u7f70\u7576\u524d\u7b56\u7565\u8207\u5177\u6709\u4e0d\u540c\u8a55\u5206\u7684\u5931\u6557\u7d93\u9a57\u4e4b\u9593\u7684\u5206\u914d\u76f8\u4f3c\u6027\uff0c\u4e26\u6839\u64da\u8a55\u5206\u985e\u5225\u70ba\u61f2\u7f70\u9805\u5206\u914d\u4e0d\u540c\u7684\u6b0a\u91cd\u3002\u540c\u6642\uff0c\u5f9e\u9019\u4e9b\u8a55\u5206\u6a23\u672c\u4e2d\u5b78\u7fd2\u734e\u52f5\u53ef\u4ee5\u8207\u65b0\u7684\u7b56\u7565\u640d\u5931\u6574\u5408\uff0c\u5f9e\u800c\u5f9e\u8a55\u5206\u6a23\u672c\u4e2d\u7372\u5f97\u6574\u5408\u7684\u734e\u52f5\u548c\u7b56\u7565\u5b78\u7fd2\u3002\u6700\u4f73\u5316\u6574\u5408\u7684\u734e\u52f5\u548c\u7b56\u7565\u640d\u5931\u51fd\u6578\u5c07\u5c0e\u81f4\u767c\u73fe\u7b56\u7565\u6539\u9032\u7684\u65b9\u5411\uff0c\u671d\u8457\u6700\u5927\u5316\u7d2f\u7a4d\u734e\u52f5\u548c\u5c0d\u6700\u4f4e\u8868\u73fe\u5c64\u7d1a\u9032\u884c\u6700\u56b4\u53b2\u7684\u61f2\u7f70\uff0c\u800c\u5c0d\u6700\u9ad8\u8868\u73fe\u5c64\u7d1a\u9032\u884c\u6700\u8f15\u5fae\u7684\u61f2\u7f70\u3002\u70ba\u4e86\u8a55\u4f30\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5728\u5e7e\u500b\u5178\u578b\u74b0\u5883\u4e2d\u9032\u884c\u7684\u5be6\u9a57\u7d50\u679c\uff0c\u9019\u4e9b\u7d50\u679c\u986f\u793a\u51fa\u6bd4\u50c5\u4f7f\u7528\u734e\u52f5\u5b78\u7fd2\u7684\u73fe\u6709\u57fa\u65bc\u8a55\u5206\u7684\u5f37\u5316\u5b78\u7fd2\u65b9\u6cd5\u6709\u66f4\u597d\u7684\u6536\u6582\u6027\u548c\u6574\u9ad4\u8868\u73fe\u3002", "author": "Mingkang Wu et.al.", "authors": "Mingkang Wu, Devin White, Vernon Lawhern, Nicholas R. Waytowich, Yongcan Cao", "id": "2501.07502v1", "paper_url": "http://arxiv.org/abs/2501.07502v1", "repo": "null"}}