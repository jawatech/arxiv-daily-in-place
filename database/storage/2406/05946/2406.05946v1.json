{"2406.05946": {"publish_time": "2024-06-10", "title": "Safety Alignment Should Be Made More Than Just a Few Tokens Deep", "paper_summary": "The safety alignment of current Large Language Models (LLMs) is vulnerable.\nRelatively simple attacks, or even benign fine-tuning, can jailbreak aligned\nmodels. We argue that many of these vulnerabilities are related to a shared\nunderlying issue: safety alignment can take shortcuts, wherein the alignment\nadapts a model's generative distribution primarily over only its very first few\noutput tokens. We refer to this issue as shallow safety alignment. In this\npaper, we present case studies to explain why shallow safety alignment can\nexist and provide evidence that current aligned LLMs are subject to this issue.\nWe also show how these findings help explain multiple recently discovered\nvulnerabilities in LLMs, including the susceptibility to adversarial suffix\nattacks, prefilling attacks, decoding parameter attacks, and fine-tuning\nattacks. Importantly, we discuss how this consolidated notion of shallow safety\nalignment sheds light on promising research directions for mitigating these\nvulnerabilities. For instance, we show that deepening the safety alignment\nbeyond just the first few tokens can often meaningfully improve robustness\nagainst some common exploits. Finally, we design a regularized finetuning\nobjective that makes the safety alignment more persistent against fine-tuning\nattacks by constraining updates on initial tokens. Overall, we advocate that\nfuture safety alignment should be made more than just a few tokens deep.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5b89\u5168\u8abf\u6574\u5bb9\u6613\u53d7\u5230\u653b\u64ca\u3002\n\u76f8\u5c0d\u7c21\u55ae\u7684\u653b\u64ca\uff0c\u751a\u81f3\u662f\u826f\u6027\u7684\u5fae\u8abf\uff0c\u90fd\u53ef\u4ee5\u7834\u89e3\u8abf\u6574\u904e\u7684\u6a21\u578b\u3002\u6211\u5011\u8a8d\u70ba\uff0c\u5176\u4e2d\u8a31\u591a\u6f0f\u6d1e\u90fd\u8207\u4e00\u500b\u5171\u540c\u7684\u6839\u672c\u554f\u984c\u6709\u95dc\uff1a\u5b89\u5168\u8abf\u6574\u53ef\u4ee5\u8d70\u6377\u5f91\uff0c\u5176\u4e2d\u8abf\u6574\u4e3b\u8981\u53ea\u91dd\u5c0d\u6a21\u578b\u751f\u6210\u5206\u5e03\u7684\u524d\u5e7e\u500b\u8f38\u51fa\u4ee3\u78bc\u3002\u6211\u5011\u5c07\u6b64\u554f\u984c\u7a31\u70ba\u6dfa\u5c64\u5b89\u5168\u8abf\u6574\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u6848\u4f8b\u7814\u7a76\u4f86\u89e3\u91cb\u70ba\u4ec0\u9ebc\u6703\u51fa\u73fe\u6dfa\u5c64\u5b89\u5168\u8abf\u6574\uff0c\u4e26\u63d0\u4f9b\u8b49\u64da\u8b49\u660e\u7576\u524d\u8abf\u6574\u904e\u7684 LLM \u6703\u53d7\u5230\u6b64\u554f\u984c\u5f71\u97ff\u3002\u6211\u5011\u9084\u5c55\u793a\u4e86\u9019\u4e9b\u767c\u73fe\u5982\u4f55\u5e6b\u52a9\u89e3\u91cb\u6700\u8fd1\u5728 LLM \u4e2d\u767c\u73fe\u7684\u591a\u500b\u6f0f\u6d1e\uff0c\u5305\u62ec\u5c0d\u6297\u6027\u5f8c\u7db4\u653b\u64ca\u3001\u9810\u586b\u5145\u653b\u64ca\u3001\u89e3\u78bc\u53c3\u6578\u653b\u64ca\u548c\u5fae\u8abf\u653b\u64ca\u7684\u654f\u611f\u6027\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86\u6dfa\u5c64\u5b89\u5168\u8abf\u6574\u7684\u9019\u500b\u7d71\u4e00\u6982\u5ff5\u5982\u4f55\u70ba\u6e1b\u8f15\u9019\u4e9b\u6f0f\u6d1e\u63d0\u4f9b\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\u3002\u4f8b\u5982\uff0c\u6211\u5011\u8868\u660e\uff0c\u5c07\u5b89\u5168\u8abf\u6574\u64f4\u5c55\u5230\u524d\u5e7e\u500b\u4ee3\u78bc\u4e4b\u5916\u901a\u5e38\u53ef\u4ee5\u986f\u8457\u63d0\u9ad8\u5c0d\u67d0\u4e9b\u5e38\u898b\u6f0f\u6d1e\u7684\u9b6f\u68d2\u6027\u3002\u6700\u5f8c\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u6b63\u5247\u5316\u7684\u5fae\u8abf\u76ee\u6a19\uff0c\u901a\u904e\u9650\u5236\u521d\u59cb\u4ee3\u78bc\u7684\u66f4\u65b0\uff0c\u4f7f\u5b89\u5168\u8abf\u6574\u5c0d\u5fae\u8abf\u653b\u64ca\u66f4\u5177\u6301\u7e8c\u6027\u3002\u7e3d\u7684\u4f86\u8aaa\uff0c\u6211\u5011\u63d0\u5021\u672a\u4f86\u7684\u5b89\u5168\u8abf\u6574\u61c9\u8a72\u4e0d\u6b62\u5e7e\u500b\u4ee3\u78bc\u6df1\u5ea6\u3002", "author": "Xiangyu Qi et.al.", "authors": "Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, Peter Henderson", "id": "2406.05946v1", "paper_url": "http://arxiv.org/abs/2406.05946v1", "repo": "https://github.com/unispac/shallow-vs-deep-alignment"}}