{"2412.19512": {"publish_time": "2024-12-27", "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging", "paper_summary": "Fine-tuning large language models (LLMs) for downstream tasks is a widely\nadopted approach, but it often leads to safety degradation in safety-aligned\nLLMs. Currently, many solutions address this issue by incorporating additional\nsafety data, which can be impractical in many cases. In this paper, we address\nthe question: How can we improve downstream task performance while preserving\nsafety in LLMs without relying on additional safety data? We propose a simple\nand effective method that maintains the inherent safety of LLMs while enhancing\ntheir downstream task performance: merging the weights of pre- and\npost-fine-tuned safety-aligned models. Experimental results across various\ndownstream tasks, models, and merging methods demonstrate that this approach\neffectively mitigates safety degradation while improving downstream task\nperformance, offering a practical solution for adapting safety-aligned LLMs.", "paper_summary_zh": "\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u57f7\u884c\u4e0b\u6e38\u4efb\u52d9\u662f\u4e00\u7a2e\u5ee3\u6cdb\u63a1\u7528\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u901a\u5e38\u6703\u5c0e\u81f4\u8207\u5b89\u5168\u5c0d\u9f4a\u7684 LLM \u4e2d\u7684\u5b89\u5168\u964d\u7d1a\u3002\u76ee\u524d\uff0c\u8a31\u591a\u89e3\u6c7a\u65b9\u6848\u900f\u904e\u7d0d\u5165\u984d\u5916\u7684\u5b89\u5168\u8cc7\u6599\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u4f46\u5728\u8a31\u591a\u60c5\u6cc1\u4e0b\u9019\u4e26\u4e0d\u53ef\u884c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4ee5\u4e0b\u554f\u984c\uff1a\u5982\u4f55\u5728\u4e0d\u4f9d\u8cf4\u984d\u5916\u5b89\u5168\u8cc7\u6599\u7684\u60c5\u6cc1\u4e0b\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u540c\u6642\u7dad\u6301 LLM \u7684\u5b89\u5168\u6027\uff1f\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5728\u63d0\u5347 LLM \u7684\u4e0b\u6e38\u4efb\u52d9\u6548\u80fd\u7684\u540c\u6642\uff0c\u7dad\u6301\u5176\u5167\u5728\u5b89\u5168\u6027\uff1a\u5408\u4f75\u5fae\u8abf\u524d\u548c\u5fae\u8abf\u5f8c\u8207\u5b89\u5168\u5c0d\u9f4a\u7684\u6a21\u578b\u7684\u6b0a\u91cd\u3002\u91dd\u5c0d\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u3001\u6a21\u578b\u548c\u5408\u4f75\u65b9\u6cd5\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u6b64\u65b9\u6cd5\u6709\u6548\u6e1b\u8f15\u5b89\u5168\u964d\u7d1a\uff0c\u540c\u6642\u63d0\u5347\u4e0b\u6e38\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u5be6\u7528\u7684\u89e3\u6c7a\u65b9\u6848\u4f86\u8abf\u6574\u8207\u5b89\u5168\u5c0d\u9f4a\u7684 LLM\u3002", "author": "Hua Farn et.al.", "authors": "Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee", "id": "2412.19512v1", "paper_url": "http://arxiv.org/abs/2412.19512v1", "repo": "null"}}