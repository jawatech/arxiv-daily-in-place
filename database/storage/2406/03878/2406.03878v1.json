{"2406.03878": {"publish_time": "2024-06-06", "title": "Decoder-only Streaming Transformer for Simultaneous Translation", "paper_summary": "Simultaneous Machine Translation (SiMT) generates translation while reading\nsource tokens, essentially producing the target prefix based on the source\nprefix. To achieve good performance, it leverages the relationship between\nsource and target prefixes to exact a policy to guide the generation of\ntranslations. Although existing SiMT methods primarily focus on the\nEncoder-Decoder architecture, we explore the potential of Decoder-only\narchitecture, owing to its superior performance in various tasks and its\ninherent compatibility with SiMT. However, directly applying the Decoder-only\narchitecture to SiMT poses challenges in terms of training and inference. To\nalleviate the above problems, we propose the first Decoder-only SiMT model,\nnamed Decoder-only Streaming Transformer (DST). Specifically, DST separately\nencodes the positions of the source and target prefixes, ensuring that the\nposition of the target prefix remains unaffected by the expansion of the source\nprefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism\ntailored for the Decoder-only architecture. It is capable of obtaining\ntranslation policy by assessing the sufficiency of input source information and\nintegrating with the soft-attention mechanism to generate translations.\nExperiments demonstrate that our approach achieves state-of-the-art performance\non three translation tasks.", "paper_summary_zh": "\u540c\u6642\u9593\u6a5f\u5668\u7ffb\u8b6f (SiMT) \u5728\u8b80\u53d6\u539f\u59cb\u8a9e\u8a00\u7b26\u865f\u6642\u7522\u751f\u7ffb\u8b6f\uff0c\u57fa\u672c\u4e0a\u6839\u64da\u539f\u59cb\u8a9e\u8a00\u524d\u7db4\u7522\u751f\u76ee\u6a19\u8a9e\u8a00\u524d\u7db4\u3002\u70ba\u4e86\u9054\u5230\u826f\u597d\u7684\u6548\u80fd\uff0c\u5b83\u5229\u7528\u539f\u59cb\u8a9e\u8a00\u548c\u76ee\u6a19\u8a9e\u8a00\u524d\u7db4\u4e4b\u9593\u7684\u95dc\u4fc2\u4f86\u5236\u5b9a\u4e00\u500b\u6e96\u5247\uff0c\u4ee5\u5f15\u5c0e\u7ffb\u8b6f\u7684\u7522\u751f\u3002\u5118\u7ba1\u73fe\u6709\u7684 SiMT \u65b9\u6cd5\u4e3b\u8981\u95dc\u6ce8\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u67b6\u69cb\uff0c\u4f46\u6211\u5011\u63a2\u7d22\u4e86\u50c5\u89e3\u78bc\u5668\u67b6\u69cb\u7684\u6f5b\u529b\uff0c\u56e0\u70ba\u5b83\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5177\u6709\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u4e26\u4e14\u8207 SiMT \u5177\u6709\u5167\u5728\u76f8\u5bb9\u6027\u3002\u7136\u800c\uff0c\u5c07\u50c5\u89e3\u78bc\u5668\u67b6\u69cb\u76f4\u63a5\u61c9\u7528\u65bc SiMT \u5728\u8a13\u7df4\u548c\u63a8\u8ad6\u65b9\u9762\u6703\u5e36\u4f86\u6311\u6230\u3002\u70ba\u4e86\u7de9\u89e3\u4e0a\u8ff0\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7b2c\u4e00\u500b\u50c5\u89e3\u78bc\u5668 SiMT \u6a21\u578b\uff0c\u7a31\u70ba\u50c5\u89e3\u78bc\u5668\u4e32\u6d41\u8f49\u63db\u5668 (DST)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cDST \u5206\u5225\u5c0d\u539f\u59cb\u8a9e\u8a00\u548c\u76ee\u6a19\u8a9e\u8a00\u524d\u7db4\u7684\u4f4d\u7f6e\u9032\u884c\u7de8\u78bc\uff0c\u78ba\u4fdd\u76ee\u6a19\u8a9e\u8a00\u524d\u7db4\u7684\u4f4d\u7f6e\u4e0d\u53d7\u539f\u59cb\u8a9e\u8a00\u524d\u7db4\u7684\u64f4\u5145\u5f71\u97ff\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5c08\u70ba\u50c5\u89e3\u78bc\u5668\u67b6\u69cb\u91cf\u8eab\u6253\u9020\u7684\u4e32\u6d41\u81ea\u6211\u6ce8\u610f\u529b (SSA) \u6a5f\u5236\u3002\u5b83\u80fd\u5920\u900f\u904e\u8a55\u4f30\u8f38\u5165\u539f\u59cb\u8a9e\u8a00\u8cc7\u8a0a\u7684\u5145\u5206\u6027\u4e26\u8207\u8edf\u6ce8\u610f\u529b\u6a5f\u5236\u6574\u5408\u4f86\u7522\u751f\u7ffb\u8b6f\uff0c\u5f9e\u800c\u7372\u5f97\u7ffb\u8b6f\u6e96\u5247\u3002\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5728\u4e09\u9805\u7ffb\u8b6f\u4efb\u52d9\u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002", "author": "Shoutao Guo et.al.", "authors": "Shoutao Guo, Shaolei Zhang, Yang Feng", "id": "2406.03878v1", "paper_url": "http://arxiv.org/abs/2406.03878v1", "repo": "null"}}