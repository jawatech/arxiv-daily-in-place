{"2411.16646": {"publish_time": "2024-11-25", "title": "Self-Generated Critiques Boost Reward Modeling for Language Models", "paper_summary": "Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.", "paper_summary_zh": "\u734e\u52f5\u5efa\u6a21\u5c0d\u65bc\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u81f3\u95dc\u91cd\u8981\uff0c\u7279\u5225\u662f\u5728\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF) \u4e2d\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u734e\u52f5\u6a21\u578b\u4e3b\u8981\u7522\u751f\u6a19\u91cf\u5206\u6578\uff0c\u4e26\u4e14\u96e3\u4ee5\u4ee5\u81ea\u7136\u8a9e\u8a00\u683c\u5f0f\u7d0d\u5165\u6279\u8a55\u3002\u6211\u5011\u5047\u8a2d\u9810\u6e2c\u6279\u8a55\u548c\u6a19\u91cf\u734e\u52f5\u90fd\u6703\u63d0\u9ad8\u734e\u52f5\u5efa\u6a21\u80fd\u529b\u3002\u57fa\u65bc\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Critic-RM\uff0c\u9019\u662f\u4e00\u500b\u5229\u7528\u81ea\u6211\u751f\u6210\u7684\u6279\u8a55\u4f86\u6539\u9032\u734e\u52f5\u6a21\u578b\u7684\u6846\u67b6\uff0c\u800c\u7121\u9700\u984d\u5916\u7684\u76e3\u7763\u3002Critic-RM \u63a1\u7528\u5169\u968e\u6bb5\u6d41\u7a0b\uff1a\u751f\u6210\u548c\u904e\u6ffe\u9ad8\u54c1\u8cea\u7684\u6279\u8a55\uff0c\u7136\u5f8c\u5728\u734e\u52f5\u9810\u6e2c\u548c\u6279\u8a55\u751f\u6210\u4e0a\u9032\u884c\u806f\u5408\u5fae\u8abf\u3002\u57fa\u6e96\u6e2c\u8a66\u7684\u5be6\u9a57\u8868\u660e\uff0c\u8207\u6a19\u6e96\u734e\u52f5\u6a21\u578b\u548c LLM \u8a55\u5be9\u76f8\u6bd4\uff0cCritic-RM \u5c07\u734e\u52f5\u5efa\u6a21\u6e96\u78ba\u5ea6\u63d0\u9ad8\u4e86 3.7%-7.3%\uff0c\u5c55\u793a\u4e86\u5f37\u5927\u7684\u6027\u80fd\u548c\u6578\u64da\u6548\u7387\u3002\u9032\u4e00\u6b65\u7684\u7814\u7a76\u9032\u4e00\u6b65\u9a57\u8b49\u4e86\u751f\u6210\u7684\u6279\u8a55\u5728\u7cfe\u6b63\u6709\u7f3a\u9677\u7684\u63a8\u7406\u6b65\u9a5f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63a8\u7406\u6e96\u78ba\u5ea6\u63d0\u9ad8\u4e86 2.5%-3.2%\u3002", "author": "Yue Yu et.al.", "authors": "Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou", "id": "2411.16646v1", "paper_url": "http://arxiv.org/abs/2411.16646v1", "repo": "null"}}