{"2502.01533": {"publish_time": "2025-02-03", "title": "Transformers trained on proteins can learn to attend to Euclidean distance", "paper_summary": "While conventional Transformers generally operate on sequence data, they can\nbe used in conjunction with structure models, typically SE(3)-invariant or\nequivariant graph neural networks (GNNs), for 3D applications such as protein\nstructure modelling. These hybrids typically involve either (1)\npreprocessing/tokenizing structural features as input for Transformers or (2)\ntaking Transformer embeddings and processing them within a structural\nrepresentation. However, there is evidence that Transformers can learn to\nprocess structural information on their own, such as the AlphaFold3 structural\ndiffusion model. In this work we show that Transformers can function\nindependently as structure models when passed linear embeddings of coordinates.\nWe first provide a theoretical explanation for how Transformers can learn to\nfilter attention as a 3D Gaussian with learned variance. We then validate this\ntheory using both simulated 3D points and in the context of masked token\nprediction for proteins. Finally, we show that pre-training protein Transformer\nencoders with structure improves performance on a downstream task, yielding\nbetter performance than custom structural models. Together, this work provides\na basis for using standard Transformers as hybrid structure-language models.", "paper_summary_zh": "\u96d6\u7136\u50b3\u7d71\u7684 Transformer \u901a\u5e38\u8655\u7406\u5e8f\u5217\u8cc7\u6599\uff0c\u4f46\u5b83\u5011\u53ef\u7528\u65bc\u7d50\u69cb\u6a21\u578b\uff0c\u901a\u5e38\u662f SE(3) \u4e0d\u8b8a\u5f0f\u6216\u7b49\u8b8a\u5f0f\u5716\u795e\u7d93\u7db2\u8def (GNN)\uff0c\u7528\u65bc\u86cb\u767d\u8cea\u7d50\u69cb\u5efa\u6a21\u7b49 3D \u61c9\u7528\u3002\u9019\u4e9b\u6df7\u5408\u6a21\u578b\u901a\u5e38\u5305\u542b (1) \u5c07\u7d50\u69cb\u7279\u5fb5\u9810\u8655\u7406/\u6a19\u8a18\u5316\u70ba Transformer \u7684\u8f38\u5165\u6216 (2) \u53d6\u7528 Transformer \u5d4c\u5165\u4e26\u5728\u7d50\u69cb\u8868\u793a\u4e2d\u8655\u7406\u5b83\u5011\u3002\u7136\u800c\uff0c\u6709\u8b49\u64da\u8868\u660e Transformer \u53ef\u4ee5\u81ea\u884c\u5b78\u7fd2\u8655\u7406\u7d50\u69cb\u8cc7\u8a0a\uff0c\u4f8b\u5982 AlphaFold3 \u7d50\u69cb\u64f4\u6563\u6a21\u578b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 Transformer \u5728\u50b3\u905e\u5ea7\u6a19\u7684\u7dda\u6027\u5d4c\u5165\u6642\uff0c\u53ef\u4ee5\u7368\u7acb\u4f5c\u70ba\u7d50\u69cb\u6a21\u578b\u904b\u4f5c\u3002\u6211\u5011\u9996\u5148\u63d0\u4f9b\u4e86 Transformer \u5982\u4f55\u5b78\u7fd2\u5c07\u6ce8\u610f\u529b\u6ffe\u6ce2\u70ba\u5177\u6709\u5b78\u7fd2\u8b8a\u7570\u7684 3D \u9ad8\u65af\u7684\u7406\u8ad6\u89e3\u91cb\u3002\u7136\u5f8c\u6211\u5011\u4f7f\u7528\u6a21\u64ec 3D \u9ede\u548c\u5728\u86cb\u767d\u8cea\u906e\u7f69\u6a19\u8a18\u9810\u6e2c\u7684\u80cc\u666f\u4e0b\u9a57\u8b49\u6b64\u7406\u8ad6\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86\u4f7f\u7528\u7d50\u69cb\u9810\u8a13\u7df4\u86cb\u767d\u8cea Transformer \u7de8\u78bc\u5668\u6703\u6539\u5584\u4e0b\u6e38\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u7522\u751f\u6bd4\u81ea\u8a02\u7d50\u69cb\u6a21\u578b\u66f4\u597d\u7684\u6548\u80fd\u3002\u7d9c\u5408\u4f86\u8aaa\uff0c\u9019\u9805\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4f7f\u7528\u6a19\u6e96 Transformer \u4f5c\u70ba\u6df7\u5408\u7d50\u69cb\u8a9e\u8a00\u6a21\u578b\u7684\u57fa\u790e\u3002", "author": "Isaac Ellmen et.al.", "authors": "Isaac Ellmen, Constantin Schneider, Matthew I. J. Raybould, Charlotte M. Deane", "id": "2502.01533v1", "paper_url": "http://arxiv.org/abs/2502.01533v1", "repo": "null"}}