{"2410.23261": {"publish_time": "2024-10-30", "title": "$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources", "paper_summary": "Pre-training is notoriously compute-intensive and academic researchers are\nnotoriously under-resourced. It is, therefore, commonly assumed that academics\ncan't pre-train models. In this paper, we seek to clarify this assumption. We\nfirst survey academic researchers to learn about their available compute and\nthen empirically measure the time to replicate models on such resources. We\nintroduce a benchmark to measure the time to pre-train models on given GPUs and\nalso identify ideal settings for maximizing training speed. We run our\nbenchmark on a range of models and academic GPUs, spending 2,000 GPU-hours on\nour experiments. Our results reveal a brighter picture for academic\npre-training: for example, although Pythia-1B was originally trained on 64 GPUs\nfor 3 days, we find it is also possible to replicate this model (with the same\nhyper-parameters) in 3x fewer GPU-days: i.e. on 4 GPUs in 18 days. We conclude\nwith a cost-benefit analysis to help clarify the trade-offs between price and\npre-training time. We believe our benchmark will help academic researchers\nconduct experiments that require training larger models on more data. We fully\nrelease our codebase at: https://github.com/apoorvkh/academic-pretraining.", "paper_summary_zh": "\u9810\u8a13\u7df4\u51fa\u4e86\u540d\u7684\u9700\u8981\u5927\u91cf\u904b\u7b97\uff0c\u800c\u5b78\u8853\u7814\u7a76\u4eba\u54e1\u51fa\u4e86\u540d\u7684\u8cc7\u6e90\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u666e\u904d\u8a8d\u70ba\u5b78\u8853\u754c\u7121\u6cd5\u9810\u8a13\u7df4\u6a21\u578b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8a66\u5716\u91d0\u6e05\u9019\u500b\u5047\u8a2d\u3002\u6211\u5011\u9996\u5148\u8abf\u67e5\u5b78\u8853\u7814\u7a76\u4eba\u54e1\uff0c\u4ee5\u4e86\u89e3\u4ed6\u5011\u53ef\u7528\u7684\u904b\u7b97\uff0c\u7136\u5f8c\u6839\u64da\u9019\u4e9b\u8cc7\u6e90\u5be6\u8b49\u6e2c\u91cf\u8907\u88fd\u6a21\u578b\u7684\u6642\u9593\u3002\u6211\u5011\u5f15\u5165\u4e00\u500b\u57fa\u6e96\uff0c\u7528\u4f86\u6e2c\u91cf\u5728\u7d66\u5b9a\u7684 GPU \u4e0a\u9810\u8a13\u7df4\u6a21\u578b\u7684\u6642\u9593\uff0c\u4e26\u627e\u51fa\u6700\u5927\u5316\u8a13\u7df4\u901f\u5ea6\u7684\u7406\u60f3\u8a2d\u5b9a\u3002\u6211\u5011\u5728\u5404\u7a2e\u6a21\u578b\u548c\u5b78\u8853 GPU \u4e0a\u57f7\u884c\u57fa\u6e96\uff0c\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\u8017\u8cbb\u4e86 2,000 \u500b GPU \u5c0f\u6642\u3002\u6211\u5011\u7684\u7d50\u679c\u5c0d\u5b78\u8853\u9810\u8a13\u7df4\u63ed\u793a\u4e86\u4e00\u500b\u66f4\u5149\u660e\u7684\u5716\u50cf\uff1a\u4f8b\u5982\uff0c\u5118\u7ba1 Pythia-1B \u6700\u521d\u5728 64 \u500b GPU \u4e0a\u8a13\u7df4\u4e86 3 \u5929\uff0c\u4f46\u6211\u5011\u767c\u73fe\u4e5f\u53ef\u4ee5\u7528\u66f4\u5c11\u7684 3 \u500d GPU \u5929\u6578\u8907\u88fd\u9019\u500b\u6a21\u578b\uff08\u4f7f\u7528\u76f8\u540c\u7684\u8d85\u53c3\u6578\uff09\uff1a\u5373\u5728 4 \u500b GPU \u4e0a\u8a13\u7df4 18 \u5929\u3002\u6211\u5011\u4ee5\u6210\u672c\u6548\u76ca\u5206\u6790\u4f5c\u70ba\u7d50\u8ad6\uff0c\u4ee5\u5e6b\u52a9\u91d0\u6e05\u50f9\u683c\u548c\u9810\u8a13\u7df4\u6642\u9593\u4e4b\u9593\u7684\u6b0a\u8861\u3002\u6211\u5011\u76f8\u4fe1\u6211\u5011\u7684\u57fa\u6e96\u5c07\u6709\u52a9\u65bc\u5b78\u8853\u7814\u7a76\u4eba\u54e1\u9032\u884c\u9700\u8981\u5728\u66f4\u591a\u8cc7\u6599\u4e0a\u8a13\u7df4\u66f4\u5927\u6a21\u578b\u7684\u5be6\u9a57\u3002\u6211\u5011\u5728 https://github.com/apoorvkh/academic-pretraining \u5b8c\u6574\u91cb\u51fa\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5eab\u3002", "author": "Apoorv Khandelwal et.al.", "authors": "Apoorv Khandelwal, Tian Yun, Nihal V. Nayak, Jack Merullo, Stephen H. Bach, Chen Sun, Ellie Pavlick", "id": "2410.23261v1", "paper_url": "http://arxiv.org/abs/2410.23261v1", "repo": "https://github.com/apoorvkh/academic-pretraining"}}