{"2408.16426": {"publish_time": "2024-08-29", "title": "COIN: Control-Inpainting Diffusion Prior for Human and Camera Motion Estimation", "paper_summary": "Estimating global human motion from moving cameras is challenging due to the\nentanglement of human and camera motions. To mitigate the ambiguity, existing\nmethods leverage learned human motion priors, which however often result in\noversmoothed motions with misaligned 2D projections. To tackle this problem, we\npropose COIN, a control-inpainting motion diffusion prior that enables\nfine-grained control to disentangle human and camera motions. Although\npre-trained motion diffusion models encode rich motion priors, we find it\nnon-trivial to leverage such knowledge to guide global motion estimation from\nRGB videos. COIN introduces a novel control-inpainting score distillation\nsampling method to ensure well-aligned, consistent, and high-quality motion\nfrom the diffusion prior within a joint optimization framework. Furthermore, we\nintroduce a new human-scene relation loss to alleviate the scale ambiguity by\nenforcing consistency among the humans, camera, and scene. Experiments on three\nchallenging benchmarks demonstrate the effectiveness of COIN, which outperforms\nthe state-of-the-art methods in terms of global human motion estimation and\ncamera motion estimation. As an illustrative example, COIN outperforms the\nstate-of-the-art method by 33% in world joint position error (W-MPJPE) on the\nRICH dataset.", "paper_summary_zh": "\u7531\u65bc\u4eba\u985e\u548c\u76f8\u6a5f\u52d5\u4f5c\u7684\u7cfe\u7e8f\uff0c\u5f9e\u79fb\u52d5\u7684\u76f8\u6a5f\u4f30\u8a08\u5168\u7403\u4eba\u985e\u904b\u52d5\u5177\u6709\u6311\u6230\u6027\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u7a2e\u6a21\u7cca\u6027\uff0c\u73fe\u6709\u65b9\u6cd5\u5229\u7528\u5b78\u7fd2\u7684\u4eba\u985e\u904b\u52d5\u5148\u9a57\uff0c\u4f46\u9019\u901a\u5e38\u6703\u5c0e\u81f4 2D \u6295\u5f71\u932f\u4f4d\u7684\u904e\u5ea6\u5e73\u6ed1\u904b\u52d5\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 COIN\uff0c\u9019\u662f\u4e00\u7a2e\u63a7\u5236\u5167\u63d2\u904b\u52d5\u64f4\u6563\u5148\u9a57\uff0c\u53ef\u4ee5\u9032\u884c\u7d30\u7c92\u5ea6\u63a7\u5236\u4ee5\u89e3\u958b\u4eba\u985e\u548c\u76f8\u6a5f\u904b\u52d5\u3002\u5118\u7ba1\u9810\u5148\u8a13\u7df4\u7684\u904b\u52d5\u64f4\u6563\u6a21\u578b\u7de8\u78bc\u4e86\u8c50\u5bcc\u7684\u904b\u52d5\u5148\u9a57\uff0c\u4f46\u6211\u5011\u767c\u73fe\u5229\u7528\u9019\u7a2e\u77e5\u8b58\u4f86\u6307\u5c0e\u5f9e RGB \u5f71\u7247\u4e2d\u4f30\u8a08\u5168\u5c40\u904b\u52d5\u4e26\u975e\u6613\u4e8b\u3002COIN \u5c0e\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u63a7\u5236\u5167\u63d2\u5206\u6578\u84b8\u993e\u63a1\u6a23\u65b9\u6cd5\uff0c\u4ee5\u78ba\u4fdd\u5728\u806f\u5408\u512a\u5316\u6846\u67b6\u5167\u5f9e\u64f4\u6563\u5148\u9a57\u4e2d\u7372\u5f97\u5c0d\u9f4a\u826f\u597d\u3001\u4e00\u81f4\u4e14\u9ad8\u54c1\u8cea\u7684\u904b\u52d5\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u4eba\u985e\u5834\u666f\u95dc\u4fc2\u640d\u5931\uff0c\u4ee5\u901a\u904e\u5f37\u5236\u4eba\u985e\u3001\u76f8\u6a5f\u548c\u5834\u666f\u4e4b\u9593\u7684\u4e00\u81f4\u6027\u4f86\u7de9\u89e3\u898f\u6a21\u6a21\u7cca\u6027\u3002\u5728\u4e09\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u57fa\u6e96\u6e2c\u8a66\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\u4e86 COIN \u7684\u6709\u6548\u6027\uff0c\u5b83\u5728\u5168\u5c40\u4eba\u985e\u904b\u52d5\u4f30\u8a08\u548c\u76f8\u6a5f\u904b\u52d5\u4f30\u8a08\u65b9\u9762\u512a\u65bc\u6700\u5148\u9032\u7684\u65b9\u6cd5\u3002\u4f5c\u70ba\u4e00\u500b\u8aaa\u660e\u6027\u7bc4\u4f8b\uff0cCOIN \u5728 RICH \u8cc7\u6599\u96c6\u4e0a\u7684\u4e16\u754c\u95dc\u7bc0\u4f4d\u7f6e\u8aa4\u5dee (W-MPJPE) \u65b9\u9762\u6bd4\u6700\u5148\u9032\u7684\u65b9\u6cd5\u9ad8\u51fa 33%\u3002", "author": "Jiefeng Li et.al.", "authors": "Jiefeng Li, Ye Yuan, Davis Rempe, Haotian Zhang, Pavlo Molchanov, Cewu Lu, Jan Kautz, Umar Iqbal", "id": "2408.16426v1", "paper_url": "http://arxiv.org/abs/2408.16426v1", "repo": "null"}}