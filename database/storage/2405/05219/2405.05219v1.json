{"2405.05219": {"publish_time": "2024-05-08", "title": "Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers", "paper_summary": "Large Language Models (LLMs) have profoundly changed the world. Their\nself-attention mechanism is the key to the success of transformers in LLMs.\nHowever, the quadratic computational cost $O(n^2)$ to the length $n$ input\nsequence is the notorious obstacle for further improvement and scalability in\nthe longer context. In this work, we leverage the convolution-like structure of\nattention matrices to develop an efficient approximation method for attention\ncomputation using convolution matrices. We propose a $\\mathsf{conv}$ basis\nsystem, \"similar\" to the rank basis, and show that any lower triangular\n(attention) matrix can always be decomposed as a sum of $k$ structured\nconvolution matrices in this basis system. We then design an algorithm to\nquickly decompose the attention matrix into $k$ convolution matrices. Thanks to\nFast Fourier Transforms (FFT), the attention {\\it inference} can be computed in\n$O(knd \\log n)$ time, where $d$ is the hidden dimension. In practice, we have $\nd \\ll n$, i.e., $d=3,072$ and $n=1,000,000$ for Gemma. Thus, when $kd =\nn^{o(1)}$, our algorithm achieve almost linear time, i.e., $n^{1+o(1)}$.\nFurthermore, the attention {\\it training forward} and {\\it backward gradient}\ncan be computed in $n^{1+o(1)}$ as well. Our approach can avoid explicitly\ncomputing the $n \\times n$ attention matrix, which may largely alleviate the\nquadratic computational complexity. Furthermore, our algorithm works on any\ninput matrices. This work provides a new paradigm for accelerating attention\ncomputation in transformers to enable their application to longer contexts.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6df1\u523b\u5730\u6539\u8b8a\u4e86\u4e16\u754c\u3002\u5b83\u5011\u7684\u81ea\u6211\u6ce8\u610f\u6a5f\u5236\u662f LLM \u4e2dTransformer\u7684\u6210\u529f\u7684\u95dc\u9375\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u9577\u5ea6\u70ba n \u7684\u8f38\u5165\u5e8f\u5217\uff0c\u4e8c\u6b21\u8a08\u7b97\u6210\u672c $O(n^2)$ \u662f\u9032\u4e00\u6b65\u6539\u9032\u548c\u64f4\u5c55\u5230\u66f4\u9577\u8a9e\u5883\u4e2d\u7684\u8457\u540d\u969c\u7919\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5229\u7528\u6ce8\u610f\u77e9\u9663\u7684\u5377\u7a4d\u985e\u4f3c\u7d50\u69cb\uff0c\u958b\u767c\u4e86\u4e00\u7a2e\u4f7f\u7528\u5377\u7a4d\u77e9\u9663\u5c0d\u6ce8\u610f\u529b\u8a08\u7b97\u9032\u884c\u6709\u6548\u8fd1\u4f3c\u7684\u65b9\u6cd5\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b $\\mathsf{conv}$ \u57fa\u790e\u7cfb\u7d71\uff0c\u5b83\u300c\u985e\u4f3c\u300d\u65bc\u79e9\u57fa\u790e\uff0c\u4e26\u8b49\u660e\u4efb\u4f55\u4e0b\u4e09\u89d2\uff08\u6ce8\u610f\u529b\uff09\u77e9\u9663\u90fd\u53ef\u4ee5\u59cb\u7d42\u5206\u89e3\u70ba\u9019\u500b\u57fa\u790e\u7cfb\u7d71\u4e2d k \u500b\u7d50\u69cb\u5316\u5377\u7a4d\u77e9\u9663\u7684\u548c\u3002\u7136\u5f8c\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u7a2e\u6f14\u7b97\u6cd5\uff0c\u5c07\u6ce8\u610f\u529b\u77e9\u9663\u5feb\u901f\u5206\u89e3\u70ba k \u500b\u5377\u7a4d\u77e9\u9663\u3002\u5f97\u76ca\u65bc\u5feb\u901f\u5085\u7acb\u8449\u8f49\u63db (FFT)\uff0c\u6ce8\u610f\u529b\u300c\u63a8\u8ad6\u300d\u53ef\u4ee5\u5728 $O(knd \\log n)$ \u6642\u9593\u5167\u8a08\u7b97\uff0c\u5176\u4e2d d \u662f\u96b1\u85cf\u7dad\u5ea6\u3002\u5728\u5be6\u52d9\u4e2d\uff0c\u6211\u5011\u6709 $d \\ll n$\uff0c\u5373\u5c0d\u65bc Gemma\uff0c$d=3,072$ \u4e14 $n=1,000,000$\u3002\u56e0\u6b64\uff0c\u7576 $kd = n^{o(1)}$ \u6642\uff0c\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u5e7e\u4e4e\u9054\u5230\u7dda\u6027\u6642\u9593\uff0c\u5373 $n^{1+o(1)}$\u3002\u6b64\u5916\uff0c\u6ce8\u610f\u529b\u300c\u8a13\u7df4\u524d\u5411\u300d\u548c\u300c\u53cd\u5411\u68af\u5ea6\u300d\u4e5f\u53ef\u4ee5\u5728 $n^{1+o(1)}$ \u4e2d\u8a08\u7b97\u3002\u6211\u5011\u7684\u505a\u6cd5\u53ef\u4ee5\u907f\u514d\u660e\u78ba\u8a08\u7b97 $n \\times n$ \u6ce8\u610f\u529b\u77e9\u9663\uff0c\u9019\u53ef\u80fd\u6703\u5927\u5e45\u6e1b\u8f15\u4e8c\u6b21\u8a08\u7b97\u8907\u96dc\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u9069\u7528\u65bc\u4efb\u4f55\u8f38\u5165\u77e9\u9663\u3002\u9019\u9805\u5de5\u4f5c\u70ba\u52a0\u901fTransformer\u4e2d\u7684\u6ce8\u610f\u529b\u8a08\u7b97\u63d0\u4f9b\u4e86\u4e00\u500b\u65b0\u7684\u5178\u7bc4\uff0c\u4ee5\u4f7f\u5176\u80fd\u5920\u61c9\u7528\u65bc\u66f4\u9577\u7684\u8a9e\u5883\u4e2d\u3002", "author": "Jiuxiang Gu et.al.", "authors": "Jiuxiang Gu, Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Junze Yin", "id": "2405.05219v1", "paper_url": "http://arxiv.org/abs/2405.05219v1", "repo": "null"}}