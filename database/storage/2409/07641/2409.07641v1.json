{"2409.07641": {"publish_time": "2024-09-11", "title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks", "paper_summary": "We introduce SimulBench, a benchmark designed to evaluate large language\nmodels (LLMs) across a diverse collection of creative simulation scenarios,\nsuch as acting as a Linux terminal or playing text games with users. While\nthese simulation tasks serve as effective measures of an LLM's general\nintelligence, they are seldom incorporated into existing benchmarks. A major\nchallenge is to develop an evaluation framework for testing different LLMs\nfairly while preserving the multi-round interactive nature of simulation tasks\nbetween users and AI. To tackle this issue, we suggest using a fixed LLM as a\nuser agent to engage with an LLM to collect dialogues first under different\ntasks. Then, challenging dialogue scripts are extracted for evaluating\ndifferent target LLMs. To facilitate automatic assessment on \\DataName{}, GPT-4\nis employed as the evaluator, tasked with reviewing the quality of the final\nresponse generated by the target LLMs given multi-turn dialogue scripts. Our\ncomprehensive experiments indicate that these simulation tasks continue to pose\na significant challenge with their unique natures and show the gap between\nproprietary models and the most advanced open LLMs. For example, GPT-4-turbo\noutperforms LLaMA-3-70b-Chat on 18.55\\% more cases.", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86 SimulBench\uff0c\u9019\u662f\u4e00\u500b\u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u5275\u610f\u6a21\u64ec\u60c5\u5883\u4e2d\u7684\u8868\u73fe\uff0c\u4f8b\u5982\u626e\u6f14 Linux \u7d42\u7aef\u6a5f\u6216\u8207\u4f7f\u7528\u8005\u73a9\u6587\u5b57\u904a\u6232\u3002\u96d6\u7136\u9019\u4e9b\u6a21\u64ec\u4efb\u52d9\u53ef\u4f5c\u70ba LLM \u4e00\u822c\u667a\u80fd\u7684\u6709\u6548\u8861\u91cf\u6a19\u6e96\uff0c\u4f46\u5b83\u5011\u5f88\u5c11\u7d0d\u5165\u73fe\u6709\u7684\u57fa\u6e96\u4e2d\u3002\u4e00\u500b\u4e3b\u8981\u7684\u6311\u6230\u662f\u958b\u767c\u4e00\u500b\u8a55\u4f30\u6846\u67b6\uff0c\u7528\u65bc\u516c\u5e73\u5730\u6e2c\u8a66\u4e0d\u540c\u7684 LLM\uff0c\u540c\u6642\u4fdd\u7559\u4f7f\u7528\u8005\u548c AI \u4e4b\u9593\u6a21\u64ec\u4efb\u52d9\u7684\u591a\u8f2a\u4e92\u52d5\u6027\u8cea\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u4e00\u500b\u56fa\u5b9a\u7684 LLM \u4f5c\u70ba\u4f7f\u7528\u8005\u4ee3\u7406\uff0c\u8207 LLM \u4e92\u52d5\uff0c\u4ee5\u4fbf\u5728\u4e0d\u540c\u7684\u4efb\u52d9\u4e0b\u9996\u5148\u6536\u96c6\u5c0d\u8a71\u3002\u7136\u5f8c\uff0c\u64f7\u53d6\u5177\u6709\u6311\u6230\u6027\u7684\u5c0d\u8a71\u8173\u672c\uff0c\u7528\u65bc\u8a55\u4f30\u4e0d\u540c\u7684\u76ee\u6a19 LLM\u3002\u70ba\u4e86\u4fc3\u9032\u5c0d \\DataName{} \u7684\u81ea\u52d5\u8a55\u4f30\uff0cGPT-4 \u88ab\u7528\u4f5c\u8a55\u4f30\u5668\uff0c\u8ca0\u8cac\u5be9\u67e5\u76ee\u6a19 LLM \u5728\u7d66\u5b9a\u591a\u8f2a\u5c0d\u8a71\u8173\u672c\u7684\u60c5\u6cc1\u4e0b\u7522\u751f\u7684\u6700\u7d42\u56de\u61c9\u7684\u54c1\u8cea\u3002\u6211\u5011\u5168\u9762\u7684\u5be6\u9a57\u8868\u660e\uff0c\u9019\u4e9b\u6a21\u64ec\u4efb\u52d9\u6301\u7e8c\u69cb\u6210\u4e00\u500b\u91cd\u5927\u7684\u6311\u6230\uff0c\u5177\u6709\u5176\u7368\u7279\u7684\u6027\u8cea\uff0c\u4e26\u986f\u793a\u51fa\u5c08\u6709\u6a21\u578b\u548c\u6700\u5148\u9032\u7684\u958b\u653e\u5f0f LLM \u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u4f8b\u5982\uff0cGPT-4-turbo \u5728 18.55% \u7684\u6848\u4f8b\u4e2d\u8868\u73fe\u512a\u65bc LLaMA-3-70b-Chat\u3002", "author": "Qi Jia et.al.", "authors": "Qi Jia, Xiang Yue, Tianyu Zheng, Jie Huang, Bill Yuchen Lin", "id": "2409.07641v1", "paper_url": "http://arxiv.org/abs/2409.07641v1", "repo": "null"}}