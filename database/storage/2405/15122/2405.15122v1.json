{"2405.15122": {"publish_time": "2024-05-24", "title": "Generalizable and Scalable Multistage Biomedical Concept Normalization Leveraging Large Language Models", "paper_summary": "Background: Biomedical entity normalization is critical to biomedical\nresearch because the richness of free-text clinical data, such as progress\nnotes, can often be fully leveraged only after translating words and phrases\ninto structured and coded representations suitable for analysis. Large Language\nModels (LLMs), in turn, have shown great potential and high performance in a\nvariety of natural language processing (NLP) tasks, but their application for\nnormalization remains understudied.\n  Methods: We applied both proprietary and open-source LLMs in combination with\nseveral rule-based normalization systems commonly used in biomedical research.\nWe used a two-step LLM integration approach, (1) using an LLM to generate\nalternative phrasings of a source utterance, and (2) to prune candidate UMLS\nconcepts, using a variety of prompting methods. We measure results by\n$F_{\\beta}$, where we favor recall over precision, and F1.\n  Results: We evaluated a total of 5,523 concept terms and text contexts from a\npublicly available dataset of human-annotated biomedical abstracts.\nIncorporating GPT-3.5-turbo increased overall $F_{\\beta}$ and F1 in\nnormalization systems +9.5 and +7.3 (MetaMapLite), +13.9 and +10.9 (QuickUMLS),\nand +10.5 and +10.3 (BM25), while the open-source Vicuna model achieved +10.8\nand +12.2 (MetaMapLite), +14.7 and +15 (QuickUMLS), and +15.6 and +18.7 (BM25).\n  Conclusions: Existing general-purpose LLMs, both propriety and open-source,\ncan be leveraged at scale to greatly improve normalization performance using\nexisting tools, with no fine-tuning.", "paper_summary_zh": "<paragraph>\u80cc\u666f\uff1a\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u6807\u51c6\u5316\u5bf9\u4e8e\u751f\u7269\u533b\u5b66\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u81ea\u7531\u6587\u672c\u4e34\u5e8a\u6570\u636e\uff08\u4f8b\u5982\u8fdb\u5c55\u8bb0\u5f55\uff09\u7684\u4e30\u5bcc\u6027\u901a\u5e38\u53ea\u6709\u5728\u5c06\u5355\u8bcd\u548c\u77ed\u8bed\u8f6c\u6362\u4e3a\u9002\u5408\u5206\u6790\u7684\u7ed3\u6784\u5316\u548c\u7f16\u7801\u8868\u793a\u540e\u624d\u80fd\u5f97\u5230\u5145\u5206\u5229\u7528\u3002\u53cd\u8fc7\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\u548c\u9ad8\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u6807\u51c6\u5316\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u7136\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\n\u65b9\u6cd5\uff1a\u6211\u4eec\u5c06\u4e13\u6709\u548c\u5f00\u6e90 LLM \u4e0e\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u5e38\u7528\u7684\u51e0\u4e2a\u57fa\u4e8e\u89c4\u5219\u7684\u6807\u51c6\u5316\u7cfb\u7edf\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u4f7f\u7528\u4e86\u4e24\u6b65 LLM \u96c6\u6210\u65b9\u6cd5\uff0c\uff081\uff09\u4f7f\u7528 LLM \u751f\u6210\u6e90\u8bdd\u8bed\u7684\u66ff\u4ee3\u63aa\u8f9e\uff0c\u4ee5\u53ca\uff082\uff09\u4f7f\u7528\u5404\u79cd\u63d0\u793a\u65b9\u6cd5\u6765\u4fee\u526a\u5019\u9009 UMLS \u6982\u5ff5\u3002\u6211\u4eec\u901a\u8fc7 $F_{\\beta}$ \u8861\u91cf\u7ed3\u679c\uff0c\u5176\u4e2d\u6211\u4eec\u66f4\u503e\u5411\u4e8e\u53ec\u56de\u7387\u800c\u4e0d\u662f\u51c6\u786e\u7387\uff0c\u4ee5\u53ca F1\u3002\n\u7ed3\u679c\uff1a\u6211\u4eec\u4ece\u4eba\u7c7b\u6ce8\u91ca\u7684\u751f\u7269\u533b\u5b66\u6458\u8981\u7684\u516c\u5f00\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u603b\u5171 5,523 \u4e2a\u6982\u5ff5\u672f\u8bed\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u3002\u7ed3\u5408 GPT-3.5-turbo \u4f7f\u6807\u51c6\u5316\u7cfb\u7edf\u4e2d\u7684\u6574\u4f53 $F_{\\beta}$ \u548c F1 \u5206\u522b\u63d0\u9ad8\u4e86 +9.5 \u548c +7.3\uff08MetaMapLite\uff09\uff0c+13.9 \u548c +10.9\uff08QuickUMLS\uff09\uff0c\u4ee5\u53ca +10.5 \u548c +10.3\uff08BM25\uff09\uff0c\u800c\u5f00\u6e90 Vicuna \u6a21\u578b\u5206\u522b\u63d0\u9ad8\u4e86 +10.8 \u548c +12.2\uff08MetaMapLite\uff09\uff0c+14.7 \u548c +15\uff08QuickUMLS\uff09\uff0c\u4ee5\u53ca +15.6 \u548c +18.7\uff08BM25\uff09\u3002\n\u7ed3\u8bba\uff1a\u73b0\u6709\u7684\u901a\u7528 LLM\uff0c\u65e0\u8bba\u662f\u4e13\u6709\u7684\u8fd8\u662f\u5f00\u6e90\u7684\uff0c\u90fd\u53ef\u4ee5\u5927\u89c4\u6a21\u5229\u7528\uff0c\u4ee5\u4f7f\u7528\u73b0\u6709\u5de5\u5177\u6781\u5927\u5730\u63d0\u9ad8\u6807\u51c6\u5316\u6027\u80fd\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u5fae\u8c03\u3002</paragraph>", "author": "Nicholas J Dobbins et.al.", "authors": "Nicholas J Dobbins", "id": "2405.15122v1", "paper_url": "http://arxiv.org/abs/2405.15122v1", "repo": "null"}}