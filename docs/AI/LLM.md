
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-15**|**Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion**|Yongyuan Liang et.al.|[2407.10973v1](http://arxiv.org/abs/2407.10973v1)|null|
|**2024-07-15**|**VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation**|Bocheng Zou et.al.|[2407.10972v1](http://arxiv.org/abs/2407.10972v1)|[link](https://github.com/vgbench/VGBench)|
|**2024-07-15**|**Q-Sparse: All Large Language Models can be Fully Sparsely-Activated**|Hongyu Wang et.al.|[2407.10969v1](http://arxiv.org/abs/2407.10969v1)|null|
|**2024-07-15**|**Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes**|Yaoting Wang et.al.|[2407.10957v1](http://arxiv.org/abs/2407.10957v1)|null|
|**2024-07-15**|**Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?**|Ruisheng Cao et.al.|[2407.10956v1](http://arxiv.org/abs/2407.10956v1)|[link](https://github.com/xlang-ai/spider2-v)|
|**2024-07-15**|**MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models**|Chengguang Gan et.al.|[2407.10953v1](http://arxiv.org/abs/2407.10953v1)|null|
|**2024-07-15**|**Representing Rule-based Chatbots with Transformers**|Dan Friedman et.al.|[2407.10949v1](http://arxiv.org/abs/2407.10949v1)|[link](https://github.com/princeton-nlp/eliza-transformer)|
|**2024-07-15**|**Learning from Naturally Occurring Feedback**|Shachar Don-Yehiya et.al.|[2407.10944v1](http://arxiv.org/abs/2407.10944v1)|null|
|**2024-07-15**|**Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together**|Dilara Soylu et.al.|[2407.10930v1](http://arxiv.org/abs/2407.10930v1)|null|
|**2024-07-15**|**Benchmarking Vision Language Models for Cultural Understanding**|Shravan Nayak et.al.|[2407.10920v1](http://arxiv.org/abs/2407.10920v1)|null|
|**2024-07-15**|**Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis**|Yunting Liu et.al.|[2407.10899v1](http://arxiv.org/abs/2407.10899v1)|null|
|**2024-07-15**|**Leveraging Multimodal CycleGAN for the Generation of Anatomically Accurate Synthetic CT Scans from MRIs**|Leonardo Crespi et.al.|[2407.10888v1](http://arxiv.org/abs/2407.10888v1)|null|
|**2024-07-15**|**Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique**|Mark Russinovich et.al.|[2407.10887v1](http://arxiv.org/abs/2407.10887v1)|null|
|**2024-07-15**|**Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models**|Rui Zhang et.al.|[2407.10873v1](http://arxiv.org/abs/2407.10873v1)|null|
|**2024-07-15**|**GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM**|Keshav Bimbraw et.al.|[2407.10870v1](http://arxiv.org/abs/2407.10870v1)|null|
|**2024-07-15**|**Weighted Grouped Query Attention in Transformers**|Sai Sena Chinnakonduru et.al.|[2407.10855v1](http://arxiv.org/abs/2407.10855v1)|null|
|**2024-07-15**|**An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases**|Dylan Bouchard et.al.|[2407.10853v1](http://arxiv.org/abs/2407.10853v1)|null|
|**2024-07-15**|**Offline Reinforcement Learning with Imputed Rewards**|Carlo Romeo et.al.|[2407.10839v1](http://arxiv.org/abs/2407.10839v1)|null|
|**2024-07-15**|**MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs**|Quang H. Nguyen et.al.|[2407.10834v1](http://arxiv.org/abs/2407.10834v1)|null|
|**2024-07-15**|**BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy**|Tim Menzner et.al.|[2407.10829v1](http://arxiv.org/abs/2407.10829v1)|null|
|**2024-07-15**|**Towards Enhanced Classification of Abnormal Lung sound in Multi-breath: A Light Weight Multi-label and Multi-head Attention Classification Method**|Yi-Wei Chua et.al.|[2407.10828v1](http://arxiv.org/abs/2407.10828v1)|null|
|**2024-07-15**|**LLM Circuit Analyses Are Consistent Across Training and Scale**|Curt Tigges et.al.|[2407.10827v1](http://arxiv.org/abs/2407.10827v1)|null|
|**2024-07-15**|**Enabling MCTS Explainability for Sequential Planning Through Computation Tree Logic**|Ziyan An et.al.|[2407.10820v1](http://arxiv.org/abs/2407.10820v1)|null|
|**2024-07-15**|**Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation**|Tu Vu et.al.|[2407.10817v1](http://arxiv.org/abs/2407.10817v1)|null|
|**2024-07-15**|**FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries**|Yuqi Jiang et.al.|[2407.10810v1](http://arxiv.org/abs/2407.10810v1)|null|
|**2024-07-15**|**Employing Sentence Space Embedding for Classification of Data Stream from Fake News Domain**|Paweł Zyblewski et.al.|[2407.10807v1](http://arxiv.org/abs/2407.10807v1)|null|
|**2024-07-15**|**Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval**|Shengjie Ma et.al.|[2407.10805v1](http://arxiv.org/abs/2407.10805v1)|null|
|**2024-07-15**|**Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment**|Jinhao Jiang et.al.|[2407.10804v1](http://arxiv.org/abs/2407.10804v1)|null|
|**2024-07-15**|**Mammographic Breast Positioning Assessment via Deep Learning**|Toygar Tanyel et.al.|[2407.10796v1](http://arxiv.org/abs/2407.10796v1)|null|
|**2024-07-15**|**Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping**|Wenhao Zhu et.al.|[2407.10795v1](http://arxiv.org/abs/2407.10795v1)|null|
|**2024-07-15**|**Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education**|Rui Yang et.al.|[2407.10794v1](http://arxiv.org/abs/2407.10794v1)|[link](https://github.com/irenezihuili/cgprompt)|
|**2024-07-15**|**GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework**|Hannah Sansford et.al.|[2407.10793v1](http://arxiv.org/abs/2407.10793v1)|null|
|**2024-07-15**|**AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler**|Changhun Kim et.al.|[2407.10784v1](http://arxiv.org/abs/2407.10784v1)|null|
|**2024-07-15**|**MSegRNN:Enhanced SegRNN Model with Mamba for Long-Term Time Series Forecasting**|GaoXiang Zhao et.al.|[2407.10768v1](http://arxiv.org/abs/2407.10768v1)|null|
|**2024-07-15**|**Qwen2-Audio Technical Report**|Yunfei Chu et.al.|[2407.10759v1](http://arxiv.org/abs/2407.10759v1)|[link](https://github.com/qwenlm/qwen2-audio)|
|**2024-07-15**|**Codebook LLMs: Adapting Political Science Codebooks for LLM Use and Adapting LLMs to Follow Codebooks**|Andrew Halterman et.al.|[2407.10747v1](http://arxiv.org/abs/2407.10747v1)|null|
|**2024-07-15**|**What distinguishes conspiracy from critical narratives? A computational analysis of oppositional discourse**|Damir Korenčić et.al.|[2407.10745v1](http://arxiv.org/abs/2407.10745v1)|null|
|**2024-07-15**|**Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs**|W. J. Meijer et.al.|[2407.10743v1](http://arxiv.org/abs/2407.10743v1)|null|
|**2024-07-15**|**Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models**|Rining Wu et.al.|[2407.10737v1](http://arxiv.org/abs/2407.10737v1)|[link](https://github.com/wurining/Vi-ST)|
|**2024-07-15**|**Transforming Agency. On the mode of existence of Large Language Models**|Xabier E. Barandiaran et.al.|[2407.10735v2](http://arxiv.org/abs/2407.10735v2)|null|
|**2024-07-15**|**When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion Image Laundering**|Sara Mandelli et.al.|[2407.10736v1](http://arxiv.org/abs/2407.10736v1)|null|
|**2024-07-15**|**On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers**|Mark Deutel et.al.|[2407.10734v1](http://arxiv.org/abs/2407.10734v1)|null|
|**2024-07-15**|**CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses**|Jing Yao et.al.|[2407.10725v1](http://arxiv.org/abs/2407.10725v1)|null|
|**2024-07-15**|**Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning**|Yulong Wang et.al.|[2407.10718v2](http://arxiv.org/abs/2407.10718v2)|[link](https://github.com/ag2s1/sibyl-system)|
|**2024-07-15**|**SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation**|Kaiming Shen et.al.|[2407.10714v1](http://arxiv.org/abs/2407.10714v1)|null|
|**2024-07-15**|**DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems**|Anni Zou et.al.|[2407.10701v1](http://arxiv.org/abs/2407.10701v1)|null|
|**2024-07-15**|**$\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity**|Fengyu Cai et.al.|[2407.10691v1](http://arxiv.org/abs/2407.10691v1)|null|
|**2024-07-15**|**Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN**|Seyed Amir Latifi et.al.|[2407.10689v1](http://arxiv.org/abs/2407.10689v1)|null|
|**2024-07-15**|**Addressing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval**|Youngsun Lim et.al.|[2407.10683v1](http://arxiv.org/abs/2407.10683v1)|null|
|**2024-07-15**|**Qwen2 Technical Report**|An Yang et.al.|[2407.10671v1](http://arxiv.org/abs/2407.10671v1)|null|
|**2024-07-15**|**Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems**|Yunxiao Shi et.al.|[2407.10670v1](http://arxiv.org/abs/2407.10670v1)|[link](https://github.com/ancientshi/erm4)|
|**2024-07-15**|**Spatio-temporal neural distance fields for conditional generative modeling of the heart**|Kristine Sørensen et.al.|[2407.10663v1](http://arxiv.org/abs/2407.10663v1)|[link](https://github.com/kristineaajuhl/spatio_temporal_generative_cardiac_model)|
|**2024-07-15**|**An Empirical Study of Validating Synthetic Data for Formula Generation**|Usneek Singh et.al.|[2407.10657v1](http://arxiv.org/abs/2407.10657v1)|null|
|**2024-07-15**|**Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models**|Louis Abraham et.al.|[2407.10645v1](http://arxiv.org/abs/2407.10645v1)|null|
|**2024-07-15**|**Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model**|Zhening Liu et.al.|[2407.10632v1](http://arxiv.org/abs/2407.10632v1)|null|
|**2024-07-15**|**Balancing the Scales: Reinforcement Learning for Fair Classification**|Leon Eshuijs et.al.|[2407.10629v1](http://arxiv.org/abs/2407.10629v1)|null|
|**2024-07-15**|**Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena**|Haipeng Luo et.al.|[2407.10627v1](http://arxiv.org/abs/2407.10627v1)|null|
|**2024-07-15**|**NoviCode: Generating Programs from Natural Language Utterances by Novices**|Asaf Achi Mordechai et.al.|[2407.10626v2](http://arxiv.org/abs/2407.10626v2)|[link](https://github.com/asafam/novicode)|
|**2024-07-15**|**Leave No Knowledge Behind During Knowledge Distillation: Towards Practical and Effective Knowledge Distillation for Code-Switching ASR Using Realistic Data**|Liang-Hsuan Tseng et.al.|[2407.10603v1](http://arxiv.org/abs/2407.10603v1)|null|
|**2024-07-15**|**An evaluation of CNN models and data augmentation techniques in hierarchical localization of mobile robots**|J. J. Cabrera et.al.|[2407.10596v1](http://arxiv.org/abs/2407.10596v1)|[link](https://github.com/juanjo-cabrera/indoorlocalizationsinglecnn)|
|**2024-07-15**|**Three Dogmas of Reinforcement Learning**|David Abel et.al.|[2407.10583v1](http://arxiv.org/abs/2407.10583v1)|null|
|**2024-07-15**|**Boosting Zero-Shot Crosslingual Performance using LLM-Based Augmentations with Effective Data Selection**|Barah Fazili et.al.|[2407.10582v1](http://arxiv.org/abs/2407.10582v1)|[link](https://github.com/csalt-research/llm-based-augmentations-with-effective-data-selection)|
|**2024-07-15**|**Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning**|Daniel Geissler et.al.|[2407.10580v1](http://arxiv.org/abs/2407.10580v1)|null|
|**2024-07-15**|**Beyond Generative Artificial Intelligence: Roadmap for Natural Language Generation**|María Miró Maestre et.al.|[2407.10554v1](http://arxiv.org/abs/2407.10554v1)|null|
|**2024-07-15**|**Learning Social Cost Functions for Human-Aware Path Planning**|Andrea Eirale et.al.|[2407.10547v1](http://arxiv.org/abs/2407.10547v1)|[link](https://github.com/pic4ser/socialcostfunction)|
|**2024-07-15**|**Understanding the Dependence of Perception Model Competency on Regions in an Image**|Sara Pohland et.al.|[2407.10543v1](http://arxiv.org/abs/2407.10543v1)|[link](https://github.com/sarapohland/explainable-competency)|
|**2024-07-15**|**An experimental evaluation of Siamese Neural Networks for robot localization using omnidirectional imaging in indoor environments**|J. J. Cabrera et.al.|[2407.10536v1](http://arxiv.org/abs/2407.10536v1)|null|
|**2024-07-15**|**TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction**|Xingzhi Zhou et.al.|[2407.10510v1](http://arxiv.org/abs/2407.10510v1)|null|
|**2024-07-15**|**CIBench: Evaluating Your LLMs with a Code Interpreter Plugin**|Songyang Zhang et.al.|[2407.10499v1](http://arxiv.org/abs/2407.10499v1)|null|
|**2024-07-15**|**Learning Dynamics of LLM Finetuning**|Yi Ren et.al.|[2407.10490v1](http://arxiv.org/abs/2407.10490v1)|[link](https://github.com/joshua-ren/learning_dynamics_llm)|
|**2024-07-15**|**How and where does CLIP process negation?**|Vincent Quantmeyer et.al.|[2407.10488v1](http://arxiv.org/abs/2407.10488v1)|null|
|**2024-07-15**|**IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization**|Jie Cao et.al.|[2407.10486v1](http://arxiv.org/abs/2407.10486v1)|null|
|**2024-07-15**|**SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation**|Jordan Juravsky et.al.|[2407.10481v1](http://arxiv.org/abs/2407.10481v1)|null|
|**2024-07-15**|**Kinetic Typography Diffusion Model**|Seonmi Park et.al.|[2407.10476v1](http://arxiv.org/abs/2407.10476v1)|null|
|**2024-07-15**|**GROOT: Generating Robust Watermark for Diffusion-Model-Based Audio Synthesis**|Weizhi Liu et.al.|[2407.10471v1](http://arxiv.org/abs/2407.10471v1)|null|
|**2024-07-15**|**LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis**|Zhenxiong Tan et.al.|[2407.10468v1](http://arxiv.org/abs/2407.10468v1)|null|
|**2024-07-15**|**BandControlNet: Parallel Transformers-based Steerable Popular Music Generation with Fine-Grained Spatiotemporal Features**|Jing Luo et.al.|[2407.10462v1](http://arxiv.org/abs/2407.10462v1)|null|
|**2024-07-15**|**The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism**|Yifan Song et.al.|[2407.10457v1](http://arxiv.org/abs/2407.10457v1)|[link](https://github.com/yifan-song793/goodbadgreedy)|
|**2024-07-15**|**Don't Throw Away Data: Better Sequence Knowledge Distillation**|Jun Wang et.al.|[2407.10456v1](http://arxiv.org/abs/2407.10456v1)|null|
|**2024-07-15**|**Enhancing Medication Recommendation with LLM Text Representation**|Yu-Tzu Lee et.al.|[2407.10453v1](http://arxiv.org/abs/2407.10453v1)|null|
|**2024-07-15**|**GraphPrint: Extracting Features from 3D Protein Structure for Drug Target Affinity Prediction**|Amritpal Singh et.al.|[2407.10452v1](http://arxiv.org/abs/2407.10452v1)|null|
|**2024-07-15**|**DDFAD: Dataset Distillation Framework for Audio Data**|Wenbo Jiang et.al.|[2407.10446v1](http://arxiv.org/abs/2407.10446v1)|null|
|**2024-07-15**|**A Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT**|Chunshi Wang et.al.|[2407.10433v1](http://arxiv.org/abs/2407.10433v1)|null|
|**2024-07-15**|**Expanding the Scope: Inductive Knowledge Graph Reasoning with Multi-Starting Progressive Propagation**|Zhoutian Shao et.al.|[2407.10430v1](http://arxiv.org/abs/2407.10430v1)|[link](https://github.com/nju-websoft/mstar)|
|**2024-07-15**|**CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization**|Yang Zhao et.al.|[2407.10424v2](http://arxiv.org/abs/2407.10424v2)|null|
|**2024-07-15**|**Melon Fruit Detection and Quality Assessment Using Generative AI-Based Image Data Augmentation**|Seungri Yoon et.al.|[2407.10413v1](http://arxiv.org/abs/2407.10413v1)|null|
|**2024-07-15**|**Cooperative Reward Shaping for Multi-Agent Pathfinding**|Zhenyu Song et.al.|[2407.10403v1](http://arxiv.org/abs/2407.10403v1)|null|
|**2024-07-15**|**Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity**|Santiago Pascual et.al.|[2407.10387v1](http://arxiv.org/abs/2407.10387v1)|null|
|**2024-07-15**|**By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting**|Hyungjun Yoon et.al.|[2407.10385v1](http://arxiv.org/abs/2407.10385v1)|null|
|**2024-07-15**|**NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models**|Pranshu Pandya et.al.|[2407.10380v1](http://arxiv.org/abs/2407.10380v1)|null|
|**2024-07-15**|**Enhanced Self-supervised Learning for Multi-modality MRI Segmentation and Classification: A Novel Approach Avoiding Model Collapse**|Linxuan Han et.al.|[2407.10377v1](http://arxiv.org/abs/2407.10377v1)|[link](https://github.com/linxuanhan/m2-mae)|
|**2024-07-15**|**Large Language Model-based FMRI Encoding of Language Functions for Subjects with Neurocognitive Disorder**|Yuejiao Wang et.al.|[2407.10376v1](http://arxiv.org/abs/2407.10376v1)|null|
|**2024-07-15**|**An Empirical Study of Mamba-based Pedestrian Attribute Recognition**|Xiao Wang et.al.|[2407.10374v1](http://arxiv.org/abs/2407.10374v1)|[link](https://github.com/event-ahu/openpar)|
|**2024-07-15**|**Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion**|Jian Ma et.al.|[2407.10373v1](http://arxiv.org/abs/2407.10373v1)|null|
|**2024-07-15**|**Accessing Vision Foundation Models at ImageNet-level Costs**|Yitian Zhang et.al.|[2407.10366v1](http://arxiv.org/abs/2407.10366v1)|[link](https://github.com/bespontaneous/proteus-pytorch)|
|**2024-07-14**|**LAB-Bench: Measuring Capabilities of Language Models for Biology Research**|Jon M. Laurent et.al.|[2407.10362v1](http://arxiv.org/abs/2407.10362v1)|null|
|**2024-07-14**|**Evolved Developmental Artificial Neural Networks for Multitasking with Advanced Activity Dependence**|Yintong Zhang et.al.|[2407.10359v1](http://arxiv.org/abs/2407.10359v1)|null|
|**2024-07-14**|**Comparing Complex Concepts with Transformers: Matching Patent Claims Against Natural Language Text**|Matthias Blume et.al.|[2407.10351v1](http://arxiv.org/abs/2407.10351v1)|null|
|**2024-07-14**|**MambaForGCN: Enhancing Long-Range Dependency with State Space Model and Kolmogorov-Arnold Networks for Aspect-Based Sentiment Analysis**|Adamu Lawan et.al.|[2407.10347v1](http://arxiv.org/abs/2407.10347v1)|null|
|**2024-07-14**|**Affordance-Guided Reinforcement Learning via Visual Prompting**|Olivia Y. Lee et.al.|[2407.10341v1](http://arxiv.org/abs/2407.10341v1)|null|

#### Abstracts
##### **Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion**
2407.10973v1 by Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu

Can we generate a control policy for an agent using just one demonstration of
desired behaviors as a prompt, as effortlessly as creating an image from a
textual description? In this paper, we present Make-An-Agent, a novel policy
parameter generator that leverages the power of conditional diffusion models
for behavior-to-policy generation. Guided by behavior embeddings that encode
trajectory information, our policy generator synthesizes latent parameter
representations, which can then be decoded into policy networks. Trained on
policy network checkpoints and their corresponding trajectories, our generation
model demonstrates remarkable versatility and scalability on multiple tasks and
has a strong generalization ability on unseen tasks to output well-performed
policies with only few-shot demonstrations as inputs. We showcase its efficacy
and efficiency on various domains and tasks, including varying objectives,
behaviors, and even across different robot manipulators. Beyond simulation, we
directly deploy policies generated by Make-An-Agent onto real-world robots on
locomotion tasks.

摘要：我們是否能僅使用一次所需行為的示範作為提示，就能為代理生成控制政策，就像從文字描述中建立影像一樣輕鬆？在本文中，我們提出 Make-An-Agent，一種新穎的政策參數產生器，它利用條件擴散模型的力量進行行為到政策的產生。在編碼軌跡資訊的行為嵌入引導下，我們的政策產生器會綜合潛在參數表示，然後可以將其解碼為政策網路。我們的產生模型在政策網路檢查點及其對應軌跡上訓練，在多項任務上展現出卓越的多功能性和可擴充性，而且在未見任務上具有強大的概化能力，僅以少次示範作為輸入就能輸出表現良好的政策。我們展示其在各種領域和任務上的功效和效率，包括不同的目標、行為，甚至跨越不同的機器人操作器。除了模擬之外，我們直接將 Make-An-Agent 產生的政策部署到現實世界的機器人上，進行運動任務。

##### **VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation**
2407.10972v1 by Bocheng Zou, Mu Cai, Jianrui Zhang, Yong Jae Lee

In the realm of vision models, the primary mode of representation is using
pixels to rasterize the visual world. Yet this is not always the best or unique
way to represent visual content, especially for designers and artists who
depict the world using geometry primitives such as polygons. Vector graphics
(VG), on the other hand, offer a textual representation of visual content,
which can be more concise and powerful for content like cartoons or sketches.
Recent studies have shown promising results on processing vector graphics with
capable Large Language Models (LLMs). However, such works focus solely on
qualitative results, understanding, or a specific type of vector graphics. We
propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics
through diverse aspects, including (a) both visual understanding and
generation, (b) evaluation of various vector graphics formats, (c) diverse
question types, (d) wide range of prompting techniques, (e) under multiple
LLMs. Evaluating on our collected 4279 understanding and 5845 generation
samples, we find that LLMs show strong capability on both aspects while
exhibiting less desirable performance on low-level formats (SVG). Both data and
evaluation pipeline will be open-sourced at https://vgbench.github.io.

摘要：在視覺模型領域中，表示的主要模式是使用像素來光柵化視覺世界。然而，這並非總是表示視覺內容的最佳或唯一方式，特別是對於使用多邊形等幾何圖元描繪世界的設計師和藝術家而言。另一方面，向量圖形 (VG) 提供視覺內容的文字表示，對於卡通或草圖等內容，它可以更簡潔有力。最近的研究顯示了使用大型語言模型 (LLM) 處理向量圖形的有希望的結果。然而，這些工作僅關注定性結果、理解或特定類型的向量圖形。我們提出 VGBench，這是一個綜合基準，用於評估 LLM 處理向量圖形的各個方面，包括 (a) 視覺理解和生成，(b) 評估各種向量圖形格式，(c) 多樣化的問題類型，(d) 廣泛的提示技術，(e) 在多個 LLM 下。在我們收集的 4279 個理解和 5845 個生成範例中進行評估，我們發現 LLM 在這兩個方面都表現出強大的能力，同時在低階格式 (SVG) 上表現出不太理想的效能。資料和評估管道都將在 https://vgbench.github.io/ 開源。

##### **Q-Sparse: All Large Language Models can be Fully Sparsely-Activated**
2407.10969v1 by Hongyu Wang, Shuming Ma, Ruiping Wang, Furu Wei

We introduce, Q-Sparse, a simple yet effective approach to training
sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity
of activations in LLMs which can bring significant efficiency gains in
inference. This is achieved by applying top-K sparsification to the activations
and the straight-through-estimator to the training. The key results from this
work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs
while being much more efficient at inference time; (2) We present an
inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is
effective in different settings, including training-from-scratch,
continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for
both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the
synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the
cornerstone and a clear path to revolutionize the efficiency, including cost
and energy consumption, of future LLMs.

摘要：我們介紹 Q-Sparse，一種簡單但有效的訓練稀疏激活大型語言模型 (LLM) 的方法。Q-Sparse 能讓 LLM 的激活達到完全稀疏，這能在推論中帶來顯著的效率提升。這是透過將前 K 稀疏化套用至激活，以及將直通估計器套用至訓練中所達成。這項工作的關鍵成果為：(1) Q-Sparse 可以達成與基準 LLM 相當的結果，同時在推論時間上更有效率；(2) 我們提出一個用於稀疏激活 LLM 的推論最佳化縮放定律；(3) Q-Sparse 在不同設定中都很有效，包含從頭訓練、現成 LLM 的持續訓練，以及微調；(4) Q-Sparse 適用於全精度和 1 位元 LLM (例如 BitNet b1.58)。特別是，BitNet b1.58 和 Q-Sparse 的協同作用 (可搭載 MoE) 提供了基石和一條明確的道路，以革新未來 LLM 的效率，包含成本和能源消耗。

##### **Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes**
2407.10957v1 by Yaoting Wang, Peiwen Sun, Dongzhan Zhou, Guangyao Li, Honggang Zhang, Di Hu

Traditional reference segmentation tasks have predominantly focused on silent
visual scenes, neglecting the integral role of multimodal perception and
interaction in human experiences. In this work, we introduce a novel task
called Reference Audio-Visual Segmentation (Ref-AVS), which seeks to segment
objects within the visual domain based on expressions containing multimodal
cues. Such expressions are articulated in natural language forms but are
enriched with multimodal cues, including audio and visual descriptions. To
facilitate this research, we construct the first Ref-AVS benchmark, which
provides pixel-level annotations for objects described in corresponding
multimodal-cue expressions. To tackle the Ref-AVS task, we propose a new method
that adequately utilizes multimodal cues to offer precise segmentation
guidance. Finally, we conduct quantitative and qualitative experiments on three
test subsets to compare our approach with existing methods from related tasks.
The results demonstrate the effectiveness of our method, highlighting its
capability to precisely segment objects using multimodal-cue expressions.
Dataset is available at
\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS}.

摘要：傳統的參考分割任務主要集中在靜默的視覺場景上，忽略了多模態感知和互動在人類體驗中的整體作用。在這項工作中，我們引入了一個名為參考音訊視覺分割 (Ref-AVS) 的新任務，它旨在根據包含多模態線索的表達式分割視覺域內的物件。此類表達式以自然語言形式表達，但加入了多模態線索，包括音訊和視覺描述。為了促進這項研究，我們構建了第一個 Ref-AVS 基準，它為對應的多模態線索表達式中描述的物件提供了像素級註解。為了應對 Ref-AVS 任務，我們提出了一種新的方法，它充分利用多模態線索來提供精確的分割指導。最後，我們在三個測試子集中進行了量化和定性實驗，以將我們的方法與相關任務中的現有方法進行比較。結果證明了我們方法的有效性，突出了其使用多模態線索表達式精確分割物件的能力。資料集可在
\href{https://gewu-lab.github.io/Ref-AVS}{https://gewu-lab.github.io/Ref-AVS} 取得。

##### **Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?**
2407.10956v1 by Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu

Data science and engineering workflows often span multiple stages, from
warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As
vision language models (VLMs) advance in multimodal understanding and code
generation, VLM-based agents could potentially automate these workflows by
generating SQL queries, Python code, and GUI operations. This automation can
improve the productivity of experts while democratizing access to large-scale
data analysis. In this paper, we introduce Spider2-V, the first multimodal
agent benchmark focusing on professional data science and engineering
workflows, featuring 494 real-world tasks in authentic computer environments
and incorporating 20 enterprise-level professional applications. These tasks,
derived from real-world use cases, evaluate the ability of a multimodal agent
to perform data-related tasks by writing code and managing the GUI in
enterprise data software systems. To balance realistic simulation with
evaluation simplicity, we devote significant effort to developing automatic
configurations for task setup and carefully crafting evaluation metrics for
each task. Furthermore, we supplement multimodal agents with comprehensive
documents of these enterprise data software systems. Our empirical evaluation
reveals that existing state-of-the-art LLM/VLM-based agents do not reliably
automate full data workflows (14.0% success). Even with step-by-step guidance,
these agents still underperform in tasks that require fine-grained,
knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted
workspaces (10.6%). We hope that Spider2-V paves the way for autonomous
multimodal agents to transform the automation of data science and engineering
workflow. Our code and data are available at https://spider2-v.github.io.

摘要：資料科學和工程工作流程通常涵蓋多個階段，從資料倉儲到編排，使用像 BigQuery、dbt 和 Airbyte 等工具。隨著視覺語言模型 (VLM) 在多模態理解和程式碼產生方面進步，基於 VLM 的代理程式有可能透過產生 SQL 查詢、Python 程式碼和 GUI 操作來自動化這些工作流程。這種自動化可以提升專家的生產力，同時讓更多人可以存取大規模資料分析。在本文中，我們介紹了 Spider2-V，這是第一個專注於專業資料科學和工程工作流程的多模態代理程式基準，在真實的電腦環境中提供 494 項真實世界的任務，並結合了 20 個企業級專業應用程式。這些任務源自真實世界的使用案例，評估多模態代理程式透過撰寫程式碼和管理企業資料軟體系統中的 GUI 來執行與資料相關任務的能力。為了平衡實際模擬和評估的簡潔性，我們投入大量心力來開發任務設定的自動組態，並仔細制定每個任務的評估指標。此外，我們還使用這些企業資料軟體系統的綜合文件來補充多模態代理程式。我們的實證評估顯示，現有的最先進 LLM/VLM 基礎代理程式無法可靠地自動化完整的資料工作流程 (14.0% 成功率)。即使有逐步指導，這些代理程式在需要細緻、知識密集的 GUI 動作 (16.2%) 和涉及遠端雲端託管工作空間 (10.6%) 的任務中仍然表現不佳。我們希望 Spider2-V 能為自體多模態代理程式轉型資料科學和工程工作流程的自動化鋪路。我們的程式碼和資料可以在 https://spider2-v.github.io/ 取得。

##### **MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models**
2407.10953v1 by Chengguang Gan, Qingyu Yin, Xinyang He, Hanjun Wei, Yunhao Liang, Younghun Lim, Shijian Wang, Hexiang Huang, Qinghao Zhang, Shiwen Ni, Tatsunori Mori

The Mutual Reinforcement Effect (MRE) represents a promising avenue in
information extraction and multitasking research. Nevertheless, its
applicability has been constrained due to the exclusive availability of MRE mix
datasets in Japanese, thereby limiting comprehensive exploration by the global
research community. To address this limitation, we introduce a Multilingual MRE
mix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, and
Chinese. In this paper, we also propose a method for dataset translation
assisted by Large Language Models (LLMs), which significantly reduces the
manual annotation time required for dataset construction by leveraging LLMs to
translate the original Japanese datasets. Additionally, we have enriched the
dataset by incorporating open-domain Named Entity Recognition (NER) and
sentence classification tasks. Utilizing this expanded dataset, we developed a
unified input-output framework to train an Open-domain Information Extraction
Large Language Model (OIELLM). The OIELLM model demonstrates the capability to
effectively process novel MMM datasets, exhibiting significant improvements in
performance.

摘要：互補強化效應 (MRE) 代表資訊萃取和多工研究中一個有前途的途徑。儘管如此，其適用性受到僅有日文 MRE 混合資料集的限制，因此限制了全球研究社群的全面探索。為了解決這個限制，我們引進一個多語言 MRE 混合資料集 (MMM)，其中包含英文、日文和中文的 21 個子資料集。在本文中，我們也提出一個由大型語言模型 (LLM) 協助的資料集翻譯方法，透過利用 LLM 翻譯原始日文資料集，大幅減少資料集建構所需的手動註解時間。此外，我們透過納入開放領域命名實體辨識 (NER) 和句子分類任務，豐富了資料集。利用這個擴充的資料集，我們開發了一個統一的輸入輸出架構，來訓練一個開放領域資訊萃取大型語言模型 (OIELLM)。OIELLM 模型展現了有效處理新 MMM 資料集的能力，在效能上展現顯著的進步。

##### **Representing Rule-based Chatbots with Transformers**
2407.10949v1 by Dan Friedman, Abhishek Panigrahi, Danqi Chen

Transformer-based chatbots can conduct fluent, natural-sounding
conversations, but we have limited understanding of the mechanisms underlying
their behavior. Prior work has taken a bottom-up approach to understanding
Transformers by constructing Transformers for various synthetic and formal
language tasks, such as regular expressions and Dyck languages. However, it is
not obvious how to extend this approach to understand more naturalistic
conversational agents. In this work, we take a step in this direction by
constructing a Transformer that implements the ELIZA program, a classic,
rule-based chatbot. ELIZA illustrates some of the distinctive challenges of the
conversational setting, including both local pattern matching and long-term
dialog state tracking. We build on constructions from prior work -- in
particular, for simulating finite-state automata -- showing how simpler
constructions can be composed and extended to give rise to more sophisticated
behavior. Next, we train Transformers on a dataset of synthetically generated
ELIZA conversations and investigate the mechanisms the models learn. Our
analysis illustrates the kinds of mechanisms these models tend to prefer -- for
example, models favor an induction head mechanism over a more precise, position
based copying mechanism; and using intermediate generations to simulate
recurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing
an explicit connection between neural chatbots and interpretable, symbolic
mechanisms, our results offer a new setting for mechanistic analysis of
conversational agents.

摘要：基於 Transformer 的聊天機器人可以進行流暢、聽起來很自然的對話，但我們對於其行為背後機制的了解有限。先前的工作已採取自下而上的方式來了解 Transformer，方法是為各種合成和形式語言任務（例如正規表達式和 Dyck 語言）建構 Transformer。然而，如何延伸此方法來了解更自然的對話代理程式並不明顯。在這項工作中，我們透過建構一個實作 ELIZA 程式（一種經典的基於規則的聊天機器人）的 Transformer，朝這個方向邁出了一步。ELIZA 說明了對話設定中的一些獨特挑戰，包括局部模式比對和長期對話狀態追蹤。我們建立在先前工作的建構之上，特別是對於模擬有限狀態自動機，展示如何將較簡單的建構組合並延伸，以產生更精密的行為。接下來，我們在一個合成產生的 ELIZA 對話資料集上訓練 Transformer，並探討模型學習的機制。我們的分析說明了這些模型傾向於偏好的機制類型，例如，模型偏好歸納頭機制，而非更精確的基於位置的複製機制；以及使用中間生成來模擬遞迴資料結構，例如 ELIZA 的記憶機制。總的來說，透過在神經聊天機器人與可解釋的符號機制之間建立明確的連結，我們的結果為對話代理程式的機制分析提供了新的設定。

##### **Learning from Naturally Occurring Feedback**
2407.10944v1 by Shachar Don-Yehiya, Leshem Choshen, Omri Abend

Human feedback data is a critical component in developing language models.
However, collecting this feedback is costly and ultimately not scalable. We
propose a scalable method for extracting feedback that users naturally include
when interacting with chat models, and leveraging it for model training. We are
further motivated by previous work that showed there are also qualitative
advantages to using naturalistic (rather than auto-generated) feedback, such as
less hallucinations and biases. We manually annotated conversation data to
confirm the presence of naturally occurring feedback in a standard corpus,
finding that as much as 30% of the chats include explicit feedback. We apply
our method to over 1M conversations to obtain hundreds of thousands of feedback
samples. Training with the extracted feedback shows significant performance
improvements over baseline models, demonstrating the efficacy of our approach
in enhancing model alignment to human preferences.

摘要：人類回饋資料是開發語言模型中的一個關鍵組成部分。
然而，收集此回饋資料的成本很高，且最終無法擴充。我們
提出了一種可擴充的方法，用於提取使用者在與聊天模型互動時自然納入的回饋，並將其用於模型訓練。我們進一步受到先前研究的啟發，該研究表明使用自然（而非自動產生）回饋也具有品質優勢，例如減少幻覺和偏差。我們手動註解對話資料，以確認標準語料庫中自然發生的回饋，發現多達 30% 的聊天包含明確的回饋。我們將方法應用於超過 100 萬個對話，以取得數十萬個回饋範例。使用提取的回饋進行訓練，顯示出比基準模型有顯著的效能提升，證明了我們的方法在增強模型與人類偏好的對齊方面是有效的。

##### **Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together**
2407.10930v1 by Dilara Soylu, Christopher Potts, Omar Khattab

Natural Language Processing (NLP) systems are increasingly taking the form of
multi-stage pipelines involving multiple distinct language models (LMs) and
prompting strategies. Here we address the question of how to fine-tune such
systems to improve their performance. We cast this as a problem of optimizing
the underlying LM weights and the prompting strategies together, and consider a
challenging but highly realistic scenario in which we have no gold labels for
any intermediate stages in the pipeline. To address this challenge, we evaluate
approximate optimization strategies in which we bootstrap training labels for
all pipeline stages and use these to optimize the pipeline's prompts and
fine-tune its weights alternatingly. In experiments with multi-hop QA,
mathematical reasoning, and feature-based classification, we find that simple
approaches for optimizing the prompts and weights together outperform directly
optimizing weights alone and prompts alone by up to 65% and 5%, respectively,
on average across LMs and tasks. We will release our new optimizers in DSPy at
http://dspy.ai

摘要：自然語言處理 (NLP) 系統正越來越以多階段管線的形式出現，其中涉及多個不同的語言模型 (LM) 和提示策略。在此，我們探討如何微調此類系統以提升其效能。我們將其視為同時最佳化基礎 LM 權重和提示策略的問題，並考慮一個具有挑戰性但高度實際的場景，在該場景中，我們沒有任何中間階段的黃金標籤。為了應對此挑戰，我們評估近似最佳化策略，其中我們為所有管線階段引導訓練標籤，並使用這些標籤交替最佳化管線的提示和微調其權重。在多跳問答、數學推理和基於特徵的分類實驗中，我們發現一起最佳化提示和權重的簡單方法，在 LM 和任務的平均表現上，分別比僅最佳化權重和僅最佳化提示高出 65% 和 5%。我們將在 http://dspy.ai 中的 DSPy 中釋出我們的新最佳化器

##### **Benchmarking Vision Language Models for Cultural Understanding**
2407.10920v1 by Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stańczak, Aishwarya Agrawal

Foundation models and vision-language pre-training have notably advanced
Vision Language Models (VLMs), enabling multimodal processing of visual and
linguistic data. However, their performance has been typically assessed on
general scene understanding - recognizing objects, attributes, and actions -
rather than cultural comprehension. This study introduces CulturalVQA, a visual
question-answering benchmark aimed at assessing VLM's geo-diverse cultural
understanding. We curate a collection of 2,378 image-question pairs with 1-5
answers per question representing cultures from 11 countries across 5
continents. The questions probe understanding of various facets of culture such
as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on
CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of
cultural understanding across regions, with strong cultural understanding
capabilities for North America while significantly lower performance for
Africa. We observe disparity in their performance across cultural facets too,
with clothing, rituals, and traditions seeing higher performances than food and
drink. These disparities help us identify areas where VLMs lack cultural
understanding and demonstrate the potential of CulturalVQA as a comprehensive
evaluation set for gauging VLM progress in understanding diverse cultures.

摘要：基礎模型和視覺語言預訓練顯著進步，
視覺語言模型 (VLM) 能夠對視覺和語言資料進行多模態處理。但是，它們的效能通常在一般場景理解（辨識物件、屬性和動作）上進行評估，而不是文化理解。本研究推出 CulturalVQA，這是一個視覺問答基準，用於評估 VLM 的地理文化理解。我們策劃了一個包含 2,378 個影像問題配對的資料集，每個問題有 1-5 個答案，代表 5 大洲 11 個國家的文化。這些問題探討對文化不同面向的理解，例如服裝、食物、飲料、儀式和傳統。在 CulturalVQA 上對 VLM（包括 GPT-4V 和 Gemini）進行基準測試，揭示了它們在不同地區文化理解程度上的差異，北美有很強的文化理解能力，而非洲的效能則顯著較低。我們也觀察到它們在不同文化面向的效能差異，服裝、儀式和傳統的效能高於食物和飲料。這些差異有助於我們找出 VLM 缺乏文化理解的地方，並證明 CulturalVQA 作為評估 VLM 在理解不同文化方面進展的全面評估集的潛力。

##### **Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis**
2407.10899v1 by Yunting Liu, Shreya Bhandari, Zachary A. Pardos

Effective educational measurement relies heavily on the curation of
well-designed item pools (i.e., possessing the right psychometric properties).
However, item calibration is time-consuming and costly, requiring a sufficient
number of respondents for the response process. We explore using six different
LLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)
and various combinations of them using sampling methods to produce responses
with psychometric properties similar to human answers. Results show that some
LLMs have comparable or higher proficiency in College Algebra than college
students. No single LLM mimics human respondents due to narrow proficiency
distributions, but an ensemble of LLMs can better resemble college students'
ability distribution. The item parameters calibrated by LLM-Respondents have
high correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated
counterparts, and closely resemble the parameters of the human subset (e.g.
0.02 Spearman correlation difference). Several augmentation strategies are
evaluated for their relative performance, with resampling methods proving most
effective, enhancing the Spearman correlation from 0.89 (human only) to 0.93
(augmented human).

摘要：有效的教育測量高度依賴於精心設計的題庫（即具備正確的心理測量特性）。
然而，題目校正既耗時又昂貴，需要足夠數量的受訪者參與答題過程。我們探討使用六種不同的 LLM（GPT-3.5、GPT-4、Llama 2、Llama 3、Gemini-Pro 和 Cohere Command R Plus）以及它們的各種組合，使用抽樣方法來產生與人類答案具有相似心理測量特性的答案。結果表明，一些 LLM 在大學代數中的熟練程度與大學生相當或更高。由於熟練度分佈較窄，沒有任何單一的 LLM 能模仿人類受訪者，但 LLM 的集合可以更好地模擬大學生的能力分佈。由 LLM 受訪者校準的題目參數與其人類校準的對應參數相比具有很高的相關性（例如 GPT-3.5 的相關性> 0.8），並且與人類子集的參數非常相似（例如 Spearman 相關性差異為 0.02）。對幾種擴充策略的相對性能進行了評估，結果證明重抽樣方法最有效，將 Spearman 相關性從 0.89（僅限人類）提高到 0.93（擴充的人類）。

##### **Leveraging Multimodal CycleGAN for the Generation of Anatomically Accurate Synthetic CT Scans from MRIs**
2407.10888v1 by Leonardo Crespi, Samuele Camnasio, Damiano Dei, Nicola Lambri, Pietro Mancosu, Marta Scorsetti, Daniele Loiacono

In many clinical settings, the use of both Computed Tomography (CT) and
Magnetic Resonance (MRI) is necessary to pursue a thorough understanding of the
patient's anatomy and to plan a suitable therapeutical strategy; this is often
the case in MRI-based radiotherapy, where CT is always necessary to prepare the
dose delivery, as it provides the essential information about the radiation
absorption properties of the tissues. Sometimes, MRI is preferred to contour
the target volumes. However, this approach is often not the most efficient, as
it is more expensive, time-consuming and, most importantly, stressful for the
patients. To overcome this issue, in this work, we analyse the capabilities of
different configurations of Deep Learning models to generate synthetic CT scans
from MRI, leveraging the power of Generative Adversarial Networks (GANs) and,
in particular, the CycleGAN architecture, capable of working in an unsupervised
manner and without paired images, which were not available. Several CycleGAN
models were trained unsupervised to generate CT scans from different MRI
modalities with and without contrast agents. To overcome the problem of not
having a ground truth, distribution-based metrics were used to assess the
model's performance quantitatively, together with a qualitative evaluation
where physicians were asked to differentiate between real and synthetic images
to understand how realistic the generated images were. The results show how,
depending on the input modalities, the models can have very different
performances; however, models with the best quantitative results, according to
the distribution-based metrics used, can generate very difficult images to
distinguish from the real ones, even for physicians, demonstrating the
approach's potential.

摘要：在許多臨床環境中，需要使用電腦斷層掃描 (CT) 和磁振造影 (MRI) 來徹底了解患者的解剖結構，並規劃適當的治療策略；這通常發生在基於 MRI 的放射治療中，其中 CT 對於準備劑量傳遞總是必要的，因為它提供了有關組織輻射吸收特性的基本資訊。有時，MRI 優先於勾勒目標體積。然而，這種方法通常不是最有效率的，因為它更昂貴、耗時，最重要的是會讓患者感到壓力。為了克服這個問題，在這項工作中，我們分析了深度學習模型的不同配置，以從 MRI 生成合成 CT 掃描的能力，利用生成對抗網路 (GAN) 的功能，特別是 CycleGAN 架構，能夠以無監督的方式工作，而且不需要成對的影像，而這些影像並不可用。幾個 CycleGAN 模型經過無監督訓練，以從不同 MRI 模式生成 CT 掃描，無論是否使用對比劑。為了克服沒有基本事實的問題，基於分佈的指標被用於定量評估模型的效能，以及定性評估，其中要求醫生區分真實和合成影像，以了解生成的影像有多逼真。結果顯示，根據輸入模式，模型的效能可能大不相同；然而，根據所使用的基於分佈的指標，具有最佳定量結果的模型可以產生非常難以與真實影像區分的影像，即使對於醫生來說也是如此，這證明了這種方法的潛力。

##### **Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique**
2407.10887v1 by Mark Russinovich, Ahmed Salem

Amid growing concerns over the ease of theft and misuse of Large Language
Models (LLMs), the need for fingerprinting models has increased.
Fingerprinting, in this context, means that the model owner can link a given
model to their original version, thereby identifying if their model is being
misused or has been completely stolen. In this paper, we first define a set
five properties a successful fingerprint should satisfy; namely, the
fingerprint should be Transparent, Efficient, Persistent, Robust, and
Unforgeable. Next, we propose Chain & Hash, a new, simple fingerprinting
approach that implements a fingerprint with a cryptographic flavor, achieving
all these properties. Chain & Hash involves generating a set of questions (the
fingerprints) along with a set of potential answers. These elements are hashed
together using a secure hashing technique to select the value for each
question, hence providing an unforgeability property-preventing adversaries
from claiming false ownership. We evaluate the Chain & Hash technique on
multiple models and demonstrate its robustness against benign transformations,
such as fine-tuning on different datasets, and adversarial attempts to erase
the fingerprint. Finally, our experiments demonstrate the efficiency of
implementing Chain & Hash and its utility, where fingerprinted models achieve
almost the same performance as non-fingerprinted ones across different
benchmarks.

摘要：隨著對大型語言模型（LLM）易於被盜用和誤用的擔憂加劇，對模型指紋辨識的需求也隨之增加。在此脈絡中，指紋辨識是指模型所有者可以將特定模型連結到其原始版本，從而識別他們的模型是否遭到誤用或完全被盜。在本文中，我們首先定義一組成功的指紋辨識應具備的五項特性，即：透明、高效、持久、強健和不可偽造。接著，我們提出「鏈結與雜湊」演算法，這是一種新的、簡單的指紋辨識方法，採用具有密碼學特色的指紋辨識，達到了所有這些特性。「鏈結與雜湊」演算法涉及產生一組問題（指紋）以及一組可能的答案。這些元素使用安全的雜湊技術進行雜湊，以選擇每個問題的值，從而提供不可偽造的特性，防止對手聲稱擁有虛假所有權。我們在多個模型上評估「鏈結與雜湊」技術，並展示其對良性轉換的強健性，例如針對不同資料集進行微調，以及對手試圖抹除指紋。最後，我們的實驗證明了實作「鏈結與雜湊」演算法的效率及其效用，其中帶有指紋的模型在不同的基準測試中達到了與未帶指紋模型幾乎相同的效能。

##### **Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models**
2407.10873v1 by Rui Zhang, Fei Liu, Xi Lin, Zhenkun Wang, Zhichao Lu, Qingfu Zhang

Automated heuristic design (AHD) has gained considerable attention for its
potential to automate the development of effective heuristics. The recent
advent of large language models (LLMs) has paved a new avenue for AHD, with
initial efforts focusing on framing AHD as an evolutionary program search (EPS)
problem. However, inconsistent benchmark settings, inadequate baselines, and a
lack of detailed component analysis have left the necessity of integrating LLMs
with search strategies and the true progress achieved by existing LLM-based EPS
methods to be inadequately justified. This work seeks to fulfill these research
queries by conducting a large-scale benchmark comprising four LLM-based EPS
methods and four AHD problems across nine LLMs and five independent runs. Our
extensive experiments yield meaningful insights, providing empirical grounding
for the importance of evolutionary search in LLM-based AHD approaches, while
also contributing to the advancement of future EPS algorithmic development. To
foster accessibility and reproducibility, we have fully open-sourced our
benchmark and corresponding results.

摘要：自動啟發式設計 (AHD) 因其自動化有效啟發式開發的潛力而備受關注。大型語言模型 (LLM) 的最新出現為 AHD 開闢了一條新途徑，最初的努力集中於將 AHD 構建為演化程式搜尋 (EPS) 問題。然而，不一致的基準設定、不足的基準線以及缺乏詳細的組件分析，使得將 LLM 與搜尋策略整合的必要性以及現有基於 LLM 的 EPS 方法取得的真正進展，缺乏充分的證明。這項工作旨在透過進行大規模基準測試來滿足這些研究查詢，其中包含四種基於 LLM 的 EPS 方法和四個 AHD 問題，涵蓋九個 LLM 和五次獨立執行。我們廣泛的實驗產生有意義的見解，為基於 LLM 的 AHD 方法中演化搜尋的重要性提供經驗依據，同時也有助於推進未來的 EPS 演算法開發。為了促進可及性和可複製性，我們已完全開放原始碼我們的基準測試和對應結果。

##### **GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM**
2407.10870v1 by Keshav Bimbraw, Ye Wang, Jing Liu, Toshiaki Koike-Akino

Large vision-language models (LVLMs), such as the Generative Pre-trained
Transformer 4-omni (GPT-4o), are emerging multi-modal foundation models which
have great potential as powerful artificial-intelligence (AI) assistance tools
for a myriad of applications, including healthcare, industrial, and academic
sectors. Although such foundation models perform well in a wide range of
general tasks, their capability without fine-tuning is often limited in
specialized tasks. However, full fine-tuning of large foundation models is
challenging due to enormous computation/memory/dataset requirements. We show
that GPT-4o can decode hand gestures from forearm ultrasound data even with no
fine-tuning, and improves with few-shot, in-context learning.

摘要：大型視覺語言模型（LVLMs），例如生成式預訓練轉換器 4-omni（GPT-4o），是新興的多模態基礎模型，作為強大的 AI 輔助工具，在包括醫療保健、工業和學術領域在內的眾多應用中具有巨大的潛力。儘管此類基礎模型在廣泛的通用任務中表現良好，但它們在沒有微調的情況下的能力通常在專門任務中受到限制。然而，由於巨大的計算/記憶體/資料集需求，大型基礎模型的完全微調具有挑戰性。我們展示了 GPT-4o 即使沒有微調也能從前臂超音波數據中解碼手勢，並且通過少次嘗試的語境學習得到改進。

##### **Weighted Grouped Query Attention in Transformers**
2407.10855v1 by Sai Sena Chinnakonduru, Astarag Mohapatra

The attention mechanism forms the foundational blocks for transformer
language models. Recent approaches show that scaling the model achieves
human-level performance. However, with increasing demands for scaling and
constraints on hardware memory, the inference costs of these models remain
high. To reduce the inference time, Multi-Query Attention (MQA) and
Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet
al., 2023) respectively. In this paper, we propose a variation of Grouped-Query
Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new
learnable parameters for each key and value head in the T5 decoder attention
blocks, enabling the model to take a weighted average during finetuning. Our
model achieves an average of 0.53% improvement over GQA, and the performance
converges to traditional Multi-head attention (MHA) with no additional overhead
during inference. We evaluated the introduction of these parameters and
subsequent finetuning informs the model about the grouping mechanism during
training, thereby enhancing performance. Additionally, we demonstrate the
scaling laws in our analysis by comparing the results between T5-small and
T5-base architecture.

摘要：注意力机制构成了 Transformer 语言模型的基础模块。最近的研究表明，扩展模型可以达到人类水平的性能。然而，随着对扩展的需求不断增加以及对硬件内存的限制，这些模型的推理成本仍然很高。为了减少推理时间，多查询注意力 (MQA) 和分组查询注意力 (GQA) 分别在 (Shazeer, 2019) 和 (Ainslieet al., 2023) 中提出。在本文中，我们提出了分组查询注意力的变体，称为加权分组查询注意力 (WGQA)。我们在 T5 解码器注意力模块中为每个键和值头引入了新的可学习参数，使模型能够在微调期间取加权平均值。我们的模型比 GQA 平均提高了 0.53%，并且在推理期间性能收敛到传统的 Multi-head 注意力 (MHA)，而没有额外的开销。我们评估了这些参数的引入，随后的微调在训练期间告知模型有关分组机制的信息，从而提高了性能。此外，我们通过比较 T5-small 和 T5-base 架构之间的结果展示了我们分析中的缩放定律。

##### **An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases**
2407.10853v1 by Dylan Bouchard

Large language models (LLMs) can exhibit bias in a variety of ways. Such
biases can create or exacerbate unfair outcomes for certain groups within a
protected attribute, including, but not limited to sex, race, sexual
orientation, or age. This paper aims to provide a technical guide for
practitioners to assess bias and fairness risks in LLM use cases. The main
contribution of this work is a decision framework that allows practitioners to
determine which metrics to use for a specific LLM use case. To achieve this,
this study categorizes LLM bias and fairness risks, maps those risks to a
taxonomy of LLM use cases, and then formally defines various metrics to assess
each type of risk. As part of this work, several new bias and fairness metrics
are introduced, including innovative counterfactual metrics as well as metrics
based on stereotype classifiers. Instead of focusing solely on the model
itself, the sensitivity of both prompt-risk and model-risk are taken into
account by defining evaluations at the level of an LLM use case, characterized
by a model and a population of prompts. Furthermore, because all of the
evaluation metrics are calculated solely using the LLM output, the proposed
framework is highly practical and easily actionable for practitioners.

摘要：大型語言模型 (LLM) 可以通過多種方式表現出偏見。此類偏見可能會造成或加劇受保護屬性中特定群體的不公平結果，包括但不限於性別、種族、性取向或年齡。本文旨在為實務工作者提供技術指南，以評估 LLM 使用案例中的偏見和公平風險。這項工作的最大貢獻是決策架構，讓實務工作者可以根據特定 LLM 使用案例來決定使用哪些指標。為此，本研究對 LLM 偏見和公平風險進行分類，將這些風險對應到 LLM 使用案例的分類法，然後正式定義各種指標來評估每種類型的風險。作為這項工作的一部分，引入了幾個新的偏見和公平指標，包括創新的反事實指標以及基於刻板印象分類器的指標。除了專注於模型本身外，還透過在 LLM 使用案例層級定義評估來考量提示風險和模型風險的敏感度，其特徵在於模型和提示族群。此外，由於所有評估指標僅使用 LLM 輸出進行計算，因此建議的架構對實務工作者來說非常實用且易於操作。

##### **Offline Reinforcement Learning with Imputed Rewards**
2407.10839v1 by Carlo Romeo, Andrew D. Bagdanov

Offline Reinforcement Learning (ORL) offers a robust solution to training
agents in applications where interactions with the environment must be strictly
limited due to cost, safety, or lack of accurate simulation environments.
Despite its potential to facilitate deployment of artificial agents in the real
world, Offline Reinforcement Learning typically requires very many
demonstrations annotated with ground-truth rewards. Consequently,
state-of-the-art ORL algorithms can be difficult or impossible to apply in
data-scarce scenarios. In this paper we propose a simple but effective Reward
Model that can estimate the reward signal from a very limited sample of
environment transitions annotated with rewards. Once the reward signal is
modeled, we use the Reward Model to impute rewards for a large sample of
reward-free transitions, thus enabling the application of ORL techniques. We
demonstrate the potential of our approach on several D4RL continuous locomotion
tasks. Our results show that, using only 1\% of reward-labeled transitions from
the original datasets, our learned reward model is able to impute rewards for
the remaining 99\% of the transitions, from which performant agents can be
learned using Offline Reinforcement Learning.

摘要：離線強化學習 (ORL) 提供一個強健的解決方案，用於在互動必須嚴格受到限制的應用程式中訓練代理，原因可能是成本、安全性或缺乏準確的模擬環境。儘管有潛力促進在真實世界中部署人工代理，離線強化學習通常需要許多標示有真實獎勵的示範。因此，最先進的 ORL 演算法可能難以或無法應用在資料稀少的場景中。在本文中，我們提出一個簡單但有效的獎勵模型，可以根據非常有限的環境轉換範例（標示有獎勵）來估計獎勵訊號。一旦獎勵訊號建模完成，我們使用獎勵模型來推算大量無獎勵轉換的獎勵，從而能夠應用 ORL 技術。我們在幾個 D4RL 連續運動任務中展示了我們方法的潛力。我們的結果顯示，僅使用原始資料集中 1% 的標籤獎勵轉換，我們學習到的獎勵模型就能夠推算出剩餘 99% 轉換的獎勵，而我們可以使用離線強化學習從中學習到高性能的代理。

##### **MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs**
2407.10834v1 by Quang H. Nguyen, Duy C. Hoang, Juliette Decugis, Saurav Manchanda, Nitesh V. Chawla, Khoa D. Doan

The rapid progress in machine learning (ML) has brought forth many large
language models (LLMs) that excel in various tasks and areas. These LLMs come
with different abilities and costs in terms of computation or pricing. Since
the demand for each query can vary, e.g., because of the queried domain or its
complexity, defaulting to one LLM in an application is not usually the best
choice, whether it is the biggest, priciest, or even the one with the best
average test performance. Consequently, picking the right LLM that is both
accurate and cost-effective for an application remains a challenge. In this
paper, we introduce MetaLLM, a framework that dynamically and intelligently
routes each query to the optimal LLM (among several available LLMs) for
classification tasks, achieving significantly improved accuracy and
cost-effectiveness. By framing the selection problem as a multi-armed bandit,
MetaLLM balances prediction accuracy and cost efficiency under uncertainty. Our
experiments, conducted on popular LLM platforms such as OpenAI's GPT models,
Amazon's Titan, Anthropic's Claude, and Meta's LLaMa, showcase MetaLLM's
efficacy in real-world scenarios, laying the groundwork for future extensions
beyond classification tasks.

摘要：機器學習 (ML) 的快速進展催生出許多大型語言模型 (LLM)，它們在各種任務和領域中表現出色。這些 LLM 在運算或定價方面具有不同的能力和成本。由於每個查詢的需求可能有所不同，例如，由於查詢的領域或其複雜性，因此在應用程式中預設使用一個 LLM 通常不是最佳選擇，無論它是最龐大、最昂貴或甚至具有最佳平均測試效能。因此，選擇既準確又具有成本效益的適當 LLM 仍然是一個挑戰。在本文中，我們介紹了 MetaLLM，這是一個動態且智慧地將每個查詢路由到最佳 LLM（在幾個可用的 LLM 中）以進行分類任務的框架，顯著提高了準確性和成本效益。透過將選擇問題設定為多臂老虎機，MetaLLM 在不確定性下平衡預測準確性和成本效益。我們的實驗是在流行的 LLM 平台上進行的，例如 OpenAI 的 GPT 模型、Amazon 的 Titan、Anthropic 的 Claude 和 Meta 的 LLaMa，展示了 MetaLLM 在實際場景中的功效，為超越分類任務的未來擴充奠定了基礎。

##### **BiasScanner: Automatic Detection and Classification of News Bias to Strengthen Democracy**
2407.10829v1 by Tim Menzner, Jochen L. Leidner

The increasing consumption of news online in the 21st century coincided with
increased publication of disinformation, biased reporting, hate speech and
other unwanted Web content. We describe BiasScanner, an application that aims
to strengthen democracy by supporting news consumers with scrutinizing news
articles they are reading online. BiasScanner contains a server-side
pre-trained large language model to identify biased sentences of news articles
and a front-end Web browser plug-in. At the time of writing, BiasScanner can
identify and classify more than two dozen types of media bias at the sentence
level, making it the most fine-grained model and only deployed application
(automatic system in use) of its kind. It was implemented in a light-weight and
privacy-respecting manner, and in addition to highlighting likely biased
sentence it also provides explanations for each classification decision as well
as a summary analysis for each news article. While prior research has addressed
news bias detection, we are not aware of any work that resulted in a deployed
browser plug-in (c.f. also biasscanner.org for a Web demo).

摘要：21 世紀網路新聞的消費量與日俱增，同時也伴隨著假訊息、偏頗報導、仇恨言論和其他不當網路內容的增加。我們介紹 BiasScanner，這是一個應用程式，旨在透過協助新聞消費者檢視他們在網路上閱讀的新聞文章，來強化民主。BiasScanner 包含了一個伺服器端的預先訓練大型語言模型，用於辨識新聞文章中的偏頗句子，以及一個前端的網路瀏覽器外掛程式。在撰寫本文時，BiasScanner 可以辨識並分類超過二十幾種媒體偏見，而且是句子層級的，這讓它成為最細緻的模型，並且是同類型中唯一已部署的應用程式（正在使用的自動化系統）。它的實作方式輕量且尊重隱私，除了標示出可能偏頗的句子之外，它也會提供每個分類決策的說明，以及每篇新聞文章的摘要分析。雖然先前的研究已經處理過新聞偏見偵測，但我們不知道有任何研究成果產生了已部署的瀏覽器外掛程式（也請參考 biasscanner.org 以取得網路展示）。

##### **Towards Enhanced Classification of Abnormal Lung sound in Multi-breath: A Light Weight Multi-label and Multi-head Attention Classification Method**
2407.10828v1 by Yi-Wei Chua, Yun-Chien Cheng

This study aims to develop an auxiliary diagnostic system for classifying
abnormal lung respiratory sounds, enhancing the accuracy of automatic abnormal
breath sound classification through an innovative multi-label learning approach
and multi-head attention mechanism. Addressing the issue of class imbalance and
lack of diversity in existing respiratory sound datasets, our study employs a
lightweight and highly accurate model, using a two-dimensional label set to
represent multiple respiratory sound characteristics. Our method achieved a
59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,
demonstrating its advantages in terms of lightweight and high accuracy. This
study not only improves the accuracy of automatic diagnosis of lung respiratory
sound abnormalities but also opens new possibilities for clinical applications.

摘要：本研究旨在開發一個輔助診斷系統，用於分類異常的肺部呼吸音，透過創新的多標籤學習方法和多頭注意力機制，提升自動異常呼吸音分類的準確度。針對現有呼吸音資料集中類別不平衡和缺乏多樣性的問題，本研究採用輕量且高精確度的模型，使用二維標籤組來表示多重呼吸音特徵。我們的模型在 ICBHI2017 資料集的四類別任務中，獲得了 59.2% 的 ICBHI 分數，證明了其在輕量化和高準確度方面的優勢。本研究不僅提升了肺部呼吸音異常自動診斷的準確度，也為臨床應用開啟了新的可能性。

##### **LLM Circuit Analyses Are Consistent Across Training and Scale**
2407.10827v1 by Curt Tigges, Michael Hanna, Qinan Yu, Stella Biderman

Most currently deployed large language models (LLMs) undergo continuous
training or additional finetuning. By contrast, most research into LLMs'
internal mechanisms focuses on models at one snapshot in time (the end of
pre-training), raising the question of whether their results generalize to
real-world settings. Existing studies of mechanisms over time focus on
encoder-only or toy models, which differ significantly from most deployed
models. In this study, we track how model mechanisms, operationalized as
circuits, emerge and evolve across 300 billion tokens of training in
decoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.
We find that task abilities and the functional components that support them
emerge consistently at similar token counts across scale. Moreover, although
such components may be implemented by different attention heads over time, the
overarching algorithm that they implement remains. Surprisingly, both these
algorithms and the types of components involved therein can replicate across
model scale. These results suggest that circuit analyses conducted on small
models at the end of pre-training can provide insights that still apply after
additional pre-training and over model scale.

摘要：目前部署的大型語言模型 (LLM) 大多會進行持續訓練或額外的微調。相比之下，大多數針對 LLM 內部機制的的研究都專注於特定時間點的模型（預訓練結束時），這引發了一個問題，即它們的結果是否能推廣到現實世界的設定。現有的關於機制隨時間推移的研究都專注於僅編碼器或玩具模型，這與大多數已部署模型有顯著不同。在這項研究中，我們追蹤模型機制（以電路形式運作）如何在僅解碼器 LLM 中的 3000 億個訓練標記中出現和演化，模型的參數範圍從 7000 萬到 28 億。我們發現任務能力和支援它們的功能組件會在相似的標記數量下持續出現。此外，儘管這些組件可能隨著時間由不同的注意力頭部實作，但它們實作的總體演算法仍然存在。令人驚訝的是，這些演算法和其中涉及的組件類型都可以在模型規模中複製。這些結果表明，在預訓練結束時對小型模型進行的電路分析可以提供見解，這些見解在額外的預訓練和模型規模中仍然適用。

##### **Enabling MCTS Explainability for Sequential Planning Through Computation Tree Logic**
2407.10820v1 by Ziyan An, Hendrik Baier, Abhishek Dubey, Ayan Mukhopadhyay, Meiyi Ma

Monte Carlo tree search (MCTS) is one of the most capable online search
algorithms for sequential planning tasks, with significant applications in
areas such as resource allocation and transit planning. Despite its strong
performance in real-world deployment, the inherent complexity of MCTS makes it
challenging to understand for users without technical background. This paper
considers the use of MCTS in transportation routing services, where the
algorithm is integrated to develop optimized route plans. These plans are
required to meet a range of constraints and requirements simultaneously,
further complicating the task of explaining the algorithm's operation in
real-world contexts. To address this critical research gap, we introduce a
novel computation tree logic-based explainer for MCTS. Our framework begins by
taking user-defined requirements and translating them into rigorous logic
specifications through the use of language templates. Then, our explainer
incorporates a logic verification and quantitative evaluation module that
validates the states and actions traversed by the MCTS algorithm. The outcomes
of this analysis are then rendered into human-readable descriptive text using a
second set of language templates. The user satisfaction of our approach was
assessed through a survey with 82 participants. The results indicated that our
explanatory approach significantly outperforms other baselines in user
preference.

摘要：蒙特卡羅樹狀搜尋 (MCTS) 是用於順序規劃任務的最有能力的線上搜尋演算法之一，在資源配置和運輸規劃等領域有重要的應用。儘管在實際部署中表現強勁，但 MCTS 的內在複雜性使得沒有技術背景的使用者難以理解。本文考慮在運輸路線服務中使用 MCTS，其中演算法被整合來開發最佳化的路線計畫。這些計畫需要同時滿足一系列限制和需求，進一步複雜化了在實際情況中解釋演算法運作的工作。為了解決這個重要的研究差距，我們引入了一個基於新穎運算樹狀邏輯的 MCTS 解釋器。我們的架構首先採用使用者定義的需求，並透過使用語言範本將它們轉換為嚴謹的邏輯規範。然後，我們的解釋器結合了邏輯驗證和定量評估模組，驗證了 MCTS 演算法所遍歷的狀態和動作。接著將此分析的結果使用第二組語言範本轉換成人類可讀的描述性文字。我們的作法透過一項有 82 位參與者的調查來評估使用者滿意度。結果顯示，我們的解釋方法在使用者偏好方面顯著優於其他基準。

##### **Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation**
2407.10817v1 by Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung

As large language models (LLMs) advance, it becomes more challenging to
reliably evaluate their output due to the high costs of human evaluation. To
make progress towards better LLM autoraters, we introduce FLAMe, a family of
Foundational Large Autorater Models. FLAMe is trained on our large and diverse
collection of 100+ quality assessment tasks comprising 5M+ human judgments,
curated and standardized using publicly released human evaluations from
previous research. FLAMe significantly improves generalization to a wide
variety of held-out tasks, outperforming LLMs trained on proprietary data like
GPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a
powerful starting point for further downstream fine-tuning, using reward
modeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our
FLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative
model trained exclusively on permissively licensed data, outperforming both
GPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more
computationally efficient approach using a novel tail-patch fine-tuning
strategy to optimize our FLAMe multitask mixture for reward modeling evaluation
(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring
approximately 25x less training datapoints. Overall, our FLAMe variants
outperform all popular proprietary LLM-as-a-Judge models we consider across 8
out of 12 autorater evaluation benchmarks, encompassing 53 quality assessment
tasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals
that FLAMe is significantly less biased than these LLM-as-a-Judge models on the
CoBBLEr autorater bias benchmark, while effectively identifying high-quality
responses for code generation.

摘要：隨著大型語言模型 (LLM) 的進步，由於人工評估成本高昂，可靠評估其輸出的難度也隨之增加。為了在改善 LLM 自動評分器方面取得進展，我們推出了 FLAMe，這是一個基礎大型自動評分器模型系列。FLAMe 經過我們龐大且多樣化的 100 多項品質評估任務訓練，包含 500 多萬個人類判斷，並使用先前研究中公開發布的人類評估進行整理和標準化。FLAMe 大幅提升了對各種保留任務的泛化能力，在許多任務上優於訓練於專有數據（例如 GPT-4 和 Claude-3）的 LLM。我們展示 FLAMe 也可以作為進一步下游微調的有力起點，使用獎勵建模評估作為案例研究 (FLAMe-RM)。值得注意的是，在 RewardBench 上，我們的 FLAMe-RM-24B 模型（準確率為 87.8%）是經過許可數據訓練的表現最佳的生成模型，優於 GPT-4-0125 (85.9%) 和 GPT-4o (84.7%)。此外，我們探索了一種使用新穎的尾部修補微調策略的更具運算效率的方法，以優化我們的 FLAMe 多任務混合進行獎勵建模評估 (FLAMe-Opt-RM)，在需要大約少 25 倍的訓練數據點的同時，提供有競爭力的 RewardBench 性能。總的來說，我們的 FLAMe 變體在 12 個自動評分器評估基準中的 8 個基準上優於所有我們考慮的流行專有 LLM 作為評分器模型，包含 53 項品質評估任務，包括 RewardBench 和 LLM-AggreFact。最後，我們的分析顯示，在 CoBBLEr 自動評分器偏差基準上，FLAMe 的偏差顯著低於這些 LLM 作為評分器模型，同時有效地識別出程式碼生成的高品質回應。

##### **FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries**
2407.10810v1 by Yuqi Jiang, Xudong Lu, Qian Jin, Qi Sun, Hanming Wu, Cheng Zhuo

Intelligence is key to advancing integrated circuit (IC) fabrication. Recent
breakthroughs in Large Multimodal Models (LMMs) have unlocked unparalleled
abilities in understanding images and text, fostering intelligent fabrication.
Leveraging the power of LMMs, we introduce FabGPT, a customized IC fabrication
large multimodal model for wafer defect knowledge query. FabGPT manifests
expertise in conducting defect detection in Scanning Electron Microscope (SEM)
images, performing root cause analysis, and providing expert question-answering
(Q&A) on fabrication processes. FabGPT matches enhanced multimodal features to
automatically detect minute defects under complex wafer backgrounds and reduce
the subjectivity of manual threshold settings. Besides, the proposed modulation
module and interactive corpus training strategy embed wafer defect knowledge
into the pre-trained model, effectively balancing Q&A queries related to defect
knowledge and original knowledge and mitigating the modality bias issues.
Experiments on in-house fab data (SEM-WaD) show that our FabGPT achieves
significant performance improvement in wafer defect detection and knowledge
querying.

摘要：智慧是推進積體電路 (IC) 製造的關鍵。最近大型多模態模型 (LMM) 的突破，開啟了理解影像與文字的無與倫比能力，促進智慧製造。我們運用 LMM 的力量，導入 FabGPT，一個客製化的 IC 製造大型多模態模型，用於晶圓缺陷知識查詢。FabGPT 展現了在掃描式電子顯微鏡 (SEM) 影像中進行缺陷檢測、執行根本原因分析，以及提供製造流程的專家問答 (Q&A) 的專業知識。FabGPT 將增強的多模態特徵與自動偵測複雜晶圓背景下的微小缺陷相結合，並降低手動閾值設定的主觀性。此外，所提出的調變模組和互動語料訓練策略，將晶圓缺陷知識嵌入預訓練模型中，有效平衡與缺陷知識相關的問答查詢與原始知識，並減輕模態偏差問題。在內部晶圓廠資料 (SEM-WaD) 上的實驗顯示，我們的 FabGPT 在晶圓缺陷檢測和知識查詢方面，獲得顯著的效能提升。

##### **Employing Sentence Space Embedding for Classification of Data Stream from Fake News Domain**
2407.10807v1 by Paweł Zyblewski, Jakub Klikowski, Weronika Borek-Marciniec, Paweł Ksieniewicz

Tabular data is considered the last unconquered castle of deep learning, yet
the task of data stream classification is stated to be an equally important and
demanding research area. Due to the temporal constraints, it is assumed that
deep learning methods are not the optimal solution for application in this
field. However, excluding the entire -- and prevalent -- group of methods seems
rather rash given the progress that has been made in recent years in its
development. For this reason, the following paper is the first to present an
approach to natural language data stream classification using the sentence
space method, which allows for encoding text into the form of a discrete
digital signal. This allows the use of convolutional deep networks dedicated to
image classification to solve the task of recognizing fake news based on text
data. Based on the real-life Fakeddit dataset, the proposed approach was
compared with state-of-the-art algorithms for data stream classification based
on generalization ability and time complexity.

摘要：表格資料被認為是深度學習最後一座未征服的堡壘，然而
資料串流分類的任務被認為是一個同樣重要且
要求嚴苛的研究領域。由於時間限制，假設
深度學習方法並非應用於此領域的最佳解決方案。然而，排除整個 -- 且普遍 -- 的方法群似乎
有點魯莽，因為近年來在它的發展上已經取得進展。因此，以下論文首次提出
使用句子空間方法進行自然語言資料串流分類的方法，該方法允許將文字編碼成離散
數位訊號的形式。這允許使用專門用於影像分類的卷積深度網路來解決根據文字
資料辨識假新聞的任務。根據真實世界的 Fakeddit 資料集，所提出的方法
與基於泛化能力和時間複雜度的資料串流分類的最新演算法進行比較。

##### **Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval**
2407.10805v1 by Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo

Retrieval-augmented generation (RAG) has significantly advanced large
language models (LLMs) by enabling dynamic information retrieval to mitigate
knowledge gaps and hallucinations in generated content. However, these systems
often falter with complex reasoning and consistency across diverse queries. In
this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns
questions with the knowledge graph and uses it as a navigational tool, which
deepens and refines the RAG paradigm for information collection and
integration. The KG-guided navigation fosters deep and long-range associations
to uphold logical consistency and optimize the scope of retrieval for precision
and interoperability. In conjunction, factual consistency can be better ensured
through semantic similarity guided by precise directives. ToG${2.0}$ not only
improves the accuracy and reliability of LLMs' responses but also demonstrates
the potential of hybrid structured knowledge systems to significantly advance
LLM reasoning, aligning it closer to human-like performance. We conducted
extensive experiments on four public datasets to demonstrate the advantages of
our method compared to the baseline.

摘要：檢索增強生成（RAG）透過啟用動態資訊檢索來減輕生成內容中的知識差距和幻覺，大幅提升大型語言模型（LLM）。然而，這些系統在複雜推理和跨不同查詢的一致性方面常常表現不佳。在這項工作中，我們提出了 Think-on-Graph 2.0，一個增強的 RAG 框架，它將問題與知識圖譜對齊，並將其用作導航工具，這加深並改進了 RAG 典範，用於資訊收集和整合。受知識圖譜引導的導航促進了深層且長程的關聯，以維持邏輯一致性並最佳化檢索範圍，以提高精確度和互操作性。同時，事實一致性可以透過由精確指示引導的語意相似性獲得更好的確保。ToG${2.0}$ 不僅提升了 LLM 回應的準確性和可靠性，也展示了混合結構化知識系統的潛力，可以大幅提升 LLM 推理，使其更接近人類般的表現。我們在四個公開資料集上進行了廣泛的實驗，以展示我們的方法相較於基線的優勢。

##### **Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment**
2407.10804v1 by Jinhao Jiang, Junyi Li, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen

Adapting general large language models (LLMs) to specialized domains presents
great challenges due to varied data distributions. This adaptation typically
requires continual pre-training on massive domain-specific corpora to
facilitate knowledge memorization, followed by training to apply this knowledge
following human instructions and preferences. However, this method may result
in inefficient knowledge memorization due to a lack of awareness of knowledge
utilization and imposes substantial demands on LLMs to simultaneously learn
knowledge utilization and format alignment with limited training samples. To
facilitate the domain adaptation of LLM, we revise this process and propose a
new domain adaptation framework including domain knowledge learning and general
format alignment, called Mix-CPT. Specifically, we first conduct a knowledge
mixture continual pre-training that concurrently focuses on knowledge
memorization and utilization, allowing for mutual reinforcement. To avoid
catastrophic forgetting during the continual pre-training process, we further
incorporate a logit swap self-distillation constraint. Subsequently, leveraging
the knowledge and capabilities acquired during continual pre-training, we
efficiently perform instruction tuning and alignment with a few general
training samples to achieve format alignment. Extensive experiments demonstrate
that our proposed Mix-CPT framework can simultaneously improve the task-solving
capabilities of LLMs on the target and general domains compared to the
traditional adaptation methods.

摘要：將通用大型語言模型 (LLM) 適應到特定領域會因為資料分佈不同而面臨極大的挑戰。這種適應通常需要持續在大量的特定領域語料庫上進行預訓練，以利於知識記憶，接著再進行訓練，以應用這些知識來遵循人類的指示和偏好。然而，這種方法可能會因為缺乏知識利用的意識而導致知識記憶效率不彰，並對 LLM 施加極大的需求，以同時學習知識利用和格式對齊，而訓練樣本有限。為了促進 LLM 的領域適應，我們修改了這個流程，並提出一個新的領域適應架構，包括領域知識學習和一般格式對齊，稱為 Mix-CPT。具體來說，我們首先進行知識混合持續預訓練，同時專注於知識記憶和利用，以利於相互強化。為了避免在持續預訓練過程中發生災難性遺忘，我們進一步納入 logit 交換自我蒸餾約束。隨後，利用在持續預訓練期間獲得的知識和能力，我們有效地執行指令調整和對齊，並使用少量的通用訓練樣本來達成格式對齊。廣泛的實驗證明，與傳統的適應方法相比，我們提出的 Mix-CPT 架構可以同時改善 LLM 在目標和一般領域的任務解決能力。

##### **Mammographic Breast Positioning Assessment via Deep Learning**
2407.10796v1 by Toygar Tanyel, Nurper Denizoglu, Mustafa Ege Seker, Deniz Alis, Esma Cerekci, Ercan Karaarslan, Erkin Aribal, Ilkay Oksuz

Breast cancer remains a leading cause of cancer-related deaths among women
worldwide, with mammography screening as the most effective method for the
early detection. Ensuring proper positioning in mammography is critical, as
poor positioning can lead to diagnostic errors, increased patient stress, and
higher costs due to recalls. Despite advancements in deep learning (DL) for
breast cancer diagnostics, limited focus has been given to evaluating
mammography positioning. This paper introduces a novel DL methodology to
quantitatively assess mammogram positioning quality, specifically in
mediolateral oblique (MLO) views using attention and coordinate convolution
modules. Our method identifies key anatomical landmarks, such as the nipple and
pectoralis muscle, and automatically draws a posterior nipple line (PNL),
offering robust and inherently explainable alternative to well-known
classification and regression-based approaches. We compare the performance of
proposed methodology with various regression and classification-based models.
The CoordAtt UNet model achieved the highest accuracy of 88.63% $\pm$ 2.84 and
specificity of 90.25% $\pm$ 4.04, along with a noteworthy sensitivity of 86.04%
$\pm$ 3.41. In landmark detection, the same model also recorded the lowest mean
errors in key anatomical points and the smallest angular error of 2.42 degrees.
Our results indicate that models incorporating attention mechanisms and
CoordConv module increase the accuracy in classifying breast positioning
quality and detecting anatomical landmarks. Furthermore, we make the labels and
source codes available to the community to initiate an open research area for
mammography, accessible at https://github.com/tanyelai/deep-breast-positioning.

摘要：<paragraph>乳癌仍然是全球女性癌症相關死亡的主要原因，而乳房攝影檢查是最有效的早期檢測方法。確保乳房攝影的正確定位至關重要，因為定位不當可能會導致診斷錯誤、增加患者壓力，並因召回而導致更高的成本。儘管深度學習 (DL) 在乳癌診斷方面取得進展，但對乳房攝影定位的評估卻關注有限。本文介紹了一種新穎的 DL 方法，用於定量評估乳房攝影定位品質，特別是在正中側斜視 (MLO) 視野中使用注意力和座標卷積模組。我們的模型識別出關鍵的解剖標誌，例如乳頭和胸大肌，並自動繪製後乳頭線 (PNL)，提供健全且本質上可解釋的替代方案，以取代眾所周知的分類和基於回歸的方法。我們比較了所提出的方法與各種基於回歸和分類的模型的性能。CoordAtt UNet 模型達到了最高的準確度 88.63% $\pm$ 2.84 和特異性 90.25% $\pm$ 4.04，以及 86.04% 的顯著敏感度 $\pm$ 3.41。在標誌檢測中，同一個模型在關鍵解剖點也記錄了最低的平均誤差和最小的 2.42 度角誤差。我們的結果表明，結合注意力機制和 CoordConv 模組的模型提高了分類乳房定位品質和檢測解剖標誌的準確性。此外，我們將標籤和原始碼提供給社群，以啟動乳房攝影的開放研究領域，網址為 https://github.com/tanyelai/deep-breast-positioning。</paragraph>

##### **Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping**
2407.10795v1 by Wenhao Zhu, Sizhe Liu, Shujian Huang, Shuaijie She, Chris Wendler, Jiajun Chen

Decoding by contrasting layers (DoLa), is designed to improve the generation
quality of large language models (LLMs) by contrasting the prediction
probabilities between an early exit output (amateur logits) and the final
output (expert logits). However, we find that this approach does not work well
on non-English tasks. Inspired by previous interpretability work on language
transition during the model's forward pass, we discover that this issue arises
from a language mismatch between early exit output and final output. In this
work, we propose an improved contrastive decoding algorithm that is effective
for diverse languages beyond English. To obtain more helpful amateur logits, we
devise two strategies to skip a set of bottom, language-agnostic layers based
on our preliminary analysis. Experimental results on multilingual reasoning
benchmarks demonstrate that our proposed method outperforms previous
contrastive decoding baselines and substantially improves LLM's
chain-of-thought reasoning accuracy across 11 languages. The project will be
available at: https://github.com/NJUNLP/SkipLayerCD.

摘要：透過對比層（DoLa）進行解碼，旨在透過對比早期退出輸出（業餘邏輯）與最終輸出（專家邏輯）之間的預測機率，來提升大型語言模型（LLM）的生成品質。然而，我們發現此方法在非英語任務中效果不佳。受到先前關於模型前向傳遞過程中語言轉換的可解釋性工作的啟發，我們發現此問題源於早期退出輸出與最終輸出之間的語言不匹配。在這項工作中，我們提出了一種改進的對比解碼演算法，其對英語以外的多種語言有效。為了獲得更有用的業餘邏輯，我們根據初步分析設計了兩個策略來略過一組底層、與語言無關的層。多語言推理基準測試的實驗結果證明，我們提出的方法優於先前的對比解碼基準，並大幅提升了 LLM 在 11 種語言中的思考鏈推理準確度。此專案將於以下網址提供：https://github.com/NJUNLP/SkipLayerCD。

##### **Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education**
2407.10794v1 by Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li

Knowledge graphs (KGs) are crucial in the field of artificial intelligence
and are widely applied in downstream tasks, such as enhancing Question
Answering (QA) systems. The construction of KGs typically requires significant
effort from domain experts. Recently, Large Language Models (LLMs) have been
used for knowledge graph construction (KGC), however, most existing approaches
focus on a local perspective, extracting knowledge triplets from individual
sentences or documents. In this work, we introduce Graphusion, a zero-shot KGC
framework from free text. The core fusion module provides a global view of
triplets, incorporating entity merging, conflict resolution, and novel triplet
discovery. We showcase how Graphusion could be applied to the natural language
processing (NLP) domain and validate it in the educational scenario.
Specifically, we introduce TutorQA, a new expert-verified benchmark for graph
reasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our
evaluation demonstrates that Graphusion surpasses supervised baselines by up to
10% in accuracy on link prediction. Additionally, it achieves average scores of
2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and
relation recognition, respectively.

摘要：<paragraph>知識圖譜 (KG) 在人工智慧領域至關重要，並廣泛應用於下游任務，例如增強問答 (QA) 系統。知識圖譜的建構通常需要領域專家的大量工作。最近，大型語言模型 (LLM) 已被用於知識圖譜建構 (KGC)，然而，現有方法大多關注局部觀點，從個別句子或文件中提取知識三元組。在這項工作中，我們介紹了 Graphusion，一個從自由文本中進行零次學習的 KGC 框架。核心融合模組提供三元組的全局觀點，包含實體合併、衝突解決和新三元組發現。我們展示了如何將 Graphusion 應用於自然語言處理 (NLP) 領域，並在教育場景中驗證它。具體來說，我們介紹了 TutorQA，一個新的由專家驗證的圖譜推理和問答基準，包含六項任務和總計 1,200 個問答對。我們的評估表明，Graphusion 在連結預測的準確度上比監督式基準高出 10%。此外，在概念實體提取和關係識別的人類評估中，它分別獲得了 3 分中的 2.92 分和 2.37 分。</paragraph>

##### **GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework**
2407.10793v1 by Hannah Sansford, Nicholas Richardson, Hermina Petric Maretic, Juba Nait Saada

Methods to evaluate Large Language Model (LLM) responses and detect
inconsistencies, also known as hallucinations, with respect to the provided
knowledge, are becoming increasingly important for LLM applications. Current
metrics fall short in their ability to provide explainable decisions,
systematically check all pieces of information in the response, and are often
too computationally expensive to be used in practice. We present GraphEval: a
hallucination evaluation framework based on representing information in
Knowledge Graph (KG) structures. Our method identifies the specific triples in
the KG that are prone to hallucinations and hence provides more insight into
where in the response a hallucination has occurred, if at all, than previous
methods. Furthermore, using our approach in conjunction with state-of-the-art
natural language inference (NLI) models leads to an improvement in balanced
accuracy on various hallucination benchmarks, compared to using the raw NLI
models. Lastly, we explore the use of GraphEval for hallucination correction by
leveraging the structure of the KG, a method we name GraphCorrect, and
demonstrate that the majority of hallucinations can indeed be rectified.

摘要：大型語言模型 (LLM) 回應評估方法和不一致性偵測（又稱為幻覺），相對於所提供的知識，對於 LLM 應用正變得越來越重要。目前的指標無法提供可解釋的決策、系統性地檢查回應中的所有資訊，而且在實務上使用時，通常過於耗費運算資源。我們提出 GraphEval：一個基於知識圖 (KG) 結構來表示資訊的幻覺評估架構。我們的技術識別出容易出現幻覺的 KG 中特定三元組，因此比以往的方法更深入地了解回應中幻覺發生在哪裡（如果有的話）。此外，將我們的方法與最先進的自然語言推論 (NLI) 模型結合使用，與使用原始 NLI 模型相比，可以在各種幻覺基準上提高平衡準確度。最後，我們探索使用 GraphEval 來進行幻覺修正，方法是利用 KG 的結構，我們將此方法命名為 GraphCorrect，並證明大多數幻覺確實可以得到糾正。

##### **AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler**
2407.10784v1 by Changhun Kim, Taewon Kim, Seungyeon Woo, June Yong Yang, Eunho Yang

In real-world applications, tabular data often suffer from distribution
shifts due to their widespread and abundant nature, leading to erroneous
predictions of pre-trained machine learning models. However, addressing such
distribution shifts in the tabular domain has been relatively underexplored due
to unique challenges such as varying attributes and dataset sizes, as well as
the limited representation learning capabilities of deep learning models for
tabular data. Particularly, with the recent promising paradigm of test-time
adaptation (TTA), where we adapt the off-the-shelf model to the unlabeled
target domain during the inference phase without accessing the source domain,
we observe that directly adopting commonly used TTA methods from other domains
often leads to model collapse. We systematically explore challenges in tabular
data test-time adaptation, including skewed entropy, complex latent space
decision boundaries, confidence calibration issues with both overconfident and
under-confident, and model bias towards source label distributions along with
class imbalances. Based on these insights, we introduce AdapTable, a novel
tabular test-time adaptation method that directly modifies output probabilities
by estimating target label distributions and adjusting initial probabilities
based on calibrated uncertainty. Extensive experiments on both natural
distribution shifts and synthetic corruptions demonstrate the adaptation
efficacy of the proposed method.

摘要：在實際應用中，表格資料由於其廣泛且豐富的特性，經常會出現分佈轉移，導致預訓練機器學習模型的預測錯誤。然而，由於表格領域中分佈轉移的獨特挑戰，例如屬性和資料集大小的變化，以及深度學習模型對表格資料的表示學習能力有限，因此對表格領域中的此類分佈轉移進行處理相對較少。特別是，隨著最近有前途的測試時適應 (TTA) 典範，我們在推論階段將現成的模型適應到未標記的目標域，而無需訪問來源域，我們觀察到直接採用其他域中常用的 TTA 方法通常會導致模型崩潰。我們系統性地探討了表格資料測試時適應中的挑戰，包括偏態熵、複雜的潛在空間決策邊界、過度自信和過度不自信的信心校準問題，以及模型對來源標籤分佈的偏差以及類別不平衡。基於這些見解，我們引入了 AdapTable，這是一種新穎的表格測試時適應方法，它通過估計目標標籤分佈並根據校準的不確定性調整初始概率，直接修改輸出概率。在自然分佈轉移和合成損壞上的大量實驗證明了所提出方法的適應效能。

##### **MSegRNN:Enhanced SegRNN Model with Mamba for Long-Term Time Series Forecasting**
2407.10768v1 by GaoXiang Zhao, XiaoQiang Wang

The field of long-term time series forecasting demands handling extensive
look-back windows and long-range prediction steps, posing significant
challenges for RNN-based methodologies. Among these, SegRNN, a robust
RNN-driven model, has gained considerable attention in LTSF analysis for
achieving state-of-the-art results while maintaining a remarkably streamlined
architecture. Concurrently, the Mamba structure has demonstrated its advantages
in small to medium-sized models due to its capability for information
selection. This study introduces a variant of SegRNN that preprocesses
information using a fine-tuned single-layer Mamba structure. Additionally, it
incorporates implicit segmentation and residual structures into the model's
encoding section to further reduce the inherent data iterative cycles of RNN
architectures and implicitly integrate inter-channel correlations. This
variant, named MSegRNN, utilizes the Mamba structure to select useful
information, resulting in a transformed sequence. The linear-strategy-adapted
derivative retains the superior memory efficiency of the original SegRNN while
demonstrating enhanced performance. Empirical evaluations on real-world LTSF
datasets demonstrate the superior performance of our model, thereby
contributing to the advancement of LTSF methodologies.

摘要：長期時間序列預測領域需要處理廣泛的回顧窗口和長程預測步驟，為基於 RNN 的方法論帶來了重大挑戰。其中，SegRNN 是一個強大的 RNN 驅動模型，在 LTSF 分析中獲得了相當大的關注，因為它在保持非常簡化的架構的同時實現了最先進的結果。同時，Mamba 結構由於其信息選擇能力而在中小型模型中展示了其優勢。本研究引入了一個 SegRNN 變體，它使用微調的單層 Mamba 結構預處理信息。此外，它將隱式分段和殘差結構整合到模型的編碼部分，以進一步減少 RNN 架構的固有數據迭代週期，並隱式整合通道間關聯。這個變體，名為 MSegRNN，利用 Mamba 結構選擇有用的信息，從而產生轉換後的序列。線性策略適應的導數保留了原始 SegRNN 的優越內存效率，同時展示了增強的性能。對真實世界 LTSF 數據集的實證評估證明了我們模型的優越性能，從而推動了 LTSF 方法論的進步。

##### **Qwen2-Audio Technical Report**
2407.10759v1 by Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou

We introduce the latest progress of Qwen-Audio, a large-scale audio-language
model called Qwen2-Audio, which is capable of accepting various audio signal
inputs and performing audio analysis or direct textual responses with regard to
speech instructions. In contrast to complex hierarchical tags, we have
simplified the pre-training process by utilizing natural language prompts for
different data and tasks, and have further expanded the data volume. We have
boosted the instruction-following capability of Qwen2-Audio and implemented two
distinct audio interaction modes for voice chat and audio analysis. In the
voice chat mode, users can freely engage in voice interactions with Qwen2-Audio
without text input. In the audio analysis mode, users could provide audio and
text instructions for analysis during the interaction. Note that we do not use
any system prompts to switch between voice chat and audio analysis modes.
Qwen2-Audio is capable of intelligently comprehending the content within audio
and following voice commands to respond appropriately. For instance, in an
audio segment that simultaneously contains sounds, multi-speaker conversations,
and a voice command, Qwen2-Audio can directly understand the command and
provide an interpretation and response to the audio. Additionally, DPO has
optimized the model's performance in terms of factuality and adherence to
desired behavior. According to the evaluation results from AIR-Bench,
Qwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests
focused on audio-centric instruction-following capabilities. Qwen2-Audio is
open-sourced with the aim of fostering the advancement of the multi-modal
language community.

摘要：<paragraph>我們介紹 Qwen-Audio 的最新進展，Qwen-Audio 是一個名為 Qwen2-Audio 的大型音訊語言模型，它能夠接受各種音訊訊號輸入，並針對語音指令執行音訊分析或直接文字回應。與複雜的分層標籤不同，我們透過使用針對不同資料和任務的自然語言提示來簡化預訓練流程，並進一步擴充資料量。我們提升了 Qwen2-Audio 的指令遵循能力，並實作了兩種不同的音訊互動模式，分別是語音聊天和音訊分析。在語音聊天模式中，使用者可以自由地與 Qwen2-Audio 進行語音互動，而不需要文字輸入。在音訊分析模式中，使用者可以在互動過程中提供音訊和文字指令進行分析。請注意，我們不使用任何系統提示在語音聊天和音訊分析模式之間進行切換。Qwen2-Audio 能夠智慧地理解音訊中的內容，並遵循語音指令適當地回應。例如，在同時包含聲音、多位講者對話和語音指令的音訊片段中，Qwen2-Audio 能夠直接理解指令，並對音訊提供詮釋和回應。此外，DPO 優化了模型在事實性和遵循預期行為方面的效能。根據 AIR-Bench 的評估結果，Qwen2-Audio 在專注於以音訊為中心的指令遵循能力的測試中，表現優於先前的 SOTA，例如 Gemini-1.5-pro。Qwen2-Audio 是開放原始碼的，目的是為了促進多模態語言社群的進步。</paragraph>

##### **Codebook LLMs: Adapting Political Science Codebooks for LLM Use and Adapting LLMs to Follow Codebooks**
2407.10747v1 by Andrew Halterman, Katherine A. Keith

Codebooks -- documents that operationalize constructs and outline annotation
procedures -- are used almost universally by social scientists when coding
unstructured political texts. Recently, to reduce manual annotation costs,
political scientists have looked to generative large language models (LLMs) to
label and analyze text data. However, previous work using LLMs for
classification has implicitly relied on the universal label assumption --
correct classification of documents is possible using only a class label or
minimal definition and the information that the LLM inductively learns during
its pre-training. In contrast, we argue that political scientists who care
about valid measurement should instead make a codebook-construct label
assumption -- an LLM should follow the definition and exclusion criteria of a
construct/label provided in a codebook. In this work, we collect and curate
three political science datasets and their original codebooks and conduct a set
of experiments to understand whether LLMs comply with codebook instructions,
whether rewriting codebooks improves performance, and whether
instruction-tuning LLMs on codebook-document-label tuples improves performance
over zero-shot classification. Using Mistral 7B Instruct as our LLM, we find
re-structuring the original codebooks gives modest gains in zero-shot
performance but the model still struggles to comply with the constraints of the
codebooks. Optimistically, instruction-tuning Mistral on one of our datasets
gives significant gains over zero-shot inference (0.76 versus 0.53 micro F1).
We hope our conceptualization of the codebook-specific task, assumptions, and
instruction-tuning pipeline as well our semi-structured LLM codebook format
will help political scientists readily adapt to the LLM era.

摘要：<paragraph>編碼手冊——將結構化概念具體化並概述標註程序的文件——在社會科學家對非結構化政治文本進行編碼時幾乎普遍使用。最近，為了降低手動標註成本，政治學家開始關注生成式大型語言模型 (LLM)，以標記和分析文本數據。然而，先前使用 LLM 進行分類的工作隱含地依賴於通用標籤假設——僅使用類別標籤或最小定義以及 LLM 在預訓練期間歸納學習的信息即可正確對文件進行分類。相比之下，我們認為重視有效測量的政治學家應該改為做出編碼手冊-結構標籤假設——LLM 應遵循編碼手冊中提供的結構/標籤的定義和排除標準。在這項工作中，我們收集並整理了三個政治科學數據集及其原始編碼手冊，並進行了一系列實驗，以了解 LLM 是否符合編碼手冊說明、重寫編碼手冊是否能提升性能，以及在編碼手冊-文件-標籤元組上對 LLM 進行指令微調是否能提升零次分類的性能。使用 Mistral 7B Instruct 作為我們的 LLM，我們發現重新架構原始編碼手冊在零次分類性能方面帶來了適度的提升，但模型仍然難以符合編碼手冊的約束。樂觀地說，在我們的一個數據集上對 Mistral 進行指令微調在零次推論上帶來了顯著的提升（0.76 對比 0.53 微 F1）。我們希望我們對編碼手冊特定任務、假設和指令微調管線的概念化以及我們半結構化的 LLM 編碼手冊格式將幫助政治學家輕易適應 LLM 時代。</paragraph>

##### **What distinguishes conspiracy from critical narratives? A computational analysis of oppositional discourse**
2407.10745v1 by Damir Korenčić, Berta Chulvi, Xavier Bonet Casals, Alejandro Toselli, Mariona Taulé, Paolo Rosso

The current prevalence of conspiracy theories on the internet is a
significant issue, tackled by many computational approaches. However, these
approaches fail to recognize the relevance of distinguishing between texts
which contain a conspiracy theory and texts which are simply critical and
oppose mainstream narratives. Furthermore, little attention is usually paid to
the role of inter-group conflict in oppositional narratives. We contribute by
proposing a novel topic-agnostic annotation scheme that differentiates between
conspiracies and critical texts, and that defines span-level categories of
inter-group conflict. We also contribute with the multilingual
XAI-DisInfodemics corpus (English and Spanish), which contains a high-quality
annotation of Telegram messages related to COVID-19 (5,000 messages per
language). We also demonstrate the feasibility of an NLP-based automatization
by performing a range of experiments that yield strong baseline solutions.
Finally, we perform an analysis which demonstrates that the promotion of
intergroup conflict and the presence of violence and anger are key aspects to
distinguish between the two types of oppositional narratives, i.e., conspiracy
vs. critical.

摘要：網路上陰謀論盛行，是當前一個重要的議題，許多計算方法都試圖解決。然而，這些方法並未意識到區分包含陰謀論的文本與僅僅批判並反對主流敘事的文本之間的相關性。此外，通常很少關注群際衝突在對立敘事中所扮演的角色。我們提出了一個創新的與主題無關的註解方案，用來區分陰謀論和批判性文本，並定義群際衝突的跨距層級類別。我們還貢獻了多語言的 XAI-DisInfodemics 語料庫（英文和西班牙文），其中包含與 COVID-19 相關的高品質 Telegram 訊息註解（每種語言 5,000 則訊息）。我們也透過執行一系列實驗來展示基於 NLP 的自動化的可行性，這些實驗產生了強大的基準解。最後，我們執行了一項分析，證明了群際衝突的推廣以及暴力和憤怒的存在，是區分兩種對立敘事（即陰謀論與批判）的主要面向。

##### **Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs**
2407.10743v1 by W. J. Meijer, A. C. Kemmeren, E. H. J. Riemens, J. E. Fransman, M. van Bekkum, G. J. Burghouts, J. D. van Mil

This paper addresses the challenge of scaling Large Multimodal Models (LMMs)
to expansive 3D environments. Solving this open problem is especially relevant
for robot deployment in many first-responder scenarios, such as
search-and-rescue missions that cover vast spaces. The use of LMMs in these
settings is currently hampered by the strict context windows that limit the
LMM's input size. We therefore introduce a novel approach that utilizes a
datagraph structure, which allows the LMM to iteratively query smaller sections
of a large environment. Using the datagraph in conjunction with graph traversal
algorithms, we can prioritize the most relevant locations to the query, thereby
improving the scalability of 3D scene language tasks. We illustrate the
datagraph using 3D scenes, but these can be easily substituted by other dense
modalities that represent the environment, such as pointclouds or Gaussian
splats. We demonstrate the potential to use the datagraph for two 3D scene
language task use cases, in a search-and-rescue mission example.

摘要：本文討論了將大型多模態模型 (LMM) 擴展到廣闊 3D 環境的挑戰。解決這個開放性問題對於機器人在許多第一反應人員場景中的部署特別相關，例如涵蓋廣闊空間的搜救任務。這些設定中使用 LMM 目前受到嚴格的上下文視窗限制，這限制了 LMM 的輸入大小。因此，我們引入了一種新穎的方法，該方法利用資料圖結構，允許 LMM 迭代查詢大型環境的較小部分。透過將資料圖與圖形遍歷演算法結合使用，我們可以優先考慮與查詢最相關的位置，從而提高 3D 場景語言任務的可擴充性。我們使用 3D 場景說明資料圖，但這些場景可以輕鬆地由其他表示環境的密集模式取代，例如點雲或高斯點。我們展示了在搜救任務範例中使用資料圖進行兩個 3D 場景語言任務用例的潛力。

##### **Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models**
2407.10737v1 by Rining Wu, Feixiang Zhou, Ziwei Yin, Jian K. Liu

Our brains represent the ever-changing environment with neurons in a highly
dynamic fashion. The temporal features of visual pixels in dynamic natural
scenes are entrapped in the neuronal responses of the retina. It is crucial to
establish the intrinsic temporal relationship between visual pixels and
neuronal responses. Recent foundation vision models have paved an advanced way
of understanding image pixels. Yet, neuronal coding in the brain largely lacks
a deep understanding of its alignment with pixels. Most previous studies employ
static images or artificial videos derived from static images for emulating
more real and complicated stimuli. Despite these simple scenarios effectively
help to separate key factors influencing visual coding, complex temporal
relationships receive no consideration. To decompose the temporal features of
visual coding in natural scenes, here we propose Vi-ST, a spatiotemporal
convolutional neural network fed with a self-supervised Vision Transformer
(ViT) prior, aimed at unraveling the temporal-based encoding patterns of
retinal neuronal populations. The model demonstrates robust predictive
performance in generalization tests. Furthermore, through detailed ablation
experiments, we demonstrate the significance of each temporal module.
Furthermore, we introduce a visual coding evaluation metric designed to
integrate temporal considerations and compare the impact of different numbers
of neuronal populations on complementary coding. In conclusion, our proposed
Vi-ST demonstrates a novel modeling framework for neuronal coding of dynamic
visual scenes in the brain, effectively aligning our brain representation of
video with neuronal activity. The code is available at
https://github.com/wurining/Vi-ST.

摘要：<paragraph>我們的大腦以高度動態的方式，用神經元表示瞬息萬變的環境。動態自然場景中視覺像素的時間特徵被困在視網膜的神經元反應中。建立視覺像素和神經元反應之間的內在時間關係至關重要。最近的基礎視覺模型為理解影像像素鋪平了一條先進的道路。然而，大腦中的神經元編碼在很大程度上缺乏對其與像素對齊的深入理解。大多數先前的研究採用靜態影像或源自靜態影像的人工影片，以模擬更真實且複雜的刺激。儘管這些簡單的場景有效地幫助分離影響視覺編碼的關鍵因素，但複雜的時間關係卻未受到考慮。為了分解自然場景中視覺編碼的時間特徵，我們在此提出 Vi-ST，一種時空卷積神經網路，由自監督視覺轉換器 (ViT) 先驗提供，旨在解開視網膜神經元群體的基於時間的編碼模式。該模型在泛化測試中展現出穩健的預測效能。此外，透過詳細的消融實驗，我們證明了每個時間模組的重要性。此外，我們引入了一個視覺編碼評估指標，旨在整合時間考量，並比較不同數量的神經元群體對互補編碼的影響。總之，我們提出的 Vi-ST 展示了一個新穎的建模架構，用於大腦中動態視覺場景的神經元編碼，有效地將我們大腦對影片的表徵與神經元活動對齊。程式碼可在 https://github.com/wurining/Vi-ST 取得。</paragraph>

##### **Transforming Agency. On the mode of existence of Large Language Models**
2407.10735v2 by Xabier E. Barandiaran, Lola S. Almendros

This paper investigates the ontological characterization of Large Language
Models (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we
pay special attention to their status as agents. This requires explaining in
detail the architecture, processing, and training procedures that enable LLMs
to display their capacities, and the extensions used to turn LLMs into
agent-like systems. After a systematic analysis we conclude that a LLM fails to
meet necessary and sufficient conditions for autonomous agency in the light of
embodied theories of mind: the individuality condition (it is not the product
of its own activity, it is not even directly affected by it), the normativity
condition (it does not generate its own norms or goals), and, partially the
interactional asymmetry condition (it is not the origin and sustained source of
its interaction with the environment). If not agents, then ... what are LLMs?
We argue that ChatGPT should be characterized as an interlocutor or linguistic
automaton, a library-that-talks, devoid of (autonomous) agency, but capable to
engage performatively on non-purposeful yet purpose-structured and
purpose-bounded tasks. When interacting with humans, a "ghostly" component of
the human-machine interaction makes it possible to enact genuine conversational
experiences with LLMs. Despite their lack of sensorimotor and biological
embodiment, LLMs textual embodiment (the training corpus) and resource-hungry
computational embodiment, significantly transform existing forms of human
agency. Beyond assisted and extended agency, the LLM-human coupling can produce
midtended forms of agency, closer to the production of intentional agency than
to the extended instrumentality of any previous technologies.

摘要：<paragraph>本文探討大型語言模型 (LLM)（例如 ChatGPT）的本體論特徵。在膨脹性和緊縮性描述之間，我們特別關注它們作為代理的身分。這需要詳細說明 LLM 能夠展現其能力的架構、處理和訓練程序，以及用於將 LLM 轉變為類似代理系統的擴充功能。經過系統分析後，我們得出結論：根據具身心靈理論，LLM 無法滿足自主代理的必要和充分條件：個別性條件（它不是其自身活動的產物，甚至不會直接受到其影響）、規範性條件（它不會產生自己的規範或目標），以及部分互動不對稱條件（它不是其與環境互動的起源和持續來源）。如果不是代理，那麼 LLM 是什麼？我們認為 ChatGPT 應被描述為對話者或語言自動機，一個會說話的圖書館，沒有（自主）代理，但能夠在非目的性但有目的結構和目的界限的任務中表演性地參與。在與人類互動時，人機互動的「幽靈」組成部分使得與 LLM 進行真正的對話體驗成為可能。儘管缺乏感官運動和生物具身性，但 LLM 的文字具身性（訓練語料庫）和資源密集型計算具身性顯著地轉變了現有的人類代理形式。除了輔助和擴展代理之外，LLM-人類結合還可以產生中介形式的代理，更接近於意向性代理的產生，而不是任何先前技術的擴展工具性。</paragraph>

##### **When Synthetic Traces Hide Real Content: Analysis of Stable Diffusion Image Laundering**
2407.10736v1 by Sara Mandelli, Paolo Bestagini, Stefano Tubaro

In recent years, methods for producing highly realistic synthetic images have
significantly advanced, allowing the creation of high-quality images from text
prompts that describe the desired content. Even more impressively, Stable
Diffusion (SD) models now provide users with the option of creating synthetic
images in an image-to-image translation fashion, modifying images in the latent
space of advanced autoencoders. This striking evolution, however, brings an
alarming consequence: it is possible to pass an image through SD autoencoders
to reproduce a synthetic copy of the image with high realism and almost no
visual artifacts. This process, known as SD image laundering, can transform
real images into lookalike synthetic ones and risks complicating forensic
analysis for content authenticity verification. Our paper investigates the
forensic implications of image laundering, revealing a serious potential to
obscure traces of real content, including sensitive and harmful materials that
could be mistakenly classified as synthetic, thereby undermining the protection
of individuals depicted. To address this issue, we propose a two-stage
detection pipeline that effectively differentiates between pristine, laundered,
and fully synthetic images (those generated from text prompts), showing
robustness across various conditions. Finally, we highlight another alarming
property of image laundering, which appears to mask the unique artifacts
exploited by forensic detectors to solve the camera model identification task,
strongly undermining their performance. Our experimental code is available at
https://github.com/polimi-ispl/synthetic-image-detection.

摘要：近年来，生成高度逼真的合成图像的方法已取得显著进展，允许从描述所需内容的文本提示创建高质量图像。更令人印象深刻的是，Stable Diffusion (SD) 模型现在为用户提供了以图像到图像翻译方式创建合成图像的选项，修改高级自动编码器潜在空间中的图像。然而，这种引人注目的演变带来了一个令人担忧的后果：有可能将图像通过 SD 自动编码器传递，以高逼真度和几乎没有视觉伪像复制图像的合成副本。这个称为 SD 图像洗涤的过程可以将真实图像转换成相似的合成图像，并有使内容真实性验证的取证分析复杂化的风险。我们的论文调查了图像洗涤的取证意义，揭示了模糊真实内容痕迹的严重潜在可能性，包括可能被错误归类为合成的敏感和有害材料，从而破坏了所描绘个人的保护。为了解决这个问题，我们提出了一种两阶段检测管道，它有效地区分了原始、洗涤和完全合成的图像（那些从文本提示生成的图像），显示了在各种条件下的鲁棒性。最后，我们强调了图像洗涤的另一个令人担忧的特性，它似乎掩盖了取证检测器利用的独特伪像，以解决相机模型识别任务，极大地削弱了它们的性能。我们的实验代码可在 https://github.com/polimi-ispl/synthetic-image-detection 获得。

##### **On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers**
2407.10734v1 by Mark Deutel, Frank Hannig, Christopher Mutschler, Jürgen Teich

On-device training of DNNs allows models to adapt and fine-tune to newly
collected data or changing domains while deployed on microcontroller units
(MCUs). However, DNN training is a resource-intensive task, making the
implementation and execution of DNN training algorithms on MCUs challenging due
to low processor speeds, constrained throughput, limited floating-point
support, and memory constraints. In this work, we explore on-device training of
DNNs for Cortex-M MCUs. We present a method that enables efficient training of
DNNs completely in place on the MCU using fully quantized training (FQT) and
dynamic partial gradient updates. We demonstrate the feasibility of our
approach on multiple vision and time-series datasets and provide insights into
the tradeoff between training accuracy, memory overhead, energy, and latency on
real hardware.

摘要：在裝置上訓練 DNN 可讓模型在部署於微控制器單元 (MCU) 時根據新收集的資料或變更的網域進行調整和微調。然而，DNN 訓練是一項資源密集的工作，由於處理器速度低、吞吐量受限、浮點支援有限和記憶體受限，因此在 MCU 上實作和執行 DNN 訓練演算法具有挑戰性。在這項工作中，我們探討了 Cortex-M MCU 的裝置上 DNN 訓練。我們提出了一種方法，可使用全量化訓練 (FQT) 和動態部分梯度更新在 MCU 上完全就地有效率地訓練 DNN。我們在多個影像和時間序列資料集上展示了我們方法的可行性，並深入探討了訓練準確度、記憶體開銷、能源和實際硬體上的延遲之間的權衡。

##### **CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses**
2407.10725v1 by Jing Yao, Xiaoyuan Yi, Xing Xie

The rapid progress in Large Language Models (LLMs) poses potential risks such
as generating unethical content. Assessing LLMs' values can help expose their
misalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or
close-source ones like GPT-4, to identify values reflected in generated
responses. Nevertheless, these evaluators face two challenges in open-ended
value evaluation: they should align with changing human value definitions with
minimal annotation, against their own bias (adaptability), and detect varying
value expressions and scenarios robustly (generalizability). To handle these
challenges, we introduce CLAVE, a novel framework which integrates two
complementary LLMs, a large one to extract high-level value concepts from a few
human labels, leveraging its extensive knowledge and generalizability, and a
smaller one fine-tuned on such concepts to better align with human value
understanding. This dual-model approach enables calibration with any value
systems using <100 human-labeled samples per value type. Then we present
ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples
across diverse domains, covering three major value systems. We benchmark the
capabilities of 12+ popular LLM evaluators and analyze their strengths and
weaknesses. Our findings reveal that combining fine-tuned small models and
prompt-based large ones serves as a superior balance in value evaluation.

摘要：大型語言模型 (LLM) 的快速進展帶來潛在風險，例如產生不道德的內容。評估 LLM 的價值觀有助於揭露其錯位，但依賴於無參考評估器，例如微調 LLM 或像 GPT-4 這樣的封閉源碼，以識別生成回應中反映的價值觀。儘管如此，這些評估器在開放式價值評估中面臨兩項挑戰：它們應在最少註解的情況下與不斷變化的價值觀定義保持一致，並針對其自身偏差（適應性），並穩健地檢測不同的價值表達和場景（概括性）。為了應對這些挑戰，我們引入了 CLAVE，一個新穎的框架，它整合了兩個互補的 LLM，一個大型 LLM 從一些人類標籤中提取高級價值概念，利用其廣泛的知識和概括性，以及一個針對這些概念進行微調的較小 LLM，以更好地與人類價值觀理解保持一致。這種雙模型方法能夠使用每種類型價值小於 100 個人標籤樣本，對任何價值系統進行校準。然後我們展示了 ValEval，一個全面的數據集，包含了跨越不同領域的 13k+（文本、價值、標籤）元組，涵蓋了三個主要的價值系統。我們對 12+ 個流行的 LLM 評估器的能力進行了基準測試，並分析了它們的優缺點。我們的研究結果表明，將微調的小模型和基於提示的大模型相結合，在價值評估中達到了更好的平衡。

##### **Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning**
2407.10718v2 by Yulong Wang, Tianhao Shen, Lifeng Liu, Jian Xie

Existing agents based on large language models (LLMs) demonstrate robust
problem-solving capabilities by integrating LLMs' inherent knowledge, strong
in-context learning and zero-shot capabilities, and the use of tools combined
with intricately designed LLM invocation workflows by humans. However, these
agents still exhibit shortcomings in long-term reasoning and under-use the
potential of existing tools, leading to noticeable deficiencies in complex
real-world reasoning scenarios. To address these limitations, we introduce
Sibyl, a simple yet powerful LLM-based agent framework designed to tackle
complex reasoning tasks by efficiently leveraging a minimal set of tools.
Drawing inspiration from Global Workspace Theory, Sibyl incorporates a global
workspace to enhance the management and sharing of knowledge and conversation
history throughout the system. Furthermore, guided by Society of Mind Theory,
Sibyl implements a multi-agent debate-based jury to self-refine the final
answers, ensuring a comprehensive and balanced approach. This approach aims to
reduce system complexity while expanding the scope of problems solvable-from
matters typically resolved by humans in minutes to those requiring hours or
even days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl
has been designed with a focus on scalability and ease of debugging by
incorporating the concept of reentrancy from functional programming from its
inception, with the aim of seamless and low effort integration in other LLM
applications to improve capabilities. Our experimental results on the GAIA
benchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves
state-of-the-art performance with an average score of 34.55%, compared to other
agents based on GPT-4. We hope that Sibyl can inspire more reliable and
reusable LLM-based agent solutions to address complex real-world reasoning
tasks.

摘要：現有的基於大型語言模型 (LLM) 的代理展示了強健的問題解決能力，方法是整合 LLM 固有的知識、強大的情境學習和零次學習能力，以及人類結合複雜設計的 LLM 呼叫工作流程來使用工具。然而，這些代理在長期推理方面仍然表現出不足，並且沒有充分利用現有工具的潛力，導致在複雜的現實世界推理場景中出現明顯的缺陷。為了解決這些限制，我們引入了 Sibyl，一個簡單但強大的基於 LLM 的代理框架，旨在通過有效利用最少的工具來解決複雜的推理任務。從全球工作空間理論中汲取靈感，Sibyl 整合了一個全球工作空間，以增強知識和對話歷史記錄在整個系統中的管理和共享。此外，在心靈社會理論的指導下，Sibyl 實施了一個基於多代理辯論的陪審團，以自我完善最終答案，確保全面且平衡的方法。這種方法旨在降低系統複雜性，同時擴大可解決問題的範圍——從人類通常在幾分鐘內解決的問題到需要數小時甚至數天才能解決的問題，從而促進從系統 1 思維向系統 2 思維的轉變。Sibyl 在設計時注重可擴展性和易於調試，從一開始就融入了函數式程式設計中的可重入性概念，目的是在其他 LLM 應用程式中無縫且低成本地整合，以提高能力。我們在 GAIA 基準測試集上的實驗結果表明，使用 GPT-4 實例化的 Sibyl 代理實現了最先進的性能，平均得分為 34.55%，而其他基於 GPT-4 的代理則為 27.61%。我們希望 Sibyl 能夠激發更多可靠且可重複使用的基於 LLM 的代理解決方案，以解決複雜的現實世界推理任務。

##### **SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation**
2407.10714v1 by Kaiming Shen, Xichen Ding, Zixiang Zheng, Yuqi Gong, Qianqian Li, Zhongyi Liu, Guannan Zhang

The modeling of users' behaviors is crucial in modern recommendation systems.
A lot of research focuses on modeling users' lifelong sequences, which can be
extremely long and sometimes exceed thousands of items. These models use the
target item to search for the most relevant items from the historical sequence.
However, training lifelong sequences in click through rate (CTR) prediction or
personalized search ranking (PSR) is extremely difficult due to the
insufficient learning problem of ID embedding, especially when the IDs in the
lifelong sequence features do not exist in the samples of training dataset.
Additionally, existing target attention mechanisms struggle to learn the
multi-modal representations of items in the sequence well. The distribution of
multi-modal embedding (text, image and attributes) output of user's interacted
items are not properly aligned and there exist divergence across modalities. We
also observe that users' search query sequences and item browsing sequences can
fully depict users' intents and benefit from each other. To address these
challenges, we propose a unified lifelong multi-modal sequence model called
SEMINAR-Search Enhanced Multi-Modal Interest Network and Approximate Retrieval.
Specifically, a network called Pretraining Search Unit (PSU) learns the
lifelong sequences of multi-modal query-item pairs in a pretraining-finetuning
manner with multiple objectives: multi-modal alignment, next query-item pair
prediction, query-item relevance prediction, etc. After pretraining, the
downstream model restores the pretrained embedding as initialization and
finetunes the network. To accelerate the online retrieval speed of multi-modal
embedding, we propose a multi-modal codebook-based product quantization
strategy to approximate the exact attention calculati

摘要：在現代推薦系統中，對使用者行為建模至關重要。許多研究專注於建模使用者的終身序列，這些序列可能極長，有時會超過數千個項目。這些模型使用目標項目從歷史序列中搜尋最相關的項目。然而，由於 ID 內嵌的學習問題不足，在點擊率 (CTR) 預測或個人化搜尋排名 (PSR) 中訓練終身序列極為困難，特別是當終身序列特徵中的 ID 不存在於訓練資料集的樣本中時。此外，現有的目標注意力機制難以很好地學習序列中項目的多模態表示。使用者的互動項目輸出之多模態內嵌（文字、影像和屬性）的分布沒有適當地對齊，而且不同模態之間存在差異。我們也觀察到使用者的搜尋查詢序列和項目瀏覽序列可以充分描述使用者的意圖，並相互受益。為了應對這些挑戰，我們提出了一個統一的終身多模態序列模型，稱為 SEMINAR（搜尋增強多模態興趣網路和近似檢索）。具體來說，一個稱為預訓練搜尋單元 (PSU) 的網路以預訓練微調的方式學習多模態查詢項目對的終身序列，並有多個目標：多模態對齊、下一個查詢項目對預測、查詢項目相關性預測等。在預訓練後，下游模型將預訓練的內嵌還原為初始化，並微調網路。為了加速多模態內嵌的線上檢索速度，我們提出了一個基於多模態碼簿的產品量化策略，以近似精確的注意力計算。

##### **DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems**
2407.10701v1 by Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, Dong Yu

Recently, there has been a growing interest among large language model (LLM)
developers in LLM-based document reading systems, which enable users to upload
their own documents and pose questions related to the document contents, going
beyond simple reading comprehension tasks. Consequently, these systems have
been carefully designed to tackle challenges such as file parsing, metadata
extraction, multi-modal information understanding and long-context reading.
However, no current benchmark exists to evaluate their performance in such
scenarios, where a raw file and questions are provided as input, and a
corresponding response is expected as output. In this paper, we introduce
DocBench, a new benchmark designed to evaluate LLM-based document reading
systems. Our benchmark involves a meticulously crafted process, including the
recruitment of human annotators and the generation of synthetic questions. It
includes 229 real documents and 1,102 questions, spanning across five different
domains and four major types of questions. We evaluate both proprietary
LLM-based systems accessible via web interfaces or APIs, and a parse-then-read
pipeline employing open-source LLMs. Our evaluations reveal noticeable gaps
between existing LLM-based document reading systems and human performance,
underscoring the challenges of developing proficient systems. To summarize,
DocBench aims to establish a standardized benchmark for evaluating LLM-based
document reading systems under diverse real-world scenarios, thereby guiding
future advancements in this research area.

摘要：<paragraph>最近，大型语言模型 (LLM) 开发人员对基于 LLM 的文档阅读系统越来越感兴趣，该系统使用户能够上传自己的文档并提出与文档内容相关的问题，超越简单的阅读理解任务。因此，这些系统经过精心设计，以应对诸如文件解析、元数据提取、多模式信息理解和长上下文阅读等挑战。然而，目前尚不存在基准来评估它们在这种情况下（其中原始文件和问题作为输入提供，并且预期相应的响应作为输出）的性能。在本文中，我们介绍了 DocBench，这是一个旨在评估基于 LLM 的文档阅读系统的新基准。我们的基准涉及一个精心制作的过程，包括招募人类注释员和生成合成问题。它包括 229 个真实文档和 1,102 个问题，跨越五个不同的领域和四种主要类型的​​问题。我们评估了可通过网络界面或 API 访问的专有基于 LLM 的系统以及采用开源 LLM 的解析然后阅读管道。我们的评估揭示了现有的基于 LLM 的文档阅读系统和人类表现之间的显着差距，强调了开发熟练系统的挑战。总之，DocBench 旨在为评估基于 LLM 的文档阅读系统在各种现实场景下的性能建立一个标准化基准，从而指导该研究领域的未来发展。</paragraph>

##### **$\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity**
2407.10691v1 by Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl

Recent studies show the growing significance of document retrieval in the
generation of LLMs, i.e., RAG, within the scientific domain by bridging their
knowledge gap. However, dense retrievers often struggle with domain-specific
retrieval and complex query-document relationships, particularly when query
segments correspond to various parts of a document. To alleviate such prevalent
challenges, this paper introduces $\texttt{MixGR}$, which improves dense
retrievers' awareness of query-document matching across various levels of
granularity in queries and documents using a zero-shot approach.
$\texttt{MixGR}$ fuses various metrics based on these granularities to a united
score that reflects a comprehensive query-document similarity. Our experiments
demonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by
24.7% and 9.8% on nDCG@5 with unsupervised and supervised retrievers,
respectively, averaged on queries containing multiple subqueries from five
scientific retrieval datasets. Moreover, the efficacy of two downstream
scientific question-answering tasks highlights the advantage of
$\texttt{MixGR}$to boost the application of LLMs in the scientific domain.

摘要：最近的研究顯示文件檢索在科學領域中生成 LLM（例如 RAG）的重要性日益提升，原因在於文件檢索可彌補 LLM 的知識差距。然而，密集檢索器經常難以應對特定領域的檢索和複雜的查詢文件關係，特別是在查詢區段對應於文件不同部分時。為了減輕此類普遍挑戰，本文介紹了 $\texttt{MixGR}$，它使用零次學習方法來提升密集檢索器對查詢文件配對的認識，並涵蓋查詢和文件中的各種粒度層級。$\texttt{MixGR}$ 將基於這些粒度的各種指標融合成一個統一分數，以反映全面的查詢文件相似度。我們的實驗證明，在包含五個科學檢索資料集的複數子查詢的查詢中，$\texttt{MixGR}$ 分別比非監督式和監督式檢索器在 nDCG@5 上高出 24.7% 和 9.8%。此外，兩個下游科學問題解答任務的效能突顯了 $\texttt{MixGR}$ 在提升科學領域中 LLM 應用方面的優勢。

##### **Classification of Heart Sounds Using Multi-Branch Deep Convolutional Network and LSTM-CNN**
2407.10689v1 by Seyed Amir Latifi, Hassan Ghassemian, Maryam Imani

This paper presents a fast and cost-effective method for diagnosing cardiac
abnormalities with high accuracy and reliability using low-cost systems in
clinics. The primary limitation of automatic diagnosing of cardiac diseases is
the rarity of correct and acceptable labeled samples, which can be expensive to
prepare. To address this issue, two methods are proposed in this work. The
first method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN)
architecture inspired by human auditory processing, specifically designed to
optimize feature extraction by employing various sizes of convolutional filters
and audio signal power spectrum as input. In the second method, called as Long
short-term memory-Convolutional Neural (LSCN) model, Additionally, the network
architecture includes Long Short-Term Memory (LSTM) network blocks to improve
feature extraction in the time domain. The innovative approach of combining
multiple parallel branches consisting of the one-dimensional convolutional
layers along with LSTM blocks helps in achieving superior results in audio
signal processing tasks. The experimental results demonstrate superiority of
the proposed methods over the state-of-the-art techniques. The overall
classification accuracy of heart sounds with the LSCN network is more than 96%.
The efficiency of this network is significant compared to common feature
extraction methods such as Mel Frequency Cepstral Coefficients (MFCC) and
wavelet transform. Therefore, the proposed method shows promising results in
the automatic analysis of heart sounds and has potential applications in the
diagnosis and early detection of cardiovascular diseases.

摘要：本文提出了一種快速且經濟有效的方法，使用低成本的系統在診所診斷心臟異常，且具有高準確度和可靠性。自動診斷心臟疾病的主要限制是正確且可接受的標籤樣本稀少，而且準備起來可能很昂貴。為了解決這個問題，這項工作提出了兩種方法。第一種方法是一種獨特的多分支深度卷積神經網路 (MBDCN) 架構，靈感來自人類聽覺處理，特別設計為透過採用各種大小的卷積濾波器和音訊訊號功率譜作為輸入，來最佳化特徵提取。在第二種方法中，稱為長短期記憶 - 卷積神經 (LSCN) 模型，此外，網路架構包括長短期記憶 (LSTM) 網路區塊，以改善時域中的特徵提取。結合由一維卷積層和 LSTM 區塊組成的多個並行分支的創新方法，有助於在音訊訊號處理任務中達成優異的結果。實驗結果證明了所提出的方法優於最先進的技術。LSCN 網路對心音的整體分類準確度超過 96%。與常見的特徵提取方法（例如梅爾頻率倒譜係數 (MFCC) 和小波轉換）相比，此網路的效率顯著。因此，所提出的方法在心音的自動分析中顯示出有希望的結果，並且在心血管疾病的診斷和早期檢測中具有潛在應用。

##### **Addressing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval**
2407.10683v1 by Youngsun Lim, Hyunjung Shim

Text-to-image generation has shown remarkable progress with the emergence of
diffusion models. However, these models often generate factually inconsistent
images, failing to accurately reflect the factual information and common sense
conveyed by the input text prompts. We refer to this issue as Image
hallucination. Drawing from studies on hallucinations in language models, we
classify this problem into three types and propose a methodology that uses
factual images retrieved from external sources to generate realistic images.
Depending on the nature of the hallucination, we employ off-the-shelf image
editing tools, either InstructPix2Pix or IP-Adapter, to leverage factual
information from the retrieved image. This approach enables the generation of
images that accurately reflect the facts and common sense.

摘要：文本到图像生成随着扩散模型的出现而取得了显着的进展。然而，这些模型经常生成事实不一致的图像，未能准确反映输入文本提示所传达的事实信息和常识。我们称这个问题为图像幻觉。借鉴语言模型中关于幻觉的研究，我们将这个问题归类为三种类型，并提出一种使用从外部来源检索的事实图像来生成真实图像的方法。根据幻觉的性质，我们采用现成的图像编辑工具，InstructPix2Pix 或 IP-Adapter，从检索到的图像中利用事实信息。这种方法能够生成准确反映事实和常识的图像。

##### **Qwen2 Technical Report**
2407.10671v1 by An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, Zhihao Fan

This report introduces the Qwen2 series, the latest addition to our large
language models and large multimodal models. We release a comprehensive suite
of foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base
language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1
on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2
demonstrates robust multilingual capabilities, proficient in approximately 30
languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,
Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and
global reach.
  To foster community innovation and accessibility, we have made the Qwen2
model weights openly available on Hugging Face1 and ModelScope2, and the
supplementary materials including example code on GitHub3. These platforms also
include resources for quantization, fine-tuning, and deployment, facilitating a
wide range of applications and research endeavors.

摘要：<paragraph>此報告介紹了 Qwen2 系列，這是我們大型語言模型和大規模多模態模型的最新成員。我們發布了一套全面的基礎和指令調整語言模型，包含從 0.5 到 720 億的參數範圍，具有密集模型和專家混合模型。Qwen2 超越了大多數先前的開放權重模型，包括其前身 Qwen1.5，並在語言理解、生成、多語言能力、編碼、數學和推理等不同基準上展現出與專有模型相比具有競爭力的效能。
旗艦模型 Qwen2-72B 展示了非凡的效能：作為基礎語言模型，在 MMLU 上為 84.2，在 GPQA 上為 37.9，在 HumanEval 上為 64.6，在 GSM8K 上為 89.5，在 BBH 上為 82.4。指令調整變體 Qwen2-72B-Instruct 在 MT-Bench 上達到 9.1，在 Arena-Hard 上達到 48.1，在 LiveCodeBench 上達到 35.7。此外，Qwen2 展示了強大的多語言能力，精通約 30 種語言，涵蓋英語、中文、西班牙語、法語、德語、阿拉伯語、俄語、韓語、日語、泰語、越南語等，強調了它的多功能性和全球影響力。
為了促進社群創新和可及性，我們已在 Hugging Face1 和 ModelScope2 上公開了 Qwen2 模型權重，以及在 GitHub3 上包含範例程式碼的補充資料。這些平台還包括量化、微調和部署的資源，促進了廣泛的應用和研究工作。</paragraph>

##### **Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems**
2407.10670v1 by Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, Min Xu

Retrieval-augmented generation (RAG) techniques leverage the in-context
learning capabilities of large language models (LLMs) to produce more accurate
and relevant responses. Originating from the simple 'retrieve-then-read'
approach, the RAG framework has evolved into a highly flexible and modular
paradigm. A critical component, the Query Rewriter module, enhances knowledge
retrieval by generating a search-friendly query. This method aligns input
questions more closely with the knowledge base. Our research identifies
opportunities to enhance the Query Rewriter module to Query Rewriter+ by
generating multiple queries to overcome the Information Plateaus associated
with a single query and by rewriting questions to eliminate Ambiguity, thereby
clarifying the underlying intent. We also find that current RAG systems exhibit
issues with Irrelevant Knowledge; to overcome this, we propose the Knowledge
Filter. These two modules are both based on the instruction-tuned Gemma-2B
model, which together enhance response quality. The final identified issue is
Redundant Retrieval; we introduce the Memory Knowledge Reservoir and the
Retriever Trigger to solve this. The former supports the dynamic expansion of
the RAG system's knowledge base in a parameter-free manner, while the latter
optimizes the cost for accessing external knowledge, thereby improving resource
utilization and response efficiency. These four RAG modules synergistically
improve the response quality and efficiency of the RAG system. The
effectiveness of these modules has been validated through experiments and
ablation studies across six common QA datasets. The source code can be accessed
at https://github.com/Ancientshi/ERM4.

摘要：擷取增強式生成 (RAG) 技術利用大型語言模型 (LLM) 的語境學習能力，產生更精確且相關的回應。RAG 架構源自簡單的「先擷取再讀取」方法，現已演變成高度彈性和模組化的範例。關鍵元件「查詢改寫器」模組透過產生搜尋友善的查詢來增強知識擷取。此方法讓輸入問題與知識庫更緊密地結合。我們的研究找出機會，透過產生多個查詢來增強「查詢改寫器」模組，成為「查詢改寫器+」，以克服與單一查詢相關的資訊高原，並透過改寫問題來消除歧義，進而釐清底層意圖。我們也發現目前的 RAG 系統會出現無關知識的問題；為了克服這個問題，我們提出「知識過濾器」。這兩個模組都基於指令調整的 Gemma-2B 模型，它們共同增強回應品質。最後一個已識別的問題是重複擷取；我們引入「記憶知識儲存庫」和「擷取器觸發器」來解決這個問題。前者以無參數的方式支援 RAG 系統知識庫的動態擴展，而後者最佳化存取外部知識的成本，進而改善資源利用和回應效率。這四個 RAG 模組能協同改善 RAG 系統的回應品質和效率。這些模組的效能已透過六個常見的問答資料集的實驗和消融研究驗證。原始程式碼可於 https://github.com/Ancientshi/ERM4 取得。

##### **Spatio-temporal neural distance fields for conditional generative modeling of the heart**
2407.10663v1 by Kristine Sørensen, Paula Diez, Jan Margeta, Yasmin El Youssef, Michael Pham, Jonas Jalili Pedersen, Tobias Kühl, Ole de Backer, Klaus Kofoed, Oscar Camara, Rasmus Paulsen

The rhythmic pumping motion of the heart stands as a cornerstone in life, as
it circulates blood to the entire human body through a series of carefully
timed contractions of the individual chambers. Changes in the size, shape and
movement of the chambers can be important markers for cardiac disease and
modeling this in relation to clinical demography or disease is therefore of
interest. Existing methods for spatio-temporal modeling of the human heart
require shape correspondence over time or suffer from large memory
requirements, making it difficult to use for complex anatomies. We introduce a
novel conditional generative model, where the shape and movement is modeled
implicitly in the form of a spatio-temporal neural distance field and
conditioned on clinical demography. The model is based on an auto-decoder
architecture and aims to disentangle the individual variations from that
related to the clinical demography. It is tested on the left atrium (including
the left atrial appendage), where it outperforms current state-of-the-art
methods for anatomical sequence completion and generates synthetic sequences
that realistically mimics the shape and motion of the real left atrium. In
practice, this means we can infer functional measurements from a static image,
generate synthetic populations with specified demography or disease and
investigate how non-imaging clinical data effect the shape and motion of
cardiac anatomies.

摘要：心臟有節奏的跳動動作是生命中的基石，因為它透過一系列仔細計時的單獨心室收縮，將血液循環到整個身體。心室的大小、形狀和運動的變化可能是心臟疾病的重要標記，因此對此進行建模以關聯臨床人口統計或疾病，因此具有意義。現有的時空建模方法需要隨著時間推移進行形狀對應，或需要大量的記憶體需求，這使得難以用於複雜的解剖結構。我們引入了一個新穎的條件生成模型，其中形狀和運動以時空神經距離場的形式隱含建模，並根據臨床人口統計進行條件設定。該模型基於自動編碼器架構，旨在解開與臨床人口統計相關的個別變異。它在左心房（包括左心耳）上進行測試，在解剖序列完成方面優於當前最先進的方法，並生成逼真地模擬真實左心房形狀和運動的合成序列。實際上，這意味著我們可以從靜態影像推斷功能性測量，生成具有特定人口統計或疾病的合成族群，並調查非影像臨床資料如何影響心臟解剖結構的形狀和運動。

##### **An Empirical Study of Validating Synthetic Data for Formula Generation**
2407.10657v1 by Usneek Singh, José Cambronero, Sumit Gulwani, Aditya Kanade, Anirudh Khatry, Vu Le, Mukul Singh, Gust Verbruggen

Large language models (LLMs) can be leveraged to help with writing formulas
in spreadsheets, but resources on these formulas are scarce, impacting both the
base performance of pre-trained models and limiting the ability to fine-tune
them. Given a corpus of formulas, we can use a(nother) model to generate
synthetic natural language utterances for fine-tuning. However, it is important
to validate whether the NL generated by the LLM is indeed accurate to be
beneficial for fine-tuning. In this paper, we provide empirical results on the
impact of validating these synthetic training examples with surrogate
objectives that evaluate the accuracy of the synthetic annotations. We
demonstrate that validation improves performance over raw data across four
models (2 open and 2 closed weight). Interestingly, we show that although
validation tends to prune more challenging examples, it increases the
complexity of problems that models can solve after being fine-tuned on
validated data.

摘要：大型語言模型 (LLM) 可用於協助撰寫試算表的公式，但這些公式的資源稀少，影響預先訓練模型的基礎效能，並限制微調它們的能力。給定公式語料庫，我們可以使用（另一個）模型來產生用於微調的合成自然語言語句。然而，驗證 LLM 產生的 NL 是否準確對於微調是有益的非常重要。在本文中，我們提供關於驗證這些合成訓練範例的影響的經驗結果，其中使用替代目標評估合成註解的準確性。我們證明驗證在四個模型（2 個開放權重和 2 個封閉權重）上改善了原始資料的效能。有趣的是，我們表明，儘管驗證傾向於修剪更具挑戰性的範例，但它增加了在驗證資料上微調後模型可以解決的問題的複雜性。

##### **Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models**
2407.10645v1 by Louis Abraham, Charles Arnal, Antoine Marie

Large Language Models have recently been applied to text annotation tasks
from social sciences, equalling or surpassing the performance of human workers
at a fraction of the cost. However, no inquiry has yet been made on the impact
of prompt selection on labelling accuracy. In this study, we show that
performance greatly varies between prompts, and we apply the method of
automatic prompt optimization to systematically craft high quality prompts. We
also provide the community with a simple, browser-based implementation of the
method at https://prompt-ultra.github.io/ .

摘要：大型語言模型最近已應用於社會科學的文字註解任務，其效能等於或超越人類工作者，且成本僅為其一小部分。然而，目前尚未針對提示選擇對標記精確度的影響進行探討。在本研究中，我們顯示效能會在提示之間產生極大差異，且我們應用自動提示最佳化方法來系統性地製作高品質提示。我們也為社群提供一種簡單的基於瀏覽器的實作方法，網址為 https://prompt-ultra.github.io/。

##### **Bidirectional Stereo Image Compression with Cross-Dimensional Entropy Model**
2407.10632v1 by Zhening Liu, Xinjie Zhang, Jiawei Shao, Zehong Lin, Jun Zhang

With the rapid advancement of stereo vision technologies, stereo image
compression has emerged as a crucial field that continues to draw significant
attention. Previous approaches have primarily employed a unidirectional
paradigm, where the compression of one view is dependent on the other,
resulting in imbalanced compression. To address this issue, we introduce a
symmetric bidirectional stereo image compression architecture, named BiSIC.
Specifically, we propose a 3D convolution based codec backbone to capture local
features and incorporate bidirectional attention blocks to exploit global
features. Moreover, we design a novel cross-dimensional entropy model that
integrates various conditioning factors, including the spatial context, channel
context, and stereo dependency, to effectively estimate the distribution of
latent representations for entropy coding. Extensive experiments demonstrate
that our proposed BiSIC outperforms conventional image/video compression
standards, as well as state-of-the-art learning-based methods, in terms of both
PSNR and MS-SSIM.

摘要：隨著立體視覺技術的快速進步，立體影像壓縮已成為一個持續受到關注的重要領域。先前的作法主要採用單向範例，其中一個視圖的壓縮取決於另一個視圖，導致壓縮不平衡。為了解決這個問題，我們引入一個稱為 BiSIC 的對稱雙向立體影像壓縮架構。具體來說，我們提出一個基於 3D 捲積的編解碼器主幹來擷取局部特徵，並納入雙向注意力區塊來利用全局特徵。此外，我們設計了一個新穎的跨維度熵模型，它整合了各種條件因素，包括空間背景、通道背景和立體依賴性，以有效估計熵編碼的潛在表示分布。廣泛的實驗證明，我們提出的 BiSIC 在 PSNR 和 MS-SSIM 方面優於傳統的影像/視訊壓縮標準，以及最先進的基於學習的方法。

##### **Balancing the Scales: Reinforcement Learning for Fair Classification**
2407.10629v1 by Leon Eshuijs, Shihan Wang, Antske Fokkens

Fairness in classification tasks has traditionally focused on bias removal
from neural representations, but recent trends favor algorithmic methods that
embed fairness into the training process. These methods steer models towards
fair performance, preventing potential elimination of valuable information that
arises from representation manipulation. Reinforcement Learning (RL), with its
capacity for learning through interaction and adjusting reward functions to
encourage desired behaviors, emerges as a promising tool in this domain. In
this paper, we explore the usage of RL to address bias in imbalanced
classification by scaling the reward function to mitigate bias. We employ the
contextual multi-armed bandit framework and adapt three popular RL algorithms
to suit our objectives, demonstrating a novel approach to mitigating bias.

摘要：分類任務中的公平性傳統上專注於消除神經表徵中的偏差，但最近的趨勢偏好將公平性嵌入訓練過程中的演算法方法。這些方法引導模型朝向公平的效能，防止因表徵操作而消除有價值的資訊。強化學習 (RL) 具備透過互動學習和調整獎勵函數以鼓勵所需行為的能力，因此成為此領域中一個有前途的工具。在本文中，我們探討使用 RL 來解決不平衡分類中的偏差，方法是調整獎勵函數以減輕偏差。我們採用情境多重選擇賭博機架構，並調整三種流行的 RL 演算法以符合我們的目標，展示減輕偏差的新穎方法。

##### **Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena**
2407.10627v1 by Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yansong Tang, Weizhu Chen

Assessing the effectiveness of large language models (LLMs) presents
substantial challenges. The method of conducting human-annotated battles in an
online Chatbot Arena is a highly effective evaluative technique. However, this
approach is limited by the costs and time required for human annotation. In
this paper, we introduce Arena Learning, an innovative offline strategy
designed to simulate these arena battles using AI-driven annotations to
evaluate battle outcomes, thus facilitating the continuous improvement of the
target model through both supervised fine-tuning and reinforcement learning.
Arena Learning comprises two key elements. First, it ensures precise
evaluations and maintains consistency between offline simulations and online
competitions via WizardArena, a pipeline developed to accurately predict the
Elo rankings of various models using a meticulously designed offline test set.
Our results demonstrate that WizardArena's predictions closely align with those
from the online Arena. Second, it involves the continuous improvement of
training data based on the battle results and the refined model. We establish a
data flywheel to iteratively update the training data by highlighting the
weaknesses of the target model based on its battle results, enabling it to
learn from the strengths of multiple different models. We apply Arena Learning
to train our target model, WizardLM-$\beta$, and demonstrate significant
performance enhancements across various metrics. This fully automated training
and evaluation pipeline sets the stage for continuous advancements in various
LLMs via post-training. Notably, Arena Learning plays a pivotal role in the
success of WizardLM-2, and this paper serves both as an exploration of its
efficacy and a foundational study for future discussions related to WizardLM-2
and its derivatives.

摘要：評估大型語言模型 (LLM) 的效能會帶來重大的挑戰。在線上聊天機器人競技場中進行人工標註對戰的方法是一種高度有效的評估技術。然而，這種方法受到人工標註所需的成本和時間所限制。在本文中，我們介紹競技場學習，這是一種創新的離線策略，旨在使用 AI 驅動的標註來模擬這些競技場對戰，以評估對戰結果，從而促進目標模型透過監督微調和強化學習持續改進。競技場學習包含兩個關鍵元素。首先，它透過 WizardArena 確保精確的評估並維持離線模擬與線上競賽之間的一致性，WizardArena 是一個管道，用於使用精心設計的離線測試集準確預測各種模型的 Elo 排名。我們的結果證明，WizardArena 的預測與線上競技場的預測非常吻合。其次，它涉及根據對戰結果和精煉模型持續改進訓練資料。我們建立一個資料飛輪，透過根據目標模型的對戰結果突顯其弱點，反覆更新訓練資料，使其能夠從多個不同模型的優點中學習。我們將競技場學習應用於訓練我們的目標模型 WizardLM-$\beta$，並展示了各種指標的顯著效能提升。這種全自動化訓練和評估管道為透過訓練後持續推進各種 LLM 奠定了基礎。值得注意的是，競技場學習在 WizardLM-2 的成功中扮演了關鍵角色，而本文既探索了其效能，也為未來與 WizardLM-2 及其衍生產品相關的討論奠定了基礎研究。

##### **NoviCode: Generating Programs from Natural Language Utterances by Novices**
2407.10626v2 by Asaf Achi Mordechai, Yoav Goldberg, Reut Tsarfaty

Current Text-to-Code models demonstrate impressive capabilities in generating
executable code from natural language snippets. However, current studies focus
on technical instructions and programmer-oriented language, and it is an open
question whether these models can effectively translate natural language
descriptions given by non-technical users and express complex goals, to an
executable program that contains an intricate flow - composed of API access and
control structures as loops, conditions, and sequences. To unlock the challenge
of generating a complete program from a plain non-technical description we
present NoviCode, a novel NL Programming task, which takes as input an API and
a natural language description by a novice non-programmer and provides an
executable program as output. To assess the efficacy of models on this task, we
provide a novel benchmark accompanied by test suites wherein the generated
program code is assessed not according to their form, but according to their
functional execution. Our experiments show that, first, NoviCode is indeed a
challenging task in the code synthesis domain, and that generating complex code
from non-technical instructions goes beyond the current Text-to-Code paradigm.
Second, we show that a novel approach wherein we align the NL utterances with
the compositional hierarchical structure of the code, greatly enhances the
performance of LLMs on this task, compared with the end-to-end Text-to-Code
counterparts.

摘要：目前文本到程式碼模型在從自然語言片段產生可執行程式碼方面展現了令人印象深刻的能力。然而，目前的研究著重於技術指令和以程式設計師為導向的語言，而這些模型是否能有效翻譯非技術使用者提供的自然語言描述，並將其表達為包含複雜流程的可執行程式（由 API 存取和控制結構組成，例如迴圈、條件和順序），這是一個開放性的問題。為了解鎖從純粹的非技術描述產生完整程式的挑戰，我們提出了 NoviCode，這是一個新穎的自然語言程式設計任務，它以 API 和非程式設計師新手的自然語言描述作為輸入，並提供可執行程式作為輸出。為了評估模型在此任務上的效能，我們提供了一個新穎的基準，並附上測試套件，其中產生的程式碼不是根據其形式評估，而是根據其功能執行來評估。我們的實驗表明，首先，NoviCode 確實是程式碼合成領域的一項具有挑戰性的任務，而且從非技術指令產生複雜程式碼超出了目前的文本到程式碼範例。其次，我們展示了一種新穎的方法，其中我們將自然語言語句與程式碼的組合階層結構對齊，與端到端的文本到程式碼對應程式相比，這大大增強了大型語言模型在此任務上的效能。

##### **Leave No Knowledge Behind During Knowledge Distillation: Towards Practical and Effective Knowledge Distillation for Code-Switching ASR Using Realistic Data**
2407.10603v1 by Liang-Hsuan Tseng, Zih-Ching Chen, Wei-Shun Chang, Cheng-Kuang Lee, Tsung-Ren Huang, Hung-yi Lee

Recent advances in automatic speech recognition (ASR) often rely on large
speech foundation models for generating high-quality transcriptions. However,
these models can be impractical due to limited computing resources. The
situation is even more severe in terms of more realistic or difficult
scenarios, such as code-switching ASR (CS-ASR). To address this, we present a
framework for developing more efficient models for CS-ASR through knowledge
distillation using realistic speech-only data. Our proposed method, Leave No
Knowledge Behind During Knowledge Distillation (K$^2$D), leverages both the
teacher model's knowledge and additional insights from a small auxiliary model.
We evaluate our approach on two in-domain and two out-domain datasets,
demonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled
realistic data, we have successfully obtained a 2-time smaller model with
5-time faster generation speed while outperforming the baseline methods and the
teacher model on all the testing sets. We have made our model publicly
available on Hugging Face
(https://huggingface.co/andybi7676/k2d-whisper.zh-en).

摘要：自動語音辨識 (ASR) 的最新進展通常仰賴大型語音基礎模型來產生高品質的轉錄。然而，這些模型由於運算資源有限，可能不切實際。在更逼真或困難的場景中，例如程式碼轉換 ASR (CS-ASR)，情況更為嚴重。為了解決這個問題，我們提出一個架構，透過使用逼真的語音資料進行知識萃取，來開發更有效率的 CS-ASR 模型。我們提出的方法，在知識萃取過程中不遺漏任何知識 (K$^2$D)，同時利用教師模型的知識和來自小型輔助模型的額外見解。我們在兩個領域內和兩個領域外的資料集上評估我們的方法，證明 K$^2$D 是有效的。透過在未標記的逼真資料上執行 K$^2$D，我們成功地獲得一個模型大小縮小 2 倍，產生速度快 5 倍，同時在所有測試集中優於基準方法和教師模型。我們已將我們的模型公開在 Hugging Face（https://huggingface.co/andybi7676/k2d-whisper.zh-en）上。

##### **An evaluation of CNN models and data augmentation techniques in hierarchical localization of mobile robots**
2407.10596v1 by J. J. Cabrera, O. J. Céspedes, S. Cebollada, O. Reinoso, L. Payá

This work presents an evaluation of CNN models and data augmentation to carry
out the hierarchical localization of a mobile robot by using omnidireccional
images. In this sense, an ablation study of different state-of-the-art CNN
models used as backbone is presented and a variety of data augmentation visual
effects are proposed for addressing the visual localization of the robot. The
proposed method is based on the adaption and re-training of a CNN with a dual
purpose: (1) to perform a rough localization step in which the model is used to
predict the room from which an image was captured, and (2) to address the fine
localization step, which consists in retrieving the most similar image of the
visual map among those contained in the previously predicted room by means of a
pairwise comparison between descriptors obtained from an intermediate layer of
the CNN. In this sense, we evaluate the impact of different state-of-the-art
CNN models such as ConvNeXt for addressing the proposed localization. Finally,
a variety of data augmentation visual effects are separately employed for
training the model and their impact is assessed. The performance of the
resulting CNNs is evaluated under real operation conditions, including changes
in the lighting conditions. Our code is publicly available on the project
website https://github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git

摘要：本研究評估 CNN 模型和資料擴充，以透過全景影像進行行動機器人的階層定位。在此意義下，提出使用最先進的 CNN 模型作為主幹的消融研究，並提出各種資料擴充視覺效果，以解決機器人的視覺定位。所提出的方法基於 CNN 的適應和重新訓練，具有雙重目的：(1) 執行粗略定位步驟，其中模型用於預測擷取影像的房間，以及 (2) 解決精細定位步驟，包括從先前預測的房間中擷取最類似的視覺地圖影像，藉由從 CNN 的中間層取得的描述符進行成對比較。在此意義下，我們評估不同最先進 CNN 模型（例如 ConvNeXt）對所提出的定位的影響。最後，各種資料擴充視覺效果會分別用於訓練模型，並評估其影響。在實際操作條件下評估所產生的 CNN 效能，包括光照條件的變化。我們的程式碼可在專案網站 https://github.com/juanjo-cabrera/IndoorLocalizationSingleCNN.git 上公開取得

##### **Three Dogmas of Reinforcement Learning**
2407.10583v1 by David Abel, Mark K. Ho, Anna Harutyunyan

Modern reinforcement learning has been conditioned by at least three dogmas.
The first is the environment spotlight, which refers to our tendency to focus
on modeling environments rather than agents. The second is our treatment of
learning as finding the solution to a task, rather than adaptation. The third
is the reward hypothesis, which states that all goals and purposes can be well
thought of as maximization of a reward signal. These three dogmas shape much of
what we think of as the science of reinforcement learning. While each of the
dogmas have played an important role in developing the field, it is time we
bring them to the surface and reflect on whether they belong as basic
ingredients of our scientific paradigm. In order to realize the potential of
reinforcement learning as a canonical frame for researching intelligent agents,
we suggest that it is time we shed dogmas one and two entirely, and embrace a
nuanced approach to the third.

摘要：現代強化學習至少受到三條教條的制約。
第一個是環境聚光燈，它指的是我們傾向於專注於建模環境，而不是代理。第二個是我們將學習視為找到任務的解決方案，而不是適應。第三個是獎勵假設，它指出所有目標和目的都可以很好地被認為是獎勵信號的最大化。這三條教條塑造了我們對強化學習科學的許多看法。雖然每條教條在該領域的發展中都發揮了重要作用，但現在是時候將它們浮出水面，並思考它們是否屬於我們科學範式的基本要素。為了實現強化學習作為研究智能代理的規範框架的潛力，我們建議是時候完全拋棄教條一和教條二，並採取對教條三的細緻方法。

##### **Boosting Zero-Shot Crosslingual Performance using LLM-Based Augmentations with Effective Data Selection**
2407.10582v1 by Barah Fazili, Ashish Sunil Agrawal, Preethi Jyothi

Large language models (LLMs) are very proficient text generators. We leverage
this capability of LLMs to generate task-specific data via zero-shot prompting
and promote cross-lingual transfer for low-resource target languages. Given
task-specific data in a source language and a teacher model trained on this
data, we propose using this teacher to label LLM generations and employ a set
of simple data selection strategies that use the teacher's label probabilities.
Our data selection strategies help us identify a representative subset of
diverse generations that help boost zero-shot accuracies while being efficient,
in comparison to using all the LLM generations (without any subset selection).
We also highlight other important design choices that affect cross-lingual
performance such as the use of translations of source data and what labels are
best to use for the LLM generations. We observe significant performance gains
across sentiment analysis and natural language inference tasks (of up to a
maximum of 7.13 absolute points and 1.5 absolute points on average) across a
number of target languages (Hindi, Marathi, Urdu, Swahili) and domains.

摘要：大型語言模型 (LLM) 非常熟練的文本生成器。我們利用 LLM 的這種能力通過零次提示生成特定任務的數據，並促進低資源目標語言的跨語言傳輸。給定源語言中的特定任務數據和在此數據上訓練的教師模型，我們建議使用此教師標記 LLM 生成，並採用一組使用教師標籤概率的簡單數據選擇策略。我們的數據選擇策略幫助我們識別有助於提高零次準確率的具有代表性的多樣化生成子集，同時與使用所有 LLM 生成（沒有任何子集選擇）相比，效率更高。我們還強調影響跨語言性能的其他重要設計選擇，例如使用源數據翻譯以及 LLM 生成最適合使用哪些標籤。我們觀察到跨情緒分析和自然語言推理任務（平均最多 7.13 個絕對點和 1.5 個絕對點）的顯著性能增益，跨多種目標語言（印地語、馬拉地語、烏爾都語、斯瓦希里語）和領域。

##### **Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning**
2407.10580v1 by Daniel Geissler, Paul Lukowicz

Hybrid intelligence aims to enhance decision-making, problem-solving, and
overall system performance by combining the strengths of both, human cognitive
abilities and artificial intelligence. With the rise of Large Language Models
(LLM), progressively participating as smart agents to accelerate machine
learning development, Hybrid Intelligence is becoming an increasingly important
topic for effective interaction between humans and machines. This paper
presents an approach to leverage Hybrid Intelligence towards sustainable and
energy-aware machine learning. When developing machine learning models, final
model performance commonly rules the optimization process while the efficiency
of the process itself is often neglected. Moreover, in recent times, energy
efficiency has become equally crucial due to the significant environmental
impact of complex and large-scale computational processes. The contribution of
this work covers the interactive inclusion of secondary knowledge sources
through Human-in-the-loop (HITL) and LLM agents to stress out and further
resolve inefficiencies in the machine learning development process.

摘要：混合智能旨在通过结合人类认知能力和人工智能的优势来增强决策制定、问题解决和整体系统性能。随着大型语言模型 (LLM) 的兴起，逐渐参与作为智能代理来加速机器学习开发，混合智能正成为人机有效交互日益重要的主题。本文提出了一种利用混合智能实现可持续和节能机器学习的方法。在开发机器学习模型时，最终模型性能通常支配优化过程，而过程本身的效率往往被忽视。此外，近年来，由于复杂和大规模计算过程对环境的重大影响，能源效率也变得同样至关重要。这项工作的贡献涵盖了通过人机循环 (HITL) 和 LLM 代理交互式地包含辅助知识来源，以强调并进一步解决机器学习开发过程中的低效率。

##### **Beyond Generative Artificial Intelligence: Roadmap for Natural Language Generation**
2407.10554v1 by María Miró Maestre, Iván Martínez-Murillo, Tania J. Martin, Borja Navarro-Colorado, Antonio Ferrández, Armando Suárez Cueto, Elena Lloret

Generative Artificial Intelligence has grown exponentially as a result of
Large Language Models (LLMs). This has been possible because of the impressive
performance of deep learning methods created within the field of Natural
Language Processing (NLP) and its subfield Natural Language Generation (NLG),
which is the focus of this paper. Within the growing LLM family are the popular
GPT-4, Bard and more specifically, tools such as ChatGPT have become a
benchmark for other LLMs when solving most of the tasks involved in NLG
research. This scenario poses new questions about the next steps for NLG and
how the field can adapt and evolve to deal with new challenges in the era of
LLMs. To address this, the present paper conducts a review of a representative
sample of surveys recently published in NLG. By doing so, we aim to provide the
scientific community with a research roadmap to identify which NLG aspects are
still not suitably addressed by LLMs, as well as suggest future lines of
research that should be addressed going forward.

摘要：生成式人工智能因大型语言模型 (LLM) 而呈指数增长。这归功于在自然语言处理 (NLP) 领域内创建的深度学习方法及其子领域自然语言生成 (NLG) 的出色表现，这也是本文的重点。在不断增长的 LLM 家族中，流行的 GPT-4、Bard 以及更具体地说，ChatGPT 等工具已成为解决 NLG 研究中涉及的大多数任务时其他 LLM 的基准。这种场景对 NLG 的下一步提出了新的问题，以及该领域如何适应和发展以应对 LLM 时代的全新挑战。为了解决这个问题，本文对最近发表在 NLG 中的代表性调查样本进行了审查。通过这样做，我们的目标是为科学界提供一个研究路线图，以确定哪些 NLG 方面仍然没有被 LLM 适当地解决，并建议未来应该解决的研究方向。

##### **Learning Social Cost Functions for Human-Aware Path Planning**
2407.10547v1 by Andrea Eirale, Matteo Leonetti, Marcello Chiaberge

Achieving social acceptance is one of the main goals of Social Robotic
Navigation. Despite this topic has received increasing interest in recent
years, most of the research has focused on driving the robotic agent along
obstacle-free trajectories, planning around estimates of future human motion to
respect personal distances and optimize navigation. However, social
interactions in everyday life are also dictated by norms that do not strictly
depend on movement, such as when standing at the end of a queue rather than
cutting it. In this paper, we propose a novel method to recognize common social
scenarios and modify a traditional planner's cost function to adapt to them.
This solution enables the robot to carry out different social navigation
behaviors that would not arise otherwise, maintaining the robustness of
traditional navigation. Our approach allows the robot to learn different social
norms with a single learned model, rather than having different modules for
each task. As a proof of concept, we consider the tasks of queuing and respect
interaction spaces of groups of people talking to one another, but the method
can be extended to other human activities that do not involve motion.

摘要：實現社會接受度是社會機器人導航的主要目標之一。儘管這個主題近年來受到越來越多的關注，但大部分的研究都集中在沿著無障礙軌跡駕駛機器人代理人、圍繞對未來人類運動的估計進行規劃，以尊重個人距離並優化導航。然而，日常生活中的社交互動也受到規範的支配，這些規範並不嚴格依賴於運動，例如排隊時站在隊伍的最後面而不是插隊。在本文中，我們提出了一種新的方法來識別常見的社交場景，並修改傳統規劃器的成本函數以適應它們。此解決方案使機器人能夠執行不同的社交導航行為，而這些行為在其他情況下不會出現，同時保持傳統導航的穩健性。我們的做法允許機器人使用單一的學習模型學習不同的社會規範，而不是為每個任務使用不同的模組。作為概念驗證，我們考慮了排隊和尊重彼此交談的人群的互動空間的任務，但此方法可以擴展到不涉及運動的其他人類活動。

##### **Understanding the Dependence of Perception Model Competency on Regions in an Image**
2407.10543v1 by Sara Pohland, Claire Tomlin

While deep neural network (DNN)-based perception models are useful for many
applications, these models are black boxes and their outputs are not yet well
understood. To confidently enable a real-world, decision-making system to
utilize such a perception model without human intervention, we must enable the
system to reason about the perception model's level of competency and respond
appropriately when the model is incompetent. In order for the system to make an
intelligent decision about the appropriate action when the model is
incompetent, it would be useful for the system to understand why the model is
incompetent. We explore five novel methods for identifying regions in the input
image contributing to low model competency, which we refer to as image
cropping, segment masking, pixel perturbation, competency gradients, and
reconstruction loss. We assess the ability of these five methods to identify
unfamiliar objects, recognize regions associated with unseen classes, and
identify unexplored areas in an environment. We find that the competency
gradients and reconstruction loss methods show great promise in identifying
regions associated with low model competency, particularly when aspects of the
image that are unfamiliar to the perception model are causing this reduction in
competency. Both of these methods boast low computation times and high levels
of accuracy in detecting image regions that are unfamiliar to the model,
allowing them to provide potential utility in decision-making pipelines. The
code for reproducing our methods and results is available on GitHub:
https://github.com/sarapohland/explainable-competency.

摘要：<paragraph>雖然基於深度神經網路 (DNN) 的感知模型對於許多應用程式很有用，但這些模型是黑盒子，其輸出尚未被充分理解。為了讓真實世界的決策系統能夠在沒有人工干預的情況下利用這樣的感知模型，我們必須讓系統能夠推論感知模型的能力水準，並在模型無能時做出適當的回應。為了讓系統在模型無能時做出明智的決策，系統了解模型無能的原因會很有用。我們探討了五種創新的方法來識別輸入影像中導致模型能力低下的區域，我們稱之為影像裁切、區段遮罩、像素擾動、能力梯度和重建損失。我們評估這五種方法識別不熟悉物體、識別與未見類別相關的區域，以及識別環境中未探索區域的能力。我們發現能力梯度和重建損失方法在識別與模型能力低下相關的區域方面顯示出巨大的前景，特別是在感知模型不熟悉的影像方面導致能力下降時。這兩種方法都具有低計算時間和高準確度來偵測模型不熟悉的影像區域，讓它們能夠在決策管道中提供潛在的效用。重現我們的方法和結果的程式碼可在 GitHub 上取得：https://github.com/sarapohland/explainable-competency。</paragraph>

##### **An experimental evaluation of Siamese Neural Networks for robot localization using omnidirectional imaging in indoor environments**
2407.10536v1 by J. J. Cabrera, V. Román, A. Gil, O. Reinoso, L. Payá

The objective of this paper is to address the localization problem using
omnidirectional images captured by a catadioptric vision system mounted on the
robot. For this purpose, we explore the potential of Siamese Neural Networks
for modeling indoor environments using panoramic images as the unique source of
information. Siamese Neural Networks are characterized by their ability to
generate a similarity function between two input data, in this case, between
two panoramic images. In this study, Siamese Neural Networks composed of two
Convolutional Neural Networks (CNNs) are used. The output of each CNN is a
descriptor which is used to characterize each image. The dissimilarity of the
images is computed by measuring the distance between these descriptors. This
fact makes Siamese Neural Networks particularly suitable to perform image
retrieval tasks. First, we evaluate an initial task strongly related to
localization that consists in detecting whether two images have been captured
in the same or in different rooms. Next, we assess Siamese Neural Networks in
the context of a global localization problem. The results outperform previous
techniques for solving the localization task using the COLD-Freiburg dataset,
in a variety of lighting conditions, specially when using images captured in
cloudy and night conditions.

摘要：本文的目的是使用安裝在機器人上的雙曲率視覺系統所擷取的全向影像來解決定位問題。為此，我們探討了使用暹羅神經網路的潛力，以全景影像作為唯一資訊來源來建模室內環境。暹羅神經網路的特點是能夠產生兩個輸入資料之間的相似性函數，在本例中，是在兩個全景影像之間。在本研究中，使用了由兩個卷積神經網路 (CNN) 組成的暹羅神經網路。每個 CNN 的輸出都是一個描述符，用於描述每個影像。影像的相異性是透過測量這些描述符之間的距離來計算的。這個事實使得暹羅神經網路特別適合執行影像檢索任務。首先，我們評估與定位密切相關的初始任務，包括偵測兩張影像是否在同一個房間或不同房間中拍攝。接下來，我們在全球定位問題的背景下評估暹羅神經網路。結果優於先前使用 COLD-Freiburg 資料集解決定位任務的技術，在各種光照條件下，特別是在陰天和夜間拍攝的影像中。

##### **TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction**
2407.10510v1 by Xingzhi Zhou, Xin Dong, Chunhao Li, Yuning Bai, Yulong Xu, Ka Chun Cheung, Simon See, Xinpeng Song, Runshun Zhang, Xuezhong Zhou, Nevin L. Zhang

Traditional Chinese medicine (TCM) relies on specific combinations of herbs
in prescriptions to treat symptoms and signs, a practice that spans thousands
of years. Predicting TCM prescriptions presents a fascinating technical
challenge with practical implications. However, this task faces limitations due
to the scarcity of high-quality clinical datasets and the intricate
relationship between symptoms and herbs. To address these issues, we introduce
DigestDS, a new dataset containing practical medical records from experienced
experts in digestive system diseases. We also propose a method, TCM-FTP (TCM
Fine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs)
through supervised fine-tuning on DigestDS. Additionally, we enhance
computational efficiency using a low-rank adaptation technique. TCM-FTP also
incorporates data augmentation by permuting herbs within prescriptions,
capitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves
an F1-score of 0.8031, surpassing previous methods significantly. Furthermore,
it demonstrates remarkable accuracy in dosage prediction, achieving a
normalized mean square error of 0.0604. In contrast, LLMs without fine-tuning
perform poorly. Although LLMs have shown capabilities on a wide range of tasks,
this work illustrates the importance of fine-tuning for TCM prescription
prediction, and we have proposed an effective way to do that.

摘要：中醫依賴特定中草藥組合來治療症狀和徵兆，這項做法已有數千年的歷史。預測中醫處方是一個引人入勝的技術挑戰，具有實際意義。然而，由於缺乏高品質的臨床數據集以及症狀與中草藥之間的複雜關係，這項任務面臨限制。為了解決這些問題，我們引入了 DigestDS，一個包含消化系統疾病經驗豐富專家實際病歷的新數據集。我們還提出了一種方法，TCM-FTP（中醫微調預訓練），通過在 DigestDS 上進行監督微調來利用預訓練的大語言模型 (LLM)。此外，我們使用低秩適應技術來提高計算效率。TCM-FTP 還通過置換處方中的中草藥來納入數據擴充，利用它們與順序無關的特性。令人印象深刻的是，TCM-FTP 達到了 0.8031 的 F1 分數，顯著超越了以前的方法。此外，它在劑量預測中表現出顯著的準確性，實現了 0.0604 的歸一化均方誤差。相比之下，未經微調的 LLM 表現不佳。儘管 LLM 已在廣泛的任務中展現出能力，但這項工作說明了微調對於中醫處方預測的重要性，而且我們提出了一個有效的方法來做到這一點。

##### **CIBench: Evaluating Your LLMs with a Code Interpreter Plugin**
2407.10499v1 by Songyang Zhang, Chuyu Zhang, Yingfan Hu, Haowen Shen, Kuikun Liu, Zerun Ma, Fengzhe Zhou, Wenwei Zhang, Xuming He, Dahua Lin, Kai Chen

While LLM-Based agents, which use external tools to solve complex problems,
have made significant progress, benchmarking their ability is challenging,
thereby hindering a clear understanding of their limitations. In this paper, we
propose an interactive evaluation framework, named CIBench, to comprehensively
assess LLMs' ability to utilize code interpreters for data science tasks. Our
evaluation framework includes an evaluation dataset and two evaluation modes.
The evaluation dataset is constructed using an LLM-human cooperative approach
and simulates an authentic workflow by leveraging consecutive and interactive
IPython sessions. The two evaluation modes assess LLMs' ability with and
without human assistance. We conduct extensive experiments to analyze the
ability of 24 LLMs on CIBench and provide valuable insights for future LLMs in
code interpreter utilization.

摘要：雖然使用外部工具來解決複雜問題的 LLM-Based 代理已取得顯著進展，但對其能力進行基準測試具有挑戰性，從而阻礙了對其限制的清晰理解。在本文中，我們提出了一個名為 CIBench 的互動式評估框架，以全面評估 LLM 利用程式碼解釋器執行資料科學任務的能力。我們的評估框架包括一個評估資料集和兩種評估模式。評估資料集是使用 LLM-human 協作方法構建的，並透過連續且互動的 IPython 會話模擬真實的工作流程。這兩種評估模式評估了 LLM 在有人協助和無人協助的情況下的能力。我們進行了大量的實驗，以分析 24 個 LLM 在 CIBench 上的能力，並為未來 LLM 在程式碼解釋器利用方面提供了有價值的見解。

##### **Learning Dynamics of LLM Finetuning**
2407.10490v1 by Yi Ren, Danica J. Sutherland

Learning dynamics, which describes how the learning of specific training
examples influences the model's prediction of other examples, give us a
powerful tool for understanding the behavior of deep learning systems. We study
the learning dynamics of large language models during finetuning, by analyzing
the step-wise decomposition and accumulated influence among different
responses. Our framework allows a uniform interpretation of many interesting
observations about the training of popular algorithms for both instruction
tuning and preference tuning. The analysis not only explains where the benefits
of these methods come from but also inspires a simple, effective method to
further improve the alignment performance. Code for experiments is available at
https://github.com/Joshua-Ren/Learning_dynamics_LLM.

摘要：學習動態描述了特定訓練範例的學習如何影響模型對其他範例的預測，為我們提供了理解深度學習系統行為的強大工具。我們透過分析不同回應之間逐步分解和累積的影響，研究大型語言模型在微調過程中的學習動態。我們的架構允許對許多有趣的觀察進行統一的詮釋，這些觀察是關於用於指令微調和偏好微調的熱門演算法的訓練。分析不僅解釋了這些方法的優點從何而來，還啟發了一種簡單、有效的方法，可以進一步改善比對效能。實驗程式碼可在 https://github.com/Joshua-Ren/Learning_dynamics_LLM 取得。

##### **How and where does CLIP process negation?**
2407.10488v1 by Vincent Quantmeyer, Pablo Mosteiro, Albert Gatt

Various benchmarks have been proposed to test linguistic understanding in
pre-trained vision \& language (VL) models. Here we build on the existence task
from the VALSE benchmark (Parcalabescu et al, 2022) which we use to test
models' understanding of negation, a particularly interesting issue for
multimodal models. However, while such VL benchmarks are useful for measuring
model performance, they do not reveal anything about the internal processes
through which these models arrive at their outputs in such visio-linguistic
tasks. We take inspiration from the growing literature on model
interpretability to explain the behaviour of VL models on the understanding of
negation. Specifically, we approach these questions through an in-depth
analysis of the text encoder in CLIP (Radford et al, 2021), a highly
influential VL model. We localise parts of the encoder that process negation
and analyse the role of attention heads in this task. Our contributions are
threefold. We demonstrate how methods from the language model interpretability
literature (such as causal tracing) can be translated to multimodal models and
tasks; we provide concrete insights into how CLIP processes negation on the
VALSE existence task; and we highlight inherent limitations in the VALSE
dataset as a benchmark for linguistic understanding.

摘要：各種基準已被提出，用來測試預訓練視覺與語言 (VL) 模型中的語言理解。在此，我們建立在 VALSE 基準 (Parcalabescu 等人，2022 年) 的存在任務上，我們使用它來測試模型對否定的理解，這對多模式模型來說是一個特別有趣的問題。然而，雖然此類 VL 基準對於衡量模型效能很有用，但它們並未揭示任何關於這些模型如何透過視覺語言任務得出其輸出的內部程序。我們從關於模型可解釋性的日益豐富的文獻中汲取靈感，以解釋 VL 模型在理解否定方面的行為。具體而言，我們透過深入分析 CLIP (Radford 等人，2021 年) 中的文本編碼器來探討這些問題，CLIP 是一個影響力極大的 VL 模型。我們將處理否定的編碼器部分定位出來，並分析注意力標頭在此任務中的作用。我們的貢獻有三方面。我們展示了語言模型可解釋性文獻中的方法 (例如因果追蹤) 如何轉換為多模式模型和任務；我們提供了 CLIP 如何在 VALSE 存在任務中處理否定的具體見解；我們強調了 VALSE 資料集作為語言理解基準的固有限制。

##### **IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization**
2407.10486v1 by Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang

Query-focused summarization (QFS) aims to produce summaries that answer
particular questions of interest, enabling greater user control and
personalization. With the advent of large language models (LLMs), shows their
impressive capability of textual understanding through large-scale pretraining,
which implies the great potential of extractive snippet generation. In this
paper, we systematically investigated two indispensable characteristics that
the LLMs-based QFS models should be harnessed, Lengthy Document Summarization
and Efficiently Fine-grained Query-LLM Alignment, respectively.
Correspondingly, we propose two modules called Query-aware HyperExpert and
Query-focused Infini-attention to access the aforementioned characteristics.
These innovations pave the way for broader application and accessibility in the
field of QFS technology. Extensive experiments conducted on existing QFS
benchmarks indicate the effectiveness and generalizability of the proposed
approach. Our code is publicly available at
https://github.com/DCDmllm/IDEAL_Summary.

摘要：以查詢為重點的摘要 (QFS) 旨在產生摘要來回答特定感興趣的問題，讓使用者能有更大的控制權和個人化設定。隨著大型語言模型 (LLM) 的出現，顯示出它們透過大規模預訓練而具備令人印象深刻的文字理解能力，這暗示了萃取片段生成的巨大潛力。在本文中，我們系統性地探討了兩個 LLM 為基礎的 QFS 模型應該具備的不可或缺特徵，分別是冗長文件摘要和高效的細粒度查詢-LLM 對齊。相應地，我們提出兩個模組，稱為查詢感知 HyperExpert 和查詢為重點的 Infini-attention，以存取上述特徵。這些創新為 QFS 技術領域的更廣泛應用和可及性鋪平了道路。在現有 QFS 基準上進行的廣泛實驗顯示了所提出方法的有效性和普遍性。我們的程式碼已公開發布在 https://github.com/DCDmllm/IDEAL_Summary。

##### **SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation**
2407.10481v1 by Jordan Juravsky, Yunrong Guo, Sanja Fidler, Xue Bin Peng

Physically-simulated models for human motion can generate high-quality
responsive character animations, often in real-time. Natural language serves as
a flexible interface for controlling these models, allowing expert and
non-expert users to quickly create and edit their animations. Many recent
physics-based animation methods, including those that use text interfaces,
train control policies using reinforcement learning (RL). However, scaling
these methods beyond several hundred motions has remained challenging.
Meanwhile, kinematic animation models are able to successfully learn from
thousands of diverse motions by leveraging supervised learning methods.
Inspired by these successes, in this work we introduce SuperPADL, a scalable
framework for physics-based text-to-motion that leverages both RL and
supervised learning to train controllers on thousands of diverse motion clips.
SuperPADL is trained in stages using progressive distillation, starting with a
large number of specialized experts using RL. These experts are then
iteratively distilled into larger, more robust policies using a combination of
reinforcement learning and supervised learning. Our final SuperPADL controller
is trained on a dataset containing over 5000 skills and runs in real time on a
consumer GPU. Moreover, our policy can naturally transition between skills,
allowing for users to interactively craft multi-stage animations. We
experimentally demonstrate that SuperPADL significantly outperforms RL-based
baselines at this large data scale.

摘要：物理模拟的人体运动模型可以生成高质量的响应式角色动画，通常是实时的。自然语言作为控制这些模型的灵活界面，允许专家和非专家用户快速创建和编辑他们的动画。许多最近基于物理的动画方法，包括那些使用文本界面的方法，使用强化学习 (RL) 训练控制策略。然而，将这些方法扩展到数百个动作之外仍然具有挑战性。同时，运动学动画模型能够通过利用监督学习方法从数千种不同的动作中成功学习。受这些成功的启发，在这项工作中，我们介绍了 SuperPADL，一个基于物理的文本到动作的可扩展框架，它利用 RL 和监督学习来训练数千个不同动作剪辑的控制器。SuperPADL 使用渐进蒸馏分阶段进行训练，从使用 RL 的大量专门专家开始。然后，使用强化学习和监督学习的组合，将这些专家迭代地蒸馏成更大、更健壮的策略。我们最终的 SuperPADL 控制器是在包含 5000 多项技能的数据集上训练的，并在消费级 GPU 上实时运行。此外，我们的策略可以在技能之间自然过渡，允许用户交互式地制作多阶段动画。我们通过实验表明，SuperPADL 在此大型数据规模上明显优于基于 RL 的基准。

##### **Kinetic Typography Diffusion Model**
2407.10476v1 by Seonmi Park, Inhwan Bae, Seunghyun Shin, Hae-Gon Jeon

This paper introduces a method for realistic kinetic typography that
generates user-preferred animatable 'text content'. We draw on recent advances
in guided video diffusion models to achieve visually-pleasing text appearances.
To do this, we first construct a kinetic typography dataset, comprising about
600K videos. Our dataset is made from a variety of combinations in 584
templates designed by professional motion graphics designers and involves
changing each letter's position, glyph, and size (i.e., flying, glitches,
chromatic aberration, reflecting effects, etc.). Next, we propose a video
diffusion model for kinetic typography. For this, there are three requirements:
aesthetic appearances, motion effects, and readable letters. This paper
identifies the requirements. For this, we present static and dynamic captions
used as spatial and temporal guidance of a video diffusion model, respectively.
The static caption describes the overall appearance of the video, such as
colors, texture and glyph which represent a shape of each letter. The dynamic
caption accounts for the movements of letters and backgrounds. We add one more
guidance with zero convolution to determine which text content should be
visible in the video. We apply the zero convolution to the text content, and
impose it on the diffusion model. Lastly, our glyph loss, only minimizing a
difference between the predicted word and its ground-truth, is proposed to make
the prediction letters readable. Experiments show that our model generates
kinetic typography videos with legible and artistic letter motions based on
text prompts.

摘要：<paragraph>本文介紹了一種逼真的動態排版方法，可產生使用者偏好的可動畫「文字內容」。我們利用引導式影片擴散模型的最新進展，以達成視覺上令人愉悅的文字外觀。為此，我們首先建構一個動態排版資料集，其中包含約 60 萬個影片。我們的資料集是由專業動態圖形設計師設計的 584 個範本中的各種組合製成，並涉及變更每個字母的位置、字形和大小（例如，飛動、故障、色差、反射效果等）。接下來，我們提出一個用於動態排版的影片擴散模型。為此，有三個要求：美觀的外觀、動作效果和可讀的字母。本文確定了這些要求。為此，我們分別呈現用作影片擴散模型的空間和時間引導的靜態和動態字幕。靜態字幕描述影片的整體外觀，例如顏色、紋理和字形，代表每個字母的形狀。動態字幕說明字母和背景的移動。我們再加入一個零卷積引導，以確定哪些文字內容應該在影片中可見。我們將零卷積應用於文字內容，並將其強加於擴散模型上。最後，我們的字形損失僅最小化預測字詞和其真實值之間的差異，目的是讓預測字母可讀。實驗顯示，我們的模型根據文字提示產生具有清晰且具藝術感的字母動作的動態排版影片。</paragraph>

##### **GROOT: Generating Robust Watermark for Diffusion-Model-Based Audio Synthesis**
2407.10471v1 by Weizhi Liu, Yue Li, Dongdong Lin, Hui Tian, Haizhou Li

Amid the burgeoning development of generative models like diffusion models,
the task of differentiating synthesized audio from its natural counterpart
grows more daunting. Deepfake detection offers a viable solution to combat this
challenge. Yet, this defensive measure unintentionally fuels the continued
refinement of generative models. Watermarking emerges as a proactive and
sustainable tactic, preemptively regulating the creation and dissemination of
synthesized content. Thus, this paper, as a pioneer, proposes the generative
robust audio watermarking method (Groot), presenting a paradigm for proactively
supervising the synthesized audio and its source diffusion models. In this
paradigm, the processes of watermark generation and audio synthesis occur
simultaneously, facilitated by parameter-fixed diffusion models equipped with a
dedicated encoder. The watermark embedded within the audio can subsequently be
retrieved by a lightweight decoder. The experimental results highlight Groot's
outstanding performance, particularly in terms of robustness, surpassing that
of the leading state-of-the-art methods. Beyond its impressive resilience
against individual post-processing attacks, Groot exhibits exceptional
robustness when facing compound attacks, maintaining an average watermark
extraction accuracy of around 95%.

摘要：在生成模型（如扩散模型）蓬勃发展的背景下，
区分合成音频与其自然对应音频的任务变得更加艰巨。
深度伪造检测提供了一个可行的解决方案来应对这一
挑战。然而，这种防御措施无意中促进了生成模型的持续
完善。水印作为一种主动且可持续的策略出现，先发制人地规范了
合成内容的创建和传播。因此，本文作为先驱，提出了生成
鲁棒音频水印方法 (Groot)，展示了一种主动监督合成音频及其源扩散模型的范例。在这个
范例中，水印生成和音频合成过程同时发生，由配备专用编码器的参数固定扩散模型促进。随后，嵌入在音频中的水印可以通过轻量级解码器检索。实验结果突出了 Groot 的
出色性能，尤其是在鲁棒性方面，超过了领先的最新方法。除了对个别后处理攻击具有令人印象深刻的弹性之外，Groot 在面对复合攻击时表现出非凡的
鲁棒性，保持了约 95% 的平均水印提取准确率。

##### **LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis**
2407.10468v1 by Zhenxiong Tan, Xinyin Ma, Gongfan Fang, Xinchao Wang

Latent diffusion models have shown promising results in audio generation,
making notable advancements over traditional methods. However, their
performance, while impressive with short audio clips, faces challenges when
extended to longer audio sequences. These challenges are due to model's
self-attention mechanism and training predominantly on 10-second clips, which
complicates the extension to longer audio without adaptation. In response to
these issues, we introduce a novel approach, LiteFocus that enhances the
inference of existing audio latent diffusion models in long audio synthesis.
Observed the attention pattern in self-attention, we employ a dual sparse form
for attention calculation, designated as same-frequency focus and
cross-frequency compensation, which curtails the attention computation under
same-frequency constraints, while enhancing audio quality through
cross-frequency refillment. LiteFocus demonstrates substantial reduction on
inference time with diffusion-based TTA model by 1.99x in synthesizing
80-second audio clips while also obtaining improved audio quality.

摘要：潛在擴散模型在音訊生成方面展現出令人滿意的成果，相較於傳統方法有顯著進步。然而，儘管在短音訊片段中表現令人印象深刻，但當擴展到較長的音訊序列時，其效能便面臨挑戰。這些挑戰是基於模型的自我注意力機制，並主要訓練 10 秒片段，這使得在沒有適應的情況下擴展到較長的音訊變得複雜。為了回應這些問題，我們引進一種新方法 LiteFocus，它增強了現有音訊潛在擴散模型在長音訊合成中的推論。觀察到自我注意力中的注意力模式，我們採用雙重稀疏形式進行注意力計算，指定為相同頻率焦點和跨頻率補償，這在相同頻率約束下縮短了注意力計算，同時透過跨頻率補充增強音訊品質。LiteFocus 證明在合成 80 秒音訊片段時，基於擴散的 TTA 模型的推論時間大幅減少了 1.99 倍，同時也獲得了更好的音訊品質。

##### **BandControlNet: Parallel Transformers-based Steerable Popular Music Generation with Fine-Grained Spatiotemporal Features**
2407.10462v1 by Jing Luo, Xinyu Yang, Dorien Herremans

Controllable music generation promotes the interaction between humans and
composition systems by projecting the users' intent on their desired music. The
challenge of introducing controllability is an increasingly important issue in
the symbolic music generation field. When building controllable generative
popular multi-instrument music systems, two main challenges typically present
themselves, namely weak controllability and poor music quality. To address
these issues, we first propose spatiotemporal features as powerful and
fine-grained controls to enhance the controllability of the generative model.
In addition, an efficient music representation called REMI_Track is designed to
convert multitrack music into multiple parallel music sequences and shorten the
sequence length of each track with Byte Pair Encoding (BPE) techniques.
Subsequently, we release BandControlNet, a conditional model based on parallel
Transformers, to tackle the multiple music sequences and generate high-quality
music samples that are conditioned to the given spatiotemporal control
features. More concretely, the two specially designed modules of
BandControlNet, namely structure-enhanced self-attention (SE-SA) and
Cross-Track Transformer (CTT), are utilized to strengthen the resulting musical
structure and inter-track harmony modeling respectively. Experimental results
tested on two popular music datasets of different lengths demonstrate that the
proposed BandControlNet outperforms other conditional music generation models
on most objective metrics in terms of fidelity and inference speed and shows
great robustness in generating long music samples. The subjective evaluations
show BandControlNet trained on short datasets can generate music with
comparable quality to state-of-the-art models, while outperforming them
significantly using longer datasets.

摘要：可控音乐生成通过将用户的意图投射到他们期望的音乐上，促进了人与作曲系统之间的交互。引入可控性的挑战在符号音乐生成领域是一个日益重要的问题。在构建可控生成流行的多乐器音乐系统时，通常会遇到两个主要挑战，即控制能力弱和音乐质量差。为了解决这些问题，我们首先提出时空特征作为强大且细粒度的控制，以增强生成模型的可控性。此外，设计了一种称为 REMI_Track 的高效音乐表示，以将多轨音乐转换为多个并行音乐序列，并使用字节对编码 (BPE) 技术缩短每条轨道的序列长度。随后，我们发布了 BandControlNet，这是一种基于并行 Transformer 的条件模型，用于处理多个音乐序列并生成高质量的音乐样本，这些样本以给定的时空控制特征为条件。更具体地说，BandControlNet 的两个专门设计的模块，即结构增强自注意力 (SE-SA) 和跨轨 Transformer (CTT)，分别用于加强生成的音乐结构和音轨间和谐建模。在两个不同长度的流行音乐数据集上测试的实验结果表明，所提出的 BandControlNet 在保真度和推理速度方面优于其他条件音乐生成模型，并且在生成较长的音乐样本时显示出很强的鲁棒性。主观评估表明，在较短的数据集上训练的 BandControlNet 可以生成质量与最先进模型相当的音乐，同时在较长的数据集上明显优于它们。

##### **The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism**
2407.10457v1 by Yifan Song, Guoyin Wang, Sujian Li, Bill Yuchen Lin

Current evaluations of large language models (LLMs) often overlook
non-determinism, typically focusing on a single output per example. This limits
our understanding of LLM performance variability in real-world applications.
Our study addresses this issue by exploring key questions about the performance
differences between greedy decoding and sampling, identifying benchmarks'
consistency regarding non-determinism, and examining unique model behaviors.
Through extensive experiments, we observe that greedy decoding generally
outperforms sampling methods for most evaluated tasks. We also observe
consistent performance across different LLM sizes and alignment methods, noting
that alignment can reduce sampling variance. Moreover, our best-of-N sampling
approach demonstrates that smaller LLMs can match or surpass larger models such
as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This
research shows the importance of considering non-determinism in LLM evaluations
and provides insights for future LLM development and evaluation.

摘要：大型語言模型 (LLM) 當前的評估通常忽略非決定論，通常只關注每個範例的單一輸出。這限制了我們對 LLM 在實際應用中效能變異的理解。我們的研究透過探討貪婪解碼和取樣之間效能差異的關鍵問題、找出基準對非決定論的一致性，以及檢視獨特的模型行為來解決這個問題。透過廣泛的實驗，我們觀察到貪婪解碼通常在大部分評估任務中優於取樣方法。我們也觀察到不同 LLM 大小和對齊方法的效能一致，並注意到對齊可以減少取樣的差異。此外，我們最佳的 N 取樣方法證明較小的 LLM 可以匹配或超越較大的模型，例如 GPT-4-Turbo，突顯了較小 LLM 未開發的潛力。這項研究顯示在 LLM 評估中考量非決定論的重要性，並為未來的 LLM 開發和評估提供見解。

##### **Don't Throw Away Data: Better Sequence Knowledge Distillation**
2407.10456v1 by Jun Wang, Eleftheria Briakou, Hamid Dadkhahi, Rishabh Agarwal, Colin Cherry, Trevor Cohn

A critical component in knowledge distillation is the means of coupling the
teacher and student. The predominant sequence knowledge distillation method
involves supervised learning of the student against teacher-decoded outputs,
and is exemplified by the current state of the art, which incorporates minimum
Bayes risk (MBR) decoding. In this paper we seek to integrate MBR more tightly
in distillation training, specifically by using several high scoring MBR
translations, rather than a single selected sequence, thus capturing a rich
diversity of teacher outputs. Our experiments on English to German and English
to Japanese translation show consistent improvements over strong baseline
methods for both tasks and with varying model sizes. Additionally, we conduct a
detailed analysis focusing on data efficiency and capacity curse aspects to
elucidate MBR-n and explore its further potential.

摘要：知識萃取中一個重要的組成部分是連結教師和學生的方法。
主要的序列知識萃取方法涉及學生針對教師解碼輸出進行監督學習，
並以包含最小貝氏風險 (MBR) 解碼的現有技術為例。
在本文中，我們尋求將 MBR 更緊密地整合到萃取訓練中，
特別是透過使用多個高分 MBR 翻譯，而不是單一選定的序列，
從而捕捉到教師輸出的豐富多樣性。
我們針對英譯德和英譯日翻譯進行的實驗顯示，
對於這兩個任務和各種模型大小，都比強大的基線方法有顯著的改進。
此外，我們進行了詳細的分析，重點關注資料效率和容量瓶頸方面，
以闡明 MBR-n 並探索其進一步的潛力。

##### **Enhancing Medication Recommendation with LLM Text Representation**
2407.10453v1 by Yu-Tzu Lee

Most of the existing medication recommendation models are predicted with only
structured data such as medical codes, with the remaining other large amount of
unstructured or semi-structured data underutilization. To increase the
utilization effectively, we proposed a method of enhancing medication
recommendation with Large Language Model (LLM) text representation. LLM
harnesses powerful language understanding and generation capabilities, enabling
the extraction of information from complex and lengthy unstructured data such
as clinical notes which contain complex terminology. This method can be applied
to several existing base models we selected and improve medication
recommendation performance with the combination representation of text and
medical codes experiments on two different datasets. LLM text representation
alone can even demonstrate a comparable ability to the medical code
representation alone. Overall, this is a general method that can be applied to
other models for improved recommendations.

摘要：現有的藥物推薦模型大多僅使用結構化資料（例如醫療代碼）進行預測，而大量未結構化或半結構化資料則未被充分利用。為了有效提高利用率，我們提出了一種使用大型語言模型 (LLM) 文字表徵來增強藥物推薦的方法。LLM 具備強大的語言理解和生成能力，能夠從複雜且冗長的非結構化資料（例如包含複雜術語的臨床筆記）中提取資訊。此方法可應用於我們挑選的幾個現有基礎模型，並透過文字和醫療代碼的組合表徵來改善藥物推薦效能，並在兩個不同的資料集上進行實驗。單獨的 LLM 文字表徵甚至可以展現出與單獨的醫療代碼表徵相當的能力。總而言之，這是一種通用方法，可應用於其他模型以改善推薦結果。

##### **GraphPrint: Extracting Features from 3D Protein Structure for Drug Target Affinity Prediction**
2407.10452v1 by Amritpal Singh

Accurate drug target affinity prediction can improve drug candidate
selection, accelerate the drug discovery process, and reduce drug production
costs. Previous work focused on traditional fingerprints or used features
extracted based on the amino acid sequence in the protein, ignoring its 3D
structure which affects its binding affinity. In this work, we propose
GraphPrint: a framework for incorporating 3D protein structure features for
drug target affinity prediction. We generate graph representations for protein
3D structures using amino acid residue location coordinates and combine them
with drug graph representation and traditional features to jointly learn drug
target affinity. Our model achieves a mean square error of 0.1378 and a
concordance index of 0.8929 on the KIBA dataset and improves over using
traditional protein features alone. Our ablation study shows that the 3D
protein structure-based features provide information complementary to
traditional features.

摘要：準確的藥物標靶親和力預測可以改善藥物候選物的篩選、加速藥物發現的過程，並降低藥物生產成本。先前的研究專注於傳統指紋或使用根據蛋白質中的胺基酸序列萃取出的特徵，忽略了影響其結合親和力的 3D 結構。在這項研究中，我們提出 GraphPrint：一個用於納入 3D 蛋白質結構特徵以進行藥物標靶親和力預測的架構。我們使用胺基酸殘基位置坐標為蛋白質 3D 結構產生圖形表示，並將它們與藥物圖形表示和傳統特徵結合，以共同學習藥物標靶親和力。我們的模型在 KIBA 資料集上達到 0.1378 的平均平方誤差和 0.8929 的一致性指標，並優於僅使用傳統蛋白質特徵。我們的消融研究表明，基於 3D 蛋白質結構的特徵提供了與傳統特徵互補的資訊。

##### **DDFAD: Dataset Distillation Framework for Audio Data**
2407.10446v1 by Wenbo Jiang, Rui Zhang, Hongwei Li, Xiaoyuan Liu, Haomiao Yang, Shui Yu

Deep neural networks (DNNs) have achieved significant success in numerous
applications. The remarkable performance of DNNs is largely attributed to the
availability of massive, high-quality training datasets. However, processing
such massive training data requires huge computational and storage resources.
Dataset distillation is a promising solution to this problem, offering the
capability to compress a large dataset into a smaller distilled dataset. The
model trained on the distilled dataset can achieve comparable performance to
the model trained on the whole dataset.
  While dataset distillation has been demonstrated in image data, none have
explored dataset distillation for audio data. In this work, for the first time,
we propose a Dataset Distillation Framework for Audio Data (DDFAD).
Specifically, we first propose the Fused Differential MFCC (FD-MFCC) as
extracted features for audio data. After that, the FD-MFCC is distilled through
the matching training trajectory distillation method. Finally, we propose an
audio signal reconstruction algorithm based on the Griffin-Lim Algorithm to
reconstruct the audio signal from the distilled FD-MFCC. Extensive experiments
demonstrate the effectiveness of DDFAD on various audio datasets. In addition,
we show that DDFAD has promising application prospects in many applications,
such as continual learning and neural architecture search.

摘要：深度神經網路 (DNN) 在許多應用程式中獲得顯著的成功。DNN 的卓越效能主要歸功於大量、高品質訓練資料集的可用性。然而，處理如此大量的訓練資料需要龐大的運算和儲存資源。資料集萃取是解決這個問題的一個有前途的方案，它提供將大型資料集壓縮成較小的萃取資料集的能力。在萃取資料集上訓練的模型可以達到與在整個資料集上訓練的模型相當的效能。
雖然資料集萃取已在影像資料中得到驗證，但沒有人探索過音訊資料的資料集萃取。在這項工作中，我們首次提出音訊資料的資料集萃取架構 (DDFAD)。具體來說，我們首先提出融合差分 MFCC (FD-MFCC) 作為音訊資料的萃取特徵。在那之後，FD-MFCC 透過匹配訓練軌跡萃取方法進行萃取。最後，我們提出一個基於 Griffin-Lim 演算法的音訊訊號重建演算法，從萃取的 FD-MFCC 重建音訊訊號。廣泛的實驗證明了 DDFAD 在各種音訊資料集上的有效性。此外，我們展示了 DDFAD 在許多應用程式中具有有前途的應用前景，例如持續學習和神經架構搜尋。

##### **A Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT**
2407.10433v1 by Chunshi Wang, Bin Zhao, Shuxue Ding

Cone beam computed tomography (CBCT) is a common way of diagnosing dental
related diseases. Accurate segmentation of 3D tooth is of importance for the
treatment. Although deep learning based methods have achieved convincing
results in medical image processing, they need a large of annotated data for
network training, making it very time-consuming in data collection and
annotation. Besides, domain shift widely existing in the distribution of data
acquired by different devices impacts severely the model generalization. To
resolve the problem, we propose a multi-stage framework for 3D tooth
segmentation in dental CBCT, which achieves the third place in the
"Semi-supervised Teeth Segmentation" 3D (STS-3D) challenge. The experiments on
validation set compared with other semi-supervised segmentation methods further
indicate the validity of our approach.

摘要：錐狀光束電腦斷層掃描 (CBCT) 是一種常見的牙科相關疾病診斷方式。3D 牙齒的精確分割對於治療至關重要。儘管基於深度學習的方法在醫學影像處理中已取得令人信服的成果，但它們需要大量的註解資料進行網路訓練，這使得資料收集和註解非常耗時。此外，在不同裝置取得的資料分佈中廣泛存在的領域轉移會嚴重影響模型的泛化能力。為了解決這個問題，我們提出了一個多階段架構，用於牙科 CBCT 中的 3D 牙齒分割，在「半監督牙齒分割」3D (STS-3D) 挑戰中獲得第三名。與其他半監督分割方法相比，在驗證集上的實驗進一步證明了我們方法的有效性。

##### **Expanding the Scope: Inductive Knowledge Graph Reasoning with Multi-Starting Progressive Propagation**
2407.10430v1 by Zhoutian Shao, Yuanning Cui, Wei Hu

Knowledge graphs (KGs) are widely acknowledged as incomplete, and new
entities are constantly emerging in the real world. Inductive KG reasoning aims
to predict missing facts for these new entities. Among existing models, graph
neural networks (GNNs) based ones have shown promising performance for this
task. However, they are still challenged by inefficient message propagation due
to the distance and scalability issues. In this paper, we propose a new
inductive KG reasoning model, MStar, by leveraging conditional message passing
neural networks (C-MPNNs). Our key insight is to select multiple query-specific
starting entities to expand the scope of progressive propagation. To propagate
query-related messages to a farther area within limited steps, we subsequently
design a highway layer to propagate information toward these selected starting
entities. Moreover, we introduce a training strategy called LinkVerify to
mitigate the impact of noisy training samples. Experimental results validate
that MStar achieves superior performance compared with state-of-the-art models,
especially for distant entities.

摘要：知識圖譜 (KG) 廣泛被認為是不完整的，而且在現實世界中不斷出現新的實體。感應 KG 推理旨在預測這些新實體的缺失事實。在現有模型中，基於圖神經網路 (GNN) 的模型已顯示出對此任務有望的效能。然而，由於距離和可擴充性的問題，它們仍受到訊息傳播效率不佳的挑戰。在本文中，我們透過利用條件訊息傳遞神經網路 (C-MPNN) 提出了一個新的感應 KG 推理模型 MStar。我們的關鍵見解是選擇多個查詢特定的起始實體來擴展漸進式傳播的範圍。為了在有限的步驟內將查詢相關訊息傳播到更遠的區域，我們隨後設計了一個高速公路層來將資訊傳播到這些選定的起始實體。此外，我們引入了一個名為 LinkVerify 的訓練策略來減輕雜訊訓練樣本的影響。實驗結果驗證了 MStar 與最先進的模型相比，特別是對於遠距離實體，達到了優異的效能。

##### **CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization**
2407.10424v2 by Yang Zhao, Di Huang, Chongxiao Li, Pengwei Jin, Ziyuan Nan, Tianyun Ma, Lei Qi, Yansong Pan, Zhenxing Zhang, Rui Zhang, Xishan Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen

The increasing complexity and high costs associated with modern processor
design have led to a surge in demand for processor design automation.
Instruction-tuned large language models (LLMs) have demonstrated remarkable
performance in automatically generating code for general-purpose programming
languages like Python. However, these methods fail on hardware description
languages (HDLs) like Verilog due to the scarcity of high-quality instruction
tuning data, as even advanced LLMs like GPT-3.5 exhibit limited performance on
Verilog generation. Regarding this issue, we observe that (1) Verilog code
collected from the real world has higher quality than those generated by LLMs.
(2) LLMs like GPT-3.5 excel in summarizing Verilog code rather than generating
it. Based on these observations, this paper introduces CodeV, a series of
open-source instruction-tuned Verilog generation LLMs. Instead of generating
descriptions first and then getting the corresponding code from advanced LLMs,
we prompt the LLM with Verilog code and let the LLM generate the corresponding
natural language description by multi-level summarization. Experimental results
show that CodeV relatively surpasses the previous open-source SOTA by 14.4%
(BetterV in VerilogEval) and 11.3% (RTLCoder in RTLLM) respectively, and also
relatively outperforms previous commercial SOTA GPT-4 by 22.1% in VerilogEval.

摘要：隨著現代處理器設計的複雜性與高成本日益增加，對於處理器設計自動化的需求也隨之激增。指令調整的大型語言模型 (LLM) 已展現出驚人的效能，能自動為 Python 等通用程式語言產生程式碼。然而，這些方法無法用於像 Verilog 等硬體描述語言 (HDL)，因為缺乏高品質的指令調整資料，即使是像 GPT-3.5 等進階 LLM 在產生 Verilog 上也表現有限。針對此問題，我們觀察到 (1) 從真實世界收集的 Verilog 程式碼品質高於 LLM 所產生的程式碼。(2) 像 GPT-3.5 等 LLM 擅長總結 Verilog 程式碼，而非產生程式碼。基於這些觀察，本文介紹 CodeV，一系列開源的指令調整 Verilog 產生 LLM。我們並非先產生描述，再從進階 LLM 取得對應的程式碼，而是提示 LLM 使用 Verilog 程式碼，並讓 LLM 透過多層次摘要產生對應的自然語言描述。實驗結果顯示，CodeV 分別相對超越先前的開源 SOTA 14.4%（VerilogEval 中的 BetterV）和 11.3%（RTLLM 中的 RTLCoder），在 VerilogEval 中也相對優於先前的商業 SOTA GPT-4 22.1%。

##### **Melon Fruit Detection and Quality Assessment Using Generative AI-Based Image Data Augmentation**
2407.10413v1 by Seungri Yoon, Yunseong Cho, Tae In Ahn

Monitoring and managing the growth and quality of fruits are very important
tasks. To effectively train deep learning models like YOLO for real-time fruit
detection, high-quality image datasets are essential. However, such datasets
are often lacking in agriculture. Generative AI models can help create
high-quality images. In this study, we used MidJourney and Firefly tools to
generate images of melon greenhouses and post-harvest fruits through
text-to-image, pre-harvest image-to-image, and post-harvest image-to-image
methods. We evaluated these AIgenerated images using PSNR and SSIM metrics and
tested the detection performance of the YOLOv9 model. We also assessed the net
quality of real and generated fruits. Our results showed that generative AI
could produce images very similar to real ones, especially for post-harvest
fruits. The YOLOv9 model detected the generated images well, and the net
quality was also measurable. This shows that generative AI can create realistic
images useful for fruit detection and quality assessment, indicating its great
potential in agriculture. This study highlights the potential of AI-generated
images for data augmentation in melon fruit detection and quality assessment
and envisions a positive future for generative AI applications in agriculture.

摘要：監控和管理水果的生長和品質是非常重要的任務。為了有效訓練深度學習模型（例如 YOLO）進行即時水果檢測，高品質的影像資料集至關重要。然而，此類資料集在農業領域往往有所欠缺。生成式 AI 模型有助於建立高品質的影像。在本研究中，我們使用 MidJourney 和 Firefly 工具透過文字轉影像、採收前影像轉影像和採收後影像轉影像方法來產生網室香瓜和採收後水果的影像。我們使用 PSNR 和 SSIM 指標評估這些 AI 生成的影像，並測試 YOLOv9 模型的檢測效能。我們也評估了真實和生成水果的淨品質。我們的結果顯示，生成式 AI 能夠產生與真實影像非常相似的影像，特別是對於採收後的水果。YOLOv9 模型對於生成影像的檢測效果良好，且淨品質也具有可測量性。這表示生成式 AI 能夠建立逼真的影像，有助於水果檢測和品質評估，顯示其在農業領域的巨大潛力。本研究強調了 AI 生成的影像在網室香瓜水果檢測和品質評估中資料擴充的潛力，並展望生成式 AI 應用在農業領域的正面未來。

##### **Cooperative Reward Shaping for Multi-Agent Pathfinding**
2407.10403v1 by Zhenyu Song, Ronghao Zheng, Senlin Zhang, Meiqin Liu

The primary objective of Multi-Agent Pathfinding (MAPF) is to plan efficient
and conflict-free paths for all agents. Traditional multi-agent path planning
algorithms struggle to achieve efficient distributed path planning for multiple
agents. In contrast, Multi-Agent Reinforcement Learning (MARL) has been
demonstrated as an effective approach to achieve this objective. By modeling
the MAPF problem as a MARL problem, agents can achieve efficient path planning
and collision avoidance through distributed strategies under partial
observation. However, MARL strategies often lack cooperation among agents due
to the absence of global information, which subsequently leads to reduced MAPF
efficiency. To address this challenge, this letter introduces a unique reward
shaping technique based on Independent Q-Learning (IQL). The aim of this method
is to evaluate the influence of one agent on its neighbors and integrate such
an interaction into the reward function, leading to active cooperation among
agents. This reward shaping method facilitates cooperation among agents while
operating in a distributed manner. The proposed approach has been evaluated
through experiments across various scenarios with different scales and agent
counts. The results are compared with those from other state-of-the-art (SOTA)
planners. The evidence suggests that the approach proposed in this letter
parallels other planners in numerous aspects, and outperforms them in scenarios
featuring a large number of agents.

摘要：多智能體路徑規劃 (MAPF) 的主要目標是規劃所有智能體的高效率且無衝突路徑。傳統的多智能體路徑規劃演算法難以達成多個智能體的高效率分散式路徑規劃。相反地，多智能體強化學習 (MARL) 已被證明是一種達成此目標的有效方法。透過將 MAPF 問題建模為 MARL 問題，智能體可以在部分觀察下透過分散式策略達成高效率的路徑規劃和避開碰撞。然而，由於缺乏全局資訊，MARL 策略通常缺乏智能體之間的合作，這隨後會導致 MAPF 效率降低。為了解決此挑戰，這封信介紹了一種基於獨立 Q 學習 (IQL) 的獨特獎勵塑造技術。此方法的目標是評估一個智能體對其鄰居的影響，並將此類互動整合到獎勵函數中，從而導致智能體之間的積極合作。此獎勵塑造方法促進智能體之間的合作，同時以分散式方式運作。已透過不同規模和智能體計數的各種情境中的實驗評估建議的方法。結果與其他最先進 (SOTA) 規劃器的結果進行比較。證據表明，這封信中提出的方法在許多方面與其他規劃器並駕齊驅，並且在具有大量智能體的情境中表現優於其他規劃器。

##### **Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity**
2407.10387v1 by Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, Joan Serrà

Video-to-audio (V2A) generation leverages visual-only video features to
render plausible sounds that match the scene. Importantly, the generated sound
onsets should match the visual actions that are aligned with them, otherwise
unnatural synchronization artifacts arise. Recent works have explored the
progression of conditioning sound generators on still images and then video
features, focusing on quality and semantic matching while ignoring
synchronization, or by sacrificing some amount of quality to focus on improving
synchronization only. In this work, we propose a V2A generative model, named
MaskVAT, that interconnects a full-band high-quality general audio codec with a
sequence-to-sequence masked generative model. This combination allows modeling
both high audio quality, semantic matching, and temporal synchronicity at the
same time. Our results show that, by combining a high-quality codec with the
proper pre-trained audio-visual features and a sequence-to-sequence parallel
structure, we are able to yield highly synchronized results on one hand, whilst
being competitive with the state of the art of non-codec generative audio
models. Sample videos and generated audios are available at
https://maskvat.github.io .

摘要：視訊轉音訊 (V2A) 產生利用純視覺視訊功能來呈現與場景相符的合理聲音。重要的是，產生的聲音開頭應與與其對齊的視覺動作相符，否則會產生不自然的同步人工製品。最近的研究探討了在靜態影像和視訊功能上對聲音產生器的條件進程，專注於品質和語意匹配，同時忽略同步，或犧牲部分品質來專注於僅改善同步。在這項研究中，我們提出一個 V2A 產生模型，稱為 MaskVAT，它將全頻段高品質一般音訊編解碼器與序列到序列遮罩產生模型互連。這種組合允許同時對高音訊品質、語意匹配和時間同步進行建模。我們的結果顯示，透過將高品質編解碼器與適當的預訓練音訊視覺功能和序列到序列並行結構相結合，我們能夠一方面產生高度同步的結果，同時在非編解碼產生音訊模型的技術水準中具有競爭力。範例視訊和產生的音訊可在 https://maskvat.github.io 取得。

##### **By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting**
2407.10385v1 by Hyungjun Yoon, Biniyam Aschalew Tolera, Taesik Gong, Kimin Lee, Sung-Ju Lee

Large language models (LLMs) have demonstrated exceptional abilities across
various domains. However, utilizing LLMs for ubiquitous sensing applications
remains challenging as existing text-prompt methods show significant
performance degradation when handling long sensor data sequences. We propose a
visual prompting approach for sensor data using multimodal LLMs (MLLMs). We
design a visual prompt that directs MLLMs to utilize visualized sensor data
alongside the target sensory task descriptions. Additionally, we introduce a
visualization generator that automates the creation of optimal visualizations
tailored to a given sensory task, eliminating the need for prior task-specific
knowledge. We evaluated our approach on nine sensory tasks involving four
sensing modalities, achieving an average of 10% higher accuracy than text-based
prompts and reducing token costs by 15.8x. Our findings highlight the
effectiveness and cost-efficiency of visual prompts with MLLMs for various
sensory tasks.

摘要：大型語言模型 (LLM) 已在各個領域展示出非凡的能力。然而，將 LLM 用於普遍感測應用程式仍具有挑戰性，因為現有的文字提示方法在處理長感測器資料序列時會顯著降低效能。我們提出使用多模態 LLM (MLLM) 的感測器資料視覺提示方法。我們設計了一個視覺提示，引導 MLLM 利用視覺化的感測器資料以及目標感測任務說明。此外，我們還引入一個視覺化產生器，用於自動建立針對特定感測任務量身打造的最佳視覺化，無需具備先前的任務特定知識。我們在涉及四種感測模式的九項感測任務上評估了我們的方法，準確度平均比基於文字的提示高出 10%，並將代幣成本降低了 15.8 倍。我們的研究結果突顯了視覺提示與 MLLM 在各種感測任務中的有效性和成本效益。

##### **NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models**
2407.10380v1 by Pranshu Pandya, Agney S Talwarr, Vatsal Gupta, Tushar Kataria, Vivek Gupta, Dan Roth

Cognitive textual and visual reasoning tasks, such as puzzles, series, and
analogies, demand the ability to quickly reason, decipher, and evaluate
patterns both textually and spatially. While LLMs and VLMs, through extensive
training on large amounts of human-curated data, have attained a high level of
pseudo-human intelligence in some common sense reasoning tasks, they still
struggle with more complex reasoning tasks that require cognitive
understanding. In this work, we introduce a new dataset, NTSEBench, designed to
evaluate the cognitive multi-modal reasoning and problem-solving skills of
large models. The dataset comprises 2,728 multiple-choice questions comprising
of a total of 4,642 images across 26 categories sampled from the NTSE
examination conducted nationwide in India, featuring both visual and textual
general aptitude questions that do not rely on rote learning. We establish
baselines on the dataset using state-of-the-art LLMs and VLMs. To facilitate a
comparison between open source and propriety models, we propose four distinct
modeling strategies to handle different modalities (text and images) in the
dataset instances.

摘要：認知文本與視覺推理任務，例如謎題、系列和類比，需要快速推理、解碼和評估文本和空間模式的能力。雖然大型語言模型 (LLM) 和大型視覺模型 (VLM) 透過大量人工整理資料的廣泛訓練，在一些常識推理任務中已達到高度的類人類智慧，但它們在需要認知理解的更複雜推理任務中仍面臨挑戰。在這項工作中，我們引入了一個新的資料集 NTSEBench，旨在評估大型模型的認知多模態推理和問題解決技能。該資料集包含 2,728 個多選題，總共包含來自印度全國 NTSE 考試的 26 個類別的 4,642 張圖片，其中包含視覺和文本的一般能力問題，不依賴死背硬記。我們使用最先進的 LLM 和 VLM 在資料集上建立基準。為了便於在開源模型和專有模型之間進行比較，我們提出了四種不同的建模策略來處理資料集實例中的不同模式（文字和圖像）。

##### **Enhanced Self-supervised Learning for Multi-modality MRI Segmentation and Classification: A Novel Approach Avoiding Model Collapse**
2407.10377v1 by Linxuan Han, Sa Xiao, Zimeng Li, Haidong Li, Xiuchao Zhao, Fumin Guo, Yeqing Han, Xin Zhou

Multi-modality magnetic resonance imaging (MRI) can provide complementary
information for computer-aided diagnosis. Traditional deep learning algorithms
are suitable for identifying specific anatomical structures segmenting lesions
and classifying diseases with magnetic resonance images. However, manual labels
are limited due to high expense, which hinders further improvement of model
accuracy. Self-supervised learning (SSL) can effectively learn feature
representations from unlabeled data by pre-training and is demonstrated to be
effective in natural image analysis. Most SSL methods ignore the similarity of
multi-modality MRI, leading to model collapse. This limits the efficiency of
pre-training, causing low accuracy in downstream segmentation and
classification tasks. To solve this challenge, we establish and validate a
multi-modality MRI masked autoencoder consisting of hybrid mask pattern (HMP)
and pyramid barlow twin (PBT) module for SSL on multi-modality MRI analysis.
The HMP concatenates three masking steps forcing the SSL to learn the semantic
connections of multi-modality images by reconstructing the masking patches. We
have proved that the proposed HMP can avoid model collapse. The PBT module
exploits the pyramidal hierarchy of the network to construct barlow twin loss
between masked and original views, aligning the semantic representations of
image patches at different vision scales in latent space. Experiments on
BraTS2023, PI-CAI, and lung gas MRI datasets further demonstrate the
superiority of our framework over the state-of-the-art. The performance of the
segmentation and classification is substantially enhanced, supporting the
accurate detection of small lesion areas. The code is available at
https://github.com/LinxuanHan/M2-MAE.

摘要：多模態磁共振成像 (MRI) 能為電腦輔助診斷提供互補資訊。傳統深度學習演算法適用於識別特定的解剖結構、分割病灶並使用磁共振影像對疾病進行分類。然而，手動標籤由於費用高昂而受到限制，這阻礙了模型精確度的進一步提升。自監督學習 (SSL) 能透過預訓練有效地從未標籤的資料中學習特徵表徵，並證明其在自然影像分析中是有效的。大多數 SSL 方法忽略了多模態 MRI 的相似性，導致模型崩潰。這限制了預訓練的效率，導致下游分割和分類任務的準確度降低。為了解決這個挑戰，我們建立並驗證了一個多模態 MRI 遮罩自動編碼器，它由混合遮罩模式 (HMP) 和金字塔 Barlow 雙胞胎 (PBT) 模組組成，用於多模態 MRI 分析的 SSL。HMP 串接三個遮罩步驟，強迫 SSL 透過重建遮罩貼片來學習多模態影像的語義連接。我們已經證明，所提出的 HMP 能避免模型崩潰。PBT 模組利用網路的金字塔層級來建構遮罩和原始檢視之間的 Barlow 雙胞胎損失，在潛在空間中對齊不同視覺尺度影像貼片的語義表徵。在 BraTS2023、PI-CAI 和肺部氣體 MRI 資料集上的實驗進一步證明了我們框架優於現有技術。分割和分類的效能顯著提升，支援準確偵測小病灶區域。程式碼可在 https://github.com/LinxuanHan/M2-MAE 取得。

##### **Large Language Model-based FMRI Encoding of Language Functions for Subjects with Neurocognitive Disorder**
2407.10376v1 by Yuejiao Wang, Xianmin Gong, Lingwei Meng, Xixin Wu, Helen Meng

Functional magnetic resonance imaging (fMRI) is essential for developing
encoding models that identify functional changes in language-related brain
areas of individuals with Neurocognitive Disorders (NCD). While large language
model (LLM)-based fMRI encoding has shown promise, existing studies
predominantly focus on healthy, young adults, overlooking older NCD populations
and cognitive level correlations. This paper explores language-related
functional changes in older NCD adults using LLM-based fMRI encoding and brain
scores, addressing current limitations. We analyze the correlation between
brain scores and cognitive scores at both whole-brain and language-related ROI
levels. Our findings reveal that higher cognitive abilities correspond to
better brain scores, with correlations peaking in the middle temporal gyrus.
This study highlights the potential of fMRI encoding models and brain scores
for detecting early functional changes in NCD patients.

摘要：功能性磁振造影 (fMRI) 對於開發編碼模型至關重要，該模型可以識別神經認知障礙 (NCD) 患者語言相關腦區的功能變化。雖然基於大型語言模型 (LLM) 的 fMRI 編碼已顯示出前景，但現有研究主要關注健康年輕的成年人，忽視了較年長的 NCD 族群和認知層次相關性。本文使用基於 LLM 的 fMRI 編碼和腦分數探討了老年 NCD 成人的語言相關功能變化，以解決當前的限制。我們分析了全腦和語言相關 ROI 層級的腦分數與認知分數之間的相關性。我們的研究結果顯示，較高的認知能力對應於較好的腦分數，相關性在中顳回達到最高峰。這項研究強調了 fMRI 編碼模型和腦分數在偵測 NCD 患者早期功能變化方面的潛力。

##### **An Empirical Study of Mamba-based Pedestrian Attribute Recognition**
2407.10374v1 by Xiao Wang, Weizhe Kong, Jiandong Jin, Shiao Wang, Ruichong Gao, Qingchuan Ma, Chenglong Li, Jin Tang

Current strong pedestrian attribute recognition models are developed based on
Transformer networks, which are computationally heavy. Recently proposed models
with linear complexity (e.g., Mamba) have garnered significant attention and
have achieved a good balance between accuracy and computational cost across a
variety of visual tasks. Relevant review articles also suggest that while these
models can perform well on some pedestrian attribute recognition datasets, they
are generally weaker than the corresponding Transformer models. To further tap
into the potential of the novel Mamba architecture for PAR tasks, this paper
designs and adapts Mamba into two typical PAR frameworks, i.e., the text-image
fusion approach and pure vision Mamba multi-label recognition framework. It is
found that interacting with attribute tags as additional input does not always
lead to an improvement, specifically, Vim can be enhanced, but VMamba cannot.
This paper further designs various hybrid Mamba-Transformer variants and
conducts thorough experimental validations. These experimental results indicate
that simply enhancing Mamba with a Transformer does not always lead to
performance improvements but yields better results under certain settings. We
hope this empirical study can further inspire research in Mamba for PAR, and
even extend into the domain of multi-label recognition, through the design of
these network structures and comprehensive experimentation. The source code of
this work will be released at \url{https://github.com/Event-AHU/OpenPAR}

摘要：當前強大的行人屬性識別模型是基於 Transformer 網路開發的，這在計算上很吃重。最近提出的線性複雜度模型（例如 Mamba）引起了極大的關注，並在各種視覺任務中達到了準確性和計算成本之間的良好平衡。相關的評論文章也表明，儘管這些模型可以在一些行人屬性識別數據集上表現良好，但它們通常比對應的 Transformer 模型弱。為了進一步發揮新穎的 Mamba 架構在 PAR 任務中的潛力，本文設計並將 Mamba 適應到兩個典型的 PAR 框架中，即文本圖像融合方法和純視覺 Mamba 多標籤識別框架。發現與屬性標籤作為額外輸入進行交互並不總是會帶來改進，具體來說，可以增強 Vim，但不能增強 VMamba。本文進一步設計了各種混合 Mamba-Transformer 變體，並進行了徹底的實驗驗證。這些實驗結果表明，僅使用 Transformer 增強 Mamba 並不總是會導致性能提升，但在某些設置下會產生更好的結果。我們希望這項實證研究能進一步激勵 Mamba 在 PAR 中的研究，甚至通過設計這些網路結構和綜合實驗，擴展到多標籤識別領域。這項工作的源代碼將在 \url{https://github.com/Event-AHU/OpenPAR} 發布

##### **Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion**
2407.10373v1 by Jian Ma, Wenguan Wang, Yi Yang, Feng Zheng

Visual acoustic matching (VAM) is pivotal for enhancing the immersive
experience, and the task of dereverberation is effective in improving audio
intelligibility. Existing methods treat each task independently, overlooking
the inherent reciprocity between them. Moreover, these methods depend on paired
training data, which is challenging to acquire, impeding the utilization of
extensive unpaired data. In this paper, we introduce MVSD, a mutual learning
framework based on diffusion models. MVSD considers the two tasks
symmetrically, exploiting the reciprocal relationship to facilitate learning
from inverse tasks and overcome data scarcity. Furthermore, we employ the
diffusion model as foundational conditional converters to circumvent the
training instability and over-smoothing drawbacks of conventional GAN
architectures. Specifically, MVSD employs two converters: one for VAM called
reverberator and one for dereverberation called dereverberator. The
dereverberator judges whether the reverberation audio generated by reverberator
sounds like being in the conditional visual scenario, and vice versa. By
forming a closed loop, these two converters can generate informative feedback
signals to optimize the inverse tasks, even with easily acquired one-way
unpaired data. Extensive experiments on two standard benchmarks, i.e.,
SoundSpaces-Speech and Acoustic AVSpeech, exhibit that our framework can
improve the performance of the reverberator and dereverberator and better match
specified visual scenarios.

摘要：視覺聲學匹配 (VAM) 對於增強沈浸式體驗至關重要，而消除殘響的任務對於提高音訊清晰度有效。現有方法將每個任務獨立處理，忽略它們之間固有的互惠性。此外，這些方法依賴於成對的訓練資料，這很難獲取，阻礙了廣泛未配對資料的利用。在本文中，我們介紹了 MVSD，一個基於擴散模型的相互學習框架。MVSD 對稱地考慮這兩個任務，利用互惠關係來促進從逆任務中學習並克服資料稀缺。此外，我們採用擴散模型作為基礎條件轉換器，以規避傳統 GAN 架構的訓練不穩定性和過度平滑的缺點。具體來說，MVSD 採用兩個轉換器：一個用於 VAM，稱為混響器，另一個用於消除殘響，稱為消除混響器。消除混響器判斷混響器產生的混響音訊是否聽起來像在條件視覺場景中，反之亦然。通過形成一個閉環，這兩個轉換器可以產生資訊性的回饋訊號來最佳化逆任務，即使使用容易獲得的單向未配對資料。在兩個標準基準上進行的廣泛實驗，即 SoundSpaces-Speech 和 Acoustic AVSpeech，表明我們的框架可以提高混響器和消除混響器的效能，並更好地匹配指定的視覺場景。

##### **Accessing Vision Foundation Models at ImageNet-level Costs**
2407.10366v1 by Yitian Zhang, Xu Ma, Yue Bai, Huan Wang, Yun Fu

Vision foundation models are renowned for their generalization ability due to
massive training data. Nevertheless, they demand tremendous training resources,
and the training data is often inaccessible, e.g., CLIP, DINOv2, posing great
challenges to developing derivatives that could advance research in this field.
In this work, we offer a very simple and general solution, named Proteus, to
distill foundation models into smaller equivalents on ImageNet-1K without
access to the original training data. Specifically, we remove the designs from
conventional knowledge distillation settings that result in dataset bias and
present three levels of training objectives, i.e., token, patch, and feature,
to maximize the efficacy of knowledge transfer. In this manner, Proteus is
trained at ImageNet-level costs with surprising ability, facilitating the
accessibility of training foundation models for the broader research community.
Leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the performance of
the Oracle method DINOv2-L/14 (142M training data) across 15 benchmarks and
outperforms other vision foundation models including CLIP-L/14 (400M),
OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M).

摘要：視覺基礎模型由於具有龐大的訓練資料，而以其泛化能力聞名。儘管如此，它們需要大量的訓練資源，而訓練資料通常無法取得，例如 CLIP、DINOv2，對發展可以推進此領域研究的衍生品構成極大的挑戰。在這項工作中，我們提供了一個非常簡單且通用的解決方案，稱為 Proteus，可以在沒有原始訓練資料的情況下，將基礎模型提煉成 ImageNet-1K 上較小的等價物。具體來說，我們從傳統知識提煉設定中移除導致資料集偏差的設計，並提出三層訓練目標，即 token、patch 和特徵，以最大化知識傳輸的功效。透過這種方式，Proteus 以驚人的能力在 ImageNet 等級的成本下進行訓練，有助於更廣泛的研究社群取得訓練基礎模型的管道。利用 DINOv2-g/14 作為教師，Proteus-L/14 在 15 個基準測試中與 Oracle 方法 DINOv2-L/14（142M 訓練資料）的效能相匹配，並且優於其他視覺基礎模型，包括 CLIP-L/14（400M）、OpenCLIP-L/14（400M/2B）和 SynCLR-L/14（600M）。

##### **LAB-Bench: Measuring Capabilities of Language Models for Biology Research**
2407.10362v1 by Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, Samuel G. Rodriques

There is widespread optimism that frontier Large Language Models (LLMs) and
LLM-augmented systems have the potential to rapidly accelerate scientific
discovery across disciplines. Today, many benchmarks exist to measure LLM
knowledge and reasoning on textbook-style science questions, but few if any
benchmarks are designed to evaluate language model performance on practical
tasks required for scientific research, such as literature search, protocol
planning, and data analysis. As a step toward building such benchmarks, we
introduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset of
over 2,400 multiple choice questions for evaluating AI systems on a range of
practical biology research capabilities, including recall and reasoning over
literature, interpretation of figures, access and navigation of databases, and
comprehension and manipulation of DNA and protein sequences. Importantly, in
contrast to previous scientific benchmarks, we expect that an AI system that
can achieve consistently high scores on the more difficult LAB-Bench tasks
would serve as a useful assistant for researchers in areas such as literature
search and molecular cloning. As an initial assessment of the emergent
scientific task capabilities of frontier language models, we measure
performance of several against our benchmark and report results compared to
human expert biology researchers. We will continue to update and expand
LAB-Bench over time, and expect it to serve as a useful tool in the development
of automated research systems going forward. A public subset of LAB-Bench is
available for use at the following URL:
https://huggingface.co/datasets/futurehouse/lab-bench

摘要：語言大型模型（LLM）和 LLM 增強系統普遍被樂觀認為有潛力能快速加速各領域的科學發現。現今，許多基準存在用於衡量 LLM 知識和教科書式科學問題的推理，但很少有基準被設計用於評估語言模型在科學研究中所需實際任務上的表現，例如文獻搜尋、實驗規畫和資料分析。作為建構此類基準的第一步，我們引入了語言代理生物基準（LAB-Bench），一個包含超過 2,400 個多選題的廣泛資料集，用於評估 AI 系統在各種實際生物研究能力上的表現，包括文獻的回溯和推理、圖表的解讀、資料庫的存取和導覽，以及 DNA 和蛋白質序列的理解和操作。重要的是，與先前的科學基準相比，我們預期一個能在較困難的 LAB-Bench 任務中持續獲得高分的 AI 系統，將能成為研究人員在文獻搜尋和分子複製等領域中的有用助手。作為對前沿語言模型新興科學任務能力的初步評估，我們測量了幾個模型針對我們基準的表現，並回報與人類專家生物研究人員比較後的結果。我們將持續更新和擴充 LAB-Bench，並預期它將成為未來自動化研究系統開發中的一個有用工具。LAB-Bench 的公開子集可於以下網址使用：
https://huggingface.co/datasets/futurehouse/lab-bench

##### **Evolved Developmental Artificial Neural Networks for Multitasking with Advanced Activity Dependence**
2407.10359v1 by Yintong Zhang, Jason A. Yoder

Recently, Cartesian Genetic Programming has been used to evolve developmental
programs to guide the formation of artificial neural networks (ANNs). This
approach has demonstrated success in enabling ANNs to perform multiple tasks
while avoiding catastrophic forgetting. One unique aspect of this approach is
the use of separate developmental programs evolved to regulate the development
of separate soma and dendrite units. An opportunity afforded by this approach
is the ability to incorporate Activity Dependence (AD) into the model such that
environmental feedback can help to regulate the behavior of each type of unit.
Previous work has shown a limited version of AD (influencing neural bias) to
provide marginal improvements over non-AD ANNs. In this work, we present
promising results from new extensions to AD. Specifically, we demonstrate a
more significant improvement via AD on new neural parameters including health
and position, as well as a combination of all of these along with bias. We
report on the implications of this work and suggest several promising
directions for future work.

摘要：最近，笛卡尔遗传规划已被用于进化发育程序，以指导人工神经网络 (ANN) 的形成。这种方法已证明能够让 ANN 执行多项任务，同时避免灾难性遗忘。这种方法的一个独特方面是使用单独的发展程序来调节单独的躯体和树突单元的发展。这种方法提供了一个机会，即能够将活动依赖性 (AD) 纳入模型，以便环境反馈可以帮助调节每种类型的单元的行为。以前的工作已经展示了 AD 的一个有限版本（影响神经偏置），以提供对非 AD ANN 的边际改进。在这项工作中，我们展示了 AD 新扩展的令人鼓舞的结果。具体来说，我们通过 AD 在新的神经参数（包括健康和位置）以及所有这些参数与偏置的组合上展示了更显着的改进。我们报告了这项工作的影响，并为未来的工作提出了几个有希望的方向。

##### **Comparing Complex Concepts with Transformers: Matching Patent Claims Against Natural Language Text**
2407.10351v1 by Matthias Blume, Ghobad Heidari, Christoph Hewel

A key capability in managing patent applications or a patent portfolio is
comparing claims to other text, e.g. a patent specification. Because the
language of claims is different from language used elsewhere in the patent
application or in non-patent text, this has been challenging for computer based
natural language processing. We test two new LLM-based approaches and find that
both provide substantially better performance than previously published values.
The ability to match dense information from one domain against much more
distributed information expressed in a different vocabulary may also be useful
beyond the intellectual property space.

摘要：專利申請或專利組合管理中的一個關鍵功能是將權利要求與其他文字（例如專利說明書）進行比較。由於權利要求的語言與專利申請書的其他部分或非專利文字中使用的語言不同，這一直是基於電腦的自然語言處理的挑戰。我們測試了兩種新的基於 LLM 的方法，發現它們都比以前發布的值提供了顯著更好的效能。將來自一個領域的密集資訊與用不同詞彙表達的更分散的資訊進行匹配的能力也可能在智慧財產權領域之外有用。

##### **MambaForGCN: Enhancing Long-Range Dependency with State Space Model and Kolmogorov-Arnold Networks for Aspect-Based Sentiment Analysis**
2407.10347v1 by Adamu Lawan, Juhua Pu, Haruna Yunusa, Aliyu Umar, Muhammad Lawan

Aspect-based sentiment Analysis (ABSA) identifies and evaluates sentiments
toward specific aspects of entities within text, providing detailed insights
beyond overall sentiment. However, Attention mechanisms and neural network
models struggle with syntactic constraints, and the quadratic complexity of
attention mechanisms hinders their adoption for capturing long-range
dependencies between aspect and opinion words in ABSA. This complexity can lead
to the misinterpretation of irrelevant con-textual words, restricting their
effectiveness to short-range dependencies. Some studies have investigated
merging semantic and syntactic approaches but face challenges in effectively
integrating these methods. To address the above problems, we present
MambaForGCN, a novel approach to enhance short and long-range dependencies
between aspect and opinion words in ABSA. This innovative approach incorporates
syntax-based Graph Convolutional Network (SynGCN) and MambaFormer
(Mamba-Transformer) modules to encode input with dependency relations and
semantic information. The Multihead Attention (MHA) and Mamba blocks in the
MambaFormer module serve as channels to enhance the model with short and
long-range dependencies between aspect and opinion words. We also introduce the
Kolmogorov-Arnold Networks (KANs) gated fusion, an adaptively integrated
feature representation system combining SynGCN and MambaFormer representations.
Experimental results on three benchmark datasets demonstrate MambaForGCN's
effectiveness, outperforming state-of-the-art (SOTA) baseline models.

摘要：<paragraph>基於面向方面的意見分析 (ABSA) 識別和評估文本中實體特定方面的意見，提供超越整體意見的詳細見解。然而，注意力機制和神經網路模型在語法約束方面面臨困難，而注意力機制的二次複雜度阻礙了它們用於捕捉 ABSA 中方面和意見詞之間的長程依賴關係。這種複雜度可能導致對無關上下文詞彙的錯誤解讀，限制其有效性僅限於短程依賴關係。一些研究探討了語義和語法方法的合併，但在有效整合這些方法方面面臨挑戰。為了解決上述問題，我們提出了 MambaForGCN，這是一種增強 ABSA 中方面和意見詞之間的短程和長程依賴關係的新方法。這種創新方法結合了基於語法的圖形卷積網路 (SynGCN) 和 MambaFormer（Mamba-Transformer）模組，以對輸入編碼，並帶有依賴關係和語義資訊。MambaFormer 模組中的多頭注意力 (MHA) 和 Mamba 區塊作為通道，以增強模型與方面和意見詞之間的短程和長程依賴關係。我們還引入了 Kolmogorov-Arnold 網路 (KAN) 門控融合，這是一種自適應整合的特徵表示系統，結合了 SynGCN 和 MambaFormer 表示。在三個基準資料集上的實驗結果證明了 MambaForGCN 的有效性，其表現優於最先進 (SOTA) 的基準模型。</paragraph>

##### **Affordance-Guided Reinforcement Learning via Visual Prompting**
2407.10341v1 by Olivia Y. Lee, Annie Xie, Kuan Fang, Karl Pertsch, Chelsea Finn

Robots equipped with reinforcement learning (RL) have the potential to learn
a wide range of skills solely from a reward signal. However, obtaining a robust
and dense reward signal for general manipulation tasks remains a challenge.
Existing learning-based approaches require significant data, such as
demonstrations or examples of success and failure, to learn task-specific
reward functions. Recently, there is also a growing adoption of large
multi-modal foundation models for robotics. These models can perform visual
reasoning in physical contexts and generate coarse robot motions for various
manipulation tasks. Motivated by this range of capability, in this work, we
propose and study rewards shaped by vision-language models (VLMs).
State-of-the-art VLMs have demonstrated an impressive ability to reason about
affordances through keypoints in zero-shot, and we leverage this to define
dense rewards for robotic learning. On a real-world manipulation task specified
by natural language description, we find that these rewards improve the sample
efficiency of autonomous RL and enable successful completion of the task in 20K
online finetuning steps. Additionally, we demonstrate the robustness of the
approach to reductions in the number of in-domain demonstrations used for
pretraining, reaching comparable performance in 35K online finetuning steps.

摘要：配備強化學習 (RL) 的機器人有潛力僅從回饋訊號中學習廣泛的技能。然而，為一般操作任務取得穩健且密集的回饋訊號仍然是一項挑戰。現有的基於學習的方法需要大量的資料，例如展示或成功與失敗的範例，才能學習特定於任務的回饋函數。最近，大型多模態基礎模型在機器人技術的採用也日益增加。這些模型可以在物理背景中執行視覺推理，並為各種操作任務產生粗略的機器人動作。受此能力範圍的啟發，在這項工作中，我們提出並研究由視覺語言模型 (VLM) 塑造的回饋。最先進的 VLM 已展現出令人印象深刻的能力，可以透過零次學習中的關鍵點來推論可負擔性，我們利用這一點來定義機器人學習的密集回饋。在由自然語言描述指定的真實世界操作任務中，我們發現這些回饋提高了自主 RL 的取樣效率，並能在 20K 線上微調步驟中成功完成任務。此外，我們展示了該方法對用於預訓練的領域內示範次數減少的穩健性，在 35K 線上微調步驟中達到相當的效能。

