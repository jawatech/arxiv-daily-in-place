{"2406.09067": {"publish_time": "2024-06-13", "title": "How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models", "paper_summary": "Forming and using symbol-like structured representations for reasoning has\nbeen considered essential for generalising over novel inputs. The primary tool\nthat allows generalisation outside training data distribution is the ability to\nabstract away irrelevant information into a compact form relevant to the task.\nAn extreme form of such abstract representations is symbols. Humans make use of\nsymbols to bind information while abstracting away irrelevant parts to utilise\nthe information consistently and meaningfully. This work estimates the state of\nsuch structured representations in vision encoders. Specifically, we evaluate\nimage encoders in large vision-language pre-trained models to address the\nquestion of which desirable properties their representations lack by applying\nthe criteria of symbolic structured reasoning described for LLMs to the image\nmodels. We test the representation space of image encoders like VIT, BLIP,\nCLIP, and FLAVA to characterise the distribution of the object representations\nin these models. In particular, we create decoding tasks using multi-object\nscenes from the COCO dataset, relating the token space to its input content for\nvarious objects in the scene. We use these tasks to characterise the network's\ntoken and layer-wise information modelling. Our analysis highlights that the\nCLS token, used for the downstream task, only focuses on a few objects\nnecessary for the trained downstream task. Still, other individual objects are\nwell-modelled separately by the tokens in the network originating from those\nobjects. We further observed a widespread distribution of scene information.\nThis demonstrates that information is far more entangled in tokens than optimal\nfor representing objects similar to symbols. Given these symbolic properties,\nwe show the network dynamics that cause failure modes of these models on basic\ndownstream tasks in a multi-object scene.", "paper_summary_zh": "<paragraph>\u5f62\u6210\u548c\u4f7f\u7528\u7b26\u53f7\u72b6\u7684\u7ed3\u6784\u5316\u8868\u5f81\u8fdb\u884c\u63a8\u7406\u88ab\u8ba4\u4e3a\u5bf9\u4e8e\u6982\u62ec\u65b0\u9896\u7684\u8f93\u5165\u81f3\u5173\u91cd\u8981\u3002\u5141\u8bb8\u5728\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e4b\u5916\u8fdb\u884c\u6982\u62ec\u7684\u4e3b\u8981\u5de5\u5177\u662f\u5c06\u4e0d\u76f8\u5173\u4fe1\u606f\u62bd\u8c61\u6210\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u7d27\u51d1\u5f62\u5f0f\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u62bd\u8c61\u8868\u5f81\u7684\u6781\u7aef\u5f62\u5f0f\u662f\u7b26\u53f7\u3002\u4eba\u7c7b\u5229\u7528\u7b26\u53f7\u6765\u7ed1\u5b9a\u4fe1\u606f\uff0c\u540c\u65f6\u62bd\u8c61\u6389\u4e0d\u76f8\u5173\u90e8\u5206\uff0c\u4ee5\u4fbf\u4e00\u81f4\u4e14\u6709\u610f\u4e49\u5730\u5229\u7528\u4fe1\u606f\u3002\u8fd9\u9879\u5de5\u4f5c\u4f30\u8ba1\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u8fd9\u79cd\u7ed3\u6784\u5316\u8868\u5f81\u7684\u72b6\u6001\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u4ee5\u89e3\u51b3\u5176\u8868\u5f81\u7f3a\u4e4f\u54ea\u4e9b\u7406\u60f3\u5c5e\u6027\u7684\u95ee\u9898\uff0c\u65b9\u6cd5\u662f\u5c06\u9488\u5bf9 LLM \u63cf\u8ff0\u7684\u7b26\u53f7\u7ed3\u6784\u5316\u63a8\u7406\u6807\u51c6\u5e94\u7528\u4e8e\u56fe\u50cf\u6a21\u578b\u3002\u6211\u4eec\u6d4b\u8bd5\u4e86 VIT\u3001BLIP\u3001CLIP \u548c FLAVA \u7b49\u56fe\u50cf\u7f16\u7801\u5668\u7684\u8868\u5f81\u7a7a\u95f4\uff0c\u4ee5\u8868\u5f81\u8fd9\u4e9b\u6a21\u578b\u4e2d\u5bf9\u8c61\u8868\u5f81\u7684\u5206\u5e03\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u4f7f\u7528 COCO \u6570\u636e\u96c6\u4e2d\u7684\u591a\u5bf9\u8c61\u573a\u666f\u521b\u5efa\u89e3\u7801\u4efb\u52a1\uff0c\u5c06\u6807\u8bb0\u7a7a\u95f4\u4e0e\u5176\u8f93\u5165\u5185\u5bb9\u5173\u8054\u8d77\u6765\uff0c\u7528\u4e8e\u573a\u666f\u4e2d\u7684\u5404\u79cd\u5bf9\u8c61\u3002\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u4efb\u52a1\u6765\u8868\u5f81\u7f51\u7edc\u7684\u6807\u8bb0\u548c\u9010\u5c42\u4fe1\u606f\u5efa\u6a21\u3002\u6211\u4eec\u7684\u5206\u6790\u5f3a\u8c03\uff0c\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684 CLS \u6807\u8bb0\u53ea\u5173\u6ce8\u8bad\u7ec3\u7684\u4e0b\u6e38\u4efb\u52a1\u6240\u9700\u7684\u51e0\u4e2a\u5bf9\u8c61\u3002\u4e0d\u8fc7\uff0c\u7f51\u7edc\u4e2d\u6e90\u81ea\u8fd9\u4e9b\u5bf9\u8c61\u7684\u6807\u8bb0\u53ef\u4ee5\u5f88\u597d\u5730\u5355\u72ec\u5bf9\u5176\u4ed6\u5404\u4e2a\u5bf9\u8c61\u8fdb\u884c\u5efa\u6a21\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u89c2\u5bdf\u5230\u573a\u666f\u4fe1\u606f\u7684\u5206\u5e03\u5f88\u5e7f\u6cdb\u3002\u8fd9\u8868\u660e\u4fe1\u606f\u5728\u6807\u8bb0\u4e2d\u7684\u7ea0\u7f20\u7a0b\u5ea6\u8fdc\u9ad8\u4e8e\u4ee5\u7c7b\u4f3c\u4e8e\u7b26\u53f7\u7684\u65b9\u5f0f\u8868\u5f81\u5bf9\u8c61\u7684\u6700\u4f73\u7a0b\u5ea6\u3002\u9274\u4e8e\u8fd9\u4e9b\u7b26\u53f7\u5c5e\u6027\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5bfc\u81f4\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u5bf9\u8c61\u573a\u666f\u4e2d\u7684\u57fa\u672c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u51fa\u73b0\u6545\u969c\u6a21\u5f0f\u7684\u7f51\u7edc\u52a8\u6001\u3002</paragraph>", "author": "Tarun Khajuria et.al.", "authors": "Tarun Khajuria, Braian Olmiro Dias, Jaan Aru", "id": "2406.09067v1", "paper_url": "http://arxiv.org/abs/2406.09067v1", "repo": "null"}}