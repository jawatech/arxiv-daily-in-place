{"2407.16837": {"publish_time": "2024-07-23", "title": "CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs", "paper_summary": "The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping, while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce CompBench, a benchmark designed to evaluate the comparative reasoning\ncapability of multimodal large language models (MLLMs). CompBench mines and\npairs images through visually oriented questions covering eight dimensions of\nrelative comparison: visual attribute, existence, state, emotion, temporality,\nspatiality, quantity, and quality. We curate a collection of around 40K image\npairs using metadata from diverse vision datasets and CLIP similarity scores.\nThese image pairs span a broad array of visual domains, including animals,\nfashion, sports, and both outdoor and indoor scenes. The questions are\ncarefully crafted to discern relative characteristics between two images and\nare labeled by human annotators for accuracy and relevance. We use CompBench to\nevaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our\nresults reveal notable shortcomings in their comparative abilities. We believe\nCompBench not only sheds light on these limitations but also establishes a\nsolid foundation for future enhancements in the comparative capability of\nMLLMs.", "paper_summary_zh": "\u6bd4\u8f03\u7269\u4ef6\u3001\u5834\u666f\u6216\u60c5\u5883\u7684\u80fd\u529b\u5c0d\u65bc\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u6709\u6548\u6c7a\u7b56\u5236\u5b9a\u548c\u554f\u984c\u89e3\u6c7a\u81f3\u95dc\u91cd\u8981\u3002\u4f8b\u5982\uff0c\u6bd4\u8f03\u860b\u679c\u7684\u65b0\u9bae\u5ea6\u53ef\u4ee5\u5728\u8cfc\u8cb7\u96dc\u8ca8\u6642\u505a\u51fa\u66f4\u597d\u7684\u9078\u64c7\uff0c\u800c\u6bd4\u8f03\u6c99\u767c\u8a2d\u8a08\u6709\u52a9\u65bc\u512a\u5316\u6211\u5011\u751f\u6d3b\u7a7a\u9593\u7684\u7f8e\u5b78\u3002\u5118\u7ba1\u5176\u91cd\u8981\u6027\uff0c\u4f46\u6bd4\u8f03\u80fd\u529b\u5728\u4eba\u5de5\u901a\u7528\u667a\u6167 (AGI) \u4e2d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 CompBench\uff0c\u9019\u662f\u4e00\u500b\u57fa\u65bc\u8a55\u4f30\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u6bd4\u8f03\u63a8\u7406\u80fd\u529b\u800c\u8a2d\u8a08\u7684\u57fa\u6e96\u6e2c\u8a66\u3002CompBench \u901a\u904e\u8996\u89ba\u5c0e\u5411\u554f\u984c\u6316\u6398\u548c\u914d\u5c0d\u5716\u50cf\uff0c\u6db5\u84cb\u76f8\u5c0d\u6bd4\u8f03\u7684\u516b\u500b\u7dad\u5ea6\uff1a\u8996\u89ba\u5c6c\u6027\u3001\u5b58\u5728\u3001\u72c0\u614b\u3001\u60c5\u7dd2\u3001\u6642\u9593\u6027\u3001\u7a7a\u9593\u6027\u3001\u6578\u91cf\u548c\u8cea\u91cf\u3002\u6211\u5011\u4f7f\u7528\u4f86\u81ea\u4e0d\u540c\u8996\u89ba\u6578\u64da\u96c6\u7684\u5143\u6578\u64da\u548c CLIP \u76f8\u4f3c\u5ea6\u5206\u6578\u7b56\u5283\u4e86\u5927\u7d04 40K \u5716\u50cf\u5c0d\u7684\u96c6\u5408\u3002\u9019\u4e9b\u5716\u50cf\u5c0d\u8de8\u8d8a\u5ee3\u6cdb\u7684\u8996\u89ba\u9818\u57df\uff0c\u5305\u62ec\u52d5\u7269\u3001\u6642\u5c1a\u3001\u904b\u52d5\u4ee5\u53ca\u6236\u5916\u548c\u5ba4\u5167\u5834\u666f\u3002\u9019\u4e9b\u554f\u984c\u7d93\u904e\u7cbe\u5fc3\u8a2d\u8a08\uff0c\u65e8\u5728\u8fa8\u5225\u5169\u5e45\u5716\u50cf\u4e4b\u9593\u7684\u76f8\u5c0d\u7279\u5fb5\uff0c\u4e26\u7531\u4eba\u985e\u8a3b\u91cb\u8005\u6a19\u8a18\u5176\u6e96\u78ba\u6027\u548c\u76f8\u95dc\u6027\u3002\u6211\u5011\u4f7f\u7528 CompBench \u4f86\u8a55\u4f30\u6700\u8fd1\u7684 MLLM\uff0c\u5305\u62ec GPT-4V(ision)\u3001Gemini-Pro \u548c LLaVA-1.6\u3002\u6211\u5011\u7684\u7d50\u679c\u63ed\u793a\u4e86\u5b83\u5011\u5728\u6bd4\u8f03\u80fd\u529b\u65b9\u9762\u7684\u986f\u8457\u7f3a\u9677\u3002\u6211\u5011\u76f8\u4fe1 CompBench \u4e0d\u50c5\u53ef\u4ee5\u95e1\u660e\u9019\u4e9b\u9650\u5236\uff0c\u800c\u4e14\u9084\u70ba MLLM \u7684\u6bd4\u8f03\u80fd\u529b\u7684\u672a\u4f86\u6539\u9032\u5960\u5b9a\u4e86\u5805\u5be6\u7684\u57fa\u790e\u3002", "author": "Jihyung Kil et.al.", "authors": "Jihyung Kil, Zheda Mai, Justin Lee, Zihe Wang, Kerrie Cheng, Lemeng Wang, Ye Liu, Arpita Chowdhury, Wei-Lun Chao", "id": "2407.16837v1", "paper_url": "http://arxiv.org/abs/2407.16837v1", "repo": "https://github.com/raptormai/compbench"}}