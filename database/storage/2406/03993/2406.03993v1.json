{"2406.03993": {"publish_time": "2024-06-06", "title": "Assessing LLMs for Zero-shot Abstractive Summarization Through the Lens of Relevance Paraphrasing", "paper_summary": "Large Language Models (LLMs) have achieved state-of-the-art performance at\nzero-shot generation of abstractive summaries for given articles. However,\nlittle is known about the robustness of such a process of zero-shot\nsummarization. To bridge this gap, we propose relevance paraphrasing, a simple\nstrategy that can be used to measure the robustness of LLMs as summarizers. The\nrelevance paraphrasing approach identifies the most relevant sentences that\ncontribute to generating an ideal summary, and then paraphrases these inputs to\nobtain a minimally perturbed dataset. Then, by evaluating model performance for\nsummarization on both the original and perturbed datasets, we can assess the\nLLM's one aspect of robustness. We conduct extensive experiments with relevance\nparaphrasing on 4 diverse datasets, as well as 4 LLMs of different sizes\n(GPT-3.5-Turbo, Llama-2-13B, Mistral-7B, and Dolly-v2-7B). Our results indicate\nthat LLMs are not consistent summarizers for the minimally perturbed articles,\nnecessitating further improvements.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u7279\u5b9a\u6587\u7ae0\u7684\u62bd\u8c61\u6458\u8981\u7684\u96f6\u6b21\u5b78\u7fd2\u751f\u6210\u4e2d\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u9019\u7a2e\u96f6\u6b21\u5b78\u7fd2\u6458\u8981\u7684\u904e\u7a0b\u7684\u7a69\u5065\u6027\u77e5\u4e4b\u751a\u5c11\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u76f8\u95dc\u6027\u6539\u8ff0\uff0c\u9019\u662f\u4e00\u7a2e\u53ef\u7528\u4e8e\u8861\u91cf LLM \u4f5c\u70ba\u6458\u8981\u7684\u7a69\u5065\u6027\u7684\u7c21\u55ae\u7b56\u7565\u3002\u76f8\u95dc\u6027\u6539\u8ff0\u65b9\u6cd5\u8b58\u5225\u51fa\u5c0d\u751f\u6210\u7406\u60f3\u6458\u8981\u8ca2\u737b\u6700\u5927\u7684\u76f8\u95dc\u53e5\u5b50\uff0c\u7136\u5f8c\u6539\u8ff0\u9019\u4e9b\u8f38\u5165\u4ee5\u7372\u53d6\u6700\u5c0f\u7a0b\u5ea6\u64fe\u52d5\u7684\u6578\u64da\u96c6\u3002\u7136\u5f8c\uff0c\u901a\u904e\u8a55\u4f30\u6a21\u578b\u5728\u539f\u59cb\u548c\u64fe\u52d5\u6578\u64da\u96c6\u4e0a\u7684\u6458\u8981\u6027\u80fd\uff0c\u6211\u5011\u53ef\u4ee5\u8a55\u4f30 LLM \u7684\u7a69\u5065\u6027\u7684\u4e00\u500b\u65b9\u9762\u3002\u6211\u5011\u5c0d 4 \u500b\u4e0d\u540c\u7684\u6578\u64da\u96c6\u4ee5\u53ca 4 \u500b\u4e0d\u540c\u898f\u6a21\u7684 LLM\uff08GPT-3.5-Turbo\u3001Llama-2-13B\u3001Mistral-7B \u548c Dolly-v2-7B\uff09\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u76f8\u95dc\u6027\u6539\u8ff0\u5be6\u9a57\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u5c0d\u65bc\u6700\u5c0f\u7a0b\u5ea6\u64fe\u52d5\u7684\u6587\u7ae0\uff0cLLM \u4e0d\u662f\u4e00\u81f4\u7684\u6458\u8981\uff0c\u9019\u9700\u8981\u9032\u4e00\u6b65\u6539\u9032\u3002", "author": "Hadi Askari et.al.", "authors": "Hadi Askari, Anshuman Chhabra, Muhao Chen, Prasant Mohapatra", "id": "2406.03993v1", "paper_url": "http://arxiv.org/abs/2406.03993v1", "repo": "null"}}