{"2410.21252": {"publish_time": "2024-10-28", "title": "LongReward: Improving Long-context Large Language Models with AI Feedback", "paper_summary": "Though significant advancements have been achieved in developing long-context\nlarge language models (LLMs), the compromised quality of LLM-synthesized data\nfor supervised fine-tuning (SFT) often affects the long-context performance of\nSFT models and leads to inherent limitations. In principle, reinforcement\nlearning (RL) with appropriate reward signals can further enhance models'\ncapacities. However, how to obtain reliable rewards in long-context scenarios\nremains unexplored. To this end, we propose LongReward, a novel method that\nutilizes an off-the-shelf LLM to provide rewards for long-context model\nresponses from four human-valued dimensions: helpfulness, logicality,\nfaithfulness, and completeness, each with a carefully designed assessment\npipeline. By combining LongReward and offline RL algorithm DPO, we are able to\neffectively improve long-context SFT models. Our experiments indicate that\nLongReward not only significantly improves models' long-context performance but\nalso enhances their ability to follow short instructions. We also find that\nlong-context DPO with LongReward and conventional short-context DPO can be used\ntogether without hurting either one's performance.", "paper_summary_zh": "\u5118\u7ba1\u5df2\u7d93\u5728\u958b\u767c\u9577\u8a9e\u5883\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u65b9\u9762\u53d6\u5f97\u91cd\u5927\u9032\u5c55\uff0c\u4f46 LLM \u5408\u6210\u8cc7\u6599\u5728\u76e3\u7763\u5fae\u8abf (SFT) \u4e2d\u7684\u54c1\u8cea\u53d7\u640d\uff0c\u901a\u5e38\u6703\u5f71\u97ff SFT \u6a21\u578b\u7684\u9577\u8a9e\u5883\u6548\u80fd\uff0c\u4e26\u5c0e\u81f4\u56fa\u6709\u7684\u9650\u5236\u3002\u539f\u5247\u4e0a\uff0c\u63a1\u7528\u9069\u7576\u734e\u52f5\u4fe1\u865f\u7684\u5f37\u5316\u5b78\u7fd2 (RL) \u53ef\u4ee5\u9032\u4e00\u6b65\u589e\u5f37\u6a21\u578b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5982\u4f55\u5728\u9577\u8a9e\u5883\u5834\u666f\u4e2d\u7372\u5f97\u53ef\u9760\u7684\u734e\u52f5\u4ecd\u7136\u672a\u88ab\u63a2\u7d22\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa LongReward\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u73fe\u6210\u7684 LLM \u70ba\u9577\u8a9e\u5883\u6a21\u578b\u56de\u61c9\u63d0\u4f9b\u734e\u52f5\uff0c\u6db5\u84cb\u56db\u500b\u4eba\u985e\u91cd\u8996\u7684\u5411\u5ea6\uff1a\u6709\u5e6b\u52a9\u6027\u3001\u908f\u8f2f\u6027\u3001\u5fe0\u5be6\u5ea6\u548c\u5b8c\u6574\u6027\uff0c\u6bcf\u500b\u5411\u5ea6\u90fd\u7d93\u904e\u7cbe\u5fc3\u8a2d\u8a08\u7684\u8a55\u4f30\u7ba1\u9053\u3002\u900f\u904e\u7d50\u5408 LongReward \u548c\u96e2\u7dda RL \u6f14\u7b97\u6cd5 DPO\uff0c\u6211\u5011\u80fd\u5920\u6709\u6548\u6539\u5584\u9577\u8a9e\u5883 SFT \u6a21\u578b\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cLongReward \u4e0d\u50c5\u986f\u8457\u6539\u5584\u6a21\u578b\u7684\u9577\u8a9e\u5883\u6548\u80fd\uff0c\u9084\u80fd\u589e\u5f37\u5176\u9075\u5faa\u7c21\u77ed\u8aaa\u660e\u7684\u80fd\u529b\u3002\u6211\u5011\u9084\u767c\u73fe\uff0c\u63a1\u7528 LongReward \u7684\u9577\u8a9e\u5883 DPO \u548c\u50b3\u7d71\u7684\u77ed\u8a9e\u5883 DPO \u53ef\u4ee5\u540c\u6642\u4f7f\u7528\uff0c\u800c\u4e0d\u6703\u640d\u5bb3\u4efb\u4f55\u4e00\u65b9\u7684\u6548\u80fd\u3002", "author": "Jiajie Zhang et.al.", "authors": "Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou, Yilin Niu, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li", "id": "2410.21252v1", "paper_url": "http://arxiv.org/abs/2410.21252v1", "repo": "null"}}