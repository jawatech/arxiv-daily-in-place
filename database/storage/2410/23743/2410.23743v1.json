{"2410.23743": {"publish_time": "2024-10-31", "title": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective", "paper_summary": "What makes a difference in the post-training of LLMs? We investigate the\ntraining patterns of different layers in large language models (LLMs), through\nthe lens of gradient, when training with different responses and initial\nmodels. We are specifically interested in how fast vs. slow thinking affects\nthe layer-wise gradients, given the recent popularity of training LLMs on\nreasoning paths such as chain-of-thoughts (CoT) and process rewards. In our\nstudy, fast thinking without CoT leads to larger gradients and larger\ndifferences of gradients across layers than slow thinking (Detailed CoT),\nindicating the learning stability brought by the latter. Moreover, pre-trained\nLLMs are less affected by the instability of fast thinking than\ninstruction-tuned LLMs. Additionally, we study whether the gradient patterns\ncan reflect the correctness of responses when training different LLMs using\nslow vs. fast thinking paths. The results show that the gradients of slow\nthinking can distinguish correct and irrelevant reasoning paths. As a\ncomparison, we conduct similar gradient analyses on non-reasoning knowledge\nlearning tasks, on which, however, trivially increasing the response length\ndoes not lead to similar behaviors of slow thinking. Our study strengthens\nfundamental understandings of LLM training and sheds novel insights on its\nefficiency and stability, which pave the way towards building a generalizable\nSystem-2 agent. Our code, data, and gradient statistics can be found in:\nhttps://github.com/MingLiiii/Layer_Gradient.", "paper_summary_zh": "\u5728 LLM \u7684\u5f8c\u8a13\u7df4\u4e2d\uff0c\u4ec0\u9ebc\u6703\u9020\u6210\u5dee\u7570\uff1f\u6211\u5011\u900f\u904e\u68af\u5ea6\u7684\u8996\u89d2\uff0c\u5728\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6642\uff0c\u63a2\u8a0e\u4e0d\u540c\u5c64\u7684\u8a13\u7df4\u6a21\u5f0f\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u7684\u56de\u61c9\u548c\u521d\u59cb\u6a21\u578b\u4e0b\u8a13\u7df4\u6642\uff0c\u9019\u4e9b\u6a21\u5f0f\u7684\u8b8a\u5316\u3002\u6211\u5011\u7279\u5225\u611f\u8208\u8da3\u7684\u662f\uff0c\u5728 LLM \u65bc\u63a8\u7406\u8def\u5f91\uff08\u5982\u601d\u7dad\u93c8 (CoT) \u548c\u904e\u7a0b\u734e\u52f5\uff09\u4e0a\u8a13\u7df4\u7684\u8fd1\u671f\u71b1\u6f6e\u4e0b\uff0c\u5feb\u901f\u601d\u8003\u8207\u6162\u601d\u8003\u5982\u4f55\u5f71\u97ff\u9010\u5c64\u68af\u5ea6\u3002\u5728\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff0c\u6c92\u6709 CoT \u7684\u5feb\u901f\u601d\u8003\u6703\u5c0e\u81f4\u6bd4\u6162\u601d\u8003\uff08\u8a73\u7d30\u7684 CoT\uff09\u66f4\u5927\u7684\u68af\u5ea6\u548c\u66f4\u5927\u5c64\u9593\u68af\u5ea6\u5dee\u7570\uff0c\u9019\u8868\u793a\u5f8c\u8005\u5e36\u4f86\u4e86\u5b78\u7fd2\u7684\u7a69\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u9810\u5148\u8a13\u7df4\u7684 LLM \u53d7\u5feb\u901f\u601d\u8003\u7684\u4e0d\u7a69\u5b9a\u6027\u5f71\u97ff\u8f03\u5c0f\uff0c\u800c\u975e\u6307\u4ee4\u8abf\u6574\u7684 LLM \u5247\u53d7\u5f71\u97ff\u8f03\u5927\u3002\u6b64\u5916\uff0c\u6211\u5011\u7814\u7a76\u68af\u5ea6\u6a21\u5f0f\u662f\u5426\u53ef\u4ee5\u5728\u4f7f\u7528\u6162\u601d\u8003\u8207\u5feb\u901f\u601d\u8003\u8def\u5f91\u8a13\u7df4\u4e0d\u540c\u7684 LLM \u6642\uff0c\u53cd\u6620\u56de\u61c9\u7684\u6b63\u78ba\u6027\u3002\u7d50\u679c\u986f\u793a\uff0c\u6162\u601d\u8003\u7684\u68af\u5ea6\u53ef\u4ee5\u5340\u5206\u6b63\u78ba\u7684\u63a8\u7406\u8def\u5f91\u548c\u4e0d\u76f8\u95dc\u7684\u63a8\u7406\u8def\u5f91\u3002\u4f5c\u70ba\u6bd4\u8f03\uff0c\u6211\u5011\u5c0d\u975e\u63a8\u7406\u77e5\u8b58\u5b78\u7fd2\u4efb\u52d9\u9032\u884c\u985e\u4f3c\u7684\u68af\u5ea6\u5206\u6790\uff0c\u7136\u800c\uff0c\u5728\u9019\u4e9b\u4efb\u52d9\u4e2d\uff0c\u55ae\u7d14\u589e\u52a0\u56de\u61c9\u9577\u5ea6\u4e0d\u6703\u5c0e\u81f4\u985e\u4f3c\u6162\u601d\u8003\u7684\u884c\u70ba\u3002\u6211\u5011\u7684\u7814\u7a76\u5f37\u5316\u4e86\u5c0d LLM \u8a13\u7df4\u7684\u57fa\u672c\u7406\u89e3\uff0c\u4e26\u5c0d\u5176\u6548\u7387\u548c\u7a69\u5b9a\u6027\u63d0\u51fa\u4e86\u65b0\u7684\u898b\u89e3\uff0c\u70ba\u5efa\u69cb\u53ef\u6982\u62ec\u7684 System-2 \u4ee3\u7406\u92ea\u5e73\u4e86\u9053\u8def\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u3001\u8cc7\u6599\u548c\u68af\u5ea6\u7d71\u8a08\u8cc7\u6599\u53ef\u4ee5\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u627e\u5230\uff1a\nhttps://github.com/MingLiiii/Layer_Gradient\u3002", "author": "Ming Li et.al.", "authors": "Ming Li, Yanhong Li, Tianyi Zhou", "id": "2410.23743v1", "paper_url": "http://arxiv.org/abs/2410.23743v1", "repo": "https://github.com/mingliiii/layer_gradient"}}