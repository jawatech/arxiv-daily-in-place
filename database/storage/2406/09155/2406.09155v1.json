{"2406.09155": {"publish_time": "2024-06-13", "title": "DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nrevolutionizing the integration of AI in daily life applications. However, they\nare prone to hallucinations, generating claims that contradict established\nfacts, deviating from prompts, and producing inconsistent responses when the\nsame prompt is presented multiple times. Addressing these issues is challenging\ndue to the lack of comprehensive and easily assessable benchmark datasets. Most\nexisting datasets are small and rely on multiple-choice questions, which are\ninadequate for evaluating the generative prowess of LLMs. To measure\nhallucination in LLMs, this paper introduces a comprehensive benchmark dataset\ncomprising over 75,000 prompts across eight domains. These prompts are designed\nto elicit definitive, concise, and informative answers. The dataset is divided\ninto two segments: one publicly available for testing and assessing LLM\nperformance and a hidden segment for benchmarking various LLMs. In our\nexperiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and\nZephyr-revealing that overall factual hallucination ranges from 59% to 82% on\nthe public dataset and 57% to 76% in the hidden benchmark. Prompt misalignment\nhallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the\nhidden counterpart. Average consistency ranges from 21% to 61% and 22% to 63%,\nrespectively. Domain-wise analysis shows that LLM performance significantly\ndeteriorates when asked for specific numeric information while performing\nmoderately with person, location, and date queries. Our dataset demonstrates\nits efficacy and serves as a comprehensive benchmark for LLM performance\nevaluation. Our dataset and LLMs responses are available at\n\\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u5fb9\u5e95\u6539\u8b8a\u4e86 AI \u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u61c9\u7528\u7684\u6574\u5408\u65b9\u5f0f\u3002\u7136\u800c\uff0c\u5b83\u5011\u5bb9\u6613\u7522\u751f\u5e7b\u89ba\uff0c\u63d0\u51fa\u8207\u65e2\u5b9a\u4e8b\u5be6\u76f8\u77db\u76fe\u7684\u4e3b\u5f35\uff0c\u504f\u96e2\u63d0\u793a\uff0c\u4ee5\u53ca\u5728\u591a\u6b21\u63d0\u51fa\u76f8\u540c\u63d0\u793a\u6642\u7522\u751f\u4e0d\u4e00\u81f4\u7684\u56de\u61c9\u3002\u7531\u65bc\u7f3a\u4e4f\u5168\u9762\u4e14\u6613\u65bc\u8a55\u4f30\u7684\u57fa\u6e96\u8cc7\u6599\u96c6\uff0c\u56e0\u6b64\u8981\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u5177\u6709\u6311\u6230\u6027\u3002\u5927\u591a\u6578\u73fe\u6709\u7684\u8cc7\u6599\u96c6\u90fd\u5f88\u5c0f\uff0c\u800c\u4e14\u4f9d\u8cf4\u65bc\u591a\u9078\u984c\uff0c\u9019\u4e0d\u8db3\u4ee5\u8a55\u4f30 LLM \u7684\u751f\u6210\u80fd\u529b\u3002\u70ba\u4e86\u8861\u91cf LLM \u4e2d\u7684\u5e7b\u89ba\uff0c\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u500b\u5168\u9762\u7684\u57fa\u6e96\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u516b\u500b\u9818\u57df\u7684\u8d85\u904e 75,000 \u500b\u63d0\u793a\u3002\u9019\u4e9b\u63d0\u793a\u65e8\u5728\u5f15\u767c\u660e\u78ba\u3001\u7c21\u6f54\u4e14\u6709\u8cc7\u8a0a\u6027\u7684\u7b54\u6848\u3002\u8a72\u8cc7\u6599\u96c6\u5206\u70ba\u5169\u500b\u90e8\u5206\uff1a\u4e00\u500b\u516c\u958b\u53ef\u4f9b\u6e2c\u8a66\u548c\u8a55\u4f30 LLM \u6548\u80fd\uff0c\u53e6\u4e00\u500b\u96b1\u85cf\u90e8\u5206\u5247\u7528\u65bc\u5c0d\u5404\u7a2e LLM \u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u3002\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u6e2c\u8a66\u4e86\u516d\u500b LLM-GPT-3.5\u3001Llama 2\u3001Llama 3\u3001Gemini\u3001Mixtral \u548c Zephyr\uff0c\u7d50\u679c\u986f\u793a\u5728\u516c\u958b\u8cc7\u6599\u96c6\u4e0a\u7684\u6574\u9ad4\u4e8b\u5be6\u5e7b\u89ba\u7bc4\u570d\u5f9e 59% \u5230 82%\uff0c\u5728\u96b1\u85cf\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7684\u7bc4\u570d\u5247\u5f9e 57% \u5230 76%\u3002\u63d0\u793a\u932f\u4f4d\u5e7b\u89ba\u5728\u516c\u958b\u8cc7\u6599\u96c6\u4e2d\u7bc4\u570d\u5f9e 6% \u5230 95%\uff0c\u5728\u96b1\u85cf\u5c0d\u61c9\u90e8\u5206\u4e2d\u7bc4\u570d\u5f9e 17% \u5230 94%\u3002\u5e73\u5747\u4e00\u81f4\u6027\u5206\u5225\u5f9e 21% \u5230 61% \u548c 22% \u5230 63%\u3002\u6309\u9818\u57df\u5206\u6790\u986f\u793a\uff0c\u7576\u88ab\u8981\u6c42\u63d0\u4f9b\u5177\u9ad4\u6578\u5b57\u8cc7\u8a0a\u6642\uff0cLLM \u6548\u80fd\u6703\u986f\u8457\u4e0b\u964d\uff0c\u800c\u5728\u8655\u7406\u4eba\u7269\u3001\u5730\u9ede\u548c\u65e5\u671f\u67e5\u8a62\u6642\u8868\u73fe\u4e2d\u7b49\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u8b49\u660e\u4e86\u5176\u6548\u529b\uff0c\u4e26\u4f5c\u70ba LLM \u6548\u80fd\u8a55\u4f30\u7684\u5168\u9762\u57fa\u6e96\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u548c LLM \u56de\u61c9\u53ef\u5728 \\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn} \u53d6\u5f97\u3002", "author": "A B M Ashikur Rahman et.al.", "authors": "A B M Ashikur Rahman, Saeed Anwar, Muhammad Usman, Ajmal Mian", "id": "2406.09155v1", "paper_url": "http://arxiv.org/abs/2406.09155v1", "repo": "null"}}