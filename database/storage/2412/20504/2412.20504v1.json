{"2412.20504": {"publish_time": "2024-12-29", "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video Understanding", "paper_summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe", "paper_summary_zh": "\u8996\u8a0a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (VideoLLM) \u5df2\u5728\u8996\u8a0a\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u986f\u8457\u9032\u5c55\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 VideoLLM \u7d93\u5e38\u6703\u7e7c\u627f\u5176\u9aa8\u5e79 LLM \u5728\u8655\u7406\u9577\u5e8f\u5217\u6642\u7684\u9650\u5236\uff0c\u5c0e\u81f4\u96e3\u4ee5\u7406\u89e3\u9577\u8996\u8a0a\u3002\u5e38\u898b\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u8981\u4e0d\u5c31\u662f\u5747\u52fb\u53d6\u6a23\u8996\u8a0a\u7684\u5f71\u683c\uff0c\u8981\u4e0d\u5c31\u662f\u58d3\u7e2e\u8996\u89ba\u4ee3\u78bc\uff0c\u9019\u4e9b\u65b9\u6cd5\u4e3b\u8981\u8457\u91cd\u65bc\u4f4e\u968e\u6642\u9593\u8996\u89ba\u5197\u9918\uff0c\u5ffd\u7565\u4e86\u9ad8\u968e\u77e5\u8b58\u5197\u9918\u3002\u9019\u6703\u9650\u5236\u53ef\u9054\u5230\u7684\u58d3\u7e2e\u7387\uff0c\u540c\u6642\u640d\u5931\u964d\u5230\u6700\u4f4e\u3002\u6709\u9451\u65bc\u6b64\uff0c\u6211\u5011\u63a8\u51fa\u4e86\u4e00\u500b\u514d\u8a13\u7df4\u65b9\u6cd5\u300cReTaKe\u300d\uff0c\u5305\u542b\u5169\u500b\u65b0\u7a4e\u7684\u6a21\u7d44 DPSelect \u548c PivotKV\uff0c\u4ee5\u5171\u540c\u5efa\u6a21\u4e26\u6e1b\u5c11\u6642\u9593\u8996\u89ba\u5197\u9918\u548c\u77e5\u8b58\u5197\u9918\uff0c\u4ee5\u5229\u65bc\u7406\u89e3\u9577\u8996\u8a0a\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cDPSelect \u6703\u6839\u64da\u8996\u89ba\u7279\u5fb5\uff0c\u627e\u51fa\u5177\u6709\u5c40\u90e8\u6700\u5927\u5cf0\u503c\u8ddd\u96e2\u7684\u95dc\u9375\u5f71\u683c\uff0c\u9019\u4e9b\u7279\u5fb5\u8207\u4eba\u985e\u7684\u8996\u8a0a\u611f\u77e5\u7dca\u5bc6\u76f8\u95dc\u3002PivotKV \u5c07\u53d6\u5f97\u7684\u95dc\u9375\u5f71\u683c\u7528\u4f5c\u6a1e\u7d10\uff0c\u4e26\u5c0d\u6ce8\u610f\u529b\u8a55\u5206\u4f4e\u7684\u975e\u6a1e\u7d10\u4ee3\u78bc\u57f7\u884c KV \u5feb\u53d6\u58d3\u7e2e\uff0c\u9019\u4e9b\u4ee3\u78bc\u4f86\u81ea\u65bc LLM \u6240\u5b78\u7fd2\u7684\u5148\u9a57\u77e5\u8b58\u3002\u5728\u57fa\u6e96 VideoMME\u3001MLVU \u548c LVBench \u4e0a\u7684\u5be6\u9a57\u986f\u793a\uff0cReTaKe \u53ef\u4ee5\u652f\u63f4\u9577\u5ea6\u70ba\u539f\u672c 4 \u500d\u7684\u8996\u8a0a\u5e8f\u5217\uff0c\u4e14\u6548\u80fd\u640d\u5931\u964d\u5230\u6700\u4f4e (<1%)\uff0c\u4e26\u4ee5 3%-5% \u7684\u5e45\u5ea6\u512a\u65bc\u6240\u6709\u985e\u4f3c\u5927\u5c0f\u7684 VideoLLM\uff0c\u751a\u81f3\u8d85\u8d8a\u6216\u8207\u5927\u5f97\u591a\u7684 VideoLLM \u76f8\u7576\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/SCZwangxiao/video-ReTaKe \u53d6\u5f97", "author": "Xiao Wang et.al.", "authors": "Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie", "id": "2412.20504v1", "paper_url": "http://arxiv.org/abs/2412.20504v1", "repo": "https://github.com/sczwangxiao/video-retake"}}