{"2406.19502": {"publish_time": "2024-06-27", "title": "Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning", "paper_summary": "Despite significant advancements, there is a limited understanding of how\nlarge language models (LLMs) utilize knowledge for reasoning. To address this,\nwe propose a method that deconstructs complex real-world questions into a\ngraph, representing each question as a node with parent nodes of background\nknowledge needed to solve the question. We develop the DepthQA dataset,\ndeconstructing questions into three depths: (i) recalling conceptual knowledge,\n(ii) applying procedural knowledge, and (iii) analyzing strategic knowledge.\nBased on a hierarchical graph, we quantify forward discrepancy, discrepancies\nin LLMs' performance on simpler sub-problems versus complex questions. We also\nmeasure backward discrepancy, where LLMs answer complex questions but struggle\nwith simpler ones. Our analysis shows that smaller models have more\ndiscrepancies than larger models. Additionally, guiding models from simpler to\ncomplex questions through multi-turn interactions improves performance across\nmodel sizes, highlighting the importance of structured intermediate steps in\nknowledge reasoning. This work enhances our understanding of LLM reasoning and\nsuggests ways to improve their problem-solving abilities.", "paper_summary_zh": "\u5118\u7ba1\u6709\u986f\u8457\u7684\u9032\u5c55\uff0c\u4f46\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5982\u4f55\u5229\u7528\u77e5\u8b58\u9032\u884c\u63a8\u7406\u7684\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\uff0c\u5c07\u8907\u96dc\u7684\u771f\u5be6\u4e16\u754c\u554f\u984c\u89e3\u69cb\u6210\u4e00\u500b\u5716\u5f62\uff0c\u5c07\u6bcf\u500b\u554f\u984c\u8868\u793a\u70ba\u4e00\u500b\u7bc0\u9ede\uff0c\u5176\u4e2d\u5305\u542b\u89e3\u6c7a\u554f\u984c\u6240\u9700\u7684\u80cc\u666f\u77e5\u8b58\u7684\u7236\u7bc0\u9ede\u3002\u6211\u5011\u958b\u767c\u4e86 DepthQA \u8cc7\u6599\u96c6\uff0c\u5c07\u554f\u984c\u89e3\u69cb\u6210\u4e09\u500b\u6df1\u5ea6\uff1a(i) \u56de\u61b6\u6982\u5ff5\u77e5\u8b58\uff0c(ii) \u61c9\u7528\u7a0b\u5e8f\u77e5\u8b58\uff0c\u4ee5\u53ca (iii) \u5206\u6790\u7b56\u7565\u77e5\u8b58\u3002\u57fa\u65bc\u4e00\u500b\u968e\u5c64\u5716\u5f62\uff0c\u6211\u5011\u91cf\u5316\u4e86\u6b63\u5411\u5dee\u7570\uff0cLLM \u5728\u8f03\u7c21\u55ae\u7684\u5b50\u554f\u984c\u548c\u8907\u96dc\u554f\u984c\u4e0a\u7684\u6548\u80fd\u5dee\u7570\u3002\u6211\u5011\u4e5f\u6e2c\u91cf\u4e86\u53cd\u5411\u5dee\u7570\uff0c\u5176\u4e2d LLM \u80fd\u56de\u7b54\u8907\u96dc\u554f\u984c\uff0c\u4f46\u5728\u8f03\u7c21\u55ae\u7684\u554f\u984c\u4e0a\u537b\u6709\u56f0\u96e3\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u8f03\u5c0f\u7684\u6a21\u578b\u6bd4\u8f03\u5927\u7684\u6a21\u578b\u6709\u66f4\u591a\u7684\u5dee\u7570\u3002\u6b64\u5916\uff0c\u900f\u904e\u591a\u56de\u5408\u4e92\u52d5\u5f15\u5c0e\u6a21\u578b\u5f9e\u8f03\u7c21\u55ae\u5230\u8907\u96dc\u7684\u554f\u984c\uff0c\u53ef\u4ee5\u6539\u5584\u6240\u6709\u6a21\u578b\u898f\u6a21\u7684\u6548\u80fd\uff0c\u7a81\u986f\u4e86\u7d50\u69cb\u5316\u4e2d\u9593\u6b65\u9a5f\u5728\u77e5\u8b58\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002\u9019\u9805\u5de5\u4f5c\u589e\u9032\u4e86\u6211\u5011\u5c0d LLM \u63a8\u7406\u7684\u7406\u89e3\uff0c\u4e26\u63d0\u51fa\u4e86\u6539\u5584\u5176\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "author": "Miyoung Ko et.al.", "authors": "Miyoung Ko, Sue Hyun Park, Joonsuk Park, Minjoon Seo", "id": "2406.19502v1", "paper_url": "http://arxiv.org/abs/2406.19502v1", "repo": "https://github.com/kaistai/knowledge-reasoning"}}