
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-01-06**|**Gaussian Masked Autoencoders**|Jathushan Rajasegaran et.al.|[2501.03229v1](http://arxiv.org/abs/2501.03229v1)|null|
|**2025-01-06**|**LightGNN: Simple Graph Neural Network for Recommendation**|Guoxuan Chen et.al.|[2501.03228v1](http://arxiv.org/abs/2501.03228v1)|null|
|**2025-01-06**|**BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning**|Beichen Zhang et.al.|[2501.03226v1](http://arxiv.org/abs/2501.03226v1)|[link](https://github.com/beichenzbc/booststep)|
|**2025-01-06**|**Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation**|Yuhui Zhang et.al.|[2501.03225v1](http://arxiv.org/abs/2501.03225v1)|[link](https://github.com/yuhui-zh15/autoconverter)|
|**2025-01-06**|**Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text**|Ayat Najjar et.al.|[2501.03212v1](http://arxiv.org/abs/2501.03212v1)|null|
|**2025-01-06**|**Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity**|Ayat A. Najjar et.al.|[2501.03203v1](http://arxiv.org/abs/2501.03203v1)|null|
|**2025-01-06**|**The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input**|Alon Jacovi et.al.|[2501.03200v1](http://arxiv.org/abs/2501.03200v1)|null|
|**2025-01-06**|**CLIX: Cross-Lingual Explanations of Idiomatic Expressions**|Aaron Gluck et.al.|[2501.03191v1](http://arxiv.org/abs/2501.03191v1)|null|
|**2025-01-06**|**Turn-based Multi-Agent Reinforcement Learning Model Checking**|Dennis Gross et.al.|[2501.03187v1](http://arxiv.org/abs/2501.03187v1)|null|
|**2025-01-06**|**GLiREL -- Generalist Model for Zero-Shot Relation Extraction**|Jack Boylan et.al.|[2501.03172v1](http://arxiv.org/abs/2501.03172v1)|null|
|**2025-01-06**|**Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text**|Ali Al-Lawati et.al.|[2501.03166v1](http://arxiv.org/abs/2501.03166v1)|[link](https://github.com/aliwister/ast-icl)|
|**2025-01-06**|**The Scaling Law for LoRA Base on Mutual Information Upper Bound**|Jing Zhang et.al.|[2501.03152v1](http://arxiv.org/abs/2501.03152v1)|null|
|**2025-01-06**|**Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches**|Alhassan Mumuni et.al.|[2501.03151v1](http://arxiv.org/abs/2501.03151v1)|null|
|**2025-01-06**|**Co-Activation Graph Analysis of Safety-Verified and Explainable Deep Reinforcement Learning Policies**|Dennis Gross et.al.|[2501.03142v1](http://arxiv.org/abs/2501.03142v1)|null|
|**2025-01-06**|**VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity**|Yerong Li et.al.|[2501.03139v1](http://arxiv.org/abs/2501.03139v1)|null|
|**2025-01-06**|**PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models**|Mingyang Song et.al.|[2501.03124v1](http://arxiv.org/abs/2501.03124v1)|null|
|**2025-01-06**|**From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning**|Chao Feng et.al.|[2501.03119v1](http://arxiv.org/abs/2501.03119v1)|null|
|**2025-01-06**|**LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases**|Dylan Bouchard et.al.|[2501.03112v1](http://arxiv.org/abs/2501.03112v1)|[link](https://github.com/cvs-health/langfair)|
|**2025-01-06**|**Sentiment-guided Commonsense-aware Response Generation for Mental Health Counseling**|Aseem Srivastava et.al.|[2501.03088v1](http://arxiv.org/abs/2501.03088v1)|null|
|**2025-01-06**|**Personalized Fashion Recommendation with Image Attributes and Aesthetics Assessment**|Chongxian Chen et.al.|[2501.03085v1](http://arxiv.org/abs/2501.03085v1)|null|
|**2025-01-06**|**Trust Modeling in Counseling Conversations: A Benchmark Study**|Aseem Srivastava et.al.|[2501.03064v1](http://arxiv.org/abs/2501.03064v1)|null|
|**2025-01-06**|**Survival Analysis Revisited: Understanding and Unifying Poisson, Exponential, and Cox Models in Fall Risk Analysis**|Tianhua Chen et.al.|[2501.03058v1](http://arxiv.org/abs/2501.03058v1)|null|
|**2025-01-06**|**Single-Channel Distance-Based Source Separation for Mobile GPU in Outdoor and Indoor Environments**|Hanbin Bae et.al.|[2501.03045v1](http://arxiv.org/abs/2501.03045v1)|null|
|**2025-01-06**|**ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events**|Duygu Sezen Islakoglu et.al.|[2501.03040v1](http://arxiv.org/abs/2501.03040v1)|null|
|**2025-01-06**|**Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders**|Dichucheng Li et.al.|[2501.03038v1](http://arxiv.org/abs/2501.03038v1)|null|
|**2025-01-06**|**Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning**|Zhen Li et.al.|[2501.03035v1](http://arxiv.org/abs/2501.03035v1)|null|
|**2025-01-06**|**Putnam's Critical and Explanatory Tendencies Interpreted from a Machine Learning Perspective**|Sheldon Z. Soudin et.al.|[2501.03026v1](http://arxiv.org/abs/2501.03026v1)|null|
|**2025-01-06**|**Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment**|Pegah Khayatan et.al.|[2501.03012v1](http://arxiv.org/abs/2501.03012v1)|[link](https://github.com/mshukor/xl-vlms)|
|**2025-01-06**|**Quality Estimation based Feedback Training for Improving Pronoun Translation**|Harshit Dhankhar et.al.|[2501.03008v1](http://arxiv.org/abs/2501.03008v1)|null|
|**2025-01-06**|**CALM: Curiosity-Driven Auditing for Large Language Models**|Xiang Zheng et.al.|[2501.02997v1](http://arxiv.org/abs/2501.02997v1)|null|
|**2025-01-06**|**A Bio-Inspired Research Paradigm of Collision Perception Neurons Enabling Neuro-Robotic Integration: The LGMD Case**|Ziyan Qin et.al.|[2501.02982v1](http://arxiv.org/abs/2501.02982v1)|null|
|**2025-01-06**|**CONTINUUM: Detecting APT Attacks through Spatial-Temporal Graph Neural Networks**|Atmane Ayoub Mansour Bahara et.al.|[2501.02981v1](http://arxiv.org/abs/2501.02981v1)|null|
|**2025-01-06**|**Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation**|Zhi Qu et.al.|[2501.02979v1](http://arxiv.org/abs/2501.02979v1)|[link](https://github.com/zhiqu22/mitre)|
|**2025-01-06**|**CAMP: Collaborative Attention Model with Profiles for Vehicle Routing Problems**|Chuanbo Hua et.al.|[2501.02977v1](http://arxiv.org/abs/2501.02977v1)|null|
|**2025-01-06**|**Proof-of-Data: A Consensus Protocol for Collaborative Intelligence**|Huiwen Liu et.al.|[2501.02971v1](http://arxiv.org/abs/2501.02971v1)|null|
|**2025-01-06**|**Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild**|Wanpeng Hu et.al.|[2501.02964v1](http://arxiv.org/abs/2501.02964v1)|[link](https://github.com/aibee00/socraticquestioning)|
|**2025-01-06**|**Key-value memory in the brain**|Samuel J. Gershman et.al.|[2501.02950v1](http://arxiv.org/abs/2501.02950v1)|[link](https://github.com/kazuki-irie/kv-memory-brain)|
|**2025-01-06**|**Label-free Concept Based Multiple Instance Learning for Gigapixel Histopathology**|Susu Sun et.al.|[2501.02922v1](http://arxiv.org/abs/2501.02922v1)|null|
|**2025-01-06**|**Skillful High-Resolution Ensemble Precipitation Forecasting with an Integrated Deep Learning Framework**|Shuangshuang He et.al.|[2501.02905v1](http://arxiv.org/abs/2501.02905v1)|null|
|**2025-01-06**|**Explaining Humour Style Classifications: An XAI Approach to Understanding Computational Humour Analysis**|Mary Ogbuka Kenneth et.al.|[2501.02891v1](http://arxiv.org/abs/2501.02891v1)|null|
|**2025-01-06**|**IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment**|Yiming Zhang et.al.|[2501.02869v1](http://arxiv.org/abs/2501.02869v1)|null|
|**2025-01-06**|**Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification**|Yubo Wang et.al.|[2501.02844v1](http://arxiv.org/abs/2501.02844v1)|null|
|**2025-01-06**|**Enhanced Rooftop Solar Panel Detection by Efficiently Aggregating Local Features**|Kuldeep Kurte et.al.|[2501.02840v1](http://arxiv.org/abs/2501.02840v1)|null|
|**2025-01-06**|**Forward Once for All: Structural Parameterized Adaptation for Efficient Cloud-coordinated On-device Recommendation**|Kairui Fu et.al.|[2501.02837v1](http://arxiv.org/abs/2501.02837v1)|null|
|**2025-01-06**|**Samba-asr state-of-the-art speech recognition leveraging structured state-space models**|Syed Abdul Gaffar Shakhadri et.al.|[2501.02832v1](http://arxiv.org/abs/2501.02832v1)|null|
|**2025-01-06**|**RDD4D: 4D Attention-Guided Road Damage Detection And Classification**|Asma Alkalbani et.al.|[2501.02822v1](http://arxiv.org/abs/2501.02822v1)|[link](https://github.com/msaqib17/road_damage_detection)|
|**2025-01-06**|**InpDiffusion: Image Inpainting Localization via Conditional Diffusion Models**|Kai Wang et.al.|[2501.02816v1](http://arxiv.org/abs/2501.02816v1)|null|
|**2025-01-06**|**Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism**|Yimin Tang et.al.|[2501.02803v1](http://arxiv.org/abs/2501.02803v1)|null|
|**2025-01-06**|**InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion**|Zhaoyi Yan et.al.|[2501.02795v1](http://arxiv.org/abs/2501.02795v1)|null|
|**2025-01-06**|**Fairness Through Matching**|Kunwoong Kim et.al.|[2501.02793v1](http://arxiv.org/abs/2501.02793v1)|[link](https://github.com/kwkimonline/ftm)|
|**2025-01-06**|**Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model**|Yueqin Yin et.al.|[2501.02790v1](http://arxiv.org/abs/2501.02790v1)|null|
|**2025-01-06**|**GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation**|Niloufar Eghbali et.al.|[2501.02788v1](http://arxiv.org/abs/2501.02788v1)|null|
|**2025-01-06**|**Hybrid deep convolution model for lung cancer detection with transfer learning**|Sugandha Saxena et.al.|[2501.02785v1](http://arxiv.org/abs/2501.02785v1)|null|
|**2025-01-06**|**GeAR: Generation Augmented Retrieval**|Haoyu Liu et.al.|[2501.02772v1](http://arxiv.org/abs/2501.02772v1)|null|
|**2025-01-06**|**Enhancing Trustworthiness of Graph Neural Networks with Rank-Based Conformal Training**|Ting Wang et.al.|[2501.02767v1](http://arxiv.org/abs/2501.02767v1)|[link](https://github.com/cityu-t/rcp-gnn)|
|**2025-01-06**|**Are GNNs Effective for Multimodal Fault Diagnosis in Microservice Systems?**|Fei Gao et.al.|[2501.02766v1](http://arxiv.org/abs/2501.02766v1)|null|
|**2025-01-06**|**Visual Large Language Models for Generalized and Specialized Applications**|Yifan Li et.al.|[2501.02765v1](http://arxiv.org/abs/2501.02765v1)|[link](https://github.com/jackyfl/awesome-vllms)|
|**2025-01-06**|**MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and Attention Distillation**|Yidong Ding et.al.|[2501.02754v1](http://arxiv.org/abs/2501.02754v1)|null|
|**2025-01-06**|**Interpretable Recognition of Fused Magnesium Furnace Working Conditions with Deep Convolutional Stochastic Configuration Networks**|Li Weitao et.al.|[2501.02740v1](http://arxiv.org/abs/2501.02740v1)|null|
|**2025-01-06**|**TARDiS : Text Augmentation for Refining Diversity and Separability**|Kyungmin Kim et.al.|[2501.02739v1](http://arxiv.org/abs/2501.02739v1)|null|
|**2025-01-06**|**AFed: Algorithmic Fair Federated Learning**|Huiqiang Chen et.al.|[2501.02732v1](http://arxiv.org/abs/2501.02732v1)|null|
|**2025-01-06**|**OpenGU: A Comprehensive Benchmark for Graph Unlearning**|Bowen Fan et.al.|[2501.02728v1](http://arxiv.org/abs/2501.02728v1)|null|
|**2025-01-06**|**Artificial Intelligence in Creative Industries: Advances Prior to 2025**|Nantheera Anantrasirichai et.al.|[2501.02725v1](http://arxiv.org/abs/2501.02725v1)|null|
|**2025-01-06**|**KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models**|Zaiyi Zheng et.al.|[2501.02711v1](http://arxiv.org/abs/2501.02711v1)|null|
|**2025-01-06**|**QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance**|Binita Saha et.al.|[2501.02702v1](http://arxiv.org/abs/2501.02702v1)|null|
|**2025-01-06**|**EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models**|Andrés Villa et.al.|[2501.02699v1](http://arxiv.org/abs/2501.02699v1)|null|
|**2025-01-05**|**Decoding specialised feature neurons in LLMs with the final projection layer**|Harry J Davies et.al.|[2501.02688v1](http://arxiv.org/abs/2501.02688v1)|null|
|**2025-01-05**|**From Superficial Patterns to Semantic Understanding: Fine-Tuning Language Models on Contrast Sets**|Daniel Petrov et.al.|[2501.02683v1](http://arxiv.org/abs/2501.02683v1)|null|
|**2025-01-05**|**From thermodynamics to protein design: Diffusion models for biomolecule generation towards autonomous protein engineering**|Wen-ran Li et.al.|[2501.02680v1](http://arxiv.org/abs/2501.02680v1)|null|
|**2025-01-05**|**Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?**|Simon Park et.al.|[2501.02669v1](http://arxiv.org/abs/2501.02669v1)|[link](https://github.com/princeton-pli/vlm_s2h)|
|**2025-01-05**|**Multi-Aggregator Time-Warping Heterogeneous Graph Neural Network for Personalized Micro-Video Recommendation**|Jinkun Han et.al.|[2501.02666v1](http://arxiv.org/abs/2501.02666v1)|null|
|**2025-01-05**|**Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence Benchmarks**|Yang Wang et.al.|[2501.02654v1](http://arxiv.org/abs/2501.02654v1)|[link](https://github.com/puredefence/advbench4text)|
|**2025-01-05**|**Tighnari: Multi-modal Plant Species Prediction Based on Hierarchical Cross-Attention Using Graph-Based and Vision Backbone-Extracted Features**|Haixu Liu et.al.|[2501.02649v1](http://arxiv.org/abs/2501.02649v1)|null|
|**2025-01-05**|**Representation Learning of Lab Values via Masked AutoEncoder**|David Restrepo et.al.|[2501.02648v1](http://arxiv.org/abs/2501.02648v1)|null|
|**2025-01-05**|**Prune or Retrain: Optimizing the Vocabulary of Multilingual Models for Estonian**|Aleksei Dorkin et.al.|[2501.02631v1](http://arxiv.org/abs/2501.02631v1)|null|
|**2025-01-05**|**Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense**|Yang Ouyang et.al.|[2501.02629v1](http://arxiv.org/abs/2501.02629v1)|null|
|**2025-01-05**|**Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets**|Mahmoud Jahanshahi et.al.|[2501.02628v1](http://arxiv.org/abs/2501.02628v1)|null|
|**2025-01-05**|**LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and Language Alignment**|Yifei Liu et.al.|[2501.02621v1](http://arxiv.org/abs/2501.02621v1)|null|
|**2025-01-05**|**TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms**|Jovan Stojkovic et.al.|[2501.02600v1](http://arxiv.org/abs/2501.02600v1)|null|
|**2025-01-05**|**Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models**|Jalisha Jashim Era et.al.|[2501.02599v1](http://arxiv.org/abs/2501.02599v1)|null|
|**2025-01-05**|**GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation**|Iustin Sîrbu et.al.|[2501.02598v1](http://arxiv.org/abs/2501.02598v1)|null|
|**2025-01-05**|**Evolving Skeletons: Motion Dynamics in Action Recognition**|Jushang Qiu et.al.|[2501.02593v1](http://arxiv.org/abs/2501.02593v1)|null|
|**2025-01-05**|**Efficient Architectures for High Resolution Vision-Language Models**|Miguel Carvalho et.al.|[2501.02584v1](http://arxiv.org/abs/2501.02584v1)|null|
|**2025-01-05**|**LeetDecoding: A PyTorch Library for Exponentially Decaying Causal Linear Attention with CUDA Implementations**|Jiaping Wang et.al.|[2501.02573v1](http://arxiv.org/abs/2501.02573v1)|[link](https://github.com/computational-machine-intelligence/leetdecoding)|
|**2025-01-05**|**Energy Optimization of Multi-task DNN Inference in MEC-assisted XR Devices: A Lyapunov-Guided Reinforcement Learning Approach**|Yanzan Sun et.al.|[2501.02572v1](http://arxiv.org/abs/2501.02572v1)|null|
|**2025-01-05**|**Decoding fMRI Data into Captions using Prefix Language Modeling**|Vyacheslav Shen et.al.|[2501.02570v1](http://arxiv.org/abs/2501.02570v1)|null|
|**2025-01-05**|**Balanced Multi-view Clustering**|Zhenglai Li et.al.|[2501.02564v1](http://arxiv.org/abs/2501.02564v1)|null|
|**2025-01-05**|**KM-UNet KAN Mamba UNet for medical image segmentation**|Yibo Zhang et.al.|[2501.02559v1](http://arxiv.org/abs/2501.02559v1)|[link](https://github.com/2760613195/km_unet)|
|**2025-01-05**|**Multi-LLM Collaborative Caption Generation in Scientific Documents**|Jaeyoung Kim et.al.|[2501.02552v1](http://arxiv.org/abs/2501.02552v1)|null|
|**2025-01-05**|**From Language To Vision: A Case Study of Text Animation**|Ping Chen et.al.|[2501.02549v1](http://arxiv.org/abs/2501.02549v1)|null|
|**2025-01-05**|**AMM: Adaptive Modularized Reinforcement Model for Multi-city Traffic Signal Control**|Zherui Huang et.al.|[2501.02548v1](http://arxiv.org/abs/2501.02548v1)|null|
|**2025-01-05**|**A completely uniform transformer for parity**|Alexander Kozachinskiy et.al.|[2501.02535v1](http://arxiv.org/abs/2501.02535v1)|null|
|**2025-01-05**|**Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm**|Ljubisa Bojic et.al.|[2501.02532v1](http://arxiv.org/abs/2501.02532v1)|null|
|**2025-01-05**|**Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI**|Ljubisa Bojic et.al.|[2501.02531v1](http://arxiv.org/abs/2501.02531v1)|null|
|**2025-01-05**|**Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation**|Dawei Dai et.al.|[2501.02523v1](http://arxiv.org/abs/2501.02523v1)|[link](https://github.com/ddw2aigroup2cqupt/face-makeup)|
|**2025-01-05**|**CHAIR-Classifier of Hallucination as Improver**|Ao Sun et.al.|[2501.02518v1](http://arxiv.org/abs/2501.02518v1)|[link](https://github.com/eggachecat/chair)|
|**2025-01-05**|**Can Impressions of Music be Extracted from Thumbnail Images?**|Takashi Harada et.al.|[2501.02511v1](http://arxiv.org/abs/2501.02511v1)|null|
|**2025-01-05**|**PTEENet: Post-Trained Early-Exit Neural Networks Augmentation for Inference Cost Optimization**|Assaf Lahiany et.al.|[2501.02508v1](http://arxiv.org/abs/2501.02508v1)|null|
|**2025-01-05**|**ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use**|Junjie Ye et.al.|[2501.02506v1](http://arxiv.org/abs/2501.02506v1)|null|
|**2025-01-05**|**Test-time Computing: from System-1 Thinking to System-2 Thinking**|Yixin Ji et.al.|[2501.02497v1](http://arxiv.org/abs/2501.02497v1)|[link](https://github.com/dereck0602/awesome_test_time_llms)|

#### Abstracts
##### **Gaussian Masked Autoencoders**
2501.03229v1 by Jathushan Rajasegaran, Xinlei Chen, Rulilong Li, Christoph Feichtenhofer, Jitendra Malik, Shiry Ginosar

This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While
reconstructive self-supervised learning frameworks such as MAE learns good
semantic abstractions, it is not trained for explicit spatial awareness. Our
approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic
abstractions and spatial understanding jointly. Like MAE, it reconstructs the
image end-to-end in the pixel space, but beyond MAE, it also introduces an
intermediate, 3D Gaussian-based representation and renders images via
splatting. We show that GMAE can enable various zero-shot learning capabilities
of spatial understanding (e.g., figure-ground segmentation, image layering,
edge detection, etc.) while preserving the high-level semantics of
self-supervised representation quality from MAE. To our knowledge, we are the
first to employ Gaussian primitives in an image representation learning
framework beyond optimization-based single-scene reconstructions. We believe
GMAE will inspire further research in this direction and contribute to
developing next-generation techniques for modeling high-fidelity visual data.
More details at https://brjathu.github.io/gmae

摘要：這篇論文探討了具有高斯潑點的遮罩式自動編碼器 (MAE)。雖然像 MAE 這樣的重建式自我監督學習架構學習了良好的語義抽象，但它並未針對明確的空間感知進行訓練。我們的方法稱為高斯遮罩式自動編碼器或 GMAE，旨在共同學習語義抽象和空間理解。它與 MAE 一樣，在像素空間中端對端重建影像，但除此之外，它還引入了中間的 3D 高斯表示，並透過潑點渲染影像。我們展示了 GMAE 能夠啟用各種空間理解的零次學習能力（例如，圖形分割、影像分層、邊緣檢測等），同時保留 MAE 自我監督表示品質的高階語義。據我們所知，我們是第一個在影像表示學習架構中採用高斯基元，而這超越了基於最佳化的單一場景重建。我們相信 GMAE 將激勵此方向的進一步研究，並有助於開發新一代的高保真視覺資料建模技術。更多詳情請見 https://brjathu.github.io/gmae

##### **LightGNN: Simple Graph Neural Network for Recommendation**
2501.03228v1 by Guoxuan Chen, Lianghao Xia, Chao Huang

Graph neural networks (GNNs) have demonstrated superior performance in
collaborative recommendation through their ability to conduct high-order
representation smoothing, effectively capturing structural information within
users' interaction patterns. However, existing GNN paradigms face significant
challenges in scalability and robustness when handling large-scale, noisy, and
real-world datasets. To address these challenges, we present LightGNN, a
lightweight and distillation-based GNN pruning framework designed to
substantially reduce model complexity while preserving essential collaboration
modeling capabilities. Our LightGNN framework introduces a computationally
efficient pruning module that adaptively identifies and removes redundant edges
and embedding entries for model compression. The framework is guided by a
resource-friendly hierarchical knowledge distillation objective, whose
intermediate layer augments the observed graph to maintain performance,
particularly in high-rate compression scenarios. Extensive experiments on
public datasets demonstrate LightGNN's effectiveness, significantly improving
both computational efficiency and recommendation accuracy. Notably, LightGNN
achieves an 80% reduction in edge count and 90% reduction in embedding entries
while maintaining performance comparable to more complex state-of-the-art
baselines. The implementation of our LightGNN framework is available at the
github repository: https://github.com/HKUDS/LightGNN.

摘要：圖形神經網路 (GNN) 已在協作推薦中展現出優異的效能，因為它們能夠執行高階表示平滑，有效擷取使用者互動模式中的結構資訊。然而，現有的 GNN 典範在處理大型、有雜訊和真實世界的資料集時，在可擴充性和穩健性方面面臨重大挑戰。為了應對這些挑戰，我們提出了 LightGNN，一個輕量級且基於蒸餾的 GNN 剪枝架構，旨在大幅降低模型複雜性，同時保留必要的協作建模功能。我們的 LightGNN 架構引入了一個計算效率高的剪枝模組，可以自適應地識別並移除冗餘邊緣和嵌入項目以進行模型壓縮。該架構由一個資源友善的分層知識蒸餾目標引導，其中間層擴充了觀察圖以維持效能，特別是在高壓縮率場景中。在公開資料集上的廣泛實驗證明了 LightGNN 的有效性，顯著提升了計算效率和推薦準確度。值得注意的是，LightGNN 在邊緣數量上減少了 80%，在嵌入項目上減少了 90%，同時維持與更複雜的最新基準相當的效能。我們的 LightGNN 框架的實作可以在 github 儲存庫中取得：https://github.com/HKUDS/LightGNN。

##### **BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning**
2501.03226v1 by Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao, Dahua Lin, Jiaqi Wang

Cutting-edge large language models (LLMs) demonstrate promising performance
in solving complex math problems with a divide-and-conquer pipeline and the
assistance of in-context learning (ICL) examples. However, their potential for
improvement is limited by two critical problems within their ICL examples:
granularity-mismatch and the ensuing negative-effect noise problem.
Specifically, the LLMs are capable of the dividing process yet mostly failed by
inaccurate reasoning within a few conquer steps, while the ICL examples
retrieved in question-grained sometimes lack relevant steps for a specific
challenging reasoning step. Further, this disconnect may hinder the correct
reasoning due to its irrelevance. To this end, we focus on improving the
reasoning quality within each step and present BoostStep. BoostStep aligns the
granularity between the retrieving and reasoning on step grained, and provides
highly related ICL examples for each reasoning step with a novel `first-try'
strategy. BoostStep provides more relevant examples than the coarse
question-grained strategy, enhancing the model reasoning quality within each
step steadily. BoostStep is a general and robust reasoning-enhancing method
that not only improves standalone reasoning performance but also integrates
seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate
generation and decision-making. Quantitatively, it improves GPT-4o and
Qwen2.5-Math-72B by 3.6\% and 2.0\% respectively on various mathematical
benchmarks, and 7.5\% gain combined with MCTS.

摘要：尖端的巨量語言模型 (LLM) 在解決複雜的數學問題上展現了有前途的表現，採用分而治之的管道和情境學習 (ICL) 範例的協助。然而，它們的進步潛力受到 ICL 範例中兩個關鍵問題的限制：粒度不匹配和隨之而來的負面效應噪音問題。具體來說，LLM 有能力進行分隔處理，但在幾個征服步驟中的不準確推理大多失敗，而以問題粒度檢索的 ICL 範例有時缺乏特定挑戰性推理步驟的相关步驟。此外，這種脫節可能會因其無關性而阻礙正確的推理。為此，我們專注於提高每個步驟中的推理品質，並提出 BoostStep。BoostStep 在步驟粒度上調整檢索和推理之間的粒度，並使用新穎的「首次嘗試」策略為每個推理步驟提供高度相關的 ICL 範例。BoostStep 提供比粗略的問題粒度策略更相關的範例，穩步提升每個步驟中的模型推理品質。BoostStep 是一種通用且強大的推理增強方法，不僅改善獨立推理表現，還能與蒙地卡羅樹狀搜尋方法 (MCTS) 無縫整合，以改善候選產生和決策制定。在量化方面，它分別在各種數學基準上將 GPT-4o 和 Qwen2.5-Math-72B 提升了 3.6% 和 2.0%，並結合 MCTS 獲得了 7.5% 的增益。

##### **Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation**
2501.03225v1 by Yuhui Zhang, Yuchang Su, Yiming Liu, Xiaohan Wang, James Burgess, Elaine Sui, Chenyu Wang, Josiah Aklilu, Alejandro Lozano, Anjiang Wei, Ludwig Schmidt, Serena Yeung-Levy

The rapid development of vision language models (VLMs) demands rigorous and
reliable evaluation. However, current visual question answering (VQA)
benchmarks often depend on open-ended questions, making accurate evaluation
difficult due to the variability in natural language responses. To address
this, we introduce AutoConverter, an agentic framework that automatically
converts these open-ended questions into multiple-choice format, enabling
objective evaluation while reducing the costly question creation process. Our
experiments demonstrate that AutoConverter can generate correct and challenging
multiple-choice questions, with VLMs demonstrating consistently similar or
lower accuracy on these questions compared to human-created ones. Using
AutoConverter, we construct VMCBench, a benchmark created by transforming 20
existing VQA datasets into a unified multiple-choice format, totaling 9,018
questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench,
setting a new standard for scalable, consistent, and reproducible VLM
evaluation.

摘要：視覺語言模型 (VLM) 的快速發展需要嚴謹且可靠的評估。然而，目前的視覺問答 (VQA) 基準通常依賴於開放式問題，由於自然語言回應的可變性，導致精確評估變得困難。為了解決這個問題，我們引入了 AutoConverter，一個自動將這些開放式問題轉換成多選題格式的代理框架，在降低成本高昂的問題建立過程的同時，實現客觀評估。我們的實驗證明，AutoConverter 可以產生正確且具有挑戰性的多選題，與人類建立的問題相比，VLM 在這些問題上表現出持續相似或較低的準確度。使用 AutoConverter，我們建立了 VMCBench，一個通過將 20 個現有的 VQA 資料集轉換成統一的多選題格式而建立的基準，總計 9,018 個問題。我們在 VMCBench 上全面評估了 33 個最先進的 VLM，為可擴展、一致且可重現的 VLM 評估設定了新標準。

##### **Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text**
2501.03212v1 by Ayat Najjar, Huthaifa I. Ashqar, Omar Darwish, Eman Hammad

The development of Generative AI Large Language Models (LLMs) raised the
alarm regarding identifying content produced through generative AI or humans.
In one case, issues arise when students heavily rely on such tools in a manner
that can affect the development of their writing or coding skills. Other issues
of plagiarism also apply. This study aims to support efforts to detect and
identify textual content generated using LLM tools. We hypothesize that
LLMs-generated text is detectable by machine learning (ML), and investigate ML
models that can recognize and differentiate texts generated by multiple LLMs
tools. We leverage several ML and Deep Learning (DL) algorithms such as Random
Forest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable
Artificial Intelligence (XAI) to understand the important features in
attribution. Our method is divided into 1) binary classification to
differentiate between human-written and AI-text, and 2) multi classification,
to differentiate between human-written text and the text generated by the five
different LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity).
Results show high accuracy in the multi and binary classification. Our model
outperformed GPTZero with 98.5\% accuracy to 78.3\%. Notably, GPTZero was
unable to recognize about 4.2\% of the observations, but our model was able to
recognize the complete test dataset. XAI results showed that understanding
feature importance across different classes enables detailed author/source
profiles. Further, aiding in attribution and supporting plagiarism detection by
highlighting unique stylistic and structural elements ensuring robust content
originality verification.

摘要：生成式 AI 大型语言模型 (LLM) 的发展引起了人们对识别通过生成式 AI 或人类产生的内容的警觉。
在一种情况下，当学生严重依赖此类工具时，可能会出现影响其写作或编码技能发展的问题。其他抄袭问题也适用。本研究旨在支持检测和识别使用 LLM 工具生成的文本内容的努力。我们假设 LLM 生成的文本可通过机器学习 (ML) 检测，并研究能够识别和区分由多个 LLM 工具生成的文本的 ML 模型。我们利用了多种 ML 和深度学习 (DL) 算法，例如随机森林 (RF) 和循环神经网络 (RNN)，并利用可解释人工智能 (XAI) 来理解归因中的重要特征。我们的方法分为 1) 二元分类，以区分人写文本和 AI 文本，以及 2) 多分类，以区分人写文本和由五种不同的 LLM 工具（ChatGPT、LLaMA、Google Bard、Claude 和 Perplexity）生成的文本。结果显示在多分类和二元分类中具有很高的准确性。我们的模型以 98.5% 的准确率优于 GPTZero，达到 78.3%。值得注意的是，GPTZero 无法识别约 4.2% 的观察结果，但我们的模型能够识别完整的测试数据集。XAI 结果表明，理解不同类别的特征重要性可以实现详细的作者/来源概况。此外，通过突出独特的风格和结构元素来辅助归因并支持抄袭检测，确保内容原创性验证的稳健性。

##### **Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity**
2501.03203v1 by Ayat A. Najjar, Huthaifa I. Ashqar, Omar A. Darwish, Eman Hammad

This study seeks to enhance academic integrity by providing tools to detect
AI-generated content in student work using advanced technologies. The findings
promote transparency and accountability, helping educators maintain ethical
standards and supporting the responsible integration of AI in education. A key
contribution of this work is the generation of the CyberHumanAI dataset, which
has 1000 observations, 500 of which are written by humans and the other 500
produced by ChatGPT. We evaluate various machine learning (ML) and deep
learning (DL) algorithms on the CyberHumanAI dataset comparing human-written
and AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).
Results demonstrate that traditional ML algorithms, specifically XGBoost and
Random Forest, achieve high performance (83% and 81% accuracies respectively).
Results also show that classifying shorter content seems to be more challenging
than classifying longer content. Further, using Explainable Artificial
Intelligence (XAI) we identify discriminative features influencing the ML
model's predictions, where human-written content tends to use a practical
language (e.g., use and allow). Meanwhile AI-generated text is characterized by
more abstract and formal terms (e.g., realm and employ). Finally, a comparative
analysis with GPTZero show that our narrowly focused, simple, and fine-tuned
model can outperform generalized systems like GPTZero. The proposed model
achieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when
tasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a
tendency to classify challenging and small-content cases as either mixed or
unrecognized while our proposed model showed a more balanced performance across
the three classes.

摘要：<paragraph>本研究旨在透過提供工具來偵測學生作業中使用進階技術產生的 AI 內容，以提升學術誠信。研究結果促進透明度和問責制，協助教育工作者維持倫理標準，並支持負責任地將 AI 整合到教育中。這項研究的一項重要貢獻是產生 CyberHumanAI 資料集，其中包含 1000 個觀察結果，其中 500 個是由人類撰寫，另外 500 個是由 ChatGPT 產生。我們在 CyberHumanAI 資料集上評估各種機器學習 (ML) 和深度學習 (DL) 演算法，比較人類撰寫的內容和大型語言模型 (LLM)（即 ChatGPT）產生的 AI 內容。結果顯示，傳統的 ML 演算法，特別是 XGBoost 和隨機森林，能達到高準確度（分別為 83% 和 81%）。結果還顯示，對較短內容進行分類似乎比對較長內容進行分類更具挑戰性。此外，透過使用可解釋人工智慧 (XAI)，我們找出影響 ML 模型預測的區別性特徵，其中人類撰寫的內容傾向於使用實用的語言（例如，使用和允許）。與此同時，AI 生成的文字的特徵是使用更抽象和正式的術語（例如，領域和使用）。最後，與 GPTZero 進行比較分析顯示，我們狹義聚焦、簡單且經過微調的模型可以勝過 GPTZero 等通用系統。在對純 AI、純人類和混合類別進行分類時，建議的模型達到約 77.5% 的準確度，而 GPTZero 的準確度為 48.5%。GPTZero 傾向於將具挑戰性和小內容的案例分類為混合或無法辨識，而我們建議的模型在三個類別中表現出更平衡的效能。</paragraph>

##### **The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input**
2501.03200v1 by Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, Sasha Goldshtein, Dipanjan Das

We introduce FACTS Grounding, an online leaderboard and associated benchmark
that evaluates language models' ability to generate text that is factually
accurate with respect to given context in the user prompt. In our benchmark,
each prompt includes a user request and a full document, with a maximum length
of 32k tokens, requiring long-form responses. The long-form responses are
required to be fully grounded in the provided context document while fulfilling
the user request. Models are evaluated using automated judge models in two
phases: (1) responses are disqualified if they do not fulfill the user request;
(2) they are judged as accurate if the response is fully grounded in the
provided document. The automated judge models were comprehensively evaluated
against a held-out test-set to pick the best prompt template, and the final
factuality score is an aggregate of multiple judge models to mitigate
evaluation bias. The FACTS Grounding leaderboard will be actively maintained
over time, and contains both public and private splits to allow for external
participation while guarding the integrity of the leaderboard. It can be found
at https://www.kaggle.com/facts-leaderboard.

摘要：我們引入了 FACTS Grounding，這是一個線上排行榜和相關基準，用於評估語言模型根據使用者提示中提供的背景，產生事實上準確的文本的能力。在我們的基準中，每個提示都包含一個使用者請求和一個完整文件，最大長度為 32k 個符號，需要長篇回應。長篇回應必須完全建立在提供的背景文件中，同時滿足使用者的請求。模型使用自動評分模型在兩個階段進行評估：(1) 如果回應不滿足使用者的請求，則取消資格；(2) 如果回應完全建立在提供的文件中，則判斷為準確。自動評分模型根據保留的測試集進行全面評估，以選擇最佳提示範本，最終的事實分數是多個評分模型的總和，以減輕評估偏差。FACTS Grounding 排行榜將隨著時間積極維護，並包含公開和私人拆分，以允許外部參與，同時保護排行榜的完整性。它可以在 https://www.kaggle.com/facts-leaderboard 找到。

##### **CLIX: Cross-Lingual Explanations of Idiomatic Expressions**
2501.03191v1 by Aaron Gluck, Katharina von der Wense, Maria Pacheco

Automated definition generation systems have been proposed to support
vocabulary expansion for language learners. The main barrier to the success of
these systems is that learners often struggle to understand definitions due to
the presence of potentially unfamiliar words and grammar, particularly when
non-standard language is involved. To address these challenges, we propose
CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We
explore the capabilities of current NLP models for this task, and observe that
while it remains challenging, large language models show promise. Finally, we
perform a detailed error analysis to highlight the key challenges that need to
be addressed before we can reliably incorporate these systems into educational
tools.

摘要：自動定義產生系統已被提議用於支援語言學習者的詞彙擴充。這些系統成功的最大障礙是，學習者經常難以理解定義，原因在於存在潛在不熟悉的字詞和文法，特別是在涉及非標準語言時。為了應對這些挑戰，我們提出 CLIX，即慣用語的跨語言解釋任務。我們探討了當前 NLP 模型對此任務的能力，並觀察到儘管這仍然具有挑戰性，但大型語言模型展現了前景。最後，我們執行詳細的錯誤分析，以強調在我們能夠將這些系統可靠地整合到教育工具之前需要應對的主要挑戰。

##### **Turn-based Multi-Agent Reinforcement Learning Model Checking**
2501.03187v1 by Dennis Gross

In this paper, we propose a novel approach for verifying the compliance of
turn-based multi-agent reinforcement learning (TMARL) agents with complex
requirements in stochastic multiplayer games. Our method overcomes the
limitations of existing verification approaches, which are inadequate for
dealing with TMARL agents and not scalable to large games with multiple agents.
Our approach relies on tight integration of TMARL and a verification technique
referred to as model checking. We demonstrate the effectiveness and scalability
of our technique through experiments in different types of environments. Our
experiments show that our method is suited to verify TMARL agents and scales
better than naive monolithic model checking.

摘要：在本文中，我們提出了一種新穎的方法來驗證基於回合的多智能體強化學習 (TMARL) 代理是否符合隨機多玩家遊戲中的複雜需求。我們的這種方法克服了現有驗證方法的限制，而這些限制不足以應對 TMARL 代理，並且無法擴展到具有多個代理的大型遊戲。我們的這種方法依賴於 TMARL 和一種稱為模型檢查的驗證技術的緊密整合。我們通過在不同類型的環境中進行實驗來證明我們的這種技術的有效性和可擴展性。我們的實驗表明，我們的這種方法適用於驗證 TMARL 代理，並且比單一的模型檢查擴展得更好。

##### **GLiREL -- Generalist Model for Zero-Shot Relation Extraction**
2501.03172v1 by Jack Boylan, Chris Hokamp, Demian Gholipour Ghalandari

We introduce GLiREL (Generalist Lightweight model for zero-shot Relation
Extraction), an efficient architecture and training paradigm for zero-shot
relation classification. Inspired by recent advancements in zero-shot named
entity recognition, this work presents an approach to efficiently and
accurately predict zero-shot relationship labels between multiple entities in a
single forward pass. Experiments using the FewRel and WikiZSL benchmarks
demonstrate that our approach achieves state-of-the-art results on the
zero-shot relation classification task. In addition, we contribute a protocol
for synthetically-generating datasets with diverse relation labels.

摘要：我們介紹 GLiREL（零次關係抽取的通才輕量級模型），這是一種用於零次關係分類的高效架構和訓練範例。受到近期零次命名實體辨識進展的啟發，本研究提出一個方法，可以在單次前向傳遞中，有效且準確地預測多個實體之間的零次關係標籤。使用 FewRel 和 WikiZSL 基準所做的實驗證明，我們的做法在零次關係分類任務中取得了最先進的成果。此外，我們提供了一個使用多元關係標籤合成產生資料集的協定。

##### **Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text**
2501.03166v1 by Ali Al-Lawati, Jason Lucas, Prasenjit Mitra

Large Language Models (LLMs) have demonstrated remarkable performance in
various NLP tasks, including semantic parsing, which trans lates natural
language into formal code representations. However, the reverse process,
translating code into natural language, termed semantic captioning, has
received less attention. This task is becoming increasingly important as LLMs
are integrated into platforms for code generation, security analysis, and
educational purposes. In this paper, we focus on the captioning of SQL query
(SQL2Text) to address the critical need for understanding and explaining SQL
queries in an era where LLM-generated code poses potential security risks. We
repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt
using GPT-4o to generate multiple additional utterances, which enhances the
robustness of the datasets for the reverse task. We conduct our experiments
using in-context learning (ICL) based on different sample selection methods,
emphasizing smaller, more computationally efficient LLMs. Our findings
demonstrate that leveraging the inherent graph properties of SQL for ICL sample
selection significantly outperforms random selection by up to 39% on BLEU score
and provides better results than alternative methods. Dataset and codes are
published: \url{https://github.com/aliwister/ast-icl}.

摘要：大型語言模型 (LLM) 已在各種 NLP 任務中展現出驚人的效能，包括語意分析，它將自然語言轉換為正式的程式碼表示。然而，反向過程，將程式碼轉換為自然語言，稱為語意標題，則較少受到關注。隨著 LLM 整合到程式碼產生、安全性分析和教育目的的平台中，這項任務正變得越來越重要。在本文中，我們專注於 SQL 查詢的標題 (SQL2Text)，以滿足在 LLM 產生的程式碼構成潛在安全風險的時代中，理解和解釋 SQL 查詢的關鍵需求。我們透過使用 GPT-4o 導入反覆的 ICL 提示來產生多個額外的語句，重新調整 Text2SQL 資料集以用於 SQL2Text，這增強了資料集對反向任務的穩健性。我們使用基於不同範例選取方法的情境學習 (ICL) 進行實驗，強調較小、計算效率較高的 LLM。我們的研究結果證明，利用 SQL 的內在圖形屬性進行 ICL 範例選取，在 BLEU 分數上顯著優於隨機選取，最多可達 39%，並提供比其他方法更好的結果。資料集和程式碼已發布：\url{https://github.com/aliwister/ast-icl}。

##### **The Scaling Law for LoRA Base on Mutual Information Upper Bound**
2501.03152v1 by Jing Zhang, Hui Gao, Peng Zhang, Shuzhen Sun, Chang Yang, Yuexian Hou

LoRA (Low-Rank Adaptation) is a widely used model fine-tuning method. In
fine-tuning, the law among model performance, model parameters, and data
complexity has been a focal issue in the field. Existing methods often leverage
external metrics (such as cross-entropy or perplexity) to evaluate model
performance. In the fine-tuning process for large models, two types of
knowledge are typically involved: the frozen, general knowledge acquired by the
model during pre-training and the new knowledge learned through the LoRA module
from the current data. Generally, the less LoRA's learned knowledge relies on
the large model, the more it captures the specific knowledge of new data,
thereby enhancing its adaptability to new tasks. However, external metrics do
not readily capture the dependency relationship between these two types of
knowledge. Therefore, we designed an internal metric based on the Mutual
Information Upper Bound (MIUB) theory to investigate the scaling law of
large-model LoRA fine-tuning. In our experiments, we validated this approach on
benchmark datasets, using the Llama3-8B and Phi3-3B models. The results show
that the proposed MIUB metric aligns more accurately and stably with the
scaling law of LoRA fine-tuning compared to cross-entropy and perplexity.

摘要：LoRA（低秩適應）是一種廣泛使用的模型微調方法。在微調中，模型效能、模型參數和資料複雜度之間的定律一直是該領域的焦點議題。現有方法通常利用外部指標（例如交叉熵或困惑度）來評估模型效能。在大型模型的微調過程中，通常涉及兩種知識：模型在預訓練期間獲得的凍結式一般知識，以及透過 LoRA 模組從當前資料中學習到的新知識。一般來說，LoRA 學習到的知識越不依賴於大型模型，它擷取新資料的特定知識就越多，從而增強其對新任務的適應性。然而，外部指標並不能輕易擷取這兩種知識之間的依賴關係。因此，我們根據互資訊上限 (MIUB) 理論設計了一個內部指標，來探討大型模型 LoRA 微調的規模定律。在我們的實驗中，我們使用 Llama3-8B 和 Phi3-3B 模型，在基準資料集上驗證了這種方法。結果表明，與交叉熵和困惑度相比，所提出的 MIUB 指標與 LoRA 微調的規模定律更準確且穩定地對齊。

##### **Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches**
2501.03151v1 by Alhassan Mumuni, Fuseini Mumuni

Generative artificial intelligence (AI) systems based on large-scale
pretrained foundation models (PFMs) such as vision-language models, large
language models (LLMs), diffusion models and vision-language-action (VLA)
models have demonstrated the ability to solve complex and truly non-trivial AI
problems in a wide variety of domains and contexts. Multimodal large language
models (MLLMs), in particular, learn from vast and diverse data sources,
allowing rich and nuanced representations of the world and, thereby, providing
extensive capabilities, including the ability to reason, engage in meaningful
dialog; collaborate with humans and other agents to jointly solve complex
problems; and understand social and emotional aspects of humans. Despite this
impressive feat, the cognitive abilities of state-of-the-art LLMs trained on
large-scale datasets are still superficial and brittle. Consequently, generic
LLMs are severely limited in their generalist capabilities. A number of
foundational problems -- embodiment, symbol grounding, causality and memory --
are required to be addressed for LLMs to attain human-level general
intelligence. These concepts are more aligned with human cognition and provide
LLMs with inherent human-like cognitive properties that support the realization
of physically-plausible, semantically meaningful, flexible and more
generalizable knowledge and intelligence. In this work, we discuss the
aforementioned foundational issues and survey state-of-the art approaches for
implementing these concepts in LLMs. Specifically, we discuss how the
principles of embodiment, symbol grounding, causality and memory can be
leveraged toward the attainment of artificial general intelligence (AGI) in an
organic manner.

摘要：<paragraph>基於大規模預訓練基礎模型 (PFM) 的生成式人工智慧 (AI) 系統，例如視覺語言模型、大型語言模型 (LLM)、擴散模型和視覺語言動作 (VLA) 模型，已展現了解決各種領域和情境中複雜且真正非平凡的人工智慧問題的能力。特別是多模態大型語言模型 (MLLM)，從廣泛且多樣化的資料來源學習，能豐富且細緻地呈現世界，因此提供了廣泛的能力，包括推理、進行有意義的對話；與人類和其他代理人合作，共同解決複雜的問題；以及了解人類的社會和情緒面向。儘管有此令人印象深刻的壯舉，在大型資料集上訓練的最先進 LLM 的認知能力仍然膚淺且脆弱。因此，通用 LLM 的通才能力受到嚴重限制。LLM 要達到人類層級的通用智慧，需要解決許多基礎問題，包括具身化、符號接地、因果關係和記憶。這些概念更符合人類認知，並為 LLM 提供了內在的人類認知特性，支持實現物理上合理、語義上意義重大、靈活且更具概括性的知識和智慧。在這項工作中，我們討論了上述基礎問題，並探討了在 LLM 中實施這些概念的最先進方法。具體來說，我們討論了如何利用具身化、符號接地、因果關係和記憶的原則，以有機的方式實現人工通用智慧 (AGI)。</paragraph>

##### **Co-Activation Graph Analysis of Safety-Verified and Explainable Deep Reinforcement Learning Policies**
2501.03142v1 by Dennis Gross, Helge Spieker

Deep reinforcement learning (RL) policies can demonstrate unsafe behaviors
and are challenging to interpret. To address these challenges, we combine RL
policy model checking--a technique for determining whether RL policies exhibit
unsafe behaviors--with co-activation graph analysis--a method that maps neural
network inner workings by analyzing neuron activation patterns--to gain insight
into the safe RL policy's sequential decision-making. This combination lets us
interpret the RL policy's inner workings for safe decision-making. We
demonstrate its applicability in various experiments.

摘要：深度強化學習 (RL) 政策可能會表現出不安全的行為，且難以解釋。為了應對這些挑戰，我們結合了 RL 政策模型檢查（一種用於確定 RL 政策是否表現出不安全行為的技術）和共激活圖形分析（一種透過分析神經元激活模式來繪製神經網路內部運作的方法），以深入了解安全的 RL 政策的順序決策制定。這種結合讓我們可以解釋 RL 政策的內部運作，以進行安全的決策制定。我們在各種實驗中展示了它的適用性。

##### **VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity**
2501.03139v1 by Yerong Li, Yiren Liu, Yun Huang

Scenario-based training has been widely adopted in many public service
sectors. Recent advancements in Large Language Models (LLMs) have shown promise
in simulating diverse personas to create these training scenarios. However,
little is known about how LLMs can be developed to simulate victims for
scenario-based training purposes. In this paper, we introduce VicSim (victim
simulator), a novel model that addresses three key dimensions of user
simulation: informational faithfulness, emotional dynamics, and language style
(e.g., grammar usage). We pioneer the integration of scenario-based victim
modeling with GAN-based training workflow and key-information-based prompting,
aiming to enhance the realism of simulated victims. Our adversarial training
approach teaches the discriminator to recognize grammar and emotional cues as
reliable indicators of synthetic content. According to evaluations by human
raters, the VicSim model outperforms GPT-4 in terms of human-likeness.

摘要：情境式訓練已廣泛應用於許多公共服務部門。大型語言模型 (LLM) 的最新進展已展現出模擬各種角色以建立這些訓練情境的潛力。然而，鮮少人了解如何開發 LLM 以模擬受害者，作為情境式訓練用途。在本文中，我們介紹了 VicSim（受害者模擬器），這是一種新穎的模型，可解決使用者模擬的三個關鍵面向：資訊真實性、情緒動態和語言風格（例如，語法使用）。我們率先將情境式受害者建模與基於 GAN 的訓練工作流程和基於關鍵資訊的提示整合在一起，旨在增強模擬受害者的真實性。我們的對抗式訓練方法教導辨識器將語法和情緒線索視為合成內容的可靠指標。根據人類評分者的評量，VicSim 模型在類人化方面優於 GPT-4。

##### **PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models**
2501.03124v1 by Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, Yu Cheng

Process-level Reward Models (PRMs) are crucial for complex reasoning and
decision-making tasks, where each intermediate step plays an important role in
the reasoning process. Since language models are prone to various types of
errors during the reasoning process, PRMs are required to possess nuanced
capabilities for detecting various implicit error types in real-world
scenarios. However, current benchmarks primarily focus on step correctness,
failing to evaluate PRMs' performance systematically. To address this gap, we
introduce PRMBench, a process-level benchmark specifically designed to assess
the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216
carefully designed problems and 83,456 step-level labels, evaluating models
across multiple dimensions, including simplicity, soundness, and sensitivity.
In our experiments on 15 models, spanning both open-source PRMs and
closed-source large language models prompted as critic models, we uncover
significant weaknesses in current PRMs. These findings underscore the
challenges inherent in process-level evaluation and highlight key directions
for future research. We hope PRMBench can be a robust bench for advancing
research on PRM evaluation and development.

摘要：<paragraph>處理層級獎勵模型 (PRM) 對於複雜的推理和決策任務至關重要，其中每個中間步驟在推理過程中扮演重要角色。由於語言模型在推理過程中容易出現各種類型的錯誤，因此需要 PRM 擁有細微入微的能力來偵測現實世界場景中的各種隱含錯誤類型。然而，目前的基準主要關注步驟正確性，無法系統性地評估 PRM 的效能。為了解決這個差距，我們引入了 PRMBench，一個專門設計用於評估 PRM 的細粒度錯誤偵測能力的流程層級基準。PRMBench 包含 6,216 個精心設計的問題和 83,456 個步驟層級標籤，從多個面向評估模型，包括簡潔性、健全性和敏感度。在我們對 15 個模型的實驗中，涵蓋開源 PRM 和封閉原始碼大型語言模型，提示為批評模型，我們發現了當前 PRM 的顯著弱點。這些發現強調了流程層級評估中固有的挑戰，並突出了未來研究的主要方向。我們希望 PRMBench 能成為推進 PRM 評估和開發研究的強大基準。</paragraph>

##### **From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning**
2501.03119v1 by Chao Feng, Yuanzhe Gao, Alberto Huertas Celdran, Gerome Bovet, Burkhard Stiller

Federated Learning (FL) is widely recognized as a privacy-preserving machine
learning paradigm due to its model-sharing mechanism that avoids direct data
exchange. However, model training inevitably leaves exploitable traces that can
be used to infer sensitive information. In Decentralized FL (DFL), the overlay
topology significantly influences its models' convergence, robustness, and
security. This study explores the feasibility of inferring the overlay topology
of DFL systems based solely on model behavior, introducing a novel Topology
Inference Attack. A taxonomy of topology inference attacks is proposed,
categorizing them by the attacker's capabilities and knowledge. Practical
attack strategies are developed for different scenarios, and quantitative
experiments are conducted to identify key factors influencing the attack
effectiveness. Experimental results demonstrate that analyzing only the public
models of individual nodes can accurately infer the DFL topology, underscoring
the risk of sensitive information leakage in DFL systems. This finding offers
valuable insights for improving privacy preservation in decentralized learning
environments.

摘要：聯邦學習 (FL) 由於其避免直接資料交換的模型共享機制，被廣泛認為是一種保護隱私的機器學習範例。然而，模型訓練不可避免地會留下可利用的痕跡，可被用於推斷敏感資訊。在分散式 FL (DFL) 中，疊加拓撲結構顯著影響其模型的收斂性、穩健性和安全性。本研究探討僅根據模型行為推斷 DFL 系統疊加拓撲結構的可行性，並引入了一種新型的拓撲推斷攻擊。提出了一種拓撲推斷攻擊分類法，根據攻擊者的能力和知識對其進行分類。針對不同的場景制定了實用的攻擊策略，並進行了定量實驗以找出影響攻擊有效性的關鍵因素。實驗結果表明，僅分析個別節點的公開模型就可以準確推斷出 DFL 拓撲結構，這強調了 DFL 系統中敏感資訊洩漏的風險。這一發現為改善分散式學習環境中的隱私保護提供了寶貴的見解。

##### **LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases**
2501.03112v1 by Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Viren Bajaj, Zeya Ahmad

Large Language Models (LLMs) have been observed to exhibit bias in numerous
ways, potentially creating or worsening outcomes for specific groups identified
by protected attributes such as sex, race, sexual orientation, or age. To help
address this gap, we introduce LangFair, an open-source Python package that
aims to equip LLM practitioners with the tools to evaluate bias and fairness
risks relevant to their specific use cases. The package offers functionality to
easily generate evaluation datasets, comprised of LLM responses to
use-case-specific prompts, and subsequently calculate applicable metrics for
the practitioner's use case. To guide in metric selection, LangFair offers an
actionable decision framework.

摘要：大型語言模型 (LLM) 已被觀察到以各種方式表現出偏見，可能會對由受保護屬性（例如性別、種族、性取向或年齡）識別的特定群體造成或惡化結果。為了幫助解決這個差距，我們引入了 LangFair，這是一個開源的 Python 套件，旨在為 LLM 從業者提供評估偏見和與其特定用例相關的公平性風險的工具。該套件提供功能，可輕鬆產生評估資料集，其中包含 LLM 對特定用例提示的回應，並隨後計算適用於從業者用例的指標。為了指導指標選擇，LangFair 提供了一個可行的決策架構。

##### **Sentiment-guided Commonsense-aware Response Generation for Mental Health Counseling**
2501.03088v1 by Aseem Srivastava, Gauri Naik, Alison Cerezo, Tanmoy Chakraborty, Md. Shad Akhtar

The crisis of mental health issues is escalating. Effective counseling serves
as a critical lifeline for individuals suffering from conditions like PTSD,
stress, etc. Therapists forge a crucial therapeutic bond with clients, steering
them towards positivity. Unfortunately, the massive shortage of professionals,
high costs, and mental health stigma pose significant barriers to consulting
therapists. As a substitute, Virtual Mental Health Assistants (VMHAs) have
emerged in the digital healthcare space. However, most existing VMHAs lack the
commonsense to understand the nuanced sentiments of clients to generate
effective responses. To this end, we propose EmpRes, a novel sentiment-guided
mechanism incorporating commonsense awareness for generating responses. By
leveraging foundation models and harnessing commonsense knowledge, EmpRes aims
to generate responses that effectively shape the client's sentiment towards
positivity. We evaluate the performance of EmpRes on HOPE, a benchmark
counseling dataset, and observe a remarkable performance improvement compared
to the existing baselines across a suite of qualitative and quantitative
metrics. Moreover, our extensive empirical analysis and human evaluation show
that the generation ability of EmpRes is well-suited and, in some cases,
surpasses the gold standard. Further, we deploy EmpRes as a chat interface for
users seeking mental health support. We address the deployed system's
effectiveness through an exhaustive user study with a significant positive
response. Our findings show that 91% of users find the system effective, 80%
express satisfaction, and over 85.45% convey a willingness to continue using
the interface and recommend it to others, demonstrating the practical
applicability of EmpRes in addressing the pressing challenges of mental health
support, emphasizing user feedback, and ethical considerations in a real-world
context.

摘要：心理健康問題的危機正在升高。有效的諮詢服務對患有創傷後壓力症候群、壓力等狀況的個人來說，是一個重要的生命線。治療師與個案建立重要的治療連結，引導他們走向正向。不幸的是，專業人員的大量短缺、高成本和心理健康污名化，對諮詢治療師造成重大的障礙。作為替代方案，虛擬心理健康助理 (VMHA) 已出現在數位醫療保健領域。然而，大多數現有的 VMHA 缺乏常識，無法理解個案細微的情緒，以產生有效的回應。為此，我們提出了 EmpRes，這是一種創新的情緒引導機制，結合常識意識來產生回應。透過利用基礎模型和運用常識知識，EmpRes 旨在產生有效塑造個案情緒走向正向的回應。我們在 HOPE，一個基準諮詢資料集上評估 EmpRes 的表現，並觀察到與現有基線相比，在各種定性和定量指標上都有顯著的表現提升。此外，我們廣泛的實證分析和人為評估顯示，EmpRes 的生成能力非常合適，在某些情況下，甚至超越了黃金標準。此外，我們將 EmpRes 部署為一個聊天介面，供尋求心理健康支持的使用者使用。我們透過一項詳盡的使用者研究，以顯著正向的回應，來探討已部署系統的有效性。我們的研究結果顯示，91% 的使用者認為該系統有效，80% 表示滿意，超過 85.45% 的人表示願意繼續使用該介面並推薦給其他人，這證明了 EmpRes 在解決心理健康支持的迫切挑戰、強調使用者回饋和在現實世界中的倫理考量方面的實際適用性。

##### **Personalized Fashion Recommendation with Image Attributes and Aesthetics Assessment**
2501.03085v1 by Chongxian Chen, Fan Mo, Xin Fan, Hayato Yamana

Personalized fashion recommendation is a difficult task because 1) the
decisions are highly correlated with users' aesthetic appetite, which previous
work frequently overlooks, and 2) many new items are constantly rolling out
that cause strict cold-start problems in the popular identity (ID)-based
recommendation methods. These new items are critical to recommend because of
trend-driven consumerism. In this work, we aim to provide more accurate
personalized fashion recommendations and solve the cold-start problem by
converting available information, especially images, into two attribute graphs
focusing on optimized image utilization and noise-reducing user modeling.
Compared with previous methods that separate image and text as two components,
the proposed method combines image and text information to create a richer
attributes graph. Capitalizing on the advancement of large language and vision
models, we experiment with extracting fine-grained attributes efficiently and
as desired using two different prompts. Preliminary experiments on the IQON3000
dataset have shown that the proposed method achieves competitive accuracy
compared with baselines.

摘要：客製化時尚推薦是一項困難的任務，因為 1) 決策與使用者的美學喜好高度相關，而先前的研究經常忽略這一點，以及 2) 許多新商品不斷推出，這會在流行的身分 (ID) 為基礎的推薦方法中造成嚴重的冷啟動問題。這些新商品對於推薦至關重要，因為它們會引領消費趨勢。在這項研究中，我們旨在提供更準確的客製化時尚推薦，並透過將可用資訊（尤其是圖片）轉換成兩個屬性圖表來解決冷啟動問題，重點在於最佳化圖片使用和降低雜訊的使用者建模。與將圖片和文字分開為兩個組成的先前方法相比，所提出的方法結合圖片和文字資訊，以建立更豐富的屬性圖表。利用大型語言和視覺模型的進步，我們嘗試使用兩種不同的提示有效率且如預期般地萃取細緻的屬性。在 IQON3000 資料集上的初步實驗顯示，與基準相比，所提出的方法達到了競爭力的準確度。

##### **Trust Modeling in Counseling Conversations: A Benchmark Study**
2501.03064v1 by Aseem Srivastava, Zuhair Hasan Shaik, Tanmoy Chakraborty, Md Shad Akhtar

In mental health counseling, a variety of earlier studies have focused on
dialogue modeling. However, most of these studies give limited to no emphasis
on the quality of interaction between a patient and a therapist. The
therapeutic bond between a patient and a therapist directly correlates with
effective mental health counseling. It involves developing the patient's trust
on the therapist over the course of counseling. To assess the therapeutic bond
in counseling, we introduce trust as a therapist-assistive metric. Our
definition of trust involves patients' willingness and openness to express
themselves and, consequently, receive better care. We conceptualize it as a
dynamic trajectory observable through textual interactions during the
counseling. To facilitate trust modeling, we present MENTAL-TRUST, a novel
counseling dataset comprising manual annotation of 212 counseling sessions with
first-of-its-kind seven expert-verified ordinal trust levels. We project our
problem statement as an ordinal classification task for trust quantification
and propose a new benchmark, TrustBench, comprising a suite of classical and
state-of-the-art language models on MENTAL-TRUST. We evaluate the performance
across a suite of metrics and lay out an exhaustive set of findings. Our study
aims to unfold how trust evolves in therapeutic interactions.

摘要：在心理健康諮商中，許多早期的研究都專注於對話建模。然而，這些研究大多僅強調或不強調患者與治療師之間互動的品質。患者與治療師之間的治療關係與有效的心理健康諮商直接相關。這涉及在諮商過程中建立患者對治療師的信任。為了評估諮商中的治療關係，我們將信任引入作為治療師輔助指標。我們對信任的定義包括患者表達自己的意願和開放性，並因此獲得更好的照護。我們將其概念化為在諮商期間透過文字互動可以觀察到的動態軌跡。為了促進信任建模，我們提出 MENTAL-TRUST，這是一個新穎的諮商資料集，包含 212 個諮商會談的手動註解，以及首創的七個由專家驗證的序數信任等級。我們將我們的問題陳述投射為信任量化的序數分類任務，並提出一個新的基準 TrustBench，其中包含一組 MENTAL-TRUST 上的經典和最先進的語言模型。我們評估了一系列指標的效能，並列出詳盡的發現。我們的研究旨在探討信任如何在治療互動中演變。

##### **Survival Analysis Revisited: Understanding and Unifying Poisson, Exponential, and Cox Models in Fall Risk Analysis**
2501.03058v1 by Tianhua Chen

This paper explores foundational and applied aspects of survival analysis,
using fall risk assessment as a case study. It revisits key time-related
probability distributions and statistical methods, including logistic
regression, Poisson regression, Exponential regression, and the Cox
Proportional Hazards model, offering a unified perspective on their
relationships within the survival analysis framework. A contribution of this
work is the step-by-step derivation and clarification of the relationships
among these models, particularly demonstrating that Poisson regression in the
survival context is a specific case of the Cox model. These insights address
gaps in understanding and reinforce the simplicity and interpretability of
survival models. The paper also emphasizes the practical utility of survival
analysis by connecting theoretical insights with real-world applications. In
the context of fall detection, it demonstrates how these models can
simultaneously predict fall risk, analyze contributing factors, and estimate
time-to-event outcomes within a single streamlined framework. In contrast,
advanced deep learning methods often require complex post-hoc interpretation
and separate training for different tasks particularly when working with
structured numerical data. This highlights the enduring relevance of classical
statistical frameworks and makes survival models especially valuable in
healthcare settings, where explainability and robustness are critical. By
unifying foundational concepts and offering a cohesive perspective on
time-to-event analysis, this work serves as an accessible resource for
understanding survival models and applying them effectively to diverse
analytical challenges.

摘要：本文探討生存分析的基本和應用面向，並以跌倒風險評估作為案例研究。本文回顧了與時間相關的主要機率分佈和統計方法，包括邏輯迴歸、泊松迴歸、指數迴歸和考克斯比例風險模型，並提供了一個統一的觀點，說明它們在生存分析架構中的關係。這項工作的貢獻在於逐步推導和釐清這些模型之間的關係，特別是證明生存背景下的泊松迴歸是考克斯模型的特定情況。這些見解解決了理解上的差距，並強化了生存模型的簡潔性和可解釋性。本文也強調了生存分析的實用性，它將理論見解與真實世界的應用連結起來。在跌倒偵測的背景下，本文示範了這些模型如何同時預測跌倒風險、分析促成因素，並在單一簡化的架構內估計事件發生時間的結果。相比之下，進階深度學習方法通常需要複雜的事後解釋，而且在處理結構化數值資料時，需要針對不同的任務進行個別訓練。這突顯了傳統統計架構的持續相關性，並使生存模型在醫療保健環境中特別有價值，因為在醫療保健環境中，可解釋性和穩健性至關重要。透過統一基本概念，並對事件發生時間分析提供一個有凝聚力的觀點，這項工作作為一個可存取的資源，協助理解生存模型並有效地將它們應用於各種分析挑戰。

##### **Single-Channel Distance-Based Source Separation for Mobile GPU in Outdoor and Indoor Environments**
2501.03045v1 by Hanbin Bae, Byungjun Kang, Jiwon Kim, Jaeyong Hwang, Hosang Sung, Hoon-Young Cho

This study emphasizes the significance of exploring distance-based source
separation (DSS) in outdoor environments. Unlike existing studies that
primarily focus on indoor settings, the proposed model is designed to capture
the unique characteristics of outdoor audio sources. It incorporates advanced
techniques, including a two-stage conformer block, a linear relation-aware
self-attention (RSA), and a TensorFlow Lite GPU delegate. While the linear RSA
may not capture physical cues as explicitly as the quadratic RSA, the linear
RSA enhances the model's context awareness, leading to improved performance on
the DSS that requires an understanding of physical cues in outdoor and indoor
environments. The experimental results demonstrated that the proposed model
overcomes the limitations of existing approaches and considerably enhances
energy efficiency and real-time inference speed on mobile devices.

摘要：本研究強調在戶外環境中探索基於距離的來源分離 (DSS) 的重要性。有別於現有主要專注於室內設定的研究，所提出的模型旨在捕捉戶外音訊來源的獨特特徵。它結合了先進的技術，包括兩階段共形器區塊、線性關係感知自注意力 (RSA) 和 TensorFlow Lite GPU 委派。雖然線性 RSA 可能無法像二次 RSA 那樣明確地捕捉物理線索，但線性 RSA 增強了模型的背景感知，從而提高了對 DSS 的效能，DSS 要求理解戶外和室內環境中的物理線索。實驗結果表明，所提出的模型克服了現有方法的限制，並顯著提高了行動裝置上的能源效率和即時推論速度。

##### **ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events**
2501.03040v1 by Duygu Sezen Islakoglu, Jan-Christoph Kalo

Large Language Models (LLMs) have achieved remarkable success in various NLP
tasks, yet they still face significant challenges in reasoning and arithmetic.
Temporal reasoning, a critical component of natural language understanding, has
raised increasing research attention. However, comprehensive testing of Allen's
interval relations (e.g., before, after, during) -- a fundamental framework for
temporal relationships -- remains underexplored. To fill this gap, we present
ChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It
includes 16 tasks, focusing on identifying the Allen relation between two
temporal events and temporal arithmetic, using both abstract events and
real-world data from Wikidata. We assess the performance of seven recent LLMs
using this benchmark and the results indicate that models handle Allen
relations, even symmetrical ones, quite differently. Moreover, the findings
suggest that the models may rely on memorization to answer time-related
questions. Overall, the models' low performance highlights the need for
improved temporal understanding in LLMs and ChronoSense offers a robust
framework for future research in this area. Our dataset and the source code are
available at https://github.com/duyguislakoglu/chronosense.

摘要：大型語言模型 (LLM) 在各種自然語言處理 (NLP) 任務中取得了顯著的成功，但它們在推理和算術方面仍然面臨著重大的挑戰。
時間推理是自然語言理解的關鍵組成部分，引起了越來越多的研究關注。然而，艾倫的區間關係（例如，之前、之後、期間）的全面測試——時間關係的基本框架——仍然未得到充分探索。為了填補這一空白，我們提出了 ChronoSense，這是一個用於評估 LLM 時間理解的新基準。它包括 16 項任務，重點是識別兩個時間事件之間的艾倫關係和時間算術，同時使用抽象事件和來自 Wikidata 的真實世界數據。我們使用此基準評估了七個最近的 LLM 的性能，結果表明模型處理艾倫關係（甚至是對稱關係）的方式截然不同。此外，研究結果表明，這些模型可能依賴於記憶來回答與時間相關的問題。總的來說，模型的低性能突顯了需要改進 LLM 中的時間理解，而 ChronoSense 為這一領域的未來研究提供了一個穩健的框架。我們的數據集和源代碼可以在 https://github.com/duyguislakoglu/chronosense 獲得。

##### **Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders**
2501.03038v1 by Dichucheng Li, Yongyi Zang, Qiuqiang Kong

Automatic Music Transcription (AMT), aiming to get musical notes from raw
audio, typically uses frame-level systems with piano-roll outputs or language
model (LM)-based systems with note-level predictions. However, frame-level
systems require manual thresholding, while the LM-based systems struggle with
long sequences. In this paper, we propose a hybrid method combining pre-trained
roll-based encoders with an LM decoder to leverage the strengths of both
methods. Besides, our approach employs a hierarchical prediction strategy,
first predicting onset and pitch, then velocity, and finally offset. The
hierarchical prediction strategy reduces computational costs by breaking down
long sequences into different hierarchies. Evaluated on two benchmark
roll-based encoders, our method outperforms traditional piano-roll outputs 0.01
and 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a
performance-enhancing plug-in for arbitrary roll-based music transcription
encoder. We release the code of this work at
https://github.com/yongyizang/AMT_train.

摘要：自動音樂轉錄 (AMT) 旨在從原始音訊中取得音符，通常使用帶有鋼琴卷軸輸出的幀級系統或帶有音符級預測的基於語言模型 (LM) 的系統。然而，幀級系統需要手動設定閾值，而基於 LM 的系統則難以處理長序列。在本文中，我們提出了一種混合方法，將預訓練的基於卷軸的編碼器與 LM 解碼器結合起來，以利用兩種方法的優勢。此外，我們的做法採用了分層預測策略，首先預測起始和音高，然後預測速度，最後預測偏移。分層預測策略通過將長序列分解為不同的層次結構來降低運算成本。在兩個基準卷軸編碼器上進行評估後，我們的模型在起始-偏移-速度 F1 分數上優於傳統的鋼琴卷軸輸出 0.01 和 0.022，證明了其作為任意基於卷軸的音樂轉錄編碼器的效能提升外掛程式之潛力。我們在 https://github.com/yongyizang/AMT_train 上發布了這項工作的程式碼。

##### **Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning**
2501.03035v1 by Zhen Li, Yupeng Su, Runming Yang, Zhongwei Xie, Ngai Wong, Hongxia Yang

Large language models have achieved significant advancements in complex
mathematical reasoning benchmarks, such as MATH. However, their substantial
computational requirements present challenges for practical deployment. Model
quantization has emerged as an effective strategy to reduce memory usage and
computational costs by employing lower precision and bit-width representations.
In this study, we systematically evaluate the impact of quantization on
mathematical reasoning tasks. We introduce a multidimensional evaluation
framework that qualitatively assesses specific capability dimensions and
conduct quantitative analyses on the step-by-step outputs of various
quantization methods. Our results demonstrate that quantization differentially
affects numerical computation and reasoning planning abilities, identifying key
areas where quantized models experience performance degradation.

摘要：大型語言模型在複雜數學推理基準中取得顯著進展，例如 MATH。然而，它們大量的運算需求對實際部署提出了挑戰。模型量化已成為一種有效的策略，透過採用較低精度和位寬表示來減少記憶體用量和運算成本。在本研究中，我們系統性地評估量化對數學推理任務的影響。我們引入了一個多維評估框架，定性評估特定的能力維度，並對各種量化方法的逐步輸出進行定量分析。我們的結果表明，量化對數值計算和推理規劃能力有不同的影響，找出量化模型效能下降的主要領域。

##### **Putnam's Critical and Explanatory Tendencies Interpreted from a Machine Learning Perspective**
2501.03026v1 by Sheldon Z. Soudin

Making sense of theory choice in normal and across extraordinary science is
central to philosophy of science. The emergence of machine learning models has
the potential to act as a wrench in the gears of current debates. In this
paper, I will attempt to reconstruct the main movements that lead to and came
out of Putnam's critical and explanatory tendency distinction, argue for the
biconditional necessity of the tendencies, and conceptualize that wrench
through a machine learning interpretation of my claim.

摘要：在正常和非凡科學中，理論選擇的意義是科學哲學的核心。機器學習模型的出現有可能成為當前辯論中的關鍵因素。在本文中，我將嘗試重建導致普特南的批判性和解釋性趨勢區分的關鍵運動，並論證這些趨勢的雙重必要性，並通過機器學習對我的主張的解釋來概念化該關鍵因素。

##### **Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment**
2501.03012v1 by Pegah Khayatan, Mustafa Shukor, Jayneel Parekh, Matthieu Cord

Multimodal LLMs have reached remarkable levels of proficiency in
understanding multimodal inputs, driving extensive research to develop
increasingly powerful models. However, much less attention has been paid to
understanding and explaining the underlying mechanisms of these models. Most
existing explainability research examines these models only in their final
states, overlooking the dynamic representational shifts that occur during
training. In this work, we systematically analyze the evolution of hidden state
representations to reveal how fine-tuning alters the internal structure of a
model to specialize in new multimodal tasks. Using a concept-based approach, we
map hidden states to interpretable visual and textual concepts, enabling us to
trace changes in encoded concepts across modalities as training progresses. We
also demonstrate the use of shift vectors to capture these concepts changes.
These shift vectors allow us to recover fine-tuned concepts by shifting those
in the original model. Finally, we explore the practical impact of our findings
on model steering, showing that we can adjust multimodal LLMs behaviors without
any training, such as modifying answer types, captions style, or biasing the
model toward specific responses. Our work sheds light on how multimodal
representations evolve through fine-tuning and offers a new perspective for
interpreting model adaptation in multimodal tasks. The code for this project is
publicly available at https://github.com/mshukor/xl-vlms.

摘要：多模态 LLM 已在理解多模态输入方面达到了显着的熟练程度，推动了广泛的研究，以开发功能越来越强大的模型。然而，对理解和解释这些模型的底层机制的关注却少得多。大多数现有的可解释性研究仅在最终状态下检查这些模型，而忽略了在训练期间发生的动态表征转换。在这项工作中，我们系统地分析了隐藏状态表示的演变，以揭示微调如何改变模型的内部结构，以便专门用于新的多模态任务。使用基于概念的方法，我们将隐藏状态映射到可解释的视觉和文本概念，使我们能够追踪训练过程中跨模态编码概念的变化。我们还演示了使用转换向量来捕获这些概念变化。这些转换向量使我们能够通过转换原始模型中的那些概念来恢复微调的概念。最后，我们探讨了我们的发现对模型引导的实际影响，表明我们可以调整多模态 LLM 行为，而无需任何训练，例如修改答案类型、标题样式或使模型偏向特定响应。我们的工作阐明了多模态表示如何通过微调演变，并为解释多模态任务中的模型适应提供了一个新的视角。该项目的代码可在 https://github.com/mshukor/xl-vlms 公开获取。

##### **Quality Estimation based Feedback Training for Improving Pronoun Translation**
2501.03008v1 by Harshit Dhankhar, Baban Gain, Asif Ekbal, Yogesh Mani Tripathi

Pronoun translation is a longstanding challenge in neural machine translation
(NMT), often requiring inter-sentential context to ensure linguistic accuracy.
To address this, we introduce ProNMT, a novel framework designed to enhance
pronoun and overall translation quality in context-aware machine translation
systems. ProNMT leverages Quality Estimation (QE) models and a unique Pronoun
Generation Likelihood-Based Feedback mechanism to iteratively fine-tune
pre-trained NMT models without relying on extensive human annotations. The
framework combines QE scores with pronoun-specific rewards to guide training,
ensuring improved handling of linguistic nuances. Extensive experiments
demonstrate significant gains in pronoun translation accuracy and general
translation quality across multiple metrics. ProNMT offers an efficient,
scalable, and context-aware approach to improving NMT systems, particularly in
translating context-dependent elements like pronouns.

摘要：代名詞翻譯是神經機器翻譯 (NMT) 中長期存在的挑戰，通常需要跨句子上下文以確保語言的準確性。
為了解決這個問題，我們引入了 ProNMT，一個新穎的架構，旨在增強上下文感知機器翻譯系統中的代名詞和整體翻譯品質。ProNMT 採用品質評估 (QE) 模型和一個獨特的基於代名詞生成可能性的回饋機制，以反覆微調預先訓練的 NMT 模型，而無需依賴大量的人工註解。
該架構將 QE 分數與特定代名詞的獎勵相結合，以指導訓練，確保對語言細微差別的改進處理。廣泛的實驗證明了在多項指標中，代名詞翻譯準確性和一般翻譯品質都有顯著的提升。ProNMT 提供了一種有效、可擴充且上下文感知的方法來改進 NMT 系統，特別是在翻譯像代名詞這樣的上下文相關元素時。

##### **CALM: Curiosity-Driven Auditing for Large Language Models**
2501.02997v1 by Xiang Zheng, Longxiang Wang, Yi Liu, Xingjun Ma, Chao Shen, Cong Wang

Auditing Large Language Models (LLMs) is a crucial and challenging task. In
this study, we focus on auditing black-box LLMs without access to their
parameters, only to the provided service. We treat this type of auditing as a
black-box optimization problem where the goal is to automatically uncover
input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe
behaviors. For instance, we may seek a non-toxic input that the target LLM
responds to with a toxic output or an input that induces the hallucinative
response from the target LLM containing politically sensitive individuals. This
black-box optimization is challenging due to the scarcity of feasible points,
the discrete nature of the prompt space, and the large search space. To address
these challenges, we propose Curiosity-Driven Auditing for Large Language
Models (CALM), which uses intrinsically motivated reinforcement learning to
finetune an LLM as the auditor agent to uncover potential harmful and biased
input-output pairs of the target LLM. CALM successfully identifies derogatory
completions involving celebrities and uncovers inputs that elicit specific
names under the black-box setting. This work offers a promising direction for
auditing black-box LLMs. Our code is available at
https://github.com/x-zheng16/CALM.git.

摘要：審核大型語言模型 (LLM) 是一項至關重要且具有挑戰性的任務。在此研究中，我們專注於審核沒有存取其參數的黑箱 LLM，僅存取提供的服務。我們將此類型的審核視為一個黑箱最佳化問題，目標是自動找出目標 LLM 的輸入輸出對，這些輸入輸出對表現出非法、不道德或不安全的行為。例如，我們可能會尋找一個非有害輸入，目標 LLM 對其做出有害輸出，或者一個輸入，它會引發目標 LLM 產生包含政治敏感個體的幻覺回應。此黑箱最佳化具有挑戰性，原因在於可行點的稀少性、提示空間的離散性質以及龐大的搜尋空間。為了應對這些挑戰，我們提出了大型語言模型的好奇心驅動審核 (CALM)，它使用內在動機的強化學習來微調 LLM，作為審核代理，以找出目標 LLM 潛在的有害和有偏見的輸入輸出對。CALM 成功找出涉及名人的貶義完成，並找出在黑箱設定下引發特定名稱的輸入。這項工作為審核黑箱 LLM 提供了一個有希望的方向。我們的程式碼可在 https://github.com/x-zheng16/CALM.git 取得。

##### **A Bio-Inspired Research Paradigm of Collision Perception Neurons Enabling Neuro-Robotic Integration: The LGMD Case**
2501.02982v1 by Ziyan Qin, Jigen Peng, Shigang Yue, Qinbing Fu

Compared to human vision, insect visual systems excel at rapid and precise
collision detection, despite relying on only tens of thousands of neurons
organized through a few neuropils. This efficiency makes them an attractive
model system for developing artificial collision-detecting systems.
Specifically, researchers have identified collision-selective neurons in the
locust's optic lobe, called lobula giant movement detectors (LGMDs), which
respond specifically to approaching objects. Research upon LGMD neurons began
in the early 1970s. Initially, due to their large size, these neurons were
identified as motion detectors, but their role as looming detectors was
recognized over time. Since then, progress in neuroscience, computational
modeling of LGMD's visual neural circuits, and LGMD-based robotics has advanced
in tandem, each field supporting and driving the others. Today, with a deeper
understanding of LGMD neurons, LGMD-based models have significantly improved
collision-free navigation in mobile robots including ground and aerial robots.
This review highlights recent developments in LGMD research from the
perspectives of neuroscience, computational modeling, and robotics. It
emphasizes a biologically plausible research paradigm, where insights from
neuroscience inform real-world applications, which would in turn validate and
advance neuroscience. With strong support from extensive research and growing
application demand, this paradigm has reached a mature stage and demonstrates
versatility across different areas of neuroscience research, thereby enhancing
our understanding of the interconnections between neuroscience, computational
modeling, and robotics. Furthermore, other motion-sensitive neurons have also
shown promising potential for adopting this research paradigm.

摘要：與人類視覺相比，昆蟲視覺系統擅長快速且精確地偵測碰撞，儘管僅依賴於數萬個神經元，並透過少數神經節組織而成。這種效率使它們成為開發人工防撞偵測系統的誘人模型系統。具體來說，研究人員已在蝗蟲的視葉中發現了選擇性碰撞神經元，稱為小葉巨型運動偵測器 (LGMD)，它會對接近的物體產生特定的反應。對 LGMD 神經元的的研究始於 1970 年代初期。最初，由於它們的體積龐大，這些神經元被認定為運動偵測器，但隨著時間推移，它們作為逼近偵測器的角色逐漸受到認可。從那時起，神經科學、LGMD 視覺神經迴路的計算建模和基於 LGMD 的機器人技術的進展齊頭並進，每個領域都相互支援和推動。如今，隨著對 LGMD 神經元有了更深入的了解，基於 LGMD 的模型已經顯著改善了行動機器人的無碰撞導航，包括地面和空中機器人。本綜述從神經科學、計算建模和機器人的角度重點介紹了 LGMD 研究的最新進展。它強調了一個生物學上合理的研究所示範，其中神經科學的見解為現實世界的應用提供資訊，而這反過來又會驗證和推進神經科學。在廣泛研究和不斷增長的應用需求的大力支持下，這種範例已達到成熟階段，並展示了跨不同神經科學研究領域的多功能性，從而增強了我們對神經科學、計算建模和機器人技術之間相互關聯的理解。此外，其他對運動敏感的神經元也顯示出採用這種研究範例的潛力。

##### **CONTINUUM: Detecting APT Attacks through Spatial-Temporal Graph Neural Networks**
2501.02981v1 by Atmane Ayoub Mansour Bahara, Kamel Soaïd Ferrahia, Mohamed-Lamine Messai, Hamida Seba, Karima Amrouche

Advanced Persistent Threats (APTs) represent a significant challenge in
cybersecurity due to their sophisticated and stealthy nature. Traditional
Intrusion Detection Systems (IDS) often fall short in detecting these
multi-stage attacks. Recently, Graph Neural Networks (GNNs) have been employed
to enhance IDS capabilities by analyzing the complex relationships within
networked data. However, existing GNN-based solutions are hampered by high
false positive rates and substantial resource consumption. In this paper, we
present a novel IDS designed to detect APTs using a Spatio-Temporal Graph
Neural Network Autoencoder. Our approach leverages spatial information to
understand the interactions between entities within a graph and temporal
information to capture the evolution of the graph over time. This dual
perspective is crucial for identifying the sequential stages of APTs.
Furthermore, to address privacy and scalability concerns, we deploy our
architecture in a federated learning environment. This setup ensures that local
data remains on-premise while encrypted model-weights are shared and aggregated
using homomorphic encryption, maintaining data privacy and security. Our
evaluation shows that this system effectively detects APTs with lower false
positive rates and optimized resource usage compared to existing methods,
highlighting the potential of spatio-temporal analysis and federated learning
in enhancing cybersecurity defenses.

摘要：進階持續性威脅 (APT) 因為其複雜且隱蔽的特性，在網路安全領域中代表著重大的挑戰。傳統的入侵偵測系統 (IDS) 在偵測這些多階段攻擊時常常力不從心。最近，圖形神經網路 (GNN) 已被用來加強 IDS 的能力，方法是分析網路化資料中複雜的關係。然而，現有的基於 GNN 的解決方案受到高誤報率和大量資源消耗的阻礙。在本文中，我們提出一個新穎的 IDS，旨在使用時空圖形神經網路自動編碼器來偵測 APT。我們的做法利用空間資訊來了解圖形中實體之間的互動，以及時間資訊來擷取圖形隨著時間演變的過程。這種雙重觀點對於識別 APT 的循序漸進階段至關重要。此外，為了解決隱私和可擴充性的問題，我們在聯邦學習環境中部署我們的架構。此設定可確保本地資料保留在公司內部，同時使用同態加密來共用和彙總加密的模型權重，維護資料隱私和安全性。我們的評估顯示，與現有方法相比，此系統可以有效地偵測 APT，且誤報率較低，資源使用也經過最佳化，突顯了時空分析和聯邦學習在加強網路安全防禦方面的潛力。

##### **Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation**
2501.02979v1 by Zhi Qu, Yiran Wang, Jiannan Mao, Chenchen Ding, Hideki Tanaka, Masao Utiyama, Taro Watanabe

The multilingual neural machine translation (MNMT) enables arbitrary
translations across multiple languages by training a model with limited
parameters using parallel data only. However, the performance of such MNMT
models still lags behind that of large language models (LLMs), limiting their
practicality. In this work, we address this limitation by introducing
registering to achieve the new state-of-the-art of decoder-only MNMT models.
Specifically, we insert a set of artificial tokens specifying the target
language, called registers, into the input sequence between the source and
target tokens. By modifying the attention mask, the target token generation
only pays attention to the activation of registers, representing the source
tokens in the target language space. Experiments on EC-40, a large-scale
benchmark, show that our method outperforms related methods driven by
optimizing multilingual representations. We further scale up and collect 9.3
billion sentence pairs across 24 languages from public datasets to pre-train
two models, namely MITRE (multilingual translation with registers). One of
them, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with
commercial LLMs, and shows strong adaptability in fine-tuning. Finally, we
open-source our models to facilitate further research and development in MNMT:
https://github.com/zhiqu22/mitre.

摘要：多語言神經機器翻譯 (MNMT) 僅使用平行數據訓練具有有限參數的模型，即可在多種語言之間進行任意翻譯。然而，此類 MNMT 模型的效能仍落後於大型語言模型 (LLM)，限制了其實用性。在這項工作中，我們透過引進註冊來解決此限制，以達成僅解碼器 MNMT 模型的最新技術水準。具體來說，我們在來源和目標標記之間的輸入序列中插入一組指定目標語言的人工標記，稱為註冊。透過修改注意力遮罩，目標標記產生只會注意註冊的啟動，表示目標語言空間中的來源標記。在大型基準 EC-40 上進行的實驗顯示，我們的方法優於由最佳化多語言表示驅動的相關方法。我們進一步擴展規模，並從公共資料集中收集了 24 種語言的 93 億個句子對，以預先訓練兩個模型，即 MITRE（具註冊的多語言翻譯）。其中之一，MITRE-913M，優於 NLLB-3.3B，達到與商用 LLM 相當的效能，並在微調中展現強大的適應性。最後，我們開放原始碼模型，以利於進一步研究和開發 MNMT：
https://github.com/zhiqu22/mitre。

##### **CAMP: Collaborative Attention Model with Profiles for Vehicle Routing Problems**
2501.02977v1 by Chuanbo Hua, Federico Berto, Jiwoo Son, Seunghyun Kang, Changhyun Kwon, Jinkyoo Park

The profiled vehicle routing problem (PVRP) is a generalization of the
heterogeneous capacitated vehicle routing problem (HCVRP) in which the
objective is to optimize the routes of vehicles to serve client demands subject
to different vehicle profiles, with each having a preference or constraint on a
per-client basis. While existing learning methods have shown promise for
solving the HCVRP in real-time, no learning method exists to solve the more
practical and challenging PVRP. In this paper, we propose a Collaborative
Attention Model with Profiles (CAMP), a novel approach that learns efficient
solvers for PVRP using multi-agent reinforcement learning. CAMP employs a
specialized attention-based encoder architecture to embed profiled client
embeddings in parallel for each vehicle profile. We design a communication
layer between agents for collaborative decision-making across profiled
embeddings at each decoding step and a batched pointer mechanism to attend to
the profiled embeddings to evaluate the likelihood of the next actions. We
evaluate CAMP on two variants of PVRPs: PVRP with preferences, which explicitly
influence the reward function, and PVRP with zone constraints with different
numbers of agents and clients, demonstrating that our learned solvers achieve
competitive results compared to both classical state-of-the-art neural
multi-agent models in terms of solution quality and computational efficiency.
We make our code openly available at https://github.com/ai4co/camp.

摘要：剖面車輛路徑問題 (PVRP) 是異質容量車輛路徑問題 (HCVRP) 的概括，其目標是最佳化車輛路徑以服務客戶需求，但受不同車輛剖面的約束，每個剖面都有對每個客戶的偏好或約束。雖然現有的學習方法已展現出即時解決 HCVRP 的希望，但沒有學習方法可以解決更實際且更具挑戰性的 PVRP。在本文中，我們提出一個帶剖面的協作注意力模型 (CAMP)，這是一種新穎的方法，它使用多智能體強化學習來學習 PVRP 的有效求解器。CAMP 使用一個基於注意力的編碼器架構，為每個車輛剖面並行嵌入剖面客戶嵌入。我們設計了一個通訊層，讓智能體在每個解碼步驟中跨剖面嵌入進行協作決策，並使用一個批次指標機制來關注剖面嵌入，以評估下一個動作的可能性。我們在 PVRP 的兩個變體上評估 CAMP：帶偏好的 PVRP，它明確影響獎勵函數，以及帶區域約束的 PVRP，其中有不同數量的智能體和客戶，證明我們學習的求解器在解決品質和計算效率方面，與傳統最先進的神經多智能體模型相比，取得了具有競爭力的結果。我們在 https://github.com/ai4co/camp 上公開我們的程式碼。

##### **Proof-of-Data: A Consensus Protocol for Collaborative Intelligence**
2501.02971v1 by Huiwen Liu, Feida Zhu, Ling Cheng

Existing research on federated learning has been focused on the setting where
learning is coordinated by a centralized entity. Yet the greatest potential of
future collaborative intelligence would be unleashed in a more open and
democratized setting with no central entity in a dominant role, referred to as
"decentralized federated learning". New challenges arise accordingly in
achieving both correct model training and fair reward allocation with
collective effort among all participating nodes, especially with the threat of
the Byzantine node jeopardising both tasks.
  In this paper, we propose a blockchain-based decentralized Byzantine
fault-tolerant federated learning framework based on a novel Proof-of-Data
(PoD) consensus protocol to resolve both the "trust" and "incentive"
components. By decoupling model training and contribution accounting, PoD is
able to enjoy not only the benefit of learning efficiency and system liveliness
from asynchronous societal-scale PoW-style learning but also the finality of
consensus and reward allocation from epoch-based BFT-style voting. To mitigate
false reward claims by data forgery from Byzantine attacks, a privacy-aware
data verification and contribution-based reward allocation mechanism is
designed to complete the framework. Our evaluation results show that PoD
demonstrates performance in model training close to that of the centralized
counterpart while achieving trust in consensus and fairness for reward
allocation with a fault tolerance ratio of 1/3.

摘要：現有關於聯合學習的研究一直集中於由集中式實體協調學習的設定。然而，未來的協作智慧的最大潛力將在沒有中心實體主導的更開放和民主化的環境中釋放，這稱為「分散聯合學習」。因此，在所有參與節點的共同努力下，在實現正確的模型訓練和公平的獎勵分配時會出現新的挑戰，特別是在拜占庭節點威脅到這兩個任務的情況下。
在本文中，我們提出一個基於區塊鏈的分散式拜占庭容錯聯合學習框架，該框架基於一個新穎的數據證明 (PoD) 共識協定來解決「信任」和「激勵」組成部分。透過解耦模型訓練和貢獻計算，PoD 不僅能夠享受來自異步社會規模 PoW 風格學習的學習效率和系統活躍性的好處，還能享受來自基於時期的 BFT 風格投票的共識和獎勵分配的最終性。為了減輕拜占庭攻擊中因數據偽造而導致的虛假獎勵索賠，設計了一個注重隱私的數據驗證和基於貢獻的獎勵分配機制來完成該框架。我們的評估結果表明，PoD 在模型訓練中的表現接近於集中式對應項，同時在共識和公平方面實現了對獎勵分配的信任，容錯率為 1/3。

##### **Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild**
2501.02964v1 by Wanpeng Hu, Haodi Liu, Lin Chen, Feng Zhou, Changming Xiao, Qi Yang, Changshui Zhang

Complex visual reasoning remains a key challenge today. Typically, the
challenge is tackled using methodologies such as Chain of Thought (COT) and
visual instruction tuning. However, how to organically combine these two
methodologies for greater success remains unexplored. Also, issues like
hallucinations and high training cost still need to be addressed. In this work,
we devise an innovative multi-round training and reasoning framework suitable
for lightweight Multimodal Large Language Models (MLLMs). Our self-questioning
approach heuristically guides MLLMs to focus on visual clues relevant to the
target problem, reducing hallucinations and enhancing the model's ability to
describe fine-grained image details. This ultimately enables the model to
perform well in complex visual reasoning and question-answering tasks. We have
named this framework Socratic Questioning(SQ). To facilitate future research,
we create a multimodal mini-dataset named CapQA, which includes 1k images of
fine-grained activities, for visual instruction tuning and evaluation, our
proposed SQ method leads to a 31.2% improvement in the hallucination score. Our
extensive experiments on various benchmarks demonstrate SQ's remarkable
capabilities in heuristic self-questioning, zero-shot visual reasoning and
hallucination mitigation. Our model and code will be publicly available.

摘要：複雜的視覺推理至今仍是一項關鍵挑戰。通常，此挑戰是透過思想鏈 (COT) 和視覺指令調整等方法來解決。然而，如何將這兩種方法有機結合以獲得更大的成功仍未被探討。此外，幻覺和高訓練成本等問題仍需要解決。在這項工作中，我們設計了一個創新的多輪訓練和推理框架，適用於輕量級多模態大型語言模型 (MLLM)。我們的自問自答方法啟發式地引導 MLLM 專注於與目標問題相關的視覺線索，減少幻覺並增強模型描述精細影像細節的能力。這最終使模型能夠在複雜的視覺推理和問答任務中表現良好。我們將此框架命名為蘇格拉底式提問 (SQ)。為了促進未來的研究，我們創建了一個名為 CapQA 的多模態迷你資料集，其中包含 1k 張精細活動的影像，用於視覺指令調整和評估，我們提出的 SQ 方法使幻覺評分提高了 31.2%。我們在各種基準上進行的廣泛實驗證明了 SQ 在啟發式自問自答、零次視覺推理和幻覺緩解方面的卓越能力。我們的模型和程式碼將公開提供。

##### **Key-value memory in the brain**
2501.02950v1 by Samuel J. Gershman, Ila Fiete, Kazuki Irie

Classical models of memory in psychology and neuroscience rely on
similarity-based retrieval of stored patterns, where similarity is a function
of retrieval cues and the stored patterns. While parsimonious, these models do
not allow distinct representations for storage and retrieval, despite their
distinct computational demands. Key-value memory systems, in contrast,
distinguish representations used for storage (values) and those used for
retrieval (keys). This allows key-value memory systems to optimize
simultaneously for fidelity in storage and discriminability in retrieval. We
review the computational foundations of key-value memory, its role in modern
machine learning systems, related ideas from psychology and neuroscience,
applications to a number of empirical puzzles, and possible biological
implementations.

摘要：古典心理學和神經科學的記憶模型依賴於儲存模式的相似性檢索，其中相似性是檢索線索和儲存模式的函數。儘管節儉，但這些模型不允許儲存和檢索有不同的表示，儘管它們有不同的計算需求。相反，鍵值記憶系統區分用於儲存（值）的表示和用於檢索（鍵）的表示。這允許鍵值記憶系統同時優化儲存中的保真度和檢索中的可辨別性。我們回顧了鍵值記憶的計算基礎、它在現代機器學習系統中的作用、心理學和神經科學的相關思想、對許多經驗謎題的應用，以及可能的生物學實現。

##### **Label-free Concept Based Multiple Instance Learning for Gigapixel Histopathology**
2501.02922v1 by Susu Sun, Leslie Tessier, Frédérique Meeuwsen, Clément Grisi, Dominique van Midden, Geert Litjens, Christian F. Baumgartner

Multiple Instance Learning (MIL) methods allow for gigapixel Whole-Slide
Image (WSI) analysis with only slide-level annotations. Interpretability is
crucial for safely deploying such algorithms in high-stakes medical domains.
Traditional MIL methods offer explanations by highlighting salient regions.
However, such spatial heatmaps provide limited insights for end users. To
address this, we propose a novel inherently interpretable WSI-classification
approach that uses human-understandable pathology concepts to generate
explanations. Our proposed Concept MIL model leverages recent advances in
vision-language models to directly predict pathology concepts based on image
features. The model's predictions are obtained through a linear combination of
the concepts identified on the top-K patches of a WSI, enabling inherent
explanations by tracing each concept's influence on the prediction. In contrast
to traditional concept-based interpretable models, our approach eliminates the
need for costly human annotations by leveraging the vision-language model. We
validate our method on two widely used pathology datasets: Camelyon16 and
PANDA. On both datasets, Concept MIL achieves AUC and accuracy scores over 0.9,
putting it on par with state-of-the-art models. We further find that 87.1\%
(Camelyon16) and 85.3\% (PANDA) of the top 20 patches fall within the tumor
region. A user study shows that the concepts identified by our model align with
the concepts used by pathologists, making it a promising strategy for
human-interpretable WSI classification.

摘要：多實例學習 (MIL) 方法僅使用玻片層級註解，即可進行吉像素全玻片影像 (WSI) 分析。可解釋性對於在高風險醫療領域安全部署此類演算法至關重要。傳統的 MIL 方法透過強調顯著區域來提供說明。然而，此類空間熱圖為最終使用者提供的見解有限。為了解決此問題，我們提出了一種新穎且本質上可解釋的 WSI 分類方法，該方法使用人類可理解的病理概念來產生說明。我們提出的概念 MIL 模型利用視覺語言模型的最新進展，根據影像特徵直接預測病理概念。該模型的預測是透過線性組合 WSI 頂部 K 個區塊上識別的概念而獲得的，透過追蹤每個概念對預測的影響，可以提供內在說明。與傳統基於概念的可解釋模型相比，我們的做法透過利用視覺語言模型，消除了對昂貴的人工註解的需求。我們在兩個廣泛使用的病理資料集：Camelyon16 和 PANDA 上驗證了我們的模型。在兩個資料集上，概念 MIL 的 AUC 和準確率都超過 0.9，與最先進的模型不相上下。我們進一步發現，前 20 個區塊中有 87.1%（Camelyon16）和 85.3%（PANDA）落在腫瘤區域內。一項使用者研究表明，我們的模型識別的概念與病理學家使用的概念一致，使其成為人類可解釋 WSI 分類的一種有前途的策略。

##### **Skillful High-Resolution Ensemble Precipitation Forecasting with an Integrated Deep Learning Framework**
2501.02905v1 by Shuangshuang He, Hongli Liang, Yuanting Zhang, Xingyuan Yuan

High-resolution precipitation forecasts are crucial for providing accurate
weather prediction and supporting effective responses to extreme weather
events. Traditional numerical models struggle with stochastic subgrid-scale
processes, while recent deep learning models often produce blurry results. To
address these challenges, we propose a physics-inspired deep learning framework
for high-resolution (0.05\textdegree{} $\times$ 0.05\textdegree{}) ensemble
precipitation forecasting. Trained on ERA5 and CMPA high-resolution
precipitation datasets, the framework integrates deterministic and
probabilistic components. The deterministic model, based on a 3D
SwinTransformer, captures average precipitation at mesoscale resolution and
incorporates strategies to enhance performance, particularly for moderate to
heavy rainfall. The probabilistic model employs conditional diffusion in latent
space to account for uncertainties in residual precipitation at convective
scales. During inference, ensemble members are generated by repeatedly sampling
latent variables, enabling the model to represent precipitation uncertainty.
Our model significantly enhances spatial resolution and forecast accuracy. Rank
histogram shows that the ensemble system is reliable and unbiased. In a case
study of heavy precipitation in southern China, the model outputs align more
closely with observed precipitation distributions than ERA5, demonstrating
superior capability in capturing extreme precipitation events. Additionally,
5-day real-time forecasts show good performance in terms of CSI scores.

摘要：高解析度降水預測對於提供準確的天氣預測和支持對極端天氣事件的有效應對至關重要。傳統的數值模型難以處理隨機子網格尺度過程，而最近的深度學習模型通常會產生模糊的結果。為了應對這些挑戰，我們提出了一個受物理啟發的深度學習框架，用於高解析度 (0.05\textdegree{} $\times$ 0.05\textdegree{}) 集合降水預測。該框架在 ERA5 和 CMPA 高解析度降水數據集上進行訓練，集成了確定性和概率性組成部分。確定性模型基於 3D SwinTransformer，捕捉中尺度解析度的平均降水量，並結合策略來增強性能，特別是對於中到大雨。概率性模型採用潛在空間中的條件擴散來考慮對流尺度上殘餘降水的影響。在推理過程中，通過重複採樣潛在變量來生成集合成員，使模型能夠表示降水不確定性。我們的模型顯著提高了空間解析度和預測精度。秩直方圖顯示集合系統是可靠且無偏差的。在中國南部大暴雨的案例研究中，模型輸出與觀測到的降水分佈相比，與 ERA5 更為接近，展示了捕捉極端降水事件的卓越能力。此外，5 天的實時預測在 CSI 分數方面表現良好。

##### **Explaining Humour Style Classifications: An XAI Approach to Understanding Computational Humour Analysis**
2501.02891v1 by Mary Ogbuka Kenneth, Foaad Khosmood, Abbas Edalat

Humour styles can have either a negative or a positive impact on well-being.
Given the importance of these styles to mental health, significant research has
been conducted on their automatic identification. However, the automated
machine learning models used for this purpose are black boxes, making their
prediction decisions opaque. Clarity and transparency are vital in the field of
mental health. This paper presents an explainable AI (XAI) framework for
understanding humour style classification, building upon previous work in
computational humour analysis. Using the best-performing single model
(ALI+XGBoost) from prior research, we apply comprehensive XAI techniques to
analyse how linguistic, emotional, and semantic features contribute to humour
style classification decisions. Our analysis reveals distinct patterns in how
different humour styles are characterised and misclassified, with particular
emphasis on the challenges in distinguishing affiliative humour from other
styles. Through detailed examination of feature importance, error patterns, and
misclassification cases, we identify key factors influencing model decisions,
including emotional ambiguity, context misinterpretation, and target
identification. The framework demonstrates significant utility in understanding
model behaviour, achieving interpretable insights into the complex interplay of
features that define different humour styles. Our findings contribute to both
the theoretical understanding of computational humour analysis and practical
applications in mental health, content moderation, and digital humanities
research.

摘要：幽默風格對幸福感可能產生負面或正面的影響。
鑑於這些風格對心理健康的重要性，已經對其自動識別進行了大量研究。然而，用於此目的的自動機器學習模型是黑盒子，使得其預測決策不透明。清晰度和透明度在心理健康領域至關重要。本文提出了一個可解釋的 AI (XAI) 框架，用於理解幽默風格分類，建立在計算幽默分析的先前工作之上。使用先前研究中表現最好的單一模型 (ALI+XGBoost)，我們應用全面的 XAI 技術來分析語言、情緒和語義特徵如何影響幽默風格分類決策。我們的分析揭示了不同幽默風格如何被表徵和錯誤分類的不同模式，特別強調了區分聯屬幽默與其他風格的挑戰。通過仔細檢查特徵重要性、錯誤模式和錯誤分類案例，我們確定了影響模型決策的關鍵因素，包括情緒模糊、情境誤解和目標識別。該框架展示了在理解模型行為方面的顯著效用，實現了對定義不同幽默風格的特徵之間複雜相互作用的可解釋見解。我們的發現有助於計算幽默分析的理論理解和心理健康、內容審核和數字人文研究中的實際應用。

##### **IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment**
2501.02869v1 by Yiming Zhang, Zheng Chang, Wentao Cai, MengXing Ren, Kang Yuan, Yining Sun, Zenghui Ding

Recent researches of large language models(LLM), which is pre-trained on
massive general-purpose corpora, have achieved breakthroughs in responding
human queries. However, these methods face challenges including limited data
insufficiency to support extensive pre-training and can not align responses
with users' instructions. To address these issues, we introduce a medical
instruction dataset, CMedINS, containing six medical instructions derived from
actual medical tasks, which effectively fine-tunes LLM in conjunction with
other data. Subsequently, We launch our medical model, IIMedGPT, employing an
efficient preference alignment method, Direct preference Optimization(DPO). The
results show that our final model outperforms existing medical models in
medical dialogue.Datsets, Code and model checkpoints will be released upon
acceptance.

摘要：最近針對大型語言模型 (LLM) 的研究，該模型預先訓練於龐大的通用語料庫中，已在回應人類查詢方面取得突破。然而，這些方法面臨的挑戰包括資料不足以支援廣泛的預訓練，且無法將回應與使用者的指示保持一致。為了解決這些問題，我們引進一個醫療指示資料集 CMedINS，其中包含六項從實際醫療任務中衍生的醫療指示，與其他資料結合後能有效微調 LLM。隨後，我們推出我們的醫療模型 IIMedGPT，採用一種有效率的偏好對齊方法，直接偏好最佳化 (DPO)。結果顯示，我們的最終模型在醫療對話中優於現有的醫療模型。資料集、程式碼和模型檢查點將在通過驗證後釋出。

##### **Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification**
2501.02844v1 by Yubo Wang, Haoyang Li, Fei Teng, Lei Chen

Text classification is a fundamental task in natural language processing,
pivotal to various applications such as query optimization, data integration,
and schema matching. While neural network-based models, such as CNN and BERT,
have demonstrated remarkable performance in text classification, their
effectiveness heavily relies on abundant labeled training data. This dependency
makes these models less effective in dynamic few-shot text classification,
where labeled data is scarce, and target labels frequently evolve based on
application needs. Recently, large language models (LLMs) have shown promise
due to their extensive pretraining and contextual understanding. Current
approaches provide LLMs with text inputs, candidate labels, and additional side
information (e.g., descriptions) to predict text labels. However, their
effectiveness is hindered by the increased input size and the noise introduced
through side information processing. To address these limitations, we propose a
graph-based online retrieval-augmented generation framework, namely GORAG, for
dynamic few-shot text classification. GORAG constructs and maintains an
adaptive information graph by extracting side information across all target
texts, rather than treating each input independently. It employs a weighted
edge mechanism to prioritize the importance and reliability of extracted
information and dynamically retrieves relevant context using a minimum-cost
spanning tree tailored for each text input. Empirical evaluations demonstrate
that GORAG outperforms existing approaches by providing more comprehensive and
accurate contextual information.

摘要：文本分類是自然語言處理中的基本任務，
對於各種應用至關重要，例如查詢優化、資料整合，
和模式匹配。雖然基於神經網路的模型，例如 CNN 和 BERT，
在文本分類中表現出色，但其
有效性在很大程度上依賴於大量的標籤訓練資料。這個依賴性
使得這些模型在動態少樣本文本分類中效果較差，
其中標籤資料稀缺，並且目標標籤會根據
應用需求頻繁演變。最近，大型語言模型 (LLM) 由於其廣泛的預訓練和上下文理解而顯示出前景。目前
方法為 LLM 提供文本輸入、候選標籤和附加側邊
資訊（例如，描述）以預測文本標籤。然而，其
有效性受到輸入大小增加和側邊資訊處理引入的雜訊的阻礙。為了解決這些限制，我們提出一個
基於圖表的線上檢索增強生成架構，即 GORAG，用於
動態少樣本文本分類。GORAG 通過提取所有目標的側邊資訊來建構並維護一個
自適應資訊圖表
文本，而不是獨立處理每個輸入。它採用加權
邊緣機制來優先考慮提取資訊的重要性及可靠性，並使用針對每個文本輸入量身打造的最小成本
生成樹動態檢索相關的上下文。實證評估表明
GORAG 通過提供更全面且準確的上下文資訊，優於現有方法。

##### **Enhanced Rooftop Solar Panel Detection by Efficiently Aggregating Local Features**
2501.02840v1 by Kuldeep Kurte, Kedar Kulkarni

In this paper, we present an enhanced Convolutional Neural Network
(CNN)-based rooftop solar photovoltaic (PV) panel detection approach using
satellite images. We propose to use pre-trained CNN-based model to extract the
local convolutional features of rooftops. These local features are then
combined using the Vectors of Locally Aggregated Descriptors (VLAD) technique
to obtain rooftop-level global features, which are then used to train
traditional Machine Learning (ML) models to identify rooftop images that do and
do not contain PV panels. On the dataset used in this study, the proposed
approach achieved rooftop-PV classification scores exceeding the predefined
threshold of 0.9 across all three cities for each of the feature extractor
networks evaluated. Moreover, we propose a 3-phase approach to enable efficient
utilization of the previously trained models on a new city or region with
limited labelled data. We illustrate the effectiveness of this 3-phase approach
for multi-city rooftop-PV detection task.

摘要：在本文中，我們提出一個增強的基於卷積神經網路 (CNN) 的屋頂太陽能光電 (PV) 面板檢測方法，使用衛星影像。我們建議使用預先訓練的基於 CNN 的模型來提取屋頂的局部卷積特徵。然後使用局部聚合描述符 (VLAD) 技術結合這些局部特徵，以獲得屋頂層級的全局特徵，然後用於訓練傳統機器學習 (ML) 模型，以識別包含或不包含 PV 面板的屋頂影像。在這個研究中使用的資料集上，所提出的方法達到了屋頂 PV 分類分數，超過了所有三個城市中每個特徵提取器網路評估的預定義 0.9 閾值。此外，我們提出了一個 3 階段的方法，以在標記資料有限的新城市或地區上有效利用先前訓練的模型。我們說明了此 3 階段方法對多城市屋頂 PV 檢測任務的有效性。

##### **Forward Once for All: Structural Parameterized Adaptation for Efficient Cloud-coordinated On-device Recommendation**
2501.02837v1 by Kairui Fu, Zheqi Lv, Shengyu Zhang, Fan Wu, Kun Kuang

In cloud-centric recommender system, regular data exchanges between user
devices and cloud could potentially elevate bandwidth demands and privacy
risks. On-device recommendation emerges as a viable solution by performing
reranking locally to alleviate these concerns. Existing methods primarily focus
on developing local adaptive parameters, while potentially neglecting the
critical role of tailor-made model architecture. Insights from broader research
domains suggest that varying data distributions might favor distinct
architectures for better fitting. In addition, imposing a uniform model
structure across heterogeneous devices may result in risking inefficacy on less
capable devices or sub-optimal performance on those with sufficient
capabilities. In response to these gaps, our paper introduces Forward-OFA, a
novel approach for the dynamic construction of device-specific networks (both
structure and parameters). Forward-OFA employs a structure controller to
selectively determine whether each block needs to be assembled for a given
device. However, during the training of the structure controller, these
assembled heterogeneous structures are jointly optimized, where the co-adaption
among blocks might encounter gradient conflicts. To mitigate this, Forward-OFA
is designed to establish a structure-guided mapping of real-time behaviors to
the parameters of assembled networks. Structure-related parameters and parallel
components within the mapper prevent each part from receiving heterogeneous
gradients from others, thus bypassing the gradient conflicts for coupled
optimization. Besides, direct mapping enables Forward-OFA to achieve adaptation
through only one forward pass, allowing for swift adaptation to changing
interests and eliminating the requirement for on-device backpropagation.
Experiments on real-world datasets demonstrate the effectiveness and efficiency
of Forward-OFA.

摘要：<paragraph>在以雲端為中心的推薦系統中，使用者裝置與雲端之間的定期資料交換可能會潛在提升頻寬需求和隱私風險。裝置上推薦作為一種可行的解決方案浮出檯面，透過在本地重新排序來減輕這些疑慮。現有方法主要專注於開發本地自適應參數，同時潛在忽略了客製化模型架構的重要角色。來自更廣泛研究領域的見解表明，不同的資料分佈可能有利於不同的架構以獲得更好的擬合。此外，在異質裝置上強加統一的模型結構可能會導致在功能較弱的裝置上造成效能不彰，或是在具有足夠功能的裝置上造成次佳效能。為了回應這些差距，我們的論文引入了 Forward-OFA，一種用於動態建構裝置特定網路（結構和參數）的新穎方法。Forward-OFA 使用結構控制器來選擇性地確定是否需要為特定裝置組裝每個區塊。然而，在結構控制器的訓練期間，這些組裝的異質結構會被聯合最佳化，其中區塊之間的共同適應可能會遭遇梯度衝突。為了減輕這一點，Forward-OFA 被設計為建立一個結構導引的即時行為對組裝網路參數的對應。結構相關參數和對應器內的平行元件可防止每個部分接收來自其他部分的異質梯度，從而繞過耦合最佳化的梯度衝突。此外，直接對應讓 Forward-OFA 能夠僅透過一次前向傳遞來達成適應，允許快速適應變化的興趣，並消除對裝置上反向傳播的要求。在真實世界資料集上的實驗證明了 Forward-OFA 的有效性和效率。</paragraph>

##### **Samba-asr state-of-the-art speech recognition leveraging structured state-space models**
2501.02832v1 by Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi

We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition
(ASR) model leveraging the novel Mamba architecture as both encoder and
decoder, built on the foundation of state-space models (SSMs). Unlike
transformer-based ASR models, which rely on self-attention mechanisms to
capture dependencies, Samba ASR effectively models both local and global
temporal dependencies using efficient state-space dynamics, achieving
remarkable performance gains. By addressing the limitations of transformers,
such as quadratic scaling with input length and difficulty in handling
long-range dependencies, Samba ASR achieves superior accuracy and efficiency.
  Experimental results demonstrate that Samba ASR surpasses existing
open-source transformer-based ASR models across various standard benchmarks,
establishing it as the new state of the art in ASR. Extensive evaluations on
benchmark datasets show significant improvements in Word Error Rate (WER), with
competitive performance even in low-resource scenarios. Furthermore, the
computational efficiency and parameter optimization of the Mamba architecture
make Samba ASR a scalable and robust solution for diverse ASR tasks.
  Our contributions include:
  A new Samba ASR architecture demonstrating the superiority of SSMs over
transformer-based models for speech sequence processing. A comprehensive
evaluation on public benchmarks showcasing state-of-the-art performance. An
analysis of computational efficiency, robustness to noise, and sequence
generalization. This work highlights the viability of Mamba SSMs as a
transformer-free alternative for efficient and accurate ASR. By leveraging
state-space modeling advancements, Samba ASR sets a new benchmark for ASR
performance and future research.

摘要：<paragraph>我們提出 Samba ASR，這是第一個利用新穎 Mamba 架構作為編碼器和解碼器的最先進自動語音辨識 (ASR) 模型，建立在狀態空間模型 (SSM) 的基礎上。與依賴自注意力機制來捕捉依賴關係的基於轉換器的 ASR 模型不同，Samba ASR 使用高效的狀態空間動態有效地對局部和全局時間依賴關係進行建模，從而實現顯著的性能提升。通過解決轉換器的限制，例如輸入長度二次縮放和處理長距離依賴關係的難度，Samba ASR 達到了更高的準確性和效率。實驗結果表明，Samba ASR 在各種標準基準測試中超越了現有的開源基於轉換器的 ASR 模型，使其成為 ASR 中的最新技術。對基準數據集的廣泛評估顯示，詞錯誤率 (WER) 有顯著改善，即使在低資源場景中也具有競爭力的性能。此外，Mamba 架構的計算效率和參數優化使 Samba ASR 成為各種 ASR 任務的可擴展且穩健的解決方案。我們的貢獻包括：一個新的 Samba ASR 架構，展示了 SSM 在語音序列處理中優於基於轉換器的模型。在公共基準測試中進行全面評估，展示最先進的性能。對計算效率、抗噪性和序列泛化的分析。這項工作強調了 Mamba SSM 作為無轉換器替代方案的可行性，以實現高效且準確的 ASR。通過利用狀態空間建模的進步，Samba ASR 為 ASR 性能和未來研究樹立了新的基準。</paragraph>

##### **RDD4D: 4D Attention-Guided Road Damage Detection And Classification**
2501.02822v1 by Asma Alkalbani, Muhammad Saqib, Ahmed Salim Alrawahi, Abbas Anwar, Chandarnath Adak, Saeed Anwar

Road damage detection and assessment are crucial components of infrastructure
maintenance. However, current methods often struggle with detecting multiple
types of road damage in a single image, particularly at varying scales. This is
due to the lack of road datasets with various damage types having varying
scales. To overcome this deficiency, first, we present a novel dataset called
Diverse Road Damage Dataset (DRDD) for road damage detection that captures the
diverse road damage types in individual images, addressing a crucial gap in
existing datasets. Then, we provide our model, RDD4D, that exploits Attention4D
blocks, enabling better feature refinement across multiple scales. The
Attention4D module processes feature maps through an attention mechanism
combining positional encoding and "Talking Head" components to capture local
and global contextual information. In our comprehensive experimental analysis
comparing various state-of-the-art models on our proposed, our enhanced model
demonstrated superior performance in detecting large-sized road cracks with an
Average Precision (AP) of 0.458 and maintained competitive performance with an
overall AP of 0.445. Moreover, we also provide results on the CrackTinyNet
dataset; our model achieved around a 0.21 increase in performance. The code,
model weights, dataset, and our results are available on
\href{https://github.com/msaqib17/Road_Damage_Detection}{https://github.com/msaqib17/Road\_Damage\_Detection}.

摘要：道路損壞檢測和評估是基礎設施維護的關鍵組成部分。然而，當前方法通常難以在單張影像中檢測多種類型的道路損壞，特別是在不同比例尺的情況下。這是因為缺乏具有不同損壞類型且具有不同比例尺的道路資料集。為了克服這個缺陷，首先，我們提出了一個名為道路損壞多樣性資料集 (DRDD) 的新資料集，用於道路損壞檢測，它捕獲了個別影像中的各種道路損壞類型，解決了現有資料集中的一個關鍵差距。然後，我們提供了我們的模型 RDD4D，它利用 Attention4D 區塊，可以在多個比例尺上實現更好的特徵細化。Attention4D 模組通過注意力機制處理特徵圖，結合位置編碼和「Talking Head」組件來捕獲局部和全局上下文資訊。在我們全面的實驗分析中，比較了我們提出的各種最先進模型，我們的增強模型在檢測大尺寸道路裂縫方面表現出卓越的效能，平均精確度 (AP) 為 0.458，並以 0.445 的整體 AP 維持競爭力。此外，我們還提供了 CrackTinyNet 資料集的結果；我們的模型將效能提升了約 0.21。程式碼、模型權重、資料集和我們的結果可在
\href{https://github.com/msaqib17/Road\_Damage\_Detection}{https://github.com/msaqib17/Road\_Damage\_Detection} 上找到。

##### **InpDiffusion: Image Inpainting Localization via Conditional Diffusion Models**
2501.02816v1 by Kai Wang, Shaozhang Niu, Qixian Hao, Jiwei Zhang

As artificial intelligence advances rapidly, particularly with the advent of
GANs and diffusion models, the accuracy of Image Inpainting Localization (IIL)
has become increasingly challenging. Current IIL methods face two main
challenges: a tendency towards overconfidence, leading to incorrect
predictions; and difficulty in detecting subtle tampering boundaries in
inpainted images. In response, we propose a new paradigm that treats IIL as a
conditional mask generation task utilizing diffusion models. Our method,
InpDiffusion, utilizes the denoising process enhanced by the integration of
image semantic conditions to progressively refine predictions. During
denoising, we employ edge conditions and introduce a novel edge supervision
strategy to enhance the model's perception of edge details in inpainted
objects. Balancing the diffusion model's stochastic sampling with edge
supervision of tampered image regions mitigates the risk of incorrect
predictions from overconfidence and prevents the loss of subtle boundaries that
can result from overly stochastic processes. Furthermore, we propose an
innovative Dual-stream Multi-scale Feature Extractor (DMFE) for extracting
multi-scale features, enhancing feature representation by considering both
semantic and edge conditions of the inpainted images. Extensive experiments
across challenging datasets demonstrate that the InpDiffusion significantly
outperforms existing state-of-the-art methods in IIL tasks, while also
showcasing excellent generalization capabilities and robustness.

摘要：随着人工智能的快速发展，特别是随着 GAN 和扩散模型的出现，图像修复定位 (IIL) 的准确性变得越来越具有挑战性。当前的 IIL 方法面临着两个主要挑战：过分自信的倾向，导致预测不正确；以及难以检测修复图像中的细微篡改边界。对此，我们提出了一个新的范例，将 IIL 视为利用扩散模型的条件掩码生成任务。我们的方法 InpDiffusion 利用了通过集成图像语义条件而增强的去噪过程来逐步细化预测。在去噪过程中，我们采用边缘条件并引入了一种新颖的边缘监督策略，以增强模型对修复对象中边缘细节的感知。通过边缘监督对扩散模型的随机采样和篡改图像区域进行平衡，可以降低因过度自信而导致预测不正确的风险，并防止因过度随机过程而导致的细微边界丢失。此外，我们提出了一种创新的双流多尺度特征提取器 (DMFE) 来提取多尺度特征，通过同时考虑修复图像的语义和边缘条件来增强特征表示。在具有挑战性的数据集上的大量实验表明，InpDiffusion 在 IIL 任务中明显优于现有的最先进方法，同时还展示了出色的泛化能力和鲁棒性。

##### **Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism**
2501.02803v1 by Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li, Sven Koenig

Multi-Agent Path Finding (MAPF), which focuses on finding collision-free
paths for multiple robots, is crucial in autonomous warehouse operations.
Lifelong MAPF (L-MAPF), where agents are continuously reassigned new targets
upon completing their current tasks, offers a more realistic approximation of
real-world warehouse scenarios. While cache storage systems can enhance
efficiency and reduce operational costs, existing approaches primarily rely on
expectations and mathematical models, often without adequately addressing the
challenges of multi-robot planning and execution. In this paper, we introduce a
novel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which
integrates high-level cache storage with low-level path planning. We have
involved a new type of map grid called cache for temporary item storage.
Additionally, we involved a task assigner (TA) with a locking mechanism to
bridge the gap between the new cache grid and L-MAPF algorithm. The TA
dynamically allocates target locations to agents based on their status in
various scenarios. We evaluated L-MAPF-CM using different cache replacement
policies and task distributions. L-MAPF-CM has demonstrated performance
improvements particularly with high cache hit rates and smooth traffic
conditions.

摘要：多智能體路徑規劃（MAPF）專注於為多個機器人找到無碰撞路徑，這在自動化倉庫操作中至關重要。終身 MAPF（L-MAPF）讓智能體在完成當前任務後持續重新分配新目標，提供了更貼近真實世界的倉庫場景的近似值。快取儲存系統雖然可以提升效率並降低營運成本，但現有方法主要依賴預期和數學模型，通常無法充分解決多機器人規劃和執行的挑戰。在本文中，我們介紹了一種名為帶快取機制的終身 MAPF（L-MAPF-CM）的新機制，它整合了高階快取儲存和低階路徑規劃。我們已經納入了一種類型的稱為快取的地圖格線，用於暫時儲存項目。此外，我們還納入了帶鎖定機制的任務分配器（TA），以彌合新快取格線和 L-MAPF 演算法之間的差距。TA 會根據智能體在各種場景中的狀態動態分配目標位置。我們使用不同的快取替換政策和任務分配評估 L-MAPF-CM。L-MAPF-CM 已證明了效能提升，特別是在快取命中率高且交通狀況順暢的情況下。

##### **InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion**
2501.02795v1 by Zhaoyi Yan, Zhijie Sang, Yiming Zhang, Yuhao Fu, Baoyi He, Qi Zhou, Yining Di, Chunlin Ji, Shengyu Zhang, Fei Wu, Hongxia Yang

Large Language Models (LLMs) have demonstrated strong performance across
various reasoning tasks, yet building a single model that consistently excels
across all domains remains challenging. This paper addresses this problem by
exploring strategies to integrate multiple domain-specialized models into an
efficient pivot model.We propose two fusion strategies to combine the strengths
of multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially
distills each source model into the pivot model, followed by a weight merging
step to integrate the distilled models into the final model. This method
achieves strong performance but requires substantial training effort; and (2) a
unified fusion approach that aggregates all source models' outputs
simultaneously.To improve the fusion process, we introduce a novel
Rate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K
ratios during parameter merging for enhanced flexibility and
stability.Furthermore, we propose an uncertainty-based weighting method for the
unified approach, which dynamically balances the contributions of source models
and outperforms other logits/distribution ensemble methods.We achieved accuracy
improvements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval
tasks, respectively.

摘要：大型語言模型 (LLM) 在各種推理任務中展現強勁的效能，但建立一個在所有領域都能持續卓越的單一模型仍然具有挑戰性。本文透過探索將多個領域專用模型整合到一個高效的樞紐模型中，來解決這個問題。我們提出兩種融合策略來結合多個 LLM 的優點：(1) 成對、多步驟融合方法，依序將每個來源模型萃取出樞紐模型，接著進行權重合併步驟，將萃取出的模型整合到最終模型中。此方法可達成強勁的效能，但需要大量的訓練工作；(2) 統一融合方法，同時彙整所有來源模型的輸出。為了改善融合流程，我們引入一種新穎的比率偏度自適應融合 (RSAF) 技術，在參數合併期間動態調整前 K 個比率，以增強彈性和穩定性。此外，我們針對統一方法提出一個基於不確定性的加權方法，可以動態平衡來源模型的貢獻，並且優於其他邏輯/分配整體方法。我們分別在 GSM8K、MATH 和 HumanEval 任務中，達到了 9.27%、8.80% 和 8.89% 的準確度提升。

##### **Fairness Through Matching**
2501.02793v1 by Kunwoong Kim, Insung Kong, Jongjin Lee, Minwoo Chae, Sangchul Park, Yongdai Kim

Group fairness requires that different protected groups, characterized by a
given sensitive attribute, receive equal outcomes overall. Typically, the level
of group fairness is measured by the statistical gap between predictions from
different protected groups. In this study, we reveal an implicit property of
existing group fairness measures, which provides an insight into how the
group-fair models behave. Then, we develop a new group-fair constraint based on
this implicit property to learn group-fair models. To do so, we first introduce
a notable theoretical observation: every group-fair model has an implicitly
corresponding transport map between the input spaces of each protected group.
Based on this observation, we introduce a new group fairness measure termed
Matched Demographic Parity (MDP), which quantifies the averaged gap between
predictions of two individuals (from different protected groups) matched by a
given transport map. Then, we prove that any transport map can be used in MDP
to learn group-fair models, and develop a novel algorithm called Fairness
Through Matching (FTM), which learns a group-fair model using MDP constraint
with an user-specified transport map. We specifically propose two favorable
types of transport maps for MDP, based on the optimal transport theory, and
discuss their advantages. Experiments reveal that FTM successfully trains
group-fair models with certain desirable properties by choosing the transport
map accordingly.

摘要：群體公平性要求由給定的敏感屬性特徵的不同受保護群體整體獲得平等的結果。通常，群體公平性的水準由不同受保護群體的預測之間的統計差距來衡量。在這個研究中，我們揭示了現有群體公平性衡量標準的隱含屬性，這提供了群體公平模型行為方式的見解。然後，我們根據這個隱含屬性開發了一個新的群體公平約束，以學習群體公平模型。為此，我們首先引入了一個顯著的理論觀察：每個群體公平模型在每個受保護群體的輸入空間之間都有隱含對應的傳輸映射。基於這個觀察，我們引入了一個新的群體公平性衡量標準，稱為匹配人口統計學平價 (MDP)，它量化了兩個個體（來自不同的受保護群體）的預測之間的平均差距，這些個體由給定的傳輸映射匹配。然後，我們證明任何傳輸映射都可以用於 MDP 中，以學習群體公平模型，並開發了一個名為 Fairness Through Matching (FTM) 的新演算法，它使用具有使用者指定傳輸映射的 MDP 約束來學習群體公平模型。我們特別根據最優傳輸理論提出了兩種有利於 MDP 的傳輸映射類型，並討論了它們的優點。實驗表明，FTM 通過相應地選擇傳輸映射，成功地訓練了具有某些理想屬性的群體公平模型。

##### **Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model**
2501.02790v1 by Yueqin Yin, Shentao Yang, Yujia Xie, Ziyi Yang, Yuting Sun, Hany Awadalla, Weizhu Chen, Mingyuan Zhou

Reinforcement learning from human feedback (RLHF) has been widely adopted to
align language models (LMs) with human preference. Prior RLHF works typically
take a bandit formulation, which, though intuitive, ignores the sequential
nature of LM generation and can suffer from the sparse reward issue. While
recent works propose dense token-level RLHF, treating each token as an action
may be oversubtle to proper reward assignment. In this paper, we seek to get
the best of both by training and utilizing a segment-level reward model, which
assigns a reward to each semantically complete text segment that spans over a
short sequence of tokens. For reward learning, our method allows dynamic text
segmentation and compatibility with standard sequence-preference datasets. For
effective RL-based LM training against segment reward, we generalize the
classical scalar bandit reward normalizers into location-aware normalizer
functions and interpolate the segment reward for further densification. With
these designs, our method performs competitively on three popular RLHF
benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation
studies are conducted to further demonstrate our method.

摘要：人類回饋強化學習 (RLHF) 已被廣泛採用，以將語言模型 (LM) 與人類偏好保持一致。先前的 RLHF 工作通常採用多臂老虎機公式，儘管直觀，但忽略了 LM 生成中的順序性質，並且可能會受到稀疏回饋問題的影響。雖然最近的工作提出了密集代幣級 RLHF，但將每個代幣視為動作對於適當的回饋分配來說可能過於微妙。在本文中，我們尋求通過訓練和利用區段級回饋模型來獲得兩全其美，該模型會對跨越短序列代幣的每個語義完整文字區段分配回饋。對於回饋學習，我們的方法允許動態文字分段，並與標準序列偏好資料集相容。對於針對區段回饋的有效基於 RL 的 LM 訓練，我們將經典標量多臂老虎機回饋正規化器概括為位置感知正規化器函數，並內插區段回饋以進一步致密化。透過這些設計，我們的模型在三個流行的 RLHF LM 政策基準上表現具競爭力：AlpacaEval 2.0、Arena-Hard 和 MT-Bench。消融研究旨在進一步展示我們的模型。

##### **GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation**
2501.02788v1 by Niloufar Eghbali, Hassan Bagher-Ebadian, Tuka Alhanai, Mohammad M. Ghassemi

Vision Transformers (ViTs) have shown promise in medical image semantic
segmentation (MISS) by capturing long-range correlations. However, ViTs often
struggle to model local spatial information effectively, which is essential for
accurately segmenting fine anatomical details, particularly when applied to
small datasets without extensive pre-training. We introduce Gabor and Laplacian
of Gaussian Convolutional Swin Network (GLoG-CSUnet), a novel architecture
enhancing Transformer-based models by incorporating learnable radiomic
features. This approach integrates dynamically adaptive Gabor and Laplacian of
Gaussian (LoG) filters to capture texture, edge, and boundary information,
enhancing the feature representation processed by the Transformer model. Our
method uniquely combines the long-range dependency modeling of Transformers
with the texture analysis capabilities of Gabor and LoG features. Evaluated on
the Synapse multi-organ and ACDC cardiac segmentation datasets, GLoG-CSUnet
demonstrates significant improvements over state-of-the-art models, achieving a
1.14\% increase in Dice score for Synapse and 0.99\% for ACDC, with minimal
computational overhead (only 15 and 30 additional parameters, respectively).
GLoG-CSUnet's flexible design allows integration with various base models,
offering a promising approach for incorporating radiomics-inspired feature
extraction in Transformer architectures for medical image analysis. The code
implementation is available on GitHub at: https://github.com/HAAIL/GLoG-CSUnet.

摘要：<paragraph>視覺轉換器 (ViT) 已在醫療影像語意分割 (MISS) 中展現出前景，藉由捕捉長距離關聯性。然而，ViT 通常難以有效建模局部空間資訊，這對於精準分割細微解剖細節至關重要，特別是在應用於沒有廣泛預訓練的小型資料集時。我們引進 Gabor 和拉普拉斯高斯卷積 Swin 網路 (GLoG-CSUnet)，這是一種創新的架構，透過整合可學習的放射特徵來增強基於 Transformer 的模型。此方法整合動態自適應 Gabor 和拉普拉斯高斯 (LoG) 濾波器來捕捉紋理、邊緣和邊界資訊，增強 Transformer 模型處理的特徵表徵。我們的技術獨特地結合了 Transformer 的長距離依賴性建模與 Gabor 和 LoG 特徵的紋理分析能力。在 Synapse 多器官和 ACDC 心臟分割資料集上進行評估後，GLoG-CSUnet 證明比最先進的模型有顯著的進步，Synapse 的 Dice 分數增加了 1.14%，ACDC 則增加了 0.99%，而運算負擔極小（分別僅有 15 和 30 個額外的參數）。GLoG-CSUnet 的彈性設計允許與各種基礎模型整合，為在 Transformer 架構中整合放射組學啟發的特徵萃取提供了一種有前景的方法，以進行醫療影像分析。程式碼實作可在 GitHub 上取得：https://github.com/HAAIL/GLoG-CSUnet。</paragraph>

##### **Hybrid deep convolution model for lung cancer detection with transfer learning**
2501.02785v1 by Sugandha Saxena, S. N. Prasad, Ashwin M Polnaya, Shweta Agarwala

Advances in healthcare research have significantly enhanced our understanding
of disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung
cancer remains one of the leading causes of cancer-related mortality worldwide
due to challenges in early and accurate diagnosis. While current lung cancer
detection models show promise, there is considerable potential for further
improving the accuracy for timely intervention. To address this challenge, we
introduce a hybrid deep convolution model leveraging transfer learning, named
the Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the
precision of lung cancer detection by refining sensitivity and specificity.
This model has surpassed existing deep learning approaches through experimental
validation, achieving an accuracy of 98% and a sensitivity of 97%. By
overlaying sensitivity maps onto lung Computed Tomography (CT) scans, it
enables the visualization of regions most indicative of malignant or benign
classifications. This innovative method demonstrates exceptional performance in
distinguishing lung cancer with minimal false positives, thereby enhancing the
accuracy of medical diagnoses.

摘要：醫療保健研究的進步顯著增進了我們對疾病機制、診斷精準度和治療選擇的了解。然而，由於早期和準確診斷的挑戰，肺癌仍然是全球癌症相關死亡的主要原因之一。雖然目前的肺癌檢測模型顯示出前景，但仍有相當大的潛力可以進一步提高準確性，以便及時介入。為了應對這一挑戰，我們引入了利用遷移學習的混合深度卷積模型，名為最大敏感度神經網路 (MSNN)。MSNN 旨在透過調整敏感度和特異性來提高肺癌檢測的準確性。此模型已透過實驗驗證超越現有的深度學習方法，達到 98% 的準確度和 97% 的敏感度。透過將敏感度圖疊加到肺部電腦斷層掃描 (CT) 上，它可以視覺化出最能代表惡性或良性分類的區域。這種創新方法在區分肺癌時表現出極佳的效能，且誤判為陽性的情況最少，從而提高了醫療診斷的準確性。

##### **GeAR: Generation Augmented Retrieval**
2501.02772v1 by Haoyu Liu, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Furu Wei, Qi Zhang

Document retrieval techniques form the foundation for the development of
large-scale information systems. The prevailing methodology is to construct a
bi-encoder and compute the semantic similarity. However, such scalar similarity
is difficult to reflect enough information and impedes our comprehension of the
retrieval results. In addition, this computational process mainly emphasizes
the global semantics and ignores the fine-grained semantic relationship between
the query and the complex text in the document. In this paper, we propose a new
method called $\textbf{Ge}$neration $\textbf{A}$ugmented $\textbf{R}$etrieval
($\textbf{GeAR}$) that incorporates well-designed fusion and decoding modules.
This enables GeAR to generate the relevant text from documents based on the
fused representation of the query and the document, thus learning to "focus on"
the fine-grained information. Also when used as a retriever, GeAR does not add
any computational burden over bi-encoders. To support the training of the new
framework, we have introduced a pipeline to efficiently synthesize high-quality
data by utilizing large language models. GeAR exhibits competitive retrieval
and localization performance across diverse scenarios and datasets. Moreover,
the qualitative analysis and the results generated by GeAR provide novel
insights into the interpretation of retrieval results. The code, data, and
models will be released after completing technical review to facilitate future
research.

摘要：文件檢索技術是建立大型資訊系統的基礎。目前普遍的方法是建構雙編碼器並計算語意相似度。然而，這種標量相似度難以反映足夠的資訊，阻礙我們理解檢索結果。此外，這個運算過程主要強調整體語意，而忽略查詢與文件中的複雜文字之間的細粒度語意關係。在本文中，我們提出了一種稱為 $\textbf{Ge}$neration $\textbf{A}$ugmented $\textbf{R}$etrieval（$\textbf{GeAR}$）的新方法，它結合了精心設計的融合和解碼模組。這使得 GeAR 能夠根據查詢和文件的融合表示來產生文件中的相關文字，從而學會「專注於」細粒度資訊。此外，當用作檢索器時，GeAR 並不會對雙編碼器增加任何運算負擔。為了支援新架構的訓練，我們引入了一個管道，利用大型語言模型有效地合成高品質資料。GeAR 在不同的場景和資料集上展現出有競爭力的檢索和定位效能。此外，GeAR 產生的定性分析和結果為檢索結果的詮釋提供了新的見解。在完成技術審查後，程式碼、資料和模型將會釋出，以利未來的研究。

##### **Enhancing Trustworthiness of Graph Neural Networks with Rank-Based Conformal Training**
2501.02767v1 by Ting Wang, Zhixin Zhou, Rui Luo

Graph Neural Networks (GNNs) has been widely used in a variety of fields
because of their great potential in representing graph-structured data.
However, lacking of rigorous uncertainty estimations limits their application
in high-stakes. Conformal Prediction (CP) can produce statistically guaranteed
uncertainty estimates by using the classifier's probability estimates to obtain
prediction sets, which contains the true class with a user-specified
probability. In this paper, we propose a Rank-based CP during training
framework to GNNs (RCP-GNN) for reliable uncertainty estimates to enhance the
trustworthiness of GNNs in the node classification scenario. By exploiting rank
information of the classifier's outcome, prediction sets with desired coverage
rate can be efficiently constructed. The strategy of CP during training with
differentiable rank-based conformity loss function is further explored to adapt
prediction sets according to network topology information. In this way, the
composition of prediction sets can be guided by the goal of jointly reducing
inefficiency and probability estimation errors. Extensive experiments on
several real-world datasets show that our model achieves any pre-defined target
marginal coverage while significantly reducing the inefficiency compared with
state-of-the-art methods.

摘要：圖神經網路 (GNN) 因其在表示圖結構資料方面的巨大潛力，而被廣泛應用於各種領域。
然而，缺乏嚴謹的不確定性估計限制了它們在高風險中的應用。共形預測 (CP) 可利用分類器的機率估計來取得預測集合，其中包含具有使用者指定機率的真實類別，進而產生具有統計保證的不確定性估計。在本文中，我們提出了一個在訓練期間基於排名 (RCP-GNN) 的共形預測框架，以提供可靠的不確定性估計，以增強 GNN 在節點分類場景中的可信度。透過利用分類器結果的排名資訊，可以有效地建構具有所需覆蓋率的預測集合。進一步探討了使用可微分基於排名的共形損失函數在訓練期間進行共形預測的策略，以根據網路拓撲資訊調整預測集合。透過這種方式，預測集合的組成可以透過共同減少低效率和機率估計誤差的目標來引導。在幾個真實世界資料集上的廣泛實驗表明，我們的模型達到了任何預先定義的目標邊際覆蓋率，同時與最先進的方法相比，顯著降低了低效率。

##### **Are GNNs Effective for Multimodal Fault Diagnosis in Microservice Systems?**
2501.02766v1 by Fei Gao, Ruyue Xin, Yaqiang Zhang

Fault diagnosis in microservice systems has increasingly embraced multimodal
observation data for a holistic and multifaceted view of the system, with Graph
Neural Networks (GNNs) commonly employed to model complex service dependencies.
However, despite the intuitive appeal, there remains a lack of compelling
justification for the adoption of GNNs, as no direct evidence supports their
necessity or effectiveness. To critically evaluate the current use of GNNs, we
propose DiagMLP, a simple topology-agnostic baseline as a substitute for GNNs
in fault diagnosis frameworks. Through experiments on five public datasets, we
surprisingly find that DiagMLP performs competitively with and even outperforms
GNN-based methods in fault diagnosis tasks, indicating that the current
paradigm of using GNNs to model service dependencies has not yet demonstrated a
tangible contribution. We further discuss potential reasons for this
observation and advocate shifting the focus from solely pursuing novel model
designs to developing challenging datasets, standardizing preprocessing
protocols, and critically evaluating the utility of advanced deep learning
modules.

摘要：微服務系統的故障診斷越來越採用多模態觀測數據，以全面且多方面的了解系統，而圖神經網路 (GNN) 通常用於建模複雜的服務依賴關係。
然而，儘管具有直觀的吸引力，但仍缺乏採用 GNN 的令人信服的理由，因為沒有直接證據支持它們的必要性或有效性。為了批判性地評估 GNN 的當前使用情況，我們提出了 DiagMLP，這是一個簡單的與拓撲無關的基準，可用於故障診斷框架中的 GNN。透過對五個公開資料集的實驗，我們驚訝地發現 DiagMLP 在故障診斷任務中表現出與 GNN 為基礎的方法競爭甚至優於它們，這表明當前使用 GNN 來建模服務依賴關係的範例尚未證明有具體的貢獻。我們進一步討論了這種觀察的潛在原因，並主張將重點從僅追求新穎的模型設計轉移到開發具有挑戰性的資料集、標準化預處理協定以及批判性地評估進階深度學習模組的效用。

##### **Visual Large Language Models for Generalized and Specialized Applications**
2501.02765v1 by Yifan Li, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, Yu Kong

Visual-language models (VLM) have emerged as a powerful tool for learning a
unified embedding space for vision and language. Inspired by large language
models, which have demonstrated strong reasoning and multi-task capabilities,
visual large language models (VLLMs) are gaining increasing attention for
building general-purpose VLMs. Despite the significant progress made in VLLMs,
the related literature remains limited, particularly from a comprehensive
application perspective, encompassing generalized and specialized applications
across vision (image, video, depth), action, and language modalities. In this
survey, we focus on the diverse applications of VLLMs, examining their using
scenarios, identifying ethics consideration and challenges, and discussing
future directions for their development. By synthesizing these contents, we aim
to provide a comprehensive guide that will pave the way for future innovations
and broader applications of VLLMs. The paper list repository is available:
https://github.com/JackYFL/awesome-VLLMs.

摘要：視覺語言模型 (VLM) 已成為學習視覺和語言統一嵌入空間的強大工具。受大型語言模型的啟發，這些模型已展現出強大的推理和多任務能力，視覺大型語言模型 (VLLM) 正越來越受到關注，用於建構通用 VLM。儘管 VLLM 已取得顯著進展，但相關文獻仍然有限，特別是從全面的應用觀點來看，涵蓋視覺（影像、影片、深度）、動作和語言模式的廣義和專門應用。在這項調查中，我們專注於 VLLM 的多樣化應用，探討它們的使用情境、找出道德考量和挑戰，並討論它們未來的發展方向。透過綜合這些內容，我們旨在提供一份全面的指南，為 VLLM 未來的創新和更廣泛的應用鋪路。論文清單儲存庫已提供：https://github.com/JackYFL/awesome-VLLMs。

##### **MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and Attention Distillation**
2501.02754v1 by Yidong Ding, Jiafei Niu, Ping Yi

In recent years, attention-based models have excelled across various domains
but remain vulnerable to backdoor attacks, often from downloading or
fine-tuning on poisoned datasets. Many current methods to mitigate backdoors in
NLP models rely on the pre-trained (unfine-tuned) weights, but these methods
fail in scenarios where the pre-trained weights are not available. In this
work, we propose MBTSAD, which can mitigate backdoors in the language model by
utilizing only a small subset of clean data and does not require pre-trained
weights. Specifically, MBTSAD retrains the backdoored model on a dataset
generated by token splitting. Then MBTSAD leverages attention distillation, the
retrained model is the teacher model, and the original backdoored model is the
student model. Experimental results demonstrate that MBTSAD achieves comparable
backdoor mitigation performance as the methods based on pre-trained weights
while maintaining the performance on clean data. MBTSAD does not rely on
pre-trained weights, enhancing its utility in scenarios where pre-trained
weights are inaccessible. In addition, we simplify the min-max problem of
adversarial training and visualize text representations to discover that the
token splitting method in MBTSAD's first step generates Out-of-Distribution
(OOD) data, leading the model to learn more generalized features and eliminate
backdoor patterns.

摘要：近年來，基於注意力的模型在各個領域都表現出色，但仍然容易受到後門攻擊，通常是從下載或微調中毒數據集而來。許多當前用於減輕 NLP 模型中後門的方法依賴於預先訓練的（未微調）權重，但這些方法在預先訓練的權重不可用的情況下會失敗。在這項工作中，我們提出了 MBTSAD，它可以通過僅使用一小部分乾淨數據來減輕語言模型中的後門，並且不需要預先訓練的權重。具體來說，MBTSAD 在通過令牌分割生成的數據集上重新訓練後門模型。然後，MBTSAD 利用注意力蒸餾，重新訓練的模型是教師模型，原始後門模型是學生模型。實驗結果表明，MBTSAD 達到了與基於預先訓練權重的方法相當的後門減輕性能，同時保持了對乾淨數據的性能。MBTSAD 不依賴於預先訓練的權重，增強了其在預先訓練的權重無法訪問的情況下的效用。此外，我們簡化了對抗訓練的 min-max 問題，並可視化文本表示，以發現 MBTSAD 第一步中的令牌分割方法生成了 Out-of-Distribution (OOD) 數據，導致模型學習更廣泛的特徵並消除後門模式。

##### **Interpretable Recognition of Fused Magnesium Furnace Working Conditions with Deep Convolutional Stochastic Configuration Networks**
2501.02740v1 by Li Weitao, Zhang Xinru, Wang Dianhui, Tong Qianqian, Chai Tianyou

To address the issues of a weak generalization capability and
interpretability in working condition recognition model of a fused magnesium
furnace, this paper proposes an interpretable working condition recognition
method based on deep convolutional stochastic configuration networks (DCSCNs).
Firstly, a supervised learning mechanism is employed to generate physically
meaningful Gaussian differential convolution kernels. An incremental method is
utilized to construct a DCSCNs model, ensuring the convergence of recognition
errors in a hierarchical manner and avoiding the iterative optimization process
of convolutional kernel parameters using the widely used backpropagation
algorithm. The independent coefficient of channel feature maps is defined to
obtain the visualization results of feature class activation maps for the fused
magnesium furnace. A joint reward function is constructed based on the
recognition accuracy, the interpretable trustworthiness evaluation metrics, and
the model parameter quantity. Reinforcement learning (RL) is applied to
adaptively prune the convolutional kernels of the DCSCNs model, aiming to build
a compact, highly performed and interpretable network. The experimental results
demonstrate that the proposed method outperforms the other deep learning
approaches in terms of recognition accuracy and interpretability.

摘要：針對熔融鎂爐工作狀態識別模型中泛化能力弱和可解釋性差的問題，本文提出了一種基於深度卷積隨機配置網路（DCSCN）的可解釋工作狀態識別方法。首先，採用監督學習機制產生物理意義上的高斯差分卷積核。利用遞增方法構建DCSCN模型，分層保證識別誤差收斂，避免使用廣泛採用的反向傳播演算法對卷積核參數進行迭代優化。定義通道特徵圖的獨立係數，得到熔融鎂爐特徵類激活圖的可視化結果。基於識別準確率、可解釋可信度評估指標和模型參數量，構建聯合獎勵函數。採用強化學習（RL）自適應剪枝DCSCN模型的卷積核，旨在構建緊湊、高效且可解釋的網路。實驗結果表明，所提出的方法在識別準確率和可解釋性方面優於其他深度學習方法。

##### **TARDiS : Text Augmentation for Refining Diversity and Separability**
2501.02739v1 by Kyungmin Kim, SangHun Im, GiBaeg Kim, Heung-Seon Oh

Text augmentation (TA) is a critical technique for text classification,
especially in few-shot settings. This paper introduces a novel LLM-based TA
method, TARDiS, to address challenges inherent in the generation and alignment
stages of two-stage TA methods. For the generation stage, we propose two
generation processes, SEG and CEG, incorporating multiple class-specific
prompts to enhance diversity and separability. For the alignment stage, we
introduce a class adaptation (CA) method to ensure that generated examples
align with their target classes through verification and modification.
Experimental results demonstrate TARDiS's effectiveness, outperforming
state-of-the-art LLM-based TA methods in various few-shot text classification
tasks. An in-depth analysis confirms the detailed behaviors at each stage.

摘要：文字擴充 (TA) 是文字分類的一項關鍵技術，特別是在少樣本設定中。這篇論文介紹了一種新穎的基於 LLM 的 TA 方法 TARDiS，以解決兩階段 TA 方法的生成和對齊階段中固有的挑戰。對於生成階段，我們提出了兩個生成程序，SEG 和 CEG，結合了多個類別特定的提示來增強多樣性和可分離性。對於對齊階段，我們引入了一個類別適應 (CA) 方法，以確保生成的範例通過驗證和修改與其目標類別對齊。實驗結果證明了 TARDiS 的有效性，在各種少樣本文字分類任務中優於最先進的基於 LLM 的 TA 方法。深入分析確認了每個階段的詳細行為。

##### **AFed: Algorithmic Fair Federated Learning**
2501.02732v1 by Huiqiang Chen, Tianqing Zhu, Wanlei Zhou, Wei Zhao

Federated Learning (FL) has gained significant attention as it facilitates
collaborative machine learning among multiple clients without centralizing
their data on a server. FL ensures the privacy of participating clients by
locally storing their data, which creates new challenges in fairness.
Traditional debiasing methods assume centralized access to sensitive
information, rendering them impractical for the FL setting. Additionally, FL is
more susceptible to fairness issues than centralized machine learning due to
the diverse client data sources that may be associated with group information.
Therefore, training a fair model in FL without access to client local data is
important and challenging. This paper presents AFed, a straightforward yet
effective framework for promoting group fairness in FL. The core idea is to
circumvent restricted data access by learning the global data distribution.
This paper proposes two approaches: AFed-G, which uses a conditional generator
trained on the server side, and AFed-GAN, which improves upon AFed-G by
training a conditional GAN on the client side. We augment the client data with
the generated samples to help remove bias. Our theoretical analysis justifies
the proposed methods, and empirical results on multiple real-world datasets
demonstrate a substantial improvement in AFed over several baselines.

摘要：聯盟式學習 (FL) 獲得顯著的關注，因為它促進多個用戶之間的協作機器學習，而無需將其資料集中在伺服器上。FL 透過在本地儲存參與用戶的資料來確保其隱私，這在公平性方面產生新的挑戰。傳統的去偏見方法假設集中存取敏感資訊，讓它們不適用於 FL 設定。此外，FL 比集中式機器學習更容易受到公平性問題的影響，因為多樣化的用戶資料來源可能與群組資訊相關聯。因此，在無法存取用戶本地資料的情況下訓練 FL 中的公平模型非常重要且具有挑戰性。本文提出 AFed，一個直接但有效的架構，用於促進 FL 中的群組公平性。核心概念是透過學習全球資料分佈來規避受限的資料存取。本文提出兩種方法：AFed-G，它使用在伺服器端訓練的條件式產生器，以及 AFed-GAN，它透過在用戶端訓練條件式 GAN 來改善 AFed-G。我們使用產生的範例擴充用戶資料，以協助消除偏見。我們的理論分析證明了所提出的方法，而多個真實世界資料集上的經驗結果證明 AFed 在多個基準上都有顯著的改善。

##### **OpenGU: A Comprehensive Benchmark for Graph Unlearning**
2501.02728v1 by Bowen Fan, Yuming Ai, Xunkai Li, Zhilin Guo, Rong-Hua Li, Guoren Wang

Graph Machine Learning is essential for understanding and analyzing
relational data. However, privacy-sensitive applications demand the ability to
efficiently remove sensitive information from trained graph neural networks
(GNNs), avoiding the unnecessary time and space overhead caused by retraining
models from scratch. To address this issue, Graph Unlearning (GU) has emerged
as a critical solution, with the potential to support dynamic graph updates in
data management systems and enable scalable unlearning in distributed data
systems while ensuring privacy compliance. Unlike machine unlearning in
computer vision or other fields, GU faces unique difficulties due to the
non-Euclidean nature of graph data and the recursive message-passing mechanism
of GNNs. Additionally, the diversity of downstream tasks and the complexity of
unlearning requests further amplify these challenges. Despite the proliferation
of diverse GU strategies, the absence of a benchmark providing fair comparisons
for GU, and the limited flexibility in combining downstream tasks and
unlearning requests, have yielded inconsistencies in evaluations, hindering the
development of this domain. To fill this gap, we present OpenGU, the first GU
benchmark, where 16 SOTA GU algorithms and 37 multi-domain datasets are
integrated, enabling various downstream tasks with 13 GNN backbones when
responding to flexible unlearning requests. Based on this unified benchmark
framework, we are able to provide a comprehensive and fair evaluation for GU.
Through extensive experimentation, we have drawn $8$ crucial conclusions about
existing GU methods, while also gaining valuable insights into their
limitations, shedding light on potential avenues for future research.

摘要：圖形機器學習對於理解和分析關係資料至關重要。然而，注重隱私的應用程式要求有能力有效率地從訓練好的圖形神經網路 (GNN) 中移除敏感資訊，避免因從頭重新訓練模型而造成的時間和空間負擔。為了解決這個問題，圖形解除學習 (GU) 已成為一個關鍵的解決方案，它有潛力支援資料管理系統中的動態圖形更新，並在確保隱私合規的同時，在分散式資料系統中實現可擴充的解除學習。與電腦視覺或其他領域的機器解除學習不同，GU 由於圖形資料的非歐幾何性質和 GNN 的遞迴訊息傳遞機制而面臨獨特的困難。此外，下游任務的多樣性和解除學習請求的複雜性進一步放大了這些挑戰。儘管 GU 策略不斷增加，但缺乏提供公平比較 GU 的基準，以及在結合下游任務和解除學習請求方面的靈活性有限，導致評估出現不一致，阻礙了這個領域的發展。為了填補這個缺口，我們提出了 OpenGU，這是第一個 GU 基準，其中整合了 16 個 SOTA GU 演算法和 37 個多領域資料集，在回應彈性的解除學習請求時，使用 13 個 GNN 主幹啟用了各種下游任務。基於這個統一的基準架構，我們能夠為 GU 提供全面且公平的評估。透過廣泛的實驗，我們對現有的 GU 方法得出了 8 個重要的結論，同時也深入了解了它們的限制，進而為未來的研究提供了潛在途徑。

##### **Artificial Intelligence in Creative Industries: Advances Prior to 2025**
2501.02725v1 by Nantheera Anantrasirichai, Fan Zhang, David Bull

The rapid advancements in artificial intelligence (AI), particularly in
generative AI and large language models (LLMs), have profoundly impacted the
creative industries by enabling innovative content creation, enhancing
workflows, and democratizing access to creative tools. This paper explores the
significant technological shifts since our previous review in 2022,
highlighting how these developments have expanded creative opportunities and
efficiency. These technological advancements have enhanced the capabilities of
text-to-image, text-to-video, and multimodal generation technologies. In
particular, key breakthroughs in LLMs have established new benchmarks in
conversational AI, while advancements in image generators have revolutionized
content creation. We also discuss AI integration into post-production
workflows, which has significantly accelerated and refined traditional
processes. Despite these innovations, challenges remain, particularly for the
media industry, due to the demands on communication traffic from creative
content. We therefore include data compression and quality assessment in this
paper. Furthermore, we highlight the trend toward unified AI frameworks capable
of addressing multiple creative tasks and underscore the importance of human
oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's
future potential in the creative sector, stressing the need to navigate
emerging challenges to maximize its benefits while addressing associated risks.

摘要：人工智能 (AI) 的快速進展，特別是生成式 AI 和大型語言模型 (LLM)，通過啟用創新的內容創作、增強工作流程和民主化訪問創意工具，對創意產業產生了深遠的影響。本文探討了自我們 2022 年上次評估以來的重大技術轉變，重點說明這些發展如何擴展了創造性機會和效率。這些技術進步增強了文字轉圖像、文字轉影片和多模態生成技術的能力。特別是，LLM 的關鍵突破在對話式 AI 中建立了新的基準，而影像生成器的進步則徹底改變了內容創作。我們還討論了 AI 整合到後期製作工作流程中，這顯著地加速並改進了傳統流程。儘管有這些創新，但挑戰仍然存在，特別是對於媒體產業，因為創意內容對通信流量的需求。因此，我們在這篇論文中納入了數據壓縮和品質評估。此外，我們強調了能夠處理多項創意任務的統一 AI 框架的趨勢，並強調人類監督對於減輕 AI 生成的錯誤的重要性。最後，我們探討了 AI 在創意領域的未來潛力，強調需要應對新興挑戰以最大化其好處，同時應對相關風險。

##### **KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models**
2501.02711v1 by Zaiyi Zheng, Yushun Dong, Song Wang, Haochen Liu, Qi Wang, Jundong Li

Large Language Models (LLMs) have shown impressive performance in various
tasks, including knowledge graph completion (KGC). However, current studies
mostly apply LLMs to classification tasks, like identifying missing triplets,
rather than ranking-based tasks, where the model ranks candidate entities based
on plausibility. This focus limits the practical use of LLMs in KGC, as
real-world applications prioritize highly plausible triplets. Additionally,
while graph paths can help infer the existence of missing triplets and improve
completion accuracy, they often contain redundant information. To address these
issues, we propose KG-CF, a framework tailored for ranking-based KGC tasks.
KG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts,
achieving superior results on real-world datasets. The code and datasets are
available at \url{https://anonymous.4open.science/r/KG-CF}.

摘要：大型語言模型 (LLM) 在各種任務中展現出令人印象深刻的表現，包括知識圖譜完成功能 (KGC)。然而，目前的研究大多將 LLM 應用於分類任務，例如識別遺漏的三元組，而非基於排名的任務，其中模型根據合理性對候選實體進行排名。這種重點限制了 LLM 在 KGC 中的實際應用，因為現實世界的應用優先考慮高度合理的的三元組。此外，儘管圖形路徑有助於推斷遺漏的三元組的存在並提高完成的準確性，但它們通常包含冗餘資訊。為了解決這些問題，我們提出 KG-CF，一個專門針對基於排名的 KGC 任務的框架。KG-CF 利用 LLM 的推理能力來過濾不相關的上下文，在現實世界的資料集上取得卓越的成果。程式碼和資料集可在 \url{https://anonymous.4open.science/r/KG-CF} 取得。

##### **QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance**
2501.02702v1 by Binita Saha, Utsha Saha, Muhammad Zubair Malik

This work presents a novel architecture for building Retrieval-Augmented
Generation (RAG) systems to improve Question Answering (QA) tasks from a target
corpus. Large Language Models (LLMs) have revolutionized the analyzing and
generation of human-like text. These models rely on pre-trained data and lack
real-time updates unless integrated with live data tools. RAG enhances LLMs by
integrating online resources and databases to generate contextually appropriate
responses. However, traditional RAG still encounters challenges like
information dilution and hallucinations when handling vast amounts of data. Our
approach addresses these challenges by converting corpora into a
domain-specific dataset and RAG architecture is constructed to generate
responses from the target document. We introduce QuIM-RAG (Question-to-question
Inverted Index Matching), a novel approach for the retrieval mechanism in our
system. This strategy generates potential questions from document chunks and
matches these with user queries to identify the most relevant text chunks for
generating accurate answers. We have implemented our RAG system on top of the
open-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on
Hugging Face. We constructed a custom corpus of 500+ pages from a high-traffic
website accessed thousands of times daily for answering complex questions,
along with manually prepared ground truth QA for evaluation. We compared our
approach with traditional RAG models using BERT-Score and RAGAS,
state-of-the-art metrics for evaluating LLM applications. Our evaluation
demonstrates that our approach outperforms traditional RAG architectures on
both metrics.

摘要：本研究提出了一種新穎的架構，用於建構檢索增強生成 (RAG) 系統，以改善目標語料庫中的問答 (QA) 任務。大型語言模型 (LLM) 徹底改變了人類語言的分析和生成方式。這些模型依賴於預先訓練的資料，除非與即時資料工具整合，否則缺乏即時更新。RAG 透過整合線上資源和資料庫，增強 LLM，以產生符合脈絡的回應。然而，傳統的 RAG 在處理大量資料時仍會遇到資訊稀釋和幻覺等挑戰。我們的做法透過將語料庫轉換成特定領域的資料集來解決這些挑戰，並建構 RAG 架構，以從目標文件產生回應。我們引入了 QuIM-RAG（問題對問題反向索引比對），這是一種我們系統中檢索機制的創新方法。此策略從文件區塊中產生潛在問題，並將這些問題與使用者查詢進行比對，以找出最相關的文字區塊，用於產生準確的答案。我們已在 Meta Inc. 開源的 Meta-LLaMA3-8B-instruct 模型上實作我們的 RAG 系統，該模型可在 Hugging Face 上取得。我們從一個每日存取數千次的流量極高的網站建構了一個包含 500 多頁的客製化語料庫，用於回答複雜的問題，並手動準備了地面實況 QA 以進行評估。我們使用 BERT-Score 和 RAGAS（評估 LLM 應用程式的最新指標）將我們的方法與傳統的 RAG 模型進行比較。我們的評估證明，我們的做法在兩個指標上都優於傳統的 RAG 架構。

##### **EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models**
2501.02699v1 by Andrés Villa, Juan León Alcázar, Motasem Alfarra, Vladimir Araujo, Alvaro Soto, Bernard Ghanem

Large language models and vision transformers have demonstrated impressive
zero-shot capabilities, enabling significant transferability in downstream
tasks. The fusion of these models has resulted in multi-modal architectures
with enhanced instructional capabilities. Despite incorporating vast image and
language pre-training, these multi-modal architectures often generate responses
that deviate from the ground truth in the image data. These failure cases are
known as hallucinations. Current methods for mitigating hallucinations
generally focus on regularizing the language component, improving the fusion
module, or ensembling multiple visual encoders to improve visual
representation. In this paper, we address the hallucination issue by directly
enhancing the capabilities of the visual component. Our approach, named EAGLE,
is fully agnostic to the LLM or fusion module and works as a post-pretraining
approach that improves the grounding and language alignment of the visual
encoder. We show that a straightforward reformulation of the original
contrastive pre-training task results in an improved visual encoder that can be
incorporated into the instructional multi-modal architecture without additional
instructional training. As a result, EAGLE achieves a significant reduction in
hallucinations across multiple challenging benchmarks and tasks.

摘要：大型语言模型和视觉转换器已展示出令人印象深刻的零样本能力，可在下游任务中实现显著的可迁移性。这些模型的融合产生了具有增强指令能力的多模态架构。尽管融合了大量的图像和语言预训练，但这些多模态架构通常会生成偏离图像数据中基本事实的响应。这些失败案例被称为幻觉。当前减轻幻觉的方法通常集中在正则化语言组件、改进融合模块或集成多个视觉编码器以改进视觉表示。在本文中，我们通过直接增强视觉组件的能力来解决幻觉问题。我们的方法名为 EAGLE，它完全独立于 LLM 或融合模块，并作为一种后预训练方法，可改进视觉编码器的接地和语言对齐。我们表明，对原始对比预训练任务的直接重新表述产生了改进的视觉编码器，该编码器可以合并到指令性多模态架构中，而无需额外的指令性训练。因此，EAGLE 在多个具有挑战性的基准和任务中显著减少了幻觉。

##### **Decoding specialised feature neurons in LLMs with the final projection layer**
2501.02688v1 by Harry J Davies

Large Language Models (LLMs) typically have billions of parameters and are
thus often difficult to interpret in their operation. Such black-box models can
pose a significant risk to safety when trusted to make important decisions. The
lack of interpretability of LLMs is more related to their sheer size, rather
than the complexity of their individual components. The TARS method for
knowledge removal (Davies et al 2024) provides strong evidence for the
hypothesis that that linear layer weights which act directly on the residual
stream may have high correlation with different concepts encoded in the
residual stream. Building upon this, we attempt to decode neuron weights
directly into token probabilities through the final projection layer of the
model (the LM-head). Firstly, we show that with Llama 3.1 8B we can utilise the
LM-head to decode specialised feature neurons that respond strongly to certain
concepts, with examples such as "dog" and "California". This is then confirmed
by demonstrating that these neurons can be clamped to affect the probability of
the concept in the output. This extends to the fine-tuned assistant Llama 3.1
8B instruct model, where we find that over 75% of neurons in the up-projection
layers have the same top associated token compared to the pretrained model.
Finally, we demonstrate that clamping the "dog" neuron leads the instruct model
to always discuss dogs when asked about its favourite animal. Through our
method, it is possible to map the entirety of Llama 3.1 8B's up-projection
neurons in less than 15 minutes with no parallelization.

摘要：大型語言模型 (LLM) 通常有數十億個參數，因此通常難以解釋其運作方式。當信任這些黑盒模型做出重要決策時，這些模型可能會對安全性構成重大風險。LLM 的難以解釋性與其龐大規模有關，而非其個別組成的複雜性。TARS 知識移除方法 (Davies 等人 2024) 為以下假設提供了強有力的證據：直接作用於殘差串流的線性層權重可能與殘差串流中編碼的不同概念高度相關。在此基礎上，我們嘗試通過模型的最終投影層（LM 頭部）將神經元權重直接解碼為代幣機率。首先，我們展示了使用 Llama 3.1 8B，我們可以使用 LM 頭部解碼對某些概念有強烈反應的專業特徵神經元，例如「狗」和「加州」。這隨後通過證明這些神經元可以被箝制以影響輸出中概念的機率而得到證實。這延伸到微調後的助理 Llama 3.1 8B 指令模型，我們發現上投影層中超過 75% 的神經元與預訓練模型相比具有相同的頂級關聯代幣。最後，我們證明箝制「狗」神經元會導致指令模型在被問及最喜歡的動物時總是討論狗。透過我們的方法，可以在不到 15 分鐘內將 Llama 3.1 8B 的上投影神經元整體對應，且無並行化。

##### **From Superficial Patterns to Semantic Understanding: Fine-Tuning Language Models on Contrast Sets**
2501.02683v1 by Daniel Petrov

Large scale pretrained language models have demonstrated high performance on
standard datasets for natural language inference (NLI) tasks. Unfortunately,
these evaluations can be misleading, as although the models can perform well on
in-distribution data, they perform poorly on out-of-distribution test sets,
such as contrast sets. Contrast sets consist of perturbed instances of data
that have very minor, but meaningful, changes to the input that alter the gold
label, revealing how models can learn superficial patterns in the training data
rather than learning more sophisticated language nuances. As an example, the
ELECTRA-small language model achieves nearly 90% accuracy on an SNLI dataset
but drops to 75% when tested on an out-of-distribution contrast set. The
research performed in this study explores how a language models' robustness can
be improved by exposing it to small amounts of more complex contrast sets
during training to help it better learn language patterns. With this approach,
the model regains performance and achieves nearly 90% accuracy on contrast
sets, highlighting the importance of diverse and challenging training data.

摘要：大型预训练语言模型在自然语言推理 (NLI) 任务的标准数据集上表现出高性能。不幸的是，这些评估可能会产生误导，因为尽管这些模型在分布内数据上表现良好，但它们在分布外测试集（例如对比集）上的表现很差。对比集由数据的扰动实例组成，这些实例对输入进行了非常细微但有意义的更改，从而改变了黄金标签，揭示了模型如何学习训练数据中的表面模式，而不是学习更复杂的语言细微差别。例如，ELECTRA-small 语言模型在 SNLI 数据集上实现了近 90% 的准确率，但在分布外对比集上测试时降至 75%。本研究中进行的研究探讨了如何通过在训练期间将其暴露于少量更复杂的对比集中来提高语言模型的鲁棒性，以帮助它更好地学习语言模式。通过这种方法，模型重新获得性能，并在对比集上实现了近 90% 的准确率，突出了多样化和具有挑战性的训练数据的重要性。

##### **From thermodynamics to protein design: Diffusion models for biomolecule generation towards autonomous protein engineering**
2501.02680v1 by Wen-ran Li, Xavier F. Cadet, David Medina-Ortiz, Mehdi D. Davari, Ramanathan Sowdhamini, Cedric Damour, Yu Li, Alain Miranville, Frederic Cadet

Protein design with desirable properties has been a significant challenge for
many decades. Generative artificial intelligence is a promising approach and
has achieved great success in various protein generation tasks. Notably,
diffusion models stand out for their robust mathematical foundations and
impressive generative capabilities, offering unique advantages in certain
applications such as protein design. In this review, we first give the
definition and characteristics of diffusion models and then focus on two
strategies: Denoising Diffusion Probabilistic Models and Score-based Generative
Models, where DDPM is the discrete form of SGM. Furthermore, we discuss their
applications in protein design, peptide generation, drug discovery, and
protein-ligand interaction. Finally, we outline the future perspectives of
diffusion models to advance autonomous protein design and engineering. The E(3)
group consists of all rotations, reflections, and translations in
three-dimensions. The equivariance on the E(3) group can keep the physical
stability of the frame of each amino acid as much as possible, and we reflect
on how to keep the diffusion model E(3) equivariant for protein generation.

摘要：蛋白設計具有理想的特性，這幾十年來一直是項重大的挑戰。生成式人工智慧是一種有前景的方法，在各種蛋白質生成任務中已取得巨大的成功。值得注意的是，擴散模型以其穩健的數學基礎和令人印象深刻的生成能力而脫穎而出，在蛋白質設計等特定應用中提供了獨特的優勢。在本篇評論中，我們首先給出擴散模型的定義和特徵，然後專注於兩種策略：去噪擴散概率模型和基於分數的生成模型，其中 DDPM 是 SGM 的離散形式。此外，我們討論了它們在蛋白質設計、胜肽生成、藥物發現和蛋白質-配體相互作用中的應用。最後，我們概述了擴散模型在推進自主蛋白質設計和工程方面的未來前景。E(3) 群組包含三維空間中的所有旋轉、反射和平移。E(3) 群組上的等變性可以盡可能保持每個胺基酸框架的物理穩定性，我們思考如何保持擴散模型 E(3) 等變性以進行蛋白質生成。

##### **Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?**
2501.02669v1 by Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, Sanjeev Arora

While Vision Language Models (VLMs) are impressive in tasks such as visual
question answering (VQA) and image captioning, their ability to apply
multi-step reasoning to images has lagged, giving rise to perceptions of
modality imbalance or brittleness. Towards systematic study of such issues, we
introduce a synthetic framework for assessing the ability of VLMs to perform
algorithmic visual reasoning (AVR), comprising three tasks: Table Readout, Grid
Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and
HARD, and even the SIMPLE versions are difficult for frontier VLMs. We seek
strategies for training on the SIMPLE version of the tasks that improve
performance on the corresponding HARD task, i.e., S2H generalization. This
synthetic framework, where each task also has a text-only version, allows a
quantification of the modality imbalance, and how it is impacted by training
strategy. Ablations highlight the importance of explicit image-to-text
conversion in promoting S2H generalization when using auto-regressive training.
We also report results of mechanistic study of this phenomenon, including a
measure of gradient alignment that seems to identify training strategies that
promote better S2H generalization.

摘要：雖然視覺語言模型 (VLM) 在視覺問答 (VQA) 和影像字幕等任務中表現出色，但它們將多步驟推理應用於影像的能力卻落後，導致人們認為它們在模態上失衡或脆弱。為了系統性地研究此類問題，我們引入了一個合成框架，用於評估 VLM 執行演算法視覺推理 (AVR) 的能力，其中包含三項任務：表格讀取、網格導航和視覺類比。每個任務都有兩個難度等級，簡單和困難，即使是簡單版本對前沿 VLM 來說也很困難。我們尋求在任務的簡單版本上進行訓練的策略，以提升對應困難任務的效能，即 S2H 廣義化。此合成框架（其中每個任務也有純文字版本）允許量化模態失衡，以及訓練策略如何影響它。消融實驗突顯了在使用自迴歸訓練時，明確的影像轉文字轉換在促進 S2H 廣義化中的重要性。我們也報告了對此現象的機制研究結果，包括一種梯度對齊測量，它似乎可以找出促進 S2H 廣義化的訓練策略。

##### **Multi-Aggregator Time-Warping Heterogeneous Graph Neural Network for Personalized Micro-Video Recommendation**
2501.02666v1 by Jinkun Han, Wei Li, Xhipeng Cai, Yingshu Li

Micro-video recommendation is attracting global attention and becoming a
popular daily service for people of all ages. Recently, Graph Neural
Networks-based micro-video recommendation has displayed performance improvement
for many kinds of recommendation tasks. However, the existing works fail to
fully consider the characteristics of micro-videos, such as the high timeliness
of news nature micro-video recommendation and sequential interactions of
frequently changed interests. In this paper, a novel Multi-aggregator
Time-warping Heterogeneous Graph Neural Network (MTHGNN) is proposed for
personalized news nature micro-video recommendation based on sequential
sessions, where characteristics of micro-videos are comprehensively studied,
users' preference is mined via multi-aggregator, the temporal and dynamic
changes of users' preference are captured, and timeliness is considered.
Through the comparison with the state-of-the-arts, the experimental results
validate the superiority of our MTHGNN model.

摘要：微影片推薦正吸引全球關注，並成為各年齡層民眾的熱門日常服務。近期，基於圖神經網路的微影片推薦在許多推薦任務中展現了效能提升。然而，現有研究未能充分考量微影片的特性，例如新聞性質微影片推薦的高度時效性，以及頻繁變動興趣的序列互動。本文提出一個新穎的多聚合器時序變形異質圖神經網路（MTHGNN），用於基於序列式瀏覽記錄的個人化新聞性質微影片推薦。其中全面探討微影片的特性，透過多聚合器挖掘使用者的偏好，捕捉使用者偏好的時序動態變化，並考量時效性。透過與現有技術的比較，實驗結果驗證了我們 MTHGNN 模型的優越性。

##### **Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence Benchmarks**
2501.02654v1 by Yang Wang, Chenghua Lin

vulnerability of deep learning models to adversarial attacks. While various
defence mechanisms have been proposed, there is a lack of comprehensive
benchmarks that evaluate these defences across diverse datasets, models, and
tasks. In this work, we address this gap by presenting an extensive benchmark
for textual adversarial defence that significantly expands upon previous work.
Our benchmark incorporates a wide range of datasets, evaluates state-of-the-art
defence mechanisms, and extends the assessment to include critical tasks such
as single-sentence classification, similarity and paraphrase identification,
natural language inference, and commonsense reasoning. This work not only
serves as a valuable resource for researchers and practitioners in the field of
adversarial robustness but also identifies key areas for future research in
textual adversarial defence. By establishing a new standard for benchmarking in
this domain, we aim to accelerate progress towards more robust and reliable
natural language processing systems.

摘要：深度學習模型對抗攻擊的脆弱性。雖然已經提出各種防禦機制，但缺乏全面的基準來評估這些防禦在不同資料集、模型和任務中的表現。在這項工作中，我們透過提出一個廣泛的文字對抗防禦基準來解決這個差距，大幅擴展了先前的研究。我們的基準包含了廣泛的資料集，評估最先進的防禦機制，並將評估擴展到包括單句分類、相似度和同義詞辨識、自然語言推論和常識推理等關鍵任務。這項工作不僅作為對抗穩健性領域的研究人員和從業人員的寶貴資源，還找出文字對抗防禦未來研究的主要領域。透過在這個領域建立新的基準標準，我們旨在加速進展，朝向更強大且可靠的自然語言處理系統。

##### **Tighnari: Multi-modal Plant Species Prediction Based on Hierarchical Cross-Attention Using Graph-Based and Vision Backbone-Extracted Features**
2501.02649v1 by Haixu Liu, Penghao Jiang, Zerui Tao, Muyan Wan, Qiuzhuang Sun

Predicting plant species composition in specific spatiotemporal contexts
plays an important role in biodiversity management and conservation, as well as
in improving species identification tools. Our work utilizes 88,987 plant
survey records conducted in specific spatiotemporal contexts across Europe. We
also use the corresponding satellite images, time series data, climate time
series, and other rasterized environmental data such as land cover, human
footprint, bioclimatic, and soil variables as training data to train the model
to predict the outcomes of 4,716 plant surveys. We propose a feature
construction and result correction method based on the graph structure. Through
comparative experiments, we select the best-performing backbone networks for
feature extraction in both temporal and image modalities. In this process, we
built a backbone network based on the Swin-Transformer Block for extracting
temporal Cubes features. We then design a hierarchical cross-attention
mechanism capable of robustly fusing features from multiple modalities. During
training, we adopt a 10-fold cross-fusion method based on fine-tuning and use a
Threshold Top-K method for post-processing. Ablation experiments demonstrate
the improvements in model performance brought by our proposed solution
pipeline.

摘要：預測特定時空背景下的植物物種組成在生物多樣性管理和保育中扮演著重要的角色，也能改善物種識別工具。我們的研究利用了在歐洲特定時空背景下進行的 88,987 筆植物調查記錄。我們也使用對應的衛星影像、時間序列資料、氣候時間序列，以及其他光柵化的環境資料，例如土地覆蓋、人類足跡、生物氣候和土壤變數，作為訓練資料來訓練模型，預測 4,716 筆植物調查的結果。我們提出一個基於圖形結構的特徵建構和結果修正方法。透過比較實驗，我們選出在時間和影像模式中特徵萃取表現最佳的主幹網路。在此過程中，我們建構了一個基於 Swin-Transformer Block 的主幹網路，用於萃取時間立方體特徵。接著，我們設計了一個階層式交叉注意力機制，能夠穩健地融合來自多種模式的特徵。在訓練期間，我們採用基於微調的 10 倍交叉融合方法，並使用閾值 Top-K 方法進行後處理。消融實驗證明了我們提出的解決方案管道改善了模型效能。

##### **Representation Learning of Lab Values via Masked AutoEncoder**
2501.02648v1 by David Restrepo, Chenwei Wu, Yueran Jia, Jaden K. Sun, Jack Gallifant, Catherine G. Bielick, Yugang Jia, Leo A. Celi

Accurate imputation of missing laboratory values in electronic health records
(EHRs) is critical to enable robust clinical predictions and reduce biases in
AI systems in healthcare. Existing methods, such as variational autoencoders
(VAEs) and decision tree-based approaches such as XGBoost, struggle to model
the complex temporal and contextual dependencies in EHR data, mainly in
underrepresented groups. In this work, we propose Lab-MAE, a novel
transformer-based masked autoencoder framework that leverages self-supervised
learning for the imputation of continuous sequential lab values. Lab-MAE
introduces a structured encoding scheme that jointly models laboratory test
values and their corresponding timestamps, enabling explicit capturing temporal
dependencies. Empirical evaluation on the MIMIC-IV dataset demonstrates that
Lab-MAE significantly outperforms the state-of-the-art baselines such as
XGBoost across multiple metrics, including root mean square error (RMSE),
R-squared (R2), and Wasserstein distance (WD). Notably, Lab-MAE achieves
equitable performance across demographic groups of patients, advancing fairness
in clinical predictions. We further investigate the role of follow-up
laboratory values as potential shortcut features, revealing Lab-MAE's
robustness in scenarios where such data is unavailable. The findings suggest
that our transformer-based architecture, adapted to the characteristics of the
EHR data, offers a foundation model for more accurate and fair clinical
imputation models. In addition, we measure and compare the carbon footprint of
Lab-MAE with the baseline XGBoost model, highlighting its environmental
requirements.

摘要：<paragraph>電子健康紀錄 (EHR) 中遺失的實驗室值的準確估算對於支援穩健的臨床預測和減少醫療保健中 AI 系統的偏差至關重要。現有的方法，例如變異自動編碼器 (VAE) 和基於決策樹的方法（例如 XGBoost），難以建模 EHR 資料中複雜的時間和背景依賴性，特別是在代表性不足的群組中。在這項工作中，我們提出 Lab-MAE，一個新穎的基於Transformer的遮罩自動編碼器框架，它利用自我監督學習來估算連續的序列實驗室值。Lab-MAE 引入了一個結構化編碼方案，它共同建模實驗室測試值及其對應的時間戳，從而能夠明確捕捉時間依賴性。在 MIMIC-IV 資料集上的經驗評估表明，Lab-MAE 在包括均方根誤差 (RMSE)、R 平方 (R2) 和 Wasserstein 距離 (WD) 在內的多項指標上顯著優於 XGBoost 等最先進的基準。值得注意的是，Lab-MAE 在患者的人口統計群組中實現了公平的表現，促進了臨床預測的公平性。我們進一步探討了後續實驗室值作為潛在捷徑特徵的作用，揭示了 Lab-MAE 在此類資料不可用的情況下的穩健性。研究結果表明，我們基於Transformer的架構適應了 EHR 資料的特徵，為更準確和公平的臨床估算模型提供了基礎模型。此外，我們測量並比較了 Lab-MAE 與基準 XGBoost 模型的碳足跡，突出了其環境要求。</paragraph>

##### **Prune or Retrain: Optimizing the Vocabulary of Multilingual Models for Estonian**
2501.02631v1 by Aleksei Dorkin, Taido Purason, Kairit Sirts

Adapting multilingual language models to specific languages can enhance both
their efficiency and performance. In this study, we explore how modifying the
vocabulary of a multilingual encoder model to better suit the Estonian language
affects its downstream performance on the Named Entity Recognition (NER) task.
The motivations for adjusting the vocabulary are twofold: practical benefits
affecting the computational cost, such as reducing the input sequence length
and the model size, and performance enhancements by tailoring the vocabulary to
the particular language. We evaluate the effectiveness of two vocabulary
adaptation approaches -- retraining the tokenizer and pruning unused tokens --
and assess their impact on the model's performance, particularly after
continual training. While retraining the tokenizer degraded the performance of
the NER task, suggesting that longer embedding tuning might be needed, we
observed no negative effects on pruning.

摘要：調整多語言語言模型以適應特定語言，可以提升其效率和效能。在這項研究中，我們探討修改多語言編碼器模型的詞彙，以更好地適應愛沙尼亞語，如何影響其在命名實體辨識 (NER) 任務的下游效能。調整詞彙的動機有兩個：影響運算成本的實際效益，例如減少輸入序列長度和模型大小，以及透過調整詞彙以適應特定語言來提升效能。我們評估兩種詞彙調整方法的有效性——重新訓練標記器和修剪未使用的標記——並評估它們對模型效能的影響，特別是在持續訓練之後。儘管重新訓練標記器會降低 NER 任務的效能，表示可能需要更長的嵌入式調整，但我們觀察到修剪沒有負面影響。

##### **Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense**
2501.02629v1 by Yang Ouyang, Hengrui Gu, Shuhang Lin, Wenyue Hua, Jie Peng, Bhavya Kailkhura, Tianlong Chen, Kaixiong Zhou

As large language models (LLMs) are increasingly deployed in diverse
applications, including chatbot assistants and code generation, aligning their
behavior with safety and ethical standards has become paramount. However,
jailbreak attacks, which exploit vulnerabilities to elicit unintended or
harmful outputs, threaten LLMs' safety significantly. In this paper, we
introduce Layer-AdvPatcher, a novel methodology designed to defend against
jailbreak attacks by utilizing an unlearning strategy to patch specific layers
within LLMs through self-augmented datasets. Our insight is that certain
layer(s), tend to produce affirmative tokens when faced with harmful prompts.
By identifying these layers and adversarially exposing them to generate more
harmful data, one can understand their inherent and diverse vulnerabilities to
attacks. With these exposures, we then "unlearn" these issues, reducing the
impact of affirmative tokens and hence minimizing jailbreak risks while keeping
the model's responses to safe queries intact. We conduct extensive experiments
on two models, four benchmark datasets, and multiple state-of-the-art jailbreak
benchmarks to demonstrate the efficacy of our approach. Results indicate that
our framework reduces the harmfulness and attack success rate of jailbreak
attacks without compromising utility for benign queries compared to recent
defense methods.

摘要：隨著大型語言模型（LLM）在各種應用程式中越來越廣泛地部署，包括聊天機器人助理和程式碼產生，使其行為與安全和倫理標準保持一致已變得至關重要。然而，利用漏洞引發意外或有害輸出的越獄攻擊，嚴重威脅到 LLM 的安全性。在本文中，我們介紹了 Layer-AdvPatcher，這是一種新穎的方法，旨在通過利用非學習策略來修補 LLM 中的特定層，從而防禦越獄攻擊通過自我擴充的資料集。我們的見解是，某些層在面對有害提示時往往會產生肯定的標記。通過識別這些層並對抗性地讓它們產生更多有害資料，人們可以了解它們固有的和對攻擊的多種漏洞。有了這些曝光，我們接著「忘記」這些問題，減少肯定標記的影響，從而最大程度地降低越獄風險，同時保持模型對安全查詢的回應不變。我們對兩個模型、四個基準資料集和多個最先進的越獄基準進行了廣泛的實驗，以證明我們方法的有效性。結果表明，與最近的防禦方法相比，我們的框架降低了越獄攻擊的危害性和攻擊成功率，同時不損害良性查詢的效用。

##### **Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets**
2501.02628v1 by Mahmoud Jahanshahi, Audris Mockus

A critical part of creating code suggestion systems is the pre-training of
Large Language Models on vast amounts of source code and natural language text,
often of questionable origin or quality. This may contribute to the presence of
bugs and vulnerabilities in code generated by LLMs. While efforts to identify
bugs at or after code generation exist, it is preferable to pre-train or
fine-tune LLMs on curated, high-quality, and compliant datasets. The need for
vast amounts of training data necessitates that such curation be automated,
minimizing human intervention.
  We propose an automated source code autocuration technique that leverages the
complete version history of open-source software projects to improve the
quality of training data. This approach leverages the version history of all
OSS projects to identify training data samples that have been modified or have
undergone changes in at least one OSS project, and pinpoint a subset of samples
that include fixes for bugs or vulnerabilities. We evaluate this method using
The Stack v2 dataset, and find that 17% of the code versions in the dataset
have newer versions, with 17% of those representing bug fixes, including 2.36%
addressing known CVEs. The deduplicated version of Stack v2 still includes
blobs vulnerable to 6,947 known CVEs. Furthermore, 58% of the blobs in the
dataset were never modified after creation, suggesting they likely represent
software with minimal or no use. Misidentified blob origins present an
additional challenge, as they lead to the inclusion of non-permissively
licensed code, raising serious compliance concerns.
  By addressing these issues, the training of new models can avoid perpetuating
buggy code patterns or license violations. We expect our results to inspire
process improvements for automated data curation, with the potential to enhance
the reliability of outputs generated by AI tools.

摘要：建立程式碼建議系統的關鍵部分是針對大量來源程式碼和自然語言文字進行大型語言模型的預訓練，這些文字的來源或品質通常有疑慮。這可能會導致 LLM 所產生的程式碼出現錯誤和漏洞。雖然已有人努力在程式碼產生期間或產生後找出錯誤，但最好是在經過整理、品質高且合規的資料集上預先訓練或微調 LLM。由於訓練資料量龐大，因此必須自動化這種整理，將人為介入降到最低。
  我們提出了一種自動化來源程式碼自動整理技術，利用開源軟體專案的完整版本歷程，以提升訓練資料的品質。這種方法利用所有 OSS 專案的版本歷程，找出已修改或至少在一個 OSS 專案中歷經變更的訓練資料範例，並找出包含錯誤或漏洞修正的範例子集。我們使用 Stack v2 資料集評估此方法，發現資料集中 17% 的程式碼版本有較新的版本，其中 17% 代表錯誤修正，包括 2.36% 解決已知的 CVE。Stack v2 的重複資料刪除版本仍包含容易受到 6,947 個已知 CVE 影響的 blob。此外，資料集中 58% 的 blob 在建立後從未修改過，這表示它們可能代表使用程度很低或完全沒在使用的軟體。錯誤辨識的 blob 來源會帶來額外挑戰，因為它們會導致包含非許可授權的程式碼，引發嚴重的合規問題。
  透過解決這些問題，新模型的訓練可以避免傳遞錯誤的程式碼模式或授權違規。我們預期我們的結果能激勵自動化資料整理的流程改善，進而提升 AI 工具產生輸出的可靠性。

##### **LLMs Help Alleviate the Cross-Subject Variability in Brain Signal and Language Alignment**
2501.02621v1 by Yifei Liu, Hengwei Ye, Shuhang Li

Decoding human activity from EEG signals has long been a popular research
topic. While recent studies have increasingly shifted focus from single-subject
to cross-subject analysis, few have explored the model's ability to perform
zero-shot predictions on EEG signals from previously unseen subjects. This
research aims to investigate whether deep learning methods can capture
subject-independent semantic information inherent in human EEG signals. Such
insights are crucial for Brain-Computer Interfaces (BCI) because, on one hand,
they demonstrate the model's robustness against subject-specific temporal
biases, and on the other, they significantly enhance the generalizability of
downstream tasks. We employ Large Language Models (LLMs) as denoising agents to
extract subject-independent semantic features from noisy EEG signals.
Experimental results, including ablation studies, highlight the pivotal role of
LLMs in decoding subject-independent semantic information from noisy EEG data.
We hope our findings will contribute to advancing BCI research and assist both
academia and industry in applying EEG signals to a broader range of
applications.

摘要：人類活動的 EEG 訊號解碼一直是熱門的研究主題。雖然最近的研究已逐步將重點從單一受試者轉移到跨受試者分析，但鮮少探討模型在從先前未見受試者取得的 EEG 訊號中執行零次學習預測的能力。本研究旨在探討深度學習方法是否能擷取人類 EEG 訊號中固有的受試者無關語義資訊。此類見解對於腦電腦介面 (BCI) 至關重要，原因在於一方面，它們展現模型對於受試者特定時間偏誤的穩健性，另一方面，它們大幅提升下游任務的概括性。我們採用大型語言模型 (LLM) 作為去雜訊代理，從雜訊 EEG 訊號中萃取受試者無關語義特徵。包含消融研究在內的實驗結果突顯出 LLM 在從雜訊 EEG 資料中解碼受試者無關語義資訊的關鍵角色。我們希望我們的發現有助於推進 BCI 研究，並協助學術界和產業將 EEG 訊號應用於更廣泛的應用範圍。

##### **TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms**
2501.02600v1 by Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Esha Choukse, Haoran Qiu, Rodrigo Fonseca, Josep Torrellas, Ricardo Bianchini

The rising demand for generative large language models (LLMs) poses
challenges for thermal and power management in cloud datacenters. Traditional
techniques often are inadequate for LLM inference due to the fine-grained,
millisecond-scale execution phases, each with distinct performance, thermal,
and power profiles. Additionally, LLM inference workloads are sensitive to
various configuration parameters (e.g., model parallelism, size, and
quantization) that involve trade-offs between performance, temperature, power,
and output quality. Moreover, clouds often co-locate SaaS and IaaS workloads,
each with different levels of visibility and flexibility. We propose TAPAS, a
thermal- and power-aware framework designed for LLM inference clusters in the
cloud. TAPAS enhances cooling and power oversubscription capabilities, reducing
the total cost of ownership (TCO) while effectively handling emergencies (e.g.,
cooling and power failures). The system leverages historical temperature and
power data, along with the adaptability of SaaS workloads, to: (1) efficiently
place new GPU workload VMs within cooling and power constraints, (2) route LLM
inference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load
spikes and emergency situations. Our evaluation on a large GPU cluster
demonstrates significant reductions in thermal and power throttling events,
boosting system efficiency.

摘要：生成式大型語言模型 (LLM) 的需求不斷上升，對雲端資料中心的熱能和電源管理提出了挑戰。傳統的技術通常不足以用於 LLM 推論，因為執行階段是細緻且以毫秒為單位的，每個階段都有不同的效能、熱能和電源設定檔。此外，LLM 推論工作負載對各種組態參數很敏感（例如，模型並行、大小和量化），這些參數涉及效能、溫度、電源和輸出品質之間的權衡。此外，雲端通常會將 SaaS 和 IaaS 工作負載配置在同一個位置，而每個工作負載的能見度和彈性都不同。我們提出 TAPAS，一個專為雲端中的 LLM 推論叢集設計的熱能和電源感知架構。TAPAS 增強了冷卻和電源超額預訂功能，降低了總持有成本 (TCO)，同時有效地處理緊急情況（例如，冷卻和電源故障）。系統利用歷史溫度和電源資料，以及 SaaS 工作負載的適應性，來：(1) 有效地將新的 GPU 工作負載 VM 放置在冷卻和電源限制內，(2) 路由 LLM 推論請求到各個 SaaS VM，以及 (3) 重新組態 SaaS VM 以管理負載高峰和緊急情況。我們在大型 GPU 叢集上的評估顯示，熱能和電源限制事件大幅減少，提升了系統效率。

##### **Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models**
2501.02599v1 by Jalisha Jashim Era, Bidyarthi Paul, Tahmid Sattar Aothoi, Mirazur Rahman Zim, Faisal Muhammad Shah

Mathematical word problems (MWPs) involve the task of converting textual
descriptions into mathematical equations. This poses a significant challenge in
natural language processing, particularly for low-resource languages such as
Bengali. This paper addresses this challenge by developing an innovative
approach to solving Bengali MWPs using transformer-based models, including
Basic Transformer, mT5, BanglaT5, and mBART50. To support this effort, the
"PatiGonit" dataset was introduced, containing 10,000 Bengali math problems,
and these models were fine-tuned to translate the word problems into equations
accurately. The evaluation revealed that the mT5 model achieved the highest
accuracy of 97.30%, demonstrating the effectiveness of transformer models in
this domain. This research marks a significant step forward in Bengali natural
language processing, offering valuable methodologies and resources for
educational AI tools. By improving math education, it also supports the
development of advanced problem-solving skills for Bengali-speaking students.

摘要：數學文字題（MWP）涉及將文字描述轉換為數學方程式的任務。這對自然語言處理構成了重大挑戰，特別是對於孟加拉語等低資源語言。本文通過開發一種使用基於Transformer的模型（包括 Basic Transformer、mT5、BanglaT5 和 mBART50）解決孟加拉語 MWP 的創新方法來應對這一挑戰。為了支持這項工作，引入了「PatiGonit」數據集，其中包含 10,000 個孟加拉語數學問題，並且對這些模型進行了微調，以準確地將文字題轉換為方程式。評估結果表明，mT5 模型達到了 97.30% 的最高準確度，證明了Transformer模型在這個領域的有效性。這項研究標誌著孟加拉語自然語言處理向前邁出了重要一步，為教育 AI 工具提供了有價值的方法和資源。通過改善數學教育，它也支持孟加拉語學生培養高級問題解決能力。

##### **GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation**
2501.02598v1 by Iustin Sîrbu, Iulia-Renata Sîrbu, Jasmina Bogojeska, Traian Rebedea

Medical imaging is crucial for diagnosing, monitoring, and treating medical
conditions. The medical reports of radiology images are the primary medium
through which medical professionals attest their findings, but their writing is
time consuming and requires specialized clinical expertise. The automated
generation of radiography reports has thus the potential to improve and
standardize patient care and significantly reduce clinicians workload. Through
our work, we have designed and evaluated an end-to-end transformer-based method
to generate accurate and factually complete radiology reports for X-ray images.
Additionally, we are the first to introduce curriculum learning for end-to-end
transformers in medical imaging and demonstrate its impact in obtaining
improved performance. The experiments have been conducted using the
MIMIC-CXR-JPG database, the largest available chest X-ray dataset. The results
obtained are comparable with the current state-of-the-art on the natural
language generation (NLG) metrics BLEU and ROUGE-L, while setting new
state-of-the-art results on F1 examples-averaged, F1-macro and F1-micro metrics
for clinical accuracy and on the METEOR metric widely used for NLG.

摘要：醫療影像對於診斷、監控和治療醫療狀況至關重要。放射影像的醫療報告是醫護人員證明其發現的主要媒介，但撰寫這些報告非常耗時，且需要專業的臨床專業知識。因此，自動產生放射科報告有可能改善和標準化患者照護，並大幅減少臨床醫師的工作量。透過我們的努力，我們設計並評估了一種端對端基於轉換器的技術，以產生準確且事實上完整的 X 光影像放射科報告。此外，我們率先在醫學影像中為端對端轉換器引入課程學習，並展示了其在獲得改善效能方面的影響。這些實驗已使用 MIMIC-CXR-JPG 資料庫進行，這是最大的可用胸部 X 光影像資料集。所獲得的結果可與自然語言產生 (NLG) 指標 BLEU 和 ROUGE-L 中的現有技術相媲美，同時在 F1 範例平均、F1 巨觀和 F1 微觀指標上設定新的技術水準，以評估臨床準確性，以及在廣泛用於 NLG 的 METEOR 指標上設定新的技術水準。

##### **Evolving Skeletons: Motion Dynamics in Action Recognition**
2501.02593v1 by Jushang Qiu, Lei Wang

Skeleton-based action recognition has gained significant attention for its
ability to efficiently represent spatiotemporal information in a lightweight
format. Most existing approaches use graph-based models to process skeleton
sequences, where each pose is represented as a skeletal graph structured around
human physical connectivity. Among these, the Spatiotemporal Graph
Convolutional Network (ST-GCN) has become a widely used framework.
Alternatively, hypergraph-based models, such as the Hyperformer, capture
higher-order correlations, offering a more expressive representation of complex
joint interactions. A recent advancement, termed Taylor Videos, introduces
motion-enhanced skeleton sequences by embedding motion concepts, providing a
fresh perspective on interpreting human actions in skeleton-based action
recognition. In this paper, we conduct a comprehensive evaluation of both
traditional skeleton sequences and Taylor-transformed skeletons using ST-GCN
and Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal
graph and hypergraph representations, analyzing static poses against
motion-injected poses. Our findings highlight the strengths and limitations of
Taylor-transformed skeletons, demonstrating their potential to enhance motion
dynamics while exposing current challenges in fully using their benefits. This
study underscores the need for innovative skeletal modelling techniques to
effectively handle motion-rich data and advance the field of action
recognition.

摘要：基於骨架的動作辨識因其能有效地以輕量化的格式呈現時空資訊而備受矚目。大多數現有方法使用基於圖形模型來處理骨架序列，其中每個姿勢都表示為圍繞人體物理連接性的骨架圖形。其中，時空圖形卷積網路 (ST-GCN) 已成為廣泛使用的框架。或者，基於超圖形的模型，例如 Hyperformer，能擷取高階關聯性，提供更具表現力的複雜關節互動表示。最近的進展，稱為 Taylor 影片，透過嵌入動作概念來引入動作增強的骨架序列，為基於骨架的動作辨識中的人類動作詮釋提供了新的觀點。在本文中，我們對傳統骨架序列和使用 ST-GCN 和 Hyperformer 模型在 NTU-60 和 NTU-120 資料集上進行 Taylor 轉換的骨架進行了全面的評估。我們比較骨架圖形和超圖形表示，分析靜態姿勢和動作注入姿勢。我們的研究結果突出了 Taylor 轉換骨架的優點和限制，證明了它們增強動作動態的潛力，同時也揭露了在充分利用其優勢方面當前的挑戰。這項研究強調了創新骨架建模技術的需求，以有效處理豐富動作的資料並推動動作辨識領域的進步。

##### **Efficient Architectures for High Resolution Vision-Language Models**
2501.02584v1 by Miguel Carvalho, Bruno Martins

Vision-Language Models (VLMs) have recently experienced significant
advancements. However, challenges persist in the accurate recognition of fine
details within high resolution images, which limits performance in multiple
tasks. This work introduces Pheye, a novel architecture that efficiently
processes high-resolution images while training fewer parameters than similarly
sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong
performance, particularly in tasks that demand fine-grained image understanding
and/or the handling of scene-text.

摘要：視覺語言模型 (VLM) 近期經歷了顯著的進展。然而，在高解析度影像中精確辨識精細細節的挑戰依然存在，這限制了多項任務的執行效能。本研究引入了 Pheye，一種創新的架構，可在訓練參數少於同等規模 VLM 的情況下，有效率地處理高解析度影像。值得注意的是，Pheye 在維持強大效能的同時，達到了高效率，特別是在需要細緻影像理解和/或處理場景文字的任務中。

##### **LeetDecoding: A PyTorch Library for Exponentially Decaying Causal Linear Attention with CUDA Implementations**
2501.02573v1 by Jiaping Wang, Simiao Zhang, Qiao-Chu He, Yifan Chen

The machine learning and data science community has made significant while
dispersive progress in accelerating transformer-based large language models
(LLMs), and one promising approach is to replace the original causal attention
in a generative pre-trained transformer (GPT) with \emph{exponentially decaying
causal linear attention}. In this paper, we present LeetDecoding, which is the
first Python package that provides a large set of computation routines for this
fundamental operator. The launch of LeetDecoding was motivated by the current
lack of (1) clear understanding of the complexity regarding this operator, (2)
a comprehensive collection of existing computation methods (usually spread in
seemingly unrelated fields), and (3) CUDA implementations for fast inference on
GPU. LeetDecoding's design is easy to integrate with existing linear-attention
LLMs, and allows for researchers to benchmark and evaluate new computation
methods for exponentially decaying causal linear attention. The usage of
LeetDecoding does not require any knowledge of GPU programming and the
underlying complexity analysis, intentionally making LeetDecoding accessible to
LLM practitioners. The source code of LeetDecoding is provided at
\href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{this
GitHub repository}, and users can simply install LeetDecoding by the command
\texttt{pip install leet-decoding}.

摘要：機器學習和資料科學社群在加速以 Transformer 為基礎的大型語言模型 (LLM) 方面取得了顯著但分散的進展，其中一種有前途的方法是將生成式預訓練 Transformer (GPT) 中的原始因果注意力替換為「指數衰減因果線性注意力」。在本文中，我們介紹 LeetDecoding，這是第一個 Python 套件，為這個基本運算子提供了一組大量的運算常式。LeetDecoding 的推出是基於目前 (1) 缺乏對這個運算子複雜度的清楚理解、(2) 現有運算方法的全面收集（通常分散在看似不相關的領域中），以及 (3) CUDA 實作，以在 GPU 上進行快速推論。LeetDecoding 的設計很容易與現有的線性注意力 LLM 整合，並允許研究人員對指數衰減因果線性注意力的新運算方法進行基準測試和評估。使用 LeetDecoding 不需要任何 GPU 程式設計和基礎複雜度分析的知識，有意地讓 LLM 實務人員可以使用 LeetDecoding。LeetDecoding 的原始程式碼提供於 \href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{這個 GitHub 儲存庫}，使用者可以透過指令 \texttt{pip install leet-decoding} 輕鬆安裝 LeetDecoding。

##### **Energy Optimization of Multi-task DNN Inference in MEC-assisted XR Devices: A Lyapunov-Guided Reinforcement Learning Approach**
2501.02572v1 by Yanzan Sun, Jiacheng Qiu, Guangjin Pan, Shugong Xu, Shunqing Zhang, Xiaoyun Wang, Shuangfeng Han

Extended reality (XR), blending virtual and real worlds, is a key application
of future networks. While AI advancements enhance XR capabilities, they also
impose significant computational and energy challenges on lightweight XR
devices. In this paper, we developed a distributed queue model for multi-task
DNN inference, addressing issues of resource competition and queue coupling. In
response to the challenges posed by the high energy consumption and limited
resources of XR devices, we designed a dual time-scale joint optimization
strategy for model partitioning and resource allocation, formulated as a
bi-level optimization problem. This strategy aims to minimize the total energy
consumption of XR devices while ensuring queue stability and adhering to
computational and communication resource constraints. To tackle this problem,
we devised a Lyapunov-guided Proximal Policy Optimization algorithm, named
LyaPPO. Numerical results demonstrate that the LyaPPO algorithm outperforms the
baselines, achieving energy conservation of 24.79% to 46.14% under varying
resource capacities. Specifically, the proposed algorithm reduces the energy
consumption of XR devices by 24.29% to 56.62% compared to baseline algorithms.

摘要：擴增實境（XR）融合虛擬與真實世界，是未來網路的主要應用。儘管 AI 的進步增強了 XR 的功能，但它們也對輕量級 XR 裝置造成顯著的運算和能源挑戰。在本文中，我們開發了一個用於多任務 DNN 推論的分布式佇列模型，解決了資源競爭和佇列耦合的問題。針對 XR 裝置的高能耗和有限資源所帶來的挑戰，我們設計了一個用於模型分割和資源分配的雙時間尺度聯合最佳化策略，並制定為一個雙層最佳化問題。此策略旨在最小化 XR 裝置的總能耗，同時確保佇列穩定性並遵守運算和通訊資源限制。為了解決這個問題，我們設計了一個名為 LyaPPO 的 Lyapunov 引導近端策略最佳化演算法。數值結果證明，LyaPPO 演算法優於基準，在不同的資源容量下實現 24.79% 至 46.14% 的節能。具體來說，與基準演算法相比，所提出的演算法將 XR 裝置的能耗降低了 24.29% 至 56.62%。

##### **Decoding fMRI Data into Captions using Prefix Language Modeling**
2501.02570v1 by Vyacheslav Shen, Kassymzhomart Kunanbayev, Dae-Shik Kim

With the advancements in Large Language and Latent Diffusion models, brain
decoding has achieved remarkable results in recent years. The works on the NSD
dataset, with stimuli images from the COCO dataset, leverage the embeddings
from the CLIP model for image reconstruction and GIT for captioning. However,
the current captioning approach introduces the challenge of potential data
contamination given that the GIT model was trained on the COCO dataset. In this
work, we present an alternative method for decoding brain signals into image
captions by predicting a DINOv2 model's embedding of an image from the
corresponding fMRI signal and then providing its [CLS] token as the prefix to
the GPT-2 language model which decreases computational requirements
considerably. Additionally, instead of commonly used Linear Regression, we
explore 3D Convolutional Neural Network mapping of fMRI signals to image
embedding space for better accounting positional information of voxels.

摘要：隨著大型語言和潛在擴散模型的進步，大腦解碼在近年來取得了顯著的成果。NSD 資料集的研究，使用來自 COCO 資料集的刺激圖像，利用 CLIP 模型的嵌入進行影像重建和 GIT 進行標題說明。然而，目前的標題說明方法引入了潛在資料污染的挑戰，因為 GIT 模型是在 COCO 資料集上訓練的。在這項工作中，我們提出了一種替代方法，將大腦訊號解碼為影像標題，方法是根據對應的 fMRI 訊號預測 DINOv2 模型的影像嵌入，然後將其 [CLS] 標記作為 GPT-2 語言模型的前綴，這顯著降低了運算需求。此外，我們探索了 3D 卷積神經網路將 fMRI 訊號對應到影像嵌入空間，以更好地說明體素的位置資訊，而不是常用的線性回歸。

##### **Balanced Multi-view Clustering**
2501.02564v1 by Zhenglai Li, Jun Wang, Chang Tang, Xinzhong Zhu, Wei Zhang, Xinwang Liu

Multi-view clustering (MvC) aims to integrate information from different
views to enhance the capability of the model in capturing the underlying data
structures. The widely used joint training paradigm in MvC is potentially not
fully leverage the multi-view information, since the imbalanced and
under-optimized view-specific features caused by the uniform learning objective
for all views. For instance, particular views with more discriminative
information could dominate the learning process in the joint training paradigm,
leading to other views being under-optimized. To alleviate this issue, we first
analyze the imbalanced phenomenon in the joint-training paradigm of multi-view
clustering from the perspective of gradient descent for each view-specific
feature extractor. Then, we propose a novel balanced multi-view clustering
(BMvC) method, which introduces a view-specific contrastive regularization
(VCR) to modulate the optimization of each view. Concretely, VCR preserves the
sample similarities captured from the joint features and view-specific ones
into the clustering distributions corresponding to view-specific features to
enhance the learning process of view-specific feature extractors. Additionally,
a theoretical analysis is provided to illustrate that VCR adaptively modulates
the magnitudes of gradients for updating the parameters of view-specific
feature extractors to achieve a balanced multi-view learning procedure. In such
a manner, BMvC achieves a better trade-off between the exploitation of
view-specific patterns and the exploration of view-invariance patterns to fully
learn the multi-view information for the clustering task. Finally, a set of
experiments are conducted to verify the superiority of the proposed method
compared with state-of-the-art approaches both on eight benchmark MvC datasets
and two spatially resolved transcriptomics datasets.

摘要：多視圖聚類 (MvC) 旨在整合來自不同視圖的資訊，以增強模型擷取基礎資料結構的能力。MvC 中廣泛使用的聯合訓練範例可能無法充分利用多視圖資訊，因為統一的學習目標導致所有視圖產生不平衡且最佳化不足的視圖特定特徵。例如，在聯合訓練範例中，具有較多區辨資訊的特定視圖可能會主導學習過程，導致其他視圖最佳化不足。為了緩解此問題，我們首先從每個視圖特定特徵萃取器的梯度下降角度分析多視圖聚類聯合訓練範例中的不平衡現象。然後，我們提出了一種新穎的平衡多視圖聚類 (BMvC) 方法，它引入了視圖特定對比正規化 (VCR) 來調節每個視圖的最佳化。具體來說，VCR 保留從聯合特徵和視圖特定特徵擷取的樣本相似性，並將其納入對應於視圖特定特徵的聚類分佈中，以增強視圖特定特徵萃取器的學習過程。此外，我們提供了理論分析來說明，VCR 能夠自適應地調節視圖特定特徵萃取器參數更新的梯度幅度，以達成平衡的多視圖學習程序。藉由這種方式，BMvC 在利用視圖特定模式和探索視圖不變模式之間取得了更好的折衷，以充分學習多視圖資訊，並進行聚類任務。最後，我們進行了一系列實驗，在八個基準 MvC 資料集和兩個空間解析轉錄體資料集上，驗證了所提出的方法優於現有技術。

##### **KM-UNet KAN Mamba UNet for medical image segmentation**
2501.02559v1 by Yibo Zhang

Medical image segmentation is a critical task in medical imaging analysis.
Traditional CNN-based methods struggle with modeling long-range dependencies,
while Transformer-based models, despite their success, suffer from quadratic
computational complexity. To address these limitations, we propose KM-UNet, a
novel U-shaped network architecture that combines the strengths of
Kolmogorov-Arnold Networks (KANs) and state-space models (SSMs). KM-UNet
leverages the Kolmogorov-Arnold representation theorem for efficient feature
representation and SSMs for scalable long-range modeling, achieving a balance
between accuracy and computational efficiency. We evaluate KM-UNet on five
benchmark datasets: ISIC17, ISIC18, CVC, BUSI, and GLAS. Experimental results
demonstrate that KM-UNet achieves competitive performance compared to
state-of-the-art methods in medical image segmentation tasks. To the best of
our knowledge, KM-UNet is the first medical image segmentation framework
integrating KANs and SSMs. This work provides a valuable baseline and new
insights for the development of more efficient and interpretable medical image
segmentation systems. The code is open source at
https://github.com/2760613195/KM_UNet
  Keywords:KAN,Manba, state-space models,UNet, Medical image segmentation, Deep
learning

摘要：醫學影像分割在醫學影像分析中是一項重要的任務。
傳統基於 CNN 的方法難以模擬長距離依賴性，
而基於 Transformer 的模型儘管成功，卻有二次計算複雜度的問題。為了解決這些限制，我們提出了 KM-UNet，這是一種新穎的 U 形網路架構，結合了 Kolmogorov-Arnold 網路 (KANs) 和狀態空間模型 (SSM) 的優點。KM-UNet 利用 Kolmogorov-Arnold 表示定理進行高效特徵表示，利用 SSM 進行可擴充長距離模擬，在準確度和計算效率之間取得平衡。我們在五個基準資料集上評估 KM-UNet：ISIC17、ISIC18、CVC、BUSI 和 GLAS。實驗結果表明，與醫學影像分割任務中的最先進方法相比，KM-UNet 達到了有競爭力的效能。據我們所知，KM-UNet 是第一個整合 KAN 和 SSM 的醫學影像分割框架。這項工作為開發更有效率且可解釋的醫學影像分割系統提供了有價值的基線和新見解。程式碼在 https://github.com/2760613195/KM_UNet 開源
關鍵字：KAN、Manba、狀態空間模型、UNet、醫學影像分割、深度學習

##### **Multi-LLM Collaborative Caption Generation in Scientific Documents**
2501.02552v1 by Jaeyoung Kim, Jongho Lee, Hong-Jun Choi, Ting-Yao Hsu, Chieh-Yang Huang, Sungchul Kim, Ryan Rossi, Tong Yu, Clyde Lee Giles, Ting-Hao 'Kenneth' Huang, Sungchul Choi

Scientific figure captioning is a complex task that requires generating
contextually appropriate descriptions of visual content. However, existing
methods often fall short by utilizing incomplete information, treating the task
solely as either an image-to-text or text summarization problem. This
limitation hinders the generation of high-quality captions that fully capture
the necessary details. Moreover, existing data sourced from arXiv papers
contain low-quality captions, posing significant challenges for training large
language models (LLMs). In this paper, we introduce a framework called
Multi-LLM Collaborative Figure Caption Generation (MLBCAP) to address these
challenges by leveraging specialized LLMs for distinct sub-tasks. Our approach
unfolds in three key modules: (Quality Assessment) We utilize multimodal LLMs
to assess the quality of training data, enabling the filtration of low-quality
captions. (Diverse Caption Generation) We then employ a strategy of
fine-tuning/prompting multiple LLMs on the captioning task to generate
candidate captions. (Judgment) Lastly, we prompt a prominent LLM to select the
highest quality caption from the candidates, followed by refining any remaining
inaccuracies. Human evaluations demonstrate that informative captions produced
by our approach rank better than human-written captions, highlighting its
effectiveness. Our code is available at https://github.com/teamreboott/MLBCAP

摘要：科學圖形說明是一個複雜的任務，需要產生視覺內容的適當描述。然而，現有的方法通常會因為利用不完整的資訊，將任務僅當作影像轉文字或文字摘要的問題而失敗。這個限制阻礙了產生高品質的說明，無法完全捕捉必要的細節。此外，現有從 arXiv 論文中取得的資料包含低品質的說明，對訓練大型語言模型 (LLM) 造成重大的挑戰。在本文中，我們介紹一個稱為多 LLM 協作圖形說明產生 (MLBCAP) 的架構，透過利用特殊 LLM 處理不同的子任務來解決這些挑戰。我們的做法分為三個關鍵模組：(品質評估) 我們利用多模態 LLM 評估訓練資料的品質，能夠過濾低品質的說明。(多樣化說明產生) 接著我們使用微調/提示多個 LLM 執行說明任務的策略，產生候選說明。(判斷) 最後，我們提示一個著名的 LLM 從候選說明中選擇品質最高的說明，接著修正任何剩下的不準確之處。人類評估顯示，我們的做法產生的資訊性說明，排名優於人類撰寫的說明，突顯其有效性。我們的程式碼可在 https://github.com/teamreboott/MLBCAP 取得

##### **From Language To Vision: A Case Study of Text Animation**
2501.02549v1 by Ping Chen, Richard Alo, Justin Rundell

Information can be expressed in multiple formats including natural language,
images, and motions. Human intelligence usually faces little difficulty to
convert from one format to another format, which often shows a true
understanding of encoded information. Moreover, such conversions have broad
application in many real-world applications. In this paper, we present a text
visualization system that can visualize free text with animations. Our system
is illustrated by visualizing example sentences of elementary Physics laws.

摘要：資訊可以透過多種格式表達，包括自然語言、圖像和動作。人類智慧通常可以輕易地從一種格式轉換到另一種格式，這通常顯示出對編碼資訊的真正理解。此外，這種轉換在許多真實世界的應用中都有廣泛的應用。在本文中，我們提出了一個文字視覺化系統，它可以用動畫視覺化自由文字。我們的系統透過視覺化基本物理定律範例句子來說明。

##### **AMM: Adaptive Modularized Reinforcement Model for Multi-city Traffic Signal Control**
2501.02548v1 by Zherui Huang, Yicheng Liu, Chumeng Liang, Guanjie Zheng

Traffic signal control (TSC) is an important and widely studied direction.
Recently, reinforcement learning (RL) methods have been used to solve TSC
problems and achieve superior performance over conventional TSC methods.
However, applying RL methods to the real world is challenging due to the huge
cost of experiments in real-world traffic environments. One possible solution
is TSC domain adaptation, which adapts trained models to target environments
and reduces the number of interactions and the training cost. However, existing
TSC domain adaptation methods still face two major issues: the lack of
consideration for differences across cities and the low utilization of
multi-city data.
  To solve aforementioned issues, we propose an approach named Adaptive
Modularized Model (AMM). By modularizing TSC problems and network models, we
overcome the challenge of possible changes in environmental observations. We
also aggregate multi-city experience through meta-learning. We conduct
extensive experiments on different cities and show that AMM can achieve
excellent performance with limited interactions in target environments and
outperform existing methods. We also demonstrate the feasibility and
generalizability of our method.

摘要：交通信號控制 (TSC) 是一個重要且廣泛研究的方向。
最近，強化學習 (RL) 方法已用於解決 TSC 問題，並在傳統 TSC 方法中獲得了卓越的效能。
然而，由於在真實世界的交通環境中進行實驗的成本很高，因此將 RL 方法應用於真實世界具有挑戰性。一種可能的解決方案是 TSC 領域適應，它將訓練過的模型適應到目標環境並減少互動次數和訓練成本。然而，現有的 TSC 領域適應方法仍面臨兩個主要問題：缺乏對城市之間差異的考量，以及對多城市資料的利用率低。
為了解決上述問題，我們提出了一種名為自適應模組化模型 (AMM) 的方法。透過模組化 TSC 問題和網路模型，我們克服了環境觀察可能改變的挑戰。我們也透過元學習彙整多個城市的經驗。我們對不同的城市進行了大量的實驗，並顯示 AMM 能在目標環境中透過有限的互動獲得極佳的效能，並優於現有方法。我們也展示了我們方法的可行性和概括性。

##### **A completely uniform transformer for parity**
2501.02535v1 by Alexander Kozachinskiy, Tomasz Steifer

We construct a 3-layer constant-dimension transformer, recognizing the parity
language, where neither parameter matrices nor the positional encoding depend
on the input length. This improves upon a construction of Chiang and Cholak who
use a positional encoding, depending on the input length (but their
construction has 2 layers).

摘要：我們建構一個 3 層常數維度變換器，用於辨識奇偶語言，其中參數矩陣和位置編碼皆與輸入長度無關。這改進了 Chiang 和 Cholak 的建構，他們使用取決於輸入長度的位置編碼（但他們的建構有 2 層）。

##### **Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm**
2501.02532v1 by Ljubisa Bojic, Olga Zagovora, Asta Zelenkauskaite, Vuk Vukovic, Milan Cabarkapa, Selma Veseljević Jerkovic, Ana Jovančevic

In the era of rapid digital communication, vast amounts of textual data are
generated daily, demanding efficient methods for latent content analysis to
extract meaningful insights. Large Language Models (LLMs) offer potential for
automating this process, yet comprehensive assessments comparing their
performance to human annotators across multiple dimensions are lacking. This
study evaluates the reliability, consistency, and quality of seven
state-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and
Mixtral, relative to human annotators in analyzing sentiment, political
leaning, emotional intensity, and sarcasm detection. A total of 33 human
annotators and eight LLM variants assessed 100 curated textual items,
generating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across
three time points to examine temporal consistency. Inter-rater reliability was
measured using Krippendorff's alpha, and intra-class correlation coefficients
assessed consistency over time. The results reveal that both humans and LLMs
exhibit high reliability in sentiment analysis and political leaning
assessments, with LLMs demonstrating higher internal consistency than humans.
In emotional intensity, LLMs displayed higher agreement compared to humans,
though humans rated emotional intensity significantly higher. Both groups
struggled with sarcasm detection, evidenced by low agreement. LLMs showed
excellent temporal consistency across all dimensions, indicating stable
performance over time. This research concludes that LLMs, especially GPT-4, can
effectively replicate human analysis in sentiment and political leaning,
although human expertise remains essential for emotional intensity
interpretation. The findings demonstrate the potential of LLMs for consistent
and high-quality performance in certain areas of latent content analysis.

摘要：<paragraph>在快速數位通訊的時代，每天產生大量文字資料，需要有效率的潛在內容分析方法，以萃取有意義的見解。大型語言模型 (LLM) 提供自動化此程序的潛力，但缺乏綜合評量，比較它們在多面向表現與人類註解者。此研究評估七種最先進 LLM 的可靠性、一致性和品質，包括 OpenAI 的 GPT-4、Gemini、Llama 和 Mixtral，相較於人類註解者在分析情緒、政治傾向、情緒強度和諷刺偵測。總計 33 位人類註解者和八種 LLM 變體評估 100 項策劃的文字項目，產生 3,300 項人類註解和 19,200 項 LLM 註解，在三個時間點評估 LLM，以檢視時間一致性。使用 Krippendorff's alpha 衡量評分者間信度，並使用類內相關係數評估時間一致性。結果顯示，人類和 LLM 在情緒分析和政治傾向評估中都展現高信度，LLM 顯示出比人類更高的內部一致性。在情緒強度方面，LLM 顯示出比人類更高的共識，儘管人類對情緒強度的評分顯著更高。兩組在諷刺偵測方面都面臨困難，這由低共識所證實。LLM 在所有面向都顯示出極佳的時間一致性，表示隨著時間推移，表現穩定。本研究結論是，LLM，特別是 GPT-4，可以在情緒和政治傾向有效複製人類分析，儘管人類專家知識對於情緒強度詮釋仍然至關重要。這些發現證實了 LLM 在潛在內容分析特定領域具有一致且高品質表現的潛力。</paragraph>

##### **Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI**
2501.02531v1 by Ljubisa Bojic, Dylan Seychell, Milan Cabarkapa

With the expansion of neural networks, such as large language models,
humanity is exponentially heading towards superintelligence. As various AI
systems are increasingly integrated into the fabric of societies-through
recommending values, devising creative solutions, and making decisions-it
becomes critical to assess how these AI systems impact humans in the long run.
This research aims to contribute towards establishing a benchmark for
evaluating the sentiment of various Large Language Models in socially importan
issues. The methodology adopted was a Likert scale survey. Seven LLMs,
including GPT-4 and Bard, were analyzed and compared against sentiment data
from three independent human sample populations. Temporal variations in
sentiment were also evaluated over three consecutive days. The results
highlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to
4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI,
whereas Bard was leaning towards the neutral sentiment. The human samples,
contrastingly, showed a lower average sentiment of 2.97. The temporal
comparison revealed differences in sentiment evolution between LLMs in three
days, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect
of potential conflicts of interest and bias possibilities in LLMs' sentiment
formation. Results indicate that LLMs, akin to human cognitive processes, could
potentially develop unique sentiments and subtly influence societies'
perceptions towards various opinions formed within the LLMs.

摘要：隨著神經網路的擴展，例如大型語言模型，
人類正以指數方式邁向超級智慧。由於各種 AI
系統正越來越融入社會結構中，透過
推薦價值觀、設計創造性的解決方案和做出決策，
評估這些 AI 系統如何對人類產生長遠影響變得至關重要。
本研究旨在為建立評估各種大型語言模型在社會重要
議題中的情緒基準做出貢獻。採用的方法是李克特量表調查。七種 LLM，
包括 GPT-4 和 Bard，經過分析並與來自三個獨立人類樣本族群的情緒數據進行比較。時間上的情緒變化
也在連續三天內進行評估。結果
突顯了 LLM 之間情緒分數的多樣性，範圍從 5 分中的 3.32 到
4.12。GPT-4 對 AGI 記錄了最正面的情緒分數，
而 Bard 則傾向於中立的情緒。相反地，人類樣本
顯示出較低的情緒平均值，為 2.97。時間
比較顯示出 LLM 在三天內情緒演化的差異，範圍從 1.03% 到 8.21%。這項研究的分析概述了 LLM 情緒
形成中潛在利益衝突和偏見的可能性。結果表明，LLM 類似於人類認知過程，可以
潛在地發展出獨特的情緒，並微妙地影響社會對 LLM 內形成的各種觀點的看法。

##### **Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation**
2501.02523v1 by Dawei Dai, Mingming Jia, Yinxiu Zhou, Hang Xing, Chenghang Li

Facial images have extensive practical applications. Although the current
large-scale text-image diffusion models exhibit strong generation capabilities,
it is challenging to generate the desired facial images using only text prompt.
Image prompts are a logical choice. However, current methods of this type
generally focus on general domain. In this paper, we aim to optimize image
makeup techniques to generate the desired facial images. Specifically, (1) we
built a dataset of 4 million high-quality face image-text pairs
(FaceCaptionHQ-4M) based on LAION-Face to train our Face-MakeUp model; (2) to
maintain consistency with the reference facial image, we extract/learn
multi-scale content features and pose features for the facial image,
integrating these into the diffusion model to enhance the preservation of
facial identity features for diffusion models. Validation on two face-related
test datasets demonstrates that our Face-MakeUp can achieve the best
comprehensive performance.All codes are available
at:https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp

摘要：人臉影像有廣泛的實際應用。儘管目前大規模的文字影像擴散模型展現出強大的生成能力，但僅使用文字提示要產生理想的人臉影像仍然具有挑戰性。影像提示是一個合乎邏輯的選擇。然而，這種類型的現有方法通常側重於一般領域。在本文中，我們旨在最佳化影像化妝技術以產生理想的人臉影像。具體來說，（1）我們根據 LAION-Face 建立了一個包含 400 萬張高品質人臉影像文字配對的資料集（FaceCaptionHQ-4M）來訓練我們的 Face-MakeUp 模型；（2）為了與參考人臉影像保持一致性，我們萃取/學習多尺度內容特徵和人臉影像的姿勢特徵，並將這些特徵整合到擴散模型中，以增強對擴散模型人臉特徵的保留。在兩個與人臉相關的測試資料集上的驗證證明我們的 Face-MakeUp 可以達到最佳的綜合效能。所有程式碼都可以在以下位置取得：https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp

##### **CHAIR-Classifier of Hallucination as Improver**
2501.02518v1 by Ao Sun

This paper presents a supervised method for detecting hallucinations in large
language models. By analyzing token scores (logitis) across layers of the LLaMA
model, we derive a small set, aiming to reduce overfitting, of
features-including maximum, minimum, mean, standard deviation, and slope. We
use logistic regression for classification and validate the model on the
TruthfulQA and MMLU datasets. The results demonstrate significant performance
gains, especially in zero-shot scenarios, highlighting the effectiveness and
potential for generalization.

摘要：本文提出了一個監督式方法，用於偵測大型語言模型中的幻覺。透過分析 LLaMA 模型各層的代幣分數 (logitis)，我們衍生出一組小型特徵，包括最大值、最小值、平均值、標準差和斜率，旨在減少過度擬合。我們使用邏輯迴歸進行分類，並在 TruthfulQA 和 MMLU 資料集上驗證模型。結果顯示出顯著的效能提升，特別是在零次學習場景中，突顯了泛化的有效性和潛力。

##### **Can Impressions of Music be Extracted from Thumbnail Images?**
2501.02511v1 by Takashi Harada, Takehiro Motomitsu, Katsuhiko Hayashi, Yusuke Sakai, Hidetaka Kamigaito

In recent years, there has been a notable increase in research on machine
learning models for music retrieval and generation systems that are capable of
taking natural language sentences as inputs. However, there is a scarcity of
large-scale publicly available datasets, consisting of music data and their
corresponding natural language descriptions known as music captions. In
particular, non-musical information such as suitable situations for listening
to a track and the emotions elicited upon listening is crucial for describing
music. This type of information is underrepresented in existing music caption
datasets due to the challenges associated with extracting it directly from
music data. To address this issue, we propose a method for generating music
caption data that incorporates non-musical aspects inferred from music
thumbnail images, and validated the effectiveness of our approach through human
evaluations. Additionally, we created a dataset with approximately 360,000
captions containing non-musical aspects. Leveraging this dataset, we trained a
music retrieval model and demonstrated its effectiveness in music retrieval
tasks through evaluation.

摘要：近年來，針對可將自然語言句子作為輸入的音樂檢索和生成系統的機器學習模型的研究顯著增加。然而，缺乏包含音樂資料及其對應的自然語言描述（稱為音樂標題）的大規模公開可用資料集。特別是，非音樂資訊，例如適合聆聽曲目和聆聽時引發的情緒，對於描述音樂至關重要。由於從音樂資料中直接擷取此類資訊的挑戰，此類資訊在現有的音樂標題資料集中呈現不足。為了解決這個問題，我們提出了一種生成音樂標題資料的方法，該方法納入了從音樂縮圖影像推斷的非音樂面向，並透過人為評估驗證了我們方法的有效性。此外，我們建立了一個包含約 360,000 個包含非音樂面向的標題的資料集。利用這個資料集，我們訓練了一個音樂檢索模型，並透過評估證明了它在音樂檢索任務中的有效性。

##### **PTEENet: Post-Trained Early-Exit Neural Networks Augmentation for Inference Cost Optimization**
2501.02508v1 by Assaf Lahiany, Yehudit Aperstein

For many practical applications, a high computational cost of inference over
deep network architectures might be unacceptable. A small degradation in the
overall inference accuracy might be a reasonable price to pay for a significant
reduction in the required computational resources. In this work, we describe a
method for introducing "shortcuts" into the DNN feedforward inference process
by skipping costly feedforward computations whenever possible. The proposed
method is based on the previously described BranchyNet (Teerapittayanon et al.,
2016) and the EEnet (Demir, 2019) architectures that jointly train the main
network and early exit branches. We extend those methods by attaching branches
to pre-trained models and, thus, eliminating the need to alter the original
weights of the network. We also suggest a new branch architecture based on
convolutional building blocks to allow enough training capacity when applied on
large DNNs. The proposed architecture includes confidence heads that are used
for predicting the confidence level in the corresponding early exits. By
defining adjusted thresholds on these confidence extensions, we can control in
real-time the amount of data exiting from each branch and the overall tradeoff
between speed and accuracy of our model. In our experiments, we evaluate our
method using image datasets (SVHN and CIFAR10) and several DNN architectures
(ResNet, DenseNet, VGG) with varied depth. Our results demonstrate that the
proposed method enables us to reduce the average inference computational cost
and further controlling the tradeoff between the model accuracy and the
computation cost.

摘要：對於許多實用的應用，深度網路架構中推論的高運算成本可能無法接受。整體推論精度的些微降低可能是為大幅減少所需的運算資源所付出的合理代價。在這項工作中，我們描述一種方法，透過在可能的情況下略過昂貴的饋送前向運算，將「捷徑」引入 DNN 饋送前向推論程序。所提出的方法基於先前描述的 BranchyNet（Teerapittayanon 等人，2016 年）和 EEnet（Demir，2019 年）架構，這些架構會聯合訓練主網路和早期退出分支。我們透過將分支附加到預訓練模型來延伸這些方法，因此消除了變更網路原始權重的需要。我們也建議一種新的分支架構，基於卷積建構區塊，以便在應用於大型 DNN 時允許足夠的訓練容量。所提出的架構包含用於預測對應早期退出的信心水準的信心區塊。透過定義這些信心延伸的調整閾值，我們可以在執行階段控制從每個分支退出的資料量，以及模型的速度與精度的整體取捨。在我們的實驗中，我們使用影像資料集（SVHN 和 CIFAR10）和各種深度（ResNet、DenseNet、VGG）的 DNN 架構來評估我們的模型。我們的結果顯示，所提出的方法能讓我們減少平均推論運算成本，並進一步控制模型精度與運算成本之間的取捨。

##### **ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use**
2501.02506v1 by Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, Jiechao Chen

Effective evaluation of multi-hop tool use is critical for analyzing the
understanding, reasoning, and function-calling capabilities of large language
models (LLMs). However, progress has been hindered by a lack of reliable
evaluation datasets. To address this, we present ToolHop, a dataset comprising
995 user queries and 3,912 associated tools, specifically designed for rigorous
evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful
interdependencies, locally executable tools, detailed feedback, and verifiable
answers through a novel query-driven data construction approach that includes
tool creation, document refinement, and code generation. We evaluate 14 LLMs
across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and
GPT), uncovering significant challenges in handling multi-hop tool-use
scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%,
underscoring substantial room for improvement. Further analysis reveals
variations in tool-use strategies for various families, offering actionable
insights to guide the development of more effective approaches. Code and data
can be found in https://huggingface.co/bytedance-research/ToolHop.

摘要：多跳工具使用的有效評估對於分析大型語言模型 (LLM) 的理解、推理和函數呼叫能力至關重要。然而，進度受到缺乏可靠評估資料集的阻礙。為了解決這個問題，我們提出了 ToolHop，一個包含 995 個使用者查詢和 3,912 個關聯工具的資料集，專門設計用於嚴格評估多跳工具的使用。ToolHop 透過一種新穎的查詢驅動資料建構方法確保了多樣化的查詢、有意義的相互依賴性、可本地執行的工具、詳細的回饋和可驗證的答案，其中包括工具建立、文件精煉和程式碼生成。我們評估了五個模型系列（即 LLaMA3.1、Qwen2.5、Gemini1.5、Claude3.5 和 GPT）中的 14 個 LLM，發現了在處理多跳工具使用場景時面臨的重大挑戰。領先的模型 GPT-4o 達到了 49.04% 的準確度，這凸顯了大幅改善的空間。進一步的分析揭示了不同系列的工具使用策略的差異，為指導更有效方法的開發提供了可行的見解。程式碼和資料可以在 https://huggingface.co/bytedance-research/ToolHop 中找到。

##### **Test-time Computing: from System-1 Thinking to System-2 Thinking**
2501.02497v1 by Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian Mo, Min Zhang

The remarkable performance of the o1 model in complex reasoning demonstrates
that test-time computing scaling can further unlock the model's potential,
enabling powerful System-2 thinking. However, there is still a lack of
comprehensive surveys for test-time computing scaling. We trace the concept of
test-time computing back to System-1 models. In System-1 models, test-time
computing addresses distribution shifts and improves robustness and
generalization through parameter updating, input modification, representation
editing, and output calibration. In System-2 models, it enhances the model's
reasoning ability to solve complex problems through repeated sampling,
self-correction, and tree search. We organize this survey according to the
trend of System-1 to System-2 thinking, highlighting the key role of test-time
computing in the transition from System-1 models to weak System-2 models, and
then to strong System-2 models. We also point out a few possible future
directions.

摘要：o1 模型在复杂推理中的卓越表现表明，测试时计算扩展可以进一步释放模型的潜力，实现强大的系统 2 思维。然而，对于测试时计算扩展，目前仍缺乏全面的调查。我们追溯了测试时计算的概念，将其追溯到系统 1 模型。在系统 1 模型中，测试时计算解决了分布偏移，并通过参数更新、输入修改、表示编辑和输出校准来提高鲁棒性和泛化能力。在系统 2 模型中，它通过重复采样、自我纠正和树搜索增强了模型解决复杂问题的推理能力。我们将这项调查按系统 1 到系统 2 思维的趋势进行组织，强调了测试时计算在从系统 1 模型过渡到弱系统 2 模型，再到强系统 2 模型中所扮演的关键角色。我们还指出了几个可能的未来方向。

