{"2501.04359": {"publish_time": "2025-01-08", "title": "Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation", "paper_summary": "Decoding speech from non-invasive brain signals, such as\nelectroencephalography (EEG), has the potential to advance brain-computer\ninterfaces (BCIs), with applications in silent communication and assistive\ntechnologies for individuals with speech impairments. However, EEG-based speech\ndecoding faces major challenges, such as noisy data, limited datasets, and poor\nperformance on complex tasks like speech perception. This study attempts to\naddress these challenges by employing variational autoencoders (VAEs) for EEG\ndata augmentation to improve data quality and applying a state-of-the-art\n(SOTA) sequence-to-sequence deep learning architecture, originally successful\nin electromyography (EMG) tasks, to EEG-based speech decoding. Additionally, we\nadapt this architecture for word classification tasks. Using the Brennan\ndataset, which contains EEG recordings of subjects listening to narrated\nspeech, we preprocess the data and evaluate both classification and\nsequence-to-sequence models for EEG-to-words/sentences tasks. Our experiments\nshow that VAEs have the potential to reconstruct artificial EEG data for\naugmentation. Meanwhile, our sequence-to-sequence model achieves more promising\nperformance in generating sentences compared to our classification model,\nthough both remain challenging tasks. These findings lay the groundwork for\nfuture research on EEG speech perception decoding, with possible extensions to\nspeech production tasks such as silent or imagined speech.", "paper_summary_zh": "\u5f9e\u975e\u4fb5\u5165\u5f0f\u8166\u4fe1\u865f\uff08\u4f8b\u5982\u8166\u96fb\u5716 (EEG)\uff09\u89e3\u78bc\u8a9e\u8a00\uff0c\u5177\u6709\u63a8\u52d5\u8166\u6a5f\u4ecb\u9762 (BCI) \u7684\u6f5b\u529b\uff0c\u4e26\u53ef\u61c9\u7528\u65bc\u7121\u8072\u6e9d\u901a\u548c\u5354\u52a9\u8a9e\u8a00\u969c\u7919\u4eba\u58eb\u7684\u8f14\u52a9\u6280\u8853\u3002\u7136\u800c\uff0c\u57fa\u65bc EEG \u7684\u8a9e\u8a00\u89e3\u78bc\u9762\u81e8\u91cd\u5927\u6311\u6230\uff0c\u4f8b\u5982\u96dc\u8a0a\u8cc7\u6599\u3001\u53d7\u9650\u7684\u8cc7\u6599\u96c6\u548c\u5728\u8a9e\u8a00\u77e5\u89ba\u7b49\u8907\u96dc\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u5617\u8a66\u900f\u904e\u63a1\u7528\u8b8a\u7570\u81ea\u52d5\u7de8\u78bc\u5668 (VAE) \u4f86\u64f4\u5145 EEG \u8cc7\u6599\u4ee5\u6539\u5584\u8cc7\u6599\u54c1\u8cea\uff0c\u4e26\u61c9\u7528\u6700\u5148\u9032 (SOTA) \u7684\u5e8f\u5217\u5c0d\u5e8f\u5217\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\uff08\u6700\u521d\u6210\u529f\u61c9\u7528\u65bc\u808c\u96fb\u5716 (EMG) \u4efb\u52d9\uff09\u5230\u57fa\u65bc EEG \u7684\u8a9e\u8a00\u89e3\u78bc\uff0c\u4f86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\u3002\u6b64\u5916\uff0c\u6211\u5011\u8abf\u6574\u6b64\u67b6\u69cb\u4ee5\u9069\u7528\u65bc\u8a5e\u5f59\u5206\u985e\u4efb\u52d9\u3002\u6211\u5011\u4f7f\u7528 Brennan \u8cc7\u6599\u96c6\uff08\u5176\u4e2d\u5305\u542b\u53d7\u8a66\u8005\u8046\u807d\u6558\u8ff0\u8a9e\u8a00\u7684 EEG \u8a18\u9304\uff09\uff0c\u9810\u8655\u7406\u8cc7\u6599\u4e26\u8a55\u4f30\u5206\u985e\u548c\u5e8f\u5217\u5c0d\u5e8f\u5217\u6a21\u578b\u5728 EEG \u8f49\u63db\u70ba\u6587\u5b57/\u53e5\u5b50\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0cVAE \u6709\u6f5b\u529b\u91cd\u5efa\u4eba\u5de5 EEG \u8cc7\u6599\u4ee5\u9032\u884c\u64f4\u5145\u3002\u540c\u6642\uff0c\u8207\u5206\u985e\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u5e8f\u5217\u5c0d\u5e8f\u5217\u6a21\u578b\u5728\u7522\u751f\u53e5\u5b50\u65b9\u9762\u8868\u73fe\u66f4\u5177\u524d\u666f\uff0c\u5118\u7ba1\u5169\u8005\u4ecd\u662f\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\u3002\u9019\u4e9b\u767c\u73fe\u70ba\u672a\u4f86 EEG \u8a9e\u8a00\u77e5\u89ba\u89e3\u78bc\u7814\u7a76\u5960\u5b9a\u57fa\u790e\uff0c\u4e26\u53ef\u80fd\u5ef6\u4f38\u81f3\u8a9e\u8a00\u7522\u751f\u4efb\u52d9\uff0c\u4f8b\u5982\u7121\u8072\u6216\u60f3\u50cf\u7684\u8a9e\u8a00\u3002", "author": "Terrance Yu-Hao Chen et.al.", "authors": "Terrance Yu-Hao Chen, Yulin Chen, Pontus Soederhaell, Sadrishya Agrawal, Kateryna Shapovalenko", "id": "2501.04359v1", "paper_url": "http://arxiv.org/abs/2501.04359v1", "repo": "null"}}