{"2405.15453": {"publish_time": "2024-05-24", "title": "Benchmarking Pre-trained Large Language Models' Potential Across Urdu NLP tasks", "paper_summary": "Large Language Models (LLMs) pre-trained on multilingual data have\nrevolutionized natural language processing research, by transitioning from\nlanguages and task specific model pipelines to a single model adapted on a\nvariety of tasks. However majority of existing multilingual NLP benchmarks for\nLLMs provide evaluation data in only few languages with little linguistic\ndiversity. In addition these benchmarks lack quality assessment against the\nrespective state-of the art models. This study presents an in-depth examination\nof prominent LLMs; GPT-3.5-turbo, Llama2-7B-Chat, Bloomz 7B1 and Bloomz 3B,\nacross 14 tasks using 15 Urdu datasets, in a zero-shot setting, and their\nperformance against state-of-the-art (SOTA) models, has been compared and\nanalysed. Our experiments show that SOTA models surpass all the encoder-decoder\npre-trained language models in all Urdu NLP tasks with zero-shot learning. Our\nresults further show that LLMs with fewer parameters, but more language\nspecific data in the base model perform better than larger computational\nmodels, but low language data.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9810\u5148\u5728\u591a\u8a9e\u8a00\u8cc7\u6599\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u900f\u904e\u5f9e\u7279\u5b9a\u8a9e\u8a00\u548c\u4efb\u52d9\u6a21\u578b\u7ba1\u7dda\u8f49\u63db\u70ba\u9069\u61c9\u5404\u7a2e\u4efb\u52d9\u7684\u55ae\u4e00\u6a21\u578b\uff0c\u5fb9\u5e95\u6539\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7814\u7a76\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u5927\u591a\u6578\u591a\u8a9e\u8a00 NLP \u57fa\u6e96\u6e2c\u8a66\u50c5\u63d0\u4f9b\u5c11\u6578\u8a9e\u8a00\u7684\u8a55\u4f30\u8cc7\u6599\uff0c\u8a9e\u8a00\u591a\u6a23\u6027\u8f03\u4f4e\u3002\u6b64\u5916\uff0c\u9019\u4e9b\u57fa\u6e96\u6e2c\u8a66\u7f3a\u4e4f\u91dd\u5c0d\u5404\u81ea\u6700\u5148\u9032\u6a21\u578b\u7684\u54c1\u8cea\u8a55\u4f30\u3002\u672c\u7814\u7a76\u6df1\u5165\u63a2\u8a0e\u4e86\u4e3b\u8981\u7684 LLM\uff1bGPT-3.5-turbo\u3001Llama2-7B-Chat\u3001Bloomz 7B1 \u548c Bloomz 3B\uff0c\u4f7f\u7528 15 \u500b\u70cf\u723e\u90fd\u8a9e\u8cc7\u6599\u96c6\uff0c\u5728\u96f6\u6b21\u5b78\u7fd2\u8a2d\u5b9a\u4e2d\u57f7\u884c 14 \u9805\u4efb\u52d9\uff0c\u4e26\u6bd4\u8f03\u548c\u5206\u6790\u5b83\u5011\u76f8\u5c0d\u65bc\u6700\u5148\u9032 (SOTA) \u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0cSOTA \u6a21\u578b\u5728\u6240\u6709\u70cf\u723e\u90fd\u8a9e NLP \u4efb\u52d9\u4e2d\u90fd\u8d85\u8d8a\u4e86\u6240\u6709\u7de8\u78bc\u5668\u89e3\u78bc\u5668\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u4e14\u63a1\u7528\u96f6\u6b21\u5b78\u7fd2\u3002\u6211\u5011\u7684\u7d50\u679c\u9032\u4e00\u6b65\u986f\u793a\uff0c\u53c3\u6578\u8f03\u5c11\u4f46\u57fa\u672c\u6a21\u578b\u4e2d\u8a9e\u8a00\u7279\u5b9a\u8cc7\u6599\u8f03\u591a\u7684 LLM\uff0c\u5176\u6548\u80fd\u512a\u65bc\u8a08\u7b97\u6a21\u578b\u8f03\u5927\u4f46\u8a9e\u8a00\u8cc7\u6599\u8f03\u5c11\u7684 LLM\u3002", "author": "Munief Hassan Tahir et.al.", "authors": "Munief Hassan Tahir, Sana Shams, Layba Fiaz, Farah Adeeba, Sarmad Hussain", "id": "2405.15453v1", "paper_url": "http://arxiv.org/abs/2405.15453v1", "repo": "null"}}