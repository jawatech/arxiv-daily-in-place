{"2405.12174": {"publish_time": "2024-05-20", "title": "CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models", "paper_summary": "Text-to-Table aims to generate structured tables to convey the key\ninformation from unstructured documents. Existing text-to-table datasets are\ntypically oriented English, limiting the research in non-English languages.\nMeanwhile, the emergence of large language models (LLMs) has shown great\nsuccess as general task solvers in multi-lingual settings (e.g., ChatGPT),\ntheoretically enabling text-to-table in other languages. In this paper, we\npropose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this\ntask. Our preliminary analysis of English text-to-table datasets highlights two\nkey factors for dataset construction: data diversity and data hallucination.\nInspired by this, the CT-Eval dataset selects a popular Chinese\nmultidisciplinary online encyclopedia as the source and covers 28 domains to\nensure data diversity. To minimize data hallucination, we first train an LLM to\njudge and filter out the task samples with hallucination, then employ human\nannotators to clean the hallucinations in the validation and testing sets.\nAfter this process, CT-Eval contains 88.6K task samples. Using CT-Eval, we\nevaluate the performance of open-source and closed-source LLMs. Our results\nreveal that zero-shot LLMs (including GPT-4) still have a significant\nperformance gap compared with human judgment. Furthermore, after fine-tuning,\nopen-source LLMs can significantly improve their text-to-table ability,\noutperforming GPT-4 by a large margin. In short, CT-Eval not only helps\nresearchers evaluate and quickly understand the Chinese text-to-table ability\nof existing LLMs but also serves as a valuable resource to significantly\nimprove the text-to-table performance of LLMs.", "paper_summary_zh": "\u6587\u672c\u5230\u8868\u683c\u65e8\u5728\u751f\u6210\u7ed3\u6784\u5316\u8868\u683c\u6765\u4f20\u8fbe\u975e\u7ed3\u6784\u5316\u6587\u6863\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u3002\u73b0\u6709\u7684\u6587\u672c\u5230\u8868\u683c\u6570\u636e\u96c6\u901a\u5e38\u9762\u5411\u82f1\u8bed\uff0c\u9650\u5236\u4e86\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u7814\u7a76\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u51fa\u73b0\u5df2\u663e\u793a\u51fa\u4f5c\u4e3a\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u901a\u7528\u4efb\u52a1\u89e3\u51b3\u8005\u7684\u5de8\u5927\u6210\u529f\uff08\u4f8b\u5982 ChatGPT\uff09\uff0c\u7406\u8bba\u4e0a\u53ef\u4ee5\u5728\u5176\u4ed6\u8bed\u8a00\u4e2d\u5b9e\u73b0\u6587\u672c\u5230\u8868\u683c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e2d\u6587\u6587\u672c\u5230\u8868\u683c\u6570\u636e\u96c6 CT-Eval\uff0c\u4ee5\u5bf9 LLM \u5728\u6b64\u4efb\u52a1\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u6211\u4eec\u5bf9\u82f1\u8bed\u6587\u672c\u5230\u8868\u683c\u6570\u636e\u96c6\u7684\u521d\u6b65\u5206\u6790\u7a81\u51fa\u4e86\u6570\u636e\u96c6\u6784\u5efa\u7684\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\uff1a\u6570\u636e\u591a\u6837\u6027\u548c\u6570\u636e\u5e7b\u89c9\u3002\u53d7\u6b64\u542f\u53d1\uff0cCT-Eval \u6570\u636e\u96c6\u9009\u62e9\u4e86\u4e00\u4e2a\u6d41\u884c\u7684\u4e2d\u6587\u591a\u5b66\u79d1\u5728\u7ebf\u767e\u79d1\u5168\u4e66\u4f5c\u4e3a\u6765\u6e90\uff0c\u5e76\u6db5\u76d6 28 \u4e2a\u9886\u57df\u4ee5\u786e\u4fdd\u6570\u636e\u591a\u6837\u6027\u3002\u4e3a\u4e86\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u6570\u636e\u5e7b\u89c9\uff0c\u6211\u4eec\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a LLM \u6765\u5224\u65ad\u5e76\u8fc7\u6ee4\u6389\u5e26\u6709\u5e7b\u89c9\u7684\u4efb\u52a1\u6837\u672c\uff0c\u7136\u540e\u96c7\u7528\u4eba\u5de5\u6ce8\u91ca\u5458\u6765\u6e05\u7406\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u4e2d\u7684\u5e7b\u89c9\u3002\u7ecf\u8fc7\u8fd9\u4e2a\u8fc7\u7a0b\uff0cCT-Eval \u5305\u542b 88.6K \u4e2a\u4efb\u52a1\u6837\u672c\u3002\u4f7f\u7528 CT-Eval\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u95ed\u6e90 LLM \u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u96f6\u6837\u672c LLM\uff08\u5305\u62ec GPT-4\uff09\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u6bd4\u4ecd\u7136\u5b58\u5728\u663e\u7740\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u5728\u5fae\u8c03\u4e4b\u540e\uff0c\u5f00\u6e90 LLM \u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u5176\u6587\u672c\u5230\u8868\u683c\u7684\u80fd\u529b\uff0c\u5927\u5e45\u4f18\u4e8e GPT-4\u3002\u7b80\u800c\u8a00\u4e4b\uff0cCT-Eval \u4e0d\u4ec5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u548c\u5feb\u901f\u4e86\u89e3\u73b0\u6709 LLM \u7684\u4e2d\u6587\u6587\u672c\u5230\u8868\u683c\u80fd\u529b\uff0c\u800c\u4e14\u8fd8\u4f5c\u4e3a\u4e00\u79cd\u5b9d\u8d35\u7684\u8d44\u6e90\u6765\u663e\u7740\u63d0\u9ad8 LLM \u7684\u6587\u672c\u5230\u8868\u683c\u6027\u80fd\u3002", "author": "Haoxiang Shi et.al.", "authors": "Haoxiang Shi, Jiaan Wang, Jiarong Xu, Cen Wang, Tetsuya Sakai", "id": "2405.12174v1", "paper_url": "http://arxiv.org/abs/2405.12174v1", "repo": "null"}}