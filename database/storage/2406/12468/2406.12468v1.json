{"2406.12468": {"publish_time": "2024-06-18", "title": "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities", "paper_summary": "The parametric knowledge memorized by large language models (LLMs) becomes\noutdated quickly. In-context editing (ICE) is currently the most effective\nmethod for updating the knowledge of LLMs. Recent advancements involve\nenhancing ICE by modifying the decoding strategy, obviating the need for\naltering internal model structures or adjusting external prompts. However, this\nenhancement operates across the entire sequence generation, encompassing a\nplethora of non-critical tokens. In this work, we introduce $\\textbf{A}$daptive\n$\\textbf{T}$oken $\\textbf{Bias}$er ($\\textbf{ATBias}$), a new decoding\ntechnique designed to enhance ICE. It focuses on the tokens that are mostly\nrelated to knowledge during decoding, biasing their logits by matching key\nentities related to new and parametric knowledge. Experimental results show\nthat ATBias significantly enhances ICE performance, achieving up to a 32.3%\nimprovement over state-of-the-art ICE methods while incurring only half the\nlatency. ATBias not only improves the knowledge editing capabilities of ICE but\ncan also be widely applied to LLMs with negligible cost.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u8a18\u61b6\u7684\u53c3\u6578\u5316\u77e5\u8b58\u6703\u5feb\u901f\u904e\u6642\u3002\u76ee\u524d\uff0c\u8a9e\u5883\u7de8\u8f2f\uff08ICE\uff09\u662f\u66f4\u65b0 LLM \u77e5\u8b58\u6700\u6709\u6548\u7684\u65b9\u6cd5\u3002\u6700\u8fd1\u7684\u9032\u5c55\u5305\u62ec\u900f\u904e\u4fee\u6539\u89e3\u78bc\u7b56\u7565\u4f86\u589e\u5f37 ICE\uff0c\u7121\u9700\u6539\u8b8a\u5167\u90e8\u6a21\u578b\u7d50\u69cb\u6216\u8abf\u6574\u5916\u90e8\u63d0\u793a\u3002\u7136\u800c\uff0c\u9019\u7a2e\u589e\u5f37\u4f5c\u7528\u65bc\u6574\u500b\u5e8f\u5217\u751f\u6210\uff0c\u6db5\u84cb\u4e86\u5927\u91cf\u7684\u975e\u95dc\u9375\u4ee3\u78bc\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 $\\textbf{A}$daptive $\\textbf{T}$oken $\\textbf{Bias}$er\uff08$\\textbf{ATBias}$\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u89e3\u78bc\u6280\u8853\uff0c\u65e8\u5728\u589e\u5f37 ICE\u3002\u5b83\u5c08\u6ce8\u65bc\u89e3\u78bc\u904e\u7a0b\u4e2d\u8207\u77e5\u8b58\u6700\u76f8\u95dc\u7684\u4ee3\u78bc\uff0c\u900f\u904e\u6bd4\u5c0d\u8207\u65b0\u53c3\u6578\u5316\u77e5\u8b58\u76f8\u95dc\u7684\u4e3b\u8981\u5be6\u9ad4\uff0c\u4f86\u504f\u5411\u5b83\u5011\u7684 logit\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cATBias \u5927\u5e45\u589e\u5f37\u4e86 ICE \u7684\u6548\u80fd\uff0c\u6bd4\u6700\u5148\u9032\u7684 ICE \u65b9\u6cd5\u63d0\u5347\u4e86 32.3%\uff0c\u540c\u6642\u50c5\u7522\u751f\u4e00\u534a\u7684\u5ef6\u9072\u3002ATBias \u4e0d\u50c5\u6539\u5584\u4e86 ICE \u7684\u77e5\u8b58\u7de8\u8f2f\u80fd\u529b\uff0c\u9084\u80fd\u5ee3\u6cdb\u61c9\u7528\u65bc LLM\uff0c\u4e14\u6210\u672c\u6975\u4f4e\u3002", "author": "Baolong Bi et.al.", "authors": "Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, Xueqi Cheng", "id": "2406.12468v1", "paper_url": "http://arxiv.org/abs/2406.12468v1", "repo": "null"}}