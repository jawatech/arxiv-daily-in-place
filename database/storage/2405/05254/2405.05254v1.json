{"2405.05254": {"publish_time": "2024-05-08", "title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models", "paper_summary": "We introduce a decoder-decoder architecture, YOCO, for large language models,\nwhich only caches key-value pairs once. It consists of two components, i.e., a\ncross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes\nglobal key-value (KV) caches that are reused by the cross-decoder via\ncross-attention. The overall model behaves like a decoder-only Transformer,\nalthough YOCO only caches once. The design substantially reduces GPU memory\ndemands, yet retains global attention capability. Additionally, the computation\nflow enables prefilling to early exit without changing the final output,\nthereby significantly speeding up the prefill stage. Experimental results\ndemonstrate that YOCO achieves favorable performance compared to Transformer in\nvarious settings of scaling up model size and number of training tokens. We\nalso extend YOCO to 1M context length with near-perfect needle retrieval\naccuracy. The profiling results show that YOCO improves inference memory,\nprefill latency, and throughput by orders of magnitude across context lengths\nand model sizes. Code is available at https://aka.ms/YOCO.", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u89e3\u78bc\u5668-\u89e3\u78bc\u5668\u67b6\u69cb\uff0cYOCO\uff0c\u9069\u7528\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u5b83\u53ea\u5feb\u53d6\u4e00\u6b21\u9375\u503c\u5c0d\u3002\u5b83\u5305\u542b\u5169\u500b\u5143\u4ef6\uff0c\u5373\u5806\u758a\u5728\u81ea\u89e3\u78bc\u5668\u4e0a\u7684\u4ea4\u53c9\u89e3\u78bc\u5668\u3002\u81ea\u89e3\u78bc\u5668\u6709\u6548\u7de8\u78bc\u5168\u5c40\u9375\u503c (KV) \u5feb\u53d6\uff0c\u800c\u4ea4\u53c9\u89e3\u78bc\u5668\u900f\u904e\u4ea4\u53c9\u6ce8\u610f\u529b\u91cd\u8907\u4f7f\u7528\u9019\u4e9b\u5feb\u53d6\u3002\u6574\u9ad4\u6a21\u578b\u7684\u884c\u70ba\u5c31\u50cf\u53ea\u542b\u89e3\u78bc\u5668\u7684 Transformer\uff0c\u5118\u7ba1 YOCO \u53ea\u5feb\u53d6\u4e00\u6b21\u3002\u6b64\u8a2d\u8a08\u5927\u5e45\u6e1b\u5c11 GPU \u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u537b\u4fdd\u7559\u4e86\u5168\u5c40\u6ce8\u610f\u529b\u529f\u80fd\u3002\u6b64\u5916\uff0c\u904b\u7b97\u6d41\u7a0b\u80fd\u9810\u5148\u586b\u5165\u4ee5\u63d0\u65e9\u7d50\u675f\uff0c\u800c\u4e0d\u6703\u8b8a\u66f4\u6700\u7d42\u8f38\u51fa\uff0c\u56e0\u800c\u5927\u5e45\u52a0\u5feb\u9810\u5148\u586b\u5165\u968e\u6bb5\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u5728\u64f4\u5145\u6a21\u578b\u5927\u5c0f\u548c\u8a13\u7df4\u6b0a\u6756\u6578\u76ee\u7684\u5404\u7a2e\u8a2d\u5b9a\u4e2d\uff0cYOCO \u9054\u5230\u6bd4 Transformer \u66f4\u597d\u7684\u6548\u80fd\u3002\u6211\u5011\u4e5f\u5c07 YOCO \u64f4\u5145\u5230 1M \u7684\u8108\u7d61\u9577\u5ea6\uff0c\u4e26\u5177\u6709\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u91dd\u982d\u64f7\u53d6\u6e96\u78ba\u5ea6\u3002\u5206\u6790\u7d50\u679c\u986f\u793a\uff0cYOCO \u5728\u5404\u7a2e\u8108\u7d61\u9577\u5ea6\u548c\u6a21\u578b\u5927\u5c0f\u4e2d\uff0c\u5927\u5e45\u6539\u5584\u4e86\u63a8\u8ad6\u8a18\u61b6\u9ad4\u3001\u9810\u5148\u586b\u5165\u5ef6\u9072\u6642\u9593\u548c\u8655\u7406\u91cf\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://aka.ms/YOCO \u53d6\u5f97\u3002", "author": "Yutao Sun et.al.", "authors": "Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, Furu Wei", "id": "2405.05254v1", "paper_url": "http://arxiv.org/abs/2405.05254v1", "repo": "null"}}