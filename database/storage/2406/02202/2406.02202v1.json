{"2406.02202": {"publish_time": "2024-06-04", "title": "Can CLIP help CLIP in learning 3D?", "paper_summary": "In this study, we explore an alternative approach to enhance contrastive\ntext-image-3D alignment in the absence of textual descriptions for 3D objects.\nWe introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP\nknowledge about textual and 2D data to compute the neural perceived similarity\nbetween two 3D samples. We employ the proposed methods to mine 3D hard\nnegatives, establishing a multimodal contrastive pipeline with hard negative\nweighting via a custom loss function. We train on different configurations of\nthe proposed hard negative mining approach, and we evaluate the accuracy of our\nmodels in 3D classification and on the cross-modal retrieval benchmark, testing\nimage-to-shape and shape-to-image retrieval. Results demonstrate that our\napproach, even without explicit text alignment, achieves comparable or superior\nperformance on zero-shot and standard 3D classification, while significantly\nimproving both image-to-shape and shape-to-image retrieval compared to previous\nmethods.", "paper_summary_zh": "\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u7a2e\u66ff\u4ee3\u65b9\u6cd5\uff0c\u4ee5\u5728\u6c92\u6709 3D \u7269\u9ad4\u7684\u6587\u5b57\u63cf\u8ff0\u7684\u60c5\u6cc1\u4e0b\u589e\u5f37\u5c0d\u6bd4\u6587\u672c-\u5716\u50cf-3D \u5c0d\u9f4a\u3002\u6211\u5011\u4ecb\u7d39\u4e86\u5169\u7a2e\u7121\u76e3\u7763\u65b9\u6cd5\uff0c$I2I$ \u548c $(I2L)^2$\uff0c\u5b83\u5011\u5229\u7528 CLIP \u95dc\u65bc\u6587\u672c\u548c 2D \u6578\u64da\u7684\u77e5\u8b58\u4f86\u8a08\u7b97\u5169\u500b 3D \u6a23\u672c\u4e4b\u9593\u7684\u795e\u7d93\u611f\u77e5\u76f8\u4f3c\u6027\u3002\u6211\u5011\u63a1\u7528\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f86\u6316\u6398 3D \u786c\u8ca0\u4f8b\uff0c\u901a\u904e\u81ea\u5b9a\u7fa9\u640d\u5931\u51fd\u6578\u5efa\u7acb\u5177\u6709\u786c\u8ca0\u4f8b\u52a0\u6b0a\u7684\u591a\u6a21\u614b\u5c0d\u6bd4\u7ba1\u9053\u3002\u6211\u5011\u91dd\u5c0d\u6240\u63d0\u51fa\u7684\u786c\u8ca0\u4f8b\u6316\u6398\u65b9\u6cd5\u7684\u4e0d\u540c\u914d\u7f6e\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u5728 3D \u5206\u985e\u548c\u8de8\u6a21\u614b\u6aa2\u7d22\u57fa\u6e96\u4e0a\u8a55\u4f30\u6211\u5011\u6a21\u578b\u7684\u6e96\u78ba\u6027\uff0c\u6e2c\u8a66\u5716\u50cf\u5230\u5f62\u72c0\u548c\u5f62\u72c0\u5230\u5716\u50cf\u6aa2\u7d22\u3002\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5373\u4f7f\u6c92\u6709\u660e\u78ba\u7684\u6587\u672c\u5c0d\u9f4a\uff0c\u4e5f\u80fd\u5728\u96f6\u6b21\u5b78\u7fd2\u548c\u6a19\u6e96 3D \u5206\u985e\u4e0a\u5be6\u73fe\u76f8\u7576\u6216\u66f4\u512a\u7684\u6027\u80fd\uff0c\u540c\u6642\u8207\u4e4b\u524d\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u986f\u8457\u6539\u9032\u4e86\u5716\u50cf\u5230\u5f62\u72c0\u548c\u5f62\u72c0\u5230\u5716\u50cf\u6aa2\u7d22\u3002", "author": "Cristian Sbrolli et.al.", "authors": "Cristian Sbrolli, Matteo Matteucci", "id": "2406.02202v1", "paper_url": "http://arxiv.org/abs/2406.02202v1", "repo": "null"}}