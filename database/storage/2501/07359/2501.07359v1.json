{"2501.07359": {"publish_time": "2025-01-13", "title": "Emergent effects of scaling on the functional hierarchies within large language models", "paper_summary": "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u67b6\u69cb\u901a\u5e38\u88ab\u63cf\u8ff0\u70ba\u529f\u80fd\u5206\u5c64\uff1a\u65e9\u671f\u5c64\u8655\u7406\u8a9e\u6cd5\uff0c\u4e2d\u9593\u5c64\u958b\u59cb\u89e3\u6790\u8a9e\u7fa9\uff0c\u665a\u671f\u5c64\u6574\u5408\u8cc7\u8a0a\u3002\u672c\u7814\u7a76\u91cd\u65b0\u63a2\u8a0e\u9019\u4e9b\u60f3\u6cd5\u3002\u672c\u7814\u7a76\u5c07\u7c21\u55ae\u6587\u5b57\u63d0\u4ea4\u7d66 LLM\uff08\u4f8b\u5982\uff0c\u300c\u4e00\u9593\u6559\u5802\u548c\u7ba1\u98a8\u7434\u300d\uff09\uff0c\u4e26\u8403\u53d6\u7522\u751f\u7684\u6d3b\u5316\u3002\u7136\u5f8c\uff0c\u5c0d\u65bc\u6bcf\u500b\u5c64\uff0c\u652f\u63f4\u5411\u91cf\u6a5f\u548c\u5dba\u8ff4\u6b78\u9069\u7528\u65bc\u9810\u6e2c\u6587\u5b57\u6a19\u7c64\uff0c\u4e26\u9032\u800c\u6aa2\u8996\u7279\u5b9a\u5c64\u662f\u5426\u7de8\u78bc\u67d0\u4e9b\u8cc7\u8a0a\u3002\u4f7f\u7528\u5c0f\u578b\u6a21\u578b\uff08Llama-3.2-3b\uff1b28 \u5c64\uff09\u7684\u5206\u6790\u90e8\u5206\u652f\u6301\u5e38\u898b\u7684\u5206\u5c64\u89c0\u9ede\uff1a\u9805\u76ee\u5c64\u7d1a\u8a9e\u7fa9\u5728\u65e9\u671f\uff08\u5c64 2-7\uff09\u6700\u6709\u529b\u5730\u88ab\u8868\u793a\uff0c\u7136\u5f8c\u662f\u5169\u500b\u9805\u76ee\u7684\u95dc\u4fc2\uff08\u5c64 8-12\uff09\uff0c\u7136\u5f8c\u662f\u56db\u500b\u9805\u76ee\u7684\u985e\u6bd4\uff08\u5c64 10-15\uff09\u3002\u4e4b\u5f8c\uff0c\u9805\u76ee\u548c\u7c21\u55ae\u95dc\u4fc2\u7684\u8868\u793a\u5728\u8f03\u6df1\u5c64\u9010\u6f38\u6e1b\u5c11\uff0c\u9019\u4e9b\u5c64\u5c08\u6ce8\u65bc\u66f4\u591a\u5168\u7403\u8cc7\u8a0a\u3002\u7136\u800c\uff0c\u4e00\u4e9b\u767c\u73fe\u8207\u7a69\u5b9a\u7684\u968e\u5c64\u89c0\u9ede\u76f8\u53cd\uff1a\u9996\u5148\uff0c\u5118\u7ba1\u6df1\u5c64\u53ef\u4ee5\u8868\u793a\u6587\u4ef6\u7bc4\u570d\u7684\u62bd\u8c61\uff0c\u4f46\u6df1\u5c64\u4e5f\u6703\u58d3\u7e2e\u4f86\u81ea\u5167\u5bb9\u8996\u7a97\u65e9\u671f\u90e8\u5206\u7684\u8cc7\u8a0a\uff0c\u800c\u6c92\u6709\u6709\u610f\u7fa9\u7684\u62bd\u8c61\u3002\u5176\u6b21\uff0c\u5728\u6aa2\u8996\u8f03\u5927\u7684\u6a21\u578b\uff08Llama-3.3-70b-Instruct\uff09\u6642\uff0c\u62bd\u8c61\u5c64\u7d1a\u51fa\u73fe\u660e\u986f\u7684\u6ce2\u52d5\uff1a\u96a8\u8457\u6df1\u5ea6\u7684\u589e\u52a0\uff0c\u5169\u500b\u9805\u76ee\u7684\u95dc\u4fc2\u548c\u56db\u500b\u9805\u76ee\u7684\u985e\u6bd4\u6700\u521d\u5728\u5b83\u5011\u7684\u8868\u793a\u4e2d\u589e\u52a0\uff0c\u7136\u5f8c\u986f\u8457\u6e1b\u5c11\uff0c\u4e4b\u5f8c\u53c8\u518d\u6b21\u589e\u52a0\u3002\u9019\u7a2e\u5947\u7279\u7684\u6a21\u5f0f\u5728\u591a\u500b\u5be6\u9a57\u4e2d\u6301\u7e8c\u51fa\u73fe\u3002\u7b2c\u4e09\uff0c\u898f\u6a21\u5316\u7684\u53e6\u4e00\u500b\u65b0\u8208\u6548\u61c9\u662f\u76f8\u9130\u5c64\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u4e4b\u9593\u7684\u5354\u8abf\u3002\u5728\u4f7f\u7528\u8f03\u5927\u6a21\u578b\u7684\u591a\u6b21\u5be6\u9a57\u4e2d\uff0c\u76f8\u9130\u5c64\u5728\u5b83\u5011\u5404\u81ea\u5c08\u9580\u8868\u793a\u7684\u8cc7\u8a0a\u4e4b\u9593\u6ce2\u52d5\u3002\u7e3d\u4e4b\uff0c\u62bd\u8c61\u968e\u5c64\u901a\u5e38\u5728\u5404\u5c64\u4e2d\u8868\u73fe\u51fa\u4f86\uff0c\u4f46\u5927\u578b\u6a21\u578b\u4e5f\u6703\u4ee5\u5947\u7279\u7684\u65b9\u5f0f\u504f\u96e2\u9019\u500b\u7d50\u69cb\u3002", "author": "Paul C. Bogdan et.al.", "authors": "Paul C. Bogdan", "id": "2501.07359v1", "paper_url": "http://arxiv.org/abs/2501.07359v1", "repo": "null"}}