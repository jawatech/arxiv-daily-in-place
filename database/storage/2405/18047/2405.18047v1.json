{"2405.18047": {"publish_time": "2024-05-28", "title": "2BP: 2-Stage Backpropagation", "paper_summary": "As Deep Neural Networks (DNNs) grow in size and complexity, they often exceed\nthe memory capacity of a single accelerator, necessitating the sharding of\nmodel parameters across multiple accelerators. Pipeline parallelism is a\ncommonly used sharding strategy for training large DNNs. However, current\nimplementations of pipeline parallelism are being unintentionally bottlenecked\nby the automatic differentiation tools provided by ML frameworks. This paper\nintroduces 2-stage backpropagation (2BP). By splitting the backward propagation\nstep into two separate stages, we can reduce idle compute time. We tested 2BP\non various model architectures and pipelining schedules, achieving increases in\nthroughput in all cases. Using 2BP, we were able to achieve a 1.70x increase in\nthroughput compared to traditional methods when training a LLaMa-like\ntransformer with 7 billion parameters across 4 GPUs.", "paper_summary_zh": "\u96a8\u8457\u6df1\u5ea6\u795e\u7d93\u7db2\u8def (DNN) \u5728\u898f\u6a21\u548c\u8907\u96dc\u5ea6\u4e0a\u4e0d\u65b7\u589e\u52a0\uff0c\u5b83\u5011\u901a\u5e38\u6703\u8d85\u904e\u55ae\u500b\u52a0\u901f\u5668\u7684\u8a18\u61b6\u9ad4\u5bb9\u91cf\uff0c\u56e0\u6b64\u5fc5\u9808\u5c07\u6a21\u578b\u53c3\u6578\u5206\u7247\u5230\u591a\u500b\u52a0\u901f\u5668\u3002\u7ba1\u7dda\u5e73\u884c\u5316\u662f\u4e00\u7a2e\u5e38\u7528\u7684\u5206\u7247\u7b56\u7565\uff0c\u7528\u65bc\u8a13\u7df4\u5927\u578b DNN\u3002\u7136\u800c\uff0c\u76ee\u524d\u7ba1\u7dda\u4e26\u884c\u7684\u5be6\u4f5c\u65b9\u5f0f\u6703\u53d7\u5230 ML \u6846\u67b6\u63d0\u4f9b\u7684\u81ea\u52d5\u5fae\u5206\u5de5\u5177\u7121\u610f\u9593\u7684\u74f6\u9838\u9650\u5236\u3002\u672c\u6587\u4ecb\u7d39\u4e86 2 \u968e\u6bb5\u53cd\u5411\u50b3\u64ad (2BP)\u3002\u900f\u904e\u5c07\u53cd\u5411\u50b3\u64ad\u6b65\u9a5f\u5206\u70ba\u5169\u500b\u7368\u7acb\u7684\u968e\u6bb5\uff0c\u6211\u5011\u53ef\u4ee5\u6e1b\u5c11\u9592\u7f6e\u904b\u7b97\u6642\u9593\u3002\u6211\u5011\u5728\u5404\u7a2e\u6a21\u578b\u67b6\u69cb\u548c\u7ba1\u7dda\u6392\u7a0b\u4e0a\u6e2c\u8a66\u4e86 2BP\uff0c\u5728\u6240\u6709\u60c5\u6cc1\u4e0b\u90fd\u9054\u5230\u4e86\u541e\u5410\u91cf\u7684\u63d0\u5347\u3002\u4f7f\u7528 2BP\uff0c\u6211\u5011\u80fd\u5920\u5728\u4f7f\u7528 4 \u500b GPU \u8a13\u7df4\u5177\u6709 70 \u5104\u500b\u53c3\u6578\u7684\u985e\u4f3c LLaMa \u7684Transformer\u6642\uff0c\u8207\u50b3\u7d71\u65b9\u6cd5\u76f8\u6bd4\uff0c\u541e\u5410\u91cf\u63d0\u5347\u4e86 1.70 \u500d\u3002", "author": "Christopher Rae et.al.", "authors": "Christopher Rae, Joseph K. L. Lee, James Richings", "id": "2405.18047v1", "paper_url": "http://arxiv.org/abs/2405.18047v1", "repo": "null"}}