{"2408.16913": {"publish_time": "2024-08-29", "title": "Analyzing Inference Privacy Risks Through Gradients in Machine Learning", "paper_summary": "In distributed learning settings, models are iteratively updated with shared\ngradients computed from potentially sensitive user data. While previous work\nhas studied various privacy risks of sharing gradients, our paper aims to\nprovide a systematic approach to analyze private information leakage from\ngradients. We present a unified game-based framework that encompasses a broad\nrange of attacks including attribute, property, distributional, and user\ndisclosures. We investigate how different uncertainties of the adversary affect\ntheir inferential power via extensive experiments on five datasets across\nvarious data modalities. Our results demonstrate the inefficacy of solely\nrelying on data aggregation to achieve privacy against inference attacks in\ndistributed learning. We further evaluate five types of defenses, namely,\ngradient pruning, signed gradient descent, adversarial perturbations,\nvariational information bottleneck, and differential privacy, under both static\nand adaptive adversary settings. We provide an information-theoretic view for\nanalyzing the effectiveness of these defenses against inference from gradients.\nFinally, we introduce a method for auditing attribute inference privacy,\nimproving the empirical estimation of worst-case privacy through crafting\nadversarial canary records.", "paper_summary_zh": "\u5728\u5206\u5e03\u5f0f\u5b78\u7fd2\u8a2d\u5b9a\u4e2d\uff0c\u6a21\u578b\u6703\u4f7f\u7528\u7531\u6f5b\u5728\u654f\u611f\u4f7f\u7528\u8005\u8cc7\u6599\u8a08\u7b97\u51fa\u7684\u5171\u4eab\u68af\u5ea6\u53cd\u8986\u66f4\u65b0\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u5df2\u63a2\u8a0e\u5206\u4eab\u68af\u5ea6\u7684\u5404\u7a2e\u96b1\u79c1\u98a8\u96aa\uff0c\u4f46\u6211\u5011\u7684\u8ad6\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u7a2e\u7cfb\u7d71\u6027\u65b9\u6cd5\u4f86\u5206\u6790\u68af\u5ea6\u4e2d\u79c1\u4eba\u8cc7\u8a0a\u7684\u6d29\u6f0f\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7d71\u4e00\u7684\u57fa\u65bc\u904a\u6232\u7684\u67b6\u69cb\uff0c\u5176\u4e2d\u5305\u542b\u5ee3\u6cdb\u7684\u653b\u64ca\uff0c\u5305\u62ec\u5c6c\u6027\u3001\u7279\u6027\u3001\u5206\u4f48\u548c\u4f7f\u7528\u8005\u63ed\u9732\u3002\u6211\u5011\u900f\u904e\u4e94\u500b\u8cc7\u6599\u96c6\u5728\u5404\u7a2e\u8cc7\u6599\u5f62\u5f0f\u4e2d\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u63a2\u8a0e\u5c0d\u624b\u7684\u4e0d\u540c\u4e0d\u78ba\u5b9a\u6027\u5982\u4f55\u5f71\u97ff\u5176\u63a8\u8ad6\u80fd\u529b\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\uff0c\u50c5\u4f9d\u8cf4\u8cc7\u6599\u5f59\u7e3d\u4f86\u5728\u5206\u5e03\u5f0f\u5b78\u7fd2\u4e2d\u5c0d\u6297\u63a8\u8ad6\u653b\u64ca\u7121\u6cd5\u9054\u5230\u96b1\u79c1\u3002\u6211\u5011\u9032\u4e00\u6b65\u8a55\u4f30\u4e94\u7a2e\u985e\u578b\u7684\u9632\u79a6\u63aa\u65bd\uff0c\u5373\u68af\u5ea6\u4fee\u526a\u3001\u6709\u7b26\u865f\u68af\u5ea6\u4e0b\u964d\u3001\u5c0d\u6297\u6027\u64fe\u52d5\u3001\u8b8a\u7570\u8cc7\u8a0a\u74f6\u9838\u548c\u5dee\u5206\u96b1\u79c1\uff0c\u5728\u975c\u614b\u548c\u81ea\u9069\u61c9\u5c0d\u624b\u7684\u8a2d\u5b9a\u4e0b\u3002\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u8cc7\u8a0a\u7406\u8ad6\u89c0\u9ede\uff0c\u7528\u65bc\u5206\u6790\u9019\u4e9b\u9632\u79a6\u63aa\u65bd\u5c0d\u6297\u68af\u5ea6\u63a8\u8ad6\u7684\u6709\u6548\u6027\u3002\u6700\u5f8c\uff0c\u6211\u5011\u4ecb\u7d39\u4e00\u7a2e\u5be9\u8a08\u5c6c\u6027\u63a8\u8ad6\u96b1\u79c1\u7684\u65b9\u6cd5\uff0c\u900f\u904e\u5efa\u7acb\u5c0d\u6297\u6027\u91d1\u7d72\u96c0\u8a18\u9304\u4f86\u6539\u5584\u6700\u5dee\u60c5\u6cc1\u96b1\u79c1\u7684\u7d93\u9a57\u4f30\u8a08\u3002", "author": "Zhuohang Li et.al.", "authors": "Zhuohang Li, Andrew Lowy, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Bradley Malin, Ye Wang", "id": "2408.16913v1", "paper_url": "http://arxiv.org/abs/2408.16913v1", "repo": "null"}}