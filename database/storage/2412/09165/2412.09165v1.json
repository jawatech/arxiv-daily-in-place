{"2412.09165": {"publish_time": "2024-12-12", "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey", "paper_summary": "Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications, such as semantic matching, clustering,\nand information retrieval, continue to rely on text embeddings for their\nefficiency and effectiveness. In this survey, we categorize the interplay\nbetween LLMs and text embeddings into three overarching themes: (1)\nLLM-augmented text embedding, enhancing traditional embedding methods with\nLLMs; (2) LLMs as text embedders, utilizing their innate capabilities for\nembedding generation; and (3) Text embedding understanding with LLMs,\nleveraging LLMs to analyze and interpret embeddings. By organizing these\nefforts based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.", "paper_summary_zh": "\u6587\u672c\u5d4c\u5165\u5df2\u6210\u70ba\u6df1\u5ea6\u5b78\u7fd2\u6642\u4ee3\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u7684\u57fa\u790e\u6280\u8853\uff0c\u63a8\u52d5\u4e86\u5ee3\u6cdb\u4e0b\u6e38\u4efb\u52d9\u7684\u9032\u5c55\u3002\u96d6\u7136\u73fe\u5728\u53ef\u4ee5\u4f7f\u7528\u751f\u6210\u7bc4\u4f8b\u5c0d\u8a31\u591a\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u6311\u6230\u9032\u884c\u5efa\u6a21\uff0c\u4e26\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5f37\u5927\u7684\u751f\u6210\u548c\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u8af8\u5982\u8a9e\u7fa9\u5339\u914d\u3001\u5206\u7fa4\u548c\u8cc7\u8a0a\u6aa2\u7d22\u7b49\u8a31\u591a\u5be6\u7528\u61c9\u7528\u4ecd\u4f9d\u8cf4\u6587\u672c\u5d4c\u5165\u4ee5\u767c\u63ee\u5176\u6548\u7387\u548c\u6548\u80fd\u3002\u5728\u672c\u6b21\u8abf\u67e5\u4e2d\uff0c\u6211\u5011\u5c07 LLM \u548c\u6587\u672c\u5d4c\u5165\u4e4b\u9593\u7684\u4ea4\u4e92\u4f5c\u7528\u5206\u70ba\u4e09\u500b\u4e3b\u8981\u7684\u985e\u5225\uff1a(1) LLM \u589e\u5f37\u6587\u672c\u5d4c\u5165\uff0c\u4f7f\u7528 LLM \u589e\u5f37\u50b3\u7d71\u5d4c\u5165\u65b9\u6cd5\uff1b(2) LLM \u4f5c\u70ba\u6587\u672c\u5d4c\u5165\u5668\uff0c\u5229\u7528\u5176\u5167\u5728\u80fd\u529b\u9032\u884c\u5d4c\u5165\u751f\u6210\uff1b\u4ee5\u53ca (3) \u4f7f\u7528 LLM \u7406\u89e3\u6587\u672c\u5d4c\u5165\uff0c\u5229\u7528 LLM \u5206\u6790\u548c\u8a6e\u91cb\u5d4c\u5165\u3002\u900f\u904e\u6839\u64da\u4e92\u52d5\u6a21\u5f0f\u800c\u975e\u7279\u5b9a\u4e0b\u6e38\u61c9\u7528\u4f86\u7d44\u7e54\u9019\u4e9b\u5de5\u4f5c\uff0c\u6211\u5011\u63d0\u4f9b\u4e86 LLM \u6642\u4ee3\u4f86\u81ea\u5404\u500b\u7814\u7a76\u548c\u61c9\u7528\u9818\u57df\u7684\u8ca2\u737b\u7684\u5275\u65b0\u4e14\u7cfb\u7d71\u6027\u7684\u6982\u89c0\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f37\u8abf\u4e86\u5728\u4f7f\u7528\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u7684 LLM \u524d\u6642\u4ee3\u6301\u7e8c\u5b58\u5728\u4e14\u5c1a\u672a\u89e3\u6c7a\u7684\u6311\u6230\uff0c\u4e26\u63a2\u8a0e\u4e86 LLM \u5e36\u4f86\u7684\u5168\u65b0\u969c\u7919\u3002\u6839\u64da\u6b64\u5206\u6790\uff0c\u6211\u5011\u6982\u8ff0\u4e86\u6587\u672c\u5d4c\u5165\u6f14\u8b8a\u7684\u672a\u4f86\u65b9\u5411\uff0c\u63a2\u8a0e\u4e86 NLP \u5feb\u901f\u767c\u5c55\u9818\u57df\u4e2d\u7684\u7406\u8ad6\u548c\u5be6\u52d9\u6a5f\u6703\u3002", "author": "Zhijie Nie et.al.", "authors": "Zhijie Nie, Zhangchi Feng, Mingxin Li, Cunwang Zhang, Yanzhao Zhang, Dingkun Long, Richong Zhang", "id": "2412.09165v1", "paper_url": "http://arxiv.org/abs/2412.09165v1", "repo": "null"}}