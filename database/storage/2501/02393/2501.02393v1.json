{"2501.02393": {"publish_time": "2025-01-04", "title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers", "paper_summary": "We present an approach to modifying Transformer architectures by integrating\ngraph-aware relational reasoning into the attention mechanism, merging concepts\nfrom graph neural networks and language modeling. Building on the inherent\nconnection between attention and graph theory, we reformulate the Transformer's\nattention mechanism as a graph operation and propose Graph-Aware Isomorphic\nAttention. This method leverages advanced graph modeling strategies, including\nGraph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),\nto enrich the representation of relational structures. Our approach captures\ncomplex dependencies and generalizes across tasks, as evidenced by a reduced\ngeneralization gap and improved learning performance. Additionally, we expand\nthe concept of graph-aware attention to introduce Sparse GIN-Attention, a\nfine-tuning approach that employs sparse GINs. By interpreting attention\nmatrices as sparse adjacency graphs, this technique enhances the adaptability\nof pre-trained foundational models with minimal computational overhead,\nendowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning\nachieves improved training dynamics and better generalization compared to\nalternative methods like low-rank adaption (LoRA). We discuss latent graph-like\nstructures within traditional attention mechanisms, offering a new lens through\nwhich Transformers can be understood. By evolving Transformers as hierarchical\nGIN models for relational reasoning. This perspective suggests profound\nimplications for foundational model development, enabling the design of\narchitectures that dynamically adapt to both local and global dependencies.\nApplications in bioinformatics, materials science, language modeling, and\nbeyond could benefit from this synthesis of relational and sequential data\nmodeling, setting the stage for interpretable and generalizable modeling\nstrategies.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u4fee\u6539 Transformer \u67b6\u69cb\u7684\u65b9\u6cd5\uff0c\u65b9\u6cd5\u662f\u5c07\u5716\u611f\u77e5\u95dc\u806f\u63a8\u7406\u6574\u5408\u5230\u6ce8\u610f\u529b\u6a5f\u5236\u4e2d\uff0c\u878d\u5408\u5716\u795e\u7d93\u7db2\u8def\u548c\u8a9e\u8a00\u5efa\u6a21\u7684\u6982\u5ff5\u3002\u6211\u5011\u5efa\u7acb\u5728\u6ce8\u610f\u529b\u548c\u5716\u8ad6\u4e4b\u9593\u7684\u5167\u5728\u806f\u7e6b\u4e4b\u4e0a\uff0c\u5c07 Transformer \u7684\u6ce8\u610f\u529b\u6a5f\u5236\u91cd\u65b0\u8868\u8ff0\u70ba\u5716\u5f62\u904b\u7b97\uff0c\u4e26\u63d0\u51fa\u5716\u611f\u77e5\u540c\u69cb\u6ce8\u610f\u529b\u3002\u6b64\u65b9\u6cd5\u5229\u7528\u5148\u9032\u7684\u5716\u5f62\u5efa\u6a21\u7b56\u7565\uff0c\u5305\u62ec\u5716\u540c\u69cb\u7db2\u8def (GIN) \u548c\u4e3b\u9130\u57df\u805a\u5408 (PNA)\uff0c\u4ee5\u8c50\u5bcc\u95dc\u4fc2\u7d50\u69cb\u7684\u8868\u793a\u3002\u6211\u5011\u7684\u505a\u6cd5\u6355\u6349\u4e86\u8907\u96dc\u7684\u4f9d\u8cf4\u95dc\u4fc2\uff0c\u4e26\u5728\u5404\u9805\u4efb\u52d9\u4e2d\u9032\u884c\u6982\u5316\uff0c\u9019\u5f9e\u7e2e\u5c0f\u7684\u6982\u5316\u5dee\u8ddd\u548c\u6539\u5584\u7684\u5b78\u7fd2\u8868\u73fe\u4e2d\u53ef\u4ee5\u5f97\u5230\u8b49\u660e\u3002\u6b64\u5916\uff0c\u6211\u5011\u64f4\u5c55\u4e86\u5716\u611f\u77e5\u6ce8\u610f\u529b\u7684\u6982\u5ff5\uff0c\u5f15\u5165\u4e86\u7a00\u758f GIN \u6ce8\u610f\u529b\uff0c\u9019\u662f\u4e00\u7a2e\u63a1\u7528\u7a00\u758f GIN \u7684\u5fae\u8abf\u65b9\u6cd5\u3002\u901a\u904e\u5c07\u6ce8\u610f\u529b\u77e9\u9663\u89e3\u91cb\u70ba\u7a00\u758f\u9130\u63a5\u5716\uff0c\u6b64\u6280\u8853\u589e\u5f37\u4e86\u9810\u5148\u8a13\u7df4\u597d\u7684\u57fa\u790e\u6a21\u578b\u7684\u9069\u61c9\u6027\uff0c\u540c\u6642\u5c07\u8a08\u7b97\u958b\u92b7\u964d\u81f3\u6700\u4f4e\uff0c\u8ce6\u4e88\u5b83\u5011\u5716\u611f\u77e5\u80fd\u529b\u3002\u8207\u4f4e\u79e9\u9069\u61c9 (LoRA) \u7b49\u66ff\u4ee3\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7a00\u758f GIN \u6ce8\u610f\u529b\u5fae\u8abf\u5be6\u73fe\u4e86\u6539\u9032\u7684\u8a13\u7df4\u52d5\u614b\u548c\u66f4\u597d\u7684\u6982\u5316\u3002\u6211\u5011\u8a0e\u8ad6\u4e86\u50b3\u7d71\u6ce8\u610f\u529b\u6a5f\u5236\u4e2d\u7684\u6f5b\u5728\u5716\u5f62\u7d50\u69cb\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u65b0\u7684\u8996\u89d2\uff0c\u901a\u904e\u5b83\u53ef\u4ee5\u7406\u89e3 Transformer\u3002\u901a\u904e\u5c07 Transformer \u6f14\u5316\u70ba\u7528\u65bc\u95dc\u4fc2\u63a8\u7406\u7684\u5206\u5c64 GIN \u6a21\u578b\u3002\u9019\u7a2e\u89c0\u9ede\u5c0d\u57fa\u790e\u6a21\u578b\u958b\u767c\u63d0\u51fa\u4e86\u6df1\u9060\u7684\u5f71\u97ff\uff0c\u80fd\u5920\u8a2d\u8a08\u51fa\u52d5\u614b\u9069\u61c9\u5c40\u90e8\u548c\u5168\u5c40\u4f9d\u8cf4\u95dc\u4fc2\u7684\u67b6\u69cb\u3002\u751f\u7269\u8cc7\u8a0a\u5b78\u3001\u6750\u6599\u79d1\u5b78\u3001\u8a9e\u8a00\u5efa\u6a21\u53ca\u5176\u4ed6\u9818\u57df\u7684\u61c9\u7528\u53ef\u4ee5\u53d7\u76ca\u65bc\u9019\u7a2e\u95dc\u4fc2\u548c\u5e8f\u5217\u6578\u64da\u5efa\u6a21\u7684\u7d9c\u5408\uff0c\u70ba\u53ef\u89e3\u91cb\u4e14\u53ef\u6982\u5316\u7684\u5efa\u6a21\u7b56\u7565\u5960\u5b9a\u57fa\u790e\u3002</paragraph>", "author": "Markus J. Buehler et.al.", "authors": "Markus J. Buehler", "id": "2501.02393v1", "paper_url": "http://arxiv.org/abs/2501.02393v1", "repo": "https://github.com/lamm-mit/graph-aware-transformers"}}