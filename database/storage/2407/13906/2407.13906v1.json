{"2407.13906": {"publish_time": "2024-07-18", "title": "Crafting Efficient Fine-Tuning Strategies for Large Language Models", "paper_summary": "This paper addresses the challenges of efficiently fine-tuning large language\nmodels (LLMs) by exploring data efficiency and hyperparameter optimization. We\ninvestigate the minimum data required for effective fine-tuning and propose a\nnovel hyperparameter optimization method that leverages early-stage model\nperformance. Our experiments demonstrate that fine-tuning with as few as 200\nsamples can improve model accuracy from 70\\% to 88\\% in a product attribute\nextraction task. We identify a saturation point of approximately 6,500 samples,\nbeyond which additional data yields diminishing returns. Our proposed bayesian\nhyperparameter optimization method, which evaluates models at 20\\% of total\ntraining time, correlates strongly with final model performance, with 4 out of\n5 top early-stage models remaining in the top 5 at completion. This approach\nled to a 2\\% improvement in accuracy over baseline models when evaluated on an\nindependent test set. These findings offer actionable insights for\npractitioners, potentially reducing computational load and dependency on\nextensive datasets while enhancing overall performance of fine-tuned LLMs.", "paper_summary_zh": "\u672c\u6587\u63a2\u8a0e\u6709\u6548\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6240\u9762\u81e8\u7684\u6311\u6230\uff0c\u65b9\u6cd5\u662f\u63a2\u7d22\u8cc7\u6599\u6548\u7387\u548c\u8d85\u53c3\u6578\u6700\u4f73\u5316\u3002\u6211\u5011\u63a2\u8a0e\u6709\u6548\u5fae\u8abf\u6240\u9700\u7684\u6700\u4f4e\u8cc7\u6599\u91cf\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u5275\u65b0\u7684\u8d85\u53c3\u6578\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u5229\u7528\u65e9\u671f\u968e\u6bb5\u7684\u6a21\u578b\u6548\u80fd\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u5728\u7522\u54c1\u5c6c\u6027\u8403\u53d6\u4efb\u52d9\u4e2d\uff0c\u4f7f\u7528\u50c5 200 \u500b\u7bc4\u4f8b\u9032\u884c\u5fae\u8abf\uff0c\u5c31\u80fd\u5c07\u6a21\u578b\u6e96\u78ba\u5ea6\u5f9e 70% \u63d0\u5347\u81f3 88%\u3002\u6211\u5011\u627e\u51fa\u4e00\u500b\u7d04 6,500 \u500b\u7bc4\u4f8b\u7684\u98fd\u548c\u9ede\uff0c\u8d85\u904e\u6b64\u9ede\u5f8c\uff0c\u984d\u5916\u7684\u8cc7\u6599\u6703\u7522\u751f\u905e\u6e1b\u5831\u916c\u3002\u6211\u5011\u63d0\u51fa\u7684\u8c9d\u6c0f\u8d85\u53c3\u6578\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u6703\u5728\u7e3d\u8a13\u7df4\u6642\u9593\u7684 20% \u8a55\u4f30\u6a21\u578b\uff0c\u8207\u6700\u7d42\u6a21\u578b\u6548\u80fd\u9ad8\u5ea6\u76f8\u95dc\uff0c5 \u500b\u9802\u5c16\u65e9\u671f\u968e\u6bb5\u6a21\u578b\u4e2d\u6709 4 \u500b\u5728\u5b8c\u6210\u6642\u4ecd\u7dad\u6301\u5728\u9802\u5c16 5 \u540d\u3002\u6b64\u65b9\u6cd5\u5728\u7368\u7acb\u6e2c\u8a66\u96c6\u4e0a\u8a55\u4f30\u6642\uff0c\u6e96\u78ba\u5ea6\u6bd4\u57fa\u6e96\u6a21\u578b\u63d0\u5347\u4e86 2%\u3002\u9019\u4e9b\u767c\u73fe\u70ba\u5be6\u52d9\u5de5\u4f5c\u8005\u63d0\u4f9b\u53ef\u884c\u7684\u898b\u89e3\uff0c\u6709\u53ef\u80fd\u5728\u63d0\u5347\u5fae\u8abf LLM \u7684\u6574\u9ad4\u6548\u80fd\u7684\u540c\u6642\uff0c\u964d\u4f4e\u904b\u7b97\u8ca0\u8f09\u548c\u5c0d\u9f90\u5927\u8cc7\u6599\u96c6\u7684\u4f9d\u8cf4\u6027\u3002", "author": "Michael Oliver et.al.", "authors": "Michael Oliver, Guan Wang", "id": "2407.13906v1", "paper_url": "http://arxiv.org/abs/2407.13906v1", "repo": "null"}}