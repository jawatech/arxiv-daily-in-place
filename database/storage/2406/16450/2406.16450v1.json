{"2406.16450": {"publish_time": "2024-06-24", "title": "Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers", "paper_summary": "State-of-the-art results in large language models (LLMs) often rely on scale,\nwhich becomes computationally expensive. This has sparked a research agenda to\nreduce these models' parameter count and computational costs without\nsignificantly impacting their performance. Our study focuses on\ntransformer-based LLMs, specifically targeting the computationally intensive\nfeedforward networks (FFN), which are less studied than attention blocks. We\nconsider three candidate linear layer approximations in the FFN by combining\nefficient low-rank and block-diagonal matrices. In contrast to many previous\nworks that examined these approximations, our study i) explores these\nstructures from the training-from-scratch perspective, ii) scales up to 1.3B\nparameters, and iii) is conducted within recent Transformer-based LLMs rather\nthan convolutional architectures. We first demonstrate they can lead to actual\ncomputational gains in various scenarios, including online decoding when using\na pre-merge technique. Additionally, we propose a novel training regime, called\n\\textit{self-guided training}, aimed at improving the poor training dynamics\nthat these approximations exhibit when used from initialization. Experiments on\nthe large RefinedWeb dataset show that our methods are both efficient and\neffective for training and inference. Interestingly, these structured FFNs\nexhibit steeper scaling curves than the original models. Further applying\nself-guided training to the structured matrices with 32\\% FFN parameters and\n2.5$\\times$ speed-up enables only a 0.4 perplexity increase under the same\ntraining FLOPs. Finally, we develop the wide and structured networks surpassing\nthe current medium-sized and large-sized Transformer in perplexity and\nthroughput performance. Our code is available at\n\\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u7d50\u679c\u901a\u5e38\u4f9d\u8cf4\u65bc\u898f\u6a21\uff0c\u9019\u5728\u8a08\u7b97\u4e0a\u6703\u5f88\u6602\u8cb4\u3002\u9019\u6fc0\u767c\u4e86\u4e00\u9805\u7814\u7a76\u8b70\u7a0b\uff0c\u65e8\u5728\u6e1b\u5c11\u9019\u4e9b\u6a21\u578b\u7684\u53c3\u6578\u8a08\u6578\u548c\u8a08\u7b97\u6210\u672c\uff0c\u800c\u4e0d\u6703\u986f\u8457\u5f71\u97ff\u5176\u6548\u80fd\u3002\u6211\u5011\u7684\u7814\u7a76\u91cd\u9ede\u662f\u57fa\u65bcTransformer\u7684 LLM\uff0c\u7279\u5225\u91dd\u5c0d\u8a08\u7b97\u5bc6\u96c6\u7684\u524d\u994b\u7db2\u8def (FFN)\uff0c\u5176\u7814\u7a76\u8f03\u6ce8\u610f\u529b\u5340\u584a\u5c11\u3002\u6211\u5011\u8003\u616e\u5728 FFN \u4e2d\u7d50\u5408\u6709\u6548\u4f4e\u79e9\u548c\u5340\u584a\u5c0d\u89d2\u77e9\u9663\uff0c\u4f86\u8003\u616e\u4e09\u500b\u5019\u9078\u7dda\u6027\u5c64\u8fd1\u4f3c\u3002\u8207\u8a31\u591a\u5148\u524d\u6aa2\u9a57\u9019\u4e9b\u8fd1\u4f3c\u7684\u7814\u7a76\u76f8\u53cd\uff0c\u6211\u5011\u7684\u7814\u7a76 i) \u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u7684\u89d2\u5ea6\u63a2\u8a0e\u9019\u4e9b\u7d50\u69cb\uff0cii) \u64f4\u5c55\u5230 1.3B \u53c3\u6578\uff0c\u4ee5\u53ca iii) \u5728\u6700\u8fd1\u7684\u57fa\u65bcTransformer\u7684 LLM \u4e2d\u9032\u884c\uff0c\u800c\u4e0d\u662f\u5377\u7a4d\u67b6\u69cb\u3002\u6211\u5011\u9996\u5148\u8b49\u660e\u5b83\u5011\u53ef\u4ee5\u5728\u5404\u7a2e\u5834\u666f\u4e2d\u5e36\u4f86\u5be6\u969b\u7684\u8a08\u7b97\u6536\u76ca\uff0c\u5305\u62ec\u5728\u4f7f\u7528\u9810\u5408\u4f75\u6280\u8853\u6642\u7684\u7dda\u4e0a\u89e3\u78bc\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba\u300c\u81ea\u6211\u5f15\u5c0e\u8a13\u7df4\u300d\u7684\u65b0\u8a13\u7df4\u6a5f\u5236\uff0c\u65e8\u5728\u6539\u5584\u9019\u4e9b\u8fd1\u4f3c\u5f9e\u521d\u59cb\u5316\u4f7f\u7528\u6642\u8868\u73fe\u51fa\u7684\u4e0d\u826f\u8a13\u7df4\u52d5\u614b\u3002\u5728\u5927\u578b RefinedWeb \u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u8a13\u7df4\u548c\u63a8\u7406\u4e0a\u90fd\u5f88\u6709\u6548\u7387\u3002\u6709\u8da3\u7684\u662f\uff0c\u9019\u4e9b\u7d50\u69cb\u5316\u7684 FFN \u5c55\u73fe\u51fa\u6bd4\u539f\u59cb\u6a21\u578b\u66f4\u9661\u5ced\u7684\u64f4\u5c55\u66f2\u7dda\u3002\u9032\u4e00\u6b65\u5c07\u81ea\u6211\u5f15\u5c0e\u8a13\u7df4\u61c9\u7528\u65bc\u5177\u6709 32% FFN \u53c3\u6578\u548c 2.5 \u500d\u52a0\u901f\u7684\u7d50\u69cb\u5316\u77e9\u9663\uff0c\u5728\u76f8\u540c\u7684\u8a13\u7df4 FLOP \u4e0b\u50c5\u80fd\u589e\u52a0 0.4 \u500b\u56f0\u60d1\u5ea6\u3002\u6700\u5f8c\uff0c\u6211\u5011\u958b\u767c\u4e86\u5ee3\u6cdb\u4e14\u7d50\u69cb\u5316\u7684\u7db2\u8def\uff0c\u5728\u56f0\u60d1\u5ea6\u548c\u541e\u5410\u91cf\u6548\u80fd\u4e0a\u8d85\u8d8a\u4e86\u76ee\u524d\u7684\u4e2d\u578b\u548c\u5927\u578bTransformer\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728\n\\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main} \u53d6\u5f97\u3002</paragraph>", "author": "Xiuying Wei et.al.", "authors": "Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre", "id": "2406.16450v1", "paper_url": "http://arxiv.org/abs/2406.16450v1", "repo": "null"}}