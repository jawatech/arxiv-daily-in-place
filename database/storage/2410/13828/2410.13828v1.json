{"2410.13828": {"publish_time": "2024-10-17", "title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement", "paper_summary": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant\napproach for language model (LM) alignment. At its core, RLHF uses a\nmargin-based loss for preference optimization, specifying ideal LM behavior\nonly by the difference between preferred and dispreferred responses. In this\npaper, we identify a common pitfall of margin-based methods -- the\nunder-specification of ideal LM behavior on preferred and dispreferred\nresponses individually, which leads to two unintended consequences as the\nmargin increases: (1) The probability of dispreferred (e.g., unsafe) responses\nmay increase, resulting in potential safety alignment failures. (2) The\nprobability of preferred responses may decrease, even when those responses are\nideal. We demystify the reasons behind these problematic behaviors:\nmargin-based losses couple the change in the preferred probability to the\ngradient of the dispreferred one, and vice versa, often preventing the\npreferred probability from increasing while the dispreferred one decreases, and\nthus causing a synchronized increase or decrease in both probabilities. We term\nthis effect, inherent in margin-based objectives, gradient entanglement.\nFormally, we derive conditions for general margin-based alignment objectives\nunder which gradient entanglement becomes concerning: the inner product of the\ngradients of preferred and dispreferred log-probabilities is large relative to\nthe individual gradient norms. We theoretically investigate why such inner\nproducts can be large when aligning language models and empirically validate\nour findings. Empirical implications of our framework extend to explaining\nimportant differences in the training dynamics of various preference\noptimization algorithms, and suggesting potential algorithm designs to mitigate\nthe under-specification issue of margin-based methods and thereby improving\nlanguage model alignment.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2\u4f86\u81ea\u4eba\u985e\u56de\u994b (RLHF) \u5df2\u6210\u70ba\u8a9e\u8a00\u6a21\u578b (LM) \u6821\u6e96\u7684\u4e3b\u8981\u65b9\u6cd5\u3002\u5728\u6838\u5fc3\u90e8\u5206\uff0cRLHF \u4f7f\u7528\u57fa\u65bc\u908a\u754c\u7684\u640d\u5931\u9032\u884c\u504f\u597d\u6700\u4f73\u5316\uff0c\u50c5\u900f\u904e\u504f\u597d\u548c\u975e\u504f\u597d\u56de\u61c9\u4e4b\u9593\u7684\u5dee\u7570\u4f86\u6307\u5b9a\u7406\u60f3\u7684 LM \u884c\u70ba\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8fa8\u8b58\u51fa\u57fa\u65bc\u908a\u754c\u65b9\u6cd5\u7684\u5e38\u898b\u9677\u9631\uff0c\u4e5f\u5c31\u662f\u5728\u504f\u597d\u548c\u975e\u504f\u597d\u56de\u61c9\u4e2d\u5c0d\u7406\u60f3 LM \u884c\u70ba\u7684\u898f\u683c\u4e0d\u8db3\uff0c\u9019\u6703\u96a8\u8457\u908a\u754c\u589e\u52a0\u800c\u5c0e\u81f4\u5169\u500b\u610f\u5916\u7684\u5f8c\u679c\uff1a(1) \u975e\u504f\u597d\uff08\u4f8b\u5982\u4e0d\u5b89\u5168\uff09\u56de\u61c9\u7684\u6a5f\u7387\u53ef\u80fd\u6703\u589e\u52a0\uff0c\u5c0e\u81f4\u6f5b\u5728\u7684\u5b89\u5168\u6821\u6e96\u5931\u6557\u3002(2) \u504f\u597d\u56de\u61c9\u7684\u6a5f\u7387\u53ef\u80fd\u6703\u964d\u4f4e\uff0c\u5373\u4f7f\u90a3\u4e9b\u56de\u61c9\u662f\u7406\u60f3\u7684\u3002\u6211\u5011\u63ed\u958b\u9019\u4e9b\u554f\u984c\u884c\u70ba\u80cc\u5f8c\u7684\u539f\u56e0\uff1a\u57fa\u65bc\u908a\u754c\u7684\u640d\u5931\u5c07\u504f\u597d\u6a5f\u7387\u7684\u6539\u8b8a\u8207\u975e\u504f\u597d\u6a5f\u7387\u7684\u68af\u5ea6\u7d50\u5408\uff0c\u53cd\u4e4b\u4ea6\u7136\uff0c\u901a\u5e38\u6703\u963b\u6b62\u504f\u597d\u6a5f\u7387\u589e\u52a0\uff0c\u540c\u6642\u975e\u504f\u597d\u6a5f\u7387\u964d\u4f4e\uff0c\u5f9e\u800c\u5c0e\u81f4\u5169\u500b\u6a5f\u7387\u540c\u6b65\u589e\u52a0\u6216\u964d\u4f4e\u3002\u6211\u5011\u5c07\u9019\u7a2e\u56fa\u6709\u65bc\u57fa\u65bc\u908a\u754c\u7684\u76ee\u6a19\u51fd\u6578\u7684\u6548\u61c9\u7a31\u70ba\u68af\u5ea6\u7cfe\u7e8f\u3002\u6b63\u5f0f\u4f86\u8aaa\uff0c\u6211\u5011\u63a8\u5c0e\u51fa\u4e00\u822c\u57fa\u65bc\u908a\u754c\u7684\u6821\u6e96\u76ee\u6a19\u51fd\u6578\u7684\u689d\u4ef6\uff0c\u5728\u8a72\u689d\u4ef6\u4e0b\u68af\u5ea6\u7cfe\u7e8f\u6703\u4ee4\u4eba\u64d4\u6182\uff1a\u504f\u597d\u548c\u975e\u504f\u597d\u5c0d\u6578\u6a5f\u7387\u7684\u68af\u5ea6\u5167\u7a4d\u76f8\u5c0d\u65bc\u500b\u5225\u68af\u5ea6\u7bc4\u6578\u5f88\u5927\u3002\u6211\u5011\u5728\u7406\u8ad6\u4e0a\u63a2\u8a0e\u70ba\u4ec0\u9ebc\u5728\u6821\u6e96\u8a9e\u8a00\u6a21\u578b\u6642\u9019\u7a2e\u5167\u7a4d\u6703\u5f88\u5927\uff0c\u4e26\u6839\u64da\u7d93\u9a57\u9a57\u8b49\u6211\u5011\u7684\u767c\u73fe\u3002\u6211\u5011\u67b6\u69cb\u7684\u7d93\u9a57\u610f\u7fa9\u5ef6\u4f38\u5230\u89e3\u91cb\u5404\u7a2e\u504f\u597d\u6700\u4f73\u5316\u6f14\u7b97\u6cd5\u8a13\u7df4\u52d5\u614b\u4e2d\u7684\u91cd\u8981\u5dee\u7570\uff0c\u4e26\u5efa\u8b70\u6f5b\u5728\u7684\u6f14\u7b97\u6cd5\u8a2d\u8a08\u4ee5\u6e1b\u8f15\u57fa\u65bc\u908a\u754c\u65b9\u6cd5\u7684\u898f\u683c\u4e0d\u8db3\u554f\u984c\uff0c\u5f9e\u800c\u6539\u5584\u8a9e\u8a00\u6a21\u578b\u6821\u6e96\u3002", "author": "Hui Yuan et.al.", "authors": "Hui Yuan, Yifan Zeng, Yue Wu, Huazheng Wang, Mengdi Wang, Liu Leqi", "id": "2410.13828v1", "paper_url": "http://arxiv.org/abs/2410.13828v1", "repo": "null"}}