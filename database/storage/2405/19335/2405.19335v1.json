{"2405.19335": {"publish_time": "2024-05-29", "title": "X-VILA: Cross-Modality Alignment for Large Language Model", "paper_summary": "We introduce X-VILA, an omni-modality model designed to extend the\ncapabilities of large language models (LLMs) by incorporating image, video, and\naudio modalities. By aligning modality-specific encoders with LLM inputs and\ndiffusion decoders with LLM outputs, X-VILA achieves cross-modality\nunderstanding, reasoning, and generation. To facilitate this cross-modality\nalignment, we curate an effective interleaved any-to-any modality\ninstruction-following dataset. Furthermore, we identify a significant problem\nwith the current cross-modality alignment method, which results in visual\ninformation loss. To address the issue, we propose a visual alignment mechanism\nwith a visual embedding highway module. We then introduce a resource-efficient\nrecipe for training X-VILA, that exhibits proficiency in any-to-any modality\nconversation, surpassing previous approaches by large margins. X-VILA also\nshowcases emergent properties across modalities even in the absence of similar\ntraining data. The project will be made open-source.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39 X-VILA\uff0c\u4e00\u7a2e\u5168\u6a21\u614b\u6a21\u578b\uff0c\u65e8\u5728\u900f\u904e\u7d0d\u5165\u5f71\u50cf\u3001\u5f71\u7247\u548c\u97f3\u8a0a\u6a21\u614b\u4f86\u64f4\u5145\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u529f\u80fd\u3002\u900f\u904e\u5c07\u7279\u5b9a\u6a21\u614b\u7684\u7de8\u78bc\u5668\u8207 LLM \u8f38\u5165\u5c0d\u9f4a\uff0c\u4ee5\u53ca\u5c07\u64f4\u6563\u89e3\u78bc\u5668\u8207 LLM \u8f38\u51fa\u5c0d\u9f4a\uff0cX-VILA \u9054\u5230\u4e86\u8de8\u6a21\u614b\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u3002\u70ba\u4e86\u4fc3\u9032\u9019\u7a2e\u8de8\u6a21\u614b\u5c0d\u9f4a\uff0c\u6211\u5011\u7b56\u5283\u4e86\u4e00\u500b\u6709\u6548\u7684\u4ea4\u932f\u4efb\u4f55\u5c0d\u4efb\u4f55\u6a21\u614b\u6307\u4ee4\u9075\u5faa\u8cc7\u6599\u96c6\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u76ee\u524d\u8de8\u6a21\u614b\u5c0d\u9f4a\u65b9\u6cd5\u5b58\u5728\u4e00\u500b\u91cd\u5927\u554f\u984c\uff0c\u9019\u6703\u5c0e\u81f4\u8996\u89ba\u8cc7\u8a0a\u907a\u5931\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5e36\u6709\u8996\u89ba\u5d4c\u5165\u516c\u8def\u6a21\u7d44\u7684\u8996\u89ba\u5c0d\u9f4a\u6a5f\u5236\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u8a13\u7df4 X-VILA \u7684\u8cc7\u6e90\u6709\u6548\u914d\u65b9\uff0c\u5b83\u5c55\u793a\u4e86\u5728\u4efb\u4f55\u5c0d\u4efb\u4f55\u6a21\u614b\u5c0d\u8a71\u4e2d\u7684\u719f\u7df4\u5ea6\uff0c\u9060\u9060\u8d85\u904e\u4e86\u4ee5\u524d\u7684\u65b9\u6cd5\u3002\u5373\u4f7f\u5728\u6c92\u6709\u985e\u4f3c\u8a13\u7df4\u8cc7\u6599\u7684\u60c5\u6cc1\u4e0b\uff0cX-VILA \u4e5f\u5c55\u793a\u4e86\u8de8\u6a21\u614b\u7684\u65b0\u8208\u5c6c\u6027\u3002\u8a72\u5c08\u6848\u5c07\u958b\u653e\u539f\u59cb\u78bc\u3002", "author": "Hanrong Ye et.al.", "authors": "Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, Hongxu Yin", "id": "2405.19335v1", "paper_url": "http://arxiv.org/abs/2405.19335v1", "repo": "null"}}