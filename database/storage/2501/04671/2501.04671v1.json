{"2501.04671": {"publish_time": "2025-01-08", "title": "DRIVINGVQA: Analyzing Visual Chain-of-Thought Reasoning of Vision Language Models in Real-World Scenarios with Driving Theory Tests", "paper_summary": "Large vision-language models (LVLMs) augment language models with visual\nunderstanding, enabling multimodal reasoning. However, due to the modality gap\nbetween textual and visual data, they often face significant challenges, such\nas over-reliance on text priors, hallucinations, and limited capacity for\ncomplex visual reasoning. Existing benchmarks to evaluate visual reasoning in\nLVLMs often rely on schematic or synthetic images and on imprecise\nmachine-generated explanations. To bridge the modality gap, we present\nDrivingVQA, a new benchmark derived from driving theory tests to evaluate\nvisual chain-of-thought reasoning in complex real-world scenarios. It offers\n3,931 expert-crafted multiple-choice problems and interleaved explanations\ngrounded with entities relevant to the reasoning process. We leverage this\ndataset to perform an extensive study of LVLMs' ability to reason about complex\nvisual scenarios. Our experiments reveal that open-source and proprietary LVLMs\nstruggle with visual chain-of-thought reasoning under zero-shot settings. We\ninvestigate training strategies that leverage relevant entities to improve\nvisual reasoning. Notably, we observe a performance boost of up to 7\\% when\nreasoning over image tokens of cropped regions tied to these entities.", "paper_summary_zh": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u4f7f\u7528\u89c6\u89c9\u7406\u89e3\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u63a8\u7406\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u5f02\uff0c\u5b83\u4eec\u901a\u5e38\u9762\u4e34\u7740\u91cd\u5927\u6311\u6218\uff0c\u4f8b\u5982\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u5148\u9a8c\u3001\u5e7b\u89c9\u4ee5\u53ca\u590d\u6742\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709\u7684\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30 LVLMs \u4e2d\u7684\u89c6\u89c9\u63a8\u7406\uff0c\u901a\u5e38\u4f9d\u8d56\u4e8e\u793a\u610f\u56fe\u6216\u5408\u6210\u56fe\u50cf\u4ee5\u53ca\u4e0d\u7cbe\u786e\u7684\u673a\u5668\u751f\u6210\u7684\u89e3\u91ca\u3002\u4e3a\u4e86\u5f25\u5408\u6a21\u6001\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DrivingVQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u6e90\u81ea\u9a7e\u9a76\u7406\u8bba\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u601d\u7ef4\u94fe\u63a8\u7406\u3002\u5b83\u63d0\u4f9b\u4e86 3,931 \u4e2a\u4e13\u5bb6\u5236\u4f5c\u7684\u591a\u9879\u9009\u62e9\u9898\u548c\u7a7f\u63d2\u7684\u89e3\u91ca\uff0c\u8fd9\u4e9b\u89e3\u91ca\u4ee5\u4e0e\u63a8\u7406\u8fc7\u7a0b\u76f8\u5173\u7684\u5b9e\u4f53\u4e3a\u57fa\u7840\u3002\u6211\u4eec\u5229\u7528\u6b64\u6570\u636e\u96c6\u5bf9 LVLMs \u63a8\u7406\u590d\u6742\u89c6\u89c9\u573a\u666f\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f00\u6e90\u548c\u4e13\u6709 LVLMs \u5728\u96f6\u6b21\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u96be\u4ee5\u8fdb\u884c\u89c6\u89c9\u601d\u7ef4\u94fe\u63a8\u7406\u3002\u6211\u4eec\u7814\u7a76\u4e86\u5229\u7528\u76f8\u5173\u5b9e\u4f53\u6765\u6539\u5584\u89c6\u89c9\u63a8\u7406\u7684\u8bad\u7ec3\u7b56\u7565\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u5bf9\u4e0e\u8fd9\u4e9b\u5b9e\u4f53\u76f8\u5173\u7684\u88c1\u526a\u533a\u57df\u7684\u56fe\u50cf\u6807\u8bb0\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe 7%\u3002", "author": "Charles Corbi\u00e8re et.al.", "authors": "Charles Corbi\u00e8re, Simon Roburin, Syrielle Montariol, Antoine Bosselut, Alexandre Alahi", "id": "2501.04671v1", "paper_url": "http://arxiv.org/abs/2501.04671v1", "repo": "null"}}