{"2406.07119": {"publish_time": "2024-06-11", "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text", "paper_summary": "In this work, we propose a two-stage sign language production (SLP) paradigm\nthat first encodes sign language sequences into discrete codes and then\nautoregressively generates sign language from text based on the learned\ncodebook. However, existing vector quantization (VQ) methods are fixed-length\nencodings, overlooking the uneven information density in sign language, which\nleads to under-encoding of important regions and over-encoding of unimportant\nregions. To address this issue, we propose a novel dynamic vector quantization\n(DVA-VAE) model that can dynamically adjust the encoding length based on the\ninformation density in sign language to achieve accurate and compact encoding.\nThen, a GPT-like model learns to generate code sequences and their\ncorresponding durations from spoken language text. Extensive experiments\nconducted on the PHOENIX14T dataset demonstrate the effectiveness of our\nproposed method. To promote sign language research, we propose a new large\nGerman sign language dataset, PHOENIX-News, which contains 486 hours of sign\nlanguage videos, audio, and transcription texts.Experimental analysis on\nPHOENIX-News shows that the performance of our model can be further improved by\nincreasing the size of the training data. Our project homepage is\nhttps://t2sgpt-demo.yinaoxiong.cn.", "paper_summary_zh": "<paragraph>\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5169\u968e\u6bb5\u624b\u8a9e\u7522\u751f (SLP) \u5178\u7bc4\uff0c\u9996\u5148\u5c07\u624b\u8a9e\u5e8f\u5217\u7de8\u78bc\u6210\u96e2\u6563\u4ee3\u78bc\uff0c\u7136\u5f8c\u6839\u64da\u5b78\u7fd2\u5230\u7684\u4ee3\u78bc\u7c3f\u81ea\u52d5\u56de\u6b78\u5730\u5f9e\u6587\u5b57\u7522\u751f\u624b\u8a9e\u3002\u4f46\u662f\uff0c\u73fe\u6709\u7684\u5411\u91cf\u91cf\u5316 (VQ) \u65b9\u6cd5\u662f\u5b9a\u9577\u7de8\u78bc\uff0c\u5ffd\u7565\u4e86\u624b\u8a9e\u4e2d\u4e0d\u5747\u52fb\u7684\u8cc7\u8a0a\u5bc6\u5ea6\uff0c\u5c0e\u81f4\u91cd\u8981\u5340\u57df\u7de8\u78bc\u4e0d\u8db3\uff0c\u800c\u5c0d\u4e0d\u91cd\u8981\u5340\u57df\u7de8\u78bc\u904e\u591a\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u52d5\u614b\u5411\u91cf\u91cf\u5316 (DVA-VAE) \u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u6839\u64da\u624b\u8a9e\u4e2d\u7684\u8cc7\u8a0a\u5bc6\u5ea6\u52d5\u614b\u8abf\u6574\u7de8\u78bc\u9577\u5ea6\uff0c\u4ee5\u5be6\u73fe\u6e96\u78ba\u4e14\u7dca\u6e4a\u7684\u7de8\u78bc\u3002\u7136\u5f8c\uff0c\u4e00\u500b\u985e\u4f3c GPT \u7684\u6a21\u578b\u5b78\u7fd2\u5f9e\u53e3\u8a9e\u6587\u672c\u7522\u751f\u4ee3\u78bc\u5e8f\u5217\u53ca\u5176\u5c0d\u61c9\u7684\u6301\u7e8c\u6642\u9593\u3002\u5728 PHOENIX14T \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u70ba\u4e86\u4fc3\u9032\u624b\u8a9e\u7814\u7a76\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684\u5927\u578b\u5fb7\u8a9e\u624b\u8a9e\u8cc7\u6599\u96c6 PHOENIX-News\uff0c\u5176\u4e2d\u5305\u542b 486 \u5c0f\u6642\u7684\u624b\u8a9e\u5f71\u7247\u3001\u97f3\u8a0a\u548c\u8f49\u9304\u6587\u5b57\u3002\u5728 PHOENIX-News \u4e0a\u7684\u5be6\u9a57\u5206\u6790\u8868\u660e\uff0c\u900f\u904e\u589e\u52a0\u8a13\u7df4\u8cc7\u6599\u7684\u5927\u5c0f\uff0c\u6211\u5011\u6a21\u578b\u7684\u6548\u80fd\u53ef\u4ee5\u9032\u4e00\u6b65\u63d0\u5347\u3002\u6211\u5011\u7684\u5c08\u6848\u9996\u9801\u662f https://t2sgpt-demo.yinaoxiong.cn\u3002</paragraph>", "author": "Aoxiong Yin et.al.", "authors": "Aoxiong Yin, Haoyuan Li, Kai Shen, Siliang Tang, Yueting Zhuang", "id": "2406.07119v1", "paper_url": "http://arxiv.org/abs/2406.07119v1", "repo": "null"}}