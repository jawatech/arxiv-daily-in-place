{"2412.14965": {"publish_time": "2024-12-19", "title": "Movie2Story: A framework for understanding videos and telling stories in the form of novel text", "paper_summary": "Multimodal video-to-text models have made considerable progress, primarily in\ngenerating brief descriptions of video content. However, there is still a\ndeficiency in generating rich long-form text descriptions that integrate both\nvideo and audio. In this paper, we introduce a framework called M2S, designed\nto generate novel-length text by combining audio, video, and character\nrecognition. M2S includes modules for video long-form text description and\ncomprehension, audio-based analysis of emotion, speech rate, and character\nalignment, and visual-based character recognition alignment. By integrating\nmultimodal information using the large language model GPT4o, M2S stands out in\nthe field of multimodal text generation. We demonstrate the effectiveness and\naccuracy of M2S through comparative experiments and human evaluation.\nAdditionally, the model framework has good scalability and significant\npotential for future research.", "paper_summary_zh": "\u591a\u6a21\u614b\u5f71\u7247\u8f49\u6587\u5b57\u6a21\u578b\u5df2\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4e3b\u8981\u5728\u65bc\u7522\u751f\u5f71\u7247\u5167\u5bb9\u7684\u7c21\u77ed\u63cf\u8ff0\u3002\u7136\u800c\uff0c\u5728\u7522\u751f\u6574\u5408\u8996\u8a0a\u548c\u97f3\u8a0a\u7684\u8c50\u5bcc\u9577\u7bc7\u6587\u5b57\u63cf\u8ff0\u65b9\u9762\uff0c\u4ecd\u6709\u4e0d\u8db3\u4e4b\u8655\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u540d\u70ba M2S \u7684\u67b6\u69cb\uff0c\u65e8\u5728\u900f\u904e\u7d50\u5408\u97f3\u8a0a\u3001\u8996\u8a0a\u548c\u89d2\u8272\u8b58\u5225\u4f86\u7522\u751f\u5c0f\u8aaa\u9577\u5ea6\u7684\u6587\u5b57\u3002M2S \u5305\u542b\u8996\u8a0a\u9577\u7bc7\u6587\u5b57\u63cf\u8ff0\u548c\u7406\u89e3\u3001\u57fa\u65bc\u97f3\u8a0a\u7684\u60c5\u7dd2\u5206\u6790\u3001\u8a9e\u901f\u548c\u89d2\u8272\u5c0d\u9f4a\uff0c\u4ee5\u53ca\u57fa\u65bc\u8996\u89ba\u7684\u89d2\u8272\u8b58\u5225\u5c0d\u9f4a\u7b49\u6a21\u7d44\u3002\u900f\u904e\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b GPT4o \u6574\u5408\u591a\u6a21\u614b\u8cc7\u8a0a\uff0cM2S \u5728\u591a\u6a21\u614b\u6587\u5b57\u7522\u751f\u7684\u9818\u57df\u4e2d\u812b\u7a4e\u800c\u51fa\u3002\u6211\u5011\u900f\u904e\u6bd4\u8f03\u5be6\u9a57\u548c\u4eba\u5de5\u8a55\u4f30\uff0c\u8b49\u660e\u4e86 M2S \u7684\u6709\u6548\u6027\u548c\u6e96\u78ba\u6027\u3002\u6b64\u5916\uff0c\u8a72\u6a21\u578b\u67b6\u69cb\u5177\u6709\u826f\u597d\u7684\u53ef\u64f4\u5145\u6027\uff0c\u4e26\u5c0d\u672a\u4f86\u7684\u7814\u7a76\u5177\u6709\u986f\u8457\u7684\u6f5b\u529b\u3002", "author": "Kangning Li et.al.", "authors": "Kangning Li, Zheyang Jia, Anyu Ying", "id": "2412.14965v1", "paper_url": "http://arxiv.org/abs/2412.14965v1", "repo": "null"}}