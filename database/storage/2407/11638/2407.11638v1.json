{"2407.11638": {"publish_time": "2024-07-16", "title": "A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting", "paper_summary": "Recently, Large Language Models (LLMs) have demonstrated great potential in\nvarious data mining tasks, such as knowledge question answering, mathematical\nreasoning, and commonsense reasoning. However, the reasoning capability of LLMs\non temporal event forecasting has been under-explored. To systematically\ninvestigate their abilities in temporal event forecasting, we conduct a\ncomprehensive evaluation of LLM-based methods for temporal event forecasting.\nDue to the lack of a high-quality dataset that involves both graph and textual\ndata, we first construct a benchmark dataset, named MidEast-TE-mini. Based on\nthis dataset, we design a series of baseline methods, characterized by various\ninput formats and retrieval augmented generation(RAG) modules. From extensive\nexperiments, we find that directly integrating raw texts into the input of LLMs\ndoes not enhance zero-shot extrapolation performance. In contrast,\nincorporating raw texts in specific complex events and fine-tuning LLMs\nsignificantly improves performance. Moreover, enhanced with retrieval modules,\nLLM can effectively capture temporal relational patterns hidden in historical\nevents. Meanwhile, issues such as popularity bias and the long-tail problem\nstill persist in LLMs, particularly in the RAG-based method. These findings not\nonly deepen our understanding of LLM-based event forecasting methods but also\nhighlight several promising research directions.We consider that this\ncomprehensive evaluation, along with the identified research opportunities,\nwill significantly contribute to future research on temporal event forecasting\nthrough LLMs.", "paper_summary_zh": "\u8fd1\u671f\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5404\u79cd\u8d44\u6599\u63a2\u52d8\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6781\u5927\u7684\u6f5c\u529b\uff0c\u4f8b\u5982\u77e5\u8bc6\u95ee\u7b54\u3001\u6570\u5b66\u63a8\u7406\u548c\u5e38\u8bc6\u63a8\u7406\u3002\u7136\u800c\uff0cLLM \u5728\u65f6\u95f4\u4e8b\u4ef6\u9884\u6d4b\u65b9\u9762\u7684\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u4e86\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u5176\u5728\u65f6\u95f4\u4e8b\u4ef6\u9884\u6d4b\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6211\u4eec\u5bf9\u57fa\u4e8e LLM \u7684\u65f6\u95f4\u4e8b\u4ef6\u9884\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u3002\u7531\u4e8e\u7f3a\u4e4f\u540c\u65f6\u5305\u542b\u56fe\u8868\u548c\u6587\u672c\u8d44\u6599\u7684\u9ad8\u54c1\u8d28\u6570\u636e\u96c6\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a MidEast-TE-mini \u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5176\u7279\u70b9\u662f\u5404\u79cd\u8f93\u5165\u683c\u5f0f\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u6a21\u5757\u3002\u4ece\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\u76f4\u63a5\u5c06\u539f\u59cb\u6587\u672c\u6574\u5408\u5230 LLM \u7684\u8f93\u5165\u4e2d\u5e76\u4e0d\u4f1a\u589e\u5f3a\u96f6\u6b21\u5b66\u4e60\u5916\u63a8\u6027\u80fd\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u7279\u5b9a\u590d\u6742\u4e8b\u4ef6\u4e2d\u7eb3\u5165\u539f\u59cb\u6587\u672c\u5e76\u5fae\u8c03 LLM \u4f1a\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u68c0\u7d22\u6a21\u5757\u7684\u589e\u5f3a\uff0cLLM \u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u9690\u85cf\u5728\u5386\u53f2\u4e8b\u4ef6\u4e2d\u7684\u65f6\u95f4\u5173\u7cfb\u6a21\u5f0f\u3002\u540c\u65f6\uff0c\u8bf8\u5982\u6d41\u884c\u5ea6\u504f\u5dee\u548c\u957f\u5c3e\u95ee\u9898\u7b49\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u4e8e LLM \u4e2d\uff0c\u5c24\u5176\u662f\u5728\u57fa\u4e8e RAG \u7684\u65b9\u6cd5\u4e2d\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e0d\u4ec5\u52a0\u6df1\u4e86\u6211\u4eec\u5bf9\u57fa\u4e8e LLM \u7684\u4e8b\u4ef6\u9884\u6d4b\u65b9\u6cd5\u7684\u7406\u89e3\uff0c\u8fd8\u7a81\u51fa\u4e86\u51e0\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u8fd9\u9879\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u8fde\u540c\u5df2\u786e\u5b9a\u7684\u7814\u7a76\u673a\u4f1a\uff0c\u5c06\u6781\u5927\u5730\u4fc3\u8fdb\u901a\u8fc7 LLM \u8fdb\u884c\u65f6\u95f4\u4e8b\u4ef6\u9884\u6d4b\u7684\u672a\u6765\u7814\u7a76\u3002", "author": "He Chang et.al.", "authors": "He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua", "id": "2407.11638v1", "paper_url": "http://arxiv.org/abs/2407.11638v1", "repo": "null"}}