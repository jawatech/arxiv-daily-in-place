{"2406.10099": {"publish_time": "2024-06-14", "title": "Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning", "paper_summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks but still face challenges such as hallucinations. One potential\nreason for hallucinations is the lack of relevant knowledge or context. Thus, a\npromising solution to mitigate this issue involves instructing LLMs to respond\nwith \"I do not know\" when a question falls outside their knowledge domain or\nthe provided context. However, in this work, we observed that LLMs struggle to\nadmit their lack of knowledge, primarily due to existing instruction datasets\ndesigned to encourage specific answers. To improve large language models'\ncapability to recognize the boundaries of their knowledge, we propose a novel\napproach called uncertainty-sensitive tuning. This method involves two-stage\ntraining designed for uncertainty recognition and prompt-sensitive activation.\nIn the first stage, we guide the LLM to reject unknown questions. In the second\nstage, we recover the decreased performance in QA tasks by incorporating\ndesigned causal instructions. By leveraging this method, we aim to enhance the\nmodel's ability to identify areas of uncertainty. The experimental results\ndemonstrate that our proposed uncertainty-sensitive tuning method significantly\nimproves the performance of the Llama2-chat-7B model. Specifically, it achieves\na substantial 34.7% improvement in handling questions involving knowledge gaps\ncompared to the original model. Moreover, our approach outperforms GPT-4,\nexhibiting a 9.4% increase in overall performance. We open-source the model and\ncode on GitHub.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u793a\u51fa\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u4f46\u4ecd\u9762\u81e8\u5e7b\u89ba\u7b49\u6311\u6230\u3002\u5e7b\u89ba\u7684\u4e00\u500b\u6f5b\u5728\u539f\u56e0\u662f\u7f3a\u4e4f\u76f8\u95dc\u77e5\u8b58\u6216\u80cc\u666f\u3002\u56e0\u6b64\uff0c\u6e1b\u8f15\u6b64\u554f\u984c\u7684\u4e00\u500b\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\u6d89\u53ca\u6307\u5c0e LLM \u5728\u554f\u984c\u8d85\u51fa\u5176\u77e5\u8b58\u9818\u57df\u6216\u63d0\u4f9b\u7684\u80cc\u666f\u6642\u56de\u7b54\u300c\u6211\u4e0d\u77e5\u9053\u300d\u3002\u7136\u800c\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u89c0\u5bdf\u5230 LLM \u96e3\u4ee5\u627f\u8a8d\u5176\u77e5\u8b58\u4e0d\u8db3\uff0c\u9019\u4e3b\u8981\u662f\u7531\u65bc\u73fe\u6709\u7684\u6307\u4ee4\u6578\u64da\u96c6\u65e8\u5728\u9f13\u52f5\u5177\u9ad4\u7b54\u6848\u3002\u70ba\u4e86\u63d0\u9ad8\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u8b58\u5225\u5176\u77e5\u8b58\u908a\u754c\u7684\u80fd\u529b\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba\u4e0d\u78ba\u5b9a\u6027\u654f\u611f\u8abf\u6574\u7684\u65b0\u65b9\u6cd5\u3002\u6b64\u65b9\u6cd5\u6d89\u53ca\u5169\u968e\u6bb5\u8a13\u7df4\uff0c\u65e8\u5728\u9032\u884c\u4e0d\u78ba\u5b9a\u6027\u8b58\u5225\u548c\u63d0\u793a\u654f\u611f\u6fc0\u6d3b\u3002\u5728\u7b2c\u4e00\u968e\u6bb5\uff0c\u6211\u5011\u6307\u5c0e LLM \u62d2\u7d55\u672a\u77e5\u554f\u984c\u3002\u5728\u7b2c\u4e8c\u968e\u6bb5\uff0c\u6211\u5011\u901a\u904e\u7d0d\u5165\u8a2d\u8a08\u7684\u56e0\u679c\u6307\u4ee4\u4f86\u6062\u5fa9\u5728 QA \u4efb\u52d9\u4e2d\u4e0b\u964d\u7684\u6027\u80fd\u3002\u901a\u904e\u5229\u7528\u9019\u7a2e\u65b9\u6cd5\uff0c\u6211\u5011\u65e8\u5728\u589e\u5f37\u6a21\u578b\u8b58\u5225\u4e0d\u78ba\u5b9a\u6027\u9818\u57df\u7684\u80fd\u529b\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u63d0\u51fa\u7684\u4e0d\u78ba\u5b9a\u6027\u654f\u611f\u8abf\u6574\u65b9\u6cd5\u986f\u8457\u63d0\u9ad8\u4e86 Llama2-chat-7B \u6a21\u578b\u7684\u6027\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u8207\u539f\u59cb\u6a21\u578b\u76f8\u6bd4\uff0c\u5b83\u5728\u8655\u7406\u6d89\u53ca\u77e5\u8b58\u5dee\u8ddd\u7684\u554f\u984c\u6642\u53d6\u5f97\u4e86 34.7% \u7684\u986f\u8457\u6539\u9032\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u505a\u6cd5\u512a\u65bc GPT-4\uff0c\u6574\u9ad4\u6027\u80fd\u63d0\u9ad8\u4e86 9.4%\u3002\u6211\u5011\u5728 GitHub \u4e0a\u958b\u6e90\u4e86\u6a21\u578b\u548c\u4ee3\u78bc\u3002", "author": "Jiaqi Li et.al.", "authors": "Jiaqi Li, Yixuan Tang, Yi Yang", "id": "2406.10099v1", "paper_url": "http://arxiv.org/abs/2406.10099v1", "repo": "null"}}