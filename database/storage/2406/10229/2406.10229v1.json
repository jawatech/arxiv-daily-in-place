{"2406.10229": {"publish_time": "2024-06-14", "title": "Quantifying Variance in Evaluation Benchmarks", "paper_summary": "Evaluation benchmarks are the cornerstone of measuring capabilities of large\nlanguage models (LLMs), as well as driving progress in said capabilities.\nOriginally designed to make claims about capabilities (or lack thereof) in\nfully pretrained models, evaluation benchmarks are now also extensively used to\ndecide between various training choices. Despite this widespread usage, we\nrarely quantify the variance in our evaluation benchmarks, which dictates\nwhether differences in performance are meaningful. Here, we define and measure\na range of metrics geared towards measuring variance in evaluation benchmarks,\nincluding seed variance across initialisations, and monotonicity during\ntraining. By studying a large number of models -- both openly available and\npretrained from scratch -- we provide empirical estimates for a variety of\nvariance metrics, with considerations and recommendations for practitioners. We\nalso evaluate the utility and tradeoffs of continuous versus discrete\nperformance measures and explore options for better understanding and reducing\nthis variance. We find that simple changes, such as framing choice tasks (like\nMMLU) as completion tasks, can often reduce variance for smaller scale\n($\\sim$7B) models, while more involved methods inspired from human testing\nliterature (such as item analysis and item response theory) struggle to\nmeaningfully reduce variance. Overall, our work provides insights into variance\nin evaluation benchmarks, suggests LM-specific techniques to reduce variance,\nand more generally encourages practitioners to carefully factor in variance\nwhen comparing models.", "paper_summary_zh": "\u8a55\u4f30\u57fa\u6e96\u662f\u8861\u91cf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u529b\u7684\u57fa\u77f3\uff0c\u540c\u6642\u4e5f\u80fd\u4fc3\u9032\u6b64\u985e\u80fd\u529b\u7684\u9032\u6b65\u3002\u8a55\u4f30\u57fa\u6e96\u6700\u521d\u8a2d\u8a08\u7528\u65bc\u63d0\u51fa\u95dc\u65bc\u5b8c\u5168\u9810\u8a13\u7df4\u6a21\u578b\u7684\u80fd\u529b\uff08\u6216\u7f3a\u4e4f\u80fd\u529b\uff09\u7684\u8aaa\u6cd5\uff0c\u73fe\u5728\u4e5f\u5ee3\u6cdb\u7528\u65bc\u5728\u5404\u7a2e\u8a13\u7df4\u9078\u9805\u4e4b\u9593\u505a\u51fa\u6c7a\u5b9a\u3002\u5118\u7ba1\u5982\u6b64\u5ee3\u6cdb\u7684\u4f7f\u7528\uff0c\u6211\u5011\u5f88\u5c11\u91cf\u5316\u8a55\u4f30\u57fa\u6e96\u4e2d\u7684\u5dee\u7570\uff0c\u9019\u6c7a\u5b9a\u4e86\u6548\u80fd\u5dee\u7570\u662f\u5426\u6709\u610f\u7fa9\u3002\u5728\u9019\u88e1\uff0c\u6211\u5011\u5b9a\u7fa9\u548c\u8861\u91cf\u4e00\u7cfb\u5217\u6307\u6a19\uff0c\u9019\u4e9b\u6307\u6a19\u65e8\u5728\u8861\u91cf\u8a55\u4f30\u57fa\u6e96\u4e2d\u7684\u5dee\u7570\uff0c\u5305\u62ec\u521d\u59cb\u5316\u6642\u7684\u7a2e\u5b50\u5dee\u7570\u548c\u8a13\u7df4\u671f\u9593\u7684\u55ae\u8abf\u6027\u3002\u900f\u904e\u7814\u7a76\u5927\u91cf\u6a21\u578b\uff08\u5305\u62ec\u516c\u958b\u53ef\u7528\u7684\u6a21\u578b\u548c\u5f9e\u982d\u958b\u59cb\u9810\u8a13\u7df4\u7684\u6a21\u578b\uff09\uff0c\u6211\u5011\u91dd\u5c0d\u5404\u7a2e\u5dee\u7570\u6307\u6a19\u63d0\u4f9b\u7d93\u9a57\u4f30\u8a08\uff0c\u4e26\u63d0\u4f9b\u7d66\u5be6\u52d9\u5de5\u4f5c\u8005\u7684\u8003\u91cf\u548c\u5efa\u8b70\u3002\u6211\u5011\u4e5f\u8a55\u4f30\u9023\u7e8c\u8207\u96e2\u6563\u6548\u80fd\u6e2c\u91cf\u7684\u65b9\u6cd5\u53ca\u5176\u6b0a\u8861\u53d6\u6368\uff0c\u4e26\u63a2\u8a0e\u66f4\u4e86\u89e3\u548c\u6e1b\u5c11\u6b64\u5dee\u7570\u7684\u9078\u9805\u3002\u6211\u5011\u767c\u73fe\uff0c\u4e00\u4e9b\u7c21\u55ae\u7684\u8b8a\u66f4\uff0c\u4f8b\u5982\u5c07\u9078\u64c7\u4efb\u52d9\uff08\u4f8b\u5982 MMLU\uff09\u8a2d\u5b9a\u70ba\u5b8c\u6210\u4efb\u52d9\uff0c\u901a\u5e38\u53ef\u4ee5\u6e1b\u5c11\u8f03\u5c0f\u898f\u6a21\uff08$\\sim$7B\uff09\u6a21\u578b\u7684\u5dee\u7570\uff0c\u800c\u53d7\u5230\u4eba\u985e\u6e2c\u8a66\u6587\u737b\u555f\u767c\u7684\u66f4\u8907\u96dc\u65b9\u6cd5\uff08\u4f8b\u5982\u9805\u76ee\u5206\u6790\u548c\u9805\u76ee\u53cd\u61c9\u7406\u8ad6\uff09\u5247\u96e3\u4ee5\u6709\u6548\u6e1b\u5c11\u5dee\u7570\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5c0d\u8a55\u4f30\u57fa\u6e96\u4e2d\u5dee\u7570\u7684\u898b\u89e3\uff0c\u63d0\u51fa\u4e86\u6e1b\u5c11\u5dee\u7570\u7684\u7279\u5b9a\u65bc LM \u7684\u6280\u8853\uff0c\u4e26\u66f4\u666e\u904d\u5730\u9f13\u52f5\u5be6\u52d9\u5de5\u4f5c\u8005\u5728\u6bd4\u8f03\u6a21\u578b\u6642\u4ed4\u7d30\u8003\u91cf\u5dee\u7570\u3002", "author": "Lovish Madaan et.al.", "authors": "Lovish Madaan, Aaditya K. Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang, Dieuwke Hupkes", "id": "2406.10229v1", "paper_url": "http://arxiv.org/abs/2406.10229v1", "repo": "null"}}