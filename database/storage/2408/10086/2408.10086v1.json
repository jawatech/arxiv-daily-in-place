{"2408.10086": {"publish_time": "2024-08-19", "title": "ARMADA: Attribute-Based Multimodal Data Augmentation", "paper_summary": "In Multimodal Language Models (MLMs), the cost of manually annotating\nhigh-quality image-text pair data for fine-tuning and alignment is extremely\nhigh. While existing multimodal data augmentation frameworks propose ways to\naugment image-text pairs, they either suffer from semantic inconsistency\nbetween texts and images, or generate unrealistic images, causing knowledge gap\nwith real world examples. To address these issues, we propose Attribute-based\nMultimodal Data Augmentation (ARMADA), a novel multimodal data augmentation\nmethod via knowledge-guided manipulation of visual attributes of the mentioned\nentities. Specifically, we extract entities and their visual attributes from\nthe original text data, then search for alternative values for the visual\nattributes under the guidance of knowledge bases (KBs) and large language\nmodels (LLMs). We then utilize an image-editing model to edit the images with\nthe extracted attributes. ARMADA is a novel multimodal data generation\nframework that: (i) extracts knowledge-grounded attributes from symbolic KBs\nfor semantically consistent yet distinctive image-text pair generation, (ii)\ngenerates visually similar images of disparate categories using neighboring\nentities in the KB hierarchy, and (iii) uses the commonsense knowledge of LLMs\nto modulate auxiliary visual attributes such as backgrounds for more robust\nrepresentation of original entities. Our empirical results over four downstream\ntasks demonstrate the efficacy of our framework to produce high-quality data\nand enhance the model performance. This also highlights the need to leverage\nexternal knowledge proxies for enhanced interpretability and real-world\ngrounding.", "paper_summary_zh": "\u5728\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b (MLM) \u4e2d\uff0c\u4eba\u5de5\u6a19\u8a3b\u9ad8\u54c1\u8cea\u5f71\u50cf\u6587\u5b57\u914d\u5c0d\u8cc7\u6599\u4ee5\u9032\u884c\u5fae\u8abf\u548c\u6bd4\u5c0d\u7684\u6210\u672c\u6975\u9ad8\u3002\u73fe\u6709\u7684\u591a\u6a21\u614b\u8cc7\u6599\u64f4\u5145\u67b6\u69cb\u96d6\u7136\u63d0\u51fa\u64f4\u5145\u5f71\u50cf\u6587\u5b57\u914d\u5c0d\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u5011\u4e0d\u662f\u6703\u9020\u6210\u6587\u5b57\u548c\u5f71\u50cf\u4e4b\u9593\u7684\u8a9e\u610f\u4e0d\u4e00\u81f4\uff0c\u5c31\u662f\u6703\u7522\u751f\u4e0d\u5207\u5be6\u969b\u7684\u5f71\u50cf\uff0c\u5c0e\u81f4\u8207\u771f\u5be6\u4e16\u754c\u7684\u7bc4\u4f8b\u7522\u751f\u77e5\u8b58\u9d3b\u6e9d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u57fa\u65bc\u5c6c\u6027\u7684\u591a\u6a21\u614b\u8cc7\u6599\u64f4\u5145 (ARMADA)\uff0c\u9019\u662f\u4e00\u7a2e\u900f\u904e\u5f15\u5c0e\u77e5\u8b58\u64cd\u4f5c\u63d0\u53ca\u5be6\u9ad4\u7684\u8996\u89ba\u5c6c\u6027\uff0c\u4f86\u9032\u884c\u591a\u6a21\u614b\u8cc7\u6599\u64f4\u5145\u7684\u65b0\u65b9\u6cd5\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f9e\u539f\u59cb\u6587\u5b57\u8cc7\u6599\u4e2d\u8403\u53d6\u5be6\u9ad4\u53ca\u5176\u8996\u89ba\u5c6c\u6027\uff0c\u7136\u5f8c\u5728\u77e5\u8b58\u5eab (KB) \u548c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6307\u5c0e\u4e0b\uff0c\u70ba\u8996\u89ba\u5c6c\u6027\u5c0b\u627e\u66ff\u4ee3\u503c\u3002\u63a5\u8457\uff0c\u6211\u5011\u5229\u7528\u5f71\u50cf\u7de8\u8f2f\u6a21\u578b\u4f86\u7de8\u8f2f\u5177\u6709\u8403\u53d6\u5c6c\u6027\u7684\u5f71\u50cf\u3002ARMADA \u662f\u4e00\u500b\u65b0\u7a4e\u7684\u591a\u6a21\u614b\u8cc7\u6599\u7522\u751f\u67b6\u69cb\uff0c\u5b83\uff1a(i) \u5f9e\u7b26\u865f KB \u4e2d\u8403\u53d6\u57fa\u65bc\u77e5\u8b58\u7684\u5c6c\u6027\uff0c\u4ee5\u7522\u751f\u8a9e\u610f\u4e00\u81f4\u4f46\u7368\u7279\u7684\u5f71\u50cf\u6587\u5b57\u914d\u5c0d\uff0c(ii) \u4f7f\u7528 KB \u5c64\u7d1a\u4e2d\u7684\u9130\u8fd1\u5be6\u9ad4\uff0c\u7522\u751f\u8996\u89ba\u4e0a\u76f8\u4f3c\u7684\u4e0d\u540c\u985e\u5225\u5f71\u50cf\uff0c\u4ee5\u53ca (iii) \u4f7f\u7528 LLM \u7684\u5e38\u8b58\u77e5\u8b58\u4f86\u8abf\u6574\u8f14\u52a9\u8996\u89ba\u5c6c\u6027\uff0c\u4f8b\u5982\u80cc\u666f\uff0c\u4ee5\u66f4\u7a69\u5065\u5730\u8868\u793a\u539f\u59cb\u5be6\u9ad4\u3002\u6211\u5011\u5728\u56db\u500b\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7684\u5be6\u8b49\u7d50\u679c\uff0c\u8b49\u660e\u4e86\u6211\u5011\u7684\u67b6\u69cb\u5728\u7522\u751f\u9ad8\u54c1\u8cea\u8cc7\u6599\u548c\u589e\u5f37\u6a21\u578b\u6548\u80fd\u65b9\u9762\u7684\u6548\u529b\u3002\u9019\u4e5f\u7a81\u986f\u4e86\u5229\u7528\u5916\u90e8\u77e5\u8b58\u4ee3\u7406\u4f86\u589e\u5f37\u53ef\u89e3\u91cb\u6027\u548c\u771f\u5be6\u4e16\u754c\u57fa\u790e\u7684\u9700\u6c42\u3002", "author": "Xiaomeng Jin et.al.", "authors": "Xiaomeng Jin, Jeonghwan Kim, Yu Zhou, Kuan-Hao Huang, Te-Lin Wu, Nanyun Peng, Heng Ji", "id": "2408.10086v1", "paper_url": "http://arxiv.org/abs/2408.10086v1", "repo": "null"}}