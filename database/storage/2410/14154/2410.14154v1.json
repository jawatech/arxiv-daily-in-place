{"2410.14154": {"publish_time": "2024-10-18", "title": "RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training", "paper_summary": "Multimodal Large Language Models (MLLMs) have recently received substantial\ninterest, which shows their emerging potential as general-purpose models for\nvarious vision-language tasks. MLLMs involve significant external knowledge\nwithin their parameters; however, it is challenging to continually update these\nmodels with the latest knowledge, which involves huge computational costs and\npoor interpretability. Retrieval augmentation techniques have proven to be\neffective plugins for both LLMs and MLLMs. In this study, we propose multimodal\nadaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training\n(RA-BLIP), a novel retrieval-augmented framework for various MLLMs. Considering\nthe redundant information within vision modality, we first leverage the\nquestion to instruct the extraction of visual information through interactions\nwith one set of learnable queries, minimizing irrelevant interference during\nretrieval and generation. Besides, we introduce a pre-trained multimodal\nadaptive fusion module to achieve question text-to-multimodal retrieval and\nintegration of multimodal knowledge by projecting visual and language\nmodalities into a unified semantic space. Furthermore, we present an Adaptive\nSelection Knowledge Generation (ASKG) strategy to train the generator to\nautonomously discern the relevance of retrieved knowledge, which realizes\nexcellent denoising performance. Extensive experiments on open multimodal\nquestion-answering datasets demonstrate that RA-BLIP achieves significant\nperformance and surpasses the state-of-the-art retrieval-augmented models.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u8fd1\u6765\u5907\u53d7\u5173\u6ce8\uff0c\u663e\u793a\u5176\u4f5c\u4e3a\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u901a\u7528\u6a21\u578b\u7684\u65b0\u5174\u6f5c\u529b\u3002MLLM \u5728\u5176\u53c2\u6570\u4e2d\u5305\u542b\u5927\u91cf\u5916\u90e8\u77e5\u8bc6\uff1b\u7136\u800c\uff0c\u6301\u7eed\u4f7f\u7528\u6700\u65b0\u77e5\u8bc6\u66f4\u65b0\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u8fd9\u6d89\u53ca\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u548c\u8f83\u5dee\u7684\u53ef\u89e3\u91ca\u6027\u3002\u68c0\u7d22\u589e\u5f3a\u6280\u672f\u5df2\u88ab\u8bc1\u660e\u662f LLM \u548c MLLM \u7684\u6709\u6548\u63d2\u4ef6\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u5f15\u5bfc\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3 (RA-BLIP)\uff0c\u4e00\u4e2a\u9488\u5bf9\u5404\u79cd MLLM \u7684\u65b0\u9896\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\u3002\u8003\u8651\u5230\u89c6\u89c9\u6a21\u6001\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\uff0c\u6211\u4eec\u9996\u5148\u5229\u7528\u95ee\u9898\u901a\u8fc7\u4e0e\u4e00\u7ec4\u53ef\u5b66\u4e60\u67e5\u8be2\u7684\u4ea4\u4e92\u6765\u6307\u5bfc\u89c6\u89c9\u4fe1\u606f\u7684\u63d0\u53d6\uff0c\u6700\u5927\u7a0b\u5ea6\u51cf\u5c11\u68c0\u7d22\u548c\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u65e0\u5173\u5e72\u6270\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u6295\u5c04\u5230\u4e00\u4e2a\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u5b9e\u73b0\u95ee\u9898\u6587\u672c\u5230\u591a\u6a21\u6001\u68c0\u7d22\u548c\u591a\u6a21\u6001\u77e5\u8bc6\u7684\u96c6\u6210\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u9009\u62e9\u77e5\u8bc6\u751f\u6210 (ASKG) \u7b56\u7565\uff0c\u4ee5\u8bad\u7ec3\u751f\u6210\u5668\u81ea\u4e3b\u8fa8\u522b\u68c0\u7d22\u77e5\u8bc6\u7684\u76f8\u5173\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u51fa\u8272\u7684\u53bb\u566a\u6027\u80fd\u3002\u5728\u5f00\u653e\u7684\u591a\u6a21\u6001\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRA-BLIP \u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\uff0c\u5e76\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u589e\u5f3a\u6a21\u578b\u3002", "author": "Muhe Ding et.al.", "authors": "Muhe Ding, Yang Ma, Pengda Qin, Jianlong Wu, Yuhong Li, Liqiang Nie", "id": "2410.14154v1", "paper_url": "http://arxiv.org/abs/2410.14154v1", "repo": "null"}}