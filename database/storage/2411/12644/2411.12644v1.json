{"2411.12644": {"publish_time": "2024-11-19", "title": "CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval", "paper_summary": "Despite the success of text retrieval in many NLP tasks, code retrieval\nremains a largely underexplored area. Most text retrieval systems are tailored\nfor natural language queries, often neglecting the specific challenges of\nretrieving code. This gap leaves existing models unable to effectively capture\nthe diversity of programming languages and tasks across different domains,\nhighlighting the need for more focused research in code retrieval. To address\nthis, we introduce CodeXEmbed, a family of large-scale code embedding models\nranging from 400M to 7B parameters. Our novel training pipeline unifies\nmultiple programming languages and transforms various code-related tasks into a\ncommon retrieval framework, enhancing model generalizability and retrieval\nperformance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,\noutperforming the previous leading model, Voyage-Code, by over 20% on CoIR\nbenchmark. In addition to excelling in code retrieval, our models demonstrate\ncompetitive performance on the widely adopted BeIR text retrieval benchmark,\noffering versatility across domains. Experimental results demonstrate that\nimproving retrieval performance significantly enhances end-to-end\nRetrieval-Augmented Generation (RAG) performance for code-related tasks.", "paper_summary_zh": "\u5118\u7ba1\u6587\u672c\u64f7\u53d6\u5728\u8a31\u591a NLP \u4efb\u52d9\u4e2d\u7372\u5f97\u6210\u529f\uff0c\u4f46\u7a0b\u5f0f\u78bc\u64f7\u53d6\n\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u662f\u4e00\u500b\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9818\u57df\u3002\u5927\u591a\u6578\u6587\u672c\u64f7\u53d6\u7cfb\u7d71\n\u90fd\u662f\u91dd\u5c0d\u81ea\u7136\u8a9e\u8a00\u67e5\u8a62\u800c\u8a2d\u8a08\uff0c\u5e38\u5e38\u5ffd\u7565\u64f7\u53d6\u7a0b\u5f0f\u78bc\u7684\u7279\u5b9a\u6311\u6230\u3002\n\u9019\u500b\u5dee\u8ddd\u4f7f\u5f97\u73fe\u6709\u7684\u6a21\u578b\u7121\u6cd5\u6709\u6548\u64f7\u53d6\u4e0d\u540c\u9818\u57df\u4e2d\u591a\u6a23\u5316\u7684\u7a0b\u5f0f\u8a9e\u8a00\u548c\u4efb\u52d9\uff0c\n\u9019\u7a81\u986f\u4e86\u5728\u7a0b\u5f0f\u78bc\u64f7\u53d6\u4e2d\u9032\u884c\u66f4\u5c08\u6ce8\u7814\u7a76\u7684\u5fc5\u8981\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\n\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 CodeXEmbed\uff0c\u9019\u662f\u4e00\u500b\u5927\u578b\u7a0b\u5f0f\u78bc\u5d4c\u5165\u6a21\u578b\u7cfb\u5217\uff0c\n\u5176\u53c3\u6578\u7bc4\u570d\u5f9e 400M \u5230 7B\u3002\u6211\u5011\u7684\u5275\u65b0\u8a13\u7df4\u7ba1\u9053\u7d71\u4e00\u4e86\n\u591a\u7a2e\u7a0b\u5f0f\u8a9e\u8a00\uff0c\u4e26\u5c07\u5404\u7a2e\u8207\u7a0b\u5f0f\u78bc\u76f8\u95dc\u7684\u4efb\u52d9\u8f49\u63db\u6210\u4e00\u500b\n\u901a\u7528\u7684\u64f7\u53d6\u67b6\u69cb\uff0c\u589e\u5f37\u4e86\u6a21\u578b\u7684\u6982\u62ec\u6027\u548c\u64f7\u53d6\n\u6548\u80fd\u3002\u6211\u5011\u7684 7B \u6a21\u578b\u5728\u7a0b\u5f0f\u78bc\u64f7\u53d6\u4e2d\u6a39\u7acb\u4e86\u65b0\u7684\u6280\u8853\u9818\u5148\u5730\u4f4d (SOTA)\uff0c\n\u5728 CoIR \u57fa\u6e96\u4e0a\u6bd4\u5148\u524d\u7684\u9818\u5148\u6a21\u578b Voyage-Code \u9ad8\u51fa 20% \u4ee5\u4e0a\u3002\u9664\u4e86\u5728\u7a0b\u5f0f\u78bc\u64f7\u53d6\u4e2d\u8868\u73fe\u51fa\u8272\u4e4b\u5916\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u5ee3\u6cdb\u63a1\u7528\u7684 BeIR \u6587\u5b57\u64f7\u53d6\u57fa\u6e96\u4e0a\u4e5f\u5c55\u73fe\u4e86\n\u7af6\u722d\u529b\u7684\u6548\u80fd\uff0c\u5728\u5404\u500b\u9818\u57df\u4e2d\u63d0\u4f9b\u591a\u529f\u80fd\u6027\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\n\u6539\u5584\u64f7\u53d6\u6548\u80fd\u986f\u8457\u589e\u5f37\u4e86\u8207\u7a0b\u5f0f\u78bc\u76f8\u95dc\u4efb\u52d9\u7684\u7aef\u5c0d\u7aef\n\u64f7\u53d6\u589e\u5f37\u7522\u751f (RAG) \u6548\u80fd\u3002", "author": "Ye Liu et.al.", "authors": "Ye Liu, Rui Meng, Shafiq Jot, Silvio Savarese, Caiming Xiong, Yingbo Zhou, Semih Yavuz", "id": "2411.12644v1", "paper_url": "http://arxiv.org/abs/2411.12644v1", "repo": "null"}}