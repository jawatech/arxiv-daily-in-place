{"2407.17379": {"publish_time": "2024-07-24", "title": "MMRA: A Benchmark for Multi-granularity Multi-image Relational Association", "paper_summary": "Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVMLs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks mainly focus on the objective fact or certain topic related\npotential knowledge within a image, but overlook the associative relations\nbetween multiple images. Therefore, we define a multi-image relation\nassociation task, and meticulously curate \\textbf{MMRA} benchmark, a\n\\textbf{M}ulti-granularity \\textbf{M}ulti-image \\textbf{R}elational\n\\textbf{A}ssociation benchmark, consisted of \\textbf{1026} samples. In order to\nsystematically and comprehensively evaluate mainstream LVLMs, we establish an\nassociational relation system among images that contain \\textbf{11 subtasks}\n(e.g, UsageSimilarity, SubEvent, etc.) at two granularity levels (i.e.,\n\"\\textbf{image}\" and \"\\textbf{entity}\") according to the relations in\nConceptNet. Our experiments demonstrate that, on our MMRA benchmark, current\nmainstream LVLMs all have their own advantages and disadvantages across\ndifferent subtasks. It is worth noting that, at the entity level, the\nperformance of all models is worse than that of them at the image level,\nindicating that the fine-grained multi-image perception task is still\nchallenging for LVLMs. The tasks related to spatial perception are relatively\ndifficult for LVLMs to handle. Furthermore, we find that LVMLs exhibit a good\nability to perceive image details, and the key to enhancing their multi-image\nassociation capability is to strengthen the reasoning ability of their language\nmodel component. All our codes and data are released at\nhtt\\url{https://github.com/Wusiwei0410/MMRA}.", "paper_summary_zh": "<paragraph>\u9274\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u5728\u56fe\u50cf\u611f\u77e5\u4efb\u52a1\u4e2d\u53d6\u5f97\u7684\u663e\u8457\u6210\u529f\uff0c\u8ba9 LVML \u50cf\u4eba\u7c7b\u4e00\u6837\u611f\u77e5\u4e16\u754c\u7684\u52aa\u529b\u6b63\u5f15\u8d77\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u5f53\u524d\u7684\u591a\u6a21\u6001\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u4e2d\u7684\u5ba2\u89c2\u4e8b\u5b9e\u6216\u4e0e\u7279\u5b9a\u4e3b\u9898\u76f8\u5173\u7684\u6f5c\u5728\u77e5\u8bc6\uff0c\u4f46\u5ffd\u7565\u4e86\u591a\u5e45\u56fe\u50cf\u4e4b\u95f4\u7684\u5173\u8054\u5173\u7cfb\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u591a\u56fe\u50cf\u5173\u7cfb\u5173\u8054\u4efb\u52a1\uff0c\u5e76\u7cbe\u5fc3\u7b56\u5212\u4e86\\textbf{MMRA}\u57fa\u51c6\uff0c\u8fd9\u662f\u4e00\u4e2a\\textbf{M}ulti-granularity \\textbf{M}ulti-image \\textbf{R}elational \\textbf{A}ssociation\u57fa\u51c6\uff0c\u7531\\textbf{1026}\u4e2a\u6837\u672c\u7ec4\u6210\u3002\u4e3a\u4e86\u7cfb\u7edf\u5168\u9762\u5730\u8bc4\u4f30\u4e3b\u6d41 LVLMs\uff0c\u6211\u4eec\u6839\u636e ConceptNet \u4e2d\u7684\u5173\u7cfb\uff0c\u5728\u5305\u542b\\textbf{11 \u4e2a\u5b50\u4efb\u52a1}(\u4f8b\u5982 UsageSimilarity\u3001SubEvent \u7b49)\u7684\u56fe\u50cf\u4e2d\u5efa\u7acb\u4e86\u4e00\u4e2a\u5173\u8054\u5173\u7cfb\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u4e24\u4e2a\u7c92\u5ea6\u7ea7\u522b(\u5373\u201c\\textbf{\u56fe\u50cf}\u201d\u548c\u201c\\textbf{\u5b9e\u4f53}\u201d)\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6211\u4eec\u7684 MMRA \u57fa\u51c6\u4e0a\uff0c\u5f53\u524d\u7684\u4e3b\u6d41 LVLMs \u5728\u4e0d\u540c\u7684\u5b50\u4efb\u52a1\u4e2d\u90fd\u6709\u5404\u81ea\u7684\u4f18\u52bf\u548c\u52a3\u52bf\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u5b9e\u4f53\u5c42\u9762\u4e0a\uff0c\u6240\u6709\u6a21\u578b\u7684\u6027\u80fd\u90fd\u6bd4\u5b83\u4eec\u5728\u56fe\u50cf\u5c42\u9762\u7684\u6027\u80fd\u5dee\uff0c\u8fd9\u8868\u660e\u7ec6\u7c92\u5ea6\u591a\u56fe\u50cf\u611f\u77e5\u4efb\u52a1\u5bf9\u4e8e LVLMs \u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e0e\u7a7a\u95f4\u611f\u77e5\u76f8\u5173\u7684\u4efb\u52a1\u5bf9\u4e8e LVLMs \u6765\u8bf4\u76f8\u5bf9\u96be\u4ee5\u5904\u7406\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0 LVMLs \u8868\u73b0\u51fa\u826f\u597d\u7684\u611f\u77e5\u56fe\u50cf\u7ec6\u8282\u7684\u80fd\u529b\uff0c\u589e\u5f3a\u5176\u591a\u56fe\u50cf\u5173\u8054\u80fd\u529b\u7684\u5173\u952e\u5728\u4e8e\u52a0\u5f3a\u5176\u8bed\u8a00\u6a21\u578b\u7ec4\u4ef6\u7684\u63a8\u7406\u80fd\u529b\u3002\u6211\u4eec\u6240\u6709\u7684\u4ee3\u7801\u548c\u6570\u636e\u90fd\u53ef\u4ee5\u5728htt\\url{https://github.com/Wusiwei0410/MMRA}\u53d1\u5e03\u3002</paragraph>", "author": "Siwei Wu et.al.", "authors": "Siwei Wu, Kang Zhu, Yu Bai, Yiming Liang, Yizhi Li, Haoning Wu, Jiaheng Liu, Ruibo Liu, Xingwei Qu, Xuxin Cheng, Ge Zhang, Wenhao Huang, Chenghua Lin", "id": "2407.17379v1", "paper_url": "http://arxiv.org/abs/2407.17379v1", "repo": "null"}}