{"2405.06067": {"publish_time": "2024-05-09", "title": "HMT: Hierarchical Memory Transformer for Long Context Language Processing", "paper_summary": "Transformer-based large language models (LLM) have been widely used in\nlanguage processing applications. However, most of them restrict the context\nwindow that permits the model to attend to every token in the inputs. Previous\nworks in recurrent models can memorize past tokens to enable unlimited context\nand maintain effectiveness. However, they have \"flat\" memory architectures,\nwhich have limitations in selecting and filtering information. Since humans are\ngood at learning and self-adjustment, we speculate that imitating brain memory\nhierarchy is beneficial for model memorization. We propose the Hierarchical\nMemory Transformer (HMT), a novel framework that enables and improves models'\nlong-context processing ability by imitating human memorization behavior.\nLeveraging memory-augmented segment-level recurrence, we organize the memory\nhierarchy by preserving tokens from early input token segments, passing memory\nembeddings along the sequence, and recalling relevant information from history.\nEvaluating general language modeling (Wikitext-103, PG-19) and\nquestion-answering tasks (PubMedQA), we show that HMT steadily improves the\nlong-context processing ability of context-constrained and long-context models.\nWith an additional 0.5% - 2% of parameters, HMT can easily plug in and augment\nfuture LLMs to handle long context effectively. Our code is open-sourced on\nGithub: https://github.com/OswaldHe/HMT-pytorch.", "paper_summary_zh": "<paragraph>\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5ee3\u6cdb\u7528\u65bc\u8a9e\u8a00\u8655\u7406\u61c9\u7528\u7a0b\u5f0f\u4e2d\u3002\u7136\u800c\uff0c\u5b83\u5011\u5927\u591a\u9650\u5236\u4e86\u5141\u8a31\u6a21\u578b\u95dc\u6ce8\u8f38\u5165\u4e2d\u6bcf\u500b\u7b26\u865f\u7684\u4e0a\u4e0b\u6587\u8996\u7a97\u3002\u4ee5\u5f80\u5728\u905e\u8ff4\u6a21\u578b\u4e2d\u7684\u5de5\u4f5c\u53ef\u4ee5\u8a18\u4f4f\u904e\u53bb\u7684\u7b26\u865f\uff0c\u4ee5\u555f\u7528\u7121\u9650\u7684\u4e0a\u4e0b\u6587\u4e26\u7dad\u6301\u6548\u80fd\u3002\u7136\u800c\uff0c\u5b83\u5011\u6709\u300c\u6241\u5e73\u300d\u7684\u8a18\u61b6\u9ad4\u67b6\u69cb\uff0c\u5728\u9078\u64c7\u548c\u904e\u6ffe\u8cc7\u8a0a\u65b9\u9762\u6709\u5176\u9650\u5236\u3002\u7531\u65bc\u4eba\u985e\u64c5\u9577\u5b78\u7fd2\u548c\u81ea\u6211\u8abf\u6574\uff0c\u6211\u5011\u63a8\u6e2c\u6a21\u4eff\u5927\u8166\u8a18\u61b6\u9ad4\u968e\u5c64\u5c0d\u65bc\u6a21\u578b\u8a18\u61b6\u5316\u662f\u6709\u76ca\u7684\u3002\u6211\u5011\u63d0\u51fa\u968e\u5c64\u5f0f\u8a18\u61b6Transformer (HMT)\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u900f\u904e\u6a21\u4eff\u4eba\u985e\u7684\u8a18\u61b6\u884c\u70ba\uff0c\u555f\u7528\u4e26\u63d0\u5347\u6a21\u578b\u7684\u9577\u4e0a\u4e0b\u6587\u8655\u7406\u80fd\u529b\u3002\u5229\u7528\u8a18\u61b6\u589e\u5f37\u7684\u5340\u6bb5\u5c64\u7d1a\u905e\u8ff4\uff0c\u6211\u5011\u900f\u904e\u4fdd\u7559\u65e9\u671f\u8f38\u5165\u7b26\u865f\u5340\u6bb5\u7684\u7b26\u865f\uff0c\u6cbf\u8457\u5e8f\u5217\u50b3\u905e\u8a18\u61b6\u5d4c\u5165\uff0c\u4e26\u5f9e\u6b77\u53f2\u4e2d\u63d0\u53d6\u76f8\u95dc\u8cc7\u8a0a\uff0c\u4f86\u7d44\u7e54\u8a18\u61b6\u9ad4\u968e\u5c64\u3002\u8a55\u4f30\u4e00\u822c\u8a9e\u8a00\u6a21\u578b\uff08Wikitext-103\u3001PG-19\uff09\u548c\u554f\u7b54\u4efb\u52d9\uff08PubMedQA\uff09\uff0c\u6211\u5011\u5c55\u793a HMT \u7a69\u5b9a\u5730\u63d0\u5347\u4e86\u53d7\u9650\u65bc\u4e0a\u4e0b\u6587\u7684\u6a21\u578b\u548c\u9577\u4e0a\u4e0b\u6587\u6a21\u578b\u7684\u9577\u4e0a\u4e0b\u6587\u8655\u7406\u80fd\u529b\u3002\u900f\u904e\u984d\u5916\u7684 0.5% - 2% \u53c3\u6578\uff0cHMT \u53ef\u4ee5\u8f15\u9b06\u5730\u63d2\u5165\u548c\u64f4\u5145\u672a\u4f86\u7684 LLM\uff0c\u4ee5\u6709\u6548\u5730\u8655\u7406\u9577\u4e0a\u4e0b\u6587\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5728 Github \u4e0a\u958b\u653e\u539f\u59cb\u78bc\uff1ahttps://github.com/OswaldHe/HMT-pytorch\u3002</paragraph>", "author": "Zifan He et.al.", "authors": "Zifan He, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong", "id": "2405.06067v1", "paper_url": "http://arxiv.org/abs/2405.06067v1", "repo": "https://github.com/OswaldHe/HMT-pytorch"}}