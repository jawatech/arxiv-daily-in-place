{"2502.11751": {"publish_time": "2025-02-17", "title": "Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning", "paper_summary": "Although Large Language Models (LLMs) excel in reasoning and generation for\nlanguage tasks, they are not specifically designed for multimodal challenges.\nTraining Multimodal Large Language Models (MLLMs), however, is\nresource-intensive and constrained by various training limitations. In this\npaper, we propose the Modular-based Visual Contrastive Decoding (MVCD)\nframework to move this obstacle. Our framework leverages LLMs' In-Context\nLearning (ICL) capability and the proposed visual contrastive-example decoding\n(CED), specifically tailored for this framework, without requiring any\nadditional training. By converting visual signals into text and focusing on\ncontrastive output distributions during decoding, we can highlight the new\ninformation introduced by contextual examples, explore their connections, and\navoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual\nperception to make it see and reason over the input visuals. To demonstrate\nMVCD's effectiveness, we conduct experiments with four LLMs across five\nquestion answering datasets. Our results not only show consistent improvement\nin model accuracy but well explain the effective components inside our decoding\nstrategy. Our code will be available at https://github.com/Pbhgit/MVCD.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a9e\u8a00\u4efb\u52d9\u7684\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u8868\u73fe\u512a\u7570\uff0c\u4f46\u5b83\u5011\u4e26\u975e\u5c08\u9580\u91dd\u5c0d\u591a\u6a21\u614b\u6311\u6230\u800c\u8a2d\u8a08\u3002\u7136\u800c\uff0c\u8a13\u7df4\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5341\u5206\u8017\u8cbb\u8cc7\u6e90\uff0c\u4e26\u53d7\u5230\u5404\u7a2e\u8a13\u7df4\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u57fa\u65bc\u6a21\u7d44\u7684\u8996\u89ba\u5c0d\u6bd4\u89e3\u78bc (MVCD) \u67b6\u69cb\u4f86\u514b\u670d\u9019\u500b\u969c\u7919\u3002\u6211\u5011\u7684\u67b6\u69cb\u5229\u7528 LLM \u7684\u60c5\u5883\u5b78\u7fd2 (ICL) \u80fd\u529b\u548c\u5c08\u9580\u70ba\u6b64\u67b6\u69cb\u91cf\u8eab\u6253\u9020\u7684\u8996\u89ba\u5c0d\u6bd4\u7bc4\u4f8b\u89e3\u78bc (CED)\uff0c\u800c\u7121\u9700\u4efb\u4f55\u984d\u5916\u8a13\u7df4\u3002\u900f\u904e\u5c07\u8996\u89ba\u4fe1\u865f\u8f49\u63db\u70ba\u6587\u5b57\uff0c\u4e26\u5728\u89e3\u78bc\u904e\u7a0b\u4e2d\u5c08\u6ce8\u65bc\u5c0d\u6bd4\u8f38\u51fa\u5206\u4f48\uff0c\u6211\u5011\u53ef\u4ee5\u7a81\u986f\u60c5\u5883\u7bc4\u4f8b\u5f15\u5165\u7684\u65b0\u8cc7\u8a0a\uff0c\u63a2\u7d22\u5b83\u5011\u7684\u95dc\u806f\u6027\uff0c\u4e26\u907f\u514d\u904e\u5ea6\u4f9d\u8cf4\u5148\u524d\u7de8\u78bc\u7684\u77e5\u8b58\u3002MVCD \u589e\u5f37\u4e86 LLM \u7684\u8996\u89ba\u611f\u77e5\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u5920\u89c0\u5bdf\u4e26\u63a8\u8ad6\u8f38\u5165\u8996\u89ba\u6548\u679c\u3002\u70ba\u4e86\u8b49\u660e MVCD \u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u4f7f\u7528\u56db\u500b LLM \u5728\u4e94\u500b\u554f\u7b54\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5be6\u9a57\u3002\u6211\u5011\u7684\u7d50\u679c\u4e0d\u50c5\u986f\u793a\u6a21\u578b\u6e96\u78ba\u5ea6\u6301\u7e8c\u63d0\u5347\uff0c\u9084\u80fd\u6e05\u695a\u8aaa\u660e\u6211\u5011\u7684\u89e3\u78bc\u7b56\u7565\u4e2d\u7684\u6709\u6548\u7d44\u6210\u90e8\u5206\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5c07\u5728 https://github.com/Pbhgit/MVCD \u516c\u958b\u3002", "author": "Yuqi Pang et.al.", "authors": "Yuqi Pang, Bowen Yang, Haoqin Tu, Yun Cao, Zeyu Zhang", "id": "2502.11751v1", "paper_url": "http://arxiv.org/abs/2502.11751v1", "repo": "null"}}