{"2411.16863": {"publish_time": "2024-11-25", "title": "Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering", "paper_summary": "Multimodal LLMs (MLLMs) are the natural extension of large language models to\nhandle multimodal inputs, combining text and image data. They have recently\ngarnered attention due to their capability to address complex tasks involving\nboth modalities. However, their effectiveness is limited to the knowledge\nacquired during training, which restricts their practical utility. In this\nwork, we introduce a novel method to enhance the adaptability of MLLMs by\nintegrating external knowledge sources. Our proposed model, Reflective LLaVA\n(ReflectiVA), utilizes reflective tokens to dynamically determine the need for\nexternal knowledge and predict the relevance of information retrieved from an\nexternal database. Tokens are trained following a two-stage two-model training\nrecipe. This ultimately enables the MLLM to manage external knowledge while\npreserving fluency and performance on tasks where external knowledge is not\nneeded. Through our experiments, we demonstrate the efficacy of ReflectiVA for\nknowledge-based visual question answering, highlighting its superior\nperformance compared to existing methods. Source code and trained models are\npublicly available at https://github.com/aimagelab/ReflectiVA.", "paper_summary_zh": "\u591a\u6a21\u614b LLM\uff08MLLM\uff09\u662f\u5927\u8a9e\u8a00\u6a21\u578b\u7684\u81ea\u7136\u5ef6\u4f38\uff0c\u7528\u65bc\u8655\u7406\u591a\u6a21\u614b\u8f38\u5165\uff0c\u7d50\u5408\u6587\u5b57\u548c\u5f71\u50cf\u8cc7\u6599\u3002\u7531\u65bc\u5b83\u5011\u6709\u80fd\u529b\u8655\u7406\u6d89\u53ca\u9019\u5169\u7a2e\u6a21\u614b\u7684\u8907\u96dc\u4efb\u52d9\uff0c\u56e0\u6b64\u6700\u8fd1\u5099\u53d7\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u6548\u80fd\u50c5\u9650\u65bc\u8a13\u7df4\u671f\u9593\u7372\u5f97\u7684\u77e5\u8b58\uff0c\u9019\u9650\u5236\u4e86\u5b83\u5011\u7684\u5be6\u7528\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u900f\u904e\u6574\u5408\u5916\u90e8\u77e5\u8b58\u4f86\u6e90\u4f86\u589e\u5f37 MLLM \u7684\u9069\u61c9\u6027\u3002\u6211\u5011\u63d0\u51fa\u7684\u6a21\u578b Reflective LLaVA\uff08ReflectiVA\uff09\u5229\u7528\u53cd\u5c04\u7b26\u865f\u52d5\u614b\u5730\u78ba\u5b9a\u5c0d\u5916\u90e8\u77e5\u8b58\u7684\u9700\u6c42\uff0c\u4e26\u9810\u6e2c\u5f9e\u5916\u90e8\u8cc7\u6599\u5eab\u4e2d\u6aa2\u7d22\u5230\u7684\u8cc7\u8a0a\u76f8\u95dc\u6027\u3002\u7b26\u865f\u662f\u6309\u7167\u5169\u968e\u6bb5\u5169\u6a21\u578b\u8a13\u7df4\u914d\u65b9\u9032\u884c\u8a13\u7df4\u7684\u3002\u9019\u6700\u7d42\u4f7f MLLM \u80fd\u5920\u7ba1\u7406\u5916\u90e8\u77e5\u8b58\uff0c\u540c\u6642\u5728\u4e0d\u9700\u8981\u5916\u90e8\u77e5\u8b58\u7684\u4efb\u52d9\u4e2d\u4fdd\u6301\u6d41\u66a2\u6027\u548c\u6548\u80fd\u3002\u900f\u904e\u6211\u5011\u7684\u5be6\u9a57\uff0c\u6211\u5011\u5c55\u793a\u4e86 ReflectiVA \u5728\u57fa\u65bc\u77e5\u8b58\u7684\u8996\u89ba\u554f\u984c\u89e3\u7b54\u65b9\u9762\u7684\u6548\u80fd\uff0c\u7a81\u986f\u4e86\u5b83\u8207\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\u7684\u512a\u7570\u6548\u80fd\u3002\u539f\u59cb\u78bc\u548c\u8a13\u7df4\u597d\u7684\u6a21\u578b\u53ef\u4ee5\u5728 https://github.com/aimagelab/ReflectiVA \u516c\u5171\u53d6\u5f97\u3002", "author": "Federico Cocchi et.al.", "authors": "Federico Cocchi, Nicholas Moratelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara", "id": "2411.16863v1", "paper_url": "http://arxiv.org/abs/2411.16863v1", "repo": "https://github.com/aimagelab/reflectiva"}}