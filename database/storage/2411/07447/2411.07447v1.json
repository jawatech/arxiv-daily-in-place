{"2411.07447": {"publish_time": "2024-11-12", "title": "The Effect of Scheduling and Preemption on the Efficiency of LLM Inference Serving", "paper_summary": "The growing usage of Large Language Models (LLMs) highlights the demands and\nchallenges in scalable LLM inference systems, affecting deployment and\ndevelopment processes. On the deployment side, there is a lack of comprehensive\nanalysis on the conditions under which a particular scheduler performs better\nor worse, with performance varying substantially across different schedulers,\nhardware, models, and workloads. Manually testing each configuration on GPUs\ncan be prohibitively expensive. On the development side, unpredictable\nperformance and unknown upper limits can lead to inconclusive trial-and-error\nprocesses, consuming resources on ideas that end up ineffective. To address\nthese challenges, we introduce INFERMAX, an analytical framework that uses\ninference cost models to compare various schedulers, including an optimal\nscheduler formulated as a constraint satisfaction problem (CSP) to establish an\nupper bound on performance. Our framework offers in-depth analysis and raises\nessential questions, challenging assumptions and exploring opportunities for\nmore efficient scheduling. Notably, our findings indicate that preempting\nrequests can reduce GPU costs by 30% compared to avoiding preemptions at all.\nWe believe our methods and insights will facilitate the cost-effective\ndeployment and development of scalable, efficient inference systems and pave\nthe way for cost-based scheduling.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f7f\u7528\u91cf\u4e0d\u65b7\u589e\u52a0\uff0c\u7a81\u986f\u4e86\u53ef\u64f4\u5145 LLM \u63a8\u8ad6\u7cfb\u7d71\u7684\u9700\u6c42\u548c\u6311\u6230\uff0c\u5f71\u97ff\u90e8\u7f72\u548c\u958b\u767c\u6d41\u7a0b\u3002\u5728\u90e8\u7f72\u65b9\u9762\uff0c\u5c0d\u65bc\u7279\u5b9a\u6392\u7a0b\u5668\u5728\u4f55\u7a2e\u689d\u4ef6\u4e0b\u57f7\u884c\u5f97\u66f4\u597d\u6216\u66f4\u5dee\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u5206\u6790\uff0c\u6548\u80fd\u56e0\u4e0d\u540c\u7684\u6392\u7a0b\u5668\u3001\u786c\u9ad4\u3001\u6a21\u578b\u548c\u5de5\u4f5c\u8ca0\u8f09\u800c\u6709\u986f\u8457\u5dee\u7570\u3002\u624b\u52d5\u5728 GPU \u4e0a\u6e2c\u8a66\u6bcf\u500b\u7d44\u614b\u53ef\u80fd\u6703\u975e\u5e38\u6602\u8cb4\u3002\u5728\u958b\u767c\u65b9\u9762\uff0c\u4e0d\u53ef\u9810\u6e2c\u7684\u6548\u80fd\u548c\u672a\u77e5\u7684\u4e0a\u9650\u53ef\u80fd\u6703\u5c0e\u81f4\u7121\u6cd5\u5f97\u51fa\u7d50\u8ad6\u7684\u8a66\u932f\u6d41\u7a0b\uff0c\u6d88\u8017\u7121\u6548\u60f3\u6cd5\u7684\u8cc7\u6e90\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86 INFERMAX\uff0c\u4e00\u500b\u5206\u6790\u67b6\u69cb\uff0c\u4f7f\u7528\u63a8\u8ad6\u6210\u672c\u6a21\u578b\u4f86\u6bd4\u8f03\u5404\u7a2e\u6392\u7a0b\u5668\uff0c\u5305\u62ec\u4e00\u500b\u6700\u4f73\u6392\u7a0b\u5668\uff0c\u8a72\u6392\u7a0b\u5668\u5236\u5b9a\u70ba\u7d04\u675f\u6eff\u8db3\u554f\u984c (CSP)\uff0c\u4ee5\u5efa\u7acb\u6548\u80fd\u7684\u4e0a\u9650\u3002\u6211\u5011\u7684\u67b6\u69cb\u63d0\u4f9b\u4e86\u6df1\u5165\u7684\u5206\u6790\uff0c\u4e26\u63d0\u51fa\u4e86\u91cd\u8981\u7684\u554f\u984c\uff0c\u6311\u6230\u5047\u8a2d\u4e26\u63a2\u7d22\u66f4\u6709\u6548\u6392\u7a0b\u7684\u6a5f\u6703\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u8207\u5b8c\u5168\u907f\u514d\u6436\u5148\u76f8\u6bd4\uff0c\u6436\u5148\u8acb\u6c42\u53ef\u4ee5\u5c07 GPU \u6210\u672c\u964d\u4f4e 30%\u3002\u6211\u5011\u76f8\u4fe1\u6211\u5011\u7684\u6280\u8853\u548c\u898b\u89e3\u5c07\u4fc3\u9032\u53ef\u64f4\u5145\u3001\u6709\u6548\u63a8\u8ad6\u7cfb\u7d71\u7684\u7d93\u6fdf\u6709\u6548\u90e8\u7f72\u548c\u958b\u767c\uff0c\u4e26\u70ba\u57fa\u65bc\u6210\u672c\u7684\u6392\u7a0b\u92ea\u8def\u3002", "author": "Kyoungmin Kim et.al.", "authors": "Kyoungmin Kim, Kijae Hong, Caglar Gulcehre, Anastasia Ailamaki", "id": "2411.07447v1", "paper_url": "http://arxiv.org/abs/2411.07447v1", "repo": "null"}}