{"2405.18166": {"publish_time": "2024-05-28", "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing", "paper_summary": "Large language models (LLMs) are increasingly being adopted in a wide range\nof real-world applications. Despite their impressive performance, recent\nstudies have shown that LLMs are vulnerable to deliberately crafted adversarial\nprompts even when aligned via Reinforcement Learning from Human Feedback or\nsupervised fine-tuning. While existing defense methods focus on either\ndetecting harmful prompts or reducing the likelihood of harmful responses\nthrough various means, defending LLMs against jailbreak attacks based on the\ninner mechanisms of LLMs remains largely unexplored. In this work, we\ninvestigate how LLMs response to harmful prompts and propose a novel defense\nmethod termed \\textbf{L}ayer-specific \\textbf{Ed}iting (LED) to enhance the\nresilience of LLMs against jailbreak attacks. Through LED, we reveal that\nseveral critical \\textit{safety layers} exist among the early layers of LLMs.\nWe then show that realigning these safety layers (and some selected additional\nlayers) with the decoded safe response from selected target layers can\nsignificantly improve the alignment of LLMs against jailbreak attacks.\nExtensive experiments across various LLMs (e.g., Llama2, Mistral) show the\neffectiveness of LED, which effectively defends against jailbreak attacks while\nmaintaining performance on benign prompts. Our code is available at\n\\url{https://github.com/ledllm/ledllm}.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6108\u4f86\u6108\u5ee3\u6cdb\u5730\u7528\u65bc\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u7684\u61c9\u7528\u4e2d\u3002\u5118\u7ba1\u5b83\u5011\u7684\u8868\u73fe\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cLLM \u5bb9\u6613\u53d7\u5230\u84c4\u610f\u8a2d\u8a08\u7684\u5c0d\u6297\u6027\u63d0\u793a\u7684\u5f71\u97ff\uff0c\u5373\u4f7f\u662f\u900f\u904e\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2\u6216\u76e3\u7763\u5fae\u8abf\u4f86\u8abf\u6574\u3002\u73fe\u6709\u7684\u9632\u79a6\u65b9\u6cd5\u8457\u91cd\u65bc\u5075\u6e2c\u6709\u5bb3\u63d0\u793a\u6216\u900f\u904e\u5404\u7a2e\u65b9\u5f0f\u964d\u4f4e\u6709\u5bb3\u56de\u61c9\u7684\u53ef\u80fd\u6027\uff0c\u800c\u91dd\u5c0d LLM \u5167\u90e8\u6a5f\u5236\u7684\u8d8a\u7344\u653b\u64ca\u9032\u884c\u9632\u79a6\u4ecd\u9bae\u5c11\u88ab\u63a2\u8a0e\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e LLM \u5982\u4f55\u56de\u61c9\u6709\u5bb3\u63d0\u793a\uff0c\u4e26\u63d0\u51fa\u7a31\u70ba\\textbf{L}ayer-specific \\textbf{Ed}iting (LED) \u7684\u65b0\u9632\u79a6\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f37 LLM \u5c0d\u6297\u8d8a\u7344\u653b\u64ca\u7684\u97cc\u6027\u3002\u900f\u904e LED\uff0c\u6211\u5011\u63ed\u793a\u4e86 LLM \u7684\u65e9\u671f\u5c64\u7d1a\u4e2d\u5b58\u5728\u6578\u500b\u91cd\u8981\u7684\\textit{\u5b89\u5168\u5c64\u7d1a}\u3002\u7136\u5f8c\u6211\u5011\u5c55\u793a\uff0c\u91cd\u65b0\u8abf\u6574\u9019\u4e9b\u5b89\u5168\u5c64\u7d1a\uff08\u4ee5\u53ca\u4e00\u4e9b\u9078\u5b9a\u7684\u5176\u4ed6\u5c64\u7d1a\uff09\u8207\u9078\u5b9a\u76ee\u6a19\u5c64\u7d1a\u4e2d\u89e3\u78bc\u7684\u5b89\u5168\u56de\u61c9\uff0c\u53ef\u4ee5\u986f\u8457\u6539\u5584 LLM \u5c0d\u6297\u8d8a\u7344\u653b\u64ca\u7684\u8abf\u6574\u3002\u91dd\u5c0d\u5404\u7a2e LLM\uff08\u4f8b\u5982 Llama2\u3001Mistral\uff09\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u986f\u793a\u4e86 LED \u7684\u6548\u80fd\uff0c\u5b83\u6709\u6548\u5730\u9632\u79a6\u4e86\u8d8a\u7344\u653b\u64ca\uff0c\u540c\u6642\u5728\u826f\u6027\u63d0\u793a\u4e0a\u7dad\u6301\u6548\u80fd\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u4ee5\u5728\\url{https://github.com/ledllm/ledllm}\u53d6\u5f97\u3002", "author": "Wei Zhao et.al.", "authors": "Wei Zhao, Zhe Li, Yige Li, Ye Zhang, Jun Sun", "id": "2405.18166v1", "paper_url": "http://arxiv.org/abs/2405.18166v1", "repo": "https://github.com/ledllm/ledllm"}}