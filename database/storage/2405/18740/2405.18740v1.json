{"2405.18740": {"publish_time": "2024-05-29", "title": "Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs", "paper_summary": "Despite impressive advances in recent multimodal large language models\n(MLLMs), state-of-the-art models such as from the GPT-4 suite still struggle\nwith knowledge-intensive tasks. To address this, we consider Reverse Image\nRetrieval (RIR) augmented generation, a simple yet effective strategy to\naugment MLLMs with web-scale reverse image search results. RIR robustly\nimproves knowledge-intensive visual question answering (VQA) of GPT-4V by\n37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA\nevaluation metrics. To our surprise, we discover that RIR helps the model to\nbetter access its own world knowledge. Concretely, our experiments suggest that\nRIR augmentation helps by providing further visual and textual cues without\nnecessarily containing the direct answer to a query. In addition, we elucidate\ncases in which RIR can hurt performance and conduct a human evaluation.\nFinally, we find that the overall advantage of using RIR makes it difficult for\nan agent that can choose to use RIR to perform better than an approach where\nRIR is the default setting.", "paper_summary_zh": "\u5118\u7ba1\u6700\u8fd1\u7684\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9032\u5c55\uff0c\u4f46\u4f86\u81ea GPT-4 \u5957\u4ef6\u7b49\u6700\u5148\u9032\u7684\u6a21\u578b\u5728\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\u4e2d\u4ecd\u9762\u81e8\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u8003\u616e\u53cd\u5411\u5716\u7247\u6aa2\u7d22 (RIR) \u589e\u5f37\u751f\u6210\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u537b\u6709\u6548\u7684\u7b56\u7565\uff0c\u53ef\u4ee5\u900f\u904e\u7db2\u8def\u898f\u6a21\u7684\u53cd\u5411\u5716\u7247\u641c\u5c0b\u7d50\u679c\u4f86\u64f4\u5145 MLLM\u3002RIR \u7a69\u5065\u5730\u6539\u5584\u4e86 GPT-4V \u7684\u77e5\u8b58\u5bc6\u96c6\u578b\u8996\u89ba\u554f\u7b54 (VQA) 37-43%\u3001GPT-4 Turbo 25-27% \u548c GPT-4o 18-20%\uff0c\u5c31\u958b\u653e\u5f0f VQA \u8a55\u4f30\u6307\u6a19\u800c\u8a00\u3002\u4ee4\u6211\u5011\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe RIR \u5e6b\u52a9\u6a21\u578b\u66f4\u4f73\u5730\u5b58\u53d6\u5176\u81ea\u8eab\u7684\u4e16\u754c\u77e5\u8b58\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cRIR \u64f4\u5145\u900f\u904e\u63d0\u4f9b\u9032\u4e00\u6b65\u7684\u8996\u89ba\u548c\u6587\u5b57\u63d0\u793a\u4f86\u63d0\u4f9b\u5e6b\u52a9\uff0c\u800c\u4e26\u975e\u4e00\u5b9a\u8981\u5305\u542b\u5c0d\u67e5\u8a62\u7684\u76f4\u63a5\u7b54\u6848\u3002\u6b64\u5916\uff0c\u6211\u5011\u95e1\u660e\u4e86 RIR \u53ef\u80fd\u640d\u5bb3\u6548\u80fd\u7684\u60c5\u6cc1\uff0c\u4e26\u9032\u884c\u4e86\u4eba\u985e\u8a55\u4f30\u3002\u6700\u5f8c\uff0c\u6211\u5011\u767c\u73fe\u4f7f\u7528 RIR \u7684\u6574\u9ad4\u512a\u52e2\u8b93\u4ee3\u7406\u96e3\u4ee5\u9078\u64c7\u4f7f\u7528 RIR \u4f86\u8868\u73fe\u5f97\u6bd4 RIR \u70ba\u9810\u8a2d\u8a2d\u5b9a\u7684\u65b9\u6cd5\u66f4\u597d\u3002", "author": "Jialiang Xu et.al.", "authors": "Jialiang Xu, Michael Moor, Jure Leskovec", "id": "2405.18740v1", "paper_url": "http://arxiv.org/abs/2405.18740v1", "repo": "null"}}