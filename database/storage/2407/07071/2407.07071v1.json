{"2407.07071": {"publish_time": "2024-07-09", "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "paper_summary": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.", "paper_summary_zh": "\u7576\u8981\u6c42\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7e3d\u7d50\u6587\u7ae0\u6216\u56de\u7b54\u7d66\u5b9a\u6bb5\u843d\u7684\u984c\u76ee\u6642\uff0c\u5b83\u5011\u53ef\u80fd\u6703\u7522\u751f\u5e7b\u89ba\u7d30\u7bc0\uff0c\u4e26\u4ee5\u8207\u8f38\u5165\u5167\u5bb9\u7121\u95dc\u7684\u4e0d\u5be6\u7b54\u6848\u56de\u61c9\u3002\u672c\u8ad6\u6587\u63cf\u8ff0\u4e86\u4e00\u7a2e\u6aa2\u6e2c\u6b64\u985e\u8a9e\u5883\u5e7b\u89ba\u7684\u7c21\u55ae\u65b9\u6cd5\u3002\u6211\u5011\u5047\u8a2d\u8a9e\u5883\u5e7b\u89ba\u8207 LLM \u95dc\u6ce8\u6240\u63d0\u4f9b\u8a9e\u5883\u4e2d\u7684\u8cc7\u8a0a\u8207\u5176\u81ea\u8eab\u7522\u751f\u7684\u8cc7\u8a0a\u7684\u7a0b\u5ea6\u6709\u95dc\u3002\u57fa\u65bc\u6b64\u76f4\u89ba\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u55ae\u7684\u5e7b\u89ba\u6aa2\u6e2c\u6a21\u578b\uff0c\u5176\u8f38\u5165\u7279\u5fb5\u7531\u8a9e\u5883\u8207\u65b0\u7522\u751f\u7684\u6b0a\u6a19 (\u5c0d\u65bc\u6bcf\u500b\u6ce8\u610f\u529b\u982d) \u4e0a\u7684\u6ce8\u610f\u529b\u6b0a\u91cd\u6bd4\u7387\u7d66\u51fa\u3002\u6211\u5011\u767c\u73fe\u57fa\u65bc\u9019\u4e9b\u56de\u9867\u6bd4\u7387\u7279\u5fb5\u7684\u7dda\u6027\u5206\u985e\u5668\u8207\u5229\u7528 LLM \u7684\u6574\u500b\u96b1\u85cf\u72c0\u614b\u6216\u57fa\u65bc\u6587\u5b57\u7684\u860a\u6db5\u6a21\u578b\u7684\u66f4\u8c50\u5bcc\u7684\u6aa2\u6e2c\u5668\u4e00\u6a23\u6709\u6548\u3002\u767c\u73fe\u57fa\u65bc\u56de\u9867\u6bd4\u7387\u7684\u6aa2\u6e2c\u5668\u2014\u2014\u56de\u9867\u93e1\u982d\u2014\u2014\u53ef\u4ee5\u8de8\u4efb\u52d9\u751a\u81f3\u8de8\u6a21\u578b\u50b3\u8f38\uff0c\u5141\u8a31\u5728 7B \u6a21\u578b\u4e0a\u8a13\u7df4\u7684\u6aa2\u6e2c\u5668\u61c9\u7528\u65bc\u66f4\u5927\u7684 13B \u6a21\u578b (\u7121\u9700\u91cd\u65b0\u8a13\u7df4)\u3002\u6211\u5011\u9032\u4e00\u6b65\u61c9\u7528\u6b64\u6aa2\u6e2c\u5668\u4f86\u6e1b\u8f15\u8a9e\u5883\u5e7b\u89ba\uff0c\u4e26\u767c\u73fe\u4e00\u500b\u7c21\u55ae\u7684\u5206\u985e\u5668\u5f15\u5c0e\u7684\u89e3\u78bc\u65b9\u6cd5\u80fd\u5920\u6e1b\u5c11\u5e7b\u89ba\u91cf\uff0c\u4f8b\u5982\u5728 XSum \u6458\u8981\u4efb\u52d9\u4e2d\u6e1b\u5c11 9.6%\u3002", "author": "Yung-Sung Chuang et.al.", "authors": "Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James Glass", "id": "2407.07071v1", "paper_url": "http://arxiv.org/abs/2407.07071v1", "repo": "https://github.com/voidism/lookback-lens"}}