{"2502.03860": {"publish_time": "2025-02-06", "title": "BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation", "paper_summary": "Large language models (LLMs), such as o1 from OpenAI, have demonstrated\nremarkable reasoning capabilities. o1 generates a long chain-of-thought\n(LongCoT) before answering a question. LongCoT allows LLMs to analyze problems,\ndevise plans, reflect, and backtrack effectively. These actions empower LLM to\nsolve complex problems. After the release of o1, many teams have attempted to\nreplicate its LongCoT and reasoning capabilities. In terms of methods, they\nprimarily rely on knowledge distillation with data from existing models with\nLongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving\nsignificant uncertainties on systematically developing such reasoning\nabilities. In terms of data domains, these works focus narrowly on math while a\nfew others include coding, limiting their generalizability. This paper\nintroduces a novel approach to enable LLM's LongCoT capacity without\ndistillation from o1-like models or expensive human annotations, where we\nbootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three\nstages: 1) LongCoT data bootstrapping with in-context learning on a standard\ninstruct model; 2) LongCoT supervised finetuning; 3) online training to further\nrefine LongCoT capacities. In BOLT, only a few in-context examples need to be\nconstructed during the bootstrapping stage; in our experiments, we created 10\nexamples, demonstrating the feasibility of this approach. We use\nLlama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various\nmodel scales (7B, 8B, 70B). We achieve impressive performance on a variety of\nbenchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which\nevaluate diverse task-solving and reasoning capabilities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4f8b\u5982 OpenAI \u7684 o1\uff0c\u5df2\u7d93\u5c55\u793a\u51fa\u975e\u51e1\u7684\u63a8\u7406\u80fd\u529b\u3002o1 \u5728\u56de\u7b54\u554f\u984c\u4e4b\u524d\u6703\u7522\u751f\u4e00\u500b\u9577\u93c8\u7684\u60f3\u6cd5 (LongCoT)\u3002LongCoT \u5141\u8a31 LLM \u5206\u6790\u554f\u984c\u3001\u5236\u5b9a\u8a08\u5283\u3001\u53cd\u601d\u548c\u6709\u6548\u56de\u6eaf\u3002\u9019\u4e9b\u52d5\u4f5c\u8ce6\u80fd LLM \u89e3\u6c7a\u8907\u96dc\u7684\u554f\u984c\u3002\u5728 o1 \u767c\u5e03\u5f8c\uff0c\u8a31\u591a\u5718\u968a\u90fd\u5617\u8a66\u8907\u88fd\u5176 LongCoT \u548c\u63a8\u7406\u80fd\u529b\u3002\u5728\u65b9\u6cd5\u65b9\u9762\uff0c\u4ed6\u5011\u4e3b\u8981\u4f9d\u8cf4\u65bc\u5177\u6709 LongCoT \u80fd\u529b\u7684\u73fe\u6709\u6a21\u578b\u7684\u6578\u64da\u9032\u884c\u77e5\u8b58\u84b8\u993e\uff08\u4f8b\u5982\uff0cOpenAI-o1\u3001Qwen-QwQ\u3001DeepSeek-R1-Preview\uff09\uff0c\u5728\u7cfb\u7d71\u6027\u5730\u958b\u767c\u9019\u7a2e\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7559\u4e0b\u91cd\u5927\u7684\u4e0d\u78ba\u5b9a\u6027\u3002\u5728\u6578\u64da\u9818\u57df\u65b9\u9762\uff0c\u9019\u4e9b\u5de5\u4f5c\u72f9\u9698\u5730\u96c6\u4e2d\u5728\u6578\u5b78\u4e0a\uff0c\u800c\u5176\u4ed6\u4e00\u4e9b\u5247\u5305\u62ec\u7de8\u78bc\uff0c\u9650\u5236\u4e86\u5b83\u5011\u7684\u666e\u904d\u6027\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f9e\u985e\u4f3c o1 \u7684\u6a21\u578b\u6216\u6602\u8cb4\u7684\u4eba\u5de5\u8a3b\u91cb\u4e2d\u9032\u884c\u84b8\u993e\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe LLM \u7684 LongCoT \u80fd\u529b\uff0c\u6211\u5011\u5f9e\u6a19\u6e96\u6307\u4ee4\u6a21\u578b\u4e2d\u5f15\u5c0e LongCoT (BOLT)\u3002BOLT \u6d89\u53ca\u4e09\u500b\u968e\u6bb5\uff1a1) \u5728\u6a19\u6e96\u6307\u4ee4\u6a21\u578b\u4e0a\u901a\u904e\u8a9e\u5883\u5b78\u7fd2\u5f15\u5c0e LongCoT \u6578\u64da\uff1b2) LongCoT \u76e3\u7763\u5fae\u8abf\uff1b3) \u5728\u7dda\u8a13\u7df4\u4ee5\u9032\u4e00\u6b65\u5b8c\u5584 LongCoT \u80fd\u529b\u3002\u5728 BOLT \u4e2d\uff0c\u5728\u5f15\u5c0e\u968e\u6bb5\u53ea\u9700\u8981\u69cb\u9020\u5e7e\u500b\u8a9e\u5883\u7bc4\u4f8b\uff1b\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5275\u5efa\u4e86 10 \u500b\u7bc4\u4f8b\uff0c\u8b49\u660e\u4e86\u9019\u7a2e\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002\u6211\u5011\u4f7f\u7528 Llama-3.1-70B-Instruct \u5f15\u5c0e LongCoT\uff0c\u4e26\u5c07\u6211\u5011\u7684\u6a21\u578b\u61c9\u7528\u65bc\u5404\u7a2e\u6a21\u578b\u898f\u6a21\uff087B\u30018B\u300170B\uff09\u3002\u6211\u5011\u5728\u5404\u7a2e\u57fa\u6e96\u6e2c\u8a66\u4e2d\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0cArena-Hard\u3001MT-Bench\u3001WildBench\u3001ZebraLogic\u3001MATH500\uff0c\u9019\u4e9b\u6e2c\u8a66\u8a55\u4f30\u4e86\u4e0d\u540c\u7684\u4efb\u52d9\u89e3\u6c7a\u548c\u63a8\u7406\u80fd\u529b\u3002", "author": "Bo Pang et.al.", "authors": "Bo Pang, Hanze Dong, Jiacheng Xu, Silvio Savarese, Yingbo Zhou, Caiming Xiong", "id": "2502.03860v1", "paper_url": "http://arxiv.org/abs/2502.03860v1", "repo": "null"}}