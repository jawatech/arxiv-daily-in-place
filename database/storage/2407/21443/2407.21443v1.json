{"2407.21443": {"publish_time": "2024-07-31", "title": "Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency", "paper_summary": "Despite large language models (LLMs) have demonstrated impressive performance\nin various tasks, they are still suffering from the factual inconsistency\nproblem called hallucinations. For instance, LLMs occasionally generate content\nthat diverges from source article, and prefer to extract information that\nappears at the beginning and end of the context, especially in long document\nsummarization. Inspired by these findings, we propose to improve the\nfaithfulness of LLMs in summarization by impelling them to process the entire\narticle more fairly and faithfully. We present a novel summary generation\nstrategy, namely SliSum, which exploits the ideas of sliding windows and\nself-consistency. Specifically, SliSum divides the source article into\noverlapping windows, and utilizes LLM to generate local summaries for the\ncontent in the windows. Finally, SliSum aggregates all local summaries using\nclustering and majority voting algorithm to produce more faithful summary of\nentire article. Extensive experiments demonstrate that SliSum significantly\nimproves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and\nGPT-3.5 in both short and long text summarization, while maintaining their\nfluency and informativeness and without additional fine-tuning and resources.\nWe further conduct qualitative and quantitative studies to investigate why\nSliSum works and impacts of hyperparameters in SliSum on performance.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\uff0c\u4f46\u5b83\u5011\u4ecd\u98fd\u53d7\u7a31\u70ba\u5e7b\u89ba\u7684\u865b\u5047\u4e0d\u4e00\u81f4\u554f\u984c\u6240\u82e6\u3002\u4f8b\u5982\uff0cLLM \u6709\u6642\u6703\u7522\u751f\u8207\u539f\u59cb\u6587\u7ae0\u4e0d\u540c\u7684\u5167\u5bb9\uff0c\u4e14\u504f\u597d\u64f7\u53d6\u51fa\u73fe\u5728\u5167\u5bb9\u958b\u982d\u548c\u7d50\u5c3e\u7684\u8cc7\u8a0a\uff0c\u7279\u5225\u662f\u5728\u9577\u7bc7\u6587\u4ef6\u6458\u8981\u4e2d\u3002\u53d7\u5230\u9019\u4e9b\u767c\u73fe\u7684\u555f\u767c\uff0c\u6211\u5011\u5efa\u8b70\u900f\u904e\u4fc3\u4f7f LLM \u66f4\u516c\u5e73\u4e14\u5fe0\u5be6\u5730\u8655\u7406\u6574\u7bc7\u6587\u7ae0\uff0c\u4f86\u6539\u5584\u5176\u5728\u6458\u8981\u4e2d\u7684\u771f\u5be6\u6027\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u6458\u8981\u7522\u751f\u7b56\u7565\uff0c\u7a31\u70ba SliSum\uff0c\u5b83\u5229\u7528\u6ed1\u52d5\u8996\u7a97\u548c\u81ea\u6211\u4e00\u81f4\u6027\u7684\u6982\u5ff5\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cSliSum \u5c07\u539f\u59cb\u6587\u7ae0\u5206\u6210\u91cd\u758a\u7684\u8996\u7a97\uff0c\u4e26\u5229\u7528 LLM \u70ba\u8996\u7a97\u4e2d\u7684\u5167\u5bb9\u7522\u751f\u5c40\u90e8\u6458\u8981\u3002\u6700\u5f8c\uff0cSliSum \u4f7f\u7528\u7fa4\u96c6\u548c\u591a\u6578\u6c7a\u6f14\u7b97\u6cd5\u5f59\u7e3d\u6240\u6709\u5c40\u90e8\u6458\u8981\uff0c\u4ee5\u7522\u751f\u66f4\u5fe0\u5be6\u7684\u6574\u7bc7\u6587\u7ae0\u6458\u8981\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cSliSum \u5728\u7c21\u77ed\u548c\u9577\u7bc7\u6587\u5b57\u6458\u8981\u4e2d\uff0c\u986f\u8457\u63d0\u5347\u4e86 LLaMA-2\u3001Claude-2 \u548c GPT-3.5 \u7b49\u4e0d\u540c LLM \u7684\u771f\u5be6\u6027\uff0c\u540c\u6642\u7dad\u6301\u5176\u6d41\u66a2\u5ea6\u548c\u8cc7\u8a0a\u91cf\uff0c\u4e14\u7121\u9700\u984d\u5916\u7684\u5fae\u8abf\u548c\u8cc7\u6e90\u3002\u6211\u5011\u9032\u4e00\u6b65\u9032\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u7814\u7a76\uff0c\u4ee5\u63a2\u8a0e SliSum \u7684\u904b\u4f5c\u539f\u7406\uff0c\u4ee5\u53ca SliSum \u4e2d\u7684\u8d85\u53c3\u6578\u5c0d\u6548\u80fd\u7684\u5f71\u97ff\u3002", "author": "Taiji Li et.al.", "authors": "Taiji Li, Zhi Li, Yin Zhang", "id": "2407.21443v1", "paper_url": "http://arxiv.org/abs/2407.21443v1", "repo": "null"}}