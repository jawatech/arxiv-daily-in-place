{"2408.14690": {"publish_time": "2024-08-26", "title": "Training-Free Activation Sparsity in Large Language Models", "paper_summary": "Activation sparsity can enable practical inference speedups in large language\nmodels (LLMs) by reducing the compute and memory-movement required for matrix\nmultiplications during the forward pass. However, existing methods face\nlimitations that inhibit widespread adoption. Some approaches are tailored\ntowards older models with ReLU-based sparsity, while others require extensive\ncontinued pre-training on up to hundreds of billions of tokens. This paper\ndescribes TEAL, a simple training-free method that applies magnitude-based\nactivation sparsity to hidden states throughout the entire model. TEAL achieves\n40-50% model-wide sparsity with minimal performance degradation across Llama-2,\nLlama-3, and Mistral families, with sizes varying from 7B to 70B. We improve\nexisting sparse kernels and demonstrate wall-clock decoding speed-ups of up to\n1.53$\\times$ and 1.8$\\times$ at 40% and 50% model-wide sparsity. TEAL is\ncompatible with weight quantization, enabling further efficiency gains.", "paper_summary_zh": "\u6fc0\u6d3b\u7a00\u758f\u6027\u53ef\u901a\u8fc7\u51cf\u5c11\u6b63\u5411\u4f20\u9012\u671f\u95f4\u77e9\u9635\u4e58\u6cd5\u6240\u9700\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u79fb\u52a8\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u5b9e\u73b0\u5b9e\u9645\u63a8\u7406\u52a0\u901f\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7740\u963b\u788d\u5e7f\u6cdb\u91c7\u7528\u7684\u5c40\u9650\u6027\u3002\u4e00\u4e9b\u65b9\u6cd5\u9488\u5bf9\u57fa\u4e8e ReLU \u7a00\u758f\u6027\u7684\u65e7\u6a21\u578b\u8fdb\u884c\u5b9a\u5236\uff0c\u800c\u53e6\u4e00\u4e9b\u65b9\u6cd5\u5219\u9700\u8981\u5728\u9ad8\u8fbe\u6570\u767e\u4ebf\u4e2a\u6807\u8bb0\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u3002\u672c\u6587\u63cf\u8ff0\u4e86 TEAL\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b83\u5c06\u57fa\u4e8e\u5e45\u5ea6\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\u5e94\u7528\u4e8e\u6574\u4e2a\u6a21\u578b\u4e2d\u7684\u9690\u85cf\u72b6\u6001\u3002TEAL \u5728 Llama-2\u3001Llama-3 \u548c Mistral \u7cfb\u5217\u4e2d\u5b9e\u73b0\u4e86 40-50% \u7684\u6a21\u578b\u8303\u56f4\u7a00\u758f\u6027\uff0c\u540c\u65f6\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\uff0c\u5176\u5927\u5c0f\u4ece 7B \u5230 70B \u4e0d\u7b49\u3002\u6211\u4eec\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u7a00\u758f\u5185\u6838\uff0c\u5e76\u5c55\u793a\u4e86\u5728 40% \u548c 50% \u7684\u6a21\u578b\u8303\u56f4\u7a00\u758f\u6027\u4e0b\uff0c\u5899\u65f6\u89e3\u7801\u901f\u5ea6\u5206\u522b\u63d0\u9ad8\u4e86 1.53 \u500d\u548c 1.8 \u500d\u3002TEAL \u4e0e\u6743\u91cd\u91cf\u5316\u517c\u5bb9\uff0c\u4ece\u800c\u5b9e\u73b0\u8fdb\u4e00\u6b65\u7684\u6548\u7387\u63d0\u5347\u3002", "author": "James Liu et.al.", "authors": "James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, Ben Athiwaratkun", "id": "2408.14690v1", "paper_url": "http://arxiv.org/abs/2408.14690v1", "repo": "null"}}