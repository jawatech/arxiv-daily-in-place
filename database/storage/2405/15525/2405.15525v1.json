{"2405.15525": {"publish_time": "2024-05-24", "title": "Sparse Matrix in Large Language Model Fine-tuning", "paper_summary": "LoRA and its variants have become popular parameter-efficient fine-tuning\n(PEFT) methods due to their ability to avoid excessive computational costs.\nHowever, an accuracy gap often exists between PEFT methods and full fine-tuning\n(FT), and this gap has yet to be systematically studied. In this work, we\nintroduce a method for selecting sparse sub-matrices that aim to minimize the\nperformance gap between PEFT vs. full fine-tuning (FT) while also reducing both\nfine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT)\nmethod begins by identifying the most significant sub-matrices in the gradient\nupdate, updating only these blocks during the fine-tuning process. In our\nexperiments, we demonstrate that SMT consistently surpasses other PEFT baseline\n(e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA\nacross a broad spectrum of tasks, while reducing the GPU memory footprint by\n67% compared to FT. We also examine how the performance of LoRA and DoRA tends\nto plateau and decline as the number of trainable parameters increases, in\ncontrast, our SMT method does not suffer from such issue.", "paper_summary_zh": "LoRA \u53ca\u5176\u8b8a\u9ad4\u5df2\u6210\u70ba\u71b1\u9580\u7684\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\uff0c\u56e0\u70ba\u5b83\u5011\u80fd\u5920\u907f\u514d\u904e\u9ad8\u7684\u8a08\u7b97\u6210\u672c\u3002\n\u7136\u800c\uff0cPEFT \u65b9\u6cd5\u548c\u5b8c\u5168\u5fae\u8abf (FT) \u4e4b\u9593\u901a\u5e38\u5b58\u5728\u6e96\u78ba\u6027\u5dee\u8ddd\uff0c\u800c\u9019\u500b\u5dee\u8ddd\u5c1a\u672a\u7d93\u904e\u7cfb\u7d71\u6027\u7814\u7a76\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\n\u63d0\u51fa\u4e86\u4e00\u7a2e\u9078\u64c7\u7a00\u758f\u5b50\u77e9\u9663\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u6700\u5c0f\u5316 PEFT \u8207\u5b8c\u5168\u5fae\u8abf (FT) \u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\uff0c\u540c\u6642\u4e5f\u80fd\u964d\u4f4e\u5fae\u8abf\u7684\u8a08\u7b97\u6210\u672c\u548c\u8a18\u61b6\u9ad4\u6210\u672c\u3002\u6211\u5011\u7684\u7a00\u758f\u77e9\u9663\u5fae\u8abf (SMT) \u65b9\u6cd5\u9996\u5148\u8b58\u5225\u68af\u5ea6\u66f4\u65b0\u4e2d\u6700\u91cd\u8981\u7684\u5b50\u77e9\u9663\uff0c\u4e26\u5728\u5fae\u8abf\u904e\u7a0b\u4e2d\u50c5\u66f4\u65b0\u9019\u4e9b\u5340\u584a\u3002\u5728\u6211\u5011\u7684\n\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u8b49\u660e SMT \u5728\u5fae\u8abf LLaMA \u7b49\u71b1\u9580\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u65b9\u9762\u59cb\u7d42\u512a\u65bc\u5176\u4ed6 PEFT \u57fa\u6e96 (\u4f8b\u5982 LoRA \u548c DoRA)\uff0c\u540c\u6642\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c07 GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86\n67%\uff0c\u8207 FT \u76f8\u6bd4\u3002\u6211\u5011\u9084\u7814\u7a76\u4e86 LoRA \u548c DoRA \u7684\u6548\u80fd\u5982\u4f55\u96a8\u8457\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u589e\u52a0\u800c\u8da8\u65bc\u5e73\u7a69\u4e26\u4e0b\u964d\uff0c\n\u76f8\u53cd\uff0c\u6211\u5011\u7684 SMT \u65b9\u6cd5\u4e26\u4e0d\u5b58\u5728\u9019\u6a23\u7684\u554f\u984c\u3002", "author": "Haoze He et.al.", "authors": "Haoze He, Juncheng Billy Li, Xuan Jiang, Heather Miller", "id": "2405.15525v1", "paper_url": "http://arxiv.org/abs/2405.15525v1", "repo": "null"}}