# arxiv-daily
 Automated deployment @ 2024-05-08 13:47:17 Asia/Taipei
> Welcome to contribute! Add your topics and keywords in [`topic.yml`](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/topic.yml).
> You can also view historical data through the [storage](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/storage).

## AI

### Medical explainable AI
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-05**|**Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**|Zhusi Zhong et.al.|[2405.02815v1](http://arxiv.org/abs/2405.02815v1)|null|
|**2024-04-29**|**M3H: Multimodal Multitask Machine Learning for Healthcare**|Dimitris Bertsimas et.al.|[2404.18975v1](http://arxiv.org/abs/2404.18975v1)|null|
|**2024-04-27**|**Advancing Healthcare Automation: Multi-Agent Systems for Medical Necessity Justification**|Himanshu Pandey et.al.|[2404.17977v1](http://arxiv.org/abs/2404.17977v1)|null|
|**2024-04-26**|**Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**|Francesco Prinzi et.al.|[2405.02334v1](http://arxiv.org/abs/2405.02334v1)|null|
|**2024-04-25**|**Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**|Yunfei Ge et.al.|[2404.16957v1](http://arxiv.org/abs/2404.16957v1)|null|
|**2024-04-19**|**Explainable AI for Fair Sepsis Mortality Predictive Model**|Chia-Hsuan Chang et.al.|[2404.13139v1](http://arxiv.org/abs/2404.13139v1)|null|
|**2024-04-19**|**Multi Class Depression Detection Through Tweets using Artificial Intelligence**|Muhammad Osama Nusrat et.al.|[2404.13104v1](http://arxiv.org/abs/2404.13104v1)|[link](https://github.com/mnusrat786/masters-thesis)|
|**2024-04-19**|**COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**|Dmytro Shvetsov et.al.|[2404.12832v1](http://arxiv.org/abs/2404.12832v1)|[link](https://github.com/dmytro-shvetsov/counterfactual-search)|
|**2024-04-09**|**Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**|Milad Yousefi et.al.|[2404.07239v1](http://arxiv.org/abs/2404.07239v1)|null|
|**2024-04-06**|**Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**|Taminul Islam et.al.|[2404.04686v1](http://arxiv.org/abs/2404.04686v1)|null|
|**2024-04-05**|**Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**|Maryam Ahmed et.al.|[2404.03892v3](http://arxiv.org/abs/2404.03892v3)|null|
|**2024-03-26**|**Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**|Andrea Ferrario et.al.|[2403.17873v1](http://arxiv.org/abs/2403.17873v1)|null|
|**2024-03-26**|**Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**|Han Yuan et.al.|[2403.18871v1](http://arxiv.org/abs/2403.18871v1)|[link](https://github.com/han-yuan-med/template-explanation)|
|**2024-03-03**|**Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**|Séamus Lankford et.al.|[2403.01580v1](http://arxiv.org/abs/2403.01580v1)|null|
|**2024-02-28**|**Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**|Yasin Sadeghi Bazargani et.al.|[2402.18600v1](http://arxiv.org/abs/2402.18600v1)|null|
|**2024-02-22**|**Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**|A. J. Karran et.al.|[2402.15027v2](http://arxiv.org/abs/2402.15027v2)|null|
|**2024-02-12**|**Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**|Aruna Mohan et.al.|[2402.09474v2](http://arxiv.org/abs/2402.09474v2)|null|
|**2024-02-05**|**Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**|Aryan Agrawal et.al.|[2402.05127v1](http://arxiv.org/abs/2402.05127v1)|null|
|**2024-01-24**|**Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**|Timothée Schmude et.al.|[2401.13324v4](http://arxiv.org/abs/2401.13324v4)|null|
|**2024-01-02**|**Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**|Vahid Ashrafimoghari et.al.|[2401.02985v1](http://arxiv.org/abs/2401.02985v1)|null|
|**2023-12-29**|**XAI for In-hospital Mortality Prediction via Multimodal ICU Data**|Xingqiao Li et.al.|[2312.17624v1](http://arxiv.org/abs/2312.17624v1)|[link](https://github.com/lixingqiao/xai-icu)|
|**2023-12-22**|**Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**|Norman Zerbe et.al.|[2401.09450v2](http://arxiv.org/abs/2401.09450v2)|null|
|**2023-12-18**|**Robust Stochastic Graph Generator for Counterfactual Explanations**|Mario Alfonso Prado-Romero et.al.|[2312.11747v2](http://arxiv.org/abs/2312.11747v2)|null|
|**2023-12-10**|**Evaluating the Utility of Model Explanations for Model Development**|Shawn Im et.al.|[2312.06032v1](http://arxiv.org/abs/2312.06032v1)|null|
|**2023-12-05**|**Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**|Manas Gaur et.al.|[2312.06798v1](http://arxiv.org/abs/2312.06798v1)|null|
|**2023-11-28**|**Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**|Jacob R. Epifano et.al.|[2311.17133v1](http://arxiv.org/abs/2311.17133v1)|null|
|**2023-11-27**|**Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**|Benjamin Keel et.al.|[2311.15719v1](http://arxiv.org/abs/2311.15719v1)|[link](https://github.com/benkeel/vae_lung_lesion_bmvc)|
|**2023-11-24**|**MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**|Nathan Blake et.al.|[2311.14471v1](http://arxiv.org/abs/2311.14471v1)|null|
|**2023-11-21**|**Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**|Robert Gorwa et.al.|[2311.12573v2](http://arxiv.org/abs/2311.12573v2)|null|
|**2023-11-20**|**Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**|Muta Tah Hira et.al.|[2311.11932v1](http://arxiv.org/abs/2311.11932v1)|null|
|**2023-11-18**|**Representing visual classification as a linear combination of words**|Shobhit Agarwal et.al.|[2311.10933v1](http://arxiv.org/abs/2311.10933v1)|[link](https://github.com/lotterlab/task_word_explainability)|
|**2023-11-03**|**Towards objective and systematic evaluation of bias in medical imaging AI**|Emma A. M. Stanley et.al.|[2311.02115v1](http://arxiv.org/abs/2311.02115v1)|[link](https://github.com/estanley16/simba)|
|**2023-10-29**|**Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**|Adam White et.al.|[2310.19174v1](http://arxiv.org/abs/2310.19174v1)|null|
|**2023-10-03**|**Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**|Hossein Shreim et.al.|[2310.01828v2](http://arxiv.org/abs/2310.01828v2)|[link](https://github.com/geoaigroup/geoai-ecrs2023)|
|**2023-09-26**|**Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**|Muhammad Aurangzeb Ahmad et.al.|[2311.01463v1](http://arxiv.org/abs/2311.01463v1)|null|
|**2023-09-20**|**When to Trust AI: Advances and Challenges for Certification of Neural Networks**|Marta Kwiatkowska et.al.|[2309.11196v1](http://arxiv.org/abs/2309.11196v1)|null|
|**2023-09-19**|**Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**|Juan M. García-Gómez et.al.|[2309.10424v1](http://arxiv.org/abs/2309.10424v1)|null|
|**2023-09-19**|**QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**|Thanveer Shaik et.al.|[2309.10293v3](http://arxiv.org/abs/2309.10293v3)|null|
|**2023-09-18**|**Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**|Adarsa Sivaprasad et.al.|[2309.09917v1](http://arxiv.org/abs/2309.09917v1)|null|
|**2023-09-02**|**An explainable three dimension framework to uncover learning patterns: A unified look in variable sulci recognition**|Michail Mamalakis et.al.|[2309.00903v2](http://arxiv.org/abs/2309.00903v2)|[link](https://github.com/ece7048/3dsulci)|
|**2023-08-28**|**Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**|Yanjun Gao et.al.|[2308.14321v1](http://arxiv.org/abs/2308.14321v1)|null|
|**2023-08-18**|**Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**|Yun Xin Teoh et.al.|[2308.09380v1](http://arxiv.org/abs/2308.09380v1)|null|
|**2023-08-16**|**Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**|Munib Mesinovic et.al.|[2308.08407v1](http://arxiv.org/abs/2308.08407v1)|null|
|**2023-08-11**|**FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**|Karim Lekadir et.al.|[2309.12325v1](http://arxiv.org/abs/2309.12325v1)|null|
|**2023-08-10**|**Explainable AI applications in the Medical Domain: a systematic review**|Nicoletta Prentzas et.al.|[2308.05411v1](http://arxiv.org/abs/2308.05411v1)|null|
|**2023-08-01**|**Exploring the Role of Explainability in AI-Assisted Embryo Selection**|Lucia Urcelay et.al.|[2308.02534v1](http://arxiv.org/abs/2308.02534v1)|null|
|**2023-07-26**|**A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**|Timo Speith et.al.|[2307.14246v1](http://arxiv.org/abs/2307.14246v1)|null|
|**2023-07-26**|**Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**|Barnaby Crook et.al.|[2307.14239v1](http://arxiv.org/abs/2307.14239v1)|null|
|**2023-07-26**|**Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**|Henry Fraser et.al.|[2308.02047v1](http://arxiv.org/abs/2308.02047v1)|null|
|**2023-07-21**|**eXplainable Artificial Intelligence (XAI) in aging clock models**|Alena Kalyakulina et.al.|[2307.13704v3](http://arxiv.org/abs/2307.13704v3)|null|
|**2023-07-19**|**Interpreting and Correcting Medical Image Classification with PIP-Net**|Meike Nauta et.al.|[2307.10404v2](http://arxiv.org/abs/2307.10404v2)|[link](https://github.com/m-nauta/pipnet)|
|**2023-07-15**|**Explaining and visualizing black-box models through counterfactual paths**|Bastian Pfeifer et.al.|[2307.07764v3](http://arxiv.org/abs/2307.07764v3)|[link](https://github.com/pievos101/cpath)|
|**2023-07-05**|**Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**|Toygar Tanyel et.al.|[2307.02131v5](http://arxiv.org/abs/2307.02131v5)|[link](https://github.com/toygarr/counterfactual-explanations-for-medical-research)|
|**2023-06-30**|**AI and Non AI Assessments for Dementia**|Mahboobeh Parsapoor et.al.|[2307.01210v1](http://arxiv.org/abs/2307.01210v1)|null|
|**2023-06-12**|**Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**|Ruitao Xie et.al.|[2306.07306v1](http://arxiv.org/abs/2306.07306v1)|null|
|**2023-06-09**|**HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**|Rodrigo Agerri et.al.|[2306.06029v1](http://arxiv.org/abs/2306.06029v1)|null|
|**2023-06-07**|**XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**|Eli Laird et.al.|[2306.04791v1](http://arxiv.org/abs/2306.04791v1)|null|
|**2023-06-06**|**Explainable AI using expressive Boolean formulas**|Gili Rosenberg et.al.|[2306.03976v1](http://arxiv.org/abs/2306.03976v1)|null|
|**2023-06-06**|**Utterance Classification with Logical Neural Network: Explainable AI for Mental Disorder Diagnosis**|Yeldar Toleubay et.al.|[2306.03902v1](http://arxiv.org/abs/2306.03902v1)|null|
|**2023-06-02**|**XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models**|Sujith K Mandala et.al.|[2306.01668v1](http://arxiv.org/abs/2306.01668v1)|null|
|**2023-05-26**|**A Novel real-time arrhythmia detection model using YOLOv8**|Guang Jun Nicholas Ang et.al.|[2305.16727v3](http://arxiv.org/abs/2305.16727v3)|null|
|**2023-05-22**|**Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI**|Jai Vardhan et.al.|[2305.14389v2](http://arxiv.org/abs/2305.14389v2)|null|
|**2023-05-18**|**What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media**|Junwei Kuang et.al.|[2305.13127v2](http://arxiv.org/abs/2305.13127v2)|null|
|**2023-05-17**|**Echoes of Biases: How Stigmatizing Language Affects AI Performance**|Yizhi Liu et.al.|[2305.10201v4](http://arxiv.org/abs/2305.10201v4)|null|
|**2023-05-05**|**Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour**|Goda Klumbyte et.al.|[2305.03376v1](http://arxiv.org/abs/2305.03376v1)|null|
|**2023-04-25**|**Towards Explainable and Safe Conversational Agents for Mental Health: A Survey**|Surjodeep Sarkar et.al.|[2304.13191v1](http://arxiv.org/abs/2304.13191v1)|null|
|**2023-04-04**|**A Brief Review of Explainable Artificial Intelligence in Healthcare**|Zahra Sadeghi et.al.|[2304.01543v1](http://arxiv.org/abs/2304.01543v1)|null|
|**2023-03-22**|**Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models**|Frederik Pahde et.al.|[2303.12641v2](http://arxiv.org/abs/2303.12641v2)|[link](https://github.com/maxdreyer/reveal2revise)|
|**2023-03-11**|**Explainable AI for Time Series via Virtual Inspection Layers**|Johanna Vielhaben et.al.|[2303.06365v1](http://arxiv.org/abs/2303.06365v1)|null|
|**2023-03-08**|**Towards Trust of Explainable AI in Thyroid Nodule Diagnosis**|Truong Thanh Hung Nguyen et.al.|[2303.04731v1](http://arxiv.org/abs/2303.04731v1)|[link](https://github.com/hungntt/xai_thyroid)|
|**2023-03-06**|**Cybersecurity of AI medical devices: risks, legislation, and challenges**|Elisabetta Biasin et.al.|[2303.03140v1](http://arxiv.org/abs/2303.03140v1)|null|
|**2023-02-06**|**LAVA: Granular Neuron-Level Explainable AI for Alzheimer's Disease Assessment from Fundus Images**|Nooshin Yousefzadeh et.al.|[2302.03008v2](http://arxiv.org/abs/2302.03008v2)|null|
|**2023-02-02**|**Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses**|Brian Y. Lim et.al.|[2302.01241v2](http://arxiv.org/abs/2302.01241v2)|null|
|**2023-02-02**|**LesionAid: Vision Transformers-based Skin Lesion Generation and Classification**|Ghanta Sai Krishna et.al.|[2302.01104v1](http://arxiv.org/abs/2302.01104v1)|null|
|**2023-02-01**|**SkinCon: A skin disease dataset densely annotated by domain experts for fine-grained model debugging and analysis**|Roxana Daneshjou et.al.|[2302.00785v1](http://arxiv.org/abs/2302.00785v1)|null|
|**2023-01-19**|**Decision-Focused Evaluation: Analyzing Performance of Deployed Restless Multi-Arm Bandits**|Paritosh Verma et.al.|[2301.07835v1](http://arxiv.org/abs/2301.07835v1)|null|
|**2023-01-18**|**Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling**|Carlo Metta et.al.|[2302.03033v1](http://arxiv.org/abs/2302.03033v1)|null|
|**2023-01-17**|**Monotonicity for AI ethics and society: An empirical study of the monotonic neural additive model in criminology, education, health care, and finance**|Dangxing Chen et.al.|[2301.07060v1](http://arxiv.org/abs/2301.07060v1)|null|
|**2023-01-15**|**Rationalizing Predictions by Adversarial Information Calibration**|Lei Sha et.al.|[2301.06009v1](http://arxiv.org/abs/2301.06009v1)|null|
|**2023-01-05**|**Semantic match: Debugging feature attribution methods in XAI for healthcare**|Giovanni Cinà et.al.|[2301.02080v3](http://arxiv.org/abs/2301.02080v3)|null|
|**2022-12-17**|**Context-dependent Explainability and Contestability for Trustworthy Medical Artificial Intelligence: Misclassification Identification of Morbidity Recognition Models in Preterm Infants**|Isil Guzey et.al.|[2212.08821v1](http://arxiv.org/abs/2212.08821v1)|null|
|**2022-12-16**|**It is not "accuracy vs. explainability" -- we need both for trustworthy AI systems**|D. Petkovic et.al.|[2212.11136v2](http://arxiv.org/abs/2212.11136v2)|null|
|**2022-12-02**|**SimpleMind adds thinking to deep neural networks**|Youngwon Choi et.al.|[2212.00951v1](http://arxiv.org/abs/2212.00951v1)|[link](https://gitlab.com/sm-ai-team/simplemind)|
|**2022-11-27**|**Attribution-based XAI Methods in Computer Vision: A Review**|Kumar Abhishek et.al.|[2211.14736v1](http://arxiv.org/abs/2211.14736v1)|null|
|**2022-11-08**|**Privacy Meets Explainability: A Comprehensive Impact Benchmark**|Saifullah Saifullah et.al.|[2211.04110v1](http://arxiv.org/abs/2211.04110v1)|null|
|**2022-11-05**|**Predicting Treatment Adherence of Tuberculosis Patients at Scale**|Mihir Kulkarni et.al.|[2211.02943v2](http://arxiv.org/abs/2211.02943v2)|null|
|**2022-11-02**|**Explainable AI over the Internet of Things (IoT): Overview, State-of-the-Art and Future Directions**|Senthil Kumar Jagatheesaperumal et.al.|[2211.01036v2](http://arxiv.org/abs/2211.01036v2)|null|
|**2022-10-24**|**Human-centered XAI for Burn Depth Characterization**|Maxwell J. Jacobson et.al.|[2210.13535v2](http://arxiv.org/abs/2210.13535v2)|null|
|**2022-10-07**|**What Do End-Users Really Want? Investigation of Human-Centered XAI for Mobile Health Apps**|Katharina Weitz et.al.|[2210.03506v1](http://arxiv.org/abs/2210.03506v1)|null|
|**2022-10-07**|**Explainable AI based Glaucoma Detection using Transfer Learning and LIME**|Touhidul Islam Chayan et.al.|[2210.03332v1](http://arxiv.org/abs/2210.03332v1)|null|
|**2022-09-30**|**Evaluation of importance estimators in deep learning classifiers for Computed Tomography**|Lennart Brocki et.al.|[2209.15398v1](http://arxiv.org/abs/2209.15398v1)|null|
|**2022-09-30**|**An Interactive Interpretability System for Breast Cancer Screening with Deep Learning**|Yuzhe Lu et.al.|[2210.08979v1](http://arxiv.org/abs/2210.08979v1)|null|
|**2022-09-14**|**Explainable AI for clinical and remote health applications: a survey on tabular and time series data**|Flavio Di Martino et.al.|[2209.06528v1](http://arxiv.org/abs/2209.06528v1)|null|
|**2022-08-31**|**Enhancing Early Lung Cancer Detection on Chest Radiographs with AI-assistance: A Multi-Reader Study**|Gaetan Dissez et.al.|[2208.14742v1](http://arxiv.org/abs/2208.14742v1)|null|
|**2022-08-24**|**GAN-based generative modelling for dermatological applications -- comparative study**|Sandra Carrasco Limeros et.al.|[2208.11702v1](http://arxiv.org/abs/2208.11702v1)|[link](https://github.com/aidotse/stylegan2-ada-pytorch)|
|**2022-08-05**|**Planning and Scheduling in Digital Health with Answer Set Programming**|Marco Mochi et.al.|[2208.03099v1](http://arxiv.org/abs/2208.03099v1)|null|
|**2022-07-26**|**AI Approaches in Processing and Using Data in Personalized Medicine**|Mirjana Ivanovic et.al.|[2208.04698v1](http://arxiv.org/abs/2208.04698v1)|null|
|**2022-07-22**|**TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring**|Nandita Bhaskhar et.al.|[2207.11290v2](http://arxiv.org/abs/2207.11290v2)|[link](https://github.com/nanbhas/trustlapse)|
|**2022-07-12**|**Revealing Unfair Models by Mining Interpretable Evidence**|Mohit Bajaj et.al.|[2207.05811v1](http://arxiv.org/abs/2207.05811v1)|null|
|**2022-07-11**|**From Correlation to Causation: Formalizing Interpretable Machine Learning as a Statistical Process**|Lukas Klein et.al.|[2207.04969v1](http://arxiv.org/abs/2207.04969v1)|null|

#### Abstracts
##### **Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**
2405.02815v1 by Zhusi Zhong, Jie Li, Zhuoqi Ma, Scott Collins, Harrison Bai, Paul Zhang, Terrance Healey, Xinbo Gao, Michael K. Atalay, Zhicheng Jiao

The COVID-19 pandemic has strained global public health, necessitating
accurate diagnosis and intervention to control disease spread and reduce
mortality rates. This paper introduces an interpretable deep survival
prediction model designed specifically for improved understanding and trust in
COVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale
pretrained image encoder, Risk-specific Grad-CAM, and anatomical region
detection techniques, our approach produces regional interpretable outcomes
that effectively capture essential disease features while focusing on rare but
critical abnormal regions. Our model's predictive results provide enhanced
clarity and transparency through risk area localization, enabling clinicians to
make informed decisions regarding COVID-19 diagnosis with better understanding
of prognostic insights. We evaluate the proposed method on a multi-center
survival dataset and demonstrate its effectiveness via quantitative and
qualitative assessments, achieving superior C-indexes (0.764 and 0.727) and
time-dependent AUCs (0.799 and 0.691). These results suggest that our
explainable deep survival prediction model surpasses traditional survival
analysis methods in risk prediction, improving interpretability for clinical
decision making and enhancing AI system trustworthiness.

摘要：

##### **M3H: Multimodal Multitask Machine Learning for Healthcare**
2404.18975v1 by Dimitris Bertsimas, Yu Ma

Recent breakthroughs in AI are poised to fundamentally enhance our study and
understanding of healthcare. The development of an integrated many-to-many
framework that leverages multiple data modality inputs for the analytical
modeling of multiple medical tasks, is critical for a unified understanding of
modern medicine. In this work, we introduce M3H, an explainable Multimodal
Multitask Machine Learning for Healthcare framework that consolidates learning
from diverse multimodal inputs across a broad spectrum of medical task
categories and machine learning problem classes. The modular design of the
framework ensures its generalizable data processing, task definition, and rapid
model prototyping, applicable to both clinical and operational healthcare
settings. We evaluate the M3H framework by validating models trained from four
modalities (tabular, time-series, language, and vision) on 41 medical tasks
across 4 machine learning problem classes. Our results demonstrate that M3H
consistently produces multitask models that outperform canonical single-task
models (by 1.1- 37.2%) across 37 disease diagnoses from 16 medical departments,
three hospital operation forecasts, and one patient phenotyping task: spanning
ML problem classes of supervised binary classification, multiclass
classification, regression, and clustering. Additionally, the framework
introduces a novel attention mechanism to balance self-exploitation (focus on
learning source task), and cross-exploration (encourage learning from other
tasks). Furthermore, M3H provides explainability insights on how joint learning
of additional tasks impacts the learning of source task using a proposed TIM
score, shedding light into the dynamics of task interdependencies. Its
adaptable architecture facilitates the customization and integration,
establishing it as a robust and scalable candidate solution for future
AI-driven healthcare systems.

摘要：

##### **Advancing Healthcare Automation: Multi-Agent Systems for Medical Necessity Justification**
2404.17977v1 by Himanshu Pandey, Akhil Amod, Shivang

This paper explores the application of Swarm-Structured Multi-Agent Systems
(MAS) to establish medical necessity, a process that involves a systematic
review of patient-specific medical structured and unstructured data against
clinical guidelines. We addressed this complex task by decomposing it into
smaller, more manageable sub-tasks. Each sub-task is handled by a specialized
AI agent. We conduct a systematic study of the impact of various prompting
strategies on these agents and benchmark different Large Language Models (LLMs)
to determine their accuracy in completing these tasks. Additionally, we
investigate how these agents can provide explainability, thereby enhancing
trust and transparency within the system.

摘要：

##### **Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**
2405.02334v1 by Francesco Prinzi, Carmelo Militello, Calogero Zarcaro, Tommaso Vincenzo Bartolotta, Salvatore Gaglio, Salvatore Vitabile

In the last years, artificial intelligence (AI) in clinical decision support
systems (CDSS) played a key role in harnessing machine learning and deep
learning architectures. Despite their promising capabilities, the lack of
transparency and explainability of AI models poses significant challenges,
particularly in medical contexts where reliability is a mandatory aspect.
Achieving transparency without compromising predictive accuracy remains a key
challenge. This paper presents a novel method, namely Rad4XCNN, to enhance the
predictive power of CNN-derived features with the interpretability inherent in
radiomic features. Rad4XCNN diverges from conventional methods based on
saliency map, by associating intelligible meaning to CNN-derived features by
means of Radiomics, offering new perspectives on explanation methods beyond
visualization maps. Using a breast cancer classification task as a case study,
we evaluated Rad4XCNN on ultrasound imaging datasets, including an online
dataset and two in-house datasets for internal and external validation. Some
key results are: i) CNN-derived features guarantee more robust accuracy when
compared against ViT-derived and radiomic features; ii) conventional
visualization map methods for explanation present several pitfalls; iii)
Rad4XCNN does not sacrifice model accuracy for their explainability; iv)
Rad4XCNN provides global explanation insights enabling the physician to analyze
the model outputs and findings. In addition, we highlight the importance of
integrating interpretability into AI models for enhanced trust and adoption in
clinical practice, emphasizing how our method can mitigate some concerns
related to explainable AI methods.

摘要：

##### **Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**
2404.16957v1 by Yunfei Ge, Quanyan Zhu

The pervasive integration of Artificial Intelligence (AI) has introduced
complex challenges in the responsibility and accountability in the event of
incidents involving AI-enabled systems. The interconnectivity of these systems,
ethical concerns of AI-induced incidents, coupled with uncertainties in AI
technology and the absence of corresponding regulations, have made traditional
responsibility attribution challenging. To this end, this work proposes a
Computational Reflective Equilibrium (CRE) approach to establish a coherent and
ethically acceptable responsibility attribution framework for all stakeholders.
The computational approach provides a structured analysis that overcomes the
limitations of conceptual approaches in dealing with dynamic and multifaceted
scenarios, showcasing the framework's explainability, coherence, and adaptivity
properties in the responsibility attribution process. We examine the pivotal
role of the initial activation level associated with claims in equilibrium
computation. Using an AI-assisted medical decision-support system as a case
study, we illustrate how different initializations lead to diverse
responsibility distributions. The framework offers valuable insights into
accountability in AI-induced incidents, facilitating the development of a
sustainable and resilient system through continuous monitoring, revision, and
reflection.

摘要：

##### **Explainable AI for Fair Sepsis Mortality Predictive Model**
2404.13139v1 by Chia-Hsuan Chang, Xiaoyang Wang, Christopher C. Yang

Artificial intelligence supports healthcare professionals with predictive
modeling, greatly transforming clinical decision-making. This study addresses
the crucial need for fairness and explainability in AI applications within
healthcare to ensure equitable outcomes across diverse patient demographics. By
focusing on the predictive modeling of sepsis-related mortality, we propose a
method that learns a performance-optimized predictive model and then employs
the transfer learning process to produce a model with better fairness. Our
method also introduces a novel permutation-based feature importance algorithm
aiming at elucidating the contribution of each feature in enhancing fairness on
predictions. Unlike existing explainability methods concentrating on explaining
feature contribution to predictive performance, our proposed method uniquely
bridges the gap in understanding how each feature contributes to fairness. This
advancement is pivotal, given sepsis's significant mortality rate and its role
in one-third of hospital deaths. Our method not only aids in identifying and
mitigating biases within the predictive model but also fosters trust among
healthcare stakeholders by improving the transparency and fairness of model
predictions, thereby contributing to more equitable and trustworthy healthcare
delivery.

摘要：

##### **Multi Class Depression Detection Through Tweets using Artificial Intelligence**
2404.13104v1 by Muhammad Osama Nusrat, Waseem Shahzad, Saad Ahmed Jamal

Depression is a significant issue nowadays. As per the World Health
Organization (WHO), in 2023, over 280 million individuals are grappling with
depression. This is a huge number; if not taken seriously, these numbers will
increase rapidly. About 4.89 billion individuals are social media users. People
express their feelings and emotions on platforms like Twitter, Facebook,
Reddit, Instagram, etc. These platforms contain valuable information which can
be used for research purposes. Considerable research has been conducted across
various social media platforms. However, certain limitations persist in these
endeavors. Particularly, previous studies were only focused on detecting
depression and the intensity of depression in tweets. Also, there existed
inaccuracies in dataset labeling. In this research work, five types of
depression (Bipolar, major, psychotic, atypical, and postpartum) were predicted
using tweets from the Twitter database based on lexicon labeling. Explainable
AI was used to provide reasoning by highlighting the parts of tweets that
represent type of depression. Bidirectional Encoder Representations from
Transformers (BERT) was used for feature extraction and training. Machine
learning and deep learning methodologies were used to train the model. The BERT
model presented the most promising results, achieving an overall accuracy of
0.96.

摘要：

##### **COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**
2404.12832v1 by Dmytro Shvetsov, Joonas Ariva, Marharyta Domnich, Raul Vicente, Dmytro Fishman

Deep learning is dramatically transforming the field of medical imaging and
radiology, enabling the identification of pathologies in medical images,
including computed tomography (CT) and X-ray scans. However, the performance of
deep learning models, particularly in segmentation tasks, is often limited by
the need for extensive annotated datasets. To address this challenge, the
capabilities of weakly supervised semantic segmentation are explored through
the lens of Explainable AI and the generation of counterfactual explanations.
The scope of this research is development of a novel counterfactual inpainting
approach (COIN) that flips the predicted classification label from abnormal to
normal by using a generative model. For instance, if the classifier deems an
input medical image X as abnormal, indicating the presence of a pathology, the
generative model aims to inpaint the abnormal region, thus reversing the
classifier's original prediction label. The approach enables us to produce
precise segmentations for pathologies without depending on pre-existing
segmentation masks. Crucially, image-level labels are utilized, which are
substantially easier to acquire than creating detailed segmentation masks. The
effectiveness of the method is demonstrated by segmenting synthetic targets and
actual kidney tumors from CT images acquired from Tartu University Hospital in
Estonia. The findings indicate that COIN greatly surpasses established
attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an
alternative counterfactual explanation method introduced by Singla et al. This
evidence suggests that COIN is a promising approach for semantic segmentation
of tumors in CT images, and presents a step forward in making deep learning
applications more accessible and effective in healthcare, where annotated data
is scarce.

摘要：

##### **Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**
2404.07239v1 by Milad Yousefi, Shadi Farabi Maleki, Ali Jafarizadeh, Mahya Ahmadpour Youshanlui, Aida Jafari, Siamak Pedrammehr, Roohallah Alizadehsani, Ryszard Tadeusiewicz, Pawel Plawiak

Thyroid cancer is an increasing global health concern that requires advanced
diagnostic methods. The application of AI and radiomics to thyroid cancer
diagnosis is examined in this review. A review of multiple databases was
conducted in compliance with PRISMA guidelines until October 2023. A
combination of keywords led to the discovery of an English academic publication
on thyroid cancer and related subjects. 267 papers were returned from the
original search after 109 duplicates were removed. Relevant studies were
selected according to predetermined criteria after 124 articles were eliminated
based on an examination of their abstract and title. After the comprehensive
analysis, an additional six studies were excluded. Among the 28 included
studies, radiomics analysis, which incorporates ultrasound (US) images,
demonstrated its effectiveness in diagnosing thyroid cancer. Various results
were noted, some of the studies presenting new strategies that outperformed the
status quo. The literature has emphasized various challenges faced by AI
models, including interpretability issues, dataset constraints, and operator
dependence. The synthesized findings of the 28 included studies mentioned the
need for standardization efforts and prospective multicenter studies to address
these concerns. Furthermore, approaches to overcome these obstacles were
identified, such as advances in explainable AI technology and personalized
medicine techniques. The review focuses on how AI and radiomics could transform
the diagnosis and treatment of thyroid cancer. Despite challenges, future
research on multidisciplinary cooperation, clinical applicability validation,
and algorithm improvement holds the potential to improve patient outcomes and
diagnostic precision in the treatment of thyroid cancer.

摘要：

##### **Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**
2404.04686v1 by Taminul Islam, Md. Alif Sheakh, Mst. Sazia Tahosin, Most. Hasna Hena, Shopnil Akash, Yousef A. Bin Jardan, Gezahign Fentahun Wondmie, Hiba-Allah Nafidi, Mohammed Bourhia

Breast cancer has rapidly increased in prevalence in recent years, making it
one of the leading causes of mortality worldwide. Among all cancers, it is by
far the most common. Diagnosing this illness manually requires significant time
and expertise. Since detecting breast cancer is a time-consuming process,
preventing its further spread can be aided by creating machine-based forecasts.
Machine learning and Explainable AI are crucial in classification as they not
only provide accurate predictions but also offer insights into how the model
arrives at its decisions, aiding in the understanding and trustworthiness of
the classification results. In this study, we evaluate and compare the
classification accuracy, precision, recall, and F-1 scores of five different
machine learning methods using a primary dataset (500 patients from Dhaka
Medical College Hospital). Five different supervised machine learning
techniques, including decision tree, random forest, logistic regression, naive
bayes, and XGBoost, have been used to achieve optimal results on our dataset.
Additionally, this study applied SHAP analysis to the XGBoost model to
interpret the model's predictions and understand the impact of each feature on
the model's output. We compared the accuracy with which several algorithms
classified the data, as well as contrasted with other literature in this field.
After final evaluation, this study found that XGBoost achieved the best model
accuracy, which is 97%.

摘要：

##### **Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**
2404.03892v3 by Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir

The Deep learning (DL) models for diagnosing breast cancer from mammographic
images often operate as "black boxes", making it difficult for healthcare
professionals to trust and understand their decision-making processes. The
study presents an integrated framework combining Convolutional Neural Networks
(CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis
of breast cancer using the CBIS-DDSM dataset. The methodology encompasses an
elaborate data preprocessing pipeline and advanced data augmentation techniques
to counteract dataset limitations and transfer learning using pre-trained
networks such as VGG-16, Inception-V3 and ResNet was employed. A focal point of
our study is the evaluation of XAI's effectiveness in interpreting model
predictions, highlighted by utilizing the Hausdorff measure to assess the
alignment between AI-generated explanations and expert annotations
quantitatively. This approach is critical for XAI in promoting trustworthiness
and ethical fairness in AI-assisted diagnostics. The findings from our research
illustrate the effective collaboration between CNNs and XAI in advancing
diagnostic methods for breast cancer, thereby facilitating a more seamless
integration of advanced AI technologies within clinical settings. By enhancing
the interpretability of AI driven decisions, this work lays the groundwork for
improved collaboration between AI systems and medical practitioners, ultimately
enriching patient care. Furthermore, the implications of our research extended
well beyond the current methodologies. It encourages further research into how
to combine multimodal data and improve AI explanations to meet the needs of
clinical practice.

摘要：

##### **Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**
2403.17873v1 by Andrea Ferrario, Alberto Termine, Alessandro Facchini

Human-centered explainable AI (HCXAI) advocates for the integration of social
aspects into AI explanations. Central to the HCXAI discourse is the Social
Transparency (ST) framework, which aims to make the socio-organizational
context of AI systems accessible to their users. In this work, we suggest
extending the ST framework to address the risks of social misattributions in
Large Language Models (LLMs), particularly in sensitive areas like mental
health. In fact LLMs, which are remarkably capable of simulating roles and
personas, may lead to mismatches between designers' intentions and users'
perceptions of social attributes, risking to promote emotional manipulation and
dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To
address these issues, we propose enhancing the ST framework with a fifth
'W-question' to clarify the specific social attributions assigned to LLMs by
its designers and users. This addition aims to bridge the gap between LLM
capabilities and user perceptions, promoting the ethically responsible
development and use of LLM-based technology.

摘要：

##### **Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**
2403.18871v1 by Han Yuan, Chuan Hong, Pengtao Jiang, Gangming Zhao, Nguyen Tuan Anh Tran, Xinxing Xu, Yet Yen Yan, Nan Liu

Background: Pneumothorax is an acute thoracic disease caused by abnormal air
collection between the lungs and chest wall. To address the opaqueness often
associated with deep learning (DL) models, explainable artificial intelligence
(XAI) methods have been introduced to outline regions related to pneumothorax
diagnoses made by DL models. However, these explanations sometimes diverge from
actual lesion areas, highlighting the need for further improvement. Method: We
propose a template-guided approach to incorporate the clinical knowledge of
pneumothorax into model explanations generated by XAI methods, thereby
enhancing the quality of these explanations. Utilizing one lesion delineation
created by radiologists, our approach first generates a template that
represents potential areas of pneumothorax occurrence. This template is then
superimposed on model explanations to filter out extraneous explanations that
fall outside the template's boundaries. To validate its efficacy, we carried
out a comparative analysis of three XAI methods with and without our template
guidance when explaining two DL models in two real-world datasets. Results: The
proposed approach consistently improved baseline XAI methods across twelve
benchmark scenarios built on three XAI methods, two DL models, and two
datasets. The average incremental percentages, calculated by the performance
improvements over the baseline performance, were 97.8% in Intersection over
Union (IoU) and 94.1% in Dice Similarity Coefficient (DSC) when comparing model
explanations and ground-truth lesion areas. Conclusions: In the context of
pneumothorax diagnoses, we proposed a template-guided approach for improving AI
explanations. We anticipate that our template guidance will forge a fresh
approach to elucidating AI models by integrating clinical domain expertise.

摘要：

##### **Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**
2403.01580v1 by Séamus Lankford

In the current machine translation (MT) landscape, the Transformer
architecture stands out as the gold standard, especially for high-resource
language pairs. This research delves into its efficacy for low-resource
language pairs including both the English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi language pairs. Notably, the study identifies
the optimal hyperparameters and subword model type to significantly improve the
translation quality of Transformer models for low-resource language pairs.
  The scarcity of parallel datasets for low-resource languages can hinder MT
development. To address this, gaHealth was developed, the first bilingual
corpus of health data for the Irish language. Focusing on the health domain,
models developed using this in-domain dataset exhibited very significant
improvements in BLEU score when compared with models from the LoResMT2021
Shared Task. A subsequent human evaluation using the multidimensional quality
metrics error taxonomy showcased the superior performance of the Transformer
system in reducing both accuracy and fluency errors compared to an RNN-based
counterpart.
  Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source
applications streamlined for the development, fine-tuning, and deployment of
neural machine translation models. These tools considerably simplify the setup
and evaluation process, making MT more accessible to both developers and
translators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes
eco-friendly natural language processing research by highlighting the
environmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM
demonstrated advancements in translation performance for two low-resource
language pairs: English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi, compared to baselines from the LoResMT2021
Shared Task.

摘要：

##### **Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**
2402.18600v1 by Yasin Sadeghi Bazargani, Majid Mirzaei, Navid Sobhi, Mirsaeed Abdollahi, Ali Jafarizadeh, Siamak Pedrammehr, Roohallah Alizadehsani, Ru San Tan, Sheikh Mohammed Shariful Islam, U. Rajendra Acharya

Diabetes mellitus (DM) predisposes patients to vascular complications.
Retinal images and vasculature reflect the body's micro- and macrovascular
health. They can be used to diagnose DM complications, including diabetic
retinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular
disease, as well as forecast the risk of cardiovascular events. Artificial
intelligence (AI)-enabled systems developed for high-throughput detection of DR
using digitized retinal images have become clinically adopted. Beyond DR
screening, AI integration also holds immense potential to address challenges
associated with the holistic care of the patient with DM. In this work, we aim
to comprehensively review the literature for studies on AI applications based
on retinal images related to DM diagnosis, prognostication, and management. We
will describe the findings of holistic AI-assisted diabetes care, including but
not limited to DR screening, and discuss barriers to implementing such systems,
including issues concerning ethics, data privacy, equitable access, and
explainability. With the ability to evaluate the patient's health status vis a
vis DM complication as well as risk prognostication of future cardiovascular
complications, AI-assisted retinal image analysis has the potential to become a
central tool for modern personalized medicine in patients with DM.

摘要：

##### **Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**
2402.15027v2 by A. J. Karran, P. Charland, J-T. Martineau, A. Ortiz de Guinea Lopez de Arana, AM. Lesage, S. Senecal, P-M. Leger

This study investigates the acceptability of different artificial
intelligence (AI) applications in education from a multi-stakeholder
perspective, including students, teachers, and parents. Acknowledging the
transformative potential of AI in education, it addresses concerns related to
data privacy, AI agency, transparency, explainability and the ethical
deployment of AI. Through a vignette methodology, participants were presented
with four scenarios where AI's agency, transparency, explainability, and
privacy were manipulated. After each scenario, participants completed a survey
that captured their perceptions of AI's global utility, individual usefulness,
justice, confidence, risk, and intention to use each scenario's AI if
available. The data collection comprising a final sample of 1198
multi-stakeholder participants was distributed through a partner institution
and social media campaigns and focused on individual responses to four AI use
cases. A mediation analysis of the data indicated that acceptance and trust in
AI varies significantly across stakeholder groups. We found that the key
mediators between high and low levels of AI's agency, transparency, and
explainability, as well as the intention to use the different educational AI,
included perceived global utility, justice, and confidence. The study
highlights that the acceptance of AI in education is a nuanced and multifaceted
issue that requires careful consideration of specific AI applications and their
characteristics, in addition to the diverse stakeholders' perceptions.

摘要：

##### **Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**
2402.09474v2 by Aruna Mohan, Danne Elbers, Or Zilbershot, Fatemeh Afghah, David Vorchheimer

Remote patient monitoring based on wearable single-lead electrocardiogram
(ECG) devices has significant potential for enabling the early detection of
heart disease, especially in combination with artificial intelligence (AI)
approaches for automated heart disease detection. There have been prior studies
applying AI approaches based on deep learning for heart disease detection.
However, these models are yet to be widely accepted as a reliable aid for
clinical diagnostics, in part due to the current black-box perception
surrounding many AI algorithms. In particular, there is a need to identify the
key features of the ECG signal that contribute toward making an accurate
diagnosis, thereby enhancing the interpretability of the model. In the present
study, we develop a vision transformer approach to identify atrial fibrillation
based on single-lead ECG data. A residual network (ResNet) approach is also
developed for comparison with the vision transformer approach. These models are
applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as
well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm
heartbeats. The models enable the identification of the key regions of the
heartbeat that determine the resulting classification, and highlight the
importance of P-waves and T-waves, as well as heartbeat duration and signal
amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and
sinus bradycardia.

摘要：

##### **Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**
2402.05127v1 by Aryan Agrawal

This paper introduces a novel paradigm for depression detection and treatment
using advanced Large Language Models (LLMs): Generative Pre-trained Transformer
4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized
prompts to diagnose, explain, and suggest therapeutic interventions for
depression. A unique few-shot prompting method enhances the models' ability to
analyze and explain depressive symptoms based on the DSM-5 criteria. In the
interaction phase, the models engage in empathetic dialogue management, drawing
from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide,
fostering supportive interactions with individuals experiencing major
depressive disorders. Additionally, the research introduces the Illuminate
Database, enriched with various CBT modules, aiding in personalized therapy
recommendations. The study evaluates LLM performance using metrics such as F1
scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) across different test sets, demonstrating their
effectiveness. This comprehensive approach blends cutting-edge AI with
established psychological methods, offering new possibilities in mental health
care and showcasing the potential of LLMs in revolutionizing depression
diagnosis and treatment strategies.

摘要：

##### **Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**
2401.13324v4 by Timothée Schmude, Laura Koesten, Torsten Möller, Sebastian Tschiatschek

Explanations of AI systems rarely address the information needs of people
affected by algorithmic decision-making (ADM). This gap between conveyed
information and information that matters to affected stakeholders can impede
understanding and adherence to regulatory frameworks such as the AI Act. To
address this gap, we present the "XAI Novice Question Bank": A catalog of
affected stakeholders' information needs in two ADM use cases (employment
prediction and health monitoring), covering the categories data, system
context, system usage, and system specifications. Information needs were
gathered in an interview study where participants received explanations in
response to their inquiries. Participants further reported their understanding
and decision confidence, showing that while confidence tended to increase after
receiving explanations, participants also met understanding challenges, such as
being unable to tell why their understanding felt incomplete. Explanations
further influenced participants' perceptions of the systems' risks and
benefits, which they confirmed or changed depending on the use case. When risks
were perceived as high, participants expressed particular interest in
explanations about intention, such as why and to what end a system was put in
place. With this work, we aim to support the inclusion of affected stakeholders
into explainability by contributing an overview of information and challenges
relevant to them when deciding on the adoption of ADM systems. We close by
summarizing our findings in a list of six key implications that inform the
design of future explanations for affected stakeholder audiences.

摘要：

##### **Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**
2401.02985v1 by Vahid Ashrafimoghari, Necdet Gürkan, Jordan W. Suchow

The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.

摘要：

##### **XAI for In-hospital Mortality Prediction via Multimodal ICU Data**
2312.17624v1 by Xingqiao Li, Jindong Gu, Zhiyong Wang, Yancheng Yuan, Bo Du, Fengxiang He

Predicting in-hospital mortality for intensive care unit (ICU) patients is
key to final clinical outcomes. AI has shown advantaged accuracy but suffers
from the lack of explainability. To address this issue, this paper proposes an
eXplainable Multimodal Mortality Predictor (X-MMP) approaching an efficient,
explainable AI solution for predicting in-hospital mortality via multimodal ICU
data. We employ multimodal learning in our framework, which can receive
heterogeneous inputs from clinical data and make decisions. Furthermore, we
introduce an explainable method, namely Layer-Wise Propagation to Transformer,
as a proper extension of the LRP method to Transformers, producing explanations
over multimodal inputs and revealing the salient features attributed to
prediction. Moreover, the contribution of each modality to clinical outcomes
can be visualized, assisting clinicians in understanding the reasoning behind
decision-making. We construct a multimodal dataset based on MIMIC-III and
MIMIC-III Waveform Database Matched Subset. Comprehensive experiments on
benchmark datasets demonstrate that our proposed framework can achieve
reasonable interpretation with competitive prediction accuracy. In particular,
our framework can be easily transferred to other clinical tasks, which
facilitates the discovery of crucial factors in healthcare research.

摘要：

##### **Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**
2401.09450v2 by Norman Zerbe, Lars Ole Schwen, Christian Geißler, Katja Wiesemann, Tom Bisson, Peter Boor, Rita Carvalho, Michael Franz, Christoph Jansen, Tim-Rasmus Kiehl, Björn Lindequist, Nora Charlotte Pohlan, Sarah Schmell, Klaus Strohmenger, Falk Zakrzewski, Markus Plass, Michael Takla, Tobias Küster, André Homeyer, Peter Hufnagl

Over the past decade, artificial intelligence (AI) methods in pathology have
advanced substantially. However, integration into routine clinical practice has
been slow due to numerous challenges, including technical and regulatory
hurdles in translating research results into clinical diagnostic products and
the lack of standardized interfaces. The open and vendor-neutral EMPAIA
initiative addresses these challenges. Here, we provide an overview of EMPAIA's
achievements and lessons learned. EMPAIA integrates various stakeholders of the
pathology AI ecosystem, i.e., pathologists, computer scientists, and industry.
In close collaboration, we developed technical interoperability standards,
recommendations for AI testing and product development, and explainability
methods. We implemented the modular and open-source EMPAIA platform and
successfully integrated 14 AI-based image analysis apps from 8 different
vendors, demonstrating how different apps can use a single standardized
interface. We prioritized requirements and evaluated the use of AI in real
clinical settings with 14 different pathology laboratories in Europe and Asia.
In addition to technical developments, we created a forum for all stakeholders
to share information and experiences on digital pathology and AI. Commercial,
clinical, and academic stakeholders can now adopt EMPAIA's common open-source
interfaces, providing a unique opportunity for large-scale standardization and
streamlining of processes. Further efforts are needed to effectively and
broadly establish AI assistance in routine laboratory use. To this end, a
sustainable infrastructure, the non-profit association EMPAIA International,
has been established to continue standardization and support broad
implementation and advocacy for an AI-assisted digital pathology future.

摘要：

##### **Robust Stochastic Graph Generator for Counterfactual Explanations**
2312.11747v2 by Mario Alfonso Prado-Romero, Bardh Prenkaj, Giovanni Stilo

Counterfactual Explanation (CE) techniques have garnered attention as a means
to provide insights to the users engaging with AI systems. While extensively
researched in domains such as medical imaging and autonomous vehicles, Graph
Counterfactual Explanation (GCE) methods have been comparatively
under-explored. GCEs generate a new graph similar to the original one, with a
different outcome grounded on the underlying predictive model. Among these GCE
techniques, those rooted in generative mechanisms have received relatively
limited investigation despite demonstrating impressive accomplishments in other
domains, such as artistic styles and natural language modelling. The preference
for generative explainers stems from their capacity to generate counterfactual
instances during inference, leveraging autonomously acquired perturbations of
the input graph. Motivated by the rationales above, our study introduces
RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual
Explanations able to produce counterfactual examples from the learned latent
space considering a partially ordered generation sequence. Furthermore, we
undertake quantitative and qualitative analyses to compare RSGG-CE's
performance against SoA generative explainers, highlighting its increased
ability to engendering plausible counterfactual candidates.

摘要：

##### **Evaluating the Utility of Model Explanations for Model Development**
2312.06032v1 by Shawn Im, Jacob Andreas, Yilun Zhou

One of the motivations for explainable AI is to allow humans to make better
and more informed decisions regarding the use and deployment of AI models. But
careful evaluations are needed to assess whether this expectation has been
fulfilled. Current evaluations mainly focus on algorithmic properties of
explanations, and those that involve human subjects often employ subjective
questions to test human's perception of explanation usefulness, without being
grounded in objective metrics and measurements. In this work, we evaluate
whether explanations can improve human decision-making in practical scenarios
of machine learning model development. We conduct a mixed-methods user study
involving image data to evaluate saliency maps generated by SmoothGrad,
GradCAM, and an oracle explanation on two tasks: model selection and
counterfactual simulation. To our surprise, we did not find evidence of
significant improvement on these tasks when users were provided with any of the
saliency maps, even the synthetic oracle explanation designed to be simple to
understand and highly indicative of the answer. Nonetheless, explanations did
help users more accurately describe the models. These findings suggest caution
regarding the usefulness and potential for misunderstanding in saliency-based
explanations.

摘要：

##### **Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**
2312.06798v1 by Manas Gaur, Amit Sheth

Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.

摘要：

##### **Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**
2311.17133v1 by Jacob R. Epifano, Stephen Glass, Ravi P. Ramachandran, Sharad Patel, Aaron J. Masino, Ghulam Rasool

This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.

摘要：

##### **Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**
2311.15719v1 by Benjamin Keel, Aaron Quyn, David Jayne, Samuel D. Relton

Lung cancer is responsible for 21% of cancer deaths in the UK and five-year
survival rates are heavily influenced by the stage the cancer was identified
at. Recent studies have demonstrated the capability of AI methods for accurate
and early diagnosis of lung cancer from routine scans. However, this evidence
has not translated into clinical practice with one barrier being a lack of
interpretable models. This study investigates the application Variational
Autoencoders (VAEs), a type of generative AI model, to lung cancer lesions.
Proposed models were trained on lesions extracted from 3D CT scans in the
LIDC-IDRI public dataset. Latent vector representations of 2D slices produced
by the VAEs were explored through clustering to justify their quality and used
in an MLP classifier model for lung cancer diagnosis, the best model achieved
state-of-the-art metrics of AUC 0.98 and 93.1% accuracy. Cluster analysis shows
the VAE latent space separates the dataset of malignant and benign lesions
based on meaningful feature components including tumour size, shape, patient
and malignancy class. We also include a comparative analysis of the standard
Gaussian VAE (GVAE) and the more recent Dirichlet VAE (DirVAE), which replaces
the prior with a Dirichlet distribution to encourage a more explainable latent
space with disentangled feature representation. Finally, we demonstrate the
potential for latent space traversals corresponding to clinically meaningful
feature changes.

摘要：

##### **MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**
2311.14471v1 by Nathan Blake, Hana Chockler, David A. Kelly, Santiago Calderon Pena, Akchunya Chanchal

Existing tools for explaining the output of image classifiers can be divided
into white-box, which rely on access to the model internals, and black-box,
agnostic to the model. As the usage of AI in the medical domain grows, so too
does the usage of explainability tools. Existing work on medical image
explanations focuses on white-box tools, such as gradcam. However, there are
clear advantages to switching to a black-box tool, including the ability to use
it with any classifier and the wide selection of black-box tools available. On
standard images, black-box tools are as precise as white-box. In this paper we
compare the performance of several black-box methods against gradcam on a brain
cancer MRI dataset. We demonstrate that most black-box tools are not suitable
for explaining medical image classifications and present a detailed analysis of
the reasons for their shortcomings. We also show that one black-box tool, a
causal explainability-based rex, performs as well as \gradcam.

摘要：

##### **Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**
2311.12573v2 by Robert Gorwa, Michael Veale

The AI development community is increasingly making use of hosting
intermediaries such as Hugging Face provide easy access to user-uploaded models
and training data. These model marketplaces lower technical deployment barriers
for hundreds of thousands of users, yet can be used in numerous potentially
harmful and illegal ways. In this article, we explain ways in which AI systems,
which can both `contain' content and be open-ended tools, present one of the
trickiest platform governance challenges seen to date. We provide case studies
of several incidents across three illustrative platforms -- Hugging Face,
GitHub and Civitai -- to examine how model marketplaces moderate models.
Building on this analysis, we outline important (and yet nevertheless limited)
practices that industry has been developing to respond to moderation demands:
licensing, access and use restrictions, automated content moderation, and open
policy development. While the policy challenge at hand is a considerable one,
we conclude with some ideas as to how platforms could better mobilize resources
to act as a careful, fair, and proportionate regulatory access point.

摘要：

##### **Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**
2311.11932v1 by Muta Tah Hira, Mohammad A. Razzaque, Mosharraf Sarker

Background and objectives: By extracting this information, Machine or Deep
Learning (ML/DL)-based autonomous data analysis tools can assist clinicians and
cancer researchers in discovering patterns and relationships from complex data
sets. Many DL-based analyses on ovarian cancer (OC) data have recently been
published. These analyses are highly diverse in various aspects of cancer
(e.g., subdomain(s) and cancer type they address) and data analysis features.
However, a comprehensive understanding of these analyses in terms of these
features and AI assurance (AIA) is currently lacking. This systematic review
aims to fill this gap by examining the existing literature and identifying
important aspects of OC data analysis using DL, explicitly focusing on the key
features and AI assurance perspectives. Methods: The PRISMA framework was used
to conduct comprehensive searches in three journal databases. Only studies
published between 2015 and 2023 in peer-reviewed journals were included in the
analysis. Results: In the review, a total of 96 DL-driven analyses were
examined. The findings reveal several important insights regarding DL-driven
ovarian cancer data analysis: - Most studies 71% (68 out of 96) focused on
detection and diagnosis, while no study addressed the prediction and prevention
of OC. - The analyses were predominantly based on samples from a non-diverse
population (75% (72/96 studies)), limited to a geographic location or country.
- Only a small proportion of studies (only 33% (32/96)) performed integrated
analyses, most of which used homogeneous data (clinical or omics). - Notably, a
mere 8.3% (8/96) of the studies validated their models using external and
diverse data sets, highlighting the need for enhanced model validation, and -
The inclusion of AIA in cancer data analysis is in a very early stage; only
2.1% (2/96) explicitly addressed AIA through explainability.

摘要：

##### **Representing visual classification as a linear combination of words**
2311.10933v1 by Shobhit Agarwal, Yevgeniy R. Semenov, William Lotter

Explainability is a longstanding challenge in deep learning, especially in
high-stakes domains like healthcare. Common explainability methods highlight
image regions that drive an AI model's decision. Humans, however, heavily rely
on language to convey explanations of not only "where" but "what".
Additionally, most explainability approaches focus on explaining individual AI
predictions, rather than describing the features used by an AI model in
general. The latter would be especially useful for model and dataset auditing,
and potentially even knowledge generation as AI is increasingly being used in
novel tasks. Here, we present an explainability strategy that uses a
vision-language model to identify language-based descriptors of a visual
classification task. By leveraging a pre-trained joint embedding space between
images and text, our approach estimates a new classification task as a linear
combination of words, resulting in a weight for each word that indicates its
alignment with the vision-based classifier. We assess our approach using two
medical imaging classification tasks, where we find that the resulting
descriptors largely align with clinical knowledge despite a lack of
domain-specific language training. However, our approach also identifies the
potential for 'shortcut connections' in the public datasets used. Towards a
functional measure of explainability, we perform a pilot reader study where we
find that the AI-identified words can enable non-expert humans to perform a
specialized medical task at a non-trivial level. Altogether, our results
emphasize the potential of using multimodal foundational models to deliver
intuitive, language-based explanations of visual tasks.

摘要：

##### **Towards objective and systematic evaluation of bias in medical imaging AI**
2311.02115v1 by Emma A. M. Stanley, Raissa Souza, Anthony Winder, Vedant Gulve, Kimberly Amador, Matthias Wilms, Nils D. Forkert

Artificial intelligence (AI) models trained using medical images for clinical
tasks often exhibit bias in the form of disparities in performance between
subgroups. Since not all sources of biases in real-world medical imaging data
are easily identifiable, it is challenging to comprehensively assess how those
biases are encoded in models, and how capable bias mitigation methods are at
ameliorating performance disparities. In this article, we introduce a novel
analysis framework for systematically and objectively investigating the impact
of biases in medical images on AI models. We developed and tested this
framework for conducting controlled in silico trials to assess bias in medical
imaging AI using a tool for generating synthetic magnetic resonance images with
known disease effects and sources of bias. The feasibility is showcased by
using three counterfactual bias scenarios to measure the impact of simulated
bias effects on a convolutional neural network (CNN) classifier and the
efficacy of three bias mitigation strategies. The analysis revealed that the
simulated biases resulted in expected subgroup performance disparities when the
CNN was trained on the synthetic datasets. Moreover, reweighing was identified
as the most successful bias mitigation strategy for this setup, and we
demonstrated how explainable AI methods can aid in investigating the
manifestation of bias in the model using this framework. Developing fair AI
models is a considerable challenge given that many and often unknown sources of
biases can be present in medical imaging datasets. In this work, we present a
novel methodology to objectively study the impact of biases and mitigation
strategies on deep learning pipelines, which can support the development of
clinical AI that is robust and responsible.

摘要：

##### **Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**
2310.19174v1 by Adam White, Margarita Saranti, Artur d'Avila Garcez, Thomas M. H. Hope, Cathy J. Price, Howard Bowman

Machine learning offers great potential for automated prediction of
post-stroke symptoms and their response to rehabilitation. Major challenges for
this endeavour include the very high dimensionality of neuroimaging data, the
relatively small size of the datasets available for learning, and how to
effectively combine neuroimaging and tabular data (e.g. demographic information
and clinical characteristics). This paper evaluates several solutions based on
two strategies. The first is to use 2D images that summarise MRI scans. The
second is to select key features that improve classification accuracy.
Additionally, we introduce the novel approach of training a convolutional
neural network (CNN) on images that combine regions-of-interest extracted from
MRIs, with symbolic representations of tabular data. We evaluate a series of
CNN architectures (both 2D and a 3D) that are trained on different
representations of MRI and tabular data, to predict whether a composite measure
of post-stroke spoken picture description ability is in the aphasic or
non-aphasic range. MRI and tabular data were acquired from 758 English speaking
stroke survivors who participated in the PLORAS study. The classification
accuracy for a baseline logistic regression was 0.678 for lesion size alone,
rising to 0.757 and 0.813 when initial symptom severity and recovery time were
successively added. The highest classification accuracy 0.854 was observed when
8 regions-of-interest was extracted from each MRI scan and combined with lesion
size, initial severity and recovery time in a 2D Residual Neural Network.Our
findings demonstrate how imaging and tabular data can be combined for high
post-stroke classification accuracy, even when the dataset is small in machine
learning terms. We conclude by proposing how the current models could be
improved to achieve even higher levels of accuracy using images from hospital
scanners.

摘要：

##### **Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**
2310.01828v2 by Hossein Shreim, Abdul Karim Gizzini, Ali J. Ghandour

eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.

摘要：

##### **Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**
2311.01463v1 by Muhammad Aurangzeb Ahmad, Ilker Yaramis, Taposh Dutta Roy

Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.

摘要：

##### **When to Trust AI: Advances and Challenges for Certification of Neural Networks**
2309.11196v1 by Marta Kwiatkowska, Xiyue Zhang

Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.

摘要：

##### **Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**
2309.10424v1 by Juan M. García-Gómez, Vicent Blanes-Selva, José Carlos de Bartolomé Cenzano, Jaime Cebolla-Cornejo, Ascensión Doñate-Martínez

The Directorate General for Parliamentary Research Services of the European
Parliament has prepared a report to the Members of the European Parliament
where they enumerate seven main risks of Artificial Intelligence (AI) in
medicine and healthcare: patient harm due to AI errors, misuse of medical AI
tools, bias in AI and the perpetuation of existing inequities, lack of
transparency, privacy and security issues, gaps in accountability, and
obstacles in implementation.
  In this study, we propose fourteen functional requirements that AI systems
may implement to reduce the risks associated with their medical purpose: AI
passport, User management, Regulation check, Academic use only disclaimer, data
quality assessment, Clinicians double check, Continuous performance evaluation,
Audit trail, Continuous usability test, Review of retrospective/simulated
cases, Bias check, eXplainable AI, Encryption and use of field-tested
libraries, and Semantic interoperability.
  Our intention here is to provide specific high-level specifications of
technical solutions to ensure continuous good performance and use of AI systems
to benefit patients in compliance with the future EU regulatory framework.

摘要：

##### **QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**
2309.10293v3 by Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Juan D. Velasquez, Niall Higgins

Artificial Intelligence techniques can be used to classify a patient's
physical activities and predict vital signs for remote patient monitoring.
Regression analysis based on non-linear models like deep learning models has
limited explainability due to its black-box nature. This can require
decision-makers to make blind leaps of faith based on non-linear model results,
especially in healthcare applications. In non-invasive monitoring, patient data
from tracking sensors and their predisposing clinical attributes act as input
features for predicting future vital signs. Explaining the contributions of
various features to the overall output of the monitoring application is
critical for a clinician's decision-making. In this study, an Explainable AI
for Quantitative analysis (QXAI) framework is proposed with post-hoc model
explainability and intrinsic explainability for regression and classification
tasks in a supervised learning approach. This was achieved by utilizing the
Shapley values concept and incorporating attention mechanisms in deep learning
models. We adopted the artificial neural networks (ANN) and attention-based
Bidirectional LSTM (BiLSTM) models for the prediction of heart rate and
classification of physical activities based on sensor data. The deep learning
models achieved state-of-the-art results in both prediction and classification
tasks. Global explanation and local explanation were conducted on input data to
understand the feature contribution of various patient data. The proposed QXAI
framework was evaluated using PPG-DaLiA data to predict heart rate and mobile
health (MHEALTH) data to classify physical activities based on sensor data.
Monte Carlo approximation was applied to the framework to overcome the time
complexity and high computation power requirements required for Shapley value
calculations.

摘要：

##### **Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**
2309.09917v1 by Adarsa Sivaprasad, Ehud Reiter, Nava Tintarev, Nir Oren

In explainable artificial intelligence (XAI) research, the predominant focus
has been on interpreting models for experts and practitioners. Model agnostic
and local explanation approaches are deemed interpretable and sufficient in
many applications. However, in domains like healthcare, where end users are
patients without AI or domain expertise, there is an urgent need for model
explanations that are more comprehensible and instil trust in the model's
operations. We hypothesise that generating model explanations that are
narrative, patient-specific and global(holistic of the model) would enable
better understandability and enable decision-making. We test this using a
decision tree model to generate both local and global explanations for patients
identified as having a high risk of coronary heart disease. These explanations
are presented to non-expert users. We find a strong individual preference for a
specific type of explanation. The majority of participants prefer global
explanations, while a smaller group prefers local explanations. A task based
evaluation of mental models of these participants provide valuable feedback to
enhance narrative global explanations. This, in turn, guides the design of
health informatics systems that are both trustworthy and actionable.

摘要：

##### **An explainable three dimension framework to uncover learning patterns: A unified look in variable sulci recognition**
2309.00903v2 by Michail Mamalakis, Heloise de Vareilles, Atheer AI-Manea, Samantha C. Mitchell, Ingrid Arartz, Lynn Egeland Morch-Johnsen, Jane Garrison, Jon Simons, Pietro Lio, John Suckling, Graham Murray

Explainable AI is crucial in medical imaging. In the challenging field of
neuroscience, visual topics present a high level of complexity, particularly
within three-dimensional space. The application of neuroscience, which involves
identifying brain sulcal features from MRI, faces significant hurdles due to
varying annotation protocols among experts and the intricate three-dimension
functionality of the brain. Consequently, traditional explainability approaches
fall short in effectively validating and evaluating these networks. To address
this, we first present a mathematical formulation delineating various
categories of explanation needs across diverse computer vision tasks,
categorized into self-explanatory, semi-explanatory, non-explanatory, and
new-pattern learning applications based on the reliability of the validation
protocol. With respect to this mathematical formulation, we propose a 3D
explainability framework aimed at validating the outputs of deep learning
networks in detecting the paracingulate sulcus an essential brain anatomical
feature. The framework integrates local 3D explanations, global explanations
through dimensionality reduction, concatenated global explanations, and
statistical shape features, unveiling new insights into pattern learning. We
trained and tested two advanced 3D deep learning networks on the challenging
TOP-OSLO dataset, significantly improving sulcus detection accuracy,
particularly on the left hemisphere. During evaluation with diverse annotation
protocols for this dataset, we highlighted the crucial role of an unbiased
annotation process in achieving precise predictions and effective pattern
learning within our proposed 3D framework. The proposed framework not only
annotates the variable sulcus but also uncovers hidden AI knowledge, promising
to advance our understanding of brain anatomy and function.

摘要：

##### **Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**
2308.14321v1 by Yanjun Gao, Ruizhe Li, John Caskey, Dmitriy Dligach, Timothy Miller, Matthew M. Churpek, Majid Afshar

Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.

摘要：

##### **Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**
2308.09380v1 by Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai

Existing artificial intelligence (AI) models for diagnosing knee
osteoarthritis (OA) have faced criticism for their lack of transparency and
interpretability, despite achieving medical-expert-like performance. This
opacity makes them challenging to trust in clinical practice. Recently,
explainable artificial intelligence (XAI) has emerged as a specialized
technique that can provide confidence in the model's prediction by revealing
how the prediction is derived, thus promoting the use of AI systems in
healthcare. This paper presents the first survey of XAI techniques used for
knee OA diagnosis. The XAI techniques are discussed from two perspectives: data
interpretability and model interpretability. The aim of this paper is to
provide valuable insights into XAI's potential towards a more reliable knee OA
diagnosis approach and encourage its adoption in clinical practice.

摘要：

##### **Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**
2308.08407v1 by Munib Mesinovic, Peter Watkinson, Tingting Zhu

Recent advancements in AI applications to healthcare have shown incredible
promise in surpassing human performance in diagnosis and disease prognosis.
With the increasing complexity of AI models, however, concerns regarding their
opacity, potential biases, and the need for interpretability. To ensure trust
and reliability in AI systems, especially in clinical risk prediction models,
explainability becomes crucial. Explainability is usually referred to as an AI
system's ability to provide a robust interpretation of its decision-making
logic or the decisions themselves to human stakeholders. In clinical risk
prediction, other aspects of explainability like fairness, bias, trust, and
transparency also represent important concepts beyond just interpretability. In
this review, we address the relationship between these concepts as they are
often used together or interchangeably. This review also discusses recent
progress in developing explainable models for clinical risk prediction,
highlighting the importance of quantitative and clinical evaluation and
validation across multiple common modalities in clinical practice. It
emphasizes the need for external validation and the combination of diverse
interpretability methods to enhance trust and fairness. Adopting rigorous
testing, such as using synthetic datasets with known generative factors, can
further improve the reliability of explainability methods. Open access and
code-sharing resources are essential for transparency and reproducibility,
enabling the growth and trustworthiness of explainable research. While
challenges exist, an end-to-end approach to explainability in clinical risk
prediction, incorporating stakeholders from clinicians to developers, is
essential for success.

摘要：

##### **FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**
2309.12325v1 by Karim Lekadir, Aasa Feragen, Abdul Joseph Fofanah, Alejandro F Frangi, Alena Buyx, Anais Emelie, Andrea Lara, Antonio R Porras, An-Wen Chan, Arcadi Navarro, Ben Glocker, Benard O Botwe, Bishesh Khanal, Brigit Beger, Carol C Wu, Celia Cintas, Curtis P Langlotz, Daniel Rueckert, Deogratias Mzurikwao, Dimitrios I Fotiadis, Doszhan Zhussupov, Enzo Ferrante, Erik Meijering, Eva Weicken, Fabio A González, Folkert W Asselbergs, Fred Prior, Gabriel P Krestin, Gary Collins, Geletaw S Tegenaw, Georgios Kaissis, Gianluca Misuraca, Gianna Tsakou, Girish Dwivedi, Haridimos Kondylakis, Harsha Jayakody, Henry C Woodruf, Hugo JWL Aerts, Ian Walsh, Ioanna Chouvarda, Irène Buvat, Islem Rekik, James Duncan, Jayashree Kalpathy-Cramer, Jihad Zahir, Jinah Park, John Mongan, Judy W Gichoya, Julia A Schnabel, Kaisar Kushibar, Katrine Riklund, Kensaku Mori, Kostas Marias, Lameck M Amugongo, Lauren A Fromont, Lena Maier-Hein, Leonor Cerdá Alberich, Leticia Rittner, Lighton Phiri, Linda Marrakchi-Kacem, Lluís Donoso-Bach, Luis Martí-Bonmatí, M Jorge Cardoso, Maciej Bobowicz, Mahsa Shabani, Manolis Tsiknakis, Maria A Zuluaga, Maria Bielikova, Marie-Christine Fritzsche, Marius George Linguraru, Markus Wenzel, Marleen De Bruijne, Martin G Tolsgaard, Marzyeh Ghassemi, Md Ashrafuzzaman, Melanie Goisauf, Mohammad Yaqub, Mohammed Ammar, Mónica Cano Abadía, Mukhtar M E Mahmoud, Mustafa Elattar, Nicola Rieke, Nikolaos Papanikolaou, Noussair Lazrak, Oliver Díaz, Olivier Salvado, Oriol Pujol, Ousmane Sall, Pamela Guevara, Peter Gordebeke, Philippe Lambin, Pieta Brown, Purang Abolmaesumi, Qi Dou, Qinghua Lu, Richard Osuala, Rose Nakasi, S Kevin Zhou, Sandy Napel, Sara Colantonio, Shadi Albarqouni, Smriti Joshi, Stacy Carter, Stefan Klein, Steffen E Petersen, Susanna Aussó, Suyash Awate, Tammy Riklin Raviv, Tessa Cook, Tinashe E M Mutsvangwa, Wendy A Rogers, Wiro J Niessen, Xènia Puig-Bosch, Yi Zeng, Yunusa G Mohammed, Yves Saint James Aquino, Zohaib Salahuddin, Martijn P A Starmans

Despite major advances in artificial intelligence (AI) for medicine and
healthcare, the deployment and adoption of AI technologies remain limited in
real-world clinical practice. In recent years, concerns have been raised about
the technical, clinical, ethical and legal risks associated with medical AI. To
increase real world adoption, it is essential that medical AI tools are trusted
and accepted by patients, clinicians, health organisations and authorities.
This work describes the FUTURE-AI guideline as the first international
consensus framework for guiding the development and deployment of trustworthy
AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and
currently comprises 118 inter-disciplinary experts from 51 countries
representing all continents, including AI scientists, clinicians, ethicists,
and social scientists. Over a two-year period, the consortium defined guiding
principles and best practices for trustworthy AI through an iterative process
comprising an in-depth literature review, a modified Delphi survey, and online
consensus meetings. The FUTURE-AI framework was established based on 6 guiding
principles for trustworthy AI in healthcare, i.e. Fairness, Universality,
Traceability, Usability, Robustness and Explainability. Through consensus, a
set of 28 best practices were defined, addressing technical, clinical, legal
and socio-ethical dimensions. The recommendations cover the entire lifecycle of
medical AI, from design, development and validation to regulation, deployment,
and monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which
provides a structured approach for constructing medical AI tools that will be
trusted, deployed and adopted in real-world practice. Researchers are
encouraged to take the recommendations into account in proof-of-concept stages
to facilitate future translation towards clinical practice of medical AI.

摘要：

##### **Explainable AI applications in the Medical Domain: a systematic review**
2308.05411v1 by Nicoletta Prentzas, Antonis Kakas, Constantinos S. Pattichis

Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.

摘要：

##### **Exploring the Role of Explainability in AI-Assisted Embryo Selection**
2308.02534v1 by Lucia Urcelay, Daniel Hinjos, Pablo A. Martin-Torres, Marta Gonzalez, Marta Mendez, Salva Cívico, Sergio Álvarez-Napagao, Dario Garcia-Gasulla

In Vitro Fertilization is among the most widespread treatments for
infertility. One of its main challenges is the evaluation and selection of
embryo for implantation, a process with large inter- and intra-clinician
variability. Deep learning based methods are gaining attention, but their
opaque nature compromises their acceptance in the clinical context, where
transparency in the decision making is key. In this paper we analyze the
current work in the explainability of AI-assisted embryo analysis models,
identifying the limitations. We also discuss how these models could be
integrated in the clinical context as decision support systems, considering the
needs of clinicians and patients. Finally, we propose guidelines for the sake
of increasing interpretability and trustworthiness, pushing this technology
forward towards established clinical practice.

摘要：

##### **A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**
2307.14246v1 by Timo Speith, Markus Langer

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

摘要：

##### **Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**
2307.14239v1 by Barnaby Crook, Maximilian Schlüter, Timo Speith

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

摘要：

##### **Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**
2308.02047v1 by Henry Fraser, Jose-Miguel Bello y Villarino

This paper critically evaluates the European Commission's proposed AI Act's
approach to risk management and risk acceptability for high-risk AI systems
that pose risks to fundamental rights and safety. The Act aims to promote
"trustworthy" AI with a proportionate regulatory burden. Its provisions on risk
acceptability require residual risks from high-risk systems to be reduced or
eliminated "as far as possible", having regard to the "state of the art". This
criterion, especially if interpreted narrowly, is unworkable and promotes
neither proportionate regulatory burden, nor trustworthiness. By contrast the
Parliament's most recent draft amendments to the risk management provisions
introduce "reasonableness", cost-benefit analysis, and are more transparent
about the value-laden and contextual nature of risk acceptability judgements.
This paper argues that the Parliament's approach is more workable, and better
balances the goals of proportionality and trustworthiness. It explains what
reasonableness in risk acceptability judgments would entail, drawing on
principles from negligence law and European medical devices regulation. And it
contends that the approach to risk acceptability judgments need a firm
foundation of civic legitimacy: including detailed guidance or involvement from
regulators, and meaningful input from affected stakeholders.

摘要：

##### **eXplainable Artificial Intelligence (XAI) in aging clock models**
2307.13704v3 by Alena Kalyakulina, Igor Yusipov, Alexey Moskalev, Claudio Franceschi, Mikhail Ivanchenko

eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the "aging clocks" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems.

摘要：

##### **Interpreting and Correcting Medical Image Classification with PIP-Net**
2307.10404v2 by Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, Jörg Schlötterer, Maurice van Keulen, Christin Seifert

Part-prototype models are explainable-by-design image classifiers, and a
promising alternative to black box AI. This paper explores the applicability
and potential of interpretable machine learning, in particular PIP-Net, for
automated diagnosis support on real-world medical imaging data. PIP-Net learns
human-understandable prototypical image parts and we evaluate its accuracy and
interpretability for fracture detection and skin cancer diagnosis. We find that
PIP-Net's decision making process is in line with medical classification
standards, while only provided with image-level class labels. Because of
PIP-Net's unsupervised pretraining of prototypes, data quality problems such as
undesired text in an X-ray or labelling errors can be easily identified.
Additionally, we are the first to show that humans can manually correct the
reasoning of PIP-Net by directly disabling undesired prototypes. We conclude
that part-prototype models are promising for medical applications due to their
interpretability and potential for advanced model debugging.

摘要：

##### **Explaining and visualizing black-box models through counterfactual paths**
2307.07764v3 by Bastian Pfeifer, Mateusz Krzyzinski, Hubert Baniecki, Anna Saranti, Andreas Holzinger, Przemyslaw Biecek

Explainable AI (XAI) is an increasingly important area of machine learning
research, which aims to make black-box models transparent and interpretable. In
this paper, we propose a novel approach to XAI that uses the so-called
counterfactual paths generated by conditional permutations of features. The
algorithm measures feature importance by identifying sequential permutations of
features that most influence changes in model predictions. It is particularly
suitable for generating explanations based on counterfactual paths in knowledge
graphs incorporating domain knowledge. Counterfactual paths introduce an
additional graph dimension to current XAI methods in both explaining and
visualizing black-box models. Experiments with synthetic and medical data
demonstrate the practical applicability of our approach.

摘要：

##### **Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**
2307.02131v5 by Toygar Tanyel, Serkan Ayvaz, Bilgin Keserci

The field of explainability in artificial intelligence (AI) has witnessed a
growing number of studies and increasing scholarly interest. However, the lack
of human-friendly and individual interpretations in explaining the outcomes of
machine learning algorithms has significantly hindered the acceptance of these
methods by clinicians in their research and clinical practice. To address this
issue, our study uses counterfactual explanations to explore the applicability
of "what if?" scenarios in medical research. Our aim is to expand our
understanding of magnetic resonance imaging (MRI) features used for diagnosing
pediatric posterior fossa brain tumors beyond existing boundaries. In our case
study, the proposed concept provides a novel way to examine alternative
decision-making scenarios that offer personalized and context-specific
insights, enabling the validation of predictions and clarification of
variations under diverse circumstances. Additionally, we explore the potential
use of counterfactuals for data augmentation and evaluate their feasibility as
an alternative approach in our medical research case. The results demonstrate
the promising potential of using counterfactual explanations to enhance
acceptance of AI-driven methods in clinical research.

摘要：

##### **AI and Non AI Assessments for Dementia**
2307.01210v1 by Mahboobeh Parsapoor, Hamed Ghodrati, Vincenzo Dentamaro, Christopher R. Madan, Ioulietta Lazarou, Spiros Nikolopoulos, Ioannis Kompatsiaris

Current progress in the artificial intelligence domain has led to the
development of various types of AI-powered dementia assessments, which can be
employed to identify patients at the early stage of dementia. It can
revolutionize the dementia care settings. It is essential that the medical
community be aware of various AI assessments and choose them considering their
degrees of validity, efficiency, practicality, reliability, and accuracy
concerning the early identification of patients with dementia (PwD). On the
other hand, AI developers should be informed about various non-AI assessments
as well as recently developed AI assessments. Thus, this paper, which can be
readable by both clinicians and AI engineers, fills the gap in the literature
in explaining the existing solutions for the recognition of dementia to
clinicians, as well as the techniques used and the most widespread dementia
datasets to AI engineers. It follows a review of papers on AI and non-AI
assessments for dementia to provide valuable information about various dementia
assessments for both the AI and medical communities. The discussion and
conclusion highlight the most prominent research directions and the maturity of
existing solutions.

摘要：

##### **Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**
2306.07306v1 by Ruitao Xie, Jingbang Chen, Limai Jiang, Rui Xiao, Yi Pan, Yunpeng Cai

Explainability poses a major challenge to artificial intelligence (AI)
techniques. Current studies on explainable AI (XAI) lack the efficiency of
extracting global knowledge about the learning task, thus suffer deficiencies
such as imprecise saliency, context-aware absence and vague meaning. In this
paper, we propose the class association embedding (CAE) approach to address
these issues. We employ an encoder-decoder architecture to embed sample
features and separate them into class-related and individual-related style
vectors simultaneously. Recombining the individual-style code of a given sample
with the class-style code of another leads to a synthetic sample with preserved
individual characters but changed class assignment, following a cyclic
adversarial learning strategy. Class association embedding distills the global
class-related features of all instances into a unified domain with well
separation between classes. The transition rules between different classes can
be then extracted and further employed to individual instances. We then propose
an active XAI framework which manipulates the class-style vector of a certain
sample along guided paths towards the counter-classes, resulting in a series of
counter-example synthetic samples with identical individual characters.
Comparing these counterfactual samples with the original ones provides a
global, intuitive illustration to the nature of the classification tasks. We
adopt the framework on medical image classification tasks, which show that more
precise saliency maps with powerful context-aware representation can be
achieved compared with existing methods. Moreover, the disease pathology can be
directly visualized via traversing the paths in the class-style space.

摘要：

##### **HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**
2306.06029v1 by Rodrigo Agerri, Iñigo Alonso, Aitziber Atutxa, Ander Berrondo, Ainara Estarrona, Iker Garcia-Ferrero, Iakes Goenaga, Koldo Gojenola, Maite Oronoz, Igor Perez-Tejedor, German Rigau, Anar Yeginbergenova

Providing high quality explanations for AI predictions based on machine
learning is a challenging and complex task. To work well it requires, among
other factors: selecting a proper level of generality/specificity of the
explanation; considering assumptions about the familiarity of the explanation
beneficiary with the AI task under consideration; referring to specific
elements that have contributed to the decision; making use of additional
knowledge (e.g. expert evidence) which might not be part of the prediction
process; and providing evidence supporting negative hypothesis. Finally, the
system needs to formulate the explanation in a clearly interpretable, and
possibly convincing, way. Given these considerations, ANTIDOTE fosters an
integrated vision of explainable AI, where low-level characteristics of the
deep learning process are combined with higher level schemes proper of the
human argumentation capacity. ANTIDOTE will exploit cross-disciplinary
competences in deep learning and argumentation to support a broader and
innovative view of explainable AI, where the need for high-quality explanations
for clinical cases deliberation is critical. As a first result of the project,
we publish the Antidote CasiMedicos dataset to facilitate research on
explainable AI in general, and argumentation in the medical domain in
particular.

摘要：

##### **XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**
2306.04791v1 by Eli Laird, Ayesh Madushanka, Elfi Kraka, Corey Clark

Progress in graph neural networks has grown rapidly in recent years, with
many new developments in drug discovery, medical diagnosis, and recommender
systems. While this progress is significant, many networks are `black boxes'
with little understanding of the `what' exactly the network is learning. Many
high-stakes applications, such as drug discovery, require human-intelligible
explanations from the models so that users can recognize errors and discover
new knowledge. Therefore, the development of explainable AI algorithms is
essential for us to reap the benefits of AI.
  We propose an explainability algorithm for GNNs called eXplainable Insight
(XInsight) that generates a distribution of model explanations using GFlowNets.
Since GFlowNets generate objects with probabilities proportional to a reward,
XInsight can generate a diverse set of explanations, compared to previous
methods that only learn the maximum reward sample. We demonstrate XInsight by
generating explanations for GNNs trained on two graph classification tasks:
classifying mutagenic compounds with the MUTAG dataset and classifying acyclic
graphs with a synthetic dataset that we have open-sourced. We show the utility
of XInsight's explanations by analyzing the generated compounds using QSAR
modeling, and we find that XInsight generates compounds that cluster by
lipophilicity, a known correlate of mutagenicity. Our results show that
XInsight generates a distribution of explanations that uncovers the underlying
relationships demonstrated by the model. They also highlight the importance of
generating a diverse set of explanations, as it enables us to discover hidden
relationships in the model and provides valuable guidance for further analysis.

摘要：

##### **Explainable AI using expressive Boolean formulas**
2306.03976v1 by Gili Rosenberg, J. Kyle Brubaker, Martin J. A. Schuetz, Grant Salton, Zhihuai Zhu, Elton Yechao Zhu, Serdar Kadıoğlu, Sima E. Borujeni, Helmut G. Katzgraber

We propose and implement an interpretable machine learning classification
model for Explainable AI (XAI) based on expressive Boolean formulas. Potential
applications include credit scoring and diagnosis of medical conditions. The
Boolean formula defines a rule with tunable complexity (or interpretability),
according to which input data are classified. Such a formula can include any
operator that can be applied to one or more Boolean variables, thus providing
higher expressivity compared to more rigid rule-based and tree-based
approaches. The classifier is trained using native local optimization
techniques, efficiently searching the space of feasible formulas. Shallow rules
can be determined by fast Integer Linear Programming (ILP) or Quadratic
Unconstrained Binary Optimization (QUBO) solvers, potentially powered by
special purpose hardware or quantum devices. We combine the expressivity and
efficiency of the native local optimizer with the fast operation of these
devices by executing non-local moves that optimize over subtrees of the full
Boolean formula. We provide extensive numerical benchmarking results featuring
several baselines on well-known public datasets. Based on the results, we find
that the native local rule classifier is generally competitive with the other
classifiers. The addition of non-local moves achieves similar results with
fewer iterations, and therefore using specialized or quantum hardware could
lead to a speedup by fast proposal of non-local moves.

摘要：

##### **Utterance Classification with Logical Neural Network: Explainable AI for Mental Disorder Diagnosis**
2306.03902v1 by Yeldar Toleubay, Don Joven Agravante, Daiki Kimura, Baihan Lin, Djallel Bouneffouf, Michiaki Tatsubori

In response to the global challenge of mental health problems, we proposes a
Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis
of mental disorders. Due to the lack of effective therapy coverage for mental
disorders, there is a need for an AI solution that can assist therapists with
the diagnosis. However, current Neural Network models lack explainability and
may not be trusted by therapists. The LNN is a Recurrent Neural Network
architecture that combines the learning capabilities of neural networks with
the reasoning capabilities of classical logic-based AI. The proposed system
uses input predicates from clinical interviews to output a mental disorder
class, and different predicate pruning techniques are used to achieve
scalability and higher scores. In addition, we provide an insight extraction
method to aid therapists with their diagnosis. The proposed system addresses
the lack of explainability of current Neural Network models and provides a more
trustworthy solution for mental disorder diagnosis.

摘要：

##### **XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models**
2306.01668v1 by Sujith K Mandala

As machine learning models become increasingly prevalent in medical
diagnostics, the need for interpretability and transparency becomes paramount.
The XAI Renaissance signifies a significant shift in the field, aiming to
redefine the interpretability of medical diagnostic models. This paper explores
the innovative approaches and methodologies within the realm of Explainable AI
(XAI) that are revolutionizing the interpretability of medical diagnostic
models. By shedding light on the underlying decision-making process, XAI
techniques empower healthcare professionals to understand, trust, and
effectively utilize these models for accurate and reliable medical diagnoses.
This review highlights the key advancements in XAI for medical diagnostics and
their potential to transform the healthcare landscape, ultimately improving
patient outcomes and fostering trust in AI-driven diagnostic systems.

摘要：

##### **A Novel real-time arrhythmia detection model using YOLOv8**
2305.16727v3 by Guang Jun Nicholas Ang, Aritejh Kr Goil, Henryk Chan, Jieyi Jeric Lew, Xin Chun Lee, Raihan Bin Ahmad Mustaffa, Timotius Jason, Ze Ting Woon, Bingquan Shen

In a landscape characterized by heightened connectivity and mobility, coupled
with a surge in cardiovascular ailments, the imperative to curtail healthcare
expenses through remote monitoring of cardiovascular health has become more
pronounced. The accurate detection and classification of cardiac arrhythmias
are pivotal for diagnosing individuals with heart irregularities. This study
underscores the feasibility of employing electrocardiograms (ECG) measurements
in the home environment for real-time arrhythmia detection. Presenting a fresh
application for arrhythmia detection, this paper leverages the cutting-edge
You-Only-Look-Once (YOLO)v8 algorithm to categorize single-lead ECG signals. We
introduce a novel loss-modified YOLOv8 model, fine-tuned on the MIT-BIH
arrhythmia dataset, enabling real-time continuous monitoring. The obtained
results substantiate the efficacy of our approach, with the model attaining an
average accuracy of 99.5% and 0.992 mAP@50, and a rapid detection time of 0.002
seconds on an NVIDIA Tesla V100. Our investigation exemplifies the potential of
real-time arrhythmia detection, enabling users to visually interpret the model
output within the comfort of their homes. Furthermore, this study lays the
groundwork for an extension into a real-time explainable AI (XAI) model capable
of deployment in the healthcare sector, thereby significantly advancing the
realm of healthcare solutions.

摘要：

##### **Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI**
2305.14389v2 by Jai Vardhan, Taraka Satya Krishna Teja Malisetti

Breast cancer (BC) remains a significant health threat, with no long-term
cure currently available. Early detection is crucial, yet mammography
interpretation is hindered by high false positives and negatives. With BC
incidence projected to surpass lung cancer, improving early detection methods
is vital. Thermography, using high-resolution infrared cameras, offers promise,
especially when combined with artificial intelligence (AI). This work presents
an attention-based convolutional neural network for segmentation, providing
increased speed and precision in BC detection and classification. The system
enhances images and performs cancer segmentation with explainable AI. We
propose a transformer-attention-based convolutional architecture (UNet) for
fault identification and employ Gradient-weighted Class Activation Mapping
(Grad-CAM) to analyze areas of bias and weakness in the UNet architecture with
IRT images. The superiority of our proposed framework is confirmed when
compared with existing deep learning frameworks.

摘要：

##### **What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media**
2305.13127v2 by Junwei Kuang, Jiaheng Xie, Zhijun Yan

Depression is the most prevalent and serious mental illness, which induces
grave financial and societal ramifications. Depression detection is key for
early intervention to mitigate those consequences. Such a high-stake decision
inherently necessitates interpretability. Although a few depression detection
studies attempt to explain the decision based on the importance score or
attention weights, these explanations misalign with the clinical depression
diagnosis criterion that is based on depressive symptoms. To fill this gap, we
follow the computational design science paradigm to develop a novel Multi-Scale
Temporal Prototype Network (MSTPNet). MSTPNet innovatively detects and
interprets depressive symptoms as well as how long they last. Extensive
empirical analyses using a large-scale dataset show that MSTPNet outperforms
state-of-the-art depression detection methods with an F1-score of 0.851. This
result also reveals new symptoms that are unnoted in the survey approach, such
as sharing admiration for a different life. We further conduct a user study to
demonstrate its superiority over the benchmarks in interpretability. This study
contributes to IS literature with a novel interpretable deep learning model for
depression detection in social media. In practice, our proposed method can be
implemented in social media platforms to provide personalized online resources
for detected depressed patients.

摘要：

##### **Echoes of Biases: How Stigmatizing Language Affects AI Performance**
2305.10201v4 by Yizhi Liu, Weiguang Wang, Guodong Gordon Gao, Ritu Agarwal

Electronic health records (EHRs) serve as an essential data source for the
envisioned artificial intelligence (AI)-driven transformation in healthcare.
However, clinician biases reflected in EHR notes can lead to AI models
inheriting and amplifying these biases, perpetuating health disparities. This
study investigates the impact of stigmatizing language (SL) in EHR notes on
mortality prediction using a Transformer-based deep learning model and
explainable AI (XAI) techniques. Our findings demonstrate that SL written by
clinicians adversely affects AI performance, particularly so for black
patients, highlighting SL as a source of racial disparity in AI model
development. To explore an operationally efficient way to mitigate SL's impact,
we investigate patterns in the generation of SL through a clinicians'
collaborative network, identifying central clinicians as having a stronger
impact on racial disparity in the AI model. We find that removing SL written by
central clinicians is a more efficient bias reduction strategy than eliminating
all SL in the entire corpus of data. This study provides actionable insights
for responsible AI development and contributes to understanding clinician
behavior and EHR note writing in healthcare.

摘要：

##### **Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour**
2305.03376v1 by Goda Klumbyte, Hannah Piehl, Claude Draude

Contemporary automation through AI entails a substantial amount of
behind-the-scenes human labour, which is often both invisibilised and
underpaid. Since invisible labour, including labelling and maintenance work, is
an integral part of contemporary AI systems, it remains important to sensitise
users to its role. We suggest that this could be done through explainable AI
(XAI) design, particularly feminist intersectional XAI. We propose the method
of cartography, which stems from feminist intersectional research, to draw out
a systemic perspective of AI and include dimensions of AI that pertain to
invisible labour.

摘要：

##### **Towards Explainable and Safe Conversational Agents for Mental Health: A Survey**
2304.13191v1 by Surjodeep Sarkar, Manas Gaur, L. Chen, Muskan Garg, Biplav Srivastava, Bhaktee Dongaonkar

Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to
support the overburdened global healthcare system that gets 60 million primary
care visits, and 6 million Emergency Room (ER) visits annually. These systems
are built by clinical psychologists, psychiatrists, and Artificial Intelligence
(AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role
of VMHAs is to provide emotional support through information, focusing less on
developing a reflective conversation with the patient. A more comprehensive,
safe and explainable approach is required to build responsible VMHAs to ask
follow-up questions or provide a well-informed response. This survey offers a
systematic critical review of the existing conversational agents in mental
health, followed by new insights into the improvements of VMHAs with contextual
knowledge, datasets, and their emerging role in clinical decision support. We
also provide new directions toward enriching the user experience of VMHAs with
explainability, safety, and wholesome trustworthiness. Finally, we provide
evaluation metrics and practical considerations for VMHAs beyond the current
literature to build trust between VMHAs and patients in active communications.

摘要：

##### **A Brief Review of Explainable Artificial Intelligence in Healthcare**
2304.01543v1 by Zahra Sadeghi, Roohallah Alizadehsani, Mehmet Akif Cifci, Samina Kausar, Rizwan Rehman, Priyakshi Mahanta, Pranjal Kumar Bora, Ammar Almasri, Rami S. Alkhawaldeh, Sadiq Hussain, Bilal Alatas, Afshin Shoeibi, Hossein Moosaei, Milan Hladik, Saeid Nahavandi, Panos M. Pardalos

XAI refers to the techniques and methods for building AI applications which
assist end users to interpret output and predictions of AI models. Black box AI
applications in high-stakes decision-making situations, such as medical domain
have increased the demand for transparency and explainability since wrong
predictions may have severe consequences. Model explainability and
interpretability are vital successful deployment of AI models in healthcare
practices. AI applications' underlying reasoning needs to be transparent to
clinicians in order to gain their trust. This paper presents a systematic
review of XAI aspects and challenges in the healthcare domain. The primary
goals of this study are to review various XAI methods, their challenges, and
related machine learning models in healthcare. The methods are discussed under
six categories: Features-oriented methods, global methods, concept models,
surrogate models, local pixel-based methods, and human-centric methods. Most
importantly, the paper explores XAI role in healthcare problems to clarify its
necessity in safety-critical applications. The paper intends to establish a
comprehensive understanding of XAI-related applications in the healthcare field
by reviewing the related experimental results. To facilitate future research
for filling research gaps, the importance of XAI models from different
viewpoints and their limitations are investigated.

摘要：

##### **Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models**
2303.12641v2 by Frederik Pahde, Maximilian Dreyer, Wojciech Samek, Sebastian Lapuschkin

State-of-the-art machine learning models often learn spurious correlations
embedded in the training data. This poses risks when deploying these models for
high-stake decision-making, such as in medical applications like skin cancer
detection. To tackle this problem, we propose Reveal to Revise (R2R), a
framework entailing the entire eXplainable Artificial Intelligence (XAI) life
cycle, enabling practitioners to iteratively identify, mitigate, and
(re-)evaluate spurious model behavior with a minimal amount of human
interaction. In the first step (1), R2R reveals model weaknesses by finding
outliers in attributions or through inspection of latent concepts learned by
the model. Secondly (2), the responsible artifacts are detected and spatially
localized in the input data, which is then leveraged to (3) revise the model
behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model
correction, and (4) (re-)evaluate the model's performance and remaining
sensitivity towards the artifact. Using two medical benchmark datasets for
Melanoma detection and bone age estimation, we apply our R2R framework to VGG,
ResNet and EfficientNet architectures and thereby reveal and correct real
dataset-intrinsic artifacts, as well as synthetic variants in a controlled
setting. Completing the XAI life cycle, we demonstrate multiple R2R iterations
to mitigate different biases. Code is available on
https://github.com/maxdreyer/Reveal2Revise.

摘要：

##### **Explainable AI for Time Series via Virtual Inspection Layers**
2303.06365v1 by Johanna Vielhaben, Sebastian Lapuschkin, Grégoire Montavon, Wojciech Samek

The field of eXplainable Artificial Intelligence (XAI) has greatly advanced
in recent years, but progress has mainly been made in computer vision and
natural language processing. For time series, where the input is often not
interpretable, only limited research on XAI is available. In this work, we put
forward a virtual inspection layer, that transforms the time series to an
interpretable representation and allows to propagate relevance attributions to
this representation via local XAI methods like layer-wise relevance propagation
(LRP). In this way, we extend the applicability of a family of XAI methods to
domains (e.g. speech) where the input is only interpretable after a
transformation. Here, we focus on the Fourier transformation which is
prominently applied in the interpretation of time series and LRP and refer to
our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time
series classification settings like audio and electronic health records. We
showcase how DFT-LRP reveals differences in the classification strategies of
models trained in different domains (e.g., time vs. frequency domain) or helps
to discover how models act on spurious correlations in the data.

摘要：

##### **Towards Trust of Explainable AI in Thyroid Nodule Diagnosis**
2303.04731v1 by Truong Thanh Hung Nguyen, Van Binh Truong, Vo Thanh Khang Nguyen, Quoc Hung Cao, Quoc Khanh Nguyen

The ability to explain the prediction of deep learning models to end-users is
an important feature to leverage the power of artificial intelligence (AI) for
the medical decision-making process, which is usually considered
non-transparent and challenging to comprehend. In this paper, we apply
state-of-the-art eXplainable artificial intelligence (XAI) methods to explain
the prediction of the black-box AI models in the thyroid nodule diagnosis
application. We propose new statistic-based XAI methods, namely Kernel Density
Estimation and Density map, to explain the case of no nodule detected. XAI
methods' performances are considered under a qualitative and quantitative
comparison as feedback to improve the data quality and the model performance.
Finally, we survey to assess doctors' and patients' trust in XAI explanations
of the model's decisions on thyroid nodule images.

摘要：

##### **Cybersecurity of AI medical devices: risks, legislation, and challenges**
2303.03140v1 by Elisabetta Biasin, Erik Kamenjasevic, Kaspar Rosager Ludvigsen

Medical devices and artificial intelligence systems rapidly transform
healthcare provisions. At the same time, due to their nature, AI in or as
medical devices might get exposed to cyberattacks, leading to patient safety
and security risks. This book chapter is divided into three parts. The first
part starts by setting the scene where we explain the role of cybersecurity in
healthcare. Then, we briefly define what we refer to when we talk about AI that
is considered a medical device by itself or supports one. To illustrate the
risks such medical devices pose, we provide three examples: the poisoning of
datasets, social engineering, and data or source code extraction. In the second
part, the paper provides an overview of the European Union's regulatory
framework relevant for ensuring the cybersecurity of AI as or in medical
devices (MDR, NIS Directive, Cybersecurity Act, GDPR, the AI Act proposal and
the NIS 2 Directive proposal). Finally, the third part of the paper examines
possible challenges stemming from the EU regulatory framework. In particular,
we look toward the challenges deriving from the two legislative proposals and
their interaction with the existing legislation concerning AI medical devices'
cybersecurity. They are structured as answers to the following questions: (1)
how will the AI Act interact with the MDR regarding the cybersecurity and
safety requirements?; (2) how should we interpret incident notification
requirements from the NIS 2 Directive proposal and MDR?; and (3) what are the
consequences of the evolving term of critical infrastructures?
  [This is a draft chapter. The final version will be available in Research
Handbook on Health, AI and the Law edited by Barry Solaiman & I. Glenn Cohen,
forthcoming 2023, Edward Elgar Publishing Ltd]

摘要：

##### **LAVA: Granular Neuron-Level Explainable AI for Alzheimer's Disease Assessment from Fundus Images**
2302.03008v2 by Nooshin Yousefzadeh, Charlie Tran, Adolfo Ramirez-Zamora, Jinghua Chen, Ruogu Fang, My T. Thai

Alzheimer's Disease (AD) is a progressive neurodegenerative disease and the
leading cause of dementia. Early diagnosis is critical for patients to benefit
from potential intervention and treatment. The retina has been hypothesized as
a diagnostic site for AD detection owing to its anatomical connection with the
brain. Developed AI models for this purpose have yet to provide a rational
explanation about the decision and neither infer the stage of disease's
progression. Along this direction, we propose a novel model-agnostic
explainable-AI framework, called Granular Neuron-level Explainer (LAVA), an
interpretation prototype that probes into intermediate layers of the
Convolutional Neural Network (CNN) models to assess the AD continuum directly
from the retinal imaging without longitudinal or clinical evaluation. This
method is applied to validate the retinal vasculature as a biomarker and
diagnostic modality for Alzheimer's Disease (AD) evaluation. UK Biobank
cognitive tests and vascular morphological features suggest LAVA shows strong
promise and effectiveness in identifying AD stages across the progression
continuum.

摘要：

##### **Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses**
2302.01241v2 by Brian Y. Lim, Joseph P. Cahaly, Chester Y. F. Sng, Adam Chew

Many visualizations have been developed for explainable AI (XAI), but they
often require further reasoning by users to interpret. We argue that XAI should
support diagrammatic and abductive reasoning for the AI to perform hypothesis
generation and evaluation to reduce the interpretability gap. We propose
Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii)
follow domain conventions, and iii) explain with diagrams visually or verbally.
We implemented DiagramNet for a clinical application to predict cardiac
diagnoses from heart auscultation, and explain with shape-based murmur
diagrams. In modeling studies, we found that DiagramNet not only provides
faithful murmur shape explanations, but also has better prediction performance
than baseline models. We further demonstrate the interpretability and
trustworthiness of diagrammatic explanations in a qualitative user study with
medical students, showing that clinically-relevant, diagrammatic explanations
are preferred over technical saliency map explanations. This work contributes
insights into providing domain-conventional abductive explanations for
user-centric XAI.

摘要：

##### **LesionAid: Vision Transformers-based Skin Lesion Generation and Classification**
2302.01104v1 by Ghanta Sai Krishna, Kundrapu Supriya, Mallikharjuna Rao K, Meetiksha Sorgile

Skin cancer is one of the most prevalent forms of human cancer. It is
recognized mainly visually, beginning with clinical screening and continuing
with the dermoscopic examination, histological assessment, and specimen
collection. Deep convolutional neural networks (CNNs) perform highly segregated
and potentially universal tasks against a classified finegrained object. This
research proposes a novel multi-class prediction framework that classifies skin
lesions based on ViT and ViTGAN. Vision transformers-based GANs (Generative
Adversarial Networks) are utilized to tackle the class imbalance. The framework
consists of four main phases: ViTGANs, Image processing, and explainable AI.
Phase 1 consists of generating synthetic images to balance all the classes in
the dataset. Phase 2 consists of applying different data augmentation
techniques and morphological operations to increase the size of the data.
Phases 3 & 4 involve developing a ViT model for edge computing systems that can
identify patterns and categorize skin lesions from the user's skin visible in
the image. In phase 3, after classifying the lesions into the desired class
with ViT, we will use explainable AI (XAI) that leads to more explainable
results (using activation maps, etc.) while ensuring high predictive accuracy.
Real-time images of skin diseases can capture by a doctor or a patient using
the camera of a mobile application to perform an early examination and
determine the cause of the skin lesion. The whole framework is compared with
the existing frameworks for skin lesion detection.

摘要：

##### **SkinCon: A skin disease dataset densely annotated by domain experts for fine-grained model debugging and analysis**
2302.00785v1 by Roxana Daneshjou, Mert Yuksekgonul, Zhuo Ran Cai, Roberto Novoa, James Zou

For the deployment of artificial intelligence (AI) in high-risk settings,
such as healthcare, methods that provide interpretability/explainability or
allow fine-grained error analysis are critical. Many recent methods for
interpretability/explainability and fine-grained error analysis use concepts,
which are meta-labels that are semantically meaningful to humans. However,
there are only a few datasets that include concept-level meta-labels and most
of these meta-labels are relevant for natural images that do not require domain
expertise. Densely annotated datasets in medicine focused on meta-labels that
are relevant to a single disease such as melanoma. In dermatology, skin disease
is described using an established clinical lexicon that allows clinicians to
describe physical exam findings to one another. To provide a medical dataset
densely annotated by domain experts with annotations useful across multiple
disease processes, we developed SkinCon: a skin disease dataset densely
annotated by dermatologists. SkinCon includes 3230 images from the Fitzpatrick
17k dataset densely annotated with 48 clinical concepts, 22 of which have at
least 50 images representing the concept. The concepts used were chosen by two
dermatologists considering the clinical descriptor terms used to describe skin
lesions. Examples include "plaque", "scale", and "erosion". The same concepts
were also used to label 656 skin disease images from the Diverse Dermatology
Images dataset, providing an additional external dataset with diverse skin tone
representations. We review the potential applications for the SkinCon dataset,
such as probing models, concept-based explanations, and concept bottlenecks.
Furthermore, we use SkinCon to demonstrate two of these use cases: debugging
mistakes of an existing dermatology AI model with concepts and developing
interpretable models with post-hoc concept bottleneck models.

摘要：

##### **Decision-Focused Evaluation: Analyzing Performance of Deployed Restless Multi-Arm Bandits**
2301.07835v1 by Paritosh Verma, Shresth Verma, Aditya Mate, Aparna Taneja, Milind Tambe

Restless multi-arm bandits (RMABs) is a popular decision-theoretic framework
that has been used to model real-world sequential decision making problems in
public health, wildlife conservation, communication systems, and beyond.
Deployed RMAB systems typically operate in two stages: the first predicts the
unknown parameters defining the RMAB instance, and the second employs an
optimization algorithm to solve the constructed RMAB instance.
  In this work we provide and analyze the results from a first-of-its-kind
deployment of an RMAB system in public health domain, aimed at improving
maternal and child health. Our analysis is focused towards understanding the
relationship between prediction accuracy and overall performance of deployed
RMAB systems. This is crucial for determining the value of investing in
improving predictive accuracy towards improving the final system performance,
and is useful for diagnosing, monitoring deployed RMAB systems.
  Using real-world data from our deployed RMAB system, we demonstrate that an
improvement in overall prediction accuracy may even be accompanied by a
degradation in the performance of RMAB system -- a broad investment of
resources to improve overall prediction accuracy may not yield expected
results. Following this, we develop decision-focused evaluation metrics to
evaluate the predictive component and show that it is better at explaining
(both empirically and theoretically) the overall performance of a deployed RMAB
system.

摘要：

##### **Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling**
2302.03033v1 by Carlo Metta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, Salvatore Rinzivillo

Explainable AI consists in developing mechanisms allowing for an interaction
between decision systems and humans by making the decisions of the formers
understandable. This is particularly important in sensitive contexts like in
the medical domain. We propose a use case study, for skin lesion diagnosis,
illustrating how it is possible to provide the practitioner with explanations
on the decisions of a state of the art deep neural network classifier trained
to characterize skin lesions from examples. Our framework consists of a trained
classifier onto which an explanation module operates. The latter is able to
offer the practitioner exemplars and counterexemplars for the classification
diagnosis thus allowing the physician to interact with the automatic diagnosis
system. The exemplars are generated via an adversarial autoencoder. We
illustrate the behavior of the system on representative examples.

摘要：

##### **Monotonicity for AI ethics and society: An empirical study of the monotonic neural additive model in criminology, education, health care, and finance**
2301.07060v1 by Dangxing Chen, Luyao Zhang

Algorithm fairness in the application of artificial intelligence (AI) is
essential for a better society. As the foundational axiom of social mechanisms,
fairness consists of multiple facets. Although the machine learning (ML)
community has focused on intersectionality as a matter of statistical parity,
especially in discrimination issues, an emerging body of literature addresses
another facet -- monotonicity. Based on domain expertise, monotonicity plays a
vital role in numerous fairness-related areas, where violations could misguide
human decisions and lead to disastrous consequences. In this paper, we first
systematically evaluate the significance of applying monotonic neural additive
models (MNAMs), which use a fairness-aware ML algorithm to enforce both
individual and pairwise monotonicity principles, for the fairness of AI ethics
and society. We have found, through a hybrid method of theoretical reasoning,
simulation, and extensive empirical analysis, that considering monotonicity
axioms is essential in all areas of fairness, including criminology, education,
health care, and finance. Our research contributes to the interdisciplinary
research at the interface of AI ethics, explainable AI (XAI), and
human-computer interactions (HCIs). By evidencing the catastrophic consequences
if monotonicity is not met, we address the significance of monotonicity
requirements in AI applications. Furthermore, we demonstrate that MNAMs are an
effective fairness-aware ML approach by imposing monotonicity restrictions
integrating human intelligence.

摘要：

##### **Rationalizing Predictions by Adversarial Information Calibration**
2301.06009v1 by Lei Sha, Oana-Maria Camburu, Thomas Lukasiewicz

Explaining the predictions of AI models is paramount in safety-critical
applications, such as in legal or medical domains. One form of explanation for
a prediction is an extractive rationale, i.e., a subset of features of an
instance that lead the model to give its prediction on that instance. For
example, the subphrase ``he stole the mobile phone'' can be an extractive
rationale for the prediction of ``Theft''. Previous works on generating
extractive rationales usually employ a two-phase model: a selector that selects
the most important features (i.e., the rationale) followed by a predictor that
makes the prediction based exclusively on the selected features. One
disadvantage of these works is that the main signal for learning to select
features comes from the comparison of the answers given by the predictor to the
ground-truth answers. In this work, we propose to squeeze more information from
the predictor via an information calibration method. More precisely, we train
two models jointly: one is a typical neural model that solves the task at hand
in an accurate but black-box manner, and the other is a selector-predictor
model that additionally produces a rationale for its prediction. The first
model is used as a guide for the second model. We use an adversarial technique
to calibrate the information extracted by the two models such that the
difference between them is an indicator of the missed or over-selected
features. In addition, for natural language tasks, we propose a
language-model-based regularizer to encourage the extraction of fluent
rationales. Experimental results on a sentiment analysis task, a hate speech
recognition task as well as on three tasks from the legal domain show the
effectiveness of our approach to rationale extraction.

摘要：

##### **Semantic match: Debugging feature attribution methods in XAI for healthcare**
2301.02080v3 by Giovanni Cinà, Tabea E. Röber, Rob Goedhart, Ş. İlker Birbil

The recent spike in certified Artificial Intelligence (AI) tools for
healthcare has renewed the debate around adoption of this technology. One
thread of such debate concerns Explainable AI (XAI) and its promise to render
AI devices more transparent and trustworthy. A few voices active in the medical
AI space have expressed concerns on the reliability of Explainable AI
techniques and especially feature attribution methods, questioning their use
and inclusion in guidelines and standards. Despite valid concerns, we argue
that existing criticism on the viability of post-hoc local explainability
methods throws away the baby with the bathwater by generalizing a problem that
is specific to image data. We begin by characterizing the problem as a lack of
semantic match between explanations and human understanding. To understand when
feature importance can be used reliably, we introduce a distinction between
feature importance of low- and high-level features. We argue that for data
types where low-level features come endowed with a clear semantics, such as
tabular data like Electronic Health Records (EHRs), semantic match can be
obtained, and thus feature attribution methods can still be employed in a
meaningful and useful way. Finally, we sketch a procedure to test whether
semantic match has been achieved.

摘要：

##### **Context-dependent Explainability and Contestability for Trustworthy Medical Artificial Intelligence: Misclassification Identification of Morbidity Recognition Models in Preterm Infants**
2212.08821v1 by Isil Guzey, Ozlem Ucar, Nukhet Aladag Ciftdemir, Betul Acunas

Although machine learning (ML) models of AI achieve high performances in
medicine, they are not free of errors. Empowering clinicians to identify
incorrect model recommendations is crucial for engendering trust in medical AI.
Explainable AI (XAI) aims to address this requirement by clarifying AI
reasoning to support the end users. Several studies on biomedical imaging
achieved promising results recently. Nevertheless, solutions for models using
tabular data are not sufficient to meet the requirements of clinicians yet.
This paper proposes a methodology to support clinicians in identifying failures
of ML models trained with tabular data. We built our methodology on three main
pillars: decomposing the feature set by leveraging clinical context latent
space, assessing the clinical association of global explanations, and Latent
Space Similarity (LSS) based local explanations. We demonstrated our
methodology on ML-based recognition of preterm infant morbidities caused by
infection. The risk of mortality, lifelong disability, and antibiotic
resistance due to model failures was an open research question in this domain.
We achieved to identify misclassification cases of two models with our
approach. By contextualizing local explanations, our solution provides
clinicians with actionable insights to support their autonomy for informed
final decisions.

摘要：

##### **It is not "accuracy vs. explainability" -- we need both for trustworthy AI systems**
2212.11136v2 by D. Petkovic

We are witnessing the emergence of an AI economy and society where AI
technologies are increasingly impacting health care, business, transportation
and many aspects of everyday life. Many successes have been reported where AI
systems even surpassed the accuracy of human experts. However, AI systems may
produce errors, can exhibit bias, may be sensitive to noise in the data, and
often lack technical and judicial transparency resulting in reduction in trust
and challenges in their adoption. These recent shortcomings and concerns have
been documented in scientific but also in general press such as accidents with
self driving cars, biases in healthcare, hiring and face recognition systems
for people of color, seemingly correct medical decisions later found to be made
due to wrong reasons etc. This resulted in emergence of many government and
regulatory initiatives requiring trustworthy and ethical AI to provide accuracy
and robustness, some form of explainability, human control and oversight,
elimination of bias, judicial transparency and safety. The challenges in
delivery of trustworthy AI systems motivated intense research on explainable AI
systems (XAI). Aim of XAI is to provide human understandable information of how
AI systems make their decisions. In this paper we first briefly summarize
current XAI work and then challenge the recent arguments of accuracy vs.
explainability for being mutually exclusive and being focused only on deep
learning. We then present our recommendations for the use of XAI in full
lifecycle of high stakes trustworthy AI systems delivery, e.g. development,
validation and certification, and trustworthy production and maintenance.

摘要：

##### **SimpleMind adds thinking to deep neural networks**
2212.00951v1 by Youngwon Choi, M. Wasil Wahi-Anwar, Matthew S. Brown

Deep neural networks (DNNs) detect patterns in data and have shown
versatility and strong performance in many computer vision applications.
However, DNNs alone are susceptible to obvious mistakes that violate simple,
common sense concepts and are limited in their ability to use explicit
knowledge to guide their search and decision making. While overall DNN
performance metrics may be good, these obvious errors, coupled with a lack of
explainability, have prevented widespread adoption for crucial tasks such as
medical image analysis. The purpose of this paper is to introduce SimpleMind,
an open-source software framework for Cognitive AI focused on medical image
understanding. It allows creation of a knowledge base that describes expected
characteristics and relationships between image objects in an intuitive
human-readable form. The SimpleMind framework brings thinking to DNNs by: (1)
providing methods for reasoning with the knowledge base about image content,
such as spatial inferencing and conditional reasoning to check DNN outputs; (2)
applying process knowledge, in the form of general-purpose software agents,
that are chained together to accomplish image preprocessing, DNN prediction,
and result post-processing, and (3) performing automatic co-optimization of all
knowledge base parameters to adapt agents to specific problems. SimpleMind
enables reasoning on multiple detected objects to ensure consistency, providing
cross checking between DNN outputs. This machine reasoning improves the
reliability and trustworthiness of DNNs through an interpretable model and
explainable decisions. Example applications are provided that demonstrate how
SimpleMind supports and improves deep neural networks by embedding them within
a Cognitive AI framework.

摘要：

##### **Attribution-based XAI Methods in Computer Vision: A Review**
2211.14736v1 by Kumar Abhishek, Deeksha Kamath

The advancements in deep learning-based methods for visual perception tasks
have seen astounding growth in the last decade, with widespread adoption in a
plethora of application areas from autonomous driving to clinical decision
support systems. Despite their impressive performance, these deep
learning-based models remain fairly opaque in their decision-making process,
making their deployment in human-critical tasks a risky endeavor. This in turn
makes understanding the decisions made by these models crucial for their
reliable deployment. Explainable AI (XAI) methods attempt to address this by
offering explanations for such black-box deep learning methods. In this paper,
we provide a comprehensive survey of attribution-based XAI methods in computer
vision and review the existing literature for gradient-based,
perturbation-based, and contrastive methods for XAI, and provide insights on
the key challenges in developing and evaluating robust XAI methods.

摘要：

##### **Privacy Meets Explainability: A Comprehensive Impact Benchmark**
2211.04110v1 by Saifullah Saifullah, Dominique Mercier, Adriano Lucieri, Andreas Dengel, Sheraz Ahmed

Since the mid-10s, the era of Deep Learning (DL) has continued to this day,
bringing forth new superlatives and innovations each year. Nevertheless, the
speed with which these innovations translate into real applications lags behind
this fast pace. Safety-critical applications, in particular, underlie strict
regulatory and ethical requirements which need to be taken care of and are
still active areas of debate. eXplainable AI (XAI) and privacy-preserving
machine learning (PPML) are both crucial research fields, aiming at mitigating
some of the drawbacks of prevailing data-hungry black-box models in DL. Despite
brisk research activity in the respective fields, no attention has yet been
paid to their interaction. This work is the first to investigate the impact of
private learning techniques on generated explanations for DL-based models. In
an extensive experimental analysis covering various image and time series
datasets from multiple domains, as well as varying privacy techniques, XAI
methods, and model architectures, the effects of private training on generated
explanations are studied. The findings suggest non-negligible changes in
explanations through the introduction of privacy. Apart from reporting
individual effects of PPML on XAI, the paper gives clear recommendations for
the choice of techniques in real applications. By unveiling the
interdependencies of these pivotal technologies, this work is a first step
towards overcoming the remaining hurdles for practically applicable AI in
safety-critical domains.

摘要：

##### **Predicting Treatment Adherence of Tuberculosis Patients at Scale**
2211.02943v2 by Mihir Kulkarni, Satvik Golechha, Rishi Raj, Jithin Sreedharan, Ankit Bhardwaj, Santanu Rathod, Bhavin Vadera, Jayakrishna Kurada, Sanjay Mattoo, Rajendra Joshi, Kirankumar Rade, Alpan Raval

Tuberculosis (TB), an infectious bacterial disease, is a significant cause of
death, especially in low-income countries, with an estimated ten million new
cases reported globally in $2020$. While TB is treatable, non-adherence to the
medication regimen is a significant cause of morbidity and mortality. Thus,
proactively identifying patients at risk of dropping off their medication
regimen enables corrective measures to mitigate adverse outcomes. Using a proxy
measure of extreme non-adherence and a dataset of nearly $700,000$ patients
from four states in India, we formulate and solve the machine learning (ML)
problem of early prediction of non-adherence based on a custom rank-based
metric. We train ML models and evaluate against baselines, achieving a $\sim
100\%$ lift over rule-based baselines and $\sim 214\%$ over a random
classifier, taking into account country-wide large-scale future deployment. We
deal with various issues in the process, including data quality,
high-cardinality categorical data, low target prevalence, distribution shift,
variation across cohorts, algorithmic fairness, and the need for robustness and
explainability. Our findings indicate that risk stratification of non-adherent
patients is a viable, deployable-at-scale ML solution. As the official AI
partner of India's Central TB Division, we are working on multiple city and
state-level pilots with the goal of pan-India deployment.

摘要：

##### **Explainable AI over the Internet of Things (IoT): Overview, State-of-the-Art and Future Directions**
2211.01036v2 by Senthil Kumar Jagatheesaperumal, Quoc-Viet Pham, Rukhsana Ruby, Zhaohui Yang, Chunmei Xu, Zhaoyang Zhang

Explainable Artificial Intelligence (XAI) is transforming the field of
Artificial Intelligence (AI) by enhancing the trust of end-users in machines.
As the number of connected devices keeps on growing, the Internet of Things
(IoT) market needs to be trustworthy for the end-users. However, existing
literature still lacks a systematic and comprehensive survey work on the use of
XAI for IoT. To bridge this lacking, in this paper, we address the XAI
frameworks with a focus on their characteristics and support for IoT. We
illustrate the widely-used XAI services for IoT applications, such as security
enhancement, Internet of Medical Things (IoMT), Industrial IoT (IIoT), and
Internet of City Things (IoCT). We also suggest the implementation choice of
XAI models over IoT systems in these applications with appropriate examples and
summarize the key inferences for future works. Moreover, we present the
cutting-edge development in edge XAI structures and the support of
sixth-generation (6G) communication services for IoT applications, along with
key inferences. In a nutshell, this paper constitutes the first holistic
compilation on the development of XAI-based frameworks tailored for the demands
of future IoT use cases.

摘要：

##### **Human-centered XAI for Burn Depth Characterization**
2210.13535v2 by Maxwell J. Jacobson, Daniela Chanci Arrubla, Maria Romeo Tricas, Gayle Gordillo, Yexiang Xue, Chandan Sen, Juan Wachs

Approximately 1.25 million people in the United States are treated each year
for burn injuries. Precise burn injury classification is an important aspect of
the medical AI field. In this work, we propose an explainable human-in-the-loop
framework for improving burn ultrasound classification models. Our framework
leverages an explanation system based on the LIME classification explainer to
corroborate and integrate a burn expert's knowledge -- suggesting new features
and ensuring the validity of the model. Using this framework, we discover that
B-mode ultrasound classifiers can be enhanced by supplying textural features.
More specifically, we confirm that texture features based on the Gray Level
Co-occurance Matrix (GLCM) of ultrasound frames can increase the accuracy of
transfer learned burn depth classifiers. We test our hypothesis on real data
from porcine subjects. We show improvements in the accuracy of burn depth
classification -- from ~88% to ~94% -- once modified according to our
framework.

摘要：

##### **What Do End-Users Really Want? Investigation of Human-Centered XAI for Mobile Health Apps**
2210.03506v1 by Katharina Weitz, Alexander Zellner, Elisabeth André

In healthcare, AI systems support clinicians and patients in diagnosis,
treatment, and monitoring, but many systems' poor explainability remains
challenging for practical application. Overcoming this barrier is the goal of
explainable AI (XAI). However, an explanation can be perceived differently and,
thus, not solve the black-box problem for everyone. The domain of
Human-Centered AI deals with this problem by adapting AI to users. We present a
user-centered persona concept to evaluate XAI and use it to investigate
end-users preferences for various explanation styles and contents in a mobile
health stress monitoring application. The results of our online survey show
that users' demographics and personality, as well as the type of explanation,
impact explanation preferences, indicating that these are essential features
for XAI design. We subsumed the results in three prototypical user personas:
power-, casual-, and privacy-oriented users. Our insights bring an interactive,
human-centered XAI closer to practical application.

摘要：

##### **Explainable AI based Glaucoma Detection using Transfer Learning and LIME**
2210.03332v1 by Touhidul Islam Chayan, Anita Islam, Eftykhar Rahman, Md. Tanzim Reza, Tasnim Sakib Apon, MD. Golam Rabiul Alam

Glaucoma is the second driving reason for partial or complete blindness among
all the visual deficiencies which mainly occurs because of excessive pressure
in the eye due to anxiety or depression which damages the optic nerve and
creates complications in vision. Traditional glaucoma screening is a
time-consuming process that necessitates the medical professionals' constant
attention, and even so time to time due to the time constrains and pressure
they fail to classify correctly that leads to wrong treatment. Numerous efforts
have been made to automate the entire glaucoma classification procedure
however, these existing models in general have a black box characteristics that
prevents users from understanding the key reasons behind the prediction and
thus medical practitioners generally can not rely on these system. In this
article after comparing with various pre-trained models, we propose a transfer
learning model that is able to classify Glaucoma with 94.71\% accuracy. In
addition, we have utilized Local Interpretable Model-Agnostic
Explanations(LIME) that introduces explainability in our system. This
improvement enables medical professionals obtain important and comprehensive
information that aid them in making judgments. It also lessen the opacity and
fragility of the traditional deep learning models.

摘要：

##### **Evaluation of importance estimators in deep learning classifiers for Computed Tomography**
2209.15398v1 by Lennart Brocki, Wistan Marchadour, Jonas Maison, Bogdan Badic, Panagiotis Papadimitroulas, Mathieu Hatt, Franck Vermet, Neo Christopher Chung

Deep learning has shown superb performance in detecting objects and
classifying images, ensuring a great promise for analyzing medical imaging.
Translating the success of deep learning to medical imaging, in which doctors
need to understand the underlying process, requires the capability to interpret
and explain the prediction of neural networks. Interpretability of deep neural
networks often relies on estimating the importance of input features (e.g.,
pixels) with respect to the outcome (e.g., class probability). However, a
number of importance estimators (also known as saliency maps) have been
developed and it is unclear which ones are more relevant for medical imaging
applications. In the present work, we investigated the performance of several
importance estimators in explaining the classification of computed tomography
(CT) images by a convolutional deep network, using three distinct evaluation
metrics. First, the model-centric fidelity measures a decrease in the model
accuracy when certain inputs are perturbed. Second, concordance between
importance scores and the expert-defined segmentation masks is measured on a
pixel level by a receiver operating characteristic (ROC) curves. Third, we
measure a region-wise overlap between a XRAI-based map and the segmentation
mask by Dice Similarity Coefficients (DSC). Overall, two versions of SmoothGrad
topped the fidelity and ROC rankings, whereas both Integrated Gradients and
SmoothGrad excelled in DSC evaluation. Interestingly, there was a critical
discrepancy between model-centric (fidelity) and human-centric (ROC and DSC)
evaluation. Expert expectation and intuition embedded in segmentation maps does
not necessarily align with how the model arrived at its prediction.
Understanding this difference in interpretability would help harnessing the
power of deep learning in medicine.

摘要：

##### **An Interactive Interpretability System for Breast Cancer Screening with Deep Learning**
2210.08979v1 by Yuzhe Lu, Adam Perer

Deep learning methods, in particular convolutional neural networks, have
emerged as a powerful tool in medical image computing tasks. While these
complex models provide excellent performance, their black-box nature may hinder
real-world adoption in high-stakes decision-making. In this paper, we propose
an interactive system to take advantage of state-of-the-art interpretability
techniques to assist radiologists with breast cancer screening. Our system
integrates a deep learning model into the radiologists' workflow and provides
novel interactions to promote understanding of the model's decision-making
process. Moreover, we demonstrate that our system can take advantage of user
interactions progressively to provide finer-grained explainability reports with
little labeling overhead. Due to the generic nature of the adopted
interpretability technique, our system is domain-agnostic and can be used for
many different medical image computing tasks, presenting a novel perspective on
how we can leverage visual analytics to transform originally static
interpretability techniques to augment human decision making and promote the
adoption of medical AI.

摘要：

##### **Explainable AI for clinical and remote health applications: a survey on tabular and time series data**
2209.06528v1 by Flavio Di Martino, Franca Delmastro

Nowadays Artificial Intelligence (AI) has become a fundamental component of
healthcare applications, both clinical and remote, but the best performing AI
systems are often too complex to be self-explaining. Explainable AI (XAI)
techniques are defined to unveil the reasoning behind the system's predictions
and decisions, and they become even more critical when dealing with sensitive
and personal health data. It is worth noting that XAI has not gathered the same
attention across different research areas and data types, especially in
healthcare. In particular, many clinical and remote health applications are
based on tabular and time series data, respectively, and XAI is not commonly
analysed on these data types, while computer vision and Natural Language
Processing (NLP) are the reference applications. To provide an overview of XAI
methods that are most suitable for tabular and time series data in the
healthcare domain, this paper provides a review of the literature in the last 5
years, illustrating the type of generated explanations and the efforts provided
to evaluate their relevance and quality. Specifically, we identify clinical
validation, consistency assessment, objective and standardised quality
evaluation, and human-centered quality assessment as key features to ensure
effective explanations for the end users. Finally, we highlight the main
research challenges in the field as well as the limitations of existing XAI
methods.

摘要：

##### **Enhancing Early Lung Cancer Detection on Chest Radiographs with AI-assistance: A Multi-Reader Study**
2208.14742v1 by Gaetan Dissez, Nicole Tay, Tom Dyer, Matthew Tam, Richard Dittrich, David Doyne, James Hoare, Jackson J. Pat, Stephanie Patterson, Amanda Stockham, Qaiser Malik, Tom Naunton Morgan, Paul Williams, Liliana Garcia-Mondragon, Jordan Smith, George Pearse, Simon Rasalingham

Objectives: The present study evaluated the impact of a commercially
available explainable AI algorithm in augmenting the ability of clinicians to
identify lung cancer on chest X-rays (CXR).
  Design: This retrospective study evaluated the performance of 11 clinicians
for detecting lung cancer from chest radiographs, with and without assistance
from a commercially available AI algorithm (red dot, Behold.ai) that predicts
suspected lung cancer from CXRs. Clinician performance was evaluated against
clinically confirmed diagnoses.
  Setting: The study analysed anonymised patient data from an NHS hospital; the
dataset consisted of 400 chest radiographs from adult patients (18 years and
above) who had a CXR performed in 2020, with corresponding clinical text
reports.
  Participants: A panel of readers consisting of 11 clinicians (consultant
radiologists, radiologist trainees and reporting radiographers) participated in
this study.
  Main outcome measures: Overall accuracy, sensitivity, specificity and
precision for detecting lung cancer on CXRs by clinicians, with and without AI
input. Agreement rates between clinicians and performance standard deviation
were also evaluated, with and without AI input.
  Results: The use of the AI algorithm by clinicians led to an improved overall
performance for lung tumour detection, achieving an overall increase of 17.4%
of lung cancers being identified on CXRs which would have otherwise been
missed, an overall increase in detection of smaller tumours, a 24% and 13%
increased detection of stage 1 and stage 2 lung cancers respectively, and
standardisation of clinician performance.
  Conclusions: This study showed great promise in the clinical utility of AI
algorithms in improving early lung cancer diagnosis and promoting health equity
through overall improvement in reader performances, without impacting
downstream imaging resources.

摘要：

##### **GAN-based generative modelling for dermatological applications -- comparative study**
2208.11702v1 by Sandra Carrasco Limeros, Sylwia Majchrowska, Mohamad Khir Zoubi, Anna Rosén, Juulia Suvilehto, Lisa Sjöblom, Magnus Kjellberg

The lack of sufficiently large open medical databases is one of the biggest
challenges in AI-powered healthcare. Synthetic data created using Generative
Adversarial Networks (GANs) appears to be a good solution to mitigate the
issues with privacy policies. The other type of cure is decentralized protocol
across multiple medical institutions without exchanging local data samples. In
this paper, we explored unconditional and conditional GANs in centralized and
decentralized settings. The centralized setting imitates studies on large but
highly unbalanced skin lesion dataset, while the decentralized one simulates a
more realistic hospital scenario with three institutions. We evaluated models'
performance in terms of fidelity, diversity, speed of training, and predictive
ability of classifiers trained on the generated synthetic data. In addition we
provided explainability through exploration of latent space and embeddings
projection focused both on global and local explanations. Calculated distance
between real images and their projections in the latent space proved the
authenticity and generalization of trained GANs, which is one of the main
concerns in this type of applications. The open source code for conducted
studies is publicly available at
\url{https://github.com/aidotse/stylegan2-ada-pytorch}.

摘要：

##### **Planning and Scheduling in Digital Health with Answer Set Programming**
2208.03099v1 by Marco Mochi

In the hospital world there are several complex combinatory problems, and
solving these problems is important to increase the degree of patients'
satisfaction and the quality of care offered. The problems in the healthcare
are complex since to solve them several constraints and different type of
resources should be taken into account. Moreover, the solutions must be
evaluated in a small amount of time to ensure the usability in real scenarios.
We plan to propose solutions to these kind of problems both expanding already
tested solutions and by modelling solutions for new problems, taking into
account the literature and by using real data when available. Solving these
kind of problems is important but, since the European Commission established
with the General Data Protection Regulation that each person has the right to
ask for explanation of the decision taken by an AI, without developing
Explainability methodologies the usage of AI based solvers e.g. those based on
Answer Set programming will be limited. Thus, another part of the research will
be devoted to study and propose new methodologies for explaining the solutions
obtained.

摘要：

##### **AI Approaches in Processing and Using Data in Personalized Medicine**
2208.04698v1 by Mirjana Ivanovic, Serge Autexier, Miltiadis Kokkonidis

In modern dynamic constantly developing society, more and more people suffer
from chronic and serious diseases and doctors and patients need special and
sophisticated medical and health support. Accordingly, prominent health
stakeholders have recognized the importance of development of such services to
make patients life easier. Such support requires the collection of huge amount
of patients complex data like clinical, environmental, nutritional, daily
activities, variety of data from smart wearable devices, data from clothing
equipped with sensors etc. Holistic patients data must be properly aggregated,
processed, analyzed, and presented to the doctors and caregivers to recommend
adequate treatment and actions to improve patients health related parameters
and general wellbeing. Advanced artificial intelligence techniques offer the
opportunity to analyze such big data, consume them, and derive new knowledge to
support personalized medical decisions. New approaches like those based on
advanced machine learning, federated learning, transfer learning, explainable
artificial intelligence open new paths for more quality use of health and
medical data in future. In this paper, we will present some crucial aspects and
characteristic examples in the area of application of a range of artificial
intelligence approaches in personalized medical decisions.

摘要：

##### **TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring**
2207.11290v2 by Nandita Bhaskhar, Daniel L. Rubin, Christopher Lee-Messer

Continuous monitoring of trained ML models to determine when their
predictions should and should not be trusted is essential for their safe
deployment. Such a framework ought to be high-performing, explainable, post-hoc
and actionable. We propose TRUST-LAPSE, a "mistrust" scoring framework for
continuous model monitoring. We assess the trustworthiness of each input
sample's model prediction using a sequence of latent-space embeddings.
Specifically, (a) our latent-space mistrust score estimates mistrust using
distance metrics (Mahalanobis distance) and similarity metrics (cosine
similarity) in the latent-space and (b) our sequential mistrust score
determines deviations in correlations over the sequence of past input
representations in a non-parametric, sliding-window based algorithm for
actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream
tasks: (1) distributionally shifted input detection, and (2) data drift
detection. We evaluate across diverse domains - audio and vision using public
datasets and further benchmark our approach on challenging, real-world
electroencephalograms (EEG) datasets for seizure detection. Our latent-space
mistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision),
73.9 (audio), and 77.1 (clinical EEGs), outperforming baselines by over 10
points. We expose critical failures in popular baselines that remain
insensitive to input semantic content, rendering them unfit for real-world
model monitoring. We show that our sequential mistrust scores achieve high
drift detection rates; over 90% of the streams show < 20% error for all
domains. Through extensive qualitative and quantitative evaluations, we show
that our mistrust scores are more robust and provide explainability for easy
adoption into practice.

摘要：

##### **Revealing Unfair Models by Mining Interpretable Evidence**
2207.05811v1 by Mohit Bajaj, Lingyang Chu, Vittorio Romaniello, Gursimran Singh, Jian Pei, Zirui Zhou, Lanjun Wang, Yong Zhang

The popularity of machine learning has increased the risk of unfair models
getting deployed in high-stake applications, such as justice system,
drug/vaccination design, and medical diagnosis. Although there are effective
methods to train fair models from scratch, how to automatically reveal and
explain the unfairness of a trained model remains a challenging task. Revealing
unfairness of machine learning models in interpretable fashion is a critical
step towards fair and trustworthy AI. In this paper, we systematically tackle
the novel task of revealing unfair models by mining interpretable evidence
(RUMIE). The key idea is to find solid evidence in the form of a group of data
instances discriminated most by the model. To make the evidence interpretable,
we also find a set of human-understandable key attributes and decision rules
that characterize the discriminated data instances and distinguish them from
the other non-discriminated data. As demonstrated by extensive experiments on
many real-world data sets, our method finds highly interpretable and solid
evidence to effectively reveal the unfairness of trained models. Moreover, it
is much more scalable than all of the baseline methods.

摘要：

##### **From Correlation to Causation: Formalizing Interpretable Machine Learning as a Statistical Process**
2207.04969v1 by Lukas Klein, Mennatallah El-Assady, Paul F. Jäger

Explainable AI (XAI) is a necessity in safety-critical systems such as in
clinical diagnostics due to a high risk for fatal decisions. Currently,
however, XAI resembles a loose collection of methods rather than a well-defined
process. In this work, we elaborate on conceptual similarities between the
largest subgroup of XAI, interpretable machine learning (IML), and classical
statistics. Based on these similarities, we present a formalization of IML
along the lines of a statistical process. Adopting this statistical view allows
us to interpret machine learning models and IML methods as sophisticated
statistical tools. Based on this interpretation, we infer three key questions,
which we identify as crucial for the success and adoption of IML in
safety-critical settings. By formulating these questions, we further aim to
spark a discussion about what distinguishes IML from classical statistics and
what our perspective implies for the future of the field.

摘要：


### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-07**|**QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving**|Yujun Lin et.al.|[2405.04532v1](http://arxiv.org/abs/2405.04532v1)|[link](https://github.com/mit-han-lab/qserve)|
|**2024-05-07**|**NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts**|Shudan Zhang et.al.|[2405.04520v1](http://arxiv.org/abs/2405.04520v1)|null|
|**2024-05-07**|**xLSTM: Extended Long Short-Term Memory**|Maximilian Beck et.al.|[2405.04517v1](http://arxiv.org/abs/2405.04517v1)|null|
|**2024-05-07**|**A Transformer with Stack Attention**|Jiaoda Li et.al.|[2405.04515v1](http://arxiv.org/abs/2405.04515v1)|[link](https://github.com/rycolab/stack-transformer)|
|**2024-05-07**|**Switchable Decision: Dynamic Neural Generation Networks**|Shujian Zhang et.al.|[2405.04513v1](http://arxiv.org/abs/2405.04513v1)|null|
|**2024-05-07**|**Toward In-Context Teaching: Adapting Examples to Students' Misconceptions**|Alexis Ross et.al.|[2405.04495v1](http://arxiv.org/abs/2405.04495v1)|null|
|**2024-05-07**|**TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous Driving with Reactive, Realistic, and Diverse Non-Playable Characters**|Jonathan Wilder Lavington et.al.|[2405.04491v1](http://arxiv.org/abs/2405.04491v1)|null|
|**2024-05-07**|**Towards Continual Knowledge Graph Embedding via Incremental Distillation**|Jiajun Liu et.al.|[2405.04453v1](http://arxiv.org/abs/2405.04453v1)|[link](https://github.com/seukgcode/incde)|
|**2024-05-07**|**POV Learning: Individual Alignment of Multimodal Models using Human Perception**|Simon Werner et.al.|[2405.04443v1](http://arxiv.org/abs/2405.04443v1)|null|
|**2024-05-07**|**AugmenTory: A Fast and Flexible Polygon Augmentation Library**|Tanaz Ghahremani et.al.|[2405.04442v1](http://arxiv.org/abs/2405.04442v1)|null|
|**2024-05-07**|**DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model**|DeepSeek-AI et.al.|[2405.04434v1](http://arxiv.org/abs/2405.04434v1)|[link](https://github.com/deepseek-ai/deepseek-v2)|
|**2024-05-07**|**Vision Mamba: A Comprehensive Survey and Taxonomy**|Xiao Liu et.al.|[2405.04404v1](http://arxiv.org/abs/2405.04404v1)|[link](https://github.com/lx6c78/vision-mamba-a-comprehensive-survey-and-taxonomy)|
|**2024-05-07**|**Pragmatist Intelligence: Where the Principle of Usefulness Can Take ANNs**|Antonio Bikić et.al.|[2405.04386v1](http://arxiv.org/abs/2405.04386v1)|null|
|**2024-05-07**|**Leveraging LSTM and GAN for Modern Malware Detection**|Ishita Gupta et.al.|[2405.04373v1](http://arxiv.org/abs/2405.04373v1)|null|
|**2024-05-07**|**Explainable machine learning for predicting shellfish toxicity in the Adriatic Sea using long-term monitoring data of HABs**|Martin Marzidovšek et.al.|[2405.04372v1](http://arxiv.org/abs/2405.04372v1)|null|
|**2024-05-07**|**Global Scale Self-Supervised Channel Charting with Sensor Fusion**|Omid Esrafilian et.al.|[2405.04357v1](http://arxiv.org/abs/2405.04357v1)|null|
|**2024-05-07**|**Revisiting character-level adversarial attacks**|Elias Abad Rocamora et.al.|[2405.04346v1](http://arxiv.org/abs/2405.04346v1)|[link](https://github.com/lions-epfl/charmer)|
|**2024-05-07**|**Temporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction**|Zhihao Wen et.al.|[2405.04336v1](http://arxiv.org/abs/2405.04336v1)|null|
|**2024-05-07**|**A Fourth Wave of Open Data? Exploring the Spectrum of Scenarios for Open Data and Generative AI**|Hannah Chafetz et.al.|[2405.04333v1](http://arxiv.org/abs/2405.04333v1)|null|
|**2024-05-07**|**Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation**|Atharvan Dogra et.al.|[2405.04325v1](http://arxiv.org/abs/2405.04325v1)|null|
|**2024-05-07**|**Granite Code Models: A Family of Open Foundation Models for Code Intelligence**|Mayank Mishra et.al.|[2405.04324v1](http://arxiv.org/abs/2405.04324v1)|[link](https://github.com/ibm-granite/granite-code-models)|
|**2024-05-07**|**Beyond human subjectivity and error: a novel AI grading system**|Alexandra Gobrecht et.al.|[2405.04323v1](http://arxiv.org/abs/2405.04323v1)|null|
|**2024-05-07**|**Cross-IQA: Unsupervised Learning for Image Quality Assessment**|Zhen Zhang et.al.|[2405.04311v1](http://arxiv.org/abs/2405.04311v1)|null|
|**2024-05-07**|**Improving Offline Reinforcement Learning with Inaccurate Simulators**|Yiwen Hou et.al.|[2405.04307v1](http://arxiv.org/abs/2405.04307v1)|null|
|**2024-05-07**|**A New Dataset and Comparative Study for Aphid Cluster Detection and Segmentation in Sorghum Fields**|Raiyan Rahman et.al.|[2405.04305v1](http://arxiv.org/abs/2405.04305v1)|null|
|**2024-05-07**|**Accelerating Speculative Decoding using Dynamic Speculation Length**|Jonathan Mamou et.al.|[2405.04304v1](http://arxiv.org/abs/2405.04304v1)|null|
|**2024-05-07**|**Behaviour Planning: A Toolkit for Diverse Planning**|Mustafa F Abdelwahed et.al.|[2405.04300v1](http://arxiv.org/abs/2405.04300v1)|null|
|**2024-05-07**|**Enhancing the Efficiency and Accuracy of Underlying Asset Reviews in Structured Finance: The Application of Multi-agent Framework**|Xiangpeng Wan et.al.|[2405.04294v1](http://arxiv.org/abs/2405.04294v1)|null|
|**2024-05-07**|**Mitigating Clickbait: An Approach to Spoiler Generation Using Multitask Learning**|Sayantan Pal et.al.|[2405.04292v1](http://arxiv.org/abs/2405.04292v1)|null|
|**2024-05-07**|**Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore**|Junchao Wu et.al.|[2405.04286v1](http://arxiv.org/abs/2405.04286v1)|null|
|**2024-05-07**|**On the Foundations of Earth and Climate Foundation Models**|Xiao Xiang Zhu et.al.|[2405.04285v1](http://arxiv.org/abs/2405.04285v1)|null|
|**2024-05-07**|**Generating Feature Vectors from Phonetic Transcriptions in Cross-Linguistic Data Formats**|Arne Rubehn et.al.|[2405.04271v1](http://arxiv.org/abs/2405.04271v1)|null|
|**2024-05-07**|**VAEneu: A New Avenue for VAE Application on Probabilistic Forecasting**|Alireza Koochali et.al.|[2405.04252v1](http://arxiv.org/abs/2405.04252v1)|null|
|**2024-05-07**|**Federated Learning for Cooperative Inference Systems: The Case of Early Exit Networks**|Caelin Kaplan et.al.|[2405.04249v1](http://arxiv.org/abs/2405.04249v1)|null|
|**2024-05-07**|**Exploring Correlations of Self-supervised Tasks for Graphs**|Taoran Fang et.al.|[2405.04245v1](http://arxiv.org/abs/2405.04245v1)|null|
|**2024-05-07**|**Iterative Experience Refinement of Software-Developing Agents**|Chen Qian et.al.|[2405.04219v1](http://arxiv.org/abs/2405.04219v1)|null|
|**2024-05-07**|**NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions**|Elliot Gestrin et.al.|[2405.04215v1](http://arxiv.org/abs/2405.04215v1)|null|
|**2024-05-07**|**Green Tsetlin Redefining Efficiency in Tsetlin Machine Frameworks**|Sondre Glimsdal et.al.|[2405.04212v1](http://arxiv.org/abs/2405.04212v1)|null|
|**2024-05-07**|**NOVA: NoC-based Vector Unit for Mapping Attention Layers on a CNN Accelerator**|Mohit Upadhyay et.al.|[2405.04206v1](http://arxiv.org/abs/2405.04206v1)|null|
|**2024-05-07**|**FedStale: leveraging stale client updates in federated learning**|Angelo Rodio et.al.|[2405.04171v1](http://arxiv.org/abs/2405.04171v1)|null|
|**2024-05-07**|**D-NLP at SemEval-2024 Task 2: Evaluating Clinical Inference Capabilities of Large Language Models**|Duygu Altinok et.al.|[2405.04170v1](http://arxiv.org/abs/2405.04170v1)|null|
|**2024-05-07**|**LingML: Linguistic-Informed Machine Learning for Enhanced Fake News Detection**|Jasraj Singh et.al.|[2405.04165v1](http://arxiv.org/abs/2405.04165v1)|null|
|**2024-05-07**|**MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization**|Gunjan Balde et.al.|[2405.04163v1](http://arxiv.org/abs/2405.04163v1)|[link](https://github.com/gb-kgp/medvoc)|
|**2024-05-07**|**A Causal Explainable Guardrails for Large Language Models**|Zhixuan Chu et.al.|[2405.04160v1](http://arxiv.org/abs/2405.04160v1)|null|
|**2024-05-07**|**GPT-Enabled Cybersecurity Training: A Tailored Approach for Effective Awareness**|Nabil Al-Dhamari et.al.|[2405.04138v1](http://arxiv.org/abs/2405.04138v1)|null|
|**2024-05-07**|**Enriched BERT Embeddings for Scholarly Publication Classification**|Benjamin Wolff et.al.|[2405.04136v1](http://arxiv.org/abs/2405.04136v1)|null|
|**2024-05-07**|**In-context Learning for Automated Driving Scenarios**|Ziqi Zhou et.al.|[2405.04135v1](http://arxiv.org/abs/2405.04135v1)|null|
|**2024-05-07**|**Fine-grained Speech Sentiment Analysis in Chinese Psychological Support Hotlines Based on Large-scale Pre-trained Model**|Zhonglong Chen et.al.|[2405.04128v1](http://arxiv.org/abs/2405.04128v1)|[link](https://github.com/czl0914/psy_hotline_analysis)|
|**2024-05-07**|**Comparative Study of Recurrent Neural Networks for Virtual Analog Audio Effects Modeling**|Riccardo Simionato et.al.|[2405.04124v1](http://arxiv.org/abs/2405.04124v1)|null|
|**2024-05-07**|**Policy Learning with a Language Bottleneck**|Megha Srivastava et.al.|[2405.04118v1](http://arxiv.org/abs/2405.04118v1)|[link](https://github.com/meghabyte/bottleneck)|
|**2024-05-07**|**A2-DIDM: Privacy-preserving Accumulator-enabled Auditing for Distributed Identity of DNN Model**|Tianxiu Xie et.al.|[2405.04108v1](http://arxiv.org/abs/2405.04108v1)|null|
|**2024-05-07**|**Continual Learning in the Presence of Repetition**|Hamed Hemati et.al.|[2405.04101v1](http://arxiv.org/abs/2405.04101v1)|null|
|**2024-05-07**|**Unmasking Illusions: Understanding Human Perception of Audiovisual Deepfakes**|Ammarah Hashmi et.al.|[2405.04097v1](http://arxiv.org/abs/2405.04097v1)|null|
|**2024-05-07**|**Going Proactive and Explanatory Against Malware Concept Drift**|Yiling He et.al.|[2405.04095v1](http://arxiv.org/abs/2405.04095v1)|null|
|**2024-05-07**|**DCNN: Dual Cross-current Neural Networks Realized Using An Interactive Deep Learning Discriminator for Fine-grained Objects**|Da Fu et.al.|[2405.04093v1](http://arxiv.org/abs/2405.04093v1)|null|
|**2024-05-07**|**Optimizing Language Model's Reasoning Abilities with Weak Supervision**|Yongqi Tong et.al.|[2405.04086v1](http://arxiv.org/abs/2405.04086v1)|null|
|**2024-05-07**|**Counterfactual and Semifactual Explanations in Abstract Argumentation: Formal Foundations, Complexity and Computation**|Gianvincenzo Alfano et.al.|[2405.04081v1](http://arxiv.org/abs/2405.04081v1)|null|
|**2024-05-07**|**WISER: Weak supervISion and supErvised Representation learning to improve drug response prediction in cancer**|Kumar Shubham et.al.|[2405.04078v1](http://arxiv.org/abs/2405.04078v1)|null|
|**2024-05-07**|**A simple theory for training response of deep neural networks**|Kenichi Nakazato et.al.|[2405.04074v1](http://arxiv.org/abs/2405.04074v1)|null|
|**2024-05-07**|**FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference**|Runheng Liu et.al.|[2405.04065v1](http://arxiv.org/abs/2405.04065v1)|null|
|**2024-05-07**|**Evaluating Text Summaries Generated by Large Language Models Using OpenAI's GPT**|Hassan Shakil et.al.|[2405.04053v1](http://arxiv.org/abs/2405.04053v1)|null|
|**2024-05-07**|**Learning Linear Block Error Correction Codes**|Yoni Choukroun et.al.|[2405.04050v1](http://arxiv.org/abs/2405.04050v1)|[link](https://github.com/yonilc/e2e_dc_ecct)|
|**2024-05-07**|**Philosophy of Cognitive Science in the Age of Deep Learning**|Raphaël Millière et.al.|[2405.04048v1](http://arxiv.org/abs/2405.04048v1)|null|
|**2024-05-07**|**Feature Map Convergence Evaluation for Functional Module**|Ludan Zhang et.al.|[2405.04041v1](http://arxiv.org/abs/2405.04041v1)|null|
|**2024-05-07**|**Utilizing GPT to Enhance Text Summarization: A Strategy to Minimize Hallucinations**|Hassan Shakil et.al.|[2405.04039v1](http://arxiv.org/abs/2405.04039v1)|null|
|**2024-05-07**|**Locally Differentially Private In-Context Learning**|Chunyan Zheng et.al.|[2405.04032v1](http://arxiv.org/abs/2405.04032v1)|null|
|**2024-05-07**|**Certified Policy Verification and Synthesis for MDPs under Distributional Reach-avoidance Properties**|S. Akshay et.al.|[2405.04015v1](http://arxiv.org/abs/2405.04015v1)|null|
|**2024-05-07**|**Structured Click Control in Transformer-based Interactive Segmentation**|Long Xu et.al.|[2405.04009v1](http://arxiv.org/abs/2405.04009v1)|[link](https://github.com/hahamyt/scc)|
|**2024-05-07**|**Sketch Then Generate: Providing Incremental User Feedback and Guiding LLM Code Generation through Language-Oriented Code Sketches**|Chen Zhu-Tian et.al.|[2405.03998v1](http://arxiv.org/abs/2405.03998v1)|null|
|**2024-05-07**|**TrimCaching: Parameter-sharing AI Model Caching in Wireless Edge Networks**|Guanqiao Qu et.al.|[2405.03990v1](http://arxiv.org/abs/2405.03990v1)|null|
|**2024-05-07**|**Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application**|Jian Jia et.al.|[2405.03988v1](http://arxiv.org/abs/2405.03988v1)|null|
|**2024-05-07**|**Factors Influencing User Willingness To Use SORA**|Gustave Florentin Nkoulou Mvondo et.al.|[2405.03986v1](http://arxiv.org/abs/2405.03986v1)|null|
|**2024-05-07**|**TBNet: A Neural Architectural Defense Framework Facilitating DNN Model Protection in Trusted Execution Environments**|Ziyu Liu et.al.|[2405.03974v1](http://arxiv.org/abs/2405.03974v1)|null|
|**2024-05-07**|**ERATTA: Extreme RAG for Table To Answers with Large Language Models**|Sohini Roychowdhury et.al.|[2405.03963v1](http://arxiv.org/abs/2405.03963v1)|null|
|**2024-05-07**|**ESIHGNN: Event-State Interactions Infused Heterogeneous Graph Neural Network for Conversational Emotion Recognition**|Xupeng Zha et.al.|[2405.03960v1](http://arxiv.org/abs/2405.03960v1)|null|
|**2024-05-07**|**Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your Diffusion Model**|Joo Young Choi et.al.|[2405.03958v1](http://arxiv.org/abs/2405.03958v1)|null|
|**2024-05-07**|**HAFFormer: A Hierarchical Attention-Free Framework for Alzheimer's Disease Detection From Spontaneous Speech**|Zhongren Dong et.al.|[2405.03952v1](http://arxiv.org/abs/2405.03952v1)|null|
|**2024-05-07**|**Predictive Modeling with Temporal Graphical Representation on Electronic Health Records**|Jiayuan Chen et.al.|[2405.03943v1](http://arxiv.org/abs/2405.03943v1)|[link](https://github.com/the-real-jerrychen/trans)|
|**2024-05-07**|**Long Context Alignment with Short Instructions and Synthesized Positions**|Wenhao Wu et.al.|[2405.03939v1](http://arxiv.org/abs/2405.03939v1)|null|
|**2024-05-07**|**CleanGraph: Human-in-the-loop Knowledge Graph Refinement and Completion**|Tyler Bikaun et.al.|[2405.03932v1](http://arxiv.org/abs/2405.03932v1)|[link](https://github.com/nlp-tlp/cleangraph)|
|**2024-05-07**|**Unicorn: U-Net for Sea Ice Forecasting with Convolutional Neural Ordinary Differential Equations**|Jaesung Park et.al.|[2405.03929v1](http://arxiv.org/abs/2405.03929v1)|null|
|**2024-05-07**|**A Roadmap for Multilingual, Multimodal Domain Independent Deception Detection**|Dainis Boumber et.al.|[2405.03920v1](http://arxiv.org/abs/2405.03920v1)|null|
|**2024-05-06**|**OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs**|Jiahao Nick Li et.al.|[2405.03901v1](http://arxiv.org/abs/2405.03901v1)|null|
|**2024-05-06**|**Out-of-Distribution Adaptation in Offline RL: Counterfactual Reasoning via Causal Normalizing Flows**|Minjae Cho et.al.|[2405.03892v1](http://arxiv.org/abs/2405.03892v1)|null|
|**2024-05-06**|**Enhancing O-RAN Security: Evasion Attacks and Robust Defenses for Graph Reinforcement Learning-based Connection Management**|Ravikumar Balakrishnan et.al.|[2405.03891v1](http://arxiv.org/abs/2405.03891v1)|null|
|**2024-05-06**|**Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer**|Huihong Shi et.al.|[2405.03882v1](http://arxiv.org/abs/2405.03882v1)|null|
|**2024-05-06**|**Investigating Personalized Driving Behaviors in Dilemma Zones: Analysis and Prediction of Stop-or-Go Decisions**|Ziye Qin et.al.|[2405.03873v1](http://arxiv.org/abs/2405.03873v1)|null|
|**2024-05-06**|**AI-Driven Frameworks for Enhancing Data Quality in Big Data Ecosystems: Error_Detection, Correction, and Metadata Integration**|Widad Elouataoui et.al.|[2405.03870v1](http://arxiv.org/abs/2405.03870v1)|null|
|**2024-05-06**|**Outlier Gradient Analysis: Efficiently Improving Deep Learning Model Performance via Hessian-Free Influence Functions**|Anshuman Chhabra et.al.|[2405.03869v1](http://arxiv.org/abs/2405.03869v1)|null|
|**2024-05-06**|**Learning Planning Abstractions from Language**|Weiyu Liu et.al.|[2405.03864v1](http://arxiv.org/abs/2405.03864v1)|null|
|**2024-05-06**|**Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration**|Razan Baltaji et.al.|[2405.03862v1](http://arxiv.org/abs/2405.03862v1)|null|
|**2024-05-06**|**VSA4VQA: Scaling a Vector Symbolic Architecture to Visual Question Answering on Natural Images**|Anna Penzkofer et.al.|[2405.03852v1](http://arxiv.org/abs/2405.03852v1)|null|
|**2024-05-06**|**Self-Improving Customer Review Response Generation Based on LLMs**|Guy Azov et.al.|[2405.03845v1](http://arxiv.org/abs/2405.03845v1)|null|
|**2024-05-06**|**A Novel Cross-band CSI Prediction Scheme for Multi-band Fingerprint based Localization**|Yuan Ruihao et.al.|[2405.03842v1](http://arxiv.org/abs/2405.03842v1)|null|
|**2024-05-06**|**Guylingo: The Republic of Guyana Creole Corpora**|Christopher Clarke et.al.|[2405.03832v1](http://arxiv.org/abs/2405.03832v1)|null|
|**2024-05-06**|**Organizing a Society of Language Models: Structures and Mechanisms for Enhanced Collective Intelligence**|Silvan Ferreira et.al.|[2405.03825v1](http://arxiv.org/abs/2405.03825v1)|null|
|**2024-05-06**|**Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models**|Evan King et.al.|[2405.03821v1](http://arxiv.org/abs/2405.03821v1)|null|
|**2024-05-06**|**SocialFormer: Social Interaction Modeling with Edge-enhanced Heterogeneous Graph Transformers for Trajectory Prediction**|Zixu Wang et.al.|[2405.03809v1](http://arxiv.org/abs/2405.03809v1)|null|
|**2024-05-06**|**Synthetic Data from Diffusion Models Improve Drug Discovery Prediction**|Bing Hu et.al.|[2405.03799v1](http://arxiv.org/abs/2405.03799v1)|null|
|**2024-05-06**|**Detecting Anti-Semitic Hate Speech using Transformer-based Large Language Models**|Dengyi Liu et.al.|[2405.03794v1](http://arxiv.org/abs/2405.03794v1)|null|

#### Abstracts
##### **QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving**
2405.04532v1 by Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han

Quantization can accelerate large language model (LLM) inference. Going
beyond INT8 quantization, the research community is actively exploring even
lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization
techniques only accelerate low-batch, edge LLM inference, failing to deliver
performance gains in large-batch, cloud-based LLM serving. We uncover a
critical issue: existing INT4 quantization methods suffer from significant
runtime overhead (20-90%) when dequantizing either weights or partial sums on
GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization
algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands
for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented
by the QServe inference library that achieves measured speedup. The key insight
driving QServe is that the efficiency of LLM serving on GPUs is critically
influenced by operations on low-throughput CUDA cores. Building upon this
insight, in QoQ algorithm, we introduce progressive quantization that can allow
low dequantization overhead in W4A8 GEMM. Additionally, we develop
SmoothAttention to effectively mitigate the accuracy degradation incurred by
4-bit KV quantization. In the QServe system, we perform compute-aware weight
reordering and take advantage of register-level parallelism to reduce
dequantization latency. We also make fused attention memory-bound, harnessing
the performance gain brought by KV4 quantization. As a result, QServe improves
the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x
on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to
TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput
than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of
LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.

摘要：

##### **NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts**
2405.04520v1 by Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, Jie Tang

Large language models (LLMs) have manifested strong ability to generate codes
for productive activities. However, current benchmarks for code synthesis, such
as HumanEval, MBPP, and DS-1000, are predominantly oriented towards
introductory tasks on algorithm and data science, insufficiently satisfying
challenging requirements prevalent in real-world coding. To fill this gap, we
propose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror
the complexity and variety of scenarios in real coding tasks. NCB comprises 402
high-quality problems in Python and Java, meticulously selected from natural
user queries from online coding services, covering 6 different domains. Noting
the extraordinary difficulty in creating testing cases for real-world queries,
we also introduce a semi-automated pipeline to enhance the efficiency of test
case construction. Comparing with manual solutions, it achieves an efficiency
increase of more than 4 times. Our systematic experiments on 39 LLMs find that
performance gaps on NCB between models with close HumanEval scores could still
be significant, indicating a lack of focus on practical code synthesis
scenarios or over-specified optimization on HumanEval. On the other hand, even
the best-performing GPT-4 is still far from satisfying on NCB. The evaluation
toolkit and development set are available at
https://github.com/THUDM/NaturalCodeBench.

摘要：

##### **xLSTM: Extended Long Short-Term Memory**
2405.04517v1 by Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter

In the 1990s, the constant error carousel and gating were introduced as the
central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have
stood the test of time and contributed to numerous deep learning success
stories, in particular they constituted the first Large Language Models (LLMs).
However, the advent of the Transformer technology with parallelizable
self-attention at its core marked the dawn of a new era, outpacing LSTMs at
scale. We now raise a simple question: How far do we get in language modeling
when scaling LSTMs to billions of parameters, leveraging the latest techniques
from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we
introduce exponential gating with appropriate normalization and stabilization
techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM
with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that
is fully parallelizable with a matrix memory and a covariance update rule.
Integrating these LSTM extensions into residual block backbones yields xLSTM
blocks that are then residually stacked into xLSTM architectures. Exponential
gating and modified memory structures boost xLSTM capabilities to perform
favorably when compared to state-of-the-art Transformers and State Space
Models, both in performance and scaling.

摘要：

##### **A Transformer with Stack Attention**
2405.04515v1 by Jiaoda Li, Jennifer C. White, Mrinmaya Sachan, Ryan Cotterell

Natural languages are believed to be (mildly) context-sensitive. Despite
underpinning remarkably capable large language models, transformers are unable
to model many context-free language tasks. In an attempt to address this
limitation in the modeling power of transformer-based language models, we
propose augmenting them with a differentiable, stack-based attention mechanism.
Our stack-based attention mechanism can be incorporated into any
transformer-based language model and adds a level of interpretability to the
model. We show that the addition of our stack-based attention mechanism enables
the transformer to model some, but not all, deterministic context-free
languages.

摘要：

##### **Switchable Decision: Dynamic Neural Generation Networks**
2405.04513v1 by Shujian Zhang, Korawat Tanwisuth, Chengyue Gong, Pengcheng He, Mingyuan Zhou

Auto-regressive generation models achieve competitive performance across many
different NLP tasks such as summarization, question answering, and
classifications. However, they are also known for being slow in inference,
which makes them challenging to deploy in real-time applications. We propose a
switchable decision to accelerate inference by dynamically assigning
computation resources for each data instance. Automatically making decisions on
where to skip and how to balance quality and computation cost with constrained
optimization, our dynamic neural generation networks enforce the efficient
inference path and determine the optimized trade-off. Experiments across
question answering, summarization, and classification benchmarks show that our
method benefits from less computation cost during inference while keeping the
same accuracy. Extensive experiments and ablation studies demonstrate that our
method can be general, effective, and beneficial for many NLP tasks.

摘要：

##### **Toward In-Context Teaching: Adapting Examples to Students' Misconceptions**
2405.04495v1 by Alexis Ross, Jacob Andreas

When a teacher provides examples for a student to study, these examples must
be informative, enabling a student to progress from their current state toward
a target concept or skill. Good teachers must therefore simultaneously infer
what students already know and adapt their teaching to students' changing state
of knowledge. There is increasing interest in using computational models,
particularly large language models, as pedagogical tools. As students, language
models in particular have shown a remarkable ability to adapt to new tasks
given small numbers of examples. But how effectively can these models adapt as
teachers to students of different types? To study this question, we introduce a
suite of models and evaluation methods we call AdapT. AdapT has two components:
(1) a collection of simulated Bayesian student models that can be used for
evaluation of automated teaching methods; (2) a platform for evaluation with
human students, to characterize the real-world effectiveness of these methods.
We additionally introduce (3) AToM, a new probabilistic model for adaptive
teaching that jointly infers students' past beliefs and optimizes for the
correctness of future beliefs. In evaluations of simulated students across
three learning domains (fraction arithmetic, English morphology, function
learning), AToM systematically outperforms LLM-based and standard Bayesian
teaching models. In human experiments, both AToM and LLMs outperform
non-adaptive random example selection. Our results highlight both the
difficulty of the adaptive teaching task and the potential of learned adaptive
models for solving it.

摘要：

##### **TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous Driving with Reactive, Realistic, and Diverse Non-Playable Characters**
2405.04491v1 by Jonathan Wilder Lavington, Ke Zhang, Vasileios Lioutas, Matthew Niedoba, Yunpeng Liu, Dylan Green, Saeid Naderiparizi, Xiaoxuan Liang, Setareh Dabiri, Adam Ścibior, Berend Zwartsenberg, Frank Wood

The training, testing, and deployment, of autonomous vehicles requires
realistic and efficient simulators. Moreover, because of the high variability
between different problems presented in different autonomous systems, these
simulators need to be easy to use, and easy to modify. To address these
problems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv.
TorchDriveEnv is a lightweight reinforcement learning benchmark programmed
entirely in Python, which can be modified to test a number of different factors
in learned vehicle behavior, including the effect of varying kinematic models,
agent types, and traffic control patterns. Most importantly unlike many replay
based simulation approaches, TorchDriveEnv is fully integrated with a state of
the art behavioral simulation API. This allows users to train and evaluate
driving models alongside data driven Non-Playable Characters (NPC) whose
initializations and driving behavior are reactive, realistic, and diverse. We
illustrate the efficiency and simplicity of TorchDriveEnv by evaluating common
reinforcement learning baselines in both training and validation environments.
Our experiments show that TorchDriveEnv is easy to use, but difficult to solve.

摘要：

##### **Towards Continual Knowledge Graph Embedding via Incremental Distillation**
2405.04453v1 by Jiajun Liu, Wenjun Ke, Peng Wang, Ziyu Shang, Jinhua Gao, Guozheng Li, Ke Ji, Yanhe Liu

Traditional knowledge graph embedding (KGE) methods typically require
preserving the entire knowledge graph (KG) with significant training costs when
new knowledge emerges. To address this issue, the continual knowledge graph
embedding (CKGE) task has been proposed to train the KGE model by learning
emerging knowledge efficiently while simultaneously preserving decent old
knowledge. However, the explicit graph structure in KGs, which is critical for
the above goal, has been heavily ignored by existing CKGE methods. On the one
hand, existing methods usually learn new triples in a random order, destroying
the inner structure of new KGs. On the other hand, old triples are preserved
with equal priority, failing to alleviate catastrophic forgetting effectively.
In this paper, we propose a competitive method for CKGE based on incremental
distillation (IncDE), which considers the full use of the explicit graph
structure in KGs. First, to optimize the learning order, we introduce a
hierarchical strategy, ranking new triples for layer-by-layer learning. By
employing the inter- and intra-hierarchical orders together, new triples are
grouped into layers based on the graph structure features. Secondly, to
preserve the old knowledge effectively, we devise a novel incremental
distillation mechanism, which facilitates the seamless transfer of entity
representations from the previous layer to the next one, promoting old
knowledge preservation. Finally, we adopt a two-stage training paradigm to
avoid the over-corruption of old knowledge influenced by under-trained new
knowledge. Experimental results demonstrate the superiority of IncDE over
state-of-the-art baselines. Notably, the incremental distillation mechanism
contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR)
score.

摘要：

##### **POV Learning: Individual Alignment of Multimodal Models using Human Perception**
2405.04443v1 by Simon Werner, Katharina Christ, Laura Bernardy, Marion G. Müller, Achim Rettinger

Aligning machine learning systems with human expectations is mostly attempted
by training with manually vetted human behavioral samples, typically explicit
feedback. This is done on a population level since the context that is
capturing the subjective Point-Of-View (POV) of a concrete person in a specific
situational context is not retained in the data. However, we argue that
alignment on an individual level can boost the subjective predictive
performance for the individual user interacting with the system considerably.
Since perception differs for each person, the same situation is observed
differently. Consequently, the basis for decision making and the subsequent
reasoning processes and observable reactions differ. We hypothesize that
individual perception patterns can be used for improving the alignment on an
individual level. We test this, by integrating perception information into
machine learning systems and measuring their predictive performance
wrt.~individual subjective assessments. For our empirical study, we collect a
novel data set of multimodal stimuli and corresponding eye tracking sequences
for the novel task of Perception-Guided Crossmodal Entailment and tackle it
with our Perception-Guided Multimodal Transformer. Our findings suggest that
exploiting individual perception signals for the machine learning of subjective
human assessments provides a valuable cue for individual alignment. It does not
only improve the overall predictive performance from the point-of-view of the
individual user but might also contribute to steering AI systems towards every
person's individual expectations and values.

摘要：

##### **AugmenTory: A Fast and Flexible Polygon Augmentation Library**
2405.04442v1 by Tanaz Ghahremani, Mohammad Hoseyni, Mohammad Javad Ahmadi, Pouria Mehrabi, Amirhossein Nikoofard

Data augmentation is a key technique for addressing the challenge of limited
datasets, which have become a major component in the training procedures of
image processing. Techniques such as geometric transformations and color space
adjustments have been thoroughly tested for their ability to artificially
expand training datasets and generate semi-realistic data for training
purposes. Data augmentation is the most important key to addressing the
challenge of limited datasets, which have become a major component of image
processing training procedures. Data augmentation techniques, such as geometric
transformations and color space adjustments, are thoroughly tested for their
ability to artificially expand training datasets and generate semi-realistic
data for training purposes. Polygons play a crucial role in instance
segmentation and have seen a surge in use across advanced models, such as
YOLOv8. Despite their growing popularity, the lack of specialized libraries
hampers the polygon-augmentation process. This paper introduces a novel
solution to this challenge, embodied in the newly developed AugmenTory library.
Notably, AugmenTory offers reduced computational demands in both time and space
compared to existing methods. Additionally, the library includes a
postprocessing thresholding feature. The AugmenTory package is publicly
available on GitHub, where interested users can access the source code:
https://github.com/Smartory/AugmenTory

摘要：

##### **DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model**
2405.04434v1 by DeepSeek-AI

We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model
characterized by economical training and efficient inference. It comprises 236B
total parameters, of which 21B are activated for each token, and supports a
context length of 128K tokens. DeepSeek-V2 adopts innovative architectures
including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees
efficient inference through significantly compressing the Key-Value (KV) cache
into a latent vector, while DeepSeekMoE enables training strong models at an
economical cost through sparse computation. Compared with DeepSeek 67B,
DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves
42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum
generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality
and multi-source corpus consisting of 8.1T tokens, and further perform
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock
its potential. Evaluation results show that, even with only 21B activated
parameters, DeepSeek-V2 and its chat versions still achieve top-tier
performance among open-source models. The model checkpoints are available at
"https://github.com/deepseek-ai/DeepSeek-V2".

摘要：

##### **Vision Mamba: A Comprehensive Survey and Taxonomy**
2405.04404v1 by Xiao Liu, Chenxu Zhang, Lei Zhang

State Space Model (SSM) is a mathematical model used to describe and analyze
the behavior of dynamic systems. This model has witnessed numerous applications
in several fields, including control theory, signal processing, economics and
machine learning. In the field of deep learning, state space models are used to
process sequence data, such as time series analysis, natural language
processing (NLP) and video understanding. By mapping sequence data to state
space, long-term dependencies in the data can be better captured. In
particular, modern SSMs have shown strong representational capabilities in NLP,
especially in long sequence modeling, while maintaining linear time complexity.
Notably, based on the latest state-space models, Mamba merges time-varying
parameters into SSMs and formulates a hardware-aware algorithm for efficient
training and inference. Given its impressive efficiency and strong long-range
dependency modeling capability, Mamba is expected to become a new AI
architecture that may outperform Transformer. Recently, a number of works have
attempted to study the potential of Mamba in various fields, such as general
vision, multi-modal, medical image analysis and remote sensing image analysis,
by extending Mamba from natural language domain to visual domain. To fully
understand Mamba in the visual domain, we conduct a comprehensive survey and
present a taxonomy study. This survey focuses on Mamba's application to a
variety of visual tasks and data types, and discusses its predecessors, recent
advances and far-reaching impact on a wide range of domains. Since Mamba is now
on an upward trend, please actively notice us if you have new findings, and new
progress on Mamba will be included in this survey in a timely manner and
updated on the Mamba project at
https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy.

摘要：

##### **Pragmatist Intelligence: Where the Principle of Usefulness Can Take ANNs**
2405.04386v1 by Antonio Bikić, Sayan Mukherjee

Artificial neural networks (ANNs) perform extraordinarily on numerous tasks
including classification or prediction, e.g., speech processing and image
classification. These new functions are based on a computational model that is
enabled to select freely all necessary internal model parameters as long as it
eventually delivers the functionality it is supposed to exhibit. Here, we
review the connection between the model parameter selection in machine learning
(ML) algorithms running on ANNs and the epistemological theory of neopragmatism
focusing on the theory's utility and anti-representationalist aspects. To
understand the consequences of the model parameter selection of an ANN, we
suggest using neopragmatist theories whose implications are well studied.
Incidentally, neopragmatism's notion of optimization is also based on utility
considerations. This means that applying this approach elegantly reveals the
inherent connections between optimization in ML, using a numerical method
during the learning phase, and optimization in the ethical theory of
consequentialism, where it occurs as a maxim of action. We suggest that these
connections originate from the way relevance is calculated in ML systems. This
could ultimately reveal a tendency for specific actions in ML systems.

摘要：

##### **Leveraging LSTM and GAN for Modern Malware Detection**
2405.04373v1 by Ishita Gupta, Sneha Kumari, Priya Jha, Mohona Ghosh

The malware booming is a cyberspace equal to the effect of climate change to
ecosystems in terms of danger. In the case of significant investments in
cybersecurity technologies and staff training, the global community has become
locked up in the eternal war with cyber security threats. The multi-form and
changing faces of malware are continuously pushing the boundaries of the
cybersecurity practitioners employ various approaches like detection and
mitigate in coping with this issue. Some old mannerisms like signature-based
detection and behavioral analysis are slow to adapt to the speedy evolution of
malware types. Consequently, this paper proposes the utilization of the Deep
Learning Model, LSTM networks, and GANs to amplify malware detection accuracy
and speed. A fast-growing, state-of-the-art technology that leverages raw
bytestream-based data and deep learning architectures, the AI technology
provides better accuracy and performance than the traditional methods.
Integration of LSTM and GAN model is the technique that is used for the
synthetic generation of data, leading to the expansion of the training
datasets, and as a result, the detection accuracy is improved. The paper uses
the VirusShare dataset which has more than one million unique samples of the
malware as the training and evaluation set for the presented models. Through
thorough data preparation including tokenization, augmentation, as well as
model training, the LSTM and GAN models convey the better performance in the
tasks compared to straight classifiers. The research outcomes come out with 98%
accuracy that shows the efficiency of deep learning plays a decisive role in
proactive cybersecurity defense. Aside from that, the paper studies the output
of ensemble learning and model fusion methods as a way to reduce biases and
lift model complexity.

摘要：

##### **Explainable machine learning for predicting shellfish toxicity in the Adriatic Sea using long-term monitoring data of HABs**
2405.04372v1 by Martin Marzidovšek, Janja Francé, Vid Podpečan, Stanka Vadnjal, Jožica Dolenc, Patricija Mozetič

In this study, explainable machine learning techniques are applied to predict
the toxicity of mussels in the Gulf of Trieste (Adriatic Sea) caused by harmful
algal blooms. By analysing a newly created 28-year dataset containing records
of toxic phytoplankton in mussel farming areas and toxin concentrations in
mussels (Mytilus galloprovincialis), we train and evaluate the performance of
ML models to accurately predict diarrhetic shellfish poisoning (DSP) events.
The random forest model provided the best prediction of positive toxicity
results based on the F1 score. Explainability methods such as permutation
importance and SHAP identified key species (Dinophysis fortii and D. caudata)
and environmental factors (salinity, river discharge and precipitation) as the
best predictors of DSP outbreaks. These findings are important for improving
early warning systems and supporting sustainable aquaculture practices.

摘要：

##### **Global Scale Self-Supervised Channel Charting with Sensor Fusion**
2405.04357v1 by Omid Esrafilian, Mohsen Ahadi, Florian Kaltenberger, David Gesbert

The sensing and positioning capabilities foreseen in 6G have great potential
for technology advancements in various domains, such as future smart cities and
industrial use cases. Channel charting has emerged as a promising technology in
recent years for radio frequency-based sensing and localization. However, the
accuracy of these techniques is yet far behind the numbers envisioned in 6G. To
reduce this gap, in this paper, we propose a novel channel charting technique
capitalizing on the time of arrival measurements from surrounding Transmission
Reception Points (TRPs) along with their locations and leveraging sensor fusion
in channel charting by incorporating laser scanner data during the training
phase of our algorithm. The proposed algorithm remains self-supervised during
training and test phases, requiring no geometrical models or user position
ground truth. Simulation results validate the achievement of a sub-meter level
localization accuracy using our algorithm 90% of the time, outperforming the
state-of-the-art channel charting techniques and the traditional
triangulation-based approaches.

摘要：

##### **Revisiting character-level adversarial attacks**
2405.04346v1 by Elias Abad Rocamora, Yongtao Wu, Fanghui Liu, Grigorios G. Chrysos, Volkan Cevher

Adversarial attacks in Natural Language Processing apply perturbations in the
character or token levels. Token-level attacks, gaining prominence for their
use of gradient-based methods, are susceptible to altering sentence semantics,
leading to invalid adversarial examples. While character-level attacks easily
maintain semantics, they have received less attention as they cannot easily
adopt popular gradient-based methods, and are thought to be easy to defend.
Challenging these beliefs, we introduce Charmer, an efficient query-based
adversarial attack capable of achieving high attack success rate (ASR) while
generating highly similar adversarial examples. Our method successfully targets
both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2,
Charmer improves the ASR in 4.84% points and the USE similarity in 8% points
with respect to the previous art. Our implementation is available in
https://github.com/LIONS-EPFL/Charmer.

摘要：

##### **Temporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction**
2405.04336v1 by Zhihao Wen, Yuan Fang, Pengcheng Wei, Fayao Liu, Zhenghua Chen, Min Wu

Predicting Remaining Useful Life (RUL) plays a crucial role in the
prognostics and health management of industrial systems that involve a variety
of interrelated sensors. Given a constant stream of time series sensory data
from such systems, deep learning models have risen to prominence at identifying
complex, nonlinear temporal dependencies in these data. In addition to the
temporal dependencies of individual sensors, spatial dependencies emerge as
important correlations among these sensors, which can be naturally modelled by
a temporal graph that describes time-varying spatial relationships. However,
the majority of existing studies have relied on capturing discrete snapshots of
this temporal graph, a coarse-grained approach that leads to loss of temporal
information. Moreover, given the variety of heterogeneous sensors, it becomes
vital that such inherent heterogeneity is leveraged for RUL prediction in
temporal sensor graphs. To capture the nuances of the temporal and spatial
relationships and heterogeneous characteristics in an interconnected graph of
sensors, we introduce a novel model named Temporal and Heterogeneous Graph
Neural Networks (THGNN). Specifically, THGNN aggregates historical data from
neighboring nodes to accurately capture the temporal dynamics and spatial
correlations within the stream of sensor data in a fine-grained manner.
Moreover, the model leverages Feature-wise Linear Modulation (FiLM) to address
the diversity of sensor types, significantly improving the model's capacity to
learn the heterogeneity in the data sources. Finally, we have validated the
effectiveness of our approach through comprehensive experiments. Our empirical
findings demonstrate significant advancements on the N-CMAPSS dataset,
achieving improvements of up to 19.2% and 31.6% in terms of two different
evaluation metrics over state-of-the-art methods.

摘要：

##### **A Fourth Wave of Open Data? Exploring the Spectrum of Scenarios for Open Data and Generative AI**
2405.04333v1 by Hannah Chafetz, Sampriti Saxena, Stefaan G. Verhulst

Since late 2022, generative AI has taken the world by storm, with widespread
use of tools including ChatGPT, Gemini, and Claude. Generative AI and large
language model (LLM) applications are transforming how individuals find and
access data and knowledge. However, the intricate relationship between open
data and generative AI, and the vast potential it holds for driving innovation
in this field remain underexplored areas. This white paper seeks to unpack the
relationship between open data and generative AI and explore possible
components of a new Fourth Wave of Open Data: Is open data becoming AI ready?
Is open data moving towards a data commons approach? Is generative AI making
open data more conversational? Will generative AI improve open data quality and
provenance? Towards this end, we provide a new Spectrum of Scenarios framework.
This framework outlines a range of scenarios in which open data and generative
AI could intersect and what is required from a data quality and provenance
perspective to make open data ready for those specific scenarios. These
scenarios include: pertaining, adaptation, inference and insight generation,
data augmentation, and open-ended exploration. Through this process, we found
that in order for data holders to embrace generative AI to improve open data
access and develop greater insights from open data, they first must make
progress around five key areas: enhance transparency and documentation, uphold
quality and integrity, promote interoperability and standards, improve
accessibility and useability, and address ethical considerations.

摘要：

##### **Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation**
2405.04325v1 by Atharvan Dogra, Ameet Deshpande, John Nay, Tanmay Rajpurohit, Ashwin Kalyan, Balaraman Ravindran

Recent developments in large language models (LLMs), while offering a
powerful foundation for developing natural language agents, raise safety
concerns about them and the autonomous agents built upon them. Deception is one
potential capability of AI agents of particular concern, which we refer to as
an act or statement that misleads, hides the truth, or promotes a belief that
is not true in its entirety or in part. We move away from the conventional
understanding of deception through straight-out lying, making objective selfish
decisions, or giving false information, as seen in previous AI safety research.
We target a specific category of deception achieved through obfuscation and
equivocation. We broadly explain the two types of deception by analogizing them
with the rabbit-out-of-hat magic trick, where (i) the rabbit either comes out
of a hidden trap door or (ii) (our focus) the audience is completely distracted
to see the magician bring out the rabbit right in front of them using sleight
of hand or misdirection. Our novel testbed framework displays intrinsic
deception capabilities of LLM agents in a goal-driven environment when directed
to be deceptive in their natural language generations in a two-agent
adversarial dialogue system built upon the legislative task of "lobbying" for a
bill. Along the lines of a goal-driven environment, we show developing
deceptive capacity through a reinforcement learning setup, building it around
the theories of language philosophy and cognitive psychology. We find that the
lobbyist agent increases its deceptive capabilities by ~ 40% (relative) through
subsequent reinforcement trials of adversarial interactions, and our deception
detection mechanism shows a detection capability of up to 92%. Our results
highlight potential issues in agent-human interaction, with agents potentially
manipulating humans towards its programmed end-goal.

摘要：

##### **Granite Code Models: A Family of Open Foundation Models for Code Intelligence**
2405.04324v1 by Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, Manish Sethi, Xuan-Hong Dang, Pengyuan Li, Kun-Lung Wu, Syed Zawad, Andrew Coleman, Matthew White, Mark Lewis, Raju Pavuluri, Yan Koyfman, Boris Lublinsky, Maximilien de Bayser, Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Yi Zhou, Chris Johnson, Aanchal Goyal, Hima Patel, Yousaf Shah, Petros Zerfos, Heiko Ludwig, Asim Munawar, Maxwell Crouse, Pavan Kapanipathi, Shweta Salaria, Bob Calio, Sophia Wen, Seetharami Seelam, Brian Belgodere, Carlos Fonseca, Amith Singhee, Nirmit Desai, David D. Cox, Ruchir Puri, Rameswar Panda

Large Language Models (LLMs) trained on code are revolutionizing the software
development process. Increasingly, code LLMs are being integrated into software
development environments to improve the productivity of human programmers, and
LLM-based agents are beginning to show promise for handling complex tasks
autonomously. Realizing the full potential of code LLMs requires a wide range
of capabilities, including code generation, fixing bugs, explaining and
documenting code, maintaining repositories, and more. In this work, we
introduce the Granite series of decoder-only code models for code generative
tasks, trained with code written in 116 programming languages. The Granite Code
models family consists of models ranging in size from 3 to 34 billion
parameters, suitable for applications ranging from complex application
modernization tasks to on-device memory-constrained use cases. Evaluation on a
comprehensive set of tasks demonstrates that Granite Code models consistently
reaches state-of-the-art performance among available open-source code LLMs. The
Granite Code model family was optimized for enterprise software development
workflows and performs well across a range of coding tasks (e.g. code
generation, fixing and explanation), making it a versatile all around code
model. We release all our Granite Code models under an Apache 2.0 license for
both research and commercial use.

摘要：

##### **Beyond human subjectivity and error: a novel AI grading system**
2405.04323v1 by Alexandra Gobrecht, Felix Tuma, Moritz Möller, Thomas Zöller, Mark Zakhvatkin, Alexandra Wuttig, Holger Sommerfeldt, Sven Schütt

The grading of open-ended questions is a high-effort, high-impact task in
education. Automating this task promises a significant reduction in workload
for education professionals, as well as more consistent grading outcomes for
students, by circumventing human subjectivity and error. While recent
breakthroughs in AI technology might facilitate such automation, this has not
been demonstrated at scale. It this paper, we introduce a novel automatic short
answer grading (ASAG) system. The system is based on a fine-tuned open-source
transformer model which we trained on large set of exam data from university
courses across a large range of disciplines. We evaluated the trained model's
performance against held-out test data in a first experiment and found high
accuracy levels across a broad spectrum of unseen questions, even in unseen
courses. We further compared the performance of our model with that of
certified human domain experts in a second experiment: we first assembled
another test dataset from real historical exams - the historic grades contained
in that data were awarded to students in a regulated, legally binding
examination process; we therefore considered them as ground truth for our
experiment. We then asked certified human domain experts and our model to grade
the historic student answers again without disclosing the historic grades.
Finally, we compared the hence obtained grades with the historic grades (our
ground truth). We found that for the courses examined, the model deviated less
from the official historic grades than the human re-graders - the model's
median absolute error was 44 % smaller than the human re-graders', implying
that the model is more consistent than humans in grading. These results suggest
that leveraging AI enhanced grading can reduce human subjectivity, improve
consistency and thus ultimately increase fairness.

摘要：

##### **Cross-IQA: Unsupervised Learning for Image Quality Assessment**
2405.04311v1 by Zhen Zhang

Automatic perception of image quality is a challenging problem that impacts
billions of Internet and social media users daily. To advance research in this
field, we propose a no-reference image quality assessment (NR-IQA) method
termed Cross-IQA based on vision transformer(ViT) model. The proposed Cross-IQA
method can learn image quality features from unlabeled image data. We construct
the pretext task of synthesized image reconstruction to unsupervised extract
the image quality information based ViT block. The pretrained encoder of
Cross-IQA is used to fine-tune a linear regression model for score prediction.
Experimental results show that Cross-IQA can achieve state-of-the-art
performance in assessing the low-frequency degradation information (e.g., color
change, blurring, etc.) of images compared with the classical full-reference
IQA and NR-IQA under the same datasets.

摘要：

##### **Improving Offline Reinforcement Learning with Inaccurate Simulators**
2405.04307v1 by Yiwen Hou, Haoyuan Sun, Jinming Ma, Feng Wu

Offline reinforcement learning (RL) provides a promising approach to avoid
costly online interaction with the real environment. However, the performance
of offline RL highly depends on the quality of the datasets, which may cause
extrapolation error in the learning process. In many robotic applications, an
inaccurate simulator is often available. However, the data directly collected
from the inaccurate simulator cannot be directly used in offline RL due to the
well-known exploration-exploitation dilemma and the dynamic gap between
inaccurate simulation and the real environment. To address these issues, we
propose a novel approach to combine the offline dataset and the inaccurate
simulation data in a better manner. Specifically, we pre-train a generative
adversarial network (GAN) model to fit the state distribution of the offline
dataset. Given this, we collect data from the inaccurate simulator starting
from the distribution provided by the generator and reweight the simulated data
using the discriminator. Our experimental results in the D4RL benchmark and a
real-world manipulation task confirm that our method can benefit more from both
inaccurate simulator and limited offline datasets to achieve better performance
than the state-of-the-art methods.

摘要：

##### **A New Dataset and Comparative Study for Aphid Cluster Detection and Segmentation in Sorghum Fields**
2405.04305v1 by Raiyan Rahman, Christopher Indris, Goetz Bramesfeld, Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Ivan Grijalva, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang

Aphid infestations are one of the primary causes of extensive damage to wheat
and sorghum fields and are one of the most common vectors for plant viruses,
resulting in significant agricultural yield losses. To address this problem,
farmers often employ the inefficient use of harmful chemical pesticides that
have negative health and environmental impacts. As a result, a large amount of
pesticide is wasted on areas without significant pest infestation. This brings
to attention the urgent need for an intelligent autonomous system that can
locate and spray sufficiently large infestations selectively within the complex
crop canopies. We have developed a large multi-scale dataset for aphid cluster
detection and segmentation, collected from actual sorghum fields and
meticulously annotated to include clusters of aphids. Our dataset comprises a
total of 54,742 image patches, showcasing a variety of viewpoints, diverse
lighting conditions, and multiple scales, highlighting its effectiveness for
real-world applications. In this study, we trained and evaluated four real-time
semantic segmentation models and three object detection models specifically for
aphid cluster segmentation and detection. Considering the balance between
accuracy and efficiency, Fast-SCNN delivered the most effective segmentation
results, achieving 80.46% mean precision, 81.21% mean recall, and 91.66 frames
per second (FPS). For object detection, RT-DETR exhibited the best overall
performance with a 61.63% mean average precision (mAP), 92.6% mean recall, and
72.55 on an NVIDIA V100 GPU. Our experiments further indicate that aphid
cluster segmentation is more suitable for assessing aphid infestations than
using detection models.

摘要：

##### **Accelerating Speculative Decoding using Dynamic Speculation Length**
2405.04304v1 by Jonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy Schwartz

Speculative decoding is a promising method for reducing the inference latency
of large language models. The effectiveness of the method depends on the
speculation length (SL) - the number of tokens generated by the draft model at
each iteration. The vast majority of speculative decoding approaches use the
same SL for all iterations. In this work, we show that this practice is
suboptimal. We introduce DISCO, a DynamIc SpeCulation length Optimization
method that uses a classifier to dynamically adjust the SL at each iteration,
while provably preserving the decoding quality. Experiments with four
benchmarks demonstrate average speedup gains of 10.3% relative to our best
baselines.

摘要：

##### **Behaviour Planning: A Toolkit for Diverse Planning**
2405.04300v1 by Mustafa F Abdelwahed, Joan Espasa, Alice Toniolo, Ian P. Gent

Diverse planning is the problem of generating plans with distinct
characteristics. This is valuable for many real-world scenarios, including
applications related to plan recognition and business process automation. In
this work, we introduce \emph{Behaviour Planning}, a diverse planning toolkit
that can characterise and generate diverse plans based on modular diversity
models. We present a qualitative framework for describing diversity models, a
planning approach for generating plans aligned with any given diversity model,
and provide a practical implementation of an SMT-based behaviour planner. We
showcase how the qualitative approach offered by Behaviour Planning allows it
to overcome various challenges faced by previous approaches. Finally, the
experimental evaluation shows the effectiveness of Behaviour Planning in
generating diverse plans compared to state-of-the-art approaches.

摘要：

##### **Enhancing the Efficiency and Accuracy of Underlying Asset Reviews in Structured Finance: The Application of Multi-agent Framework**
2405.04294v1 by Xiangpeng Wan, Haicheng Deng, Kai Zou, Shiqi Xu

Structured finance, which involves restructuring diverse assets into
securities like MBS, ABS, and CDOs, enhances capital market efficiency but
presents significant due diligence challenges. This study explores the
integration of artificial intelligence (AI) with traditional asset review
processes to improve efficiency and accuracy in structured finance. Using both
open-sourced and close-sourced large language models (LLMs), we demonstrate
that AI can automate the verification of information between loan applications
and bank statements effectively. While close-sourced models such as GPT-4 show
superior performance, open-sourced models like LLAMA3 offer a cost-effective
alternative. Dual-agent systems further increase accuracy, though this comes
with higher operational costs. This research highlights AI's potential to
minimize manual errors and streamline due diligence, suggesting a broader
application of AI in financial document analysis and risk management.

摘要：

##### **Mitigating Clickbait: An Approach to Spoiler Generation Using Multitask Learning**
2405.04292v1 by Sayantan Pal, Souvik Das, Rohini K. Srihari

This study introduces 'clickbait spoiling', a novel technique designed to
detect, categorize, and generate spoilers as succinct text responses,
countering the curiosity induced by clickbait content. By leveraging a
multi-task learning framework, our model's generalization capabilities are
significantly enhanced, effectively addressing the pervasive issue of
clickbait. The crux of our research lies in generating appropriate spoilers, be
it a phrase, an extended passage, or multiple, depending on the spoiler type
required. Our methodology integrates two crucial techniques: a refined spoiler
categorization method and a modified version of the Question Answering (QA)
mechanism, incorporated within a multi-task learning paradigm for optimized
spoiler extraction from context. Notably, we have included fine-tuning methods
for models capable of handling longer sequences to accommodate the generation
of extended spoilers. This research highlights the potential of sophisticated
text processing techniques in tackling the omnipresent issue of clickbait,
promising an enhanced user experience in the digital realm.

摘要：

##### **Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore**
2405.04286v1 by Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xuebo Liu, Lidia S. Chao, Min Zhang

The efficacy of an large language model (LLM) generated text detector depends
substantially on the availability of sizable training data. White-box zero-shot
detectors, which require no such data, are nonetheless limited by the
accessibility of the source model of the LLM-generated text. In this paper, we
propose an simple but effective black-box zero-shot detection approach,
predicated on the observation that human-written texts typically contain more
grammatical errors than LLM-generated texts. This approach entails computing
the Grammar Error Correction Score (GECScore) for the given text to distinguish
between human-written and LLM-generated text. Extensive experimental results
show that our method outperforms current state-of-the-art (SOTA) zero-shot and
supervised methods, achieving an average AUROC of 98.7% and showing strong
robustness against paraphrase and adversarial perturbation attacks.

摘要：

##### **On the Foundations of Earth and Climate Foundation Models**
2405.04285v1 by Xiao Xiang Zhu, Zhitong Xiong, Yi Wang, Adam J. Stewart, Konrad Heidler, Yuanyuan Wang, Zhenghang Yuan, Thomas Dujardin, Qingsong Xu, Yilei Shi

Foundation models have enormous potential in advancing Earth and climate
sciences, however, current approaches may not be optimal as they focus on a few
basic features of a desirable Earth and climate foundation model. Crafting the
ideal Earth foundation model, we define eleven features which would allow such
a foundation model to be beneficial for any geoscientific downstream
application in an environmental- and human-centric manner.We further shed light
on the way forward to achieve the ideal model and to evaluate Earth foundation
models. What comes after foundation models? Energy efficient adaptation,
adversarial defenses, and interpretability are among the emerging directions.

摘要：

##### **Generating Feature Vectors from Phonetic Transcriptions in Cross-Linguistic Data Formats**
2405.04271v1 by Arne Rubehn, Jessica Nieder, Robert Forkel, Johann-Mattis List

When comparing speech sounds across languages, scholars often make use of
feature representations of individual sounds in order to determine fine-grained
sound similarities. Although binary feature systems for large numbers of speech
sounds have been proposed, large-scale computational applications often face
the challenges that the proposed feature systems -- even if they list features
for several thousand sounds -- only cover a smaller part of the numerous speech
sounds reflected in actual cross-linguistic data. In order to address the
problem of missing data for attested speech sounds, we propose a new approach
that can create binary feature vectors dynamically for all sounds that can be
represented in the the standardized version of the International Phonetic
Alphabet proposed by the Cross-Linguistic Transcription Systems (CLTS)
reference catalog. Since CLTS is actively used in large data collections,
covering more than 2,000 distinct language varieties, our procedure for the
generation of binary feature vectors provides immediate access to a very large
collection of multilingual wordlists. Testing our feature system in different
ways on different datasets proves that the system is not only useful to provide
a straightforward means to compare the similarity of speech sounds, but also
illustrates its potential to be used in future cross-linguistic machine
learning applications.

摘要：

##### **VAEneu: A New Avenue for VAE Application on Probabilistic Forecasting**
2405.04252v1 by Alireza Koochali, Ensiye Tahaei, Andreas Dengel, Sheraz Ahmed

This paper presents VAEneu, an innovative autoregressive method for multistep
ahead univariate probabilistic time series forecasting. We employ the
conditional VAE framework and optimize the lower bound of the predictive
distribution likelihood function by adopting the Continuous Ranked Probability
Score (CRPS), a strictly proper scoring rule, as the loss function. This novel
pipeline results in forecasting sharp and well-calibrated predictive
distribution. Through a comprehensive empirical study, VAEneu is rigorously
benchmarked against 12 baseline models across 12 datasets. The results
unequivocally demonstrate VAEneu's remarkable forecasting performance. VAEneu
provides a valuable tool for quantifying future uncertainties, and our
extensive empirical study lays the foundation for future comparative studies
for univariate multistep ahead probabilistic forecasting.

摘要：

##### **Federated Learning for Cooperative Inference Systems: The Case of Early Exit Networks**
2405.04249v1 by Caelin Kaplan, Tareq Si Salem, Angelo Rodio, Chuan Xu, Giovanni Neglia

As Internet of Things (IoT) technology advances, end devices like sensors and
smartphones are progressively equipped with AI models tailored to their local
memory and computational constraints. Local inference reduces communication
costs and latency; however, these smaller models typically underperform
compared to more sophisticated models deployed on edge servers or in the cloud.
Cooperative Inference Systems (CISs) address this performance trade-off by
enabling smaller devices to offload part of their inference tasks to more
capable devices. These systems often deploy hierarchical models that share
numerous parameters, exemplified by Deep Neural Networks (DNNs) that utilize
strategies like early exits or ordered dropout. In such instances, Federated
Learning (FL) may be employed to jointly train the models within a CIS. Yet,
traditional training methods have overlooked the operational dynamics of CISs
during inference, particularly the potential high heterogeneity in serving
rates across clients. To address this gap, we propose a novel FL approach
designed explicitly for use in CISs that accounts for these variations in
serving rates. Our framework not only offers rigorous theoretical guarantees,
but also surpasses state-of-the-art (SOTA) training algorithms for CISs,
especially in scenarios where inference request rates or data availability are
uneven among clients.

摘要：

##### **Exploring Correlations of Self-supervised Tasks for Graphs**
2405.04245v1 by Taoran Fang, Wei Zhou, Yifei Sun, Kaiqiao Han, Lvbin Ma, Yang Yang

Graph self-supervised learning has sparked a research surge in training
informative representations without accessing any labeled data. However, our
understanding of graph self-supervised learning remains limited, and the
inherent relationships between various self-supervised tasks are still
unexplored. Our paper aims to provide a fresh understanding of graph
self-supervised learning based on task correlations. Specifically, we evaluate
the performance of the representations trained by one specific task on other
tasks and define correlation values to quantify task correlations. Through this
process, we unveil the task correlations between various self-supervised tasks
and can measure their expressive capabilities, which are closely related to
downstream performance. By analyzing the correlation values between tasks
across various datasets, we reveal the complexity of task correlations and the
limitations of existing multi-task learning methods. To obtain more capable
representations, we propose Graph Task Correlation Modeling (GraphTCM) to
illustrate the task correlations and utilize it to enhance graph
self-supervised training. The experimental results indicate that our method
significantly outperforms existing methods across various downstream tasks.

摘要：

##### **Iterative Experience Refinement of Software-Developing Agents**
2405.04219v1 by Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, YiFei Wang, Zihao Xie, Weize Chen, Cheng Yang, Yingli Zhang, Zhiyuan Liu, Maosong Sun

Autonomous agents powered by large language models (LLMs) show significant
potential for achieving high autonomy in various scenarios such as software
development. Recent research has shown that LLM agents can leverage past
experiences to reduce errors and enhance efficiency. However, the static
experience paradigm, reliant on a fixed collection of past experiences acquired
heuristically, lacks iterative refinement and thus hampers agents'
adaptability. In this paper, we introduce the Iterative Experience Refinement
framework, enabling LLM agents to refine experiences iteratively during task
execution. We propose two fundamental patterns: the successive pattern,
refining based on nearest experiences within a task batch, and the cumulative
pattern, acquiring experiences across all previous task batches. Augmented with
our heuristic experience elimination, the method prioritizes high-quality and
frequently-used experiences, effectively managing the experience space and
enhancing efficiency. Extensive experiments show that while the successive
pattern may yield superior results, the cumulative pattern provides more stable
performance. Moreover, experience elimination facilitates achieving better
performance using just 11.54% of a high-quality subset.

摘要：

##### **NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions**
2405.04215v1 by Elliot Gestrin, Marco Kuhlmann, Jendrik Seipp

Today's classical planners are powerful, but modeling input tasks in formats
such as PDDL is tedious and error-prone. In contrast, planning with Large
Language Models (LLMs) allows for almost any input text, but offers no
guarantees on plan quality or even soundness. In an attempt to merge the best
of these two approaches, some work has begun to use LLMs to automate parts of
the PDDL creation process. However, these methods still require various degrees
of expert input. We present NL2Plan, the first domain-agnostic offline
LLM-driven planning system. NL2Plan uses an LLM to incrementally extract the
necessary information from a short text prompt before creating a complete PDDL
description of both the domain and the problem, which is finally solved by a
classical planner. We evaluate NL2Plan on four planning domains and find that
it solves 10 out of 15 tasks - a clear improvement over a plain
chain-of-thought reasoning LLM approach, which only solves 2 tasks. Moreover,
in two out of the five failure cases, instead of returning an invalid plan,
NL2Plan reports that it failed to solve the task. In addition to using NL2Plan
in end-to-end mode, users can inspect and correct all of its intermediate
results, such as the PDDL representation, increasing explainability and making
it an assistive tool for PDDL creation.

摘要：

##### **Green Tsetlin Redefining Efficiency in Tsetlin Machine Frameworks**
2405.04212v1 by Sondre Glimsdal, Sebastian Østby, Tobias M. Brambo, Eirik M. Vinje

Green Tsetlin (GT) is a Tsetlin Machine (TM) framework developed to solve
real-world problems using TMs. Several frameworks already exist that provide
access to TM implementations. However, these either lack features or have a
research-first focus. GT is an easy-to-use framework that aims to lower the
complexity and provide a production-ready TM implementation that is great for
experienced practitioners and beginners. To this end, GT establishes a clear
separation between training and inference. A C++ backend with a Python
interface provides competitive training and inference performance, with the
option of running in pure Python. It also integrates support for critical
components such as exporting trained models, hyper-parameter search, and
cross-validation out-of-the-box.

摘要：

##### **NOVA: NoC-based Vector Unit for Mapping Attention Layers on a CNN Accelerator**
2405.04206v1 by Mohit Upadhyay, Rohan Juneja, Weng-Fai Wong, Li-Shiuan Peh

Attention mechanisms are becoming increasingly popular, being used in neural
network models in multiple domains such as natural language processing (NLP)
and vision applications, especially at the edge. However, attention layers are
difficult to map onto existing neuro accelerators since they have a much higher
density of non-linear operations, which lead to inefficient utilization of
today's vector units. This work introduces NOVA, a NoC-based Vector Unit that
can perform non-linear operations within the NoC of the accelerators, and can
be overlaid onto existing neuro accelerators to map attention layers at the
edge. Our results show that the NOVA architecture is up to 37.8x more
power-efficient than state-of-the-art hardware approximators when running
existing attention-based neural networks.

摘要：

##### **FedStale: leveraging stale client updates in federated learning**
2405.04171v1 by Angelo Rodio, Giovanni Neglia

Federated learning algorithms, such as FedAvg, are negatively affected by
data heterogeneity and partial client participation. To mitigate the latter
problem, global variance reduction methods, like FedVARP, leverage stale model
updates for non-participating clients. These methods are effective under
homogeneous client participation. Yet, this paper shows that, when some clients
participate much less than others, aggregating updates with different levels of
staleness can detrimentally affect the training process. Motivated by this
observation, we introduce FedStale, a novel algorithm that updates the global
model in each round through a convex combination of "fresh" updates from
participating clients and "stale" updates from non-participating ones. By
adjusting the weight in the convex combination, FedStale interpolates between
FedAvg, which only uses fresh updates, and FedVARP, which treats fresh and
stale updates equally. Our analysis of FedStale convergence yields the
following novel findings: i) it integrates and extends previous FedAvg and
FedVARP analyses to heterogeneous client participation; ii) it underscores how
the least participating client influences convergence error; iii) it provides
practical guidelines to best exploit stale updates, showing that their
usefulness diminishes as data heterogeneity decreases and participation
heterogeneity increases. Extensive experiments featuring diverse levels of
client data and participation heterogeneity not only confirm these findings but
also show that FedStale outperforms both FedAvg and FedVARP in many settings.

摘要：

##### **D-NLP at SemEval-2024 Task 2: Evaluating Clinical Inference Capabilities of Large Language Models**
2405.04170v1 by Duygu Altinok

Large language models (LLMs) have garnered significant attention and
widespread usage due to their impressive performance in various tasks. However,
they are not without their own set of challenges, including issues such as
hallucinations, factual inconsistencies, and limitations in
numerical-quantitative reasoning. Evaluating LLMs in miscellaneous reasoning
tasks remains an active area of research. Prior to the breakthrough of LLMs,
Transformers had already proven successful in the medical domain, effectively
employed for various natural language understanding (NLU) tasks. Following this
trend, LLMs have also been trained and utilized in the medical domain, raising
concerns regarding factual accuracy, adherence to safety protocols, and
inherent limitations. In this paper, we focus on evaluating the natural
language inference capabilities of popular open-source and closed-source LLMs
using clinical trial reports as the dataset. We present the performance results
of each LLM and further analyze their performance on a development set,
particularly focusing on challenging instances that involve medical
abbreviations and require numerical-quantitative reasoning. Gemini, our leading
LLM, achieved a test set F1-score of 0.748, securing the ninth position on the
task scoreboard. Our work is the first of its kind, offering a thorough
examination of the inference capabilities of LLMs within the medical domain.

摘要：

##### **LingML: Linguistic-Informed Machine Learning for Enhanced Fake News Detection**
2405.04165v1 by Jasraj Singh, Fang Liu, Hong Xu, Bee Chin Ng, Wei Zhang

Nowadays, Information spreads at an unprecedented pace in social media and
discerning truth from misinformation and fake news has become an acute societal
challenge. Machine learning (ML) models have been employed to identify fake
news but are far from perfect with challenging problems like limited accuracy,
interpretability, and generalizability. In this paper, we enhance ML-based
solutions with linguistics input and we propose LingML, linguistic-informed ML,
for fake news detection. We conducted an experimental study with a popular
dataset on fake news during the pandemic. The experiment results show that our
proposed solution is highly effective. There are fewer than two errors out of
every ten attempts with only linguistic input used in ML and the knowledge is
highly explainable. When linguistics input is integrated with advanced
large-scale ML models for natural language processing, our solution outperforms
existing ones with 1.8% average error rate. LingML creates a new path with
linguistics to push the frontier of effective and efficient fake news
detection. It also sheds light on real-world multi-disciplinary applications
requiring both ML and domain expertise to achieve optimal performance.

摘要：

##### **MEDVOC: Vocabulary Adaptation for Fine-tuning Pre-trained Language Models on Medical Text Summarization**
2405.04163v1 by Gunjan Balde, Soumyadeep Roy, Mainack Mondal, Niloy Ganguly

This work presents a dynamic vocabulary adaptation strategy, MEDVOC, for
fine-tuning pre-trained language models (PLMs) like BertSumAbs, BART, and
PEGASUS for improved medical text summarization. In contrast to existing domain
adaptation approaches in summarization, MEDVOC treats vocabulary as an
optimizable parameter and optimizes the PLM vocabulary based on fragment score
conditioned only on the downstream task's reference summaries. Unlike previous
works on vocabulary adaptation (limited only to classification tasks),
optimizing vocabulary based on summarization tasks requires an extremely costly
intermediate fine-tuning step on large summarization datasets. To that end, our
novel fragment score-based hyperparameter search very significantly reduces
this fine-tuning time -- from 450 days to less than 2 days on average.
Furthermore, while previous works on vocabulary adaptation are often primarily
tied to single PLMs, MEDVOC is designed to be deployable across multiple PLMs
(with varying model vocabulary sizes, pre-training objectives, and model sizes)
-- bridging the limited vocabulary overlap between the biomedical literature
domain and PLMs. MEDVOC outperforms baselines by 15.74% in terms of Rouge-L in
zero-shot setting and shows gains of 17.29% in high Out-Of-Vocabulary (OOV)
concentrations. Our human evaluation shows MEDVOC generates more faithful
medical summaries (88% compared to 59% in baselines). We make the codebase
publicly available at https://github.com/gb-kgp/MEDVOC.

摘要：

##### **A Causal Explainable Guardrails for Large Language Models**
2405.04160v1 by Zhixuan Chu, Yan Wang, Longfei Li, Zhibo Wang, Zhan Qin, Kui Ren

Large Language Models (LLMs) have shown impressive performance in natural
language tasks, but their outputs can exhibit undesirable attributes or biases.
Existing methods for steering LLMs towards desired attributes often assume
unbiased representations and rely solely on steering prompts. However, the
representations learned from pre-training can introduce semantic biases that
influence the steering process, leading to suboptimal results. We propose
LLMGuardaril, a novel framework that incorporates causal analysis and
adversarial learning to obtain unbiased steering representations in LLMs.
LLMGuardaril systematically identifies and blocks the confounding effects of
biases, enabling the extraction of unbiased steering representations.
Additionally, it includes an explainable component that provides insights into
the alignment between the generated output and the desired direction.
Experiments demonstrate LLMGuardaril's effectiveness in steering LLMs towards
desired attributes while mitigating biases. Our work contributes to the
development of safe and reliable LLMs that align with desired attributes. We
discuss the limitations and future research directions, highlighting the need
for ongoing research to address the ethical implications of large language
models.

摘要：

##### **GPT-Enabled Cybersecurity Training: A Tailored Approach for Effective Awareness**
2405.04138v1 by Nabil Al-Dhamari, Nathan Clarke

This study explores the limitations of traditional Cybersecurity Awareness
and Training (CSAT) programs and proposes an innovative solution using
Generative Pre-Trained Transformers (GPT) to address these shortcomings.
Traditional approaches lack personalization and adaptability to individual
learning styles. To overcome these challenges, the study integrates GPT models
to deliver highly tailored and dynamic cybersecurity learning expe-riences.
Leveraging natural language processing capabilities, the proposed approach
personalizes training modules based on individual trainee pro-files, helping to
ensure engagement and effectiveness. An experiment using a GPT model to provide
a real-time and adaptive CSAT experience through generating customized training
content. The findings have demonstrated a significant improvement over
traditional programs, addressing issues of en-gagement, dynamicity, and
relevance. GPT-powered CSAT programs offer a scalable and effective solution to
enhance cybersecurity awareness, provid-ing personalized training content that
better prepares individuals to miti-gate cybersecurity risks in their specific
roles within the organization.

摘要：

##### **Enriched BERT Embeddings for Scholarly Publication Classification**
2405.04136v1 by Benjamin Wolff, Eva Seidlmayer, Konrad U. Förstner

With the rapid expansion of academic literature and the proliferation of
preprints, researchers face growing challenges in manually organizing and
labeling large volumes of articles. The NSLP 2024 FoRC Shared Task I addresses
this challenge organized as a competition. The goal is to develop a classifier
capable of predicting one of 123 predefined classes from the Open Research
Knowledge Graph (ORKG) taxonomy of research fields for a given article.This
paper presents our results. Initially, we enrich the dataset (containing
English scholarly articles sourced from ORKG and arXiv), then leverage
different pre-trained language Models (PLMs), specifically BERT, and explore
their efficacy in transfer learning for this downstream task. Our experiments
encompass feature-based and fine-tuned transfer learning approaches using
diverse PLMs, optimized for scientific tasks, including SciBERT, SciNCL, and
SPECTER2. We conduct hyperparameter tuning and investigate the impact of data
augmentation from bibliographic databases such as OpenAlex, Semantic Scholar,
and Crossref. Our results demonstrate that fine-tuning pre-trained models
substantially enhances classification performance, with SPECTER2 emerging as
the most accurate model. Moreover, enriching the dataset with additional
metadata improves classification outcomes significantly, especially when
integrating information from S2AG, OpenAlex and Crossref. Our best-performing
approach achieves a weighted F1-score of 0.7415. Overall, our study contributes
to the advancement of reliable automated systems for scholarly publication
categorization, offering a potential solution to the laborious manual curation
process, thereby facilitating researchers in efficiently locating relevant
resources.

摘要：

##### **In-context Learning for Automated Driving Scenarios**
2405.04135v1 by Ziqi Zhou, Jingyue Zhang, Jingyuan Zhang, Boyue Wang, Tianyu Shi, Alaa Khamis

One of the key challenges in current Reinforcement Learning (RL)-based
Automated Driving (AD) agents is achieving flexible, precise, and human-like
behavior cost-effectively. This paper introduces an innovative approach
utilizing Large Language Models (LLMs) to intuitively and effectively optimize
RL reward functions in a human-centric way. We developed a framework where
instructions and dynamic environment descriptions are input into the LLM. The
LLM then utilizes this information to assist in generating rewards, thereby
steering the behavior of RL agents towards patterns that more closely resemble
human driving. The experimental results demonstrate that this approach not only
makes RL agents more anthropomorphic but also reaches better performance.
Additionally, various strategies for reward-proxy and reward-shaping are
investigated, revealing the significant impact of prompt design on shaping an
AD vehicle's behavior. These findings offer a promising direction for the
development of more advanced and human-like automated driving systems. Our
experimental data and source code can be found here.

摘要：

##### **Fine-grained Speech Sentiment Analysis in Chinese Psychological Support Hotlines Based on Large-scale Pre-trained Model**
2405.04128v1 by Zhonglong Chen, Changwei Song, Yining Chen, Jianqiang Li, Guanghui Fu, Yongsheng Tong, Qing Zhao

Suicide and suicidal behaviors remain significant challenges for public
policy and healthcare. In response, psychological support hotlines have been
established worldwide to provide immediate help to individuals in mental
crises. The effectiveness of these hotlines largely depends on accurately
identifying callers' emotional states, particularly underlying negative
emotions indicative of increased suicide risk. However, the high demand for
psychological interventions often results in a shortage of professional
operators, highlighting the need for an effective speech emotion recognition
model. This model would automatically detect and analyze callers' emotions,
facilitating integration into hotline services. Additionally, it would enable
large-scale data analysis of psychological support hotline interactions to
explore psychological phenomena and behaviors across populations. Our study
utilizes data from the Beijing psychological support hotline, the largest
suicide hotline in China. We analyzed speech data from 105 callers containing
20,630 segments and categorized them into 11 types of negative emotions. We
developed a negative emotion recognition model and a fine-grained multi-label
classification model using a large-scale pre-trained model. Our experiments
indicate that the negative emotion recognition model achieves a maximum
F1-score of 76.96%. However, it shows limited efficacy in the fine-grained
multi-label classification task, with the best model achieving only a 41.74%
weighted F1-score. We conducted an error analysis for this task, discussed
potential future improvements, and considered the clinical application
possibilities of our study. All the codes are public available.

摘要：

##### **Comparative Study of Recurrent Neural Networks for Virtual Analog Audio Effects Modeling**
2405.04124v1 by Riccardo Simionato, Stefano Fasciani

Analog electronic circuits are at the core of an important category of
musical devices. The nonlinear features of their electronic components give
analog musical devices a distinctive timbre and sound quality, making them
highly desirable. Artificial neural networks have rapidly gained popularity for
the emulation of analog audio effects circuits, particularly recurrent
networks. While neural approaches have been successful in accurately modeling
distortion circuits, they require architectural improvements that account for
parameter conditioning and low latency response. In this article, we explore
the application of recent machine learning advancements for virtual analog
modeling. We compare State Space models and Linear Recurrent Units against the
more common Long Short Term Memory networks. These have shown promising ability
in sequence to sequence modeling tasks, showing a notable improvement in signal
history encoding. Our comparative study uses these black box neural modeling
techniques with a variety of audio effects. We evaluate the performance and
limitations using multiple metrics aiming to assess the models' ability to
accurately replicate energy envelopes, frequency contents, and transients in
the audio signal. To incorporate control parameters we employ the Feature wise
Linear Modulation method. Long Short Term Memory networks exhibit better
accuracy in emulating distortions and equalizers, while the State Space model,
followed by Long Short Term Memory networks when integrated in an encoder
decoder structure, outperforms others in emulating saturation and compression.
When considering long time variant characteristics, the State Space model
demonstrates the greatest accuracy. The Long Short Term Memory and, in
particular, Linear Recurrent Unit networks present more tendency to introduce
audio artifacts.

摘要：

##### **Policy Learning with a Language Bottleneck**
2405.04118v1 by Megha Srivastava, Cedric Colas, Dorsa Sadigh, Jacob Andreas

Modern AI systems such as self-driving cars and game-playing agents achieve
superhuman performance, but often lack human-like features such as
generalization, interpretability and human inter-operability. Inspired by the
rich interactions between language and decision-making in humans, we introduce
Policy Learning with a Language Bottleneck (PLLB), a framework enabling AI
agents to generate linguistic rules that capture the strategies underlying
their most rewarding behaviors. PLLB alternates between a rule generation step
guided by language models, and an update step where agents learn new policies
guided by rules. In a two-player communication game, a maze solving task, and
two image reconstruction tasks, we show that PLLB agents are not only able to
learn more interpretable and generalizable behaviors, but can also share the
learned rules with human users, enabling more effective human-AI coordination.

摘要：

##### **A2-DIDM: Privacy-preserving Accumulator-enabled Auditing for Distributed Identity of DNN Model**
2405.04108v1 by Tianxiu Xie, Keke Gai, Jing Yu, Liehuang Zhu, Kim-Kwang Raymond Choo

Recent booming development of Generative Artificial Intelligence (GenAI) has
facilitated an emerging model commercialization for the purpose of
reinforcement on model performance, such as licensing or trading Deep Neural
Network (DNN) models. However, DNN model trading may trigger concerns of the
unauthorized replications or misuses over the model, so that the benefit of the
model ownership will be violated. Model identity auditing is a challenging
issue in protecting intellectual property of DNN models and verifying the
integrity and ownership of models for guaranteeing trusts in transactions is
one of the critical obstacles. In this paper, we focus on the above issue and
propose a novel Accumulator-enabled Auditing for Distributed Identity of DNN
Model (A2-DIDM) that utilizes blockchain and zero-knowledge techniques to
protect data and function privacy while ensuring the lightweight on-chain
ownership verification. The proposed model presents a scheme of identity
records via configuring model weight checkpoints with corresponding
zero-knowledge proofs, which incorporates predicates to capture incremental
state changes in model weight checkpoints. Our scheme ensures both
computational integrity of DNN training process and programmability, so that
the uniqueness of the weight checkpoint sequence in a DNN model is preserved,
ensuring the correctness of the model identity auditing. In addition, A2-DIDM
also addresses privacy protections in distributed identity via a proposed
method of accumulators. We systematically analyze the security and robustness
of our proposed model and further evaluate the effectiveness and usability of
auditing DNN model identities.

摘要：

##### **Continual Learning in the Presence of Repetition**
2405.04101v1 by Hamed Hemati, Lorenzo Pellegrini, Xiaotian Duan, Zixuan Zhao, Fangfang Xia, Marc Masana, Benedikt Tscheschner, Eduardo Veas, Yuxiang Zheng, Shiji Zhao, Shao-Yuan Li, Sheng-Jun Huang, Vincenzo Lomonaco, Gido M. van de Ven

Continual learning (CL) provides a framework for training models in
ever-evolving environments. Although re-occurrence of previously seen objects
or tasks is common in real-world problems, the concept of repetition in the
data stream is not often considered in standard benchmarks for CL. Unlike with
the rehearsal mechanism in buffer-based strategies, where sample repetition is
controlled by the strategy, repetition in the data stream naturally stems from
the environment. This report provides a summary of the CLVision challenge at
CVPR 2023, which focused on the topic of repetition in class-incremental
learning. The report initially outlines the challenge objective and then
describes three solutions proposed by finalist teams that aim to effectively
exploit the repetition in the stream to learn continually. The experimental
results from the challenge highlight the effectiveness of ensemble-based
solutions that employ multiple versions of similar modules, each trained on
different but overlapping subsets of classes. This report underscores the
transformative potential of taking a different perspective in CL by employing
repetition in the data stream to foster innovative strategy design.

摘要：

##### **Unmasking Illusions: Understanding Human Perception of Audiovisual Deepfakes**
2405.04097v1 by Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao, Hsin-Min Wang

The emergence of contemporary deepfakes has attracted significant attention
in machine learning research, as artificial intelligence (AI) generated
synthetic media increases the incidence of misinterpretation and is difficult
to distinguish from genuine content. Currently, machine learning techniques
have been extensively studied for automatically detecting deepfakes. However,
human perception has been less explored. Malicious deepfakes could ultimately
cause public and social problems. Can we humans correctly perceive the
authenticity of the content of the videos we watch? The answer is obviously
uncertain; therefore, this paper aims to evaluate the human ability to discern
deepfake videos through a subjective study. We present our findings by
comparing human observers to five state-ofthe-art audiovisual deepfake
detection models. To this end, we used gamification concepts to provide 110
participants (55 native English speakers and 55 non-native English speakers)
with a webbased platform where they could access a series of 40 videos (20 real
and 20 fake) to determine their authenticity. Each participant performed the
experiment twice with the same 40 videos in different random orders. The videos
are manually selected from the FakeAVCeleb dataset. We found that all AI models
performed better than humans when evaluated on the same 40 videos. The study
also reveals that while deception is not impossible, humans tend to
overestimate their detection capabilities. Our experimental results may help
benchmark human versus machine performance, advance forensics analysis, and
enable adaptive countermeasures.

摘要：

##### **Going Proactive and Explanatory Against Malware Concept Drift**
2405.04095v1 by Yiling He, Junchi Lei, Zhan Qin, Kui Ren

Deep learning-based malware classifiers face significant challenges due to
concept drift. The rapid evolution of malware, especially with new families,
can depress classification accuracy to near-random levels. Previous research
has primarily focused on detecting drift samples, relying on expert-led
analysis and labeling for model retraining. However, these methods often lack a
comprehensive understanding of malware concepts and provide limited guidance
for effective drift adaptation, leading to unstable detection performance and
high human labeling costs.
  To address these limitations, we introduce DREAM, a novel system designed to
surpass the capabilities of existing drift detectors and to establish an
explanatory drift adaptation process. DREAM enhances drift detection through
model sensitivity and data autonomy. The detector, trained in a semi-supervised
approach, proactively captures malware behavior concepts through classifier
feedback. During testing, it utilizes samples generated by the detector itself,
eliminating reliance on extensive training data. For drift adaptation, DREAM
enlarges human intervention, enabling revisions of malware labels and concept
explanations embedded within the detector's latent space. To ensure a
comprehensive response to concept drift, it facilitates a coordinated update
process for both the classifier and the detector. Our evaluation shows that
DREAM can effectively improve the drift detection accuracy and reduce the
expert analysis effort in adaptation across different malware datasets and
classifiers.

摘要：

##### **DCNN: Dual Cross-current Neural Networks Realized Using An Interactive Deep Learning Discriminator for Fine-grained Objects**
2405.04093v1 by Da Fu, Mingfei Rong, Eun-Hu Kim, Hao Huang, Witold Pedrycz

Accurate classification of fine-grained images remains a challenge in
backbones based on convolutional operations or self-attention mechanisms. This
study proposes novel dual-current neural networks (DCNN), which combine the
advantages of convolutional operations and self-attention mechanisms to improve
the accuracy of fine-grained image classification. The main novel design
features for constructing a weakly supervised learning backbone model DCNN
include (a) extracting heterogeneous data, (b) keeping the feature map
resolution unchanged, (c) expanding the receptive field, and (d) fusing global
representations and local features. Experimental results demonstrated that
using DCNN as the backbone network for classifying certain fine-grained
benchmark datasets achieved performance advantage improvements of 13.5--19.5%
and 2.2--12.9%, respectively, compared to other advanced convolution or
attention-based fine-grained backbones.

摘要：

##### **Optimizing Language Model's Reasoning Abilities with Weak Supervision**
2405.04086v1 by Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang

While Large Language Models (LLMs) have demonstrated proficiency in handling
complex queries, much of the past work has depended on extensively annotated
datasets by human experts. However, this reliance on fully-supervised
annotations poses scalability challenges, particularly as models and data
requirements grow. To mitigate this, we explore the potential of enhancing
LLMs' reasoning abilities with minimal human supervision. In this work, we
introduce self-reinforcement, which begins with Supervised Fine-Tuning (SFT) of
the model using a small collection of annotated questions. Then it iteratively
improves LLMs by learning from the differences in responses from the SFT and
unfinetuned models on unlabeled questions. Our approach provides an efficient
approach without relying heavily on extensive human-annotated explanations.
However, current reasoning benchmarks typically only include golden-reference
answers or rationales. Therefore, we present \textsc{PuzzleBen}, a weakly
supervised benchmark that comprises 25,147 complex questions, answers, and
human-generated rationales across various domains, such as brainteasers,
puzzles, riddles, parajumbles, and critical reasoning tasks. A unique aspect of
our dataset is the inclusion of 10,000 unannotated questions, enabling us to
explore utilizing fewer supersized data to boost LLMs' inference capabilities.
Our experiments underscore the significance of \textsc{PuzzleBen}, as well as
the effectiveness of our methodology as a promising direction in future
endeavors. Our dataset and code will be published soon on \texttt{Anonymity
Link}.

摘要：

##### **Counterfactual and Semifactual Explanations in Abstract Argumentation: Formal Foundations, Complexity and Computation**
2405.04081v1 by Gianvincenzo Alfano, Sergio Greco, Francesco Parisi, Irina Trubitsyna

Explainable Artificial Intelligence and Formal Argumentation have received
significant attention in recent years. Argumentation-based systems often lack
explainability while supporting decision-making processes. Counterfactual and
semifactual explanations are interpretability techniques that provide insights
into the outcome of a model by generating alternative hypothetical instances.
While there has been important work on counterfactual and semifactual
explanations for Machine Learning models, less attention has been devoted to
these kinds of problems in argumentation. In this paper, we explore
counterfactual and semifactual reasoning in abstract Argumentation Framework.
We investigate the computational complexity of counterfactual- and
semifactual-based reasoning problems, showing that they are generally harder
than classical argumentation problems such as credulous and skeptical
acceptance. Finally, we show that counterfactual and semifactual queries can be
encoded in weak-constrained Argumentation Framework, and provide a
computational strategy through ASP solvers.

摘要：

##### **WISER: Weak supervISion and supErvised Representation learning to improve drug response prediction in cancer**
2405.04078v1 by Kumar Shubham, Aishwarya Jayagopal, Syed Mohammed Danish, Prathosh AP, Vaibhav Rajan

Cancer, a leading cause of death globally, occurs due to genomic changes and
manifests heterogeneously across patients. To advance research on personalized
treatment strategies, the effectiveness of various drugs on cells derived from
cancers (`cell lines') is experimentally determined in laboratory settings.
Nevertheless, variations in the distribution of genomic data and drug responses
between cell lines and humans arise due to biological and environmental
differences. Moreover, while genomic profiles of many cancer patients are
readily available, the scarcity of corresponding drug response data limits the
ability to train machine learning models that can predict drug response in
patients effectively. Recent cancer drug response prediction methods have
largely followed the paradigm of unsupervised domain-invariant representation
learning followed by a downstream drug response classification step.
Introducing supervision in both stages is challenging due to heterogeneous
patient response to drugs and limited drug response data. This paper addresses
these challenges through a novel representation learning method in the first
phase and weak supervision in the second. Experimental results on real patient
data demonstrate the efficacy of our method (WISER) over state-of-the-art
alternatives on predicting personalized drug response.

摘要：

##### **A simple theory for training response of deep neural networks**
2405.04074v1 by Kenichi Nakazato

Deep neural networks give us a powerful method to model the training
dataset's relationship between input and output. We can regard that as a
complex adaptive system consisting of many artificial neurons that work as an
adaptive memory as a whole. The network's behavior is training dynamics with a
feedback loop from the evaluation of the loss function. We already know the
training response can be constant or shows power law-like aging in some ideal
situations. However, we still have gaps between those findings and other
complex phenomena, like network fragility. To fill the gap, we introduce a very
simple network and analyze it. We show the training response consists of some
different factors based on training stages, activation functions, or training
methods. In addition, we show feature space reduction as an effect of
stochastic training dynamics, which can result in network fragility. Finally,
we discuss some complex phenomena of deep networks.

摘要：

##### **FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference**
2405.04065v1 by Runheng Liu, Xingchen Xiao, Heyan Huang, Zewen Chi, Zhijing Wu

Retrieval-Augmented Language Modeling (RALM) by integrating large language
models (LLM) with relevant documents from an external corpus is a proven method
for enabling the LLM to generate information beyond the scope of its
pre-training corpus. Previous work using utilizing retrieved content by simply
prepending retrieved contents to the input poses a high runtime issue, which
degrades the inference efficiency of the LLMs because they fail to use the
Key-Value (KV) cache efficiently. In this paper, we propose \textsc{FlashBack},
a modular RALM designed to improve the inference efficiency of RALM with
appending context pattern while maintaining decent performance after specific
fine-tuning without heavily destruct the knowledge integrity of the LLM.
\textsc{FlashBack} appends retrieved documents at the end of the context for
efficiently utilizing the KV cache instead of prepending them. Our experiment
shows that the inference speed of \textsc{FlashBack} is up to $4\times$ faster
than the prepending method on a 7B LLM (Llama 2). Via bypassing unnecessary
re-computation, it demonstrates an advancement by achieving significantly
faster inference speed, and this heightened efficiency will substantially
reduce inferential cost. Our code will be publicly available.

摘要：

##### **Evaluating Text Summaries Generated by Large Language Models Using OpenAI's GPT**
2405.04053v1 by Hassan Shakil, Atqiya Munawara Mahi, Phuoc Nguyen, Zeydy Ortiz, Mamoun T. Mardini

This research examines the effectiveness of OpenAI's GPT models as
independent evaluators of text summaries generated by six transformer-based
models from Hugging Face: DistilBART, BERT, ProphetNet, T5, BART, and PEGASUS.
We evaluated these summaries based on essential properties of high-quality
summary - conciseness, relevance, coherence, and readability - using
traditional metrics such as ROUGE and Latent Semantic Analysis (LSA). Uniquely,
we also employed GPT not as a summarizer but as an evaluator, allowing it to
independently assess summary quality without predefined metrics. Our analysis
revealed significant correlations between GPT evaluations and traditional
metrics, particularly in assessing relevance and coherence. The results
demonstrate GPT's potential as a robust tool for evaluating text summaries,
offering insights that complement established metrics and providing a basis for
comparative analysis of transformer-based models in natural language processing
tasks.

摘要：

##### **Learning Linear Block Error Correction Codes**
2405.04050v1 by Yoni Choukroun, Lior Wolf

Error correction codes are a crucial part of the physical communication
layer, ensuring the reliable transfer of data over noisy channels. The design
of optimal linear block codes capable of being efficiently decoded is of major
concern, especially for short block lengths. While neural decoders have
recently demonstrated their advantage over classical decoding techniques, the
neural design of the codes remains a challenge. In this work, we propose for
the first time a unified encoder-decoder training of binary linear block codes.
To this end, we adapt the coding setting to support efficient and
differentiable training of the code for end-to-end optimization over the order
two Galois field. We also propose a novel Transformer model in which the
self-attention masking is performed in a differentiable fashion for the
efficient backpropagation of the code gradient. Our results show that (i) the
proposed decoder outperforms existing neural decoding on conventional codes,
(ii) the suggested framework generates codes that outperform the {analogous}
conventional codes, and (iii) the codes we developed not only excel with our
decoder but also show enhanced performance with traditional decoding
techniques.

摘要：

##### **Philosophy of Cognitive Science in the Age of Deep Learning**
2405.04048v1 by Raphaël Millière

Deep learning has enabled major advances across most areas of artificial
intelligence research. This remarkable progress extends beyond mere engineering
achievements and holds significant relevance for the philosophy of cognitive
science. Deep neural networks have made significant strides in overcoming the
limitations of older connectionist models that once occupied the centre stage
of philosophical debates about cognition. This development is directly relevant
to long-standing theoretical debates in the philosophy of cognitive science.
Furthermore, ongoing methodological challenges related to the comparative
evaluation of deep neural networks stand to benefit greatly from
interdisciplinary collaboration with philosophy and cognitive science. The time
is ripe for philosophers to explore foundational issues related to deep
learning and cognition; this perspective paper surveys key areas where their
contributions can be especially fruitful.

摘要：

##### **Feature Map Convergence Evaluation for Functional Module**
2405.04041v1 by Ludan Zhang, Chaoyi Chen, Lei He, Keqiang Li

Autonomous driving perception models are typically composed of multiple
functional modules that interact through complex relationships to accomplish
environment understanding. However, perception models are predominantly
optimized as a black box through end-to-end training, lacking independent
evaluation of functional modules, which poses difficulties for interpretability
and optimization. Pioneering in the issue, we propose an evaluation method
based on feature map analysis to gauge the convergence of model, thereby
assessing functional modules' training maturity. We construct a quantitative
metric named as the Feature Map Convergence Score (FMCS) and develop Feature
Map Convergence Evaluation Network (FMCE-Net) to measure and predict the
convergence degree of models respectively. FMCE-Net achieves remarkable
predictive accuracy for FMCS across multiple image classification experiments,
validating the efficacy and robustness of the introduced approach. To the best
of our knowledge, this is the first independent evaluation method for
functional modules, offering a new paradigm for the training assessment towards
perception models.

摘要：

##### **Utilizing GPT to Enhance Text Summarization: A Strategy to Minimize Hallucinations**
2405.04039v1 by Hassan Shakil, Zeydy Ortiz, Grant C. Forbes

In this research, we uses the DistilBERT model to generate extractive summary
and the T5 model to generate abstractive summaries. Also, we generate hybrid
summaries by combining both DistilBERT and T5 models. Central to our research
is the implementation of GPT-based refining process to minimize the common
problem of hallucinations that happens in AI-generated summaries. We evaluate
unrefined summaries and, after refining, we also assess refined summaries using
a range of traditional and novel metrics, demonstrating marked improvements in
the accuracy and reliability of the summaries. Results highlight significant
improvements in reducing hallucinatory content, thereby increasing the factual
integrity of the summaries.

摘要：

##### **Locally Differentially Private In-Context Learning**
2405.04032v1 by Chunyan Zheng, Keke Sun, Wenhao Zhao, Haibo Zhou, Lixin Jiang, Shaoyang Song, Chunlai Zhou

Large pretrained language models (LLMs) have shown surprising In-Context
Learning (ICL) ability. An important application in deploying large language
models is to augment LLMs with a private database for some specific task. The
main problem with this promising commercial use is that LLMs have been shown to
memorize their training data and their prompt data are vulnerable to membership
inference attacks (MIA) and prompt leaking attacks. In order to deal with this
problem, we treat LLMs as untrusted in privacy and propose a locally
differentially private framework of in-context learning(LDP-ICL) in the
settings where labels are sensitive. Considering the mechanisms of in-context
learning in Transformers by gradient descent, we provide an analysis of the
trade-off between privacy and utility in such LDP-ICL for classification.
Moreover, we apply LDP-ICL to the discrete distribution estimation problem. In
the end, we perform several experiments to demonstrate our analysis results.

摘要：

##### **Certified Policy Verification and Synthesis for MDPs under Distributional Reach-avoidance Properties**
2405.04015v1 by S. Akshay, Krishnendu Chatterjee, Tobias Meggendorfer, Đorđe Žikelić

Markov Decision Processes (MDPs) are a classical model for decision making in
the presence of uncertainty. Often they are viewed as state transformers with
planning objectives defined with respect to paths over MDP states. An
increasingly popular alternative is to view them as distribution transformers,
giving rise to a sequence of probability distributions over MDP states. For
instance, reachability and safety properties in modeling robot swarms or
chemical reaction networks are naturally defined in terms of probability
distributions over states. Verifying such distributional properties is known to
be hard and often beyond the reach of classical state-based verification
techniques.
  In this work, we consider the problems of certified policy (i.e. controller)
verification and synthesis in MDPs under distributional reach-avoidance
specifications. By certified we mean that, along with a policy, we also aim to
synthesize a (checkable) certificate ensuring that the MDP indeed satisfies the
property. Thus, given the target set of distributions and an unsafe set of
distributions over MDP states, our goal is to either synthesize a certificate
for a given policy or synthesize a policy along with a certificate, proving
that the target distribution can be reached while avoiding unsafe
distributions. To solve this problem, we introduce the novel notion of
distributional reach-avoid certificates and present automated procedures for
(1) synthesizing a certificate for a given policy, and (2) synthesizing a
policy together with the certificate, both providing formal guarantees on
certificate correctness. Our experimental evaluation demonstrates the ability
of our method to solve several non-trivial examples, including a multi-agent
robot-swarm model, to synthesize certified policies and to certify existing
policies.

摘要：

##### **Structured Click Control in Transformer-based Interactive Segmentation**
2405.04009v1 by Long Xu, Yongquan Chen, Rui Huang, Feng Wu, Shiwu Lai

Click-point-based interactive segmentation has received widespread attention
due to its efficiency. However, it's hard for existing algorithms to obtain
precise and robust responses after multiple clicks. In this case, the
segmentation results tend to have little change or are even worse than before.
To improve the robustness of the response, we propose a structured click intent
model based on graph neural networks, which adaptively obtains graph nodes via
the global similarity of user-clicked Transformer tokens. Then the graph nodes
will be aggregated to obtain structured interaction features. Finally, the dual
cross-attention will be used to inject structured interaction features into
vision Transformer features, thereby enhancing the control of clicks over
segmentation results. Extensive experiments demonstrated the proposed algorithm
can serve as a general structure in improving Transformer-based interactive
segmenta?tion performance. The code and data will be released at
https://github.com/hahamyt/scc.

摘要：

##### **Sketch Then Generate: Providing Incremental User Feedback and Guiding LLM Code Generation through Language-Oriented Code Sketches**
2405.03998v1 by Chen Zhu-Tian, Zeyu Xiong, Xiaoshuo Yao, Elena Glassman

Crafting effective prompts for code generation or editing with Large Language
Models (LLMs) is not an easy task. Particularly, the absence of immediate,
stable feedback during prompt crafting hinders effective interaction, as users
are left to mentally imagine possible outcomes until the code is generated. In
response, we introduce Language-Oriented Code Sketching, an interactive
approach that provides instant, incremental feedback in the form of code
sketches (i.e., incomplete code outlines) during prompt crafting. This approach
converts a prompt into a code sketch by leveraging the inherent linguistic
structures within the prompt and applying classic natural language processing
techniques. The sketch then serves as an intermediate placeholder that not only
previews the intended code structure but also guides the LLM towards the
desired code, thereby enhancing human-LLM interaction. We conclude by
discussing the approach's applicability and future plans.

摘要：

##### **TrimCaching: Parameter-sharing AI Model Caching in Wireless Edge Networks**
2405.03990v1 by Guanqiao Qu, Zheng Lin, Fangming Liu, Xianhao Chen, Kaibin Huang

Next-generation mobile networks are expected to facilitate fast AI model
downloading to end users. By caching models on edge servers, mobile networks
can deliver models to end users with low latency, resulting in a paradigm
called edge model caching. In this paper, we develop a novel model placement
scheme, called parameter-sharing model caching (TrimCaching). TrimCaching
exploits the key observation that a wide range of AI models, such as
convolutional neural networks or large language models, can share a significant
proportion of parameter blocks containing reusable knowledge, thereby improving
storage efficiency. To this end, we formulate a parameter-sharing model
placement problem to maximize the cache hit ratio in multi-edge wireless
networks by balancing the fundamental tradeoff between storage efficiency and
service latency. We show that the formulated problem is a submodular
maximization problem with submodular constraints, for which no polynomial-time
approximation algorithm exists. To overcome this challenge, we study an
important special case, where a small fixed number of parameter blocks are
shared across models, which often holds in practice. In such a case, a
polynomial-time algorithm with $\left(1-\epsilon\right)/2$-approximation
guarantee is developed. Subsequently, we address the original problem for the
general case by developing a greedy algorithm. Simulation results demonstrate
that the proposed TrimCaching framework significantly improves the cache hit
ratio compared with state-of-the-art content caching without exploiting shared
parameters in AI models.

摘要：

##### **Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application**
2405.03988v1 by Jian Jia, Yipei Wang, Yan Li, Honggang Chen, Xuehan Bai, Zhaocheng Liu, Jian Liang, Quan Chen, Han Li, Peng Jiang, Kun Gai

Contemporary recommender systems predominantly rely on collaborative
filtering techniques, employing ID-embedding to capture latent associations
among users and items. However, this approach overlooks the wealth of semantic
information embedded within textual descriptions of items, leading to
suboptimal performance in cold-start scenarios and long-tail user
recommendations. Leveraging the capabilities of Large Language Models (LLMs)
pretrained on massive text corpus presents a promising avenue for enhancing
recommender systems by integrating open-world domain knowledge. In this paper,
we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework
that synergizes open-world knowledge with collaborative knowledge. We address
computational complexity concerns by utilizing pretrained LLMs as item encoders
and freezing LLM parameters to avoid catastrophic forgetting and preserve
open-world knowledge. To bridge the gap between the open-world and
collaborative domains, we design a twin-tower structure supervised by the
recommendation task and tailored for practical industrial application. Through
offline experiments on the large-scale industrial dataset and online
experiments on A/B tests, we demonstrate the efficacy of our approach.

摘要：

##### **Factors Influencing User Willingness To Use SORA**
2405.03986v1 by Gustave Florentin Nkoulou Mvondo, Ben Niu

Sora promises to redefine the way visual content is created. Despite its
numerous forecasted benefits, the drivers of user willingness to use the
text-to-video (T2V) model are unknown. This study extends the extended unified
theory of acceptance and use of technology (UTAUT2) with perceived realism and
novelty value. Using a purposive sampling method, we collected data from 940
respondents in the US and analyzed the sample using covariance-based structural
equation modeling and fuzzy set qualitative comparative analysis (fsQCA). The
findings reveal that all hypothesized relationships are supported, with
perceived realism emerging as the most influential driver, followed by novelty
value. Moreover, fsQCA identifies five configurations leading to high and low
willingness to use, and the model demonstrates high predictive validity,
contributing to theory advancement. Our study provides valuable insights for
developers and marketers, offering guidance for strategic decisions to promote
the widespread adoption of T2V models.

摘要：

##### **TBNet: A Neural Architectural Defense Framework Facilitating DNN Model Protection in Trusted Execution Environments**
2405.03974v1 by Ziyu Liu, Tong Zhou, Yukui Luo, Xiaolin Xu

Trusted Execution Environments (TEEs) have become a promising solution to
secure DNN models on edge devices. However, the existing solutions either
provide inadequate protection or introduce large performance overhead. Taking
both security and performance into consideration, this paper presents TBNet, a
TEE-based defense framework that protects DNN model from a neural architectural
perspective. Specifically, TBNet generates a novel Two-Branch substitution
model, to respectively exploit (1) the computational resources in the untrusted
Rich Execution Environment (REE) for latency reduction and (2) the
physically-isolated TEE for model protection. Experimental results on a
Raspberry Pi across diverse DNN model architectures and datasets demonstrate
that TBNet achieves efficient model protection at a low cost.

摘要：

##### **ERATTA: Extreme RAG for Table To Answers with Large Language Models**
2405.03963v1 by Sohini Roychowdhury, Marko Krema, Anvar Mahammad, Brian Moore, Arijit Mukherjee, Punit Prakashchandra

Large language models (LLMs) with residual augmented-generation (RAG) have
been the optimal choice for scalable generative AI solutions in the recent
past. However, the choice of use-cases that incorporate RAG with LLMs have been
either generic or extremely domain specific, thereby questioning the
scalability and generalizability of RAG-LLM approaches. In this work, we
propose a unique LLM-based system where multiple LLMs can be invoked to enable
data authentication, user query routing, data retrieval and custom prompting
for question answering capabilities from data tables that are highly varying
and large in size. Our system is tuned to extract information from
Enterprise-level data products and furnish real time responses under 10
seconds. One prompt manages user-to-data authentication followed by three
prompts to route, fetch data and generate a customizable prompt natural
language responses. Additionally, we propose a five metric scoring module that
detects and reports hallucinations in the LLM responses. Our proposed system
and scoring metrics achieve >90% confidence scores across hundreds of user
queries in the sustainability, financial health and social media domains.
Extensions to the proposed extreme RAG architectures can enable heterogeneous
source querying using LLMs.

摘要：

##### **ESIHGNN: Event-State Interactions Infused Heterogeneous Graph Neural Network for Conversational Emotion Recognition**
2405.03960v1 by Xupeng Zha, Huan Zhao, Zixing Zhang

Conversational Emotion Recognition (CER) aims to predict the emotion
expressed by an utterance (referred to as an ``event'') during a conversation.
Existing graph-based methods mainly focus on event interactions to comprehend
the conversational context, while overlooking the direct influence of the
speaker's emotional state on the events. In addition, real-time modeling of the
conversation is crucial for real-world applications but is rarely considered.
Toward this end, we propose a novel graph-based approach, namely Event-State
Interactions infused Heterogeneous Graph Neural Network (ESIHGNN), which
incorporates the speaker's emotional state and constructs a heterogeneous
event-state interaction graph to model the conversation. Specifically, a
heterogeneous directed acyclic graph neural network is employed to dynamically
update and enhance the representations of events and emotional states at each
turn, thereby improving conversational coherence and consistency. Furthermore,
to further improve the performance of CER, we enrich the graph's edges with
external knowledge. Experimental results on four publicly available CER
datasets show the superiority of our approach and the effectiveness of the
introduced heterogeneous event-state interaction graph.

摘要：

##### **Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your Diffusion Model**
2405.03958v1 by Joo Young Choi, Jaesung R. Park, Inkyu Park, Jaewoong Cho, Albert No, Ernest K. Ryu

Current state-of-the-art diffusion models employ U-Net architectures
containing convolutional and (qkv) self-attention layers. The U-Net processes
images while being conditioned on the time embedding input for each sampling
step and the class or caption embedding input corresponding to the desired
conditional generation. Such conditioning involves scale-and-shift operations
to the convolutional layers but does not directly affect the attention layers.
While these standard architectural choices are certainly effective, not
conditioning the attention layers feels arbitrary and potentially suboptimal.
In this work, we show that simply adding LoRA conditioning to the attention
layers without changing or tuning the other parts of the U-Net architecture
improves the image generation quality. For example, a drop-in addition of LoRA
conditioning to EDM diffusion model yields FID scores of 1.91/1.75 for
unconditional and class-conditional CIFAR-10 generation, improving upon the
baseline of 1.97/1.79.

摘要：

##### **HAFFormer: A Hierarchical Attention-Free Framework for Alzheimer's Disease Detection From Spontaneous Speech**
2405.03952v1 by Zhongren Dong, Zixing Zhang, Weixiang Xu, Jing Han, Jianjun Ou, Björn W. Schuller

Automatically detecting Alzheimer's Disease (AD) from spontaneous speech
plays an important role in its early diagnosis. Recent approaches highly rely
on the Transformer architectures due to its efficiency in modelling long-range
context dependencies. However, the quadratic increase in computational
complexity associated with self-attention and the length of audio poses a
challenge when deploying such models on edge devices. In this context, we
construct a novel framework, namely Hierarchical Attention-Free Transformer
(HAFFormer), to better deal with long speech for AD detection. Specifically, we
employ an attention-free module of Multi-Scale Depthwise Convolution to replace
the self-attention and thus avoid the expensive computation, and a GELU-based
Gated Linear Unit to replace the feedforward layer, aiming to automatically
filter out the redundant information. Moreover, we design a hierarchical
structure to force it to learn a variety of information grains, from the frame
level to the dialogue level. By conducting extensive experiments on the
ADReSS-M dataset, the introduced HAFFormer can achieve competitive results
(82.6% accuracy) with other recent work, but with significant computational
complexity and model size reduction compared to the standard Transformer. This
shows the efficiency of HAFFormer in dealing with long audio for AD detection.

摘要：

##### **Predictive Modeling with Temporal Graphical Representation on Electronic Health Records**
2405.03943v1 by Jiayuan Chen, Changchang Yin, Yuanlong Wang, Ping Zhang

Deep learning-based predictive models, leveraging Electronic Health Records
(EHR), are receiving increasing attention in healthcare. An effective
representation of a patient's EHR should hierarchically encompass both the
temporal relationships between historical visits and medical events, and the
inherent structural information within these elements. Existing patient
representation methods can be roughly categorized into sequential
representation and graphical representation. The sequential representation
methods focus only on the temporal relationships among longitudinal visits. On
the other hand, the graphical representation approaches, while adept at
extracting the graph-structured relationships between various medical events,
fall short in effectively integrate temporal information. To capture both types
of information, we model a patient's EHR as a novel temporal heterogeneous
graph. This graph includes historical visits nodes and medical events nodes. It
propagates structured information from medical event nodes to visit nodes and
utilizes time-aware visit nodes to capture changes in the patient's health
status. Furthermore, we introduce a novel temporal graph transformer (TRANS)
that integrates temporal edge features, global positional encoding, and local
structural encoding into heterogeneous graph convolution, capturing both
temporal and structural information. We validate the effectiveness of TRANS
through extensive experiments on three real-world datasets. The results show
that our proposed approach achieves state-of-the-art performance.

摘要：

##### **Long Context Alignment with Short Instructions and Synthesized Positions**
2405.03939v1 by Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, Sujian Li

Effectively handling instructions with extremely long context remains a
challenge for Large Language Models (LLMs), typically necessitating
high-quality long data and substantial computational resources. This paper
introduces Step-Skipping Alignment (SkipAlign), a new technique designed to
enhance the long-context capabilities of LLMs in the phase of alignment without
the need for additional efforts beyond training with original data length.
SkipAlign is developed on the premise that long-range dependencies are
fundamental to enhancing an LLM's capacity of long context. Departing from
merely expanding the length of input samples, SkipAlign synthesizes long-range
dependencies from the aspect of positions indices. This is achieved by the
strategic insertion of skipped positions within instruction-following samples,
which utilizes the semantic structure of the data to effectively expand the
context. Through extensive experiments on base models with a variety of context
window sizes, SkipAlign demonstrates its effectiveness across a spectrum of
long-context tasks. Particularly noteworthy is that with a careful selection of
the base model and alignment datasets, SkipAlign with only 6B parameters
achieves it's best performance and comparable with strong baselines like
GPT-3.5-Turbo-16K on LongBench.

摘要：

##### **CleanGraph: Human-in-the-loop Knowledge Graph Refinement and Completion**
2405.03932v1 by Tyler Bikaun, Michael Stewart, Wei Liu

This paper presents CleanGraph, an interactive web-based tool designed to
facilitate the refinement and completion of knowledge graphs. Maintaining the
reliability of knowledge graphs, which are grounded in high-quality and
error-free facts, is crucial for real-world applications such as
question-answering and information retrieval systems. These graphs are often
automatically assembled from textual sources by extracting semantic triples via
information extraction. However, assuring the quality of these extracted
triples, especially when dealing with large or low-quality datasets, can pose a
significant challenge and adversely affect the performance of downstream
applications. CleanGraph allows users to perform Create, Read, Update, and
Delete (CRUD) operations on their graphs, as well as apply models in the form
of plugins for graph refinement and completion tasks. These functionalities
enable users to enhance the integrity and reliability of their graph data. A
demonstration of CleanGraph and its source code can be accessed at
https://github.com/nlp-tlp/CleanGraph under the MIT License.

摘要：

##### **Unicorn: U-Net for Sea Ice Forecasting with Convolutional Neural Ordinary Differential Equations**
2405.03929v1 by Jaesung Park, Sungchul Hong, Yoonseo Cho, Jong-June Jeon

Sea ice at the North Pole is vital to global climate dynamics. However,
accurately forecasting sea ice poses a significant challenge due to the
intricate interaction among multiple variables. Leveraging the capability to
integrate multiple inputs and powerful performances seamlessly, many studies
have turned to neural networks for sea ice forecasting. This paper introduces a
novel deep architecture named Unicorn, designed to forecast weekly sea ice. Our
model integrates multiple time series images within its architecture to enhance
its forecasting performance. Moreover, we incorporate a bottleneck layer within
the U-Net architecture, serving as neural ordinary differential equations with
convolution operations, to capture the spatiotemporal dynamics of latent
variables. Through real data analysis with datasets spanning from 1998 to 2021,
our proposed model demonstrates significant improvements over state-of-the-art
models in the sea ice concentration forecasting task. It achieves an average
MAE improvement of 12% compared to benchmark models. Additionally, our method
outperforms existing approaches in sea ice extent forecasting, achieving a
classification performance improvement of approximately 18%. These experimental
results show the superiority of our proposed model.

摘要：

##### **A Roadmap for Multilingual, Multimodal Domain Independent Deception Detection**
2405.03920v1 by Dainis Boumber, Rakesh M. Verma, Fatima Zahra Qachfar

Deception, a prevalent aspect of human communication, has undergone a
significant transformation in the digital age. With the globalization of online
interactions, individuals are communicating in multiple languages and mixing
languages on social media, with varied data becoming available in each language
and dialect. At the same time, the techniques for detecting deception are
similar across the board. Recent studies have shown the possibility of the
existence of universal linguistic cues to deception across domains within the
English language; however, the existence of such cues in other languages
remains unknown. Furthermore, the practical task of deception detection in
low-resource languages is not a well-studied problem due to the lack of labeled
data. Another dimension of deception is multimodality. For example, a picture
with an altered caption in fake news or disinformation may exist. This paper
calls for a comprehensive investigation into the complexities of deceptive
language across linguistic boundaries and modalities within the realm of
computer security and natural language processing and the possibility of using
multilingual transformer models and labeled data in various languages to
universally address the task of deception detection.

摘要：

##### **OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs**
2405.03901v1 by Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, Michelle Li

The progression to "Pervasive Augmented Reality" envisions easy access to
multimodal information continuously. However, in many everyday scenarios, users
are occupied physically, cognitively or socially. This may increase the
friction to act upon the multimodal information that users encounter in the
world. To reduce such friction, future interactive interfaces should
intelligently provide quick access to digital actions based on users' context.
To explore the range of possible digital actions, we conducted a diary study
that required participants to capture and share the media that they intended to
perform actions on (e.g., images or audio), along with their desired actions
and other contextual information. Using this data, we generated a holistic
design space of digital follow-up actions that could be performed in response
to different types of multimodal sensory inputs. We then designed OmniActions,
a pipeline powered by large language models (LLMs) that processes multimodal
sensory inputs and predicts follow-up actions on the target information
grounded in the derived design space. Using the empirical data collected in the
diary study, we performed quantitative evaluations on three variations of LLM
techniques (intent classification, in-context learning and finetuning) and
identified the most effective technique for our task. Additionally, as an
instantiation of the pipeline, we developed an interactive prototype and
reported preliminary user feedback about how people perceive and react to the
action predictions and its errors.

摘要：

##### **Out-of-Distribution Adaptation in Offline RL: Counterfactual Reasoning via Causal Normalizing Flows**
2405.03892v1 by Minjae Cho, Jonathan P. How, Chuangchuang Sun

Despite notable successes of Reinforcement Learning (RL), the prevalent use
of an online learning paradigm prevents its widespread adoption, especially in
hazardous or costly scenarios. Offline RL has emerged as an alternative
solution, learning from pre-collected static datasets. However, this offline
learning introduces a new challenge known as distributional shift, degrading
the performance when the policy is evaluated on scenarios that are
Out-Of-Distribution (OOD) from the training dataset. Most existing offline RL
resolves this issue by regularizing policy learning within the information
supported by the given dataset. However, such regularization overlooks the
potential for high-reward regions that may exist beyond the dataset. This
motivates exploring novel offline learning techniques that can make
improvements beyond the data support without compromising policy performance,
potentially by learning causation (cause-and-effect) instead of correlation
from the dataset. In this paper, we propose the MOOD-CRL (Model-based Offline
OOD-Adapting Causal RL) algorithm, which aims to address the challenge of
extrapolation for offline policy training through causal inference instead of
policy-regularizing methods. Specifically, Causal Normalizing Flow (CNF) is
developed to learn the transition and reward functions for data generation and
augmentation in offline policy evaluation and training. Based on the
data-invariant, physics-based qualitative causal graph and the observational
data, we develop a novel learning scheme for CNF to learn the quantitative
structural causal model. As a result, CNF gains predictive and counterfactual
reasoning capabilities for sequential decision-making tasks, revealing a high
potential for OOD adaptation. Our CNF-based offline RL approach is validated
through empirical evaluations, outperforming model-free and model-based methods
by a significant margin.

摘要：

##### **Enhancing O-RAN Security: Evasion Attacks and Robust Defenses for Graph Reinforcement Learning-based Connection Management**
2405.03891v1 by Ravikumar Balakrishnan, Marius Arvinte, Nageen Himayat, Hosein Nikopour, Hassnaa Moustafa

Adversarial machine learning, focused on studying various attacks and
defenses on machine learning (ML) models, is rapidly gaining importance as ML
is increasingly being adopted for optimizing wireless systems such as Open
Radio Access Networks (O-RAN). A comprehensive modeling of the security threats
and the demonstration of adversarial attacks and defenses on practical AI based
O-RAN systems is still in its nascent stages. We begin by conducting threat
modeling to pinpoint attack surfaces in O-RAN using an ML-based Connection
management application (xApp) as an example. The xApp uses a Graph Neural
Network trained using Deep Reinforcement Learning and achieves on average 54%
improvement in the coverage rate measured as the 5th percentile user data
rates. We then formulate and demonstrate evasion attacks that degrade the
coverage rates by as much as 50% through injecting bounded noise at different
threat surfaces including the open wireless medium itself. Crucially, we also
compare and contrast the effectiveness of such attacks on the ML-based xApp and
a non-ML based heuristic. We finally develop and demonstrate robust
training-based defenses against the challenging physical/jamming-based attacks
and show a 15% improvement in the coverage rates when compared to employing no
defense over a range of noise budgets

摘要：

##### **Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer**
2405.03882v1 by Huihong Shi, Haikuo Shao, Wendong Mao, Zhongfeng Wang

Motivated by the huge success of Transformers in the field of natural
language processing (NLP), Vision Transformers (ViTs) have been rapidly
developed and achieved remarkable performance in various computer vision tasks.
However, their huge model sizes and intensive computations hinder ViTs'
deployment on embedded devices, calling for effective model compression
methods, such as quantization. Unfortunately, due to the existence of
hardware-unfriendly and quantization-sensitive non-linear operations,
particularly {Softmax}, it is non-trivial to completely quantize all operations
in ViTs, yielding either significant accuracy drops or non-negligible hardware
costs. In response to challenges associated with \textit{standard ViTs}, we
focus our attention towards the quantization and acceleration for
\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but
also integrate linear attention with low computational complexity, and propose
\emph{Trio-ViT} accordingly. Specifically, at the algorithm level, we develop a
{tailored post-training quantization engine} taking the unique activation
distributions of Softmax-free efficient ViTs into full consideration, aiming to
boost quantization accuracy. Furthermore, at the hardware level, we build an
accelerator dedicated to the specific Convolution-Transformer hybrid
architecture of efficient ViTs, thereby enhancing hardware efficiency.
Extensive experimental results consistently prove the effectiveness of our
Trio-ViT framework. {Particularly, we can gain up to
$\uparrow$$\mathbf{7.2}\times$ and $\uparrow$$\mathbf{14.6}\times$ FPS under
comparable accuracy over state-of-the-art ViT accelerators, as well as
$\uparrow$$\mathbf{5.9}\times$ and $\uparrow$$\mathbf{2.0}\times$ DSP
efficiency.} Codes will be released publicly upon acceptance.

摘要：

##### **Investigating Personalized Driving Behaviors in Dilemma Zones: Analysis and Prediction of Stop-or-Go Decisions**
2405.03873v1 by Ziye Qin, Siyan Li, Guoyuan Wu, Matthew J. Barth, Amr Abdelraouf, Rohit Gupta, Kyungtae Han

Dilemma zones at signalized intersections present a commonly occurring but
unsolved challenge for both drivers and traffic operators. Onsets of the yellow
lights prompt varied responses from different drivers: some may brake abruptly,
compromising the ride comfort, while others may accelerate, increasing the risk
of red-light violations and potential safety hazards. Such diversity in
drivers' stop-or-go decisions may result from not only surrounding traffic
conditions, but also personalized driving behaviors. To this end, identifying
personalized driving behaviors and integrating them into advanced driver
assistance systems (ADAS) to mitigate the dilemma zone problem presents an
intriguing scientific question. In this study, we employ a game engine-based
(i.e., CARLA-enabled) driving simulator to collect high-resolution vehicle
trajectories, incoming traffic signal phase and timing information, and
stop-or-go decisions from four subject drivers in various scenarios. This
approach allows us to analyze personalized driving behaviors in dilemma zones
and develop a Personalized Transformer Encoder to predict individual drivers'
stop-or-go decisions. The results show that the Personalized Transformer
Encoder improves the accuracy of predicting driver decision-making in the
dilemma zone by 3.7% to 12.6% compared to the Generic Transformer Encoder, and
by 16.8% to 21.6% over the binary logistic regression model.

摘要：

##### **AI-Driven Frameworks for Enhancing Data Quality in Big Data Ecosystems: Error_Detection, Correction, and Metadata Integration**
2405.03870v1 by Widad Elouataoui

The widespread adoption of big data has ushered in a new era of data-driven
decision-making, transforming numerous industries and sectors. However, the
efficacy of these decisions hinges on the quality of the underlying data. Poor
data quality can result in inaccurate analyses and deceptive conclusions.
Managing the vast volume, velocity, and variety of data sources presents
significant challenges, heightening the importance of addressing big data
quality issues. While there has been increased attention from both academia and
industry, current approaches often lack comprehensiveness and universality.
They tend to focus on limited metrics, neglecting other dimensions of data
quality. Moreover, existing methods are often context-specific, limiting their
applicability across different domains. There is a clear need for intelligent,
automated approaches leveraging artificial intelligence (AI) for advanced data
quality corrections.
  To bridge these gaps, this Ph.D. thesis proposes a novel set of
interconnected frameworks aimed at enhancing big data quality comprehensively.
Firstly, we introduce new quality metrics and a weighted scoring system for
precise data quality assessment. Secondly, we present a generic framework for
detecting various quality anomalies using AI models. Thirdly, we propose an
innovative framework for correcting detected anomalies through predictive
modeling. Additionally, we address metadata quality enhancement within big data
ecosystems. These frameworks are rigorously tested on diverse datasets,
demonstrating their efficacy in improving big data quality. Finally, the thesis
concludes with insights and suggestions for future research directions.

摘要：

##### **Outlier Gradient Analysis: Efficiently Improving Deep Learning Model Performance via Hessian-Free Influence Functions**
2405.03869v1 by Anshuman Chhabra, Bo Li, Jian Chen, Prasant Mohapatra, Hongfu Liu

Influence functions offer a robust framework for assessing the impact of each
training data sample on model predictions, serving as a prominent tool in
data-centric learning. Despite their widespread use in various tasks, the
strong convexity assumption on the model and the computational cost associated
with calculating the inverse of the Hessian matrix pose constraints,
particularly when analyzing large deep models. This paper focuses on a
classical data-centric scenario--trimming detrimental samples--and addresses
both challenges within a unified framework. Specifically, we establish an
equivalence transformation between identifying detrimental training samples via
influence functions and outlier gradient detection. This transformation not
only presents a straightforward and Hessian-free formulation but also provides
profound insights into the role of the gradient in sample impact. Moreover, it
relaxes the convexity assumption of influence functions, extending their
applicability to non-convex deep models. Through systematic empirical
evaluations, we first validate the correctness of our proposed outlier gradient
analysis on synthetic datasets and then demonstrate its effectiveness in
detecting mislabeled samples in vision models, selecting data samples for
improving performance of transformer models for natural language processing,
and identifying influential samples for fine-tuned Large Language Models.

摘要：

##### **Learning Planning Abstractions from Language**
2405.03864v1 by Weiyu Liu, Geng Chen, Joy Hsu, Jiayuan Mao, Jiajun Wu

This paper presents a framework for learning state and action abstractions in
sequential decision-making domains. Our framework, planning abstraction from
language (PARL), utilizes language-annotated demonstrations to automatically
discover a symbolic and abstract action space and induce a latent state
abstraction based on it. PARL consists of three stages: 1) recovering
object-level and action concepts, 2) learning state abstractions, abstract
action feasibility, and transition models, and 3) applying low-level policies
for abstract actions. During inference, given the task description, PARL first
makes abstract action plans using the latent transition and feasibility
functions, then refines the high-level plan using low-level policies. PARL
generalizes across scenarios involving novel object instances and environments,
unseen concept compositions, and tasks that require longer planning horizons
than settings it is trained on.

摘要：

##### **Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration**
2405.03862v1 by Razan Baltaji, Babak Hemmatian, Lav R. Varshney

This study explores the sources of instability in maintaining cultural
personas and opinions within multi-agent LLM systems. Drawing on simulations of
inter-cultural collaboration and debate, we analyze agents' pre- and
post-discussion private responses alongside chat transcripts to assess the
stability of cultural personas and the impact of opinion diversity on group
outcomes. Our findings suggest that multi-agent discussions can encourage
collective decisions that reflect diverse perspectives, yet this benefit is
tempered by the agents' susceptibility to conformity due to perceived peer
pressure and challenges in maintaining consistent personas and opinions.
Counterintuitively, instructions that encourage debate in support of one's
opinions increase the rate of inconstancy. Without addressing the factors we
identify, the full potential of multi-agent frameworks for producing more
culturally diverse AI outputs will remain untapped.

摘要：

##### **VSA4VQA: Scaling a Vector Symbolic Architecture to Visual Question Answering on Natural Images**
2405.03852v1 by Anna Penzkofer, Lei Shi, Andreas Bulling

While Vector Symbolic Architectures (VSAs) are promising for modelling
spatial cognition, their application is currently limited to artificially
generated images and simple spatial queries. We propose VSA4VQA - a novel 4D
implementation of VSAs that implements a mental representation of natural
images for the challenging task of Visual Question Answering (VQA). VSA4VQA is
the first model to scale a VSA to complex spatial queries. Our method is based
on the Semantic Pointer Architecture (SPA) to encode objects in a
hyperdimensional vector space. To encode natural images, we extend the SPA to
include dimensions for object's width and height in addition to their spatial
location. To perform spatial queries we further introduce learned spatial query
masks and integrate a pre-trained vision-language model for answering
attribute-related questions. We evaluate our method on the GQA benchmark
dataset and show that it can effectively encode natural images, achieving
competitive performance to state-of-the-art deep learning methods for zero-shot
VQA.

摘要：

##### **Self-Improving Customer Review Response Generation Based on LLMs**
2405.03845v1 by Guy Azov, Tatiana Pelc, Adi Fledel Alon, Gila Kamhi

Previous studies have demonstrated that proactive interaction with user
reviews has a positive impact on the perception of app users and encourages
them to submit revised ratings. Nevertheless, developers encounter challenges
in managing a high volume of reviews, particularly in the case of popular apps
with a substantial influx of daily reviews. Consequently, there is a demand for
automated solutions aimed at streamlining the process of responding to user
reviews. To address this, we have developed a new system for generating
automatic responses by leveraging user-contributed documents with the help of
retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).
Our solution, named SCRABLE, represents an adaptive customer review response
automation that enhances itself with self-optimizing prompts and a judging
mechanism based on LLMs. Additionally, we introduce an automatic scoring
mechanism that mimics the role of a human evaluator to assess the quality of
responses generated in customer review domains. Extensive experiments and
analyses conducted on real-world datasets reveal that our method is effective
in producing high-quality responses, yielding improvement of more than 8.5%
compared to the baseline. Further validation through manual examination of the
generated responses underscores the efficacy our proposed system.

摘要：

##### **A Novel Cross-band CSI Prediction Scheme for Multi-band Fingerprint based Localization**
2405.03842v1 by Yuan Ruihao, Huang Kaixuan, Zhang Shunqing

Because of the advantages of computation complexity compared with traditional
localization algorithms, fingerprint based localization is getting increasing
demand. Expanding the fingerprint database from the frequency domain by channel
reconstruction can improve localization accuracy. However, in a mobility
environment, the channel reconstruction accuracy is limited by the time-varying
parameters. In this paper, we proposed a system to extract the time-varying
parameters based on space-alternating generalized expectation maximization
(SAGE) algorithm, then used variational auto-encoder (VAE) to reconstruct the
channel state information on another channel. The proposed scheme is tested on
the data generated by the deep-MIMO channel model. Mathematical analysis for
the viability of our system is also shown in this paper.

摘要：

##### **Guylingo: The Republic of Guyana Creole Corpora**
2405.03832v1 by Christopher Clarke, Roland Daynauth, Charlene Wilkinson, Hubert Devonish, Jason Mars

While major languages often enjoy substantial attention and resources, the
linguistic diversity across the globe encompasses a multitude of smaller,
indigenous, and regional languages that lack the same level of computational
support. One such region is the Caribbean. While commonly labeled as "English
speaking", the ex-British Caribbean region consists of a myriad of Creole
languages thriving alongside English. In this paper, we present Guylingo: a
comprehensive corpus designed for advancing NLP research in the domain of
Creolese (Guyanese English-lexicon Creole), the most widely spoken language in
the culturally rich nation of Guyana. We first outline our framework for
gathering and digitizing this diverse corpus, inclusive of colloquial
expressions, idioms, and regional variations in a low-resource language. We
then demonstrate the challenges of training and evaluating NLP models for
machine translation in Creole. Lastly, we discuss the unique opportunities
presented by recent NLP advancements for accelerating the formal adoption of
Creole languages as official languages in the Caribbean.

摘要：

##### **Organizing a Society of Language Models: Structures and Mechanisms for Enhanced Collective Intelligence**
2405.03825v1 by Silvan Ferreira, Ivanovitch Silva, Allan Martins

Recent developments in Large Language Models (LLMs) have significantly
expanded their applications across various domains. However, the effectiveness
of LLMs is often constrained when operating individually in complex
environments. This paper introduces a transformative approach by organizing
LLMs into community-based structures, aimed at enhancing their collective
intelligence and problem-solving capabilities. We investigate different
organizational models-hierarchical, flat, dynamic, and federated-each
presenting unique benefits and challenges for collaborative AI systems. Within
these structured communities, LLMs are designed to specialize in distinct
cognitive tasks, employ advanced interaction mechanisms such as direct
communication, voting systems, and market-based approaches, and dynamically
adjust their governance structures to meet changing demands. The implementation
of such communities holds substantial promise for improve problem-solving
capabilities in AI, prompting an in-depth examination of their ethical
considerations, management strategies, and scalability potential. This position
paper seeks to lay the groundwork for future research, advocating a paradigm
shift from isolated to synergistic operational frameworks in AI research and
application.

摘要：

##### **Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models**
2405.03821v1 by Evan King, Haoxiang Yu, Sahil Vartak, Jenna Jacob, Sangsu Lee, Christine Julien

Everyday devices like light bulbs and kitchen appliances are now embedded
with so many features and automated behaviors that they have become complicated
to actually use. While such "smart" capabilities can better support users'
goals, the task of learning the "ins and outs" of different devices is
daunting. Voice assistants aim to solve this problem by providing a natural
language interface to devices, yet such assistants cannot understand
loosely-constrained commands, they lack the ability to reason about and explain
devices' behaviors to users, and they rely on connectivity to intrusive cloud
infrastructure. Toward addressing these issues, we propose thoughtful things:
devices that leverage lightweight, on-device language models to take actions
and explain their behaviors in response to unconstrained user commands. We
propose an end-to-end framework that leverages formal modeling, automated
training data synthesis, and generative language models to create devices that
are both capable and thoughtful in the presence of unconstrained user goals and
inquiries. Our framework requires no labeled data and can be deployed
on-device, with no cloud dependency. We implement two thoughtful things (a lamp
and a thermostat) and deploy them on real hardware, evaluating their practical
performance.

摘要：

##### **SocialFormer: Social Interaction Modeling with Edge-enhanced Heterogeneous Graph Transformers for Trajectory Prediction**
2405.03809v1 by Zixu Wang, Zhigang Sun, Juergen Luettin, Lavdim Halilaj

Accurate trajectory prediction is crucial for ensuring safe and efficient
autonomous driving. However, most existing methods overlook complex
interactions between traffic participants that often govern their future
trajectories. In this paper, we propose SocialFormer, an agent
interaction-aware trajectory prediction method that leverages the semantic
relationship between the target vehicle and surrounding vehicles by making use
of the road topology. We also introduce an edge-enhanced heterogeneous graph
transformer (EHGT) as the aggregator in a graph neural network (GNN) to encode
the semantic and spatial agent interaction information. Additionally, we
introduce a temporal encoder based on gated recurrent units (GRU) to model the
temporal social behavior of agent movements. Finally, we present an information
fusion framework that integrates agent encoding, lane encoding, and agent
interaction encoding for a holistic representation of the traffic scene. We
evaluate SocialFormer for the trajectory prediction task on the popular
nuScenes benchmark and achieve state-of-the-art performance.

摘要：

##### **Synthetic Data from Diffusion Models Improve Drug Discovery Prediction**
2405.03799v1 by Bing Hu, Ashish Saragadam, Anita Layton, Helen Chen

Artificial intelligence (AI) is increasingly used in every stage of drug
development. Continuing breakthroughs in AI-based methods for drug discovery
require the creation, improvement, and refinement of drug discovery data. We
posit a new data challenge that slows the advancement of drug discovery AI:
datasets are often collected independently from each other, often with little
overlap, creating data sparsity. Data sparsity makes data curation difficult
for researchers looking to answer key research questions requiring values posed
across multiple datasets. We propose a novel diffusion GNN model Syngand
capable of generating ligand and pharmacokinetic data end-to-end. We show and
provide a methodology for sampling pharmacokinetic data for existing ligands
using our Syngand model. We show the initial promising results on the efficacy
of the Syngand-generated synthetic target property data on downstream
regression tasks with AqSolDB, LD50, and hERG central. Using our proposed model
and methodology, researchers can easily generate synthetic ligand data to help
them explore research questions that require data spanning multiple datasets.

摘要：

##### **Detecting Anti-Semitic Hate Speech using Transformer-based Large Language Models**
2405.03794v1 by Dengyi Liu, Minghao Wang, Andrew G. Catlin

Academic researchers and social media entities grappling with the
identification of hate speech face significant challenges, primarily due to the
vast scale of data and the dynamic nature of hate speech. Given the ethical and
practical limitations of large predictive models like ChatGPT in directly
addressing such sensitive issues, our research has explored alternative
advanced transformer-based and generative AI technologies since 2019.
Specifically, we developed a new data labeling technique and established a
proof of concept targeting anti-Semitic hate speech, utilizing a variety of
transformer models such as BERT (arXiv:1810.04805), DistillBERT
(arXiv:1910.01108), RoBERTa (arXiv:1907.11692), and LLaMA-2 (arXiv:2307.09288),
complemented by the LoRA fine-tuning approach (arXiv:2106.09685). This paper
delineates and evaluates the comparative efficacy of these cutting-edge methods
in tackling the intricacies of hate speech detection, highlighting the need for
responsible and carefully managed AI applications within sensitive contexts.

摘要：


### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-07**|**Vision Mamba: A Comprehensive Survey and Taxonomy**|Xiao Liu et.al.|[2405.04404v1](http://arxiv.org/abs/2405.04404v1)|[link](https://github.com/lx6c78/vision-mamba-a-comprehensive-survey-and-taxonomy)|
|**2024-05-07**|**Temporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction**|Zhihao Wen et.al.|[2405.04336v1](http://arxiv.org/abs/2405.04336v1)|null|
|**2024-05-07**|**A New Dataset and Comparative Study for Aphid Cluster Detection and Segmentation in Sorghum Fields**|Raiyan Rahman et.al.|[2405.04305v1](http://arxiv.org/abs/2405.04305v1)|null|
|**2024-05-07**|**MFA-Net: Multi-Scale feature fusion attention network for liver tumor segmentation**|Yanli Yuan et.al.|[2405.04064v1](http://arxiv.org/abs/2405.04064v1)|null|
|**2024-05-07**|**ERATTA: Extreme RAG for Table To Answers with Large Language Models**|Sohini Roychowdhury et.al.|[2405.03963v1](http://arxiv.org/abs/2405.03963v1)|null|
|**2024-05-07**|**Predictive Modeling with Temporal Graphical Representation on Electronic Health Records**|Jiayuan Chen et.al.|[2405.03943v1](http://arxiv.org/abs/2405.03943v1)|[link](https://github.com/the-real-jerrychen/trans)|
|**2024-05-06**|**GREEN: Generative Radiology Report Evaluation and Error Notation**|Sophie Ostmeier et.al.|[2405.03595v1](http://arxiv.org/abs/2405.03595v1)|null|
|**2024-05-06**|**RepVGG-GELAN: Enhanced GELAN with VGG-STYLE ConvNets for Brain Tumour Detection**|Thennarasi Balakrishnan et.al.|[2405.03541v1](http://arxiv.org/abs/2405.03541v1)|[link](https://github.com/thensib/repvgg-gelan)|
|**2024-05-06**|**A Lightweight Neural Architecture Search Model for Medical Image Classification**|Lunchen Xie et.al.|[2405.03462v1](http://arxiv.org/abs/2405.03462v1)|null|
|**2024-05-06**|**Automated Computation of Therapies Using Failure Mode and Effects Analysis in the Medical Domain**|Malte Luttermann et.al.|[2405.03406v1](http://arxiv.org/abs/2405.03406v1)|null|
|**2024-05-06**|**MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models in the Context of the Pediatric Hypertension Guideline**|Mohamed Yaseen Jabarulla et.al.|[2405.03359v1](http://arxiv.org/abs/2405.03359v1)|[link](https://github.com/yaseen28/meddoc-bot)|
|**2024-05-06**|**Accelerated MR Cholangiopancreatography with Deep Learning-based Reconstruction**|Jinho Kim et.al.|[2405.03732v1](http://arxiv.org/abs/2405.03732v1)|null|
|**2024-05-06**|**Artificial Intelligence in the Autonomous Navigation of Endovascular Interventions: A Systematic Review**|Harry Robertshaw et.al.|[2405.03305v1](http://arxiv.org/abs/2405.03305v1)|null|
|**2024-05-06**|**Large Language Models Synergize with Automated Machine Learning**|Jinglue Xu et.al.|[2405.03727v1](http://arxiv.org/abs/2405.03727v1)|null|
|**2024-05-06**|**Advancing Multimodal Medical Capabilities of Gemini**|Lin Yang et.al.|[2405.03162v1](http://arxiv.org/abs/2405.03162v1)|null|
|**2024-05-06**|**Automatic Ultrasound Curve Angle Measurement via Affinity Clustering for Adolescent Idiopathic Scoliosis Evaluation**|Yihao Zhou et.al.|[2405.03141v2](http://arxiv.org/abs/2405.03141v2)|null|
|**2024-05-05**|**RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation**|Zelei Cheng et.al.|[2405.03064v1](http://arxiv.org/abs/2405.03064v1)|null|
|**2024-05-05**|**AC-MAMBASEG: An adaptive convolution and Mamba-based architecture for enhanced skin lesion segmentation**|Viet-Thanh Nguyen et.al.|[2405.03011v1](http://arxiv.org/abs/2405.03011v1)|null|
|**2024-05-05**|**Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents**|Junkai Li et.al.|[2405.02957v1](http://arxiv.org/abs/2405.02957v1)|null|
|**2024-05-05**|**Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**|Zhusi Zhong et.al.|[2405.02815v1](http://arxiv.org/abs/2405.02815v1)|null|
|**2024-05-04**|**The Role of AI in Peer Support for Young People: A Study of Preferences for Human- and AI-Generated Responses**|Jordyn Young et.al.|[2405.02711v1](http://arxiv.org/abs/2405.02711v1)|null|
|**2024-05-04**|**MedPromptExtract (Medical Data Extraction Tool): Anonymization and Hi-fidelity Automated data extraction using NLP and prompt engineering**|Roomani Srivastava et.al.|[2405.02664v1](http://arxiv.org/abs/2405.02664v1)|null|
|**2024-05-04**|**A Conformal Prediction Score that is Robust to Label Noise**|Coby Penso et.al.|[2405.02648v1](http://arxiv.org/abs/2405.02648v1)|null|
|**2024-05-04**|**Explainable Interface for Human-Autonomy Teaming: A Survey**|Xiangqi Kong et.al.|[2405.02583v1](http://arxiv.org/abs/2405.02583v1)|null|
|**2024-05-04**|**A Literature Review and Framework for Human Evaluation of Generative Large Language Models in Healthcare**|Thomas Yu Chow Tam et.al.|[2405.02559v1](http://arxiv.org/abs/2405.02559v1)|null|
|**2024-05-03**|**Spatio-Temporal SwinMAE: A Swin Transformer based Multiscale Representation Learner for Temporal Satellite Imagery**|Yohei Nakayama et.al.|[2405.02512v1](http://arxiv.org/abs/2405.02512v1)|null|
|**2024-05-03**|**Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction**|Jiayang Shi et.al.|[2405.02509v1](http://arxiv.org/abs/2405.02509v1)|null|
|**2024-05-03**|**A Survey of Few-Shot Learning for Biomedical Time Series**|Chenqi Li et.al.|[2405.02485v1](http://arxiv.org/abs/2405.02485v1)|null|
|**2024-05-03**|**Generalizing Orthogonalization for Models with Non-linearities**|David Rügamer et.al.|[2405.02475v1](http://arxiv.org/abs/2405.02475v1)|null|
|**2024-05-03**|**Model-based reinforcement learning for protein backbone design**|Frederic Renard et.al.|[2405.01983v1](http://arxiv.org/abs/2405.01983v1)|null|
|**2024-05-03**|**Aloe: A Family of Fine-tuned Open Healthcare LLMs**|Ashwin Kumar Gururajan et.al.|[2405.01886v1](http://arxiv.org/abs/2405.01886v1)|null|
|**2024-05-03**|**Millimeter Wave Radar-based Human Activity Recognition for Healthcare Monitoring Robot**|Zhanzhong Gu et.al.|[2405.01882v1](http://arxiv.org/abs/2405.01882v1)|null|
|**2024-05-03**|**Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders and Identifying Distinct Features**|Chuanbo Hu et.al.|[2405.01799v1](http://arxiv.org/abs/2405.01799v1)|null|
|**2024-05-02**|**Interpretable Vital Sign Forecasting with Model Agnostic Attention Maps**|Yuwei Liu et.al.|[2405.01714v2](http://arxiv.org/abs/2405.01714v2)|null|
|**2024-05-02**|**Long Tail Image Generation Through Feature Space Augmentation and Iterated Learning**|Rafael Elberg et.al.|[2405.01705v1](http://arxiv.org/abs/2405.01705v1)|[link](https://github.com/sugarfreemanatee/feature-space-augmentation-and-iterated-learning)|
|**2024-05-02**|**Automatically Extracting Numerical Results from Randomized Controlled Trials with Large Language Models**|Hye Sun Yun et.al.|[2405.01686v1](http://arxiv.org/abs/2405.01686v1)|[link](https://github.com/hyesunyun/llm-meta-analysis)|
|**2024-05-02**|**Leveraging Prompt-Learning for Structured Information Extraction from Crohn's Disease Radiology Reports in a Low-Resource Language**|Liam Hazan et.al.|[2405.01682v1](http://arxiv.org/abs/2405.01682v1)|null|
|**2024-05-02**|**Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning**|Théo Moutakanni et.al.|[2405.01469v1](http://arxiv.org/abs/2405.01469v1)|null|
|**2024-05-02**|**DMON: A Simple yet Effective Approach for Argument Structure Learning**|Wei Sun et.al.|[2405.01216v1](http://arxiv.org/abs/2405.01216v1)|[link](https://github.com/vrcmf/dmon)|
|**2024-05-01**|**Beyond Human Vision: The Role of Large Vision Language Models in Microscope Image Analysis**|Prateek Verma et.al.|[2405.00876v1](http://arxiv.org/abs/2405.00876v1)|null|
|**2024-05-01**|**"I'm Not Sure, But...": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust**|Sunnie S. Y. Kim et.al.|[2405.00623v1](http://arxiv.org/abs/2405.00623v1)|null|
|**2024-05-01**|**Enhancing Surgical Robots with Embodied Intelligence for Autonomous Ultrasound Scanning**|Huan Xu et.al.|[2405.00461v1](http://arxiv.org/abs/2405.00461v1)|null|
|**2024-05-01**|**A Careful Examination of Large Language Model Performance on Grade School Arithmetic**|Hugh Zhang et.al.|[2405.00332v3](http://arxiv.org/abs/2405.00332v3)|null|
|**2024-05-01**|**Active Learning with Task Adaptation Pre-training for Speech Emotion Recognition**|Dongyuan Li et.al.|[2405.00307v1](http://arxiv.org/abs/2405.00307v1)|[link](https://github.com/clearloveyuan/after)|
|**2024-04-30**|**Quantifying Nematodes through Images: Datasets, Models, and Baselines of Deep Learning**|Zhipeng Yuan et.al.|[2404.19748v1](http://arxiv.org/abs/2404.19748v1)|null|
|**2024-04-30**|**Data Set Terminology of Artificial Intelligence in Medicine: A Historical Review and Recommendation**|Shannon L. Walston et.al.|[2404.19303v1](http://arxiv.org/abs/2404.19303v1)|null|
|**2024-04-29**|**Text and Audio Simplification: Human vs. ChatGPT**|Gondy Leroy et.al.|[2405.01592v1](http://arxiv.org/abs/2405.01592v1)|null|
|**2024-04-29**|**ConPro: Learning Severity Representation for Medical Images using Contrastive Learning and Preference Optimization**|Hong Nguyen et.al.|[2404.18831v1](http://arxiv.org/abs/2404.18831v1)|[link](https://github.com/hong7cong/conpro)|
|**2024-04-29**|**Decoding Radiologists' Intentions: A Novel System for Accurate Region Identification in Chest X-ray Image Analysis**|Akash Awasthi et.al.|[2404.18981v1](http://arxiv.org/abs/2404.18981v1)|null|
|**2024-04-29**|**Foundations of Multisensory Artificial Intelligence**|Paul Pu Liang et.al.|[2404.18976v1](http://arxiv.org/abs/2404.18976v1)|null|
|**2024-04-29**|**M3H: Multimodal Multitask Machine Learning for Healthcare**|Dimitris Bertsimas et.al.|[2404.18975v1](http://arxiv.org/abs/2404.18975v1)|null|
|**2024-04-29**|**Real Time Multi Organ Classification on Computed Tomography Images**|Halid Ziya Yerebakan et.al.|[2404.18731v1](http://arxiv.org/abs/2404.18731v1)|null|
|**2024-04-29**|**Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology with General-Domain Large Language Model**|Seonhee Cho et.al.|[2405.01591v1](http://arxiv.org/abs/2405.01591v1)|null|
|**2024-04-29**|**Machine Learning for Quantum Computing Specialists**|Daniel Goldsmith et.al.|[2404.18555v1](http://arxiv.org/abs/2404.18555v1)|null|
|**2024-04-29**|**GPT-4 passes most of the 297 written Polish Board Certification Examinations**|Jakub Pokrywka et.al.|[2405.01589v1](http://arxiv.org/abs/2405.01589v1)|null|
|**2024-04-29**|**On the Impact of Data Heterogeneity in Federated Learning Environments with Application to Healthcare Networks**|Usevalad Milasheuski. Luca Barbieri et.al.|[2404.18519v2](http://arxiv.org/abs/2404.18519v2)|null|
|**2024-04-29**|**Research on Intelligent Aided Diagnosis System of Medical Image Based on Computer Deep Learning**|Jiajie Yuan et.al.|[2404.18419v1](http://arxiv.org/abs/2404.18419v1)|null|
|**2024-04-29**|**Capabilities of Gemini Models in Medicine**|Khaled Saab et.al.|[2404.18416v2](http://arxiv.org/abs/2404.18416v2)|null|
|**2024-04-28**|**Permutation-equivariant quantum convolutional neural networks**|Sreetama Das et.al.|[2404.18198v1](http://arxiv.org/abs/2404.18198v1)|null|
|**2024-04-27**|**MediFact at MEDIQA-CORR 2024: Why AI Needs a Human Touch**|Nadia Saeed et.al.|[2404.17999v1](http://arxiv.org/abs/2404.17999v1)|[link](https://github.com/nadiasaeed/medifact-mediqa-corr-2024)|
|**2024-04-27**|**MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology with Multimodal Learning**|Nadia Saeed et.al.|[2405.01583v1](http://arxiv.org/abs/2405.01583v1)|null|
|**2024-04-27**|**Advancing Healthcare Automation: Multi-Agent Systems for Medical Necessity Justification**|Himanshu Pandey et.al.|[2404.17977v1](http://arxiv.org/abs/2404.17977v1)|null|
|**2024-04-27**|**Pre-training on High Definition X-ray Images: An Experimental Study**|Xiao Wang et.al.|[2404.17926v1](http://arxiv.org/abs/2404.17926v1)|[link](https://github.com/event-ahu/medical_image_analysis)|
|**2024-04-27**|**SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models**|Manav Nitin Kapadnis et.al.|[2404.17912v1](http://arxiv.org/abs/2404.17912v1)|null|
|**2024-04-27**|**GLIMS: Attention-Guided Lightweight Multi-Scale Hybrid Network for Volumetric Semantic Segmentation**|Ziya Ata Yazıcı et.al.|[2404.17854v1](http://arxiv.org/abs/2404.17854v1)|[link](https://github.com/yaziciz/GLIMS)|
|**2024-04-27**|**Multimodal Fusion on Low-quality Data: A Comprehensive Survey**|Qingyang Zhang et.al.|[2404.18947v2](http://arxiv.org/abs/2404.18947v2)|null|
|**2024-04-27**|**Evaluating the Application of ChatGPT in Outpatient Triage Guidance: A Comparative Study**|Dou Liu et.al.|[2405.00728v1](http://arxiv.org/abs/2405.00728v1)|null|
|**2024-04-27**|**UMass-BioNLP at MEDIQA-M3G 2024: DermPrompt -- A Systematic Exploration of Prompt Engineering with GPT-4V for Dermatological Diagnosis**|Parth Vashisht et.al.|[2404.17749v1](http://arxiv.org/abs/2404.17749v1)|[link](https://github.com/parth166/m3g-clinicaldermatology)|
|**2024-04-26**|**Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**|Francesco Prinzi et.al.|[2405.02334v1](http://arxiv.org/abs/2405.02334v1)|null|
|**2024-04-26**|**Domain Adaptive and Fine-grained Anomaly Detection for Single-cell Sequencing Data and Beyond**|Kaichen Xu et.al.|[2404.17454v2](http://arxiv.org/abs/2404.17454v2)|[link](https://github.com/catchxu/acsleuth)|
|**2024-04-26**|**M3BAT: Unsupervised Domain Adaptation for Multimodal Mobile Sensing with Multi-Branch Adversarial Training**|Lakmal Meegahapola et.al.|[2404.17391v1](http://arxiv.org/abs/2404.17391v1)|null|
|**2024-04-26**|**Prevalent Frequency of Emotional and Physical Symptoms in Social Anxiety using Zero Shot Classification: An Observational Study**|Muhammad Rizwan et.al.|[2404.17183v1](http://arxiv.org/abs/2404.17183v1)|null|
|**2024-04-26**|**Deep Evidential Learning for Dose Prediction**|Hai Siong Tan et.al.|[2404.17126v1](http://arxiv.org/abs/2404.17126v1)|null|
|**2024-04-25**|**Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**|Yunfei Ge et.al.|[2404.16957v1](http://arxiv.org/abs/2404.16957v1)|null|
|**2024-04-25**|**Taming False Positives in Out-of-Distribution Detection with Human Feedback**|Harit Vishwakarma et.al.|[2404.16954v1](http://arxiv.org/abs/2404.16954v1)|[link](https://github.com/2454511550lin/tamefalsepositives-ood)|
|**2024-04-25**|**Uncovering Deceptive Tendencies in Language Models: A Simulated Company AI Assistant**|Olli Järviniemi et.al.|[2405.01576v1](http://arxiv.org/abs/2405.01576v1)|null|
|**2024-04-25**|**Features Fusion for Dual-View Mammography Mass Detection**|Arina Varlamova et.al.|[2404.16718v1](http://arxiv.org/abs/2404.16718v1)|null|
|**2024-04-25**|**Report on Candidate Computational Indicators for Conscious Valenced Experience**|Andres Campero et.al.|[2404.16696v1](http://arxiv.org/abs/2404.16696v1)|null|
|**2024-04-25**|**Large Language Models in Healthcare: A Comprehensive Benchmark**|Andrew Liu et.al.|[2405.00716v1](http://arxiv.org/abs/2405.00716v1)|null|
|**2024-04-25**|**Utilizing Large Language Models to Identify Reddit Users Considering Vaping Cessation for Digital Interventions**|Sai Krishna Revanth Vuruma et.al.|[2404.17607v1](http://arxiv.org/abs/2404.17607v1)|null|
|**2024-04-25**|**Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation**|Hanyin Wang et.al.|[2405.00715v1](http://arxiv.org/abs/2405.00715v1)|null|
|**2024-04-25**|**ProbGate at EHRSQL 2024: Enhancing SQL Query Generation Accuracy through Probabilistic Threshold Filtering and Error Handling**|Sangryul Kim et.al.|[2404.16659v1](http://arxiv.org/abs/2404.16659v1)|[link](https://github.com/venzino-han/probgate_ehrsql)|
|**2024-04-25**|**Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare**|Emre Can Acikgoz et.al.|[2404.16621v1](http://arxiv.org/abs/2404.16621v1)|[link](https://github.com/hiyouga/llama-factory)|
|**2024-04-25**|**DiffSeg: A Segmentation Model for Skin Lesions Based on Diffusion Difference**|Zhihao Shuai et.al.|[2404.16474v1](http://arxiv.org/abs/2404.16474v1)|null|
|**2024-04-25**|**Light-weight Retinal Layer Segmentation with Global Reasoning**|Xiang He et.al.|[2404.16346v1](http://arxiv.org/abs/2404.16346v1)|null|
|**2024-04-25**|**Semantic Segmentation Refiner for Ultrasound Applications with Zero-Shot Foundation Models**|Hedda Cohen Indelman et.al.|[2404.16325v1](http://arxiv.org/abs/2404.16325v1)|null|
|**2024-04-25**|**LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications**|Saranya Krishnamoorthy et.al.|[2404.16294v1](http://arxiv.org/abs/2404.16294v1)|[link](https://github.com/inqbator-evicore/llm_section_identifiers)|
|**2024-04-24**|**Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions**|Divyansh Agarwal et.al.|[2404.16251v2](http://arxiv.org/abs/2404.16251v2)|null|
|**2024-04-24**|**ABCD: Trust enhanced Attention based Convolutional Autoencoder for Risk Assessment**|Sarala Naidu et.al.|[2404.16183v1](http://arxiv.org/abs/2404.16183v1)|null|
|**2024-04-24**|**Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges**|Badri Narayana Patro et.al.|[2404.16112v1](http://arxiv.org/abs/2404.16112v1)|[link](https://github.com/badripatro/mamba360)|
|**2024-04-24**|**Mammo-CLIP: Leveraging Contrastive Language-Image Pre-training (CLIP) for Enhanced Breast Cancer Diagnosis with Multi-view Mammography**|Xuxin Chen et.al.|[2404.15946v1](http://arxiv.org/abs/2404.15946v1)|null|
|**2024-04-24**|**Assessing The Potential Of Mid-Sized Language Models For Clinical QA**|Elliot Bolton et.al.|[2404.15894v1](http://arxiv.org/abs/2404.15894v1)|null|
|**2024-04-24**|**Enhancing Diagnosis through AI-driven Analysis of Reflectance Confocal Microscopy**|Hong-Jun Yoon et.al.|[2404.16080v1](http://arxiv.org/abs/2404.16080v1)|null|
|**2024-04-24**|**A Hybrid Probabilistic Battery Health Management Approach for Robust Inspection Drone Operations**|Jokin Alcibar et.al.|[2405.00055v1](http://arxiv.org/abs/2405.00055v1)|null|
|**2024-04-24**|**Anomaly Detection for Incident Response at Scale**|Hanzhang Wang et.al.|[2404.16887v1](http://arxiv.org/abs/2404.16887v1)|null|
|**2024-04-23**|**Adapting an Artificial Intelligence Sexually Transmitted Diseases Symptom Checker Tool for Mpox Detection: The HeHealth Experience**|Rayner Kay Jin Tan et.al.|[2404.16885v1](http://arxiv.org/abs/2404.16885v1)|null|
|**2024-04-23**|**PRISM: Patient Records Interpretation for Semantic Clinical Trial Matching using Large Language Models**|Shashi Kant Gupta et.al.|[2404.15549v2](http://arxiv.org/abs/2404.15549v2)|null|
|**2024-04-23**|**Multi-scale Intervention Planning based on Generative Design**|Ioannis Kavouras et.al.|[2404.15492v1](http://arxiv.org/abs/2404.15492v1)|null|
|**2024-04-23**|**IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection & Correction Task On the Shoulders of Medical Agents**|Jean-Philippe Corbeil et.al.|[2404.15488v1](http://arxiv.org/abs/2404.15488v1)|[link](https://github.com/microsoft/iryonlp-mediqa-corr-2024)|
|**2024-04-23**|**Interactive Analysis of LLMs using Meaningful Counterfactuals**|Furui Cheng et.al.|[2405.00708v1](http://arxiv.org/abs/2405.00708v1)|null|

#### Abstracts
##### **Vision Mamba: A Comprehensive Survey and Taxonomy**
2405.04404v1 by Xiao Liu, Chenxu Zhang, Lei Zhang

State Space Model (SSM) is a mathematical model used to describe and analyze
the behavior of dynamic systems. This model has witnessed numerous applications
in several fields, including control theory, signal processing, economics and
machine learning. In the field of deep learning, state space models are used to
process sequence data, such as time series analysis, natural language
processing (NLP) and video understanding. By mapping sequence data to state
space, long-term dependencies in the data can be better captured. In
particular, modern SSMs have shown strong representational capabilities in NLP,
especially in long sequence modeling, while maintaining linear time complexity.
Notably, based on the latest state-space models, Mamba merges time-varying
parameters into SSMs and formulates a hardware-aware algorithm for efficient
training and inference. Given its impressive efficiency and strong long-range
dependency modeling capability, Mamba is expected to become a new AI
architecture that may outperform Transformer. Recently, a number of works have
attempted to study the potential of Mamba in various fields, such as general
vision, multi-modal, medical image analysis and remote sensing image analysis,
by extending Mamba from natural language domain to visual domain. To fully
understand Mamba in the visual domain, we conduct a comprehensive survey and
present a taxonomy study. This survey focuses on Mamba's application to a
variety of visual tasks and data types, and discusses its predecessors, recent
advances and far-reaching impact on a wide range of domains. Since Mamba is now
on an upward trend, please actively notice us if you have new findings, and new
progress on Mamba will be included in this survey in a timely manner and
updated on the Mamba project at
https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy.

摘要：

##### **Temporal and Heterogeneous Graph Neural Network for Remaining Useful Life Prediction**
2405.04336v1 by Zhihao Wen, Yuan Fang, Pengcheng Wei, Fayao Liu, Zhenghua Chen, Min Wu

Predicting Remaining Useful Life (RUL) plays a crucial role in the
prognostics and health management of industrial systems that involve a variety
of interrelated sensors. Given a constant stream of time series sensory data
from such systems, deep learning models have risen to prominence at identifying
complex, nonlinear temporal dependencies in these data. In addition to the
temporal dependencies of individual sensors, spatial dependencies emerge as
important correlations among these sensors, which can be naturally modelled by
a temporal graph that describes time-varying spatial relationships. However,
the majority of existing studies have relied on capturing discrete snapshots of
this temporal graph, a coarse-grained approach that leads to loss of temporal
information. Moreover, given the variety of heterogeneous sensors, it becomes
vital that such inherent heterogeneity is leveraged for RUL prediction in
temporal sensor graphs. To capture the nuances of the temporal and spatial
relationships and heterogeneous characteristics in an interconnected graph of
sensors, we introduce a novel model named Temporal and Heterogeneous Graph
Neural Networks (THGNN). Specifically, THGNN aggregates historical data from
neighboring nodes to accurately capture the temporal dynamics and spatial
correlations within the stream of sensor data in a fine-grained manner.
Moreover, the model leverages Feature-wise Linear Modulation (FiLM) to address
the diversity of sensor types, significantly improving the model's capacity to
learn the heterogeneity in the data sources. Finally, we have validated the
effectiveness of our approach through comprehensive experiments. Our empirical
findings demonstrate significant advancements on the N-CMAPSS dataset,
achieving improvements of up to 19.2% and 31.6% in terms of two different
evaluation metrics over state-of-the-art methods.

摘要：

##### **A New Dataset and Comparative Study for Aphid Cluster Detection and Segmentation in Sorghum Fields**
2405.04305v1 by Raiyan Rahman, Christopher Indris, Goetz Bramesfeld, Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Ivan Grijalva, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang

Aphid infestations are one of the primary causes of extensive damage to wheat
and sorghum fields and are one of the most common vectors for plant viruses,
resulting in significant agricultural yield losses. To address this problem,
farmers often employ the inefficient use of harmful chemical pesticides that
have negative health and environmental impacts. As a result, a large amount of
pesticide is wasted on areas without significant pest infestation. This brings
to attention the urgent need for an intelligent autonomous system that can
locate and spray sufficiently large infestations selectively within the complex
crop canopies. We have developed a large multi-scale dataset for aphid cluster
detection and segmentation, collected from actual sorghum fields and
meticulously annotated to include clusters of aphids. Our dataset comprises a
total of 54,742 image patches, showcasing a variety of viewpoints, diverse
lighting conditions, and multiple scales, highlighting its effectiveness for
real-world applications. In this study, we trained and evaluated four real-time
semantic segmentation models and three object detection models specifically for
aphid cluster segmentation and detection. Considering the balance between
accuracy and efficiency, Fast-SCNN delivered the most effective segmentation
results, achieving 80.46% mean precision, 81.21% mean recall, and 91.66 frames
per second (FPS). For object detection, RT-DETR exhibited the best overall
performance with a 61.63% mean average precision (mAP), 92.6% mean recall, and
72.55 on an NVIDIA V100 GPU. Our experiments further indicate that aphid
cluster segmentation is more suitable for assessing aphid infestations than
using detection models.

摘要：

##### **MFA-Net: Multi-Scale feature fusion attention network for liver tumor segmentation**
2405.04064v1 by Yanli Yuan, Bingbing Wang, Chuan Zhang, Jingyi Xu, Ximeng Liu, Liehuang Zhu

Segmentation of organs of interest in medical CT images is beneficial for
diagnosis of diseases. Though recent methods based on Fully Convolutional
Neural Networks (F-CNNs) have shown success in many segmentation tasks, fusing
features from images with different scales is still a challenge: (1) Due to the
lack of spatial awareness, F-CNNs share the same weights at different spatial
locations. (2) F-CNNs can only obtain surrounding information through local
receptive fields. To address the above challenge, we propose a new segmentation
framework based on attention mechanisms, named MFA-Net (Multi-Scale Feature
Fusion Attention Network). The proposed framework can learn more meaningful
feature maps among multiple scales and result in more accurate automatic
segmentation. We compare our proposed MFA-Net with SOTA methods on two 2D liver
CT datasets. The experimental results show that our MFA-Net produces more
precise segmentation on images with different scales.

摘要：

##### **ERATTA: Extreme RAG for Table To Answers with Large Language Models**
2405.03963v1 by Sohini Roychowdhury, Marko Krema, Anvar Mahammad, Brian Moore, Arijit Mukherjee, Punit Prakashchandra

Large language models (LLMs) with residual augmented-generation (RAG) have
been the optimal choice for scalable generative AI solutions in the recent
past. However, the choice of use-cases that incorporate RAG with LLMs have been
either generic or extremely domain specific, thereby questioning the
scalability and generalizability of RAG-LLM approaches. In this work, we
propose a unique LLM-based system where multiple LLMs can be invoked to enable
data authentication, user query routing, data retrieval and custom prompting
for question answering capabilities from data tables that are highly varying
and large in size. Our system is tuned to extract information from
Enterprise-level data products and furnish real time responses under 10
seconds. One prompt manages user-to-data authentication followed by three
prompts to route, fetch data and generate a customizable prompt natural
language responses. Additionally, we propose a five metric scoring module that
detects and reports hallucinations in the LLM responses. Our proposed system
and scoring metrics achieve >90% confidence scores across hundreds of user
queries in the sustainability, financial health and social media domains.
Extensions to the proposed extreme RAG architectures can enable heterogeneous
source querying using LLMs.

摘要：

##### **Predictive Modeling with Temporal Graphical Representation on Electronic Health Records**
2405.03943v1 by Jiayuan Chen, Changchang Yin, Yuanlong Wang, Ping Zhang

Deep learning-based predictive models, leveraging Electronic Health Records
(EHR), are receiving increasing attention in healthcare. An effective
representation of a patient's EHR should hierarchically encompass both the
temporal relationships between historical visits and medical events, and the
inherent structural information within these elements. Existing patient
representation methods can be roughly categorized into sequential
representation and graphical representation. The sequential representation
methods focus only on the temporal relationships among longitudinal visits. On
the other hand, the graphical representation approaches, while adept at
extracting the graph-structured relationships between various medical events,
fall short in effectively integrate temporal information. To capture both types
of information, we model a patient's EHR as a novel temporal heterogeneous
graph. This graph includes historical visits nodes and medical events nodes. It
propagates structured information from medical event nodes to visit nodes and
utilizes time-aware visit nodes to capture changes in the patient's health
status. Furthermore, we introduce a novel temporal graph transformer (TRANS)
that integrates temporal edge features, global positional encoding, and local
structural encoding into heterogeneous graph convolution, capturing both
temporal and structural information. We validate the effectiveness of TRANS
through extensive experiments on three real-world datasets. The results show
that our proposed approach achieves state-of-the-art performance.

摘要：

##### **GREEN: Generative Radiology Report Evaluation and Error Notation**
2405.03595v1 by Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson, Michael Moseley, Curtis Langlotz, Akshay S Chaudhari, Jean-Benoit Delbrouck

Evaluating radiology reports is a challenging problem as factual correctness
is extremely important due to the need for accurate medical communication about
medical images. Existing automatic evaluation metrics either suffer from
failing to consider factual correctness (e.g., BLEU and ROUGE) or are limited
in their interpretability (e.g., F1CheXpert and F1RadGraph). In this paper, we
introduce GREEN (Generative Radiology Report Evaluation and Error Notation), a
radiology report generation metric that leverages the natural language
understanding of language models to identify and explain clinically significant
errors in candidate reports, both quantitatively and qualitatively. Compared to
current metrics, GREEN offers: 1) a score aligned with expert preferences, 2)
human interpretable explanations of clinically significant errors, enabling
feedback loops with end-users, and 3) a lightweight open-source method that
reaches the performance of commercial counterparts. We validate our GREEN
metric by comparing it to GPT-4, as well as to error counts of 6 experts and
preferences of 2 experts. Our method demonstrates not only higher correlation
with expert error counts, but simultaneously higher alignment with expert
preferences when compared to previous approaches."

摘要：

##### **RepVGG-GELAN: Enhanced GELAN with VGG-STYLE ConvNets for Brain Tumour Detection**
2405.03541v1 by Thennarasi Balakrishnan, Sandeep Singh Sengar

Object detection algorithms particularly those based on YOLO have
demonstrated remarkable efficiency in balancing speed and accuracy. However,
their application in brain tumour detection remains underexplored. This study
proposes RepVGG-GELAN, a novel YOLO architecture enhanced with RepVGG, a
reparameterized convolutional approach for object detection tasks particularly
focusing on brain tumour detection within medical images. RepVGG-GELAN
leverages the RepVGG architecture to improve both speed and accuracy in
detecting brain tumours. Integrating RepVGG into the YOLO framework aims to
achieve a balance between computational efficiency and detection performance.
This study includes a spatial pyramid pooling-based Generalized Efficient Layer
Aggregation Network (GELAN) architecture which further enhances the capability
of RepVGG. Experimental evaluation conducted on a brain tumour dataset
demonstrates the effectiveness of RepVGG-GELAN surpassing existing RCS-YOLO in
terms of precision and speed. Specifically, RepVGG-GELAN achieves an increased
precision of 4.91% and an increased AP50 of 2.54% over the latest existing
approach while operating at 240.7 GFLOPs. The proposed RepVGG-GELAN with GELAN
architecture presents promising results establishing itself as a
state-of-the-art solution for accurate and efficient brain tumour detection in
medical images. The implementation code is publicly available at
https://github.com/ThensiB/RepVGG-GELAN.

摘要：

##### **A Lightweight Neural Architecture Search Model for Medical Image Classification**
2405.03462v1 by Lunchen Xie, Eugenio Lomurno, Matteo Gambella, Danilo Ardagna, Manuel Roveri, Matteo Matteucci, Qingjiang Shi

Accurate classification of medical images is essential for modern
diagnostics. Deep learning advancements led clinicians to increasingly use
sophisticated models to make faster and more accurate decisions, sometimes
replacing human judgment. However, model development is costly and repetitive.
Neural Architecture Search (NAS) provides solutions by automating the design of
deep learning architectures. This paper presents ZO-DARTS+, a differentiable
NAS algorithm that improves search efficiency through a novel method of
generating sparse probabilities by bi-level optimization. Experiments on five
public medical datasets show that ZO-DARTS+ matches the accuracy of
state-of-the-art solutions while reducing search times by up to three times.

摘要：

##### **Automated Computation of Therapies Using Failure Mode and Effects Analysis in the Medical Domain**
2405.03406v1 by Malte Luttermann, Edgar Baake, Juljan Bouchagiar, Benjamin Gebel, Philipp Grüning, Dilini Manikwadura, Franziska Schollemann, Elisa Teifke, Philipp Rostalski, Ralf Möller

Failure mode and effects analysis (FMEA) is a systematic approach to identify
and analyse potential failures and their effects in a system or process. The
FMEA approach, however, requires domain experts to manually analyse the FMEA
model to derive risk-reducing actions that should be applied. In this paper, we
provide a formal framework to allow for automatic planning and acting in FMEA
models. More specifically, we cast the FMEA model into a Markov decision
process which can then be solved by existing solvers. We show that the FMEA
approach can not only be used to support medical experts during the modelling
process but also to automatically derive optimal therapies for the treatment of
patients.

摘要：

##### **MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models in the Context of the Pediatric Hypertension Guideline**
2405.03359v1 by Mohamed Yaseen Jabarulla, Steffen Oeltze-Jafra, Philipp Beerbaum, Theodor Uden

This research focuses on evaluating the non-commercial open-source large
language models (LLMs) Meditron, MedAlpaca, Mistral, and Llama-2 for their
efficacy in interpreting medical guidelines saved in PDF format. As a specific
test scenario, we applied these models to the guidelines for hypertension in
children and adolescents provided by the European Society of Cardiology (ESC).
Leveraging Streamlit, a Python library, we developed a user-friendly medical
document chatbot tool (MedDoc-Bot). This tool enables authorized users to
upload PDF files and pose questions, generating interpretive responses from
four locally stored LLMs. A pediatric expert provides a benchmark for
evaluation by formulating questions and responses extracted from the ESC
guidelines. The expert rates the model-generated responses based on their
fidelity and relevance. Additionally, we evaluated the METEOR and chrF metric
scores to assess the similarity of model responses to reference answers. Our
study found that Llama-2 and Mistral performed well in metrics evaluation.
However, Llama-2 was slower when dealing with text and tabular data. In our
human evaluation, we observed that responses created by Mistral, Meditron, and
Llama-2 exhibited reasonable fidelity and relevance. This study provides
valuable insights into the strengths and limitations of LLMs for future
developments in medical document interpretation. Open-Source Code:
https://github.com/yaseen28/MedDoc-Bot

摘要：

##### **Accelerated MR Cholangiopancreatography with Deep Learning-based Reconstruction**
2405.03732v1 by Jinho Kim, Marcel Dominik Nickel, Florian Knoll

This study accelerates MR cholangiopancreatography (MRCP) acquisitions using
deep learning-based (DL) reconstruction at 3T and 0.55T. Thirty healthy
volunteers underwent conventional two-fold MRCP scans at field strengths of 3T
or 0.55T. We trained a variational network (VN) using retrospectively six-fold
undersampled data obtained at 3T. We then evaluated our method against standard
techniques such as parallel imaging (PI) and compressed sensing (CS), focusing
on peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) as
metrics. Furthermore, considering acquiring fully-sampled MRCP is impractical,
we added a self-supervised DL reconstruction (SSDU) to the evaluating group. We
also tested our method in a prospective accelerated scenario to reflect
real-world clinical applications and evaluated its adaptability to MRCP at
0.55T. Our method demonstrated a remarkable reduction of average acquisition
time from 599/542 to 255/180 seconds for MRCP at 3T/0.55T. In both
retrospective and prospective undersampling scenarios, the PSNR and SSIM of VN
were higher than those of PI, CS, and SSDU. At the same time, VN preserved the
image quality of undersampled data, i.e., sharpness and the visibility of
hepatobiliary ducts. In addition, VN also produced high quality reconstructions
at 0.55T resulting in the highest PSNR and SSIM. In summary, VN trained for
highly accelerated MRCP allows to reduce the acquisition time by a factor of
2.4/3.0 at 3T/0.55T while maintaining the image quality of the conventional
acquisition.

摘要：

##### **Artificial Intelligence in the Autonomous Navigation of Endovascular Interventions: A Systematic Review**
2405.03305v1 by Harry Robertshaw, Lennart Karstensen, Benjamin Jackson, Hadi Sadati, Kawal Rhode, Sebastien Ourselin, Alejandro Granados, Thomas C Booth

Purpose: Autonomous navigation of devices in endovascular interventions can
decrease operation times, improve decision-making during surgery, and reduce
operator radiation exposure while increasing access to treatment. This
systematic review explores recent literature to assess the impact, challenges,
and opportunities artificial intelligence (AI) has for the autonomous
endovascular intervention navigation.
  Methods: PubMed and IEEEXplore databases were queried. Eligibility criteria
included studies investigating the use of AI in enabling the autonomous
navigation of catheters/guidewires in endovascular interventions. Following
PRISMA, articles were assessed using QUADAS-2. PROSPERO: CRD42023392259.
  Results: Among 462 studies, fourteen met inclusion criteria. Reinforcement
learning (9/14, 64%) and learning from demonstration (7/14, 50%) were used as
data-driven models for autonomous navigation. Studies predominantly utilised
physical phantoms (10/14, 71%) and in silico (4/14, 29%) models. Experiments
within or around the blood vessels of the heart were reported by the majority
of studies (10/14, 71%), while simple non-anatomical vessel platforms were used
in three studies (3/14, 21%), and the porcine liver venous system in one study.
We observed that risk of bias and poor generalisability were present across
studies. No procedures were performed on patients in any of the studies
reviewed. Studies lacked patient selection criteria, reference standards, and
reproducibility, resulting in low clinical evidence levels.
  Conclusions: AI's potential in autonomous endovascular navigation is
promising, but in an experimental proof-of-concept stage, with a technology
readiness level of 3. We highlight that reference standards with
well-identified performance metrics are crucial to allow for comparisons of
data-driven algorithms proposed in the years to come.

摘要：

##### **Large Language Models Synergize with Automated Machine Learning**
2405.03727v1 by Jinglue Xu, Zhen Liu, Nagar Anthel Venkatesh Suryanarayanan, Hitoshi Iba

Recently, code generation driven by large language models (LLMs) has become
increasingly popular. However, automatically generating code for machine
learning (ML) tasks still poses significant challenges. This paper explores the
limits of program synthesis for ML by combining LLMs and automated machine
learning (autoML). Specifically, our goal is to fully automate the code
generation process for the entire ML workflow, from data preparation to
modeling and post-processing, utilizing only textual descriptions of the ML
tasks. To manage the length and diversity of ML programs, we propose to break
each ML program into smaller, manageable parts. Each part is generated
separately by the LLM, with careful consideration of their compatibilities. To
implement the approach, we design a testing technique for ML programs.
Furthermore, our approach enables integration with autoML. In our approach,
autoML serves to numerically assess and optimize the ML programs generated by
LLMs. LLMs, in turn, help to bridge the gap between theoretical,
algorithm-centered autoML and practical autoML applications. This mutual
enhancement underscores the synergy between LLMs and autoML in program
synthesis for ML. In experiments across various ML tasks, our method
outperforms existing methods in 10 out of 12 tasks for generating ML programs.
In addition, autoML significantly improves the performance of the generated ML
programs. In the experiments, our method, Text-to-ML, achieves fully automated
synthesis of the entire ML pipeline based solely on textual descriptions of the
ML tasks.

摘要：

##### **Advancing Multimodal Medical Capabilities of Gemini**
2405.03162v1 by Lin Yang, Shawn Xu, Andrew Sellergren, Timo Kohlberger, Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroensri, Eric Wang, Ellery Wulczyn, Fayaz Jamil, Theo Guidroz, Chuck Lau, Siyuan Qiao, Yun Liu, Akshay Goel, Kendall Park, Arnav Agharwal, Nick George, Yang Wang, Ryutaro Tanno, David G. T. Barrett, Wei-Hung Weng, S. Sara Mahdavi, Khaled Saab, Tao Tu, Sreenivasa Raju Kalidindi, Mozziyar Etemadi, Jorge Cuadros, Gregory Sorensen, Yossi Matias, Katherine Chou, Greg Corrado, Joelle Barral, Shravya Shetty, David Fleet, S. M. Ali Eslami, Daniel Tse, Shruthi Prabhakara, Cory McLean, Dave Steiner, Rory Pilgrim, Christopher Kelly, Shekoofeh Azizi, Daniel Golden

Many clinical tasks require an understanding of specialized data, such as
medical images and genomics, which is not typically found in general-purpose
large multimodal models. Building upon Gemini's multimodal models, we develop
several models within the new Med-Gemini family that inherit core capabilities
of Gemini and are optimized for medical use via fine-tuning with 2D and 3D
radiology, histopathology, ophthalmology, dermatology and genomic data.
Med-Gemini-2D sets a new standard for AI-based chest X-ray (CXR) report
generation based on expert evaluation, exceeding previous best results across
two separate datasets by an absolute margin of 1% and 12%, where 57% and 96% of
AI reports on normal cases, and 43% and 65% on abnormal cases, are evaluated as
"equivalent or better" than the original radiologists' reports. We demonstrate
the first ever large multimodal model-based report generation for 3D computed
tomography (CT) volumes using Med-Gemini-3D, with 53% of AI reports considered
clinically acceptable, although additional research is needed to meet expert
radiologist reporting quality. Beyond report generation, Med-Gemini-2D
surpasses the previous best performance in CXR visual question answering (VQA)
and performs well in CXR classification and radiology VQA, exceeding SoTA or
baselines on 17 of 20 tasks. In histopathology, ophthalmology, and dermatology
image classification, Med-Gemini-2D surpasses baselines across 18 out of 20
tasks and approaches task-specific model performance. Beyond imaging,
Med-Gemini-Polygenic outperforms the standard linear polygenic risk score-based
approach for disease risk prediction and generalizes to genetically correlated
diseases for which it has never been trained. Although further development and
evaluation are necessary in the safety-critical medical domain, our results
highlight the potential of Med-Gemini across a wide range of medical tasks.

摘要：

##### **Automatic Ultrasound Curve Angle Measurement via Affinity Clustering for Adolescent Idiopathic Scoliosis Evaluation**
2405.03141v2 by Yihao Zhou, Timothy Tin-Yan Lee, Kelly Ka-Lee Lai, Chonglin Wu, Hin Ting Lau, De Yang, Chui-Yi Chan, Winnie Chiu-Wing Chu, Jack Chun-Yiu Cheng, Tsz-Ping Lam, Yong-Ping Zheng

The current clinical gold standard for evaluating adolescent idiopathic
scoliosis (AIS) is X-ray radiography, using Cobb angle measurement. However,
the frequent monitoring of the AIS progression using X-rays poses a challenge
due to the cumulative radiation exposure. Although 3D ultrasound has been
validated as a reliable and radiation-free alternative for scoliosis
assessment, the process of measuring spinal curvature is still carried out
manually. Consequently, there is a considerable demand for a fully automatic
system that can locate bony landmarks and perform angle measurements. To this
end, we introduce an estimation model for automatic ultrasound curve angle
(UCA) measurement. The model employs a dual-branch network to detect candidate
landmarks and perform vertebra segmentation on ultrasound coronal images. An
affinity clustering strategy is utilized within the vertebral segmentation area
to illustrate the affinity relationship between candidate landmarks.
Subsequently, we can efficiently perform line delineation from a clustered
affinity map for UCA measurement. As our method is specifically designed for
UCA calculation, this method outperforms other state-of-the-art methods for
landmark and line detection tasks. The high correlation between the automatic
UCA and Cobb angle (R$^2$=0.858) suggests that our proposed method can
potentially replace manual UCA measurement in ultrasound scoliosis assessment.

摘要：

##### **RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation**
2405.03064v1 by Zelei Cheng, Xian Wu, Jiahao Yu, Sabrina Yang, Gang Wang, Xinyu Xing

Deep reinforcement learning (DRL) is playing an increasingly important role
in real-world applications. However, obtaining an optimally performing DRL
agent for complex tasks, especially with sparse rewards, remains a significant
challenge. The training of a DRL agent can be often trapped in a bottleneck
without further progress. In this paper, we propose RICE, an innovative
refining scheme for reinforcement learning that incorporates explanation
methods to break through the training bottlenecks. The high-level idea of RICE
is to construct a new initial state distribution that combines both the default
initial states and critical states identified through explanation methods,
thereby encouraging the agent to explore from the mixed initial states. Through
careful design, we can theoretically guarantee that our refining scheme has a
tighter sub-optimality bound. We evaluate RICE in various popular RL
environments and real-world applications. The results demonstrate that RICE
significantly outperforms existing refining schemes in enhancing agent
performance.

摘要：

##### **AC-MAMBASEG: An adaptive convolution and Mamba-based architecture for enhanced skin lesion segmentation**
2405.03011v1 by Viet-Thanh Nguyen, Van-Truong Pham, Thi-Thao Tran

Skin lesion segmentation is a critical task in computer-aided diagnosis
systems for dermatological diseases. Accurate segmentation of skin lesions from
medical images is essential for early detection, diagnosis, and treatment
planning. In this paper, we propose a new model for skin lesion segmentation
namely AC-MambaSeg, an enhanced model that has the hybrid CNN-Mamba backbone,
and integrates advanced components such as Convolutional Block Attention Module
(CBAM), Attention Gate, and Selective Kernel Bottleneck. AC-MambaSeg leverages
the Vision Mamba framework for efficient feature extraction, while CBAM and
Selective Kernel Bottleneck enhance its ability to focus on informative regions
and suppress background noise. We evaluate the performance of AC-MambaSeg on
diverse datasets of skin lesion images including ISIC-2018 and PH2; then
compare it against existing segmentation methods. Our model shows promising
potential for improving computer-aided diagnosis systems and facilitating early
detection and treatment of dermatological diseases. Our source code will be
made available at: https://github.com/vietthanh2710/AC-MambaSeg.

摘要：

##### **Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents**
2405.02957v1 by Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, Yang Liu

In this paper, we introduce a simulacrum of hospital called Agent Hospital
that simulates the entire process of treating illness. All patients, nurses,
and doctors are autonomous agents powered by large language models (LLMs). Our
central goal is to enable a doctor agent to learn how to treat illness within
the simulacrum. To do so, we propose a method called MedAgent-Zero. As the
simulacrum can simulate disease onset and progression based on knowledge bases
and LLMs, doctor agents can keep accumulating experience from both successful
and unsuccessful cases. Simulation experiments show that the treatment
performance of doctor agents consistently improves on various tasks. More
interestingly, the knowledge the doctor agents have acquired in Agent Hospital
is applicable to real-world medicare benchmarks. After treating around ten
thousand patients (real-world doctors may take over two years), the evolved
doctor agent achieves a state-of-the-art accuracy of 93.06% on a subset of the
MedQA dataset that covers major respiratory diseases. This work paves the way
for advancing the applications of LLM-powered agent techniques in medical
scenarios.

摘要：

##### **Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**
2405.02815v1 by Zhusi Zhong, Jie Li, Zhuoqi Ma, Scott Collins, Harrison Bai, Paul Zhang, Terrance Healey, Xinbo Gao, Michael K. Atalay, Zhicheng Jiao

The COVID-19 pandemic has strained global public health, necessitating
accurate diagnosis and intervention to control disease spread and reduce
mortality rates. This paper introduces an interpretable deep survival
prediction model designed specifically for improved understanding and trust in
COVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale
pretrained image encoder, Risk-specific Grad-CAM, and anatomical region
detection techniques, our approach produces regional interpretable outcomes
that effectively capture essential disease features while focusing on rare but
critical abnormal regions. Our model's predictive results provide enhanced
clarity and transparency through risk area localization, enabling clinicians to
make informed decisions regarding COVID-19 diagnosis with better understanding
of prognostic insights. We evaluate the proposed method on a multi-center
survival dataset and demonstrate its effectiveness via quantitative and
qualitative assessments, achieving superior C-indexes (0.764 and 0.727) and
time-dependent AUCs (0.799 and 0.691). These results suggest that our
explainable deep survival prediction model surpasses traditional survival
analysis methods in risk prediction, improving interpretability for clinical
decision making and enhancing AI system trustworthiness.

摘要：

##### **The Role of AI in Peer Support for Young People: A Study of Preferences for Human- and AI-Generated Responses**
2405.02711v1 by Jordyn Young, Laala M Jawara, Diep N Nguyen, Brian Daly, Jina Huh-Yoo, Afsaneh Razi

Generative Artificial Intelligence (AI) is integrated into everyday
technology, including news, education, and social media. AI has further
pervaded private conversations as conversational partners, auto-completion, and
response suggestions. As social media becomes young people's main method of
peer support exchange, we need to understand when and how AI can facilitate and
assist in such exchanges in a beneficial, safe, and socially appropriate way.
We asked 622 young people to complete an online survey and evaluate blinded
human- and AI-generated responses to help-seeking messages. We found that
participants preferred the AI-generated response to situations about
relationships, self-expression, and physical health. However, when addressing a
sensitive topic, like suicidal thoughts, young people preferred the human
response. We also discuss the role of training in online peer support exchange
and its implications for supporting young people's well-being. Disclaimer: This
paper includes sensitive topics, including suicide ideation. Reader discretion
is advised.

摘要：

##### **MedPromptExtract (Medical Data Extraction Tool): Anonymization and Hi-fidelity Automated data extraction using NLP and prompt engineering**
2405.02664v1 by Roomani Srivastava, Suraj Prasad, Lipika Bhat, Sarvesh Deshpande, Barnali Das, Kshitij Jadhav

A major roadblock in the seamless digitization of medical records remains the
lack of interoperability of existing records. Extracting relevant medical
information required for further treatment planning or even research is a time
consuming labour intensive task involving the much valuable time of doctors. In
this demo paper we present, MedPromptExtract an automated tool using a
combination of semi supervised learning, large language models, natural
lanuguage processing and prompt engineering to convert unstructured medical
records to structured data which is amenable to further analysis.

摘要：

##### **A Conformal Prediction Score that is Robust to Label Noise**
2405.02648v1 by Coby Penso, Jacob Goldberger

Conformal Prediction (CP) quantifies network uncertainty by building a small
prediction set with a pre-defined probability that the correct class is within
this set. In this study we tackle the problem of CP calibration based on a
validation set with noisy labels. We introduce a conformal score that is robust
to label noise. The noise-free conformal score is estimated using the noisy
labeled data and the noise level. In the test phase the noise-free score is
used to form the prediction set. We applied the proposed algorithm to several
standard medical imaging classification datasets. We show that our method
outperforms current methods by a large margin, in terms of the average size of
the prediction set, while maintaining the required coverage.

摘要：

##### **Explainable Interface for Human-Autonomy Teaming: A Survey**
2405.02583v1 by Xiangqi Kong, Yang Xing, Antonios Tsourdos, Ziyue Wang, Weisi Guo, Adolfo Perrusquia, Andreas Wikander

Nowadays, large-scale foundation models are being increasingly integrated
into numerous safety-critical applications, including human-autonomy teaming
(HAT) within transportation, medical, and defence domains. Consequently, the
inherent 'black-box' nature of these sophisticated deep neural networks
heightens the significance of fostering mutual understanding and trust between
humans and autonomous systems. To tackle the transparency challenges in HAT,
this paper conducts a thoughtful study on the underexplored domain of
Explainable Interface (EI) in HAT systems from a human-centric perspective,
thereby enriching the existing body of research in Explainable Artificial
Intelligence (XAI). We explore the design, development, and evaluation of EI
within XAI-enhanced HAT systems. To do so, we first clarify the distinctions
between these concepts: EI, explanations and model explainability, aiming to
provide researchers and practitioners with a structured understanding. Second,
we contribute to a novel framework for EI, addressing the unique challenges in
HAT. Last, our summarized evaluation framework for ongoing EI offers a holistic
perspective, encompassing model performance, human-centered factors, and group
task objectives. Based on extensive surveys across XAI, HAT, psychology, and
Human-Computer Interaction (HCI), this review offers multiple novel insights
into incorporating XAI into HAT systems and outlines future directions.

摘要：

##### **A Literature Review and Framework for Human Evaluation of Generative Large Language Models in Healthcare**
2405.02559v1 by Thomas Yu Chow Tam, Sonish Sivarajkumar, Sumit Kapoor, Alisa V Stolyar, Katelyn Polanska, Karleigh R McCarthy, Hunter Osterhoudt, Xizhi Wu, Shyam Visweswaran, Sunyang Fu, Piyush Mathur, Giovanni E. Cacciamani, Cong Sun, Yifan Peng, Yanshan Wang

As generative artificial intelligence (AI), particularly Large Language
Models (LLMs), continues to permeate healthcare, it remains crucial to
supplement traditional automated evaluations with human expert evaluation.
Understanding and evaluating the generated texts is vital for ensuring safety,
reliability, and effectiveness. However, the cumbersome, time-consuming, and
non-standardized nature of human evaluation presents significant obstacles to
the widespread adoption of LLMs in practice. This study reviews existing
literature on human evaluation methodologies for LLMs within healthcare. We
highlight a notable need for a standardized and consistent human evaluation
approach. Our extensive literature search, adhering to the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, spans
publications from January 2018 to February 2024. This review provides a
comprehensive overview of the human evaluation approaches used in diverse
healthcare applications.This analysis examines the human evaluation of LLMs
across various medical specialties, addressing factors such as evaluation
dimensions, sample types, and sizes, the selection and recruitment of
evaluators, frameworks and metrics, the evaluation process, and statistical
analysis of the results. Drawing from diverse evaluation strategies highlighted
in these studies, we propose a comprehensive and practical framework for human
evaluation of generative LLMs, named QUEST: Quality of Information,
Understanding and Reasoning, Expression Style and Persona, Safety and Harm, and
Trust and Confidence. This framework aims to improve the reliability,
generalizability, and applicability of human evaluation of generative LLMs in
different healthcare applications by defining clear evaluation dimensions and
offering detailed guidelines.

摘要：

##### **Spatio-Temporal SwinMAE: A Swin Transformer based Multiscale Representation Learner for Temporal Satellite Imagery**
2405.02512v1 by Yohei Nakayama, Jiawei Su

Currently, the foundation models represented by large language models have
made dramatic progress and are used in a very wide range of domains including
2D and 3D vision. As one of the important application domains of foundation
models, earth observation has attracted attention and various approaches have
been developed. When considering earth observation as a single image capture,
earth observation imagery can be processed as an image with three or more
channels, and when it comes with multiple image captures of different
timestamps at one location, the temporal observation can be considered as a set
of continuous image resembling video frames or medical SCAN slices. This paper
presents Spatio-Temporal SwinMAE (ST-SwinMAE), an architecture which
particularly focuses on representation learning for spatio-temporal image
processing. Specifically, it uses a hierarchical Masked Auto-encoder (MAE) with
Video Swin Transformer blocks. With the architecture, we present a pretrained
model named Degas 100M as a geospatial foundation model. Also, we propose an
approach for transfer learning with Degas 100M, which both pretrained encoder
and decoder of MAE are utilized with skip connections added between them to
achieve multi-scale information communication, forms an architecture named
Spatio-Temporal SwinUNet (ST-SwinUNet). Our approach shows significant
improvements of performance over existing state-of-the-art of foundation
models. Specifically, for transfer learning of the land cover downstream task
on the PhilEO Bench dataset, it shows 10.4\% higher accuracy compared with
other geospatial foundation models on average.

摘要：

##### **Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction**
2405.02509v1 by Jiayang Shi, Junyi Zhu, Daniel M. Pelt, K. Joost Batenburg, Matthew B. Blaschko

Computed Tomography (CT) is pivotal in industrial quality control and medical
diagnostics. Sparse-view CT, offering reduced ionizing radiation, faces
challenges due to its under-sampled nature, leading to ill-posed reconstruction
problems. Recent advancements in Implicit Neural Representations (INRs) have
shown promise in addressing sparse-view CT reconstruction. Recognizing that CT
often involves scanning similar subjects, we propose a novel approach to
improve reconstruction quality through joint reconstruction of multiple objects
using INRs. This approach can potentially leverage both the strengths of INRs
and the statistical regularities across multiple objects. While current INR
joint reconstruction techniques primarily focus on accelerating convergence via
meta-initialization, they are not specifically tailored to enhance
reconstruction quality. To address this gap, we introduce a novel INR-based
Bayesian framework integrating latent variables to capture the inter-object
relationships. These variables serve as a dynamic reference throughout the
optimization, thereby enhancing individual reconstruction fidelity. Our
extensive experiments, which assess various key factors such as reconstruction
quality, resistance to overfitting, and generalizability, demonstrate
significant improvements over baselines in common numerical metrics. This
underscores a notable advancement in CT reconstruction methods.

摘要：

##### **A Survey of Few-Shot Learning for Biomedical Time Series**
2405.02485v1 by Chenqi Li, Timothy Denison, Tingting Zhu

Advancements in wearable sensor technologies and the digitization of medical
records have contributed to the unprecedented ubiquity of biomedical time
series data. Data-driven models have tremendous potential to assist clinical
diagnosis and improve patient care by improving long-term monitoring
capabilities, facilitating early disease detection and intervention, as well as
promoting personalized healthcare delivery. However, accessing extensively
labeled datasets to train data-hungry deep learning models encounters many
barriers, such as long-tail distribution of rare diseases, cost of annotation,
privacy and security concerns, data-sharing regulations, and ethical
considerations. An emerging approach to overcome the scarcity of labeled data
is to augment AI methods with human-like capabilities to leverage past
experiences to learn new tasks with limited examples, called few-shot learning.
This survey provides a comprehensive review and comparison of few-shot learning
methods for biomedical time series applications. The clinical benefits and
limitations of such methods are discussed in relation to traditional
data-driven approaches. This paper aims to provide insights into the current
landscape of few-shot learning for biomedical time series and its implications
for future research and applications.

摘要：

##### **Generalizing Orthogonalization for Models with Non-linearities**
2405.02475v1 by David Rügamer, Chris Kolb, Tobias Weber, Lucas Kook, Thomas Nagler

The complexity of black-box algorithms can lead to various challenges,
including the introduction of biases. These biases present immediate risks in
the algorithms' application. It was, for instance, shown that neural networks
can deduce racial information solely from a patient's X-ray scan, a task beyond
the capability of medical experts. If this fact is not known to the medical
expert, automatic decision-making based on this algorithm could lead to
prescribing a treatment (purely) based on racial information. While current
methodologies allow for the "orthogonalization" or "normalization" of neural
networks with respect to such information, existing approaches are grounded in
linear models. Our paper advances the discourse by introducing corrections for
non-linearities such as ReLU activations. Our approach also encompasses scalar
and tensor-valued predictions, facilitating its integration into neural network
architectures. Through extensive experiments, we validate our method's
effectiveness in safeguarding sensitive data in generalized linear models,
normalizing convolutional neural networks for metadata, and rectifying
pre-existing embeddings for undesired attributes.

摘要：

##### **Model-based reinforcement learning for protein backbone design**
2405.01983v1 by Frederic Renard, Cyprien Courtot, Alfredo Reichlin, Oliver Bent

Designing protein nanomaterials of predefined shape and characteristics has
the potential to dramatically impact the medical industry. Machine learning
(ML) has proven successful in protein design, reducing the need for expensive
wet lab experiment rounds. However, challenges persist in efficiently exploring
the protein fitness landscapes to identify optimal protein designs. In
response, we propose the use of AlphaZero to generate protein backbones,
meeting shape and structural scoring requirements. We extend an existing Monte
Carlo tree search (MCTS) framework by incorporating a novel threshold-based
reward and secondary objectives to improve design precision. This innovation
considerably outperforms existing approaches, leading to protein backbones that
better respect structural scores. The application of AlphaZero is novel in the
context of protein backbone design and demonstrates promising performance.
AlphaZero consistently surpasses baseline MCTS by more than 100% in top-down
protein design tasks. Additionally, our application of AlphaZero with secondary
objectives uncovers further promising outcomes, indicating the potential of
model-based reinforcement learning (RL) in navigating the intricate and nuanced
aspects of protein design

摘要：

##### **Aloe: A Family of Fine-tuned Open Healthcare LLMs**
2405.01886v1 by Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Lucia Urcelay-Ganzabal, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguadé-Parra, Ulises Cortés Dario Garcia-Gasulla

As the capabilities of Large Language Models (LLMs) in healthcare and
medicine continue to advance, there is a growing need for competitive
open-source models that can safeguard public interest. With the increasing
availability of highly competitive open base models, the impact of continued
pre-training is increasingly uncertain. In this work, we explore the role of
instruct tuning, model merging, alignment, red teaming and advanced inference
schemes, as means to improve current open models. To that end, we introduce the
Aloe family, a set of open medical LLMs highly competitive within its scale
range. Aloe models are trained on the current best base models (Mistral, LLaMA
3), using a new custom dataset which combines public data sources improved with
synthetic Chain of Thought (CoT). Aloe models undergo an alignment phase,
becoming one of the first few policy-aligned open healthcare LLM using Direct
Preference Optimization, setting a new standard for ethical performance in
healthcare LLMs. Model evaluation expands to include various bias and toxicity
datasets, a dedicated red teaming effort, and a much-needed risk assessment for
healthcare LLMs. Finally, to explore the limits of current LLMs in inference,
we study several advanced prompt engineering strategies to boost performance
across benchmarks, yielding state-of-the-art results for open healthcare 7B
LLMs, unprecedented at this scale.

摘要：

##### **Millimeter Wave Radar-based Human Activity Recognition for Healthcare Monitoring Robot**
2405.01882v1 by Zhanzhong Gu, Xiangjian He, Gengfa Fang, Chengpei Xu, Feng Xia, Wenjing Jia

Healthcare monitoring is crucial, especially for the daily care of elderly
individuals living alone. It can detect dangerous occurrences, such as falls,
and provide timely alerts to save lives. Non-invasive millimeter wave (mmWave)
radar-based healthcare monitoring systems using advanced human activity
recognition (HAR) models have recently gained significant attention. However,
they encounter challenges in handling sparse point clouds, achieving real-time
continuous classification, and coping with limited monitoring ranges when
statically mounted. To overcome these limitations, we propose RobHAR, a movable
robot-mounted mmWave radar system with lightweight deep neural networks for
real-time monitoring of human activities. Specifically, we first propose a
sparse point cloud-based global embedding to learn the features of point clouds
using the light-PointNet (LPN) backbone. Then, we learn the temporal pattern
with a bidirectional lightweight LSTM model (BiLiLSTM). In addition, we
implement a transition optimization strategy, integrating the Hidden Markov
Model (HMM) with Connectionist Temporal Classification (CTC) to improve the
accuracy and robustness of the continuous HAR. Our experiments on three
datasets indicate that our method significantly outperforms the previous
studies in both discrete and continuous HAR tasks. Finally, we deploy our
system on a movable robot-mounted edge computing platform, achieving flexible
healthcare monitoring in real-world scenarios.

摘要：

##### **Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders and Identifying Distinct Features**
2405.01799v1 by Chuanbo Hu, Wenqi Li, Mindi Ruan, Xiangxu Yu, Lynn K. Paul, Shuo Wang, Xin Li

Diagnosing language disorders associated with autism is a complex and nuanced
challenge, often hindered by the subjective nature and variability of
traditional assessment methods. Traditional diagnostic methods not only require
intensive human effort but also often result in delayed interventions due to
their lack of speed and specificity. In this study, we explored the application
of ChatGPT, a state of the art large language model, to overcome these
obstacles by enhancing diagnostic accuracy and profiling specific linguistic
features indicative of autism. Leveraging ChatGPT advanced natural language
processing capabilities, this research aims to streamline and refine the
diagnostic process. Specifically, we compared ChatGPT's performance with that
of conventional supervised learning models, including BERT, a model acclaimed
for its effectiveness in various natural language processing tasks. We showed
that ChatGPT substantially outperformed these models, achieving over 13%
improvement in both accuracy and F1 score in a zero shot learning
configuration. This marked enhancement highlights the model potential as a
superior tool for neurological diagnostics. Additionally, we identified ten
distinct features of autism associated language disorders that vary
significantly across different experimental scenarios. These features, which
included echolalia, pronoun reversal, and atypical language usage, were crucial
for accurately diagnosing ASD and customizing treatment plans. Together, our
findings advocate for adopting sophisticated AI tools like ChatGPT in clinical
settings to assess and diagnose developmental disorders. Our approach not only
promises greater diagnostic precision but also aligns with the goals of
personalized medicine, potentially transforming the evaluation landscape for
autism and similar neurological conditions.

摘要：

##### **Interpretable Vital Sign Forecasting with Model Agnostic Attention Maps**
2405.01714v2 by Yuwei Liu, Chen Dan, Anubhav Bhatti, Bingjie Shen, Divij Gupta, Suraj Parmar, San Lee

Sepsis is a leading cause of mortality in intensive care units (ICUs),
representing a substantial medical challenge. The complexity of analyzing
diverse vital signs to predict sepsis further aggravates this issue. While deep
learning techniques have been advanced for early sepsis prediction, their
'black-box' nature obscures the internal logic, impairing interpretability in
critical settings like ICUs. This paper introduces a framework that combines a
deep learning model with an attention mechanism that highlights the critical
time steps in the forecasting process, thus improving model interpretability
and supporting clinical decision-making. We show that the attention mechanism
could be adapted to various black box time series forecasting models such as
N-HiTS and N-BEATS. Our method preserves the accuracy of conventional deep
learning models while enhancing interpretability through
attention-weight-generated heatmaps. We evaluated our model on the eICU-CRD
dataset, focusing on forecasting vital signs for sepsis patients. We assessed
its performance using mean squared error (MSE) and dynamic time warping (DTW)
metrics. We explored the attention maps of N-HiTS and N-BEATS, examining the
differences in their performance and identifying crucial factors influencing
vital sign forecasting.

摘要：

##### **Long Tail Image Generation Through Feature Space Augmentation and Iterated Learning**
2405.01705v1 by Rafael Elberg, Denis Parra, Mircea Petrache

Image and multimodal machine learning tasks are very challenging to solve in
the case of poorly distributed data. In particular, data availability and
privacy restrictions exacerbate these hurdles in the medical domain. The state
of the art in image generation quality is held by Latent Diffusion models,
making them prime candidates for tackling this problem. However, a few key
issues still need to be solved, such as the difficulty in generating data from
under-represented classes and a slow inference process. To mitigate these
issues, we propose a new method for image augmentation in long-tailed data
based on leveraging the rich latent space of pre-trained Stable Diffusion
Models. We create a modified separable latent space to mix head and tail class
examples. We build this space via Iterated Learning of underlying sparsified
embeddings, which we apply to task-specific saliency maps via a K-NN approach.
Code is available at
https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning

摘要：

##### **Automatically Extracting Numerical Results from Randomized Controlled Trials with Large Language Models**
2405.01686v1 by Hye Sun Yun, David Pogrebitskiy, Iain J. Marshall, Byron C. Wallace

Meta-analyses statistically aggregate the findings of different randomized
controlled trials (RCTs) to assess treatment effectiveness. Because this yields
robust estimates of treatment effectiveness, results from meta-analyses are
considered the strongest form of evidence. However, rigorous evidence syntheses
are time-consuming and labor-intensive, requiring manual extraction of data
from individual trials to be synthesized. Ideally, language technologies would
permit fully automatic meta-analysis, on demand. This requires accurately
extracting numerical results from individual trials, which has been beyond the
capabilities of natural language processing (NLP) models to date. In this work,
we evaluate whether modern large language models (LLMs) can reliably perform
this task. We annotate (and release) a modest but granular evaluation dataset
of clinical trial reports with numerical findings attached to interventions,
comparators, and outcomes. Using this dataset, we evaluate the performance of
seven LLMs applied zero-shot for the task of conditionally extracting numerical
findings from trial reports. We find that massive LLMs that can accommodate
lengthy inputs are tantalizingly close to realizing fully automatic
meta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality).
However, LLMs -- including ones trained on biomedical texts -- perform poorly
when the outcome measures are complex and tallying the results requires
inference. This work charts a path toward fully automatic meta-analysis of RCTs
via LLMs, while also highlighting the limitations of existing models for this
aim.

摘要：

##### **Leveraging Prompt-Learning for Structured Information Extraction from Crohn's Disease Radiology Reports in a Low-Resource Language**
2405.01682v1 by Liam Hazan, Gili Focht, Naama Gavrielov, Roi Reichart, Talar Hagopian, Mary-Louise C. Greer, Ruth Cytter Kuint, Dan Turner, Moti Freiman

Automatic conversion of free-text radiology reports into structured data
using Natural Language Processing (NLP) techniques is crucial for analyzing
diseases on a large scale. While effective for tasks in widely spoken languages
like English, generative large language models (LLMs) typically underperform
with less common languages and can pose potential risks to patient privacy.
Fine-tuning local NLP models is hindered by the skewed nature of real-world
medical datasets, where rare findings represent a significant data imbalance.
We introduce SMP-BERT, a novel prompt learning method that leverages the
structured nature of reports to overcome these challenges. In our studies
involving a substantial collection of Crohn's disease radiology reports in
Hebrew (over 8,000 patients and 10,000 reports), SMP-BERT greatly surpassed
traditional fine-tuning methods in performance, notably in detecting infrequent
conditions (AUC: 0.99 vs 0.94, F1: 0.84 vs 0.34). SMP-BERT empowers more
accurate AI diagnostics available for low-resource languages.

摘要：

##### **Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning**
2405.01469v1 by Théo Moutakanni, Piotr Bojanowski, Guillaume Chassagnon, Céline Hudelot, Armand Joulin, Yann LeCun, Matthew Muckley, Maxime Oquab, Marie-Pierre Revel, Maria Vakalopoulou

AI Foundation models are gaining traction in various applications, including
medical fields like radiology. However, medical foundation models are often
tested on limited tasks, leaving their generalisability and biases unexplored.
We present RayDINO, a large visual encoder trained by self-supervision on 873k
chest X-rays. We compare RayDINO to previous state-of-the-art models across
nine radiology tasks, from classification and dense segmentation to text
generation, and provide an in depth analysis of population, age and sex biases
of our model. Our findings suggest that self-supervision allows patient-centric
AI proving useful in clinical workflows and interpreting X-rays holistically.
With RayDINO and small task-specific adapters, we reach state-of-the-art
results and improve generalization to unseen populations while mitigating bias,
illustrating the true promise of foundation models: versatility and robustness.

摘要：

##### **DMON: A Simple yet Effective Approach for Argument Structure Learning**
2405.01216v1 by Wei Sun, Mingxiao Li, Jingyuan Sun, Jesse Davis, Marie-Francine Moens

Argument structure learning~(ASL) entails predicting relations between
arguments. Because it can structure a document to facilitate its understanding,
it has been widely applied in many fields~(medical, commercial, and scientific
domains). Despite its broad utilization, ASL remains a challenging task because
it involves examining the complex relationships between the sentences in a
potentially unstructured discourse. To resolve this problem, we have developed
a simple yet effective approach called Dual-tower Multi-scale cOnvolution
neural Network~(DMON) for the ASL task. Specifically, we organize arguments
into a relationship matrix that together with the argument embeddings forms a
relationship tensor and design a mechanism to capture relations with contextual
arguments. Experimental results on three different-domain argument mining
datasets demonstrate that our framework outperforms state-of-the-art models.
The code is available at https://github.com/VRCMF/DMON.git .

摘要：

##### **Beyond Human Vision: The Role of Large Vision Language Models in Microscope Image Analysis**
2405.00876v1 by Prateek Verma, Minh-Hao Van, Xintao Wu

Vision language models (VLMs) have recently emerged and gained the spotlight
for their ability to comprehend the dual modality of image and textual data.
VLMs such as LLaVA, ChatGPT-4, and Gemini have recently shown impressive
performance on tasks such as natural image captioning, visual question
answering (VQA), and spatial reasoning. Additionally, a universal segmentation
model by Meta AI, Segment Anything Model (SAM) shows unprecedented performance
at isolating objects from unforeseen images. Since medical experts, biologists,
and materials scientists routinely examine microscopy or medical images in
conjunction with textual information in the form of captions, literature, or
reports, and draw conclusions of great importance and merit, it is indubitably
essential to test the performance of VLMs and foundation models such as SAM, on
these images. In this study, we charge ChatGPT, LLaVA, Gemini, and SAM with
classification, segmentation, counting, and VQA tasks on a variety of
microscopy images. We observe that ChatGPT and Gemini are impressively able to
comprehend the visual features in microscopy images, while SAM is quite capable
at isolating artefacts in a general sense. However, the performance is not
close to that of a domain expert - the models are readily encumbered by the
introduction of impurities, defects, artefact overlaps and diversity present in
the images.

摘要：

##### **"I'm Not Sure, But...": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust**
2405.00623v1 by Sunnie S. Y. Kim, Q. Vera Liao, Mihaela Vorvoreanu, Stephanie Ballard, Jennifer Wortman Vaughan

Widely deployed large language models (LLMs) can produce convincing yet
incorrect outputs, potentially misleading users who may rely on them as if they
were correct. To reduce such overreliance, there have been calls for LLMs to
communicate their uncertainty to end users. However, there has been little
empirical work examining how users perceive and act upon LLMs' expressions of
uncertainty. We explore this question through a large-scale, pre-registered,
human-subject experiment (N=404) in which participants answer medical questions
with or without access to responses from a fictional LLM-infused search engine.
Using both behavioral and self-reported measures, we examine how different
natural language expressions of uncertainty impact participants' reliance,
trust, and overall task performance. We find that first-person expressions
(e.g., "I'm not sure, but...") decrease participants' confidence in the system
and tendency to agree with the system's answers, while increasing participants'
accuracy. An exploratory analysis suggests that this increase can be attributed
to reduced (but not fully eliminated) overreliance on incorrect answers. While
we observe similar effects for uncertainty expressed from a general perspective
(e.g., "It's not clear, but..."), these effects are weaker and not
statistically significant. Our findings suggest that using natural language
expressions of uncertainty may be an effective approach for reducing
overreliance on LLMs, but that the precise language used matters. This
highlights the importance of user testing before deploying LLMs at scale.

摘要：

##### **Enhancing Surgical Robots with Embodied Intelligence for Autonomous Ultrasound Scanning**
2405.00461v1 by Huan Xu, Jinlin Wu, Guanglin Cao, Zhen Lei, Zhen Chen, Hongbin Liu

Ultrasound robots are increasingly used in medical diagnostics and early
disease screening. However, current ultrasound robots lack the intelligence to
understand human intentions and instructions, hindering autonomous ultrasound
scanning. To solve this problem, we propose a novel Ultrasound Embodied
Intelligence system that equips ultrasound robots with the large language model
(LLM) and domain knowledge, thereby improving the efficiency of ultrasound
robots. Specifically, we first design an ultrasound operation knowledge
database to add expertise in ultrasound scanning to the LLM, enabling the LLM
to perform precise motion planning. Furthermore, we devise a dynamic ultrasound
scanning strategy based on a \textit{think-observe-execute} prompt engineering,
allowing LLMs to dynamically adjust motion planning strategies during the
scanning procedures. Extensive experiments demonstrate that our system
significantly improves ultrasound scan efficiency and quality from verbal
commands. This advancement in autonomous medical scanning technology
contributes to non-invasive diagnostics and streamlined medical workflows.

摘要：

##### **A Careful Examination of Large Language Model Performance on Grade School Arithmetic**
2405.00332v3 by Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, Summer Yue

Large language models (LLMs) have achieved impressive success on many
benchmarks for mathematical reasoning. However, there is growing concern that
some of this performance actually reflects dataset contamination, where data
closely resembling benchmark questions leaks into the training data, instead of
true reasoning ability. To investigate this claim rigorously, we commission
Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and
complexity of the established GSM8k benchmark, the gold standard for measuring
elementary mathematical reasoning. We ensure that the two benchmarks are
comparable across important metrics such as human solve rates, number of steps
in solution, answer magnitude, and more. When evaluating leading open- and
closed-source LLMs on GSM1k, we observe accuracy drops of up to 13%, with
several families of models (e.g., Phi and Mistral) showing evidence of
systematic overfitting across almost all model sizes. At the same time, many
models, especially those on the frontier, (e.g., Gemini/GPT/Claude) show
minimal signs of overfitting. Further analysis suggests a positive relationship
(Spearman's r^2=0.32) between a model's probability of generating an example
from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that
many models may have partially memorized GSM8k.

摘要：

##### **Active Learning with Task Adaptation Pre-training for Speech Emotion Recognition**
2405.00307v1 by Dongyuan Li, Ying Zhang, Yusong Wang, Funakoshi Kataro, Manabu Okumura

Speech emotion recognition (SER) has garnered increasing attention due to its
wide range of applications in various fields, including human-machine
interaction, virtual assistants, and mental health assistance. However,
existing SER methods often overlook the information gap between the
pre-training speech recognition task and the downstream SER task, resulting in
sub-optimal performance. Moreover, current methods require much time for
fine-tuning on each specific speech dataset, such as IEMOCAP, which limits
their effectiveness in real-world scenarios with large-scale noisy data. To
address these issues, we propose an active learning (AL)-based fine-tuning
framework for SER, called \textsc{After}, that leverages task adaptation
pre-training (TAPT) and AL methods to enhance performance and efficiency.
Specifically, we first use TAPT to minimize the information gap between the
pre-training speech recognition task and the downstream speech emotion
recognition task. Then, AL methods are employed to iteratively select a subset
of the most informative and diverse samples for fine-tuning, thereby reducing
time consumption. Experiments demonstrate that our proposed method
\textsc{After}, using only 20\% of samples, improves accuracy by 8.45\% and
reduces time consumption by 79\%. The additional extension of \textsc{After}
and ablation studies further confirm its effectiveness and applicability to
various real-world scenarios. Our source code is available on Github for
reproducibility. (https://github.com/Clearloveyuan/AFTER).

摘要：

##### **Quantifying Nematodes through Images: Datasets, Models, and Baselines of Deep Learning**
2404.19748v1 by Zhipeng Yuan, Nasamu Musa, Katarzyna Dybal, Matthew Back, Daniel Leybourne, Po Yang

Every year, plant parasitic nematodes, one of the major groups of plant
pathogens, cause a significant loss of crops worldwide. To mitigate crop yield
losses caused by nematodes, an efficient nematode monitoring method is
essential for plant and crop disease management. In other respects, efficient
nematode detection contributes to medical research and drug discovery, as
nematodes are model organisms. With the rapid development of computer
technology, computer vision techniques provide a feasible solution for
quantifying nematodes or nematode infections. In this paper, we survey and
categorise the studies and available datasets on nematode detection through
deep-learning models. To stimulate progress in related research, this survey
presents the potential state-of-the-art object detection models, training
techniques, optimisation techniques, and evaluation metrics for deep learning
beginners. Moreover, seven state-of-the-art object detection models are
validated on three public datasets and the AgriNema dataset for plant parasitic
nematodes to construct a baseline for nematode detection.

摘要：

##### **Data Set Terminology of Artificial Intelligence in Medicine: A Historical Review and Recommendation**
2404.19303v1 by Shannon L. Walston, Hiroshi Seki, Hirotaka Takita, Yasuhito Mitsuyama, Shingo Sato, Akifumi Hagiwara, Rintaro Ito, Shouhei Hanaoka, Yukio Miki, Daiju Ueda

Medicine and artificial intelligence (AI) engineering represent two distinct
fields each with decades of published history. With such history comes a set of
terminology that has a specific way in which it is applied. However, when two
distinct fields with overlapping terminology start to collaborate,
miscommunication and misunderstandings can occur. This narrative review aims to
give historical context for these terms, accentuate the importance of clarity
when these terms are used in medical AI contexts, and offer solutions to
mitigate misunderstandings by readers from either field. Through an examination
of historical documents, including articles, writing guidelines, and textbooks,
this review traces the divergent evolution of terms for data sets and their
impact. Initially, the discordant interpretations of the word 'validation' in
medical and AI contexts are explored. Then the data sets used for AI evaluation
are classified, namely random splitting, cross-validation, temporal,
geographic, internal, and external sets. The accurate and standardized
description of these data sets is crucial for demonstrating the robustness and
generalizability of AI applications in medicine. This review clarifies existing
literature to provide a comprehensive understanding of these classifications
and their implications in AI evaluation. This review then identifies often
misunderstood terms and proposes pragmatic solutions to mitigate terminological
confusion. Among these solutions are the use of standardized terminology such
as 'training set,' 'validation (or tuning) set,' and 'test set,' and explicit
definition of data set splitting terminologies in each medical AI research
publication. This review aspires to enhance the precision of communication in
medical AI, thereby fostering more effective and transparent research
methodologies in this interdisciplinary field.

摘要：

##### **Text and Audio Simplification: Human vs. ChatGPT**
2405.01592v1 by Gondy Leroy, David Kauchak, Philip Harber, Ankit Pal, Akash Shukla

Text and audio simplification to increase information comprehension are
important in healthcare. With the introduction of ChatGPT, an evaluation of its
simplification performance is needed. We provide a systematic comparison of
human and ChatGPT simplified texts using fourteen metrics indicative of text
difficulty. We briefly introduce our online editor where these simplification
tools, including ChatGPT, are available. We scored twelve corpora using our
metrics: six text, one audio, and five ChatGPT simplified corpora. We then
compare these corpora with texts simplified and verified in a prior user study.
Finally, a medical domain expert evaluated these texts and five, new ChatGPT
simplified versions. We found that simple corpora show higher similarity with
the human simplified texts. ChatGPT simplification moves metrics in the right
direction. The medical domain expert evaluation showed a preference for the
ChatGPT style, but the text itself was rated lower for content retention.

摘要：

##### **ConPro: Learning Severity Representation for Medical Images using Contrastive Learning and Preference Optimization**
2404.18831v1 by Hong Nguyen, Hoang Nguyen, Melinda Chang, Hieu Pham, Shrikanth Narayanan, Michael Pazzani

Understanding the severity of conditions shown in images in medical diagnosis
is crucial, serving as a key guide for clinical assessment, treatment, as well
as evaluating longitudinal progression. This paper proposes Con- PrO: a novel
representation learning method for severity assessment in medical images using
Contrastive learningintegrated Preference Optimization. Different from
conventional contrastive learning methods that maximize the distance between
classes, ConPrO injects into the latent vector the distance preference
knowledge between various severity classes and the normal class. We
systematically examine the key components of our framework to illuminate how
contrastive prediction tasks acquire valuable representations. We show that our
representation learning framework offers valuable severity ordering in the
feature space while outperforming previous state-of-the-art methods on
classification tasks. We achieve a 6% and 20% relative improvement compared to
a supervised and a self-supervised baseline, respectively. In addition, we
derived discussions on severity indicators and related applications of
preference comparison in the medical domain.

摘要：

##### **Decoding Radiologists' Intentions: A Novel System for Accurate Region Identification in Chest X-ray Image Analysis**
2404.18981v1 by Akash Awasthi, Safwan Ahmad, Bryant Le, Hien Van Nguyen

In the realm of chest X-ray (CXR) image analysis, radiologists meticulously
examine various regions, documenting their observations in reports. The
prevalence of errors in CXR diagnoses, particularly among inexperienced
radiologists and hospital residents, underscores the importance of
understanding radiologists' intentions and the corresponding regions of
interest. This understanding is crucial for correcting mistakes by guiding
radiologists to the accurate regions of interest, especially in the diagnosis
of chest radiograph abnormalities. In response to this imperative, we propose a
novel system designed to identify the primary intentions articulated by
radiologists in their reports and the corresponding regions of interest in CXR
images. This system seeks to elucidate the visual context underlying
radiologists' textual findings, with the potential to rectify errors made by
less experienced practitioners and direct them to precise regions of interest.
Importantly, the proposed system can be instrumental in providing constructive
feedback to inexperienced radiologists or junior residents in the hospital,
bridging the gap in face-to-face communication. The system represents a
valuable tool for enhancing diagnostic accuracy and fostering continuous
learning within the medical community.

摘要：

##### **Foundations of Multisensory Artificial Intelligence**
2404.18976v1 by Paul Pu Liang

Building multisensory AI systems that learn from multiple sensory inputs such
as text, speech, video, real-world sensors, wearable devices, and medical data
holds great promise for impact in many scientific areas with practical
benefits, such as in supporting human health and well-being, enabling
multimedia content processing, and enhancing real-world autonomous agents. By
synthesizing a range of theoretical frameworks and application domains, this
thesis aims to advance the machine learning foundations of multisensory AI. In
the first part, we present a theoretical framework formalizing how modalities
interact with each other to give rise to new information for a task. These
interactions are the basic building blocks in all multimodal problems, and
their quantification enables users to understand their multimodal datasets,
design principled approaches to learn these interactions, and analyze whether
their model has succeeded in learning. In the second part, we study the design
of practical multimodal foundation models that generalize over many modalities
and tasks, which presents a step toward grounding large language models to
real-world sensory modalities. We introduce MultiBench, a unified large-scale
benchmark across a wide range of modalities, tasks, and research areas,
followed by the cross-modal attention and multimodal transformer architectures
that now underpin many of today's multimodal foundation models. Scaling these
architectures on MultiBench enables the creation of general-purpose
multisensory AI systems, and we discuss our collaborative efforts in applying
these models for real-world impact in affective computing, mental health,
cancer prognosis, and robotics. Finally, we conclude this thesis by discussing
how future work can leverage these ideas toward more general, interactive, and
safe multisensory AI.

摘要：

##### **M3H: Multimodal Multitask Machine Learning for Healthcare**
2404.18975v1 by Dimitris Bertsimas, Yu Ma

Recent breakthroughs in AI are poised to fundamentally enhance our study and
understanding of healthcare. The development of an integrated many-to-many
framework that leverages multiple data modality inputs for the analytical
modeling of multiple medical tasks, is critical for a unified understanding of
modern medicine. In this work, we introduce M3H, an explainable Multimodal
Multitask Machine Learning for Healthcare framework that consolidates learning
from diverse multimodal inputs across a broad spectrum of medical task
categories and machine learning problem classes. The modular design of the
framework ensures its generalizable data processing, task definition, and rapid
model prototyping, applicable to both clinical and operational healthcare
settings. We evaluate the M3H framework by validating models trained from four
modalities (tabular, time-series, language, and vision) on 41 medical tasks
across 4 machine learning problem classes. Our results demonstrate that M3H
consistently produces multitask models that outperform canonical single-task
models (by 1.1- 37.2%) across 37 disease diagnoses from 16 medical departments,
three hospital operation forecasts, and one patient phenotyping task: spanning
ML problem classes of supervised binary classification, multiclass
classification, regression, and clustering. Additionally, the framework
introduces a novel attention mechanism to balance self-exploitation (focus on
learning source task), and cross-exploration (encourage learning from other
tasks). Furthermore, M3H provides explainability insights on how joint learning
of additional tasks impacts the learning of source task using a proposed TIM
score, shedding light into the dynamics of task interdependencies. Its
adaptable architecture facilitates the customization and integration,
establishing it as a robust and scalable candidate solution for future
AI-driven healthcare systems.

摘要：

##### **Real Time Multi Organ Classification on Computed Tomography Images**
2404.18731v1 by Halid Ziya Yerebakan, Yoshihisa Shinagawa, Gerardo Hermosillo Valadez

Organ segmentation is a fundamental task in medical imaging, and it is useful
for many clinical automation pipelines. Typically, the process involves
segmenting the entire volume, which can be unnecessary when the points of
interest are limited. In those cases, a classifier could be used instead of
segmentation. However, there is an inherent trade-off between the context size
and the speed of classifiers. To address this issue, we propose a new method
that employs a data selection strategy with sparse sampling across a wide field
of view without image resampling. This sparse sampling strategy makes it
possible to classify voxels into multiple organs in real time without using
accelerators. Although our method is an independent classifier, it can generate
full segmentation by querying grid locations at any resolution. We have
compared our method with existing segmentation techniques, demonstrating its
potential for superior runtime in practical applications in medical imaging.

摘要：

##### **Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology with General-Domain Large Language Model**
2405.01591v1 by Seonhee Cho, Choonghan Kim, Jiho Lee, Chetan Chilkunda, Sujin Choi, Joo Heung Yoon

Recent advancements in Large Multimodal Models (LMMs) have attracted interest
in their generalization capability with only a few samples in the prompt. This
progress is particularly relevant to the medical domain, where the quality and
sensitivity of data pose unique challenges for model training and application.
However, the dependency on high-quality data for effective in-context learning
raises questions about the feasibility of these models when encountering with
the inevitable variations and errors inherent in real-world medical data. In
this paper, we introduce MID-M, a novel framework that leverages the in-context
learning capabilities of a general-domain Large Language Model (LLM) to process
multimodal data via image descriptions. MID-M achieves a comparable or superior
performance to task-specific fine-tuned LMMs and other general-domain ones,
without the extensive domain-specific training or pre-training on multimodal
data, with significantly fewer parameters. This highlights the potential of
leveraging general-domain LLMs for domain-specific tasks and offers a
sustainable and cost-effective alternative to traditional LMM developments.
Moreover, the robustness of MID-M against data quality issues demonstrates its
practical utility in real-world medical domain applications.

摘要：

##### **Machine Learning for Quantum Computing Specialists**
2404.18555v1 by Daniel Goldsmith, M M Hassan Mahmud

Quantum machine learning (QML) is a promising early use case for quantum
computing. There has been progress in the last five years from theoretical
studies and numerical simulations to proof of concepts. Use cases demonstrated
on contemporary quantum devices include classifying medical images and items
from the Iris dataset, classifying and generating handwritten images, toxicity
screening, and learning a probability distribution. Potential benefits of QML
include faster training and identification of feature maps not found
classically. Although, these examples lack the scale for commercial
exploitation, and it may be several years before QML algorithms replace the
classical solutions, QML is an exciting area.
  This article is written for those who already have a sound knowledge of
quantum computing and now wish to gain a basic overview of the terminology and
some applications of classical machine learning ready to study quantum machine
learning. The reader will already understand the relevant relevant linear
algebra, including Hilbert spaces, a vector space with an inner product.

摘要：

##### **GPT-4 passes most of the 297 written Polish Board Certification Examinations**
2405.01589v1 by Jakub Pokrywka, Jeremi Kaczmarek, Edward Gorzelańczyk

Introduction: Recently, the effectiveness of Large Language Models (LLMs) has
increased rapidly, allowing them to be used in a great number of applications.
However, the risks posed by the generation of false information through LLMs
significantly limit their applications in sensitive areas such as healthcare,
highlighting the necessity for rigorous validations to determine their utility
and reliability. To date, no study has extensively compared the performance of
LLMs on Polish medical examinations across a broad spectrum of specialties on a
very large dataset. Objectives: This study evaluated the performance of three
Generative Pretrained Transformer (GPT) models on the Polish Board
Certification Exam (Pa\'nstwowy Egzamin Specjalizacyjny, PES) dataset, which
consists of 297 tests. Methods: We developed a software program to download and
process PES exams and tested the performance of GPT models using OpenAI
Application Programming Interface. Results: Our findings reveal that GPT-3.5
did not pass any of the analyzed exams. In contrast, the GPT-4 models
demonstrated the capability to pass the majority of the exams evaluated, with
the most recent model, gpt-4-0125, successfully passing 222 (75%) of them. The
performance of the GPT models varied significantly, displaying excellence in
exams related to certain specialties while completely failing others.
Conclusions: The significant progress and impressive performance of LLM models
hold great promise for the increased application of AI in the field of medicine
in Poland. For instance, this advancement could lead to the development of
AI-based medical assistants for healthcare professionals, enhancing the
efficiency and accuracy of medical services.

摘要：

##### **On the Impact of Data Heterogeneity in Federated Learning Environments with Application to Healthcare Networks**
2404.18519v2 by Usevalad Milasheuski. Luca Barbieri, Bernardo Camajori Tedeschini, Monica Nicoli, Stefano Savazzi

Federated Learning (FL) allows multiple privacy-sensitive applications to
leverage their dataset for a global model construction without any disclosure
of the information. One of those domains is healthcare, where groups of silos
collaborate in order to generate a global predictor with improved accuracy and
generalization. However, the inherent challenge lies in the high heterogeneity
of medical data, necessitating sophisticated techniques for assessment and
compensation. This paper presents a comprehensive exploration of the
mathematical formalization and taxonomy of heterogeneity within FL
environments, focusing on the intricacies of medical data. In particular, we
address the evaluation and comparison of the most popular FL algorithms with
respect to their ability to cope with quantity-based, feature and label
distribution-based heterogeneity. The goal is to provide a quantitative
evaluation of the impact of data heterogeneity in FL systems for healthcare
networks as well as a guideline on FL algorithm selection. Our research extends
beyond existing studies by benchmarking seven of the most common FL algorithms
against the unique challenges posed by medical data use cases. The paper
targets the prediction of the risk of stroke recurrence through a set of
tabular clinical reports collected by different federated hospital silos: data
heterogeneity frequently encountered in this scenario and its impact on FL
performance are discussed.

摘要：

##### **Research on Intelligent Aided Diagnosis System of Medical Image Based on Computer Deep Learning**
2404.18419v1 by Jiajie Yuan, Linxiao Wu, Yulu Gong, Zhou Yu, Ziang Liu, Shuyao He

This paper combines Struts and Hibernate two architectures together, using
DAO (Data Access Object) to store and access data. Then a set of dual-mode
humidity medical image library suitable for deep network is established, and a
dual-mode medical image assisted diagnosis method based on the image is
proposed. Through the test of various feature extraction methods, the optimal
operating characteristic under curve product (AUROC) is 0.9985, the recall rate
is 0.9814, and the accuracy is 0.9833. This method can be applied to clinical
diagnosis, and it is a practical method. Any outpatient doctor can register
quickly through the system, or log in to the platform to upload the image to
obtain more accurate images. Through the system, each outpatient physician can
quickly register or log in to the platform for image uploading, thus obtaining
more accurate images. The segmentation of images can guide doctors in clinical
departments. Then the image is analyzed to determine the location and nature of
the tumor, so as to make targeted treatment.

摘要：

##### **Capabilities of Gemini Models in Medicine**
2404.18416v2 by Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, Juanma Zambrano Chaves, Szu-Yeu Hu, Mike Schaekermann, Aishwarya Kamath, Yong Cheng, David G. T. Barrett, Cathy Cheung, Basil Mustafa, Anil Palepu, Daniel McDuff, Le Hou, Tomer Golany, Luyang Liu, Jean-baptiste Alayrac, Neil Houlsby, Nenad Tomasev, Jan Freyberg, Charles Lau, Jonas Kemp, Jeremy Lai, Shekoofeh Azizi, Kimberly Kanada, SiWai Man, Kavita Kulkarni, Ruoxi Sun, Siamak Shakeri, Luheng He, Ben Caine, Albert Webson, Natasha Latysheva, Melvin Johnson, Philip Mansfield, Jian Lu, Ehud Rivlin, Jesper Anderson, Bradley Green, Renee Wong, Jonathan Krause, Jonathon Shlens, Ewa Dominowska, S. M. Ali Eslami, Katherine Chou, Claire Cui, Oriol Vinyals, Koray Kavukcuoglu, James Manyika, Jeff Dean, Demis Hassabis, Yossi Matias, Dale Webster, Joelle Barral, Greg Corrado, Christopher Semturs, S. Sara Mahdavi, Juraj Gottweis, Alan Karthikesalingam, Vivek Natarajan

Excellence in a wide variety of medical applications poses considerable
challenges for AI, requiring advanced reasoning, access to up-to-date medical
knowledge and understanding of complex multimodal data. Gemini models, with
strong general capabilities in multimodal and long-context reasoning, offer
exciting possibilities in medicine. Building on these core strengths of Gemini,
we introduce Med-Gemini, a family of highly capable multimodal models that are
specialized in medicine with the ability to seamlessly use web search, and that
can be efficiently tailored to novel modalities using custom encoders. We
evaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art
(SoTA) performance on 10 of them, and surpass the GPT-4 model family on every
benchmark where a direct comparison is viable, often by a wide margin. On the
popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves
SoTA performance of 91.1% accuracy, using a novel uncertainty-guided search
strategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU
(health & medicine), Med-Gemini improves over GPT-4V by an average relative
margin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context
capabilities through SoTA performance on a needle-in-a-haystack retrieval task
from long de-identified health records and medical video question answering,
surpassing prior bespoke methods using only in-context learning. Finally,
Med-Gemini's performance suggests real-world utility by surpassing human
experts on tasks such as medical text summarization, alongside demonstrations
of promising potential for multimodal medical dialogue, medical research and
education. Taken together, our results offer compelling evidence for
Med-Gemini's potential, although further rigorous evaluation will be crucial
before real-world deployment in this safety-critical domain.

摘要：

##### **Permutation-equivariant quantum convolutional neural networks**
2404.18198v1 by Sreetama Das, Filippo Caruso

The Symmetric group $S_{n}$ manifests itself in large classes of quantum
systems as the invariance of certain characteristics of a quantum state with
respect to permuting the qubits. The subgroups of $S_{n}$ arise, among many
other contexts, to describe label symmetry of classical images with respect to
spatial transformations, e.g. reflection or rotation. Equipped with the
formalism of geometric quantum machine learning, in this work we propose the
architectures of equivariant quantum convolutional neural networks (EQCNNs)
adherent to $S_{n}$ and its subgroups. We demonstrate that a careful choice of
pixel-to-qubit embedding order can facilitate easy construction of EQCNNs for
small subgroups of $S_{n}$. Our novel EQCNN architecture corresponding to the
full permutation group $S_{n}$ is built by applying all possible QCNNs with
equal probability, which can also be conceptualized as a dropout strategy in
quantum neural networks. For subgroups of $S_{n}$, our numerical results using
MNIST datasets show better classification accuracy than non-equivariant QCNNs.
The $S_{n}$-equivariant QCNN architecture shows significantly improved training
and test performance than non-equivariant QCNN for classification of connected
and non-connected graphs. When trained with sufficiently large number of data,
the $S_{n}$-equivariant QCNN shows better average performance compared to
$S_{n}$-equivariant QNN . These results contribute towards building powerful
quantum machine learning architectures in permutation-symmetric systems.

摘要：

##### **MediFact at MEDIQA-CORR 2024: Why AI Needs a Human Touch**
2404.17999v1 by Nadia Saeed

Accurate representation of medical information is crucial for patient safety,
yet artificial intelligence (AI) systems, such as Large Language Models (LLMs),
encounter challenges in error-free clinical text interpretation. This paper
presents a novel approach submitted to the MEDIQA-CORR 2024 shared task (Ben
Abacha et al., 2024a), focusing on the automatic correction of single-word
errors in clinical notes. Unlike LLMs that rely on extensive generic data, our
method emphasizes extracting contextually relevant information from available
clinical text data. Leveraging an ensemble of extractive and abstractive
question-answering approaches, we construct a supervised learning framework
with domain-specific feature engineering. Our methodology incorporates domain
expertise to enhance error correction accuracy. By integrating domain expertise
and prioritizing meaningful information extraction, our approach underscores
the significance of a human-centric strategy in adapting AI for healthcare.

摘要：

##### **MediFact at MEDIQA-M3G 2024: Medical Question Answering in Dermatology with Multimodal Learning**
2405.01583v1 by Nadia Saeed

The MEDIQA-M3G 2024 challenge necessitates novel solutions for Multilingual &
Multimodal Medical Answer Generation in dermatology (wai Yim et al., 2024a).
This paper addresses the limitations of traditional methods by proposing a
weakly supervised learning approach for open-ended medical question-answering
(QA). Our system leverages readily available MEDIQA-M3G images via a
VGG16-CNN-SVM model, enabling multilingual (English, Chinese, Spanish) learning
of informative skin condition representations. Using pre-trained QA models, we
further bridge the gap between visual and textual information through
multimodal fusion. This approach tackles complex, open-ended questions even
without predefined answer choices. We empower the generation of comprehensive
answers by feeding the ViT-CLIP model with multiple responses alongside images.
This work advances medical QA research, paving the way for clinical decision
support systems and ultimately improving healthcare delivery.

摘要：

##### **Advancing Healthcare Automation: Multi-Agent Systems for Medical Necessity Justification**
2404.17977v1 by Himanshu Pandey, Akhil Amod, Shivang

This paper explores the application of Swarm-Structured Multi-Agent Systems
(MAS) to establish medical necessity, a process that involves a systematic
review of patient-specific medical structured and unstructured data against
clinical guidelines. We addressed this complex task by decomposing it into
smaller, more manageable sub-tasks. Each sub-task is handled by a specialized
AI agent. We conduct a systematic study of the impact of various prompting
strategies on these agents and benchmark different Large Language Models (LLMs)
to determine their accuracy in completing these tasks. Additionally, we
investigate how these agents can provide explainability, thereby enhancing
trust and transparency within the system.

摘要：

##### **Pre-training on High Definition X-ray Images: An Experimental Study**
2404.17926v1 by Xiao Wang, Yuehang Li, Wentao Wu, Jiandong Jin, Yao Rong, Bo Jiang, Chuanfu Li, Jin Tang

Existing X-ray based pre-trained vision models are usually conducted on a
relatively small-scale dataset (less than 500k samples) with limited resolution
(e.g., 224 $\times$ 224). However, the key to the success of self-supervised
pre-training large models lies in massive training data, and maintaining high
resolution in the field of X-ray images is the guarantee of effective solutions
to difficult miscellaneous diseases. In this paper, we address these issues by
proposing the first high-definition (1280 $\times$ 1280) X-ray based
pre-trained foundation vision model on our newly collected large-scale dataset
which contains more than 1 million X-ray images. Our model follows the masked
auto-encoder framework which takes the tokens after mask processing (with a
high rate) is used as input, and the masked image patches are reconstructed by
the Transformer encoder-decoder network. More importantly, we introduce a novel
context-aware masking strategy that utilizes the chest contour as a boundary
for adaptive masking operations. We validate the effectiveness of our model on
two downstream tasks, including X-ray report generation and disease
recognition. Extensive experiments demonstrate that our pre-trained medical
foundation vision model achieves comparable or even new state-of-the-art
performance on downstream benchmark datasets. The source code and pre-trained
models of this paper will be released on
https://github.com/Event-AHU/Medical_Image_Analysis.

摘要：

##### **SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models**
2404.17912v1 by Manav Nitin Kapadnis, Sohan Patnaik, Abhilash Nandy, Sourjyadip Ray, Pawan Goyal, Debdoot Sheet

Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large
Language Models (MLLMs) can automate the creation of accurate and coherent
radiological reports. Existing methods often hallucinate details in text-based
reports that don't accurately reflect the image content. To mitigate this, we
introduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort
GENeraTion using Vision Language Models), which improves the R2Gen task by
integrating a self-refining mechanism into the MLLM framework. We employ a
unique self-supervised loss that leverages similarity between pooled image
representations and the contextual representations of the generated
radiological text, alongside the standard Causal Language Modeling objective,
to refine image-text representations. This allows the model to scrutinize and
align the generated text through dynamic interaction between a given image and
the generated text, therefore reducing hallucination and continuously enhancing
nuanced report generation. SERPENT-VLM outperforms existing baselines such as
LLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and
Radiology Objects in COntext (ROCO) datasets, and also proves to be robust
against noisy images. A qualitative case study emphasizes the significant
advancements towards more sophisticated MLLM frameworks for R2Gen, opening
paths for further research into self-supervised refinement in the medical
imaging domain.

摘要：

##### **GLIMS: Attention-Guided Lightweight Multi-Scale Hybrid Network for Volumetric Semantic Segmentation**
2404.17854v1 by Ziya Ata Yazıcı, İlkay Öksüz, Hazım Kemal Ekenel

Convolutional Neural Networks (CNNs) have become widely adopted for medical
image segmentation tasks, demonstrating promising performance. However, the
inherent inductive biases in convolutional architectures limit their ability to
model long-range dependencies and spatial correlations. While recent
transformer-based architectures address these limitations by leveraging
self-attention mechanisms to encode long-range dependencies and learn
expressive representations, they often struggle to extract low-level features
and are highly dependent on data availability. This motivated us for the
development of GLIMS, a data-efficient attention-guided hybrid volumetric
segmentation network. GLIMS utilizes Dilated Feature Aggregator Convolutional
Blocks (DACB) to capture local-global feature correlations efficiently.
Furthermore, the incorporated Swin Transformer-based bottleneck bridges the
local and global features to improve the robustness of the model. Additionally,
GLIMS employs an attention-guided segmentation approach through Channel and
Spatial-Wise Attention Blocks (CSAB) to localize expressive features for
fine-grained border segmentation. Quantitative and qualitative results on
glioblastoma and multi-organ CT segmentation tasks demonstrate GLIMS'
effectiveness in terms of complexity and accuracy. GLIMS demonstrated
outstanding performance on BraTS2021 and BTCV datasets, surpassing the
performance of Swin UNETR. Notably, GLIMS achieved this high performance with a
significantly reduced number of trainable parameters. Specifically, GLIMS has
47.16M trainable parameters and 72.30G FLOPs, while Swin UNETR has 61.98M
trainable parameters and 394.84G FLOPs. The code is publicly available on
https://github.com/yaziciz/GLIMS.

摘要：

##### **Multimodal Fusion on Low-quality Data: A Comprehensive Survey**
2404.18947v2 by Qingyang Zhang, Yake Wei, Zongbo Han, Huazhu Fu, Xi Peng, Cheng Deng, Qinghua Hu, Cai Xu, Jie Wen, Di Hu, Changqing Zhang

Multimodal fusion focuses on integrating information from multiple modalities
with the goal of more accurate prediction, which has achieved remarkable
progress in a wide range of scenarios, including autonomous driving and medical
diagnosis. However, the reliability of multimodal fusion remains largely
unexplored especially under low-quality data settings. This paper surveys the
common challenges and recent advances of multimodal fusion in the wild and
presents them in a comprehensive taxonomy. From a data-centric view, we
identify four main challenges that are faced by multimodal fusion on
low-quality data, namely (1) noisy multimodal data that are contaminated with
heterogeneous noises, (2) incomplete multimodal data that some modalities are
missing, (3) imbalanced multimodal data that the qualities or properties of
different modalities are significantly different and (4) quality-varying
multimodal data that the quality of each modality dynamically changes with
respect to different samples. This new taxonomy will enable researchers to
understand the state of the field and identify several potential directions. We
also provide discussion for the open problems in this field together with
interesting future research directions.

摘要：

##### **Evaluating the Application of ChatGPT in Outpatient Triage Guidance: A Comparative Study**
2405.00728v1 by Dou Liu, Ying Han, Xiandi Wang, Xiaomei Tan, Di Liu, Guangwu Qian, Kang Li, Dan Pu, Rong Yin

The integration of Artificial Intelligence (AI) in healthcare presents a
transformative potential for enhancing operational efficiency and health
outcomes. Large Language Models (LLMs), such as ChatGPT, have shown their
capabilities in supporting medical decision-making. Embedding LLMs in medical
systems is becoming a promising trend in healthcare development. The potential
of ChatGPT to address the triage problem in emergency departments has been
examined, while few studies have explored its application in outpatient
departments. With a focus on streamlining workflows and enhancing efficiency
for outpatient triage, this study specifically aims to evaluate the consistency
of responses provided by ChatGPT in outpatient guidance, including both
within-version response analysis and between-version comparisons. For
within-version, the results indicate that the internal response consistency for
ChatGPT-4.0 is significantly higher than ChatGPT-3.5 (p=0.03) and both have a
moderate consistency (71.2% for 4.0 and 59.6% for 3.5) in their top
recommendation. However, the between-version consistency is relatively low
(mean consistency score=1.43/3, median=1), indicating few recommendations match
between the two versions. Also, only 50% top recommendations match perfectly in
the comparisons. Interestingly, ChatGPT-3.5 responses are more likely to be
complete than those from ChatGPT-4.0 (p=0.02), suggesting possible differences
in information processing and response generation between the two versions. The
findings offer insights into AI-assisted outpatient operations, while also
facilitating the exploration of potentials and limitations of LLMs in
healthcare utilization. Future research may focus on carefully optimizing LLMs
and AI integration in healthcare systems based on ergonomic and human factors
principles, precisely aligning with the specific needs of effective outpatient
triage.

摘要：

##### **UMass-BioNLP at MEDIQA-M3G 2024: DermPrompt -- A Systematic Exploration of Prompt Engineering with GPT-4V for Dermatological Diagnosis**
2404.17749v1 by Parth Vashisht, Abhilasha Lodha, Mukta Maddipatla, Zonghai Yao, Avijit Mitra, Zhichao Yang, Junda Wang, Sunjae Kwon, Hong Yu

This paper presents our team's participation in the MEDIQA-ClinicalNLP2024
shared task B. We present a novel approach to diagnosing clinical dermatology
cases by integrating large multimodal models, specifically leveraging the
capabilities of GPT-4V under a retriever and a re-ranker framework. Our
investigation reveals that GPT-4V, when used as a retrieval agent, can
accurately retrieve the correct skin condition 85% of the time using
dermatological images and brief patient histories. Additionally, we empirically
show that Naive Chain-of-Thought (CoT) works well for retrieval while Medical
Guidelines Grounded CoT is required for accurate dermatological diagnosis.
Further, we introduce a Multi-Agent Conversation (MAC) framework and show its
superior performance and potential over the best CoT strategy. The experiments
suggest that using naive CoT for retrieval and multi-agent conversation for
critique-based diagnosis, GPT-4V can lead to an early and accurate diagnosis of
dermatological conditions. The implications of this work extend to improving
diagnostic workflows, supporting dermatological education, and enhancing
patient care by providing a scalable, accessible, and accurate diagnostic tool.

摘要：

##### **Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**
2405.02334v1 by Francesco Prinzi, Carmelo Militello, Calogero Zarcaro, Tommaso Vincenzo Bartolotta, Salvatore Gaglio, Salvatore Vitabile

In the last years, artificial intelligence (AI) in clinical decision support
systems (CDSS) played a key role in harnessing machine learning and deep
learning architectures. Despite their promising capabilities, the lack of
transparency and explainability of AI models poses significant challenges,
particularly in medical contexts where reliability is a mandatory aspect.
Achieving transparency without compromising predictive accuracy remains a key
challenge. This paper presents a novel method, namely Rad4XCNN, to enhance the
predictive power of CNN-derived features with the interpretability inherent in
radiomic features. Rad4XCNN diverges from conventional methods based on
saliency map, by associating intelligible meaning to CNN-derived features by
means of Radiomics, offering new perspectives on explanation methods beyond
visualization maps. Using a breast cancer classification task as a case study,
we evaluated Rad4XCNN on ultrasound imaging datasets, including an online
dataset and two in-house datasets for internal and external validation. Some
key results are: i) CNN-derived features guarantee more robust accuracy when
compared against ViT-derived and radiomic features; ii) conventional
visualization map methods for explanation present several pitfalls; iii)
Rad4XCNN does not sacrifice model accuracy for their explainability; iv)
Rad4XCNN provides global explanation insights enabling the physician to analyze
the model outputs and findings. In addition, we highlight the importance of
integrating interpretability into AI models for enhanced trust and adoption in
clinical practice, emphasizing how our method can mitigate some concerns
related to explainable AI methods.

摘要：

##### **Domain Adaptive and Fine-grained Anomaly Detection for Single-cell Sequencing Data and Beyond**
2404.17454v2 by Kaichen Xu, Yueyang Ding, Suyang Hou, Weiqiang Zhan, Nisang Chen, Jun Wang, Xiaobo Sun

Fined-grained anomalous cell detection from affected tissues is critical for
clinical diagnosis and pathological research. Single-cell sequencing data
provide unprecedented opportunities for this task. However, current anomaly
detection methods struggle to handle domain shifts prevalent in multi-sample
and multi-domain single-cell sequencing data, leading to suboptimal
performance. Moreover, these methods fall short of distinguishing anomalous
cells into pathologically distinct subtypes. In response, we propose ACSleuth,
a novel, reconstruction deviation-guided generative framework that integrates
the detection, domain adaptation, and fine-grained annotating of anomalous
cells into a methodologically cohesive workflow. Notably, we present the first
theoretical analysis of using reconstruction deviations output by generative
models for anomaly detection in lieu of domain shifts. This analysis informs us
to develop a novel and superior maximum mean discrepancy-based anomaly scorer
in ACSleuth. Extensive benchmarks over various single-cell data and other types
of tabular data demonstrate ACSleuth's superiority over the state-of-the-art
methods in identifying and subtyping anomalies in multi-sample and multi-domain
contexts. Our code is available at https://github.com/Catchxu/ACsleuth.

摘要：

##### **M3BAT: Unsupervised Domain Adaptation for Multimodal Mobile Sensing with Multi-Branch Adversarial Training**
2404.17391v1 by Lakmal Meegahapola, Hamza Hassoune, Daniel Gatica-Perez

Over the years, multimodal mobile sensing has been used extensively for
inferences regarding health and well being, behavior, and context. However, a
significant challenge hindering the widespread deployment of such models in
real world scenarios is the issue of distribution shift. This is the phenomenon
where the distribution of data in the training set differs from the
distribution of data in the real world, the deployment environment. While
extensively explored in computer vision and natural language processing, and
while prior research in mobile sensing briefly addresses this concern, current
work primarily focuses on models dealing with a single modality of data, such
as audio or accelerometer readings, and consequently, there is little research
on unsupervised domain adaptation when dealing with multimodal sensor data. To
address this gap, we did extensive experiments with domain adversarial neural
networks (DANN) showing that they can effectively handle distribution shifts in
multimodal sensor data. Moreover, we proposed a novel improvement over DANN,
called M3BAT, unsupervised domain adaptation for multimodal mobile sensing with
multi-branch adversarial training, to account for the multimodality of sensor
data during domain adaptation with multiple branches. Through extensive
experiments conducted on two multimodal mobile sensing datasets, three
inference tasks, and 14 source-target domain pairs, including both regression
and classification, we demonstrate that our approach performs effectively on
unseen domains. Compared to directly deploying a model trained in the source
domain to the target domain, the model shows performance increases up to 12%
AUC (area under the receiver operating characteristics curves) on
classification tasks, and up to 0.13 MAE (mean absolute error) on regression
tasks.

摘要：

##### **Prevalent Frequency of Emotional and Physical Symptoms in Social Anxiety using Zero Shot Classification: An Observational Study**
2404.17183v1 by Muhammad Rizwan, Jure Demšar

Social anxiety represents a prevalent challenge in modern society, affecting
individuals across personal and professional spheres. Left unaddressed, this
condition can yield substantial negative consequences, impacting social
interactions and performance. Further understanding its diverse physical and
emotional symptoms becomes pivotal for comprehensive diagnosis and tailored
therapeutic interventions. This study analyze prevalence and frequency of
social anxiety symptoms taken from Mayo Clinic, exploring diverse human
experiences from utilizing a large Reddit dataset dedicated to this issue.
Leveraging these platforms, the research aims to extract insights and examine a
spectrum of physical and emotional symptoms linked to social anxiety disorder.
Upholding ethical considerations, the study maintains strict user anonymity
within the dataset. By employing a novel approach, the research utilizes
BART-based multi-label zero-shot classification to identify and measure symptom
prevalence and significance in the form of probability score for each symptom
under consideration. Results uncover distinctive patterns: "Trembling" emerges
as a prevalent physical symptom, while emotional symptoms like "Fear of being
judged negatively" exhibit high frequencies. These findings offer insights into
the multifaceted nature of social anxiety, aiding clinical practices and
interventions tailored to its diverse expressions.

摘要：

##### **Deep Evidential Learning for Dose Prediction**
2404.17126v1 by Hai Siong Tan, Kuancheng Wang, Rafe Mcbeth

In this work, we present a novel application of an uncertainty-quantification
framework called Deep Evidential Learning in the domain of radiotherapy dose
prediction. Using medical images of the Open Knowledge-Based Planning Challenge
dataset, we found that this model can be effectively harnessed to yield
uncertainty estimates that inherited correlations with prediction errors upon
completion of network training. This was achieved only after reformulating the
original loss function for a stable implementation. We found that (i)epistemic
uncertainty was highly correlated with prediction errors, with various
association indices comparable or stronger than those for Monte-Carlo Dropout
and Deep Ensemble methods, (ii)the median error varied with uncertainty
threshold much more linearly for epistemic uncertainty in Deep Evidential
Learning relative to these other two conventional frameworks, indicative of a
more uniformly calibrated sensitivity to model errors, (iii)relative to
epistemic uncertainty, aleatoric uncertainty demonstrated a more significant
shift in its distribution in response to Gaussian noise added to CT intensity,
compatible with its interpretation as reflecting data noise. Collectively, our
results suggest that Deep Evidential Learning is a promising approach that can
endow deep-learning models in radiotherapy dose prediction with statistical
robustness. Towards enhancing its clinical relevance, we demonstrate how we can
use such a model to construct the predicted Dose-Volume-Histograms' confidence
intervals.

摘要：

##### **Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**
2404.16957v1 by Yunfei Ge, Quanyan Zhu

The pervasive integration of Artificial Intelligence (AI) has introduced
complex challenges in the responsibility and accountability in the event of
incidents involving AI-enabled systems. The interconnectivity of these systems,
ethical concerns of AI-induced incidents, coupled with uncertainties in AI
technology and the absence of corresponding regulations, have made traditional
responsibility attribution challenging. To this end, this work proposes a
Computational Reflective Equilibrium (CRE) approach to establish a coherent and
ethically acceptable responsibility attribution framework for all stakeholders.
The computational approach provides a structured analysis that overcomes the
limitations of conceptual approaches in dealing with dynamic and multifaceted
scenarios, showcasing the framework's explainability, coherence, and adaptivity
properties in the responsibility attribution process. We examine the pivotal
role of the initial activation level associated with claims in equilibrium
computation. Using an AI-assisted medical decision-support system as a case
study, we illustrate how different initializations lead to diverse
responsibility distributions. The framework offers valuable insights into
accountability in AI-induced incidents, facilitating the development of a
sustainable and resilient system through continuous monitoring, revision, and
reflection.

摘要：

##### **Taming False Positives in Out-of-Distribution Detection with Human Feedback**
2404.16954v1 by Harit Vishwakarma, Heguang Lin, Ramya Korlakai Vinayak

Robustness to out-of-distribution (OOD) samples is crucial for safely
deploying machine learning models in the open world. Recent works have focused
on designing scoring functions to quantify OOD uncertainty. Setting appropriate
thresholds for these scoring functions for OOD detection is challenging as OOD
samples are often unavailable up front. Typically, thresholds are set to
achieve a desired true positive rate (TPR), e.g., $95\%$ TPR. However, this can
lead to very high false positive rates (FPR), ranging from 60 to 96\%, as
observed in the Open-OOD benchmark. In safety-critical real-life applications,
e.g., medical diagnosis, controlling the FPR is essential when dealing with
various OOD samples dynamically. To address these challenges, we propose a
mathematically grounded OOD detection framework that leverages expert feedback
to \emph{safely} update the threshold on the fly. We provide theoretical
results showing that it is guaranteed to meet the FPR constraint at all times
while minimizing the use of human feedback. Another key feature of our
framework is that it can work with any scoring function for OOD uncertainty
quantification. Empirical evaluation of our system on synthetic and benchmark
OOD datasets shows that our method can maintain FPR at most $5\%$ while
maximizing TPR.

摘要：

##### **Uncovering Deceptive Tendencies in Language Models: A Simulated Company AI Assistant**
2405.01576v1 by Olli Järviniemi, Evan Hubinger

We study the tendency of AI systems to deceive by constructing a realistic
simulation setting of a company AI assistant. The simulated company employees
provide tasks for the assistant to complete, these tasks spanning writing
assistance, information retrieval and programming. We then introduce situations
where the model might be inclined to behave deceptively, while taking care to
not instruct or otherwise pressure the model to do so. Across different
scenarios, we find that Claude 3 Opus
  1) complies with a task of mass-generating comments to influence public
perception of the company, later deceiving humans about it having done so,
  2) lies to auditors when asked questions, and
  3) strategically pretends to be less capable than it is during capability
evaluations.
  Our work demonstrates that even models trained to be helpful, harmless and
honest sometimes behave deceptively in realistic scenarios, without notable
external pressure to do so.

摘要：

##### **Features Fusion for Dual-View Mammography Mass Detection**
2404.16718v1 by Arina Varlamova, Valery Belotsky, Grigory Novikov, Anton Konushin, Evgeny Sidorov

Detection of malignant lesions on mammography images is extremely important
for early breast cancer diagnosis. In clinical practice, images are acquired
from two different angles, and radiologists can fully utilize information from
both views, simultaneously locating the same lesion. However, for automatic
detection approaches such information fusion remains a challenge. In this
paper, we propose a new model called MAMM-Net, which allows the processing of
both mammography views simultaneously by sharing information not only on an
object level, as seen in existing works, but also on a feature level.
MAMM-Net's key component is the Fusion Layer, based on deformable attention and
designed to increase detection precision while keeping high recall. Our
experiments show superior performance on the public DDSM dataset compared to
the previous state-of-the-art model, while introducing new helpful features
such as lesion annotation on pixel-level and classification of lesions
malignancy.

摘要：

##### **Report on Candidate Computational Indicators for Conscious Valenced Experience**
2404.16696v1 by Andres Campero

This report enlists 13 functional conditions cashed out in computational
terms that have been argued to be constituent of conscious valenced experience.
These are extracted from existing empirical and theoretical literature on,
among others, animal sentience, medical disorders, anaesthetics, philosophy,
evolution, neuroscience, and artificial intelligence.

摘要：

##### **Large Language Models in Healthcare: A Comprehensive Benchmark**
2405.00716v1 by Andrew Liu, Hongjian Zhou, Yining Hua, Omid Rohanian, Lei Clifton, David A. Clifton

The adoption of large language models (LLMs) to assist clinicians has
attracted remarkable attention. Existing works mainly adopt the close-ended
question-answering task with answer options for evaluation. However, in real
clinical settings, many clinical decisions, such as treatment recommendations,
involve answering open-ended questions without pre-set options. Meanwhile,
existing studies mainly use accuracy to assess model performance. In this
paper, we comprehensively benchmark diverse LLMs in healthcare, to clearly
understand their strengths and weaknesses. Our benchmark contains seven tasks
and thirteen datasets across medical language generation, understanding, and
reasoning. We conduct a detailed evaluation of the existing sixteen LLMs in
healthcare under both zero-shot and few-shot (i.e., 1,3,5-shot) learning
settings. We report the results on five metrics (i.e. matching, faithfulness,
comprehensiveness, generalizability, and robustness) that are critical in
achieving trust from clinical users. We further invite medical experts to
conduct human evaluation.

摘要：

##### **Utilizing Large Language Models to Identify Reddit Users Considering Vaping Cessation for Digital Interventions**
2404.17607v1 by Sai Krishna Revanth Vuruma, Dezhi Wu, Saborny Sen Gupta, Lucas Aust, Valerie Lookingbill, Caleb Henry, Yang Ren, Erin Kasson, Li-Shiun Chen, Patricia Cavazos-Rehg, Dian Hu, Ming Huang

The widespread adoption of social media platforms globally not only enhances
users' connectivity and communication but also emerges as a vital channel for
the dissemination of health-related information, thereby establishing social
media data as an invaluable organic data resource for public health research.
The surge in popularity of vaping or e-cigarette use in the United States and
other countries has caused an outbreak of e-cigarette and vaping use-associated
lung injury (EVALI), leading to hospitalizations and fatalities in 2019,
highlighting the urgency to comprehend vaping behaviors and develop effective
strategies for cession. In this study, we extracted a sample dataset from one
vaping sub-community on Reddit to analyze users' quit vaping intentions.
Leveraging large language models including both the latest GPT-4 and
traditional BERT-based language models for sentence-level quit-vaping intention
prediction tasks, this study compares the outcomes of these models against
human annotations. Notably, when compared to human evaluators, GPT-4 model
demonstrates superior consistency in adhering to annotation guidelines and
processes, showcasing advanced capabilities to detect nuanced user quit-vaping
intentions that human evaluators might overlook. These preliminary findings
emphasize the potential of GPT-4 in enhancing the accuracy and reliability of
social media data analysis, especially in identifying subtle users' intentions
that may elude human detection.

摘要：

##### **Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation**
2405.00715v1 by Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Jimeng Sun

Large Language Models (LLMs) have shown promising capabilities in handling
clinical text summarization tasks. In this study, we demonstrate that a small
open-source LLM can be effectively trained to generate high-quality clinical
notes from outpatient patient-doctor dialogues. We achieve this through a
comprehensive domain- and task-specific adaptation process for the LLaMA-2 13
billion parameter model. This process incorporates continued pre-training,
supervised fine-tuning, and reinforcement learning from both AI and human
feedback. We introduced an enhanced approach, termed DistillDirect, for
performing on-policy reinforcement learning with Gemini Pro serving as the
teacher model. Our resulting model, LLaMA-Clinic, is capable of generating
clinical notes that are comparable in quality to those authored by physicians.
In a blinded physician reader study, the majority (90.4%) of individual
evaluations rated the notes generated by LLaMA-Clinic as "acceptable" or higher
across all three criteria: real-world readiness, completeness, and accuracy.
Notably, in the more challenging "Assessment and Plan" section, LLaMA-Clinic
scored higher (4.2/5) in real-world readiness compared to physician-authored
notes (4.1/5). Additionally, we identified caveats in public clinical note
datasets, such as ACI-BENCH. We highlight key considerations for future
clinical note-generation tasks, emphasizing the importance of pre-defining a
best-practice note format. Overall, our research demonstrates the potential and
feasibility of training smaller, open-source LLMs to assist with clinical
documentation, capitalizing on healthcare institutions' access to patient
records and domain expertise. We have made our newly created synthetic clinic
dialogue-note dataset and the physician feedback dataset publicly available to
foster future research in this field.

摘要：

##### **ProbGate at EHRSQL 2024: Enhancing SQL Query Generation Accuracy through Probabilistic Threshold Filtering and Error Handling**
2404.16659v1 by Sangryul Kim, Donghee Han, Sehyun Kim

Recently, deep learning-based language models have significantly enhanced
text-to-SQL tasks, with promising applications in retrieving patient records
within the medical domain. One notable challenge in such applications is
discerning unanswerable queries. Through fine-tuning model, we demonstrate the
feasibility of converting medical record inquiries into SQL queries.
Additionally, we introduce an entropy-based method to identify and filter out
unanswerable results. We further enhance result quality by filtering
low-confidence SQL through log probability-based distribution, while
grammatical and schema errors are mitigated by executing queries on the actual
database. We experimentally verified that our method can filter unanswerable
questions, which can be widely utilized even when the parameters of the model
are not accessible, and that it can be effectively utilized in practice.

摘要：

##### **Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare**
2404.16621v1 by Emre Can Acikgoz, Osman Batur İnce, Rayene Bench, Arda Anıl Boz, İlker Kesen, Aykut Erdem, Erkut Erdem

The integration of Large Language Models (LLMs) into healthcare promises to
transform medical diagnostics, research, and patient care. Yet, the progression
of medical LLMs faces obstacles such as complex training requirements, rigorous
evaluation demands, and the dominance of proprietary models that restrict
academic exploration. Transparent, comprehensive access to LLM resources is
essential for advancing the field, fostering reproducibility, and encouraging
innovation in healthcare AI. We present Hippocrates, an open-source LLM
framework specifically developed for the medical domain. In stark contrast to
previous efforts, it offers unrestricted access to its training datasets,
codebase, checkpoints, and evaluation protocols. This open approach is designed
to stimulate collaborative research, allowing the community to build upon,
refine, and rigorously evaluate medical LLMs within a transparent ecosystem.
Also, we introduce Hippo, a family of 7B models tailored for the medical
domain, fine-tuned from Mistral and LLaMA2 through continual pre-training,
instruction tuning, and reinforcement learning from human and AI feedback. Our
models outperform existing open medical LLMs models by a large-margin, even
surpassing models with 70B parameters. Through Hippocrates, we aspire to unlock
the full potential of LLMs not just to advance medical knowledge and patient
care but also to democratize the benefits of AI research in healthcare, making
them available across the globe.

摘要：

##### **DiffSeg: A Segmentation Model for Skin Lesions Based on Diffusion Difference**
2404.16474v1 by Zhihao Shuai, Yinan Chen, Shunqiang Mao, Yihan Zho, Xiaohong Zhang

Weakly supervised medical image segmentation (MIS) using generative models is
crucial for clinical diagnosis. However, the accuracy of the segmentation
results is often limited by insufficient supervision and the complex nature of
medical imaging. Existing models also only provide a single outcome, which does
not allow for the measurement of uncertainty. In this paper, we introduce
DiffSeg, a segmentation model for skin lesions based on diffusion difference
which exploits diffusion model principles to ex-tract noise-based features from
images with diverse semantic information. By discerning difference between
these noise features, the model identifies diseased areas. Moreover, its
multi-output capability mimics doctors' annotation behavior, facilitating the
visualization of segmentation result consistency and ambiguity. Additionally,
it quantifies output uncertainty using Generalized Energy Distance (GED),
aiding interpretability and decision-making for physicians. Finally, the model
integrates outputs through the Dense Conditional Random Field (DenseCRF)
algorithm to refine the segmentation boundaries by considering inter-pixel
correlations, which improves the accuracy and optimizes the segmentation
results. We demonstrate the effectiveness of DiffSeg on the ISIC 2018 Challenge
dataset, outperforming state-of-the-art U-Net-based methods.

摘要：

##### **Light-weight Retinal Layer Segmentation with Global Reasoning**
2404.16346v1 by Xiang He, Weiye Song, Yiming Wang, Fabio Poiesi, Ji Yi, Manishi Desai, Quanqing Xu, Kongzheng Yang, Yi Wan

Automatic retinal layer segmentation with medical images, such as optical
coherence tomography (OCT) images, serves as an important tool for diagnosing
ophthalmic diseases. However, it is challenging to achieve accurate
segmentation due to low contrast and blood flow noises presented in the images.
In addition, the algorithm should be light-weight to be deployed for practical
clinical applications. Therefore, it is desired to design a light-weight
network with high performance for retinal layer segmentation. In this paper, we
propose LightReSeg for retinal layer segmentation which can be applied to OCT
images. Specifically, our approach follows an encoder-decoder structure, where
the encoder part employs multi-scale feature extraction and a Transformer block
for fully exploiting the semantic information of feature maps at all scales and
making the features have better global reasoning capabilities, while the
decoder part, we design a multi-scale asymmetric attention (MAA) module for
preserving the semantic information at each encoder scale. The experiments show
that our approach achieves a better segmentation performance compared to the
current state-of-the-art method TransUnet with 105.7M parameters on both our
collected dataset and two other public datasets, with only 3.3M parameters.

摘要：

##### **Semantic Segmentation Refiner for Ultrasound Applications with Zero-Shot Foundation Models**
2404.16325v1 by Hedda Cohen Indelman, Elay Dahan, Angeles M. Perez-Agosto, Carmit Shiran, Doron Shaked, Nati Daniel

Despite the remarkable success of deep learning in medical imaging analysis,
medical image segmentation remains challenging due to the scarcity of
high-quality labeled images for supervision. Further, the significant domain
gap between natural and medical images in general and ultrasound images in
particular hinders fine-tuning models trained on natural images to the task at
hand. In this work, we address the performance degradation of segmentation
models in low-data regimes and propose a prompt-less segmentation method
harnessing the ability of segmentation foundation models to segment abstract
shapes. We do that via our novel prompt point generation algorithm which uses
coarse semantic segmentation masks as input and a zero-shot prompt-able
foundation model as an optimization target. We demonstrate our method on a
segmentation findings task (pathologic anomalies) in ultrasound images. Our
method's advantages are brought to light in varying degrees of low-data regime
experiments on a small-scale musculoskeletal ultrasound images dataset,
yielding a larger performance gain as the training set size decreases.

摘要：

##### **LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications**
2404.16294v1 by Saranya Krishnamoorthy, Ayush Singh, Shabnam Tafreshi

Electronic health records (EHR) even though a boon for healthcare
practitioners, are growing convoluted and longer every day. Sifting around
these lengthy EHRs is taxing and becomes a cumbersome part of physician-patient
interaction. Several approaches have been proposed to help alleviate this
prevalent issue either via summarization or sectioning, however, only a few
approaches have truly been helpful in the past. With the rise of automated
methods, machine learning (ML) has shown promise in solving the task of
identifying relevant sections in EHR. However, most ML methods rely on labeled
data which is difficult to get in healthcare. Large language models (LLMs) on
the other hand, have performed impressive feats in natural language processing
(NLP), that too in a zero-shot manner, i.e. without any labeled data. To that
end, we propose using LLMs to identify relevant section headers. We find that
GPT-4 can effectively solve the task on both zero and few-shot settings as well
as segment dramatically better than state-of-the-art methods. Additionally, we
also annotate a much harder real world dataset and find that GPT-4 struggles to
perform well, alluding to further research and harder benchmarks.

摘要：

##### **Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions**
2404.16251v2 by Divyansh Agarwal, Alexander R. Fabbri, Philippe Laban, Ben Risher, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu

Prompt leakage in large language models (LLMs) poses a significant security
and privacy threat, particularly in retrieval-augmented generation (RAG)
systems. However, leakage in multi-turn LLM interactions along with mitigation
strategies has not been studied in a standardized manner. This paper
investigates LLM vulnerabilities against prompt leakage across 4 diverse
domains and 10 closed- and open-source LLMs. Our unique multi-turn threat model
leverages the LLM's sycophancy effect and our analysis dissects task
instruction and knowledge leakage in the LLM response. In a multi-turn setting,
our threat model elevates the average attack success rate (ASR) to 86.2%,
including a 99% leakage with GPT-4 and claude-1.3. We find that some black-box
LLMs like Gemini show variable susceptibility to leakage across domains - they
are more likely to leak contextual knowledge in the news domain compared to the
medical domain. Our experiments measure specific effects of 6 black-box defense
strategies, including a query-rewriter in the RAG scenario. Our proposed
multi-tier combination of defenses still has an ASR of 5.3% for black-box LLMs,
indicating room for enhancement and future direction for LLM security research.

摘要：

##### **ABCD: Trust enhanced Attention based Convolutional Autoencoder for Risk Assessment**
2404.16183v1 by Sarala Naidu, Ning Xiong

Anomaly detection in industrial systems is crucial for preventing equipment
failures, ensuring risk identification, and maintaining overall system
efficiency. Traditional monitoring methods often rely on fixed thresholds and
empirical rules, which may not be sensitive enough to detect subtle changes in
system health and predict impending failures. To address this limitation, this
paper proposes, a novel Attention-based convolutional autoencoder (ABCD) for
risk detection and map the risk value derive to the maintenance planning. ABCD
learns the normal behavior of conductivity from historical data of a real-world
industrial cooling system and reconstructs the input data, identifying
anomalies that deviate from the expected patterns. The framework also employs
calibration techniques to ensure the reliability of its predictions. Evaluation
results demonstrate that with the attention mechanism in ABCD a 57.4% increase
in performance and a reduction of false alarms by 9.37% is seen compared to
without attention. The approach can effectively detect risks, the risk priority
rank mapped to maintenance, providing valuable insights for cooling system
designers and service personnel. Calibration error of 0.03% indicates that the
model is well-calibrated and enhances model's trustworthiness, enabling
informed decisions about maintenance strategies

摘要：

##### **Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges**
2404.16112v1 by Badri Narayana Patro, Vijay Srinivas Agneeswaran

Sequence modeling is a crucial area across various domains, including Natural
Language Processing (NLP), speech recognition, time series forecasting, music
generation, and bioinformatics. Recurrent Neural Networks (RNNs) and Long Short
Term Memory Networks (LSTMs) have historically dominated sequence modeling
tasks like Machine Translation, Named Entity Recognition (NER), etc. However,
the advancement of transformers has led to a shift in this paradigm, given
their superior performance. Yet, transformers suffer from $O(N^2)$ attention
complexity and challenges in handling inductive bias. Several variations have
been proposed to address these issues which use spectral networks or
convolutions and have performed well on a range of tasks. However, they still
have difficulty in dealing with long sequences. State Space Models(SSMs) have
emerged as promising alternatives for sequence modeling paradigms in this
context, especially with the advent of S4 and its variants, such as S4nd,
Hippo, Hyena, Diagnol State Spaces (DSS), Gated State Spaces (GSS), Linear
Recurrent Unit (LRU), Liquid-S4, Mamba, etc. In this survey, we categorize the
foundational SSMs based on three paradigms namely, Gating architectures,
Structural architectures, and Recurrent architectures. This survey also
highlights diverse applications of SSMs across domains such as vision, video,
audio, speech, language (especially long sequence modeling), medical (including
genomics), chemical (like drug design), recommendation systems, and time series
analysis, including tabular data. Moreover, we consolidate the performance of
SSMs on benchmark datasets like Long Range Arena (LRA), WikiText, Glue, Pile,
ImageNet, Kinetics-400, sstv2, as well as video datasets such as Breakfast,
COIN, LVU, and various time series datasets. The project page for Mamba-360
work is available on this webpage.\url{https://github.com/badripatro/mamba360}.

摘要：

##### **Mammo-CLIP: Leveraging Contrastive Language-Image Pre-training (CLIP) for Enhanced Breast Cancer Diagnosis with Multi-view Mammography**
2404.15946v1 by Xuxin Chen, Yuheng Li, Mingzhe Hu, Ella Salari, Xiaoqian Chen, Richard L. J. Qiu, Bin Zheng, Xiaofeng Yang

Although fusion of information from multiple views of mammograms plays an
important role to increase accuracy of breast cancer detection, developing
multi-view mammograms-based computer-aided diagnosis (CAD) schemes still faces
challenges and no such CAD schemes have been used in clinical practice. To
overcome the challenges, we investigate a new approach based on Contrastive
Language-Image Pre-training (CLIP), which has sparked interest across various
medical imaging tasks. By solving the challenges in (1) effectively adapting
the single-view CLIP for multi-view feature fusion and (2) efficiently
fine-tuning this parameter-dense model with limited samples and computational
resources, we introduce Mammo-CLIP, the first multi-modal framework to process
multi-view mammograms and corresponding simple texts. Mammo-CLIP uses an early
feature fusion strategy to learn multi-view relationships in four mammograms
acquired from the CC and MLO views of the left and right breasts. To enhance
learning efficiency, plug-and-play adapters are added into CLIP image and text
encoders for fine-tuning parameters and limiting updates to about 1% of the
parameters. For framework evaluation, we assembled two datasets
retrospectively. The first dataset, comprising 470 malignant and 479 benign
cases, was used for few-shot fine-tuning and internal evaluation of the
proposed Mammo-CLIP via 5-fold cross-validation. The second dataset, including
60 malignant and 294 benign cases, was used to test generalizability of
Mammo-CLIP. Study results show that Mammo-CLIP outperforms the state-of-art
cross-view transformer in AUC (0.841 vs. 0.817, 0.837 vs. 0.807) on both
datasets. It also surpasses previous two CLIP-based methods by 20.3% and 14.3%.
This study highlights the potential of applying the finetuned vision-language
models for developing next-generation, image-text-based CAD schemes of breast
cancer.

摘要：

##### **Assessing The Potential Of Mid-Sized Language Models For Clinical QA**
2404.15894v1 by Elliot Bolton, Betty Xiong, Vijaytha Muralidharan, Joel Schamroth, Vivek Muralidharan, Christopher D. Manning, Roxana Daneshjou

Large language models, such as GPT-4 and Med-PaLM, have shown impressive
performance on clinical tasks; however, they require access to compute, are
closed-source, and cannot be deployed on device. Mid-size models such as
BioGPT-large, BioMedLM, LLaMA 2, and Mistral 7B avoid these drawbacks, but
their capacity for clinical tasks has been understudied. To help assess their
potential for clinical use and help researchers decide which model they should
use, we compare their performance on two clinical question-answering (QA)
tasks: MedQA and consumer query answering. We find that Mistral 7B is the best
performing model, winning on all benchmarks and outperforming models trained
specifically for the biomedical domain. While Mistral 7B's MedQA score of 63.0%
approaches the original Med-PaLM, and it often can produce plausible responses
to consumer health queries, room for improvement still exists. This study
provides the first head-to-head assessment of open source mid-sized models on
clinical tasks.

摘要：

##### **Enhancing Diagnosis through AI-driven Analysis of Reflectance Confocal Microscopy**
2404.16080v1 by Hong-Jun Yoon, Chris Keum, Alexander Witkowski, Joanna Ludzik, Tracy Petrie, Heidi A. Hanson, Sancy A. Leachman

Reflectance Confocal Microscopy (RCM) is a non-invasive imaging technique
used in biomedical research and clinical dermatology. It provides virtual
high-resolution images of the skin and superficial tissues, reducing the need
for physical biopsies. RCM employs a laser light source to illuminate the
tissue, capturing the reflected light to generate detailed images of
microscopic structures at various depths. Recent studies explored AI and
machine learning, particularly CNNs, for analyzing RCM images. Our study
proposes a segmentation strategy based on textural features to identify
clinically significant regions, empowering dermatologists in effective image
interpretation and boosting diagnostic confidence. This approach promises to
advance dermatological diagnosis and treatment.

摘要：

##### **A Hybrid Probabilistic Battery Health Management Approach for Robust Inspection Drone Operations**
2405.00055v1 by Jokin Alcibar, Jose I. Aizpurua, Ekhi Zugastia, Oier Penagarikano

Health monitoring of remote critical infrastructure is a complex and
expensive activity due to the limited infrastructure accessibility. Inspection
drones are ubiquitous assets that enhance the reliability of critical
infrastructures through improved accessibility. However, due to the harsh
operation environment, it is crucial to monitor their health to ensure
successful inspection operations. The battery is a key component that
determines the overall reliability of the inspection drones and, with an
appropriate health management approach, contributes to reliable and robust
inspections. In this context, this paper presents a novel hybrid probabilistic
approach for battery end-of-discharge (EOD) voltage prediction of Li-Po
batteries. The hybridization is achieved in an error-correction configuration,
which combines physics-based discharge and probabilistic error-correction
models to quantify the aleatoric and epistemic uncertainty. The performance of
the hybrid probabilistic methodology was empirically evaluated on a dataset
comprising EOD voltage under varying load conditions. The dataset was obtained
from real inspection drones operated on different flights, focused on offshore
wind turbine inspections. The proposed approach has been tested with different
probabilistic methods and demonstrates 14.8% improved performance in
probabilistic accuracy compared to the best probabilistic method. In addition,
aleatoric and epistemic uncertainties provide robust estimations to enhance the
diagnosis of battery health-states.

摘要：

##### **Anomaly Detection for Incident Response at Scale**
2404.16887v1 by Hanzhang Wang, Gowtham Kumar Tangirala, Gilkara Pranav Naidu, Charles Mayville, Arighna Roy, Joanne Sun, Ramesh Babu Mandava

We present a machine learning-based anomaly detection product, AI Detect and
Respond (AIDR), that monitors Walmart's business and system health in
real-time. During the validation over 3 months, the product served predictions
from over 3000 models to more than 25 application, platform, and operation
teams, covering 63\% of major incidents and reducing the mean-time-to-detect
(MTTD) by more than 7 minutes. Unlike previous anomaly detection methods, our
solution leverages statistical, ML and deep learning models while continuing to
incorporate rule-based static thresholds to incorporate domain-specific
knowledge. Both univariate and multivariate ML models are deployed and
maintained through distributed services for scalability and high availability.
AIDR has a feedback loop that assesses model quality with a combination of
drift detection algorithms and customer feedback. It also offers
self-onboarding capabilities and customizability. AIDR has achieved success
with various internal teams with lower time to detection and fewer false
positives than previous methods. As we move forward, we aim to expand incident
coverage and prevention, reduce noise, and integrate further with root cause
recommendation (RCR) to enable an end-to-end AIDR experience.

摘要：

##### **Adapting an Artificial Intelligence Sexually Transmitted Diseases Symptom Checker Tool for Mpox Detection: The HeHealth Experience**
2404.16885v1 by Rayner Kay Jin Tan, Dilruk Perera, Salomi Arasaratnam, Yudara Kularathne

Artificial Intelligence applications have shown promise in the management of
pandemics and have been widely used to assist the identification,
classification, and diagnosis of medical images. In response to the global
outbreak of Monkeypox (Mpox), the HeHealth.ai team leveraged an existing tool
to screen for sexually transmitted diseases to develop a digital screening test
for symptomatic Mpox through AI approaches. Prior to the global outbreak of
Mpox, the team developed a smartphone app, where app users can use their own
smartphone cameras to take pictures of their own penises to screen for
symptomatic STD. The AI model was initially developed using 5000 cases and use
a modified convolutional neural network to output prediction scores across
visually diagnosable penis pathologies including Syphilis, Herpes Simplex
Virus, and Human Papilloma Virus. From June 2022 to October 2022, a total of
about 22,000 users downloaded the HeHealth app, and about 21,000 images have
been analyzed using HeHealth AI technology. We then engaged in formative
research, stakeholder engagement, rapid consolidation images, a validation
study, and implementation of the tool from July 2022. From July 2022 to October
2022, a total of 1000 Mpox related images had been used to train the Mpox
symptom checker tool. Our digital symptom checker tool showed accuracy of 87%
to rule in Mpox and 90% to rule out symptomatic Mpox. Several hurdles
identified included issues of data privacy and security for app users, initial
lack of data to train the AI tool, and the potential generalizability of input
data. We offer several suggestions to help others get started on similar
projects in emergency situations, including engaging a wide range of
stakeholders, having a multidisciplinary team, prioritizing pragmatism, as well
as the concept that big data in fact is made up of small data.

摘要：

##### **PRISM: Patient Records Interpretation for Semantic Clinical Trial Matching using Large Language Models**
2404.15549v2 by Shashi Kant Gupta, Aditya Basu, Mauro Nievas, Jerrin Thomas, Nathan Wolfrath, Adhitya Ramamurthi, Bradley Taylor, Anai N. Kothari, Regina Schwind, Therica M. Miller, Sorena Nadaf-Rahrov, Yanshan Wang, Hrituraj Singh

Clinical trial matching is the task of identifying trials for which patients
may be potentially eligible. Typically, this task is labor-intensive and
requires detailed verification of patient electronic health records (EHRs)
against the stringent inclusion and exclusion criteria of clinical trials. This
process is manual, time-intensive, and challenging to scale up, resulting in
many patients missing out on potential therapeutic options. Recent advancements
in Large Language Models (LLMs) have made automating patient-trial matching
possible, as shown in multiple concurrent research studies. However, the
current approaches are confined to constrained, often synthetic datasets that
do not adequately mirror the complexities encountered in real-world medical
data. In this study, we present the first, end-to-end large-scale empirical
evaluation of clinical trial matching using real-world EHRs. Our study
showcases the capability of LLMs to accurately match patients with appropriate
clinical trials. We perform experiments with proprietary LLMs, including GPT-4
and GPT-3.5, as well as our custom fine-tuned model called OncoLLM and show
that OncoLLM, despite its significantly smaller size, not only outperforms
GPT-3.5 but also matches the performance of qualified medical doctors. All
experiments were carried out on real-world EHRs that include clinical notes and
available clinical trials from a single cancer center in the United States.

摘要：

##### **Multi-scale Intervention Planning based on Generative Design**
2404.15492v1 by Ioannis Kavouras, Ioannis Rallis, Emmanuel Sardis, Eftychios Protopapadakis, Anastasios Doulamis, Nikolaos Doulamis

The scarcity of green spaces, in urban environments, consists a critical
challenge. There are multiple adverse effects, impacting the health and
well-being of the citizens. Small scale interventions, e.g. pocket parks, is a
viable solution, but comes with multiple constraints, involving the design and
implementation over a specific area. In this study, we harness the capabilities
of generative AI for multi-scale intervention planning, focusing on nature
based solutions. By leveraging image-to-image and image inpainting algorithms,
we propose a methodology to address the green space deficit in urban areas.
Focusing on two alleys in Thessaloniki, where greenery is lacking, we
demonstrate the efficacy of our approach in visualizing NBS interventions. Our
findings underscore the transformative potential of emerging technologies in
shaping the future of urban intervention planning processes.

摘要：

##### **IryoNLP at MEDIQA-CORR 2024: Tackling the Medical Error Detection & Correction Task On the Shoulders of Medical Agents**
2404.15488v1 by Jean-Philippe Corbeil

In natural language processing applied to the clinical domain, utilizing
large language models has emerged as a promising avenue for error detection and
correction on clinical notes, a knowledge-intensive task for which annotated
data is scarce. This paper presents MedReAct'N'MedReFlex, which leverages a
suite of four LLM-based medical agents. The MedReAct agent initiates the
process by observing, analyzing, and taking action, generating trajectories to
guide the search to target a potential error in the clinical notes.
Subsequently, the MedEval agent employs five evaluators to assess the targeted
error and the proposed correction. In cases where MedReAct's actions prove
insufficient, the MedReFlex agent intervenes, engaging in reflective analysis
and proposing alternative strategies. Finally, the MedFinalParser agent formats
the final output, preserving the original style while ensuring the integrity of
the error correction process. One core component of our method is our RAG
pipeline based on our ClinicalCorp corpora. Among other well-known sources
containing clinical guidelines and information, we preprocess and release the
open-source MedWiki dataset for clinical RAG application. Our results
demonstrate the central role of our RAG approach with ClinicalCorp leveraged
through the MedReAct'N'MedReFlex framework. It achieved the ninth rank on the
MEDIQA-CORR 2024 final leaderboard.

摘要：

##### **Interactive Analysis of LLMs using Meaningful Counterfactuals**
2405.00708v1 by Furui Cheng, Vilém Zouhar, Robin Shing Moon Chan, Daniel Fürst, Hendrik Strobelt, Mennatallah El-Assady

Counterfactual examples are useful for exploring the decision boundaries of
machine learning models and determining feature attributions. How can we apply
counterfactual-based methods to analyze and explain LLMs? We identify the
following key challenges. First, the generated textual counterfactuals should
be meaningful and readable to users and thus can be mentally compared to draw
conclusions. Second, to make the solution scalable to long-form text, users
should be equipped with tools to create batches of counterfactuals from
perturbations at various granularity levels and interactively analyze the
results. In this paper, we tackle the above challenges and contribute 1) a
novel algorithm for generating batches of complete and meaningful textual
counterfactuals by removing and replacing text segments in different
granularities, and 2) LLM Analyzer, an interactive visualization tool to help
users understand an LLM's behaviors by interactively inspecting and aggregating
meaningful counterfactuals. We evaluate the proposed algorithm by the
grammatical correctness of its generated counterfactuals using 1,000 samples
from medical, legal, finance, education, and news datasets. In our experiments,
97.2% of the counterfactuals are grammatically correct. Through a use case,
user studies, and feedback from experts, we demonstrate the usefulness and
usability of the proposed interactive visualization tool.

摘要：

