{"2409.09788": {"publish_time": "2024-09-15", "title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models", "paper_summary": "Despite recent advances demonstrating vision-language models' (VLMs)\nabilities to describe complex relationships in images using natural language,\ntheir capability to quantitatively reason about object sizes and distances\nremains underexplored. In this work, we introduce a manually annotated\nbenchmark, Q-Spatial Bench, with 271 questions across five categories designed\nfor quantitative spatial reasoning and systematically investigate the\nperformance of state-of-the-art VLMs on this task. Our analysis reveals that\nreasoning about distances between objects is particularly challenging for SoTA\nVLMs; however, some VLMs significantly outperform others, with an over 40-point\ngap between the two best performing models. We also make the surprising\nobservation that the success rate of the top-performing VLM increases by 19\npoints when a reasoning path using a reference object emerges naturally in the\nresponse. Inspired by this observation, we develop a zero-shot prompting\ntechnique, SpatialPrompt, that encourages VLMs to answer quantitative spatial\nquestions using reference objects as visual cues. By instructing VLMs to use\nreference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro,\nGemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30\npoints, respectively. We emphasize that these significant improvements are\nobtained without needing more data, model architectural modifications, or\nfine-tuning.", "paper_summary_zh": "\u5118\u7ba1\u6700\u8fd1\u7684\u9032\u5c55\u5c55\u793a\u4e86\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4f7f\u7528\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\u5f71\u50cf\u4e2d\u8907\u96dc\u95dc\u4fc2\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u5c0d\u7269\u4ef6\u5927\u5c0f\u548c\u8ddd\u96e2\u9032\u884c\u91cf\u5316\u63a8\u7406\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u624b\u52d5\u8a3b\u89e3\u57fa\u6e96 Q-Spatial Bench\uff0c\u5176\u4e2d\u5305\u542b 271 \u500b\u554f\u984c\uff0c\u6a6b\u8de8\u4e94\u500b\u985e\u5225\uff0c\u5c08\u9580\u7528\u65bc\u91cf\u5316\u7a7a\u9593\u63a8\u7406\uff0c\u4e26\u7cfb\u7d71\u6027\u5730\u7814\u7a76\u4e86\u6700\u5148\u9032\u7684 VLM \u5728\u9019\u9805\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u5c0d\u7269\u4ef6\u4e4b\u9593\u8ddd\u96e2\u7684\u63a8\u7406\u5c0d SoTA VLM \u4f86\u8aaa\u7279\u5225\u5177\u6709\u6311\u6230\u6027\uff1b\u7136\u800c\uff0c\u4e00\u4e9b VLM \u7684\u8868\u73fe\u660e\u986f\u512a\u65bc\u5176\u4ed6 VLM\uff0c\u8868\u73fe\u6700\u597d\u7684\u5169\u500b\u6a21\u578b\u4e4b\u9593\u7684\u5dee\u8ddd\u8d85\u904e 40 \u5206\u3002\u6211\u5011\u9084\u9a5a\u8a1d\u5730\u89c0\u5bdf\u5230\uff0c\u7576\u5728\u56de\u61c9\u4e2d\u81ea\u7136\u51fa\u73fe\u4f7f\u7528\u53c3\u8003\u7269\u4ef6\u7684\u63a8\u7406\u8def\u5f91\u6642\uff0c\u8868\u73fe\u6700\u4f73\u7684 VLM \u7684\u6210\u529f\u7387\u589e\u52a0\u4e86 19 \u5206\u3002\u53d7\u5230\u9019\u4e00\u89c0\u5bdf\u7684\u555f\u767c\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u96f6\u6b21\u63d0\u793a\u6280\u8853 SpatialPrompt\uff0c\u5b83\u9f13\u52f5 VLM \u4f7f\u7528\u53c3\u8003\u7269\u4ef6\u4f5c\u70ba\u8996\u89ba\u63d0\u793a\u4f86\u56de\u7b54\u91cf\u5316\u7a7a\u9593\u554f\u984c\u3002\u901a\u904e SpatialPrompt \u6307\u5c0e VLM \u5728\u5176\u63a8\u7406\u8def\u5f91\u4e2d\u4f7f\u7528\u53c3\u8003\u7269\u4ef6\uff0cGemini 1.5 Pro\u3001Gemini 1.5 Flash \u548c GPT-4V \u5206\u5225\u5c07\u5176\u6210\u529f\u7387\u63d0\u9ad8\u4e86 40\u300120 \u548c 30 \u5206\u3002\u6211\u5011\u5f37\u8abf\uff0c\u9019\u4e9b\u986f\u8457\u7684\u6539\u9032\u662f\u5728\u4e0d\u9700\u8981\u66f4\u591a\u8cc7\u6599\u3001\u6a21\u578b\u67b6\u69cb\u4fee\u6539\u6216\u5fae\u8abf\u7684\u60c5\u6cc1\u4e0b\u7372\u5f97\u7684\u3002", "author": "Yuan-Hong Liao et.al.", "authors": "Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna", "id": "2409.09788v1", "paper_url": "http://arxiv.org/abs/2409.09788v1", "repo": "null"}}