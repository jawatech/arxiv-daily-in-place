
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-01-24**|**Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**|Ipek Baris Schlicht et.al.|[2501.14719v1](http://arxiv.org/abs/2501.14719v1)|null|
|**2025-01-24**|**Rethinking Table Instruction Tuning**|Naihao Deng et.al.|[2501.14693v1](http://arxiv.org/abs/2501.14693v1)|null|
|**2025-01-24**|**Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI**|Dmitry Ryabtsev et.al.|[2501.14689v1](http://arxiv.org/abs/2501.14689v1)|null|
|**2025-01-24**|**Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST**|Fuping Wu et.al.|[2501.14685v1](http://arxiv.org/abs/2501.14685v1)|null|
|**2025-01-24**|**MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**|Yixing Jiang et.al.|[2501.14654v1](http://arxiv.org/abs/2501.14654v1)|[link](https://github.com/stanfordmlgroup/medagentbench)|
|**2025-01-24**|**Registration of Longitudinal Liver Examinations for Tumor Progress Assessment**|Walid Yassine et.al.|[2501.14483v1](http://arxiv.org/abs/2501.14483v1)|null|
|**2025-01-24**|**Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design**|Taehan Kim et.al.|[2501.14469v1](http://arxiv.org/abs/2501.14469v1)|null|
|**2025-01-24**|**ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer**|Yoni Schirris et.al.|[2501.14379v1](http://arxiv.org/abs/2501.14379v1)|null|
|**2025-01-24**|**Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation**|Cong-Duy Nguyen et.al.|[2501.14166v1](http://arxiv.org/abs/2501.14166v1)|null|
|**2025-01-24**|**Advancing MRI Reconstruction: A Systematic Review of Deep Learning and Compressed Sensing Integration**|Mojtaba Safari et.al.|[2501.14158v1](http://arxiv.org/abs/2501.14158v1)|[link](https://github.com/mosaf/awesome-dl-based-cs-mri)|
|**2025-01-23**|**MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning**|Joshua Davis et.al.|[2501.14105v1](http://arxiv.org/abs/2501.14105v1)|[link](https://github.com/lindvalllab/medslice)|
|**2025-01-23**|**Revisiting CLIP: Efficient Alignment of 3D MRI and Tabular Data using Domain-Specific Foundation Models**|Jakob Krogh Petersen et.al.|[2501.14051v1](http://arxiv.org/abs/2501.14051v1)|[link](https://github.com/jakekrogh/3d-clip-for-brain-mri)|
|**2025-01-23**|**Leveraging Multiphase CT for Quality Enhancement of Portal Venous CT: Utility for Pancreas Segmentation**|Xinya Wang et.al.|[2501.14013v1](http://arxiv.org/abs/2501.14013v1)|null|
|**2025-01-23**|**Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data**|Frederik Pahde et.al.|[2501.13818v1](http://arxiv.org/abs/2501.13818v1)|[link](https://github.com/frederikpahde/medical-ai-safety)|
|**2025-01-23**|**Question Answering on Patient Medical Records with Private Fine-Tuned LLMs**|Sara Kothari et.al.|[2501.13687v1](http://arxiv.org/abs/2501.13687v1)|null|
|**2025-01-23**|**How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization**|Shezheng Song et.al.|[2501.13669v1](http://arxiv.org/abs/2501.13669v1)|null|
|**2025-01-23**|**Contrastive Representation Learning Helps Cross-institutional Knowledge Transfer: A Study in Pediatric Ventilation Management**|Yuxuan et.al.|[2501.13587v1](http://arxiv.org/abs/2501.13587v1)|null|
|**2025-01-23**|**LLMs Can Plan Only If We Tell Them**|Bilgehan Sel et.al.|[2501.13545v1](http://arxiv.org/abs/2501.13545v1)|null|
|**2025-01-23**|**Comprehensive Modeling and Question Answering of Cancer Clinical Practice Guidelines using LLMs**|Bhumika Gupta et.al.|[2501.13984v1](http://arxiv.org/abs/2501.13984v1)|null|
|**2025-01-23**|**A review on development of eco-friendly filters in Nepal for use in cigarettes and masks and Air Pollution Analysis with Machine Learning and SHAP Interpretability**|Bishwash Paneru et.al.|[2501.13369v1](http://arxiv.org/abs/2501.13369v1)|null|
|**2025-01-22**|**QuFeX: Quantum feature extraction module for hybrid quantum-classical deep neural networks**|Naman Jain et.al.|[2501.13165v1](http://arxiv.org/abs/2501.13165v1)|null|
|**2025-01-22**|**AirRadar: Inferring Nationwide Air Quality in China with Deep Neural Networks**|Qiongyan Wang et.al.|[2501.13141v1](http://arxiv.org/abs/2501.13141v1)|null|
|**2025-01-22**|**Estimating the Conformal Prediction Threshold from Noisy Labels**|Coby Penso et.al.|[2501.12749v1](http://arxiv.org/abs/2501.12749v1)|[link](https://github.com/cobypenso/noise-aware-conformal-prediction)|
|**2025-01-22**|**Applications and Challenges of AI and Microscopy in Life Science Research: A Review**|Himanshu Buckchash et.al.|[2501.13135v1](http://arxiv.org/abs/2501.13135v1)|null|
|**2025-01-22**|**FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis**|Haoxuan Che et.al.|[2501.13967v1](http://arxiv.org/abs/2501.13967v1)|null|
|**2025-01-21**|**Academic Case Reports Lack Diversity: Assessing the Presence and Diversity of Sociodemographic and Behavioral Factors related to Post COVID-19 Condition**|Juan Andres Medina Florez et.al.|[2501.12538v2](http://arxiv.org/abs/2501.12538v2)|null|
|**2025-01-21**|**Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature Extractor**|Jiaqi Guo et.al.|[2501.12524v1](http://arxiv.org/abs/2501.12524v1)|[link](https://github.com/guojiaqi-1020/medivlad)|
|**2025-01-21**|**FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression**|Phuoc Duong Huy Chu et.al.|[2501.12336v1](http://arxiv.org/abs/2501.12336v1)|null|
|**2025-01-21**|**CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification**|Cristiano Patr√≠cio et.al.|[2501.12266v1](http://arxiv.org/abs/2501.12266v1)|null|
|**2025-01-21**|**Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes**|Stefan Lenz et.al.|[2501.12106v1](http://arxiv.org/abs/2501.12106v1)|[link](https://github.com/stefan-m-lenz/urollmeval)|
|**2025-01-21**|**Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from CT and PET**|Fatih Aksu et.al.|[2501.12425v1](http://arxiv.org/abs/2501.12425v1)|null|
|**2025-01-21**|**Adaptive Class Learning to Screen Diabetic Disorders in Fundus Images of Eye**|Shramana Dey et.al.|[2501.12048v1](http://arxiv.org/abs/2501.12048v1)|null|
|**2025-01-21**|**Tackling Small Sample Survival Analysis via Transfer Learning: A Study of Colorectal Cancer Prognosis**|Yonghao Zhao et.al.|[2501.12421v1](http://arxiv.org/abs/2501.12421v1)|[link](https://github.com/yonghaozhao722/tsf)|
|**2025-01-21**|**Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)**|Jadon Geathers et.al.|[2501.13957v1](http://arxiv.org/abs/2501.13957v1)|null|
|**2025-01-21**|**Data-driven Detection and Evaluation of Damages in Concrete Structures: Using Deep Learning and Computer Vision**|Saeid Ataei et.al.|[2501.11836v1](http://arxiv.org/abs/2501.11836v1)|null|
|**2025-01-20**|**GL-ICNN: An End-To-End Interpretable Convolutional Neural Network for the Diagnosis and Prediction of Alzheimer's Disease**|Wenjie Kang et.al.|[2501.11715v1](http://arxiv.org/abs/2501.11715v1)|null|
|**2025-01-20**|**Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)**|Brian E. Perron et.al.|[2501.11705v1](http://arxiv.org/abs/2501.11705v1)|null|
|**2025-01-20**|**Spatially-Delineated Domain-Adapted AI Classification: An Application for Oncology Data**|Majid Farhadloo et.al.|[2501.11695v1](http://arxiv.org/abs/2501.11695v1)|null|
|**2025-01-20**|**Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects on Task Performance and Fairness**|Ambreesh Parthasarathy et.al.|[2501.13120v1](http://arxiv.org/abs/2501.13120v1)|null|
|**2025-01-20**|**Biomedical Knowledge Graph: A Survey of Domains, Tasks, and Real-World Applications**|Yuxing Lu et.al.|[2501.11632v2](http://arxiv.org/abs/2501.11632v2)|null|
|**2025-01-20**|**Training-free Ultra Small Model for Universal Sparse Reconstruction in Compressed Sensing**|Chaoqing Tang et.al.|[2501.11592v2](http://arxiv.org/abs/2501.11592v2)|[link](https://github.com/billttzqgbt/cscoefficientslearning)|
|**2025-01-20**|**Enhancing Coronary Artery Calcium Scoring via Multi-Organ Segmentation on Non-Contrast Cardiac Computed Tomography**|Jakub Nalepa et.al.|[2501.11428v1](http://arxiv.org/abs/2501.11428v1)|null|
|**2025-01-20**|**RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?**|Haotian Xu et.al.|[2501.11284v1](http://arxiv.org/abs/2501.11284v1)|null|
|**2025-01-20**|**Spatiotemporal Air Quality Mapping in Urban Areas Using Sparse Sensor Data, Satellite Imagery, Meteorological Factors, and Spatial Features**|Osama Ahmad et.al.|[2501.11270v1](http://arxiv.org/abs/2501.11270v1)|null|
|**2025-01-20**|**A Layered Multi-Expert Framework for Long-Context Mental Health Assessments**|Jinwen Tang et.al.|[2501.13951v1](http://arxiv.org/abs/2501.13951v1)|null|
|**2025-01-19**|**Clinical trial cohort selection using Large Language Models on n2c2 Challenges**|Chi-en Amy Tai et.al.|[2501.11114v1](http://arxiv.org/abs/2501.11114v1)|null|
|**2025-01-19**|**Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model**|Mohaiminul Islam Bhuiyan et.al.|[2501.11094v1](http://arxiv.org/abs/2501.11094v1)|null|
|**2025-01-18**|**No More Sliding Window: Efficient 3D Medical Image Segmentation with Differentiable Top-k Patch Sampling**|Young Seok Jeon et.al.|[2501.10814v1](http://arxiv.org/abs/2501.10814v1)|null|
|**2025-01-18**|**Efficient Auto-Labeling of Large-Scale Poultry Datasets (ALPD) Using Semi-Supervised Models, Active Learning, and Prompt-then-Detect Approach**|Ramesh Bahadur Bist et.al.|[2501.10809v1](http://arxiv.org/abs/2501.10809v1)|null|
|**2025-01-18**|**MedFILIP: Medical Fine-grained Language-Image Pre-training**|Xinjie Liang et.al.|[2501.10775v1](http://arxiv.org/abs/2501.10775v1)|[link](https://github.com/perceptioncomputinglab/medfilip)|
|**2025-01-18**|**Enhancing Diagnostic in 3D COVID-19 Pneumonia CT-scans through Explainable Uncertainty Bayesian Quantification**|Juan Manuel Liscano Fierro et.al.|[2501.10770v1](http://arxiv.org/abs/2501.10770v1)|null|
|**2025-01-18**|**In the Picture: Medical Imaging Datasets, Artifacts, and their Living Review**|Amelia Jim√©nez-S√°nchez et.al.|[2501.10727v1](http://arxiv.org/abs/2501.10727v1)|null|
|**2025-01-17**|**An Ontology for Social Determinants of Education (SDoEd) based on Human-AI Collaborative Approach**|Navya Martin Kollapally et.al.|[2501.10300v1](http://arxiv.org/abs/2501.10300v1)|null|
|**2025-01-17**|**SEANN: A Domain-Informed Neural Network for Epidemiological Insights**|Jean-Baptiste Guimbaud et.al.|[2501.10273v1](http://arxiv.org/abs/2501.10273v1)|null|
|**2025-01-17**|**Challenges and recommendations for Electronic Health Records data extraction and preparation for dynamic prediction modelling in hospitalized patients -- a practical guide**|Elena Albu et.al.|[2501.10240v1](http://arxiv.org/abs/2501.10240v1)|null|
|**2025-01-17**|**Generative Artificial Intelligence: Implications for Biomedical and Health Professions Education**|William Hersh et.al.|[2501.10186v1](http://arxiv.org/abs/2501.10186v1)|null|
|**2025-01-17**|**CSSDM Ontology to Enable Continuity of Care Data Interoperability**|Subhashis Das et.al.|[2501.10160v1](http://arxiv.org/abs/2501.10160v1)|null|
|**2025-01-17**|**landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D Images**|Jef Jonkers et.al.|[2501.10098v1](http://arxiv.org/abs/2501.10098v1)|[link](https://github.com/predict-idlab/landmarker)|
|**2025-01-17**|**Deep Learning for Early Alzheimer Disease Detection with MRI Scans**|Mohammad Rafsan et.al.|[2501.09999v1](http://arxiv.org/abs/2501.09999v1)|[link](https://github.com/rafusan/dl-alzheimer)|
|**2025-01-17**|**Aneumo: A Large-Scale Comprehensive Synthetic Dataset of Aneurysm Hemodynamics**|Xigui Li et.al.|[2501.09980v1](http://arxiv.org/abs/2501.09980v1)|[link](https://github.com/xigui-li/aneumo)|
|**2025-01-17**|**Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude**|Yile Yan et.al.|[2501.10484v1](http://arxiv.org/abs/2501.10484v1)|null|
|**2025-01-16**|**Bridging Language Barriers in Healthcare: A Study on Arabic LLMs**|Nada Saadi et.al.|[2501.09825v1](http://arxiv.org/abs/2501.09825v1)|null|
|**2025-01-16**|**KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity Recognition and Normalization for Dysmorphology Physical Examination Reports**|Hajung Kim et.al.|[2501.09744v1](http://arxiv.org/abs/2501.09744v1)|null|
|**2025-01-16**|**Electronic Health Records: Towards Digital Twins in Healthcare**|Muhammet Alkan et.al.|[2501.09640v1](http://arxiv.org/abs/2501.09640v1)|null|
|**2025-01-16**|**Artificial Intelligence-Driven Clinical Decision Support Systems**|Muhammet Alkan et.al.|[2501.09628v1](http://arxiv.org/abs/2501.09628v1)|null|
|**2025-01-16**|**IFRA: a machine learning-based Instrumented Fall Risk Assessment Scale derived from Instrumented Timed Up and Go test in stroke patients**|Simone Macci√≤ et.al.|[2501.09595v1](http://arxiv.org/abs/2501.09595v1)|null|
|**2025-01-16**|**Understanding Mental Health Content on Social Media and Its Effect Towards Suicidal Ideation**|Mohaiminul Islam Bhuiyan et.al.|[2501.09309v1](http://arxiv.org/abs/2501.09309v1)|null|
|**2025-01-16**|**Interpretable Droplet Digital PCR Assay for Trustworthy Molecular Diagnostics**|Yuanyuan Wei et.al.|[2501.09218v1](http://arxiv.org/abs/2501.09218v1)|null|
|**2025-01-15**|**AutoLoop: Fast Visual SLAM Fine-tuning through Agentic Curriculum Learning**|Assaf Lahiany et.al.|[2501.09160v1](http://arxiv.org/abs/2501.09160v1)|null|
|**2025-01-15**|**Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval**|Demetrio Deanda et.al.|[2501.09134v1](http://arxiv.org/abs/2501.09134v1)|null|
|**2025-01-15**|**Generative Medical Image Anonymization Based on Latent Code Projection and Optimization**|Huiyu Li et.al.|[2501.09114v1](http://arxiv.org/abs/2501.09114v1)|[link](https://github.com/huiyu-li/gmia)|
|**2025-01-15**|**Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models**|Emma Croxford et.al.|[2501.08977v2](http://arxiv.org/abs/2501.08977v2)|null|
|**2025-01-15**|**An analysis of data variation and bias in image-based dermatological datasets for machine learning classification**|Francisco Mauro et.al.|[2501.08962v1](http://arxiv.org/abs/2501.08962v1)|null|
|**2025-01-15**|**Improving the Efficiency of Self-Supervised Adversarial Training through Latent Clustering-Based Selection**|Somrita Ghosh et.al.|[2501.10466v1](http://arxiv.org/abs/2501.10466v1)|null|
|**2025-01-15**|**Digital Phenotyping for Adolescent Mental Health: A Feasibility Study Employing Machine Learning to Predict Mental Health Risk From Active and Passive Smartphone Data**|Balasundaram Kadirvelu et.al.|[2501.08851v1](http://arxiv.org/abs/2501.08851v1)|null|
|**2025-01-15**|**Spatio-Temporal Foundation Models: Vision, Challenges, and Opportunities**|Adam Goodge et.al.|[2501.09045v1](http://arxiv.org/abs/2501.09045v1)|null|
|**2025-01-14**|**ADAM-1: AI and Bioinformatics for Alzheimer's Detection and Microbiome-Clinical Data Integrations**|Ziyuan Huang et.al.|[2501.08324v1](http://arxiv.org/abs/2501.08324v1)|null|
|**2025-01-14**|**A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images using Choquet Integral and Differential Evolution Optimization**|Amir Reza Takhsha et.al.|[2501.08241v1](http://arxiv.org/abs/2501.08241v1)|null|
|**2025-01-14**|**ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems**|Mohita Chowdhury et.al.|[2501.08208v1](http://arxiv.org/abs/2501.08208v1)|null|
|**2025-01-14**|**Potential and Perils of Large Language Models as Judges of Unstructured Textual Data**|Rewina Bedemariam et.al.|[2501.08167v2](http://arxiv.org/abs/2501.08167v2)|null|
|**2025-01-14**|**FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware Classification**|Nurit Cohen-Inger et.al.|[2501.08155v1](http://arxiv.org/abs/2501.08155v1)|[link](https://github.com/nuritci/fairttts)|
|**2025-01-14**|**Guiding the classification of hepatocellular carcinoma on 3D CT-scans using deep and handcrafted radiological features**|E. Sarfati et.al.|[2501.08097v1](http://arxiv.org/abs/2501.08097v1)|null|
|**2025-01-14**|**Exploring visual language models as a powerful tool in the diagnosis of Ewing Sarcoma**|Alvaro Pastor-Naranjo et.al.|[2501.08042v1](http://arxiv.org/abs/2501.08042v1)|null|
|**2025-01-14**|**Comprehensive Metapath-based Heterogeneous Graph Transformer for Gene-Disease Association Prediction**|Wentao Cui et.al.|[2501.07970v1](http://arxiv.org/abs/2501.07970v1)|null|
|**2025-01-14**|**Advice for Diabetes Self-Management by ChatGPT Models: Challenges and Recommendations**|Waqar Hussain et.al.|[2501.07931v1](http://arxiv.org/abs/2501.07931v1)|null|
|**2025-01-14**|**Evaluating Computational Accuracy of Large Language Models in Numerical Reasoning Tasks for Healthcare Applications**|Arjun R. Malghan et.al.|[2501.13936v1](http://arxiv.org/abs/2501.13936v1)|null|
|**2025-01-13**|**Large Language Models for Interpretable Mental Health Diagnosis**|Brian Hyeongseok Kim et.al.|[2501.07653v1](http://arxiv.org/abs/2501.07653v1)|null|
|**2025-01-13**|**RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment**|Difei Gu et.al.|[2501.07525v1](http://arxiv.org/abs/2501.07525v1)|[link](https://github.com/difeigu/radalign)|
|**2025-01-13**|**A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities**|Yihao Liu et.al.|[2501.07468v1](http://arxiv.org/abs/2501.07468v1)|null|
|**2025-01-13**|**Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for Volume-to-Volume Medical Image Translation**|Xiyue Zhu et.al.|[2501.07430v1](http://arxiv.org/abs/2501.07430v1)|null|
|**2025-01-13**|**Synthetic Data and Health Privacy**|Gw√©nol√© Abgrall et.al.|[2501.09031v1](http://arxiv.org/abs/2501.09031v1)|null|
|**2025-01-13**|**Natural Language-Assisted Multi-modal Medication Recommendation**|Jie Tan et.al.|[2501.07166v1](http://arxiv.org/abs/2501.07166v1)|[link](https://github.com/jtan1102/nla-mmr_cikm_2024)|
|**2025-01-13**|**CureGraph: Contrastive Multi-Modal Graph Representation Learning for Urban Living Circle Health Profiling and Prediction**|Jinlin Li et.al.|[2501.07157v1](http://arxiv.org/abs/2501.07157v1)|[link](https://github.com/jinlin2021/curegraph)|
|**2025-01-13**|**UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN Powered Vision-LSTM**|Xuhui Guo et.al.|[2501.07017v2](http://arxiv.org/abs/2501.07017v2)|null|
|**2025-01-13**|**Combining LLM decision and RL action selection to improve RL policy for adaptive interventions**|Karine Karine et.al.|[2501.06980v1](http://arxiv.org/abs/2501.06980v1)|null|
|**2025-01-12**|**Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient Perspectives**|Xinyao Ma et.al.|[2501.06964v1](http://arxiv.org/abs/2501.06964v1)|null|
|**2025-01-12**|**MedGrad E-CLIP: Enhancing Trust and Transparency in AI-Driven Skin Lesion Diagnosis**|Sadia Kamal et.al.|[2501.06887v1](http://arxiv.org/abs/2501.06887v1)|null|
|**2025-01-12**|**A Foundational Generative Model for Breast Ultrasound Image Analysis**|Haojun Yu et.al.|[2501.06869v1](http://arxiv.org/abs/2501.06869v1)|null|
|**2025-01-12**|**A Comprehensive Evaluation of Large Language Models on Mental Illnesses in Arabic Context**|Noureldin Zahran et.al.|[2501.06859v1](http://arxiv.org/abs/2501.06859v1)|null|
|**2025-01-12**|**MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome Prediction**|Yiqing Zhang et.al.|[2501.06823v1](http://arxiv.org/abs/2501.06823v1)|null|

#### Abstracts
##### **Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?**
2501.14719v1 by Ipek Baris Schlicht, Zhixue Zhao, Burcu Sayin, Lucie Flek, Paolo Rosso

Equitable access to reliable health information is vital for public health,
but the quality of online health resources varies by language, raising concerns
about inconsistencies in Large Language Models (LLMs) for healthcare. In this
study, we examine the consistency of responses provided by LLMs to
health-related questions across English, German, Turkish, and Chinese. We
largely expand the HealthFC dataset by categorizing health-related questions by
disease type and broadening its multilingual scope with Turkish and Chinese
translations. We reveal significant inconsistencies in responses that could
spread healthcare misinformation. Our main contributions are 1) a multilingual
health-related inquiry dataset with meta-information on disease categories, and
2) a novel prompt-based evaluation workflow that enables sub-dimensional
comparisons between two languages through parsing. Our findings highlight key
challenges in deploying LLM-based tools in multilingual contexts and emphasize
the need for improved cross-lingual alignment to ensure accurate and equitable
healthcare information.

ÊëòË¶ÅÔºöÂèØÈù†ÁöÑÂÅ•Â∫∑Ë≥áË®äÁöÑÂÖ¨Âπ≥ÂèñÂæóÂ∞çÂÖ¨ÂÖ±Ë°õÁîüËá≥ÈóúÈáçË¶ÅÔºå
‰ΩÜÁ∂≤Ë∑ØÂÅ•Â∫∑Ë≥áÊ∫êÁöÑÂìÅË≥™Âõ†Ë™ûË®ÄËÄåÁï∞ÔºåÈÄôÂºïÁôº‰∫ÜÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÜ´ÁôÇ‰øùÂÅ•ÊñπÈù¢ÁöÑ‰∏ç‰∏ÄËá¥ÊÄßÁöÑÊìîÊÜÇ„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Â∞çËã±Ë™û„ÄÅÂæ∑Ë™û„ÄÅÂúüËÄ≥ÂÖ∂Ë™ûÂíå‰∏≠ÊñáÁöÑÂÅ•Â∫∑Áõ∏ÈóúÂïèÈ°åÊâÄÊèê‰æõÂõûÊáâÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàëÂÄëÈÄèÈÅé‰æùÁñæÁóÖÈ°ûÂûãÂàÜÈ°ûÂÅ•Â∫∑Áõ∏ÈóúÂïèÈ°åÔºå‰∏¶ÈÄèÈÅéÂúüËÄ≥ÂÖ∂Ë™ûÂíå‰∏≠ÊñáÁøªË≠ØÊì¥Â±ïÂÖ∂Â§öË™ûË®ÄÁØÑÂúçÔºåÂ§ßÂπÖÊì¥Â±ï‰∫Ü HealthFC Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÊè≠Èú≤‰∫ÜÂõûÊáâ‰∏≠Â≠òÂú®È°ØËëóÁöÑ‰∏ç‰∏ÄËá¥ÊÄßÔºåÈÄôÂèØËÉΩÊúÉÊï£Â∏ÉÈÜ´ÁôÇ‰øùÂÅ•ÈåØË™§Ë≥áË®ä„ÄÇÊàëÂÄëÁöÑË≤¢Áçª‰∏ªË¶ÅÊúâ 1) ‰∏ÄÂÄãÂåÖÂê´ÁñæÁóÖÈ°ûÂà•ÂÖÉË≥áË®äÁöÑÂ§öË™ûË®ÄÂÅ•Â∫∑Áõ∏ÈóúÊü•Ë©¢Ë≥áÊñôÈõÜÔºå‰ª•Âèä 2) ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊèêÁ§∫ÂºèË©ï‰º∞Â∑•‰ΩúÊµÅÁ®ãÔºåÂÆÉËÉΩÈÄèÈÅéËß£ÊûêÂú®ÂÖ©Á®ÆË™ûË®Ä‰πãÈñìÈÄ≤Ë°åÊ¨°Á∂≠Â∫¶ÊØîËºÉ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÁ™ÅÈ°Ø‰∫ÜÂú®Â§öË™ûË®ÄÁí∞Â¢É‰∏≠ÈÉ®ÁΩ≤Âü∫Êñº LLM ÁöÑÂ∑•ÂÖ∑ÁöÑ‰∏ªË¶ÅÊåëÊà∞Ôºå‰∏¶Âº∑Ë™øÈúÄË¶ÅÊîπÂñÑË∑®Ë™ûË®ÄÂ∞çÈΩä‰ª•Á¢∫‰øùÊ∫ñÁ¢∫‰∏îÂÖ¨Âπ≥ÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ë≥áË®ä„ÄÇ

##### **Rethinking Table Instruction Tuning**
2501.14693v1 by Naihao Deng, Rada Mihalcea

Recent advances in table understanding have focused on instruction-tuning
large language models (LLMs) for table-related tasks. However, existing
research has overlooked the impact of hyperparameter choices and lacks a
comprehensive evaluation of the out-of-domain table understanding ability and
the general capabilities of these table LLMs. In this paper, we evaluate these
abilities in existing table LLMs, and reveal significant declines in both
out-of-domain table understanding and general capabilities compared to their
base models. Through systematic analysis, we show that hyperparameters, such as
learning rate, can significantly influence both table-specific and general
capabilities. Contrary to the existing table instruction-tuning works, we
demonstrate that smaller learning rates and fewer training instances can
enhance table understanding while preserving general capabilities. Based on our
findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B
Instruct, which achieves performance on par with, or surpassing GPT-3.5 and
GPT-4 on table tasks, while maintaining strong out-of-domain generalization and
general capabilities. Our findings highlight the potential for reduced data
annotation costs and more efficient model development through careful
hyperparameter selection.

ÊëòË¶ÅÔºöÊúÄËøëË°®ÁêÜËß£ÁöÑÈÄ≤Â±ïÈõÜ‰∏≠Âú®Êåá‰ª§Ë™øÊ†°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ª•Âü∑Ë°åËàáË°®Ê†ºÁõ∏ÈóúÁöÑ‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂ÂøΩÁï•‰∫ÜË∂ÖÂèÉÊï∏ÈÅ∏ÊìáÁöÑÂΩ±ÈüøÔºå‰∏¶‰∏îÁº∫‰πèÂ∞çÈ†òÂüüÂ§ñË°®Ê†ºÁêÜËß£ËÉΩÂäõÂíåÈÄô‰∫õË°®Ê†º LLM ÁöÑ‰∏ÄËà¨ËÉΩÂäõÁöÑÂÖ®Èù¢Ë©ï‰º∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË©ï‰º∞‰∫ÜÁèæÊúâË°®Ê†º LLM ‰∏≠ÁöÑÈÄô‰∫õËÉΩÂäõÔºå‰∏¶Êè≠Á§∫‰∫ÜËàáÂÖ∂Âü∫Á§éÊ®°ÂûãÁõ∏ÊØîÔºåÈ†òÂüüÂ§ñË°®Ê†ºÁêÜËß£Âíå‰∏ÄËà¨ËÉΩÂäõÈÉΩÊúâÈ°ØËëó‰∏ãÈôç„ÄÇÈÄèÈÅéÁ≥ªÁµ±ÂàÜÊûêÔºåÊàëÂÄëË°®ÊòéË∂ÖÂèÉÊï∏Ôºà‰æãÂ¶ÇÂ≠∏ÁøíÁéáÔºâÂèØ‰ª•È°ØËëóÂΩ±ÈüøÁâπÂÆöË°®Ê†ºÂíå‰∏ÄËà¨ËÉΩÂäõ„ÄÇËàáÁèæÊúâË°®Ê†ºÊåá‰ª§Ë™øÊ†°Â∑•‰ΩúÁõ∏ÂèçÔºåÊàëÂÄëË≠âÊòéËºÉÂ∞èÁöÑÂ≠∏ÁøíÁéáÂíåËºÉÂ∞ëÁöÑË®ìÁ∑¥ÂØ¶‰æãÂèØ‰ª•Âú®‰øùÁïô‰∏ÄËà¨ËÉΩÂäõÁöÑÂêåÊôÇÂ¢ûÂº∑Ë°®Ê†ºÁêÜËß£„ÄÇÊ†πÊìöÊàëÂÄëÁöÑÁôºÁèæÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü TAMAÔºåÈÄôÊòØ‰∏ÄÂÄãÂæû LLaMA 3.1 8B Instruct Ë™øÊ†°ÁöÑË°®Ê†º LLMÔºåÂÆÉÂú®Ë°®Ê†º‰ªªÂãô‰∏äÂØ¶Áèæ‰∫ÜËàá GPT-3.5 Âíå GPT-4 Áõ∏Áï∂ÊàñË∂ÖË∂äÁöÑÊïàËÉΩÔºåÂêåÊôÇ‰øùÊåÅÂº∑Â§ßÁöÑÈ†òÂüüÂ§ñÊ¶ÇÂåñÂíå‰∏ÄËà¨ËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁôºÁèæÂº∑Ë™ø‰∫ÜÈÄèÈÅé‰ªîÁ¥∞ÈÅ∏ÊìáË∂ÖÂèÉÊï∏ÔºåÈôç‰ΩéË≥áÊñôÊ®ôË®ªÊàêÊú¨ÂíåÊõ¥ÊúâÊïàÁéáÁöÑÊ®°ÂûãÈñãÁôºÁöÑÂèØËÉΩÊÄß„ÄÇ

##### **Approach to Designing CV Systems for Medical Applications: Data, Architecture and AI**
2501.14689v1 by Dmitry Ryabtsev, Boris Vasilyev, Sergey Shershakov

This paper introduces an innovative software system for fundus image analysis
that deliberately diverges from the conventional screening approach, opting not
to predict specific diagnoses. Instead, our methodology mimics the diagnostic
process by thoroughly analyzing both normal and pathological features of fundus
structures, leaving the ultimate decision-making authority in the hands of
healthcare professionals. Our initiative addresses the need for objective
clinical analysis and seeks to automate and enhance the clinical workflow of
fundus image examination. The system, from its overarching architecture to the
modular analysis design powered by artificial intelligence (AI) models, aligns
seamlessly with ophthalmological practices. Our unique approach utilizes a
combination of state-of-the-art deep learning methods and traditional computer
vision algorithms to provide a comprehensive and nuanced analysis of fundus
structures. We present a distinctive methodology for designing medical
applications, using our system as an illustrative example. Comprehensive
verification and validation results demonstrate the efficacy of our approach in
revolutionizing fundus image analysis, with potential applications across
various medical domains.

ÊëòË¶ÅÔºöÊú¨Ë´ñÊñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑËªüÈ´îÁ≥ªÁµ±ÔºåÁî®ÊñºÁúºÂ∫ïÂΩ±ÂÉèÂàÜÊûêÔºåÂÆÉÂàªÊÑèÂÅèÈõ¢ÂÇ≥Áµ±ÁöÑÁØ©Ê™¢ÊñπÊ≥ïÔºåÈÅ∏Êìá‰∏çÈ†êÊ∏¨ÂÖ∑È´îÁöÑË®∫Êñ∑„ÄÇÁõ∏ÂèçÂú∞ÔºåÊàëÂÄëÁöÑÂàÜÊûêÊñπÊ≥ïÊ®°Êì¨Ë®∫Êñ∑ÈÅéÁ®ãÔºåÂæπÂ∫ïÂàÜÊûêÁúºÂ∫ïÁµêÊßãÁöÑÊ≠£Â∏∏ÂíåÁóÖÁêÜÁâπÂæµÔºåÂ∞áÊúÄÁµÇÁöÑÊ±∫Á≠ñÊ¨ä‰∫§Âà∞ÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°Êâã‰∏≠„ÄÇÊàëÂÄëÁöÑË®àÁï´Êó®Âú®ÊªøË∂≥ÂÆ¢ËßÄËá®Â∫äÂàÜÊûêÁöÑÈúÄÊ±ÇÔºå‰∏¶Â∞ãÊ±ÇËá™ÂãïÂåñÂíåÂº∑ÂåñÁúºÂ∫ïÂΩ±ÂÉèÊ™¢Êü•ÁöÑËá®Â∫äÂ∑•‰ΩúÊµÅÁ®ã„ÄÇË©≤Á≥ªÁµ±ÂæûÂÖ∂Êï¥È´îÊû∂ÊßãÂà∞Áî±‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÈ©ÖÂãïÁöÑÊ®°ÁµÑÂåñÂàÜÊûêË®≠Ë®àÔºåÈÉΩËàáÁúºÁßëÂØ¶ÂãôÁÑ°Á∏´Â∞çÈΩä„ÄÇÊàëÂÄëÁç®ÁâπÁöÑÊñπÊ≥ïÁµêÂêà‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÂíåÂÇ≥Áµ±ÁöÑÈõªËÖ¶Ë¶ñË¶∫ÊºîÁÆóÊ≥ïÔºåÊèê‰æõÁúºÂ∫ïÁµêÊßãÁöÑÂÖ®Èù¢‰∏îÁ¥∞Á∑ªÁöÑÂàÜÊûê„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁç®ÁâπÁöÑË®≠Ë®àÈÜ´ÁôÇÊáâÁî®ÊñπÊ≥ïÔºå‰∏¶‰ª•ÊàëÂÄëÁöÑÁ≥ªÁµ±‰ΩúÁÇ∫Ë™™ÊòéÁØÑ‰æã„ÄÇÂÖ®Èù¢ÁöÑÈ©óË≠âÂíåÈ©óË≠âÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Èù©Êñ∞ÁúºÂ∫ïÂΩ±ÂÉèÂàÜÊûêÊñπÈù¢ÁöÑÊïàÂäõÔºå‰∏¶ÂÖ∑ÊúâÂú®ÂêÑÁ®ÆÈÜ´ÁôÇÈ†òÂüüÁöÑÊΩõÂú®ÊáâÁî®„ÄÇ

##### **Rethinking Foundation Models for Medical Image Classification through a Benchmark Study on MedMNIST**
2501.14685v1 by Fuping Wu, Bartlomiej W. Papiez

Foundation models are widely employed in medical image analysis, due to their
high adaptability and generalizability for downstream tasks. With the
increasing number of foundation models being released, model selection has
become an important issue. In this work, we study the capabilities of
foundation models in medical image classification tasks by conducting a
benchmark study on the MedMNIST dataset. Specifically, we adopt various
foundation models ranging from convolutional to Transformer-based models and
implement both end-to-end training and linear probing for all classification
tasks. The results demonstrate the significant potential of these pre-trained
models when transferred for medical image classification. We further conduct
experiments with different image sizes and various sizes of training data. By
analyzing all the results, we provide preliminary, yet useful insights and
conclusions on this topic.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂª£Ê≥õÁî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÂ∞ç‰∏ãÊ∏∏‰ªªÂãôÂÖ∑ÊúâÈ´òÂ∫¶ÁöÑÈÅ©ÊáâÊÄßÂíåÊ¶ÇÊã¨ÊÄß„ÄÇÈö®ËëóÁôºÂ∏ÉÁöÑÂü∫Á§éÊ®°ÂûãÊï∏ÈáèË∂ä‰æÜË∂äÂ§öÔºåÊ®°ÂûãÈÅ∏ÊìáÂ∑≤ÊàêÁÇ∫‰∏ÄÂÄãÈáçË¶ÅÂïèÈ°å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄöÈÅéÂ∞ç MedMNIST Ë≥áÊñôÈõÜÈÄ≤Ë°åÂü∫Ê∫ñÁ†îÁ©∂‰æÜÁ†îÁ©∂Âü∫Á§éÊ®°ÂûãÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãô‰∏≠ÁöÑËÉΩÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé°Áî®‰∫ÜÂæûÂç∑Á©çÂà∞Âü∫Êñº Transformer ÁöÑÊ®°ÂûãÁ≠âÂêÑÁ®ÆÂü∫Á§éÊ®°ÂûãÔºå‰∏¶Â∞çÊâÄÊúâÂàÜÈ°û‰ªªÂãôÂØ¶ÊñΩÁ´ØÂà∞Á´ØË®ìÁ∑¥ÂíåÁ∑öÊÄßÊé¢Ê∏¨„ÄÇÁµêÊûúË≠âÊòé‰∫ÜÈÄô‰∫õÈ†êË®ìÁ∑¥Ê®°ÂûãÂú®ËΩâÁßªÂà∞ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûÊôÇÂÖ∑ÊúâÈ°ØËëóÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄ≤Ë°å‰∫Ü‰∏çÂêåÂΩ±ÂÉèÂ§ßÂ∞èÂíåÂêÑÁ®ÆË®ìÁ∑¥Ë≥áÊñôÂ§ßÂ∞èÁöÑÂØ¶È©ó„ÄÇÈÄöÈÅéÂàÜÊûêÊâÄÊúâÁµêÊûúÔºåÊàëÂÄëÂ∞çÊ≠§‰∏ªÈ°åÊèê‰æõ‰∫ÜÂàùÊ≠•‰ΩÜÊúâÁî®ÁöÑË¶ãËß£ÂíåÁµêË´ñ„ÄÇ

##### **MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications**
2501.14654v1 by Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, Andrew Y. Ng, Jonathan H. Chen

Recent large language models (LLMs) have demonstrated significant
advancements, particularly in their ability to serve as agents thereby
surpassing their traditional role as chatbots. These agents can leverage their
planning and tool utilization capabilities to address tasks specified at a high
level. However, a standardized dataset to benchmark the agent capabilities of
LLMs in medical applications is currently lacking, making the evaluation of
LLMs on complex tasks in interactive healthcare environments challenging. To
address this gap, we introduce MedAgentBench, a broad evaluation suite designed
to assess the agent capabilities of large language models within medical
records contexts. MedAgentBench encompasses 100 patient-specific
clinically-derived tasks from 10 categories written by human physicians,
realistic profiles of 100 patients with over 700,000 data elements, a
FHIR-compliant interactive environment, and an accompanying codebase. The
environment uses the standard APIs and communication infrastructure used in
modern EMR systems, so it can be easily migrated into live EMR systems.
MedAgentBench presents an unsaturated agent-oriented benchmark that current
state-of-the-art LLMs exhibit some ability to succeed at. The best model
(GPT-4o) achieves a success rate of 72%. However, there is still substantial
space for improvement to give the community a next direction to optimize.
Furthermore, there is significant variation in performance across task
categories. MedAgentBench establishes this and is publicly available at
https://github.com/stanfordmlgroup/MedAgentBench , offering a valuable
framework for model developers to track progress and drive continuous
improvements in the agent capabilities of large language models within the
medical domain.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁ§∫Âá∫ÊòæËëóÁöÑËøõÊ≠•ÔºåÁâπÂà´ÊòØÂú®ÂÖ∂‰Ωú‰∏∫‰ª£ÁêÜÁöÑËÉΩÂäõÊñπÈù¢Ôºå‰ªéËÄåË∂ÖË∂ä‰∫ÜÂÖ∂‰Ωú‰∏∫ËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑ‰º†ÁªüËßíËâ≤„ÄÇËøô‰∫õ‰ª£ÁêÜÂèØ‰ª•Âà©Áî®ÂÖ∂ËßÑÂàíÂíåÂ∑•ÂÖ∑Âà©Áî®ËÉΩÂäõÊù•Ëß£ÂÜ≥Âú®È´òÂ±ÇÊåáÂÆöÁöÑ‰ªªÂä°„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁº∫‰πèÁî®‰∫éÂØπÂåªÁñóÂ∫îÁî®‰∏≠ LLM ÁöÑ‰ª£ÁêÜËÉΩÂäõËøõË°åÂü∫ÂáÜÊµãËØïÁöÑÊ†áÂáÜÂåñÊï∞ÊçÆÈõÜÔºåËøô‰ΩøÂæóÂú®‰∫§‰∫íÂºèÂåªÁñó‰øùÂÅ•ÁéØÂ¢É‰∏≠ÂØπ LLM Âú®Â§çÊùÇ‰ªªÂä°‰∏äÁöÑËØÑ‰º∞ÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÂ∑ÆË∑ùÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü MedAgentBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂπøÊ≥õÁöÑËØÑ‰º∞Â•ó‰ª∂ÔºåÊó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÁñóËÆ∞ÂΩïËÉåÊôØ‰∏ãÁöÑ‰ª£ÁêÜËÉΩÂäõ„ÄÇMedAgentBench ÂåÖÂê´ 100 ‰∏™Áî±‰∫∫Á±ªÂåªÁîüÁºñÂÜôÁöÑÊù•Ëá™ 10 ‰∏™Á±ªÂà´ÁöÑÁâπÂÆö‰∫éÊÇ£ËÄÖÁöÑ‰∏¥Â∫ä‰ªªÂä°„ÄÅ100 ‰∏™ÊÇ£ËÄÖÁöÑÁúüÂÆû‰∏™‰∫∫ËµÑÊñôÔºàÂåÖÂê´Ë∂ÖËøá 700,000 ‰∏™Êï∞ÊçÆÂÖÉÁ¥†Ôºâ„ÄÅ‰∏Ä‰∏™Á¨¶Âêà FHIR ÁöÑ‰∫§‰∫íÂºèÁéØÂ¢É‰ª•Âèä‰∏Ä‰∏™ÈÖçÂ•óÁöÑ‰ª£Á†ÅÂ∫ì„ÄÇËØ•ÁéØÂ¢É‰ΩøÁî®Áé∞‰ª£ EMR Á≥ªÁªü‰∏≠‰ΩøÁî®ÁöÑÊ†áÂáÜ API ÂíåÈÄö‰ø°Âü∫Á°ÄËÆæÊñΩÔºåÂõ†Ê≠§ÂèØ‰ª•ËΩªÊùæÂú∞ËøÅÁßªÂà∞ÂÆûÊó∂ EMR Á≥ªÁªü‰∏≠„ÄÇMedAgentBench ÂëàÁé∞‰∫Ü‰∏Ä‰∏™Êú™È•±ÂíåÁöÑ‰ª•‰ª£ÁêÜ‰∏∫ÂØºÂêëÁöÑÂü∫ÂáÜÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑ LLM Ë°®Áé∞Âá∫‰∏ÄÂÆöÁ®ãÂ∫¶ÁöÑÊàêÂäüËÉΩÂäõ„ÄÇÊúÄÂ•ΩÁöÑÊ®°Âûã (GPT-4o) ÁöÑÊàêÂäüÁéáËææÂà∞ 72%„ÄÇÁÑ∂ËÄåÔºå‰ªçÁÑ∂ÊúâÂæàÂ§ßÁöÑÊîπËøõÁ©∫Èó¥ÔºåÂèØ‰ª•‰∏∫Á§æÂå∫Êèê‰æõ‰ºòÂåñÊñπÂêë„ÄÇÊ≠§Â§ñÔºå‰∏çÂêå‰ªªÂä°Á±ªÂà´‰πãÈó¥ÁöÑÊÄßËÉΩÂ∑ÆÂºÇÂæàÂ§ß„ÄÇMedAgentBench Âª∫Á´ã‰∫ÜËøô‰∏ÄÁÇπÔºåÂπ∂Âú® https://github.com/stanfordmlgroup/MedAgentBench ÂÖ¨ÂºÄÊèê‰æõÔºå‰∏∫Ê®°ÂûãÂºÄÂèëËÄÖÊèê‰æõ‰∫Ü‰∏Ä‰∏™Êúâ‰ª∑ÂÄºÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éË∑üË∏™ËøõÂ∫¶Âπ∂Êé®Âä®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÁñóÈ¢ÜÂüüÁöÑ‰ª£ÁêÜËÉΩÂäõÁöÑÊåÅÁª≠ÊîπËøõ„ÄÇ</paragraph>

##### **Registration of Longitudinal Liver Examinations for Tumor Progress Assessment**
2501.14483v1 by Walid Yassine, Martin Charachon, C√©line Hudelot, Roberto Ardon

Assessing cancer progression in liver CT scans is a clinical challenge,
requiring a comparison of scans at different times for the same patient.
Practitioners must identify existing tumors, compare them with prior exams,
identify new tumors, and evaluate overall disease evolution. This process is
particularly complex in liver examinations due to misalignment between exams
caused by several factors. Indeed, longitudinal liver examinations can undergo
different non-pathological and pathological changes due to non-rigid
deformations, the appearance or disappearance of pathologies, and other
variations. In such cases, existing registration approaches, mainly based on
intrinsic features may distort tumor regions, biasing the tumor progress
evaluation step and the corresponding diagnosis. This work proposes a
registration method based only on geometrical and anatomical information from
liver segmentation, aimed at aligning longitudinal liver images for aided
diagnosis. The proposed method is trained and tested on longitudinal liver CT
scans, with 317 patients for training and 53 for testing. Our experimental
results support our claims by showing that our method is better than other
registration techniques by providing a smoother deformation while preserving
the tumor burden (total volume of tissues considered as tumor) within the
volume. Qualitative results emphasize the importance of smooth deformations in
preserving tumor appearance.

ÊëòË¶ÅÔºöË©ï‰º∞ËÇùËáüÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè‰∏≠ÁöÑÁôåÁóáÈÄ≤Á®ãÊòØ‰∏ÄÈ†ÖËá®Â∫ä‰∏äÁöÑÊåëÊà∞Ôºå
ÈúÄË¶ÅÊØîËºÉÂêå‰∏ÄÁóÖÊÇ£Âú®‰∏çÂêåÊôÇÈñìÈªûÁöÑÊéÉÊèèÁµêÊûú„ÄÇ
ÂæûÊ•≠‰∫∫Âì°ÂøÖÈ†àËæ®Ë≠òÁèæÊúâÁöÑËÖ´Áò§ÔºåÂ∞áÂÖ∂ËàáÂÖàÂâçÁöÑÊ™¢Êü•ÁµêÊûúÈÄ≤Ë°åÊØîËºÉÔºå
Ëæ®Ë≠òÊñ∞ÁöÑËÖ´Áò§Ôºå‰∏¶Ë©ï‰º∞Êï¥È´îÁñæÁóÖÁöÑÊºîËÆä„ÄÇÁî±ÊñºÁ®ÆÁ®ÆÂõ†Á¥†ÈÄ†ÊàêÊ™¢Êü•ÁµêÊûú‰πãÈñìÁöÑÈåØ‰ΩçÔºåÈÄôÂÄãÈÅéÁ®ãÂú®ËÇùËáüÊ™¢Êü•‰∏≠ÁâπÂà•Ë§áÈõú„ÄÇ‰∫ãÂØ¶‰∏äÔºåÁ∏±ÂêëÁöÑËÇùËáüÊ™¢Êü•ÂèØËÉΩÊúÉÂõ†ÁÇ∫ÈùûÂâõÊÄßËÆäÂΩ¢„ÄÅÁóÖÁêÜÁöÑÂá∫ÁèæÊàñÊ∂àÂ§±Ôºå‰ª•ÂèäÂÖ∂‰ªñËÆäÂåñËÄåÁî¢Áîü‰∏çÂêåÁöÑÈùûÁóÖÁêÜÊÄßÂíåÁóÖÁêÜÊÄßÁöÑËÆäÂåñ„ÄÇÂú®ÈÄôÁ®ÆÊÉÖÊ≥Å‰∏ãÔºåÁèæÊúâÁöÑÈÖçÊ∫ñÊñπÊ≥ïÔºà‰∏ªË¶ÅÂü∫ÊñºÂÖßÂú®ÁâπÂæµÔºâÂèØËÉΩÊúÉÊâ≠Êõ≤ËÖ´Áò§ÂçÄÂüüÔºåÈÄ†ÊàêËÖ´Áò§ÈÄ≤Á®ãË©ï‰º∞Ê≠•È©üÂíåÁõ∏ÊáâË®∫Êñ∑ÁöÑÂÅèÂ∑Æ„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂÉÖÂü∫ÊñºËÇùËáüÂàÜÂâ≤ÁöÑÂπæ‰ΩïÂíåËß£ÂâñË≥áË®äÁöÑÈÖçÊ∫ñÊñπÊ≥ïÔºåÊó®Âú®Â∞çÁ∏±ÂêëËÇùËáüÂΩ±ÂÉèÈÄ≤Ë°åÈÖçÊ∫ñÔºå‰ª•ÂçîÂä©Ë®∫Êñ∑„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Á∏±ÂêëËÇùËáüÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÂíåÊ∏¨Ë©¶ÔºåË®ìÁ∑¥Ë≥áÊñôÊúâ 317 ‰ΩçÁóÖÊÇ£ÔºåÊ∏¨Ë©¶Ë≥áÊñôÊúâ 53 ‰Ωç„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÊîØÊåÅÊàëÂÄëÁöÑË™™Ê≥ïÔºåË≠âÊòéÊàëÂÄëÁöÑÈÖçÊ∫ñÊñπÊ≥ïÊØîÂÖ∂‰ªñÈÖçÊ∫ñÊäÄË°ìÊõ¥Â•ΩÔºåÂõ†ÁÇ∫ÂÆÉÂú®‰øùÁïôËÖ´Áò§Ë≤†ÊìîÔºàË¢´Ë¶ñÁÇ∫ËÖ´Áò§ÁöÑÁµÑÁπîÁ∏ΩÈ´îÁ©çÔºâÁöÑÂêåÊôÇÔºåÊèê‰æõ‰∫ÜÊõ¥Âπ≥ÊªëÁöÑËÆäÂΩ¢„ÄÇÂÆöÊÄßÁµêÊûúÂº∑Ë™ø‰∫ÜÂπ≥ÊªëËÆäÂΩ¢Âú®‰øùÁïôËÖ´Áò§Â§ñËßÄÊñπÈù¢ÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware Pesticide Design**
2501.14469v1 by Taehan Kim, Wonduk Seo

Global climate change has reduced crop resilience and pesticide efficacy,
making reliance on synthetic pesticides inevitable, even though their
widespread use poses significant health and environmental risks. While these
pesticides remain a key tool in pest management, previous machine-learning
applications in pesticide and agriculture have focused on classification or
regression, leaving the fundamental challenge of generating new molecular
structures or designing novel candidates unaddressed. In this paper, we propose
Pesti-Gen, a novel generative model based on variational auto-encoders,
designed to create pesticide candidates with optimized properties for the first
time. Specifically, Pesti-Gen leverages a two-stage learning process: an
initial pre-training phase that captures a generalized chemical structure
representation, followed by a fine-tuning stage that incorporates
toxicity-specific information. The model simultaneously optimizes over multiple
toxicity metrics, such as (1) livestock toxicity and (2) aqua toxicity to
generate environmentally friendly pesticide candidates. Notably, Pesti-Gen
achieves approximately 68\% structural validity in generating new molecular
structures, demonstrating the model's effectiveness in producing optimized and
feasible pesticide candidates, thereby providing a new way for safer and more
sustainable pest management solutions.

ÊëòË¶ÅÔºöÂÖ®ÁêÉÊ∞£ÂÄôËÆäÈÅ∑Èôç‰Ωé‰∫Ü‰ΩúÁâ©ÁöÑÂæ©ÂéüÂäõËàáÊÆ∫Ëü≤ÂäëÁöÑÊïàÂäõÔºå
‰ΩøÂæó‰ª∞Ë≥¥ÂêàÊàêÊÆ∫Ëü≤ÂäëÊàêÁÇ∫ÁÑ°ÂèØÈÅøÂÖçÁöÑË∂®Âã¢ÔºåÂÑòÁÆ°ÂÆÉÂÄëÁöÑÂª£Ê≥õ‰ΩøÁî®ÊúÉÂ∏∂‰æÜÈáçÂ§ßÁöÑÂÅ•Â∫∑ÂíåÁí∞Â¢ÉÈ¢®Èö™„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊÆ∫Ëü≤Âäë‰ªçÁÑ∂ÊòØËü≤ÂÆ≥ÁÆ°ÁêÜ‰∏≠ÁöÑÈóúÈçµÂ∑•ÂÖ∑ÔºåÈÅéÂéªÂú®ÊÆ∫Ëü≤ÂäëÂíåËæ≤Ê•≠ÊñπÈù¢ÁöÑÊ©üÂô®Â≠∏ÁøíÊáâÁî®ÈÉΩËëóÈáçÊñºÂàÜÈ°ûÊàñËø¥Ê≠∏ÔºåËÄåÊú™Ëß£Ê±∫Áî¢ÁîüÊñ∞ÁöÑÂàÜÂ≠êÁµêÊßãÊàñË®≠Ë®àÊñ∞ÂÄôÈÅ∏Ëó•ÂäëÁöÑÂü∫Êú¨ÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ Pesti-GenÔºå‰∏ÄÁ®ÆÂü∫ÊñºËÆäÁï∞Ëá™ÂãïÁ∑®Á¢ºÂô®ÁöÑÂâµÊñ∞ÁîüÊàêÊ®°ÂûãÔºåÊó®Âú®È¶ñÊ¨°Âª∫Á´ãÂÖ∑ÊúâÊúÄ‰Ω≥ÂåñÁâπÊÄßÁöÑÊÆ∫Ëü≤ÂäëÂÄôÈÅ∏Ëó•Âäë„ÄÇÂÖ∑È´î‰æÜË™™ÔºåPesti-Gen Êé°Áî®ÂÖ©ÈöéÊÆµÂ≠∏ÁøíÊµÅÁ®ãÔºö‰∏ÄÂÄãÊì∑ÂèñÂª£Áæ©ÂåñÂ≠∏ÁµêÊßãË°®Á§∫ÁöÑÂàùÂßãÈ†êË®ìÁ∑¥ÈöéÊÆµÔºåÊé•ËëóÊòØ‰∏ÄÂÄãÁ¥çÂÖ•ÊØíÊÄßÁâπÂÆöË≥áË®äÁöÑÂæÆË™øÈöéÊÆµ„ÄÇÊ≠§Ê®°ÂûãÂêåÊôÇÈáùÂ∞çÂ§öÁ®ÆÊØíÊÄßÊåáÊ®ôÈÄ≤Ë°åÊúÄ‰Ω≥ÂåñÔºå‰æãÂ¶Ç (1) Áâ≤ÁïúÊØíÊÄßÂíå (2) Ê∞¥ÁîüÊØíÊÄßÔºå‰ª•Áî¢ÁîüÂ∞çÁí∞Â¢ÉÂèãÂñÑÁöÑÊÆ∫Ëü≤ÂäëÂÄôÈÅ∏Ëó•Âäë„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåPesti-Gen Âú®Áî¢ÁîüÊñ∞ÁöÑÂàÜÂ≠êÁµêÊßãÊñπÈù¢ÈÅîÂà∞‰∫ÜÁ¥Ñ 68% ÁöÑÁµêÊßãÊïàÂ∫¶ÔºåË≠âÊòé‰∫ÜÊ≠§Ê®°ÂûãÂú®Áî¢ÁîüÊúÄ‰Ω≥Âåñ‰∏îÂèØË°åÁöÑÊÆ∫Ëü≤ÂäëÂÄôÈÅ∏Ëó•ÂäëÊñπÈù¢ÁöÑÊïàËÉΩÔºåÈÄ≤ËÄåÁÇ∫Êõ¥ÂÆâÂÖ®‰∏îÊõ¥Ê∞∏Á∫åÁöÑËü≤ÂÆ≥ÁÆ°ÁêÜËß£Ê±∫ÊñπÊ°àÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ï„ÄÇ

##### **ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer**
2501.14379v1 by Yoni Schirris, Rosie Voorthuis, Mark Opdam, Marte Liefaard, Gabe S Sonke, Gwen Dackus, Vincent de Jong, Yuwei Wang, Annelot Van Rossum, Tessa G Steenbruggen, Lars C Steggink, Liesbeth G. E. de Vries, Marc van de Vijver, Roberto Salgado, Efstratios Gavves, Paul J van Diest, Sabine C Linn, Jonas Teuwen, Renee Menezes, Marleen Kok, Hugo Horlings

The level of tumour-infiltrating lymphocytes (TILs) is a prognostic factor
for patients with (triple-negative) breast cancer (BC). Computational TIL
assessment (CTA) has the potential to assist pathologists in this
labour-intensive task, but current CTA models rely heavily on many detailed
annotations. We propose and validate a fundamentally simpler deep learning
based CTA that can be trained in only ten minutes on hundredfold fewer
pathologist annotations. We collected whole slide images (WSIs) with TILs
scores and clinical data of 2,340 patients with BC from six cohorts including
three randomised clinical trials. Morphological features were extracted from
whole slide images (WSIs) using a pathology foundation model. Our
label-efficient Computational stromal TIL assessment model (ECTIL) directly
regresses the TILs score from these features. ECTIL trained on only a few
hundred samples (ECTIL-TCGA) showed concordance with the pathologist over five
heterogeneous external cohorts (r=0.54-0.74, AUROC=0.80-0.94). Training on all
slides of five cohorts (ECTIL-combined) improved results on a held-out test set
(r=0.69, AUROC=0.85). Multivariable Cox regression analyses indicated that
every 10% increase of ECTIL scores was associated with improved overall
survival independent of clinicopathological variables (HR 0.86, p<0.01),
similar to the pathologist score (HR 0.87, p<0.001). We demonstrate that ECTIL
is highly concordant with an expert pathologist and obtains a similar hazard
ratio. ECTIL has a fundamentally simpler design than existing methods and can
be trained on orders of magnitude fewer annotations. Such a CTA may be used to
pre-screen patients for, e.g., immunotherapy clinical trial inclusion, or as a
tool to assist clinicians in the diagnostic work-up of patients with BC. Our
model is available under an open source licence
(https://github.com/nki-ai/ectil).

ÊëòË¶ÅÔºöËÇøÁò§Êµ∏Ê∂¶Ê∑ãÂ∑¥ÁªÜËÉû (TIL) ÁöÑÊ∞¥Âπ≥ÊòØ (‰∏âÈò¥ÊÄß) ‰π≥ËÖ∫Áôå (BC) ÊÇ£ËÄÖÁöÑÈ¢ÑÂêéÂõ†Á¥†„ÄÇËÆ°ÁÆó TIL ËØÑ‰º∞ (CTA) ÊúâÂèØËÉΩÂçèÂä©ÁóÖÁêÜÂ≠¶ÂÆ∂ÂÆåÊàêËøôÈ°πÂä≥Âä®ÂØÜÈõÜÂûã‰ªªÂä°Ôºå‰ΩÜÁõÆÂâçÁöÑ CTA Ê®°Âûã‰∏•Èáç‰æùËµñ‰∫éËÆ∏Â§öËØ¶ÁªÜÁöÑÊ≥®Èáä„ÄÇÊàë‰ª¨ÊèêÂá∫Âπ∂È™åËØÅ‰∫Ü‰∏Ä‰∏™Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ CTAÔºåÂÆÉÂèØ‰ª•Âú®Âá†ÁôæÂÄçÊõ¥Â∞ëÁöÑÁóÖÁêÜÂ≠¶ÂÆ∂Ê≥®Èáä‰∏ä‰ªÖÂú®ÂçÅÂàÜÈíüÂÜÖËøõË°åËÆ≠ÁªÉ„ÄÇÊàë‰ª¨‰ªéÂÖ≠‰∏™ÈòüÂàó‰∏≠Êî∂ÈõÜ‰∫Ü 2,340 Âêç BC ÊÇ£ËÄÖÁöÑ TILs ËØÑÂàÜÂíå‰∏¥Â∫äÊï∞ÊçÆÁöÑÂÖ®ÁéªÁâáÂõæÂÉè (WSI)ÔºåÂÖ∂‰∏≠ÂåÖÊã¨‰∏âÈ°πÈöèÊú∫‰∏¥Â∫äËØïÈ™å„ÄÇ‰ΩøÁî®ÁóÖÁêÜÂü∫Á°ÄÊ®°Âûã‰ªéÂÖ®ÁéªÁâáÂõæÂÉè (WSI) ‰∏≠ÊèêÂèñÂΩ¢ÊÄÅÂ≠¶ÁâπÂæÅ„ÄÇÊàë‰ª¨ÁöÑÊ†áÁ≠æÈ´òÊïàËÆ°ÁÆóÂü∫Ë¥® TIL ËØÑ‰º∞Ê®°Âûã (ECTIL) Áõ¥Êé•‰ªéËøô‰∫õÁâπÂæÅ‰∏≠ÂõûÂΩí TILs ËØÑÂàÜ„ÄÇ‰ªÖÂú®Âá†Áôæ‰∏™Ê†∑Êú¨‰∏äËøõË°åËÆ≠ÁªÉÁöÑ ECTILÔºàECTIL-TCGAÔºâÊòæÁ§∫Âá∫‰∏éÁóÖÁêÜÂ≠¶ÂÆ∂Âú®‰∫î‰∏™ÂºÇË¥®Â§ñÈÉ®ÈòüÂàó‰∏≠ÁöÑ‰∏ÄËá¥ÊÄßÔºàr=0.54-0.74ÔºåAUROC=0.80-0.94Ôºâ„ÄÇÂú®‰∫î‰∏™ÈòüÂàóÁöÑÊâÄÊúâÁéªÁâá‰∏äËøõË°åËÆ≠ÁªÉÔºàECTIL-combinedÔºâÊîπÂñÑ‰∫Ü‰øùÁïôÊµãËØïÈõÜ‰∏äÁöÑÁªìÊûúÔºàr=0.69ÔºåAUROC=0.85Ôºâ„ÄÇÂ§öÂèòÈáè Cox ÂõûÂΩíÂàÜÊûêË°®ÊòéÔºåECTIL ËØÑÂàÜÊØèÂ¢ûÂä† 10%Ôºå‰∏é‰∏¥Â∫äÁóÖÁêÜÂ≠¶ÂèòÈáèÊó†ÂÖ≥ÁöÑÊÄª‰ΩìÁîüÂ≠òÁéáÂ∞±‰ºöÊèêÈ´òÔºàHR 0.86Ôºåp<0.01ÔºâÔºåÁ±ª‰ºº‰∫éÁóÖÁêÜÂ≠¶ÂÆ∂ËØÑÂàÜÔºàHR 0.87Ôºåp<0.001Ôºâ„ÄÇÊàë‰ª¨ËØÅÊòé ECTIL ‰∏é‰∏ìÂÆ∂ÁóÖÁêÜÂ≠¶ÂÆ∂È´òÂ∫¶‰∏ÄËá¥ÔºåÂπ∂Ëé∑Âæó‰∫ÜÁ±ª‰ººÁöÑÈ£éÈô©ÊØî„ÄÇECTIL ÁöÑËÆæËÆ°ÊØîÁé∞ÊúâÊñπÊ≥ï‰ªéÊ†πÊú¨‰∏äÊõ¥ÁÆÄÂçïÔºåÂπ∂‰∏îÂèØ‰ª•Âú®Êï∞ÈáèÁ∫ßÊõ¥Â∞ëÁöÑÊ≥®Èáä‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇËøôÁßç CTA ÂèØÁî®‰∫éÂØπÊÇ£ËÄÖËøõË°åÈ¢ÑÁ≠õÈÄâÔºå‰æãÂ¶ÇÂÖçÁñ´Ê≤ªÁñó‰∏¥Â∫äËØïÈ™åÁ∫≥ÂÖ•ÔºåÊàñ‰Ωú‰∏∫‰∏ÄÁßçÂ∑•ÂÖ∑Êù•Â∏ÆÂä©‰∏¥Â∫äÂåªÁîüÂØπ BC ÊÇ£ËÄÖËøõË°åËØäÊñ≠Ê£ÄÊü•„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂèØÂú®ÂºÄÊîæÊ∫ê‰ª£Á†ÅËÆ∏ÂèØ‰∏ãËé∑Âæó (https://github.com/nki-ai/ectil)„ÄÇ

##### **Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation**
2501.14166v1 by Cong-Duy Nguyen, Xiaobao Wu, Thong Nguyen, Shuai Zhao, Khoi Le, Viet-Anh Nguyen, Feng Yichao, Anh Tuan Luu

Previous research on multimodal entity linking (MEL) has primarily employed
contrastive learning as the primary objective. However, using the rest of the
batch as negative samples without careful consideration, these studies risk
leveraging easy features and potentially overlook essential details that make
entities unique. In this work, we propose JD-CCL (Jaccard Distance-based
Conditional Contrastive Learning), a novel approach designed to enhance the
ability to match multimodal entity linking models. JD-CCL leverages
meta-information to select negative samples with similar attributes, making the
linking task more challenging and robust. Additionally, to address the
limitations caused by the variations within the visual modality among mentions
and entities, we introduce a novel method, CVaCPT (Contextual Visual-aid
Controllable Patch Transform). It enhances visual representations by
incorporating multi-view synthetic images and contextual textual
representations to scale and shift patch representations. Experimental results
on benchmark MEL datasets demonstrate the strong effectiveness of our approach.

ÊëòË¶ÅÔºöÂÖàÂâçÈáùÂ∞çÂ§öÊ®°ÊÖãÂØ¶È´îÈÄ£Áµê (MEL) ÁöÑÁ†îÁ©∂‰∏ªË¶ÅÊé°Áî®Â∞çÊØîÂ≠∏Áøí‰ΩúÁÇ∫‰∏ªË¶ÅÁõÆÊ®ô„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÁ†îÁ©∂Âú®Êú™Á∂ì‰ªîÁ¥∞ËÄÉÈáèÁöÑÊÉÖÊ≥Å‰∏ãÂ∞áÊâπÊ¨°ÂÖ∂È§òÈÉ®ÂàÜÁî®‰ΩúË≤†Ê®£Êú¨ÔºåÂõ†Ê≠§ÊúâÈ¢®Èö™ÊúÉÂà©Áî®ÂÆπÊòìËæ®Ë≠òÁöÑÁâπÂæµÔºå‰∏¶ÂèØËÉΩÂøΩÁï•‰ΩøÂØ¶È´îÁç®‰∏ÄÁÑ°‰∫åÁöÑÈáçË¶ÅÁ¥∞ÁØÄ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ JD-CCLÔºàJaccard Ë∑ùÈõ¢Âü∫Á§éÊ¢ù‰ª∂Â∞çÊØîÂ≠∏ÁøíÔºâÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∑Â§öÊ®°ÊÖãÂØ¶È´îÈÄ£ÁµêÊ®°ÂûãÁöÑÂåπÈÖçËÉΩÂäõ„ÄÇJD-CCL Âà©Áî®ÂÖÉË≥áË®ä‰æÜÈÅ∏ÊìáÂÖ∑ÊúâÈ°û‰ººÂ±¨ÊÄßÁöÑË≤†Ê®£Êú¨Ôºå‰ΩøÈÄ£Áµê‰ªªÂãôÊõ¥ÂÖ∑ÊåëÊà∞ÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜËß£Ê±∫Âú®ÊèêÂèäÂíåÂØ¶È´î‰πãÈñìÁöÑË¶ñË¶∫Ê®°Âºè‰∏≠ËÆäÁï∞ÊâÄÈÄ†ÊàêÁöÑÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÁ®±ÁÇ∫ CVaCPTÔºàËÑàÁµ°Ë¶ñË¶∫ËºîÂä©ÂèØÊéßÂçÄÂ°äËΩâÊèõÔºâ„ÄÇÂÆÉÈÄèÈÅéÁµêÂêàÂ§öË¶ñËßíÂêàÊàêÂΩ±ÂÉèÂíåËÑàÁµ°ÊñáÂ≠óË°®Âæµ‰æÜÂ¢ûÂº∑Ë¶ñË¶∫Ë°®ÂæµÔºå‰ª•Á∏ÆÊîæÂíåËΩâÁßªÂçÄÂ°äË°®Âæµ„ÄÇÂú®Âü∫Ê∫ñ MEL Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÂº∑Â§ßÊïàËÉΩ„ÄÇ

##### **Advancing MRI Reconstruction: A Systematic Review of Deep Learning and Compressed Sensing Integration**
2501.14158v1 by Mojtaba Safari, Zach Eidex, Chih-Wei Chang, Richard L. J. Qiu, Xiaofeng Yang

Magnetic resonance imaging (MRI) is a non-invasive imaging modality and
provides comprehensive anatomical and functional insights into the human body.
However, its long acquisition times can lead to patient discomfort, motion
artifacts, and limiting real-time applications. To address these challenges,
strategies such as parallel imaging have been applied, which utilize multiple
receiver coils to speed up the data acquisition process. Additionally,
compressed sensing (CS) is a method that facilitates image reconstruction from
sparse data, significantly reducing image acquisition time by minimizing the
amount of data collection needed. Recently, deep learning (DL) has emerged as a
powerful tool for improving MRI reconstruction. It has been integrated with
parallel imaging and CS principles to achieve faster and more accurate MRI
reconstructions. This review comprehensively examines DL-based techniques for
MRI reconstruction. We categorize and discuss various DL-based methods,
including end-to-end approaches, unrolled optimization, and federated learning,
highlighting their potential benefits. Our systematic review highlights
significant contributions and underscores the potential of DL in MRI
reconstruction. Additionally, we summarize key results and trends in DL-based
MRI reconstruction, including quantitative metrics, the dataset, acceleration
factors, and the progress of and research interest in DL techniques over time.
Finally, we discuss potential future directions and the importance of DL-based
MRI reconstruction in advancing medical imaging. To facilitate further research
in this area, we provide a GitHub repository that includes up-to-date DL-based
MRI reconstruction publications and public
datasets-https://github.com/mosaf/Awesome-DL-based-CS-MRI.

ÊëòË¶ÅÔºöÁ£ÅÂÖ±ÊåØÊàêÂÉè (MRI) ÊòØ‰∏ÄÁ®ÆÈùû‰æµÂÖ•ÊÄßÁöÑÂΩ±ÂÉèÊ®°ÂºèÔºåÂèØÊèê‰æõ‰∫∫È´îÂÖ®Èù¢ÁöÑËß£ÂâñÂíåÂäüËÉΩË¶ãËß£„ÄÇÁÑ∂ËÄåÔºåÂÖ∂Êº´Èï∑ÁöÑÊì∑ÂèñÊôÇÈñìÂèØËÉΩÊúÉÂ∞éËá¥ÊÇ£ËÄÖ‰∏çÈÅ©„ÄÅÂãï‰ΩúÂÅΩÂΩ±Ôºå‰∏¶ÈôêÂà∂ÂØ¶ÊôÇÊáâÁî®„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÂ∑≤ÊáâÁî®Âπ≥Ë°åÂΩ±ÂÉèÁ≠âÁ≠ñÁï•ÔºåÂà©Áî®Â§öÂÄãÊé•Êî∂Âô®Á∑öÂúà‰æÜÂä†ÈÄüË≥áÊñôÊì∑ÂèñÈÅéÁ®ã„ÄÇÊ≠§Â§ñÔºåÂ£ìÁ∏ÆÊÑüÊ∏¨ (CS) ÊòØ‰∏ÄÁ®Æ‰øÉÈÄ≤ÂæûÁ®ÄÁñèË≥áÊñô‰∏≠ÈáçÂª∫ÂΩ±ÂÉèÁöÑÊñπÊ≥ïÔºåÈÄèÈÅéÂ∞áÊâÄÈúÄÁöÑË≥áÊñôÊî∂ÈõÜÈáèÊ∏õËá≥ÊúÄÂ∞ëÔºåÂ§ßÂπÖÁ∏ÆÁü≠ÂΩ±ÂÉèÊì∑ÂèñÊôÇÈñì„ÄÇÊúÄËøëÔºåÊ∑±Â∫¶Â≠∏Áøí (DL) Â∑≤ÊàêÁÇ∫ÊîπÈÄ≤ MRI ÈáçÂª∫ÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÂÆÉÂ∑≤ËàáÂπ≥Ë°åÂΩ±ÂÉèÂíå CS ÂéüÁêÜÊï¥ÂêàÔºå‰ª•ÂØ¶ÁèæÊõ¥Âø´„ÄÅÊõ¥Ê∫ñÁ¢∫ÁöÑ MRI ÈáçÂª∫„ÄÇÊú¨ÁØáË©ïË´ñÂÖ®Èù¢Êé¢Ë®é‰∫ÜÂü∫Êñº DL ÁöÑ MRI ÈáçÂª∫ÊäÄË°ì„ÄÇÊàëÂÄëÂ∞çÂêÑÁ®ÆÂü∫Êñº DL ÁöÑÊñπÊ≥ïÈÄ≤Ë°åÂàÜÈ°ûÂíåË®éË´ñÔºåÂåÖÊã¨Á´ØÂà∞Á´ØÊñπÊ≥ï„ÄÅÂ±ïÈñãÊúÄ‰Ω≥ÂåñÂíåËÅØÂêàÂ≠∏ÁøíÔºå‰∏¶Âº∑Ë™øÂÖ∂ÊΩõÂú®ÂÑ™Èªû„ÄÇÊàëÂÄëÁöÑÁ≥ªÁµ±ÊÄßË©ïË´ñÁ™ÅÂá∫‰∫ÜÈáçË¶ÅÁöÑË≤¢ÁçªÔºå‰∏¶Âº∑Ë™ø‰∫Ü DL Âú® MRI ÈáçÂª∫‰∏≠ÁöÑÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁ∏ΩÁµê‰∫ÜÂü∫Êñº DL ÁöÑ MRI ÈáçÂª∫‰∏≠ÁöÑÈóúÈçµÁµêÊûúÂíåË∂®Âã¢ÔºåÂåÖÊã¨ÈáèÂåñÊåáÊ®ô„ÄÅË≥áÊñôÈõÜ„ÄÅÂä†ÈÄüÂõ†Â≠êÔºå‰ª•Âèä DL ÊäÄË°ìÈö®ÊôÇÈñìÁöÑÈÄ≤Â±ïÂíåÁ†îÁ©∂ËààË∂£„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÊΩõÂú®ÁöÑÊú™‰æÜÊñπÂêëÔºå‰ª•ÂèäÂü∫Êñº DL ÁöÑ MRI ÈáçÂª∫Âú®Êé®ÈÄ≤ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÈÄôÊñπÈù¢ÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄã GitHub ÂÑ≤Â≠òÂ∫´ÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÊúÄÊñ∞ÁöÑÂü∫Êñº DL ÁöÑ MRI ÈáçÂª∫Âá∫ÁâàÁâ©ÂíåÂÖ¨ÈñãË≥áÊñôÈõÜ - https://github.com/mosaf/Awesome-DL-based-CS-MRI„ÄÇ

##### **MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning**
2501.14105v1 by Joshua Davis, Thomas Sounack, Kate Sciacca, Jessie M Brain, Brigitte N Durieux, Nicole D Agaronnik, Charlotta Lindvall

Extracting sections from clinical notes is crucial for downstream analysis
but is challenging due to variability in formatting and labor-intensive nature
of manual sectioning. While proprietary large language models (LLMs) have shown
promise, privacy concerns limit their accessibility. This study develops a
pipeline for automated note sectioning using open-source LLMs, focusing on
three sections: History of Present Illness, Interval History, and Assessment
and Plan. We fine-tuned three open-source LLMs to extract sections using a
curated dataset of 487 progress notes, comparing results relative to
proprietary models (GPT-4o, GPT-4o mini). Internal and external validity were
assessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B
outperformed GPT-4o (F1=0.92). On the external validity test set, performance
remained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary
models in clinical note sectioning, offering advantages in cost, performance,
and accessibility.

ÊëòË¶ÅÔºöÂæûËá®Â∫äË®òÈåÑ‰∏≠ËêÉÂèñÂçÄÂ°äÂ∞çÊñº‰∏ãÊ∏∏ÂàÜÊûêËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÁî±ÊñºÊ†ºÂºèËÆäÁï∞ÂíåÊâãÂãïÂàÜÂçÄÁöÑÂãûÂäõÂØÜÈõÜÊÄßË≥™ÔºåÈÄôÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂ∞àÊúâÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÊΩõÂäõÔºå‰ΩÜÈö±ÁßÅÂïèÈ°åÈôêÂà∂‰∫ÜÂÖ∂ÂèØÂèäÊÄß„ÄÇÊú¨Á†îÁ©∂ÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ΩøÁî®ÈñãÊîæÂéüÂßãÁ¢º LLM ÁöÑËá™ÂãïÂåñË®òÈåÑÂàÜÂçÄÁÆ°Á∑öÔºåÂ∞àÊ≥®Êñº‰∏âÂÄãÂçÄÂ°äÔºöÁèæÁóÖÂè≤„ÄÅÈñìÈöîÁóÖÂè≤‰ª•ÂèäË©ï‰º∞ÂíåË®àÁï´„ÄÇÊàëÂÄëÂæÆË™ø‰∫Ü‰∏âÂÄãÈñãÊîæÂéüÂßãÁ¢º LLM ‰ª•‰ΩøÁî® 487 ÂÄãÈÄ≤Â∫¶Ë®òÈåÑÁöÑÁ≤æÈÅ∏Ë≥áÊñôÈõÜËêÉÂèñÂçÄÂ°äÔºå‰∏¶Â∞áÁµêÊûúËàáÂ∞àÊúâÊ®°Âûã (GPT-4o„ÄÅGPT-4o mini) ÈÄ≤Ë°åÊØîËºÉ„ÄÇÂÖßÈÉ®ÂíåÂ§ñÈÉ®ÊïàÂ∫¶ÈÄèÈÅéÊ∫ñÁ¢∫Â∫¶„ÄÅÂè¨ÂõûÁéáÂíå F1 ÂàÜÊï∏ÈÄ≤Ë°åË©ï‰º∞„ÄÇÂæÆË™øÂæåÁöÑ Llama 3.1 8B ÂÑ™Êñº GPT-4o (F1=0.92)„ÄÇÂú®Â§ñÈÉ®ÊïàÂ∫¶Ê∏¨Ë©¶ÈõÜ‰∏≠ÔºåÊïàËÉΩ‰ªçÁÑ∂ÂæàÈ´ò (F1= 0.85)„ÄÇÂæÆË™øÂæåÁöÑÈñãÊîæÂéüÂßãÁ¢º LLM ËÉΩÂú®Ëá®Â∫äË®òÈåÑÂàÜÂçÄ‰∏≠Ë∂ÖË∂äÂ∞àÊúâÊ®°ÂûãÔºåÂú®ÊàêÊú¨„ÄÅÊïàËÉΩÂíåÂèØÂèäÊÄßÊñπÈù¢Êèê‰æõÂÑ™Âã¢„ÄÇ

##### **Revisiting CLIP: Efficient Alignment of 3D MRI and Tabular Data using Domain-Specific Foundation Models**
2501.14051v1 by Jakob Krogh Petersen, Valdemar Licht, Mads Nielsen, Asbj√∏rn Munk

Multi-modal models require aligned, shared embedding spaces. However, common
CLIP-based approaches need large amounts of samples and do not natively support
3D or tabular data, both of which are crucial in the medical domain. To address
these issues, we revisit CLIP-style alignment by training a domain-specific 3D
foundation model as an image encoder and demonstrate that modality alignment is
feasible with only 62 MRI scans. Our approach is enabled by a simple embedding
accumulation strategy required for training in 3D, which scales the amount of
negative pairs across batches in order to stabilize training. We perform a
thorough evaluation of various design choices, including the choice of backbone
and loss functions, and evaluate the proposed methodology on zero-shot
classification and image-retrieval tasks. While zero-shot image-retrieval
remains challenging, zero-shot classification results demonstrate that the
proposed approach can meaningfully align the representations of 3D MRI with
tabular data.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÖãÊ®°ÂûãÈúÄË¶ÅÂ∞çÈΩäÁöÑÂÖ±Áî®ÂµåÂÖ•Á©∫Èñì„ÄÇÁÑ∂ËÄåÔºåÂ∏∏Ë¶ãÁöÑÂü∫Êñº CLIP ÁöÑÊñπÊ≥ïÈúÄË¶ÅÂ§ßÈáèÁöÑÊ®£Êú¨Ôºå‰∏¶‰∏îÂéüÁîü‰∏çÊîØÊè¥ 3D ÊàñË°®Ê†ºË≥áÊñôÔºåËÄåÈÄôÂÖ©ËÄÖÂú®ÈÜ´ÁôÇÈ†òÂüü‰∏≠ÈÉΩËá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÈÄèÈÅéË®ìÁ∑¥‰∏ÄÂÄãÈ†òÂüüÁâπÂÆöÁöÑ 3D Âü∫Á§éÊ®°Âûã‰ΩúÁÇ∫ÂΩ±ÂÉèÁ∑®Á¢ºÂô®ÔºåÈáçÊñ∞Ê™¢Ë¶ñ CLIP È¢®Ê†ºÁöÑÂ∞çÈΩäÔºå‰∏¶Ë≠âÊòéÂè™Ë¶Å 62 ÂÄã MRI ÊéÉÊèèÂç≥ÂèØÈÅîÊàêÊ®°ÊÖãÂ∞çÈΩä„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂæóÁõäÊñº‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÂµåÂÖ•Á¥ØÁ©çÁ≠ñÁï•ÔºåÈÄôÊòØ 3D Ë®ìÁ∑¥ÊâÄÂøÖÈúÄÁöÑÔºåÂÆÉÊúÉË™øÊï¥ÊâπÊ¨°‰∏≠ÁöÑË≤†Â∞çÊï∏Èáè‰ª•Á©©ÂÆöË®ìÁ∑¥„ÄÇÊàëÂÄëÂ∞çÂêÑÁ®ÆË®≠Ë®àÈÅ∏ÊìáÈÄ≤Ë°å‰∫ÜÂæπÂ∫ïÁöÑË©ï‰º∞ÔºåÂåÖÊã¨‰∏ªÂππÂíåÊêçÂ§±ÂáΩÊï∏ÁöÑÈÅ∏ÊìáÔºå‰∏¶Âú®Èõ∂Ê®£Êú¨ÂàÜÈ°ûÂíåÂΩ±ÂÉèÊ™¢Á¥¢‰ªªÂãô‰∏äË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï„ÄÇÂÑòÁÆ°Èõ∂Ê®£Êú¨ÂΩ±ÂÉèÊ™¢Á¥¢‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰ΩÜÈõ∂Ê®£Êú¨ÂàÜÈ°ûÁµêÊûúË≠âÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂèØ‰ª•ÊúâÊÑèÁæ©Âú∞Â∞á 3D MRI ÁöÑË°®Á§∫ËàáË°®Ê†ºË≥áÊñôÂ∞çÈΩä„ÄÇ

##### **Leveraging Multiphase CT for Quality Enhancement of Portal Venous CT: Utility for Pancreas Segmentation**
2501.14013v1 by Xinya Wang, Tejas Sudharshan Mathai, Boah Kim, Ronald M. Summers

Multiphase CT studies are routinely obtained in clinical practice for
diagnosis and management of various diseases, such as cancer. However, the CT
studies can be acquired with low radiation doses, different scanners, and are
frequently affected by motion and metal artifacts. Prior approaches have
targeted the quality improvement of one specific CT phase (e.g., non-contrast
CT). In this work, we hypothesized that leveraging multiple CT phases for the
quality enhancement of one phase may prove advantageous for downstream tasks,
such as segmentation. A 3D progressive fusion and non-local (PFNL) network was
developed. It was trained with three degraded (low-quality) phases
(non-contrast, arterial, and portal venous) to enhance the quality of the
portal venous phase. Then, the effect of scan quality enhancement was evaluated
using a proxy task of pancreas segmentation, which is useful for tracking
pancreatic cancer. The proposed approach improved the pancreas segmentation by
3% over the corresponding low-quality CT scan. To the best of our knowledge, we
are the first to harness multiphase CT for scan quality enhancement and
improved pancreas segmentation.

ÊëòË¶ÅÔºöÂ§öÁõ∏ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÁ†îÁ©∂Âú®Ëá®Â∫äÂØ¶Âãô‰∏≠Â∏∏Ë¶èÂèñÂæóÔºåÁî®ÊñºË®∫Êñ∑ÂíåÁÆ°ÁêÜÂêÑÁ®ÆÁñæÁóÖÔºå‰æãÂ¶ÇÁôåÁóá„ÄÇÁÑ∂ËÄåÔºåÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÁ†îÁ©∂ÂèØ‰ª•Áî®‰ΩéËºªÂ∞ÑÂäëÈáè„ÄÅ‰∏çÂêåÁöÑÊéÉÊèèÂÑÄÂèñÂæóÔºå‰∏îÁ∂ìÂ∏∏ÂèóÂà∞ÈÅãÂãïÂíåÈáëÂ±¨Ë£ΩÂìÅÂΩ±Èüø„ÄÇÂÖàÂâçÁöÑÂÅöÊ≥ïÂ∑≤ÈáùÂ∞çÁâπÂÆöÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÁõ∏‰ΩçÔºà‰æãÂ¶ÇÈùûÂ∞çÊØîÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÔºâÁöÑÂìÅË≥™ÊîπÂñÑ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂÅáË®≠Âà©Áî®Â§öÂÄãÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÁõ∏‰Ωç‰æÜÊîπÂñÑ‰∏ÄÂÄãÁõ∏‰ΩçÁöÑÂìÅË≥™ÔºåÂèØËÉΩÊúÉÂ∞ç‰∏ãÊ∏∏‰ªªÂãôÔºà‰æãÂ¶ÇÂàÜÂâ≤ÔºâÊúâÂà©„ÄÇÈñãÁôº‰∫Ü‰∏ÄÂÄã 3D Êº∏ÈÄ≤ËûçÂêàÂíåÈùûÂ±ÄÈÉ® (PFNL) Á∂≤Ë∑Ø„ÄÇÂÆÉ‰ΩøÁî®‰∏âÂÄãÈÄÄÂåñÁöÑÔºà‰ΩéÂìÅË≥™ÔºâÁõ∏‰ΩçÔºàÈùûÂ∞çÊØî„ÄÅÂãïËÑàÂíåÈñÄÈùúËÑàÔºâÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰ª•Â¢ûÂº∑ÈñÄÈùúËÑàÁõ∏‰ΩçÁöÑÂìÅË≥™„ÄÇÁÑ∂ÂæåÔºå‰ΩøÁî®ËÉ∞ËáüÂàÜÂâ≤ÁöÑ‰ª£ÁêÜ‰ªªÂãôË©ï‰º∞ÊéÉÊèèÂìÅË≥™ÊîπÂñÑÁöÑÊïàÊûúÔºåÈÄôÂ∞çÊñºËøΩËπ§ËÉ∞ËáüÁôåÂæàÊúâÁî®„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂ∞áËÉ∞ËáüÂàÜÂâ≤ÊîπÂñÑ‰∫Ü 3%ÔºåÈ´òÊñºÂ∞çÊáâÁöÑ‰ΩéÂìÅË≥™ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÊàëÂÄëÊòØÁ¨¨‰∏ÄÂÄãÂà©Áî®Â§öÁõ∏ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÈÄ≤Ë°åÊéÉÊèèÂìÅË≥™ÊîπÂñÑÂíåÊîπÂñÑËÉ∞ËáüÂàÜÂâ≤ÁöÑ‰∫∫„ÄÇ

##### **Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data**
2501.13818v1 by Frederik Pahde, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek

Deep neural networks are increasingly employed in high-stakes medical
applications, despite their tendency for shortcut learning in the presence of
spurious correlations, which can have potentially fatal consequences in
practice. Detecting and mitigating shortcut behavior is a challenging task that
often requires significant labeling efforts from domain experts. To alleviate
this problem, we introduce a semi-automated framework for the identification of
spurious behavior from both data and model perspective by leveraging insights
from eXplainable Artificial Intelligence (XAI). This allows the retrieval of
spurious data points and the detection of model circuits that encode the
associated prediction rules. Moreover, we demonstrate how these shortcut
encodings can be used for XAI-based sample- and pixel-level data annotation,
providing valuable information for bias mitigation methods to unlearn the
undesired shortcut behavior. We show the applicability of our framework using
four medical datasets across two modalities, featuring controlled and
real-world spurious correlations caused by data artifacts. We successfully
identify and mitigate these biases in VGG16, ResNet50, and contemporary Vision
Transformer models, ultimately increasing their robustness and applicability
for real-world medical tasks.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúË∂äÊù•Ë∂äÂ§öÂú∞Áî®‰∫éÈ´òÈ£éÈô©ÂåªÁñóÂ∫îÁî®‰∏≠ÔºåÂ∞ΩÁÆ°ÂÆÉ‰ª¨Âú®Â≠òÂú®ËôöÂÅáÁõ∏ÂÖ≥ÊÄßÁöÑÊÉÖÂÜµ‰∏ãÂÄæÂêë‰∫éÊç∑ÂæÑÂ≠¶‰π†ÔºåËøôÂú®ÂÆûË∑µ‰∏≠ÂèØËÉΩ‰∫ßÁîüËá¥ÂëΩÁöÑÂêéÊûú„ÄÇÊ£ÄÊµãÂíåÁºìËß£Êç∑ÂæÑË°å‰∏∫ÊòØ‰∏ÄÈ°πËâ∞Â∑®ÁöÑ‰ªªÂä°ÔºåÈÄöÂ∏∏ÈúÄË¶ÅÈ¢ÜÂüü‰∏ìÂÆ∂ÁöÑÂ§ßÈáèÊ†áËÆ∞Â∑•‰Ωú„ÄÇ‰∏∫‰∫ÜÁºìËß£Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂçäËá™Âä®Ê°ÜÊû∂ÔºåÁî®‰∫é‰ªéÊï∞ÊçÆÂíåÊ®°ÂûãÁöÑËßíÂ∫¶ËØÜÂà´ËôöÂÅáË°å‰∏∫ÔºåÊñπÊ≥ïÊòØÂà©Áî®ÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩ (XAI) ÁöÑËßÅËß£„ÄÇËøôÂÖÅËÆ∏Ê£ÄÁ¥¢ËôöÂÅáÊï∞ÊçÆÁÇπÂπ∂Ê£ÄÊµãÂØπÂÖ≥ËÅîÈ¢ÑÊµãËßÑÂàôËøõË°åÁºñÁ†ÅÁöÑÊ®°ÂûãÁîµË∑Ø„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊºîÁ§∫‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî®Ëøô‰∫õÊç∑ÂæÑÁºñÁ†ÅËøõË°åÂü∫‰∫é XAI ÁöÑÊ†∑Êú¨ÂíåÂÉèÁ¥†Á∫ßÊï∞ÊçÆÊ≥®ÈáäÔºå‰∏∫ÂÅèÂ∑ÆÁºìËß£ÊñπÊ≥ïÊèê‰æõÊúâ‰ª∑ÂÄºÁöÑ‰ø°ÊÅØÔºå‰ª•Ê∂àÈô§‰∏çÈúÄË¶ÅÁöÑÊç∑ÂæÑË°å‰∏∫„ÄÇÊàë‰ª¨‰ΩøÁî®Ë∑®Ë∂ä‰∏§ÁßçÊñπÂºèÁöÑÂõõ‰∏™ÂåªÂ≠¶Êï∞ÊçÆÈõÜÂ±ïÁ§∫‰∫ÜÊàë‰ª¨Ê°ÜÊû∂ÁöÑÈÄÇÁî®ÊÄßÔºåËøô‰∫õÊï∞ÊçÆÈõÜÂÖ∑ÊúâÁî±Êï∞ÊçÆ‰º™ÂÉèÂºïËµ∑ÁöÑÂèóÊéßÂíåÁúüÂÆû‰∏ñÁïåËôöÂÅáÁõ∏ÂÖ≥ÊÄß„ÄÇÊàë‰ª¨ÊàêÂäüÂú∞ËØÜÂà´Âπ∂ÂáèËΩª‰∫Ü VGG16„ÄÅResNet50 ÂíåÂΩì‰ª£ Vision Transformer Ê®°Âûã‰∏≠ÁöÑËøô‰∫õÂÅèÂ∑ÆÔºåÊúÄÁªàÊèêÈ´ò‰∫ÜÂÆÉ‰ª¨ÁöÑÈ≤ÅÊ£íÊÄßÂíåÂú®ÁúüÂÆû‰∏ñÁïåÂåªÁñó‰ªªÂä°‰∏≠ÁöÑÈÄÇÁî®ÊÄß„ÄÇ

##### **Question Answering on Patient Medical Records with Private Fine-Tuned LLMs**
2501.13687v1 by Sara Kothari, Ayush Gupta

Healthcare systems continuously generate vast amounts of electronic health
records (EHRs), commonly stored in the Fast Healthcare Interoperability
Resources (FHIR) standard. Despite the wealth of information in these records,
their complexity and volume make it difficult for users to retrieve and
interpret crucial health insights. Recent advances in Large Language Models
(LLMs) offer a solution, enabling semantic question answering (QA) over medical
data, allowing users to interact with their health records more effectively.
However, ensuring privacy and compliance requires edge and private deployments
of LLMs.
  This paper proposes a novel approach to semantic QA over EHRs by first
identifying the most relevant FHIR resources for a user query (Task1) and
subsequently answering the query based on these resources (Task2). We explore
the performance of privately hosted, fine-tuned LLMs, evaluating them against
benchmark models such as GPT-4 and GPT-4o. Our results demonstrate that
fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by
0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we
examine advanced aspects of LLM usage, including sequential fine-tuning, model
self-evaluation (narcissistic evaluation), and the impact of training data size
on performance. The models and datasets are available here:
https://huggingface.co/genloop

ÊëòË¶ÅÔºöÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÊåÅÁ∫åÁî¢ÁîüÂ§ßÈáèÁöÑÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR)ÔºåÈÄöÂ∏∏ÂÑ≤Â≠òÂú®Âø´ÈÄüÈÜ´ÁôÇ‰∫íÈÄöÊÄßË≥áÊ∫ê (FHIR) Ê®ôÊ∫ñ‰∏≠„ÄÇÂÑòÁÆ°ÈÄô‰∫õÁ¥ÄÈåÑ‰∏≠ÂåÖÂê´Ë±êÂØåÁöÑË≥áË®äÔºå‰ΩÜÂÖ∂Ë§áÈõúÊÄßÂíåÈæêÂ§ßÊï∏ÈáèËÆì‰ΩøÁî®ËÄÖÈõ£‰ª•Êì∑ÂèñÂíåË©ÆÈáãÈáçË¶ÅÁöÑÂÅ•Â∫∑Ë¶ãËß£„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊèê‰æõ‰∫ÜËß£Ê±∫ÊñπÊ°àÔºåËÉΩÂ∞çÈÜ´ÁôÇË≥áÊñôÈÄ≤Ë°åË™ûÁæ©ÂïèÁ≠î (QA)ÔºåËÆì‰ΩøÁî®ËÄÖËÉΩÊõ¥ÊúâÊïàÂú∞ËàáÂÖ∂ÂÅ•Â∫∑Á¥ÄÈåÑ‰∫íÂãï„ÄÇÁÑ∂ËÄåÔºåÁ¢∫‰øùÈö±ÁßÅÂíåÁõ∏ÂÆπÊÄßÈúÄË¶Å LLM ÁöÑÈÇäÁ∑£ÂíåÁßÅ‰∫∫ÈÉ®ÁΩ≤„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜË™ûÁæ©ÂïèÁ≠îÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÖàÊâæÂá∫Ëàá‰ΩøÁî®ËÄÖÊü•Ë©¢ÊúÄÁõ∏ÈóúÁöÑ FHIR Ë≥áÊ∫ê (‰ªªÂãô 1)ÔºåÁÑ∂ÂæåÊ†πÊìöÈÄô‰∫õË≥áÊ∫êÂõûÁ≠îÊü•Ë©¢ (‰ªªÂãô 2)„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜÁßÅ‰∫∫‰∏ªÊ©ü„ÄÅÂæÆË™ø LLM ÁöÑÊïàËÉΩÔºå‰∏¶Ê†πÊìö GPT-4 Âíå GPT-4o Á≠âÂü∫Ê∫ñÊ®°ÂûãË©ï‰º∞ÂÆÉÂÄë„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÂæÆË™ø LLM ÁöÑÂ§ßÂ∞èÈõñÁÑ∂Â∞è 250 ÂÄçÔºå‰ΩÜÂú®‰ªªÂãô 1 ÁöÑ F1 ÂàÜÊï∏‰∏äÂÑ™Êñº GPT-4 Á≥ªÂàóÊ®°Âûã 0.55%ÔºåÂú®‰ªªÂãô 2 ÁöÑ Meteor ‰ªªÂãô‰∏≠ÂÑ™Êñº 42%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM ‰ΩøÁî®ÁöÑÈ´òÈöéÈù¢ÂêëÔºåÂåÖÊã¨Âæ™Â∫èÂæÆË™ø„ÄÅÊ®°ÂûãËá™ÊàëË©ï‰º∞ÔºàËá™ÊàÄÂºèË©ï‰º∞ÔºâÂíåË®ìÁ∑¥Ë≥áÊñôÂ§ßÂ∞èÂ∞çÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇÊ®°ÂûãÂíåË≥áÊñôÈõÜÂú®Ê≠§ËôïÊèê‰æõÔºöhttps://huggingface.co/genloop

##### **How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization**
2501.13669v1 by Shezheng Song, Hao Xu, Jun Ma, Shasha Li, Long Peng, Qian Wan, Xiaodong Liu, Jie Yu

Large Language Models (LLMs) exhibit strong general-purpose language
capabilities. However, fine-tuning these models on domain-specific tasks often
leads to catastrophic forgetting, where the model overwrites or loses essential
knowledge acquired during pretraining. This phenomenon significantly limits the
broader applicability of LLMs. To address this challenge, we propose a novel
approach to compute the element-wise importance of model parameters crucial for
preserving general knowledge during fine-tuning. Our method utilizes a
dual-objective optimization strategy: (1) regularization loss to retain the
parameter crucial for general knowledge; (2) cross-entropy loss to adapt to
domain-specific tasks. Additionally, we introduce layer-wise coefficients to
account for the varying contributions of different layers, dynamically
balancing the dual-objective optimization. Extensive experiments on scientific,
medical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our
approach mitigates catastrophic forgetting while enhancing model adaptability.
Compared to previous methods, our solution is approximately 20 times faster and
requires only 10%-15% of the storage, highlighting the practical efficiency.
The code will be released.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â±ïÁèæÂº∑Â§ßÁöÑÈÄöÁî®Ë™ûË®ÄËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÈáùÂ∞çÁâπÂÆöÈ†òÂüü‰ªªÂãôÂæÆË™øÈÄô‰∫õÊ®°ÂûãÊôÇÔºåÂ∏∏Â∏∏ÊúÉÂ∞éËá¥ÁÅΩÈõ£ÊÄßÈÅ∫ÂøòÔºåÊ®°ÂûãÊúÉË¶ÜÂØ´ÊàñÈÅ∫Â§±È†êË®ìÁ∑¥ÊúüÈñìÁøíÂæóÁöÑÂü∫Êú¨Áü•Ë≠ò„ÄÇÈÄôÁ®ÆÁèæË±°Â§ßÂπÖÈôêÂà∂‰∫Ü LLM ÁöÑÂª£Ê≥õÈÅ©Áî®ÊÄß„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÈ†ÖÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÊñπÊ≥ïÔºåÁî®ÊñºË®àÁÆóÊ®°ÂûãÂèÉÊï∏ÁöÑÂÖÉÁ¥†Á¥öÈáçË¶ÅÊÄßÔºåÈÄô‰∫õÂèÉÊï∏Â∞çÊñºÂú®ÂæÆË™øÊúüÈñì‰øùÁïô‰∏ÄËà¨Áü•Ë≠òËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊé°Áî®ÈõôÁõÆÊ®ôÂÑ™ÂåñÁ≠ñÁï•Ôºö(1) Ê≠£ÂâáÂåñÊêçÂ§±ÔºåÁî®Êñº‰øùÁïôÂ∞ç‰∏ÄËà¨Áü•Ë≠òËá≥ÈóúÈáçË¶ÅÁöÑÂèÉÊï∏Ôºõ(2) ‰∫§ÂèâÁÜµÊêçÂ§±ÔºåÁî®ÊñºÈÅ©ÊáâÁâπÂÆöÈ†òÂüüÁöÑ‰ªªÂãô„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ±§Á¥ö‰øÇÊï∏ÔºåÁî®ÊñºËÄÉÈáè‰∏çÂêåÂ±§ÁöÑËÆäÁï∞Ë≤¢ÁçªÔºå‰∏¶ÂãïÊÖãÂπ≥Ë°°ÈõôÁõÆÊ®ôÂÑ™Âåñ„ÄÇ‰ΩøÁî® GPT-J Âíå LLaMA-3 Âú®ÁßëÂ≠∏„ÄÅÈÜ´ÁôÇÂíåÁâ©ÁêÜ‰ªªÂãô‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊ∏õËºï‰∫ÜÁÅΩÈõ£ÊÄßÈÅ∫ÂøòÔºåÂêåÊôÇÂ¢ûÂº∑‰∫ÜÊ®°ÂûãÈÅ©ÊáâÊÄß„ÄÇËàá‰πãÂâçÁöÑÂÅöÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑËß£Ê±∫ÊñπÊ°àÈÄüÂ∫¶Âø´‰∫ÜÁ¥Ñ 20 ÂÄçÔºåËÄå‰∏îÂè™ÈúÄË¶Å 10%-15% ÁöÑÂÑ≤Â≠òÁ©∫ÈñìÔºåÁ™ÅÈ°Ø‰∫ÜÂÖ∂ÂØ¶Áî®ÁöÑÊïàÁéá„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÊúÉÈáãÂá∫„ÄÇ

##### **Contrastive Representation Learning Helps Cross-institutional Knowledge Transfer: A Study in Pediatric Ventilation Management**
2501.13587v1 by Yuxuan, Liu, Jinpei Han, Padmanabhan Ramnarayan, A. Aldo Faisal

Clinical machine learning deployment across institutions faces significant
challenges when patient populations and clinical practices differ
substantially. We present a systematic framework for cross-institutional
knowledge transfer in clinical time series, demonstrated through pediatric
ventilation management between a general pediatric intensive care unit (PICU)
and a cardiac-focused unit. Using contrastive predictive coding (CPC) for
representation learning, we investigate how different data regimes and
fine-tuning strategies affect knowledge transfer across institutional
boundaries. Our results show that while direct model transfer performs poorly,
CPC with appropriate fine-tuning enables effective knowledge sharing between
institutions, with benefits particularly evident in limited data scenarios.
Analysis of transfer patterns reveals an important asymmetry: temporal
progression patterns transfer more readily than point-of-care decisions,
suggesting practical pathways for cross-institutional deployment. Through a
systematic evaluation of fine-tuning approaches and transfer patterns, our work
provides insights for developing more generalizable clinical decision support
systems while enabling smaller specialized units to leverage knowledge from
larger centers.

ÊëòË¶ÅÔºöËá®Â∫äÊ©üÂô®Â≠∏ÁøíÈÉ®ÁΩ≤Âú®Ê©üÊßãÈñìÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞ÔºåÁï∂ÊÇ£ËÄÖÊóèÁæ§ÂíåËá®Â∫äÂØ¶ÂãôÊúâÈ°ØËëóÂ∑ÆÁï∞ÊôÇ„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁî®ÊñºËá®Â∫äÊôÇÈñìÂ∫èÂàóÁöÑË∑®Ê©üÊßãÁü•Ë≠òËΩâÁßªÁöÑÁ≥ªÁµ±ÂåñÊû∂ÊßãÔºåÈÄèÈÅé‰∏ÄËà¨Â∞èÂÖíÂä†Ë≠∑ÁóÖÊàø (PICU) ÂíåÂøÉËáüÂ∞àÁßëÁóÖÊàø‰πãÈñìÁöÑÂÖíÁßëÂëºÂê∏Âô®ÁÆ°ÁêÜÂä†‰ª•Ë≠âÊòé„ÄÇ‰ΩøÁî®Â∞çÊØîÈ†êÊ∏¨Á∑®Á¢º (CPC) ÈÄ≤Ë°åË°®ÂæµÂ≠∏ÁøíÔºåÊàëÂÄëÊé¢Ë®é‰∏çÂêåÁöÑË≥áÊñôÂà∂Â∫¶ÂíåÂæÆË™øÁ≠ñÁï•Â¶Ç‰ΩïÂΩ±ÈüøË∑®Ê©üÊßãÈÇäÁïåÁöÑÁü•Ë≠òËΩâÁßª„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÂÑòÁÆ°Áõ¥Êé•Ê®°ÂûãËΩâÁßªÂü∑Ë°å‰∏ç‰Ω≥Ôºå‰ΩÜ‰ΩøÁî®ÈÅ©Áï∂ÂæÆË™øÁöÑ CPC ËÉΩÂ§†Âú®Ê©üÊßãÈñìÈÄ≤Ë°åÊúâÊïàÁöÑÁü•Ë≠òÂàÜ‰∫´ÔºåÂÖ∂Â•ΩËôïÂú®ÊúâÈôêË≥áÊñôÊÉÖÂ¢É‰∏≠ÁâπÂà•ÊòéÈ°Ø„ÄÇËΩâÁßªÊ®°ÂºèÂàÜÊûêÊè≠Èú≤‰∫Ü‰∏ÄÂÄãÈáçË¶ÅÁöÑ‰∏çÂ∞çÁ®±ÊÄßÔºöÊôÇÈñìÈÄ≤Á®ãÊ®°ÂºèÊØîÁÖßË≠∑ÈªûÊ±∫Á≠ñÊõ¥ÂÆπÊòìËΩâÁßªÔºåÈÄôË°®Á§∫Ë∑®Ê©üÊßãÈÉ®ÁΩ≤ÁöÑÂØ¶ÂãôÈÄîÂæë„ÄÇÈÄèÈÅéÂæÆË™øÊñπÊ≥ïÂíåËΩâÁßªÊ®°ÂºèÁöÑÁ≥ªÁµ±ÊÄßË©ï‰º∞ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂Êèê‰æõË¶ãËß£ÔºåÁî®ÊñºÈñãÁôºÊõ¥ÂÖ∑Ê¶ÇÊã¨ÊÄßÁöÑËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ±ÔºåÂêåÊôÇËÆìËºÉÂ∞èÁöÑÂ∞àÁßëÂñÆ‰ΩçËÉΩÂ§†Âà©Áî®‰æÜËá™ËºÉÂ§ß‰∏≠ÂøÉÁöÑÁü•Ë≠ò„ÄÇ

##### **LLMs Can Plan Only If We Tell Them**
2501.13545v1 by Bilgehan Sel, Ruoxi Jia, Ming Jin

Large language models (LLMs) have demonstrated significant capabilities in
natural language processing and reasoning, yet their effectiveness in
autonomous planning has been under debate. While existing studies have utilized
LLMs with external feedback mechanisms or in controlled environments for
planning, these approaches often involve substantial computational and
development resources due to the requirement for careful design and iterative
backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to
match human performance on standard planning benchmarks, such as the
Blocksworld, without additional support. This paper investigates whether LLMs
can independently generate long-horizon plans that rival human baselines. Our
novel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help
achieve state-of-the-art results in planning benchmarks out-competing prior
methods and human baselines all autonomously.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÊé®ÁêÜÊñπÈù¢Â±ïÁ§∫‰∫ÜÈ°ØËëóÁöÑËÉΩÂäõÔºå‰ΩÜÂÆÉÂÄëÂú®Ëá™‰∏ªË¶èÂäÉ‰∏≠ÁöÑÊúâÊïàÊÄß‰∏ÄÁõ¥Â≠òÂú®Áà≠Ë≠∞„ÄÇÂÑòÁÆ°ÁèæÊúâÁ†îÁ©∂Â∑≤Â∞á LLM ËàáÂ§ñÈÉ®ÂõûÈ•ãÊ©üÂà∂ÁµêÂêà‰ΩøÁî®ÔºåÊàñÂú®ÂèóÊéßÁí∞Â¢É‰∏≠ÈÄ≤Ë°åË¶èÂäÉÔºå‰ΩÜÁî±ÊñºÈúÄË¶Å‰ªîÁ¥∞Ë®≠Ë®àÂíåÂèçË¶ÜÊèêÁ§∫ÔºåÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏Ê∂âÂèäÂ§ßÈáèÁöÑË®àÁÆóÂíåÈñãÁôºË≥áÊ∫ê„ÄÇÊ≠§Â§ñÔºåÂç≥‰ΩøÊòØÊúÄÂÖàÈÄ≤ÁöÑ LLMÔºà‰æãÂ¶Ç GPT-4ÔºâÂú®Ê≤íÊúâÈ°çÂ§ñÊîØÊè¥ÁöÑÊÉÖÊ≥Å‰∏ãÔºå‰πüÂæàÈõ£Âú®Ê®ôÊ∫ñË¶èÂäÉÂü∫Ê∫ñÔºà‰æãÂ¶Ç BlocksworldÔºâ‰∏äÈÅîÂà∞‰∫∫È°ûÁöÑË°®Áèæ„ÄÇÊú¨ÊñáÊé¢Ë®é LLM ÊòØÂê¶ËÉΩÁç®Á´ãÁîüÊàêËàá‰∫∫È°ûÂü∫Ê∫ñÁõ∏Â™≤ÁæéÁöÑÈï∑ÈÅ†Ë®àÁï´„ÄÇÊàëÂÄëÂ∞çÊÄùÊÉ≥ÊºîÁÆóÊ≥ï (AoT) ÁöÑÂâµÊñ∞Âº∑ÂåñÔºàÊàëÂÄëÁ®±‰πãÁÇ∫ AoT+ÔºâÊúâÂä©ÊñºÂú®Ë¶èÂäÉÂü∫Ê∫ñ‰∏≠ÂèñÂæóÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûúÔºåÂú®ÂÆåÂÖ®Ëá™‰∏ªÁöÑÊÉÖÊ≥Å‰∏ãÂãùÈÅéÂÖàÂâçÁöÑÂêÑÁ®ÆÊñπÊ≥ïÂíå‰∫∫È°ûÂü∫Ê∫ñ„ÄÇ

##### **Comprehensive Modeling and Question Answering of Cancer Clinical Practice Guidelines using LLMs**
2501.13984v1 by Bhumika Gupta, Pralaypati Ta, Keerthi Ram, Mohanasankar Sivaprakasam

The updated recommendations on diagnostic procedures and treatment pathways
for a medical condition are documented as graphical flows in Clinical Practice
Guidelines (CPGs). For effective use of the CPGs in helping medical
professionals in the treatment decision process, it is necessary to fully
capture the guideline knowledge, particularly the contexts and their
relationships in the graph. While several existing works have utilized these
guidelines to create rule bases for Clinical Decision Support Systems, limited
work has been done toward directly capturing the full medical knowledge
contained in CPGs. This work proposes an approach to create a contextually
enriched, faithful digital representation of National Comprehensive Cancer
Network (NCCN) Cancer CPGs in the form of graphs using automated extraction and
node & relationship classification. We also implement semantic enrichment of
the model by using Large Language Models (LLMs) for node classification,
achieving an accuracy of 80.86% and 88.47% with zero-shot learning and few-shot
learning, respectively. Additionally, we introduce a methodology for answering
natural language questions with constraints to guideline text by leveraging
LLMs to extract the relevant subgraph from the guideline knowledge base. By
generating natural language answers based on subgraph paths and semantic
information, we mitigate the risk of incorrect answers and hallucination
associated with LLMs, ensuring factual accuracy in medical domain Question
Answering.

ÊëòË¶ÅÔºöÂ∑≤Êõ¥Êñ∞ÁöÑÈÜ´ÁôÇÁãÄÊ≥ÅË®∫Êñ∑Á®ãÂ∫èÂíåÊ≤ªÁôÇÈÄîÂæëÂª∫Ë≠∞Ôºå‰ª•Ëá®Â∫äÂØ¶ÂãôÊåáÂçó (CPG) ‰∏≠ÁöÑÂúñÂΩ¢ÊµÅÁ®ãË®òÈåÑ„ÄÇÁÇ∫‰∫ÜÊúâÊïà‰ΩøÁî® CPG ÂçîÂä©ÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°ÈÄ≤Ë°åÊ≤ªÁôÇÊ±∫Á≠ñÔºåÂøÖÈ†àÂÆåÊï¥Êì∑ÂèñÊåáÂçóÁü•Ë≠òÔºåÁâπÂà•ÊòØÂúñË°®‰∏≠ÁöÑËÑàÁµ°ÂèäÂÖ∂Èóú‰øÇ„ÄÇÈõñÁÑ∂ÁèæÊúâË®±Â§öÁ†îÁ©∂Â∑≤Âà©Áî®ÈÄô‰∫õÊåáÂçóÁÇ∫Ëá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ±Âª∫Á´ãË¶èÂâáÂü∫Á§éÔºå‰ΩÜÁõ¥Êé•Êì∑Âèñ CPG ‰∏≠ÂåÖÂê´ÁöÑÂÆåÊï¥ÈÜ´ÁôÇÁü•Ë≠òÁöÑÂ∑•‰ΩúÂçªÊúâÈôê„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºå‰ª•Ëá™ÂãïÂåñÊì∑ÂèñÂíåÁØÄÈªûËàáÈóú‰øÇÂàÜÈ°ûÁöÑÊñπÂºèÔºåÂª∫Á´ãËÑàÁµ°Ë±êÂØå„ÄÅÂø†ÂØ¶ÁöÑÂúãÂÆ∂Á∂úÂêàÁôåÁóáÁ∂≤Ë∑Ø (NCCN) ÁôåÁóá CPG ÂúñÂΩ¢Êï∏‰ΩçË°®Á§∫„ÄÇÊàëÂÄë‰πüÈÄèÈÅé‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åÁØÄÈªûÂàÜÈ°ûÔºåÂØ¶‰ΩúÊ®°ÂûãÁöÑË™ûÊÑèË±êÂØåÂåñÔºåÂàÜÂà•Âú®Èõ∂Ê¨°Â≠∏ÁøíÂíåÂ∞ëÊ¨°Â≠∏Áøí‰∏≠ÈÅîÂà∞ 80.86% Âíå 88.47% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÈÄèÈÅéÈÅãÁî® LLM ÂæûÊåáÂçóÁü•Ë≠òÂ∫´‰∏≠Êì∑ÂèñÁõ∏ÈóúÂ≠êÂúñÔºå‰æÜÂõûÁ≠îÂÖ∑ÊúâÊåáÂçóÊñáÂ≠óÈôêÂà∂ÁöÑËá™ÁÑ∂Ë™ûË®ÄÂïèÈ°å„ÄÇÈÄèÈÅéÊ†πÊìöÂ≠êÂúñË∑ØÂæëÂíåË™ûÊÑèË≥áË®äÁî¢ÁîüËá™ÁÑ∂Ë™ûË®ÄÁ≠îÊ°àÔºåÊàëÂÄëÈôç‰Ωé‰∫ÜËàá LLM Áõ∏ÈóúÁöÑÈåØË™§Á≠îÊ°àÂíåÂπªË¶∫È¢®Èö™ÔºåÁ¢∫‰øù‰∫ÜÈÜ´ÁôÇÈ†òÂüüÂïèÈ°åËß£Á≠î‰∏≠ÁöÑ‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄß„ÄÇ

##### **A review on development of eco-friendly filters in Nepal for use in cigarettes and masks and Air Pollution Analysis with Machine Learning and SHAP Interpretability**
2501.13369v1 by Bishwash Paneru, Biplov Paneru, Tanka Mukhiya, Khem Narayan Poudyal

In Nepal, air pollution is a serious public health concern, especially in
cities like Kathmandu where particulate matter (PM2.5 and PM10) has a major
influence on respiratory health and air quality. The Air Quality Index (AQI) is
predicted in this work using a Random Forest Regressor, and the model's
predictions are interpreted using SHAP (SHapley Additive exPlanations)
analysis. With the lowest Testing RMSE (0.23) and flawless R2 scores (1.00),
CatBoost performs better than other models, demonstrating its greater accuracy
and generalization which is cross validated using a nested cross validation
approach. NowCast Concentration and Raw Concentration are the most important
elements influencing AQI values, according to SHAP research, which shows that
the machine learning results are highly accurate. Their significance as major
contributors to air pollution is highlighted by the fact that high values of
these characteristics significantly raise the AQI. This study investigates the
Hydrogen-Alpha (HA) biodegradable filter as a novel way to reduce the related
health hazards. With removal efficiency of more than 98% for PM2.5 and 99.24%
for PM10, the HA filter offers exceptional defense against dangerous airborne
particles. These devices, which are biodegradable face masks and cigarette
filters, address the environmental issues associated with traditional filters'
non-biodegradable trash while also lowering exposure to air contaminants.

ÊëòË¶ÅÔºöÂú®Â∞ºÊ≥äÁàæÔºåÁ©∫Ê∞£Ê±°ÊüìÊòØ‰∏ÄÂÄãÂö¥ÈáçÁöÑÂÖ¨ÂÖ±Ë°õÁîüÂïèÈ°åÔºåÁâπÂà•ÊòØÂú®Âä†Âæ∑ÊªøÈÉΩÁ≠âÂüéÂ∏ÇÔºåÈÇ£Ë£°ÁöÑÊá∏ÊµÆÂæÆÁ≤íÔºàPM2.5 Âíå PM10ÔºâÂ∞çÂëºÂê∏Á≥ªÁµ±ÂÅ•Â∫∑ÂíåÁ©∫Ê∞£ÂìÅË≥™ÊúâÈáçÂ§ßÂΩ±Èüø„ÄÇÈÄôÈ†ÖÂ∑•‰Ωú‰ΩøÁî®Èö®Ê©üÊ£ÆÊûóÂõûÊ≠∏Âô®È†êÊ∏¨Á©∫Ê∞£ÂìÅË≥™ÊåáÊï∏ (AQI)Ôºå‰∏¶‰ΩøÁî® SHAPÔºàSHapley Âä†Ê≥ïËß£ÈáãÔºâÂàÜÊûê‰æÜËß£ÈáãÊ®°ÂûãÁöÑÈ†êÊ∏¨„ÄÇCatBoost ÁöÑÊ∏¨Ë©¶ RMSE ÊúÄ‰ΩéÔºà0.23ÔºâÔºåR2 ÂàÜÊï∏ÂÆåÁæéÔºà1.00ÔºâÔºåË°®ÁèæÂÑ™ÊñºÂÖ∂‰ªñÊ®°ÂûãÔºåË≠âÊòéÂÖ∂ÂÖ∑ÊúâÊõ¥È´òÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊ≥õÂåñÊÄßÔºå‰∏¶‰ΩøÁî®ÂµåÂ•ó‰∫§ÂèâÈ©óË≠âÊñπÊ≥ïÈÄ≤Ë°å‰∫§ÂèâÈ©óË≠â„ÄÇÊ†πÊìö SHAP Á†îÁ©∂ÔºåÁèæÂú®ÊøÉÂ∫¶ÂíåÂéüÂßãÊøÉÂ∫¶ÊòØÂΩ±Èüø AQI ÂÄºÊúÄÈáçË¶ÅÁöÑÂÖÉÁ¥†ÔºåÈÄôË°®ÊòéÊ©üÂô®Â≠∏ÁøíÁµêÊûúÈùûÂ∏∏Ê∫ñÁ¢∫„ÄÇÂÆÉÂÄë‰ΩúÁÇ∫Á©∫Ê∞£Ê±°ÊüìÁöÑ‰∏ªË¶ÅË≤¢ÁçªËÄÖÁöÑÈáçË¶ÅÊÄßÂú®ÊñºÔºåÈÄô‰∫õÁâπÂæµÁöÑÈ´òÂÄºÊúÉÈ°ØËëóÊèêÈ´ò AQI„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÊ∞´-Œ±ÔºàHAÔºâÂèØÁîüÁâ©ÈôçËß£ÈÅéÊøæÂô®‰ΩúÁÇ∫Ê∏õÂ∞ëÁõ∏ÈóúÂÅ•Â∫∑Âç±ÂÆ≥ÁöÑ‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ï„ÄÇHA ÈÅéÊøæÂô®Â∞ç PM2.5 ÁöÑÂéªÈô§ÊïàÁéáË∂ÖÈÅé 98%ÔºåÂ∞ç PM10 ÁöÑÂéªÈô§ÊïàÁéáË∂ÖÈÅé 99.24%ÔºåÂèØÊèê‰æõÈò≤ÁØÑÂç±Èö™Á©∫Ê∞£Êá∏ÊµÆÂæÆÁ≤íÁöÑÂá∫Ëâ≤Èò≤Ë≠∑„ÄÇÈÄô‰∫õÂèØÁîüÁâ©ÈôçËß£Âè£ÁΩ©ÂíåÈ¶ôËè∏ÈÅéÊøæÂô®ÁöÑË£ùÁΩÆËß£Ê±∫‰∫ÜÂÇ≥Áµ±ÈÅéÊøæÂô®‰∏çÂèØÁîüÁâ©ÈôçËß£ÂûÉÂúæÁõ∏ÈóúÁöÑÁí∞Â¢ÉÂïèÈ°åÔºåÂêåÊôÇ‰πüÈôç‰Ωé‰∫ÜÊé•Ëß∏Á©∫Ê∞£Ê±°ÊüìÁâ©ÁöÑÈ¢®Èö™„ÄÇ

##### **QuFeX: Quantum feature extraction module for hybrid quantum-classical deep neural networks**
2501.13165v1 by Naman Jain, Amir Kalev

We introduce Quantum Feature Extraction (QuFeX), a novel quantum machine
learning module. The proposed module enables feature extraction in a
reduced-dimensional space, significantly decreasing the number of parallel
evaluations required in typical quantum convolutional neural network
architectures. Its design allows seamless integration into deep classical
neural networks, making it particularly suitable for hybrid quantum-classical
models. As an application of QuFeX, we propose Qu-Net -- a hybrid architecture
which integrates QuFeX at the bottleneck of a U-Net architecture. The latter is
widely used for image segmentation tasks such as medical imaging and autonomous
driving. Our numerical analysis indicates that the Qu-Net can achieve superior
segmentation performance compared to a U-Net baseline. These results highlight
the potential of QuFeX to enhance deep neural networks by leveraging hybrid
computational paradigms, providing a path towards a robust framework for
real-world applications requiring precise feature extraction.

ÊëòË¶ÅÔºöÊàëÂÄëÂºïÂÖ•‰∫ÜÈáèÂ≠êÁâπÂæµËêÉÂèñ (QuFeX)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂâµÊñ∞ÁöÑÈáèÂ≠êÊ©üÂô®Â≠∏ÁøíÊ®°ÁµÑ„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÁµÑÂèØ‰ª•Âú®ÈôçÁ∂≠Á©∫Èñì‰∏≠ÈÄ≤Ë°åÁâπÂæµËêÉÂèñÔºåÂ§ßÂπÖÊ∏õÂ∞ëÂÖ∏ÂûãÈáèÂ≠êÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊû∂Êßã‰∏≠ÊâÄÈúÄÁöÑ‰∏¶Ë°åË©ï‰º∞Êï∏Èáè„ÄÇÂÖ∂Ë®≠Ë®àÂÖÅË®±ÁÑ°Á∏´Êï¥ÂêàÂà∞Ê∑±Â∫¶Âè§ÂÖ∏Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠Ôºå‰ΩøÂÖ∂ÁâπÂà•ÈÅ©ÂêàÊñºÊ∑∑ÂêàÈáèÂ≠êÂè§ÂÖ∏Ê®°Âûã„ÄÇ‰ΩúÁÇ∫ QuFeX ÁöÑÊáâÁî®ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü Qu-NetÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊ∑∑ÂêàÊû∂ÊßãÔºåÂÆÉÂú® U-Net Êû∂ÊßãÁöÑÁì∂È†∏ËôïÊï¥Âêà‰∫Ü QuFeX„ÄÇÂæåËÄÖÂª£Ê≥õÁî®ÊñºÂΩ±ÂÉèÂàÜÂâ≤‰ªªÂãôÔºå‰æãÂ¶ÇÈÜ´Â≠∏ÂΩ±ÂÉèÂíåËá™ÂãïÈßïÈßõ„ÄÇÊàëÂÄëÁöÑÊï∏ÂÄºÂàÜÊûêË°®ÊòéÔºåËàá U-Net Âü∫Ê∫ñÁõ∏ÊØîÔºåQu-Net ÂèØ‰ª•ÂØ¶ÁèæÂÑ™Áï∞ÁöÑÂàÜÂâ≤ÊïàËÉΩ„ÄÇÈÄô‰∫õÁµêÊûúÁ™ÅÈ°Ø‰∫Ü QuFeX ÈÄèÈÅéÂà©Áî®Ê∑∑ÂêàÈÅãÁÆóÁØÑ‰æã‰æÜÂ¢ûÂº∑Ê∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊΩõÂäõÔºåÁÇ∫ÈúÄË¶ÅÁ≤æÁ¢∫ÁâπÂæµËêÉÂèñÁöÑÁúüÂØ¶‰∏ñÁïåÊáâÁî®Á®ãÂºèÊèê‰æõ‰∫Ü‰∏ÄÂÄãÈÇÅÂêëÁ©©ÂÅ•Êû∂ÊßãÁöÑÈÄîÂæë„ÄÇ

##### **AirRadar: Inferring Nationwide Air Quality in China with Deep Neural Networks**
2501.13141v1 by Qiongyan Wang, Yutong Xia, Siru ZHong, Weichuang Li, Yuankai Wu, Shifen Cheng, Junbo Zhang, Yu Zheng, Yuxuan Liang

Monitoring real-time air quality is essential for safeguarding public health
and fostering social progress. However, the widespread deployment of air
quality monitoring stations is constrained by their significant costs. To
address this limitation, we introduce \emph{AirRadar}, a deep neural network
designed to accurately infer real-time air quality in locations lacking
monitoring stations by utilizing data from existing ones. By leveraging
learnable mask tokens, AirRadar reconstructs air quality features in
unmonitored regions. Specifically, it operates in two stages: first capturing
spatial correlations and then adjusting for distribution shifts. We validate
AirRadar's efficacy using a year-long dataset from 1,085 monitoring stations
across China, demonstrating its superiority over multiple baselines, even with
varying degrees of unobserved data. The source code can be accessed at
https://github.com/CityMind-Lab/AirRadar.

ÊëòË¶ÅÔºöÁõ£ÊéßÂç≥ÊôÇÁ©∫Ê∞£ÂìÅË≥™Â∞çÊñº‰øùÈöúÂÖ¨ÂÖ±ÂÅ•Â∫∑Âíå‰øÉÈÄ≤Á§æÊúÉÈÄ≤Ê≠•Ëá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁ©∫Ê∞£ÂìÅË≥™Áõ£Ê∏¨Á´ôÁöÑÂª£Ê≥õÈÉ®ÁΩ≤ÂèóÂà∞ÂÖ∂È´òÊòÇÊàêÊú¨ÁöÑÈôêÂà∂„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü \emph{AirRadar}ÔºåÈÄôÊòØ‰∏ÄÂÄãÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÔºåÊó®Âú®Âà©Áî®ÁèæÊúâÁõ£Ê∏¨Á´ôÁöÑË≥áÊñôÔºåÁ≤æÊ∫ñÊé®Ë´ñÊ≤íÊúâÁõ£Ê∏¨Á´ôÁöÑÂú∞ÂçÄÁöÑÂç≥ÊôÇÁ©∫Ê∞£ÂìÅË≥™„ÄÇÈÄèÈÅéÂà©Áî®ÂèØÂ≠∏ÁøíÁöÑÈÅÆÁΩ©Á¨¶ËôüÔºåAirRadar ÈáçÂª∫Êú™Áõ£ÊéßÂçÄÂüüÁöÑÁ©∫Ê∞£ÂìÅË≥™ÁâπÂæµ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂÆÉÂàÜÂÖ©ÂÄãÈöéÊÆµÈÅã‰ΩúÔºöÈ¶ñÂÖàÊì∑ÂèñÁ©∫ÈñìÈóúËÅØÊÄßÔºåÁÑ∂ÂæåË™øÊï¥ÂàÜ‰ΩàËΩâÁßª„ÄÇÊàëÂÄë‰ΩøÁî®‰æÜËá™‰∏≠Âúã 1,085 ÂÄãÁõ£Ê∏¨Á´ô‰∏ÄÊï¥Âπ¥ÁöÑË≥áÊñôÈõÜÈ©óË≠â‰∫Ü AirRadar ÁöÑÂäüÊïàÔºåË≠âÊòé‰∫ÜÂÆÉÂÑ™ÊñºÂ§öÂÄãÂü∫Ê∫ñÔºåÂç≥‰ΩøÂú®‰∏çÂêåÁ®ãÂ∫¶ÁöÑÊú™ËßÄÂØüË≥áÊñô‰∏≠‰πüÊòØÂ¶ÇÊ≠§„ÄÇÂèØ‰ª•Âú® https://github.com/CityMind-Lab/AirRadar ÂèñÂæóÂéüÂßãÁ®ãÂºèÁ¢º„ÄÇ

##### **Estimating the Conformal Prediction Threshold from Noisy Labels**
2501.12749v1 by Coby Penso, Jacob Goldberger, Ethan Fetaya

Conformal Prediction (CP) is a method to control prediction uncertainty by
producing a small prediction set, ensuring a predetermined probability that the
true class lies within this set. This is commonly done by defining a score,
based on the model predictions, and setting a threshold on this score using a
validation set. In this study, we address the problem of CP calibration when we
only have access to a validation set with noisy labels. We show how we can
estimate the noise-free conformal threshold based on the noisy labeled data.
Our solution is flexible and can accommodate various modeling assumptions
regarding the label contamination process, without needing any information
about the underlying data distribution or the internal mechanisms of the
machine learning classifier. We develop a coverage guarantee for uniform noise
that is effective even in tasks with a large number of classes. We dub our
approach Noise-Aware Conformal Prediction (NACP) and show on several natural
and medical image classification datasets, including ImageNet, that it
significantly outperforms current noisy label methods and achieves results
comparable to those obtained with a clean validation set.

ÊëòË¶ÅÔºöÂÖ±ÂΩ¢È¢ÑÊµã (CP) ÊòØ‰∏ÄÁ®ÆÈÄèÈÅéÁî¢Áîü‰∏ÄÂÄãÂ∞èÂûãÈ†êÊ∏¨ÈõÜÂêà‰æÜÊéßÂà∂È†êÊ∏¨‰∏çÁ¢∫ÂÆöÊÄßÁöÑÊñπÊ≥ïÔºåÁ¢∫‰øùÁúüÊ≠£ÁöÑÈ°ûÂà•ËêΩÂú®ÈÄôÂÄãÈõÜÂêàÂÖßÁöÑÈ†êÂÖàÁ¢∫ÂÆöÁöÑÊ©üÁéá„ÄÇÈÄôÈÄöÂ∏∏ÊòØÈÄèÈÅéÂÆöÁæ©‰∏ÄÂÄãÂü∫ÊñºÊ®°ÂûãÈ†êÊ∏¨ÁöÑÂàÜÊï∏‰æÜÂÆåÊàêÔºå‰∏¶‰ΩøÁî®È©óË≠âÈõÜÂêàÂ∞çÈÄôÂÄãÂàÜÊï∏Ë®≠ÂÆö‰∏ÄÂÄãÈñæÂÄº„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÁï∂ÊàëÂÄëÂè™ËÉΩÂ≠òÂèñÂÖ∑ÊúâÈõúË®äÊ®ôÁ±§ÁöÑÈ©óË≠âÈõÜÂêàÊôÇÔºåCP Ê†°Ê≠£ÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÊ†πÊìöÈõúË®äÊ®ôÁ±§Ë≥áÊñô‰º∞Ë®àÁÑ°ÈõúË®äÁöÑÂÖ±ÂΩ¢ÈñæÂÄº„ÄÇÊàëÂÄëÁöÑËß£Ê±∫ÊñπÊ°àÂÖ∑ÊúâÂΩàÊÄßÔºå‰∏¶‰∏îÂèØ‰ª•ÈÅ©ÊáâÈóúÊñºÊ®ôÁ±§Ê±°ÊüìÈÅéÁ®ãÁöÑÂêÑÁ®ÆÂª∫Ê®°ÂÅáË®≠ÔºåËÄå‰∏çÈúÄË¶Å‰ªª‰ΩïÈóúÊñºÂ∫ïÂ±§Ë≥áÊñôÂàÜ‰ΩàÊàñÊ©üÂô®Â≠∏ÁøíÂàÜÈ°ûÂô®ÂÖßÈÉ®Ê©üÂà∂ÁöÑË≥áË®ä„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÂ∞çÊñºÂùáÂãªÈõúË®äÁöÑË¶ÜËìã‰øùË≠âÔºåÂç≥‰ΩøÂú®ÂÖ∑ÊúâÂ§ßÈáèÈ°ûÂà•ÁöÑ‰ªªÂãô‰∏≠‰πüÂæàÊúâÊïà„ÄÇÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÂÅöÊ≥ïÁ®±ÁÇ∫ÈõúË®äÊÑüÁü•ÂÖ±ÂΩ¢È†êÊ∏¨ (NACP)Ôºå‰∏¶Âú®ÂπæÂÄãËá™ÁÑ∂ÂíåÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûË≥áÊñôÈõÜÔºàÂåÖÊã¨ ImageNetÔºâ‰∏äÂ±ïÁ§∫‰∫ÜÂÆÉÈ°ØËëóÂÑ™ÊñºÁõÆÂâçÁöÑÈõúË®äÊ®ôÁ±§ÊñπÊ≥ïÔºå‰∏¶‰∏îÈÅîÂà∞‰∫ÜËàá‰ΩøÁî®‰πæÊ∑®È©óË≠âÈõÜÂêàÁç≤ÂæóÁöÑÁµêÊûúÁõ∏Áï∂ÁöÑÁµêÊûú„ÄÇ

##### **Applications and Challenges of AI and Microscopy in Life Science Research: A Review**
2501.13135v1 by Himanshu Buckchash, Gyanendra Kumar Verma, Dilip K. Prasad

The complexity of human biology and its intricate systems holds immense
potential for advancing human health, disease treatment, and scientific
discovery. However, traditional manual methods for studying biological
interactions are often constrained by the sheer volume and complexity of
biological data. Artificial Intelligence (AI), with its proven ability to
analyze vast datasets, offers a transformative approach to addressing these
challenges. This paper explores the intersection of AI and microscopy in life
sciences, emphasizing their potential applications and associated challenges.
We provide a detailed review of how various biological systems can benefit from
AI, highlighting the types of data and labeling requirements unique to this
domain. Particular attention is given to microscopy data, exploring the
specific AI techniques required to process and interpret this information. By
addressing challenges such as data heterogeneity and annotation scarcity, we
outline potential solutions and emerging trends in the field. Written primarily
from an AI perspective, this paper aims to serve as a valuable resource for
researchers working at the intersection of AI, microscopy, and biology. It
summarizes current advancements, key insights, and open problems, fostering an
understanding that encourages interdisciplinary collaborations. By offering a
comprehensive yet concise synthesis of the field, this paper aspires to
catalyze innovation, promote cross-disciplinary engagement, and accelerate the
adoption of AI in life science research.

ÊëòË¶ÅÔºö‰∫∫È°ûÁîüÁâ©Â≠∏ÂèäÂÖ∂Ë§áÈõúÁ≥ªÁµ±ÁöÑË§áÈõúÊÄßËòäËóèËëó‰øÉÈÄ≤‰∫∫È°ûÂÅ•Â∫∑„ÄÅÁñæÁóÖÊ≤ªÁôÇÂíåÁßëÂ≠∏ÁôºÁèæÁöÑÂ∑®Â§ßÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑ‰∫∫Â∑•ÁîüÁâ©‰∫§‰∫íÁ†îÁ©∂ÊñπÊ≥ïÈÄöÂ∏∏ÂèóÂà∞ÁîüÁâ©Êï∏ÊìöÈæêÂ§ßÁöÑÊï∏ÈáèÂíåË§áÈõúÊÄßÁöÑÈôêÂà∂„ÄÇ‰∫∫Â∑•Êô∫ÊÖß (AI) Â∑≤Ë¢´Ë≠âÂØ¶ÂÖ∑ÊúâÂàÜÊûêÈæêÂ§ßÊï∏ÊìöÈõÜÁöÑËÉΩÂäõÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆËÆäÈù©ÊÄßÁöÑÊñπÊ≥ï‰æÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫Ü AI ÂíåÈ°ØÂæÆÈè°Âú®ÁîüÂëΩÁßëÂ≠∏‰∏≠ÁöÑ‰∫§ÈõÜÔºåÂº∑Ë™ø‰∫ÜÂÆÉÂÄëÁöÑÊΩõÂú®ÊáâÁî®ÂíåÁõ∏ÈóúÊåëÊà∞„ÄÇÊàëÂÄëË©≥Á¥∞ÂõûÈ°ß‰∫ÜÂêÑÁ®ÆÁîüÁâ©Á≥ªÁµ±Â¶Ç‰ΩïÂæû AI ‰∏≠ÂèóÁõäÔºåÈáçÈªû‰ªãÁ¥π‰∫ÜÊ≠§È†òÂüüÁç®ÊúâÁöÑÊï∏ÊìöÈ°ûÂûãÂíåÊ®ôË®òË¶ÅÊ±Ç„ÄÇÁâπÂà•ÈóúÊ≥®È°ØÂæÆÈè°Êï∏ÊìöÔºåÊé¢Ë®éËôïÁêÜÂíåËß£ÈáãÊ≠§‰ø°ÊÅØÁöÑÁâπÂÆö AI ÊäÄË°ì„ÄÇÈÄöÈÅéÊáâÂ∞çÊï∏ÊìöÁï∞Ë≥™ÊÄßÂíåË®ªÈáãÁ®ÄÁº∫ÊÄßÁ≠âÊåëÊà∞ÔºåÊàëÂÄëÊ¶ÇËø∞‰∫ÜË©≤È†òÂüüÁöÑÊΩõÂú®Ëß£Ê±∫ÊñπÊ°àÂíåÊñ∞Ë∂®Âã¢„ÄÇÊú¨Êñá‰∏ªË¶ÅÂæû AI ÁöÑËßíÂ∫¶Êí∞ÂØ´ÔºåÊó®Âú®ÁÇ∫Âú® AI„ÄÅÈ°ØÂæÆÈè°ÂíåÁîüÁâ©Â≠∏‰∫§ÂèâÈ†òÂüüÂ∑•‰ΩúÁöÑÁ†îÁ©∂‰∫∫Âì°Êèê‰æõÂØ∂Ë≤¥ÁöÑË≥áÊ∫ê„ÄÇÂÆÉÁ∏ΩÁµê‰∫ÜÁï∂ÂâçÁöÑÈÄ≤Â±ï„ÄÅÈóúÈçµË¶ãËß£ÂíåÊú™Ëß£Ê±∫ÁöÑÂïèÈ°åÔºåÂüπÈ§ä‰∫ÜÈºìÂãµË∑®Â≠∏ÁßëÂêà‰ΩúÁöÑÁêÜËß£„ÄÇÈÄöÈÅéÊèê‰æõË©≤È†òÂüüÂÖ®Èù¢ËÄåÁ∞°ÊΩîÁöÑÁ∂úÂêàÔºåÊú¨ÊñáÊó®Âú®ÂÇ¨ÂåñÂâµÊñ∞„ÄÅ‰øÉÈÄ≤Ë∑®Â≠∏ÁßëÂèÉËàáÔºå‰∏¶Âä†ÈÄü AI Âú®ÁîüÂëΩÁßëÂ≠∏Á†îÁ©∂‰∏≠ÁöÑÊé°Áî®„ÄÇ

##### **FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis**
2501.13967v1 by Haoxuan Che, Yifei Wu, Haibo Jin, Yong Xia, Hao Chen

Federated domain generalization aims to train a global model from multiple
source domains and ensure its generalization ability to unseen target domains.
{Due to the target domain being with unknown domain shifts, attempting to
approximate these gaps by source domains may be the key to improving model
generalization capability.} Existing works mainly focus on sharing and
recombining local domain-specific attributes to increase data diversity and
simulate potential domain shifts. {However, these methods may be insufficient
since only the local attribute recombination can be hard to touch the
out-of-distribution of global data.} In this paper, we propose a
simple-yet-efficient framework named Federated Domain Adversarial Generation
(FedDAG). {It aims to simulate the domain shift and improve the model
generalization by adversarially generating novel domains different from local
and global source domains.} Specifically, it generates novel-style images by
maximizing the instance-level feature discrepancy between original and
generated images and trains a generalizable task model by minimizing their
feature discrepancy. {Further, we observed that FedDAG could cause different
performance improvements for local models. It may be due to inherent data
isolation and heterogeneity among clients, exacerbating the imbalance in their
generalization contributions to the global model.} {Ignoring this imbalance can
lead the global model's generalization ability to be sub-optimal, further
limiting the novel domain generation procedure. } Thus, to mitigate this
imbalance, FedDAG hierarchically aggregates local models at the within-client
and across-client levels by using the sharpness concept to evaluate client
model generalization contributions. {Extensive experiments across four medical
benchmarks demonstrate FedDAG's ability to enhance generalization in federated
medical scenarios.}

ÊëòË¶ÅÔºö<paragraph>ËÅØÈÇ¶È†òÂüüÊ≥õÂåñÊó®Âú®ÂæûÂ§öÂÄã‰æÜÊ∫êÈ†òÂüüË®ìÁ∑¥‰∏ÄÂÄãÂÖ®Â±ÄÊ®°ÂûãÔºå‰∏¶Á¢∫‰øùÂÖ∂Â∞çÊú™Ë¶ãÁõÆÊ®ôÈ†òÂüüÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ
{Áî±ÊñºÁõÆÊ®ôÈ†òÂüüÂ≠òÂú®Êú™Áü•ÁöÑÈ†òÂüüËΩâÁßªÔºåÂòóË©¶ÈÄöÈÅé‰æÜÊ∫êÈ†òÂüü‰æÜËøë‰ººÈÄô‰∫õÂ∑ÆË∑ùÂèØËÉΩÊòØÊèêÈ´òÊ®°ÂûãÊ≥õÂåñËÉΩÂäõÁöÑÈóúÈçµ„ÄÇ} ÁèæÊúâÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂÖ±‰∫´ÂíåÈáçÊñ∞ÁµÑÂêàÂ±ÄÈÉ®È†òÂüüÁâπÂÆöÂ±¨ÊÄßÔºå‰ª•Â¢ûÂä†Êï∏ÊìöÂ§öÊ®£ÊÄßÂíåÊ®°Êì¨ÊΩõÂú®ÁöÑÈ†òÂüüËΩâÁßª„ÄÇ {ÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂèØËÉΩ‰∏çË∂≥ÔºåÂõ†ÁÇ∫Âè™ÊúâÂ±ÄÈÉ®Â±¨ÊÄßÈáçÁµÑÈõ£‰ª•Ëß∏ÂèäÂÖ®Â±ÄÊï∏ÊìöÁöÑÂàÜÂ∏ÉÂ§ñ„ÄÇ} Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆËÄåÈ´òÊïàÁöÑÊ°ÜÊû∂ÔºåÂêçÁÇ∫ËÅØÈÇ¶È†òÂüüÂ∞çÊäóÁîüÊàêÔºàFedDAGÔºâ„ÄÇ {ÂÆÉÊó®Âú®Ê®°Êì¨È†òÂüüËΩâÁßªÔºå‰∏¶ÈÄöÈÅéÂ∞çÊäóÁîüÊàê‰∏çÂêåÊñºÂ±ÄÈÉ®ÂíåÂÖ®Â±Ä‰æÜÊ∫êÈ†òÂüüÁöÑÊñ∞Á©éÈ†òÂüü‰æÜÊèêÈ´òÊ®°ÂûãÊ≥õÂåñ„ÄÇ} ÂÖ∑È´î‰æÜË™™ÔºåÂÆÉÈÄöÈÅéÊúÄÂ§ßÂåñÂéüÂßãÂúñÂÉèÂíåÁîüÊàêÂúñÂÉè‰πãÈñìÁöÑÂØ¶‰æãÁ¥öÂà•ÁâπÂæµÂ∑ÆÁï∞‰æÜÁîüÊàêÊñ∞Ê®£ÂºèÁöÑÂúñÂÉèÔºå‰∏¶ÈÄöÈÅéÊúÄÂ∞èÂåñÂÆÉÂÄëÁöÑÁâπÂæµÂ∑ÆÁï∞‰æÜË®ìÁ∑¥‰∏ÄÂÄãÂèØÊ≥õÂåñÁöÑ‰ªªÂãôÊ®°Âûã„ÄÇ {Ê≠§Â§ñÔºåÊàëÂÄëËßÄÂØüÂà∞ FedDAG ÂèØ‰ª•Â∞çÂ±ÄÈÉ®Ê®°ÂûãÈÄ†Êàê‰∏çÂêåÁöÑÊÄßËÉΩÊèêÂçá„ÄÇÈÄôÂèØËÉΩÊòØÁî±ÊñºÂÆ¢Êà∂Á´Ø‰πãÈñìÂõ∫ÊúâÁöÑÊï∏ÊìöÈöîÈõ¢ÂíåÁï∞Ë≥™ÊÄßÔºåÂä†Âäá‰∫ÜÂÆÉÂÄëÂ∞çÂÖ®Â±ÄÊ®°ÂûãÊ≥õÂåñË≤¢ÁçªÁöÑ‰∏çÂπ≥Ë°°„ÄÇ} {ÂøΩÁï•ÈÄôÁ®Æ‰∏çÂπ≥Ë°°ÊúÉÂ∞éËá¥ÂÖ®Â±ÄÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÊ¨°ÂÑ™ÔºåÈÄ≤‰∏ÄÊ≠•ÈôêÂà∂Êñ∞Á©éÈ†òÂüüÁîüÊàêÈÅéÁ®ã„ÄÇ} Âõ†Ê≠§ÔºåÁÇ∫‰∫ÜÊ∏õËºïÈÄôÁ®Æ‰∏çÂπ≥Ë°°ÔºåFedDAG ‰ΩøÁî®Ê∏ÖÊô∞Â∫¶Ê¶ÇÂøµ‰æÜË©ï‰º∞ÂÆ¢Êà∂Á´ØÊ®°ÂûãÊ≥õÂåñË≤¢ÁçªÔºåÂú®ÂÆ¢Êà∂Á´ØÂÖßÂíåÂÆ¢Êà∂Á´Ø‰πãÈñìÂàÜÂ±§ËÅöÂêàÂ±ÄÈÉ®Ê®°Âûã„ÄÇ {Âú®ÂõõÂÄãÈÜ´ÁôÇÂü∫Ê∫ñ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫Ü FedDAG Â¢ûÂº∑ËÅØÈÇ¶ÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠Ê≥õÂåñÁöÑËÉΩÂäõ„ÄÇ}</paragraph>

##### **Academic Case Reports Lack Diversity: Assessing the Presence and Diversity of Sociodemographic and Behavioral Factors related to Post COVID-19 Condition**
2501.12538v2 by Juan Andres Medina Florez, Shaina Raza, Rashida Lynn, Zahra Shakeri, Brendan T. Smith, Elham Dolatabadi

Understanding the prevalence, disparities, and symptom variations of Post
COVID-19 Condition (PCC) for vulnerable populations is crucial to improving
care and addressing intersecting inequities. This study aims to develop a
comprehensive framework for integrating social determinants of health (SDOH)
into PCC research by leveraging NLP techniques to analyze disparities and
variations in SDOH representation within PCC case reports. Following
construction of a PCC Case Report Corpus, comprising over 7,000 case reports
from the LitCOVID repository, a subset of 709 reports were annotated with 26
core SDOH-related entity types using pre-trained named entity recognition (NER)
models, human review, and data augmentation to improve quality, diversity and
representation of entity types. An NLP pipeline integrating NER, natural
language inference (NLI), trigram and frequency analyses was developed to
extract and analyze these entities. Both encoder-only transformer models and
RNN-based models were assessed for the NER objective.
  Fine-tuned encoder-only BERT models outperformed traditional RNN-based models
in generalizability to distinct sentence structures and greater class sparsity.
Exploratory analysis revealed variability in entity richness, with prevalent
entities like condition, age, and access to care, and underrepresentation of
sensitive categories like race and housing status. Trigram analysis highlighted
frequent co-occurrences among entities, including age, gender, and condition.
The NLI objective (entailment and contradiction analysis) showed attributes
like "Experienced violence or abuse" and "Has medical insurance" had high
entailment rates (82.4%-80.3%), while attributes such as "Is
female-identifying," "Is married," and "Has a terminal condition" exhibited
high contradiction rates (70.8%-98.5%).

ÊëòË¶ÅÔºö‰∫ÜËß£ËÑÜÂº±‰∫∫Áæ§ÁöÑ COVID-19 ÂæåÈÅ∫Áóá (PCC) ÁöÑÊµÅË°åÁãÄÊ≥Å„ÄÅÂ∑ÆÁï∞ÂíåÁóáÁãÄËÆäÂåñÂ∞çÊñºÊîπÂñÑÁÖßË≠∑ÂíåËß£Ê±∫‰∫§ÁπîÁöÑ‰∏çÂπ≥Á≠âËá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ÈÄèÈÅéÂà©Áî®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊäÄË°ìÂàÜÊûê PCC ÁóÖ‰æãÂ†±Âëä‰∏≠ SDOH ÁöÑ‰ª£Ë°®ÊÄßÂ∑ÆÁï∞ÂíåËÆäÂåñÔºåÁÇ∫Â∞áÁ§æÊúÉÂÅ•Â∫∑Ê±∫ÂÆöÂõ†Á¥† (SDOH) Êï¥ÂêàÂà∞ PCC Á†îÁ©∂‰∏≠Âª∫Á´ã‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊû∂Êßã„ÄÇÂú®Âª∫ÊßãÂåÖÂê´‰æÜËá™ LitCOVID ÂÑ≤Â≠òÂ∫´ÁöÑ 7,000 Â§ö‰ªΩÁóÖ‰æãÂ†±ÂëäÁöÑ PCC ÁóÖ‰æãÂ†±ÂëäË™ûÊñôÂ∫´ÂæåÔºå‰ΩøÁî®È†êÂÖàË®ìÁ∑¥ÁöÑÂêçÁ®±ÂØ¶È´îË≠òÂà• (NER) Ê®°Âûã„ÄÅ‰∫∫Â∑•ÂØ©Êü•ÂíåË≥áÊñôÊì¥ÂÖÖÂ∞ç 709 ‰ªΩÂ†±ÂëäÁöÑ 26 ÂÄãÊ†∏ÂøÉ SDOH Áõ∏ÈóúÂØ¶È´îÈ°ûÂûãÈÄ≤Ë°åË®ªËß£Ôºå‰ª•ÊèêÈ´òÂØ¶È´îÈ°ûÂûãÁöÑÂìÅË≥™„ÄÅÂ§öÊ®£ÊÄßÂíå‰ª£Ë°®ÊÄß„ÄÇÈñãÁôº‰∫Ü‰∏ÄÂÄãÊï¥Âêà NER„ÄÅËá™ÁÑ∂Ë™ûË®ÄÊé®ÁêÜ (NLI)„ÄÅ‰∏âÂÖÉÁµÑÂíåÈ†ªÁéáÂàÜÊûêÁöÑ NLP ÁÆ°Á∑ö‰æÜËêÉÂèñÂíåÂàÜÊûêÈÄô‰∫õÂØ¶È´î„ÄÇË©ï‰º∞‰∫ÜÂÉÖÁ∑®Á¢ºÂô®ËΩâÊèõÂô®Ê®°ÂûãÂíåÂü∫Êñº RNN ÁöÑÊ®°ÂûãÁöÑ NER ÁõÆÊ®ô„ÄÇÁ∂ìÈÅéÂæÆË™øÁöÑÂÉÖÁ∑®Á¢ºÂô® BERT Ê®°ÂûãÂú®Â∞ç‰∏çÂêåÂè•Â≠êÁµêÊßãÂíåÊõ¥Â§ßÁöÑÈ°ûÂà•Á®ÄÁñèÊÄßÁöÑÊ¶ÇÊã¨ÊÄßÊñπÈù¢ÂÑ™ÊñºÂÇ≥Áµ±ÁöÑÂü∫Êñº RNN ÁöÑÊ®°Âûã„ÄÇÊé¢Á¥¢ÊÄßÂàÜÊûêÊè≠Á§∫‰∫ÜÂØ¶È´îË±êÂØåÂ∫¶ÁöÑËÆäÁï∞ÊÄßÔºåÂÖ∂‰∏≠ÁõõË°åÁöÑÂØ¶È´îÂåÖÊã¨ÁãÄÊ≥Å„ÄÅÂπ¥ÈΩ°ÂíåÁç≤ÂæóÁÖßË≠∑ÁöÑÊ©üÊúÉÔºåËÄåÁ®ÆÊóèÂíå‰ΩèÊàøÁãÄÊ≥ÅÁ≠âÊïèÊÑüÈ°ûÂà•ÁöÑ‰ª£Ë°®ÊÄß‰∏çË∂≥„ÄÇ‰∏âÂÖÉÁµÑÂàÜÊûêÁ™ÅÂá∫‰∫ÜÂØ¶È´î‰πãÈñìÁöÑÈ†ªÁπÅÂÖ±ÁèæÔºåÂåÖÊã¨Âπ¥ÈΩ°„ÄÅÊÄßÂà•ÂíåÁãÄÊ≥Å„ÄÇNLI ÁõÆÊ®ôÔºàËòäÊ∂µÂíåÁüõÁõæÂàÜÊûêÔºâÈ°ØÁ§∫„ÄåÁ∂ìÊ≠∑ÈÅéÊö¥ÂäõÊàñËôêÂæÖ„ÄçÂíå„ÄåÊúâÈÜ´ÁôÇ‰øùÈö™„ÄçÁ≠âÂ±¨ÊÄßÂÖ∑ÊúâÂæàÈ´òÁöÑËòäÊ∂µÁéáÔºà82.4%-80.3%ÔºâÔºåËÄå„ÄåË™çÂêåËá™Â∑±ÊòØÂ•≥ÊÄß„Äç„ÄÅ„ÄåÂ∑≤Â©ö„ÄçÂíå„ÄåÊúâÊú´ÊúüÁñæÁóÖ„ÄçÁ≠âÂ±¨ÊÄßÂâáË°®ÁèæÂá∫ÂæàÈ´òÁöÑÁüõÁõæÁéáÔºà70.8%-98.5%Ôºâ„ÄÇ

##### **Efficient Lung Ultrasound Severity Scoring Using Dedicated Feature Extractor**
2501.12524v1 by Jiaqi Guo, Yunnan Wu, Evangelos Kaimakamis, Georgios Petmezas, Vasileios E. Papageorgiou, Nicos Maglaveras, Aggelos K. Katsaggelos

With the advent of the COVID-19 pandemic, ultrasound imaging has emerged as a
promising technique for COVID-19 detection, due to its non-invasive nature,
affordability, and portability. In response, researchers have focused on
developing AI-based scoring systems to provide real-time diagnostic support.
However, the limited size and lack of proper annotation in publicly available
ultrasound datasets pose significant challenges for training a robust AI model.
This paper proposes MeDiVLAD, a novel pipeline to address the above issue for
multi-level lung-ultrasound (LUS) severity scoring. In particular, we leverage
self-knowledge distillation to pretrain a vision transformer (ViT) without
label and aggregate frame-level features via dual-level VLAD aggregation. We
show that with minimal finetuning, MeDiVLAD outperforms conventional
fully-supervised methods in both frame- and video-level scoring, while offering
classification reasoning with exceptional quality. This superior performance
enables key applications such as the automatic identification of critical lung
pathology areas and provides a robust solution for broader medical video
classification tasks.

ÊëòË¶ÅÔºöÈö®Ëëó COVID-19 Â§ßÊµÅË°åÁöÑÂà∞‰æÜÔºåË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÂ∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑ COVID-19 Ê™¢Ê∏¨ÊäÄË°ìÔºåÂõ†ÁÇ∫ÂÆÉÂÖ∑ÊúâÈùû‰æµÂÖ•ÊÄß„ÄÅÂÉπÊ†ºÂØ¶ÊÉ†‰∏îÂèØÊîúÂ∏∂Á≠âÁâπÊÄß„ÄÇÊúâÈëëÊñºÊ≠§ÔºåÁ†îÁ©∂‰∫∫Âì°Â∞àÊ≥®ÊñºÈñãÁôºÂü∫Êñº AI ÁöÑË©ïÂàÜÁ≥ªÁµ±Ôºå‰ª•Êèê‰æõÂç≥ÊôÇÁöÑË®∫Êñ∑ÊîØÊè¥„ÄÇÁÑ∂ËÄåÔºåÂÖ¨ÈñãÂèØÁî®ÁöÑË∂ÖÈü≥Ê≥¢Ë≥áÊñôÈõÜË¶èÊ®°ÊúâÈôê‰∏îÁº∫‰πèÈÅ©Áï∂ÁöÑË®ªËß£ÔºåÈÄôÂ∞çË®ìÁ∑¥Á©©ÂÅ•ÁöÑ AI Ê®°ÂûãÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫ MeDiVLADÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÁÆ°ÈÅìÔºåÁî®ÊñºËß£Ê±∫‰∏äËø∞Â§öÂ±§Á¥öËÇ∫ÈÉ®Ë∂ÖÈü≥Ê≥¢ (LUS) Âö¥ÈáçÂ∫¶Ë©ïÂàÜÁöÑË≠∞È°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂà©Áî®Ëá™ÊàëÁü•Ë≠òËí∏È§æÊäÄË°ìÔºåÂú®Ê≤íÊúâÊ®ôÁ±§ÁöÑÊÉÖÊ≥Å‰∏ãÈ†êË®ìÁ∑¥Ë¶ñË¶∫ËΩâÊèõÂô® (ViT)Ôºå‰∏¶ÈÄèÈÅéÈõôÂ±§Á¥ö VLAD ËÅöÂêà‰æÜÂΩôÁ∏ΩÂπÄÁ¥öÁâπÂæµ„ÄÇÊàëÂÄëË≠âÊòéÔºåÈÄèÈÅéÊúÄÂ∞èÁöÑÂæÆË™øÔºåMeDiVLAD Âú®ÂπÄÁ¥öÂíåÂΩ±ÁâáÁ¥öË©ïÂàÜ‰∏≠ÈÉΩÂÑ™ÊñºÂÇ≥Áµ±ÁöÑÂÖ®Áõ£Áù£ÂºèÊñπÊ≥ïÔºåÂêåÊôÇÊèê‰æõÂìÅË≥™Ê•µ‰Ω≥ÁöÑÂàÜÈ°ûÊé®ÁêÜ„ÄÇÈÄôÁ®ÆÂÑ™Áï∞ÁöÑÊïàËÉΩÊîØÊè¥‰∫ÜÈóúÈçµÊáâÁî®Ôºå‰æãÂ¶ÇËá™ÂãïË≠òÂà•ËÇ∫ÈÉ®ÁóÖÁÅ∂ÂçÄÂüüÔºå‰∏¶ÁÇ∫Êõ¥Âª£Ê≥õÁöÑÈÜ´Â≠∏ÂΩ±ÁâáÂàÜÈ°û‰ªªÂãôÊèê‰æõÁ©©ÂÅ•ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression**
2501.12336v1 by Phuoc Duong Huy Chu

This paper presents results of our system for CoMeDi Shared Task, focusing on
Subtask 2: Disagreement Ranking. Our system leverages sentence embeddings
generated by the paraphrase-xlm-r-multilingual-v1 model, combined with a deep
neural regression model incorporating batch normalization and dropout for
improved generalization. By predicting the mean of pairwise judgment
differences between annotators, our method explicitly targets disagreement
ranking, diverging from traditional "gold label" aggregation approaches. We
optimized our system with a customized architecture and training procedure,
achieving competitive performance in Spearman correlation against mean
disagreement labels. Our results highlight the importance of robust embeddings,
effective model architecture, and careful handling of judgment differences for
ranking disagreement in multilingual contexts. These findings provide insights
into the use of contextualized representations for ordinal judgment tasks and
open avenues for further refinement of disagreement prediction models.

ÊëòË¶ÅÔºöÊú¨ÊñáÂ±ïÁ§∫‰∫ÜÊàëÂÄëÂú® CoMeDi ÂÖ±‰∫´‰ªªÂãôÁ≥ªÁµ±‰∏≠ÁöÑÁµêÊûúÔºåÈáçÈªûÂú®
Â≠ê‰ªªÂãô 2ÔºöÂàÜÊ≠ßÊéíÂêç„ÄÇÊàëÂÄëÁöÑÁ≥ªÁµ±Âà©Áî® paraphrase-xlm-r-multilingual-v1 Ê®°ÂûãÁî¢ÁîüÁöÑÂè•Â≠êÂµåÂÖ•ÔºåÁµêÂêàÊ∑±Â∫¶
Á•ûÁ∂ìËø¥Ê≠∏Ê®°ÂûãÔºå‰∏¶Âä†ÂÖ•ÊâπÊ¨°Ê≠£Ë¶èÂåñÂíå‰∏≠Êñ∑‰ª•ÊîπÂñÑÊ¶ÇÂåñ„ÄÇÈÄèÈÅéÈ†êÊ∏¨Ë®ªËß£ËÄÖ‰πãÈñìÊàêÂ∞çÂà§Êñ∑Â∑ÆÁï∞ÁöÑÂπ≥ÂùáÂÄºÔºåÊàëÂÄëÁöÑ
ÊñπÊ≥ïÊòéÁ¢∫ÈáùÂ∞çÂàÜÊ≠ßÊéíÂêçÔºåÂÅèÈõ¢ÂÇ≥Áµ±ÁöÑ„ÄåÈªÉÈáëÊ®ôÁ±§„ÄçËÅöÂêàÊñπÊ≥ï„ÄÇÊàëÂÄë‰ΩøÁî®Ëá™Ë®ÇÊû∂ÊßãÂíåË®ìÁ∑¥Á®ãÂ∫èÂÑ™ÂåñÁ≥ªÁµ±Ôºå
Âú®ËàáÂπ≥ÂùáÂàÜÊ≠ßÊ®ôÁ±§ÁöÑ Spearman Áõ∏ÈóúÊÄß‰∏≠Áç≤ÂæóÁ´∂Áà≠ÂäõË°®Áèæ„ÄÇÊàëÂÄëÁöÑÁµêÊûúÂº∑Ë™ø‰∫ÜÁ©©ÂÅ•ÂµåÂÖ•„ÄÅÊúâÊïàÊ®°ÂûãÊû∂ÊßãÂíå
Ë¨πÊÖéËôïÁêÜÂà§Êñ∑Â∑ÆÁï∞Â∞çÊñºÂú®Â§öË™ûË®ÄÁí∞Â¢É‰∏≠Â∞çÂàÜÊ≠ßÈÄ≤Ë°åÊéíÂêçÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄô‰∫õÁôºÁèæÊèê‰æõ‰∫Ü‰ΩøÁî®ÊÉÖÂ¢ÉÂåñË°®ÂæµÈÄ≤Ë°åÂ∫èÊï∏Âà§Êñ∑‰ªªÂãôÁöÑË¶ãËß£Ôºå‰∏¶ÁÇ∫ÈÄ≤‰∏ÄÊ≠•ÂÑ™ÂåñÂàÜÊ≠ßÈ†êÊ∏¨Ê®°ÂûãÈñãÈó¢‰∫ÜÈÅìË∑Ø„ÄÇ

##### **CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification**
2501.12266v1 by Cristiano Patr√≠cio, Isabel Rio-Torto, Jaime S. Cardoso, Lu√≠s F. Teixeira, Jo√£o C. Neves

The main challenges limiting the adoption of deep learning-based solutions in
medical workflows are the availability of annotated data and the lack of
interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the
latter by constraining the final disease prediction on a set of predefined and
human-interpretable concepts. However, the increased interpretability achieved
through these concept-based explanations implies a higher annotation burden.
Moreover, if a new concept needs to be added, the whole system needs to be
retrained. Inspired by the remarkable performance shown by Large
Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet
effective, methodology, CBVLM, which tackles both of the aforementioned
challenges. First, for each concept, we prompt the LVLM to answer if the
concept is present in the input image. Then, we ask the LVLM to classify the
image based on the previous concept predictions. Moreover, in both stages, we
incorporate a retrieval module responsible for selecting the best examples for
in-context learning. By grounding the final diagnosis on the predicted
concepts, we ensure explainability, and by leveraging the few-shot capabilities
of LVLMs, we drastically lower the annotation cost. We validate our approach
with extensive experiments across four medical datasets and twelve LVLMs (both
generic and medical) and show that CBVLM consistently outperforms CBMs and
task-specific supervised methods without requiring any training and using just
a few annotated examples. More information on our project page:
https://cristianopatricio.github.io/CBVLM/.

ÊëòË¶ÅÔºöÈôêÂà∂Âú®ÈÜ´ÁôÇÂ∑•‰ΩúÊµÅÁ®ã‰∏≠Êé°Áî®Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑËß£Ê±∫ÊñπÊ°àÁöÑ‰∏ªË¶ÅÊåëÊà∞ÊòØÊ®ôË®òË≥áÊñôÁöÑÂèØÁî®ÊÄß‰ª•ÂèäÊ≠§È°ûÁ≥ªÁµ±ÁöÑÂèØËß£ÈáãÊÄß‰∏çË∂≥„ÄÇÊ¶ÇÂøµÁì∂È†∏Ê®°Âûã (CBM) ÈÄèÈÅéÈôêÂà∂‰∏ÄÁµÑÈ†êÂÆöÁæ©‰∏î‰∫∫È°ûÂèØËß£ÈáãÁöÑÊ¶ÇÂøµÂ∞çÊúÄÁµÇÁñæÁóÖÈ†êÊ∏¨Ôºå‰æÜËß£Ê±∫ÂæåËÄÖ„ÄÇÁÑ∂ËÄåÔºåÈÄèÈÅéÈÄô‰∫õÂü∫ÊñºÊ¶ÇÂøµÁöÑËß£ÈáãÊâÄÂØ¶ÁèæÁöÑÂèØËß£ÈáãÊÄßÊèêÂçáÔºåÊÑèÂë≥ËëóÊõ¥È´òÁöÑÊ®ôË®òË≤†Êìî„ÄÇÊ≠§Â§ñÔºåÂ¶ÇÊûúÈúÄË¶ÅÊñ∞Â¢û‰∏ÄÂÄãÊñ∞Ê¶ÇÂøµÔºåÂâáÈúÄË¶ÅÈáçÊñ∞Ë®ìÁ∑¥Êï¥ÂÄãÁ≥ªÁµ±„ÄÇÂèóÂà∞Â§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (LVLMs) Âú®Â∞èÊ®£Êú¨Ë®≠ÂÆö‰∏≠Â±ïÁèæÁöÑÂçìË∂äÊïàËÉΩÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑ CBVLM ÊñπÊ≥ïÔºå‰æÜËß£Ê±∫‰∏äËø∞ÂÖ©ÂÄãÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåÂ∞çÊñºÊØèÂÄãÊ¶ÇÂøµÔºåÊàëÂÄëÊèêÁ§∫ LVLM ÂõûÁ≠îËº∏ÂÖ•ÂΩ±ÂÉè‰∏≠ÊòØÂê¶ÂåÖÂê´Ë©≤Ê¶ÇÂøµ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË¶ÅÊ±Ç LVLM Ê†πÊìöÂÖàÂâçÁöÑÊ¶ÇÂøµÈ†êÊ∏¨Â∞çÂΩ±ÂÉèÈÄ≤Ë°åÂàÜÈ°û„ÄÇÊ≠§Â§ñÔºåÂú®ÂÖ©ÂÄãÈöéÊÆµ‰∏≠ÔºåÊàëÂÄëÈÉΩÁ¥çÂÖ•‰∏ÄÂÄãÊ™¢Á¥¢Ê®°ÁµÑÔºåË≤†Ë≤¨ÈÅ∏Âá∫ÊúÄÈÅ©ÂêàÊñºÊÉÖÂ¢ÉÂ≠∏ÁøíÁöÑÁØÑ‰æã„ÄÇÈÄèÈÅéÂ∞áÊúÄÁµÇË®∫Êñ∑Âª∫Á´ãÂú®È†êÊ∏¨Ê¶ÇÂøµ‰πã‰∏äÔºåÊàëÂÄëÁ¢∫‰øù‰∫ÜÂèØËß£ÈáãÊÄßÔºå‰∏¶ÈÄèÈÅéÂà©Áî® LVLMs ÁöÑÂ∞èÊ®£Êú¨ËÉΩÂäõÔºåÊàëÂÄëÂ§ßÂπÖÈôç‰Ωé‰∫ÜÊ®ôË®òÊàêÊú¨„ÄÇÊàëÂÄëÈÄèÈÅéÂõõÂÄãÈÜ´ÁôÇË≥áÊñôÈõÜÂíåÂçÅ‰∫åÂÄã LVLMÔºàÈÄöÁî®ÂíåÈÜ´ÁôÇÔºâÁöÑÂª£Ê≥õÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑ‰ΩúÊ≥ïÔºå‰∏¶È°ØÁ§∫ CBVLM Âú®ÁÑ°ÈúÄ‰ªª‰ΩïË®ìÁ∑¥‰∏îÂÉÖ‰ΩøÁî®Â∞ëÊï∏Ê®ôË®òÁØÑ‰æãÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂßãÁµÇÂÑ™Êñº CBM ÂíåÁâπÂÆöÊñº‰ªªÂãôÁöÑÁõ£Áù£ÂºèÊñπÊ≥ï„ÄÇÊõ¥Â§öË≥áË®äË´ãË¶ãÊàëÂÄëÁöÑÂ∞àÊ°àÈ†ÅÈù¢Ôºöhttps://cristianopatricio.github.io/CBVLM/„ÄÇ

##### **Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes**
2501.12106v1 by Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer

Tumor documentation in Germany is largely done manually, requiring reading
patient records and entering data into structured databases. Large language
models (LLMs) could potentially enhance this process by improving efficiency
and reliability. This evaluation tests eleven different open source LLMs with
sizes ranging from 1-70 billion model parameters on three basic tasks of the
tumor documentation process: identifying tumor diagnoses, assigning ICD-10
codes, and extracting the date of first diagnosis. For evaluating the LLMs on
these tasks, a dataset of annotated text snippets based on anonymized doctors'
notes from urology was prepared. Different prompting strategies were used to
investigate the effect of the number of examples in few-shot prompting and to
explore the capabilities of the LLMs in general. The models Llama 3.1 8B,
Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.
Models with less extensive training data or having fewer than 7 billion
parameters showed notably lower performance, while larger models did not
display performance gains. Examples from a different medical domain than
urology could also improve the outcome in few-shot prompting, which
demonstrates the ability of LLMs to handle tasks needed for tumor
documentation. Open source LLMs show a strong potential for automating tumor
documentation. Models from 7-12 billion parameters could offer an optimal
balance between performance and resource efficiency. With tailored fine-tuning
and well-designed prompting, these models might become important tools for
clinical documentation in the future. The code for the evaluation is available
from https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset
as a new valuable resource that addresses the shortage of authentic and easily
accessible benchmarks in German-language medical NLP.

ÊëòË¶ÅÔºöÂæ∑ÂúãÁöÑËÖ´Áò§Êñá‰ª∂Ë®òÈåÑÂ§ßÈÉ®ÂàÜÊòØÊâãÂãïÂÆåÊàêÔºåÈúÄË¶ÅÈñ±ËÆÄÁóÖÊ≠∑‰∏¶Â∞áË≥áÊñôËº∏ÂÖ•ÁµêÊßãÂåñÁöÑË≥áÊñôÂ∫´‰∏≠„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂèØËÉΩÈÄèÈÅéÊèêÂçáÊïàÁéáÂíåÂèØÈù†ÊÄß‰æÜÂ¢ûÂº∑Ê≠§Á®ãÂ∫è„ÄÇÊ≠§Ë©ïÈáèÊ∏¨Ë©¶‰∫Ü 11 ÂÄã‰∏çÂêåÁöÑÈñãÊ∫ê LLMÔºåÊ®°ÂûãÂèÉÊï∏Â§ßÂ∞èÂæû 10 ÂÑÑÂà∞ 700 ÂÑÑ‰∏çÁ≠âÔºåÈáùÂ∞çËÖ´Áò§Êñá‰ª∂Ë®òÈåÑÁ®ãÂ∫èÁöÑ‰∏âÈ†ÖÂü∫Êú¨‰ªªÂãôÔºöË≠òÂà•ËÖ´Áò§Ë®∫Êñ∑„ÄÅÊåáÂÆö ICD-10 ‰ª£Á¢ºÔºå‰ª•ÂèäÊì∑ÂèñÈ¶ñÊ¨°Ë®∫Êñ∑Êó•Êúü„ÄÇÁÇ∫‰∫ÜÈáùÂ∞çÈÄô‰∫õ‰ªªÂãôË©ï‰º∞ LLMÔºåÊ∫ñÂÇô‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÊ≥åÂ∞øÁßëÈÜ´ÁîüÂåøÂêçÁ≠ÜË®òÁöÑË®ªËß£ÊñáÂ≠óÁâáÊÆµË≥áÊñôÈõÜ„ÄÇ‰ΩøÁî®‰∏çÂêåÁöÑÊèêÁ§∫Á≠ñÁï•‰æÜË™øÊü•Â∞ëÈáèÊèêÁ§∫‰∏≠ÁØÑ‰æãÊï∏ÈáèÁöÑÂΩ±ÈüøÔºå‰∏¶Êé¢Á¥¢ LLM ÁöÑ‰∏ÄËà¨ËÉΩÂäõ„ÄÇLlama 3.1 8B„ÄÅMistral 7B Âíå Mistral NeMo 12 B Á≠âÊ®°ÂûãÂú®ÈÄô‰∫õ‰ªªÂãô‰∏≠Ë°®ÁèæÁõ∏Áï∂Â•Ω„ÄÇË®ìÁ∑¥Ë≥áÊñôËºÉÂ∞ëÊàñÂèÉÊï∏Â∞ëÊñº 70 ÂÑÑÁöÑÊ®°ÂûãË°®ÁèæÊòéÈ°ØËºÉÂ∑ÆÔºåËÄåËºÉÂ§ßÁöÑÊ®°Âûã‰∏¶Êú™Â±ïÁèæÊïàËÉΩÊèêÂçá„ÄÇËàáÊ≥åÂ∞øÁßë‰∏çÂêåÁöÑÈÜ´ÁôÇÈ†òÂüüÁöÑÁØÑ‰æã‰πüÂèØ‰ª•ÊîπÂñÑÂ∞ëÈáèÊèêÁ§∫ÁöÑÁµêÊûúÔºåÈÄôË≠âÊòé‰∫Ü LLM ËôïÁêÜËÖ´Áò§Êñá‰ª∂Ë®òÈåÑÊâÄÈúÄ‰ªªÂãôÁöÑËÉΩÂäõ„ÄÇÈñãÊ∫ê LLM Âú®Ëá™ÂãïÂåñËÖ´Áò§Êñá‰ª∂Ë®òÈåÑÊñπÈù¢È°ØÁ§∫Âá∫Âº∑Â§ßÁöÑÊΩõÂäõ„ÄÇÂèÉÊï∏‰ªãÊñº 70 ÂÑÑÂà∞ 120 ÂÑÑÁöÑÊ®°ÂûãÂèØ‰ª•Âú®ÊïàËÉΩÂíåË≥áÊ∫êÊïàÁéá‰πãÈñìÊèê‰æõÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇÈÄèÈÅéÈáèË∫´ÊâìÈÄ†ÂæÆË™øÂíåÁ≤æÂøÉË®≠Ë®àÁöÑÊèêÁ§∫ÔºåÈÄô‰∫õÊ®°ÂûãÊú™‰æÜÂèØËÉΩÊúÉÊàêÁÇ∫Ëá®Â∫äÊñá‰ª∂Ë®òÈåÑÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇË©ï‰º∞Á®ãÂºèÁ¢ºÂèØÂæû https://github.com/stefan-m-lenz/UroLlmEval ÂèñÂæó„ÄÇÊàëÂÄë‰πüÈáãÂá∫Ë≥áÊñôÈõÜ‰ΩúÁÇ∫‰∏ÄÂÄãÊñ∞ÁöÑÊúâÂÉπÂÄºË≥áÊ∫êÔºåÁî®ÊñºËß£Ê±∫Âæ∑Ë™ûÈÜ´ÁôÇËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÁúüÂØ¶‰∏îÊòìÊñºÂèñÂæóÁöÑÂü∫Ê∫ñÁü≠Áº∫ÂïèÈ°å„ÄÇ

##### **Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from CT and PET**
2501.12425v1 by Fatih Aksu, Fabrizia Gelardi, Arturo Chiti, Paolo Soda

Accurate classification of histological subtypes of non-small cell lung
cancer (NSCLC) is essential in the era of precision medicine, yet current
invasive techniques are not always feasible and may lead to clinical
complications. This study presents a multi-stage intermediate fusion approach
to classify NSCLC subtypes from CT and PET images. Our method integrates the
two modalities at different stages of feature extraction, using voxel-wise
fusion to exploit complementary information across varying abstraction levels
while preserving spatial correlations. We compare our method against unimodal
approaches using only CT or PET images to demonstrate the benefits of modality
fusion, and further benchmark it against early and late fusion techniques to
highlight the advantages of intermediate fusion during feature extraction.
Additionally, we compare our model with the only existing intermediate fusion
method for histological subtype classification using PET/CT images. Our results
demonstrate that the proposed method outperforms all alternatives across key
metrics, with an accuracy and AUC equal to 0.724 and 0.681, respectively. This
non-invasive approach has the potential to significantly improve diagnostic
accuracy, facilitate more informed treatment decisions, and advance
personalized care in lung cancer management.

ÊëòË¶ÅÔºöÂú®Á≤æÊ∫ñÈÜ´ÁôÇÁöÑÊôÇ‰ª£ÔºåÊ∫ñÁ¢∫ÂàÜÈ°ûÈùûÂ∞èÁ¥∞ËÉûËÇ∫Áôå (NSCLC) ÁöÑÁµÑÁπîÂ≠∏‰∫ûÂûãËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÁõÆÂâçÁöÑ‰æµÂÖ•ÊÄßÊäÄË°ì‰∏¶‰∏çÁ∏ΩÊòØÂèØË°åÔºå‰∏îÂèØËÉΩÊúÉÂ∞éËá¥Ëá®Â∫ä‰ΩµÁôºÁóá„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂ§öÈöéÊÆµ‰∏≠ÈñìËûçÂêàÊñπÊ≥ïÔºåÂæûÈõªËÖ¶Êñ∑Â±§ (CT) ÂíåÊ≠£Â≠êÊñ∑Â±§ÊéÉÊèè (PET) ÂΩ±ÂÉè‰∏≠ÂàÜÈ°û NSCLC ‰∫ûÂûã„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂú®ÁâπÂæµËêÉÂèñÁöÑ‰∏çÂêåÈöéÊÆµÊï¥ÂêàÈÄôÂÖ©Á®ÆÊñπÂºèÔºåÂà©Áî®ÈÄêÈ´îÁ¥†ËûçÂêà‰æÜÂà©Áî®‰∏çÂêåÊäΩË±°Â±§Á¥öÁöÑ‰∫íË£úË≥áË®äÔºåÂêåÊôÇ‰øùÁïôÁ©∫ÈñìÁõ∏ÈóúÊÄß„ÄÇÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÊäÄË°ìËàáÂÉÖ‰ΩøÁî®ÈõªËÖ¶Êñ∑Â±§ÊàñÊ≠£Â≠êÊñ∑Â±§ÊéÉÊèèÂΩ±ÂÉèÁöÑÂñÆ‰∏ÄÊ®°ÂºèÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉÔºå‰ª•Ë≠âÊòéÊ®°ÂºèËûçÂêàÁöÑÂÑ™ÈªûÔºå‰∏¶ÈÄ≤‰∏ÄÊ≠•Â∞áÂÖ∂ËàáÊó©ÊúüÂíåÊôöÊúüËûçÂêàÊäÄË°ìÈÄ≤Ë°åÊØîËºÉÔºå‰ª•Âº∑Ë™øÁâπÂæµËêÉÂèñÊúüÈñì‰∏≠ÈñìËûçÂêàÁöÑÂÑ™Èªû„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÊ®°ÂûãËàáÂîØ‰∏ÄÁèæÊúâÁöÑ‰∏≠ÈñìËûçÂêàÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®Ê≠£Â≠êÊñ∑Â±§ÊéÉÊèè/ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂΩ±ÂÉèÈÄ≤Ë°åÁµÑÁπîÂ≠∏‰∫ûÂûãÂàÜÈ°û„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÊâÄÊúâÊõø‰ª£ÊñπÊ°à‰∏≠Ë°®ÁèæÂÑ™Áï∞ÔºåÊ∫ñÁ¢∫ÁéáÂíå AUC ÂàÜÂà•Á≠âÊñº 0.724 Âíå 0.681„ÄÇÈÄôÁ®ÆÈùû‰æµÂÖ•ÊÄßÊñπÊ≥ïÊúâÂèØËÉΩÈ°ØËëóÊèêÈ´òË®∫Êñ∑Ê∫ñÁ¢∫ÁéáÔºå‰øÉÈÄ≤Êõ¥ÊòéÊô∫ÁöÑÊ≤ªÁôÇÊ±∫Á≠ñÔºå‰∏¶Êé®ÈÄ≤ËÇ∫ÁôåÁÆ°ÁêÜ‰∏≠ÁöÑÂÄã‰∫∫ÂåñÁÖßË≠∑„ÄÇ

##### **Adaptive Class Learning to Screen Diabetic Disorders in Fundus Images of Eye**
2501.12048v1 by Shramana Dey, Pallabi Dutta, Riddhasree Bhattacharyya, Surochita Pal, Sushmita Mitra, Rajiv Raman

The prevalence of ocular illnesses is growing globally, presenting a
substantial public health challenge. Early detection and timely intervention
are crucial for averting visual impairment and enhancing patient prognosis.
This research introduces a new framework called Class Extension with Limited
Data (CELD) to train a classifier to categorize retinal fundus images. The
classifier is initially trained to identify relevant features concerning
Healthy and Diabetic Retinopathy (DR) classes and later fine-tuned to adapt to
the task of classifying the input images into three classes: Healthy, DR, and
Glaucoma. This strategy allows the model to gradually enhance its
classification capabilities, which is beneficial in situations where there are
only a limited number of labeled datasets available. Perturbation methods are
also used to identify the input image characteristics responsible for
influencing the models decision-making process. We achieve an overall accuracy
of 91% on publicly available datasets.

ÊëòË¶ÅÔºöÂÖ®ÁêÉÁúºÁñæÊÇ£ÁóÖÁéáÊåÅÁ∫å‰∏äÂçáÔºåÂ∞çÂÖ¨ÂÖ±Ë°õÁîüÈÄ†ÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÊó©ÊúüÁôºÁèæÂíåÂèäÊôÇÂπ≤È†êÂ∞çÊñºÈ†êÈò≤Ë¶ñÂäõÈöúÁ§ôÂíåÊîπÂñÑÊÇ£ËÄÖÈ†êÂæåËá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ÊúâÈôêÊï∏ÊìöÈ°ûÂà•Êì¥Â±ï (CELD) ÁöÑÊñ∞Ê°ÜÊû∂ÔºåÁî®ÊñºË®ìÁ∑¥ÂàÜÈ°ûÂô®Â∞çË¶ñÁ∂≤ËÜúÁúºÂ∫ïÂúñÂÉèÈÄ≤Ë°åÂàÜÈ°û„ÄÇË©≤ÂàÜÈ°ûÂô®ÊúÄÂàùÊé•ÂèóË®ìÁ∑¥‰ª•Ë≠òÂà•ËàáÂÅ•Â∫∑ÂíåÁ≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆä (DR) È°ûÂà•Áõ∏ÈóúÁöÑÁâπÂæµÔºåÁÑ∂ÂæåÈÄ≤Ë°åÂæÆË™ø‰ª•ÈÅ©ÊáâÂ∞áËº∏ÂÖ•ÂúñÂÉèÂàÜÈ°ûÁÇ∫‰∏âÈ°ûÁöÑ‰ªªÂãôÔºöÂÅ•Â∫∑„ÄÅDR ÂíåÈùíÂÖâÁúº„ÄÇÊ≠§Á≠ñÁï•ÂÖÅË®±Ê®°ÂûãÈÄêÊ≠•Â¢ûÂº∑ÂÖ∂ÂàÜÈ°ûËÉΩÂäõÔºåÈÄôÂú®Ê®ôË®òÊï∏ÊìöÈõÜÊï∏ÈáèÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÊòØÊúâÁõäÁöÑ„ÄÇÊìæÂãïÊñπÊ≥ï‰πüÁî®ÊñºË≠òÂà•Ë≤†Ë≤¨ÂΩ±ÈüøÊ®°ÂûãÊ±∫Á≠ñÈÅéÁ®ãÁöÑËº∏ÂÖ•ÂúñÂÉèÁâπÂæµ„ÄÇÊàëÂÄëÂú®ÂÖ¨ÈñãÊï∏ÊìöÈõÜ‰∏äÂØ¶Áèæ‰∫Ü 91% ÁöÑÊï¥È´îÊ∫ñÁ¢∫Â∫¶„ÄÇ

##### **Tackling Small Sample Survival Analysis via Transfer Learning: A Study of Colorectal Cancer Prognosis**
2501.12421v1 by Yonghao Zhao, Changtao Li, Chi Shu, Qingbin Wu, Hong Li, Chuan Xu, Tianrui Li, Ziqiang Wang, Zhipeng Luo, Yazhou He

Survival prognosis is crucial for medical informatics. Practitioners often
confront small-sized clinical data, especially cancer patient cases, which can
be insufficient to induce useful patterns for survival predictions. This study
deals with small sample survival analysis by leveraging transfer learning, a
useful machine learning technique that can enhance the target analysis with
related knowledge pre-learned from other data. We propose and develop various
transfer learning methods designed for common survival models. For parametric
models such as DeepSurv, Cox-CC (Cox-based neural networks), and DeepHit
(end-to-end deep learning model), we apply standard transfer learning
techniques like pretraining and fine-tuning. For non-parametric models such as
Random Survival Forest, we propose a new transfer survival forest (TSF) model
that transfers tree structures from source tasks and fine-tunes them with
target data. We evaluated the transfer learning methods on colorectal cancer
(CRC) prognosis. The source data are 27,379 SEER CRC stage I patients, and the
target data are 728 CRC stage I patients from the West China Hospital. When
enhanced by transfer learning, Cox-CC's $C^{td}$ value was boosted from 0.7868
to 0.8111, DeepHit's from 0.8085 to 0.8135, DeepSurv's from 0.7722 to 0.8043,
and RSF's from 0.7940 to 0.8297 (the highest performance). All models trained
with data as small as 50 demonstrated even more significant improvement.
Conclusions: Therefore, the current survival models used for cancer prognosis
can be enhanced and improved by properly designed transfer learning techniques.
The source code used in this study is available at
https://github.com/YonghaoZhao722/TSF.

ÊëòË¶ÅÔºö<paragraph>Â≠òÊ¥ªÈ†êÊ∏¨Â∞çÈÜ´ÁôÇË≥áË®äÂ≠∏Ëá≥ÈóúÈáçË¶Å„ÄÇÂØ¶ÂãôÂ∑•‰ΩúËÄÖÁ∂ìÂ∏∏Èù¢Â∞çÂ∞èË¶èÊ®°ÁöÑËá®Â∫äË≥áÊñôÔºåÁâπÂà•ÊòØÁôåÁóáÁóÖÊÇ£ÂÄãÊ°àÔºåÈÄô‰∫õË≥áÊñôÂèØËÉΩ‰∏çË∂≥‰ª•Ë™òÁôºÊúâÁî®ÁöÑÊ®°Âºè‰æÜÈÄ≤Ë°åÂ≠òÊ¥ªÈ†êÊ∏¨„ÄÇÊ≠§Á†îÁ©∂ÈÄèÈÅéÂà©Áî®ËΩâÁßªÂ≠∏Áøí‰æÜËôïÁêÜÂ∞èÊ®£Êú¨Â≠òÊ¥ªÂàÜÊûêÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊúâÁî®ÁöÑÊ©üÂô®Â≠∏ÁøíÊäÄË°ìÔºåÂèØ‰ª•ÈÄèÈÅéÂæûÂÖ∂‰ªñË≥áÊñôÈ†êÂÖàÂ≠∏ÁøíÂà∞ÁöÑÁõ∏ÈóúÁü•Ë≠ò‰æÜÂ¢ûÂº∑ÁõÆÊ®ôÂàÜÊûê„ÄÇÊàëÂÄëÊèêÂá∫‰∏¶ÈñãÁôºÂêÑÁ®ÆÂ∞àÁÇ∫Â∏∏Ë¶ãÂ≠òÊ¥ªÊ®°ÂûãË®≠Ë®àÁöÑËΩâÁßªÂ≠∏ÁøíÊñπÊ≥ï„ÄÇÂ∞çÊñºÂèÉÊï∏ÂåñÊ®°ÂûãÔºå‰æãÂ¶Ç DeepSurv„ÄÅCox-CCÔºàÂü∫Êñº Cox ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºâÂíå DeepHitÔºàÁ´ØÂà∞Á´ØÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºâÔºåÊàëÂÄëÊáâÁî®Ê®ôÊ∫ñËΩâÁßªÂ≠∏ÁøíÊäÄË°ìÔºå‰æãÂ¶ÇÈ†êË®ìÁ∑¥ÂíåÂæÆË™ø„ÄÇÂ∞çÊñºÈùûÂèÉÊï∏ÂåñÊ®°ÂûãÔºå‰æãÂ¶ÇÈö®Ê©üÂ≠òÊ¥ªÊ£ÆÊûóÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑËΩâÁßªÂ≠òÊ¥ªÊ£ÆÊûóÔºàTSFÔºâÊ®°ÂûãÔºåÂÆÉÂæû‰æÜÊ∫ê‰ªªÂãôÂÇ≥Ëº∏Ê®πÁãÄÁµêÊßãÔºå‰∏¶‰ΩøÁî®ÁõÆÊ®ôË≥áÊñôÂæÆË™øÂÆÉÂÄë„ÄÇÊàëÂÄëÂú®ÁµêÁõ¥ËÖ∏ÁôåÔºàCRCÔºâÈ†êÂæå‰∏äË©ï‰º∞‰∫ÜËΩâÁßªÂ≠∏ÁøíÊñπÊ≥ï„ÄÇ‰æÜÊ∫êË≥áÊñôÁÇ∫ 27,379 Âêç SEER CRC Á¨¨‰∏ÄÊúüÊÇ£ËÄÖÔºåÁõÆÊ®ôË≥áÊñôÁÇ∫‰æÜËá™‰∏≠ÂúãË•øÈÉ®ÈÜ´Èô¢ÁöÑ 728 Âêç CRC Á¨¨‰∏ÄÊúüÊÇ£ËÄÖ„ÄÇÂú®ÈÄèÈÅéËΩâÁßªÂ≠∏ÁøíÂ¢ûÂº∑ÂæåÔºåCox-CC ÁöÑ $C^{td}$ ÂÄºÂæû 0.7868 ÊèêÂçáÂà∞ 0.8111ÔºåDeepHit ÁöÑÂæû 0.8085 ÊèêÂçáÂà∞ 0.8135ÔºåDeepSurv ÁöÑÂæû 0.7722 ÊèêÂçáÂà∞ 0.8043ÔºåRSF ÁöÑÂæû 0.7940 ÊèêÂçáÂà∞ 0.8297ÔºàÊúÄÈ´òÊïàËÉΩÔºâ„ÄÇÊâÄÊúâ‰ª•Â∞èËá≥ 50 ÁöÑË≥áÊñôË®ìÁ∑¥ÁöÑÊ®°ÂûãÈÉΩÂ±ïÁ§∫Âá∫Êõ¥È°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÁµêË´ñÔºöÂõ†Ê≠§ÔºåÁõÆÂâçÁî®ÊñºÁôåÁóáÈ†êÂæåÁöÑÂ≠òÊ¥ªÊ®°ÂûãÂèØ‰ª•ÈÄèÈÅéÈÅ©Áï∂Ë®≠Ë®àÁöÑËΩâÁßªÂ≠∏ÁøíÊäÄË°ì‰æÜÂ¢ûÂº∑ÂíåÊîπÂñÑ„ÄÇÊú¨Á†îÁ©∂‰∏≠‰ΩøÁî®ÁöÑÂéüÂßãÁ¢ºÂèØÂú® https://github.com/YonghaoZhao722/TSF ÂèñÂæó„ÄÇ</paragraph>

##### **Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)**
2501.13957v1 by Jadon Geathers, Yann Hicke, Colleen Chan, Niroop Rajashekar, Justin Sewell, Susannah Cornes, Rene Kizilcec, Dennis Shung

Introduction. Objective Structured Clinical Examinations (OSCEs) are widely
used to assess medical students' communication skills, but scoring
interview-based assessments is time-consuming and potentially subject to human
bias. This study explored the potential of large language models (LLMs) to
automate OSCE evaluations using the Master Interview Rating Scale (MIRS).
  Methods. We compared the performance of four state-of-the-art LLMs (GPT-4o,
Claude 3.5, Llama 3.1, and Gemini 1.5 Pro) in evaluating OSCE transcripts
across all 28 items of the MIRS under the conditions of zero-shot,
chain-of-thought (CoT), few-shot, and multi-step prompting. The models were
benchmarked against a dataset of 10 OSCE cases with 174 expert consensus scores
available. Model performance was measured using three accuracy metrics (exact,
off-by-one, thresholded).
  Results. Averaging across all MIRS items and OSCE cases, LLMs performed with
low exact accuracy (0.27 to 0.44), and moderate to high off-by-one accuracy
(0.67 to 0.87) and thresholded accuracy (0.75 to 0.88). A zero temperature
parameter ensured high intra-rater reliability ($\alpha = 0.98$ for GPT-4o).
CoT, few-shot, and multi-step techniques proved valuable when tailored to
specific assessment items. The performance was consistent across MIRS items
independent of encounter phases and communication domains.
  Conclusion. We demonstrated the feasibility of AI-assisted OSCE evaluation
and provided benchmarking of multiple LLMs across multiple prompt techniques.
Our work provides a baseline performance assessment for LLMs that lays a
foundation for future research in automated assessment of clinical
communication skills.

ÊëòË¶ÅÔºö<paragraph>Á∑íË´ñ„ÄÇÂÆ¢ËßÄÁµêÊßãÂåñËá®Â∫äËÄÉË©¶ (OSCE) Âª£Ê≥õÁî®ÊñºË©ïÈáèÈÜ´Â≠∏ÁîüÁöÑÊ∫ùÈÄöÊäÄÂ∑ßÔºå‰ΩÜË©ïÂàÜÂü∫ÊñºË®™Ë´áÁöÑË©ïÈáèÈùûÂ∏∏ËÄóÊôÇÔºå‰∏îÊΩõÂú®ÂèóÂà∞‰∫∫È°ûÂÅèË¶ãÁöÑÂΩ±Èüø„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩøÁî®Â§ßÂ∏´Ë®™Ë´áË©ïÂàÜÈáèË°® (MIRS) Ëá™ÂãïÂåñ OSCE Ë©ïÈáèÁöÑÂèØËÉΩÊÄß„ÄÇ
ÊñπÊ≥ï„ÄÇÊàëÂÄëÊØîËºÉ‰∫ÜÂõõÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑ LLMÔºàGPT-4o„ÄÅClaude 3.5„ÄÅLlama 3.1 Âíå Gemini 1.5 ProÔºâÂú®Ë©ïÈáè OSCE ÊàêÁ∏æÂñÆÁöÑË°®ÁèæÔºåÁØÑÂúçÊ∂µËìã MIRS ÁöÑÊâÄÊúâ 28 ÂÄãÈ†ÖÁõÆÔºåÊ¢ù‰ª∂ÁÇ∫Èõ∂Ê¨°Â≠∏Áøí„ÄÅÊÄùËÄÉÈèà (CoT)„ÄÅÂ∞ëÊ¨°Â≠∏ÁøíÂíåÂ§öÊ≠•È©üÊèêÁ§∫„ÄÇÈÄô‰∫õÊ®°Âûã‰ª• 10 ÂÄã OSCE Ê°à‰æãÁöÑË≥áÊñôÈõÜÁÇ∫Âü∫Ê∫ñÔºåÂÖ∂‰∏≠Êúâ 174 ÂÄãÂ∞àÂÆ∂ÂÖ±Ë≠òÂàÜÊï∏ÂèØÁî®„ÄÇÊ®°ÂûãË°®Áèæ‰ΩøÁî®‰∏âÂÄãÊ∫ñÁ¢∫ÊÄßÊåáÊ®ôÔºàÂÆåÂÖ®„ÄÅÂÅèÈõ¢‰∏Ä„ÄÅÈñæÂÄºÔºâÈÄ≤Ë°åË°°Èáè„ÄÇ
ÁµêÊûú„ÄÇÂπ≥ÂùáÊâÄÊúâ MIRS È†ÖÁõÆÂíå OSCE Ê°à‰æãÔºåLLM ÁöÑÂÆåÂÖ®Ê∫ñÁ¢∫ÊÄß‰ΩéÔºà0.27 Âà∞ 0.44ÔºâÔºåÂÅèÈõ¢‰∏ÄÊ∫ñÁ¢∫ÊÄß‰∏≠Á≠âËá≥È´òÔºà0.67 Âà∞ 0.87ÔºâÔºåÈñæÂÄºÊ∫ñÁ¢∫ÊÄßÈ´òÔºà0.75 Âà∞ 0.88Ôºâ„ÄÇÈõ∂Ê∫´Â∫¶ÂèÉÊï∏Á¢∫‰øù‰∫ÜÂæàÈ´òÁöÑË©ïÂàÜËÄÖÂÖßÈÉ®‰ø°Â∫¶ÔºàGPT-4o ÁöÑ Œ± = 0.98Ôºâ„ÄÇÁï∂ÈáùÂ∞çÁâπÂÆöË©ïÈáèÈ†ÖÁõÆÈÄ≤Ë°åË™øÊï¥ÊôÇÔºåCoT„ÄÅÂ∞ëÊ¨°Â≠∏ÁøíÂíåÂ§öÊ≠•È©üÊäÄË°ìË¢´Ë≠âÊòéÊòØÊúâÂÉπÂÄºÁöÑ„ÄÇË°®ÁèæËàá MIRS È†ÖÁõÆ‰∏ÄËá¥ÔºåËàáÈÅ≠ÈÅáÈöéÊÆµÂíåÊ∫ùÈÄöÈ†òÂüüÁÑ°Èóú„ÄÇ
ÁµêË´ñ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü AI ËºîÂä© OSCE Ë©ïÈáèÁöÑÂèØË°åÊÄßÔºå‰∏¶Êèê‰æõ‰∫ÜÂ§öÁ®ÆÊèêÁ§∫ÊäÄË°ìÁöÑ LLM Âü∫Ê∫ñÊ∏¨Ë©¶„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁÇ∫ LLM Êèê‰æõ‰∫ÜÂü∫Ê∫ñË°®ÁèæË©ïÈáèÔºåÁÇ∫Ëá®Â∫äÊ∫ùÈÄöÊäÄÂ∑ßËá™ÂãïÂåñË©ïÈáèÁöÑÊú™‰æÜÁ†îÁ©∂Â•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ</paragraph>

##### **Data-driven Detection and Evaluation of Damages in Concrete Structures: Using Deep Learning and Computer Vision**
2501.11836v1 by Saeid Ataei, Saeed Adibnazari, Seyyed Taghi Ataei

Structural integrity is vital for maintaining the safety and longevity of
concrete infrastructures such as bridges, tunnels, and walls. Traditional
methods for detecting damages like cracks and spalls are labor-intensive,
time-consuming, and prone to human error. To address these challenges, this
study explores advanced data-driven techniques using deep learning for
automated damage detection and analysis. Two state-of-the-art instance
segmentation models, YOLO-v7 instance segmentation and Mask R-CNN, were
evaluated using a dataset comprising 400 images, augmented to 10,995 images
through geometric and color-based transformations to enhance robustness. The
models were trained and validated using a dataset split into 90% training set,
validation and test set 10%. Performance metrics such as precision, recall,
mean average precision (mAP@0.5), and frames per second (FPS) were used for
evaluation. YOLO-v7 achieved a superior mAP@0.5 of 96.1% and processed 40 FPS,
outperforming Mask R-CNN, which achieved a mAP@0.5 of 92.1% with a slower
processing speed of 18 FPS. The findings recommend YOLO-v7 instance
segmentation model for real-time, high-speed structural health monitoring,
while Mask R-CNN is better suited for detailed offline assessments. This study
demonstrates the potential of deep learning to revolutionize infrastructure
maintenance, offering a scalable and efficient solution for automated damage
detection.

ÊëòË¶ÅÔºöÁµêÊßãÂÆåÊï¥ÊÄßÂ∞çÊñºÁ∂≠Ë≠∑Ê©ãÊ®ë„ÄÅÈößÈÅìÂíåÁâÜÂ£ÅÁ≠âÊ∑∑ÂáùÂúüÂü∫Á§éË®≠ÊñΩÁöÑÂÆâÂÖ®ÊÄßÂíå‰ΩøÁî®Â£ΩÂëΩËá≥ÈóúÈáçË¶Å„ÄÇÂÇ≥Áµ±ÁöÑÊêçÂ£ûÊ™¢Ê∏¨ÊñπÊ≥ïÔºå‰æãÂ¶ÇË£ÇÁ∏´ÂíåÂâùËêΩÔºåÈúÄË¶ÅÂ§ßÈáè‰∫∫Â∑•ÔºåËÄóÊôÇ‰∏îÂÆπÊòìÂá∫Áèæ‰∫∫ÁÇ∫ÈåØË™§„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü‰ΩøÁî®Ê∑±Â∫¶Â≠∏ÁøíÁöÑÂÖàÈÄ≤Êï∏ÊìöÈ©ÖÂãïÊäÄË°ìÔºåÁî®ÊñºËá™ÂãïÊêçÂ£ûÊ™¢Ê∏¨ÂíåÂàÜÊûê„ÄÇ‰ΩøÁî®ÂåÖÂê´ 400 ÂºµÂúñÂÉèÁöÑÊï∏ÊìöÈõÜË©ï‰º∞‰∫ÜÂÖ©ÂÄãÊúÄÂÖàÈÄ≤ÁöÑÂØ¶‰æãÂàÜÂâ≤Ê®°ÂûãÔºåYOLO-v7 ÂØ¶‰æãÂàÜÂâ≤Âíå Mask R-CNNÔºåÈÄöÈÅéÂπæ‰ΩïÂíåÂü∫ÊñºÈ°èËâ≤ÁöÑËΩâÊèõÊì¥Â±ïÂà∞ 10,995 ÂºµÂúñÂÉèÔºå‰ª•Â¢ûÂº∑È≠ØÊ£íÊÄß„ÄÇ‰ΩøÁî®ÂàÜÁÇ∫ 90% Ë®ìÁ∑¥ÈõÜ„ÄÅÈ©óË≠âÂíåÊ∏¨Ë©¶ÈõÜ 10% ÁöÑÊï∏ÊìöÈõÜË®ìÁ∑¥ÂíåÈ©óË≠âÊ®°Âûã„ÄÇ‰ΩøÁî®Á≤æÁ¢∫Â∫¶„ÄÅÂè¨ÂõûÁéá„ÄÅÂπ≥ÂùáÂπ≥ÂùáÁ≤æÁ¢∫Â∫¶ (mAP@0.5) ÂíåÊØèÁßíÂπÄÊï∏ (FPS) Á≠âÊÄßËÉΩÊåáÊ®ôÈÄ≤Ë°åË©ï‰º∞„ÄÇYOLO-v7 ÈÅîÂà∞‰∫Ü 96.1% ÁöÑÂÑ™Áï∞ mAP@0.5Ôºå‰∏¶ËôïÁêÜ‰∫Ü 40 FPSÔºåÂÑ™Êñº Mask R-CNNÔºåÂæåËÄÖ‰ª• 18 FPS ÁöÑËºÉÊÖ¢ËôïÁêÜÈÄüÂ∫¶ÈÅîÂà∞‰∫Ü 92.1% ÁöÑ mAP@0.5„ÄÇÁ†îÁ©∂ÁµêÊûúÊé®Ëñ¶‰ΩøÁî® YOLO-v7 ÂØ¶‰æãÂàÜÂâ≤Ê®°ÂûãÈÄ≤Ë°åÂØ¶ÊôÇ„ÄÅÈ´òÈÄüÁµêÊßãÂÅ•Â∫∑Áõ£Ê∏¨ÔºåËÄå Mask R-CNN Êõ¥ÈÅ©ÂêàË©≥Á¥∞ÁöÑÈõ¢Á∑öË©ï‰º∞„ÄÇÊú¨Á†îÁ©∂Â±ïÁ§∫‰∫ÜÊ∑±Â∫¶Â≠∏ÁøíÂú®Âü∫Á§éË®≠ÊñΩÁ∂≠Ë≠∑ÊñπÈù¢ÂÖ∑ÊúâÈù©ÂëΩÊÄßÁöÑÊΩõÂäõÔºåÁÇ∫Ëá™ÂãïÊêçÂ£ûÊ™¢Ê∏¨Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØÊì¥Â±ï‰∏îÈ´òÊïàÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **GL-ICNN: An End-To-End Interpretable Convolutional Neural Network for the Diagnosis and Prediction of Alzheimer's Disease**
2501.11715v1 by Wenjie Kang, Lize Jiskoot, Peter De Deyn, Geert Biessels, Huiberdina Koek, Jurgen Claassen, Huub Middelkoop, Wiesje Flier, Willemijn J. Jansen, Stefan Klein, Esther Bron

Deep learning methods based on Convolutional Neural Networks (CNNs) have
shown great potential to improve early and accurate diagnosis of Alzheimer's
disease (AD) dementia based on imaging data. However, these methods have yet to
be widely adopted in clinical practice, possibly due to the limited
interpretability of deep learning models. The Explainable Boosting Machine
(EBM) is a glass-box model but cannot learn features directly from input
imaging data. In this study, we propose a novel interpretable model that
combines CNNs and EBMs for the diagnosis and prediction of AD. We develop an
innovative training strategy that alternatingly trains the CNN component as a
feature extractor and the EBM component as the output block to form an
end-to-end model. The model takes imaging data as input and provides both
predictions and interpretable feature importance measures. We validated the
proposed model on the Alzheimer's Disease Neuroimaging Initiative (ADNI)
dataset and the Health-RI Parelsnoer Neurodegenerative Diseases Biobank (PND)
as an external testing set. The proposed model achieved an area-under-the-curve
(AUC) of 0.956 for AD and control classification, and 0.694 for the prediction
of conversion of mild cognitive impairment (MCI) to AD on the ADNI cohort. The
proposed model is a glass-box model that achieves a comparable performance with
other state-of-the-art black-box models. Our code is publicly available at:
https://anonymous.4open.science/r/GL-ICNN.

ÊëòË¶ÅÔºö<paragraph>Âü∫ÊñºÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÂ∑≤È°ØÁ§∫Âá∫Ê•µÂ§ßÁöÑÊΩõÂäõÔºåÂèØÊ†πÊìöÂΩ±ÂÉèË≥áÊñôÊîπÂñÑÈòøËå≤Êµ∑ÈªòÁóá (AD) Â§±Êô∫ÁóáÁöÑÊó©ÊúüÊ∫ñÁ¢∫Ë®∫Êñ∑„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂ∞öÊú™Âª£Ê≥õÊáâÁî®ÊñºËá®Â∫äÂØ¶Âãô‰∏≠ÔºåÈÄôÂèØËÉΩÊòØÁî±ÊñºÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÊúâÈôê„ÄÇÂèØËß£ÈáãÊèêÂçáÊ©ü (EBM) ÊòØÂÄãÁéªÁíÉÁõíÊ®°ÂûãÔºå‰ΩÜÁÑ°Ê≥ïÁõ¥Êé•ÂæûËº∏ÂÖ•ÂΩ±ÂÉèË≥áÊñô‰∏≠Â≠∏ÁøíÁâπÂæµ„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁµêÂêà CNN Âíå EBM ÁöÑÊñ∞ÂèØËß£ÈáãÊ®°ÂûãÔºåÁî®ÊñºË®∫Êñ∑ÂíåÈ†êÊ∏¨ AD„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑË®ìÁ∑¥Á≠ñÁï•Ôºå‰∫§ÊõøË®ìÁ∑¥ CNN ÁµÑ‰ª∂‰ΩúÁÇ∫ÁâπÂæµËêÉÂèñÂô®Ôºå‰∏¶Ë®ìÁ∑¥ EBM ÁµÑ‰ª∂‰ΩúÁÇ∫Ëº∏Âá∫ÂçÄÂ°äÔºå‰ª•ÂΩ¢ÊàêÁ´ØÂ∞çÁ´ØÊ®°Âûã„ÄÇÊ≠§Ê®°ÂûãÂ∞áÂΩ±ÂÉèË≥áÊñô‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰∏¶Êèê‰æõÈ†êÊ∏¨ÂíåÂèØËß£ÈáãÁöÑÁâπÂæµÈáçË¶ÅÊÄßÊ∏¨Èáè„ÄÇÊàëÂÄëÂú®ÈòøËå≤Êµ∑ÈªòÁóáÁ•ûÁ∂ìÂΩ±ÂÉèÂÄ°Ë≠∞ (ADNI) Ë≥áÊñôÈõÜÂíå Health-RI Parelsnoer Á•ûÁ∂ìÈÄÄÂåñÁñæÁóÖÁîüÁâ©Ë≥áÊñôÂ∫´ (PND) ‰∏äÈ©óË≠â‰∫ÜÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÔºå‰ΩúÁÇ∫Â§ñÈÉ®Ê∏¨Ë©¶ÈõÜ„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂú® AD ÂíåÂ∞çÁÖßÂàÜÈ°û‰∏≠ÈÅîÂà∞‰∫Ü 0.956 ÁöÑÊõ≤Á∑ö‰∏ãÈù¢Á©ç (AUC)Ôºå‰∏¶Âú® ADNI ÈöäÂàó‰∏≠È†êÊ∏¨ËºïÂ∫¶Ë™çÁü•ÈöúÁ§ô (MCI) ËΩâÂåñÁÇ∫ AD ÊôÇÈÅîÂà∞‰∫Ü 0.694„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÊòØ‰∏ÄÂÄãÁéªÁíÉÁõíÊ®°ÂûãÔºåÂÖ∂ÊïàËÉΩËàáÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑÈªëÁõíÊ®°ÂûãÁõ∏Áï∂„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂÖ¨ÈñãÂèñÂæóÔºöhttps://anonymous.4open.science/r/GL-ICNN„ÄÇ</paragraph>

##### **Human services organizations and the responsible integration of AI: Considering ethics and contextualizing risk(s)**
2501.11705v1 by Brian E. Perron, Lauri Goldkind, Zia Qi, Bryan G. Victor

This paper examines the responsible integration of artificial intelligence
(AI) in human services organizations (HSOs), proposing a nuanced framework for
evaluating AI applications across multiple dimensions of risk. The authors
argue that ethical concerns about AI deployment -- including professional
judgment displacement, environmental impact, model bias, and data laborer
exploitation -- vary significantly based on implementation context and specific
use cases. They challenge the binary view of AI adoption, demonstrating how
different applications present varying levels of risk that can often be
effectively managed through careful implementation strategies. The paper
highlights promising solutions, such as local large language models, that can
facilitate responsible AI integration while addressing common ethical concerns.
The authors propose a dimensional risk assessment approach that considers
factors like data sensitivity, professional oversight requirements, and
potential impact on client wellbeing. They conclude by outlining a path forward
that emphasizes empirical evaluation, starting with lower-risk applications and
building evidence-based understanding through careful experimentation. This
approach enables organizations to maintain high ethical standards while
thoughtfully exploring how AI might enhance their capacity to serve clients and
communities effectively.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ÊÖß (AI) Âú®‰∫∫È°ûÊúçÂãôÁµÑÁπî (HSO) ‰∏≠Ë≤†Ë≤¨‰ªªÁöÑÊï¥ÂêàÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ¥∞Á∑ªÁöÑÊ°ÜÊû∂ÔºåÁî®ÊñºË©ï‰º∞ AI ÊáâÁî®Âú®Â§öÂÄãÈ¢®Èö™Á∂≠Â∫¶„ÄÇ‰ΩúËÄÖË™çÁÇ∫ÔºåÂ∞ç AI ÈÉ®ÁΩ≤ÁöÑÈÅìÂæ∑ËÄÉÈáè‚Äî‚ÄîÂåÖÊã¨Â∞àÊ•≠Âà§Êñ∑ÁöÑÂèñ‰ª£„ÄÅÁí∞Â¢ÉÂΩ±Èüø„ÄÅÊ®°ÂûãÂÅèÂ∑ÆÂíåË≥áÊñôÂ∑•‰ΩúËÄÖÁöÑÂâùÂâä‚Äî‚ÄîÊúÉÊ†πÊìöÂØ¶ÊñΩËÉåÊôØÂíåÂÖ∑È´î‰ΩøÁî®Ê°à‰æãËÄåÊúâÈ°ØËëóÁöÑ‰∏çÂêå„ÄÇ‰ªñÂÄëÊåëÊà∞‰∫Ü AI Êé°Áî®‰∫åÂÖÉË´ñÁöÑËßÄÈªûÔºåË™™Êòé‰∫Ü‰∏çÂêåÁöÑÊáâÁî®Â¶Ç‰ΩïÂëàÁèæ‰∏çÂêåÁ®ãÂ∫¶ÁöÑÈ¢®Èö™ÔºåËÄåÈÄô‰∫õÈ¢®Èö™ÈÄöÂ∏∏ÂèØ‰ª•ÈÄèÈÅé‰ªîÁ¥∞ÁöÑÂØ¶ÊñΩÁ≠ñÁï•‰æÜÊúâÊïàÁÆ°ÁêÜ„ÄÇÊú¨ÊñáÈáçÈªû‰ªãÁ¥π‰∫ÜÊúâÂâçÊôØÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰æãÂ¶ÇÊú¨Âú∞Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåÂÆÉÂèØ‰ª•Âú®Ëß£Ê±∫Â∏∏Ë¶ãÁöÑÈÅìÂæ∑ÂïèÈ°åÁöÑÂêåÊôÇÔºå‰øÉÈÄ≤Ë≤†Ë≤¨‰ªªÁöÑ AI Êï¥Âêà„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ∂≠Â∫¶È¢®Èö™Ë©ï‰º∞ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïËÄÉÊÖÆ‰∫ÜË≥áÊñôÊïèÊÑüÂ∫¶„ÄÅÂ∞àÊ•≠Áõ£Áù£ÈúÄÊ±ÇÂíåÂ∞çÂÆ¢Êà∂Á¶èÁ•âÁöÑÊΩõÂú®ÂΩ±ÈüøÁ≠âÂõ†Á¥†„ÄÇ‰ªñÂÄëÊúÄÂæåÊ¶ÇËø∞‰∫Ü‰∏ÄÊ¢ùÂâçÈÄ≤ÁöÑÈÅìË∑ØÔºåÂº∑Ë™øÂØ¶Ë≠âË©ï‰º∞ÔºåÂæû‰ΩéÈ¢®Èö™ÊáâÁî®ÈñãÂßãÔºå‰∏¶ÈÄèÈÅé‰ªîÁ¥∞ÁöÑÂØ¶È©óÂª∫Á´ãÂü∫ÊñºË≠âÊìöÁöÑÁêÜËß£„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰ΩøÁµÑÁπîËÉΩÂ§†Âú®Ê∑±ÊÄùÁÜüÊÖÆÂú∞Êé¢Ë®é AI Â¶Ç‰ΩïÂ¢ûÂº∑ÂÖ∂ÊúâÊïàÊúçÂãôÂÆ¢Êà∂ÂíåÁ§æÁæ§ÁöÑËÉΩÂäõÁöÑÂêåÊôÇÔºåÁ∂≠ÊåÅÈ´òÈÅìÂæ∑Ê®ôÊ∫ñ„ÄÇ

##### **Spatially-Delineated Domain-Adapted AI Classification: An Application for Oncology Data**
2501.11695v1 by Majid Farhadloo, Arun Sharma, Alexey Leontovich, Svetomir N. Markovic, Shashi Shekhar

Given multi-type point maps from different place-types (e.g., tumor regions),
our objective is to develop a classifier trained on the source place-type to
accurately distinguish between two classes of the target place-type based on
their point arrangements. This problem is societally important for many
applications, such as generating clinical hypotheses for designing new
immunotherapies for cancer treatment. The challenge lies in the spatial
variability, the inherent heterogeneity and variation observed in spatial
properties or arrangements across different locations (i.e., place-types).
Previous techniques focus on self-supervised tasks to learn domain-invariant
features and mitigate domain differences; however, they often neglect the
underlying spatial arrangements among data points, leading to significant
discrepancies across different place-types. We explore a novel multi-task
self-learning framework that targets spatial arrangements, such as spatial
mix-up masking and spatial contrastive predictive coding, for
spatially-delineated domain-adapted AI classification. Experimental results on
real-world datasets (e.g., oncology data) show that the proposed framework
provides higher prediction accuracy than baseline methods.

ÊëòË¶ÅÔºöÂæû‰∏çÂêåÈ°ûÂûãÁöÑÈªûÂúñÔºà‰æãÂ¶ÇÔºåËÖ´Áò§ÂçÄÂüüÔºâ‰∏≠Áµ¶ÂÆöÂ§öÈ°ûÂûãÈªûÂúñÔºå
ÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÈñãÁôº‰∏ÄÂÄãÂú®‰æÜÊ∫êÈ°ûÂûã‰∏äË®ìÁ∑¥ÁöÑÂàÜÈ°ûÂô®Ôºå‰ª•
Ê†πÊìöÂÖ∂ÈªûÊéíÂàóÊ∫ñÁ¢∫ÂçÄÂàÜÁõÆÊ®ôÈ°ûÂûã‰∏≠ÁöÑÂÖ©È°û„ÄÇÈÄôÂÄãÂïèÈ°åÂ∞çÊñºË®±Â§ö
ÊáâÁî®‰æÜË™™ÂÖ∑ÊúâÁ§æÊúÉÈáçË¶ÅÊÄßÔºå‰æãÂ¶ÇÁÇ∫ÁôåÁóáÊ≤ªÁôÇË®≠Ë®àÊñ∞ÁöÑÂÖçÁñ´ÁôÇÊ≥ïËÄåÁîüÊàêËá®Â∫äÂÅáË®≠„ÄÇÊåëÊà∞Âú®ÊñºÁ©∫Èñì
ËÆäÁï∞ÊÄß„ÄÅÂõ∫ÊúâÁöÑÁï∞Ë≥™ÊÄßÂíåÂú®‰∏çÂêå‰ΩçÁΩÆÔºàÂç≥È°ûÂûãÔºâ‰∏≠ËßÄÂØüÂà∞ÁöÑÁ©∫Èñì
Â±¨ÊÄßÊàñÊéíÂàóÁöÑËÆäÂåñ„ÄÇÂÖàÂâçÁöÑÊäÄË°ìÂ∞àÊ≥®ÊñºËá™Áõ£Áù£‰ªªÂãô‰ª•Â≠∏Áøí‰∏çËÆäÈ†òÂüü
ÁâπÂæµ‰∏¶Ê∏õËºïÈ†òÂüüÂ∑ÆÁï∞ÔºõÁÑ∂ËÄåÔºåÂÆÉÂÄëÈÄöÂ∏∏ÂøΩË¶ñÊï∏ÊìöÈªû‰πãÈñìÁöÑ
Â∫ïÂ±§Á©∫ÈñìÊéíÂàóÔºåÂ∞éËá¥‰∏çÂêåÈ°ûÂûã‰πãÈñìÂ≠òÂú®È°ØËëóÂ∑ÆÁï∞„ÄÇÊàëÂÄëÊé¢Á¥¢‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂ§ö‰ªªÂãô
Ëá™Â≠∏ÁøíÊ°ÜÊû∂Ôºå‰ª•ÈáùÂ∞çÁ©∫ÈñìÊéíÂàóÔºå‰æãÂ¶ÇÁ©∫ÈñìÊ∑∑ÂêàÊé©ËîΩÂíåÁ©∫ÈñìÂ∞çÊØîÈ†êÊ∏¨Á∑®Á¢ºÔºåÁî®Êñº
Á©∫ÈñìÂäÉÂàÜÁöÑÈ†òÂüüÈÅ©Êáâ AI ÂàÜÈ°û„ÄÇÂú®
ÁúüÂØ¶‰∏ñÁïåÊï∏ÊìöÈõÜÔºà‰æãÂ¶ÇÔºåËÖ´Áò§Â≠∏Êï∏ÊìöÔºâ‰∏äÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂
Êèê‰æõÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶È´òÊñºÂü∫Á∑öÊñπÊ≥ï„ÄÇ

##### **Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects on Task Performance and Fairness**
2501.13120v1 by Ambreesh Parthasarathy, Chandrasekar Subramanian, Ganesh Senrayan, Shreyash Adappanavar, Aparna Taneja, Balaraman Ravindran, Milind Tambe

Restless Multi-Armed Bandits (RMABs) have been successfully applied to
resource allocation problems in a variety of settings, including public health.
With the rapid development of powerful large language models (LLMs), they are
increasingly used to design reward functions to better match human preferences.
Recent work has shown that LLMs can be used to tailor automated allocation
decisions to community needs using language prompts. However, this has been
studied primarily for English prompts and with a focus on task performance
only. This can be an issue since grassroots workers, especially in developing
countries like India, prefer to work in local languages, some of which are
low-resource. Further, given the nature of the problem, biases along population
groups unintended by the user are also undesirable. In this work, we study the
effects on both task performance and fairness when the DLM algorithm, a recent
work on using LLMs to design reward functions for RMABs, is prompted with
non-English language commands. Specifically, we run the model on a synthetic
environment for various prompts translated into multiple languages. The prompts
themselves vary in complexity. Our results show that the LLM-proposed reward
functions are significantly better when prompted in English compared to other
languages. We also find that the exact phrasing of the prompt impacts task
performance. Further, as prompt complexity increases, performance worsens for
all languages; however, it is more robust with English prompts than with
lower-resource languages. On the fairness side, we find that low-resource
languages and more complex prompts are both highly likely to create unfairness
along unintended dimensions.

ÊëòË¶ÅÔºö<paragraph>‰∏çÂÆâÂàÜÁöÑÂ§öËáÇË≥≠Âæí (RMAB) Â∑≤ÊàêÂäüÊáâÁî®ÊñºÂêÑÁ®ÆÁí∞Â¢É‰∏≠ÁöÑË≥áÊ∫êÂàÜÈÖçÂïèÈ°åÔºåÂåÖÊã¨ÂÖ¨ÂÖ±Ë°õÁîü„ÄÇÈö®ËëóÂº∑Â§ßÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÁôºÂ±ïÔºåÂÆÉÂÄëÊ≠£Ë∂ä‰æÜË∂äÂ§öÂú∞Áî®ÊñºË®≠Ë®àÁçéÂãµÂáΩÊï∏Ôºå‰ª•Êõ¥Â•ΩÂú∞ÂåπÈÖç‰∫∫È°ûÂÅèÂ•Ω„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåLLM ÂèØÁî®Êñº‰ΩøÁî®Ë™ûË®ÄÊèêÁ§∫Ê†πÊìöÁ§æÂçÄÈúÄÊ±ÇË™øÊï¥Ëá™ÂãïÂàÜÈÖçÊ±∫Á≠ñ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∏ªË¶ÅÈáùÂ∞çËã±Ë™ûÊèêÁ§∫ÈÄ≤Ë°å‰∫ÜÁ†îÁ©∂Ôºå‰∏¶‰∏îÂÉÖÈóúÊ≥®‰ªªÂãôÁ∏æÊïà„ÄÇÈÄôÂèØËÉΩÊòØ‰∏ÄÂÄãÂïèÈ°åÔºåÂõ†ÁÇ∫Âü∫Â±§Â∑•‰ΩúËÄÖÔºåÁâπÂà•ÊòØÂÉèÂç∞Â∫¶ÈÄôÊ®£ÁöÑÁôºÂ±ï‰∏≠ÂúãÂÆ∂ÁöÑÂ∑•‰ΩúËÄÖÔºåÊõ¥È°òÊÑè‰ΩøÁî®Áï∂Âú∞Ë™ûË®ÄÔºåÂÖ∂‰∏≠‰∏Ä‰∫õË™ûË®ÄÊòØ‰ΩéË≥áÊ∫êÁöÑ„ÄÇÊ≠§Â§ñÔºåÈëëÊñºÂïèÈ°åÁöÑÊÄßË≥™ÔºåÁî®Êà∂ÁÑ°ÊÑè‰∏≠Â∞ç‰∫∫Âè£Áæ§È´îÁî¢ÁîüÁöÑÂÅèË¶ã‰πüÊòØ‰∏çÂèóÊ≠°ËøéÁöÑ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÁï∂ DLM ÊºîÁÆóÊ≥ïÔºàÊúÄËøë‰ΩøÁî® LLM ÁÇ∫ RMAB Ë®≠Ë®àÁçéÂãµÂáΩÊï∏ÁöÑÂ∑•‰ΩúÔºâÊî∂Âà∞ÈùûËã±Ë™ûË™ûË®ÄÂëΩ‰ª§ÊôÇÔºåÂ∞ç‰ªªÂãôÁ∏æÊïàÂíåÂÖ¨Âπ≥ÊÄßÁöÑÂΩ±Èüø„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂú®ÂêàÊàêÁí∞Â¢É‰∏≠ÈÅãË°åÊ®°ÂûãÔºåÂ∞çÁøªË≠ØÊàêÂ§öÁ®ÆË™ûË®ÄÁöÑÂêÑÁ®ÆÊèêÁ§∫ÈÄ≤Ë°åÈÅãË°å„ÄÇÊèêÁ§∫Êú¨Ë∫´ÁöÑË§áÈõúÊÄßÂêÑ‰∏çÁõ∏Âêå„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåËàáÂÖ∂‰ªñË™ûË®ÄÁõ∏ÊØîÔºåÁî®Ëã±Ë™ûÊèêÁ§∫ÊôÇÔºåLLM ÊèêÂá∫ÁöÑÁçéÂãµÂáΩÊï∏È°ØËëóÊõ¥Â•Ω„ÄÇÊàëÂÄëÈÇÑÁôºÁèæÊèêÁ§∫ÁöÑÁ¢∫ÂàáÊé™Ëæ≠ÊúÉÂΩ±Èüø‰ªªÂãôÁ∏æÊïà„ÄÇÊ≠§Â§ñÔºåÈö®ËëóÊèêÁ§∫Ë§áÈõúÊÄßÁöÑÂ¢ûÂä†ÔºåÊâÄÊúâË™ûË®ÄÁöÑÊÄßËÉΩÈÉΩÊúÉ‰∏ãÈôçÔºõÁÑ∂ËÄåÔºåÂÆÉÊØî‰ΩéË≥áÊ∫êË™ûË®ÄÊõ¥ÂÅ•Â£Ø„ÄÇÂú®ÂÖ¨Âπ≥ÊÄßÊñπÈù¢ÔºåÊàëÂÄëÁôºÁèæ‰ΩéË≥áÊ∫êË™ûË®ÄÂíåÊõ¥Ë§áÈõúÁöÑÊèêÁ§∫ÈÉΩÊ•µÊúâÂèØËÉΩÂú®ÊÑèÂ§ñÁöÑÁ∂≠Â∫¶‰∏äÈÄ†Êàê‰∏çÂÖ¨Âπ≥„ÄÇ</paragraph>

##### **Biomedical Knowledge Graph: A Survey of Domains, Tasks, and Real-World Applications**
2501.11632v2 by Yuxing Lu, Sin Yee Goi, Xukai Zhao, Jinzhuo Wang

Biomedical knowledge graphs (BKGs) have emerged as powerful tools for
organizing and leveraging the vast and complex data found across the biomedical
field. Yet, current reviews of BKGs often limit their scope to specific domains
or methods, overlooking the broader landscape and the rapid technological
progress reshaping it. In this survey, we address this gap by offering a
systematic review of BKGs from three core perspectives: domains, tasks, and
applications. We begin by examining how BKGs are constructed from diverse data
sources, including molecular interactions, pharmacological datasets, and
clinical records. Next, we discuss the essential tasks enabled by BKGs,
focusing on knowledge management, retrieval, reasoning, and interpretation.
Finally, we highlight real-world applications in precision medicine, drug
discovery, and scientific research, illustrating the translational impact of
BKGs across multiple sectors. By synthesizing these perspectives into a unified
framework, this survey not only clarifies the current state of BKG research but
also establishes a foundation for future exploration, enabling both innovative
methodological advances and practical implementations.

ÊëòË¶ÅÔºöÁîüÁâ©ÂåªÂ≠¶Áü•ËØÜÂõæË∞±ÔºàBKGÔºâÂ∑≤Êàê‰∏∫ÁªÑÁªáÂíåÂà©Áî®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüü‰∏≠ÂèëÁé∞ÁöÑÂ∫ûÂ§ß‰∏îÂ§çÊùÇÊï∞ÊçÆÁöÑÂº∫Â§ßÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÂØπ BKG ÁöÑÂÆ°Êü•ÈÄöÂ∏∏Â∞ÜÂÖ∂ËåÉÂõ¥ÈôêÂà∂Âú®ÁâπÂÆöÈ¢ÜÂüüÊàñÊñπÊ≥ïÔºåÂøΩËßÜ‰∫ÜÊõ¥ÂπøÊ≥õÁöÑÊ†ºÂ±ÄÂíåÊ≠£Âú®ÈáçÂ°ëÂÆÉÁöÑÂø´ÈÄüÊäÄÊúØËøõÊ≠•„ÄÇÂú®ËøôÈ°πË∞ÉÊü•‰∏≠ÔºåÊàë‰ª¨ÈÄöËøá‰ªé‰∏â‰∏™Ê†∏ÂøÉËßíÂ∫¶ÔºàÈ¢ÜÂüü„ÄÅ‰ªªÂä°ÂíåÂ∫îÁî®ÔºâÂØπ BKG ËøõË°åÁ≥ªÁªüÂÆ°Êü•Êù•Ëß£ÂÜ≥Ëøô‰∏ÄÂ∑ÆË∑ù„ÄÇÊàë‰ª¨È¶ñÂÖàÊ£ÄÊü•Â¶Ç‰Ωï‰ªéÂåÖÊã¨ÂàÜÂ≠êÁõ∏‰∫í‰ΩúÁî®„ÄÅËçØÁêÜÊï∞ÊçÆÈõÜÂíå‰∏¥Â∫äËÆ∞ÂΩïÂú®ÂÜÖÁöÑÂêÑÁßçÊï∞ÊçÆÊ∫êÊûÑÂª∫ BKG„ÄÇÊé•‰∏ãÊù•ÔºåÊàë‰ª¨ËÆ®ËÆ∫ BKG ÂêØÁî®ÁöÑÂü∫Êú¨‰ªªÂä°ÔºåÈáçÁÇπÂÖ≥Ê≥®Áü•ËØÜÁÆ°ÁêÜ„ÄÅÊ£ÄÁ¥¢„ÄÅÊé®ÁêÜÂíåËß£Èáä„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÈáçÁÇπ‰ªãÁªç‰∫ÜÁ≤æÂáÜÂåªÁñó„ÄÅËçØÁâ©ÂèëÁé∞ÂíåÁßëÂ≠¶Á†îÁ©∂‰∏≠ÁöÑÂÆûÈôÖÂ∫îÁî®ÔºåËØ¥Êòé‰∫Ü BKG Âú®Â§ö‰∏™È¢ÜÂüüÁöÑËΩ¨ÂåñÂΩ±Âìç„ÄÇÈÄöËøáÂ∞ÜËøô‰∫õËßÇÁÇπÁªºÂêàÂà∞‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂‰∏≠ÔºåÊú¨Ë∞ÉÊü•‰∏ç‰ªÖÈòêÊòé‰∫Ü BKG Á†îÁ©∂ÁöÑÁé∞Áä∂ÔºåËøò‰∏∫Êú™Êù•ÁöÑÊé¢Á¥¢Â•†ÂÆö‰∫ÜÂü∫Á°ÄÔºåÊó¢‰øÉËøõ‰∫ÜÂàõÊñ∞ÊñπÊ≥ïÁöÑËøõÊ≠•Ôºå‰πü‰øÉËøõ‰∫ÜÂÆûÈôÖÂÆûÊñΩ„ÄÇ

##### **Training-free Ultra Small Model for Universal Sparse Reconstruction in Compressed Sensing**
2501.11592v2 by Chaoqing Tang, Huanze Zhuang, Guiyun Tian, Zhenli Zeng, Yi Ding, Wenzhong Liu, Xiang Bai

Pre-trained large models attract widespread attention in recent years, but
they face challenges in applications that require high interpretability or have
limited resources, such as physical sensing, medical imaging, and
bioinformatics. Compressed Sensing (CS) is a well-proved theory that drives
many recent breakthroughs in these applications. However, as a typical
under-determined linear system, CS suffers from excessively long sparse
reconstruction times when using traditional iterative methods, particularly
with large-scale data. Current AI methods like deep unfolding fail to
substitute them because pre-trained models exhibit poor generality beyond their
training conditions and dataset distributions, or lack interpretability.
Instead of following the big model fervor, this paper proposes ultra-small
artificial neural models called coefficients learning (CL), enabling
training-free and rapid sparse reconstruction while perfectly inheriting the
generality and interpretability of traditional iterative methods, bringing new
feature of incorporating prior knowledges. In CL, a signal of length $n$ only
needs a minimal of $n$ trainable parameters. A case study model called CLOMP is
implemented for evaluation. Experiments are conducted on both synthetic and
real one-dimensional and two-dimensional signals, demonstrating significant
improvements in efficiency and accuracy. Compared to representative iterative
methods, CLOMP improves efficiency by 100 to 1000 folds for large-scale data.
Test results on eight diverse image datasets indicate that CLOMP improves
structural similarity index by 292%, 98%, 45% for sampling rates of 0.1, 0.3,
0.5, respectively. We believe this method can truly usher CS reconstruction
into the AI era, benefiting countless under-determined linear systems that rely
on sparse solution.

ÊëòË¶ÅÔºö<paragraph>È†êË®ìÁ∑¥Â§ßÂûãÊ®°ÂûãËøëÂπ¥‰æÜÂª£ÂèóÈóúÊ≥®Ôºå‰ΩÜÂÆÉÂÄëÂú®ÈúÄË¶ÅÈ´òÂèØËß£ÈáãÊÄßÊàñË≥áÊ∫êÂèóÈôêÁöÑÊáâÁî®‰∏≠Èù¢Ëá®ÊåëÊà∞Ôºå‰æãÂ¶ÇÁâ©ÁêÜÊÑüÊ∏¨„ÄÅÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÁîüÁâ©Ë≥áË®äÂ≠∏„ÄÇÂ£ìÁ∏ÆÊÑüÊ∏¨ (CS) ÊòØ‰∏ÄÂÄãÁ∂ìÈÅéÈ©óË≠âÁöÑÁêÜË´ñÔºåÊé®Âãï‰∫ÜÈÄô‰∫õÊáâÁî®‰∏≠ÁöÑË®±Â§öËøëÊúüÁ™ÅÁ†¥„ÄÇÁÑ∂ËÄåÔºå‰ΩúÁÇ∫‰∏ÄÂÄãÂÖ∏ÂûãÁöÑÊ¨†ÂÆöÁ∑öÊÄßÁ≥ªÁµ±ÔºåCS Âú®‰ΩøÁî®ÂÇ≥Áµ±Ëø≠‰ª£ÊñπÊ≥ïÊôÇÊúÉÂ∞éËá¥ÈÅéÈï∑ÁöÑÁ®ÄÁñèÈáçÂª∫ÊôÇÈñìÔºåÁâπÂà•ÊòØÂú®Â§ßË¶èÊ®°Ë≥áÊñôÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÂÉèÊ∑±Â∫¶Â±ïÈñãÁ≠âÁï∂Ââç AI ÊñπÊ≥ïÁÑ°Ê≥ïÂèñ‰ª£ÂÆÉÂÄëÔºåÂõ†ÁÇ∫È†êË®ìÁ∑¥Ê®°ÂûãÂú®Ë®ìÁ∑¥Ê¢ù‰ª∂ÂíåË≥áÊñôÈõÜÂàÜ‰Ωà‰πãÂ§ñË°®ÁèæÂá∫ËºÉÂ∑ÆÁöÑÊ¶ÇÊã¨ÊÄßÔºåÊàñÁº∫‰πèÂèØËß£ÈáãÊÄß„ÄÇÊú¨Ë´ñÊñáÊ≤íÊúâËøΩÈö®Â§ßÂûãÊ®°ÂûãÁÜ±ÊΩÆÔºåËÄåÊòØÊèêÂá∫‰∫ÜÁ®±ÁÇ∫‰øÇÊï∏Â≠∏Áøí (CL) ÁöÑË∂ÖÂ∞èÂûã‰∫∫Â∑•Á•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÔºåÂØ¶ÁèæÁÑ°Ë®ìÁ∑¥‰∏îÂø´ÈÄüÁöÑÁ®ÄÁñèÈáçÂª∫ÔºåÂêåÊôÇÂÆåÁæéÁπºÊâøÂÇ≥Áµ±Ëø≠‰ª£ÊñπÊ≥ïÁöÑÊ¶ÇÊã¨ÊÄßÂíåÂèØËß£ÈáãÊÄßÔºåÂ∏∂‰æÜÁµêÂêàÂÖàÈ©óÁü•Ë≠òÁöÑÊñ∞ÁâπÈªû„ÄÇÂú® CL ‰∏≠ÔºåÈï∑Â∫¶ÁÇ∫ $n$ ÁöÑ‰ø°ËôüÂè™ÈúÄË¶ÅÊúÄÂ∞ë $n$ ÂÄãÂèØË®ìÁ∑¥ÂèÉÊï∏„ÄÇÂØ¶‰Ωú‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ CLOMP ÁöÑÊ°à‰æãÁ†îÁ©∂Ê®°ÂûãÈÄ≤Ë°åË©ï‰º∞„ÄÇÂú®ÂêàÊàêÂíåÁúüÂØ¶ÁöÑ‰∏ÄÁ∂≠Âíå‰∫åÁ∂≠‰ø°Ëôü‰∏äÈÄ≤Ë°å‰∫ÜÂØ¶È©óÔºåË≠âÊòé‰∫ÜÊïàÁéáÂíåÊ∫ñÁ¢∫ÊÄßÁöÑÈ°ØËëóÊèêÂçá„ÄÇËàáÂÖ∑‰ª£Ë°®ÊÄßÁöÑËø≠‰ª£ÊñπÊ≥ïÁõ∏ÊØîÔºåCLOMP Â∞áÂ§ßÂûãË≥áÊñôÁöÑÊïàÁéáÊèêÂçá‰∫Ü 100 Âà∞ 1000 ÂÄç„ÄÇÂú®ÂÖ´ÂÄã‰∏çÂêåÁöÑÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏äÁöÑÊ∏¨Ë©¶ÁµêÊûúË°®ÊòéÔºåCLOMP ÂàÜÂà•Â∞áÊé°Ê®£ÁéáÁÇ∫ 0.1„ÄÅ0.3„ÄÅ0.5 ÁöÑÁµêÊßãÁõ∏‰ººÊÄßÊåáÊ®ôÊèêÂçá‰∫Ü 292%„ÄÅ98%„ÄÅ45%„ÄÇÊàëÂÄëÁõ∏‰ø°ÈÄôÁ®ÆÊñπÊ≥ïÂèØ‰ª•ÁúüÊ≠£Â∞á CS ÈáçÂª∫Â∏∂ÂÖ• AI ÊôÇ‰ª£Ôºå‰Ωø‰æùË≥¥Á®ÄÁñèËß£ÁöÑÁÑ°Êï∏Ê¨†ÂÆöÁ∑öÊÄßÁ≥ªÁµ±ÂèóÁõä„ÄÇ</paragraph>

##### **Enhancing Coronary Artery Calcium Scoring via Multi-Organ Segmentation on Non-Contrast Cardiac Computed Tomography**
2501.11428v1 by Jakub Nalepa, Tomasz Bartczak, Mariusz Bujny, Jaros≈Çaw Go≈õli≈Ñski, Katarzyna Jesionek, Wojciech Malara, Filip Malawski, Karol Miszalski-Jamka, Patrycja Rewa, Marcin Kostur

Despite coronary artery calcium scoring being considered a largely solved
problem within the realm of medical artificial intelligence, this paper argues
that significant improvements can still be made. By shifting the focus from
pathology detection to a deeper understanding of anatomy, the novel algorithm
proposed in the paper both achieves high accuracy in coronary artery calcium
scoring and offers enhanced interpretability of the results. This approach not
only aids in the precise quantification of calcifications in coronary arteries,
but also provides valuable insights into the underlying anatomical structures.
Through this anatomically-informed methodology, the paper shows how a nuanced
understanding of the heart's anatomy can lead to more accurate and
interpretable results in the field of cardiovascular health. We demonstrate the
superior accuracy of the proposed method by evaluating it on an open-source
multi-vendor dataset, where we obtain results at the inter-observer level,
surpassing the current state of the art. Finally, the qualitative analyses show
the practical value of the algorithm in such tasks as labeling coronary artery
calcifications, identifying aortic calcifications, and filtering out false
positive detections due to noise.

ÊëòË¶ÅÔºöÂÑòÁÆ°ÂÜ†ÁãÄÂãïËÑàÈà£ÂåñË©ïÂàÜÂú®ÈÜ´Â≠∏‰∫∫Â∑•Êô∫ÊÖßÈ†òÂüüË¢´Ë™çÁÇ∫ÊòØ‰∏ÄÂÄãÂ∑≤Ëß£Ê±∫ÁöÑÂïèÈ°åÔºå‰ΩÜÊú¨ÊñáË´ñË≠â‰ªçÊúâÈ°ØËëóÈÄ≤Ê≠•ÁöÑÁ©∫Èñì„ÄÇÈÄèÈÅéÂ∞áÁÑ¶ÈªûÂæûÁóÖÁêÜÊ™¢Ê∏¨ËΩâÁßªÂà∞Â∞çËß£ÂâñÁµêÊßãÁöÑÊõ¥Ê∑±ÂÖ•ÁêÜËß£ÔºåÊú¨ÊñáÊèêÂá∫ÁöÑÊñ∞ÊºîÁÆóÊ≥ïÂú®ÂÜ†ÁãÄÂãïËÑàÈà£ÂåñË©ïÂàÜ‰∏≠Áç≤ÂæóÈ´òÊ∫ñÁ¢∫Â∫¶Ôºå‰∏¶Êèê‰æõ‰∫ÜÂ¢ûÂº∑ÁöÑÁµêÊûúÂèØËß£ÈáãÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰∏çÂÉÖÊúâÂä©ÊñºÁ≤æÁ¢∫ÈáèÂåñÂÜ†ÁãÄÂãïËÑàÁöÑÈà£ÂåñÔºåÈÇÑÊèê‰æõ‰∫ÜÂ∞çÂ∫ïÂ±§Ëß£ÂâñÁµêÊßãÁöÑÂØ∂Ë≤¥Ë¶ãËß£„ÄÇÈÄèÈÅéÈÄôÁ®ÆËß£ÂâñÂ≠∏ÊñπÊ≥ïÔºåÊú¨ÊñáÂ±ïÁ§∫‰∫ÜÂ∞çÂøÉËáüËß£ÂâñÁµêÊßãÁöÑÁ¥∞Á∑ªÁêÜËß£Â¶Ç‰ΩïËÉΩÂ∞éËá¥ÂøÉË°ÄÁÆ°ÂÅ•Â∫∑È†òÂüüÊõ¥Ê∫ñÁ¢∫‰∏îÂèØËß£ÈáãÁöÑÁµêÊûú„ÄÇÊàëÂÄëÈÄèÈÅéÂú®ÈñãÊîæÂéüÂßãÁ¢ºÁöÑÂ§öÂª†ÂïÜË≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºåË≠âÊòé‰∫ÜÂÖ∂ÂÑ™Ë∂äÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÊàëÂÄëÂú®ËßÄÂØüËÄÖÈñìÂ±§Á¥öÁç≤ÂæóÁöÑÁµêÊûúË∂ÖË∂ä‰∫ÜÁõÆÂâçÁöÑÊäÄË°ìÊ∞¥Ê∫ñ„ÄÇÊúÄÂæåÔºåÂÆöÊÄßÂàÜÊûêÈ°ØÁ§∫‰∫ÜË©≤ÊºîÁÆóÊ≥ïÂú®Ê®ôË®òÂÜ†ÁãÄÂãïËÑàÈà£Âåñ„ÄÅË≠òÂà•‰∏ªÂãïËÑàÈà£Âåñ‰ª•ÂèäÈÅéÊøæÊéâÂõ†ÈõúË®äËÄåÁî¢ÁîüÁöÑÂÅáÈôΩÊÄßÂÅµÊ∏¨Á≠â‰ªªÂãô‰∏≠ÁöÑÂØ¶Áî®ÂÉπÂÄº„ÄÇ

##### **RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?**
2501.11284v1 by Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, Debing Zhang

Can scaling transform reasoning? In this work, we explore the untapped
potential of scaling Long Chain-of-Thought (Long-CoT) data to 1000k samples,
pioneering the development of a slow-thinking model, RedStar. Through extensive
experiments with various LLMs and different sizes, we uncover the ingredients
for specialization and scale for Long-CoT training. Surprisingly, even smaller
models show significant performance gains with limited data, revealing the
sample efficiency of Long-CoT and the critical role of sample difficulty in the
learning process. Our findings demonstrate that Long-CoT reasoning can be
effectively triggered with just a few thousand examples, while larger models
achieve unparalleled improvements. We also introduce reinforcement learning
(RL)-scale training as a promising direction for advancing slow-thinking
systems. RedStar shines across domains: on the MATH-Hard benchmark,
RedStar-code-math boosts performance from 66.2\% to 81.6\%, and on the USA Math
Olympiad (AIME), it solves 46.7\% of problems using only 21k mixed-code-math
datasets. In multimodal tasks like GeoQA and MathVista-GEO, RedStar-Geo
achieves competitive results with minimal Long-CoT data, outperforming other
slow-thinking systems like QvQ-Preview. Compared to QwQ, RedStar strikes the
perfect balance between reasoning and generalizability. Our work highlights
that, with careful tuning, scaling Long-CoT can unlock extraordinary reasoning
capabilities-even with limited dataset and set a new standard for slow-thinking
models across diverse challenges. Our data and models are released at
https://huggingface.co/RedStar-Reasoning.

ÊëòË¶ÅÔºö<paragraph>Á∏ÆÊîæÂèØ‰ª•ËΩâÊèõÊé®ÁêÜÂóéÔºüÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Á¥¢Â∞áÈï∑ÈèàÊÄùËÄÉÔºàLong-CoTÔºâË≥áÊñôÁ∏ÆÊîæÂà∞ 1000k ÁØÑ‰æãÁöÑÊú™ÈñãÁôºÊΩõÂäõÔºåÁéáÂÖàÈñãÁôºÊÖ¢ÊÄùËÄÉÊ®°Âûã RedStar„ÄÇÈÄèÈÅé‰ΩøÁî®ÂêÑÁ®Æ LLM Âíå‰∏çÂêåÂ§ßÂ∞èÈÄ≤Ë°åÂª£Ê≥õÂØ¶È©óÔºåÊàëÂÄëÊè≠Á§∫‰∫Ü Long-CoT Ë®ìÁ∑¥ÁöÑÂ∞àÊ•≠ÂåñÂíåË¶èÊ®°Ë¶ÅÁ¥†„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÂç≥‰ΩøËºÉÂ∞èÁöÑÊ®°ÂûãÂú®Ë≥áÊñôÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ã‰πüÂ±ïÁèæÂá∫È°ØËëóÁöÑÊïàËÉΩÊèêÂçáÔºåÊè≠Á§∫‰∫Ü Long-CoT ÁöÑÁØÑ‰æãÊïàÁéáÂíåÁØÑ‰æãÈõ£Â∫¶Âú®Â≠∏ÁøíÈÅéÁ®ã‰∏≠ÊâÆÊºîÁöÑÈóúÈçµËßíËâ≤„ÄÇÊàëÂÄëÁöÑÁôºÁèæË≠âÊòéÔºåÂè™Ë¶ÅÊúâÊï∏ÂçÉÂÄãÁØÑ‰æãÔºåÂ∞±ÂèØ‰ª•ÊúâÊïàËß∏Áôº Long-CoT Êé®ÁêÜÔºåËÄåËºÉÂ§ßÁöÑÊ®°ÂûãÂâáÂèØÁç≤ÂæóÁÑ°ËàáÂÄ´ÊØîÁöÑÊîπÈÄ≤„ÄÇÊàëÂÄëÈÇÑÂ∞éÂÖ•Âº∑ÂåñÂ≠∏Áøí (RL) Ë¶èÊ®°Ë®ìÁ∑¥Ôºå‰ΩúÁÇ∫Êé®ÈÄ≤ÊÖ¢ÊÄùËÄÉÁ≥ªÁµ±ÁöÑ‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÊñπÂêë„ÄÇRedStar Âú®ÂêÑÂÄãÈ†òÂüü‰∏≠Ë°®ÁèæÂá∫Ëâ≤ÔºöÂú® MATH-Hard Âü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåRedStar-code-math Â∞áÊïàËÉΩÂæû 66.2% ÊèêÂçáËá≥ 81.6%ÔºåËÄåÂú®ÁæéÂúãÊï∏Â≠∏Â•ßÊûóÂåπÂÖãÔºàAIMEÔºâ‰∏≠ÔºåÂÆÉÂÉÖ‰ΩøÁî® 21k ÂÄãÊ∑∑ÂêàÁ®ãÂºèÁ¢ºÊï∏Â≠∏Ë≥áÊñôÈõÜÂ∞±Ëß£Ê±∫‰∫Ü 46.7% ÁöÑÂïèÈ°å„ÄÇÂú® GeoQA Âíå MathVista-GEO Á≠âÂ§öÊ®°ÊÖã‰ªªÂãô‰∏≠ÔºåRedStar-Geo Âú® Long-CoT Ë≥áÊñôÊúÄÂ∞ëÁöÑÊÉÖÊ≥Å‰∏ãÂèñÂæóÁ´∂Áà≠ÂäõÁöÑÁµêÊûúÔºåÂÑ™ÊñºÂÖ∂‰ªñÊÖ¢ÊÄùËÄÉÁ≥ªÁµ±Ôºå‰æãÂ¶Ç QvQ-Preview„ÄÇËàá QwQ Áõ∏ÊØîÔºåRedStar Âú®Êé®ÁêÜÂíåÊ¶ÇÊã¨ÊÄß‰πãÈñìÂèñÂæó‰∫ÜÂÆåÁæéÁöÑÂπ≥Ë°°„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÈáçÈªûÂú®ÊñºÔºåÈÄèÈÅé‰ªîÁ¥∞Ë™øÊï¥ÔºåÁ∏ÆÊîæ Long-CoT ÂèØ‰ª•Ëß£ÈéñÈùûÂá°ÁöÑÊé®ÁêÜËÉΩÂäõÔºåÂç≥‰ΩøÂú®Ë≥áÊñôÈõÜÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÔºå‰πüËÉΩÁÇ∫ÂêÑÁ®ÆÊåëÊà∞Ë®≠ÂÆöÊÖ¢ÊÄùËÄÉÊ®°ÂûãÁöÑÊñ∞Ê®ôÊ∫ñ„ÄÇÊàëÂÄëÁöÑË≥áÊñôÂíåÊ®°ÂûãÂ∑≤Êñº https://huggingface.co/RedStar-Reasoning ÁôºÂ∏É„ÄÇ</paragraph>

##### **Spatiotemporal Air Quality Mapping in Urban Areas Using Sparse Sensor Data, Satellite Imagery, Meteorological Factors, and Spatial Features**
2501.11270v1 by Osama Ahmad, Zubair Khalid, Muhammad Tahir, Momin Uppal

Monitoring air pollution is crucial for protecting human health from exposure
to harmful substances. Traditional methods of air quality monitoring, such as
ground-based sensors and satellite-based remote sensing, face limitations due
to high deployment costs, sparse sensor coverage, and environmental
interferences. To address these challenges, this paper proposes a framework for
high-resolution spatiotemporal Air Quality Index (AQI) mapping using sparse
sensor data, satellite imagery, and various spatiotemporal factors. By
leveraging Graph Neural Networks (GNNs), we estimate AQI values at unmonitored
locations based on both spatial and temporal dependencies. The framework
incorporates a wide range of environmental features, including meteorological
data, road networks, points of interest (PoIs), population density, and urban
green spaces, which enhance prediction accuracy. We illustrate the use of our
approach through a case study in Lahore, Pakistan, where multi-resolution data
is used to generate the air quality index map at a fine spatiotemporal scale.

ÊëòË¶ÅÔºöÁõ£ÊéßÁ©∫Ê∞£Ê±°ÊüìÂ∞çÊñº‰øùË≠∑‰∫∫È°ûÂÅ•Â∫∑ÂÖçÊñºÊé•Ëß∏ÊúâÂÆ≥Áâ©Ë≥™Ëá≥ÈóúÈáçË¶Å„ÄÇÂÇ≥Áµ±ÁöÑÁ©∫Ê∞£ÂìÅË≥™Áõ£Ê∏¨ÊñπÊ≥ïÔºå‰æãÂ¶ÇÂú∞Èù¢ÊÑüÊ∏¨Âô®ÂíåË°õÊòüÈÅôÊ∏¨ÔºåÁî±ÊñºÈÉ®ÁΩ≤ÊàêÊú¨È´ò„ÄÅÊÑüÊ∏¨Âô®Ë¶ÜËìãÁØÑÂúçÁ®ÄÁñè‰ª•ÂèäÁí∞Â¢ÉÂπ≤ÊìæËÄåÈù¢Ëá®ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰ΩøÁî®Á®ÄÁñèÊÑüÊ∏¨Âô®Ë≥áÊñô„ÄÅË°õÊòüÂΩ±ÂÉèÂíåÂêÑÁ®ÆÊôÇÁ©∫Âõ†Â≠ê‰æÜÁπ™Ë£ΩÈ´òËß£ÊûêÂ∫¶ÊôÇÁ©∫Á©∫Ê∞£ÂìÅË≥™ÊåáÊï∏ (AQI) ÁöÑÊû∂Êßã„ÄÇÈÄèÈÅéÂà©Áî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN)ÔºåÊàëÂÄëÊ†πÊìöÁ©∫ÈñìÂíåÊôÇÈñì‰æùË≥¥ÊÄß‰æÜ‰º∞Ë®àÊú™Áõ£ÊéßÂú∞ÈªûÁöÑ AQI ÂÄº„ÄÇË©≤Êû∂ÊßãÁµêÂêà‰∫ÜÂª£Ê≥õÁöÑÁí∞Â¢ÉÁâπÂæµÔºåÂåÖÊã¨Ê∞£Ë±°Ë≥áÊñô„ÄÅÈÅìË∑ØÁ∂≤Ë∑Ø„ÄÅËààË∂£Èªû (PoI)„ÄÅ‰∫∫Âè£ÂØÜÂ∫¶ÂíåÂüéÂ∏ÇÁ∂†Âú∞ÔºåÈÄô‰∫õÁâπÂæµÂ¢ûÂº∑‰∫ÜÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÈÄèÈÅéÂ∑¥Âü∫ÊñØÂù¶ÊãâÂêàÁàæÁöÑ‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂‰æÜË™™ÊòéÊàëÂÄëÊñπÊ≥ïÁöÑ‰ΩøÁî®ÔºåÂÖ∂‰∏≠‰ΩøÁî®Â§öËß£ÊûêÂ∫¶Ë≥áÊñô‰æÜÁîüÊàêÁ≤æÁ¥∞ÊôÇÁ©∫Â∞∫Â∫¶ÁöÑÁ©∫Ê∞£ÂìÅË≥™ÊåáÊï∏Âú∞Âúñ„ÄÇ

##### **A Layered Multi-Expert Framework for Long-Context Mental Health Assessments**
2501.13951v1 by Jinwen Tang, Qiming Guo, Wenbo Sun, Yi Shang

Long-form mental health assessments pose unique challenges for large language
models (LLMs), which often exhibit hallucinations or inconsistent reasoning
when handling extended, domain-specific contexts. We introduce Stacked
Multi-Model Reasoning (SMMR), a layered framework that leverages multiple LLMs
and specialized smaller models as coequal 'experts'. Early layers isolate
short, discrete subtasks, while later layers integrate and refine these partial
outputs through more advanced long-context models. We evaluate SMMR on the
DAIC-WOZ depression-screening dataset and 48 curated case studies with
psychiatric diagnoses, demonstrating consistent improvements over single-model
baselines in terms of accuracy, F1-score, and PHQ-8 error reduction. By
harnessing diverse 'second opinions', SMMR mitigates hallucinations, captures
subtle clinical nuances, and enhances reliability in high-stakes mental health
assessments. Our findings underscore the value of multi-expert frameworks for
more trustworthy AI-driven screening.

ÊëòË¶ÅÔºöÈï∑ÁØáÂøÉÁêÜÂÅ•Â∫∑Ë©ï‰º∞Â∞çÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊßãÊàêÁç®ÁâπÊåëÊà∞ÔºåÂú®ËôïÁêÜÂª∂‰º∏ÁöÑÁâπÂÆöÈ†òÂüüËÑàÁµ°ÊôÇÔºåLLM Á∂ìÂ∏∏Âá∫ÁèæÂπªË¶∫Êàñ‰∏ç‰∏ÄËá¥ÁöÑÊé®ÁêÜ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÂ†ÜÁñäÂ§öÊ®°ÂûãÊé®ÁêÜ (SMMR)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂàÜÂ±§Êû∂ÊßãÔºåÂà©Áî®Â§öÂÄã LLM ÂíåÂ∞àÊ•≠ÁöÑÂ∞èÂûãÊ®°Âûã‰ΩúÁÇ∫Â∞çÁ≠âÁöÑ„ÄåÂ∞àÂÆ∂„Äç„ÄÇÊó©ÊúüÂ±§ÈöîÈõ¢Áü≠Â∞è„ÄÅÈõ¢Êï£ÁöÑÂ≠ê‰ªªÂãôÔºåËÄåÂæåÁ∫åÂ±§ÂâáÈÄèÈÅéÊõ¥ÂÖàÈÄ≤ÁöÑÈï∑ËÑàÁµ°Ê®°ÂûãÊï¥Âêà‰∏¶Á≤æÁÖâÈÄô‰∫õÈÉ®ÂàÜËº∏Âá∫„ÄÇÊàëÂÄëÂú® DAIC-WOZ ÊÜÇÈ¨±ÁóáÁØ©ÈÅ∏Ë≥áÊñôÈõÜÂíå 48 ÂÄãÁ∂ìÈÅéÊï¥ÁêÜÁöÑÊ°à‰æãÁ†îÁ©∂‰∏≠Ë©ï‰º∞ SMMRÔºåÂÖ∂‰∏≠ÂåÖÂê´Á≤æÁ•ûÁñæÁóÖË®∫Êñ∑ÔºåË≠âÊòéÂÖ∂Âú®Ê∫ñÁ¢∫ÊÄß„ÄÅF1 ÂàÜÊï∏Âíå PHQ-8 ÈåØË™§Ê∏õÂ∞ëÊñπÈù¢ÊåÅÁ∫åÂÑ™ÊñºÂñÆ‰∏ÄÊ®°ÂûãÂü∫Ê∫ñ„ÄÇÈÄèÈÅéÂà©Áî®‰∏çÂêåÁöÑ„ÄåÁ¨¨‰∫åÊÑèË¶ã„ÄçÔºåSMMR Ê∏õËºï‰∫ÜÂπªË¶∫ÔºåÊçïÊçâÂà∞Á¥∞ÂæÆÁöÑËá®Â∫äÂ∑ÆÁï∞Ôºå‰∏¶ÊèêÈ´ò‰∫ÜÈ´òÈ¢®Èö™ÂøÉÁêÜÂÅ•Â∫∑Ë©ï‰º∞ÁöÑÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÁöÑÁôºÁèæÂº∑Ë™ø‰∫ÜÂ§öÂ∞àÂÆ∂Êû∂ÊßãÂú®Êõ¥ÂÄºÂæó‰ø°Ë≥¥ÁöÑ AI È©ÖÂãïÁØ©ÈÅ∏‰∏≠ÁöÑÂÉπÂÄº„ÄÇ

##### **Clinical trial cohort selection using Large Language Models on n2c2 Challenges**
2501.11114v1 by Chi-en Amy Tai, Xavier Tannier

Clinical trials are a critical process in the medical field for introducing
new treatments and innovations. However, cohort selection for clinical trials
is a time-consuming process that often requires manual review of patient text
records for specific keywords. Though there have been studies on standardizing
the information across the various platforms, Natural Language Processing (NLP)
tools remain crucial for spotting eligibility criteria in textual reports.
Recently, pre-trained large language models (LLMs) have gained popularity for
various NLP tasks due to their ability to acquire a nuanced understanding of
text. In this paper, we study the performance of large language models on
clinical trial cohort selection and leverage the n2c2 challenges to benchmark
their performance. Our results are promising with regard to the incorporation
of LLMs for simple cohort selection tasks, but also highlight the difficulties
encountered by these models as soon as fine-grained knowledge and reasoning are
required.

ÊëòË¶ÅÔºöËá®Â∫äË©¶È©óÊòØÈÜ´Â≠∏È†òÂüü‰∏≠ÂºïÂÖ•Êñ∞ÁôÇÊ≥ïÂíåÂâµÊñ∞ÁöÑÈóúÈçµÈÅéÁ®ã„ÄÇÁÑ∂ËÄåÔºåËá®Â∫äË©¶È©óÁöÑÊÇ£ËÄÖÁæ§È´îÈÅ∏ÊìáÊòØ‰∏ÄÂÄãËÄóÊôÇÁöÑÈÅéÁ®ãÔºåÈÄöÂ∏∏ÈúÄË¶Å‰∫∫Â∑•ÂØ©Êü•ÁóÖÊÇ£ÁöÑÊñáÂ≠óË®òÈåÑÔºå‰ª•Â∞ãÊâæÁâπÂÆöÁöÑÈóúÈçµÂ≠ó„ÄÇÂÑòÁÆ°ÊúâÁ†îÁ©∂ÈáùÂ∞ç‰∏çÂêåÂπ≥Âè∞‰∏äÁöÑË≥áË®äÈÄ≤Ë°åÊ®ôÊ∫ñÂåñÔºåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) Â∑•ÂÖ∑Â∞çÊñºÂú®ÊñáÂ≠óÂ†±Âëä‰∏≠ÊâæÂá∫Á¨¶ÂêàË≥áÊ†ºÁöÑÊ®ôÊ∫ñ‰ªçÁÑ∂Ëá≥ÈóúÈáçË¶Å„ÄÇÊúÄËøëÔºåÈ†êÂÖàË®ìÁ∑¥ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âõ†ÂÖ∂Áç≤ÂèñÁ¥∞Á∑ªÊñáÊú¨ÁêÜËß£ÁöÑËÉΩÂäõËÄåÂª£ÂèóÂêÑÁ®Æ NLP ‰ªªÂãôÊ≠°Ëøé„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÁ†îÁ©∂Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Ëá®Â∫äË©¶È©óÊÇ£ËÄÖÁæ§È´îÈÅ∏Êìá‰∏äÁöÑË°®ÁèæÔºå‰∏¶Âà©Áî® n2c2 ÊåëÊà∞‰æÜË©ïÈáèÂÖ∂Ë°®Áèæ„ÄÇÊàëÂÄëÁöÑÁµêÊûúÂ∞çÊñºÂ∞á LLM Á¥çÂÖ•Á∞°ÂñÆÁöÑÊÇ£ËÄÖÁæ§È´îÈÅ∏Êìá‰ªªÂãôËÄåË®ÄÊòØÂæàÊúâÂ∏åÊúõÁöÑÔºå‰ΩÜ‰πüÂº∑Ë™ø‰∫ÜÈÄô‰∫õÊ®°ÂûãÂú®ÈúÄË¶ÅÂÖ∑ÂÇôÁ¥∞Á∑ªÁü•Ë≠òÂíåÊé®ÁêÜËÉΩÂäõÊôÇÊâÄÈÅáÂà∞ÁöÑÂõ∞Èõ£„ÄÇ

##### **Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model**
2501.11094v1 by Mohaiminul Islam Bhuiyan, Nur Shazwani Kamarudin, Nur Hafieza Ismail

Suicidal ideation detection is crucial for preventing suicides, a leading
cause of death worldwide. Many individuals express suicidal thoughts on social
media, offering a vital opportunity for early detection through advanced
machine learning techniques. The identification of suicidal ideation in social
media text is improved by utilising a hybrid framework that integrates
Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory
(BiLSTM), enhanced with an attention mechanism. To enhance the interpretability
of the model's predictions, Explainable AI (XAI) methods are applied, with a
particular focus on SHapley Additive exPlanations (SHAP), are incorporated. At
first, the model managed to reach an accuracy of 92.81%. By applying
fine-tuning and early stopping techniques, the accuracy improved to 94.29%. The
SHAP analysis revealed key features influencing the model's predictions, such
as terms related to mental health struggles. This level of transparency boosts
the model's credibility while helping mental health professionals understand
and trust the predictions. This work highlights the potential for improving the
accuracy and interpretability of detecting suicidal tendencies, making a
valuable contribution to the progress of mental health monitoring systems. It
emphasizes the significance of blending powerful machine learning methods with
explainability to develop reliable and impactful mental health solutions.

ÊëòË¶ÅÔºöËá™ÊÆ∫ÊÑèÂøµÂÅµÊ∏¨Â∞çÊñºÈ†êÈò≤Ëá™ÊÆ∫Ëá≥ÈóúÈáçË¶ÅÔºåËÄåËá™ÊÆ∫ÊòØÂÖ®ÁêÉ‰∏ªË¶ÅÁöÑÊ≠ª‰∫°ÂéüÂõ†„ÄÇË®±Â§ö‰∫∫Âú®Á§æÁæ§Â™íÈ´î‰∏äË°®ÈÅîËá™ÊÆ∫ÂøµÈ†≠ÔºåÈÄôÊèê‰æõ‰∫ÜÈÄèÈÅéÈÄ≤ÈöéÊ©üÂô®Â≠∏ÁøíÊäÄË°ìÈÄ≤Ë°åÊó©ÊúüÂÅµÊ∏¨ÁöÑÈáçË¶ÅÊ©üÊúÉ„ÄÇÈÄèÈÅéÊï¥ÂêàÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåÈõôÂêëÈï∑Áü≠ÊúüË®òÊÜ∂ (BiLSTM) ÁöÑÊ∑∑ÂêàÊû∂ÊßãÔºå‰∏¶Âä†ÂÖ•Ê≥®ÊÑèÂäõÊ©üÂà∂ÔºåÂèØ‰ª•ÊèêÂçáÂú®Á§æÁæ§Â™íÈ´îÊñáÂ≠ó‰∏≠Ëæ®Ë≠òËá™ÊÆ∫ÊÑèÂøµÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÂä†Âº∑Ê®°ÂûãÈ†êÊ∏¨ÁöÑÂèØËß£ÈáãÊÄßÔºåÊàëÂÄëÊé°Áî®ÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊñπÊ≥ïÔºåÁâπÂà•ËëóÈáçÊñº SHapley Âä†Ê≥ïËß£Èáã (SHAP)„ÄÇ‰∏ÄÈñãÂßãÔºåÊ®°ÂûãÊàêÂäüÈÅîÂà∞ 92.81% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÈÄèÈÅéÂ•óÁî®ÂæÆË™øÂíåÊó©ÊúüÂÅúÊ≠¢ÊäÄË°ìÔºåÊ∫ñÁ¢∫Â∫¶ÊèêÂçáËá≥ 94.29%„ÄÇSHAP ÂàÜÊûêÊè≠Èú≤‰∫ÜÂΩ±ÈüøÊ®°ÂûãÈ†êÊ∏¨ÁöÑÈóúÈçµÁâπÂæµÔºå‰æãÂ¶ÇËàáÂøÉÁêÜÂÅ•Â∫∑Âõ∞Â¢ÉÁõ∏ÈóúÁöÑË©ûÂΩô„ÄÇÈÄôÁ®ÆÈÄèÊòéÂ∫¶ÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÂèØ‰ø°Â∫¶ÔºåÂêåÊôÇÂçîÂä©ÂøÉÁêÜÂÅ•Â∫∑Â∞àÊ•≠‰∫∫Âì°ÁêÜËß£Âíå‰ø°Ë≥¥È†êÊ∏¨ÁµêÊûú„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁ™ÅÈ°Ø‰∫ÜÊèêÂçáÂÅµÊ∏¨Ëá™ÊÆ∫ÂÇæÂêëÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂèØËß£ÈáãÊÄßÁöÑÊΩõÂäõÔºåÁÇ∫ÂøÉÁêÜÂÅ•Â∫∑Áõ£ÊéßÁ≥ªÁµ±ÁöÑÈÄ≤Â±ïÂÅöÂá∫ÂØ∂Ë≤¥ÁöÑË≤¢Áçª„ÄÇÂÆÉÂº∑Ë™ø‰∫ÜÂ∞áÂº∑Â§ßÁöÑÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïËàáÂèØËß£ÈáãÊÄßÁõ∏ÁµêÂêà‰ª•ÈñãÁôºÂèØÈù†‰∏îÊúâÂΩ±ÈüøÂäõÁöÑÂøÉÁêÜÂÅ•Â∫∑Ëß£Ê±∫ÊñπÊ°àÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **No More Sliding Window: Efficient 3D Medical Image Segmentation with Differentiable Top-k Patch Sampling**
2501.10814v1 by Young Seok Jeon, Hongfei Yang, Huazhu Fu, Mengling Feng

3D models are favored over 2D for 3D medical image segmentation tasks due to
their ability to leverage inter-slice relationship, yielding higher
segmentation accuracy. However, 3D models demand significantly more GPU memory
with increased model size and intermediate tensors. A common solution is to use
patch-based training and make whole-volume predictions with sliding window (SW)
inference. SW inference reduces memory usage but is slower due to equal
resource allocation across patches and less accurate as it overlooks global
features beyond patches.
  We propose NMSW-Net (No-More-Sliding-Window-Net), a novel framework that
enhances efficiency and accuracy of any given 3D segmentation model by
eliminating SW inference and incorporating global predictions when necessary.
NMSW-Net incorporates a differentiable Top-k module to sample only the relevant
patches that enhance segmentation accuracy, thereby minimizing redundant
computations. Additionally, it learns to leverage coarse global predictions
when patch prediction alone is insufficient. NMSW-Net is model-agnostic, making
it compatible with any 3D segmentation model that previously relied on SW
inference.
  Evaluated across 3 tasks with 3 segmentation backbones, NMSW-Net achieves
competitive or sometimes superior accuracy compared to SW, while reducing
computational complexity by 90% (87.5 to 7.95 TFLOPS), delivering 4x faster
inference on the H100 GPU (19.0 to 4.3 sec), and 7x faster inference on the
Intel Xeon Gold CPU (1710 to 230 seconds).

ÊëòË¶ÅÔºö<paragraph>3D Ê®°ÂûãÂú® 3D ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÂâ≤‰ªªÂä°‰∏≠‰ºò‰∫é 2DÔºåÂõ†‰∏∫
ÂÆÉ‰ª¨ËÉΩÂ§üÂà©Áî®ÂàáÁâáÈó¥ÂÖ≥Á≥ªÔºå‰ªéËÄå‰∫ßÁîüÊõ¥È´òÁöÑ
ÂàÜÂâ≤Á≤æÂ∫¶„ÄÇÁÑ∂ËÄåÔºå3D Ê®°ÂûãÈúÄË¶ÅÂ§ßÈáè GPU ÂÜÖÂ≠ò
ÈöèÁùÄÊ®°ÂûãÂ§ßÂ∞èÂíå‰∏≠Èó¥Âº†ÈáèÁöÑÂ¢ûÂä†„ÄÇ‰∏ÄÁßçÂ∏∏ËßÅÁöÑËß£ÂÜ≥ÊñπÊ°àÊòØ‰ΩøÁî®
Âü∫‰∫é patch ÁöÑËÆ≠ÁªÉÂπ∂‰ΩøÁî®ÊªëÂä®Á™óÂè£ (SW)
Êé®ÁêÜËøõË°åÂÖ®Âç∑È¢ÑÊµã„ÄÇSW Êé®ÁêÜÂáèÂ∞ë‰∫ÜÂÜÖÂ≠ò‰ΩøÁî®ÈáèÔºå‰ΩÜÁî±‰∫é
Âú® patch ‰πãÈó¥Âπ≥ÂùáÂàÜÈÖçËµÑÊ∫êÂπ∂‰∏îÁî±‰∫éÂøΩÁï•‰∫Ü patch ‰πãÂ§ñÁöÑÂÖ®Â±Ä
ÁâπÂæÅËÄåÂØºËá¥ÈÄüÂ∫¶ËæÉÊÖ¢‰∏îÂáÜÁ°ÆÂ∫¶ËæÉ‰Ωé„ÄÇ
Êàë‰ª¨ÊèêÂá∫‰∫Ü NMSW-NetÔºàÊó†ÊªëÂä®Á™óÂè£ÁΩëÁªúÔºâÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂÆÉ
ÈÄöËøáÊ∂àÈô§ SW Êé®ÁêÜÂπ∂Âú®ÂøÖË¶ÅÊó∂ÂêàÂπ∂ÂÖ®Â±ÄÈ¢ÑÊµãÊù•ÊèêÈ´ò‰ªª‰ΩïÁªôÂÆö 3D ÂàÜÂâ≤Ê®°ÂûãÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇ
NMSW-Net ÁªìÂêà‰∫Ü‰∏Ä‰∏™ÂèØÂæÆÂàÜÁöÑ Top-k Ê®°ÂùóÊù•‰ªÖÈááÊ†∑Áõ∏ÂÖ≥
patchÔºå‰ª•ÊèêÈ´òÂàÜÂâ≤Á≤æÂ∫¶Ôºå‰ªéËÄåÊúÄÂ§ßÈôêÂ∫¶Âú∞ÂáèÂ∞ëÂÜó‰Ωô
ËÆ°ÁÆó„ÄÇÊ≠§Â§ñÔºåÂÆÉÂ≠¶‰ºö‰∫ÜÂú®‰ªÖ patch È¢ÑÊµã‰∏çË∂≥Êó∂Âà©Áî®Á≤óÁï•ÁöÑÂÖ®Â±ÄÈ¢ÑÊµã„ÄÇNMSW-Net ‰∏éÊ®°ÂûãÊó†ÂÖ≥Ôºå‰ΩøÂÖ∂
‰∏é‰ª•Ââç‰æùËµñ SW ÁöÑ‰ªª‰Ωï 3D ÂàÜÂâ≤Ê®°ÂûãÂÖºÂÆπ
Êé®ÁêÜ„ÄÇ
Âú® 3 ‰∏™Â∏¶Êúâ 3 ‰∏™ÂàÜÂâ≤‰∏ªÂπ≤ÁöÑ‰ªªÂä°‰∏≠ËøõË°åËØÑ‰º∞ÔºåNMSW-Net ÂÆûÁé∞‰∫Ü
‰∏é SW Áõ∏ÊØîÂÖ∑ÊúâÁ´û‰∫âÂäõÊàñÊúâÊó∂Êõ¥È´òÁöÑÂáÜÁ°ÆÊÄßÔºåÂêåÊó∂ÂáèÂ∞ë
ËÆ°ÁÆóÂ§çÊùÇÂ∫¶Èôç‰Ωé‰∫Ü 90%Ôºà87.5 Âà∞ 7.95 TFLOPSÔºâÔºåÂú® H100 GPU ‰∏äÊèê‰æõ 4 ÂÄçÊõ¥Âø´ÁöÑ
Êé®ÁêÜÔºà19.0 Âà∞ 4.3 ÁßíÔºâÔºå‰ª•ÂèäÂú®
Ëã±ÁâπÂ∞îËá≥Âº∫Èáë CPU ‰∏äÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò 7 ÂÄçÔºà1710 Âà∞ 230 ÁßíÔºâ„ÄÇ</paragraph>

##### **Efficient Auto-Labeling of Large-Scale Poultry Datasets (ALPD) Using Semi-Supervised Models, Active Learning, and Prompt-then-Detect Approach**
2501.10809v1 by Ramesh Bahadur Bist, Lilong Chai, Shawna Weimer, Hannah Atungulua, Chantel Pennicott, Xiao Yang, Sachin Subedi, Chaitanya Pallerla, Yang Tian, Dongyi Wang

The rapid growth of AI in poultry farming has highlighted the challenge of
efficiently labeling large, diverse datasets. Manual annotation is
time-consuming, making it impractical for modern systems that continuously
generate data. This study explores semi-supervised auto-labeling methods,
integrating active learning, and prompt-then-detect paradigm to develop an
efficient framework for auto-labeling of large poultry datasets aimed at
advancing AI-driven behavior and health monitoring. Viideo data were collected
from broilers and laying hens housed at the University of Arkansas and the
University of Georgia. The collected videos were converted into images,
pre-processed, augmented, and labeled. Various machine learning models,
including zero-shot models like Grounding DINO, YOLO-World, and CLIP, and
supervised models like YOLO and Faster-RCNN, were utilized for broilers, hens,
and behavior detection. The results showed that YOLOv8s-World and YOLOv9s
performed better when compared performance metrics for broiler and hen
detection under supervised learning, while among the semi-supervised model,
YOLOv8s-ALPD achieved the highest precision (96.1%) and recall (99.0%) with an
RMSE of 1.9. The hybrid YOLO-World model, incorporating the optimal YOLOv8s
backbone, demonstrated the highest overall performance. It achieved a precision
of 99.2%, recall of 99.4%, and an F1 score of 98.7% for breed detection,
alongside a precision of 88.4%, recall of 83.1%, and an F1 score of 84.5% for
individual behavior detection. Additionally, semi-supervised models showed
significant improvements in behavior detection, achieving up to 31% improvement
in precision and 16% in F1-score. The semi-supervised models with minimal
active learning reduced annotation time by over 80% compared to full manual
labeling. Moreover, integrating zero-shot models enhanced detection and
behavior identification.

ÊëòË¶ÅÔºö<paragraph>ÂÆ∂Á¶ΩÂÖªÊÆñ‰∏≠‰∫∫Â∑•Êô∫ËÉΩÁöÑÂø´ÈÄüÂ¢ûÈïøÂá∏Êòæ‰∫ÜÈ´òÊïàÊ†áÊ≥®Â§ßÂûã„ÄÅÂ§öÊ†∑ÂåñÊï∞ÊçÆÈõÜÁöÑÊåëÊàò„ÄÇÊâãÂä®Ê†áÊ≥®ÈùûÂ∏∏ËÄóÊó∂ÔºåÂØπ‰∫éÊåÅÁª≠ÁîüÊàêÊï∞ÊçÆÁöÑÁé∞‰ª£Á≥ªÁªüËÄåË®Ä‰∏çÂàáÂÆûÈôÖ„ÄÇÊú¨Á†îÁ©∂Êé¢Á¥¢‰∫ÜÂçäÁõëÁù£Ëá™Âä®Ê†áÊ≥®ÊñπÊ≥ïÔºåÈõÜÊàê‰∫Ü‰∏ªÂä®Â≠¶‰π†ÂíåÊèêÁ§∫ÂÜçÊ£ÄÊµãËåÉÂºèÔºå‰ª•ÂºÄÂèë‰∏Ä‰∏™È´òÊïàÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éËá™Âä®Ê†áÊ≥®Â§ßÂûãÂÆ∂Á¶ΩÊï∞ÊçÆÈõÜÔºåÊó®Âú®Êé®Ëøõ‰∫∫Â∑•Êô∫ËÉΩÈ©±Âä®ÁöÑË°å‰∏∫ÂíåÂÅ•Â∫∑ÁõëÊµã„ÄÇËßÜÈ¢ëÊï∞ÊçÆÊòØ‰ªéÈòøËÇØËâ≤Â§ßÂ≠¶Âíå‰ΩêÊ≤ª‰∫öÂ§ßÂ≠¶È•≤ÂÖªÁöÑËÇâÈ∏°ÂíåËõãÈ∏°‰∏≠Êî∂ÈõÜÁöÑ„ÄÇÊî∂ÈõÜÁöÑËßÜÈ¢ëË¢´ËΩ¨Êç¢ÊàêÂõæÂÉèÔºåÁªèËøáÈ¢ÑÂ§ÑÁêÜ„ÄÅÂ¢ûÂº∫ÂíåÊ†áÊ≥®„ÄÇÂêÑÁßçÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÔºåÂåÖÊã¨ Grounding DINO„ÄÅYOLO-World Âíå CLIP Á≠âÈõ∂Ê†∑Êú¨Â≠¶‰π†Ê®°ÂûãÔºå‰ª•Âèä YOLO Âíå Faster-RCNN Á≠âÁõëÁù£Ê®°ÂûãÔºåË¢´Áî®‰∫éËÇâÈ∏°„ÄÅÊØçÈ∏°ÂíåË°å‰∏∫Ê£ÄÊµã„ÄÇÁªìÊûúË°®ÊòéÔºåÂú®ÁõëÁù£Â≠¶‰π†‰∏ãÔºåYOLOv8s-World Âíå YOLOv9s Âú®ËÇâÈ∏°ÂíåÊØçÈ∏°Ê£ÄÊµãÁöÑÊÄßËÉΩÊåáÊ†áÊØîËæÉ‰∏≠Ë°®Áé∞ÂæóÊõ¥Â•ΩÔºåËÄåÂú®ÂçäÁõëÁù£Ê®°Âûã‰∏≠ÔºåYOLOv8s-ALPD ‰ª• 1.9 ÁöÑ RMSE ÂÆûÁé∞‰∫ÜÊúÄÈ´òÁöÑÁ≤æÂ∫¶ (96.1%) ÂíåÂè¨ÂõûÁéá (99.0%)„ÄÇÁªìÂêà‰∫ÜÊúÄ‰Ω≥ YOLOv8s ‰∏ªÂπ≤ÁΩëÁªúÁöÑÊ∑∑Âêà YOLO-World Ê®°ÂûãÂ±ïÁ§∫‰∫ÜÊúÄÈ´òÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇÂÆÉÂú®ÂìÅÁßçÊ£ÄÊµã‰∏≠ÂÆûÁé∞‰∫Ü 99.2% ÁöÑÁ≤æÂ∫¶„ÄÅ99.4% ÁöÑÂè¨ÂõûÁéáÂíå 98.7% ÁöÑ F1 ÂàÜÊï∞ÔºåÂú®‰∏™‰ΩìË°å‰∏∫Ê£ÄÊµã‰∏≠ÂÆûÁé∞‰∫Ü 88.4% ÁöÑÁ≤æÂ∫¶„ÄÅ83.1% ÁöÑÂè¨ÂõûÁéáÂíå 84.5% ÁöÑ F1 ÂàÜÊï∞„ÄÇÊ≠§Â§ñÔºåÂçäÁõëÁù£Ê®°ÂûãÂú®Ë°å‰∏∫Ê£ÄÊµã‰∏≠ÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊîπËøõÔºåÂú®Á≤æÂ∫¶‰∏äÊèêÈ´ò‰∫Ü 31%ÔºåÂú® F1 ÂàÜÊï∞‰∏äÊèêÈ´ò‰∫Ü 16%„ÄÇ‰∏éÂÆåÂÖ®ÊâãÂä®Ê†áÊ≥®Áõ∏ÊØîÔºåÂÖ∑ÊúâÊúÄÂ∞ë‰∏ªÂä®Â≠¶‰π†ÁöÑÂçäÁõëÁù£Ê®°ÂûãÂ∞ÜÊ†áÊ≥®Êó∂Èó¥ÂáèÂ∞ë‰∫Ü 80% ‰ª•‰∏ä„ÄÇÊ≠§Â§ñÔºåÈõÜÊàêÈõ∂Ê†∑Êú¨Â≠¶‰π†Ê®°ÂûãÂ¢ûÂº∫‰∫ÜÊ£ÄÊµãÂíåË°å‰∏∫ËØÜÂà´„ÄÇ</paragraph>

##### **MedFILIP: Medical Fine-grained Language-Image Pre-training**
2501.10775v1 by Xinjie Liang, Xiangyu Li, Fanding Li, Jie Jiang, Qing Dong, Wei Wang, Kuanquan Wang, Suyu Dong, Gongning Luo, Shuo Li

Medical vision-language pretraining (VLP) that leverages naturally-paired
medical image-report data is crucial for medical image analysis. However,
existing methods struggle to accurately characterize associations between
images and diseases, leading to inaccurate or incomplete diagnostic results. In
this work, we propose MedFILIP, a fine-grained VLP model, introduces medical
image-specific knowledge through contrastive learning, specifically: 1) An
information extractor based on a large language model is proposed to decouple
comprehensive disease details from reports, which excels in extracting disease
deals through flexible prompt engineering, thereby effectively reducing text
complexity while retaining rich information at a tiny cost. 2) A knowledge
injector is proposed to construct relationships between categories and visual
attributes, which help the model to make judgments based on image features, and
fosters knowledge extrapolation to unfamiliar disease categories. 3) A semantic
similarity matrix based on fine-grained annotations is proposed, providing
smoother, information-richer labels, thus allowing fine-grained image-text
alignment. 4) We validate MedFILIP on numerous datasets, e.g., RSNA-Pneumonia,
NIH ChestX-ray14, VinBigData, and COVID-19. For single-label, multi-label, and
fine-grained classification, our model achieves state-of-the-art performance,
the classification accuracy has increased by a maximum of 6.69\%. The code is
available in https://github.com/PerceptionComputingLab/MedFILIP.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÂΩ±ÂÉèË™ûË®ÄÈ†êË®ìÁ∑¥ÔºàVLPÔºâÂà©Áî®Ëá™ÁÑ∂ÈÖçÂ∞çÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂ†±ÂëäÊï∏ÊìöÔºåÂ∞çÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÈõ£‰ª•Ê∫ñÁ¢∫ÊèèËø∞ÂΩ±ÂÉèËàáÁñæÁóÖ‰πãÈñìÁöÑÈóúËÅØÊÄßÔºåÂ∞éËá¥Ë®∫Êñ∑ÁµêÊûú‰∏çÊ∫ñÁ¢∫Êàñ‰∏çÂÆåÊï¥„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ MedFILIPÔºå‰∏ÄÂÄãÁ¥∞Á≤íÂ∫¶ÁöÑ VLP Ê®°ÂûãÔºåÈÄèÈÅéÂ∞çÊØîÂ≠∏ÁøíÂºïÂÖ•ÈÜ´Â≠∏ÂΩ±ÂÉèÁâπÂÆöÁü•Ë≠òÔºåÂÖ∑È´î‰æÜË™™Ôºö1) ÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑË≥áË®äËêÉÂèñÂô®ÔºåÂæûÂ†±Âëä‰∏≠Ëß£ËÄ¶ÂÖ®Èù¢ÁöÑÁñæÁóÖÁ¥∞ÁØÄÔºåÈÄèÈÅéÈùàÊ¥ªÁöÑÊèêÁ§∫Â∑•Á®ãÔºåÂú®ÊèêÂèñÁñæÁóÖ‰∫§ÊòìÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÔºåÂæûËÄåÊúâÊïàÈôç‰ΩéÊñáÂ≠óË§áÈõúÊÄßÔºåÂêåÊôÇ‰ª•Ê•µÂ∞èÁöÑ‰ª£ÂÉπ‰øùÁïôË±êÂØåÁöÑË≥áË®ä„ÄÇ2) ÊèêÂá∫‰∏ÄÂÄãÁü•Ë≠òÊ≥®ÂÖ•Âô®ÔºåÁî®ÊñºÂª∫ÊßãÈ°ûÂà•ËàáË¶ñË¶∫Â±¨ÊÄß‰πãÈñìÁöÑÈóú‰øÇÔºåÈÄôÊúâÂä©ÊñºÊ®°ÂûãÊ†πÊìöÂΩ±ÂÉèÁâπÂæµÈÄ≤Ë°åÂà§Êñ∑Ôºå‰∏¶‰øÉÈÄ≤Áü•Ë≠òÂ§ñÊé®Âà∞‰∏çÁÜüÊÇâÁöÑÁñæÁóÖÈ°ûÂà•„ÄÇ3) ÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÁ¥∞Á≤íÂ∫¶Ë®ªËß£ÁöÑË™ûÁæ©Áõ∏‰ººÁü©Èô£ÔºåÊèê‰æõÊõ¥Âπ≥Êªë„ÄÅË≥áË®äÊõ¥Ë±êÂØåÁöÑÊ®ôÁ±§ÔºåÂæûËÄåÂÖÅË®±ÈÄ≤Ë°åÁ¥∞Á≤íÂ∫¶ÁöÑÂΩ±ÂÉèÊñáÂ≠óÂ∞çÈΩä„ÄÇ4) ÊàëÂÄëÂú®Ë®±Â§öË≥áÊñôÈõÜ‰∏äÈ©óË≠â MedFILIPÔºå‰æãÂ¶Ç RSNA-Pneumonia„ÄÅNIH ChestX-ray14„ÄÅVinBigData Âíå COVID-19„ÄÇÂ∞çÊñºÂñÆÊ®ôÁ±§„ÄÅÂ§öÊ®ôÁ±§ÂíåÁ¥∞Á≤íÂ∫¶ÂàÜÈ°ûÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂàÜÈ°ûÊ∫ñÁ¢∫ÁéáÊúÄÈ´òÊèêÈ´ò‰∫Ü 6.69%„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/PerceptionComputingLab/MedFILIP ‰∏≠ÂèñÂæó„ÄÇ

##### **Enhancing Diagnostic in 3D COVID-19 Pneumonia CT-scans through Explainable Uncertainty Bayesian Quantification**
2501.10770v1 by Juan Manuel Liscano Fierro, Hector J. Hortua

Accurately classifying COVID-19 pneumonia in 3D CT scans remains a
significant challenge in the field of medical image analysis. Although
deterministic neural networks have shown promising results in this area, they
provide only point estimates outputs yielding poor diagnostic in clinical
decision-making. In this paper, we explore the use of Bayesian neural networks
for classifying COVID-19 pneumonia in 3D CT scans providing uncertainties in
their predictions. We compare deterministic networks and their Bayesian
counterpart, enhancing the decision-making accuracy under uncertainty
information. Remarkably, our findings reveal that lightweight architectures
achieve the highest accuracy of 96\% after developing extensive hyperparameter
tuning. Furthermore, the Bayesian counterpart of these architectures via
Multiplied Normalizing Flow technique kept a similar performance along with
calibrated uncertainty estimates. Finally, we have developed a 3D-visualization
approach to explain the neural network outcomes based on SHAP values. We
conclude that explainability along with uncertainty quantification will offer
better clinical decisions in medical image analysis, contributing to ongoing
efforts for improving the diagnosis and treatment of COVID-19 pneumonia.

ÊëòË¶ÅÔºöÊ∫ñÁ¢∫ÂàÜÈ°û 3D ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè‰∏≠ÁöÑ COVID-19 ËÇ∫ÁÇéÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÈ†òÂüü‰∏≠‰ªçÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇÂÑòÁÆ°Á¢∫ÂÆöÊÄßÁ•ûÁ∂ìÁ∂≤Ë∑ØÂ∑≤Âú®Ê≠§È†òÂüü‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫ÊªøÊÑèÁöÑÁµêÊûúÔºå‰ΩÜÂÆÉÂÄëÂÉÖÊèê‰æõÈªû‰º∞Ë®àËº∏Âá∫ÔºåÂú®Ëá®Â∫äÊ±∫Á≠ñ‰∏≠Áî¢Áîü‰∏çËâØÁöÑË®∫Êñ∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰ΩøÁî®Ë≤ùÊ∞èÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰æÜÂàÜÈ°û 3D ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè‰∏≠ÁöÑ COVID-19 ËÇ∫ÁÇéÔºå‰∏¶Âú®È†êÊ∏¨‰∏≠Êèê‰æõ‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÊàëÂÄëÊØîËºÉÁ¢∫ÂÆöÊÄßÁ∂≤Ë∑ØÂèäÂÖ∂Ë≤ùÊ∞èÂ∞çÊáâÁ∂≤Ë∑ØÔºåÂú®‰∏çÁ¢∫ÂÆöÊÄßË≥áË®ä‰∏ãÊèêÂçáÊ±∫Á≠ñÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÁöÑÁôºÁèæÈ°ØÁ§∫ÔºåÂú®Á∂ìÈÅéÂª£Ê≥õÁöÑË∂ÖÂèÉÊï∏Ë™øÊï¥ÂæåÔºåËºïÈáèÁ¥öÊû∂ÊßãÂèØÈÅîÂà∞ 96% ÁöÑÊúÄÈ´òÊ∫ñÁ¢∫Â∫¶„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÊû∂ÊßãÁöÑË≤ùÊ∞èÂ∞çÊáâÁ∂≤Ë∑ØÈÄèÈÅé‰πòÊ≥ïÊ≠£Ë¶èÂåñÊµÅÊäÄË°ìÔºåÂú®Ê†°Ê∫ñ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÁöÑÂêåÊôÇÔºåÁ∂≠ÊåÅÈ°û‰ººÁöÑÊïàËÉΩ„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ∑≤ÈñãÁôºÂá∫ 3D Ë¶ñË¶∫ÂåñÊñπÊ≥ïÔºå‰ª•Ê†πÊìö SHAP ÂÄº‰æÜËß£ÈáãÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÁµêÊûú„ÄÇÊàëÂÄëÂæóÂá∫ÁµêË´ñÔºåÂèØËß£ÈáãÊÄßËàá‰∏çÁ¢∫ÂÆöÊÄßÈáèÂåñÂ∞áÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰∏≠Êèê‰æõÊõ¥Â•ΩÁöÑËá®Â∫äÊ±∫Á≠ñÔºåÊúâÂä©ÊñºÊåÅÁ∫åÊîπÂñÑ COVID-19 ËÇ∫ÁÇéÁöÑË®∫Êñ∑ÂíåÊ≤ªÁôÇ„ÄÇ

##### **In the Picture: Medical Imaging Datasets, Artifacts, and their Living Review**
2501.10727v1 by Amelia Jim√©nez-S√°nchez, Natalia-Rozalia Avlona, Sarah de Boer, V√≠ctor M. Campello, Aasa Feragen, Enzo Ferrante, Melanie Ganz, Judy Wawira Gichoya, Camila Gonz√°lez, Steff Groefsema, Alessa Hering, Adam Hulman, Leo Joskowicz, Dovile Juodelyte, Melih Kandemir, Thijs Kooi, Jorge del Pozo L√©rida, Livie Yumeng Li, Andre Pacheco, Tim R√§dsch, Mauricio Reyes, Th√©o Sourget, Bram van Ginneken, David Wen, Nina Weng, Jack Junchi Xu, Hubert Dariusz ZajƒÖc, Maria A. Zuluaga, Veronika Cheplygina

Datasets play a critical role in medical imaging research, yet issues such as
label quality, shortcuts, and metadata are often overlooked. This lack of
attention may harm the generalizability of algorithms and, consequently,
negatively impact patient outcomes. While existing medical imaging literature
reviews mostly focus on machine learning (ML) methods, with only a few focusing
on datasets for specific applications, these reviews remain static -- they are
published once and not updated thereafter. This fails to account for emerging
evidence, such as biases, shortcuts, and additional annotations that other
researchers may contribute after the dataset is published. We refer to these
newly discovered findings of datasets as research artifacts. To address this
gap, we propose a living review that continuously tracks public datasets and
their associated research artifacts across multiple medical imaging
applications. Our approach includes a framework for the living review to
monitor data documentation artifacts, and an SQL database to visualize the
citation relationships between research artifact and dataset. Lastly, we
discuss key considerations for creating medical imaging datasets, review best
practices for data annotation, discuss the significance of shortcuts and
demographic diversity, and emphasize the importance of managing datasets
throughout their entire lifecycle. Our demo is publicly available at
http://130.226.140.142.

ÊëòË¶ÅÔºö<paragraph>Ë≥áÊñôÈõÜÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÁ†îÁ©∂‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤ÔºåÁÑ∂ËÄåÊ®ôÁ±§ÂìÅË≥™„ÄÅÊç∑ÂæëÂíåÂÖÉË≥áÊñôÁ≠âÂïèÈ°åÂçªÂ∏∏Â∏∏Ë¢´ÂøΩÁï•„ÄÇÈÄôÁ®ÆÁº∫‰πèÈóúÊ≥®ÁöÑÁèæË±°ÂèØËÉΩÊúÉÊêçÂÆ≥ÊºîÁÆóÊ≥ïÁöÑÊ¶ÇÊã¨ÊÄßÔºåÈÄ≤ËÄåÂ∞çÁóÖÊÇ£ÁöÑÊ≤ªÁôÇÁµêÊûúÈÄ†ÊàêË≤†Èù¢ÂΩ±Èüø„ÄÇÈõñÁÑ∂ÁèæÊúâÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÊñáÁçªÂõûÈ°ßÂ§ßÂ§öÈõÜ‰∏≠ÊñºÊ©üÂô®Â≠∏Áøí (ML) ÊñπÊ≥ïÔºåÂè™ÊúâÂ∞ëÊï∏ÂõûÈ°ßËëóÈáçÊñºÁâπÂÆöÊáâÁî®Á®ãÂºèÁöÑË≥áÊñôÈõÜÔºå‰ΩÜÈÄô‰∫õÂõûÈ°ß‰ªçÁÑ∂ÊòØÈùúÊÖãÁöÑ‚Äî‚ÄîÂÆÉÂÄëÂè™ÊúÉÁôºË°®‰∏ÄÊ¨°Ôºå‰πãÂæå‰∏çÊúÉÂÜçÊõ¥Êñ∞„ÄÇÈÄôÁÑ°Ê≥ïËÄÉÈáèÊñ∞Âá∫ÁèæÁöÑË≠âÊìöÔºå‰æãÂ¶ÇÂÅèË™§„ÄÅÊç∑ÂæëÂíåË≥áÊñôÈõÜÂú®ÁôºË°®ÂæåÂÖ∂‰ªñÁ†îÁ©∂‰∫∫Âì°ÂèØËÉΩÊèê‰æõÁöÑÈ°çÂ§ñË®ªËß£„ÄÇÊàëÂÄëÂ∞áÈÄô‰∫õÊñ∞ÁôºÁèæÁöÑË≥áÊñôÈõÜÁôºÁèæÁ®±ÁÇ∫Á†îÁ©∂ÊàêÊûú„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊåÅÁ∫åËøΩËπ§ÂÖ¨ÈñãË≥áÊñôÈõÜÂèäÂÖ∂ËàáÂ§öÂÄãÈÜ´Â≠∏ÂΩ±ÂÉèÊáâÁî®Á®ãÂºèÁõ∏ÈóúÁöÑÁ†îÁ©∂ÊàêÊûúÁöÑÂãïÊÖãÂõûÈ°ß„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨‰∏ÄÂÄãÁî®ÊñºÁõ£ÊéßË≥áÊñôÊñá‰ª∂ÊàêÊûúÁöÑÂãïÊÖãÂõûÈ°ßÊû∂ÊßãÔºå‰ª•Âèä‰∏ÄÂÄãÁî®ÊñºË¶ñË¶∫ÂåñÁ†îÁ©∂ÊàêÊûúËàáË≥áÊñôÈõÜ‰πãÈñìÂºïÁî®Èóú‰øÇÁöÑ SQL Ë≥áÊñôÂ∫´„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñÂª∫Á´ãÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÊôÇÁöÑ‰∏ªË¶ÅËÄÉÈáèÂõ†Á¥†ÔºåÂõûÈ°ßË≥áÊñôË®ªËß£ÁöÑÊúÄ‰Ω≥ÂØ¶ÂãôÔºåÊé¢Ë®éÊç∑ÂæëÂíå‰∫∫Âè£Áµ±Ë®àÂ§öÊ®£ÊÄßÁöÑÈáçË¶ÅÊÄßÔºå‰∏¶Âº∑Ë™øÂú®Ë≥áÊñôÈõÜÁöÑÊï¥ÂÄãÁîüÂëΩÈÄ±Êúü‰∏≠ÁÆ°ÁêÜË≥áÊñôÈõÜÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÁöÑÁ§∫ÁØÑÂèØÊñº http://130.226.140.142 ÂÖ¨ÈñãÂèñÂæó„ÄÇ</paragraph>

##### **An Ontology for Social Determinants of Education (SDoEd) based on Human-AI Collaborative Approach**
2501.10300v1 by Navya Martin Kollapally, James Geller, Patricia Morreale, Daehan Kwak

The use of computational ontologies is well-established in the field of
Medical Informatics. The topic of Social Determinants of Health (SDoH) has also
received extensive attention. Work at the intersection of ontologies and SDoH
has been published. However, a standardized framework for Social Determinants
of Education (SDoEd) is lacking. In this paper, we are closing the gap by
introducing an SDoEd ontology for creating a precise conceptualization of the
interplay between life circumstances of students and their possible educational
achievements. The ontology was developed utilizing suggestions from
ChatGPT-3.5-010422 and validated using peer-reviewed research articles. The
first version of developed ontology was evaluated by human experts in the field
of education and validated using standard ontology evaluation software. This
version of the SDoEd ontology contains 231 domain concepts, 10 object
properties, and 24 data properties

ÊëòË¶ÅÔºöÂú®ÈÜ´Â≠∏Ë≥áË®äÂ≠∏È†òÂüü‰∏≠ÔºåË®àÁÆóÊú¨È´îÁöÑ‰ΩøÁî®Â∑≤Á∂ìÁõ∏Áï∂ÊôÆÈÅç„ÄÇÁ§æÊúÉÂÅ•Â∫∑Ê±∫ÂÆöÂõ†Á¥†ÔºàSDoHÔºâÁöÑ‰∏ªÈ°å‰πüÂèóÂà∞Âª£Ê≥õÁöÑÈóúÊ≥®„ÄÇÊú¨È´îËàá SDoH ‰∫§ÈõÜËôïÁöÑÂ∑•‰ΩúÂ∑≤Á∂ìÁôºË°®„ÄÇÁÑ∂ËÄåÔºåÁ§æÊúÉÊïôËÇ≤Ê±∫ÂÆöÂõ†Á¥†ÔºàSDoEdÔºâÁöÑÊ®ôÊ∫ñÂåñÊû∂ÊßãÂçª‰ªò‰πãÈóïÂ¶Ç„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂºïÂÖ• SDoEd Êú¨È´î‰æÜÂ°´Ë£úÈÄôÂÄãÁº∫Âè£Ôºå‰ª•Âª∫Á´ãÂ≠∏ÁîüÁîüÊ¥ªÁí∞Â¢ÉËàáÂÖ∂ÂèØËÉΩÊïôËÇ≤ÊàêÂ∞±‰πãÈñìÁõ∏‰∫í‰ΩúÁî®ÁöÑÁ≤æÁ¢∫Ê¶ÇÂøµÂåñ„ÄÇÊú¨È´îÊòØÂà©Áî® ChatGPT-3.5-010422 ÁöÑÂª∫Ë≠∞ÈñãÁôºÁöÑÔºå‰∏¶‰ΩøÁî®ÂêåË°åË©ïÂØ©ÁöÑÁ†îÁ©∂ÊñáÁ´†ÈÄ≤Ë°åÈ©óË≠â„ÄÇÈñãÁôºÊú¨È´îÁöÑÁ¨¨‰∏ÄÂÄãÁâàÊú¨Áî±ÊïôËÇ≤È†òÂüüÁöÑ‰∫∫È°ûÂ∞àÂÆ∂Ë©ï‰º∞Ôºå‰∏¶‰ΩøÁî®Ê®ôÊ∫ñÊú¨È´îË©ï‰º∞ËªüÈ´îÈÄ≤Ë°åÈ©óË≠â„ÄÇÊ≠§ÁâàÊú¨ÁöÑ SDoEd Êú¨È´îÂåÖÂê´ 231 ÂÄãÁ∂≤ÂüüÊ¶ÇÂøµ„ÄÅ10 ÂÄãÁâ©‰ª∂Â±¨ÊÄßÂíå 24 ÂÄãË≥áÊñôÂ±¨ÊÄß

##### **SEANN: A Domain-Informed Neural Network for Epidemiological Insights**
2501.10273v1 by Jean-Baptiste Guimbaud, Marc Plantevit, L√©a Ma√Ætre, R√©my Cazabet

In epidemiology, traditional statistical methods such as logistic regression,
linear regression, and other parametric models are commonly employed to
investigate associations between predictors and health outcomes. However,
non-parametric machine learning techniques, such as deep neural networks
(DNNs), coupled with explainable AI (XAI) tools, offer new opportunities for
this task. Despite their potential, these methods face challenges due to the
limited availability of high-quality, high-quantity data in this field. To
address these challenges, we introduce SEANN, a novel approach for informed
DNNs that leverages a prevalent form of domain-specific knowledge: Pooled
Effect Sizes (PES). PESs are commonly found in published Meta-Analysis studies,
in different forms, and represent a quantitative form of a scientific
consensus. By direct integration within the learning procedure using a custom
loss, we experimentally demonstrate significant improvements in the
generalizability of predictive performances and the scientific plausibility of
extracted relationships compared to a domain-knowledge agnostic neural network
in a scarce and noisy data setting.

ÊëòË¶ÅÔºöÂú®ÊµÅË°åÁóÖÂ≠∏‰∏≠ÔºåÂÇ≥Áµ±ÁöÑÁµ±Ë®àÊñπÊ≥ïÔºå‰æãÂ¶ÇÈÇèËºØËø¥Ê≠∏„ÄÅÁ∑öÊÄßËø¥Ê≠∏ÂíåÂÖ∂‰ªñÂèÉÊï∏Ê®°ÂûãÈÄöÂ∏∏Áî®ÊñºË™øÊü•È†êÊ∏¨Âõ†Â≠êËàáÂÅ•Â∫∑ÁµêÊûú‰πãÈñìÁöÑÈóúËÅØ„ÄÇÁÑ∂ËÄåÔºåÈùûÂèÉÊï∏Ê©üÂô®Â≠∏ÁøíÊäÄË°ìÔºå‰æãÂ¶ÇÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø (DNN)ÔºåÁµêÂêàÂèØËß£ÈáãÁöÑ AI (XAI) Â∑•ÂÖ∑ÔºåÁÇ∫ÈÄôÈ†Ö‰ªªÂãôÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊ©üÊúÉ„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊñπÊ≥ïÂÖ∑ÊúâÊΩõÂäõÔºå‰ΩÜÁî±ÊñºË©≤È†òÂüüÁº∫‰πèÈ´òÂìÅË≥™„ÄÅÈ´òÊï∏ÈáèË≥áÊñôÔºåÂõ†Ê≠§ÈÄô‰∫õÊñπÊ≥ïÈù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü SEANNÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÁç≤ÂèñÁü•Ë≠òÁöÑ DNNÔºåÂÆÉÂà©Áî®‰∫Ü‰∏ÄÁ®ÆÊµÅË°åÁöÑÈ†òÂüüÁâπÂÆöÁü•Ë≠òÂΩ¢ÂºèÔºöÂΩôÁ∏ΩÊïàÊáâÈáè (PES)„ÄÇPES ÈÄöÂ∏∏‰ª•‰∏çÂêåÁöÑÂΩ¢ÂºèÂá∫ÁèæÂú®Â∑≤ÁôºË°®ÁöÑ Meta ÂàÜÊûêÁ†îÁ©∂‰∏≠Ôºå‰∏¶‰ª£Ë°®ÁßëÂ≠∏ÂÖ±Ë≠òÁöÑÈáèÂåñÂΩ¢Âºè„ÄÇÈÄöÈÅé‰ΩøÁî®Ëá™Ë®ÇÊêçÂ§±ÂáΩÊï∏Áõ¥Êé•Êï¥ÂêàÂú®Â≠∏ÁøíÁ®ãÂ∫è‰∏≠ÔºåÊàëÂÄë‰ª•ÂØ¶È©óÊñπÂºèË≠âÊòé‰∫ÜÈ†êÊ∏¨ÊïàËÉΩÁöÑÊ¶ÇÊã¨ÊÄß‰ª•ÂèäËàáÂæûÁº∫‰πèÈ†òÂüüÁü•Ë≠òÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÊèêÂèñÁöÑÈóú‰øÇÁõ∏ÊØîÔºåÁßëÂ≠∏ÂêàÁêÜÊÄßÁöÑÈ°ØËëóÊèêÂçáÔºå‰∏îÊòØÂú®Á®ÄÂ∞ë‰∏îÊúâÈõúË®äÁöÑË≥áÊñôË®≠ÂÆö‰∏≠„ÄÇ

##### **Challenges and recommendations for Electronic Health Records data extraction and preparation for dynamic prediction modelling in hospitalized patients -- a practical guide**
2501.10240v1 by Elena Albu, Shan Gao, Pieter Stijnen, Frank E. Rademakers, Bas C T van Bussel, Taya Collyer, Tina Hernandez-Boussard, Laure Wynants, Ben Van Calster

Dynamic predictive modeling using electronic health record (EHR) data has
gained significant attention in recent years. The reliability and
trustworthiness of such models depend heavily on the quality of the underlying
data, which is largely determined by the stages preceding the model
development: data extraction from EHR systems and data preparation. We list
over forty challenges encountered during these stages and provide actionable
recommendations for addressing them. These challenges are organized into four
categories: cohort definition, outcome definition, feature engineering, and
data cleaning. This list is designed to serve as a practical guide for data
extraction engineers and researchers, supporting better practices and improving
the quality and real-world applicability of dynamic prediction models in
clinical settings.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºå‰ΩøÁî®ÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑ (EHR) Ë≥áÊñôÁöÑÂãïÊÖãÈ†êÊ∏¨Ê®°ÂûãÁç≤Âæó‰∫ÜÊ•µÂ§ßÁöÑÈóúÊ≥®„ÄÇÊ≠§È°ûÊ®°ÂûãÁöÑÂèØÈù†ÊÄßÂíåÂèØ‰ø°Â∫¶Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºÂü∫Á§éË≥áÊñôÁöÑÂìÅË≥™ÔºåËÄåÈÄôÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºÊ®°ÂûãÈñãÁôº‰πãÂâçÁöÑÈöéÊÆµÔºöÂæû EHR Á≥ªÁµ±‰∏≠ÊèêÂèñË≥áÊñôÂíåË≥áÊñôÊ∫ñÂÇô„ÄÇÊàëÂÄëÂàóÂá∫‰∫ÜÈÄô‰∫õÈöéÊÆµ‰∏≠ÈÅáÂà∞ÁöÑÂõõÂçÅÂ§öÈ†ÖÊåëÊà∞Ôºå‰∏¶Êèê‰æõ‰∫ÜÂÖ∑È´îÂèØË°åÁöÑÂª∫Ë≠∞‰æÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞„ÄÇÈÄô‰∫õÊåëÊà∞ÂàÜÁÇ∫ÂõõÈ°ûÔºöÁæ§ÁµÑÂÆöÁæ©„ÄÅÁµêÊûúÂÆöÁæ©„ÄÅÁâπÂæµÂ∑•Á®ãÂíåË≥áÊñôÊ∏ÖÁêÜ„ÄÇÊ≠§Ê∏ÖÂñÆÊó®Âú®‰ΩúÁÇ∫Ë≥áÊñôÊèêÂèñÂ∑•Á®ãÂ∏´ÂíåÁ†îÁ©∂‰∫∫Âì°ÁöÑÂØ¶Áî®ÊåáÂçóÔºåÊîØÊè¥Êõ¥Â•ΩÁöÑÂØ¶ÂãôÔºå‰∏¶ÊîπÂñÑÂãïÊÖãÈ†êÊ∏¨Ê®°ÂûãÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÂìÅË≥™ÂíåÂØ¶ÈöõÊáâÁî®ÊÄß„ÄÇ

##### **Generative Artificial Intelligence: Implications for Biomedical and Health Professions Education**
2501.10186v1 by William Hersh

Generative AI has had a profound impact on biomedicine and health, both in
professional work and in education. Based on large language models (LLMs),
generative AI has been found to perform as well as humans in simulated
situations taking medical board exams, answering clinical questions, solving
clinical cases, applying clinical reasoning, and summarizing information.
Generative AI is also being used widely in education, performing well in
academic courses and their assessments. This review summarizes the successes of
LLMs and highlights some of their challenges in the context of education, most
notably aspects that may undermines the acquisition of knowledge and skills for
professional work. It then provides recommendations for best practices
overcoming shortcomings for LLM use in education. Although there are challenges
for use of generative AI in education, all students and faculty, in biomedicine
and health and beyond, must have understanding and be competent in its use.

ÊëòË¶ÅÔºöÁîüÊàêÂºè AI Â∞çÁîüÁâ©ÈÜ´Â≠∏ÂíåÂÅ•Â∫∑È†òÂüüÁî¢Áîü‰∫ÜÊ∑±ÈÅ†ÁöÑÂΩ±ÈüøÔºåÁÑ°Ë´ñÊòØÂú®Â∞àÊ•≠Â∑•‰ΩúÈÇÑÊòØÊïôËÇ≤ÊñπÈù¢„ÄÇÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÁôºÁèæÁîüÊàêÂºè AI Âú®Ê®°Êì¨ÈÜ´ÁôÇÂßîÂì°ÊúÉËÄÉË©¶„ÄÅÂõûÁ≠îËá®Â∫äÂïèÈ°å„ÄÅËß£Ê±∫Ëá®Â∫äÊ°à‰æã„ÄÅÊáâÁî®Ëá®Â∫äÊé®ÁêÜÂíåÁ∏ΩÁµêË≥áË®äÁ≠âÊÉÖÊ≥Å‰∏ãÔºåË°®ÁèæÂæóËàá‰∫∫È°û‰∏ÄÊ®£Â•Ω„ÄÇÁîüÊàêÂºè AI ‰πüÂª£Ê≥õÊáâÁî®ÊñºÊïôËÇ≤‰∏≠ÔºåÂú®Â≠∏Ë°ìË™≤Á®ãÂèäÂÖ∂Ë©ï‰º∞‰∏≠Ë°®ÁèæËâØÂ•Ω„ÄÇÊú¨ÁØáË©ïË´ñÁ∏ΩÁµê‰∫Ü LLM ÁöÑÊàêÂäüÔºå‰∏¶Âº∑Ë™ø‰∫ÜÂÆÉÂÄëÂú®ÊïôËÇ≤ËÉåÊôØ‰∏ãÁöÑ‰∏Ä‰∫õÊåëÊà∞ÔºåÊúÄÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÂèØËÉΩÊêçÂÆ≥Â∞àÊ•≠Â∑•‰ΩúÁü•Ë≠òÂíåÊäÄËÉΩÁøíÂæóÁöÑÊñπÈù¢„ÄÇÁÑ∂ÂæåÔºåÂÆÉÈáùÂ∞çÂÖãÊúç LLM Âú®ÊïôËÇ≤‰∏≠‰ΩøÁî®ÁöÑÁº∫ÈªûÊèê‰æõ‰∫ÜÊúÄ‰Ω≥ÂØ¶ÂãôÂª∫Ë≠∞„ÄÇÂÑòÁÆ°ÁîüÊàêÂºè AI Âú®ÊïôËÇ≤‰∏≠‰ΩøÁî®Â≠òÂú®ÊåëÊà∞Ôºå‰ΩÜÁîüÁâ©ÈÜ´Â≠∏ÂíåÂÅ•Â∫∑È†òÂüü‰ª•ÂèäÂÖ∂‰ªñÈ†òÂüüÁöÑÊâÄÊúâÂ≠∏ÁîüÂíåÊïôËÅ∑Âì°Â∑•ÈÉΩÂøÖÈ†à‰∫ÜËß£‰∏¶ÁÜüÁ∑¥‰ΩøÁî®ÂÆÉ„ÄÇ

##### **CSSDM Ontology to Enable Continuity of Care Data Interoperability**
2501.10160v1 by Subhashis Das, Debashis Naskar, Sara Rodriguez Gonzalez, Pamela Hussey

The rapid advancement of digital technologies and recent global pandemic
scenarios have led to a growing focus on how these technologies can enhance
healthcare service delivery and workflow to address crises. Action plans that
consolidate existing digital transformation programs are being reviewed to
establish core infrastructure and foundations for sustainable healthcare
solutions. Reforming health and social care to personalize home care, for
example, can help avoid treatment in overcrowded acute hospital settings and
improve the experiences and outcomes for both healthcare professionals and
service users. In this information-intensive domain, addressing the
interoperability challenge through standards-based roadmaps is crucial for
enabling effective connections between health and social care services. This
approach facilitates safe and trustworthy data workflows between different
healthcare system providers. In this paper, we present a methodology for
extracting, transforming, and loading data through a semi-automated process
using a Common Semantic Standardized Data Model (CSSDM) to create personalized
healthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology
of ISO 13940 ContSys and incorporates FHIR-based specifications to support
structural attributes for generating KGs. We propose that the CSSDM facilitates
data harmonization and linking, offering an alternative approach to
interoperability. This approach promotes a novel form of collaboration between
companies developing health information systems and cloud-enabled health
services. Consequently, it provides multiple stakeholders with access to
high-quality data and information sharing.

ÊëòË¶ÅÔºöÊï∏‰ΩçÁßëÊäÄÂø´ÈÄüÈÄ≤Ê≠•ÂíåÊúÄËøëÁöÑÂÖ®ÁêÉÂ§ßÊµÅË°åÁóÖÊÉÖÂ¢ÉÂ∑≤Â∞éËá¥Ë∂ä‰æÜË∂äÂ§ö‰∫∫Â∞àÊ≥®ÊñºÈÄô‰∫õÁßëÊäÄÂ¶Ç‰ΩïÂ¢ûÂº∑ÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãôÊèê‰æõÂíåÂ∑•‰ΩúÊµÅÁ®ã‰ª•ÊáâÂ∞çÂç±Ê©ü„ÄÇÊï¥ÂêàÁèæÊúâÊï∏‰ΩçËΩâÂûãË®àÁï´ÁöÑË°åÂãïË®àÁï´Ê≠£Ë¢´Ê™¢Ë¶ñÔºå‰ª•Âª∫Á´ãÊ∞∏Á∫åÈÜ´ÁôÇ‰øùÂÅ•Ëß£Ê±∫ÊñπÊ°àÁöÑÊ†∏ÂøÉÂü∫Á§éÊû∂ÊßãÂíåÂü∫Á§é„ÄÇ‰æãÂ¶ÇÔºåÊîπÈù©ÈÜ´ÁôÇÂíåÁ§æÊúÉÁÖßË≠∑‰ª•ÂÄã‰∫∫ÂåñÂ±ÖÂÆ∂ÁÖßË≠∑ÔºåÊúâÂä©ÊñºÈÅøÂÖçÂú®‰∫∫ÊªøÁÇ∫ÊÇ£ÁöÑÊÄ•ÊÄßÈÜ´Èô¢Áí∞Â¢É‰∏≠Êé•ÂèóÊ≤ªÁôÇÔºå‰∏¶ÊîπÂñÑÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°ÂíåÊúçÂãô‰ΩøÁî®ËÄÖÁöÑÁ∂ìÈ©óÂíåÁµêÊûú„ÄÇÂú®ÈÄôÂÄãË≥áË®äÂØÜÈõÜÁöÑÈ†òÂüüÔºåÈÄèÈÅéÂü∫ÊñºÊ®ôÊ∫ñÁöÑË∑ØÂæëÂúñ‰æÜËß£Ê±∫‰∫íÈÄöÊÄßÊåëÊà∞ÔºåÂ∞çÊñº‰øÉÊàêÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãôÂíåÁ§æÊúÉÁÖßË≠∑ÊúçÂãô‰πãÈñìÁöÑÊúâÊïàÈÄ£ÁµêËá≥ÈóúÈáçË¶Å„ÄÇÊ≠§ÊñπÊ≥ï‰øÉÊàê‰∏çÂêåÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±‰æõÊáâÂïÜ‰πãÈñìÂÆâÂÖ®‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑË≥áÊñôÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñπÊ≥ïÔºåÈÄèÈÅéÂçäËá™ÂãïÂåñÊµÅÁ®ã‰ΩøÁî®ÈÄöÁî®Ë™ûÊÑèÊ®ôÊ∫ñÂåñË≥áÊñôÊ®°Âûã (CSSDM) ‰æÜËêÉÂèñ„ÄÅËΩâÊèõÂíåËºâÂÖ•Ë≥áÊñôÔºå‰ª•Âª∫Á´ãÂÄã‰∫∫ÂåñÁöÑÈÜ´ÁôÇ‰øùÂÅ•Áü•Ë≠òÂúñË≠ú (KG)„ÄÇCSSDM ‰ª• ISO 13940 ContSys ÁöÑÊ≠£ÂºèÊú¨‰ΩìË´ñÁÇ∫Âü∫Á§éÔºå‰∏¶ÁµêÂêàÂü∫Êñº FHIR ÁöÑË¶èÊ†º‰æÜÊîØÊè¥Áî®ÊñºÁî¢Áîü KG ÁöÑÁµêÊßãÂ±¨ÊÄß„ÄÇÊàëÂÄëÊèêÂá∫ CSSDM ‰øÉÈÄ≤Ë≥áÊñôË™øÂíåÂíåÈÄ£ÁµêÔºåÊèê‰æõ‰∏ÄÁ®Æ‰∫íÈÄöÊÄßÁöÑÊõø‰ª£ÊñπÊ≥ï„ÄÇÊ≠§ÊñπÊ≥ï‰øÉÊàêÈñãÁôºÈÜ´ÁôÇË≥áË®äÁ≥ªÁµ±ÂíåÈõ≤Á´ØÈÜ´ÁôÇÊúçÂãôÁöÑÂÖ¨Âè∏‰πãÈñìÁöÑ‰∏ÄÁ®ÆÊñ∞ÂûãÂêà‰ΩúÂΩ¢Âºè„ÄÇÂõ†Ê≠§ÔºåÂÆÉÊèê‰æõÂ§öÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫Â≠òÂèñÈ´òÂìÅË≥™Ë≥áÊñôÂíåË≥áË®äÂÖ±‰∫´„ÄÇ

##### **landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D Images**
2501.10098v1 by Jef Jonkers, Luc Duchateau, Glenn Van Wallendael, Sofie Van Hoecke

Anatomical landmark localization in 2D/3D images is a critical task in
medical imaging. Although many general-purpose tools exist for landmark
localization in classical computer vision tasks, such as pose estimation, they
lack the specialized features and modularity necessary for anatomical landmark
localization applications in the medical domain. Therefore, we introduce
landmarker, a Python package built on PyTorch. The package provides a
comprehensive, flexible toolkit for developing and evaluating landmark
localization algorithms, supporting a range of methodologies, including static
and adaptive heatmap regression. landmarker enhances the accuracy of landmark
identification, streamlines research and development processes, and supports
various image formats and preprocessing pipelines. Its modular design allows
users to customize and extend the toolkit for specific datasets and
applications, accelerating innovation in medical imaging. landmarker addresses
a critical need for precision and customization in landmark localization tasks
not adequately met by existing general-purpose pose estimation tools.

ÊëòË¶ÅÔºöÂú® 2D/3D ÂΩ±ÂÉè‰∏≠ÈÄ≤Ë°åËß£ÂâñÊ®ôË™åÂÆö‰ΩçÊòØÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑ‰∏ÄÈ†ÖÈóúÈçµ‰ªªÂãô„ÄÇÂÑòÁÆ°ÊúâË®±Â§öÈÄöÁî®Â∑•ÂÖ∑ÂèØÁî®ÊñºÁ∂ìÂÖ∏ÈõªËÖ¶Ë¶ñË¶∫‰ªªÂãô‰∏≠ÁöÑÊ®ôË™åÂÆö‰ΩçÔºå‰æãÂ¶ÇÂßøÂã¢‰º∞Ë®àÔºå‰ΩÜÂÆÉÂÄëÁº∫‰πèËß£ÂâñÊ®ôË™åÂÆö‰ΩçÊáâÁî®Âú®ÈÜ´Â≠∏È†òÂüü‰∏≠ÊâÄÈúÄÁöÑÂ∞àÊ•≠ÂäüËÉΩÂíåÊ®°ÁµÑÂåñ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü landmarkerÔºå‰∏ÄÂÄãÂª∫Á´ãÂú® PyTorch ‰∏äÁöÑ Python Â•ó‰ª∂„ÄÇË©≤Â•ó‰ª∂Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢‰∏îÈùàÊ¥ªÁöÑÂ∑•ÂÖ∑ÂåÖÔºåÁî®ÊñºÈñãÁôºÂíåË©ï‰º∞Ê®ôË™åÂÆö‰ΩçÊºîÁÆóÊ≥ïÔºåÊîØÊè¥ÂêÑÁ®ÆÊñπÊ≥ïÔºåÂåÖÊã¨ÈùúÊÖãÂíåËá™ÈÅ©ÊáâÁÜ±ÂúñÂõûÊ≠∏„ÄÇlandmarker ÊèêÂçá‰∫ÜÊ®ôË™åË≠òÂà•ÁöÑÊ∫ñÁ¢∫ÊÄßÔºåÁ∞°Âåñ‰∫ÜÁ†îÁ©∂ÂíåÈñãÁôºÊµÅÁ®ãÔºå‰∏¶ÊîØÊè¥ÂêÑÁ®ÆÂΩ±ÂÉèÊ†ºÂºèÂíåÂâçËôïÁêÜÁÆ°ÈÅì„ÄÇÂÖ∂Ê®°ÁµÑÂåñË®≠Ë®à‰ΩøÁî®Êà∂ËÉΩÂ§†Ëá™Ë®ÇÂíåÂª∂‰º∏Â∑•ÂÖ∑ÂåÖÔºå‰ª•ÈÅ©Áî®ÊñºÁâπÂÆöË≥áÊñôÈõÜÂíåÊáâÁî®ÔºåÂä†ÈÄüÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÂâµÊñ∞„ÄÇlandmarker ÊªøË∂≥‰∫ÜÁèæÊúâÈÄöÁî®ÂßøÂã¢‰º∞Ë®àÂ∑•ÂÖ∑ÁÑ°Ê≥ïÂÖÖÂàÜÊªøË∂≥ÁöÑÊ®ôË™åÂÆö‰Ωç‰ªªÂãô‰∏≠Â∞çÊñºÁ≤æÁ¢∫Â∫¶ÂíåËá™Ë®ÇÂåñÁöÑÈóúÈçµÈúÄÊ±Ç„ÄÇ

##### **Deep Learning for Early Alzheimer Disease Detection with MRI Scans**
2501.09999v1 by Mohammad Rafsan, Tamer Oraby, Upal Roy, Sanjeev Kumar, Hansapani Rodrigo

Alzheimer's Disease is a neurodegenerative condition characterized by
dementia and impairment in neurological function. The study primarily focuses
on the individuals above age 40, affecting their memory, behavior, and
cognitive processes of the brain. Alzheimer's disease requires diagnosis by a
detailed assessment of MRI scans and neuropsychological tests of the patients.
This project compares existing deep learning models in the pursuit of enhancing
the accuracy and efficiency of AD diagnosis, specifically focusing on the
Convolutional Neural Network, Bayesian Convolutional Neural Network, and the
U-net model with the Open Access Series of Imaging Studies brain MRI dataset.
Besides, to ensure robustness and reliability in the model evaluations, we
address the challenge of imbalance in data. We then perform rigorous evaluation
to determine strengths and weaknesses for each model by considering
sensitivity, specificity, and computational efficiency. This comparative
analysis would shed light on the future role of AI in revolutionizing AD
diagnostics but also paved ways for future innovation in medical imaging and
the management of neurodegenerative diseases.

ÊëòË¶ÅÔºöÈòøËå≤Êµ∑ÈªòÁóáÊòØ‰∏ÄÁ®ÆÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖÔºåÁâπÂæµÁÇ∫Â§±Êô∫ÂíåÁ•ûÁ∂ìÂäüËÉΩÂèóÊêç„ÄÇÊú¨Á†îÁ©∂‰∏ªË¶ÅÈáùÂ∞ç 40 Ê≠≤‰ª•‰∏äÁöÑÂÄã‰∫∫ÔºåÂΩ±Èüø‰ªñÂÄëÁöÑË®òÊÜ∂Âäõ„ÄÅË°åÁÇ∫ÂíåË™çÁü•ÈÅéÁ®ã„ÄÇÈòøËå≤Êµ∑ÈªòÁóáÈúÄË¶ÅÈÄèÈÅéË©≥Á¥∞Ë©ï‰º∞ÁóÖÊÇ£ÁöÑ MRI ÊéÉÊèèÂíåÁ•ûÁ∂ìÂøÉÁêÜÊ∏¨Ë©¶‰æÜË®∫Êñ∑„ÄÇÊú¨Â∞àÊ°àÊØîËºÉÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰ª•Â∞ãÊ±ÇÊèêÂçá AD Ë®∫Êñ∑ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊïàÁéáÔºåÁâπÂà•ËëóÈáçÊñºÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅË≤ùÊ∞èÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíå U-net Ê®°ÂûãÔºå‰ª•ÂèäÈñãÊîæÂèñÁî®ÂΩ±ÂÉèÁ†îÁ©∂Á≥ªÂàóÁöÑËÖ¶ÈÉ® MRI Ë≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜÁ¢∫‰øùÊ®°ÂûãË©ï‰º∞ÁöÑÁ©©ÂÅ•ÊÄßÂíåÂèØÈù†ÊÄßÔºåÊàëÂÄëËß£Ê±∫‰∫ÜË≥áÊñô‰∏çÂπ≥Ë°°ÁöÑÊåëÊà∞„ÄÇÊé•ËëóÊàëÂÄëÂü∑Ë°åÂö¥Ë¨πÁöÑË©ï‰º∞ÔºåÈÄèÈÅéËÄÉÈáèÊïèÊÑüÂ∫¶„ÄÅÁâπÁï∞Â∫¶ÂíåË®àÁÆóÊïàÁéá‰æÜÁ¢∫ÂÆöÊØèÂÄãÊ®°ÂûãÁöÑÂÑ™Áº∫Èªû„ÄÇÊ≠§ÊØîËºÉÂàÜÊûêÂ∞áÈó°Êòé AI Âú®Èù©Êñ∞ AD Ë®∫Êñ∑ÊñπÈù¢ÁöÑÊú™‰æÜËßíËâ≤Ôºå‰πüÁÇ∫ÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖÁÆ°ÁêÜÁöÑÊú™‰æÜÂâµÊñ∞Èã™Ë∑Ø„ÄÇ

##### **Aneumo: A Large-Scale Comprehensive Synthetic Dataset of Aneurysm Hemodynamics**
2501.09980v1 by Xigui Li, Yuanye Zhou, Feiyang Xiao, Xin Guo, Yichi Zhang, Chen Jiang, Jianchao Ge, Xiansheng Wang, Qimeng Wang, Taiwei Zhang, Chensen Lin, Yuan Cheng, Yuan Qi

Intracranial aneurysm (IA) is a common cerebrovascular disease that is
usually asymptomatic but may cause severe subarachnoid hemorrhage (SAH) if
ruptured. Although clinical practice is usually based on individual factors and
morphological features of the aneurysm, its pathophysiology and hemodynamic
mechanisms remain controversial. To address the limitations of current
research, this study constructed a comprehensive hemodynamic dataset of
intracranial aneurysms. The dataset is based on 466 real aneurysm models, and
10,000 synthetic models were generated by resection and deformation operations,
including 466 aneurysm-free models and 9,534 deformed aneurysm models. The
dataset also provides medical image-like segmentation mask files to support
insightful analysis. In addition, the dataset contains hemodynamic data
measured at eight steady-state flow rates (0.001 to 0.004 kg/s), including
critical parameters such as flow velocity, pressure, and wall shear stress,
providing a valuable resource for investigating aneurysm pathogenesis and
clinical prediction. This dataset will help advance the understanding of the
pathologic features and hemodynamic mechanisms of intracranial aneurysms and
support in-depth research in related fields. Dataset hosted at
https://github.com/Xigui-Li/Aneumo.

ÊëòË¶ÅÔºöÈ°±ÂÖßÂãïËÑàÁò§ÔºàIAÔºâÊòØ‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑËÖ¶Ë°ÄÁÆ°ÁñæÁóÖÔºåÈÄöÂ∏∏ÁÑ°ÁóáÁãÄÔºå‰ΩÜÂ¶ÇÊûúÁ†¥Ë£ÇÂèØËÉΩÊúÉÂ∞éËá¥Âö¥ÈáçÁöÑËõõÁ∂≤ËÜú‰∏ãËÖîÂá∫Ë°ÄÔºàSAHÔºâ„ÄÇÂÑòÁÆ°Ëá®Â∫äÂØ¶ÂãôÈÄöÂ∏∏Âü∫ÊñºÂÄãÈ´îÂõ†Á¥†ÂíåÂãïËÑàÁò§ÁöÑÂΩ¢ÊÖãÁâπÂæµÔºå‰ΩÜÂÖ∂ÁóÖÁêÜÁîüÁêÜÂ≠∏ÂíåË°ÄÊµÅÂãïÂäõÂ≠∏Ê©üÂà∂‰ªçÂ≠òÂú®Áà≠Ë≠∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Áï∂ÂâçÁ†îÁ©∂ÁöÑÈôêÂà∂ÔºåÊú¨Á†îÁ©∂ÊßãÂª∫‰∫Ü‰∏ÄÂÄãÈ°±ÂÖßÂãïËÑàÁò§ÁöÑÂÖ®Èù¢Ë°ÄÊµÅÂãïÂäõÂ≠∏Êï∏ÊìöÈõÜ„ÄÇË©≤Êï∏ÊìöÈõÜÂü∫Êñº 466 ÂÄãÁúüÂØ¶ÂãïËÑàÁò§Ê®°ÂûãÔºå‰∏¶ÈÄöÈÅéÂàáÈô§ÂíåËÆäÂΩ¢Êìç‰ΩúÁîüÊàê‰∫Ü 10,000 ÂÄãÂêàÊàêÊ®°ÂûãÔºåÂåÖÊã¨ 466 ÂÄãÁÑ°ÂãïËÑàÁò§Ê®°ÂûãÂíå 9,534 ÂÄãËÆäÂΩ¢ÂãïËÑàÁò§Ê®°Âûã„ÄÇË©≤Êï∏ÊìöÈõÜÈÇÑÊèê‰æõ‰∫ÜÈ°ûÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÂàÜÂâ≤ÈÅÆÁΩ©Ê™îÊ°àÔºå‰ª•ÊîØÊåÅÊ∑±ÂÖ•ÂàÜÊûê„ÄÇÊ≠§Â§ñÔºåË©≤Êï∏ÊìöÈõÜÂåÖÂê´Âú®ÂÖ´ÂÄãÁ©©ÊÖãÊµÅÈÄüÔºà0.001 Ëá≥ 0.004 kg/sÔºâ‰∏ãÊ∏¨ÈáèÁöÑË°ÄÊµÅÂãïÂäõÂ≠∏Êï∏ÊìöÔºåÂåÖÊã¨ÊµÅÈÄü„ÄÅÂ£ìÂäõÂíåÂ£ÅÈù¢Ââ™ÊáâÂäõÁ≠âÈóúÈçµÂèÉÊï∏ÔºåÁÇ∫Á†îÁ©∂ÂãïËÑàÁò§ÁôºÁóÖÊ©üÂà∂ÂíåËá®Â∫äÈ†êÊ∏¨Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË≥áÊ∫ê„ÄÇÊ≠§Êï∏ÊìöÈõÜÂ∞áÊúâÂä©ÊñºÂ¢ûÈÄ≤Â∞çÈ°±ÂÖßÂãïËÑàÁò§ÁóÖÁêÜÁâπÂæµÂíåË°ÄÊµÅÂãïÂäõÂ≠∏Ê©üÂà∂ÁöÑ‰∫ÜËß£Ôºå‰∏¶ÊîØÊåÅÁõ∏ÈóúÈ†òÂüüÁöÑÊ∑±ÂÖ•Á†îÁ©∂„ÄÇÊï∏ÊìöÈõÜË®óÁÆ°Êñº https://github.com/Xigui-Li/Aneumo„ÄÇ

##### **Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude**
2501.10484v1 by Yile Yan, Yuqi Zhu, Wentao Xu

Recent advances in Large Language Models (LLMs) have enabled human-like
responses across various tasks, raising questions about their ethical
decision-making capabilities and potential biases. This study investigates
protected attributes in LLMs through systematic evaluation of their responses
to ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5
Sonnet - we analyzed their decision-making patterns across multiple protected
attributes including age, gender, race, appearance, and disability status.
Through 11,200 experimental trials involving both single-factor and two-factor
protected attribute combinations, we evaluated the models' ethical preferences,
sensitivity, stability, and clustering of preferences. Our findings reveal
significant protected attributeses in both models, with consistent preferences
for certain features (e.g., "good-looking") and systematic neglect of others.
Notably, while GPT-3.5 Turbo showed stronger preferences aligned with
traditional power structures, Claude 3.5 Sonnet demonstrated more diverse
protected attribute choices. We also found that ethical sensitivity
significantly decreases in more complex scenarios involving multiple protected
attributes. Additionally, linguistic referents heavily influence the models'
ethical evaluations, as demonstrated by differing responses to racial
descriptors (e.g., "Yellow" versus "Asian"). These findings highlight critical
concerns about the potential impact of LLM biases in autonomous decision-making
systems and emphasize the need for careful consideration of protected
attributes in AI development. Our study contributes to the growing body of
research on AI ethics by providing a systematic framework for evaluating
protected attributes in LLMs' ethical decision-making capabilities.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËøëÊúüÁöÑÈÄ≤Â±ïÔºåËÆì‰∫∫ÂÄëÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÈÉΩËÉΩÂÅöÂá∫È°û‰ºº‰∫∫È°ûÁöÑÂõûÊáâÔºåÈÄô‰πüÂºïÁôº‰∫Ü‰∫∫ÂÄëÂ∞çÂÖ∂ÈÅìÂæ∑Ê±∫Á≠ñËÉΩÂäõÂíåÊΩõÂú®ÂÅèË¶ãÁöÑË≥™Áñë„ÄÇÊú¨Á†îÁ©∂ÈÄèÈÅéÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞ LLM Â∞çÈÅìÂæ∑Âõ∞Â¢ÉÁöÑÂõûÊáâÔºå‰æÜÊé¢Ë®éÂèó‰øùË≠∑Â±¨ÊÄßÂú® LLM ‰∏≠ÁöÑË°®Áèæ„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãËëóÂêçÁöÑÊ®°Âûã - GPT-3.5 Turbo Âíå Claude 3.5 Sonnet - ÂàÜÊûê‰∫ÜÂÆÉÂÄëÂú®Â§öÂÄãÂèó‰øùË≠∑Â±¨ÊÄßÔºàÂåÖÊã¨Âπ¥ÈΩ°„ÄÅÊÄßÂà•„ÄÅÁ®ÆÊóè„ÄÅÂ§ñË≤åÂíåÊÆòÁñæÁãÄÊÖãÔºâ‰∏äÁöÑÊ±∫Á≠ñÊ®°Âºè„ÄÇÈÄèÈÅé 11,200 Ê¨°ÂØ¶È©óË©¶È©óÔºàÂåÖÊã¨ÂñÆÂõ†Á¥†ÂíåÈõôÂõ†Á¥†Âèó‰øùË≠∑Â±¨ÊÄßÁµÑÂêàÔºâÔºåÊàëÂÄëË©ï‰º∞‰∫ÜÊ®°ÂûãÁöÑÈÅìÂæ∑ÂÅèÂ•Ω„ÄÅÊïèÊÑüÂ∫¶„ÄÅÁ©©ÂÆöÊÄßÂíåÂÅèÂ•ΩÁæ§ÈõÜ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÈÄôÂÖ©ÂÄãÊ®°Âûã‰∏≠È°ØËëóÁöÑÂèó‰øùË≠∑Â±¨ÊÄßÔºåÂÆÉÂÄëÂ∞çÊüê‰∫õÁâπÂæµÔºà‰æãÂ¶Ç„ÄåÂ•ΩÁúã„ÄçÔºâÊúâÊåÅÁ∫åÁöÑÂÅèÂ•ΩÔºå‰∏¶‰∏îÁ≥ªÁµ±ÊÄßÂú∞ÂøΩÁï•ÂÖ∂‰ªñÁâπÂæµ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÈõñÁÑ∂ GPT-3.5 Turbo Ë°®ÁèæÂá∫ËàáÂÇ≥Áµ±Ê¨äÂäõÁµêÊßã‰∏ÄËá¥ÁöÑÂº∑ÁÉàÂÅèÂ•ΩÔºå‰ΩÜ Claude 3.5 Sonnet ÂâáË°®ÁèæÂá∫Êõ¥Â§öÊ®£ÂåñÁöÑÂèó‰øùË≠∑Â±¨ÊÄßÈÅ∏Êìá„ÄÇÊàëÂÄëÈÇÑÁôºÁèæÔºåÂú®Ê∂âÂèäÂ§öÂÄãÂèó‰øùË≠∑Â±¨ÊÄßÁöÑÊõ¥Ë§áÈõúÂ†¥ÊôØ‰∏≠ÔºåÈÅìÂæ∑ÊïèÊÑüÂ∫¶ÊúÉÈ°ØËëóÈôç‰Ωé„ÄÇÊ≠§Â§ñÔºåË™ûË®ÄÊåáÁ®±ÊúÉÂö¥ÈáçÂΩ±ÈüøÊ®°ÂûãÁöÑÈÅìÂæ∑Ë©ï‰º∞ÔºåÈÄôÂæûÂ∞çÁ®ÆÊóèÊèèËø∞Á¨¶Ôºà‰æãÂ¶Ç„ÄåÈªÉËâ≤„ÄçËàá„Äå‰∫ûÊ¥≤‰∫∫„ÄçÔºâÁöÑ‰∏çÂêåÂõûÊáâ‰∏≠ÂèØ‰ª•ÁúãÂá∫„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫Ü LLM ÂÅèË¶ãÂú®Ëá™‰∏ªÊ±∫Á≠ñÁ≥ªÁµ±‰∏≠ÊΩõÂú®ÂΩ±ÈüøÁöÑÈóúÈçµÂïèÈ°åÔºå‰∏¶Âº∑Ë™øÂú® AI ÈñãÁôº‰∏≠‰ªîÁ¥∞ËÄÉÊÖÆÂèó‰øùË≠∑Â±¨ÊÄßÁöÑÂøÖË¶ÅÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÈÄèÈÅéÊèê‰æõ‰∏ÄÂÄãÁ≥ªÁµ±ÊÄßÁöÑÊû∂Êßã‰æÜË©ï‰º∞ LLM ÈÅìÂæ∑Ê±∫Á≠ñËÉΩÂäõ‰∏≠ÁöÑÂèó‰øùË≠∑Â±¨ÊÄßÔºåÁÇ∫ AI ÂÄ´ÁêÜÈ†òÂüüÁöÑÁ†îÁ©∂ÂÅöÂá∫‰∫ÜË≤¢Áçª„ÄÇ

##### **Bridging Language Barriers in Healthcare: A Study on Arabic LLMs**
2501.09825v1 by Nada Saadi, Tathagata Raha, Cl√©ment Christophe, Marco AF Pimentel, Ronnie Rajan, Praveen K Kanithi

This paper investigates the challenges of developing large language models
(LLMs) proficient in both multilingual understanding and medical knowledge. We
demonstrate that simply translating medical data does not guarantee strong
performance on clinical tasks in the target language. Our experiments reveal
that the optimal language mix in training data varies significantly across
different medical tasks. We find that larger models with carefully calibrated
language ratios achieve superior performance on native-language clinical tasks.
Furthermore, our results suggest that relying solely on fine-tuning may not be
the most effective approach for incorporating new language knowledge into LLMs.
Instead, data and computationally intensive pretraining methods may still be
necessary to achieve optimal performance in multilingual medical settings.
These findings provide valuable guidance for building effective and inclusive
medical AI systems for diverse linguistic communities.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫ÜÈñãÁôºÊó¢Á≤æÈÄöÂ§öË™ûË®ÄÁêÜËß£ÂèàÁ≤æÈÄöÈÜ´ÁôÇÁü•Ë≠òÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊåëÊà∞„ÄÇÊàëÂÄëË≠âÊòéÔºåÂÉÖÁøªË≠ØÈÜ´ÁôÇË≥áÊñô‰∏¶‰∏çËÉΩ‰øùË≠âÂú®ÁõÆÊ®ôË™ûË®ÄÁöÑËá®Â∫ä‰ªªÂãô‰∏≠Ë°®ÁèæÂá∫Ëâ≤„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÊè≠Á§∫ÔºåË®ìÁ∑¥Ë≥áÊñô‰∏≠ÁöÑÊúÄ‰Ω≥Ë™ûË®ÄÁµÑÂêàÂõ†‰∏çÂêåÁöÑÈÜ´ÁôÇ‰ªªÂãôËÄåÁï∞„ÄÇÊàëÂÄëÁôºÁèæÔºåÂÖ∑Êúâ‰ªîÁ¥∞Ê†°Ê∫ñË™ûË®ÄÊØî‰æãÁöÑËºÉÂ§ßÊ®°ÂûãÂú®ÊØçË™ûËá®Â∫ä‰ªªÂãô‰∏≠Ë°®ÁèæÊõ¥‰Ω≥„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂÉÖ‰æùË≥¥ÂæÆË™øÂèØËÉΩ‰∏çÊòØÂ∞áÊñ∞ÁöÑË™ûË®ÄÁü•Ë≠òÁ¥çÂÖ• LLM ÁöÑÊúÄÊúâÊïàÊñπÊ≥ï„ÄÇÁõ∏ÂèçÔºåË≥áÊñôÂíåË®àÁÆóÂØÜÈõÜÂûãÈ†êË®ìÁ∑¥ÊñπÊ≥ïÂ∞çÊñºÂú®Â§öË™ûË®ÄÈÜ´ÁôÇÁí∞Â¢É‰∏≠ÂØ¶ÁèæÊúÄ‰Ω≥ÊïàËÉΩÂèØËÉΩ‰ªçÁÑ∂ÂøÖË¶Å„ÄÇÈÄô‰∫õÁôºÁèæÁÇ∫Âª∫Á´ãÊúâÊïà‰∏îÂåÖÂÆπÊÄßÁöÑÈÜ´ÁôÇ AI Á≥ªÁµ±Ôºå‰ª•ÊúçÂãôÊñº‰∏çÂêåÁöÑË™ûË®ÄÁ§æÁæ§ÔºåÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑÊåáÂ∞éÊñπÈáù„ÄÇ

##### **KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity Recognition and Normalization for Dysmorphology Physical Examination Reports**
2501.09744v1 by Hajung Kim, Chanhwi Kim, Jiwoong Sohn, Tim Beck, Marek Rei, Sunkyu Kim, T Ian Simpson, Joram M Posma, Antoine Lain, Mujeen Sung, Jaewoo Kang

The objective of BioCreative8 Track 3 is to extract phenotypic key medical
findings embedded within EHR texts and subsequently normalize these findings to
their Human Phenotype Ontology (HPO) terms. However, the presence of diverse
surface forms in phenotypic findings makes it challenging to accurately
normalize them to the correct HPO terms. To address this challenge, we explored
various models for named entity recognition and implemented data augmentation
techniques such as synonym marginalization to enhance the normalization step.
Our pipeline resulted in an exact extraction and normalization F1 score 2.6\%
higher than the mean score of all submissions received in response to the
challenge. Furthermore, in terms of the normalization F1 score, our approach
surpassed the average performance by 1.9\%. These findings contribute to the
advancement of automated medical data extraction and normalization techniques,
showcasing potential pathways for future research and application in the
biomedical domain.

ÊëòË¶ÅÔºöBioCreative8 ËªåÈÅì 3 ÁöÑÁõÆÊ®ôÊòØÂæûÈõªÂ≠êÁóÖÊ≠∑ÊñáÊú¨‰∏≠ËêÉÂèñË°®ÂûãÈóúÈçµÈÜ´ÁôÇÁôºÁèæÔºå‰∏¶Â∞áÈÄô‰∫õÁôºÁèæÊ®ôÊ∫ñÂåñÁÇ∫‰∫∫È°ûË°®ÂûãÊú¨‰Ωì (HPO) Ê¢ùÊ¨æ„ÄÇÁÑ∂ËÄåÔºåË°®ÂûãÁôºÁèæ‰∏≠Â≠òÂú®Â§öÊ®£ÂåñÁöÑË°®Èù¢ÂΩ¢ÂºèÔºåÈÄô‰ΩøÂæóÂ∞áÂÖ∂Ê∫ñÁ¢∫Ê®ôÊ∫ñÂåñÁÇ∫Ê≠£Á¢∫ÁöÑ HPO Ê¢ùÊ¨æÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂëΩÂêçÂØ¶È´îË≠òÂà•ÁöÑÂêÑÁ®ÆÊ®°ÂûãÔºå‰∏¶ÂØ¶‰Ωú‰∫ÜË≥áÊñôÊì¥ÂÖÖÊäÄË°ìÔºå‰æãÂ¶ÇÂêåÁæ©Ë©ûÈÇäÁ∑£ÂåñÔºå‰ª•Â¢ûÂº∑Ê®ôÊ∫ñÂåñÊ≠•È©ü„ÄÇÊàëÂÄëÁöÑÁÆ°ÈÅìÁî¢Áîü‰∫ÜÁ≤æÁ¢∫ÁöÑËêÉÂèñÂíåÊ®ôÊ∫ñÂåñ F1 ÂàÜÊï∏ÔºåÊØîÂõûÊáâÊåëÊà∞ÊâÄÊî∂Âà∞ÁöÑÊâÄÊúâÊèê‰∫§ÁöÑÂπ≥ÂùáÂàÜÊï∏È´ò 2.6%„ÄÇÊ≠§Â§ñÔºåÂú®Ê®ôÊ∫ñÂåñ F1 ÂàÜÊï∏ÊñπÈù¢ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊØîÂπ≥ÂùáË°®ÁèæÈ´òÂá∫ 1.9%„ÄÇÈÄô‰∫õÁôºÁèæÊúâÂä©ÊñºËá™ÂãïÂåñÈÜ´ÁôÇË≥áÊñôËêÉÂèñÂíåÊ®ôÊ∫ñÂåñÊäÄË°ìÁöÑÈÄ≤Â±ïÔºåÂ±ïÁ§∫‰∫ÜÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÊú™‰æÜÁ†îÁ©∂ÂíåÊáâÁî®ÁöÑÊΩõÂú®ÈÄîÂæë„ÄÇ

##### **Electronic Health Records: Towards Digital Twins in Healthcare**
2501.09640v1 by Muhammet Alkan, Hester Huijsdens, Yola Jones, Fani Deligianni

The pivotal shift from traditional paper-based records to sophisticated
Electronic Health Records (EHR), enabled systematic collection and analysis of
patient data through descriptive statistics, providing insight into patterns
and trends across patient populations. This evolution continued toward
predictive analytics, allowing healthcare providers to anticipate patient
outcomes and potential complications before they occur. This progression from
basic digital record-keeping to sophisticated predictive modelling and digital
twins reflects healthcare's broader evolution toward more integrated,
patient-centred approaches that combine data-driven insights with personalized
care delivery. This chapter explores the evolution and significance of
healthcare information systems, beginning with an examination of the
implementation of EHR in the UK and the USA. It provides a comprehensive
overview of the International Classification of Diseases (ICD) system, tracing
its development from ICD-9 to ICD-10. Central to this discussion is the
MIMIC-III database, a landmark achievement in healthcare data sharing and
arguably the most comprehensive critical care database freely available to
researchers worldwide. MIMIC-III has democratized access to high-quality
healthcare data, enabling unprecedented opportunities for research and
analysis. The chapter examines its structure, clinical outcome analysis
capabilities, and practical applications through case studies, with a
particular focus on mortality and length of stay metrics, vital signs
extraction, and ICD coding. Through detailed entity-relationship diagrams and
practical examples, the text illustrates MIMIC's complex data structure and
demonstrates how different querying approaches can lead to subtly different
results, emphasizing the critical importance of understanding the database's
architecture for accurate data extraction.

ÊëòË¶ÅÔºöÂæûÂÇ≥Áµ±Á¥ôÊú¨Ë®òÈåÑËΩâËÆäÁÇ∫ÂÖàÈÄ≤ÁöÑÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑÔºàEHRÔºâÔºå‰øÉ‰ΩøÈÄèÈÅéÊèèËø∞ÊÄßÁµ±Ë®àÁ≥ªÁµ±ÊÄßÂú∞Êî∂ÈõÜÂíåÂàÜÊûêÁóÖÊÇ£Ë≥áÊñôÔºåÈÄ≤ËÄåÊ∑±ÂÖ•‰∫ÜËß£ÁóÖÊÇ£ÊóèÁæ§ÁöÑÊ®°ÂºèÂíåË∂®Âã¢„ÄÇÈÄôÈ†ÖÊºîÈÄ≤ÊåÅÁ∫åÊúùÂêëÈ†êÊ∏¨ÂàÜÊûêÁôºÂ±ïÔºåËÆìÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖËÉΩÂ§†Âú®ÁóÖÊÇ£Âá∫ÁèæÁµêÊûúÂíåÊΩõÂú®‰ΩµÁôºÁóá‰πãÂâçÈ†êÊ∏¨ÈÄô‰∫õÁãÄÊ≥Å„ÄÇÂæûÂü∫Êú¨ÁöÑÊï∏‰ΩçË®òÈåÑ‰øùÂ≠òÈÄ≤Â±ïÂà∞ÂÖàÈÄ≤ÁöÑÈ†êÊ∏¨Ê®°ÂûãÂíåÊï∏‰ΩçÈõôËÉûËÉéÔºåÂèçÊò†‰∫ÜÈÜ´ÁôÇ‰øùÂÅ•ÊúùÂêëÊõ¥Êï¥Âêà„ÄÅ‰ª•ÁóÖÊÇ£ÁÇ∫‰∏≠ÂøÉÁöÑÂÅöÊ≥ïÊâÄÂÅöÁöÑÊõ¥Âª£Ê≥õÊºîÈÄ≤ÔºåÈÄô‰∫õÂÅöÊ≥ïÁµêÂêà‰∫ÜË≥áÊñôÈ©ÖÂãïÁöÑË¶ãËß£ËàáÂÄã‰∫∫ÂåñÁÖßË≠∑ÊúçÂãô„ÄÇÊú¨Á´†Êé¢Ë®éÈÜ´ÁôÇ‰øùÂÅ•Ë≥áË®äÁ≥ªÁµ±ÁöÑÊºîÈÄ≤ÂíåÈáçË¶ÅÊÄßÔºåÂæûÂØ©Êü•Ëã±ÂúãÂíåÁæéÂúãÂØ¶ÊñΩ EHR ÈñãÂßã„ÄÇÂÆÉÊèê‰æõ‰∫ÜÁñæÁóÖÂúãÈöõÂàÜÈ°ûÔºàICDÔºâÁ≥ªÁµ±ÁöÑÂÖ®Èù¢Ê¶ÇËø∞ÔºåËøΩÊ∫ØÂÖ∂Âæû ICD-9 ÁôºÂ±ïÂà∞ ICD-10 ÁöÑÈÅéÁ®ã„ÄÇÊ≠§Ë®éË´ñÁöÑÊ†∏ÂøÉÊòØ MIMIC-III Ë≥áÊñôÂ∫´ÔºåÈÄôÊòØÈÜ´ÁôÇ‰øùÂÅ•Ë≥áÊñôÂÖ±‰∫´ÁöÑ‰∏ÄÈ†ÖÈáåÁ®ãÁ¢ëÂºèÊàêÂ∞±ÔºåÂèØ‰ª•Ë™™ÊòØÂÖ®ÁêÉÁ†îÁ©∂‰∫∫Âì°ÂèØ‰ª•ÂÖçË≤ªÂèñÂæóÁöÑÊúÄÂÖ®Èù¢ÁöÑÈáçÁóáÁÖßË≠∑Ë≥áÊñôÂ∫´„ÄÇMIMIC-III Ê∞ë‰∏ªÂåñ‰∫ÜÂ∞çÈ´òÂìÅË≥™ÈÜ´ÁôÇ‰øùÂÅ•Ë≥áÊñôÁöÑÂ≠òÂèñÔºåÁÇ∫Á†îÁ©∂ÂíåÂàÜÊûêÂâµÈÄ†‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑÊ©üÊúÉ„ÄÇÊú¨Á´†ÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂Êé¢Ë®éÂÖ∂ÁµêÊßã„ÄÅËá®Â∫äÁµêÊûúÂàÜÊûêËÉΩÂäõÂíåÂØ¶ÈöõÊáâÁî®ÔºåÁâπÂà•ÈóúÊ≥®Ê≠ª‰∫°ÁéáÂíå‰ΩèÈô¢ÊôÇÈñìÊåáÊ®ô„ÄÅÁîüÂëΩÂæµË±°ËêÉÂèñÂíå ICD Á∑®Á¢º„ÄÇÈÄèÈÅéË©≥Á¥∞ÁöÑÂØ¶È´îÈóú‰øÇÂúñÂíåÂØ¶ÂãôÁØÑ‰æãÔºåÊú¨ÊñáË™™Êòé‰∫Ü MIMIC Ë§áÈõúÁöÑË≥áÊñôÁµêÊßãÔºå‰∏¶Â±ïÁ§∫‰∫Ü‰∏çÂêåÁöÑÊü•Ë©¢ÊñπÊ≥ïÂ¶Ç‰ΩïÂ∞éËá¥Á¥∞ÂæÆ‰∏çÂêåÁöÑÁµêÊûúÔºåÂº∑Ë™ø‰∫Ü‰∫ÜËß£Ë≥áÊñôÂ∫´Êû∂ÊßãÂ∞çÊñºÊ∫ñÁ¢∫ËêÉÂèñË≥áÊñôËá≥ÈóúÈáçË¶ÅÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **Artificial Intelligence-Driven Clinical Decision Support Systems**
2501.09628v1 by Muhammet Alkan, Idris Zakariyya, Samuel Leighton, Kaushik Bhargav Sivangi, Christos Anagnostopoulos, Fani Deligianni

As artificial intelligence (AI) becomes increasingly embedded in healthcare
delivery, this chapter explores the critical aspects of developing reliable and
ethical Clinical Decision Support Systems (CDSS). Beginning with the
fundamental transition from traditional statistical models to sophisticated
machine learning approaches, this work examines rigorous validation strategies
and performance assessment methods, including the crucial role of model
calibration and decision curve analysis. The chapter emphasizes that creating
trustworthy AI systems in healthcare requires more than just technical
accuracy; it demands careful consideration of fairness, explainability, and
privacy. The challenge of ensuring equitable healthcare delivery through AI is
stressed, discussing methods to identify and mitigate bias in clinical
predictive models. The chapter then delves into explainability as a cornerstone
of human-centered CDSS. This focus reflects the understanding that healthcare
professionals must not only trust AI recommendations but also comprehend their
underlying reasoning. The discussion advances in an analysis of privacy
vulnerabilities in medical AI systems, from data leakage in deep learning
models to sophisticated attacks against model explanations. The text explores
privacy-preservation strategies such as differential privacy and federated
learning, while acknowledging the inherent trade-offs between privacy
protection and model performance. This progression, from technical validation
to ethical considerations, reflects the multifaceted challenges of developing
AI systems that can be seamlessly and reliably integrated into daily clinical
practice while maintaining the highest standards of patient care and data
protection.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖß (AI) Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊáâÁî®Êó•ÁõäÊôÆÂèäÔºåÊú¨Á´†Êé¢Ë®é‰∫ÜÈñãÁôºÂèØÈù†‰∏îÁ¨¶ÂêàÈÅìÂæ∑Ê®ôÊ∫ñÁöÑËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ± (CDSS) ÁöÑÈóúÈçµÈù¢Âêë„ÄÇÂæûÂÇ≥Áµ±Áµ±Ë®àÊ®°ÂûãÂà∞Ë§áÈõúÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁöÑÂü∫Êú¨ËΩâËÆäÈñãÂßãÔºåÈÄôÈ†ÖÂ∑•‰ΩúÂØ©Êü•‰∫ÜÂö¥Ë¨πÁöÑÈ©óË≠âÁ≠ñÁï•ÂíåÊïàËÉΩË©ï‰º∞ÊñπÊ≥ïÔºåÂåÖÊã¨Ê®°ÂûãÊ†°Ê∫ñÂíåÊ±∫Á≠ñÊõ≤Á∑öÂàÜÊûêÁöÑÈóúÈçµËßíËâ≤„ÄÇÊú¨Á´†Âº∑Ë™øÔºåÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠Âª∫Á´ãÂÄºÂæó‰ø°Ë≥¥ÁöÑ AI Á≥ªÁµ±‰∏çÂè™ÊòØÊäÄË°ì‰∏äÁöÑÊ∫ñÁ¢∫ÊÄßÔºõÂÆÉÈúÄË¶Å‰ªîÁ¥∞ËÄÉÈáèÂÖ¨Âπ≥ÊÄß„ÄÅÂèØËß£ÈáãÊÄßÂíåÈö±ÁßÅÊ¨ä„ÄÇÊú¨Á´†Âº∑Ë™ø‰∫ÜÈÄèÈÅé AI Á¢∫‰øùÂÖ¨Âπ≥ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãôÁöÑÊåëÊà∞Ôºå‰∏¶Ë®éË´ñ‰∫ÜË≠òÂà•ÂíåÊ∏õËºïËá®Â∫äÈ†êÊ∏¨Ê®°Âûã‰∏≠ÂÅèÂ∑ÆÁöÑÊñπÊ≥ï„ÄÇÊé•ËëóÔºåÊú¨Á´†Ê∑±ÂÖ•Êé¢Ë®éÂèØËß£ÈáãÊÄßÔºå‰ΩúÁÇ∫‰ª•‰∫∫ÁÇ∫‰∏≠ÂøÉÁöÑ CDSS ÁöÑÂü∫Áü≥„ÄÇÈÄôÁ®ÆÈóúÊ≥®ÂèçÊò†‰∫ÜÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°‰∏çÂÉÖÂøÖÈ†à‰ø°‰ªª AI Âª∫Ë≠∞ÔºåÈÇÑÂøÖÈ†àÁêÜËß£ÂÖ∂ËÉåÂæåÁöÑÊé®ÁêÜ„ÄÇË®éË´ñÈÄ≤‰∏ÄÊ≠•ÂàÜÊûê‰∫ÜÈÜ´ÁôÇ AI Á≥ªÁµ±‰∏≠ÁöÑÈö±ÁßÅÊºèÊ¥ûÔºåÂæûÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰∏≠ÁöÑË≥áÊñôÂ§ñÊ¥©Âà∞ÈáùÂ∞çÊ®°ÂûãËß£ÈáãÁöÑË§áÈõúÊîªÊìä„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÈö±ÁßÅ‰øùË≠∑Á≠ñÁï•Ôºå‰æãÂ¶ÇÂ∑ÆÂàÜÈö±ÁßÅÂíåËÅØÂêàÂ≠∏ÁøíÔºåÂêåÊôÇÊâøË™çÈö±ÁßÅ‰øùË≠∑ÂíåÊ®°ÂûãÊïàËÉΩ‰πãÈñìÁöÑÂõ∫ÊúâÂèñÊç®„ÄÇÈÄôÁ®ÆÂæûÊäÄË°ìÈ©óË≠âÂà∞ÈÅìÂæ∑ËÄÉÈáèÁöÑÈÄ≤Â±ïÔºåÂèçÊò†‰∫ÜÈñãÁôº AI Á≥ªÁµ±ÁöÑÂ§öÈù¢ÂêëÊåëÊà∞ÔºåÈÄô‰∫õÁ≥ªÁµ±ÂèØ‰ª•ÁÑ°Á∏´‰∏îÂèØÈù†Âú∞Êï¥ÂêàÂà∞Êó•Â∏∏Ëá®Â∫äÂØ¶Âãô‰∏≠ÔºåÂêåÊôÇÁ∂≠ÊåÅÊúÄÈ´òÁöÑÁóÖÊÇ£ÁÖßË≠∑ÂíåË≥áÊñô‰øùË≠∑Ê®ôÊ∫ñ„ÄÇ

##### **IFRA: a machine learning-based Instrumented Fall Risk Assessment Scale derived from Instrumented Timed Up and Go test in stroke patients**
2501.09595v1 by Simone Macci√≤, Alessandro Carf√¨, Alessio Capitanelli, Peppino Tropea, Massimo Corbo, Fulvio Mastrogiovanni, Michela Picardi

Effective fall risk assessment is critical for post-stroke patients. The
present study proposes a novel, data-informed fall risk assessment method based
on the instrumented Timed Up and Go (ITUG) test data, bringing in many mobility
measures that traditional clinical scales fail to capture. IFRA, which stands
for Instrumented Fall Risk Assessment, has been developed using a two-step
process: first, features with the highest predictive power among those
collected in a ITUG test have been identified using machine learning
techniques; then, a strategy is proposed to stratify patients into low, medium,
or high-risk strata. The dataset used in our analysis consists of 142
participants, out of which 93 were used for training (15 synthetically
generated), 17 for validation and 32 to test the resulting IFRA scale (22
non-fallers and 10 fallers). Features considered in the IFRA scale include gait
speed, vertical acceleration during sit-to-walk transition, and turning angular
velocity, which align well with established literature on the risk of fall in
neurological patients. In a comparison with traditional clinical scales such as
the traditional Timed Up & Go and the Mini-BESTest, IFRA demonstrates
competitive performance, being the only scale to correctly assign more than
half of the fallers to the high-risk stratum (Fischer's Exact test p = 0.004).
Despite the dataset's limited size, this is the first proof-of-concept study to
pave the way for future evidence regarding the use of IFRA tool for continuous
patient monitoring and fall prevention both in clinical stroke rehabilitation
and at home post-discharge.

ÊëòË¶ÅÔºö<paragraph>Â∞ç‰∏≠È¢®ÂæåÊÇ£ËÄÖËÄåË®ÄÔºåÊúâÊïàÁöÑË∑åÂÄíÈ¢®Èö™Ë©ï‰º∞Ëá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÂâµÊñ∞ÁöÑ„ÄÅÂü∫ÊñºË≥áÊñôÁöÑË∑åÂÄíÈ¢®Èö™Ë©ï‰º∞ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂü∫ÊñºÂÑÄÂô®ÂåñÁöÑË®àÊôÇËµ∑Ë∫´ÂèäË°åËµ∞ (ITUG) Ê∏¨Ë©¶Ë≥áÊñôÔºåÁ¥çÂÖ•‰∫ÜË®±Â§öÂÇ≥Áµ±Ëá®Â∫äÈáèË°®Êú™ËÉΩÊçïÊçâÂà∞ÁöÑÊ¥ªÂãïËÉΩÂäõÊ∏¨ÈáèÊåáÊ®ô„ÄÇIFRAÔºå‰ª£Ë°®ÂÑÄÂô®ÂåñË∑åÂÄíÈ¢®Èö™Ë©ï‰º∞ÔºåÂ∑≤‰ΩøÁî®ÂÖ©Ê≠•È©üÊµÅÁ®ãÈñãÁôºÔºöÈ¶ñÂÖàÔºåÂ∑≤‰ΩøÁî®Ê©üÂô®Â≠∏ÁøíÊäÄË°ìË≠òÂà•Âá∫Âú® ITUG Ê∏¨Ë©¶‰∏≠Êî∂ÈõÜÁöÑÈÇ£‰∫õÂÖ∑ÊúâÊúÄÈ´òÈ†êÊ∏¨ËÉΩÂäõÁöÑÁâπÂæµÔºõÁÑ∂ÂæåÔºåÊèêÂá∫‰∫Ü‰∏ÄÈ†ÖÁ≠ñÁï•Â∞áÊÇ£ËÄÖÂàÜÂ±§ÁÇ∫‰ΩéÈ¢®Èö™„ÄÅ‰∏≠È¢®Èö™ÊàñÈ´òÈ¢®Èö™Á≠âÁ¥ö„ÄÇÊàëÂÄëÁöÑÂàÜÊûê‰∏≠‰ΩøÁî®ÁöÑË≥áÊñôÈõÜÂåÖÂê´ 142 ÂêçÂèÉËàáËÄÖÔºåÂÖ∂‰∏≠ 93 ÂêçÁî®ÊñºË®ìÁ∑¥Ôºà15 ÂêçÂêàÊàêÁî¢ÁîüÔºâÔºå17 ÂêçÁî®ÊñºÈ©óË≠âÔºå32 ÂêçÁî®ÊñºÊ∏¨Ë©¶Áî¢ÁîüÁöÑ IFRA ÈáèË°®Ôºà22 ÂêçÈùûË∑åÂÄíËÄÖÂíå 10 ÂêçË∑åÂÄíËÄÖÔºâ„ÄÇIFRA ÈáèË°®‰∏≠ËÄÉÊÖÆÁöÑÁâπÂæµÂåÖÊã¨Ê≠•ÊÖãÈÄüÂ∫¶„ÄÅÂùêÂà∞Ëµ∞ÈÅéÊ∏°ÊúüÈñìÁöÑÂûÇÁõ¥Âä†ÈÄüÂ∫¶ÂíåËΩâÂΩéËßíÈÄüÂ∫¶ÔºåÈÄô‰∫õÁâπÂæµËàáÂ∑≤Âª∫Á´ãÁöÑÁ•ûÁ∂ìÁóÖÊÇ£Ë∑åÂÄíÈ¢®Èö™ÊñáÁçªÈùûÂ∏∏ÂêªÂêà„ÄÇËàáÂÇ≥Áµ±Ëá®Â∫äÈáèË°®Ôºà‰æãÂ¶ÇÂÇ≥Áµ±ÁöÑË®àÊôÇËµ∑Ë∫´ÂèäË°åËµ∞ÂíåËø∑‰Ω† BESTestÔºâÁõ∏ÊØîÔºåIFRA Ë°®ÁèæÂá∫Á´∂Áà≠ÂÑ™Âã¢ÔºåÊòØÂîØ‰∏ÄÂ∞áË∂ÖÈÅé‰∏ÄÂçäÁöÑË∑åÂÄíËÄÖÊ≠£Á¢∫ÂàÜÈÖçÂà∞È´òÈ¢®Èö™ÈöéÂ±§ÁöÑÈáèË°®ÔºàFisher Á≤æÁ¢∫Ê™¢ÂÆö p = 0.004Ôºâ„ÄÇÂÑòÁÆ°Ë≥áÊñôÈõÜË¶èÊ®°ÊúâÈôêÔºå‰ΩÜÈÄôÊòØÁ¨¨‰∏ÄÂÄãÊ¶ÇÂøµÈ©óË≠âÁ†îÁ©∂ÔºåÁÇ∫Êú™‰æÜÈóúÊñºÂú®Ëá®Â∫ä‰∏≠È¢®Â∫∑Âæ©ÂíåÂá∫Èô¢ÂæåÂ±ÖÂÆ∂Ë∑åÂÄíÈ†êÈò≤‰∏≠‰ΩøÁî® IFRA Â∑•ÂÖ∑ÁöÑË≠âÊìöÈã™Ë∑Ø„ÄÇ</paragraph>

##### **Understanding Mental Health Content on Social Media and Its Effect Towards Suicidal Ideation**
2501.09309v1 by Mohaiminul Islam Bhuiyan, Nur Shazwani Kamarudin, Nur Hafieza Ismail

This review underscores the critical need for effective strategies to
identify and support individuals with suicidal ideation, exploiting
technological innovations in ML and DL to further suicide prevention efforts.
The study details the application of these technologies in analyzing vast
amounts of unstructured social media data to detect linguistic patterns,
keywords, phrases, tones, and contextual cues associated with suicidal
thoughts. It explores various ML and DL models like SVMs, CNNs, LSTM, neural
networks, and their effectiveness in interpreting complex data patterns and
emotional nuances within text data. The review discusses the potential of these
technologies to serve as a life-saving tool by identifying at-risk individuals
through their digital traces. Furthermore, it evaluates the real-world
effectiveness, limitations, and ethical considerations of employing these
technologies for suicide prevention, stressing the importance of responsible
development and usage. The study aims to fill critical knowledge gaps by
analyzing recent studies, methodologies, tools, and techniques in this field.
It highlights the importance of synthesizing current literature to inform
practical tools and suicide prevention efforts, guiding innovation in reliable,
ethical systems for early intervention. This research synthesis evaluates the
intersection of technology and mental health, advocating for the ethical and
responsible application of ML, DL, and NLP to offer life-saving potential
worldwide while addressing challenges like generalizability, biases, privacy,
and the need for further research to ensure these technologies do not
exacerbate existing inequities and harms.

ÊëòË¶ÅÔºöÈÄôÁØáË©ïË´ñÂº∑Ë™ø‰∫ÜÊúâÊïàÁ≠ñÁï•ÁöÑÈáçË¶ÅÈúÄÊ±ÇÔºå‰ª•ÈÄèÈÅéÂà©Áî®Ê©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊäÄË°ìÂâµÊñ∞‰æÜË≠òÂà•ÂíåÊîØÊåÅÊúâËá™ÊÆ∫ÊÑèÂøµÁöÑ‰∫∫ÔºåÈÄ≤‰∏ÄÊ≠•‰øÉÈÄ≤Ëá™ÊÆ∫Èò≤Ê≤ªÂ∑•‰Ωú„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Ë©≥Á¥∞Ë™™Êòé‰∫ÜÈÄô‰∫õÊäÄË°ìÂú®ÂàÜÊûêÂ§ßÈáèÈùûÁµêÊßãÂåñÁ§æÁæ§Â™íÈ´îË≥áÊñô‰∏≠ÁöÑÊáâÁî®Ôºå‰ª•ÂÅµÊ∏¨ËàáËá™ÊÆ∫ÂøµÈ†≠Áõ∏ÈóúÁöÑË™ûË®ÄÊ®°Âºè„ÄÅÈóúÈçµÂ≠ó„ÄÅË©ûÁµÑ„ÄÅË™ûÊ∞£ÂíåËÑàÁµ°Á∑öÁ¥¢„ÄÇÂÆÉÊé¢Ë®é‰∫ÜÂêÑÁ®ÆÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰æãÂ¶ÇÊîØÊè¥ÂêëÈáèÊ©ü„ÄÅÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅÈï∑Áü≠ÊúüË®òÊÜ∂Á∂≤Ë∑Ø„ÄÅÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºå‰ª•ÂèäÂÆÉÂÄëÂú®Ëß£ËÆÄÊñáÂ≠óË≥áÊñô‰∏≠ÁöÑË§áÈõúË≥áÊñôÊ®°ÂºèÂíåÊÉÖÁ∑íÁ¥∞ÂæÆÂ∑ÆÂà•ÊñπÈù¢ÁöÑÊïàËÉΩ„ÄÇÈÄôÁØáË©ïË´ñË®éË´ñ‰∫ÜÈÄô‰∫õÊäÄË°ì‰ΩúÁÇ∫ÊïëÂëΩÂ∑•ÂÖ∑ÁöÑÊΩõÂäõÔºåÈÄèÈÅéÊï∏‰ΩçË∂≥Ë∑°‰æÜË≠òÂà•ÊúâÈ¢®Èö™ÁöÑÂÄã‰∫∫„ÄÇÊ≠§Â§ñÔºåÂÆÉË©ï‰º∞‰∫ÜÊé°Áî®ÈÄô‰∫õÊäÄË°ìÈÄ≤Ë°åËá™ÊÆ∫Èò≤Ê≤ªÁöÑÂØ¶ÈöõÊïàËÉΩ„ÄÅÈôêÂà∂ÂíåÈÅìÂæ∑ËÄÉÈáèÔºåÂº∑Ë™øË≤†Ë≤¨‰ªªÁöÑÈñãÁôºÂíå‰ΩøÁî®ÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Êó®Âú®ÈÄèÈÅéÂàÜÊûêÈÄôÂÄãÈ†òÂüüÁöÑËøëÊúüÁ†îÁ©∂„ÄÅÊñπÊ≥ï„ÄÅÂ∑•ÂÖ∑ÂíåÊäÄË°ìÔºåÂ°´Ë£úÈáçË¶ÅÁöÑÁü•Ë≠òÂ∑ÆË∑ù„ÄÇÂÆÉÂº∑Ë™ø‰∫ÜÁ∂úÂêàÁèæÊúâÊñáÁçªÂ∞çÊñºÊèê‰æõÂØ¶Áî®Â∑•ÂÖ∑ÂíåËá™ÊÆ∫Èò≤Ê≤ªÂ∑•‰ΩúÁöÑÈáçË¶ÅÊÄßÔºåÂºïÂ∞éÂú®Êó©Êúü‰ªãÂÖ•‰∏≠Âª∫Á´ãÂèØÈù†ÁöÑ„ÄÅÁ¨¶ÂêàÈÅìÂæ∑ÁöÑÁ≥ªÁµ±ÁöÑÂâµÊñ∞„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Á∂úÂêàË©ï‰º∞‰∫ÜÊäÄË°ìÂíåÂøÉÁêÜÂÅ•Â∫∑‰πãÈñìÁöÑ‰∫§ÈõÜÔºåÂÄ°Â∞éÈÅìÂæ∑‰∏îË≤†Ë≤¨‰ªªÂú∞ÊáâÁî®Ê©üÂô®Â≠∏Áøí„ÄÅÊ∑±Â∫¶Â≠∏ÁøíÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÔºå‰ª•Êèê‰æõÂÖ®ÁêÉÊÄßÁöÑÊïëÂëΩÊΩõÂäõÔºåÂêåÊôÇËß£Ê±∫Ê¶ÇÊã¨ÊÄß„ÄÅÂÅèË™§„ÄÅÈö±ÁßÅÁ≠âÊåëÊà∞Ôºå‰∏¶ÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂‰ª•Á¢∫‰øùÈÄô‰∫õÊäÄË°ì‰∏çÊúÉÂä†ÂäáÁèæÊúâÁöÑ‰∏çÂπ≥Á≠âÂíåÂÇ∑ÂÆ≥„ÄÇ

##### **Interpretable Droplet Digital PCR Assay for Trustworthy Molecular Diagnostics**
2501.09218v1 by Yuanyuan Wei, Yucheng Wu, Fuyang Qu, Yao Mu, Yi-Ping Ho, Ho-Pui Ho, Wu Yuan, Mingkun Xu

Accurate molecular quantification is essential for advancing research and
diagnostics in fields such as infectious diseases, cancer biology, and genetic
disorders. Droplet digital PCR (ddPCR) has emerged as a gold standard for
achieving absolute quantification. While computational ddPCR technologies have
advanced significantly, achieving automatic interpretation and consistent
adaptability across diverse operational environments remains a challenge. To
address these limitations, we introduce the intelligent interpretable droplet
digital PCR (I2ddPCR) assay, a comprehensive framework integrating front-end
predictive models (for droplet segmentation and classification) with GPT-4o
multimodal large language model (MLLM, for context-aware explanations and
recommendations) to automate and enhance ddPCR image analysis. This approach
surpasses the state-of-the-art models, affording 99.05% accuracy in processing
complex ddPCR images containing over 300 droplets per image with varying
signal-to-noise ratios (SNRs). By combining specialized neural networks and
large language models, the I2ddPCR assay offers a robust and adaptable solution
for absolute molecular quantification, achieving a sensitivity capable of
detecting low-abundance targets as low as 90.32 copies/{\mu}L. Furthermore, it
improves model's transparency through detailed explanation and troubleshooting
guidance, empowering users to make informed decisions. This innovative
framework has the potential to benefit molecular diagnostics, disease research,
and clinical applications, especially in resource-constrained settings.

ÊëòË¶ÅÔºöÊ∫ñÁ¢∫ÁöÑÂàÜÂ≠êÈáèÂåñÂ∞çÊñºÊé®ÈÄ≤ÂÇ≥ÊüìÁóÖ„ÄÅÁôåÁóáÁîüÁâ©Â≠∏ÂíåÈÅ∫ÂÇ≥ÁñæÁóÖÁ≠âÈ†òÂüüÁöÑÁ†îÁ©∂ÂíåË®∫Êñ∑Ëá≥ÈóúÈáçË¶Å„ÄÇÈ£õÊ≤´Êï∏‰Ωç PCR (ddPCR) Â∑≤ÊàêÁÇ∫ÂØ¶ÁèæÁµïÂ∞çÈáèÂåñÁöÑÈªÉÈáëÊ®ôÊ∫ñ„ÄÇÂÑòÁÆ°ÈÅãÁÆóÂºè ddPCR ÊäÄË°ìÂ∑≤Â§ßÂπÖÈÄ≤Ê≠•Ôºå‰ΩÜÂú®‰∏çÂêåÊìç‰ΩúÁí∞Â¢É‰∏≠ÂØ¶ÁèæËá™ÂãïÂåñËß£ËÆÄÂíå‰∏ÄËá¥ÁöÑÈÅ©ÊáâÊÄß‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊô∫ÊÖßÂèØËß£ËÆÄÈ£õÊ≤´Êï∏‰Ωç PCR (I2ddPCR) ÂàÜÊûêÔºå‰∏ÄÂÄãÊï¥ÂêàÂâçÁûªÊÄßÈ†êÊ∏¨Ê®°ÂûãÔºàÁî®ÊñºÈ£õÊ≤´ÂàÜÂâ≤ÂíåÂàÜÈ°ûÔºâËàá GPT-4o Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàMLLMÔºåÁî®ÊñºÊÉÖÂ¢ÉÊÑüÁü•Ëß£ÈáãÂíåÂª∫Ë≠∞ÔºâÁöÑÁ∂úÂêàÊû∂ÊßãÔºå‰ª•Ëá™ÂãïÂåñ‰∏¶Â¢ûÂº∑ ddPCR ÂΩ±ÂÉèÂàÜÊûê„ÄÇÊ≠§ÊñπÊ≥ïË∂ÖË∂ä‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÔºåÂú®ËôïÁêÜÊØèÂºµÂΩ±ÂÉèÂê´ÊúâË∂ÖÈÅé 300 ÂÄãÈ£õÊ≤´‰∏î‰ø°Âô™ÊØî (SNR) ‰∏çÂêåÁöÑË§áÈõú ddPCR ÂΩ±ÂÉèÊôÇÔºåÊ∫ñÁ¢∫Â∫¶È´òÈÅî 99.05%„ÄÇÈÄèÈÅéÁµêÂêàÂ∞àÈñÄÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåI2ddPCR ÂàÜÊûêÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂº∑ÂÅ•‰∏îÈÅ©ÊáâÊÄßÈ´òÁöÑÁµïÂ∞çÂàÜÂ≠êÈáèÂåñËß£Ê±∫ÊñπÊ°àÔºåÈùàÊïèÂ∫¶È´òÔºåËÉΩÂÅµÊ∏¨‰ΩéËá≥ 90.32 ÂÄãÊã∑Ë≤ùÊï∏/{\mu}L ÁöÑ‰ΩéË±êÂ∫¶ÁõÆÊ®ô„ÄÇÊ≠§Â§ñÔºåÂÆÉÈÄèÈÅéË©≥Á¥∞ÁöÑË™™ÊòéÂíåÊïÖÈöúÊéíÈô§ÊåáÂçó‰æÜÊèêÂçáÊ®°ÂûãÁöÑÈÄèÊòéÂ∫¶Ôºå‰ΩøÁî®Êà∂ËÉΩÂ§†ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇÈÄôÂÄãÂâµÊñ∞ÁöÑÊû∂ÊßãÊúâÊΩõÂäõÈÄ†Á¶èÂàÜÂ≠êË®∫Êñ∑„ÄÅÁñæÁóÖÁ†îÁ©∂ÂíåËá®Â∫äÊáâÁî®ÔºåÁâπÂà•ÊòØÂú®Ë≥áÊ∫êÂèóÈôêÁöÑÁí∞Â¢É‰∏≠„ÄÇ

##### **AutoLoop: Fast Visual SLAM Fine-tuning through Agentic Curriculum Learning**
2501.09160v1 by Assaf Lahiany, Oren Gal

Current visual SLAM systems face significant challenges in balancing
computational efficiency with robust loop closure handling. Traditional
approaches require careful manual tuning and incur substantial computational
overhead, while learning-based methods either lack explicit loop closure
capabilities or implement them through computationally expensive methods. We
present AutoLoop, a novel approach that combines automated curriculum learning
with efficient fine-tuning for visual SLAM systems. Our method employs a DDPG
(Deep Deterministic Policy Gradient) agent to dynamically adjust loop closure
weights during training, eliminating the need for manual hyperparameter search
while significantly reducing the required training steps. The approach
pre-computes potential loop closure pairs offline and leverages them through an
agent-guided curriculum, allowing the model to adapt efficiently to new
scenarios. Experiments conducted on TartanAir for training and validated across
multiple benchmarks including KITTI, EuRoC, ICL-NUIM and TUM RGB-D demonstrate
that AutoLoop achieves comparable or superior performance while reducing
training time by an order of magnitude compared to traditional approaches.
AutoLoop provides a practical solution for rapid adaptation of visual SLAM
systems, automating the weight tuning process that traditionally requires
multiple manual iterations. Our results show that this automated curriculum
strategy not only accelerates training but also maintains or improves the
model's performance across diverse environmental conditions.

ÊëòË¶ÅÔºöÁï∂ÂâçÁöÑË¶ñË¶∫ SLAM Á≥ªÁµ±Âú®Âπ≥Ë°°ÈÅãÁÆóÊïàÁéáËàáÁ©©ÂÅ•ÁöÑËø¥Ë∑ØÈñâÂêàËôïÁêÜ‰∏äÔºåÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÈúÄË¶Å‰ªîÁ¥∞ÁöÑÊâãÂãïË™øÊï¥Ôºå‰∏¶ÊúÉÁî¢ÁîüÂ§ßÈáèÁöÑÈÅãÁÆóË≤†ÊìîÔºåËÄåÂü∫ÊñºÂ≠∏ÁøíÁöÑÊñπÊ≥ïÂâáÁº∫‰πèÊòéÁ¢∫ÁöÑËø¥Ë∑ØÈñâÂêàÂäüËÉΩÔºåÊàñÈÄèÈÅéÈÅãÁÆóÊàêÊú¨È´òÊòÇÁöÑÊñπÊ≥ï‰æÜÂØ¶‰Ωú„ÄÇÊàëÂÄëÊèêÂá∫ AutoLoopÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÂÆÉÁµêÂêà‰∫ÜËá™ÂãïÂåñÁöÑË™≤Á®ãÂ≠∏ÁøíËàáË¶ñË¶∫ SLAM Á≥ªÁµ±ÁöÑÊúâÊïàÂæÆË™ø„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÊé°Áî® DDPGÔºàÊ∑±Â∫¶Á¢∫ÂÆöÊÄßÁ≠ñÁï•Ê¢ØÂ∫¶Ôºâ‰ª£ÁêÜÔºåÂú®Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠ÂãïÊÖãË™øÊï¥Ëø¥Ë∑ØÈñâÂêàÊ¨äÈáçÔºåÊ∂àÈô§‰∫Ü‰∫∫Â∑•Ë∂ÖÂèÉÊï∏ÊêúÂ∞ãÁöÑÈúÄË¶ÅÔºåÂêåÊôÇÂ§ßÂπÖÊ∏õÂ∞ë‰∫ÜÊâÄÈúÄÁöÑË®ìÁ∑¥Ê≠•È©ü„ÄÇÊ≠§ÊñπÊ≥ïÊúÉÈõ¢Á∑öÈ†êÂÖàË®àÁÆóÊΩõÂú®ÁöÑËø¥Ë∑ØÈñâÂêàÂ∞çÔºå‰∏¶ÈÄèÈÅé‰ª£ÁêÜÂ∞éÂêëÁöÑË™≤Á®ã‰æÜÂà©Áî®ÂÆÉÂÄëÔºåËÆìÊ®°ÂûãËÉΩÂ§†ÊúâÊïàÂú∞ÈÅ©ÊáâÊñ∞ÁöÑÂ†¥ÊôØ„ÄÇÂú® TartanAir ‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óÔºåÁî®ÊñºË®ìÁ∑¥‰∏¶È©óË≠âË∑®Â§öÂÄãÂü∫Ê∫ñÔºåÂåÖÊã¨ KITTI„ÄÅEuRoC„ÄÅICL-NUIM Âíå TUM RGB-DÔºåË≠âÊòé AutoLoop ÈÅîÂà∞Áõ∏Áï∂ÊàñÊõ¥‰Ω≥ÁöÑÊïàËÉΩÔºåÂêåÊôÇÂ∞áË®ìÁ∑¥ÊôÇÈñìÊ∏õÂ∞ë‰∫Ü‰∏ÄÂÄãÊï∏ÈáèÁ¥öÔºåËàáÂÇ≥Áµ±ÊñπÊ≥ïÁõ∏ÊØî„ÄÇAutoLoop Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂØ¶Áî®ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºÂø´ÈÄüÈÅ©ÊáâË¶ñË¶∫ SLAM Á≥ªÁµ±ÔºåËá™ÂãïÂåñÂÇ≥Áµ±‰∏äÈúÄË¶ÅÂ§öÊ¨°‰∫∫Â∑•ÂèçË¶ÜÈÅãÁÆóÁöÑÊ¨äÈáçË™øÊï¥ÈÅéÁ®ã„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÈÄôÁ®ÆËá™ÂãïÂåñÁöÑË™≤Á®ãÁ≠ñÁï•‰∏çÂÉÖÂä†ÈÄü‰∫ÜË®ìÁ∑¥ÔºåÈÇÑÁ∂≠ÊåÅÊàñÊîπÂñÑ‰∫ÜÊ®°ÂûãÂú®ÂêÑÁ®ÆÁí∞Â¢ÉÊ¢ù‰ª∂‰∏ãÁöÑÊïàËÉΩ„ÄÇ

##### **Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval**
2501.09134v1 by Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, Gongbo Liang

Medical images and reports offer invaluable insights into patient health. The
heterogeneity and complexity of these data hinder effective analysis. To bridge
this gap, we investigate contrastive learning models for cross-domain
retrieval, which associates medical images with their corresponding clinical
reports. This study benchmarks the robustness of four state-of-the-art
contrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We
introduce an occlusion retrieval task to evaluate model performance under
varying levels of image corruption. Our findings reveal that all evaluated
models are highly sensitive to out-of-distribution data, as evidenced by the
proportional decrease in performance with increasing occlusion levels. While
MedCLIP exhibits slightly more robustness, its overall performance remains
significantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a
general-purpose dataset, struggles with medical image-report retrieval,
highlighting the importance of domain-specific training data. The evaluation of
this work suggests that more effort needs to be spent on improving the
robustness of these models. By addressing these limitations, we can develop
more reliable cross-domain retrieval models for medical applications.

ÊëòË¶ÅÔºöÈÜ´ÁôÇÂΩ±ÂÉèÂíåÂ†±ÂëäÊèê‰æõÂØ∂Ë≤¥ÁöÑË¶ãËß£ÔºåÊ∑±ÂÖ•‰∫ÜËß£ÊÇ£ËÄÖÂÅ•Â∫∑„ÄÇÈÄô‰∫õÊï∏ÊìöÁöÑÁï∞Ë≥™ÊÄßÂíåË§áÈõúÊÄßÈòªÁ§ô‰∫ÜÊúâÊïàÁöÑÂàÜÊûê„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÁ†îÁ©∂Â∞çÊØîÂ≠∏ÁøíÊ®°ÂûãÈÄ≤Ë°åË∑®È†òÂüüÊ™¢Á¥¢ÔºåÂ∞áÈÜ´Â≠∏ÂΩ±ÂÉèËàáÂÖ∂Â∞çÊáâÁöÑËá®Â∫äÂ†±ÂëäËÅØÁπ´Ëµ∑‰æÜ„ÄÇÊú¨Á†îÁ©∂Â∞çÂõõÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑÂ∞çÊØîÂ≠∏ÁøíÊ®°ÂûãÁöÑÂÅ•Â£ØÊÄßÈÄ≤Ë°å‰∫ÜÂü∫Ê∫ñÊ∏¨Ë©¶ÔºöCLIP„ÄÅCXR-RePaiR„ÄÅMedCLIP Âíå CXR-CLIP„ÄÇÊàëÂÄëÂºïÂÖ•ÈÅÆÊìãÊ™¢Á¥¢‰ªªÂãôÔºå‰ª•Ë©ï‰º∞Ê®°ÂûãÂú®‰∏çÂêåÁ®ãÂ∫¶ÁöÑÂΩ±ÂÉèÊêçÂ£û‰∏ãÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÊâÄÊúâË©ï‰º∞ÁöÑÊ®°ÂûãÂ∞çÂàÜ‰ΩàÂ§ñÊï∏ÊìöÈÉΩÈ´òÂ∫¶ÊïèÊÑüÔºåÈÄôÂæûÈö®ËëóÈÅÆÊìãÁ®ãÂ∫¶ÁöÑÂ¢ûÂä†ËÄåÂ∞éËá¥ÁöÑÊÄßËÉΩÊàêÊØî‰æã‰∏ãÈôçÂ∞±ÂèØ‰ª•Ë≠âÊòé„ÄÇÈõñÁÑ∂ MedCLIP Ë°®ÁèæÂá∫Á®çÈ´òÁöÑÂÅ•Â£ØÊÄßÔºå‰ΩÜÂÖ∂Êï¥È´îÊÄßËÉΩ‰ªçÈÅ†ÈÅ†ËêΩÂæåÊñº CXR-CLIP Âíå CXR-RePaiR„ÄÇCLIP Âú®ÈÄöÁî®Êï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂ†±ÂëäÊ™¢Á¥¢‰∏≠ÈÅáÂà∞Âõ∞Èõ£ÔºåÁ™ÅÈ°Ø‰∫ÜÁâπÂÆöÈ†òÂüüË®ìÁ∑¥Êï∏ÊìöÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁöÑË©ï‰º∞Ë°®ÊòéÔºåÈúÄË¶ÅËä±Ë≤ªÊõ¥Â§öÁ≤æÂäõ‰æÜÊèêÈ´òÈÄô‰∫õÊ®°ÂûãÁöÑÂÅ•Â£ØÊÄß„ÄÇÈÄöÈÅéËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂèØ‰ª•ÁÇ∫ÈÜ´ÁôÇÊáâÁî®ÈñãÁôºÊõ¥ÂèØÈù†ÁöÑË∑®È†òÂüüÊ™¢Á¥¢Ê®°Âûã„ÄÇ

##### **Generative Medical Image Anonymization Based on Latent Code Projection and Optimization**
2501.09114v1 by Huiyu Li, Nicholas Ayache, Herv√© Delingette

Medical image anonymization aims to protect patient privacy by removing
identifying information, while preserving the data utility to solve downstream
tasks. In this paper, we address the medical image anonymization problem with a
two-stage solution: latent code projection and optimization. In the projection
stage, we design a streamlined encoder to project input images into a latent
space and propose a co-training scheme to enhance the projection process. In
the optimization stage, we refine the latent code using two deep loss functions
designed to address the trade-off between identity protection and data utility
dedicated to medical images. Through a comprehensive set of qualitative and
quantitative experiments, we showcase the effectiveness of our approach on the
MIMIC-CXR chest X-ray dataset by generating anonymized synthetic images that
can serve as training set for detecting lung pathologies. Source codes are
available at https://github.com/Huiyu-Li/GMIA.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÂΩ±ÂÉèÂåøÂêçÂåñÊó®Âú®ÈÄèÈÅéÁßªÈô§Ë≠òÂà•Ë≥áË®ä‰æÜ‰øùË≠∑ÁóÖÊÇ£Èö±ÁßÅÔºåÂêåÊôÇ‰øùÁïôË≥áÊñôÊïàÁî®‰ª•Ëß£Ê±∫‰∏ãÊ∏∏‰ªªÂãô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂÖ©ÈöéÊÆµËß£Ê±∫ÊñπÊ°à‰æÜËß£Ê±∫ÈÜ´Â≠∏ÂΩ±ÂÉèÂåøÂêçÂåñÂïèÈ°åÔºöÊΩõÂú®Á¢ºÊäïÂΩ±ÂíåÊúÄ‰Ω≥Âåñ„ÄÇÂú®ÊäïÂΩ±ÈöéÊÆµÔºåÊàëÂÄëË®≠Ë®à‰∏ÄÂÄãÁ∞°ÂåñÁöÑÁ∑®Á¢ºÂô®ÔºåÂ∞áËº∏ÂÖ•ÂΩ±ÂÉèÊäïÂΩ±Âà∞ÊΩõÂú®Á©∫ÈñìÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂÖ±ÂêåË®ìÁ∑¥Êû∂Êßã‰æÜÊèêÂçáÊäïÂΩ±Á®ãÂ∫è„ÄÇÂú®ÊúÄ‰Ω≥ÂåñÈöéÊÆµÔºåÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãÊ∑±Â∫¶ÊêçÂ§±ÂáΩÊï∏‰æÜË™øÊï¥ÊΩõÂú®Á¢ºÔºåÈÄô‰∫õÂáΩÊï∏Êó®Âú®Ëß£Ê±∫Ë∫´ÂàÜ‰øùË≠∑ËàáÂ∞àÈñÄÁî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑË≥áÊñôÊïàÁî®‰πãÈñìÁöÑÊ¨äË°°„ÄÇÈÄèÈÅé‰∏ÄÁµÑÂÖ®Èù¢ÁöÑÂÆöÊÄßÂíåÂÆöÈáèÂØ¶È©óÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊñπÊ≥ïÂú® MIMIC-CXR ËÉ∏ÈÉ® X ÂÖâÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏äÁöÑÊúâÊïàÊÄßÔºåÊñπÊ≥ïÊòØÁî¢ÁîüÂèØ‰ΩúÁÇ∫Ë®ìÁ∑¥ÈõÜ‰æÜÂÅµÊ∏¨ËÇ∫ÈÉ®ÁóÖÁêÜÁöÑÂåøÂêçÂêàÊàêÂΩ±ÂÉè„ÄÇÂéüÂßãÁ¢ºÂèØÊñº https://github.com/Huiyu-Li/GMIA ÂèñÂæó„ÄÇ

##### **Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models**
2501.08977v2 by Emma Croxford, Yanjun Gao, Nicholas Pellegrino, Karen K. Wong, Graham Wills, Elliot First, Miranda Schnier, Kyle Burton, Cris G. Ebby, Jillian Gorskic, Matthew Kalscheur, Samy Khalil, Marie Pisani, Tyler Rubeor, Peter Stetson, Frank Liao, Cherodeep Goswami, Brian Patterson, Majid Afshar

As Large Language Models (LLMs) are integrated into electronic health record
(EHR) workflows, validated instruments are essential to evaluate their
performance before implementation. Existing instruments for provider
documentation quality are often unsuitable for the complexities of
LLM-generated text and lack validation on real-world data. The Provider
Documentation Summarization Quality Instrument (PDSQI-9) was developed to
evaluate LLM-generated clinical summaries. Multi-document summaries were
generated from real-world EHR data across multiple specialties using several
LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson
correlation for substantive validity, factor analysis and Cronbach's alpha for
structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for
generalizability, a semi-Delphi process for content validity, and comparisons
of high-versus low-quality summaries for discriminant validity. Seven physician
raters evaluated 779 summaries and answered 8,329 questions, achieving over 80%
power for inter-rater reliability. The PDSQI-9 demonstrated strong internal
consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high
inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting
structural validity and generalizability. Factor analysis identified a 4-factor
model explaining 58% of the variance, representing organization, clarity,
accuracy, and utility. Substantive validity was supported by correlations
between note length and scores for Succinct (rho = -0.200, p = 0.029) and
Organized ($\rho = -0.190$, $p = 0.037$). Discriminant validity distinguished
high- from low-quality summaries ($p < 0.001$). The PDSQI-9 demonstrates robust
construct validity, supporting its use in clinical practice to evaluate
LLM-generated summaries and facilitate safer integration of LLMs into
healthcare workflows.

ÊëòË¶ÅÔºö<paragraph>Èö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êï¥ÂêàÂà∞ÈõªÂ≠êÁóÖÊ≠∑
(EHR) Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÔºåÂú®ÂØ¶ÊñΩ‰πãÂâçÔºåÁ∂ìÈÅéÈ©óË≠âÁöÑÂÑÄÂô®Â∞çÊñºË©ï‰º∞ÂÖ∂
ÊïàËÉΩËá≥ÈóúÈáçË¶Å„ÄÇÁèæÊúâÁöÑÊèê‰æõËÄÖÊñá‰ª∂ÂìÅË≥™ÂÑÄÂô®ÈÄöÂ∏∏‰∏çÈÅ©Âêà
LLM ÁîüÊàêÁöÑÊñáÂ≠óÁöÑË§áÈõúÊÄßÔºå‰∏îÁº∫‰πèÂ∞çÁúüÂØ¶‰∏ñÁïåË≥áÊñôÁöÑÈ©óË≠â„ÄÇÊèê‰æõËÄÖ
Êñá‰ª∂ÊëòË¶ÅÂìÅË≥™ÂÑÄÂô® (PDSQI-9) ÊòØÁÇ∫‰∫ÜË©ï‰º∞ LLM ÁîüÊàêÁöÑËá®Â∫äÊëòË¶ÅËÄå
ÈñãÁôºÁöÑ„ÄÇ‰ΩøÁî®Â§öÂÄã LLMÔºàGPT-4o„ÄÅMixtral 8x7b Âíå Llama 3-8bÔºâÔºå
ÂæûË∑®Â§öÂÄãÂ∞àÁßëÁöÑÁúüÂØ¶‰∏ñÁïå EHR Ë≥áÊñô‰∏≠Áî¢Áîü‰∫ÜÂ§öÊñá‰ª∂ÊëòË¶Å„ÄÇÈ©óË≠âÂåÖÊã¨
ÁöÆÁàæÊ£ÆÁõ∏ÈóúÊÄßÔºàÂØ¶Ë≥™ÊïàÂ∫¶Ôºâ„ÄÅÂõ†Â≠êÂàÜÊûêÂíåÂÖãÊúóÂ∑¥Ëµ´ Œ±ÔºàÁµêÊßãÊïàÂ∫¶Ôºâ„ÄÅ
Ë©ïÂàÜËÄÖÈñì‰ø°Â∫¶ÔºàICC Âíå Krippendorff Œ±ÔºâÔºàÊ¶ÇÂåñÊÄßÔºâ„ÄÅÂÖßÂÆπÊïàÂ∫¶ÁöÑÂçäÂæ∑Áàæ
Ëè≤Á®ãÂ∫èÔºå‰ª•ÂèäÊØîËºÉÈ´òÂìÅË≥™Âíå‰ΩéÂìÅË≥™ÊëòË¶ÅÔºàÂà§Âà•ÊïàÂ∫¶Ôºâ„ÄÇ‰∏É‰ΩçÈÜ´Â∏´
Ë©ïÂàÜËÄÖË©ï‰º∞‰∫Ü 779 ‰ªΩÊëòË¶Å‰∏¶ÂõûÁ≠î‰∫Ü 8,329 ÂÄãÂïèÈ°åÔºåË©ïÂàÜËÄÖÈñì‰ø°Â∫¶ÈÅî
Âà∞‰∫Ü 80% ‰ª•‰∏ä„ÄÇPDSQI-9 Ë°®ÁèæÂá∫Âº∑Â§ßÁöÑÂÖßÈÉ®‰∏ÄËá¥ÊÄßÔºàÂÖãÊúóÂ∑¥Ëµ´ Œ± =
0.879Ôºõ95% CIÔºö0.867-0.891ÔºâÂíåÈ´òË©ïÂàÜËÄÖÈñì‰ø°Â∫¶ÔºàICC = 0.867Ôºõ95%
CIÔºö0.867-0.868ÔºâÔºåÊîØÊåÅÁµêÊßãÊïàÂ∫¶ÂíåÊ¶ÇÂåñÊÄß„ÄÇÂõ†Â≠êÂàÜÊûêË≠òÂà•Âá∫‰∏ÄÂÄã
4 Âõ†Â≠êÊ®°ÂûãÔºåËß£Èáã‰∫Ü 58% ÁöÑËÆäÁï∞Ôºå‰ª£Ë°®ÁµÑÁπî„ÄÅÊ∏ÖÊô∞Â∫¶„ÄÅÊ∫ñÁ¢∫ÊÄßÂíåÂØ¶Áî®
ÊÄß„ÄÇÂØ¶Ë≥™ÊïàÂ∫¶ÂèóÂà∞ÂÇôÂøòÈåÑÈï∑Â∫¶ËàáÁ∞°ÊΩîÔºàrho = -0.200Ôºåp = 0.029ÔºâÂíå
Ê¢ùÁêÜÔºà$\rho = -0.190$Ôºå$p = 0.037$ÔºâÁöÑÂàÜÊï∏‰πãÈñìÁõ∏ÈóúÊÄßÁöÑÊîØÊåÅ„ÄÇÂà§Âà•
ÊïàÂ∫¶ÂçÄÂàÜ‰∫ÜÈ´òÂìÅË≥™Âíå‰ΩéÂìÅË≥™ÊëòË¶ÅÔºà$p < 0.001$Ôºâ„ÄÇPDSQI-9 Â±ïÁ§∫‰∫ÜÂº∑ÂÅ•
ÁöÑÂª∫ÊßãÊïàÂ∫¶ÔºåÊîØÊåÅÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠‰ΩøÁî®ÂÆÉ‰æÜË©ï‰º∞ LLM ÁîüÊàêÁöÑÊëòË¶ÅÔºå‰∏¶
‰øÉÈÄ≤ LLM Êõ¥ÂÆâÂÖ®ÁöÑÊï¥ÂêàÂà∞ÈÜ´ÁôÇ‰øùÂÅ•Â∑•‰ΩúÊµÅÁ®ã‰∏≠„ÄÇ</paragraph>

##### **An analysis of data variation and bias in image-based dermatological datasets for machine learning classification**
2501.08962v1 by Francisco Mauro, Emanoel Thyago, Othon Vinicius, Rodrigo Abreu, Kelvin Cunha, Jos√© Gabriel, Rafael Barros, Thales Bezerra, Manoel Henriques, Natalia Lopes, √ârico Moutinho, J√©ssica Guido, Tsang Ing Ren, Paulo Borba

AI algorithms have become valuable in aiding professionals in healthcare. The
increasing confidence obtained by these models is helpful in critical decision
demands. In clinical dermatology, classification models can detect malignant
lesions on patients' skin using only RGB images as input. However, most
learning-based methods employ data acquired from dermoscopic datasets on
training, which are large and validated by a gold standard. Clinical models aim
to deal with classification on users' smartphone cameras that do not contain
the corresponding resolution provided by dermoscopy. Also, clinical
applications bring new challenges. It can contain captures from uncontrolled
environments, skin tone variations, viewpoint changes, noises in data and
labels, and unbalanced classes. A possible alternative would be to use transfer
learning to deal with the clinical images. However, as the number of samples is
low, it can cause degradations on the model's performance; the source
distribution used in training differs from the test set. This work aims to
evaluate the gap between dermoscopic and clinical samples and understand how
the dataset variations impact training. It assesses the main differences
between distributions that disturb the model's prediction. Finally, from
experiments on different architectures, we argue how to combine the data from
divergent distributions, decreasing the impact on the model's final accuracy.

ÊëòË¶ÅÔºöAI ÊºîÁÆóÊ≥ïÂ∑≤ÊàêÁÇ∫ÂçîÂä©ÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°ÁöÑÂØ∂Ë≤¥Â∑•ÂÖ∑„ÄÇÈÄô‰∫õÊ®°ÂûãÁç≤ÂæóÁöÑ‰ø°ÂøÉÊó•ÁõäÊèêÂçáÔºåÊúâÂä©ÊñºÈóúÈçµÊ±∫Á≠ñÈúÄÊ±Ç„ÄÇÂú®Ëá®Â∫äÁöÆËÜöÁßëÔºåÂàÜÈ°ûÊ®°ÂûãÂÉÖ‰ΩøÁî® RGB ÂΩ±ÂÉè‰ΩúÁÇ∫Ëº∏ÂÖ•ÔºåÂç≥ÂèØÂÅµÊ∏¨ÊÇ£ËÄÖÁöÆËÜö‰∏äÁöÑÊÉ°ÊÄßÁóÖÁÅ∂„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏Âü∫ÊñºÂ≠∏ÁøíÁöÑÊñπÊ≥ïÊé°Áî®ÂæûÁöÆËÜöÈè°Ë≥áÊñôÈõÜÂèñÂæóÁöÑË≥áÊñôÈÄ≤Ë°åË®ìÁ∑¥ÔºåÈÄô‰∫õË≥áÊñôÈõÜÈæêÂ§ß‰∏îÂ∑≤ÈÄöÈÅéÈáëÊ®ôÊ∫ñÈ©óË≠â„ÄÇËá®Â∫äÊ®°ÂûãÊó®Âú®ËôïÁêÜ‰ΩøÁî®ËÄÖÊô∫ÊÖßÂûãÊâãÊ©üÁõ∏Ê©ü‰∏äÁöÑÂàÜÈ°ûÔºåÈÄô‰∫õÁõ∏Ê©ü‰∏çÂåÖÂê´ÁöÆËÜöÈè°Êèê‰æõÁöÑÂ∞çÊáâËß£ÊûêÂ∫¶„ÄÇÊ≠§Â§ñÔºåËá®Â∫äÊáâÁî®Á®ãÂºèÂ∏∂‰æÜÊñ∞ÁöÑÊåëÊà∞„ÄÇÂÆÉÂèØËÉΩÂåÖÂê´‰æÜËá™‰∏çÂèóÊéßÁí∞Â¢ÉÁöÑÊì∑Âèñ„ÄÅËÜöËâ≤ËÆäÂåñ„ÄÅË¶ñÈªûËÆäÊõ¥„ÄÅË≥áÊñôÂíåÊ®ôÁ±§‰∏≠ÁöÑÈõúË®äÔºå‰ª•Âèä‰∏çÂπ≥Ë°°ÁöÑÈ°ûÂà•„ÄÇ‰∏ÄÁ®ÆÂèØËÉΩÁöÑÊõø‰ª£ÊñπÊ°àÊòØ‰ΩøÁî®ÈÅ∑ÁßªÂ≠∏Áøí‰æÜËôïÁêÜËá®Â∫äÂΩ±ÂÉè„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÊ®£Êú¨Êï∏ÈáèÂ∞ëÔºåÂèØËÉΩÊúÉÂ∞éËá¥Ê®°ÂûãÊïàËÉΩ‰∏ãÈôçÔºõË®ìÁ∑¥‰∏≠‰ΩøÁî®ÁöÑ‰æÜÊ∫êÂàÜ‰ΩàËàáÊ∏¨Ë©¶ÈõÜ‰∏çÂêå„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊó®Âú®Ë©ï‰º∞ÁöÆËÜöÈè°ÂíåËá®Â∫äÊ®£Êú¨‰πãÈñìÁöÑÂ∑ÆË∑ùÔºå‰∏¶‰∫ÜËß£Ë≥áÊñôÈõÜËÆäÂåñÂ¶Ç‰ΩïÂΩ±ÈüøË®ìÁ∑¥„ÄÇÂÆÉË©ï‰º∞ÊúÉÂπ≤ÊìæÊ®°ÂûãÈ†êÊ∏¨ÁöÑ‰∏ªË¶ÅÂàÜ‰ΩàÂ∑ÆÁï∞„ÄÇÊúÄÂæåÔºåÂæû‰∏çÂêåÊû∂ÊßãÁöÑÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëË´ñË≠âÂ¶Ç‰ΩïÁµêÂêà‰æÜËá™‰∏çÂêåÂàÜ‰ΩàÁöÑË≥áÊñôÔºåÈôç‰ΩéÂ∞çÊ®°ÂûãÊúÄÁµÇÊ∫ñÁ¢∫Â∫¶ÁöÑÂΩ±Èüø„ÄÇ

##### **Improving the Efficiency of Self-Supervised Adversarial Training through Latent Clustering-Based Selection**
2501.10466v1 by Somrita Ghosh, Yuelin Xu, Xiao Zhang

Compared with standard learning, adversarially robust learning is widely
recognized to demand significantly more training examples. Recent works propose
the use of self-supervised adversarial training (SSAT) with external or
synthetically generated unlabeled data to enhance model robustness. However,
SSAT requires a substantial amount of extra unlabeled data, significantly
increasing memory usage and model training times. To address these challenges,
we propose novel methods to strategically select a small subset of unlabeled
data essential for SSAT and robustness improvement. Our selection prioritizes
data points near the model's decision boundary based on latent clustering-based
techniques, efficiently identifying a critical subset of unlabeled data with a
higher concentration of boundary-adjacent points. While focusing on
near-boundary data, our methods are designed to maintain a balanced ratio
between boundary and non-boundary data points to avoid overfitting. Our
experiments on image benchmarks show that integrating our selection strategies
into self-supervised adversarial training can largely reduce memory and
computational requirements while achieving high model robustness. In
particular, our latent clustering-based selection method with k-means is the
most effective, achieving nearly identical test-time robust accuracies with 5
to 10 times less external or generated unlabeled data when applied to image
benchmarks. Additionally, we validate the generalizability of our approach
across various application scenarios, including a real-world medical dataset
for COVID-19 chest X-ray classification.

ÊëòË¶ÅÔºöËàáÊ®ôÊ∫ñÂ≠∏ÁøíÁõ∏ÊØîÔºåÂ∞çÊäóÊÄßÁ©©ÂÅ•Â≠∏ÁøíÂª£Ê≥õË¢´Ë™çÁÇ∫ÈúÄË¶ÅÊõ¥Â§öË®ìÁ∑¥ÁØÑ‰æã„ÄÇËøëÊúüÁ†îÁ©∂ÊèêÂá∫‰ΩøÁî®ÂÖ∑ÊúâÂ§ñÈÉ®ÊàñÂêàÊàêÁî¢ÁîüÊ®ôÁ±§Ë≥áÊñôÁöÑËá™Áõ£Áù£Â∞çÊäóË®ìÁ∑¥ (SSAT) ‰æÜÂ¢ûÂº∑Ê®°ÂûãÁ©©ÂÅ•ÊÄß„ÄÇÁÑ∂ËÄåÔºåSSAT ÈúÄË¶ÅÂ§ßÈáèÁöÑÈ°çÂ§ñÊú™Ê®ôÁ±§Ë≥áÊñôÔºåÈ°ØËëóÂ¢ûÂä†Ë®òÊÜ∂È´î‰ΩøÁî®ÈáèÂíåÊ®°ÂûãË®ìÁ∑¥ÊôÇÈñì„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫Êñ∞Á©éÁöÑÊñπÊ≥ï‰æÜÁ≠ñÁï•ÊÄßÂú∞ÈÅ∏Êìá‰∏ÄÂ∞èÈÉ®ÂàÜÊú™Ê®ôÁ±§Ë≥áÊñôÔºåÈÄôÂ∞ç SSAT ÂíåÁ©©ÂÅ•ÊÄßÊîπÈÄ≤Ëá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÁöÑÈÅ∏ÊìáÂü∫ÊñºÊΩõÂú®Áæ§ÈõÜÊäÄË°ìÔºåÂÑ™ÂÖàËÄÉÊÖÆÊ®°ÂûãÊ±∫Á≠ñÈÇäÁïåÈôÑËøëÁöÑË≥áÊñôÈªûÔºåÊúâÊïàÂú∞Ë≠òÂà•Âá∫‰∏ÄÁµÑÈóúÈçµÁöÑÊú™Ê®ôÁ±§Ë≥áÊñôÂ≠êÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ËºÉÈ´òÊøÉÂ∫¶ÁöÑÈÇäÁïåÁõ∏ÈÑ∞Èªû„ÄÇÈõñÁÑ∂Â∞àÊ≥®ÊñºËøëÈÇäÁïåË≥áÊñôÔºå‰ΩÜÊàëÂÄëÁöÑÊ®°ÂûãÊó®Âú®‰øùÊåÅÈÇäÁïåÂíåÈùûÈÇäÁïåË≥áÊñôÈªû‰πãÈñìÁöÑÂπ≥Ë°°ÊØîÁéáÔºå‰ª•ÈÅøÂÖçÈÅéÂ∫¶Êì¨Âêà„ÄÇÊàëÂÄëÂú®ÂΩ±ÂÉèÂü∫Ê∫ñ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåÂ∞áÊàëÂÄëÁöÑÈÅ∏ÊìáÁ≠ñÁï•Êï¥ÂêàÂà∞Ëá™Áõ£Áù£Â∞çÊäóË®ìÁ∑¥‰∏≠ÔºåÂèØ‰ª•Âú®ÂØ¶ÁèæÈ´òÊ®°ÂûãÁ©©ÂÅ•ÊÄßÁöÑÂêåÊôÇÔºåÂ§ßÂπÖÊ∏õÂ∞ëË®òÊÜ∂È´îÂíåË®àÁÆóÈúÄÊ±Ç„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÂü∫Êñº k Âπ≥ÂùáÂÄºÁöÑÊΩõÂú®Áæ§ÈõÜÈÅ∏ÊìáÊñπÊ≥ïÊúÄÊúâÊïàÔºåÂú®ÊáâÁî®ÊñºÂΩ±ÂÉèÂü∫Ê∫ñÊôÇÔºå‰ª•Â∞ë 5 Âà∞ 10 ÂÄçÁöÑÂ§ñÈÉ®ÊàñÁîüÊàêÊú™Ê®ôÁ±§Ë≥áÊñôÔºåÈÅîÂà∞Âπæ‰πéÁõ∏ÂêåÁöÑÊ∏¨Ë©¶ÊôÇÈñìÁ©©ÂÅ•Ê∫ñÁ¢∫Â∫¶„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÂêÑÁ®ÆÊáâÁî®Â†¥ÊôØ‰∏≠ÁöÑÈÄöÁî®ÊÄßÔºåÂåÖÊã¨Áî®Êñº COVID-19 ËÉ∏ÈÉ® X ÂÖâÂàÜÈ°ûÁöÑÁúüÂØ¶‰∏ñÁïåÈÜ´ÁôÇË≥áÊñôÈõÜ„ÄÇ

##### **Digital Phenotyping for Adolescent Mental Health: A Feasibility Study Employing Machine Learning to Predict Mental Health Risk From Active and Passive Smartphone Data**
2501.08851v1 by Balasundaram Kadirvelu, Teresa Bellido Bel, Aglaia Freccero, Martina Di Simplicio, Dasha Nicholls, A Aldo Faisal

Background: Adolescents are particularly vulnerable to mental disorders, with
over 75% of cases manifesting before the age of 25. Research indicates that
only 18 to 34% of young people experiencing high levels of depression or
anxiety symptoms seek support. Digital tools leveraging smartphones offer
scalable and early intervention opportunities. Objective: Using a novel machine
learning framework, this study evaluated the feasibility of integrating active
and passive smartphone data to predict mental disorders in non-clinical
adolescents. Specifically, we investigated the utility of the Mindcraft app in
predicting risks for internalising and externalising disorders, eating
disorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean
age 16.1 years) were recruited from three London schools. Participants
completed the Strengths and Difficulties Questionnaire, the Eating Disorders-15
Questionnaire, Sleep Condition Indicator Questionnaire and indicated the
presence/absence of suicidal ideation. They used the Mindcraft app for 14 days,
contributing active data via self-reports and passive data from smartphone
sensors. A contrastive pretraining phase was applied to enhance user-specific
feature stability, followed by supervised fine-tuning. The model evaluation
employed leave-one-subject-out cross-validation using balanced accuracy as the
primary metric. Results: The integration of active and passive data achieved
superior performance compared to individual data sources, with mean balanced
accuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal
ideation and 0.70 for eating disorders. The contrastive learning framework
stabilised daily behavioural representations, enhancing predictive robustness.
This study demonstrates the potential of integrating active and passive
smartphone data with advanced machine-learning techniques for predicting mental
health risks.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöÈùíÂ∞ëÂπ¥ÁâπÂà´ÂÆπÊòìÁΩπÊÇ£Á≤æÁ•ûÁñæÁóÖÔºå75% ‰ª•‰∏äÁöÑÁóÖ‰æãÂú® 25 Â≤Å‰πãÂâçÊòæÁé∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂè™Êúâ 18% Âà∞ 34% ÁªèÂéÜÈ´òÂ∫¶ÊäëÈÉÅÊàñÁÑ¶ËôëÁóáÁä∂ÁöÑÂπ¥ËΩª‰∫∫ÂØªÊ±ÇÊîØÊåÅ„ÄÇÂà©Áî®Êô∫ËÉΩÊâãÊú∫ÁöÑÊï∞‰ΩçÂ∑•ÂÖ∑Êèê‰æõÂèØÊâ©Â±ïÁöÑÊó©Êúü‰ªãÂÖ•Êú∫‰ºö„ÄÇÁõÆÊ†áÔºöÊú¨Á†îÁ©∂‰ΩøÁî®Êñ∞È¢ñÁöÑÊú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ÔºåËØÑ‰º∞Â∞Ü‰∏ªÂä®ÂíåË¢´Âä®Êô∫ËÉΩÊâãÊú∫Êï∞ÊçÆÊï¥ÂêàÊù•È¢ÑÊµãÈùû‰∏¥Â∫äÈùíÂ∞ëÂπ¥Á≤æÁ•ûÁñæÁóÖÁöÑÂèØË°åÊÄß„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Ë∞ÉÊü•‰∫Ü Mindcraft Â∫îÁî®Á®ãÂ∫èÂú®È¢ÑÊµãÂÜÖÂåñÂíåÂ§ñÂåñÈöúÁ¢ç„ÄÅÈ•ÆÈ£üÂ§±Ë∞É„ÄÅÂ§±Áú†ÂíåËá™ÊùÄÊÑèÂøµÊñπÈù¢ÁöÑÊïàÁî®„ÄÇÊñπÊ≥ïÔºöÂèÇ‰∏éËÄÖÔºàN=103ÔºõÂπ≥ÂùáÂπ¥ÈæÑ 16.1 Â≤ÅÔºâÊù•Ëá™‰º¶Êï¶ÁöÑ‰∏âÊâÄÂ≠¶Ê†°„ÄÇÂèÇ‰∏éËÄÖÂÆåÊàê‰∫Ü‰ºòÂäøÂíåÂõ∞ÈöæÈóÆÂç∑„ÄÅËøõÈ£üÈöúÁ¢ç-15 ÈóÆÂç∑„ÄÅÁù°Áú†Áä∂ÂÜµÊåáÊ†áÈóÆÂç∑ÔºåÂπ∂ÊåáÂá∫‰∫ÜÊòØÂê¶Â≠òÂú®Ëá™ÊùÄÊÑèÂøµ„ÄÇ‰ªñ‰ª¨‰ΩøÁî® Mindcraft Â∫îÁî®Á®ãÂ∫è 14 Â§©ÔºåÈÄöËøáËá™ÊàëÊä•ÂëäÊèê‰æõ‰∏ªÂä®Êï∞ÊçÆÔºåÂπ∂‰ªéÊô∫ËÉΩÊâãÊú∫‰º†ÊÑüÂô®Êèê‰æõË¢´Âä®Êï∞ÊçÆ„ÄÇÂ∫îÁî®ÂØπÊØîÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÊù•Â¢ûÂº∫ÁâπÂÆöÁî®Êà∑ÁöÑÁâπÂæÅÁ®≥ÂÆöÊÄßÔºåÁÑ∂ÂêéËøõË°åÁõëÁù£ÂæÆË∞É„ÄÇÊ®°ÂûãËØÑ‰º∞ÈááÁî®Áïô‰∏ÄÊ≥ï‰∫§ÂèâÈ™åËØÅÔºå‰ΩøÁî®Âπ≥Ë°°ÂáÜÁ°ÆÂ∫¶‰Ωú‰∏∫‰∏ªË¶ÅÊåáÊ†á„ÄÇÁªìÊûúÔºö‰∏é‰∏™Âà´Êï∞ÊçÆÊ∫êÁõ∏ÊØîÔºå‰∏ªÂä®ÂíåË¢´Âä®Êï∞ÊçÆÁöÑÊï¥ÂêàÂÆûÁé∞‰∫ÜÊõ¥Â•ΩÁöÑÊÄßËÉΩÔºåSDQ È´òÈ£éÈô©ÁöÑÂπ≥ÂùáÂπ≥Ë°°ÂáÜÁ°ÆÂ∫¶‰∏∫ 0.71ÔºåÂ§±Áú†‰∏∫ 0.67ÔºåËá™ÊùÄÊÑèÂøµ‰∏∫ 0.77ÔºåÈ•ÆÈ£üÂ§±Ë∞É‰∏∫ 0.70„ÄÇÂØπÊØîÂ≠¶‰π†Ê°ÜÊû∂Á®≥ÂÆö‰∫ÜÊØèÊó•Ë°å‰∏∫Ë°®ÂæÅÔºåÂ¢ûÂº∫‰∫ÜÈ¢ÑÊµãÈ≤ÅÊ£íÊÄß„ÄÇÊú¨Á†îÁ©∂Â±ïÁ§∫‰∫ÜÂ∞Ü‰∏ªÂä®ÂíåË¢´Âä®Êô∫ËÉΩÊâãÊú∫Êï∞ÊçÆ‰∏éÂÖàËøõÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÁõ∏ÁªìÂêà‰ª•È¢ÑÊµãÂøÉÁêÜÂÅ•Â∫∑È£éÈô©ÁöÑÊΩúÂäõ„ÄÇ</paragraph>

##### **Spatio-Temporal Foundation Models: Vision, Challenges, and Opportunities**
2501.09045v1 by Adam Goodge, Wee Siong Ng, Bryan Hooi, See Kiong Ng

Foundation models have revolutionized artificial intelligence, setting new
benchmarks in performance and enabling transformative capabilities across a
wide range of vision and language tasks. However, despite the prevalence of
spatio-temporal data in critical domains such as transportation, public health,
and environmental monitoring, spatio-temporal foundation models (STFMs) have
not yet achieved comparable success. In this paper, we articulate a vision for
the future of STFMs, outlining their essential characteristics and the
generalization capabilities necessary for broad applicability. We critically
assess the current state of research, identifying gaps relative to these ideal
traits, and highlight key challenges that impede their progress. Finally, we
explore potential opportunities and directions to advance research towards the
aim of effective and broadly applicable STFMs.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂæπÂ∫ïÊîπËÆä‰∫Ü‰∫∫Â∑•Êô∫ÊÖßÔºåÂú®ÊïàËÉΩ‰∏äÊ®πÁ´ãÊñ∞ÁöÑÂü∫Ê∫ñÔºå‰∏¶Âú®Âª£Ê≥õÁöÑË¶ñË¶∫ÂíåË™ûË®Ä‰ªªÂãô‰∏≠ÂØ¶ÁèæËΩâÂûãËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÑòÁÆ°ÊôÇÁ©∫Ë≥áÊñôÊôÆÈÅçÂ≠òÂú®ÊñºÈÅãËº∏„ÄÅÂÖ¨ÂÖ±Ë°õÁîüÂíåÁí∞Â¢ÉÁõ£ÊéßÁ≠âÈóúÈçµÈ†òÂüüÔºå‰ΩÜÊôÇÁ©∫Âü∫Á§éÊ®°Âûã (STFM) Â∞öÊú™ÂèñÂæóÂêåÁ≠âÊàêÂ∞±„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈó°Ëø∞‰∫ÜÂ∞ç STFM Êú™‰æÜÁöÑÈ°òÊôØÔºåÊ¶ÇËø∞‰∫ÜÂÖ∂Âü∫Êú¨ÁâπÂæµÂíåÂª£Ê≥õÈÅ©Áî®ÁöÑÂøÖË¶ÅÊ¶ÇÊã¨ËÉΩÂäõ„ÄÇÊàëÂÄëÊâπÂà§ÊÄßÂú∞Ë©ï‰º∞‰∫ÜÁï∂ÂâçÁ†îÁ©∂ÁöÑÁãÄÊÖãÔºåÊâæÂá∫Áõ∏Â∞çÊñºÈÄô‰∫õÁêÜÊÉ≥ÁâπË≥™ÁöÑÂ∑ÆË∑ùÔºå‰∏¶Âº∑Ë™øÈòªÁ§ôÂÖ∂ÈÄ≤Â±ïÁöÑÈóúÈçµÊåëÊà∞„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÊé®ÈÄ≤Á†îÁ©∂ÁöÑÊΩõÂú®Ê©üÊúÉÂíåÊñπÂêëÔºå‰ª•ÂØ¶ÁèæÊúâÊïà‰∏îÂª£Ê≥õÈÅ©Áî®ÁöÑ STFM„ÄÇ

##### **ADAM-1: AI and Bioinformatics for Alzheimer's Detection and Microbiome-Clinical Data Integrations**
2501.08324v1 by Ziyuan Huang, Vishaldeep Kaur Sekhon, Ouyang Guo, Mark Newman, Roozbeh Sadeghian, Maria L. Vaida, Cynthia Jo, Doyle Ward, Vanni Bucci, John P. Haran

The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent
large language model (LLM) framework designed to integrate and analyze
multi-modal data, including microbiome profiles, clinical datasets, and
external knowledge bases, to enhance the understanding and detection of
Alzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)
techniques along with its multi-agent architecture, ADAM-1 synthesizes insights
from diverse data sources and contextualizes findings using literature-driven
evidence. Comparative evaluation against XGBoost revealed similar mean F1
scores but significantly reduced variance for ADAM-1, highlighting its
robustness and consistency, particularly in small laboratory datasets. While
currently tailored for binary classification tasks, future iterations aim to
incorporate additional data modalities, such as neuroimaging and biomarkers, to
broaden the scalability and applicability for Alzheimer's research and
diagnostics.

ÊëòË¶ÅÔºöÈòøËå≤Êµ∑ÈªòÁóáÂàÜÊûêÊ®°ÂûãÁîüÊàê 1 (ADAM) ÊòØ‰∏ÄÂÄãÂ§ö‰ª£ÁêÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êû∂ÊßãÔºåÊó®Âú®Êï¥ÂêàÂíåÂàÜÊûêÂ§öÊ®°ÂºèÊï∏ÊìöÔºåÂåÖÊã¨ÂæÆÁîüÁâ©ÁµÑÁâπÂæµ„ÄÅËá®Â∫äÊï∏ÊìöÈõÜÂíåÂ§ñÈÉ®Áü•Ë≠òÂ∫´Ôºå‰ª•Â¢ûÈÄ≤Â∞çÈòøËå≤Êµ∑ÈªòÁóá (AD) ÁöÑÁêÜËß£ÂíåÂÅµÊ∏¨„ÄÇÈÄèÈÅéÂà©Áî®Êì∑ÂèñÂ¢ûÂº∑ÁîüÊàê (RAG) ÊäÄË°ì‰ª•ÂèäÂÖ∂Â§ö‰ª£ÁêÜÊû∂ÊßãÔºåADAM-1 Âæû‰∏çÂêåÁöÑÊï∏Êìö‰æÜÊ∫ê‰∏≠Á∂úÂêàË¶ãËß£Ôºå‰∏¶‰ΩøÁî®ÊñáÁçªÈ©ÖÂãïÁöÑË≠âÊìöÂ∞çÁôºÁèæÈÄ≤Ë°åÊÉÖÂ¢ÉÂåñ„ÄÇËàá XGBoost ÁöÑÊØîËºÉË©ï‰º∞È°ØÁ§∫È°û‰ººÁöÑÂπ≥Âùá F1 ÂàÜÊï∏Ôºå‰ΩÜ ADAM-1 ÁöÑËÆäÁï∞È°ØËëóÈôç‰ΩéÔºåÁ™ÅÈ°ØÂÖ∂Á©©ÂÅ•ÊÄßÂíå‰∏ÄËá¥ÊÄßÔºåÁâπÂà•ÊòØÂú®Â∞èÂûãÂØ¶È©óÂÆ§Êï∏ÊìöÈõÜ‰∏≠„ÄÇÈõñÁÑ∂ÁõÆÂâçÈáùÂ∞ç‰∫åÂÖÉÂàÜÈ°û‰ªªÂãôÈÄ≤Ë°åË™øÊï¥Ôºå‰ΩÜÊú™‰æÜÁöÑËø≠‰ª£Êó®Âú®Á¥çÂÖ•ÂÖ∂‰ªñÊï∏ÊìöÊ®°ÂºèÔºå‰æãÂ¶ÇÁ•ûÁ∂ìÂΩ±ÂÉèÂíåÁîüÁâ©Ê®ôË®òÔºå‰ª•Êì¥Â§ßÈòøËå≤Êµ∑ÈªòÁóáÁ†îÁ©∂ÂíåË®∫Êñ∑ÁöÑÂèØÊì¥ÂÖÖÊÄßÂíåÈÅ©Áî®ÊÄß„ÄÇ

##### **A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images using Choquet Integral and Differential Evolution Optimization**
2501.08241v1 by Amir Reza Takhsha, Maryam Rastgarpour, Mozhgan Naderi

The COVID-19 pandemic has profoundly impacted billions globally. It
challenges public health and healthcare systems due to its rapid spread and
severe respiratory effects. An effective strategy to mitigate the COVID-19
pandemic involves integrating testing to identify infected individuals. While
RT-PCR is considered the gold standard for diagnosing COVID-19, it has some
limitations such as the risk of false negatives. To address this problem, this
paper introduces a novel Deep Learning Diagnosis System that integrates
pre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble
learning framework to achieve precise identification of COVID-19 cases from
Chest X-ray (CXR) images. We combine feature vectors from the final hidden
layers of pre-trained DCNNs using the Choquet integral to capture interactions
between different DCNNs that a linear approach cannot. We employed
Sugeno-$\lambda$ measure theory to derive fuzzy measures for subsets of
networks to enable aggregation. We utilized Differential Evolution to estimate
fuzzy densities. We developed a TensorFlow-based layer for Choquet operation to
facilitate efficient aggregation, due to the intricacies involved in
aggregating feature vectors. Experimental results on the COVIDx dataset show
that our ensemble model achieved 98\% accuracy in three-class classification
and 99.50\% in binary classification, outperforming its components-DenseNet-201
(97\% for three-class, 98.75\% for binary), Inception-v3 (96.25\% for
three-class, 98.50\% for binary), and Xception (94.50\% for three-class, 98\%
for binary)-and surpassing many previous methods.

ÊëòË¶ÅÔºöÊñ∞ÂÜ†ËÇ∫ÁÇéÁñ´ÊÉÖÂ∑≤ÂØπÂÖ®ÁêÉÊï∞ÂçÅ‰∫ø‰∫∫‰∫ßÁîüÊ∑±ËøúÂΩ±Âìç„ÄÇÁî±‰∫éÂÖ∂‰º†Êí≠ËøÖÈÄü‰∏îÂëºÂê∏ÈÅìÁóáÁä∂‰∏•ÈáçÔºåÂÆÉÂØπÂÖ¨ÂÖ±Âç´ÁîüÂíåÂåªÁñó‰øùÂÅ•Á≥ªÁªüÊûÑÊàêÊåëÊàò„ÄÇÂáèËΩªÊñ∞ÂÜ†ËÇ∫ÁÇéÁñ´ÊÉÖÁöÑÊúâÊïàÁ≠ñÁï•ÂåÖÊã¨Êï¥ÂêàÊ£ÄÊµã‰ª•ËØÜÂà´ÂèóÊÑüÊüìËÄÖ„ÄÇËôΩÁÑ∂ RT-PCR Ë¢´ËÆ§‰∏∫ÊòØËØäÊñ≠Êñ∞ÂÜ†ËÇ∫ÁÇéÁöÑÈªÑÈáëÊ†áÂáÜÔºå‰ΩÜÂÆÉ‰πüÊúâ‰∏Ä‰∫õÈôêÂà∂Ôºå‰æãÂ¶ÇÂÅáÈò¥ÊÄßÁöÑÈ£éÈô©„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ∑±Â∫¶Â≠¶‰π†ËØäÊñ≠Á≥ªÁªüÔºåËØ•Á≥ªÁªüÂ∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑÊ∑±Â∫¶Âç∑ÁßØÁ•ûÁªèÁΩëÁªú (DCNN) ÈõÜÊàêÂà∞ÈõÜÊàêÂ≠¶‰π†Ê°ÜÊû∂‰∏≠Ôºå‰ª•‰ªéËÉ∏ÈÉ® X Â∞ÑÁ∫ø (CXR) ÂõæÂÉè‰∏≠Á≤æÁ°ÆËØÜÂà´Êñ∞ÂÜ†ËÇ∫ÁÇéÁóÖ‰æã„ÄÇÊàë‰ª¨‰ΩøÁî® Choquet ÁßØÂàÜÁªìÂêàÊù•Ëá™È¢ÑËÆ≠ÁªÉ DCNN ÁöÑÊúÄÂêé‰∏Ä‰∏™ÈöêËóèÂ±ÇÁöÑÁâπÂæÅÂêëÈáèÔºå‰ª•ÊçïËé∑Á∫øÊÄßÊñπÊ≥ïÊó†Ê≥ïÂÆûÁé∞ÁöÑ‰∏çÂêå DCNN ‰πãÈó¥ÁöÑ‰∫§‰∫í„ÄÇÊàë‰ª¨ÈááÁî® Sugeno-$\lambda$ ÊµãÂ∫¶ÁêÜËÆ∫Êù•ÂØºÂá∫ÁΩëÁªúÂ≠êÈõÜÁöÑÊ®°Á≥äÊµãÂ∫¶‰ª•ÂÆûÁé∞ËÅöÂêà„ÄÇÊàë‰ª¨Âà©Áî®Â∑ÆÂàÜËøõÂåñÊù•‰º∞ËÆ°Ê®°Á≥äÂØÜÂ∫¶„ÄÇÁî±‰∫éËÅöÂêàÁâπÂæÅÂêëÈáèÁöÑÂ§çÊùÇÊÄßÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Âü∫‰∫é TensorFlow ÁöÑ Choquet Êìç‰ΩúÂ±Ç‰ª•‰øÉËøõÈ´òÊïàËÅöÂêà„ÄÇCOVIDx Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÈõÜÊàêÊ®°ÂûãÂú®‰∏âÁ±ªÂàÜÁ±ª‰∏≠ËææÂà∞ 98% ÁöÑÂáÜÁ°ÆÁéáÔºåÂú®‰∫åÂÖÉÂàÜÁ±ª‰∏≠ËææÂà∞ 99.50%Ôºå‰ºò‰∫éÂÖ∂ÁªÑ‰ª∂ DenseNet-201Ôºà‰∏âÁ±ª‰∏∫ 97%Ôºå‰∫åÂÖÉ‰∏∫ 98.75%Ôºâ„ÄÅInception-v3Ôºà‰∏âÁ±ª‰∏∫ 96.25%Ôºå‰∫åÂÖÉ‰∏∫ 98.50%ÔºâÂíå XceptionÔºà‰∏âÁ±ª‰∏∫ 94.50%Ôºå‰∫åÂÖÉ‰∏∫ 98%ÔºâÔºåÂπ∂Ë∂ÖË∂ä‰∫ÜËÆ∏Â§ö‰ª•ÂâçÁöÑÊñπÊ≥ï„ÄÇ

##### **ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems**
2501.08208v1 by Mohita Chowdhury, Yajie Vera He, Aisling Higham, Ernest Lim

Large Language Models (LLMs) have shown impressive potential in clinical
question answering (QA), with Retrieval Augmented Generation (RAG) emerging as
a leading approach for ensuring the factual accuracy of model responses.
However, current automated RAG metrics perform poorly in clinical and
conversational use cases. Using clinical human evaluations of responses is
expensive, unscalable, and not conducive to the continuous iterative
development of RAG systems. To address these challenges, we introduce ASTRID -
an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging
RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy
(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is
designed to better capture the faithfulness of a model's response to the
knowledge base without penalising conversational elements. To validate our
triad, we curate a dataset of over 200 real-world patient questions posed to an
LLM-based QA agent during surgical follow-up for cataract surgery - the highest
volume operation in the world - augmented with clinician-selected questions for
emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate
that CF can predict human ratings of faithfulness better than existing
definitions for conversational use cases. Furthermore, we show that evaluation
using our triad consisting of CF, RA, and CR exhibits alignment with clinician
assessment for inappropriate, harmful, or unhelpful responses. Finally, using
nine different LLMs, we demonstrate that the three metrics can closely agree
with human evaluations, highlighting the potential of these metrics for use in
LLM-driven automated evaluation pipelines. We also publish the prompts and
datasets for these experiments, providing valuable resources for further
research and development.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá®Â∫äÂïèÁ≠î (QA) ‰∏≠Â±ïÁèæ‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊΩõÂäõÔºåÂÖ∂‰∏≠Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊàêÁÇ∫Á¢∫‰øùÊ®°ÂûãÂõûÊáâ‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄßÁöÑÈ†òÂÖàÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑËá™ÂãïÂåñ RAG ÊåáÊ®ôÂú®Ëá®Â∫äÂíåÂ∞çË©±ÂºèÁî®‰æã‰∏≠Ë°®Áèæ‰∏ç‰Ω≥„ÄÇ‰ΩøÁî®Ëá®Â∫ä‰∫∫È°ûÂ∞çÂõûÊáâÁöÑË©ï‰º∞Êó¢ÊòÇË≤¥Âèà‰∏çÂÖ∑ÂèØÊì¥ÂÖÖÊÄßÔºå‰πü‰∏çÂà©Êñº RAG Á≥ªÁµ±ÁöÑÊåÅÁ∫åËø≠‰ª£ÈñãÁôº„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü ASTRID - ‰∏ÄÁ®ÆÁî®ÊñºË©ï‰º∞Âà©Áî® RAG ÁöÑËá®Â∫ä QA Á≥ªÁµ±ÁöÑËá™ÂãïÂåñ‰∏îÂèØÊì¥ÂÖÖÁöÑ TRIaD - ÂåÖÂê´‰∏âÂÄãÊåáÊ®ôÔºöËÑàÁµ°Áõ∏ÈóúÊÄß (CR)„ÄÅÊãíÁµïÊ∫ñÁ¢∫ÊÄß (RA) ÂíåÂ∞çË©±Âø†ÂØ¶Â∫¶ (CF)„ÄÇÊàëÂÄëÊñ∞Á©éÁöÑË©ï‰º∞ÊåáÊ®ô CF Êó®Âú®Êõ¥Â•ΩÂú∞ÊçïÊçâÊ®°ÂûãÂ∞çÁü•Ë≠òÂ∫´ÁöÑÂõûÊáâÁöÑÂø†ÂØ¶Â∫¶ÔºåÂêåÊôÇ‰∏çÊá≤ÁΩ∞Â∞çË©±ÂÖÉÁ¥†„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÊàëÂÄëÁöÑ‰∏âÂÖÉÁµÑÔºåÊàëÂÄëÁ≠ñÂäÉ‰∫Ü‰∏ÄÂÄãÊï∏ÊìöÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´Âú®ÁôΩÂÖßÈöúÊâãË°ìË°ìÂæåÈö®Ë®™ÊúüÈñìÂêë LLM Âü∫Êñº QA ÁöÑ‰ª£ÁêÜÊèêÂá∫ÁöÑ 200 Â§öÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑÊÇ£ËÄÖÂïèÈ°å - ‰∏ñÁïå‰∏äÊâãË°ìÈáèÊúÄÂ§ßÁöÑÊâãË°ì - ‰∏¶Â¢ûÂä†‰∫ÜËá®Â∫äÈÜ´ÁîüÈÅ∏ÊìáÁöÑÂïèÈ°åÔºåÁî®ÊñºÁ∑äÊÄ•„ÄÅËá®Â∫äÂíåÈùûËá®Â∫äÈ†òÂüüÂ§ñÊÉÖÂ¢É„ÄÇÊàëÂÄëË≠âÊòéÔºåËàáÂ∞çË©±ÂºèÁî®‰æãÁèæÊúâÂÆöÁæ©Áõ∏ÊØîÔºåCF ÂèØ‰ª•Êõ¥Â•ΩÂú∞È†êÊ∏¨‰∫∫È°ûÂ∞çÂø†ÂØ¶Â∫¶ÁöÑË©ïÂàÜ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË°®Êòé‰ΩøÁî®Áî± CF„ÄÅRA Âíå CR ÁµÑÊàêÁöÑ‰∏âÂÖÉÁµÑÈÄ≤Ë°åË©ï‰º∞ËàáËá®Â∫äÈÜ´ÁîüÂ∞ç‰∏çÈÅ©Áï∂„ÄÅÊúâÂÆ≥ÊàñÁÑ°ÁõäÁöÑÂõûÊáâÁöÑË©ï‰º∞‰øùÊåÅ‰∏ÄËá¥„ÄÇÊúÄÂæåÔºå‰ΩøÁî®‰πùÁ®Æ‰∏çÂêåÁöÑ LLMÔºåÊàëÂÄëË≠âÊòéÈÄô‰∏âÂÄãÊåáÊ®ôÂèØ‰ª•Ëàá‰∫∫È°ûË©ï‰º∞Á∑äÂØÜ‰∏ÄËá¥ÔºåÁ™ÅÈ°Ø‰∫ÜÈÄô‰∫õÊåáÊ®ôÂú® LLM È©ÖÂãïÁöÑËá™ÂãïÂåñË©ï‰º∞ÁÆ°ÈÅì‰∏≠‰ΩøÁî®ÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÈÇÑÂÖ¨‰Ωà‰∫ÜÈÄô‰∫õÂØ¶È©óÁöÑÊèêÁ§∫ÂíåÊï∏ÊìöÈõÜÔºåÁÇ∫ÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂ÂíåÈñãÁôºÊèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË≥áÊ∫ê„ÄÇ

##### **Potential and Perils of Large Language Models as Judges of Unstructured Textual Data**
2501.08167v2 by Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar

Rapid advancements in large language models have unlocked remarkable
capabilities when it comes to processing and summarizing unstructured text
data. This has implications for the analysis of rich, open-ended datasets, such
as survey responses, where LLMs hold the promise of efficiently distilling key
themes and sentiments. However, as organizations increasingly turn to these
powerful AI systems to make sense of textual feedback, a critical question
arises, can we trust LLMs to accurately represent the perspectives contained
within these text based datasets? While LLMs excel at generating human-like
summaries, there is a risk that their outputs may inadvertently diverge from
the true substance of the original responses. Discrepancies between the
LLM-generated outputs and the actual themes present in the data could lead to
flawed decision-making, with far-reaching consequences for organizations. This
research investigates the effectiveness of LLM-as-judge models to evaluate the
thematic alignment of summaries generated by other LLMs. We utilized an
Anthropic Claude model to generate thematic summaries from open-ended survey
responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as
judges. This LLM-as-judge approach was compared to human evaluations using
Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable
alternative to traditional human centric evaluation methods. Our findings
reveal that while LLM-as-judge offer a scalable solution comparable to human
raters, humans may still excel at detecting subtle, context-specific nuances.
Our research contributes to the growing body of knowledge on AI assisted text
analysis. Further, we provide recommendations for future research, emphasizing
the need for careful consideration when generalizing LLM-as-judge models across
various contexts and use cases.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂø´ÈÄüÈÄ≤Ê≠•ÔºåÂú®ËôïÁêÜÂíåÁ∏ΩÁµêÈùûÁµêÊßãÂåñÊñáÂ≠óË≥áÊñôÊñπÈù¢ÔºåËß£Èéñ‰∫ÜÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÈÄôÂ∞çË±êÂØå„ÄÅÈñãÊîæÂºèË≥áÊñôÈõÜÁöÑÂàÜÊûêÊúâÂΩ±ÈüøÔºå‰æãÂ¶ÇË™øÊü•ÂõûÊáâÔºåÂÖ∂‰∏≠ LLM ÊâøË´æÊúâÊïàÂú∞ÊèêÁÖâÂá∫ÈóúÈçµ‰∏ªÈ°åÂíåÊÉÖÁ∑í„ÄÇÁÑ∂ËÄåÔºåÈö®ËëóÁµÑÁπîË∂ä‰æÜË∂ä‰æùË≥¥ÈÄô‰∫õÂº∑Â§ßÁöÑ AI Á≥ªÁµ±‰æÜÁêÜËß£ÊñáÂ≠óÂõûÈ•ãÔºå‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÂá∫Áèæ‰∫ÜÔºåÊàëÂÄëËÉΩÁõ∏‰ø° LLM ËÉΩÊ∫ñÁ¢∫Âú∞‰ª£Ë°®ÈÄô‰∫õÂü∫ÊñºÊñáÂ≠óÁöÑË≥áÊñôÈõÜÊâÄÂåÖÂê´ÁöÑËßÄÈªûÂóéÔºüÈõñÁÑ∂ LLM Âú®ÁîüÊàêÈ°û‰ºº‰∫∫È°ûÁöÑÊëòË¶ÅÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂ≠òÂú®ÂÖ∂Ëº∏Âá∫ÂèØËÉΩÁÑ°ÊÑèÈñìÂÅèÈõ¢ÂéüÂßãÂõûÊáâÁöÑÁúüÂØ¶ÂÖßÂÆπÁöÑÈ¢®Èö™„ÄÇLLM ÁîüÊàêÁöÑËº∏Âá∫ËàáË≥áÊñô‰∏≠Â≠òÂú®ÁöÑÂØ¶Èöõ‰∏ªÈ°å‰πãÈñìÁöÑÂ∑ÆÁï∞ÂèØËÉΩÂ∞éËá¥ÊúâÁº∫Èô∑ÁöÑÊ±∫Á≠ñÂà∂ÂÆöÔºåÂ∞çÁµÑÁπîÁî¢ÁîüÊ∑±ÈÅ†ÂΩ±Èüø„ÄÇÊú¨Á†îÁ©∂Ë™øÊü•‰∫Ü LLM ‰ΩúÁÇ∫Ë©ïÂØ©Ê®°ÂûãË©ï‰º∞ÂÖ∂‰ªñ LLM ÁîüÊàêÁöÑÊëòË¶ÅÁöÑ‰∏ªÈ°åÂ∞çÈΩäÊÄßÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÂà©Áî® Anthropic Claude Ê®°ÂûãÂæûÈñãÊîæÂºèË™øÊü•ÂõûÊáâ‰∏≠ÁîüÊàê‰∏ªÈ°åÊëòË¶ÅÔºåËÄå Amazon ÁöÑ Titan Express„ÄÅNova Pro Âíå Meta ÁöÑ Llama Ââá‰ΩúÁÇ∫Ë©ïÂØ©„ÄÇÈÄôÁ®Æ LLM ‰ΩúÁÇ∫Ë©ïÂØ©ÁöÑÊñπÊ≥ï‰ΩøÁî® Cohen's kappa„ÄÅSpearman's rho Âíå Krippendorff's alpha Ëàá‰∫∫È°ûË©ï‰º∞ÈÄ≤Ë°åÊØîËºÉÔºåÈ©óË≠â‰∫ÜÂÇ≥Áµ±‰ª•‰∫∫È°ûÁÇ∫‰∏≠ÂøÉÁöÑË©ï‰º∞ÊñπÊ≥ïÁöÑÂèØÊì¥ÂÖÖÊõø‰ª£ÊñπÊ°à„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÈõñÁÑ∂ LLM ‰ΩúÁÇ∫Ë©ïÂØ©Êèê‰æõ‰∫ÜËàá‰∫∫È°ûË©ïÂàÜËÄÖÁõ∏Áï∂ÁöÑÂèØÊì¥ÂÖÖËß£Ê±∫ÊñπÊ°àÔºå‰ΩÜ‰∫∫È°ûÂú®Ê™¢Ê∏¨ÂæÆÂ¶ôÁöÑ„ÄÅÁâπÂÆöÊñº‰∏ä‰∏ãÊñáÁöÑÁ¥∞ÂæÆÂ∑ÆÂà•ÊñπÈù¢‰ªçÁÑ∂ÂèØËÉΩË°®ÁèæÂá∫Ëâ≤„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÊúâÂä©ÊñºÊì¥ÂÖÖÈóúÊñº AI ËºîÂä©ÊñáÂ≠óÂàÜÊûêÁöÑÁü•Ë≠òÈ´îÁ≥ª„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂ∞çÊú™‰æÜÁ†îÁ©∂ÁöÑÂª∫Ë≠∞ÔºåÂº∑Ë™øÂú®ÂêÑÁ®ÆËÉåÊôØÂíå‰ΩøÁî®Ê°à‰æã‰∏≠Ê¶ÇÊã¨ LLM ‰ΩúÁÇ∫Ë©ïÂØ©Ê®°ÂûãÊôÇÈúÄË¶Å‰ªîÁ¥∞ËÄÉÈáè„ÄÇ

##### **FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware Classification**
2501.08155v1 by Nurit Cohen-Inger, Lior Rokach, Bracha Shapira, Seffi Cohen

Algorithmic decision-making has become deeply ingrained in many domains, yet
biases in machine learning models can still produce discriminatory outcomes,
often harming unprivileged groups. Achieving fair classification is inherently
challenging, requiring a careful balance between predictive performance and
ethical considerations. We present FairTTTS, a novel post-processing bias
mitigation method inspired by the Tree Test Time Simulation (TTTS) method.
Originally developed to enhance accuracy and robustness against adversarial
inputs through probabilistic decision-path adjustments, TTTS serves as the
foundation for FairTTTS. By building on this accuracy-enhancing technique,
FairTTTS mitigates bias and improves predictive performance. FairTTTS uses a
distance-based heuristic to adjust decisions at protected attribute nodes,
ensuring fairness for unprivileged samples. This fairness-oriented adjustment
occurs as a post-processing step, allowing FairTTTS to be applied to
pre-trained models, diverse datasets, and various fairness metrics without
retraining. Extensive evaluation on seven benchmark datasets shows that
FairTTTS outperforms traditional methods in fairness improvement, achieving a
20.96% average increase over the baseline compared to 18.78% for related work,
and further enhances accuracy by 0.55%. In contrast, competing methods
typically reduce accuracy by 0.42%. These results confirm that FairTTTS
effectively promotes more equitable decision-making while simultaneously
improving predictive performance.

ÊëòË¶ÅÔºöÊºîÁÆóÊ≥ïÊ±∫Á≠ñÂà∂ÂÆöÂ∑≤Ê∑±Ê§çÊñºË®±Â§öÈ†òÂüü‰∏≠ÔºåÁÑ∂ËÄåÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰∏≠ÁöÑÂÅèË¶ã‰ªçÂèØËÉΩÁî¢ÁîüÊ≠ßË¶ñÊÄßÁöÑÁµêÊûúÔºåÈÄöÂ∏∏ÊúÉÂÇ∑ÂÆ≥Êú™Âèó‰øùÈöúÁöÑÁæ§È´î„ÄÇÈÅîÊàêÂÖ¨Âπ≥ÂàÜÈ°ûÊú¨Ë≥™‰∏äÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÈúÄË¶ÅÂú®È†êÊ∏¨ÊïàËÉΩËàáÈÅìÂæ∑ËÄÉÈáè‰πãÈñìÂèñÂæó‰ªîÁ¥∞ÁöÑÂπ≥Ë°°„ÄÇÊàëÂÄëÊèêÂá∫ FairTTTSÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂæåËôïÁêÜÂÅèË™§Á∑©Ëß£ÊñπÊ≥ïÔºåÂÖ∂ÈùàÊÑü‰æÜËá™Ê®πÊ∏¨Ë©¶ÊôÇÈñìÊ®°Êì¨ (TTTS) ÊñπÊ≥ï„ÄÇTTTS ÊúÄÂàùÊòØÁÇ∫‰∫ÜÈÄèÈÅéÊ©üÁéáÊ±∫Á≠ñË∑ØÂæëË™øÊï¥‰æÜÂ¢ûÂº∑ÈáùÂ∞çÂ∞çÊäóËº∏ÂÖ•ÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÁ©©ÂÅ•ÊÄßËÄåÈñãÁôºÔºå‰∏¶‰ΩúÁÇ∫ FairTTTS ÁöÑÂü∫Á§é„ÄÇÈÄèÈÅéÂª∫Á´ãÂú®ÈÄôÁ®ÆÂ¢ûÂº∑Ê∫ñÁ¢∫Â∫¶ÁöÑÊäÄË°ì‰πã‰∏äÔºåFairTTTS ÂèØ‰ª•Ê∏õËºïÂÅèË™§‰∏¶ÊîπÂñÑÈ†êÊ∏¨ÊïàËÉΩ„ÄÇFairTTTS ‰ΩøÁî®Âü∫ÊñºË∑ùÈõ¢ÁöÑÂïüÁôºÊ≥ï‰æÜË™øÊï¥Âèó‰øùË≠∑Â±¨ÊÄßÁØÄÈªûÁöÑÊ±∫Á≠ñÔºåÁ¢∫‰øùÊú™Âèó‰øùÈöúÊ®£Êú¨ÁöÑÂÖ¨Âπ≥ÊÄß„ÄÇÈÄôÁ®Æ‰ª•ÂÖ¨Âπ≥ÊÄßÁÇ∫Â∞éÂêëÁöÑË™øÊï¥ÊúÉÂú®ÂæåËôïÁêÜÊ≠•È©ü‰∏≠ÁôºÁîüÔºåÂÖÅË®± FairTTTS Â•óÁî®Ëá≥È†êÂÖàË®ìÁ∑¥ÁöÑÊ®°Âûã„ÄÅÂ§öÊ®£ÂåñÁöÑË≥áÊñôÈõÜÂíåÂêÑÁ®ÆÂÖ¨Âπ≥ÊÄßÊåáÊ®ôÔºåËÄåÁÑ°ÈúÄÈáçÊñ∞Ë®ìÁ∑¥„ÄÇÂú®‰∏ÉÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õË©ï‰º∞È°ØÁ§∫ÔºåFairTTTS Âú®ÂÖ¨Âπ≥ÊÄßÊîπÂñÑÊñπÈù¢ÂÑ™ÊñºÂÇ≥Áµ±ÊñπÊ≥ïÔºåËàáÁõ∏ÈóúÂ∑•‰ΩúÁöÑ 18.78% Áõ∏ÊØîÔºåÂπ≥ÂùáÊèêÂçá‰∫Ü 20.96%Ôºå‰∏¶ÈÄ≤‰∏ÄÊ≠•Â∞áÊ∫ñÁ¢∫Â∫¶ÊèêÂçá‰∫Ü 0.55%„ÄÇÁõ∏ÂèçÂú∞ÔºåÁ´∂Áà≠ÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÂ∞áÊ∫ñÁ¢∫Â∫¶Èôç‰Ωé 0.42%„ÄÇÈÄô‰∫õÁµêÊûúË≠âÂØ¶ÔºåFairTTTS ÊúâÊïàÂú∞‰øÉÈÄ≤‰∫ÜÊõ¥ÂÖ¨Âπ≥ÁöÑÊ±∫Á≠ñÂà∂ÂÆöÔºåÂêåÊôÇ‰πüÊîπÂñÑ‰∫ÜÈ†êÊ∏¨ÊïàËÉΩ„ÄÇ

##### **Guiding the classification of hepatocellular carcinoma on 3D CT-scans using deep and handcrafted radiological features**
2501.08097v1 by E. Sarfati, A. B√¥ne, M-M. Roh√©, C. Aub√©, M. Ronot, P. Gori, I. Bloch

Hepatocellular carcinoma is the most spread primary liver cancer across the
world ($\sim$80\% of the liver tumors). The gold standard for HCC diagnosis is
liver biopsy. However, in the clinical routine, expert radiologists provide a
visual diagnosis by interpreting hepatic CT-scans according to a standardized
protocol, the LI-RADS, which uses five radiological criteria with an associated
decision tree. In this paper, we propose an automatic approach to predict
histology-proven HCC from CT images in order to reduce radiologists'
inter-variability. We first show that standard deep learning methods fail to
accurately predict HCC from CT-scans on a challenging database, and propose a
two-step approach inspired by the LI-RADS system to improve the performance. We
achieve improvements from 6 to 18 points of AUC with respect to deep learning
baselines trained with different architectures. We also provide clinical
validation of our method, achieving results that outperform non-expert
radiologists and are on par with expert ones.

ÊëòË¶ÅÔºöËÇùÁ¥∞ËÉûÁôåÊòØÊúÄÂ∏∏Ë¶ãÁöÑÂéüÁôºÊÄßËÇùÁôåÔºåÈÅçÂ∏ÉÂÖ®ÁêÉÔºàÁ¥Ñ‰ΩîËÇùËáüËÖ´Áò§ÁöÑ 80%Ôºâ„ÄÇHCC Ë®∫Êñ∑ÁöÑÈªÉÈáëÊ®ôÊ∫ñÊòØËÇùËáüÊ¥ªÊ™¢„ÄÇÁÑ∂ËÄåÔºåÂú®Ëá®Â∫äÂ∏∏Ë¶è‰∏≠ÔºåÂ∞àÂÆ∂ÊîæÂ∞ÑÁßëÈÜ´Â∏´ÊúÉÊ†πÊìöÊ®ôÊ∫ñÂåñÂçîÂÆö LI-RADS ‰æÜËß£ËÆÄËÇùËáüÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÔºåÊèê‰æõË¶ñË¶∫Ë®∫Êñ∑ÔºåÊ≠§ÂçîÂÆö‰ΩøÁî®‰∫îÈ†ÖÊîæÂ∞ÑÂ≠∏Ê®ôÊ∫ñÔºå‰∏¶ÈôÑÊúâÁõ∏ÈóúÊ±∫Á≠ñÊ®π„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆËá™ÂãïÂåñÊñπÊ≥ïÔºåÁî®ÊñºÂæûÈõªËÖ¶Êñ∑Â±§ÂΩ±ÂÉèÈ†êÊ∏¨ÁµÑÁπîÁóÖÁêÜÂ≠∏Ë≠âÂØ¶ÁöÑ HCCÔºå‰ª•Ê∏õÂ∞ëÊîæÂ∞ÑÁßëÈÜ´Â∏´ÁöÑËÆäÁï∞ÊÄß„ÄÇÊàëÂÄëÈ¶ñÂÖàË°®ÊòéÔºåÊ®ôÊ∫ñÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÁÑ°Ê≥ïÊ∫ñÁ¢∫Âú∞ÂæûÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑË≥áÊñôÂ∫´‰∏≠ÁöÑÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÈ†êÊ∏¨ HCCÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèó LI-RADS Á≥ªÁµ±ÂïüÁôºÁöÑÂÖ©Ê≠•È©üÊñπÊ≥ï‰æÜÊîπÂñÑÊïàËÉΩ„ÄÇÁõ∏ËºÉÊñº‰ΩøÁî®‰∏çÂêåÊû∂ÊßãË®ìÁ∑¥ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÂü∫Ê∫ñÔºåÊàëÂÄëÂú® AUC ‰∏≠Áç≤Âæó‰∫Ü 6 Âà∞ 18 ÂÄãÈªûÁöÑÈÄ≤Ê≠•„ÄÇÊàëÂÄë‰πüÊèê‰æõ‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑËá®Â∫äÈ©óË≠âÔºåÊâÄÁç≤ÂæóÁöÑÁµêÊûúÂÑ™ÊñºÈùûÂ∞àÂÆ∂ÊîæÂ∞ÑÁßëÈÜ´Â∏´Ôºå‰∏îËàáÂ∞àÂÆ∂Áõ∏Áï∂„ÄÇ

##### **Exploring visual language models as a powerful tool in the diagnosis of Ewing Sarcoma**
2501.08042v1 by Alvaro Pastor-Naranjo, Pablo Meseguer, Roc√≠o del Amor, Jose Antonio Lopez-Guerrero, Samuel Navarro, Katia Scotlandi, Antonio Llombart-Bosch, Isidro Machado, Valery Naranjo

Ewing's sarcoma (ES), characterized by a high density of small round blue
cells without structural organization, presents a significant health concern,
particularly among adolescents aged 10 to 19. Artificial intelligence-based
systems for automated analysis of histopathological images are promising to
contribute to an accurate diagnosis of ES. In this context, this study explores
the feature extraction ability of different pre-training strategies for
distinguishing ES from other soft tissue or bone sarcomas with similar
morphology in digitized tissue microarrays for the first time, as far as we
know. Vision-language supervision (VLS) is compared to fully-supervised
ImageNet pre-training within a multiple instance learning paradigm. Our
findings indicate a substantial improvement in diagnostic accuracy with the
adaption of VLS using an in-domain dataset. Notably, these models not only
enhance the accuracy of predicted classes but also drastically reduce the
number of trainable parameters and computational costs.

ÊëòË¶ÅÔºöÂ∞§Âõ†Ê∞èËÇâÁò§ (ES) ÁöÑÁâπÂæÅÊòØÈ´òÂØÜÂ∫¶ÁöÑÊó†ÁªìÊûÑÁªÑÁªáÁöÑÂ∞èÂúÜÂΩ¢ËìùËâ≤ÁªÜËÉûÔºåÂØπÂÅ•Â∫∑ÊûÑÊàêÈáçÂ§ßÂ®ÅËÉÅÔºåÂ∞§ÂÖ∂ÊòØÂú® 10 Ëá≥ 19 Â≤ÅÁöÑÈùíÂ∞ëÂπ¥‰∏≠„ÄÇÂü∫‰∫é‰∫∫Â∑•Êô∫ËÉΩÁöÑÁªÑÁªáÁóÖÁêÜÂ≠¶ÂõæÂÉèËá™Âä®ÂàÜÊûêÁ≥ªÁªüÊúâÊúõÊúâÂä©‰∫é ES ÁöÑÂáÜÁ°ÆËØäÊñ≠„ÄÇÂú®Ê≠§ËÉåÊôØ‰∏ãÔºåÊú¨Á†îÁ©∂È¶ñÊ¨°Êé¢ËÆ®‰∫Ü‰∏çÂêåÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÁöÑÁâπÂæÅÊèêÂèñËÉΩÂäõÔºå‰ª•Âå∫ÂàÜ ES ‰∏éÊï∞Â≠óÂåñÁªÑÁªáÂæÆÈòµÂàó‰∏≠ÂΩ¢ÊÄÅÁõ∏‰ººÁöÑÂÖ∂‰ªñËΩØÁªÑÁªáÊàñÈ™®ËÇâÁò§ÔºåÊçÆÊàë‰ª¨ÊâÄÁü•„ÄÇËßÜËßâËØ≠Ë®ÄÁõëÁù£ (VLS) ‰∏éÂ§öÂÆû‰æãÂ≠¶‰π†ËåÉÂºè‰∏≠ÁöÑÂÆåÂÖ®ÁõëÁù£ ImageNet È¢ÑËÆ≠ÁªÉËøõË°å‰∫ÜÊØîËæÉ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®ÂüüÂÜÖÊï∞ÊçÆÈõÜË∞ÉÊï¥ VLS ÂèØÂ§ßÂπÖÊèêÈ´òËØäÊñ≠ÂáÜÁ°ÆÊÄß„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåËøô‰∫õÊ®°Âûã‰∏ç‰ªÖÊèêÈ´ò‰∫ÜÈ¢ÑÊµãÁ±ªÂà´ÁöÑÂáÜÁ°ÆÊÄßÔºåËøòÂ§ßÂπÖÂáèÂ∞ë‰∫ÜÂèØËÆ≠ÁªÉÂèÇÊï∞ÂíåËÆ°ÁÆóÊàêÊú¨„ÄÇ

##### **Comprehensive Metapath-based Heterogeneous Graph Transformer for Gene-Disease Association Prediction**
2501.07970v1 by Wentao Cui, Shoubo Li, Chen Fang, Qingqing Long, Chengrui Wang, Xuezhi Wang, Yuanchun Zhou

Discovering gene-disease associations is crucial for understanding disease
mechanisms, yet identifying these associations remains challenging due to the
time and cost of biological experiments. Computational methods are increasingly
vital for efficient and scalable gene-disease association prediction.
Graph-based learning models, which leverage node features and network
relationships, are commonly employed for biomolecular predictions. However,
existing methods often struggle to effectively integrate node features,
heterogeneous structures, and semantic information. To address these
challenges, we propose COmprehensive MEtapath-based heterogeneous graph
Transformer(COMET) for predicting gene-disease associations. COMET integrates
diverse datasets to construct comprehensive heterogeneous networks,
initializing node features with BioGPT. We define seven Metapaths and utilize a
transformer framework to aggregate Metapath instances, capturing global
contexts and long-distance dependencies. Through intra- and inter-metapath
aggregation using attention mechanisms, COMET fuses latent vectors from
multiple Metapaths to enhance GDA prediction accuracy. Our method demonstrates
superior robustness compared to state-of-the-art approaches. Ablation studies
and visualizations validate COMET's effectiveness, providing valuable insights
for advancing human health research.

ÊëòË¶ÅÔºöÁôºÁèæÂü∫Âõ†ÁñæÁóÖÈóúËÅØÂ∞çÊñºÁêÜËß£ÁñæÁóÖÊ©üÂà∂Ëá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÁî±ÊñºÁîüÁâ©ÂØ¶È©óÁöÑÊôÇÈñìÂíåÊàêÊú¨ÔºåË≠òÂà•ÈÄô‰∫õÈóúËÅØ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇË®àÁÆóÊñπÊ≥ïÂ∞çÊñºÈ´òÊïà‰∏îÂèØÊì¥ÂÖÖÁöÑÂü∫Âõ†ÁñæÁóÖÈóúËÅØÈ†êÊ∏¨Ë∂ä‰æÜË∂äÈáçË¶Å„ÄÇÂü∫ÊñºÂúñÁöÑÂ≠∏ÁøíÊ®°ÂûãÂà©Áî®ÁØÄÈªûÁâπÂæµÂíåÁ∂≤Ë∑ØÈóú‰øÇÔºåÈÄöÂ∏∏Áî®ÊñºÁîüÁâ©ÂàÜÂ≠êÈ†êÊ∏¨„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÈÄöÂ∏∏Èõ£‰ª•ÊúâÊïàÊï¥ÂêàÁØÄÈªûÁâπÂæµ„ÄÅÁï∞Ë≥™ÁµêÊßãÂíåË™ûÁæ©Ë≥áË®ä„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂü∫ÊñºÁ∂úÂêàÂÖÉË∑ØÂæëÁöÑÁï∞Ë≥™ÂúñËΩâÊèõÂô® (COMET)ÔºåÁî®ÊñºÈ†êÊ∏¨Âü∫Âõ†ÁñæÁóÖÈóúËÅØ„ÄÇCOMET Êï¥Âêà‰∫Ü‰∏çÂêåÁöÑË≥áÊñôÈõÜ‰æÜÊßãÂª∫ÂÖ®Èù¢ÁöÑÁï∞Ë≥™Á∂≤Ë∑ØÔºå‰ΩøÁî® BioGPT ÂàùÂßãÂåñÁØÄÈªûÁâπÂæµ„ÄÇÊàëÂÄëÂÆöÁæ©‰∫Ü‰∏ÉÂÄãÂÖÉË∑ØÂæëÔºå‰∏¶Âà©Áî®ËΩâÊèõÂô®Ê°ÜÊû∂‰æÜËÅöÂêàÂÖÉË∑ØÂæëÂØ¶‰æãÔºåÊì∑ÂèñÂÖ®Â±Ä‰∏ä‰∏ãÊñáÂíåÈï∑Ë∑ùÈõ¢‰æùË≥¥Èóú‰øÇ„ÄÇÈÄöÈÅé‰ΩøÁî®Ê≥®ÊÑèÊ©üÂà∂ÈÄ≤Ë°åÂÖÉË∑ØÂæëÂÖßÈÉ®ÂíåÂÖÉË∑ØÂæëÈñìËÅöÂêàÔºåCOMET ËûçÂêà‰∫Ü‰æÜËá™Â§öÂÄãÂÖÉË∑ØÂæëÁöÑÊΩõÂú®ÂêëÈáèÔºå‰ª•Â¢ûÂº∑ GDA È†êÊ∏¨Ê∫ñÁ¢∫ÊÄß„ÄÇËàáÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊ∂àËûçÁ†îÁ©∂ÂíåË¶ñË¶∫ÂåñÈ©óË≠â‰∫Ü COMET ÁöÑÊúâÊïàÊÄßÔºåÁÇ∫Êé®ÈÄ≤‰∫∫È°ûÂÅ•Â∫∑Á†îÁ©∂Êèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£„ÄÇ

##### **Advice for Diabetes Self-Management by ChatGPT Models: Challenges and Recommendations**
2501.07931v1 by Waqar Hussain, John Grundy

Given their ability for advanced reasoning, extensive contextual
understanding, and robust question-answering abilities, large language models
have become prominent in healthcare management research. Despite adeptly
handling a broad spectrum of healthcare inquiries, these models face
significant challenges in delivering accurate and practical advice for chronic
conditions such as diabetes. We evaluate the responses of ChatGPT versions 3.5
and 4 to diabetes patient queries, assessing their depth of medical knowledge
and their capacity to deliver personalized, context-specific advice for
diabetes self-management. Our findings reveal discrepancies in accuracy and
embedded biases, emphasizing the models' limitations in providing tailored
advice unless activated by sophisticated prompting techniques. Additionally, we
observe that both models often provide advice without seeking necessary
clarification, a practice that can result in potentially dangerous advice. This
underscores the limited practical effectiveness of these models without human
oversight in clinical settings. To address these issues, we propose a
commonsense evaluation layer for prompt evaluation and incorporating
disease-specific external memory using an advanced Retrieval Augmented
Generation technique. This approach aims to improve information quality and
reduce misinformation risks, contributing to more reliable AI applications in
healthcare settings. Our findings seek to influence the future direction of AI
in healthcare, enhancing both the scope and quality of its integration.

ÊëòË¶ÅÔºöÁî±ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂÖ∑ÊúâÂÖàÈÄ≤Êé®ÁêÜËÉΩÂäõ„ÄÅÂª£Ê≥õÁöÑËÉåÊôØÁêÜËß£ËÉΩÂäõÂíåÂº∑Â§ßÁöÑÂïèÈ°åÂõûÁ≠îËÉΩÂäõÔºåÂõ†Ê≠§Âú®ÈÜ´ÁôÇ‰øùÂÅ•ÁÆ°ÁêÜÁ†îÁ©∂‰∏≠ËÆäÂæóÁ™ÅÂá∫„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊ®°ÂûãËÉΩÁÜüÁ∑¥Âú∞ËôïÁêÜÂª£Ê≥õÁöÑÈÜ´ÁôÇ‰øùÂÅ•Êü•Ë©¢Ôºå‰ΩÜÂú®Êèê‰æõÊÖ¢ÊÄßÁñæÁóÖÔºà‰æãÂ¶ÇÁ≥ñÂ∞øÁóÖÔºâÁöÑÊ∫ñÁ¢∫‰∏îÂØ¶Áî®ÁöÑÂª∫Ë≠∞ÊñπÈù¢ÔºåÈÄô‰∫õÊ®°ÂûãÈù¢Ëá®ËëóÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü ChatGPT ÁâàÊú¨ 3.5 Âíå 4 Â∞çÁ≥ñÂ∞øÁóÖÊÇ£ËÄÖÊü•Ë©¢ÁöÑÂõûÊáâÔºåË©ï‰º∞‰∫Ü‰ªñÂÄëÁöÑÈÜ´Â≠∏Áü•Ë≠òÊ∑±Â∫¶‰ª•ÂèäÊèê‰æõÈáùÂ∞çÁ≥ñÂ∞øÁóÖËá™ÊàëÁÆ°ÁêÜÁöÑÂÄãÊÄßÂåñ„ÄÅÁâπÂÆöÊñºËÉåÊôØÁöÑÂª∫Ë≠∞ÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÊ∫ñÁ¢∫ÊÄßÂíåÂÖßÂµåÂÅèÂ∑ÆÁöÑÂ∑ÆÁï∞ÔºåÂº∑Ë™ø‰∫ÜÈÄô‰∫õÊ®°ÂûãÂú®Êú™Á∂ìË§áÈõúÊèêÁ§∫ÊäÄË°ìÂïüÁî®ÊôÇÊèê‰æõÂÆöÂà∂Âª∫Ë≠∞ÁöÑÂ±ÄÈôêÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëËßÄÂØüÂà∞ÈÄôÂÖ©ÂÄãÊ®°ÂûãÈÄöÂ∏∏Âú®‰∏çÂ∞ãÊ±ÇÂøÖË¶ÅÁöÑÊæÑÊ∏ÖÁöÑÊÉÖÊ≥Å‰∏ãÊèê‰æõÂª∫Ë≠∞ÔºåÈÄôÁ®ÆÂÅöÊ≥ïÂèØËÉΩÊúÉÂ∞éËá¥ÊΩõÂú®ÁöÑÂç±Èö™Âª∫Ë≠∞„ÄÇÈÄôÂá∏È°Ø‰∫ÜÈÄô‰∫õÊ®°ÂûãÂú®Ê≤íÊúâËá®Â∫äÁí∞Â¢É‰∏≠ÁöÑ‰∫∫Â∑•Áõ£Áù£ÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶Áî®ÊúâÊïàÊÄßÊúâÈôê„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∏∏Ë≠òË©ï‰º∞Â±§ÔºåÁî®ÊñºÊèêÁ§∫Ë©ï‰º∞Âíå‰ΩøÁî®ÂÖàÈÄ≤ÁöÑÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÊäÄË°ìÊï¥ÂêàÁâπÂÆöÁñæÁóÖÁöÑÂ§ñÈÉ®Ë®òÊÜ∂È´î„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÊó®Âú®ÊèêÈ´òË≥áË®äÂìÅË≥™‰∏¶Èôç‰ΩéÈåØË™§Ë≥áË®äÈ¢®Èö™ÔºåÊúâÂä©ÊñºÂú®ÈÜ´ÁôÇ‰øùÂÅ•Áí∞Â¢É‰∏≠Âª∫Á´ãÊõ¥ÂèØÈù†ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊáâÁî®Á®ãÂºè„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊó®Âú®ÂΩ±Èüø‰∫∫Â∑•Êô∫ÊÖßÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊú™‰æÜÊñπÂêëÔºåÂêåÊôÇÊèêÂçáÂÖ∂Êï¥ÂêàÁöÑÁØÑÂúçÂíåÂìÅË≥™„ÄÇ

##### **Evaluating Computational Accuracy of Large Language Models in Numerical Reasoning Tasks for Healthcare Applications**
2501.13936v1 by Arjun R. Malghan

Large Language Models (LLMs) have emerged as transformative tools in the
healthcare sector, demonstrating remarkable capabilities in natural language
understanding and generation. However, their proficiency in numerical
reasoning, particularly in high-stakes domains like in clinical applications,
remains underexplored. Numerical reasoning is critical in healthcare
applications, influencing patient outcomes, treatment planning, and resource
allocation. This study investigates the computational accuracy of LLMs in
numerical reasoning tasks within healthcare contexts. Using a curated dataset
of 1,000 numerical problems, encompassing real-world scenarios such as dosage
calculations and lab result interpretations, the performance of a refined LLM
based on the GPT-3 architecture was evaluated. The methodology includes prompt
engineering, integration of fact-checking pipelines, and application of
regularization techniques to enhance model accuracy and generalization. Key
metrics such as precision, recall, and F1-score were utilized to assess the
model's efficacy. The results indicate an overall accuracy of 84.10%, with
improved performance in straightforward numerical tasks and challenges in
multi-step reasoning. The integration of a fact-checking pipeline improved
accuracy by 11%, underscoring the importance of validation mechanisms. This
research highlights the potential of LLMs in healthcare numerical reasoning and
identifies avenues for further refinement to support critical decision-making
in clinical environments. The findings aim to contribute to the development of
reliable, interpretable, and contextually relevant AI tools for healthcare.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊàêÁÇ∫ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑËÆäÈù©ÊÄßÂ∑•ÂÖ∑ÔºåÂú®Ëá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢Â±ïÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®Êï∏Â≠óÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõÔºåÁâπÂà•ÊòØÂú®Ëá®Â∫äÊáâÁî®Á≠âÈ´òÈ¢®Èö™È†òÂüüÔºå‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊï∏Â≠óÊé®ÁêÜÂú®ÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåÂÆÉÂΩ±ÈüøÊÇ£ËÄÖÁöÑÊ≤ªÁôÇÁµêÊûú„ÄÅÊ≤ªÁôÇË®àÁï´ÂíåË≥áÊ∫êÂàÜÈÖç„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü LLM Âú®ÈÜ´ÁôÇ‰øùÂÅ•ËÉåÊôØ‰∏ãÁöÑÊï∏Â≠óÊé®ÁêÜ‰ªªÂãô‰∏≠ÁöÑË®àÁÆóÊ∫ñÁ¢∫ÊÄß„ÄÇ‰ΩøÁî®‰∏ÄÂÄãÁ≤æÂøÉÊï¥ÁêÜÁöÑÂåÖÂê´ 1,000 ÂÄãÊï∏Â≠óÂïèÈ°åÁöÑÊï∏ÊìöÈõÜÔºåÊ∂µËìãÂäëÈáèË®àÁÆóÂíåÂØ¶È©óÂÆ§ÁµêÊûúËß£ÈáãÁ≠âÁúüÂØ¶Â†¥ÊôØÔºåË©ï‰º∞‰∫ÜÂü∫Êñº GPT-3 Êû∂ÊßãÁöÑÁ≤æÁÖâ LLM ÁöÑÊÄßËÉΩ„ÄÇÊñπÊ≥ïÂåÖÊã¨ÊèêÁ§∫Â∑•Á®ã„ÄÅÊï¥Âêà‰∫ãÂØ¶Êü•Ê†∏ÁÆ°ÈÅìÔºå‰ª•ÂèäÊáâÁî®Ê≠£ÂâáÂåñÊäÄË°ì‰ª•Â¢ûÂº∑Ê®°ÂûãÊ∫ñÁ¢∫ÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÂà©Áî®Á≤æÁ¢∫Â∫¶„ÄÅÂè¨ÂõûÁéáÂíå F1 ÂàÜÊï∏Á≠âÈóúÈçµÊåáÊ®ô‰æÜË©ï‰º∞Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÁµêÊûúË°®ÊòéÊï¥È´îÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 84.10%ÔºåÂú®Áõ¥Êé•Êï∏Â≠ó‰ªªÂãô‰∏≠Ë°®ÁèæËºÉÂ•ΩÔºåËÄåÂú®Â§öÊ≠•È©üÊé®ÁêÜ‰∏≠Â≠òÂú®ÊåëÊà∞„ÄÇÊï¥Âêà‰∫ãÂØ¶Êü•Ê†∏ÁÆ°ÈÅìÂ∞áÊ∫ñÁ¢∫Â∫¶ÊèêÈ´ò‰∫Ü 11%ÔºåÂº∑Ë™ø‰∫ÜÈ©óË≠âÊ©üÂà∂ÁöÑ –≤–∞–∂–Ω–æ—Å—Ç—å„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫Ü LLM Âú®ÈÜ´ÁôÇ‰øùÂÅ•Êï∏Â≠óÊé®ÁêÜ‰∏≠ÁöÑÊΩõÂäõÔºå‰∏¶ÊâæÂá∫ÈÄ≤‰∏ÄÊ≠•ÊîπÈÄ≤ÁöÑÈÄîÂæëÔºå‰ª•ÊîØÊåÅËá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÈóúÈçµÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÁ†îÁ©∂ÁµêÊûúÊó®Âú®ÁÇ∫ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑÂèØÈù†„ÄÅÂèØËß£Èáã‰∏îËàáË™ûÂ¢ÉÁõ∏ÈóúÁöÑ‰∫∫Â∑•Êô∫ÊÖßÂ∑•ÂÖ∑ÁöÑÈñãÁôºÂÅöÂá∫Ë≤¢Áçª„ÄÇ

##### **Large Language Models for Interpretable Mental Health Diagnosis**
2501.07653v1 by Brian Hyeongseok Kim, Chao Wang

We propose a clinical decision support system (CDSS) for mental health
diagnosis that combines the strengths of large language models (LLMs) and
constraint logic programming (CLP). Having a CDSS is important because of the
high complexity of diagnostic manuals used by mental health professionals and
the danger of diagnostic errors. Our CDSS is a software tool that uses an LLM
to translate diagnostic manuals to a logic program and solves the program using
an off-the-shelf CLP engine to query a patient's diagnosis based on the encoded
rules and provided data. By giving domain experts the opportunity to inspect
the LLM-generated logic program, and making modifications when needed, our CDSS
ensures that the diagnosis is not only accurate but also interpretable. We
experimentally compare it with two baseline approaches of using LLMs:
diagnosing patients using the LLM-only approach, and using the LLM-generated
logic program but without expert inspection. The results show that, while LLMs
are extremely useful in generating candidate logic programs, these programs
still require expert inspection and modification to guarantee faithfulness to
the official diagnostic manuals. Additionally, ethical concerns arise from the
direct use of patient data in LLMs, underscoring the need for a safer hybrid
approach like our proposed method.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁî®ÊñºÂøÉÁêÜÂÅ•Â∫∑Ë®∫Êñ∑ÁöÑËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ± (CDSS)ÔºåÂÆÉÁµêÂêà‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁ¥ÑÊùüÈÇèËºØÁ®ãÂºèË®≠Ë®à (CLP) ÁöÑÂÑ™Èªû„ÄÇÊìÅÊúâ CDSS ÂæàÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂøÉÁêÜÂÅ•Â∫∑Â∞àÊ•≠‰∫∫Â£´‰ΩøÁî®ÁöÑË®∫Êñ∑ÊâãÂÜäÈùûÂ∏∏Ë§áÈõúÔºåËÄå‰∏îË®∫Êñ∑ÈåØË™§ÂæàÂç±Èö™„ÄÇÊàëÂÄëÁöÑ CDSS ÊòØ‰∏ÄÂÄãËªüÈ´îÂ∑•ÂÖ∑ÔºåÂÆÉ‰ΩøÁî® LLM Â∞áË®∫Êñ∑ÊâãÂÜäËΩâÊèõÊàêÈÇèËºØÁ®ãÂºèÔºå‰∏¶‰ΩøÁî®ÁèæÊàêÁöÑ CLP ÂºïÊìéËß£Ê±∫Á®ãÂºèÔºå‰ª•Ê†πÊìöÁ∑®Á¢ºË¶èÂâáÂíåÊèê‰æõÁöÑË≥áÊñôÊü•Ë©¢ÁóÖ‰∫∫ÁöÑË®∫Êñ∑„ÄÇÈÄèÈÅéËÆìÈ†òÂüüÂ∞àÂÆ∂ÊúâÊ©üÊúÉÊ™¢Êü• LLM ÁîüÊàêÁöÑÈÇèËºØÁ®ãÂºèÔºå‰∏¶Âú®ÈúÄË¶ÅÊôÇÈÄ≤Ë°å‰øÆÊîπÔºåÊàëÂÄëÁöÑ CDSS ÂèØÁ¢∫‰øùË®∫Êñ∑‰∏çÂÉÖÊ∫ñÁ¢∫ÔºåËÄå‰∏îÂèØËß£ËÆÄ„ÄÇÊàëÂÄë‰ª•ÂØ¶È©óÁöÑÊñπÂºèÂ∞áÂÖ∂ËàáÂÖ©Á®Æ‰ΩøÁî® LLM ÁöÑÂü∫Á∑öÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉÔºöÂÉÖ‰ΩøÁî® LLM ÊñπÊ≥ïË®∫Êñ∑ÁóÖ‰∫∫Ôºå‰ª•Âèä‰ΩøÁî® LLM ÁîüÊàêÁöÑÈÇèËºØÁ®ãÂºèÔºå‰ΩÜÊ≤íÊúâÂ∞àÂÆ∂Ê™¢Êü•„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÈõñÁÑ∂ LLM Âú®Áî¢ÁîüÂÄôÈÅ∏ÈÇèËºØÁ®ãÂºèÊñπÈù¢ÈùûÂ∏∏ÊúâÁî®Ôºå‰ΩÜÈÄô‰∫õÁ®ãÂºè‰ªçÁÑ∂ÈúÄË¶ÅÂ∞àÂÆ∂Ê™¢Êü•Âíå‰øÆÊîπÔºå‰ª•Á¢∫‰øùÂ∞çÂÆòÊñπË®∫Êñ∑ÊâãÂÜäÁöÑÂø†ÂØ¶Â∫¶„ÄÇÊ≠§Â§ñÔºåÁõ¥Êé•Âú® LLM ‰∏≠‰ΩøÁî®ÁóÖ‰∫∫Ë≥áÊñôÊúÉÂºïÁôºÂÄ´ÁêÜÂïèÈ°åÔºåÈÄôÂº∑Ë™ø‰∫ÜÈúÄË¶Å‰∏ÄÁ®ÆÊõ¥ÂÆâÂÖ®ÁöÑÊ∑∑ÂêàÊñπÊ≥ïÔºå‰æãÂ¶ÇÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ï„ÄÇ</paragraph>

##### **RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment**
2501.07525v1 by Difei Gu, Yunhe Gao, Yang Zhou, Mu Zhou, Dimitris Metaxas

Automated chest radiographs interpretation requires both accurate disease
classification and detailed radiology report generation, presenting a
significant challenge in the clinical workflow. Current approaches either focus
on classification accuracy at the expense of interpretability or generate
detailed but potentially unreliable reports through image captioning
techniques. In this study, we present RadAlign, a novel framework that combines
the predictive accuracy of vision-language models (VLMs) with the reasoning
capabilities of large language models (LLMs). Inspired by the radiologist's
workflow, RadAlign first employs a specialized VLM to align visual features
with key medical concepts, achieving superior disease classification with an
average AUC of 0.885 across multiple diseases. These recognized medical
conditions, represented as text-based concepts in the aligned visual-language
space, are then used to prompt LLM-based report generation. Enhanced by a
retrieval-augmented generation mechanism that grounds outputs in similar
historical cases, RadAlign delivers superior report quality with a GREEN score
of 0.678, outperforming state-of-the-art methods' 0.634. Our framework
maintains strong clinical interpretability while reducing hallucinations,
advancing automated medical imaging and report analysis through integrated
predictive and generative AI. Code is available at
https://github.com/difeigu/RadAlign.

ÊëòË¶ÅÔºöËá™ÂãïÂåñËÉ∏ÈÉ® X ÂÖâÁâáËß£ËÆÄÈúÄË¶ÅÁ≤æÊ∫ñÁöÑÁñæÁóÖÂàÜÈ°ûÂíåË©≥Á¥∞ÁöÑÊîæÂ∞ÑÁßëÂ†±ÂëäÁîüÊàêÔºåÈÄôÂ∞çËá®Â∫äÂ∑•‰ΩúÊµÅÁ®ãÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÁõÆÂâçÁöÑÂÅöÊ≥ïË¶Å‰∏çÂ∞±ÊòØ‰ª•ÁäßÁâ≤ÂèØËß£ËÆÄÊÄßÁÇ∫‰ª£ÂÉπÂ∞àÊ≥®ÊñºÂàÜÈ°ûÊ∫ñÁ¢∫ÊÄßÔºåË¶Å‰∏çÂ∞±ÊòØÈÄèÈÅéÂΩ±ÂÉèÊ®ôÈ°åÊäÄË°ìÁî¢ÁîüË©≥Á¥∞‰ΩÜÂèØËÉΩ‰∏çÂèØÈù†ÁöÑÂ†±Âëä„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ RadAlignÔºå‰∏ÄÂÄãÁµêÂêà‰∫ÜË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜËÉΩÂäõÁöÑÊñ∞Á©éÊû∂Êßã„ÄÇÂèóÂà∞ÊîæÂ∞ÑÁßëÈÜ´Â∏´Â∑•‰ΩúÊµÅÁ®ãÁöÑÂïüÁôºÔºåRadAlign È¶ñÂÖàÊé°Áî®Â∞àÈñÄÁöÑ VLM Â∞áË¶ñË¶∫ÁâπÂæµËàáÈóúÈçµÈÜ´ÁôÇÊ¶ÇÂøµÂ∞çÈΩäÔºåÂú®Â§öÁ®ÆÁñæÁóÖ‰∏≠ÈÅîÊàêÂÑ™Áï∞ÁöÑÁñæÁóÖÂàÜÈ°ûÔºåÂπ≥Âùá AUC ÁÇ∫ 0.885„ÄÇÈÄô‰∫õË≠òÂà•Âá∫ÁöÑÈÜ´ÁôÇÁãÄÊ≥ÅÊúÉÂú®Â∞çÈΩäÁöÑË¶ñË¶∫Ë™ûË®ÄÁ©∫Èñì‰∏≠Ë°®Á§∫ÁÇ∫Âü∫ÊñºÊñáÂ≠óÁöÑÊ¶ÇÂøµÔºåÁÑ∂ÂæåÁî®‰æÜÊèêÁ§∫Âü∫Êñº LLM ÁöÑÂ†±ÂëäÁîüÊàê„ÄÇÈÄèÈÅé‰∏ÄÁ®ÆÂ∞áËº∏Âá∫ÁµêÊûúÂª∫Á´ãÂú®È°û‰ººÈÅéÂæÄÊ°à‰æã‰∏≠ÁöÑÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÊ©üÂà∂ÔºåRadAlign Êèê‰æõÂÑ™Áï∞ÁöÑÂ†±ÂëäÂìÅË≥™ÔºåGREEN ÂàÜÊï∏ÁÇ∫ 0.678ÔºåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÊñπÊ≥ïÁöÑ 0.634„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÁ∂≠ÊåÅÂº∑Â§ßÁöÑËá®Â∫äÂèØËß£ËÆÄÊÄßÔºåÂêåÊôÇÊ∏õÂ∞ëÂπªË¶∫ÔºåÈÄèÈÅéÊï¥ÂêàÈ†êÊ∏¨ÂíåÁîüÊàêÂºè AIÔºåÊé®ÈÄ≤Ëá™ÂãïÂåñÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÂ†±ÂëäÂàÜÊûê„ÄÇÁ®ãÂºèÁ¢ºÂèØÊñº https://github.com/difeigu/RadAlign ÂèñÂæó„ÄÇ

##### **A Survey of Embodied AI in Healthcare: Techniques, Applications, and Opportunities**
2501.07468v1 by Yihao Liu, Xu Cao, Tingting Chen, Yankai Jiang, Junjie You, Minghua Wu, Xiaosong Wang, Mengling Feng, Yaochu Jin, Jintai Chen

Healthcare systems worldwide face persistent challenges in efficiency,
accessibility, and personalization. Powered by modern AI technologies such as
multimodal large language models and world models, Embodied AI (EmAI)
represents a transformative frontier, offering enhanced autonomy and the
ability to interact with the physical world to address these challenges. As an
interdisciplinary and rapidly evolving research domain, "EmAI in healthcare"
spans diverse fields such as algorithms, robotics, and biomedicine. This
complexity underscores the importance of timely reviews and analyses to track
advancements, address challenges, and foster cross-disciplinary collaboration.
In this paper, we provide a comprehensive overview of the "brain" of EmAI for
healthcare, wherein we introduce foundational AI algorithms for perception,
actuation, planning, and memory, and focus on presenting the healthcare
applications spanning clinical interventions, daily care & companionship,
infrastructure support, and biomedical research. Despite its promise, the
development of EmAI for healthcare is hindered by critical challenges such as
safety concerns, gaps between simulation platforms and real-world applications,
the absence of standardized benchmarks, and uneven progress across
interdisciplinary domains. We discuss the technical barriers and explore
ethical considerations, offering a forward-looking perspective on the future of
EmAI in healthcare. A hierarchical framework of intelligent levels for EmAI
systems is also introduced to guide further development. By providing
systematic insights, this work aims to inspire innovation and practical
applications, paving the way for a new era of intelligent, patient-centered
healthcare.

ÊëòË¶ÅÔºö<paragraph>ÂÖ®ÁêÉÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±Âú®ÊïàÁéá„ÄÅÂèØÂèäÊÄßÂíåÂÄã‰∫∫ÂåñÊñπÈù¢ÊåÅÁ∫åÈù¢Ëá®ÊåëÊà∞„ÄÇÈ´îÁèæÂºè AI (EmAI) Áî±Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂíå‰∏ñÁïåÊ®°ÂûãÁ≠âÁèæ‰ª£ AI ÊäÄË°ìÊèê‰æõÊîØÊåÅÔºå‰ª£Ë°®‰∫Ü‰∏ÄÂÄãËΩâÂûãÂâçÊ≤øÔºåÊèê‰æõÂ¢ûÂº∑ÁöÑËá™‰∏ªÊÄßÔºå‰ª•ÂèäËàáÁâ©ÁêÜ‰∏ñÁïå‰∫íÂãï‰ª•ÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÁöÑËÉΩÂäõ„ÄÇ‰ΩúÁÇ∫‰∏ÄÂÄãË∑®Â≠∏Áßë‰∏îÂø´ÈÄüÁôºÂ±ïÁöÑÁ†îÁ©∂È†òÂüüÔºå„ÄåÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ EmAI„ÄçÊ∂µËìã‰∫ÜÊºîÁÆóÊ≥ï„ÄÅÊ©üÂô®‰∫∫ÂíåÁîüÁâ©ÈÜ´Â≠∏Á≠âÂ§öÂÖÉÈ†òÂüü„ÄÇÈÄôÁ®ÆË§áÈõúÊÄßÁ™ÅÈ°Ø‰∫ÜÂèäÊôÇÂØ©Êü•ÂíåÂàÜÊûêÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ËøΩËπ§ÈÄ≤Â±ï„ÄÅÊáâÂ∞çÊåëÊà∞‰∏¶‰øÉÈÄ≤Ë∑®Â≠∏ÁßëÂêà‰Ωú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèê‰æõ‰∫Ü EmAI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ„ÄåÂ§ßËÖ¶„ÄçÁöÑÂÖ®Èù¢Ê¶ÇËø∞ÔºåÊàëÂÄëÂú®ÂÖ∂‰∏≠‰ªãÁ¥π‰∫ÜÊÑüÁü•„ÄÅÂü∑Ë°å„ÄÅË¶èÂäÉÂíåË®òÊÜ∂ÁöÑÂü∫Êú¨ AI ÊºîÁÆóÊ≥ïÔºå‰∏¶Â∞àÊ≥®ÊñºÂëàÁèæÊ∂µËìãËá®Â∫äÂπ≤È†ê„ÄÅÊó•Â∏∏ÁÖßË≠∑ÂíåÈô™‰º¥„ÄÅÂü∫Á§éË®≠ÊñΩÊîØÊè¥ÂíåÁîüÁâ©ÈÜ´Â≠∏Á†îÁ©∂ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®„ÄÇÂÑòÁÆ°ÂâçÊôØÁúãÂ•ΩÔºå‰ΩÜ EmAI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÁôºÂ±ïÂèóÂà∞ÈóúÈçµÊåëÊà∞ÁöÑÈòªÁ§ôÔºå‰æãÂ¶ÇÂÆâÂÖ®ÂïèÈ°å„ÄÅÊ®°Êì¨Âπ≥Âè∞ÂíåÂØ¶ÈöõÊáâÁî®‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÅÁº∫‰πèÊ®ôÊ∫ñÂåñÂü∫Ê∫ñÔºå‰ª•ÂèäË∑®Â≠∏ÁßëÈ†òÂüüÈÄ≤Â±ï‰∏çÂùá„ÄÇÊàëÂÄëË®éË´ñ‰∫ÜÊäÄË°ìÈöúÁ§ô‰∏¶Êé¢Ë®é‰∫ÜÈÅìÂæ∑ËÄÉÈáèÔºåÂ∞ç EmAI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊú™‰æÜÊèê‰æõ‰∫ÜÂâçÁûªÊÄßÁöÑËßÄÈªû„ÄÇÈÇÑÂºïÂÖ•‰∫Ü EmAI Á≥ªÁµ±ÁöÑÊô∫ÊÖßÂ±§Á¥öÊû∂ÊßãÔºå‰ª•ÊåáÂ∞éÈÄ≤‰∏ÄÊ≠•ÁöÑÁôºÂ±ï„ÄÇÈÄèÈÅéÊèê‰æõÁ≥ªÁµ±ÊÄßÁöÑË¶ãËß£ÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊó®Âú®ÊøÄÁôºÂâµÊñ∞ÂíåÂØ¶Áî®ÊáâÁî®ÔºåÁÇ∫Êô∫ÊÖß‰∏î‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÈÜ´ÁôÇ‰øùÂÅ•Êñ∞ÊôÇ‰ª£Èã™Ë∑Ø„ÄÇ</paragraph>

##### **Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for Volume-to-Volume Medical Image Translation**
2501.07430v1 by Xiyue Zhu, Dou Hoon Kwark, Ruike Zhu, Kaiwen Hong, Yiqi Tao, Shirui Luo, Yudu Li, Zhi-Pei Liang, Volodymyr Kindratenko

Despite success in volume-to-volume translations in medical images, most
existing models struggle to effectively capture the inherent volumetric
distribution using 3D representations. The current state-of-the-art approach
combines multiple 2D-based networks through weighted averaging, thereby
neglecting the 3D spatial structures. Directly training 3D models in medical
imaging presents significant challenges due to high computational demands and
the need for large-scale datasets. To address these challenges, we introduce
Diff-Ensembler, a novel hybrid 2D-3D model for efficient and effective
volumetric translations by ensembling perpendicularly trained 2D diffusion
models with a 3D network in each diffusion step. Moreover, our model can
naturally be used to ensemble diffusion models conditioned on different
modalities, allowing flexible and accurate fusion of input conditions.
Extensive experiments demonstrate that Diff-Ensembler attains superior accuracy
and volumetric realism in 3D medical image super-resolution and modality
translation. We further demonstrate the strength of our model's volumetric
realism using tumor segmentation as a downstream task.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠È´îÁ©çÂà∞È´îÁ©çÁöÑÁøªË≠ØÂèñÂæóÊàêÂäüÔºå‰ΩÜÁèæÊúâÁöÑÊ®°ÂûãÂ§ßÂ§öÈõ£‰ª•ÊúâÊïàÂú∞‰ΩøÁî® 3D ÂëàÁèæ‰æÜÊì∑ÂèñÂõ∫ÊúâÁöÑÈ´îÁ©çÂàÜ‰Ωà„ÄÇÁõÆÂâçÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÊòØÈÄèÈÅéÂä†Ê¨äÂπ≥Âùá‰æÜÁµêÂêàÂ§öÂÄãÂü∫Êñº 2D ÁöÑÁ∂≤Ë∑ØÔºåÂõ†Ê≠§ÂøΩÁï•‰∫Ü 3D Á©∫ÈñìÁµêÊßã„ÄÇÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠Áõ¥Êé•Ë®ìÁ∑¥ 3D Ê®°ÂûãÊúÉÁî¢ÁîüÈ°ØËëóÁöÑÊåëÊà∞ÔºåÂéüÂõ†Âú®ÊñºÈ´òÈÅãÁÆóÈúÄÊ±ÇÂíåÂ§ßË¶èÊ®°Ë≥áÊñôÈõÜÁöÑÈúÄÊ±Ç„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü Diff-EnsemblerÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ∑∑Âêà 2D-3D Ê®°ÂûãÔºåÂèØÈÄèÈÅéÂú®ÊØèÂÄãÊì¥Êï£Ê≠•È©ü‰∏≠Â∞áÂûÇÁõ¥Ë®ìÁ∑¥ÁöÑ 2D Êì¥Êï£Ê®°ÂûãËàá 3D Á∂≤Ë∑ØÁµêÂêàÔºå‰æÜÊúâÊïàÁéá‰∏îÊúâÊïàÂú∞ÈÄ≤Ë°åÈ´îÁ©çËΩâÊèõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•Ëá™ÁÑ∂Âú∞Áî®ÊñºÁµêÂêàÂü∫Êñº‰∏çÂêåÂΩ¢ÂºèÁöÑÊì¥Êï£Ê®°ÂûãÔºåÂæûËÄåÈùàÊ¥ª‰∏îÊ∫ñÁ¢∫Âú∞ËûçÂêàËº∏ÂÖ•Ê¢ù‰ª∂„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåDiff-Ensembler Âú® 3D ÈÜ´Â≠∏ÂΩ±ÂÉèË∂ÖËß£ÊûêÂ∫¶ÂíåÂΩ¢ÂºèËΩâÊèõ‰∏≠ÈÅîÂà∞‰∫ÜÊõ¥È´òÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÈ´îÁ©çÁúüÂØ¶ÊÑü„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®ËÖ´Áò§ÂàÜÂâ≤‰ΩúÁÇ∫‰∏ãÊ∏∏‰ªªÂãôÔºå‰æÜË≠âÊòéÊàëÂÄëÊ®°ÂûãÁöÑÈ´îÁ©çÁúüÂØ¶ÊÑü„ÄÇ

##### **Synthetic Data and Health Privacy**
2501.09031v1 by Gw√©nol√© Abgrall, Xavier Monnet, Anmol Arora

This Viewpoint discusses generative artificial intelligence and safeguarding
privacy by using synthetic data as a substitute for private health data.

ÊëòË¶ÅÔºöÊ≠§ËßÄÈªûÊé¢Ë®éÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖß‰ª•Âèä‰ΩøÁî®ÂêàÊàêË≥áÊñôÂèñ‰ª£ÁßÅ‰∫∫ÂÅ•Â∫∑Ë≥áÊñô‰ª•‰øùË≠∑Èö±ÁßÅ„ÄÇ

##### **Natural Language-Assisted Multi-modal Medication Recommendation**
2501.07166v1 by Jie Tan, Yu Rong, Kangfei Zhao, Tian Bian, Tingyang Xu, Junzhou Huang, Hong Cheng, Helen Meng

Combinatorial medication recommendation(CMR) is a fundamental task of
healthcare, which offers opportunities for clinical physicians to provide more
precise prescriptions for patients with intricate health conditions,
particularly in the scenarios of long-term medical care. Previous research
efforts have sought to extract meaningful information from electronic health
records (EHRs) to facilitate combinatorial medication recommendations. Existing
learning-based approaches further consider the chemical structures of
medications, but ignore the textual medication descriptions in which the
functionalities are clearly described. Furthermore, the textual knowledge
derived from the EHRs of patients remains largely underutilized. To address
these issues, we introduce the Natural Language-Assisted Multi-modal Medication
Recommendation(NLA-MMR), a multi-modal alignment framework designed to learn
knowledge from the patient view and medication view jointly. Specifically,
NLA-MMR formulates CMR as an alignment problem from patient and medication
modalities. In this vein, we employ pretrained language models(PLMs) to extract
in-domain knowledge regarding patients and medications, serving as the
foundational representation for both modalities. In the medication modality, we
exploit both chemical structures and textual descriptions to create medication
representations. In the patient modality, we generate the patient
representations based on textual descriptions of diagnosis, procedure, and
symptom. Extensive experiments conducted on three publicly accessible datasets
demonstrate that NLA-MMR achieves new state-of-the-art performance, with a
notable average improvement of 4.72% in Jaccard score. Our source code is
publicly available on https://github.com/jtan1102/NLA-MMR_CIKM_2024.

ÊëòË¶ÅÔºöÁµÑÂêàÂºèËó•Áâ©Êé®Ëñ¶ (CMR) ÊòØÈÜ´ÁôÇ‰øùÂÅ•ÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨‰ªªÂãôÔºåÂÆÉÁÇ∫Ëá®Â∫äÈÜ´ÁîüÊèê‰æõ‰∫ÜÈáùÂ∞çÂÖ∑ÊúâË§áÈõúÂÅ•Â∫∑ÁãÄÊ≥ÅÁöÑÊÇ£ËÄÖÊèê‰æõÊõ¥Á≤æÁ¢∫ËôïÊñπÁöÑÊ©üÊúÉÔºåÁâπÂà•ÊòØÂú®Èï∑ÊúüÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∑•‰ΩúË©¶ÂúñÂæûÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑ (EHR) ‰∏≠ÊèêÂèñÊúâÊÑèÁæ©ÁöÑË≥áË®äÔºå‰ª•‰øÉÈÄ≤ÁµÑÂêàÂºèËó•Áâ©Êé®Ëñ¶„ÄÇÁèæÊúâÁöÑÂü∫ÊñºÂ≠∏ÁøíÁöÑÊñπÊ≥ïÈÄ≤‰∏ÄÊ≠•ËÄÉÊÖÆ‰∫ÜËó•Áâ©ÁöÑÂåñÂ≠∏ÁµêÊßãÔºå‰ΩÜÂøΩÁï•‰∫ÜÂäüËÉΩÊ∏ÖÊ•öÊèèËø∞ÊñºÂÖ∂‰∏≠ÁöÑÊñáÊú¨Ëó•Áâ©Ë™™Êòé„ÄÇÊ≠§Â§ñÔºåÂæûÊÇ£ËÄÖÁöÑ EHR ‰∏≠Ë°çÁîüÁöÑÊñáÊú¨Áü•Ë≠òÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÂà©Áî®„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜËá™ÁÑ∂Ë™ûË®ÄËºîÂä©Â§öÊ®°ÂºèËó•Áâ©Êé®Ëñ¶ (NLA-MMR)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§öÊ®°ÂºèÂ∞çÈΩäÊ°ÜÊû∂ÔºåÊó®Âú®ÂæûÊÇ£ËÄÖË¶ñËßíÂíåËó•Áâ©Ë¶ñËßíÂÖ±ÂêåÂ≠∏ÁøíÁü•Ë≠ò„ÄÇÂÖ∑È´î‰æÜË™™ÔºåNLA-MMR Â∞á CMR ÊßãÂª∫ÁÇ∫ÊÇ£ËÄÖÂíåËó•Áâ©Ê®°ÂºèÁöÑÂ∞çÈΩäÂïèÈ°å„ÄÇÂú®Ê≠§ËÑàÁµ°‰∏≠ÔºåÊàëÂÄëÊé°Áî®È†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) ‰æÜÊèêÂèñÊúâÈóúÊÇ£ËÄÖÂíåËó•Áâ©ÁöÑÈ†òÂüüÂÖßÁü•Ë≠òÔºå‰ΩúÁÇ∫ÈÄôÂÖ©Á®ÆÊ®°ÂºèÁöÑÂü∫Êú¨Ë°®Á§∫„ÄÇÂú®Ëó•Áâ©Ê®°Âºè‰∏≠ÔºåÊàëÂÄëÂà©Áî®ÂåñÂ≠∏ÁµêÊßãÂíåÊñáÊú¨Ë™™Êòé‰æÜÂª∫Á´ãËó•Áâ©Ë°®Á§∫„ÄÇÂú®ÊÇ£ËÄÖÊ®°Âºè‰∏≠ÔºåÊàëÂÄëÊ†πÊìöË®∫Êñ∑„ÄÅÁ®ãÂ∫èÂíåÁóáÁãÄÁöÑÊñáÂ≠óË™™Êòé‰æÜÁîüÊàêÊÇ£ËÄÖË°®Á§∫„ÄÇÂú®‰∏âÂÄãÂÖ¨ÈñãÂ≠òÂèñÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË°®ÊòéÔºåNLA-MMR ÈÅîÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàÈÄ≤ÊïàËÉΩÔºåÂÇëÂç°Âæ∑ÊåáÊï∏Âπ≥ÂùáÊîπÈÄ≤‰∫Ü 4.72%„ÄÇÊàëÂÄëÁöÑÂéüÂßãÁ¢ºÂÖ¨ÈñãÊñº https://github.com/jtan1102/NLA-MMR_CIKM_2024„ÄÇ

##### **CureGraph: Contrastive Multi-Modal Graph Representation Learning for Urban Living Circle Health Profiling and Prediction**
2501.07157v1 by Jinlin Li, Xiao Zhou

The early detection and prediction of health status decline among the elderly
at the neighborhood level are of great significance for urban planning and
public health policymaking. While existing studies affirm the connection
between living environments and health outcomes, most rely on single data
modalities or simplistic feature concatenation of multi-modal information,
limiting their ability to comprehensively profile the health-oriented urban
environments. To fill this gap, we propose CureGraph, a contrastive multi-modal
representation learning framework for urban health prediction that employs
graph-based techniques to infer the prevalence of common chronic diseases among
the elderly within the urban living circles of each neighborhood. CureGraph
leverages rich multi-modal information, including photos and textual reviews of
residential areas and their surrounding points of interest, to generate urban
neighborhood embeddings. By integrating pre-trained visual and textual encoders
with graph modeling techniques, CureGraph captures cross-modal spatial
dependencies, offering a comprehensive understanding of urban environments
tailored to elderly health considerations. Extensive experiments on real-world
datasets demonstrate that CureGraph improves the best baseline by $28\%$ on
average in terms of $R^2$ across elderly disease risk prediction tasks.
Moreover, the model enables the identification of stage-wise chronic disease
progression and supports comparative public health analysis across
neighborhoods, offering actionable insights for sustainable urban development
and enhanced quality of life. The code is publicly available at
https://github.com/jinlin2021/CureGraph.

ÊëòË¶ÅÔºöÂú®ÈÑ∞ÈáåÂ±§Á¥öÊó©ÊúüÂÅµÊ∏¨ÂíåÈ†êÊ∏¨ËÄÅÂπ¥‰∫∫ÁöÑÂÅ•Â∫∑ÁãÄÊ≥Å‰∏ãÈôçÂ∞çÂüéÂ∏ÇË¶èÂäÉÂíåÂÖ¨ÂÖ±Ë°õÁîüÊîøÁ≠ñÂà∂ÂÆöÂÖ∑ÊúâÈáçÂ§ßÊÑèÁæ©„ÄÇÂÑòÁÆ°ÁèæÊúâÁ†îÁ©∂ËÇØÂÆö‰∫ÜÁîüÊ¥ªÁí∞Â¢ÉËàáÂÅ•Â∫∑ÁµêÊûú‰πãÈñìÁöÑÈóúËÅØÊÄßÔºå‰ΩÜÂ§ßÂ§ö‰æùË≥¥ÂñÆ‰∏ÄË≥áÊñôÊ®°ÂºèÊàñÂ§öÊ®°ÂºèË≥áË®äÁöÑÁ∞°ÂåñÁâπÂæµ‰∏≤Êé•ÔºåÈôêÂà∂‰∫Ü‰ªñÂÄëÂÖ®Èù¢ÊèèÁπ™‰ª•ÂÅ•Â∫∑ÁÇ∫Â∞éÂêëÁöÑÂüéÂ∏ÇÁí∞Â¢ÉÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü CureGraphÔºå‰∏ÄÂÄãÁî®ÊñºÂüéÂ∏ÇÂÅ•Â∫∑È†êÊ∏¨ÁöÑÂ∞çÊØîÂºèÂ§öÊ®°ÂºèË°®Á§∫Â≠∏ÁøíÊû∂ÊßãÔºåÂÆÉÊé°Áî®Âü∫ÊñºÂúñÂΩ¢ÊäÄË°ì‰æÜÊé®Ë´ñÊØèÂÄãÈÑ∞ÈáåÂüéÂ∏ÇÁîüÊ¥ªÂúà‰∏≠ËÄÅÂπ¥‰∫∫Â∏∏Ë¶ãÊÖ¢ÊÄßÁñæÁóÖÁöÑÊµÅË°åÁéá„ÄÇCureGraph Âà©Áî®Ë±êÂØåÁöÑÂ§öÊ®°ÂºèË≥áË®äÔºåÂåÖÊã¨‰ΩèÂÆÖÂçÄÂèäÂÖ∂Âë®ÂúçÊôØÈªûÁöÑÁÖßÁâáÂíåÊñáÂ≠óË©ïË´ñÔºå‰æÜÁî¢ÁîüÂüéÂ∏ÇÈÑ∞ÈáåÂµåÂÖ•„ÄÇÈÄèÈÅéÊï¥ÂêàÈ†êÂÖàË®ìÁ∑¥ÁöÑË¶ñË¶∫ÂíåÊñáÂ≠óÁ∑®Á¢ºÂô®ËàáÂúñÂΩ¢Âª∫Ê®°ÊäÄË°ìÔºåCureGraph ÊçïÊçâË∑®Ê®°ÂºèÁ©∫Èñì‰æùË≥¥ÊÄßÔºåÊèê‰æõÂ∞çÂüéÂ∏ÇÁí∞Â¢ÉÁöÑÂÖ®Èù¢ÁêÜËß£ÔºåÂ∞àÈñÄÈáùÂ∞çËÄÅÂπ¥‰∫∫ÁöÑÂÅ•Â∫∑ËÄÉÈáè„ÄÇÂú®ÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåCureGraph Âú®ËÄÅÂπ¥‰∫∫ÁñæÁóÖÈ¢®Èö™È†êÊ∏¨‰ªªÂãô‰∏≠ÔºåÂπ≥ÂùáÂú® R2 ÊñπÈù¢Â∞áÊúÄ‰Ω≥Âü∫Ê∫ñÁ∑öÊèêÈ´ò‰∫Ü 28%„ÄÇÊ≠§Â§ñÔºåË©≤Ê®°ÂûãËÉΩÂ§†Ë≠òÂà•ÈöéÊÆµÊÄßÁöÑÊÖ¢ÊÄßÁñæÁóÖÈÄ≤Á®ãÔºå‰∏¶ÊîØÊè¥Ë∑®ÈÑ∞ÈáåÁöÑÊØîËºÉÂÖ¨ÂÖ±Ë°õÁîüÂàÜÊûêÔºåÁÇ∫Ê∞∏Á∫åÁöÑÂüéÂ∏ÇÁôºÂ±ïÂíåÊèêÂçáÁîüÊ¥ªÂìÅË≥™Êèê‰æõÂèØË°åÁöÑË¶ãËß£„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº https://github.com/jinlin2021/CureGraph„ÄÇ

##### **UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN Powered Vision-LSTM**
2501.07017v2 by Xuhui Guo, Tanmoy Dam, Rohan Dhamdhere, Gourav Modanwal, Anant Madabhushi

3D medical image segmentation has progressed considerably due to
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these
methods struggle to balance long-range dependency acquisition with
computational efficiency. To address this challenge, we propose UNETVL (U-Net
Vision-LSTM), a novel architecture that leverages recent advancements in
temporal information processing. UNETVL incorporates Vision-LSTM (ViL) for
improved scalability and memory functions, alongside an efficient Chebyshev
Kolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency
patterns more effectively. We validated our method on the ACDC and AMOS2022
(post challenge Task 2) benchmark datasets, showing a significant improvement
in mean Dice score compared to recent state-of-the-art approaches, especially
over its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS,
respectively. Extensive ablation studies were conducted to demonstrate the
impact of each component in UNETVL, providing a comprehensive understanding of
its architecture. Our code is available at https://github.com/tgrex6/UNETVL,
facilitating further research and applications in this domain.

ÊëòË¶ÅÔºö3D ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Áî±ÊñºÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåË¶ñË¶∫Transformer (ViT) ËÄåÈÄ≤Ê≠•Ë®±Â§öÔºåÁÑ∂ËÄåÈÄô‰∫õÊñπÊ≥ïÈõ£‰ª•Âπ≥Ë°°Èï∑Á®ã‰æùË≥¥Èóú‰øÇÊì∑ÂèñËàáÈÅãÁÆóÊïàÁéá„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÂÄãÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫ UNETVL (U-Net Ë¶ñË¶∫ LSTM)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊû∂ÊßãÔºåÂÆÉÂà©Áî®ÊôÇÈñìË≥áË®äËôïÁêÜÁöÑÊúÄÊñ∞ÈÄ≤Â±ï„ÄÇUNETVL ÁµêÂêàË¶ñË¶∫ LSTM (ViL) ‰ª•ÊèêÂçáÂèØÊì¥ÂÖÖÊÄßÂíåË®òÊÜ∂ÂäüËÉΩÔºå‰∏¶ÁµêÂêàÈ´òÊïàÁöÑÂàáÊØîÈõ™Â§´ Kolmogorov-Arnold Á∂≤Ë∑Ø (KAN) ‰ª•Êõ¥ÊúâÊïàÁéáÂú∞ËôïÁêÜË§áÈõú‰∏îÈï∑Á®ãÁöÑ‰æùË≥¥Èóú‰øÇÊ®°Âºè„ÄÇÊàëÂÄëÂú® ACDC Âíå AMOS2022ÔºàÊåëÊà∞‰ªªÂãô 2 ‰πãÂæåÔºâÂü∫Ê∫ñË≥áÊñôÈõÜÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÔºåËàáÊúÄËøëÁöÑÊúÄÊñ∞ÊäÄË°ìÊñπÊ≥ïÁõ∏ÊØîÔºåÂπ≥Âùá Dice ÂàÜÊï∏ÊúâÈ°ØËëóÊèêÂçáÔºåÁâπÂà•ÊòØËàáÂÖ∂ÂâçË∫´ UNETR Áõ∏ÊØîÔºåÂú® ACDC ‰∏äÊèêÂçá‰∫Ü 7.3%ÔºåÂú® AMOS ‰∏äÊèêÂçá‰∫Ü 15.6%„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÊ∂àËûçÁ†îÁ©∂Ôºå‰ª•Â±ïÁ§∫ UNETVL ‰∏≠ÊØèÂÄãÂÖÉ‰ª∂ÁöÑÂΩ±ÈüøÔºåÊèê‰æõÂ∞çÂÖ∂Êû∂ÊßãÁöÑÂÖ®Èù¢ÁêÜËß£„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/tgrex6/UNETVL ÂèñÂæóÔºå‰øÉÈÄ≤ÈÄ≤‰∏ÄÊ≠•ÁöÑÂú®ÈÄôÊñπÈù¢ÁöÑÁ†îÁ©∂ÂíåÊáâÁî®„ÄÇ

##### **Combining LLM decision and RL action selection to improve RL policy for adaptive interventions**
2501.06980v1 by Karine Karine, Benjamin M. Marlin

Reinforcement learning (RL) is increasingly being used in the healthcare
domain, particularly for the development of personalized health adaptive
interventions. Inspired by the success of Large Language Models (LLMs), we are
interested in using LLMs to update the RL policy in real time, with the goal of
accelerating personalization. We use the text-based user preference to
influence the action selection on the fly, in order to immediately incorporate
the user preference. We use the term "user preference" as a broad term to refer
to a user personal preference, constraint, health status, or a statement
expressing like or dislike, etc. Our novel approach is a hybrid method that
combines the LLM response and the RL action selection to improve the RL policy.
Given an LLM prompt that incorporates the user preference, the LLM acts as a
filter in the typical RL action selection. We investigate different prompting
strategies and action selection strategies. To evaluate our approach, we
implement a simulation environment that generates the text-based user
preferences and models the constraints that impact behavioral dynamics. We show
that our approach is able to take into account the text-based user preferences,
while improving the RL policy, thus improving personalization in adaptive
intervention.

ÊëòË¶ÅÔºöÂº∑ÂåñÂ≠∏ÁøíÔºàRLÔºâÂú®ÈÜ´ÁôÇÈ†òÂüüÁöÑÊáâÁî®Êó•ÁõäÂª£Ê≥õÔºåÁâπÂà•ÊòØÁî®ÊñºÈñãÁôºÂÄã‰∫∫ÂåñÂÅ•Â∫∑ÈÅ©ÊáâÊÄßÂπ≤È†êÊé™ÊñΩ„ÄÇÂèóÂà∞Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÊàêÂäüÁöÑÂïüÁôºÔºåÊàëÂÄëÊúâËààË∂£‰ΩøÁî® LLM Âç≥ÊôÇÊõ¥Êñ∞ RL ÊîøÁ≠ñÔºåÁõÆÊ®ôÊòØÂä†ÈÄüÂÄã‰∫∫Âåñ„ÄÇÊàëÂÄë‰ΩøÁî®Âü∫ÊñºÊñáÂ≠óÁöÑ‰ΩøÁî®ËÄÖÂÅèÂ•Ω‰æÜÂΩ±ÈüøË°åÂãïÈÅ∏ÊìáÔºå‰ª•‰æøÁ´ãÂç≥Á¥çÂÖ•‰ΩøÁî®ËÄÖÂÅèÂ•Ω„ÄÇÊàëÂÄë‰ΩøÁî®„Äå‰ΩøÁî®ËÄÖÂÅèÂ•Ω„Äç‰∏ÄË©û‰ΩúÁÇ∫Âª£Áæ©Ë©ûÔºåÁî®‰æÜÊåá‰ΩøÁî®ËÄÖÁöÑÂÄã‰∫∫ÂÅèÂ•Ω„ÄÅÈôêÂà∂„ÄÅÂÅ•Â∫∑ÁãÄÊ≥ÅÊàñË°®ÈÅîÂ•ΩÊÉ°ÁöÑÈô≥Ëø∞Á≠â„ÄÇÊàëÂÄëÁöÑÊñ∞Á©éÊñπÊ≥ïÊòØ‰∏ÄÁ®ÆÊ∑∑ÂêàÊñπÊ≥ïÔºåÁµêÂêà‰∫Ü LLM ÂõûÊáâÂíå RL Ë°åÂãïÈÅ∏Êìá‰ª•ÊîπÂñÑ RL ÊîøÁ≠ñ„ÄÇÁµ¶ÂÆöÂåÖÂê´‰ΩøÁî®ËÄÖÂÅèÂ•ΩÁöÑ LLM ÊèêÁ§∫ÔºåLLM Âú®ÂÖ∏ÂûãÁöÑ RL Ë°åÂãïÈÅ∏Êìá‰∏≠ÂÖÖÁï∂ÈÅéÊøæÂô®„ÄÇÊàëÂÄëÁ†îÁ©∂‰∫Ü‰∏çÂêåÁöÑÊèêÁ§∫Á≠ñÁï•ÂíåË°åÂãïÈÅ∏ÊìáÁ≠ñÁï•„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÊàëÂÄëÂØ¶‰Ωú‰∫Ü‰∏ÄÂÄãÊ®°Êì¨Áí∞Â¢ÉÔºåÁî®ÊñºÁî¢ÁîüÂü∫ÊñºÊñáÂ≠óÁöÑ‰ΩøÁî®ËÄÖÂÅèÂ•ΩÔºå‰∏¶Â∞çÂΩ±ÈüøË°åÁÇ∫ÂãïÊÖãÁöÑÈôêÂà∂ÈÄ≤Ë°åÂª∫Ê®°„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïËÉΩÂ§†ËÄÉÈáèÂü∫ÊñºÊñáÂ≠óÁöÑ‰ΩøÁî®ËÄÖÂÅèÂ•ΩÔºåÂêåÊôÇÊîπÂñÑ RL ÊîøÁ≠ñÔºåÂæûËÄåÊîπÂñÑÈÅ©ÊáâÊÄßÂπ≤È†ê‰∏≠ÁöÑÂÄã‰∫∫Âåñ„ÄÇ

##### **Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate Patient Perspectives**
2501.06964v1 by Xinyao Ma, Rui Zhu, Zihao Wang, Jingwei Xiong, Qingyu Chen, Haixu Tang, L. Jean Camp, Lucila Ohno-Machado

Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing scenarios, particularly in simulating domain-specific experts
using tailored prompts. This ability enables LLMs to adopt the persona of
individuals with specific backgrounds, offering a cost-effective and efficient
alternative to traditional, resource-intensive user studies. By mimicking human
behavior, LLMs can anticipate responses based on concrete demographic or
professional profiles. In this paper, we evaluate the effectiveness of LLMs in
simulating individuals with diverse backgrounds and analyze the consistency of
these simulated behaviors compared to real-world outcomes. In particular, we
explore the potential of LLMs to interpret and respond to discharge summaries
provided to patients leaving the Intensive Care Unit (ICU). We evaluate and
compare with human responses the comprehensibility of discharge summaries among
individuals with varying educational backgrounds, using this analysis to assess
the strengths and limitations of LLM-driven simulations. Notably, when LLMs are
primed with educational background information, they deliver accurate and
actionable medical guidance 88% of the time. However, when other information is
provided, performance significantly drops, falling below random chance levels.
This preliminary study shows the potential benefits and pitfalls of
automatically generating patient-specific health information from diverse
populations. While LLMs show promise in simulating health personas, our results
highlight critical gaps that must be addressed before they can be reliably used
in clinical settings. Our findings suggest that a straightforward
query-response model could outperform a more tailored approach in delivering
health information. This is a crucial first step in understanding how LLMs can
be optimized for personalized health communication while maintaining accuracy.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ËßíËâ≤ÊâÆÊºîÂ†¥ÊôØ‰∏≠Â±ïÁèæ‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåÁâπÂà•ÊòØÂú®Ê®°Êì¨ÁâπÂÆöÈ†òÂüüÁöÑÂ∞àÂÆ∂ÊôÇÔºåÊúÉ‰ΩøÁî®ÈáèË∫´ÊâìÈÄ†ÁöÑÊèêÁ§∫„ÄÇÈÄôÁ®ÆËÉΩÂäõ‰Ωø LLM ËÉΩÂ§†Êé°Áî®ÂÖ∑ÊúâÁâπÂÆöËÉåÊôØÁöÑÂÄã‰∫∫ËßíËâ≤ÔºåÊèê‰æõ‰∏ÄÁ®ÆÁ∂ìÊøüÂØ¶ÊÉ†‰∏îÊúâÊïàÁéáÁöÑÊõø‰ª£ÊñπÊ°àÔºåÁî®ÊñºÂÇ≥Áµ±‰∏îË≥áÊ∫êÂØÜÈõÜÁöÑ‰ΩøÁî®ËÄÖÁ†îÁ©∂„ÄÇÈÄèÈÅéÊ®°Êì¨‰∫∫È°ûË°åÁÇ∫ÔºåLLM ËÉΩÂ§†Ê†πÊìöÂÖ∑È´îÁöÑ‰∫∫Âè£Áµ±Ë®àÊàñÂ∞àÊ•≠ÁâπÂæµÈ†êÊ∏¨ÂèçÊáâ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË©ï‰º∞‰∫Ü LLM Âú®Ê®°Êì¨ÂÖ∑Êúâ‰∏çÂêåËÉåÊôØÁöÑÂÄã‰∫∫ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºå‰∏¶ÂàÜÊûê‰∫ÜÈÄô‰∫õÊ®°Êì¨Ë°åÁÇ∫ËàáÂØ¶ÈöõÁµêÊûúÁõ∏ÊØîÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Ëß£ÈáãÂíåÂõûÊáâÊèê‰æõÁµ¶Èõ¢ÈñãÂä†Ë≠∑ÁóÖÊàø (ICU) ÊÇ£ËÄÖÁöÑÂá∫Èô¢ÊëòË¶ÅÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëË©ï‰º∞‰∏¶Ëàá‰∫∫È°ûÁöÑÂèçÊáâÊØîËºÉ‰∫Ü‰∏çÂêåÊïôËÇ≤ËÉåÊôØÁöÑÂÄã‰∫∫Â∞çÂá∫Èô¢ÊëòË¶ÅÁöÑÂèØÁêÜËß£ÊÄßÔºå‰∏¶‰ΩøÁî®Ê≠§ÂàÜÊûê‰æÜË©ï‰º∞ LLM È©ÖÂãïÊ®°Êì¨ÁöÑÂÑ™ÈªûÂíåÈôêÂà∂„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÁï∂ LLM Ë¢´Ê§çÂÖ•ÊïôËÇ≤ËÉåÊôØË≥áË®äÊôÇÔºå‰ªñÂÄëÂú® 88% ÁöÑÊôÇÈñìÂÖßÈÉΩËÉΩÊèê‰æõÊ∫ñÁ¢∫‰∏îÂèØË°åÁöÑÈÜ´ÁôÇÊåáÂ∞é„ÄÇ‰ΩÜÊòØÔºåÁï∂Êèê‰æõÂÖ∂‰ªñË≥áË®äÊôÇÔºåÊïàËÉΩÊúÉÈ°ØËëó‰∏ãÈôçÔºå‰ΩéÊñºÈö®Ê©üÊ©üÊúÉÁöÑÁ≠âÁ¥ö„ÄÇÈÄôÈ†ÖÂàùÊ≠•Á†îÁ©∂È°ØÁ§∫‰∫ÜËá™ÂãïÁî¢Áîü‰æÜËá™‰∏çÂêåÁæ§È´îÁöÑÁâπÂÆöÊñºÊÇ£ËÄÖÁöÑÂÅ•Â∫∑Ë≥áË®äÁöÑÊΩõÂú®Â•ΩËôïÂíåÁº∫Èªû„ÄÇÂÑòÁÆ° LLM Âú®Ê®°Êì¨ÂÅ•Â∫∑ËßíËâ≤ÊñπÈù¢È°ØÁ§∫Âá∫ÂâçÊôØÔºå‰ΩÜÊàëÂÄëÁöÑÁµêÊûúÁ™ÅÂá∫‰∫ÜÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÂèØÈù†‰ΩøÁî®‰πãÂâçÂøÖÈ†àËß£Ê±∫ÁöÑÈóúÈçµÂ∑ÆË∑ù„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂú®Êèê‰æõÂÅ•Â∫∑Ë≥áË®äÊñπÈù¢Ôºå‰∏ÄÂÄãÁõ¥Êé•ÁöÑÊü•Ë©¢ÂõûÊáâÊ®°ÂûãÂèØ‰ª•ÂÑ™Êñº‰∏ÄÂÄãÊõ¥ÈáèË∫´ÊâìÈÄ†ÁöÑÊñπÊ≥ï„ÄÇÈÄôÊòØ‰∫ÜËß£Â¶Ç‰ΩïÈáùÂ∞çÂÄã‰∫∫ÂåñÂÅ•Â∫∑Ê∫ùÈÄöÂÑ™Âåñ LLM ÂêåÊôÇÁ∂≠ÊåÅÊ∫ñÁ¢∫ÊÄßÁöÑÁ¨¨‰∏ÄÊ≠•„ÄÇ

##### **MedGrad E-CLIP: Enhancing Trust and Transparency in AI-Driven Skin Lesion Diagnosis**
2501.06887v1 by Sadia Kamal, Tim Oates

As deep learning models gain attraction in medical data, ensuring transparent
and trustworthy decision-making is essential. In skin cancer diagnosis, while
advancements in lesion detection and classification have improved accuracy, the
black-box nature of these methods poses challenges in understanding their
decision processes, leading to trust issues among physicians. This study
leverages the CLIP (Contrastive Language-Image Pretraining) model, trained on
different skin lesion datasets, to capture meaningful relationships between
visual features and diagnostic criteria terms. To further enhance transparency,
we propose a method called MedGrad E-CLIP, which builds on gradient-based
E-CLIP by incorporating a weighted entropy mechanism designed for complex
medical imaging like skin lesions. This approach highlights critical image
regions linked to specific diagnostic descriptions. The developed integrated
pipeline not only classifies skin lesions by matching corresponding
descriptions but also adds an essential layer of explainability developed
especially for medical data. By visually explaining how different features in
an image relates to diagnostic criteria, this approach demonstrates the
potential of advanced vision-language models in medical image analysis,
ultimately improving transparency, robustness, and trust in AI-driven
diagnostic systems.

ÊëòË¶ÅÔºöÈöèÁùÄÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÂú®ÂåªÂ≠¶Êï∞ÊçÆ‰∏≠Ëé∑ÂæóÂÖ≥Ê≥®ÔºåÁ°Æ‰øùÈÄèÊòé‰∏îÂÄºÂæó‰ø°ËµñÁöÑÂÜ≥Á≠ñËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®ÁöÆËÇ§ÁôåËØäÊñ≠‰∏≠ÔºåËôΩÁÑ∂ÁóÖÁÅ∂Ê£ÄÊµãÂíåÂàÜÁ±ªÁöÑËøõÊ≠•ÊèêÈ´ò‰∫ÜÂáÜÁ°ÆÊÄßÔºå‰ΩÜËøô‰∫õÊñπÊ≥ïÁöÑÈªëÁõíÊÄßË¥®ÂØπÁêÜËß£ÂÖ∂ÂÜ≥Á≠ñËøáÁ®ãÊûÑÊàê‰∫ÜÊåëÊàòÔºåÂØºËá¥ÂåªÁîü‰πãÈó¥ÁöÑ‰ø°‰ªªÈóÆÈ¢ò„ÄÇÊú¨Á†îÁ©∂Âà©Áî®Âú®‰∏çÂêåÁöÆËÇ§ÁóÖÂèòÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁöÑ CLIPÔºàÂØπÊØîËØ≠Ë®ÄÂõæÂÉèÈ¢ÑËÆ≠ÁªÉÔºâÊ®°ÂûãÔºå‰ª•ÊçïÊçâËßÜËßâÁâπÂæÅÂíåËØäÊñ≠Ê†áÂáÜÊúØËØ≠‰πãÈó¥ÁöÑÊúâÊÑè‰πâÂÖ≥Á≥ª„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÊèêÈ´òÈÄèÊòéÂ∫¶ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ MedGrad E-CLIP ÁöÑÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÈÄöËøáÁªìÂêà‰∏ì‰∏∫ÁöÆËÇ§ÁóÖÂèòÁ≠âÂ§çÊùÇÂåªÂ≠¶ÂΩ±ÂÉèËÆæËÆ°ÁöÑÂä†ÊùÉÁÜµÊú∫Âà∂ÔºåÂª∫Á´ãÂú®Âü∫‰∫éÊ¢ØÂ∫¶ÁöÑ E-CLIP ‰πã‰∏ä„ÄÇÊ≠§ÊñπÊ≥ïÁ™ÅÂá∫‰∫Ü‰∏éÁâπÂÆöËØäÊñ≠ÊèèËø∞Áõ∏ÂÖ≥ËÅîÁöÑÂÖ≥ÈîÆÂõæÂÉèÂå∫Âüü„ÄÇÂºÄÂèëÁöÑÈõÜÊàêÁÆ°ÈÅì‰∏ç‰ªÖÈÄöËøáÂåπÈÖçÁõ∏Â∫îÁöÑÊèèËø∞ÂØπÁöÆËÇ§ÁóÖÂèòËøõË°åÂàÜÁ±ªÔºåËøòÊ∑ªÂä†‰∫Ü‰∏ÄÂ±Ç‰∏ìÈó®‰∏∫ÂåªÂ≠¶Êï∞ÊçÆÂºÄÂèëÁöÑÂü∫Êú¨ÂèØËß£ÈáäÊÄß„ÄÇÈÄöËøáÁõ¥ËßÇÂú∞Ëß£ÈáäÂõæÂÉè‰∏≠‰∏çÂêåÁâπÂæÅ‰∏éËØäÊñ≠Ê†áÂáÜÁöÑÂÖ≥Á≥ªÔºåËøôÁßçÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÈ´òÁ∫ßËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÂ≠¶ÂõæÂÉèÂàÜÊûê‰∏≠ÁöÑÊΩúÂäõÔºåÊúÄÁªàÊèêÈ´ò‰∫ÜÈÄèÊòéÂ∫¶„ÄÅÁ®≥ÂÅ•ÊÄßÂíåÂØπ‰∫∫Â∑•Êô∫ËÉΩÈ©±Âä®ÁöÑËØäÊñ≠Á≥ªÁªüÁöÑ‰ø°‰ªª„ÄÇ

##### **A Foundational Generative Model for Breast Ultrasound Image Analysis**
2501.06869v1 by Haojun Yu, Youcheng Li, Nan Zhang, Zihan Niu, Xuantong Gong, Yanwen Luo, Haotian Ye, Siyu He, Quanlin Wu, Wangyan Qin, Mengyuan Zhou, Jie Han, Jia Tao, Ziwei Zhao, Di Dai, Di He, Dong Wang, Binghui Tang, Ling Huo, James Zou, Qingli Zhu, Yong Wang, Liwei Wang

Foundational models have emerged as powerful tools for addressing various
tasks in clinical settings. However, their potential development to breast
ultrasound analysis remains untapped. In this paper, we present BUSGen, the
first foundational generative model specifically designed for breast ultrasound
image analysis. Pretrained on over 3.5 million breast ultrasound images, BUSGen
has acquired extensive knowledge of breast structures, pathological features,
and clinical variations. With few-shot adaptation, BUSGen can generate
repositories of realistic and informative task-specific data, facilitating the
development of models for a wide range of downstream tasks. Extensive
experiments highlight BUSGen's exceptional adaptability, significantly
exceeding real-data-trained foundational models in breast cancer screening,
diagnosis, and prognosis. In breast cancer early diagnosis, our approach
outperformed all board-certified radiologists (n=9), achieving an average
sensitivity improvement of 16.5% (P-value<0.0001). Additionally, we
characterized the scaling effect of using generated data which was as effective
as the collected real-world data for training diagnostic models. Moreover,
extensive experiments demonstrated that our approach improved the
generalization ability of downstream models. Importantly, BUSGen protected
patient privacy by enabling fully de-identified data sharing, making progress
forward in secure medical data utilization. An online demo of BUSGen is
available at https://aibus.bio.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂ∑≤ÊàêÁÇ∫Ëß£Ê±∫Ëá®Â∫äÁí∞Â¢É‰∏≠ÂêÑÁ®Æ‰ªªÂãôÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®‰π≥ÊàøË∂ÖÈü≥Ê≥¢ÂàÜÊûêÁöÑÊΩõÂú®ÁôºÂ±ï‰ªçÊú™ÈñãÁôº„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ BUSGenÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞àÈñÄË®≠Ë®àÁî®Êñº‰π≥ÊàøË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÂàÜÊûêÁöÑÂü∫Á§éÁîüÊàêÊ®°Âûã„ÄÇBUSGen Âú®Ë∂ÖÈÅé 350 Ëê¨Âºµ‰π≥ÊàøË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉè‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥ÔºåÂ∑≤Áç≤Âæó‰π≥ÊàøÁµêÊßã„ÄÅÁóÖÁêÜÁâπÂæµÂíåËá®Â∫äËÆäÁï∞ÁöÑÂª£Ê≥õÁü•Ë≠ò„ÄÇÈÄèÈÅéÂ∞ëÈáèÈÅ©ÊáâÔºåBUSGen ÂèØ‰ª•Áî¢ÁîüÈÄºÁúü‰∏îÂÖ∑ÊúâË≥áË®äÊÄßÁöÑÁâπÂÆö‰ªªÂãôË≥áÊñôÂÑ≤Â≠òÂ∫´Ôºå‰øÉÈÄ≤ÈñãÁôºÂª£Ê≥õÁöÑ‰∏ãÊ∏∏‰ªªÂãôÊ®°Âûã„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÁ™ÅÈ°Ø‰∫Ü BUSGen ÁöÑÂá∫Ëâ≤ÈÅ©ÊáâÊÄßÔºåÂú®‰π≥ÁôåÁØ©Ê™¢„ÄÅË®∫Êñ∑ÂíåÈ†êÂæåÊñπÈù¢È°ØËëóË∂ÖË∂ä‰ª•ÁúüÂØ¶Ë≥áÊñôË®ìÁ∑¥ÁöÑÂü∫Á§éÊ®°Âûã„ÄÇÂú®‰π≥ÁôåÊó©ÊúüË®∫Êñ∑‰∏≠ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂÑ™ÊñºÊâÄÊúâÈÄöÈÅéË™çË≠âÁöÑÊîæÂ∞ÑÁßëÈÜ´Â∏´ (n=9)ÔºåÂπ≥ÂùáÊïèÊÑüÂ∫¶ÊèêÈ´ò‰∫Ü 16.5%ÔºàP ÂÄº <0.0001Ôºâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèèËø∞‰∫Ü‰ΩøÁî®ÁîüÊàêË≥áÊñôÁöÑË¶èÊ®°ÊïàÊáâÔºåÂÖ∂ËàáÊî∂ÈõÜÁöÑÁúüÂØ¶‰∏ñÁïåË≥áÊñô‰∏ÄÊ®£ÊúâÊïàÔºåÂèØÁî®ÊñºË®ìÁ∑¥Ë®∫Êñ∑Ê®°Âûã„ÄÇÊ≠§Â§ñÔºåÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊîπÂñÑ‰∫Ü‰∏ãÊ∏∏Ê®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÈáçË¶ÅÁöÑÊòØÔºåBUSGen ‰øùË≠∑‰∫ÜÊÇ£ËÄÖÈö±ÁßÅÔºåÂõ†ÁÇ∫ÂÆÉËÉΩÂ§†ÂÆåÂÖ®ÂéªË≠òÂà•Ë≥áÊñôÂÖ±‰∫´ÔºåÂú®ÂÆâÂÖ®ÈÜ´ÁôÇË≥áÊñôÂà©Áî®ÊñπÈù¢ÂèñÂæóÈÄ≤Â±ï„ÄÇBUSGen ÁöÑÁ∑ö‰∏äÁ§∫ÁØÑÂèØÂú® https://aibus.bio ÂèñÂæó„ÄÇ

##### **A Comprehensive Evaluation of Large Language Models on Mental Illnesses in Arabic Context**
2501.06859v1 by Noureldin Zahran, Aya E. Fouda, Radwa J. Hanafy, Mohammed E. Fouda

Mental health disorders pose a growing public health concern in the Arab
world, emphasizing the need for accessible diagnostic and intervention tools.
Large language models (LLMs) offer a promising approach, but their application
in Arabic contexts faces challenges including limited labeled datasets,
linguistic complexity, and translation biases. This study comprehensively
evaluates 8 LLMs, including general multi-lingual models, as well as bi-lingual
ones, on diverse mental health datasets (such as AraDepSu, Dreaddit, MedMCQA),
investigating the impact of prompt design, language configuration (native
Arabic vs. translated English, and vice versa), and few-shot prompting on
diagnostic performance. We find that prompt engineering significantly
influences LLM scores mainly due to reduced instruction following, with our
structured prompt outperforming a less structured variant on multi-class
datasets, with an average difference of 14.5\%. While language influence on
performance was modest, model selection proved crucial: Phi-3.5 MoE excelled in
balanced accuracy, particularly for binary classification, while Mistral NeMo
showed superior performance in mean absolute error for severity prediction
tasks. Few-shot prompting consistently improved performance, with particularly
substantial gains observed for GPT-4o Mini on multi-class classification,
boosting accuracy by an average factor of 1.58. These findings underscore the
importance of prompt optimization, multilingual analysis, and few-shot learning
for developing culturally sensitive and effective LLM-based mental health tools
for Arabic-speaking populations.

ÊëòË¶ÅÔºö<paragraph>ÂøÉÁêÜÂÅ•Â∫∑ÈöúÁ§ôÂú®ÈòøÊãâ‰ºØ‰∏ñÁïå‰∏≠ÊßãÊàêÊó•ÁõäÂö¥ÈáçÁöÑÂÖ¨ÂÖ±Ë°õÁîüÂïèÈ°åÔºåÂº∑Ë™ø‰∫ÜÂ∞çÂèØÂèäÁöÑË®∫Êñ∑ÂíåÂπ≤È†êÂ∑•ÂÖ∑ÁöÑÈúÄÊ±Ç„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÔºå‰ΩÜÂÆÉÂÄëÂú®ÈòøÊãâ‰ºØË™ûÁí∞Â¢É‰∏≠ÁöÑÊáâÁî®Èù¢Ëá®ËëóÊåëÊà∞ÔºåÂåÖÊã¨Ê®ôË®òË≥áÊñôÈõÜÊúâÈôê„ÄÅË™ûË®ÄË§áÈõúÊÄßÂíåÁøªË≠ØÂÅèÂ∑Æ„ÄÇÊú¨Á†îÁ©∂ÂÖ®Èù¢Ë©ï‰º∞‰∫Ü 8 ÂÄã LLMÔºåÂåÖÊã¨‰∏ÄËà¨Â§öË™ûË®ÄÊ®°ÂûãÂíåÈõôË™ûÊ®°ÂûãÔºåÂú®‰∏çÂêåÁöÑÂøÉÁêÜÂÅ•Â∫∑Ë≥áÊñôÈõÜÔºà‰æãÂ¶Ç AraDepSu„ÄÅDreaddit„ÄÅMedMCQAÔºâ‰∏äÔºåÊé¢Ë®éÊèêÁ§∫Ë®≠Ë®à„ÄÅË™ûË®ÄÈÖçÁΩÆÔºàÈòøÊãâ‰ºØË™ûÂéüÊñáËàáÁøªË≠ØÂæåÁöÑËã±Ë™ûÔºåÂèç‰πã‰∫¶ÁÑ∂ÔºâÂíåÂ∞ëÊ¨°ÊèêÁ§∫Â∞çË®∫Êñ∑Ë°®ÁèæÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁôºÁèæÊèêÁ§∫Â∑•Á®ãÈ°ØËëóÂΩ±Èüø LLM ÂàÜÊï∏Ôºå‰∏ªË¶ÅÊòØÁî±ÊñºÊ∏õÂ∞ë‰∫ÜË™™ÊòéÈÅµÂæ™ÔºåÊàëÂÄëÁöÑÁµêÊßãÂåñÊèêÁ§∫Âú®Â§öÈ°ûË≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÁµêÊßãËºÉ‰∏çÂö¥Ë¨πÁöÑËÆäÈ´îÔºåÂπ≥ÂùáÂ∑ÆÁï∞ÁÇ∫ 14.5%„ÄÇÈõñÁÑ∂Ë™ûË®ÄÂ∞çË°®ÁèæÁöÑÂΩ±Èüø‰∏çÂ§ßÔºå‰ΩÜÊ®°ÂûãÈÅ∏ÊìáË¢´Ë≠âÊòéËá≥ÈóúÈáçË¶ÅÔºöPhi-3.5 MoE Âú®Âπ≥Ë°°Ê∫ñÁ¢∫Â∫¶ÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤ÔºåÁâπÂà•ÊòØÂú®‰∫åÂÖÉÂàÜÈ°ûÊñπÈù¢ÔºåËÄå Mistral NeMo Âú®Âö¥ÈáçÊÄßÈ†êÊ∏¨‰ªªÂãôÁöÑÂπ≥ÂùáÁµïÂ∞çË™§Â∑ÆÊñπÈù¢Ë°®ÁèæÂá∫ÂÑ™Áï∞ÁöÑË°®Áèæ„ÄÇÂ∞ëÊ¨°ÊèêÁ§∫ÂßãÁµÇÊîπÂñÑË°®ÁèæÔºåÁâπÂà•ÊòØÂú® GPT-4o Mini ‰∏äËßÄÂØüÂà∞Â§öÈ°ûÂàÜÈ°ûÁöÑÈ°ØËëóÂ¢ûÁõäÔºåÂ∞áÊ∫ñÁ¢∫Â∫¶ÊèêÈ´ò‰∫ÜÂπ≥Âùá 1.58 ÂÄç„ÄÇÈÄô‰∫õÁôºÁèæÂº∑Ë™ø‰∫ÜÊèêÁ§∫ÊúÄ‰Ω≥Âåñ„ÄÅÂ§öË™ûË®ÄÂàÜÊûêÂíåÂ∞ëÊ¨°Â≠∏ÁøíÂ∞çÊñºÈñãÁôºÈÅ©ÂêàÊñáÂåñ‰∏îÊúâÊïàÁöÑÂü∫Êñº LLM ÁöÑÂøÉÁêÜÂÅ•Â∫∑Â∑•ÂÖ∑‰ª•ÊúçÂãôÈòøÊãâ‰ºØË™û‰∫∫Âè£ÁöÑÈáçË¶ÅÊÄß„ÄÇ</paragraph>

##### **MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome Prediction**
2501.06823v1 by Yiqing Zhang, Xiaozhong Liu, Fabricio Murai

Clinical trials are the gold standard for assessing the effectiveness and
safety of drugs for treating diseases. Given the vast design space of drug
molecules, elevated financial cost, and multi-year timeline of these trials,
research on clinical trial outcome prediction has gained immense traction.
Accurate predictions must leverage data of diverse modes such as drug
molecules, target diseases, and eligibility criteria to infer successes and
failures. Previous Deep Learning approaches for this task, such as HINT, often
require wet lab data from synthesized molecules and/or rely on prior knowledge
to encode interactions as part of the model architecture. To address these
limitations, we propose a light-weight attention-based model, MEXA-CTP, to
integrate readily-available multi-modal data and generate effective
representations via specialized modules dubbed "mode experts", while avoiding
human biases in model design. We optimize MEXA-CTP with the Cauchy loss to
capture relevant interactions across modes. Our experiments on the Trial
Outcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon
existing approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,
and 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to
quantify the effectiveness of each component in our proposed method.

ÊëòË¶ÅÔºöËá®Â∫äË©¶È©óÊòØË©ï‰º∞Ê≤ªÁôÇÁñæÁóÖÁöÑËó•Áâ©ÊúâÊïàÊÄßÂíåÂÆâÂÖ®ÊÄßÁöÑÈªÉÈáëÊ®ôÊ∫ñ„ÄÇÈëëÊñºËó•Áâ©ÂàÜÂ≠êÁöÑÂª£Ê≥õË®≠Ë®àÁ©∫Èñì„ÄÅÈ´òÊòÇÁöÑË≤°ÂãôÊàêÊú¨ÂíåÈÄô‰∫õË©¶È©óÂ§öÂπ¥ÁöÑÊôÇÈñìË°®ÔºåËá®Â∫äË©¶È©óÁµêÊûúÈ†êÊ∏¨ÁöÑÁ†îÁ©∂Áç≤Âæó‰∫ÜÂ∑®Â§ßÁöÑÈóúÊ≥®„ÄÇÊ∫ñÁ¢∫ÁöÑÈ†êÊ∏¨ÂøÖÈ†àÂà©Áî®Ëó•Áâ©ÂàÜÂ≠ê„ÄÅÁõÆÊ®ôÁñæÁóÖÂíåÁ¨¶ÂêàË≥áÊ†ºÊ®ôÊ∫ñÁ≠âÂ§öÁ®ÆÊ®°ÂºèÁöÑÊï∏Êìö‰æÜÊé®Êñ∑ÊàêÂäüÂíåÂ§±Êïó„ÄÇÊ≠§‰ªªÂãôÁöÑÂÖàÂâçÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºà‰æãÂ¶Ç HINTÔºâÈÄöÂ∏∏ÈúÄË¶ÅÂêàÊàêÂàÜÂ≠êÁöÑÊøïÂØ¶È©óÂÆ§Êï∏ÊìöÂíå/Êàñ‰æùË≥¥ÊñºÂÖàÈ©óÁü•Ë≠òÂ∞á‰∫§‰∫íÁ∑®Á¢ºÁÇ∫Ê®°ÂûãÊû∂ÊßãÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËºïÈáèÁ¥öÁöÑÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÊ®°Âûã MEXA-CTPÔºå‰ª•Êï¥ÂêàÁèæÊàêÁöÑÂ§öÊ®°ÂºèÊï∏Êìö‰∏¶ÈÄöÈÅéÁ®±ÁÇ∫„ÄåÊ®°ÂºèÂ∞àÂÆ∂„ÄçÁöÑÂ∞àÁî®Ê®°ÁµÑÁî¢ÁîüÊúâÊïàÁöÑË°®Á§∫ÔºåÂêåÊôÇÈÅøÂÖçÊ®°ÂûãË®≠Ë®à‰∏≠ÁöÑ‰∫∫ÁÇ∫ÂÅèÂ∑Æ„ÄÇÊàëÂÄë‰ΩøÁî®ÊüØË•øÊêçÂ§±ÂáΩÊï∏ÊúÄ‰Ω≥Âåñ MEXA-CTPÔºå‰ª•ÊçïÊçâË∑®Ê®°ÂºèÁõ∏ÈóúÁöÑ‰∫§‰∫í„ÄÇÊàëÂÄëÂú®Ë©¶È©óÁµêÊûúÈ†êÊ∏¨ (TOP) Âü∫Ê∫ñ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåËàá HINT Áõ∏ÊØîÔºåMEXA-CTP ÂàÜÂà•Âú® F1 ÂàÜÊï∏‰∏äÊèêÈ´ò‰∫Ü 11.3%„ÄÅPR-AUC ‰∏äÊèêÈ´ò‰∫Ü 12.2%„ÄÅROC-AUC ‰∏äÊèêÈ´ò‰∫Ü 2.5%„ÄÇÊèê‰æõ‰∫ÜÊ∂àËûçÁ†îÁ©∂‰æÜÈáèÂåñÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ï‰∏≠ÊØèÂÄãÁµÑ‰ª∂ÁöÑÊúâÊïàÊÄß„ÄÇ

