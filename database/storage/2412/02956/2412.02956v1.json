{"2412.02956": {"publish_time": "2024-12-04", "title": "Curriculum-style Data Augmentation for LLM-based Metaphor Detection", "paper_summary": "Recently, utilizing large language models (LLMs) for metaphor detection has\nachieved promising results. However, these methods heavily rely on the\ncapabilities of closed-source LLMs, which come with relatively high inference\ncosts and latency. To address this, we propose a method for metaphor detection\nby fine-tuning open-source LLMs, effectively reducing inference costs and\nlatency with a single inference step. Furthermore, metaphor detection suffers\nfrom a severe data scarcity problem, which hinders effective fine-tuning of\nLLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA).\nSpecifically, before fine-tuning, we evaluate the training data to identify\ncorrectly predicted instances for fine-tuning, while incorrectly predicted\ninstances are used as seed data for data augmentation. This approach enables\nthe model to quickly learn simpler knowledge and progressively acquire more\ncomplex knowledge, thereby improving performance incrementally. Experimental\nresults demonstrate that our method achieves state-of-the-art performance\nacross all baselines. Additionally, we provide detailed ablation studies to\nvalidate the effectiveness of CDA.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9032\u884c\u96b1\u55bb\u5075\u6e2c\u5df2\u7372\u5f97\u4ee4\u4eba\u6eff\u610f\u7684\u7d50\u679c\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u56b4\u91cd\u4f9d\u8cf4\u9589\u6e90 LLM \u7684\u529f\u80fd\uff0c\u800c\u9019\u6703\u5e36\u4f86\u76f8\u5c0d\u9ad8\u7684\u63a8\u8ad6\u6210\u672c\u548c\u5ef6\u9072\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u900f\u904e\u5fae\u8abf\u958b\u6e90 LLM \u4f86\u9032\u884c\u96b1\u55bb\u5075\u6e2c\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u5730\u6e1b\u5c11\u4e86\u55ae\u4e00\u63a8\u8ad6\u6b65\u9a5f\u7684\u63a8\u8ad6\u6210\u672c\u548c\u5ef6\u9072\u3002\u6b64\u5916\uff0c\u96b1\u55bb\u5075\u6e2c\u6703\u906d\u53d7\u56b4\u91cd\u7684\u8cc7\u6599\u7a00\u5c11\u554f\u984c\uff0c\u9019\u6703\u963b\u7919 LLM \u7684\u6709\u6548\u5fae\u8abf\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u8ab2\u7a0b\u5f0f\u8cc7\u6599\u64f4\u5145 (CDA)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5728\u5fae\u8abf\u4e4b\u524d\uff0c\u6211\u5011\u6703\u8a55\u4f30\u8a13\u7df4\u8cc7\u6599\u4ee5\u627e\u51fa\u6b63\u78ba\u9810\u6e2c\u7684\u5fae\u8abf\u5be6\u4f8b\uff0c\u800c\u932f\u8aa4\u9810\u6e2c\u7684\u5be6\u4f8b\u5247\u7528\u4f5c\u8cc7\u6599\u64f4\u5145\u7684\u7a2e\u5b50\u8cc7\u6599\u3002\u9019\u7a2e\u65b9\u6cd5\u8b93\u6a21\u578b\u80fd\u5920\u5feb\u901f\u5b78\u7fd2\u8f03\u7c21\u55ae\u7684\u77e5\u8b58\uff0c\u4e26\u9010\u6b65\u7fd2\u5f97\u8f03\u8907\u96dc\u7684\u77e5\u8b58\uff0c\u5f9e\u800c\u9010\u6b65\u63d0\u5347\u6548\u80fd\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u6240\u6709\u57fa\u6e96\u4e2d\u90fd\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u8a73\u7d30\u7684\u6d88\u878d\u7814\u7a76\uff0c\u4ee5\u9a57\u8b49 CDA \u7684\u6709\u6548\u6027\u3002</paragraph>", "author": "Kaidi Jia et.al.", "authors": "Kaidi Jia, Yanxia Wu, Rongsheng Li", "id": "2412.02956v1", "paper_url": "http://arxiv.org/abs/2412.02956v1", "repo": "null"}}