{"2405.20588": {"publish_time": "2024-05-31", "title": "DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models", "paper_summary": "Recently, while large language models (LLMs) have demonstrated impressive\nresults, they still suffer from hallucination, i.e., the generation of false\ninformation. Model editing is the task of fixing factual mistakes in LLMs; yet,\nmost previous works treat it as a one-time task, paying little attention to\never-emerging mistakes generated by LLMs. We address the task of sequential\nmodel editing (SME) that aims to rectify mistakes continuously. A Dynamic\nAuxiliary Fusion Network (DAFNet) is designed to enhance the semantic\ninteraction among the factual knowledge within the entire sequence, preventing\ncatastrophic forgetting during the editing process of multiple knowledge\ntriples. Specifically, (1) for semantic fusion within a relation triple, we\naggregate the intra-editing attention flow into auto-regressive self-attention\nwith token-level granularity in LLMs. We further leverage multi-layer diagonal\ninter-editing attention flow to update the weighted representations of the\nentire sequence-level granularity. (2) Considering that auxiliary parameters\nare required to store the knowledge for sequential editing, we construct a new\ndataset named \\textbf{DAFSet}, fulfilling recent, popular, long-tail and robust\nproperties to enhance the generality of sequential editing. Experiments show\nDAFNet significantly outperforms strong baselines in single-turn and sequential\nediting. The usage of DAFSet also consistently improves the performance of\nother auxiliary network-based methods in various scenarios", "paper_summary_zh": "<paragraph>\u8fd1\u671f\uff0c\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u5b83\u4eec\u4ecd\u9971\u53d7\u5e7b\u89c9\u56f0\u6270\uff0c\u5373\u751f\u6210\u865a\u5047\u4fe1\u606f\u3002\u6a21\u578b\u7f16\u8f91\u662f\u4fee\u590d LLM \u4e2d\u4e8b\u5b9e\u9519\u8bef\u7684\u4efb\u52a1\uff1b\u7136\u800c\uff0c\u5927\u591a\u6570\u4ee5\u524d\u7684\u4f5c\u54c1\u5c06\u5176\u89c6\u4e3a\u4e00\u6b21\u6027\u4efb\u52a1\uff0c\u5f88\u5c11\u5173\u6ce8 LLM \u751f\u6210\u7684\u4e0d\u65ad\u51fa\u73b0\u7684\u9519\u8bef\u3002\u6211\u4eec\u89e3\u51b3\u987a\u5e8f\u6a21\u578b\u7f16\u8f91 (SME) \u7684\u4efb\u52a1\uff0c\u65e8\u5728\u6301\u7eed\u7ea0\u6b63\u9519\u8bef\u3002\u52a8\u6001\u8f85\u52a9\u878d\u5408\u7f51\u7edc (DAFNet) \u88ab\u8bbe\u8ba1\u7528\u4e8e\u589e\u5f3a\u6574\u4e2a\u5e8f\u5217\u4e2d\u4e8b\u5b9e\u77e5\u8bc6\u4e4b\u95f4\u7684\u8bed\u4e49\u4ea4\u4e92\uff0c\u9632\u6b62\u5728\u7f16\u8f91\u591a\u4e2a\u77e5\u8bc6\u4e09\u5143\u7ec4\u7684\u8fc7\u7a0b\u4e2d\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u3002\u5177\u4f53\u6765\u8bf4\uff0c(1) \u5bf9\u4e8e\u5173\u7cfb\u4e09\u5143\u7ec4\u5185\u7684\u8bed\u4e49\u878d\u5408\uff0c\u6211\u4eec\u5c06\u5185\u90e8\u7f16\u8f91\u6ce8\u610f\u529b\u6d41\u805a\u5408\u5230 LLM \u4e2d\u5177\u6709\u6807\u8bb0\u7ea7\u522b\u7c92\u5ea6\u7684\u81ea\u56de\u5f52\u81ea\u6ce8\u610f\u529b\u4e2d\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5229\u7528\u591a\u5c42\u5bf9\u89d2\u7ebf\u4e92\u7f16\u8f91\u6ce8\u610f\u529b\u6d41\u6765\u66f4\u65b0\u6574\u4e2a\u5e8f\u5217\u7ea7\u522b\u7c92\u5ea6\u7684\u52a0\u6743\u8868\u793a\u3002(2) \u8003\u8651\u5230\u9700\u8981\u8f85\u52a9\u53c2\u6570\u6765\u5b58\u50a8\u987a\u5e8f\u7f16\u8f91\u7684\u77e5\u8bc6\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a \\textbf{DAFSet} \u7684\u65b0\u6570\u636e\u96c6\uff0c\u6ee1\u8db3\u6700\u65b0\u3001\u6d41\u884c\u3001\u957f\u5c3e\u548c\u9c81\u68d2\u5c5e\u6027\u4ee5\u589e\u5f3a\u987a\u5e8f\u7f16\u8f91\u7684\u901a\u7528\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDAFNet \u5728\u5355\u8f6e\u548c\u987a\u5e8f\u7f16\u8f91\u4e2d\u660e\u663e\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002DAFSet \u7684\u4f7f\u7528\u4e5f\u6301\u7eed\u63d0\u9ad8\u4e86\u5176\u4ed6\u57fa\u4e8e\u8f85\u52a9\u7f51\u7edc\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u573a\u666f\u4e2d\u7684\u6027\u80fd</paragraph>", "author": "Taolin Zhang et.al.", "authors": "Taolin Zhang, Qizhou Chen, Dongyang Li, Chengyu Wang, Xiaofeng He, Longtao Huang, Hui Xue, Jun Huang", "id": "2405.20588v1", "paper_url": "http://arxiv.org/abs/2405.20588v1", "repo": "null"}}