{"2412.11768": {"publish_time": "2024-12-16", "title": "No More Adam: Learning Rate Scaling at Initialization is All You Need", "paper_summary": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings.", "paper_summary_zh": "<paragraph>\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8cea\u7591\u81ea\u9069\u61c9\u68af\u5ea6\u65b9\u6cd5\u8a13\u7df4\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u7684\u5fc5\u8981\u6027\u3002SGD-SaI \u662f\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u589e\u5f37\uff0c\u53ef\u4ee5\u642d\u914d\u5e36\u52d5\u80fd\u7684\u96a8\u6a5f\u68af\u5ea6\u4e0b\u964d\u6cd5 (SGDM) \u4f7f\u7528\u3002SGD-SaI \u57f7\u884c\u521d\u59cb\u5316\u6642\u5b78\u7fd2\u7387\u7e2e\u653e (SaI) \u81f3\u4e0d\u540c\u7684\u53c3\u6578\u7fa4\u7d44\uff0c\u4e26\u7531\u5b83\u5011\u5404\u81ea\u7684\u68af\u5ea6\u4fe1\u566a\u6bd4 (g-SNR) \u5f15\u5c0e\u3002\u900f\u904e\u8abf\u6574\u5b78\u7fd2\u7387\uff0c\u800c\u4e0d\u4f9d\u8cf4\u81ea\u9069\u61c9\u4e8c\u968e\u52d5\u80fd\uff0cSGD-SaI \u6709\u52a9\u65bc\u5f9e\u7b2c\u4e00\u500b\u53cd\u8986\u904b\u7b97\u958b\u59cb\u9632\u6b62\u8a13\u7df4\u4e0d\u5e73\u8861\uff0c\u4e26\u5c07\u6700\u4f73\u5316\u5668\u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u534a\uff0c\u8207 AdamW \u76f8\u6bd4\u3002\u5118\u7ba1 SGD-SaI \u65e2\u7c21\u55ae\u53c8\u6709\u6548\u7387\uff0c\u4f46\u5728\u8a13\u7df4\u5404\u7a2e\u57fa\u65bc Transformer \u7684\u4efb\u52d9\u6642\uff0c\u5b83\u59cb\u7d42\u80fd\u8207 AdamW \u76f8\u5339\u914d\uff0c\u751a\u81f3\u8868\u73fe\u5f97\u66f4\u597d\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f7f\u7528 SGD \u8a13\u7df4 Transformer \u7684\u9577\u671f\u6311\u6230\u3002SGD-SaI \u5728\u4f7f\u7528\u8996\u89ba Transformer (ViT) \u9032\u884c ImageNet-1K \u5206\u985e\u4ee5\u53ca\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM\uff0c\u50c5\u89e3\u78bc\u5668 Transformer) \u7684 GPT-2 \u9810\u8a13\u7df4\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u8b49\u660e\u4e86\u5b83\u5c0d\u8d85\u53c3\u6578\u8b8a\u5316\u7684\u7a69\u5065\u6027\u4ee5\u53ca\u5c0d\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u7684\u5be6\u7528\u6027\u3002\u6211\u5011\u9032\u4e00\u6b65\u6e2c\u8a66\u4e86\u5b83\u5728 LLM \u548c\u64f4\u6563\u6a21\u578b\u7684 LoRA \u5fae\u8abf\u7b49\u4efb\u52d9\u4e0a\u7684\u7a69\u5065\u6027\uff0c\u5b83\u59cb\u7d42\u512a\u65bc\u6700\u5148\u9032\u7684\u6700\u4f73\u5316\u5668\u3002\u5f9e\u8a18\u61b6\u9ad4\u6548\u7387\u7684\u89d2\u5ea6\u4f86\u770b\uff0cSGD-SaI \u70ba\u6700\u4f73\u5316\u5668\u72c0\u614b\u7bc0\u7701\u4e86\u5927\u91cf\u7684\u8a18\u61b6\u9ad4\uff0c\u8207 AdamW \u5728\u5168\u7cbe\u5ea6\u8a13\u7df4\u8a2d\u5b9a\u4e2d\u76f8\u6bd4\uff0cGPT-2 (1.5B \u53c3\u6578) \u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86 5.93 GB\uff0c\u800c Llama2-7B \u5247\u6e1b\u5c11\u4e86 25.15 GB\u3002</paragraph>", "author": "Minghao Xu et.al.", "authors": "Minghao Xu, Lichuan Xiang, Xu Cai, Hongkai Wen", "id": "2412.11768v1", "paper_url": "http://arxiv.org/abs/2412.11768v1", "repo": "https://github.com/anonymousalethiometer/sgd_sai"}}