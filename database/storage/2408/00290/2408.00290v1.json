{"2408.00290": {"publish_time": "2024-08-01", "title": "Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network", "paper_summary": "With the advent of the era of foundation models, pre-training and fine-tuning\nhave become common paradigms. Recently, parameter-efficient fine-tuning has\ngarnered widespread attention due to its better balance between the number of\nlearnable parameters and performance. However, some current parameter-efficient\nfine-tuning methods only model a single modality and lack the utilization of\nstructural knowledge in downstream tasks. To address this issue, this paper\nproposes a multi-modal parameter-efficient fine-tuning method based on graph\nnetworks. Each image is fed into a multi-modal large language model (MLLM) to\ngenerate a text description. The image and its corresponding text description\nare then processed by a frozen image encoder and text encoder to generate image\nfeatures and text features, respectively. A graph is constructed based on the\nsimilarity of the multi-modal feature nodes, and knowledge and relationships\nrelevant to these features are extracted from each node. Additionally, Elastic\nWeight Consolidation (EWC) regularization is incorporated into the loss\nfunction to mitigate the problem of forgetting during task learning. The\nproposed model achieves test accuracies on the OxfordPets, Flowers102, and\nFood101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The\ncode is available at https://github.com/yunche0/GA-Net/tree/master.", "paper_summary_zh": "\u96a8\u8457\u57fa\u790e\u6a21\u578b\u6642\u4ee3\u7684\u5230\u4f86\uff0c\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u5df2\u6210\u70ba\u5e38\u898b\u7684\u7bc4\u4f8b\u3002\u6700\u8fd1\uff0c\u7531\u65bc\u53c3\u6578\u6709\u6548\u5fae\u8abf\u5728\u53ef\u5b78\u7fd2\u53c3\u6578\u6578\u91cf\u548c\u6548\u80fd\u4e4b\u9593\u53d6\u5f97\u66f4\u597d\u7684\u5e73\u8861\uff0c\u56e0\u6b64\u5099\u53d7\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u4e00\u4e9b\u76ee\u524d\u7684\u53c3\u6578\u6709\u6548\u5fae\u8abf\u65b9\u6cd5\u50c5\u5efa\u6a21\u55ae\u4e00\u6a21\u614b\uff0c\u4e14\u7f3a\u4e4f\u5728\u4e0b\u6e38\u4efb\u52d9\u4e2d\u5229\u7528\u7d50\u69cb\u77e5\u8b58\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc\u5716\u5f62\u7db2\u8def\u7684\u591a\u6a21\u614b\u53c3\u6578\u6709\u6548\u5fae\u8abf\u65b9\u6cd5\u3002\u6bcf\u500b\u5f71\u50cf\u90fd\u6703\u8f38\u5165\u5230\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4e2d\uff0c\u4ee5\u7522\u751f\u6587\u5b57\u63cf\u8ff0\u3002\u7136\u5f8c\uff0c\u5f71\u50cf\u53ca\u5176\u5c0d\u61c9\u7684\u6587\u5b57\u63cf\u8ff0\u6703\u7531\u51cd\u7d50\u7684\u5f71\u50cf\u7de8\u78bc\u5668\u548c\u6587\u5b57\u7de8\u78bc\u5668\u8655\u7406\uff0c\u5206\u5225\u7522\u751f\u5f71\u50cf\u7279\u5fb5\u548c\u6587\u5b57\u7279\u5fb5\u3002\u6839\u64da\u591a\u6a21\u614b\u7279\u5fb5\u7bc0\u9ede\u7684\u76f8\u4f3c\u6027\u5efa\u69cb\u4e00\u500b\u5716\u5f62\uff0c\u4e26\u5f9e\u6bcf\u500b\u7bc0\u9ede\u4e2d\u8403\u53d6\u51fa\u8207\u9019\u4e9b\u7279\u5fb5\u76f8\u95dc\u7684\u77e5\u8b58\u548c\u95dc\u4fc2\u3002\u6b64\u5916\uff0c\u5f48\u6027\u6b0a\u91cd\u6574\u5408 (EWC) \u6b63\u5247\u5316\u6703\u7d0d\u5165\u640d\u5931\u51fd\u6578\u4e2d\uff0c\u4ee5\u6e1b\u8f15\u5728\u4efb\u52d9\u5b78\u7fd2\u671f\u9593\u907a\u5fd8\u7684\u554f\u984c\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728 OxfordPets\u3001Flowers102 \u548c Food101 \u8cc7\u6599\u96c6\u4e0a\u9054\u6210\u7684\u6e2c\u8a66\u6e96\u78ba\u5ea6\u5206\u5225\u63d0\u5347\u4e86 4.45%\u30012.92% \u548c 0.23%\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/yunche0/GA-Net/tree/master \u53d6\u5f97\u3002", "author": "Bin Cheng et.al.", "authors": "Bin Cheng, Jiaxuan Lu", "id": "2408.00290v1", "paper_url": "http://arxiv.org/abs/2408.00290v1", "repo": "null"}}