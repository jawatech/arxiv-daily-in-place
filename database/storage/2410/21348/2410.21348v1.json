{"2410.21348": {"publish_time": "2024-10-28", "title": "Large Language Model Benchmarks in Medical Tasks", "paper_summary": "With the increasing application of large language models (LLMs) in the\nmedical domain, evaluating these models' performance using benchmark datasets\nhas become crucial. This paper presents a comprehensive survey of various\nbenchmark datasets employed in medical LLM tasks. These datasets span multiple\nmodalities including text, image, and multimodal benchmarks, focusing on\ndifferent aspects of medical knowledge such as electronic health records\n(EHRs), doctor-patient dialogues, medical question-answering, and medical image\ncaptioning. The survey categorizes the datasets by modality, discussing their\nsignificance, data structure, and impact on the development of LLMs for\nclinical tasks such as diagnosis, report generation, and predictive decision\nsupport. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and\nCheXpert, which have facilitated advancements in tasks like medical report\ngeneration, clinical summarization, and synthetic data generation. The paper\nsummarizes the challenges and opportunities in leveraging these benchmarks for\nadvancing multimodal medical intelligence, emphasizing the need for datasets\nwith a greater degree of language diversity, structured omics data, and\ninnovative approaches to synthesis. This work also provides a foundation for\nfuture research in the application of LLMs in medicine, contributing to the\nevolving field of medical artificial intelligence.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u91ab\u7642\u9818\u57df\u7684\u61c9\u7528\u65e5\u76ca\u5ee3\u6cdb\uff0c\u4f7f\u7528\u57fa\u6e96\u8cc7\u6599\u96c6\u8a55\u4f30\u9019\u4e9b\u6a21\u578b\u7684\u6548\u80fd\u5df2\u8b8a\u5f97\u81f3\u95dc\u91cd\u8981\u3002\u672c\u6587\u5c0d\u7528\u65bc\u91ab\u7642 LLM \u4efb\u52d9\u7684\u5404\u7a2e\u57fa\u6e96\u8cc7\u6599\u96c6\u9032\u884c\u4e86\u5168\u9762\u7684\u8abf\u67e5\u3002\u9019\u4e9b\u8cc7\u6599\u96c6\u8de8\u8d8a\u591a\u7a2e\u6a21\u5f0f\uff0c\u5305\u62ec\u6587\u5b57\u3001\u5f71\u50cf\u548c\u591a\u6a21\u614b\u57fa\u6e96\uff0c\u91cd\u9ede\u95dc\u6ce8\u96fb\u5b50\u5065\u5eb7\u7d00\u9304 (EHR)\u3001\u91ab\u75c5\u5c0d\u8a71\u3001\u91ab\u7642\u554f\u7b54\u548c\u91ab\u7642\u5f71\u50cf\u6a19\u984c\u7b49\u91ab\u7642\u77e5\u8b58\u7684\u4e0d\u540c\u9762\u5411\u3002\u8abf\u67e5\u6309\u6a21\u5f0f\u5c0d\u8cc7\u6599\u96c6\u9032\u884c\u5206\u985e\uff0c\u8a0e\u8ad6\u5b83\u5011\u7684\u91cd\u8981\u6027\u3001\u8cc7\u6599\u7d50\u69cb\u548c\u5c0d\u7528\u65bc\u8a3a\u65b7\u3001\u5831\u544a\u751f\u6210\u548c\u9810\u6e2c\u6027\u6c7a\u7b56\u652f\u63f4\u7b49\u81e8\u5e8a\u4efb\u52d9\u7684 LLM \u958b\u767c\u7684\u5f71\u97ff\u3002\u4e3b\u8981\u57fa\u6e96\u5305\u62ec MIMIC-III\u3001MIMIC-IV\u3001BioASQ\u3001PubMedQA \u548c CheXpert\uff0c\u5b83\u5011\u4fc3\u8fdb\u4e86\u91ab\u7642\u5831\u544a\u751f\u6210\u3001\u81e8\u5e8a\u6458\u8981\u548c\u5408\u6210\u8cc7\u6599\u751f\u6210\u7b49\u4efb\u52d9\u7684\u9032\u5c55\u3002\u672c\u6587\u7e3d\u7d50\u4e86\u5229\u7528\u9019\u4e9b\u57fa\u6e96\u4f86\u63a8\u9032\u591a\u6a21\u614b\u91ab\u7642\u667a\u80fd\u7684\u6311\u6230\u548c\u6a5f\u9047\uff0c\u5f37\u8abf\u4e86\u5c0d\u5177\u6709\u66f4\u5927\u8a9e\u8a00\u591a\u6a23\u6027\u3001\u7d50\u69cb\u5316\u7d44\u5b78\u8cc7\u6599\u548c\u5275\u65b0\u5408\u6210\u65b9\u6cd5\u7684\u8cc7\u6599\u96c6\u7684\u9700\u6c42\u3002\u9019\u9805\u5de5\u4f5c\u4e5f\u70ba LLM \u5728\u91ab\u5b78\u4e2d\u7684\u61c9\u7528\u63d0\u4f9b\u4e86\u672a\u4f86\u7814\u7a76\u7684\u57fa\u790e\uff0c\u70ba\u91ab\u7642\u4eba\u5de5\u667a\u6167\u7684\u6f14\u9032\u9818\u57df\u505a\u51fa\u8ca2\u737b\u3002", "author": "Lawrence K. Q. Yan et.al.", "authors": "Lawrence K. Q. Yan, Ming Li, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Benji Peng, Ziqian Bi, Pohsun Feng, Keyu Chen, Junyu Liu, Qian Niu", "id": "2410.21348v1", "paper_url": "http://arxiv.org/abs/2410.21348v1", "repo": "null"}}