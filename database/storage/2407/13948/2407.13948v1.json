{"2407.13948": {"publish_time": "2024-07-18", "title": "Assurance of AI Systems From a Dependability Perspective", "paper_summary": "We outline the principles of classical assurance for computer-based systems\nthat pose significant risks. We then consider application of these principles\nto systems that employ Artificial Intelligence (AI) and Machine Learning (ML).\nA key element in this \"dependability\" perspective is a requirement to have\nnear-complete understanding of the behavior of critical components, and this is\nconsidered infeasible for AI and ML. Hence the dependability perspective aims\nto minimize trust in AI and ML elements by using \"defense in depth\" with a\nhierarchy of less complex systems, some of which may be highly assured\nconventionally engineered components, to \"guard\" them. This may be contrasted\nwith the \"trustworthy\" perspective that seeks to apply assurance to the AI and\nML elements themselves. In cyber-physical and many other systems, it is\ndifficult to provide guards that do not depend on AI and ML to perceive their\nenvironment (e.g., other vehicles sharing the road with a self-driving car), so\nboth perspectives are needed and there is a continuum or spectrum between them.\nWe focus on architectures toward the dependability end of the continuum and\ninvite others to consider additional points along the spectrum. For guards that\nrequire perception using AI and ML, we examine ways to minimize the trust\nplaced in these elements; they include diversity, defense in depth,\nexplanations, and micro-ODDs. We also examine methods to enforce acceptable\nbehavior, given a model of the world. These include classical cyber-physical\ncalculations and envelopes, and normative rules based on overarching\nprinciples, constitutions, ethics, or reputation. We apply our perspective to\nautonomous systems, AI systems for specific functions, generic AI such as Large\nLanguage Models, and to Artificial General Intelligence (AGI), and we propose\ncurrent best practice and an agenda for research.", "paper_summary_zh": "<paragraph>\u6211\u5011\u6982\u8ff0\u4e86\u5c0d\u69cb\u6210\u91cd\u5927\u98a8\u96aa\u7684\u96fb\u8166\u7cfb\u7d71\u7684\u50b3\u7d71\u4fdd\u8b49\u539f\u5247\u3002\u6211\u5011\u63a5\u8457\u8003\u616e\u5c07\u9019\u4e9b\u539f\u5247\u61c9\u7528\u65bc\u63a1\u7528\u4eba\u5de5\u667a\u6167 (AI) \u548c\u6a5f\u5668\u5b78\u7fd2 (ML) \u7684\u7cfb\u7d71\u3002\u9019\u500b\u300c\u53ef\u9760\u6027\u300d\u89c0\u9ede\u4e2d\u7684\u95dc\u9375\u8981\u7d20\u662f\u9700\u8981\u8fd1\u4e4e\u5b8c\u5168\u7406\u89e3\u95dc\u9375\u5143\u4ef6\u7684\u884c\u70ba\uff0c\u800c\u9019\u88ab\u8a8d\u70ba\u5c0d AI \u548c ML \u4f86\u8aaa\u662f\u4e0d\u53ef\u884c\u7684\u3002\u56e0\u6b64\uff0c\u53ef\u9760\u6027\u89c0\u9ede\u65e8\u5728\u900f\u904e\u300c\u7e31\u6df1\u9632\u79a6\u300d\u4f86\u6700\u5c0f\u5316\u5c0d AI \u548c ML \u5143\u4ef6\u7684\u4fe1\u4efb\uff0c\u4e26\u63a1\u7528\u8f03\u4e0d\u8907\u96dc\u7cfb\u7d71\u7684\u5c64\u7d1a\uff0c\u5176\u4e2d\u4e00\u4e9b\u7cfb\u7d71\u53ef\u80fd\u662f\u7d93\u904e\u9ad8\u5ea6\u4fdd\u8b49\u7684\u50b3\u7d71\u5de5\u7a0b\u5143\u4ef6\uff0c\u4ee5\u300c\u4fdd\u8b77\u300d\u5b83\u5011\u3002\u9019\u53ef\u80fd\u8207\u300c\u503c\u5f97\u4fe1\u8cf4\u300d\u89c0\u9ede\u5f62\u6210\u5c0d\u6bd4\uff0c\u5f8c\u8005\u5c0b\u6c42\u5c0d AI \u548c ML \u5143\u4ef6\u672c\u8eab\u65bd\u52a0\u4fdd\u8b49\u3002\u5728\u7db2\u8def\u5be6\u9ad4\u7cfb\u7d71\u548c\u8a31\u591a\u5176\u4ed6\u7cfb\u7d71\u4e2d\uff0c\u5f88\u96e3\u63d0\u4f9b\u4e0d\u4f9d\u8cf4 AI \u548c ML \u4f86\u611f\u77e5\u5176\u74b0\u5883\u7684\u9632\u8b77\uff08\u4f8b\u5982\uff0c\u8207\u81ea\u99d5\u8eca\u5171\u7528\u9053\u8def\u7684\u5176\u4ed6\u8eca\u8f1b\uff09\uff0c\u56e0\u6b64\u9700\u8981\u9019\u5169\u7a2e\u89c0\u9ede\uff0c\u4e14\u5b83\u5011\u4e4b\u9593\u5b58\u5728\u9023\u7e8c\u9ad4\u6216\u5149\u8b5c\u3002\u6211\u5011\u5c08\u6ce8\u65bc\u9023\u7e8c\u9ad4\u4e2d\u504f\u5411\u53ef\u9760\u6027\u7684\u67b6\u69cb\uff0c\u4e26\u9080\u8acb\u5176\u4ed6\u4eba\u8003\u616e\u5149\u8b5c\u4e2d\u7684\u5176\u4ed6\u89c0\u9ede\u3002\u5c0d\u65bc\u9700\u8981\u4f7f\u7528 AI \u548c ML \u4f86\u611f\u77e5\u7684\u9632\u8b77\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u6700\u5c0f\u5316\u5c0d\u9019\u4e9b\u5143\u4ef6\u7684\u4fe1\u4efb\u7684\u65b9\u6cd5\uff1b\u5b83\u5011\u5305\u62ec\u591a\u6a23\u6027\u3001\u7e31\u6df1\u9632\u79a6\u3001\u89e3\u91cb\u548c\u5fae\u578b ODD\u3002\u6211\u5011\u4e5f\u63a2\u8a0e\u4e86\u5728\u7d66\u5b9a\u4e16\u754c\u6a21\u578b\u7684\u60c5\u6cc1\u4e0b\u5f37\u5236\u57f7\u884c\u53ef\u63a5\u53d7\u884c\u70ba\u7684\u65b9\u6cd5\u3002\u9019\u4e9b\u65b9\u6cd5\u5305\u62ec\u50b3\u7d71\u7684\u7db2\u8def\u5be6\u9ad4\u8a08\u7b97\u548c\u5c01\u5957\uff0c\u4ee5\u53ca\u57fa\u65bc\u7d71\u7c4c\u539f\u5247\u3001\u61b2\u6cd5\u3001\u9053\u5fb7\u6216\u8072\u8b7d\u7684\u898f\u7bc4\u6027\u898f\u5247\u3002\u6211\u5011\u5c07\u6211\u5011\u7684\u89c0\u9ede\u61c9\u7528\u65bc\u81ea\u4e3b\u7cfb\u7d71\u3001\u7279\u5b9a\u529f\u80fd\u7684 AI \u7cfb\u7d71\u3001\u901a\u7528 AI\uff08\u4f8b\u5982\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff09\u548c\u4eba\u5de5\u901a\u7528\u667a\u6167 (AGI)\uff0c\u4e26\u63d0\u51fa\u76ee\u524d\u7684\u6700\u4f73\u5be6\u52d9\u548c\u7814\u7a76\u8b70\u7a0b\u3002</paragraph>", "author": "Robin Bloomfield et.al.", "authors": "Robin Bloomfield, John Rushby", "id": "2407.13948v1", "paper_url": "http://arxiv.org/abs/2407.13948v1", "repo": "null"}}