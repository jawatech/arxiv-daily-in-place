{"2411.05882": {"publish_time": "2024-11-08", "title": "When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization", "paper_summary": "Contemporary machine learning models, such as language models, are powerful,\nbut come with immense resource requirements both at training and inference\ntime. It has been shown that decoder-only language models can be trained to a\ncompetitive state with ternary weights (1.58 bits per weight), facilitating\nefficient inference. Here, we start our exploration with non-transformer model\narchitectures, investigating 1.58-bit training for multi-layer perceptrons and\ngraph neural networks. Then, we explore 1.58-bit training in other\ntransformer-based language models, namely encoder-only and encoder-decoder\nmodels. Our results show that in all of these settings, 1.58-bit training is on\npar with or sometimes even better than the standard 32/16-bit models.", "paper_summary_zh": "\u7576\u4ee3\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\uff08\u4f8b\u5982\u8a9e\u8a00\u6a21\u578b\uff09\u529f\u80fd\u5f37\u5927\uff0c\n\u4f46\u5728\u8a13\u7df4\u548c\u63a8\u8ad6\u6642\u9593\u4e0a\u90fd\u9700\u8981\u5927\u91cf\u7684\u8cc7\u6e90\u3002\u5df2\u7d93\u8b49\u660e\uff0c\u50c5\u89e3\u78bc\u5668\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u7528\u4e09\u5143\u6b0a\u91cd\uff08\u6bcf\u500b\u6b0a\u91cd 1.58 \u4f4d\u5143\uff09\u8a13\u7df4\u5230\u7af6\u722d\u72c0\u614b\uff0c\u4fc3\u9032\u6709\u6548\u7387\u7684\u63a8\u8ad6\u3002\u5728\u6b64\uff0c\u6211\u5011\u5f9e\u975eTransformer\u6a21\u578b\u67b6\u69cb\u958b\u59cb\u63a2\u8a0e\uff0c\u7814\u7a76\u591a\u5c64\u611f\u77e5\u5668\u548c\u5716\u795e\u7d93\u7db2\u8def\u7684 1.58 \u4f4d\u5143\u8a13\u7df4\u3002\u63a5\u8457\uff0c\u6211\u5011\u63a2\u8a0e\u5176\u4ed6\u57fa\u65bcTransformer\u7684\u8a9e\u8a00\u6a21\u578b\uff08\u5373\u50c5\u7de8\u78bc\u5668\u548c\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b\uff09\u7684 1.58 \u4f4d\u5143\u8a13\u7df4\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u5728\u6240\u6709\u9019\u4e9b\u8a2d\u5b9a\u4e2d\uff0c1.58 \u4f4d\u5143\u8a13\u7df4\u8207\u6a19\u6e96 32/16 \u4f4d\u5143\u6a21\u578b\u76f8\u7576\uff0c\u6709\u6642\u751a\u81f3\u66f4\u597d\u3002", "author": "Jacob Nielsen et.al.", "authors": "Jacob Nielsen, Lukas Galke, Peter Schneider-Kamp", "id": "2411.05882v1", "paper_url": "http://arxiv.org/abs/2411.05882v1", "repo": "null"}}