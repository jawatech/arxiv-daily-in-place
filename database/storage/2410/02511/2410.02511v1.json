{"2410.02511": {"publish_time": "2024-10-03", "title": "Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration", "paper_summary": "With expansive state-action spaces, efficient multi-agent exploration remains\na longstanding challenge in reinforcement learning. Although pursuing novelty,\ndiversity, or uncertainty attracts increasing attention, redundant efforts\nbrought by exploration without proper guidance choices poses a practical issue\nfor the community. This paper introduces a systematic approach, termed LEMAE,\nchoosing to channel informative task-relevant guidance from a knowledgeable\nLarge Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically,\nwe ground linguistic knowledge from LLM into symbolic key states, that are\ncritical for task fulfillment, in a discriminative manner at low LLM inference\ncosts. To unleash the power of key states, we design Subspace-based Hindsight\nIntrinsic Reward (SHIR) to guide agents toward key states by increasing reward\ndensity. Additionally, we build the Key State Memory Tree (KSMT) to track\ntransitions between key states in a specific task for organized exploration.\nBenefiting from diminishing redundant explorations, LEMAE outperforms existing\nSOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large\nmargin, achieving a 10x acceleration in certain scenarios.", "paper_summary_zh": "\u5728\u5ee3\u6cdb\u7684\u72c0\u614b\u52d5\u4f5c\u7a7a\u9593\u4e2d\uff0c\u6709\u6548\u7684\u591a\u667a\u80fd\u9ad4\u63a2\u7d22\u4ecd\u7136\u662f\u5f37\u5316\u5b78\u7fd2\u4e2d\u4e00\u500b\u9577\u671f\u7684\u6311\u6230\u3002\u5118\u7ba1\u8ffd\u6c42\u65b0\u5947\u6027\u3001\u591a\u6a23\u6027\u6216\u4e0d\u78ba\u5b9a\u6027\u5438\u5f15\u4e86\u8d8a\u4f86\u8d8a\u591a\u7684\u95dc\u6ce8\uff0c\u4f46\u63a2\u7d22\u5e36\u4f86\u7684\u91cd\u8907\u5de5\u4f5c\uff0c\u800c\u6c92\u6709\u9069\u7576\u7684\u6307\u5c0e\u9078\u64c7\uff0c\u5c0d\u793e\u7fa4\u4f86\u8aaa\u662f\u4e00\u500b\u5be6\u969b\u554f\u984c\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u7cfb\u7d71\u5316\u65b9\u6cd5\uff0c\u7a31\u70ba LEMAE\uff0c\u9078\u64c7\u5f9e\u77e5\u8b58\u6df5\u535a\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u5f15\u5c0e\u6709\u7528\u7684\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u6307\u5c0e\uff0c\u4ee5\u9032\u884c\u9ad8\u6548\u7684\u591a\u667a\u80fd\u9ad4\u63a2\u7d22\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4ee5\u4e00\u7a2e\u5340\u5225\u6027\u7684\u65b9\u5f0f\uff0c\u4ee5\u4f4e LLM \u63a8\u8ad6\u6210\u672c\uff0c\u5c07\u4f86\u81ea LLM \u7684\u8a9e\u8a00\u77e5\u8b58\u57fa\u790e\u5316\u70ba\u7b26\u865f\u95dc\u9375\u72c0\u614b\uff0c\u9019\u5c0d\u65bc\u4efb\u52d9\u7684\u5b8c\u6210\u81f3\u95dc\u91cd\u8981\u3002\u70ba\u4e86\u91cb\u653e\u95dc\u9375\u72c0\u614b\u7684\u529b\u91cf\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u57fa\u65bc\u5b50\u7a7a\u9593\u7684\u56de\u9867\u5167\u5728\u734e\u52f5 (SHIR)\uff0c\u901a\u904e\u589e\u52a0\u734e\u52f5\u5bc6\u5ea6\u4f86\u5f15\u5c0e\u667a\u80fd\u9ad4\u8d70\u5411\u95dc\u9375\u72c0\u614b\u3002\u6b64\u5916\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u95dc\u9375\u72c0\u614b\u8a18\u61b6\u6a39 (KSMT) \u4f86\u8ffd\u8e64\u7279\u5b9a\u4efb\u52d9\u4e2d\u95dc\u9375\u72c0\u614b\u4e4b\u9593\u7684\u8f49\u63db\uff0c\u4ee5\u9032\u884c\u6709\u7d44\u7e54\u7684\u63a2\u7d22\u3002\u53d7\u76ca\u65bc\u6e1b\u5c11\u91cd\u8907\u63a2\u7d22\uff0cLEMAE \u5728\u5177\u6709\u6311\u6230\u6027\u7684\u57fa\u6e96\u6e2c\u8a66\uff08\u4f8b\u5982 SMAC \u548c MPE\uff09\u4e0a\u512a\u65bc\u73fe\u6709\u7684 SOTA \u65b9\u6cd5\uff0c\u5728\u67d0\u4e9b\u5834\u666f\u4e2d\u5be6\u73fe\u4e86 10 \u500d\u7684\u52a0\u901f\u3002", "author": "Yun Qu et.al.", "authors": "Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhun Shao, Yixiu Mao, Cheems Wang, Chang Liu, Xiangyang Ji", "id": "2410.02511v1", "paper_url": "http://arxiv.org/abs/2410.02511v1", "repo": "https://github.com/hijkzzz/pymarl2"}}