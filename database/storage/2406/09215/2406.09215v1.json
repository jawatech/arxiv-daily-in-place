{"2406.09215": {"publish_time": "2024-06-13", "title": "On Softmax Direct Preference Optimization for Recommendation", "paper_summary": "Recommender systems aim to predict personalized rankings based on user\npreference data. With the rise of Language Models (LMs), LM-based recommenders\nhave been widely explored due to their extensive world knowledge and powerful\nreasoning abilities. Most of the LM-based recommenders convert historical\ninteractions into language prompts, pairing with a positive item as the target\nresponse and fine-tuning LM with a language modeling loss. However, the current\nobjective fails to fully leverage preference data and is not optimized for\npersonalized ranking tasks, which hinders the performance of LM-based\nrecommenders. Inspired by the current advancement of Direct Preference\nOptimization (DPO) in human preference alignment and the success of softmax\nloss in recommendations, we propose Softmax-DPO (\\textbf{S-DPO}) to instill\nranking information into the LM to help LM-based recommenders distinguish\npreferred items from negatives, rather than solely focusing on positives.\nSpecifically, we incorporate multiple negatives in user preference data and\ndevise an alternative version of DPO loss tailored for LM-based recommenders,\nconnected to softmax sampling strategies. Theoretically, we bridge S-DPO with\nthe softmax loss over negative sampling and find that it has a side effect of\nmining hard negatives, which assures its exceptional capabilities in\nrecommendation tasks. Empirically, extensive experiments conducted on three\nreal-world datasets demonstrate the superiority of S-DPO to effectively model\nuser preference and further boost recommendation performance while mitigating\nthe data likelihood decline issue of DPO. Our codes are available at\nhttps://github.com/chenyuxin1999/S-DPO.", "paper_summary_zh": "\u63a8\u85a6\u7cfb\u7d71\u65e8\u5728\u6839\u64da\u4f7f\u7528\u8005\u504f\u597d\u8cc7\u6599\u9810\u6e2c\u500b\u4eba\u5316\u6392\u540d\u3002\u96a8\u8457\u8a9e\u8a00\u6a21\u578b (LM) \u7684\u8208\u8d77\uff0c\u57fa\u65bc LM \u7684\u63a8\u85a6\u7cfb\u7d71\u56e0\u5176\u5ee3\u6cdb\u7684\u4e16\u754c\u77e5\u8b58\u548c\u5f37\u5927\u7684\u63a8\u7406\u80fd\u529b\u800c\u88ab\u5ee3\u6cdb\u63a2\u7d22\u3002\u5927\u591a\u6578\u57fa\u65bc LM \u7684\u63a8\u85a6\u7cfb\u7d71\u6703\u5c07\u6b77\u53f2\u4e92\u52d5\u8f49\u63db\u70ba\u8a9e\u8a00\u63d0\u793a\uff0c\u8207\u6b63\u5411\u9805\u76ee\u914d\u5c0d\u4f5c\u70ba\u76ee\u6a19\u56de\u61c9\uff0c\u4e26\u4f7f\u7528\u8a9e\u8a00\u5efa\u6a21\u640d\u5931\u5fae\u8abf LM\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u76ee\u6a19\u672a\u80fd\u5145\u5206\u5229\u7528\u504f\u597d\u8cc7\u6599\uff0c\u4e14\u672a\u91dd\u5c0d\u500b\u4eba\u5316\u6392\u540d\u4efb\u52d9\u9032\u884c\u6700\u4f73\u5316\uff0c\u9019\u963b\u7919\u4e86\u57fa\u65bc LM \u7684\u63a8\u85a6\u7cfb\u7d71\u7684\u6548\u80fd\u3002\u53d7\u5230\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u4e2d\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u7684\u6700\u65b0\u9032\u5c55\u4ee5\u53ca\u63a8\u85a6\u4e2d softmax \u640d\u5931\u6210\u529f\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa Softmax-DPO (\\textbf{S-DPO})\uff0c\u5c07\u6392\u540d\u8cc7\u8a0a\u704c\u8f38\u5230 LM \u4e2d\uff0c\u4ee5\u5354\u52a9\u57fa\u65bc LM \u7684\u63a8\u85a6\u7cfb\u7d71\u5340\u5206\u504f\u597d\u7684\u9805\u76ee\u548c\u8ca0\u9762\u9805\u76ee\uff0c\u800c\u975e\u50c5\u5c08\u6ce8\u65bc\u6b63\u5411\u9805\u76ee\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5728\u4f7f\u7528\u8005\u504f\u597d\u8cc7\u6599\u4e2d\u7d0d\u5165\u591a\u500b\u8ca0\u9762\u9805\u76ee\uff0c\u4e26\u8a2d\u8a08\u51fa\u91dd\u5c0d\u57fa\u65bc LM \u7684\u63a8\u85a6\u7cfb\u7d71\u91cf\u8eab\u6253\u9020\u7684 DPO \u640d\u5931\u66ff\u4ee3\u7248\u672c\uff0c\u4e26\u8207 softmax \u53d6\u6a23\u7b56\u7565\u76f8\u9023\u3002\u5728\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u5c07 S-DPO \u8207\u8ca0\u9762\u53d6\u6a23\u4e0a\u7684 softmax \u640d\u5931\u7d50\u5408\u8d77\u4f86\uff0c\u4e26\u767c\u73fe\u5b83\u5177\u6709\u6316\u6398\u56f0\u96e3\u8ca0\u9762\u9805\u76ee\u7684\u526f\u4f5c\u7528\uff0c\u9019\u78ba\u4fdd\u4e86\u5b83\u5728\u63a8\u85a6\u4efb\u52d9\u4e2d\u7684\u5353\u8d8a\u80fd\u529b\u3002\u5728\u7d93\u9a57\u4e0a\uff0c\u5728\u4e09\u500b\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 S-DPO \u5728\u6709\u6548\u5efa\u6a21\u4f7f\u7528\u8005\u504f\u597d\u65b9\u9762\u7684\u512a\u8d8a\u6027\uff0c\u4e26\u9032\u4e00\u6b65\u63d0\u5347\u4e86\u63a8\u85a6\u6548\u80fd\uff0c\u540c\u6642\u6e1b\u8f15\u4e86 DPO \u7684\u8cc7\u6599\u53ef\u80fd\u6027\u4e0b\u964d\u554f\u984c\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/chenyuxin1999/S-DPO \u53d6\u5f97\u3002", "author": "Yuxin Chen et.al.", "authors": "Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, Tat-Seng Chua", "id": "2406.09215v1", "paper_url": "http://arxiv.org/abs/2406.09215v1", "repo": "https://github.com/chenyuxin1999/s-dpo"}}