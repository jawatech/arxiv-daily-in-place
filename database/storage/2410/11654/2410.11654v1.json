{"2410.11654": {"publish_time": "2024-10-15", "title": "Transformer Layer Injection: A Novel Approach for Efficient Upscaling of Large Language Models", "paper_summary": "In this paper, we propose Transformer Layer Injection (TLI), a novel method\nfor efficiently upscaling large language models (LLMs) while minimizing\ncomputational costs and maintaining model performance. Model scale is a key\nfactor in enhancing the quality of machine learning models, and TLI addresses\nthe challenge of scaling by reducing initial loss, minimizing fine-tuning\nrequirements, and preserving model complexity. Our approach improves upon the\nconventional Depth Up-Scaling (DUS) technique by injecting new layers into\nevery set of K layers, enabling hidden representations to pass through\ntransformer blocks with minimal disruption. We compare TLI with existing\napproaches, including Mixture of Experts (MoE) and DUS, and validate its\nefficiency through experiments on small LLMs (LLama3 1B, 3B, and 8B). Results\nshow that TLI achieves better initialization, requires fewer training steps,\nand delivers superior accuracy on tasks such as KoBEST and KMCQA, with models\nperforming effectively even without additional training. TLI is demonstrated to\nbe both data-efficient and cost-effective, significantly outperforming existing\nmethods. Its scalability and simplicity make it a promising solution for\nupscaling transformer-based models, with potential applications in scaling\nmodels from 10B to 405B parameters.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa Transformer Layer Injection (TLI)\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u65bc\u6709\u6548\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u540c\u6642\u5c07\u904b\u7b97\u6210\u672c\u964d\u5230\u6700\u4f4e\uff0c\u4e26\u7dad\u6301\u6a21\u578b\u6548\u80fd\u3002\u6a21\u578b\u898f\u6a21\u662f\u63d0\u5347\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u54c1\u8cea\u7684\u95dc\u9375\u56e0\u7d20\uff0c\u800c TLI \u900f\u904e\u6e1b\u5c11\u521d\u59cb\u640d\u5931\u3001\u5c07\u5fae\u8abf\u9700\u6c42\u964d\u5230\u6700\u4f4e\uff0c\u4ee5\u53ca\u4fdd\u7559\u6a21\u578b\u8907\u96dc\u5ea6\uff0c\u4f86\u89e3\u6c7a\u64f4\u5145\u7684\u6311\u6230\u3002\u6211\u5011\u7684\u505a\u6cd5\u900f\u904e\u5728\u6bcf\u4e00\u7d44 K \u5c64\u4e2d\u6ce8\u5165\u65b0\u5c64\uff0c\u9032\u800c\u6539\u5584\u50b3\u7d71\u6df1\u5ea6\u64f4\u5145 (DUS) \u6280\u8853\uff0c\u8b93\u96b1\u85cf\u5f0f\u8868\u793a\u80fd\u5920\u4ee5\u6700\u5c0f\u7684\u4e2d\u65b7\u901a\u904e Transformer \u5340\u584a\u3002\u6211\u5011\u5c07 TLI \u8207\u73fe\u6709\u65b9\u6cd5\uff08\u5305\u62ec Mixture of Experts (MoE) \u548c DUS\uff09\u9032\u884c\u6bd4\u8f03\uff0c\u4e26\u900f\u904e\u91dd\u5c0d\u5c0f\u578b LLM\uff08LLama3 1B\u30013B \u548c 8B\uff09\u9032\u884c\u7684\u5be6\u9a57\u9a57\u8b49\u5176\u6548\u7387\u3002\u7d50\u679c\u986f\u793a\uff0cTLI \u53ef\u9054\u6210\u66f4\u597d\u7684\u521d\u59cb\u5316\uff0c\u6240\u9700\u7684\u8a13\u7df4\u6b65\u9a5f\u8f03\u5c11\uff0c\u4e14\u5728 KoBEST \u548c KMCQA \u7b49\u4efb\u52d9\u4e0a\u63d0\u4f9b\u512a\u7570\u7684\u6e96\u78ba\u5ea6\uff0c\u5373\u4f7f\u5728\u6c92\u6709\u984d\u5916\u8a13\u7df4\u7684\u60c5\u6cc1\u4e0b\uff0c\u6a21\u578b\u4e5f\u80fd\u6709\u6548\u57f7\u884c\u3002TLI \u5df2\u88ab\u8b49\u660e\u8cc7\u6599\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u517c\u5177\uff0c\u5927\u5e45\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002\u5176\u53ef\u64f4\u5145\u6027\u548c\u7c21\u4fbf\u6027\u4f7f\u5176\u6210\u70ba\u64f4\u5145\u57fa\u65bc Transformer \u7684\u6a21\u578b\u7684\u7d55\u4f73\u89e3\u6c7a\u65b9\u6848\uff0c\u5728\u5c07\u6a21\u578b\u5f9e 10B \u64f4\u5145\u5230 405B \u53c3\u6578\u65b9\u9762\u5177\u6709\u6f5b\u5728\u61c9\u7528\u3002</paragraph>", "author": "James Vo et.al.", "authors": "James Vo", "id": "2410.11654v1", "paper_url": "http://arxiv.org/abs/2410.11654v1", "repo": "null"}}