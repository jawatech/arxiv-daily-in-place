{"2405.02774": {"publish_time": "2024-05-05", "title": "Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs", "paper_summary": "This work focuses on leveraging and selecting from vast, unlabeled, open data\nto pre-fine-tune a pre-trained language model. The goal is to minimize the need\nfor costly domain-specific data for subsequent fine-tuning while achieving\ndesired performance levels. While many data selection algorithms have been\ndesigned for small-scale applications, rendering them unsuitable for our\ncontext, some emerging methods do cater to language data scales. However, they\noften prioritize data that aligns with the target distribution. While this\nstrategy may be effective when training a model from scratch, it can yield\nlimited results when the model has already been pre-trained on a different\ndistribution. Differing from prior work, our key idea is to select data that\nnudges the pre-training distribution closer to the target distribution. We show\nthe optimality of this approach for fine-tuning tasks under certain conditions.\nWe demonstrate the efficacy of our methodology across a diverse array of tasks\n(NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently\nsurpasses other selection methods. Moreover, our proposed method is\nsignificantly faster than existing techniques, scaling to millions of samples\nwithin a single GPU hour. Our code is open-sourced (Code repository:\nhttps://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers\nsignificant potential for enhancing performance across diverse tasks, its\nassociated costs often limit its widespread adoption; with this work, we hope\nto lay the groundwork for cost-effective fine-tuning, making its benefits more\naccessible.", "paper_summary_zh": "", "author": "Feiyang Kang et.al.", "authors": "Feiyang Kang,Hoang Anh Just,Yifan Sun,Himanshu Jahagirdar,Yuanzhi Zhang,Rongxing Du,Anit Kumar Sahu,Ruoxi Jia", "id": "2405.02774v1", "paper_url": "http://arxiv.org/abs/2405.02774v1", "repo": "null"}}