{"2405.20867": {"publish_time": "2024-05-31", "title": "Automatic Channel Pruning for Multi-Head Attention", "paper_summary": "Despite the strong performance of Transformers, their quadratic computation\ncomplexity presents challenges in applying them to vision tasks. Automatic\npruning is one of effective methods for reducing computation complexity without\nheuristic approaches. However, directly applying it to multi-head attention is\nnot straightforward due to channel misalignment. In this paper, we propose an\nautomatic channel pruning method to take into account the multi-head attention\nmechanism. First, we incorporate channel similarity-based weights into the\npruning indicator to preserve more informative channels in each head. Then, we\nadjust pruning indicator to enforce removal of channels in equal proportions\nacross all heads, preventing the channel misalignment. We also add a reweight\nmodule to compensate for information loss resulting from channel removal, and\nan effective initialization step for pruning indicator based on difference of\nattention between original structure and each channel. Our proposed method can\nbe used to not only original attention, but also linear attention, which is\nmore efficient as linear complexity with respect to the number of tokens. On\nImageNet-1K, applying our pruning method to the FLattenTransformer, which\nincludes both attention mechanisms, shows outperformed accuracy for several\nMACs compared with previous state-of-the-art efficient models and pruned\nmethods. Code will be available soon.", "paper_summary_zh": "\u5118\u7ba1 Transformer \u6709\u5f37\u52c1\u7684\u6548\u80fd\uff0c\u4f46\u5176\u4e8c\u6b21\u904b\u7b97\u8907\u96dc\u5ea6\u5728\u61c9\u7528\u65bc\u8996\u89ba\u4efb\u52d9\u6642\u6703\u7522\u751f\u6311\u6230\u3002\u81ea\u52d5\u526a\u679d\u662f\u964d\u4f4e\u904b\u7b97\u8907\u96dc\u5ea6\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u4e14\u7121\u9700\u4f7f\u7528\u555f\u767c\u5f0f\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u7531\u65bc\u901a\u9053\u672a\u5c0d\u9f4a\uff0c\u7121\u6cd5\u76f4\u63a5\u5c07\u5176\u61c9\u7528\u65bc\u591a\u982d\u6ce8\u610f\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u81ea\u52d5\u901a\u9053\u526a\u679d\u65b9\u6cd5\u4f86\u8003\u91cf\u591a\u982d\u6ce8\u610f\u529b\u6a5f\u5236\u3002\u9996\u5148\uff0c\u6211\u5011\u5c07\u57fa\u65bc\u901a\u9053\u76f8\u4f3c\u6027\u7684\u6b0a\u91cd\u7d0d\u5165\u526a\u679d\u6307\u6a19\uff0c\u4ee5\u4fdd\u7559\u6bcf\u500b\u982d\u4e2d\u66f4\u591a\u6709\u8cc7\u8a0a\u6027\u7684\u901a\u9053\u3002\u7136\u5f8c\uff0c\u6211\u5011\u8abf\u6574\u526a\u679d\u6307\u6a19\uff0c\u4ee5\u5f37\u5236\u5728\u6240\u6709\u982d\u4e2d\u6309\u76f8\u7b49\u6bd4\u4f8b\u79fb\u9664\u901a\u9053\uff0c\u9632\u6b62\u901a\u9053\u672a\u5c0d\u9f4a\u3002\u6211\u5011\u9084\u65b0\u589e\u4e00\u500b\u91cd\u65b0\u52a0\u6b0a\u6a21\u7d44\uff0c\u4ee5\u88dc\u511f\u56e0\u901a\u9053\u79fb\u9664\u800c\u9020\u6210\u7684\u8cc7\u8a0a\u640d\u5931\uff0c\u4ee5\u53ca\u4e00\u500b\u57fa\u65bc\u539f\u59cb\u7d50\u69cb\u8207\u6bcf\u500b\u901a\u9053\u4e4b\u9593\u6ce8\u610f\u529b\u7684\u5dee\u7570\u7684\u526a\u679d\u6307\u6a19\u6709\u6548\u521d\u59cb\u5316\u6b65\u9a5f\u3002\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u50c5\u53ef\u7528\u65bc\u539f\u59cb\u6ce8\u610f\u529b\uff0c\u9084\u80fd\u7528\u65bc\u7dda\u6027\u6ce8\u610f\u529b\uff0c\u5176\u6548\u7387\u66f4\u9ad8\uff0c\u56e0\u70ba\u5176\u7dda\u6027\u8907\u96dc\u5ea6\u8207 token \u6578\u91cf\u6709\u95dc\u3002\u5728 ImageNet-1K \u4e0a\uff0c\u5c07\u6211\u5011\u7684\u526a\u679d\u65b9\u6cd5\u61c9\u7528\u65bc\u5305\u542b\u5169\u7a2e\u6ce8\u610f\u529b\u6a5f\u5236\u7684 FLattenTransformer\uff0c\u8207\u5148\u524d\u7684\u6700\u5148\u9032\u9ad8\u6548\u6a21\u578b\u548c\u526a\u679d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c0d\u65bc\u591a\u500b MAC \u986f\u793a\u51fa\u512a\u7570\u7684\u6e96\u78ba\u5ea6\u3002\u7a0b\u5f0f\u78bc\u5c07\u5f88\u5feb\u63d0\u4f9b\u3002", "author": "Eunho Lee et.al.", "authors": "Eunho Lee, Youngbae Hwang", "id": "2405.20867v1", "paper_url": "http://arxiv.org/abs/2405.20867v1", "repo": "null"}}