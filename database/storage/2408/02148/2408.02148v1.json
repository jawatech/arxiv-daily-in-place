{"2408.02148": {"publish_time": "2024-08-04", "title": "Environment Complexity and Nash Equilibria in a Sequential Social Dilemma", "paper_summary": "Multi-agent reinforcement learning (MARL) methods, while effective in\nzero-sum or positive-sum games, often yield suboptimal outcomes in general-sum\ngames where cooperation is essential for achieving globally optimal outcomes.\nMatrix game social dilemmas, which abstract key aspects of general-sum\ninteractions, such as cooperation, risk, and trust, fail to model the temporal\nand spatial dynamics characteristic of real-world scenarios. In response, our\nstudy extends matrix game social dilemmas into more complex, higher-dimensional\nMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma\nto more closely match the decision-space of a one-shot matrix game while also\nintroducing variable environment complexity. Our findings indicate that as\ncomplexity increases, MARL agents trained in these environments converge to\nsuboptimal strategies, consistent with the risk-dominant Nash equilibria\nstrategies found in matrix games. Our work highlights the impact of environment\ncomplexity on achieving optimal outcomes in higher-dimensional game-theoretic\nMARL environments.", "paper_summary_zh": "\u591a\u667a\u80fd\u9ad4\u5f37\u5316\u5b78\u7fd2 (MARL) \u65b9\u6cd5\u5728\u96f6\u548c\u6216\u6b63\u548c\u904a\u6232\u4e2d\u96d6\u7136\u6709\u6548\uff0c\u4f46\u5728\u5408\u4f5c\u5c0d\u9054\u6210\u5168\u7403\u6700\u4f73\u7d50\u679c\u81f3\u95dc\u91cd\u8981\u7684\u7e3d\u548c\u904a\u6232\u4e2d\uff0c\u5f80\u5f80\u6703\u7522\u751f\u6b21\u4f73\u7d50\u679c\u3002\u77e9\u9663\u904a\u6232\u4e2d\u7684\u793e\u6703\u56f0\u5883\uff0c\u62bd\u8c61\u5316\u4e86\u7e3d\u548c\u4e92\u52d5\u4e2d\u7684\u95dc\u9375\u9762\u5411\uff0c\u4f8b\u5982\u5408\u4f5c\u3001\u98a8\u96aa\u548c\u4fe1\u4efb\uff0c\u4f46\u7121\u6cd5\u6a21\u64ec\u771f\u5be6\u4e16\u754c\u60c5\u5883\u4e2d\u7279\u6709\u7684\u6642\u9593\u548c\u7a7a\u9593\u52d5\u614b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u7684\u7814\u7a76\u5c07\u77e9\u9663\u904a\u6232\u4e2d\u7684\u793e\u6703\u56f0\u5883\u64f4\u5c55\u5230\u66f4\u8907\u96dc\u3001\u66f4\u9ad8\u7dad\u5ea6\u7684 MARL \u74b0\u5883\u4e2d\u3002\u6211\u5011\u8abf\u6574\u4e86 Stag Hunt \u56f0\u5883\u7684\u7db2\u683c\u4e16\u754c\u5be6\u4f5c\uff0c\u8b93\u5b83\u66f4\u63a5\u8fd1\u4e00\u6b21\u6027\u77e9\u9663\u904a\u6232\u7684\u6c7a\u7b56\u7a7a\u9593\uff0c\u540c\u6642\u4e5f\u5f15\u5165\u4e86\u53ef\u8b8a\u74b0\u5883\u8907\u96dc\u5ea6\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u96a8\u8457\u8907\u96dc\u5ea6\u7684\u589e\u52a0\uff0c\u5728\u9019\u4e9b\u74b0\u5883\u4e2d\u8a13\u7df4\u7684 MARL \u667a\u80fd\u9ad4\u6703\u6536\u6582\u5230\u6b21\u4f73\u7b56\u7565\uff0c\u9019\u8207\u77e9\u9663\u904a\u6232\u4e2d\u767c\u73fe\u7684\u98a8\u96aa\u4e3b\u5c0e\u7d0d\u8a31\u5747\u8861\u7b56\u7565\u4e00\u81f4\u3002\u6211\u5011\u7684\u7814\u7a76\u7a81\u986f\u4e86\u74b0\u5883\u8907\u96dc\u5ea6\u5c0d\u5728\u66f4\u9ad8\u7dad\u5ea6\u535a\u5f08\u8ad6 MARL \u74b0\u5883\u4e2d\u9054\u6210\u6700\u4f73\u7d50\u679c\u7684\u5f71\u97ff\u3002", "author": "Mustafa Yasir et.al.", "authors": "Mustafa Yasir, Andrew Howes, Vasilios Mavroudis, Chris Hicks", "id": "2408.02148v1", "paper_url": "http://arxiv.org/abs/2408.02148v1", "repo": "null"}}