{"2411.17945": {"publish_time": "2024-11-26", "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation", "paper_summary": "Generating high-fidelity 3D content from text prompts remains a significant\nchallenge in computer vision due to the limited size, diversity, and annotation\ndepth of the existing datasets. To address this, we introduce MARVEL-40M+, an\nextensive dataset with 40 million text annotations for over 8.9 million 3D\nassets aggregated from seven major 3D datasets. Our contribution is a novel\nmulti-stage annotation pipeline that integrates open-source pretrained\nmulti-view VLMs and LLMs to automatically produce multi-level descriptions,\nranging from detailed (150-200 words) to concise semantic tags (10-20 words).\nThis structure supports both fine-grained 3D reconstruction and rapid\nprototyping. Furthermore, we incorporate human metadata from source datasets\ninto our annotation pipeline to add domain-specific information in our\nannotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,\na two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our\nannotations and use a pretrained image-to-3D network to generate 3D textured\nmeshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly\noutperforms existing datasets in annotation quality and linguistic diversity,\nachieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators.", "paper_summary_zh": "\u5f9e\u6587\u5b57\u63d0\u793a\u7522\u751f\u9ad8\u4fdd\u771f 3D \u5167\u5bb9\u5728\u96fb\u8166\u8996\u89ba\u4e2d\u4ecd\u7136\u662f\u4e00\u500b\u91cd\u5927\u6311\u6230\uff0c\u56e0\u70ba\u73fe\u6709\u8cc7\u6599\u96c6\u7684\u5927\u5c0f\u3001\u591a\u6a23\u6027\u548c\u8a3b\u89e3\u6df1\u5ea6\u6709\u9650\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 MARVEL-40M+\uff0c\u4e00\u500b\u5305\u542b 4000 \u842c\u500b\u6587\u5b57\u8a3b\u89e3\u7684\u5ee3\u6cdb\u8cc7\u6599\u96c6\uff0c\u9019\u4e9b\u8a3b\u89e3\u9069\u7528\u65bc\u5f9e\u4e03\u500b\u4e3b\u8981 3D \u8cc7\u6599\u96c6\u5f59\u7e3d\u7684 890 \u842c\u500b 3D \u8cc7\u7522\u3002\u6211\u5011\u7684\u8ca2\u737b\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u591a\u968e\u6bb5\u8a3b\u89e3\u7ba1\u9053\uff0c\u5b83\u6574\u5408\u4e86\u958b\u653e\u539f\u59cb\u78bc\u7684\u9810\u8a13\u7df4\u591a\u8996\u5716 VLM \u548c LLM\uff0c\u4ee5\u81ea\u52d5\u7522\u751f\u591a\u5c64\u7d1a\u63cf\u8ff0\uff0c\u5f9e\u8a73\u7d30\uff08150-200 \u500b\u5b57\uff09\u5230\u7c21\u6f54\u7684\u8a9e\u7fa9\u6a19\u7c64\uff0810-20 \u500b\u5b57\uff09\u3002\u6b64\u7d50\u69cb\u652f\u63f4\u7d30\u7c92\u5ea6\u7684 3D \u91cd\u5efa\u548c\u5feb\u901f\u539f\u578b\u88fd\u4f5c\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07\u4f86\u81ea\u4f86\u6e90\u8cc7\u6599\u96c6\u7684\u4eba\u985e\u5143\u8cc7\u6599\u7d0d\u5165\u6211\u5011\u7684\u8a3b\u89e3\u7ba1\u9053\uff0c\u4ee5\u5728\u6211\u5011\u7684\u8a3b\u89e3\u4e2d\u65b0\u589e\u7279\u5b9a\u65bc\u9818\u57df\u7684\u8cc7\u8a0a\u4e26\u6e1b\u5c11 VLM \u5e7b\u89ba\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86 MARVEL-FX3D\uff0c\u4e00\u500b\u5169\u968e\u6bb5\u7684\u6587\u5b57\u5230 3D \u7ba1\u9053\u3002\u6211\u5011\u4f7f\u7528\u6211\u5011\u7684\u8a3b\u89e3\u5fae\u8abf Stable Diffusion\uff0c\u4e26\u4f7f\u7528\u9810\u8a13\u7df4\u7684\u5f71\u50cf\u5230 3D \u7db2\u8def\u5728 15 \u79d2\u5167\u7522\u751f 3D \u7d0b\u7406\u7db2\u683c\u3002\u5ee3\u6cdb\u7684\u8a55\u4f30\u986f\u793a\uff0cMARVEL-40M+ \u5728\u8a3b\u89e3\u54c1\u8cea\u548c\u8a9e\u8a00\u591a\u6a23\u6027\u65b9\u9762\u660e\u986f\u512a\u65bc\u73fe\u6709\u7684\u8cc7\u6599\u96c6\uff0cGPT-4 \u7684\u7372\u52dd\u7387\u9054\u5230 72.41%\uff0c\u4eba\u985e\u8a55\u4f30\u8005\u7684\u7372\u52dd\u7387\u9054\u5230 73.40%\u3002", "author": "Sankalp Sinha et.al.", "authors": "Sankalp Sinha, Mohammad Sadil Khan, Muhammad Usama, Shino Sam, Didier Stricker, Sk Aziz Ali, Muhammad Zeshan Afzal", "id": "2411.17945v1", "paper_url": "http://arxiv.org/abs/2411.17945v1", "repo": "null"}}