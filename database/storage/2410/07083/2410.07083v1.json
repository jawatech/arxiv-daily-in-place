{"2410.07083": {"publish_time": "2024-10-09", "title": "Stanceformer: Target-Aware Transformer for Stance Detection", "paper_summary": "The task of Stance Detection involves discerning the stance expressed in a\ntext towards a specific subject or target. Prior works have relied on existing\ntransformer models that lack the capability to prioritize targets effectively.\nConsequently, these models yield similar performance regardless of whether we\nutilize or disregard target information, undermining the task's significance.\nTo address this challenge, we introduce Stanceformer, a target-aware\ntransformer model that incorporates enhanced attention towards the targets\nduring both training and inference. Specifically, we design a \\textit{Target\nAwareness} matrix that increases the self-attention scores assigned to the\ntargets. We demonstrate the efficacy of the Stanceformer with various\nBERT-based models, including state-of-the-art models and Large Language Models\n(LLMs), and evaluate its performance across three stance detection datasets,\nalongside a zero-shot dataset. Our approach Stanceformer not only provides\nsuperior performance but also generalizes even to other domains, such as\nAspect-based Sentiment Analysis. We make the code publicly\navailable.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}", "paper_summary_zh": "\u7acb\u5834\u5075\u6e2c\u7684\u4efb\u52d9\u6d89\u53ca\u5206\u8fa8\u6587\u672c\u4e2d\u91dd\u5c0d\u7279\u5b9a\u4e3b\u65e8\u6216\u76ee\u6a19\u6240\u8868\u9054\u7684\u7acb\u5834\u3002\u5148\u524d\u7684\u8457\u4f5c\u4ef0\u8cf4\u73fe\u6709\u7684\u8f49\u63db\u5668\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u7f3a\u4e4f\u6709\u6548\u512a\u5148\u8655\u7406\u76ee\u6a19\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9019\u4e9b\u6a21\u578b\u7522\u751f\u985e\u4f3c\u7684\u6548\u80fd\uff0c\u7121\u8ad6\u6211\u5011\u662f\u5426\u4f7f\u7528\u6216\u5ffd\u7565\u76ee\u6a19\u8cc7\u8a0a\uff0c\u90fd\u640d\u5bb3\u4e86\u4efb\u52d9\u7684\u91cd\u8981\u6027\u3002\u70ba\u4e86\u61c9\u5c0d\u6b64\u6311\u6230\uff0c\u6211\u5011\u5f15\u9032\u4e86\u7acb\u5834\u8f49\u63db\u5668\uff0c\u4e00\u7a2e\u76ee\u6a19\u611f\u77e5\u8f49\u63db\u5668\u6a21\u578b\uff0c\u5728\u8a13\u7df4\u548c\u63a8\u8ad6\u671f\u9593\u90fd\u7d0d\u5165\u4e86\u5c0d\u76ee\u6a19\u7684\u52a0\u5f37\u95dc\u6ce8\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u300c\u76ee\u6a19\u611f\u77e5\u300d\u77e9\u9663\uff0c\u4ee5\u589e\u52a0\u6307\u5b9a\u7d66\u76ee\u6a19\u7684\u81ea\u6ce8\u610f\u529b\u5206\u6578\u3002\u6211\u5011\u5c55\u793a\u4e86\u7acb\u5834\u8f49\u63db\u5668\u8207\u5404\u7a2e\u57fa\u65bc BERT \u7684\u6a21\u578b\u7684\u6548\u80fd\uff0c\u5305\u62ec\u6700\u5148\u9032\u7684\u6a21\u578b\u548c\u5927\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4e26\u8a55\u4f30\u5176\u5728\u4e09\u500b\u7acb\u5834\u5075\u6e2c\u8cc7\u6599\u96c6\u4ee5\u53ca\u96f6\u6b21\u5b78\u7fd2\u8cc7\u6599\u96c6\u4e2d\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u7acb\u5834\u8f49\u63db\u5668\u65b9\u6cd5\u4e0d\u50c5\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u751a\u81f3\u53ef\u4ee5\u6982\u62ec\u5230\u5176\u4ed6\u9818\u57df\uff0c\u4f8b\u5982\u57fa\u65bc\u9762\u5411\u7684\u89c0\u9ede\u5206\u6790\u3002\u6211\u5011\u516c\u958b\u4e86\u7a0b\u5f0f\u78bc\u3002\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}", "author": "Krishna Garg et.al.", "authors": "Krishna Garg, Cornelia Caragea", "id": "2410.07083v1", "paper_url": "http://arxiv.org/abs/2410.07083v1", "repo": "null"}}