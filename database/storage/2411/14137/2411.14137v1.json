{"2411.14137": {"publish_time": "2024-11-21", "title": "Visual Contexts Clarify Ambiguous Expressions: A Benchmark Dataset", "paper_summary": "The ability to perform complex reasoning across multimodal inputs is\nessential for models to effectively interact with humans in real-world\nscenarios. Advancements in vision-language models have significantly improved\nperformance on tasks that require processing explicit and direct textual\ninputs, such as Visual Question Answering (VQA) and Visual Grounding (VG).\nHowever, less attention has been given to improving the model capabilities to\ncomprehend nuanced and ambiguous forms of communication. This presents a\ncritical challenge, as human language in real-world interactions often convey\nhidden intentions that rely on context for accurate interpretation. To address\nthis gap, we propose VAGUE, a multimodal benchmark comprising 3.9K indirect\nhuman utterances paired with corresponding scenes. Additionally, we contribute\na model-based pipeline for generating prompt-solution pairs from input images.\nOur work aims to delve deeper into the ability of models to understand indirect\ncommunication and seek to contribute to the development of models capable of\nmore refined and human-like interactions. Extensive evaluation on multiple VLMs\nreveals that mainstream models still struggle with indirect communication when\nrequired to perform complex linguistic and visual reasoning. We release our\ncode and data at https://github.com/Hazel-Heejeong-Nam/VAGUE.git.", "paper_summary_zh": "\u5728\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u4e2d\uff0c\u6a21\u578b\u8981\u6709\u6548\u5730\u8207\u4eba\u985e\u4e92\u52d5\uff0c\u5c31\u5fc5\u9808\u5177\u5099\u8de8\u591a\u6a21\u614b\u8f38\u5165\u9032\u884c\u8907\u96dc\u63a8\u7406\u7684\u80fd\u529b\u3002\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u9032\u6b65\u986f\u8457\u63d0\u5347\u4e86\u9700\u8981\u8655\u7406\u660e\u78ba\u4e14\u76f4\u63a5\u7684\u6587\u5b57\u8f38\u5165\u7684\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u4f8b\u5982\u8996\u89ba\u554f\u7b54 (VQA) \u548c\u8996\u89ba\u57fa\u790e (VG)\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u63d0\u5347\u6a21\u578b\u7406\u89e3\u7d30\u5fae\u4e14\u6a21\u7a1c\u5169\u53ef\u7684\u6e9d\u901a\u5f62\u5f0f\u7684\u80fd\u529b\uff0c\u95dc\u6ce8\u8f03\u5c11\u3002\u9019\u662f\u4e00\u500b\u91cd\u5927\u7684\u6311\u6230\uff0c\u56e0\u70ba\u5728\u771f\u5be6\u4e16\u754c\u7684\u4e92\u52d5\u4e2d\uff0c\u4eba\u985e\u8a9e\u8a00\u901a\u5e38\u6703\u50b3\u9054\u96b1\u85cf\u7684\u610f\u5716\uff0c\u800c\u9019\u4e9b\u610f\u5716\u4f9d\u8cf4\u65bc\u80cc\u666f\u624d\u80fd\u6e96\u78ba\u89e3\u8b80\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa VAGUE\uff0c\u4e00\u500b\u7531 3.9K \u500b\u9593\u63a5\u7684\u4eba\u985e\u8a71\u8a9e\u8207\u5c0d\u61c9\u5834\u666f\u914d\u5c0d\u7d44\u6210\u7684\u591a\u6a21\u614b\u57fa\u6e96\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u57fa\u65bc\u6a21\u578b\u7684\u7ba1\u9053\uff0c\u7528\u65bc\u5f9e\u8f38\u5165\u5f71\u50cf\u7522\u751f\u63d0\u793a\u89e3\u6c7a\u65b9\u6848\u5c0d\u3002\u6211\u5011\u7684\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u8a0e\u6a21\u578b\u7406\u89e3\u9593\u63a5\u6e9d\u901a\u7684\u80fd\u529b\uff0c\u4e26\u81f4\u529b\u65bc\u958b\u767c\u80fd\u5920\u9032\u884c\u66f4\u7cbe\u7dfb\u4e14\u66f4\u985e\u4f3c\u4eba\u985e\u4e92\u52d5\u7684\u6a21\u578b\u3002\u5c0d\u591a\u500b VLM \u7684\u5ee3\u6cdb\u8a55\u4f30\u986f\u793a\uff0c\u4e3b\u6d41\u6a21\u578b\u5728\u9700\u8981\u57f7\u884c\u8907\u96dc\u7684\u8a9e\u8a00\u548c\u8996\u89ba\u63a8\u7406\u6642\uff0c\u4ecd\u7136\u96e3\u4ee5\u8655\u7406\u9593\u63a5\u6e9d\u901a\u3002\u6211\u5011\u5728 https://github.com/Hazel-Heejeong-Nam/VAGUE.git/ \u91cb\u51fa\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u3002", "author": "Heejeong Nam et.al.", "authors": "Heejeong Nam, Jinwoo Ahn", "id": "2411.14137v1", "paper_url": "http://arxiv.org/abs/2411.14137v1", "repo": "null"}}