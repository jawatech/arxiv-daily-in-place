{"2410.21965": {"publish_time": "2024-10-29", "title": "SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types", "paper_summary": "Ensuring the safety of large language model (LLM) applications is essential\nfor developing trustworthy artificial intelligence. Current LLM safety\nbenchmarks have two limitations. First, they focus solely on either\ndiscriminative or generative evaluation paradigms while ignoring their\ninterconnection. Second, they rely on standardized inputs, overlooking the\neffects of widespread prompting techniques, such as system prompts, few-shot\ndemonstrations, and chain-of-thought prompting. To overcome these issues, we\ndeveloped SG-Bench, a novel benchmark to assess the generalization of LLM\nsafety across various tasks and prompt types. This benchmark integrates both\ngenerative and discriminative evaluation tasks and includes extended data to\nexamine the impact of prompt engineering and jailbreak on LLM safety. Our\nassessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the\nbenchmark reveals that most LLMs perform worse on discriminative tasks than\ngenerative ones, and are highly susceptible to prompts, indicating poor\ngeneralization in safety alignment. We also explain these findings\nquantitatively and qualitatively to provide insights for future research.", "paper_summary_zh": "\u78ba\u4fdd\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u61c9\u7528\u7a0b\u5f0f\u7684\u5b89\u5168\u6027\u5c0d\u65bc\u958b\u767c\u53ef\u4fe1\u8cf4\u7684\u4eba\u5de5\u667a\u6167\u81f3\u95dc\u91cd\u8981\u3002\u76ee\u524d\u7684 LLM \u5b89\u5168\u57fa\u6e96\u6709\u5169\u500b\u9650\u5236\u3002\u9996\u5148\uff0c\u5b83\u5011\u50c5\u5c08\u6ce8\u65bc\u6b67\u8996\u6027\u6216\u751f\u6210\u6027\u8a55\u4f30\u7bc4\u4f8b\uff0c\u800c\u5ffd\u7565\u4e86\u5b83\u5011\u4e4b\u9593\u7684\u76f8\u4e92\u95dc\u806f\u3002\u5176\u6b21\uff0c\u5b83\u5011\u4f9d\u8cf4\u65bc\u6a19\u6e96\u5316\u8f38\u5165\uff0c\u5ffd\u7565\u4e86\u5ee3\u6cdb\u63d0\u793a\u6280\u8853\u7684\u5f71\u97ff\uff0c\u4f8b\u5982\u7cfb\u7d71\u63d0\u793a\u3001\u5c11\u6b21\u6578\u793a\u7bc4\u548c\u601d\u8003\u93c8\u63d0\u793a\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u958b\u767c\u4e86 SG-Bench\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30 LLM \u5b89\u5168\u5728\u5404\u7a2e\u4efb\u52d9\u548c\u63d0\u793a\u985e\u578b\u4e2d\u7684\u6982\u62ec\u6027\u3002\u6b64\u57fa\u6e96\u6574\u5408\u4e86\u751f\u6210\u6027\u548c\u6b67\u8996\u6027\u8a55\u4f30\u4efb\u52d9\uff0c\u4e26\u5305\u542b\u4e86\u5ef6\u4f38\u8cc7\u6599\uff0c\u4ee5\u6aa2\u67e5\u63d0\u793a\u5de5\u7a0b\u548c\u8d8a\u7344\u5c0d LLM \u5b89\u5168\u6027\u7684\u5f71\u97ff\u3002\u6211\u5011\u4f7f\u7528\u6b64\u57fa\u6e96\u8a55\u4f30\u4e86 3 \u500b\u9032\u968e\u5c08\u6709 LLM \u548c 10 \u500b\u958b\u6e90 LLM\uff0c\u7d50\u679c\u986f\u793a\uff0c\u5927\u591a\u6578 LLM \u5728\u6b67\u8996\u6027\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u6bd4\u751f\u6210\u6027\u4efb\u52d9\u5dee\uff0c\u800c\u4e14\u6975\u6613\u53d7\u5230\u63d0\u793a\u7684\u5f71\u97ff\uff0c\u9019\u8868\u793a\u5728\u5b89\u5168\u6027\u8abf\u6574\u65b9\u9762\u6982\u62ec\u6027\u4e0d\u4f73\u3002\u6211\u5011\u4e5f\u5c0d\u9019\u4e9b\u767c\u73fe\u9032\u884c\u91cf\u5316\u548c\u5b9a\u6027\u8aaa\u660e\uff0c\u4ee5\u63d0\u4f9b\u898b\u89e3\u4f9b\u5c07\u4f86\u7684\u7814\u7a76\u4f7f\u7528\u3002", "author": "Yutao Mou et.al.", "authors": "Yutao Mou, Shikun Zhang, Wei Ye", "id": "2410.21965v1", "paper_url": "http://arxiv.org/abs/2410.21965v1", "repo": "https://github.com/MurrayTom/SG-Bench"}}