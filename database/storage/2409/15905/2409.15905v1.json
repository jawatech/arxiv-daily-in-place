{"2409.15905": {"publish_time": "2024-09-24", "title": "Boosting Code-Switching ASR with Mixture of Experts Enhanced Speech-Conditioned LLM", "paper_summary": "In this paper, we introduce a speech-conditioned Large Language Model (LLM)\nintegrated with a Mixture of Experts (MoE) based connector to address the\nchallenge of Code-Switching (CS) in Automatic Speech Recognition (ASR).\nSpecifically, we propose an Insertion and Deletion of Interruption Token (IDIT)\nmechanism for better transfer text generation ability of LLM to speech\nrecognition task. We also present a connecter with MoE architecture that\nmanages multiple languages efficiently. To further enhance the collaboration of\nmultiple experts and leverage the understanding capabilities of LLM, we propose\na two-stage progressive training strategy: 1) The connector is unfrozen and\ntrained with language-specialized experts to map speech representations to the\ntext space. 2) The connector and LLM LoRA adaptor are trained with the proposed\nIDIT mechanism and all experts are activated to learn general representations.\nExperimental results demonstrate that our method significantly outperforms\nstate-of-the-art models, including end-to-end and large-scale audio-language\nmodels.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u8a9e\u97f3\u689d\u4ef6\u5316\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u5b83\u8207\u57fa\u65bc\u5c08\u5bb6\u6df7\u5408 (MoE) \u7684\u9023\u63a5\u5668\u6574\u5408\uff0c\u4ee5\u89e3\u6c7a\u81ea\u52d5\u8a9e\u97f3\u8b58\u5225 (ASR) \u4e2d\u7684\u4ee3\u78bc\u8f49\u63db (CS) \u6311\u6230\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u4e2d\u65b7\u6a19\u8a18\u7684\u63d2\u5165\u548c\u522a\u9664 (IDIT) \u6a5f\u5236\uff0c\u4ee5\u63d0\u9ad8 LLM \u5c07\u6587\u672c\u751f\u6210\u80fd\u529b\u8f49\u79fb\u5230\u8a9e\u97f3\u8b58\u5225\u4efb\u52d9\u7684\u80fd\u529b\u3002\u6211\u5011\u9084\u63d0\u51fa\u4e86\u4e00\u500b\u5177\u6709 MoE \u67b6\u69cb\u7684\u9023\u63a5\u5668\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u7ba1\u7406\u591a\u7a2e\u8a9e\u8a00\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u589e\u5f37\u591a\u500b\u5c08\u5bb6\u7684\u5354\u4f5c\u4e26\u5229\u7528 LLM \u7684\u7406\u89e3\u80fd\u529b\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5169\u968e\u6bb5\u6f38\u9032\u5f0f\u8a13\u7df4\u7b56\u7565\uff1a1) \u9023\u63a5\u5668\u88ab\u89e3\u51cd\u4e26\u8207\u8a9e\u8a00\u5c08\u5bb6\u4e00\u8d77\u8a13\u7df4\uff0c\u4ee5\u5c07\u8a9e\u97f3\u8868\u793a\u6620\u5c04\u5230\u6587\u672c\u7a7a\u9593\u30022) \u9023\u63a5\u5668\u548c LLM LoRA \u9069\u914d\u5668\u4f7f\u7528\u5efa\u8b70\u7684 IDIT \u6a5f\u5236\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u4e14\u6240\u6709\u5c08\u5bb6\u90fd\u88ab\u6fc0\u6d3b\u4ee5\u5b78\u7fd2\u4e00\u822c\u8868\u793a\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u660e\u986f\u512a\u65bc\u6700\u5148\u9032\u7684\u6a21\u578b\uff0c\u5305\u62ec\u7aef\u5230\u7aef\u548c\u5927\u898f\u6a21\u97f3\u983b\u8a9e\u8a00\u6a21\u578b\u3002", "author": "Fengrun Zhang et.al.", "authors": "Fengrun Zhang, Wang Geng, Hukai Huang, Cheng Yi, He Qu", "id": "2409.15905v1", "paper_url": "http://arxiv.org/abs/2409.15905v1", "repo": "null"}}