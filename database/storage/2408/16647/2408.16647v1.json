{"2408.16647": {"publish_time": "2024-08-29", "title": "DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving", "paper_summary": "The advancement of autonomous driving technologies necessitates increasingly\nsophisticated methods for understanding and predicting real-world scenarios.\nVision language models (VLMs) are emerging as revolutionary tools with\nsignificant potential to influence autonomous driving. In this paper, we\npropose the DriveGenVLM framework to generate driving videos and use VLMs to\nunderstand them. To achieve this, we employ a video generation framework\ngrounded in denoising diffusion probabilistic models (DDPM) aimed at predicting\nreal-world video sequences. We then explore the adequacy of our generated\nvideos for use in VLMs by employing a pre-trained model known as Efficient\nIn-context Learning on Egocentric Videos (EILEV). The diffusion model is\ntrained with the Waymo open dataset and evaluated using the Fr\\'echet Video\nDistance (FVD) score to ensure the quality and realism of the generated videos.\nCorresponding narrations are provided by EILEV for these generated videos,\nwhich may be beneficial in the autonomous driving domain. These narrations can\nenhance traffic scene understanding, aid in navigation, and improve planning\ncapabilities. The integration of video generation with VLMs in the DriveGenVLM\nframework represents a significant step forward in leveraging advanced AI\nmodels to address complex challenges in autonomous driving.", "paper_summary_zh": "\u81ea\u52d5\u99d5\u99db\u6280\u8853\u7684\u9032\u5c55\u9700\u8981\u8d8a\u4f86\u8d8a\u7cbe\u5bc6\u7684\u65b9\u6cd5\u4f86\u7406\u89e3\u548c\u9810\u6e2c\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\u3002\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u6b63\u5728\u6210\u70ba\u9769\u547d\u6027\u7684\u5de5\u5177\uff0c\u5177\u6709\u5f71\u97ff\u81ea\u52d5\u99d5\u99db\u7684\u5de8\u5927\u6f5b\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa DriveGenVLM \u67b6\u69cb\u4f86\u751f\u6210\u99d5\u99db\u5f71\u7247\uff0c\u4e26\u4f7f\u7528 VLM \u4f86\u7406\u89e3\u5b83\u5011\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63a1\u7528\u4e86\u4e00\u500b\u57fa\u65bc\u53bb\u566a\u64f4\u6563\u6a5f\u7387\u6a21\u578b (DDPM) \u7684\u5f71\u7247\u751f\u6210\u67b6\u69cb\uff0c\u65e8\u5728\u9810\u6e2c\u771f\u5be6\u4e16\u754c\u7684\u5f71\u7247\u5e8f\u5217\u3002\u7136\u5f8c\uff0c\u6211\u5011\u900f\u904e\u63a1\u7528\u4e00\u7a2e\u7a31\u70ba\u81ea\u8996\u5f71\u7247\u4e0a\u9ad8\u6548\u60c5\u5883\u5b78\u7fd2 (EILEV) \u7684\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u4f86\u63a2\u8a0e\u6211\u5011\u751f\u6210\u7684\u5f71\u7247\u662f\u5426\u8db3\u4ee5\u7528\u65bc VLM\u3002\u64f4\u6563\u6a21\u578b\u4f7f\u7528 Waymo \u958b\u653e\u8cc7\u6599\u96c6\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u4f7f\u7528 Fr\\'echet \u5f71\u7247\u8ddd\u96e2 (FVD) \u5206\u6578\u9032\u884c\u8a55\u4f30\uff0c\u4ee5\u78ba\u4fdd\u751f\u6210\u5f71\u7247\u7684\u54c1\u8cea\u548c\u771f\u5be6\u6027\u3002EILEV \u70ba\u9019\u4e9b\u751f\u6210\u7684\u5f71\u7247\u63d0\u4f9b\u4e86\u5c0d\u61c9\u7684\u65c1\u767d\uff0c\u9019\u53ef\u80fd\u6709\u52a9\u65bc\u81ea\u52d5\u99d5\u99db\u9818\u57df\u3002\u9019\u4e9b\u65c1\u767d\u53ef\u4ee5\u589e\u5f37\u5c0d\u4ea4\u901a\u5834\u666f\u7684\u7406\u89e3\u3001\u5354\u52a9\u5c0e\u822a\uff0c\u4e26\u6539\u5584\u898f\u5283\u80fd\u529b\u3002\u5728 DriveGenVLM \u67b6\u69cb\u4e2d\u6574\u5408\u5f71\u7247\u751f\u6210\u548c VLM\uff0c\u4ee3\u8868\u4e86\u5728\u5229\u7528\u9032\u968e AI \u6a21\u578b\u4f86\u89e3\u6c7a\u81ea\u52d5\u99d5\u99db\u4e2d\u7684\u8907\u96dc\u6311\u6230\u65b9\u9762\u5411\u524d\u9081\u9032\u4e86\u4e00\u5927\u6b65\u3002", "author": "Yongjie Fu et.al.", "authors": "Yongjie Fu, Anmol Jain, Xuan Di, Xu Chen, Zhaobin Mo", "id": "2408.16647v1", "paper_url": "http://arxiv.org/abs/2408.16647v1", "repo": "null"}}