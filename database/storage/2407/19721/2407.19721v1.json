{"2407.19721": {"publish_time": "2024-07-29", "title": "Rina: Enhancing Ring-AllReduce with In-network Aggregation in Distributed Model Training", "paper_summary": "Parameter Server (PS) and Ring-AllReduce (RAR) are two widely utilized\nsynchronization architectures in multi-worker Deep Learning (DL), also referred\nto as Distributed Deep Learning (DDL). However, PS encounters challenges with\nthe ``incast'' issue, while RAR struggles with problems caused by the long\ndependency chain. The emerging In-network Aggregation (INA) has been proposed\nto integrate with PS to mitigate its incast issue. However, such PS-based INA\nhas poor incremental deployment abilities as it requires replacing all the\nswitches to show significant performance improvement, which is not\ncost-effective. In this study, we present the incorporation of INA capabilities\ninto RAR, called RAR with In-Network Aggregation (Rina), to tackle both the\nproblems above. Rina features its agent-worker mechanism. When an INA-capable\nToR switch is deployed, all workers in this rack run as one abstracted worker\nwith the help of the agent, resulting in both excellent incremental deployment\ncapabilities and better throughput. We conducted extensive testbed and\nsimulation evaluations to substantiate the throughput advantages of Rina over\nexisting DDL training synchronization structures. Compared with the\nstate-of-the-art PS-based INA methods ATP, Rina can achieve more than 50\\%\nthroughput with the same hardware cost.", "paper_summary_zh": "\u53c3\u6578\u4f3a\u670d\u5668 (PS) \u548c\u74b0\u5f62\u5168\u6b78\u7d04 (RAR) \u662f\u591a\u5de5\u4f5c\u8005\u6df1\u5ea6\u5b78\u7fd2 (DL) \u4e2d\u5ee3\u6cdb\u4f7f\u7528\u7684\u5169\u7a2e\u540c\u6b65\u67b6\u69cb\uff0c\u4e5f\u7a31\u70ba\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b78\u7fd2 (DDL)\u3002\u7136\u800c\uff0cPS \u5728\u300c\u5167\u6295\u300d\u554f\u984c\u4e0a\u9047\u5230\u6311\u6230\uff0c\u800c RAR \u5247\u5728\u9577\u4f9d\u8cf4\u93c8\u9020\u6210\u7684\u554f\u984c\u4e0a\u82e6\u82e6\u6399\u624e\u3002\u65b0\u8208\u7684\u7db2\u8def\u5167\u805a\u5408 (INA) \u5df2\u88ab\u63d0\u51fa\u8207 PS \u6574\u5408\u4ee5\u6e1b\u8f15\u5176\u5167\u6295\u554f\u984c\u3002\u7136\u800c\uff0c\u9019\u7a2e\u57fa\u65bc PS \u7684 INA \u5177\u6709\u8f03\u5dee\u7684\u589e\u91cf\u90e8\u7f72\u80fd\u529b\uff0c\u56e0\u70ba\u5b83\u9700\u8981\u66f4\u63db\u6240\u6709\u4ea4\u63db\u5668\u624d\u80fd\u986f\u793a\u51fa\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u9019\u4e26\u4e0d\u5177\u6709\u6210\u672c\u6548\u76ca\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u5c07 INA \u529f\u80fd\u6574\u5408\u5230 RAR \u4e2d\uff0c\u7a31\u70ba\u5177\u6709\u7db2\u8def\u5167\u805a\u5408\u7684 RAR (Rina)\uff0c\u4ee5\u89e3\u6c7a\u4e0a\u8ff0\u5169\u500b\u554f\u984c\u3002Rina \u5177\u5099\u5176\u4ee3\u7406\u5de5\u4f5c\u8005\u6a5f\u5236\u3002\u7576\u90e8\u7f72\u4e86\u5177\u6709 INA \u529f\u80fd\u7684 ToR \u4ea4\u63db\u5668\u6642\uff0c\u6b64\u6a5f\u67b6\u4e2d\u7684\u6240\u6709\u5de5\u4f5c\u8005\u5728\u4ee3\u7406\u7684\u5e6b\u52a9\u4e0b\u4f5c\u70ba\u4e00\u500b\u62bd\u8c61\u5de5\u4f5c\u8005\u57f7\u884c\uff0c\u5f9e\u800c\u540c\u6642\u5177\u6709\u51fa\u8272\u7684\u589e\u91cf\u90e8\u7f72\u80fd\u529b\u548c\u66f4\u597d\u7684\u541e\u5410\u91cf\u3002\u6211\u5011\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u6e2c\u8a66\u5e73\u53f0\u548c\u6a21\u64ec\u8a55\u4f30\uff0c\u4ee5\u8b49\u5be6 Rina \u5728\u73fe\u6709 DDL \u8a13\u7df4\u540c\u6b65\u7d50\u69cb\u4e2d\u5177\u6709\u541e\u5410\u91cf\u512a\u52e2\u3002\u8207\u6700\u5148\u9032\u7684\u57fa\u65bc PS \u7684 INA \u65b9\u6cd5 ATP \u76f8\u6bd4\uff0cRina \u5728\u76f8\u540c\u7684\u786c\u9ad4\u6210\u672c\u4e0b\u53ef\u4ee5\u5be6\u73fe\u8d85\u904e 50% \u7684\u541e\u5410\u91cf\u3002", "author": "Zixuan Chen et.al.", "authors": "Zixuan Chen, Xuandong Liu, Minglin Li, Yinfan Hu, Hao Mei, Huifeng Xing, Hao Wang, Wanxin Shi, Sen Liu, Yang Xu", "id": "2407.19721v1", "paper_url": "http://arxiv.org/abs/2407.19721v1", "repo": "null"}}