{"2407.09050": {"publish_time": "2024-07-12", "title": "Refusing Safe Prompts for Multi-modal Large Language Models", "paper_summary": "Multimodal large language models (MLLMs) have become the cornerstone of\ntoday's generative AI ecosystem, sparking intense competition among tech giants\nand startups. In particular, an MLLM generates a text response given a prompt\nconsisting of an image and a question. While state-of-the-art MLLMs use safety\nfilters and alignment techniques to refuse unsafe prompts, in this work, we\nintroduce MLLM-Refusal, the first method that induces refusals for safe\nprompts. In particular, our MLLM-Refusal optimizes a nearly-imperceptible\nrefusal perturbation and adds it to an image, causing target MLLMs to likely\nrefuse a safe prompt containing the perturbed image and a safe question.\nSpecifically, we formulate MLLM-Refusal as a constrained optimization problem\nand propose an algorithm to solve it. Our method offers competitive advantages\nfor MLLM model providers by potentially disrupting user experiences of\ncompeting MLLMs, since competing MLLM's users will receive unexpected refusals\nwhen they unwittingly use these perturbed images in their prompts. We evaluate\nMLLM-Refusal on four MLLMs across four datasets, demonstrating its\neffectiveness in causing competing MLLMs to refuse safe prompts while not\naffecting non-competing MLLMs. Furthermore, we explore three potential\ncountermeasures -- adding Gaussian noise, DiffPure, and adversarial training.\nOur results show that they are insufficient: though they can mitigate\nMLLM-Refusal's effectiveness, they also sacrifice the accuracy and/or\nefficiency of the competing MLLM. The code is available at\nhttps://github.com/Sadcardation/MLLM-Refusal.", "paper_summary_zh": "\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5df2\u6210\u70ba\n\u7576\u4eca\u751f\u6210\u5f0f AI \u751f\u614b\u7cfb\u7d71\u7684\u57fa\u77f3\uff0c\u5728\u79d1\u6280\u5de8\u982d\n\u548c\u65b0\u5275\u516c\u53f8\u4e4b\u9593\u5f15\u767c\u6fc0\u70c8\u7684\u7af6\u722d\u3002\u7279\u5225\u662f\uff0cMLLM \u6703\u6839\u64da\u63d0\u793a\u7522\u751f\u6587\u5b57\u56de\u61c9\n\u5305\u542b\u5716\u50cf\u548c\u554f\u984c\u3002\u96d6\u7136\u6700\u5148\u9032\u7684 MLLM \u4f7f\u7528\u5b89\u5168\n\u904e\u6ffe\u5668\u548c\u5c0d\u9f4a\u6280\u8853\u4f86\u62d2\u7d55\u4e0d\u5b89\u5168\u7684\u63d0\u793a\uff0c\u4f46\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\n\u4ecb\u7d39 MLLM-Refusal\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u8a98\u5c0e\u62d2\u7d55\u5b89\u5168\u7684\u65b9\u6cd5\n\u63d0\u793a\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u7684 MLLM-Refusal \u6700\u4f73\u5316\u4e86\u5e7e\u4e4e\u7121\u6cd5\u5bdf\u89ba\u7684\n\u62d2\u7d55\u64fe\u52d5\uff0c\u4e26\u5c07\u5176\u65b0\u589e\u5230\u5716\u50cf\u4e2d\uff0c\u5c0e\u81f4\u76ee\u6a19 MLLM \u53ef\u80fd\n\u62d2\u7d55\u5305\u542b\u64fe\u52d5\u5716\u50cf\u548c\u5b89\u5168\u554f\u984c\u7684\u5b89\u5168\u63d0\u793a\u3002\n\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07 MLLM-Refusal \u516c\u5f0f\u5316\u70ba\u53d7\u7d04\u675f\u7684\u6700\u4f73\u5316\u554f\u984c\n\u4e26\u63d0\u51fa\u4e00\u500b\u6f14\u7b97\u6cd5\u4f86\u89e3\u6c7a\u5b83\u3002\u6211\u5011\u7684\u6280\u8853\u70ba MLLM \u6a21\u578b\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86\u7af6\u722d\u512a\u52e2\n\u901a\u904e\u6f5b\u5728\u7834\u58de\u7af6\u722d MLLM \u7684\u4f7f\u7528\u8005\u9ad4\u9a57\uff0c\u56e0\u70ba\u7af6\u722d MLLM \u7684\u4f7f\u7528\u8005\u6703\u6536\u5230\u610f\u5916\u7684\u62d2\u7d55\n\u7576\u4ed6\u5011\u5728\u63d0\u793a\u4e2d\u4e0d\u77e5\u4e0d\u89ba\u5730\u4f7f\u7528\u9019\u4e9b\u64fe\u52d5\u5716\u50cf\u6642\u3002\u6211\u5011\u8a55\u4f30\n\u56db\u500b\u8cc7\u6599\u96c6\u4e0a\u7684\u56db\u500b MLLM \u7684 MLLM-Refusal\uff0c\u8b49\u660e\u5176\n\u5728\u5c0e\u81f4\u7af6\u722d MLLM \u62d2\u7d55\u5b89\u5168\u63d0\u793a\u6642\u4e0d\u6703\u5f71\u97ff\u975e\u7af6\u722d MLLM \u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a2\u7d22\u4e09\u7a2e\u6f5b\u5728\u7684\n\u5c0d\u7b56 -- \u52a0\u5165\u9ad8\u65af\u96dc\u8a0a\u3001DiffPure \u548c\u5c0d\u6297\u8a13\u7df4\u3002\n\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\u5b83\u5011\u662f\u4e0d\u5920\u7684\uff1a\u5118\u7ba1\u5b83\u5011\u53ef\u4ee5\u6e1b\u8f15\nMLLM-Refusal \u7684\u6709\u6548\u6027\uff0c\u5b83\u5011\u4e5f\u72a7\u7272\u4e86\u7af6\u722d MLLM \u7684\u6e96\u78ba\u6027\u548c/\u6216\n\u6548\u7387\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728\nhttps://github.com/Sadcardation/MLLM-Refusal\u3002", "author": "Zedian Shao et.al.", "authors": "Zedian Shao, Hongbin Liu, Yuepeng Hu, Neil Zhenqiang Gong", "id": "2407.09050v1", "paper_url": "http://arxiv.org/abs/2407.09050v1", "repo": "null"}}