{"2410.10781": {"publish_time": "2024-10-14", "title": "When Attention Sink Emerges in Language Models: An Empirical View", "paper_summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b (LM) \u6703\u5c07\u986f\u8457\u7684\u6ce8\u610f\u529b\u5206\u914d\u7d66\u7b2c\u4e00\u500b\u7b26\u865f\uff0c\u5373\u4f7f\u5b83\u5728\u8a9e\u7fa9\u4e0a\u4e26\u4e0d\u91cd\u8981\uff0c\u9019\u7a31\u70ba\u6ce8\u610f\u529b\u63a5\u6536\u5668\u3002\u9019\u7a2e\u73fe\u8c61\u5df2\u88ab\u5ee3\u6cdb\u61c9\u7528\u65bc\u4e32\u6d41/\u9577\u5167\u5bb9\u751f\u6210\u3001KV \u5feb\u53d6\u6700\u4f73\u5316\u3001\u63a8\u8ad6\u52a0\u901f\u3001\u6a21\u578b\u91cf\u5316\u7b49\u61c9\u7528\u7a0b\u5f0f\u4e2d\u3002\u5118\u7ba1\u5b83\u88ab\u5ee3\u6cdb\u4f7f\u7528\uff0c\u4f46\u5c0d\u65bc LM \u4e2d\u7684\u6ce8\u610f\u529b\u63a5\u6536\u5668\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7684\u4e86\u89e3\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u5148\u8b49\u660e\u6ce8\u610f\u529b\u63a5\u6536\u5668\u666e\u904d\u5b58\u5728\u65bc\u5177\u6709\u5404\u7a2e\u8f38\u5165\u7684 LM \u4e2d\uff0c\u5373\u4f7f\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u4e5f\u662f\u5982\u6b64\u3002\u6b64\u5916\uff0c\u89c0\u5bdf\u5230\u6ce8\u610f\u529b\u63a5\u6536\u5668\u6703\u5728 LM \u9810\u8a13\u7df4\u671f\u9593\u51fa\u73fe\uff0c\u4fc3\u4f7f\u6211\u5011\u63a2\u8a0e LM \u9810\u8a13\u7df4\u4e2d\u7684\u6700\u4f73\u5316\u3001\u8cc7\u6599\u5206\u4f48\u3001\u640d\u5931\u51fd\u6578\u548c\u6a21\u578b\u67b6\u69cb\u5982\u4f55\u5f71\u97ff\u5176\u51fa\u73fe\u3002\u6211\u5011\u5f37\u8abf\uff0c\u5728\u5145\u8db3\u7684\u8a13\u7df4\u8cc7\u6599\u4e0a\u9032\u884c\u6709\u6548\u6700\u4f73\u5316\u5f8c\uff0c\u6ce8\u610f\u529b\u63a5\u6536\u5668\u5c31\u6703\u51fa\u73fe\u3002\u63a5\u6536\u5668\u4f4d\u7f6e\u8207\u640d\u5931\u51fd\u6578\u548c\u8cc7\u6599\u5206\u4f48\u9ad8\u5ea6\u76f8\u95dc\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe\u6ce8\u610f\u529b\u63a5\u6536\u5668\u66f4\u50cf\u662f\u9375\u504f\u5dee\uff0c\u5132\u5b58\u984d\u5916\u7684\u6ce8\u610f\u529b\u5206\u6578\uff0c\u9019\u4e9b\u5206\u6578\u53ef\u80fd\u662f\u7121\u610f\u7fa9\u7684\uff0c\u800c\u4e14\u4e0d\u6703\u6709\u52a9\u65bc\u503c\u904b\u7b97\u3002\u6211\u5011\u9084\u89c0\u5bdf\u5230\uff0c\u9019\u7a2e\u73fe\u8c61\uff08\u81f3\u5c11\u90e8\u5206\uff09\u6e90\u65bc\u7b26\u865f\u5c0d\u6ce8\u610f\u529b\u5206\u6578\u7684\u5167\u90e8\u4f9d\u8cf4\u6027\uff0c\u9019\u662f softmax \u6b63\u898f\u5316\u7684\u7d50\u679c\u3002\u5728\u900f\u904e\u5c07 softmax \u6ce8\u610f\u529b\u66ff\u63db\u70ba\u5176\u4ed6\u6ce8\u610f\u529b\u904b\u7b97\uff08\u4f8b\u5982\u6c92\u6709\u6b63\u898f\u5316\u7684 sigmoid \u6ce8\u610f\u529b\uff09\u4f86\u653e\u5bec\u9019\u7a2e\u4f9d\u8cf4\u6027\u5f8c\uff0c\u6ce8\u610f\u529b\u63a5\u6536\u5668\u4e0d\u6703\u51fa\u73fe\u5728\u9ad8\u9054 1B \u53c3\u6578\u7684 LM \u4e2d\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/sail-sg/Attention-Sink \u53d6\u5f97\u3002", "author": "Xiangming Gu et.al.", "authors": "Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin", "id": "2410.10781v1", "paper_url": "http://arxiv.org/abs/2410.10781v1", "repo": "https://github.com/sail-sg/attention-sink"}}