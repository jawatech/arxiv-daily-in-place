{"2408.04211": {"publish_time": "2024-08-08", "title": "MMREC: LLM Based Multi-Modal Recommender System", "paper_summary": "The importance of recommender systems is growing rapidly due to the\nexponential increase in the volume of content generated daily. This surge in\ncontent presents unique challenges for designing effective recommender systems.\nKey among these challenges is the need to effectively leverage the vast amounts\nof natural language data and images that represent user preferences. This paper\npresents a novel approach to enhancing recommender systems by leveraging Large\nLanguage Models (LLMs) and deep learning techniques. The proposed framework\naims to improve the accuracy and relevance of recommendations by incorporating\nmulti-modal information processing and by the use of unified latent space\nrepresentation. The study explores the potential of LLMs to better understand\nand utilize natural language data in recommendation contexts, addressing the\nlimitations of previous methods. The framework efficiently extracts and\nintegrates text and image information through LLMs, unifying diverse modalities\nin a latent space to simplify the learning process for the ranking model.\nExperimental results demonstrate the enhanced discriminative power of the model\nwhen utilizing multi-modal information. This research contributes to the\nevolving field of recommender systems by showcasing the potential of LLMs and\nmulti-modal data integration to create more personalized and contextually\nrelevant recommendations.", "paper_summary_zh": "\u63a8\u85a6\u7cfb\u7d71\u7684\u91cd\u8981\u6027\u6b63\u5feb\u901f\u63d0\u5347\uff0c\u539f\u56e0\u5728\u65bc\u6bcf\u5929\u7522\u751f\u7684\u5167\u5bb9\u91cf\u5448\u6307\u6578\u7d1a\u589e\u9577\u3002\u9019\u7a2e\u5167\u5bb9\u6fc0\u589e\u5c0d\u8a2d\u8a08\u6709\u6548\u7684\u63a8\u85a6\u7cfb\u7d71\u63d0\u51fa\u4e86\u7368\u7279\u7684\u6311\u6230\u3002\u9019\u4e9b\u6311\u6230\u4e2d\u7684\u95dc\u9375\u5728\u65bc\u9700\u8981\u6709\u6548\u5229\u7528\u5927\u91cf\u7684\u81ea\u7136\u8a9e\u8a00\u8cc7\u6599\u548c\u4ee3\u8868\u4f7f\u7528\u8005\u504f\u597d\u7684\u5716\u7247\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u900f\u904e\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u6df1\u5ea6\u5b78\u7fd2\u6280\u8853\u4f86\u589e\u5f37\u63a8\u85a6\u7cfb\u7d71\u3002\u6240\u63d0\u51fa\u7684\u67b6\u69cb\u65e8\u5728\u900f\u904e\u6574\u5408\u591a\u6a21\u614b\u8cc7\u8a0a\u8655\u7406\u548c\u4f7f\u7528\u7d71\u4e00\u7684\u6f5b\u5728\u7a7a\u9593\u8868\u793a\u4f86\u6539\u5584\u63a8\u85a6\u7684\u6e96\u78ba\u6027\u548c\u76f8\u95dc\u6027\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u63a8\u85a6\u60c5\u5883\u4e2d\u66f4\u4e86\u89e3\u548c\u5229\u7528\u81ea\u7136\u8a9e\u8a00\u8cc7\u6599\u7684\u6f5b\u529b\uff0c\u4e26\u89e3\u6c7a\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u9650\u5236\u3002\u8a72\u67b6\u69cb\u900f\u904e\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u6709\u6548\u5730\u8403\u53d6\u548c\u6574\u5408\u6587\u5b57\u548c\u5716\u7247\u8cc7\u8a0a\uff0c\u5728\u6f5b\u5728\u7a7a\u9593\u4e2d\u7d71\u4e00\u4e0d\u540c\u7684\u6a21\u614b\uff0c\u4ee5\u7c21\u5316\u6392\u540d\u6a21\u578b\u7684\u5b78\u7fd2\u6b77\u7a0b\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u8a72\u6a21\u578b\u5728\u4f7f\u7528\u591a\u6a21\u614b\u8cc7\u8a0a\u6642\u589e\u5f37\u7684\u5224\u5225\u80fd\u529b\u3002\u9019\u9805\u7814\u7a76\u900f\u904e\u5c55\u793a\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u548c\u591a\u6a21\u614b\u8cc7\u6599\u6574\u5408\u5728\u5efa\u7acb\u66f4\u500b\u4eba\u5316\u548c\u8207\u60c5\u5883\u76f8\u95dc\u7684\u63a8\u85a6\u65b9\u9762\u7684\u6f5b\u529b\uff0c\u70ba\u63a8\u85a6\u7cfb\u7d71\u7684\u6f14\u9032\u9818\u57df\u505a\u51fa\u8ca2\u737b\u3002", "author": "Jiahao Tian et.al.", "authors": "Jiahao Tian, Jinman Zhao, Zhenkai Wang, Zhicheng Ding", "id": "2408.04211v1", "paper_url": "http://arxiv.org/abs/2408.04211v1", "repo": "null"}}