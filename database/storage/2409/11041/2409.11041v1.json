{"2409.11041": {"publish_time": "2024-09-17", "title": "Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming", "paper_summary": "While there has been a lot of research recently on robots in household\nenvironments, at the present time, most robots in existence can be found on\nshop floors, and most interactions between humans and robots happen there.\n``Collaborative robots'' (cobots) designed to work alongside humans on assembly\nlines traditionally require expert programming, limiting ability to make\nchanges, or manual guidance, limiting expressivity of the resulting programs.\nTo address these limitations, we explore using Large Language Models (LLMs),\nand in particular, their abilities of doing in-context learning, for\nconversational code generation. As a first step, we define RATS, the\n``Repetitive Assembly Task'', a 2D building task designed to lay the foundation\nfor simulating industry assembly scenarios. In this task, a `programmer'\ninstructs a cobot, using natural language, on how a certain assembly is to be\nbuilt; that is, the programmer induces a program, through natural language. We\ncreate a dataset that pairs target structures with various example instructions\n(human-authored, template-based, and model-generated) and example code. With\nthis, we systematically evaluate the capabilities of state-of-the-art LLMs for\nsynthesising this kind of code, given in-context examples. Evaluating in a\nsimulated environment, we find that LLMs are capable of generating accurate\n`first order code' (instruction sequences), but have problems producing\n`higher-order code' (abstractions such as functions, or use of loops).", "paper_summary_zh": "<paragraph>\u96d6\u7136\u6700\u8fd1\u5c0d\u65bc\u5bb6\u5ead\u74b0\u5883\u4e2d\u7684\u6a5f\u5668\u4eba\u6709\u8a31\u591a\u7814\u7a76\uff0c\u76ee\u524d\u70ba\u6b62\uff0c\u5927\u591a\u6578\u7684\u6a5f\u5668\u4eba\u5b58\u5728\u65bc\u5546\u5e97\u6a13\u5c64\uff0c\u800c\u4eba\u985e\u8207\u6a5f\u5668\u4eba\u4e4b\u9593\u7684\u5927\u591a\u6578\u4e92\u52d5\u4e5f\u767c\u751f\u5728\u9019\u4e9b\u5730\u65b9\u3002\u50b3\u7d71\u4e0a\u8a2d\u8a08\u7528\u65bc\u8207\u4eba\u985e\u5728\u7d44\u88dd\u7dda\u4e0a\u4e00\u8d77\u5de5\u4f5c\u7684\u300c\u5354\u4f5c\u6a5f\u5668\u4eba\u300d\uff08cobots\uff09\u9700\u8981\u5c08\u5bb6\u7a0b\u5f0f\u8a2d\u8a08\uff0c\u9019\u6703\u9650\u5236\u505a\u51fa\u8b8a\u66f4\u7684\u80fd\u529b\uff0c\u6216\u9650\u5236\u624b\u52d5\u5f15\u5c0e\uff0c\u9032\u800c\u9650\u5236\u6240\u7522\u751f\u7a0b\u5f0f\u78bc\u7684\u8868\u9054\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u7279\u5225\u662f\u5b83\u5011\u5728\u60c5\u5883\u5b78\u7fd2\u4e2d\u7684\u80fd\u529b\uff0c\u7528\u65bc\u5c0d\u8a71\u5f0f\u7a0b\u5f0f\u78bc\u7522\u751f\u3002\u4f5c\u70ba\u7b2c\u4e00\u6b65\uff0c\u6211\u5011\u5b9a\u7fa9 RATS\uff0c\u5373\u300c\u91cd\u8907\u7d44\u88dd\u4efb\u52d9\u300d\uff0c\u9019\u662f\u4e00\u500b 2D \u5efa\u69cb\u4efb\u52d9\uff0c\u65e8\u5728\u5960\u5b9a\u6a21\u64ec\u7522\u696d\u7d44\u88dd\u5834\u666f\u7684\u57fa\u790e\u3002\u5728\u9019\u500b\u4efb\u52d9\u4e2d\uff0c\u4e00\u500b\u300c\u7a0b\u5f0f\u8a2d\u8a08\u5e2b\u300d\u4f7f\u7528\u81ea\u7136\u8a9e\u8a00\u6307\u793a\u4e00\u500b cobot \u5982\u4f55\u5efa\u69cb\u67d0\u500b\u7d44\u88dd\uff1b\u4e5f\u5c31\u662f\u8aaa\uff0c\u7a0b\u5f0f\u8a2d\u8a08\u5e2b\u900f\u904e\u81ea\u7136\u8a9e\u8a00\u5f15\u5c0e\u4e00\u500b\u7a0b\u5f0f\u3002\u6211\u5011\u5efa\u7acb\u4e00\u500b\u5c07\u76ee\u6a19\u7d50\u69cb\u8207\u5404\u7a2e\u7bc4\u4f8b\u6307\u793a\uff08\u4eba\u5de5\u64b0\u5beb\u3001\u57fa\u65bc\u7bc4\u672c\uff0c\u4ee5\u53ca\u6a21\u578b\u7522\u751f\uff09\u548c\u7bc4\u4f8b\u7a0b\u5f0f\u78bc\u914d\u5c0d\u7684\u8cc7\u6599\u96c6\u3002\u900f\u904e\u9019\u500b\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u6700\u5148\u9032\u7684 LLM \u5728\u7d66\u5b9a\u60c5\u5883\u7bc4\u4f8b\u7684\u60c5\u6cc1\u4e0b\uff0c\u7d9c\u5408\u9019\u7a2e\u7a0b\u5f0f\u78bc\u7684\u80fd\u529b\u3002\u5728\u6a21\u64ec\u74b0\u5883\u4e2d\u8a55\u4f30\uff0c\u6211\u5011\u767c\u73fe LLM \u80fd\u5920\u7522\u751f\u6e96\u78ba\u7684\u300c\u4e00\u968e\u7a0b\u5f0f\u78bc\u300d\uff08\u6307\u4ee4\u5e8f\u5217\uff09\uff0c\u4f46\u96e3\u4ee5\u7522\u751f\u300c\u9ad8\u968e\u7a0b\u5f0f\u78bc\u300d\uff08\u4f8b\u5982\u51fd\u5f0f\u6216\u8ff4\u5708\u4f7f\u7528\u7684\u62bd\u8c61\u6982\u5ff5\uff09\u3002</paragraph>", "author": "Kranti Chalamalasetti et.al.", "authors": "Kranti Chalamalasetti, Sherzod Hakimov, David Schlangen", "id": "2409.11041v1", "paper_url": "http://arxiv.org/abs/2409.11041v1", "repo": "null"}}