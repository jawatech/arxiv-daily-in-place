{"2412.02674": {"publish_time": "2024-12-03", "title": "Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models", "paper_summary": "Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries.", "paper_summary_zh": "\u81ea\u6211\u63d0\u5347\u662f\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u9884\u8bad\u7ec3\u3001\u540e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u95f4\u63a8\u7406\u4e2d\u7684\u673a\u5236\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5176\u4e2d\u6a21\u578b\u9a8c\u8bc1\u5176\u81ea\u5df1\u7684\u8f93\u51fa\uff0c\u6839\u636e\u6b64\u9a8c\u8bc1\u8fc7\u6ee4\u6216\u91cd\u65b0\u52a0\u6743\u6570\u636e\uff0c\u5e76\u63d0\u53d6\u8fc7\u6ee4\u540e\u7684\u6570\u636e\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u4e00\u4e9b\u7ecf\u9a8c\u4e0a\u7684\u6210\u529f\uff0c\u4f46\u4ecd\u7136\u7f3a\u4e4f\u57fa\u672c\u7684\u7406\u89e3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5bf9 LLM \u81ea\u6211\u63d0\u5347\u53d1\u8d77\u4e86\u4e00\u9879\u5168\u9762\u3001\u6a21\u5757\u5316\u4e14\u53d7\u63a7\u7684\u7814\u7a76\u3002\u6211\u4eec\u4e3a\u81ea\u6211\u63d0\u5347\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6570\u5b66\u516c\u5f0f\uff0c\u8be5\u516c\u5f0f\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u7531\u6211\u4eec\u5f62\u5f0f\u5316\u4e3a\u751f\u6210\u9a8c\u8bc1\u5dee\u8ddd\u7684\u6570\u91cf\u6240\u63a7\u5236\u3002\u901a\u8fc7\u5bf9\u5404\u79cd\u6a21\u578b\u7cfb\u5217\u548c\u4efb\u52a1\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u81ea\u6211\u63d0\u5347\u7684\u7f29\u653e\u73b0\u8c61\u2014\u2014\u751f\u6210\u9a8c\u8bc1\u5dee\u8ddd\u7684\u4e00\u4e2a\u53d8\u4f53\u4e0e\u6a21\u578b\u9884\u8bad\u7ec3 flops \u5355\u8c03\u7f29\u653e\u3002\u6211\u4eec\u8fd8\u68c0\u67e5\u4e86\u81ea\u6211\u63d0\u5347\u4f55\u65f6\u53ef\u80fd\uff0c\u8fed\u4ee3\u81ea\u6211\u63d0\u5347\u8fc7\u7a0b\u4ee5\u53ca\u63d0\u9ad8\u5176\u6027\u80fd\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u53d1\u73b0\u4e0d\u4ec5\u4fc3\u8fdb\u4e86\u5bf9 LLM \u81ea\u6211\u63d0\u5347\u7684\u7406\u89e3\u5e76\u5177\u6709\u5b9e\u9645\u610f\u4e49\uff0c\u800c\u4e14\u8fd8\u4e3a\u672a\u6765\u5bf9\u5176\u80fd\u529b\u548c\u754c\u9650\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u4f17\u591a\u9014\u5f84\u3002", "author": "Yuda Song et.al.", "authors": "Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, Udaya Ghai", "id": "2412.02674v1", "paper_url": "http://arxiv.org/abs/2412.02674v1", "repo": "null"}}