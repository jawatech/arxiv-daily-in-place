{"2408.06273": {"publish_time": "2024-08-12", "title": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data", "paper_summary": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5ee3\u6cdb\u7684\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5176\u80fd\u529b\u3002\u7136\u800c\uff0c\u8a31\u591a LLM \u5728\u9ad8\u8cc7\u6e90\u548c\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u4e4b\u9593\u8868\u73fe\u51fa\u986f\u8457\u7684\u5dee\u7570\u3002\u70ba\u4e86\u7de9\u89e3\u6b64\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa FuxiTranyu\uff0c\u9019\u662f\u4e00\u500b\u958b\u6e90\u7684\u591a\u8a9e\u8a00 LLM\uff0c\u65e8\u5728\u6eff\u8db3\u7814\u7a76\u793e\u7fa4\u5c0d\u5e73\u8861\u4e14\u9ad8\u6027\u80fd\u591a\u8a9e\u8a00\u529f\u80fd\u7684\u9700\u6c42\u3002FuxiTranyu-8B \u662f\u5177\u5099 80 \u5104\u500b\u53c3\u6578\u7684\u57fa\u672c\u6a21\u578b\uff0c\u5f9e\u4e00\u500b\u7cbe\u5fc3\u5e73\u8861\u7684\u591a\u8a9e\u8a00\u8cc7\u6599\u5132\u5b58\u5eab\u9032\u884c\u8a13\u7df4\uff0c\u5176\u4e2d\u5305\u542b\u6db5\u84cb 43 \u7a2e\u81ea\u7136\u8a9e\u8a00\u548c 16 \u7a2e\u7a0b\u5f0f\u8a9e\u8a00\u7684 6000 \u5104\u500b\u4ee3\u5e63\u3002\u9664\u4e86\u57fa\u672c\u6a21\u578b\u5916\uff0c\u6211\u5011\u9084\u958b\u767c\u4e86\u5169\u500b\u6307\u4ee4\u5fae\u8abf\u6a21\u578b\uff1aFuxiTranyu-8B-SFT\uff0c\u5728\u4e00\u500b\u591a\u6a23\u5316\u7684\u591a\u8a9e\u8a00\u6307\u4ee4\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5fae\u8abf\uff0c\u4ee5\u53ca FuxiTranyu-8B-DPO\uff0c\u5728\u4e00\u500b\u504f\u597d\u8cc7\u6599\u96c6\u4e0a\u9032\u4e00\u6b65\u4f7f\u7528 DPO \u9032\u884c\u5fae\u8abf\u4ee5\u589e\u5f37\u5c0d\u9f4a\u80fd\u529b\u3002\u5728\u5ee3\u6cdb\u7684\u591a\u8a9e\u8a00\u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 FuxiTranyu \u76f8\u5c0d\u65bc\u73fe\u6709\u7684\u591a\u8a9e\u8a00 LLM\uff08\u4f8b\u5982 BLOOM-7B\u3001PolyLM-13B\u3001Llama-2-Chat-7B \u548c Mistral-7B-Instruct\uff09\u7684\u7af6\u722d\u529b\u3002\u5728\u795e\u7d93\u5143\u548c\u8868\u793a\u5c64\u7d1a\u7684\u8a6e\u91cb\u6027\u5206\u6790\u8868\u660e\uff0cFuxiTranyu \u80fd\u5920\u8de8\u4e0d\u540c\u8a9e\u8a00\u5b78\u7fd2\u4e00\u81f4\u7684\u591a\u8a9e\u8a00\u8868\u793a\u3002\u70ba\u4e86\u4fc3\u9032\u5c0d\u591a\u8a9e\u8a00 LLM \u53ca\u5176\u5de5\u4f5c\u6a5f\u5236\u7684\u9032\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u5011\u5728 HuggingFace \u548c Github \u4e0a\u91cb\u51fa\u57fa\u672c\u548c\u6307\u4ee4\u5fae\u8abf\u7684 FuxiTranyu \u6a21\u578b\uff0c\u4ee5\u53ca 58 \u500b\u9810\u8a13\u7df4\u6aa2\u67e5\u9ede\u3002", "author": "Haoran Sun et.al.", "authors": "Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Dui, Yikun Lei, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, Deyi Xiong", "id": "2408.06273v1", "paper_url": "http://arxiv.org/abs/2408.06273v1", "repo": "null"}}