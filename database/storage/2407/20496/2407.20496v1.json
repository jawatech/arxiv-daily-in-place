{"2407.20496": {"publish_time": "2024-07-30", "title": "Toward Efficient Permutation for Hierarchical N:M Sparsity on GPUs", "paper_summary": "N:M sparsity pruning is a powerful technique for compressing deep neural\nnetworks, utilizing NVIDIA's Sparse Tensor Core technology. This method\nbenefits from hardware support for sparse indexing, enabling the adoption of\nfine-grained sparsity to maintain model accuracy while minimizing the overhead\ntypically associated with irregular data access. Although restricted to a fixed\nlevel of sparsity due to its reliance on hardware, N:M sparsity can be combined\nwith coarser sparsity techniques to achieve diverse compression ratios.\nInitially, column-wise vector sparsity is applied to a dense model, followed by\nrow-wise N:M sparsity on the preserved column vectors. We call this multi-level\napproach as hierarchical N:M (HiNM) sparsity. Similar to earlier single-level\nsparsity techniques, HiNM sparsity necessitates an effective channel\npermutation strategy to maximize the accuracy of the compressed networks.\nHowever, it introduces further complexities by requiring the rearrangement of\nboth input and output channels, addressing challenges such as permutation\nsequence, HiNM-sparsity-aware permutation, and maintaining consistency in\nchannel ordering across layers. In this paper, we introduce a channel\npermutation method designed specifically for HiNM sparsity, named\ngyro-permutation. This method is crafted to exploit the unique characteristics\nof HiNM pruning, incorporating a strategic policy in each permutation phase,\nincluding channel sampling, clustering, and assignment, to circumvent local\nminima. Additionally, we have developed a GPU kernel that facilitates\nindependent layer permutation during the execution of HiNM sparse networks. Our\nextensive experimental evaluations on various DNN models demonstrate that our\ngyro-permutation significantly enhances the accuracy of HiNM sparse networks,\nallowing them to reach performance levels comparable to those of unstructured\nsparse networks.", "paper_summary_zh": "N:M \u7a00\u758f\u526a\u679d\u662f\u4e00\u79cd\u538b\u7f29\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5f3a\u5927\u6280\u672f\uff0c\u5229\u7528\u4e86 NVIDIA \u7684\u7a00\u758f\u5f20\u91cf\u6838\u6280\u672f\u3002\u8fd9\u79cd\u65b9\u6cd5\u5f97\u76ca\u4e8e\u5bf9\u7a00\u758f\u7d22\u5f15\u7684\u786c\u4ef6\u652f\u6301\uff0c\u4ece\u800c\u80fd\u591f\u91c7\u7528\u7ec6\u7c92\u5ea6\u7684\u7a00\u758f\u6027\u6765\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u901a\u5e38\u4e0e\u4e0d\u89c4\u5219\u6570\u636e\u8bbf\u95ee\u76f8\u5173\u7684\u5f00\u9500\u3002\u867d\u7136\u7531\u4e8e\u4f9d\u8d56\u4e8e\u786c\u4ef6\u800c\u9650\u5236\u5728\u56fa\u5b9a\u7684\u7a00\u758f\u6027\u7ea7\u522b\uff0c\u4f46 N:M \u7a00\u758f\u6027\u53ef\u4ee5\u4e0e\u66f4\u7c97\u7cd9\u7684\u7a00\u758f\u6027\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u4e0d\u540c\u7684\u538b\u7f29\u6bd4\u3002\u6700\u521d\uff0c\u5217\u5411\u91cf\u7a00\u758f\u6027\u5e94\u7528\u4e8e\u7a20\u5bc6\u6a21\u578b\uff0c\u7136\u540e\u5728\u4fdd\u7559\u7684\u5217\u5411\u91cf\u4e0a\u5e94\u7528\u884c N:M \u7a00\u758f\u6027\u3002\u6211\u4eec\u5c06\u8fd9\u79cd\u591a\u7ea7\u65b9\u6cd5\u79f0\u4e3a\u5206\u5c42 N:M (HiNM) \u7a00\u758f\u6027\u3002\u4e0e\u65e9\u671f\u7684\u5355\u7ea7\u7a00\u758f\u6027\u6280\u672f\u7c7b\u4f3c\uff0cHiNM \u7a00\u758f\u6027\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u901a\u9053\u7f6e\u6362\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u538b\u7f29\u7f51\u7edc\u7684\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u5b83\u901a\u8fc7\u8981\u6c42\u91cd\u65b0\u6392\u5217\u8f93\u5165\u548c\u8f93\u51fa\u901a\u9053\u5f15\u5165\u4e86\u8fdb\u4e00\u6b65\u7684\u590d\u6742\u6027\uff0c\u89e3\u51b3\u4e86\u8bf8\u5982\u7f6e\u6362\u5e8f\u5217\u3001HiNM \u7a00\u758f\u611f\u77e5\u7f6e\u6362\u4ee5\u53ca\u8de8\u5c42\u4fdd\u6301\u901a\u9053\u987a\u5e8f\u7684\u4e00\u81f4\u6027\u7b49\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e13\u95e8\u4e3a HiNM \u7a00\u758f\u6027\u8bbe\u8ba1\u7684\u901a\u9053\u7f6e\u6362\u65b9\u6cd5\uff0c\u79f0\u4e3a\u9640\u87ba\u7f6e\u6362\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u5229\u7528 HiNM \u526a\u679d\u7684\u72ec\u7279\u7279\u6027\uff0c\u5728\u6bcf\u4e2a\u7f6e\u6362\u9636\u6bb5\u7eb3\u5165\u6218\u7565\u7b56\u7565\uff0c\u5305\u62ec\u901a\u9053\u91c7\u6837\u3001\u805a\u7c7b\u548c\u5206\u914d\uff0c\u4ee5\u89c4\u907f\u5c40\u90e8\u6781\u5c0f\u503c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a GPU \u5185\u6838\uff0c\u8be5\u5185\u6838\u5728 HiNM \u7a00\u758f\u7f51\u7edc\u6267\u884c\u671f\u95f4\u4fc3\u8fdb\u4e86\u72ec\u7acb\u5c42\u7f6e\u6362\u3002\u6211\u4eec\u5728\u5404\u79cd DNN \u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u9640\u87ba\u7f6e\u6362\u663e\u8457\u63d0\u9ad8\u4e86 HiNM \u7a00\u758f\u7f51\u7edc\u7684\u51c6\u786e\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u8fbe\u5230\u4e0e\u975e\u7ed3\u6784\u5316\u7a00\u758f\u7f51\u7edc\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "author": "Seungmin Yu et.al.", "authors": "Seungmin Yu, Xiaodie Yi, Hayun Lee, Dongkun Shin", "id": "2407.20496v1", "paper_url": "http://arxiv.org/abs/2407.20496v1", "repo": "null"}}