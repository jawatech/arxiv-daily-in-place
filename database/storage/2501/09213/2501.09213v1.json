{"2501.09213": {"publish_time": "2025-01-16", "title": "FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training", "paper_summary": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the advanced reasoning required for\ncomplex clinical scenarios, such as differential diagnosis or personalized\ntreatment suggestions. We proposed FineMedLM-o1, which leverages high-quality\nsynthetic medical data and long-form reasoning data for Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and\ndeep reasoning capabilities. Additionally, we introduced Test-Time Training\n(TTT) in the medical domain for the first time, facilitating domain adaptation\nand ensuring reliable, accurate reasoning. Experimental results demonstrate\nthat FineMedLM-o1 achieves a 23% average performance improvement over prior\nmodels on key medical benchmarks. Furthermore, the introduction of TTT provides\nan additional 14% performance boost, highlighting its effectiveness in\nenhancing medical reasoning capabilities. To support this process, we also\nproposed a novel method for synthesizing medical dialogue. Compared to other\nopen-source datasets, our dataset stands out as superior in both quality and\ncomplexity. The project and data will be released on GitHub.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u5728\u75be\u75c5\u8a3a\u65b7\u548c\u6cbb\u7642\u898f\u5283\u7b49\u91ab\u7642\u61c9\u7528\u4e2d\u5c55\u73fe\u51fa\u524d\u666f\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u91ab\u7642 LLM \u5927\u591a\u96e3\u4ee5\u61c9\u5c0d\u8907\u96dc\u81e8\u5e8a\u60c5\u5883\u6240\u9700\u7684\u9032\u968e\u63a8\u7406\uff0c\u4f8b\u5982\u9451\u5225\u8a3a\u65b7\u6216\u500b\u4eba\u5316\u6cbb\u7642\u5efa\u8b70\u3002\u6211\u5011\u63d0\u51fa\u4e86 FineMedLM-o1\uff0c\u5b83\u5229\u7528\u512a\u8cea\u7684\u5408\u6210\u91ab\u7642\u6578\u64da\u548c\u9577\u7bc7\u63a8\u7406\u6578\u64da\u9032\u884c\u76e3\u7763\u5fae\u8abf (SFT) \u548c\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\uff0c\u5be6\u73fe\u9032\u968e\u5c0d\u8a71\u548c\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u9996\u6b21\u5728\u91ab\u7642\u9818\u57df\u5f15\u5165\u4e86\u6e2c\u8a66\u6642\u8a13\u7df4 (TTT)\uff0c\u4fc3\u9032\u9818\u57df\u9069\u61c9\u4e26\u78ba\u4fdd\u63a8\u7406\u53ef\u9760\u4e14\u6e96\u78ba\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cFineMedLM-o1 \u5728\u4e3b\u8981\u91ab\u7642\u57fa\u6e96\u4e0a\u6bd4\u5148\u524d\u7684\u6a21\u578b\u5e73\u5747\u63d0\u5347\u4e86 23% \u7684\u6548\u80fd\u3002\u6b64\u5916\uff0cTTT \u7684\u5f15\u5165\u63d0\u4f9b\u4e86\u984d\u5916\u7684 14% \u6548\u80fd\u63d0\u5347\uff0c\u51f8\u986f\u5176\u5728\u589e\u5f37\u91ab\u7642\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u70ba\u4e86\u652f\u63f4\u6b64\u6d41\u7a0b\uff0c\u6211\u5011\u9084\u63d0\u51fa\u4e86\u4e00\u7a2e\u5408\u6210\u91ab\u7642\u5c0d\u8a71\u7684\u65b0\u65b9\u6cd5\u3002\u8207\u5176\u4ed6\u958b\u6e90\u6578\u64da\u96c6\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6578\u64da\u96c6\u5728\u54c1\u8cea\u548c\u8907\u96dc\u6027\u65b9\u9762\u90fd\u812b\u7a4e\u800c\u51fa\u3002\u8a72\u5c08\u6848\u548c\u6578\u64da\u5c07\u5728 GitHub \u4e0a\u767c\u5e03\u3002", "author": "Hongzhou Yu et.al.", "authors": "Hongzhou Yu, Tianhao Cheng, Ying Cheng, Rui Feng", "id": "2501.09213v1", "paper_url": "http://arxiv.org/abs/2501.09213v1", "repo": "null"}}