{"2412.00315": {"publish_time": "2024-11-30", "title": "One Model for One Graph: A New Perspective for Pretraining with Cross-domain Graphs", "paper_summary": "Graph Neural Networks (GNNs) have emerged as a powerful tool to capture\nintricate network patterns, achieving success across different domains.\nHowever, existing GNNs require careful domain-specific architecture designs and\ntraining from scratch on each dataset, leading to an expertise-intensive\nprocess with difficulty in generalizing across graphs from different domains.\nTherefore, it can be hard for practitioners to infer which GNN model can\ngeneralize well to graphs from their domains. To address this challenge, we\npropose a novel cross-domain pretraining framework, \"one model for one graph,\"\nwhich overcomes the limitations of previous approaches that failed to use a\nsingle GNN to capture diverse graph patterns across domains with significant\ngaps. Specifically, we pretrain a bank of expert models, with each one\ncorresponding to a specific dataset. When inferring to a new graph, gating\nfunctions choose a subset of experts to effectively integrate prior model\nknowledge while avoiding negative transfer. Extensive experiments consistently\ndemonstrate the superiority of our proposed method on both link prediction and\nnode classification tasks.", "paper_summary_zh": "\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u5df2\u6210\u70ba\u6355\u6349\u8907\u96dc\u7db2\u8def\u6a21\u5f0f\u7684\u5f37\u5927\u5de5\u5177\uff0c\u5728\u4e0d\u540c\u9818\u57df\u7686\u53d6\u5f97\u6210\u529f\u3002\n\u7136\u800c\uff0c\u73fe\u6709\u7684 GNN \u9700\u8981\u4ed4\u7d30\u7684\u7279\u5b9a\u65bc\u9818\u57df\u7684\u67b6\u69cb\u8a2d\u8a08\uff0c\u4e26\u91dd\u5c0d\u6bcf\u500b\u8cc7\u6599\u96c6\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\uff0c\u5c0e\u81f4\u5c08\u696d\u77e5\u8b58\u5bc6\u96c6\u7684\u904e\u7a0b\uff0c\u96e3\u4ee5\u6982\u62ec\u4f86\u81ea\u4e0d\u540c\u9818\u57df\u7684\u5716\u5f62\u3002\n\u56e0\u6b64\uff0c\u5f9e\u696d\u8005\u5f88\u96e3\u63a8\u65b7\u54ea\u500b GNN \u6a21\u578b\u53ef\u4ee5\u5f88\u597d\u5730\u6982\u62ec\u5230\u5176\u9818\u57df\u7684\u5716\u5f62\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u8de8\u9818\u57df\u9810\u8a13\u7df4\u6846\u67b6\uff0c\u300c\u4e00\u500b\u6a21\u578b\u5c0d\u61c9\u4e00\u500b\u5716\u5f62\u300d\uff0c\u5b83\u514b\u670d\u4e86\u5148\u524d\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u9019\u4e9b\u9650\u5236\u7121\u6cd5\u4f7f\u7528\u55ae\u500b GNN \u4f86\u6355\u6349\u8de8\u8d8a\u5177\u6709\u986f\u8457\u5dee\u8ddd\u7684\u9818\u57df\u7684\u4e0d\u540c\u5716\u5f62\u6a21\u5f0f\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9810\u8a13\u7df4\u4e86\u4e00\u7d44\u5c08\u5bb6\u6a21\u578b\uff0c\u6bcf\u4e00\u500b\u90fd\u5c0d\u61c9\u4e00\u500b\u7279\u5b9a\u8cc7\u6599\u96c6\u3002\u5728\u63a8\u8ad6\u5230\u4e00\u500b\u65b0\u5716\u5f62\u6642\uff0c\u9598\u63a7\u51fd\u6578\u6703\u9078\u64c7\u4e00\u500b\u5c08\u5bb6\u5b50\u96c6\uff0c\u4ee5\u6709\u6548\u6574\u5408\u5148\u524d\u7684\u6a21\u578b\u77e5\u8b58\uff0c\u540c\u6642\u907f\u514d\u8ca0\u9762\u50b3\u905e\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u6301\u7e8c\u8b49\u660e\u4e86\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9023\u7d50\u9810\u6e2c\u548c\u7bc0\u9ede\u5206\u985e\u4efb\u52d9\u4e0a\u7684\u512a\u8d8a\u6027\u3002", "author": "Jingzhe Liu et.al.", "authors": "Jingzhe Liu, Haitao Mao, Zhikai Chen, Wenqi Fan, Mingxuan Ju, Tong Zhao, Neil Shah, Jiliang Tang", "id": "2412.00315v1", "paper_url": "http://arxiv.org/abs/2412.00315v1", "repo": "null"}}