{"2502.01549": {"publish_time": "2025-02-03", "title": "VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos", "paper_summary": "Retrieval-Augmented Generation (RAG) has demonstrated remarkable success in\nenhancing Large Language Models (LLMs) through external knowledge integration,\nyet its application has primarily focused on textual content, leaving the rich\ndomain of multi-modal video knowledge predominantly unexplored. This paper\nintroduces VideoRAG, the first retrieval-augmented generation framework\nspecifically designed for processing and understanding extremely long-context\nvideos. Our core innovation lies in its dual-channel architecture that\nseamlessly integrates (i) graph-based textual knowledge grounding for capturing\ncross-video semantic relationships, and (ii) multi-modal context encoding for\nefficiently preserving visual features. This novel design empowers VideoRAG to\nprocess unlimited-length videos by constructing precise knowledge graphs that\nspan multiple videos while maintaining semantic dependencies through\nspecialized multi-modal retrieval paradigms. Through comprehensive empirical\nevaluation on our proposed LongerVideos benchmark-comprising over 160 videos\ntotaling 134+ hours across lecture, documentary, and entertainment\ncategories-VideoRAG demonstrates substantial performance compared to existing\nRAG alternatives and long video understanding methods. The source code of\nVideoRAG implementation and the benchmark dataset are openly available at:\nhttps://github.com/HKUDS/VideoRAG.", "paper_summary_zh": "\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u5df2\u8b49\u660e\u5728\u900f\u904e\u5916\u90e8\u77e5\u8b58\u6574\u5408\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u65b9\u9762\u53d6\u5f97\u986f\u8457\u6210\u529f\uff0c\u4f46\u5176\u61c9\u7528\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u5b57\u5167\u5bb9\u4e0a\uff0c\u800c\u8c50\u5bcc\u7684\u591a\u6a21\u614b\u5f71\u7247\u77e5\u8b58\u9818\u57df\u5247\u9bae\u5c11\u88ab\u63a2\u7d22\u3002\u672c\u6587\u4ecb\u7d39 VideoRAG\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u6aa2\u7d22\u589e\u5f37\u751f\u6210\u67b6\u69cb\uff0c\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u8655\u7406\u548c\u7406\u89e3\u6975\u9577\u8a9e\u5883\u7684\u5f71\u7247\u3002\u6211\u5011\u7684\u6838\u5fc3\u5275\u65b0\u5728\u65bc\u5176\u96d9\u901a\u9053\u67b6\u69cb\uff0c\u5b83\u7121\u7e2b\u6574\u5408 (i) \u57fa\u65bc\u5716\u5f62\u6587\u5b57\u77e5\u8b58\u57fa\u790e\uff0c\u7528\u65bc\u64f7\u53d6\u8de8\u5f71\u7247\u8a9e\u7fa9\u95dc\u4fc2\uff0c\u4ee5\u53ca (ii) \u591a\u6a21\u614b\u8a9e\u5883\u7de8\u78bc\uff0c\u7528\u65bc\u6709\u6548\u4fdd\u7559\u8996\u89ba\u7279\u5fb5\u3002\u9019\u500b\u65b0\u7a4e\u7684\u8a2d\u8a08\u8b93 VideoRAG \u80fd\u5920\u900f\u904e\u5efa\u69cb\u8de8\u8d8a\u591a\u500b\u5f71\u7247\u7684\u7cbe\u78ba\u77e5\u8b58\u5716\u8b5c\u4f86\u8655\u7406\u9577\u5ea6\u4e0d\u9650\u7684\u5f71\u7247\uff0c\u540c\u6642\u900f\u904e\u5c08\u9580\u7684\u591a\u6a21\u614b\u6aa2\u7d22\u7bc4\u4f8b\u4f86\u7dad\u6301\u8a9e\u7fa9\u4f9d\u8cf4\u6027\u3002\u900f\u904e\u6211\u5011\u63d0\u51fa\u7684 LongerVideos \u57fa\u6e96\u7684\u5168\u9762\u7d93\u9a57\u8a55\u4f30\uff0c\u8a72\u57fa\u6e96\u5305\u542b\u8d85\u904e 160 \u90e8\u5f71\u7247\uff0c\u7e3d\u6642\u6578\u8d85\u904e 134 \u5c0f\u6642\uff0c\u6db5\u84cb\u6f14\u8b1b\u3001\u7d00\u9304\u7247\u548c\u5a1b\u6a02\u985e\u5225\uff0cVideoRAG \u8207\u73fe\u6709\u7684 RAG \u66ff\u4ee3\u65b9\u6848\u548c\u9577\u5f71\u7247\u7406\u89e3\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c55\u73fe\u51fa\u986f\u8457\u7684\u6548\u80fd\u3002VideoRAG \u5be6\u4f5c\u7684\u539f\u59cb\u78bc\u548c\u57fa\u6e96\u8cc7\u6599\u96c6\u5df2\u516c\u958b\u65bc\uff1ahttps://github.com/HKUDS/VideoRAG\u3002", "author": "Xubin Ren et.al.", "authors": "Xubin Ren, Lingrui Xu, Long Xia, Shuaiqiang Wang, Dawei Yin, Chao Huang", "id": "2502.01549v1", "paper_url": "http://arxiv.org/abs/2502.01549v1", "repo": "null"}}