{"2405.19334": {"publish_time": "2024-05-29", "title": "LLMs Meet Multimodal Generation and Editing: A Survey", "paper_summary": "With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on understanding. This\nsurvey elaborates on multimodal generation across different domains, including\nimage, video, 3D, and audio, where we highlight the notable advancements with\nmilestone works in these fields. Specifically, we exhaustively investigate the\nkey technical components behind methods and multimodal datasets utilized in\nthese studies. Moreover, we dig into tool-augmented multimodal agents that can\nuse existing generative models for human-computer interaction. Lastly, we also\ncomprehensively discuss the advancement in AI safety and investigate emerging\napplications as well as future prospects. Our work provides a systematic and\ninsightful overview of multimodal generation, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\uff0c\u5c07 LLM \u8207\u591a\u6a21\u614b\u5b78\u7fd2\u76f8\u7d50\u5408\u7684\u8208\u8da3\u8207\u65e5\u4ff1\u589e\u3002\u5148\u524d\u5c0d\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u8abf\u67e5\u4e3b\u8981\u96c6\u4e2d\u5728\u7406\u89e3\u65b9\u9762\u3002\u672c\u8abf\u67e5\u95e1\u8ff0\u4e86\u4e0d\u540c\u9818\u57df\u7684\u591a\u6a21\u614b\u751f\u6210\uff0c\u5305\u62ec\u5716\u50cf\u3001\u5f71\u7247\u30013D \u548c\u97f3\u8a0a\uff0c\u6211\u5011\u5728\u9019\u4e9b\u9818\u57df\u4e2d\u5f37\u8abf\u4e86\u5177\u6709\u91cc\u7a0b\u7891\u610f\u7fa9\u7684\u4f5c\u54c1\u7684\u986f\u8457\u9032\u5c55\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8a73\u76e1\u5730\u63a2\u8a0e\u4e86\u9019\u4e9b\u7814\u7a76\u4e2d\u6240\u7528\u65b9\u6cd5\u548c\u591a\u6a21\u614b\u8cc7\u6599\u96c6\u80cc\u5f8c\u7684\u4e3b\u8981\u6280\u8853\u7d44\u6210\u90e8\u5206\u3002\u6b64\u5916\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u4e86\u5de5\u5177\u589e\u5f37\u7684\u591a\u6a21\u614b\u4ee3\u7406\uff0c\u9019\u4e9b\u4ee3\u7406\u53ef\u4ee5\u4f7f\u7528\u73fe\u6709\u7684\u751f\u6210\u6a21\u578b\u9032\u884c\u4eba\u6a5f\u4e92\u52d5\u3002\u6700\u5f8c\uff0c\u6211\u5011\u9084\u5168\u9762\u8a0e\u8ad6\u4e86 AI \u5b89\u5168\u6027\u7684\u9032\u5c55\uff0c\u4e26\u63a2\u8a0e\u4e86\u65b0\u8208\u61c9\u7528\u4ee5\u53ca\u672a\u4f86\u524d\u666f\u3002\u6211\u5011\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u591a\u6a21\u614b\u751f\u6210\u7684\u7cfb\u7d71\u4e14\u5bcc\u6709\u6d1e\u5bdf\u529b\u7684\u6982\u8ff0\uff0c\u9810\u8a08\u5c07\u63a8\u9032\u751f\u6210\u5167\u5bb9 (AIGC) \u548c\u4e16\u754c\u6a21\u578b\u7684\u4eba\u5de5\u667a\u6167\u767c\u5c55\u3002\u6240\u6709\u76f8\u95dc\u8ad6\u6587\u7684\u7cbe\u9078\u6e05\u55ae\u53ef\u5728 https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation \u627e\u5230", "author": "Yingqing He et.al.", "authors": "Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen", "id": "2405.19334v1", "paper_url": "http://arxiv.org/abs/2405.19334v1", "repo": "https://github.com/yingqinghe/awesome-llms-meet-multimodal-generation"}}