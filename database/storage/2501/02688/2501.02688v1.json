{"2501.02688": {"publish_time": "2025-01-05", "title": "Decoding specialised feature neurons in LLMs with the final projection layer", "paper_summary": "Large Language Models (LLMs) typically have billions of parameters and are\nthus often difficult to interpret in their operation. Such black-box models can\npose a significant risk to safety when trusted to make important decisions. The\nlack of interpretability of LLMs is more related to their sheer size, rather\nthan the complexity of their individual components. The TARS method for\nknowledge removal (Davies et al 2024) provides strong evidence for the\nhypothesis that that linear layer weights which act directly on the residual\nstream may have high correlation with different concepts encoded in the\nresidual stream. Building upon this, we attempt to decode neuron weights\ndirectly into token probabilities through the final projection layer of the\nmodel (the LM-head). Firstly, we show that with Llama 3.1 8B we can utilise the\nLM-head to decode specialised feature neurons that respond strongly to certain\nconcepts, with examples such as \"dog\" and \"California\". This is then confirmed\nby demonstrating that these neurons can be clamped to affect the probability of\nthe concept in the output. This extends to the fine-tuned assistant Llama 3.1\n8B instruct model, where we find that over 75% of neurons in the up-projection\nlayers have the same top associated token compared to the pretrained model.\nFinally, we demonstrate that clamping the \"dog\" neuron leads the instruct model\nto always discuss dogs when asked about its favourite animal. Through our\nmethod, it is possible to map the entirety of Llama 3.1 8B's up-projection\nneurons in less than 15 minutes with no parallelization.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u901a\u5e38\u6709\u6578\u5341\u5104\u500b\u53c3\u6578\uff0c\u56e0\u6b64\u901a\u5e38\u96e3\u4ee5\u89e3\u91cb\u5176\u904b\u4f5c\u65b9\u5f0f\u3002\u7576\u4fe1\u4efb\u9019\u4e9b\u9ed1\u76d2\u6a21\u578b\u505a\u51fa\u91cd\u8981\u6c7a\u7b56\u6642\uff0c\u9019\u4e9b\u6a21\u578b\u53ef\u80fd\u6703\u5c0d\u5b89\u5168\u6027\u69cb\u6210\u91cd\u5927\u98a8\u96aa\u3002LLM \u7684\u96e3\u4ee5\u89e3\u91cb\u6027\u8207\u5176\u9f90\u5927\u898f\u6a21\u6709\u95dc\uff0c\u800c\u975e\u5176\u500b\u5225\u7d44\u6210\u7684\u8907\u96dc\u6027\u3002TARS \u77e5\u8b58\u79fb\u9664\u65b9\u6cd5 (Davies \u7b49\u4eba 2024) \u70ba\u4ee5\u4e0b\u5047\u8a2d\u63d0\u4f9b\u4e86\u5f37\u6709\u529b\u7684\u8b49\u64da\uff1a\u76f4\u63a5\u4f5c\u7528\u65bc\u6b98\u5dee\u4e32\u6d41\u7684\u7dda\u6027\u5c64\u6b0a\u91cd\u53ef\u80fd\u8207\u6b98\u5dee\u4e32\u6d41\u4e2d\u7de8\u78bc\u7684\u4e0d\u540c\u6982\u5ff5\u9ad8\u5ea6\u76f8\u95dc\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u5617\u8a66\u901a\u904e\u6a21\u578b\u7684\u6700\u7d42\u6295\u5f71\u5c64\uff08LM \u982d\u90e8\uff09\u5c07\u795e\u7d93\u5143\u6b0a\u91cd\u76f4\u63a5\u89e3\u78bc\u70ba\u4ee3\u5e63\u6a5f\u7387\u3002\u9996\u5148\uff0c\u6211\u5011\u5c55\u793a\u4e86\u4f7f\u7528 Llama 3.1 8B\uff0c\u6211\u5011\u53ef\u4ee5\u4f7f\u7528 LM \u982d\u90e8\u89e3\u78bc\u5c0d\u67d0\u4e9b\u6982\u5ff5\u6709\u5f37\u70c8\u53cd\u61c9\u7684\u5c08\u696d\u7279\u5fb5\u795e\u7d93\u5143\uff0c\u4f8b\u5982\u300c\u72d7\u300d\u548c\u300c\u52a0\u5dde\u300d\u3002\u9019\u96a8\u5f8c\u901a\u904e\u8b49\u660e\u9019\u4e9b\u795e\u7d93\u5143\u53ef\u4ee5\u88ab\u7b9d\u5236\u4ee5\u5f71\u97ff\u8f38\u51fa\u4e2d\u6982\u5ff5\u7684\u6a5f\u7387\u800c\u5f97\u5230\u8b49\u5be6\u3002\u9019\u5ef6\u4f38\u5230\u5fae\u8abf\u5f8c\u7684\u52a9\u7406 Llama 3.1 8B \u6307\u4ee4\u6a21\u578b\uff0c\u6211\u5011\u767c\u73fe\u4e0a\u6295\u5f71\u5c64\u4e2d\u8d85\u904e 75% \u7684\u795e\u7d93\u5143\u8207\u9810\u8a13\u7df4\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u76f8\u540c\u7684\u9802\u7d1a\u95dc\u806f\u4ee3\u5e63\u3002\u6700\u5f8c\uff0c\u6211\u5011\u8b49\u660e\u7b9d\u5236\u300c\u72d7\u300d\u795e\u7d93\u5143\u6703\u5c0e\u81f4\u6307\u4ee4\u6a21\u578b\u5728\u88ab\u554f\u53ca\u6700\u559c\u6b61\u7684\u52d5\u7269\u6642\u7e3d\u662f\u8a0e\u8ad6\u72d7\u3002\u900f\u904e\u6211\u5011\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u5230 15 \u5206\u9418\u5167\u5c07 Llama 3.1 8B \u7684\u4e0a\u6295\u5f71\u795e\u7d93\u5143\u6574\u9ad4\u5c0d\u61c9\uff0c\u4e14\u7121\u4e26\u884c\u5316\u3002", "author": "Harry J Davies et.al.", "authors": "Harry J Davies", "id": "2501.02688v1", "paper_url": "http://arxiv.org/abs/2501.02688v1", "repo": "null"}}