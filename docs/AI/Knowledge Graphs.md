
### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-12**|**Body Transformer: Leveraging Robot Embodiment for Policy Learning**|Carmelo Sferrazza et.al.|[2408.06316v1](http://arxiv.org/abs/2408.06316v1)|null|
|**2024-08-12**|**ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation Using Large Language Models and Transformers**|Aristi Papastavrou et.al.|[2408.06040v1](http://arxiv.org/abs/2408.06040v1)|null|
|**2024-08-12**|**ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models**|Ronak Pradeep et.al.|[2408.05948v1](http://arxiv.org/abs/2408.05948v1)|null|
|**2024-08-11**|**The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms**|Adam Davies et.al.|[2408.05859v1](http://arxiv.org/abs/2408.05859v1)|null|
|**2024-08-10**|**Investigating Instruction Tuning Large Language Models on Graphs**|Kerui Zhu et.al.|[2408.05457v1](http://arxiv.org/abs/2408.05457v1)|null|
|**2024-08-10**|**Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation**|Wenbo Shang et.al.|[2408.05456v1](http://arxiv.org/abs/2408.05456v1)|null|
|**2024-08-10**|**LaiDA: Linguistics-aware In-context Learning with Data Augmentation for Metaphor Components Identification**|Hongde Liu et.al.|[2408.05404v1](http://arxiv.org/abs/2408.05404v1)|null|
|**2024-08-09**|**SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV Battery Supply Chain Disruptions**|Zhi-Qi Cheng et.al.|[2408.05357v1](http://arxiv.org/abs/2408.05357v1)|null|
|**2024-08-09**|**A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning**|Ye Yuan et.al.|[2408.05141v1](http://arxiv.org/abs/2408.05141v1)|null|
|**2024-08-09**|**HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction**|Bhaskarjit Sarmah et.al.|[2408.04948v1](http://arxiv.org/abs/2408.04948v1)|null|
|**2024-08-08**|**DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization**|Xin Sun et.al.|[2408.04400v1](http://arxiv.org/abs/2408.04400v1)|null|
|**2024-08-08**|**MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models**|Haoxuan Li et.al.|[2408.04388v1](http://arxiv.org/abs/2408.04388v1)|[link](https://github.com/luminosityx/mm-forecast)|
|**2024-08-08**|**Judgment2vec: Apply Graph Analytics to Searching and Recommendation of Similar Judgments**|Hsuan-Lei Shao et.al.|[2408.04382v1](http://arxiv.org/abs/2408.04382v1)|null|
|**2024-08-08**|**wav2graph: A Framework for Supervised Learning Knowledge Graph from Speech**|Khai Le-Duc et.al.|[2408.04174v1](http://arxiv.org/abs/2408.04174v1)|[link](https://github.com/leduckhai/wav2graph)|
|**2024-08-07**|**ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling**|William Y. Zhu et.al.|[2408.04102v1](http://arxiv.org/abs/2408.04102v1)|null|
|**2024-08-07**|**CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases**|Xiangyan Liu et.al.|[2408.03910v2](http://arxiv.org/abs/2408.03910v2)|[link](https://github.com/modelscope/modelscope-agent)|
|**2024-08-07**|**PAGED: A Benchmark for Procedural Graphs Extraction from Documents**|Weihong Du et.al.|[2408.03630v2](http://arxiv.org/abs/2408.03630v2)|null|
|**2024-08-07**|**Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks**|Zaijing Li et.al.|[2408.03615v1](http://arxiv.org/abs/2408.03615v1)|null|
|**2024-08-07**|**Exploring the extent of similarities in software failures across industries using LLMs**|Martin Detloff et.al.|[2408.03528v2](http://arxiv.org/abs/2408.03528v2)|null|
|**2024-08-06**|**Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion**|Jinglong Gao et.al.|[2408.03079v1](http://arxiv.org/abs/2408.03079v1)|null|
|**2024-08-06**|**Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs**|Daniel Steinigen et.al.|[2408.03010v1](http://arxiv.org/abs/2408.03010v1)|null|
|**2024-08-06**|**Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large Language Model-Based Question Answering**|Tiezheng Guo et.al.|[2408.02907v1](http://arxiv.org/abs/2408.02907v1)|null|
|**2024-08-05**|**MaterioMiner -- An ontology-based text mining dataset for extraction of process-structure-property entities**|Ali Riza Durmaz et.al.|[2408.04661v1](http://arxiv.org/abs/2408.04661v1)|null|
|**2024-08-05**|**A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models**|Vanni Zavarella et.al.|[2408.02377v1](http://arxiv.org/abs/2408.02377v1)|null|
|**2024-08-05**|**Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction**|Albert Sawczyn et.al.|[2408.02337v1](http://arxiv.org/abs/2408.02337v1)|null|
|**2024-08-04**|**MedSyn: LLM-based Synthetic Medical Text Generation Framework**|Gleb Kumichev et.al.|[2408.02056v1](http://arxiv.org/abs/2408.02056v1)|null|
|**2024-08-04**|**DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models**|Bowen Wang et.al.|[2408.01933v2](http://arxiv.org/abs/2408.01933v2)|null|
|**2024-08-03**|**PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language Models**|Alexey Tikhonov et.al.|[2408.04648v1](http://arxiv.org/abs/2408.04648v1)|[link](https://github.com/altsoph/plugh)|
|**2024-08-03**|**Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data**|Antonio De Santis et.al.|[2408.01700v1](http://arxiv.org/abs/2408.01700v1)|null|
|**2024-08-02**|**DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs**|Zhichun Wang et.al.|[2408.01154v1](http://arxiv.org/abs/2408.01154v1)|null|
|**2024-08-02**|**Bridging Information Gaps in Dialogues With Grounded Exchanges Using Knowledge Graphs**|Phillip Schneider et.al.|[2408.01088v2](http://arxiv.org/abs/2408.01088v2)|[link](https://github.com/philotron/bridge-kg)|
|**2024-08-02**|**Automatic Extraction of Relationships among Motivations, Emotions and Actions from Natural Language Texts**|Fei Yang et.al.|[2408.00966v1](http://arxiv.org/abs/2408.00966v1)|null|
|**2024-08-01**|**DisTrack: a new Tool for Semi-automatic Misinformation Tracking in Online Social Networks**|Guillermo Villar-Rodr√≠guez et.al.|[2408.00633v1](http://arxiv.org/abs/2408.00633v1)|null|
|**2024-08-01**|**On the Limitations and Prospects of Machine Unlearning for Generative AI**|Shiji Zhou et.al.|[2408.00376v1](http://arxiv.org/abs/2408.00376v1)|null|
|**2024-08-01**|**Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network**|Bin Cheng et.al.|[2408.00290v1](http://arxiv.org/abs/2408.00290v1)|null|
|**2024-07-31**|**CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**|Stefan Langer et.al.|[2407.21708v1](http://arxiv.org/abs/2407.21708v1)|null|
|**2024-07-31**|**eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**|Xinyi Pan et.al.|[2407.21483v3](http://arxiv.org/abs/2407.21483v3)|null|
|**2024-07-31**|**Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**|Haodong Hong et.al.|[2407.21452v1](http://arxiv.org/abs/2407.21452v1)|null|
|**2024-07-31**|**Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**|Elan Markowitz et.al.|[2407.21358v1](http://arxiv.org/abs/2407.21358v1)|[link](https://github.com/amazon-science/tree-of-traversals)|
|**2024-07-31**|**SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**|Peiru Zheng et.al.|[2407.21293v1](http://arxiv.org/abs/2407.21293v1)|null|
|**2024-07-30**|**Be aware of overfitting by hyperparameter optimization!**|Igor V. Tetko et.al.|[2407.20786v1](http://arxiv.org/abs/2407.20786v1)|null|
|**2024-07-30**|**Harvesting Textual and Structured Data from the HAL Publication Repository**|Francis Kulumba et.al.|[2407.20595v1](http://arxiv.org/abs/2407.20595v1)|null|
|**2024-07-30**|**CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge**|Tianshi Zheng et.al.|[2407.20564v1](http://arxiv.org/abs/2407.20564v1)|null|
|**2024-07-30**|**Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language**|Hossein Rajaby Faghihi et.al.|[2407.20513v1](http://arxiv.org/abs/2407.20513v1)|null|
|**2024-07-29**|**What if Red Can Talk? Dynamic Dialogue Generation Using Large Language Models**|Navapat Nananukul et.al.|[2407.20382v1](http://arxiv.org/abs/2407.20382v1)|null|
|**2024-07-29**|**MindSearch: Mimicking Human Minds Elicits Deep AI Searcher**|Zehui Chen et.al.|[2407.20183v1](http://arxiv.org/abs/2407.20183v1)|[link](https://github.com/internlm/mindsearch)|
|**2024-07-29**|**rLLM: Relational Table Learning with LLMs**|Weichen Li et.al.|[2407.20157v1](http://arxiv.org/abs/2407.20157v1)|[link](https://github.com/rllm-project/rllm)|
|**2024-07-29**|**Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation**|Yunsheng Wang et.al.|[2407.19643v2](http://arxiv.org/abs/2407.19643v2)|[link](https://github.com/iamryanshengwang/prometheus-chatbot)|
|**2024-07-29**|**TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs**|Selma Wanna et.al.|[2407.19616v1](http://arxiv.org/abs/2407.19616v1)|null|
|**2024-07-27**|**Semantic Communication Enhanced by Knowledge Graph Representation Learning**|Nour Hello et.al.|[2407.19338v1](http://arxiv.org/abs/2407.19338v1)|null|
|**2024-07-26**|**GraphBPE: Molecular Graphs Meet Byte-Pair Encoding**|Yuchen Shen et.al.|[2407.19039v1](http://arxiv.org/abs/2407.19039v1)|[link](https://github.com/A-Chicharito-S/GraphBPE)|
|**2024-07-26**|**Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery**|Yuni Susanti et.al.|[2407.18752v3](http://arxiv.org/abs/2407.18752v3)|[link](https://github.com/littleflow3r/kg-structure-as-prompt)|
|**2024-07-26**|**Using GPT-4 to guide causal machine learning**|Anthony C. Constantinou et.al.|[2407.18607v1](http://arxiv.org/abs/2407.18607v1)|null|
|**2024-07-26**|**Multi-turn Response Selection with Commonsense-enhanced Language Models**|Yuandong Wang et.al.|[2407.18479v1](http://arxiv.org/abs/2407.18479v1)|null|
|**2024-07-25**|**Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning**|Sindhura Kommu et.al.|[2407.18181v1](http://arxiv.org/abs/2407.18181v1)|null|
|**2024-07-24**|**MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents**|Arya Bulusu et.al.|[2407.17544v1](http://arxiv.org/abs/2407.17544v1)|[link](https://github.com/emergenceai/mathviz-e)|
|**2024-07-23**|**Ranking protein-protein models with large language models and graph neural networks**|Xiaotong Xu et.al.|[2407.16375v1](http://arxiv.org/abs/2407.16375v1)|[link](https://github.com/haddocking/deeprank-gnn-esm)|
|**2024-07-23**|**PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets**|Jaeyoung Kim et.al.|[2407.16329v1](http://arxiv.org/abs/2407.16329v1)|null|
|**2024-07-23**|**Graph-Structured Speculative Decoding**|Zhuocheng Gong et.al.|[2407.16207v1](http://arxiv.org/abs/2407.16207v1)|null|
|**2024-07-23**|**Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval**|Yannick Assogba et.al.|[2407.21049v1](http://arxiv.org/abs/2407.21049v1)|null|
|**2024-07-23**|**Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion**|Yang Liu et.al.|[2407.16127v1](http://arxiv.org/abs/2407.16127v1)|[link](https://github.com/nju-websoft/dift)|
|**2024-07-22**|**Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts**|Soojin Yoon et.al.|[2407.15588v1](http://arxiv.org/abs/2407.15588v1)|[link](https://github.com/eralign/eralign)|
|**2024-07-22**|**The Ontoverse: Democratising Access to Knowledge Graph-based Data Through a Cartographic Interface**|Johannes Zimmermann et.al.|[2408.03339v1](http://arxiv.org/abs/2408.03339v1)|null|
|**2024-07-22**|**Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs**|Huanjing Zhao et.al.|[2407.15431v1](http://arxiv.org/abs/2407.15431v1)|null|
|**2024-07-22**|**LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation**|Jiaxing Zhang et.al.|[2407.15351v2](http://arxiv.org/abs/2407.15351v2)|null|
|**2024-07-21**|**Text-Augmented Multimodal LLMs for Chemical Reaction Condition Recommendation**|Yu Zhang et.al.|[2407.15141v1](http://arxiv.org/abs/2407.15141v1)|null|
|**2024-07-20**|**On the Design and Analysis of LLM-Based Algorithms**|Yanxi Chen et.al.|[2407.14788v1](http://arxiv.org/abs/2407.14788v1)|[link](https://github.com/modelscope/agentscope)|
|**2024-07-19**|**LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits**|Chen-Chia Chang et.al.|[2407.18269v1](http://arxiv.org/abs/2407.18269v1)|null|
|**2024-07-19**|**Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition**|Suvajit Patra et.al.|[2407.14224v1](http://arxiv.org/abs/2407.14224v1)|null|
|**2024-07-19**|**Enhancing Data-Limited Graph Neural Networks by Actively Distilling Knowledge from Large Language Models**|Quan Li et.al.|[2407.13989v1](http://arxiv.org/abs/2407.13989v1)|null|
|**2024-07-18**|**A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice**|Shaina Raza et.al.|[2407.13699v1](http://arxiv.org/abs/2407.13699v1)|null|
|**2024-07-18**|**MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains**|Guoli Yin et.al.|[2407.18961v2](http://arxiv.org/abs/2407.18961v2)|[link](https://github.com/apple/axlearn)|
|**2024-07-17**|**Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?**|Ben Yao et.al.|[2407.12725v1](http://arxiv.org/abs/2407.12725v1)|null|
|**2024-07-17**|**Subgraph-Aware Training of Text-based Methods for Knowledge Graph Completion**|Youmin Ko et.al.|[2407.12703v3](http://arxiv.org/abs/2407.12703v3)|null|
|**2024-07-17**|**Abstraction Alignment: Comparing Model and Human Conceptual Relationships**|Angie Boggust et.al.|[2407.12543v1](http://arxiv.org/abs/2407.12543v1)|[link](https://github.com/mitvis/abstraction-alignment)|
|**2024-07-17**|**Struct-X: Enhancing Large Language Models Reasoning with Structured Data**|Xiaoyu Tan et.al.|[2407.12522v1](http://arxiv.org/abs/2407.12522v1)|null|
|**2024-07-17**|**Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models**|Alexander R. Pelletier et.al.|[2407.12888v1](http://arxiv.org/abs/2407.12888v1)|[link](https://github.com/pinglab-utils/rugged)|
|**2024-07-16**|**A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting**|He Chang et.al.|[2407.11638v1](http://arxiv.org/abs/2407.11638v1)|null|
|**2024-07-16**|**Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness**|Kai Guo et.al.|[2407.12068v2](http://arxiv.org/abs/2407.12068v2)|null|
|**2024-07-16**|**CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**|Kalliopi Basioti et.al.|[2407.11393v2](http://arxiv.org/abs/2407.11393v2)|[link](https://github.com/SamsungLabs/CIC-BART-SSA)|
|**2024-07-15**|**Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval**|Shengjie Ma et.al.|[2407.10805v3](http://arxiv.org/abs/2407.10805v3)|null|
|**2024-07-15**|**Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education**|Rui Yang et.al.|[2407.10794v1](http://arxiv.org/abs/2407.10794v1)|[link](https://github.com/irenezihuili/cgprompt)|
|**2024-07-15**|**GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework**|Hannah Sansford et.al.|[2407.10793v1](http://arxiv.org/abs/2407.10793v1)|null|
|**2024-07-15**|**Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs**|W. J. Meijer et.al.|[2407.10743v1](http://arxiv.org/abs/2407.10743v1)|null|
|**2024-07-14**|**AutoGRAMS: Autonomous Graphical Agent Modeling Software**|Ben Krause et.al.|[2407.10049v1](http://arxiv.org/abs/2407.10049v1)|[link](https://github.com/autograms/autograms)|
|**2024-07-13**|**FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language based on Textually Represented Environments**|Dimitris Papadopoulos et.al.|[2407.09888v1](http://arxiv.org/abs/2407.09888v1)|[link](https://github.com/lighteternal/farfetched_nlp)|
|**2024-07-12**|**GOFA: A Generative One-For-All Model for Joint Graph Language Modeling**|Lecheng Kong et.al.|[2407.09709v1](http://arxiv.org/abs/2407.09709v1)|[link](https://github.com/jiaruifeng/gofa)|
|**2024-07-12**|**Human-like Episodic Memory for Infinite Context LLMs**|Zafeirios Fountas et.al.|[2407.09450v1](http://arxiv.org/abs/2407.09450v1)|null|
|**2024-07-12**|**The $Œº\mathcal{G}$ Language for Programming Graph Neural Networks**|Matteo Belenchia et.al.|[2407.09441v1](http://arxiv.org/abs/2407.09441v1)|null|
|**2024-07-12**|**Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations**|David N. Palacio et.al.|[2407.08983v1](http://arxiv.org/abs/2407.08983v1)|null|
|**2024-07-12**|**Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification**|Ke Ji et.al.|[2407.08959v1](http://arxiv.org/abs/2407.08959v1)|null|
|**2024-07-11**|**Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight**|Zhiqiang Xie et.al.|[2407.08694v1](http://arxiv.org/abs/2407.08694v1)|null|
|**2024-07-11**|**Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents**|Haoyi Xiong et.al.|[2407.08516v4](http://arxiv.org/abs/2407.08516v4)|null|
|**2024-07-10**|**A Comprehensive Survey on the Security of Smart Grid: Challenges, Mitigations, and Future Research Opportunities**|Arastoo Zibaeirad et.al.|[2407.07966v1](http://arxiv.org/abs/2407.07966v1)|null|
|**2024-07-10**|**Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**|Hao-Tien Lewis Chiang et.al.|[2407.07775v2](http://arxiv.org/abs/2407.07775v2)|null|
|**2024-07-10**|**Teaching Transformers Causal Reasoning through Axiomatic Training**|Aniket Vashishtha et.al.|[2407.07612v1](http://arxiv.org/abs/2407.07612v1)|null|
|**2024-07-10**|**STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained LLMs**|Aaron Zolnai-Lucas et.al.|[2407.12860v1](http://arxiv.org/abs/2407.12860v1)|[link](https://github.com/aaronzo/STAGE)|
|**2024-07-10**|**GLBench: A Comprehensive Benchmark for Graph with Large Language Models**|Yuhan Li et.al.|[2407.07457v2](http://arxiv.org/abs/2407.07457v2)|[link](https://github.com/nineabyss/glbench)|
|**2024-07-09**|**Decoding Climate Disagreement: A Graph Neural Network-Based Approach to Understanding Social Media Dynamics**|Ruiran Su et.al.|[2407.07038v1](http://arxiv.org/abs/2407.07038v1)|null|
|**2024-07-09**|**Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions**|Yu-Guan Hsieh et.al.|[2407.06723v1](http://arxiv.org/abs/2407.06723v1)|null|

#### Abstracts
##### **Body Transformer: Leveraging Robot Embodiment for Policy Learning**
2408.06316v1 by Carmelo Sferrazza, Dun-Ming Huang, Fangchen Liu, Jongmin Lee, Pieter Abbeel

In recent years, the transformer architecture has become the de facto
standard for machine learning algorithms applied to natural language processing
and computer vision. Despite notable evidence of successful deployment of this
architecture in the context of robot learning, we claim that vanilla
transformers do not fully exploit the structure of the robot learning problem.
Therefore, we propose Body Transformer (BoT), an architecture that leverages
the robot embodiment by providing an inductive bias that guides the learning
process. We represent the robot body as a graph of sensors and actuators, and
rely on masked attention to pool information throughout the architecture. The
resulting architecture outperforms the vanilla transformer, as well as the
classical multilayer perceptron, in terms of task completion, scaling
properties, and computational efficiency when representing either imitation or
reinforcement learning policies. Additional material including the open-source
code is available at https://sferrazza.cc/bot_site.

ÊëòË¶ÅÔºöËøëÂπ¥Êù•ÔºåÂèòÂéãÂô®Êû∂ÊûÑÂ∑≤Êàê‰∏∫Â∫îÁî®‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆóÊú∫ËßÜËßâÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÁöÑÂÆûÈôÖÊ†áÂáÜ„ÄÇÂ∞ΩÁÆ°ÊúâÊòæÁùÄËØÅÊçÆË°®ÊòéÂú®Êú∫Âô®‰∫∫Â≠¶‰π†ÁöÑËÉåÊôØ‰∏ãÊàêÂäüÈÉ®ÁΩ≤‰∫ÜÊ≠§Êû∂ÊûÑÔºå‰ΩÜÊàë‰ª¨Â£∞Áß∞ÂéüÂßãÂèòÂéãÂô®Âπ∂Êú™ÂÖÖÂàÜÂà©Áî®Êú∫Âô®‰∫∫Â≠¶‰π†ÈóÆÈ¢òÁöÑÁªìÊûÑ„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü Body Transformer (BoT)Ôºå‰∏ÄÁßçÈÄöËøáÊèê‰æõÊåáÂØºÂ≠¶‰π†ËøáÁ®ãÁöÑÂΩíÁ∫≥ÂÅèÂ∑ÆÊù•Âà©Áî®Êú∫Âô®‰∫∫‰ΩìÁé∞ÁöÑÊû∂ÊûÑ„ÄÇÊàë‰ª¨Â∞ÜÊú∫Âô®‰∫∫‰∏ª‰ΩìË°®Á§∫‰∏∫‰º†ÊÑüÂô®ÂíåÊâßË°åÂô®ÁöÑÂõæÂΩ¢ÔºåÂπ∂‰æùÈù†Êé©Á†ÅÊ≥®ÊÑèÂäõÊù•Ê±áÈõÜÊï¥‰∏™Êû∂ÊûÑ‰∏≠ÁöÑ‰ø°ÊÅØ„ÄÇÂú®ÂÆåÊàê‰ªªÂä°„ÄÅÁº©ÊîæÂ±ûÊÄßÂíåËÆ°ÁÆóÊïàÁéáÊñπÈù¢ÔºåÊó†ËÆ∫ÊòØË°®Á§∫Ê®°‰ªøËøòÊòØÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÔºåÁî±Ê≠§‰∫ßÁîüÁöÑÊû∂ÊûÑÈÉΩ‰ºò‰∫éÂéüÂßãÂèòÂéãÂô®‰ª•ÂèäÁªèÂÖ∏ÁöÑÂ§öÂ±ÇÊÑüÁü•Âô®„ÄÇÂåÖÊã¨ÂºÄÊ∫ê‰ª£Á†ÅÂú®ÂÜÖÁöÑÂÖ∂‰ªñÊùêÊñôÂèØ‰ªé https://sferrazza.cc/bot_site Ëé∑Âæó„ÄÇ

##### **ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation Using Large Language Models and Transformers**
2408.06040v1 by Aristi Papastavrou, Maria Lymperaiou, Giorgos Stamou

In the rapidly evolving fields of natural language processing and computer
vision, Visual Word Sense Disambiguation (VWSD) stands as a critical, yet
challenging task. The quest for models that can seamlessly integrate and
interpret multimodal data is more pressing than ever. Imagine a system that can
understand language with the depth and nuance of human cognition, while
simultaneously interpreting the rich visual context of the world around it.
  We present ARPA, an architecture that fuses the unparalleled contextual
understanding of large language models with the advanced feature extraction
capabilities of transformers, which then pass through a custom Graph Neural
Network (GNN) layer to learn intricate relationships and subtle nuances within
the data. This innovative architecture not only sets a new benchmark in visual
word disambiguation but also introduces a versatile framework poised to
transform how linguistic and visual data interact by harnessing the synergistic
strengths of its components, ensuring robust performance even in the most
complex disambiguation scenarios. Through a series of experiments and
comparative analysis, we reveal the substantial advantages of our model,
underscoring its potential to redefine standards in the field. Beyond its
architectural prowess, our architecture excels through experimental
enrichments, including sophisticated data augmentation and multi-modal training
techniques.
  ARPA's introduction marks a significant milestone in visual word
disambiguation, offering a compelling solution that bridges the gap between
linguistic and visual modalities. We invite researchers and practitioners to
explore the capabilities of our model, envisioning a future where such hybrid
models drive unprecedented advancements in artificial intelligence.

ÊëòË¶ÅÔºöÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÈõªËÖ¶Ë¶ñË¶∫Âø´ÈÄüÊºîÈÄ≤ÁöÑÈ†òÂüü‰∏≠ÔºåË¶ñË¶∫Ë©ûÂΩôÊ∂àÊ≠ß (VWSD) ÊòØ‰∏ÄÂÄãÈóúÈçµ‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇÂ∞ãÊâæËÉΩÂ§†ÁÑ°Á∏´Êï¥ÂêàÂíåË©ÆÈáãÂ§öÊ®°ÊÖãË≥áÊñôÁöÑÊ®°ÂûãÊØî‰ª•ÂæÄ‰ªª‰ΩïÊôÇÂÄôÈÉΩÊõ¥Âä†Ëø´Âàá„ÄÇÊÉ≥ÂÉè‰∏ÄÂÄãÁ≥ªÁµ±ÔºåÂÆÉÂèØ‰ª•ÂÉè‰∫∫È°ûË™çÁü•‰∏ÄÊ®£Ê∑±ÂÖ•‰∏îÁ¥∞Á∑ªÂú∞ÁêÜËß£Ë™ûË®ÄÔºåÂêåÊôÇÈÇÑËÉΩË©ÆÈáãÂë®Âúç‰∏ñÁïåÁöÑË±êÂØåË¶ñË¶∫ËÑàÁµ°„ÄÇ
ÊàëÂÄëÊèêÂá∫ ARPAÔºå‰∏ÄÁ®ÆÊû∂ÊßãÔºåÂÆÉËûçÂêà‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁÑ°ËàáÂÄ´ÊØîÁöÑËÑàÁµ°ÁêÜËß£ËÉΩÂäõÂíå Transformer ÁöÑÈÄ≤ÈöéÁâπÂæµËêÉÂèñËÉΩÂäõÔºåÁÑ∂ÂæåÈÄöÈÅé‰∏ÄÂÄãËá™Ë®ÇÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â±§‰æÜÂ≠∏ÁøíË≥áÊñô‰∏≠ÁöÑË§áÈõúÈóú‰øÇÂíåÁ¥∞ÂæÆÂ∑ÆÁï∞„ÄÇÈÄôÁ®ÆÂâµÊñ∞ÁöÑÊû∂Êßã‰∏çÂÉÖÂú®Ë¶ñË¶∫Ë©ûÂΩôÊ∂àÊ≠ß‰∏≠Ë®≠ÂÆö‰∫ÜÊñ∞ÁöÑÂü∫Ê∫ñÔºåÈÇÑÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂ§öÂäüËÉΩÁöÑÊ°ÜÊû∂ÔºåÊ∫ñÂÇôÈÄöÈÅéÂà©Áî®ÂÖ∂ÁµÑÊàêÈÉ®ÂàÜÁöÑÂçîÂêåÂÑ™Âã¢‰æÜËΩâËÆäË™ûË®ÄÂíåË¶ñË¶∫Ë≥áÊñôÁöÑ‰∫íÂãïÊñπÂºèÔºåÁ¢∫‰øùÂç≥‰ΩøÂú®ÊúÄË§áÈõúÁöÑÊ∂àÊ≠ßÂ†¥ÊôØ‰∏≠‰πüËÉΩÊúâÂº∑ÂÅ•ÁöÑÊïàËÉΩ„ÄÇÈÄèÈÅé‰∏ÄÁ≥ªÂàóÁöÑÂØ¶È©óÂíåÊØîËºÉÂàÜÊûêÔºåÊàëÂÄëÊè≠Á§∫‰∫ÜÊàëÂÄëÊ®°ÂûãÁöÑÈ°ØËëóÂÑ™Âã¢ÔºåÂº∑Ë™ø‰∫ÜÂÆÉÂú®ÈáçÊñ∞ÂÆöÁæ©Ë©≤È†òÂüüÊ®ôÊ∫ñÁöÑÊΩõÂäõ„ÄÇÈô§‰∫ÜÂÖ∂Êû∂ÊßãÂÑ™Âã¢‰πãÂ§ñÔºåÊàëÂÄëÁöÑÊû∂ÊßãÈÇÑÈÄöÈÅéÂØ¶È©óË±êÂØåÂåñËÄåË°®ÁèæÂá∫Ëâ≤ÔºåÂåÖÊã¨Á≤æÂØÜÁöÑË≥áÊñôÊì¥ÂÖÖÂíåÂ§öÊ®°ÊÖãË®ìÁ∑¥ÊäÄË°ì„ÄÇ
ARPA ÁöÑÊé®Âá∫Ê®ôË™åËëóË¶ñË¶∫Ë©ûÂΩôÊ∂àÊ≠ßÁöÑ‰∏ÄÂÄãÈáçË¶ÅÈáåÁ®ãÁ¢ëÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂºï‰∫∫Ê≥®ÁõÆÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂΩåÂêà‰∫ÜË™ûË®ÄÂíåË¶ñË¶∫Ê®°ÊÖã‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇÊàëÂÄëÈÇÄË´ãÁ†îÁ©∂‰∫∫Âì°ÂíåÂæûÊ•≠‰∫∫Âì°Êé¢Á¥¢ÊàëÂÄëÊ®°ÂûãÁöÑËÉΩÂäõÔºåÂ±ïÊúõ‰∏ÄÂÄãÁî±ÈÄôÁ®ÆÊ∑∑ÂêàÊ®°ÂûãÊé®Âãï‰∫∫Â∑•Êô∫ÊÖßÂâçÊâÄÊú™ÊúâÁöÑÈÄ≤Ê≠•ÁöÑÊú™‰æÜ„ÄÇ

##### **ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models**
2408.05948v1 by Ronak Pradeep, Daniel Lee, Ali Mousavi, Jeff Pound, Yisi Sang, Jimmy Lin, Ihab Ilyas, Saloni Potdar, Mostafa Arefiyan, Yunyao Li

The rapid advancement of Large Language Models (LLMs) and conversational
assistants necessitates dynamic, scalable, and configurable conversational
datasets for training and evaluation. These datasets must accommodate diverse
user interaction modes, including text and voice, each presenting unique
modeling challenges. Knowledge Graphs (KGs), with their structured and evolving
nature, offer an ideal foundation for current and precise knowledge. Although
human-curated KG-based conversational datasets exist, they struggle to keep
pace with the rapidly changing user information needs. We present ConvKGYarn, a
scalable method for generating up-to-date and configurable conversational KGQA
datasets. Qualitative psychometric analyses confirm our method can generate
high-quality datasets rivaling a popular conversational KGQA dataset while
offering it at scale and covering a wide range of human-interaction
configurations. We showcase its utility by testing LLMs on diverse
conversations - exploring model behavior on conversational KGQA sets with
different configurations grounded in the same KG fact set. Our results
highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate
parametric knowledge of LLMs, thus offering a robust solution to the constantly
evolving landscape of conversational assistants.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÂ∞çË©±ÂºèÂä©ÁêÜÁöÑÂø´ÈÄüÈÄ≤Ê≠•ÔºåÈúÄË¶ÅÂãïÊÖã„ÄÅÂèØÊì¥ÂÖÖ‰∏îÂèØË®≠ÂÆöÁöÑÂ∞çË©±ÂºèË≥áÊñôÈõÜ‰æÜÈÄ≤Ë°åË®ìÁ∑¥ÂíåË©ï‰º∞„ÄÇÈÄô‰∫õË≥áÊñôÈõÜÂøÖÈ†àÂÆπÁ¥ç‰∏çÂêåÁöÑ‰ΩøÁî®ËÄÖ‰∫íÂãïÊ®°ÂºèÔºåÂåÖÊã¨ÊñáÂ≠óÂíåË™ûÈü≥ÔºåÊØèÁ®ÆÊ®°ÂºèÈÉΩÂëàÁèæÁç®ÁâπÁöÑÂª∫Ê®°ÊåëÊà∞„ÄÇÁü•Ë≠òÂúñË≠ú (KG) ÂÖ∑ÊúâÁµêÊßãÂåñ‰∏î‰∏çÊñ∑ÊºîÈÄ≤ÁöÑÁâπÊÄßÔºåÁÇ∫Áï∂ÂâçÂíåÁ≤æÁ¢∫ÁöÑÁü•Ë≠òÊèê‰æõ‰∫ÜÁêÜÊÉ≥ÁöÑÂü∫Á§é„ÄÇÂÑòÁÆ°Â≠òÂú®‰∫∫Â∑•Á≠ñÂ±ïÁöÑÂü∫ÊñºÁü•Ë≠òÂúñË≠úÁöÑÂ∞çË©±ÂºèË≥áÊñôÈõÜÔºå‰ΩÜÂÆÉÂÄëÈõ£‰ª•Ë∑ü‰∏äÂø´ÈÄüËÆäÂåñÁöÑ‰ΩøÁî®ËÄÖË≥áË®äÈúÄÊ±Ç„ÄÇÊàëÂÄëÊèêÂá∫ ConvKGYarnÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂèØÊì¥ÂÖÖÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÁî¢ÁîüÊúÄÊñ∞ÁöÑ‰∏îÂèØË®≠ÂÆöÁöÑÂ∞çË©±Âºè KGQA Ë≥áÊñôÈõÜ„ÄÇÂÆöÊÄßÁöÑÂøÉÁêÜÊ∏¨ÈáèÂàÜÊûêË≠âÂØ¶ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•Áî¢ÁîüËàáÊµÅË°åÁöÑÂ∞çË©±Âºè KGQA Ë≥áÊñôÈõÜÁõ∏Â™≤ÁæéÁöÑÂÑ™Ë≥™Ë≥áÊñôÈõÜÔºåÂêåÊôÇÂ§ßË¶èÊ®°Êèê‰æõË≥áÊñôÈõÜÔºå‰∏¶Ê∂µËìãÂª£Ê≥õÁöÑ‰∫∫Ê©ü‰∫íÂãïË®≠ÂÆö„ÄÇÊàëÂÄëÈÄèÈÅéÂú®‰∏çÂêåÁöÑÂ∞çË©±‰∏≠Ê∏¨Ë©¶ LLM ‰æÜÂ±ïÁ§∫ÂÖ∂ÊïàÁî®ÔºåÊé¢Á¥¢Ê®°ÂûãÂú®Â∞çË©±Âºè KGQA Ë®≠ÂÆö‰∏äÁöÑË°åÁÇ∫ÔºåÈÄô‰∫õË®≠ÂÆöÂü∫ÊñºÁõ∏ÂêåÁöÑÁü•Ë≠òÂúñË≠ú‰∫ãÂØ¶ÈõÜ„ÄÇÊàëÂÄëÁöÑÁµêÊûúÁ™ÅÈ°Ø‰∫Ü ConvKGYarn ÊîπÂñÑ KGQA Âü∫Á§éÂíåË©ï‰º∞ LLM ÂèÉÊï∏ÂåñÁü•Ë≠òÁöÑËÉΩÂäõÔºåÂæûËÄåÁÇ∫‰∏çÊñ∑ÊºîÈÄ≤ÁöÑÂ∞çË©±ÂºèÂä©ÁêÜÈ†òÂüüÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂº∑Â§ßÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms**
2408.05859v1 by Adam Davies, Ashkan Khakzar

Artificial neural networks have long been understood as "black boxes": though
we know their computation graphs and learned parameters, the knowledge encoded
by these weights and functions they perform are not inherently interpretable.
As such, from the early days of deep learning, there have been efforts to
explain these models' behavior and understand them internally; and recently,
mechanistic interpretability (MI) has emerged as a distinct research area
studying the features and implicit algorithms learned by foundation models such
as large language models. In this work, we aim to ground MI in the context of
cognitive science, which has long struggled with analogous questions in
studying and explaining the behavior of "black box" intelligent systems like
the human brain. We leverage several important ideas and developments in the
history of cognitive science to disentangle divergent objectives in MI and
indicate a clear path forward. First, we argue that current methods are ripe to
facilitate a transition in deep learning interpretation echoing the "cognitive
revolution" in 20th-century psychology that shifted the study of human
psychology from pure behaviorism toward mental representations and processing.
Second, we propose a taxonomy mirroring key parallels in computational
neuroscience to describe two broad categories of MI research, semantic
interpretation (what latent representations are learned and used) and
algorithmic interpretation (what operations are performed over representations)
to elucidate their divergent goals and objects of study. Finally, we elaborate
the parallels and distinctions between various approaches in both categories,
analyze the respective strengths and weaknesses of representative works,
clarify underlying assumptions, outline key challenges, and discuss the
possibility of unifying these modes of interpretation under a common framework.

ÊëòË¶ÅÔºö‰∫∫Â∑•Á•ûÁ∂ìÁ∂≤Ë∑ØÈï∑Êúü‰ª•‰æÜÈÉΩË¢´Ë¶ñÁÇ∫„ÄåÈªëÁõíÂ≠ê„ÄçÔºöÂÑòÁÆ°ÊàëÂÄëÁü•ÈÅìÂÆÉÂÄëÁöÑÈÅãÁÆóÂúñË°®ÂíåÂ≠∏ÁøíÂèÉÊï∏Ôºå‰ΩÜÈÄô‰∫õÊ¨äÈáçÂíåÂÆÉÂÄëÂü∑Ë°åÁöÑÂáΩÊï∏ÊâÄÁ∑®Á¢ºÁöÑÁü•Ë≠ò‰∏¶ÈùûÂ§©ÁîüÂ∞±ÂèØËß£Èáã„ÄÇÂõ†Ê≠§ÔºåÂæûÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊó©ÊúüÈñãÂßãÔºåÂ∞±ÊúâË®±Â§ö‰∫∫Ëá¥ÂäõÊñºËß£ÈáãÈÄô‰∫õÊ®°ÂûãÁöÑË°åÁÇ∫‰∏¶Âú®ÂÖßÈÉ®ÁêÜËß£ÂÆÉÂÄëÔºõÊúÄËøëÔºåÊ©üÂà∂ÂèØËß£ÈáãÊÄß (MI) Â∑≤ÊàêÁÇ∫‰∏ÄÂÄãÁç®ÁâπÁöÑÁöÑÁ†îÁ©∂È†òÂüüÔºåÊé¢Ë®éÂü∫Á§éÊ®°ÂûãÔºà‰æãÂ¶ÇÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºâÂ≠∏ÁøíÂà∞ÁöÑÁâπÂæµÂíåÈö±ÂºèÊºîÁÆóÊ≥ï„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊó®Âú®Â∞á MI Âü∫ÊñºË™çÁü•ÁßëÂ≠∏ÁöÑËÉåÊôØÔºåË™çÁü•ÁßëÂ≠∏Èï∑Êúü‰ª•‰æÜ‰∏ÄÁõ¥Âú®Á†îÁ©∂ÂíåËß£Èáã„ÄåÈªëÁõíÂ≠ê„ÄçÊô∫ËÉΩÁ≥ªÁµ±Ôºà‰æãÂ¶Ç‰∫∫ËÖ¶ÔºâÁöÑË°åÁÇ∫ÊôÇÔºåÂä™ÂäõËß£Ê±∫È°û‰ººÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÂà©Áî®Ë™çÁü•ÁßëÂ≠∏Âè≤‰∏äÂπæÂÄãÈáçË¶ÅÁöÑÊÉ≥Ê≥ïÂíåÁôºÂ±ïÔºå‰æÜËß£Èñã MI ‰∏≠‰∏çÂêåÁöÑÁõÆÊ®ôÔºå‰∏¶ÊåáÂá∫ÊòéÁ¢∫ÁöÑÂâçÈÄ≤ÈÅìË∑Ø„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëË™çÁÇ∫Áï∂ÂâçÁöÑÂêÑÁ®ÆÊñπÊ≥ïÂ∑≤Ê∫ñÂÇôÂ•Ω‰øÉÈÄ≤Ê∑±Â∫¶Â≠∏ÁøíËß£ÈáãÁöÑËΩâËÆäÔºåÈÄôÂëºÊáâ‰∫Ü 20 ‰∏ñÁ¥ÄÂøÉÁêÜÂ≠∏‰∏≠ÁöÑ„ÄåË™çÁü•Èù©ÂëΩ„ÄçÔºåÂ∞á‰∫∫È°ûÂøÉÁêÜÂ≠∏ÁöÑÁ†îÁ©∂ÂæûÁ¥îÁ≤πÁöÑË°åÁÇ∫‰∏ªÁæ©ËΩâÂêëÂøÉÊô∫Ë°®ÂæµÂíåËôïÁêÜ„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂàÜÈ°ûÊ≥ïÔºåÂèçÊò†Ë®àÁÆóÁ•ûÁ∂ìÁßëÂ≠∏‰∏≠ÁöÑÈóúÈçµÁõ∏‰ºº‰πãËôïÔºå‰ª•ÊèèËø∞ MI Á†îÁ©∂ÁöÑÂÖ©ÂÄãÂª£Ê≥õÈ°ûÂà•ÔºåË™ûÁæ©Ëß£ÈáãÔºàÂ≠∏ÁøíÂíå‰ΩøÁî®ÁöÑÊΩõÂú®Ë°®ÂæµÊòØ‰ªÄÈ∫ºÔºâÂíåÊºîÁÆóÊ≥ïËß£ÈáãÔºàÂú®Ë°®Âæµ‰∏äÂü∑Ë°åÁöÑÈÅãÁÆóÊòØ‰ªÄÈ∫ºÔºâÔºå‰ª•Èó°ÊòéÂÆÉÂÄë‰∏çÂêåÁöÑÁõÆÊ®ôÂíåÁ†îÁ©∂Â∞çË±°„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈó°Ëø∞‰∫ÜÈÄôÂÖ©ÂÄãÈ°ûÂà•‰∏≠ÂêÑÁ®ÆÊñπÊ≥ï‰πãÈñìÁöÑÁõ∏‰ºº‰πãËôïÂíåÂçÄÂà•ÔºåÂàÜÊûê‰ª£Ë°®ÊÄß‰ΩúÂìÅÁöÑÂÑ™Áº∫ÈªûÔºåÈáêÊ∏ÖÂü∫Êú¨ÂÅáË®≠ÔºåÊ¶ÇËø∞ÈóúÈçµÊåëÊà∞Ôºå‰∏¶Ë®éË´ñÂú®‰∏ÄÂÄãÂÖ±ÂêåÊû∂Êßã‰∏ãÁµ±‰∏ÄÈÄô‰∫õËß£ÈáãÊ®°ÂºèÁöÑÂèØËÉΩÊÄß„ÄÇ

##### **Investigating Instruction Tuning Large Language Models on Graphs**
2408.05457v1 by Kerui Zhu, Bo-Wei Huang, Bowen Jin, Yizhu Jiao, Ming Zhong, Kevin Chang, Shou-De Lin, Jiawei Han

Inspired by the recent advancements of Large Language Models (LLMs) in NLP
tasks, there's growing interest in applying LLMs to graph-related tasks. This
study delves into the capabilities of instruction-following LLMs for engaging
with real-world graphs, aiming to offer empirical insights into how LLMs can
effectively interact with graphs and generalize across graph tasks. We begin by
constructing a dataset designed for instruction tuning, which comprises a
diverse collection of 79 graph-related tasks from academic and e-commerce
domains, featuring 44,240 training instances and 18,960 test samples. Utilizing
this benchmark, our initial investigation focuses on identifying the optimal
graph representation that serves as a conduit for LLMs to understand complex
graph structures. Our findings indicate that JSON format for graph
representation consistently outperforms natural language and code formats
across various LLMs and graph types. Furthermore, we examine the key factors
that influence the generalization abilities of instruction-tuned LLMs by
evaluating their performance on both in-domain and out-of-domain graph tasks.

ÊëòË¶ÅÔºöÂèóÂà∞Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰∏≠Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËøëÊúüÈÄ≤Â±ïÁöÑÂïüÁôºÔºåÂ∞á LLM ÊáâÁî®ÊñºËàáÂúñË°®Áõ∏Èóú‰ªªÂãôÁöÑËààË∂£Êó•ÁõäÊøÉÂéö„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÈÅµÂæ™Êåá‰ª§ÁöÑ LLM ÁöÑÂäüËÉΩÔºå‰ª•Âæû‰∫ãÁúüÂØ¶‰∏ñÁïåÁöÑÂúñË°®ÔºåÊó®Âú®Êèê‰æõ LLM Â¶Ç‰ΩïÊúâÊïàÂú∞ËàáÂúñË°®‰∫íÂãï‰∏¶Âú®ÂúñË°®‰ªªÂãô‰∏≠ÈÄ≤Ë°åÊ¶ÇÊã¨ÁöÑÁ∂ìÈ©óË¶ãËß£„ÄÇÊàëÂÄëÂæûÊßãÂª∫‰∏ÄÂÄãÂ∞àÁÇ∫Êåá‰ª§Ë™øÊï¥ËÄåË®≠Ë®àÁöÑË≥áÊñôÈõÜÈñãÂßãÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™Â≠∏Ë°ìÂíåÈõªÂ≠êÂïÜÂãôÈ†òÂüüÁöÑ 79 ÂÄãÂúñË°®Áõ∏Èóú‰ªªÂãôÁöÑÂ§öÂÖÉÂåñÈõÜÂêàÔºåÂåÖÂê´ 44,240 ÂÄãË®ìÁ∑¥ÂØ¶‰æãÂíå 18,960 ÂÄãÊ∏¨Ë©¶Ê®£Êú¨„ÄÇÂà©Áî®Ê≠§Âü∫Ê∫ñÔºåÊàëÂÄëÁöÑÂàùÊ≠•Ë™øÊü•ÈáçÈªûÂú®ÊñºË≠òÂà•ÊúÄ‰Ω≥ÂúñË°®Ë°®Á§∫Ôºå‰ΩúÁÇ∫ LLM ÁêÜËß£Ë§áÈõúÂúñË°®ÁµêÊßãÁöÑÁÆ°ÈÅì„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåJSON Ê†ºÂºèÁöÑÂúñË°®Ë°®Á§∫Âú®ÂêÑÁ®Æ LLM ÂíåÂúñË°®È°ûÂûã‰∏≠ÂßãÁµÇÂÑ™ÊñºËá™ÁÑ∂Ë™ûË®ÄÂíåÁ®ãÂºèÁ¢ºÊ†ºÂºè„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂΩ±ÈüøÊåá‰ª§Ë™øÊï¥ LLM Ê¶ÇÊã¨ËÉΩÂäõÁöÑ‰∏ªË¶ÅÂõ†Á¥†ÔºåÊñπÊ≥ïÊòØË©ï‰º∞ÂÆÉÂÄëÂú®È†òÂüüÂÖßÂíåÈ†òÂüüÂ§ñÂúñË°®‰ªªÂãô‰∏äÁöÑË°®Áèæ„ÄÇ

##### **Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation**
2408.05456v1 by Wenbo Shang, Xuliang Zhu, Xin Huang

Unified graph representation learning aims to produce node embeddings, which
can be applied to multiple downstream applications. However, existing studies
based on graph neural networks and language models either suffer from the
limitations of numerous training needed toward specific downstream predictions
or have shallow semantic features. In this work, we propose a novel Path-LLM
model to learn unified graph representation, which leverages a powerful large
language model (LLM) to incorporate our proposed path features. Our Path-LLM
framework consists of several well-designed techniques. First, we develop a new
mechanism of long-to-short shortest path (L2SP) selection, which covers
essential connections between different dense groups. An in-depth comparison of
different path selection plans is offered to illustrate the strength of our
designed L2SP. Then, we design path textualization to obtain L2SP-based
training texts. Next, we feed the texts into a self-supervised LLM training
process to learn embeddings. Extensive experiments on benchmarks validate the
superiority of Path-LLM against the state-of-the-art WalkLM method on two
classical graph learning tasks (node classification and link prediction) and
one NP-hard graph query processing task (keyword search), meanwhile saving more
than 90% of training paths.

ÊëòË¶ÅÔºöÁµ±‰∏ÄÂúñÂΩ¢Ë°®ÂæµÂ≠∏ÁøíÊó®Âú®Áî¢ÁîüÁØÄÈªûÂµåÂÖ•ÔºåÂèØÁî®ÊñºÂ§öÂÄã‰∏ãÊ∏∏ÊáâÁî®Á®ãÂºè„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂü∫ÊñºÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂíåË™ûË®ÄÊ®°ÂûãÁöÑÁ†îÁ©∂Ôºå‰∏çÊòØÂõ†ÈúÄË¶ÅÈáùÂ∞çÁâπÂÆö‰∏ãÊ∏∏È†êÊ∏¨ËÄåÈÄ≤Ë°åÂ§ßÈáèË®ìÁ∑¥ËÄåÂèóÂà∞ÈôêÂà∂ÔºåÂ∞±ÊòØË™ûÊÑèÁâπÂæµÊ∑∫ËñÑ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑ Path-LLM Ê®°Âûã‰æÜÂ≠∏ÁøíÁµ±‰∏ÄÁöÑÂúñÂΩ¢Ë°®ÂæµÔºåÂÆÉÂà©Áî®Âº∑Â§ßÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁ¥çÂÖ•ÊàëÂÄëÊèêÂá∫ÁöÑË∑ØÂæëÁâπÂæµ„ÄÇÊàëÂÄëÁöÑ Path-LLM Ê°ÜÊû∂ÂåÖÂê´‰∫ÜÂ§öÈ†ÖË®≠Ë®àËâØÂ•ΩÁöÑÊäÄË°ì„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÈï∑Âà∞Áü≠ÊúÄÁü≠Ë∑ØÂæë (L2SP) ÈÅ∏ÊìáÊ©üÂà∂ÔºåÂÆÉÊ∂µËìã‰∫Ü‰∏çÂêåÂØÜÈõÜÁæ§ÁµÑ‰πãÈñìÁöÑÂøÖË¶ÅÈÄ£Êé•„ÄÇÊèê‰æõ‰∫Ü‰∏çÂêåË∑ØÂæëÈÅ∏ÊìáÊñπÊ°àÁöÑÊ∑±ÂÖ•ÊØîËºÉÔºå‰ª•Ë™™ÊòéÊàëÂÄëË®≠Ë®àÁöÑ L2SP ÁöÑÂÑ™Âã¢„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË®≠Ë®àË∑ØÂæëÊñáÂ≠óÂåñ‰ª•Áç≤ÂæóÂü∫Êñº L2SP ÁöÑË®ìÁ∑¥ÊñáÊú¨„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÂ∞áÊñáÊú¨Ëº∏ÂÖ•Âà∞Ëá™Áõ£Áù£ LLM Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠‰ª•Â≠∏ÁøíÂµåÂÖ•„ÄÇÂú®Âü∫Ê∫ñ‰∏äÁöÑÂ§ßÈáèÂØ¶È©óÈ©óË≠â‰∫Ü Path-LLM Âú®ÂÖ©ÂÄãÁ∂ìÂÖ∏ÂúñÂΩ¢Â≠∏Áøí‰ªªÂãôÔºàÁØÄÈªûÂàÜÈ°ûÂíåÈÄ£ÁµêÈ†êÊ∏¨ÔºâÂíå‰∏ÄÂÄã NP Èõ£ÂúñÂΩ¢Êü•Ë©¢ËôïÁêÜ‰ªªÂãôÔºàÈóúÈçµÂ≠óÊêúÂ∞ãÔºâ‰∏äÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑ WalkLM ÊñπÊ≥ïÔºåÂêåÊôÇÁØÄÁúÅ‰∫ÜË∂ÖÈÅé 90% ÁöÑË®ìÁ∑¥Ë∑ØÂæë„ÄÇ

##### **LaiDA: Linguistics-aware In-context Learning with Data Augmentation for Metaphor Components Identification**
2408.05404v1 by Hongde Liu, Chenyuan He, Feiyang Meng, Changyong Niu, Yuxiang Jia

Metaphor Components Identification (MCI) contributes to enhancing machine
understanding of metaphors, thereby advancing downstream natural language
processing tasks. However, the complexity, diversity, and dependency on context
and background knowledge pose significant challenges for MCI. Large language
models (LLMs) offer new avenues for accurate comprehension of complex natural
language texts due to their strong semantic analysis and extensive commonsense
knowledge. In this research, a new LLM-based framework is proposed, named
Linguistics-aware In-context Learning with Data Augmentation (LaiDA).
Specifically, ChatGPT and supervised fine-tuning are utilized to tailor a
high-quality dataset. LaiDA incorporates a simile dataset for pre-training. A
graph attention network encoder generates linguistically rich feature
representations to retrieve similar examples. Subsequently, LLM is fine-tuned
with prompts that integrate linguistically similar examples. LaiDA ranked 2nd
in Subtask 2 of NLPCC2024 Shared Task 9, demonstrating its effectiveness. Code
and data are available at https://github.com/WXLJZ/LaiDA.

ÊëòË¶ÅÔºöÈö±ÂñªÁµÑÊàêËæ®Ë≠ò (MCI) ÊúâÂä©ÊñºÊèêÂçáÊ©üÂô®Â∞çÈö±ÂñªÁöÑÁêÜËß£ÔºåÈÄ≤ËÄåÊé®Âãï‰∏ãÊ∏∏ÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô„ÄÇ‰∏çÈÅéÔºåË§áÈõúÊÄß„ÄÅÂ§öÊ®£ÊÄßÔºå‰ª•ÂèäÂ∞çËÑàÁµ°ÂíåËÉåÊôØÁü•Ë≠òÁöÑ‰æùË≥¥ÊÄßÔºåÂ∞ç MCI ËÄåË®ÄÊòØÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî±ÊñºÂÖ∂Âº∑Â§ßÁöÑË™ûÊÑèÂàÜÊûêÂíåÂª£Ê≥õÁöÑÂ∏∏Ë≠òÁü•Ë≠òÔºåÁÇ∫Ê∫ñÁ¢∫ÁêÜËß£Ë§áÈõúÁöÑËá™ÁÑ∂Ë™ûË®ÄÊñáÊú¨Êèê‰æõ‰∫ÜÊñ∞ÈÄîÂæë„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Êñº LLM ÁöÑÊû∂ÊßãÔºåÁ®±ÁÇ∫ÂÖ∑ÂÇôË≥áÊñôÊì¥ÂÖÖÂäüËÉΩÁöÑË™ûË®ÄÊÑüÁü•ÊÉÖÂ¢ÉÂ≠∏Áøí (LaiDA)„ÄÇÂÖ∑È´î‰æÜË™™ÔºåChatGPT ÂíåÁõ£Áù£ÂæÆË™øÁî®ÊñºË™øÊï¥‰∏ÄÂÄãÈ´òÂìÅË≥™ÁöÑË≥áÊñôÈõÜ„ÄÇLaiDA ÁµêÂêà‰∫Ü‰∏ÄÂÄãÊØîÂñªË≥áÊñôÈõÜÈÄ≤Ë°åÈ†êË®ìÁ∑¥„ÄÇ‰∏ÄÂÄãÂúñÂΩ¢Ê≥®ÊÑèÂäõÁ∂≤Ë∑ØÁ∑®Á¢ºÂô®Áî¢ÁîüË™ûË®ÄË±êÂØåÁöÑÁâπÂæµË°®Á§∫Ôºå‰ª•Êì∑ÂèñÈ°û‰ººÁöÑÁØÑ‰æã„ÄÇÈö®ÂæåÔºåLLM ‰ΩøÁî®Êï¥Âêà‰∫ÜË™ûË®ÄÁõ∏‰ººÁØÑ‰æãÁöÑÊèêÁ§∫ÈÄ≤Ë°åÂæÆË™ø„ÄÇLaiDA Âú® NLPCC2024 ÂÖ±‰∫´‰ªªÂãô 9 ÁöÑÂ≠ê‰ªªÂãô 2 ‰∏≠ÊéíÂêçÁ¨¨ 2ÔºåË≠âÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú® https://github.com/WXLJZ/LaiDA ÂèñÂæó„ÄÇ

##### **SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV Battery Supply Chain Disruptions**
2408.05357v1 by Zhi-Qi Cheng, Yifei Dong, Aike Shi, Wei Liu, Yuzhi Hu, Jason O'Connor, Alexander Hauptmann, Kate Whitefoot

The electric vehicle (EV) battery supply chain's vulnerability to disruptions
necessitates advanced predictive analytics. We present SHIELD (Schema-based
Hierarchical Induction for EV supply chain Disruption), a system integrating
Large Language Models (LLMs) with domain expertise for EV battery supply chain
risk assessment. SHIELD combines: (1) LLM-driven schema learning to construct a
comprehensive knowledge library, (2) a disruption analysis system utilizing
fine-tuned language models for event extraction, multi-dimensional similarity
matching for schema matching, and Graph Convolutional Networks (GCNs) with
logical constraints for prediction, and (3) an interactive interface for
visualizing results and incorporating expert feedback to enhance
decision-making. Evaluated on 12,070 paragraphs from 365 sources (2022-2023),
SHIELD outperforms baseline GCNs and LLM+prompt methods (e.g., GPT-4o) in
disruption prediction. These results demonstrate SHIELD's effectiveness in
combining LLM capabilities with domain expertise for enhanced supply chain risk
assessment.

ÊëòË¶ÅÔºöÈõªÂãïËªä (EV) ÈõªÊ±†‰æõÊáâÈèàÂÆπÊòìÂèóÂà∞Âπ≤ÊìæÔºåÂõ†Ê≠§ÈúÄË¶ÅÈÄ≤ÈöéÁöÑÈ†êÊ∏¨ÂàÜÊûê„ÄÇÊàëÂÄëÊèêÂá∫ SHIELDÔºàÂü∫ÊñºÊû∂ÊßãÁöÑ EV ‰æõÊáâÈèà‰∏≠Êñ∑ÈöéÂ±§ÂºèÊ≠∏Á¥çÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊï¥ÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ëàá EV ÈõªÊ±†‰æõÊáâÈèàÈ¢®Èö™Ë©ï‰º∞È†òÂüüÂ∞àÊ•≠Áü•Ë≠òÁöÑÁ≥ªÁµ±„ÄÇSHIELD ÁµêÂêàÔºö(1) LLM È©ÖÂãïÁöÑÊû∂ÊßãÂ≠∏ÁøíÔºåÁî®ÊñºÂª∫ÁΩÆ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÁü•Ë≠òÂ∫´Ôºå(2) ‰∏ÄÂÄã‰∏≠Êñ∑ÂàÜÊûêÁ≥ªÁµ±ÔºåÂà©Áî®ÂæÆË™øË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°å‰∫ã‰ª∂ËêÉÂèñ„ÄÅÂ§öÁ∂≠Â∫¶Áõ∏‰ººÊÄßÊØîÂ∞çÁî®ÊñºÊû∂ÊßãÊØîÂ∞çÔºå‰ª•ÂèäÂ∏∂ÊúâÈÇèËºØÁ¥ÑÊùüÁöÑÂúñÂΩ¢Âç∑Á©çÁ∂≤Ë∑Ø (GCN) Áî®ÊñºÈ†êÊ∏¨Ôºå‰ª•Âèä (3) ‰∏ÄÂÄã‰∫íÂãï‰ªãÈù¢ÔºåÁî®ÊñºË¶ñË¶∫ÂåñÁµêÊûúÂíåÁ¥çÂÖ•Â∞àÂÆ∂ÂõûÈ•ã‰ª•Â¢ûÂº∑Ê±∫Á≠ñÂà∂ÂÆö„ÄÇÂú®‰æÜËá™ 365 ÂÄã‰æÜÊ∫êÁöÑ 12,070 ÊÆµËêΩÔºà2022-2023 Âπ¥Ôºâ‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåSHIELD Âú®‰∏≠Êñ∑È†êÊ∏¨ÊñπÈù¢ÂÑ™ÊñºÂü∫Ê∫ñ GCN Âíå LLM+ÊèêÁ§∫ÊñπÊ≥ïÔºà‰æãÂ¶ÇÔºåGPT-4oÔºâ„ÄÇÈÄô‰∫õÁµêÊûúË≠âÊòé‰∫Ü SHIELD Âú®ÁµêÂêà LLM ÂäüËÉΩËàáÈ†òÂüüÂ∞àÊ•≠Áü•Ë≠ò‰ª•Â¢ûÂº∑‰æõÊáâÈèàÈ¢®Èö™Ë©ï‰º∞ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning**
2408.05141v1 by Ye Yuan, Chengwu Liu, Jingyang Yuan, Gongbo Sun, Siqi Li, Ming Zhang

Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊòØ‰∏ÄÂÄãÊû∂ÊßãÔºå‰ΩøÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËÉΩÂ§†ÈÄèÈÅéÊï¥ÂêàÂ§ñÈÉ®Áü•Ë≠òÂ∫´‰æÜÂ¢ûÂº∑ÂÖ∂Ê∫ñÁ¢∫ÊÄß‰∏¶Ê∏õÂ∞ëÂπªË¶∫„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊ∑∑Âêà RAG Á≥ªÁµ±ÔºåÈÄèÈÅéÂÖ®Èù¢ÁöÑÊúÄ‰Ω≥ÂåñÂ•ó‰ª∂ÈÄ≤Ë°åÂ¢ûÂº∑ÔºåÂèØÈ°ØËëóÊèêÂçáÊ™¢Á¥¢ÂìÅË≥™„ÄÅÂ¢ûÂº∑Êé®ÁêÜËÉΩÂäõÔºå‰∏¶ÊîπÂñÑÊï∏ÂÄºË®àÁÆóËÉΩÂäõ„ÄÇÊàëÂÄëÊîπÈÄ≤‰∫ÜÁ∂≤È†Å‰∏≠ÁöÑÊñáÂ≠óÂçÄÂ°äÂíåË°®Ê†ºÔºåÂä†ÂÖ•Â±¨ÊÄßÈ†êÊ∏¨Âô®‰ª•Ê∏õÂ∞ëÂπªË¶∫ÔºåÂü∑Ë°å LLM Áü•Ë≠òËêÉÂèñÂô®ÂíåÁü•Ë≠òÂúñË°®ËêÉÂèñÂô®ÔºåÊúÄÂæåÂª∫Êßã‰∫Ü‰∏ÄÂÄãÂåÖÂê´ÊâÄÊúâÂèÉËÄÉÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄÇÊàëÂÄëÈÄèÈÅé Meta CRAG KDD Cup 2024 Á´∂Ë≥ΩÂú® CRAG Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÁ≥ªÁµ±„ÄÇÂú®Âú∞Á´ØÂíåÁ∑ö‰∏äË©ï‰º∞ÈÉΩË≠âÊòéÊàëÂÄëÁöÑÁ≥ªÁµ±È°ØËëóÂ¢ûÂº∑‰∫ÜË§áÈõúÊé®ÁêÜËÉΩÂäõ„ÄÇÂú®Âú∞Á´ØË©ï‰º∞‰∏≠ÔºåÊàëÂÄëËàáÂü∫Ê∫ñÊ®°ÂûãÁõ∏ÊØîÔºåÈ°ØËëóÊèêÂçá‰∫ÜÊ∫ñÁ¢∫ÊÄß‰∏¶Èôç‰Ωé‰∫ÜÈåØË™§ÁéáÔºåÈÅîÂà∞‰∫ÜÈ°ØËëóÁöÑÂàÜÊï∏ÊèêÂçá„ÄÇÂêåÊôÇÔºåÊàëÂÄëÂú®Á∑ö‰∏äË©ï‰º∞‰∏≠ÂèñÂæó‰∫ÜÂÇëÂá∫ÁöÑÊàêÊûúÔºåË≠âÊòé‰∫ÜÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±ÁöÑÊïàËÉΩÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÊàëÂÄëÁ≥ªÁµ±ÁöÑÂéüÂßãÁ¢ºÂ∑≤ÁôºÂ∏ÉÂú® \url{https://gitlab.aicrowd.com/shizueyy/crag-new}„ÄÇ

##### **HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction**
2408.04948v1 by Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, Dhagash Mehta

Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain

ÊëòË¶ÅÔºöÂæûÈùûÁµêÊßãÂåñÊñáÊú¨Ë≥áÊñô‰∏≠Êì∑ÂèñÂíåË©ÆÈáãË§áÈõúË≥áË®äÔºå‰æãÂ¶ÇË≤°ÂãôÊáâÁî®‰∏≠Áî¢ÁîüÁöÑÊî∂ÁõäÈõªË©±ÊúÉË≠∞Ë®òÈåÑÔºåÂç≥‰Ωø‰ΩøÁî®Áï∂Ââç‰ΩøÁî®Ê™¢Á¥¢Êì¥ÂÖÖÁîüÊàê (RAG) ÁöÑÊúÄ‰Ω≥ÂØ¶ÂãôÔºàÁ®±ÁÇ∫ VectorRAG ÊäÄË°ìÔºåÂÆÉ‰ΩøÁî®ÂêëÈáèË≥áÊñôÂ∫´‰æÜÈÄ≤Ë°åË≥áË®äÊ™¢Á¥¢ÔºâÔºåÁî±ÊñºÈ†òÂüüÁâπÂÆöË°ìË™ûÂíåÊñá‰ª∂Ê†ºÂºèË§áÈõúÁ≠âÊåëÊà∞ÔºåÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËÄåË®Ä‰ªçÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÁ®±ÁÇ∫ HybridRAG ÁöÑÁµÑÂêàÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÖ∂ÁµêÂêà‰∫ÜÂü∫ÊñºÁü•Ë≠òÂúñË≠ú (KG) ÁöÑ RAG ÊäÄË°ìÔºàÁ®±ÁÇ∫ GraphRAGÔºâÂíå VectorRAG ÊäÄË°ìÔºå‰ª•Â¢ûÂº∑Ë≤°ÂãôÊñá‰ª∂Ë≥áË®äÊì∑ÂèñÁöÑÂïèÁ≠î (Q&A) Á≥ªÁµ±ÔºåË≠âÊòéÂÆÉËÉΩÂ§†Áî¢ÁîüÊ∫ñÁ¢∫‰∏îËàáËÑàÁµ°Áõ∏ÈóúÁöÑÁ≠îÊ°à„ÄÇ‰ΩøÁî®‰∏ÄÁµÑ‰ª•ÂïèÁ≠îÊ†ºÂºèÂëàÁèæÁöÑË≤°ÂãôÊî∂ÁõäÈõªË©±ÊúÉË≠∞Ë®òÈåÑÊñá‰ª∂ÈÄ≤Ë°åÂØ¶È©óÔºåÂõ†Ê≠§Êèê‰æõ‰∫ÜËá™ÁÑ∂ÁöÑ‰∏ÄÁµÑÁúüÂØ¶ÂïèÁ≠îÂ∞çÔºåÊàëÂÄëË°®Êòé HybridRAG ÂæûÂêëÈáèË≥áÊñôÂ∫´Âíå KG ‰∏≠Êì∑ÂèñËÑàÁµ°ÔºåÂú®Ê™¢Á¥¢ÂíåÁîüÊàêÈöéÊÆµÁöÑÊ™¢Á¥¢Ê∫ñÁ¢∫Â∫¶ÂíåÁ≠îÊ°àÁîüÊàêÊñπÈù¢ÔºåÈÉΩÂÑ™ÊñºÂÇ≥Áµ±ÁöÑ VectorRAG Âíå GraphRAG„ÄÇÊâÄÊèêÂá∫ÁöÑÊäÄË°ìÂÖ∑ÊúâË∂ÖÂá∫Ë≤°ÂãôÈ†òÂüüÁöÑÊáâÁî®

##### **DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization**
2408.04400v1 by Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, Liang Wang

This paper addresses the challenge of out-of-distribution (OOD)
generalization in graph machine learning, a field rapidly advancing yet
grappling with the discrepancy between source and target data distributions.
Traditional graph learning algorithms, based on the assumption of uniform
distribution between training and test data, falter in real-world scenarios
where this assumption fails, resulting in suboptimal performance. A principal
factor contributing to this suboptimal performance is the inherent simplicity
bias of neural networks trained through Stochastic Gradient Descent (SGD),
which prefer simpler features over more complex yet equally or more predictive
ones. This bias leads to a reliance on spurious correlations, adversely
affecting OOD performance in various tasks such as image recognition, natural
language understanding, and graph classification. Current methodologies,
including subgraph-mixup and information bottleneck approaches, have achieved
partial success but struggle to overcome simplicity bias, often reinforcing
spurious correlations. To tackle this, we propose DIVE, training a collection
of models to focus on all label-predictive subgraphs by encouraging the models
to foster divergence on the subgraph mask, which circumvents the limitation of
a model solely focusing on the subgraph corresponding to simple structural
patterns. Specifically, we employs a regularizer to punish overlap in extracted
subgraphs across models, thereby encouraging different models to concentrate on
distinct structural patterns. Model selection for robust OOD performance is
achieved through validation accuracy. Tested across four datasets from GOOD
benchmark and one dataset from DrugOOD benchmark, our approach demonstrates
significant improvement over existing methods, effectively addressing the
simplicity bias and enhancing generalization in graph machine learning.

ÊëòË¶ÅÔºö<paragraph>ÈÄôÁØáË´ñÊñáÊé¢Ë®é‰∫ÜÂúñÂΩ¢Ê©üÂô®Â≠∏Áøí‰∏≠ÈùûÂàÜ‰Ωà (OOD) Ê¶ÇÂåñÁöÑÊåëÊà∞ÔºåÈÄôÊòØ‰∏ÄÂÄãÂø´ÈÄüÁôºÂ±ïÁöÑÈ†òÂüüÔºå‰ΩÜÂçªÂú®ÊáâÂ∞ç‰æÜÊ∫êÂíåÁõÆÊ®ôË≥áÊñôÂàÜ‰Ωà‰πãÈñìÁöÑÂ∑ÆÁï∞‰∏äÈÅáÂà∞Âõ∞Èõ£„ÄÇÂÇ≥Áµ±ÁöÑÂúñÂΩ¢Â≠∏ÁøíÊºîÁÆóÊ≥ïÂü∫ÊñºË®ìÁ∑¥Ë≥áÊñôÂíåÊ∏¨Ë©¶Ë≥áÊñô‰πãÈñìÂùáÂãªÂàÜ‰ΩàÁöÑÂÅáË®≠Ôºå‰ΩÜÂú®ÈÄôÂÄãÂÅáË®≠Â§±ÊïàÁöÑÂØ¶ÈöõÊÉÖÊ≥Å‰∏≠ÊúÉÂá∫ÁèæÂïèÈ°åÔºåÂ∞éËá¥Ê¨°‰Ω≥ÊïàËÉΩ„ÄÇÈÄ†ÊàêÈÄôÁ®ÆÊ¨°‰Ω≥ÊïàËÉΩÁöÑ‰∏ªË¶ÅÂõ†Á¥†ÊòØÈÄèÈÅéÈö®Ê©üÊ¢ØÂ∫¶‰∏ãÈôç (SGD) Ë®ìÁ∑¥ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÂõ∫ÊúâÁöÑÁ∞°ÂåñÂÅèÂ∑ÆÔºåÂÆÉÂÅèÂ•ΩËºÉÁ∞°ÂñÆÁöÑÁâπÂæµÔºåËÄåÈùûÊõ¥Ë§áÈõú‰ΩÜÈ†êÊ∏¨ËÉΩÂäõÁõ∏ÂêåÊàñÊõ¥È´òÁöÑÁâπÂæµ„ÄÇÈÄôÁ®ÆÂÅèÂ∑ÆÊúÉÂ∞éËá¥‰æùË≥¥ËôõÂÅáÁõ∏ÈóúÊÄßÔºåÂ∞çÂêÑÁ®Æ‰ªªÂãôÔºà‰æãÂ¶ÇÂΩ±ÂÉèËæ®Ë≠ò„ÄÅËá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ÂíåÂúñÂΩ¢ÂàÜÈ°ûÔºâÁöÑ OOD ÊïàËÉΩÁî¢ÁîüË≤†Èù¢ÂΩ±Èüø„ÄÇÁõÆÂâçÁöÑÊäÄË°ìÊñπÊ≥ïÔºåÂåÖÊã¨Â≠êÂúñÊ∑∑ÂêàÂíåË≥áË®äÁì∂È†∏ÊñπÊ≥ïÔºåÂ∑≤ÂèñÂæóÈÉ®ÂàÜÊàêÂäüÔºå‰ΩÜ‰ªçÈõ£‰ª•ÂÖãÊúçÁ∞°ÂåñÂÅèÂ∑ÆÔºåËÄå‰∏îÂ∏∏Â∏∏ÊúÉÂº∑ÂåñËôõÂÅáÁõ∏ÈóúÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü DIVEÔºåË®ìÁ∑¥‰∏ÄÁµÑÊ®°Âûã‰ª•ÈóúÊ≥®ÊâÄÊúâÊ®ôÁ±§È†êÊ∏¨Â≠êÂúñÔºåÊñπÊ≥ïÊòØÈºìÂãµÊ®°ÂûãÂú®Â≠êÂúñÈÅÆÁΩ©‰∏ä‰øÉÈÄ≤Â∑ÆÁï∞ÔºåÈÄôÈÅøÈñã‰∫ÜÊ®°ÂûãÂÉÖÈóúÊ≥®Â∞çÊáâÊñºÁ∞°ÂñÆÁµêÊßãÊ®°ÂºèÁöÑÂ≠êÂúñÁöÑÈôêÂà∂„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé°Áî®‰∏ÄÂÄãÊ≠£Ë¶èÂåñÂô®‰æÜÊá≤ÁΩ∞Ê®°Âûã‰πãÈñìÊèêÂèñÁöÑÂ≠êÂúñ‰∏≠ÁöÑÈáçÁñäÔºåÂæûËÄåÈºìÂãµ‰∏çÂêåÁöÑÊ®°ÂûãÂ∞àÊ≥®Êñº‰∏çÂêåÁöÑÁµêÊßãÊ®°Âºè„ÄÇÈÄèÈÅéÈ©óË≠âÊ∫ñÁ¢∫Â∫¶ÔºåÂèØ‰ª•ÈÅ∏ÊìáÊ®°Âûã‰ª•Áç≤ÂæóÁ©©ÂÅ•ÁöÑ OOD ÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® GOOD Âü∫Ê∫ñ‰∏≠ÁöÑÂõõÂÄãË≥áÊñôÈõÜÂíå DrugOOD Âü∫Ê∫ñ‰∏≠ÁöÑÂÖ∂‰∏≠‰∏ÄÂÄãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°å‰∫ÜÊ∏¨Ë©¶ÔºåÁµêÊûúÈ°ØÁ§∫Âá∫ÊØîÁèæÊúâÊñπÊ≥ïÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•ÔºåÊúâÊïàÂú∞Ëß£Ê±∫‰∫ÜÁ∞°ÂåñÂÅèÂ∑ÆÔºå‰∏¶Â¢ûÂº∑‰∫ÜÂúñÂΩ¢Ê©üÂô®Â≠∏Áøí‰∏≠ÁöÑÊ¶ÇÂåñËÉΩÂäõ„ÄÇ</paragraph>

##### **MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models**
2408.04388v1 by Haoxuan Li, Zhengmao Yang, Yunshan Ma, Yi Bin, Yang Yang, Tat-Seng Chua

We study an emerging and intriguing problem of multimodal temporal event
forecasting with large language models. Compared to using text or graph
modalities, the investigation of utilizing images for temporal event
forecasting has not been fully explored, especially in the era of large
language models (LLMs). To bridge this gap, we are particularly interested in
two key questions of: 1) why images will help in temporal event forecasting,
and 2) how to integrate images into the LLM-based forecasting framework. To
answer these research questions, we propose to identify two essential functions
that images play in the scenario of temporal event forecasting, i.e.,
highlighting and complementary. Then, we develop a novel framework, named
MM-Forecast. It employs an Image Function Identification module to recognize
these functions as verbal descriptions using multimodal large language models
(MLLMs), and subsequently incorporates these function descriptions into
LLM-based forecasting models. To evaluate our approach, we construct a new
multimodal dataset, MidEast-TE-mm, by extending an existing event dataset
MidEast-TE-mini with images. Empirical studies demonstrate that our MM-Forecast
can correctly identify the image functions, and further more, incorporating
these verbal function descriptions significantly improves the forecasting
performance. The dataset, code, and prompts are available at
https://github.com/LuminosityX/MM-Forecast.

ÊëòË¶ÅÔºöÊàëÂÄëÁ†îÁ©∂Â§öÊ®°ÊÖãÊôÇÈñì‰∫ã‰ª∂È†êÊ∏¨‰∏≠‰∏ÄÂÄãÊñ∞Ëàà‰∏îÊúâË∂£ÁöÑË™ûË®ÄÊ®°ÂûãÂïèÈ°å„ÄÇÁõ∏ËºÉÊñº‰ΩøÁî®ÊñáÂ≠óÊàñÂúñË°®Ê®°ÊÖãÔºåÂà©Áî®ÂΩ±ÂÉèÈÄ≤Ë°åÊôÇÈñì‰∫ã‰ª∂È†êÊ∏¨ÁöÑÁ†îÁ©∂Â∞öÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢ÔºåÁâπÂà•ÊòØÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊôÇ‰ª£„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩÔºåÊàëÂÄëÁâπÂà•ÊÑüËààË∂£ÁöÑÂÖ©ÂÄãÈóúÈçµÂïèÈ°åÊòØÔºö1) ÁÇ∫‰ªÄÈ∫ºÂΩ±ÂÉèÊúâÂä©ÊñºÊôÇÈñì‰∫ã‰ª∂È†êÊ∏¨Ôºå‰ª•Âèä 2) Â¶Ç‰ΩïÂ∞áÂΩ±ÂÉèÊï¥ÂêàÂà∞Âü∫Êñº LLM ÁöÑÈ†êÊ∏¨Ê°ÜÊû∂‰∏≠„ÄÇÁÇ∫‰∫ÜÂõûÁ≠îÈÄô‰∫õÁ†îÁ©∂ÂïèÈ°åÔºåÊàëÂÄëÊèêË≠∞ÊâæÂá∫ÂΩ±ÂÉèÂú®ÊôÇÈñì‰∫ã‰ª∂È†êÊ∏¨Â†¥ÊôØ‰∏≠ÊâÆÊºîÁöÑÂÖ©ÂÄãÂü∫Êú¨ÂäüËÉΩÔºåÂç≥Á™ÅÈ°ØÂíåË£úÂÖÖ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÈñãÁôº‰∏ÄÂÄãÂêçÁÇ∫ MM-Forecast ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇÂÆÉ‰ΩøÁî®ÂΩ±ÂÉèÂäüËÉΩË≠òÂà•Ê®°ÁµÑÔºå‰ΩøÁî®Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) Â∞áÈÄô‰∫õÂäüËÉΩË≠òÂà•ÁÇ∫ÊñáÂ≠óÊèèËø∞Ôºå‰∏¶Èö®ÂæåÂ∞áÈÄô‰∫õÂäüËÉΩÊèèËø∞Á¥çÂÖ•Âü∫Êñº LLM ÁöÑÈ†êÊ∏¨Ê®°Âûã‰∏≠„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÁöÑÊñπÊ≥ïÔºåÊàëÂÄëÈÄöÈÅé‰ΩøÁî®ÂΩ±ÂÉèÊì¥ÂÖÖÁèæÊúâÁöÑ‰∫ã‰ª∂Ë≥áÊñôÈõÜ MidEast-TE-miniÔºåÂª∫Êßã‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂ§öÊ®°ÊÖãË≥áÊñôÈõÜ MidEast-TE-mm„ÄÇÂØ¶Ë≠âÁ†îÁ©∂Ë°®ÊòéÔºåÊàëÂÄëÁöÑ MM-Forecast ÂèØ‰ª•Ê≠£Á¢∫Ë≠òÂà•ÂΩ±ÂÉèÂäüËÉΩÔºåÊ≠§Â§ñÔºåÁ¥çÂÖ•ÈÄô‰∫õÊñáÂ≠óÂäüËÉΩÊèèËø∞ÂèØ‰ª•È°ØËëóÊîπÂñÑÈ†êÊ∏¨ÊïàËÉΩ„ÄÇË≥áÊñôÈõÜ„ÄÅÁ®ãÂºèÁ¢ºÂíåÊèêÁ§∫ÂèØÂú® https://github.com/LuminosityX/MM-Forecast ÂèñÂæó„ÄÇ

##### **Judgment2vec: Apply Graph Analytics to Searching and Recommendation of Similar Judgments**
2408.04382v1 by Hsuan-Lei Shao

In court practice, legal professionals rely on their training to provide
opinions that resolve cases, one of the most crucial aspects being the ability
to identify similar judgments from previous courts efficiently. However,
finding a similar case is challenging and often depends on experience, legal
domain knowledge, and extensive labor hours, making veteran lawyers or judges
indispensable. This research aims to automate the analysis of judgment text
similarity. We utilized a judgment dataset labeled as the "golden standard" by
experts, which includes human-verified features that can be converted into an
"expert similarity score." We then constructed a knowledge graph based on
"case-article" relationships, ranking each case using natural language
processing to derive a "Node2vec similarity score." By evaluating these two
similarity scores, we identified their discrepancies and relationships. The
results can significantly reduce the labor hours required for legal searches
and recommendations, with potential applications extending to various fields of
information retrieval.

ÊëòË¶ÅÔºöÂú®Ê≥ïÂ∫≠ÂØ¶Âãô‰∏≠ÔºåÊ≥ïÂæãÂ∞àÊ•≠‰∫∫Â£´‰æùË≥¥ÂÖ∂ÂüπË®ìÊèê‰æõÊÑèË¶ã‰ª•Ëß£Ê±∫Ê°à‰ª∂ÔºåÂÖ∂‰∏≠ÊúÄÈóúÈçµÁöÑÊñπÈù¢‰πã‰∏ÄÊòØÊúâÊïàË≠òÂà•ÂÖàÂâçÊ≥ïÈô¢ÁöÑÈ°û‰ººÂà§Ê±∫ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÊâæÂá∫È°û‰ººÊ°à‰ª∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰∏îÈÄöÂ∏∏ÂèñÊ±∫ÊñºÁ∂ìÈ©ó„ÄÅÊ≥ïÂæãÈ†òÂüüÁü•Ë≠òÂíåÂ§ßÈáèÁöÑÂãûÂãïÊôÇÈñìÔºåÈÄô‰ΩøÂæóË≥áÊ∑±ÂæãÂ∏´ÊàñÊ≥ïÂÆò‰∏çÂèØÊàñÁº∫„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Ëá™ÂãïÂåñÂà§Ê±∫ÊñáÊú¨Áõ∏‰ººÊÄßÁöÑÂàÜÊûê„ÄÇÊàëÂÄëÂà©Áî®Â∞àÂÆ∂Ê®ôË®òÁÇ∫„ÄåÈªÉÈáëÊ®ôÊ∫ñ„ÄçÁöÑÂà§Ê±∫Ë≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÂèØËΩâÊèõÁÇ∫„ÄåÂ∞àÂÆ∂Áõ∏‰ººÊÄßË©ïÂàÜ„ÄçÁöÑ‰∫∫Â∑•È©óË≠âÁâπÂæµ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊ†πÊìö„ÄåÊ°à‰æã-Ê¢ùÊñá„ÄçÈóú‰øÇÂª∫ÊßãÁü•Ë≠òÂúñË≠úÔºå‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂ∞çÊØèÂÄãÊ°à‰æãÈÄ≤Ë°åÊéíÂêçÔºå‰ª•ÂæóÂá∫„ÄåNode2vec Áõ∏‰ººÊÄßË©ïÂàÜ„Äç„ÄÇÈÄèÈÅéË©ï‰º∞ÈÄôÂÖ©ÂÄãÁõ∏‰ººÊÄßË©ïÂàÜÔºåÊàëÂÄëÊâæÂá∫ÂÖ∂Â∑ÆÁï∞ÂíåÈóú‰øÇ„ÄÇÁµêÊûúÂèØ‰ª•Â§ßÂπÖÊ∏õÂ∞ëÊ≥ïÂæãÊêúÂ∞ãÂíåÂª∫Ë≠∞ÊâÄÈúÄÁöÑÂãûÂãïÊôÇÈñìÔºåÊΩõÂú®ÊáâÁî®ÁØÑÂúçÊì¥ÂèäË≥áË®äÊ™¢Á¥¢ÁöÑÂêÑÂÄãÈ†òÂüü„ÄÇ

##### **wav2graph: A Framework for Supervised Learning Knowledge Graph from Speech**
2408.04174v1 by Khai Le-Duc, Quy-Anh Dang, Tan-Hanh Pham, Truong-Son Hy

Knowledge graphs (KGs) enhance the performance of large language models
(LLMs) and search engines by providing structured, interconnected data that
improves reasoning and context-awareness. However, KGs only focus on text data,
thereby neglecting other modalities such as speech. In this work, we introduce
wav2graph, the first framework for supervised learning knowledge graph from
speech data. Our pipeline are straightforward: (1) constructing a KG based on
transcribed spoken utterances and a named entity database, (2) converting KG
into embedding vectors, and (3) training graph neural networks (GNNs) for node
classification and link prediction tasks. Through extensive experiments
conducted in inductive and transductive learning contexts using
state-of-the-art GNN models, we provide baseline results and error analysis for
node classification and link prediction tasks on human transcripts and
automatic speech recognition (ASR) transcripts, including evaluations using
both encoder-based and decoder-based node embeddings, as well as monolingual
and multilingual acoustic pre-trained models. All related code, data, and
models are published online.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ÈÄèÈÅéÊèê‰æõÁµêÊßãÂåñ„ÄÅÁõ∏‰∫íÈÄ£ÁµêÁöÑË≥áÊñôÔºåÈÄ≤ËÄåÊîπÂñÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÊêúÂ∞ãÂºïÊìéÁöÑÊïàËÉΩÔºåÊèêÂçáÊé®ÁêÜÂíåËÑàÁµ°ÊÑüÁü•„ÄÇÁÑ∂ËÄåÔºåKG Âè™ÈóúÊ≥®ÊñáÂ≠óË≥áÊñôÔºåÂõ†Ê≠§ÂøΩÁï•‰∫ÜÂÖ∂‰ªñÂΩ¢ÂºèÔºå‰æãÂ¶ÇË™ûÈü≥„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π wav2graphÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂæûË™ûÈü≥Ë≥áÊñô‰∏≠Áõ£Áù£Â≠∏ÁøíÁü•Ë≠òÂúñË≠úÁöÑÊû∂Êßã„ÄÇÊàëÂÄëÁöÑÊµÅÁ®ãÂæàÁõ¥Êé•Ôºö(1) Ê†πÊìöËΩâÈåÑÁöÑÂè£Ë™ûË°®ÈÅîÂíåÂëΩÂêçÂØ¶È´îË≥áÊñôÂ∫´Âª∫Êßã KGÔºå(2) Â∞á KG ËΩâÊèõÁÇ∫ÂµåÂÖ•ÂêëÈáèÔºå‰ª•Âèä (3) Ë®ìÁ∑¥ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ‰ª•ÈÄ≤Ë°åÁØÄÈªûÂàÜÈ°ûÂíåÈÄ£ÁµêÈ†êÊ∏¨‰ªªÂãô„ÄÇÈÄèÈÅé‰ΩøÁî®ÊúÄÂÖàÈÄ≤ÁöÑ GNN Ê®°ÂûãÂú®Ê≠∏Á¥çÂíåËΩâÂ∞éÂ≠∏ÁøíÁöÑÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºåÊàëÂÄëÊèê‰æõÁØÄÈªûÂàÜÈ°ûÂíåÈÄ£ÁµêÈ†êÊ∏¨‰ªªÂãôÁöÑÂü∫Ê∫ñÁµêÊûúÂíåÈåØË™§ÂàÜÊûêÔºåÂÖ∂‰∏≠ÂåÖÊã¨‰ΩøÁî®Á∑®Á¢ºÂô®ÁÇ∫Âü∫Á§éÂíåËß£Á¢ºÂô®ÁÇ∫Âü∫Á§éÁöÑÁØÄÈªûÂµåÂÖ•Ôºå‰ª•ÂèäÂñÆË™ûÂíåÂ§öË™ûÈü≥Â≠∏È†êË®ìÁ∑¥Ê®°ÂûãÁöÑË©ï‰º∞„ÄÇÊâÄÊúâÁõ∏ÈóúÁ®ãÂºèÁ¢º„ÄÅË≥áÊñôÂíåÊ®°ÂûãÁöÜÂ∑≤Âú®Á∑ö‰∏äÁôºÂ∏É„ÄÇ

##### **ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling**
2408.04102v1 by William Y. Zhu, Keren Ye, Junjie Ke, Jiahui Yu, Leonidas Guibas, Peyman Milanfar, Feng Yang

Recognizing and disentangling visual attributes from objects is a foundation
to many computer vision applications. While large vision language
representations like CLIP had largely resolved the task of zero-shot object
recognition, zero-shot visual attribute recognition remains a challenge because
CLIP's contrastively-learned vision-language representation cannot effectively
capture object-attribute dependencies. In this paper, we target this weakness
and propose a sentence generation-based retrieval formulation for attribute
recognition that is novel in 1) explicitly modeling a to-be-measured and
retrieved object-attribute relation as a conditional probability graph, which
converts the recognition problem into a dependency-sensitive language-modeling
problem, and 2) applying a large pretrained Vision-Language Model (VLM) on this
reformulation and naturally distilling its knowledge of image-object-attribute
relations to use towards attribute recognition. Specifically, for each
attribute to be recognized on an image, we measure the visual-conditioned
probability of generating a short sentence encoding the attribute's relation to
objects on the image. Unlike contrastive retrieval, which measures likelihood
by globally aligning elements of the sentence to the image, generative
retrieval is sensitive to the order and dependency of objects and attributes in
the sentence. We demonstrate through experiments that generative retrieval
consistently outperforms contrastive retrieval on two visual reasoning
datasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual
Genome Attribute Ranking (VGARank).

ÊëòË¶ÅÔºöËæ®Ë≠òÂíåÂçÄÂàÜÁâ©‰ª∂ÁöÑË¶ñË¶∫Â±¨ÊÄßÔºåÊòØË®±Â§öÈõªËÖ¶Ë¶ñË¶∫ÊáâÁî®Á®ãÂºèÁöÑÂü∫Á§é„ÄÇÈõñÁÑ∂ÂÉè CLIP ÈÄôÊ®£ÁöÑÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄË°®ÂæµÔºåÂ∑≤Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äËß£Ê±∫‰∫ÜÈõ∂Ê¨°Â≠∏ÁøíÁâ©‰ª∂Ëæ®Ë≠òÁöÑ‰ªªÂãôÔºå‰ΩÜÈõ∂Ê¨°Â≠∏ÁøíË¶ñË¶∫Â±¨ÊÄßËæ®Ë≠ò‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÊåëÊà∞ÔºåÂõ†ÁÇ∫ CLIP Â∞çÊØîÂ≠∏ÁøíÁöÑË¶ñË¶∫Ë™ûË®ÄË°®ÂæµÔºåÁÑ°Ê≥ïÊúâÊïàÊì∑ÂèñÁâ©‰ª∂Â±¨ÊÄß‰æùË≥¥ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈáùÂ∞çÊ≠§Âº±ÈªûÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÂè•Â≠êÁîüÊàêÁöÑÊ™¢Á¥¢ÂÖ¨ÂºèÔºåÁî®ÊñºÂ±¨ÊÄßËæ®Ë≠òÔºåÂÖ∂Êñ∞Á©é‰πãËôïÂú®ÊñºÔºö1) ÊòéÁ¢∫Âú∞Â∞áÂæÖÊ∏¨ÈáèÂíåÊ™¢Á¥¢ÁöÑÁâ©‰ª∂Â±¨ÊÄßÈóú‰øÇÂª∫Ê®°ÁÇ∫Ê¢ù‰ª∂Ê©üÁéáÂúñÔºåÈÄôÂ∞áËæ®Ë≠òÂïèÈ°åËΩâÊèõÁÇ∫‰æùË≥¥ÊïèÊÑüÁöÑË™ûË®ÄÊ®°ÂûãÂïèÈ°åÔºõ2) Âú®Ê≠§ÈáçÊñ∞ÂÖ¨ÂºèÂåñ‰∏äÊáâÁî®Â§ßÂûãÈ†êË®ìÁ∑¥ÁöÑË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)Ôºå‰∏¶Ëá™ÁÑ∂Âú∞ËêÉÂèñÂÖ∂Â∞çÂΩ±ÂÉèÁâ©‰ª∂Â±¨ÊÄßÈóú‰øÇÁöÑÁü•Ë≠òÔºåÁî®ÊñºÂ±¨ÊÄßËæ®Ë≠ò„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂ∞çÊñºË¶ÅÂú®ÂΩ±ÂÉè‰∏äËæ®Ë≠òÁöÑÊØèÂÄãÂ±¨ÊÄßÔºåÊàëÂÄëÊ∏¨ÈáèÂú®ÂΩ±ÂÉè‰∏äÁ∑®Á¢ºÂ±¨ÊÄßËàáÁâ©‰ª∂Èóú‰øÇÁöÑÁ∞°Áü≠Âè•Â≠êÁöÑË¶ñË¶∫Ê¢ù‰ª∂Ê©üÁéá„ÄÇËàáÂ∞çÊØîÊ™¢Á¥¢‰∏çÂêåÔºåÂ∞çÊØîÊ™¢Á¥¢ÊòØÈÄèÈÅéÂ∞áÂè•Â≠êÁöÑÂÖÉÁ¥†Êï¥È´îÊØîÂ∞çÂà∞ÂΩ±ÂÉè‰æÜÊ∏¨ÈáèÂèØËÉΩÊÄßÔºåÁîüÊàêÊ™¢Á¥¢ÂâáÂ∞çÂè•Â≠ê‰∏≠Áâ©‰ª∂ÂíåÂ±¨ÊÄßÁöÑÈ†ÜÂ∫èÂíå‰æùË≥¥ÊÄßÂæàÊïèÊÑü„ÄÇÊàëÂÄëÈÄèÈÅéÂØ¶È©óË≠âÊòéÔºåÁîüÊàêÊ™¢Á¥¢Âú®ÂÖ©ÂÄãË¶ñË¶∫Êé®ÁêÜË≥áÊñôÈõÜÔºåÈáéÂ§ñË¶ñË¶∫Â±¨ÊÄß (VAW) ÂíåÊàëÂÄëÊñ∞ÊèêÂá∫ÁöÑË¶ñË¶∫Âü∫Âõ†ÁµÑÂ±¨ÊÄßÊéíÂêç (VGARank) ‰∏äÔºåÂßãÁµÇÂÑ™ÊñºÂ∞çÊØîÊ™¢Á¥¢„ÄÇ

##### **CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases**
2408.03910v2 by Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, Wenmeng Zhou

Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval
and MBPP, but struggle with handling entire code repositories. This challenge
has prompted research on enhancing LLM-codebase interaction at a repository
scale. Current solutions rely on similarity-based retrieval or manual tools and
APIs, each with notable drawbacks. Similarity-based retrieval often has low
recall in complex tasks, while manual tools and APIs are typically
task-specific and require expert knowledge, reducing their generalizability
across diverse code tasks and real-world applications. To mitigate these
limitations, we introduce CodexGraph, a system that integrates LLM agents with
graph database interfaces extracted from code repositories. By leveraging the
structural properties of graph databases and the flexibility of the graph query
language, CodexGraph enables the LLM agent to construct and execute queries,
allowing for precise, code structure-aware context retrieval and code
navigation. We assess CodexGraph using three benchmarks: CrossCodeEval,
SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding
applications. With a unified graph database schema, CodexGraph demonstrates
competitive performance and potential in both academic and real-world
environments, showcasing its versatility and efficacy in software engineering.
Our application demo:
https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.

ÊëòË¶ÅÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®Áã¨Á´ã‰ª£Á†Å‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰æãÂ¶Ç HumanEval Âíå MBPPÔºå‰ΩÜÂú®Â§ÑÁêÜÊï¥‰∏™‰ª£Á†ÅÂ≠òÂÇ®Â∫ìÊó∂Âç¥ÈÅáÂà∞‰∫ÜÂõ∞Èöæ„ÄÇËøô‰∏™ÊåëÊàò‰øÉËøõ‰∫ÜÂØπÂú®Â≠òÂÇ®Â∫ìËßÑÊ®°‰∏äÂ¢ûÂº∫ LLM ‰ª£Á†ÅÂ∫ì‰∫§‰∫íÁöÑÁ†îÁ©∂„ÄÇÂΩìÂâçÁöÑËß£ÂÜ≥ÊñπÊ°à‰æùËµñ‰∫éÂü∫‰∫éÁõ∏‰ººÊÄßÁöÑÊ£ÄÁ¥¢ÊàñÊâãÂä®Â∑•ÂÖ∑Âíå APIÔºåÊØèÁßçËß£ÂÜ≥ÊñπÊ°àÈÉΩÊúâÊòéÊòæÁöÑÁº∫ÁÇπ„ÄÇÂü∫‰∫éÁõ∏‰ººÊÄßÁöÑÊ£ÄÁ¥¢Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÈÄöÂ∏∏Âè¨ÂõûÁéáËæÉ‰ΩéÔºåËÄåÊâãÂä®Â∑•ÂÖ∑Âíå API ÈÄöÂ∏∏ÊòØÁâπÂÆö‰∫é‰ªªÂä°ÁöÑÔºåÂπ∂‰∏îÈúÄË¶Å‰∏ì‰∏öÁü•ËØÜÔºå‰ªéËÄåÈôç‰Ωé‰∫ÜÂÆÉ‰ª¨Âú®‰∏çÂêå‰ª£Á†Å‰ªªÂä°ÂíåÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊ≥õÂåñÊÄß„ÄÇ‰∏∫‰∫ÜÂáèËΩªËøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü CodexGraphÔºåËøôÊòØ‰∏Ä‰∏™Â∞Ü LLM ‰ª£ÁêÜ‰∏é‰ªé‰ª£Á†ÅÂ≠òÂÇ®Â∫ì‰∏≠ÊèêÂèñÁöÑÂõæÂΩ¢Êï∞ÊçÆÂ∫ìÁïåÈù¢ÈõÜÊàêÁöÑÁ≥ªÁªü„ÄÇÈÄöËøáÂà©Áî®ÂõæÂΩ¢Êï∞ÊçÆÂ∫ìÁöÑÁªìÊûÑÂ±ûÊÄßÂíåÂõæÂΩ¢Êü•ËØ¢ËØ≠Ë®ÄÁöÑÁÅµÊ¥ªÊÄßÔºåCodexGraph ‰Ωø LLM ‰ª£ÁêÜËÉΩÂ§üÊûÑÂª∫ÂíåÊâßË°åÊü•ËØ¢Ôºå‰ªéËÄåÂÆûÁé∞Á≤æÁ°ÆÁöÑ„ÄÅ‰ª£Á†ÅÁªìÊûÑÊÑüÁü•ÁöÑ‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢Âíå‰ª£Á†ÅÂØºËà™„ÄÇÊàë‰ª¨‰ΩøÁî®‰∏â‰∏™Âü∫ÂáÜÂØπ CodexGraph ËøõË°å‰∫ÜËØÑ‰º∞ÔºöCrossCodeEval„ÄÅSWE-bench Âíå EvoCodeBench„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÂºÄÂèë‰∫Ü‰∫î‰∏™ÂÆûÈôÖÁöÑÁºñÁ†ÅÂ∫îÁî®Á®ãÂ∫è„ÄÇÈÄöËøáÁªü‰∏ÄÁöÑÂõæÂΩ¢Êï∞ÊçÆÂ∫ìÊ®°ÂºèÔºåCodexGraph Âú®Â≠¶ÊúØÂíåÁé∞ÂÆû‰∏ñÁïåÁéØÂ¢É‰∏≠ÈÉΩÂ±ïÁ§∫‰∫ÜÁ´û‰∫âÊÄßËÉΩÂíåÊΩúÂäõÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ËΩØ‰ª∂Â∑•Á®ã‰∏≠ÁöÑÂ§öÂäüËÉΩÊÄßÂíåÊúâÊïàÊÄß„ÄÇÊàë‰ª¨ÁöÑÂ∫îÁî®Á®ãÂ∫èÊºîÁ§∫Ôºöhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent„ÄÇ

##### **PAGED: A Benchmark for Procedural Graphs Extraction from Documents**
2408.03630v2 by Weihong Du, Wenrui Liao, Hongru Liang, Wenqiang Lei

Automatic extraction of procedural graphs from documents creates a low-cost
way for users to easily understand a complex procedure by skimming visual
graphs. Despite the progress in recent studies, it remains unanswered: whether
the existing studies have well solved this task (Q1) and whether the emerging
large language models (LLMs) can bring new opportunities to this task (Q2). To
this end, we propose a new benchmark PAGED, equipped with a large high-quality
dataset and standard evaluations. It investigates five state-of-the-art
baselines, revealing that they fail to extract optimal procedural graphs well
because of their heavy reliance on hand-written rules and limited available
data. We further involve three advanced LLMs in PAGED and enhance them with a
novel self-refine strategy. The results point out the advantages of LLMs in
identifying textual elements and their gaps in building logical structures. We
hope PAGED can serve as a major landmark for automatic procedural graph
extraction and the investigations in PAGED can offer insights into the research
on logic reasoning among non-sequential elements.

ÊëòË¶ÅÔºöËá™ÂãïÂæûÊñá‰ª∂‰∏≠ËêÉÂèñÁ®ãÂ∫èÂúñË°®ÊòØ‰∏ÄÁ®Æ‰ΩéÊàêÊú¨ÁöÑÊñπÂºèÔºåËÆì‰ΩøÁî®ËÄÖËÉΩÈÄèÈÅéÁÄèË¶ΩË¶ñË¶∫ÂåñÂúñË°®ÔºåËºïÈ¨ÜÁêÜËß£Ë§áÈõúÁöÑÁ®ãÂ∫è„ÄÇÂÑòÁÆ°ËøëÊúüÁ†îÁ©∂Â∑≤ÊúâÊâÄÈÄ≤Â±ïÔºå‰ΩÜ‰ªçÊúâÂæÖËß£Á≠îÁöÑÂïèÈ°åÔºöÁèæÊúâÁöÑÁ†îÁ©∂ÊòØÂê¶Â∑≤Â¶•ÂñÑËß£Ê±∫Ê≠§‰ªªÂãôÔºàQ1ÔºâÔºå‰ª•ÂèäÊñ∞ËààÁöÑÂ§ßË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÊòØÂê¶ËÉΩÁÇ∫Ê≠§‰ªªÂãôÂ∏∂‰æÜÊñ∞ÁöÑÂ•ëÊ©üÔºàQ2Ôºâ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Ê∫ñ PAGEDÔºåÈÖçÂÇôÂ§ßÂûãÈ´òÂìÅË≥™Ë≥áÊñôÈõÜÂíåÊ®ôÊ∫ñË©ïÈáè„ÄÇÂÆÉÊé¢Ë®é‰∫Ü‰∫îÂÄãÊúÄÂÖàÈÄ≤ÁöÑÂü∫Á∑öÔºåÊè≠Á§∫‰∫ÜÂÆÉÂÄëÁÑ°Ê≥ïËâØÂ•ΩÂú∞ËêÉÂèñÊúÄ‰Ω≥Á®ãÂ∫èÂúñË°®ÔºåÂéüÂõ†Âú®ÊñºÂÆÉÂÄëÈÅéÂ∫¶‰æùË≥¥ÊâãÂØ´Ë¶èÂâáÂíåÊúâÈôêÁöÑÂèØÁî®Ë≥áÊñô„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Âú® PAGED ‰∏≠Á¥çÂÖ•‰∏âÂÄãÂÖàÈÄ≤ÁöÑ LLMÔºå‰∏¶ÈÄèÈÅéÊñ∞Á©éÁöÑËá™Á≤æÈÄ≤Á≠ñÁï•Âä†‰ª•Âº∑Âåñ„ÄÇÁµêÊûúÊåáÂá∫ LLM Âú®Ë≠òÂà•ÊñáÊú¨ÂÖÉÁ¥†ÊñπÈù¢ÁöÑÂÑ™Âã¢Ôºå‰ª•ÂèäÂÆÉÂÄëÂú®Âª∫Á´ãÈÇèËºØÁµêÊßãÊñπÈù¢ÁöÑÂ∑ÆË∑ù„ÄÇÊàëÂÄëÂ∏åÊúõ PAGED ËÉΩÊàêÁÇ∫Ëá™ÂãïÁ®ãÂ∫èÂúñË°®ËêÉÂèñÁöÑ‰∏ªË¶ÅÈáåÁ®ãÁ¢ëÔºåËÄå PAGED ‰∏≠ÁöÑÊé¢Ë®éËÉΩÁÇ∫ÈùûÈ†ÜÂ∫èÂÖÉÁ¥†ÈñìÁöÑÈÇèËºØÊé®ÁêÜÁ†îÁ©∂Êèê‰æõË¶ãËß£„ÄÇ

##### **Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks**
2408.03615v1 by Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie

Building a general-purpose agent is a long-standing vision in the field of
artificial intelligence. Existing agents have made remarkable progress in many
domains, yet they still struggle to complete long-horizon tasks in an open
world. We attribute this to the lack of necessary world knowledge and
multimodal experience that can guide agents through a variety of long-horizon
tasks. In this paper, we propose a Hybrid Multimodal Memory module to address
the above challenges. It 1) transforms knowledge into Hierarchical Directed
Knowledge Graph that allows agents to explicitly represent and learn world
knowledge, and 2) summarises historical information into Abstracted Multimodal
Experience Pool that provide agents with rich references for in-context
learning. On top of the Hybrid Multimodal Memory module, a multimodal agent,
Optimus-1, is constructed with dedicated Knowledge-guided Planner and
Experience-Driven Reflector, contributing to a better planning and reflection
in the face of long-horizon tasks in Minecraft. Extensive experimental results
show that Optimus-1 significantly outperforms all existing agents on
challenging long-horizon task benchmarks, and exhibits near human-level
performance on many tasks. In addition, we introduce various Multimodal Large
Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show
that Optimus-1 exhibits strong generalization with the help of the Hybrid
Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.

ÊëòË¶ÅÔºöÊâìÈÄ†‰∏ÄÂÄãÈÄöÁî®‰ª£ÁêÜÊòØ‰∫∫Â∑•Êô∫ÊÖßÈ†òÂüüÈï∑‰πÖ‰ª•‰æÜÁöÑÈ°òÊôØ„ÄÇÁèæÊúâÁöÑ‰ª£ÁêÜÂú®Ë®±Â§öÈ†òÂüüÈÉΩÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•Ôºå‰ΩÜÂÆÉÂÄë‰ªçÈõ£‰ª•Âú®ÈñãÊîæ‰∏ñÁïå‰∏≠ÂÆåÊàêÈï∑ÊôÇÁ®ã‰ªªÂãô„ÄÇÊàëÂÄëÂ∞áÊ≠§Ê≠∏Âõ†ÊñºÁº∫‰πèÂøÖË¶ÅÁöÑÁü•Ë≠òÂíåÂ§öÊ®°ÊÖãÁ∂ìÈ©óÔºåÈÄô‰∫õÁü•Ë≠òÂíåÁ∂ìÈ©óÂèØ‰ª•ÂºïÂ∞é‰ª£ÁêÜÂÆåÊàêÂêÑÁ®ÆÈï∑ÊôÇÁ®ã‰ªªÂãô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ∑∑ÂêàÂ§öÊ®°ÊÖãË®òÊÜ∂È´îÊ®°ÁµÑ‰æÜËß£Ê±∫‰∏äËø∞ÊåëÊà∞„ÄÇÂÆÉ 1) Â∞áÁü•Ë≠òËΩâÊèõÁÇ∫ÈöéÂ±§ÂºèÂ∞éÂêëÁü•Ë≠òÂúñÔºåËÆì‰ª£ÁêÜËÉΩÂ§†ÊòéÁ¢∫Âú∞Ë°®Á§∫ÂíåÂ≠∏Áøí‰∏ñÁïåÁü•Ë≠òÔºå‰ª•Âèä 2) Â∞áÊ≠∑Âè≤Ë≥áË®äÊëòË¶ÅÊàêÊäΩË±°ÁöÑÂ§öÊ®°ÊÖãÁ∂ìÈ©óÊ±†ÔºåÁÇ∫‰ª£ÁêÜÊèê‰æõË±êÂØåÁöÑÂèÉËÄÉÔºå‰ª•‰æøÈÄ≤Ë°åÊÉÖÂ¢ÉÂ≠∏Áøí„ÄÇÂú®Ê∑∑ÂêàÂ§öÊ®°ÊÖãË®òÊÜ∂È´îÊ®°ÁµÑ‰πã‰∏äÔºåÂª∫Êßã‰∫Ü‰∏ÄÂÄãÂ§öÊ®°ÊÖã‰ª£ÁêÜÔºåOptimus-1ÔºåÂÆÉÂÖ∑ÂÇôÂ∞àÁî®ÁöÑÁü•Ë≠òÂ∞éÂêëË¶èÂäÉÂô®ÂíåÁ∂ìÈ©óÈ©ÖÂãïÁöÑÂèçÂ∞ÑÂô®ÔºåÊúâÂä©ÊñºÂú® Minecraft ‰∏≠Èù¢Â∞çÈï∑ÊôÇÁ®ã‰ªªÂãôÊôÇÈÄ≤Ë°åÊõ¥Â•ΩÁöÑË¶èÂäÉÂíåÂèçÊÄù„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåOptimus-1 Âú®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÈï∑ÊôÇÁ®ã‰ªªÂãôÂü∫Ê∫ñ‰∏äÈ°ØËëóÂÑ™ÊñºÊâÄÊúâÁèæÊúâ‰ª£ÁêÜÔºå‰∏¶‰∏îÂú®Ë®±Â§ö‰ªªÂãô‰∏äÂ±ïÁèæÂá∫Êé•Ëøë‰∫∫È°ûÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•ÂêÑÁ®ÆÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ‰ΩúÁÇ∫ Optimus-1 ÁöÑÈ™®Âππ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåOptimus-1 Âú®Ê∑∑ÂêàÂ§öÊ®°ÊÖãË®òÊÜ∂È´îÊ®°ÁµÑÁöÑÂπ´Âä©‰∏ãÂ±ïÁèæÂá∫Âº∑Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂú®Ë®±Â§ö‰ªªÂãô‰∏äÂÑ™Êñº GPT-4V Âü∫Ê∫ñ„ÄÇ

##### **Exploring the extent of similarities in software failures across industries using LLMs**
2408.03528v2 by Martin Detloff

The rapid evolution of software development necessitates enhanced safety
measures. Extracting information about software failures from companies is
becoming increasingly more available through news articles.
  This research utilizes the Failure Analysis Investigation with LLMs (FAIL)
model to extract industry-specific information. Although the FAIL model's
database is rich in information, it could benefit from further categorization
and industry-specific insights to further assist software engineers.
  In previous work news articles were collected from reputable sources and
categorized by incidents inside a database. Prompt engineering and Large
Language Models (LLMs) were then applied to extract relevant information
regarding the software failure. This research extends these methods by
categorizing articles into specific domains and types of software failures. The
results are visually represented through graphs.
  The analysis shows that throughout the database some software failures occur
significantly more often in specific industries. This categorization provides a
valuable resource for software engineers and companies to identify and address
common failures.
  This research highlights the synergy between software engineering and Large
Language Models (LLMs) to automate and enhance the analysis of software
failures. By transforming data from the database into an industry specific
model, we provide a valuable resource that can be used to identify common
vulnerabilities, predict potential risks, and implement proactive measures for
preventing software failures. Leveraging the power of the current FAIL database
and data visualization, we aim to provide an avenue for safer and more secure
software in the future.

ÊëòË¶ÅÔºö<paragraph>ËªüÈ´îÈñãÁôºÂø´ÈÄüÊºîÈÄ≤ÔºåËø´ÂàáÈúÄË¶ÅÂ¢ûÂº∑ÂÆâÂÖ®Êé™ÊñΩ„ÄÇÂæûÂÖ¨Âè∏Êñ∞ËÅûÊñáÁ´†‰∏≠ËêÉÂèñËªüÈ´îÊïÖÈöúË≥áË®äÊ≠£ËÆäÂæóË∂ä‰æÜË∂äÂÆπÊòì„ÄÇ
Ê≠§Á†îÁ©∂Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÊïÖÈöúÂàÜÊûêË™øÊü•ÔºàFAILÔºâÊ®°ÂûãËêÉÂèñÁî¢Ê•≠ÁâπÂÆöË≥áË®ä„ÄÇÂÑòÁÆ° FAIL Ê®°ÂûãÁöÑË≥áÊñôÂ∫´Ë≥áË®äË±êÂØåÔºå‰ΩÜËã•ËÉΩÈÄ≤‰∏ÄÊ≠•ÂàÜÈ°û‰∏¶Êèê‰æõÁî¢Ê•≠ÁâπÂÆöË¶ãËß£ÔºåÂ∞áÊúâÂä©ÊñºËªüÈ´îÂ∑•Á®ãÂ∏´„ÄÇ
Âú®ÂÖàÂâçÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂæû‰ø°Ë≠ΩËâØÂ•ΩÁöÑ‰æÜÊ∫êÊî∂ÈõÜÊñ∞ËÅûÊñáÁ´†Ôºå‰∏¶Â∞áÂÖ∂ÂàÜÈ°ûÁÇ∫Ë≥áÊñôÂ∫´‰∏≠ÁöÑ‰∫ã‰ª∂„ÄÇÊé•ËëóÊáâÁî®ÊèêÁ§∫Â∑•Á®ãÂíåÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâËêÉÂèñËàáËªüÈ´îÊïÖÈöúÁõ∏ÈóúÁöÑË≥áË®ä„ÄÇÊ≠§Á†îÁ©∂ÈÄèÈÅéÂ∞áÊñáÁ´†ÂàÜÈ°ûÂà∞ÁâπÂÆöÈ†òÂüüÂíåËªüÈ´îÊïÖÈöúÈ°ûÂûãÔºåÂª∂‰º∏‰∫ÜÈÄô‰∫õÊñπÊ≥ï„ÄÇÁµêÊûúÈÄèÈÅéÂúñË°®Ë¶ñË¶∫ÂåñÂëàÁèæ„ÄÇ
ÂàÜÊûêÈ°ØÁ§∫ÔºåÂú®Êï¥ÂÄãË≥áÊñôÂ∫´‰∏≠ÔºåÊüê‰∫õËªüÈ´îÊïÖÈöúÂú®ÁâπÂÆöÁî¢Ê•≠‰∏≠ÁôºÁîüÁöÑÈ†ªÁéáÈ°ØËëóËºÉÈ´ò„ÄÇÊ≠§ÂàÜÈ°ûÁÇ∫ËªüÈ´îÂ∑•Á®ãÂ∏´ÂíåÂÖ¨Âè∏Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË≥áÊ∫êÔºåÂèØË≠òÂà•‰∏¶Ëß£Ê±∫Â∏∏Ë¶ãÊïÖÈöú„ÄÇ
Ê≠§Á†îÁ©∂Âº∑Ë™ø‰∫ÜËªüÈ´îÂ∑•Á®ãËàáÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰πãÈñìÁöÑÁ∂úÊïà‰ΩúÁî®ÔºåÂèØËá™ÂãïÂåñ‰∏¶Â¢ûÂº∑ËªüÈ´îÊïÖÈöúÂàÜÊûê„ÄÇÈÄèÈÅéÂ∞áË≥áÊñôÂ∫´‰∏≠ÁöÑË≥áÊñôËΩâÊèõÁÇ∫Áî¢Ê•≠ÁâπÂÆöÊ®°ÂûãÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÈ†ÖÂØ∂Ë≤¥ÁöÑË≥áÊ∫êÔºåÂèØÁî®ÊñºË≠òÂà•Â∏∏Ë¶ãÊºèÊ¥û„ÄÅÈ†êÊ∏¨ÊΩõÂú®È¢®Èö™Ôºå‰∏¶ÂØ¶ÊñΩ‰∏ªÂãïÊé™ÊñΩ‰æÜÈ†êÈò≤ËªüÈ´îÊïÖÈöú„ÄÇÊàëÂÄëÂà©Áî®ÁèæÊúâ FAIL Ë≥áÊñôÂ∫´ÂíåË≥áÊñôË¶ñË¶∫ÂåñÁöÑÂÑ™Âã¢ÔºåÊó®Âú®ÁÇ∫Êú™‰æÜÊèê‰æõÊõ¥ÂÆâÂÖ®‰∏îÁ©©ÂÆöÁöÑËªüÈ´î„ÄÇ</paragraph>

##### **Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion**
2408.03079v1 by Jinglong Gao, Chen Lu, Xiao Ding, Zhongyang Li, Ting Liu, Bing Qin

Event Causality Extraction (ECE) aims at extracting causal event pairs from
texts. Despite ChatGPT's recent success, fine-tuning small models remains the
best approach for the ECE task. However, existing fine-tuning based ECE methods
cannot address all three key challenges in ECE simultaneously: 1) Complex
Causality Extraction, where multiple causal-effect pairs occur within a single
sentence; 2) Subtask~ Interaction, which involves modeling the mutual
dependence between the two subtasks of ECE, i.e., extracting events and
identifying the causal relationship between extracted events; and 3) Knowledge
Fusion, which requires effectively fusing the knowledge in two modalities,
i.e., the expressive pretrained language models and the structured knowledge
graphs. In this paper, we propose a unified ECE framework (UniCE to address all
three issues in ECE simultaneously. Specifically, we design a subtask
interaction mechanism to enable mutual interaction between the two ECE
subtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in
the two modalities. Furthermore, we employ separate decoders for each subtask
to facilitate complex causality extraction. Experiments on three benchmark
datasets demonstrate that our method achieves state-of-the-art performance and
outperforms ChatGPT with a margin of at least 30% F1-score. More importantly,
our model can also be used to effectively improve the ECE performance of
ChatGPT via in-context learning.

ÊëòË¶ÅÔºö‰∫ã‰ª∂Âõ†ÊûúÈóú‰øÇËêÉÂèñ (ECE) ÁöÑÁõÆÊ®ôÊòØÂæûÊñáÊú¨‰∏≠ËêÉÂèñÂá∫Âõ†Êûú‰∫ã‰ª∂Â∞ç„ÄÇÂÑòÁÆ° ChatGPT ÊúÄËøëÁç≤ÂæóÊàêÂäüÔºåÂæÆË™øÂ∞èÂûãÊ®°Âûã‰ªçÊòØ ECE ‰ªªÂãôÁöÑÊúÄ‰Ω≥ÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂü∫ÊñºÂæÆË™øÁöÑ ECE ÊñπÊ≥ïÁÑ°Ê≥ïÂêåÊôÇËß£Ê±∫ ECE ‰∏≠ÁöÑ‰∏âÂÄã‰∏ªË¶ÅÊåëÊà∞Ôºö1) Ë§áÈõúÂõ†ÊûúÈóú‰øÇËêÉÂèñÔºåÂÖ∂‰∏≠Â§öÂÄãÂõ†ÊûúÈóú‰øÇÂ∞çÂá∫ÁèæÂú®ÂñÆ‰∏ÄÂè•Â≠ê‰∏≠Ôºõ2) Â≠ê‰ªªÂãô‰∫íÂãïÔºåÈÄôÊ∂âÂèäÂ∞ç ECE ÁöÑÂÖ©ÂÄãÂ≠ê‰ªªÂãôÔºàÂç≥ËêÉÂèñ‰∫ã‰ª∂ÂíåË≠òÂà•ËêÉÂèñ‰∫ã‰ª∂‰πãÈñìÁöÑÂõ†ÊûúÈóú‰øÇÔºâ‰πãÈñìÁöÑÁõ∏‰∫í‰æùË≥¥ÊÄßÈÄ≤Ë°åÂª∫Ê®°Ôºõ3) Áü•Ë≠òËûçÂêàÔºåÈÄôÈúÄË¶ÅÊúâÊïàÂú∞ËûçÂêàÂÖ©Á®ÆÊ®°Âºè‰∏≠ÁöÑÁü•Ë≠òÔºåÂç≥Ë°®ÈÅîÂºèÁöÑÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÂíåÁµêÊßãÂåñÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁµ±‰∏ÄÁöÑ ECE Ê°ÜÊû∂ (UniCE)Ôºå‰ª•ÂêåÊôÇËß£Ê±∫ ECE ‰∏≠ÁöÑÊâÄÊúâ‰∏âÂÄãÂïèÈ°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂ≠ê‰ªªÂãô‰∫íÂãïÊ©üÂà∂Ôºå‰ª•ÂØ¶ÁèæÂÖ©ÂÄã ECE Â≠ê‰ªªÂãô‰πãÈñìÁöÑÁõ∏‰∫í‰∫íÂãï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÁü•Ë≠òËûçÂêàÊ©üÂà∂‰æÜËûçÂêàÂÖ©Á®ÆÊ®°Âºè‰∏≠ÁöÑÁü•Ë≠ò„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈáùÂ∞çÊØèÂÄãÂ≠ê‰ªªÂãôÊé°Áî®ÂñÆÁç®ÁöÑËß£Á¢ºÂô®Ôºå‰ª•‰øÉÈÄ≤Ë§áÈõúÂõ†ÊûúÈóú‰øÇÁöÑËêÉÂèñ„ÄÇÂú®‰∏âÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºå‰∏¶‰∏î‰ª•Ëá≥Â∞ë 30% ÁöÑ F1 ÂàÜÊï∏ÂÑ™Êñº ChatGPT„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÁöÑÊ®°Âûã‰πüÂèØ‰ª•ÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏ÁøíÊúâÊïàÂú∞ÊèêÂçá ChatGPT ÁöÑ ECE ÊïàËÉΩ„ÄÇ

##### **Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs**
2408.03010v1 by Daniel Steinigen, Roman Teucher, Timm Heine Ruland, Max Rudat, Nicolas Flores-Herr, Peter Fischer, Nikola Milosevic, Christopher Schymura, Angelo Ziletti

Recent advancements in Large Language Models (LLMs) have showcased their
proficiency in answering natural language queries. However, their effectiveness
is hindered by limited domain-specific knowledge, raising concerns about the
reliability of their responses. We introduce a hybrid system that augments LLMs
with domain-specific knowledge graphs (KGs), thereby aiming to enhance factual
correctness using a KG-based retrieval approach. We focus on a medical KG to
demonstrate our methodology, which includes (1) pre-processing, (2) Cypher
query generation, (3) Cypher query processing, (4) KG retrieval, and (5)
LLM-enhanced response generation. We evaluate our system on a curated dataset
of 69 samples, achieving a precision of 78\% in retrieving correct KG nodes.
Our findings indicate that the hybrid system surpasses a standalone LLM in
accuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.
This positions the system as a promising tool for applications that demand
factual correctness and completeness, such as target identification -- a
critical process in pinpointing biological entities for disease treatment or
crop enhancement. Moreover, its intuitive search interface and ability to
provide accurate responses within seconds make it well-suited for
time-sensitive, precision-focused research contexts. We publish the source code
together with the dataset and the prompt templates used.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ±ïÁ§∫‰∫ÜÂÆÉÂÄëÂú®ÂõûÁ≠îËá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÁöÑÊúâÊïàÊÄßÂèóÂà∞ÁâπÂÆöÈ†òÂüüÁü•Ë≠òÊúâÈôêÁöÑÈòªÁ§ôÔºåÈÄôÂºïËµ∑‰∫ÜÂ∞çÂÖ∂ÂõûÊáâÂèØÈù†ÊÄßÁöÑÊìîÊÜÇ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊ∑∑ÂêàÁ≥ªÁµ±ÔºåË©≤Á≥ªÁµ±‰ΩøÁî®ÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÂúñË≠ú (KG) ‰æÜÊì¥ÂÖÖ LLMÔºåÂæûËÄåÊó®Âú®‰ΩøÁî®Âü∫Êñº KG ÁöÑÊ™¢Á¥¢ÊñπÊ≥ï‰æÜÂ¢ûÂº∑‰∫ãÂØ¶Ê≠£Á¢∫ÊÄß„ÄÇÊàëÂÄëÂ∞àÊ≥®Êñº‰∏ÄÂÄãÈÜ´Â≠∏ KG ‰æÜÊºîÁ§∫ÊàëÂÄëÁöÑ methodologyÔºåÂÖ∂‰∏≠ÂåÖÊã¨ (1) È†êËôïÁêÜÔºå(2) Cypher Êü•Ë©¢ÁîüÊàêÔºå(3) Cypher Êü•Ë©¢ËôïÁêÜÔºå(4) KG Ê™¢Á¥¢Ôºå‰ª•Âèä (5) LLM Â¢ûÂº∑ÁöÑÂõûÊáâÁîüÊàê„ÄÇÊàëÂÄëÂú®‰∏ÄÂÄãÁî± 69 ÂÄãÊ®£Êú¨ÁµÑÊàêÁöÑÁ≤æÈÅ∏Êï∏ÊìöÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÁ≥ªÁµ±ÔºåÂú®Ê™¢Á¥¢Ê≠£Á¢∫ÁöÑ KG ÁØÄÈªûÊôÇÈÅîÂà∞‰∫Ü 78% ÁöÑÁ≤æÂ∫¶„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÊ∑∑ÂêàÁ≥ªÁµ±Âú®Ê∫ñÁ¢∫ÊÄßÂíåÂÆåÊï¥ÊÄßÊñπÈù¢ÈÉΩË∂ÖÈÅé‰∫ÜÂñÆÁç®ÁöÑ LLMÔºåÈÄôÈÄöÈÅé LLM ‰ΩúÁÇ∫Ë©ïÂØ©Ë©ï‰º∞ÊñπÊ≥ïÂæóÂà∞È©óË≠â„ÄÇÈÄôÂ∞áÁ≥ªÁµ±ÂÆö‰ΩçÁÇ∫Â∞çÊáâÁî®Á®ãÂºè‰æÜË™™‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÂ∑•ÂÖ∑ÔºåÈÄô‰∫õÊáâÁî®Á®ãÂºèÈúÄË¶Å‰∫ãÂØ¶Ê≠£Á¢∫ÊÄßÂíåÂÆåÊï¥ÊÄßÔºå‰æãÂ¶ÇÁõÆÊ®ôË≠òÂà•‚Äî‚ÄîÂú®ÁñæÁóÖÊ≤ªÁôÇÊàñ‰ΩúÁâ©ÊîπËâØ‰∏≠Á≤æÁ¢∫ÂÆö‰ΩçÁîüÁâ©ÂØ¶È´îÁöÑÈóúÈçµÈÅéÁ®ã„ÄÇÊ≠§Â§ñÔºåÂÖ∂Áõ¥ËßÄÁöÑÊêúÂ∞ã‰ªãÈù¢ÂíåÂú®Êï∏ÁßíÂÖßÊèê‰æõÊ∫ñÁ¢∫ÂõûÊáâÁöÑËÉΩÂäõ‰ΩøÂÖ∂ÈùûÂ∏∏ÈÅ©ÂêàÊôÇÈñìÊïèÊÑü„ÄÅÊ≥®ÈáçÁ≤æÁ¢∫Â∫¶ÁöÑÁ†îÁ©∂ÊÉÖÂ¢É„ÄÇÊàëÂÄëÂ∞áÂéüÂßãÁ¢ºËàáÊï∏ÊìöÈõÜÂíå‰ΩøÁî®ÁöÑÊèêÁ§∫ÁØÑÊú¨‰∏ÄËµ∑ÁôºÂ∏É„ÄÇ

##### **Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large Language Model-Based Question Answering**
2408.02907v1 by Tiezheng Guo, Chen Wang, Yanyi Liu, Jiawei Tang, Pan Li, Sai Xu, Qingwen Yang, Xianlin Gao, Zhi Li, Yingyou Wen

Retrieving external knowledge and prompting large language models with
relevant information is an effective paradigm to enhance the performance of
question-answering tasks. Previous research typically handles paragraphs from
external documents in isolation, resulting in a lack of context and ambiguous
references, particularly in multi-document and complex tasks. To overcome these
challenges, we propose a new retrieval framework IIER, that leverages
Inter-chunk Interactions to Enhance Retrieval. This framework captures the
internal connections between document chunks by considering three types of
interactions: structural, keyword, and semantic. We then construct a unified
Chunk-Interaction Graph to represent all external documents comprehensively.
Additionally, we design a graph-based evidence chain retriever that utilizes
previous paths and chunk interactions to guide the retrieval process. It
identifies multiple seed nodes based on the target question and iteratively
searches for relevant chunks to gather supporting evidence. This retrieval
process refines the context and reasoning chain, aiding the large language
model in reasoning and answer generation. Extensive experiments demonstrate
that IIER outperforms strong baselines across four datasets, highlighting its
effectiveness in improving retrieval and reasoning capabilities.

ÊëòË¶ÅÔºöÂèñÂæóÂ§ñÈÉ®Áü•Ë≠ò‰∏¶ÊèêÁ§∫Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊèê‰æõÁõ∏ÈóúË≥áË®äÔºåÊòØÊèêÂçáÂïèÁ≠î‰ªªÂãôÊïàËÉΩÁöÑÊúâÊïàÂÖ∏ÁØÑ„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂ÈÄöÂ∏∏Â≠§Á´ãÂú∞ËôïÁêÜÂ§ñÈÉ®Êñá‰ª∂‰∏≠ÁöÑÊÆµËêΩÔºåÂ∞éËá¥Áº∫‰πèËÑàÁµ°ÂíåÊ®°Á®úÂÖ©ÂèØÁöÑÂèÉËÄÉÔºåÁâπÂà•ÊòØÂú®Â§öÊñá‰ª∂ÂíåË§áÈõúÁöÑ‰ªªÂãô‰∏≠„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÊ™¢Á¥¢Êû∂Êßã IIERÔºåÂà©Áî®ÂçÄÂ°äÈñì‰∫íÂãï‰æÜÂ¢ûÂº∑Ê™¢Á¥¢„ÄÇÈÄôÂÄãÊû∂ÊßãÈÄèÈÅéËÄÉÈáè‰∏âÁ®ÆÈ°ûÂûãÁöÑ‰∫íÂãï‰æÜÊì∑ÂèñÊñá‰ª∂ÂçÄÂ°ä‰πãÈñìÁöÑÂÖßÈÉ®ÈÄ£ÁµêÔºöÁµêÊßã„ÄÅÈóúÈçµÂ≠óÂíåË™ûÊÑè„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂª∫Êßã‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÂçÄÂ°ä‰∫íÂãïÂúñÔºå‰ª•ÂÖ®Èù¢Ë°®Á§∫ÊâÄÊúâÂ§ñÈÉ®Êñá‰ª∂„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∏ÄÂÄãÂü∫ÊñºÂúñÂΩ¢ÁöÑË≠âÊìöÈèàÊ™¢Á¥¢Âô®ÔºåÂà©Áî®ÂÖàÂâçÁöÑË∑ØÂæëÂíåÂçÄÂ°ä‰∫íÂãï‰æÜÂºïÂ∞éÊ™¢Á¥¢Á®ãÂ∫è„ÄÇÂÆÉÊ†πÊìöÁõÆÊ®ôÂïèÈ°åË≠òÂà•Â§öÂÄãÁ®ÆÂ≠êÁØÄÈªûÔºå‰∏¶ÂèçË¶ÜÊêúÂ∞ãÁõ∏ÈóúÂçÄÂ°ä‰ª•Êî∂ÈõÜ‰ΩêË≠âË≠âÊìö„ÄÇÈÄôÂÄãÊ™¢Á¥¢Á®ãÂ∫èÁ≤æÁÖâ‰∫ÜËÑàÁµ°ÂíåÊé®ÁêÜÈèàÔºåÂçîÂä©Â§ßÂûãË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°åÊé®ÁêÜÂíåÁ≠îÊ°àÁî¢Áîü„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåIIER Âú®ÂõõÂÄãË≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÂº∑Â§ßÁöÑÂü∫Ê∫ñÔºåÁ™ÅÈ°ØÂÖ∂Âú®ÊîπÂñÑÊ™¢Á¥¢ÂíåÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑÊïàËÉΩ„ÄÇ

##### **MaterioMiner -- An ontology-based text mining dataset for extraction of process-structure-property entities**
2408.04661v1 by Ali Riza Durmaz, Akhil Thomas, Lokesh Mishra, Rachana Niranjan Murthy, Thomas Straub

While large language models learn sound statistical representations of the
language and information therein, ontologies are symbolic knowledge
representations that can complement the former ideally. Research at this
critical intersection relies on datasets that intertwine ontologies and text
corpora to enable training and comprehensive benchmarking of neurosymbolic
models. We present the MaterioMiner dataset and the linked materials mechanics
ontology where ontological concepts from the mechanics of materials domain are
associated with textual entities within the literature corpus. Another
distinctive feature of the dataset is its eminently fine-granular annotation.
Specifically, 179 distinct classes are manually annotated by three raters
within four publications, amounting to a total of 2191 entities that were
annotated and curated. Conceptual work is presented for the symbolic
representation of causal composition-process-microstructure-property
relationships. We explore the annotation consistency between the three raters
and perform fine-tuning of pre-trained models to showcase the feasibility of
named-entity recognition model training. Reusing the dataset can foster
training and benchmarking of materials language models, automated ontology
construction, and knowledge graph generation from textual data.

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ≠¶‰π†ËØ≠Ë®ÄÂíåÂÖ∂‰∏≠‰ø°ÊÅØÁöÑÂÅ•ÂÖ®ÁªüËÆ°Ë°®Á§∫ÔºåÊú¨‰ΩìÊòØÁ¨¶Âè∑Áü•ËØÜË°®Á§∫ÔºåÁêÜÊÉ≥ÊÉÖÂÜµ‰∏ãÂèØ‰ª•Ë°•ÂÖÖÂâçËÄÖ„ÄÇÂú®Ëøô‰∏™ÂÖ≥ÈîÆ‰∫§ÂèâÁÇπ‰∏äÁöÑÁ†îÁ©∂‰æùËµñ‰∫éÂ∞ÜÊú¨‰ΩìÂíåÊñáÊú¨ËØ≠ÊñôÂ∫ì‰∫§ÁªáÂú®‰∏ÄËµ∑ÁöÑÊï∞ÊçÆÈõÜÔºå‰ª•ÂÆûÁé∞Á•ûÁªèÁ¨¶Âè∑Ê®°ÂûãÁöÑËÆ≠ÁªÉÂíåÂÖ®Èù¢Âü∫ÂáÜÊµãËØï„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫Ü MaterioMiner Êï∞ÊçÆÈõÜÂíåÈìæÊé•ÁöÑÊùêÊñôÂäõÂ≠¶Êú¨‰ΩìÔºåÂÖ∂‰∏≠ÊùêÊñôÂäõÂ≠¶È¢ÜÂüüÁöÑÊú¨‰ΩìÊ¶ÇÂøµ‰∏éÊñáÁåÆËØ≠ÊñôÂ∫ì‰∏≠ÁöÑÊñáÊú¨ÂÆû‰ΩìÁõ∏ÂÖ≥ËÅî„ÄÇËØ•Êï∞ÊçÆÈõÜÁöÑÂè¶‰∏Ä‰∏™ÊòæÁùÄÁâπÂæÅÊòØÂÖ∂ÊûÅÂÖ∂ÁªÜÁ≤íÂ∫¶ÁöÑÊ≥®Èáä„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå179 ‰∏™‰∏çÂêåÁöÑÁ±ªÂà´Áî±‰∏â‰∏™ËØÑÁ∫ßÂëòÂú®ÂõõÁØáÂá∫ÁâàÁâ©‰∏≠ÊâãÂä®Ê≥®ÈáäÔºåÊÄªÂÖ±Ê≥®ÈáäÂíåÊï¥ÁêÜ‰∫Ü 2191 ‰∏™ÂÆû‰Ωì„ÄÇÊèêÂá∫‰∫ÜÂõ†ÊûúÊàêÂàÜ-ËøáÁ®ã-ÂæÆËßÇÁªìÊûÑ-ÊÄßË¥®ÂÖ≥Á≥ªÁöÑÁ¨¶Âè∑Ë°®Á§∫ÁöÑÊ¶ÇÂøµÊÄßÂ∑•‰Ωú„ÄÇÊàë‰ª¨Êé¢ËÆ®‰∫Ü‰∏â‰∏™ËØÑÁ∫ßÂëò‰πãÈó¥ÁöÑÊ≥®Èáä‰∏ÄËá¥ÊÄßÔºåÂπ∂ÂØπÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ª•Â±ïÁ§∫ÂëΩÂêçÂÆû‰ΩìËØÜÂà´Ê®°ÂûãËÆ≠ÁªÉÁöÑÂèØË°åÊÄß„ÄÇÈáçÂ§ç‰ΩøÁî®ËØ•Êï∞ÊçÆÈõÜÂèØ‰ª•‰øÉËøõÊùêÊñôËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÂü∫ÂáÜÊµãËØï„ÄÅËá™Âä®Êú¨‰ΩìÊûÑÂª∫‰ª•ÂèäÂü∫‰∫éÊñáÊú¨Êï∞ÊçÆÁöÑÁü•ËØÜÂõæË∞±ÁîüÊàê„ÄÇ</paragraph>

##### **A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models**
2408.02377v1 by Vanni Zavarella, Juan Carlos Gamero-Salinas, Sergio Consoli

Knowledge graphs (KGs) have been successfully applied to the analysis of
complex scientific and technological domains, with automatic KG generation
methods typically building upon relation extraction models capturing
fine-grained relations between domain entities in text. While these relations
are fully applicable across scientific areas, existing models are trained on
few domain-specific datasets such as SciERC and do not perform well on new
target domains. In this paper, we experiment with leveraging in-context
learning capabilities of Large Language Models to perform schema-constrained
data annotation, collecting in-domain training instances for a
Transformer-based relation extraction model deployed on titles and abstracts of
research papers in the Architecture, Construction, Engineering and Operations
(AECO) domain. By assessing the performance gain with respect to a baseline
Deep Learning architecture trained on off-domain data, we show that by using a
few-shot learning strategy with structured prompts and only minimal expert
annotation the presented approach can potentially support domain adaptation of
a science KG generation model.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) Â∑≤ÊàêÂäüÊáâÁî®ÊñºÂàÜÊûêË§áÈõúÁöÑÁßëÂ≠∏ÊäÄË°ìÈ†òÂüüÔºåËá™Âãï KG ÁîüÊàêÊñπÊ≥ïÈÄöÂ∏∏Âª∫ÊßãÊñºÈóú‰øÇËêÉÂèñÊ®°Âûã‰∏äÔºåÊçïÊçâÊñáÊú¨‰∏≠È†òÂüüÂØ¶È´î‰πãÈñìÁöÑÁ¥∞Á≤íÂ∫¶Èóú‰øÇ„ÄÇÈõñÁÑ∂ÈÄô‰∫õÈóú‰øÇÂÆåÂÖ®ÈÅ©Áî®ÊñºÂêÑÁßëÂ≠∏È†òÂüüÔºå‰ΩÜÁèæÊúâÊ®°ÂûãÊòØÁî® SciERC Á≠âÂ∞ëÊï∏ÁâπÂÆöÈ†òÂüüÁöÑË≥áÊñôÈõÜË®ìÁ∑¥ÔºåËÄå‰∏îÂú®Êñ∞ÁõÆÊ®ôÈ†òÂüüÁöÑË°®Áèæ‰∏ç‰Ω≥„ÄÇÂú®Êú¨Ë´ñÊñá‰∏≠ÔºåÊàëÂÄëÂòóË©¶Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑËÑàÁµ°Â≠∏ÁøíËÉΩÂäõÔºåÂü∑Ë°åÂèóÊû∂ÊßãÁ¥ÑÊùüÁöÑË≥áÊñôÊ®ôË®ªÔºåÊî∂ÈõÜÈ†òÂüüÂÖßË®ìÁ∑¥ÂØ¶‰æãÔºåÁî®ÊñºÈÉ®ÁΩ≤Âú®Âª∫ÁØâ„ÄÅÁáüÈÄ†„ÄÅÂ∑•Á®ãÂíåÁáüÈÅã (AECO) È†òÂüüÁ†îÁ©∂Ë´ñÊñáÊ®ôÈ°åÂíåÊëòË¶ÅÁöÑÂü∫Êñº Transformer ÁöÑÈóú‰øÇËêÉÂèñÊ®°Âûã„ÄÇÈÄèÈÅéË©ï‰º∞Áõ∏Â∞çÊñºÂú®È†òÂüüÂ§ñË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÂü∫Ê∫ñÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÁöÑÊïàËÉΩÊèêÂçáÔºåÊàëÂÄëÂ±ïÁ§∫ÈÄèÈÅé‰ΩøÁî®Â∏∂ÊúâÁµêÊßãÂåñÊèêÁ§∫ÁöÑÂ∞ëÈáèÂ≠∏ÁøíÁ≠ñÁï•Ôºå‰ª•ÂèäÂÉÖÊúÄÂ∞ëÁöÑÂ∞àÂÆ∂Ê®ôË®ªÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊúâÂèØËÉΩÊîØÊè¥ÁßëÂ≠∏ KG ÁîüÊàêÊ®°ÂûãÁöÑÈ†òÂüüÈÅ©Êáâ„ÄÇ

##### **Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction**
2408.02337v1 by Albert Sawczyn, Katsiaryna Viarenich, Konrad Wojtasik, Aleksandra Domoga≈Ça, Marcin Oleksy, Maciej Piasecki, Tomasz Kajdanowicz

Advancements in AI and natural language processing have revolutionized
machine-human language interactions, with question answering (QA) systems
playing a pivotal role. The knowledge base question answering (KBQA) task,
utilizing structured knowledge graphs (KG), allows for handling extensive
knowledge-intensive questions. However, a significant gap exists in KBQA
datasets, especially for low-resource languages. Many existing construction
pipelines for these datasets are outdated and inefficient in human labor, and
modern assisting tools like Large Language Models (LLM) are not utilized to
reduce the workload. To address this, we have designed and implemented a
modern, semi-automated approach for creating datasets, encompassing tasks such
as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),
tailored explicitly for low-resource environments. We executed this pipeline
and introduced the PUGG dataset, the first Polish KBQA dataset, and novel
datasets for MRC and IR. Additionally, we provide a comprehensive
implementation, insightful findings, detailed statistics, and evaluation of
baseline models.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ËÉΩÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈÄ≤Â±ïÂæπÂ∫ïÊîπËÆä‰∫ÜÊ©üÂô®Ëàá‰∫∫È°ûÁöÑË™ûË®Ä‰∫íÂãïÔºåÂÖ∂‰∏≠ÂïèÁ≠î (QA) Á≥ªÁµ±ÊâÆÊºî‰∫ÜÈóúÈçµËßíËâ≤„ÄÇÁü•Ë≠òÂ∫´ÂïèÁ≠î (KBQA) ‰ªªÂãôÂà©Áî®ÁµêÊßãÂåñÁöÑÁü•Ë≠òÂúñË≠ú (KG)ÔºåÂèØ‰ª•ËôïÁêÜÂ§ßÈáèÁöÑÁü•Ë≠òÂØÜÈõÜÂûãÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåKBQA Ë≥áÊñôÈõÜÂ≠òÂú®ËëóÈ°ØËëóÁöÑÂ∑ÆË∑ùÔºåÁâπÂà•ÊòØÂ∞çÊñº‰ΩéË≥áÊ∫êË™ûË®Ä„ÄÇË®±Â§öÁèæÊúâÁöÑÈÄô‰∫õË≥áÊñôÈõÜÂª∫ÊßãÁÆ°ÈÅìÂ∑≤Á∂ìÈÅéÊôÇ‰∏îÂú®‰∫∫Âäõ‰∏äÊïàÁéá‰Ωé‰∏ãÔºåËÄåÂÉèÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄôÊ®£ÁöÑÁèæ‰ª£ËºîÂä©Â∑•ÂÖ∑‰∏¶Êú™Ë¢´Áî®ÊñºÊ∏õÂ∞ëÂ∑•‰ΩúË≤†Ëºâ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëË®≠Ë®à‰∏¶ÂØ¶‰Ωú‰∫Ü‰∏ÄÁ®ÆÁèæ‰ª£ÁöÑÂçäËá™ÂãïÂåñÊñπÊ≥ï‰æÜÂª∫Á´ãË≥áÊñôÈõÜÔºåÊ∂µËìã‰∫ÜÂ∞àÈñÄÈáùÂ∞ç‰ΩéË≥áÊ∫êÁí∞Â¢ÉÈáèË∫´ÊâìÈÄ†ÁöÑ‰ªªÂãôÔºå‰æãÂ¶Ç KBQA„ÄÅÊ©üÂô®Èñ±ËÆÄÁêÜËß£ (MRC) ÂíåË≥áË®äÊ™¢Á¥¢ (IR)„ÄÇÊàëÂÄëÂü∑Ë°å‰∫ÜÈÄôÂÄãÁÆ°ÈÅì‰∏¶ÂºïÂÖ•‰∫Ü PUGG Ë≥áÊñôÈõÜÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÊ≥¢Ëò≠ KBQA Ë≥áÊñôÈõÜÔºå‰ª•Âèä MRC Âíå IR ÁöÑÊñ∞Á©éË≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶‰Ωú„ÄÅÊúâË¶ãÂú∞ÁöÑÁôºÁèæ„ÄÅË©≥Á¥∞ÁöÑÁµ±Ë®àË≥áÊñôÂíåÂü∫Ê∫ñÊ®°ÂûãÁöÑË©ï‰º∞„ÄÇ

##### **MedSyn: LLM-based Synthetic Medical Text Generation Framework**
2408.02056v1 by Gleb Kumichev, Pavel Blinov, Yulia Kuzkina, Vasily Goncharov, Galina Zubkova, Nikolai Zenovkin, Aleksei Goncharov, Andrey Savchenko

Generating synthetic text addresses the challenge of data availability in
privacy-sensitive domains such as healthcare. This study explores the
applicability of synthetic data in real-world medical settings. We introduce
MedSyn, a novel medical text generation framework that integrates large
language models with a Medical Knowledge Graph (MKG). We use MKG to sample
prior medical information for the prompt and generate synthetic clinical notes
with GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data
through application in the ICD code prediction task. Our research indicates
that synthetic data can increase the classification accuracy of vital and
challenging codes by up to 17.8% compared to settings without synthetic data.
Furthermore, to provide new data for further research in the healthcare domain,
we present the largest open-source synthetic dataset of clinical notes for the
Russian language, comprising over 41k samples covering 219 ICD-10 codes.

ÊëòË¶ÅÔºöÂêàÊàêÊñáÊú¨ÁöÑÁîüÊàêËß£ÂÜ≥‰∫ÜÈöêÁßÅÊïèÊÑüÈ¢ÜÂüüÔºàÂ¶ÇÂåªÁñó‰øùÂÅ•Ôºâ‰∏≠Êï∞ÊçÆÂèØÁî®ÊÄßÁöÑÊåëÊàò„ÄÇÊú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂêàÊàêÊï∞ÊçÆÂú®ÂÆûÈôÖÂåªÁñóÁéØÂ¢É‰∏≠ÁöÑÈÄÇÁî®ÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü MedSynÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂåªÂ≠¶ÊñáÊú¨ÁîüÊàêÊ°ÜÊû∂ÔºåÂÆÉÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏éÂåªÂ≠¶Áü•ËØÜÂõæË∞± (MKG) Áõ∏ÁªìÂêà„ÄÇÊàë‰ª¨‰ΩøÁî® MKG ‰∏∫ÊèêÁ§∫ÈááÊ†∑ÂÖàÈ™åÂåªÂ≠¶‰ø°ÊÅØÔºåÂπ∂‰ΩøÁî® GPT-4 ÂíåÂæÆË∞ÉÁöÑ LLaMA Ê®°ÂûãÁîüÊàêÂêàÊàê‰∏¥Â∫äÊ≥®Èáä„ÄÇÊàë‰ª¨ÈÄöËøáÂú® ICD ‰ª£Á†ÅÈ¢ÑÊµã‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ËØÑ‰º∞‰∫ÜÂêàÊàêÊï∞ÊçÆÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºå‰∏éÊ≤°ÊúâÂêàÊàêÊï∞ÊçÆÁöÑËÆæÁΩÆÁõ∏ÊØîÔºåÂêàÊàêÊï∞ÊçÆÂèØ‰ª•Â∞ÜÈáçË¶Å‰∏îÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰ª£Á†ÅÁöÑÂàÜÁ±ªÂáÜÁ°ÆÊÄßÊèêÈ´òÂ§öËææ 17.8%„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫Ü‰∏∫ÂåªÁñó‰øùÂÅ•È¢ÜÂüüÁöÑËøõ‰∏ÄÊ≠•Á†îÁ©∂Êèê‰æõÊñ∞Êï∞ÊçÆÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÊúÄÂ§ßÁöÑÂºÄÊîæÊ∫ê‰ª£Á†ÅÂêàÊàêÊï∞ÊçÆÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´Ë∂ÖËøá 41k ‰∏™Ê∂µÁõñ 219 ‰∏™ ICD-10 ‰ª£Á†ÅÁöÑ‰∏¥Â∫äÊ≥®Èáä„ÄÇ

##### **DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models**
2408.01933v2 by Bowen Wang, Jiuyang Chang, Yiming Qian, Guoxin Chen, Junhao Chen, Zhouqiang Jiang, Jiahao Zhang, Yuta Nakashima, Hajime Nagahara

Large language models (LLMs) have recently showcased remarkable capabilities,
spanning a wide range of tasks and applications, including those in the medical
domain. Models like GPT-4 excel in medical question answering but may face
challenges in the lack of interpretability when handling complex tasks in real
clinical settings. We thus introduce the diagnostic reasoning dataset for
clinical notes (DiReCT), aiming at evaluating the reasoning ability and
interpretability of LLMs compared to human doctors. It contains 511 clinical
notes, each meticulously annotated by physicians, detailing the diagnostic
reasoning process from observations in a clinical note to the final diagnosis.
Additionally, a diagnostic knowledge graph is provided to offer essential
knowledge for reasoning, which may not be covered in the training data of
existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant
gap between their reasoning ability and that of human doctors, highlighting the
critical need for models that can reason effectively in real-world clinical
scenarios.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊúÄËøëÂ±ïÁ§∫‰∫ÜÈùûÂá°ÁöÑËÉΩÂäõÔºåÊ∂µËìãÂª£Ê≥õÁöÑ‰ªªÂãôÂíåÊáâÁî®ÔºåÂåÖÊã¨ÈÜ´ÁôÇÈ†òÂüüÁöÑ‰ªªÂãôÂíåÊáâÁî®„ÄÇGPT-4 Á≠âÊ®°ÂûãÂú®ÈÜ´ÁôÇÂïèÈ°åËß£Á≠îÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂú®ËôïÁêÜÂØ¶ÈöõËá®Â∫äÂ†¥ÊôØ‰∏≠ÁöÑË§áÈõú‰ªªÂãôÊôÇÔºåÂèØËÉΩÊúÉÈù¢Ëá®Áº∫‰πèÂèØËß£ÈáãÊÄßÁöÑÊåëÊà∞„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜËá®Â∫äÁ≠ÜË®òË®∫Êñ∑Êé®ÁêÜÊï∏ÊìöÈõÜ (DiReCT)ÔºåÊó®Âú®Ë©ï‰º∞ LLM Ëàá‰∫∫È°ûÈÜ´ÁîüÁõ∏ÊØîÁöÑÊé®ÁêÜËÉΩÂäõÂíåÂèØËß£ÈáãÊÄß„ÄÇÂÆÉÂåÖÂê´ 511 ÂÄãËá®Â∫äÁ≠ÜË®òÔºåÊØèÂÄãÁ≠ÜË®òÈÉΩÁ∂ìÈÅéÈÜ´Áîü‰ªîÁ¥∞Ë®ªËß£ÔºåË©≥Á¥∞Ë™™Êòé‰∫ÜÂæûËá®Â∫äÁ≠ÜË®ò‰∏≠ÁöÑËßÄÂØüÁµêÊûúÂà∞ÊúÄÁµÇË®∫Êñ∑ÁöÑË®∫Êñ∑Êé®ÁêÜÈÅéÁ®ã„ÄÇÊ≠§Â§ñÔºåÈÇÑÊèê‰æõ‰∫ÜË®∫Êñ∑Áü•Ë≠òÂúñË≠úÔºå‰ª•Êèê‰æõÊé®ÁêÜÊâÄÈúÄÁöÑÂü∫Êú¨Áü•Ë≠òÔºåÈÄôÂèØËÉΩÊú™Ê∂µËìãÂú®ÁèæÊúâ LLM ÁöÑË®ìÁ∑¥Êï∏Êìö‰∏≠„ÄÇÂú® DiReCT ‰∏äÂ∞çÈ†òÂÖàÁöÑ LLM ÈÄ≤Ë°åË©ï‰º∞ÔºåÁôºÁèæÂÆÉÂÄëÁöÑÊé®ÁêÜËÉΩÂäõËàá‰∫∫È°ûÈÜ´ÁîüÁöÑÊé®ÁêÜËÉΩÂäõ‰πãÈñìÂ≠òÂú®È°ØËëóÂ∑ÆË∑ùÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜÂú®ÁèæÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÂ†¥ÊôØ‰∏≠ËÉΩÂ§†ÊúâÊïàÊé®ÁêÜÁöÑÊ®°ÂûãÁöÑÈóúÈçµÈúÄÊ±Ç„ÄÇ

##### **PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language Models**
2408.04648v1 by Alexey Tikhonov

We present PLUGH (https://www.urbandictionary.com/define.php?term=plugh), a
modern benchmark that currently consists of 5 tasks, each with 125 input texts
extracted from 48 different games and representing 61 different
(non-isomorphic) spatial graphs to assess the abilities of Large Language
Models (LLMs) for spatial understanding and reasoning. Our evaluation of
API-based and open-sourced LLMs shows that while some commercial LLMs exhibit
strong reasoning abilities, open-sourced competitors can demonstrate almost the
same level of quality; however, all models still have significant room for
improvement. We identify typical reasons for LLM failures and discuss possible
ways to deal with them. Datasets and evaluation code are released
(https://github.com/altsoph/PLUGH).

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫ PLUGH (https://www.urbandictionary.com/define.php?term=plugh)Ôºå‰∏ÄÂÄãÁèæ‰ª£Âü∫Ê∫ñÔºåÁõÆÂâçÂåÖÂê´ 5 È†Ö‰ªªÂãôÔºåÊØèÂÄã‰ªªÂãôÊúâ 125 ÂÄãËº∏ÂÖ•ÊñáÂ≠óÔºåÈÄô‰∫õÊñáÂ≠óÂæû 48 ÂÄã‰∏çÂêåÁöÑÈÅäÊà≤‰∏≠Êì∑ÂèñÔºå‰∏¶‰ª£Ë°® 61 ÂÄã‰∏çÂêåÁöÑÔºàÈùûÂêåÊßãÔºâÁ©∫ÈñìÂúñÂΩ¢ÔºåÁî®ÊñºË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÁ©∫ÈñìÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÊàëÂÄëÂ∞çÂü∫Êñº API ÂíåÈñãÊ∫êÁöÑ LLM ÈÄ≤Ë°åË©ï‰º∞ÔºåÁµêÊûúÈ°ØÁ§∫ÔºåÂÑòÁÆ°‰∏Ä‰∫õÂïÜÊ•≠ LLM Â±ïÁèæÂá∫Âº∑Â§ßÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÈñãÊ∫êÁöÑÁ´∂Áà≠ËÄÖÂèØ‰ª•Â±ïÁèæÂπæ‰πéÁõ∏ÂêåÁ≠âÁ¥öÁöÑÂìÅË≥™ÔºõÁÑ∂ËÄåÔºåÊâÄÊúâÊ®°Âûã‰ªçÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•Á©∫Èñì„ÄÇÊàëÂÄëÊâæÂá∫ LLM Â§±ÊïóÁöÑÂÖ∏ÂûãÂéüÂõ†Ôºå‰∏¶Ë®éË´ñÊáâÂ∞çÈÄô‰∫õÂéüÂõ†ÁöÑÂèØËÉΩÊñπÊ≥ï„ÄÇË≥áÊñôÈõÜÂíåË©ï‰º∞Á®ãÂºèÁ¢ºÂ∑≤ÈáãÂá∫Ôºàhttps://github.com/altsoph/PLUGHÔºâ„ÄÇ

##### **Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data**
2408.01700v1 by Antonio De Santis, Marco Balduini, Federico De Santis, Andrea Proia, Arsenio Leo, Marco Brambilla, Emanuele Della Valle

Aerospace manufacturing companies, such as Thales Alenia Space, design,
develop, integrate, verify, and validate products characterized by high
complexity and low volume. They carefully document all phases for each product
but analyses across products are challenging due to the heterogeneity and
unstructured nature of the data in documents. In this paper, we propose a
hybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with
Large Language Models (LLMs) to extract and validate data contained in these
documents. We consider a case study focused on test data related to electronic
boards for satellites. To do so, we extend the Semantic Sensor Network
ontology. We store the metadata of the reports in a KG, while the actual test
results are stored in parquet accessible via a Virtual Knowledge Graph. The
validation process is managed using an LLM-based approach. We also conduct a
benchmarking study to evaluate the performance of state-of-the-art LLMs in
executing this task. Finally, we analyze the costs and benefits of automating
preexisting processes of manual data extraction and validation for subsequent
cross-report analyses.

ÊëòË¶ÅÔºöËà™Â§™Ë£ΩÈÄ†ÂÖ¨Âè∏Ôºå‰æãÂ¶ÇÊ≥∞Èõ∑Ëå≤ÈòøËêäÂ∞º‰∫ûÂ§™Á©∫ÂÖ¨Âè∏ÔºåË®≠Ë®à„ÄÅÈñãÁôº„ÄÅÊï¥Âêà„ÄÅÈ©óË≠âÂíåÈ©óË≠â‰ª•È´òË§áÈõúÂ∫¶Âíå‰ΩéÈ´îÁ©çÁÇ∫ÁâπÂæµÁöÑÁî¢ÂìÅ„ÄÇ‰ªñÂÄë‰ªîÁ¥∞Ë®òÈåÑÊØèÂÄãÁî¢ÂìÅÁöÑÊâÄÊúâÈöéÊÆµÔºå‰ΩÜÁî±ÊñºÊñá‰ª∂‰∏≠Ë≥áÊñôÁöÑÁï∞Ë≥™ÊÄßÂíåÈùûÁµêÊßãÂåñÊÄßË≥™ÔºåÂ∞éËá¥Ë∑®Áî¢ÂìÅÁöÑÂàÜÊûêÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ∑∑ÂêàÊñπÊ≥ïÔºåÂà©Áî®Áü•Ë≠òÂúñË≠ú (KG) ÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰æÜÊì∑ÂèñÂíåÈ©óË≠âÈÄô‰∫õÊñá‰ª∂‰∏≠ÂåÖÂê´ÁöÑË≥áÊñô„ÄÇÊàëÂÄëËÄÉÊÖÆ‰∫Ü‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂ÔºåÈáçÈªûÂú®ÊñºË°õÊòüÈõªÂ≠êÈõªË∑ØÊùøÁöÑÊ∏¨Ë©¶Ë≥áÊñô„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊì¥ÂÖÖ‰∫ÜË™ûÁæ©ÊÑüÊ∏¨Âô®Á∂≤Ë∑ØÊú¨‰Ωì„ÄÇÊàëÂÄëÂ∞áÂ†±ÂëäÁöÑÂÖÉË≥áÊñôÂÑ≤Â≠òÂú® KG ‰∏≠ÔºåËÄåÂØ¶ÈöõÊ∏¨Ë©¶ÁµêÊûúÂÑ≤Â≠òÂú®ÂèØÈÄèÈÅéËôõÊì¨Áü•Ë≠òÂúñË≠úÂ≠òÂèñÁöÑ Parquet ‰∏≠„ÄÇÈ©óË≠âÈÅéÁ®ã‰ΩøÁî®Âü∫Êñº LLM ÁöÑÊñπÊ≥ïÁÆ°ÁêÜ„ÄÇÊàëÂÄëÈÇÑÈÄ≤Ë°åÂü∫Ê∫ñÁ†îÁ©∂Ôºå‰ª•Ë©ï‰º∞ÊúÄÂÖàÈÄ≤ÁöÑ LLM Âú®Âü∑Ë°åÊ≠§‰ªªÂãôÊôÇÁöÑÊïàËÉΩ„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂàÜÊûê‰∫ÜËá™ÂãïÂåñÁèæÊúâÊâãÂãïË≥áÊñôÊì∑ÂèñÂíåÈ©óË≠âÁ®ãÂ∫èÁöÑÊàêÊú¨ÂíåÂ•ΩËôïÔºå‰ª•ÈÄ≤Ë°åÂæåÁ∫åÁöÑË∑®Â†±ÂëäÂàÜÊûê„ÄÇ

##### **DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs**
2408.01154v1 by Zhichun Wang, Xuan Chen

Entity Alignment (EA) aims to match equivalent entities in different
Knowledge Graphs (KGs), which is essential for knowledge fusion and
integration. Recently, embedding-based EA has attracted significant attention
and many approaches have been proposed. Early approaches primarily focus on
learning entity embeddings from the structural features of KGs, defined by
relation triples. Later methods incorporated entities' names and attributes as
auxiliary information to enhance embeddings for EA. However, these approaches
often used different techniques to encode structural and attribute information,
limiting their interaction and mutual enhancement. In this work, we propose a
dense entity retrieval framework for EA, leveraging language models to
uniformly encode various features of entities and facilitate nearest entity
search across KGs. Alignment candidates are first generated through entity
retrieval, which are subsequently reranked to determine the final alignments.
We conduct comprehensive experiments on both cross-lingual and monolingual EA
datasets, demonstrating that our approach achieves state-of-the-art performance
compared to existing EA methods.

ÊëòË¶ÅÔºöÂØ¶È´îÂ∞çÈΩä (EA) Êó®Âú®ÊØîÂ∞ç‰∏çÂêåÁü•Ë≠òÂúñË≠ú (KG) ‰∏≠ÁöÑÁ≠âÊïàÂØ¶È´îÔºåÈÄôÂ∞çÊñºÁü•Ë≠òËûçÂêàÂíåÊï¥ÂêàÈùûÂ∏∏ÈáçË¶Å„ÄÇÊúÄËøëÔºåÂü∫ÊñºÂµåÂÖ•ÁöÑ EA Â∑≤ÂºïËµ∑Áõ∏Áï∂Â§ßÁöÑÈóúÊ≥®Ôºå‰∏¶‰∏îÂ∑≤ÊèêÂá∫Ë®±Â§öÊñπÊ≥ï„ÄÇÊó©ÊúüÁöÑÊñπÊ≥ï‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂæû KG ÁöÑÁµêÊßãÁâπÂæµ‰∏≠Â≠∏ÁøíÂØ¶È´îÂµåÂÖ•ÔºåÈÄô‰∫õÁâπÂæµÁî±Èóú‰øÇ‰∏âÂÖÉÁµÑÂÆöÁæ©„ÄÇÂæåÁ∫åÁöÑÊñπÊ≥ïÂ∞áÂØ¶È´îÁöÑÂêçÁ®±ÂíåÂ±¨ÊÄß‰ΩúÁÇ∫ËºîÂä©Ë≥áË®äÔºå‰ª•Â¢ûÂº∑ EA ÁöÑÂµåÂÖ•„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏‰ΩøÁî®‰∏çÂêåÁöÑÊäÄË°ì‰æÜÁ∑®Á¢ºÁµêÊßãÂíåÂ±¨ÊÄßË≥áË®äÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÁöÑ‰∫íÂãïÂíåÁõ∏‰∫íÂ¢ûÂº∑„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂØÜÈõÜÂØ¶È´îÊì∑ÂèñÊû∂ÊßãÔºåÁî®Êñº EAÔºåÂà©Áî®Ë™ûË®ÄÊ®°Âûã‰æÜÁµ±‰∏ÄÁ∑®Á¢ºÂØ¶È´îÁöÑÂêÑÁ®ÆÁâπÂæµÔºå‰∏¶‰øÉÈÄ≤Ë∑® KG ÁöÑÊúÄËøëÂØ¶È´îÊêúÂ∞ã„ÄÇÂ∞çÈΩäÂÄôÈÅ∏ËÄÖÈ¶ñÂÖàÈÄèÈÅéÂØ¶È´îÊì∑ÂèñÁî¢ÁîüÔºåÁÑ∂ÂæåÈáçÊñ∞ÊéíÂ∫è‰ª•Á¢∫ÂÆöÊúÄÁµÇÂ∞çÈΩä„ÄÇÊàëÂÄëÂ∞çË∑®Ë™ûË®ÄÂíåÂñÆË™ûË®Ä EA Ë≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶È©óÔºåË≠âÊòéËàáÁèæÊúâÁöÑ EA ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇ

##### **Bridging Information Gaps in Dialogues With Grounded Exchanges Using Knowledge Graphs**
2408.01088v2 by Phillip Schneider, Nektarios Machner, Kristiina Jokinen, Florian Matthes

Knowledge models are fundamental to dialogue systems for enabling
conversational interactions, which require handling domain-specific knowledge.
Ensuring effective communication in information-providing conversations entails
aligning user understanding with the knowledge available to the system.
However, dialogue systems often face challenges arising from semantic
inconsistencies in how information is expressed in natural language compared to
how it is represented within the system's internal knowledge. To address this
problem, we study the potential of large language models for conversational
grounding, a mechanism to bridge information gaps by establishing shared
knowledge between dialogue participants. Our approach involves annotating human
conversations across five knowledge domains to create a new dialogue corpus
called BridgeKG. Through a series of experiments on this dataset, we
empirically evaluate the capabilities of large language models in classifying
grounding acts and identifying grounded information items within a knowledge
graph structure. Our findings offer insights into how these models use
in-context learning for conversational grounding tasks and common prediction
errors, which we illustrate with examples from challenging dialogues. We
discuss how the models handle knowledge graphs as a semantic layer between
unstructured dialogue utterances and structured information items.

ÊëòË¶ÅÔºöÁü•Ë≠òÊ®°ÂûãÂ∞çÊñºÂ∞çË©±Á≥ªÁµ±Ëá≥ÈóúÈáçË¶ÅÔºåÂèØÈÄ≤Ë°åÂ∞çË©±‰∫íÂãïÔºåÈúÄË¶ÅËôïÁêÜÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠ò„ÄÇÁ¢∫‰øùÂú®Êèê‰æõË≥áË®äÁöÑÂ∞çË©±‰∏≠ÈÄ≤Ë°åÊúâÊïàÁöÑÊ∫ùÈÄöÔºåÈúÄË¶ÅÂ∞á‰ΩøÁî®ËÄÖÁöÑÁêÜËß£ËàáÁ≥ªÁµ±‰∏≠ÂèØÁî®ÁöÑÁü•Ë≠òÂ∞çÈΩä„ÄÇÁÑ∂ËÄåÔºåÂ∞çË©±Á≥ªÁµ±Á∂ìÂ∏∏ÊúÉÈù¢Ëá®Ë™ûÊÑè‰∏ç‰∏ÄËá¥ÁöÑÊåëÊà∞ÔºåÂú®ÊñºËá™ÁÑ∂Ë™ûË®Ä‰∏≠Ë°®ÈÅîË≥áË®äÁöÑÊñπÂºèËàáÁ≥ªÁµ±ÂÖßÈÉ®Áü•Ë≠òÁöÑË°®Á§∫ÊñπÂºè‰∏çÂêå„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÁ†îÁ©∂Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Â∞çË©±Âü∫Á§é‰∏≠ÁöÑÊΩõÂäõÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈÄèÈÅéÂª∫Á´ãÂ∞çË©±ÂèÉËàáËÄÖ‰πãÈñìÁöÑÂÖ±Áî®Áü•Ë≠ò‰æÜÂΩåË£úË≥áË®äÂ∑ÆË∑ùÁöÑÊ©üÂà∂„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊ∂âÂèäÊ®ôË®ª‰∫îÂÄãÁü•Ë≠òÈ†òÂüü‰∏≠ÁöÑ‰∫∫È°ûÂ∞çË©±Ôºå‰ª•Âª∫Á´ã‰∏ÄÂÄãÂêçÁÇ∫ BridgeKG ÁöÑÊñ∞Â∞çË©±Ë™ûÊñôÂ∫´„ÄÇÈÄèÈÅéÂ∞çÊ≠§Ë≥áÊñôÈõÜÈÄ≤Ë°å‰∏ÄÁ≥ªÂàóÁöÑÂØ¶È©óÔºåÊàëÂÄëÂØ¶Ë≠âË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ÂàÜÈ°ûÂü∫Á§éË°åÁÇ∫ÂíåË≠òÂà•Áü•Ë≠òÂúñÁµêÊßã‰∏≠Â∑≤Êé•Âú∞ÁöÑË≥áË®äÈ†ÖÁõÆÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊèê‰æõ‰∫ÜË¶ãËß£ÔºåË™™ÊòéÈÄô‰∫õÊ®°ÂûãÂ¶Ç‰Ωï‰ΩøÁî®ÊÉÖÂ¢ÉÂ≠∏ÁøíÈÄ≤Ë°åÂ∞çË©±Âü∫Á§é‰ªªÂãôÂíåÂ∏∏Ë¶ãÁöÑÈ†êÊ∏¨ÈåØË™§ÔºåÊàëÂÄëÁî®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂ∞çË©±ÁØÑ‰æã‰æÜË™™Êòé„ÄÇÊàëÂÄëË®éË´ñÊ®°ÂûãÂ¶Ç‰ΩïÂ∞áÁü•Ë≠òÂúñË°®Ë¶ñÁÇ∫ÈùûÁµêÊßãÂåñÂ∞çË©±Ë©±Ë™ûÂíåÁµêÊßãÂåñË≥áË®äÈ†ÖÁõÆ‰πãÈñìÁöÑË™ûÊÑèÂ±§„ÄÇ

##### **Automatic Extraction of Relationships among Motivations, Emotions and Actions from Natural Language Texts**
2408.00966v1 by Fei Yang

We propose a new graph-based framework to reveal relationships among
motivations, emotions and actions explicitly given natural language texts. A
directed acyclic graph is designed to describe human's nature. Nurture beliefs
are incorporated to connect outside events and the human's nature graph. No
annotation resources are required due to the power of large language models.
Amazon Fine Foods Reviews dataset is used as corpus and food-related
motivations are focused. Totally 92,990 relationship graphs are generated, of
which 63% make logical sense. We make further analysis to investigate error
types for optimization direction in future research.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÂü∫ÊñºÂúñÂΩ¢ÁöÑÊû∂ÊßãÔºåÁî®ÊñºÊè≠Á§∫Âú®Ëá™ÁÑ∂Ë™ûË®ÄÊñáÊú¨‰∏≠ÊòéÁ¢∫Áµ¶Âá∫ÁöÑÂãïÊ©ü„ÄÅÊÉÖÁ∑íÂíåÂãï‰Ωú‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊúâÂêëÁÑ°Áí∞ÂúñË¢´Ë®≠Ë®àÁî®ÊñºÊèèËø∞‰∫∫È°ûÁöÑÊú¨ÊÄß„ÄÇÂüπÈ§ä‰ø°ÂøµË¢´Á¥çÂÖ•ÂÖ∂‰∏≠ÔºåÁî®ÊñºÈÄ£Êé•Â§ñÈÉ®‰∫ã‰ª∂Âíå‰∫∫È°ûÁöÑÊú¨ÊÄßÂúñ„ÄÇÁî±ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂº∑Â§ßÂäüËÉΩÔºå‰∏çÈúÄË¶ÅË®ªËß£Ë≥áÊ∫ê„ÄÇ‰∫ûÈ¶¨ÈÅúÁæéÈ£üË©ïË´ñÊï∏ÊìöÈõÜË¢´Áî®‰ΩúË™ûÊñôÂ∫´Ôºå‰∏¶‰∏îÈáçÈªûÈóúÊ≥®ËàáÈ£üÁâ©Áõ∏ÈóúÁöÑÂãïÊ©ü„ÄÇÁ∏ΩÂÖ±ÁîüÊàê‰∫Ü 92,990 ÂÄãÈóú‰øÇÂúñÔºåÂÖ∂‰∏≠ 63% ÂÖ∑ÊúâÈÇèËºØÊÑèÁæ©„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÂàÜÊûê‰ª•Ë™øÊü•ÈåØË™§È°ûÂûãÔºå‰ª•‰æøÁÇ∫Êú™‰æÜÁöÑÁ†îÁ©∂Êèê‰æõÂÑ™ÂåñÊñπÂêë„ÄÇ

##### **DisTrack: a new Tool for Semi-automatic Misinformation Tracking in Online Social Networks**
2408.00633v1 by Guillermo Villar-Rodr√≠guez, √Ålvaro Huertas-Garc√≠a, Alejandro Mart√≠n, Javier Huertas-Tato, David Camacho

Introduction: This article introduces DisTrack, a methodology and a tool
developed for tracking and analyzing misinformation within Online Social
Networks (OSNs). DisTrack is designed to combat the spread of misinformation
through a combination of Natural Language Processing (NLP) Social Network
Analysis (SNA) and graph visualization. The primary goal is to detect
misinformation, track its propagation, identify its sources, and assess the
influence of various actors within the network.
  Methods: DisTrack's architecture incorporates a variety of methodologies
including keyword search, semantic similarity assessments, and graph generation
techniques. These methods collectively facilitate the monitoring of
misinformation, the categorization of content based on alignment with known
false claims, and the visualization of dissemination cascades through detailed
graphs. The tool is tailored to capture and analyze the dynamic nature of
misinformation spread in digital environments.
  Results: The effectiveness of DisTrack is demonstrated through three case
studies focused on different themes: discredit/hate speech, anti-vaccine
misinformation, and false narratives about the Russia-Ukraine conflict. These
studies show DisTrack's capabilities in distinguishing posts that propagate
falsehoods from those that counteract them, and tracing the evolution of
misinformation from its inception.
  Conclusions: The research confirms that DisTrack is a valuable tool in the
field of misinformation analysis. It effectively distinguishes between
different types of misinformation and traces their development over time. By
providing a comprehensive approach to understanding and combating
misinformation in digital spaces, DisTrack proves to be an essential asset for
researchers and practitioners working to mitigate the impact of false
information in online social environments.

ÊëòË¶ÅÔºö<paragraph>ÂºïË®ÄÔºöÊú¨Êñá‰ªãÁ¥π DisTrackÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñπÊ≥ïÂíåÂ∑•ÂÖ∑ÔºåÁî®ÊñºËøΩËπ§ÂíåÂàÜÊûêÁ∑ö‰∏äÁ§æ‰∫§Á∂≤Ë∑ØÔºàOSNÔºâ‰∏≠ÁöÑÈåØË™§Ë≥áË®ä„ÄÇDisTrack ÁöÑË®≠Ë®àÁõÆÁöÑÊòØÈÄèÈÅéÁµêÂêàËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÔºàNLPÔºâ„ÄÅÁ§æ‰∫§Á∂≤Ë∑ØÂàÜÊûêÔºàSNAÔºâÂíåÂúñÂΩ¢Ë¶ñË¶∫Âåñ‰æÜÂ∞çÊäóÈåØË™§Ë≥áË®äÁöÑÊï£Â∏É„ÄÇ‰∏ªË¶ÅÁõÆÊ®ôÊòØÂÅµÊ∏¨ÈåØË™§Ë≥áË®ä„ÄÅËøΩËπ§ÂÖ∂ÂÇ≥Êí≠„ÄÅÊâæÂá∫ÂÖ∂‰æÜÊ∫êÔºå‰∏¶Ë©ï‰º∞Á∂≤Ë∑Ø‰∏≠ÂêÑÂÄãÂèÉËàáËÄÖÁöÑÂΩ±ÈüøÂäõ„ÄÇ
ÊñπÊ≥ïÔºöDisTrack ÁöÑÊû∂ÊßãÁµêÂêà‰∫ÜÂ§öÁ®ÆÊñπÊ≥ïÔºåÂåÖÊã¨ÈóúÈçµÂ≠óÊêúÂ∞ã„ÄÅË™ûÊÑèÁõ∏‰ººÊÄßË©ï‰º∞ÂíåÂúñÂΩ¢Áî¢ÁîüÊäÄË°ì„ÄÇÈÄô‰∫õÊñπÊ≥ïÂÖ±Âêå‰øÉÈÄ≤‰∫ÜÈåØË™§Ë≥áË®äÁöÑÁõ£Êéß„ÄÅÂü∫ÊñºËàáÂ∑≤Áü•ËôõÂÅáË™™Ê≥ïÁöÑÊØîÂ∞ç‰æÜÂàÜÈ°ûÂÖßÂÆπÔºå‰ª•ÂèäÈÄèÈÅéË©≥Á¥∞ÂúñÂΩ¢Ë¶ñË¶∫ÂåñÂÇ≥Êí≠Â±§Áñä„ÄÇÊ≠§Â∑•ÂÖ∑Á∂ìÈÅéÈáèË∫´ÊâìÈÄ†ÔºåÁî®ÊñºÊì∑ÂèñÂíåÂàÜÊûêÊï∏‰ΩçÁí∞Â¢É‰∏≠ÈåØË™§Ë≥áË®äÊï£Â∏ÉÁöÑÂãïÊÖãÁâπÊÄß„ÄÇ
ÁµêÊûúÔºöDisTrack ÁöÑÊïàËÉΩÈÄèÈÅé‰∏âÂÄãÊ°à‰æãÁ†îÁ©∂Áç≤ÂæóÈ©óË≠âÔºåÈÄô‰∫õÁ†îÁ©∂Â∞àÊ≥®Êñº‰∏çÂêåÁöÑ‰∏ªÈ°åÔºöË≤∂‰Ωé/‰ªáÊÅ®Ë®ÄË´ñ„ÄÅÂèçÁñ´ËãóÈåØË™§Ë≥áË®äÔºå‰ª•ÂèäÈóúÊñº‰øÑÁæÖÊñØ-ÁÉèÂÖãËò≠Ë°ùÁ™ÅÁöÑËôõÂÅáÊïòËø∞„ÄÇÈÄô‰∫õÁ†îÁ©∂È°ØÁ§∫Âá∫ DisTrack Âú®ÂçÄÂàÜÂÇ≥Êí≠ËôõÂÅáË≥áË®äÂíåÂèçÂà∂ËôõÂÅáË≥áË®äÁöÑË≤ºÊñáÔºå‰ª•ÂèäËøΩËπ§ÈåØË™§Ë≥áË®äÂæûÂÖ∂ÈñãÁ´ØÊºîËÆäÁöÑÈÅéÁ®ã‰∏≠ÊâÄÂÖ∑ÂÇôÁöÑËÉΩÂäõ„ÄÇ
ÁµêË´ñÔºöÁ†îÁ©∂Ë≠âÂØ¶ DisTrack ÊòØÈåØË™§Ë≥áË®äÂàÜÊûêÈ†òÂüü‰∏≠‰∏ÄÂÄãÊúâÂÉπÂÄºÁöÑÂ∑•ÂÖ∑„ÄÇÂÆÉÊúâÊïàÂçÄÂàÜ‰∫Ü‰∏çÂêåÈ°ûÂûãÁöÑÈåØË™§Ë≥áË®äÔºå‰∏¶ËøΩËπ§ÂÖ∂Èö®ËëóÊôÇÈñìÊé®ÁßªÁöÑÁôºÂ±ï„ÄÇÈÄèÈÅéÊèê‰æõ‰∏ÄÁ®ÆÂÖ®Èù¢ÁöÑÊñπÊ≥ï‰æÜÁêÜËß£ÂíåÂ∞çÊäóÊï∏‰ΩçÁ©∫Èñì‰∏≠ÁöÑÈåØË™§Ë≥áË®äÔºåDisTrack Ë≠âÊòé‰∫ÜËá™Â∑±ÊòØÂçîÂä©Á†îÁ©∂‰∫∫Âì°ÂíåÂØ¶ÂãôÂ∑•‰ΩúËÄÖÊ∏õËºïÁ∑ö‰∏äÁ§æ‰∫§Áí∞Â¢É‰∏≠ËôõÂÅáË≥áË®äÂΩ±ÈüøÂäõÁöÑÈáçË¶ÅË≥áÁî¢„ÄÇ</paragraph>

##### **On the Limitations and Prospects of Machine Unlearning for Generative AI**
2408.00376v1 by Shiji Zhou, Lianzhe Wang, Jiangnan Ye, Yongliang Wu, Heng Chang

Generative AI (GenAI), which aims to synthesize realistic and diverse data
samples from latent variables or other data modalities, has achieved remarkable
results in various domains, such as natural language, images, audio, and
graphs. However, they also pose challenges and risks to data privacy, security,
and ethics. Machine unlearning is the process of removing or weakening the
influence of specific data samples or features from a trained model, without
affecting its performance on other data or tasks. While machine unlearning has
shown significant efficacy in traditional machine learning tasks, it is still
unclear if it could help GenAI become safer and aligned with human desire. To
this end, this position paper provides an in-depth discussion of the machine
unlearning approaches for GenAI. Firstly, we formulate the problem of machine
unlearning tasks on GenAI and introduce the background. Subsequently, we
systematically examine the limitations of machine unlearning on GenAI models by
focusing on the two representative branches: LLMs and image generative
(diffusion) models. Finally, we provide our prospects mainly from three
aspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and
conscientiously advocate for the future development of this field.

ÊëòË¶ÅÔºöÁîüÊàêÂºè AI (GenAI) Êó®Âú®ÂæûÊΩõÂú®ËÆäÊï∏ÊàñÂÖ∂‰ªñË≥áÊñôÊ®°Âºè‰∏≠ÂêàÊàêÈÄºÁúü‰∏îÂ§öÊ®£ÂåñÁöÑË≥áÊñôÁØÑ‰æãÔºåÂ∑≤Âú®Ëá™ÁÑ∂Ë™ûË®Ä„ÄÅÂΩ±ÂÉè„ÄÅÈü≥Ë®äÂíåÂúñÂΩ¢Á≠âÂêÑÁ®ÆÈ†òÂüü‰∏≠ÂèñÂæóÈ°ØËëóÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰πüÂ∞çË≥áÊñôÈö±ÁßÅ„ÄÅÂÆâÂÖ®ÊÄßËàáÈÅìÂæ∑ÊßãÊàêÊåëÊà∞ÂíåÈ¢®Èö™„ÄÇÊ©üÂô®ÈÅ∫ÂøòÊòØÁßªÈô§ÊàñÊ∏õÂº±ÁâπÂÆöË≥áÊñôÁØÑ‰æãÊàñÁâπÂæµÂ∞çÂ∑≤Ë®ìÁ∑¥Ê®°ÂûãÁöÑÂΩ±ÈüøÔºåÂêåÊôÇ‰∏çÂΩ±ÈüøÂÖ∂Âú®ÂÖ∂‰ªñË≥áÊñôÊàñ‰ªªÂãô‰∏äÁöÑÊïàËÉΩ„ÄÇÈõñÁÑ∂Ê©üÂô®ÈÅ∫ÂøòÂ∑≤Âú®ÂÇ≥Áµ±Ê©üÂô®Â≠∏Áøí‰ªªÂãô‰∏≠Â±ïÁèæÈ°ØËëóÁöÑÂäüÊïàÔºå‰ΩÜ‰ªç‰∏çÊ∏ÖÊ•öÂÆÉÊòØÂê¶ËÉΩÂçîÂä© GenAI ËÆäÂæóÊõ¥ÂÆâÂÖ®‰∏îÁ¨¶Âêà‰∫∫È°ûÁöÑÊúüÊúõ„ÄÇÁÇ∫Ê≠§ÔºåÊú¨Á´ãÂ†¥Êñá‰ª∂Ê∑±ÂÖ•Êé¢Ë®é‰∫Ü GenAI ÁöÑÊ©üÂô®ÈÅ∫ÂøòÊñπÊ≥ï„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂà∂ÂÆö GenAI ‰∏äÊ©üÂô®ÈÅ∫Âøò‰ªªÂãôÁöÑÂïèÈ°åÔºå‰∏¶‰ªãÁ¥πËÉåÊôØ„ÄÇÊé•ËëóÔºåÊàëÂÄëÊúâÁ≥ªÁµ±Âú∞Ê™¢Ë¶ñÊ©üÂô®ÈÅ∫ÂøòÂú® GenAI Ê®°Âûã‰∏äÁöÑÈôêÂà∂ÔºåÈáçÈªûÊîæÂú®ÂÖ©ÂÄã‰ª£Ë°®ÊÄßÁöÑÂàÜÊîØÔºöLLM ÂíåÂΩ±ÂÉèÁîüÊàêÔºàÊì¥Êï£ÔºâÊ®°Âûã„ÄÇÊúÄÂæåÔºåÊàëÂÄë‰∏ªË¶ÅÂæûÂü∫Ê∫ñ„ÄÅË©ï‰º∞ÊåáÊ®ôÂíåÊïàÁî®ÈÅ∫ÂøòÊ¨äË°°‰∏âÂÄãÈù¢ÂêëÊèê‰æõÊàëÂÄëÁöÑÂ±ïÊúõÔºå‰∏¶ÂØ©ÊÖéÂÄ°Ë≠∞Ë©≤È†òÂüüÁöÑÊú™‰æÜÁôºÂ±ï„ÄÇ

##### **Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network**
2408.00290v1 by Bin Cheng, Jiaxuan Lu

With the advent of the era of foundation models, pre-training and fine-tuning
have become common paradigms. Recently, parameter-efficient fine-tuning has
garnered widespread attention due to its better balance between the number of
learnable parameters and performance. However, some current parameter-efficient
fine-tuning methods only model a single modality and lack the utilization of
structural knowledge in downstream tasks. To address this issue, this paper
proposes a multi-modal parameter-efficient fine-tuning method based on graph
networks. Each image is fed into a multi-modal large language model (MLLM) to
generate a text description. The image and its corresponding text description
are then processed by a frozen image encoder and text encoder to generate image
features and text features, respectively. A graph is constructed based on the
similarity of the multi-modal feature nodes, and knowledge and relationships
relevant to these features are extracted from each node. Additionally, Elastic
Weight Consolidation (EWC) regularization is incorporated into the loss
function to mitigate the problem of forgetting during task learning. The
proposed model achieves test accuracies on the OxfordPets, Flowers102, and
Food101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The
code is available at https://github.com/yunche0/GA-Net/tree/master.

ÊëòË¶ÅÔºöÈö®ËëóÂü∫Á§éÊ®°ÂûãÊôÇ‰ª£ÁöÑÂà∞‰æÜÔºåÈ†êË®ìÁ∑¥ÂíåÂæÆË™øÂ∑≤ÊàêÁÇ∫Â∏∏Ë¶ãÁöÑÁØÑ‰æã„ÄÇÊúÄËøëÔºåÁî±ÊñºÂèÉÊï∏ÊúâÊïàÂæÆË™øÂú®ÂèØÂ≠∏ÁøíÂèÉÊï∏Êï∏ÈáèÂíåÊïàËÉΩ‰πãÈñìÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°ÔºåÂõ†Ê≠§ÂÇôÂèóÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºå‰∏Ä‰∫õÁõÆÂâçÁöÑÂèÉÊï∏ÊúâÊïàÂæÆË™øÊñπÊ≥ïÂÉÖÂª∫Ê®°ÂñÆ‰∏ÄÊ®°ÊÖãÔºå‰∏îÁº∫‰πèÂú®‰∏ãÊ∏∏‰ªªÂãô‰∏≠Âà©Áî®ÁµêÊßãÁü•Ë≠ò„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂúñÂΩ¢Á∂≤Ë∑ØÁöÑÂ§öÊ®°ÊÖãÂèÉÊï∏ÊúâÊïàÂæÆË™øÊñπÊ≥ï„ÄÇÊØèÂÄãÂΩ±ÂÉèÈÉΩÊúÉËº∏ÂÖ•Âà∞Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ‰∏≠Ôºå‰ª•Áî¢ÁîüÊñáÂ≠óÊèèËø∞„ÄÇÁÑ∂ÂæåÔºåÂΩ±ÂÉèÂèäÂÖ∂Â∞çÊáâÁöÑÊñáÂ≠óÊèèËø∞ÊúÉÁî±ÂáçÁµêÁöÑÂΩ±ÂÉèÁ∑®Á¢ºÂô®ÂíåÊñáÂ≠óÁ∑®Á¢ºÂô®ËôïÁêÜÔºåÂàÜÂà•Áî¢ÁîüÂΩ±ÂÉèÁâπÂæµÂíåÊñáÂ≠óÁâπÂæµ„ÄÇÊ†πÊìöÂ§öÊ®°ÊÖãÁâπÂæµÁØÄÈªûÁöÑÁõ∏‰ººÊÄßÂª∫Êßã‰∏ÄÂÄãÂúñÂΩ¢Ôºå‰∏¶ÂæûÊØèÂÄãÁØÄÈªû‰∏≠ËêÉÂèñÂá∫ËàáÈÄô‰∫õÁâπÂæµÁõ∏ÈóúÁöÑÁü•Ë≠òÂíåÈóú‰øÇ„ÄÇÊ≠§Â§ñÔºåÂΩàÊÄßÊ¨äÈáçÊï¥Âêà (EWC) Ê≠£ÂâáÂåñÊúÉÁ¥çÂÖ•ÊêçÂ§±ÂáΩÊï∏‰∏≠Ôºå‰ª•Ê∏õËºïÂú®‰ªªÂãôÂ≠∏ÁøíÊúüÈñìÈÅ∫ÂøòÁöÑÂïèÈ°å„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂú® OxfordPets„ÄÅFlowers102 Âíå Food101 Ë≥áÊñôÈõÜ‰∏äÈÅîÊàêÁöÑÊ∏¨Ë©¶Ê∫ñÁ¢∫Â∫¶ÂàÜÂà•ÊèêÂçá‰∫Ü 4.45%„ÄÅ2.92% Âíå 0.23%„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/yunche0/GA-Net/tree/master ÂèñÂæó„ÄÇ

##### **CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**
2407.21708v1 by Stefan Langer, Fabian Neuhaus, Andreas N√ºrnberger

Ontologies are formal representations of knowledge in specific domains that
provide a structured framework for organizing and understanding complex
information. Creating ontologies, however, is a complex and time-consuming
endeavor. ChEBI is a well-known ontology in the field of chemistry, which
provides a comprehensive resource for defining chemical entities and their
properties. However, it covers only a small fraction of the rapidly growing
knowledge in chemistry and does not provide references to the scientific
literature. To address this, we propose a methodology that involves augmenting
existing annotated text corpora with knowledge from Chebi and fine-tuning a
large language model (LLM) to recognize chemical entities and their roles in
scientific text. Our experiments demonstrate the effectiveness of our approach.
By combining ontological knowledge and the language understanding capabilities
of LLMs, we achieve high precision and recall rates in identifying both the
chemical entities and roles in scientific literature. Furthermore, we extract
them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a
knowledge graph (KG) of chemical entities and roles (CEAR), which provides
complementary information to ChEBI, and can help to extend it.

ÊëòË¶ÅÔºöÊú¨‰ΩìÊòØÁâπÂÆöÈ†òÂüü‰∏≠Áü•Ë≠òÁöÑÂΩ¢ÂºèÂåñË°®Á§∫ÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÊ°ÜÊû∂ÔºåÁî®ÊñºÁµÑÁπîÂíåÁêÜËß£Ë§áÈõúÁöÑË≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÂª∫Á´ãÊú¨‰ΩìÊòØ‰∏ÄÈ†ÖË§áÈõú‰∏îËÄóÊôÇÁöÑÂä™Âäõ„ÄÇChEBI ÊòØÂåñÂ≠∏È†òÂüü‰∏≠‰∏ÄÂÄãËëóÂêçÁöÑÊú¨‰ΩìÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑË≥áÊ∫êÔºåÁî®ÊñºÂÆöÁæ©ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Â±¨ÊÄß„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÉÖÊ∂µËìã‰∫ÜÂåñÂ≠∏È†òÂüüÂø´ÈÄüÂ¢ûÈï∑ÁöÑÁü•Ë≠ò‰∏≠ÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜÔºå‰∏¶‰∏îÊ≤íÊúâÊèê‰æõÁßëÂ≠∏ÊñáÁçªÁöÑÂèÉËÄÉ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂÆÉÊ∂âÂèä‰ΩøÁî®‰æÜËá™ Chebi ÁöÑÁü•Ë≠òÊì¥ÂÖÖÁèæÊúâÁöÑË®ªÈáãÊñáÊú¨Ë™ûÊñôÂ∫´Ôºå‰∏¶ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•Ë≠òÂà•ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Âú®ÁßëÂ≠∏ÊñáÊú¨‰∏≠ÁöÑ‰ΩúÁî®„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÈÄèÈÅéÁµêÂêàÊú¨‰ΩìÁü•Ë≠òÂíå LLM ÁöÑË™ûË®ÄÁêÜËß£ËÉΩÂäõÔºåÊàëÂÄëÂú®Ë≠òÂà•ÁßëÂ≠∏ÊñáÁçª‰∏≠ÁöÑÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî®ÊñπÈù¢ÈÅîÂà∞‰∫ÜÂæàÈ´òÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂè¨ÂõûÁéá„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæû‰∏ÄÁµÑ 8,000 ÁØá ChemRxiv ÊñáÁ´†‰∏≠ÊèêÂèñÂÆÉÂÄëÔºå‰∏¶ÊáâÁî®Á¨¨‰∫åÂÄã LLM ‰æÜÂª∫Á´ã‰∏ÄÂÄãÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî® (CEAR) ÁöÑÁü•Ë≠òÂúñË≠ú (KG)ÔºåÂÆÉÊèê‰æõË£úÂÖÖ ChEBI ÁöÑË≥áË®äÔºå‰∏¶ÊúâÂä©ÊñºÊì¥ÂÖÖÂÆÉ„ÄÇ

##### **eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**
2407.21483v3 by Xinyi Pan, Daniel Hern√°ndez, Philipp Seifer, Ralf L√§mmel, Steffen Staab

Over the past few years, we have seen the emergence of large knowledge graphs
combining information from multiple sources. Sometimes, this information is
provided in the form of assertions about other assertions, defining contexts
where assertions are valid. A recent extension to RDF which admits statements
over statements, called RDF-star, is in revision to become a W3C standard.
However, there is no proposal for a semantics of these RDF-star statements nor
a built-in facility to operate over them. In this paper, we propose a query
language for epistemic RDF-star metadata based on a four-valued logic, called
eSPARQL. Our proposed query language extends SPARQL-star, the query language
for RDF-star, with a new type of FROM clause to facilitate operating with
multiple and sometimes conflicting beliefs. We show that the proposed query
language can express four use case queries, including the following features:
(i) querying the belief of an individual, (ii) the aggregating of beliefs,
(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs
(i.e., nesting of beliefs).

ÊëòË¶ÅÔºöÂú®ÈÅéÂéªÂπæÂπ¥ÔºåÊàëÂÄëË¶ãË≠â‰∫ÜÂ§ßÂûãÁü•Ë≠òÂúñË≠úÁöÑÂá∫ÁèæÔºåÁµêÂêà‰æÜËá™Â§öÂÄã‰æÜÊ∫êÁöÑË≥áË®ä„ÄÇÊúâÊôÇÔºåÈÄô‰∫õË≥áË®äÊúÉ‰ª•Â∞çÂÖ∂‰ªñÊñ∑Ë®ÄÁöÑÊñ∑Ë®ÄÂΩ¢ÂºèÊèê‰æõÔºåÂÆöÁæ©Êñ∑Ë®ÄÊúâÊïàÁöÑËÑàÁµ°„ÄÇÊúÄËøëÂ∞ç RDF ÁöÑÊì¥ÂÖÖÔºåÂÖÅË®±Â∞çÈô≥Ëø∞ÈÄ≤Ë°åÈô≥Ëø∞ÔºåÁ®±ÁÇ∫ RDF-starÔºåÊ≠£Âú®‰øÆË®ÇÁÇ∫ W3C Ê®ôÊ∫ñ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÊ≤íÊúâÈáùÂ∞çÈÄô‰∫õ RDF-star Èô≥Ëø∞ÁöÑË™ûÊÑèÂª∫Ë≠∞Ôºå‰πüÊ≤íÊúâÂÖßÂª∫ÁöÑÈÅã‰ΩúÂäüËÉΩ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂõõÂÄºÈÇèËºØÁöÑÁü•Ë≠ò RDF-star ÂÖÉË≥áÊñôÊü•Ë©¢Ë™ûË®ÄÔºåÁ®±ÁÇ∫ eSPARQL„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÊì¥ÂÖÖ‰∫Ü RDF-star ÁöÑÊü•Ë©¢Ë™ûË®Ä SPARQL-starÔºåÊñ∞Â¢û‰∏ÄÁ®Æ FROM Â≠êÂè•È°ûÂûãÔºå‰ª•Âà©Êñº‰ΩøÁî®Â§öÈáç‰∏îÊúâÊôÇÁõ∏‰∫íË°ùÁ™ÅÁöÑ‰ø°ÂøµÈÄ≤Ë°åÈÅã‰Ωú„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊâÄÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÂèØ‰ª•Ë°®ÈÅîÂõõÁ®Æ‰ΩøÁî®Ê°à‰æãÊü•Ë©¢ÔºåÂåÖÊã¨‰ª•‰∏ãÂäüËÉΩÔºö(i) Êü•Ë©¢ÂÄã‰∫∫ÁöÑ‰ø°ÂøµÔºå(ii) ÂΩôÁ∏Ω‰ø°ÂøµÔºå(iii) Êü•Ë©¢ËàáÊüê‰∫∫Ë°ùÁ™ÅÁöÑÊòØË™∞Ôºå‰ª•Âèä (iv) ÈóúÊñº‰ø°ÂøµÁöÑ‰ø°ÂøµÔºàÂç≥‰ø°ÂøµÁöÑÂ∑¢ÁãÄÔºâ„ÄÇ

##### **Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**
2407.21452v1 by Haodong Hong, Sen Wang, Zi Huang, Qi Wu, Jiajun Liu

Real-world navigation often involves dealing with unexpected obstructions
such as closed doors, moved objects, and unpredictable entities. However,
mainstream Vision-and-Language Navigation (VLN) tasks typically assume
instructions perfectly align with the fixed and predefined navigation graphs
without any obstructions. This assumption overlooks potential discrepancies in
actual navigation graphs and given instructions, which can cause major failures
for both indoor and outdoor agents. To address this issue, we integrate diverse
obstructions into the R2R dataset by modifying both the navigation graphs and
visual observations, introducing an innovative dataset and task, R2R with
UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers
of path obstructions to generate instruction-reality mismatches for VLN
research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods
inevitably encounter significant challenges when facing such mismatches,
indicating that they rigidly follow instructions rather than navigate
adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),
which includes a curriculum training strategy and virtual graph construction to
help agents effectively adapt to obstructed environments. Empirical results
show that ObVLN not only maintains robust performance in unobstructed scenarios
but also achieves a substantial performance advantage with unexpected
obstructions.

ÊëòË¶ÅÔºöÁé∞ÂÆû‰∏ñÁïåÁöÑÂØºËà™ÈÄöÂ∏∏Ê∂âÂèäÂ§ÑÁêÜÊÑèÂ§ñÁöÑÈöúÁ¢çÔºå‰æãÂ¶ÇÂÖ≥ÁùÄÁöÑÈó®„ÄÅÁßªÂä®ÁöÑÁâ©‰ΩìÂíå‰∏çÂèØÈ¢ÑÊµãÁöÑÂÆû‰Ωì„ÄÇÁÑ∂ËÄåÔºå‰∏ªÊµÅÁöÑËßÜËßâÂíåËØ≠Ë®ÄÂØºËà™ (VLN) ‰ªªÂä°ÈÄöÂ∏∏ÂÅáËÆæÊåá‰ª§‰∏éÂõ∫ÂÆöÁöÑÂíåÈ¢ÑÂÆö‰πâÁöÑÂØºËà™ÂõæÂÆåÂÖ®‰∏ÄËá¥ÔºåÊ≤°Êúâ‰ªª‰ΩïÈöúÁ¢ç„ÄÇËøôÁßçÂÅáËÆæÂøΩÁï•‰∫ÜÂÆûÈôÖÂØºËà™ÂõæÂíåÁªôÂÆöÊåá‰ª§‰∏≠ÊΩúÂú®ÁöÑÂ∑ÆÂºÇÔºåËøôÂèØËÉΩ‰ºöÂØºËá¥ÂÆ§ÂÜÖÂíåÂÆ§Â§ñ‰ª£ÁêÜÂá∫Áé∞ÈáçÂ§ßÊïÖÈöú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÈÄöËøá‰øÆÊîπÂØºËà™ÂõæÂíåËßÜËßâËßÇÂØüÔºåÂ∞ÜÂêÑÁßçÈöúÁ¢çÊï¥ÂêàÂà∞ R2R Êï∞ÊçÆÈõÜ‰∏≠ÔºåÂºïÂÖ•‰∫ÜÂàõÊñ∞Êï∞ÊçÆÈõÜÂíå‰ªªÂä°ÔºåÂç≥Â∏¶ÊúâÊÑèÂ§ñÈöúÁ¢çÁöÑ R2R (R2R-UNO)„ÄÇR2R-UNO ÂåÖÂê´ÂêÑÁßçÁ±ªÂûãÂíåÊï∞ÈáèÁöÑË∑ØÂæÑÈöúÁ¢çÔºå‰ª•ÁîüÊàê VLN Á†îÁ©∂ÁöÑÊåá‰ª§-Áé∞ÂÆû‰∏çÂåπÈÖç„ÄÇÂú® R2R-UNO ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊúÄÂÖàËøõÁöÑ VLN ÊñπÊ≥ïÂú®Èù¢ÂØπÊ≠§Á±ª‰∏çÂåπÈÖçÊó∂‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÈÅáÂà∞ÈáçÂ§ßÊåëÊàòÔºåËøôË°®ÊòéÂÆÉ‰ª¨‰∏•Ê†ºÈÅµÂæ™Êåá‰ª§ÔºåËÄå‰∏çÊòØËá™ÈÄÇÂ∫îÂú∞ÂØºËà™„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁß∞‰∏∫ ObVLNÔºàÂèóÈòª VLNÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÂåÖÊã¨ËØæÁ®ãËÆ≠ÁªÉÁ≠ñÁï•ÂíåËôöÊãüÂõæÊûÑÂª∫Ôºå‰ª•Â∏ÆÂä©‰ª£ÁêÜÊúâÊïàÂú∞ÈÄÇÂ∫îÂèóÈòªÁéØÂ¢É„ÄÇÁªèÈ™åÁªìÊûúË°®ÊòéÔºåObVLN ‰∏ç‰ªÖÂú®Êó†ÈöúÁ¢çÂú∫ÊôØ‰∏≠‰øùÊåÅ‰∫ÜÁ®≥ÂÅ•ÁöÑÊÄßËÉΩÔºåËÄå‰∏îÂú®ÊÑèÂ§ñÈöúÁ¢ç‰∏≠‰πüËé∑Âæó‰∫ÜÂÆûË¥®ÊÄßÁöÑÊÄßËÉΩ‰ºòÂäø„ÄÇ

##### **Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**
2407.21358v1 by Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan

Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing
reliable, structured, domain-specific, and up-to-date external knowledge.
However, KGs and LLMs are often developed separately and must be integrated
after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning
algorithm that enables augmentation of black-box LLMs with one or more KGs. The
algorithm equips a LLM with actions for interfacing a KG and enables the LLM to
perform tree search over possible thoughts and actions to find high confidence
reasoning paths. We evaluate on two popular benchmark datasets. Our results
show that Tree-of-Traversals significantly improves performance on question
answering and KG question answering tasks. Code is available at
\url{https://github.com/amazon-science/tree-of-traversals}

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ÈÄèÈÅéÊèê‰æõÂèØÈù†„ÄÅÁµêÊßãÂåñ„ÄÅÁâπÂÆöÊñºÈ†òÂüü‰∏îÊúÄÊñ∞ÁöÑÂ§ñÈÉ®Áü•Ë≠òÔºå‰æÜË£úÂÖÖÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇ
ÁÑ∂ËÄåÔºåKG Âíå LLM ÈÄöÂ∏∏ÊòØÂàÜÈñãÈñãÁôºÔºå‰∏¶‰∏îÂøÖÈ†àÂú®Ë®ìÁ∑¥ÂæåÊï¥Âêà„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü Tree-of-TraversalsÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈõ∂Ê¨°Êé®ÁêÜÊºîÁÆóÊ≥ïÔºåÂÆÉËÉΩËÆìÈªëÁõí LLM ‰ΩøÁî®‰∏ÄÂÄãÊàñÂ§öÂÄã KG„ÄÇË©≤ÊºîÁÆóÊ≥ïÁÇ∫ LLM Êèê‰æõËàá KG ‰ªãÈù¢ÁöÑÂãï‰ΩúÔºå‰∏¶ËÆì LLM ËÉΩÂú®ÂèØËÉΩÁöÑÊÄùËÄÉÂíåÂãï‰Ωú‰∏äÂü∑Ë°åÊ®πÁãÄÊêúÂ∞ãÔºå‰ª•ÊâæÂá∫È´òÂ∫¶‰ø°ÂøÉÁöÑÊé®ÁêÜË∑ØÂæë„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÁÜ±ÈñÄÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåTree-of-Traversals Â§ßÂπÖÊèêÂçá‰∫ÜÂïèÈ°åËß£Á≠îÂíå KG ÂïèÈ°åËß£Á≠î‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® \url{https://github.com/amazon-science/tree-of-traversals} ÂèñÂæó

##### **SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**
2407.21293v1 by Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, Shaohua Wu

Many fields could benefit from the rapid development of the large language
models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the
typically fields facing new opportunities as the LLMs have supported more and
more modalities. Here, by utilizing vision-language model (VLM), we proposed an
e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided
into four stages, which are perception, prediction, planning, and behavior.
Each stage consists of several visual question answering (VQA) pairs and VQA
pairs interconnect with each other constructing a graph called Graph VQA
(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our
method could achieve e2e driving with language. In our method, vision
transformers (ViT) models are employed to process nuScenes visual data, while
VLM are utilized to interpret and reason about the information extracted from
the visual inputs. In the perception stage, the system identifies and
classifies objects from the driving environment. The prediction stage involves
forecasting the potential movements of these objects. The planning stage
utilizes the gathered information to develop a driving strategy, ensuring the
safety and efficiency of the autonomous vehicle. Finally, the behavior stage
translates the planned actions into executable commands for the vehicle. Our
experiments demonstrate that SimpleLLM4AD achieves competitive performance in
complex driving scenarios.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÁôºÂ±ïÂèØËÉΩ‰ΩøË®±Â§öÈ†òÂüüÂèóÁõä„ÄÇÁ´ØÂà∞Á´ØËá™ÂãïÈßïÈßõ (e2eAD) ÊòØÂÖ∏ÂûãÈ†òÂüü‰πã‰∏ÄÔºåÂõ†ÁÇ∫ LLM ÊîØÊè¥Ë∂ä‰æÜË∂äÂ§öÁöÑÊ®°ÂºèÔºåÂõ†Ê≠§Èù¢Ëá®Êñ∞ÁöÑÊ©üÊúÉ„ÄÇÂú®Ê≠§ÔºåÈÄèÈÅéÂà©Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ SimpleLLM4AD ÁöÑ e2eAD ÊñπÊ≥ï„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠Ôºåe2eAD ‰ªªÂãôÂàÜÁÇ∫ÂõõÂÄãÈöéÊÆµÔºåÂàÜÂà•ÊòØÊÑüÁü•„ÄÅÈ†êÊ∏¨„ÄÅË¶èÂäÉÂíåË°åÁÇ∫„ÄÇÊØèÂÄãÈöéÊÆµÂåÖÂê´Â§öÂÄãË¶ñË¶∫ÂïèÁ≠î (VQA) ÈÖçÂ∞çÔºå‰∏î VQA ÈÖçÂ∞çÁõ∏‰∫íÈÄ£Êé•ÔºåÊßãÂª∫‰∏ÄÂÄãÁ®±ÁÇ∫ÂúñÂΩ¢ VQA (GVQA) ÁöÑÂúñÂΩ¢„ÄÇÈÄèÈÅé VLM ÂàÜÈöéÊÆµÊé®ÁêÜ GVQA ‰∏≠ÁöÑÊØèÂÄã VQA ÈÖçÂ∞çÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•ÈÄèÈÅéË™ûË®ÄÂØ¶ÁèæÁ´ØÂà∞Á´ØÈßïÈßõ„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠ÔºåÊé°Áî®Ë¶ñË¶∫Transformer (ViT) Ê®°Âûã‰æÜËôïÁêÜ nuScenes Ë¶ñË¶∫Ë≥áÊñôÔºåÂêåÊôÇÂà©Áî® VLM ‰æÜË©ÆÈáãÂíåÊé®ÁêÜÂæûË¶ñË¶∫Ëº∏ÂÖ•‰∏≠ÊèêÂèñÁöÑË≥áË®ä„ÄÇÂú®ÊÑüÁü•ÈöéÊÆµÔºåÁ≥ªÁµ±Ë≠òÂà•ÂíåÂàÜÈ°ûÈßïÈßõÁí∞Â¢É‰∏≠ÁöÑÁâ©‰ª∂„ÄÇÈ†êÊ∏¨ÈöéÊÆµÊ∂âÂèäÈ†êÊ∏¨ÈÄô‰∫õÁâ©‰ª∂ÁöÑÊΩõÂú®ÁßªÂãï„ÄÇË¶èÂäÉÈöéÊÆµÂà©Áî®Êî∂ÈõÜÁöÑË≥áË®ä‰æÜÂà∂ÂÆöÈßïÈßõÁ≠ñÁï•ÔºåÁ¢∫‰øùËá™ÂãïÈßïÈßõÊ±ΩËªäÁöÑÂÆâÂÖ®ÊÄßÂíåÊïàÁéá„ÄÇÊúÄÂæåÔºåË°åÁÇ∫ÈöéÊÆµÂ∞áË¶èÂäÉÁöÑÂãï‰ΩúËΩâÊèõÁÇ∫ËªäËºõÂèØÂü∑Ë°åÁöÑÂëΩ‰ª§„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåSimpleLLM4AD Âú®Ë§áÈõúÁöÑÈßïÈßõÂ†¥ÊôØ‰∏≠ÂØ¶Áèæ‰∫ÜÁ´∂Áà≠Âäõ„ÄÇ

##### **Be aware of overfitting by hyperparameter optimization!**
2407.20786v1 by Igor V. Tetko, Ruud van Deursen, Guillaume Godin

Hyperparameter optimization is very frequently employed in machine learning.
However, an optimization of a large space of parameters could result in
overfitting of models. In recent studies on solubility prediction the authors
collected seven thermodynamic and kinetic solubility datasets from different
data sources. They used state-of-the-art graph-based methods and compared
models developed for each dataset using different data cleaning protocols and
hyperparameter optimization. In our study we showed that hyperparameter
optimization did not always result in better models, possibly due to
overfitting when using the same statistical measures. Similar results could be
calculated using pre-set hyperparameters, reducing the computational effort by
around 10,000 times. We also extended the previous analysis by adding a
representation learning method based on Natural Language Processing of smiles
called Transformer CNN. We show that across all analyzed sets using exactly the
same protocol, Transformer CNN provided better results than graph-based methods
for 26 out of 28 pairwise comparisons by using only a tiny fraction of time as
compared to other methods. Last but not least we stressed the importance of
comparing calculation results using exactly the same statistical measures.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏Áøí‰∏≠ÈùûÂ∏∏È†ªÁπÅÂú∞‰ΩøÁî®Ë∂ÖÂèÉÊï∏ÊúÄ‰Ω≥Âåñ„ÄÇ
ÁÑ∂ËÄåÔºåÂ∞çÂ§ßÂèÉÊï∏Á©∫ÈñìÈÄ≤Ë°åÊúÄ‰Ω≥ÂåñÂèØËÉΩÊúÉÂ∞éËá¥Ê®°ÂûãÈÅéÊì¨Âêà„ÄÇÂú®ÊúÄËøëÂ∞çÊ∫∂Ëß£Â∫¶È†êÊ∏¨ÁöÑÁ†îÁ©∂‰∏≠Ôºå‰ΩúËÄÖÂæû‰∏çÂêåÁöÑÊï∏ÊìöÊ∫êÊî∂ÈõÜ‰∫Ü‰∏ÉÂÄãÁÜ±ÂäõÂ≠∏ÂíåÂãïÂäõÂ≠∏Ê∫∂Ëß£Â∫¶Êï∏ÊìöÈõÜ„ÄÇ‰ªñÂÄë‰ΩøÁî®‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÂúñÂΩ¢ÁöÑÊñπÊ≥ïÔºå‰∏¶ÊØîËºÉ‰∫Ü‰ΩøÁî®‰∏çÂêåÁöÑÊï∏ÊìöÊ∏ÖÊ¥óÂçîË≠∞ÂíåË∂ÖÂèÉÊï∏ÊúÄ‰Ω≥ÂåñÁÇ∫ÊØèÂÄãÊï∏ÊìöÈõÜÈñãÁôºÁöÑÊ®°Âûã„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëË°®ÊòéË∂ÖÂèÉÊï∏ÊúÄ‰Ω≥Âåñ‰∏¶ÈùûÁ∏ΩÊòØÊúÉÁî¢ÁîüÊõ¥Â•ΩÁöÑÊ®°ÂûãÔºåÈÄôÂèØËÉΩÊòØÁî±ÊñºÂú®‰ΩøÁî®Áõ∏ÂêåÁöÑÁµ±Ë®àÊ∏¨ÈáèÊôÇÁôºÁîüÈÅéÊì¨Âêà„ÄÇÂèØ‰ª•‰ΩøÁî®È†êË®≠ÁöÑË∂ÖÂèÉÊï∏Ë®àÁÆóÈ°û‰ººÁöÑÁµêÊûúÔºåÂæûËÄåÂ∞áË®àÁÆóÂ∑•‰ΩúÈáèÊ∏õÂ∞ëÁ¥Ñ 10,000 ÂÄç„ÄÇÊàëÂÄëÈÇÑÈÄöÈÅéÊ∑ªÂä†Âü∫ÊñºÁ¨ëÂÆπÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑË°®Á§∫Â≠∏ÁøíÊñπÊ≥ïÔºàÁ®±ÁÇ∫ Transformer CNNÔºâ‰æÜÊì¥Â±ïÂÖàÂâçÁöÑÂàÜÊûê„ÄÇÊàëÂÄëË°®ÊòéÔºåÂú®‰ΩøÁî®ÂÆåÂÖ®Áõ∏ÂêåÁöÑÂçîË≠∞Â∞çÊâÄÊúâÂàÜÊûêÁöÑÈõÜÂêàÈÄ≤Ë°åÂàÜÊûêÊôÇÔºåTransformer CNN Âú® 28 ÂÄãÊàêÂ∞çÊØîËºÉ‰∏≠Êúâ 26 ÂÄãÊØîËºÉÊØîÂü∫ÊñºÂúñÂΩ¢ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÊõ¥Â•ΩÁöÑÁµêÊûúÔºåËÄåËàáÂÖ∂‰ªñÊñπÊ≥ïÁõ∏ÊØîÔºåÊâÄÁî®ÁöÑÊôÇÈñìÂè™ÊòØÂæàÂ∞èÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÊúÄÂæå‰ΩÜ‰∏¶ÈùûÊúÄ‰∏çÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÂº∑Ë™ø‰∫Ü‰ΩøÁî®ÂÆåÂÖ®Áõ∏ÂêåÁöÑÁµ±Ë®àÊ∏¨Èáè‰æÜÊØîËºÉË®àÁÆóÁµêÊûúÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **Harvesting Textual and Structured Data from the HAL Publication Repository**
2407.20595v1 by Francis Kulumba, Wissam Antoun, Guillaume Vimont, Laurent Romary

HAL (Hyper Articles en Ligne) is the French national publication repository,
used by most higher education and research organizations for their open science
policy. As a digital library, it is a rich repository of scholarly documents,
but its potential for advanced research has been underutilized. We present
HALvest, a unique dataset that bridges the gap between citation networks and
the full text of papers submitted on HAL. We craft our dataset by filtering HAL
for scholarly publications, resulting in approximately 700,000 documents,
spanning 34 languages across 13 identified domains, suitable for language model
training, and yielding approximately 16.5 billion tokens (with 8 billion in
French and 7 billion in English, the most represented languages). We transform
the metadata of each paper into a citation network, producing a directed
heterogeneous graph. This graph includes uniquely identified authors on HAL, as
well as all open submitted papers, and their citations. We provide a baseline
for authorship attribution using the dataset, implement a range of
state-of-the-art models in graph representation learning for link prediction,
and discuss the usefulness of our generated knowledge graph structure.

ÊëòË¶ÅÔºöHALÔºàÁ∑ö‰∏äË∂ÖÈÄ£ÁµêÊñáÁ´†ÔºâÊòØÊ≥ïÂúãÂúãÂÆ∂Âá∫ÁâàÁâ©Ë≥áÊñôÂ∫´Ôºå
Â§ßÂ§öÊï∏È´òÁ≠âÊïôËÇ≤ÂíåÁ†îÁ©∂ÁµÑÁπîÈÉΩ‰ΩøÁî®ÂÆÉ‰æÜÂà∂ÂÆöÈñãÊîæÁßëÂ≠∏
ÊîøÁ≠ñ„ÄÇ‰ΩúÁÇ∫‰∏ÄÂÄãÊï∏‰ΩçÂúñÊõ∏È§®ÔºåÂÆÉÊòØ‰∏ÄÂÄãË±êÂØåÁöÑÂ≠∏Ë°ìÊñá‰ª∂Ë≥áÊñôÂ∫´Ôºå
‰ΩÜÂÆÉÂú®ÈÄ≤ÈöéÁ†îÁ©∂ÁöÑÊΩõÂäõÂ∞öÊú™Ë¢´ÂÖÖÂàÜÂà©Áî®„ÄÇÊàëÂÄëÊèêÂá∫
HALvestÔºå‰∏ÄÂÄãÁç®ÁâπÁöÑË≥áÊñôÈõÜÔºåÂÆÉÂΩåË£ú‰∫ÜÂºïÊñáÁ∂≤Ë∑ØÂíå
Âú® HAL ‰∏äÊèê‰∫§ÁöÑË´ñÊñáÂÖ®Êñá‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇÊàëÂÄëÈÄèÈÅéÁØ©ÈÅ∏ HAL
‰∏≠ÁöÑÂ≠∏Ë°ìÂá∫ÁâàÂìÅ‰æÜÂª∫Á´ãÊàëÂÄëÁöÑË≥áÊñôÈõÜÔºåÊúÄÂæåÂæóÂà∞Á¥Ñ 70 Ëê¨‰ªΩÊñá‰ª∂Ôºå
Ê∂µËìã 13 ÂÄãÂ∑≤Ë≠òÂà•È†òÂüüÁöÑ 34 Á®ÆË™ûË®ÄÔºåÈÅ©ÂêàË™ûË®ÄÊ®°Âûã
Ë®ìÁ∑¥Ôºå‰∏¶Áî¢ÁîüÁ¥Ñ 165 ÂÑÑÂÄãË©ûÂΩôÔºàÂÖ∂‰∏≠Ê≥ïÊñáÊúâ 80 ÂÑÑÂÄãÔºå
Ëã±ÊñáÊúâ 70 ÂÑÑÂÄãÔºåÊòØÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑË™ûË®ÄÔºâ„ÄÇÊàëÂÄëÂ∞á
ÊØèÁØáË´ñÊñáÁöÑÂÖÉË≥áÊñôËΩâÊèõÊàêÂºïÊñáÁ∂≤Ë∑ØÔºåÁî¢Áîü‰∏ÄÂÄãÊúâÂêë
Áï∞Ë≥™ÂúñÂΩ¢„ÄÇÊ≠§ÂúñÂΩ¢ÂåÖÂê´Âú® HAL ‰∏äÂîØ‰∏ÄË≠òÂà•ÁöÑ‰ΩúËÄÖÔºå‰ª•Âèä
ÊâÄÊúâÂÖ¨ÈñãÊèê‰∫§ÁöÑË´ñÊñáÂèäÂÖ∂ÂºïÊñá„ÄÇÊàëÂÄëÊèê‰æõ‰∏ÄÂÄãÂü∫Ê∫ñ
‰ΩøÁî®Ë≥áÊñôÈõÜÈÄ≤Ë°å‰ΩúËÄÖÊ≠∏Â±¨ÔºåÂØ¶‰Ωú‰∏ÄÁ≥ªÂàó
ÊúÄÂÖàÈÄ≤ÁöÑÂúñÂΩ¢Ë°®Á§∫Â≠∏ÁøíÊ®°ÂûãÈÄ≤Ë°åÈÄ£ÁµêÈ†êÊ∏¨Ôºå
‰∏¶Ë®éË´ñÊàëÂÄëÁî¢ÁîüÁöÑÁü•Ë≠òÂúñÂΩ¢ÁµêÊßãÁöÑÂØ¶Áî®ÊÄß„ÄÇ

##### **CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge**
2407.20564v1 by Tianshi Zheng, Jiaxin Bai, Yicheng Wang, Tianqing Fang, Yue Guo, Yauwai Yim, Yangqiu Song

While large language models (LLMs) have demonstrated impressive capabilities
across various natural language processing tasks by acquiring rich factual
knowledge from their broad training data, their ability to synthesize and
logically reason with this knowledge in complex ways remains underexplored. In
this work, we present a systematic evaluation of state-of-the-art LLMs' complex
logical reasoning abilities through a novel benchmark of automatically
generated complex reasoning questions over general domain and biomedical
knowledge graphs. Our extensive experiments, employing diverse in-context
learning techniques, reveal that LLMs excel at reasoning over general world
knowledge but face significant challenges with specialized domain-specific
knowledge. We find that prompting with explicit Chain-of-Thought demonstrations
can substantially improve LLM performance on complex logical reasoning tasks
with diverse logical operations. Interestingly, our controlled evaluations
uncover an asymmetry where LLMs display proficiency at set union operations,
but struggle considerably with set intersections - a key building block of
logical reasoning. To foster further work, we will publicly release our
evaluation benchmark and code.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåÂèØÈÄèÈÅéÂæûÂª£Ê≥õÁöÑË®ìÁ∑¥Ë≥áÊñô‰∏≠Áç≤ÂèñË±êÂØåÁöÑ‰∫ãÂØ¶Áü•Ë≠òÔºåÂü∑Ë°åÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãôÔºå‰ΩÜÂÆÉÂÄëÁ∂úÂêàÈÅãÁî®‰∏¶‰ª•Ë§áÈõúÁöÑÊñπÂºèÈÅãÁî®Ê≠§Áü•Ë≠òÈÄ≤Ë°åÈÇèËºØÊé®ÁêÜÁöÑËÉΩÂäõ‰ªçÊúâÂæÖÈÄ≤‰∏ÄÊ≠•Êé¢Ë®é„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅé‰∏ÄÂÄãËá™ÂãïÁîüÊàêÁöÑ‰∏ÄËà¨È†òÂüüÂíåÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÂúñË°®Ë§áÈõúÊé®ÁêÜÂïèÈ°åÁöÑÊñ∞Âü∫Ê∫ñÔºåÂ∞çÊúÄÂÖàÈÄ≤ÁöÑ LLM Ë§áÈõúÈÇèËºØÊé®ÁêÜËÉΩÂäõÈÄ≤Ë°åÁ≥ªÁµ±ÊÄßË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÂª£Ê≥õÂØ¶È©óÊé°Áî®Â§öÊ®£ÂåñÁöÑÊÉÖÂ¢ÉÂ≠∏ÁøíÊäÄË°ìÔºåÊè≠Á§∫Âá∫ LLM ÊìÖÈï∑Â∞ç‰∏ÄËà¨‰∏ñÁïåÁü•Ë≠òÈÄ≤Ë°åÊé®ÁêÜÔºå‰ΩÜÂú®ËôïÁêÜÁâπÂÆöÈ†òÂüüÁöÑÂ∞àÊ•≠Áü•Ë≠òÊôÇÂâáÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞„ÄÇÊàëÂÄëÁôºÁèæÔºå‰ΩøÁî®ÊòéÁ¢∫ÁöÑÊÄùËÄÉÈèàÊ¢ùÁ§∫ÁØÑÈÄ≤Ë°åÊèêÁ§∫ÔºåÂèØ‰ª•Â§ßÂπÖÊîπÂñÑ LLM Âú®ÂÖ∑ÊúâÂ§öÊ®£ÂåñÈÇèËºØÈÅãÁÆóÁöÑË§áÈõúÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏≠ÁöÑË°®Áèæ„ÄÇÊúâË∂£ÁöÑÊòØÔºåÊàëÂÄëÁöÑÂèóÊéßË©ï‰º∞Êè≠Èú≤‰∫Ü‰∏ÄÂÄã‰∏çÂ∞çÁ®±ÊÄßÔºåÂÖ∂‰∏≠ LLM Â±ïÁèæÂá∫Âú®ÈõÜÂêàËÅØÈõÜÈÅãÁÆóÊñπÈù¢ÁöÑÁÜüÁ∑¥Â∫¶Ôºå‰ΩÜÂú®ÈõÜÂêà‰∫§ÈõÜÊñπÈù¢ÂçªÈ°ØÂæóÁõ∏Áï∂ÂêÉÂäõÔºåËÄåÈõÜÂêà‰∫§ÈõÜÊ≠£ÊòØÈÇèËºØÊé®ÁêÜÁöÑÈóúÈçµÁµÑÊàêÈÉ®ÂàÜ„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÂæåÁ∫åÁ†îÁ©∂ÔºåÊàëÂÄëÂ∞áÂÖ¨ÈñãÁôºÂ∏ÉÊàëÂÄëÁöÑË©ï‰º∞Âü∫Ê∫ñÂíåÁ®ãÂºèÁ¢º„ÄÇ

##### **Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language**
2407.20513v1 by Hossein Rajaby Faghihi, Aliakbar Nafar, Andrzej Uszok, Hamid Karimian, Parisa Kordjamshidi

This paper presents a conversational pipeline for crafting domain knowledge
for complex neuro-symbolic models through natural language prompts. It
leverages large language models to generate declarative programs in the
DomiKnowS framework. The programs in this framework express concepts and their
relationships as a graph in addition to logical constraints between them. The
graph, later, can be connected to trainable neural models according to those
specifications. Our proposed pipeline utilizes techniques like dynamic
in-context demonstration retrieval, model refinement based on feedback from a
symbolic parser, visualization, and user interaction to generate the tasks'
structure and formal knowledge representation. This approach empowers domain
experts, even those not well-versed in ML/AI, to formally declare their
knowledge to be incorporated in customized neural models in the DomiKnowS
framework.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∞çË©±ÂºèÁÆ°ÈÅìÔºåÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÊèêÁ§∫ÔºåÁÇ∫Ë§áÈõúÁöÑÁ•ûÁ∂ìÁ¨¶ËôüÊ®°ÂûãÂª∫Á´ãÈ†òÂüüÁü•Ë≠ò„ÄÇÂÆÉÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú® DomiKnowS Ê°ÜÊû∂‰∏≠Áî¢ÁîüÂÆ£ÂëäÂºèÁ®ãÂºè„ÄÇÊ≠§Ê°ÜÊû∂‰∏≠ÁöÑÁ®ãÂºèÊúÉÂ∞áÊ¶ÇÂøµÂèäÂÖ∂Èóú‰øÇË°®Á§∫ÁÇ∫ÂúñÂΩ¢Ôºå‰∏¶Âú®ÂÆÉÂÄë‰πãÈñìÂä†‰∏äÈÇèËºØÁ¥ÑÊùü„ÄÇ‰πãÂæåÔºåÂèØ‰ª•Ê†πÊìöÈÄô‰∫õË¶èÊ†ºÂ∞áÂúñÂΩ¢ÈÄ£Êé•Âà∞ÂèØË®ìÁ∑¥ÁöÑÁ•ûÁ∂ìÊ®°Âûã„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÁÆ°ÈÅìÂà©Áî®ÂãïÊÖãÊÉÖÂ¢É‰∏≠Á§∫ÁØÑÊ™¢Á¥¢„ÄÅÂü∫ÊñºÁ¨¶ËôüËß£ÊûêÂô®ÂõûÈ•ãÁöÑÊ®°ÂûãÁ≤æÁÖâ„ÄÅË¶ñË¶∫ÂåñÂíå‰ΩøÁî®ËÄÖ‰∫íÂãïÁ≠âÊäÄË°ìÔºå‰ª•Áî¢Áîü‰ªªÂãôÁµêÊßãÂíåÂΩ¢ÂºèÁü•Ë≠òË°®Á§∫„ÄÇÈÄôÁ®ÆÊñπÊ≥ïËÆìÈ†òÂüüÂ∞àÂÆ∂ÔºåÂç≥‰ΩøÊòØ‰∏çÁÜüÊÇâÊ©üÂô®Â≠∏ÁøíÔºè‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰∫∫Ôºå‰πüËÉΩÊ≠£ÂºèÂÆ£Âëä‰ªñÂÄëÁöÑÁü•Ë≠òÔºå‰∏¶Â∞áÂÖ∂Á¥çÂÖ• DomiKnowS Ê°ÜÊû∂‰∏≠ÁöÑËá™Ë®ÇÁ•ûÁ∂ìÊ®°Âûã„ÄÇ

##### **What if Red Can Talk? Dynamic Dialogue Generation Using Large Language Models**
2407.20382v1 by Navapat Nananukul, Wichayaporn Wongkamjan

Role-playing games (RPGs) provide players with a rich, interactive world to
explore. Dialogue serves as the primary means of communication between
developers and players, manifesting in various forms such as guides, NPC
interactions, and storytelling. While most games rely on written scripts to
define the main story and character personalities, player immersion can be
significantly enhanced through casual interactions between characters. With the
advent of large language models (LLMs), we introduce a dialogue filler
framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic
and contextually appropriate character interactions. We test this framework
within the environments of Final Fantasy VII Remake and Pokemon, providing
qualitative and quantitative evidence that demonstrates GPT-4's capability to
act with defined personalities and generate dialogue. However, some flaws
remain, such as GPT-4 being overly positive or more subtle personalities, such
as maturity, tend to be of lower quality compared to more overt traits like
timidity. This study aims to assist developers in crafting more nuanced filler
dialogues, thereby enriching player immersion and enhancing the overall RPG
experience.

ÊëòË¶ÅÔºöËßíËâ≤ÊâÆÊºîÈÅäÊà≤ (RPG) ÁÇ∫Áé©ÂÆ∂Êèê‰æõ‰∏ÄÂÄãË±êÂØå‰∏î‰∫íÂãïÁöÑ‰∏ñÁïå‰æõÂÖ∂Êé¢Á¥¢„ÄÇÂ∞çË©±‰ΩúÁÇ∫ÈñãÁôºËÄÖËàáÁé©ÂÆ∂‰πãÈñìÁöÑ‰∏ªË¶ÅÊ∫ùÈÄöÊñπÂºèÔºå‰ª•ÊåáÂçó„ÄÅNPC ‰∫íÂãïÂíåË™™ÊïÖ‰∫ãÁ≠âÂêÑÁ®ÆÂΩ¢ÂºèÂëàÁèæ„ÄÇÈõñÁÑ∂Â§ßÂ§öÊï∏ÈÅäÊà≤‰æùË≥¥ÊñºÊõ∏Èù¢ËÖ≥Êú¨‰æÜÂÆöÁæ©‰∏ªÁ∑öÊïÖ‰∫ãÂíåËßíËâ≤ÂÄãÊÄßÔºå‰ΩÜÈÄèÈÅéËßíËâ≤‰πãÈñìÁöÑÈñíËÅä‰∫íÂãïÔºåÂèØ‰ª•Â§ßÂπÖÊèêÂçáÁé©ÂÆ∂ÁöÑÊ≤âÊµ∏ÊÑü„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂ∞çË©±Â°´ÂÖÖÊ°ÜÊû∂ÔºåÂà©Áî®Áî±Áü•Ë≠òÂúñË≠úÂ¢ûÂº∑ÁöÑ LLM ‰æÜÁî¢ÁîüÂãïÊÖã‰∏îÁ¨¶ÂêàÊÉÖÂ¢ÉÁöÑÂ∞çË©±‰∫íÂãï„ÄÇÊàëÂÄëÂú® Final Fantasy VII Remake ÂíåÂØ∂ÂèØÂ§¢ÁöÑÁí∞Â¢É‰∏≠Ê∏¨Ë©¶‰∫ÜÈÄôÂÄãÊ°ÜÊû∂ÔºåÊèê‰æõ‰∫ÜÂÆöÊÄßÂíåÂÆöÈáèÁöÑË≠âÊìöÔºåË≠âÊòé‰∫Ü GPT-4 ÂÖ∑ÂÇô‰ª•ÂÆöÁæ©Â•ΩÁöÑÂÄãÊÄßË°åÂãï‰∏¶Áî¢ÁîüÂ∞çË©±ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºå‰ªçÂ≠òÂú®‰∏Ä‰∫õÁº∫Èô∑Ôºå‰æãÂ¶Ç GPT-4 ÈÅéÊñºÊ≠£Èù¢ÔºåÊàñËÄÖËºÉÁÇ∫Á¥∞ÂæÆÁöÑÂÄãÊÄßÔºå‰æãÂ¶ÇÊàêÁÜüÂ∫¶ÔºåÂæÄÂæÄÂìÅË≥™‰ΩéÊñºËºÉÊòéÈ°ØÁöÑÁâπË≥™Ôºå‰æãÂ¶ÇËÜΩÊÄØ„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ÂçîÂä©ÈñãÁôºËÄÖÊâìÈÄ†Êõ¥Á¥∞Á∑ªÁöÑÂ°´ÂÖÖÂ∞çË©±ÔºåÂæûËÄåË±êÂØåÁé©ÂÆ∂ÁöÑÊ≤âÊµ∏ÊÑü‰∏¶ÊèêÂçáÊï¥È´î RPG È´îÈ©ó„ÄÇ

##### **MindSearch: Mimicking Human Minds Elicits Deep AI Searcher**
2407.20183v1 by Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao

Information seeking and integration is a complex cognitive task that consumes
enormous time and effort. Inspired by the remarkable progress of Large Language
Models, recent works attempt to solve this task by combining LLMs and search
engines. However, these methods still obtain unsatisfying performance due to
three challenges: (1) complex requests often cannot be accurately and
completely retrieved by the search engine once (2) corresponding information to
be integrated is spread over multiple web pages along with massive noise, and
(3) a large number of web pages with long contents may quickly exceed the
maximum context length of LLMs. Inspired by the cognitive process when humans
solve these problems, we introduce MindSearch to mimic the human minds in web
information seeking and integration, which can be instantiated by a simple yet
effective LLM-based multi-agent framework. The WebPlanner models the human mind
of multi-step information seeking as a dynamic graph construction process: it
decomposes the user query into atomic sub-questions as nodes in the graph and
progressively extends the graph based on the search result from WebSearcher.
Tasked with each sub-question, WebSearcher performs hierarchical information
retrieval with search engines and collects valuable information for WebPlanner.
The multi-agent design of MindSearch enables the whole framework to seek and
integrate information parallelly from larger-scale (e.g., more than 300) web
pages in 3 minutes, which is worth 3 hours of human effort. MindSearch
demonstrates significant improvement in the response quality in terms of depth
and breadth, on both close-set and open-set QA problems. Besides, responses
from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web
and Perplexity.ai applications, which implies that MindSearch can already
deliver a competitive solution to the proprietary AI search engine.

ÊëòË¶ÅÔºöË≥áË®äÊêúÂ∞ãËàáÊï¥ÂêàÊòØ‰∏ÄÈ†ÖË§áÈõúÁöÑË™çÁü•‰ªªÂãôÔºåÊúÉËÄóË≤ªÂ§ßÈáèÊôÇÈñìËàáÁ≤æÂäõ„ÄÇÂú®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÈ°ØËëóÈÄ≤Â±ïÁöÑÂïüÁôº‰∏ãÔºåËøëÊúüÁ†îÁ©∂ÂòóË©¶ÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°ÂûãËàáÊêúÂ∞ãÂºïÊìé‰æÜËß£Ê±∫Ê≠§‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ï‰ªçÂõ†‰∏âÈ†ÖÊåëÊà∞ËÄåÁÑ°Ê≥ïÁç≤Âæó‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩÔºö(1) Ë§áÈõúÁöÑÊü•Ë©¢ÈÄöÂ∏∏ÁÑ°Ê≥ïÁî±ÊêúÂ∞ãÂºïÊìé‰∏ÄÊ¨°Ê∫ñÁ¢∫‰∏îÂÆåÊï¥Âú∞Êì∑ÂèñÔºå(2) Ë¶ÅÊï¥ÂêàÁöÑÂ∞çÊáâË≥áË®äÊï£Â∏ÉÂú®Â§öÂÄãÁ∂≤È†Å‰∏≠‰∏î‰º¥Èö®ËëóÂ§ßÈáèÈõúË®äÔºå‰ª•Âèä (3) Â§ßÈáèÂÖßÂÆπÈÅéÈï∑ÁöÑÁ∂≤È†ÅÂèØËÉΩÊúÉÂø´ÈÄüË∂ÖÈÅéÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊúÄÂ§ßËÑàÁµ°Èï∑Â∫¶„ÄÇÂú®‰∫∫È°ûËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÁöÑË™çÁü•ÈÅéÁ®ã‰∏≠Áç≤ÂæóÈùàÊÑüÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü MindSearch ‰æÜÊ®°Êì¨‰∫∫È°ûÂøÉÊô∫Âú®Á∂≤È†ÅË≥áË®äÊêúÂ∞ãËàáÊï¥Âêà‰∏≠ÁöÑË°åÁÇ∫ÔºåÈÄôÂèØ‰ª•Áî®‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂ§ö‰ª£ÁêÜÊû∂Êßã‰æÜÂØ¶‰æãÂåñ„ÄÇWebPlanner ‰ª•ÂãïÊÖãÂúñÂΩ¢Âª∫ÊßãÈÅéÁ®ã‰æÜÂª∫Ê®°‰∫∫È°ûÂøÉÊô∫ÁöÑÂ§öÊ≠•È©üË≥áË®äÊêúÂ∞ãÔºöÂÆÉÂ∞á‰ΩøÁî®ËÄÖÊü•Ë©¢ÂàÜËß£ÊàêÂúñÂΩ¢‰∏≠ÁöÑÁØÄÈªûÔºå‰ΩúÁÇ∫ÂéüÂ≠êÂåñÂ≠êÂïèÈ°åÔºå‰∏¶Ê†πÊìö WebSearcher ÁöÑÊêúÂ∞ãÁµêÊûúÈÄêÊ≠•Âª∂‰º∏ÂúñÂΩ¢„ÄÇWebSearcher ‰ª•ÊØèÂÄãÂ≠êÂïèÈ°åÁÇ∫‰ªªÂãôÔºåÂü∑Ë°åÊêúÂ∞ãÂºïÊìéÁöÑÂàÜÂ±§ÂºèË≥áË®äÊì∑ÂèñÔºå‰∏¶ÁÇ∫ WebPlanner Êî∂ÈõÜÊúâÂÉπÂÄºÁöÑË≥áË®ä„ÄÇMindSearch ÁöÑÂ§ö‰ª£ÁêÜË®≠Ë®àËÆìÊï¥ÂÄãÊû∂ÊßãÂèØ‰ª•Âú® 3 ÂàÜÈêòÂÖßÂπ≥Ë°åÂú∞ÂæûÊõ¥Â§ßË¶èÊ®°Ôºà‰æãÂ¶ÇË∂ÖÈÅé 300 ÂÄãÔºâÁöÑÁ∂≤È†Å‰∏≠ÊêúÂ∞ã‰∏¶Êï¥ÂêàË≥áË®äÔºåÈÄôÁõ∏Áï∂Êñº 3 Â∞èÊôÇÁöÑ‰∫∫Âäõ„ÄÇMindSearch Âú®Ê∑±Â∫¶ÂíåÂª£Â∫¶ÊñπÈù¢ÈÉΩÈ°ØËëóÊèêÂçá‰∫ÜÂõûÊáâÂìÅË≥™ÔºåÁÑ°Ë´ñÊòØÂú®Â∞ÅÈñâÂºèÊàñÈñãÊîæÂºèÂïèÁ≠îÂïèÈ°å‰∏ä„ÄÇÊ≠§Â§ñÔºå‰∫∫È°ûÊõ¥ÂÅèÂ•ΩÂü∫Êñº InternLM2.5-7B ÁöÑ MindSearch ÂõûÊáâÔºåÂãùÈÅé ChatGPT-Web Âíå Perplexity.ai ÊáâÁî®Á®ãÂºèÔºåÈÄôË°®Á§∫ MindSearch Â∑≤Á∂ìÂèØ‰ª•ÁÇ∫Â∞àÊúâ AI ÊêúÂ∞ãÂºïÊìéÊèê‰æõÊúâÁ´∂Áà≠ÂäõÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **rLLM: Relational Table Learning with LLMs**
2407.20157v1 by Weichen Li, Xiaotong Huang, Jianwu Zheng, Zheng Wang, Chaokun Wang, Li Pan, Jianhua Li

We introduce rLLM (relationLLM), a PyTorch library designed for Relational
Table Learning (RTL) with Large Language Models (LLMs). The core idea is to
decompose state-of-the-art Graph Neural Networks, LLMs, and Table Neural
Networks into standardized modules, to enable the fast construction of novel
RTL-type models in a simple "combine, align, and co-train" manner. To
illustrate the usage of rLLM, we introduce a simple RTL method named
\textbf{BRIDGE}. Additionally, we present three novel relational tabular
datasets (TML1M, TLF2K, and TACM12K) by enhancing classic datasets. We hope
rLLM can serve as a useful and easy-to-use development framework for
RTL-related tasks. Our code is available at:
https://github.com/rllm-project/rllm.

ÊëòË¶ÅÔºöÊàëÂÄëÂºïÂÖ•‰∫Ü rLLM (relationLLM)Ôºå‰∏ÄÂÄãÂ∞àÁÇ∫Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈóú‰øÇË°®Â≠∏Áøí (RTL) ÊâÄË®≠Ë®àÁöÑ PyTorch ÂáΩÂºèÂ∫´„ÄÇÊ†∏ÂøÉÊ¶ÇÂøµÊòØÂ∞áÊúÄÂÖàÈÄ≤ÁöÑÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅLLM ÂíåË°®Á•ûÁ∂ìÁ∂≤Ë∑ØÂàÜËß£ÁÇ∫Ê®ôÊ∫ñÂåñÊ®°ÁµÑÔºå‰ª•‰æø‰ª•Á∞°ÂñÆÁöÑ„ÄåÁµÑÂêà„ÄÅÂ∞çÈΩäÂíåÂÖ±ÂêåË®ìÁ∑¥„ÄçÊñπÂºèÂø´ÈÄüÂª∫ÊßãÊñ∞Âûã RTL È°ûÂûãÊ®°Âûã„ÄÇÁÇ∫‰∫ÜË™™Êòé rLLM ÁöÑÁî®Ê≥ïÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂêçÁÇ∫ \textbf{BRIDGE} ÁöÑÁ∞°ÂñÆ RTL ÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄèÈÅéÂº∑ÂåñÁ∂ìÂÖ∏Ë≥áÊñôÈõÜ‰æÜÂëàÁèæ‰∏âÂÄãÊñ∞Á©éÁöÑÈóú‰øÇË°®Ê†ºË≥áÊñôÈõÜ (TML1M„ÄÅTLF2K Âíå TACM12K)„ÄÇÊàëÂÄëÂ∏åÊúõ rLLM ËÉΩÂ§†‰ΩúÁÇ∫ RTL Áõ∏Èóú‰ªªÂãôÊúâÁî®ÁöÑ‰∏îÊòìÊñº‰ΩøÁî®ÁöÑÈñãÁôºÊû∂Êßã„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºö
https://github.com/rllm-project/rllm„ÄÇ

##### **Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation**
2407.19643v2 by Yunsheng Wang, Songhao Chen, Kevin Jin

Knowledge graphs (KGs) are essential in applications such as network
alignment, question-answering, and recommender systems (RSs) since they offer
structured relational data that facilitate the inference of indirect
relationships. However, the development of KG-based RSs capable of processing
user inputs in natural language faces significant challenges. Firstly, natural
language processing units must effectively handle the ambiguity and variability
in human language to interpret user intents accurately. Secondly, the system
must precisely identify and link entities, like product names, to their
corresponding nodes in KGs. To overcome these challenges, supported by Lenovo,
we developed a novel chatbot called "Prometheus," which integrates a KG with a
large language model (LLM), specifically designed for recommending computer
components. This chatbot can accurately decode user requests and deliver
personalized recommendations derived from KGs, ensuring precise comprehension
and response to their computer setup needs.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) Âú®Á∂≤Ë∑ØÊØîÂ∞ç„ÄÅÂïèÁ≠îÂíåÊé®Ëñ¶Á≥ªÁµ± (RS) Á≠âÊáâÁî®‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÊèê‰æõÁµêÊßãÂåñÁöÑÈóú‰øÇË≥áÊñôÔºåÊúâÂä©ÊñºÊé®Êñ∑ÈñìÊé•Èóú‰øÇ„ÄÇÁÑ∂ËÄåÔºåÈñãÁôºËÉΩÂ§†ËôïÁêÜËá™ÁÑ∂Ë™ûË®Ä‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁöÑÂü∫Êñº KG ÁöÑ RS Èù¢Ëá®ËëóÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂñÆÂÖÉÂøÖÈ†àÊúâÊïàËôïÁêÜ‰∫∫È°ûË™ûË®Ä‰∏≠ÁöÑÊ®°Á≥äÊÄßÂíåËÆäÁï∞ÊÄßÔºåÊâçËÉΩÊ∫ñÁ¢∫Âú∞Ëß£Èáã‰ΩøÁî®ËÄÖÊÑèÂúñ„ÄÇÂÖ∂Ê¨°ÔºåÁ≥ªÁµ±ÂøÖÈ†àÊ∫ñÁ¢∫Ë≠òÂà•ÂíåÈÄ£ÁµêÂØ¶È´îÔºà‰æãÂ¶ÇÁî¢ÂìÅÂêçÁ®±ÔºâÂà∞ KG ‰∏≠Â∞çÊáâÁöÑÁØÄÈªû„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÂú®ËÅØÊÉ≥ÁöÑÊîØÊè¥‰∏ãÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÊ¨æÂêçÁÇ∫„ÄåÊôÆÁæÖÁ±≥‰øÆÊñØ„ÄçÁöÑÊñ∞ËÅäÂ§©Ê©üÂô®‰∫∫ÔºåÂÆÉÂ∞á KG ËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êï¥ÂêàÂú®‰∏ÄËµ∑ÔºåÂ∞àÈñÄÁî®ÊñºÊé®Ëñ¶ÈõªËÖ¶ÁµÑ‰ª∂„ÄÇÊ≠§ËÅäÂ§©Ê©üÂô®‰∫∫ÂèØ‰ª•Ê∫ñÁ¢∫Âú∞Ëß£Á¢º‰ΩøÁî®ËÄÖË¶ÅÊ±ÇÔºå‰∏¶Êèê‰æõÂæû KG ‰∏≠Ë°çÁîüÁöÑÂÄã‰∫∫ÂåñÊé®Ëñ¶ÔºåÁ¢∫‰øùÁ≤æÁ¢∫ÁêÜËß£ÂíåÂõûÊáâÂÖ∂ÈõªËÖ¶Ë®≠ÂÆöÈúÄÊ±Ç„ÄÇ

##### **TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs**
2407.19616v1 by Selma Wanna, Ryan Barron, Nick Solovyev, Maksim E. Eren, Manish Bhattarai, Kim Rasmussen, Boian S. Alexandrov

Topic modeling is a technique for organizing and extracting themes from large
collections of unstructured text. Non-negative matrix factorization (NMF) is a
common unsupervised approach that decomposes a term frequency-inverse document
frequency (TF-IDF) matrix to uncover latent topics and segment the dataset
accordingly. While useful for highlighting patterns and clustering documents,
NMF does not provide explicit topic labels, necessitating subject matter
experts (SMEs) to assign labels manually. We present a methodology for
automating topic labeling in documents clustered via NMF with automatic model
determination (NMFk). By leveraging the output of NMFk and employing prompt
engineering, we utilize large language models (LLMs) to generate accurate topic
labels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs
demonstrates the effectiveness of our method in enhancing knowledge management
and document organization.

ÊëòË¶ÅÔºö‰∏ªÈ°åÂª∫Ê®°ÊòØ‰∏ÄÁ®ÆÂæûÂ§ßÈáèÈùûÁµêÊßãÂåñÊñáÊú¨‰∏≠ÁµÑÁπîÂíåÊèêÂèñ‰∏ªÈ°åÁöÑÊäÄË°ì„ÄÇÈùûË≤†Áü©Èô£ÂàÜËß£ (NMF) ÊòØ‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÁÑ°Áõ£Áù£ÊñπÊ≥ïÔºåÂÆÉÂ∞áË©ûÈ†ª-ÈÄÜÊñá‰ª∂È†ªÁéá (TF-IDF) Áü©Èô£ÂàÜËß£ÁÇ∫ÊΩõÂú®‰∏ªÈ°åÔºå‰∏¶ÊìöÊ≠§Â∞çÊï∏ÊìöÈõÜÈÄ≤Ë°åÂàÜÊÆµ„ÄÇÂÑòÁÆ° NMF ÂèØÁî®ÊñºÂº∑Ë™øÊ®°ÂºèÂíåÁæ§ÁµÑÊñá‰ª∂Ôºå‰ΩÜÂÆÉ‰∏çÊèê‰æõÊòéÁ¢∫ÁöÑ‰∏ªÈ°åÊ®ôÁ±§ÔºåÈÄôÈúÄË¶Å‰∏ªÈ°åÂ∞àÂÆ∂ (SME) ÊâãÂãïÂàÜÈÖçÊ®ôÁ±§„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÁî®ÊñºËá™ÂãïÊ®ôË®òÈÄöÈÅé NMF ÈÄ≤Ë°åÁæ§ÁµÑÁöÑÊñá‰ª∂Ôºå‰∏¶Ëá™ÂãïÁ¢∫ÂÆöÊ®°Âûã (NMFk)„ÄÇÈÄöÈÅéÂà©Áî® NMFk ÁöÑËº∏Âá∫‰∏¶Êé°Áî®ÊèêÁ§∫Â∑•Á®ãÔºåÊàëÂÄëÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁîüÊàêÊ∫ñÁ¢∫ÁöÑ‰∏ªÈ°åÊ®ôÁ±§„ÄÇÊàëÂÄëÂ∞çË∂ÖÈÅé 34,000 ÁØáÈóúÊñºÁü•Ë≠òÂúñË≠úÁöÑÁßëÂ≠∏ÊëòË¶ÅÈÄ≤Ë°åÁöÑÊ°à‰æãÁ†îÁ©∂Ë≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Â¢ûÂº∑Áü•Ë≠òÁÆ°ÁêÜÂíåÊñá‰ª∂ÁµÑÁπîÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Semantic Communication Enhanced by Knowledge Graph Representation Learning**
2407.19338v1 by Nour Hello, Paolo Di Lorenzo, Emilio Calvanese Strinati

This paper investigates the advantages of representing and processing
semantic knowledge extracted into graphs within the emerging paradigm of
semantic communications. The proposed approach leverages semantic and pragmatic
aspects, incorporating recent advances on large language models (LLMs) to
achieve compact representations of knowledge to be processed and exchanged
between intelligent agents. This is accomplished by using the cascade of LLMs
and graph neural networks (GNNs) as semantic encoders, where information to be
shared is selected to be meaningful at the receiver. The embedding vectors
produced by the proposed semantic encoder represent information in the form of
triplets: nodes (semantic concepts entities), edges(relations between
concepts), nodes. Thus, semantic information is associated with the
representation of relationships among elements in the space of semantic concept
abstractions. In this paper, we investigate the potential of achieving high
compression rates in communication by incorporating relations that link
elements within graph embeddings. We propose sending semantic symbols solely
equivalent to node embeddings through the wireless channel and inferring the
complete knowledge graph at the receiver. Numerical simulations illustrate the
effectiveness of leveraging knowledge graphs to semantically compress and
transmit information.

ÊëòË¶ÅÔºöÊú¨ÊñáÁ†îÁ©∂‰∫ÜÂú®ËØ≠‰πâÈÄö‰ø°ÁöÑÊñ∞ÂÖ¥ËåÉ‰æã‰∏≠Â∞ÜÊèêÂèñÂà∞Âõæ‰∏≠ÁöÑËØ≠‰πâÁü•ËØÜË°®Á§∫ÂíåÂ§ÑÁêÜÁöÑ‰ºòÂäø„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂà©Áî®ËØ≠‰πâÂíåËØ≠Áî®ÊñπÈù¢ÔºåÁªìÂêà‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ËøõÂ±ïÔºå‰ª•ÂÆûÁé∞Ë¶ÅÂ§ÑÁêÜÂíåÂú®Êô∫ËÉΩ‰ª£ÁêÜ‰πãÈó¥‰∫§Êç¢ÁöÑÁü•ËØÜÁöÑÁ¥ßÂáëË°®Á§∫„ÄÇËøôÊòØÈÄöËøá‰ΩøÁî® LLM ÂíåÂõæÁ•ûÁªèÁΩëÁªú (GNN) ÁöÑÁ∫ßËÅî‰Ωú‰∏∫ËØ≠‰πâÁºñÁ†ÅÂô®Êù•ÂÆåÊàêÁöÑÔºåÂÖ∂‰∏≠Ë¶ÅÂÖ±‰∫´ÁöÑ‰ø°ÊÅØË¢´ÈÄâÊã©‰∏∫ÂØπÊé•Êî∂ËÄÖÊúâÊÑè‰πâ„ÄÇÁî±ÊâÄÊèêÂá∫ÁöÑËØ≠‰πâÁºñÁ†ÅÂô®‰∫ßÁîüÁöÑÂµåÂÖ•ÂêëÈáè‰ª•‰∏âÂÖÉÁªÑÁöÑÂΩ¢ÂºèË°®Á§∫‰ø°ÊÅØÔºöËäÇÁÇπÔºàËØ≠‰πâÊ¶ÇÂøµÂÆû‰ΩìÔºâ„ÄÅËæπÔºàÊ¶ÇÂøµ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºâ„ÄÅËäÇÁÇπ„ÄÇÂõ†Ê≠§ÔºåËØ≠‰πâ‰ø°ÊÅØ‰∏éËØ≠‰πâÊ¶ÇÂøµÊäΩË±°Á©∫Èó¥‰∏≠ÂÖÉÁ¥†‰πãÈó¥ÂÖ≥Á≥ªÁöÑË°®Á§∫Áõ∏ÂÖ≥ËÅî„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Á†îÁ©∂‰∫ÜÈÄöËøáÂêàÂπ∂Â∞ÜÂõæÂµåÂÖ•‰∏≠ÁöÑÂÖÉÁ¥†ËÅîÁ≥ªËµ∑Êù•ÁöÑÂÖ≥ËÅîÊù•ÂÆûÁé∞È´òÂéãÁº©ÁéáÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨Âª∫ËÆÆ‰ªÖÈÄöËøáÊó†Á∫ø‰ø°ÈÅìÂèëÈÄÅËØ≠‰πâÁ¨¶Âè∑ÔºåËøô‰∫õÁ¨¶Âè∑ÂÆåÂÖ®Á≠âÊïà‰∫éËäÇÁÇπÂµåÂÖ•ÔºåÂπ∂Âú®Êé•Êî∂Âô®Â§ÑÊé®Êñ≠Âá∫ÂÆåÊï¥ÁöÑÁü•ËØÜÂõæ„ÄÇÊï∞ÂÄºÊ®°ÊãüËØ¥Êòé‰∫ÜÂà©Áî®Áü•ËØÜÂõæËØ≠‰πâÂéãÁº©Âíå‰º†Ëæì‰ø°ÊÅØÁöÑÊúâÊïàÊÄß„ÄÇ

##### **GraphBPE: Molecular Graphs Meet Byte-Pair Encoding**
2407.19039v1 by Yuchen Shen, Barnab√°s P√≥czos

With the increasing attention to molecular machine learning, various
innovations have been made in designing better models or proposing more
comprehensive benchmarks. However, less is studied on the data preprocessing
schedule for molecular graphs, where a different view of the molecular graph
could potentially boost the model's performance. Inspired by the Byte-Pair
Encoding (BPE) algorithm, a subword tokenization method popularly adopted in
Natural Language Processing, we propose GraphBPE, which tokenizes a molecular
graph into different substructures and acts as a preprocessing schedule
independent of the model architectures. Our experiments on 3 graph-level
classification and 3 graph-level regression datasets show that data
preprocessing could boost the performance of models for molecular graphs, and
GraphBPE is effective for small classification datasets and it performs on par
with other tokenization methods across different model architectures.

ÊëòË¶ÅÔºöÈö®ËëóÂàÜÂ≠êÊ©üÂô®Â≠∏ÁøíÂèóÂà∞ÁöÑÈóúÊ≥®Â∫¶Ë∂ä‰æÜË∂äÈ´òÔºåÂú®Ë®≠Ë®àÊõ¥Â•ΩÁöÑÊ®°ÂûãÊàñÊèêÂá∫Êõ¥ÂÖ®Èù¢ÁöÑÂü∫Ê∫ñÊñπÈù¢Â∑≤Á∂ìÊúâ‰∫ÜÂêÑÁ®ÆÂâµÊñ∞„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÂàÜÂ≠êÂúñÁöÑÊï∏ÊìöÈ†êËôïÁêÜË®àÁï´Á†îÁ©∂ËºÉÂ∞ëÔºåÂú®Ë©≤Ë®àÁï´‰∏≠ÔºåÂàÜÂ≠êÂúñÁöÑ‰∏çÂêåË¶ñÂúñÂèØËÉΩÊúÉÊèêÂçáÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÂèóÂà∞Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠Âª£Ê≥õÊé°Áî®ÁöÑÂ≠êË©ûÂΩôÊ®ôË®òÂåñÊñπÊ≥ï Byte-Pair Á∑®Á¢º (BPE) ÊºîÁÆóÊ≥ïÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü GraphBPEÔºåÂÆÉÂ∞áÂàÜÂ≠êÂúñÊ®ôË®òÂåñÁÇ∫‰∏çÂêåÁöÑÂ≠êÁµêÊßãÔºå‰∏¶‰ΩúÁÇ∫ËàáÊ®°ÂûãÊû∂ÊßãÁÑ°ÈóúÁöÑÈ†êËôïÁêÜË®àÁï´„ÄÇÊàëÂÄëÂú® 3 ÂÄãÂúñÂΩ¢Â±§Á¥öÂàÜÈ°ûÂíå 3 ÂÄãÂúñÂΩ¢Â±§Á¥öÂõûÊ≠∏Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåË≥áÊñôÈ†êËôïÁêÜÂèØ‰ª•ÊèêÂçáÂàÜÂ≠êÂúñÊ®°ÂûãÁöÑÊïàËÉΩÔºåËÄå GraphBPE Â∞çÊñºÂ∞èÂûãÂàÜÈ°ûË≥áÊñôÈõÜÊúâÊïàÔºå‰∏¶‰∏îÂú®‰∏çÂêåÁöÑÊ®°ÂûãÊû∂Êßã‰∏≠ËàáÂÖ∂‰ªñÊ®ôË®òÂåñÊñπÊ≥ïË°®ÁèæÁõ∏Áï∂„ÄÇ

##### **Knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-based Causal Discovery**
2407.18752v3 by Yuni Susanti, Michael F√§rber

Causal discovery aims to estimate causal structures among variables based on
observational data. Large Language Models (LLMs) offer a fresh perspective to
tackle the causal discovery problem by reasoning on the metadata associated
with variables rather than their actual data values, an approach referred to as
knowledge-based causal discovery. In this paper, we investigate the
capabilities of Small Language Models (SLMs, defined as LLMs with fewer than 1
billion parameters) with prompt-based learning for knowledge-based causal
discovery. Specifically, we present KG Structure as Prompt, a novel approach
for integrating structural information from a knowledge graph, such as common
neighbor nodes and metapaths, into prompt-based learning to enhance the
capabilities of SLMs. Experimental results on three types of biomedical and
open-domain datasets under few-shot settings demonstrate the effectiveness of
our approach, surpassing most baselines and even conventional fine-tuning
approaches trained on full datasets. Our findings further highlight the strong
capabilities of SLMs: in combination with knowledge graphs and prompt-based
learning, SLMs demonstrate the potential to surpass LLMs with larger number of
parameters. Our code and datasets are available on GitHub.

ÊëòË¶ÅÔºöÂõ†ÊûúÁôºÁèæÊó®Âú®Ê†πÊìöËßÄÊ∏¨Êï∏Êìö‰º∞Ë®àËÆäÊï∏‰πãÈñìÁöÑÂõ†ÊûúÁµêÊßã„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑËßÄÈªû‰æÜËß£Ê±∫Âõ†ÊûúÁôºÁèæÂïèÈ°åÔºåÊñπÊ≥ïÊòØÊé®Ë´ñËàáËÆäÊï∏Áõ∏ÈóúÁöÑÂÖÉÊï∏ÊìöÔºåËÄå‰∏çÊòØÂÆÉÂÄëÁöÑÂØ¶ÈöõÊï∏ÊìöÂÄºÔºåÈÄôÁ®ÆÊñπÊ≥ïÁ®±ÁÇ∫Âü∫ÊñºÁü•Ë≠òÁöÑÂõ†ÊûúÁôºÁèæ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂ∞èË™ûË®ÄÊ®°Âûã (SLMÔºåÂÆöÁæ©ÁÇ∫ÂèÉÊï∏Â∞ëÊñº 10 ÂÑÑÁöÑ LLM) ÁöÑËÉΩÂäõÔºå‰∏¶Êé°Áî®Âü∫ÊñºÊèêÁ§∫ÁöÑÂ≠∏ÁøíÈÄ≤Ë°åÂü∫ÊñºÁü•Ë≠òÁöÑÂõ†ÊûúÁôºÁèæ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü KG Structure as PromptÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞á‰æÜËá™Áü•Ë≠òÂúñË≠úÁöÑÁµêÊßãË≥áË®äÔºå‰æãÂ¶ÇÂÖ±ÂêåÈÑ∞Â±ÖÁØÄÈªûÂíåÂÖÉË∑ØÂæëÔºåÊï¥ÂêàÂà∞Âü∫ÊñºÊèêÁ§∫ÁöÑÂ≠∏Áøí‰∏≠Ôºå‰ª•Â¢ûÂº∑ SLM ÁöÑËÉΩÂäõ„ÄÇÂú®Â∞ëÊ¨°ÂòóË©¶Ë®≠ÂÆö‰∏ãÔºåÈáùÂ∞ç‰∏âÁ®ÆÈ°ûÂûãÁöÑÁîüÁâ©ÈÜ´Â≠∏ÂíåÈñãÊîæÈ†òÂüüË≥áÊñôÈõÜÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåË∂ÖË∂ä‰∫ÜÂ§ßÂ§öÊï∏Âü∫Ê∫ñÔºåÁîöËá≥Ë∂ÖË∂ä‰∫ÜÂú®ÂÆåÊï¥Ë≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑÂÇ≥Áµ±ÂæÆË™øÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁôºÁèæÈÄ≤‰∏ÄÊ≠•Á™ÅÂá∫‰∫Ü SLM ÁöÑÂº∑Â§ßÂäüËÉΩÔºöÁµêÂêàÁü•Ë≠òÂúñË≠úÂíåÂü∫ÊñºÊèêÁ§∫ÁöÑÂ≠∏ÁøíÔºåSLM Â±ïÁ§∫‰∫ÜË∂ÖË∂äÂÖ∑ÊúâÊõ¥Â§öÂèÉÊï∏ÁöÑ LLM ÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂèØÂú® GitHub ‰∏äÂèñÂæó„ÄÇ

##### **Using GPT-4 to guide causal machine learning**
2407.18607v1 by Anthony C. Constantinou, Neville K. Kitson, Alessio Zanga

Since its introduction to the public, ChatGPT has had an unprecedented
impact. While some experts praised AI advancements and highlighted their
potential risks, others have been critical about the accuracy and usefulness of
Large Language Models (LLMs). In this paper, we are interested in the ability
of LLMs to identify causal relationships. We focus on the well-established
GPT-4 (Turbo) and evaluate its performance under the most restrictive
conditions, by isolating its ability to infer causal relationships based solely
on the variable labels without being given any context, demonstrating the
minimum level of effectiveness one can expect when it is provided with
label-only information. We show that questionnaire participants judge the GPT-4
graphs as the most accurate in the evaluated categories, closely followed by
knowledge graphs constructed by domain experts, with causal Machine Learning
(ML) far behind. We use these results to highlight the important limitation of
causal ML, which often produces causal graphs that violate common sense,
affecting trust in them. However, we show that pairing GPT-4 with causal ML
overcomes this limitation, resulting in graphical structures learnt from real
data that align more closely with those identified by domain experts, compared
to structures learnt by causal ML alone. Overall, our findings suggest that
despite GPT-4 not being explicitly designed to reason causally, it can still be
a valuable tool for causal representation, as it improves the causal discovery
process of causal ML algorithms that are designed to do just that.

ÊëòË¶ÅÔºöËá™ ChatGPT ÂêëÂÖ¨‰ºóÂèëÂ∏É‰ª•Êù•ÔºåÂÆÉ‰∫ßÁîü‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑÂΩ±Âìç„ÄÇËôΩÁÑ∂‰∏Ä‰∫õ‰∏ìÂÆ∂ËµûÊâ¨‰∫Ü AI ÁöÑËøõÊ≠•Âπ∂Âº∫Ë∞É‰∫ÜÂÖ∂ÊΩúÂú®È£éÈô©Ôºå‰ΩÜÂÖ∂‰ªñ‰∫∫‰∏ÄÁõ¥ÊâπËØÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÂáÜÁ°ÆÊÄßÂíåÊúâÁî®ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂØπ LLM ËØÜÂà´Âõ†ÊûúÂÖ≥Á≥ªÁöÑËÉΩÂäõÊÑüÂÖ¥Ë∂£„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÊàêÁÜüÁöÑ GPT-4ÔºàTurboÔºâÔºåÂπ∂Âú®ÊúÄ‰∏•Ê†ºÁöÑÊù°‰ª∂‰∏ãËØÑ‰º∞ÂÖ∂ÊÄßËÉΩÔºåÈÄöËøáÂ≠§Á´ãÂÖ∂‰ªÖÊ†πÊçÆÂèòÈáèÊ†áÁ≠æÊé®Êñ≠Âõ†ÊûúÂÖ≥Á≥ªÁöÑËÉΩÂäõÔºåËÄå‰∏çÊèê‰æõ‰ªª‰Ωï‰∏ä‰∏ãÊñáÔºåÂ±ïÁ§∫‰∫ÜÂΩì‰ªÖÊèê‰æõÊ†áÁ≠æ‰ø°ÊÅØÊó∂‰∫∫‰ª¨ÂèØ‰ª•È¢ÑÊúüÁöÑÊúÄ‰ΩéÊúâÊïàÊÄßÊ∞¥Âπ≥„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÈóÆÂç∑ÂèÇ‰∏éËÄÖËÆ§‰∏∫ GPT-4 ÂõæÂΩ¢Âú®ËØÑ‰º∞Á±ªÂà´‰∏≠ÊòØÊúÄÂáÜÁ°ÆÁöÑÔºåÁ¥ßÈöèÂÖ∂ÂêéÁöÑÊòØÁî±È¢ÜÂüü‰∏ìÂÆ∂ÊûÑÂª∫ÁöÑÁü•ËØÜÂõæË∞±ÔºåÂõ†ÊûúÊú∫Âô®Â≠¶‰π† (ML) ËøúËøúËêΩÂêé„ÄÇÊàë‰ª¨‰ΩøÁî®Ëøô‰∫õÁªìÊûúÊù•Âº∫Ë∞ÉÂõ†Êûú ML ÁöÑÈáçË¶ÅÂ±ÄÈôêÊÄßÔºåÂÆÉÁªèÂ∏∏‰∫ßÁîüËøùËÉåÂ∏∏ËØÜÁöÑÂõ†ÊûúÂõæÔºåÂΩ±Âìç‰∫∫‰ª¨ÂØπÂÆÉ‰ª¨ÁöÑ‰ø°‰ªª„ÄÇÁÑ∂ËÄåÔºåÊàë‰ª¨Ë°®ÊòéÂ∞Ü GPT-4 ‰∏éÂõ†Êûú ML ÈÖçÂØπÂèØ‰ª•ÂÖãÊúçËøô‰∏ÄÈôêÂà∂Ôºå‰ªéËÄå‰∫ßÁîü‰ªéÁúüÂÆûÊï∞ÊçÆ‰∏≠Â≠¶Âà∞ÁöÑÂõæÂΩ¢ÁªìÊûÑÔºå‰∏éÈ¢ÜÂüü‰∏ìÂÆ∂ËØÜÂà´ÁöÑÁªìÊûÑÁõ∏ÊØîÔºåÊõ¥Á¥ßÂØÜÂú∞‰∏é‰πãÂØπÈΩêÔºåËÄå‰∏çÊòØ‰ªÖÁî±Âõ†Êûú ML Â≠¶Âà∞ÁöÑÁªìÊûÑ„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ° GPT-4 Âπ∂Êú™ÊòéÁ°ÆËÆæËÆ°‰∏∫Âõ†ÊûúÊé®ÁêÜÔºå‰ΩÜÂÆÉ‰ªçÁÑ∂ÂèØ‰ª•Êàê‰∏∫Âõ†ÊûúË°®Á§∫ÁöÑÂÆùË¥µÂ∑•ÂÖ∑ÔºåÂõ†‰∏∫ÂÆÉÊîπËøõ‰∫ÜÊó®Âú®ÊâßË°åÊ≠§Êìç‰ΩúÁöÑÂõ†Êûú ML ÁÆóÊ≥ïÁöÑÂõ†ÊûúÂèëÁé∞ËøáÁ®ã„ÄÇ

##### **Multi-turn Response Selection with Commonsense-enhanced Language Models**
2407.18479v1 by Yuandong Wang, Xuhui Ren, Tong Chen, Yuxiao Dong, Nguyen Quoc Viet Hung, Jie Tang

As a branch of advanced artificial intelligence, dialogue systems are
prospering. Multi-turn response selection is a general research problem in
dialogue systems. With the assistance of background information and pre-trained
language models, the performance of state-of-the-art methods on this problem
gains impressive improvement. However, existing studies neglect the importance
of external commonsense knowledge. Hence, we design a Siamese network where a
pre-trained Language model merges with a Graph neural network (SinLG). SinLG
takes advantage of Pre-trained Language Models (PLMs) to catch the word
correlations in the context and response candidates and utilizes a Graph Neural
Network (GNN) to reason helpful common sense from an external knowledge graph.
The GNN aims to assist the PLM in fine-tuning, and arousing its related
memories to attain better performance. Specifically, we first extract related
concepts as nodes from an external knowledge graph to construct a subgraph with
the context response pair as a super node for each sample. Next, we learn two
representations for the context response pair via both the PLM and GNN. A
similarity loss between the two representations is utilized to transfer the
commonsense knowledge from the GNN to the PLM. Then only the PLM is used to
infer online so that efficiency can be guaranteed. Finally, we conduct
extensive experiments on two variants of the PERSONA-CHAT dataset, which proves
that our solution can not only improve the performance of the PLM but also
achieve an efficient inference.

ÊëòË¶ÅÔºö‰ΩúÁÇ∫È´òÁ¥ö‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰∏ÄÂÄãÂàÜÊîØÔºåÂ∞çË©±Á≥ªÁµ±Ê≠£Ëì¨ÂãÉÁôºÂ±ï„ÄÇÂ§öËº™ÂõûÊáâÁî®Êà∂ÂõûÊáâÈÅ∏ÊìáÊòØÂ∞çË©±Á≥ªÁµ±‰∏≠‰∏ÄÂÄãÈÄöÁî®ÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÂú®ËÉåÊôØË≥áË®äÂíåÈ†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÁöÑÂçîÂä©‰∏ãÔºåÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÂú®Ê≠§ÂïèÈ°å‰∏äÁöÑË°®ÁèæÁç≤Ëá¥‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂ÂøΩÁï•‰∫ÜÂ§ñÈÉ®Â∏∏Ë≠òÁü•Ë≠òÁöÑÈáçË¶ÅÊÄß„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊöπÁæÖÁ∂≤Ë∑ØÔºåÂÖ∂‰∏≠‰∏ÄÂÄãÈ†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãËàá‰∏ÄÂÄãÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàSinLGÔºâÂêà‰Ωµ„ÄÇSinLG Âà©Áî®È†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÔºàPLMÔºâ‰æÜÊçïÊçâË™ûÂ¢ÉÂíåÂõûÊáâÂÄôÈÅ∏‰∏≠ÁöÑË©ûÂΩôÈóúËÅØÔºå‰∏¶Âà©Áî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàGNNÔºâÂæûÂ§ñÈÉ®Áü•Ë≠òÂúñË≠úÊé®ÁêÜÊúâÁî®ÁöÑÂ∏∏Ë≠ò„ÄÇGNN Êó®Âú®ÂçîÂä© PLM ÈÄ≤Ë°åÂæÆË™øÔºå‰∏¶ÂñöÈÜíÂÖ∂Áõ∏ÈóúË®òÊÜ∂‰ª•Áç≤ÂæóÊõ¥Â•ΩÁöÑË°®Áèæ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÂæûÂ§ñÈÉ®Áü•Ë≠òÂúñË≠ú‰∏≠ÊèêÂèñÁõ∏ÈóúÊ¶ÇÂøµ‰ΩúÁÇ∫ÁØÄÈªûÔºå‰ª•ÊßãÂª∫‰∏ÄÂÄãÂ≠êÂúñÔºåÂÖ∂‰∏≠Ë™ûÂ¢ÉÂõûÊáâÂ∞ç‰ΩúÁÇ∫ÊØèÂÄãÁØÑ‰æãÁöÑË∂ÖÁ¥öÁØÄÈªû„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÈÄèÈÅé PLM Âíå GNN ÁÇ∫Ë™ûÂ¢ÉÂõûÊáâÂ∞çÂ≠∏ÁøíÂÖ©ÂÄãË°®Á§∫„ÄÇÂÖ©ÂÄãË°®Á§∫‰πãÈñìÁöÑÁõ∏‰ººÊÄßÊêçÂ§±Áî®ÊñºÂ∞áÂ∏∏Ë≠òÁü•Ë≠òÂæû GNN ËΩâÁßªÂà∞ PLM„ÄÇÁÑ∂ÂæåÂÉÖ‰ΩøÁî® PLM ‰æÜÈÄ≤Ë°åÁ∑ö‰∏äÊé®Ë´ñÔºå‰ª•‰æø‰øùË≠âÊïàÁéá„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ∞ç PERSONA-CHAT Ë≥áÊñôÈõÜÁöÑÂÖ©ÂÄãËÆäÈ´îÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåÈÄôË≠âÊòéÊàëÂÄëÁöÑËß£Ê±∫ÊñπÊ°à‰∏çÂÉÖÂèØ‰ª•ÊèêÈ´ò PLM ÁöÑÊïàËÉΩÔºåÈÇÑËÉΩÂØ¶ÁèæÈ´òÊïàÁöÑÊé®Ë´ñ„ÄÇ

##### **Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning**
2407.18181v1 by Sindhura Kommu, Yizhi Wang, Yue Wang, Xuan Wang

Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing
(scRNA-seq) data is a complex challenge that requires capturing the intricate
relationships between genes and their regulatory interactions. In this study,
we tackle this challenge by leveraging the single-cell BERT-based pre-trained
transformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to
augment structured biological knowledge from existing GRNs. We introduce a
novel joint graph learning approach that combines the rich contextual
representations learned by pre-trained single-cell language models with the
structured knowledge encoded in GRNs using graph neural networks (GNNs). By
integrating these two modalities, our approach effectively reasons over boththe
gene expression level constraints provided by the scRNA-seq data and the
structured biological knowledge inherent in GRNs. We evaluate our method on
human cell benchmark datasets from the BEELINE study with cell type-specific
ground truth networks. The results demonstrate superior performance over
current state-of-the-art baselines, offering a deeper understanding of cellular
regulatory mechanisms.

ÊëòË¶ÅÔºöÂæûÂñÆÁ¥∞ËÉû RNA ÂÆöÂ∫è (scRNA-seq) Ë≥áÊñôÊé®Ë´ñÂü∫Âõ†Ë™øÊéßÁ∂≤Ë∑Ø (GRN) ÊòØ‰∏ÄÈ†ÖË§áÈõúÁöÑÊåëÊà∞ÔºåÈúÄË¶ÅÊéåÊè°Âü∫Âõ†ËàáÂÖ∂Ë™øÊéß‰∫§‰∫í‰ΩúÁî®‰πãÈñìÁöÑË§áÈõúÈóú‰øÇ„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂà©Áî®Âú®Âª£Ê≥õÁöÑÊú™Ê®ôË®ò scRNA-seq Ë≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÂñÆÁ¥∞ËÉû BERT Âü∫ÊñºÈ†êË®ìÁ∑¥ËΩâÊèõÂô®Ê®°Âûã (scBERT)Ôºå‰æÜÂÖãÊúçÊ≠§ÊåëÊà∞Ôºå‰ª•Êì¥ÂÖÖÁèæÊúâ GRN ‰∏≠ÁöÑÁµêÊßãÂåñÁîüÁâ©Áü•Ë≠ò„ÄÇÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÊñ∞Á©éÁöÑËÅØÂêàÂúñÂΩ¢Â≠∏ÁøíÊñπÊ≥ïÔºåÂÆÉÁµêÂêà‰∫ÜÈ†êË®ìÁ∑¥ÂñÆÁ¥∞ËÉûË™ûË®ÄÊ®°ÂûãÊâÄÂ≠∏ÁøíÂà∞ÁöÑË±êÂØåËÑàÁµ°Ë°®ÂæµÔºå‰ª•Âèä‰ΩøÁî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∞ç GRN ‰∏≠Á∑®Á¢ºÁöÑÁµêÊßãÂåñÁü•Ë≠ò„ÄÇÈÄèÈÅéÊï¥ÂêàÈÄôÂÖ©Á®ÆÊñπÂºèÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊúâÊïàÂú∞Â∞ç scRNA-seq Ë≥áÊñôÊèê‰æõÁöÑÂü∫Âõ†Ë°®ÁèæÂ±§Á¥öÁ¥ÑÊùüÂíå GRN ‰∏≠Âõ∫ÊúâÁöÑÁµêÊßãÂåñÁîüÁâ©Áü•Ë≠òÈÄ≤Ë°åÊé®ÁêÜ„ÄÇÊàëÂÄë‰ΩøÁî® BEELINE Á†îÁ©∂‰∏≠ÁöÑ‰∫∫È°ûÁ¥∞ËÉûÂü∫Ê∫ñË≥áÊñôÈõÜÔºå‰ª•ÂèäÁ¥∞ËÉûÈ°ûÂûãÁâπÂÆöÁöÑÂü∫Êú¨‰∫ãÂØ¶Á∂≤Ë∑ØÔºå‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÊñπÊ≥ï„ÄÇÁµêÊûúË≠âÊòéÂÖ∂ÊïàËÉΩÂÑ™ÊñºÁõÆÂâçÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÔºåÊèê‰æõ‰∫ÜÂ∞çÁ¥∞ËÉûË™øÊéßÊ©üÂà∂ÁöÑÊõ¥Ê∑±ÂÖ•ÁêÜËß£„ÄÇ

##### **MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents**
2407.17544v1 by Arya Bulusu, Brandon Man, Ashish Jagmohan, Aditya Vempaty, Jennifer Mari-Wyka, Deepak Akkil

There has been significant recent interest in harnessing LLMs to control
software systems through multi-step reasoning, planning and tool-usage. While
some promising results have been obtained, application to specific domains
raises several general issues including the control of specialized domain
tools, the lack of existing datasets for training and evaluation, and the
non-triviality of automated system evaluation and improvement. In this paper,
we present a case-study where we examine these issues in the context of a
specific domain. Specifically, we present an automated math visualizer and
solver system for mathematical pedagogy. The system orchestrates mathematical
solvers and math graphing tools to produce accurate visualizations from simple
natural language commands. We describe the creation of specialized data-sets,
and also develop an auto-evaluator to easily evaluate the outputs of our system
by comparing them to ground-truth expressions. We have open sourced the
data-sets and code for the proposed system.

ÊëòË¶ÅÔºöÊúÄËøëÔºå‰∫∫‰ª¨ÂØπÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Êù•ÈÄöËøáÂ§öÊ≠•È™§Êé®ÁêÜ„ÄÅËßÑÂàíÂíåÂ∑•ÂÖ∑‰ΩøÁî®Êù•ÊéßÂà∂ËΩØ‰ª∂Á≥ªÁªü‰∫ßÁîü‰∫ÜÊûÅÂ§ßÁöÑÂÖ¥Ë∂£„ÄÇËôΩÁÑ∂Â∑≤ÁªèÂèñÂæó‰∫Ü‰∏Ä‰∫õÊúâÂ∏åÊúõÁöÑÁªìÊûúÔºå‰ΩÜÂ∫îÁî®‰∫éÁâπÂÆöÈ¢ÜÂüü‰ºöÂºïÂèëÂá†‰∏™ÊôÆÈÅçÊÄßÈóÆÈ¢òÔºåÂåÖÊã¨ÂØπ‰∏ì‰∏öÈ¢ÜÂüüÂ∑•ÂÖ∑ÁöÑÊéßÂà∂„ÄÅÁº∫‰πèÁî®‰∫éËÆ≠ÁªÉÂíåËØÑ‰º∞ÁöÑÁé∞ÊúâÊï∞ÊçÆÈõÜÔºå‰ª•ÂèäËá™Âä®ÂåñÁ≥ªÁªüËØÑ‰º∞ÂíåÊîπËøõÁöÑÈùûÂπ≥Âá°ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ê°à‰æãÁ†îÁ©∂ÔºåÂÖ∂‰∏≠Êàë‰ª¨Á†îÁ©∂‰∫ÜÁâπÂÆöÈ¢ÜÂüüËÉåÊôØ‰∏ãÁöÑËøô‰∫õÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Â±ïÁ§∫‰∫Ü‰∏Ä‰∏™Áî®‰∫éÊï∞Â≠¶ÊïôËÇ≤ÁöÑËá™Âä®ÂåñÊï∞Â≠¶ÂèØËßÜÂåñÂô®ÂíåÊ±ÇËß£Âô®Á≥ªÁªü„ÄÇËØ•Á≥ªÁªüÂçèË∞ÉÊï∞Â≠¶Ê±ÇËß£Âô®ÂíåÊï∞Â≠¶ÁªòÂõæÂ∑•ÂÖ∑Ôºå‰ª•Ê†πÊçÆÁÆÄÂçïÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂëΩ‰ª§ÁîüÊàêÂáÜÁ°ÆÁöÑÂèØËßÜÂåñÊïàÊûú„ÄÇÊàë‰ª¨ÊèèËø∞‰∫Ü‰∏ìÈó®Êï∞ÊçÆÈõÜÁöÑÂàõÂª∫ÔºåËøòÂºÄÂèë‰∫Ü‰∏Ä‰∏™Ëá™Âä®ËØÑ‰º∞Âô®ÔºåÈÄöËøáÂ∞ÜÊàë‰ª¨ÁöÑÁ≥ªÁªüËæìÂá∫‰∏éÁúüÂÆûË°®ËææÂºèËøõË°åÊØîËæÉÔºåËΩªÊùæËØÑ‰º∞ÂÖ∂ËæìÂá∫„ÄÇÊàë‰ª¨Â∑≤ÁªèÂºÄÊ∫ê‰∫ÜÊâÄÊèêËÆÆÁ≥ªÁªüÁöÑ‰ª£Á†ÅÂíåÊï∞ÊçÆÈõÜ„ÄÇ

##### **Ranking protein-protein models with large language models and graph neural networks**
2407.16375v1 by Xiaotong Xu, Alexandre M. J. J. Bonvin

Protein-protein interactions (PPIs) are associated with various diseases,
including cancer, infections, and neurodegenerative disorders. Obtaining
three-dimensional structural information on these PPIs serves as a foundation
to interfere with those or to guide drug design. Various strategies can be
followed to model those complexes, all typically resulting in a large number of
models. A challenging step in this process is the identification of good models
(near-native PPI conformations) from the large pool of generated models. To
address this challenge, we previously developed DeepRank-GNN-esm, a graph-based
deep learning algorithm for ranking modelled PPI structures harnessing the
power of protein language models. Here, we detail the use of our software with
examples. DeepRank-GNN-esm is freely available at
https://github.com/haddocking/DeepRank-GNN-esm

ÊëòË¶ÅÔºöËõãÁôΩ-ËõãÁôΩ‰∫§‰∫í‰ΩúÁî® (PPI) ËàáÂêÑÁ®ÆÁñæÁóÖÁõ∏ÈóúÔºåÂåÖÊã¨ÁôåÁóá„ÄÅÊÑüÊüìÂíåÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖ„ÄÇÂèñÂæóÈÄô‰∫õ PPI ÁöÑ‰∏âÁ∂≠ÁµêÊßãË≥áË®äÔºå‰ΩúÁÇ∫Âπ≤ÊìæÂÆÉÂÄëÊàñÂºïÂ∞éËó•Áâ©Ë®≠Ë®àÁöÑÂü∫Á§é„ÄÇÂèØ‰ª•ÈÅµÂæ™ÂêÑÁ®ÆÁ≠ñÁï•‰æÜÂª∫Ê®°ÈÄô‰∫õË§áÂêàÈ´îÔºåÊâÄÊúâÈÄô‰∫õÁ≠ñÁï•ÈÄöÂ∏∏ÊúÉÁî¢ÁîüÂ§ßÈáèÁöÑÊ®°Âûã„ÄÇÊ≠§ÈÅéÁ®ã‰∏≠ÁöÑÊåëÊà∞ÊÄßÊ≠•È©üÔºåÊòØÂæûÂ§ßÈáèÁî¢ÁîüÁöÑÊ®°Âûã‰∏≠ÊâæÂá∫Â•ΩÁöÑÊ®°ÂûãÔºàÊé•ËøëÂéüÁîü PPI ÊßãË±°Ôºâ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÂÄãÊåëÊà∞ÔºåÊàëÂÄë‰πãÂâçÈñãÁôº‰∫Ü DeepRank-GNN-esmÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂü∫ÊñºÂúñÂΩ¢ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ïÔºåÁî®ÊñºÂ∞çÂª∫Ê®°ÁöÑ PPI ÁµêÊßãÈÄ≤Ë°åÊéíÂêçÔºåÂà©Áî®ËõãÁôΩË≥™Ë™ûË®ÄÊ®°ÂûãÁöÑÂäõÈáè„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëË©≥Á¥∞Ë™™Êòé‰∫ÜÊàëÂÄëËªüÈ´îÁöÑ‰ΩøÁî®ÁØÑ‰æã„ÄÇDeepRank-GNN-esm ÂèØÂú® https://github.com/haddocking/DeepRank-GNN-esm ÂÖçË≤ªÂèñÂæó

##### **PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets**
2407.16329v1 by Jaeyoung Kim, Sihyeon Lee, Hyeon Jeon, Keon-Joo Lee, Hee-Joon Bae, Bohyoung Kim, Jinwook Seo

Acute stroke demands prompt diagnosis and treatment to achieve optimal
patient outcomes. However, the intricate and irregular nature of clinical data
associated with acute stroke, particularly blood pressure (BP) measurements,
presents substantial obstacles to effective visual analytics and
decision-making. Through a year-long collaboration with experienced
neurologists, we developed PhenoFlow, a visual analytics system that leverages
the collaboration between human and Large Language Models (LLMs) to analyze the
extensive and complex data of acute ischemic stroke patients. PhenoFlow
pioneers an innovative workflow, where the LLM serves as a data wrangler while
neurologists explore and supervise the output using visualizations and natural
language interactions. This approach enables neurologists to focus more on
decision-making with reduced cognitive load. To protect sensitive patient
information, PhenoFlow only utilizes metadata to make inferences and synthesize
executable codes, without accessing raw patient data. This ensures that the
results are both reproducible and interpretable while maintaining patient
privacy. The system incorporates a slice-and-wrap design that employs temporal
folding to create an overlaid circular visualization. Combined with a linear
bar graph, this design aids in exploring meaningful patterns within irregularly
measured BP data. Through case studies, PhenoFlow has demonstrated its
capability to support iterative analysis of extensive clinical datasets,
reducing cognitive load and enabling neurologists to make well-informed
decisions. Grounded in long-term collaboration with domain experts, our
research demonstrates the potential of utilizing LLMs to tackle current
challenges in data-driven clinical decision-making for acute ischemic stroke
patients.

ÊëòË¶ÅÔºö<paragraph>ÊÄ•ÊÄß‰∏≠È¢®ÈúÄË¶ÅËøÖÈÄüË®∫Êñ∑ÂíåÊ≤ªÁôÇÔºåÊâçËÉΩÈÅîÂà∞ÊúÄ‰Ω≥ÁöÑÁóÖ‰∫∫Ê≤ªÁôÇÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåËàáÊÄ•ÊÄß‰∏≠È¢®Áõ∏ÈóúÁöÑËá®Â∫äË≥áÊñôË§áÈõú‰∏î‰∏çË¶èÂâáÔºåÁâπÂà•ÊòØË°ÄÂ£ì (BP) Ê∏¨ÈáèÔºåÂ∞çÊúâÊïàÁöÑË¶ñË¶∫ÂàÜÊûêÂíåÊ±∫Á≠ñÂà∂ÂÆöÊßãÊàêÈáçÂ§ßÈöúÁ§ô„ÄÇÈÄèÈÅéËàáÁ∂ìÈ©óË±êÂØåÁöÑÁ•ûÁ∂ìÁßëÈÜ´Â∏´Èï∑ÈÅî‰∏ÄÂπ¥ÁöÑÂêà‰ΩúÔºåÊàëÂÄëÈñãÁôº‰∫Ü PhenoFlowÔºåÈÄôÊòØ‰∏ÄÂÄãË¶ñË¶∫ÂàÜÊûêÁ≥ªÁµ±ÔºåÂà©Áî®‰∫∫ËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰πãÈñìÁöÑÂçî‰Ωú‰æÜÂàÜÊûêÊÄ•ÊÄßÁº∫Ë°ÄÊÄß‰∏≠È¢®ÊÇ£ËÄÖÁöÑÂª£Ê≥õ‰∏îË§áÈõúË≥áÊñô„ÄÇPhenoFlow ÈñãÂâµ‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÖ∂‰∏≠ LLM Êìî‰ªªË≥áÊñôÊï¥ÁêÜÂì°ÔºåËÄåÁ•ûÁ∂ìÁßëÈÜ´Â∏´Ââá‰ΩøÁî®Ë¶ñË¶∫ÂåñÂíåËá™ÁÑ∂Ë™ûË®Ä‰∫íÂãï‰æÜÊé¢Á¥¢ÂíåÁõ£Áù£Ëº∏Âá∫„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰ΩøÁ•ûÁ∂ìÁßëÈÜ´Â∏´ËÉΩÂ§†Êõ¥Â∞àÊ≥®ÊñºÊ±∫Á≠ñÂà∂ÂÆöÔºåÂêåÊôÇÈôç‰ΩéË™çÁü•Ë≤†Êìî„ÄÇÁÇ∫‰∫Ü‰øùË≠∑ÊïèÊÑüÁöÑÁóÖ‰∫∫Ë≥áË®äÔºåPhenoFlow ÂÉÖÂà©Áî®ÂÖÉË≥áÊñôÈÄ≤Ë°åÊé®Ë´ñ‰∏¶ÂêàÊàêÂèØÂü∑Ë°åÁ®ãÂºèÁ¢ºÔºåËÄå‰∏çÊúÉÂ≠òÂèñÂéüÂßãÁóÖ‰∫∫Ë≥áÊñô„ÄÇÈÄôÁ¢∫‰øù‰∫ÜÁµêÊûúÊó¢ÂèØÈáçÁèæÂèàÂèØËß£ÈáãÔºåÂêåÊôÇÁ∂≠Ë≠∑ÁóÖ‰∫∫ÁöÑÈö±ÁßÅ„ÄÇË©≤Á≥ªÁµ±Êé°Áî®ÂàÜÊÆµÂíåÂåÖË£ùË®≠Ë®àÔºåÊé°Áî®ÊôÇÈñìÊë∫Áñä‰æÜÂª∫Á´ãÁñäÂä†ÁöÑÂúìÂΩ¢Ë¶ñË¶∫Âåñ„ÄÇÁµêÂêàÁ∑öÊÄßÈï∑Ê¢ùÂúñÔºåÊ≠§Ë®≠Ë®àÊúâÂä©ÊñºÊé¢Á¥¢‰∏çË¶èÂâáÊ∏¨ÈáèË°ÄÂ£ìË≥áÊñô‰∏≠ÁöÑÊúâÊÑèÁæ©Ê®°Âºè„ÄÇÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÔºåPhenoFlow Â∑≤Ë≠âÊòéÂÖ∂ÊîØÊè¥Â∞çÂª£Ê≥õËá®Â∫äË≥áÊñôÈõÜÈÄ≤Ë°åÂèçË¶ÜÂàÜÊûêÁöÑËÉΩÂäõÔºåÈôç‰ΩéË™çÁü•Ë≤†Êìî‰∏¶‰ΩøÁ•ûÁ∂ìÁßëÈÜ´Â∏´ËÉΩÂ§†ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰ª•ËàáÈ†òÂüüÂ∞àÂÆ∂Èï∑ÊúüÂêà‰ΩúÁÇ∫Âü∫Á§éÔºåË≠âÊòé‰∫ÜÂà©Áî® LLM ‰æÜÊáâÂ∞çÁï∂ÂâçÊÄ•ÊÄßÁº∫Ë°ÄÊÄß‰∏≠È¢®ÊÇ£ËÄÖË≥áÊñôÈ©ÖÂãïËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÊåëÊà∞ÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **Graph-Structured Speculative Decoding**
2407.16207v1 by Zhuocheng Gong, Jiahao Liu, Ziyue Wang, Pengfei Wu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan

Speculative decoding has emerged as a promising technique to accelerate the
inference of Large Language Models (LLMs) by employing a small language model
to draft a hypothesis sequence, which is then validated by the LLM. The
effectiveness of this approach heavily relies on the balance between
performance and efficiency of the draft model. In our research, we focus on
enhancing the proportion of draft tokens that are accepted to the final output
by generating multiple hypotheses instead of just one. This allows the LLM more
options to choose from and select the longest sequence that meets its
standards. Our analysis reveals that hypotheses produced by the draft model
share many common token sequences, suggesting a potential for optimizing
computation. Leveraging this observation, we introduce an innovative approach
utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This
structure enables us to efficiently predict and merge recurring token
sequences, vastly reducing the computational demands of the draft model. We
term this approach Graph-structured Speculative Decoding (GSD). We apply GSD
across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and
observe a remarkable speedup of 1.73$\times$ to 1.96$\times$, significantly
surpassing standard speculative decoding.

ÊëòË¶ÅÔºö<paragraph>Êé®Ê∏¨ÊÄßËß£Á¢ºÂ∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊäÄË°ìÔºåÂèØÈÄöÈÅé‰ΩøÁî®Â∞èÂûãË™ûË®ÄÊ®°ÂûãËµ∑ËçâÂÅáË®≠Â∫èÂàóÔºåÁÑ∂ÂæåÁî±Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) È©óË≠âË©≤Â∫èÂàóÔºåÂæûËÄåÂä†ÈÄüÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜ„ÄÇÊ≠§ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºËçâÁ®øÊ®°ÂûãÁöÑÊÄßËÉΩÂíåÊïàÁéá‰πãÈñìÁöÑÂπ≥Ë°°„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÈÄöÈÅéÁîüÊàêÂ§öÂÄãÂÅáË®≠ËÄå‰∏çÊòØÂè™ÁîüÊàê‰∏ÄÂÄãÂÅáË®≠‰æÜÊèêÈ´òË¢´Êé•ÂèóÁÇ∫ÊúÄÁµÇËº∏Âá∫ÁöÑËçâÁ®ø‰ª§ÁâåÁöÑÊØî‰æã„ÄÇÈÄôÂÖÅË®± LLM Âæû‰∏≠ÈÅ∏ÊìáÊõ¥Â§öÈÅ∏È†ÖÔºå‰∏¶ÈÅ∏ÊìáÁ¨¶ÂêàÂÖ∂Ê®ôÊ∫ñÁöÑÊúÄÈï∑Â∫èÂàó„ÄÇÊàëÂÄëÁöÑÂàÜÊûêË°®ÊòéÔºåËçâÁ®øÊ®°ÂûãÁî¢ÁîüÁöÑÂÅáË®≠ÂÖ±‰∫´Ë®±Â§öÂÖ¨ÂÖ±‰ª§ÁâåÂ∫èÂàóÔºåÈÄôË°®ÊòéÂÑ™ÂåñË®àÁÆóÁöÑÂèØËÉΩÊÄß„ÄÇÂà©Áî®ÈÄô‰∏ÄËßÄÂØüÁµêÊûúÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂà©Áî®ÊúâÂêëÁÑ°Áí∞Âúñ (DAG) ‰æÜÁÆ°ÁêÜÂ∑≤Á∑®Âà∂ÁöÑÂÅáË®≠„ÄÇÈÄôÁ®ÆÁµêÊßã‰ΩøÊàëÂÄëËÉΩÂ§†ÊúâÊïàÂú∞È†êÊ∏¨ÂíåÂêà‰ΩµÈáçË§áÁöÑ‰ª§ÁâåÂ∫èÂàóÔºåÂæûËÄåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜËçâÁ®øÊ®°ÂûãÁöÑË®àÁÆóÈúÄÊ±Ç„ÄÇÊàëÂÄëÂ∞áÈÄôÁ®ÆÊñπÊ≥ïÁ®±ÁÇ∫ÂúñÁµêÊßãÊé®Ê∏¨ÊÄßËß£Á¢º (GSD)„ÄÇÊàëÂÄëÂ∞á GSD ÊáâÁî®Êñº‰∏ÄÁ≥ªÂàó LLMÔºåÂåÖÊã¨‰∏ÄÂÄã 700 ÂÑÑÂèÉÊï∏ÁöÑ LLaMA-2 Ê®°ÂûãÔºå‰∏¶ËßÄÂØüÂà∞È°ØËëóÁöÑÂä†ÈÄüÔºåÂæû 1.73 ÂÄçÂà∞ 1.96 ÂÄçÔºåÈ°ØËëóË∂ÖÈÅéÊ®ôÊ∫ñÊé®Ê∏¨ÊÄßËß£Á¢º„ÄÇ</paragraph>

##### **Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval**
2407.21049v1 by Yannick Assogba, Donghao Ren

As language models support larger and larger context sizes, evaluating their
ability to make effective use of that context becomes increasingly important.
We analyze the ability of several code generation models to handle long range
dependencies using a suite of multi-step key retrieval tasks in context windows
up to 8k tokens in length. The tasks progressively increase in difficulty and
allow more nuanced evaluation of model capabilities than tests like the popular
needle-in-the-haystack test. We find that performance degrades significantly
(up to 2x) when a function references another function that is defined later in
the prompt. We also observe that models that use sliding window attention
mechanisms have difficulty handling references further than the size of a
single window. We perform simple prompt modifications using call graph
information to improve multi-step retrieval performance up to 3x. Our analysis
highlights different facets of long-context performance and is suggestive of
prompt construction strategies for code completion tools

ÊëòË¶ÅÔºöÈö®ËëóË™ûË®ÄÊ®°ÂûãÊîØÊè¥ÁöÑÂÖßÂÆπÂ§ßÂ∞èË∂ä‰æÜË∂äÂ§ßÔºåË©ï‰º∞ÂÖ∂ÊúâÊïàÂà©Áî®Ë©≤ÂÖßÂÆπÁöÑËÉΩÂäõËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÊàëÂÄëÂàÜÊûê‰∫ÜÂπæÂÄãÁ®ãÂºèÁ¢ºÁîüÊàêÊ®°ÂûãËôïÁêÜÈï∑Ë∑ùÈõ¢‰æùË≥¥Èóú‰øÇÁöÑËÉΩÂäõÔºå‰ΩøÁî®‰∏ÄÁµÑÂ§öÊ≠•È©üÈóúÈçµÊ™¢Á¥¢‰ªªÂãôÔºåÂú®Èï∑ÈÅî 8k ‰ª§ÁâåÁöÑÂÖßÂÆπË¶ñÁ™ó‰∏≠„ÄÇ‰ªªÂãôÈÄêÊº∏Â¢ûÂä†Èõ£Â∫¶Ôºå‰∏¶ÂÖÅË®±Â∞çÊ®°ÂûãÂäüËÉΩÈÄ≤Ë°åÊØîÊµÅË°åÁöÑÈáùÈ†≠‰πæËçâÂ†ÜÊ∏¨Ë©¶Êõ¥Á¥∞Á∑ªÁöÑË©ï‰º∞„ÄÇÊàëÂÄëÁôºÁèæÔºåÁï∂ÂáΩÂºèÂèÉÁÖßÁ®çÂæåÂú®ÊèêÁ§∫‰∏≠ÂÆöÁæ©ÁöÑÂè¶‰∏ÄÂÄãÂáΩÂºèÊôÇÔºåÊïàËÉΩÊúÉÈ°ØËëó‰∏ãÈôçÔºàÊúÄÂ§ö 2 ÂÄçÔºâ„ÄÇÊàëÂÄëÈÇÑËßÄÂØüÂà∞Ôºå‰ΩøÁî®ÊªëÂãïË¶ñÁ™óÊ≥®ÊÑèÊ©üÂà∂ÁöÑÊ®°ÂûãÈõ£‰ª•ËôïÁêÜË∂ÖÂá∫ÂñÆ‰∏ÄË¶ñÁ™óÂ§ßÂ∞èÁöÑÂèÉÁÖß„ÄÇÊàëÂÄë‰ΩøÁî®ÂëºÂè´ÂúñÂΩ¢Ë≥áË®äÂü∑Ë°åÁ∞°ÂñÆÁöÑÊèêÁ§∫‰øÆÊîπÔºå‰ª•Â∞áÂ§öÊ≠•È©üÊ™¢Á¥¢ÊïàËÉΩÊèêÂçáËá≥ 3 ÂÄç„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÁ™ÅÈ°Ø‰∫ÜÈï∑ÂÖßÂÆπÊïàËÉΩÁöÑ‰∏çÂêåÈù¢ÂêëÔºå‰∏¶ÊöóÁ§∫‰∫ÜÁ®ãÂºèÁ¢ºÂÆåÊàêÂ∑•ÂÖ∑ÁöÑÊèêÁ§∫Âª∫ÊßãÁ≠ñÁï•

##### **Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion**
2407.16127v1 by Yang Liu, Xiaobin Tian, Zequn Sun, Wei Hu

Traditional knowledge graph (KG) completion models learn embeddings to
predict missing facts. Recent works attempt to complete KGs in a
text-generation manner with large language models (LLMs). However, they need to
ground the output of LLMs to KG entities, which inevitably brings errors. In
this paper, we present a finetuning framework, DIFT, aiming to unleash the KG
completion ability of LLMs and avoid grounding errors. Given an incomplete
fact, DIFT employs a lightweight model to obtain candidate entities and
finetunes an LLM with discrimination instructions to select the correct one
from the given candidates. To improve performance while reducing instruction
data, DIFT uses a truncated sampling method to select useful facts for
finetuning and injects KG embeddings into the LLM. Extensive experiments on
benchmark datasets demonstrate the effectiveness of our proposed framework.

ÊëòË¶ÅÔºöÂÇ≥Áµ±Áü•Ë≠òÂúñË≠úÔºàKGÔºâÂÆåÊàêÂäüËÉΩÊ®°ÂûãÂ≠∏ÁøíÂµåÂÖ•Ôºå‰ª•È†êÊ∏¨ÈÅ∫Â§±ÁöÑ‰∫ãÂØ¶„ÄÇÊúÄËøëÁöÑÂ∑•‰ΩúÂòóË©¶‰ª•Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª•ÊñáÂ≠óÁîüÊàêÁöÑÊñπÂºèÂÆåÊàê KG„ÄÇÁÑ∂ËÄåÔºå‰ªñÂÄëÈúÄË¶ÅÂ∞á LLM ÁöÑËº∏Âá∫Âü∫Á§éÂª∫Á´ãÂú® KG ÂØ¶È´î‰∏äÔºåÈÄô‰∏çÂèØÈÅøÂÖçÂú∞ÊúÉÂ∏∂‰æÜÈåØË™§„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂæÆË™øÊ°ÜÊû∂ DIFTÔºåÊó®Âú®ÈáãÊîæ LLM ÁöÑ KG ÂÆåÊàêÂäüËÉΩÔºå‰∏¶ÈÅøÂÖçÂü∫Á§éÈåØË™§„ÄÇÁµ¶ÂÆö‰∏ÄÂÄã‰∏çÂÆåÊï¥ÁöÑ‰∫ãÂØ¶ÔºåDIFT ‰ΩøÁî®‰∏ÄÂÄãËºïÈáèÁ¥öÊ®°Âûã‰æÜÁç≤ÂæóÂÄôÈÅ∏ÂØ¶È´îÔºå‰∏¶ÂæÆË™ø‰∏ÄÂÄã LLMÔºå‰∏¶‰ΩøÁî®Ëæ®Âà•Êåá‰ª§ÂæûÁµ¶ÂÆöÁöÑÂÄôÈÅ∏È†Ö‰∏≠ÈÅ∏ÊìáÊ≠£Á¢∫ÁöÑÂØ¶È´î„ÄÇÁÇ∫‰∫ÜÂú®Ê∏õÂ∞ëÊåá‰ª§Êï∏ÊìöÁöÑÂêåÊôÇÊèêÂçáÊïàËÉΩÔºåDIFT ‰ΩøÁî®‰∏ÄÂÄãÊà™Êñ∑ÊäΩÊ®£ÊñπÊ≥ï‰æÜÈÅ∏ÊìáÊúâÁî®ÁöÑ‰∫ãÂØ¶‰ª•ÈÄ≤Ë°åÂæÆË™øÔºå‰∏¶Â∞á KG ÂµåÂÖ•Ê≥®ÂÖ•Âà∞ LLM ‰∏≠„ÄÇÂú®Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊ°ÜÊû∂ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts**
2407.15588v1 by Soojin Yoon, Sungho Ko, Tongyoung Kim, SeongKu Kang, Jinyoung Yeo, Dongha Lee

Cross-lingual entity alignment (EA) enables the integration of multiple
knowledge graphs (KGs) across different languages, providing users with
seamless access to diverse and comprehensive knowledge.Existing methods, mostly
supervised, face challenges in obtaining labeled entity pairs. To address this,
recent studies have shifted towards a self-supervised and unsupervised
frameworks. Despite their effectiveness, these approaches have limitations: (1)
they mainly focus on entity features, neglecting the semantic information of
relations, (2) they assume isomorphism between source and target graphs,
leading to noise and reduced alignment accuracy, and (3) they are susceptible
to noise in the textual features, especially when encountering inconsistent
translations or Out-Of-Vocabulary (OOV) problems.
  In this paper, we propose ERAlign, an unsupervised and robust cross-lingual
EA framework that jointly performs Entity-level and Relation-level Alignment
using semantic textual features of relations and entities. Its refinement
process iteratively enhances results by fusing entity-level and relation-level
alignments based on neighbor triple matching. The additional verification
process examines the entities' neighbor triples as the linearized text. This
\textit{Align-and-Verify} pipeline that rigorously assesses alignment results,
achieving near-perfect alignment even in the presence of noisy textual features
of entities. Our extensive experiments demonstrate that robustness and general
applicability of \proposed improved the accuracy and effectiveness of EA tasks,
contributing significantly to knowledge-oriented applications.

ÊëòË¶ÅÔºöË∑®Ë™ûË®ÄÂØ¶È´îÂ∞çÈΩä (EA) ËÉΩÂ§†Êï¥Âêà‰∏çÂêåË™ûË®Ä‰∏≠ÁöÑÂ§öÂÄãÁü•Ë≠òÂúñË≠ú (KG)ÔºåËÆì‰ΩøÁî®ËÄÖËÉΩÁÑ°Á∏´Âú∞Â≠òÂèñÂ§öÂÖÉ‰∏îÂÖ®Èù¢ÁöÑÁü•Ë≠ò„ÄÇÁèæÊúâÊñπÊ≥ïÂ§ßÂ§öÊòØÊúâÁõ£Áù£ÁöÑÔºåÂú®ÂèñÂæóÊ®ôË®òÂØ¶È´îÂ∞çÊôÇÈù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊúÄËøëÁöÑÁ†îÁ©∂Â∑≤ËΩâÂêëËá™Áõ£Áù£ÂíåÁÑ°Áõ£Áù£ÁöÑÊû∂Êßã„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊñπÊ≥ïÂæàÊúâÊïàÔºå‰ΩÜÂÆÉÂÄëÊúâ‰ª•‰∏ãÈôêÂà∂Ôºö(1) ÂÆÉÂÄë‰∏ªË¶ÅÈóúÊ≥®ÂØ¶È´îÁâπÂæµÔºåÂøΩÁï•Èóú‰øÇÁöÑË™ûÁæ©Ë≥áË®äÔºå(2) ÂÆÉÂÄëÂÅáË®≠‰æÜÊ∫êÂúñË≠úÂíåÁõÆÊ®ôÂúñË≠ú‰πãÈñìÂêåÊßãÔºåÂ∞éËá¥ÈõúË®äÂíåÂ∞çÈΩäÊ∫ñÁ¢∫Â∫¶Èôç‰ΩéÔºå(3) ÂÆÉÂÄëÂÆπÊòìÂèóÂà∞ÊñáÂ≠óÁâπÂæµ‰∏≠ÁöÑÈõúË®äÂΩ±ÈüøÔºåÁâπÂà•ÊòØÂú®ÈÅáÂà∞‰∏ç‰∏ÄËá¥ÁöÑÁøªË≠ØÊàñË©ûÂΩôÂ§ñÂïèÈ°å (OOV) ÊôÇ„ÄÇ
Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ ERAlignÔºå‰∏ÄÂÄãÁÑ°Áõ£Áù£‰∏îÁ©©ÂÅ•ÁöÑË∑®Ë™ûË®Ä EA Êû∂ÊßãÔºåÂÆÉ‰ΩøÁî®Èóú‰øÇÂíåÂØ¶È´îÁöÑË™ûÁæ©ÊñáÂ≠óÁâπÂæµÔºåÂêåÊôÇÂü∑Ë°åÂØ¶È´îÂ±§Á¥öÂíåÈóú‰øÇÂ±§Á¥öÂ∞çÈΩä„ÄÇÂÆÉÁöÑÁ≤æÁÖâÁ®ãÂ∫èÈÄèÈÅéÊ†πÊìöÈÑ∞Êé•‰∏âÂÖÉÁµÑÂåπÈÖçËûçÂêàÂØ¶È´îÂ±§Á¥öÂíåÈóú‰øÇÂ±§Á¥öÂ∞çÈΩäÔºåÂèçË¶ÜÂ¢ûÂº∑ÁµêÊûú„ÄÇÈ°çÂ§ñÁöÑÈ©óË≠âÁ®ãÂ∫èÂ∞áÂØ¶È´îÁöÑÈÑ∞Êé•‰∏âÂÖÉÁµÑË¶ñÁÇ∫Á∑öÊÄßÂåñÊñáÂ≠óÈÄ≤Ë°åÊ™¢Êü•„ÄÇÈÄôÂÄãÂö¥Ê†ºË©ï‰º∞Â∞çÈΩäÁµêÊûúÁöÑ„ÄåÂ∞çÈΩäÂíåÈ©óË≠â„ÄçÁÆ°Á∑öÔºåÂç≥‰ΩøÂú®Â≠òÂú®ÂØ¶È´îÁöÑÈõúË®äÊñáÂ≠óÁâπÂæµÊôÇ‰πüËÉΩÈÅîÊàêËøë‰πéÂÆåÁæéÁöÑÂ∞çÈΩä„ÄÇÊàëÂÄëÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºå\proposed ÁöÑÁ©©ÂÅ•ÊÄßÂíåÊôÆÈÅçÈÅ©Áî®ÊÄßÊèêÂçá‰∫Ü EA ‰ªªÂãôÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÊúâÊïàÊÄßÔºåÂ∞çÁü•Ë≠òÂ∞éÂêëÊáâÁî®Á®ãÂºèÊúâÈ°ØËëóÁöÑË≤¢Áçª„ÄÇ

##### **The Ontoverse: Democratising Access to Knowledge Graph-based Data Through a Cartographic Interface**
2408.03339v1 by Johannes Zimmermann, Dariusz Wiktorek, Thomas Meusburger, Miquel Monge-Dalmau, Antonio Fabregat, Alexander Jarasch, G√ºnter Schmidt, Jorge S. Reis-Filho, T. Ian Simpson

As the number of scientific publications and preprints is growing
exponentially, several attempts have been made to navigate this complex and
increasingly detailed landscape. These have almost exclusively taken
unsupervised approaches that fail to incorporate domain knowledge and lack the
structural organisation required for intuitive interactive human exploration
and discovery. Especially in highly interdisciplinary fields, a deep
understanding of the connectedness of research works across topics is essential
for generating insights. We have developed a unique approach to data navigation
that leans on geographical visualisation and uses hierarchically structured
domain knowledge to enable end-users to explore knowledge spaces grounded in
their desired domains of interest. This can take advantage of existing
ontologies, proprietary intelligence schemata, or be directly derived from the
underlying data through hierarchical topic modelling. Our approach uses natural
language processing techniques to extract named entities from the underlying
data and normalise them against relevant domain references and navigational
structures. The knowledge is integrated by first calculating similarities
between entities based on their shared extracted feature space and then by
alignment to the navigational structures. The result is a knowledge graph that
allows for full text and semantic graph query and structured topic driven
navigation. This allows end-users to identify entities relevant to their needs
and access extensive graph analytics. The user interface facilitates graphical
interaction with the underlying knowledge graph and mimics a cartographic map
to maximise ease of use and widen adoption. We demonstrate an exemplar project
using our generalisable and scalable infrastructure for an academic biomedical
literature corpus that is grounded against hundreds of different named domain
entities.

ÊëòË¶ÅÔºö<paragraph>Èö®ËëóÁßëÂ≠∏Âá∫ÁâàÁâ©ÂíåÈ†êÂç∞Êú¨Êï∏ÈáèÂëàÊåáÊï∏Â¢ûÈï∑ÔºåÂ∑≤Á∂ìÈÄ≤Ë°å‰∫ÜÂ§öÈ†ÖÂòóË©¶‰æÜÊé¢Á¥¢ÈÄôÂÄãË§áÈõú‰∏îÊó•ÁõäË©≥Á¥∞ÁöÑÈ†òÂüü„ÄÇÈÄô‰∫õÂòóË©¶Âπæ‰πéÂÆåÂÖ®Êé°Áî®‰∫ÜÁÑ°Ê≥ïÁ¥çÂÖ•È†òÂüüÁü•Ë≠ò‰∏îÁº∫‰πèÁõ¥ËßÄ‰∫íÂãïÂºè‰∫∫È°ûÊé¢Á¥¢ÂíåÁôºÁèæÊâÄÈúÄÁöÑÁµêÊßãÊÄßÁµÑÁπîÁöÑÁÑ°Áõ£Áù£ÊñπÊ≥ï„ÄÇÁâπÂà•ÊòØÂú®È´òÂ∫¶Ë∑®Â≠∏ÁßëÁöÑÈ†òÂüü‰∏≠ÔºåÊ∑±ÂÖ•‰∫ÜËß£Ë∑®‰∏ªÈ°åÁöÑÁ†îÁ©∂Â∑•‰ΩúÁöÑÈÄ£ÈÄöÊÄßÂ∞çÊñºÁî¢ÁîüË¶ãËß£Ëá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÁç®ÁâπÁöÑÊñπÊ≥ï‰æÜÈÄ≤Ë°åË≥áÊñôÂ∞éËà™ÔºåË©≤ÊñπÊ≥ï‰æùË≥¥ÊñºÂú∞ÁêÜË¶ñË¶∫ÂåñÔºå‰∏¶‰ΩøÁî®ÂàÜÂ±§ÁµêÊßãÁöÑÈ†òÂüüÁü•Ë≠òÔºå‰ΩøÁî®Êà∂ËÉΩÂ§†Êé¢Á¥¢Âª∫Á´ãÂú®‰ªñÂÄëÊÑüËààË∂£ÁöÑÁõÆÊ®ôÈ†òÂüü‰∏≠ÁöÑÁü•Ë≠òÁ©∫Èñì„ÄÇÈÄôÂèØ‰ª•Âà©Áî®ÁèæÊúâÁöÑÊú¨‰Ωì„ÄÅÂ∞àÊúâÊô∫ÊÖßÊ®°ÂºèÔºåÊàñÁõ¥Êé•ÂæûÂü∫Á§éË≥áÊñô‰∏≠ÈÄèÈÅéÂàÜÂ±§‰∏ªÈ°åÂª∫Ê®°Ë°çÁîüÂá∫‰æÜ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊäÄË°ìÂæûÂü∫Á§éË≥áÊñô‰∏≠ÊèêÂèñÂëΩÂêçÂØ¶È´îÔºå‰∏¶Ê†πÊìöÁõ∏ÈóúÁöÑÈ†òÂüüÂèÉËÄÉÂíåÂ∞éËà™ÁµêÊßãÂ∞çÂÆÉÂÄëÈÄ≤Ë°åÊ®ôÊ∫ñÂåñ„ÄÇÁü•Ë≠òÁöÑÊï¥ÂêàÈ¶ñÂÖàÈÄèÈÅéÊ†πÊìöÂÖ±‰∫´ÁöÑÊèêÂèñÁâπÂæµÁ©∫ÈñìË®àÁÆóÂØ¶È´î‰πãÈñìÁöÑÁõ∏‰ººÊÄßÔºåÁÑ∂ÂæåÈÄèÈÅéËàáÂ∞éËà™ÁµêÊßãÁöÑÂ∞çÈΩä‰æÜÈÄ≤Ë°å„ÄÇÁµêÊûúÊòØ‰∏ÄÂÄãÁü•Ë≠òÂúñÔºåÂÖÅË®±ÈÄ≤Ë°åÂÖ®ÊñáÂíåË™ûÁæ©ÂúñÊü•Ë©¢‰ª•ÂèäÁµêÊßãÂåñ‰∏ªÈ°åÈ©ÖÂãïÂ∞éËà™„ÄÇÈÄô‰ΩøÁî®Êà∂ËÉΩÂ§†Ë≠òÂà•ËàáÂÖ∂ÈúÄÊ±ÇÁõ∏ÈóúÁöÑÂØ¶È´îÔºå‰∏¶Â≠òÂèñÂª£Ê≥õÁöÑÂúñÂΩ¢ÂàÜÊûê„ÄÇ‰ΩøÁî®ËÄÖ‰ªãÈù¢‰øÉÈÄ≤‰∫ÜËàáÂü∫Á§éÁü•Ë≠òÂúñÂΩ¢ÁöÑÂúñÂΩ¢‰∫íÂãïÔºå‰∏¶Ê®°Êì¨Ë£ΩÂúñÂú∞Âúñ‰ª•ÊúÄÂ§ßÈôêÂ∫¶Âú∞ÊèêÈ´òÊòìÁî®ÊÄßÂíåÊì¥Â§ßÊé°Áî®Áéá„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏ÄÂÄãÁØÑ‰æãÂ∞àÊ°àÔºå‰ΩøÁî®ÊàëÂÄëÈáùÂ∞çÊï∏ÁôæÂÄã‰∏çÂêåÁöÑÂëΩÂêçÈ†òÂüüÂØ¶È´îÂª∫Á´ãÁöÑÈÄöÁî®‰∏îÂèØÊì¥ÂÖÖÁöÑÂü∫Á§éÊû∂ÊßãÔºåÁî®ÊñºÂ≠∏Ë°ìÁîüÁâ©ÈÜ´Â≠∏ÊñáÁçªË™ûÊñôÂ∫´„ÄÇ</paragraph>

##### **Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs**
2407.15431v1 by Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang, Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, Jie Tang

The text-attributed graph (TAG) is one kind of important real-world
graph-structured data with each node associated with raw texts. For TAGs,
traditional few-shot node classification methods directly conduct training on
the pre-processed node features and do not consider the raw texts. The
performance is highly dependent on the choice of the feature pre-processing
method. In this paper, we propose P2TAG, a framework designed for few-shot node
classification on TAGs with graph pre-training and prompting. P2TAG first
pre-trains the language model (LM) and graph neural network (GNN) on TAGs with
self-supervised loss. To fully utilize the ability of language models, we adapt
the masked language modeling objective for our framework. The pre-trained model
is then used for the few-shot node classification with a mixed prompt method,
which simultaneously considers both text and graph information. We conduct
experiments on six real-world TAGs, including paper citation networks and
product co-purchasing networks. Experimental results demonstrate that our
proposed framework outperforms existing graph few-shot learning methods on
these datasets with +18.98% ~ +35.98% improvements.

ÊëòË¶ÅÔºöÊñáÊú¨Â±ûÊÄßÂõæ (TAG) ÊòØ‰∏ÄÁßçÈáçË¶ÅÁöÑÁúüÂÆû‰∏ñÁïåÂõæÁªìÊûÑÂåñÊï∞ÊçÆÔºåÂÖ∂‰∏≠ÊØè‰∏™ËäÇÁÇπÈÉΩ‰∏éÂéüÂßãÊñáÊú¨Áõ∏ÂÖ≥ËÅî„ÄÇÂØπ‰∫é TAGÔºå‰º†ÁªüÁöÑÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªÊñπÊ≥ïÁõ¥Êé•ÂØπÈ¢ÑÂ§ÑÁêÜÁöÑËäÇÁÇπÁâπÂæÅËøõË°åËÆ≠ÁªÉÔºåËÄå‰∏çËÄÉËôëÂéüÂßãÊñáÊú¨„ÄÇÊÄßËÉΩÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÂÜ≥‰∫éÁâπÂæÅÈ¢ÑÂ§ÑÁêÜÊñπÊ≥ïÁöÑÈÄâÊã©„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü P2TAGÔºåËøôÊòØ‰∏Ä‰∏™‰∏ì‰∏∫ TAG ‰∏äÁöÑÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªËÆæËÆ°ÁöÑÊ°ÜÊû∂ÔºåÂÖ∑ÊúâÂõæÈ¢ÑËÆ≠ÁªÉÂíåÊèêÁ§∫„ÄÇP2TAG È¶ñÂÖà‰ΩøÁî®Ëá™ÊàëÁõëÁù£ÊçüÂ§±ÂØπ TAG ‰∏äÁöÑËØ≠Ë®ÄÊ®°Âûã (LM) ÂíåÂõæÁ•ûÁªèÁΩëÁªú (GNN) ËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇ‰∏∫‰∫ÜÂÖÖÂàÜÂà©Áî®ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºåÊàë‰ª¨‰∏∫Êàë‰ª¨ÁöÑÊ°ÜÊû∂Ë∞ÉÊï¥‰∫ÜÊé©Á†ÅËØ≠Ë®ÄÂª∫Ê®°ÁõÆÊ†á„ÄÇÁÑ∂Âêé‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÂ∞ëÊï∞ÈïúÂ§¥ËäÇÁÇπÂàÜÁ±ªÔºåÈááÁî®Ê∑∑ÂêàÊèêÁ§∫ÊñπÊ≥ïÔºåÂêåÊó∂ËÄÉËôëÊñáÊú¨ÂíåÂõæ‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÂØπÂÖ≠‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑ TAG ËøõË°å‰∫ÜÂÆûÈ™åÔºåÂåÖÊã¨ËÆ∫ÊñáÂºïÁî®ÁΩëÁªúÂíå‰∫ßÂìÅÂÖ±ÂêåË¥≠‰π∞ÁΩëÁªú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊ°ÜÊû∂Âú®Ëøô‰∫õÊï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÂõæÂ∞ëÊï∞ÈïúÂ§¥Â≠¶‰π†ÊñπÊ≥ïÔºåÊîπËøõ‰∫Ü +18.98% ~ +35.98%„ÄÇ

##### **LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation**
2407.15351v2 by Jiaxing Zhang, Jiayi Liu, Dongsheng Luo, Jennifer Neville, Hua Wei

Recent studies seek to provide Graph Neural Network (GNN) interpretability
via multiple unsupervised learning models. Due to the scarcity of datasets,
current methods easily suffer from learning bias. To solve this problem, we
embed a Large Language Model (LLM) as knowledge into the GNN explanation
network to avoid the learning bias problem. We inject LLM as a Bayesian
Inference (BI) module to mitigate learning bias. The efficacy of the BI module
has been proven both theoretically and experimentally. We conduct experiments
on both synthetic and real-world datasets. The innovation of our work lies in
two parts: 1. We provide a novel view of the possibility of an LLM functioning
as a Bayesian inference to improve the performance of existing algorithms; 2.
We are the first to discuss the learning bias issues in the GNN explanation
problem.

ÊëòË¶ÅÔºöËøëÊúüÁ†îÁ©∂Ë©¶ÂúñÈÄèÈÅéÂ§öÁ®ÆÈùûÁõ£Áù£ÂºèÂ≠∏ÁøíÊ®°Âûã‰æÜÊèê‰æõÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÁöÑÂèØËß£ÈáãÊÄß„ÄÇÁî±ÊñºË≥áÊñôÈõÜÁöÑÁ®ÄÂ∞ëÔºåÁõÆÂâçÁöÑÊºîÁÆóÊ≥ïÂÆπÊòìÂèóÂà∞Â≠∏ÁøíÂÅèÂ∑ÆÁöÑÂΩ±Èüø„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫Áü•Ë≠òÂµåÂÖ•Âà∞ GNN Ëß£ÈáãÁ∂≤Ë∑Ø‰∏≠Ôºå‰ª•ÈÅøÂÖçÂ≠∏ÁøíÂÅèÂ∑ÆÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÂ∞á LLM ‰ΩúÁÇ∫Ë≤ùÊ∞èÊé®Ë´ñ (BI) Ê®°ÁµÑÊ≥®ÂÖ•Ôºå‰ª•Ê∏õËºïÂ≠∏ÁøíÂÅèÂ∑Æ„ÄÇBI Ê®°ÁµÑÁöÑÊïàËÉΩÂ∑≤Âú®ÁêÜË´ñ‰∏äÂíåÂØ¶È©ó‰∏äÂæóÂà∞Ë≠âÂØ¶„ÄÇÊàëÂÄëÂú®ÂêàÊàêÂíåÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂØ¶È©ó„ÄÇÊàëÂÄëÂ∑•‰ΩúÁöÑÂâµÊñ∞‰πãËôïÂú®ÊñºÂÖ©ÈÉ®ÂàÜÔºö1. ÊàëÂÄëÊèê‰æõ LLM ‰ΩúÁÇ∫Ë≤ùÊ∞èÊé®Ë´ñ‰ª•ÊîπÂñÑÁèæÊúâÊºîÁÆóÊ≥ïÊïàËÉΩÁöÑÂèØËÉΩÊÄß‰πãÊñ∞ËßÄÈªûÔºõ2. ÊàëÂÄëÁéáÂÖàË®éË´ñ GNN Ëß£ÈáãÂïèÈ°å‰∏≠ÁöÑÂ≠∏ÁøíÂÅèÂ∑ÆÂïèÈ°å„ÄÇ

##### **Text-Augmented Multimodal LLMs for Chemical Reaction Condition Recommendation**
2407.15141v1 by Yu Zhang, Ruijie Yu, Kaipeng Zeng, Ding Li, Feng Zhu, Xiaokang Yang, Yaohui Jin, Yanyan Xu

High-throughput reaction condition (RC) screening is fundamental to chemical
synthesis. However, current RC screening suffers from laborious and costly
trial-and-error workflows. Traditional computer-aided synthesis planning (CASP)
tools fail to find suitable RCs due to data sparsity and inadequate reaction
representations. Nowadays, large language models (LLMs) are capable of tackling
chemistry-related problems, such as molecule design, and chemical logic Q\&A
tasks. However, LLMs have not yet achieved accurate predictions of chemical
reaction conditions. Here, we present MM-RCR, a text-augmented multimodal LLM
that learns a unified reaction representation from SMILES, reaction graphs, and
textual corpus for chemical reaction recommendation (RCR). To train MM-RCR, we
construct 1.2 million pair-wised Q\&A instruction datasets. Our experimental
results demonstrate that MM-RCR achieves state-of-the-art performance on two
open benchmark datasets and exhibits strong generalization capabilities on
out-of-domain (OOD) and High-Throughput Experimentation (HTE) datasets. MM-RCR
has the potential to accelerate high-throughput condition screening in chemical
synthesis.

ÊëòË¶ÅÔºöÈ´òÈÄöÈáèÂèçÊáâÊ¢ù‰ª∂ (RC) ÁØ©ÈÅ∏ÊòØÂåñÂ≠∏ÂêàÊàê‰∏≠ÁöÑÂü∫Á§é„ÄÇÁÑ∂ËÄåÔºåÁï∂ÂâçÁöÑ RC ÁØ©ÈÅ∏ÊúÉÈÅáÂà∞ÁπÅÁë£‰∏îÊòÇË≤¥ÁöÑË©¶ÈåØÂ∑•‰ΩúÊµÅÁ®ã„ÄÇÂÇ≥Áµ±ÁöÑÈõªËÖ¶ËºîÂä©ÂêàÊàêË¶èÂäÉ (CASP) Â∑•ÂÖ∑ÁÑ°Ê≥ïÊâæÂà∞ÂêàÈÅ©ÁöÑ RCÔºåÈÄôÊòØÂõ†ÁÇ∫Ë≥áÊñôÁ®ÄÁñè‰∏îÂèçÊáâË°®Á§∫‰∏çË∂≥„ÄÇÂ¶Ç‰ªäÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËÉΩÂ§†Ëß£Ê±∫ËàáÂåñÂ≠∏Áõ∏ÈóúÁöÑÂïèÈ°åÔºå‰æãÂ¶ÇÂàÜÂ≠êË®≠Ë®àÂíåÂåñÂ≠∏ÈÇèËºØÂïèÁ≠î‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåLLM Â∞öÊú™ÈÅîÊàêÂåñÂ≠∏ÂèçÊáâÊ¢ù‰ª∂ÁöÑÊ∫ñÁ¢∫È†êÊ∏¨„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ MM-RCRÔºå‰∏ÄÂÄãÊñáÊú¨Â¢ûÂº∑ÁöÑÂ§öÊ®°ÊÖã LLMÔºåÂÆÉÂæû SMILES„ÄÅÂèçÊáâÂúñÂíåÊñáÊú¨Ë™ûÊñôÂ∫´Â≠∏ÁøíÁµ±‰∏ÄÁöÑÂèçÊáâË°®Á§∫Ôºå‰ª•ÈÄ≤Ë°åÂåñÂ≠∏ÂèçÊáâÊé®Ëñ¶ (RCR)„ÄÇÁÇ∫‰∫ÜË®ìÁ∑¥ MM-RCRÔºåÊàëÂÄëÂª∫Êßã‰∫Ü 120 Ëê¨Â∞çÈÖçÂ∞çÁöÑÂïèÁ≠îÊåá‰ª§Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåMM-RCR Âú®ÂÖ©ÂÄãÈñãÊîæÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºå‰∏¶Âú®È†òÂüüÂ§ñ (OOD) ÂíåÈ´òÈÄöÈáèÂØ¶È©ó (HTE) Ë≥áÊñôÈõÜ‰∏äÂ±ïÁèæÂá∫Âº∑Â§ßÁöÑÊ¶ÇÂåñËÉΩÂäõ„ÄÇMM-RCR ÊúâÂèØËÉΩÂä†ÈÄüÂåñÂ≠∏ÂêàÊàê‰∏≠ÁöÑÈ´òÈÄöÈáèÊ¢ù‰ª∂ÁØ©ÈÅ∏„ÄÇ

##### **On the Design and Analysis of LLM-Based Algorithms**
2407.14788v1 by Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou

We initiate a formal investigation into the design and analysis of LLM-based
algorithms, i.e. algorithms that contain one or multiple calls of large
language models (LLMs) as sub-routines and critically rely on the capabilities
of LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt
engineering to complicated LLM-powered agent systems and compound AI systems,
have achieved remarkable empirical success, the design and optimization of them
have mostly relied on heuristics and trial-and-errors, which is largely due to
a lack of formal and analytical study for these algorithms. To fill this gap,
we start by identifying the computational-graph representation of LLM-based
algorithms, the design principle of task decomposition, and some key
abstractions, which then facilitate our formal analysis for the accuracy and
efficiency of LLM-based algorithms, despite the black-box nature of LLMs. We
further consider parallel decomposition for a case study, providing extensive
analytical and empirical study for four concrete examples of this pattern. Our
proposed framework holds promise for advancing LLM-based algorithms, by
revealing the reasons behind curious empirical phenomena, guiding the choices
of hyperparameters, predicting the empirical performance of algorithms, and
inspiring new algorithm design. To promote further study of LLM-based
algorithms, we release our source code at
https://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÂ∞çÂü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑË®≠Ë®àÂíåÂàÜÊûêÂ±ïÈñãÊ≠£ÂºèË™øÊü•ÔºåÂç≥ÂåÖÂê´‰∏ÄÂÄãÊàñÂ§öÂÄãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫Â≠êÂ∏∏ÂºèÂëºÂè´ÁöÑÊºîÁÆóÊ≥ïÔºå‰∏¶Ê•µÂ∫¶‰æùË≥¥ LLM ÁöÑÂäüËÉΩ„ÄÇÂÑòÁÆ°Âü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÔºåÂæûÂ∏∂ÊèêÁ§∫Â∑•Á®ãÁöÑÂü∫Êú¨ LLM ÂëºÂè´Âà∞Ë§áÈõúÁöÑ LLM È©ÖÂãïÁöÑ‰ª£ÁêÜÁ≥ªÁµ±ÂíåË§áÂêàÂºè AI Á≥ªÁµ±ÔºåÂ∑≤ÂèñÂæóÈ°ØËëóÁöÑÂØ¶Ë≠âÊàêÂäüÔºå‰ΩÜÂÖ∂Ë®≠Ë®àÂíåÊúÄ‰Ω≥ÂåñÂ§ßÂ§ö‰æùË≥¥Ë©¶È©óÊ≥ïÂíåÈåØË™§ÔºåÈÄôÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÂõ†ÁÇ∫Áº∫‰πèÂ∞çÈÄô‰∫õÊºîÁÆóÊ≥ïÁöÑÊ≠£ÂºèÂíåÂàÜÊûêÁ†îÁ©∂„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩÔºåÊàëÂÄëÂæûË≠òÂà•Âü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑË®àÁÆóÂúñË°®Á§∫„ÄÅ‰ªªÂãôÂàÜËß£ÁöÑË®≠Ë®àÂéüÂâáÔºå‰ª•Âèä‰∏Ä‰∫õÈóúÈçµÊäΩË±°ÂåñÈñãÂßãÔºåÁÑ∂Âæå‰øÉÈÄ≤ÊàëÂÄëÂ∞çÂü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊïàÁéáÈÄ≤Ë°åÊ≠£ÂºèÂàÜÊûêÔºåÂÑòÁÆ° LLM Êú¨Ë∫´ÂÖ∑ÊúâÈªëÁõíÁâπÊÄß„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ËÄÉÊÖÆ‰∏¶Ë°åÂàÜËß£‰ΩúÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåÁÇ∫Ê≠§Ê®°ÂºèÁöÑÂõõÂÄãÂÖ∑È´îÁØÑ‰æãÊèê‰æõÂª£Ê≥õÁöÑÂàÜÊûêÂíåÂØ¶Ë≠âÁ†îÁ©∂„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÊúâÊúõÊé®ÈÄ≤Âü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÔºåÊñπÊ≥ïÊòØÊè≠Á§∫Â•áÊÄ™ÁöÑÂØ¶Ë≠âÁèæË±°ËÉåÂæåÁöÑÂéüÂõ†„ÄÅÊåáÂ∞éË∂ÖÂèÉÊï∏ÁöÑÈÅ∏Êìá„ÄÅÈ†êÊ∏¨ÊºîÁÆóÊ≥ïÁöÑÂØ¶Ë≠âÊïàËÉΩÔºå‰∏¶ÊøÄÁôºÊñ∞ÁöÑÊºîÁÆóÊ≥ïË®≠Ë®à„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Â∞çÂü∫Êñº LLM ÁöÑÊºîÁÆóÊ≥ïÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÔºåÊàëÂÄëÂú® https://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm/ ÁôºÂ∏ÉÊàëÂÄëÁöÑÂéüÂßãÁ¢º„ÄÇ</paragraph>

##### **LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits**
2407.18269v1 by Chen-Chia Chang, Yikang Shan, Shaoze Fan, Jing Li, Shun Zhang, Ningyuan Cao, Yiran Chen, Xin Zhang

In the realm of electronic and electrical engineering, automation of analog
circuit is increasingly vital given the complexity and customized requirements
of modern applications. However, existing methods only develop search-based
algorithms that require many simulation iterations to design a custom circuit
topology, which is usually a time-consuming process. To this end, we introduce
LaMAGIC, a pioneering language model-based topology generation model that
leverages supervised finetuning for automated analog circuit design. LaMAGIC
can efficiently generate an optimized circuit design from the custom
specification in a single pass. Our approach involves a meticulous development
and analysis of various input and output formulations for circuit. These
formulations can ensure canonical representations of circuits and align with
the autoregressive nature of LMs to effectively addressing the challenges of
representing analog circuits as graphs. The experimental results show that
LaMAGIC achieves a success rate of up to 96\% under a strict tolerance of 0.01.
We also examine the scalability and adaptability of LaMAGIC, specifically
testing its performance on more complex circuits. Our findings reveal the
enhanced effectiveness of our adjacency matrix-based circuit formulation with
floating-point input, suggesting its suitability for handling intricate circuit
designs. This research not only demonstrates the potential of language models
in graph generation, but also builds a foundational framework for future
explorations in automated analog circuit design.

ÊëòË¶ÅÔºöÂú®ÈõªÂ≠êÂíåÈõªÊ∞£Â∑•Á®ãÈ†òÂüü‰∏≠ÔºåËá™ÂãïÂåñÈ°ûÊØîÈõªË∑ØË∂ä‰æÜË∂äÈáçË¶ÅÔºåÂõ†ÁÇ∫Áèæ‰ª£ÊáâÁî®Á®ãÂºèÂÖ∑ÊúâË§áÈõú‰∏îÂÆ¢Ë£ΩÂåñÁöÑÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊñπÊ≥ïÂÉÖÈñãÁôºÂü∫ÊñºÊêúÂ∞ãÁöÑÊºîÁÆóÊ≥ïÔºåÈúÄË¶ÅË®±Â§öÊ®°Êì¨ÂèçË¶ÜÈÅãÁÆóÊâçËÉΩË®≠Ë®àÂÆ¢Ë£ΩÂåñÈõªË∑ØÊãìÊí≤ÔºåÈÄôÈÄöÂ∏∏ÊòØ‰∏ÄÂÄãËÄóÊôÇÁöÑÈÅéÁ®ã„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü LaMAGICÔºå‰∏ÄÂÄãÂü∫ÊñºÂÖàÈ©ÖË™ûË®ÄÊ®°ÂûãÁöÑÊãìÊí≤ÁîüÊàêÊ®°ÂûãÔºåÂÆÉÂà©Áî®Áõ£Áù£ÂæÆË™øÈÄ≤Ë°åËá™ÂãïÂåñÈ°ûÊØîÈõªË∑ØË®≠Ë®à„ÄÇLaMAGIC ÂèØ‰ª•ÊúâÊïàÁéáÂú∞ÂæûÂÆ¢Ë£ΩÂåñË¶èÊ†º‰∏≠ÁîüÊàêÊúÄ‰Ω≥ÂåñÁöÑÈõªË∑ØË®≠Ë®àÔºåÂè™ÈúÄ‰∏ÄÊ¨°ÈÄöÈÅé„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨‰ªîÁ¥∞ÈñãÁôºÂíåÂàÜÊûêÈõªË∑ØÁöÑÂêÑÁ®ÆËº∏ÂÖ•ÂíåËº∏Âá∫ÂÖ¨Âºè„ÄÇÈÄô‰∫õÂÖ¨ÂºèÂèØ‰ª•Á¢∫‰øùÈõªË∑ØÁöÑÊ®ôÊ∫ñË°®Á§∫Ôºå‰∏¶Ëàá LM ÁöÑËá™Ëø¥Ê≠∏ÊÄßË≥™‰øùÊåÅ‰∏ÄËá¥Ôºå‰ª•ÊúâÊïàËß£Ê±∫Â∞áÈ°ûÊØîÈõªË∑ØË°®Á§∫ÁÇ∫ÂúñÂΩ¢ÁöÑÊåëÊà∞„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåLaMAGIC Âú® 0.01 ÁöÑÂö¥Ê†ºÂÆπÂ∑Æ‰∏ãÂØ¶Áèæ‰∫ÜÈ´òÈÅî 96% ÁöÑÊàêÂäüÁéá„ÄÇÊàëÂÄëÈÇÑÊ™¢Êü•‰∫Ü LaMAGIC ÁöÑÂèØÊì¥ÂÖÖÊÄßÂíåÈÅ©ÊáâÊÄßÔºåÁâπÂà•ÊòØÊ∏¨Ë©¶‰∫ÜÂÆÉÂú®Êõ¥Ë§áÈõúÈõªË∑Ø‰∏äÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÊàëÂÄëÂü∫ÊñºÈÑ∞Êé•Áü©Èô£ÁöÑÈõªË∑ØÂÖ¨ÂºèËàáÊµÆÈªûËº∏ÂÖ•ÁöÑÂ¢ûÂº∑ÊïàËÉΩÔºåË°®ÊòéÂÆÉÈÅ©Áî®ÊñºËôïÁêÜË§áÈõúÁöÑÈõªË∑ØË®≠Ë®à„ÄÇÈÄôÈ†ÖÁ†îÁ©∂‰∏çÂÉÖÂ±ïÁ§∫‰∫ÜË™ûË®ÄÊ®°ÂûãÂú®ÂúñÂΩ¢ÁîüÊàê‰∏≠ÁöÑÊΩõÂäõÔºå‰πüÁÇ∫Êú™‰æÜÂú®Ëá™ÂãïÂåñÈ°ûÊØîÈõªË∑ØË®≠Ë®à‰∏≠ÁöÑÊé¢Á¥¢Âª∫Á´ã‰∫ÜÂü∫Á§éÊ°ÜÊû∂„ÄÇ

##### **Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition**
2407.14224v1 by Suvajit Patra, Arkadip Maitra, Megha Tiwari, K. Kumaran, Swathy Prabhu, Swami Punyeshwarananda, Soumitra Samanta

Automatic Sign Language (SL) recognition is an important task in the computer
vision community. To build a robust SL recognition system, we need a
considerable amount of data which is lacking particularly in Indian sign
language (ISL). In this paper, we propose a large-scale isolated ISL dataset
and a novel SL recognition model based on skeleton graph structure. The dataset
covers 2,002 daily used common words in the deaf community recorded by 20 (10
male and 10 female) deaf adult signers (contains 40033 videos). We propose a SL
recognition model namely Hierarchical Windowed Graph Attention Network (HWGAT)
by utilizing the human upper body skeleton graph structure. The HWGAT tries to
capture distinctive motions by giving attention to different body parts induced
by the human skeleton graph structure. The utility of the proposed dataset and
the usefulness of our model are evaluated through extensive experiments. We
pre-trained the proposed model on the proposed dataset and fine-tuned it across
different sign language datasets further boosting the performance of 1.10,
0.46, 0.78, and 6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL
respectively compared to the existing state-of-the-art skeleton-based models.

ÊëòË¶ÅÔºöËá™ÂãïÊâãË™û (SL) Ë≠òÂà•ÊòØÈõªËÖ¶Ë¶ñË¶∫Á§æÁæ§‰∏≠ÁöÑÈáçË¶Å‰ªªÂãô„ÄÇË¶ÅÂª∫Á´ãÂº∑ÂÅ•ÁöÑ SL Ë≠òÂà•Á≥ªÁµ±ÔºåÊàëÂÄëÈúÄË¶ÅÂ§ßÈáèÁöÑË≥áÊñôÔºåËÄåÈÄôÂú®Âç∞Â∫¶ÊâãË™û (ISL) ‰∏≠ÁâπÂà•Áº∫‰πè„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ§ßË¶èÊ®°ÁöÑÂ≠§Á´ã ISL Ë≥áÊñôÈõÜÔºå‰ª•Âèä‰∏ÄÂÄãÂü∫ÊñºÈ™®Êû∂ÂúñÁµêÊßãÁöÑÊñ∞Âûã SL Ë≠òÂà•Ê®°Âûã„ÄÇË©≤Ë≥áÊñôÈõÜÊ∂µËìã 2,002 ÂÄãËÅæÂïûÁ§æÁæ§‰∏≠Â∏∏Áî®ÁöÑÊó•Â∏∏ÂñÆÂ≠óÔºåÁî± 20 ‰Ωç (10 Áî∑ 10 Â•≥) ËÅæÂïûÊàê‰∫∫ÊâãË™ûËÄÖÈåÑË£ΩÔºàÂåÖÂê´ 40033 ÈÉ®ÂΩ±ÁâáÔºâ„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã SL Ë≠òÂà•Ê®°ÂûãÔºåÂç≥ÂàÜÂ±§Ë¶ñÁ™óÂúñÊ≥®ÊÑèÂäõÁ∂≤Ë∑Ø (HWGAT)ÔºåÂà©Áî®‰∫∫È´î‰∏äÂçäË∫´È™®Êû∂ÂúñÁµêÊßã„ÄÇHWGAT ÂòóË©¶ÈÄèÈÅéÈóúÊ≥®Áî±‰∫∫È´îÈ™®Êû∂ÂúñÁµêÊßãË™òÂ∞éÁöÑ‰∏çÂêåË∫´È´îÈÉ®‰Ωç‰æÜÊçïÊçâÁç®ÁâπÁöÑÂãï‰Ωú„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑË≥áÊñôÈõÜÁöÑÊïàÁî®ÂíåÊàëÂÄëÊ®°ÂûãÁöÑÊúâÁî®ÊÄß„ÄÇÊàëÂÄëÂú®ÊâÄÊèêÂá∫ÁöÑË≥áÊñôÈõÜ‰∏äÈ†êË®ìÁ∑¥ÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÔºå‰∏¶Âú®‰∏çÂêåÁöÑÊâãË™ûË≥áÊñôÈõÜ‰∏äÂæÆË™øÂÆÉÔºåÈÄ≤‰∏ÄÊ≠•ÊèêÂçá‰∫Ü INCLUDE„ÄÅLSA64„ÄÅAUTSL Âíå WLASL ‰∏ä 1.10„ÄÅ0.46„ÄÅ0.78 Âíå 6.84 ÂÄãÁôæÂàÜÈªûÁöÑÊïàËÉΩÔºåÂàÜÂà•ËàáÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÈ™®Êû∂ÁöÑÊ®°ÂûãÁõ∏ÊØî„ÄÇ

##### **Enhancing Data-Limited Graph Neural Networks by Actively Distilling Knowledge from Large Language Models**
2407.13989v1 by Quan Li, Tianxiang Zhao, Lingwei Chen, Junjie Xu, Suhang Wang

Graphs have emerged as critical data structures for content analysis in
various domains, such as social network analysis, bioinformatics, and
recommendation systems. Node classification, a fundamental task in this
context, is typically tackled using graph neural networks (GNNs).
Unfortunately, conventional GNNs still face challenges in scenarios with few
labeled nodes, despite the prevalence of few-shot node classification tasks in
real-world applications. To address this challenge, various approaches have
been proposed, including graph meta-learning, transfer learning, and methods
based on Large Language Models (LLMs). However, traditional meta-learning and
transfer learning methods often require prior knowledge from base classes or
fail to exploit the potential advantages of unlabeled nodes. Meanwhile,
LLM-based methods may overlook the zero-shot capabilities of LLMs and rely
heavily on the quality of generated contexts. In this paper, we propose a novel
approach that integrates LLMs and GNNs, leveraging the zero-shot inference and
reasoning capabilities of LLMs and employing a Graph-LLM-based active learning
paradigm to enhance GNNs' performance. Extensive experiments demonstrate the
effectiveness of our model in improving node classification accuracy with
considerably limited labeled data, surpassing state-of-the-art baselines by
significant margins.

ÊëòË¶ÅÔºöÂúñË°®Â∑≤ÊàêÁÇ∫ÂêÑÁ®ÆÈ†òÂüü‰∏≠ÂÖßÂÆπÂàÜÊûêÁöÑÈóúÈçµÊï∏ÊìöÁµêÊßãÔºå‰æãÂ¶ÇÁ§æ‰∫§Á∂≤Ë∑ØÂàÜÊûê„ÄÅÁîüÁâ©Ë≥áË®äÂ≠∏ÂíåÊé®Ëñ¶Á≥ªÁµ±„ÄÇÁØÄÈªûÂàÜÈ°ûÊòØÊ≠§ËÑàÁµ°‰∏≠ÁöÑÂü∫Êú¨‰ªªÂãôÔºåÈÄöÂ∏∏‰ΩøÁî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ‰æÜËôïÁêÜ„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÂÑòÁÆ°ÁèæÂØ¶‰∏ñÁïåÊáâÁî®‰∏≠ÊôÆÈÅçÂ≠òÂú®Â∞ëÊ®£Êú¨ÁØÄÈªûÂàÜÈ°û‰ªªÂãôÔºå‰ΩÜÂÇ≥Áµ±ÁöÑ GNN Âú®Ê®ôË®òÁØÄÈªûÂæàÂ∞ëÁöÑÊÉÖÊ≥Å‰∏ã‰ªçÈù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÂ∑≤ÊèêÂá∫ÂêÑÁ®ÆÊñπÊ≥ïÔºåÂåÖÊã¨ÂúñÂΩ¢ÂÖÉÂ≠∏Áøí„ÄÅÈÅ∑ÁßªÂ≠∏ÁøíÂíåÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑÂÖÉÂ≠∏ÁøíÂíåÈÅ∑ÁßªÂ≠∏ÁøíÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶Å‰æÜËá™Âü∫Á§éÈ°ûÂà•ÁöÑÂÖàÈ©óÁü•Ë≠òÔºåÊàñËÄÖÁÑ°Ê≥ïÂà©Áî®Êú™Ê®ôË®òÁØÄÈªûÁöÑÊΩõÂú®ÂÑ™Âã¢„ÄÇÂêåÊôÇÔºåÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÂèØËÉΩÊúÉÂøΩË¶ñ LLM ÁöÑÈõ∂Ê®£Êú¨ËÉΩÂäõÔºå‰∏¶‰∏îÈÅéÂ∫¶‰æùË≥¥ÁîüÊàêË™ûÂ¢ÉÁöÑÂìÅË≥™„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÂÆÉÊï¥Âêà‰∫Ü LLM Âíå GNNÔºåÂà©Áî® LLM ÁöÑÈõ∂Ê®£Êú¨Êé®Ë´ñÂíåÊé®ÁêÜËÉΩÂäõÔºå‰∏¶Êé°Áî®Âü∫Êñº Graph-LLM ÁöÑ‰∏ªÂãïÂ≠∏ÁøíÁØÑ‰æã‰æÜÂ¢ûÂº∑ GNN ÁöÑÊïàËÉΩ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊîπÈÄ≤ÁØÄÈªûÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÊ®ôË®òÊï∏ÊìöÁõ∏Áï∂ÊúâÈôêÔºåÈ°ØËëóË∂ÖË∂ä‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñ„ÄÇ

##### **A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice**
2407.13699v1 by Shaina Raza, Mizanur Rahman, Safiullah Kamawal, Armin Toroghi, Ananya Raval, Farshad Navah, Amirmohammad Kazemeini

Recommender Systems (RS) play an integral role in enhancing user experiences
by providing personalized item suggestions. This survey reviews the progress in
RS inclusively from 2017 to 2024, effectively connecting theoretical advances
with practical applications. We explore the development from traditional RS
techniques like content-based and collaborative filtering to advanced methods
involving deep learning, graph-based models, reinforcement learning, and large
language models. We also discuss specialized systems such as context-aware,
review-based, and fairness-aware RS. The primary goal of this survey is to
bridge theory with practice. It addresses challenges across various sectors,
including e-commerce, healthcare, and finance, emphasizing the need for
scalable, real-time, and trustworthy solutions. Through this survey, we promote
stronger partnerships between academic research and industry practices. The
insights offered by this survey aim to guide industry professionals in
optimizing RS deployment and to inspire future research directions, especially
in addressing emerging technological and societal trends

ÊëòË¶ÅÔºöÊé®Ëñ¶Á≥ªÁµ± (RS) Âú®ÊèêÂçá‰ΩøÁî®ËÄÖÈ´îÈ©ó‰∏≠ÊâÆÊºîËëó‰∏çÂèØÊàñÁº∫ÁöÑËßíËâ≤ÔºåÈÄèÈÅéÊèê‰æõÂÄã‰∫∫ÂåñÁöÑÂïÜÂìÅÂª∫Ë≠∞„ÄÇÈÄôÈ†ÖË™øÊü•ÂõûÈ°ß‰∫Ü RS Âú® 2017 Âπ¥Âà∞ 2024 Âπ¥ÈñìÁöÑÈÄ≤Â±ïÔºåÊúâÊïàÂú∞Â∞áÁêÜË´ñÈÄ≤Â±ïËàáÂØ¶ÈöõÊáâÁî®ÈÄ£ÁµêËµ∑‰æÜ„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜÂæûÂÇ≥Áµ±ÁöÑ RS ÊäÄË°ìÔºå‰æãÂ¶ÇÂü∫ÊñºÂÖßÂÆπÂíåÂçîÂêåÈÅéÊøæÔºåÂà∞Ê∂âÂèäÊ∑±Â∫¶Â≠∏Áøí„ÄÅÂü∫ÊñºÂúñÂΩ¢ÁöÑÊ®°Âûã„ÄÅÂº∑ÂåñÂ≠∏ÁøíÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÁ≠âÂÖàÈÄ≤ÊñπÊ≥ïÁöÑÁôºÂ±ï„ÄÇÊàëÂÄë‰πüË®éË´ñ‰∫ÜÂ∞àÈñÄÁöÑÁ≥ªÁµ±Ôºå‰æãÂ¶ÇÊÉÖÂ¢ÉÊÑüÁü•„ÄÅÂü∫ÊñºË©ïË´ñÂíåÂÖ¨Âπ≥ÊÑüÁü•ÁöÑ RS„ÄÇÈÄôÈ†ÖË™øÊü•ÁöÑ‰∏ªË¶ÅÁõÆÊ®ôÊòØÂ∞áÁêÜË´ñËàáÂØ¶ÂãôÁµêÂêàËµ∑‰æÜ„ÄÇÂÆÉËß£Ê±∫‰∫ÜÂêÑÂÄãÈ†òÂüüÁöÑÊåëÊà∞ÔºåÂåÖÊã¨ÈõªÂ≠êÂïÜÂãô„ÄÅÈÜ´ÁôÇ‰øùÂÅ•ÂíåÈáëËûçÔºåÂº∑Ë™ø‰∫ÜÂ∞çÂèØÊì¥ÂÖÖ„ÄÅÂç≥ÊôÇÂíåÂèØ‰ø°Ë≥¥ÁöÑËß£Ê±∫ÊñπÊ°àÁöÑÈúÄÊ±Ç„ÄÇÈÄèÈÅéÈÄôÈ†ÖË™øÊü•ÔºåÊàëÂÄë‰øÉÈÄ≤‰∫ÜÂ≠∏Ë°ìÁ†îÁ©∂ÂíåÁî¢Ê•≠ÂØ¶Âãô‰πãÈñìÊõ¥Âº∑Â§ßÁöÑÂ§•‰º¥Èóú‰øÇ„ÄÇÈÄôÈ†ÖË™øÊü•Êèê‰æõÁöÑË¶ãËß£Êó®Âú®ÂºïÂ∞éÁî¢Ê•≠Â∞àÊ•≠‰∫∫Â£´ÂÑ™Âåñ RS ÈÉ®ÁΩ≤Ôºå‰∏¶ÊøÄÂãµÊú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêëÔºåÁâπÂà•ÊòØÂú®Ëß£Ê±∫Êñ∞ËààÁöÑÊäÄË°ìÂíåÁ§æÊúÉË∂®Âã¢ÊñπÈù¢„ÄÇ

##### **MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains**
2407.18961v2 by Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu, Xiang Kong, Aonan Zhang, Dian Ang Yap, Yizhe zhang, Karsten Ahnert, Vik Kamath, Mathias Berglund, Dominic Walsh, Tobias Gindele, Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng Cao, Ruoming Pang, Zirui Wang

Recent advances in large language models (LLMs) have increased the demand for
comprehensive benchmarks to evaluate their capabilities as human-like agents.
Existing benchmarks, while useful, often focus on specific application
scenarios, emphasizing task completion but failing to dissect the underlying
skills that drive these outcomes. This lack of granularity makes it difficult
to deeply discern where failures stem from. Additionally, setting up these
environments requires considerable effort, and issues of unreliability and
reproducibility sometimes arise, especially in interactive tasks. To address
these limitations, we introduce the Massive Multitask Agent Understanding
(MMAU) benchmark, featuring comprehensive offline tasks that eliminate the need
for complex environment setups. It evaluates models across five domains,
including Tool-use, Directed Acyclic Graph (DAG) QA, Data Science and Machine
Learning coding, Contest-level programming and Mathematics, and covers five
essential capabilities: Understanding, Reasoning, Planning, Problem-solving,
and Self-correction. With a total of 20 meticulously designed tasks
encompassing over 3K distinct prompts, MMAU provides a comprehensive framework
for evaluating the strengths and limitations of LLM agents. By testing 18
representative models on MMAU, we provide deep and insightful analyses.
Ultimately, MMAU not only sheds light on the capabilities and limitations of
LLM agents but also enhances the interpretability of their performance.
Datasets and evaluation scripts of MMAU are released at
https://github.com/apple/axlearn/tree/main/docs/research/mmau.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ¢ûÂä†‰∫ÜÂ∞çÂÖ®Èù¢Âü∫Ê∫ñÊ∏¨Ë©¶ÁöÑÈúÄÊ±ÇÔºå‰ª•Ë©ï‰º∞ÂÖ∂‰ΩúÁÇ∫È°û‰∫∫‰ª£ÁêÜÁöÑËÉΩÂäõ„ÄÇÁèæÊúâÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÈõñÁÑ∂ÊúâÁî®Ôºå‰ΩÜÈÄöÂ∏∏Â∞àÊ≥®ÊñºÂÖ∑È´îÁöÑÊáâÁî®Â†¥ÊôØÔºåÂº∑Ë™ø‰ªªÂãôÂÆåÊàêÔºå‰ΩÜÊú™ËÉΩÂâñÊûêÈ©ÖÂãïÈÄô‰∫õÁµêÊûúÁöÑÂ∫ïÂ±§ÊäÄËÉΩ„ÄÇÈÄôÁ®ÆÁº∫‰πèÁ≤íÂ∫¶‰ΩøÂæóÈõ£‰ª•Ê∑±ÂÖ•Ëæ®Âà•Â§±ÊïóÁöÑÊ†πÊ∫ê„ÄÇÊ≠§Â§ñÔºåË®≠ÁΩÆÈÄô‰∫õÁí∞Â¢ÉÈúÄË¶ÅÂ§ßÈáèÁöÑÁ≤æÂäõÔºåÊúâÊôÇÊúÉÂá∫Áèæ‰∏çÂèØÈù†ÊÄßÂíåÂèØÈáçË§áÊÄßÁöÑÂïèÈ°åÔºåÁâπÂà•ÊòØÂú®‰∫íÂãï‰ªªÂãô‰∏≠„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ§ßË¶èÊ®°Â§ö‰ªªÂãô‰ª£ÁêÜÁêÜËß£ (MMAU) Âü∫Ê∫ñÊ∏¨Ë©¶ÔºåÂÆÉÂÖ∑ÊúâÂÖ®Èù¢ÁöÑÈõ¢Á∑ö‰ªªÂãôÔºåÊ∂àÈô§‰∫ÜÂ∞çË§áÈõúÁí∞Â¢ÉË®≠ÁΩÆÁöÑÈúÄÊ±Ç„ÄÇÂÆÉË∑®Ë∂ä‰∫îÂÄãÈ†òÂüüË©ï‰º∞Ê®°ÂûãÔºåÂåÖÊã¨Â∑•ÂÖ∑‰ΩøÁî®„ÄÅÊúâÂêëÁÑ°Áí∞Âúñ (DAG) ÂïèÁ≠î„ÄÅÊï∏ÊìöÁßëÂ≠∏ÂíåÊ©üÂô®Â≠∏ÁøíÁ∑®Á¢º„ÄÅÁ´∂Ë≥ΩÁ¥öÁ∑®Á®ãÂíåÊï∏Â≠∏Ôºå‰∏¶Ê∂µËìã‰∫îÈ†ÖÂü∫Êú¨ËÉΩÂäõÔºöÁêÜËß£„ÄÅÊé®ÁêÜ„ÄÅË¶èÂäÉ„ÄÅÂïèÈ°åËß£Ê±∫ÂíåËá™ÊàëÁ≥æÊ≠£„ÄÇMMAU Á∏ΩÂÖ±ÂåÖÂê´ 20 È†ÖÁ≤æÂøÉË®≠Ë®àÁöÑ‰ªªÂãôÔºåÊ∂µËìãË∂ÖÈÅé 3K ÂÄã‰∏çÂêåÁöÑÊèêÁ§∫ÔºåÁÇ∫Ë©ï‰º∞ LLM ‰ª£ÁêÜÁöÑÂÑ™Âã¢ÂíåÂ±ÄÈôêÊÄßÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊ°ÜÊû∂„ÄÇÈÄöÈÅéÂú® MMAU ‰∏äÊ∏¨Ë©¶ 18 ÂÄã‰ª£Ë°®ÊÄßÊ®°ÂûãÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÊ∑±ÂÖ•ËÄåÊúâË¶ãÂú∞ÁöÑÂàÜÊûê„ÄÇÊúÄÁµÇÔºåMMAU ‰∏çÂÉÖÈó°Êòé‰∫Ü LLM ‰ª£ÁêÜÁöÑËÉΩÂäõÂíåÂ±ÄÈôêÊÄßÔºåÈÇÑÂ¢ûÂº∑‰∫ÜÂÖ∂ÊÄßËÉΩÁöÑÂèØËß£ÈáãÊÄß„ÄÇMMAU ÁöÑÊï∏ÊìöÈõÜÂíåË©ï‰º∞ËÖ≥Êú¨Â∑≤ÁôºÂ∏ÉÂú® https://github.com/apple/axlearn/tree/main/docs/research/mmau„ÄÇ

##### **Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?**
2407.12725v1 by Ben Yao, Yazhou Zhang, Qiuchi Li, Jing Qin

Elaborating a series of intermediate reasoning steps significantly improves
the ability of large language models (LLMs) to solve complex problems, as such
steps would evoke LLMs to think sequentially. However, human sarcasm
understanding is often considered an intuitive and holistic cognitive process,
in which various linguistic, contextual, and emotional cues are integrated to
form a comprehensive understanding of the speaker's true intention, which is
argued not be limited to a step-by-step reasoning process. To verify this
argument, we introduce a new prompting framework called SarcasmCue, which
contains four prompting strategies, $viz.$ chain of contradiction (CoC), graph
of cues (GoC), bagging of cues (BoC) and tensor of cues (ToC), which elicits
LLMs to detect human sarcasm by considering sequential and non-sequential
prompting methods. Through a comprehensive empirical comparison on four
benchmarking datasets, we show that the proposed four prompting methods
outperforms standard IO prompting, CoT and ToT with a considerable margin, and
non-sequential prompting generally outperforms sequential prompting.

ÊëòË¶ÅÔºöÈÄöÈÅéÈó°Ëø∞‰∏ÄÁ≥ªÂàó‰∏≠ÈñìÊé®ÁêÜÊ≠•È©üÔºåÂ§ßÂπÖÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ëß£Ê±∫Ë§áÈõúÂïèÈ°åÁöÑËÉΩÂäõÔºåÂõ†ÁÇ∫ÈÄô‰∫õÊ≠•È©üÊúÉ‰øÉ‰Ωø LLM ÊåâÈ†ÜÂ∫èÊÄùËÄÉ„ÄÇÁÑ∂ËÄåÔºå‰∫∫È°ûÁöÑË´∑Âà∫ÁêÜËß£ÈÄöÂ∏∏Ë¢´Ë™çÁÇ∫ÊòØ‰∏ÄÁ®ÆÁõ¥Ë¶∫‰∏îÂÖ®Èù¢ÁöÑË™çÁü•ÈÅéÁ®ãÔºåÂÖ∂‰∏≠ÂêÑÁ®ÆË™ûË®Ä„ÄÅË™ûÂ¢ÉÂíåÊÉÖÁ∑íÁ∑öÁ¥¢Êï¥ÂêàÂú®‰∏ÄËµ∑Ôºå‰ª•ÂÖ®Èù¢‰∫ÜËß£Ë™™Ë©±ËÄÖÁöÑÁúüÂØ¶ÊÑèÂúñÔºåÈÄôË¢´Ë™çÁÇ∫‰∏çÂÉÖÈôêÊñºÂæ™Â∫èÊº∏ÈÄ≤ÁöÑÊé®ÁêÜÈÅéÁ®ã„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÈÄôÂÄãË´ñÈªûÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊèêÁ§∫Ê°ÜÊû∂ÔºåÁ®±ÁÇ∫ SarcasmCueÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂõõÁ®ÆÊèêÁ§∫Á≠ñÁï•ÔºåÂç≥ÁüõÁõæÈèà (CoC)„ÄÅÁ∑öÁ¥¢Âúñ (GoC)„ÄÅÁ∑öÁ¥¢Ë¢ã (BoC) ÂíåÁ∑öÁ¥¢ÂºµÈáè (ToC)ÔºåÂÆÉÂºïÁôº LLM ÈÄöÈÅéËÄÉÊÖÆÈ†ÜÂ∫èÂíåÈùûÈ†ÜÂ∫èÊèêÁ§∫ÊñπÊ≥ï‰æÜÊ™¢Ê∏¨‰∫∫È°ûÁöÑË´∑Âà∫„ÄÇÈÄöÈÅéÂ∞çÂõõÂÄãÂü∫Ê∫ñÊï∏ÊìöÈõÜÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÂØ¶Ë≠âÊØîËºÉÔºåÊàëÂÄëË°®ÊòéÊâÄÊèêÂá∫ÁöÑÂõõÁ®ÆÊèêÁ§∫ÊñπÊ≥ï‰ª•Áõ∏Áï∂Â§ßÁöÑÂπÖÂ∫¶ÂÑ™ÊñºÊ®ôÊ∫ñ IO ÊèêÁ§∫„ÄÅCoT Âíå ToTÔºå‰∏¶‰∏îÈùûÈ†ÜÂ∫èÊèêÁ§∫ÈÄöÂ∏∏ÂÑ™ÊñºÈ†ÜÂ∫èÊèêÁ§∫„ÄÇ

##### **Subgraph-Aware Training of Text-based Methods for Knowledge Graph Completion**
2407.12703v3 by Youmin Ko, Hyemin Yang, Taeuk Kim, Hyunjoon Kim

Fine-tuning pre-trained language models (PLMs) has recently shown a potential
to improve knowledge graph completion (KGC). However, most PLM-based methods
encode only textual information, neglecting various topological structures of
knowledge graphs (KGs). In this paper, we empirically validate the significant
relations between the structural properties of KGs and the performance of the
PLM-based methods. To leverage the structural knowledge, we propose a
Subgraph-Aware Training framework for KGC (SATKGC) that combines (i)
subgraph-aware mini-batching to encourage hard negative sampling, and (ii) a
new contrastive learning method to focus more on harder entities and harder
negative triples in terms of the structural properties. To the best of our
knowledge, this is the first study to comprehensively incorporate the
structural inductive bias of the subgraphs into fine-tuning PLMs. Extensive
experiments on four KGC benchmarks demonstrate the superiority of SATKGC. Our
code is available.

ÊëòË¶ÅÔºöÂæÆË™øÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) Ëøë‰æÜÈ°ØÁ§∫Âá∫ÊîπÂñÑÁü•Ë≠òÂúñË≠úÂÆåÊàêÂäüËÉΩ (KGC) ÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏Âü∫Êñº PLM ÁöÑÊñπÊ≥ïÂÉÖÁ∑®Á¢ºÊñáÂ≠óË≥áË®äÔºåÂøΩÁï•‰∫ÜÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÂêÑÁ®ÆÊãìÊí≤ÁµêÊßã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÁ∂ìÈ©óÈ©óË≠â‰∫Ü KG ÁöÑÁµêÊßãÂ±¨ÊÄßËàáÂü∫Êñº PLM ÁöÑÊñπÊ≥ïÊïàËÉΩ‰πãÈñìÁöÑÈáçË¶ÅÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÂà©Áî®ÁµêÊßãÁü•Ë≠òÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî®Êñº KGC ÁöÑÂ≠êÂúñÊÑüÁü•Ë®ìÁ∑¥Êû∂Êßã (SATKGC)ÔºåÂÆÉÁµêÂêà‰∫ÜÔºö(i) Â≠êÂúñÊÑüÁü•Â∞èÊâπÊ¨°ËôïÁêÜ‰ª•ÈºìÂãµÂõ∞Èõ£Ë≤†Èù¢ÊäΩÊ®£Ôºå‰ª•Âèä (ii) ‰∏ÄÁ®ÆÊñ∞ÁöÑÂ∞çÊØîÂ≠∏ÁøíÊñπÊ≥ïÔºåÂú®ÁµêÊßãÂ±¨ÊÄßÊñπÈù¢Êõ¥Â∞àÊ≥®ÊñºÊõ¥Âõ∞Èõ£ÁöÑÂØ¶È´îÂíåÊõ¥Âõ∞Èõ£ÁöÑË≤†‰∏âÂÖÉÁµÑ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞áÂ≠êÂúñÁöÑÁµêÊßãÊ≠∏Á¥çÂÅèË™§ÂÖ®Èù¢Á¥çÂÖ• PLM ÂæÆË™øÁöÑÁ†îÁ©∂„ÄÇÂú®ÂõõÂÄã KGC Âü∫Ê∫ñ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫Ü SATKGC ÁöÑÂÑ™Ë∂äÊÄß„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÁèæÂ∑≤ÂÖ¨Èñã„ÄÇ

##### **Abstraction Alignment: Comparing Model and Human Conceptual Relationships**
2407.12543v1 by Angie Boggust, Hyemin Bang, Hendrik Strobelt, Arvind Satyanarayan

Abstraction -- the process of generalizing specific examples into broad
reusable patterns -- is central to how people efficiently process and store
information and apply their knowledge to new data. Promisingly, research has
shown that ML models learn representations that span levels of abstraction,
from specific concepts like "bolo tie" and "car tire" to more general concepts
like "CEO" and "model". However, existing techniques analyze these
representations in isolation, treating learned concepts as independent
artifacts rather than an interconnected web of abstraction. As a result,
although we can identify the concepts a model uses to produce its output, it is
difficult to assess if it has learned a human-aligned abstraction of the
concepts that will generalize to new data. To address this gap, we introduce
abstraction alignment, a methodology to measure the agreement between a model's
learned abstraction and the expected human abstraction. We quantify abstraction
alignment by comparing model outputs against a human abstraction graph, such as
linguistic relationships or medical disease hierarchies. In evaluation tasks
interpreting image models, benchmarking language models, and analyzing medical
datasets, abstraction alignment provides a deeper understanding of model
behavior and dataset content, differentiating errors based on their agreement
with human knowledge, expanding the verbosity of current model quality metrics,
and revealing ways to improve existing human abstractions.

ÊëòË¶ÅÔºöÊäΩË±°Âåñ‚Äî‚ÄîÂ∞áÁâπÂÆöÁØÑ‰æãÊ¶ÇÊã¨ÁÇ∫Âª£Ê≥õÂèØÈáçË§á‰ΩøÁî®ÁöÑÊ®°ÂºèÁöÑÈÅéÁ®ã‚Äî‚ÄîÊòØ‰∫∫ÂÄëÊúâÊïàËôïÁêÜÂíåÂÑ≤Â≠òË≥áË®äÔºå‰∏¶Â∞áÂÖ∂Áü•Ë≠òÊáâÁî®ÊñºÊñ∞Ë≥áÊñôÁöÑÊ†∏ÂøÉ„ÄÇÊúâÂ∏åÊúõÁöÑÊòØÔºåÁ†îÁ©∂È°ØÁ§∫ ML Ê®°ÂûãÂ≠∏ÁøíË∑®Ë∂äÊäΩË±°Â±§Á¥öÁöÑË°®ÂæµÔºåÂæû„ÄåÁ¥∞È†òÂ∏∂„ÄçÂíå„ÄåÊ±ΩËªäËº™ËÉé„ÄçÁ≠âÂÖ∑È´îÊ¶ÇÂøµÂà∞„ÄåÂü∑Ë°åÈï∑„ÄçÂíå„ÄåÊ®°Âûã„ÄçÁ≠âÊõ¥‰∏ÄËà¨ÁöÑÊ¶ÇÂøµ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊäÄË°ìÂ≠§Á´ãÂú∞ÂàÜÊûêÈÄô‰∫õË°®ÂæµÔºåÂ∞áÂ≠∏ÁøíÂà∞ÁöÑÊ¶ÇÂøµË¶ñÁÇ∫Áç®Á´ãÁöÑÁî¢Áâ©ÔºåËÄå‰∏çÊòØÊäΩË±°ÁöÑÁõ∏‰∫íÈÄ£ÁµêÁ∂≤Ë∑Ø„ÄÇÂõ†Ê≠§ÔºåÂÑòÁÆ°ÊàëÂÄëÂèØ‰ª•Ë≠òÂà•Ê®°ÂûãÁî®‰æÜÁî¢ÁîüÂÖ∂Ëº∏Âá∫ÁöÑÊ¶ÇÂøµÔºå‰ΩÜÂæàÈõ£Ë©ï‰º∞ÂÆÉÊòØÂê¶Â≠∏ÁøíÂà∞Ê¶ÇÂøµÁöÑ‰∫∫È°ûÂ∞çÈΩäÊäΩË±°ÔºåÈÄô‰∫õÊ¶ÇÂøµÂ∞áÊ¶ÇÊã¨Âà∞Êñ∞ÁöÑË≥áÊñô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊäΩË±°Â∞çÈΩäÔºå‰∏ÄÁ®ÆË°°ÈáèÊ®°ÂûãÂ≠∏ÁøíÁöÑÊäΩË±°ËàáÈ†êÊúüÁöÑÊäΩË±°‰πãÈñì‰∏ÄËá¥ÊÄßÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÊ®°ÂûãËº∏Âá∫Ëàá‰∫∫È°ûÊäΩË±°ÂúñÂΩ¢Ôºà‰æãÂ¶ÇË™ûË®ÄÈóú‰øÇÊàñÈÜ´ÁôÇÁñæÁóÖÂ±§Á¥öÁµêÊßãÔºâÈÄ≤Ë°åÊØîËºÉ‰æÜÈáèÂåñÊäΩË±°Â∞çÈΩä„ÄÇÂú®Ëß£ÈáãÂΩ±ÂÉèÊ®°Âûã„ÄÅÂü∫Ê∫ñË™ûË®ÄÊ®°ÂûãÂíåÂàÜÊûêÈÜ´ÁôÇË≥áÊñôÈõÜÁöÑË©ï‰º∞‰ªªÂãô‰∏≠ÔºåÊäΩË±°Â∞çÈΩäÊèê‰æõ‰∫ÜÂ∞çÊ®°ÂûãË°åÁÇ∫ÂíåË≥áÊñôÈõÜÂÖßÂÆπÊõ¥Ê∑±ÂÖ•ÁöÑÁêÜËß£ÔºåÊ†πÊìöËàá‰∫∫È°ûÁü•Ë≠òÁöÑ‰∏ÄËá¥ÊÄßÂçÄÂàÜÈåØË™§ÔºåÊì¥Â±ïÁï∂ÂâçÊ®°ÂûãÂìÅË≥™ÊåáÊ®ôÁöÑË©≥Á¥∞Á®ãÂ∫¶Ôºå‰∏¶Êè≠Á§∫ÊîπÂñÑÁèæÊúâ‰∫∫È°ûÊäΩË±°ÁöÑÊñπÊ≥ï„ÄÇ

##### **Struct-X: Enhancing Large Language Models Reasoning with Structured Data**
2407.12522v1 by Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Yuan Cheng, Yinghui Xu, Wei Chu, Yuan Qi

Structured data, rich in logical and relational information, has the
potential to enhance the reasoning abilities of large language models (LLMs).
Still, its integration poses a challenge due to the risk of overwhelming LLMs
with excessive tokens and irrelevant context information. To address this, we
propose Struct-X, a novel framework that operates through five key phases:
``read-model-fill-reflect-reason'' efficiently enabling LLMs to utilize
structured data. It begins by encoding structured data into a topological space
using graph embeddings, followed by filling in missing entity information with
knowledge retrieval modules, and filtering out irrelevant tokens via a
self-supervised module. The final phase involves constructing a topological
network with selected tokens to further reduce the total token length for more
effective LLM inference. Additionally, Struct-X includes an Auxiliary Module
trained to generate prompts, aiding LLMs in analyzing structured data.
Extensive experiments on benchmarks, including the knowledge graph
question-answer task and the long document reading comprehension task, show
that Struct-X notably improves LLM reasoning, demonstrating the effectiveness
of structured data augmentation in improving LLM inference with complex input
context.

ÊëòË¶ÅÔºöÁµêÊßãÂåñË≥áÊñôÂØåÂê´ÈÇèËºØÂíåÈóú‰øÇË≥áË®äÔºåÊúâÊΩõÂäõÂ¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁî±ÊñºÈÅéÂ§öÁ¨¶ËôüÂíåÁÑ°ÈóúËÑàÁµ°Ë≥áË®äÂèØËÉΩÊúÉËÆì LLM ‰∏çÂ†™Ë≤†Ëç∑ÔºåÂõ†Ê≠§Êï¥ÂêàÊ≠§È°ûË≥áÊñôÊßãÊàê‰∫Ü‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ Struct-XÔºåÈÄôÊòØ‰∏ÄÂÄãÈÄèÈÅé‰∫îÂÄãÈóúÈçµÈöéÊÆµÈÅã‰ΩúÁöÑÊñ∞Á©éÊû∂ÊßãÔºö``ËÆÄÂèñ-Âª∫Ê®°-Â°´Ë£ú-ÂèçÊÄù-Êé®ÁêÜ''ÔºåÊúâÊïàÂú∞ËÆì LLM ËÉΩÂ§†Âà©Áî®ÁµêÊßãÂåñË≥áÊñô„ÄÇÂÆÉÈ¶ñÂÖà‰ΩøÁî®ÂúñÂΩ¢ÂµåÂÖ•Â∞áÁµêÊßãÂåñË≥áÊñôÁ∑®Á¢ºÂà∞ÊãìÊí≤Á©∫Èñì‰∏≠ÔºåÊé•ËëóÂà©Áî®Áü•Ë≠òÊì∑ÂèñÊ®°ÁµÑÂ°´Ë£úÈÅ∫Â§±ÁöÑÂØ¶È´îË≥áË®äÔºå‰∏¶ÈÄèÈÅéËá™ÊàëÁõ£Áù£Ê®°ÁµÑÁØ©ÈÅ∏Âá∫ÁÑ°ÈóúÁ¨¶Ëôü„ÄÇÊúÄÂæå‰∏ÄÂÄãÈöéÊÆµÊ∂âÂèäÂª∫Êßã‰∏ÄÂÄãÊãìÊí≤Á∂≤Ë∑ØÔºåÂÖ∂‰∏≠ÂåÖÂê´ÈÅ∏ÂÆöÁöÑÁ¨¶ËôüÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•Ê∏õÂ∞ëÁ∏ΩÁ¨¶ËôüÈï∑Â∫¶Ôºå‰ª•‰æøÊõ¥ÊúâÊïàÂú∞ÈÄ≤Ë°å LLM Êé®Ë´ñ„ÄÇÊ≠§Â§ñÔºåStruct-X ÈÇÑÂåÖÊã¨‰∏ÄÂÄãËºîÂä©Ê®°ÁµÑÔºåÁ∂ìÈÅéË®ìÁ∑¥ÂèØ‰ª•Áî¢ÁîüÊèêÁ§∫ÔºåÂçîÂä© LLM ÂàÜÊûêÁµêÊßãÂåñË≥áÊñô„ÄÇÂú®Âü∫Ê∫ñ‰∏äÁöÑÂ§ßÈáèÂØ¶È©óÔºåÂåÖÊã¨Áü•Ë≠òÂúñË≠úÂïèÁ≠î‰ªªÂãôÂíåÈï∑ÁØáÊñá‰ª∂Èñ±ËÆÄÁêÜËß£‰ªªÂãôÔºåÈ°ØÁ§∫ Struct-X ÊòéÈ°ØÊîπÂñÑ‰∫Ü LLM Êé®ÁêÜÔºåË≠âÊòé‰∫ÜÁµêÊßãÂåñË≥áÊñôÊì¥ÂÖÖÂú®ÊîπÂñÑ LLM Êé®Ë´ñÊôÇÁöÑÊúâÊïàÊÄßÔºåÁâπÂà•ÊòØÂú®Ëº∏ÂÖ•ËÑàÁµ°Ë§áÈõúÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇ

##### **Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models**
2407.12888v1 by Alexander R. Pelletier, Joseph Ramirez, Irsyad Adam, Simha Sankar, Yu Yan, Ding Wang, Dylan Steinecke, Wei Wang, Peipei Ping

The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.

ÊëòË¶ÅÔºö<paragraph>Áèæ‰ªäÂ§ßÈáèÁöÑÁîüÁâ©ÈÜ´Â≠∏Ë≥áË®äÂ∞çË©¶ÂúñÊúâÊïàÊ∂àÂåñ„ÄÅËôïÁêÜÂíåÁêÜËß£ÈÄô‰∫õÁôºÁèæÁöÑÁ†îÁ©∂‰∫∫Âì°ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊàêÁÇ∫Âú®ÈÄôÂÄãË§áÈõú‰∏îÂÖ∑ÊåëÊà∞ÊÄßÁöÑË≥áÊñôÁí∞Â¢É‰∏≠Â∞éËà™ÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåLLM ÂèØËÉΩÊúÉÂ∞éËá¥ÂπªË¶∫ÂèçÊáâÔºåÈÄô‰ΩøÂæóÊ™¢Á¥¢Êì¥Â¢ûÁîüÊàê (RAG) Â∞çÊñºÁç≤ÂæóÊ∫ñÁ¢∫Ë≥áË®äËá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÂÄãÂçîÂÆö‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ RUGGEDÔºàÂúñÂΩ¢Â∞éÂºïÂèØËß£ÈáãÁñæÁóÖÂçÄÂàÜÁöÑÊ™¢Á¥¢ÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÊó®Âú®ÊîØÊè¥Á†îÁ©∂‰∫∫Âì°ÈÄ≤Ë°åÁü•Ë≠òÊï¥ÂêàÂíåÂÅáË®≠Áî¢ÁîüÔºåÊâæÂá∫Á∂ìÈÅéÈ©óË≠âÁöÑÈÄ≤Â±ïË∑ØÂæë„ÄÇ‰æÜËá™Âá∫ÁâàÁâ©ÂíåÁü•Ë≠òÂ∫´ÁöÑÁõ∏ÈóúÁîüÁâ©ÈÜ´Â≠∏Ë≥áË®äÊúÉÈÄèÈÅéÊñáÊú¨Êé¢ÂãòÈóúËÅØÂàÜÊûêÂíåÁñæÁóÖÁØÄÈªûÁöÑÂèØËß£ÈáãÂúñÂΩ¢È†êÊ∏¨Ê®°ÂûãÈÄ≤Ë°åÊ™¢Èñ±„ÄÅÊï¥ÂêàÂíåËêÉÂèñÔºåÈ†êÊ∏¨Ëó•Áâ©ÂíåÁñæÁóÖ‰πãÈñìÁöÑÊΩõÂú®ÈóúËÅØ„ÄÇÈÄô‰∫õÂàÜÊûêÈÄ£ÂêåÁîüÁâ©ÈÜ´Â≠∏ÊñáÊú¨ÊúÉÊï¥ÂêàÂà∞‰∏ÄÂÄãÊû∂Êßã‰∏≠ÔºåË©≤Êû∂Êßã‰øÉÈÄ≤‰ΩøÁî®ËÄÖÂ∞éÂêëÁöÑÊ©üÂà∂Èó°ÊòéÔºå‰ª•ÂèäÈÄèÈÅé RAG ÂïüÁî®ÁöÑ LLM ÈÄ≤Ë°åÂÅáË®≠Êé¢Ë®é„ÄÇ‰∏ÄÂÄãËá®Â∫ä‰ΩøÁî®Ê°à‰æãÂ±ïÁ§∫‰∫Ü RUGGED Ë©ï‰º∞ÂíåÊé®Ëñ¶Áî®ÊñºÂøÉÂæãÂ§±Â∏∏ÊÄßÂøÉËÇåÁóÖËÆä (ACM) ÂíåÊì¥ÂºµÂûãÂøÉËÇåÁóÖËÆä (DCM) ÁöÑÊ≤ªÁôÇÊñπÊ≥ïÁöÑËÉΩÂäõÔºåÂàÜÊûêËôïÊñπËó•Áâ©ÁöÑÂàÜÂ≠ê‰∫§‰∫í‰ΩúÁî®ÂíåÊú™Êé¢Á¥¢ÁöÑÁî®ÈÄî„ÄÇÈÄôÂÄãÂπ≥Âè∞Â∞á LLM ÂπªË¶∫ÈôçÂà∞ÊúÄ‰ΩéÔºåÊèê‰æõÂèØÊìç‰ΩúÁöÑË¶ãËß£Ôºå‰∏¶ÊîπÂñÑÊñ∞Ê≤ªÁôÇÊñπÊ≥ïÁöÑÁ†îÁ©∂„ÄÇ</paragraph>

##### **A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting**
2407.11638v1 by He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua

Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation(RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast,
incorporating raw texts in specific complex events and fine-tuning LLMs
significantly improves performance. Moreover, enhanced with retrieval modules,
LLM can effectively capture temporal relational patterns hidden in historical
events. Meanwhile, issues such as popularity bias and the long-tail problem
still persist in LLMs, particularly in the RAG-based method. These findings not
only deepen our understanding of LLM-based event forecasting methods but also
highlight several promising research directions.We consider that this
comprehensive evaluation, along with the identified research opportunities,
will significantly contribute to future research on temporal event forecasting
through LLMs.

ÊëòË¶ÅÔºöËøëÊúüÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁßçËµÑÊñôÊé¢Âãò‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫ÊûÅÂ§ßÁöÑÊΩúÂäõÔºå‰æãÂ¶ÇÁü•ËØÜÈóÆÁ≠î„ÄÅÊï∞Â≠¶Êé®ÁêÜÂíåÂ∏∏ËØÜÊé®ÁêÜ„ÄÇÁÑ∂ËÄåÔºåLLM Âú®Êó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÈù¢ÁöÑÊé®ÁêÜËÉΩÂäõÂ∞öÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜÁ≥ªÁªüÊÄßÂú∞Ë∞ÉÊü•ÂÖ∂Âú®Êó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÈù¢ÁöÑËÉΩÂäõÔºåÊàë‰ª¨ÂØπÂü∫‰∫é LLM ÁöÑÊó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÊ≥ïËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑËØÑ‰º∞„ÄÇÁî±‰∫éÁº∫‰πèÂêåÊó∂ÂåÖÂê´ÂõæË°®ÂíåÊñáÊú¨ËµÑÊñôÁöÑÈ´òÂìÅË¥®Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨È¶ñÂÖàÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ MidEast-TE-mini ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ„ÄÇÂü∫‰∫éÊ≠§Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁ≥ªÂàóÂü∫Á∫øÊñπÊ≥ïÔºåÂÖ∂ÁâπÁÇπÊòØÂêÑÁßçËæìÂÖ•Ê†ºÂºèÂíåÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) Ê®°Âùó„ÄÇ‰ªéÂπøÊ≥õÁöÑÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨ÂèëÁé∞Áõ¥Êé•Â∞ÜÂéüÂßãÊñáÊú¨Êï¥ÂêàÂà∞ LLM ÁöÑËæìÂÖ•‰∏≠Âπ∂‰∏ç‰ºöÂ¢ûÂº∫Èõ∂Ê¨°Â≠¶‰π†Â§ñÊé®ÊÄßËÉΩ„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂú®ÁâπÂÆöÂ§çÊùÇ‰∫ã‰ª∂‰∏≠Á∫≥ÂÖ•ÂéüÂßãÊñáÊú¨Âπ∂ÂæÆË∞É LLM ‰ºöÊòæËëóÊèêÈ´òÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊ£ÄÁ¥¢Ê®°ÂùóÁöÑÂ¢ûÂº∫ÔºåLLM ÂèØ‰ª•ÊúâÊïàÂú∞ÊçïÊçâÈöêËóèÂú®ÂéÜÂè≤‰∫ã‰ª∂‰∏≠ÁöÑÊó∂Èó¥ÂÖ≥Á≥ªÊ®°Âºè„ÄÇÂêåÊó∂ÔºåËØ∏Â¶ÇÊµÅË°åÂ∫¶ÂÅèÂ∑ÆÂíåÈïøÂ∞æÈóÆÈ¢òÁ≠âÈóÆÈ¢ò‰ªçÁÑ∂Â≠òÂú®‰∫é LLM ‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÂú®Âü∫‰∫é RAG ÁöÑÊñπÊ≥ï‰∏≠„ÄÇËøô‰∫õÂèëÁé∞‰∏ç‰ªÖÂä†Ê∑±‰∫ÜÊàë‰ª¨ÂØπÂü∫‰∫é LLM ÁöÑ‰∫ã‰ª∂È¢ÑÊµãÊñπÊ≥ïÁöÑÁêÜËß£ÔºåËøòÁ™ÅÂá∫‰∫ÜÂá†‰∏™ÊúâÂâçÊôØÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËøôÈ°πÂÖ®Èù¢ÁöÑËØÑ‰º∞ÔºåËøûÂêåÂ∑≤Á°ÆÂÆöÁöÑÁ†îÁ©∂Êú∫‰ºöÔºåÂ∞ÜÊûÅÂ§ßÂú∞‰øÉËøõÈÄöËøá LLM ËøõË°åÊó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÁöÑÊú™Êù•Á†îÁ©∂„ÄÇ

##### **Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness**
2407.12068v2 by Kai Guo, Zewen Liu, Zhikai Chen, Hongzhi Wen, Wei Jin, Jiliang Tang, Yi Chang

Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing tasks. Recently, several LLMs-based
pipelines have been developed to enhance learning on graphs with text
attributes, showcasing promising performance. However, graphs are well-known to
be susceptible to adversarial attacks and it remains unclear whether LLMs
exhibit robustness in learning on graphs. To address this gap, our work aims to
explore the potential of LLMs in the context of adversarial attacks on graphs.
Specifically, we investigate the robustness against graph structural and
textual perturbations in terms of two dimensions: LLMs-as-Enhancers and
LLMs-as-Predictors. Through extensive experiments, we find that, compared to
shallow models, both LLMs-as-Enhancers and LLMs-as-Predictors offer superior
robustness against structural and textual attacks.Based on these findings, we
carried out additional analyses to investigate the underlying causes.
Furthermore, we have made our benchmark library openly available to facilitate
quick and fair evaluations, and to encourage ongoing innovative research in
this field.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠ÈÉΩÂ±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩ„ÄÇÊúÄËøëÔºåÂ∑≤ÈñãÁôºÂá∫Â§öÂÄãÂü∫Êñº LLM ÁöÑÁÆ°ÈÅìÔºå‰ª•Â¢ûÂº∑ÂÖ∑ÊúâÊñáÂ≠óÂ±¨ÊÄßÁöÑÂúñÂΩ¢Â≠∏ÁøíÔºåÂ±ïÁèæÂá∫‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂúñÂΩ¢ÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÊîªÊìäÔºåËÄå LLM Âú®ÂúñÂΩ¢Â≠∏Áøí‰∏≠ÊòØÂê¶Â±ïÁèæÂá∫Á©©ÂÅ•ÊÄß‰ªç‰∏çÊ∏ÖÊ•ö„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÁöÑÁ†îÁ©∂Êó®Âú®Êé¢Ë®é LLM Âú®ÂúñÂΩ¢Â∞çÊäóÊÄßÊîªÊìä‰∏≠ÁöÑÊΩõÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈáùÂ∞çÂÖ©ÂÄãÈù¢ÂêëÊé¢Ë®éÂÖ∂Â∞çÂúñÂΩ¢ÁµêÊßãÂíåÊñáÂ≠óÊìæÂãïÁöÑÁ©©ÂÅ•ÊÄßÔºöLLM ‰ΩúÁÇ∫Â¢ûÂº∑Âô®Âíå LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óÔºåÊàëÂÄëÁôºÁèæÔºåËàáÊ∑∫Â±§Ê®°ÂûãÁõ∏ÊØîÔºåLLM ‰ΩúÁÇ∫Â¢ûÂº∑Âô®Âíå LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®Âú®ÁµêÊßãÊÄßÂíåÊñáÂ≠óÊîªÊìä‰∏≠ÈÉΩÊèê‰æõÂÑ™Áï∞ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÈÄ≤Ë°å‰∫ÜÈ°çÂ§ñÁöÑÂàÜÊûê‰æÜÊé¢Ë®éÂÖ∂Ê†πÊú¨ÂéüÂõ†„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∑≤ÂÖ¨ÈñãÊàëÂÄëÁöÑÂü∫Ê∫ñÂ∫´Ôºå‰ª•Âà©Âø´ÈÄü‰∏îÂÖ¨Âπ≥ÁöÑË©ï‰º∞Ôºå‰∏¶ÈºìÂãµÊåÅÁ∫åÈÄ≤Ë°åÈÄôÊñπÈù¢ÁöÑÂâµÊñ∞Á†îÁ©∂„ÄÇ

##### **CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**
2407.11393v2 by Kalliopi Basioti, Mohamed A. Abdelsalam, Federico Fancellu, Vladimir Pavlovic, Afsaneh Fazly

Controllable Image Captioning (CIC) aims at generating natural language
descriptions for an image, conditioned on information provided by end users,
e.g., regions, entities or events of interest. However, available
image-language datasets mainly contain captions that describe the entirety of
an image, making them ineffective for training CIC models that can potentially
attend to any subset of regions or relationships. To tackle this challenge, we
propose a novel, fully automatic method to sample additional focused and
visually grounded captions using a unified structured semantic representation
built on top of the existing set of captions associated with an image. We
leverage Abstract Meaning Representation (AMR), a cross-lingual graph-based
semantic formalism, to encode all possible spatio-semantic relations between
entities, beyond the typical spatial-relations-only focus of current methods.
We use this Structured Semantic Augmentation (SSA) framework to augment
existing image-caption datasets with the grounded controlled captions,
increasing their spatial and semantic diversity and focal coverage. We then
develop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that
sources its control signals from SSA-diversified datasets. We empirically show
that, compared to SOTA CIC models, CIC-BART-SSA generates captions that are
superior in diversity and text quality, are competitive in controllability,
and, importantly, minimize the gap between broad and highly focused controlled
captioning performance by efficiently generalizing to the challenging highly
focused scenarios. Code is available at
https://github.com/SamsungLabs/CIC-BART-SSA.

ÊëòË¶ÅÔºöÂèØÊéßÂõæÂÉèÊ†áÊ≥® (CIC) Êó®Âú®ÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞‰ª•ÊèèËø∞ÂõæÂÉèÔºåÊù°‰ª∂ÊòØÊ†πÊçÆÊúÄÁªàÁî®Êà∑Êèê‰æõÁöÑËµÑËÆØÔºå‰æãÂ¶ÇÂå∫Âüü„ÄÅÂÆû‰ΩìÊàñÊÑüÂÖ¥Ë∂£ÁöÑ‰∫ã‰ª∂„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂõæÂÉèËØ≠Ë®ÄÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÂåÖÂê´ÊèèËø∞Êï¥‰∏™ÂõæÂÉèÁöÑÊ†áÊ≥®Ôºå‰ΩøÂÖ∂Êó†Ê≥ïÊúâÊïàËÆ≠ÁªÉ CIC Ê®°ÂûãÔºåËÄåËøô‰∫õÊ®°ÂûãÊúâÂèØËÉΩÂÖ≥Ê≥®‰ªª‰ΩïÂå∫ÂüüÊàñÂÖ≥Á≥ªÁöÑÂ≠êÈõÜ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∏ÄÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ„ÄÅÂÖ®Ëá™Âä®ÁöÑÊñπÊ≥ïÔºå‰ΩøÁî®Âª∫Á´ãÂú®‰∏éÂõæÂÉèÂÖ≥ËÅîÁöÑÁé∞ÊúâÊ†áÊ≥®ÈõÜ‰πã‰∏äÁöÑÁªü‰∏ÄÁªìÊûÑÂåñËØ≠‰πâË°®Á§∫Êù•ÊäΩÊ†∑ÂÖ∂‰ªñËÅöÁÑ¶‰∏îËßÜËßâÊé•Âú∞ÁöÑÊ†áÊ≥®„ÄÇÊàë‰ª¨Âà©Áî®Ë∑®ËØ≠Ë®ÄÂõæÂºèËØ≠‰πâÂΩ¢ÂºèÂåñÊäΩË±°ÊÑè‰πâË°®Á§∫ (AMR) Êù•ÁºñÁ†ÅÂÆû‰Ωì‰πãÈó¥ÊâÄÊúâÂèØËÉΩÁöÑÁ©∫Èó¥ËØ≠‰πâÂÖ≥Á≥ªÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂΩìÂâçÊñπÊ≥ï‰∏≠‰ªÖÂÖ≥Ê≥®ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇÊàë‰ª¨‰ΩøÁî®ËøôÁßçÁªìÊûÑÂåñËØ≠‰πâÂ¢ûÂº∫ (SSA) Ê°ÜÊû∂Êù•Â¢ûÂº∫Áé∞ÊúâÁöÑÂõæÂÉèÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºå‰ΩøÂÖ∂Êé•Âú∞‰∏îÂèØÊéßÁöÑÊ†áÊ≥®ÔºåÂ¢ûÂä†ÂÆÉ‰ª¨ÁöÑÁ©∫Èó¥ÂíåËØ≠‰πâÂ§öÊ†∑ÊÄß‰ª•ÂèäÁÑ¶ÁÇπË¶ÜÁõñËåÉÂõ¥„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Êñ∞Ê®°Âûã CIC-BART-SSAÔºå‰∏ìÈó®ÈíàÂØπ CIC ‰ªªÂä°ÈáèË∫´ÂÆöÂà∂ÔºåÂÖ∂ÊéßÂà∂‰ø°Âè∑Êù•Ëá™ SSA Â§öÊ†∑ÂåñÁöÑÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨Âá≠ÁªèÈ™åË°®ÊòéÔºå‰∏é SOTA CIC Ê®°ÂûãÁõ∏ÊØîÔºåCIC-BART-SSA ÁîüÊàêÁöÑÊ†áÊ≥®Âú®Â§öÊ†∑ÊÄßÂíåÊñáÊú¨Ë¥®ÈáèÊñπÈù¢Êõ¥ËÉú‰∏ÄÁ≠πÔºåÂú®ÂèØÊéßÊÄßÊñπÈù¢ÂÖ∑ÊúâÁ´û‰∫âÂäõÔºåËÄå‰∏îÈáçË¶ÅÁöÑÊòØÔºåÈÄöËøáÊúâÊïàÂú∞Êé®ÂπøÂà∞ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈ´òÂ∫¶ËÅöÁÑ¶Âú∫ÊôØÔºåÊúÄÂ§ßÈôêÂ∫¶Âú∞Áº©Â∞è‰∫ÜÂπøÊ≥õÂíåÈ´òÂ∫¶ËÅöÁÑ¶ÁöÑÂèóÊéßÊ†áÊ≥®ÊÄßËÉΩ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ‰ª£Á†ÅÂèØ‰ªé https://github.com/SamsungLabs/CIC-BART-SSA Ëé∑Âæó„ÄÇ

##### **Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval**
2407.10805v3 by Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo

Retrieval-augmented generation (RAG) has significantly advanced large
language models (LLMs) by enabling dynamic information retrieval to mitigate
knowledge gaps and hallucinations in generated content. However, these systems
often falter with complex reasoning and consistency across diverse queries. In
this work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns
questions with the knowledge graph and uses it as a navigational tool, which
deepens and refines the RAG paradigm for information collection and
integration. The KG-guided navigation fosters deep and long-range associations
to uphold logical consistency and optimize the scope of retrieval for precision
and interoperability. In conjunction, factual consistency can be better ensured
through semantic similarity guided by precise directives. ToG${2.0}$ not only
improves the accuracy and reliability of LLMs' responses but also demonstrates
the potential of hybrid structured knowledge systems to significantly advance
LLM reasoning, aligning it closer to human-like performance. We conducted
extensive experiments on four public datasets to demonstrate the advantages of
our method compared to the baseline.

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Â∑≤Â§ßÂπÖÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåËóâÁî±ÂïüÁî®ÂãïÊÖãË≥áË®äÊ™¢Á¥¢‰æÜÊ∏õËºïÁî¢ÁîüÂÖßÂÆπ‰∏≠ÁöÑÁü•Ë≠òÂ∑ÆË∑ùÂíåÂπªË¶∫„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÁ≥ªÁµ±Âú®Ë§áÈõúÊé®ÁêÜÂíå‰∏çÂêåÊü•Ë©¢ÈñìÁöÑ‰∏ÄËá¥ÊÄßÊñπÈù¢Á∂ìÂ∏∏ÊúÉÂá∫ÈåØ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ Think-on-Graph 2.0Ôºå‰∏ÄÂÄãÂ¢ûÂº∑ÁöÑ RAG Êû∂ÊßãÔºåÂÆÉÊúÉÂ∞áÂïèÈ°åËàáÁü•Ë≠òÂúñË≠úÂ∞çÈΩäÔºå‰∏¶Â∞áÂÖ∂Áî®‰ΩúÂ∞éËà™Â∑•ÂÖ∑ÔºåÈÄôÊúÉÂä†Ê∑±ÂíåÊîπÂñÑ RAG ÂÖ∏ÁØÑ‰ª•ÈÄ≤Ë°åË≥áË®äÊî∂ÈõÜÂíåÊï¥Âêà„ÄÇKG ÂºïÂ∞éÁöÑÂ∞éËà™‰øÉÈÄ≤Ê∑±Â∫¶‰∏îÈï∑Á®ãÈóúËÅØÔºå‰ª•Á∂≠ÊåÅÈÇèËºØ‰∏ÄËá¥ÊÄßÔºå‰∏¶ÊúÄ‰Ω≥ÂåñÊ™¢Á¥¢ÁØÑÂúç‰ª•ÊèêÂçáÁ≤æÊ∫ñÂ∫¶Âíå‰∫íÊìç‰ΩúÊÄß„ÄÇÁµêÂêà‰ΩøÁî®Ôºå‰∫ãÂØ¶‰∏ÄËá¥ÊÄßÂèØÈÄèÈÅéÁî±Á≤æÁ¢∫ÊåáÁ§∫ÂºïÂ∞éÁöÑË™ûÁæ©Áõ∏‰ººÊÄßÁç≤ÂæóÊõ¥Â•ΩÁöÑÁ¢∫‰øù„ÄÇToG${2.0}$ ‰∏çÂÉÖÊèêÂçá LLM ÂõûÊáâÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂèØÈù†Â∫¶Ôºå‰πüË≠âÊòéÊ∑∑ÂêàÁµêÊßãÂåñÁü•Ë≠òÁ≥ªÁµ±ÊúâÊΩõÂäõÂ§ßÂπÖÊèêÂçá LLM Êé®ÁêÜÔºå‰ΩøÂÖ∂Êõ¥Êé•Ëøë‰∫∫È°ûËà¨ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÂú®ÂõõÂÄãÂÖ¨ÈñãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•Ë≠âÊòéÊàëÂÄëÁöÑÊñπÊ≥ïÂÑ™ÊñºÂü∫Á∑ö„ÄÇ

##### **Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education**
2407.10794v1 by Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang Jiang, Freddy Lecue, Jinghui Lu, Irene Li

Knowledge graphs (KGs) are crucial in the field of artificial intelligence
and are widely applied in downstream tasks, such as enhancing Question
Answering (QA) systems. The construction of KGs typically requires significant
effort from domain experts. Recently, Large Language Models (LLMs) have been
used for knowledge graph construction (KGC), however, most existing approaches
focus on a local perspective, extracting knowledge triplets from individual
sentences or documents. In this work, we introduce Graphusion, a zero-shot KGC
framework from free text. The core fusion module provides a global view of
triplets, incorporating entity merging, conflict resolution, and novel triplet
discovery. We showcase how Graphusion could be applied to the natural language
processing (NLP) domain and validate it in the educational scenario.
Specifically, we introduce TutorQA, a new expert-verified benchmark for graph
reasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our
evaluation demonstrates that Graphusion surpasses supervised baselines by up to
10% in accuracy on link prediction. Additionally, it achieves average scores of
2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and
relation recognition, respectively.

ÊëòË¶ÅÔºö<paragraph>Áü•Ë≠òÂúñË≠ú (KG) Âú®‰∫∫Â∑•Êô∫ÊÖßÈ†òÂüüËá≥ÈóúÈáçË¶ÅÔºå‰∏¶Âª£Ê≥õÊáâÁî®Êñº‰∏ãÊ∏∏‰ªªÂãôÔºå‰æãÂ¶ÇÂ¢ûÂº∑ÂïèÁ≠î (QA) Á≥ªÁµ±„ÄÇÁü•Ë≠òÂúñË≠úÁöÑÂª∫ÊßãÈÄöÂ∏∏ÈúÄË¶ÅÈ†òÂüüÂ∞àÂÆ∂ÁöÑÂ§ßÈáèÂ∑•‰Ωú„ÄÇÊúÄËøëÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Ë¢´Áî®ÊñºÁü•Ë≠òÂúñË≠úÂª∫Êßã (KGC)ÔºåÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÂ§ßÂ§öÈóúÊ≥®Â±ÄÈÉ®ËßÄÈªûÔºåÂæûÂÄãÂà•Âè•Â≠êÊàñÊñá‰ª∂‰∏≠ÊèêÂèñÁü•Ë≠ò‰∏âÂÖÉÁµÑ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü GraphusionÔºå‰∏ÄÂÄãÂæûËá™Áî±ÊñáÊú¨‰∏≠ÈÄ≤Ë°åÈõ∂Ê¨°Â≠∏ÁøíÁöÑ KGC Ê°ÜÊû∂„ÄÇÊ†∏ÂøÉËûçÂêàÊ®°ÁµÑÊèê‰æõ‰∏âÂÖÉÁµÑÁöÑÂÖ®Â±ÄËßÄÈªûÔºåÂåÖÂê´ÂØ¶È´îÂêà‰Ωµ„ÄÅË°ùÁ™ÅËß£Ê±∫ÂíåÊñ∞‰∏âÂÖÉÁµÑÁôºÁèæ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞á Graphusion ÊáâÁî®ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) È†òÂüüÔºå‰∏¶Âú®ÊïôËÇ≤Â†¥ÊôØ‰∏≠È©óË≠âÂÆÉ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü TutorQAÔºå‰∏ÄÂÄãÊñ∞ÁöÑÁî±Â∞àÂÆ∂È©óË≠âÁöÑÂúñË≠úÊé®ÁêÜÂíåÂïèÁ≠îÂü∫Ê∫ñÔºåÂåÖÂê´ÂÖ≠È†Ö‰ªªÂãôÂíåÁ∏ΩË®à 1,200 ÂÄãÂïèÁ≠îÂ∞ç„ÄÇÊàëÂÄëÁöÑË©ï‰º∞Ë°®ÊòéÔºåGraphusion Âú®ÈÄ£ÁµêÈ†êÊ∏¨ÁöÑÊ∫ñÁ¢∫Â∫¶‰∏äÊØîÁõ£Áù£ÂºèÂü∫Ê∫ñÈ´òÂá∫ 10%„ÄÇÊ≠§Â§ñÔºåÂú®Ê¶ÇÂøµÂØ¶È´îÊèêÂèñÂíåÈóú‰øÇË≠òÂà•ÁöÑ‰∫∫È°ûË©ï‰º∞‰∏≠ÔºåÂÆÉÂàÜÂà•Áç≤Âæó‰∫Ü 3 ÂàÜ‰∏≠ÁöÑ 2.92 ÂàÜÂíå 2.37 ÂàÜ„ÄÇ</paragraph>

##### **GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework**
2407.10793v1 by Hannah Sansford, Nicholas Richardson, Hermina Petric Maretic, Juba Nait Saada

Methods to evaluate Large Language Model (LLM) responses and detect
inconsistencies, also known as hallucinations, with respect to the provided
knowledge, are becoming increasingly important for LLM applications. Current
metrics fall short in their ability to provide explainable decisions,
systematically check all pieces of information in the response, and are often
too computationally expensive to be used in practice. We present GraphEval: a
hallucination evaluation framework based on representing information in
Knowledge Graph (KG) structures. Our method identifies the specific triples in
the KG that are prone to hallucinations and hence provides more insight into
where in the response a hallucination has occurred, if at all, than previous
methods. Furthermore, using our approach in conjunction with state-of-the-art
natural language inference (NLI) models leads to an improvement in balanced
accuracy on various hallucination benchmarks, compared to using the raw NLI
models. Lastly, we explore the use of GraphEval for hallucination correction by
leveraging the structure of the KG, a method we name GraphCorrect, and
demonstrate that the majority of hallucinations can indeed be rectified.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂõûÊáâË©ï‰º∞ÊñπÊ≥ïÂíå‰∏ç‰∏ÄËá¥ÊÄßÂÅµÊ∏¨ÔºàÂèàÁ®±ÁÇ∫ÂπªË¶∫ÔºâÔºåÁõ∏Â∞çÊñºÊâÄÊèê‰æõÁöÑÁü•Ë≠òÔºåÂ∞çÊñº LLM ÊáâÁî®Ê≠£ËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÁõÆÂâçÁöÑÊåáÊ®ôÁÑ°Ê≥ïÊèê‰æõÂèØËß£ÈáãÁöÑÊ±∫Á≠ñ„ÄÅÁ≥ªÁµ±ÊÄßÂú∞Ê™¢Êü•ÂõûÊáâ‰∏≠ÁöÑÊâÄÊúâË≥áË®äÔºåËÄå‰∏îÂú®ÂØ¶Âãô‰∏ä‰ΩøÁî®ÊôÇÔºåÈÄöÂ∏∏ÈÅéÊñºËÄóË≤ªÈÅãÁÆóË≥áÊ∫ê„ÄÇÊàëÂÄëÊèêÂá∫ GraphEvalÔºö‰∏ÄÂÄãÂü∫ÊñºÁü•Ë≠òÂúñ (KG) ÁµêÊßã‰æÜË°®Á§∫Ë≥áË®äÁöÑÂπªË¶∫Ë©ï‰º∞Êû∂Êßã„ÄÇÊàëÂÄëÁöÑÊäÄË°ìË≠òÂà•Âá∫ÂÆπÊòìÂá∫ÁèæÂπªË¶∫ÁöÑ KG ‰∏≠ÁâπÂÆö‰∏âÂÖÉÁµÑÔºåÂõ†Ê≠§ÊØî‰ª•ÂæÄÁöÑÊñπÊ≥ïÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÂõûÊáâ‰∏≠ÂπªË¶∫ÁôºÁîüÂú®Âì™Ë£°ÔºàÂ¶ÇÊûúÊúâÁöÑË©±Ôºâ„ÄÇÊ≠§Â§ñÔºåÂ∞áÊàëÂÄëÁöÑÊñπÊ≥ïËàáÊúÄÂÖàÈÄ≤ÁöÑËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI) Ê®°ÂûãÁµêÂêà‰ΩøÁî®ÔºåËàá‰ΩøÁî®ÂéüÂßã NLI Ê®°ÂûãÁõ∏ÊØîÔºåÂèØ‰ª•Âú®ÂêÑÁ®ÆÂπªË¶∫Âü∫Ê∫ñ‰∏äÊèêÈ´òÂπ≥Ë°°Ê∫ñÁ¢∫Â∫¶„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Á¥¢‰ΩøÁî® GraphEval ‰æÜÈÄ≤Ë°åÂπªË¶∫‰øÆÊ≠£ÔºåÊñπÊ≥ïÊòØÂà©Áî® KG ÁöÑÁµêÊßãÔºåÊàëÂÄëÂ∞áÊ≠§ÊñπÊ≥ïÂëΩÂêçÁÇ∫ GraphCorrectÔºå‰∏¶Ë≠âÊòéÂ§ßÂ§öÊï∏ÂπªË¶∫Á¢∫ÂØ¶ÂèØ‰ª•ÂæóÂà∞Á≥æÊ≠£„ÄÇ

##### **Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using Datagraphs**
2407.10743v1 by W. J. Meijer, A. C. Kemmeren, E. H. J. Riemens, J. E. Fransman, M. van Bekkum, G. J. Burghouts, J. D. van Mil

This paper addresses the challenge of scaling Large Multimodal Models (LMMs)
to expansive 3D environments. Solving this open problem is especially relevant
for robot deployment in many first-responder scenarios, such as
search-and-rescue missions that cover vast spaces. The use of LMMs in these
settings is currently hampered by the strict context windows that limit the
LMM's input size. We therefore introduce a novel approach that utilizes a
datagraph structure, which allows the LMM to iteratively query smaller sections
of a large environment. Using the datagraph in conjunction with graph traversal
algorithms, we can prioritize the most relevant locations to the query, thereby
improving the scalability of 3D scene language tasks. We illustrate the
datagraph using 3D scenes, but these can be easily substituted by other dense
modalities that represent the environment, such as pointclouds or Gaussian
splats. We demonstrate the potential to use the datagraph for two 3D scene
language task use cases, in a search-and-rescue mission example.

ÊëòË¶ÅÔºöÊú¨ÊñáË®éË´ñ‰∫ÜÂ∞áÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°Âûã (LMM) Êì¥Â±ïÂà∞Âª£Èóä 3D Áí∞Â¢ÉÁöÑÊåëÊà∞„ÄÇËß£Ê±∫ÈÄôÂÄãÈñãÊîæÊÄßÂïèÈ°åÂ∞çÊñºÊ©üÂô®‰∫∫Âú®Ë®±Â§öÁ¨¨‰∏ÄÂèçÊáâ‰∫∫Âì°Â†¥ÊôØ‰∏≠ÁöÑÈÉ®ÁΩ≤ÁâπÂà•Áõ∏ÈóúÔºå‰æãÂ¶ÇÊ∂µËìãÂª£ÈóäÁ©∫ÈñìÁöÑÊêúÊïë‰ªªÂãô„ÄÇÈÄô‰∫õË®≠ÂÆö‰∏≠‰ΩøÁî® LMM ÁõÆÂâçÂèóÂà∞Âö¥Ê†ºÁöÑ‰∏ä‰∏ãÊñáË¶ñÁ™óÈôêÂà∂ÔºåÈÄôÈôêÂà∂‰∫Ü LMM ÁöÑËº∏ÂÖ•Â§ßÂ∞è„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂà©Áî®Ë≥áÊñôÂúñÁµêÊßãÔºåÂÖÅË®± LMM Ëø≠‰ª£Êü•Ë©¢Â§ßÂûãÁí∞Â¢ÉÁöÑËºÉÂ∞èÈÉ®ÂàÜ„ÄÇÈÄèÈÅéÂ∞áË≥áÊñôÂúñËàáÂúñÂΩ¢ÈÅçÊ≠∑ÊºîÁÆóÊ≥ïÁµêÂêà‰ΩøÁî®ÔºåÊàëÂÄëÂèØ‰ª•ÂÑ™ÂÖàËÄÉÊÖÆËàáÊü•Ë©¢ÊúÄÁõ∏ÈóúÁöÑ‰ΩçÁΩÆÔºåÂæûËÄåÊèêÈ´ò 3D Â†¥ÊôØË™ûË®Ä‰ªªÂãôÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇÊàëÂÄë‰ΩøÁî® 3D Â†¥ÊôØË™™ÊòéË≥áÊñôÂúñÔºå‰ΩÜÈÄô‰∫õÂ†¥ÊôØÂèØ‰ª•ËºïÈ¨ÜÂú∞Áî±ÂÖ∂‰ªñË°®Á§∫Áí∞Â¢ÉÁöÑÂØÜÈõÜÊ®°ÂºèÂèñ‰ª£Ôºå‰æãÂ¶ÇÈªûÈõ≤ÊàñÈ´òÊñØÈªû„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂú®ÊêúÊïë‰ªªÂãôÁØÑ‰æã‰∏≠‰ΩøÁî®Ë≥áÊñôÂúñÈÄ≤Ë°åÂÖ©ÂÄã 3D Â†¥ÊôØË™ûË®Ä‰ªªÂãôÁî®‰æãÁöÑÊΩõÂäõ„ÄÇ

##### **AutoGRAMS: Autonomous Graphical Agent Modeling Software**
2407.10049v1 by Ben Krause, Lucia Chen, Emmanuel Kahembwe

We introduce the AutoGRAMS framework for programming multi-step interactions
with language models. AutoGRAMS represents AI agents as a graph, where each
node can execute either a language modeling instruction or traditional code.
Likewise, transitions in the graph can be governed by either language modeling
decisions or traditional branch logic. AutoGRAMS supports using variables as
memory and allows nodes to call other AutoGRAMS graphs as functions. We show
how AutoGRAMS can be used to design highly sophisticated agents, including
self-referential agents that can modify their own graph. AutoGRAMS's
graph-centric approach aids interpretability, controllability, and safety
during the design, development, and deployment of AI agents. We provide our
framework as open source at https://github.com/autograms/autograms .

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π AutoGRAMS Ê°ÜÊû∂ÔºåÁî®ÊñºÁ∑®ÂØ´ËàáË™ûË®ÄÊ®°ÂûãÁöÑÂ§öÊ≠•È©ü‰∫íÂãï„ÄÇAutoGRAMS Â∞á AI ‰ª£ÁêÜË°®Á§∫ÁÇ∫‰∏ÄÂÄãÂúñÂΩ¢ÔºåÂÖ∂‰∏≠ÊØèÂÄãÁØÄÈªûÂèØ‰ª•Âü∑Ë°åË™ûË®ÄÂª∫Ê®°Êåá‰ª§ÊàñÂÇ≥Áµ±‰ª£Á¢º„ÄÇÂêåÊ®£Âú∞ÔºåÂúñÂΩ¢‰∏≠ÁöÑËΩâÊèõÂèØ‰ª•Áî±Ë™ûË®ÄÂª∫Ê®°Ê±∫Á≠ñÊàñÂÇ≥Áµ±ÂàÜÊîØÈÇèËºØÊéßÂà∂„ÄÇAutoGRAMS ÊîØÊè¥‰ΩøÁî®ËÆäÊï∏‰ΩúÁÇ∫Ë®òÊÜ∂È´îÔºå‰∏¶ÂÖÅË®±ÁØÄÈªûÂëºÂè´ÂÖ∂‰ªñ AutoGRAMS ÂúñÂΩ¢‰ΩúÁÇ∫ÂáΩÂºè„ÄÇÊàëÂÄëÂ±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî® AutoGRAMS Ë®≠Ë®àÈ´òÂ∫¶Ë§áÈõúÁöÑ‰ª£ÁêÜÔºåÂåÖÊã¨ÂèØ‰ª•‰øÆÊîπËá™Ë∫´ÂúñÂΩ¢ÁöÑËá™ÂèÉÁÖß‰ª£ÁêÜ„ÄÇAutoGRAMS ‰ª•ÂúñÂΩ¢ÁÇ∫‰∏≠ÂøÉÁöÑÊñπÊ≥ïÊúâÂä©ÊñºÂú® AI ‰ª£ÁêÜÁöÑË®≠Ë®à„ÄÅÈñãÁôºÂíåÈÉ®ÁΩ≤ÈÅéÁ®ã‰∏≠ÊèêÈ´òÂèØËß£ÈáãÊÄß„ÄÅÂèØÊéßÊÄßÂíåÂÆâÂÖ®ÊÄß„ÄÇÊàëÂÄëÂú® https://github.com/autograms/autograms Êèê‰æõÊàëÂÄëÁöÑÊ°ÜÊû∂‰ΩúÁÇ∫ÈñãÊ∫ê„ÄÇ

##### **FarFetched: Entity-centric Reasoning and Claim Validation for the Greek Language based on Textually Represented Environments**
2407.09888v1 by Dimitris Papadopoulos, Katerina Metropoulou, Nikolaos Matsatsinis, Nikolaos Papadakis

Our collective attention span is shortened by the flood of online
information. With \textit{FarFetched}, we address the need for automated claim
validation based on the aggregated evidence derived from multiple online news
sources. We introduce an entity-centric reasoning framework in which latent
connections between events, actions, or statements are revealed via entity
mentions and represented in a graph database. Using entity linking and semantic
similarity, we offer a way for collecting and combining information from
diverse sources in order to generate evidence relevant to the user's claim.
Then, we leverage textual entailment recognition to quantitatively determine
whether this assertion is credible, based on the created evidence. Our approach
tries to fill the gap in automated claim validation for less-resourced
languages and is showcased on the Greek language, complemented by the training
of relevant semantic textual similarity (STS) and natural language inference
(NLI) models that are evaluated on translated versions of common benchmarks.

ÊëòË¶ÅÔºöÁ∂≤Ë∑ØË≥áË®äÁöÑÊ¥™ÊµÅÁ∏ÆÁü≠‰∫ÜÊàëÂÄëÁöÑÈõÜÈ´îÊ≥®ÊÑèÂäõÊôÇÈñì„ÄÇÈÄèÈÅé \textit{FarFetched}ÔºåÊàëÂÄëËß£Ê±∫‰∫ÜÊ†πÊìöÂæûÂ§öÂÄãÁ∑ö‰∏äÊñ∞ËÅû‰æÜÊ∫êÂΩôÁ∏ΩÁöÑË≠âÊìöÈÄ≤Ë°åËá™ÂãïÂåñËÅ≤ÊòéÈ©óË≠âÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄã‰ª•ÂØ¶È´îÁÇ∫‰∏≠ÂøÉÁöÑÊé®ÁêÜÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠‰∫ã‰ª∂„ÄÅÂãï‰ΩúÊàñÈô≥Ëø∞‰πãÈñìÁöÑÊΩõÂú®ÈóúËÅØÈÄèÈÅéÂØ¶È´îÊèêÂèäË¢´Êè≠Èú≤Ôºå‰∏¶Âú®ÂúñÂΩ¢Ë≥áÊñôÂ∫´‰∏≠Ë°®Á§∫„ÄÇ‰ΩøÁî®ÂØ¶È´îÈÄ£ÁµêÂíåË™ûÁæ©Áõ∏‰ººÊÄßÔºåÊàëÂÄëÊèê‰æõ‰∏ÄÁ®ÆÊñπÂºè‰æÜÊî∂ÈõÜÂíåÁµÑÂêà‰æÜËá™‰∏çÂêå‰æÜÊ∫êÁöÑË≥áË®äÔºå‰ª•Áî¢ÁîüËàá‰ΩøÁî®ËÄÖËÅ≤ÊòéÁõ∏ÈóúÁöÑË≠âÊìö„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂà©Áî®ÊñáÊú¨ËòäÊ∂µË≠òÂà•‰æÜÊ†πÊìöÂª∫Á´ãÁöÑË≠âÊìöÈáèÂåñÁ¢∫ÂÆöÊ≠§Êñ∑Ë®ÄÊòØÂê¶ÂèØ‰ø°„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïË©¶ÂúñÂ°´Ë£úË≥áÊ∫êËºÉÂ∞ëÁöÑË™ûË®ÄÁöÑËá™ÂãïÂåñËÅ≤ÊòéÈ©óË≠âÊñπÈù¢ÁöÑÁ©∫ÁôΩÔºå‰∏¶Âú®Â∏åËáòË™û‰∏≠Â±ïÁ§∫ÔºåËºî‰ª•Â∞çÁõ∏ÈóúË™ûÁæ©ÊñáÊú¨Áõ∏‰ººÊÄß (STS) ÂíåËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI) Ê®°ÂûãÁöÑË®ìÁ∑¥ÔºåÈÄô‰∫õÊ®°ÂûãÂú®Â∏∏Ë¶ãÂü∫Ê∫ñÁöÑÁøªË≠ØÁâàÊú¨‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇ

##### **GOFA: A Generative One-For-All Model for Joint Graph Language Modeling**
2407.09709v1 by Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang

Foundation models, such as Large Language Models (LLMs) or Large Vision
Models (LVMs), have emerged as one of the most powerful tools in the respective
fields. However, unlike text and image data, graph data do not have a
definitive structure, posing great challenges to developing a Graph Foundation
Model (GFM). For example, current attempts at designing general graph models
either transform graph data into a language format for LLM-based prediction or
still train a GNN model with LLM as an assistant. The former can handle
unlimited tasks, while the latter captures graph structure much better -- yet,
no existing work can achieve both simultaneously. In this paper, we identify
three key desirable properties of a GFM: self-supervised pretraining, fluidity
in tasks, and graph awareness. To account for these properties, we extend the
conventional language modeling to the graph domain and propose a novel
generative graph language model GOFA to solve the problem. The model
interleaves randomly initialized GNN layers into a frozen pre-trained LLM so
that the semantic and structural modeling abilities are organically combined.
GOFA is pre-trained on newly proposed graph-level next-word prediction,
question-answering, and structural tasks to obtain the above GFM properties.
The pre-trained model is further fine-tuned on downstream tasks to obtain
task-solving ability. The fine-tuned model is evaluated on various downstream
tasks, demonstrating a strong ability to solve structural and contextual
problems in zero-shot scenarios. The code is available at
https://github.com/JiaruiFeng/GOFA.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÔºå‰æãÂ¶ÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊàñÂ§ßÂûãË¶ñË¶∫Ê®°Âûã (LVM)ÔºåÂ∑≤ÊàêÁÇ∫ÂêÑËá™È†òÂüü‰∏≠ÊúÄÊúâÂäõÁöÑÂ∑•ÂÖ∑‰πã‰∏Ä„ÄÇÁÑ∂ËÄåÔºåËàáÊñáÊú¨ÂíåÂΩ±ÂÉèË≥áÊñô‰∏çÂêåÔºåÂúñÂΩ¢Ë≥áÊñôÊ≤íÊúâÊòéÁ¢∫ÁöÑÁµêÊßãÔºåÂ∞çÈñãÁôºÂúñÂΩ¢Âü∫Á§éÊ®°Âûã (GFM) ÊßãÊàêÊ•µÂ§ßÁöÑÊåëÊà∞„ÄÇ‰æãÂ¶ÇÔºåÁõÆÂâçË®≠Ë®àÈÄöÁî®ÂúñÂΩ¢Ê®°ÂûãÁöÑÂòóË©¶Ôºå‰∏çÊòØÂ∞áÂúñÂΩ¢Ë≥áÊñôËΩâÊèõÁÇ∫Ë™ûË®ÄÊ†ºÂºè‰ª•‰æõÂü∫Êñº LLM ÁöÑÈ†êÊ∏¨ÔºåÂ∞±ÊòØË®ìÁ∑¥ GNN Ê®°ÂûãÔºå‰∏¶‰ª• LLM ‰ΩúÁÇ∫ËºîÂä©„ÄÇÂâçËÄÖÂèØ‰ª•ËôïÁêÜÁÑ°ÈôêÁöÑ‰ªªÂãôÔºåËÄåÂæåËÄÖÂèØ‰ª•Êõ¥Â•ΩÂú∞Êì∑ÂèñÂúñÂΩ¢ÁµêÊßãÔºå‰ΩÜÁèæÊúâÁöÑÂ∑•‰ΩúÁÑ°Ê≥ïÂêåÊôÇÈÅîÊàêÈÄôÂÖ©ËÄÖ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊâæÂá∫ GFM ÁöÑ‰∏âÂÄãÈóúÈçµÁêÜÊÉ≥ÁâπÊÄßÔºöËá™ÊàëÁõ£Áù£È†êË®ìÁ∑¥„ÄÅ‰ªªÂãôÊµÅÊö¢Â∫¶ÂíåÂúñÂΩ¢ÊÑüÁü•„ÄÇÁÇ∫‰∫ÜËÄÉÈáèÈÄô‰∫õÁâπÊÄßÔºåÊàëÂÄëÂ∞áÂÇ≥Áµ±ÁöÑË™ûË®ÄÂª∫Ê®°Êì¥ÂÖÖÂà∞ÂúñÂΩ¢È†òÂüüÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÁîüÊàêÂºèÂúñÂΩ¢Ë™ûË®ÄÊ®°Âûã GOFA ‰æÜËß£Ê±∫ÂïèÈ°å„ÄÇÊ≠§Ê®°ÂûãÂ∞áÈö®Ê©üÂàùÂßãÂåñÁöÑ GNN Â±§‰∫§ÈåØÊèíÂÖ•ÂáçÁµêÁöÑÈ†êË®ìÁ∑¥ LLM ‰∏≠Ôºå‰ª•‰æøË™ûÊÑèÂíåÁµêÊßãÂª∫Ê®°ËÉΩÂäõÊúâÊ©üÁµêÂêà„ÄÇGOFA Êé°Áî®Êñ∞ÊèêÂá∫ÁöÑÂúñÂΩ¢Â±§Á¥ö‰∏ã‰∏ÄÂÄãÂ≠óÈ†êÊ∏¨„ÄÅÂïèÁ≠îÂíåÁµêÊßã‰ªªÂãôÈÄ≤Ë°åÈ†êË®ìÁ∑¥Ôºå‰ª•ÂèñÂæó‰∏äËø∞ GFM ÁâπÊÄß„ÄÇÈ†êË®ìÁ∑¥Ê®°ÂûãÈÄ≤‰∏ÄÊ≠•Âú®‰∏ãÊ∏∏‰ªªÂãô‰∏äÈÄ≤Ë°åÂæÆË™øÔºå‰ª•ÂèñÂæóËß£Ê±∫‰ªªÂãôÁöÑËÉΩÂäõ„ÄÇÂæÆË™øÊ®°ÂûãÂú®ÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãô‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåË≠âÊòé‰∫ÜÂú®Èõ∂Ê¨°Â≠∏ÁøíÂ†¥ÊôØ‰∏≠Ëß£Ê±∫ÁµêÊßãÂíå‰∏ä‰∏ãÊñáÂïèÈ°åÁöÑÂº∑Â§ßËÉΩÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/JiaruiFeng/GOFA ÂèñÂæó„ÄÇ

##### **Human-like Episodic Memory for Infinite Context LLMs**
2407.09450v1 by Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang

Large language models (LLMs) have shown remarkable capabilities, but still
struggle with processing extensive contexts, limiting their ability to maintain
coherence and accuracy over long sequences. In contrast, the human brain excels
at organising and retrieving episodic experiences across vast temporal scales,
spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that
integrates key aspects of human episodic memory and event cognition into LLMs,
enabling them to effectively handle practically infinite context lengths while
maintaining computational efficiency. EM-LLM organises sequences of tokens into
coherent episodic events using a combination of Bayesian surprise and
graph-theoretic boundary refinement in an on-line fashion. When needed, these
events are retrieved through a two-stage memory process, combining
similarity-based and temporally contiguous retrieval for efficient and
human-like access to relevant information. Experiments on the LongBench dataset
demonstrate EM-LLM's superior performance, outperforming the state-of-the-art
InfLLM model with an overall relative improvement of 4.3% across various tasks,
including a 33% improvement on the PassageRetrieval task. Furthermore, our
analysis reveals strong correlations between EM-LLM's event segmentation and
human-perceived events, suggesting a bridge between this artificial system and
its biological counterpart. This work not only advances LLM capabilities in
processing extended contexts but also provides a computational framework for
exploring human memory mechanisms, opening new avenues for interdisciplinary
research in AI and cognitive science.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõÔºå‰ΩÜ‰ªçÈõ£‰ª•ËôïÁêÜÂª£Ê≥õÁöÑËÑàÁµ°ÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®Èï∑Â∫èÂàó‰∏≠Á∂≠ÊåÅÈÄ£Ë≤´ÊÄßÂíåÊ∫ñÁ¢∫ÊÄßÁöÑËÉΩÂäõ„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºå‰∫∫ËÖ¶ÊìÖÈï∑Âú®Âª£Â§ßÁöÑÊôÇÈñìÂ∞∫Â∫¶‰∏äÁµÑÁπîÂíåÊèêÂèñÊÉÖÁØÄÈ´îÈ©óÔºåË∑®Ë∂ä‰∏ÄÁîü„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü EM-LLMÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞á‰∫∫È°ûÊÉÖÁØÄË®òÊÜ∂Âíå‰∫ã‰ª∂Ë™çÁü•ÁöÑÈóúÈçµÈù¢ÂêëÊï¥ÂêàÂà∞ LLM ‰∏≠ÔºåËÆìÂÆÉÂÄëËÉΩÂ§†ÊúâÊïàÂú∞ËôïÁêÜÂØ¶Èöõ‰∏äÁÑ°ÈôêÁöÑËÑàÁµ°Èï∑Â∫¶ÔºåÂêåÊôÇÁ∂≠ÊåÅÈÅãÁÆóÊïàÁéá„ÄÇEM-LLM ‰ΩøÁî®Ë≤ùÊ∞èÈ©öÂñúÂíåÂúñË´ñÈÇäÁïåÁ≤æÁÖâÁöÑÁµÑÂêàÔºå‰ª•Á∑ö‰∏äÊñπÂºèÂ∞áÂ∫èÂàóÊ®ôË®òÁµÑÁπîÊàêÈÄ£Ë≤´ÁöÑÊÉÖÁØÄ‰∫ã‰ª∂„ÄÇÂú®ÈúÄË¶ÅÊôÇÔºåÈÄô‰∫õ‰∫ã‰ª∂ÊúÉÈÄèÈÅéÂÖ©ÈöéÊÆµÁöÑË®òÊÜ∂ÈÅéÁ®ã‰æÜÊèêÂèñÔºåÁµêÂêàÂü∫ÊñºÁõ∏‰ººÊÄßÂíåÊôÇÈñìÈÄ£Á∫åÊÄßÁöÑÊèêÂèñÔºå‰ª•ÊúâÊïà‰∏îÈ°û‰ºº‰∫∫È°ûÁöÑÊñπÂºèÂ≠òÂèñÁõ∏ÈóúË≥áË®ä„ÄÇÂú® LongBench Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË≠âÊòé‰∫Ü EM-LLM ÁöÑÂçìË∂äÊïàËÉΩÔºåÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑ InfLLM Ê®°ÂûãÔºåÂú® PassageRetrieval ‰ªªÂãô‰∏≠ÊîπÈÄ≤‰∫Ü 33%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂàÜÊûêÊè≠Á§∫‰∫Ü EM-LLM ÁöÑ‰∫ã‰ª∂ÂàÜÂâ≤Ëàá‰∫∫È°ûÊÑüÁü•‰∫ã‰ª∂‰πãÈñìÁöÑÂº∑Áõ∏ÈóúÊÄßÔºåÈ°ØÁ§∫‰∫ÜÈÄôÂÄã‰∫∫Â∑•Á≥ªÁµ±ËàáÂÖ∂ÁîüÁâ©Â∞çÊáâÁâ©‰πãÈñìÁöÑÊ©ãÊ®ë„ÄÇÈÄôÈ†ÖÂ∑•‰Ωú‰∏çÂÉÖÊèêÂçá‰∫Ü LLM Âú®ËôïÁêÜÂª∂‰º∏ËÑàÁµ°ÊñπÈù¢ÁöÑËÉΩÂäõÔºå‰πüÊèê‰æõ‰∫Ü‰∏ÄÂÄãÈÅãÁÆóÊû∂Êßã‰æÜÊé¢Á¥¢‰∫∫È°ûË®òÊÜ∂Ê©üÂà∂ÔºåÁÇ∫ AI ÂíåË™çÁü•ÁßëÂ≠∏ÁöÑË∑®È†òÂüüÁ†îÁ©∂ÈñãÂïü‰∫ÜÊñ∞ÁöÑÈÄîÂæë„ÄÇ

##### **The $Œº\mathcal{G}$ Language for Programming Graph Neural Networks**
2407.09441v1 by Matteo Belenchia, Flavio Corradini, Michela Quadrini, Michele Loreti

Graph neural networks form a class of deep learning architectures
specifically designed to work with graph-structured data. As such, they share
the inherent limitations and problems of deep learning, especially regarding
the issues of explainability and trustworthiness. We propose $\mu\mathcal{G}$,
an original domain-specific language for the specification of graph neural
networks that aims to overcome these issues. The language's syntax is
introduced, and its meaning is rigorously defined by a denotational semantics.
An equivalent characterization in the form of an operational semantics is also
provided and, together with a type system, is used to prove the type soundness
of $\mu\mathcal{G}$. We show how $\mu\mathcal{G}$ programs can be represented
in a more user-friendly graphical visualization, and provide examples of its
generality by showing how it can be used to define some of the most popular
graph neural network models, or to develop any custom graph processing
application.

ÊëòË¶ÅÔºöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂΩ¢Êàê‰∏ÄÈ°ûÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÔºåÁâπÂà•Ë®≠Ë®àÁî®ÊñºËôïÁêÜÂúñÂΩ¢ÁµêÊßãÂåñÁöÑË≥áÊñô„ÄÇÂõ†Ê≠§ÔºåÂÆÉÂÄëÂÖ∑ÊúâÊ∑±Â∫¶Â≠∏ÁøíÂõ∫ÊúâÁöÑÈôêÂà∂ÂíåÂïèÈ°åÔºåÁâπÂà•ÊòØÂú®ÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Ë≥¥ÊÄßÂïèÈ°å‰∏ä„ÄÇÊàëÂÄëÊèêÂá∫ $\mu\mathcal{G}$Ôºå‰∏ÄÁ®ÆÁî®ÊñºÊåáÂÆöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂéüÂâµÈ†òÂüüÁâπÂÆöË™ûË®ÄÔºåÊó®Âú®ÂÖãÊúçÈÄô‰∫õÂïèÈ°å„ÄÇÂºïÂÖ•‰∫ÜË™ûË®ÄÁöÑË™ûÊ≥ïÔºå‰∏¶ÈÄèÈÅéÊåáÁ§∫Ë™ûÁæ©Âö¥Ê†ºÂÆöÁæ©ÂÖ∂Âê´Áæ©„ÄÇÈÇÑÊèê‰æõ‰∫ÜÈÅãÁÆóË™ûÁæ©ÂΩ¢ÂºèÁöÑÁ≠âÊïàÁâπÂæµÊèèËø∞Ôºå‰∏¶ËàáÈ°ûÂûãÁ≥ªÁµ±‰∏ÄËµ∑Áî®ÊñºË≠âÊòé $\mu\mathcal{G}$ ÁöÑÈ°ûÂûãÂÅ•ÂÖ®ÊÄß„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞á $\mu\mathcal{G}$ Á®ãÂºèË°®Á§∫ÁÇ∫Êõ¥ÂèãÂñÑÁöÑÂúñÂΩ¢Ë¶ñË¶∫ÂåñÔºå‰∏¶ÈÄèÈÅéÂ±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî®ÂÆÉÂÆöÁæ©‰∏Ä‰∫õÊúÄÊµÅË°åÁöÑÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÊàñÈñãÁôº‰ªª‰ΩïËá™Ë®ÇÂúñÂΩ¢ËôïÁêÜÊáâÁî®Á®ãÂºèÔºå‰æÜÊèê‰æõÂÖ∂ÈÄöÁî®ÊÄßÁöÑÁØÑ‰æã„ÄÇ

##### **Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations**
2407.08983v1 by David N. Palacio, Daniel Rodriguez-Cardenas, Alejandro Velasco, Dipin Khati, Kevin Moran, Denys Poshyvanyk

Trustworthiness and interpretability are inextricably linked concepts for
LLMs. The more interpretable an LLM is, the more trustworthy it becomes.
However, current techniques for interpreting LLMs when applied to code-related
tasks largely focus on accuracy measurements, measures of how models react to
change, or individual task performance instead of the fine-grained explanations
needed at prediction time for greater interpretability, and hence trust. To
improve upon this status quo, this paper introduces ASTrust, an
interpretability method for LLMs of code that generates explanations grounded
in the relationship between model confidence and syntactic structures of
programming languages. ASTrust explains generated code in the context of syntax
categories based on Abstract Syntax Trees and aids practitioners in
understanding model predictions at both local (individual code snippets) and
global (larger datasets of code) levels. By distributing and assigning model
confidence scores to well-known syntactic structures that exist within ASTs,
our approach moves beyond prior techniques that perform token-level confidence
mapping by offering a view of model confidence that directly aligns with
programming language concepts with which developers are familiar. To put
ASTrust into practice, we developed an automated visualization that illustrates
the aggregated model confidence scores superimposed on sequence, heat-map, and
graph-based visuals of syntactic structures from ASTs. We examine both the
practical benefit that ASTrust can provide through a data science study on 12
popular LLMs on a curated set of GitHub repos and the usefulness of ASTrust
through a human study.

ÊëòË¶ÅÔºöÂèØ‰ø°Â∫¶ÂíåÂèØËß£ÈáãÊÄßÊòØ LLM ‰∏≠ÂØÜ‰∏çÂèØÂàÜÁöÑÊ¶ÇÂøµ„ÄÇLLM ÁöÑÂèØËß£ÈáãÊÄßË∂äÈ´òÔºåÂÆÉÁöÑÂèØ‰ø°Â∫¶Â∞±Ë∂äÈ´ò„ÄÇÁÑ∂ËÄåÔºåÁï∂ÊáâÁî®ÊñºËàáÁ®ãÂºèÁ¢ºÁõ∏ÈóúÁöÑ‰ªªÂãôÊôÇÔºåÁõÆÂâçËß£Èáã LLM ÁöÑÊäÄË°ì‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ê∫ñÁ¢∫ÊÄßÊ∏¨Èáè„ÄÅÊ®°ÂûãÂ∞çËÆäÂåñÁöÑÂèçÊáâÊ∏¨ÈáèÊàñÂÄãÂà•‰ªªÂãôË°®ÁèæÔºåËÄå‰∏çÊòØÂú®È†êÊ∏¨ÊôÇÈñìÊâÄÈúÄÁöÑÁ¥∞Á≤íÂ∫¶Ëß£ÈáãÔºåÂæûËÄåÊèêÈ´òÂèØËß£ÈáãÊÄßÂíåÂõ†Ê≠§ÊèêÈ´ò‰ø°‰ªªÂ∫¶„ÄÇÁÇ∫‰∫ÜÊîπÂñÑÈÄôÁ®ÆÁèæÁãÄÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü ASTrustÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®ÊñºÁ®ãÂºèÁ¢º LLM ÁöÑÂèØËß£ÈáãÊÄßÊñπÊ≥ïÔºåÂÆÉÊúÉÊ†πÊìöÊ®°Âûã‰ø°ÂøÉËàáÁ®ãÂºèË™ûË®ÄÁöÑË™ûÊ≥ïÁµêÊßã‰πãÈñìÁöÑÈóú‰øÇÁî¢ÁîüËß£Èáã„ÄÇASTrust Âú®Âü∫ÊñºÊäΩË±°Ë™ûÊ≥ïÊ®πÁöÑË™ûÊ≥ïÈ°ûÂà•ÁöÑ‰∏ä‰∏ãÊñá‰∏≠Ëß£ÈáãÁî¢ÁîüÁöÑÁ®ãÂºèÁ¢ºÔºå‰∏¶Âπ´Âä©ÂØ¶Âãô‰∫∫Âì°Âú®Â±ÄÈÉ®ÔºàÂÄãÂà•Á®ãÂºèÁ¢ºÁâáÊÆµÔºâÂíåÂÖ®ÂüüÔºàËºÉÂ§ßÁöÑÁ®ãÂºèÁ¢ºË≥áÊñôÈõÜÔºâÂ±§Á¥ö‰∫ÜËß£Ê®°ÂûãÈ†êÊ∏¨„ÄÇÈÄèÈÅéÂ∞áÊ®°Âûã‰ø°ÂøÉÂàÜÊï∏ÂàÜÈÖçÂíåÊåáÂÆöÁµ¶ AST ‰∏≠Â≠òÂú®ÁöÑÁúæÊâÄÂë®Áü•ÁöÑË™ûÊ≥ïÁµêÊßãÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïË∂ÖË∂ä‰∫ÜÂÖàÂâçÁöÑÊäÄË°ìÔºåÈÄô‰∫õÊäÄË°ìÈÄèÈÅéÊèê‰æõËàáÈñãÁôº‰∫∫Âì°ÁÜüÊÇâÁöÑÁ®ãÂºèË™ûË®ÄÊ¶ÇÂøµÁõ¥Êé•Â∞çÈΩäÁöÑÊ®°Âûã‰ø°ÂøÉË¶ñÂúñ‰æÜÂü∑Ë°å‰ª§ÁâåÁ¥öÂà•ÁöÑ‰ø°ÂøÉÂ∞çÊáâ„ÄÇÁÇ∫‰∫ÜÂØ¶Ë∏ê ASTrustÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãËá™ÂãïÂåñË¶ñË¶∫ÂåñÂ∑•ÂÖ∑ÔºåÂÆÉË™™Êòé‰∫ÜÁñäÂä†Âú® AST Ë™ûÊ≥ïÁµêÊßãÁöÑÂ∫èÂàó„ÄÅÁÜ±ÂúñÂíåÂü∫ÊñºÂúñÂΩ¢ÁöÑË¶ñË¶∫ÊïàÊûú‰∏äÁöÑËÅöÂêàÊ®°Âûã‰ø°ÂøÉÂàÜÊï∏„ÄÇÊàëÂÄëÊ™¢Êü•‰∫Ü ASTrust ÂèØ‰ª•ÈÄèÈÅéÂ∞ç 12 ÂÄãÊµÅË°åÁöÑ LLM Âú®‰∏ÄÁµÑÁ≤æÈÅ∏ÁöÑ GitHub ÂÑ≤Â≠òÂ∫´‰∏äÈÄ≤Ë°åË≥áÊñôÁßëÂ≠∏Á†îÁ©∂Êèê‰æõÁöÑÂØ¶ÈöõÂ•ΩËôïÔºå‰ª•ÂèäÈÄèÈÅé‰∫∫È´îÁ†îÁ©∂Êèê‰æõÁöÑ ASTrust ÁöÑÊúâÁî®ÊÄß„ÄÇ

##### **Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification**
2407.08959v1 by Ke Ji, Peng Wang, Wenjun Ke, Guozheng Li, Jiajun Liu, Jingsheng Gao, Ziyu Shang

Recently, various pre-trained language models (PLMs) have been proposed to
prove their impressive performances on a wide range of few-shot tasks. However,
limited by the unstructured prior knowledge in PLMs, it is difficult to
maintain consistent performance on complex structured scenarios, such as
hierarchical text classification (HTC), especially when the downstream data is
extremely scarce. The main challenge is how to transfer the unstructured
semantic space in PLMs to the downstream domain hierarchy. Unlike previous work
on HTC which directly performs multi-label classification or uses graph neural
network (GNN) to inject label hierarchy, in this work, we study the HTC problem
under a few-shot setting to adapt knowledge in PLMs from an unstructured manner
to the downstream hierarchy. Technically, we design a simple yet effective
method named Hierarchical Iterative Conditional Random Field (HierICRF) to
search the most domain-challenging directions and exquisitely crafts
domain-hierarchy adaptation as a hierarchical iterative language modeling
problem, and then it encourages the model to make hierarchical consistency
self-correction during the inference, thereby achieving knowledge transfer with
hierarchical consistency preservation. We perform HierICRF on various
architectures, and extensive experiments on two popular HTC datasets
demonstrate that prompt with HierICRF significantly boosts the few-shot HTC
performance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%
to 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot
settings, while remaining SOTA hierarchical consistency performance.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºåÂ∑≤ÁªèÊèêÂá∫‰∫ÜÂ§öÁßçÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã (PLM)Ôºå‰ª•ËØÅÊòéÂÆÉ‰ª¨Âú®ÂπøÊ≥õÁöÑÂ∞ëÈáèÊ†∑Êú¨‰ªªÂä°‰∏äÂÖ∑Êúâ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫é PLM ‰∏≠ÈùûÁªìÊûÑÂåñÁöÑÂÖàÈ™åÁü•ËØÜÂèóÂà∞ÈôêÂà∂ÔºåÂõ†Ê≠§Èöæ‰ª•Âú®Â§çÊùÇÁªìÊûÑÂåñÂú∫ÊôØÔºà‰æãÂ¶ÇÂ±ÇÊ¨°ÊñáÊú¨ÂàÜÁ±ª (HTC)Ôºâ‰∏≠‰øùÊåÅ‰∏ÄËá¥ÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏ãÊ∏∏Êï∞ÊçÆÊûÅÂÖ∂Á®ÄÂ∞ëÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ‰∏ªË¶ÅÁöÑÊåëÊàòÊòØÂ¶Ç‰ΩïÂ∞Ü PLM ‰∏≠ÈùûÁªìÊûÑÂåñÁöÑËØ≠‰πâÁ©∫Èó¥ËΩ¨ÁßªÂà∞‰∏ãÊ∏∏ÂüüÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇ‰∏é‰ª•ÂâçÁõ¥Êé•ÊâßË°åÂ§öÊ†áÁ≠æÂàÜÁ±ªÊàñ‰ΩøÁî®ÂõæÁ•ûÁªèÁΩëÁªú (GNN) Ê≥®ÂÖ•Ê†áÁ≠æÂ±ÇÊ¨°ÁªìÊûÑÁöÑ HTC Â∑•‰Ωú‰∏çÂêåÔºåÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨Âú®Â∞ëÈáèÊ†∑Êú¨ËÆæÁΩÆ‰∏ãÁ†îÁ©∂ HTC ÈóÆÈ¢òÔºå‰ª•Â∞Ü PLM ‰∏≠ÁöÑÁü•ËØÜ‰ªéÈùûÁªìÊûÑÂåñÊñπÂºèÈÄÇÂ∫îÂà∞‰∏ãÊ∏∏Â±ÇÊ¨°ÁªìÊûÑ„ÄÇ‰ªéÊäÄÊúØ‰∏äËÆ≤ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫Â±ÇÊ¨°Ëø≠‰ª£Êù°‰ª∂ÈöèÊú∫Âú∫ (HierICRF)Ôºå‰ª•ÊêúÁ¥¢ÊúÄÂÖ∑È¢ÜÂüüÊåëÊàòÊÄßÁöÑÊñπÂêëÔºåÂπ∂Á≤æÁªÜÂú∞Â∞ÜÈ¢ÜÂüüÂ±ÇÊ¨°ÁªìÊûÑÈÄÇÂ∫î‰Ωú‰∏∫ÂàÜÂ±ÇËø≠‰ª£ËØ≠Ë®ÄÂª∫Ê®°ÈóÆÈ¢òÔºåÁÑ∂ÂêéÂÆÉÈºìÂä±Ê®°ÂûãÂú®Êé®ÁêÜÊúüÈó¥ËøõË°åÂ±ÇÊ¨°‰∏ÄËá¥ÊÄßËá™ÊàëÊ†°Ê≠£Ôºå‰ªéËÄåÂÆûÁé∞ÂÖ∑ÊúâÂ±ÇÊ¨°‰∏ÄËá¥ÊÄß‰øùÁïôÁöÑÁü•ËØÜËΩ¨Áßª„ÄÇÊàë‰ª¨Âú®ÂêÑÁßçÊû∂ÊûÑ‰∏äÊâßË°å HierICRFÔºåÂú®‰∏§‰∏™ÊµÅË°åÁöÑ HTC Êï∞ÊçÆÈõÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºå‰ΩøÁî® HierICRF ÁöÑÊèêÁ§∫ÊòæÁùÄÊèêÈ´ò‰∫ÜÂ∞ëÈáèÊ†∑Êú¨ HTC ÊÄßËÉΩÔºåÂπ≥Âùá Micro-F1 ‰ªé 28.80% ÊèêÈ´òÂà∞ 1.50%ÔºåMacro-F1 ‰ªé 36.29% ÊèêÈ´òÂà∞ 1.5% Âú®Â∞ëÈáèÊ†∑Êú¨ËÆæÁΩÆ‰∏ãË∂ÖËøá‰∫Ü‰ª•ÂâçÊúÄÂÖàËøõ (SOTA) Âü∫ÂáÜÔºåÂêåÊó∂‰øùÊåÅ SOTA Â±ÇÊ¨°‰∏ÄËá¥ÊÄßÊÄßËÉΩ„ÄÇ</paragraph>

##### **Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight**
2407.08694v1 by Zhiqiang Xie, Yujia Zheng, Lizi Ottens, Kun Zhang, Christos Kozyrakis, Jonathan Mace

Runtime failure and performance degradation is commonplace in modern cloud
systems. For cloud providers, automatically determining the root cause of
incidents is paramount to ensuring high reliability and availability as prompt
fault localization can enable faster diagnosis and triage for timely
resolution. A compelling solution explored in recent work is causal reasoning
using causal graphs to capture relationships between varied cloud system
performance metrics. To be effective, however, systems developers must
correctly define the causal graph of their system, which is a time-consuming,
brittle, and challenging task that increases in difficulty for large and
dynamic systems and requires domain expertise. Alternatively, automated
data-driven approaches have limited efficacy for cloud systems due to the
inherent rarity of incidents. In this work, we present Atlas, a novel approach
to automatically synthesizing causal graphs for cloud systems. Atlas leverages
large language models (LLMs) to generate causal graphs using system
documentation, telemetry, and deployment feedback. Atlas is complementary to
data-driven causal discovery techniques, and we further enhance Atlas with a
data-driven validation step. We evaluate Atlas across a range of fault
localization scenarios and demonstrate that Atlas is capable of generating
causal graphs in a scalable and generalizable manner, with performance that far
surpasses that of data-driven algorithms and is commensurate to the
ground-truth baseline.

ÊëòË¶ÅÔºöÂú®Áèæ‰ª£Èõ≤Á´ØÁ≥ªÁµ±‰∏≠ÔºåÂü∑Ë°åÊôÇÊúüÊïÖÈöúÂíåÊïàËÉΩÈôç‰ΩéÊòØÂè∏Á©∫Ë¶ãÊÖ£ÁöÑ‰∫ã„ÄÇÂ∞çÊñºÈõ≤Á´Ø‰æõÊáâÂïÜËÄåË®ÄÔºåËá™ÂãïÊâæÂá∫‰∫ã‰ª∂ÁöÑÊ†πÊú¨ÂéüÂõ†Â∞çÊñºÁ¢∫‰øùÈ´òÂèØÈù†ÊÄßÂíåÂèØÁî®ÊÄßËá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂèäÊôÇÁöÑÊïÖÈöúÂÆö‰ΩçÂèØ‰ª•ËÆìË®∫Êñ∑ÂíåÂàÜÈ°ûÊõ¥Âø´ÈÄüÔºå‰ª•Âà©ÊñºÂèäÊôÇËß£Ê±∫ÂïèÈ°å„ÄÇÊúÄËøëÁöÑÂ∑•‰Ωú‰∏≠Êé¢Ë®é‰∫Ü‰∏ÄÂÄãÂºï‰∫∫Ê≥®ÁõÆÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂç≥‰ΩøÁî®Âõ†ÊûúÂúñ‰æÜÊì∑ÂèñÂêÑÁ®ÆÈõ≤Á´ØÁ≥ªÁµ±ÊïàËÉΩÊåáÊ®ô‰πãÈñìÈóú‰øÇÁöÑÂõ†ÊûúÊé®ÁêÜ„ÄÇÁÑ∂ËÄåÔºåÁ≥ªÁµ±ÈñãÁôº‰∫∫Âì°ÂøÖÈ†àÊ≠£Á¢∫ÂÆöÁæ©ÂÖ∂Á≥ªÁµ±ÁöÑÂõ†ÊûúÂúñÊâçËÉΩÁôºÊèÆÊïàÁî®ÔºåËÄåÈÄôÈ†Ö‰ªªÂãôËÄóÊôÇ„ÄÅËÑÜÂº±‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂ∞çÊñºÂ§ßÂûã‰∏îÂãïÊÖãÁöÑÁ≥ªÁµ±ËÄåË®ÄÈõ£Â∫¶Êõ¥È´òÔºåËÄå‰∏îÈúÄË¶ÅÈ†òÂüüÂ∞àÂÆ∂Áü•Ë≠ò„ÄÇÊàñËÄÖÔºåÁî±Êñº‰∫ã‰ª∂ÁöÑÂõ∫ÊúâÁ®ÄÂ∞ëÊÄßÔºåËá™ÂãïÂåñË≥áÊñôÈ©ÖÂãïÊñπÊ≥ïÂ∞çÊñºÈõ≤Á´ØÁ≥ªÁµ±ÁöÑÊïàÂäõÊúâÈôê„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ AtlasÔºå‰∏ÄÁ®ÆËá™ÂãïÂêàÊàêÈõ≤Á´ØÁ≥ªÁµ±Âõ†ÊûúÂúñÁöÑÊñ∞ÊñπÊ≥ï„ÄÇAtlas Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩøÁî®Á≥ªÁµ±Êñá‰ª∂„ÄÅÈÅôÊ∏¨ÂíåÈÉ®ÁΩ≤ÂõûÈ•ã‰æÜÁî¢ÁîüÂõ†ÊûúÂúñ„ÄÇAtlas ÊòØË≥áÊñôÈ©ÖÂãïÂõ†ÊûúÁôºÁèæÊäÄË°ìÁöÑË£úÂÖÖÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®Ë≥áÊñôÈ©ÖÂãïÈ©óË≠âÊ≠•È©ü‰æÜÂ¢ûÂº∑ Atlas„ÄÇÊàëÂÄëÂú®ÂêÑÁ®ÆÊïÖÈöúÂÆö‰ΩçÊÉÖÂ¢É‰∏≠Ë©ï‰º∞ AtlasÔºå‰∏¶Ë≠âÊòé Atlas ËÉΩÂ§†‰ª•ÂèØÊì¥ÂÖÖ‰∏îÂèØÊ¶ÇÂåñÁöÑÊñπÂºèÁî¢ÁîüÂõ†ÊûúÂúñÔºåÂÖ∂ÊïàËÉΩÈÅ†ÈÅ†Ë∂ÖÈÅéË≥áÊñôÈ©ÖÂãïÊºîÁÆóÊ≥ïÔºå‰∏¶‰∏îËàáÁúüÂØ¶Âü∫Á∑öÁõ∏Áï∂„ÄÇ

##### **Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents**
2407.08516v4 by Haoyi Xiong, Zhiyuan Wang, Xuhong Li, Jiang Bian, Zeke Xie, Shahid Mumtaz, Laura E. Barnes

This article explores the convergence of connectionist and symbolic
artificial intelligence (AI), from historical debates to contemporary
advancements. Traditionally considered distinct paradigms, connectionist AI
focuses on neural networks, while symbolic AI emphasizes symbolic
representation and logic. Recent advancements in large language models (LLMs),
exemplified by ChatGPT and GPT-4, highlight the potential of connectionist
architectures in handling human language as a form of symbols. The study argues
that LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.
By utilizing LLMs for text-based knowledge modeling and representation, LAAs
integrate neuro-symbolic AI principles, showcasing enhanced reasoning and
decision-making capabilities. Comparing LAAs with Knowledge Graphs within the
neuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking
human-like reasoning processes, scaling effectively with large datasets, and
leveraging in-context samples without explicit re-training. The research
underscores promising avenues in neuro-vector-symbolic integration,
instructional encoding, and implicit reasoning, aimed at further enhancing LAA
capabilities. By exploring the progression of neuro-symbolic AI and proposing
future research trajectories, this work advances the understanding and
development of AI technologies.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫ÜÈÄ£Êé•‰∏ªÁæ©ÂíåÁ¨¶Ëôü‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÁöÑÂåØÊµÅÔºåÂæûÊ≠∑Âè≤ËæØË´ñÂà∞Áï∂‰ª£ÈÄ≤Â±ï„ÄÇÂÇ≥Áµ±‰∏äË¢´Ë™çÁÇ∫ÊòØ‰∏çÂêåÁöÑÁØÑÂºèÔºåÈÄ£Êé•‰∏ªÁæ© AI Â∞àÊ≥®ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåËÄåÁ¨¶Ëôü AI ÂâáÂº∑Ë™øÁ¨¶ËôüË°®ÂæµÂíåÈÇèËºØ„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºå‰ª• ChatGPT Âíå GPT-4 ÁÇ∫‰æãÔºåÁ™ÅÈ°Ø‰∫ÜÈÄ£Êé•‰∏ªÁæ©Êû∂ÊßãÂú®ËôïÁêÜ‰∫∫È°ûË™ûË®Ä‰ΩúÁÇ∫Á¨¶ËôüÂΩ¢ÂºèÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÁ†îÁ©∂Ë™çÁÇ∫ÔºåÁî± LLM Ë≥¶ËÉΩÁöÑËá™‰∏ª‰ª£ÁêÜÔºàLAAÔºâÈ´îÁèæ‰∫ÜÈÄôÁ®ÆÁØÑÂºèÊî∂ÊñÇ„ÄÇÈÄèÈÅéÂà©Áî® LLM ÈÄ≤Ë°åÂü∫ÊñºÊñáÂ≠óÁöÑÁü•Ë≠òÂª∫Ê®°ÂíåË°®ÂæµÔºåLAA Êï¥Âêà‰∫ÜÁ•ûÁ∂ìÁ¨¶Ëôü AI ÂéüÁêÜÔºåÂ±ïÁ§∫‰∫ÜÂ¢ûÂº∑ÁöÑÊé®ÁêÜÂíåÊ±∫Á≠ñËÉΩÂäõ„ÄÇÂú®Á•ûÁ∂ìÁ¨¶Ëôü AI ‰∏ªÈ°å‰∏≠ÊØîËºÉ LAA ËàáÁü•Ë≠òÂúñË≠úÔºåÁ™ÅÈ°Ø‰∫Ü LAA Âú®Ê®°Êì¨È°û‰∫∫Êé®ÁêÜÈÅéÁ®ã„ÄÅÊúâÊïàÊì¥ÂÖÖÂ§ßÂûãË≥áÊñôÈõÜ‰ª•ÂèäÂà©Áî®ÊÉÖÂ¢ÉÁØÑ‰æãËÄå‰∏çÈúÄÊòéÁ¢∫ÈáçÊñ∞Ë®ìÁ∑¥ÊñπÈù¢ÁöÑÁç®ÁâπÂÑ™Âã¢„ÄÇÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÁ•ûÁ∂ìÂêëÈáèÁ¨¶ËôüÊï¥Âêà„ÄÅÊåá‰ª§Á∑®Á¢ºÂíåÂÖßÈö±Êé®ÁêÜ‰∏≠ÂâçÊôØÁúãÂ•ΩÁöÑÈÄîÂæëÔºåÊó®Âú®ÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑ LAA ÁöÑËÉΩÂäõ„ÄÇÈÄèÈÅéÊé¢Á¥¢Á•ûÁ∂ìÁ¨¶Ëôü AI ÁöÑÈÄ≤Â±ï‰∏¶ÊèêÂá∫Êú™‰æÜÁöÑÁ†îÁ©∂ËªåË∑°ÔºåÈÄôÈ†ÖÂ∑•‰Ωú‰øÉÈÄ≤‰∫ÜÂ∞ç AI ÊäÄË°ìÁöÑÁêÜËß£ÂíåÁôºÂ±ï„ÄÇ

##### **A Comprehensive Survey on the Security of Smart Grid: Challenges, Mitigations, and Future Research Opportunities**
2407.07966v1 by Arastoo Zibaeirad, Farnoosh Koleini, Shengping Bi, Tao Hou, Tao Wang

In this study, we conduct a comprehensive review of smart grid security,
exploring system architectures, attack methodologies, defense strategies, and
future research opportunities. We provide an in-depth analysis of various
attack vectors, focusing on new attack surfaces introduced by advanced
components in smart grids. The review particularly includes an extensive
analysis of coordinated attacks that incorporate multiple attack strategies and
exploit vulnerabilities across various smart grid components to increase their
adverse impact, demonstrating the complexity and potential severity of these
threats. Following this, we examine innovative detection and mitigation
strategies, including game theory, graph theory, blockchain, and machine
learning, discussing their advancements in counteracting evolving threats and
associated research challenges. In particular, our review covers a thorough
examination of widely used machine learning-based mitigation strategies,
analyzing their applications and research challenges spanning across
supervised, unsupervised, semi-supervised, ensemble, and reinforcement
learning. Further, we outline future research directions and explore new
techniques and concerns. We first discuss the research opportunities for
existing and emerging strategies, and then explore the potential role of new
techniques, such as large language models (LLMs), and the emerging threat of
adversarial machine learning in the future of smart grid security.

ÊëòË¶ÅÔºöÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞çÊô∫ÊÖßÈõªÁ∂≤ÂÆâÂÖ®ÊÄßÈÄ≤Ë°åÂÖ®Èù¢Ê™¢Ë¶ñÔºåÊé¢Ë®éÁ≥ªÁµ±Êû∂Êßã„ÄÅÊîªÊìäÊñπÊ≥ï„ÄÅÈò≤Á¶¶Á≠ñÁï•ÂíåÊú™‰æÜÁöÑÁ†îÁ©∂Ê©üÊúÉ„ÄÇÊàëÂÄëÊ∑±ÂÖ•ÂàÜÊûêÂêÑÁ®ÆÊîªÊìäÂ™í‰ªãÔºåÂ∞àÊ≥®ÊñºÊô∫ÊÖßÈõªÁ∂≤‰∏≠ÂÖàÈÄ≤ÁµÑ‰ª∂ÊâÄÂºïÂÖ•ÁöÑÊñ∞ÊîªÊìäÈù¢„ÄÇÊú¨Ê™¢Ë¶ñÁâπÂà•ÂåÖÂê´Â∞çÂçîË™øÊîªÊìäÁöÑÂª£Ê≥õÂàÜÊûêÔºåÂÖ∂‰∏≠ÂåÖÂê´Â§öÁ®ÆÊîªÊìäÁ≠ñÁï•‰∏¶Âà©Áî®ÂêÑÁ®ÆÊô∫ÊÖßÈõªÁ∂≤ÁµÑ‰ª∂‰∏≠ÁöÑÊºèÊ¥û‰æÜÂ¢ûÂä†ÂÖ∂Ë≤†Èù¢ÂΩ±ÈüøÔºåÂ±ïÁ§∫ÈÄô‰∫õÂ®ÅËÑÖÁöÑË§áÈõúÊÄßÂíåÊΩõÂú®Âö¥ÈáçÊÄß„ÄÇÂú®Ê≠§‰πãÂæåÔºåÊàëÂÄëÊé¢Ë®éÂâµÊñ∞ÁöÑÂÅµÊ∏¨ÂíåÁ∑©Ëß£Á≠ñÁï•ÔºåÂåÖÊã¨ÂçöÂºàË´ñ„ÄÅÂúñË´ñ„ÄÅÂçÄÂ°äÈèàÂíåÊ©üÂô®Â≠∏ÁøíÔºåË®éË´ñÂÆÉÂÄëÂú®Â∞çÊäó‰∏çÊñ∑ÊºîËÆäÁöÑÂ®ÅËÑÖÂíåÁõ∏ÈóúÁ†îÁ©∂ÊåëÊà∞ÊñπÈù¢ÁöÑÈÄ≤Â±ï„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁöÑÊ™¢Ë¶ñÊ∂µËìãÂ∞çÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑÁ∑©Ëß£Á≠ñÁï•ÁöÑÂæπÂ∫ïÊ™¢È©óÔºåÂàÜÊûêÂÆÉÂÄëÂú®Áõ£Áù£Âºè„ÄÅÈùûÁõ£Áù£Âºè„ÄÅÂçäÁõ£Áù£Âºè„ÄÅÊï¥È´îÂºèÂíåÂº∑ÂåñÂ≠∏Áøí‰∏≠ÁöÑÊáâÁî®ÂíåÁ†îÁ©∂ÊåëÊà∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊ¶ÇËø∞Êú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêë‰∏¶Êé¢Ë®éÊñ∞ÊäÄË°ìÂíåÂïèÈ°å„ÄÇÊàëÂÄëÈ¶ñÂÖàË®éË´ñÁèæÊúâÂíåÊñ∞ËààÁ≠ñÁï•ÁöÑÁ†îÁ©∂Ê©üÊúÉÔºåÁÑ∂ÂæåÊé¢Ë®éÊñ∞ÊäÄË°ìÁöÑÊΩõÂú®‰ΩúÁî®Ôºå‰æãÂ¶ÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•ÂèäÂ∞çÊäóÂºèÊ©üÂô®Â≠∏ÁøíÂú®Êô∫ÊÖßÈõªÁ∂≤ÂÆâÂÖ®Êú™‰æÜÁöÑÂ®ÅËÑÖ„ÄÇ

##### **Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**
2407.07775v2 by Hao-Tien Lewis Chiang, Zhuo Xu, Zipeng Fu, Mithun George Jacob, Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, Fei Xia, Jasmine Hsu, Jonathan Hoech, Pete Florence, Sean Kirmani, Sumeet Singh, Vikas Sindhwani, Carolina Parada, Chelsea Finn, Peng Xu, Sergey Levine, Jie Tan

An elusive goal in navigation research is to build an intelligent agent that
can understand multimodal instructions including natural language and image,
and perform useful navigation. To achieve this, we study a widely useful
category of navigation tasks we call Multimodal Instruction Navigation with
demonstration Tours (MINT), in which the environment prior is provided through
a previously recorded demonstration video. Recent advances in Vision Language
Models (VLMs) have shown a promising path in achieving this goal as it
demonstrates capabilities in perceiving and reasoning about multimodal inputs.
However, VLMs are typically trained to predict textual output and it is an open
research question about how to best utilize them in navigation. To solve MINT,
we present Mobility VLA, a hierarchical Vision-Language-Action (VLA) navigation
policy that combines the environment understanding and common sense reasoning
power of long-context VLMs and a robust low-level navigation policy based on
topological graphs. The high-level policy consists of a long-context VLM that
takes the demonstration tour video and the multimodal user instruction as input
to find the goal frame in the tour video. Next, a low-level policy uses the
goal frame and an offline constructed topological graph to generate robot
actions at every timestep. We evaluated Mobility VLA in a 836m^2 real world
environment and show that Mobility VLA has a high end-to-end success rates on
previously unsolved multimodal instructions such as "Where should I return
this?" while holding a plastic bin. A video demonstrating Mobility VLA can be
found here: https://youtu.be/-Tof__Q8_5s

ÊëòË¶ÅÔºö<paragraph>Â∞éËà™Á†îÁ©∂‰∏≠‰∏ÄÂÄãÈõ£‰ª•ÊçâÊë∏ÁöÑÁõÆÊ®ôÔºåÊòØÂª∫Á´ã‰∏ÄÂÄãÊô∫ËÉΩ‰ª£ÁêÜÔºåÂÆÉÂèØ‰ª•ÁêÜËß£ÂåÖÊã¨Ëá™ÁÑ∂Ë™ûË®ÄÂíåÂΩ±ÂÉèÁöÑÂ§öÊ®°ÊÖãÊåá‰ª§Ôºå‰∏¶Âü∑Ë°åÊúâÁî®ÁöÑÂ∞éËà™„ÄÇÁÇ∫‰∫ÜÈÅîÊàêÊ≠§ÁõÆÊ®ôÔºåÊàëÂÄëÁ†îÁ©∂‰∫Ü‰∏ÄÈ°ûÂª£Ê≥õÊúâÁî®ÁöÑÂ∞éËà™‰ªªÂãôÔºåÊàëÂÄëÁ®±‰πãÁÇ∫Á§∫ÁØÑÂ∞éË¶ΩÁöÑÂ§öÊ®°ÊÖãÊåá‰ª§Â∞éËà™ (MINT)ÔºåÂÖ∂‰∏≠Áí∞Â¢ÉÂÖàÈ©óÊòØÈÄèÈÅéÂÖàÂâçÈåÑË£ΩÁöÑÁ§∫ÁØÑÂΩ±ÁâáÊèê‰æõÁöÑ„ÄÇË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑËøëÊúüÈÄ≤Â±ïÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄÊ¢ùÂØ¶ÁèæÊ≠§ÁõÆÊ®ôÁöÑÊúâÂâçÊôØË∑ØÂæëÔºåÂõ†ÁÇ∫ÂÆÉÂ±ïÁ§∫‰∫ÜÊÑüÁü•ÂíåÊé®ÁêÜÂ§öÊ®°ÊÖãËº∏ÂÖ•ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåVLM ÈÄöÂ∏∏Ë®ìÁ∑¥Áî®ÊñºÈ†êÊ∏¨ÊñáÂ≠óËº∏Âá∫ÔºåËÄåÂ¶Ç‰ΩïÊúÄ‰Ω≥Âà©Áî®ÂÆÉÂÄëÈÄ≤Ë°åÂ∞éËà™ÔºåÂâáÊòØ‰∏ÄÂÄãÈñãÊîæÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ MINTÔºåÊàëÂÄëÊèêÂá∫‰∫Ü Mobility VLAÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂàÜÂ±§ÁöÑË¶ñË¶∫-Ë™ûË®Ä-Âãï‰Ωú (VLA) Â∞éËà™ÊîøÁ≠ñÔºåÂÆÉÁµêÂêà‰∫ÜÈï∑Ë™ûÂ¢É VLM ÁöÑÁí∞Â¢ÉÁêÜËß£ÂíåÂ∏∏Ë≠òÊé®ÁêÜËÉΩÂäõÔºå‰ª•ÂèäÂü∫ÊñºÊãìÊí≤ÂúñÁöÑÂº∑ÂÅ•‰ΩéÈöéÂ∞éËà™ÊîøÁ≠ñ„ÄÇÈ´òÈöéÊîøÁ≠ñÂåÖÂê´‰∏ÄÂÄãÈï∑Ë™ûÂ¢É VLMÔºåÂÆÉÊé°Áî®Á§∫ÁØÑÂ∞éË¶ΩÂΩ±ÁâáÂíåÂ§öÊ®°ÊÖã‰ΩøÁî®ËÄÖÊåá‰ª§‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰ª•Âú®Â∞éË¶ΩÂΩ±Áâá‰∏≠ÊâæÂà∞ÁõÆÊ®ôÂπÄ„ÄÇÊé•‰∏ã‰æÜÔºå‰ΩéÈöéÊîøÁ≠ñ‰ΩøÁî®ÁõÆÊ®ôÂπÄÂíåÈõ¢Á∑öÂª∫ÊßãÁöÑÊãìÊí≤ÂúñÔºåÂú®ÊØèÂÄãÊôÇÈñìÊ≠•Áî¢ÁîüÊ©üÂô®‰∫∫Âãï‰Ωú„ÄÇÊàëÂÄëÂú® 836 Âπ≥ÊñπÂÖ¨Â∞∫ÁöÑÁúüÂØ¶‰∏ñÁïåÁí∞Â¢É‰∏≠Ë©ï‰º∞‰∫Ü Mobility VLAÔºå‰∏¶Â±ïÁ§∫‰∫Ü Mobility VLA Âú®ÂÖàÂâçÊú™Ëß£Ê±∫ÁöÑÂ§öÊ®°ÊÖãÊåá‰ª§Ôºà‰æãÂ¶Ç„ÄåÊàëÊáâË©≤ÊääÈÄôÂÄãÂ°ëËÜ†ÁÆ±Ê≠∏ÈÇÑÂà∞Âì™Ë£°Ôºü„ÄçÔºâ‰∏äÂÖ∑ÊúâÂæàÈ´òÁöÑÁ´ØÂà∞Á´ØÊàêÂäüÁéáÔºåÂêåÊôÇÊãøËëó‰∏ÄÂÄãÂ°ëËÜ†ÁÆ±„ÄÇÂ±ïÁ§∫ Mobility VLA ÁöÑÂΩ±ÁâáÂèØ‰ª•Âú®ÈÄôË£°ÊâæÂà∞Ôºöhttps://youtu.be/-Tof__Q8_5s</paragraph>

##### **Teaching Transformers Causal Reasoning through Axiomatic Training**
2407.07612v1 by Aniket Vashishtha, Abhinav Kumar, Abbavaram Gowtham Reddy, Vineeth N Balasubramanian, Amit Sharma

For text-based AI systems to interact in the real world, causal reasoning is
an essential skill. Since interventional data is costly to generate, we study
to what extent an agent can learn causal reasoning from passive data.
Specifically, we consider an axiomatic training setup where an agent learns
from multiple demonstrations of a causal axiom (or rule), rather than
incorporating the axiom as an inductive bias or inferring it from data values.
A key question is whether the agent would learn to generalize from the axiom
demonstrations to new scenarios. For example, if a transformer model is trained
on demonstrations of the causal transitivity axiom over small graphs, would it
generalize to applying the transitivity axiom over large graphs? Our results,
based on a novel axiomatic training scheme, indicate that such generalization
is possible. We consider the task of inferring whether a variable causes
another variable, given a causal graph structure. We find that a 67 million
parameter transformer model, when trained on linear causal chains (along with
some noisy variations) can generalize well to new kinds of graphs, including
longer causal chains, causal chains with reversed order, and graphs with
branching; even when it is not explicitly trained for such settings. Our model
performs at par (or even better) than many larger language models such as
GPT-4, Gemini Pro, and Phi-3. Overall, our axiomatic training framework
provides a new paradigm of learning causal reasoning from passive data that can
be used to learn arbitrary axioms, as long as sufficient demonstrations can be
generated.

ÊëòË¶ÅÔºö<paragraph>Â∞çÊñºÂü∫ÊñºÊñáÂ≠óÁöÑ‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ËàáÁúüÂØ¶‰∏ñÁïå‰∫íÂãï‰æÜË™™ÔºåÂõ†ÊûúÊé®ÁêÜÊòØ‰∏ÄÈ†ÖÂøÖË¶ÅÁöÑÊäÄËÉΩ„ÄÇÁî±Êñº‰ªãÂÖ•Ë≥áÊñôÁöÑÁî¢ÁîüÊàêÊú¨ÂæàÈ´òÔºåÊàëÂÄëÁ†îÁ©∂‰∏Ä‰Ωç‰ª£ÁêÜ‰∫∫ÂæûË¢´ÂãïË≥áÊñô‰∏≠Â≠∏ÁøíÂõ†ÊûúÊé®ÁêÜÁöÑÁ®ãÂ∫¶„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëËÄÉÊÖÆ‰∏ÄÂÄãÂÖ¨ÁêÜË®ìÁ∑¥Ë®≠ÁΩÆÔºåÂÖ∂‰∏≠‰∏Ä‰Ωç‰ª£ÁêÜ‰∫∫ÂæûÂõ†ÊûúÂÖ¨ÁêÜÔºàÊàñË¶èÂâáÔºâÁöÑÂ§öÂÄãÁ§∫ÁØÑ‰∏≠Â≠∏ÁøíÔºåËÄå‰∏çÊòØÂ∞áÂÖ¨ÁêÜ‰ΩúÁÇ∫Ê≠∏Á¥çÂÅèË™§ÊàñÂæûË≥áÊñôÂÄº‰∏≠Êé®Êñ∑Âá∫‰æÜ„ÄÇ‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÊòØ‰ª£ÁêÜ‰∫∫ÊòØÂê¶ÊúÉÂ≠∏ÊúÉÂæûÂÖ¨ÁêÜÁ§∫ÁØÑÊé®Âª£Âà∞Êñ∞ÁöÑÂ†¥ÊôØ„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûú‰∏ÄÂÄãTransformerÊ®°ÂûãÂú®Â∞èÂúñË°®‰∏äÂõ†ÊûúÂÇ≥ÈÅûÊÄßÂÖ¨ÁêÜÁöÑÁ§∫ÁØÑ‰∏≠Êé•ÂèóË®ìÁ∑¥ÔºåÂÆÉÊòØÂê¶ÊúÉÊé®Âª£Âà∞Âú®Â§ßÂúñË°®‰∏äÊáâÁî®ÂÇ≥ÈÅûÊÄßÂÖ¨ÁêÜÔºüÊàëÂÄëÁöÑÁµêÊûúÂü∫Êñº‰∏ÄÂÄãÊñ∞Á©éÁöÑÂÖ¨ÁêÜË®ìÁ∑¥ÊñπÊ°àÔºåË°®ÊòéÈÄôÊ®£ÁöÑÊ¶ÇÊã¨ÊòØÂèØËÉΩÁöÑ„ÄÇÊàëÂÄëËÄÉÊÖÆÊé®Ë´ñ‰∏ÄÂÄãËÆäÊï∏ÊòØÂê¶Â∞éËá¥Âè¶‰∏ÄÂÄãËÆäÊï∏ÁöÑ‰ªªÂãôÔºåÁµ¶ÂÆö‰∏ÄÂÄãÂõ†ÊûúÂúñÁµêÊßã„ÄÇÊàëÂÄëÁôºÁèæ‰∏ÄÂÄã 6700 Ëê¨ÂÄãÂèÉÊï∏ÁöÑTransformerÊ®°ÂûãÔºåÂú®Á∑öÊÄßÂõ†ÊûúÈèàÔºà‰ª•Âèä‰∏Ä‰∫õÈõúË®äËÆäÂåñÔºâ‰∏äË®ìÁ∑¥ÊôÇÔºåÂèØ‰ª•ÂæàÂ•ΩÂú∞Ê¶ÇÊã¨Âà∞Êñ∞È°ûÂûãÁöÑÂúñÂΩ¢ÔºåÂåÖÊã¨Êõ¥Èï∑ÁöÑÂõ†ÊûúÈèà„ÄÅÈ†ÜÂ∫èÁõ∏ÂèçÁöÑÂõ†ÊûúÈèàÂíåÂÖ∑ÊúâÂàÜÊîØÁöÑÂúñÂΩ¢ÔºõÂç≥‰ΩøÂÆÉÊ≤íÊúâÈáùÂ∞çÊ≠§È°ûË®≠ÁΩÆÈÄ≤Ë°åÊòéÁ¢∫Ë®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãË°®ÁèæËàáË®±Â§öËºÉÂ§ßÁöÑË™ûË®ÄÊ®°ÂûãÔºà‰æãÂ¶Ç GPT-4„ÄÅGemini Pro Âíå Phi-3ÔºâÁõ∏Áï∂ÔºàÁîöËá≥Êõ¥Â•ΩÔºâ„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÂÖ¨ÁêÜË®ìÁ∑¥Ê°ÜÊû∂Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂæûË¢´ÂãïË≥áÊñô‰∏≠Â≠∏ÁøíÂõ†ÊûúÊé®ÁêÜÁöÑÊñ∞ÁØÑ‰æãÔºåÂè™Ë¶ÅÂèØ‰ª•Áî¢ÁîüË∂≥Â§†ÁöÑÁ§∫ÁØÑÔºåÂ∞±ÂèØ‰ª•Áî®ÊñºÂ≠∏Áøí‰ªªÊÑèÂÖ¨ÁêÜ„ÄÇ</paragraph>

##### **STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained LLMs**
2407.12860v1 by Aaron Zolnai-Lucas, Jack Boylan, Chris Hokamp, Parsa Ghaffari

We present Simplified Text-Attributed Graph Embeddings (STAGE), a
straightforward yet effective method for enhancing node features in Graph
Neural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our
approach leverages Large-Language Models (LLMs) to generate embeddings for
textual attributes. STAGE achieves competitive results on various node
classification benchmarks while also maintaining a simplicity in implementation
relative to current state-of-the-art (SoTA) techniques. We show that utilizing
pre-trained LLMs as embedding generators provides robust features for ensemble
GNN training, enabling pipelines that are simpler than current SoTA approaches
which require multiple expensive training and prompting stages. We also
implement diffusion-pattern GNNs in an effort to make this pipeline scalable to
graphs beyond academic benchmarks.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫ÜÁ∞°ÂåñÊñáÂ≠óÂ±¨ÊÄßÂúñÂµåÂÖ• (STAGE)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁõ¥Êé•‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ¢ûÂº∑ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Ê®°Âûã‰∏≠ÁöÑÁØÄÈªûÁâπÂæµÔºåÈÄô‰∫õÊ®°ÂûãÊúÉÁ∑®Á¢ºÊñáÂ≠óÂ±¨ÊÄßÂúñ (TAG)„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁÇ∫ÊñáÂ≠óÂ±¨ÊÄßÁî¢ÁîüÂµåÂÖ•„ÄÇSTAGE Âú®ÂêÑÁ®ÆÁØÄÈªûÂàÜÈ°ûÂü∫Ê∫ñ‰∏äÂèñÂæó‰∫ÜÊúâÁ´∂Áà≠ÂäõÁöÑÁµêÊûúÔºåÂêåÊôÇÂú®ÂØ¶‰Ωú‰∏ä‰πüÁ∂≠ÊåÅ‰∫ÜÁ∞°ÊΩîÊÄßÔºåÁõ∏ËºÉÊñºÁõÆÂâçÁöÑÊäÄË°ìÊ∞¥Ê∫ñ (SoTA)„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰ΩøÁî®È†êË®ìÁ∑¥ÁöÑ LLM ‰ΩúÁÇ∫ÂµåÂÖ•Áî¢ÁîüÂô®ÔºåÂèØÁÇ∫Êï¥È´î GNN Ë®ìÁ∑¥Êèê‰æõÂº∑ÂÅ•ÁöÑÁâπÂæµÔºåÈÄ≤ËÄåÂª∫ÊßãÊØîÁõÆÂâç SoTA ÂÅöÊ≥ïÊõ¥Á∞°ÂñÆÁöÑÁÆ°ÈÅìÔºåËÄåÂæåËÄÖÈúÄË¶ÅÂ§öÂÄãÊòÇË≤¥ÁöÑË®ìÁ∑¥ÂíåÊèêÁ§∫ÈöéÊÆµ„ÄÇÊàëÂÄë‰πüÂØ¶‰Ωú‰∫ÜÊì¥Êï£Ê®°Âºè GNNÔºå‰ª•ÊúüËÆìÈÄôÂÄãÁÆ°ÈÅìËÉΩÊì¥ÂÖÖÂà∞Â≠∏Ë°ìÂü∫Ê∫ñ‰πãÂ§ñÁöÑÂúñÂΩ¢„ÄÇ

##### **GLBench: A Comprehensive Benchmark for Graph with Large Language Models**
2407.07457v2 by Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kin Chan, Jia Li

The emergence of large language models (LLMs) has revolutionized the way we
interact with graphs, leading to a new paradigm called GraphLLM. Despite the
rapid development of GraphLLM methods in recent years, the progress and
understanding of this field remain unclear due to the lack of a benchmark with
consistent experimental protocols. To bridge this gap, we introduce GLBench,
the first comprehensive benchmark for evaluating GraphLLM methods in both
supervised and zero-shot scenarios. GLBench provides a fair and thorough
evaluation of different categories of GraphLLM methods, along with traditional
baselines such as graph neural networks. Through extensive experiments on a
collection of real-world datasets with consistent data processing and splitting
strategies, we have uncovered several key findings. Firstly, GraphLLM methods
outperform traditional baselines in supervised settings, with LLM-as-enhancers
showing the most robust performance. However, using LLMs as predictors is less
effective and often leads to uncontrollable output issues. We also notice that
no clear scaling laws exist for current GraphLLM methods. In addition, both
structures and semantics are crucial for effective zero-shot transfer, and our
proposed simple baseline can even outperform several models tailored for
zero-shot scenarios. The data and code of the benchmark can be found at
https://github.com/NineAbyss/GLBench.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÂæπÂ∫ïÊîπËÆä‰∫ÜÊàëÂÄëËàáÂúñË°®‰∫íÂãïÁöÑÊñπÂºèÔºåÈÄ≤ËÄåÁî¢Áîü‰∏ÄÁ®ÆÁ®±ÁÇ∫ GraphLLM ÁöÑÊñ∞ÂÖ∏ÁØÑ„ÄÇÂÑòÁÆ°ËøëÂπ¥‰æÜ GraphLLM ÊñπÊ≥ïÂø´ÈÄüÁôºÂ±ïÔºå‰ΩÜÁî±ÊñºÁº∫‰πèÂÖ∑Êúâ‰∏ÄËá¥ÂØ¶È©óÂçîÂÆöÁöÑÂü∫Ê∫ñÔºåÂõ†Ê≠§Ë©≤È†òÂüüÁöÑÈÄ≤Â±ïÂíåÁêÜËß£‰ªç‰∏çÊòéÁ¢∫„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü GLBenchÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÁî®ÊñºË©ï‰º∞ GraphLLM ÊñπÊ≥ïÂú®Áõ£Áù£ÂºèÂíåÈõ∂Ê¨°Â≠∏ÁøíÂ†¥ÊôØ‰∏≠ÁöÑÁ∂úÂêàÂü∫Ê∫ñ„ÄÇGLBench Êèê‰æõÂ∞ç‰∏çÂêåÈ°ûÂà•ÁöÑ GraphLLM ÊñπÊ≥ïÈÄ≤Ë°åÂÖ¨Âπ≥‰∏îÂæπÂ∫ïÁöÑË©ï‰º∞Ôºå‰ª•ÂèäÂÇ≥Áµ±Âü∫Ê∫ñÔºå‰æãÂ¶ÇÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇÈÄèÈÅéÂ∞ç‰∏ÄÁµÑÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÈÄ≤Ë°åÂª£Ê≥õÂØ¶È©óÔºå‰∏¶Êé°Áî®‰∏ÄËá¥ÁöÑË≥áÊñôËôïÁêÜÂíåÂàÜÂâ≤Á≠ñÁï•ÔºåÊàëÂÄëÁôºÁèæ‰∫ÜÂπæÂÄãÈóúÈçµÁôºÁèæ„ÄÇÈ¶ñÂÖàÔºåGraphLLM ÊñπÊ≥ïÂú®Áõ£Áù£ÂºèË®≠ÂÆö‰∏≠ÂÑ™ÊñºÂÇ≥Áµ±Âü∫Ê∫ñÔºåÂÖ∂‰∏≠ LLM ‰ΩúÁÇ∫Â¢ûÂº∑Âô®È°ØÁ§∫Âá∫ÊúÄÁ©©ÂÅ•ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî® LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®ËºÉ‰∏çÊúâÊïàÔºåËÄå‰∏îÁ∂ìÂ∏∏Â∞éËá¥ÁÑ°Ê≥ïÊéßÂà∂ÁöÑËº∏Âá∫ÂïèÈ°å„ÄÇÊàëÂÄëÈÇÑÊ≥®ÊÑèÂà∞ÔºåÂ∞çÊñºÁõÆÂâçÁöÑ GraphLLM ÊñπÊ≥ï‰∏¶‰∏çÂ≠òÂú®ÊòéÁ¢∫ÁöÑÁ∏ÆÊîæÂÆöÂæã„ÄÇÊ≠§Â§ñÔºåÁµêÊßãÂíåË™ûÁæ©Â∞çÊñºÊúâÊïàÁöÑÈõ∂Ê¨°Â≠∏ÁøíÂÇ≥Ëº∏Ëá≥ÈóúÈáçË¶ÅÔºåËÄåÊàëÂÄëÊèêÂá∫ÁöÑÁ∞°ÂñÆÂü∫Ê∫ñÁîöËá≥ÂèØ‰ª•ÂÑ™ÊñºÈáùÂ∞çÈõ∂Ê¨°Â≠∏ÁøíÂ†¥ÊôØÈáèË∫´ÊâìÈÄ†ÁöÑÂπæÂÄãÊ®°Âûã„ÄÇÂü∫Ê∫ñÁöÑË≥áÊñôÂíåÁ®ãÂºèÁ¢ºÂèØ‰ª•Âú® https://github.com/NineAbyss/GLBench ‰∏≠ÊâæÂà∞„ÄÇ

##### **Decoding Climate Disagreement: A Graph Neural Network-Based Approach to Understanding Social Media Dynamics**
2407.07038v1 by Ruiran Su, Janet B. Pierrehumbert

This work introduces the ClimateSent-GAT Model, an innovative method that
integrates Graph Attention Networks (GATs) with techniques from natural
language processing to accurately identify and predict disagreements within
Reddit comment-reply pairs. Our model classifies disagreements into three
categories: agree, disagree, and neutral. Leveraging the inherent graph
structure of Reddit comment-reply pairs, the model significantly outperforms
existing benchmarks by capturing complex interaction patterns and sentiment
dynamics. This research advances graph-based NLP methodologies and provides
actionable insights for policymakers and educators in climate science
communication.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂‰ªãÁ¥π ClimateSent-GAT Ê®°ÂûãÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞áÂúñÊ≥®ÊÑèÂäõÁ∂≤Ë∑Ø (GAT) ËàáËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊäÄË°ìÊï¥ÂêàÔºå‰ª•Ê∫ñÁ¢∫Ë≠òÂà•‰∏¶È†êÊ∏¨ Reddit ÁïôË®ÄÂõûË¶ÜÂ∞ç‰∏≠ÁöÑÂàÜÊ≠ß„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂ∞áÂàÜÊ≠ßÂàÜÁÇ∫‰∏âÈ°ûÔºöÂêåÊÑè„ÄÅ‰∏çÂêåÊÑèÂíå‰∏≠Á´ã„ÄÇÈÄèÈÅéÂà©Áî® Reddit ÁïôË®ÄÂõûË¶ÜÂ∞çÁöÑÂÖßÂú®ÂúñÂΩ¢ÁµêÊßãÔºåÊ≠§Ê®°ÂûãËÉΩÂ§ßÂπÖË∂ÖË∂äÁèæÊúâÂü∫Ê∫ñÔºåÊçïÊçâË§áÈõúÁöÑ‰∫íÂãïÊ®°ÂºèÂíåÊÉÖÁ∑íÂãïÊÖã„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Êé®Âãï‰∫ÜÂü∫ÊñºÂúñÂΩ¢ÁöÑ NLP ÊñπÊ≥ïÔºå‰∏¶ÁÇ∫Ê∞£ÂÄôÁßëÂ≠∏Ê∫ùÈÄö‰∏≠ÁöÑÊîøÁ≠ñÂà∂ÂÆöËÄÖÂíåÊïôËÇ≤Â∑•‰ΩúËÄÖÊèê‰æõÂèØË°åÁöÑË¶ãËß£„ÄÇ

##### **Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions**
2407.06723v1 by Yu-Guan Hsieh, Cheng-Yu Hsieh, Shih-Ying Yeh, Louis B√©thune, Hadi Pour Ansari, Pavan Kumar Anasosalu Vasu, Chun-Liang Li, Ranjay Krishna, Oncel Tuzel, Marco Cuturi

Humans describe complex scenes with compositionality, using simple text
descriptions enriched with links and relationships. While vision-language
research has aimed to develop models with compositional understanding
capabilities, this is not reflected yet in existing datasets which, for the
most part, still use plain text to describe images. In this work, we propose a
new annotation strategy, graph-based captioning (GBC) that describes an image
using a labelled graph structure, with nodes of various types. The nodes in GBC
are created using, in a first stage, object detection and dense captioning
tools nested recursively to uncover and describe entity nodes, further linked
together in a second stage by highlighting, using new types of nodes,
compositions and relations among entities. Since all GBC nodes hold plain text
descriptions, GBC retains the flexibility found in natural language, but can
also encode hierarchical information in its edges. We demonstrate that GBC can
be produced automatically, using off-the-shelf multimodal LLMs and
open-vocabulary detection models, by building a new dataset, GBC10M, gathering
GBC annotations for about 10M images of the CC12M dataset. We use GBC10M to
showcase the wealth of node captions uncovered by GBC, as measured with CLIP
training. We show that using GBC nodes' annotations -- notably those stored in
composition and relation nodes -- results in significant performance boost on
downstream models when compared to other dataset formats. To further explore
the opportunities provided by GBC, we also propose a new attention mechanism
that can leverage the entire GBC graph, with encouraging experimental results
that show the extra benefits of incorporating the graph structure. Our datasets
are released at \url{https://huggingface.co/graph-based-captions}.

ÊëòË¶ÅÔºö<paragraph>‰∫∫È°û‰ΩøÁî®Á∞°ÂñÆÁöÑÊñáÂ≠óÊèèËø∞ÔºåË±êÂØåÁöÑÈÄ£ÁµêÂíåÈóú‰øÇÔºå‰æÜÊèèËø∞Ë§áÈõúÁöÑÂ†¥ÊôØ„ÄÇÈõñÁÑ∂Ë¶ñË¶∫Ë™ûË®ÄÁöÑÁ†îÁ©∂Êó®Âú®ÈñãÁôºÂÖ∑ÊúâÁµÑÂêàÁêÜËß£ËÉΩÂäõÁöÑÊ®°ÂûãÔºå‰ΩÜÁèæÊúâÁöÑÊï∏ÊìöÈõÜÂ∞öÊú™ÂèçÊò†ÈÄô‰∏ÄÈªûÔºåÈÄô‰∫õÊï∏ÊìöÈõÜÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªç‰ΩøÁî®Á¥îÊñáÊú¨‰æÜÊèèËø∞ÂúñÂÉè„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑË®ªÈáãÁ≠ñÁï•ÔºåÂü∫ÊñºÂúñË°®ÁöÑÊ®ôÈ°å (GBC)ÔºåÂÆÉ‰ΩøÁî®Ê®ôÁ±§ÂúñË°®ÁµêÊßã‰æÜÊèèËø∞ÂúñÂÉèÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂêÑÁ®ÆÈ°ûÂûãÁöÑÁØÄÈªû„ÄÇGBC ‰∏≠ÁöÑÁØÄÈªûÊòØ‰ΩøÁî®Áâ©È´îÊ™¢Ê∏¨ÂíåÂØÜÈõÜÊ®ôÈ°åÂ∑•ÂÖ∑Âú®Á¨¨‰∏ÄÈöéÊÆµÂâµÂª∫ÁöÑÔºå‰ª•ÈÅûËø¥ÂµåÂ•óÁöÑÊñπÂºèÁôºÁèæÂíåÊèèËø∞ÂØ¶È´îÁØÄÈªûÔºå‰∏¶Âú®Á¨¨‰∫åÈöéÊÆµ‰ΩøÁî®Êñ∞È°ûÂûãÁöÑÁØÄÈªûÁ™ÅÂá∫È°ØÁ§∫ÔºåÂæûËÄåÂ∞áÂÆÉÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄ£ÁµêÂú®‰∏ÄËµ∑ÔºåÂØ¶È´î‰πãÈñìÁöÑÁµÑÂêàÂíåÈóú‰øÇ„ÄÇÁî±ÊñºÊâÄÊúâ GBC ÁØÄÈªûÈÉΩÂåÖÂê´Á¥îÊñáÊú¨ÊèèËø∞ÔºåÂõ†Ê≠§ GBC ‰øùÁïô‰∫ÜËá™ÁÑ∂Ë™ûË®Ä‰∏≠ÁöÑÈùàÊ¥ªÊÄßÔºå‰ΩÜ‰πüÂèØ‰ª•Âú®ÂÖ∂ÈÇäÁ∑£Á∑®Á¢ºÂàÜÂ±§‰ø°ÊÅØ„ÄÇÊàëÂÄëË≠âÊòé‰∫Ü GBC ÂèØ‰ª•‰ΩøÁî®ÁèæÊàêÁöÑÂ§öÊ®°ÊÖã LLM ÂíåÈñãÊîæË©ûÂΩôÊ™¢Ê∏¨Ê®°ÂûãËá™ÂãïÁîüÊàêÔºåÈÄöÈÅéÊßãÂª∫‰∏ÄÂÄãÊñ∞ÁöÑÊï∏ÊìöÈõÜ GBC10MÔºåÊî∂ÈõÜ‰∫ÜÂ§ßÁ¥Ñ 10M CC12M Êï∏ÊìöÈõÜÂúñÂÉèÁöÑ GBC Ë®ªÈáã„ÄÇÊàëÂÄë‰ΩøÁî® GBC10M ‰æÜÂ±ïÁ§∫ GBC ÁôºÁèæÁöÑË±êÂØåÁØÄÈªûÊ®ôÈ°åÔºå‰∏¶‰ΩøÁî® CLIP Ë®ìÁ∑¥ÈÄ≤Ë°åÊ∏¨Èáè„ÄÇÊàëÂÄëË°®ÊòéÔºåËàáÂÖ∂‰ªñÊï∏ÊìöÈõÜÊ†ºÂºèÁõ∏ÊØîÔºå‰ΩøÁî® GBC ÁØÄÈªûÁöÑË®ªÈáã‚Äî‚ÄîÁâπÂà•ÊòØÂ≠òÂÑ≤Âú®ÁµÑÂêàÂíåÈóú‰øÇÁØÄÈªû‰∏≠ÁöÑË®ªÈáã‚Äî‚ÄîÊúÉÈ°ØËëóÊèêÂçá‰∏ãÊ∏∏Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢ GBC Êèê‰æõÁöÑÊ©üÊúÉÔºåÊàëÂÄëÈÇÑÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊ≥®ÊÑèÊ©üÂà∂ÔºåÂÆÉÂèØ‰ª•Âà©Áî®Êï¥ÂÄã GBC ÂúñË°®Ôºå‰∏¶ÈÄöÈÅéÈºìÂãµÊÄßÁöÑÂØ¶È©óÁµêÊûúÂ±ïÁ§∫‰∫ÜÁµêÂêàÂúñË°®ÁµêÊßãÁöÑÈ°çÂ§ñÂ•ΩËôï„ÄÇÊàëÂÄëÁöÑÊï∏ÊìöÈõÜÁôºÂ∏ÉÂú® \url{https://huggingface.co/graph-based-captions}„ÄÇ</paragraph>

