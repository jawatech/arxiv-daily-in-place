{"2407.05841": {"publish_time": "2024-07-08", "title": "An Empirical Comparison of Vocabulary Expansion and Initialization Approaches for Language Models", "paper_summary": "Language Models (LMs) excel in natural language processing tasks for English\nbut show reduced performance in most other languages. This problem is commonly\ntackled by continually pre-training and fine-tuning these models for said\nlanguages. A significant issue in this process is the limited vocabulary\ncoverage in the original model's tokenizer, leading to inadequate\nrepresentation of new languages and necessitating an expansion of the\ntokenizer. The initialization of the embeddings corresponding to new vocabulary\nitems presents a further challenge. Current strategies require cross-lingual\nembeddings and lack a solid theoretical foundation as well as comparisons with\nstrong baselines. In this paper, we first establish theoretically that\ninitializing within the convex hull of existing embeddings is a good\ninitialization, followed by a novel but simple approach, Constrained Word2Vec\n(CW2V), which does not require cross-lingual embeddings. Our study evaluates\ndifferent initialization methods for expanding RoBERTa and LLaMA 2 across four\nlanguages and five tasks. The results show that CW2V performs equally well or\neven better than more advanced techniques. Additionally, simpler approaches\nlike multivariate initialization perform on par with these advanced methods\nindicating that efficient large-scale multilingual continued pretraining can be\nachieved even with simpler initialization methods.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b (LM) \u5728\u82f1\u6587\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5728\u5927\u591a\u6578\u5176\u4ed6\u8a9e\u8a00\u4e2d\u8868\u73fe\u4e0d\u4f73\u3002\u9019\u500b\u554f\u984c\u901a\u5e38\u900f\u904e\u6301\u7e8c\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u9019\u4e9b\u6a21\u578b\u4f86\u89e3\u6c7a\u3002\u9019\u500b\u904e\u7a0b\u4e2d\u7684\u4e00\u500b\u91cd\u5927\u554f\u984c\u662f\u539f\u59cb\u6a21\u578b\u7684 tokenizer \u4e2d\u8a5e\u5f59\u91cf\u6709\u9650\uff0c\u5c0e\u81f4\u65b0\u8a9e\u8a00\u7684\u8868\u73fe\u4e0d\u8db3\uff0c\u4e26\u9700\u8981\u64f4\u5145 tokenizer\u3002\u8207\u65b0\u8a5e\u5f59\u9805\u76ee\u5c0d\u61c9\u7684\u5d4c\u5165\u521d\u59cb\u5316\u63d0\u51fa\u4e86\u9032\u4e00\u6b65\u7684\u6311\u6230\u3002\u76ee\u524d\u7684\u7b56\u7565\u9700\u8981\u8de8\u8a9e\u8a00\u5d4c\u5165\uff0c\u4e26\u4e14\u7f3a\u4e4f\u7a69\u56fa\u7684\u7406\u8ad6\u57fa\u790e\u4ee5\u53ca\u8207\u5f37\u5927\u57fa\u6e96\u7684\u6bd4\u8f03\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u5148\u5f9e\u7406\u8ad6\u4e0a\u5efa\u7acb\u5728\u73fe\u6709\u5d4c\u5165\u7684\u51f8\u5305\u5167\u9032\u884c\u521d\u59cb\u5316\u662f\u4e00\u500b\u597d\u7684\u521d\u59cb\u5316\uff0c\u7136\u5f8c\u63a1\u7528\u4e00\u7a2e\u65b0\u7a4e\u4f46\u7c21\u55ae\u7684\u65b9\u6cd5\uff0c\u53d7\u7d04\u675f\u7684 Word2Vec (CW2V)\uff0c\u5b83\u4e0d\u9700\u8981\u8de8\u8a9e\u8a00\u5d4c\u5165\u3002\u6211\u5011\u7684\u7814\u7a76\u8a55\u4f30\u4e86\u64f4\u5c55 RoBERTa \u548c LLaMA 2 \u7684\u4e0d\u540c\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u6db5\u84cb\u56db\u7a2e\u8a9e\u8a00\u548c\u4e94\u9805\u4efb\u52d9\u3002\u7d50\u679c\u8868\u660e\uff0cCW2V \u7684\u8868\u73fe\u8207\u66f4\u5148\u9032\u7684\u6280\u8853\u4e00\u6a23\u597d\uff0c\u751a\u81f3\u66f4\u597d\u3002\u6b64\u5916\uff0c\u50cf\u591a\u8b8a\u91cf\u521d\u59cb\u5316\u9019\u6a23\u7684\u66f4\u7c21\u55ae\u65b9\u6cd5\u8207\u9019\u4e9b\u5148\u9032\u65b9\u6cd5\u7684\u8868\u73fe\u4e0d\u76f8\u4e0a\u4e0b\uff0c\u9019\u8868\u660e\u5373\u4f7f\u4f7f\u7528\u66f4\u7c21\u55ae\u7684\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u4e5f\u53ef\u4ee5\u5be6\u73fe\u9ad8\u6548\u7684\u5927\u898f\u6a21\u591a\u8a9e\u8a00\u6301\u7e8c\u9810\u8a13\u7df4\u3002", "author": "Nandini Mundra et.al.", "authors": "Nandini Mundra, Aditya Nanda Kishore, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Mitesh M. Khapra", "id": "2407.05841v1", "paper_url": "http://arxiv.org/abs/2407.05841v1", "repo": "null"}}