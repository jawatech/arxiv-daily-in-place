{"2409.10790": {"publish_time": "2024-09-16", "title": "Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering", "paper_summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious real-world tasks. However, they often struggle to fully comprehend and\neffectively utilize their input contexts, resulting in responses that are\nunfaithful or hallucinated. This difficulty increases for contexts that are\nlong or contain distracting information, which can divert LLMs from fully\ncapturing essential evidence. To address this issue, many works use prompting\nto help LLMs utilize contextual information more faithfully. For instance,\niterative prompting highlights key information in two steps that first ask the\nLLM to identify important pieces of context and then derive answers\naccordingly. However, prompting methods are constrained to highlighting key\ninformation implicitly in token space, which is often insufficient to fully\nsteer the model's attention. To improve model faithfulness more reliably, we\npropose AutoPASTA, a method that automatically identifies key contextual\ninformation and explicitly highlights it by steering an LLM's attention scores.\nLike prompting, AutoPASTA is applied at inference time and does not require\nchanging any model parameters. Our experiments on open-book QA demonstrate that\nAutoPASTA effectively enables models to grasp essential contextual information,\nleading to substantially improved model faithfulness and performance, e.g., an\naverage improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly\navailable at https://github.com/QingruZhang/AutoPASTA .", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u5b83\u5011\u7d93\u5e38\u96e3\u4ee5\u5b8c\u5168\u7406\u89e3\u4e26\u6709\u6548\u5229\u7528\u5176\u8f38\u5165\u5167\u5bb9\uff0c\u5c0e\u81f4\u56de\u61c9\u4e0d\u5fe0\u5be6\u6216\u51fa\u73fe\u5e7b\u89ba\u3002\u5c0d\u65bc\u9577\u7bc7\u5167\u5bb9\u6216\u5305\u542b\u5206\u6563\u6ce8\u610f\u529b\u7684\u8cc7\u8a0a\u7684\u5167\u5bb9\uff0c\u9019\u7a2e\u56f0\u96e3\u6703\u589e\u52a0\uff0c\u9019\u53ef\u80fd\u6703\u8b93 LLM \u7121\u6cd5\u5b8c\u5168\u64f7\u53d6\u5fc5\u8981\u7684\u8b49\u64da\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u8a31\u591a\u4f5c\u54c1\u4f7f\u7528\u63d0\u793a\u4f86\u5e6b\u52a9 LLM \u66f4\u5fe0\u5be6\u5730\u5229\u7528\u5167\u5bb9\u8cc7\u8a0a\u3002\u4f8b\u5982\uff0c\u53cd\u8986\u63d0\u793a\u6703\u5728\u5169\u500b\u6b65\u9a5f\u4e2d\u5f37\u8abf\u95dc\u9375\u8cc7\u8a0a\uff0c\u9996\u5148\u8981\u6c42 LLM \u627e\u51fa\u91cd\u8981\u7684\u5167\u5bb9\u90e8\u5206\uff0c\u7136\u5f8c\u64da\u6b64\u63a8\u5c0e\u7b54\u6848\u3002\u7136\u800c\uff0c\u63d0\u793a\u65b9\u6cd5\u50c5\u9650\u65bc\u5728\u6a19\u8a18\u7a7a\u9593\u4e2d\u96b1\u542b\u5730\u5f37\u8abf\u95dc\u9375\u8cc7\u8a0a\uff0c\u9019\u901a\u5e38\u4e0d\u8db3\u4ee5\u5b8c\u5168\u5f15\u5c0e\u6a21\u578b\u7684\u6ce8\u610f\u529b\u3002\u70ba\u4e86\u66f4\u53ef\u9760\u5730\u6539\u5584\u6a21\u578b\u7684\u5fe0\u5be6\u5ea6\uff0c\u6211\u5011\u63d0\u51fa AutoPASTA\uff0c\u9019\u662f\u4e00\u7a2e\u81ea\u52d5\u627e\u51fa\u95dc\u9375\u5167\u5bb9\u8cc7\u8a0a\u4e26\u900f\u904e\u5f15\u5c0e LLM \u7684\u6ce8\u610f\u529b\u5206\u6578\u4f86\u660e\u78ba\u5f37\u8abf\u5b83\u7684\u65b9\u6cd5\u3002\u8207\u63d0\u793a\u985e\u4f3c\uff0cAutoPASTA \u61c9\u7528\u65bc\u63a8\u8ad6\u6642\u9593\uff0c\u4e0d\u9700\u8981\u8b8a\u66f4\u4efb\u4f55\u6a21\u578b\u53c3\u6578\u3002\u6211\u5011\u5728\u958b\u653e\u5f0f\u554f\u7b54\u4e0a\u7684\u5be6\u9a57\u986f\u793a\uff0cAutoPASTA \u6709\u6548\u5730\u8b93\u6a21\u578b\u638c\u63e1\u5fc5\u8981\u7684\u5167\u5bb9\u8cc7\u8a0a\uff0c\u5927\u5e45\u6539\u5584\u6a21\u578b\u7684\u5fe0\u5be6\u5ea6\u548c\u6548\u80fd\uff0c\u4f8b\u5982\uff0cLLAMA3-70B-Instruct \u7684\u5e73\u5747\u6539\u5584\u5e45\u5ea6\u70ba 7.95%\u3002\u7a0b\u5f0f\u78bc\u5c07\u516c\u958b\u65bc https://github.com/QingruZhang/AutoPASTA\u3002", "author": "Qingru Zhang et.al.", "authors": "Qingru Zhang, Xiaodong Yu, Chandan Singh, Xiaodong Liu, Liyuan Liu, Jianfeng Gao, Tuo Zhao, Dan Roth, Hao Cheng", "id": "2409.10790v1", "paper_url": "http://arxiv.org/abs/2409.10790v1", "repo": "null"}}