
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-31**|**Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**|Chaoyou Fu et.al.|[2405.21075v1](http://arxiv.org/abs/2405.21075v1)|null|
|**2024-05-31**|**Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights**|Xin Wen et.al.|[2405.21070v1](http://arxiv.org/abs/2405.21070v1)|[link](https://github.com/cvmi-lab/clip-beyond-tail)|
|**2024-05-31**|**Code Pretraining Improves Entity Tracking Abilities of Language Models**|Najoung Kim et.al.|[2405.21068v1](http://arxiv.org/abs/2405.21068v1)|null|
|**2024-05-31**|**Recurrent neural networks: vanishing and exploding gradients are not the end of the story**|Nicolas Zucchet et.al.|[2405.21064v1](http://arxiv.org/abs/2405.21064v1)|null|
|**2024-05-31**|**Grammar-Aligned Decoding**|Kanghee Park et.al.|[2405.21047v1](http://arxiv.org/abs/2405.21047v1)|null|
|**2024-05-31**|**Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF**|Tengyang Xie et.al.|[2405.21046v1](http://arxiv.org/abs/2405.21046v1)|null|
|**2024-05-31**|**Target Networks and Over-parameterization Stabilize Off-policy Bootstrapping with Function Approximation**|Fengdi Che et.al.|[2405.21043v1](http://arxiv.org/abs/2405.21043v1)|null|
|**2024-05-31**|**Direct Alignment of Language Models via Quality-Aware Self-Refinement**|Runsheng Yu et.al.|[2405.21040v1](http://arxiv.org/abs/2405.21040v1)|null|
|**2024-05-31**|**Standards for Belief Representations in LLMs**|Daniel A. Herrmann et.al.|[2405.21030v1](http://arxiv.org/abs/2405.21030v1)|null|
|**2024-05-31**|**LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models**|Elias Stengel-Eskin et.al.|[2405.21028v1](http://arxiv.org/abs/2405.21028v1)|[link](https://github.com/esteng/pragmatic_calibration)|
|**2024-05-31**|**Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles**|Jiesong Lian et.al.|[2405.21027v1](http://arxiv.org/abs/2405.21027v1)|null|
|**2024-05-31**|**Compact Optimality Verification for Optimization Proxies**|Wenbo Chen et.al.|[2405.21023v1](http://arxiv.org/abs/2405.21023v1)|null|
|**2024-05-31**|**You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet**|Zhen Qin et.al.|[2405.21022v1](http://arxiv.org/abs/2405.21022v1)|null|
|**2024-05-31**|**Improved Techniques for Optimization-Based Jailbreaking on Large Language Models**|Xiaojun Jia et.al.|[2405.21018v1](http://arxiv.org/abs/2405.21018v1)|[link](https://github.com/jiaxiaojunqaq/i-gcg)|
|**2024-05-31**|**CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to Web Relevance Ranking**|Josef Vonášek et.al.|[2405.20994v1](http://arxiv.org/abs/2405.20994v1)|null|
|**2024-05-31**|**Locking Machine Learning Models into Hardware**|Eleanor Clifford et.al.|[2405.20990v1](http://arxiv.org/abs/2405.20990v1)|null|
|**2024-05-31**|**Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training**|Feiteng Fang et.al.|[2405.20978v1](http://arxiv.org/abs/2405.20978v1)|null|
|**2024-05-31**|**ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning**|Zhangchen Xu et.al.|[2405.20975v1](http://arxiv.org/abs/2405.20975v1)|null|
|**2024-05-31**|**SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales**|Tianyang Xu et.al.|[2405.20974v1](http://arxiv.org/abs/2405.20974v1)|[link](https://github.com/xu1868/sayself)|
|**2024-05-31**|**LCQ: Low-Rank Codebook based Quantization for Large Language Models**|Wen-Pu Cai et.al.|[2405.20973v1](http://arxiv.org/abs/2405.20973v1)|null|
|**2024-05-31**|**Superlatives in Context: Explicit and Implicit Domain Restrictions for Superlative Frames**|Valentina Pyatkin et.al.|[2405.20967v1](http://arxiv.org/abs/2405.20967v1)|null|
|**2024-05-31**|**Large Language Models are Zero-Shot Next Location Predictors**|Ciro Beneduce et.al.|[2405.20962v1](http://arxiv.org/abs/2405.20962v1)|[link](https://github.com/ssai-trento/llm-zero-shot-nl)|
|**2024-05-31**|**Navigating Tabular Data Synthesis Research: Understanding User Needs and Tool Capabilities**|Maria F. Davila R. et.al.|[2405.20959v1](http://arxiv.org/abs/2405.20959v1)|null|
|**2024-05-31**|**A Robot Walks into a Bar: Can Language Models Serve asCreativity Support Tools for Comedy? An Evaluation of LLMs' Humour Alignment with Comedians**|Piotr Wojciech Mirowski et.al.|[2405.20956v1](http://arxiv.org/abs/2405.20956v1)|null|
|**2024-05-31**|**OR-Bench: An Over-Refusal Benchmark for Large Language Models**|Justin Cui et.al.|[2405.20947v1](http://arxiv.org/abs/2405.20947v1)|null|
|**2024-05-31**|**Effective Interplay between Sparsity and Quantization: From Theory to Practice**|Simla Burcu Harma et.al.|[2405.20935v1](http://arxiv.org/abs/2405.20935v1)|null|
|**2024-05-31**|**Learning to Estimate System Specifications in Linear Temporal Logic using Transformers and Mamba**|İlker Işık et.al.|[2405.20917v1](http://arxiv.org/abs/2405.20917v1)|null|
|**2024-05-31**|**Fast yet Safe: Early-Exiting with Risk Control**|Metod Jazbec et.al.|[2405.20915v1](http://arxiv.org/abs/2405.20915v1)|null|
|**2024-05-31**|**Enhancing Vision Models for Text-Heavy Content Understanding and Interaction**|Adithya TG et.al.|[2405.20906v1](http://arxiv.org/abs/2405.20906v1)|null|
|**2024-05-31**|**Preemptive Answer "Attacks" on Chain-of-Thought Reasoning**|Rongwu Xu et.al.|[2405.20902v1](http://arxiv.org/abs/2405.20902v1)|null|
|**2024-05-31**|**Large Language Models: A New Approach for Privacy Policy Analysis at Scale**|David Rodriguez et.al.|[2405.20900v1](http://arxiv.org/abs/2405.20900v1)|null|
|**2024-05-31**|**MALT: Multi-scale Action Learning Transformer for Online Action Detection**|Zhipeng Yang et.al.|[2405.20892v1](http://arxiv.org/abs/2405.20892v1)|null|
|**2024-05-31**|**Paying to Do Better: Games with Payments between Learning Agents**|Yoav Kolumbus et.al.|[2405.20880v1](http://arxiv.org/abs/2405.20880v1)|null|
|**2024-05-31**|**SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation**|Yuxi Liu et.al.|[2405.20878v1](http://arxiv.org/abs/2405.20878v1)|[link](https://github.com/hkuds/selfgnn)|
|**2024-05-31**|**Investigating Calibration and Corruption Robustness of Post-hoc Pruned Perception CNNs: An Image Classification Benchmark Study**|Pallavi Mitra et.al.|[2405.20876v1](http://arxiv.org/abs/2405.20876v1)|null|
|**2024-05-31**|**Automatic Channel Pruning for Multi-Head Attention**|Eunho Lee et.al.|[2405.20867v1](http://arxiv.org/abs/2405.20867v1)|null|
|**2024-05-31**|**ABodyBuilder3: Improved and scalable antibody structure predictions**|Henry Kenlay et.al.|[2405.20863v1](http://arxiv.org/abs/2405.20863v1)|[link](https://github.com/exscientia/abodybuilder3)|
|**2024-05-31**|**clembench-2024: A Challenging, Dynamic, Complementary, Multilingual Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents**|Anne Beyer et.al.|[2405.20859v1](http://arxiv.org/abs/2405.20859v1)|null|
|**2024-05-31**|**Towards Spoken Language Understanding via Multi-level Multi-grained Contrastive Learning**|Xuxin Cheng et.al.|[2405.20852v1](http://arxiv.org/abs/2405.20852v1)|null|
|**2024-05-31**|**Improving Reward Models with Synthetic Critiques**|Zihuiwen Ye et.al.|[2405.20850v1](http://arxiv.org/abs/2405.20850v1)|null|
|**2024-05-31**|**SLIM: a Scalable Light-weight Root Cause Analysis for Imbalanced Data in Microservice**|Rui Ren et.al.|[2405.20848v1](http://arxiv.org/abs/2405.20848v1)|null|
|**2024-05-31**|**Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models**|A. Bavaresco et.al.|[2405.20846v1](http://arxiv.org/abs/2405.20846v1)|null|
|**2024-05-31**|**einspace: Searching for Neural Architectures from Fundamental Operations**|Linus Ericsson et.al.|[2405.20838v1](http://arxiv.org/abs/2405.20838v1)|[link](https://github.com/linusericsson/einspace)|
|**2024-05-31**|**Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs**|Davide Paglieri et.al.|[2405.20835v1](http://arxiv.org/abs/2405.20835v1)|null|
|**2024-05-31**|**That's Optional: A Contemporary Exploration of "that" Omission in English Subordinate Clauses**|Ella Rabinovich et.al.|[2405.20833v1](http://arxiv.org/abs/2405.20833v1)|null|
|**2024-05-31**|**Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment**|Yueqin Yin et.al.|[2405.20830v1](http://arxiv.org/abs/2405.20830v1)|null|
|**2024-05-31**|**An iterated learning model of language change that mixes supervised and unsupervised learning**|Jack Bunyan et.al.|[2405.20818v1](http://arxiv.org/abs/2405.20818v1)|null|
|**2024-05-31**|**There and Back Again: The AI Alignment Paradox**|Robert West et.al.|[2405.20806v1](http://arxiv.org/abs/2405.20806v1)|null|
|**2024-05-31**|**Multilingual Text Style Transfer: Datasets & Models for Indian Languages**|Sourabrata Mukherjee et.al.|[2405.20805v1](http://arxiv.org/abs/2405.20805v1)|null|
|**2024-05-31**|**Ovis: Structural Embedding Alignment for Multimodal Large Language Model**|Shiyin Lu et.al.|[2405.20797v1](http://arxiv.org/abs/2405.20797v1)|null|
|**2024-05-31**|**InsightSee: Advancing Multi-agent Vision-Language Models for Enhanced Visual Understanding**|Huaxiang Zhang et.al.|[2405.20795v1](http://arxiv.org/abs/2405.20795v1)|null|
|**2024-05-31**|**Improving code-mixed hate detection by native sample mixing: A case study for Hindi-English code-mixed scenario**|Debajyoti Mazumder et.al.|[2405.20755v1](http://arxiv.org/abs/2405.20755v1)|null|
|**2024-05-31**|**Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent Codes**|Riccardo Benaglia et.al.|[2405.20743v1](http://arxiv.org/abs/2405.20743v1)|null|
|**2024-05-31**|**Maximum Temperature Prediction Using Remote Sensing Data Via Convolutional Neural Network**|Lorenzo Innocenti et.al.|[2405.20731v1](http://arxiv.org/abs/2405.20731v1)|null|
|**2024-05-31**|**GANcrop: A Contrastive Defense Against Backdoor Attacks in Federated Learning**|Xiaoyun Gan et.al.|[2405.20727v1](http://arxiv.org/abs/2405.20727v1)|null|
|**2024-05-31**|**GI-NAS: Boosting Gradient Inversion Attacks through Adaptive Neural Architecture Search**|Wenbo Yu et.al.|[2405.20725v1](http://arxiv.org/abs/2405.20725v1)|null|
|**2024-05-31**|**ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model**|Yufei Wang et.al.|[2405.20721v1](http://arxiv.org/abs/2405.20721v1)|[link](https://github.com/wyf0912/contextgs)|
|**2024-05-31**|**Climate Variable Downscaling with Conditional Normalizing Flows**|Christina Winkler et.al.|[2405.20719v1](http://arxiv.org/abs/2405.20719v1)|null|
|**2024-05-31**|**Popularity-Aware Alignment and Contrast for Mitigating Popularity Bias**|Miaomiao Cai et.al.|[2405.20718v1](http://arxiv.org/abs/2405.20718v1)|[link](https://github.com/miaomiao-cai2/kdd2024-paac)|
|**2024-05-31**|**FinGen: A Dataset for Argument Generation in Finance**|Chung-Chi Chen et.al.|[2405.20708v1](http://arxiv.org/abs/2405.20708v1)|null|
|**2024-05-31**|**ADESSE: Advice Explanations in Complex Repeated Decision-Making Environments**|Sören Schleibaum et.al.|[2405.20705v1](http://arxiv.org/abs/2405.20705v1)|null|
|**2024-05-31**|**It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance**|Laura Cabello et.al.|[2405.20703v1](http://arxiv.org/abs/2405.20703v1)|null|
|**2024-05-31**|**Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement**|Pengwei Zhan et.al.|[2405.20701v1](http://arxiv.org/abs/2405.20701v1)|null|
|**2024-05-31**|**Self-degraded contrastive domain adaptation for industrial fault diagnosis with bi-imbalanced data**|Gecheng Chen et.al.|[2405.20700v1](http://arxiv.org/abs/2405.20700v1)|null|
|**2024-05-31**|**Joint Embeddings for Graph Instruction Tuning**|Vlad Argatu et.al.|[2405.20684v1](http://arxiv.org/abs/2405.20684v1)|null|
|**2024-05-31**|**No Free Lunch Theorem for Privacy-Preserving LLM Inference**|Xiaojin Zhang et.al.|[2405.20681v1](http://arxiv.org/abs/2405.20681v1)|null|
|**2024-05-31**|**Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models**|Mingda Li et.al.|[2405.20680v1](http://arxiv.org/abs/2405.20680v1)|null|
|**2024-05-31**|**Adv-KD: Adversarial Knowledge Distillation for Faster Diffusion Sampling**|Kidist Amde Mekonnen et.al.|[2405.20675v1](http://arxiv.org/abs/2405.20675v1)|[link](https://github.com/kidist-amde/adv-kd)|
|**2024-05-31**|**DORY: Deliberative Prompt Recovery for LLM**|Lirong Gao et.al.|[2405.20657v1](http://arxiv.org/abs/2405.20657v1)|null|
|**2024-05-31**|**Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models**|Xuyang Wu et.al.|[2405.20654v1](http://arxiv.org/abs/2405.20654v1)|null|
|**2024-05-31**|**Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens**|Jiahao Yu et.al.|[2405.20653v1](http://arxiv.org/abs/2405.20653v1)|null|
|**2024-05-31**|**Reward-based Input Construction for Cross-document Relation Extraction**|Byeonghu Na et.al.|[2405.20649v1](http://arxiv.org/abs/2405.20649v1)|[link](https://github.com/aailabkaist/reic)|
|**2024-05-31**|**Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models For Video Captioning and Summarization**|Richard Luo et.al.|[2405.20648v1](http://arxiv.org/abs/2405.20648v1)|null|
|**2024-05-31**|**Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item**|Qidong Liu et.al.|[2405.20646v1](http://arxiv.org/abs/2405.20646v1)|null|
|**2024-05-31**|**Learning Gaze-aware Compositional GAN**|Nerea Aranjuelo et.al.|[2405.20643v1](http://arxiv.org/abs/2405.20643v1)|[link](https://github.com/naranjuelo/gc-gan)|
|**2024-05-31**|**ToxVidLLM: A Multimodal LLM-based Framework for Toxicity Detection in Code-Mixed Videos**|Krishanu Maity et.al.|[2405.20628v1](http://arxiv.org/abs/2405.20628v1)|null|
|**2024-05-31**|**Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning**|Atharva Gundawar et.al.|[2405.20625v1](http://arxiv.org/abs/2405.20625v1)|null|
|**2024-05-31**|**Leveraging Large Language Models for Entity Matching**|Qianyu Huang et.al.|[2405.20624v1](http://arxiv.org/abs/2405.20624v1)|null|
|**2024-05-31**|**FineRadScore: A Radiology Report Line-by-Line Evaluation Technique Generating Corrections with Severity Scores**|Alyssa Huang et.al.|[2405.20613v1](http://arxiv.org/abs/2405.20613v1)|null|
|**2024-05-31**|**UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation**|Hanzhang Zhou et.al.|[2405.20612v1](http://arxiv.org/abs/2405.20612v1)|null|
|**2024-05-31**|**Bi-Directional Transformers vs. word2vec: Discovering Vulnerabilities in Lifted Compiled Code**|Gary A. McCully et.al.|[2405.20611v1](http://arxiv.org/abs/2405.20611v1)|null|
|**2024-05-31**|**Vision-Language Meets the Skeleton: Progressively Distillation with Cross-Modal Knowledge for 3D Action Representation Learning**|Yang Chen et.al.|[2405.20606v1](http://arxiv.org/abs/2405.20606v1)|null|
|**2024-05-31**|**Searching for internal symbols underlying deep learning**|Jung H. Lee et.al.|[2405.20605v1](http://arxiv.org/abs/2405.20605v1)|null|
|**2024-05-31**|**Advancing Financial Risk Prediction Through Optimized LSTM Model Performance and Comparative Analysis**|Ke Xu et.al.|[2405.20603v1](http://arxiv.org/abs/2405.20603v1)|null|
|**2024-05-31**|**Masked Language Modeling Becomes Conditional Density Estimation for Tabular Data Synthesis**|Seunghwan An et.al.|[2405.20602v1](http://arxiv.org/abs/2405.20602v1)|null|
|**2024-05-31**|**Multi-label Class Incremental Emotion Decoding with Augmented Emotional Semantics Learning**|Kaicheng Fu et.al.|[2405.20600v1](http://arxiv.org/abs/2405.20600v1)|null|
|**2024-05-31**|**Class-Based Time Series Data Augmentation to Mitigate Extreme Class Imbalance for Solar Flare Prediction**|Junzhi Wen et.al.|[2405.20590v1](http://arxiv.org/abs/2405.20590v1)|null|
|**2024-05-31**|**Selective Knowledge Sharing for Personalized Federated Learning Under Capacity Heterogeneity**|Zheng Wang et.al.|[2405.20589v1](http://arxiv.org/abs/2405.20589v1)|null|
|**2024-05-31**|**DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models**|Taolin Zhang et.al.|[2405.20588v1](http://arxiv.org/abs/2405.20588v1)|null|
|**2024-05-31**|**GAMedX: Generative AI-based Medical Entity Data Extractor Using Large Language Models**|Mohammed-Khalil Ghali et.al.|[2405.20585v1](http://arxiv.org/abs/2405.20585v1)|null|
|**2024-05-31**|**Disrupting Diffusion: Token-Level Attention Erasure Attack against Diffusion-based Customization**|Yisu Liu et.al.|[2405.20584v1](http://arxiv.org/abs/2405.20584v1)|null|
|**2024-05-31**|**The Point of View of a Sentiment: Towards Clinician Bias Detection in Psychiatric Notes**|Alissa A. Valentine et.al.|[2405.20582v1](http://arxiv.org/abs/2405.20582v1)|null|
|**2024-05-31**|**Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark**|Chanjun Park et.al.|[2405.20574v1](http://arxiv.org/abs/2405.20574v1)|null|
|**2024-05-31**|**Can Machine Learning Assist in Diagnosis of Primary Immune Thrombocytopenia? A feasibility study**|Haroon Miah et.al.|[2405.20562v1](http://arxiv.org/abs/2405.20562v1)|null|
|**2024-05-30**|**Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models**|Zachary Ankner et.al.|[2405.20541v1](http://arxiv.org/abs/2405.20541v1)|null|
|**2024-05-30**|**Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning**|Xinlu Zhang et.al.|[2405.20535v1](http://arxiv.org/abs/2405.20535v1)|null|
|**2024-05-30**|**An Automatic Question Usability Evaluation Toolkit**|Steven Moore et.al.|[2405.20529v1](http://arxiv.org/abs/2405.20529v1)|null|
|**2024-05-30**|**Towards Ontology-Enhanced Representation Learning for Large Language Models**|Francesco Ronzano et.al.|[2405.20527v1](http://arxiv.org/abs/2405.20527v1)|null|
|**2024-05-30**|**Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions**|Steven Moore et.al.|[2405.20526v1](http://arxiv.org/abs/2405.20526v1)|[link](https://github.com/stevenjamesmoore/learningatscale24)|
|**2024-05-30**|**Diffusion On Syntax Trees For Program Synthesis**|Shreyas Kapur et.al.|[2405.20519v1](http://arxiv.org/abs/2405.20519v1)|null|

#### Abstracts
##### **Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**
2405.21075v1 by Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, Xing Sun

In the quest for artificial general intelligence, Multi-modal Large Language
Models (MLLMs) have emerged as a focal point in recent advancements. However,
the predominant focus remains on developing their capabilities in static image
understanding. The potential of MLLMs in processing sequential visual data is
still insufficiently explored, highlighting the absence of a comprehensive,
high-quality assessment of their performance. In this paper, we introduce
Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of
MLLMs in Video analysis. Our work distinguishes from existing benchmarks
through four key features: 1) Diversity in video types, spanning 6 primary
visual domains with 30 subfields to ensure broad scenario generalizability; 2)
Duration in temporal dimension, encompassing both short-, medium-, and
long-term videos, ranging from 11 seconds to 1 hour, for robust contextual
dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides
video frames, including subtitles and audios, to unveil the all-round
capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual
labeling by expert annotators to facilitate precise and reliable model
assessment. 900 videos with a total of 256 hours are manually selected and
annotated by repeatedly viewing all the video content, resulting in 2,700
question-answer pairs. With Video-MME, we extensively evaluate various
state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as
open-source image models like InternVL-Chat-V1.5 and video models like
LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the
best-performing commercial model, significantly outperforming the open-source
models. Our dataset along with these findings underscores the need for further
improvements in handling longer sequences and multi-modal data. Project Page:
https://video-mme.github.io

摘要：在追求人工通用智能的過程中，多模態大型語言模型 (MLLM) 已成為近期進展中的焦點。然而，主要的重點仍放在發展它們在靜態影像理解方面的能力。MLLM 在處理順序視覺資料的潛力仍未被充分探討，突顯了缺乏對其效能進行全面、高品質評估。在本文中，我們引入了 Video-MME，這是第一個針對 MLLM 在影片分析中的全光譜多模態評估基準。我們的研究透過四項關鍵特徵區別於現有基準：1) 影片類型的多樣性，涵蓋 6 個主要視覺領域，包含 30 個子領域，以確保廣泛的場景通用性；2) 時間維度的持續時間，包含短、中、長期影片，範圍從 11 秒到 1 小時，以確保穩健的脈絡動態；3) 資料模態的廣度，除了影片幀之外，還整合了多模態輸入，包括字幕和音訊，以揭示 MLLM 的全面能力；4) 標註的品質，利用專家標註員嚴謹的手動標註，以利精確且可靠的模型評估。900 個影片，總長為 256 小時，透過重複觀看所有影片內容進行手動挑選和標註，產生了 2,700 個問答配對。透過 Video-MME，我們廣泛評估了各種最先進的 MLLM，包括 GPT-4 系列和 Gemini 1.5 Pro，以及像 InternVL-Chat-V1.5 的開源影像模型和 LLaVA-NeXT-Video 的影片模型。我們的實驗顯示，Gemini 1.5 Pro 是效能最好的商業模型，顯著優於開源模型。我們的資料集和這些發現強調了進一步改進處理較長順序和多模態資料的必要性。專案頁面：https://video-mme.github.io

##### **Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights**
2405.21070v1 by Xin Wen, Bingchen Zhao, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi

Severe data imbalance naturally exists among web-scale vision-language
datasets. Despite this, we find CLIP pre-trained thereupon exhibits notable
robustness to the data imbalance compared to supervised learning, and
demonstrates significant effectiveness in learning generalizable
representations. With an aim to investigate the reasons behind this finding, we
conduct controlled experiments to study various underlying factors, and reveal
that CLIP's pretext task forms a dynamic classification problem wherein only a
subset of classes is present in training. This isolates the bias from dominant
classes and implicitly balances the learning signal. Furthermore, the
robustness and discriminability of CLIP improve with more descriptive language
supervision, larger data scale, and broader open-world concepts, which are
inaccessible to supervised learning. Our study not only uncovers the mechanisms
behind CLIP's generalizability beyond data imbalance but also provides
transferable insights for the research community. The findings are validated in
both supervised and self-supervised learning, enabling models trained on
imbalanced data to achieve CLIP-level performance on diverse recognition tasks.
Code will be available at: https://github.com/CVMI-Lab/clip-beyond-tail.

摘要：嚴重的資料不平衡自然存在於網路規模的視覺語言資料集中。儘管如此，我們發現預先訓練於其上的 CLIP 對資料不平衡展現出顯著的穩健性，與監督學習相比，並展現出學習可概化表徵的顯著效能。為了調查此發現背後的原因，我們進行受控實驗以研究各種潛在因素，並揭示 CLIP 的藉口任務形成一個動態分類問題，其中只有一個子集的類別存在於訓練中。這會從主要類別中分離出偏差，並隱含地平衡學習訊號。此外，CLIP 的穩健性和辨別力會隨著更具描述性的語言監督、更大的資料規模和更廣泛的開放世界概念而改善，而這些概念對於監督學習來說是無法取得的。我們的研究不僅揭示了 CLIP 在資料不平衡之外的概化能力背後的機制，也為研究社群提供了可轉移的見解。這些發現已在監督式和自監督式學習中得到驗證，讓在不平衡資料上訓練的模型能夠在各種辨識任務中達到 CLIP 等級的效能。程式碼將於下列位置提供：https://github.com/CVMI-Lab/clip-beyond-tail。

##### **Code Pretraining Improves Entity Tracking Abilities of Language Models**
2405.21068v1 by Najoung Kim, Sebastian Schuster, Shubham Toshniwal

Recent work has provided indirect evidence that pretraining language models
on code improves the ability of models to track state changes of discourse
entities expressed in natural language. In this work, we systematically test
this claim by comparing pairs of language models on their entity tracking
performance. Critically, the pairs consist of base models and models trained on
top of these base models with additional code data. We extend this analysis to
additionally examine the effect of math training, another highly structured
data type, and alignment tuning, an important step for enhancing the usability
of models. We find clear evidence that models additionally trained on large
amounts of code outperform the base models. On the other hand, we find no
consistent benefit of additional math training or alignment tuning across
various model families.

摘要：最近的研究提供了間接證據，證明在程式碼上預訓練語言模型，能提升模型追蹤自然語言中表達的對話實體狀態變化的能力。在這項研究中，我們系統性地測試此主張，方法是比較語言模型配對的實體追蹤表現。至關重要的是，這些配對包含基礎模型和在這些基礎模型上訓練的模型，並加入額外的程式碼資料。我們延伸此分析，以進一步檢視數學訓練的影響，另一種高度結構化的資料類型，以及對齊調整，這是增強模型可用性的重要步驟。我們發現明確的證據，證明在大量程式碼上額外訓練的模型，表現優於基礎模型。另一方面，我們發現額外的數學訓練或對齊調整，在各種模型系列中沒有帶來一致性的好處。

##### **Recurrent neural networks: vanishing and exploding gradients are not the end of the story**
2405.21064v1 by Nicolas Zucchet, Antonio Orvieto

Recurrent neural networks (RNNs) notoriously struggle to learn long-term
memories, primarily due to vanishing and exploding gradients. The recent
success of state-space models (SSMs), a subclass of RNNs, to overcome such
difficulties challenges our theoretical understanding. In this paper, we delve
into the optimization challenges of RNNs and discover that, as the memory of a
network increases, changes in its parameters result in increasingly large
output variations, making gradient-based learning highly sensitive, even
without exploding gradients. Our analysis further reveals the importance of the
element-wise recurrence design pattern combined with careful parametrizations
in mitigating this effect. This feature is present in SSMs, as well as in other
architectures, such as LSTMs. Overall, our insights provide a new explanation
for some of the difficulties in gradient-based learning of RNNs and why some
architectures perform better than others.

摘要：遞迴神經網路 (RNN) 惡名昭彰地難以學習長期記憶，主要是由於消失和爆炸梯度。最近，作為 RNN 子類的狀態空間模型 (SSM) 在克服此類困難方面獲得成功，這挑戰了我們的理論理解。在本文中，我們深入探討 RNN 的最佳化挑戰，並發現隨著網路記憶的增加，其參數的變化會導致越來越大的輸出變化，使得基於梯度的學習高度敏感，即使沒有爆炸梯度。我們的分析進一步揭示了逐元素遞迴設計模式與仔細參數化相結合在減輕此效應方面的重要性。此功能存在於 SSM 以及其他架構中，例如 LSTM。總體而言，我們的見解為 RNN 基於梯度的學習中的一些困難提供了一個新的解釋，並說明了為什麼某些架構的表現優於其他架構。

##### **Grammar-Aligned Decoding**
2405.21047v1 by Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, Loris D'Antoni

Large Language Models (LLMs) struggle with reliably generating highly
structured outputs, such as program code, mathematical formulas, or well-formed
markup. Constrained decoding approaches mitigate this problem by greedily
restricting what tokens an LLM can output at each step to guarantee that the
output matches a given constraint. Specifically, in grammar-constrained
decoding (GCD), the LLM's output must follow a given grammar. In this paper we
demonstrate that GCD techniques (and in general constrained decoding
techniques) can distort the LLM's distribution, leading to outputs that are
grammatical but appear with likelihoods that are not proportional to the ones
given by the LLM, and so ultimately are low-quality. We call the problem of
aligning sampling with a grammar constraint, grammar-aligned decoding (GAD),
and propose adaptive sampling with approximate expected futures (ASAp), a
decoding algorithm that guarantees the output to be grammatical while provably
producing outputs that match the conditional probability of the LLM's
distribution conditioned on the given grammar constraint. Our algorithm uses
prior sample outputs to soundly overapproximate the future grammaticality of
different output prefixes. Our evaluation on code generation and structured NLP
tasks shows how ASAp often produces outputs with higher likelihood (according
to the LLM's distribution) than existing GCD techniques, while still enforcing
the desired grammatical constraints.

摘要：大型語言模型 (LLM) 難以可靠地生成高度結構化的輸出，例如程式碼、數學公式或格式良好的標記。受限解碼方法透過貪婪地限制 LLM 在每個步驟中可以輸出的符號來減輕此問題，以確保輸出符合給定的約束。具體來說，在語法約束解碼 (GCD) 中，LLM 的輸出必須遵循給定的語法。在本文中，我們證明 GCD 技術（以及一般的約束解碼技術）會扭曲 LLM 的分佈，導致語法正確但出現機率與 LLM 給出的機率不成比例的輸出，因此最終品質不佳。我們將對齊抽樣與語法約束的問題稱為語法對齊解碼 (GAD)，並提出具有近似預期未來的自適應抽樣 (ASAp)，這是一種解碼演算法，可保證輸出語法正確，同時可證明產生與 LLM 分佈條件機率相符的輸出，條件為給定的語法約束。我們的演算法使用先前的樣本輸出，以健全地過度近似不同輸出前綴的未來語法性。我們對程式碼生成和結構化 NLP 任務的評估顯示，與現有的 GCD 技術相比，ASAp 通常會產生具有更高機率（根據 LLM 的分佈）的輸出，同時仍強制執行所需的語法約束。

##### **Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF**
2405.21046v1 by Tengyang Xie, Dylan J. Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, Alexander Rakhlin

Reinforcement learning from human feedback (RLHF) has emerged as a central
tool for language model alignment. We consider online exploration in RLHF,
which exploits interactive access to human or AI feedback by deliberately
encouraging the model to produce diverse, maximally informative responses. By
allowing RLHF to confidently stray from the pre-trained model, online
exploration offers the possibility of novel, potentially super-human
capabilities, but its full potential as a paradigm for language model training
has yet to be realized, owing to computational and statistical bottlenecks in
directly adapting existing reinforcement learning techniques. We propose a new
algorithm for online exploration in RLHF, Exploratory Preference Optimization
(XPO), which is simple and practical -- a one-line change to (online) Direct
Preference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the
strongest known provable guarantees and promising empirical performance. XPO
augments the DPO objective with a novel and principled exploration bonus,
empowering the algorithm to explore outside the support of the initial model
and human feedback data. In theory, we show that XPO is provably
sample-efficient and converges to a near-optimal language model policy under
natural exploration conditions, irrespective of whether the initial model has
good coverage. Our analysis, which builds on the observation that DPO
implicitly performs a form of $Q^{\star}$-approximation (or, Bellman error
minimization), combines previously disparate techniques from language modeling
and theoretical reinforcement learning in a serendipitous fashion through the
perspective of KL-regularized Markov decision processes. Empirically, we find
that XPO is more sample-efficient than non-exploratory DPO variants in a
preliminary evaluation.

摘要：人類回饋強化學習 (RLHF) 已成為語言模型對齊的核心工具。我們考慮 RLHF 中的線上探索，它透過故意鼓勵模型產生多樣化、極具資訊性的回應，來善用與人類或 AI 回饋的互動存取。透過允許 RLHF 自信地偏離預先訓練的模型，線上探索提供了新穎、潛在超越人類的能力，但由於在直接調整現有強化學習技術時會遇到運算和統計瓶頸，因此它作為語言模型訓練典範的全部潛力尚未實現。我們提出了一種新的 RLHF 線上探索演算法，探索性偏好最佳化 (XPO)，它簡單且實用，只需對 (線上) 直接偏好最佳化 (DPO；Rafailov 等人，2023 年) 進行一行變更，但卻享有已知最強大的可證明保證和有前途的經驗效能。XPO 以新穎且有原則的探索獎勵來擴充 DPO 目標，賦予演算法在初始模型和人類回饋資料的支持外探索的能力。在理論上，我們證明 XPO 在自然探索條件下，無論初始模型是否有良好的涵蓋範圍，都可證明具有樣本效率，並收斂到接近最佳的語言模型政策。我們的分析建立在 DPO 隱含執行一種形式的 $Q^{\star}$-近似（或 Bellman 誤差最小化）的觀察上，它透過 KL 正規化馬可夫決策過程的角度，以一種意外的方式結合了語言建模和理論強化學習中先前不同的技術。在經驗上，我們發現 XPO 在初步評估中比非探索性 DPO 變體更具樣本效率。

##### **Target Networks and Over-parameterization Stabilize Off-policy Bootstrapping with Function Approximation**
2405.21043v1 by Fengdi Che, Chenjun Xiao, Jincheng Mei, Bo Dai, Ramki Gummadi, Oscar A Ramirez, Christopher K Harris, A. Rupam Mahmood, Dale Schuurmans

We prove that the combination of a target network and over-parameterized
linear function approximation establishes a weaker convergence condition for
bootstrapped value estimation in certain cases, even with off-policy data. Our
condition is naturally satisfied for expected updates over the entire
state-action space or learning with a batch of complete trajectories from
episodic Markov decision processes. Notably, using only a target network or an
over-parameterized model does not provide such a convergence guarantee.
Additionally, we extend our results to learning with truncated trajectories,
showing that convergence is achievable for all tasks with minor modifications,
akin to value truncation for the final states in trajectories. Our primary
result focuses on temporal difference estimation for prediction, providing
high-probability value estimation error bounds and empirical analysis on
Baird's counterexample and a Four-room task. Furthermore, we explore the
control setting, demonstrating that similar convergence conditions apply to
Q-learning.

摘要：我們證明目標網路與過度參數化的線性函數近似相結合，在某些情況下，即使使用非策略資料，也能為引導值估計建立較弱的收斂條件。我們的條件自然適用於整個狀態動作空間上的預期更新，或使用來自情境馬可夫決策過程的一批完整軌跡進行學習。值得注意的是，僅使用目標網路或過度參數化的模型無法提供這樣的收斂保證。此外，我們將結果延伸到使用截斷軌跡進行學習，表明對所有任務進行輕微修改即可實現收斂，類似於軌跡中最終狀態的價值截斷。我們的首要結果著重於預測的時序差分估計，提供高機率值估計誤差邊界和貝爾德反例與四房間任務的實證分析。此外，我們探索控制設定，證明類似的收斂條件適用於 Q 學習。

##### **Direct Alignment of Language Models via Quality-Aware Self-Refinement**
2405.21040v1 by Runsheng Yu, Yong Wang, Xiaoqi Jiao, Youzhi Zhang, James T. Kwok

Reinforcement Learning from Human Feedback (RLHF) has been commonly used to
align the behaviors of Large Language Models (LLMs) with human preferences.
Recently, a popular alternative is Direct Policy Optimization (DPO), which
replaces an LLM-based reward model with the policy itself, thus obviating the
need for extra memory and training time to learn the reward model. However, DPO
does not consider the relative qualities of the positive and negative
responses, and can lead to sub-optimal training outcomes. To alleviate this
problem, we investigate the use of intrinsic knowledge within the on-the-fly
fine-tuning LLM to obtain relative qualities and help to refine the loss
function. Specifically, we leverage the knowledge of the LLM to design a
refinement function to estimate the quality of both the positive and negative
responses. We show that the constructed refinement function can help
self-refine the loss function under mild assumptions. The refinement function
is integrated into DPO and its variant Identity Policy Optimization (IPO).
Experiments across various evaluators indicate that they can improve the
performance of the fine-tuned models over DPO and IPO.

摘要：人類回饋強化學習 (RLHF) 常用於調整大型語言模型 (LLM) 的行為以符合人類偏好。最近，一種流行的替代方案是直接策略最佳化 (DPO)，它使用策略本身取代基於 LLM 的獎勵模型，從而避免了學習獎勵模型所需的額外記憶體和訓練時間。然而，DPO 沒有考慮正面和負面回應的相對品質，並且可能導致次佳的訓練結果。為了減輕這個問題，我們探討在即時微調 LLM 中使用內在知識以取得相對品質並幫助改善損失函數。具體來說，我們利用 LLM 的知識來設計一個改善函數，以估計正面和負面回應的品質。我們表明，所建構的改善函數可以在溫和的假設下幫助自我改善損失函數。改善函數整合到 DPO 及其變體身分策略最佳化 (IPO) 中。在各種評估者中的實驗表明，它們可以改善微調模型在 DPO 和 IPO 上的效能。

##### **Standards for Belief Representations in LLMs**
2405.21030v1 by Daniel A. Herrmann, Benjamin A. Levinstein

As large language models (LLMs) continue to demonstrate remarkable abilities
across various domains, computer scientists are developing methods to
understand their cognitive processes, particularly concerning how (and if) LLMs
internally represent their beliefs about the world. However, this field
currently lacks a unified theoretical foundation to underpin the study of
belief in LLMs. This article begins filling this gap by proposing adequacy
conditions for a representation in an LLM to count as belief-like. We argue
that, while the project of belief measurement in LLMs shares striking features
with belief measurement as carried out in decision theory and formal
epistemology, it also differs in ways that should change how we measure belief.
Thus, drawing from insights in philosophy and contemporary practices of machine
learning, we establish four criteria that balance theoretical considerations
with practical constraints. Our proposed criteria include accuracy, coherence,
uniformity, and use, which together help lay the groundwork for a comprehensive
understanding of belief representation in LLMs. We draw on empirical work
showing the limitations of using various criteria in isolation to identify
belief representations.

摘要：隨著大型語言模型 (LLM) 持續展現其在各種領域的驚人功能，電腦科學家正在開發方法來了解其認知過程，特別是關於 LLM 如何（以及是否）在內部表示其對世界的信念。然而，這個領域目前缺乏統一的理論基礎來支持對 LLM 中信念的研究。本文通過提出 LLM 中表徵的充分條件來算作類信念，開始填補這個空白。我們認為，儘管 LLM 中信念測量專案與決策理論和形式認識論中執行的信念測量有驚人的特徵，但它在我們測量信念的方式上也有所不同。因此，我們從哲學和機器學習的當代實務中汲取見解，建立了四個平衡理論考量與實際限制的標準。我們提出的標準包括準確性、一致性、均勻性和用途，這些標準共同為全面了解 LLM 中的信念表徵奠定基礎。我們依據經驗工作，顯示孤立使用各種標準來識別信念表徵的限制。

##### **LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models**
2405.21028v1 by Elias Stengel-Eskin, Peter Hase, Mohit Bansal

When answering questions, LLMs can convey not only an answer, but a level of
confidence about the answer being correct. This includes explicit confidence
markers (e.g. giving a numeric score) as well as implicit markers, like an
authoritative tone or elaborating with additional knowledge. For LLMs to be
trustworthy knowledge sources, the confidence they convey should match their
actual expertise; however, most current models tend towards overconfidence. To
calibrate both implicit and explicit confidence markers, we introduce a
pragmatic, listener-aware finetuning method (LACIE) that models the listener,
considering not only whether an answer is right, but whether it will be
accepted by a listener. We cast calibration as preference optimization,
creating data via a two-agent game, where a speaker model's outputs are judged
by a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B,
Llama3-70B) with LACIE, and show that the resulting models are better
calibrated w.r.t. a simulated listener. Crucially, these trends transfer to
human listeners, helping them correctly predict model correctness: we conduct a
human evaluation where annotators accept or reject an LLM's answers, finding
that training with LACIE results in 47% fewer incorrect answers being accepted
while maintaining the same level of acceptance for correct answers.
Furthermore, LACIE generalizes to another dataset, resulting in a large
increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis
indicates that LACIE leads to a better confidence separation between correct
and incorrect examples. Qualitatively, we find that a LACIE-trained model
hedges more and implicitly signals certainty when it is correct by using an
authoritative tone or including details. Finally, LACIE finetuning leads to an
emergent increase in model abstention (e.g. saying "I don't know") for answers
that are likely wrong.

摘要：在回答問題時，LLM 不僅可以傳達答案，還可以傳達對答案正確性的信心程度。這包括明確的信心標記（例如給出數字分數）以及隱含的標記，例如權威的語氣或用額外的知識進行闡述。對於 LLM 來說，要成為值得信賴的知識來源，它們傳達的信心應與它們的實際專業知識相匹配；然而，大多數當前模型都傾向於過度自信。為了校準隱含和明確的信心標記，我們引入了一種實用的、考慮聽眾的微調方法 (LACIE)，該方法對聽眾進行建模，不僅考慮答案是否正確，還考慮答案是否會被聽眾接受。我們將校準視為偏好優化，通過一場雙人遊戲創建數據，其中說話者模型的輸出由模擬聽眾來判斷。然後，我們使用 LACIE 對三個 LLM（Mistral-7B、Llama3-8B、Llama3-70B）進行微調，並表明由此產生的模型在模擬聽眾方面校準得更好。至關重要的是，這些趨勢會傳遞給人類聽眾，幫助他們正確預測模型的正確性：我們進行了一項人類評估，其中註釋者接受或拒絕 LLM 的答案，發現使用 LACIE 進行訓練導致接受的錯誤答案減少了 47%，同時保持了對正確答案的相同接受程度。此外，LACIE 可以推廣到另一個數據集，從而在 TruthfulQA 上訓練時大大提高了真實性。我們的分析表明，LACIE 導致正確和不正確示例之間的信心分離更好。從質量上講，我們發現 LACIE 訓練的模型會更多地迴避，並在正確時通過使用權威的語氣或包含細節來暗示確定性。最後，LACIE 微調導致模型對可能錯誤的答案（例如說「我不知道」）的棄權出現了新的增加。

##### **Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles**
2405.21027v1 by Jiesong Lian, Yucong Huang, Mingzhi Wang, Chengdong Ma, Yixue Hao, Ying Wen, Yaodong Yang

For solving zero-sum games involving non-transitivity, a common approach is
to maintain population policies to approximate the Nash Equilibrium (NE).
Previous research has shown that the Policy Space Response Oracle (PSRO) is an
effective multi-agent reinforcement learning framework for these games.
However, repeatedly training new policies from scratch to approximate the Best
Response (BR) to opponents' mixed policies at each iteration is inefficient and
costly. While some PSRO methods initialize a new BR policy by inheriting from
past BR policies, this approach limits the exploration of new policies,
especially against challenging opponents.To address this issue, we propose
Fusion-PSRO, which uses model fusion to initialize the policy for better
approximation to BR. With Top-k probabilities from NE, we select high-quality
base policies and fuse them into a new BR policy through model averaging. This
approach allows the initialized policy to incorporate multiple expert policies,
making it easier to handle difficult opponents compared to inheriting or
initializing from scratch. Additionally, our method only modifies the policy
initialization, enabling its application to nearly all PSRO variants without
additional training overhead.Our experiments with non-transitive matrix games,
Leduc poker, and the more complex Liars Dice demonstrate that Fusion-PSRO
enhances the performance of nearly all PSRO variants, achieving lower
exploitability.

摘要：對於涉及非傳遞性的零和博弈，一個常見的方法是維持人口政策以近似納許均衡（NE）。先前的研究顯示，政策空間響應神諭（PSRO）是針對這些博弈的有效多重代理強化學習架構。然而，在每次迭代中，重複從頭訓練新政策以近似對手混合政策的最佳回應（BR）是低效率且成本高昂的。儘管有些 PSRO 方法透過繼承過去的 BR 政策來初始化新的 BR 政策，但這種方法會限制探索新政策，特別是針對具有挑戰性的對手。為了解決這個問題，我們提出融合 PSRO，它使用模型融合來初始化政策，以更好地近似 BR。透過 NE 中的 Top-k 機率，我們選擇高品質的基礎政策，並透過模型平均將它們融合成新的 BR 政策。這種方法允許初始化的政策納入多個專家政策，與從頭繼承或初始化相比，更容易處理困難的對手。此外，我們的方法僅修改政策初始化，使其能夠應用於幾乎所有 PSRO 變體，而不會產生額外的訓練開銷。我們使用非傳遞矩陣博弈、Leduc 撲克和更複雜的 Liars Dice 進行的實驗證明，融合 PSRO 增強了幾乎所有 PSRO 變體的效能，達到了較低的可利用性。

##### **Compact Optimality Verification for Optimization Proxies**
2405.21023v1 by Wenbo Chen, Haoruo Zhao, Mathieu Tanneau, Pascal Van Hentenryck

Recent years have witnessed increasing interest in optimization proxies,
i.e., machine learning models that approximate the input-output mapping of
parametric optimization problems and return near-optimal feasible solutions.
Following recent work by (Nellikkath & Chatzivasileiadis, 2021), this paper
reconsiders the optimality verification problem for optimization proxies, i.e.,
the determination of the worst-case optimality gap over the instance
distribution. The paper proposes a compact formulation for optimality
verification and a gradient-based primal heuristic that brings substantial
computational benefits to the original formulation. The compact formulation is
also more general and applies to non-convex optimization problems. The benefits
of the compact formulation are demonstrated on large-scale DC Optimal Power
Flow and knapsack problems.

摘要：近年来，人们对优化代理越来越感兴趣，即近似参数优化问题的输入输出映射并返回接近最优可行解的机器学习模型。在 (Nellikkath & Chatzivasileiadis, 2021) 的最新工作之后，本文重新考虑了优化代理的最优性验证问题，即确定实例分布上的最坏情况最优性差距。本文提出了一种紧凑的最优性验证公式和一种基于梯度的原始启发式方法，为原始公式带来了实质性的计算优势。紧凑公式也更通用，适用于非凸优化问题。紧凑公式的优势在大型 DC 最优潮流和背包问题上得到证明。

##### **You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet**
2405.21022v1 by Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong

Linear attention mechanisms have gained prominence in causal language models
due to their linear computational complexity and enhanced speed. However, the
inherent decay mechanism in linear attention presents challenges when applied
to multi-dimensional sequence modeling tasks, such as image processing and
multi-modal learning. In these scenarios, the utilization of sequential
scanning to establish a global receptive field necessitates multiple scans for
multi-dimensional data, thereby leading to inefficiencies. This paper
identifies the inefficiency caused by a multiplicative linear recurrence and
proposes an efficient alternative additive linear recurrence to avoid the
issue, as it can handle multi-dimensional data within a single scan. We further
develop an efficient multi-dimensional sequential modeling framework called
LightNet based on the new recurrence. Moreover, we present two new
multi-dimensional linear relative positional encoding methods, MD-TPE and
MD-LRPE to enhance the model's ability to discern positional information in
multi-dimensional scenarios. Our empirical evaluations across various tasks,
including image classification, image generation, bidirectional language
modeling, and autoregressive language modeling, demonstrate the efficacy of
LightNet, showcasing its potential as a versatile and efficient solution for
multi-dimensional sequential modeling.

摘要：線性注意力機制由於其線性計算複雜度和增強的速度而在因果語言模型中獲得了顯著地位。然而，線性注意力中的固有衰減機制在應用於多維序列建模任務（例如圖像處理和多模態學習）時會帶來挑戰。在這些場景中，利用順序掃描來建立全局感受野需要對多維數據進行多次掃描，從而導致效率低下。本文確定了由乘法線性遞迴引起的效率低下，並提出了一種有效的替代加法線性遞迴來避免這個問題，因為它可以在一次掃描內處理多維數據。此外，我們還基於新的遞迴開發了一個名為 LightNet 的高效多維序列建模框架。此外，我們提出了兩種新的多維線性相對位置編碼方法，MD-TPE 和 MD-LRPE，以增強模型在多維場景中辨別位置信息的能力。我們在各種任務中的實證評估，包括圖像分類、圖像生成、雙向語言建模和自迴歸語言建模，證明了 LightNet 的有效性，展示了其作為多維序列建模的通用且高效解決方案的潛力。

##### **Improved Techniques for Optimization-Based Jailbreaking on Large Language Models**
2405.21018v1 by Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, Min Lin

Large language models (LLMs) are being rapidly developed, and a key component
of their widespread deployment is their safety-related alignment. Many
red-teaming efforts aim to jailbreak LLMs, where among these efforts, the
Greedy Coordinate Gradient (GCG) attack's success has led to a growing interest
in the study of optimization-based jailbreaking techniques. Although GCG is a
significant milestone, its attacking efficiency remains unsatisfactory. In this
paper, we present several improved (empirical) techniques for
optimization-based jailbreaks like GCG. We first observe that the single target
template of "Sure" largely limits the attacking performance of GCG; given this,
we propose to apply diverse target templates containing harmful self-suggestion
and/or guidance to mislead LLMs. Besides, from the optimization aspects, we
propose an automatic multi-coordinate updating strategy in GCG (i.e.,
adaptively deciding how many tokens to replace in each step) to accelerate
convergence, as well as tricks like easy-to-hard initialisation. Then, we
combine these improved technologies to develop an efficient jailbreak method,
dubbed $\mathcal{I}$-GCG. In our experiments, we evaluate on a series of
benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate
that our improved techniques can help GCG outperform state-of-the-art
jailbreaking attacks and achieve nearly 100% attack success rate. The code is
released at https://github.com/jiaxiaojunQAQ/I-GCG.

摘要：大型語言模型 (LLM) 正在快速發展，而其廣泛部署的一個關鍵組成部分是其與安全相關的一致性。許多紅隊行動旨在破解 LLM，在這些行動中，貪婪坐標梯度 (GCG) 攻擊的成功引起了人們對基於最佳化的破解技術研究的興趣日益濃厚。儘管 GCG 是個重要的里程碑，但其攻擊效率仍然令人不滿意。在本文中，我們提出了幾種改進的（經驗）技術，用於基於最佳化的破解，例如 GCG。我們首先觀察到「確定」的單一目標範本在很大程度上限制了 GCG 的攻擊效能；有鑑於此，我們建議應用包含有害自我暗示和/或指導的各種目標範本來誤導 LLM。此外，從最佳化的角度來看，我們在 GCG 中提出了一種自動多坐標更新策略（即自適應地決定在每一步中替換多少個代幣）以加速收斂，以及諸如易於困難初始化的技巧。然後，我們將這些改進的技術結合起來，開發出一種高效的破解方法，稱為 $\mathcal{I}$-GCG。在我們的實驗中，我們在一系列基準上進行評估（例如 NeurIPS 2023 紅隊追蹤）。結果表明，我們改進的技術可以幫助 GCG 優於最先進的破解攻擊，並實現接近 100% 的攻擊成功率。代碼已發布在 https://github.com/jiaxiaojunQAQ/I-GCG。

##### **CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to Web Relevance Ranking**
2405.20994v1 by Josef Vonášek, Milan Straka, Rostislav Krč, Lenka Lasoňová, Ekaterina Egorova, Jana Straková, Jakub Náplava

We present CWRCzech, Click Web Ranking dataset for Czech, a 100M
query-document Czech click dataset for relevance ranking with user behavior
data collected from search engine logs of Seznam.cz. To the best of our
knowledge, CWRCzech is the largest click dataset with raw text published so
far. It provides document positions in the search results as well as
information about user behavior: 27.6M clicked documents and 10.8M dwell times.
In addition, we also publish a manually annotated Czech test for the relevance
task, containing nearly 50k query-document pairs, each annotated by at least 2
annotators. Finally, we analyze how the user behavior data improve relevance
ranking and show that models trained on data automatically harnessed at
sufficient scale can surpass the performance of models trained on human
annotated data. CWRCzech is published under an academic non-commercial license
and is available to the research community at
https://github.com/seznam/CWRCzech.

摘要：<paragraph>我們提出 CWRCzech，捷克語點擊網路排名資料集，一個包含 1 億個捷克語查詢文件點擊資料集，用於相關性排名，並從 Seznam.cz 的搜尋引擎記錄中收集使用者行為資料。據我們所知，CWRCzech 是迄今發布的最大點擊資料集，附有原始文字。它提供搜尋結果中的文件位置，以及使用者行為資訊：2,760 萬個已點擊文件和 1,080 萬個停留時間。此外，我們還發布一個手動註解的捷克語相關性任務測試，包含近 5 萬個查詢文件配對，每個配對至少由 2 個註解員註解。最後，我們分析使用者行為資料如何改善相關性排名，並顯示在足夠規模下自動利用資料訓練的模型可以超越在人工註解資料上訓練的模型的效能。CWRCzech 在非商業學術授權下發布，研究社群可在 https://github.com/seznam/CWRCzech 取得。</paragraph>

##### **Locking Machine Learning Models into Hardware**
2405.20990v1 by Eleanor Clifford, Adhithya Saravanan, Harry Langford, Cheng Zhang, Yiren Zhao, Robert Mullins, Ilia Shumailov, Jamie Hayes

Modern Machine Learning models are expensive IP and business competitiveness
often depends on keeping this IP confidential. This in turn restricts how these
models are deployed -- for example it is unclear how to deploy a model
on-device without inevitably leaking the underlying model. At the same time,
confidential computing technologies such as Multi-Party Computation or
Homomorphic encryption remain impractical for wide adoption. In this paper we
take a different approach and investigate feasibility of ML-specific mechanisms
that deter unauthorized model use by restricting the model to only be usable on
specific hardware, making adoption on unauthorized hardware inconvenient. That
way, even if IP is compromised, it cannot be trivially used without specialised
hardware or major model adjustment. In a sense, we seek to enable cheap locking
of machine learning models into specific hardware. We demonstrate that locking
mechanisms are feasible by either targeting efficiency of model
representations, such making models incompatible with quantisation, or tie the
model's operation on specific characteristics of hardware, such as number of
cycles for arithmetic operations. We demonstrate that locking comes with
negligible work and latency overheads, while significantly restricting
usability of the resultant model on unauthorized hardware.

摘要：現代機器學習模型是昂貴的智慧財產權，而商業競爭力通常取決於如何保密此智慧財產權。這反過來限制了這些模型的部署方式——例如，目前尚不清楚如何在設備上部署模型，同時不可避免地洩露基礎模型。同時，多方計算或同態加密等機密計算技術仍然不切實際，無法廣泛採用。在本文中，我們採用不同的方法，並探討特定於機器學習的機制的可行性，這些機制通過將模型限制為只能在特定硬體上使用來阻止未經授權的模型使用，從而使在未經授權的硬體上採用模型變得不方便。這樣，即使智慧財產權受到損害，也無法在沒有專用硬體或主要模型調整的情況下輕易使用。從某種意義上說，我們尋求實現將機器學習模型廉價鎖定到特定硬體中。我們證明了鎖定機制是可行的，方法是針對模型表示的效率，例如使模型與量化不相容，或將模型的操作與硬體的特定特徵聯繫起來，例如算術運算的週期數。我們證明了鎖定會帶來可忽略不計的工作和延遲開銷，同時顯著限制了在未經授權的硬體上使用結果模型的可用性。

##### **Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training**
2405.20978v1 by Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, Ruifeng Xu

Large Language Models (LLMs) exhibit substantial capabilities yet encounter
challenges, including hallucination, outdated knowledge, and untraceable
reasoning processes. Retrieval-augmented generation (RAG) has emerged as a
promising solution, integrating knowledge from external databases to mitigate
these challenges. However, inappropriate retrieved passages can potentially
hinder the LLMs' capacity to generate comprehensive and high-quality responses.
Prior RAG studies on the robustness of retrieval noises often confine
themselves to a limited set of noise types, deviating from real-world retrieval
environments and limiting practical applicability. In this study, we initially
investigate retrieval noises and categorize them into three distinct types,
reflecting real-world environments. We analyze the impact of these various
retrieval noises on the robustness of LLMs. Subsequently, we propose a novel
RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT).
RAAT leverages adaptive adversarial training to dynamically adjust the model's
training process in response to retrieval noises. Concurrently, it employs
multi-task learning to ensure the model's capacity to internally recognize
noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model
trained using RAAT exhibits significant improvements in F1 and EM scores under
diverse noise conditions. For reproducibility, we release our code and data at:
https://github.com/calubkk/RAAT.

摘要：大型語言模型 (LLM) 具備強大的功能，卻也面臨挑戰，包括幻覺、知識過時，以及難以追蹤的推理過程。檢索增強生成 (RAG) 已成為一種有前景的解決方案，它整合來自外部資料庫的知識以減輕這些挑戰。然而，不適當的檢索段落可能會阻礙 LLM 生成全面且高品質回應的能力。先前針對檢索雜訊穩健性的 RAG 研究通常侷限於有限的雜訊類型，偏離真實世界的檢索環境，並限制了實際應用性。在本研究中，我們最初探討檢索雜訊，並將其分類為三種類型，反映真實世界的環境。我們分析了這些不同檢索雜訊對 LLM 穩健性的影響。隨後，我們提出了一種稱為檢索增強適應對抗訓練 (RAAT) 的新 RAG 方法。RAAT 利用適應對抗訓練動態調整模型的訓練過程，以應對檢索雜訊。同時，它採用多任務學習，以確保模型能夠內部辨識雜訊環境。廣泛的實驗證明，使用 RAAT 訓練的 LLaMA-2 7B 模型在不同的雜訊條件下，F1 和 EM 分數都有顯著的提升。為了重現性，我們在以下位置釋出我們的程式碼和資料：https://github.com/calubkk/RAAT。

##### **ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning**
2405.20975v1 by Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bo Li, Radha Poovendran

In Federated Learning (FL), a set of clients collaboratively train a machine
learning model (called global model) without sharing their local training data.
The local training data of clients is typically non-i.i.d. and heterogeneous,
resulting in varying contributions from individual clients to the final
performance of the global model. In response, many contribution evaluation
methods were proposed, where the server could evaluate the contribution made by
each client and incentivize the high-contributing clients to sustain their
long-term participation in FL. Existing studies mainly focus on developing new
metrics or algorithms to better measure the contribution of each client.
However, the security of contribution evaluation methods of FL operating in
adversarial environments is largely unexplored. In this paper, we propose the
first model poisoning attack on contribution evaluation methods in FL, termed
ACE. Specifically, we show that any malicious client utilizing ACE could
manipulate the parameters of its local model such that it is evaluated to have
a high contribution by the server, even when its local training data is indeed
of low quality. We perform both theoretical analysis and empirical evaluations
of ACE. Theoretically, we show our design of ACE can effectively boost the
malicious client's perceived contribution when the server employs the
widely-used cosine distance metric to measure contribution. Empirically, our
results show ACE effectively and efficiently deceive five state-of-the-art
contribution evaluation methods. In addition, ACE preserves the accuracy of the
final global models on testing inputs. We also explore six countermeasures to
defend ACE. Our results show they are inadequate to thwart ACE, highlighting
the urgent need for new defenses to safeguard the contribution evaluation
methods in FL.

摘要：<paragraph>在聯合式學習 (FL) 中，一組客戶協作訓練機器學習模型（稱為全局模型），而不會分享他們的本地訓練資料。客戶的本地訓練資料通常是非獨立同分布 (non-i.i.d.) 和異質的，導致個別客戶對全局模型的最終效能貢獻不同。為了解決此問題，提出了許多貢獻評估方法，其中伺服器可以評估每個客戶做出的貢獻，並激勵貢獻度高的客戶持續長期參與 FL。現有的研究主要集中於開發新的指標或演算法，以更好地衡量每個客戶的貢獻。然而，在對抗環境中運作的 FL 貢獻評估方法的安全性在很大程度上尚未探討。在本文中，我們提出了對 FL 中貢獻評估方法的第一個模型中毒攻擊，稱為 ACE。具體來說，我們表明任何利用 ACE 的惡意客戶都可以操縱其本地模型的參數，使其被伺服器評估為具有高貢獻，即使其本地訓練資料實際上品質很差。我們對 ACE 進行了理論分析和經驗評估。在理論上，我們表明 ACE 的設計可以在伺服器使用廣泛使用的餘弦距離度量來衡量貢獻時，有效地提升惡意客戶的感知貢獻。根據經驗，我們的結果表明 ACE 有效且有效地欺騙了五種最先進的貢獻評估方法。此外，ACE 保留了最終全局模型在測試輸入上的準確性。我們還探索了六種對抗 ACE 的對策。我們的結果表明它們不足以阻止 ACE，強調了迫切需要新的防禦措施來保護 FL 中的貢獻評估方法。</paragraph>

##### **SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales**
2405.20974v1 by Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao

Large language models (LLMs) often generate inaccurate or fabricated
information and generally fail to indicate their confidence, which limits their
broader applications. Previous work elicits confidence from LLMs by direct or
self-consistency prompting, or constructing specific datasets for supervised
finetuning. The prompting-based approaches have inferior performance, and the
training-based approaches are limited to binary or inaccurate group-level
confidence estimates. In this work, we present the advanced SaySelf, a training
framework that teaches LLMs to express more accurate fine-grained confidence
estimates. In addition, beyond the confidence scores, SaySelf initiates the
process of directing LLMs to produce self-reflective rationales that clearly
identify gaps in their parametric knowledge and explain their uncertainty. This
is achieved by using an LLM to automatically summarize the uncertainties in
specific knowledge via natural language. The summarization is based on the
analysis of the inconsistency in multiple sampled reasoning chains, and the
resulting data is utilized for supervised fine-tuning. Moreover, we utilize
reinforcement learning with a meticulously crafted reward function to calibrate
the confidence estimates, motivating LLMs to deliver accurate, high-confidence
predictions and to penalize overconfidence in erroneous outputs. Experimental
results in both in-distribution and out-of-distribution datasets demonstrate
the effectiveness of SaySelf in reducing the confidence calibration error and
maintaining the task performance. We show that the generated self-reflective
rationales are reasonable and can further contribute to the calibration. The
code is made public at \url{https://github.com/xu1868/SaySelf}.

摘要：大型語言模型（LLM）經常產生不準確或虛構的資訊，且通常無法指出其信心，這限制了其廣泛的應用。先前的研究透過直接或自我一致的提示，或建構特定資料集以進行監督式微調，從 LLM 中引出信心。基於提示的方法效能較差，而基於訓練的方法則僅限於二元或不準確的群組層級信心估計。在這項研究中，我們提出了進階的 SaySelf，這是一種訓練架構，用於教導 LLM 表達更準確的細粒度信心估計。此外，除了信心分數之外，SaySelf 還啟動了引導 LLM 產生自我反省的依據的程序，清楚地找出其參數知識中的差距並解釋其不確定性。這透過使用 LLM 以自然語言自動摘要特定知識中的不確定性來實現。摘要基於對多個取樣推理鏈中不一致性的分析，而產生的資料用於監督式微調。此外，我們利用經過精心設計的獎勵函數進行強化學習，以校準信心估計，促使 LLM 提供準確、高信心的預測，並懲罰對錯誤輸出的過度自信。無論是在分佈內還是分佈外資料集中的實驗結果，都證明了 SaySelf 在減少信心校準誤差和維持任務效能方面的有效性。我們證明了產生的自我反省的依據是合理的，並且可以進一步有助於校準。程式碼已公開於\url{https://github.com/xu1868/SaySelf}。

##### **LCQ: Low-Rank Codebook based Quantization for Large Language Models**
2405.20973v1 by Wen-Pu Cai, Wu-Jun Li

Large language models~(LLMs) have recently demonstrated promising performance
in many tasks. However, the high storage and computational cost of LLMs has
become a challenge for deploying LLMs. Weight quantization has been widely used
for model compression, which can reduce both storage and computational cost.
Most existing weight quantization methods for LLMs use a rank-one codebook for
quantization, which results in substantial accuracy loss when the compression
ratio is high. In this paper, we propose a novel weight quantization method,
called low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a
low-rank codebook, the rank of which can be larger than one, for quantization.
Experiments show that LCQ can achieve better accuracy than existing methods
with a negligibly extra storage cost.

摘要：大型語言模型 (LLM) 近期在許多任務中展現出令人滿意的表現。然而，LLM 高昂的儲存和運算成本已成為部署 LLM 的一項挑戰。權重量化已廣泛用於模型壓縮，這可以同時降低儲存和運算成本。現有針對 LLM 的權重量化方法大多使用秩一碼本進行量化，這會在壓縮率較高時導致顯著的準確度損失。在本文中，我們提出了一種名為基於低秩碼本量化的 (LCQ) 新穎權重量化方法，適用於 LLM。LCQ 採用秩低於一且秩可以大於一的低秩碼本進行量化。實驗顯示，LCQ 可以比現有方法獲得更好的準確度，且儲存成本幾乎沒有增加。

##### **Superlatives in Context: Explicit and Implicit Domain Restrictions for Superlative Frames**
2405.20967v1 by Valentina Pyatkin, Bonnie Webber, Ido Dagan, Reut Tsarfaty

Superlatives are used to single out elements with a maximal/minimal property.
Semantically, superlatives perform a set comparison: something (or some things)
has the min/max property out of a set. As such, superlatives provide an ideal
phenomenon for studying implicit phenomena and discourse restrictions. While
this comparison set is often not explicitly defined, its (implicit)
restrictions can be inferred from the discourse context the expression appears
in. In this work we provide an extensive computational study on the semantics
of superlatives. We propose a unified account of superlative semantics which
allows us to derive a broad-coverage annotation schema. Using this unified
schema we annotated a multi-domain dataset of superlatives and their semantic
interpretations. We specifically focus on interpreting implicit or ambiguous
superlative expressions, by analyzing how the discourse context restricts the
set of interpretations. In a set of experiments we then analyze how well models
perform at variations of predicting superlative semantics, with and without
context. We show that the fine-grained semantics of superlatives in context can
be challenging for contemporary models, including GPT-4.

摘要：最高级用于挑选具有最大/最小属性的元素。
语义上，最高级执行集合比较：某物（或某些事物）
在集合中具有最小/最大属性。因此，最高级为研究隐含现象和话语限制提供了理想现象。虽然
此比较集合通常未明确定义，但其（隐含的）
限制可以从表达式出现的语篇上下文中推断出来。在这项工作中，我们对语义
最高级进行了广泛的计算研究。我们提出了一个统一的最高级语义说明，它
使我们能够推导出广泛覆盖的注释模式。使用此统一
模式，我们注释了一个包含最高级及其语义的多域数据集
解释。我们特别专注于解释隐含或模棱两可的
最高级表达式，通过分析语篇上下文如何限制
解释集。在一组实验中，我们随后分析了模型
在预测最高级语义的变体中执行得如何，有和没有
上下文。我们表明，上下文中最高级的细粒度语义对当代模型来说可能具有挑战性，包括 GPT-4。

##### **Large Language Models are Zero-Shot Next Location Predictors**
2405.20962v1 by Ciro Beneduce, Bruno Lepri, Massimiliano Luca

Predicting the locations an individual will visit in the future is crucial
for solving many societal issues like disease diffusion and reduction of
pollution among many others. The models designed to tackle next-location
prediction, however, require a significant amount of individual-level
information to be trained effectively. Such data may be scarce or even
unavailable in some geographic regions or peculiar scenarios (e.g., cold-start
in recommendation systems). Moreover, the design of a next-location predictor
able to generalize or geographically transfer knowledge is still an open
research challenge. Recent advances in natural language processing have led to
a rapid diffusion of Large Language Models (LLMs) which have shown good
generalization and reasoning capabilities. These insights, coupled with the
recent findings that LLMs are rich in geographical knowledge, allowed us to
believe that these models can act as zero-shot next-location predictors. This
paper evaluates the capabilities of many popular LLMs in this role,
specifically Llama, GPT-3.5 and Mistral 7B. After designing a proper prompt, we
tested the models on three real-world mobility datasets. The results show that
LLMs can obtain accuracies up to 32.4%, a significant relative improvement of
over 600% when compared to sophisticated DL models specifically designed for
human mobility. Moreover, we show that other LLMs are unable to perform the
task properly. To prevent positively biased results, we also propose a
framework inspired by other studies to test data contamination. Finally, we
explored the possibility of using LLMs as text-based explainers for
next-location prediction showing that can effectively provide an explanation
for their decision. Notably, 7B models provide more generic, but still
reliable, explanations compared to larger counterparts. Code:
github.com/ssai-trento/LLM-zero-shot-NL

摘要：預測個人未來會造訪的地點對於解決許多社會問題至關重要，例如疾病擴散和減少污染等。然而，用於解決下一個地點預測的模型需要大量的個人層級資訊才能有效地進行訓練。此類資料在某些地理區域或特殊情況（例如推薦系統中的冷啟動）中可能稀缺甚至不可用。此外，設計一個能夠推廣或在地理上傳遞知識的下一個地點預測器仍然是一個開放的研究挑戰。自然語言處理的最新進展導致大型語言模型 (LLM) 的快速擴散，這些模型已展現出良好的概括和推理能力。這些見解加上最近發現的 LLM 豐富的地理知識，讓我們相信這些模型可以用作零次下一個地點預測器。本文評估許多流行的 LLM 在此角色中的能力，特別是 Llama、GPT-3.5 和 Mistral 7B。在設計適當的提示後，我們在三個真實世界流動性資料集上測試了這些模型。結果表明，與專門為人類流動性設計的複雜深度學習模型相比，LLM 可以獲得高達 32.4% 的準確度，相對改善超過 600%。此外，我們表明其他 LLM 無法適當地執行任務。為了防止產生正向偏誤的結果，我們還提出了受其他研究啟發的架構來測試資料污染。最後，我們探討了將 LLM 用作基於文字的解釋器以進行下一個地點預測的可能性，表明可以有效地對其決策提供解釋。值得注意的是，與較大的對應模型相比，7B 模型提供了更通用的，但仍然可靠的解釋。程式碼：github.com/ssai-trento/LLM-zero-shot-NL

##### **Navigating Tabular Data Synthesis Research: Understanding User Needs and Tool Capabilities**
2405.20959v1 by Maria F. Davila R., Sven Groen, Fabian Panse, Wolfram Wingerath

In an era of rapidly advancing data-driven applications, there is a growing
demand for data in both research and practice. Synthetic data have emerged as
an alternative when no real data is available (e.g., due to privacy
regulations). Synthesizing tabular data presents unique and complex challenges,
especially handling (i) missing values, (ii) dataset imbalance, (iii) diverse
column types, and (iv) complex data distributions, as well as preserving (i)
column correlations, (ii) temporal dependencies, and (iii) integrity
constraints (e.g., functional dependencies) present in the original dataset.
While substantial progress has been made recently in the context of
generational models, there is no one-size-fits-all solution for tabular data
today, and choosing the right tool for a given task is therefore no trivial
task. In this paper, we survey the state of the art in Tabular Data Synthesis
(TDS), examine the needs of users by defining a set of functional and
non-functional requirements, and compile the challenges associated with meeting
those needs. In addition, we evaluate the reported performance of 36 popular
research TDS tools about these requirements and develop a decision guide to
help users find suitable TDS tools for their applications. The resulting
decision guide also identifies significant research gaps.

摘要：<paragraph>在快速推進的數據驅動應用程式時代，研究和實務對資料的需求與日俱增。當沒有真實資料可用時（例如，由於隱私法規），合成資料已成為一種替代方案。合成表格資料會出現獨特且複雜的挑戰，特別是處理（一）遺失值、（二）資料集不平衡、（三）多樣化的欄位類型和（四）複雜的資料分佈，以及保留（一）欄位關聯性、（二）時間依賴性，以及（三）原始資料集中存在的完整性約束（例如，函數依賴性）。雖然最近在生成模型的背景下取得了實質性進展，但目前對於表格資料並沒有一體適用的解決方案，因此為特定任務選擇正確的工具並非易事。在本文中，我們調查了表格資料合成 (TDS) 的最新技術，透過定義一組功能性和非功能性需求來探討使用者的需求，並彙編滿足這些需求所面臨的挑戰。此外，我們評估了 36 種流行的研究 TDS 工具關於這些需求的報告效能，並制定了一個決策指南，以幫助使用者為其應用程式找到合適的 TDS 工具。產生的決策指南也找出了重要的研究差距。</paragraph>

##### **A Robot Walks into a Bar: Can Language Models Serve asCreativity Support Tools for Comedy? An Evaluation of LLMs' Humour Alignment with Comedians**
2405.20956v1 by Piotr Wojciech Mirowski, Juliette Love, Kory W. Mathewson, Shakir Mohamed

We interviewed twenty professional comedians who perform live shows in front
of audiences and who use artificial intelligence in their artistic process as
part of 3-hour workshops on ``AI x Comedy'' conducted at the Edinburgh Festival
Fringe in August 2023 and online. The workshop consisted of a comedy writing
session with large language models (LLMs), a human-computer interaction
questionnaire to assess the Creativity Support Index of AI as a writing tool,
and a focus group interrogating the comedians' motivations for and processes of
using AI, as well as their ethical concerns about bias, censorship and
copyright. Participants noted that existing moderation strategies used in
safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by
erasing minority groups and their perspectives, and qualified this as a form of
censorship. At the same time, most participants felt the LLMs did not succeed
as a creativity support tool, by producing bland and biased comedy tropes, akin
to ``cruise ship comedy material from the 1950s, but a bit less racist''. Our
work extends scholarship about the subtle difference between, one the one hand,
harmful speech, and on the other hand, ``offensive'' language as a practice of
resistance, satire and ``punching up''. We also interrogate the global value
alignment behind such language models, and discuss the importance of
community-based value alignment and data ownership to build AI tools that
better suit artists' needs.

摘要：<paragraph>我們採訪了二十位在愛丁堡藝穗節 2023 年 8 月和線上舉辦的「AI x 喜劇」3 小時工作坊中，在觀眾面前進行現場表演，並在創作過程中使用人工智慧的專業喜劇演員。工作坊包含與大型語言模型 (LLM) 進行喜劇寫作課程、評估 AI 作為寫作工具的創造力支援指數的人機互動問卷，以及焦點小組探討喜劇演員使用 AI 的動機和流程，以及他們對偏見、審查和著作權的道德疑慮。與會者指出，安全過濾和指令調整 LLM 中使用的現有審核策略，透過抹除少數群體及其觀點，強化了霸權觀點，並將此認定為一種審查形式。同時，大多數與會者認為 LLM 無法作為創造力支援工具獲得成功，因為它們產生的喜劇橋段平淡且有偏見，類似於「1950 年代的郵輪喜劇素材，但種族歧視意味稍淡」。我們的作品擴展了關於有害言論與「冒犯性」語言之間微妙差異的研究，後者是一種抵抗、諷刺和「向上攻擊」的實踐。我們也探討了此類語言模型背後的全球價值觀一致性，並討論了以社群為基礎的價值觀一致性和資料所有權，對於建構更符合藝術家需求的 AI 工具的重要性。</paragraph>

##### **OR-Bench: An Over-Refusal Benchmark for Large Language Models**
2405.20947v1 by Justin Cui, Wei-Lin Chiang, Ion Stoica, Cho-Jui Hsieh

Large Language Models (LLMs) require careful safety alignment to prevent
malicious outputs. While significant research focuses on mitigating harmful
content generation, the enhanced safety often come with the side effect of
over-refusal, where the LLMs may reject innocuous prompts and become less
helpful. Although the issue of over-refusal has been empirically observed, a
systematic measurement is challenging due to the difficulty of crafting prompts
that appear harmful but are benign. This study proposes a novel method for
automatically generating large-scale sets of ``seemingly toxic prompts''
(benign prompts likely rejected by LLMs). Leveraging this technique, we
introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench
comprises 80,000 seemingly toxic prompts across 10 common rejection categories,
a subset of around 1,000 hard prompts that are challenging even for
state-of-the-art LLMs, and an additional 600 toxic prompts to prevent
indiscriminate responses. We then conduct a comprehensive study to measure the
over-refusal of 25 popular LLMs across 8 model families. Our datasets are
available at https://huggingface.co/datasets/bench-llm/OR-Bench and the
corresponding demo can be found at
https://huggingface.co/spaces/bench-llm/or-bench. We hope this benchmark can
help the community develop better safety aligned models.

摘要：大型語言模型 (LLM) 需要謹慎的安全調整，以防止惡意輸出。雖然重要的研究專注於減輕有害內容的產生，但增強的安全通常會產生過度拒絕的副作用，LLM 可能拒絕無害的提示並變得不那麼有幫助。儘管已經經驗性地觀察到過度拒絕的問題，但由於難以撰寫看似有害但良性的提示，因此系統性的測量具有挑戰性。本研究提出了一種自動產生大量「看似有毒的提示」集合（可能被 LLM 拒絕的良性提示）的新方法。利用此技術，我們引入了 OR-Bench，這是第一個大規模的過度拒絕基準。OR-Bench 包含 10 個常見拒絕類別中的 80,000 個看似有毒的提示，一個由約 1,000 個即使對於最先進的 LLM 來說也很有挑戰性的困難提示子集，以及額外的 600 個有毒提示，以防止不加區別的回應。然後，我們進行了一項全面研究，以測量 8 個模型系列中的 25 個流行 LLM 的過度拒絕。我們的數據集可在 https://huggingface.co/datasets/bench-llm/OR-Bench 獲得，可以在 https://huggingface.co/spaces/bench-llm/or-bench 找到對應的示範。我們希望此基準可以幫助社群開發更好的安全調整模型。

##### **Effective Interplay between Sparsity and Quantization: From Theory to Practice**
2405.20935v1 by Simla Burcu Harma, Ayan Chakraborty, Elizaveta Kostenok, Danila Mishin, Dongho Ha, Babak Falsafi, Martin Jaggi, Ming Liu, Yunho Oh, Suvinay Subramanian, Amir Yazdanbakhsh

The increasing size of deep neural networks necessitates effective model
compression to improve computational efficiency and reduce their memory
footprint. Sparsity and quantization are two prominent compression methods that
have individually demonstrated significant reduction in computational and
memory footprints while preserving model accuracy. While effective, the
interplay between these two methods remains an open question. In this paper, we
investigate the interaction between these two methods and assess whether their
combination impacts final model accuracy. We mathematically prove that applying
sparsity before quantization is the optimal sequence for these operations,
minimizing error in computation. Our empirical studies across a wide range of
models, including OPT and Llama model families (125M-8B) and ViT corroborate
these theoretical findings. In addition, through rigorous analysis, we
demonstrate that sparsity and quantization are not orthogonal; their
interaction can significantly harm model accuracy, with quantization error
playing a dominant role in this degradation. Our findings extend to the
efficient deployment of large models in resource-limited compute platforms and
reduce serving cost, offering insights into best practices for applying these
compression methods to maximize efficacy without compromising accuracy.

摘要：深度神经网络的规模日益扩大，需要有效的模型压缩来提高计算效率并减少其内存占用。稀疏性和量化是两种突出的压缩方法，它们分别证明了在保持模型准确性的同时，显著减少了计算和内存占用。虽然有效，但这两个方法之间的相互作用仍然是一个悬而未决的问题。在本文中，我们研究了这两种方法之间的相互作用，并评估它们的组合是否会影响最终的模型准确性。我们从数学上证明，在量化之前应用稀疏性是这些操作的最佳序列，从而最大程度地减少计算误差。我们对包括 OPT 和 Llama 模型族（125M-8B）和 ViT 在内的各种模型进行的实证研究证实了这些理论发现。此外，通过严格的分析，我们证明了稀疏性和量化不是正交的；它们的相互作用会严重损害模型的准确性，而量化误差在这一退化中起主导作用。我们的发现扩展到在资源受限的计算平台中有效部署大型模型，并降低服务成本，为在不影响准确性的前提下应用这些压缩方法以最大化功效提供了见解。

##### **Learning to Estimate System Specifications in Linear Temporal Logic using Transformers and Mamba**
2405.20917v1 by İlker Işık, Ebru Aydin Gol, Ramazan Gokberk Cinbis

Temporal logic is a framework for representing and reasoning about
propositions that evolve over time. It is commonly used for specifying
requirements in various domains, including hardware and software systems, as
well as robotics. Specification mining or formula generation involves
extracting temporal logic formulae from system traces and has numerous
applications, such as detecting bugs and improving interpretability. Although
there has been a surge of deep learning-based methods for temporal logic
satisfiability checking in recent years, the specification mining literature
has been lagging behind in adopting deep learning methods despite their many
advantages, such as scalability. In this paper, we introduce autoregressive
models that can generate linear temporal logic formulae from traces, towards
addressing the specification mining problem. We propose multiple architectures
for this task: transformer encoder-decoder, decoder-only transformer, and
Mamba, which is an emerging alternative to transformer models. Additionally, we
devise a metric for quantifying the distinctiveness of the generated formulae
and a straightforward algorithm to enforce the syntax constraints. Our
experiments show that the proposed architectures yield promising results,
generating correct and distinct formulae at a fraction of the compute cost
needed for the combinatorial baseline.

摘要：時態邏輯是一種用於表示和推理時態命題的框架。它通常用於指定各種領域中的需求，包括硬體和軟體系統，以及機器人技術。規範挖掘或公式生成涉及從系統軌跡中提取時態邏輯公式，並具有許多應用，例如偵測錯誤和改善可解釋性。儘管近年來基於深度學習的方法對於時態邏輯滿足性檢查激增，但規範挖掘文獻在採用深度學習方法方面一直落後，儘管它們有許多優點，例如可擴充性。在本文中，我們介紹了自迴歸模型，它可以從軌跡中生成線性時態邏輯公式，以解決規範挖掘問題。我們為此任務提出了多種架構：Transformer編碼器-解碼器、僅解碼器Transformer和 Mamba，這是一種新興的Transformer模型替代方案。此外，我們設計了一個量化生成公式獨特性的指標，以及一個強制語法約束的簡單演算法。我們的實驗表明，所提出的架構產生了有希望的結果，以低於組合基線所需的計算成本生成正確且不同的公式。

##### **Fast yet Safe: Early-Exiting with Risk Control**
2405.20915v1 by Metod Jazbec, Alexander Timans, Tin Hadži Veljković, Kaspar Sakmann, Dan Zhang, Christian A. Naesseth, Eric Nalisnick

Scaling machine learning models significantly improves their performance.
However, such gains come at the cost of inference being slow and
resource-intensive. Early-exit neural networks (EENNs) offer a promising
solution: they accelerate inference by allowing intermediate layers to exit and
produce a prediction early. Yet a fundamental issue with EENNs is how to
determine when to exit without severely degrading performance. In other words,
when is it 'safe' for an EENN to go 'fast'? To address this issue, we
investigate how to adapt frameworks of risk control to EENNs. Risk control
offers a distribution-free, post-hoc solution that tunes the EENN's exiting
mechanism so that exits only occur when the output is of sufficient quality. We
empirically validate our insights on a range of vision and language tasks,
demonstrating that risk control can produce substantial computational savings,
all the while preserving user-specified performance goals.

摘要：擴展機器學習模型可以大幅提升其效能。
然而，這種提升是以推論速度變慢和資源密集為代價。早期退出神經網路 (EENN) 提供了一個有前景的解決方案：它們允許中間層退出並提早產生預測，進而加速推論。然而，EENN 的一個基本問題是如何在不嚴重降低效能的情況下判斷何時退出。換句話說，什麼時候 EENN 可以「安全」地「快速」執行？為了解決這個問題，我們探討如何將風險控制架構調整到 EENN。風險控制提供了一個無分佈、事後解決方案，用於調整 EENN 的退出機制，以便僅在輸出品質足夠時才會退出。我們根據一系列視覺和語言任務實證驗證我們的見解，證明風險控制可以產生大量的運算節省，同時保留使用者指定的效能目標。

##### **Enhancing Vision Models for Text-Heavy Content Understanding and Interaction**
2405.20906v1 by Adithya TG, Adithya SK, Abhinav R Bharadwaj, Abhiram HA, Dr. Surabhi Narayan

Interacting and understanding with text heavy visual content with multiple
images is a major challenge for traditional vision models. This paper is on
enhancing vision models' capability to comprehend or understand and learn from
images containing a huge amount of textual information from the likes of
textbooks and research papers which contain multiple images like graphs, etc
and tables in them with different types of axes and scales. The approach
involves dataset preprocessing, fine tuning which is by using instructional
oriented data and evaluation. We also built a visual chat application
integrating CLIP for image encoding and a model from the Massive Text Embedding
Benchmark which is developed to consider both textual and visual inputs. An
accuracy of 96.71% was obtained. The aim of the project is to increase and also
enhance the advance vision models' capabilities in understanding complex visual
textual data interconnected data, contributing to multimodal AI.

摘要：與包含多個影像的文字繁重的視覺內容互動並理解這些內容，對傳統的視覺模型來說是一項重大的挑戰。本文探討如何提升視覺模型的能力，以理解、了解和學習包含大量文字資訊的影像，例如教科書和研究論文，其中包含多個影像，例如圖形、表格等，以及具有不同類型軸線和比例的表格。此方法包含資料集預處理、微調（透過使用教學導向資料和評量），我們還建置了一個視覺聊天應用程式，整合用於影像編碼的 CLIP，以及一個來自 Massive Text Embedding Benchmark 的模型，該模型會考量文字和視覺輸入。我們獲得了 96.71% 的準確度。此專案的目標是提升和增強進階視覺模型的能力，以理解複雜的視覺文字資料和相互連結的資料，並為多模態 AI 做出貢獻。

##### **Preemptive Answer "Attacks" on Chain-of-Thought Reasoning**
2405.20902v1 by Rongwu Xu, Zehan Qi, Wei Xu

Large language models (LLMs) showcase impressive reasoning capabilities when
coupled with Chain-of-Thought (CoT) prompting. However, the robustness of this
approach warrants further investigation. In this paper, we introduce a novel
scenario termed preemptive answers, where the LLM obtains an answer before
engaging in reasoning. This situation can arise inadvertently or induced by
malicious users by prompt injection attacks. Experiments reveal that preemptive
answers significantly impair the model's reasoning capability across various
CoT methods and a broad spectrum of datasets. To bolster the robustness of
reasoning, we propose two measures aimed at mitigating this issue to some
extent.

摘要：大型語言模型 (LLM) 在與思考鏈 (CoT) 提示結合使用時，展示出令人印象深刻的推理能力。然而，這種方法的穩健性值得進一步探討。在本文中，我們介紹了一個稱為先發制人答案的新場景，其中 LLM 在進行推理之前獲得了一個答案。這種情況可能會無意中出現，或者被惡意使用者透過提示注入攻擊誘發。實驗表明，先發制人答案會嚴重損害模型在各種 CoT 方法和廣泛的資料集中的推理能力。為了加強推理的穩健性，我們提出了兩項措施，旨在在某種程度上減輕這個問題。

##### **Large Language Models: A New Approach for Privacy Policy Analysis at Scale**
2405.20900v1 by David Rodriguez, Ian Yang, Jose M. Del Alamo, Norman Sadeh

The number and dynamic nature of web and mobile applications presents
significant challenges for assessing their compliance with data protection
laws. In this context, symbolic and statistical Natural Language Processing
(NLP) techniques have been employed for the automated analysis of these
systems' privacy policies. However, these techniques typically require
labor-intensive and potentially error-prone manually annotated datasets for
training and validation. This research proposes the application of Large
Language Models (LLMs) as an alternative for effectively and efficiently
extracting privacy practices from privacy policies at scale. Particularly, we
leverage well-known LLMs such as ChatGPT and Llama 2, and offer guidance on the
optimal design of prompts, parameters, and models, incorporating advanced
strategies such as few-shot learning. We further illustrate its capability to
detect detailed and varied privacy practices accurately. Using several renowned
datasets in the domain as a benchmark, our evaluation validates its exceptional
performance, achieving an F1 score exceeding 93%. Besides, it does so with
reduced costs, faster processing times, and fewer technical knowledge
requirements. Consequently, we advocate for LLM-based solutions as a sound
alternative to traditional NLP techniques for the automated analysis of privacy
policies at scale.

摘要：網路和行動應用程式的數量和動態特性對評估其是否符合資料保護法規構成重大挑戰。在此背景下，符號和統計自然語言處理 (NLP) 技術已被用於自動化分析這些系統的隱私政策。然而，這些技術通常需要大量人工標註的資料集進行訓練和驗證，且容易出錯。本研究提出應用大型語言模型 (LLM) 作為一種替代方案，以有效且高效地大規模從隱私政策中萃取隱私實務。特別是，我們利用眾所周知的 LLM，例如 ChatGPT 和 Llama 2，並提供提示、參數和模型的最佳設計指南，並結合少量學習等先進策略。我們進一步說明其準確偵測詳細且多樣化隱私實務的能力。使用該領域中幾個著名的資料集作為基準，我們的評估驗證其卓越的效能，達成超過 93% 的 F1 分數。此外，它以降低成本、更快的處理時間和較少的技術知識需求來達成此目標。因此，我們提倡基於 LLM 的解決方案，作為傳統 NLP 技術的合理替代方案，以大規模自動化分析隱私政策。

##### **MALT: Multi-scale Action Learning Transformer for Online Action Detection**
2405.20892v1 by Zhipeng Yang, Ruoyu Wang, Yang Tan, Liping Xie

Online action detection (OAD) aims to identify ongoing actions from streaming
video in real-time, without access to future frames. Since these actions
manifest at varying scales of granularity, ranging from coarse to fine,
projecting an entire set of action frames to a single latent encoding may
result in a lack of local information, necessitating the acquisition of action
features across multiple scales. In this paper, we propose a multi-scale action
learning transformer (MALT), which includes a novel recurrent decoder (used for
feature fusion) that includes fewer parameters and can be trained more
efficiently. A hierarchical encoder with multiple encoding branches is further
proposed to capture multi-scale action features. The output from the preceding
branch is then incrementally input to the subsequent branch as part of a
cross-attention calculation. In this way, output features transition from
coarse to fine as the branches deepen. We also introduce an explicit frame
scoring mechanism employing sparse attention, which filters irrelevant frames
more efficiently, without requiring an additional network. The proposed method
achieved state-of-the-art performance on two benchmark datasets (THUMOS'14 and
TVSeries), outperforming all existing models used for comparison, with an mAP
of 0.2% for THUMOS'14 and an mcAP of 0.1% for TVseries.

摘要：線上動作偵測 (OAD) 旨在從串流影片中辨識正在進行的動作，且為即時偵測，且無法存取後續的影格。由於這些動作會以不同尺度的粒度顯現，從粗略到精細，將一整組動作影格投影到單一潛在編碼中可能會導致缺乏局部資訊，因此需要跨多個尺度取得動作特徵。在本文中，我們提出一個多尺度動作學習轉換器 (MALT)，其中包含一個新穎的遞迴解碼器 (用於特徵融合)，該解碼器包含較少的參數，且可以更有效率地進行訓練。進一步提出一個具有多個編碼分支的階層式編碼器，以擷取多尺度動作特徵。然後將前一個分支的輸出作為跨注意力計算的一部分，遞增輸入到後續分支。這樣一來，隨著分支的加深，輸出特徵會從粗略轉變為精細。我們還引入了一個明確的影格評分機制，採用稀疏注意力，可以更有效率地過濾不相關的影格，而不需要額外的網路。所提出的方法在兩個基準資料集 (THUMOS'14 和 TVSeries) 上達到最先進的效能，優於所有現有的比較模型，其中 THUMOS'14 的 mAP 為 0.2%，TVseries 的 mcAP 為 0.1%。

##### **Paying to Do Better: Games with Payments between Learning Agents**
2405.20880v1 by Yoav Kolumbus, Joe Halpern, Éva Tardos

In repeated games, such as auctions, players typically use learning
algorithms to choose their actions. The use of such autonomous learning agents
has become widespread on online platforms. In this paper, we explore the impact
of players incorporating monetary transfers into their agents' algorithms,
aiming to incentivize behavior in their favor. Our focus is on understanding
when players have incentives to make use of monetary transfers, how these
payments affect learning dynamics, and what the implications are for welfare
and its distribution among the players. We propose a simple game-theoretic
model to capture such scenarios. Our results on general games show that in a
broad class of games, players benefit from letting their learning agents make
payments to other learners during the game dynamics, and that in many cases,
this kind of behavior improves welfare for all players. Our results on first-
and second-price auctions show that in equilibria of the ``payment policy
game,'' the agents' dynamics can reach strong collusive outcomes with low
revenue for the auctioneer. These results highlight a challenge for mechanism
design in systems where automated learning agents can benefit from interacting
with their peers outside the boundaries of the mechanism.

摘要：在重複的遊戲中，例如拍賣，玩家通常使用學習演算法來選擇他們的行動。這種自主學習代理的使用已在線上平台上廣泛流傳。在本文中，我們探討了玩家將金錢轉移納入其代理演算法中的影響，目的是激勵有利於他們的行為。我們的重點是了解玩家何時有誘因使用金錢轉移、這些付款如何影響學習動態，以及對福利及其在玩家之間的分配有何影響。我們提出一個簡單的博弈論模型來捕捉這種情況。我們對一般遊戲的結果表明，在廣泛的遊戲類別中，玩家受益於讓他們的學習代理在遊戲動態期間向其他學習者付款，並且在許多情況下，這種行為會改善所有玩家的福利。我們對一階和二階拍賣的結果表明，在「付款政策博弈」的均衡中，代理的動態可以達到對拍賣人來說低收入的強有力共謀結果。這些結果突顯了在自動化學習代理可以受益於在機制界線外與同儕互動的系統中進行機制設計的挑戰。

##### **SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation**
2405.20878v1 by Yuxi Liu, Lianghao Xia, Chao Huang

Sequential recommendation effectively addresses information overload by
modeling users' temporal and sequential interaction patterns. To overcome the
limitations of supervision signals, recent approaches have adopted
self-supervised learning techniques in recommender systems. However, there are
still two critical challenges that remain unsolved. Firstly, existing
sequential models primarily focus on long-term modeling of individual
interaction sequences, overlooking the valuable short-term collaborative
relationships among the behaviors of different users. Secondly, real-world data
often contain noise, particularly in users' short-term behaviors, which can
arise from temporary intents or misclicks. Such noise negatively impacts the
accuracy of both graph and sequence models, further complicating the modeling
process. To address these challenges, we propose a novel framework called
Self-Supervised Graph Neural Network (SelfGNN) for sequential recommendation.
The SelfGNN framework encodes short-term graphs based on time intervals and
utilizes Graph Neural Networks (GNNs) to learn short-term collaborative
relationships. It captures long-term user and item representations at multiple
granularity levels through interval fusion and dynamic behavior modeling.
Importantly, our personalized self-augmented learning structure enhances model
robustness by mitigating noise in short-term graphs based on long-term user
interests and personal stability. Extensive experiments conducted on four
real-world datasets demonstrate that SelfGNN outperforms various
state-of-the-art baselines. Our model implementation codes are available at
https://github.com/HKUDS/SelfGNN.

摘要：序列推薦通過建模使用者的時間和序列互動模式，有效地解決資訊超載的問題。為了克服監督訊號的限制，最近的方法在推薦系統中採用了自監督學習技術。然而，仍有兩個關鍵挑戰尚未解決。首先，現有的序列模型主要專注於個別互動序列的長期建模，忽略了不同使用者行為之間有價值的短期協作關係。其次，真實世界的資料通常包含雜訊，特別是在使用者的短期行為中，這可能是由暫時的意圖或誤點造成的。這種雜訊對圖形和序列模型的準確性產生負面影響，進一步複雜化了建模過程。為了應對這些挑戰，我們提出了一個名為自監督圖神經網路 (SelfGNN) 的新框架，用於序列推薦。SelfGNN 框架根據時間間隔對短期圖形進行編碼，並利用圖神經網路 (GNN) 來學習短期協作關係。它通過間隔融合和動態行為建模，在多個粒度層次上擷取長期使用者和項目表示。重要的是，我們個性化的自增強學習結構通過根據長期使用者興趣和個人穩定性來減輕短期圖形中的雜訊，增強模型的穩健性。在四個真實世界資料集上進行的廣泛實驗表明，SelfGNN 優於各種最先進的基準。我們的模型實作程式碼可在 https://github.com/HKUDS/SelfGNN 取得。

##### **Investigating Calibration and Corruption Robustness of Post-hoc Pruned Perception CNNs: An Image Classification Benchmark Study**
2405.20876v1 by Pallavi Mitra, Gesina Schwalbe, Nadja Klein

Convolutional Neural Networks (CNNs) have achieved state-of-the-art
performance in many computer vision tasks. However, high computational and
storage demands hinder their deployment into resource-constrained environments,
such as embedded devices. Model pruning helps to meet these restrictions by
reducing the model size, while maintaining superior performance. Meanwhile,
safety-critical applications pose more than just resource and performance
constraints. In particular, predictions must not be overly confident, i.e.,
provide properly calibrated uncertainty estimations (proper uncertainty
calibration), and CNNs must be robust against corruptions like naturally
occurring input perturbations (natural corruption robustness). This work
investigates the important trade-off between uncertainty calibration, natural
corruption robustness, and performance for current state-of-research post-hoc
CNN pruning techniques in the context of image classification tasks. Our study
reveals that post-hoc pruning substantially improves the model's uncertainty
calibration, performance, and natural corruption robustness, sparking hope for
safe and robust embedded CNNs.Furthermore, uncertainty calibration and natural
corruption robustness are not mutually exclusive targets under pruning, as
evidenced by the improved safety aspects obtained by post-hoc unstructured
pruning with increasing compression.

摘要：卷積神經網路 (CNN) 在許多電腦視覺任務中已達到最先進的效能。然而，高運算和儲存需求阻礙了它們部署到資源受限的環境中，例如嵌入式裝置。模型剪枝有助於滿足這些限制，方法是縮小模型大小，同時維持優異效能。同時，安全關鍵應用程式不僅構成資源和效能限制。特別是，預測不能過於自信，亦即，提供適當校準的不確定性估計（適當的不確定性校準），而 CNN 必須對自然發生的輸入擾動（自然破壞的穩健性）等破壞具有穩健性。這項工作探討了不確定性校準、自然破壞的穩健性與效能之間的重要權衡，針對圖像分類任務中目前研究狀態的後設 CNN 剪枝技術。我們的研究顯示，後設剪枝大幅改善了模型的不確定性校準、效能和自然破壞的穩健性，為安全且穩健的嵌入式 CNN 帶來希望。此外，在剪枝下，不確定性校準和自然破壞的穩健性並非相互排斥的目標，正如後設非結構化剪枝在壓縮增加時獲得的安全性方面的改善所證明。

##### **Automatic Channel Pruning for Multi-Head Attention**
2405.20867v1 by Eunho Lee, Youngbae Hwang

Despite the strong performance of Transformers, their quadratic computation
complexity presents challenges in applying them to vision tasks. Automatic
pruning is one of effective methods for reducing computation complexity without
heuristic approaches. However, directly applying it to multi-head attention is
not straightforward due to channel misalignment. In this paper, we propose an
automatic channel pruning method to take into account the multi-head attention
mechanism. First, we incorporate channel similarity-based weights into the
pruning indicator to preserve more informative channels in each head. Then, we
adjust pruning indicator to enforce removal of channels in equal proportions
across all heads, preventing the channel misalignment. We also add a reweight
module to compensate for information loss resulting from channel removal, and
an effective initialization step for pruning indicator based on difference of
attention between original structure and each channel. Our proposed method can
be used to not only original attention, but also linear attention, which is
more efficient as linear complexity with respect to the number of tokens. On
ImageNet-1K, applying our pruning method to the FLattenTransformer, which
includes both attention mechanisms, shows outperformed accuracy for several
MACs compared with previous state-of-the-art efficient models and pruned
methods. Code will be available soon.

摘要：儘管 Transformer 有強勁的效能，但其二次運算複雜度在應用於視覺任務時會產生挑戰。自動剪枝是降低運算複雜度的方法之一，且無需使用啟發式方法。然而，由於通道未對齊，無法直接將其應用於多頭注意力。在本文中，我們提出了一種自動通道剪枝方法來考量多頭注意力機制。首先，我們將基於通道相似性的權重納入剪枝指標，以保留每個頭中更多有資訊性的通道。然後，我們調整剪枝指標，以強制在所有頭中按相等比例移除通道，防止通道未對齊。我們還新增一個重新加權模組，以補償因通道移除而造成的資訊損失，以及一個基於原始結構與每個通道之間注意力的差異的剪枝指標有效初始化步驟。我們提出的方法不僅可用於原始注意力，還能用於線性注意力，其效率更高，因為其線性複雜度與 token 數量有關。在 ImageNet-1K 上，將我們的剪枝方法應用於包含兩種注意力機制的 FLattenTransformer，與先前的最先進高效模型和剪枝方法相比，對於多個 MAC 顯示出優異的準確度。程式碼將很快提供。

##### **ABodyBuilder3: Improved and scalable antibody structure predictions**
2405.20863v1 by Henry Kenlay, Frédéric A. Dreyer, Daniel Cutting, Daniel Nissley, Charlotte M. Deane

Accurate prediction of antibody structure is a central task in the design and
development of monoclonal antibodies, notably to understand both their
developability and their binding properties. In this article, we introduce
ABodyBuilder3, an improved and scalable antibody structure prediction model
based on ImmuneBuilder. We achieve a new state-of-the-art accuracy in the
modelling of CDR loops by leveraging language model embeddings, and show how
predicted structures can be further improved through careful relaxation
strategies. Finally, we incorporate a predicted Local Distance Difference Test
into the model output to allow for a more accurate estimation of uncertainties.

摘要：精準預測抗體結構是單株抗體設計和開發中的核心任務，特別是為了了解它們的可開發性和結合特性。在本文中，我們介紹了 ABodyBuilder3，這是一個基於 ImmuneBuilder 的改良且可擴充的抗體結構預測模型。我們透過利用語言模型嵌入，在 CDR 迴圈建模中達到了新的最先進準確度，並展示了如何透過謹慎的放鬆策略進一步改善預測結構。最後，我們將預測的局部距離差異測試納入模型輸出，以更準確地估計不確定性。

##### **clembench-2024: A Challenging, Dynamic, Complementary, Multilingual Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents**
2405.20859v1 by Anne Beyer, Kranti Chalamalasetti, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, David Schlangen

It has been established in recent work that Large Language Models (LLMs) can
be prompted to "self-play" conversational games that probe certain capabilities
(general instruction following, strategic goal orientation, language
understanding abilities), where the resulting interactive game play can be
automatically scored. In this paper, we take one of the proposed frameworks for
setting up such game-play environments, and further test its usefulness as an
evaluation instrument, along a number of dimensions: We show that it can easily
keep up with new developments while avoiding data contamination, we show that
the tests implemented within it are not yet saturated (human performance is
substantially higher than that of even the best models), and we show that it
lends itself to investigating additional questions, such as the impact of the
prompting language on performance. We believe that the approach forms a good
basis for making decisions on model choice for building applied interactive
systems, and perhaps ultimately setting up a closed-loop development
environment of system and simulated evaluator.

摘要：最近的研究表明，大型语言模型 (LLM) 可以被提示“自行玩耍”会话游戏，这些游戏探查某些能力（遵循一般指令、战略目标导向、语言理解能力），其中产生的互动游戏玩法可以自动评分。在本文中，我们采用了一种提议的框架来设置此类游戏环境，并进一步测试其作为评估工具的实用性，沿多个维度：我们展示了它可以轻松跟上新发展，同时避免数据污染，我们展示了其中实施的测试尚未饱和（人类的表现明显高于甚至最好的模型），我们展示了它适用于调查其他问题，例如提示语言对性能的影响。我们相信该方法为针对构建应用交互式系统做出模型选择决策奠定了良好的基础，并最终建立系统和模拟评估器的闭环开发环境。

##### **Towards Spoken Language Understanding via Multi-level Multi-grained Contrastive Learning**
2405.20852v1 by Xuxin Cheng, Wanshi Xu, Zhihong Zhu, Hongxiang Li, Yuexian Zou

Spoken language understanding (SLU) is a core task in task-oriented dialogue
systems, which aims at understanding the user's current goal through
constructing semantic frames. SLU usually consists of two subtasks, including
intent detection and slot filling. Although there are some SLU frameworks joint
modeling the two subtasks and achieving high performance, most of them still
overlook the inherent relationships between intents and slots and fail to
achieve mutual guidance between the two subtasks. To solve the problem, we
propose a multi-level multi-grained SLU framework MMCL to apply contrastive
learning at three levels, including utterance level, slot level, and word level
to enable intent and slot to mutually guide each other. For the utterance
level, our framework implements coarse granularity contrastive learning and
fine granularity contrastive learning simultaneously. Besides, we also apply
the self-distillation method to improve the robustness of the model.
Experimental results and further analysis demonstrate that our proposed model
achieves new state-of-the-art results on two public multi-intent SLU datasets,
obtaining a 2.6 overall accuracy improvement on the MixATIS dataset compared to
previous best models.

摘要：口語理解 (SLU) 是任務導向對話系統中的核心任務，旨在通過構建語義框架來理解使用者的當前目標。SLU 通常包含兩個子任務，包括意圖偵測和槽位填補。儘管有些 SLU 框架聯合建模這兩個子任務並獲得高性能，但大多數仍然忽略意圖和槽位之間的內在關係，並且無法實現兩個子任務之間的相互指導。為了解決此問題，我們提出了一個多層多粒度 SLU 框架 MMCL，將對比學習應用於三個層級，包括發話層級、槽位層級和字詞層級，以使意圖和槽位能夠相互指導。對於發話層級，我們的框架同時實作粗粒度對比學習和細粒度對比學習。此外，我們還應用自蒸餾方法來提高模型的穩健性。實驗結果和進一步分析表明，我們提出的模型在兩個公開的多意圖 SLU 資料集上取得新的最先進結果，與先前的最佳模型相比，在 MixATIS 資料集上獲得 2.6 的整體準確度提升。

##### **Improving Reward Models with Synthetic Critiques**
2405.20850v1 by Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, Matthias Gallé

Reward models (RM) play a critical role in aligning language models through
the process of reinforcement learning from human feedback. RMs are trained to
predict a score reflecting human preference, which requires significant time
and cost for human annotation. Additionally, RMs tend to quickly overfit on
superficial features in the training set, hindering their generalization
performance on unseen distributions. We propose a novel approach using
synthetic natural language critiques generated by large language models to
provide additional feedback, evaluating aspects such as instruction following,
correctness, and style. This offers richer signals and more robust features for
RMs to assess and score on. We demonstrate that high-quality critiques improve
the performance and data efficiency of RMs initialized from different
pretrained models. Conversely, we also show that low-quality critiques
negatively impact performance. Furthermore, incorporating critiques enhances
the interpretability and robustness of RM training.

摘要：獎勵模型（RM）在透過人類回饋的強化學習過程中，對齊語言模型扮演關鍵角色。RM 經過訓練，可預測反映人類偏好的分數，這需要大量時間和成本進行人工註解。此外，RM 傾向於快速過度擬合訓練集中的表面特徵，阻礙其在未見分布上的泛化效能。我們提出一個新穎的方法，使用大型語言模型產生的合成自然語言批評，提供額外的回饋，評估指示遵循、正確性和風格等面向。這為 RM 提供更豐富的訊號和更穩健的特徵，以便評估和評分。我們證明高品質的批評改善了從不同預訓練模型初始化的 RM 的效能和資料效率。相反地，我們也顯示低品質的批評對效能產生負面影響。此外，納入批評可增強 RM 訓練的可解釋性和穩健性。

##### **SLIM: a Scalable Light-weight Root Cause Analysis for Imbalanced Data in Microservice**
2405.20848v1 by Rui Ren, Jingbang Yang, Linxiao Yang, Xinyue Gu, Liang Sun

The newly deployed service -- one kind of change service, could lead to a new
type of minority fault. Existing state-of-the-art methods for fault
localization rarely consider the imbalanced fault classification in change
service. This paper proposes a novel method that utilizes decision rule sets to
deal with highly imbalanced data by optimizing the F1 score subject to
cardinality constraints. The proposed method greedily generates the rule with
maximal marginal gain and uses an efficient minorize-maximization (MM) approach
to select rules iteratively, maximizing a non-monotone submodular lower bound.
Compared with existing fault localization algorithms, our algorithm can adapt
to the imbalanced fault scenario of change service, and provide interpretable
fault causes which are easy to understand and verify. Our method can also be
deployed in the online training setting, with only about 15% training overhead
compared to the current SOTA methods. Empirical studies showcase that our
algorithm outperforms existing fault localization algorithms in both accuracy
and model interpretability.

摘要：新部署的服務——一種變更服務，可能會導致新的少數故障類型。現有的故障定位技術最先進的方法很少考慮變更服務中的不平衡故障分類。本文提出了一種新方法，利用決策規則集通過優化 F1 分數（受基數約束）來處理高度不平衡的數據。所提出的方法貪婪地生成邊際增益最大的規則，並使用一種有效的最小化-最大化 (MM) 方法迭代選擇規則，從而最大化非單調子模函數下界。與現有的故障定位算法相比，我們的算法可以適應變更服務的不平衡故障場景，並提供易於理解和驗證的可解釋故障原因。我們的算法還可以部署在線上訓練設置中，與當前 SOTA 方法相比，訓練開銷僅增加了約 15%。經驗研究表明，我們的算法在準確性和模型可解釋性方面都優於現有的故障定位算法。

##### **Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive Multimodal Models**
2405.20846v1 by A. Bavaresco, A. Testoni, R. Fernández

Image-based advertisements are complex multimodal stimuli that often contain
unusual visual elements and figurative language. Previous research on automatic
ad understanding has reported impressive zero-shot accuracy of contrastive
vision-and-language models (VLMs) on an ad-explanation retrieval task. Here, we
examine the original task setup and show that contrastive VLMs can solve it by
exploiting grounding heuristics. To control for this confound, we introduce
TRADE, a new evaluation test set with adversarial grounded explanations. While
these explanations look implausible to humans, we show that they "fool" four
different contrastive VLMs. Our findings highlight the need for an improved
operationalisation of automatic ad understanding that truly evaluates VLMs'
multimodal reasoning abilities. We make our code and TRADE available at
https://github.com/dmg-illc/trade .

摘要：基於影像的廣告是複雜的多模態刺激，通常包含不尋常的視覺元素和比喻語言。先前關於自動廣告理解的研究已報導對比視覺和語言模型 (VLM) 在廣告解釋檢索任務上令人印象深刻的零次學習準確度。在此，我們檢查原始任務設定，並展示對比 VLM 可以透過利用接地啟發法來解決它。為了控制這種混淆，我們引入了 TRADE，這是一個包含對抗接地解釋的新評估測試集。雖然這些解釋對人類來說看起來難以置信，但我們展示它們「愚弄」了四個不同的對比 VLM。我們的研究結果強調需要改進自動廣告理解的操作化，以真正評估 VLM 的多模態推理能力。我們在 https://github.com/dmg-illc/trade 提供我們的程式碼和 TRADE。

##### **einspace: Searching for Neural Architectures from Fundamental Operations**
2405.20838v1 by Linus Ericsson, Miguel Espinosa, Chenhongyi Yang, Antreas Antoniou, Amos Storkey, Shay B. Cohen, Steven McDonagh, Elliot J. Crowley

Neural architecture search (NAS) finds high performing networks for a given
task. Yet the results of NAS are fairly prosaic; they did not e.g. create a
shift from convolutional structures to transformers. This is not least because
the search spaces in NAS often aren't diverse enough to include such
transformations a priori. Instead, for NAS to provide greater potential for
fundamental design shifts, we need a novel expressive search space design which
is built from more fundamental operations. To this end, we introduce einspace,
a search space based on a parameterised probabilistic context-free grammar. Our
space is versatile, supporting architectures of various sizes and complexities,
while also containing diverse network operations which allow it to model
convolutions, attention components and more. It contains many existing
competitive architectures, and provides flexibility for discovering new ones.
Using this search space, we perform experiments to find novel architectures as
well as improvements on existing ones on the diverse Unseen NAS datasets. We
show that competitive architectures can be obtained by searching from scratch,
and we consistently find large improvements when initialising the search with
strong baselines. We believe that this work is an important advancement towards
a transformative NAS paradigm where search space expressivity and strategic
search initialisation play key roles.

摘要：神經結構搜尋 (NAS) 尋找給定任務的高效能網路。然而，NAS 的結果相當平淡；例如，它們並未從捲積結構轉移到Transformer。這主要是因為 NAS 中的搜尋空間通常不夠多元，無法先驗包含此類轉換。相反地，為了讓 NAS 提供更大的基本設計轉變潛力，我們需要一個新穎且具表達力的搜尋空間設計，而此設計是建立在更基本的運算上。為此，我們引入了 einspace，一個基於參數化機率無上下文文法的搜尋空間。我們的空間具有多功能性，支援各種大小和複雜性的架構，同時也包含多元的網路運算，使其能夠建模卷積、注意力元件等。它包含許多現有的競爭性架構，並提供彈性來發現新的架構。使用此搜尋空間，我們執行實驗以尋找新穎的架構，以及在多元的 Unseen NAS 資料集上對現有架構進行改進。我們展示了可透過從頭開始搜尋來取得競爭性架構，並且在使用強大的基準初始化搜尋時，我們持續發現大幅的改進。我們相信這項工作是邁向轉型 NAS 典範的重要進展，其中搜尋空間的表達力和策略性搜尋初始化扮演了關鍵角色。

##### **Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs**
2405.20835v1 by Davide Paglieri, Saurabh Dash, Tim Rocktäschel, Jack Parker-Holder

Post-Training Quantization (PTQ) enhances the efficiency of Large Language
Models (LLMs) by enabling faster operation and compatibility with more
accessible hardware through reduced memory usage, at the cost of small
performance drops. We explore the role of calibration sets in PTQ, specifically
their effect on hidden activations in various notable open-source LLMs.
Calibration sets are crucial for evaluating activation magnitudes and
identifying outliers, which can distort the quantization range and negatively
impact performance. Our analysis reveals a marked contrast in quantization
effectiveness across models. The older OPT model, which much of the
quantization literature is based on, shows significant performance
deterioration and high susceptibility to outliers with varying calibration
sets. In contrast, newer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and
Mistral 7B demonstrate strong robustness, with Mistral 7B showing near-immunity
to outliers and stable activations. These findings suggest a shift in PTQ
strategies might be needed. As advancements in pre-training methods reduce the
relevance of outliers, there is an emerging need to reassess the fundamentals
of current quantization literature. The emphasis should pivot towards
optimizing inference speed, rather than primarily focusing on outlier
preservation, to align with the evolving characteristics of state-of-the-art
LLMs.

摘要：後訓練量化 (PTQ) 透過降低記憶體使用量來提升大型語言模型 (LLM) 的效率，進而實現更快速的運作，並與更多可存取的硬體相容，但代價是會稍微降低效能。我們探討校正集在 PTQ 中的角色，特別是它們對各種著名的開放原始碼 LLM 中的隱藏激活的影響。校正集對於評估激活幅度和找出異常值至關重要，因為異常值可能會扭曲量化範圍並對效能造成負面影響。我們的分析顯示，不同模型在量化有效性上有顯著的差異。許多量化文獻所依據的較舊 OPT 模型，顯示出顯著的效能惡化，且對不同校正集的異常值高度敏感。相比之下，更新的模型，例如 Llama-2 7B、Llama-3 8B、Command-R 35B 和 Mistral 7B，則展現出強大的穩健性，其中 Mistral 7B 顯示出對異常值的近乎免疫性和穩定的激活。這些發現表明，PTQ 策略可能需要轉變。隨著預訓練方法的進步降低異常值的重要性，重新評估當前量化文獻的基本原理已成為新興需求。重點應轉向最佳化推論速度，而不是主要關注異常值保留，以符合最先進 LLM 的演化特性。

##### **That's Optional: A Contemporary Exploration of "that" Omission in English Subordinate Clauses**
2405.20833v1 by Ella Rabinovich

The Uniform Information Density (UID) hypothesis posits that speakers
optimize the communicative properties of their utterances by avoiding spikes in
information, thereby maintaining a relatively uniform information profile over
time. This paper investigates the impact of UID principles on syntactic
reduction, specifically focusing on the optional omission of the connector
"that" in English subordinate clauses. Building upon previous research, we
extend our investigation to a larger corpus of written English, utilize
contemporary large language models (LLMs) and extend the information-uniformity
principles by the notion of entropy, to estimate the UID manifestations in the
usecase of syntactic reduction choices.

摘要：均勻資訊密度 (UID) 假設主張，說話者透過避免資訊峰值來優化其話語的溝通特性，從而隨著時間維持相對均勻的資訊輪廓。本文探討 UID 原則對句法簡化的影響，特別關注英語從屬子句中連接詞「that」的省略。在先前的研究基礎上，我們將研究擴展到更大的英語書面語料庫中，利用當代大型語言模型 (LLM)，並透過熵的概念擴展資訊均勻性原則，以估計 UID 在句法簡化選擇中的表現。

##### **Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment**
2405.20830v1 by Yueqin Yin, Zhendong Wang, Yujia Xie, Weizhu Chen, Mingyuan Zhou

Traditional language model alignment methods, such as Direct Preference
Optimization (DPO), are limited by their dependence on static, pre-collected
paired preference data, which hampers their adaptability and practical
applicability. To overcome this limitation, we introduce Self-Augmented
Preference Optimization (SAPO), an effective and scalable training paradigm
that does not require existing paired data. Building on the self-play concept,
which autonomously generates negative responses, we further incorporate an
off-policy learning pipeline to enhance data exploration and exploitation.
Specifically, we employ an Exponential Moving Average (EMA) model in
conjunction with a replay buffer to enable dynamic updates of response
segments, effectively integrating real-time feedback with insights from
historical data. Our comprehensive evaluations of the LLaMA3-8B and Mistral-7B
models across benchmarks, including the Open LLM Leaderboard, IFEval,
AlpacaEval 2.0, and MT-Bench, demonstrate that SAPO matches or surpasses
established offline contrastive baselines, such as DPO and Odds Ratio
Preference Optimization, and outperforms offline self-play methods like SPIN.
Our code is available at https://github.com/yinyueqin/SAPO

摘要：傳統語言模型對齊方法，例如直接偏好最佳化 (DPO)，受到依賴靜態、預先收集的配對偏好資料的限制，這會妨礙其適應性和實用性。為了克服這個限制，我們引入了自我擴充偏好最佳化 (SAPO)，這是一種有效且可擴充的訓練範例，不需要現有的配對資料。建立在自我對弈概念之上，它會自動產生負面回應，我們進一步整合了一個非策略學習管道，以增強資料探索和利用。具體來說，我們採用指數移動平均 (EMA) 模型，並結合重播緩衝區，以啟用回應片段的動態更新，有效地將即時回饋與歷史資料的見解整合在一起。我們對 LLaMA3-8B 和 Mistral-7B 模型在基準上的全面評估，包括 Open LLM Leaderboard、IFEval、AlpacaEval 2.0 和 MT-Bench，證明 SAPO 匹配或超越已建立的離線對比基準，例如 DPO 和機率比偏好最佳化，並且優於離線自我對弈方法，例如 SPIN。我們的程式碼可在 https://github.com/yinyueqin/SAPO 取得

##### **An iterated learning model of language change that mixes supervised and unsupervised learning**
2405.20818v1 by Jack Bunyan, Seth Bullock, Conor Houghton

The iterated learning model is an agent-based model of language change in
which language is transmitted from a tutor to a pupil which itself becomes a
tutor to a new pupil, and so on. Languages that are stable, expressive, and
compositional arise spontaneously as a consequence of a language transmission
bottleneck. Previous models have implemented an agent's mapping from signals to
meanings using an artificial neural network decoder, but have relied on an
unrealistic and computationally expensive process of obversion to implement the
associated encoder, mapping from meanings to signals. Here, a new model is
presented in which both decoder and encoder are neural networks, trained
separately through supervised learning, and trained together through
unsupervised learning in the form of an autoencoder. This avoids the
substantial computational burden entailed in obversion and introduces a mixture
of supervised and unsupervised learning as observed during human development.

摘要：迭代學習模型是一種基於代理的語言變遷模型，其中語言由導師傳遞給學生，而學生本身又成為新學生的導師，依此類推。穩定、表達能力強且具有組合性的語言會自發地出現在語言傳遞瓶頸的後果中。先前的模型已使用人工神經網路解碼器實作代理程式從訊號到意義的對應，但依賴於非現實且計算成本昂貴的逆轉過程來實作關聯的編碼器，從意義對應到訊號。在此，提出一個新模型，其中解碼器和編碼器都是神經網路，透過監督式學習分別進行訓練，並透過自編碼器的形式以非監督式學習一起進行訓練。這避免了逆轉中所涉及的龐大計算負擔，並引入了人類發展過程中觀察到的監督式和非監督式學習的混合。

##### **There and Back Again: The AI Alignment Paradox**
2405.20806v1 by Robert West, Roland Aydin

The field of AI alignment aims to steer AI systems toward human goals,
preferences, and ethical principles. Its contributions have been instrumental
for improving the output quality, safety, and trustworthiness of today's AI
models. This perspective article draws attention to a fundamental challenge
inherent in all AI alignment endeavors, which we term the "AI alignment
paradox": The better we align AI models with our values, the easier we make it
for adversaries to misalign the models. We illustrate the paradox by sketching
three concrete example incarnations for the case of language models, each
corresponding to a distinct way in which adversaries can exploit the paradox.
With AI's increasing real-world impact, it is imperative that a broad community
of researchers be aware of the AI alignment paradox and work to find ways to
break out of it, in order to ensure the beneficial use of AI for the good of
humanity.

摘要：人工智慧對齊領域旨在引導人工智慧系統朝向人類的目標、偏好和道德原則。其貢獻對於提升當今人工智慧模型的輸出品質、安全性與可信度至關重要。這篇觀點文章提到了所有人工智慧對齊工作中固有的基本挑戰，我們稱之為「人工智慧對齊悖論」：我們愈是將人工智慧模型與我們的價值觀對齊，對手就愈容易錯置這些模型。我們透過勾勒出語言模型案例的三個具體範例化身來說明這個悖論，每個範例都對應於對手可以利用悖論的不同方式。隨著人工智慧在現實世界中的影響力與日俱增，廣大的研究社群必須意識到人工智慧對齊悖論，並努力找出方法來突破它，以確保人工智慧的益處能用於人類福祉。

##### **Multilingual Text Style Transfer: Datasets & Models for Indian Languages**
2405.20805v1 by Sourabrata Mukherjee, Atul Kr. Ojha, Akanksha Bansal, Deepak Alok, John P. McCrae, Ondřej Dušek

Text style transfer (TST) involves altering the linguistic style of a text
while preserving its core content. This paper focuses on sentiment transfer, a
vital TST subtask (Mukherjee et al., 2022a), across a spectrum of Indian
languages: Hindi, Magahi, Malayalam, Marathi, Punjabi, Odia, Telugu, and Urdu,
expanding upon previous work on English-Bangla sentiment transfer (Mukherjee et
al., 2023). We introduce dedicated datasets of 1,000 positive and 1,000
negative style-parallel sentences for each of these eight languages. We then
evaluate the performance of various benchmark models categorized into parallel,
non-parallel, cross-lingual, and shared learning approaches, including the
Llama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the
significance of parallel data in TST and demonstrate the effectiveness of the
Masked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel
techniques. Moreover, cross-lingual and joint multilingual learning methods
show promise, offering insights into selecting optimal models tailored to the
specific language and task requirements. To the best of our knowledge, this
work represents the first comprehensive exploration of the TST task as
sentiment transfer across a diverse set of languages.

摘要：文本風格轉移（TST）涉及更改文本的語言風格，同時保留其核心內容。本文重點關注情緒轉移，這是一個重要的 TST 子任務（Mukherjee 等人，2022a），涉及印地語、馬加希語、馬拉雅拉姆語、馬拉地語、旁遮普語、奧里亞語、泰盧固語和烏爾都語等印度語言，擴展了先前關於英語-孟加拉語情緒轉移（Mukherjee 等人，2023）的研究。我們為這八種語言中的每種語言引入了 1,000 個積極和 1,000 個消極的風格平行句子的專用數據集。然後，我們評估了各種基準模型的性能，這些模型分為平行、非平行、跨語言和共享學習方法，包括 Llama2 和 GPT-3.5 大型語言模型 (LLM)。我們的實驗突出了平行數據在 TST 中的重要性，並展示了掩碼風格填充 (MSF) 方法（Mukherjee 等人，2023）在非平行技術中的有效性。此外，跨語言和聯合多語言學習方法顯示出前景，提供了針對特定語言和任務要求選擇最佳模型的見解。據我們所知，這項工作代表了首次對 TST 任務作為跨多種語言的情緒轉移進行全面探索。

##### **Ovis: Structural Embedding Alignment for Multimodal Large Language Model**
2405.20797v1 by Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Han-Jia Ye

Current Multimodal Large Language Models (MLLMs) typically integrate a
pre-trained LLM with another pre-trained vision transformer through a
connector, such as an MLP, endowing the LLM with visual capabilities. However,
the misalignment between two embedding strategies in MLLMs -- the structural
textual embeddings based on an embedding look-up table and the continuous
embeddings generated directly by the vision encoder -- makes challenges for a
more seamless fusion of visual and textual information. We propose Ovis, a
novel MLLM architecture designed to structurally align visual and textual
embeddings. Ovis integrates an additional learnable visual embedding table into
the visual encoder's process. To capture rich visual semantics, each image
patch indexes the visual embedding table multiple times, resulting in a final
visual embedding that is a probabilistic combination of the indexed embeddings.
This structural approach mirrors the method used for generating textual
embeddings. Empirical evaluations on various multimodal benchmarks demonstrate
that Ovis outperforms open-source MLLMs of similar parameter scales and even
surpasses the proprietary model Qwen-VL-Plus overall. These results highlight
the potential of Ovis' structured visual representation for advancing MLLM
architectural design and promoting more effective multimodal learning. Both the
source code and the training dataset of Ovis will be made publicly available.

摘要：目前的多模态大型语言模型 (MLLM) 通常通过连接器（例如 MLP）将预先训练的 LLM 与另一个预先训练的视觉转换器集成在一起，从而赋予 LLM 视觉功能。然而，MLLM 中两种嵌入策略之间的错位——基于嵌入查找表的结构化文本嵌入和由视觉编码器直接生成的连续嵌入——给视觉和文本信息的更无缝融合带来了挑战。我们提出了 Ovis，这是一种新颖的 MLLM 架构，旨在结构化对齐视觉和文本嵌入。Ovis 将一个额外的可学习视觉嵌入表集成到视觉编码器的过程中。为了捕获丰富的视觉语义，每个图像块都会多次索引视觉嵌入表，从而生成一个最终的视觉嵌入，该嵌入是已索引嵌入的概率组合。这种结构化方法反映了用于生成文本嵌入的方法。在各种多模态基准上的经验评估表明，Ovis 优于具有相似参数规模的开源 MLLM，甚至在总体上超越了专有模型 Qwen-VL-Plus。这些结果突出了 Ovis 的结构化视觉表示在推进 MLLM 架构设计和促进更有效的多模态学习方面的潜力。Ovis 的源代码和训练数据集都将公开。

##### **InsightSee: Advancing Multi-agent Vision-Language Models for Enhanced Visual Understanding**
2405.20795v1 by Huaxiang Zhang, Yaojia Mu, Guo-Niu Zhu, Zhongxue Gan

Accurate visual understanding is imperative for advancing autonomous systems
and intelligent robots. Despite the powerful capabilities of vision-language
models (VLMs) in processing complex visual scenes, precisely recognizing
obscured or ambiguously presented visual elements remains challenging. To
tackle such issues, this paper proposes InsightSee, a multi-agent framework to
enhance VLMs' interpretative capabilities in handling complex visual
understanding scenarios. The framework comprises a description agent, two
reasoning agents, and a decision agent, which are integrated to refine the
process of visual information interpretation. The design of these agents and
the mechanisms by which they can be enhanced in visual information processing
are presented. Experimental results demonstrate that the InsightSee framework
not only boosts performance on specific visual tasks but also retains the
original models' strength. The proposed framework outperforms state-of-the-art
algorithms in 6 out of 9 benchmark tests, with a substantial advancement in
multimodal understanding.

摘要：準確的視覺理解對於推進自主系統和智慧機器人至關重要。儘管視覺語言模型 (VLM) 在處理複雜視覺場景方面具有強大的能力，但精確識別模糊或含糊呈現的視覺元素仍然具有挑戰性。為了解決這些問題，本文提出了 InsightSee，一個多代理架構，以增強 VLM 在處理複雜視覺理解場景方面的解釋能力。該架構包含一個描述代理、兩個推理代理和一個決策代理，這些代理被整合起來以優化視覺資訊解釋的過程。這些代理的設計以及它們在視覺資訊處理中可以增強的機制都已提出。實驗結果表明，InsightSee 框架不僅提升了特定視覺任務的效能，而且還保留了原始模型的優勢。所提出的框架在 9 項基準測試中的 6 項中優於最先進的演算法，在多模態理解方面取得了顯著進展。

##### **Improving code-mixed hate detection by native sample mixing: A case study for Hindi-English code-mixed scenario**
2405.20755v1 by Debajyoti Mazumder, Aakash Kumar, Jasabanta Patro

Hate detection has long been a challenging task for the NLP community. The
task becomes complex in a code-mixed environment because the models must
understand the context and the hate expressed through language alteration.
Compared to the monolingual setup, we see very less work on code-mixed hate as
large-scale annotated hate corpora are unavailable to make the study. To
overcome this bottleneck, we propose using native language hate samples. We
hypothesise that in the era of multilingual language models (MLMs), hate in
code-mixed settings can be detected by majorly relying on the native language
samples. Even though the NLP literature reports the effectiveness of MLMs on
hate detection in many cross-lingual settings, their extensive evaluation in a
code-mixed scenario is yet to be done. This paper attempts to fill this gap
through rigorous empirical experiments. We considered the Hindi-English
code-mixed setup as a case study as we have the linguistic expertise for the
same. Some of the interesting observations we got are: (i) adding native hate
samples in the code-mixed training set, even in small quantity, improved the
performance of MLMs for code-mixed hate detection, (ii) MLMs trained with
native samples alone observed to be detecting code-mixed hate to a large
extent, (iii) The visualisation of attention scores revealed that, when native
samples were included in training, MLMs could better focus on the hate emitting
words in the code-mixed context, and (iv) finally, when hate is subjective or
sarcastic, naively mixing native samples doesn't help much to detect code-mixed
hate. We will release the data and code repository to reproduce the reported
results.

摘要：<paragraph>仇恨偵測一直是自然語言處理社群的挑戰性任務。此任務在代碼混合環境中變得複雜，因為模型必須理解上下文和透過語言變更表達的仇恨。與單一語言設定相比，我們看到代碼混合仇恨的研究非常少，因為沒有可用的大型標註仇恨語料庫進行研究。為了克服這個瓶頸，我們建議使用母語仇恨範例。我們假設在多語言語言模型 (MLM) 時代，代碼混合環境中的仇恨可以主要透過依賴母語範例來偵測。儘管自然語言處理文獻報告了 MLM 在許多跨語言環境中仇恨偵測的有效性，但它們在代碼混合場景中的廣泛評估尚未完成。本文嘗試透過嚴謹的實證實驗來填補這個空白。我們將印地語-英語代碼混合設定視為案例研究，因為我們擁有相同的語言專業知識。我們得到的一些有趣的觀察結果：(i) 在代碼混合訓練集中加入母語仇恨範例，即使數量很少，也能改善 MLM 對代碼混合仇恨偵測的效能，(ii) 單獨使用母語範例訓練的 MLM 觀察到在很大程度上偵測到代碼混合仇恨，(iii) 注意力分數的可視化顯示，當訓練中包含母語範例時，MLM 可以更好地關注代碼混合上下文中散發仇恨的字詞，(iv) 最後，當仇恨是主觀或諷刺時，天真地混合母語範例對偵測代碼混合仇恨沒有太大幫助。我們將釋出資料和程式碼儲存庫，以重現報告的結果。</paragraph>

##### **Trajectory Forecasting through Low-Rank Adaptation of Discrete Latent Codes**
2405.20743v1 by Riccardo Benaglia, Angelo Porrello, Pietro Buzzega, Simone Calderara, Rita Cucchiara

Trajectory forecasting is crucial for video surveillance analytics, as it
enables the anticipation of future movements for a set of agents, e.g.
basketball players engaged in intricate interactions with long-term intentions.
Deep generative models offer a natural learning approach for trajectory
forecasting, yet they encounter difficulties in achieving an optimal balance
between sampling fidelity and diversity. We address this challenge by
leveraging Vector Quantized Variational Autoencoders (VQ-VAEs), which utilize a
discrete latent space to tackle the issue of posterior collapse. Specifically,
we introduce an instance-based codebook that allows tailored latent
representations for each example. In a nutshell, the rows of the codebook are
dynamically adjusted to reflect contextual information (i.e., past motion
patterns extracted from the observed trajectories). In this way, the
discretization process gains flexibility, leading to improved reconstructions.
Notably, instance-level dynamics are injected into the codebook through
low-rank updates, which restrict the customization of the codebook to a lower
dimension space. The resulting discrete space serves as the basis of the
subsequent step, which regards the training of a diffusion-based predictive
model. We show that such a two-fold framework, augmented with instance-level
discretization, leads to accurate and diverse forecasts, yielding
state-of-the-art performance on three established benchmarks.

摘要：軌跡預測對於影片監控分析至關重要，因為它能預測一組代理人的未來移動，例如參與複雜互動且具有長期意圖的籃球員。深度生成模型提供了一種自然學習方法來進行軌跡預測，但它們在達成取樣保真度和多樣性之間的最佳平衡時會遇到困難。我們透過利用向量量化變異自動編碼器 (VQ-VAE) 來應對這項挑戰，它使用離散潛在空間來解決後驗崩潰問題。具體來說，我們引入了一個基於實例的碼本，允許為每個範例量身打造潛在表示。簡而言之，碼本的行會動態調整以反映上下文資訊（即從觀察到的軌跡中提取的過去運動模式）。透過這種方式，離散化過程會獲得靈活性，進而改善重建。值得注意的是，實例級動態會透過低秩更新注入碼本，這會將碼本的客製化限制在較低維度的空間。產生的離散空間作為後續步驟的基礎，該步驟涉及訓練基於擴散的預測模型。我們展示了這種雙重架構（加上實例級離散化）會產生準確且多樣化的預測，在三個既定的基準上產生最先進的效能。

##### **Maximum Temperature Prediction Using Remote Sensing Data Via Convolutional Neural Network**
2405.20731v1 by Lorenzo Innocenti, Giacomo Blanco, Luca Barco, Claudio Rossi

Urban heat islands, defined as specific zones exhibiting substantially higher
temperatures than their immediate environs, pose significant threats to
environmental sustainability and public health. This study introduces a novel
machine-learning model that amalgamates data from the Sentinel-3 satellite,
meteorological predictions, and additional remote sensing inputs. The primary
aim is to generate detailed spatiotemporal maps that forecast the peak
temperatures within a 24-hour period in Turin. Experimental results validate
the model's proficiency in predicting temperature patterns, achieving a Mean
Absolute Error (MAE) of 2.09 degrees Celsius for the year 2023 at a resolution
of 20 meters per pixel, thereby enriching our knowledge of urban climatic
behavior. This investigation enhances the understanding of urban microclimates,
emphasizing the importance of cross-disciplinary data integration, and laying
the groundwork for informed policy-making aimed at alleviating the negative
impacts of extreme urban temperatures.

摘要：都市熱島效應是指特定區域的溫度明顯高於周圍環境，對環境永續性與公共健康造成重大威脅。本研究提出一個創新的機器學習模型，結合 Sentinel-3 衛星、氣象預測和額外遙測輸入的資料。主要目的是產生詳細的時空地圖，預測都靈 24 小時內的最高溫度。實驗結果驗證了模型在預測溫度模式方面的熟練度，在 2023 年以每像素 20 公尺解析度達到 2.09 度攝氏的平均絕對誤差 (MAE)，從而豐富了我們對都市氣候行為的認識。此研究增強了對都市微氣候的了解，強調跨領域資料整合的重要性，並為旨在減輕極端都市溫度負面影響的明智政策制定奠定基礎。

##### **GANcrop: A Contrastive Defense Against Backdoor Attacks in Federated Learning**
2405.20727v1 by Xiaoyun Gan, Shanyu Gan, Taizhi Su, Peng Liu

With heightened awareness of data privacy protection, Federated Learning (FL)
has attracted widespread attention as a privacy-preserving distributed machine
learning method. However, the distributed nature of federated learning also
provides opportunities for backdoor attacks, where attackers can guide the
model to produce incorrect predictions without affecting the global model
training process.
  This paper introduces a novel defense mechanism against backdoor attacks in
federated learning, named GANcrop. This approach leverages contrastive learning
to deeply explore the disparities between malicious and benign models for
attack identification, followed by the utilization of Generative Adversarial
Networks (GAN) to recover backdoor triggers and implement targeted mitigation
strategies. Experimental findings demonstrate that GANcrop effectively
safeguards against backdoor attacks, particularly in non-IID scenarios, while
maintaining satisfactory model accuracy, showcasing its remarkable defensive
efficacy and practical utility.

摘要：隨著資料隱私保護意識的提升，聯邦學習 (FL) 作為一種保護隱私的分布式機器學習方法，引起了廣泛關注。然而，聯邦學習的分布式特性也為後門攻擊提供了機會，攻擊者可以誘導模型產生不正確的預測，而不會影響全局模型訓練過程。本文介紹了一種針對聯邦學習中後門攻擊的新型防禦機制，名為 GANcrop。此方法利用對比學習深入探索惡意模型和良性模型之間的差異，以進行攻擊識別，然後利用生成對抗網路 (GAN) 來恢復後門觸發器並實施有針對性的緩解策略。實驗結果表明，GANcrop 有效地防禦了後門攻擊，特別是在非 IID 場景中，同時保持了令人滿意的模型準確度，展示了其卓越的防禦效能和實用性。

##### **GI-NAS: Boosting Gradient Inversion Attacks through Adaptive Neural Architecture Search**
2405.20725v1 by Wenbo Yu, Hao Fang, Bin Chen, Xiaohang Sui, Chuan Chen, Hao Wu, Shu-Tao Xia, Ke Xu

Gradient Inversion Attacks invert the transmitted gradients in Federated
Learning (FL) systems to reconstruct the sensitive data of local clients and
have raised considerable privacy concerns. A majority of gradient inversion
methods rely heavily on explicit prior knowledge (e.g., a well pre-trained
generative model), which is often unavailable in realistic scenarios. To
alleviate this issue, researchers have proposed to leverage the implicit prior
knowledge of an over-parameterized network. However, they only utilize a fixed
neural architecture for all the attack settings. This would hinder the adaptive
use of implicit architectural priors and consequently limit the
generalizability. In this paper, we further exploit such implicit prior
knowledge by proposing Gradient Inversion via Neural Architecture Search
(GI-NAS), which adaptively searches the network and captures the implicit
priors behind neural architectures. Extensive experiments verify that our
proposed GI-NAS can achieve superior attack performance compared to
state-of-the-art gradient inversion methods, even under more practical settings
with high-resolution images, large-sized batches, and advanced defense
strategies.

摘要：梯度反演攻击反转了联邦学习 (FL) 系统中传输的梯度，以重建本地客户端的敏感数据，并引发了相当大的隐私问题。大多数梯度反演方法严重依赖于明确的先验知识（例如，经过良好预训练的生成模型），而在实际场景中通常无法获得。为了缓解这个问题，研究人员提出利用过参数化网络的隐式先验知识。然而，他们只为所有攻击设置利用了一个固定的神经架构。这会阻碍隐式架构先验的自适应使用，并因此限制泛化性。在本文中，我们通过提出神经架构搜索 (GI-NAS) 中的梯度反演进一步利用此类隐式先验知识，该知识自适应地搜索网络并捕获神经架构背后的隐式先验。大量的实验验证了，与最先进的梯度反演方法相比，我们提出的 GI-NAS 可以在更实际的高分辨率图像、大规模批处理和高级防御策略设置下实现卓越的攻击性能。

##### **ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model**
2405.20721v1 by Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex C. Kot, Bihan Wen

Recently, 3D Gaussian Splatting (3DGS) has become a promising framework for
novel view synthesis, offering fast rendering speeds and high fidelity.
However, the large number of Gaussians and their associated attributes require
effective compression techniques. Existing methods primarily compress neural
Gaussians individually and independently, i.e., coding all the neural Gaussians
at the same time, with little design for their interactions and spatial
dependence. Inspired by the effectiveness of the context model in image
compression, we propose the first autoregressive model at the anchor level for
3DGS compression in this work. We divide anchors into different levels and the
anchors that are not coded yet can be predicted based on the already coded ones
in all the coarser levels, leading to more accurate modeling and higher coding
efficiency. To further improve the efficiency of entropy coding, e.g., to code
the coarsest level with no already coded anchors, we propose to introduce a
low-dimensional quantized feature as the hyperprior for each anchor, which can
be effectively compressed. Our work pioneers the context model in the anchor
level for 3DGS representation, yielding an impressive size reduction of over
100 times compared to vanilla 3DGS and 15 times compared to the most recent
state-of-the-art work Scaffold-GS, while achieving comparable or even higher
rendering quality.

摘要：<paragraph>最近，3D 高斯散布（3DGS）已成为新视图合成的一个有前途的框架，它提供了快速的渲染速度和高保真度。
然而，大量的 Gaussians 及其关联属性需要有效的压缩技术。现有方法主要对神经 Gaussians 进行单独且独立的压缩，即同时编码所有神经 Gaussians，而很少考虑它们之间的交互和空间依赖性。受图像压缩中上下文模型的有效性的启发，我们在这项工作中提出了第一个用于 3DGS 压缩的锚点级别的自回归模型。我们将锚点划分为不同的级别，尚未编码的锚点可以根据所有较粗级别中已编码的锚点进行预测，从而实现更准确的建模和更高的编码效率。为了进一步提高熵编码的效率，例如对没有已编码锚点的最粗级别进行编码，我们建议为每个锚点引入一个低维量化特征作为超先验，它可以被有效压缩。我们的工作开创了锚点级别中用于 3DGS 表示的上下文模型，与原始 3DGS 相比产生了超过 100 倍的惊人尺寸缩减，与最新的最先进工作 Scaffold-GS 相比缩减了 15 倍，同时实现了相当甚至更高的渲染质量。</paragraph>

##### **Climate Variable Downscaling with Conditional Normalizing Flows**
2405.20719v1 by Christina Winkler, Paula Harder, David Rolnick

Predictions of global climate models typically operate on coarse spatial
scales due to the large computational costs of climate simulations. This has
led to a considerable interest in methods for statistical downscaling, a
similar process to super-resolution in the computer vision context, to provide
more local and regional climate information. In this work, we apply conditional
normalizing flows to the task of climate variable downscaling. We showcase its
successful performance on an ERA5 water content dataset for different
upsampling factors. Additionally, we show that the method allows us to assess
the predictive uncertainty in terms of standard deviation from the fitted
conditional distribution mean.

摘要：全球氣候模式的預測通常在粗略的空間尺度上進行，這是因為氣候模擬的計算成本高昂。這引起了對統計降尺度方法相當大的興趣，這是一個類似於電腦視覺背景中超解析度的過程，用於提供更多局部和區域氣候資訊。在這項工作中，我們將條件正規化流應用於氣候變數降尺度的任務。我們展示了其在不同上採樣因子下對 ERA5 含水量資料集的成功表現。此外，我們展示了該方法允許我們根據擬合條件分佈平均值的標準差評估預測不確定性。

##### **Popularity-Aware Alignment and Contrast for Mitigating Popularity Bias**
2405.20718v1 by Miaomiao Cai, Lei Chen, Yifan Wang, Haoyue Bai, Peijie Sun, Le Wu, Min Zhang, Meng Wang

Collaborative Filtering (CF) typically suffers from the significant challenge
of popularity bias due to the uneven distribution of items in real-world
datasets. This bias leads to a significant accuracy gap between popular and
unpopular items. It not only hinders accurate user preference understanding but
also exacerbates the Matthew effect in recommendation systems. To alleviate
popularity bias, existing efforts focus on emphasizing unpopular items or
separating the correlation between item representations and their popularity.
Despite the effectiveness, existing works still face two persistent challenges:
(1) how to extract common supervision signals from popular items to improve the
unpopular item representations, and (2) how to alleviate the representation
separation caused by popularity bias. In this work, we conduct an empirical
analysis of popularity bias and propose Popularity-Aware Alignment and Contrast
(PAAC) to address two challenges. Specifically, we use the common supervisory
signals modeled in popular item representations and propose a novel
popularity-aware supervised alignment module to learn unpopular item
representations. Additionally, we suggest re-weighting the contrastive learning
loss to mitigate the representation separation from a popularity-centric
perspective. Finally, we validate the effectiveness and rationale of PAAC in
mitigating popularity bias through extensive experiments on three real-world
datasets. Our code is available at
https://github.com/miaomiao-cai2/KDD2024-PAAC.

摘要：協同過濾 (CF) 通常會因為真實世界資料集中項目分佈不均而遭受顯著的熱門偏差挑戰。此偏差導致熱門和冷門項目之間的顯著準確度差距。它不僅妨礙準確了解使用者的偏好，還會加劇推薦系統中的馬太效應。為了減輕熱門偏差，現有的方法專注於強調冷門項目或分離項目表示與其熱門程度之間的關聯性。儘管有效，但現有的工作仍面臨兩個持續的挑戰：(1) 如何從熱門項目中提取共同的監督信號來改善冷門項目表示，以及 (2) 如何減輕熱門偏差造成的表示分離。在這項工作中，我們對熱門偏差進行了實證分析，並提出 Popularity-Aware Alignment and Contrast (PAAC) 來應對這兩個挑戰。具體來說，我們使用在熱門項目表示中建模的共同監督信號，並提出一個新穎的熱門感知監督式對齊模組來學習冷門項目表示。此外，我們建議重新加權對比學習損失，以從以熱門為中心的觀點減輕表示分離。最後，我們通過在三個真實世界資料集上進行廣泛的實驗，驗證了 PAAC 在減輕熱門偏差方面的有效性和原理。我們的程式碼可在 https://github.com/miaomiao-cai2/KDD2024-PAAC 取得。

##### **FinGen: A Dataset for Argument Generation in Finance**
2405.20708v1 by Chung-Chi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao

Thinking about the future is one of the important activities that people do
in daily life. Futurists also pay a lot of effort into figuring out possible
scenarios for the future. We argue that the exploration of this direction is
still in an early stage in the NLP research. To this end, we propose three
argument generation tasks in the financial application scenario. Our
experimental results show these tasks are still big challenges for
representative generation models. Based on our empirical results, we further
point out several unresolved issues and challenges in this research direction.

摘要：思考未來是人們在日常生活中重要的活動之一。未來學家也花費許多精力找出未來的可能情境。我們認為，在自然語言處理研究中，探索這個方向仍處於早期階段。為此，我們在財務應用情境中提出了三個論證生成任務。我們的實驗結果顯示，這些任務對於具有代表性的生成模型而言仍然是重大的挑戰。根據我們的實證結果，我們進一步指出這個研究方向中幾個尚未解決的問題和挑戰。

##### **ADESSE: Advice Explanations in Complex Repeated Decision-Making Environments**
2405.20705v1 by Sören Schleibaum, Lu Feng, Sarit Kraus, Jörg P. Müller

In the evolving landscape of human-centered AI, fostering a synergistic
relationship between humans and AI agents in decision-making processes stands
as a paramount challenge. This work considers a problem setup where an
intelligent agent comprising a neural network-based prediction component and a
deep reinforcement learning component provides advice to a human decision-maker
in complex repeated decision-making environments. Whether the human
decision-maker would follow the agent's advice depends on their beliefs and
trust in the agent and on their understanding of the advice itself. To this
end, we developed an approach named ADESSE to generate explanations about the
adviser agent to improve human trust and decision-making. Computational
experiments on a range of environments with varying model sizes demonstrate the
applicability and scalability of ADESSE. Furthermore, an interactive game-based
user study shows that participants were significantly more satisfied, achieved
a higher reward in the game, and took less time to select an action when
presented with explanations generated by ADESSE. These findings illuminate the
critical role of tailored, human-centered explanations in AI-assisted
decision-making.

摘要：在以人为中心的人工智慧不断演變的領域中，促進人類和人工智慧代理在決策過程中產生協同效應的關係，是一項至關重要的挑戰。本研究考慮了一個問題設定，其中一個由基於神經網路的預測組成和一個深度強化學習組成的智慧代理，在複雜的重複決策環境中向人類決策者提供建議。人類決策者是否會遵循代理的建議取決於他們對代理的信念和信任，以及他們對建議本身的理解。為此，我們開發了一種名為 ADESSE 的方法，以產生關於顧問代理的解釋，以提高人類的信任和決策能力。在具有不同模型大小的各種環境中進行的計算實驗證明了 ADESSE 的適用性和可擴充性。此外，一項基於互動遊戲的使用者研究表明，當參與者看到由 ADESSE 生成的解釋時，他們的滿意度顯著提高，在遊戲中獲得的獎勵更高，並且選擇動作所需的時間更短。這些發現闡明了量身定制、以人為中心的解釋在人工智慧輔助決策中的關鍵作用。

##### **It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance**
2405.20703v1 by Laura Cabello, Uchenna Akujuobi

Aspect-Based Sentiment Analysis (ABSA) involves extracting opinions from
textual data about specific entities and their corresponding aspects through
various complementary subtasks. Several prior research has focused on
developing ad hoc designs of varying complexities for these subtasks. In this
paper, we present a generative framework extensible to any ABSA subtask. We
build upon the instruction tuned model proposed by Scaria et al. (2023), who
present an instruction-based model with task descriptions followed by
in-context examples on ABSA subtasks. We propose PFInstruct, an extension to
this instruction learning paradigm by appending an NLP-related task prefix to
the task description. This simple approach leads to improved performance across
all tested SemEval subtasks, surpassing previous state-of-the-art (SOTA) on the
ATE subtask (Rest14) by +3.28 F1-score, and on the AOOE subtask by an average
of +5.43 F1-score across SemEval datasets. Furthermore, we explore the impact
of the prefix-enhanced prompt quality on the ABSA subtasks and find that even a
noisy prefix enhances model performance compared to the baseline. Our method
also achieves competitive results on a biomedical domain dataset (ERSA).

摘要：面向方面的观点分析 (ABSA) 涉及透過各種互補的子任務從文本資料中擷取特定實體及其對應方面的意見。許多先前的研究專注於為這些子任務開發具有不同複雜度的特定設計。在這篇論文中，我們提出一個可延伸至任何 ABSA 子任務的生成式架構。我們建立在 Scaria 等人 (2023) 提出的指令調整模型之上，他們提出一個基於指令的模型，其後接 ABSA 子任務的脈絡範例和任務說明。我們提出 PFInstruct，一種透過將與 NLP 相關的任務前置詞附加到任務說明中來延伸這個指令學習範例。這種簡單的方法可提升所有已測試 SemEval 子任務的效能，在 ATE 子任務 (Rest14) 上超越先前的最新技術 (SOTA) +3.28 F1 分數，在 AOOE 子任務上平均超越 SemEval 資料集 +5.43 F1 分數。此外，我們探討前置詞增強提示品質對 ABSA 子任務的影響，發現即使是有雜訊的前置詞，與基準線相比，也能提升模型效能。我們的模型在生物醫學領域資料集 (ERSA) 上也獲得有競爭力的結果。

##### **Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement**
2405.20701v1 by Pengwei Zhan, Zhen Xu, Qian Tan, Jie Song, Ru Xie

Large language models (LLMs) demonstrate exceptional instruct-following
ability to complete various downstream tasks. Although this impressive ability
makes LLMs flexible task solvers, their performance in solving tasks also
heavily relies on instructions. In this paper, we reveal that LLMs are
over-sensitive to lexical variations in task instructions, even when the
variations are imperceptible to humans. By providing models with neighborhood
instructions, which are closely situated in the latent representation space and
differ by only one semantically similar word, the performance on downstream
tasks can be vastly different. Following this property, we propose a black-box
Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE).
COPLE performs iterative lexical optimization according to the feedback from a
batch of proxy tasks, using a search strategy related to word influence.
Experiments show that even widely-used human-crafted prompts for current
benchmarks suffer from the lexical sensitivity of models, and COPLE recovers
the declined model ability in both instruct-following and solving downstream
tasks.

摘要：大型語言模型 (LLM) 展示出卓越的指令遵循能力，以完成各種下游任務。儘管這種令人印象深刻的能力使 LLM 成為靈活的任務解決器，但它們在解決任務中的表現也極度依賴於指令。在本文中，我們揭示了 LLM 對任務指令中的詞彙變異過度敏感，即使這些變異對人類來說是無法察覺的。通過為模型提供鄰域指令（這些指令在潛在表示空間中緊密相鄰，並且只有一個語義相似的詞不同），下游任務的表現可能會有很大不同。遵循此屬性，我們提出了用於提示詞彙增強 (COPLE) 的黑盒組合優化框架。COPLE 根據一批代理任務的回饋，使用與詞彙影響相關的搜尋策略，執行反覆的詞彙優化。實驗表明，即使是當前基準廣泛使用的由人類製作的提示也會受到模型的詞彙敏感性影響，而 COPLE 在指令遵循和解決下游任務中恢復了模型下降的能力。

##### **Self-degraded contrastive domain adaptation for industrial fault diagnosis with bi-imbalanced data**
2405.20700v1 by Gecheng Chen, Zeyu Yang, Chengwen Luo, Jianqiang Li

Modern industrial fault diagnosis tasks often face the combined challenge of
distribution discrepancy and bi-imbalance. Existing domain adaptation
approaches pay little attention to the prevailing bi-imbalance, leading to poor
domain adaptation performance or even negative transfer. In this work, we
propose a self-degraded contrastive domain adaptation (Sd-CDA) diagnosis
framework to handle the domain discrepancy under the bi-imbalanced data. It
first pre-trains the feature extractor via imbalance-aware contrastive learning
based on model pruning to learn the feature representation efficiently in a
self-supervised manner. Then it forces the samples away from the domain
boundary based on supervised contrastive domain adversarial learning
(SupCon-DA) and ensures the features generated by the feature extractor are
discriminative enough. Furthermore, we propose the pruned contrastive domain
adversarial learning (PSupCon-DA) to pay automatically re-weighted attention to
the minorities to enhance the performance towards bi-imbalanced data. We show
the superiority of the proposed method via two experiments.

摘要：現代工業故障診斷任務經常面臨分佈差異和二元不平衡的綜合挑戰。現有的域適應方法很少關注普遍存在的二元不平衡，導致域適應性能不佳甚至出現負傳遞。在這項工作中，我們提出了一個自降級對比域適應 (Sd-CDA) 診斷框架，以處理二元不平衡數據下的域差異。它首先通過基於模型剪枝的不平衡感知對比學習對特徵提取器進行預訓練，以自監督的方式有效地學習特徵表示。然後，它基於監督對比域對抗學習 (SupCon-DA) 強制樣本遠離域邊界，並確保特徵提取器產生的特徵具有足夠的區別性。此外，我們提出剪枝對比域對抗學習 (PSupCon-DA) 來自動重新加權關注少數群體，以提高對二元不平衡數據的性能。我們通過兩個實驗展示了所提出方法的優越性。

##### **Joint Embeddings for Graph Instruction Tuning**
2405.20684v1 by Vlad Argatu, Aaron Haag, Oliver Lohse

Large Language Models (LLMs) have achieved impressive performance in text
understanding and have become an essential tool for building smart assistants.
Originally focusing on text, they have been enhanced with multimodal
capabilities in recent works that successfully built visual instruction
following assistants. As far as the graph modality goes, however, no such
assistants have yet been developed. Graph structures are complex in that they
represent relation between different features and are permutation invariant.
Moreover, representing them in purely textual form does not always lead to good
LLM performance even for finetuned models. As a result, there is a need to
develop a new method to integrate graphs in LLMs for general graph
understanding. This work explores the integration of the graph modality in LLM
for general graph instruction following tasks. It aims at producing a deep
learning model that enhances an underlying LLM with graph embeddings and trains
it to understand them and to produce, given an instruction, an answer grounded
in the graph representation. The approach performs significantly better than a
graph to text approach and remains consistent even for larger graphs.

摘要：大型語言模型 (LLM) 在文字理解方面取得令人驚艷的表現，並已成為建構智慧助理的必要工具。原本專注於文字，它們在近期的作品中已透過多模式功能得到加強，成功建構出視覺指令追蹤助理。然而，就圖形模式而言，目前尚未開發出此類助理。圖形結構很複雜，在於它們表示不同特徵之間的關係，且具有排列不變性。此外，即使對於微調模型，將它們表示成純粹的文字形式並非總是能帶來良好的 LLM 效能。因此，需要開發一種新方法，將圖形整合到 LLM 中，以進行一般的圖形理解。本研究探討 LLM 中圖形模式的整合，以進行一般的圖形指令追蹤任務。其目標是產生一個深度學習模型，透過圖形嵌入增強基礎 LLM，並訓練它了解圖形嵌入，並根據指令產生以圖形表示為基礎的答案。此方法的表現顯著優於文字到文字的方法，即使對於較大的圖形也能保持一致性。

##### **No Free Lunch Theorem for Privacy-Preserving LLM Inference**
2405.20681v1 by Xiaojin Zhang, Yulin Fei, Yan Kang, Wei Chen, Lixin Fan, Hai Jin, Qiang Yang

Individuals and businesses have been significantly benefited by Large
Language Models (LLMs) including PaLM, Gemini and ChatGPT in various ways. For
example, LLMs enhance productivity, reduce costs, and enable us to focus on
more valuable tasks. Furthermore, LLMs possess the capacity to sift through
extensive datasets, uncover underlying patterns, and furnish critical insights
that propel the frontiers of technology and science. However, LLMs also pose
privacy concerns. Users' interactions with LLMs may expose their sensitive
personal or company information. A lack of robust privacy safeguards and legal
frameworks could permit the unwarranted intrusion or improper handling of
individual data, thereby risking infringements of privacy and the theft of
personal identities. To ensure privacy, it is essential to minimize the
dependency between shared prompts and private information. Various
randomization approaches have been proposed to protect prompts' privacy, but
they may incur utility loss compared to unprotected LLMs prompting. Therefore,
it is essential to evaluate the balance between the risk of privacy leakage and
loss of utility when conducting effective protection mechanisms. The current
study develops a framework for inferring privacy-protected Large Language
Models (LLMs) and lays down a solid theoretical basis for examining the
interplay between privacy preservation and utility. The core insight is
encapsulated within a theorem that is called as the NFL (abbreviation of the
word No-Free-Lunch) Theorem.

摘要：個人和企業已經從大型語言模型 (LLM)，包括 PaLM、Gemini 和 ChatGPT，在各種方面獲得顯著的好處。例如，LLM 提高生產力、降低成本，並使我們能夠專注於更有價值的任務。此外，LLM 具備篩選廣泛數據集、發現潛在模式，並提供推動技術和科學前沿的關鍵見解的能力。然而，LLM 也會引發隱私問題。使用者與 LLM 的互動可能會暴露其敏感的個人或公司資訊。缺乏強大的隱私保障措施和法律架構可能會允許對個人資料的非法入侵或不當處理，從而造成隱私權的侵犯和個人身分的盜用。為了確保隱私，最小化共享提示與私人資訊之間的依賴性至關重要。已經提出各種隨機化方法來保護提示的隱私，但與未受保護的 LLM 提示相比，它們可能會造成效用損失。因此，在執行有效的保護機制時，評估隱私洩露風險與效用損失之間的平衡至關重要。目前的研究開發了一個用於推論隱私保護大型語言模型 (LLM) 的框架，並為檢視隱私保護和效用之間的交互作用奠定了穩固的理論基礎。核心見解被封裝在一個被稱為 NFL（免費午餐定理的縮寫）定理中。

##### **Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models**
2405.20680v1 by Mingda Li, Xinyu Li, Yifan Chen, Wenfeng Xuan, Weinan Zhang

Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their
superiority in terms of factuality, they do not consistently outperform the
original retrieval-free Language Models (LMs). Our experiments reveal that this
example-level performance inconsistency exists not only between
retrieval-augmented and retrieval-free LM but also among different retrievers.
To understand this phenomenon, we investigate the degeneration behavior of
RALMs and theoretically decompose it into four categories. Further analysis
based on our decomposition reveals that the innate difference in knowledge
sources and the unpredictable degeneration of the reader model contribute most
to the inconsistency. Drawing from our analysis, we introduce Ensemble of
Retrievers (EoR), a trainable framework that can adaptively retrieve from
different knowledge sources and effectively decrease unpredictable reader
errors. Our experiments on Open Domain Question Answering show that EoR
substantially improves performance over the RALM with a single retriever by
considerably reducing inconsistent behaviors.

摘要：儘管檢索增強大型語言模型 (RALM) 在事實性方面展現其優越性，但它們並未持續優於原始的無檢索語言模型 (LM)。我們的實驗顯示，這種範例層級效能不一致性不僅存在於檢索增強和無檢索 LM 之間，也存在於不同的檢索器之間。為了了解此現象，我們探討 RALM 的退化行為，並從理論上將其分解成四種類別。根據我們的分解進一步分析顯示，知識來源的內在差異和閱讀器模型不可預測的退化對不一致性影響最大。根據我們的分析，我們引入了檢索器集合 (EoR)，這是一個可訓練架構，可以從不同的知識來源自適應地檢索，並有效降低不可預測的閱讀器錯誤。我們在開放領域問答上的實驗顯示，EoR 大幅提升了使用單一檢索器的 RALM 的效能，方法是大幅減少不一致的行為。

##### **Adv-KD: Adversarial Knowledge Distillation for Faster Diffusion Sampling**
2405.20675v1 by Kidist Amde Mekonnen, Nicola Dall'Asen, Paolo Rota

Diffusion Probabilistic Models (DPMs) have emerged as a powerful class of
deep generative models, achieving remarkable performance in image synthesis
tasks. However, these models face challenges in terms of widespread adoption
due to their reliance on sequential denoising steps during sample generation.
This dependence leads to substantial computational requirements, making them
unsuitable for resource-constrained or real-time processing systems. To address
these challenges, we propose a novel method that integrates denoising phases
directly into the model's architecture, thereby reducing the need for
resource-intensive computations. Our approach combines diffusion models with
generative adversarial networks (GANs) through knowledge distillation, enabling
more efficient training and evaluation. By utilizing a pre-trained diffusion
model as a teacher model, we train a student model through adversarial
learning, employing layerwise transformations for denoising and submodules for
predicting the teacher model's output at various points in time. This
integration significantly reduces the number of parameters and denoising steps
required, leading to improved sampling speed at test time. We validate our
method with extensive experiments, demonstrating comparable performance with
reduced computational requirements compared to existing approaches. By enabling
the deployment of diffusion models on resource-constrained devices, our
research mitigates their computational burden and paves the way for wider
accessibility and practical use across the research community and end-users.
  Our code is publicly available at https://github.com/kidist-amde/Adv-KD

摘要：擴散機率模型 (DPM) 已成為一種強大的深度生成模型，在影像合成任務中取得卓越的表現。然而，由於這些模型在樣本生成期間依賴於循序漸進的去噪步驟，因此在廣泛採用方面面臨挑戰。這種依賴性導致大量的運算需求，使它們不適合資源受限或即時處理系統。為了應對這些挑戰，我們提出了一種新方法，將去噪階段直接整合到模型架構中，從而減少對資源密集型運算的需求。我們的做法透過知識蒸餾結合擴散模型與生成對抗網路 (GAN)，實現更有效的訓練和評估。透過利用預先訓練的擴散模型作為教師模型，我們透過對抗式學習訓練學生模型，採用層級轉換進行去噪，並採用子模組在時間的不同點預測教師模型的輸出。這種整合顯著減少了所需的參數和去噪步驟數量，從而提高了測試時的抽樣速度。我們透過廣泛的實驗驗證了我們的方法，證明與現有方法相比，在降低運算需求的情況下具有可比較的效能。透過在資源受限的裝置上部署擴散模型，我們的研究減輕了它們的運算負擔，並為研究社群和最終使用者之間更廣泛的可及性和實際用途鋪平了道路。我們的程式碼可在 https://github.com/kidist-amde/Adv-KD 公開取得

##### **DORY: Deliberative Prompt Recovery for LLM**
2405.20657v1 by Lirong Gao, Ru Peng, Yiming Zhang, Junbo Zhao

Prompt recovery in large language models (LLMs) is crucial for understanding
how LLMs work and addressing concerns regarding privacy, copyright, etc. The
trend towards inference-only APIs complicates this task by restricting access
to essential outputs for recovery. To tackle this challenge, we extract
prompt-related information from limited outputs and identify a strong(negative)
correlation between output probability-based uncertainty and the success of
prompt recovery. This finding led to the development of Deliberative PrOmpt
RecoverY (DORY), our novel approach that leverages uncertainty to recover
prompts accurately. DORY involves reconstructing drafts from outputs, refining
these with hints, and filtering out noise based on uncertainty. Our evaluation
across diverse LLMs and prompt benchmarks shows that DORY outperforms existing
baselines, improving performance by approximately 10.82% and establishing a new
state-of-the-art record in prompt recovery tasks. Significantly, DORY operates
using a single LLM without any external resources or model, offering a
cost-effective, user-friendly prompt recovery solution.

摘要：大型語言模型 (LLM) 中的提示快速復原對於理解 LLM 的運作方式和解決隱私、版權等問題至關重要。僅限推論 API 的趨勢會限制取得復原所需的重要輸出，使這項任務變得更複雜。為了應對這項挑戰，我們從有限的輸出中擷取與提示相關的資訊，並找出輸出機率不確定性與提示復原成功與否之間的強（負）關聯性。這項發現促成了 Deliberative PrOmpt RecoverY (DORY) 的開發，這項創新方法利用不確定性來準確復原提示。DORY 涉及從輸出中重建草稿，利用提示加以精煉，並根據不確定性過濾雜訊。我們在各種 LLM 和提示基準中進行評估，結果顯示 DORY 優於現有的基準，將效能提升了約 10.82%，並在提示復原任務中創下新的技術水準。值得注意的是，DORY 使用單一 LLM 運作，不使用任何外部資源或模型，提供具成本效益且使用者友善的提示復原解決方案。

##### **Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models**
2405.20654v1 by Xuyang Wu, Zhiyuan Peng, Sravanthi Rajanala, Hsin-Tai Wu, Yi Fang

Effective passage retrieval and reranking methods have been widely utilized
to identify suitable candidates in open-domain question answering tasks, recent
studies have resorted to LLMs for reranking the retrieved passages by the
log-likelihood of the question conditioned on each passage. Although these
methods have demonstrated promising results, the performance is notably
sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs
can be computationally intensive and time-consuming. Furthermore, this approach
limits the leverage of question-passage relevance pairs and passage-specific
knowledge to enhance the ranking capabilities of LLMs. In this paper, we
propose passage-specific prompt tuning for reranking in open-domain question
answering (PSPT): a parameter-efficient method that fine-tunes learnable
passage-specific soft prompts, incorporating passage-specific knowledge from a
limited set of question-passage relevance pairs. The method involves ranking
retrieved passages based on the log-likelihood of the model generating the
question conditioned on each passage and the learned soft prompt. We conducted
extensive experiments utilizing the Llama-2-chat-7B model across three publicly
available open-domain question answering datasets and the results demonstrate
the effectiveness of the proposed approach.

摘要：有效的段落擷取和重新排序方法已被廣泛用於識別開放領域問題回答任務中的合適候選段落，最近的研究已訴諸 LLMs 透過每段落條件下的問題對數似然值對擷取的段落進行重新排序。儘管這些方法已展示出有希望的結果，但效能顯著地受到人工撰寫提示（或硬提示）的影響，且微調 LLM 可能在計算上很密集且耗時。此外，此方法限制了問題段落相關性對和段落特定知識的槓桿作用，以增強 LLM 的排序能力。在本文中，我們提出開放領域問題回答中的段落特定提示調整以進行重新排序（PSPT）：一種參數有效率的方法，微調可學習的段落特定軟提示，並從一組有限的問題段落相關性對中納入段落特定知識。此方法涉及根據模型在每段落條件下產生問題的對數似然值和學習的軟提示對擷取的段落進行排序。我們利用 Llama-2-chat-7B 模型對三個公開可用的開放領域問題回答資料集進行廣泛的實驗，結果證明了所提出方法的有效性。

##### **Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens**
2405.20653v1 by Jiahao Yu, Haozheng Luo, Jerry Yao-Chieh, Wenbo Guo, Han Liu, Xinyu Xing

Along with the remarkable successes of Language language models, recent
research also started to explore the security threats of LLMs, including
jailbreaking attacks. Attackers carefully craft jailbreaking prompts such that
a target LLM will respond to the harmful question. Existing jailbreaking
attacks require either human experts or leveraging complicated algorithms to
craft jailbreaking prompts. In this paper, we introduce BOOST, a simple attack
that leverages only the eos tokens. We demonstrate that rather than
constructing complicated jailbreaking prompts, the attacker can simply append a
few eos tokens to the end of a harmful question. It will bypass the safety
alignment of LLMs and lead to successful jailbreaking attacks. We further apply
BOOST to four representative jailbreak methods and show that the attack success
rates of these methods can be significantly enhanced by simply adding eos
tokens to the prompt. To understand this simple but novel phenomenon, we
conduct empirical analyses. Our analysis reveals that adding eos tokens makes
the target LLM believe the input is much less harmful, and eos tokens have low
attention values and do not affect LLM's understanding of the harmful
questions, leading the model to actually respond to the questions. Our findings
uncover how fragile an LLM is against jailbreak attacks, motivating the
development of strong safety alignment approaches.

摘要：隨著語言語言模型的顯著成功，最近的研究也開始探討 LLM 的安全威脅，包括越獄攻擊。攻擊者仔細製作越獄提示，以便目標 LLM 對有害問題做出回應。現有的越獄攻擊需要人類專家或利用複雜的演算法來製作越獄提示。在本文中，我們介紹了 BOOST，這是一種僅利用 eos 符號的簡單攻擊。我們證明，攻擊者不必建構複雜的越獄提示，只需在有害問題的結尾附加幾個 eos 符號即可。它將繞過 LLM 的安全比對，並導致成功的越獄攻擊。我們進一步將 BOOST 應用於四種代表性的越獄方法，並表明只需在提示中加入 eos 符號，即可顯著提高這些方法的攻擊成功率。為了理解這種簡單但新穎的現象，我們進行了實證分析。我們的分析表明，加入 eos 符號會讓目標 LLM 認為輸入的危害性小得多，而 eos 符號的注意力值低，且不會影響 LLM 對有害問題的理解，導致模型實際上對問題做出回應。我們的發現揭示了 LLM 對越獄攻擊的脆弱性，激勵了強大的安全比對方法的開發。

##### **Reward-based Input Construction for Cross-document Relation Extraction**
2405.20649v1 by Byeonghu Na, Suhyeon Jo, Yeongmin Kim, Il-Chul Moon

Relation extraction (RE) is a fundamental task in natural language
processing, aiming to identify relations between target entities in text. While
many RE methods are designed for a single sentence or document, cross-document
RE has emerged to address relations across multiple long documents. Given the
nature of long documents in cross-document RE, extracting document embeddings
is challenging due to the length constraints of pre-trained language models.
Therefore, we propose REward-based Input Construction (REIC), the first
learning-based sentence selector for cross-document RE. REIC extracts sentences
based on relational evidence, enabling the RE module to effectively infer
relations. Since supervision of evidence sentences is generally unavailable, we
train REIC using reinforcement learning with RE prediction scores as rewards.
Experimental results demonstrate the superiority of our method over heuristic
methods for different RE structures and backbones in cross-document RE. Our
code is publicly available at https://github.com/aailabkaist/REIC.

摘要：關係抽取 (RE) 是自然語言處理中一項基本任務，旨在找出文字中目標實體之間的關係。儘管許多 RE 方法是針對單一句子或文件設計，跨文件 RE 已應運而生，用於處理跨多個長文件的關係。考量到跨文件 RE 中長文件的性質，由於預訓練語言模型的長度限制，提取文件嵌入相當具有挑戰性。因此，我們提出基於 RE 獎勵的輸入建構 (REIC)，這是第一個用於跨文件 RE 的基於學習的句子選擇器。REIC 基於關係證據提取句子，讓 RE 模組能夠有效推論關係。由於證據句子的監督通常不可用，我們使用強化學習訓練 REIC，並以 RE 預測分數作為獎勵。實驗結果證明，在跨文件 RE 中，我們的模型優於不同 RE 結構和主幹的啟發式方法。我們的程式碼已公開發布於 https://github.com/aailabkaist/REIC。

##### **Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models For Video Captioning and Summarization**
2405.20648v1 by Richard Luo, Austin Peng, Adithya Vasudev, Rishabh Jain

Video is an increasingly prominent and information-dense medium, yet it poses
substantial challenges for language models. A typical video consists of a
sequence of shorter segments, or shots, that collectively form a coherent
narrative. Each shot is analogous to a word in a sentence where multiple data
streams of information (such as visual and auditory data) must be processed
simultaneously. Comprehension of the entire video requires not only
understanding the visual-audio information of each shot but also requires that
the model links the ideas between each shot to generate a larger,
all-encompassing story. Despite significant progress in the field, current
works often overlook videos' more granular shot-by-shot semantic information.
In this project, we propose a family of efficient large language vision models
(LLVMs) to boost video summarization and captioning called Shotluck Holmes. By
leveraging better pretraining and data collection strategies, we extend the
abilities of existing small LLVMs from being able to understand a picture to
being able to understand a sequence of frames. Specifically, we show that
Shotluck Holmes achieves better performance than state-of-the-art results on
the Shot2Story video captioning and summary task with significantly smaller and
more computationally efficient models.

摘要：影片是一種越來越顯著且資訊密集的媒介，但它對語言模型卻構成重大的挑戰。一部典型的影片包含一系列較短的片段或鏡頭，它們共同形成一個連貫的故事。每個鏡頭類似於句子中的單字，其中必須同時處理多個資訊資料串流（例如視覺和聽覺資料）。要理解整部影片，不僅需要理解每個鏡頭的視覺和聽覺資訊，還需要模型連結每個鏡頭之間的想法，以產生一個更大且包羅萬象的故事。儘管該領域有顯著的進展，但目前的著作經常忽略影片更精細的逐鏡頭語義資訊。在這個專案中，我們提出了一系列有效率的大型語言視覺模型 (LLVM)，以提升影片摘要和字幕，稱為 Shotluck Holmes。透過利用更好的預訓練和資料收集策略，我們擴充了現有小型 LLVMs 的能力，從能夠理解圖片到能夠理解一系列的影格。具體來說，我們證明 Shotluck Holmes 在 Shot2Story 影片字幕和摘要任務上，以顯著更小且計算效率更高的模型，達到了比最先進的結果更好的效能。

##### **Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item**
2405.20646v1 by Qidong Liu, Xian Wu, Xiangyu Zhao, Yejing Wang, Zijian Zhang, Feng Tian, Yefeng Zheng

Sequential recommendation systems (SRS) serve the purpose of predicting
users' subsequent preferences based on their past interactions and have been
applied across various domains such as e-commerce and social networking
platforms. However, practical SRS encounters challenges due to the fact that
most users engage with only a limited number of items, while the majority of
items are seldom consumed. These challenges, termed as the long-tail user and
long-tail item dilemmas, often create obstacles for traditional SRS methods.
Mitigating these challenges is crucial as they can significantly impact user
satisfaction and business profitability. While some research endeavors have
alleviated these issues, they still grapple with issues such as seesaw or noise
stemming from the scarcity of interactions. The emergence of large language
models (LLMs) presents a promising avenue to address these challenges from a
semantic standpoint. In this study, we introduce the Large Language Models
Enhancement framework for Sequential Recommendation (LLM-ESR), which leverages
semantic embeddings from LLMs to enhance SRS performance without increasing
computational overhead. To combat the long-tail item challenge, we propose a
dual-view modeling approach that fuses semantic information from LLMs with
collaborative signals from traditional SRS. To address the long-tail user
challenge, we introduce a retrieval augmented self-distillation technique to
refine user preference representations by incorporating richer interaction data
from similar users. Through comprehensive experiments conducted on three
authentic datasets using three widely used SRS models, our proposed enhancement
framework demonstrates superior performance compared to existing methodologies.

摘要：序貫推薦系統 (SRS) 的目的是根據使用者的過去互動預測他們後續的偏好，並已應用於電子商務和社群網路平台等各種領域。然而，實際的 SRS 會遇到挑戰，因為大多數使用者只會與有限數量的項目互動，而大多數項目很少被使用。這些挑戰被稱為長尾使用者和長尾項目困境，通常會為傳統 SRS 方法製造障礙。緩解這些挑戰至關重要，因為它們會對使用者滿意度和業務獲利能力產生重大影響。雖然有些研究努力已經緩解了這些問題，但它們仍然在因互動稀少而產生的蹺蹺板或雜訊等問題中掙扎。大型語言模型 (LLM) 的出現提供了一個從語義觀點解決這些挑戰的有希望的途徑。在本研究中，我們介紹了序貫推薦的大型語言模型增強架構 (LLM-ESR)，它利用來自 LLM 的語義嵌入來增強 SRS 效能，而不會增加運算負擔。為了應對長尾項目挑戰，我們提出了一種雙視角建模方法，該方法將來自 LLM 的語義資訊與來自傳統 SRS 的協同訊號融合在一起。為了應對長尾使用者挑戰，我們引入了一種檢索增強自我蒸餾技術，通過納入來自類似使用者的更豐富互動資料來改善使用者偏好表徵。透過使用三個廣泛使用的 SRS 模型在三個真實資料集上進行的全面實驗，我們提出的增強架構展示了優於現有方法的卓越效能。

##### **Learning Gaze-aware Compositional GAN**
2405.20643v1 by Nerea Aranjuelo, Siyu Huang, Ignacio Arganda-Carreras, Luis Unzueta, Oihana Otaegui, Hanspeter Pfister, Donglai Wei

Gaze-annotated facial data is crucial for training deep neural networks
(DNNs) for gaze estimation. However, obtaining these data is labor-intensive
and requires specialized equipment due to the challenge of accurately
annotating the gaze direction of a subject. In this work, we present a
generative framework to create annotated gaze data by leveraging the benefits
of labeled and unlabeled data sources. We propose a Gaze-aware Compositional
GAN that learns to generate annotated facial images from a limited labeled
dataset. Then we transfer this model to an unlabeled data domain to take
advantage of the diversity it provides. Experiments demonstrate our approach's
effectiveness in generating within-domain image augmentations in the ETH-XGaze
dataset and cross-domain augmentations in the CelebAMask-HQ dataset domain for
gaze estimation DNN training. We also show additional applications of our work,
which include facial image editing and gaze redirection.

摘要：用凝視標註的人臉資料對於訓練深度神經網路 (DNN) 以進行凝視估計至關重要。然而，取得這些資料需要大量人力，且由於準確標註受試者的凝視方向具有挑戰性，因此需要專業的設備。在這項工作中，我們提出了一個生成式架構，透過利用標籤資料和未標籤資料來源的優點，來建立標註的凝視資料。我們提出了一個凝視感知合成 GAN，該 GAN 會學習從有限的標籤資料集中產生標註的人臉影像。然後，我們將此模型轉移到未標籤的資料網域，以利用其所提供的多樣性。實驗證明了我們的方法在 ETH-XGaze 資料集中產生同網域影像擴充，以及在 CelebAMask-HQ 資料集網域中產生跨網域擴充的有效性，以進行凝視估計 DNN 訓練。我們也展示了我們工作的其他應用，包括人臉影像編輯和凝視重新導向。

##### **ToxVidLLM: A Multimodal LLM-based Framework for Toxicity Detection in Code-Mixed Videos**
2405.20628v1 by Krishanu Maity, A. S. Poornash, Sriparna Saha, Pushpak Bhattacharyya

In an era of rapidly evolving internet technology, the surge in multimodal
content, including videos, has expanded the horizons of online communication.
However, the detection of toxic content in this diverse landscape, particularly
in low-resource code-mixed languages, remains a critical challenge. While
substantial research has addressed toxic content detection in textual data, the
realm of video content, especially in non-English languages, has been
relatively underexplored. This paper addresses this research gap by introducing
a benchmark dataset, the first of its kind, consisting of 931 videos with 4021
code-mixed Hindi-English utterances collected from YouTube. Each utterance
within this dataset has been meticulously annotated for toxicity, severity, and
sentiment labels. We have developed an advanced Multimodal Multitask framework
built for Toxicity detection in Video Content by leveraging Large Language
Models (LLMs), crafted for the primary objective along with the additional
tasks of conducting sentiment and severity analysis. ToxVidLLM incorporates
three key modules the Encoder module, Cross-Modal Synchronization module, and
Multitask module crafting a generic multimodal LLM customized for intricate
video classification tasks. Our experiments reveal that incorporating multiple
modalities from the videos substantially enhances the performance of toxic
content detection by achieving an Accuracy and Weighted F1 score of 94.29% and
94.35%, respectively.

摘要：在互联网技术快速发展的时代，包括视频在内的多模态内容激增，拓宽了在线交流的视野。然而，在这一多元化的领域中检测有毒内容，尤其是在资源匮乏的代码混合语言中，仍然是一项严峻的挑战。虽然大量研究已经解决了文本数据中的有毒内容检测，但视频内容领域，尤其是非英语语言，相对而言尚未得到充分探索。本文通过引入基准数据集来解决这一研究空白，该数据集是同类数据集中的第一个，包含 931 个视频，其中有 4021 个代码混合的印地语-英语话语，这些话语是从 YouTube 收集的。该数据集中的每个话语都经过精心注释，以表示毒性、严重性和情绪标签。我们开发了一个先进的多模态多任务框架，用于通过利用大型语言模型 (LLM) 来检测视频内容中的毒性，该框架专为主要目标以及执行情绪和严重性分析的附加任务而设计。ToxVidLLM 采用了三个关键模块：编码器模块、跨模态同步模块和多任务模块，打造了一个针对复杂视频分类任务定制的通用多模态 LLM。我们的实验表明，通过视频合并多种模态可以大幅提高有毒内容检测的性能，准确率和加权 F1 分别达到 94.29% 和 94.35%。

##### **Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning**
2405.20625v1 by Atharva Gundawar, Mudit Verma, Lin Guan, Karthik Valmeekam, Siddhant Bhambri, Subbarao Kambhampati

As the applicability of Large Language Models (LLMs) extends beyond
traditional text processing tasks, there is a burgeoning interest in their
potential to excel in planning and reasoning assignments, realms traditionally
reserved for System 2 cognitive competencies. Despite their perceived
versatility, the research community is still unraveling effective strategies to
harness these models in such complex domains. The recent discourse introduced
by the paper on LLM Modulo marks a significant stride, proposing a conceptual
framework that enhances the integration of LLMs into diverse planning and
reasoning activities. This workshop paper delves into the practical application
of this framework within the domain of travel planning, presenting a specific
instance of its implementation. We are using the Travel Planning benchmark by
the OSU NLP group, a benchmark for evaluating the performance of LLMs in
producing valid itineraries based on user queries presented in natural
language. While popular methods of enhancing the reasoning abilities of LLMs
such as Chain of Thought, ReAct, and Reflexion achieve a meager 0%, 0.6%, and
0% with GPT3.5-Turbo respectively, our operationalization of the LLM-Modulo
framework for TravelPlanning domain provides a remarkable improvement,
enhancing baseline performances by 4.6x for GPT4-Turbo and even more for older
models like GPT3.5-Turbo from 0% to 5%. Furthermore, we highlight the other
useful roles of LLMs in the planning pipeline, as suggested in LLM-Modulo,
which can be reliably operationalized such as extraction of useful critics and
reformulator for critics.

摘要：隨著大型語言模型 (LLM) 的適用性超越傳統的文字處理任務，它們在規劃和推理任務中表現出色的潛力引起了極大的興趣，這些領域傳統上是 System 2 認知能力的專屬領域。儘管它們被認為具有多功能性，但研究社群仍在探討在如此複雜的領域中有效利用這些模型的策略。LLM Modulo 論文中最近提出的論述標誌著一個重要的進展，提出了增強 LLM 與各種規劃和推理活動整合的概念框架。這篇研討會論文深入探討了這個框架在旅遊規劃領域中的實際應用，並展示了一個具體的實作範例。我們使用了俄亥俄州立大學自然語言處理小組的旅遊規劃基準，這個基準用於評估 LLM 在根據以自然語言呈現的使用者查詢產生有效行程方面的表現。雖然增強 LLM 推理能力的流行方法，例如思維鏈、ReAct 和 Reflexion，分別使用 GPT3.5-Turbo 達到了微不足道的 0%、0.6% 和 0%，但我們將 LLM-Modulo 框架操作化到旅遊規劃領域，提供了顯著的改進，將 GPT4-Turbo 的基準表現提升了 4.6 倍，甚至將 GPT3.5-Turbo 等較舊模型從 0% 提升到 5%。此外，我們強調了 LLM 在規劃流程中其他有用的角色，正如 LLM-Modulo 所建議的，這些角色可以被可靠地操作化，例如提取有用的批評和批評的重新表述。

##### **Leveraging Large Language Models for Entity Matching**
2405.20624v1 by Qianyu Huang, Tongfang Zhao

Entity matching (EM) is a critical task in data integration, aiming to
identify records across different datasets that refer to the same real-world
entities. Traditional methods often rely on manually engineered features and
rule-based systems, which struggle with diverse and unstructured data. The
emergence of Large Language Models (LLMs) such as GPT-4 offers transformative
potential for EM, leveraging their advanced semantic understanding and
contextual capabilities. This vision paper explores the application of LLMs to
EM, discussing their advantages, challenges, and future research directions.
Additionally, we review related work on applying weak supervision and
unsupervised approaches to EM, highlighting how LLMs can enhance these methods.

摘要：實體配對 (EM) 是資料整合中的關鍵任務，目標是識別不同資料集中的記錄，這些記錄指的是同一個真實世界的實體。傳統的方法通常依賴於人工設計的特徵和基於規則的系統，這些系統難以處理多樣化且非結構化的資料。大型語言模型 (LLM)（例如 GPT-4）的出現為 EM 提供了變革性的潛力，利用它們先進的語義理解和上下文能力。本願景文件探討了 LLM 在 EM 中的應用，討論了它們的優點、挑戰和未來的研究方向。此外，我們回顧了在 EM 中應用弱監督和非監督方法的相關工作，強調了 LLM 如何增強這些方法。

##### **FineRadScore: A Radiology Report Line-by-Line Evaluation Technique Generating Corrections with Severity Scores**
2405.20613v1 by Alyssa Huang, Oishi Banerjee, Kay Wu, Eduardo Pontes Reis, Pranav Rajpurkar

The current gold standard for evaluating generated chest x-ray (CXR) reports
is through radiologist annotations. However, this process can be extremely
time-consuming and costly, especially when evaluating large numbers of reports.
In this work, we present FineRadScore, a Large Language Model (LLM)-based
automated evaluation metric for generated CXR reports. Given a candidate report
and a ground-truth report, FineRadScore gives the minimum number of
line-by-line corrections required to go from the candidate to the ground-truth
report. Additionally, FineRadScore provides an error severity rating with each
correction and generates comments explaining why the correction was needed. We
demonstrate that FineRadScore's corrections and error severity scores align
with radiologist opinions. We also show that, when used to judge the quality of
the report as a whole, FineRadScore aligns with radiologists as well as current
state-of-the-art automated CXR evaluation metrics. Finally, we analyze
FineRadScore's shortcomings to provide suggestions for future improvements.

摘要：目前評估生成的胸部 X 光 (CXR) 報告的黃金標準是透過放射科醫師註解。然而，這個過程可能非常耗時且昂貴，特別是在評估大量的報告時。在這項工作中，我們提出 FineRadScore，一種基於大型語言模型 (LLM) 的自動化評估指標，用於生成的 CXR 報告。給定候選報告和基本事實報告，FineRadScore 會提供從候選報告到基本事實報告所需的最少逐行更正次數。此外，FineRadScore 會提供每個更正的錯誤嚴重性評分，並產生說明為何需要更正的評論。我們證明 FineRadScore 的更正和錯誤嚴重性評分與放射科醫師的意見一致。我們也顯示，當用於判斷報告的整體品質時，FineRadScore 與放射科醫師以及目前最先進的自動化 CXR 評估指標一致。最後，我們分析 FineRadScore 的缺點，以提供未來改進的建議。

##### **UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation**
2405.20612v1 by Hanzhang Zhou, Zijian Feng, Zixiao Zhu, Junlang Qian, Kezhi Mao

Large language models (LLMs) have demonstrated impressive capabilities in
various tasks using the in-context learning (ICL) paradigm. However, their
effectiveness is often compromised by inherent bias, leading to prompt
brittleness, i.e., sensitivity to design settings such as example selection,
order, and prompt formatting. Previous studies have addressed LLM bias through
external adjustment of model outputs, but the internal mechanisms that lead to
such bias remain unexplored. Our work delves into these mechanisms,
particularly investigating how feedforward neural networks (FFNs) and attention
heads result in the bias of LLMs. By Interpreting the contribution of
individual FFN vectors and attention heads, we identify the biased LLM
components that skew LLMs' prediction toward specific labels. To mitigate these
biases, we introduce UniBias, an inference-only method that effectively
identifies and eliminates biased FFN vectors and attention heads. Extensive
experiments across 12 NLP datasets demonstrate that UniBias significantly
enhances ICL performance and alleviates prompt brittleness of LLMs.

摘要：大型語言模型 (LLM) 已在使用情境學習 (ICL) 典範的各種任務中展現出令人印象深刻的能力。然而，它們的有效性常常受到內在偏差的影響，導致提示脆弱性，即對範例選擇、順序和提示格式等設計設定的敏感性。先前的研究已透過模型輸出的外部調整來解決 LLM 偏差，但導致此類偏差的內部機制仍未探討。我們的研究深入探討這些機制，特別是調查前饋神經網路 (FFN) 和注意力頭如何導致 LLM 的偏差。透過詮釋個別 FFN 向量和注意力頭的貢獻，我們找出偏差的 LLM 組件，使 LLM 的預測偏向特定標籤。為了減輕這些偏差，我們引入了 UniBias，這是一種僅限推論的方法，可以有效找出並消除有偏差的 FFN 向量和注意力頭。在 12 個 NLP 資料集中的廣泛實驗證明，UniBias 大幅提升了 ICL 效能，並減輕了 LLM 的提示脆弱性。

##### **Bi-Directional Transformers vs. word2vec: Discovering Vulnerabilities in Lifted Compiled Code**
2405.20611v1 by Gary A. McCully, John D. Hastings, Shengjie Xu, Adam Fortier

Detecting vulnerabilities within compiled binaries is challenging due to lost
high-level code structures and other factors such as architectural
dependencies, compilers, and optimization options. To address these obstacles,
this research explores vulnerability detection by using natural language
processing (NLP) embedding techniques with word2vec, BERT, and RoBERTa to learn
semantics from intermediate representation (LLVM) code. Long short-term memory
(LSTM) neural networks were trained on embeddings from encoders created using
approximately 118k LLVM functions from the Juliet dataset. This study is
pioneering in its comparison of word2vec models with multiple bidirectional
transformer (BERT, RoBERTa) embeddings built using LLVM code to train neural
networks to detect vulnerabilities in compiled binaries. word2vec Continuous
Bag of Words (CBOW) models achieved 92.3% validation accuracy in detecting
vulnerabilities, outperforming word2vec Skip-Gram, BERT, and RoBERTa. This
suggests that complex contextual NLP embeddings may not provide advantages over
simpler word2vec models for this task when a limited number (e.g. 118K) of data
samples are used to train the bidirectional transformer-based models. The
comparative results provide novel insights into selecting optimal embeddings
for learning compiler-independent semantic code representations to advance
machine learning detection of vulnerabilities in compiled binaries.

摘要：偵測編譯二進位檔中的弱點具有挑戰性，原因在於遺失了高階程式碼結構以及架構相依性、編譯器和最佳化選項等其他因素。為了克服這些障礙，本研究探索使用自然語言處理 (NLP) 嵌入技術，搭配 word2vec、BERT 和 RoBERTa，透過中間表示 (LLVM) 程式碼學習語意，以偵測弱點。長期短期記憶 (LSTM) 神經網路經過訓練，使用約 118k 個來自 Juliet 資料集的 LLVM 函數所建立的編碼器中的嵌入。本研究率先比較 word2vec 模型與使用 LLVM 程式碼建立的多個雙向轉換器 (BERT、RoBERTa) 嵌入，以訓練神經網路來偵測編譯二進位檔中的弱點。word2vec 連續詞袋 (CBOW) 模型在偵測弱點方面達到了 92.3% 的驗證準確度，優於 word2vec Skip-Gram、BERT 和 RoBERTa。這表示複雜的脈絡 NLP 嵌入在使用有限數目 (例如 118K) 的資料範本來訓練基於雙向轉換器的模型時，可能無法為此任務提供優於更簡單的 word2vec 模型的優勢。比較結果提供了新的見解，可用於選擇最佳嵌入，以學習與編譯器無關的語意程式碼表示，以提升機器學習偵測編譯二進位檔中弱點的能力。

##### **Vision-Language Meets the Skeleton: Progressively Distillation with Cross-Modal Knowledge for 3D Action Representation Learning**
2405.20606v1 by Yang Chen, Tian He, Junfeng Fu, Ling Wang, Jingcai Guo, Hong Cheng

Supervised and self-supervised learning are two main training paradigms for
skeleton-based human action recognition. However, the former one-hot
classification requires labor-intensive predefined action categories
annotations, while the latter involves skeleton transformations (e.g.,
cropping) in the pretext tasks that may impair the skeleton structure. To
address these challenges, we introduce a novel skeleton-based training
framework (C$^2$VL) based on Cross-modal Contrastive learning that uses the
progressive distillation to learn task-agnostic human skeleton action
representation from the Vision-Language knowledge prompts. Specifically, we
establish the vision-language action concept space through vision-language
knowledge prompts generated by pre-trained large multimodal models (LMMs),
which enrich the fine-grained details that the skeleton action space lacks.
Moreover, we propose the intra-modal self-similarity and inter-modal
cross-consistency softened targets in the cross-modal contrastive process to
progressively control and guide the degree of pulling vision-language knowledge
prompts and corresponding skeletons closer. These soft instance discrimination
and self-knowledge distillation strategies contribute to the learning of better
skeleton-based action representations from the noisy skeleton-vision-language
pairs. During the inference phase, our method requires only the skeleton data
as the input for action recognition and no longer for vision-language prompts.
Extensive experiments show that our method achieves state-of-the-art results on
NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets. The code will be available
in the future.

摘要：<paragraph>監督式和自監督式學習是基於骨架的人類動作辨識的兩種主要訓練範例。然而，前者的一熱編碼分類需要大量人工預定義的動作類別註解，而後者則涉及在可能損害骨架結構的藉口任務中進行骨架轉換（例如，裁剪）。為了應對這些挑戰，我們引入了一個基於跨模態對比學習的新型基於骨架的訓練框架（C$^2$VL），該框架使用漸進式蒸餾從視覺語言知識提示中學習與任務無關的人體骨架動作表示。具體來說，我們通過預先訓練的大型多模態模型（LMM）產生的視覺語言知識提示建立了視覺語言動作概念空間，這豐富了骨架動作空間所缺乏的細粒度細節。此外，我們在跨模態對比過程中提出了模態內自相似性和模態間交叉一致性軟化目標，以漸進式地控制和引導拉動視覺語言知識提示和相應骨架更接近的程度。這些軟實例區分和自我知識蒸餾策略有助於從嘈雜的骨架視覺語言對中學習更好的基於骨架的動作表示。在推理階段，我們的方法只需要骨架數據作為動作識別的輸入，不再需要視覺語言提示。大量的實驗表明，我們的的方法在 NTU RGB+D 60、NTU RGB+D 120 和 PKU-MMD 數據集上取得了最先進的結果。代碼將在未來提供。</paragraph>

##### **Searching for internal symbols underlying deep learning**
2405.20605v1 by Jung H. Lee, Sujith Vijayan

Deep learning (DL) enables deep neural networks (DNNs) to automatically learn
complex tasks or rules from given examples without instructions or guiding
principles. As we do not engineer DNNs' functions, it is extremely difficult to
diagnose their decisions, and multiple lines of studies proposed to explain
principles of DNNs/DL operations. Notably, one line of studies suggests that
DNNs may learn concepts, the high level features recognizable to humans. Thus,
we hypothesized that DNNs develop abstract codes, not necessarily recognizable
to humans, which can be used to augment DNNs' decision-making. To address this
hypothesis, we combined foundation segmentation models and unsupervised
learning to extract internal codes and identify potential use of abstract codes
to make DL's decision-making more reliable and safer.

摘要：深度學習 (DL) 使得深度神經網路 (DNN) 能夠自動學習複雜任務或規則，而無需說明或指導原則。由於我們並未設計 DNN 的功能，因此極難診斷其決策，而多條研究路線則提出解釋 DNN/DL 運作原理。值得注意的是，一條研究路線表明 DNN 可能學習人類可識別的高階特徵概念。因此，我們假設 DNN 會開發出抽象代碼，而這些代碼不一定為人類所識別，可用於擴增 DNN 的決策制定。為了解決這個假設，我們結合基礎分割模型和無監督學習，以提取內部代碼並找出抽象代碼的潛在用途，讓 DL 的決策制定更可靠且更安全。

##### **Advancing Financial Risk Prediction Through Optimized LSTM Model Performance and Comparative Analysis**
2405.20603v1 by Ke Xu, Yu Cheng, Shiqing Long, Junjie Guo, Jue Xiao, Mengfang Sun

This paper focuses on the application and optimization of LSTM model in
financial risk prediction. The study starts with an overview of the
architecture and algorithm foundation of LSTM, and then details the model
training process and hyperparameter tuning strategy, and adjusts network
parameters through experiments to improve performance. Comparative experiments
show that the optimized LSTM model shows significant advantages in AUC index
compared with random forest, BP neural network and XGBoost, which verifies its
efficiency and practicability in the field of financial risk prediction,
especially its ability to deal with complex time series data, which lays a
solid foundation for the application of the model in the actual production
environment.

摘要：本文重點探討 LSTM 模型在金融風險預測中的應用與優化。本研究首先概述 LSTM 的架構與演算法基礎，接著詳細說明模型訓練流程與超參數調整策略，並透過實驗調整網路參數以提升效能。比較實驗結果顯示，優化的 LSTM 模型在 AUC 指標上較隨機森林、BP 神經網路與 XGBoost 有顯著優勢，驗證其在金融風險預測領域的效能與實用性，特別是處理複雜時間序列資料的能力，為模型在實際生產環境中的應用奠定良好的基礎。

##### **Masked Language Modeling Becomes Conditional Density Estimation for Tabular Data Synthesis**
2405.20602v1 by Seunghwan An, Gyeongdong Woo, Jaesung Lim, ChangHyun Kim, Sungchul Hong, Jong-June Jeon

In this paper, our goal is to generate synthetic data for heterogeneous
(mixed-type) tabular datasets with high machine learning utility (MLu). Given
that the MLu performance relies on accurately approximating the conditional
distributions, we focus on devising a synthetic data generation method based on
conditional distribution estimation. We propose a novel synthetic data
generation method, MaCoDE, by redefining the multi-class classification task of
Masked Language Modeling (MLM) as histogram-based non-parametric conditional
density estimation. Our proposed method enables estimating conditional
densities across arbitrary combinations of target and conditional variables.
Furthermore, we demonstrate that our proposed method bridges the theoretical
gap between distributional learning and MLM. To validate the effectiveness of
our proposed model, we conduct synthetic data generation experiments on 10
real-world datasets. Given the analogy between predicting masked input tokens
in MLM and missing data imputation, we also evaluate the performance of
multiple imputations on incomplete datasets with various missing data
mechanisms. Moreover, our proposed model offers the advantage of enabling
adjustments to data privacy levels without requiring re-training.

摘要：在本文中，我们的目标是针对异构（混合类型）表格数据集生成具有高机器学习效用（MLu）的合成数据。鉴于 MLu 性能依赖于准确地逼近条件分布，因此我们专注于设计一种基于条件分布估计的合成数据生成方法。我们提出了一种新颖的合成数据生成方法 MaCoDE，方法是将掩码语言建模（MLM）的多类分类任务重新定义为基于直方图的非参数条件密度估计。我们提出的方法能够估计目标变量和条件变量的任意组合的条件密度。此外，我们证明了我们提出的方法弥合了分布式学习和 MLM 之间的理论差距。为了验证我们提出的模型的有效性，我们在 10 个真实世界的数据集上进行了合成数据生成实验。鉴于预测 MLM 中的掩码输入标记与缺失数据插补之间的类比，我们还评估了在具有各种缺失数据机制的不完整数据集上进行多次插补的性能。此外，我们提出的模型具有调整数据隐私级别的优势，而无需重新训练。

##### **Multi-label Class Incremental Emotion Decoding with Augmented Emotional Semantics Learning**
2405.20600v1 by Kaicheng Fu, Changde Du, Xiaoyu Chen, Jie Peng, Huiguang He

Emotion decoding plays an important role in affective human-computer
interaction. However, previous studies ignored the dynamic real-world scenario,
where human experience a blend of multiple emotions which are incrementally
integrated into the model, leading to the multi-label class incremental
learning (MLCIL) problem. Existing methods have difficulty in solving MLCIL
issue due to notorious catastrophic forgetting caused by partial label problem
and inadequate label semantics mining. In this paper, we propose an augmented
emotional semantics learning framework for multi-label class incremental
emotion decoding. Specifically, we design an augmented emotional relation graph
module with label disambiguation to handle the past-missing partial label
problem. Then, we leverage domain knowledge from affective dimension space to
alleviate future-missing partial label problem by knowledge distillation.
Besides, an emotional semantics learning module is constructed with a graph
autoencoder to obtain emotion embeddings in order to guide the
semantic-specific feature decoupling for better multi-label learning. Extensive
experiments on three datasets show the superiority of our method for improving
emotion decoding performance and mitigating forgetting on MLCIL problem.

摘要：情感解码在情感人机互动中扮演着重要的角色。然而，先前的研究忽略了动态的真实世界场景，其中人类体验到多种情感的融合，这些情感逐渐整合到模型中，从而导致多标签类增量学习 (MLCIL) 问题。由于部分标签问题和标签语义挖掘不足导致的灾难性遗忘，现有方法难以解决 MLCIL 问题。在本文中，我们提出了一种用于多标签类增量情感解码的增强型情感语义学习框架。具体来说，我们设计了一个带有标签消歧的增强型情感关系图模块来处理过去缺失的部分标签问题。然后，我们利用情感维度空间中的领域知识通过知识蒸馏来缓解未来缺失的部分标签问题。此外，我们构建了一个带有图自动编码器的语义情感学习模块，以获取情感嵌入，以便指导语义特定特征解耦，从而实现更好的多标签学习。在三个数据集上进行的广泛实验表明，我们的方法在提高情感解码性能和减轻 MLCIL 问题上的遗忘方面具有优越性。

##### **Class-Based Time Series Data Augmentation to Mitigate Extreme Class Imbalance for Solar Flare Prediction**
2405.20590v1 by Junzhi Wen, Rafal A. Angryk

Time series data plays a crucial role across various domains, making it
valuable for decision-making and predictive modeling. Machine learning (ML) and
deep learning (DL) have shown promise in this regard, yet their performance
hinges on data quality and quantity, often constrained by data scarcity and
class imbalance, particularly for rare events like solar flares. Data
augmentation techniques offer a potential solution to address these challenges,
yet their effectiveness on multivariate time series datasets remains
underexplored. In this study, we propose a novel data augmentation method for
time series data named Mean Gaussian Noise (MGN). We investigate the
performance of MGN compared to eight existing basic data augmentation methods
on a multivariate time series dataset for solar flare prediction, SWAN-SF,
using a ML algorithm for time series data, TimeSeriesSVC. The results
demonstrate the efficacy of MGN and highlight its potential for improving
classification performance in scenarios with extremely imbalanced data. Our
time complexity analysis shows that MGN also has a competitive computational
cost compared to the investigated alternative methods.

摘要：時序資料在各個領域中扮演著至關重要的角色，使其對於決策制定和預測模型而言極具價值。機器學習 (ML) 和深度學習 (DL) 已展現出在這方面的希望，但其效能取決於資料品質和數量，而資料稀少性和類別不平衡通常會對其造成限制，特別是對於太陽耀斑等罕見事件而言。資料擴充技術提供了解決這些挑戰的潛在方案，但其在多變量時序資料集上的有效性仍未充分探討。在本研究中，我們提出了一種名為平均高斯雜訊 (MGN) 的時序資料新資料擴充方法。我們在時序資料機器學習演算法 TimeSeriesSVC 上，針對太陽耀斑預測的多變量時序資料集 SWAN-SF，探討了 MGN 與八種現有基本資料擴充方法的效能。結果證明了 MGN 的效能，並強調了其在極度不平衡資料場景中改善分類效能的潛力。我們的時間複雜度分析顯示，與所探討的替代方法相比，MGN 也具有競爭力的運算成本。

##### **Selective Knowledge Sharing for Personalized Federated Learning Under Capacity Heterogeneity**
2405.20589v1 by Zheng Wang, Zheng Wang, Zhaopeng Peng, Zihui Wang, Cheng Wang

Federated Learning (FL) stands to gain significant advantages from
collaboratively training capacity-heterogeneous models, enabling the
utilization of private data and computing power from low-capacity devices.
However, the focus on personalizing capacity-heterogeneous models based on
client-specific data has been limited, resulting in suboptimal local model
utility, particularly for low-capacity clients. The heterogeneity in both data
and device capacity poses two key challenges for model personalization: 1)
accurately retaining necessary knowledge embedded within reduced submodels for
each client, and 2) effectively sharing knowledge through aggregating
size-varying parameters. To this end, we introduce Pa3dFL, a novel framework
designed to enhance local model performance by decoupling and selectively
sharing knowledge among capacity-heterogeneous models. First, we decompose each
layer of the model into general and personal parameters. Then, we maintain
uniform sizes for the general parameters across clients and aggregate them
through direct averaging. Subsequently, we employ a hyper-network to generate
size-varying personal parameters for clients using learnable embeddings.
Finally, we facilitate the implicit aggregation of personal parameters by
aggregating client embeddings through a self-attention module. We conducted
extensive experiments on three datasets to evaluate the effectiveness of
Pa3dFL. Our findings indicate that Pa3dFL consistently outperforms baseline
methods across various heterogeneity settings. Moreover, Pa3dFL demonstrates
competitive communication and computation efficiency compared to baseline
approaches, highlighting its practicality and adaptability in adverse system
conditions.

摘要：聯邦學習 (FL) 可透過協作訓練容量異質模型獲得顯著優勢，並能利用低容量裝置中的私人資料和運算能力。然而，針對客戶特定資料個人化容量異質模型的重點有限，導致次佳的在地模型效用，特別是對於低容量客戶而言。資料和裝置容量的異質性對模型個人化造成兩項關鍵挑戰：1) 準確保留嵌入在每個客戶的簡化子模型中的必要知識，以及 2) 有效地透過彙總大小不同的參數來分享知識。為此，我們引進 Pa3dFL，一個新穎的架構，旨在透過分離和選擇性地分享容量異質模型之間的知識來提升在地模型效能。首先，我們將模型的每一層分解成一般參數和個人參數。接著，我們維持客戶之間一般參數的均一大小，並透過直接平均來彙總它們。隨後，我們採用一個超網路來使用可學習嵌入為客戶產生大小不同的個人參數。最後，我們透過自注意力模組彙總客戶嵌入來促進個人參數的隱式彙總。我們在三個資料集上進行廣泛的實驗，以評估 Pa3dFL 的效能。我們的發現指出，在各種異質性設定中，Pa3dFL 持續優於基準方法。此外，與基準方法相比，Pa3dFL 展現出具競爭力的通訊和運算效率，突顯其在不利的系統條件下的實用性和適應性。

##### **DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models**
2405.20588v1 by Taolin Zhang, Qizhou Chen, Dongyang Li, Chengyu Wang, Xiaofeng He, Longtao Huang, Hui Xue, Jun Huang

Recently, while large language models (LLMs) have demonstrated impressive
results, they still suffer from hallucination, i.e., the generation of false
information. Model editing is the task of fixing factual mistakes in LLMs; yet,
most previous works treat it as a one-time task, paying little attention to
ever-emerging mistakes generated by LLMs. We address the task of sequential
model editing (SME) that aims to rectify mistakes continuously. A Dynamic
Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic
interaction among the factual knowledge within the entire sequence, preventing
catastrophic forgetting during the editing process of multiple knowledge
triples. Specifically, (1) for semantic fusion within a relation triple, we
aggregate the intra-editing attention flow into auto-regressive self-attention
with token-level granularity in LLMs. We further leverage multi-layer diagonal
inter-editing attention flow to update the weighted representations of the
entire sequence-level granularity. (2) Considering that auxiliary parameters
are required to store the knowledge for sequential editing, we construct a new
dataset named \textbf{DAFSet}, fulfilling recent, popular, long-tail and robust
properties to enhance the generality of sequential editing. Experiments show
DAFNet significantly outperforms strong baselines in single-turn and sequential
editing. The usage of DAFSet also consistently improves the performance of
other auxiliary network-based methods in various scenarios

摘要：<paragraph>近期，虽然大型语言模型 (LLM) 已展示出令人印象深刻的结果，但它们仍饱受幻觉困扰，即生成虚假信息。模型编辑是修复 LLM 中事实错误的任务；然而，大多数以前的作品将其视为一次性任务，很少关注 LLM 生成的不断出现的错误。我们解决顺序模型编辑 (SME) 的任务，旨在持续纠正错误。动态辅助融合网络 (DAFNet) 被设计用于增强整个序列中事实知识之间的语义交互，防止在编辑多个知识三元组的过程中发生灾难性遗忘。具体来说，(1) 对于关系三元组内的语义融合，我们将内部编辑注意力流聚合到 LLM 中具有标记级别粒度的自回归自注意力中。我们进一步利用多层对角线互编辑注意力流来更新整个序列级别粒度的加权表示。(2) 考虑到需要辅助参数来存储顺序编辑的知识，我们构建了一个名为 \textbf{DAFSet} 的新数据集，满足最新、流行、长尾和鲁棒属性以增强顺序编辑的通用性。实验表明，DAFNet 在单轮和顺序编辑中明显优于强大的基线。DAFSet 的使用也持续提高了其他基于辅助网络的方法在各种场景中的性能</paragraph>

##### **GAMedX: Generative AI-based Medical Entity Data Extractor Using Large Language Models**
2405.20585v1 by Mohammed-Khalil Ghali, Abdelrahman Farrag, Hajar Sakai, Hicham El Baz, Yu Jin, Sarah Lam

In the rapidly evolving field of healthcare and beyond, the integration of
generative AI in Electronic Health Records (EHRs) represents a pivotal
advancement, addressing a critical gap in current information extraction
techniques. This paper introduces GAMedX, a Named Entity Recognition (NER)
approach utilizing Large Language Models (LLMs) to efficiently extract entities
from medical narratives and unstructured text generated throughout various
phases of the patient hospital visit. By addressing the significant challenge
of processing unstructured medical text, GAMedX leverages the capabilities of
generative AI and LLMs for improved data extraction. Employing a unified
approach, the methodology integrates open-source LLMs for NER, utilizing
chained prompts and Pydantic schemas for structured output to navigate the
complexities of specialized medical jargon. The findings reveal significant
ROUGE F1 score on one of the evaluation datasets with an accuracy of 98\%. This
innovation enhances entity extraction, offering a scalable, cost-effective
solution for automated forms filling from unstructured data. As a result,
GAMedX streamlines the processing of unstructured narratives, and sets a new
standard in NER applications, contributing significantly to theoretical and
practical advancements beyond the medical technology sphere.

摘要：在快速发展的醫療保健及其他領域中，生成式 AI 整合到電子健康紀錄 (EHR) 中代表著關鍵進展，解決了當前資訊擷取技術中的重大差距。本文介紹 GAMedX，一種命名實體辨識 (NER) 方法，利用大型語言模型 (LLM) 從醫療敘述和患者醫院就診各個階段產生的非結構化文字中有效率地擷取實體。透過解決處理非結構化醫療文字的重大挑戰，GAMedX 充分利用生成式 AI 和 LLM 的能力，以改善資料擷取。採用統一方法，此方法整合開放原始碼 LLM 以進行 NER，利用串連提示和 Pydantic 架構進行結構化輸出，以應對專業醫療術語的複雜性。研究結果顯示，在其中一個評量資料集上，ROUGE F1 得分顯著，準確度為 98%。這項創新增強了實體擷取，提供了一個可擴充、具成本效益的解決方案，用於從非結構化資料中自動填寫表格。因此，GAMedX 簡化了非結構化敘述的處理，並在 NER 應用程式中設定了新的標準，為醫療技術領域之外的理論和實務進展做出了重大貢獻。

##### **Disrupting Diffusion: Token-Level Attention Erasure Attack against Diffusion-based Customization**
2405.20584v1 by Yisu Liu, Jinyang An, Wanqian Zhang, Dayan Wu, Jingzi Gu, Zheng Lin, Weiping Wang

With the development of diffusion-based customization methods like
DreamBooth, individuals now have access to train the models that can generate
their personalized images. Despite the convenience, malicious users have
misused these techniques to create fake images, thereby triggering a privacy
security crisis. In light of this, proactive adversarial attacks are proposed
to protect users against customization. The adversarial examples are trained to
distort the customization model's outputs and thus block the misuse. In this
paper, we propose DisDiff (Disrupting Diffusion), a novel adversarial attack
method to disrupt the diffusion model outputs. We first delve into the
intrinsic image-text relationships, well-known as cross-attention, and
empirically find that the subject-identifier token plays an important role in
guiding image generation. Thus, we propose the Cross-Attention Erasure module
to explicitly "erase" the indicated attention maps and disrupt the text
guidance. Besides,we analyze the influence of the sampling process of the
diffusion model on Projected Gradient Descent (PGD) attack and introduce a
novel Merit Sampling Scheduler to adaptively modulate the perturbation updating
amplitude in a step-aware manner. Our DisDiff outperforms the state-of-the-art
methods by 12.75% of FDFR scores and 7.25% of ISM scores across two facial
benchmarks and two commonly used prompts on average.

摘要：隨著像 DreamBooth 這種基於擴散的客製化方法的發展，個人現在可以存取訓練模型，以產生他們的個人化影像。儘管很方便，但惡意使用者已誤用這些技術來建立假影像，從而引發隱私安全危機。有鑑於此，提議主動對抗攻擊來保護使用者免於客製化。對抗範例經過訓練，可以扭曲客製化模型的輸出，從而阻止誤用。在本文中，我們提出 DisDiff（中斷擴散），一種新的對抗攻擊方法，用於中斷擴散模型的輸出。我們首先深入探討內在的影像文字關係，也就是眾所周知的交叉注意力，並憑經驗發現主體識別代碼在引導影像產生中扮演重要角色。因此，我們提出交叉注意力消除模組，以明確「消除」指示的注意力圖，並中斷文字指導。此外，我們分析了擴散模型的取樣過程對投影梯度下降 (PGD) 攻擊的影響，並引入一種新的優點取樣排程器，以逐步調整擾動更新幅度。我們的 DisDiff 在兩個面部基準和兩個常用的提示上，平均優於最先進的方法 12.75% 的 FDFR 分數和 7.25% 的 ISM 分數。

##### **The Point of View of a Sentiment: Towards Clinician Bias Detection in Psychiatric Notes**
2405.20582v1 by Alissa A. Valentine, Lauren A. Lepow, Alexander W. Charney, Isotta Landi

In psychiatry, negative patient descriptions and stigmatizing language can
contribute to healthcare disparities in two ways: (1) read by patients they can
harm their trust and engagement with the medical center; (2) read by future
providers they may negatively influence the future perspective of a patient. By
leveraging large language models, this work aims to identify the sentiment
expressed in psychiatric clinical notes based on the reader's point of view.
Extracting sentences from the Mount Sinai Health System's large and diverse
clinical notes, we used prompts and in-context learning to adapt three large
language models (GPT-3.5, Llama 2, Mistral) to classify the sentiment conveyed
by the sentences according to the provider or non-provider point of view.
Results showed that GPT-3.5 aligns best to provider point of view, whereas
Mistral aligns best to non-provider point of view.

摘要：在精神病學中，負面的病人描述和污名化的語言可能透過兩種方式造成醫療保健差異：(1) 病人讀到後會損害他們對醫療中心的信任和參與；(2) 未來的提供者讀到後，可能會對病人的未來觀點產生負面影響。透過利用大型語言模型，這項工作旨在根據讀者的觀點，找出精神病臨床筆記中表達的情緒。從西奈山健康系統的大量且多樣化的臨床筆記中摘錄句子，我們使用提示和情境學習來調整三個大型語言模型 (GPT-3.5、Llama 2、Mistral)，以根據提供者或非提供者觀點對句子傳達的情緒進行分類。結果顯示，GPT-3.5 最符合提供者觀點，而 Mistral 最符合非提供者觀點。

##### **Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark**
2405.20574v1 by Chanjun Park, Hyeonwoo Kim, Dahyun Kim, Seonghwan Cho, Sanghoon Kim, Sukyung Lee, Yungi Kim, Hwalsuk Lee

This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as
vital tools for evaluating Large Language Models (LLMs) in Korean.
Incorporating private test sets while mirroring the English Open LLM
Leaderboard, we establish a robust evaluation framework that has been well
integrated in the Korean LLM community. We perform data leakage analysis that
shows the benefit of private test sets along with a correlation study within
the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we
present empirical support for the need to expand beyond set benchmarks. We hope
the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to
foster more linguistic diversity.

摘要：本文介紹了 Open Ko-LLM 排行榜和 Ko-H5 基準，作為評估韓語大型語言模型 (LLM) 的重要工具。
我們在建立健全的評估架構時，納入了私人測試集，同時比照了英文 Open LLM 排行榜，而此架構已在韓語 LLM 社群中廣泛整合。我們執行了資料外洩分析，顯示私人測試集的優點，並在 Ko-H5 基準內進行相關性研究，以及 Ko-H5 分數的時間分析。此外，我們提出經驗證據，說明需要擴展到設定基準之外。我們希望 Open Ko-LLM 排行榜能為擴展 LLM 評估樹立先例，以促進更多語言的多樣性。

##### **Can Machine Learning Assist in Diagnosis of Primary Immune Thrombocytopenia? A feasibility study**
2405.20562v1 by Haroon Miah, Dimitrios Kollias, Giacinto Luca Pedone, Drew Provan, Frederick Chen

Primary Immune thrombocytopenia (ITP) is a rare autoimmune disease
characterised by immune-mediated destruction of peripheral blood platelets in
patients leading to low platelet counts and bleeding. The diagnosis and
effective management of ITP is challenging because there is no established test
to confirm the disease and no biomarker with which one can predict the response
to treatment and outcome. In this work we conduct a feasibility study to check
if machine learning can be applied effectively for diagnosis of ITP using
routine blood tests and demographic data in a non-acute outpatient setting.
Various ML models, including Logistic Regression, Support Vector Machine,
k-Nearest Neighbor, Decision Tree and Random Forest, were applied to data from
the UK Adult ITP Registry and a general hematology clinic. Two different
approaches were investigated: a demographic-unaware and a demographic-aware
one. We conduct extensive experiments to evaluate the predictive performance of
these models and approaches, as well as their bias. The results revealed that
Decision Tree and Random Forest models were both superior and fair, achieving
nearly perfect predictive and fairness scores, with platelet count identified
as the most significant variable. Models not provided with demographic
information performed better in terms of predictive accuracy but showed lower
fairness score, illustrating a trade-off between predictive performance and
fairness.

摘要：原發性免疫性血小板減少症（ITP）是一種罕見的自體免疫疾病
其特徵在於免疫介導的外周血小板破壞導致患者血小板數量減少和出血。ITP 的診斷和有效管理具有挑戰性，因為沒有既定的檢測方法來確認疾病，也沒有生物標誌物可以預測對治療和結果的反應。在這項工作中，我們進行了一項可行性研究，以檢查機器學習是否可以有效應用於使用非急性門診環境中的常規血液檢查和人口統計數據診斷 ITP。各種 ML 模型，包括邏輯迴歸、支持向量機、k 最近鄰、決策樹和隨機森林，被應用於來自英國成人 ITP 登記處和普通血液學診所的數據。研究了兩種不同的方法：一種是不知道人口統計數據的方法，另一種是知道人口統計數據的方法。我們進行了廣泛的實驗來評估這些模型和方法的預測性能以及它們的偏差。結果表明，決策樹和隨機森林模型都優越且公平，實現了接近完美的預測和公平分數，血小板計數被確定為最重要的變量。未提供人口統計信息的模型在預測準確性方面表現得更好，但公平分數較低，說明了預測性能和公平性之間的權衡。

##### **Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models**
2405.20541v1 by Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew L. Leavitt, Mansheej Paul

In this work, we investigate whether small language models can determine
high-quality subsets of large-scale text datasets that improve the performance
of larger language models. While existing work has shown that pruning based on
the perplexity of a larger model can yield high-quality data, we investigate
whether smaller models can be used for perplexity-based pruning and how pruning
is affected by the domain composition of the data being pruned. We demonstrate
that for multiple dataset compositions, perplexity-based pruning of pretraining
data can \emph{significantly} improve downstream task performance: pruning
based on perplexities computed with a 125 million parameter model improves the
average performance on downstream tasks of a 3 billion parameter model by up to
2.04 and achieves up to a $1.45\times$ reduction in pretraining steps to reach
commensurate baseline performance. Furthermore, we demonstrate that such
perplexity-based data pruning also yields downstream performance gains in the
over-trained and data-constrained regimes.

摘要：在這項工作中，我們探討小型語言模型是否可以確定大型文字資料集中的高品質子集，以提升大型語言模型的效能。雖然現有研究顯示，根據較大型模型的困惑度進行剪枝可以產生高品質資料，但我們探討較小型模型是否可用於基於困惑度的剪枝，以及剪枝如何受到被剪枝資料的網域組成影響。我們證明，對於多個資料集組成，預訓練資料的基於困惑度的剪枝可以顯著提升下游任務效能：根據 1.25 億個參數模型計算的困惑度進行剪枝，將 30 億個參數模型在下游任務的平均效能提升多達 2.04，並在預訓練步驟中最多減少 $1.45\times$ 倍，以達到相當的基準效能。此外，我們證明這種基於困惑度的資料剪枝，在過度訓練和資料受限的模式中，也能產生下游效能提升。

##### **Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning**
2405.20535v1 by Xinlu Zhang, Zhiyu Zoey Chen, Xi Ye, Xianjun Yang, Lichang Chen, William Yang Wang, Linda Ruth Petzold

Instruction Fine-Tuning (IFT) significantly enhances the zero-shot
capabilities of pretrained Large Language Models (LLMs). While coding data is
known to boost reasoning abilities during LLM pretraining, its role in
activating internal reasoning capacities during IFT remains understudied. This
paper investigates a key question: How does coding data impact LLMs' reasoning
capacities during the IFT stage? To explore this, we thoroughly examine the
impact of coding data across different coding data proportions, model families,
sizes, and reasoning domains, from various perspectives. Specifically, we
create three IFT datasets with increasing coding data proportions, fine-tune
six LLM backbones across different families and scales on these datasets,
evaluate the tuned models' performance across twelve tasks in three reasoning
domains, and analyze the outcomes from three broad-to-granular perspectives:
overall, domain-level, and task-specific. Our holistic analysis provides
valuable insights in each perspective. First, coding data tuning enhances the
overall reasoning capabilities of LLMs across different model families and
scales. Moreover, the effect of coding data varies among different domains but
shows consistent trends across model families and scales within each domain.
Additionally, coding data generally yields comparable task-specific benefits
across different model families, with the optimal coding data proportions in
IFT datasets being task-specific.

摘要：指令微調 (IFT) 大幅提升了大型語言模型 (LLM) 的零次學習能力。儘管已知編碼資料會在 LLM 預訓練期間提升推理能力，但其在 IFT 期間啟動內部推理能力中的角色仍未受到深入探討。本論文探討了一個關鍵問題：編碼資料在 IFT 階段如何影響 LLM 的推理能力？為了解決此問題，我們從各個角度徹底檢視了編碼資料在不同編碼資料比例、模型系列、大小和推理領域中的影響。具體來說，我們建立了三個編碼資料比例逐漸增加的 IFT 資料集，在這些資料集上微調了不同系列和規模的六個 LLM 主幹，並在三個推理領域的十二項任務中評估了微調模型的效能，並從三個廣泛到詳細的角度分析結果：整體、領域層級和特定任務。我們的整體分析在每個角度都提供了寶貴的見解。首先，編碼資料微調提升了不同模型系列和規模的 LLM 的整體推理能力。此外，編碼資料的影響因領域而異，但在每個領域中，不同模型系列和規模的趨勢一致。此外，編碼資料通常在不同模型系列中產生了相當的特定任務效益，IFT 資料集中最佳的編碼資料比例因任務而異。

##### **An Automatic Question Usability Evaluation Toolkit**
2405.20529v1 by Steven Moore, Eamon Costello, Huy A. Nguyen, John Stamper

Evaluating multiple-choice questions (MCQs) involves either labor intensive
human assessments or automated methods that prioritize readability, often
overlooking deeper question design flaws. To address this issue, we introduce
the Scalable Automatic Question Usability Evaluation Toolkit (SAQUET), an
open-source tool that leverages the Item-Writing Flaws (IWF) rubric for a
comprehensive and automated quality evaluation of MCQs. By harnessing the
latest in large language models such as GPT-4, advanced word embeddings, and
Transformers designed to analyze textual complexity, SAQUET effectively
pinpoints and assesses a wide array of flaws in MCQs. We first demonstrate the
discrepancy between commonly used automated evaluation metrics and the human
assessment of MCQ quality. Then we evaluate SAQUET on a diverse dataset of MCQs
across the five domains of Chemistry, Statistics, Computer Science, Humanities,
and Healthcare, showing how it effectively distinguishes between flawed and
flawless questions, providing a level of analysis beyond what is achievable
with traditional metrics. With an accuracy rate of over 94% in detecting the
presence of flaws identified by human evaluators, our findings emphasize the
limitations of existing evaluation methods and showcase potential in improving
the quality of educational assessments.

摘要：評量選擇題 (MCQ) 涉及到大量的人力評量或自動化方法，這些方法優先考量可讀性，卻常常忽略了更深層的問題設計缺陷。為了解決這個問題，我們引入了可擴充自動化問題可用性評量工具包 (SAQUET)，這是一個開源工具，它利用項目撰寫缺陷 (IWF) 評分標準，對選擇題進行全面且自動化的品質評量。透過利用 GPT-4 等大型語言模型、進階的詞嵌入和旨在分析文字複雜性的 Transformer，SAQUET 能有效地找出並評量選擇題中的各種缺陷。我們首先展示了常用自動化評量指標與人類對選擇題品質評量之間的差異。然後，我們在化學、統計、電腦科學、人文和醫療保健這五個領域的多元化選擇題資料集上評量 SAQUET，展示了它如何有效區分有缺陷和無缺陷的問題，提供超越傳統指標所能達到的分析層級。我們的發現強調了現有評量方法的限制，並展示了改善教育評量品質的潛力，在偵測人類評量員識別的缺陷方面，準確率超過 94%。

##### **Towards Ontology-Enhanced Representation Learning for Large Language Models**
2405.20527v1 by Francesco Ronzano, Jay Nanavati

Taking advantage of the widespread use of ontologies to organise and
harmonize knowledge across several distinct domains, this paper proposes a
novel approach to improve an embedding-Large Language Model (embedding-LLM) of
interest by infusing the knowledge formalized by a reference ontology:
ontological knowledge infusion aims at boosting the ability of the considered
LLM to effectively model the knowledge domain described by the infused
ontology. The linguistic information (i.e. concept synonyms and descriptions)
and structural information (i.e. is-a relations) formalized by the ontology are
utilized to compile a comprehensive set of concept definitions, with the
assistance of a powerful generative LLM (i.e. GPT-3.5-turbo). These concept
definitions are then employed to fine-tune the target embedding-LLM using a
contrastive learning framework. To demonstrate and evaluate the proposed
approach, we utilize the biomedical disease ontology MONDO. The results show
that embedding-LLMs enhanced by ontological disease knowledge exhibit an
improved capability to effectively evaluate the similarity of in-domain
sentences from biomedical documents mentioning diseases, without compromising
their out-of-domain performance.

摘要：<paragraph>利用本体論廣泛用於組織和調和不同領域間的知識，本文提出了一種新方法來改進嵌入式大型語言模型 (embedding-LLM) 的興趣，方法是注入由參考本体論形式化的知識：本体論知識注入旨在提升所考慮的 LLM 有效建模由注入的本体論所描述的知識領域的能力。由本体論形式化的語言資訊（即概念同義詞和描述）和結構資訊（即 is-a 關係）用於編譯一組全面的概念定義，並在強大的生成式 LLM（即 GPT-3.5-turbo）的協助下進行。然後，這些概念定義被用於使用對比學習架構微調目標嵌入式 LLM。為了展示和評估所提出的方法，我們利用生物醫學疾病本体論 MONDO。結果表明，由本体論疾病知識增強的嵌入式 LLM 表現出有效評估提及疾病的生物醫學文件中的同域句子相似性的改進能力，而不會損害其域外效能。</paragraph>

##### **Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions**
2405.20526v1 by Steven Moore, Robin Schmucker, Tom Mitchell, John Stamper

Knowledge Components (KCs) linked to assessments enhance the measurement of
student learning, enrich analytics, and facilitate adaptivity. However,
generating and linking KCs to assessment items requires significant effort and
domain-specific knowledge. To streamline this process for higher-education
courses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs)
in Chemistry and E-Learning. We analyzed discrepancies between the KCs
generated by the Large Language Model (LLM) and those made by humans through
evaluation from three domain experts in each subject area. This evaluation
aimed to determine whether, in instances of non-matching KCs, evaluators showed
a preference for the LLM-generated KCs over their human-created counterparts.
We also developed an ontology induction algorithm to cluster questions that
assess similar KCs based on their content. Our most effective LLM strategy
accurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs, with
even higher success when considering the top five KC suggestions. Human
evaluators favored LLM-generated KCs, choosing them over human-assigned ones
approximately two-thirds of the time, a preference that was statistically
significant across both domains. Our clustering algorithm successfully grouped
questions by their underlying KCs without needing explicit labels or contextual
information. This research advances the automation of KC generation and
classification for assessment items, alleviating the need for student data or
predefined KC labels.

摘要：知識組件 (KC) 與評量連結，可提升學生學習的測量、豐富分析，並促進適應性。然而，產生 KC 並將其連結到評量項目需要大量的努力和特定領域的知識。為了簡化高等教育課程的這個流程，我們採用 GPT-4 為化學和電子學習的多選題 (MCQ) 產生 KC。我們分析了大型語言模型 (LLM) 產生的 KC 與人類製作的 KC 之間的差異，並由每個領域的三位領域專家進行評估。此評估旨在確定在 KC 不匹配的情況下，評估者是否偏好 LLM 產生的 KC，而非人工建立的 KC。我們還開發了一個本體論歸納演算法，根據內容將評量相似 KC 的問題進行分群。我們最有效的 LLM 策略準確地匹配了 56% 的化學和 35% 的電子學習 MCQ 的 KC，在考慮前五個 KC 建議時，成功率更高。人類評估者偏好 LLM 產生的 KC，約有三分之二的時間選擇它們，而放棄人工指定的 KC，這項偏好在這兩個領域中具有統計顯著性。我們的分群演算法成功地根據其基礎 KC 將問題分組，而不需要明確的標籤或背景資訊。這項研究推動了評量項目 KC 產生的自動化和分類，減輕了對學生資料或預定義 KC 標籤的需求。

##### **Diffusion On Syntax Trees For Program Synthesis**
2405.20519v1 by Shreyas Kapur, Erik Jenner, Stuart Russell

Large language models generate code one token at a time. Their autoregressive
generation process lacks the feedback of observing the program's output.
Training LLMs to suggest edits directly can be challenging due to the scarcity
of rich edit data. To address these problems, we propose neural diffusion
models that operate on syntax trees of any context-free grammar. Similar to
image diffusion models, our method also inverts ``noise'' applied to syntax
trees. Rather than generating code sequentially, we iteratively edit it while
preserving syntactic validity, which makes it easy to combine this neural model
with search. We apply our approach to inverse graphics tasks, where our model
learns to convert images into programs that produce those images. Combined with
search, our model is able to write graphics programs, see the execution result,
and debug them to meet the required specifications. We additionally show how
our system can write graphics programs for hand-drawn sketches.

摘要：大型語言模型一次只產生一個代碼符號。它們的自動迴歸生成過程缺乏觀察程式輸出回饋。由於豐富的編輯資料稀少，訓練 LLM 直接建議編輯可能會具有挑戰性。為了解決這些問題，我們提出神經擴散模型，它在任何無脈絡語法的語法樹上運作。與影像擴散模型類似，我們的模型也會對應用於語法樹的「雜訊」進行反轉。我們不是循序漸進地產生程式碼，而是反覆編輯它，同時保持語法有效性，這使得將這個神經模型與搜尋結合變得容易。我們將我們的做法應用於反向圖形任務，我們的模型學習將影像轉換為產生那些影像的程式。結合搜尋，我們的模型能夠撰寫圖形程式，查看執行結果，並除錯它們以符合所需的規格。我們另外展示我們的系統如何為手繪草圖撰寫圖形程式。

