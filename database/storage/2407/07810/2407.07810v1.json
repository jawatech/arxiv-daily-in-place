{"2407.07810": {"publish_time": "2024-07-10", "title": "Transformer Alignment in Large Language Models", "paper_summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing, and a precise understanding of the internal mechanisms\ndriving their success is essential. We regard LLMs as transforming embeddings\nvia a discrete, coupled, nonlinear, dynamical system in high dimensions. This\nperspective motivates tracing the trajectories of individual tokens as they\npass through transformer blocks, and linearizing the system along these\ntrajectories through their Jacobian matrices. In our analysis of 38 openly\navailable LLMs, we uncover the alignment of top left and right singular vectors\nof Residual Jacobians, as well as the emergence of linearity and layer-wise\nexponential growth. Notably, we discover that increased alignment\n$\\textit{positively correlates}$ with model performance. Metrics evaluated\npost-training show significant improvement in comparison to measurements made\nwith randomly initialized weights, highlighting the significant effects of\ntraining in transformers. These findings reveal a remarkable level of\nregularity that has previously been overlooked, reinforcing the dynamical\ninterpretation and paving the way for deeper understanding and optimization of\nLLM architectures.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u9032\u5c55\uff0c\u800c\u6e96\u78ba\u7406\u89e3\u63a8\u52d5\u5176\u6210\u529f\u7684\u5167\u90e8\u6a5f\u5236\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u5c07 LLM \u8996\u70ba\u901a\u904e\u9ad8\u7dad\u5ea6\u7684\u96e2\u6563\u3001\u8026\u5408\u3001\u975e\u7dda\u6027\u52d5\u614b\u7cfb\u7d71\u8f49\u63db\u5d4c\u5165\u3002\u9019\u7a2e\u89c0\u9ede\u6fc0\u52f5\u6211\u5011\u8ffd\u8e64\u500b\u5225\u7b26\u865f\u5728\u901a\u904e\u8b8a\u63db\u5668\u5340\u584a\u6642\u7684\u8ecc\u8de1\uff0c\u4e26\u901a\u904e\u5176\u96c5\u53ef\u6bd4\u77e9\u9663\u6cbf\u9019\u4e9b\u8ecc\u8de1\u5c0d\u7cfb\u7d71\u9032\u884c\u7dda\u6027\u5316\u3002\u5728\u6211\u5011\u5c0d 38 \u500b\u516c\u958b\u53ef\u7528\u7684 LLM \u7684\u5206\u6790\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u4e86\u6b98\u5dee\u96c5\u53ef\u6bd4\u77e9\u9663\u7684\u5de6\u5947\u7570\u5411\u91cf\u548c\u53f3\u5947\u7570\u5411\u91cf\u7684\u5c0d\u9f4a\uff0c\u4ee5\u53ca\u7dda\u6027\u548c\u9010\u5c64\u6307\u6578\u589e\u9577\u7684\u51fa\u73fe\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe\u589e\u52a0\u5c0d\u9f4a\u5ea6\u6703\u8207\u6a21\u578b\u6027\u80fd\u5448\u6b63\u76f8\u95dc\u3002\u8207\u4f7f\u7528\u96a8\u6a5f\u521d\u59cb\u5316\u6b0a\u91cd\u9032\u884c\u7684\u6e2c\u91cf\u76f8\u6bd4\uff0c\u8a13\u7df4\u5f8c\u8a55\u4f30\u7684\u6307\u6a19\u986f\u793a\u51fa\u986f\u8457\u6539\u9032\uff0c\u7a81\u51fa\u4e86Transformer\u8a13\u7df4\u7684\u986f\u8457\u5f71\u97ff\u3002\u9019\u4e9b\u767c\u73fe\u63ed\u793a\u4e86\u4e00\u500b\u4ee5\u524d\u88ab\u5ffd\u8996\u7684\u986f\u8457\u898f\u5f8b\u6027\uff0c\u52a0\u5f37\u4e86\u52d5\u614b\u89e3\u91cb\uff0c\u4e26\u70ba LLM \u67b6\u69cb\u7684\u66f4\u6df1\u5165\u7406\u89e3\u548c\u512a\u5316\u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Murdock Aubry et.al.", "authors": "Murdock Aubry, Haoming Meng, Anton Sugolov, Vardan Papyan", "id": "2407.07810v1", "paper_url": "http://arxiv.org/abs/2407.07810v1", "repo": "null"}}