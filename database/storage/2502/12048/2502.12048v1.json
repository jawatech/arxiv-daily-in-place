{"2502.12048": {"publish_time": "2025-02-17", "title": "A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond", "paper_summary": "Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial\nIntelligence (GenAI) has opened new frontiers in brain signal decoding,\nenabling assistive communication, neural representation learning, and\nmultimodal integration. BCIs, particularly those leveraging\nElectroencephalography (EEG), provide a non-invasive means of translating\nneural activity into meaningful outputs. Recent advances in deep learning,\nincluding Generative Adversarial Networks (GANs) and Transformer-based Large\nLanguage Models (LLMs), have significantly improved EEG-based generation of\nimages, text, and speech. This paper provides a literature review of the\nstate-of-the-art in EEG-based multimodal generation, focusing on (i)\nEEG-to-image generation through GANs, Variational Autoencoders (VAEs), and\nDiffusion Models, and (ii) EEG-to-text generation leveraging Transformer based\nlanguage models and contrastive learning methods. Additionally, we discuss the\nemerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We\nhighlight key datasets, use cases, challenges, and EEG feature encoding methods\nthat underpin generative approaches. By providing a structured overview of\nEEG-based generative AI, this survey aims to equip researchers and\npractitioners with insights to advance neural decoding, enhance assistive\ntechnologies, and expand the frontiers of brain-computer interaction.", "paper_summary_zh": "\u8166\u6a5f\u4ecb\u9762\uff08BCIs\uff09\u8207\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u6167\uff08GenAI\uff09\u7684\u6574\u5408\u70ba\u8166\u4fe1\u865f\u89e3\u78bc\u958b\u555f\u4e86\u65b0\u9818\u57df\uff0c\u80fd\u5354\u52a9\u6e9d\u901a\u3001\u795e\u7d93\u8868\u5fb5\u5b78\u7fd2\u8207\u591a\u6a21\u5f0f\u6574\u5408\u3002BCIs\uff0c\u7279\u5225\u662f\u5229\u7528\u8166\u96fb\u5716\uff08EEG\uff09\u7684 BCIs\uff0c\u63d0\u4f9b\u4e86\u4e00\u7a2e\u975e\u4fb5\u5165\u6027\u7684\u65b9\u5f0f\uff0c\u53ef\u5c07\u795e\u7d93\u6d3b\u52d5\u8f49\u63db\u70ba\u6709\u610f\u7fa9\u7684\u8f38\u51fa\u3002\u6df1\u5ea6\u5b78\u7fd2\u7684\u6700\u65b0\u9032\u5c55\uff0c\u5305\u62ec\u751f\u6210\u5c0d\u6297\u7db2\u8def\uff08GANs\uff09\u8207\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5927\u5e45\u6539\u5584\u4e86\u57fa\u65bc EEG \u7684\u5f71\u50cf\u3001\u6587\u5b57\u8207\u8a9e\u97f3\u751f\u6210\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4efd\u57fa\u65bc EEG \u7684\u591a\u6a21\u5f0f\u751f\u6210\u7684\u6700\u65b0\u6587\u737b\u56de\u9867\uff0c\u91cd\u9ede\u5728\u65bc\uff08\u4e00\uff09\u900f\u904e GANs\u3001\u8b8a\u7570\u81ea\u52d5\u7de8\u78bc\u5668\uff08VAEs\uff09\u8207\u64f4\u6563\u6a21\u578b\u9032\u884c EEG \u5230\u5f71\u50cf\u7684\u751f\u6210\uff0c\u4ee5\u53ca\uff08\u4e8c\uff09\u5229\u7528\u57fa\u65bc Transformer \u7684\u8a9e\u8a00\u6a21\u578b\u8207\u5c0d\u6bd4\u5b78\u7fd2\u65b9\u6cd5\u9032\u884c EEG \u5230\u6587\u5b57\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86 EEG \u5230\u8a9e\u97f3\u5408\u6210\u7684\u65b0\u8208\u9818\u57df\uff0c\u9019\u662f\u4e00\u500b\u4e0d\u65b7\u6f14\u9032\u7684\u591a\u6a21\u5f0f\u9818\u57df\u3002\u6211\u5011\u91cd\u9ede\u4ecb\u7d39\u4e86\u95dc\u9375\u7684\u8cc7\u6599\u96c6\u3001\u7528\u4f8b\u3001\u6311\u6230\u8207\u652f\u6490\u751f\u6210\u65b9\u6cd5\u7684 EEG \u7279\u5fb5\u7de8\u78bc\u65b9\u6cd5\u3002\u900f\u904e\u63d0\u4f9b\u57fa\u65bc EEG \u7684\u751f\u6210\u5f0f AI \u7684\u7d50\u69cb\u5316\u6982\u89c0\uff0c\u672c\u8abf\u67e5\u65e8\u5728\u70ba\u7814\u7a76\u4eba\u54e1\u8207\u5f9e\u696d\u4eba\u54e1\u63d0\u4f9b\u898b\u89e3\uff0c\u4ee5\u63a8\u9032\u795e\u7d93\u89e3\u78bc\u3001\u589e\u5f37\u8f14\u52a9\u6280\u8853\u4e26\u64f4\u5c55\u8166\u6a5f\u4e92\u52d5\u7684\u9818\u57df\u3002", "author": "Shreya Shukla et.al.", "authors": "Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury", "id": "2502.12048v1", "paper_url": "http://arxiv.org/abs/2502.12048v1", "repo": "null"}}