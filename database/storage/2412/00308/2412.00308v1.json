{"2412.00308": {"publish_time": "2024-11-30", "title": "BOTS: Batch Bayesian Optimization of Extended Thompson Sampling for Severely Episode-Limited RL Settings", "paper_summary": "In settings where the application of reinforcement learning (RL) requires\nrunning real-world trials, including the optimization of adaptive health\ninterventions, the number of episodes available for learning can be severely\nlimited due to cost or time constraints. In this setting, the bias-variance\ntrade-off of contextual bandit methods can be significantly better than that of\nmore complex full RL methods. However, Thompson sampling bandits are limited to\nselecting actions based on distributions of immediate rewards. In this paper,\nwe extend the linear Thompson sampling bandit to select actions based on a\nstate-action utility function consisting of the Thompson sampler's estimate of\nthe expected immediate reward combined with an action bias term. We use batch\nBayesian optimization over episodes to learn the action bias terms with the\ngoal of maximizing the expected return of the extended Thompson sampler. The\nproposed approach is able to learn optimal policies for a strictly broader\nclass of Markov decision processes (MDPs) than standard Thompson sampling.\nUsing an adaptive intervention simulation environment that captures key aspects\nof behavioral dynamics, we show that the proposed method can significantly\nout-perform standard Thompson sampling in terms of total return, while\nrequiring significantly fewer episodes than standard value function and policy\ngradient methods.", "paper_summary_zh": "\u5728\u9700\u8981\u4f7f\u7528\u5f37\u5316\u5b78\u7fd2 (RL) \u9032\u884c\u5be6\u969b\u4e16\u754c\u8a66\u9a57\uff0c\u5305\u62ec\u6700\u4f73\u5316\u9069\u61c9\u6027\u5065\u5eb7\u5e72\u9810\u63aa\u65bd\u7684\u8a2d\u5b9a\u4e2d\uff0c\u53ef\u7528\u65bc\u5b78\u7fd2\u7684\u56de\u5408\u6578\u53ef\u80fd\u6703\u56e0\u70ba\u6210\u672c\u6216\u6642\u9593\u9650\u5236\u800c\u53d7\u5230\u56b4\u91cd\u9650\u5236\u3002\u5728\u6b64\u8a2d\u5b9a\u4e2d\uff0c\u60c5\u5883\u5f37\u76dc\u65b9\u6cd5\u7684\u504f\u5dee\u8b8a\u7570\u53d6\u6368\u6703\u986f\u8457\u512a\u65bc\u66f4\u8907\u96dc\u7684\u5b8c\u6574 RL \u65b9\u6cd5\u3002\u4e0d\u904e\uff0c\u6e6f\u666e\u68ee\u62bd\u6a23\u5f37\u76dc\u53ea\u80fd\u6839\u64da\u7acb\u5373\u734e\u52f5\u7684\u5206\u914d\u4f86\u9078\u64c7\u884c\u52d5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5ef6\u4f38\u7dda\u6027\u6e6f\u666e\u68ee\u62bd\u6a23\u5f37\u76dc\uff0c\u4ee5\u6839\u64da\u72c0\u614b\u884c\u52d5\u6548\u7528\u51fd\u6578\u9078\u64c7\u884c\u52d5\uff0c\u8a72\u51fd\u6578\u5305\u542b\u6e6f\u666e\u68ee\u63a1\u6a23\u5668\u5c0d\u9810\u671f\u7acb\u5373\u734e\u52f5\u7684\u4f30\u8a08\u503c\uff0c\u4ee5\u53ca\u52d5\u4f5c\u504f\u5dee\u9805\u3002\u6211\u5011\u4f7f\u7528\u6279\u6b21\u8c9d\u6c0f\u6700\u4f73\u5316\u5728\u56de\u5408\u4e2d\u5b78\u7fd2\u52d5\u4f5c\u504f\u5dee\u9805\uff0c\u76ee\u6a19\u662f\u6700\u5927\u5316\u5ef6\u4f38\u6e6f\u666e\u68ee\u63a1\u6a23\u5668\u7684\u9810\u671f\u56de\u5831\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u5920\u70ba\u6bd4\u6a19\u6e96\u6e6f\u666e\u68ee\u62bd\u6a23\u66f4\u5ee3\u6cdb\u7684\u99ac\u53ef\u592b\u6c7a\u7b56\u7a0b\u5e8f (MDP) \u985e\u5225\u5b78\u7fd2\u6700\u4f73\u7b56\u7565\u3002\u4f7f\u7528\u6355\u6349\u884c\u70ba\u52d5\u614b\u95dc\u9375\u5c64\u9762\u7684\u9069\u61c9\u6027\u5e72\u9810\u6a21\u64ec\u74b0\u5883\uff0c\u6211\u5011\u8b49\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7e3d\u56de\u5831\u65b9\u9762\u53ef\u4ee5\u986f\u8457\u512a\u65bc\u6a19\u6e96\u6e6f\u666e\u68ee\u62bd\u6a23\uff0c\u540c\u6642\u6240\u9700\u56de\u5408\u6578\u9060\u5c11\u65bc\u6a19\u6e96\u50f9\u503c\u51fd\u6578\u548c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002", "author": "Karine Karine et.al.", "authors": "Karine Karine, Susan A. Murphy, Benjamin M. Marlin", "id": "2412.00308v1", "paper_url": "http://arxiv.org/abs/2412.00308v1", "repo": "null"}}