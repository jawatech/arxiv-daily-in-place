{"2411.19804": {"publish_time": "2024-11-29", "title": "Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation", "paper_summary": "Integrating multiple (sub-)systems is essential to create advanced\nInformation Systems (ISs). Difficulties mainly arise when integrating dynamic\nenvironments across the IS lifecycle. A traditional approach is a registry that\nprovides the API documentation of the systems' endpoints. Large Language Models\n(LLMs) have shown to be capable of automatically creating system integrations\n(e.g., as service composition) based on this documentation but require concise\ninput due to input token limitations, especially regarding comprehensive API\ndescriptions. Currently, it is unknown how best to preprocess these API\ndescriptions. Within this work, we (i) analyze the usage of Retrieval Augmented\nGeneration (RAG) for endpoint discovery and the chunking, i.e., preprocessing,\nof OpenAPIs to reduce the input token length while preserving the most relevant\ninformation. To further reduce the input token length for the composition\nprompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves details on\ndemand. We evaluate RAG for endpoint discovery using the RestBench benchmark,\nfirst, for the different chunking possibilities and parameters measuring the\nendpoint retrieval recall, precision, and F1 score. Then, we assess the\nDiscovery Agent using the same test set. With our prototype, we demonstrate how\nto successfully employ RAG for endpoint discovery to reduce the token count.\nWhile revealing high values for recall, precision, and F1, further research is\nnecessary to retrieve all requisite endpoints. Our experiments show that for\npreprocessing, LLM-based and format-specific approaches outperform na\\\"ive\nchunking methods. Relying on an agent further enhances these results as the\nagent splits the tasks into multiple fine granular subtasks, improving the\noverall RAG performance in the token count, precision, and F1 score.", "paper_summary_zh": "\u6574\u5408\u591a\u500b\uff08\u5b50\uff09\u7cfb\u7d71\u5c0d\u65bc\u5efa\u7acb\u9032\u968e\u8cc7\u8a0a\u7cfb\u7d71\uff08IS\uff09\u81f3\u95dc\u91cd\u8981\u3002\u56f0\u96e3\u4e3b\u8981\u51fa\u73fe\u5728\u6574\u5408 IS \u751f\u547d\u9031\u671f\u4e2d\u7684\u52d5\u614b\u74b0\u5883\u6642\u3002\u50b3\u7d71\u65b9\u6cd5\u662f\u767b\u9304\u6a94\uff0c\u5b83\u63d0\u4f9b\u7cfb\u7d71\u7aef\u9ede\u7684 API \u6587\u4ef6\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5df2\u8b49\u660e\u6709\u80fd\u529b\u6839\u64da\u6b64\u6587\u4ef6\u81ea\u52d5\u5efa\u7acb\u7cfb\u7d71\u6574\u5408\uff08\u4f8b\u5982\uff0c\u4f5c\u70ba\u670d\u52d9\u7d44\u5408\uff09\uff0c\u4f46\u7531\u65bc\u8f38\u5165\u4ee4\u724c\u9650\u5236\uff0c\u7279\u5225\u662f\u95dc\u65bc\u5168\u9762\u7684 API \u63cf\u8ff0\uff0c\u56e0\u6b64\u9700\u8981\u7c21\u6f54\u7684\u8f38\u5165\u3002\u76ee\u524d\uff0c\u5c1a\u4e0d\u77e5\u9053\u5982\u4f55\u6700\u597d\u5730\u9810\u8655\u7406\u9019\u4e9b API \u63cf\u8ff0\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\uff08i\uff09\u5206\u6790\u4e86\u6aa2\u7d22\u589e\u5f37\u751f\u6210\uff08RAG\uff09\u5728\u7aef\u9ede\u767c\u73fe\u4e2d\u7684\u7528\u6cd5\uff0c\u4ee5\u53ca OpenAPI \u7684\u5206\u584a\uff0c\u5373\u9810\u8655\u7406\uff0c\u4ee5\u6e1b\u5c11\u8f38\u5165\u4ee4\u724c\u9577\u5ea6\uff0c\u540c\u6642\u4fdd\u7559\u6700\u76f8\u95dc\u7684\u8cc7\u8a0a\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u6e1b\u5c11\u7d44\u5408\u63d0\u793a\u7684\u8f38\u5165\u4ee4\u724c\u9577\u5ea6\u4e26\u6539\u5584\u7aef\u9ede\u6aa2\u7d22\uff0c\u6211\u5011\u63d0\u51fa\uff08ii\uff09\u4e00\u500b\u767c\u73fe\u4ee3\u7406\uff0c\u5b83\u53ea\u63a5\u6536\u6700\u76f8\u95dc\u7aef\u9ede\u7684\u6458\u8981\uff0c\u4e26\u6839\u64da\u9700\u8981\u6aa2\u7d22\u8a73\u7d30\u8cc7\u8a0a\u3002\u6211\u5011\u4f7f\u7528 RestBench \u57fa\u6e96\u8a55\u4f30 RAG \u7684\u7aef\u9ede\u767c\u73fe\uff0c\u9996\u5148\uff0c\u5c0d\u65bc\u4e0d\u540c\u7684\u5206\u584a\u53ef\u80fd\u6027\u548c\u53c3\u6578\u6e2c\u91cf\u7aef\u9ede\u6aa2\u7d22\u53ec\u56de\u7387\u3001\u6e96\u78ba\u5ea6\u548c F1 \u5206\u6578\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u76f8\u540c\u7684\u6e2c\u8a66\u96c6\u8a55\u4f30\u767c\u73fe\u4ee3\u7406\u3002\u900f\u904e\u6211\u5011\u7684\u539f\u578b\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5982\u4f55\u6210\u529f\u4f7f\u7528 RAG \u9032\u884c\u7aef\u9ede\u767c\u73fe\u4ee5\u6e1b\u5c11\u4ee4\u724c\u8a08\u6578\u3002\u96d6\u7136\u986f\u793a\u51fa\u53ec\u56de\u7387\u3001\u6e96\u78ba\u5ea6\u548c F1 \u7684\u9ad8\u503c\uff0c\u4f46\u4ecd\u9700\u8981\u9032\u4e00\u6b65\u7684\u7814\u7a76\u4f86\u6aa2\u7d22\u6240\u6709\u5fc5\u8981\u7684\u7aef\u9ede\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u5c0d\u65bc\u9810\u8655\u7406\uff0c\u57fa\u65bc LLM \u548c\u7279\u5b9a\u683c\u5f0f\u7684\u65b9\u6cd5\u512a\u65bc\u5929\u771f\u7684\u5206\u584a\u65b9\u6cd5\u3002\u4f9d\u8cf4\u4ee3\u7406\u9032\u4e00\u6b65\u589e\u5f37\u4e86\u9019\u4e9b\u7d50\u679c\uff0c\u56e0\u70ba\u4ee3\u7406\u5c07\u4efb\u52d9\u5206\u70ba\u591a\u500b\u7d30\u7dfb\u7684\u5b50\u4efb\u52d9\uff0c\u6539\u5584\u4e86\u4ee4\u724c\u8a08\u6578\u3001\u6e96\u78ba\u5ea6\u548c F1 \u5206\u6578\u4e2d\u7684\u6574\u9ad4 RAG \u6548\u80fd\u3002", "author": "Robin D. Pesl et.al.", "authors": "Robin D. Pesl, Jerin G. Mathew, Massimo Mecella, Marco Aiello", "id": "2411.19804v1", "paper_url": "http://arxiv.org/abs/2411.19804v1", "repo": "null"}}