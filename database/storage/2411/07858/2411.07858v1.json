{"2411.07858": {"publish_time": "2024-11-12", "title": "Verbosity $\\neq$ Veracity: Demystify Verbosity Compensation Behavior of Large Language Models", "paper_summary": "When unsure about an answer, humans often respond with more words than\nnecessary, hoping that part of the response will be correct. We observe a\nsimilar behavior in large language models (LLMs), which we term \"Verbosity\nCompensation\" (VC). VC is harmful because it confuses the user understanding,\nleading to low efficiency, and influences the LLM services by increasing the\nlatency and cost of generating useless tokens. In this paper, we present the\nfirst work that defines and analyzes Verbosity Compensation, explores its\ncauses, and proposes a simple mitigating approach. We define Verbosity\nCompensation as the behavior of generating responses that can be compressed\nwithout information loss when prompted to write concisely. Our experiments,\nconducted on five datasets of knowledge and reasoning-based QA tasks with 14\nnewly developed LLMs, reveal three conclusions. 1) We reveal a pervasive\npresence of verbosity compensation across all models and all datasets. Notably,\nGPT-4 exhibits a VC frequency of 50.40%. 2) We reveal the large performance gap\nbetween verbose and concise responses, with a notable difference of 27.61% on\nthe Qasper dataset. We also demonstrate that this difference does not naturally\ndiminish as LLM capability increases. Both 1) and 2) highlight the urgent need\nto mitigate the frequency of VC behavior and disentangle verbosity with\nveracity. We propose a simple yet effective cascade algorithm that replaces the\nverbose responses with the other model-generated responses. The results show\nthat our approach effectively alleviates the VC of the Mistral model from\n63.81% to 16.16% on the Qasper dataset. 3) We also find that verbose responses\nexhibit higher uncertainty across all five datasets, suggesting a strong\nconnection between verbosity and model uncertainty. Our dataset and code are\navailable at https://github.com/psunlpgroup/VerbosityLLM.", "paper_summary_zh": "<paragraph>\u7576\u4e0d\u78ba\u5b9a\u7b54\u6848\u6642\uff0c\u4eba\u985e\u901a\u5e38\u6703\u7528\u6bd4\u5fc5\u8981\u66f4\u591a\u7684\u5b57\u8a5e\u4f86\u56de\u7b54\uff0c\u5e0c\u671b\u7b54\u6848\u7684\u4e00\u90e8\u5206\u6703\u662f\u6b63\u78ba\u7684\u3002\u6211\u5011\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u89c0\u5bdf\u5230\u985e\u4f3c\u7684\u884c\u70ba\uff0c\u6211\u5011\u7a31\u4e4b\u70ba\u300c\u5197\u9918\u88dc\u511f\u300d(VC)\u3002VC \u6709\u5bb3\uff0c\u56e0\u70ba\u5b83\u6703\u6df7\u6dc6\u4f7f\u7528\u8005\u7684\u7406\u89e3\uff0c\u5c0e\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u4e26\u900f\u904e\u589e\u52a0\u7522\u751f\u7121\u7528\u4ee3\u5e63\u7684\u5ef6\u9072\u548c\u6210\u672c\u4f86\u5f71\u97ff LLM \u670d\u52d9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7b2c\u4e00\u500b\u5b9a\u7fa9\u548c\u5206\u6790\u5197\u9918\u88dc\u511f\u7684\u5de5\u4f5c\uff0c\u63a2\u8a0e\u5176\u539f\u56e0\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u7684\u7de9\u89e3\u65b9\u6cd5\u3002\u6211\u5011\u5c07\u5197\u9918\u88dc\u511f\u5b9a\u7fa9\u70ba\u5728\u63d0\u793a\u7c21\u6f54\u5beb\u4f5c\u6642\uff0c\u7522\u751f\u53ef\u4ee5\u58d3\u7e2e\u4e14\u4e0d\u5931\u8cc7\u8a0a\u7684\u56de\u61c9\u7684\u884c\u70ba\u3002\u6211\u5011\u7684\u5be6\u9a57\u5728\u4e94\u500b\u77e5\u8b58\u548c\u57fa\u65bc\u63a8\u7406\u7684\u554f\u7b54\u4efb\u52d9\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\uff0c\u4e26\u4f7f\u7528 14 \u500b\u65b0\u958b\u767c\u7684 LLM\uff0c\u63ed\u793a\u4e86\u4e09\u500b\u7d50\u8ad6\u30021) \u6211\u5011\u63ed\u793a\u4e86\u6240\u6709\u6a21\u578b\u548c\u6240\u6709\u8cc7\u6599\u96c6\u4e2d\u666e\u904d\u5b58\u5728\u5197\u9918\u88dc\u511f\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cGPT-4 \u7684 VC \u983b\u7387\u70ba 50.40%\u30022) \u6211\u5011\u63ed\u793a\u4e86\u5197\u9577\u548c\u7c21\u6f54\u56de\u61c9\u4e4b\u9593\u7684\u5de8\u5927\u6548\u80fd\u5dee\u8ddd\uff0c\u5728 Qasper \u8cc7\u6599\u96c6\u4e0a\u5b58\u5728 27.61% \u7684\u986f\u8457\u5dee\u7570\u3002\u6211\u5011\u9084\u8b49\u660e\u4e86\u96a8\u8457 LLM \u80fd\u529b\u7684\u63d0\u9ad8\uff0c\u9019\u7a2e\u5dee\u7570\u4e26\u4e0d\u6703\u81ea\u7136\u6d88\u5931\u30021) \u548c 2) \u90fd\u5f37\u8abf\u4e86\u7de9\u89e3 VC \u884c\u70ba\u983b\u7387\u548c\u5340\u5206\u5197\u9918\u8207\u771f\u5be6\u6027\u7684\u8feb\u5207\u9700\u8981\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u4e32\u806f\u6f14\u7b97\u6cd5\uff0c\u7528\u5176\u4ed6\u6a21\u578b\u7522\u751f\u7684\u56de\u61c9\u53d6\u4ee3\u5197\u9577\u7684\u56de\u61c9\u3002\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u6709\u6548\u5730\u5c07 Mistral \u6a21\u578b\u5728 Qasper \u8cc7\u6599\u96c6\u4e0a\u7684 VC \u5f9e 63.81% \u6e1b\u8f15\u5230 16.16%\u30023) \u6211\u5011\u9084\u767c\u73fe\uff0c\u5728\u6240\u6709\u4e94\u500b\u8cc7\u6599\u96c6\u4e2d\uff0c\u5197\u9577\u7684\u56de\u61c9\u8868\u73fe\u51fa\u66f4\u9ad8\u7684\u4e0d\u78ba\u5b9a\u6027\uff0c\u9019\u8868\u660e\u5197\u9918\u8207\u6a21\u578b\u4e0d\u78ba\u5b9a\u6027\u4e4b\u9593\u5b58\u5728\u5f37\u70c8\u7684\u95dc\u806f\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/psunlpgroup/VerbosityLLM \u53d6\u5f97\u3002</paragraph>", "author": "Yusen Zhang et.al.", "authors": "Yusen Zhang, Sarkar Snigdha Sarathi Das, Rui Zhang", "id": "2411.07858v1", "paper_url": "http://arxiv.org/abs/2411.07858v1", "repo": "https://github.com/psunlpgroup/verbosityllm"}}