{"2411.16260": {"publish_time": "2024-11-25", "title": "Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures", "paper_summary": "Large language models (LLMs) have demonstrated remarkable mathematical\ncapabilities, largely driven by chain-of-thought (CoT) prompting, which\ndecomposes complex reasoning into step-by-step solutions. This approach has\nenabled significant advancements, as evidenced by performance on benchmarks\nlike GSM8K and MATH. However, the mechanisms underlying LLMs' ability to\nperform arithmetic in a single step of CoT remain poorly understood. Existing\nstudies debate whether LLMs encode numerical values or rely on symbolic\nreasoning, while others explore attention and multi-layered processing in\narithmetic tasks. In this work, we propose that LLMs learn arithmetic by\ncapturing algebraic structures, such as \\emph{Commutativity} and\n\\emph{Identity} properties. Since these structures are observable through\ninput-output relationships, they can generalize to unseen data. We empirically\ndemonstrate that LLMs can learn algebraic structures using a custom dataset of\narithmetic problems. Our findings indicate that leveraging algebraic structures\ncan enhance the LLMs' arithmetic capabilities, offering insights into improving\ntheir arithmetic performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6578\u5b78\u80fd\u529b\uff0c\u9019\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u7531\u601d\u8003\u93c8 (CoT) \u63d0\u793a\u9a45\u52d5\u7684\uff0c\u5b83\u5c07\u8907\u96dc\u7684\u63a8\u7406\u5206\u89e3\u70ba\u9010\u6b65\u89e3\u6c7a\u65b9\u6848\u3002\u9019\u7a2e\u65b9\u6cd5\u5df2\u5be6\u73fe\u986f\u8457\u7684\u9032\u6b65\uff0c\u5982 GSM8K \u548c MATH \u7b49\u57fa\u6e96\u6e2c\u8a66\u7684\u8868\u73fe\u6240\u8b49\u660e\u3002\u7136\u800c\uff0cLLM \u5728 CoT \u7684\u55ae\u4e00\u6b65\u9a5f\u4e2d\u57f7\u884c\u7b97\u8853\u7684\u80fd\u529b\u80cc\u5f8c\u6a5f\u5236\u4ecd\u9bae\u70ba\u4eba\u77e5\u3002\u73fe\u6709\u7814\u7a76\u722d\u8ad6 LLM \u662f\u5426\u7de8\u78bc\u6578\u503c\u6216\u4f9d\u8cf4\u65bc\u7b26\u865f\u63a8\u7406\uff0c\u800c\u53e6\u4e00\u4e9b\u7814\u7a76\u5247\u63a2\u8a0e\u7b97\u8853\u4efb\u52d9\u4e2d\u7684\u6ce8\u610f\u529b\u548c\u591a\u5c64\u8655\u7406\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa LLM \u900f\u904e\u64f7\u53d6\u4ee3\u6578\u7d50\u69cb\uff08\u4f8b\u5982\u4ea4\u63db\u5f8b\u548c\u6046\u7b49\u6027\uff09\u4f86\u5b78\u7fd2\u7b97\u8853\u3002\u7531\u65bc\u9019\u4e9b\u7d50\u69cb\u53ef\u900f\u904e\u8f38\u5165\u8f38\u51fa\u95dc\u4fc2\u89c0\u5bdf\u5230\uff0c\u56e0\u6b64\u5b83\u5011\u53ef\u4ee5\u63a8\u5ee3\u5230\u672a\u898b\u7684\u6578\u64da\u3002\u6211\u5011\u900f\u904e\u4f7f\u7528\u7b97\u8853\u554f\u984c\u81ea\u8a02\u8cc7\u6599\u96c6\u5be6\u8b49\u8b49\u660e LLM \u53ef\u4ee5\u5b78\u7fd2\u4ee3\u6578\u7d50\u69cb\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u5229\u7528\u4ee3\u6578\u7d50\u69cb\u53ef\u4ee5\u589e\u5f37 LLM \u7684\u7b97\u8853\u80fd\u529b\uff0c\u4e26\u63d0\u4f9b\u898b\u89e3\u4ee5\u6539\u5584\u5176\u7b97\u8853\u8868\u73fe\u3002", "author": "Fu-Chieh Chang et.al.", "authors": "Fu-Chieh Chang, Pei-Yuan Wu", "id": "2411.16260v1", "paper_url": "http://arxiv.org/abs/2411.16260v1", "repo": "null"}}