{"2411.11531": {"publish_time": "2024-11-18", "title": "Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality", "paper_summary": "In this paper we present an approach to reduce hallucinations in Large\nLanguage Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional\nmodality. Our method involves transforming input text into a set of KG\nembeddings and using an adapter to integrate these embeddings into the language\nmodel space, without relying on external retrieval processes.\n  To facilitate this, we created WikiEntities, a dataset containing over 3\nmillion Wikipedia texts annotated with entities from Wikidata and their\ncorresponding embeddings from PyTorch-BigGraph. This dataset serves as a\nvaluable resource for training Entity Linking models and adapting the described\nmethod to various LLMs using specialized adapters.\n  Our method does not require fine-tuning of the language models themselves;\ninstead, we only train the adapter. This ensures that the model's performance\non other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA\n2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and\ndemonstrated that our approach improves performance on the HaluEval, True-False\nbenchmarks and FEVER dataset. The results indicate that incorporating KGs as a\nnew modality can effectively reduce hallucinations and improve the factual\naccuracy of language models, all without the need for external retrieval.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\uff0c\u900f\u904e\u5c07\u77e5\u8b58\u5716\u8b5c (KG) \u4f5c\u70ba\u9644\u52a0\u65b9\u5f0f\u7d0d\u5165\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4ee5\u6e1b\u5c11\u5e7b\u89ba\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u62ec\u5c07\u8f38\u5165\u6587\u5b57\u8f49\u63db\u6210\u4e00\u7d44 KG \u5d4c\u5165\uff0c\u4e26\u4f7f\u7528\u9069\u914d\u5668\u5c07\u9019\u4e9b\u5d4c\u5165\u6574\u5408\u5230\u8a9e\u8a00\u6a21\u578b\u7a7a\u9593\uff0c\u800c\u7121\u9700\u4f9d\u8cf4\u5916\u90e8\u6aa2\u7d22\u7a0b\u5e8f\u3002\n\u70ba\u4e86\u4fc3\u9032\u9019\u4e00\u9ede\uff0c\u6211\u5011\u5efa\u7acb\u4e86 WikiEntities\uff0c\u9019\u662f\u4e00\u500b\u5305\u542b\u8d85\u904e 300 \u842c\u500b\u7dad\u57fa\u767e\u79d1\u6587\u5b57\u7684\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u9644\u6709\u4f86\u81ea Wikidata \u7684\u5be6\u9ad4\u8a3b\u89e3\uff0c\u4ee5\u53ca\u5b83\u5011\u4f86\u81ea PyTorch-BigGraph \u7684\u5c0d\u61c9\u5d4c\u5165\u3002\u6b64\u8cc7\u6599\u96c6\u4f5c\u70ba\u8a13\u7df4\u5be6\u9ad4\u9023\u7d50\u6a21\u578b\u548c\u4f7f\u7528\u5c08\u9580\u9069\u914d\u5668\u5c07\u6240\u8ff0\u65b9\u6cd5\u8abf\u6574\u5230\u5404\u7a2e LLM \u7684\u5bf6\u8cb4\u8cc7\u6e90\u3002\n\u6211\u5011\u7684\u505a\u6cd5\u4e0d\u9700\u8981\u5fae\u8abf\u8a9e\u8a00\u6a21\u578b\u672c\u8eab\uff1b\u76f8\u53cd\uff0c\u6211\u5011\u53ea\u8a13\u7df4\u9069\u914d\u5668\u3002\u9019\u78ba\u4fdd\u4e86\u6a21\u578b\u5728\u5176\u4ed6\u4efb\u52d9\u4e0a\u7684\u6548\u80fd\u4e0d\u53d7\u5f71\u97ff\u3002\u6211\u5011\u4f7f\u7528\u6b64\u8cc7\u6599\u96c6\u8a13\u7df4\u4e86 Mistral 7B\u3001LLaMA 2-7B (\u804a\u5929) \u548c LLaMA 3-8B (\u6307\u4ee4) \u6a21\u578b\u7684\u9069\u914d\u5668\uff0c\u4e26\u8b49\u660e\u4e86\u6211\u5011\u7684\u505a\u6cd5\u6539\u5584\u4e86 HaluEval\u3001\u771f\u5047\u57fa\u6e96\u548c FEVER \u8cc7\u6599\u96c6\u7684\u6548\u80fd\u3002\u7d50\u679c\u8868\u660e\uff0c\u5c07 KG \u4f5c\u70ba\u4e00\u7a2e\u65b0\u65b9\u5f0f\u7d0d\u5165\u53ef\u4ee5\u6709\u6548\u6e1b\u5c11\u5e7b\u89ba\uff0c\u4e26\u63d0\u9ad8\u8a9e\u8a00\u6a21\u578b\u7684\u4e8b\u5be6\u6e96\u78ba\u6027\uff0c\u800c\u7121\u9700\u5916\u90e8\u6aa2\u7d22\u3002</paragraph>", "author": "Viktoriia Chekalina et.al.", "authors": "Viktoriia Chekalina, Anton Razzigaev, Elizaveta Goncharova, Andrey Kuznetsov", "id": "2411.11531v1", "paper_url": "http://arxiv.org/abs/2411.11531v1", "repo": "null"}}