{"2406.12036": {"publish_time": "2024-06-17", "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations", "paper_summary": "As opposed to evaluating computation and logic-based reasoning, current\nbench2 marks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive rea4 soning. While such qualitative capabilities are vital to\nmedical diagnosis, in real5 world scenarios, doctors frequently use clinical\ncalculators that follow quantitative equations and rule-based reasoning\nparadigms for evidence-based decision support. To this end, we propose\nMedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical\ncalculation capability of LLMs. MedCalc-Bench contains an evaluation set of\nover 1000 manually reviewed instances from 55 different medical calculation\ntasks. Each instance in MedCalc-Bench consists of a patient note, a question\nrequesting to compute a specific medical value, a ground truth answer, and a\nstep-by-step explanation showing how the answer is obtained. While our\nevaluation results show the potential of LLMs in this area, none of them are\neffective enough for clinical settings. Common issues include extracting the\nincorrect entities, not using the correct equation or rules for a calculation\ntask, or incorrectly performing the arithmetic for the computation. We hope our\nstudy highlights the quantitative knowledge and reasoning gaps in LLMs within\nmedical settings, encouraging future improvements of LLMs for various clinical\ncalculation tasks.", "paper_summary_zh": "\u8207\u8a55\u4f30\u904b\u7b97\u548c\u57fa\u65bc\u908f\u8f2f\u7684\u63a8\u7406\u76f8\u53cd\uff0c\u76ee\u524d\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u91ab\u5b78\u4e2d\u7684\u57fa\u6e96\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u6d89\u53ca\u9818\u57df\u77e5\u8b58\u548c\u63cf\u8ff0\u6027\u63a8\u7406\u7684\u554f\u7b54\u4e0a\u3002\u96d6\u7136\u9019\u4e9b\u5b9a\u6027\u80fd\u529b\u5c0d\u91ab\u7642\u8a3a\u65b7\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u5728\u5be6\u969b\u60c5\u6cc1\u4e2d\uff0c\u91ab\u751f\u7d93\u5e38\u4f7f\u7528\u9075\u5faa\u5b9a\u91cf\u65b9\u7a0b\u5f0f\u548c\u57fa\u65bc\u898f\u5247\u7684\u63a8\u7406\u7bc4\u5f0f\u7684\u81e8\u5e8a\u8a08\u7b97\u5668\uff0c\u4ee5\u9032\u884c\u5faa\u8b49\u6c7a\u7b56\u652f\u63f4\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 MedCalc-Bench\uff0c\u9019\u662f\u4e00\u500b\u9996\u5275\u7684\u8cc7\u6599\u96c6\uff0c\u5c08\u6ce8\u65bc\u8a55\u4f30 LLM \u7684\u91ab\u7642\u8a08\u7b97\u80fd\u529b\u3002MedCalc-Bench \u5305\u542b\u4e00\u500b\u8a55\u4f30\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4f86\u81ea 55 \u7a2e\u4e0d\u540c\u91ab\u7642\u8a08\u7b97\u4efb\u52d9\u7684 1000 \u591a\u500b\u4eba\u5de5\u5be9\u67e5\u5be6\u4f8b\u3002MedCalc-Bench \u4e2d\u7684\u6bcf\u500b\u5be6\u4f8b\u90fd\u5305\u542b\u4e00\u500b\u60a3\u8005\u5099\u8a3b\u3001\u4e00\u500b\u8a08\u7b97\u7279\u5b9a\u91ab\u7642\u6578\u503c\u7684\u8acb\u6c42\u554f\u984c\u3001\u4e00\u500b\u57fa\u672c\u4e8b\u5be6\u7b54\u6848\uff0c\u4ee5\u53ca\u4e00\u500b\u5206\u6b65\u8aaa\u660e\uff0c\u8aaa\u660e\u5982\u4f55\u7372\u5f97\u7b54\u6848\u3002\u96d6\u7136\u6211\u5011\u7684\u8a55\u4f30\u7d50\u679c\u986f\u793a\u4e86 LLM \u5728\u9019\u65b9\u9762\u7684\u6f5b\u529b\uff0c\u4f46\u5b83\u5011\u90fd\u4e0d\u8db3\u4ee5\u7528\u65bc\u81e8\u5e8a\u74b0\u5883\u3002\u5e38\u898b\u554f\u984c\u5305\u62ec\u63d0\u53d6\u4e0d\u6b63\u78ba\u7684\u5be6\u9ad4\u3001\u672a\u91dd\u5c0d\u8a08\u7b97\u4efb\u52d9\u4f7f\u7528\u6b63\u78ba\u7684\u65b9\u7a0b\u5f0f\u6216\u898f\u5247\uff0c\u6216\u8a08\u7b97\u7b97\u8853\u6642\u57f7\u884c\u4e0d\u6b63\u78ba\u3002\u6211\u5011\u5e0c\u671b\u6211\u5011\u7684\u7814\u7a76\u80fd\u5f37\u8abf LLM \u5728\u91ab\u7642\u74b0\u5883\u4e2d\u7684\u91cf\u5316\u77e5\u8b58\u548c\u63a8\u7406\u5dee\u8ddd\uff0c\u4e26\u9f13\u52f5\u672a\u4f86\u6539\u9032 LLM\uff0c\u4ee5\u61c9\u4ed8\u5404\u7a2e\u81e8\u5e8a\u8a08\u7b97\u4efb\u52d9\u3002", "author": "Nikhil Khandekar et.al.", "authors": "Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu", "id": "2406.12036v1", "paper_url": "http://arxiv.org/abs/2406.12036v1", "repo": "https://github.com/ncbi-nlp/medcalc-bench"}}