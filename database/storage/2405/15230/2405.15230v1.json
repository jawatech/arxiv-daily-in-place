{"2405.15230": {"publish_time": "2024-05-24", "title": "$i$REPO: $i$mplicit Reward Pairwise Difference based Empirical Preference Optimization", "paper_summary": "While astonishingly capable, large Language Models (LLM) can sometimes\nproduce outputs that deviate from human expectations. Such deviations\nnecessitate an alignment phase to prevent disseminating untruthful, toxic, or\nbiased information. Traditional alignment methods based on reinforcement\nlearning often struggle with the identified instability, whereas preference\noptimization methods are limited by their overfitting to pre-collected\nhard-label datasets. In this paper, we propose a novel LLM alignment framework\nnamed $i$REPO, which utilizes implicit Reward pairwise difference regression\nfor Empirical Preference Optimization. Particularly, $i$REPO employs\nself-generated datasets labelled by empirical human (or AI annotator)\npreference to iteratively refine the aligned policy through a novel\nregression-based loss function. Furthermore, we introduce an innovative\nalgorithm backed by theoretical guarantees for achieving optimal results under\nideal assumptions and providing a practical performance-gap result without such\nassumptions. Experimental results with Phi-2 and Mistral-7B demonstrate that\n$i$REPO effectively achieves self-alignment using soft-label, self-generated\nresponses and the logit of empirical AI annotators. Furthermore, our approach\nsurpasses preference optimization baselines in evaluations using the Language\nModel Evaluation Harness and Multi-turn benchmarks.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5177\u6709\u9a5a\u4eba\u7684\u80fd\u529b\uff0c\u4f46\u6709\u6642\u4ecd\u6703\u7522\u751f\u504f\u96e2\u4eba\u985e\u9810\u671f\u7684\u8f38\u51fa\u3002\u6b64\u985e\u504f\u5dee\u9700\u8981\u6821\u6e96\u968e\u6bb5\u624d\u80fd\u9632\u6b62\u6563\u5e03\u4e0d\u5be6\u3001\u6709\u5bb3\u6216\u6709\u504f\u898b\u7684\u8cc7\u8a0a\u3002\u50b3\u7d71\u7684\u6821\u6e96\u65b9\u6cd5\u57fa\u65bc\u5f37\u5316\u5b78\u7fd2\uff0c\u901a\u5e38\u96e3\u4ee5\u8655\u7406\u5df2\u8b58\u5225\u7684\u4e0d\u7a69\u5b9a\u6027\uff0c\u800c\u504f\u597d\u6700\u4f73\u5316\u65b9\u6cd5\u5247\u53d7\u9650\u65bc\u904e\u5ea6\u64ec\u5408\u9810\u5148\u6536\u96c6\u7684\u786c\u6a19\u7c64\u8cc7\u6599\u96c6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u540d\u70ba $i$REPO \u7684\u65b0\u578b LLM \u6821\u6e96\u67b6\u69cb\uff0c\u5b83\u5229\u7528\u96b1\u5f0f\u734e\u52f5\u6210\u5c0d\u5dee\u7570\u56de\u6b78\u9032\u884c\u7d93\u9a57\u504f\u597d\u6700\u4f73\u5316\u3002\u7279\u5225\u662f\uff0c$i$REPO \u4f7f\u7528\u7531\u7d93\u9a57\u4eba\u985e\uff08\u6216 AI \u6a19\u8a3b\u54e1\uff09\u504f\u597d\u6a19\u7c64\u7684\u81ea\u751f\u8cc7\u6599\u96c6\uff0c\u900f\u904e\u65b0\u578b\u7684\u57fa\u65bc\u56de\u6b78\u7684\u640d\u5931\u51fd\u6578\u53cd\u8986\u512a\u5316\u6821\u6e96\u653f\u7b56\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u5275\u65b0\u7684\u6f14\u7b97\u6cd5\uff0c\u4e26\u63d0\u4f9b\u7406\u8ad6\u4fdd\u8b49\uff0c\u53ef\u5728\u7406\u60f3\u5047\u8a2d\u4e0b\u9054\u6210\u6700\u4f73\u7d50\u679c\uff0c\u4e26\u5728\u6c92\u6709\u6b64\u985e\u5047\u8a2d\u7684\u60c5\u6cc1\u4e0b\u63d0\u4f9b\u5be6\u969b\u7684\u6548\u80fd\u5dee\u8ddd\u7d50\u679c\u3002\u4f7f\u7528 Phi-2 \u548c Mistral-7B \u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c$i$REPO \u6709\u6548\u5730\u4f7f\u7528\u8edf\u6a19\u7c64\u3001\u81ea\u751f\u56de\u61c9\u548c\u7d93\u9a57 AI \u6a19\u8a3b\u54e1\u7684 logit \u9054\u5230\u81ea\u6211\u6821\u6e96\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u4f7f\u7528\u8a9e\u8a00\u6a21\u578b\u8a55\u4f30\u5de5\u5177\u548c\u591a\u8f2a\u57fa\u6e96\u9032\u884c\u8a55\u4f30\u6642\uff0c\u8d85\u8d8a\u4e86\u504f\u597d\u6700\u4f73\u5316\u57fa\u6e96\u3002", "author": "Long Tan Le et.al.", "authors": "Long Tan Le, Han Shu, Tung-Anh Nguyen, Choong Seon Hong, Nguyen H. Tran", "id": "2405.15230v1", "paper_url": "http://arxiv.org/abs/2405.15230v1", "repo": "null"}}