{"2409.05486": {"publish_time": "2024-09-09", "title": "Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models", "paper_summary": "The quality and capabilities of large language models cannot be currently\nfully assessed with automated, benchmark evaluations. Instead, human\nevaluations that expand on traditional qualitative techniques from natural\nlanguage generation literature are required. One recent best-practice consists\nin using A/B-testing frameworks, which capture preferences of human evaluators\nfor specific models. In this paper we describe a human evaluation experiment\nfocused on the biomedical domain (health, biology, chemistry/pharmacology)\ncarried out at Elsevier. In it a large but not massive (8.8B parameter)\ndecoder-only foundational transformer trained on a relatively small (135B\ntokens) but highly curated collection of Elsevier datasets is compared to\nOpenAI's GPT-3.5-turbo and Meta's foundational 7B parameter Llama 2 model\nagainst multiple criteria. Results indicate -- even if IRR scores were\ngenerally low -- a preference towards GPT-3.5-turbo, and hence towards models\nthat possess conversational abilities, are very large and were trained on very\nlarge datasets. But at the same time, indicate that for less massive models\ntraining on smaller but well-curated training sets can potentially give rise to\nviable alternatives in the biomedical domain.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u54c1\u8cea\u548c\u80fd\u529b\u76ee\u524d\u7121\u6cd5\u4f7f\u7528\u81ea\u52d5\u5316\u57fa\u6e96\u8a55\u4f30\u4f86\u5b8c\u5168\u8a55\u4f30\u3002\u76f8\u53cd\uff0c\u9700\u8981\u4f7f\u7528\u64f4\u5c55\u81ea\u7136\u8a9e\u8a00\u751f\u6210\u6587\u737b\u4e2d\u50b3\u7d71\u5b9a\u6027\u6280\u8853\u7684\u4eba\u985e\u8a55\u4f30\u3002\u4e00\u500b\u6700\u8fd1\u7684\u6700\u4f73\u5be6\u52d9\u662f\u4f7f\u7528 A/B \u6e2c\u8a66\u6846\u67b6\uff0c\u8a72\u6846\u67b6\u6703\u64f7\u53d6\u4eba\u985e\u8a55\u4f30\u8005\u5c0d\u7279\u5b9a\u6a21\u578b\u7684\u504f\u597d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u5c08\u6ce8\u65bc\u751f\u7269\u91ab\u5b78\u9818\u57df\uff08\u5065\u5eb7\u3001\u751f\u7269\u5b78\u3001\u5316\u5b78/\u85e5\u7406\u5b78\uff09\u7684\u4eba\u985e\u8a55\u4f30\u5be6\u9a57\uff0c\u8a72\u5be6\u9a57\u5728 Elsevier \u9032\u884c\u3002\u5176\u4e2d\uff0c\u4e00\u500b\u5927\u578b\u4f46\u4e26\u975e\u9f90\u5927\uff088.8B \u53c3\u6578\uff09\u50c5\u89e3\u78bc\u5668\u57fa\u790eTransformer\u5728\u76f8\u5c0d\u8f03\u5c0f\uff08135B \u4ee4\u724c\uff09\u4f46\u7d93\u904e\u9ad8\u5ea6\u7b56\u5c55\u7684 Elsevier \u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\uff0c\u8207 OpenAI \u7684 GPT-3.5-turbo \u548c Meta \u7684\u57fa\u790e 7B \u53c3\u6578 Llama 2 \u6a21\u578b\u9032\u884c\u6bd4\u8f03\uff0c\u91dd\u5c0d\u591a\u500b\u6a19\u6e96\u3002\u7d50\u679c\u8868\u660e\u2014\u2014\u5373\u4f7f IRR \u5206\u6578\u666e\u904d\u8f03\u4f4e\u2014\u2014\u504f\u597d GPT-3.5-turbo\uff0c\u56e0\u6b64\u504f\u597d\u5177\u5099\u5c0d\u8a71\u80fd\u529b\u3001\u975e\u5e38\u9f90\u5927\u4e14\u5728\u975e\u5e38\u9f90\u5927\u7684\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u3002\u4f46\u540c\u6642\uff0c\u8868\u660e\u5c0d\u65bc\u8f03\u5c0f\u898f\u6a21\u7684\u6a21\u578b\uff0c\u5728\u8f03\u5c0f\u4f46\u7d93\u904e\u826f\u597d\u7b56\u5c55\u7684\u8a13\u7df4\u96c6\u4e0a\u8a13\u7df4\uff0c\u6709\u53ef\u80fd\u5728\u751f\u7269\u91ab\u5b78\u9818\u57df\u7522\u751f\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "author": "Camilo Thorne et.al.", "authors": "Camilo Thorne, Christian Druckenbrodt, Kinga Szarkowska, Deepika Goyal, Pranita Marajan, Vijay Somanath, Corey Harper, Mao Yan, Tony Scerri", "id": "2409.05486v1", "paper_url": "http://arxiv.org/abs/2409.05486v1", "repo": "null"}}