{"2502.13107": {"publish_time": "2025-02-18", "title": "MatterChat: A Multi-Modal LLM for Material Science", "paper_summary": "Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis.", "paper_summary_zh": "\u4e86\u89e3\u548c\u9810\u6e2c\u7121\u6a5f\u6750\u6599\u7684\u7279\u6027\u5c0d\u65bc\u52a0\u901f\u6750\u6599\u79d1\u5b78\u7684\u9032\u6b65\u548c\u63a8\u52d5\u80fd\u6e90\u3001\u96fb\u5b50\u7b49\u65b9\u9762\u7684\u61c9\u7528\u81f3\u95dc\u91cd\u8981\u3002\u900f\u904e\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c07\u6750\u6599\u7d50\u69cb\u6578\u64da\u8207\u57fa\u65bc\u8a9e\u8a00\u7684\u8cc7\u8a0a\u6574\u5408\uff0c\u53ef\u4ee5\u6975\u5927\u7a0b\u5ea6\u5730\u652f\u6301\u9019\u4e9b\u5de5\u4f5c\uff0c\u85c9\u6b64\u589e\u5f37\u4eba\u985e\u8207 AI \u7684\u4e92\u52d5\u3002\u7136\u800c\uff0c\u4e00\u500b\u95dc\u9375\u6311\u6230\u5728\u65bc\u5c07\u539f\u5b50\u7d50\u69cb\u4ee5\u5b8c\u6574\u89e3\u6790\u5ea6\u6574\u5408\u5230 LLM \u4e2d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 MatterChat\uff0c\u9019\u662f\u4e00\u500b\u901a\u7528\u7684\u7d50\u69cb\u611f\u77e5\u591a\u6a21\u614b LLM\uff0c\u5b83\u5c07\u6750\u6599\u7d50\u69cb\u6578\u64da\u548c\u6587\u5b57\u8f38\u5165\u7d71\u4e00\u5230\u4e00\u500b\u55ae\u4e00\u7684\u5167\u805a\u6a21\u578b\u4e2d\u3002MatterChat \u63a1\u7528\u6a4b\u63a5\u6a21\u7d44\uff0c\u5c07\u9810\u5148\u8a13\u7df4\u597d\u7684\u6a5f\u5668\u5b78\u7fd2\u539f\u5b50\u9593\u96fb\u4f4d\u8207\u9810\u5148\u8a13\u7df4\u597d\u7684 LLM \u6709\u6548\u5730\u5c0d\u9f4a\uff0c\u5f9e\u800c\u964d\u4f4e\u8a13\u7df4\u6210\u672c\u4e26\u589e\u5f37\u9748\u6d3b\u6027\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0cMatterChat \u5927\u5e45\u63d0\u5347\u4e86\u6750\u6599\u7279\u6027\u9810\u6e2c\u548c\u4eba\u985e\u8207 AI \u4e92\u52d5\u7684\u6548\u80fd\uff0c\u8d85\u8d8a\u4e86 GPT-4 \u7b49\u901a\u7528 LLM\u3002\u6211\u5011\u4e5f\u5c55\u793a\u4e86\u5b83\u5728\u66f4\u9032\u968e\u7684\u79d1\u5b78\u63a8\u7406\u548c\u9010\u6b65\u6750\u6599\u5408\u6210\u7b49\u61c9\u7528\u4e2d\u7684\u6548\u7528\u3002", "author": "Yingheng Tang et.al.", "authors": "Yingheng Tang, Wenbin Xu, Jie Cao, Jianzhu Ma, Weilu Gao, Steve Farrell, Benjamin Erichson, Michael W. Mahoney, Andy Nonaka, Zhi Yao", "id": "2502.13107v1", "paper_url": "http://arxiv.org/abs/2502.13107v1", "repo": "null"}}