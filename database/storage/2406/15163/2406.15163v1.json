{"2406.15163": {"publish_time": "2024-06-21", "title": "A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis", "paper_summary": "Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing\n(NLP), addressing subjective assessments in textual content. Syntactic parsing\nis useful in SA because explicit syntactic information can improve accuracy\nwhile providing explainability, but it tends to be a computational bottleneck\nin practice due to the slowness of parsing algorithms. This paper addresses\nsaid bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject\nsyntax into SA. By treating dependency parsing as a sequence labeling problem,\nwe greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated\non a ternary polarity classification task, demonstrating its faster performance\nand better accuracy in polarity prediction tasks compared to conventional\nparsers like Stanza and to heuristic approaches that use shallow syntactic\nrules for SA like VADER. This increased speed and improved accuracy make SELSP\nparticularly appealing to SA practitioners in both research and industry. In\naddition, we test several sentiment dictionaries on our SELSP to see which one\nimproves the performance in polarity prediction tasks. Moreover, we compare the\nSELSP with Transformer-based models trained on a 5-label classification task.\nThe results show that dictionaries that capture polarity judgment variation\nprovide better results than dictionaries that ignore polarity judgment\nvariation. Moreover, we show that SELSP is considerably faster than\nTransformer-based models in polarity prediction tasks.", "paper_summary_zh": "\u60c5\u7dd2\u5206\u6790 (SA) \u662f\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u7684\u4e00\u500b\u95dc\u9375\u9762\u5411\uff0c\u7528\u65bc\u8655\u7406\u6587\u672c\u5167\u5bb9\u4e2d\u7684\u4e3b\u89c0\u8a55\u4f30\u3002\u53e5\u6cd5\u5206\u6790\u5728 SA \u4e2d\u5f88\u6709\u7528\uff0c\u56e0\u70ba\u660e\u78ba\u7684\u53e5\u6cd5\u8cc7\u8a0a\u53ef\u4ee5\u63d0\u9ad8\u6e96\u78ba\u6027\uff0c\u540c\u6642\u63d0\u4f9b\u53ef\u89e3\u91cb\u6027\uff0c\u4f46\u7531\u65bc\u5206\u6790\u6f14\u7b97\u6cd5\u7684\u901f\u5ea6\u6162\uff0c\u5728\u5be6\u52d9\u4e0a\u5f80\u5f80\u6703\u6210\u70ba\u904b\u7b97\u74f6\u9838\u3002\u672c\u6587\u900f\u904e\u4f7f\u7528\u5e8f\u5217\u6a19\u7c64\u53e5\u6cd5\u5206\u6790\u5668 (SELSP) \u4f86\u5c07\u53e5\u6cd5\u6ce8\u5165 SA\uff0c\u4ee5\u89e3\u6c7a\u4e0a\u8ff0\u74f6\u9838\u3002\u900f\u904e\u5c07\u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u8996\u70ba\u5e8f\u5217\u6a19\u7c64\u554f\u984c\uff0c\u6211\u5011\u5927\u5e45\u63d0\u5347\u4e86\u57fa\u65bc\u53e5\u6cd5\u7684 SA \u901f\u5ea6\u3002SELSP \u63a5\u53d7\u4e09\u5143\u6975\u6027\u5206\u985e\u4efb\u52d9\u7684\u8a13\u7df4\u548c\u8a55\u4f30\uff0c\u8b49\u660e\u5176\u5728\u6975\u6027\u9810\u6e2c\u4efb\u52d9\u4e2d\u7684\u57f7\u884c\u901f\u5ea6\u8f03\u5feb\uff0c\u4e14\u6e96\u78ba\u5ea6\u8f03\u9ad8\uff0c\u512a\u65bc Stanza \u7b49\u50b3\u7d71\u5206\u6790\u5668\u548c\u4f7f\u7528\u6dfa\u5c64\u53e5\u6cd5\u898f\u5247\u9032\u884c SA \u7684\u555f\u767c\u5f0f\u65b9\u6cd5\uff0c\u4f8b\u5982 VADER\u3002\u9019\u7a2e\u901f\u5ea6\u63d0\u5347\u548c\u6e96\u78ba\u5ea6\u6539\u5584\u8b93 SELSP \u7279\u5225\u53d7\u5230\u7814\u7a76\u548c\u7522\u696d\u4e2d SA \u5f9e\u696d\u4eba\u54e1\u7684\u9752\u775e\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728 SELSP \u4e0a\u6e2c\u8a66\u4e86\u591a\u500b\u60c5\u7dd2\u8a5e\u5178\uff0c\u4ee5\u627e\u51fa\u54ea\u4e00\u500b\u8a5e\u5178\u80fd\u6539\u5584\u6975\u6027\u9810\u6e2c\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07 SELSP \u8207\u8a13\u7df4\u5728 5 \u6a19\u7c64\u5206\u985e\u4efb\u52d9\u4e0a\u7684 Transformer-based \u6a21\u578b\u9032\u884c\u6bd4\u8f03\u3002\u7d50\u679c\u986f\u793a\uff0c\u80fd\u6355\u6349\u6975\u6027\u5224\u65b7\u8b8a\u5316\u7684\u8a5e\u5178\uff0c\u5176\u63d0\u4f9b\u7684\u7d50\u679c\u512a\u65bc\u5ffd\u7565\u6975\u6027\u5224\u65b7\u8b8a\u5316\u7684\u8a5e\u5178\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e\u4e86 SELSP \u5728\u6975\u6027\u9810\u6e2c\u4efb\u52d9\u4e2d\u6bd4 Transformer-based \u6a21\u578b\u5feb\u4e0a\u8a31\u591a\u3002", "author": "Muhammad Imran et.al.", "authors": "Muhammad Imran, Olga Kellert, Carlos G\u00f3mez-Rodr\u00edguez", "id": "2406.15163v1", "paper_url": "http://arxiv.org/abs/2406.15163v1", "repo": "null"}}