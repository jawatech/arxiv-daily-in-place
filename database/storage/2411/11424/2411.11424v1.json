{"2411.11424": {"publish_time": "2024-11-18", "title": "Membership Inference Attack against Long-Context Large Language Models", "paper_summary": "Recent advances in Large Language Models (LLMs) have enabled them to overcome\ntheir context window limitations, and demonstrate exceptional retrieval and\nreasoning capacities on longer context. Quesion-answering systems augmented\nwith Long-Context Language Models (LCLMs) can automatically search massive\nexternal data and incorporate it into their contexts, enabling faithful\npredictions and reducing issues such as hallucinations and knowledge staleness.\nExisting studies targeting LCLMs mainly concentrate on addressing the so-called\nlost-in-the-middle problem or improving the inference effiencicy, leaving their\nprivacy risks largely unexplored. In this paper, we aim to bridge this gap and\nargue that integrating all information into the long context makes it a\nrepository of sensitive information, which often contains private data such as\nmedical records or personal identities. We further investigate the membership\nprivacy within LCLMs external context, with the aim of determining whether a\ngiven document or sequence is included in the LCLMs context. Our basic idea is\nthat if a document lies in the context, it will exhibit a low generation loss\nor a high degree of semantic similarity to the contents generated by LCLMs. We\nfor the first time propose six membership inference attack (MIA) strategies\ntailored for LCLMs and conduct extensive experiments on various popular models.\nEmpirical results demonstrate that our attacks can accurately infer membership\nstatus in most cases, e.g., 90.66% attack F1-score on Multi-document QA\ndatasets with LongChat-7b-v1.5-32k, highlighting significant risks of\nmembership leakage within LCLMs input contexts. Furthermore, we examine the\nunderlying reasons why LCLMs are susceptible to revealing such membership\ninformation.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u8b93\u5b83\u5011\u514b\u670d\n\u5176\u80cc\u666f\u7a97\u53e3\u9650\u5236\uff0c\u4e26\u5728\u8f03\u9577\u7684\u80cc\u666f\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u6aa2\u7d22\u548c\n\u63a8\u7406\u80fd\u529b\u3002\u52a0\u5f37 Long-Context Language Models (LCLM) \u7684\u554f\u984c\u89e3\u7b54\u7cfb\u7d71\u53ef\u4ee5\u81ea\u52d5\u641c\u5c0b\u5927\u91cf\n\u5916\u90e8\u8cc7\u6599\u4e26\u5c07\u5176\u7d0d\u5165\u5176\u80cc\u666f\u4e2d\uff0c\u9032\u800c\u5be6\u73fe\u5fe0\u5be6\n\u9810\u6e2c\u4e26\u6e1b\u5c11\u5e7b\u89ba\u548c\u77e5\u8b58\u8001\u820a\u7b49\u554f\u984c\u3002\u91dd\u5c0d LCLM \u7684\u73fe\u6709\u7814\u7a76\u4e3b\u8981\u5c08\u6ce8\u65bc\u89e3\u6c7a\u6240\u8b02\u7684\n\u8ff7\u5931\u5728\u4e2d\u9593\u554f\u984c\u6216\u6539\u5584\u63a8\u8ad6\u6548\u7387\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u63a2\u8a0e\u5176\n\u96b1\u79c1\u98a8\u96aa\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\u4e26\n\u8ad6\u8b49\u5c07\u6240\u6709\u8cc7\u8a0a\u6574\u5408\u5230\u9577\u80cc\u666f\u4e2d\u6703\u4f7f\u5176\u6210\u70ba\u654f\u611f\u8cc7\u8a0a\u7684\u5132\u5b58\u5eab\uff0c\u5176\u4e2d\u901a\u5e38\u5305\u542b\u79c1\u4eba\u8cc7\u6599\uff0c\u4f8b\u5982\n\u75c5\u6b77\u6216\u500b\u4eba\u8eab\u5206\u3002\u6211\u5011\u9032\u4e00\u6b65\u8abf\u67e5 LCLM \u5916\u90e8\u80cc\u666f\u4e2d\u7684\u6210\u54e1\u8eab\u5206\u96b1\u79c1\uff0c\u76ee\u7684\u662f\u78ba\u5b9a\u7d66\u5b9a\u7684\n\u6587\u4ef6\u6216\u5e8f\u5217\u662f\u5426\u5305\u542b\u5728 LCLM \u80cc\u666f\u4e2d\u3002\u6211\u5011\u7684\u57fa\u672c\u60f3\u6cd5\u662f\n\u5982\u679c\u6587\u4ef6\u4f4d\u65bc\u80cc\u666f\u4e2d\uff0c\u5b83\u5c07\u5c55\u73fe\u51fa\u8f03\u4f4e\u7684\u7522\u751f\u640d\u5931\n\u6216\u8207 LCLM \u7522\u751f\u7684\u5167\u5bb9\u6709\u9ad8\u5ea6\u8a9e\u7fa9\u76f8\u4f3c\u6027\u3002\u6211\u5011\n\u9996\u6b21\u63d0\u51fa\u516d\u7a2e\u91dd\u5c0d LCLM \u91cf\u8eab\u6253\u9020\u7684\u6210\u54e1\u8eab\u5206\u63a8\u8ad6\u653b\u64ca (MIA) \u7b56\u7565\uff0c\u4e26\u5c0d\u5404\u7a2e\u71b1\u9580\u6a21\u578b\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\n\u5be6\u8b49\u7d50\u679c\u8b49\u660e\uff0c\u6211\u5011\u7684\u653b\u64ca\u5728\u591a\u6578\u60c5\u6cc1\u4e0b\u53ef\u4ee5\u6e96\u78ba\u63a8\u8ad6\u6210\u54e1\u8eab\u5206\u72c0\u614b\uff0c\u4f8b\u5982\uff0c\u5728\u4f7f\u7528 LongChat-7b-v1.5-32k \u7684\u591a\u6587\u4ef6\u554f\u7b54\n\u8cc7\u6599\u96c6\u4e0a\u653b\u64ca F1 \u5f97\u5206\u70ba 90.66%\uff0c\u7a81\u986f\u4e86 LCLM \u8f38\u5165\u80cc\u666f\u4e2d\u6210\u54e1\u8eab\u5206\u6d29\u6f0f\u7684\u91cd\u5927\u98a8\u96aa\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a2\u8a0e\u4e86 LCLM \u5bb9\u6613\u63ed\u9732\u6b64\u985e\u6210\u54e1\u8eab\u5206\n\u8cc7\u8a0a\u7684\u6839\u672c\u539f\u56e0\u3002</paragraph>", "author": "Zixiong Wang et.al.", "authors": "Zixiong Wang, Gaoyang Liu, Yang Yang, Chen Wang", "id": "2411.11424v1", "paper_url": "http://arxiv.org/abs/2411.11424v1", "repo": "null"}}