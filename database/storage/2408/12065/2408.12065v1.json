{"2408.12065": {"publish_time": "2024-08-22", "title": "Transformers As Approximations of Solomonoff Induction", "paper_summary": "Solomonoff Induction is an optimal-in-the-limit unbounded algorithm for\nsequence prediction, representing a Bayesian mixture of every computable\nprobability distribution and performing close to optimally in predicting any\ncomputable sequence.\n  Being an optimal form of computational sequence prediction, it seems\nplausible that it may be used as a model against which other methods of\nsequence prediction might be compared.\n  We put forth and explore the hypothesis that Transformer models - the basis\nof Large Language Models - approximate Solomonoff Induction better than any\nother extant sequence prediction method. We explore evidence for and against\nthis hypothesis, give alternate hypotheses that take this evidence into\naccount, and outline next steps for modelling Transformers and other kinds of\nAI in this way.", "paper_summary_zh": "Solomonoff \u6b78\u7d0d\u6cd5\u662f\u4e00\u7a2e\u7528\u65bc\u5e8f\u5217\u9810\u6e2c\u7684\u6975\u9650\u6700\u512a\u7121\u754c\u6f14\u7b97\u6cd5\uff0c\u5b83\u4ee3\u8868\u4e86\u6240\u6709\u53ef\u8a08\u7b97\u6a5f\u7387\u5206\u5e03\u7684\u8c9d\u6c0f\u6df7\u5408\uff0c\u4e26\u4e14\u5728\u9810\u6e2c\u4efb\u4f55\u53ef\u8a08\u7b97\u5e8f\u5217\u6642\u63a5\u8fd1\u6700\u4f73\u3002\n\u4f5c\u70ba\u4e00\u7a2e\u8a08\u7b97\u5e8f\u5217\u9810\u6e2c\u7684\u6700\u4f73\u5f62\u5f0f\uff0c\u4f3c\u4e4e\u6709\u53ef\u80fd\u5c07\u5b83\u7528\u4f5c\u4e00\u500b\u6a21\u578b\uff0c\u7528\u4ee5\u6bd4\u8f03\u5176\u4ed6\u5e8f\u5217\u9810\u6e2c\u65b9\u6cd5\u3002\n\u6211\u5011\u63d0\u51fa\u4e26\u63a2\u8a0e\u4e86\u4ee5\u4e0b\u5047\u8a2d\uff1aTransformer \u6a21\u578b\uff08\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u57fa\u790e\uff09\u6bd4\u4efb\u4f55\u5176\u4ed6\u73fe\u5b58\u5e8f\u5217\u9810\u6e2c\u65b9\u6cd5\u66f4\u63a5\u8fd1 Solomonoff \u6b78\u7d0d\u6cd5\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u652f\u6301\u548c\u53cd\u5c0d\u6b64\u5047\u8a2d\u7684\u8b49\u64da\uff0c\u63d0\u51fa\u4e86\u8003\u616e\u6b64\u8b49\u64da\u7684\u5099\u7528\u5047\u8a2d\uff0c\u4e26\u6982\u8ff0\u4e86\u4ee5\u9019\u7a2e\u65b9\u5f0f\u5c0d Transformer \u548c\u5176\u4ed6\u7a2e\u985e\u7684 AI \u9032\u884c\u5efa\u6a21\u7684\u5f8c\u7e8c\u6b65\u9a5f\u3002", "author": "Nathan Young et.al.", "authors": "Nathan Young, Michael Witbrock", "id": "2408.12065v1", "paper_url": "http://arxiv.org/abs/2408.12065v1", "repo": "null"}}