{"2405.07798": {"publish_time": "2024-05-13", "title": "FreeVA: Offline MLLM as Training-Free Video Assistant", "paper_summary": "This paper undertakes an empirical study to revisit the latest advancements\nin Multimodal Large Language Models (MLLMs): Video Assistant. This study,\nnamely FreeVA, aims to extend existing image-based MLLM to the video domain in\na training-free manner. The study provides an essential, yet must-know\nbaseline, and reveals several surprising findings: 1) FreeVA, leveraging only\noffline image-based MLLM without additional training, excels in zero-shot video\nquestion-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even\nsurpassing state-of-the-art methods that involve video instruction tuning. 2)\nWhile mainstream video-based MLLMs typically initialize with an image-based\nMLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study\nindicates that utilizing the widely adopted VideoInstruct-100K for video\ninstruction tuning doesn't actually lead to better performance compared to not\ntraining at all. 3) The commonly used evaluation metrics in existing works are\nsignificantly influenced by changes in the GPT API version over time. If\nignored, this could affect the fairness and uniformity of comparisons between\ndifferent methods and impact the analysis and judgment of researchers in the\nfield. The advancement of MLLMs is currently thriving, drawing numerous\nresearchers into the field. We aim for this work to serve as a plug-and-play,\nsimple yet effective baseline, encouraging the direct evaluation of existing\nMLLMs in video domain while also standardizing the field of video\nconversational models to a certain extent. Also, we encourage researchers to\nreconsider: Have current video MLLM methods truly acquired knowledge beyond\nimage MLLM? Code is available at https://github.com/whwu95/FreeVA", "paper_summary_zh": "<paragraph>\u672c\u6587\u9032\u884c\u4e00\u9805\u5be6\u8b49\u7814\u7a76\uff0c\u4ee5\u91cd\u65b0\u63a2\u8a0e\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4e2d\u7684\u6700\u65b0\u9032\u5c55\uff1a\u5f71\u7247\u52a9\u7406\u3002\u672c\u7814\u7a76\uff0c\u5373 FreeVA\uff0c\u65e8\u5728\u4ee5\u7121\u9700\u8a13\u7df4\u7684\u65b9\u5f0f\u5c07\u73fe\u6709\u7684\u57fa\u65bc\u5f71\u50cf\u7684 MLLM \u64f4\u5c55\u5230\u5f71\u7247\u9818\u57df\u3002\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u500b\u5fc5\u8981\u7684\u4e14\u5fc5\u9808\u77e5\u9053\u7684\u57fa\u6e96\uff0c\u4e26\u63ed\u793a\u4e86\u5e7e\u500b\u4ee4\u4eba\u9a5a\u8a1d\u7684\u767c\u73fe\uff1a1) FreeVA \u50c5\u5229\u7528\u96e2\u7dda\u7684\u57fa\u65bc\u5f71\u50cf\u7684 MLLM\uff0c\u800c\u7121\u9700\u984d\u5916\u7684\u8a13\u7df4\uff0c\u5c31\u80fd\u5728\u96f6\u6b21\u5b78\u7fd2\u5f71\u7247\u554f\u7b54\uff08\u4f8b\u5982 MSVD-QA\u3001ActivityNet-QA \u548c MSRVTT-QA\uff09\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u6d89\u53ca\u5f71\u7247\u6307\u4ee4\u5fae\u8abf\u7684\u73fe\u6709\u65b9\u6cd5\u30022) \u5118\u7ba1\u4e3b\u6d41\u7684\u57fa\u65bc\u5f71\u7247\u7684 MLLM \u901a\u5e38\u4f7f\u7528\u57fa\u65bc\u5f71\u50cf\u7684 MLLM\uff08\u4f8b\u5982 LLaVA\uff09\u521d\u59cb\u5316\uff0c\u7136\u5f8c\u4f7f\u7528\u5f71\u7247\u6307\u4ee4\u5fae\u8abf\u9032\u884c\u5fae\u8abf\uff0c\u4f46\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u5ee3\u6cdb\u63a1\u7528\u7684 VideoInstruct-100K \u9032\u884c\u5f71\u7247\u6307\u4ee4\u5fae\u8abf\u4e26\u672a\u771f\u6b63\u5e36\u4f86\u6bd4\u5b8c\u5168\u4e0d\u9032\u884c\u8a13\u7df4\u66f4\u597d\u7684\u6548\u80fd\u30023) \u73fe\u6709\u7814\u7a76\u4e2d\u5e38\u7528\u7684\u8a55\u91cf\u6307\u6a19\u6703\u96a8\u8457\u6642\u9593\u63a8\u79fb\u800c\u53d7\u5230 GPT API \u7248\u672c\u8b8a\u66f4\u7684\u986f\u8457\u5f71\u97ff\u3002\u5982\u679c\u5ffd\u7565\u9019\u4e00\u9ede\uff0c\u53ef\u80fd\u6703\u5f71\u97ff\u4e0d\u540c\u65b9\u6cd5\u4e4b\u9593\u6bd4\u8f03\u6642\u516c\u5e73\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4e26\u5f71\u97ff\u8a72\u9818\u57df\u7814\u7a76\u4eba\u54e1\u7684\u5206\u6790\u548c\u5224\u65b7\u3002MLLM \u7684\u9032\u5c55\u76ee\u524d\u84ec\u52c3\u767c\u5c55\uff0c\u5438\u5f15\u4e86\u8a31\u591a\u7814\u7a76\u4eba\u54e1\u9032\u5165\u8a72\u9818\u57df\u3002\u6211\u5011\u5e0c\u671b\u9019\u9805\u5de5\u4f5c\u80fd\u4f5c\u70ba\u4e00\u500b\u5373\u63d2\u5373\u7528\u3001\u7c21\u55ae\u4f46\u6709\u6548\u7684\u57fa\u6e96\uff0c\u9f13\u52f5\u76f4\u63a5\u8a55\u4f30\u5f71\u7247\u9818\u57df\u4e2d\u73fe\u6709\u7684 MLLM\uff0c\u540c\u6642\u4e5f\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6a19\u6e96\u5316\u5f71\u7247\u5c0d\u8a71\u6a21\u578b\u7684\u9818\u57df\u3002\u6b64\u5916\uff0c\u6211\u5011\u9f13\u52f5\u7814\u7a76\u4eba\u54e1\u91cd\u65b0\u8003\u616e\uff1a\u76ee\u524d\u7684\u5f71\u7247 MLLM \u65b9\u6cd5\u662f\u5426\u771f\u6b63\u7372\u5f97\u4e86\u8d85\u8d8a\u5f71\u50cf MLLM \u7684\u77e5\u8b58\uff1f\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/whwu95/FreeVA \u53d6\u5f97</paragraph>", "author": "Wenhao Wu et.al.", "authors": "Wenhao Wu", "id": "2405.07798v1", "paper_url": "http://arxiv.org/abs/2405.07798v1", "repo": "https://github.com/whwu95/freeva"}}