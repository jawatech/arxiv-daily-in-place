{"2408.15339": {"publish_time": "2024-08-27", "title": "UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized Implicit Reward Function", "paper_summary": "An LLM is pretrained on trillions of tokens, but the pretrained LLM may still\ngenerate undesired responses. To solve this problem, alignment techniques such\nas RLHF, DPO and KTO are proposed. However, these alignment techniques have\nlimitations. For example, RLHF requires training the reward model and policy\nseparately, which is complex, time-consuming, memory intensive and unstable\nduring training processes. DPO proposes a mapping between an optimal policy and\na reward, greatly simplifying the training process of RLHF. However, it can not\ntake full advantages of a reward model and it is limited to pairwise preference\ndata.\n  In this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which\nunifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the\nclassical RLHF objective, the optimal policy is induced by a generalize\nimplicit reward function. With this novel mapping between a reward model and an\noptimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised\nlearning of minimizing the difference between an implicit reward and an\nexplicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and\nreduce memory burden of RL fine-tuning process; 3. accommodate different\nfeedback types including pairwise, binary and scalar feedback. Downstream\nexperiments show UNA outperforms DPO, KTO and RLHF.", "paper_summary_zh": "LLM \u5728\u6578\u5146\u500b\u7b26\u865f\u4e0a\u9032\u884c\u9810\u8a13\u7df4\uff0c\u4f46\u9810\u8a13\u7df4\u7684 LLM \u4ecd\u53ef\u80fd\u7522\u751f\u4e0d\u9700\u8981\u7684\u56de\u61c9\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u63d0\u51fa\u4e86 RLHF\u3001DPO \u548c KTO \u7b49\u5c0d\u9f4a\u6280\u8853\u3002\u7136\u800c\uff0c\u9019\u4e9b\u5c0d\u9f4a\u6280\u8853\u6709\u5176\u9650\u5236\u3002\u4f8b\u5982\uff0cRLHF \u9700\u8981\u5206\u5225\u8a13\u7df4\u734e\u52f5\u6a21\u578b\u548c\u7b56\u7565\uff0c\u9019\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u5f88\u8907\u96dc\u3001\u8017\u6642\u3001\u4f54\u7528\u5927\u91cf\u8a18\u61b6\u9ad4\u4e14\u4e0d\u7a69\u5b9a\u3002DPO \u63d0\u51fa\u4e86\u4e00\u500b\u6700\u512a\u7b56\u7565\u548c\u734e\u52f5\u4e4b\u9593\u7684\u5c0d\u61c9\u95dc\u4fc2\uff0c\u5927\u5927\u7c21\u5316\u4e86 RLHF \u7684\u8a13\u7df4\u904e\u7a0b\u3002\u7136\u800c\uff0c\u5b83\u7121\u6cd5\u5145\u5206\u5229\u7528\u734e\u52f5\u6a21\u578b\uff0c\u4e26\u4e14\u50c5\u9650\u65bc\u6210\u5c0d\u504f\u597d\u8cc7\u6599\u3002\n\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7d71\u4e00\u5c0d\u9f4a (UNA)\uff0c\u5b83\u7d71\u4e00\u4e86 RLHF/PPO\u3001DPO \u548c KTO\u3002\u9996\u5148\uff0c\u6211\u5011\u6578\u5b78\u4e0a\u8b49\u660e\uff0c\u7d66\u5b9a\u7d93\u5178\u7684 RLHF \u76ee\u6a19\uff0c\u6700\u512a\u7b56\u7565\u662f\u7531\u4e00\u500b\u5ee3\u7fa9\u7684\u96b1\u5f0f\u734e\u52f5\u51fd\u6578\u8a98\u5c0e\u7684\u3002\u6709\u4e86\u9019\u500b\u734e\u52f5\u6a21\u578b\u548c\u6700\u512a\u7b56\u7565\u4e4b\u9593\u7684\u65b0\u7a4e\u5c0d\u61c9\u95dc\u4fc2\uff0cUNA \u53ef\u4ee5 1. \u5c07 RLHF/PPO\u3001DPO \u548c KTO \u7d71\u4e00\u70ba\u76e3\u7763\u5b78\u7fd2\uff0c\u4ee5\u6700\u5c0f\u5316\u96b1\u5f0f\u734e\u52f5\u548c\u986f\u5f0f\u734e\u52f5\u4e4b\u9593\u7684\u5dee\u7570\uff1b2. \u5728\u7c21\u5316\u3001\u7a69\u5b9a\u3001\u52a0\u901f\u548c\u6e1b\u5c11 RL \u5fae\u8abf\u904e\u7a0b\u7684\u8a18\u61b6\u9ad4\u8ca0\u64d4\u7684\u540c\u6642\uff0c\u512a\u65bc RLHF/PPO\uff1b3. \u5bb9\u7d0d\u4e0d\u540c\u7684\u56de\u994b\u985e\u578b\uff0c\u5305\u62ec\u6210\u5c0d\u3001\u4e8c\u5143\u548c\u6a19\u91cf\u56de\u994b\u3002\u4e0b\u6e38\u5be6\u9a57\u8868\u660e\uff0cUNA \u512a\u65bc DPO\u3001KTO \u548c RLHF\u3002", "author": "Zhichao Wang et.al.", "authors": "Zhichao Wang, Bin Bi, Can Huang, Shiva Kumar Pentyala, Zixu James Zhu, Sitaram Asur, Na Claire Cheng", "id": "2408.15339v1", "paper_url": "http://arxiv.org/abs/2408.15339v1", "repo": "null"}}