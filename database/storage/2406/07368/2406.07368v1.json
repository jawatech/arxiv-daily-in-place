{"2406.07368": {"publish_time": "2024-06-11", "title": "When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models", "paper_summary": "Autoregressive Large Language Models (LLMs) have achieved impressive\nperformance in language tasks but face two significant bottlenecks: (1)\nquadratic complexity in the attention module as the number of tokens increases,\nand (2) limited efficiency due to the sequential processing nature of\nautoregressive LLMs during generation. While linear attention and speculative\ndecoding offer potential solutions, their applicability and synergistic\npotential for enhancing autoregressive LLMs remain uncertain. We conduct the\nfirst comprehensive study on the efficacy of existing linear attention methods\nfor autoregressive LLMs, integrating them with speculative decoding. We\nintroduce an augmentation technique for linear attention that ensures\ncompatibility with speculative decoding, enabling more efficient training and\nserving of LLMs. Extensive experiments and ablation studies involving seven\nexisting linear attention models and five encoder/decoder-based LLMs\nconsistently validate the effectiveness of our augmented linearized LLMs.\nNotably, our approach achieves up to a 6.67 reduction in perplexity on the\nLLaMA model and up to a 2$\\times$ speedup during generation compared to prior\nlinear attention methods. Codes and models are available at\nhttps://github.com/GATECH-EIC/Linearized-LLM.", "paper_summary_zh": "\u81ea\u8ff4\u6b78\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a9e\u8a00\u4efb\u52d9\u4e2d\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8868\u73fe\uff0c\u4f46\u9762\u81e8\u5169\u500b\u91cd\u5927\u7684\u74f6\u9838\uff1a(1) \u96a8\u8457\u7b26\u865f\u6578\u91cf\u589e\u52a0\uff0c\u6ce8\u610f\u529b\u6a21\u7d44\u4e2d\u7684\u4e8c\u6b21\u8907\u96dc\u5ea6\uff0c\u4ee5\u53ca (2) \u7531\u65bc\u81ea\u8ff4\u6b78 LLM \u5728\u751f\u6210\u671f\u9593\u7684\u9806\u5e8f\u8655\u7406\u7279\u6027\u6240\u5c0e\u81f4\u7684\u6548\u7387\u53d7\u9650\u3002\u96d6\u7136\u7dda\u6027\u6ce8\u610f\u529b\u548c\u63a8\u6e2c\u6027\u89e3\u78bc\u63d0\u4f9b\u4e86\u6f5b\u5728\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u4f46\u5b83\u5011\u5728\u81ea\u8ff4\u6b78 LLM \u4e2d\u7684\u9069\u7528\u6027\u548c\u5354\u540c\u6f5b\u529b\u4ecd\u4e0d\u78ba\u5b9a\u3002\u6211\u5011\u5c0d\u73fe\u6709\u7dda\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u5728\u81ea\u8ff4\u6b78 LLM \u4e2d\u7684\u6548\u80fd\u9032\u884c\u4e86\u7b2c\u4e00\u500b\u5168\u9762\u7684\u7814\u7a76\uff0c\u5c07\u5b83\u5011\u8207\u63a8\u6e2c\u6027\u89e3\u78bc\u6574\u5408\u5728\u4e00\u8d77\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u7dda\u6027\u6ce8\u610f\u529b\u7684\u64f4\u5145\u6280\u8853\uff0c\u78ba\u4fdd\u8207\u63a8\u6e2c\u6027\u89e3\u78bc\u76f8\u5bb9\uff0c\u5f9e\u800c\u5be6\u73fe LLM \u7684\u66f4\u6709\u6548\u7387\u8a13\u7df4\u548c\u670d\u52d9\u3002\u6d89\u53ca\u4e03\u500b\u73fe\u6709\u7dda\u6027\u6ce8\u610f\u529b\u6a21\u578b\u548c\u4e94\u500b\u7de8\u78bc\u5668/\u89e3\u78bc\u5668\u57fa\u790e LLM \u7684\u5ee3\u6cdb\u5be6\u9a57\u548c\u6d88\u878d\u7814\u7a76\uff0c\u4e00\u81f4\u9a57\u8b49\u4e86\u6211\u5011\u64f4\u5145\u7684\u7dda\u6027\u5316 LLM \u7684\u6709\u6548\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8207\u5148\u524d\u7684\u7dda\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5728 LLaMA \u6a21\u578b\u4e0a\u5be6\u73fe\u4e86\u56f0\u60d1\u5ea6\u964d\u4f4e\u591a\u9054 6.67 \u500d\uff0c\u4e26\u4e14\u5728\u751f\u6210\u671f\u9593\u52a0\u901f\u591a\u9054 2 \u500d\u3002\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u4ee5\u5728 https://github.com/GATECH-EIC/Linearized-LLM \u53d6\u5f97\u3002", "author": "Haoran You et.al.", "authors": "Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Yingyan, Lin", "id": "2406.07368v1", "paper_url": "http://arxiv.org/abs/2406.07368v1", "repo": "https://github.com/gatech-eic/linearized-llm"}}