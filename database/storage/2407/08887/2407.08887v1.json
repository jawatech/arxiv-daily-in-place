{"2407.08887": {"publish_time": "2024-07-11", "title": "Automatic Pruning of Fine-tuning Datasets for Transformer-based Language Models", "paper_summary": "Transformer-based language models have shown state-of-the-art performance on\na variety of natural language understanding tasks. To achieve this performance,\nthese models are first pre-trained on general corpus and then fine-tuned on\ndownstream tasks. Previous work studied the effect of pruning the training set\nof the downstream tasks on the performance of the model on its evaluation set.\nIn this work, we propose an automatic dataset pruning method for the training\nset of fine-tuning tasks. Our method is based on the model's success rate in\ncorrectly classifying each training data point. Unlike previous work which\nrelies on user feedback to determine subset size, our method automatically\nextracts training subsets that are adapted for each pair of model and\nfine-tuning task. Our method provides multiple subsets for use in dataset\npruning that navigate the trade-off between subset size and evaluation\naccuracy. Our largest subset, which we also refer to as the winning ticket\nsubset, is on average $3 \\times$ smaller than the original training set of the\nfine-tuning task. Our experiments on 5 downstream tasks and 2 language models\nshow that, on average, fine-tuning on the winning ticket subsets results in a\n$0.1 \\%$ increase in the evaluation performance of the model.", "paper_summary_zh": "<paragraph>\u57fa\u65bc Transformer \u7684\u8a9e\u8a00\u6a21\u578b\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u4efb\u52d9\u4e2d\u5c55\u73fe\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u70ba\u4e86\u9054\u5230\u6b64\u6548\u80fd\uff0c\u9019\u4e9b\u6a21\u578b\u6703\u5148\u5728\u4e00\u822c\u8a9e\u6599\u5eab\u4e0a\u9032\u884c\u9810\u5148\u8a13\u7df4\uff0c\u7136\u5f8c\u91dd\u5c0d\u4e0b\u6e38\u4efb\u52d9\u9032\u884c\u5fae\u8abf\u3002\u5148\u524d\u7684\u7814\u7a76\u63a2\u8a0e\u4e86\u4fee\u526a\u4e0b\u6e38\u4efb\u52d9\u8a13\u7df4\u96c6\u5c0d\u6a21\u578b\u5728\u8a55\u4f30\u96c6\u4e0a\u6548\u80fd\u7684\u5f71\u97ff\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u91dd\u5c0d\u5fae\u8abf\u4efb\u52d9\u8a13\u7df4\u96c6\u7684\u81ea\u52d5\u5316\u8cc7\u6599\u96c6\u4fee\u526a\u65b9\u6cd5\u3002\u6211\u5011\u7684\u9019\u9805\u65b9\u6cd5\u662f\u57fa\u65bc\u6a21\u578b\u6b63\u78ba\u5206\u985e\u6bcf\u500b\u8a13\u7df4\u8cc7\u6599\u9ede\u7684\u6210\u529f\u7387\u3002\u8207\u4f9d\u8cf4\u4f7f\u7528\u8005\u56de\u994b\u4f86\u6c7a\u5b9a\u5b50\u96c6\u5927\u5c0f\u7684\u5148\u524d\u7814\u7a76\u4e0d\u540c\uff0c\u6211\u5011\u7684\u9019\u9805\u65b9\u6cd5\u6703\u81ea\u52d5\u64f7\u53d6\u91dd\u5c0d\u6bcf\u5c0d\u6a21\u578b\u548c\u5fae\u8abf\u4efb\u52d9\u8abf\u6574\u7684\u8a13\u7df4\u5b50\u96c6\u3002\u6211\u5011\u7684\u9019\u9805\u65b9\u6cd5\u63d0\u4f9b\u591a\u500b\u5b50\u96c6\uff0c\u7528\u65bc\u8cc7\u6599\u96c6\u4fee\u526a\uff0c\u5728\u5b50\u96c6\u5927\u5c0f\u548c\u8a55\u4f30\u6e96\u78ba\u5ea6\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002\u6211\u5011\u6700\u5927\u7684\u5b50\u96c6\uff0c\u6211\u5011\u4e5f\u7a31\u4e4b\u70ba\u4e2d\u734e\u5f69\u5238\u5b50\u96c6\uff0c\u5e73\u5747\u6bd4\u5fae\u8abf\u4efb\u52d9\u7684\u539f\u59cb\u8a13\u7df4\u96c6\u5c0f $3 \\times$\u3002\u6211\u5011\u91dd\u5c0d 5 \u500b\u4e0b\u6e38\u4efb\u52d9\u548c 2 \u500b\u8a9e\u8a00\u6a21\u578b\u7684\u5be6\u9a57\u986f\u793a\uff0c\u5e73\u5747\u800c\u8a00\uff0c\u5c0d\u4e2d\u734e\u5f69\u5238\u5b50\u96c6\u9032\u884c\u5fae\u8abf\u6703\u8b93\u6a21\u578b\u7684\u8a55\u4f30\u6548\u80fd\u63d0\u5347 $0.1 \\%$\u3002</paragraph>", "author": "Mohammadreza Tayaranian et.al.", "authors": "Mohammadreza Tayaranian, Seyyed Hasan Mozafari, Brett H. Meyer, James J. Clark, Warren J. Gross", "id": "2407.08887v1", "paper_url": "http://arxiv.org/abs/2407.08887v1", "repo": "https://github.com/mthcom/hscore-dataset-pruning"}}