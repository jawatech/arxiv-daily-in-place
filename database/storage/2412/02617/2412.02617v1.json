{"2412.02617": {"publish_time": "2024-12-03", "title": "Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback", "paper_summary": "Large text-to-video models hold immense potential for a wide range of\ndownstream applications. However, these models struggle to accurately depict\ndynamic object interactions, often resulting in unrealistic movements and\nfrequent violations of real-world physics. One solution inspired by large\nlanguage models is to align generated outputs with desired outcomes using\nexternal feedback. This enables the model to refine its responses autonomously,\neliminating extensive manual data collection. In this work, we investigate the\nuse of feedback to enhance the object dynamics in text-to-video models. We aim\nto answer a critical question: what types of feedback, paired with which\nspecific self-improvement algorithms, can most effectively improve text-video\nalignment and realistic object interactions? We begin by deriving a unified\nprobabilistic objective for offline RL finetuning of text-to-video models. This\nperspective highlights how design elements in existing algorithms like KL\nregularization and policy projection emerge as specific choices within a\nunified framework. We then use derived methods to optimize a set of text-video\nalignment metrics (e.g., CLIP scores, optical flow), but notice that they often\nfail to align with human perceptions of generation quality. To address this\nlimitation, we propose leveraging vision-language models to provide more\nnuanced feedback specifically tailored to object dynamics in videos. Our\nexperiments demonstrate that our method can effectively optimize a wide variety\nof rewards, with binary AI feedback driving the most significant improvements\nin video quality for dynamic interactions, as confirmed by both AI and human\nevaluations. Notably, we observe substantial gains when using reward signals\nderived from AI feedback, particularly in scenarios involving complex\ninteractions between multiple objects and realistic depictions of objects\nfalling.", "paper_summary_zh": "\u5927\u578b\u6587\u672c\u5230\u5f71\u7247\u6a21\u578b\u5728\u5ee3\u6cdb\u7684\u4e0b\u6e38\u61c9\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5b\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u96e3\u4ee5\u6e96\u78ba\u63cf\u7e6a\u52d5\u614b\u7269\u9ad4\u4ea4\u4e92\uff0c\u7d93\u5e38\u5c0e\u81f4\u4e0d\u5207\u5be6\u969b\u7684\u52d5\u4f5c\u548c\u983b\u7e41\u9055\u53cd\u73fe\u5be6\u4e16\u754c\u7269\u7406\u5b9a\u5f8b\u3002\u4e00\u7a2e\u53d7\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u555f\u767c\u7684\u89e3\u6c7a\u65b9\u6848\u662f\u4f7f\u7528\u5916\u90e8\u56de\u994b\u5c07\u751f\u6210\u7684\u8f38\u51fa\u8207\u9810\u671f\u7684\u7d50\u679c\u5c0d\u9f4a\u3002\u9019\u4f7f\u6a21\u578b\u80fd\u5920\u81ea\u4e3b\u512a\u5316\u5176\u56de\u61c9\uff0c\u5f9e\u800c\u6d88\u9664\u5ee3\u6cdb\u7684\u624b\u52d5\u6578\u64da\u6536\u96c6\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86\u4f7f\u7528\u56de\u994b\u4f86\u589e\u5f37\u6587\u672c\u5230\u5f71\u7247\u6a21\u578b\u4e2d\u7684\u7269\u9ad4\u52d5\u614b\u3002\u6211\u5011\u65e8\u5728\u56de\u7b54\u4e00\u500b\u95dc\u9375\u554f\u984c\uff1a\u54ea\u7a2e\u985e\u578b\u7684\u56de\u994b\u8207\u54ea\u4e9b\u5177\u9ad4\u7684\u81ea\u5b8c\u5584\u6f14\u7b97\u6cd5\u914d\u5c0d\uff0c\u53ef\u4ee5\u6700\u6709\u6548\u5730\u6539\u5584\u6587\u672c\u5f71\u7247\u5c0d\u9f4a\u548c\u903c\u771f\u7684\u7269\u9ad4\u4ea4\u4e92\uff1f\u6211\u5011\u9996\u5148\u70ba\u6587\u672c\u5230\u5f71\u7247\u6a21\u578b\u7684\u96e2\u7dda RL \u5fae\u8abf\u63a8\u5c0e\u51fa\u4e00\u500b\u7d71\u4e00\u7684\u6a5f\u7387\u76ee\u6a19\u3002\u6b64\u89c0\u9ede\u5f37\u8abf\u4e86\u73fe\u6709\u6f14\u7b97\u6cd5\u4e2d\u7684\u8a2d\u8a08\u5143\u7d20\uff0c\u4f8b\u5982 KL \u6b63\u898f\u5316\u548c\u7b56\u7565\u6295\u5f71\uff0c\u5982\u4f55\u4f5c\u70ba\u7d71\u4e00\u67b6\u69cb\u4e2d\u7684\u7279\u5b9a\u9078\u64c7\u51fa\u73fe\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u884d\u751f\u65b9\u6cd5\u4f86\u6700\u4f73\u5316\u4e00\u7d44\u6587\u672c\u5f71\u7247\u5c0d\u9f4a\u6307\u6a19\uff08\u4f8b\u5982 CLIP \u5206\u6578\u3001\u5149\u6d41\uff09\uff0c\u4f46\u6ce8\u610f\u5230\u5b83\u5011\u901a\u5e38\u7121\u6cd5\u8207\u4eba\u985e\u5c0d\u751f\u6210\u54c1\u8cea\u7684\u611f\u77e5\u76f8\u7b26\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5efa\u8b70\u5229\u7528\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u4f86\u63d0\u4f9b\u66f4\u7d30\u7dfb\u7684\u56de\u994b\uff0c\u7279\u5225\u91dd\u5c0d\u5f71\u7247\u4e2d\u7684\u7269\u9ad4\u52d5\u614b\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u6700\u4f73\u5316\u5404\u7a2e\u734e\u52f5\uff0c\u5176\u4e2d\u4e8c\u5143 AI \u56de\u994b\u5728\u5f71\u7247\u54c1\u8cea\u4e2d\u63a8\u52d5\u4e86\u52d5\u614b\u4ea4\u4e92\u65b9\u9762\u6700\u986f\u8457\u7684\u6539\u5584\uff0c\u9019\u7531 AI \u548c\u4eba\u985e\u8a55\u4f30\u8b49\u5be6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u5728\u4f7f\u7528\u6e90\u81ea AI \u56de\u994b\u7684\u734e\u52f5\u4fe1\u865f\u6642\u89c0\u5bdf\u5230\u986f\u8457\u7684\u6536\u76ca\uff0c\u7279\u5225\u662f\u5728\u6d89\u53ca\u591a\u500b\u7269\u9ad4\u4e4b\u9593\u8907\u96dc\u4ea4\u4e92\u548c\u7269\u9ad4\u4e0b\u843d\u903c\u771f\u63cf\u7e6a\u7684\u60c5\u5883\u4e2d\u3002", "author": "Hiroki Furuta et.al.", "authors": "Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, Sherry Yang", "id": "2412.02617v1", "paper_url": "http://arxiv.org/abs/2412.02617v1", "repo": "null"}}