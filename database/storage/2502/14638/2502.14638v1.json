{"2502.14638": {"publish_time": "2025-02-20", "title": "NAVIG: Natural Language-guided Analysis with Vision Language Models for Image Geo-localization", "paper_summary": "Image geo-localization is the task of predicting the specific location of an\nimage and requires complex reasoning across visual, geographical, and cultural\ncontexts. While prior Vision Language Models (VLMs) have the best accuracy at\nthis task, there is a dearth of high-quality datasets and models for analytical\nreasoning. We first create NaviClues, a high-quality dataset derived from\nGeoGuessr, a popular geography game, to supply examples of expert reasoning\nfrom language. Using this dataset, we present Navig, a comprehensive image\ngeo-localization framework integrating global and fine-grained image\ninformation. By reasoning with language, Navig reduces the average distance\nerror by 14% compared to previous state-of-the-art models while requiring fewer\nthan 1000 training samples. Our dataset and code are available at\nhttps://github.com/SparrowZheyuan18/Navig/.", "paper_summary_zh": "\u5f71\u50cf\u5730\u7406\u5b9a\u4f4d\u662f\u9810\u6e2c\u5f71\u50cf\u7279\u5b9a\u4f4d\u7f6e\u7684\u4efb\u52d9\uff0c\u9700\u8981\u8de8\u8996\u89ba\u3001\u5730\u7406\u548c\u6587\u5316\u8108\u7d61\u9032\u884c\u8907\u96dc\u7684\u63a8\u7406\u3002\u96d6\u7136\u5148\u524d\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u6b64\u4efb\u52d9\u4e2d\u64c1\u6709\u6700\u4f73\u6e96\u78ba\u5ea6\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u54c1\u8cea\u7684\u8cc7\u6599\u96c6\u548c\u5206\u6790\u63a8\u7406\u6a21\u578b\u3002\u6211\u5011\u9996\u5148\u5efa\u7acb NaviClues\uff0c\u9019\u662f\u4e00\u500b\u6e90\u81ea GeoGuessr \u7684\u9ad8\u54c1\u8cea\u8cc7\u6599\u96c6\uff0cGeoGuessr \u662f\u4e00\u6b3e\u6d41\u884c\u7684\u5730\u7406\u904a\u6232\uff0c\u53ef\u63d0\u4f9b\u4f86\u81ea\u8a9e\u8a00\u7684\u5c08\u5bb6\u63a8\u7406\u7bc4\u4f8b\u3002\u4f7f\u7528\u6b64\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u63d0\u51fa Navig\uff0c\u9019\u662f\u4e00\u500b\u7d9c\u5408\u6027\u7684\u5f71\u50cf\u5730\u7406\u5b9a\u4f4d\u67b6\u69cb\uff0c\u6574\u5408\u4e86\u5168\u7403\u548c\u7d30\u7dfb\u7684\u5f71\u50cf\u8cc7\u8a0a\u3002\u900f\u904e\u8a9e\u8a00\u63a8\u7406\uff0cNavig \u5c07\u5e73\u5747\u8ddd\u96e2\u8aa4\u5dee\u6e1b\u5c11\u4e86 14%\uff0c\u8207\u5148\u524d\u7684\u6700\u5148\u9032\u6a21\u578b\u76f8\u6bd4\uff0c\u540c\u6642\u53ea\u9700\u8981\u4e0d\u5230 1000 \u500b\u8a13\u7df4\u6a23\u672c\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/SparrowZheyuan18/Navig/ \u53d6\u5f97\u3002", "author": "Zheyuan Zhang et.al.", "authors": "Zheyuan Zhang, Runze Li, Tasnim Kabir, Jordan Boyd-Graber", "id": "2502.14638v1", "paper_url": "http://arxiv.org/abs/2502.14638v1", "repo": "null"}}