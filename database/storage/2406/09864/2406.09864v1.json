{"2406.09864": {"publish_time": "2024-06-14", "title": "LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal Data", "paper_summary": "Multimodal Deep Learning enhances decision-making by integrating diverse\ninformation sources, such as texts, images, audio, and videos. To develop\ntrustworthy multimodal approaches, it is essential to understand how\nuncertainty impacts these models. We introduce LUMA, a unique benchmark\ndataset, featuring audio, image, and textual data from 50 classes, for learning\nfrom uncertain and multimodal data. It extends the well-known CIFAR 10/100\ndataset with audio samples extracted from three audio corpora, and text data\ngenerated using the Gemma-7B Large Language Model (LLM). The LUMA dataset\nenables the controlled injection of varying types and degrees of uncertainty to\nachieve and tailor specific experiments and benchmarking initiatives. LUMA is\nalso available as a Python package including the functions for generating\nmultiple variants of the dataset with controlling the diversity of the data,\nthe amount of noise for each modality, and adding out-of-distribution samples.\nA baseline pre-trained model is also provided alongside three uncertainty\nquantification methods: Monte-Carlo Dropout, Deep Ensemble, and Reliable\nConflictive Multi-View Learning. This comprehensive dataset and its tools are\nintended to promote and support the development and benchmarking of trustworthy\nand robust multimodal deep learning approaches.", "paper_summary_zh": "\u591a\u6a21\u614b\u6df1\u5ea6\u5b78\u7fd2\u900f\u904e\u6574\u5408\u6587\u672c\u3001\u5f71\u50cf\u3001\u97f3\u8a0a\u548c\u5f71\u7247\u7b49\u591a\u5143\u8cc7\u8a0a\u4f86\u6e90\uff0c\u63d0\u5347\u6c7a\u7b56\u54c1\u8cea\u3002\u70ba\u4e86\u767c\u5c55\u503c\u5f97\u4fe1\u8cf4\u7684\u591a\u6a21\u614b\u65b9\u6cd5\uff0c\u4e86\u89e3\u4e0d\u78ba\u5b9a\u6027\u5982\u4f55\u5f71\u97ff\u9019\u4e9b\u6a21\u578b\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u5f15\u9032 LUMA\uff0c\u4e00\u500b\u7368\u7279\u7684\u57fa\u6e96\u8cc7\u6599\u96c6\uff0c\u5305\u542b\u4f86\u81ea 50 \u500b\u985e\u5225\u7684\u97f3\u8a0a\u3001\u5f71\u50cf\u548c\u6587\u5b57\u8cc7\u6599\uff0c\u7528\u65bc\u5f9e\u4e0d\u78ba\u5b9a\u7684\u591a\u6a21\u614b\u8cc7\u6599\u4e2d\u5b78\u7fd2\u3002\u5b83\u64f4\u5145\u4e86\u8457\u540d\u7684 CIFAR 10/100 \u8cc7\u6599\u96c6\uff0c\u52a0\u5165\u4e86\u5f9e\u4e09\u500b\u97f3\u8a0a\u8a9e\u6599\u5eab\u4e2d\u8403\u53d6\u7684\u97f3\u8a0a\u7bc4\u4f8b\uff0c\u4ee5\u53ca\u4f7f\u7528 Gemma-7B \u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u751f\u6210\u7684\u6587\u5b57\u8cc7\u6599\u3002LUMA \u8cc7\u6599\u96c6\u80fd\u63a7\u5236\u6ce8\u5165\u4e0d\u540c\u985e\u578b\u548c\u7a0b\u5ea6\u7684\u4e0d\u78ba\u5b9a\u6027\uff0c\u4ee5\u9054\u6210\u4e26\u8abf\u6574\u7279\u5b9a\u7684\u5be6\u9a57\u548c\u57fa\u6e96\u8a55\u91cf\u8a08\u756b\u3002LUMA \u4e5f\u53ef\u7528\u4f5c Python \u5957\u4ef6\uff0c\u5305\u542b\u7528\u65bc\u7522\u751f\u591a\u7a2e\u8cc7\u6599\u96c6\u8b8a\u9ad4\u7684\u51fd\u5f0f\uff0c\u4e26\u63a7\u5236\u8cc7\u6599\u7684\u591a\u6a23\u6027\u3001\u6bcf\u500b\u6a21\u614b\u7684\u96dc\u8a0a\u91cf\uff0c\u4ee5\u53ca\u52a0\u5165\u5206\u5e03\u5916\u7bc4\u4f8b\u3002\u4e5f\u63d0\u4f9b\u4e86\u4e00\u500b\u9810\u5148\u8a13\u7df4\u7684\u57fa\u6e96\u6a21\u578b\uff0c\u4ee5\u53ca\u4e09\u7a2e\u4e0d\u78ba\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff1a\u8499\u5730\u5361\u7f85\u4e2d\u8f1f\u3001\u6df1\u5ea6\u6574\u5408\u548c\u53ef\u9760\u885d\u7a81\u591a\u8996\u89d2\u5b78\u7fd2\u3002\u9019\u500b\u5168\u9762\u7684\u8cc7\u6599\u96c6\u53ca\u5176\u5de5\u5177\u65e8\u5728\u63a8\u5ee3\u548c\u652f\u63f4\u503c\u5f97\u4fe1\u8cf4\u4e14\u7a69\u5065\u7684\u591a\u6a21\u614b\u6df1\u5ea6\u5b78\u7fd2\u65b9\u6cd5\u7684\u958b\u767c\u548c\u57fa\u6e96\u8a55\u91cf\u3002", "author": "Grigor Bezirganyan et.al.", "authors": "Grigor Bezirganyan, Sana Sellami, Laure Berti-\u00c9quille, S\u00e9bastien Fournier", "id": "2406.09864v1", "paper_url": "http://arxiv.org/abs/2406.09864v1", "repo": "https://github.com/bezirganyan/luma"}}