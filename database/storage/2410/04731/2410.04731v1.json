{"2410.04731": {"publish_time": "2024-10-07", "title": "Efficient transformer with reinforced position embedding for language models", "paper_summary": "In this paper, we propose an efficient transformer architecture that uses\nreinforced positional embedding to obtain superior performance with half the\nnumber of encoder decoder layers. We demonstrate that concatenating positional\nencoding with trainable token embeddings, normalizing columns in the token\nembedding matrix, and using the normalized token embedding matrix as the value\nof the attention layer improve the training and validation loss and the\ntraining time in an encoder-decoder Transformer model for a Portuguese-English\ntranslation task with 10 epochs or 12 hours of training across 10 trials. Our\nmethod, with roughly a threefold parameter reduction compared to the baseline\nmodel, yields a mean training loss of 1.21, a mean validation loss of 1.51, and\nan average training time of 1352.27 seconds per epoch, surpassing the baseline\nmodel with the same embedding dimension that employs addition of positional\nencoding and token embeddings, which achieves a mean training loss of 1.96, a\nvalidation loss of 2.18, and an average training time of 4297.79 seconds per\nepoch. Additionally, we evaluated our proposed architecture and the baseline\nacross 14 diverse translation datasets from TensorFlow. The results indicate\nthat our method consistently achieves lower or comparable training and\nvalidation losses, suggesting enhanced learning efficiency.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u9ad8\u6548\u7684 Transformer \u67b6\u69cb\uff0c\u5b83\u4f7f\u7528\u589e\u5f37\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4ee5\u4f7f\u7528\u7de8\u78bc\u5668\u89e3\u78bc\u5668\u5c64\u7684\u4e00\u534a\u4f86\u7372\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002\u6211\u5011\u8b49\u660e\u4e86\u5c07\u4f4d\u7f6e\u7de8\u78bc\u8207\u53ef\u8a13\u7df4\u7684\u4ee4\u724c\u5d4c\u5165\u4e32\u806f\u3001\u5c0d\u4ee4\u724c\u5d4c\u5165\u77e9\u9663\u4e2d\u7684\u5217\u9032\u884c\u6b63\u898f\u5316\u4ee5\u53ca\u4f7f\u7528\u6b63\u898f\u5316\u7684\u4ee4\u724c\u5d4c\u5165\u77e9\u9663\u4f5c\u70ba\u6ce8\u610f\u529b\u5c64\u7684\u503c\uff0c\u53ef\u4ee5\u6539\u5584\u7de8\u78bc\u5668-\u89e3\u78bc\u5668 Transformer \u6a21\u578b\u7684\u8a13\u7df4\u548c\u9a57\u8b49\u640d\u5931\u4ee5\u53ca\u8a13\u7df4\u6642\u9593\uff0c\u7528\u65bc\u8461\u8404\u7259\u8a9e-\u82f1\u8a9e\u7ffb\u8b6f\u4efb\u52d9\uff0c\u5728 10 \u6b21\u8a66\u9a57\u4e2d\u9032\u884c\u4e86 10 \u500b\u6642\u671f\u6216 12 \u5c0f\u6642\u7684\u8a13\u7df4\u3002\u8207\u57fa\u7dda\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u5c07\u53c3\u6578\u6e1b\u5c11\u4e86\u5927\u7d04\u4e09\u5206\u4e4b\u4e00\uff0c\u5e73\u5747\u8a13\u7df4\u640d\u5931\u70ba 1.21\uff0c\u5e73\u5747\u9a57\u8b49\u640d\u5931\u70ba 1.51\uff0c\u5e73\u5747\u8a13\u7df4\u6642\u9593\u70ba\u6bcf\u500b\u6642\u671f 1352.27 \u79d2\uff0c\u8d85\u904e\u4e86\u63a1\u7528\u4f4d\u7f6e\u7de8\u78bc\u548c\u4ee4\u724c\u5d4c\u5165\u76f8\u52a0\u7684\u76f8\u540c\u5d4c\u5165\u7dad\u5ea6\u7684\u57fa\u7dda\u6a21\u578b\uff0c\u5f8c\u8005\u5be6\u73fe\u4e86 1.96 \u7684\u5e73\u5747\u8a13\u7df4\u640d\u5931\u30012.18 \u7684\u9a57\u8b49\u640d\u5931\u4ee5\u53ca\u6bcf\u500b\u6642\u671f 4297.79 \u79d2\u7684\u5e73\u5747\u8a13\u7df4\u6642\u9593\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728 TensorFlow \u7684 14 \u500b\u4e0d\u540c\u7684\u7ffb\u8b6f\u6578\u64da\u96c6\u4e2d\u8a55\u4f30\u4e86\u6211\u5011\u63d0\u51fa\u7684\u67b6\u69cb\u548c\u57fa\u7dda\u3002\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u59cb\u7d42\u5be6\u73fe\u66f4\u4f4e\u6216\u76f8\u7576\u7684\u8a13\u7df4\u548c\u9a57\u8b49\u640d\u5931\uff0c\u9019\u8868\u660e\u589e\u5f37\u7684\u5b78\u7fd2\u6548\u7387\u3002</paragraph>", "author": "Yen-Che Hsiao et.al.", "authors": "Yen-Che Hsiao, Abhishek Dutta", "id": "2410.04731v1", "paper_url": "http://arxiv.org/abs/2410.04731v1", "repo": "null"}}