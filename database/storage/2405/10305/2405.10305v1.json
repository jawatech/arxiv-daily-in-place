{"2405.10305": {"publish_time": "2024-05-16", "title": "4D Panoptic Scene Graph Generation", "paper_summary": "We are living in a three-dimensional space while moving forward through a\nfourth dimension: time. To allow artificial intelligence to develop a\ncomprehensive understanding of such a 4D environment, we introduce 4D Panoptic\nScene Graph (PSG-4D), a new representation that bridges the raw visual data\nperceived in a dynamic 4D world and high-level visual understanding.\nSpecifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent\nentities with precise location and status information, and edges, which capture\nthe temporal relations. To facilitate research in this new area, we build a\nrichly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of\n1M frames, each of which is labeled with 4D panoptic segmentation masks as well\nas fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer,\na Transformer-based model that can predict panoptic segmentation masks, track\nmasks along the time axis, and generate the corresponding scene graphs via a\nrelation component. Extensive experiments on the new dataset show that our\nmethod can serve as a strong baseline for future research on PSG-4D. In the\nend, we provide a real-world application example to demonstrate how we can\nachieve dynamic scene understanding by integrating a large language model into\nour PSG-4D system.", "paper_summary_zh": "\u6211\u5011\u751f\u6d3b\u5728\u4e09\u7dad\u7a7a\u9593\u4e2d\uff0c\u540c\u6642\u5728\u7b2c\u56db\u7dad\u5ea6\uff1a\u6642\u9593\u4e2d\u524d\u9032\u3002\u70ba\u4e86\u8b93\u4eba\u5de5\u667a\u6167\u767c\u5c55\u5c0d\u9019\u7a2e 4D \u74b0\u5883\u7684\u5168\u9762\u7406\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86 4D \u5168\u666f\u5834\u666f\u5716 (PSG-4D)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u8868\u793a\u65b9\u5f0f\uff0c\u5b83\u5f4c\u5408\u4e86\u5728\u52d5\u614b 4D \u4e16\u754c\u4e2d\u611f\u77e5\u5230\u7684\u539f\u59cb\u8996\u89ba\u6578\u64da\u548c\u9ad8\u5c64\u7d1a\u8996\u89ba\u7406\u89e3\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cPSG-4D \u5c07\u8c50\u5bcc\u7684 4D \u611f\u6e2c\u6578\u64da\u62bd\u8c61\u70ba\u7bc0\u9ede\uff0c\u9019\u4e9b\u7bc0\u9ede\u8868\u793a\u5177\u6709\u7cbe\u78ba\u4f4d\u7f6e\u548c\u72c0\u614b\u4fe1\u606f\u7684\u5be6\u9ad4\uff0c\u4ee5\u53ca\u908a\u7de3\uff0c\u9019\u4e9b\u908a\u7de3\u6355\u7372\u6642\u9593\u95dc\u4fc2\u3002\u70ba\u4e86\u4fc3\u9032\u9019\u4e00\u65b0\u9818\u57df\u7684\u7814\u7a76\uff0c\u6211\u5011\u69cb\u5efa\u4e86\u4e00\u500b\u8c50\u5bcc\u8a3b\u91cb\u7684 PSG-4D \u6578\u64da\u96c6\uff0c\u5176\u4e2d\u5305\u542b 3K RGB-D \u8996\u983b\uff0c\u7e3d\u5171 1M \u5e40\uff0c\u6bcf\u500b\u5e40\u90fd\u6a19\u8a18\u6709 4D \u5168\u666f\u5206\u5272\u8499\u7248\u4ee5\u53ca\u7d30\u7c92\u5ea6\u3001\u52d5\u614b\u5834\u666f\u5716\u3002\u70ba\u4e86\u89e3\u6c7a PSG-4D\uff0c\u6211\u5011\u63d0\u51fa\u4e86 PSG4DFormer\uff0c\u9019\u662f\u4e00\u500b\u57fa\u65bc Transformer \u7684\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u9810\u6e2c\u5168\u666f\u5206\u5272\u8499\u7248\u3001\u6cbf\u6642\u9593\u8ef8\u8ffd\u8e64\u8499\u7248\uff0c\u4e26\u901a\u904e\u95dc\u4fc2\u7d44\u6210\u7522\u751f\u5c0d\u61c9\u7684\u5834\u666f\u5716\u3002\u5728\u65b0\u7684\u6578\u64da\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u7528\u4f5c\u672a\u4f86 PSG-4D \u7814\u7a76\u7684\u5f37\u5927\u57fa\u7dda\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u771f\u5be6\u4e16\u754c\u7684\u61c9\u7528\u7bc4\u4f8b\uff0c\u4ee5\u5c55\u793a\u5982\u4f55\u901a\u904e\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u6574\u5408\u5230\u6211\u5011\u7684 PSG-4D \u7cfb\u7d71\u4e2d\u4f86\u5be6\u73fe\u52d5\u614b\u5834\u666f\u7406\u89e3\u3002", "author": "Jingkang Yang et.al.", "authors": "Jingkang Yang, Jun Cen, Wenxuan Peng, Shuai Liu, Fangzhou Hong, Xiangtai Li, Kaiyang Zhou, Qifeng Chen, Ziwei Liu", "id": "2405.10305v1", "paper_url": "http://arxiv.org/abs/2405.10305v1", "repo": "https://github.com/Jingkang50/OpenPSG"}}