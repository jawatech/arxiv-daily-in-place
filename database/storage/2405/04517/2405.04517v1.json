{"2405.04517": {"publish_time": "2024-05-07", "title": "xLSTM: Extended Long Short-Term Memory", "paper_summary": "In the 1990s, the constant error carousel and gating were introduced as the\ncentral ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have\nstood the test of time and contributed to numerous deep learning success\nstories, in particular they constituted the first Large Language Models (LLMs).\nHowever, the advent of the Transformer technology with parallelizable\nself-attention at its core marked the dawn of a new era, outpacing LSTMs at\nscale. We now raise a simple question: How far do we get in language modeling\nwhen scaling LSTMs to billions of parameters, leveraging the latest techniques\nfrom modern LLMs, but mitigating known limitations of LSTMs? Firstly, we\nintroduce exponential gating with appropriate normalization and stabilization\ntechniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM\nwith a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that\nis fully parallelizable with a matrix memory and a covariance update rule.\nIntegrating these LSTM extensions into residual block backbones yields xLSTM\nblocks that are then residually stacked into xLSTM architectures. Exponential\ngating and modified memory structures boost xLSTM capabilities to perform\nfavorably when compared to state-of-the-art Transformers and State Space\nModels, both in performance and scaling.", "paper_summary_zh": "\u5728 1990 \u5e74\u4ee3\uff0c\u6046\u5b9a\u932f\u8aa4\u8ff4\u65cb\u6728\u99ac\u548c\u9598\u63a7\u88ab\u5f15\u5165\u70ba\u9577\u671f\u77ed\u671f\u8a18\u61b6 (LSTM) \u7684\u6838\u5fc3\u6982\u5ff5\u3002\u5f9e\u90a3\u6642\u8d77\uff0cLSTM \u7d93\u53d7\u4f4f\u4e86\u6642\u9593\u7684\u8003\u9a57\uff0c\u4e26\u70ba\u8a31\u591a\u6df1\u5ea6\u5b78\u7fd2\u7684\u6210\u529f\u6848\u4f8b\u505a\u51fa\u4e86\u8ca2\u737b\uff0c\u7279\u5225\u662f\u5b83\u5011\u69cb\u6210\u4e86\u7b2c\u4e00\u6279\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u7136\u800c\uff0c\u4ee5\u53ef\u4e26\u884c\u5316\u81ea\u6ce8\u610f\u529b\u70ba\u6838\u5fc3\u7684 Transformer \u6280\u8853\u7684\u51fa\u73fe\u6a19\u8a8c\u8457\u4e00\u500b\u65b0\u6642\u4ee3\u7684\u5230\u4f86\uff0c\u5176\u898f\u6a21\u8d85\u904e\u4e86 LSTM\u3002\u6211\u5011\u73fe\u5728\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u7684\u554f\u984c\uff1a\u7576\u6211\u5011\u5c07 LSTM \u64f4\u5c55\u5230\u6578\u5341\u5104\u500b\u53c3\u6578\u3001\u5229\u7528\u73fe\u4ee3 LLM \u7684\u6700\u65b0\u6280\u8853\uff0c\u4f46\u6e1b\u8f15 LSTM \u7684\u5df2\u77e5\u9650\u5236\u6642\uff0c\u6211\u5011\u5728\u8a9e\u8a00\u5efa\u6a21\u4e2d\u80fd\u8d70\u591a\u9060\uff1f\u9996\u5148\uff0c\u6211\u5011\u5f15\u5165\u5177\u6709\u9069\u7576\u6b63\u898f\u5316\u548c\u7a69\u5b9a\u6280\u8853\u7684\u6307\u6578\u9598\u63a7\u3002\u5176\u6b21\uff0c\u6211\u5011\u4fee\u6539 LSTM \u8a18\u61b6\u7d50\u69cb\uff0c\u7372\u5f97\uff1a(i) \u5e36\u6709\u6a19\u91cf\u8a18\u61b6\u9ad4\u3001\u6a19\u91cf\u66f4\u65b0\u548c\u65b0\u8a18\u61b6\u9ad4\u6df7\u5408\u7684 sLSTM\uff0c(ii) mLSTM\uff0c\u5b83\u4f7f\u7528\u77e9\u9663\u8a18\u61b6\u9ad4\u548c\u5354\u65b9\u5dee\u66f4\u65b0\u898f\u5247\u5b8c\u5168\u53ef\u4e26\u884c\u5316\u3002\u5c07\u9019\u4e9b LSTM \u64f4\u5145\u6574\u5408\u5230\u6b98\u5dee\u584a\u4e3b\u5e79\u4e2d\u6703\u7522\u751f xLSTM \u584a\uff0c\u7136\u5f8c\u5c07\u5b83\u5011\u6b98\u5dee\u5806\u758a\u5230 xLSTM \u67b6\u69cb\u4e2d\u3002\u8207\u6700\u5148\u9032\u7684 Transformer \u548c\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u76f8\u6bd4\uff0c\u6307\u6578\u9598\u63a7\u548c\u4fee\u6539\u7684\u8a18\u61b6\u9ad4\u7d50\u69cb\u63d0\u5347\u4e86 xLSTM \u7684\u80fd\u529b\uff0c\u5728\u6548\u80fd\u548c\u64f4\u5145\u6027\u65b9\u9762\u8868\u73fe\u51fa\u8272\u3002", "author": "Maximilian Beck et.al.", "authors": "Maximilian Beck, Korbinian P\u00f6ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, Sepp Hochreiter", "id": "2405.04517v1", "paper_url": "http://arxiv.org/abs/2405.04517v1", "repo": "null"}}