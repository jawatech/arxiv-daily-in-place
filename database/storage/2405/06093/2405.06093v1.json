{"2405.06093": {"publish_time": "2024-05-09", "title": "Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection", "paper_summary": "Large Language Models (LLMs) have demonstrated their efficacy across a broad\nspectrum of tasks in healthcare applications. However, often LLMs need to be\nfine-tuned on task-specific expert annotated data to achieve optimal\nperformance, which can be expensive and time consuming. In this study, we\nfine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels\nobtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)\ntables, which specify care plan in clinical trial protocols. We introduce a\nfiltering mechanism to select high-confidence labels for this table\nclassification task, thereby reducing the noise in the auto-generated labels.\nWe show that fine-tuned PaLM-2 with those labels achieves performance that\nexceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is\nclose to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our\nresults show that leveraging LLM-generated labels through powerful models like\ngemini-pro can potentially serve as a viable strategy for improving LLM\nperformance through fine-tuning in specialized tasks, particularly in domains\nwhere expert annotations are scarce, expensive, or time-consuming to obtain.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u8b49\u660e\u5176\u5728\u91ab\u7642\u4fdd\u5065\u61c9\u7528\u4e2d\u5ee3\u6cdb\u4efb\u52d9\u7684\u529f\u6548\u3002\u7136\u800c\uff0cLLM \u901a\u5e38\u9700\u8981\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u7684\u5c08\u5bb6\u8a3b\u91cb\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u624d\u80fd\u9054\u5230\u6700\u4f73\u6548\u80fd\uff0c\u9019\u53ef\u80fd\u6703\u5f88\u6602\u8cb4\u4e14\u8017\u6642\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u5f9e gemini-pro 1.0 \u53d6\u5f97\u7684\u96dc\u8a0a\u6a19\u7c64\uff0c\u4ee5\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u5fae\u8abf PaLM-2\uff0c\u4ee5\u5075\u6e2c\u81e8\u5e8a\u8a66\u9a57\u5354\u5b9a\u4e2d\u7684\u7167\u8b77\u8a08\u756b\u898f\u7bc4\u7684\u4e8b\u4ef6\u6642\u7a0b\u8868 (SoE) \u8868\u683c\u3002\u6211\u5011\u5f15\u5165\u4e00\u500b\u904e\u6ffe\u6a5f\u5236\uff0c\u4ee5\u9078\u64c7\u6b64\u8868\u683c\u5206\u985e\u4efb\u52d9\u7684\u9ad8\u4fe1\u5fc3\u6a19\u7c64\uff0c\u5f9e\u800c\u964d\u4f4e\u81ea\u52d5\u7522\u751f\u6a19\u7c64\u4e2d\u7684\u96dc\u8a0a\u3002\u6211\u5011\u8b49\u660e\u5fae\u8abf\u5f8c\u7684 PaLM-2 \u4f7f\u7528\u9019\u4e9b\u6a19\u7c64\uff0c\u5176\u6548\u80fd\u8d85\u8d8a gemini-pro 1.0 \u548c\u5176\u4ed6 LLM\u3002\u6b64\u5916\uff0c\u5176\u6548\u80fd\u63a5\u8fd1\u6839\u64da\u975e\u5c08\u5bb6\u8a3b\u91cb\u8005\u53d6\u5f97\u7684\u6a19\u7c64\u5fae\u8abf\u7684 PaLM-2\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u900f\u904e\u529f\u80fd\u5f37\u5927\u7684\u6a21\u578b\uff08\u4f8b\u5982 gemini-pro\uff09\u5229\u7528 LLM \u7522\u751f\u7684\u6a19\u7c64\uff0c\u6709\u53ef\u80fd\u6210\u70ba\u900f\u904e\u5fae\u8abf\u6539\u5584 LLM \u6548\u80fd\u7684\u53ef\u884c\u7b56\u7565\uff0c\u7279\u5225\u662f\u5728\u53d6\u5f97\u5c08\u5bb6\u8a3b\u91cb\u7a00\u5c11\u3001\u6602\u8cb4\u6216\u8017\u6642\u7684\u9818\u57df\u4e2d\u3002", "author": "Bhawesh Kumar et.al.", "authors": "Bhawesh Kumar, Jonathan Amar, Eric Yang, Nan Li, Yugang Jia", "id": "2405.06093v1", "paper_url": "http://arxiv.org/abs/2405.06093v1", "repo": "null"}}