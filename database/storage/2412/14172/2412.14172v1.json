{"2412.14172": {"publish_time": "2024-12-18", "title": "Learning from Massive Human Videos for Universal Humanoid Pose Control", "paper_summary": "Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.", "paper_summary_zh": "\u985e\u4eba\u6a5f\u5668\u4eba\u7684\u53ef\u64f4\u5145\u5b78\u7fd2\u5c0d\u65bc\u5b83\u5011\u5728\u771f\u5be6\u4e16\u754c\u61c9\u7528\u4e2d\u81f3\u95dc\u91cd\u8981\u3002\u96d6\u7136\u50b3\u7d71\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8cf4\u5f37\u5316\u5b78\u7fd2\u6216\u9060\u7a0b\u64cd\u4f5c\u4f86\u5be6\u73fe\u5168\u8eab\u63a7\u5236\uff0c\u4f46\u5b83\u5011\u5f80\u5f80\u53d7\u5230\u6a21\u64ec\u74b0\u5883\u591a\u6a23\u6027\u548c\u793a\u7bc4\u6536\u96c6\u6210\u672c\u9ad8\u7684\u9650\u5236\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u985e\u5f71\u7247\u7121\u8655\u4e0d\u5728\uff0c\u4e26\u63d0\u4f9b\u8a9e\u7fa9\u548c\u52d5\u4f5c\u8cc7\u8a0a\u7684\u672a\u958b\u767c\u4f86\u6e90\uff0c\u9019\u53ef\u4ee5\u986f\u8457\u589e\u5f37\u985e\u4eba\u6a5f\u5668\u4eba\u7684\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u4ecb\u7d39 Humanoid-X\uff0c\u4e00\u500b\u5305\u542b\u8d85\u904e 2000 \u842c\u500b\u985e\u4eba\u6a5f\u5668\u4eba\u59ff\u52e2\u7684\u5927\u578b\u8cc7\u6599\u96c6\uff0c\u4e26\u9644\u6709\u5c0d\u61c9\u7684\u6587\u5b57\u52d5\u4f5c\u63cf\u8ff0\uff0c\u65e8\u5728\u5229\u7528\u9019\u4e9b\u8c50\u5bcc\u7684\u8cc7\u6599\u3002Humanoid-X \u662f\u900f\u904e\u4e00\u500b\u5168\u9762\u7684\u7ba1\u9053\u7b56\u5c55\uff1a\u5f9e\u7db2\u969b\u7db2\u8def\u4e2d\u6316\u6398\u8cc7\u6599\u3001\u7522\u751f\u5f71\u7247\u5b57\u5e55\u3001\u5c07\u4eba\u985e\u52d5\u4f5c\u91cd\u65b0\u5b9a\u4f4d\u5230\u985e\u4eba\u6a5f\u5668\u4eba\uff0c\u4ee5\u53ca\u91dd\u5c0d\u771f\u5be6\u4e16\u754c\u90e8\u7f72\u9032\u884c\u7b56\u7565\u5b78\u7fd2\u3002\u6709\u4e86 Humanoid-X\uff0c\u6211\u5011\u9032\u4e00\u6b65\u8a13\u7df4\u4e00\u500b\u5927\u578b\u985e\u4eba\u6a21\u578b UH-1\uff0c\u5b83\u4ee5\u6587\u5b57\u6307\u4ee4\u4f5c\u70ba\u8f38\u5165\uff0c\u4e26\u8f38\u51fa\u5c0d\u61c9\u7684\u52d5\u4f5c\u4f86\u63a7\u5236\u985e\u4eba\u6a5f\u5668\u4eba\u3002\u5ee3\u6cdb\u7684\u6a21\u64ec\u548c\u771f\u5be6\u4e16\u754c\u5be6\u9a57\u9a57\u8b49\u4e86\u6211\u5011\u53ef\u64f4\u5145\u7684\u8a13\u7df4\u65b9\u6cd5\u6703\u5e36\u4f86\u6587\u5b57\u985e\u4eba\u63a7\u5236\u7684\u512a\u7570\u6cdb\u5316\uff0c\u9019\u6a19\u8a8c\u8457\u9081\u5411\u9069\u61c9\u6027\u3001\u771f\u5be6\u4e16\u754c\u5c31\u7dd2\u7684\u985e\u4eba\u6a5f\u5668\u4eba\u9081\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002", "author": "Jiageng Mao et.al.", "authors": "Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Guizilini, Yue Wang", "id": "2412.14172v1", "paper_url": "http://arxiv.org/abs/2412.14172v1", "repo": "null"}}