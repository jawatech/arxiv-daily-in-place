{"2412.07754": {"publish_time": "2024-12-10", "title": "PortraitTalk: Towards Customizable One-Shot Audio-to-Talking Face Generation", "paper_summary": "Audio-driven talking face generation is a challenging task in digital\ncommunication. Despite significant progress in the area, most existing methods\nconcentrate on audio-lip synchronization, often overlooking aspects such as\nvisual quality, customization, and generalization that are crucial to producing\nrealistic talking faces. To address these limitations, we introduce a novel,\ncustomizable one-shot audio-driven talking face generation framework, named\nPortraitTalk. Our proposed method utilizes a latent diffusion framework\nconsisting of two main components: IdentityNet and AnimateNet. IdentityNet is\ndesigned to preserve identity features consistently across the generated video\nframes, while AnimateNet aims to enhance temporal coherence and motion\nconsistency. This framework also integrates an audio input with the reference\nimages, thereby reducing the reliance on reference-style videos prevalent in\nexisting approaches. A key innovation of PortraitTalk is the incorporation of\ntext prompts through decoupled cross-attention mechanisms, which significantly\nexpands creative control over the generated videos. Through extensive\nexperiments, including a newly developed evaluation metric, our model\ndemonstrates superior performance over the state-of-the-art methods, setting a\nnew standard for the generation of customizable realistic talking faces\nsuitable for real-world applications.", "paper_summary_zh": "\u8a9e\u97f3\u9a45\u52d5\u5c0d\u8a71\u4eba\u81c9\u751f\u6210\u662f\u6578\u4f4d\u6e9d\u901a\u4e2d\u7684\u4e00\u9805\u8271\u9245\u4efb\u52d9\u3002\u5118\u7ba1\u8a72\u9818\u57df\u5df2\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4f46\u73fe\u6709\u65b9\u6cd5\u5927\u591a\u96c6\u4e2d\u5728\u97f3\u8a0a\u5507\u5f62\u540c\u6b65\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u8996\u89ba\u54c1\u8cea\u3001\u81ea\u8a02\u5316\u548c\u6cdb\u5316\u7b49\u5c0d\u65bc\u7522\u751f\u903c\u771f\u5c0d\u8a71\u4eba\u81c9\u81f3\u95dc\u91cd\u8981\u7684\u9762\u5411\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u3001\u53ef\u81ea\u8a02\u7684\u55ae\u6b21\u97f3\u8a0a\u9a45\u52d5\u5c0d\u8a71\u4eba\u81c9\u751f\u6210\u67b6\u69cb\uff0c\u540d\u70ba PortraitTalk\u3002\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5229\u7528\u4e00\u500b\u6f5b\u5728\u64f4\u6563\u67b6\u69cb\uff0c\u8a72\u67b6\u69cb\u5305\u542b\u5169\u500b\u4e3b\u8981\u5143\u4ef6\uff1aIdentityNet \u548c AnimateNet\u3002IdentityNet \u65e8\u5728\u5728\u751f\u6210\u7684\u5f71\u7247\u756b\u683c\u4e2d\u6301\u7e8c\u4fdd\u7559\u8eab\u5206\u7279\u5fb5\uff0c\u800c AnimateNet \u5247\u65e8\u5728\u589e\u5f37\u6642\u9593\u76f8\u5e72\u6027\u548c\u52d5\u4f5c\u4e00\u81f4\u6027\u3002\u6b64\u67b6\u69cb\u9084\u5c07\u97f3\u8a0a\u8f38\u5165\u8207\u53c3\u8003\u5f71\u50cf\u6574\u5408\uff0c\u5f9e\u800c\u6e1b\u5c11\u73fe\u6709\u65b9\u6cd5\u4e2d\u666e\u904d\u4f9d\u8cf4\u53c3\u8003\u98a8\u683c\u5f71\u7247\u7684\u60c5\u6cc1\u3002PortraitTalk \u7684\u4e00\u9805\u95dc\u9375\u5275\u65b0\u662f\u900f\u904e\u89e3\u8026\u4ea4\u53c9\u6ce8\u610f\u529b\u6a5f\u5236\u7d0d\u5165\u6587\u5b57\u63d0\u793a\uff0c\u9019\u986f\u8457\u64f4\u5c55\u4e86\u5c0d\u751f\u6210\u5f71\u7247\u7684\u5275\u610f\u63a7\u5236\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u5305\u62ec\u4e00\u500b\u65b0\u958b\u767c\u7684\u8a55\u4f30\u6307\u6a19\uff0c\u6211\u5011\u7684\u6a21\u578b\u8b49\u660e\u4e86\u5176\u512a\u65bc\u6700\u5148\u9032\u65b9\u6cd5\u7684\u6548\u80fd\uff0c\u70ba\u53ef\u81ea\u8a02\u7684\u903c\u771f\u5c0d\u8a71\u4eba\u81c9\u751f\u6210\u6a39\u7acb\u4e86\u65b0\u6a19\u6e96\uff0c\u9069\u7528\u65bc\u771f\u5be6\u4e16\u754c\u7684\u61c9\u7528\u3002", "author": "Fatemeh Nazarieh et.al.", "authors": "Fatemeh Nazarieh, Zhenhua Feng, Diptesh Kanojia, Muhammad Awais, Josef Kittler", "id": "2412.07754v1", "paper_url": "http://arxiv.org/abs/2412.07754v1", "repo": "null"}}