{"2411.12593": {"publish_time": "2024-11-19", "title": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction", "paper_summary": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9032\u6b65\u63a8\u9032\u4e86\u5f71\u7247\u7406\u89e3\u4efb\u52d9\u7684\u6539\u9032\uff0c\u65b9\u6cd5\u662f\u5c07 LLM \u8207\u8996\u89ba\u6a21\u578b\u7d50\u5408\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u73fe\u6709\u7684\u57fa\u65bc LLM \u7684\u6a21\u578b\uff08\u4f8b\u5982 VideoLLaMA\u3001VideoChat\uff09\u90fd\u53d7\u5230\u8655\u7406\u77ed\u6642\u5f71\u7247\u7684\u9650\u5236\u3002\u6700\u8fd1\u5617\u8a66\u900f\u904e\u63d0\u53d6\u548c\u58d3\u7e2e\u8996\u89ba\u7279\u5fb5\u5230\u56fa\u5b9a\u8a18\u61b6\u9ad4\u5927\u5c0f\u4f86\u7406\u89e3\u9577\u671f\u5f71\u7247\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u9019\u4e9b\u65b9\u6cd5\u50c5\u5229\u7528\u8996\u89ba\u6a21\u5f0f\u4f86\u5408\u4f75\u5f71\u7247\u4ee3\u5e63\uff0c\u4e26\u5ffd\u7565\u8996\u89ba\u548c\u6587\u5b57\u67e5\u8a62\u4e4b\u9593\u7684\u95dc\u806f\u6027\uff0c\u5c0e\u81f4\u96e3\u4ee5\u6709\u6548\u8655\u7406\u8907\u96dc\u7684\u554f\u7b54\u4efb\u52d9\u3002\u70ba\u4e86\u61c9\u5c0d\u9577\u5f71\u7247\u548c\u8907\u96dc\u63d0\u793a\u7684\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa AdaCM$^2$\uff0c\u5b83\u9996\u6b21\u5728\u5f71\u7247\u4e32\u6d41\u4e2d\u4ee5\u81ea\u8ff4\u6b78\u7684\u65b9\u5f0f\u5f15\u5165\u4e86\u9069\u61c9\u6027\u8de8\u6a21\u5f0f\u8a18\u61b6\u9ad4\u6e1b\u5c11\u65b9\u6cd5\uff0c\u7528\u65bc\u5f71\u7247\u8207\u6587\u5b57\u5c0d\u9f4a\u3002\u6211\u5011\u5728\u5404\u7a2e\u5f71\u7247\u7406\u89e3\u4efb\u52d9\uff08\u4f8b\u5982\u5f71\u7247\u5b57\u5e55\u3001\u5f71\u7247\u554f\u7b54\u548c\u5f71\u7247\u5206\u985e\uff09\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cAdaCM$^2$ \u5728\u591a\u500b\u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u540c\u6642\u5927\u5e45\u6e1b\u5c11\u4e86\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u5728 LVU \u8cc7\u6599\u96c6\u4e2d\u7684\u591a\u9805\u4efb\u52d9\u4e2d\u53d6\u5f97\u4e86 4.5% \u7684\u9032\u6b65\uff0c\u540c\u6642\u5c07 GPU \u8a18\u61b6\u9ad4\u6d88\u8017\u6e1b\u5c11\u4e86 65%\u3002", "author": "Yuanbin Man et.al.", "authors": "Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, Miao Yin", "id": "2411.12593v1", "paper_url": "http://arxiv.org/abs/2411.12593v1", "repo": "null"}}