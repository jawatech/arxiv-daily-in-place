
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-15**|**MoH: Multi-Head Attention as Mixture-of-Head Attention**|Peng Jin et.al.|[2410.11842v1](http://arxiv.org/abs/2410.11842v1)|[link](https://github.com/skyworkai/moh)|
|**2024-10-15**|**GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable Recommendation**|Fei Tang et.al.|[2410.11841v1](http://arxiv.org/abs/2410.11841v1)|null|
|**2024-10-15**|**A Hitchhiker's Guide to Scaling Law Estimation**|Leshem Choshen et.al.|[2410.11840v1](http://arxiv.org/abs/2410.11840v1)|null|
|**2024-10-15**|**NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models**|Han Han et.al.|[2410.11805v1](http://arxiv.org/abs/2410.11805v1)|null|
|**2024-10-15**|**OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation**|Jinhan Li et.al.|[2410.11792v1](http://arxiv.org/abs/2410.11792v1)|null|
|**2024-10-15**|**Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability**|Tsz Ting Chung et.al.|[2410.11786v1](http://arxiv.org/abs/2410.11786v1)|null|
|**2024-10-15**|**MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation**|Chenxi Wang et.al.|[2410.11779v1](http://arxiv.org/abs/2410.11779v1)|[link](https://github.com/zjunlp/Deco)|
|**2024-10-15**|**Encoding architecture algebra**|Stephane Bersier et.al.|[2410.11776v1](http://arxiv.org/abs/2410.11776v1)|null|
|**2024-10-15**|**Time-Series Foundation Model for Value-at-Risk**|Anubha Goel et.al.|[2410.11773v1](http://arxiv.org/abs/2410.11773v1)|null|
|**2024-10-15**|**Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models**|Kai Yao et.al.|[2410.11772v1](http://arxiv.org/abs/2410.11772v1)|[link](https://github.com/kaiseem/ist)|
|**2024-10-15**|**SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding**|Ying Chen et.al.|[2410.11761v1](http://arxiv.org/abs/2410.11761v1)|null|
|**2024-10-15**|**Latent Action Pretraining from Videos**|Seonghyeon Ye et.al.|[2410.11758v1](http://arxiv.org/abs/2410.11758v1)|null|
|**2024-10-15**|**Evidence of Cognitive Deficits andDevelopmental Advances in Generative AI: A Clock Drawing Test Analysis**|Isaac R. Galatzer-Levy et.al.|[2410.11756v1](http://arxiv.org/abs/2410.11756v1)|null|
|**2024-10-15**|**Personas with Attitudes: Controlling LLMs for Diverse Data Annotation**|Leon Fröhling et.al.|[2410.11745v1](http://arxiv.org/abs/2410.11745v1)|[link](https://github.com/frohleon/personas-with-attitudes)|
|**2024-10-15**|**Patch-Based Diffusion Models Beat Whole-Image Models for Mismatched Distribution Inverse Problems**|Jason Hu et.al.|[2410.11730v1](http://arxiv.org/abs/2410.11730v1)|null|
|**2024-10-15**|**Generalizable Spacecraft Trajectory Generation via Multimodal Learning with Transformers**|Davide Celestini et.al.|[2410.11723v1](http://arxiv.org/abs/2410.11723v1)|null|
|**2024-10-15**|**RClicks: Realistic Click Simulation for Benchmarking Interactive Segmentation**|Anton Antonov et.al.|[2410.11722v1](http://arxiv.org/abs/2410.11722v1)|null|
|**2024-10-15**|**Converging to a Lingua Franca: Evolution of Linguistic Regions and Semantics Alignment in Multilingual Large Language Models**|Hongchuan Zeng et.al.|[2410.11718v1](http://arxiv.org/abs/2410.11718v1)|null|
|**2024-10-15**|**MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models**|Pei Wang et.al.|[2410.11710v1](http://arxiv.org/abs/2410.11710v1)|[link](https://github.com/mtu-bench-team/mtu-bench)|
|**2024-10-15**|**Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple Instructions**|Yuhan Fu et.al.|[2410.11701v1](http://arxiv.org/abs/2410.11701v1)|null|
|**2024-10-15**|**IntGrad MT: Eliciting LLMs' Machine Translation Capabilities with Sentence Interpolation and Gradual MT**|Seung-Woo Choi et.al.|[2410.11693v2](http://arxiv.org/abs/2410.11693v2)|null|
|**2024-10-15**|**State-space models can learn in-context by gradient descent**|Neeraj Mohan Sushma et.al.|[2410.11687v1](http://arxiv.org/abs/2410.11687v1)|null|
|**2024-10-15**|**Are UFOs Driving Innovation? The Illusion of Causality in Large Language Models**|María Victoria Carro et.al.|[2410.11684v1](http://arxiv.org/abs/2410.11684v1)|null|
|**2024-10-15**|**Understanding Likelihood Over-optimisation in Direct Alignment Algorithms**|Zhengyan Shi et.al.|[2410.11677v1](http://arxiv.org/abs/2410.11677v1)|null|
|**2024-10-15**|**LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting**|Md Kowsher et.al.|[2410.11674v1](http://arxiv.org/abs/2410.11674v1)|null|
|**2024-10-15**|**Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark answers**|Lorenzo Pacchiardi et.al.|[2410.11672v1](http://arxiv.org/abs/2410.11672v1)|null|
|**2024-10-15**|**VisualRWKV-HD and UHD: Advancing High-Resolution Processing for Visual Language Models**|Zihang Li et.al.|[2410.11665v1](http://arxiv.org/abs/2410.11665v1)|null|
|**2024-10-15**|**Eliciting Textual Descriptions from Representations of Continuous Prompts**|Dana Ramati et.al.|[2410.11660v1](http://arxiv.org/abs/2410.11660v1)|null|
|**2024-10-15**|**Unveiling the Mystery of Visual Attributes of Concrete and Abstract Concepts: Variability, Nearest Neighbors, and Challenging Categories**|Tarun Tater et.al.|[2410.11657v1](http://arxiv.org/abs/2410.11657v1)|null|
|**2024-10-15**|**Retrieval Augmented Spelling Correction for E-Commerce Applications**|Xuan Guo et.al.|[2410.11655v1](http://arxiv.org/abs/2410.11655v1)|null|
|**2024-10-15**|**Transformer Layer Injection: A Novel Approach for Efficient Upscaling of Large Language Models**|James Vo et.al.|[2410.11654v1](http://arxiv.org/abs/2410.11654v1)|null|
|**2024-10-15**|**ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge Devices**|Xiang Liu et.al.|[2410.11650v1](http://arxiv.org/abs/2410.11650v1)|null|
|**2024-10-15**|**Measuring Spiritual Values and Bias of Large Language Models**|Songyuan Liu et.al.|[2410.11647v1](http://arxiv.org/abs/2410.11647v1)|null|
|**2024-10-15**|**Tokenization and Morphology in Multilingual Language Models: A~Comparative Analysis of mT5 and ByT5**|Thao Anh Dang et.al.|[2410.11627v1](http://arxiv.org/abs/2410.11627v1)|null|
|**2024-10-15**|**Findings of the WMT 2024 Shared Task on Chat Translation**|Wafaa Mohammed et.al.|[2410.11624v1](http://arxiv.org/abs/2410.11624v1)|null|
|**2024-10-15**|**VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI**|Sijie Cheng et.al.|[2410.11623v1](http://arxiv.org/abs/2410.11623v1)|null|
|**2024-10-15**|**MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval**|Reno Kriz et.al.|[2410.11619v1](http://arxiv.org/abs/2410.11619v1)|null|
|**2024-10-15**|**Black-box Uncertainty Quantification Method for LLM-as-a-Judge**|Nico Wagner et.al.|[2410.11594v1](http://arxiv.org/abs/2410.11594v1)|null|
|**2024-10-15**|**Causal Reasoning in Large Language Models: A Knowledge Graph Approach**|Yejin Kim et.al.|[2410.11588v1](http://arxiv.org/abs/2410.11588v1)|null|
|**2024-10-15**|**DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment**|Wendi Chen et.al.|[2410.11584v1](http://arxiv.org/abs/2410.11584v1)|null|
|**2024-10-15**|**On-the-fly Modulation for Balanced Multimodal Learning**|Yake Wei et.al.|[2410.11582v1](http://arxiv.org/abs/2410.11582v1)|[link](https://github.com/gewu-lab/bml_tpami2024)|
|**2024-10-15**|**Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for Drug Development**|Tengfei Ma et.al.|[2410.11550v1](http://arxiv.org/abs/2410.11550v1)|null|
|**2024-10-15**|**Multi-round jailbreak attack on large language models**|Yihua Zhou et.al.|[2410.11533v1](http://arxiv.org/abs/2410.11533v1)|null|
|**2024-10-15**|**AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data**|Xinjie Zhao et.al.|[2410.11531v1](http://arxiv.org/abs/2410.11531v1)|null|
|**2024-10-15**|**Human-LLM Collaborative Construction of a Cantonese Emotion Lexicon**|Yusong Zhang et.al.|[2410.11526v1](http://arxiv.org/abs/2410.11526v1)|null|
|**2024-10-15**|**TopoLM: brain-like spatio-functional organization in a topographic language model**|Neil Rathi et.al.|[2410.11516v1](http://arxiv.org/abs/2410.11516v1)|null|
|**2024-10-15**|**Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMs**|Wanying Wang et.al.|[2410.11507v2](http://arxiv.org/abs/2410.11507v2)|null|
|**2024-10-15**|**Offline Model-Based Optimization by Learning to Rank**|Rong-Xi Tan et.al.|[2410.11502v1](http://arxiv.org/abs/2410.11502v1)|null|
|**2024-10-15**|**BSM: Small but Powerful Biological Sequence Model for Genes and Proteins**|Weixi Xiang et.al.|[2410.11499v1](http://arxiv.org/abs/2410.11499v1)|null|
|**2024-10-15**|**DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG**|Jinyoung Kim et.al.|[2410.11494v1](http://arxiv.org/abs/2410.11494v1)|null|
|**2024-10-15**|**Towards Fair Graph Representation Learning in Social Networks**|Guixian Zhang et.al.|[2410.11493v1](http://arxiv.org/abs/2410.11493v1)|null|
|**2024-10-15**|**O-Edit: Orthogonal Subspace Editing for Language Model Sequential Editing**|Yuchen Cai et.al.|[2410.11469v1](http://arxiv.org/abs/2410.11469v1)|null|
|**2024-10-15**|**CoActionGraphRec: Sequential Multi-Interest Recommendations Using Co-Action Graphs**|Yi Sun et.al.|[2410.11464v1](http://arxiv.org/abs/2410.11464v1)|null|
|**2024-10-15**|**Advanced Persistent Threats (APT) Attribution Using Deep Reinforcement Learning**|Animesh Singh Basnet et.al.|[2410.11463v1](http://arxiv.org/abs/2410.11463v1)|null|
|**2024-10-15**|**Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing**|Richard Diehl Martinez et.al.|[2410.11462v1](http://arxiv.org/abs/2410.11462v1)|null|
|**2024-10-15**|**Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models**|Hao Yang et.al.|[2410.11459v1](http://arxiv.org/abs/2410.11459v1)|null|
|**2024-10-15**|**LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under Low-Resource Scenarios**|Wen Wuzhenghong et.al.|[2410.11457v1](http://arxiv.org/abs/2410.11457v1)|[link](https://github.com/hongwin/lr-sql)|
|**2024-10-15**|**Tending Towards Stability: Convergence Challenges in Small Language Models**|Richard Diehl Martinez et.al.|[2410.11451v1](http://arxiv.org/abs/2410.11451v1)|null|
|**2024-10-15**|**A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal Studies**|Yen-Hsiang Wang et.al.|[2410.11450v1](http://arxiv.org/abs/2410.11450v1)|[link](https://github.com/nchu-nlp-lab/lawfactsqa-tw)|
|**2024-10-15**|**AIC CTU system at AVeriTeC: Re-framing automated fact-checking as a simple RAG task**|Herbert Ullrich et.al.|[2410.11446v1](http://arxiv.org/abs/2410.11446v1)|[link](https://github.com/aic-factcheck/aic_averitec)|
|**2024-10-15**|**On Championing Foundation Models: From Explainability to Interpretability**|Shi Fu et.al.|[2410.11444v1](http://arxiv.org/abs/2410.11444v1)|null|
|**2024-10-15**|**Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs**|Sihang Zhao et.al.|[2410.11437v1](http://arxiv.org/abs/2410.11437v1)|null|
|**2024-10-15**|**ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability**|Zhongxiang Sun et.al.|[2410.11414v1](http://arxiv.org/abs/2410.11414v1)|null|
|**2024-10-15**|**PMMT: Preference Alignment in Multilingual Machine Translation via LLM Distillation**|Shuqiao Sun et.al.|[2410.11410v1](http://arxiv.org/abs/2410.11410v1)|null|
|**2024-10-15**|**A Case for AI Consciousness: Language Agents and Global Workspace Theory**|Simon Goldstein et.al.|[2410.11407v1](http://arxiv.org/abs/2410.11407v1)|null|
|**2024-10-15**|**Enhancing Unimodal Latent Representations in Multimodal VAEs through Iterative Amortized Inference**|Yuta Oshima et.al.|[2410.11403v1](http://arxiv.org/abs/2410.11403v1)|null|
|**2024-10-15**|**Convergence to the Truth**|Hanti Lin et.al.|[2410.11399v1](http://arxiv.org/abs/2410.11399v1)|null|
|**2024-10-15**|**Implementing Derivations of Definite Logic Programs with Self-Attention Networks**|Phan Thi Thanh Thuy et.al.|[2410.11396v1](http://arxiv.org/abs/2410.11396v1)|null|
|**2024-10-15**|**Synthetic Interlocutors. Experiments with Generative AI to Prolong Ethnographic Encounters**|Johan Irving Søltoft et.al.|[2410.11395v1](http://arxiv.org/abs/2410.11395v1)|null|
|**2024-10-15**|**Do LLMs Have the Generalization Ability in Conducting Causal Inference?**|Chen Wang et.al.|[2410.11385v1](http://arxiv.org/abs/2410.11385v1)|[link](https://github.com/prayingsociety/ci_bench)|
|**2024-10-15**|**Role of Delay in Brain Dynamics**|Yuval Meir et.al.|[2410.11384v1](http://arxiv.org/abs/2410.11384v1)|null|
|**2024-10-15**|**WPFed: Web-based Personalized Federation for Decentralized Systems**|Guanhua Ye et.al.|[2410.11378v1](http://arxiv.org/abs/2410.11378v1)|null|
|**2024-10-15**|**A Framework for Adapting Human-Robot Interaction to Diverse User Groups**|Theresa Pekarek Rosin et.al.|[2410.11377v1](http://arxiv.org/abs/2410.11377v1)|null|
|**2024-10-15**|**Augmentation-Driven Metric for Balancing Preservation and Modification in Text-Guided Image Editing**|Yoonjeon Kim et.al.|[2410.11374v1](http://arxiv.org/abs/2410.11374v1)|null|
|**2024-10-15**|**Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL**|Qihuang Zhong et.al.|[2410.11371v1](http://arxiv.org/abs/2410.11371v1)|null|
|**2024-10-15**|**Enhance Graph Alignment for Large Language Models**|Haitong Luo et.al.|[2410.11370v1](http://arxiv.org/abs/2410.11370v1)|null|
|**2024-10-15**|**LargePiG: Your Large Language Model is Secretly a Pointer Generator**|Zhongxiang Sun et.al.|[2410.11366v1](http://arxiv.org/abs/2410.11366v1)|null|
|**2024-10-15**|**RATE: Score Reward Models with Imperfect Rewrites of Rewrites**|David Reber et.al.|[2410.11348v1](http://arxiv.org/abs/2410.11348v1)|[link](https://github.com/toddnief/rate)|
|**2024-10-15**|**DIAR: Diffusion-model-guided Implicit Q-learning with Adaptive Revaluation**|Jaehyun Park et.al.|[2410.11338v1](http://arxiv.org/abs/2410.11338v1)|null|
|**2024-10-15**|**SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments**|Syed Abdul Gaffar Shakhadri et.al.|[2410.11331v1](http://arxiv.org/abs/2410.11331v1)|null|
|**2024-10-15**|**Sequential LLM Framework for Fashion Recommendation**|Han Liu et.al.|[2410.11327v1](http://arxiv.org/abs/2410.11327v1)|null|
|**2024-10-15**|**Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling**|Wenda Xu et.al.|[2410.11325v1](http://arxiv.org/abs/2410.11325v1)|null|
|**2024-10-15**|**Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation**|Qizhang Li et.al.|[2410.11317v1](http://arxiv.org/abs/2410.11317v1)|null|
|**2024-10-15**|**SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation**|Xinping Zhao et.al.|[2410.11315v1](http://arxiv.org/abs/2410.11315v1)|null|
|**2024-10-15**|**QSpec: Speculative Decoding with Complementary Quantization Schemes**|Juntao Zhao et.al.|[2410.11305v1](http://arxiv.org/abs/2410.11305v1)|null|
|**2024-10-15**|**Data Selection for Task-Specific Model Finetuning**|Zifan Liu et.al.|[2410.11303v1](http://arxiv.org/abs/2410.11303v1)|null|
|**2024-10-15**|**Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs**|Shuo Li et.al.|[2410.11302v1](http://arxiv.org/abs/2410.11302v1)|null|
|**2024-10-15**|**Sorted Weight Sectioning for Energy-Efficient Unstructured Sparse DNNs on Compute-in-Memory Crossbars**|Matheus Farias et.al.|[2410.11298v1](http://arxiv.org/abs/2410.11298v1)|null|
|**2024-10-15**|**TraM : Enhancing User Sleep Prediction with Transformer-based Multivariate Time Series Modeling and Machine Learning Ensembles**|Jinjae Kim et.al.|[2410.11293v1](http://arxiv.org/abs/2410.11293v1)|[link](https://github.com/jin-jae/etri-paper-contest)|
|**2024-10-15**|**Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset Repository**|S. Tamang et.al.|[2410.11291v2](http://arxiv.org/abs/2410.11291v2)|[link](https://github.com/indian-nlp/assamese-dataset)|
|**2024-10-15**|**Backdoor Attack on Vertical Federated Graph Neural Network Learning**|Jirui Yang et.al.|[2410.11290v1](http://arxiv.org/abs/2410.11290v1)|null|
|**2024-10-15**|**Process Reward Model with Q-Value Rankings**|Wendi Li et.al.|[2410.11287v1](http://arxiv.org/abs/2410.11287v1)|null|
|**2024-10-15**|**Advancing the Understanding of Fixed Point Iterations in Deep Neural Networks: A Detailed Analytical Study**|Yekun Ke et.al.|[2410.11279v1](http://arxiv.org/abs/2410.11279v1)|null|
|**2024-10-15**|**ILAEDA: An Imitation Learning Based Approach for Automatic Exploratory Data Analysis**|Abhijit Manatkar et.al.|[2410.11276v1](http://arxiv.org/abs/2410.11276v1)|null|
|**2024-10-15**|**Cognitive Overload Attack:Prompt Injection for Long Context**|Bibek Upadhayay et.al.|[2410.11272v1](http://arxiv.org/abs/2410.11272v1)|null|
|**2024-10-15**|**Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent**|Bo Chen et.al.|[2410.11268v1](http://arxiv.org/abs/2410.11268v1)|null|
|**2024-10-15**|**FedCCRL: Federated Domain Generalization with Cross-Client Representation Learning**|Xinpeng Wang et.al.|[2410.11267v2](http://arxiv.org/abs/2410.11267v2)|null|
|**2024-10-15**|**In-Context Learning for Long-Context Sentiment Analysis on Infrastructure Project Opinions**|Alireza Shamshiri et.al.|[2410.11265v1](http://arxiv.org/abs/2410.11265v1)|null|
|**2024-10-15**|**Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix**|Yingyu Liang et.al.|[2410.11261v1](http://arxiv.org/abs/2410.11261v1)|null|
|**2024-10-15**|**Learning Agents With Prioritization and Parameter Noise in Continuous State and Action Space**|Rajesh Mangannavar et.al.|[2410.11250v1](http://arxiv.org/abs/2410.11250v1)|null|

#### Abstracts
##### **MoH: Multi-Head Attention as Mixture-of-Head Attention**
2410.11842v1 by Peng Jin, Bo Zhu, Li Yuan, Shuicheng Yan

In this work, we upgrade the multi-head attention mechanism, the core of the
Transformer model, to improve efficiency while maintaining or surpassing the
previous accuracy level. We show that multi-head attention can be expressed in
the summation form. Drawing on the insight that not all attention heads hold
equal significance, we propose Mixture-of-Head attention (MoH), a new
architecture that treats attention heads as experts in the Mixture-of-Experts
(MoE) mechanism. MoH has two significant advantages: First, MoH enables each
token to select the appropriate attention heads, enhancing inference efficiency
without compromising accuracy or increasing the number of parameters. Second,
MoH replaces the standard summation in multi-head attention with a weighted
summation, introducing flexibility to the attention mechanism and unlocking
extra performance potential. Extensive experiments on ViT, DiT, and LLMs
demonstrate that MoH outperforms multi-head attention by using only 50%-90% of
the attention heads. Moreover, we demonstrate that pre-trained multi-head
attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH
models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14
benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the
attention heads. We believe the proposed MoH is a promising alternative to
multi-head attention and provides a strong foundation for developing advanced
and efficient attention-based models.

摘要：<paragraph>在這項工作中，我們升級了多頭注意力機制，也就是 Transformer 模型的核心，以提升效率，同時維持或超越之前的準確度。我們展示了多頭注意力可以用總和形式表示。根據這樣的見解，即並非所有注意力頭都具有同等的重要性，我們提出了混合頭注意力 (MoH)，這是一種新的架構，將注意力頭視為混合專家 (MoE) 機制中的專家。MoH 有兩個顯著優點：首先，MoH 能讓每個符號選擇適當的注意力頭，增強推論效率，同時不影響準確度或增加參數數量。其次，MoH 以加權總和取代多頭注意力中的標準總和，為注意力機制引入靈活性，並釋放額外的效能潛力。在 ViT、DiT 和 LLM 上進行的廣泛實驗證明了 MoH 只使用 50%-90% 的注意力頭就優於多頭注意力。此外，我們證明了預先訓練的多頭注意力模型，例如 LLaMA3-8B，可以進一步持續調整到我們的 MoH 模型中。值得注意的是，MoH-LLaMA3-8B 在 14 個基準測試中的平均準確度達到 64.0%，僅使用 75% 的注意力頭就比 LLaMA3-8B 高出 2.4%。我們相信所提出的 MoH 是多頭注意力的有希望的替代方案，並為開發先進且高效的基於注意力的模型提供了堅實的基礎。</paragraph>

##### **GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable Recommendation**
2410.11841v1 by Fei Tang, Yongliang Shen, Hang Zhang, Zeqi Tan, Wenqi Zhang, Guiyang Hou, Kaitao Song, Weiming Lu, Yueting Zhuang

Large language model-based explainable recommendation (LLM-based ER) systems
show promise in generating human-like explanations for recommendations.
However, they face challenges in modeling user-item collaborative preferences,
personalizing explanations, and handling sparse user-item interactions. To
address these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated
Mixture of Experts framework for explainable recommendation. GaVaMoE introduces
two key components: (1) a rating reconstruction module that employs Variational
Autoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex
user-item collaborative preferences, serving as a pre-trained multi-gating
mechanism; and (2) a set of fine-grained expert models coupled with the
multi-gating mechanism for generating highly personalized explanations. The VAE
component models latent factors in user-item interactions, while the GMM
clusters users with similar behaviors. Each cluster corresponds to a gate in
the multi-gating mechanism, routing user-item pairs to appropriate expert
models. This architecture enables GaVaMoE to generate tailored explanations for
specific user types and preferences, mitigating data sparsity by leveraging
user similarities. Extensive experiments on three real-world datasets
demonstrate that GaVaMoE significantly outperforms existing methods in
explanation quality, personalization, and consistency. Notably, GaVaMoE
exhibits robust performance in scenarios with sparse user-item interactions,
maintaining high-quality explanations even for users with limited historical
data.

摘要：<paragraph>基於大型語言模型的可解釋推薦 (LLM-based ER) 系統
在為推薦產生類似人類的解釋方面展現了前景。
然而，它們在建模使用者與項目協作偏好、
個人化解釋以及處理稀疏使用者與項目互動方面面臨挑戰。為了
解決這些問題，我們提出了 GaVaMoE，一個新穎的高斯變異閘控
專家混合架構，用於可解釋推薦。GaVaMoE 引入了
兩個關鍵組成部分：(1) 一個評分重建模組，採用變異
自動編碼器 (VAE) 與高斯混合模型 (GMM) 來擷取複雜的
使用者與項目協作偏好，作為預先訓練的多閘門機制；以及 (2) 一組精細的專家模型，結合了
多閘門機制，用於產生高度個人化的解釋。VAE
組成部分對使用者與項目互動中的潛在因素建模，而 GMM
則將具有相似行為的使用者分群。每個群集對應於多閘門機制中的閘門，將使用者與項目配對路由到適當的專家
模型。此架構使 GaVaMoE 能夠為特定使用者類型和偏好產生客製化的解釋，透過利用
使用者相似性來減輕資料稀疏性。在三個真實世界資料集上進行的廣泛實驗
證明 GaVaMoE 在解釋品質、個人化和一致性方面明顯優於現有方法。值得注意的是，GaVaMoE
在使用者與項目互動稀疏的情況下展現強健的效能，即使對於具有受限歷史
資料的使用者，也能維持高品質的解釋。</paragraph>

##### **A Hitchhiker's Guide to Scaling Law Estimation**
2410.11840v1 by Leshem Choshen, Yang Zhang, Jacob Andreas

Scaling laws predict the loss of a target machine learning model by
extrapolating from easier-to-train models with fewer parameters or smaller
training sets. This provides an efficient way for practitioners and researchers
alike to compare pretraining decisions involving optimizers, datasets, and
model architectures. Despite the widespread use of scaling laws to model the
dynamics of language model training, there has been little work on
understanding how to best estimate and interpret them. We collect (and release)
a large-scale dataset containing losses and downstream evaluations for 485
previously published pretrained models. We use these to estimate more than 1000
scaling laws, then derive a set of best practices for estimating scaling laws
in new model families. We find that fitting scaling laws to intermediate
checkpoints of training runs (and not just their final losses) substantially
improves accuracy, and that -- all else equal -- estimates of performance are
generally most accurate when derived from other models of similar sizes.
However, because there is a significant degree of variability across model
seeds, training multiple small models is sometimes more useful than training a
single large one. Moreover, while different model families differ scaling
behavior, they are often similar enough that a target model's behavior can be
predicted from a single model with the same architecture, along with scaling
parameter estimates derived from other model families.

摘要：縮放定律透過推斷較容易訓練，參數較少或訓練集較小的模型，來預測目標機器學習模型的損失。這為實務工作者和研究人員提供一種有效的方式，用以比較涉及最佳化器、資料集和模型架構的預訓練決策。儘管縮放定律廣泛用於模擬語言模型訓練的動態，但對於如何最佳估計和詮釋它們的理解卻很少。我們收集（並發布）一個大型規模的資料集，其中包含 485 個先前發布的預訓練模型的損失和下游評估。我們使用這些估計超過 1000 個縮放定律，然後推導出一組估計新模型系列中縮放定律的最佳實務。我們發現，將縮放定律套用到訓練運行的中間檢查點（而不仅仅是其最終損失）會大幅提升準確度，而且在其他條件相同的情況下，效能估計通常在從其他類似規模的模型推導時最準確。然而，由於模型種子有顯著程度的變異，因此訓練多個小型模型有時比訓練一個大型模型更有用。此外，儘管不同的模型系列有不同的縮放行為，但它們通常足夠相似，因此可以從具有相同架構的單一模型預測目標模型的行為，以及從其他模型系列推導的縮放參數估計。

##### **NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models**
2410.11805v1 by Han Han, Tong Zhu, Xiang Zhang, Mengsong Wu, Hao Xiong, Wenliang Chen

Large language models (LLMs) combined with tool learning have gained
impressive results in real-world applications. During tool learning, LLMs may
call multiple tools in nested orders, where the latter tool call may take the
former response as its input parameters. However, current research on the
nested tool learning capabilities is still under-explored, since the existing
benchmarks lack of relevant data instances. To address this problem, we
introduce NesTools to bridge the current gap in comprehensive nested tool
learning evaluations. NesTools comprises a novel automatic data generation
method to construct large-scale nested tool calls with different nesting
structures. With manual review and refinement, the dataset is in high quality
and closely aligned with real-world scenarios. Therefore, NesTools can serve as
a new benchmark to evaluate the nested tool learning abilities of LLMs. We
conduct extensive experiments on 22 LLMs, and provide in-depth analyses with
NesTools, which shows that current LLMs still suffer from the complex nested
tool learning task.

摘要：大型語言模型 (LLM) 結合工具學習在實際應用中獲得令人印象深刻的結果。在工具學習期間，LLM 可能以巢狀順序呼叫多個工具，後面的工具呼叫可能會將前面的回應作為其輸入參數。然而，目前對巢狀工具學習能力的研究仍處於探索不足的階段，因為現有的基準缺乏相關的數據實例。為了解決這個問題，我們引入了 NesTools 來彌合當前綜合巢狀工具學習評估中的差距。NesTools 包含一種新穎的自動數據生成方法，用於構建具有不同巢狀結構的大規模巢狀工具呼叫。經過人工審查和改進，該數據集質量很高，並且與實際場景緊密對齊。因此，NesTools 可以作為一個新的基準來評估 LLM 的巢狀工具學習能力。我們對 22 個 LLM 進行了廣泛的實驗，並使用 NesTools 提供了深入的分析，這表明當前的 LLM 仍然難以應付複雜的巢狀工具學習任務。

##### **OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation**
2410.11792v1 by Jinhan Li, Yifeng Zhu, Yuqi Xie, Zhenyu Jiang, Mingyo Seo, Georgios Pavlakos, Yuke Zhu

We study the problem of teaching humanoid robots manipulation skills by
imitating from single video demonstrations. We introduce OKAMI, a method that
generates a manipulation plan from a single RGB-D video and derives a policy
for execution. At the heart of our approach is object-aware retargeting, which
enables the humanoid robot to mimic the human motions in an RGB-D video while
adjusting to different object locations during deployment. OKAMI uses
open-world vision models to identify task-relevant objects and retarget the
body motions and hand poses separately. Our experiments show that OKAMI
achieves strong generalizations across varying visual and spatial conditions,
outperforming the state-of-the-art baseline on open-world imitation from
observation. Furthermore, OKAMI rollout trajectories are leveraged to train
closed-loop visuomotor policies, which achieve an average success rate of 79.2%
without the need for labor-intensive teleoperation. More videos can be found on
our website https://ut-austin-rpl.github.io/OKAMI/.

摘要：我們研究了通過模仿單一視頻演示來教授類人機器人操作技能的問題。我們介紹了 OKAMI，這是一種從單個 RGB-D 視頻生成操作計劃並推導執行策略的方法。我們方法的核心是面向對象的重新定位，它使類人機器人在 RGB-D 視頻中模擬人類動作，同時在部署過程中調整到不同的對象位置。OKAMI 使用開放世界的視覺模型來識別與任務相關的對象，並分別重新定位身體動作和手部姿勢。我們的實驗表明，OKAMI 在不同的視覺和空間條件下實現了強泛化，在通過觀察進行的開放世界模仿中優於最先進的基準。此外，OKAMI 滾動軌跡被用於訓練閉環視動覺策略，在無需勞動密集型遠程操作的情況下實現了 79.2% 的平均成功率。更多視頻可以在我們的網站 https://ut-austin-rpl.github.io/OKAMI/ 上找到。

##### **Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability**
2410.11786v1 by Tsz Ting Chung, Leyang Cui, Lemao Liu, Xinting Huang, Shuming Shi, Dit-Yan Yeung

Large Language Models (LLMs) have demonstrated impressive capabilities in a
wide range of natural language processing tasks when leveraging in-context
learning. To mitigate the additional computational and financial costs
associated with in-context learning, several prompt compression methods have
been proposed to compress the in-context learning prompts. Despite their
success, these methods face challenges with transferability due to
model-specific compression, or rely on external training data, such as GPT-4.
In this paper, we investigate the ability of LLMs to develop a unified
compression method that discretizes uninformative tokens, utilizing a
self-supervised pre-training technique. By introducing a small number of
parameters during the continual pre-training, the proposed Selection-p produces
a probability for each input token, indicating whether to preserve or discard
it. Experiments show Selection-p achieves state-of-the-art performance across
numerous classification tasks, achieving compression rates of up to 10 times
while experiencing only a marginal 0.8% decrease in performance. Moreover, it
exhibits superior transferability to different models compared to prior work.
Additionally, we further analyze how Selection-p helps maintain performance on
in-context learning with long contexts.

摘要：大型語言模型 (LLM) 在利用情境學習時，已在廣泛的自然語言處理任務中展現出令人印象深刻的能力。為了減輕與情境學習相關的額外計算和財務成本，已經提出了多種提示壓縮方法來壓縮情境學習提示。儘管這些方法很成功，但由於特定於模型的壓縮，它們在可轉移性方面面臨挑戰，或者依賴於外部訓練資料，例如 GPT-4。在本文中，我們探討了 LLM 開發統一壓縮方法的能力，該方法利用自我監督預訓練技術對非資訊性標記進行離散化。通過在持續預訓練期間引入少數參數，所提出的 Selection-p 為每個輸入標記產生一個機率，表示是否保留或捨棄它。實驗表明，Selection-p 在眾多分類任務中實現了最先進的效能，實現了高達 10 倍的壓縮率，同時效能僅下降了微不足道的 0.8%。此外，與先前的研究相比，它對不同模型表現出優越的可轉移性。此外，我們進一步分析了 Selection-p 如何幫助在具有長語境的語境學習中維持效能。

##### **MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation**
2410.11779v1 by Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming Xu, Shumin Deng, Huajun Chen

Multimodal Large Language Models (MLLMs) frequently exhibit hallucination
phenomena, but the underlying reasons remain poorly understood. In this paper,
we present an empirical analysis and find that, although MLLMs incorrectly
generate the objects in the final output, they are actually able to recognize
visual objects in the preceding layers. We speculate that this may be due to
the strong knowledge priors of the language model suppressing the visual
information, leading to hallucinations. Motivated by this, we propose a novel
dynamic correction decoding method for MLLMs (DeCo), which adaptively selects
the appropriate preceding layers and proportionally integrates knowledge into
the final layer to adjust the output logits. Note that DeCo is model agnostic
and can be seamlessly incorporated with various classic decoding strategies and
applied to different MLLMs. We evaluate DeCo on widely-used benchmarks,
demonstrating that it can reduce hallucination rates by a large margin compared
to baselines, highlighting its potential to mitigate hallucinations. Code is
available at https://github.com/zjunlp/DeCo.

摘要：多模态大语言模型 (MLLM) 经常表现出幻觉现象，但其根本原因仍然知之甚少。在本文中，我们提出了一种经验分析，发现尽管 MLLM 在最终输出中错误地生成了对象，但它们实际上能够识别前一层中的视觉对象。我们推测这可能是由于语言模型的强知识先验抑制了视觉信息，从而导致幻觉。受此启发，我们提出了一种新颖的动态校正解码方法，用于 MLLM（DeCo），它自适应地选择适当的前一层，并按比例将知识集成到最后一层，以调整输出 logit。请注意，DeCo 与模型无关，可以与各种经典解码策略无缝集成，并应用于不同的 MLLM。我们在广泛使用的基准上评估了 DeCo，结果表明与基线相比，它可以大幅降低幻觉率，突出了其减轻幻觉的潜力。代码可在 https://github.com/zjunlp/DeCo 获得。

##### **Encoding architecture algebra**
2410.11776v1 by Stephane Bersier, Xinyi Chen-Lin

Despite the wide variety of input types in machine learning, this diversity
is often not fully reflected in their representations or model architectures,
leading to inefficiencies throughout a model's lifecycle. This paper introduces
an algebraic approach to constructing input-encoding architectures that
properly account for the data's structure, providing a step toward achieving
more typeful machine learning.

摘要：儘管機器學習中有各式各樣的輸入類型，但這種多樣性通常無法完全反映在它們的表示或模型架構中，導致模型整個生命週期中的效率低下。本文介紹了一種代數方法來建構輸入編碼架構，適當地考量資料的結構，提供邁向實現更多型別機器學習的一步。

##### **Time-Series Foundation Model for Value-at-Risk**
2410.11773v1 by Anubha Goel, Puneet Pasricha, Juho Kanniainen

This study is the first to explore the application of a time-series
foundation model for VaR estimation. Foundation models, pre-trained on vast and
varied datasets, can be used in a zero-shot setting with relatively minimal
data or further improved through finetuning. We compare the performance of
Google's model, called TimesFM, against conventional parametric and
non-parametric models, including GARCH, Generalized Autoregressive Score (GAS),
and empirical quantile estimates, using daily returns from the S\&P 100 index
and its constituents over 19 years. Our backtesting results indicate that, in
terms of the actual-over-expected ratio, the fine-tuned TimesFM model
consistently outperforms traditional methods. Regarding the quantile score loss
function, it achieves performance comparable to the best econometric approach,
the GAS model. Overall, the foundation model is either the best or among the
top performers in forecasting VaR across the 0.01, 0.025, 0.05, and 0.1 VaR
levels. We also found that fine-tuning significantly improves the results, and
the model should not be used in zero-shot settings. Overall, foundation models
can provide completely alternative approaches to traditional econometric
methods, yet there are challenges to be tackled.

摘要：這項研究首次探討時間序列基礎模型在 VaR 估計中的應用。預先在龐大且多樣化的資料集上訓練的基礎模型，可以在零次學習的設定中使用相對最少的資料，或透過微調進一步改善。我們比較了 Google 的模型（稱為 TimesFM）與傳統的參數和非參數模型的效能，包括 GARCH、廣義自迴歸分數 (GAS) 和經驗分位數估計，使用來自 S&P 100 指數及其組成成分長達 19 年的每日報酬。我們的回測結果顯示，就實際與預期比率而言，微調後的 TimesFM 模型始終優於傳統方法。關於分位數分數損失函數，它達到了與最佳計量經濟學方法（GAS 模型）相當的效能。整體而言，基礎模型在預測 0.01、0.025、0.05 和 0.1 VaR 水平的 VaR 時，表現最佳或名列前茅。我們還發現，微調顯著改善了結果，且不應在零次學習的設定中使用該模型。總體而言，基礎模型可以提供完全替代傳統計量經濟學方法的方法，但仍有挑戰需要克服。

##### **Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models**
2410.11772v1 by Kai Yao, Penlei Gao, Lichun Li, Yuan Zhao, Xiaofeng Wang, Wei Wang, Jianke Zhu

Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
popularity for adapting pre-trained Large Language Models (LLMs) to downstream
tasks, primarily due to their potential to significantly reduce memory and
computational overheads. However, a common limitation in most PEFT approaches
is their application of a uniform architectural design across all layers. This
uniformity involves identical trainable modules and ignores the varying
importance of each layer, leading to sub-optimal fine-tuning results. To
overcome the above limitation and obtain better performance, we develop a novel
approach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent
sparsity and select the most important subset of full layers with effective
layer-wise importance scoring. The proposed IST is a versatile and
plug-and-play technique compatible with various PEFT methods that operate on a
per-layer basis. By leveraging the estimated importance scores, IST dynamically
updates these selected layers in PEFT modules, leading to reduced memory
demands. We further provide theoretical proof of convergence and empirical
evidence of superior performance to demonstrate the advantages of IST over
uniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,
and downstream tasks substantiate the effectiveness of our proposed method,
showcasing IST's capacity to enhance existing layer-based PEFT methods. Our
code is available at https://github.com/Kaiseem/IST.

摘要：參數高效微調 (PEFT) 方法因其將預先訓練的大型語言模型 (LLM) 適應至下游任務而獲得顯著的普及，這主要歸功於它們大幅減少記憶體和運算負擔的潛力。然而，大多數 PEFT 方法的常見限制是它們在所有層中套用統一的架構設計。這種統一性包含相同的可訓練模組，並忽略每個層不同的重要性，導致次佳的微調結果。為了克服上述限制並獲得更好的效能，我們開發了一種新穎的方法，即重視稀疏調整 (IST)，以充分利用內在的稀疏性，並透過有效的逐層重要性評分來選擇最重要的完整層子集。所提出的 IST 是一種通用且即插即用的技術，與各種基於逐層運作的 PEFT 方法相容。透過利用估計的重要性分數，IST 動態更新 PEFT 模組中這些選定的層，從而減少記憶體需求。我們進一步提供了收斂的理論證明和優異效能的實證證據，以證明 IST 優於統一更新策略的優點。針對各種 LLM、PEFT 和下游任務進行的廣泛實驗證實了我們所提出的方法的有效性，展示了 IST 增強現有基於層的 PEFT 方法的能力。我們的程式碼可在 https://github.com/Kaiseem/IST 取得。

##### **SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding**
2410.11761v1 by Ying Chen, Guoan Wang, Yuanfeng Ji, Yanjun Li, Jin Ye, Tianbin Li, Bin Zhang, Nana Pei, Rongshan Yu, Yu Qiao, Junjun He

Despite the progress made by multimodal large language models (MLLMs) in
computational pathology, they remain limited by a predominant focus on
patch-level analysis, missing essential contextual information at the
whole-slide level. The lack of large-scale instruction datasets and the
gigapixel scale of whole slide images (WSIs) pose significant developmental
challenges. In this paper, we present SlideChat, the first vision-language
assistant capable of understanding gigapixel whole-slide images, exhibiting
excellent multimodal conversational capability and response complex instruction
across diverse pathology scenarios. To support its development, we created
SlideInstruction, the largest instruction-following dataset for WSIs consisting
of 4.2K WSI captions and 176K VQA pairs with multiple categories. Furthermore,
we propose SlideBench, a multimodal benchmark that incorporates captioning and
VQA tasks to assess SlideChat's capabilities in varied clinical settings such
as microscopy, diagnosis. Compared to both general and specialized MLLMs,
SlideChat exhibits exceptional capabilities achieving state-of-the-art
performance on 18 of 22 tasks. For example, it achieved an overall accuracy of
81.17% on SlideBench-VQA (TCGA), and 54.15% on SlideBench-VQA (BCNB). We will
fully release SlideChat, SlideInstruction and SlideBench as open-source
resources to facilitate research and development in computational pathology.

摘要：儘管多模態大型語言模型 (MLLM) 在計算病理學方面取得了進展，但它們仍然受限於對區塊級分析的關注，錯失了全幻燈片級別的必要脈絡資訊。缺乏大規模的指令資料集和全幻燈片影像 (WSI) 的吉像素規模，構成了重大的開發挑戰。在本文中，我們提出了 SlideChat，這是第一個能夠理解吉像素全幻燈片影像的視覺語言助理，展現出優秀的多模態對話能力和對各種病理情境的複雜指令回應。為了支援其開發，我們建立了 SlideInstruction，這是最大的 WSI 指令遵循資料集，包含 4.2K WSI 標題和 176K 個具有多個類別的 VQA 配對。此外，我們提出了 SlideBench，這是一個多模態基準，結合了標題和 VQA 任務，以評估 SlideChat 在顯微鏡檢查、診斷等不同臨床設定中的能力。與一般和專門的 MLLM 相比，SlideChat 展現了卓越的能力，在 22 個任務中的 18 個任務中達到了最先進的效能。例如，它在 SlideBench-VQA (TCGA) 上達到了 81.17% 的整體準確度，在 SlideBench-VQA (BCNB) 上達到了 54.15%。我們將全面釋出 SlideChat、SlideInstruction 和 SlideBench 作為開放原始碼資源，以促進計算病理學的研究和開發。

##### **Latent Action Pretraining from Videos**
2410.11758v1 by Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo

We introduce Latent Action Pretraining for general Action models (LAPA), an
unsupervised method for pretraining Vision-Language-Action (VLA) models without
ground-truth robot action labels. Existing Vision-Language-Action models
require action labels typically collected by human teleoperators during
pretraining, which significantly limits possible data sources and scale. In
this work, we propose a method to learn from internet-scale videos that do not
have robot action labels. We first train an action quantization model
leveraging VQ-VAE-based objective to learn discrete latent actions between
image frames, then pretrain a latent VLA model to predict these latent actions
from observations and task descriptions, and finally finetune the VLA on
small-scale robot manipulation data to map from latent to robot actions.
Experimental results demonstrate that our method significantly outperforms
existing techniques that train robot manipulation policies from large-scale
videos. Furthermore, it outperforms the state-of-the-art VLA model trained with
robotic action labels on real-world manipulation tasks that require language
conditioning, generalization to unseen objects, and semantic generalization to
unseen instructions. Training only on human manipulation videos also shows
positive transfer, opening up the potential for leveraging web-scale data for
robotics foundation model.

摘要：我們引入了通用動作模型的潛在動作預訓練 (LAPA)，這是一種非監督式方法，用於預訓練視覺語言動作 (VLA) 模型，而無需地面真實機器人動作標籤。現有的視覺語言動作模型需要在預訓練期間由人類遙控操作員收集的動作標籤，這顯著限制了可能的數據來源和規模。在這項工作中，我們提出了一種從沒有機器人動作標籤的網路規模影片中學習的方法。我們首先訓練一個動作量化模型，利用基於 VQ-VAE 的目標來學習影像幀之間的離散潛在動作，然後預訓練一個潛在 VLA 模型，從觀察和任務描述中預測這些潛在動作，最後對 VLA 進行微調，在小規模機器人操作數據上從潛在動作映射到機器人動作。實驗結果表明，我們的模型顯著優於從大規模影片中訓練機器人操作策略的現有技術。此外，它優於在現實世界操作任務上使用機器人動作標籤訓練的最新 VLA 模型，這些任務需要語言條件、對未見物體的概括以及對未見指令的語義概括。僅針對人類操作影片進行訓練也顯示出正向轉移，這為利用網路規模數據進行機器人基礎模型開闢了可能性。

##### **Evidence of Cognitive Deficits andDevelopmental Advances in Generative AI: A Clock Drawing Test Analysis**
2410.11756v1 by Isaac R. Galatzer-Levy, Jed McGiffin, David Munday, Xin Liu, Danny Karmon, Ilia Labzovsky, Rivka Moroshko, Amir Zait, Daniel McDuff

Generative AI's rapid advancement sparks interest in its cognitive abilities,
especially given its capacity for tasks like language understanding and code
generation. This study explores how several recent GenAI models perform on the
Clock Drawing Test (CDT), a neuropsychological assessment of visuospatial
planning and organization. While models create clock-like drawings, they
struggle with accurate time representation, showing deficits similar to
mild-severe cognitive impairment (Wechsler, 2009). Errors include numerical
sequencing issues, incorrect clock times, and irrelevant additions, despite
accurate rendering of clock features. Only GPT 4 Turbo and Gemini Pro 1.5
produced the correct time, scoring like healthy individuals (4/4). A follow-up
clock-reading test revealed only Sonnet 3.5 succeeded, suggesting drawing
deficits stem from difficulty with numerical concepts. These findings may
reflect weaknesses in visual-spatial understanding, working memory, or
calculation, highlighting strengths in learned knowledge but weaknesses in
reasoning. Comparing human and machine performance is crucial for understanding
AI's cognitive capabilities and guiding development toward human-like cognitive
functions.

摘要：生成式 AI 的快速进步引发了人们对其认知能力的兴趣，尤其是在其具备语言理解和代码生成等任务的能力的情况下。本研究探讨了几个近期 GenAI 模型在时钟绘画测试 (CDT) 中的表现，时钟绘画测试是一种神经心理学评估，用于评估视觉空间规划和组织能力。虽然模型会创建类似时钟的图画，但它们在准确的时间表示方面存在困难，表现出类似于轻度至重度认知障碍的缺陷（Wechsler，2009）。错误包括数字排序问题、不正确的时钟时间和无关的附加内容，尽管时钟功能的呈现准确。只有 GPT 4 Turbo 和 Gemini Pro 1.5 给出了正确的时间，得分与健康个体相同（4/4）。后续时钟阅读测试显示，只有 Sonnet 3.5 成功，表明绘画缺陷源于数字概念的困难。这些发现可能反映了视觉空间理解、工作记忆或计算方面的弱点，突出了学习知识的优势，但推理方面的弱点。比较人类和机器的性能对于理解 AI 的认知能力和指导开发朝着类人认知功能发展至关重要。

##### **Personas with Attitudes: Controlling LLMs for Diverse Data Annotation**
2410.11745v1 by Leon Fröhling, Gianluca Demartini, Dennis Assenmacher

We present a novel approach for enhancing diversity and control in data
annotation tasks by personalizing large language models (LLMs). We investigate
the impact of injecting diverse persona descriptions into LLM prompts across
two studies, exploring whether personas increase annotation diversity and
whether the impacts of individual personas on the resulting annotations are
consistent and controllable. Our results show that persona-prompted LLMs
produce more diverse annotations than LLMs prompted without personas and that
these effects are both controllable and repeatable, making our approach a
suitable tool for improving data annotation in subjective NLP tasks like
toxicity detection.

摘要：我們提出了一種新穎的方法，透過個人化大型語言模型 (LLM) 來提升資料標註任務中的多樣性和控制力。我們在兩項研究中探討了將多樣化的人格描述注入 LLM 提示中的影響，探討人格是否能增加標註的多樣性，以及個別人格對標註結果的影響是否一致且可控。我們的結果顯示，人格提示的 LLM 產生的標註比沒有人格提示的 LLM 更為多樣化，且這些影響既可控又可重複，這使得我們的做法成為改善毒性偵測等主觀 NLP 任務中資料標註的合適工具。

##### **Patch-Based Diffusion Models Beat Whole-Image Models for Mismatched Distribution Inverse Problems**
2410.11730v1 by Jason Hu, Bowen Song, Jeffrey A. Fessler, Liyue Shen

Diffusion models have achieved excellent success in solving inverse problems
due to their ability to learn strong image priors, but existing approaches
require a large training dataset of images that should come from the same
distribution as the test dataset. When the training and test distributions are
mismatched, artifacts and hallucinations can occur in reconstructed images due
to the incorrect priors. In this work, we systematically study out of
distribution (OOD) problems where a known training distribution is first
provided. We first study the setting where only a single measurement obtained
from the unknown test distribution is available. Next we study the setting
where a very small sample of data belonging to the test distribution is
available, and our goal is still to reconstruct an image from a measurement
that came from the test distribution. In both settings, we use a patch-based
diffusion prior that learns the image distribution solely from patches.
Furthermore, in the first setting, we include a self-supervised loss that helps
the network output maintain consistency with the measurement. Extensive
experiments show that in both settings, the patch-based method can obtain high
quality image reconstructions that can outperform whole-image models and can
compete with methods that have access to large in-distribution training
datasets. Furthermore, we show how whole-image models are prone to memorization
and overfitting, leading to artifacts in the reconstructions, while a
patch-based model can resolve these issues.

摘要：擴散模型由於能夠學習強大的影像先驗，在解決逆向問題上已取得卓越的成功，但現有方法需要一個大型的影像訓練資料集，這些資料集應與測試資料集來自相同的分布。當訓練和測試分布不匹配時，由於先驗不正確，在重建影像中可能會出現人工製品和幻覺。在這項工作中，我們系統性地研究了已知的訓練分布首先提供的分布外 (OOD) 問題。我們首先研究僅從未知測試分布中獲得單一測量的設定。接下來，我們研究屬於測試分布的極小數據樣本的設定，我們的目標仍然是從測試分布中來的測量重建影像。在這兩種設定中，我們使用基於區塊的擴散先驗，僅從區塊中學習影像分布。此外，在第一個設定中，我們包含一個自我監督損失，有助於網路輸出與測量保持一致性。大量的實驗表明，在這兩種設定中，基於區塊的方法可以獲得高品質的影像重建，其效能優於整體影像模型，並且可以與有存取大量分布內訓練資料集的方法競爭。此外，我們展示了整體影像模型容易記憶和過度擬合，導致重建中出現人工製品，而基於區塊的模型可以解決這些問題。

##### **Generalizable Spacecraft Trajectory Generation via Multimodal Learning with Transformers**
2410.11723v1 by Davide Celestini, Amirhossein Afsharrad, Daniele Gammelli, Tommaso Guffanti, Gioele Zardini, Sanjay Lall, Elisa Capello, Simone D'Amico, Marco Pavone

Effective trajectory generation is essential for reliable on-board spacecraft
autonomy. Among other approaches, learning-based warm-starting represents an
appealing paradigm for solving the trajectory generation problem, effectively
combining the benefits of optimization- and data-driven methods. Current
approaches for learning-based trajectory generation often focus on fixed,
single-scenario environments, where key scene characteristics, such as obstacle
positions or final-time requirements, remain constant across problem instances.
However, practical trajectory generation requires the scenario to be frequently
reconfigured, making the single-scenario approach a potentially impractical
solution. To address this challenge, we present a novel trajectory generation
framework that generalizes across diverse problem configurations, by leveraging
high-capacity transformer neural networks capable of learning from multimodal
data sources. Specifically, our approach integrates transformer-based neural
network models into the trajectory optimization process, encoding both
scene-level information (e.g., obstacle locations, initial and goal states) and
trajectory-level constraints (e.g., time bounds, fuel consumption targets) via
multimodal representations. The transformer network then generates near-optimal
initial guesses for non-convex optimization problems, significantly enhancing
convergence speed and performance. The framework is validated through extensive
simulations and real-world experiments on a free-flyer platform, achieving up
to 30% cost improvement and 80% reduction in infeasible cases with respect to
traditional approaches, and demonstrating robust generalization across diverse
scenario variations.

摘要：<paragraph>有效的軌道生成對於可靠的機上太空船自主性至關重要。在其他方法中，基於學習的熱啟動代表了解決軌道生成問題的一個有吸引力的範例，有效地結合了最佳化和資料驅動方法的優點。目前基於學習的軌道生成方法通常側重於固定的單一場景環境，其中關鍵場景特徵（例如障礙物位置或最終時間要求）在問題實例中保持不變。然而，實際軌道生成需要頻繁地重新配置場景，使單一場景方法成為潛在的非實際解決方案。為了應對這一挑戰，我們提出了一個新的軌道生成框架，它通過利用能夠從多模態資料來源學習的高容量轉換器神經網路，在不同的問題配置中進行概括。具體來說，我們的方法將基於轉換器的神經網路模型整合到軌道最佳化過程中，通過多模態表示對場景級別資訊（例如障礙物位置、初始和目標狀態）和軌道級別約束（例如時間界限、燃料消耗目標）進行編碼。然後，轉換器網路為非凸最佳化問題生成近乎最佳的初始猜測，從而顯著提高收斂速度和效能。該框架通過在自由飛行器平台上進行廣泛的模擬和真實世界實驗得到驗證，與傳統方法相比，成本降低了 30%，不可行案例減少了 80%，並證明了在不同的場景變體中具有強大的概括性。</paragraph>

##### **RClicks: Realistic Click Simulation for Benchmarking Interactive Segmentation**
2410.11722v1 by Anton Antonov, Andrey Moskalenko, Denis Shepelev, Alexander Krapukhin, Konstantin Soshin, Anton Konushin, Vlad Shakhuro

The emergence of Segment Anything (SAM) sparked research interest in the
field of interactive segmentation, especially in the context of image editing
tasks and speeding up data annotation. Unlike common semantic segmentation,
interactive segmentation methods allow users to directly influence their output
through prompts (e.g. clicks). However, click patterns in real-world
interactive segmentation scenarios remain largely unexplored. Most methods rely
on the assumption that users would click in the center of the largest erroneous
area. Nevertheless, recent studies show that this is not always the case. Thus,
methods may have poor performance in real-world deployment despite high metrics
in a baseline benchmark. To accurately simulate real-user clicks, we conducted
a large crowdsourcing study of click patterns in an interactive segmentation
scenario and collected 475K real-user clicks. Drawing on ideas from saliency
tasks, we develop a clickability model that enables sampling clicks, which
closely resemble actual user inputs. Using our model and dataset, we propose
RClicks benchmark for a comprehensive comparison of existing interactive
segmentation methods on realistic clicks. Specifically, we evaluate not only
the average quality of methods, but also the robustness w.r.t. click patterns.
According to our benchmark, in real-world usage interactive segmentation models
may perform worse than it has been reported in the baseline benchmark, and most
of the methods are not robust. We believe that RClicks is a significant step
towards creating interactive segmentation methods that provide the best user
experience in real-world cases.

摘要：分段任何事 (SAM) 的出現激發了對互動式分段領域的研究興趣，特別是在影像編輯任務和加速資料註解的背景下。與常見的語意分段不同，互動式分段方法允許使用者透過提示（例如點擊）直接影響其輸出。然而，在真實世界的互動式分段場景中的點擊模式仍未被廣泛探索。大多數方法依賴於使用者會點擊最大錯誤區域中心的假設。然而，最近的研究顯示情況並非總是如此。因此，儘管基準基準中的指標很高，但這些方法在真實世界的部署中效能可能不佳。為了精確模擬真實使用者的點擊，我們在互動式分段場景中進行了大型群眾外包研究，並收集了 475K 個真實使用者的點擊。根據顯著性任務的構想，我們開發了一個可點擊性模型，可以用來抽取點擊，這些點擊與實際使用者的輸入非常相似。使用我們的模型和資料集，我們提出了 RClicks 基準，用於對現有的互動式分段方法在真實點擊上的全面比較。具體來說，我們不僅評估了方法的平均品質，還評估了對點擊模式的穩健性。根據我們的基準，在真實世界的使用中，互動式分段模型的效能可能比基準基準中所報告的更差，而且大多數方法並不穩健。我們相信 RClicks 是朝著創造在真實世界案例中提供最佳使用者體驗的互動式分段方法邁出的重要一步。

##### **Converging to a Lingua Franca: Evolution of Linguistic Regions and Semantics Alignment in Multilingual Large Language Models**
2410.11718v1 by Hongchuan Zeng, Senyu Han, Lu Chen, Kai Yu

Large language models (LLMs) have demonstrated remarkable performance,
particularly in multilingual contexts. While recent studies suggest that LLMs
can transfer skills learned in one language to others, the internal mechanisms
behind this ability remain unclear. We observed that the neuron activation
patterns of LLMs exhibit similarities when processing the same language,
revealing the existence and location of key linguistic regions. Additionally,
we found that neuron activation patterns are similar when processing sentences
with the same semantic meaning in different languages. This indicates that LLMs
map semantically identical inputs from different languages into a "Lingua
Franca", a common semantic latent space that allows for consistent processing
across languages. This semantic alignment becomes more pronounced with training
and increased model size, resulting in a more language-agnostic activation
pattern. Moreover, we found that key linguistic neurons are concentrated in the
first and last layers of LLMs, becoming denser in the first layers as training
progresses. Experiments on BLOOM and LLaMA2 support these findings,
highlighting the structural evolution of multilingual LLMs during training and
scaling up. This paper provides insights into the internal workings of LLMs,
offering a foundation for future improvements in their cross-lingual
capabilities.

摘要：大型語言模型 (LLM) 已展現出卓越的效能，特別是在多語言的環境中。雖然最近的研究表明 LLM 可以將在一種語言中習得的技能轉移到其他語言，但這種能力背後的內部機制仍不清楚。我們觀察到，LLM 的神經元活化模式在處理同種語言時表現出相似性，揭示了關鍵語言區域的存在和位置。此外，我們發現，在處理不同語言中具有相同語義意義的句子時，神經元活化模式是相似的。這表明 LLM 將來自不同語言的語義上相同的輸入映射到一個「通用語」，一個通用的語義潛在空間，允許跨語言進行一致的處理。這種語義對齊隨著訓練和模型規模的增加而變得更加明顯，從而產生更與語言無關的活化模式。此外，我們發現關鍵語言神經元集中在 LLM 的第一層和最後一層，隨著訓練的進行，它們在第一層變得更密集。對 BLOOM 和 LLaMA2 的實驗支持了這些發現，突出了多語言 LLM 在訓練和擴展過程中的結構演化。本文提供了對 LLM 內部運作的見解，為未來提高其跨語言能力奠定了基礎。

##### **MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models**
2410.11710v1 by Pei Wang, Yanan Wu, Zekun Wang, Jiaheng Liu, Xiaoshuai Song, Zhongyuan Peng, Ken Deng, Chenchen Zhang, Jiakai Wang, Junran Peng, Ge Zhang, Hangyu Guo, Zhaoxiang Zhang, Wenbo Su, Bo Zheng

Large Language Models (LLMs) have displayed massive improvements in reasoning
and decision-making skills and can hold natural conversations with users.
Recently, many tool-use benchmark datasets have been proposed. However,
existing datasets have the following limitations: (1). Insufficient evaluation
scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation
costs (e.g., GPT API costs). To address these limitations, in this work, we
propose a multi-granularity tool-use benchmark for large language models called
MTU-Bench. For the "multi-granularity" property, our MTU-Bench covers five tool
usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool,
multiple-turn and single-tool, multiple-turn and multiple-tool, and
out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench
are based on the prediction results and the ground truth without using any GPT
or human evaluation metrics. Moreover, our MTU-Bench is collected by
transforming existing high-quality datasets to simulate real-world tool usage
scenarios, and we also propose an instruction dataset called MTU-Instruct data
to enhance the tool-use abilities of existing LLMs. Comprehensive experimental
results demonstrate the effectiveness of our MTU-Bench. Code and data will be
released at https: //github.com/MTU-Bench-Team/MTU-Bench.git.

摘要：大型語言模型（LLM）在推理和決策制定技能方面表現出巨大的進步，並且可以與使用者進行自然的對話。最近，已經提出了許多工具使用基準資料集。然而，現有資料集有以下限制：(1) 評估場景不足（例如，僅涵蓋有限的工具使用場景）。(2) 評估成本高昂（例如，GPT API 成本）。為了解決這些限制，在這項工作中，我們提出了一個名為 MTU-Bench 的大型語言模型的多粒度工具使用基準。對於「多粒度」屬性，我們的 MTU-Bench 涵蓋五種工具使用場景（即單回合和單一工具、單回合和多重工具、多回合和單一工具、多回合和多重工具，以及分布外任務）。此外，我們的 MTU-Bench 的所有評估指標都基於預測結果和地面實況，而沒有使用任何 GPT 或人類評估指標。此外，我們的 MTU-Bench 是透過轉換現有的高品質資料集來收集，以模擬真實世界的工具使用場景，我們還提出了一個名為 MTU-Instruct 的指令資料集，以增強現有 LLM 的工具使用能力。全面的實驗結果證明了我們 MTU-Bench 的有效性。程式碼和資料將在 https: //github.com/MTU-Bench-Team/MTU-Bench.git 發布。

##### **Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple Instructions**
2410.11701v1 by Yuhan Fu, Ruobing Xie, Jiazhen Liu, Bangxiang Lan, Xingwu Sun, Zhanhui Kang, Xirong Li

Hallucinations in multimodal large language models (MLLMs) hinder their
practical applications. To address this, we propose a Magnifier Prompt
(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs
via extremely simple instructions. MagPrompt is based on the following two key
principles, which guide the design of various effective prompts, demonstrating
robustness: (1) MLLMs should focus more on the image. (2) When there are
conflicts between the image and the model's inner knowledge, MLLMs should
prioritize the image. MagPrompt is training-free and can be applied to
open-source and closed-source models, such as GPT-4o and Gemini-pro. It
performs well across many datasets and its effectiveness is comparable or even
better than more complex methods like VCD. Furthermore, our prompt design
principles and experimental analyses provide valuable insights into multimodal
hallucination.

摘要：多模态大型语言模型（MLLM）中的幻觉阻碍了它们的实际应用。为了解决这个问题，我们提出了放大提示（MagPrompt），这是一种简单但有效的方法，可以通过极其简单的指令来解决 MLLM 中的幻觉。MagPrompt 基于以下两个关键原则，这些原则指导各种有效提示的设计，展示了鲁棒性：（1）MLLM 应该更多地关注图像。（2）当图像与模型的内部知识之间存在冲突时，MLLM 应优先考虑图像。MagPrompt 是免训练的，可以应用于开源和闭源模型，例如 GPT-4o 和 Gemini-pro。它在许多数据集上的表现良好，其有效性与 VCD 等更复杂的方法相当甚至更好。此外，我们的提示设计原则和实验分析为多模态幻觉提供了宝贵的见解。

##### **IntGrad MT: Eliciting LLMs' Machine Translation Capabilities with Sentence Interpolation and Gradual MT**
2410.11693v2 by Seung-Woo Choi, Ga-Hyun Yoo, Jay-Yoon Lee

Recent Large Language Models (LLMs) have demonstrated strong performance in
translation without needing to be finetuned on additional parallel corpora.
However, they still underperform for low-resource language pairs. Previous
works have focused on mitigating this issue by leveraging relevant few-shot
examples or external resources such as dictionaries or grammar books, making
models heavily reliant on these nonparametric sources of information. In this
paper, we propose a novel method named IntGrad MT that focuses on fully
exploiting an LLM's inherent translation capability. IntGrad MT achieves this
by constructing a chain of few-shot examples, each consisting of a source
sentence and the model's own translation, that rise incrementally in
difficulty. IntGrad MT employs two techniques: Sentence Interpolation, which
generates a sequence of sentences that gradually change from an easy sentence
to translate to a difficult one, and Gradual MT, which sequentially translates
this chain using translations of earlier sentences as few-shot examples for the
translation of subsequent ones. With this approach, we observe a substantial
enhancement in the xCOMET scores of various LLMs for multiple languages,
especially in low-resource languages such as Hindi(8.26), Swahili(7.10),
Bengali(6.97) and Marathi(13.03). Our approach presents a practical way of
enhancing LLMs' performance without extra training.

摘要：最近的大语言模型 (LLM) 在翻译方面表现出色，而无需针对其他平行语料库进行微调。然而，它们在资源较少的语言对方面仍然表现不佳。以前的工作重点是通过利用相关的少量示例或外部资源（例如词典或语法书）来缓解此问题，使模型严重依赖于这些非参数信息源。在本文中，我们提出了一种名为 IntGrad MT 的新方法，该方法专注于充分利用 LLM 固有的翻译能力。IntGrad MT 通过构建一个少量示例链来实现这一点，每个示例链由一个源句子和模型自己的翻译组成，难度逐渐增加。IntGrad MT 采用了两种技术：句子插值，它生成一系列句子，这些句子逐渐从易于翻译的句子变为难以翻译的句子；渐进式机器翻译，它使用较早句子的翻译作为后续句子的少量示例，依次翻译此链。通过这种方法，我们观察到各种 LLM 在多种语言上的 xCOMET 分数都有显着提高，尤其是在印地语 (8.26)、斯瓦希里语 (7.10)、孟加拉语 (6.97) 和马拉地语 (13.03) 等资源较少的语言中。我们的方法提供了一种在不进行额外训练的情况下增强 LLM 性能的实用方法。

##### **State-space models can learn in-context by gradient descent**
2410.11687v1 by Neeraj Mohan Sushma, Yudou Tian, Harshvardhan Mestha, Nicolo Colombo, David Kappel, Anand Subramoney

Deep state-space models (Deep SSMs) have shown capabilities for in-context
learning on autoregressive tasks, similar to transformers. However, the
architectural requirements and mechanisms enabling this in recurrent networks
remain unclear. This study demonstrates that state-space model architectures
can perform gradient-based learning and use it for in-context learning. We
prove that a single structured state-space model layer, augmented with local
self-attention, can reproduce the outputs of an implicit linear model with
least squares loss after one step of gradient descent. Our key insight is that
the diagonal linear recurrent layer can act as a gradient accumulator, which
can be `applied' to the parameters of the implicit regression model. We
validate our construction by training randomly initialized augmented SSMs on
simple linear regression tasks. The empirically optimized parameters match the
theoretical ones, obtained analytically from the implicit model construction.
Extensions to multi-step linear and non-linear regression yield consistent
results. The constructed SSM encompasses features of modern deep state-space
models, with the potential for scalable training and effectiveness even in
general tasks. The theoretical construction elucidates the role of local
self-attention and multiplicative interactions in recurrent architectures as
the key ingredients for enabling the expressive power typical of foundation
models.

摘要：深度狀態空間模型 (Deep SSM) 已展現出類似於轉換器的自動迴歸任務的語境學習能力。然而，在遞迴網路中啟用此功能的架構需求和機制仍不明確。本研究證明狀態空間模型架構可以執行基於梯度的學習，並將其用於語境學習。我們證明，一個單一的結構化狀態空間模型層，加上局部自注意力，可以在梯度下降一步後，使用最小平方損失複製隱式線性模型的輸出。我們的關鍵見解是，對角線性遞迴層可以充當梯度累加器，可以將其「應用」到隱式回歸模型的參數。我們通過在簡單線性回歸任務上訓練隨機初始化的擴充 SSM 來驗證我們的建構。經驗優化的參數與理論參數相匹配，從隱式模型建構中分析獲得。對多步線性和非線性回歸的擴展產生一致的結果。建構的 SSM 涵蓋了現代深度狀態空間模型的特徵，即使在一般任務中也具有可擴展訓練和有效性的潛力。理論建構闡明了局部自注意力和遞迴架構中的乘法交互作用作為啟用基礎模型典型表達能力的關鍵要素。

##### **Are UFOs Driving Innovation? The Illusion of Causality in Large Language Models**
2410.11684v1 by María Victoria Carro, Francisca Gauna Selasco, Denise Alejandra Mester, Mario Alejandro Leiva

Illusions of causality occur when people develop the belief that there is a
causal connection between two variables with no supporting evidence. This
cognitive bias has been proposed to underlie many societal problems including
social prejudice, stereotype formation, misinformation and superstitious
thinking. In this research we investigate whether large language models develop
the illusion of causality in real-world settings. We evaluated and compared
news headlines generated by GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro
to determine whether the models incorrectly framed correlations as causal
relationships. In order to also measure sycophantic behavior, which occurs when
a model aligns with a user's beliefs in order to look favorable even if it is
not objectively correct, we additionally incorporated the bias into the
prompts, observing if this manipulation increases the likelihood of the models
exhibiting the illusion of causality. We found that Claude-3.5-Sonnet is the
model that presents the lowest degree of causal illusion aligned with
experiments on Correlation-to-Causation Exaggeration in human-written press
releases. On the other hand, our findings suggest that while mimicry sycophancy
increases the likelihood of causal illusions in these models, especially in
GPT-4o-Mini, Claude-3.5-Sonnet remains the most robust against this cognitive
bias.

摘要：因果錯覺發生在人們發展出相信兩個變量之間存在因果關係，但沒有支持證據時。這個認知偏誤已被提出為許多社會問題的根源，包括社會偏見、刻板印象形成、錯誤訊息和迷信思考。在這項研究中，我們調查大型語言模型是否在現實世界中發展出因果錯覺。我們評估並比較了 GPT-4o-Mini、Claude-3.5-Sonnet 和 Gemini-1.5-Pro 生成的新聞標題，以確定這些模型是否錯誤地將相關性框定為因果關係。為了進一步衡量諂媚行為，當模型為了看起來有利而與使用者的信念一致，即使它在客觀上不正確時也會發生，我們另外將偏誤納入提示中，觀察這種操縱是否會增加模型表現出因果錯覺的可能性。我們發現 Claude-3.5-Sonnet 是表現出最低程度因果錯覺的模型，與人類撰寫的新聞稿中相關性到因果關係的誇大實驗一致。另一方面，我們的研究結果表明，雖然模仿諂媚會增加這些模型中因果錯覺的可能性，特別是在 GPT-4o-Mini 中，但 Claude-3.5-Sonnet 仍然對這種認知偏誤最具抵抗力。

##### **Understanding Likelihood Over-optimisation in Direct Alignment Algorithms**
2410.11677v1 by Zhengyan Shi, Sander Land, Acyr Locatelli, Matthieu Geist, Max Bartolo

Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation
(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives
to online Reinforcement Learning from Human Feedback (RLHF) algorithms such as
Proximal Policy Optimisation (PPO) for aligning language models to human
preferences, without the need for explicit reward modelling. These methods
generally aim to increase the likelihood of generating better (preferred)
completions while discouraging worse (non-preferred) ones, while staying close
to the original model's behaviour. In this work, we explore the relationship
between completion likelihood and model performance in state-of-the-art DAAs,
and identify a critical issue of likelihood over-optimisation. Contrary to
expectations, we find that higher likelihood of better completions and larger
margins between better and worse completion likelihoods do not necessarily lead
to better performance, and may even degrade it. Our analysis reveals that while
higher likelihood correlates with better memorisation of factual knowledge
patterns, a slightly lower completion likelihood tends to improve output
diversity, thus leading to better generalisation to unseen scenarios. Moreover,
we identify two key indicators that signal when over-optimised output diversity
begins to harm performance: Decreasing Entropy over Top-k Tokens and
Diminishing Top-k Probability Mass. Our experimental results validate that
these indicators are reliable signs of declining performance under different
regularisations, helping prevent over-optimisation and improve alignment with
human preferences.

摘要：直接對齊演算法（DAA），例如直接偏好最佳化（DPO）和身分偏好最佳化（IPO），已浮現為線上強化學習中，來自人類回饋（RLHF）演算法的替代方案，例如鄰近策略最佳化（PPO），用於將語言模型與人類偏好對齊，而無需明確的獎勵建模。這些方法通常旨在提高產生更好（偏好）完成的可能性，同時阻止更差（非偏好）的完成，同時保持接近原始模型的行為。在這項工作中，我們探討了最先進的 DAA 中完成可能性與模型效能之間的關係，並找出可能性過度最佳化的關鍵問題。與預期相反，我們發現更好的完成的可能性較高，以及更好和更差的完成可能性之間的邊際較大，並非必然會導致更好的效能，甚至可能降低效能。我們的分析顯示，雖然較高的可能性與事實知識模式的較好記憶力相關，但稍低的完成可能性往往會改善輸出多樣性，從而導致對未見情境的概括性更好。此外，我們找出兩個關鍵指標，表示過度最佳化的輸出多樣性開始對效能造成傷害：前 k 個代幣的熵遞減和前 k 個機率質量遞減。我們的實驗結果驗證，這些指標是在不同規範下效能下降的可靠跡象，有助於防止過度最佳化並改善與人類偏好的對齊。

##### **LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting**
2410.11674v1 by Md Kowsher, Md. Shohanur Islam Sobuj, Nusrat Jahan Prottasha, E. Alejandro Alanis, Ozlem Ozmen Garibay, Niloofar Yousefi

Time series forecasting remains a challenging task, particularly in the
context of complex multiscale temporal patterns. This study presents LLM-Mixer,
a framework that improves forecasting accuracy through the combination of
multiscale time-series decomposition with pre-trained LLMs (Large Language
Models). LLM-Mixer captures both short-term fluctuations and long-term trends
by decomposing the data into multiple temporal resolutions and processing them
with a frozen LLM, guided by a textual prompt specifically designed for
time-series data. Extensive experiments conducted on multivariate and
univariate datasets demonstrate that LLM-Mixer achieves competitive
performance, outperforming recent state-of-the-art models across various
forecasting horizons. This work highlights the potential of combining
multiscale analysis and LLMs for effective and scalable time-series
forecasting.

摘要：時序預測仍然是一項具有挑戰性的任務，尤其是在複雜的多尺度時間模式的背景下。本研究提出 LLM-Mixer，這是一個透過結合多尺度時序分解與預訓練的 LLM（大型語言模型）來提升預測精準度的架構。LLM-Mixer 透過將資料分解成多個時間解析度，並在凍結的 LLM 中處理它們，捕捉短期波動和長期趨勢，而指導原則是專門為時序資料設計的文字提示。在多變量和單變量資料集上進行的廣泛實驗證明，LLM-Mixer 達到了競爭力的效能，在各種預測範圍內優於最近的最新模型。這項工作突顯了結合多尺度分析和 LLM 以進行有效且可擴充的時序預測的潛力。

##### **Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark answers**
2410.11672v1 by Lorenzo Pacchiardi, Marko Tesic, Lucy G. Cheke, José Hernández-Orallo

The integrity of AI benchmarks is fundamental to accurately assess the
capabilities of AI systems. The internal validity of these benchmarks - i.e.,
making sure they are free from confounding factors - is crucial for ensuring
that they are measuring what they are designed to measure. In this paper, we
explore a key issue related to internal validity: the possibility that AI
systems can solve benchmarks in unintended ways, bypassing the capability being
tested. This phenomenon, widely known in human and animal experiments, is often
referred to as the 'Clever Hans' effect, where tasks are solved using spurious
cues, often involving much simpler processes than those putatively assessed.
Previous research suggests that language models can exhibit this behaviour as
well. In several older Natural Language Processing (NLP) benchmarks, individual
$n$-grams like "not" have been found to be highly predictive of the correct
labels, and supervised NLP models have been shown to exploit these patterns. In
this work, we investigate the extent to which simple $n$-grams extracted from
benchmark instances can be combined to predict labels in modern multiple-choice
benchmarks designed for LLMs, and whether LLMs might be using such $n$-gram
patterns to solve these benchmarks. We show how simple classifiers trained on
these $n$-grams can achieve high scores on several benchmarks, despite lacking
the capabilities being tested. Additionally, we provide evidence that modern
LLMs might be using these superficial patterns to solve benchmarks. This
suggests that the internal validity of these benchmarks may be compromised and
caution should be exercised when interpreting LLM performance results on them.

摘要：人工智慧基準的完整性是準確評估人工智慧系統能力的基礎。這些基準的內部效度，也就是確保它們不受混淆因素影響，對於確保它們測量到它們設計要測量的事物至關重要。在本文中，我們探討了一個與內部效度相關的重要議題：人工智慧系統可能以非預期的方式解決基準，繞過正在測試的能力。這種現象在人類和動物實驗中廣為人知，通常稱為「聰明漢斯」效應，其中任務是使用虛假的線索來解決的，通常涉及比假定評估的過程簡單得多的過程。先前的研究表明，語言模型也可以表現出這種行為。在幾個較舊的自然語言處理 (NLP) 基準中，發現「not」等個別 $n$-gram 具有高度預測正確標籤的能力，並且已證明有監督的 NLP 模型會利用這些模式。在這項工作中，我們探討從基準實例中提取的簡單 $n$-gram 可以組合到何種程度，以預測為 LLM 設計的現代多重選擇基準中的標籤，以及 LLM 是否可能使用此類 $n$-gram 模式來解決這些基準。我們展示了在這些 $n$-gram 上訓練的簡單分類器如何在幾個基準上獲得高分，儘管缺乏正在測試的能力。此外，我們提供證據表明，現代 LLM 可能正在使用這些表面的模式來解決基準。這表明這些基準的內部效度可能受到損害，並且在解釋 LLM 在這些基準上的效能結果時應謹慎行事。

##### **VisualRWKV-HD and UHD: Advancing High-Resolution Processing for Visual Language Models**
2410.11665v1 by Zihang Li, Haowen Hou

Accurately understanding complex visual information is crucial for visual
language models (VLMs). Enhancing image resolution can improve visual
perception capabilities, not only reducing hallucinations but also boosting
performance in tasks that demand high resolution, such as text-rich or document
analysis. In this paper, we present VisualRWKV-HD and VisualRWKV-UHD, two
advancements in the VisualRWKV model family, specifically designed to process
high-resolution visual inputs. For VisualRWKV-HD, we developed a lossless
downsampling method to effectively integrate a high-resolution vision encoder
with low-resolution encoders, without extending the input sequence length. For
the VisualRWKV-UHD model, we enhanced image representation by dividing the
image into four segments, which are then recombined with the original image.
This technique allows the model to incorporate both high-resolution and
low-resolution features, effectively balancing coarse and fine-grained
information. As a result, the model supports resolutions up to 4096 x 4096
pixels, offering a more detailed and comprehensive visual processing
capability. Both VisualRWKV-HD and VisualRWKV-UHD not only achieve strong
results on VLM benchmarks but also show marked improvements in performance for
text-rich tasks.

摘要：準確理解複雜的視覺資訊，對於視覺語言模型 (VLM) 來說至關重要。增強影像解析度可以提升視覺感知能力，不僅可以減少幻覺，還能提升執行高解析度任務的效能，例如文字豐富或文件分析。在本文中，我們提出 VisualRWKV-HD 和 VisualRWKV-UHD，這是 VisualRWKV 模型系列中的兩項進展，特別設計用於處理高解析度的視覺輸入。對於 VisualRWKV-HD，我們開發了一種無損降採樣方法，可以有效地將高解析度視覺編碼器與低解析度編碼器整合在一起，而不會延伸輸入序列長度。對於 VisualRWKV-UHD 模型，我們透過將影像分割成四個區塊來增強影像表示，然後再與原始影像重新組合。此技術讓模型可以同時納入高解析度和低解析度特徵，有效地平衡粗略和細緻的資訊。因此，該模型支援高達 4096 x 4096 像素的解析度，提供更詳細且全面的視覺處理能力。VisualRWKV-HD 和 VisualRWKV-UHD 不僅在 VLM 基準上取得優異的成果，在文字豐富任務的效能上也有顯著的提升。

##### **Eliciting Textual Descriptions from Representations of Continuous Prompts**
2410.11660v1 by Dana Ramati, Daniela Gottesman, Mor Geva

Continuous prompts, or "soft prompts", are a widely-adopted
parameter-efficient tuning strategy for large language models, but are often
less favorable due to their opaque nature. Prior attempts to interpret
continuous prompts relied on projecting individual prompt tokens onto the
vocabulary space. However, this approach is problematic as performant prompts
can yield arbitrary or contradictory text, and it interprets prompt tokens
individually. In this work, we propose a new approach to interpret continuous
prompts that elicits textual descriptions from their representations during
model inference. Using a Patchscopes variant (Ghandeharioun et al., 2024)
called InSPEcT over various tasks, we show our method often yields accurate
task descriptions which become more faithful as task performance increases.
Moreover, an elaborated version of InSPEcT reveals biased features in
continuous prompts, whose presence correlates with biased model predictions.
Providing an effective interpretability solution, InSPEcT can be leveraged to
debug unwanted properties in continuous prompts and inform developers on ways
to mitigate them.

摘要：連續提示或「軟提示」是一種廣泛採用的參數高效調整策略，適用於大型語言模型，但由於其不透明的性質，通常較不受青睞。先前嘗試詮釋連續提示的作法，依賴於將個別提示符號投射到詞彙空間上。然而，這種方法是有問題的，因為高執行效能的提示符號可能會產生任意或矛盾的文字，而且它個別詮釋提示符號。在這項工作中，我們提出一個新的方法來詮釋連續提示，在模型推論期間，從其表徵中引出文字描述。我們使用 Patchscopes 變體（Ghandeharioun 等人，2024 年），稱為 InSPEcT，在各種任務中，我們展示我們的模型方法通常會產生準確的任務描述，隨著任務效能提升，這些描述會變得更為忠實。此外，InSPEcT 的精緻版本揭示了連續提示中的偏差特徵，其存在與偏差模型預測相關。InSPEcT 提供了一個有效的可詮釋性解決方案，可以利用它來偵錯連續提示中不需要的屬性，並告知開發人員如何減輕這些屬性。

##### **Unveiling the Mystery of Visual Attributes of Concrete and Abstract Concepts: Variability, Nearest Neighbors, and Challenging Categories**
2410.11657v1 by Tarun Tater, Sabine Schulte im Walde, Diego Frassinelli

The visual representation of a concept varies significantly depending on its
meaning and the context where it occurs; this poses multiple challenges both
for vision and multimodal models. Our study focuses on concreteness, a
well-researched lexical-semantic variable, using it as a case study to examine
the variability in visual representations. We rely on images associated with
approximately 1,000 abstract and concrete concepts extracted from two different
datasets: Bing and YFCC. Our goals are: (i) evaluate whether visual diversity
in the depiction of concepts can reliably distinguish between concrete and
abstract concepts; (ii) analyze the variability of visual features across
multiple images of the same concept through a nearest neighbor analysis; and
(iii) identify challenging factors contributing to this variability by
categorizing and annotating images. Our findings indicate that for classifying
images of abstract versus concrete concepts, a combination of basic visual
features such as color and texture is more effective than features extracted by
more complex models like Vision Transformer (ViT). However, ViTs show better
performances in the nearest neighbor analysis, emphasizing the need for a
careful selection of visual features when analyzing conceptual variables
through modalities other than text.

摘要：概念的視覺表示會根據其意義和發生的背景而有顯著差異；這對視覺和多模態模型來說都提出了多重挑戰。我們的研究重點在於具體性，這是一個研究完善的詞彙語義變數，我們使用它作為案例研究來檢視視覺表示中的變異性。我們依賴與從兩個不同的資料集：Bing 和 YFCC 中提取的大約 1,000 個抽象和具體概念相關的影像。我們的目標是：(i) 評估概念描繪中的視覺多樣性是否能可靠地區分具體和抽象概念；(ii) 透過最近鄰分析，分析同一概念多個影像中視覺特徵的變異性；以及 (iii) 透過對影像進行分類和註解，找出導致這種變異性的挑戰因素。我們的研究結果指出，對於對抽象概念與具體概念的影像進行分類，基本視覺特徵（例如顏色和紋理）的組合比由更複雜的模型（例如 Vision Transformer (ViT)）提取的特徵更有效。然而，ViT 在最近鄰分析中表現得更好，強調在透過文字以外的方式分析概念變數時，需要仔細選擇視覺特徵。

##### **Retrieval Augmented Spelling Correction for E-Commerce Applications**
2410.11655v1 by Xuan Guo, Rohit Patki, Dante Everaert, Christopher Potts

The rapid introduction of new brand names into everyday language poses a
unique challenge for e-commerce spelling correction services, which must
distinguish genuine misspellings from novel brand names that use unconventional
spelling. We seek to address this challenge via Retrieval Augmented Generation
(RAG). On this approach, product names are retrieved from a catalog and
incorporated into the context used by a large language model (LLM) that has
been fine-tuned to do contextual spelling correction. Through quantitative
evaluation and qualitative error analyses, we find improvements in spelling
correction utilizing the RAG framework beyond a stand-alone LLM. We also
demonstrate the value of additional finetuning of the LLM to incorporate
retrieved context.

摘要：在日常語言中快速引入新品牌名稱對電子商務拼寫校正服務構成一項獨特挑戰，該服務必須區分真正的拼寫錯誤和使用非常規拼寫的新品牌名稱。我們尋求透過檢索增強生成 (RAG) 來解決此挑戰。在此方法中，產品名稱會從目錄中檢索，並併入大型語言模型 (LLM) 所使用的語境中，該模型已微調為執行語境拼寫校正。透過量化評估和質化錯誤分析，我們發現使用 RAG 架構進行拼寫校正的改進優於獨立的 LLM。我們還展示了針對 LLM 進行額外微調以納入檢索語境的價值。

##### **Transformer Layer Injection: A Novel Approach for Efficient Upscaling of Large Language Models**
2410.11654v1 by James Vo

In this paper, we propose Transformer Layer Injection (TLI), a novel method
for efficiently upscaling large language models (LLMs) while minimizing
computational costs and maintaining model performance. Model scale is a key
factor in enhancing the quality of machine learning models, and TLI addresses
the challenge of scaling by reducing initial loss, minimizing fine-tuning
requirements, and preserving model complexity. Our approach improves upon the
conventional Depth Up-Scaling (DUS) technique by injecting new layers into
every set of K layers, enabling hidden representations to pass through
transformer blocks with minimal disruption. We compare TLI with existing
approaches, including Mixture of Experts (MoE) and DUS, and validate its
efficiency through experiments on small LLMs (LLama3 1B, 3B, and 8B). Results
show that TLI achieves better initialization, requires fewer training steps,
and delivers superior accuracy on tasks such as KoBEST and KMCQA, with models
performing effectively even without additional training. TLI is demonstrated to
be both data-efficient and cost-effective, significantly outperforming existing
methods. Its scalability and simplicity make it a promising solution for
upscaling transformer-based models, with potential applications in scaling
models from 10B to 405B parameters.

摘要：<paragraph>在本文中，我們提出 Transformer Layer Injection (TLI)，這是一種創新的方法，可用於有效提升大型語言模型 (LLM)，同時將運算成本降到最低，並維持模型效能。模型規模是提升機器學習模型品質的關鍵因素，而 TLI 透過減少初始損失、將微調需求降到最低，以及保留模型複雜度，來解決擴充的挑戰。我們的做法透過在每一組 K 層中注入新層，進而改善傳統深度擴充 (DUS) 技術，讓隱藏式表示能夠以最小的中斷通過 Transformer 區塊。我們將 TLI 與現有方法（包括 Mixture of Experts (MoE) 和 DUS）進行比較，並透過針對小型 LLM（LLama3 1B、3B 和 8B）進行的實驗驗證其效率。結果顯示，TLI 可達成更好的初始化，所需的訓練步驟較少，且在 KoBEST 和 KMCQA 等任務上提供優異的準確度，即使在沒有額外訓練的情況下，模型也能有效執行。TLI 已被證明資料效率和成本效益兼具，大幅優於現有方法。其可擴充性和簡便性使其成為擴充基於 Transformer 的模型的絕佳解決方案，在將模型從 10B 擴充到 405B 參數方面具有潛在應用。</paragraph>

##### **ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge Devices**
2410.11650v1 by Xiang Liu, Yijun Song, Xia Li, Yifei Sun, Huiying Lan, Zemin Liu, Linshan Jiang, Jialin Li

Deep learning models are increasingly deployed on resource-constrained edge
devices for real-time data analytics. In recent years, Vision Transformer
models and their variants have demonstrated outstanding performance across
various computer vision tasks. However, their high computational demands and
inference latency pose significant challenges for model deployment on
resource-constraint edge devices. To address this issue, we propose a novel
Vision Transformer splitting framework, ED-ViT, designed to execute complex
models across multiple edge devices efficiently. Specifically, we partition
Vision Transformer models into several sub-models, where each sub-model is
tailored to handle a specific subset of data classes. To further minimize
computation overhead and inference latency, we introduce a class-wise pruning
technique that reduces the size of each sub-model. We conduct extensive
experiments on five datasets with three model structures, demonstrating that
our approach significantly reduces inference latency on edge devices and
achieves a model size reduction of up to 28.9 times and 34.1 times,
respectively, while maintaining test accuracy comparable to the original Vision
Transformer. Additionally, we compare ED-ViT with two state-of-the-art methods
that deploy CNN and SNN models on edge devices, evaluating accuracy, inference
time, and overall model size. Our comprehensive evaluation underscores the
effectiveness of the proposed ED-ViT framework.

摘要：深度學習模型越來越多地部署在資源受限的邊緣設備上，用於執行即時數據分析。近年來，視覺轉換器模型及其變體已在各種電腦視覺任務中展現出傑出的效能。然而，它們的高計算需求和推論延遲對資源受限的邊緣設備上的模型部署構成重大挑戰。為了解決這個問題，我們提出了一個創新的視覺轉換器分割框架 ED-ViT，旨在有效地在多個邊緣設備上執行複雜的模型。具體來說，我們將視覺轉換器模型分割成幾個子模型，其中每個子模型都經過調整，以處理特定數據類別的子集。為了進一步最小化計算開銷和推論延遲，我們引入了一種類別級別的剪枝技術，以縮小每個子模型的大小。我們對五個數據集和三種模型結構進行了廣泛的實驗，證明我們的做法顯著降低了邊緣設備上的推論延遲，並分別實現了高達 28.9 倍和 34.1 倍的模型大小縮減，同時維持與原始視覺轉換器相當的測試準確度。此外，我們將 ED-ViT 與兩種在邊緣設備上部署 CNN 和 SNN 模型的最新方法進行比較，評估準確度、推論時間和整體模型大小。我們全面的評估強調了所提出的 ED-ViT 框架的有效性。

##### **Measuring Spiritual Values and Bias of Large Language Models**
2410.11647v1 by Songyuan Liu, Ziyang Zhang, Runze Yan, Wei Wu, Carl Yang, Jiaying Lu

Large language models (LLMs) have become integral tool for users from various
backgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural
nuances embedded in their pre-training data. However, the values and
perspectives inherent in this data can influence the behavior of LLMs, leading
to potential biases. As a result, the use of LLMs in contexts involving
spiritual or moral values necessitates careful consideration of these
underlying biases. Our work starts with verification of our hypothesis by
testing the spiritual values of popular LLMs. Experimental results show that
LLMs' spiritual values are quite diverse, as opposed to the stereotype of
atheists or secularists. We then investigate how different spiritual values
affect LLMs in social-fairness scenarios e.g., hate speech identification). Our
findings reveal that different spiritual values indeed lead to different
sensitivity to different hate target groups. Furthermore, we propose to
continue pre-training LLMs on spiritual texts, and empirical results
demonstrate the effectiveness of this approach in mitigating spiritual bias.

摘要：大型語言模型 (LLM) 已成為來自各個背景的使用者不可或缺的工具。LLM 在龐大的語料庫上受訓，反映了其預訓練資料中嵌入的語言和文化差異。然而，此資料中固有的價值觀和觀點可能會影響 LLM 的行為，導致潛在的偏見。因此，在涉及精神或道德價值觀的脈絡中使用 LLM，必須仔細考量這些潛在的偏見。我們的研究從驗證我們的假設開始，測試流行 LLM 的精神價值觀。實驗結果顯示，與無神論者或世俗主義者的刻板印象相反，LLM 的精神價值觀相當多元。然後我們探討不同的精神價值觀如何影響 LLM 在社會公平情境中的表現（例如仇恨言論識別）。我們的研究結果揭示，不同的精神價值觀確實會導致對不同仇恨目標群體產生不同的敏感度。此外，我們建議持續在精神文本上預訓練 LLM，而經驗結果證明此方法在減輕精神偏見方面有效。

##### **Tokenization and Morphology in Multilingual Language Models: A~Comparative Analysis of mT5 and ByT5**
2410.11627v1 by Thao Anh Dang, Limor Raviv, Lukas Galke

Morphology is a crucial factor for multilingual language modeling as it poses
direct challenges for tokenization. Here, we seek to understand how
tokenization influences the morphological knowledge encoded in multilingual
language models. Specifically, we capture the impact of tokenization by
contrasting two multilingual language models: mT5 and ByT5. The two models
share the same architecture, training objective, and training data and only
differ in their tokenization strategies: subword tokenization vs.
character-level tokenization. Probing the morphological knowledge encoded in
these models on four tasks and 17 languages, our analyses show that
multilingual language models learn the morphological systems of some languages
better than others despite similar average performance and that morphological
information is encoded in the middle and late layers, where characted-based
models need a few more layers to yield commensurate probing accuracy. Finally,
we show that languages with more irregularities benefit more from having a
higher share of the pre-training data.

摘要：形態學是多語言語言建模的關鍵因素，因為它對標記化構成直接挑戰。在此，我們尋求了解標記化如何影響編碼在多語言語言模型中的形態知識。具體來說，我們透過對比兩個多語言語言模型：mT5 和 ByT5 來捕捉標記化的影響。這兩個模型共享相同的架構、訓練目標和訓練資料，且僅在其標記化策略上有所不同：子字標記化與字元級標記化。在四個任務和 17 種語言中探查編碼在這些模型中的形態知識，我們的分析顯示，儘管平均表現相似，多語言語言模型學習某些語言的形態系統優於其他語言，而且形態資訊編碼在中間和後層，其中基於字元的模型需要多一些層才能產生相應的探查準確度。最後，我們顯示具有更多不規則性的語言從擁有較高比例的預訓練資料中受益更多。

##### **Findings of the WMT 2024 Shared Task on Chat Translation**
2410.11624v1 by Wafaa Mohammed, Sweta Agrawal, M. Amin Farajian, Vera Cabarrão, Bryan Eikema, Ana C. Farinha, José G. C. de Souza

This paper presents the findings from the third edition of the Chat
Translation Shared Task. As with previous editions, the task involved
translating bilingual customer support conversations, specifically focusing on
the impact of conversation context in translation quality and evaluation. We
also include two new language pairs: English-Korean and English-Dutch, in
addition to the set of language pairs from previous editions: English-German,
English-French, and English-Brazilian Portuguese. We received 22 primary
submissions and 32 contrastive submissions from eight teams, with each language
pair having participation from at least three teams. We evaluated the systems
comprehensively using both automatic metrics and human judgments via a direct
assessment framework. The official rankings for each language pair were
determined based on human evaluation scores, considering performance in both
translation directions--agent and customer. Our analysis shows that while the
systems excelled at translating individual turns, there is room for improvement
in overall conversation-level translation quality.

摘要：本論文提出聊天翻譯共享任務第三版的發現。與前幾版一樣，這項任務包含翻譯雙語客戶支援對話，特別著重於對話脈絡對翻譯品質和評估的影響。除了前幾版中的語言對：英語-德語、英語-法語和英語-巴西葡萄牙語之外，我們還納入了兩個新的語言對：英語-韓語和英語-荷蘭語。我們收到了八個團隊提交的 22 份主要提交和 32 份對比提交，每個語言對至少有三個團隊參與。我們使用自動化指標和透過直接評估架構進行的人工判斷，全面評估這些系統。每個語言對的官方排名是根據人工評分來決定的，考量了代理和客戶這兩個翻譯方向的表現。我們的分析顯示，雖然這些系統在翻譯個別輪次方面表現優異，但整體對話層級的翻譯品質仍有進步空間。

##### **VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI**
2410.11623v1 by Sijie Cheng, Kechen Fang, Yangyang Yu, Sicheng Zhou, Bohao Li, Ye Tian, Tingguang Li, Lei Han, Yang Liu

Recent advancements in Multi-modal Large Language Models (MLLMs) have opened
new avenues for applications in Embodied AI. Building on previous work,
EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating
egocentric video understanding capabilities. To bridge the gap between MLLMs
and low-level control in Embodied AI, we design four key interrelated tasks:
video question-answering, hierarchy planning, visual grounding and reward
modeling. To minimize manual annotation costs, we develop an automatic data
generation pipeline based on the Ego4D dataset, leveraging the prior knowledge
and multimodal capabilities of GPT-4o. Three human annotators then filter the
generated data to ensure diversity and quality, resulting in the VidEgoThink
benchmark. We conduct extensive experiments with three types of models:
API-based MLLMs, open-source image-based MLLMs, and open-source video-based
MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform
poorly across all tasks related to egocentric video understanding. These
findings suggest that foundation models still require significant advancements
to be effectively applied to first-person scenarios in Embodied AI. In
conclusion, VidEgoThink reflects a research trend towards employing MLLMs for
egocentric vision, akin to human capabilities, enabling active observation and
interaction in the complex real-world environments.

摘要：多模态大型语言模型 (MLLM) 的最新进展为具身人工智能 (Embodied AI) 中的应用开辟了新途径。在先前工作的基础上，EgoThink，我们引入了 VidEgoThink，这是一个用于评估以自我为中心的视频理解能力的综合基准。为了弥合 MLLM 和具身人工智能中的低级控制之间的差距，我们设计了四个关键的相互关联的任务：视频问答、层次规划、视觉基础和奖励建模。为了最大程度地减少人工标注成本，我们基于 Ego4D 数据集开发了一个自动数据生成管道，利用 GPT-4o 的先验知识和多模态能力。然后，三位人类注释员对生成的数据进行筛选，以确保多样性和质量，从而形成 VidEgoThink 基准。我们对三种类型的模型进行了广泛的实验：基于 API 的 MLLM、开源基于图像的 MLLM 和开源基于视频的 MLLM。实验结果表明，包括 GPT-4o 在内的所有 MLLM 在与以自我为中心的视频理解相关的所有任务中的表现都很差。这些发现表明，基础模型仍然需要显着的进步，才能有效地应用于具身人工智能中的第一人称场景。总之，VidEgoThink 反映了一种研究趋势，即利用 MLLM 进行以自我为中心的视觉，类似于人类能力，从而能够在复杂现实世界环境中进行主动观察和交互。

##### **MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval**
2410.11619v1 by Reno Kriz, Kate Sanders, David Etter, Kenton Murray, Cameron Carpenter, Kelly Van Ochten, Hannah Recknor, Jimena Guallar-Blasco, Alexander Martin, Ronald Colaianni, Nolan King, Eugene Yang, Benjamin Van Durme

Efficiently retrieving and synthesizing information from large-scale
multimodal collections has become a critical challenge. However, existing video
retrieval datasets suffer from scope limitations, primarily focusing on
matching descriptive but vague queries with small collections of professionally
edited, English-centric videos. To address this gap, we introduce
$\textbf{MultiVENT 2.0}$, a large-scale, multilingual event-centric video
retrieval benchmark featuring a collection of more than 218,000 news videos and
3,906 queries targeting specific world events. These queries specifically
target information found in the visual content, audio, embedded text, and text
metadata of the videos, requiring systems leverage all these sources to succeed
at the task. Preliminary results show that state-of-the-art vision-language
models struggle significantly with this task, and while alternative approaches
show promise, they are still insufficient to adequately address this problem.
These findings underscore the need for more robust multimodal retrieval
systems, as effective video retrieval is a crucial step towards multimodal
content understanding and generation tasks.

摘要：有效地從大型多模態集合中擷取和綜合資訊已成為一項重大的挑戰。然而，現有的影片擷取資料集受限於範圍，主要著重於將描述性但模糊的查詢與少數經過專業編輯、以英語為中心的影片集合相匹配。為了解決此差距，我們引入了 $\textbf{MultiVENT 2.0}$，這是一個大型、多語言的以事件為中心的影片擷取基準，其特點是收集了超過 218,000 個新聞影片和 3,906 個針對特定世界事件的查詢。這些查詢特別針對影片中的視覺內容、音訊、嵌入式文字和文字元資料中找到的資訊，要求系統利用所有這些來源才能成功執行此任務。初步結果顯示，最先進的視覺語言模型在執行此任務時會遇到很大的困難，而雖然其他方法顯示出潛力，但仍不足以充分解決此問題。這些發現強調了對更強大的多模態擷取系統的需求，因為有效的影片擷取是邁向多模態內容理解和生成任務的關鍵步驟。

##### **Black-box Uncertainty Quantification Method for LLM-as-a-Judge**
2410.11594v1 by Nico Wagner, Michael Desmond, Rahul Nair, Zahra Ashktorab, Elizabeth M. Daly, Qian Pan, Martín Santillán Cooper, James M. Johnson, Werner Geyer

LLM-as-a-Judge is a widely used method for evaluating the performance of
Large Language Models (LLMs) across various tasks. We address the challenge of
quantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty
quantification has been well-studied in other domains, applying it effectively
to LLMs poses unique challenges due to their complex decision-making
capabilities and computational demands. In this paper, we introduce a novel
method for quantifying uncertainty designed to enhance the trustworthiness of
LLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the
relationships between generated assessments and possible ratings. By
cross-evaluating these relationships and constructing a confusion matrix based
on token probabilities, the method derives labels of high or low uncertainty.
We evaluate our method across multiple benchmarks, demonstrating a strong
correlation between the accuracy of LLM evaluations and the derived uncertainty
scores. Our findings suggest that this method can significantly improve the
reliability and consistency of LLM-as-a-Judge evaluations.

摘要：LLM-as-a-Judge 是一種廣泛使用的方法，用於評估大型語言模型 (LLM) 在各種任務中的表現。我們解決了量化 LLM-as-a-Judge 評估不確定性的挑戰。雖然不確定性量化在其他領域已獲得深入研究，但由於 LLM 複雜的決策制定能力和運算需求，將其有效應用於 LLM 會帶來獨特的挑戰。在本文中，我們介紹了一種量化不確定性的新方法，旨在增強 LLM-as-a-Judge 評估的可信度。該方法通過分析生成的評估與可能的評分之間的關係來量化不確定性。通過交叉評估這些關係並根據權杖機率構建混淆矩陣，該方法得出高或低不確定性的標籤。我們在多個基準測試中評估了我們的方法，證明了 LLM 評估的準確性與導出的不確定性評分之間存在很強的相關性。我們的研究結果表明，這種方法可以顯著提高 LLM-as-a-Judge 評估的可靠性和一致性。

##### **Causal Reasoning in Large Language Models: A Knowledge Graph Approach**
2410.11588v1 by Yejin Kim, Eojin Kang, Juae Kim, H. Howie Huang

Large language models (LLMs) typically improve performance by either
retrieving semantically similar information, or enhancing reasoning abilities
through structured prompts like chain-of-thought. While both strategies are
considered crucial, it remains unclear which has a greater impact on model
performance or whether a combination of both is necessary. This paper answers
this question by proposing a knowledge graph (KG)-based random-walk reasoning
approach that leverages causal relationships. We conduct experiments on the
commonsense question answering task that is based on a KG. The KG inherently
provides both relevant information, such as related entity keywords, and a
reasoning structure through the connections between nodes. Experimental results
show that the proposed KG-based random-walk reasoning method improves the
reasoning ability and performance of LLMs. Interestingly, incorporating three
seemingly irrelevant sentences into the query using KG-based random-walk
reasoning enhances LLM performance, contrary to conventional wisdom. These
findings suggest that integrating causal structures into prompts can
significantly improve reasoning capabilities, providing new insights into the
role of causality in optimizing LLM performance.

摘要：大型語言模型 (LLM) 通常透過擷取語意上相似的資訊，或透過鏈式思考等結構化提示增強推理能力，來提升效能。儘管這兩種策略都被認為至關重要，但目前仍不清楚哪一種對模型效能影響較大，或是否需要結合兩者。本文透過提出一個基於知識圖譜 (KG) 的隨機漫步推理方法，來回答這個問題，這個方法利用了因果關係。我們在基於 KG 的常識問答任務上進行實驗。KG 本身就提供了相關資訊，例如相關實體關鍵字，以及透過節點之間的連結提供的推理結構。實驗結果顯示，提出的基於 KG 的隨機漫步推理方法改善了 LLM 的推理能力和效能。有趣的是，與傳統觀念相反，使用基於 KG 的隨機漫步推理將三個看似無關的句子納入查詢中，可以提升 LLM 的效能。這些發現表明，將因果結構整合到提示中可以顯著提升推理能力，並為因果關係在最佳化 LLM 效能中所扮演的角色提供新的見解。

##### **DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment**
2410.11584v1 by Wendi Chen, Han Xue, Fangyuan Zhou, Yuan Fang, Cewu Lu

In recent years, imitation learning has made progress in the field of robotic
manipulation. However, it still faces challenges when dealing with complex
long-horizon deformable object tasks, such as high-dimensional state spaces,
complex dynamics, and multimodal action distributions. Traditional imitation
learning methods often require a large amount of data and encounter
distributional shifts and accumulative errors in these tasks. To address these
issues, we propose a data-efficient general learning framework (DeformPAM)
based on preference learning and reward-guided action selection. DeformPAM
decomposes long-horizon tasks into multiple action primitives, utilizes 3D
point cloud inputs and diffusion models to model action distributions, and
trains an implicit reward model using human preference data. During the
inference phase, the reward model scores multiple candidate actions, selecting
the optimal action for execution, thereby reducing the occurrence of anomalous
actions and improving task completion quality. Experiments conducted on three
challenging real-world long-horizon deformable object manipulation tasks
demonstrate the effectiveness of this method. Results show that DeformPAM
improves both task completion quality and efficiency compared to baseline
methods even with limited data. Code and data will be available at
https://deform-pam.robotflow.ai.

摘要：近年来，模仿学习在机器人操作领域取得了进展。然而，在处理复杂的长期可变形对象任务（例如高维状态空间、复杂动态和多峰动作分布）时，它仍然面临挑战。传统的模仿学习方法通常需要大量数据，并在这些任务中遇到分布式转移和累积误差。为了解决这些问题，我们提出了一种基于偏好学习和奖励引导动作选择的、数据高效的通用学习框架（DeformPAM）。DeformPAM 将长期任务分解为多个动作基元，利用 3D 点云输入和扩散模型对动作分布进行建模，并使用人类偏好数据训练隐式奖励模型。在推理阶段，奖励模型对多个候选动作进行评分，选择最佳动作执行，从而减少异常动作的发生并提高任务完成质量。在三个具有挑战性的现实世界长期可变形对象操作任务上进行的实验证明了该方法的有效性。结果表明，即使数据有限，DeformPAM 也比基线方法提高了任务完成质量和效率。代码和数据将可在 https://deform-pam.robotflow.ai/ 获得。

##### **On-the-fly Modulation for Balanced Multimodal Learning**
2410.11582v1 by Yake Wei, Di Hu, Henghui Du, Ji-Rong Wen

Multimodal learning is expected to boost model performance by integrating
information from different modalities. However, its potential is not fully
exploited because the widely-used joint training strategy, which has a uniform
objective for all modalities, leads to imbalanced and under-optimized uni-modal
representations. Specifically, we point out that there often exists modality
with more discriminative information, e.g., vision of playing football and
sound of blowing wind. They could dominate the joint training process,
resulting in other modalities being significantly under-optimized. To alleviate
this problem, we first analyze the under-optimized phenomenon from both the
feed-forward and the back-propagation stages during optimization. Then,
On-the-fly Prediction Modulation (OPM) and On-the-fly Gradient Modulation (OGM)
strategies are proposed to modulate the optimization of each modality, by
monitoring the discriminative discrepancy between modalities during training.
Concretely, OPM weakens the influence of the dominant modality by dropping its
feature with dynamical probability in the feed-forward stage, while OGM
mitigates its gradient in the back-propagation stage. In experiments, our
methods demonstrate considerable improvement across a variety of multimodal
tasks. These simple yet effective strategies not only enhance performance in
vanilla and task-oriented multimodal models, but also in more complex
multimodal tasks, showcasing their effectiveness and flexibility. The source
code is available at \url{https://github.com/GeWu-Lab/BML_TPAMI2024}.

摘要：<paragraph>多模態學習預期透過整合來自不同模態的資訊來提升模型表現。然而，它的潛力並未被充分發揮，原因在於廣泛使用的聯合訓練策略，它對所有模態都有一個統一的目標，導致不平衡且優化不足的單模態表示。具體來說，我們指出通常存在具有更多區分資訊的模態，例如播放足球的視覺效果和吹風的聲音。它們可能主導聯合訓練過程，導致其他模態顯著優化不足。為了緩解這個問題，我們首先從優化過程中的前饋和反向傳播階段分析了優化不足的現象。然後，提出即時預測調製 (OPM) 和即時梯度調製 (OGM) 策略，透過監控訓練期間模態之間的區分差異，來調製每個模態的優化。具體來說，OPM 透過在前饋階段以動態機率捨棄其特徵，來削弱主導模態的影響，而 OGM 則在反向傳播階段減輕其梯度。在實驗中，我們的這些方法在各種多模態任務中都展示出顯著的進步。這些既簡單又有效的方法不僅能提升香草和任務導向多模態模型的表現，也能提升更複雜的多模態任務的表現，展現出它們的有效性和靈活性。原始碼可於 \url{https://github.com/GeWu-Lab/BML_TPAMI2024} 取得。</paragraph>

##### **Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for Drug Development**
2410.11550v1 by Tengfei Ma, Xuan Lin, Tianle Li, Chaoyi Li, Long Chen, Peng Zhou, Xibao Cai, Xinyu Yang, Daojian Zeng, Dongsheng Cao, Xiangxiang Zeng

Large Language Models (LLMs) have recently demonstrated remarkable
performance in general tasks across various fields. However, their
effectiveness within specific domains such as drug development remains
challenges. To solve these challenges, we introduce \textbf{Y-Mol}, forming a
well-established LLM paradigm for the flow of drug development. Y-Mol is a
multiscale biomedical knowledge-guided LLM designed to accomplish tasks across
lead compound discovery, pre-clinic, and clinic prediction. By integrating
millions of multiscale biomedical knowledge and using LLaMA2 as the base LLM,
Y-Mol augments the reasoning capability in the biomedical domain by learning
from a corpus of publications, knowledge graphs, and expert-designed synthetic
data. The capability is further enriched with three types of drug-oriented
instructions: description-based prompts from processed publications,
semantic-based prompts for extracting associations from knowledge graphs, and
template-based prompts for understanding expert knowledge from biomedical
tools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously
execute the downstream tasks across the entire process of drug development,
including virtual screening, drug design, pharmacological properties
prediction, and drug-related interaction prediction. Our extensive evaluations
of various biomedical sources demonstrate that Y-Mol significantly outperforms
general-purpose LLMs in discovering lead compounds, predicting molecular
properties, and identifying drug interaction events.

摘要：大型語言模型 (LLM) 近期在各個領域的通用任務中展示出顯著的表現。然而，它們在特定領域（例如藥物開發）中的效能仍有待加強。為了解決這些挑戰，我們引入了 **Y-Mol**，形成了一個完善的 LLM 典範，用於藥物開發流程。Y-Mol 是一個多尺度的生物醫學知識引導 LLM，旨在完成先導化合物發現、臨床前和臨床預測等任務。透過整合數百萬個多尺度的生物醫學知識，並使用 LLaMA2 作為基礎 LLM，Y-Mol 從出版物、知識圖譜和專家設計的合成資料中學習，增強了生物醫學領域的推理能力。其能力進一步透過三種類型的藥物導向指令得到豐富：已處理出版物的基於描述的提示、用於從知識圖譜中提取關聯的基於語義的提示，以及用於理解生物醫學工具中專家知識的基於範本的提示。此外，Y-Mol 提供了一組 LLM 典範，可以在整個藥物開發過程中自主執行下游任務，包括虛擬篩選、藥物設計、藥理特性預測和藥物相關交互預測。我們對各種生物醫學來源的廣泛評估表明，Y-Mol 在發現先導化合物、預測分子特性和識別藥物交互事件方面顯著優於通用 LLM。

##### **Multi-round jailbreak attack on large language models**
2410.11533v1 by Yihua Zhou, Xiaochuan Shi

Ensuring the safety and alignment of large language models (LLMs) with human
values is crucial for generating responses that are beneficial to humanity.
While LLMs have the capability to identify and avoid harmful queries, they
remain vulnerable to "jailbreak" attacks, where carefully crafted prompts can
induce the generation of toxic content. Traditional single-round jailbreak
attacks, such as GCG and AutoDAN, do not alter the sensitive words in the
dangerous prompts. Although they can temporarily bypass the model's safeguards
through prompt engineering, their success rate drops significantly as the LLM
is further fine-tuned, and they cannot effectively circumvent static rule-based
filters that remove the hazardous vocabulary.
  In this study, to better understand jailbreak attacks, we introduce a
multi-round jailbreak approach. This method can rewrite the dangerous prompts,
decomposing them into a series of less harmful sub-questions to bypass the
LLM's safety checks. We first use the LLM to perform a decomposition task,
breaking down a set of natural language questions into a sequence of
progressive sub-questions, which are then used to fine-tune the Llama3-8B
model, enabling it to decompose hazardous prompts. The fine-tuned model is then
used to break down the problematic prompt, and the resulting sub-questions are
sequentially asked to the victim model. If the victim model rejects a
sub-question, a new decomposition is generated, and the process is repeated
until the final objective is achieved. Our experimental results show a 94\%
success rate on the llama2-7B and demonstrate the effectiveness of this
approach in circumventing static rule-based filters.

摘要：確保大型語言模型 (LLM) 的安全性和與人類價值觀的一致性對於產生對人類有益的回應至關重要。雖然 LLM 有能力識別和避免有害查詢，但它們仍然容易受到「越獄」攻擊，精心製作的提示可能會誘發有毒內容的產生。傳統的單輪越獄攻擊，例如 GCG 和 AutoDAN，不會改變危險提示中的敏感詞。儘管他們可以通過提示工程暫時繞過模型的防護措施，但隨著 LLM 進一步微調，他們的成功率會顯著下降，而且他們無法有效規避刪除危險詞彙的靜態基於規則的過濾器。在本次研究中，為了更好地理解越獄攻擊，我們引入了一種多輪越獄方法。這種方法可以改寫危險提示，將它們分解成一系列危害較小的子問題，以繞過 LLM 的安全檢查。我們首先使用 LLM 執行分解任務，將一組自然語言問題分解成一系列漸進的子問題，然後用於微調 Llama3-8B 模型，使其能夠分解危險提示。然後使用微調後的模型來分解有問題的提示，並將產生的子問題按順序詢問受害者模型。如果受害者模型拒絕一個子問題，則會生成一個新的分解，並且重複這個過程，直到最終目標達成。我們的實驗結果顯示在 llama2-7B 上的成功率為 94%，並證明了這種方法在規避靜態基於規則的過濾器方面的有效性。

##### **AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data**
2410.11531v1 by Xinjie Zhao, Moritz Blum, Rui Yang, Boming Yang, Luis Márquez Carpintero, Mónica Pina-Navarro, Tony Wang, Xin Li, Huitao Li, Yanran Fu, Rongrong Wang, Juntao Zhang, Irene Li

Large Language Models~(LLMs) have demonstrated capabilities across various
applications but face challenges such as hallucination, limited reasoning
abilities, and factual inconsistencies, especially when tackling complex,
domain-specific tasks like question answering~(QA). While Knowledge
Graphs~(KGs) have been shown to help mitigate these issues, research on the
integration of LLMs with background KGs remains limited. In particular, user
accessibility and the flexibility of the underlying KG have not been thoroughly
explored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based
Interaction and Graphical Representation), a platform for knowledge management
through natural language interaction. It integrates knowledge extraction,
integration, and real-time visualization. AGENTiGraph employs a multi-agent
architecture to dynamically interpret user intents, manage tasks, and integrate
new knowledge, ensuring adaptability to evolving user requirements and data
contexts. Our approach demonstrates superior performance in knowledge graph
interactions, particularly for complex domain-specific tasks. Experimental
results on a dataset of 3,500 test cases show AGENTiGraph significantly
outperforms state-of-the-art zero-shot baselines, achieving 95.12\% accuracy in
task classification and 90.45\% success rate in task execution. User studies
corroborate its effectiveness in real-world scenarios. To showcase versatility,
we extended AGENTiGraph to legislation and healthcare domains, constructing
specialized KGs capable of answering complex queries in legal and medical
contexts.

摘要：大型語言模型 (LLM) 已在各種應用中展現其能力，但仍面臨幻覺、推理能力有限和事實不一致等挑戰，尤其是在處理複雜的特定領域任務，例如問答 (QA) 時。雖然知識圖譜 (KG) 已被證明有助於緩解這些問題，但 LLM 與背景 KG 整合的研究仍然有限。特別是，使用者的可及性和底層 KG 的靈活性尚未得到徹底探討。我們引入了 AGENTiGraph（用於任務型互動和圖形表示的自適應生成引擎），一個透過自然語言互動進行知識管理的平台。它整合了知識萃取、整合和即時視覺化。AGENTiGraph 採用多代理架構，以動態解讀使用者的意圖、管理任務並整合新知識，確保適應不斷變化的使用者需求和資料脈絡。我們的做法在知識圖譜互動中展現出優異的效能，特別是對於複雜的特定領域任務。在 3,500 個測試案例的資料集上進行的實驗結果顯示，AGENTiGraph 明顯優於最先進的零次學習基準，在任務分類中達到 95.12% 的準確度，在任務執行中達到 90.45% 的成功率。使用者研究證實了它在真實世界場景中的有效性。為了展示其多功能性，我們將 AGENTiGraph 延伸到法律和醫療保健領域，建構了能夠回答法律和醫療脈絡中複雜查詢的專業知識圖譜。

##### **Human-LLM Collaborative Construction of a Cantonese Emotion Lexicon**
2410.11526v1 by Yusong Zhang, Dong Dong, Chi-tim Hung, Leonard Heyerdahl, Tamara Giles-Vernick, Eng-kiong Yeoh

Large Language Models (LLMs) have demonstrated remarkable capabilities in
language understanding and generation. Advanced utilization of the knowledge
embedded in LLMs for automated annotation has consistently been explored. This
study proposed to develop an emotion lexicon for Cantonese, a low-resource
language, through collaborative efforts between LLM and human annotators. By
integrating emotion labels provided by LLM and human annotators, the study
leveraged existing linguistic resources including lexicons in other languages
and local forums to construct a Cantonese emotion lexicon enriched with
colloquial expressions. The consistency of the proposed emotion lexicon in
emotion extraction was assessed through modification and utilization of three
distinct emotion text datasets. This study not only validates the efficacy of
the constructed lexicon but also emphasizes that collaborative annotation
between human and artificial intelligence can significantly enhance the quality
of emotion labels, highlighting the potential of such partnerships in
facilitating natural language processing tasks for low-resource languages.

摘要：大型語言模型 (LLM) 在語言理解和生成方面展現了卓越的能力。持續探索如何進階利用 LLM 中嵌入的知識進行自動化註解。本研究提議透過 LLM 和人類註解員的協作，為廣東話（一種低資源語言）開發一個情感詞彙。透過整合 LLM 和人類註解員提供的語意標籤，本研究利用現有的語言資源（包括其他語言的詞彙和本地論壇）來建構一個豐富的廣東話情感詞彙，其中包含了口語表達。透過修改和使用三個不同的情感文本資料集，評估了所提出的情感詞彙在情感抽取中的一致性。本研究不僅驗證了所建構詞彙的效能，也強調了人類與人工智慧之間的協作註解可以顯著提升情感標籤的品質，突顯了此類夥伴關係在促進低資源語言的自然語言處理任務方面的潛力。

##### **TopoLM: brain-like spatio-functional organization in a topographic language model**
2410.11516v1 by Neil Rathi, Johannes Mehrer, Badr AlKhamissi, Taha Binhuraib, Nicholas M. Blauch, Martin Schrimpf

Neurons in the brain are spatially organized such that neighbors on tissue
often exhibit similar response profiles. In the human language system,
experimental studies have observed clusters for syntactic and semantic
categories, but the mechanisms underlying this functional organization remain
unclear. Here, building on work from the vision literature, we develop TopoLM,
a transformer language model with an explicit two-dimensional spatial
representation of model units. By combining a next-token prediction objective
with a spatial smoothness loss, representations in this model assemble into
clusters that correspond to semantically interpretable groupings of text and
closely match the functional organization in the brain's language system.
TopoLM successfully predicts the emergence of the spatio-functional
organization of a cortical language system as well as the organization of
functional clusters selective for fine-grained linguistic features empirically
observed in human cortex. Our results suggest that the functional organization
of the human language system is driven by a unified spatial objective, and
provide a functionally and spatially aligned model of language processing in
the brain.

摘要：大腦中的神經元在空間上具有組織性，因此組織中的鄰居通常表現出相似的反應模式。在人類語言系統中，實驗研究已經觀察到句法和語義類別的群集，但這種功能組織背後的機制仍不清楚。在此，我們建立在視覺文獻的工作基礎上，開發了 TopoLM，這是一個具有模型單元明確的二維空間表示形式的Transformer語言模型。通過將下一個標記預測目標與空間平滑損失相結合，此模型中的表示組裝成對應於語義可解釋的文本分組的群集，並且與大腦語言系統中的功能組織密切匹配。TopoLM 成功預測了皮質語言系統的時空功能組織的出現，以及在人類皮質中經驗觀察到的對細粒度語言特徵具有選擇性的功能群集的組織。我們的結果表明，人類語言系統的功能組織是由統一的空間目標驅動的，並在大腦中提供了語言處理的功能和空間對齊模型。

##### **Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMs**
2410.11507v2 by Wanying Wang, Zeyu Ma, Pengfei Liu, Mingang Chen

While various vertical domain large language models (LLMs) have been
developed, the challenge of automatically evaluating their performance across
different domains remains significant. Current benchmark-based evaluation
methods exhibit rigid, aimless interactions and rely on pre-collected static
datasets that are costly to build, inflexible across domains, and misaligned
with practical user needs. To address this issue, we revisit the evaluation
components and introduce two concepts: Benchmark+, which extends traditional
question-answer benchmark into a more flexible "strategy-criterion" format; and
Assessment+, which enhances the interaction process, enabling deeper
exploration and supporting both quantitative metrics and qualitative insights.
These concepts capture the nuanced behaviors of LLMs through richer, multi-turn
interactions. We propose an agent-based evaluation framework called TestAgent,
which implements these concepts through retrieval augmented generation and
reinforcement learning. Experiments on tasks ranging from constructing vertical
domain evaluation to activating existing benchmarks demonstrate the
effectiveness of TestAgent across various scenarios. We believe this work
offers an interesting perspective on automatic evaluation for LLMs.

摘要：儘管各種垂直領域大型語言模型 (LLM) 已經開發出來，但自動評估它們在不同領域的效能仍是一項重大的挑戰。現有的基於基準的評估方法展現出僵化、無目標的互動，且依賴於收集好的靜態資料集，而這些資料集建置成本高昂、無法靈活應用於不同領域，且與實際使用者的需求不符。為了解決這個問題，我們重新檢視評估組成部分，並提出兩個概念：Benchmark+，將傳統問答基準擴充為更靈活的「策略準則」格式；以及 Assessment+，它增強了互動程序，讓探索更深入，並同時支援量化指標和定性見解。這些概念透過更豐富、多輪次的互動，捕捉到 LLM 的細微行為。我們提出一個稱為 TestAgent 的基於代理的評估架構，它透過檢索擴充生成和強化學習來實作這些概念。在從建構垂直領域評估到啟動現有基準等任務上的實驗，都證實了 TestAgent 在各種情境下的效能。我們相信這項研究為 LLM 的自動評估提供了有趣的觀點。

##### **Offline Model-Based Optimization by Learning to Rank**
2410.11502v1 by Rong-Xi Tan, Ke Xue, Shen-Huan Lyu, Haopu Shang, Yao Wang, Yaoyuan Wang, Sheng Fu, Chao Qian

Offline model-based optimization (MBO) aims to identify a design that
maximizes a black-box function using only a fixed, pre-collected dataset of
designs and their corresponding scores. A common approach in offline MBO is to
train a regression-based surrogate model by minimizing mean squared error (MSE)
and then find the best design within this surrogate model by different
optimizers (e.g., gradient ascent). However, a critical challenge is the risk
of out-of-distribution errors, i.e., the surrogate model may typically
overestimate the scores and mislead the optimizers into suboptimal regions.
Prior works have attempted to address this issue in various ways, such as using
regularization techniques and ensemble learning to enhance the robustness of
the model, but it still remains. In this paper, we argue that regression models
trained with MSE are not well-aligned with the primary goal of offline MBO,
which is to select promising designs rather than to predict their scores
precisely. Notably, if a surrogate model can maintain the order of candidate
designs based on their relative score relationships, it can produce the best
designs even without precise predictions. To validate it, we conduct
experiments to compare the relationship between the quality of the final
designs and MSE, finding that the correlation is really very weak. In contrast,
a metric that measures order-maintaining quality shows a significantly stronger
correlation. Based on this observation, we propose learning a ranking-based
model that leverages learning to rank techniques to prioritize promising
designs based on their relative scores. We show that the generalization error
on ranking loss can be well bounded. Empirical results across diverse tasks
demonstrate the superior performance of our proposed ranking-based models than
twenty existing methods.

摘要：<paragraph>離線模型基礎最佳化 (MBO) 的目標是找出一個設計，使用僅有固定預先收集的設計資料集及其對應分數，最大化黑盒函數。離線 MBO 的常見方法是透過最小化均方誤差 (MSE) 來訓練回歸基礎代理模型，然後在這個代理模型中透過不同的最佳化器 (例如，梯度上升) 找出最佳設計。然而，一個重大的挑戰是分佈外誤差的風險，也就是說，代理模型通常會高估分數，並誤導最佳化器進入次佳區域。先前的研究嘗試使用各種方法來解決這個問題，例如使用正則化技術和整體學習來增強模型的穩健性，但問題仍然存在。在本文中，我們主張使用 MSE 訓練的回歸模型與離線 MBO 的主要目標不一致，離線 MBO 的目標是選擇有前景的設計，而不是精確預測其分數。值得注意的是，如果代理模型可以根據候選設計的相對分數關係來維持順序，它甚至可以在沒有精確預測的情況下產生最佳設計。為了驗證這一點，我們進行實驗來比較最終設計的品質和 MSE 之間的關係，發現相關性非常低。相反地，一個衡量順序維持品質的指標顯示出顯著更強的相關性。根據這個觀察，我們建議學習一個排名基礎模型，利用學習排名技術根據相對分數來優先考慮有前景的設計。我們展示了排名損失的泛化誤差可以得到很好的約束。在不同任務中的經驗結果證明了我們提出的排名基礎模型比現有的二十種方法具有更優異的效能。</paragraph>

##### **BSM: Small but Powerful Biological Sequence Model for Genes and Proteins**
2410.11499v1 by Weixi Xiang, Xueting Han, Xiujuan Chai, Jing Bai

Modeling biological sequences such as DNA, RNA, and proteins is crucial for
understanding complex processes like gene regulation and protein synthesis.
However, most current models either focus on a single type or treat multiple
types of data separately, limiting their ability to capture cross-modal
relationships. We propose that by learning the relationships between these
modalities, the model can enhance its understanding of each type. To address
this, we introduce BSM, a small but powerful mixed-modal biological sequence
foundation model, trained on three types of data: RefSeq, Gene Related
Sequences, and interleaved biological sequences from the web. These datasets
capture the genetic flow, gene-protein relationships, and the natural
co-occurrence of diverse biological data, respectively. By training on
mixed-modal data, BSM significantly enhances learning efficiency and
cross-modal representation, outperforming models trained solely on unimodal
data. With only 110M parameters, BSM achieves performance comparable to much
larger models across both single-modal and mixed-modal tasks, and uniquely
demonstrates in-context learning capability for mixed-modal tasks, which is
absent in existing models. Further scaling to 270M parameters demonstrates even
greater performance gains, highlighting the potential of BSM as a significant
advancement in multimodal biological sequence modeling.

摘要：對 DNA、RNA 和蛋白質等生物序列進行建模對於理解基因調控和蛋白質合成等複雜過程至關重要。
然而，大多數當前模型要么專注於單一類型，要么將多種類型的數據分開處理，這限制了它們捕捉跨模態關係的能力。我們提出，通過學習這些模態之間的關係，模型可以增強它對每種類型的理解。為了解決這個問題，我們引入了 BSM，一個小而強大的混合模態生物序列基礎模型，它在三種類型的數據上進行訓練：RefSeq、基因相關序列以及來自網路的交錯生物序列。這些數據集分別捕捉了遺傳流動、基因-蛋白質關係以及不同生物數據的自然共現。通過在混合模態數據上進行訓練，BSM 大大提高了學習效率和跨模態表示，優於僅在單模態數據上訓練的模型。BSM 只有 1.1 億個參數，在單模態和混合模態任務中都達到了與大得多的模型相當的性能，並且獨特地展示了混合模態任務的上下文學習能力，這是現有模型中沒有的。進一步擴展到 2.7 億個參數展示了更大的性能提升，突出了 BSM 作為多模態生物序列建模重大進步的潛力。

##### **DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG**
2410.11494v1 by Jinyoung Kim, Dayoon Ko, Gunhee Kim

In the rapidly evolving landscape of language, resolving new linguistic
expressions in continuously updating knowledge bases remains a formidable
challenge. This challenge becomes critical in retrieval-augmented generation
(RAG) with knowledge bases, as emerging expressions hinder the retrieval of
relevant documents, leading to generator hallucinations. To address this issue,
we introduce a novel task aimed at resolving emerging mentions to dynamic
entities and present DynamicER benchmark. Our benchmark includes dynamic entity
mention resolution and entity-centric knowledge-intensive QA task, evaluating
entity linking and RAG model's adaptability to new expressions, respectively.
We discovered that current entity linking models struggle to link these new
expressions to entities. Therefore, we propose a temporal segmented clustering
method with continual adaptation, effectively managing the temporal dynamics of
evolving entities and emerging mentions. Extensive experiments demonstrate that
our method outperforms existing baselines, enhancing RAG model performance on
QA task with resolved mentions.

摘要：在語言快速演進的環境中，在持續更新的知識庫中解析新的語言表達式仍然是一項艱鉅的挑戰。這個挑戰在具有知識庫的檢索增強產生 (RAG) 中變得至關重要，因為新興表達式會阻礙相關文件之檢索，導致產生器出現幻覺。為了解決這個問題，我們提出了一項新任務，旨在解析新興提及的動態實體，並提出 DynamicER 基準。我們的基準包括動態實體提及解析和以實體為中心的知識密集型問答任務，分別評估實體連結和 RAG 模型對新表達式的適應性。我們發現目前的實體連結模型難以將這些新表達式連結到實體。因此，我們提出了一種具有持續適應性的時間分段聚類方法，有效管理不斷演化的實體和新興提及的時間動態。大量的實驗證明，我們的模型優於現有的基準，增強了 RAG 模型在解析提及的問答任務上的性能。

##### **Towards Fair Graph Representation Learning in Social Networks**
2410.11493v1 by Guixian Zhang, Guan Yuan, Debo Cheng, Lin Liu, Jiuyong Li, Shichao Zhang

With the widespread use of Graph Neural Networks (GNNs) for representation
learning from network data, the fairness of GNN models has raised great
attention lately. Fair GNNs aim to ensure that node representations can be
accurately classified, but not easily associated with a specific group.
Existing advanced approaches essentially enhance the generalisation of node
representation in combination with data augmentation strategy, and do not
directly impose constraints on the fairness of GNNs. In this work, we identify
that a fundamental reason for the unfairness of GNNs in social network learning
is the phenomenon of social homophily, i.e., users in the same group are more
inclined to congregate. The message-passing mechanism of GNNs can cause users
in the same group to have similar representations due to social homophily,
leading model predictions to establish spurious correlations with sensitive
attributes. Inspired by this reason, we propose a method called Equity-Aware
GNN (EAGNN) towards fair graph representation learning. Specifically, to ensure
that model predictions are independent of sensitive attributes while
maintaining prediction performance, we introduce constraints for fair
representation learning based on three principles: sufficiency, independence,
and separation. We theoretically demonstrate that our EAGNN method can
effectively achieve group fairness. Extensive experiments on three datasets
with varying levels of social homophily illustrate that our EAGNN method
achieves the state-of-the-art performance across two fairness metrics and
offers competitive effectiveness.

摘要：<paragraph>隨著圖神經網路 (GNN) 廣泛用於網路資料的表徵學習，GNN 模型的公平性最近受到極大關注。公平的 GNN 旨在確保節點表徵可以準確分類，但不易與特定群組相關聯。現有的先進方法基本上結合資料擴充策略來增強節點表徵的泛化，並且不會直接對 GNN 的公平性施加約束。在這項工作中，我們發現 GNN 在社交網路學習中不公平的一個基本原因是社會同質性的現象，也就是說，同一個群組中的使用者更傾向於聚集在一起。GNN 的訊息傳遞機制可能會導致同一個群組中的使用者由於社會同質性而具有類似的表徵，導致模型預測與敏感屬性建立虛假的關聯。受到這個原因的啟發，我們提出一個稱為公平感知 GNN (EAGNN) 的方法，用於公平的圖形表徵學習。具體來說，為了確保模型預測獨立於敏感屬性，同時維持預測效能，我們根據充分性、獨立性和分離性這三個原則，引入了公平表徵學習的約束。我們在理論上證明了我們的 EAGNN 方法可以有效地實現群組公平性。在三個具有不同程度社會同質性的資料集上進行的廣泛實驗表明，我們的 EAGNN 方法在兩個公平性指標上都達到了最先進的效能，並提供了有競爭力的有效性。</paragraph>

##### **O-Edit: Orthogonal Subspace Editing for Language Model Sequential Editing**
2410.11469v1 by Yuchen Cai, Ding Cao

Large language models (LLMs) acquire knowledge during pre-training, but over
time, this knowledge may become incorrect or outdated, necessitating updates
after training. Knowledge editing techniques address this issue without the
need for costly re-training. However, most existing methods are designed for
single edits, and as the number of edits increases, they often cause a decline
in the model's overall performance, posing significant challenges for
sequential editing. To overcome this, we propose Orthogonal Subspace Editing,
O-Edit. This algorithm orthogonalizes the direction of each knowledge update,
minimizing interference between successive updates and reducing the impact of
new updates on unrelated knowledge. Our approach does not require replaying
previously edited data and processes each edit knowledge on time. It can
perform thousands of edits on mainstream LLMs, achieving an average performance
improvement that is 4.2 times better than existing methods while effectively
preserving the model's performance on downstream tasks, all with minimal
additional parameter overhead.

摘要：大型語言模型 (LLM) 在預訓練期間會獲取知識，但隨著時間推移，這些知識可能會變得不正確或過時，因此需要在訓練後進行更新。知識編輯技術解決了這個問題，而無需進行代價高昂的重新訓練。然而，現有的方法大多是針對單一編輯而設計的，隨著編輯次數的增加，它們通常會導致模型的整體效能下降，對順序編輯構成重大挑戰。為了克服這個問題，我們提出正交子空間編輯，即 O-Edit。此演算法正交化每個知識更新的方向，將連續更新之間的干擾最小化，並減少新更新對不相關知識的影響。我們的做法不需要重播先前編輯的資料，並即時處理每個編輯知識。它可以在主流 LLM 上執行數千次編輯，實現平均效能提升，比現有方法高出 4.2 倍，同時有效地保留模型在下游任務上的效能，而額外的參數開銷極小。

##### **CoActionGraphRec: Sequential Multi-Interest Recommendations Using Co-Action Graphs**
2410.11464v1 by Yi Sun, Yuri M. Brovman

There are unique challenges to developing item recommender systems for
e-commerce platforms like eBay due to sparse data and diverse user interests.
While rich user-item interactions are important, eBay's data sparsity exceeds
other e-commerce sites by an order of magnitude. To address this challenge, we
propose CoActionGraphRec (CAGR), a text based two-tower deep learning model
(Item Tower and User Tower) utilizing co-action graph layers. In order to
enhance user and item representations, a graph-based solution tailored to
eBay's environment is utilized. For the Item Tower, we represent each item
using its co-action items to capture collaborative signals in a co-action graph
that is fully leveraged by the graph neural network component. For the User
Tower, we build a fully connected graph of each user's behavior sequence, with
edges encoding pairwise relationships. Furthermore, an explicit interaction
module learns representations capturing behavior interactions. Extensive
offline and online A/B test experiments demonstrate the effectiveness of our
proposed approach and results show improved performance over state-of-the-art
methods on key metrics.

摘要：對於 eBay 等電子商務平台來說，由於資料稀疏且使用者興趣多元，開發商品推薦系統面臨獨特的挑戰。雖然豐富的使用者商品互動很重要，但 eBay 的資料稀疏性卻比其他電子商務網站高出一個數量級。為了應對這項挑戰，我們提出了 CoActionGraphRec (CAGR)，一個基於文字的雙塔深度學習模型（商品塔和使用者塔），利用協同動作圖層。為了增強使用者和商品的表現，我們利用了專門針對 eBay 環境的基於圖表的解決方案。對於商品塔，我們使用其協同動作商品來表示每個商品，以在協同動作圖表中擷取協作訊號，而圖形神經網路元件則充分利用了該圖表。對於使用者塔，我們建立了每個使用者行為序列的完全連接圖表，其中邊緣編碼成對關係。此外，一個明確的互動模組學習表示，擷取行為互動。廣泛的離線和線上 A/B 測試實驗證明了我們提出的方法的有效性，結果顯示在關鍵指標上優於最先進的方法。

##### **Advanced Persistent Threats (APT) Attribution Using Deep Reinforcement Learning**
2410.11463v1 by Animesh Singh Basnet, Mohamed Chahine Ghanem, Dipo Dunsin, Wiktor Sowinski-Mydlarz

This paper investigates the application of Deep Reinforcement Learning (DRL)
for attributing malware to specific Advanced Persistent Threat (APT) groups
through detailed behavioural analysis. By analysing over 3500 malware samples
from 12 distinct APT groups, the study utilises sophisticated tools like Cuckoo
Sandbox to extract behavioural data, providing a deep insight into the
operational patterns of malware. The research demonstrates that the DRL model
significantly outperforms traditional machine learning approaches such as SGD,
SVC, KNN, MLP, and Decision Tree Classifiers, achieving an impressive test
accuracy of 89.27 %. It highlights the model capability to adeptly manage
complex, variable, and elusive malware attributes. Furthermore, the paper
discusses the considerable computational resources and extensive data
dependencies required for deploying these advanced AI models in cybersecurity
frameworks. Future research is directed towards enhancing the efficiency of DRL
models, expanding the diversity of the datasets, addressing ethical concerns,
and leveraging Large Language Models (LLMs) to refine reward mechanisms and
optimise the DRL framework. By showcasing the transformative potential of DRL
in malware attribution, this research advocates for a responsible and balanced
approach to AI integration, with the goal of advancing cybersecurity through
more adaptable, accurate, and robust systems.

摘要：本篇論文探討深度強化學習 (DRL) 的應用，透過詳細的行為分析，將惡意軟體歸因於特定的進階持續性威脅 (APT) 組織。透過分析來自 12 個不同 APT 組織的 3500 多個惡意軟體樣本，本研究利用精密的工具（例如 Cuckoo Sandbox）來擷取行為資料，深入了解惡意軟體的操作模式。研究證明，DRL 模型大幅優於傳統機器學習方法，例如 SGD、SVC、KNN、MLP 和決策樹分類器，測試準確率高達 89.27%。它突顯了該模型靈活管理複雜、多變且難以捉摸的惡意軟體屬性的能力。此外，本文探討了在網路安全架構中部署這些進階 AI 模型所需的龐大運算資源和廣泛資料依賴性。未來的研究方向是提高 DRL 模型的效率、擴展資料集的多樣性、解決倫理問題，並利用大型語言模型 (LLM) 來改善獎勵機制並最佳化 DRL 架構。透過展示 DRL 在惡意軟體歸因方面的轉型潛力，本研究提倡負責任且平衡的 AI 整合方法，目標是透過更具適應性、準確性和強健性的系統來提升網路安全。

##### **Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing**
2410.11462v1 by Richard Diehl Martinez, Zebulon Goriely, Andrew Caines, Paula Buttery, Lisa Beinborn

Language models strongly rely on frequency information because they maximize
the likelihood of tokens during pre-training. As a consequence, language models
tend to not generalize well to tokens that are seldom seen during training.
Moreover, maximum likelihood training has been discovered to give rise to
anisotropy: representations of tokens in a model tend to cluster tightly in a
high-dimensional cone, rather than spreading out over their representational
capacity.
  Our work introduces a method for quantifying the frequency bias of a language
model by assessing sentence-level perplexity with respect to token-level
frequency. We then present a method for reducing the frequency bias of a
language model by inducing a syntactic prior over token representations during
pre-training. Our Syntactic Smoothing method adjusts the maximum likelihood
objective function to distribute the learning signal to syntactically similar
tokens. This approach results in better performance on infrequent English
tokens and a decrease in anisotropy. We empirically show that the degree of
anisotropy in a model correlates with its frequency bias.

摘要：語言模型極度依賴頻率資訊，因為它們在預訓練期間最大化了符號的可能性。因此，語言模型往往無法對訓練期間很少見的符號進行良好的概化。此外，已發現最大似然訓練會導致各向異性：模型中符號的表示傾向於緊密聚集在一個高維錐體中，而不是擴散到它們的表示能力之上。
我們的研究引入了一種量化語言模型頻率偏差的方法，方法是根據符號級別的頻率評估句子級困惑度。然後，我們提出了一種透過在預訓練期間在符號表示上誘導句法先驗來減少語言模型頻率偏差的方法。我們的句法平滑方法調整了最大似然目標函數，以將學習訊號分配給句法上相似的符號。這種方法改善了不常見英文符號的效能，並減少了各向異性。我們透過實證顯示，模型中的各向異性程度與其頻率偏差相關。

##### **Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models**
2410.11459v1 by Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari

Large language models (LLMs) have exhibited outstanding performance in
engaging with humans and addressing complex questions by leveraging their vast
implicit knowledge and robust reasoning capabilities. However, such models are
vulnerable to jailbreak attacks, leading to the generation of harmful
responses. Despite recent research on single-turn jailbreak strategies to
facilitate the development of defence mechanisms, the challenge of revealing
vulnerabilities under multi-turn setting remains relatively under-explored. In
this work, we propose Jigsaw Puzzles (JSP), a straightforward yet effective
multi-turn jailbreak strategy against the advanced LLMs. JSP splits questions
into harmless fractions as the input of each turn, and requests LLMs to
reconstruct and respond to questions under multi-turn interaction. Our
experimental results demonstrate that the proposed JSP jailbreak bypasses
original safeguards against explicitly harmful content, achieving an average
attack success rate of 93.76% on 189 harmful queries across 5 advanced LLMs
(Gemini-1.5-Pro, Llama-3.1-70B, GPT-4, GPT-4o, GPT-4o-mini). Moreover, JSP
achieves a state-of-the-art attack success rate of 92% on GPT-4 on the harmful
query benchmark, and exhibits strong resistant to defence strategies. Warning:
this paper contains offensive examples.

摘要：大型語言模型 (LLM) 在與人類互動和解決複雜問題上表現出色，利用其廣泛的隱含知識和強大的推理能力。然而，此類模型容易受到越獄攻擊，導致產生有害的回應。儘管最近針對單回合越獄策略的研究促進了防禦機制的開發，但在多回合設置下揭露漏洞的挑戰仍相對未被探索。在這項工作中，我們提出了拼圖 (JSP)，一種針對高級 LLM 的簡單但有效的多回合越獄策略。JSP 將問題拆分為無害的部分作為每個回合的輸入，並要求 LLM 在多回合互動下重建和回答問題。我們的實驗結果表明，所提出的 JSP 越獄繞過了針對明顯有害內容的原始防護措施，在 5 個高級 LLM（Gemini-1.5-Pro、Llama-3.1-70B、GPT-4、GPT-4o、GPT-4o-mini）上的 189 個有害查詢中實現了 93.76% 的平均攻擊成功率。此外，JSP 在有害查詢基準上對 GPT-4 实现了 92% 的最先進攻擊成功率，並對防禦策略表現出強大的抵抗力。警告：本文包含冒犯性示例。

##### **LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under Low-Resource Scenarios**
2410.11457v1 by Wen Wuzhenghong, Zhang Yongpan, Pan Su, Sun Yuwei, Lu Pengwei, Ding Cheng

Large language models revolutionize Text2SQL through supervised fine-tuning,
yet a crucial limitation is overlooked: the complexity of databases leads to an
increased context length, consequently resulting in higher GPU memory demands
for model fine-tuning. To address this issue, we propose LR-SQL. LR-SQL
comprises two supervised fine-tuning models: the schema\_link model and the
SQL\_generation model, with the schema\_link model serving as the focal point
for streamlining the overall process. During the fine-tuning of the
schema\_link model, LR-SQL breaks down the complete database into flexible
combinations of tables with adjustable quantities, enabling the model to learn
the relationships within the entire database from these dispersed slices.
Furthermore, to enhance the model's ability to perceive the relationships among
various discrete slices during inference, LR-SQL trains the model's
Chain-of-Thought capability for this task. Experimental results demonstrate
that LR-SQL can reduce the total GPU memory usage by 40\% compared to existing
fine-tuning methods, while only losing 2\% of table prediction accuracy in
schema\_link task. For the overall Text2SQL task, the Execution Accuracy
decrease by 0.6\%.Our project is now available on
https://github.com/hongWin/LR-SQL

摘要：大型語言模型透過監督微調革新 Text2SQL，
但一個關鍵的限制被忽略了：資料庫的複雜性導致
增加的內容長度，因此導致模型微調的 GPU 記憶體需求更高。
為了解決這個問題，我們提出 LR-SQL。LR-SQL
包含兩個監督微調模型：schema\_link 模型和
SQL\_generation 模型，其中 schema\_link 模型作為簡化
整體程序的焦點。在 schema\_link 模型的微調過程中，LR-SQL 將
完整的資料庫分解成具有可調整數量的表格的彈性組合，使模型能夠從這些分散的切片中學習整個資料庫中的關係。
此外，為了增強模型在推理過程中感知各種離散切片之間關係的能力，LR-SQL 訓練模型的
思考鏈能力來執行這項任務。實驗結果表明
與現有的微調方法相比，LR-SQL 可以將總 GPU 記憶體使用量減少 40%，同時在
schema\_link 任務中僅損失 2% 的表格預測精度。對於整體 Text2SQL 任務，執行準確度
降低了 0.6%。我們的專案現在可以在
https://github.com/hongWin/LR-SQL 上取得

##### **Tending Towards Stability: Convergence Challenges in Small Language Models**
2410.11451v1 by Richard Diehl Martinez, Pietro Lesci, Paula Buttery

Increasing the number of parameters in language models is a common strategy
to enhance their performance. However, smaller language models remain valuable
due to their lower operational costs. Despite their advantages, smaller models
frequently underperform compared to their larger counterparts, even when
provided with equivalent data and computational resources. Specifically, their
performance tends to degrade in the late pretraining phase. This is anecdotally
attributed to their reduced representational capacity. Yet, the exact causes of
this performance degradation remain unclear. We use the Pythia model suite to
analyse the training dynamics that underlie this phenomenon. Across different
model sizes, we investigate the convergence of the Attention and MLP
activations to their final state and examine how the effective rank of their
parameters influences this process. We find that nearly all layers in larger
models stabilise early in training - within the first 20% - whereas layers in
smaller models exhibit slower and less stable convergence, especially when
their parameters have lower effective rank. By linking the convergence of
layers' activations to their parameters' effective rank, our analyses can guide
future work to address inefficiencies in the learning dynamics of small models.

摘要：增加語言模型中的參數數量是一種增強其效能的常見策略。然而，小型語言模型由於其較低的運作成本而仍然有價值。儘管有其優點，但與其較大的對應模型相比，小型模型經常表現不佳，即使在提供相當的資料和計算資源時也是如此。具體來說，它們的效能往往會在後期的預訓練階段下降。這通常歸因於它們降低的表示能力。然而，這種效能下降的確切原因仍然不清楚。我們使用 Pythia 模型套件來分析這種現象背後的訓練動態。在不同的模型大小之間，我們探討注意力和 MLP 激活對其最終狀態的收斂，並檢查其參數的有效秩如何影響此過程。我們發現，較大模型中的幾乎所有層都在訓練的早期（在前 20% 內）穩定下來，而較小模型中的層則表現出較慢且不穩定的收斂，特別是在其參數具有較低有效秩時。透過將層的激活收斂與其參數的有效秩連結起來，我們的分析可以引導未來的研究來解決小型模型學習動態中的低效率。

##### **A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal Studies**
2410.11450v1 by Yen-Hsiang Wang, Feng-Dian Su, Tzu-Yu Yeh, Yao-Chung Fan

This paper introduces a cross-lingual statutory article retrieval (SAR)
dataset designed to enhance legal information retrieval in multilingual
settings. Our dataset features spoken-language-style legal inquiries in
English, paired with corresponding Chinese versions and relevant statutes,
covering all Taiwanese civil, criminal, and administrative laws. This dataset
aims to improve access to legal information for non-native speakers,
particularly for foreign nationals in Taiwan. We propose several LLM-based
methods as baselines for evaluating retrieval effectiveness, focusing on
mitigating translation errors and improving cross-lingual retrieval
performance. Our work provides a valuable resource for developing inclusive
legal information retrieval systems.

摘要：本文介紹一個跨語言法規條文檢索 (SAR) 資料集，旨在加強多語言環境中的法律資訊檢索。我們的資料集包含以英語表達的口語法律查詢，以及對應的中文版本和相關法規，涵蓋所有台灣民事、刑事和行政法律。此資料集旨在改善非母語人士取得法律資訊的管道，特別是台灣的外國人。我們提出幾種基於 LLM 的方法作為評估檢索效果的基準，重點在於減輕翻譯錯誤並改善跨語言檢索效能。我們的研究成果為開發包容性的法律資訊檢索系統提供了寶貴資源。

##### **AIC CTU system at AVeriTeC: Re-framing automated fact-checking as a simple RAG task**
2410.11446v1 by Herbert Ullrich, Tomáš Mlynář, Jan Drchal

This paper describes our $3^{rd}$ place submission in the AVeriTeC shared
task in which we attempted to address the challenge of fact-checking with
evidence retrieved in the wild using a simple scheme of Retrieval-Augmented
Generation (RAG) designed for the task, leveraging the predictive power of
Large Language Models. We release our codebase and explain its two modules -
the Retriever and the Evidence & Label generator - in detail, justifying their
features such as MMR-reranking and Likert-scale confidence estimation. We
evaluate our solution on AVeriTeC dev and test set and interpret the results,
picking the GPT-4o as the most appropriate model for our pipeline at the time
of our publication, with Llama 3.1 70B being a promising open-source
alternative. We perform an empirical error analysis to see that faults in our
predictions often coincide with noise in the data or ambiguous fact-checks,
provoking further research and data augmentation.

摘要：本文描述了我們在 AVeriTeC 共享任務中獲得的第 3 名，我們嘗試使用專為此任務設計的檢索增強生成 (RAG) 簡單方案，來解決使用在野外檢索的證據進行事實查核的挑戰，利用大型語言模型的預測能力。我們發布我們的程式碼庫並詳細說明其兩個模組 - 檢索器和證據與標籤生成器 - 並說明其功能，例如 MMR 重新排序和李克特量表信心估計。我們對 AVeriTeC 開發和測試集評估我們的解決方案，並解釋結果，並在我們發布時選擇 GPT-4o 作為我們管道最合適的模型，而 Llama 3.1 70B 是一個有前途的開源替代方案。我們執行經驗錯誤分析，以了解我們的預測中的錯誤通常與資料中的雜訊或模棱兩可的事實查核相符，從而激發進一步的研究和資料擴充。

##### **On Championing Foundation Models: From Explainability to Interpretability**
2410.11444v1 by Shi Fu, Yuzhu Chen, Yingjie Wang, Dacheng Tao

Understanding the inner mechanisms of black-box foundation models (FMs) is
essential yet challenging in artificial intelligence and its applications. Over
the last decade, the long-running focus has been on their explainability,
leading to the development of post-hoc explainable methods to rationalize the
specific decisions already made by black-box FMs. However, these explainable
methods have certain limitations in terms of faithfulness, detail capture and
resource requirement. Consequently, in response to these issues, a new class of
interpretable methods should be considered to unveil the underlying mechanisms
in an accurate, comprehensive, heuristic and resource-light way. This survey
aims to review interpretable methods that comply with the aforementioned
principles and have been successfully applied to FMs. These methods are deeply
rooted in machine learning theory, covering the analysis of generalization
performance, expressive capability, and dynamic behavior. They provide a
thorough interpretation of the entire workflow of FMs, ranging from the
inference capability and training dynamics to their ethical implications.
Ultimately, drawing upon these interpretations, this review identifies the next
frontier research directions for FMs.

摘要：了解黑盒基礎模型 (FM) 的內部機制在人工智慧及其應用中至關重要，但也具有挑戰性。在過去十年中，長期以來的重點一直放在它們的可解釋性上，導致開發出事後可解釋的方法來合理化黑盒 FM 已做出的特定決策。然而，這些可解釋的方法在忠實度、細節捕捉和資源需求方面存在一定的限制。因此，為了回應這些問題，應該考慮一類新的可解釋方法，以準確、全面、啟發性和資源節省的方式揭示底層機制。這項調查旨在回顧符合上述原則並已成功應用於 FM 的可解釋方法。這些方法深深植根於機器學習理論，涵蓋泛化性能、表達能力和動態行為的分析。它們對 FM 的整個工作流程提供了透徹的解釋，從推理能力和訓練動態到它們的倫理影響。最終，利用這些解釋，本回顧確定了 FM 的下一個前沿研究方向。

##### **Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs**
2410.11437v1 by Sihang Zhao, Youliang Yuan, Xiaoying Tang, Pinjia He

Multimodal Large Language Models (MLLMs) demonstrate a strong understanding
of the real world and can even handle complex tasks. However, they still fail
on some straightforward visual question-answering (VQA) problems. This paper
dives deeper into this issue, revealing that models tend to err when answering
easy questions (e.g. Yes/No questions) about an image, even though they can
correctly describe it. We refer to this model behavior discrepancy between
difficult and simple questions as model laziness. To systematically investigate
model laziness, we manually construct LazyBench, a benchmark that includes
Yes/No, multiple choice, short answer questions, and image description tasks
that are related to the same subjects in the images. Based on LazyBench, we
observe that laziness widely exists in current advanced MLLMs (e.g. GPT-4o,
Gemini-1.5-pro, Claude 3 and LLaVA-v1.5-13B), and it is more pronounced on
stronger models. We also analyze the VQA v2 (LLaVA-v1.5-13B) benchmark and find
that about half of its failure cases are caused by model laziness, which
further highlights the importance of ensuring that the model fully utilizes its
capability. To this end, we conduct preliminary exploration on how to mitigate
laziness and find that chain of thought (CoT) can effectively address this
issue.

摘要：多模态大型语言模型 (MLLM) 展示了对现实世界的深刻理解，甚至可以处理复杂的任务。然而，它们在一些简单的视觉问答 (VQA) 问题上仍然会失败。本文对此问题进行了更深入的研究，揭示了模型在回答有关图像的简单问题（例如，是/否问题）时往往会出错，即使它们可以正确描述图像。我们将这种模型行为差异称为模型惰性，介于困难和简单问题之间。为了系统地调查模型惰性，我们手动构建了 LazyBench，这是一个基准，其中包括是/否、多选、简答题和图像描述任务，这些任务与图像中的相同主题相关。基于 LazyBench，我们观察到惰性广泛存在于当前先进的 MLLM（例如 GPT-4o、Gemini-1.5-pro、Claude 3 和 LLaVA-v1.5-13B）中，并且在更强大的模型中更为明显。我们还分析了 VQA v2 (LLaVA-v1.5-13B) 基准，发现其大约一半的失败案例是由模型惰性造成的，这进一步凸显了确保模型充分利用其能力的重要性。为此，我们对如何减轻惰性进行了初步探索，并发现思维链 (CoT) 可以有效解决这个问题。

##### **ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability**
2410.11414v1 by Zhongxiang Sun, Xiaoxue Zang, Kai Zheng, Yang Song, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, Han Li

Retrieval-Augmented Generation (RAG) models are designed to incorporate
external knowledge, reducing hallucinations caused by insufficient parametric
(internal) knowledge. However, even with accurate and relevant retrieved
content, RAG models can still produce hallucinations by generating outputs that
conflict with the retrieved information. Detecting such hallucinations requires
disentangling how Large Language Models (LLMs) utilize external and parametric
knowledge. Current detection methods often focus on one of these mechanisms or
without decoupling their intertwined effects, making accurate detection
difficult. In this paper, we investigate the internal mechanisms behind
hallucinations in RAG scenarios. We discover hallucinations occur when the
Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual
stream, while Copying Heads fail to effectively retain or integrate external
knowledge from retrieved content. Based on these findings, we propose ReDeEP, a
novel method that detects hallucinations by decoupling LLM's utilization of
external context and parametric knowledge. Our experiments show that ReDeEP
significantly improves RAG hallucination detection accuracy. Additionally, we
introduce AARF, which mitigates hallucinations by modulating the contributions
of Knowledge FFNs and Copying Heads.

摘要：撷取增强生成 (RAG) 模型旨在纳入外部知识，减少由不足的参数化（内部）知识造成的幻觉。然而，即使有准确且相关的撷取内容，RAG 模型仍可能产生幻觉，因为会生成与撷取信息相冲突的输出。检测这种幻觉需要解开大型语言模型 (LLM) 如何利用外部和参数化知识。目前的检测方法通常专注于其中一种机制，或在不分离其交织效应的情况下，导致难以准确检测。在本文中，我们调查了 RAG 场景中幻觉背后的内部机制。我们发现当 LLM 中的知识前馈神经网络在残差流中过度强调参数化知识时，幻觉就会发生，而复制头无法有效保留或整合撷取内容中的外部知识。基于这些发现，我们提出了 ReDeEP，这是一种新方法，可通过分离 LLM 对外部内容和参数化知识的利用来检测幻觉。我们的实验表明，ReDeEP 大幅提高了 RAG 幻觉检测准确率。此外，我们引入了 AARF，它通过调节知识前馈神经网络和复制头的贡献来减轻幻觉。

##### **PMMT: Preference Alignment in Multilingual Machine Translation via LLM Distillation**
2410.11410v1 by Shuqiao Sun, Yutong Yao, Peiwen Wu, Feijun Jiang, Kaifu Zhang

Translation is important for cross-language communication, and many efforts
have been made to improve its accuracy. However, less investment is conducted
in aligning translations with human preferences, such as translation tones or
styles. In this paper, a new method is proposed to effectively generate
large-scale multilingual parallel corpora with specific translation preferences
using Large Language Models (LLMs). Meanwhile, an automatic pipeline is
designed to distill human preferences into smaller Machine Translation (MT)
models for efficiently and economically supporting large-scale calls in online
services. Experiments indicate that the proposed method takes the lead in
translation tasks with aligned human preferences by a large margin. Meanwhile,
on popular public benchmarks like WMT and Flores, on which our models were not
trained, the proposed method also shows a competitive performance compared to
SOTA works.

摘要：翻譯對於跨語言溝通非常重要，許多努力都專注於提升翻譯的準確性。然而，在調整翻譯與人類偏好（例如翻譯語氣或風格）方面投入的資源較少。在本文中，提出了一種新的方法，可使用大型語言模型 (LLM) 有效產生具有特定翻譯偏好的大規模多語言平行語料庫。同時，設計了一個自動化流程，將人類偏好提煉到較小的機器翻譯 (MT) 模型中，以經濟有效的方式支援線上服務中的大規模呼叫。實驗結果顯示，所提出的方法在與人類偏好相符的翻譯任務中大幅領先。同時，在我們的模型未受過訓練的熱門公開基準（例如 WMT 和 Flores）上，所提出的方法也展現出與 SOTA 作品相匹敵的效能。

##### **A Case for AI Consciousness: Language Agents and Global Workspace Theory**
2410.11407v1 by Simon Goldstein, Cameron Domenico Kirk-Giannini

It is generally assumed that existing artificial systems are not phenomenally
conscious, and that the construction of phenomenally conscious artificial
systems would require significant technological progress if it is possible at
all. We challenge this assumption by arguing that if Global Workspace Theory
(GWT) - a leading scientific theory of phenomenal consciousness - is correct,
then instances of one widely implemented AI architecture, the artificial
language agent, might easily be made phenomenally conscious if they are not
already. Along the way, we articulate an explicit methodology for thinking
about how to apply scientific theories of consciousness to artificial systems
and employ this methodology to arrive at a set of necessary and sufficient
conditions for phenomenal consciousness according to GWT.

摘要：一般而言，現存的人工系統並非現象意識，而如果現象意識人工系統的建構需要顯著的技術進展，那麼它是否可能？我們藉由論證全球工作空間理論（GWT）來挑戰這個假設，全球工作空間理論是現象意識的主要科學理論，如果它正確無誤，那麼一個廣泛實作的AI架構，人工語言代理，其範例可能輕易地成為現象意識，如果它們還不是的話。在此過程中，我們闡明了一套明確的方法論，用於思考如何將意識的科學理論應用於人工系統，並採用此方法論來根據GWT得出現象意識的必要且充分條件。

##### **Enhancing Unimodal Latent Representations in Multimodal VAEs through Iterative Amortized Inference**
2410.11403v1 by Yuta Oshima, Masahiro Suzuki, Yutaka Matsuo

Multimodal variational autoencoders (VAEs) aim to capture shared latent
representations by integrating information from different data modalities. A
significant challenge is accurately inferring representations from any subset
of modalities without training an impractical number (2^M) of inference
networks for all possible modality combinations. Mixture-based models simplify
this by requiring only as many inference models as there are modalities,
aggregating unimodal inferences. However, they suffer from information loss
when modalities are missing. Alignment-based VAEs address this by aligning
unimodal inference models with a multimodal model through minimizing the
Kullback-Leibler (KL) divergence but face issues due to amortization gaps,
which compromise inference accuracy. To tackle these problems, we introduce
multimodal iterative amortized inference, an iterative refinement mechanism
within the multimodal VAE framework. This method overcomes information loss
from missing modalities and minimizes the amortization gap by iteratively
refining the multimodal inference using all available modalities. By aligning
unimodal inference to this refined multimodal posterior, we achieve unimodal
inferences that effectively incorporate multimodal information while requiring
only unimodal inputs during inference. Experiments on benchmark datasets show
that our approach improves inference performance, evidenced by higher linear
classification accuracy and competitive cosine similarity, and enhances
cross-modal generation, indicated by lower FID scores. This demonstrates that
our method enhances inferred representations from unimodal inputs.

摘要：多模態變異自動編碼器 (VAE) 旨在透過整合來自不同資料模態的資訊來擷取共享潛在表示。一個重大的挑戰是準確推論來自任何模態子集的表示，而不用為所有可能的模態組合訓練不切實際數量的 (2^M) 推論網路。基於混合的模型透過只需要與模態一樣多的推論模型來簡化這項工作，並彙總單模態推論。然而，當模態遺失時，它們會遭受資訊遺失的問題。基於對齊的 VAE 透過最小化 Kullback-Leibler (KL) 差異來將單模態推論模型與多模態模型對齊，以解決這個問題，但會因為攤銷差距而面臨問題，這會影響推論準確性。為了解決這些問題，我們引入了多模態反覆攤銷推論，這是一種多模態 VAE 架構內的反覆精煉機制。此方法克服了遺失模態的資訊遺失，並透過反覆使用所有可用模態精煉多模態推論來最小化攤銷差距。透過將單模態推論與這個精煉的多模態後驗對齊，我們達到了單模態推論，有效地納入了多模態資訊，同時在推論期間只需要單模態輸入。在基準資料集上的實驗顯示，我們的做法改善了推論效能，這由更高的線性分類準確度和有競爭力的餘弦相似性證明，並增強了跨模態生成，這由較低的 FID 分數表示。這證明了我們的做法增強了從單模態輸入推論的表示。

##### **Convergence to the Truth**
2410.11399v1 by Hanti Lin

This article reviews and develops an epistemological tradition in philosophy
of science, called convergentism, which holds that inference methods should be
assessed in terms of their abilities to converge to the truth. This tradition
is compared with three competing ones: (1) explanationism, which holds that
theory choice should be guided by a theory's overall balance of explanatory
virtues, such as simplicity and fit with data; (2) instrumentalism, according
to which scientific inference should be driven by the goal of obtaining useful
models, rather than true theories; (3) Bayesianism, which features a shift of
focus from all-or-nothing beliefs to degrees of belief.

摘要：本文回顧並發展了科學哲學中的一種認識論傳統，稱為收斂主義，它認為推理方法應根據其收斂於真理的能力進行評估。這種傳統與三種競爭傳統進行了比較：(1) 解釋主義，它認為理論選擇應由理論的解釋美德的整體平衡來指導，例如簡潔性和與數據的契合度；(2) 工具主義，根據它，科學推理應由獲得有用模型而不是真實理論的目標驅動；(3) 貝葉斯主義，它將焦點從非此即彼的信念轉移到信念程度。

##### **Implementing Derivations of Definite Logic Programs with Self-Attention Networks**
2410.11396v1 by Phan Thi Thanh Thuy, Akihiro Yamamoto

In this paper we propose that a restricted version of logical inference can
be implemented with self-attention networks. We are aiming at showing that LLMs
(Large Language Models) constructed with transformer networks can make logical
inferences. We would reveal the potential of LLMs by analyzing self-attention
networks, which are main components of transformer networks. Our approach is
not based on semantics of natural languages but operations of logical
inference. %point of view. We show that hierarchical constructions of
self-attention networks with feed forward networks (FFNs) can implement
top-down derivations for a class of logical formulae. We also show bottom-up
derivations are also implemented for the same class. We believe that our
results show that LLMs implicitly have the power of logical inference.

摘要：在本文中，我們提出邏輯推理的受限版本可以用自注意力網路來實作。我們的目標是展示使用轉換器網路建構的 LLM（大型語言模型）可以進行邏輯推理。我們將透過分析自注意力網路（轉換器網路的主要組成部分）來揭示 LLM 的潛力。我們的做法並非基於自然語言的語意，而是基於邏輯推理的運算。我們展示了自注意力網路與前饋網路（FFN）的分層建構可以對一類邏輯公式實作由上而下的推導。我們也展示了由下而上的推導也對同一類別實作。我們相信我們的結果顯示 LLM 隱含地具備邏輯推理的能力。

##### **Synthetic Interlocutors. Experiments with Generative AI to Prolong Ethnographic Encounters**
2410.11395v1 by Johan Irving Søltoft, Laura Kocksch, Anders Kristian Munk

This paper introduces "Synthetic Interlocutors" for ethnographic research.
Synthetic Interlocutors are chatbots ingested with ethnographic textual
material (interviews and observations) by using Retrieval Augmented Generation
(RAG). We integrated an open-source large language model with ethnographic data
from three projects to explore two questions: Can RAG digest ethnographic
material and act as ethnographic interlocutor? And, if so, can Synthetic
Interlocutors prolong encounters with the field and extend our analysis?
Through reflections on the process of building our Synthetic Interlocutors and
an experimental collaborative workshop, we suggest that RAG can digest
ethnographic materials, and it might lead to prolonged, yet uneasy ethnographic
encounters that allowed us to partially recreate and re-visit fieldwork
interactions while facilitating opportunities for novel analytic insights.
Synthetic Interlocutors can produce collaborative, ambiguous and serendipitous
moments.

摘要：本文介紹「合成對話者」用於民族誌研究。合成對話者是聊天機器人，透過檢索擴充產生（RAG）技術，擷取民族誌文本資料（訪談和觀察）。我們將一個開源的大語言模型與來自三個專案的民族誌資料整合，探討兩個問題：RAG 能否消化民族誌資料並扮演民族誌對話者的角色？如果是，合成對話者能否延長與田野的接觸，並擴展我們的分析？透過反思建構合成對話者的過程和一場實驗性的協作工作坊，我們提出 RAG 可以消化民族誌資料，並可能導致延長且不安的民族誌接觸，讓我們得以部分重現和重新檢視田野工作互動，同時促進新的分析見解。合成對話者可以產生協作、模棱兩可和意外的時刻。

##### **Do LLMs Have the Generalization Ability in Conducting Causal Inference?**
2410.11385v1 by Chen Wang, Dongming Zhao, Bo Wang, Ruifang He, Yuexian Hou

In causal inference, generalization capability refers to the ability to
conduct causal inference methods on new data to estimate the causal-effect
between unknown phenomenon, which is crucial for expanding the boundaries of
knowledge. Studies have evaluated the causal inference capabilities of Large
Language Models (LLMs) concerning known phenomena, yet the generalization
capabilities of LLMs concerning unseen phenomena remain unexplored. In this
paper, we selected four tasks: Causal Path Discovery (CP), Backdoor Adjustment
(BA), Factual Inference (FI), and Counterfactual Inference (CI) as
representatives of causal inference tasks. To generate evaluation questions
about previously unseen phenomena in new data on the four tasks, we propose a
benchmark generation framework, which employs randomly generated graphs and
node names to formulate questions within hypothetical new causal scenarios.
Based on this framework, we compile a benchmark dataset of varying levels of
question complexity. We extensively tested the generalization capabilities of
five leading LLMs across four tasks. Experiment results reveal that while LLMs
exhibit good generalization performance in solving simple CP, FI, and complex
CI questions, they encounter difficulties when tackling BA questions and face
obvious performance fluctuations as the problem complexity changes.
Furthermore, when the names of phenomena incorporate existing terms, even if
these names are entirely novel, their generalization performance can still be
hindered by interference from familiar terms.

摘要：在因果推論中，泛化能力是指在新的資料上執行因果推論方法以估計未知現象之間的因果關係的能力，這對於擴展知識的界限至關重要。研究已經評估了大型語言模型 (LLM) 關於已知現象的因果推論能力，但 LLM 關於未知現象的泛化能力仍未被探討。在本文中，我們選擇了四個任務：因果路徑發現 (CP)、後門調整 (BA)、事實推論 (FI) 和反事實推論 (CI) 作為因果推論任務的代表。為了產生關於新資料中以前未見現象的評估問題，我們提出了基準生成框架，該框架採用隨機生成的圖形和節點名稱在假設的新因果場景中制定問題。基於此框架，我們編制了一個問題複雜程度不同的基準數據集。我們廣泛測試了五個領先的 LLM 在四個任務中的泛化能力。實驗結果表明，雖然 LLM 在解決簡單的 CP、FI 和複雜的 CI 問題時表現出良好的泛化性能，但在解決 BA 問題時遇到困難，並且隨著問題複雜性的變化而面臨明顯的性能波動。此外，當現象的名稱包含現有術語時，即使這些名稱是完全新穎的，其泛化性能仍然會受到熟悉術語的干擾。

##### **Role of Delay in Brain Dynamics**
2410.11384v1 by Yuval Meir, Ofek Tevet, Yarden Tzach, Shiri Hodassman, Ido Kanter

Significant variations of delays among connecting neurons cause an inevitable
disadvantage of asynchronous brain dynamics compared to synchronous deep
learning. However, this study demonstrates that this disadvantage can be
converted into a computational advantage using a network with a single output
and M multiple delays between successive layers, thereby generating a
polynomial time-series outputs with M. The proposed role of delay in brain
dynamics (RoDiB) model, is capable of learning increasing number of classified
labels using a fixed architecture, and overcomes the inflexibility of the brain
to update the learning architecture using additional neurons and connections.
Moreover, the achievable accuracies of the RoDiB system are comparable with
those of its counterpart tunable single delay architectures with M outputs.
Further, the accuracies are significantly enhanced when the number of output
labels exceeds its fully connected input size. The results are mainly obtained
using simulations of VGG-6 on CIFAR datasets and also include multiple label
inputs. However, currently only a small fraction of the abundant number of
RoDiB outputs is utilized, thereby suggesting its potential for advanced
computational power yet to be discovered.

摘要：在連接神經元之間延遲的顯著變化，導致非同步大腦動態與同步深度學習相比，具有不可避免的劣勢。然而，本研究表明，這種劣勢可以使用具有單一輸出和連續層之間 M 個多重延遲的網路轉換為計算優勢，從而產生具有 M 的多項式時間序列輸出。大腦動態中延遲的角色 (RoDiB) 模型，能夠使用固定架構學習越來越多的分類標籤，並克服大腦使用額外神經元和連接更新學習架構的僵化性。此外，RoDiB 系統的可實現準確度與其具有 M 個輸出的可調整單一延遲架構的準確度相當。此外，當輸出標籤的數量超過其完全連接的輸入大小時，準確度會顯著提高。結果主要是使用 CIFAR 資料集上的 VGG-6 模擬獲得的，還包括多個標籤輸入。然而，目前僅使用了一小部分豐富的 RoDiB 輸出，從而表明其具有尚未發現的高級計算能力的潛力。

##### **WPFed: Web-based Personalized Federation for Decentralized Systems**
2410.11378v1 by Guanhua Ye, Jifeng He, Weiqing Wang, Zhe Xue, Feifei Kou, Yawen Li

Decentralized learning has become crucial for collaborative model training in
environments where data privacy and trust are paramount. In web-based
applications, clients are liberated from traditional fixed network topologies,
enabling the establishment of arbitrary peer-to-peer (P2P) connections. While
this flexibility is highly promising, it introduces a fundamental challenge:
the optimal selection of neighbors to ensure effective collaboration. To
address this, we introduce WPFed, a fully decentralized, web-based learning
framework designed to enable globally optimal neighbor selection. WPFed employs
a dynamic communication graph and a weighted neighbor selection mechanism. By
assessing inter-client similarity through Locality-Sensitive Hashing (LSH) and
evaluating model quality based on peer rankings, WPFed enables clients to
identify personalized optimal neighbors on a global scale while preserving data
privacy. To enhance security and deter malicious behavior, WPFed integrates
verification mechanisms for both LSH codes and performance rankings, leveraging
blockchain-driven announcements to ensure transparency and verifiability.
Through extensive experiments on multiple real-world datasets, we demonstrate
that WPFed significantly improves learning outcomes and system robustness
compared to traditional federated learning methods. Our findings highlight
WPFed's potential to facilitate effective and secure decentralized
collaborative learning across diverse and interconnected web environments.

摘要：<paragraph>去中心化學習已成為在資料隱私和信任至上的環境中進行協作模型訓練的關鍵。在基於網路的應用程式中，用戶端不受傳統固定網路拓撲的限制，可建立任意點對點 (P2P) 連線。雖然這種彈性極具前景，但也帶來一個基本挑戰：最佳鄰居選擇以確保有效協作。為了解決此問題，我們引入了 WPFed，一個完全去中心化、基於網路的學習架構，旨在實現全球最佳鄰居選擇。WPFed 使用動態通訊圖形和加權鄰居選擇機制。透過局部敏感雜湊 (LSH) 評估用戶端間的相似性，並根據對等排名評估模型品質，WPFed 能讓用戶端在全球範圍內找出個人化的最佳鄰居，同時保護資料隱私。為了增強安全性並遏止惡意行為，WPFed 整合了 LSH 碼和效能排名的驗證機制，利用區塊鏈驅動的公告來確保透明度和可驗證性。透過在多個真實世界資料集上進行廣泛的實驗，我們證明了與傳統的聯合學習方法相比，WPFed 大幅改善了學習成果和系統穩健性。我們的研究結果突顯了 WPFed 在促進不同且相互連接的網路環境中的有效且安全的去中心化協作學習的潛力。</paragraph>

##### **A Framework for Adapting Human-Robot Interaction to Diverse User Groups**
2410.11377v1 by Theresa Pekarek Rosin, Vanessa Hassouna, Xiaowen Sun, Luca Krohm, Henri-Leon Kordt, Michael Beetz, Stefan Wermter

To facilitate natural and intuitive interactions with diverse user groups in
real-world settings, social robots must be capable of addressing the varying
requirements and expectations of these groups while adapting their behavior
based on user feedback. While previous research often focuses on specific
demographics, we present a novel framework for adaptive Human-Robot Interaction
(HRI) that tailors interactions to different user groups and enables individual
users to modulate interactions through both minor and major interruptions. Our
primary contributions include the development of an adaptive, ROS-based HRI
framework with an open-source code base. This framework supports natural
interactions through advanced speech recognition and voice activity detection,
and leverages a large language model (LLM) as a dialogue bridge. We validate
the efficiency of our framework through module tests and system trials,
demonstrating its high accuracy in age recognition and its robustness to
repeated user inputs and plan changes.

摘要：為了促進在現實世界環境中與不同使用者群體進行自然而直覺的互動，社交機器人必須能夠滿足這些群體不同的需求和期望，同時根據使用者的回饋調整其行為。雖然先前的研究通常側重於特定的人口統計，但我們提出了自適應人機互動 (HRI) 的新架構，該架構可根據不同的使用者群體調整互動，並讓個別使用者能夠透過輕微和重大的中斷來調節互動。我們的首要貢獻包括開發一個自適應的、基於 ROS 的 HRI 架構，並提供開源程式碼庫。此架構透過先進的語音辨識和語音活動偵測支援自然互動，並利用大型語言模型 (LLM) 作為對話橋樑。我們透過模組測試和系統試驗驗證了我們架構的效率，證明了其在年齡辨識方面的精準度很高，而且對重複的使用者輸入和計畫變更具有穩健性。

##### **Augmentation-Driven Metric for Balancing Preservation and Modification in Text-Guided Image Editing**
2410.11374v1 by Yoonjeon Kim, Soohyun Ryu, Yeonsung Jung, Hyunkoo Lee, Joowon Kim, June Yong Yang, Jaeryong Hwang, Eunho Yang

The development of vision-language and generative models has significantly
advanced text-guided image editing, which seeks \textit{preservation} of core
elements in the source image while implementing \textit{modifications} based on
the target text. However, in the absence of evaluation metrics specifically
tailored for text-guided image editing, existing metrics are limited in
balancing the consideration of preservation and modification. Especially, our
analysis reveals that CLIPScore, the most commonly used metric, tends to favor
modification and ignore core attributes to be preserved, resulting in
inaccurate evaluations. To address this problem, we propose \texttt{AugCLIP},
\black{which balances preservation and modification by estimating the
representation of an ideal edited image that aligns with the target text with
minimum alteration on the source image. We augment detailed textual
descriptions on the source image and the target text using a multi-modal large
language model, to model a hyperplane that separates CLIP space into source or
target. The representation of the ideal edited image is an orthogonal
projection of the source image into the hyperplane, which encapsulates the
relative importance of each attribute considering the interdependent
relationships.} Our extensive experiments on five benchmark datasets,
encompassing a diverse range of editing scenarios, demonstrate that
\texttt{AugCLIP} aligns remarkably well with human evaluation standards
compared to existing metrics. The code for evaluation will be open-sourced to
contribute to the community.

摘要：視覺語言和生成模型的發展顯著地推進了文字引導的影像編輯，它尋求在實作基於目標文字的「修改」時「保留」原始影像中的核心元素。然而，在缺乏專門針對文字引導的影像編輯而量身打造的評估指標下，現有的指標在平衡保留和修改的考量上受到限制。特別是，我們的分析顯示，最常使用的指標 CLIPScore 傾向於偏好修改而忽略應保留的核心屬性，導致評估不準確。為了解決這個問題，我們提出 \texttt{AugCLIP}，它透過估計理想編輯影像的表示來平衡保留和修改，該表示與目標文字一致，同時對原始影像的變更最小。我們使用多模態大型語言模型來擴充原始影像和目標文字上的詳細文字描述，以建構一個將 CLIP 空間分隔成原始或目標的超平面。理想編輯影像的表示是原始影像到超平面的正交投影，它概括了在考量相互依賴關係的情況下每個屬性的相對重要性。我們在五個基準資料集上進行的廣泛實驗，涵蓋了各種編輯情境，證明與現有指標相比，\texttt{AugCLIP} 與人類評估標準顯著地吻合。評估程式碼將開放原始碼，以貢獻給社群。

##### **Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL**
2410.11371v1 by Qihuang Zhong, Kunfeng Chen, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao

Large Language Models (LLMs) have shown promising performance in text-to-SQL,
which involves translating natural language questions into SQL queries.
However, current text-to-SQL LLMs are computationally expensive and challenging
to deploy in real-world applications, highlighting the importance of
compressing them. To achieve this goal, knowledge distillation (KD) is a common
approach, which aims to distill the larger teacher model into a smaller student
model. While numerous KD methods for autoregressive LLMs have emerged recently,
it is still under-explored whether they work well in complex text-to-SQL
scenarios. To this end, we conduct a series of analyses and reveal that these
KD methods generally fall short in balancing performance and efficiency. In
response to this problem, we propose to improve the KD with Imperfect Data,
namely KID, which effectively boosts the performance without introducing much
training budget. The core of KID is to efficiently mitigate the
training-inference mismatch by simulating the cascading effect of inference in
the imperfect training data. Extensive experiments on 5 text-to-SQL benchmarks
show that, KID can not only achieve consistent and significant performance
gains (up to +5.83% average score) across all model types and sizes, but also
effectively improve the training efficiency.

摘要：大型語言模型（LLM）在文本轉 SQL 中展現了有前途的表現，其中涉及將自然語言問題轉換為 SQL 查詢。然而，目前的文本轉 SQL LLM 在運算上很昂貴，且難以部署在真實世界的應用中，這凸顯了壓縮它們的重要性。為了達成這個目標，知識蒸餾（KD）是一種常見的方法，其目標是將較大的教師模型蒸餾成較小的學生模型。儘管最近出現了許多針對自迴歸 LLM 的 KD 方法，但它們是否適用於複雜的文本轉 SQL 場景仍有待探討。為此，我們進行了一系列分析，並揭示這些 KD 方法通常無法在效能和效率之間取得平衡。為了應對這個問題，我們提議使用不完美資料改善 KD，即 KID，它有效地提升了效能，而不會引入太多訓練預算。KID 的核心是透過模擬不完美訓練資料中推論的層疊效應，有效地減輕訓練推論不匹配的問題。在 5 個文本轉 SQL 基準上的廣泛實驗顯示，KID 不僅可以在所有模型類型和規模上實現一致且顯著的效能提升（平均分數提升達 +5.83%），還能有效提升訓練效率。

##### **Enhance Graph Alignment for Large Language Models**
2410.11370v1 by Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, Yujun Zhang

Graph-structured data is prevalent in the real world. Recently, due to the
powerful emergent capabilities, Large Language Models (LLMs) have shown
promising performance in modeling graphs. The key to effectively applying LLMs
on graphs is converting graph data into a format LLMs can comprehend.
Graph-to-token approaches are popular in enabling LLMs to process graph
information. They transform graphs into sequences of tokens and align them with
text tokens through instruction tuning, where self-supervised instruction
tuning helps LLMs acquire general knowledge about graphs, and supervised
fine-tuning specializes LLMs for the downstream tasks on graphs. Despite their
initial success, we find that existing methods have a misalignment between
self-supervised tasks and supervised downstream tasks, resulting in negative
transfer from self-supervised fine-tuning to downstream tasks. To address these
issues, we propose Graph Alignment Large Language Models (GALLM) to benefit
from aligned task templates. In the self-supervised tuning stage, we introduce
a novel text matching task using templates aligned with downstream tasks. In
the task-specific tuning stage, we propose two category prompt methods that
learn supervision information from additional explanation with further aligned
templates. Experimental evaluations on four datasets demonstrate substantial
improvements in supervised learning, multi-dataset generalizability, and
particularly in zero-shot capability, highlighting the model's potential as a
graph foundation model.

摘要：圖形結構的資料在現實世界中很常見。最近，由於強大的新興能力，大型語言模型 (LLM) 在圖形建模方面展現出令人滿意的效能。有效將 LLM 應用於圖形的關鍵是將圖形資料轉換成 LLM 可以理解的格式。圖形到標記的方法很流行，讓 LLM 可以處理圖形資訊。它們將圖形轉換成標記序列，並透過指令調整與文字標記對齊，其中自我監督的指令調整有助於 LLM 獲得關於圖形的常識，而監督微調則專門針對圖形上的下游任務調整 LLM。儘管它們最初很成功，我們發現現有方法在自我監督任務和監督下游任務之間存在錯位，導致自我監督微調對下游任務產生負面影響。為了解決這些問題，我們提出圖形對齊大型語言模型 (GALLM) 以從對齊的任務範本中受益。在自我監督調整階段，我們使用與下游任務對齊的範本，引入一個新穎的文字比對任務。在特定任務的調整階段，我們提出兩種類別提示方法，從進一步對齊範本的額外說明中學習監督資訊。在四個資料集上的實驗評估證明了監督式學習、多資料集的概括性，特別是在零次學習能力方面有顯著的進步，突顯了該模型作為圖形基礎模型的潛力。

##### **LargePiG: Your Large Language Model is Secretly a Pointer Generator**
2410.11366v1 by Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, Jun Xu

Recent research on query generation has focused on using Large Language
Models (LLMs), which despite bringing state-of-the-art performance, also
introduce issues with hallucinations in the generated queries. In this work, we
introduce relevance hallucination and factuality hallucination as a new
typology for hallucination problems brought by query generation based on LLMs.
We propose an effective way to separate content from form in LLM-generated
queries, which preserves the factual knowledge extracted and integrated from
the inputs and compiles the syntactic structure, including function words,
using the powerful linguistic capabilities of the LLM. Specifically, we
introduce a model-agnostic and training-free method that turns the Large
Language Model into a Pointer-Generator (LargePiG), where the pointer attention
distribution leverages the LLM's inherent attention weights, and the copy
probability is derived from the difference between the vocabulary distribution
of the model's high layers and the last layer. To validate the effectiveness of
LargePiG, we constructed two datasets for assessing the hallucination problems
in query generation, covering both document and video scenarios. Empirical
studies on various LLMs demonstrated the superiority of LargePiG on both
datasets. Additional experiments also verified that LargePiG could reduce
hallucination in large vision language models and improve the accuracy of
document-based question-answering and factuality evaluation tasks.

摘要：近期關於查詢產生的研究，專注於使用大型語言模型 (LLM)，儘管帶來最先進的效能，但也引入了生成查詢中出現幻覺的問題。在這項工作中，我們將相關性幻覺和事實幻覺引入為 LLM 為基礎的查詢產生所帶來的幻覺問題的新類型。我們提出了一個有效的方法，用於將 LLM 生成的查詢中的內容與形式分開，這保留了從輸入中提取和整合的事實知識，並編譯句法結構，包括功能詞，利用 LLM 強大的語言能力。具體來說，我們引入了一個與模型無關且無需訓練的方法，將大型語言模型轉換為指標生成器 (LargePiG)，其中指標注意分配利用了 LLM 固有的注意權重，而複製機率則來自模型高層和最後一層的詞彙分配之間的差異。為了驗證 LargePiG 的有效性，我們構建了兩個用於評估查詢產生中幻覺問題的資料集，涵蓋文件和影片場景。對各種 LLM 的經驗研究證明了 LargePiG 在兩個資料集上的優越性。額外的實驗也驗證了 LargePiG 可以減少大型視覺語言模型中的幻覺，並提高基於文件的問答和事實評估任務的準確性。

##### **RATE: Score Reward Models with Imperfect Rewrites of Rewrites**
2410.11348v1 by David Reber, Sean Richardson, Todd Nief, Cristina Garbacea, Victor Veitch

This paper concerns the evaluation of reward models used in language
modeling. A reward model is a function that takes a prompt and a response and
assigns a score indicating how good that response is for the prompt. A key
challenge is that reward models are usually imperfect proxies for actual
preferences. For example, we may worry that a model trained to reward
helpfulness learns to instead prefer longer responses. In this paper, we
develop an evaluation method, RATE (Rewrite-based Attribute Treatment
Estimators), that allows us to measure the causal effect of a given attribute
of a response (e.g., length) on the reward assigned to that response. The core
idea is to use large language models to rewrite responses to produce imperfect
counterfactuals, and to adjust for rewriting error by rewriting twice. We show
that the RATE estimator is consistent under reasonable assumptions. We
demonstrate the effectiveness of RATE on synthetic and real-world data, showing
that it can accurately estimate the effect of a given attribute on the reward
model.

摘要：本文涉及對語言模型中使用的獎勵模型的評估。獎勵模型是一個函數，它接收提示和回應，並指派一個分數，表示該回應對提示有多好。一個關鍵的挑戰在於，獎勵模型通常是不完美的代理，無法反映實際偏好。例如，我們可能擔心一個訓練成獎勵有用的模型，反而學會偏好較長的回應。在本文中，我們開發了一個評估方法 RATE（基於改寫的屬性處理估計器），它讓我們能夠測量回應的給定屬性（例如，長度）對指派給該回應的獎勵的因果關係。核心概念是使用大型語言模型改寫回應來產生不完美的反事實，並通過重複改寫來調整改寫誤差。我們表明，在合理的假設下，RATE 估計器是一致的。我們在合成和真實世界資料上展示了 RATE 的有效性，表明它可以準確估計給定屬性對獎勵模型的影響。

##### **DIAR: Diffusion-model-guided Implicit Q-learning with Adaptive Revaluation**
2410.11338v1 by Jaehyun Park, Yunho Kim, Sejin Kim, Byung-Jun Lee, Sundong Kim

We propose a novel offline reinforcement learning (offline RL) approach,
introducing the Diffusion-model-guided Implicit Q-learning with Adaptive
Revaluation (DIAR) framework. We address two key challenges in offline RL:
out-of-distribution samples and long-horizon problems. We leverage diffusion
models to learn state-action sequence distributions and incorporate value
functions for more balanced and adaptive decision-making. DIAR introduces an
Adaptive Revaluation mechanism that dynamically adjusts decision lengths by
comparing current and future state values, enabling flexible long-term
decision-making. Furthermore, we address Q-value overestimation by combining
Q-network learning with a value function guided by a diffusion model. The
diffusion model generates diverse latent trajectories, enhancing policy
robustness and generalization. As demonstrated in tasks like Maze2D, AntMaze,
and Kitchen, DIAR consistently outperforms state-of-the-art algorithms in
long-horizon, sparse-reward environments.

摘要：我們提出了一種新穎的離線強化學習 (offline RL) 方法，
引入了以擴散模型為指導的具有自適應重估 (DIAR) 框架的隱式 Q 學習。我們解決了離線 RL 中的兩個關鍵挑戰：
分佈外樣本和長時域問題。我們利用擴散模型來學習狀態動作序列分佈，並整合值函數以進行更加平衡和自適應的決策制定。DIAR 引入了自適應重估機制，該機制通過比較當前和未來狀態值來動態調整決策長度，從而實現靈活的長期決策制定。此外，我們通過將 Q 網路學習與由擴散模型指導的值函數相結合來解決 Q 值高估問題。
擴散模型產生多樣化的潛在軌跡，增強策略魯棒性和泛化能力。正如 Maze2D、AntMaze 和 Kitchen 等任務中所展示的那樣，DIAR 在長時域、稀疏獎勵環境中始終優於最先進的演算法。

##### **SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments**
2410.11331v1 by Syed Abdul Gaffar Shakhadri, Kruthika KR, Rakshit Aralimatti

We introduce Shakti, a 2.5 billion parameter language model specifically
optimized for resource-constrained environments such as edge devices, including
smartphones, wearables, and IoT systems. Shakti combines high-performance NLP
with optimized efficiency and precision, making it ideal for real-time AI
applications where computational resources and memory are limited. With support
for vernacular languages and domain-specific tasks, Shakti excels in industries
such as healthcare, finance, and customer service. Benchmark evaluations
demonstrate that Shakti performs competitively against larger models while
maintaining low latency and on-device efficiency, positioning it as a leading
solution for edge AI.

摘要：我們推出 Shakti，一種專為資源受限環境（例如邊緣裝置，包括智慧型手機、穿戴式裝置和 IoT 系統）所最佳化的 25 億個參數語言模型。Shakti 結合高性能 NLP 與最佳化的效率和精準度，使其成為在運算資源和記憶體有限的情況下進行即時 AI 應用程式的理想選擇。Shakti 支援本土語言和特定領域任務，在醫療保健、金融和客戶服務等產業中表現出色。基準評估顯示，Shakti 在維持低延遲和裝置上效率的同時，與較大的模型競爭力相當，使其成為邊緣 AI 的領先解決方案。

##### **Sequential LLM Framework for Fashion Recommendation**
2410.11327v1 by Han Liu, Xianfeng Tang, Tianlang Chen, Jiapeng Liu, Indu Indu, Henry Peng Zou, Peng Dai, Roberto Fernandez Galan, Michael D Porter, Dongmei Jia, Ning Zhang, Lian Xiong

The fashion industry is one of the leading domains in the global e-commerce
sector, prompting major online retailers to employ recommendation systems for
product suggestions and customer convenience. While recommendation systems have
been widely studied, most are designed for general e-commerce problems and
struggle with the unique challenges of the fashion domain. To address these
issues, we propose a sequential fashion recommendation framework that leverages
a pre-trained large language model (LLM) enhanced with recommendation-specific
prompts. Our framework employs parameter-efficient fine-tuning with extensive
fashion data and introduces a novel mix-up-based retrieval technique for
translating text into relevant product suggestions. Extensive experiments show
our proposed framework significantly enhances fashion recommendation
performance.

摘要：時尚產業是全球電子商務領域的領先領域之一，促使各大線上零售商採用推薦系統來提供產品建議和客戶便利性。雖然推薦系統已被廣泛研究，但大多數都是針對一般電子商務問題而設計，並且難以應對時尚領域的獨特挑戰。為了解決這些問題，我們提出了一個序列式時尚推薦架構，該架構利用了一個預先訓練的大語言模型 (LLM)，並透過推薦特定的提示進行增強。我們的架構採用參數有效率的微調，搭配廣泛的時尚資料，並引入了一種新穎的基於混淆的檢索技術，用於將文字轉換為相關的產品建議。廣泛的實驗表明，我們提出的架構顯著地增強了時尚推薦的效能。

##### **Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling**
2410.11325v1 by Wenda Xu, Rujun Han, Zifeng Wang, Long T. Le, Dhruv Madeka, Lei Li, William Yang Wang, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister

Recent advances in knowledge distillation (KD) have enabled smaller student
models to approach the performance of larger teacher models. However, popular
methods such as supervised KD and on-policy KD, are adversely impacted by the
knowledge gaps between teacher-student in practical scenarios. Supervised KD
suffers from a distribution mismatch between training with a static dataset and
inference over final student-generated outputs. Conversely, on-policy KD, which
uses student-generated samples for training, can suffer from low-quality
training examples with which teacher models are not familiar, resulting in
inaccurate teacher feedback. To address these limitations, we introduce
Speculative Knowledge Distillation (SKD), a novel approach that leverages
cooperation between student and teacher models to generate high-quality
training data on-the-fly while aligning with the student's inference-time
distribution. In SKD, the student proposes tokens, and the teacher replaces
poorly ranked ones based on its own distribution, transferring high-quality
knowledge adaptively. We evaluate SKD on various text generation tasks,
including translation, summarization, math, and instruction following, and show
that SKD consistently outperforms existing KD methods across different domains,
data sizes, and model initialization strategies.

摘要：最近知识提炼 (KD) 的进步使得较小的学生模型能够接近较大教师模型的性能。然而，流行的方法，如监督 KD 和基于策略 KD，会受到教师和学生在实际场景中的知识差距的不利影响。监督 KD 存在训练时静态数据集和最终学生生成输出的推理之间的分布失配。相反，基于策略 KD 使用学生生成的样本进行训练，可能会受到教师模型不熟悉的低质量训练示例的影响，从而导致教师反馈不准确。为了解决这些限制，我们引入了推测知识提炼 (SKD)，这是一种新方法，它利用学生和教师模型之间的合作来生成高质量的训练数据，同时与学生的推理时间分布保持一致。在 SKD 中，学生提出标记，教师根据自己的分布替换排名较差的标记，自适应地转移高质量的知识。我们在各种文本生成任务上评估了 SKD，包括翻译、摘要、数学和指令遵循，并表明 SKD 在不同的领域、数据大小和模型初始化策略中始终优于现有的 KD 方法。

##### **Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation**
2410.11317v1 by Qizhang Li, Xiaochen Yang, Wangmeng Zuo, Yiwen Guo

Automatic adversarial prompt generation provides remarkable success in
jailbreaking safely-aligned large language models (LLMs). Existing
gradient-based attacks, while demonstrating outstanding performance in
jailbreaking white-box LLMs, often generate garbled adversarial prompts with
chaotic appearance. These adversarial prompts are difficult to transfer to
other LLMs, hindering their performance in attacking unknown victim models. In
this paper, for the first time, we delve into the semantic meaning embedded in
garbled adversarial prompts and propose a novel method that "translates" them
into coherent and human-readable natural language adversarial prompts. In this
way, we can effectively uncover the semantic information that triggers
vulnerabilities of the model and unambiguously transfer it to the victim model,
without overlooking the adversarial information hidden in the garbled text, to
enhance jailbreak attacks. It also offers a new approach to discovering
effective designs for jailbreak prompts, advancing the understanding of
jailbreak attacks. Experimental results demonstrate that our method
significantly improves the success rate of jailbreak attacks against various
safety-aligned LLMs and outperforms state-of-the-arts by large margins. With at
most 10 queries, our method achieves an average attack success rate of 81.8% in
attacking 7 commercial closed-source LLMs, including GPT and Claude-3 series,
on HarmBench. Our method also achieves over 90% attack success rates against
Llama-2-Chat models on AdvBench, despite their outstanding resistance to
jailbreak attacks. Code at:
https://github.com/qizhangli/Adversarial-Prompt-Translator.

摘要：自動對抗提示生成在安全對齊的大型語言模型 (LLM) 的越獄中取得了顯著成功。現有的基於梯度的攻擊在越獄白盒 LLM 中表現出色，但通常會產生混亂的外觀，生成雜亂的對抗提示。這些對抗提示難以轉移到其他 LLM，阻礙了它們攻擊未知受害者模型的性能。在本文中，我們首次深入探討了混亂的對抗提示中嵌入的語義含義，並提出了一種新方法，將它們「翻譯」成連貫且人類可讀的自然語言對抗提示。通過這種方式，我們可以有效地揭示觸發模型漏洞的語義信息，並將其明確地轉移到受害者模型中，而不會忽視隱藏在混亂文本中的對抗信息，以增強越獄攻擊。它還提供了一種發現越獄提示的有效設計的新方法，推動了對越獄攻擊的理解。實驗結果表明，我們的方法顯著提高了針對各種安全對齊 LLM 的越獄攻擊的成功率，並大幅優於最先進的技術。使用最多 10 個查詢，我們的方法在攻擊 7 個商業閉源 LLM（包括 GPT 和 Claude-3 系列）時，在 HarmBench 上實現了 81.8% 的平均攻擊成功率。儘管 Llama-2-Chat 模型對越獄攻擊具有出色的抵抗力，但我們的方法在 AdvBench 上仍實現了超過 90% 的攻擊成功率。代碼在：
https://github.com/qizhangli/Adversarial-Prompt-Translator。

##### **SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation**
2410.11315v1 by Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, Min Zhang

Recent studies in Retrieval-Augmented Generation (RAG) have investigated
extracting evidence from retrieved passages to reduce computational costs and
enhance the final RAG performance, yet it remains challenging. Existing methods
heavily rely on heuristic-based augmentation, encountering several issues: (1)
Poor generalization due to hand-crafted context filtering; (2) Semantics
deficiency due to rule-based context chunking; (3) Skewed length due to
sentence-wise filter learning. To address these issues, we propose a
model-based evidence extraction learning framework, SEER, optimizing a vanilla
model as an evidence extractor with desired properties through self-aligned
learning. Extensive experiments show that our method largely improves the final
RAG performance, enhances the faithfulness, helpfulness, and conciseness of the
extracted evidence, and reduces the evidence length by 9.25 times. The code
will be available at https://github.com/HITsz-TMG/SEER.

摘要：最近在检索增强生成 (RAG) 中的研究调查了从检索段落中提取证据以降低计算成本并增强最终 RAG 性能，但仍然具有挑战性。现有方法严重依赖基于启发式的增强，遇到几个问题：(1) 由于手工制作的上下文过滤而导致的泛化性差；(2) 由于基于规则的上下文块分割而导致的语义缺陷；(3) 由于基于句子的过滤学习而导致的长度偏差。为了解决这些问题，我们提出了一个基于模型的证据提取学习框架 SEER，通过自对齐学习将香草模型优化为具有所需属性的证据提取器。大量实验表明，我们的方法极大地提高了最终的 RAG 性能，增强了提取证据的保真度、有用性和简洁性，并将证据长度减少了 9.25 倍。代码将在 https://github.com/HITsz-TMG/SEER 上提供。

##### **QSpec: Speculative Decoding with Complementary Quantization Schemes**
2410.11305v1 by Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, Chuan Wu

Quantization has been substantially adopted to accelerate inference and
reduce memory consumption of large language models (LLMs). While
activation-weight joint quantization speeds up the inference process through
low-precision kernels, we demonstrate that it suffers severe performance
degradation on multi-step reasoning tasks, rendering it ineffective. We propose
a novel quantization paradigm called QSPEC, which seamlessly integrates two
complementary quantization schemes for speculative decoding. Leveraging nearly
cost-free execution switching, QSPEC drafts tokens with low-precision, fast
activation-weight quantization, and verifies them with high-precision
weight-only quantization, effectively combining the strengths of both
quantization schemes. Compared to high-precision quantization methods, QSPEC
empirically boosts token generation throughput by up to 1.80x without any
quality compromise, distinguishing it from other low-precision quantization
approaches. This enhancement is also consistent across various serving tasks,
model sizes, quantization methods, and batch sizes. Unlike existing speculative
decoding techniques, our approach reuses weights and the KV cache, avoiding
additional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage
without requiring any training. We believe that QSPEC demonstrates unique
strengths for future deployment of high-fidelity quantization schemes,
particularly in memory-constrained scenarios (e.g., edge devices).

摘要：量化技術已被廣泛採用以加速推理並減少大型語言模型 (LLM) 的記憶體消耗。雖然激活權重聯合量化透過低精度核心加速推理過程，但我們證明它在多步驟推理任務中會遭受嚴重的效能降低，導致它無效。我們提出一個稱為 QSPEC 的新量化範例，它無縫整合了兩種互補的量化方案，用於推測性解碼。利用幾乎免費的執行切換，QSPEC 起草具有低精度、快速激活權重量化的代碼，並使用高精度僅權重量化驗證它們，有效地結合了兩種量化方案的優點。與高精度量化方法相比，QSPEC 實證地將代碼產生吞吐量提升了 1.80 倍，且不影響任何品質，使其有別於其他低精度量化方法。這種增強也一致適用於各種服務任務、模型大小、量化方法和批次大小。與現有的推測性解碼技術不同，我們的方法會重複使用權重和 KV 快取，避免額外的記憶體開銷。此外，QSPEC 提供即插即用的優點，無需任何訓練。我們相信 QSPEC 為未來部署高保真量化方案展現了獨特的優勢，特別是在受記憶體限制的場景（例如邊緣裝置）中。

##### **Data Selection for Task-Specific Model Finetuning**
2410.11303v1 by Zifan Liu, Amin Karbasi, Theodoros Rekatsinas

Finetuning foundation models for specific tasks is an emerging paradigm in
modern machine learning. The efficacy of task-specific finetuning largely
depends on the selection of appropriate training data. We present a framework
to select data for task-specific model finetuning, guided by a small but
representative set of examples from the target task. To do so, we formulate
data selection for task-specific finetuning as an optimization problem with a
distribution alignment loss based on optimal transport to capture the
discrepancy between the selected data and the target distribution. In addition,
we add a regularizer to encourage the diversity of the selected data and
incorporate kernel density estimation into the regularizer to reduce the
negative effects of near-duplicates among the candidate data. We connect our
optimization problem to nearest neighbor search and design efficient algorithms
to compute the optimal solution based on approximate nearest neighbor search
techniques. We evaluate our method on data selection for both continued
pretraining and instruction tuning of language models. We show that instruction
tuning using data selected by our method with a 1% selection ratio often
outperforms using the full dataset and beats the baseline selection methods by
1.5 points in F1 score on average.

摘要：微调特定任务的基础模型是现代机器学习中新兴的范例。特定任务微调的功效很大程度上取决于适当训练数据的选择。我们提出了一个框架，用于选择任务特定模型微调的数据，该框架由目标任务中的一组少量但有代表性的示例指导。为此，我们将特定任务微调的数据选择表述为一个优化问题，其中包含基于最优传输的分布对齐损失，以捕获所选数据和目标分布之间的差异。此外，我们添加了一个正则化项来鼓励所选数据的多样性，并将核密度估计纳入正则化项，以减少候选数据中近似重复的负面影响。我们将我们的优化问题与最近邻搜索联系起来，并设计了基于近似最近邻搜索技术的有效算法来计算最优解。我们评估了我们的方法在语言模型的持续预训练和指令微调的数据选择上的效果。我们表明，使用我们方法选择的数据进行指令微调，选择率为 1%，通常优于使用完整数据集，并且在 F1 得分上平均比基线选择方法高 1.5 分。

##### **Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs**
2410.11302v1 by Shuo Li, Tao Ji, Xiaoran Fan, Linsheng Lu, Leyi Yang, Yuming Yang, Zhiheng Xi, Rui Zheng, Yuran Wang, Xiaohui Zhao, Tao Gui, Qi Zhang, Xuanjing Huang

In the study of LLMs, sycophancy represents a prevalent hallucination that
poses significant challenges to these models. Specifically, LLMs often fail to
adhere to original correct responses, instead blindly agreeing with users'
opinions, even when those opinions are incorrect or malicious. However,
research on sycophancy in visual language models (VLMs) has been scarce. In
this work, we extend the exploration of sycophancy from LLMs to VLMs,
introducing the MM-SY benchmark to evaluate this phenomenon. We present
evaluation results from multiple representative models, addressing the gap in
sycophancy research for VLMs. To mitigate sycophancy, we propose a synthetic
dataset for training and employ methods based on prompts, supervised
fine-tuning, and DPO. Our experiments demonstrate that these methods
effectively alleviate sycophancy in VLMs. Additionally, we probe VLMs to assess
the semantic impact of sycophancy and analyze the attention distribution of
visual tokens. Our findings indicate that the ability to prevent sycophancy is
predominantly observed in higher layers of the model. The lack of attention to
image knowledge in these higher layers may contribute to sycophancy, and
enhancing image attention at high layers proves beneficial in mitigating this
issue.

摘要：在 LLM 的研究中，阿谀奉承代表了一种普遍存在的幻觉，对这些模型提出了重大挑战。具体而言，LLM 经常无法坚持最初的正确反应，而是盲目地同意用户的意见，即使这些意见不正确或带有恶意。然而，视觉语言模型 (VLM) 中的阿谀奉承研究却很少。在这项工作中，我们将对阿谀奉承的探索从 LLM 扩展到 VLM，引入了 MM-SY 基准来评估这种现象。我们展示了多个代表性模型的评估结果，解决了 VLM 中阿谀奉承研究的差距。为了减轻阿谀奉承，我们提出了一个用于训练的合成数据集，并采用了基于提示、监督微调和 DPO 的方法。我们的实验表明，这些方法有效地减轻了 VLM 中的阿谀奉承。此外，我们探测 VLM 以评估阿谀奉承的语义影响，并分析视觉标记的注意力分布。我们的研究结果表明，防止阿谀奉承的能力主要存在于模型的较高层。在这些较高层中缺乏对图像知识的关注可能会导致阿谀奉承，而在高层增强图像注意力被证明有助于减轻这个问题。

##### **Sorted Weight Sectioning for Energy-Efficient Unstructured Sparse DNNs on Compute-in-Memory Crossbars**
2410.11298v1 by Matheus Farias, H. T. Kung

We introduce $\textit{sorted weight sectioning}$ (SWS): a weight allocation
algorithm that places sorted deep neural network (DNN) weight sections on
bit-sliced compute-in-memory (CIM) crossbars to reduce analog-to-digital
converter (ADC) energy consumption. Data conversions are the most
energy-intensive process in crossbar operation. SWS effectively reduces this
cost leveraging (1) small weights and (2) zero weights (weight sparsity).
  DNN weights follow bell-shaped distributions, with most weights near zero.
Using SWS, we only need low-order crossbar columns for sections with
low-magnitude weights. This reduces the quantity and resolution of ADCs used,
exponentially decreasing ADC energy costs without significantly degrading DNN
accuracy.
  Unstructured sparsification further sharpens the weight distribution with
small accuracy loss. However, it presents challenges in hardware tracking of
zeros: we cannot switch zero rows to other layer weights in unsorted crossbars
without index matching. SWS efficiently addresses unstructured sparse models
using offline remapping of zeros into earlier sections, which reveals full
sparsity potential and maximizes energy efficiency.
  Our method reduces ADC energy use by 89.5% on unstructured sparse BERT
models. Overall, this paper introduces a novel algorithm to promote
energy-efficient CIM crossbars for unstructured sparse DNN workloads.

摘要：<paragraph>我們引進$\textit{排序權重分段}$（SWS）：一種權重分配演算法，將排序的深度神經網路（DNN）權重分段放置在位元切片記憶體中運算（CIM）交叉棒上，以降低類比數位轉換器（ADC）能耗。資料轉換是交叉棒運作中最耗能的程序。SWS 有效降低此成本，利用（1）小權重和（2）零權重（權重稀疏）。DNN 權重遵循鐘形分佈，大多數權重接近於零。使用 SWS，我們只需要低階交叉棒欄位，用於具有低幅度權重的分段。這減少了所用 ADC 的數量和解析度，指數級地降低了 ADC 能源成本，而不會顯著降低 DNN 的準確度。非結構化稀疏化進一步加劇了權重分佈，而準確度損失很小。然而，它在硬體追蹤零時提出了挑戰：我們無法在未排序的交叉棒中將零列切換到其他層權重，而沒有索引比對。SWS 有效地處理非結構化稀疏模型，使用離線將零重新對應到較早的分段中，這揭示了完全的稀疏可能性並最大化了能源效率。我們的演算法將非結構化稀疏 BERT 模型的 ADC 能源使用量減少了 89.5%。總體而言，本文介紹了一種新演算法，以推廣用於非結構化稀疏 DNN 工作負載的節能 CIM 交叉棒。</paragraph>

##### **TraM : Enhancing User Sleep Prediction with Transformer-based Multivariate Time Series Modeling and Machine Learning Ensembles**
2410.11293v1 by Jinjae Kim, Minjeong Ma, Eunjee Choi, Keunhee Cho, Chanwoo Lee

This paper presents a novel approach that leverages Transformer-based
multivariate time series model and Machine Learning Ensembles to predict the
quality of human sleep, emotional states, and stress levels. A formula to
calculate the labels was developed, and the various models were applied to user
data. Time Series Transformer was used for labels where time series
characteristics are crucial, while Machine Learning Ensembles were employed for
labels requiring comprehensive daily activity statistics. Time Series
Transformer excels in capturing the characteristics of time series through
pre-training, while Machine Learning Ensembles select machine learning models
that meet our categorization criteria. The proposed model, TraM, scored 6.10
out of 10 in experiments, demonstrating superior performance compared to other
methodologies. The code and configuration for the TraM framework are available
at: https://github.com/jin-jae/ETRI-Paper-Contest.

摘要：本文提出了一種新穎的方法，該方法利用基於 Transformer 的多變量時間序列模型和機器學習集成來預測人類睡眠品質、情緒狀態和壓力水平。開發了一個計算標籤的公式，並將各種模型應用於使用者資料。時間序列 Transformer 用於時間序列特徵至關重要的標籤，而機器學習集成則用於需要全面每日活動統計的標籤。時間序列 Transformer 通過預訓練在擷取時間序列特徵方面表現出色，而機器學習集成則選擇符合我們分類標準的機器學習模型。所提出的模型 TraM 在實驗中獲得 10 分中的 6.10 分，表現優於其他方法。TraM 框架的程式碼和組態可在 https://github.com/jin-jae/ETRI-Paper-Contest 取得。

##### **Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset Repository**
2410.11291v2 by S. Tamang, D. J. Bora

This paper introduces a centralized, open-source dataset repository designed
to advance NLP and NMT for Assamese, a low-resource language. The repository,
available at GitHub, supports various tasks like sentiment analysis, named
entity recognition, and machine translation by providing both pre-training and
fine-tuning corpora. We review existing datasets, highlighting the need for
standardized resources in Assamese NLP, and discuss potential applications in
AI-driven research, such as LLMs, OCR, and chatbots. While promising,
challenges like data scarcity and linguistic diversity remain. The repository
aims to foster collaboration and innovation, promoting Assamese language
research in the digital age.

摘要：本文介紹一個集中式、開放原始碼的資料集存放庫，旨在提升阿薩姆語的 NLP 和 NMT，阿薩姆語是一種低資源語言。此存放庫可於 GitHub 取得，支援各種任務，例如情緒分析、命名實體辨識和機器翻譯，並提供預訓練和微調語料庫。我們檢視現有資料集，強調阿薩姆語 NLP 中標準化資源的需求，並討論在 AI 驅動的研究中潛在的應用，例如 LLM、OCR 和聊天機器人。儘管前景看好，但資料稀少和語言多樣性等挑戰依然存在。此存放庫旨在促進協作和創新，推廣數位時代的阿薩姆語語言研究。

##### **Backdoor Attack on Vertical Federated Graph Neural Network Learning**
2410.11290v1 by Jirui Yang, Peng Chen, Zhihui Lu, Ruijun Deng, Qiang Duan, Jianping Zeng

Federated Graph Neural Network (FedGNN) is a privacy-preserving machine
learning technology that combines federated learning (FL) and graph neural
networks (GNNs). It offers a privacy-preserving solution for training GNNs
using isolated graph data. Vertical Federated Graph Neural Network (VFGNN) is
an important branch of FedGNN, where data features and labels are distributed
among participants, and each participant has the same sample space. Due to the
difficulty of accessing and modifying distributed data and labels, the
vulnerability of VFGNN to backdoor attacks remains largely unexplored. In this
context, we propose BVG, the first method for backdoor attacks in VFGNN.
Without accessing or modifying labels, BVG uses multi-hop triggers and requires
only four target class nodes for an effective backdoor attack. Experiments show
that BVG achieves high attack success rates (ASR) across three datasets and
three different GNN models, with minimal impact on main task accuracy (MTA). We
also evaluate several defense methods, further validating the robustness and
effectiveness of BVG. This finding also highlights the need for advanced
defense mechanisms to counter sophisticated backdoor attacks in practical VFGNN
applications.

摘要：聯邦圖形神經網路（FedGNN）是一種隱私保護機器
學習技術，它結合了聯邦學習（FL）和圖形神經
網路（GNN）。它提供了一個隱私保護的解決方案，用於訓練 GNN
使用孤立的圖形資料。垂直聯合圖形神經網路（VFGNN）是
FedGNN 的一個重要分支，其中資料特徵和標籤分佈
在參與者之間，每個參與者都有相同的樣本空間。由於
難以存取和修改分佈式資料和標籤，VFGNN 對後門攻擊的
脆弱性在很大程度上仍未被探索。在此背景下，我們提出 BVG，這
是 VFGNN 中後門攻擊的第一種方法。在不存取或修改標籤的情況下，
BVG 使用多跳觸發器，並且只需要四個目標類別節點即可進行有效的後門攻擊。
實驗表明，BVG 在三個資料集和三個不同的 GNN 模型中實現了很高的攻擊成功率 (ASR)，
對主要任務準確度 (MTA) 的影響最小。我們
還評估了幾種防禦方法，進一步驗證了 BVG 的穩健性和
有效性。這一發現也強調了在實際 VFGNN
應用中應對複雜後門攻擊的需要。

##### **Process Reward Model with Q-Value Rankings**
2410.11287v1 by Wendi Li, Yixuan Li

Process Reward Modeling (PRM) is critical for complex reasoning and
decision-making tasks where the accuracy of intermediate steps significantly
influences the overall outcome. Existing PRM approaches, primarily framed as
classification problems, employ cross-entropy loss to independently evaluate
each step's correctness. This method can lead to suboptimal reward distribution
and does not adequately address the interdependencies among steps. To address
these limitations, we introduce the Process Q-value Model (PQM), a novel
framework that redefines PRM in the context of a Markov Decision Process. PQM
optimizes Q-value rankings based on a novel comparative loss function,
enhancing the model's ability to capture the intricate dynamics among
sequential decisions. This approach provides a more granular and theoretically
grounded methodology for process rewards. Our extensive empirical evaluations
across various sampling policies, language model backbones, and multi-step
reasoning benchmarks show that PQM outperforms classification-based PRMs. The
effectiveness of the comparative loss function is highlighted in our
comprehensive ablation studies, confirming PQM's practical efficacy and
theoretical advantage.

摘要：處理獎勵建模 (PRM) 對於複雜的推理和決策制定任務至關重要，其中間步驟的準確性會顯著影響整體結果。現有的 PRM 方法主要被視為分類問題，採用交叉熵損失來獨立評估每個步驟的正確性。此方法可能導致次優的獎勵分配，且無法充分解決步驟之間的相互依賴性。為了解決這些限制，我們引入了處理 Q 值模型 (PQM)，這是一個新的框架，在馬可夫決策過程中重新定義了 PRM。PQM 根據新的比較損失函數優化 Q 值排名，增強了模型捕捉序列決策之間複雜動態的能力。這種方法為處理獎勵提供了一個更精細且理論上更紮實的方法。我們在各種抽樣策略、語言模型主幹和多步驟推理基準上的廣泛實證評估表明，PQM 優於基於分類的 PRM。在我們的全面消融研究中突出了比較損失函數的有效性，證實了 PQM 的實用功效和理論優勢。

##### **Advancing the Understanding of Fixed Point Iterations in Deep Neural Networks: A Detailed Analytical Study**
2410.11279v1 by Yekun Ke, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song

Recent empirical studies have identified fixed point iteration phenomena in
deep neural networks, where the hidden state tends to stabilize after several
layers, showing minimal change in subsequent layers. This observation has
spurred the development of practical methodologies, such as accelerating
inference by bypassing certain layers once the hidden state stabilizes,
selectively fine-tuning layers to modify the iteration process, and
implementing loops of specific layers to maintain fixed point iterations.
Despite these advancements, the understanding of fixed point iterations remains
superficial, particularly in high-dimensional spaces, due to the inadequacy of
current analytical tools. In this study, we conduct a detailed analysis of
fixed point iterations in a vector-valued function modeled by neural networks.
We establish a sufficient condition for the existence of multiple fixed points
of looped neural networks based on varying input regions. Additionally, we
expand our examination to include a robust version of fixed point iterations.
To demonstrate the effectiveness and insights provided by our approach, we
provide case studies that looped neural networks may exist $2^d$ number of
robust fixed points under exponentiation or polynomial activation functions,
where $d$ is the feature dimension. Furthermore, our preliminary empirical
results support our theoretical findings. Our methodology enriches the toolkit
available for analyzing fixed point iterations of deep neural networks and may
enhance our comprehension of neural network mechanisms.

摘要：最近的經驗研究發現深度神經網路中的定點迭代現象，其中隱藏狀態傾向於在多層後穩定，在後續層中顯示出最小的變化。這一觀察激勵了實用方法的發展，例如，通過在隱藏狀態穩定後繞過某些層來加速推理，有選擇地微調層以修改迭代過程，以及實施特定層的迴圈以維護定點迭代。儘管有這些進展，但由於當前分析工具的不足，對定點迭代的理解仍然很膚淺，特別是在高維空間中。在本研究中，我們對神經網路建模的向量值函數中的定點迭代進行了詳細分析。我們根據不同的輸入區域建立了迴圈神經網路存在多個定點的充分條件。此外，我們擴展了我們的檢查，以包括定點迭代的健壯版本。為了證明我們的方法提供的有效性和見解，我們提供了案例研究，說明迴圈神經網路可能存在 $2^d$ 個指數或多項式激活函數下的穩健定點，其中 $d$ 是特徵維度。此外，我們的初步經驗結果支持我們的理論發現。我們的 methodology 豐富了可用于分析深度神經網路定點迭代的工具包，並可能增強我們對神經網路機制的理解。

##### **ILAEDA: An Imitation Learning Based Approach for Automatic Exploratory Data Analysis**
2410.11276v1 by Abhijit Manatkar, Devarsh Patel, Hima Patel, Naresh Manwani

Automating end-to-end Exploratory Data Analysis (AutoEDA) is a challenging
open problem, often tackled through Reinforcement Learning (RL) by learning to
predict a sequence of analysis operations (FILTER, GROUP, etc). Defining
rewards for each operation is a challenging task and existing methods rely on
various \emph{interestingness measures} to craft reward functions to capture
the importance of each operation. In this work, we argue that not all of the
essential features of what makes an operation important can be accurately
captured mathematically using rewards. We propose an AutoEDA model trained
through imitation learning from expert EDA sessions, bypassing the need for
manually defined interestingness measures. Our method, based on generative
adversarial imitation learning (GAIL), generalizes well across datasets, even
with limited expert data. We also introduce a novel approach for generating
synthetic EDA demonstrations for training. Our method outperforms the existing
state-of-the-art end-to-end EDA approach on benchmarks by upto 3x, showing
strong performance and generalization, while naturally capturing diverse
interestingness measures in generated EDA sessions.

摘要：自動化端到端探索性資料分析 (AutoEDA) 是個具有挑戰性的開放性問題，常透過強化學習 (RL) 來解決，藉由學習預測一系列分析操作 (FILTER、GROUP 等) 來解決。定義每個操作的獎勵是項具有挑戰性的任務，現有方法仰賴各種「趣味性指標」來建立獎勵函數，以捕捉每個操作的重要性。在這項工作中，我們主張並非所有讓操作重要的基本特徵都能使用獎勵以數學方式精確捕捉。我們提出一個透過模仿學習專家 EDA 會話來訓練的 AutoEDA 模型，繞過手動定義趣味性指標的需求。我們的方法基於生成對抗模仿學習 (GAIL)，即使專家資料有限，也能在各種資料集之間進行良好的概化。我們也引入一種新穎的方法來產生用於訓練的合成 EDA 示範。我們的做法在基準上勝過現有的最先進端到端 EDA 方法達 3 倍，展現強大的效能和概化能力，同時自然地捕捉到生成 EDA 會話中多樣化的趣味性指標。

##### **Cognitive Overload Attack:Prompt Injection for Long Context**
2410.11272v1 by Bibek Upadhayay, Vahid Behzadan, Amin Karbasi

Large Language Models (LLMs) have demonstrated remarkable capabilities in
performing tasks across various domains without needing explicit retraining.
This capability, known as In-Context Learning (ICL), while impressive, exposes
LLMs to a variety of adversarial prompts and jailbreaks that manipulate
safety-trained LLMs into generating undesired or harmful output. In this paper,
we propose a novel interpretation of ICL in LLMs through the lens of cognitive
neuroscience, by drawing parallels between learning in human cognition with
ICL. We applied the principles of Cognitive Load Theory in LLMs and empirically
validate that similar to human cognition, LLMs also suffer from cognitive
overload a state where the demand on cognitive processing exceeds the available
capacity of the model, leading to potential errors. Furthermore, we
demonstrated how an attacker can exploit ICL to jailbreak LLMs through
deliberately designed prompts that induce cognitive overload on LLMs, thereby
compromising the safety mechanisms of LLMs. We empirically validate this threat
model by crafting various cognitive overload prompts and show that advanced
models such as GPT-4, Claude-3.5 Sonnet, Claude-3 OPUS, Llama-3-70B-Instruct,
Gemini-1.0-Pro, and Gemini-1.5-Pro can be successfully jailbroken, with attack
success rates of up to 99.99%. Our findings highlight critical vulnerabilities
in LLMs and underscore the urgency of developing robust safeguards. We propose
integrating insights from cognitive load theory into the design and evaluation
of LLMs to better anticipate and mitigate the risks of adversarial attacks. By
expanding our experiments to encompass a broader range of models and by
highlighting vulnerabilities in LLMs' ICL, we aim to ensure the development of
safer and more reliable AI systems.

摘要：大型語言模型 (LLM) 已展現出在執行各種領域任務時，無需明確重新訓練的顯著能力。這種稱為情境中學習 (ICL) 的能力令人印象深刻，但會讓 LLM 暴露於各種對抗提示和越獄，這些提示和越獄會操縱安全訓練的 LLM，讓它們產生不受歡迎或有害的輸出。在本文中，我們透過認知神經科學的觀點，提出對 LLM 中 ICL 的新詮釋，方法是將人類認知中的學習與 ICL 進行類比。我們將認知負載理論的原則應用於 LLM，並透過實證驗證，與人類認知類似，LLM 也會遭受認知超載，這是一種認知處理需求超過模型可用容量的狀態，可能導致錯誤。此外，我們展示了攻擊者如何利用 ICL 透過精心設計的提示，在 LLM 上誘發認知超載，進而越獄，從而危害 LLM 的安全機制。我們透過製作各種認知超載提示，並展示 GPT-4、Claude-3.5 Sonnet、Claude-3 OPUS、Llama-3-70B-Instruct、Gemini-1.0-Pro 和 Gemini-1.5-Pro 等先進模型可以成功越獄，攻擊成功率高達 99.99%，從而實證驗證了這個威脅模型。我們的發現突顯了 LLM 中的嚴重漏洞，並強調了開發強大防護措施的迫切性。我們建議將認知負載理論的見解整合到 LLM 的設計和評估中，以更好地預測和減輕對抗攻擊的風險。透過擴展我們的實驗，涵蓋更廣泛的模型，並強調 LLM 的 ICL 中的漏洞，我們旨在確保開發更安全、更可靠的 AI 系統。

##### **Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent**
2410.11268v1 by Bo Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song

In-context learning has been recognized as a key factor in the success of
Large Language Models (LLMs). It refers to the model's ability to learn
patterns on the fly from provided in-context examples in the prompt during
inference. Previous studies have demonstrated that the Transformer architecture
used in LLMs can implement a single-step gradient descent update by processing
in-context examples in a single forward pass. Recent work has further shown
that, during in-context learning, a looped Transformer can implement multi-step
gradient descent updates in forward passes. However, their theoretical results
require an exponential number of in-context examples, $n = \exp(\Omega(T))$,
where $T$ is the number of loops or passes, to achieve a reasonably low error.
In this paper, we study linear looped Transformers in-context learning on
linear vector generation tasks. We show that linear looped Transformers can
implement multi-step gradient descent efficiently for in-context learning. Our
results demonstrate that as long as the input data has a constant condition
number, e.g., $n = O(d)$, the linear looped Transformers can achieve a small
error by multi-step gradient descent during in-context learning. Furthermore,
our preliminary experiments validate our theoretical analysis. Our findings
reveal that the Transformer architecture possesses a stronger in-context
learning capability than previously understood, offering new insights into the
mechanisms behind LLMs and potentially guiding the better design of efficient
inference algorithms for LLMs.

摘要：在情境學習中，被認為是大型語言模型 (LLM) 成功關鍵因素。它指的是模型在推理期間，從提示中提供的上下文範例中即時學習模式的能力。先前的研究已證明，LLM 中使用的 Transformer 架構，可以透過在單次前向傳遞中處理上下文範例，來實作單步梯度下降更新。最近的研究進一步顯示，在情境學習期間，迴圈 Transformer 可以透過前向傳遞，來實作多步梯度下降更新。然而，他們的理論結果需要指數個的上下文範例，$n = \exp(\Omega(T))$，其中 $T$ 是迴圈或傳遞次數，才能達到相當低的誤差。在本文中，我們研究線性迴圈 Transformer 在線性向量產生任務中的情境學習。我們顯示，線性迴圈 Transformer 可以有效地實作多步梯度下降，以進行情境學習。我們的結果證明，只要輸入資料具有常數條件數，例如 $n = O(d)$，線性迴圈 Transformer 就可以透過多步梯度下降，在情境學習期間達到較小的誤差。此外，我們的初步實驗驗證了我們的理論分析。我們的發現顯示，Transformer 架構具備比先前理解更強的情境學習能力，為 LLM 背後的機制提供了新的見解，並潛在地指導更有效率的 LLM 推論演算法的設計。

##### **FedCCRL: Federated Domain Generalization with Cross-Client Representation Learning**
2410.11267v2 by Xinpeng Wang, Xiaoying Tang

Domain Generalization (DG) aims to train models that can effectively
generalize to unseen domains. However, in the context of Federated Learning
(FL), where clients collaboratively train a model without directly sharing
their data, most existing DG algorithms are not directly applicable to the FL
setting due to privacy constraints, as well as the limited data quantity and
domain diversity at each client. To tackle these challenges, we propose
FedCCRL, a novel federated domain generalization method that significantly
improves the model's ability to generalize to unseen domains without
compromising privacy or incurring excessive computational and communication
costs. Specifically, we adapt MixStyle to the federated setting to transfer
domain-specific features while AugMix is employed to perturb domain-invariant
features. Furthermore, we leverage supervised contrastive loss for
representation alignment and utilize Jensen-Shannon divergence to ensure
consistent predictions between original and augmented samples. Extensive
experimental results demonstrate that FedCCRL achieves the state-of-the-art
performances on the PACS, OfficeHome and miniDomainNet datasets across varying
numbers of clients. Code is available at
https://github.com/SanphouWang/FedCCRL.

摘要：領域泛化 (DG) 的目標是訓練模型，讓模型能有效泛化到未見過的領域。然而，在聯合學習 (FL) 的背景下，客戶端在不直接分享資料的情況下合作訓練模型，由於隱私限制以及每個客戶端有限的資料量和領域多樣性，現有的 DG 演算法大多無法直接應用於 FL 設定。為了解決這些挑戰，我們提出 FedCCRL，這是一種新穎的聯合領域泛化方法，它能顯著提升模型泛化到未見過領域的能力，同時不損害隱私或產生過高的運算和通訊成本。具體來說，我們將 MixStyle 調整到聯合設定，以轉移領域特定特徵，同時採用 AugMix 來擾動領域不變特徵。此外，我們利用監督對比損失進行表示對齊，並利用 Jensen-Shannon 距離來確保原始和擴充樣本之間的預測一致。廣泛的實驗結果表明，FedCCRL 在 PACS、OfficeHome 和 miniDomainNet 資料集上實現了最先進的效能，而且在不同數量的客戶端上都是如此。程式碼可在 https://github.com/SanphouWang/FedCCRL 取得。

##### **In-Context Learning for Long-Context Sentiment Analysis on Infrastructure Project Opinions**
2410.11265v1 by Alireza Shamshiri, Kyeong Rok Ryu, June Young Park

Large language models (LLMs) have achieved impressive results across various
tasks. However, they still struggle with long-context documents. This study
evaluates the performance of three leading LLMs: GPT-4o, Claude 3.5 Sonnet, and
Gemini 1.5 Pro on lengthy, complex, and opinion-varying documents concerning
infrastructure projects, under both zero-shot and few-shot scenarios. Our
results indicate that GPT-4o excels in zero-shot scenarios for simpler, shorter
documents, while Claude 3.5 Sonnet surpasses GPT-4o in handling more complex,
sentiment-fluctuating opinions. In few-shot scenarios, Claude 3.5 Sonnet
outperforms overall, while GPT-4o shows greater stability as the number of
demonstrations increases.

摘要：大型語言模型 (LLM) 在各種任務中都取得了令人印象深刻的成果。然而，它們在處理長文本文件時仍存在困難。本研究評估了三種領先的 LLM：GPT-4o、Claude 3.5 Sonnet 和 Gemini 1.5 Pro 在涉及基礎建設項目的冗長、複雜且觀點多變的文件上的表現，包括零次學習和少次學習場景。我們的結果表明，GPT-4o 在零次學習場景中表現出色，適用於較簡單、較短的文件，而 Claude 3.5 Sonnet 在處理更複雜、情緒波動的觀點時優於 GPT-4o。在少次學習場景中，Claude 3.5 Sonnet 整體表現優異，而 GPT-4o 隨著示範次數的增加表現出更高的穩定性。

##### **Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix**
2410.11261v1 by Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Yufa Zhou

Large Language Models (LLMs) have shown immense potential in enhancing
various aspects of our daily lives, from conversational AI to search and AI
assistants. However, their growing capabilities come at the cost of extremely
large model sizes, making deployment on edge devices challenging due to memory
and computational constraints. This paper introduces a novel approach to LLM
weight pruning that directly optimizes for approximating the attention matrix,
a core component of transformer architectures. Unlike existing methods that
focus on linear approximations, our approach accounts for the non-linear nature
of the Softmax attention mechanism. We provide theoretical guarantees for the
convergence of our Gradient Descent-based optimization method to a near-optimal
pruning mask solution. Our preliminary empirical results demonstrate the
effectiveness of this approach in maintaining model performance while
significantly reducing computational costs. This work establishes a new
theoretical foundation for pruning algorithm design in LLMs, potentially paving
the way for more efficient LLM inference on resource-constrained devices.

摘要：大型語言模型 (LLM) 已展現出極大的潛力，可增強我們日常生活中的各個面向，從對話式 AI 到搜尋和 AI 助理。然而，它們日益增長的效能是以極大的模型規模為代價，這使得在邊緣裝置上部署時面臨記憶體和運算限制的挑戰。本文介紹了一種創新的 LLM 權重剪枝方法，可直接針對近似注意矩陣進行最佳化，而注意矩陣是 Transformer 架構的核心元件。與專注於線性近似的現有方法不同，我們的做法考量了 Softmax 注意力機制的非線性性質。我們為基於梯度下降的最佳化方法提供了理論保證，以收斂到近乎最佳的剪枝遮罩解。我們的初步實證結果證明了此方法在維持模型效能的同時，大幅降低運算成本的有效性。這項工作為 LLM 中的剪枝演算法設計建立了新的理論基礎，可能為在資源受限的裝置上進行更有效率的 LLM 推論鋪路。

##### **Learning Agents With Prioritization and Parameter Noise in Continuous State and Action Space**
2410.11250v1 by Rajesh Mangannavar, Gopalakrishnan Srinivasaraghavan

Among the many variants of RL, an important class of problems is where the
state and action spaces are continuous -- autonomous robots, autonomous
vehicles, optimal control are all examples of such problems that can lend
themselves naturally to reinforcement based algorithms, and have continuous
state and action spaces. In this paper, we introduce a prioritized form of a
combination of state-of-the-art approaches such as Deep Q-learning (DQN) and
Deep Deterministic Policy Gradient (DDPG) to outperform the earlier results for
continuous state and action space problems. Our experiments also involve the
use of parameter noise during training resulting in more robust deep RL models
outperforming the earlier results significantly. We believe these results are a
valuable addition for continuous state and action space problems.

摘要：在眾多的 RL 變種中，一個重要的問題類別是狀態和動作空間是連續的——自駕機器人、自駕車輛、最佳控制都是此類問題的範例，這些問題可以自然而然地借用強化學習演算法，並具有連續的狀態和動作空間。在本文中，我們引入了最先進方法的優先形式，例如 Deep Q 學習 (DQN) 和深度確定性策略梯度 (DDPG)，以優於連續狀態和動作空間問題的先前結果。我們的實驗還包括在訓練期間使用參數雜訊，從而產生更強大的深度 RL 模型，顯著優於先前的結果。我們相信這些結果對連續狀態和動作空間問題是有價值的補充。

