{"2405.21070": {"publish_time": "2024-05-31", "title": "Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights", "paper_summary": "Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode will be available at: https://github.com/CVMI-Lab/clip-beyond-tail.", "paper_summary_zh": "\u56b4\u91cd\u7684\u8cc7\u6599\u4e0d\u5e73\u8861\u81ea\u7136\u5b58\u5728\u65bc\u7db2\u8def\u898f\u6a21\u7684\u8996\u89ba\u8a9e\u8a00\u8cc7\u6599\u96c6\u4e2d\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u6211\u5011\u767c\u73fe\u9810\u5148\u8a13\u7df4\u65bc\u5176\u4e0a\u7684 CLIP \u5c0d\u8cc7\u6599\u4e0d\u5e73\u8861\u5c55\u73fe\u51fa\u986f\u8457\u7684\u7a69\u5065\u6027\uff0c\u8207\u76e3\u7763\u5b78\u7fd2\u76f8\u6bd4\uff0c\u4e26\u5c55\u73fe\u51fa\u5b78\u7fd2\u53ef\u6982\u5316\u8868\u5fb5\u7684\u986f\u8457\u6548\u80fd\u3002\u70ba\u4e86\u8abf\u67e5\u6b64\u767c\u73fe\u80cc\u5f8c\u7684\u539f\u56e0\uff0c\u6211\u5011\u9032\u884c\u53d7\u63a7\u5be6\u9a57\u4ee5\u7814\u7a76\u5404\u7a2e\u6f5b\u5728\u56e0\u7d20\uff0c\u4e26\u63ed\u793a CLIP \u7684\u85c9\u53e3\u4efb\u52d9\u5f62\u6210\u4e00\u500b\u52d5\u614b\u5206\u985e\u554f\u984c\uff0c\u5176\u4e2d\u53ea\u6709\u4e00\u500b\u5b50\u96c6\u7684\u985e\u5225\u5b58\u5728\u65bc\u8a13\u7df4\u4e2d\u3002\u9019\u6703\u5f9e\u4e3b\u8981\u985e\u5225\u4e2d\u5206\u96e2\u51fa\u504f\u5dee\uff0c\u4e26\u96b1\u542b\u5730\u5e73\u8861\u5b78\u7fd2\u8a0a\u865f\u3002\u6b64\u5916\uff0cCLIP \u7684\u7a69\u5065\u6027\u548c\u8fa8\u5225\u529b\u6703\u96a8\u8457\u66f4\u5177\u63cf\u8ff0\u6027\u7684\u8a9e\u8a00\u76e3\u7763\u3001\u66f4\u5927\u7684\u8cc7\u6599\u898f\u6a21\u548c\u66f4\u5ee3\u6cdb\u7684\u958b\u653e\u4e16\u754c\u6982\u5ff5\u800c\u6539\u5584\uff0c\u800c\u9019\u4e9b\u6982\u5ff5\u5c0d\u65bc\u76e3\u7763\u5b78\u7fd2\u4f86\u8aaa\u662f\u7121\u6cd5\u53d6\u5f97\u7684\u3002\u6211\u5011\u7684\u7814\u7a76\u4e0d\u50c5\u63ed\u793a\u4e86 CLIP \u5728\u8cc7\u6599\u4e0d\u5e73\u8861\u4e4b\u5916\u7684\u6982\u5316\u80fd\u529b\u80cc\u5f8c\u7684\u6a5f\u5236\uff0c\u4e5f\u70ba\u7814\u7a76\u793e\u7fa4\u63d0\u4f9b\u4e86\u53ef\u8f49\u79fb\u7684\u898b\u89e3\u3002\u9019\u4e9b\u767c\u73fe\u5df2\u5728\u76e3\u7763\u5f0f\u548c\u81ea\u76e3\u7763\u5f0f\u5b78\u7fd2\u4e2d\u5f97\u5230\u9a57\u8b49\uff0c\u8b93\u5728\u4e0d\u5e73\u8861\u8cc7\u6599\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u80fd\u5920\u5728\u5404\u7a2e\u8fa8\u8b58\u4efb\u52d9\u4e2d\u9054\u5230 CLIP \u7b49\u7d1a\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u5c07\u65bc\u4e0b\u5217\u4f4d\u7f6e\u63d0\u4f9b\uff1ahttps://github.com/CVMI-Lab/clip-beyond-tail\u3002", "author": "Xin Wen et.al.", "authors": "Xin Wen, Bingchen Zhao, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi", "id": "2405.21070v1", "paper_url": "http://arxiv.org/abs/2405.21070v1", "repo": "https://github.com/cvmi-lab/clip-beyond-tail"}}