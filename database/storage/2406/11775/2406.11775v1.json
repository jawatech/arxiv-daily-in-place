{"2406.11775": {"publish_time": "2024-06-17", "title": "Task Me Anything", "paper_summary": "Benchmarks for large multimodal language models (MLMs) now serve to\nsimultaneously assess the general capabilities of models instead of evaluating\nfor a specific capability. As a result, when a developer wants to identify\nwhich models to use for their application, they are overwhelmed by the number\nof benchmarks and remain uncertain about which benchmark's results are most\nreflective of their specific use case. This paper introduces Task-Me-Anything,\na benchmark generation engine which produces a benchmark tailored to a user's\nneeds. Task-Me-Anything maintains an extendable taxonomy of visual assets and\ncan programmatically generate a vast number of task instances. Additionally, it\nalgorithmically addresses user queries regarding MLM performance efficiently\nwithin a computational budget. It contains 113K images, 10K videos, 2K 3D\nobject assets, over 365 object categories, 655 attributes, and 335\nrelationships. It can generate 750M image/video question-answering pairs, which\nfocus on evaluating MLM perceptual capabilities. Task-Me-Anything reveals\ncritical insights: open-source MLMs excel in object and attribute recognition\nbut lack spatial and temporal understanding; each model exhibits unique\nstrengths and weaknesses; larger models generally perform better, though\nexceptions exist; and GPT4o demonstrates challenges in recognizing\nrotating/moving objects and distinguishing colors.", "paper_summary_zh": "\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b (MLM) \u7684\u57fa\u51c6\u73b0\u5728\u53ef\u7528\u4e8e\u540c\u65f6\u8bc4\u4f30\u6a21\u578b\u7684\u901a\u7528\u529f\u80fd\uff0c\u800c\u4e0d\u662f\u8bc4\u4f30\u7279\u5b9a\u529f\u80fd\u3002\u56e0\u6b64\uff0c\u5f53\u5f00\u53d1\u4eba\u5458\u60f3\u8981\u786e\u5b9a\u4e3a\u5176\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u54ea\u4e9b\u6a21\u578b\u65f6\uff0c\u4ed6\u4eec\u4f1a\u88ab\u57fa\u51c6\u6570\u91cf\u6240\u6df9\u6ca1\uff0c\u5e76\u4e14\u4ecd\u7136\u4e0d\u786e\u5b9a\u54ea\u4e2a\u57fa\u51c6\u7684\u7ed3\u679c\u6700\u80fd\u53cd\u6620\u5176\u7279\u5b9a\u7528\u4f8b\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Task-Me-Anything\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u51c6\u751f\u6210\u5f15\u64ce\uff0c\u53ef\u4ee5\u6839\u636e\u7528\u6237\u7684\u9700\u6c42\u751f\u6210\u5b9a\u5236\u57fa\u51c6\u3002Task-Me-Anything \u7ef4\u62a4\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u53ef\u89c6\u8d44\u4ea7\u5206\u7c7b\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ee5\u7f16\u7a0b\u65b9\u5f0f\u751f\u6210\u5927\u91cf\u4efb\u52a1\u5b9e\u4f8b\u3002\u6b64\u5916\uff0c\u5b83\u5728\u8ba1\u7b97\u9884\u7b97\u5185\u901a\u8fc7\u7b97\u6cd5\u9ad8\u6548\u5730\u5904\u7406\u7528\u6237\u5173\u4e8e MLM \u6027\u80fd\u7684\u67e5\u8be2\u3002\u5b83\u5305\u542b 113K \u5f20\u56fe\u50cf\u300110K \u4e2a\u89c6\u9891\u30012K \u4e2a 3D \u5bf9\u8c61\u8d44\u4ea7\u3001\u8d85\u8fc7 365 \u4e2a\u5bf9\u8c61\u7c7b\u522b\u3001655 \u4e2a\u5c5e\u6027\u548c 335 \u4e2a\u5173\u7cfb\u3002\u5b83\u53ef\u4ee5\u751f\u6210 750M \u4e2a\u56fe\u50cf/\u89c6\u9891\u95ee\u7b54\u5bf9\uff0c\u91cd\u70b9\u662f\u8bc4\u4f30 MLM \u611f\u77e5\u80fd\u529b\u3002Task-Me-Anything \u63ed\u793a\u4e86\u5173\u952e\u89c1\u89e3\uff1a\u5f00\u6e90 MLM \u5728\u5bf9\u8c61\u548c\u5c5e\u6027\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u7a7a\u95f4\u548c\u65f6\u95f4\u7406\u89e3\uff1b\u6bcf\u4e2a\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u72ec\u7279\u7684\u4f18\u52bf\u548c\u52a3\u52bf\uff1b\u8f83\u5927\u7684\u6a21\u578b\u901a\u5e38\u8868\u73b0\u5f97\u66f4\u597d\uff0c\u5c3d\u7ba1\u5b58\u5728\u4f8b\u5916\uff1bGPT4o \u5728\u8bc6\u522b\u65cb\u8f6c/\u79fb\u52a8\u7269\u4f53\u548c\u533a\u5206\u989c\u8272\u65b9\u9762\u8868\u73b0\u51fa\u6311\u6218\u3002", "author": "Jieyu Zhang et.al.", "authors": "Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, Ranjay Krishna", "id": "2406.11775v1", "paper_url": "http://arxiv.org/abs/2406.11775v1", "repo": "https://github.com/jieyuz2/taskmeanything"}}