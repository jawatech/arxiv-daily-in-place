{"2406.11430": {"publish_time": "2024-06-17", "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression", "paper_summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u90e8\u7f72\u901a\u5e38\u53d7\u5230 Key-Value (KV) \u5feb\u53d6\u5ee3\u6cdb\u7684\u8a18\u61b6\u9ad4\u9700\u6c42\u6240\u963b\u7919\uff0c\u7279\u5225\u662f\u5728\u8108\u7d61\u9577\u5ea6\u589e\u52a0\u6642\u3002\u73fe\u6709\u7684\u6e1b\u5c11 KV \u5feb\u53d6\u5927\u5c0f\u7684\u65b9\u6cd5\u5305\u62ec\u5fae\u8abf\u6a21\u578b\u4ee5\u5b78\u7fd2\u58d3\u7e2e\u7b56\u7565\u6216\u5229\u7528\u6ce8\u610f\u529b\u5206\u6578\u4f86\u6e1b\u5c11\u5e8f\u5217\u9577\u5ea6\u3002\u6211\u5011\u5206\u6790\u4e86\u50c5\u89e3\u78bc\u5668 Transformer \u57fa\u790e\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u4f48\uff0c\u4e26\u89c0\u5bdf\u5230\u6ce8\u610f\u529b\u5206\u914d\u6a21\u5f0f\u5728\u5927\u90e8\u5206\u5c64\u4e2d\u4fdd\u6301\u4e00\u81f4\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe $L_2$ \u548c\u5feb\u53d6 KV \u5c0d\u7684\u6ce8\u610f\u529b\u5206\u6578\u4e4b\u9593\u5b58\u5728\u660e\u986f\u7684\u95dc\u806f\u6027\uff0c\u5176\u4e2d\u9375\u5d4c\u5165\u7684\u4f4e $L_2$ \u901a\u5e38\u6703\u5c0e\u81f4\u89e3\u78bc\u671f\u9593\u7684\u9ad8\u6ce8\u610f\u529b\u5206\u6578\u3002\u9019\u4e00\u767c\u73fe\u8868\u660e\uff0cKV \u5c0d\u7684\u5f71\u97ff\u53ef\u80fd\u5728\u67e5\u8a62\u4e4b\u524d\u5c31\u7531\u9375\u5d4c\u5165\u672c\u8eab\u6c7a\u5b9a\u3002\u57fa\u65bc\u9019\u4e00\u89c0\u5bdf\uff0c\u6211\u5011\u6839\u64da\u9375\u5d4c\u5165\u7684 $L_2$ \u58d3\u7e2e KV \u5feb\u53d6\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u9019\u7a2e\u7c21\u55ae\u7684\u7b56\u7565\u53ef\u4ee5\u5728\u8a9e\u8a00\u5efa\u6a21\u548c\u5927\u6d77\u6488\u91dd\u4efb\u52d9\u4e2d\u5c07 KV \u5feb\u53d6\u5927\u5c0f\u6e1b\u5c11 50%\uff0c\u5728\u5bc6\u9470\u6aa2\u7d22\u4efb\u52d9\u4e2d\u6e1b\u5c11 90%\uff0c\u800c\u4e0d\u6703\u964d\u4f4e\u6e96\u78ba\u5ea6\u3002", "author": "Alessio Devoto et.al.", "authors": "Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini", "id": "2406.11430v1", "paper_url": "http://arxiv.org/abs/2406.11430v1", "repo": "null"}}