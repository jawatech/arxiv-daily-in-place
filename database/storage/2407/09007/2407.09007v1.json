{"2407.09007": {"publish_time": "2024-07-12", "title": "Benchmarking Language Model Creativity: A Case Study on Code Generation", "paper_summary": "As LLMs become increasingly prevalent, it is interesting to consider how\n``creative'' these models can be. From cognitive science, creativity consists\nof at least two key characteristics: \\emph{convergent} thinking (purposefulness\nto achieve a given goal) and \\emph{divergent} thinking (adaptability to new\nenvironments or constraints) \\citep{runco2003critical}. In this work, we\nintroduce a framework for quantifying LLM creativity that incorporates the two\ncharacteristics. This is achieved by (1) Denial Prompting pushes LLMs to come\nup with more creative solutions to a given problem by incrementally imposing\nnew constraints on the previous solution, compelling LLMs to adopt new\nstrategies, and (2) defining and computing the NeoGauge metric which examines\nboth convergent and divergent thinking in the generated creative responses by\nLLMs. We apply the proposed framework on Codeforces problems, a natural data\nsource for collecting human coding solutions. We quantify NeoGauge for various\nproprietary and open-source models and find that even the most creative model,\nGPT-4, still falls short of demonstrating human-like creativity. We also\nexperiment with advanced reasoning strategies (MCTS, self-correction, etc.) and\nobserve no significant improvement in creativity. As a by-product of our\nanalysis, we release NeoCoder dataset for reproducing our results on future\nmodels.", "paper_summary_zh": "\u96a8\u8457 LLM \u8b8a\u5f97\u8d8a\u4f86\u8d8a\u666e\u904d\uff0c\u601d\u8003\u9019\u4e9b\u6a21\u578b\u53ef\u4ee5\u6709\u591a\u300c\u6709\u5275\u610f\u300d\u662f\u4e00\u4ef6\u5f88\u6709\u8da3\u7684\u4e8b\u3002\u5f9e\u8a8d\u77e5\u79d1\u5b78\u7684\u89d2\u5ea6\u4f86\u770b\uff0c\u5275\u610f\u81f3\u5c11\u5305\u542b\u5169\u500b\u95dc\u9375\u7279\u5fb5\uff1a\\emph{\u6536\u6582}\u601d\u8003\uff08\u5be6\u73fe\u7279\u5b9a\u76ee\u6a19\u7684\u76ee\u6a19\u6027\uff09\u548c\\emph{\u767c\u6563}\u601d\u8003\uff08\u9069\u61c9\u65b0\u74b0\u5883\u6216\u9650\u5236\uff09\\citep{runco2003critical}\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u91cf\u5316 LLM \u5275\u610f\u7684\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u9019\u5169\u500b\u7279\u5fb5\u3002\u9019\u53ef\u4ee5\u900f\u904e (1) \u5426\u5b9a\u63d0\u793a\u8b93 LLM \u900f\u904e\u9010\u6b65\u5c0d\u5148\u524d\u7684\u89e3\u6c7a\u65b9\u6848\u65bd\u52a0\u65b0\u7684\u9650\u5236\uff0c\u63d0\u51fa\u66f4\u591a\u6709\u5275\u610f\u7684\u89e3\u6c7a\u65b9\u6848\u4f86\u89e3\u6c7a\u7279\u5b9a\u554f\u984c\uff0c\u8feb\u4f7f LLM \u63a1\u7528\u65b0\u7684\u7b56\u7565\uff0c\u4ee5\u53ca (2) \u5b9a\u7fa9\u548c\u8a08\u7b97 NeoGauge \u6307\u6a19\uff0c\u5b83\u6703\u6aa2\u67e5 LLM \u751f\u6210\u7684\u5275\u610f\u56de\u61c9\u4e2d\u7684\u6536\u6582\u548c\u767c\u6563\u601d\u8003\u3002\u6211\u5011\u5728 Codeforces \u554f\u984c\u4e0a\u61c9\u7528\u6240\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u9019\u662f\u6536\u96c6\u4eba\u985e\u7de8\u78bc\u89e3\u6c7a\u65b9\u6848\u7684\u81ea\u7136\u8cc7\u6599\u4f86\u6e90\u3002\u6211\u5011\u91cf\u5316\u4e86\u5404\u7a2e\u5c08\u6709\u548c\u958b\u6e90\u6a21\u578b\u7684 NeoGauge\uff0c\u767c\u73fe\u5373\u4f7f\u662f\u6700\u6709\u5275\u610f\u7684\u6a21\u578b GPT-4\uff0c\u4ecd\u672a\u9054\u5230\u4eba\u985e\u822c\u7684\u5275\u610f\u3002\u6211\u5011\u4e5f\u5617\u8a66\u4e86\u9032\u968e\u63a8\u7406\u7b56\u7565\uff08MCTS\u3001\u81ea\u6211\u4fee\u6b63\u7b49\uff09\uff0c\u4e26\u89c0\u5bdf\u5230\u5275\u610f\u6c92\u6709\u986f\u8457\u6539\u5584\u3002\u4f5c\u70ba\u6211\u5011\u5206\u6790\u7684\u526f\u7522\u54c1\uff0c\u6211\u5011\u767c\u5e03\u4e86 NeoCoder \u8cc7\u6599\u96c6\uff0c\u4ee5\u4fbf\u5728\u672a\u4f86\u7684\u6a21\u578b\u4e0a\u91cd\u73fe\u6211\u5011\u7684\u7d50\u679c\u3002", "author": "Yining Lu et.al.", "authors": "Yining Lu, Dixuan Wang, Tianjian Li, Dongwei Jiang, Daniel Khashabi", "id": "2407.09007v1", "paper_url": "http://arxiv.org/abs/2407.09007v1", "repo": "null"}}