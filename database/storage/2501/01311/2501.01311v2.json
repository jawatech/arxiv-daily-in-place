{"2501.01311": {"publish_time": "2025-01-02", "title": "Multi-Head Explainer: A General Framework to Improve Explainability in CNNs and Transformers", "paper_summary": "In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and\nmodular framework that enhances both the explainability and accuracy of\nConvolutional Neural Networks (CNNs) and Transformer-based models. MHEX\nconsists of three core components: an Attention Gate that dynamically\nhighlights task-relevant features, Deep Supervision that guides early layers to\ncapture fine-grained details pertinent to the target class, and an Equivalent\nMatrix that unifies refined local and global representations to generate\ncomprehensive saliency maps. Our approach demonstrates superior compatibility,\nenabling effortless integration into existing residual networks like ResNet and\nTransformer architectures such as BERT with minimal modifications. Extensive\nexperiments on benchmark datasets in medical imaging and text classification\nshow that MHEX not only improves classification accuracy but also produces\nhighly interpretable and detailed saliency scores.", "paper_summary_zh": "\u5728\u672c\u6b21\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u591a\u982d\u89e3\u91cb\u5668 (MHEX)\uff0c\u9019\u662f\u4e00\u500b\u591a\u529f\u80fd\u4e14\u6a21\u7d44\u5316\u7684\u67b6\u69cb\uff0c\u53ef\u589e\u5f37\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN) \u548c Transformer \u6a21\u578b\u7684\u53ef\u89e3\u91cb\u6027\u548c\u6e96\u78ba\u6027\u3002MHEX \u5305\u542b\u4e09\u500b\u6838\u5fc3\u7d44\u6210\u90e8\u5206\uff1a\u52d5\u614b\u7a81\u986f\u8207\u4efb\u52d9\u76f8\u95dc\u7279\u5fb5\u7684\u6ce8\u610f\u529b\u9598\u9580\u3001\u5f15\u5c0e\u65e9\u671f\u5c64\u6355\u6349\u8207\u76ee\u6a19\u985e\u5225\u76f8\u95dc\u7684\u7d30\u5fae\u7d30\u7bc0\u7684\u6df1\u5ea6\u76e3\u7763\uff0c\u4ee5\u53ca\u7d71\u4e00\u7cbe\u7dfb\u5c40\u90e8\u548c\u5168\u5c40\u8868\u793a\u4ee5\u7522\u751f\u5168\u9762\u986f\u8457\u6027\u5716\u7684\u7b49\u6548\u77e9\u9663\u3002\u6211\u5011\u7684\u505a\u6cd5\u5c55\u73fe\u51fa\u512a\u7570\u7684\u76f8\u5bb9\u6027\uff0c\u80fd\u8f15\u9b06\u6574\u5408\u5230\u73fe\u6709\u7684\u6b98\u5dee\u7db2\u8def\uff08\u5982 ResNet\uff09\u548c Transformer \u67b6\u69cb\uff08\u5982 BERT\uff09\uff0c\u4e14\u53ea\u9700\u9032\u884c\u6700\u5c0f\u7684\u4fee\u6539\u3002\u5728\u91ab\u5b78\u5f71\u50cf\u548c\u6587\u5b57\u5206\u985e\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u986f\u793a\uff0cMHEX \u4e0d\u50c5\u80fd\u63d0\u5347\u5206\u985e\u6e96\u78ba\u6027\uff0c\u9084\u80fd\u7522\u751f\u9ad8\u5ea6\u53ef\u89e3\u91cb\u4e14\u8a73\u7d30\u7684\u986f\u8457\u6027\u5206\u6578\u3002", "author": "Bohang Sun et.al.", "authors": "Bohang Sun, Pietro Li\u00f2", "id": "2501.01311v2", "paper_url": "http://arxiv.org/abs/2501.01311v2", "repo": "null"}}