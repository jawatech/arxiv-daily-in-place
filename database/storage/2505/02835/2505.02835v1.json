{"2505.02835": {"publish_time": "2025-05-05", "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning", "paper_summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.", "paper_summary_zh": "<paragraph>\u591a\u6a21\u614b\u734e\u52f5\u6a21\u578b (MRMs) \u5728\u63d0\u5347\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLMs) \u7684\u6548\u80fd\u65b9\u9762\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u96d6\u7136\u8fd1\u671f\u7684\u9032\u5c55\u4e3b\u8981\u96c6\u4e2d\u5728\u6539\u9032 MRMs \u7684\u6a21\u578b\u7d50\u69cb\u548c\u8a13\u7df4\u6578\u64da\uff0c\u4f46\u5c0d\u65bc\u9577\u671f\u63a8\u7406\u80fd\u529b\u5728\u734e\u52f5\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u6fc0\u767c MRMs \u7684\u9019\u4e9b\u80fd\u529b\uff0c\u76f8\u95dc\u7814\u7a76\u537b\u76f8\u7576\u6709\u9650\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u5982\u4f55\u5229\u7528\u5f37\u5316\u5b78\u7fd2 (RL) \u4f86\u6539\u9032\u734e\u52f5\u5efa\u6a21\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u734e\u52f5\u5efa\u6a21\u554f\u984c\u91cd\u65b0\u5b9a\u7fa9\u70ba\u4e00\u500b\u57fa\u65bc\u898f\u5247\u7684\u5f37\u5316\u5b78\u7fd2\u4efb\u52d9\u3002\u7136\u800c\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u76f4\u63a5\u61c9\u7528\u73fe\u6709\u7684\u5f37\u5316\u5b78\u7fd2\u6f14\u7b97\u6cd5\uff08\u4f8b\u5982 Reinforce++\uff09\u5230\u734e\u52f5\u5efa\u6a21\u4e2d\uff0c\u5f80\u5f80\u6703\u56e0\u70ba\u9019\u4e9b\u6f14\u7b97\u6cd5\u672c\u8eab\u7684\u9650\u5236\u800c\u5c0e\u81f4\u8a13\u7df4\u4e0d\u7a69\u5b9a\uff0c\u751a\u81f3\u5d29\u6f70\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 StableReinforce \u6f14\u7b97\u6cd5\uff0c\u5b83\u6539\u9032\u4e86\u73fe\u6709\u5f37\u5316\u5b78\u7fd2\u65b9\u6cd5\u7684\u8a13\u7df4\u640d\u5931\u3001\u512a\u52e2\u4f30\u8a08\u7b56\u7565\u548c\u734e\u52f5\u8a2d\u8a08\u3002\u9019\u4e9b\u6539\u9032\u4f7f\u5f97\u8a13\u7df4\u52d5\u614b\u66f4\u52a0\u7a69\u5b9a\uff0c\u4e26\u5e36\u4f86\u66f4\u512a\u7570\u7684\u6548\u80fd\u3002\u70ba\u4e86\u4fc3\u9032 MRM \u8a13\u7df4\uff0c\u6211\u5011\u5f9e\u5404\u7a2e\u6578\u64da\u96c6\u4e2d\u6536\u96c6\u4e86 200K \u7684\u504f\u597d\u6578\u64da\u3002\u6211\u5011\u7684\u734e\u52f5\u6a21\u578b R1-Reward \u4f7f\u7528 StableReinforce \u6f14\u7b97\u6cd5\u5728\u6b64\u6578\u64da\u96c6\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u986f\u8457\u63d0\u5347\u4e86\u591a\u6a21\u614b\u734e\u52f5\u5efa\u6a21\u57fa\u6e96\u6e2c\u8a66\u7684\u6548\u80fd\u3002\u8207\u4e4b\u524d\u7684 SOTA \u6a21\u578b\u76f8\u6bd4\uff0cR1-Reward \u5728 VL Reward-Bench \u4e0a\u63d0\u5347\u4e86 $8.4\\%$\uff0c\u5728 Multimodal Reward Bench \u4e0a\u63d0\u5347\u4e86 $14.3\\%$\u3002\u6b64\u5916\uff0c\u96a8\u8457\u63a8\u7406\u8a08\u7b97\u91cf\u7684\u589e\u52a0\uff0cR1-Reward \u7684\u6548\u80fd\u5f97\u5230\u9032\u4e00\u6b65\u63d0\u5347\uff0c\u9019\u51f8\u986f\u4e86\u5f37\u5316\u5b78\u7fd2\u6f14\u7b97\u6cd5\u5728\u512a\u5316 MRMs \u65b9\u9762\u7684\u6f5b\u529b\u3002</paragraph>\n", "author": "Yi-Fan Zhang et.al.", "authors": "Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, Haojie Ding, Jiankang Chen, Fan Yang, Zhang Zhang, Tingting Gao, Liang Wang", "id": "2505.02835v1", "paper_url": "http://arxiv.org/abs/2505.02835v1", "repo": "null"}}