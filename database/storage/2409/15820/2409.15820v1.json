{"2409.15820": {"publish_time": "2024-09-24", "title": "Supervised Fine-Tuning: An Activation Pattern Optimization Process for Attention Heads", "paper_summary": "Though demonstrating promising potential, LLMs' performance on complex tasks,\nsuch as advanced mathematics and complex disease diagnosis is still\nunsatisfactory. A key issue is the present LLMs learn in a data-driven schema,\nwhile the instruction dataset about these complex tasks is both scarce and hard\nto collect or construct. On the contrary, a prominent phenomenon is that LLMs\ncan learn rather fast on those simpler tasks with adequate prior knowledge\ncaptured during pretraining stage. Thus, if the prerequisite and mechanism of\nsuch rapid generalization could be elucidated, it could be highly beneficial in\nenhancing the efficiency and effectiveness of the LLM's ability to learn\ncomplex tasks. Thus, in this paper, we employ a gradient-based method, to\ndissect the process that the SFT process adapts LLMs to downstream tasks via\nthe perspective of attention patterns. We find that: (1) LLMs selectively\nactivate task-specific attention heads during SFT; (2) activation patterns for\ncomplex tasks are combinations of basic task patterns; and (3) changes in a few\nparameters can significantly impact activation patterns after SFT on a small\nnumber of samples. Based on these insights, we conduct experiments to examine\nwhether these conclusions could effectively enhance the efficiency and\neffectiveness of SFT, particularly in handling complex tasks and when\ninstructional resources are scarce. Our research not only uncovers the\nunderlying reasons behind LLMs' rapid learning and generalization mechanisms\nbut also provides practical solutions for addressing data challenges in complex\nand specialized tasks.", "paper_summary_zh": "\u5118\u7ba1\u5c55\u793a\u4e86\u6709\u5e0c\u671b\u7684\u6f5b\u529b\uff0c\u4f46 LLM \u5728\u8907\u96dc\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\uff0c\u4f8b\u5982\u9032\u968e\u6578\u5b78\u548c\u8907\u96dc\u75be\u75c5\u8a3a\u65b7\uff0c\u4ecd\u7136\u4e0d\u4ee4\u4eba\u6eff\u610f\u3002\u4e00\u500b\u95dc\u9375\u554f\u984c\u662f\u76ee\u524d\u7684 LLM \u4ee5\u8cc7\u6599\u9a45\u52d5\u7684\u6a21\u5f0f\u5b78\u7fd2\uff0c\u800c\u95dc\u65bc\u9019\u4e9b\u8907\u96dc\u4efb\u52d9\u7684\u6307\u4ee4\u8cc7\u6599\u96c6\u65e2\u7a00\u5c11\u53c8\u96e3\u4ee5\u6536\u96c6\u6216\u5efa\u69cb\u3002\u76f8\u53cd\uff0c\u4e00\u500b\u986f\u8457\u7684\u73fe\u8c61\u662f\uff0cLLM \u80fd\u5728\u9810\u8a13\u7df4\u968e\u6bb5\u64f7\u53d6\u7684\u8db3\u5920\u5148\u5099\u77e5\u8b58\u4e0b\uff0c\u5728\u90a3\u4e9b\u8f03\u7c21\u55ae\u7684\u4efb\u52d9\u4e0a\u5feb\u901f\u5b78\u7fd2\u3002\u56e0\u6b64\uff0c\u5982\u679c\u53ef\u4ee5\u95e1\u660e\u9019\u7a2e\u5feb\u901f\u6982\u5316\u7684\u524d\u63d0\u689d\u4ef6\u548c\u6a5f\u5236\uff0c\u5b83\u53ef\u80fd\u5c0d\u63d0\u9ad8 LLM \u5b78\u7fd2\u8907\u96dc\u4efb\u52d9\u7684\u80fd\u529b\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u5927\u6709\u5e6b\u52a9\u3002\u56e0\u6b64\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a1\u7528\u57fa\u65bc\u68af\u5ea6\u7684\u7684\u65b9\u6cd5\uff0c\u5f9e\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u89d2\u5ea6\u5256\u6790 SFT \u904e\u7a0b\u9069\u61c9 LLM \u5230\u4e0b\u6e38\u4efb\u52d9\u7684\u904e\u7a0b\u3002\u6211\u5011\u767c\u73fe\uff1a(1) LLM \u5728 SFT \u671f\u9593\u9078\u64c7\u6027\u5730\u555f\u52d5\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u6ce8\u610f\u529b\u982d\uff1b(2) \u8907\u96dc\u4efb\u52d9\u7684\u555f\u52d5\u6a21\u5f0f\u662f\u57fa\u672c\u4efb\u52d9\u6a21\u5f0f\u7684\u7d44\u5408\uff1b(3) \u5c11\u6578\u53c3\u6578\u7684\u8b8a\u5316\u6703\u986f\u8457\u5f71\u97ff SFT \u5f8c\u5728\u5c11\u6578\u6a23\u672c\u4e0a\u7684\u555f\u52d5\u6a21\u5f0f\u3002\u57fa\u65bc\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u9032\u884c\u5be6\u9a57\u4ee5\u6aa2\u9a57\u9019\u4e9b\u7d50\u8ad6\u662f\u5426\u80fd\u6709\u6548\u63d0\u9ad8 SFT \u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u7279\u5225\u662f\u5728\u8655\u7406\u8907\u96dc\u4efb\u52d9\u548c\u6307\u4ee4\u8cc7\u6e90\u7a00\u7f3a\u6642\u3002\u6211\u5011\u7684\u7814\u7a76\u4e0d\u50c5\u63ed\u793a\u4e86 LLM \u5feb\u901f\u5b78\u7fd2\u548c\u6982\u5316\u6a5f\u5236\u80cc\u5f8c\u7684\u57fa\u672c\u539f\u56e0\uff0c\u9084\u70ba\u89e3\u6c7a\u8907\u96dc\u548c\u5c08\u696d\u4efb\u52d9\u4e2d\u7684\u8cc7\u6599\u6311\u6230\u63d0\u4f9b\u4e86\u5be6\u7528\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Yang Zhao et.al.", "authors": "Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin", "id": "2409.15820v1", "paper_url": "http://arxiv.org/abs/2409.15820v1", "repo": "null"}}