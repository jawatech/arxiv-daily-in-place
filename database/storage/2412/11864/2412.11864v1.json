{"2412.11864": {"publish_time": "2024-12-16", "title": "Investigating Mixture of Experts in Dense Retrieval", "paper_summary": "While Dense Retrieval Models (DRMs) have advanced Information Retrieval (IR),\none limitation of these neural models is their narrow generalizability and\nrobustness. To cope with this issue, one can leverage the Mixture-of-Experts\n(MoE) architecture. While previous IR studies have incorporated MoE\narchitectures within the Transformer layers of DRMs, our work investigates an\narchitecture that integrates a single MoE block (SB-MoE) after the output of\nthe final Transformer layer. Our empirical evaluation investigates how SB-MoE\ncompares, in terms of retrieval effectiveness, to standard fine-tuning. In\ndetail, we fine-tune three DRMs (TinyBERT, BERT, and Contriever) across four\nbenchmark collections with and without adding the MoE block. Moreover, since\nMoE showcases performance variations with respect to its parameters (i.e., the\nnumber of experts), we conduct additional experiments to investigate this\naspect further. The findings show the effectiveness of SB-MoE especially for\nDRMs with a low number of parameters (i.e., TinyBERT), as it consistently\noutperforms the fine-tuned underlying model on all four benchmarks. For DRMs\nwith a higher number of parameters (i.e., BERT and Contriever), SB-MoE requires\nlarger numbers of training samples to yield better retrieval performance.", "paper_summary_zh": "\u5728\u7a20\u5bc6\u68c0\u7d22\u6a21\u578b (DRM) \u63d0\u5347\u4e86\u4fe1\u606f\u68c0\u7d22 (IR) \u7684\u540c\u65f6\uff0c\u8fd9\u4e9b\u795e\u7ecf\u6a21\u578b\u7684\u4e00\u4e2a\u5c40\u9650\u6027\u5728\u4e8e\u5b83\u4eec\u8f83\u7a84\u7684\u6cdb\u5316\u6027\u548c\u7a33\u5065\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e2a\u95ee\u9898\uff0c\u4eba\u4eec\u53ef\u4ee5\u5229\u7528\u6df7\u5408\u4e13\u5bb6 (MoE) \u67b6\u6784\u3002\u867d\u7136\u4e4b\u524d\u7684 IR \u7814\u7a76\u5df2\u7ecf\u5c06 MoE \u67b6\u6784\u7eb3\u5165\u4e86 DRM \u7684 Transformer \u5c42\uff0c\u4f46\u6211\u4eec\u7684\u5de5\u4f5c\u7814\u7a76\u4e86\u4e00\u79cd\u5728\u6700\u7ec8 Transformer \u5c42\u7684\u8f93\u51fa\u4e4b\u540e\u96c6\u6210\u5355\u4e2a MoE \u5757 (SB-MoE) \u7684\u67b6\u6784\u3002\u6211\u4eec\u7684\u7ecf\u9a8c\u8bc4\u4f30\u7814\u7a76\u4e86 SB-MoE \u5728\u68c0\u7d22\u6709\u6548\u6027\u65b9\u9762\u5982\u4f55\u4e0e\u6807\u51c6\u5fae\u8c03\u8fdb\u884c\u6bd4\u8f83\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5728\u56db\u4e2a\u57fa\u51c6\u96c6\u5408\u4e2d\u5bf9\u4e09\u4e2a DRM\uff08TinyBERT\u3001BERT \u548c Contriever\uff09\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5206\u522b\u6709\u548c\u6ca1\u6709\u6dfb\u52a0 MoE \u5757\u3002\u6b64\u5916\uff0c\u7531\u4e8e MoE \u5728\u5176\u53c2\u6570\uff08\u5373\u4e13\u5bb6\u6570\u91cf\uff09\u65b9\u9762\u8868\u73b0\u51fa\u6027\u80fd\u5dee\u5f02\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u989d\u5916\u7684\u5b9e\u9a8c\u4ee5\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd9\u4e2a\u65b9\u9762\u3002\u7ed3\u679c\u8868\u660e SB-MoE \u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u8f83\u5c11\u53c2\u6570\u7684 DRM\uff08\u5373 TinyBERT\uff09\uff0c\u56e0\u4e3a\u5b83\u5728\u6240\u6709\u56db\u4e2a\u57fa\u51c6\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u5fae\u8c03\u540e\u7684\u5e95\u5c42\u6a21\u578b\u3002\u5bf9\u4e8e\u5177\u6709\u66f4\u591a\u53c2\u6570\u7684 DRM\uff08\u5373 BERT \u548c Contriever\uff09\uff0cSB-MoE \u9700\u8981\u66f4\u591a\u7684\u8bad\u7ec3\u6837\u672c\u624d\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u68c0\u7d22\u6027\u80fd\u3002", "author": "Effrosyni Sokli et.al.", "authors": "Effrosyni Sokli, Pranav Kasela, Georgios Peikos, Gabriella Pasi", "id": "2412.11864v1", "paper_url": "http://arxiv.org/abs/2412.11864v1", "repo": "null"}}