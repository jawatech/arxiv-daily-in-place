{"2407.13578": {"publish_time": "2024-07-18", "title": "Large Language Models as Reliable Knowledge Bases?", "paper_summary": "The NLP community has recently shown a growing interest in leveraging Large\nLanguage Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential\nknowledge bases (KBs). However, the reliability and extent to which LLMs can\nfunction as KBs remain underexplored. While previous studies suggest LLMs can\nencode knowledge within their parameters, the amount of parametric knowledge\nalone is not sufficient to evaluate their effectiveness as KBs. This study\ndefines criteria that a reliable LLM-as-KB should meet, focusing on factuality\nand consistency, and covering both seen and unseen knowledge. We develop\nseveral metrics based on these criteria and use them to evaluate 26 popular\nLLMs, while providing a comprehensive analysis of the effects of model size,\ninstruction tuning, and in-context learning (ICL). Our results paint a worrying\npicture. Even a high-performant model like GPT-3.5-turbo is not factual or\nconsistent, and strategies like ICL and fine-tuning are unsuccessful at making\nLLMs better KBs.", "paper_summary_zh": "NLP \u793e\u7fa4\u6700\u8fd1\u986f\u793a\u51fa\u5c0d\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u57f7\u884c\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\u7684\u8208\u8da3\u65e5\u76ca\u589e\u9577\uff0c\u5c07 LLM \u8996\u70ba\u6f5b\u5728\u7684\u77e5\u8b58\u5eab (KB)\u3002\u7136\u800c\uff0cLLM \u4f5c\u70ba KB \u7684\u53ef\u9760\u6027\u548c\u7a0b\u5ea6\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8a0e\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u8868\u660e LLM \u53ef\u4ee5\u5c0d\u5176\u53c3\u6578\u5167\u7684\u77e5\u8b58\u9032\u884c\u7de8\u78bc\uff0c\u4f46\u50c5\u53c3\u6578\u77e5\u8b58\u7684\u6578\u91cf\u4e26\u4e0d\u8db3\u4ee5\u8a55\u4f30\u5176\u4f5c\u70ba KB \u7684\u6709\u6548\u6027\u3002\u672c\u7814\u7a76\u5b9a\u7fa9\u4e86\u4e00\u500b\u53ef\u9760\u7684 LLM \u4f5c\u70ba KB \u61c9\u8a72\u7b26\u5408\u7684\u6a19\u6e96\uff0c\u91cd\u9ede\u95dc\u6ce8\u4e8b\u5be6\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4e26\u6db5\u84cb\u5df2\u898b\u548c\u672a\u898b\u7684\u77e5\u8b58\u3002\u6211\u5011\u6839\u64da\u9019\u4e9b\u6a19\u6e96\u5236\u5b9a\u4e86\u5e7e\u500b\u6307\u6a19\uff0c\u4e26\u4f7f\u7528\u5b83\u5011\u4f86\u8a55\u4f30 26 \u500b\u6d41\u884c\u7684 LLM\uff0c\u540c\u6642\u5c0d\u6a21\u578b\u5927\u5c0f\u3001\u6307\u4ee4\u8abf\u6574\u548c\u60c5\u5883\u5b78\u7fd2 (ICL) \u7684\u5f71\u97ff\u9032\u884c\u5168\u9762\u5206\u6790\u3002\u6211\u5011\u7684\u7d50\u679c\u63cf\u7e6a\u4e86\u4e00\u5e45\u4ee4\u4eba\u64d4\u6182\u7684\u756b\u9762\u3002\u5373\u4f7f\u662f\u50cf GPT-3.5-turbo \u9019\u6a23\u7684\u9ad8\u6027\u80fd\u6a21\u578b\uff0c\u4e8b\u5be6\u4e0a\u4e5f\u4e0d\u4e00\u81f4\uff0c\u800c ICL \u548c\u5fae\u8abf\u7b49\u7b56\u7565\u4e5f\u7121\u6cd5\u8b93 LLM \u6210\u70ba\u66f4\u597d\u7684 KB\u3002", "author": "Danna Zheng et.al.", "authors": "Danna Zheng, Mirella Lapata, Jeff Z. Pan", "id": "2407.13578v1", "paper_url": "http://arxiv.org/abs/2407.13578v1", "repo": "null"}}