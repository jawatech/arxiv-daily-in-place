{"2408.14721": {"publish_time": "2024-08-27", "title": "PAT: Pruning-Aware Tuning for Large Language Models", "paper_summary": "Large language models (LLMs) excel in language tasks, especially with\nsupervised fine-tuning after pre-training. However, their substantial memory\nand computational requirements hinder practical applications. Structural\npruning, which reduces less significant weight dimensions, is one solution.\nYet, traditional post-hoc pruning often leads to significant performance loss,\nwith limited recovery from further fine-tuning due to reduced capacity. Since\nthe model fine-tuning refines the general and chaotic knowledge in pre-trained\nmodels, we aim to incorporate structural pruning with the fine-tuning, and\npropose the Pruning-Aware Tuning (PAT) paradigm to eliminate model redundancy\nwhile preserving the model performance to the maximum extend. Specifically, we\ninsert the innovative Hybrid Sparsification Modules (HSMs) between the\nAttention and FFN components to accordingly sparsify the upstream and\ndownstream linear modules. The HSM comprises a lightweight operator and a\nglobally shared trainable mask. The lightweight operator maintains a training\noverhead comparable to that of LoRA, while the trainable mask unifies the\nchannels to be sparsified, ensuring structural pruning. Additionally, we\npropose the Identity Loss which decouples the transformation and scaling\nproperties of the HSMs to enhance training robustness. Extensive experiments\ndemonstrate that PAT excels in both performance and efficiency. For example,\nour Llama2-7b model with a 25\\% pruning ratio achieves 1.33$\\times$ speedup\nwhile outperforming the LoRA-finetuned model by up to 1.26\\% in accuracy with a\nsimilar training cost. Code:\nhttps://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a9e\u8a00\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u7279\u5225\u662f\u5728\u9810\u8a13\u7df4\u5f8c\u7684\u76e3\u7763\u5fae\u8abf\u3002\u7136\u800c\uff0c\u5b83\u5011\u9f90\u5927\u7684\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u9700\u6c42\u963b\u7919\u4e86\u5be6\u969b\u61c9\u7528\u3002\u7d50\u69cb\u6027\u526a\u679d\uff08\u6e1b\u5c11\u8f03\u4e0d\u91cd\u8981\u7684\u6b0a\u91cd\u7dad\u5ea6\uff09\u662f\u4e00\u7a2e\u89e3\u6c7a\u65b9\u6848\u3002\u7136\u800c\uff0c\u50b3\u7d71\u7684\u5f8c\u8a2d\u526a\u679d\u901a\u5e38\u6703\u5c0e\u81f4\u986f\u8457\u7684\u6548\u80fd\u640d\u5931\uff0c\u7531\u65bc\u5bb9\u91cf\u6e1b\u5c11\uff0c\u9032\u4e00\u6b65\u5fae\u8abf\u7684\u6062\u5fa9\u80fd\u529b\u6709\u9650\u3002\u7531\u65bc\u6a21\u578b\u5fae\u8abf\u6539\u9032\u4e86\u9810\u8a13\u7df4\u6a21\u578b\u4e2d\u7684\u4e00\u822c\u6027\u548c\u6df7\u4e82\u77e5\u8b58\uff0c\u6211\u5011\u65e8\u5728\u5c07\u7d50\u69cb\u6027\u526a\u679d\u8207\u5fae\u8abf\u7d50\u5408\u8d77\u4f86\uff0c\u4e26\u63d0\u51fa\u4fee\u526a\u611f\u77e5\u8abf\u6574 (PAT) \u5178\u7bc4\uff0c\u4ee5\u6d88\u9664\u6a21\u578b\u5197\u9918\uff0c\u540c\u6642\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u7559\u6a21\u578b\u6548\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5728\u6ce8\u610f\u529b\u548c FFN \u7d44\u4ef6\u4e4b\u9593\u63d2\u5165\u5275\u65b0\u7684\u6df7\u5408\u7a00\u758f\u5316\u6a21\u7d44 (HSM)\uff0c\u4ee5\u76f8\u61c9\u5730\u7a00\u758f\u5316\u4e0a\u6e38\u548c\u4e0b\u6e38\u7dda\u6027\u6a21\u7d44\u3002HSM \u5305\u542b\u4e00\u500b\u8f15\u91cf\u7d1a\u904b\u7b97\u5b50\uff0c\u4ee5\u53ca\u4e00\u500b\u5168\u57df\u5171\u4eab\u7684\u53ef\u8a13\u7df4\u906e\u7f69\u3002\u8f15\u91cf\u7d1a\u904b\u7b97\u5b50\u7dad\u6301\u8207 LoRA \u76f8\u7576\u7684\u8a13\u7df4\u958b\u92b7\uff0c\u800c\u53ef\u8a13\u7df4\u906e\u7f69\u7d71\u4e00\u4e86\u8981\u7a00\u758f\u5316\u7684\u901a\u9053\uff0c\u78ba\u4fdd\u7d50\u69cb\u6027\u526a\u679d\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u8eab\u5206\u640d\u5931\uff0c\u5b83\u89e3\u8026\u4e86 HSM \u7684\u8f49\u63db\u548c\u7e2e\u653e\u5c6c\u6027\uff0c\u4ee5\u589e\u5f37\u8a13\u7df4\u7684\u7a69\u5065\u6027\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8b49\u660e\uff0cPAT \u5728\u6548\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u8868\u73fe\u51fa\u8272\u3002\u4f8b\u5982\uff0c\u6211\u5011\u7684 Llama2-7b \u6a21\u578b\u4ee5 25% \u7684\u526a\u679d\u7387\u5be6\u73fe\u4e86 1.33 \u500d\u7684\u52a0\u901f\uff0c\u540c\u6642\u5728\u6e96\u78ba\u5ea6\u4e0a\u6bd4 LoRA \u5fae\u8abf\u6a21\u578b\u9ad8\u51fa 1.26%\uff0c\u4e14\u8a13\u7df4\u6210\u672c\u985e\u4f3c\u3002\u7a0b\u5f0f\u78bc\uff1a\nhttps://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning", "author": "Yijiang Liu et.al.", "authors": "Yijiang Liu, Huanrui Yang, Youxin Chen, Rongyu Zhang, Miao Wang, Yuan Du, Li Du", "id": "2408.14721v1", "paper_url": "http://arxiv.org/abs/2408.14721v1", "repo": "https://github.com/kriskrisliu/pat_pruning-aware-tuning"}}