{"2407.21368": {"publish_time": "2024-07-31", "title": "Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering", "paper_summary": "Large Vision-Language Models (LVLMs) have achieved significant success in\nrecent years, and they have been extended to the medical domain. Although\ndemonstrating satisfactory performance on medical Visual Question Answering\n(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,\nwhich makes them fail to diagnose complex pathologies. Moreover, they readily\nfail to learn minority pathologies due to imbalanced training data. We propose\ntwo prompting strategies for MLVLMs that reduce hallucination and improve VQA\nperformance. In the first strategy, we provide a detailed explanation of the\nqueried pathology. In the second strategy, we fine-tune a cheap, weak learner\nto achieve high performance on a specific metric, and textually provide its\njudgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our\nmethods significantly improve the diagnostic F1 score, with the highest\nincrease being 0.27. We also demonstrate that our prompting strategies can be\nextended to general LVLM domains. Based on POPE metrics, it effectively\nsuppresses the false negative predictions of existing LVLMs and improves Recall\nby approximately 0.07.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5728\u8fd1\u5e74\u4f86\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\uff0c\u4e26\u5df2\u64f4\u5c55\u5230\u91ab\u7642\u9818\u57df\u3002\u5118\u7ba1\u5728\u91ab\u5b78\u8996\u89ba\u554f\u7b54 (VQA) \u4efb\u52d9\u4e2d\u8868\u73fe\u4ee4\u4eba\u6eff\u610f\uff0c\u4f46\u91ab\u5b78 LVLMs (MLVLMs) \u4ecd\u5b58\u5728\u5e7b\u89ba\u554f\u984c\uff0c\u5c0e\u81f4\u5b83\u5011\u7121\u6cd5\u8a3a\u65b7\u51fa\u8907\u96dc\u7684\u75c5\u7406\u3002\u6b64\u5916\uff0c\u7531\u65bc\u8a13\u7df4\u8cc7\u6599\u4e0d\u5e73\u8861\uff0c\u5b83\u5011\u5f88\u5bb9\u6613\u7121\u6cd5\u5b78\u7fd2\u5c11\u6578\u75c5\u7406\u3002\u6211\u5011\u63d0\u51fa\u5169\u7a2e\u91dd\u5c0d MLVLMs \u7684\u63d0\u793a\u7b56\u7565\uff0c\u4ee5\u6e1b\u5c11\u5e7b\u89ba\u4e26\u6539\u5584 VQA \u6548\u80fd\u3002\u5728\u7b2c\u4e00\u500b\u7b56\u7565\u4e2d\uff0c\u6211\u5011\u63d0\u4f9b\u67e5\u8a62\u75c5\u7406\u7684\u8a73\u7d30\u8aaa\u660e\u3002\u5728\u7b2c\u4e8c\u500b\u7b56\u7565\u4e2d\uff0c\u6211\u5011\u5fae\u8abf\u4e00\u500b\u4fbf\u5b9c\u3001\u6548\u80fd\u4e0d\u4f73\u7684\u5b78\u7fd2\u5668\uff0c\u4ee5\u5728\u7279\u5b9a\u6307\u6a19\u4e0a\u7372\u5f97\u9ad8\u6548\u80fd\uff0c\u4e26\u4ee5\u6587\u5b57\u65b9\u5f0f\u5411 MLVLM \u63d0\u4f9b\u5176\u5224\u65b7\u3002\u5728 MIMIC-CXR-JPG \u548c Chexpert \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u6e2c\u8a66\u5f8c\uff0c\u6211\u5011\u7684\u6a21\u578b\u986f\u8457\u6539\u5584\u4e86\u8a3a\u65b7 F1 \u5206\u6578\uff0c\u6700\u9ad8\u63d0\u5347\u5e45\u5ea6\u70ba 0.27\u3002\u6211\u5011\u9084\u5c55\u793a\u4e86\u6211\u5011\u7684\u63d0\u793a\u7b56\u7565\u53ef\u4ee5\u64f4\u5c55\u5230\u4e00\u822c\u7684 LVLM \u9818\u57df\u3002\u6839\u64da POPE \u6307\u6a19\uff0c\u5b83\u6709\u6548\u5730\u6291\u5236\u4e86\u73fe\u6709 LVLMs \u7684\u5047\u9670\u6027\u9810\u6e2c\uff0c\u4e26\u5c07\u53ec\u56de\u7387\u63d0\u9ad8\u4e86\u7d04 0.07\u3002", "author": "Danfeng Guo et.al.", "authors": "Danfeng Guo, Demetri Terzopoulos", "id": "2407.21368v1", "paper_url": "http://arxiv.org/abs/2407.21368v1", "repo": "null"}}