{"2412.14838": {"publish_time": "2024-12-19", "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs", "paper_summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.", "paper_summary_zh": "\u5728 LLMs \u4e2d\u9032\u884c\u9ad8\u6548\u7684 KV \u5feb\u53d6\u7ba1\u7406\u5c0d\u65bc RAG \u548c\u6458\u8981\u7b49\u9577\u80cc\u666f\u4efb\u52d9\u81f3\u95dc\u91cd\u8981\u3002\u73fe\u6709\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u65b9\u6cd5\u5f37\u5236\u57f7\u884c\u4e00\u500b\u56fa\u5b9a\u7684\u6a21\u5f0f\uff0c\u5ffd\u7565\u7279\u5b9a\u4efb\u52d9\u7684\u7279\u5fb5\u4e26\u6e1b\u5c11\u4fdd\u7559\u91cd\u8981\u8cc7\u8a0a\u3002\u7136\u800c\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u4e0d\u540c\u4efb\u52d9\u4e2d\u7684\u5404\u500b\u5c64\u7d1a\u9593\u6709\u4e0d\u540c\u7684\u555f\u52d5\u6a21\u5f0f\uff0c\u9019\u7a81\u986f\u4e86\u9700\u8981\u91dd\u5c0d\u6bcf\u500b\u4efb\u52d9\u7684\u7368\u7279\u9700\u6c42\u91cf\u8eab\u6253\u9020\u81ea\u9069\u61c9\u7b56\u7565\u3002\u57fa\u65bc\u6b64\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86 DynamicKV\uff0c\u9019\u662f\u4e00\u7a2e\u900f\u904e\u8abf\u6574\u6bcf\u500b\u5c64\u7d1a\u4fdd\u7559\u7684\u4ee3\u6578\u91cf\u4f86\u52d5\u614b\u6700\u4f73\u5316\u4ee3\u4fdd\u7559\u7684\u65b9\u6cd5\uff0c\u4ee5\u9069\u61c9\u7279\u5b9a\u4efb\u52d9\u3002DynamicKV \u5efa\u7acb\u4e86\u5168\u57df\u548c\u6bcf\u500b\u5c64\u7d1a\u7684\u6700\u5927 KV \u5feb\u53d6\u9810\u7b97\uff0c\u66ab\u6642\u4fdd\u7559\u7576\u524d\u5c64\u7d1a\u7684\u6700\u5927\u9810\u7b97\uff0c\u4e26\u5728\u63a8\u8ad6\u671f\u9593\u5b9a\u671f\u66f4\u65b0\u6240\u6709\u524d\u7f6e\u5c64\u7d1a\u7684 KV \u5feb\u53d6\u5927\u5c0f\u3002\u6211\u5011\u7684\u9019\u500b\u65b9\u6cd5\u50c5\u4fdd\u7559 1.7% \u7684 KV \u5feb\u53d6\u5927\u5c0f\uff0c\u540c\u6642\u5728 LongBench \u4e0a\u9054\u5230\u4e86\u7d04 85% \u7684\u5b8c\u6574 KV \u5feb\u53d6\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5373\u4f7f\u5728\u6975\u7aef\u58d3\u7e2e (0.9%) \u4e0b\uff0cDynamicKV \u5728\u4f7f\u7528 Mistral-7B-Instruct-v0.2 \u7684\u5927\u6d77\u6488\u91dd\u6e2c\u8a66\u4e2d\u4e5f\u6bd4\u6700\u5148\u9032 (SOTA) \u7684\u65b9\u6cd5\u9ad8\u51fa 11%\u3002\u7a0b\u5f0f\u78bc\u5c07\u6703\u91cb\u51fa\u3002", "author": "Xiabin Zhou et.al.", "authors": "Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding", "id": "2412.14838v1", "paper_url": "http://arxiv.org/abs/2412.14838v1", "repo": "null"}}