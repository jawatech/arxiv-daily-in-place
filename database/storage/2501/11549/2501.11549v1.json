{"2501.11549": {"publish_time": "2025-01-20", "title": "Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas", "paper_summary": "LLMs are tuned to follow instructions (aligned) by learning which of two\noutputs users prefer for a prompt. However, this preference data format does\nnot convey why users prefer responses that are chosen or rejected, so LLMs\ntrained on these datasets cannot tailor responses to varied user needs. To\nsurface these parameters of personalization, we apply abductive reasoning to\npreference data, inferring needs and interests of users, i.e. personas, that\nmay prefer each output. We test this idea in two steps: Persona Inference\n(PI)-abductively inferring personas of users who prefer chosen or rejected\noutputs-and Persona Tailoring (PT)-training models to tailor responses to\npersonas from PI. We find: 1) LLMs infer personas accurately explaining why\ndifferent users may prefer both chosen or rejected outputs; 2) Training on\npreference data augmented with PI personas via PT boosts personalization,\nenabling models to support user-written personas; and 3) Rejected response\npersonas form harder personalization evaluations, showing PT better aids users\nwith uncommon preferences versus typical alignment methods. We argue for an\nabductive view of preferences for personalization, asking not only which\nresponse is better but when, why, and for whom.", "paper_summary_zh": "LLM \u6703\u900f\u904e\u5b78\u7fd2\u4f7f\u7528\u8005\u504f\u597d\u7684\u5169\u500b\u8f38\u51fa\uff0c\u8abf\u6574\u70ba\u9075\u5faa\u6307\u4ee4\uff08\u5c0d\u9f4a\uff09\u3002\u7136\u800c\uff0c\u9019\u7a2e\u504f\u597d\u8cc7\u6599\u683c\u5f0f\u4e26\u672a\u50b3\u9054\u4f7f\u7528\u8005\u504f\u597d\u9078\u64c7\u6216\u62d2\u7d55\u56de\u61c9\u7684\u539f\u56e0\uff0c\u56e0\u6b64\u5728\u9019\u4e9b\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684 LLM \u7121\u6cd5\u91dd\u5c0d\u4e0d\u540c\u7684\u4f7f\u7528\u8005\u9700\u6c42\u8abf\u6574\u56de\u61c9\u3002\u70ba\u4e86\u4e86\u89e3\u9019\u4e9b\u500b\u4eba\u5316\u53c3\u6578\uff0c\u6211\u5011\u5c07\u6f14\u7e79\u63a8\u7406\u61c9\u7528\u65bc\u504f\u597d\u8cc7\u6599\uff0c\u63a8\u8ad6\u51fa\u53ef\u80fd\u504f\u597d\u6bcf\u500b\u8f38\u51fa\u7684\u4f7f\u7528\u8005\u9700\u6c42\u548c\u8208\u8da3\uff0c\u5373\u89d2\u8272\u3002\u6211\u5011\u5206\u70ba\u5169\u500b\u6b65\u9a5f\u6e2c\u8a66\u9019\u500b\u60f3\u6cd5\uff1a\u89d2\u8272\u63a8\u8ad6 (PI) - \u6f14\u7e79\u63a8\u8ad6\u504f\u597d\u9078\u53d6\u6216\u62d2\u7d55\u8f38\u51fa\u7684\u4f7f\u7528\u8005\u7684\u89d2\u8272\uff0c\u4ee5\u53ca\u89d2\u8272\u8abf\u6574 (PT) - \u8a13\u7df4\u6a21\u578b\u4ee5\u8abf\u6574\u56de\u61c9\u4ee5\u7b26\u5408 PI \u4e2d\u7684\u89d2\u8272\u3002\u6211\u5011\u767c\u73fe\uff1a1) LLM \u63a8\u8ad6\u89d2\u8272\u6e96\u78ba\u89e3\u91cb\u4e86\u70ba\u4ec0\u9ebc\u4e0d\u540c\u7684\u4f7f\u7528\u8005\u53ef\u80fd\u504f\u597d\u9078\u53d6\u6216\u62d2\u7d55\u7684\u8f38\u51fa\uff1b2) \u900f\u904e PT\uff0c\u4f7f\u7528 PI \u89d2\u8272\u64f4\u5145\u504f\u597d\u8cc7\u6599\u9032\u884c\u8a13\u7df4\uff0c\u6703\u63d0\u5347\u500b\u4eba\u5316\uff0c\u8b93\u6a21\u578b\u80fd\u652f\u63f4\u4f7f\u7528\u8005\u64b0\u5beb\u7684\u89d2\u8272\uff1b3) \u88ab\u62d2\u7d55\u7684\u56de\u61c9\u89d2\u8272\u5f62\u6210\u66f4\u56f0\u96e3\u7684\u500b\u4eba\u5316\u8a55\u4f30\uff0c\u986f\u793a PT \u6bd4\u5178\u578b\u7684\u5c0d\u9f4a\u65b9\u6cd5\u66f4\u80fd\u5354\u52a9\u5177\u6709\u4e0d\u5e38\u898b\u504f\u597d\u7684\u4f7f\u7528\u8005\u3002\u6211\u5011\u4e3b\u5f35\u6f14\u7e79\u504f\u597d\u4ee5\u9032\u884c\u500b\u4eba\u5316\uff0c\u4e0d\u50c5\u8a62\u554f\u54ea\u500b\u56de\u61c9\u8f03\u597d\uff0c\u9084\u8981\u8a62\u554f\u4f55\u6642\u3001\u70ba\u4f55\u4ee5\u53ca\u5c0d\u8ab0\u8f03\u597d\u3002", "author": "Nishant Balepur et.al.", "authors": "Nishant Balepur, Vishakh Padmakumar, Fumeng Yang, Shi Feng, Rachel Rudinger, Jordan Lee Boyd-Graber", "id": "2501.11549v1", "paper_url": "http://arxiv.org/abs/2501.11549v1", "repo": "https://github.com/pinafore/alignment-personalization"}}