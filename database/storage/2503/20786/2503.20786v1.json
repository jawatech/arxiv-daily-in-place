{"2503.20786": {"publish_time": "2025-03-26", "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark", "paper_summary": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u767c\u5c55\uff0c\u63d0\u9ad8\u4e86\u4eba\u5011\u5c07\u5176\u90e8\u7f72\u5728\u884c\u52d5\u88dd\u7f6e\u4e0a\u4ee5\u5be6\u73fe\u88dd\u7f6e\u7aef AI \u61c9\u7528\u7684\u8208\u8da3\u3002\u8207\u684c\u9762\u7528\u6236\u76f8\u6bd4\uff0c\u884c\u52d5\u7528\u6236\u8207 LLM \u7684\u4e92\u52d5\u65b9\u5f0f\u4e0d\u540c\uff0c\u7522\u751f\u4e86\u7368\u7279\u7684\u671f\u671b\u548c\u6578\u64da\u504f\u5dee\u3002\u76ee\u524d\u7684\u57fa\u6e96\u6578\u64da\u96c6\u4e3b\u8981\u91dd\u5c0d\u4f3a\u670d\u5668\u548c\u684c\u9762\u74b0\u5883\uff0c\u986f\u7136\u7f3a\u4e4f\u5c08\u70ba\u884c\u52d5\u88dd\u7f6e\u74b0\u5883\u8a2d\u8a08\u7684\u5927\u578b\u6578\u64da\u96c6\u3002\u6b64\u5916\uff0c\u884c\u52d5\u88dd\u7f6e\u5728\u5132\u5b58\u548c\u904b\u7b97\u8cc7\u6e90\u65b9\u9762\u53d7\u5230\u56b4\u683c\u9650\u5236\uff0c\u9019\u4e5f\u9650\u5236\u4e86\u6a21\u578b\u7684\u5927\u5c0f\u548c\u529f\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u6700\u4f73\u5316\u7684\u6548\u7387\u548c\u512a\u5148\u7684\u77e5\u8b58\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63a8\u51fa\u4e86 Mobile-MMLU\uff0c\u9019\u662f\u4e00\u500b\u5c08\u70ba\u884c\u52d5\u667a\u6167\u91cf\u8eab\u6253\u9020\u7684\u5927\u578b\u57fa\u6e96\u6578\u64da\u96c6\u3002\u5b83\u5305\u542b 80 \u500b\u884c\u52d5\u76f8\u95dc\u9818\u57df\u7684 16,186 \u500b\u554f\u984c\uff0c\u65e8\u5728\u8a55\u4f30 LLM \u5728\u771f\u5be6\u884c\u52d5\u88dd\u7f6e\u5834\u666f\u4e2d\u7684\u6548\u80fd\u3002Mobile-MMLU-Pro \u9019\u500b\u66f4\u5177\u6311\u6230\u6027\u7684\u5b50\u96c6\uff0c\u63d0\u4f9b\u4e86\u8207 MMLU-Pro \u898f\u6a21\u76f8\u4f3c\u7684\u9032\u968e\u8a55\u4f30\uff0c\u4f46\u6bd4\u6211\u5011\u7684\u6a19\u6e96\u5b8c\u6574\u6578\u64da\u96c6\u8981\u56f0\u96e3\u5f97\u591a\u3002\u9019\u5169\u500b\u57fa\u6e96\u6e2c\u8a66\u90fd\u4f7f\u7528\u591a\u9078\u984c\u3001\u9806\u5e8f\u4e0d\u8b8a\u7684\u554f\u984c\uff0c\u5074\u91cd\u65bc\u5be6\u969b\u7684\u884c\u52d5\u88dd\u7f6e\u4e92\u52d5\uff0c\u4f8b\u5982\u98df\u8b5c\u5efa\u8b70\u3001\u65c5\u884c\u898f\u5283\u548c\u5fc5\u8981\u7684\u65e5\u5e38\u4efb\u52d9\u3002\u8a72\u6578\u64da\u96c6\u5f37\u8abf\u95dc\u9375\u7684\u884c\u52d5\u88dd\u7f6e\u7279\u5b9a\u6307\u6a19\uff0c\u5982\u63a8\u8ad6\u5ef6\u9072\u3001\u80fd\u8017\u3001\u8a18\u61b6\u9ad4\u4f7f\u7528\u548c\u56de\u61c9\u54c1\u8cea\uff0c\u63d0\u4f9b\u5728\u884c\u52d5\u88dd\u7f6e\u9650\u5236\u4e0b\u6a21\u578b\u6548\u80fd\u7684\u5168\u9762\u6d1e\u5bdf\u3002\u6b64\u5916\uff0c\u5b83\u9084\u512a\u5148\u8003\u616e\u96b1\u79c1\u548c\u9069\u61c9\u6027\uff0c\u8a55\u4f30\u6a21\u578b\u5728\u88dd\u7f6e\u4e0a\u57f7\u884c\u8655\u7406\u3001\u7dad\u8b77\u7528\u6236\u96b1\u79c1\u4ee5\u53ca\u9069\u61c9\u500b\u4eba\u5316\u4f7f\u7528\u6a21\u5f0f\u7684\u80fd\u529b\u3002Mobile-MMLU \u7cfb\u5217\u63d0\u4f9b\u4e86\u4e00\u500b\u6a19\u6e96\u5316\u6846\u67b6\uff0c\u7528\u65bc\u958b\u767c\u548c\u6bd4\u8f03\u884c\u52d5\u88dd\u7f6e\u6700\u4f73\u5316\u7684 LLM\uff0c\u5f9e\u800c\u4fc3\u9032\u884c\u52d5\u904b\u7b97\u74b0\u5883\u4e2d\u751f\u7522\u529b\u548c\u6c7a\u7b56\u7684\u9032\u6b65\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6578\u64da\u53ef\u5728\u4ee5\u4e0b\u7db2\u5740\u7372\u5f97\uff1ahttps://github.com/VILA-Lab/Mobile-MMLU\u3002\n", "author": "Sondos Mahmoud Bsharat et.al.", "authors": "Sondos Mahmoud Bsharat, Mukul Ranjan, Aidar Myrzakhan, Jiacheng Liu, Bowei Guo, Shengkun Tang, Zhuang Liu, Yuanzhi Li, Zhiqiang Shen", "id": "2503.20786v1", "paper_url": "http://arxiv.org/abs/2503.20786v1", "repo": "null"}}