{"2501.04987": {"publish_time": "2025-01-09", "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures", "paper_summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency.", "paper_summary_zh": "\u9ad8\u6548\u7684\u9375\u503c (KV) \u5feb\u53d6\u58d3\u7e2e\u5c0d\u65bc\u7e2e\u653e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684Transformer\u5728\u9577\u5e8f\u5217\u548c\u8cc7\u6e90\u53d7\u9650\u7684\u8a2d\u5b9a\u4e2d\u81f3\u95dc\u91cd\u8981\u3002\u73fe\u6709\u65b9\u6cd5\u57fa\u65bc\u5b83\u5011\u7684\u4f4d\u7f6e\u6216\u91cd\u8981\u6027\u5206\u6578\u9a45\u9010\u7b26\u865f\uff0c\u4f46\u57fa\u65bc\u4f4d\u7f6e\u7684\u7b56\u7565\u53ef\u80fd\u6703\u932f\u5931\u9810\u5b9a\u7fa9\u5340\u57df\u5916\u7684\u95dc\u9375\u8cc7\u8a0a\uff0c\u800c\u4f9d\u8cf4\u65bc\u5168\u7403\u91cd\u8981\u6027\u5206\u6578\u7684\u7b56\u7565\u5247\u6703\u5c0e\u81f4\u5f37\u70c8\u7684\u5340\u57df\u504f\u8aa4\uff0c\u9650\u5236 KV \u5feb\u53d6\u7684\u6574\u9ad4\u5167\u5bb9\u4fdd\u7559\uff0c\u4e26\u53ef\u80fd\u640d\u5bb3 LLM \u5728\u8907\u96dc\u4efb\u52d9\u4e0a\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u6ce2\u6bb5\u5206\u6790\u986f\u793a\uff0c\u7576\u7b26\u865f\u63a5\u8fd1\u5e8f\u5217\u7684\u5c3e\u7aef\u6642\uff0c\u5b83\u5011\u5c0d\u751f\u6210\u7684\u8ca2\u737b\u6703\u9010\u6f38\u589e\u52a0\uff0c\u4e26\u4e14\u50be\u5411\u65bc\u8207\u9130\u8fd1\u7b26\u865f\u6709\u66f4\u591a\u5dee\u7570\uff0c\u9019\u8868\u793a\u5f9e\u9060\u8655\u5230\u9644\u8fd1\u7684\u5167\u5bb9\u5177\u6709\u5e73\u7a69\u7684\u8f49\u63db\uff0c\u4e14\u8907\u96dc\u6027\u548c\u53ef\u8b8a\u6027\u589e\u52a0\u3002\u53d7\u5230\u6b64\u89c0\u5bdf\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa TreeKV\uff0c\u9019\u662f\u4e00\u7a2e\u76f4\u89ba\u4e14\u7121\u9700\u8a13\u7df4\u7684\u65b9\u6cd5\uff0c\u5b83\u63a1\u7528\u6a39\u72c0\u7d50\u69cb\u9032\u884c\u5e73\u7a69\u7684\u5feb\u53d6\u58d3\u7e2e\u3002TreeKV \u7dad\u8b77\u56fa\u5b9a\u7684\u5feb\u53d6\u5927\u5c0f\uff0c\u8b93 LLM \u5373\u4f7f\u5728\u9577\u6587\u5b57\u5834\u666f\u4e2d\u4e5f\u80fd\u63d0\u4f9b\u9ad8\u54c1\u8cea\u7684\u8f38\u51fa\u3002\u8207\u5927\u591a\u6578\u58d3\u7e2e\u65b9\u6cd5\u4e0d\u540c\uff0cTreeKV \u53ef\u61c9\u7528\u65bc\u751f\u6210\u548c\u9810\u5148\u586b\u5165\u968e\u6bb5\u3002\u5b83\u5728 PG19 \u548c OpenWebText2 \u4e0a\u7684\u8a9e\u8a00\u5efa\u6a21\u4efb\u52d9\u4e2d\u6301\u7e8c\u8d85\u8d8a\u6240\u6709\u57fa\u6e96\u6a21\u578b\uff0c\u8b93\u4f7f\u7528\u77ed\u5167\u5bb9\u8996\u7a97\u8a13\u7df4\u7684 LLM \u80fd\u5920\u6982\u62ec\u70ba\u8f03\u9577\u7684\u8996\u7a97\uff0c\u5feb\u53d6\u6e1b\u5c11 16 \u500d\u3002\u5728 Longbench \u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0cTreeKV \u4ee5\u6700\u4f73\u6548\u7387\u50c5 6% \u7684\u9810\u7b97\u9054\u6210\u6700\u4f73\u6548\u80fd\u3002", "author": "Ziwei He et.al.", "authors": "Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang", "id": "2501.04987v1", "paper_url": "http://arxiv.org/abs/2501.04987v1", "repo": "null"}}