{"2404.12014": {"publish_time": "2024-04-18", "title": "Enhance Robustness of Language Models Against Variation Attack through Graph Integration", "paper_summary": "The widespread use of pre-trained language models (PLMs) in natural language\nprocessing (NLP) has greatly improved performance outcomes. However, these\nmodels' vulnerability to adversarial attacks (e.g., camouflaged hints from drug\ndealers), particularly in the Chinese language with its rich character\ndiversity/variation and complex structures, hatches vital apprehension. In this\nstudy, we propose a novel method, CHinese vAriatioN Graph Enhancement (CHANGE),\nto increase the robustness of PLMs against character variation attacks in\nChinese content. CHANGE presents a novel approach for incorporating a Chinese\ncharacter variation graph into the PLMs. Through designing different\nsupplementary tasks utilizing the graph structure, CHANGE essentially enhances\nPLMs' interpretation of adversarially manipulated text. Experiments conducted\nin a multitude of NLP tasks show that CHANGE outperforms current language\nmodels in combating against adversarial attacks and serves as a valuable\ncontribution to robust language model research. These findings contribute to\nthe groundwork on robust language models and highlight the substantial\npotential of graph-guided pre-training strategies for real-world applications.", "paper_summary_zh": "\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u7684\u5ee3\u6cdb\u4f7f\u7528\u5df2\u5927\u5e45\u6539\u5584\u4e86\u6548\u80fd\u7d50\u679c\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5c0d\u6297\u6027\u653b\u64ca (\u4f8b\u5982\uff0c\u4f86\u81ea\u85e5\u7269\u8ca9\u5b50\u7684\u507d\u88dd\u63d0\u793a)\uff0c\u7279\u5225\u662f\u5728\u5177\u6709\u8c50\u5bcc\u5b57\u5143\u591a\u6a23\u6027/\u8b8a\u5316\u548c\u8907\u96dc\u7d50\u69cb\u7684\u4e2d\u6587\u4e2d\uff0c\u5f15\u8d77\u4e86\u6975\u5927\u7684\u7591\u616e\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u7a31\u70ba\u300c\u4e2d\u6587\u8b8a\u7570\u5716\u589e\u5f37\u300d(CHANGE)\uff0c\u4ee5\u63d0\u9ad8 PLM \u5c0d\u4e2d\u6587\u5167\u5bb9\u4e2d\u5b57\u5143\u8b8a\u7570\u653b\u64ca\u7684\u7a69\u5065\u6027\u3002CHANGE \u63d0\u51fa\u4e86\u4e00\u7a2e\u5c07\u4e2d\u6587\u5b57\u5143\u8b8a\u7570\u5716\u6574\u5408\u5230 PLM \u4e2d\u7684\u65b0\u65b9\u6cd5\u3002\u900f\u904e\u8a2d\u8a08\u5229\u7528\u5716\u5f62\u7d50\u69cb\u7684\u4e0d\u540c\u88dc\u5145\u4efb\u52d9\uff0cCHANGE \u672c\u8cea\u4e0a\u589e\u5f37\u4e86 PLM \u5c0d\u5c0d\u6297\u6027\u64cd\u7e31\u6587\u5b57\u7684\u8a6e\u91cb\u3002\u5728\u5927\u91cf NLP \u4efb\u52d9\u4e2d\u9032\u884c\u7684\u5be6\u9a57\u986f\u793a\uff0cCHANGE \u5728\u5c0d\u6297\u5c0d\u6297\u6027\u653b\u64ca\u65b9\u9762\u512a\u65bc\u76ee\u524d\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u4e26\u4f5c\u70ba\u5c0d\u7a69\u5065\u8a9e\u8a00\u6a21\u578b\u7814\u7a76\u7684\u5bf6\u8cb4\u8ca2\u737b\u3002\u9019\u4e9b\u767c\u73fe\u70ba\u7a69\u5065\u8a9e\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u790e\uff0c\u4e26\u7a81\u986f\u4e86\u5716\u5f62\u5f15\u5c0e\u9810\u8a13\u7df4\u7b56\u7565\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u7684\u5de8\u5927\u6f5b\u529b\u3002", "author": "Zi Xiong et.al.", "authors": "Zi Xiong, Lizhi Qing, Yangyang Kang, Jiawei Liu, Hongsong Li, Changlong Sun, Xiaozhong Liu, Wei Lu", "id": "2404.12014v1", "paper_url": "http://arxiv.org/abs/2404.12014v1", "repo": "null"}}