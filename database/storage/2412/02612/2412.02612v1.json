{"2412.02612": {"publish_time": "2024-12-03", "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot", "paper_summary": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken\nchatbot. It supports both Chinese and English, engages in real-time voice\nconversations, and varies vocal nuances such as emotion, intonation, speech\nrate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low\nbitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate\nderived from an automatic speech recognition (ASR) model by incorporating a\nvector-quantized bottleneck into the encoder. To efficiently transfer knowledge\nfrom text to speech modalities, we synthesize speech-text interleaved data from\nexisting text pre-training corpora using a text-to-token model. We continue\npre-training from the pre-trained text language model GLM-4-9B with a\ncombination of unsupervised speech data, interleaved speech-text data, and\nsupervised speech-text data, scaling up to 1 trillion tokens, achieving\nstate-of-the-art performance in both speech language modeling and spoken\nquestion answering. We then fine-tune the pre-trained model with high-quality\nconversational speech data, achieving superior performance compared to existing\nbaselines in both conversational ability and speech quality. The open models\ncan be accessed through https://github.com/THUDM/GLM-4-Voice and\nhttps://huggingface.co/THUDM/glm-4-voice-9b.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a8\u51fa GLM-4-Voice\uff0c\u4e00\u6b3e\u667a\u6167\u4e14\u64ec\u4eba\u7684\u7aef\u5c0d\u7aef\u8a9e\u97f3\u804a\u5929\u6a5f\u5668\u4eba\u3002\u5b83\u652f\u63f4\u4e2d\u6587\u548c\u82f1\u6587\uff0c\u9032\u884c\u5373\u6642\u8a9e\u97f3\u5c0d\u8a71\uff0c\u4e26\u6839\u64da\u4f7f\u7528\u8005\u7684\u6307\u793a\u8abf\u6574\u8a9e\u97f3\u7684\u7d30\u5fae\u5dee\u5225\uff0c\u4f8b\u5982\u60c5\u7dd2\u3001\u8a9e\u8abf\u3001\u8a9e\u901f\u548c\u65b9\u8a00\u3002GLM-4-Voice \u4f7f\u7528\u8d85\u4f4e\u4f4d\u5143\u7387 (175bps)\u3001\u55ae\u78bc\u672c\u8a9e\u97f3\u5206\u8a5e\u5668\uff0c\u5177\u6709 12.5Hz \u5e40\u7387\uff0c\u6e90\u81ea\u65bc\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u6a21\u578b\uff0c\u65b9\u6cd5\u662f\u5c07\u5411\u91cf\u91cf\u5316\u74f6\u9838\u7d0d\u5165\u7de8\u78bc\u5668\u4e2d\u3002\u70ba\u4e86\u6709\u6548\u7387\u5730\u5c07\u77e5\u8b58\u5f9e\u6587\u5b57\u50b3\u8f38\u5230\u8a9e\u97f3\u6a21\u5f0f\uff0c\u6211\u5011\u4f7f\u7528\u6587\u5b57\u5230\u4ee3\u78bc\u6a21\u578b\uff0c\u5f9e\u73fe\u6709\u7684\u6587\u5b57\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u4e2d\u5408\u6210\u8a9e\u97f3\u6587\u5b57\u4ea4\u932f\u8cc7\u6599\u3002\u6211\u5011\u7e7c\u7e8c\u4f7f\u7528\u9810\u8a13\u7df4\u6587\u5b57\u8a9e\u8a00\u6a21\u578b GLM-4-9B \u9032\u884c\u9810\u8a13\u7df4\uff0c\u4e26\u7d50\u5408\u975e\u76e3\u7763\u5f0f\u8a9e\u97f3\u8cc7\u6599\u3001\u4ea4\u932f\u8a9e\u97f3\u6587\u5b57\u8cc7\u6599\uff0c\u4ee5\u53ca\u76e3\u7763\u5f0f\u8a9e\u97f3\u6587\u5b57\u8cc7\u6599\uff0c\u64f4\u5145\u5230 1 \u5146\u500b\u4ee3\u78bc\uff0c\u5728\u8a9e\u97f3\u8a9e\u8a00\u5efa\u6a21\u548c\u8a9e\u97f3\u554f\u7b54\u4e2d\u9054\u6210\u6700\u5148\u9032\u7684\u8868\u73fe\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u9ad8\u54c1\u8cea\u5c0d\u8a71\u8a9e\u97f3\u8cc7\u6599\u5fae\u8abf\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u5728\u5c0d\u8a71\u80fd\u529b\u548c\u8a9e\u97f3\u54c1\u8cea\u4e0a\u90fd\u9054\u6210\u6bd4\u73fe\u6709\u57fa\u6e96\u66f4\u597d\u7684\u8868\u73fe\u3002\u958b\u653e\u6a21\u578b\u53ef\u900f\u904e https://github.com/THUDM/GLM-4-Voice \u548c https://huggingface.co/THUDM/glm-4-voice-9b \u5b58\u53d6\u3002</paragraph>", "author": "Aohan Zeng et.al.", "authors": "Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, Jie Tang", "id": "2412.02612v1", "paper_url": "http://arxiv.org/abs/2412.02612v1", "repo": "https://github.com/thudm/glm-4-voice"}}