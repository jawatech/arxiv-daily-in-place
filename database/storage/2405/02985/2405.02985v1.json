{"2405.02985": {"publish_time": "2024-05-05", "title": "Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability to Mark Short Answer Questions in K-12 Education", "paper_summary": "This paper presents reports on a series of experiments with a novel dataset\nevaluating how well Large Language Models (LLMs) can mark (i.e. grade) open\ntext responses to short answer questions, Specifically, we explore how well\ndifferent combinations of GPT version and prompt engineering strategies\nperformed at marking real student answers to short answer across different\ndomain areas (Science and History) and grade-levels (spanning ages 5-16) using\na new, never-used-before dataset from Carousel, a quizzing platform. We found\nthat GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and,\nimportantly, very close to human-level performance (0.75). This research builds\non prior findings that GPT-4 could reliably score short answer reading\ncomprehension questions at a performance-level very close to that of expert\nhuman raters. The proximity to human-level performance, across a variety of\nsubjects and grade levels suggests that LLMs could be a valuable tool for\nsupporting low-stakes formative assessment tasks in K-12 education and has\nimportant implications for real-world education delivery.", "paper_summary_zh": "", "author": "Owen Henkel et.al.", "authors": "Owen Henkel,Adam Boxer,Libby Hills,Bill Roberts", "id": "2405.02985v1", "paper_url": "http://arxiv.org/abs/2405.02985v1", "repo": "null"}}