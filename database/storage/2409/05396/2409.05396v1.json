{"2409.05396": {"publish_time": "2024-09-09", "title": "FacialFlowNet: Advancing Facial Optical Flow Estimation with a Diverse Dataset and a Decomposed Model", "paper_summary": "Facial movements play a crucial role in conveying altitude and intentions,\nand facial optical flow provides a dynamic and detailed representation of it.\nHowever, the scarcity of datasets and a modern baseline hinders the progress in\nfacial optical flow research. This paper proposes FacialFlowNet (FFN), a novel\nlarge-scale facial optical flow dataset, and the Decomposed Facial Flow Model\n(DecFlow), the first method capable of decomposing facial flow. FFN comprises\n9,635 identities and 105,970 image pairs, offering unprecedented diversity for\ndetailed facial and head motion analysis. DecFlow features a facial\nsemantic-aware encoder and a decomposed flow decoder, excelling in accurately\nestimating and decomposing facial flow into head and expression components.\nComprehensive experiments demonstrate that FFN significantly enhances the\naccuracy of facial flow estimation across various optical flow methods,\nachieving up to an 11% reduction in Endpoint Error (EPE) (from 3.91 to 3.48).\nMoreover, DecFlow, when coupled with FFN, outperforms existing methods in both\nsynthetic and real-world scenarios, enhancing facial expression analysis. The\ndecomposed expression flow achieves a substantial accuracy improvement of 18%\n(from 69.1% to 82.1%) in micro-expressions recognition. These contributions\nrepresent a significant advancement in facial motion analysis and optical flow\nestimation. Codes and datasets can be found.", "paper_summary_zh": "\u9762\u90e8\u52d5\u4f5c\u5728\u50b3\u9054\u9ad8\u5ea6\u548c\u610f\u5716\u65b9\u9762\u767c\u63ee\u8457\u81f3\u95dc\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u800c\u9762\u90e8\u5149\u6d41\u63d0\u4f9b\u4e86\u52d5\u614b\u4e14\u8a73\u7d30\u7684\u8868\u793a\u3002\u7136\u800c\uff0c\u6578\u64da\u96c6\u7684\u7a00\u7f3a\u548c\u73fe\u4ee3\u57fa\u6e96\u963b\u7919\u4e86\u9762\u90e8\u5149\u6d41\u7814\u7a76\u7684\u9032\u5c55\u3002\u672c\u6587\u63d0\u51fa\u4e86 FacialFlowNet (FFN)\uff0c\u4e00\u500b\u65b0\u7a4e\u7684\u5927\u898f\u6a21\u9762\u90e8\u5149\u6d41\u6578\u64da\u96c6\uff0c\u4ee5\u53ca\u5206\u89e3\u9762\u90e8\u6d41\u6a21\u578b (DecFlow)\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u80fd\u5920\u5206\u89e3\u9762\u90e8\u6d41\u7684\u65b9\u6cd5\u3002FFN \u5305\u542b 9,635 \u500b\u8eab\u4efd\u548c 105,970 \u5c0d\u5716\u50cf\uff0c\u70ba\u8a73\u7d30\u7684\u9762\u90e8\u548c\u982d\u90e8\u904b\u52d5\u5206\u6790\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u591a\u6a23\u6027\u3002DecFlow \u63a1\u7528\u9762\u90e8\u8a9e\u7fa9\u611f\u77e5\u7de8\u78bc\u5668\u548c\u5206\u89e3\u6d41\u89e3\u78bc\u5668\uff0c\u64c5\u9577\u6e96\u78ba\u4f30\u8a08\u548c\u5c07\u9762\u90e8\u6d41\u5206\u89e3\u70ba\u982d\u90e8\u548c\u8868\u60c5\u7d44\u6210\u90e8\u5206\u3002\u7d9c\u5408\u5be6\u9a57\u8868\u660e\uff0cFFN \u660e\u986f\u63d0\u9ad8\u4e86\u5404\u7a2e\u5149\u6d41\u65b9\u6cd5\u7684\u9762\u90e8\u6d41\u4f30\u8a08\u7cbe\u5ea6\uff0c\u5c07\u7aef\u9ede\u8aa4\u5dee (EPE) \u964d\u4f4e\u4e86 11%\uff08\u5f9e 3.91 \u964d\u81f3 3.48\uff09\u3002\u6b64\u5916\uff0cDecFlow \u8207 FFN \u7d50\u5408\u4f7f\u7528\u6642\uff0c\u5728\u5408\u6210\u548c\u771f\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u5747\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u589e\u5f37\u4e86\u9762\u90e8\u8868\u60c5\u5206\u6790\u3002\u5206\u89e3\u7684\u8868\u60c5\u6d41\u5728\u5fae\u8868\u60c5\u8b58\u5225\u4e2d\u5be6\u73fe\u4e86 18% \u7684\u986f\u8457\u7cbe\u5ea6\u63d0\u5347\uff08\u5f9e 69.1% \u5230 82.1%\uff09\u3002\u9019\u4e9b\u8ca2\u737b\u4ee3\u8868\u4e86\u9762\u90e8\u904b\u52d5\u5206\u6790\u548c\u5149\u6d41\u4f30\u8a08\u7684\u91cd\u5927\u9032\u6b65\u3002\u53ef\u4ee5\u627e\u5230\u4ee3\u78bc\u548c\u6578\u64da\u96c6\u3002", "author": "Jianzhi Lu et.al.", "authors": "Jianzhi Lu, Ruian He, Shili Zhou, Weimin Tan, Bo Yan", "id": "2409.05396v1", "paper_url": "http://arxiv.org/abs/2409.05396v1", "repo": "null"}}