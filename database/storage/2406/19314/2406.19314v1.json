{"2406.19314": {"publish_time": "2024-06-27", "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark", "paper_summary": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.", "paper_summary_zh": "\u6e2c\u8a66\u96c6\u6c61\u67d3\uff0c\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7684\u6e2c\u8a66\u8cc7\u6599\u6700\u7d42\u51fa\u73fe\u5728\u8f03\u65b0\u7684\u6a21\u578b\u8a13\u7df4\u96c6\u4e2d\uff0c\u662f\u516c\u5e73 LLM \u8a55\u4f30\u7684\u773e\u6240\u5468\u77e5\u969c\u7919\uff0c\u4e14\u80fd\u5feb\u901f\u4f7f\u57fa\u6e96\u6e2c\u8a66\u904e\u6642\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u4e00\u9ede\uff0c\u8a31\u591a\u6700\u8fd1\u7684\u57fa\u6e96\u6e2c\u8a66\u5f9e\u4eba\u985e\u6216 LLM \u8a55\u5be9\u4e2d\u7fa4\u773e\u5916\u5305\u65b0\u7684\u63d0\u793a\u548c\u8a55\u4f30\uff1b\u7136\u800c\uff0c\u9019\u4e9b\u53ef\u80fd\u6703\u5f15\u5165\u986f\u8457\u7684\u504f\u898b\uff0c\u4e14\u5728\u8a55\u5206\u56f0\u96e3\u7684\u554f\u984c\u6642\u6703\u5d29\u6f70\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u70ba LLM \u4ecb\u7d39\u4e86\u4e00\u500b\u65b0\u7684\u57fa\u6e96\u6e2c\u8a66\uff0c\u65e8\u5728\u5c0d\u6e2c\u8a66\u96c6\u6c61\u67d3\u548c LLM \u8a55\u5be9\u548c\u4eba\u985e\u7fa4\u773e\u5916\u5305\u7684\u9677\u9631\u514d\u75ab\u3002\u6211\u5011\u767c\u5e03 LiveBench\uff0c\u9019\u662f\u7b2c\u4e00\u500b (1) \u5305\u542b\u4f86\u81ea\u8fd1\u671f\u8cc7\u8a0a\u4f86\u6e90\u7684\u983b\u7e41\u66f4\u65b0\u554f\u984c\u3001(2) \u6839\u64da\u5ba2\u89c0\u57fa\u672c\u4e8b\u5be6\u503c\u81ea\u52d5\u8a55\u5206\u7b54\u6848\uff0c\u4ee5\u53ca (3) \u5305\u542b\u5404\u7a2e\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\uff0c\u6db5\u84cb\u6578\u5b78\u3001\u7de8\u78bc\u3001\u63a8\u7406\u3001\u8a9e\u8a00\u3001\u9075\u5faa\u6307\u793a\u548c\u8cc7\u6599\u5206\u6790\u7684\u57fa\u6e96\u6e2c\u8a66\u3002\u70ba\u4e86\u9054\u6210\u9019\u4e00\u9ede\uff0cLiveBench \u5305\u542b\u57fa\u65bc\u6700\u8fd1\u767c\u5e03\u7684\u6578\u5b78\u7af6\u8cfd\u3001arXiv \u8ad6\u6587\u3001\u65b0\u805e\u6587\u7ae0\u548c\u8cc7\u6599\u96c6\u7684\u554f\u984c\uff0c\u4e26\u4e14\u5b83\u5305\u542b\u4f86\u81ea\u5148\u524d\u57fa\u6e96\u6e2c\u8a66\uff08\u4f8b\u5982 Big-Bench Hard\u3001AMPS \u548c IFEval\uff09\u7684\u66f4\u96e3\u3001\u7121\u6c61\u67d3\u7684\u4efb\u52d9\u7248\u672c\u3002\u6211\u5011\u8a55\u4f30\u4e86\u8a31\u591a\u8457\u540d\u7684\u9589\u6e90\u6a21\u578b\uff0c\u4ee5\u53ca\u6578\u5341\u500b\u5927\u5c0f\u5f9e 0.5B \u5230 110B \u7684\u958b\u6e90\u6a21\u578b\u3002LiveBench \u5f88\u56f0\u96e3\uff0c\u9802\u5c16\u6a21\u578b\u7684\u6e96\u78ba\u7387\u4f4e\u65bc 65%\u3002\u6211\u5011\u767c\u5e03\u6240\u6709\u554f\u984c\u3001\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u7b54\u6848\u3002\u554f\u984c\u5c07\u6309\u6708\u65b0\u589e\u548c\u66f4\u65b0\uff0c\u6211\u5011\u5c07\u96a8\u8457\u6642\u9593\u63a8\u79fb\u767c\u5e03\u65b0\u7684\u4efb\u52d9\u548c\u66f4\u56f0\u96e3\u7684\u4efb\u52d9\u7248\u672c\uff0c\u4ee5\u4fbf LiveBench \u80fd\u5340\u5206 LLM \u5728\u672a\u4f86\u6539\u9032\u6642\u7684\u5404\u7a2e\u529f\u80fd\u3002\u6211\u5011\u6b61\u8fce\u793e\u7fa4\u53c3\u8207\u548c\u5408\u4f5c\uff0c\u4ee5\u64f4\u5c55\u57fa\u6e96\u6e2c\u8a66\u4efb\u52d9\u548c\u6a21\u578b\u3002", "author": "Colin White et.al.", "authors": "Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, Micah Goldblum", "id": "2406.19314v1", "paper_url": "http://arxiv.org/abs/2406.19314v1", "repo": "https://github.com/livebench/livebench"}}