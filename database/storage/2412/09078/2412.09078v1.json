{"2412.09078": {"publish_time": "2024-12-12", "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning", "paper_summary": "Large Language Models (LLMs) have shown remarkable abilities across various\nlanguage tasks, but solving complex reasoning problems remains a challenge.\nWhile existing methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT)\nenhance reasoning by decomposing problems or structuring prompts, they\ntypically perform a single pass of reasoning and may fail to revisit flawed\npaths, compromising accuracy. To address this, we propose a novel reasoning\nframework called Forest-of-Thought (FoT), which integrates multiple reasoning\ntrees to leverage collective decision-making for solving complex logical\nproblems. FoT utilizes sparse activation strategies to select the most relevant\nreasoning paths, improving both efficiency and accuracy. Additionally, we\nintroduce a dynamic self-correction strategy that enables real-time error\ncorrection and learning from past mistakes, as well as consensus-guided\ndecision making strategies to optimize correctness and computational resources.\nExperimental results demonstrate that the FoT framework, combined with these\nstrategies, significantly enhances the reasoning capabilities of LLMs, enabling\nthem to solve complex tasks with greater precision and efficiency.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u8a9e\u8a00\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f46\u89e3\u6c7a\u8907\u96dc\u63a8\u7406\u554f\u984c\u4ecd\u7136\u662f\u4e00\u9805\u6311\u6230\u3002\u96d6\u7136\u73fe\u6709\u65b9\u6cd5\uff08\u4f8b\u5982\u601d\u8003\u93c8 (CoT) \u548c\u601d\u8003\u6a39 (ToT)\uff09\u900f\u904e\u5206\u89e3\u554f\u984c\u6216\u7d50\u69cb\u5316\u63d0\u793a\u4f86\u589e\u5f37\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u57f7\u884c\u4e00\u6b21\u63a8\u7406\uff0c\u53ef\u80fd\u7121\u6cd5\u91cd\u65b0\u5be9\u8996\u6709\u7f3a\u9677\u7684\u8def\u5f91\uff0c\u9032\u800c\u5f71\u97ff\u6e96\u78ba\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba\u601d\u8003\u68ee\u6797 (FoT) \u7684\u5275\u65b0\u63a8\u7406\u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u591a\u500b\u63a8\u7406\u6a39\uff0c\u4ee5\u5229\u7528\u96c6\u9ad4\u6c7a\u7b56\u4f86\u89e3\u6c7a\u8907\u96dc\u7684\u908f\u8f2f\u554f\u984c\u3002FoT \u5229\u7528\u7a00\u758f\u6fc0\u6d3b\u7b56\u7565\u4f86\u9078\u64c7\u6700\u76f8\u95dc\u7684\u63a8\u7406\u8def\u5f91\uff0c\u540c\u6642\u63d0\u5347\u6548\u7387\u548c\u6e96\u78ba\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u52d5\u614b\u81ea\u6211\u4fee\u6b63\u7b56\u7565\uff0c\u5b83\u80fd\u9032\u884c\u5373\u6642\u932f\u8aa4\u4fee\u6b63\uff0c\u4e26\u5f9e\u904e\u53bb\u7684\u932f\u8aa4\u4e2d\u5b78\u7fd2\uff0c\u4ee5\u53ca\u5171\u8b58\u5f15\u5c0e\u7684\u6c7a\u7b56\u5236\u5b9a\u7b56\u7565\uff0c\u4ee5\u6700\u4f73\u5316\u6b63\u78ba\u6027\u548c\u904b\u7b97\u8cc7\u6e90\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cFoT \u6846\u67b6\u7d50\u5408\u9019\u4e9b\u7b56\u7565\uff0c\u986f\u8457\u589e\u5f37\u4e86 LLM \u7684\u63a8\u7406\u80fd\u529b\uff0c\u8b93\u5b83\u5011\u80fd\u5920\u4ee5\u66f4\u9ad8\u7684\u6e96\u78ba\u6027\u548c\u6548\u7387\u89e3\u6c7a\u8907\u96dc\u4efb\u52d9\u3002", "author": "Zhenni Bi et.al.", "authors": "Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, Yunhe Wang", "id": "2412.09078v1", "paper_url": "http://arxiv.org/abs/2412.09078v1", "repo": "null"}}