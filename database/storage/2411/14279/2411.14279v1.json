{"2411.14279": {"publish_time": "2024-11-21", "title": "Looking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance", "paper_summary": "Large vision-language models (LVLMs) have achieved impressive results in\nvarious vision-language tasks. However, despite showing promising performance,\nLVLMs suffer from hallucinations caused by language bias, leading to diminished\nfocus on images and ineffective visual comprehension. We identify two primary\nreasons for this bias: 1. Different scales of training data between the\npretraining stage of LLM and multimodal alignment stage. 2. The learned\ninference bias due to short-term dependency of text data. Therefore, we propose\nLACING, a systemic framework designed to address the language bias of LVLMs\nwith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).\nSpecifically, MDA introduces a parallel dual-attention mechanism that enhances\nthe integration of visual inputs across the model. IFG introduces a learnable\nsoft visual prompt during training and inference to replace visual inputs,\ndesigned to compel LVLMs to prioritize text inputs. Then, IFG further proposes\na novel decoding strategy using the soft visual prompt to mitigate the model's\nover-reliance on adjacent text inputs. Comprehensive experiments demonstrate\nthat our method effectively debiases LVLMs from their language bias, enhancing\nvisual comprehension and reducing hallucinations without requiring additional\ntraining resources or data. The code and model are available at\n[lacing-lvlm.github.io](https://lacing-lvlm.github.io).", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5df2\u5728\u5404\u7a2e\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e2d\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u5118\u7ba1\u8868\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u6548\u80fd\uff0cLVLMs \u4ecd\u6703\u56e0\u8a9e\u8a00\u504f\u8aa4\u800c\u7522\u751f\u5e7b\u89ba\uff0c\u5c0e\u81f4\u5c0d\u5f71\u50cf\u7684\u95dc\u6ce8\u5ea6\u964d\u4f4e\u548c\u8996\u89ba\u7406\u89e3\u529b\u4e0d\u4f73\u3002\u6211\u5011\u627e\u51fa\u9020\u6210\u9019\u7a2e\u504f\u8aa4\u7684\u5169\u500b\u4e3b\u8981\u539f\u56e0\uff1a1. LLM \u9810\u8a13\u7df4\u968e\u6bb5\u8207\u591a\u6a21\u614b\u5c0d\u9f4a\u968e\u6bb5\u7684\u8a13\u7df4\u8cc7\u6599\u898f\u6a21\u4e0d\u540c\u30022. \u7531\u65bc\u6587\u5b57\u8cc7\u6599\u7684\u77ed\u671f\u4f9d\u8cf4\u6027\u6240\u7522\u751f\u7684\u5b78\u7fd2\u63a8\u8ad6\u504f\u8aa4\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa LACING\uff0c\u4e00\u500b\u7cfb\u7d71\u6027\u6846\u67b6\uff0c\u65e8\u5728\u900f\u904e\u591a\u6a21\u614b\u96d9\u6ce8\u610f\u529b\u6a5f\u5236 (MDA) \u548c\u8edf\u5f71\u50cf\u5f15\u5c0e (IFG) \u4f86\u89e3\u6c7a LVLMs \u7684\u8a9e\u8a00\u504f\u8aa4\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cMDA \u5f15\u5165\u4e00\u500b\u5e73\u884c\u7684\u96d9\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u589e\u5f37\u6a21\u578b\u4e2d\u8996\u89ba\u8f38\u5165\u7684\u6574\u5408\u3002IFG \u5728\u8a13\u7df4\u548c\u63a8\u8ad6\u671f\u9593\u5f15\u5165\u4e00\u500b\u53ef\u5b78\u7fd2\u7684\u8edf\u8996\u89ba\u63d0\u793a\uff0c\u4ee5\u53d6\u4ee3\u8996\u89ba\u8f38\u5165\uff0c\u65e8\u5728\u8feb\u4f7f LVLMs \u512a\u5148\u8003\u616e\u6587\u5b57\u8f38\u5165\u3002\u7136\u5f8c\uff0cIFG \u9032\u4e00\u6b65\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u89e3\u78bc\u7b56\u7565\uff0c\u4f7f\u7528\u8edf\u8996\u89ba\u63d0\u793a\u4f86\u6e1b\u8f15\u6a21\u578b\u5c0d\u76f8\u9130\u6587\u5b57\u8f38\u5165\u7684\u904e\u5ea6\u4f9d\u8cf4\u3002\u5168\u9762\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u6709\u6548\u5730\u6d88\u9664\u4e86 LVLMs \u7684\u8a9e\u8a00\u504f\u8aa4\uff0c\u589e\u5f37\u4e86\u8996\u89ba\u7406\u89e3\u529b\uff0c\u4e26\u6e1b\u5c11\u4e86\u5e7b\u89ba\uff0c\u800c\u4e0d\u9700\u8981\u984d\u5916\u7684\u8a13\u7df4\u8cc7\u6e90\u6216\u8cc7\u6599\u3002\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u5728 [lacing-lvlm.github.io](https://lacing-lvlm.github.io) \u53d6\u5f97\u3002", "author": "Haozhe Zhao et.al.", "authors": "Haozhe Zhao, Shuzheng Si, Liang Chen, Yichi Zhang, Maosong Sun, Mingjia Zhang, Baobao Chang", "id": "2411.14279v1", "paper_url": "http://arxiv.org/abs/2411.14279v1", "repo": "null"}}