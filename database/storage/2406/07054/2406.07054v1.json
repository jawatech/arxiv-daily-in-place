{"2406.07054": {"publish_time": "2024-06-11", "title": "CoEvol: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation", "paper_summary": "In recent years, instruction fine-tuning (IFT) on large language models\n(LLMs) has garnered considerable attention to enhance model performance on\nunseen tasks. Attempts have been made on automatic construction and effective\nselection for IFT data. However, we posit that previous methods have not fully\nharnessed the potential of LLMs for enhancing data quality. The responses\nwithin IFT data could be further enhanced by leveraging the capabilities of\nLLMs themselves. In this paper, we propose CoEvol, an LLM-based multi-agent\ncooperation framework for the improvement of responses to instructions. To\neffectively refine the responses, we develop an iterative framework following a\ndebate-advise-edit-judge paradigm. A two-stage multi-agent debate strategy is\nfurther devised to ensure the diversity and reliability of editing suggestions\nwithin the framework. Empirically, models equipped with CoEvol outperform\ncompetitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its\neffectiveness in enhancing instruction-following capabilities for LLMs.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e0a\u7684\u6307\u4ee4\u5fae\u8abf (IFT) \u5df2\u5f15\u8d77\u76f8\u7576\u5927\u7684\u95dc\u6ce8\uff0c\u4ee5\u589e\u5f37\u6a21\u578b\u5728 unseen \u4efb\u52d9\u4e0a\u7684\u6548\u80fd\u3002\u5df2\u5617\u8a66\u81ea\u52d5\u5efa\u69cb\u548c\u6709\u6548\u9078\u64c7 IFT \u8cc7\u6599\u3002\u7136\u800c\uff0c\u6211\u5011\u8a8d\u70ba\u5148\u524d\u7684\u6280\u8853\u4e26\u672a\u5145\u5206\u5229\u7528 LLM \u4f86\u589e\u5f37\u8cc7\u6599\u54c1\u8cea\u3002IFT \u8cc7\u6599\u4e2d\u7684\u56de\u61c9\u53ef\u4ee5\u900f\u904e\u5229\u7528 LLM \u672c\u8eab\u7684\u6548\u80fd\u9032\u4e00\u6b65\u589e\u5f37\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa CoEvol\uff0c\u4e00\u500b\u57fa\u65bc LLM \u7684\u591a\u4e3b\u9ad4\u5408\u4f5c\u67b6\u69cb\uff0c\u7528\u65bc\u6539\u5584\u5c0d\u6307\u4ee4\u7684\u56de\u61c9\u3002\u70ba\u4e86\u6709\u6548\u6539\u5584\u56de\u61c9\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u9075\u5faa\u8faf\u8ad6-\u5efa\u8b70-\u7de8\u8f2f-\u5224\u65b7\u7bc4\u5f0f\u7684\u53cd\u8986\u904b\u7b97\u67b6\u69cb\u3002\u9032\u4e00\u6b65\u8a2d\u8a08\u4e86\u4e00\u500b\u5169\u968e\u6bb5\u591a\u4e3b\u9ad4\u8faf\u8ad6\u7b56\u7565\uff0c\u4ee5\u78ba\u4fdd\u67b6\u69cb\u4e2d\u7de8\u8f2f\u5efa\u8b70\u7684\u591a\u6a23\u6027\u548c\u53ef\u9760\u6027\u3002\u6839\u64da\u7d93\u9a57\uff0c\u914d\u5099 CoEvol \u7684\u6a21\u578b\u512a\u65bc\u7531 MT-Bench \u548c AlpacaEval \u8a55\u4f30\u7684\u7af6\u722d\u57fa\u7dda\uff0c\u8b49\u660e\u5176\u5728\u589e\u5f37 LLM \u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Renhao Li et.al.", "authors": "Renhao Li, Minghuan Tan, Derek F. Wong, Min Yang", "id": "2406.07054v1", "paper_url": "http://arxiv.org/abs/2406.07054v1", "repo": "https://github.com/lirenhao1997/coevol"}}