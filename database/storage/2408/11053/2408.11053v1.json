{"2408.11053": {"publish_time": "2024-08-20", "title": "Revisiting VerilogEval: Newer LLMs, In-Context Learning, and Specification-to-RTL Tasks", "paper_summary": "The application of large-language models (LLMs) to digital hardware code\ngeneration is an emerging field. Most LLMs are primarily trained on natural\nlanguage and software code. Hardware code, such as Verilog, represents only a\nsmall portion of the training data and few hardware benchmarks exist. To\naddress this gap, the open-source VerilogEval benchmark was released in 2023,\nproviding a consistent evaluation framework for LLMs on code completion tasks.\nIt was tested on state-of-the-art models at the time including GPT-4. However,\nVerilogEval and other Verilog generation benchmarks lack failure analysis and,\nin present form, are not conducive to exploring prompting techniques. Also,\nsince VerilogEval's release, both commercial and open-source models have seen\ncontinued development.\n  In this work, we evaluate new commercial and open-source models of varying\nsizes against an improved VerilogEval benchmark suite. We enhance VerilogEval's\ninfrastructure and dataset by automatically classifying failures, introduce new\nprompts for supporting in-context learning (ICL) examples, and extend the\nsupported tasks to specification-to-RTL translation. We find a measurable\nimprovement in commercial state-of-the-art models, with GPT-4 Turbo achieving a\n59% pass rate on spec-to-RTL tasks. We also study the performance of\nopen-source and domain-specific models that have emerged, and demonstrate that\nmodels can benefit substantially from ICL. We find that recently-released Llama\n3.1 405B achieves a pass rate of 58%, effectively matching that of GPT-4 Turbo,\nand that the much smaller domain-specific RTL-Coder 6.7B models achieve an\nimpressive 37% pass rate. However, prompt engineering is key to achieving good\npass rates, and varies widely with model and task. A benchmark infrastructure\nthat allows for prompt engineering and failure analysis is key to continued\nmodel development and deployment.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6578\u4f4d\u786c\u9ad4\u7a0b\u5f0f\u78bc\u7522\u751f\u4e2d\u7684\u61c9\u7528\u662f\u4e00\u500b\u65b0\u8208\u9818\u57df\u3002\u5927\u591a\u6578\u7684 LLM \u4e3b\u8981\u662f\u5728\u81ea\u7136\u8a9e\u8a00\u548c\u8edf\u9ad4\u7a0b\u5f0f\u78bc\u4e0a\u8a13\u7df4\u3002\u786c\u9ad4\u7a0b\u5f0f\u78bc\uff0c\u4f8b\u5982 Verilog\uff0c\u53ea\u4f54\u8a13\u7df4\u8cc7\u6599\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u800c\u4e14\u53ea\u6709\u5c11\u6578\u786c\u9ad4\u57fa\u6e96\u6e2c\u8a66\u5b58\u5728\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u958b\u653e\u539f\u59cb\u78bc\u7684 VerilogEval \u57fa\u6e96\u6e2c\u8a66\u65bc 2023 \u5e74\u767c\u5e03\uff0c\u70ba LLM \u5728\u7a0b\u5f0f\u78bc\u5b8c\u6210\u4efb\u52d9\u4e0a\u63d0\u4f9b\u4e86\u4e00\u500b\u4e00\u81f4\u7684\u8a55\u4f30\u67b6\u69cb\u3002\u5b83\u5728\u7576\u6642\u6700\u5148\u9032\u7684\u6a21\u578b\u4e0a\u9032\u884c\u4e86\u6e2c\u8a66\uff0c\u5305\u62ec GPT-4\u3002\u7136\u800c\uff0cVerilogEval \u548c\u5176\u4ed6 Verilog \u7522\u751f\u57fa\u6e96\u6e2c\u8a66\u7f3a\u4e4f\u6545\u969c\u5206\u6790\uff0c\u800c\u4e14\u4ee5\u76ee\u524d\u7684\u578b\u5f0f\uff0c\u4e0d\u5229\u65bc\u63a2\u7d22\u63d0\u793a\u6280\u8853\u3002\u6b64\u5916\uff0c\u81ea VerilogEval \u767c\u5e03\u4ee5\u4f86\uff0c\u5546\u696d\u548c\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u90fd\u6301\u7e8c\u767c\u5c55\u3002\n\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u91dd\u5c0d\u6539\u826f\u7684 VerilogEval \u57fa\u6e96\u6e2c\u8a66\u5957\u4ef6\u8a55\u4f30\u5404\u7a2e\u898f\u6a21\u7684\u65b0\u5546\u696d\u548c\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u3002\u6211\u5011\u900f\u904e\u81ea\u52d5\u5206\u985e\u6545\u969c\u4f86\u589e\u5f37 VerilogEval \u7684\u57fa\u790e\u67b6\u69cb\u548c\u8cc7\u6599\u96c6\uff0c\u5f15\u5165\u65b0\u7684\u63d0\u793a\u4f86\u652f\u63f4\u60c5\u5883\u5b78\u7fd2 (ICL) \u7bc4\u4f8b\uff0c\u4e26\u5c07\u652f\u63f4\u7684\u4efb\u52d9\u64f4\u5145\u5230\u898f\u683c\u5230 RTL \u7684\u8f49\u63db\u3002\u6211\u5011\u767c\u73fe\u5546\u696d\u6700\u5148\u9032\u7684\u6a21\u578b\u6709\u986f\u8457\u7684\u9032\u6b65\uff0cGPT-4 Turbo \u5728\u898f\u683c\u5230 RTL \u4efb\u52d9\u4e2d\u9054\u5230 59% \u7684\u901a\u904e\u7387\u3002\u6211\u5011\u4e5f\u7814\u7a76\u4e86\u65b0\u8208\u7684\u958b\u653e\u539f\u59cb\u78bc\u548c\u7279\u5b9a\u9818\u57df\u6a21\u578b\u7684\u6548\u80fd\uff0c\u4e26\u8b49\u660e\u6a21\u578b\u53ef\u4ee5\u5f9e ICL \u4e2d\u7372\u5f97\u986f\u8457\u7684\u76ca\u8655\u3002\u6211\u5011\u767c\u73fe\u6700\u8fd1\u767c\u5e03\u7684 Llama 3.1 405B \u9054\u5230\u4e86 58% \u7684\u901a\u904e\u7387\uff0c\u6709\u6548\u5730\u8207 GPT-4 Turbo \u76f8\u5339\u914d\uff0c\u800c\u5c0f\u5f97\u591a\u7684\u7279\u5b9a\u9818\u57df RTL-Coder 6.7B \u6a21\u578b\u5247\u9054\u5230\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684 37% \u901a\u904e\u7387\u3002\u7136\u800c\uff0c\u63d0\u793a\u5de5\u7a0b\u662f\u9054\u6210\u826f\u597d\u901a\u904e\u7387\u7684\u95dc\u9375\uff0c\u800c\u4e14\u6703\u96a8\u8457\u6a21\u578b\u548c\u4efb\u52d9\u800c\u6709\u5f88\u5927\u7684\u4e0d\u540c\u3002\u4e00\u500b\u5141\u8a31\u63d0\u793a\u5de5\u7a0b\u548c\u6545\u969c\u5206\u6790\u7684\u57fa\u6e96\u6e2c\u8a66\u57fa\u790e\u67b6\u69cb\u662f\u6301\u7e8c\u6a21\u578b\u958b\u767c\u548c\u90e8\u7f72\u7684\u95dc\u9375\u3002", "author": "Nathaniel Pinckney et.al.", "authors": "Nathaniel Pinckney, Christopher Batten, Mingjie Liu, Haoxing Ren, Brucek Khailany", "id": "2408.11053v1", "paper_url": "http://arxiv.org/abs/2408.11053v1", "repo": "null"}}