{"2501.18578": {"publish_time": "2025-01-30", "title": "R.I.P.: Better Models by Survival of the Fittest Prompts", "paper_summary": "Training data quality is one of the most important drivers of final model\nquality. In this work, we introduce a method for evaluating data integrity\nbased on the assumption that low-quality input prompts result in high variance\nand low quality responses. This is achieved by measuring the rejected response\nquality and the reward gap between the chosen and rejected preference pair. Our\nmethod, Rejecting Instruction Preferences (RIP) can be used to filter prompts\nfrom existing training sets, or to make high quality synthetic datasets,\nyielding large performance gains across various benchmarks compared to\nunfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win\nRate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama\n3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th\nplace to 6th overall in the leaderboard.", "paper_summary_zh": "\u8a13\u7df4\u8cc7\u6599\u54c1\u8cea\u662f\u6700\u7d42\u6a21\u578b\u54c1\u8cea\u6700\u91cd\u8981\u7684\u9a45\u52d5\u529b\u4e4b\u4e00\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u8a55\u4f30\u8cc7\u6599\u5b8c\u6574\u6027\u7684\u65b9\u6cd5\uff0c\u57fa\u65bc\u4f4e\u54c1\u8cea\u8f38\u5165\u63d0\u793a\u6703\u5c0e\u81f4\u9ad8\u8b8a\u7570\u548c\u4f4e\u54c1\u8cea\u56de\u61c9\u7684\u5047\u8a2d\u3002\u9019\u662f\u900f\u904e\u6e2c\u91cf\u88ab\u62d2\u7d55\u7684\u56de\u61c9\u54c1\u8cea\uff0c\u4ee5\u53ca\u9078\u64c7\u548c\u88ab\u62d2\u7d55\u7684\u504f\u597d\u914d\u5c0d\u4e4b\u9593\u7684\u734e\u52f5\u5dee\u8ddd\u4f86\u5be6\u73fe\u7684\u3002\u6211\u5011\u7684 Rejecting Instruction Preferences (RIP) \u65b9\u6cd5\u53ef\u7528\u65bc\u5f9e\u73fe\u6709\u7684\u8a13\u7df4\u96c6\u4e2d\u904e\u6ffe\u63d0\u793a\uff0c\u6216\u5efa\u7acb\u9ad8\u54c1\u8cea\u7684\u5408\u6210\u8cc7\u6599\u96c6\uff0c\u8207\u672a\u904e\u6ffe\u8cc7\u6599\u76f8\u6bd4\uff0c\u5728\u5404\u7a2e\u57fa\u6e96\u4e0a\u7522\u751f\u5de8\u5927\u7684\u6548\u80fd\u63d0\u5347\u3002\u4f7f\u7528 Llama 3.1-8B-Instruct\uff0cRIP \u5c07 AlpacaEval2 LC \u52dd\u7387\u63d0\u5347\u4e86 9.4%\uff0cArena-Hard \u63d0\u5347\u4e86 8.7%\uff0cWildBench \u63d0\u5347\u4e86 9.9%\u3002\u4f7f\u7528 Llama 3.3-70B-Instruct\uff0cRIP \u5c07 Arena-Hard \u5f9e 67.5 \u63d0\u5347\u5230 82.9\uff0c\u5728\u6392\u884c\u699c\u4e0a\u5f9e\u7b2c 18 \u540d\u8e8d\u5347\u5230\u7b2c 6 \u540d\u3002", "author": "Ping Yu et.al.", "authors": "Ping Yu, Weizhe Yuan, Olga Golovneva, Tianhao Wu, Sainbayar Sukhbaatar, Jason Weston, Jing Xu", "id": "2501.18578v1", "paper_url": "http://arxiv.org/abs/2501.18578v1", "repo": "null"}}