{"2412.11242": {"publish_time": "2024-12-15", "title": "TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs", "paper_summary": "Specializing large language models (LLMs) for local deployment in\ndomain-specific use cases is necessary for strong performance while meeting\nlatency and privacy constraints. However, conventional task-specific adaptation\napproaches do not show simultaneous memory saving and inference speedup at\ndeployment time. Practical compression techniques like quantization and pruning\nrequire dedicated hardware or kernel support to achieve measured inference\nspeedup. We develop TrimLLM based on the layer-wise specialization phenomenon\nwe empirically observed and verified on contemporary LLMs. TrimLLM reduces the\ndepth of LLMs via progressive layer dropping. We show it retains LLMs' capacity\nin specific domains and achieves inference speedup irrespective of hardware and\ndeep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for\ninference; models adapted on medical, legal, and financial datasets all\ndemonstrate $2.1-5.7\\times$ inference speedup on consumer GPUs and up to\n$3.1\\times$ speedup on A100 when compared to state-of-the-art model compression\nalgorithms, with no loss in accuracy at 50$\\sim$60\\% model compression ratio.", "paper_summary_zh": "\u91dd\u5c0d\u7279\u5b9a\u9818\u57df\u4f7f\u7528\u6848\u4f8b\uff0c\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c08\u9580\u5316\u70ba\u5728\u5730\u90e8\u7f72\u5c0d\u65bc\u5728\u6eff\u8db3\u5ef6\u9072\u548c\u96b1\u79c1\u9650\u5236\u7684\u540c\u6642\uff0c\u5c55\u73fe\u5f37\u52c1\u6548\u80fd\u662f\u5fc5\u8981\u7684\u3002\u7136\u800c\uff0c\u50b3\u7d71\u7684\u4efb\u52d9\u7279\u5b9a\u9069\u61c9\u65b9\u6cd5\u4e26\u672a\u5728\u90e8\u7f72\u6642\u540c\u6642\u5c55\u73fe\u8a18\u61b6\u9ad4\u7bc0\u7701\u548c\u63a8\u8ad6\u52a0\u901f\u3002\u91cf\u5316\u548c\u526a\u679d\u7b49\u5be6\u7528\u7684\u58d3\u7e2e\u6280\u8853\u9700\u8981\u5c08\u7528\u7684\u786c\u9ad4\u6216\u6838\u5fc3\u652f\u63f4\uff0c\u624d\u80fd\u9054\u6210\u91cf\u6e2c\u7684\u63a8\u8ad6\u52a0\u901f\u3002\u6211\u5011\u6839\u64da\u7d93\u9a57\u89c0\u5bdf\u4e26\u9a57\u8b49\u5728\u7576\u4ee3 LLM \u4e0a\u7684\u9010\u5c64\u5c08\u696d\u5316\u73fe\u8c61\uff0c\u958b\u767c\u4e86 TrimLLM\u3002TrimLLM \u900f\u904e\u6f38\u9032\u5f0f\u5c64\u7d1a\u6368\u68c4\uff0c\u6e1b\u5c11 LLM \u7684\u6df1\u5ea6\u3002\u6211\u5011\u986f\u793a\u5b83\u4fdd\u7559\u4e86 LLM \u5728\u7279\u5b9a\u9818\u57df\u7684\u80fd\u529b\uff0c\u4e26\u5728\u4e0d\u8ad6\u786c\u9ad4\u548c\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\u7684\u60c5\u6cc1\u4e0b\uff0c\u9054\u6210\u63a8\u8ad6\u52a0\u901f\u3002\u6211\u5011\u91dd\u5c0d\u4e0d\u540c\u898f\u6a21\u7684 LLM \u8a55\u4f30 TrimLLM \u7684\u63a8\u8ad6\uff1b\u91dd\u5c0d\u91ab\u7642\u3001\u6cd5\u5f8b\u548c\u8ca1\u52d9\u8cc7\u6599\u96c6\u6240\u8abf\u6574\u7684\u6a21\u578b\uff0c\u8207\u6700\u5148\u9032\u7684\u6a21\u578b\u58d3\u7e2e\u6f14\u7b97\u6cd5\u76f8\u6bd4\uff0c\u5728\u6d88\u8cbb\u8005 GPU \u4e0a\u7686\u5c55\u73fe $2.1-5.7\\times$ \u7684\u63a8\u8ad6\u52a0\u901f\uff0c\u800c\u5728 A100 \u4e0a\u5247\u52a0\u901f\u9054 $3.1\\times$\uff0c\u5728\u6a21\u578b\u58d3\u7e2e\u7387\u70ba 50$\\sim$60% \u6642\uff0c\u6e96\u78ba\u5ea6\u6c92\u6709\u640d\u5931\u3002", "author": "Lanxiang Hu et.al.", "authors": "Lanxiang Hu, Tajana Rosing, Hao Zhang", "id": "2412.11242v1", "paper_url": "http://arxiv.org/abs/2412.11242v1", "repo": "null"}}