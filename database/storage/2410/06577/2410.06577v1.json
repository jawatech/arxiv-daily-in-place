{"2410.06577": {"publish_time": "2024-10-09", "title": "Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions", "paper_summary": "Recent advancements in Transformer-based large language models (LLMs) have\nset new standards in natural language processing. However, the classical\nsoftmax attention incurs significant computational costs, leading to a $O(T)$\ncomplexity for per-token generation, where $T$ represents the context length.\nThis work explores reducing LLMs' complexity while maintaining performance by\nintroducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an\ninnovative data-dependent tempered selection (DDTS) mechanism within a linear\nattention-based, purely recurrent framework, achieving significant accuracy\nwhile drastically reducing the memory usage typically associated with recurrent\nmodels. This method exemplifies semantic compression by maintaining essential\ninput information with fixed-size hidden states. Building on this, Rodimus$+$\ncombines Rodimus with the innovative Sliding Window Shared-Key Attention\n(SW-SKA) in a hybrid approach, effectively leveraging the complementary\nsemantic, token, and head compression techniques. Our experiments demonstrate\nthat Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior\ndownstream performance against models trained on more tokens, including\nQwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the\naccuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints\nwill be available soon.", "paper_summary_zh": "<paragraph>Transformer\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4e2d\u6a39\u7acb\u4e86\u65b0\u6a19\u6e96\u3002\u7136\u800c\uff0c\u50b3\u7d71\u7684 softmax \u6ce8\u610f\u529b\u6703\u7522\u751f\u5927\u91cf\u7684\u904b\u7b97\u6210\u672c\uff0c\u5c0e\u81f4\u6bcf\u500b token \u7522\u751f\u7684\u8907\u96dc\u5ea6\u70ba $O(T)$, \u5176\u4e2d $T$ \u4ee3\u8868\u8108\u7d61\u9577\u5ea6\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u5728\u7dad\u6301\u6548\u80fd\u7684\u540c\u6642\u964d\u4f4e LLM \u8907\u96dc\u5ea6\u7684\u53ef\u80fd\u6027\uff0c\u65b9\u6cd5\u662f\u5f15\u5165 Rodimus \u53ca\u5176\u589e\u5f37\u7248\u672c Rodimus$+$. Rodimus \u5728\u7d14\u905e\u8ff4\u67b6\u69cb\u7684\u7dda\u6027\u6ce8\u610f\u529b\u57fa\u790e\u4e0a\u63a1\u7528\u5275\u65b0\u7684\u8cc7\u6599\u76f8\u95dc\u7de9\u548c\u9078\u64c7 (DDTS) \u6a5f\u5236\uff0c\u5728\u5927\u5e45\u6e1b\u5c11\u901a\u5e38\u8207\u905e\u8ff4\u6a21\u578b\u76f8\u95dc\u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u7684\u60c5\u6cc1\u4e0b\uff0c\u9054\u5230\u4e86\u986f\u8457\u7684\u6e96\u78ba\u6027\u3002\u6b64\u65b9\u6cd5\u900f\u904e\u7dad\u6301\u5177\u6709\u56fa\u5b9a\u5927\u5c0f\u96b1\u85cf\u72c0\u614b\u7684\u57fa\u672c\u8f38\u5165\u8cc7\u8a0a\uff0c\u4f86\u4f8b\u8b49\u8a9e\u610f\u58d3\u7e2e\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0cRodimus$+$ \u5c07 Rodimus \u8207\u5275\u65b0\u7684\u6ed1\u52d5\u8996\u7a97\u5171\u4eab\u91d1\u9470\u6ce8\u610f\u529b (SW-SKA) \u7d50\u5408\u5728\u6df7\u5408\u65b9\u6cd5\u4e2d\uff0c\u6709\u6548\u5730\u5229\u7528\u4e86\u4e92\u88dc\u7684\u8a9e\u610f\u3001token \u548c\u982d\u90e8\u58d3\u7e2e\u6280\u8853\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u5728 1 \u5146\u500b token \u4e0a\u8a13\u7df4\u7684 Rodimus$+$-1.6B\uff0c\u76f8\u8f03\u65bc\u5728\u66f4\u591a token \u4e0a\u8a13\u7df4\u7684\u6a21\u578b\uff08\u5305\u62ec Qwen2-1.5B \u548c RWKV6-1.6B\uff09\u9054\u5230\u4e86\u66f4\u4f73\u7684\u4e0b\u6e38\u6548\u80fd\uff0c\u7a81\u986f\u4e86\u5176\u91cd\u65b0\u5b9a\u7fa9 LLM \u4e2d\u6e96\u78ba\u6027\u8207\u6548\u7387\u5e73\u8861\u7684\u6f5b\u529b\u3002\u6a21\u578b\u7a0b\u5f0f\u78bc\u548c\u9810\u5148\u8a13\u7df4\u7684\u6aa2\u67e5\u9ede\u5c07\u5f88\u5feb\u63d0\u4f9b\u3002</paragraph>", "author": "Zhihao He et.al.", "authors": "Zhihao He, Hang Yu, Zi Gong, Shizhan Liu, Jianguo Li, Weiyao Lin", "id": "2410.06577v1", "paper_url": "http://arxiv.org/abs/2410.06577v1", "repo": "null"}}