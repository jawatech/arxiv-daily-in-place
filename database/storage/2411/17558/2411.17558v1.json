{"2411.17558": {"publish_time": "2024-11-26", "title": "Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey", "paper_summary": "Visual Question Answering (VQA) is a challenge task that combines natural\nlanguage processing and computer vision techniques and gradually becomes a\nbenchmark test task in multimodal large language models (MLLMs). The goal of\nour survey is to provide an overview of the development of VQA and a detailed\ndescription of the latest models with high timeliness. This survey gives an\nup-to-date synthesis of natural language understanding of images and text, as\nwell as the knowledge reasoning module based on image-question information on\nthe core VQA tasks. In addition, we elaborate on recent advances in extracting\nand fusing modal information with vision-language pretraining models and\nmultimodal large language models in VQA. We also exhaustively review the\nprogress of knowledge reasoning in VQA by detailing the extraction of internal\nknowledge and the introduction of external knowledge. Finally, we present the\ndatasets of VQA and different evaluation metrics and discuss possible\ndirections for future work.", "paper_summary_zh": "\u8996\u89ba\u554f\u7b54 (VQA) \u662f\u4e00\u9805\u7d50\u5408\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u548c\u96fb\u8166\u8996\u89ba\u6280\u8853\u7684\u6311\u6230\u6027\u4efb\u52d9\uff0c\u4e26\u9010\u6f38\u6210\u70ba\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u57fa\u6e96\u6e2c\u8a66\u4efb\u52d9\u3002\u6211\u5011\u7684\u8abf\u67e5\u76ee\u6a19\u662f\u63d0\u4f9b VQA \u767c\u5c55\u7684\u6982\u89c0\uff0c\u4e26\u8a73\u7d30\u63cf\u8ff0\u5177\u6709\u9ad8\u5ea6\u6642\u6548\u6027\u7684\u6700\u65b0\u6a21\u578b\u3002\u9019\u9805\u8abf\u67e5\u63d0\u4f9b\u4e86\u5716\u50cf\u548c\u6587\u5b57\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u7684\u6700\u65b0\u7d9c\u5408\uff0c\u4ee5\u53ca\u57fa\u65bc\u6838\u5fc3 VQA \u4efb\u52d9\u4e2d\u5716\u50cf\u554f\u984c\u8cc7\u8a0a\u7684\u77e5\u8b58\u63a8\u7406\u6a21\u7d44\u3002\u6b64\u5916\uff0c\u6211\u5011\u95e1\u8ff0\u4e86\u5728 VQA \u4e2d\u4f7f\u7528\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4\u6a21\u578b\u548c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u63d0\u53d6\u548c\u878d\u5408\u6a21\u614b\u8cc7\u8a0a\u7684\u6700\u65b0\u9032\u5c55\u3002\u6211\u5011\u4e5f\u8a73\u76e1\u5730\u56de\u9867\u4e86 VQA \u4e2d\u77e5\u8b58\u63a8\u7406\u7684\u9032\u5c55\uff0c\u8a73\u7d30\u8aaa\u660e\u5167\u90e8\u77e5\u8b58\u7684\u63d0\u53d6\u548c\u5916\u90e8\u77e5\u8b58\u7684\u5f15\u5165\u3002\u6700\u5f8c\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 VQA \u7684\u8cc7\u6599\u96c6\u548c\u4e0d\u540c\u7684\u8a55\u4f30\u6307\u6a19\uff0c\u4e26\u8a0e\u8ad6\u672a\u4f86\u5de5\u4f5c\u7684\u53ef\u80fd\u65b9\u5411\u3002", "author": "Jiayi Kuang et.al.", "authors": "Jiayi Kuang, Jingyou Xie, Haohao Luo, Ronghao Li, Zhe Xu, Xianfeng Cheng, Yinghui Li, Xika Lin, Ying Shen", "id": "2411.17558v1", "paper_url": "http://arxiv.org/abs/2411.17558v1", "repo": "null"}}