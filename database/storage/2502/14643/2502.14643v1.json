{"2502.14643": {"publish_time": "2025-02-20", "title": "Length-Controlled Margin-Based Preference Optimization without Reference Model", "paper_summary": "Direct Preference Optimization (DPO) is a widely adopted offline algorithm\nfor preference-based reinforcement learning from human feedback (RLHF),\ndesigned to improve training simplicity and stability by redefining reward\nfunctions. However, DPO is hindered by several limitations, including length\nbias, memory inefficiency, and probability degradation. To address these\nchallenges, we propose Length-Controlled Margin-Based Preference Optimization\n(LMPO), a more efficient and robust alternative. LMPO introduces a uniform\nreference model as an upper bound for the DPO loss, enabling a more accurate\napproximation of the original optimization objective. Additionally, an average\nlog-probability optimization strategy is employed to minimize discrepancies\nbetween training and inference phases. A key innovation of LMPO lies in its\nLength-Controlled Margin-Based loss function, integrated within the\nBradley-Terry framework. This loss function regulates response length while\nsimultaneously widening the margin between preferred and rejected outputs. By\ndoing so, it mitigates probability degradation for both accepted and discarded\nresponses, addressing a significant limitation of existing methods. We evaluate\nLMPO against state-of-the-art preference optimization techniques on two\nopen-ended large language models, Mistral and LLaMA3, across six conditional\nbenchmarks. Our experimental results demonstrate that LMPO effectively controls\nresponse length, reduces probability degradation, and outperforms existing\napproaches. The code is available at \\url{https://github.com/gengxuli/LMPO}.", "paper_summary_zh": "\u76f4\u63a5\u504f\u597d\u512a\u5316 (DPO) \u662f\u4e00\u7a2e\u5ee3\u6cdb\u63a1\u7528\u7684\u96e2\u7dda\u6f14\u7b97\u6cd5\uff0c\u7528\u65bc\u5f9e\u4eba\u985e\u56de\u994b (RLHF) \u4e2d\u9032\u884c\u57fa\u65bc\u504f\u597d\u7684\u5f37\u5316\u5b78\u7fd2\uff0c\u65e8\u5728\u900f\u904e\u91cd\u65b0\u5b9a\u7fa9\u734e\u52f5\u51fd\u6578\u4f86\u63d0\u5347\u8a13\u7df4\u7684\u7c21\u6f54\u6027\u548c\u7a69\u5b9a\u6027\u3002\u7136\u800c\uff0cDPO \u53d7\u5230\u82e5\u5e72\u9650\u5236\u7684\u963b\u7919\uff0c\u5305\u62ec\u9577\u5ea6\u504f\u5dee\u3001\u8a18\u61b6\u9ad4\u6548\u7387\u4f4e\u4e0b\u548c\u6a5f\u7387\u4e0b\u964d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u9577\u5ea6\u63a7\u5236\u908a\u969b\u504f\u597d\u512a\u5316 (LMPO)\uff0c\u4e00\u7a2e\u66f4\u6709\u6548\u7387\u4e14\u7a69\u5065\u7684\u66ff\u4ee3\u65b9\u6848\u3002LMPO \u5f15\u5165\u7d71\u4e00\u53c3\u8003\u6a21\u578b\u4f5c\u70ba DPO \u640d\u5931\u7684\u4e0a\u9650\uff0c\u80fd\u5920\u66f4\u6e96\u78ba\u5730\u8fd1\u4f3c\u539f\u59cb\u6700\u4f73\u5316\u76ee\u6a19\u3002\u6b64\u5916\uff0c\u63a1\u7528\u5e73\u5747\u5c0d\u6578\u6a5f\u7387\u6700\u4f73\u5316\u7b56\u7565\u4f86\u6700\u5c0f\u5316\u8a13\u7df4\u548c\u63a8\u8ad6\u968e\u6bb5\u4e4b\u9593\u7684\u5dee\u7570\u3002LMPO \u7684\u4e00\u9805\u95dc\u9375\u5275\u65b0\u5728\u65bc\u5176\u9577\u5ea6\u63a7\u5236\u908a\u969b\u640d\u5931\u51fd\u6578\uff0c\u6574\u5408\u5728 Bradley-Terry \u67b6\u69cb\u4e2d\u3002\u6b64\u640d\u5931\u51fd\u6578\u8abf\u7bc0\u56de\u61c9\u9577\u5ea6\uff0c\u540c\u6642\u64f4\u5927\u504f\u597d\u548c\u62d2\u7d55\u8f38\u51fa\u4e4b\u9593\u7684\u908a\u969b\u3002\u85c9\u7531\u9019\u9ebc\u505a\uff0c\u5b83\u6e1b\u8f15\u4e86\u5df2\u63a5\u53d7\u548c\u5df2\u6368\u68c4\u56de\u61c9\u7684\u6a5f\u7387\u4e0b\u964d\uff0c\u89e3\u6c7a\u4e86\u73fe\u6709\u65b9\u6cd5\u7684\u91cd\u5927\u9650\u5236\u3002\u6211\u5011\u5728\u5169\u500b\u958b\u653e\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b Mistral \u548c LLaMA3 \u4e0a\uff0c\u91dd\u5c0d\u516d\u500b\u689d\u4ef6\u57fa\u6e96\uff0c\u8a55\u4f30 LMPO \u8207\u6700\u5148\u9032\u7684\u504f\u597d\u512a\u5316\u6280\u8853\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0cLMPO \u6709\u6548\u63a7\u5236\u56de\u61c9\u9577\u5ea6\uff0c\u6e1b\u5c11\u6a5f\u7387\u4e0b\u964d\uff0c\u4e26\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 \\url{https://github.com/gengxuli/LMPO} \u53d6\u5f97\u3002", "author": "Gengxu Li et.al.", "authors": "Gengxu Li, Tingyu Xia, Yi Chang, Yuan Wu", "id": "2502.14643v1", "paper_url": "http://arxiv.org/abs/2502.14643v1", "repo": "null"}}