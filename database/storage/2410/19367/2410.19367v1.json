{"2410.19367": {"publish_time": "2024-10-25", "title": "BitPipe: Bidirectional Interleaved Pipeline Parallelism for Accelerating Large Models Training", "paper_summary": "With the increasing scale of models, the need for efficient distributed\ntraining has become increasingly urgent. Recently, many synchronous pipeline\nparallelism approaches have been proposed to improve training throughput.\nHowever, these approaches still suffer from two major issues, i.e., pipeline\nbubbles caused by periodic flushing and extra communication due to the\nincreasing number of pipeline stages. To this end, we propose BitPipe, a\nbidirectional interleaved pipeline parallelism for accelerating large models\ntraining. Specifically, a hybrid scheme of fusing interleaved pipelines with\nbidirectional pipelines is proposed to reduce the computational time of each\nsingle micro-batch and multiply the number of devices executing simultaneously.\nA V-shaped schedule with eager gradient synchronization is introduced to reduce\nand overlap the communication between devices. Experiments conducted on up to\n32 GPUs show that BitPipe improves the training throughput of GPT-style and\nBERT-style models by 1.05x-1.28x compared to the state-of-the-art synchronous\napproaches. The code of our implementation is available at\nhttps://github.com/wuhouming/BitPipe.", "paper_summary_zh": "\u96a8\u8457\u6a21\u578b\u898f\u6a21\u7684\u64f4\u5927\uff0c\u5c0d\u65bc\u9ad8\u6548\u5206\u5e03\u5f0f\u8a13\u7df4\u7684\u9700\u6c42\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u8feb\u5207\u3002\u6700\u8fd1\uff0c\u5df2\u7d93\u63d0\u51fa\u4e86\u8a31\u591a\u540c\u6b65\u7ba1\u9053\u4e26\u884c\u65b9\u6cd5\u4f86\u6539\u5584\u8a13\u7df4\u541e\u5410\u91cf\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u4ecd\u7136\u5b58\u5728\u5169\u500b\u4e3b\u8981\u554f\u984c\uff0c\u5373\u7531\u9031\u671f\u6027\u6c96\u6d17\u5f15\u8d77\u7684\u7ba1\u9053\u6c23\u6ce1\uff0c\u4ee5\u53ca\u7531\u65bc\u7ba1\u9053\u968e\u6bb5\u6578\u7684\u589e\u52a0\u800c\u5c0e\u81f4\u7684\u984d\u5916\u901a\u8a0a\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa BitPipe\uff0c\u9019\u662f\u4e00\u7a2e\u96d9\u5411\u4ea4\u932f\u7ba1\u9053\u4e26\u884c\u65b9\u6cd5\uff0c\u7528\u65bc\u52a0\u901f\u5927\u578b\u6a21\u578b\u8a13\u7df4\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u63d0\u51fa\u4e86\u4e00\u7a2e\u878d\u5408\u4ea4\u932f\u7ba1\u9053\u548c\u96d9\u5411\u7ba1\u9053\u7684\u6df7\u5408\u65b9\u6848\uff0c\u4ee5\u6e1b\u5c11\u6bcf\u500b\u55ae\u4e00\u5fae\u6279\u6b21\u7684\u8a08\u7b97\u6642\u9593\uff0c\u4e26\u589e\u52a0\u540c\u6642\u57f7\u884c\u7684\u8a2d\u5099\u6578\u91cf\u3002\u5f15\u5165\u4e86\u5e36\u6709\u6025\u5207\u68af\u5ea6\u540c\u6b65\u7684 V \u5f62\u8abf\u5ea6\uff0c\u4ee5\u6e1b\u5c11\u548c\u91cd\u758a\u8a2d\u5099\u4e4b\u9593\u7684\u901a\u8a0a\u3002\u5728\u591a\u9054 32 \u500b GPU \u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8868\u660e\uff0c\u8207\u6700\u5148\u9032\u7684\u540c\u6b65\u65b9\u6cd5\u76f8\u6bd4\uff0cBitPipe \u5c07 GPT \u98a8\u683c\u548c BERT \u98a8\u683c\u6a21\u578b\u7684\u8a13\u7df4\u541e\u5410\u91cf\u63d0\u9ad8\u4e86 1.05 \u500d\u81f3 1.28 \u500d\u3002\u6211\u5011\u7684\u5be6\u73fe\u4ee3\u78bc\u53ef\u5728 https://github.com/wuhouming/BitPipe \u7372\u5f97\u3002", "author": "Houming Wu et.al.", "authors": "Houming Wu, Ling Chen, Wenjie Yu", "id": "2410.19367v1", "paper_url": "http://arxiv.org/abs/2410.19367v1", "repo": "https://github.com/wuhouming/bitpipe"}}