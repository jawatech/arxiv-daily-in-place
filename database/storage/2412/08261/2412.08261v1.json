{"2412.08261": {"publish_time": "2024-12-11", "title": "FLIP: Flow-Centric Generative Planning for General-Purpose Manipulation Tasks", "paper_summary": "We aim to develop a model-based planning framework for world models that can\nbe scaled with increasing model and data budgets for general-purpose\nmanipulation tasks with only language and vision inputs. To this end, we\npresent FLow-centric generative Planning (FLIP), a model-based planning\nalgorithm on visual space that features three key modules: 1. a multi-modal\nflow generation model as the general-purpose action proposal module; 2. a\nflow-conditioned video generation model as the dynamics module; and 3. a\nvision-language representation learning model as the value module. Given an\ninitial image and language instruction as the goal, FLIP can progressively\nsearch for long-horizon flow and video plans that maximize the discounted\nreturn to accomplish the task. FLIP is able to synthesize long-horizon plans\nacross objects, robots, and tasks with image flows as the general action\nrepresentation, and the dense flow information also provides rich guidance for\nlong-horizon video generation. In addition, the synthesized flow and video\nplans can guide the training of low-level control policies for robot execution.\nExperiments on diverse benchmarks demonstrate that FLIP can improve both the\nsuccess rates and quality of long-horizon video plan synthesis and has the\ninteractive world model property, opening up wider applications for future\nworks.", "paper_summary_zh": "\u6211\u4eec\u65e8\u5728\u4e3a\u4e16\u754c\u6a21\u578b\u5f00\u53d1\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u968f\u7740\u6a21\u578b\u548c\u6570\u636e\u9884\u7b97\u7684\u589e\u52a0\u800c\u6269\u5c55\uff0c\u4ec5\u4f7f\u7528\u8bed\u8a00\u548c\u89c6\u89c9\u8f93\u5165\u5373\u53ef\u5b8c\u6210\u901a\u7528\u64cd\u4f5c\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4ee5\u6d41\u4e3a\u4e2d\u5fc3\u7684\u751f\u6210\u5f0f\u89c4\u5212 (FLIP)\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7a7a\u95f4\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u7b97\u6cd5\uff0c\u5177\u6709\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a1. \u591a\u6a21\u6001\u6d41\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u901a\u7528\u52a8\u4f5c\u63d0\u8bae\u6a21\u5757\uff1b2. \u6d41\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u52a8\u529b\u5b66\u6a21\u5757\uff1b3. \u89c6\u89c9\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u4ef7\u503c\u6a21\u5757\u3002\u7ed9\u5b9a\u4e00\u4e2a\u521d\u59cb\u56fe\u50cf\u548c\u8bed\u8a00\u6307\u4ee4\u4f5c\u4e3a\u76ee\u6807\uff0cFLIP \u53ef\u4ee5\u9010\u6b65\u641c\u7d22\u957f\u89c6\u91ce\u6d41\u548c\u89c6\u9891\u8ba1\u5212\uff0c\u4ee5\u6700\u5927\u5316\u6298\u6263\u56de\u62a5\u4ee5\u5b8c\u6210\u4efb\u52a1\u3002FLIP \u80fd\u591f\u8de8\u5bf9\u8c61\u3001\u673a\u5668\u4eba\u548c\u4efb\u52a1\u7efc\u5408\u957f\u89c6\u91ce\u8ba1\u5212\uff0c\u4ee5\u56fe\u50cf\u6d41\u4f5c\u4e3a\u901a\u7528\u52a8\u4f5c\u8868\u793a\uff0c\u5e76\u4e14\u5bc6\u96c6\u6d41\u4fe1\u606f\u8fd8\u4e3a\u957f\u89c6\u91ce\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6307\u5bfc\u3002\u6b64\u5916\uff0c\u5408\u6210\u7684\u6d41\u548c\u89c6\u9891\u8ba1\u5212\u53ef\u4ee5\u6307\u5bfc\u4f4e\u7ea7\u63a7\u5236\u7b56\u7565\u7684\u8bad\u7ec3\u4ee5\u8fdb\u884c\u673a\u5668\u4eba\u6267\u884c\u3002\u5728\u4e0d\u540c\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFLIP \u53ef\u4ee5\u63d0\u9ad8\u957f\u89c6\u91ce\u89c6\u9891\u8ba1\u5212\u5408\u6210\u7684\u6210\u529f\u7387\u548c\u8d28\u91cf\uff0c\u5e76\u4e14\u5177\u6709\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u5c5e\u6027\uff0c\u4e3a\u672a\u6765\u7684\u5de5\u4f5c\u5f00\u8f9f\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002", "author": "Chongkai Gao et.al.", "authors": "Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, Lin Shao", "id": "2412.08261v1", "paper_url": "http://arxiv.org/abs/2412.08261v1", "repo": "null"}}