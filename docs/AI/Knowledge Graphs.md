
### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-25**|**Probing Omissions and Distortions in Transformer-based RDF-to-Text Models**|Juliette Faille et.al.|[2409.16707v1](http://arxiv.org/abs/2409.16707v1)|null|
|**2024-09-25**|**GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning**|Zhe-Rui Yang et.al.|[2409.16670v1](http://arxiv.org/abs/2409.16670v1)|null|
|**2024-09-24**|**Cyber Knowledge Completion Using Large Language Models**|Braden K Webb et.al.|[2409.16176v1](http://arxiv.org/abs/2409.16176v1)|null|
|**2024-09-24**|**Konstruktor: A Strong Baseline for Simple Knowledge Graph Question Answering**|Maria Lysyuk et.al.|[2409.15902v1](http://arxiv.org/abs/2409.15902v1)|[link](https://github.com/s-nlp/konstruktor)|
|**2024-09-24**|**Symmetries and Expressive Requirements for Learning General Policies**|Dominik Drexler et.al.|[2409.15892v1](http://arxiv.org/abs/2409.15892v1)|null|
|**2024-09-23**|**GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation**|Brendan Hogan Rappazzo et.al.|[2409.15566v1](http://arxiv.org/abs/2409.15566v1)|null|
|**2024-09-23**|**KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems**|Zixuan Wang et.al.|[2409.14908v1](http://arxiv.org/abs/2409.14908v1)|null|
|**2024-09-23**|**End-to-End Graph Flattening Method for Large Language Models**|Bin Hong et.al.|[2409.14880v1](http://arxiv.org/abs/2409.14880v1)|null|
|**2024-09-22**|**RACOON: An LLM-based Framework for Retrieval-Augmented Column Type Annotation with a Knowledge Graph**|Linxi Wei et.al.|[2409.14556v1](http://arxiv.org/abs/2409.14556v1)|null|
|**2024-09-21**|**Graph Neural Network Framework for Sentiment Analysis Using Syntactic Feature**|Linxiao Wu et.al.|[2409.14000v1](http://arxiv.org/abs/2409.14000v1)|null|
|**2024-09-20**|**ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources**|Shuting Yang et.al.|[2409.13537v1](http://arxiv.org/abs/2409.13537v1)|[link](https://github.com/zaiwen/cropgpt)|
|**2024-09-20**|**LM-assisted keyword biasing with Aho-Corasick algorithm for Transducer-based ASR**|Iuliia Thorbecke et.al.|[2409.13514v1](http://arxiv.org/abs/2409.13514v1)|null|
|**2024-09-20**|**AQA: Adaptive Question Answering in a Society of LLMs via Contextual Multi-Armed Bandit**|Mohanna Hoveyda et.al.|[2409.13447v2](http://arxiv.org/abs/2409.13447v2)|null|
|**2024-09-20**|**GAProtoNet: A Multi-head Graph Attention-based Prototypical Network for Interpretable Text Classification**|Ximing Wen et.al.|[2409.13312v1](http://arxiv.org/abs/2409.13312v1)|null|
|**2024-09-20**|**Leveraging Knowledge Graphs and LLMs to Support and Monitor Legislative Systems**|Andrea Colombo et.al.|[2409.13252v1](http://arxiv.org/abs/2409.13252v1)|null|
|**2024-09-19**|**Knowledge-Based Domain-Oriented Data Augmentation for Enhancing Unsupervised Sentence Embedding**|Peichao Lai et.al.|[2409.12887v1](http://arxiv.org/abs/2409.12887v1)|null|
|**2024-09-19**|**KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning**|Junnan Liu et.al.|[2409.12865v1](http://arxiv.org/abs/2409.12865v1)|null|
|**2024-09-19**|**A New Perspective on ADHD Research: Knowledge Graph Construction with LLMs and Network Based Insights**|Hakan T. Otal et.al.|[2409.12853v1](http://arxiv.org/abs/2409.12853v1)|null|
|**2024-09-19**|**Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data**|Jiaming Zhou et.al.|[2409.12437v1](http://arxiv.org/abs/2409.12437v1)|[link](https://github.com/riddickzhou/llm-graph-synthetic-reasoning)|
|**2024-09-19**|**Textualized Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation**|Chen Liang et.al.|[2409.12411v1](http://arxiv.org/abs/2409.12411v1)|null|
|**2024-09-18**|**GUNet: A Graph Convolutional Network United Diffusion Model for Stable and Diversity Pose Generation**|Shuowen Liang et.al.|[2409.11689v1](http://arxiv.org/abs/2409.11689v1)|[link](https://github.com/liangshuowen/posediffusion)|
|**2024-09-17**|**FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction**|Ziwei Li et.al.|[2409.11509v1](http://arxiv.org/abs/2409.11509v1)|null|
|**2024-09-17**|**Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**|Yukang Lin et.al.|[2409.11147v1](http://arxiv.org/abs/2409.11147v1)|[link](https://github.com/yukang-lin/rger)|
|**2024-09-17**|**Semformer: Transformer Language Models with Semantic Planning**|Yongjing Yin et.al.|[2409.11143v1](http://arxiv.org/abs/2409.11143v1)|null|
|**2024-09-17**|**KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**|Yanbei Jiang et.al.|[2409.10921v1](http://arxiv.org/abs/2409.10921v1)|[link](https://github.com/yanbei-jiang/artwork-interpretation)|
|**2024-09-16**|**A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**|Zhang Zheng et.al.|[2409.10403v1](http://arxiv.org/abs/2409.10403v1)|null|
|**2024-09-16**|**MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation**|Shanshan Wang et.al.|[2409.10294v2](http://arxiv.org/abs/2409.10294v2)|null|
|**2024-09-16**|**LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain**|Le Xiao et.al.|[2409.10077v1](http://arxiv.org/abs/2409.10077v1)|null|
|**2024-09-16**|**On the Diagram of Thought**|Yifan Zhang et.al.|[2409.10038v1](http://arxiv.org/abs/2409.10038v1)|[link](https://github.com/diagram-of-thought/diagram-of-thought)|
|**2024-09-15**|**Entity-Aware Self-Attention and Contextualized GCN for Enhanced Relation Extraction in Long Sentences**|Xin Wang et.al.|[2409.13755v1](http://arxiv.org/abs/2409.13755v1)|null|
|**2024-09-14**|**Generating Event-oriented Attribution for Movies via Two-Stage Prefix-Enhanced Multimodal LLM**|Yuanjie Lyu et.al.|[2409.09362v1](http://arxiv.org/abs/2409.09362v1)|null|
|**2024-09-14**|**ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models**|Yahan Tu et.al.|[2409.09318v1](http://arxiv.org/abs/2409.09318v1)|null|
|**2024-09-13**|**Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks**|Florian Gr√∂tschla et.al.|[2409.09026v1](http://arxiv.org/abs/2409.09026v1)|null|
|**2024-09-13**|**Contri(e)ve: Context + Retrieve for Scholarly Question Answering**|Kanchan Shivashankar et.al.|[2409.09010v1](http://arxiv.org/abs/2409.09010v1)|null|
|**2024-09-13**|**SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity**|Qitian Wu et.al.|[2409.09007v1](http://arxiv.org/abs/2409.09007v1)|[link](https://github.com/qitianwu/sgformer)|
|**2024-09-13**|**Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies**|Zhiqiang Zhong et.al.|[2409.08864v1](http://arxiv.org/abs/2409.08864v1)|null|
|**2024-09-13**|**A RAG Approach for Generating Competency Questions in Ontology Engineering**|Xueli Pan et.al.|[2409.08820v1](http://arxiv.org/abs/2409.08820v1)|null|
|**2024-09-13**|**ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model**|Zezheng Qin et.al.|[2409.08543v1](http://arxiv.org/abs/2409.08543v1)|null|
|**2024-09-12**|**What Makes a Maze Look Like a Maze?**|Joy Hsu et.al.|[2409.08202v1](http://arxiv.org/abs/2409.08202v1)|null|
|**2024-09-12**|**Towards a graph-based foundation model for network traffic analysis**|Louis Van Langendonck et.al.|[2409.08111v1](http://arxiv.org/abs/2409.08111v1)|null|
|**2024-09-12**|**Learning Rules from KGs Guided by Language Models**|Zihang Peng et.al.|[2409.07869v1](http://arxiv.org/abs/2409.07869v1)|[link](https://github.com/pzh97/learning-rules-from-kgs-guided-by-language-models)|
|**2024-09-12**|**Multi-object event graph representation learning for Video Question Answering**|Yanan Wang et.al.|[2409.07747v1](http://arxiv.org/abs/2409.07747v1)|null|
|**2024-09-11**|**Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**|Khiem Ton et.al.|[2409.07368v3](http://arxiv.org/abs/2409.07368v3)|null|
|**2024-09-11**|**Semantic Interoperability on Blockchain by Generating Smart Contracts Based on Knowledge Graphs**|William Van Woensel et.al.|[2409.12171v1](http://arxiv.org/abs/2409.12171v1)|null|
|**2024-09-11**|**Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model**|Daehee Kim et.al.|[2409.07088v1](http://arxiv.org/abs/2409.07088v1)|[link](https://github.com/daehuikim/WikiOFGraph)|
|**2024-09-11**|**Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence**|Jiun-Ting Li et.al.|[2409.07064v1](http://arxiv.org/abs/2409.07064v1)|null|
|**2024-09-11**|**FreeRide: Harvesting Bubbles in Pipeline Parallelism**|Jiashu Zhang et.al.|[2409.06941v1](http://arxiv.org/abs/2409.06941v1)|null|
|**2024-09-10**|**Generative Hierarchical Materials Search**|Sherry Yang et.al.|[2409.06762v1](http://arxiv.org/abs/2409.06762v1)|null|
|**2024-09-10**|**Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization**|Gollam Rabby et.al.|[2409.06433v1](http://arxiv.org/abs/2409.06433v1)|null|
|**2024-09-10**|**TopoChat: Enhancing Topological Materials Retrieval With Large Language Model and Multi-Source Knowledge**|HuangChao Xu et.al.|[2409.13732v1](http://arxiv.org/abs/2409.13732v1)|null|
|**2024-09-10**|**KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation**|Lei Liang et.al.|[2409.13731v3](http://arxiv.org/abs/2409.13731v3)|null|
|**2024-09-09**|**Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity**|Dongyue Li et.al.|[2409.06091v1](http://arxiv.org/abs/2409.06091v1)|[link](https://github.com/virtuosoresearch/scalablemtl)|
|**2024-09-09**|**OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System**|Ningyu Zhang et.al.|[2409.07497v1](http://arxiv.org/abs/2409.07497v1)|[link](https://github.com/zjunlp/oneedit)|
|**2024-09-09**|**SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning**|Alireza Ghafarollahi et.al.|[2409.05556v1](http://arxiv.org/abs/2409.05556v1)|[link](https://github.com/lamm-mit/SciAgentsDiscovery)|
|**2024-09-09**|**Assessing SPARQL capabilities of Large Language Models**|Lars-Peter Meyer et.al.|[2409.05925v1](http://arxiv.org/abs/2409.05925v1)|[link](https://github.com/aksw/llm-kg-bench)|
|**2024-09-09**|**KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**|Yingshu Li et.al.|[2409.05370v1](http://arxiv.org/abs/2409.05370v1)|null|
|**2024-09-07**|**Action is the primary key: a categorical framework for episode description and logical reasoning**|Yoshiki Fukada et.al.|[2409.04793v1](http://arxiv.org/abs/2409.04793v1)|null|
|**2024-09-06**|**Accelerating Training with Neuron Interaction and Nowcasting Networks**|Boris Knyazev et.al.|[2409.04434v1](http://arxiv.org/abs/2409.04434v1)|[link](https://github.com/samsungsailmontreal/nino)|
|**2024-09-06**|**Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**|Desiree Heim et.al.|[2409.04286v1](http://arxiv.org/abs/2409.04286v1)|null|
|**2024-09-06**|**GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**|Ziyin Zhang et.al.|[2409.04183v1](http://arxiv.org/abs/2409.04183v1)|null|
|**2024-09-06**|**Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**|Larissa Pusch et.al.|[2409.04181v1](http://arxiv.org/abs/2409.04181v1)|null|
|**2024-09-06**|**Refining Wikidata Taxonomy using Large Language Models**|Yiwen Peng et.al.|[2409.04056v1](http://arxiv.org/abs/2409.04056v1)|[link](https://github.com/peng-yiwen/WiKC)|
|**2024-09-06**|**Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features**|Miao Fan et.al.|[2409.04009v1](http://arxiv.org/abs/2409.04009v1)|null|
|**2024-09-05**|**Rx Strategist: Prescription Verification using LLM Agents System**|Phuc Phan Van et.al.|[2409.03440v1](http://arxiv.org/abs/2409.03440v1)|null|
|**2024-09-05**|**iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models**|Yassir Lairgi et.al.|[2409.03284v1](http://arxiv.org/abs/2409.03284v1)|[link](https://github.com/AuvaLab/itext2kg)|
|**2024-09-05**|**GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**|Yukun Cao et.al.|[2409.03258v1](http://arxiv.org/abs/2409.03258v1)|null|
|**2024-09-05**|**Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models**|Jie Ma et.al.|[2409.03155v1](http://arxiv.org/abs/2409.03155v1)|[link](https://github.com/reml-group/dog)|
|**2024-09-04**|**Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification**|Junyoung Lee et.al.|[2409.02481v1](http://arxiv.org/abs/2409.02481v1)|null|
|**2024-09-04**|**Multi-modal Situated Reasoning in 3D Scenes**|Xiongkun Linghu et.al.|[2409.02389v1](http://arxiv.org/abs/2409.02389v1)|null|
|**2024-09-02**|**Grounding Language Models in Autonomous Loco-manipulation Tasks**|Jin Wang et.al.|[2409.01326v1](http://arxiv.org/abs/2409.01326v1)|null|
|**2024-09-02**|**LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning**|Haoran Yang et.al.|[2409.01145v1](http://arxiv.org/abs/2409.01145v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph**|Yuxiang Wang et.al.|[2409.00727v1](http://arxiv.org/abs/2409.00727v1)|null|
|**2024-08-31**|**WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction**|Oktie Hassanzadeh et.al.|[2409.00331v1](http://arxiv.org/abs/2409.00331v1)|[link](https://github.com/IBM/wikicausal)|
|**2024-08-29**|**HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications**|Rishi Kalra et.al.|[2409.09046v1](http://arxiv.org/abs/2409.09046v1)|null|
|**2024-08-29**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|Jingyi Wang et.al.|[2408.16224v2](http://arxiv.org/abs/2408.16224v2)|null|
|**2024-08-28**|**LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**|Ruirui Chen et.al.|[2408.15903v1](http://arxiv.org/abs/2408.15903v1)|null|
|**2024-08-27**|**VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**|Shusaku Egami et.al.|[2408.14895v2](http://arxiv.org/abs/2408.14895v2)|[link](https://github.com/aistairc/virtualhome_aist)|
|**2024-08-27**|**XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model**|Yasir Ali Farrukh et.al.|[2408.16021v1](http://arxiv.org/abs/2408.16021v1)|[link](https://github.com/yasir-ali-farrukh/gnn4id)|
|**2024-08-26**|**Process Trace Querying using Knowledge Graphs and Notation3**|William Van Woensel et.al.|[2409.04452v1](http://arxiv.org/abs/2409.04452v1)|null|
|**2024-08-26**|**PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method**|Runtao Ren et.al.|[2409.00092v1](http://arxiv.org/abs/2409.00092v1)|null|
|**2024-08-26**|**DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**|Ziai Zhou et.al.|[2408.14185v1](http://arxiv.org/abs/2408.14185v1)|null|
|**2024-08-26**|**Exploring the Potential of Large Language Models for Heterophilic Graphs**|Yuxia Wu et.al.|[2408.14134v1](http://arxiv.org/abs/2408.14134v1)|null|
|**2024-08-26**|**Towards Graph Prompt Learning: A Survey and Beyond**|Qingqing Long et.al.|[2408.14520v3](http://arxiv.org/abs/2408.14520v3)|null|
|**2024-08-25**|**CodeGraph: Enhancing Graph Reasoning of LLMs with Code**|Qiaolong Cai et.al.|[2408.13863v1](http://arxiv.org/abs/2408.13863v1)|null|
|**2024-08-25**|**LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings**|Duo Wang et.al.|[2408.14512v1](http://arxiv.org/abs/2408.14512v1)|null|
|**2024-08-24**|**Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models**|Sakhinana Sagar Srinivas et.al.|[2408.13661v1](http://arxiv.org/abs/2408.13661v1)|null|
|**2024-08-24**|**GNN: Graph Neural Network and Large Language Model for Data Discovery**|Thomas Hoang et.al.|[2408.13609v2](http://arxiv.org/abs/2408.13609v2)|null|
|**2024-08-24**|**HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation**|Azmine Toushik Wasi et.al.|[2408.13521v1](http://arxiv.org/abs/2408.13521v1)|[link](https://github.com/azminewasi/hrgraph)|
|**2024-08-24**|**Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation**|Yi-Hui Chen et.al.|[2408.13432v1](http://arxiv.org/abs/2408.13432v1)|null|
|**2024-08-23**|**CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers**|Ekaterina Trofimova et.al.|[2408.13366v1](http://arxiv.org/abs/2408.13366v1)|null|
|**2024-08-23**|**Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving**|Sakhinana Sagar Srinivas et.al.|[2408.14494v1](http://arxiv.org/abs/2408.14494v1)|null|
|**2024-08-22**|**A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language**|Ekdeep Singh Lubana et.al.|[2408.12578v2](http://arxiv.org/abs/2408.12578v2)|[link](https://github.com/ekdeepslubana/conceptpercolation)|
|**2024-08-22**|**Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language**|Arief Purnama Muharram et.al.|[2409.00061v1](http://arxiv.org/abs/2409.00061v1)|null|
|**2024-08-22**|**Cell-ontology guided transcriptome foundation model**|Xinyu Yuan et.al.|[2408.12373v1](http://arxiv.org/abs/2408.12373v1)|null|
|**2024-08-22**|**Graph Retrieval Augmented Trustworthiness Reasoning**|Ying Zhu et.al.|[2408.12333v2](http://arxiv.org/abs/2408.12333v2)|[link](https://github.com/EvoNexusX/Graph-Retrieval-Augmented-Trustworthiness-Reasoning)|
|**2024-08-22**|**MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**|Yanzeng Li et.al.|[2408.12236v1](http://arxiv.org/abs/2408.12236v1)|null|
|**2024-08-22**|**Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning**|Junlin He et.al.|[2408.12116v1](http://arxiv.org/abs/2408.12116v1)|null|
|**2024-08-21**|**Enabling Small Models for Zero-Shot Classification through Model Label Learning**|Jia Zhang et.al.|[2408.11449v1](http://arxiv.org/abs/2408.11449v1)|null|

#### Abstracts
##### **Probing Omissions and Distortions in Transformer-based RDF-to-Text Models**
2409.16707v1 by Juliette Faille, Albert Gatt, Claire Gardent

In Natural Language Generation (NLG), important information is sometimes
omitted in the output text. To better understand and analyse how this type of
mistake arises, we focus on RDF-to-Text generation and explore two methods of
probing omissions in the encoder output of BART (Lewis et al, 2020) and of T5
(Raffel et al, 2019): (i) a novel parameter-free probing method based on the
computation of cosine similarity between embeddings of RDF graphs and of RDF
graphs in which we removed some entities and (ii) a parametric probe which
performs binary classification on the encoder embeddings to detect omitted
entities. We also extend our analysis to distorted entities, i.e. entities that
are not fully correctly mentioned in the generated text (e.g. misspelling of
entity, wrong units of measurement). We found that both omitted and distorted
entities can be probed in the encoder's output embeddings. This suggests that
the encoder emits a weaker signal for these entities and therefore is
responsible for some loss of information. This also shows that probing methods
can be used to detect mistakes in the output of NLG models.

ÊëòË¶ÅÔºöÂú®Ëá™ÁÑ∂Ë™ûË®ÄÁîüÊàê (NLG) ‰∏≠ÔºåÈáçË¶ÅË≥áË®äÊúâÊôÇÊúÉÂú®Ëº∏Âá∫ÊñáÂ≠ó‰∏≠Ë¢´ÁúÅÁï•„ÄÇÁÇ∫‰∫ÜÊõ¥‰∫ÜËß£‰∏¶ÂàÜÊûêÈÄôÈ°ûÈåØË™§ÊòØÂ¶Ç‰ΩïÁî¢ÁîüÁöÑÔºåÊàëÂÄëÂ∞àÊ≥®Êñº RDF ËΩâÊñáÂ≠óÁöÑÁîüÊàêÔºå‰∏¶Êé¢Ë®éÂÖ©Á®ÆÊé¢Ê∏¨ BART (Lewis Á≠â‰∫∫Ôºå2020) Âíå T5 (Raffel Á≠â‰∫∫Ôºå2019) ÁöÑÁ∑®Á¢ºÂô®Ëº∏Âá∫‰∏≠ÈÅ∫ÊºèÁöÑÊñπÊ≥ïÔºö(i) ‰∏ÄÁ®ÆÂü∫Êñº RDF ÂúñÂΩ¢ÂµåÂÖ•ÂíåÊàëÂÄëÁßªÈô§‰∏Ä‰∫õÂØ¶È´îÁöÑ RDF ÂúñÂΩ¢‰πãÈñìÁöÑÈ§òÂº¶Áõ∏‰ººÂ∫¶Ë®àÁÆóÁöÑÊñ∞ÂûãÁÑ°ÂèÉÊï∏Êé¢Ê∏¨ÊñπÊ≥ïÔºå‰ª•Âèä (ii) ‰∏ÄÁ®ÆÂú®Á∑®Á¢ºÂô®ÂµåÂÖ•‰∏≠Âü∑Ë°å‰∫åÂÖÉÂàÜÈ°û‰ª•ÂÅµÊ∏¨ÈÅ∫ÊºèÂØ¶È´îÁöÑÂèÉÊï∏ÂåñÊé¢Ê∏¨„ÄÇÊàëÂÄë‰πüÂ∞áÊàëÂÄëÁöÑÂàÜÊûêÂª∂‰º∏Âà∞Êâ≠Êõ≤ÁöÑÂØ¶È´îÔºå‰πüÂ∞±ÊòØÂú®Áî¢ÁîüÁöÑÊñáÂ≠ó‰∏≠Ê≤íÊúâË¢´ÂÆåÂÖ®Ê≠£Á¢∫ÊèêÂèäÁöÑÂØ¶È´î (‰æãÂ¶ÇÂØ¶È´îÊãºÂØ´ÈåØË™§„ÄÅÊ∏¨ÈáèÂñÆ‰ΩçÈåØË™§)„ÄÇÊàëÂÄëÁôºÁèæÈÅ∫ÊºèÂíåÊâ≠Êõ≤ÁöÑÂØ¶È´îÈÉΩÂèØ‰ª•Ë¢´Êé¢Ê∏¨Âà∞Âú®Á∑®Á¢ºÂô®ÁöÑËº∏Âá∫ÂµåÂÖ•‰∏≠„ÄÇÈÄôË°®Á§∫Á∑®Á¢ºÂô®ÈáùÂ∞çÈÄô‰∫õÂØ¶È´îÁôºÂ∞ÑËºÉÂº±ÁöÑË®äËôüÔºåÂõ†Ê≠§Â∞éËá¥‰∏Ä‰∫õË≥áË®äÈÅ∫Â§±„ÄÇÈÄô‰πüÈ°ØÁ§∫Êé¢Ê∏¨ÊñπÊ≥ïÂèØ‰ª•Áî®ÊñºÂÅµÊ∏¨ NLG Ê®°ÂûãËº∏Âá∫‰∏≠ÁöÑÈåØË™§„ÄÇ

##### **GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning**
2409.16670v1 by Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu

Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in
handling a range of graph analytical tasks across various domains, such as
e-commerce and social networks. Despite their versatility, GNNs face
significant challenges in transferability, limiting their utility in real-world
applications. Existing research in GNN transfer learning overlooks
discrepancies in distribution among various graph datasets, facing challenges
when transferring across different distributions. How to effectively adopt a
well-trained GNN to new graphs with varying feature and structural
distributions remains an under-explored problem. Taking inspiration from the
success of Low-Rank Adaptation (LoRA) in adapting large language models to
various domains, we propose GraphLoRA, an effective and parameter-efficient
method for transferring well-trained GNNs to diverse graph domains.
Specifically, we first propose a Structure-aware Maximum Mean Discrepancy
(SMMD) to align divergent node feature distributions across source and target
graphs. Moreover, we introduce low-rank adaptation by injecting a small
trainable GNN alongside the pre-trained one, effectively bridging structural
distribution gaps while mitigating the catastrophic forgetting. Additionally, a
structure-aware regularization objective is proposed to enhance the
adaptability of the pre-trained GNN to target graph with scarce supervision
labels. Extensive experiments on six real-world datasets demonstrate the
effectiveness of GraphLoRA against eleven baselines by tuning only 20% of
parameters, even across disparate graph domains. The code is available at
https://anonymous.4open.science/r/GraphLoRA.

ÊëòË¶ÅÔºöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∑≤Â±ïÁèæÂá∫Âú®ÂêÑÁ®ÆÈ†òÂüüËôïÁêÜ‰∏ÄÁ≥ªÂàóÂúñÂΩ¢ÂàÜÊûê‰ªªÂãôÁöÑÂçìË∂äËÉΩÂäõÔºå‰æãÂ¶ÇÈõªÂ≠êÂïÜÂãôÂíåÁ§æÁæ§Á∂≤Ë∑Ø„ÄÇÂÑòÁÆ° GNN ÂÖ∑ÊúâÂ§öÂäüËÉΩÊÄßÔºå‰ΩÜÂú®ÂèØËΩâÁßªÊÄßÊñπÈù¢‰ªçÈù¢Ëá®ÈáçÂ§ßÊåëÊà∞ÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®ÁèæÂØ¶‰∏ñÁïåÊáâÁî®‰∏≠ÁöÑÊïàÁî®„ÄÇÁèæÊúâÁöÑ GNN ËΩâÁßªÂ≠∏ÁøíÁ†îÁ©∂ÂøΩË¶ñ‰∫ÜÂêÑÁ®ÆÂúñÂΩ¢Ë≥áÊñôÈõÜ‰πãÈñìÁöÑÂàÜÂ∏ÉÂ∑ÆÁï∞ÔºåÂú®Ë∑®‰∏çÂêåÂàÜÂ∏ÉËΩâÁßªÊôÇÈù¢Ëá®ÊåëÊà∞„ÄÇÂ¶Ç‰ΩïÊúâÊïàÂú∞Â∞áË®ìÁ∑¥ËâØÂ•ΩÁöÑ GNN ÊáâÁî®ÊñºÂÖ∑Êúâ‰∏çÂêåÁâπÂæµÂíåÁµêÊßãÂàÜÂ∏ÉÁöÑÊñ∞ÂúñÂΩ¢Ôºå‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÂ∞öÊú™ÂÖÖÂàÜÊé¢Ë®éÁöÑÂïèÈ°å„ÄÇÂæû‰ΩéÁß©ÈÅ©Êáâ (LoRA) Âú®Â∞áÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÈÅ©ÊáâÂà∞ÂêÑÁ®ÆÈ†òÂüüÊñπÈù¢Áç≤ÂæóÁöÑÊàêÂäü‰∏≠Ê±≤ÂèñÈùàÊÑüÔºåÊàëÂÄëÊèêÂá∫‰∫Ü GraphLoRAÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊúâÊïà‰∏îÂèÉÊï∏ÊïàÁéáÈ´òÁöÑÊñπÊ≥ïÔºåÂèØÁî®ÊñºÂ∞áË®ìÁ∑¥ËâØÂ•ΩÁöÑ GNN ËΩâÁßªÂà∞‰∏çÂêåÁöÑÂúñÂΩ¢È†òÂüü„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÊèêÂá∫‰∏ÄÂÄãÁµêÊßãÊÑüÁü•ÊúÄÂ§ßÂπ≥ÂùáÂ∑ÆÁï∞ (SMMD) ‰æÜË™øÊï¥‰æÜÊ∫êÂíåÁõÆÊ®ôÂúñÂΩ¢‰∏≠ÁöÑ‰∏çÂêåÁØÄÈªûÁâπÂæµÂàÜÂ∏É„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄöÈÅéÂú®È†êÂÖàË®ìÁ∑¥ÁöÑ GNN ÊóÅÈÇäÊ≥®ÂÖ•‰∏ÄÂÄãÂ∞èÁöÑÂèØË®ìÁ∑¥ GNN ‰æÜÂºïÂÖ•‰ΩéÁß©ÈÅ©ÊáâÔºåÂæûËÄåÊúâÊïàÂú∞ÂΩåÂêàÁµêÊßãÂàÜÂ∏ÉÂ∑ÆË∑ùÔºåÂêåÊôÇÊ∏õËºïÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇÊ≠§Â§ñÔºåÈÇÑÊèêÂá∫‰∫ÜÁµêÊßãÊÑüÁü•Ê≠£ÂâáÂåñÁõÆÊ®ôÔºå‰ª•Â¢ûÂº∑È†êÂÖàË®ìÁ∑¥ÁöÑ GNN Â∞çÂÖ∑ÊúâÁ®ÄÁñèÁõ£Áù£Ê®ôÁ±§ÁöÑÁõÆÊ®ôÂúñÂΩ¢ÁöÑÈÅ©ÊáâÊÄß„ÄÇÂú®ÂÖ≠ÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂ§ßÈáèÂØ¶È©óË≠âÊòé‰∫Ü GraphLoRA ÁöÑÊúâÊïàÊÄßÔºåÂÆÉÂÉÖË™øÊï¥‰∫Ü 20% ÁöÑÂèÉÊï∏ÔºåÂç≥‰ΩøÂú®‰∏çÂêåÁöÑÂúñÂΩ¢È†òÂüü‰∏≠‰πüËÉΩÂ§†ÂãùÈÅéÂçÅ‰∏ÄÁ®ÆÂü∫Ê∫ñ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://anonymous.4open.science/r/GraphLoRA ÂèñÂæó„ÄÇ

##### **Cyber Knowledge Completion Using Large Language Models**
2409.16176v1 by Braden K Webb, Sumit Purohit, Rounak Meyur

The integration of the Internet of Things (IoT) into Cyber-Physical Systems
(CPSs) has expanded their cyber-attack surface, introducing new and
sophisticated threats with potential to exploit emerging vulnerabilities.
Assessing the risks of CPSs is increasingly difficult due to incomplete and
outdated cybersecurity knowledge. This highlights the urgent need for
better-informed risk assessments and mitigation strategies. While previous
efforts have relied on rule-based natural language processing (NLP) tools to
map vulnerabilities, weaknesses, and attack patterns, recent advancements in
Large Language Models (LLMs) present a unique opportunity to enhance
cyber-attack knowledge completion through improved reasoning, inference, and
summarization capabilities. We apply embedding models to encapsulate
information on attack patterns and adversarial techniques, generating mappings
between them using vector embeddings. Additionally, we propose a
Retrieval-Augmented Generation (RAG)-based approach that leverages pre-trained
models to create structured mappings between different taxonomies of threat
patterns. Further, we use a small hand-labeled dataset to compare the proposed
RAG-based approach to a baseline standard binary classification model. Thus,
the proposed approach provides a comprehensive framework to address the
challenge of cyber-attack knowledge graph completion.

ÊëòË¶ÅÔºöÁâ©ËÅØÁ∂≤ (IoT) ËàáÁ∂≤Ë∑ØÂØ¶È´îÁ≥ªÁµ± (CPS) ÁöÑÊï¥ÂêàÊì¥Â§ß‰∫ÜÂÖ∂Á∂≤Ë∑ØÊîªÊìäÈù¢ÔºåÂºïÂÖ•‰∫ÜÊñ∞ÁöÑÂíåË§áÈõúÁöÑÂ®ÅËÑÖÔºåÂÖ∑ÊúâÂà©Áî®Êñ∞ËààÊºèÊ¥ûÁöÑÊΩõÂäõ„ÄÇÁî±ÊñºÁ∂≤Ë∑ØÂÆâÂÖ®Áü•Ë≠ò‰∏çÂÆåÊï¥‰∏îÈÅéÊôÇÔºåË©ï‰º∞ CPS ÁöÑÈ¢®Èö™ËÆäÂæóË∂ä‰æÜË∂äÂõ∞Èõ£„ÄÇÈÄôÁ™ÅÈ°Ø‰∫ÜËø´ÂàáÈúÄË¶ÅÊõ¥ÂÆåÂñÑÁöÑÈ¢®Èö™Ë©ï‰º∞ÂíåÁ∑©Ëß£Á≠ñÁï•„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÂä™Âäõ‰æùË≥¥ÊñºÂü∫ÊñºË¶èÂâáÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) Â∑•ÂÖ∑‰æÜÁπ™Ë£ΩÊºèÊ¥û„ÄÅÂº±ÈªûÂíåÊîªÊìäÊ®°ÂºèÔºå‰ΩÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁç®ÁâπÁöÑÊ©üÊúÉÔºåÂèØ‰ª•ÈÄèÈÅéÊîπÈÄ≤ÁöÑÊé®ÁêÜ„ÄÅÊé®Ë´ñÂíåÊëòË¶ÅËÉΩÂäõ‰æÜÂ¢ûÂº∑Á∂≤Ë∑ØÊîªÊìäÁü•Ë≠òÁöÑÂÆåÊàêÂ∫¶„ÄÇÊàëÂÄëÊáâÁî®ÂµåÂÖ•Ê®°Âûã‰æÜÂ∞ÅË£ùÊúâÈóúÊîªÊìäÊ®°ÂºèÂíåÂ∞çÊäóÊäÄË°ìÁöÑË≥áË®äÔºå‰ΩøÁî®ÂêëÈáèÂµåÂÖ•Âú®ÂÆÉÂÄë‰πãÈñìÁî¢ÁîüÂ∞çÊáâÈóú‰øÇ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÁöÑÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂà©Áî®È†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÂú®Â®ÅËÑÖÊ®°ÂºèÁöÑ‰∏çÂêåÂàÜÈ°ûÊ≥ï‰πãÈñìÂª∫Á´ãÁµêÊßãÂåñÁöÑÂ∞çÊáâÈóú‰øÇ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰ΩøÁî®‰∏ÄÂÄãÂ∞èÂûãÁöÑÊâãÂãïÊ®ôË®òË≥áÊñôÈõÜ‰æÜÊØîËºÉÊâÄÊèêÂá∫ÁöÑÂü∫Êñº RAG ÁöÑÊñπÊ≥ïËàáÂü∫Á∑öÊ®ôÊ∫ñ‰∫åÂÖÉÂàÜÈ°ûÊ®°Âûã„ÄÇÂõ†Ê≠§ÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊû∂Êßã‰æÜËß£Ê±∫Á∂≤Ë∑ØÊîªÊìäÁü•Ë≠òÂúñÂÆåÊàêÁöÑÊåëÊà∞„ÄÇ

##### **Konstruktor: A Strong Baseline for Simple Knowledge Graph Question Answering**
2409.15902v1 by Maria Lysyuk, Mikhail Salnikov, Pavel Braslavski, Alexander Panchenko

While being one of the most popular question types, simple questions such as
"Who is the author of Cinderella?", are still not completely solved.
Surprisingly, even the most powerful modern Large Language Models are prone to
errors when dealing with such questions, especially when dealing with rare
entities. At the same time, as an answer may be one hop away from the question
entity, one can try to develop a method that uses structured knowledge graphs
(KGs) to answer such questions. In this paper, we introduce Konstruktor - an
efficient and robust approach that breaks down the problem into three steps:
(i) entity extraction and entity linking, (ii) relation prediction, and (iii)
querying the knowledge graph. Our approach integrates language models and
knowledge graphs, exploiting the power of the former and the interpretability
of the latter. We experiment with two named entity recognition and entity
linking methods and several relation detection techniques. We show that for
relation detection, the most challenging step of the workflow, a combination of
relation classification/generation and ranking outperforms other methods. We
report Konstruktor's strong results on four datasets.

ÊëòË¶ÅÔºöÂÑòÁÆ°ÊòØÊúÄÂ∏∏Ë¶ãÁöÑÂïèÈ°åÈ°ûÂûã‰πã‰∏ÄÔºå‰ΩÜË´∏Â¶Ç„ÄåÁÅ∞ÂßëÂ®òÁöÑ‰ΩúËÄÖÊòØË™∞Ôºü„ÄçÈÄôÈ°ûÁ∞°ÂñÆÁöÑÂïèÈ°å‰ªçÊú™ÂÆåÂÖ®Áç≤ÂæóËß£Á≠î„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÂç≥‰ΩøÊòØÊúÄÂº∑Â§ßÁöÑÁèæ‰ª£Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ËôïÁêÜÊ≠§È°ûÂïèÈ°åÊôÇ‰πüÂÆπÊòìÂá∫ÈåØÔºåÁâπÂà•ÊòØÂú®ËôïÁêÜÁΩïË¶ãÂØ¶È´îÊôÇ„ÄÇËàáÊ≠§ÂêåÊôÇÔºåÁî±ÊñºÁ≠îÊ°àÂèØËÉΩË∑ùÈõ¢ÂïèÈ°åÂØ¶È´îÂÉÖ‰∏ÄÊ≠•‰πãÈÅôÔºåÂõ†Ê≠§ÂèØ‰ª•ÂòóË©¶ÈñãÁôº‰∏ÄÁ®Æ‰ΩøÁî®ÁµêÊßãÂåñÁü•Ë≠òÂúñË≠ú (KG) ‰æÜÂõûÁ≠îÊ≠§È°ûÂïèÈ°åÁöÑÊñπÊ≥ï„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π Konstruktor - ‰∏ÄÁ®ÆÈ´òÊïà‰∏îÂº∑Â§ßÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞áÂïèÈ°åÂàÜËß£ÁÇ∫‰∏âÂÄãÊ≠•È©üÔºö(i) ÂØ¶È´îËêÉÂèñÂíåÂØ¶È´îÈÄ£Áµê„ÄÅ(ii) Èóú‰øÇÈ†êÊ∏¨‰ª•Âèä (iii) Êü•Ë©¢Áü•Ë≠òÂúñË≠ú„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊï¥Âêà‰∫ÜË™ûË®ÄÊ®°ÂûãÂíåÁü•Ë≠òÂúñË≠úÔºåÁôºÊèÆ‰∫ÜÂâçËÄÖÁöÑËÉΩÂäõÂíåÂæåËÄÖÁöÑÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÂØ¶È©ó‰∫ÜÂÖ©Á®ÆÂëΩÂêçÂØ¶È´îË≠òÂà•ÂíåÂØ¶È´îÈÄ£ÁµêÊñπÊ≥ï‰ª•ÂèäÂ§öÁ®ÆÈóú‰øÇÂÅµÊ∏¨ÊäÄË°ì„ÄÇÊàëÂÄëË°®ÊòéÔºåÂ∞çÊñºÈóú‰øÇÂÅµÊ∏¨Ôºå‰πüÂ∞±ÊòØÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÊúÄÂÖ∑ÊåëÊà∞ÊÄßÁöÑÊ≠•È©üÔºåÈóú‰øÇÂàÜÈ°û/ÁîüÊàêÂíåÊéíÂêçÁõ∏ÁµêÂêàÁöÑÁµÑÂêàÂÑ™ÊñºÂÖ∂‰ªñÊñπÊ≥ï„ÄÇÊàëÂÄëÂ†±Âëä‰∫Ü Konstruktor Âú®ÂõõÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÂº∑ÂãÅÊàêÊûú„ÄÇ

##### **Symmetries and Expressive Requirements for Learning General Policies**
2409.15892v1 by Dominik Drexler, Simon St√•hlberg, Blai Bonet, Hector Geffner

State symmetries play an important role in planning and generalized planning.
In the first case, state symmetries can be used to reduce the size of the
search; in the second, to reduce the size of the training set. In the case of
general planning, however, it is also critical to distinguish non-symmetric
states, i.e., states that represent non-isomorphic relational structures.
However, while the language of first-order logic distinguishes non-symmetric
states, the languages and architectures used to represent and learn general
policies do not. In particular, recent approaches for learning general policies
use state features derived from description logics or learned via graph neural
networks (GNNs) that are known to be limited by the expressive power of C_2,
first-order logic with two variables and counting. In this work, we address the
problem of detecting symmetries in planning and generalized planning and use
the results to assess the expressive requirements for learning general policies
over various planning domains. For this, we map planning states to plain
graphs, run off-the-shelf algorithms to determine whether two states are
isomorphic with respect to the goal, and run coloring algorithms to determine
if C_2 features computed logically or via GNNs distinguish non-isomorphic
states. Symmetry detection results in more effective learning, while the
failure to detect non-symmetries prevents general policies from being learned
at all in certain domains.

ÊëòË¶ÅÔºöÁãÄÊÖãÂ∞çÁ®±ÊÄßÂú®Ë¶èÂäÉÂíåÂª£Áæ©Ë¶èÂäÉ‰∏≠ÊâÆÊºîËëóÈáçË¶ÅÁöÑËßíËâ≤„ÄÇ
Âú®Á¨¨‰∏ÄÁ®ÆÊÉÖÊ≥Å‰∏≠ÔºåÁãÄÊÖãÂ∞çÁ®±ÊÄßÂèØÁî®ÊñºÁ∏ÆÂ∞èÊêúÂ∞ãÁöÑË¶èÊ®°ÔºõÂú®Á¨¨‰∫åÁ®ÆÊÉÖÊ≥Å‰∏≠ÔºåÂèØÁî®ÊñºÁ∏ÆÂ∞èË®ìÁ∑¥ÈõÜÁöÑË¶èÊ®°„ÄÇÁÑ∂ËÄåÔºåÂú®Âª£Áæ©Ë¶èÂäÉÁöÑÊÉÖÊ≥Å‰∏≠ÔºåÂçÄÂàÜÈùûÂ∞çÁ®±ÁãÄÊÖãÔºàÂç≥Ë°®Á§∫ÈùûÂêåÊßãÈóú‰øÇÁµêÊßãÁöÑÁãÄÊÖãÔºâ‰πüÂæàÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÈõñÁÑ∂‰∏ÄÈöéÈÇèËºØÁöÑË™ûË®ÄÂçÄÂàÜ‰∫ÜÈùûÂ∞çÁ®±ÁãÄÊÖãÔºå‰ΩÜÁî®ÊñºË°®Á§∫ÂíåÂ≠∏Áøí‰∏ÄËà¨Á≠ñÁï•ÁöÑË™ûË®ÄÂíåÊû∂ÊßãÂçªÊ≤íÊúâ„ÄÇÁâπÂà•ÊòØÔºåÊúÄËøëÁî®ÊñºÂ≠∏Áøí‰∏ÄËà¨Á≠ñÁï•ÁöÑÊñπÊ≥ï‰ΩøÁî®ÂæûÊèèËø∞ÈÇèËºØ‰∏≠Ë°çÁîüÁöÑÁãÄÊÖãÁâπÂæµÔºåÊàñÈÄöÈÅéÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â≠∏ÁøíÔºåÂ∑≤Áü•ÈÄô‰∫õÁâπÂæµÂèóÂà∞ÂÖ∑ÊúâÂÖ©ÂÄãËÆäÊï∏ÂíåË®àÊï∏ÁöÑ‰∏ÄÈöéÈÇèËºØ C_2 ÁöÑË°®ÈÅîËÉΩÂäõÈôêÂà∂„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëËß£Ê±∫‰∫ÜÂú®Ë¶èÂäÉÂíåÂª£Áæ©Ë¶èÂäÉ‰∏≠Ê™¢Ê∏¨Â∞çÁ®±ÊÄßÁöÑÂïèÈ°åÔºå‰∏¶‰ΩøÁî®ÁµêÊûúË©ï‰º∞Âú®ÂêÑÁ®ÆË¶èÂäÉÈ†òÂüü‰∏≠Â≠∏Áøí‰∏ÄËà¨Á≠ñÁï•ÁöÑË°®ÈÅîÈúÄÊ±Ç„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂ∞áË¶èÂäÉÁãÄÊÖãÊò†Â∞ÑÂà∞Âπ≥Èù¢ÂúñÂΩ¢ÔºåÂü∑Ë°åÁèæÊàêÁöÑÊºîÁÆóÊ≥ï‰æÜÁ¢∫ÂÆöÂÖ©ÂÄãÁãÄÊÖãÊòØÂê¶Áõ∏Â∞çÊñºÁõÆÊ®ôÂêåÊßãÔºå‰∏¶Âü∑Ë°åËëóËâ≤ÊºîÁÆóÊ≥ï‰æÜÁ¢∫ÂÆöÈÄèÈÅéÈÇèËºØÊàñ GNN Ë®àÁÆóÁöÑ C_2 ÁâπÂæµÊòØÂê¶ÂçÄÂàÜÈùûÂêåÊßãÁãÄÊÖã„ÄÇÂ∞çÁ®±ÊÄßÊ™¢Ê∏¨ÊúÉÂ∏∂‰æÜÊõ¥ÊúâÊïàÁöÑÂ≠∏ÁøíÔºåËÄåÁÑ°Ê≥ïÊ™¢Ê∏¨ÈùûÂ∞çÁ®±ÊÄßÂâáÊúÉÂÆåÂÖ®ÈòªÊ≠¢Âú®Êüê‰∫õÈ†òÂüü‰∏≠Â≠∏Áøí‰∏ÄËà¨Á≠ñÁï•„ÄÇ

##### **GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation**
2409.15566v1 by Brendan Hogan Rappazzo, Yingheng Wang, Aaron Ferber, Carla Gomes

The ability to form, retrieve, and reason about memories in response to
stimuli serves as the cornerstone for general intelligence - shaping entities
capable of learning, adaptation, and intuitive insight. Large Language Models
(LLMs) have proven their ability, given the proper memories or context, to
reason and respond meaningfully to stimuli. However, they are still unable to
optimally encode, store, and retrieve memories - the ability to do this would
unlock their full ability to operate as AI agents, and to specialize to niche
domains. To remedy this, one promising area of research is Retrieval Augmented
Generation (RAG), which aims to augment LLMs by providing them with rich
in-context examples and information. In question-answering (QA) applications,
RAG methods embed the text of interest in chunks, and retrieve the most
relevant chunks for a prompt using text embeddings. Motivated by human memory
encoding and retrieval, we aim to improve over standard RAG methods by
generating and encoding higher-level information and tagging the chunks by
their utility to answer questions. We introduce Graphical Eigen Memories For
Retrieval Augmented Generation (GEM-RAG). GEM-RAG works by tagging each chunk
of text in a given text corpus with LLM generated ``utility'' questions,
connecting chunks in a graph based on the similarity of both their text and
utility questions, and then using the eigendecomposition of the memory graph to
build higher level summary nodes that capture the main themes of the text. We
evaluate GEM-RAG, using both UnifiedQA and GPT-3.5 Turbo as the LLMs, with
SBERT, and OpenAI's text encoders on two standard QA tasks, showing that
GEM-RAG outperforms other state-of-the-art RAG methods on these tasks. We also
discuss the implications of having a robust RAG system and future directions.

ÊëòË¶ÅÔºö<paragraph>Ê†πÊìöÂà∫ÊøÄÂΩ¢Êàê„ÄÅÊ™¢Á¥¢ÂíåÊé®ÁêÜË®òÊÜ∂ÁöÑËÉΩÂäõÊòØÈÄöÁî®Êô∫ÊÖßÁöÑÂü∫Áü≥ÔºåÂ°ëÈÄ†‰∫ÜÂÖ∑ÂÇôÂ≠∏Áøí„ÄÅÈÅ©ÊáâÂíåÁõ¥Ë¶∫Ê¥ûÂØüÂäõÁöÑÂØ¶È´î„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Ë≠âÊòéÂÖ∂ËÉΩÂäõÔºåÂú®ÈÅ©Áï∂ÁöÑË®òÊÜ∂ÊàñËÉåÊôØ‰∏ãÔºåÂ∞çÂà∫ÊøÄÈÄ≤Ë°åÊé®ÁêÜÂíåÊúâÊÑèÁæ©Âú∞ÂõûÊáâ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰ªçÁÑ∂ÁÑ°Ê≥ïÊúÄ‰Ω≥Âú∞Á∑®Á¢º„ÄÅÂÑ≤Â≠òÂíåÊ™¢Á¥¢Ë®òÊÜ∂ÔºåÂü∑Ë°åÊ≠§Êìç‰ΩúÁöÑËÉΩÂäõÂ∞áËß£ÈéñÂÆÉÂÄë‰ΩúÁÇ∫ AI ‰ª£ÁêÜÈÅã‰Ωú‰∏¶Â∞àÈñÄÂåñÁÇ∫Âà©Âü∫È†òÂüüÁöÑÂÖ®ÈÉ®ËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜË£úÊïëÊ≠§ÂïèÈ°åÔºå‰∏ÄÂÄãÊúâÂâçÊôØÁöÑÁ†îÁ©∂È†òÂüüÊòØÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG)ÔºåÂÖ∂ÁõÆÊ®ôÊòØÈÄèÈÅéÊèê‰æõË±êÂØåÁöÑ‰∏ä‰∏ãÊñáÁØÑ‰æãÂíåË≥áË®ä‰æÜÊì¥ÂÖÖ LLM„ÄÇÂú®ÂïèÁ≠î (QA) ÊáâÁî®Á®ãÂºè‰∏≠ÔºåRAG ÊñπÊ≥ïÂ∞áÊÑüËààË∂£ÁöÑÊñáÂ≠óÂàÜÂ°äÂµåÂÖ•Ôºå‰∏¶‰ΩøÁî®ÊñáÂ≠óÂµåÂÖ•ÁÇ∫ÊèêÁ§∫Ê™¢Á¥¢ÊúÄÁõ∏ÈóúÁöÑÂçÄÂ°ä„ÄÇÂèó‰∫∫È°ûË®òÊÜ∂Á∑®Á¢ºÂíåÊ™¢Á¥¢ÁöÑÂïüÁôºÔºåÊàëÂÄëÊó®Âú®ÈÄèÈÅéÁî¢ÁîüÂíåÁ∑®Á¢ºÊõ¥È´òÁ¥öÂà•ÁöÑË≥áË®ä‰∏¶Ê†πÊìöÂçÄÂ°äÂõûÁ≠îÂïèÈ°åÁöÑÊïàÁî®Ê®ôË®òÂçÄÂ°äÔºåÂæûËÄåÊîπÈÄ≤Ê®ôÊ∫ñ RAG ÊñπÊ≥ï„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÁî®ÊñºÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÁöÑÂúñÂΩ¢ÁâπÂæµË®òÊÜ∂ (GEM-RAG)„ÄÇGEM-RAG ÁöÑÂ∑•‰ΩúÂéüÁêÜÊòØ‰ΩøÁî® LLM ÁîüÊàêÁöÑ„ÄåÊïàÁî®„ÄçÂïèÈ°åÊ®ôË®òÁµ¶ÂÆöÊñáÂ≠óË™ûÊñôÂ∫´‰∏≠ÊØèÂÄãÊñáÂ≠óÂçÄÂ°äÔºåÊ†πÊìöÊñáÂ≠óÂíåÊïàÁî®ÂïèÈ°åÁöÑÁõ∏‰ººÊÄßÂ∞áÂçÄÂ°äÈÄ£Êé•Âú®ÂúñÂΩ¢‰∏≠ÔºåÁÑ∂Âæå‰ΩøÁî®Ë®òÊÜ∂ÂúñÂΩ¢ÁöÑÁâπÂæµÂàÜËß£‰æÜÂª∫Á´ãÊì∑ÂèñÊñáÂ≠ó‰∏ªÈ°åÁöÑÈ´òÈöéÊëòË¶ÅÁØÄÈªû„ÄÇÊàëÂÄë‰ΩøÁî® UnifiedQA Âíå GPT-3.5 Turbo ‰ΩúÁÇ∫ LLMÔºå‰ª•Âèä SBERT Âíå OpenAI ÁöÑÊñáÂ≠óÁ∑®Á¢ºÂô®ÔºåÂú®ÂÖ©ÂÄãÊ®ôÊ∫ñ QA ‰ªªÂãô‰∏≠Ë©ï‰º∞ GEM-RAGÔºåÈ°ØÁ§∫ GEM-RAG Âú®ÈÄô‰∫õ‰ªªÂãô‰∏≠ÂÑ™ÊñºÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑ RAG ÊñπÊ≥ï„ÄÇÊàëÂÄëÈÇÑË®éË´ñ‰∫ÜÊìÅÊúâÂº∑Â§ßÁöÑ RAG Á≥ªÁµ±ÁöÑÂê´ÊÑèÂíåÊú™‰æÜÁöÑÊñπÂêë„ÄÇ</paragraph>

##### **KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems**
2409.14908v1 by Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming Gan

Embodied AI agents responsible for executing interconnected, long-sequence
household tasks often face difficulties with in-context memory, leading to
inefficiencies and errors in task execution. To address this issue, we
introduce KARMA, an innovative memory system that integrates long-term and
short-term memory modules, enhancing large language models (LLMs) for planning
in embodied agents through memory-augmented prompting. KARMA distinguishes
between long-term and short-term memory, with long-term memory capturing
comprehensive 3D scene graphs as representations of the environment, while
short-term memory dynamically records changes in objects' positions and states.
This dual-memory structure allows agents to retrieve relevant past scene
experiences, thereby improving the accuracy and efficiency of task planning.
Short-term memory employs strategies for effective and adaptive memory
replacement, ensuring the retention of critical information while discarding
less pertinent data. Compared to state-of-the-art embodied agents enhanced with
memory, our memory-augmented embodied AI agent improves success rates by 1.3x
and 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator,
respectively, and enhances task execution efficiency by 3.4x and 62.7x.
Furthermore, we demonstrate that KARMA's plug-and-play capability allows for
seamless deployment on real-world robotic systems, such as mobile manipulation
platforms.Through this plug-and-play memory system, KARMA significantly
enhances the ability of embodied agents to generate coherent and contextually
appropriate plans, making the execution of complex household tasks more
efficient. The experimental videos from the work can be found at
https://youtu.be/4BT7fnw9ehs.

ÊëòË¶ÅÔºöË≤†Ë≤¨Âü∑Ë°åÁõ∏‰∫íÈÄ£Êé•ÁöÑÈï∑Â∫èÂàóÂÆ∂Â∫≠‰ªªÂãôÁöÑÂÖ∑Ë∫´Âåñ AI ‰ª£ÁêÜÁ∂ìÂ∏∏Èù¢Ëá®ÊÉÖÂ¢ÉË®òÊÜ∂ÁöÑÂõ∞Èõ£ÔºåÂ∞éËá¥‰ªªÂãôÂü∑Ë°åÊïàÁéá‰Ωé‰∏ãÂíåÈåØË™§„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü KARMAÔºåÈÄôÊòØ‰∏ÄÂÄãÂâµÊñ∞ÁöÑË®òÊÜ∂Á≥ªÁµ±ÔºåÂÆÉÊï¥Âêà‰∫ÜÈï∑ÊúüÂíåÁü≠ÊúüË®òÊÜ∂Ê®°ÁµÑÔºåÈÄèÈÅéË®òÊÜ∂Â¢ûÂº∑ÊèêÁ§∫ÔºåÂ¢ûÂº∑ÂÖ∑Ë∫´Âåñ‰ª£ÁêÜ‰∏≠Áî®ÊñºË¶èÂäÉÁöÑÂ§ßË™ûË®ÄÊ®°Âûã (LLM)„ÄÇKARMA ÂçÄÂàÜÈï∑ÊúüÂíåÁü≠ÊúüË®òÊÜ∂ÔºåÈï∑ÊúüË®òÊÜ∂Êì∑ÂèñÂÖ®Èù¢ÁöÑ 3D Â†¥ÊôØÂúñÂΩ¢‰ΩúÁÇ∫Áí∞Â¢ÉÁöÑË°®Á§∫ÔºåËÄåÁü≠ÊúüË®òÊÜ∂ÂâáÂãïÊÖãË®òÈåÑÁâ©‰ª∂‰ΩçÁΩÆÂíåÁãÄÊÖãÁöÑËÆäÂåñ„ÄÇÈÄôÁ®ÆÈõôÈáçË®òÊÜ∂ÁµêÊßãÂÖÅË®±‰ª£ÁêÜÊì∑ÂèñÁõ∏ÈóúÁöÑÈÅéÂéªÂ†¥ÊôØÁ∂ìÈ©óÔºåÂæûËÄåÊèêÈ´ò‰ªªÂãôË¶èÂäÉÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊïàÁéá„ÄÇÁü≠ÊúüË®òÊÜ∂Êé°Áî®Á≠ñÁï•‰æÜÈÄ≤Ë°åÊúâÊïàÂíåÈÅ©ÊáâÊÄßÁöÑË®òÊÜ∂ÊõøÊèõÔºåÁ¢∫‰øù‰øùÁïôÈóúÈçµË≥áË®äÔºåÂêåÊôÇÊç®Ê£ÑËºÉ‰∏çÁõ∏ÈóúÁöÑË≥áÊñô„ÄÇËàáÂÖ∑ÂÇôÂ¢ûÂº∑Ë®òÊÜ∂ÂäüËÉΩÁöÑÊúÄÊñ∞ÂÖ∑Ë∫´Âåñ‰ª£ÁêÜÁõ∏ÊØîÔºåÊàëÂÄëÁöÑË®òÊÜ∂Â¢ûÂº∑ÂÖ∑Ë∫´Âåñ AI ‰ª£ÁêÜÂú® AI2-THOR Ê®°Êì¨Âô®‰∏≠ÁöÑË§áÂêà‰ªªÂãôÂíåË§áÈõú‰ªªÂãô‰∏≠ÔºåÂàÜÂà•Â∞áÊàêÂäüÁéáÊèêÈ´ò‰∫Ü 1.3 ÂÄçÂíå 2.3 ÂÄçÔºå‰∏¶Â∞á‰ªªÂãôÂü∑Ë°åÊïàÁéáÊèêÈ´ò‰∫Ü 3.4 ÂÄçÂíå 62.7 ÂÄç„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË≠âÊòé‰∫Ü KARMA ÁöÑÂç≥ÊèíÂç≥Áî®ÂäüËÉΩÂÖÅË®±Âú®ÁúüÂØ¶‰∏ñÁïåÁöÑÊ©üÂô®‰∫∫Á≥ªÁµ±‰∏äÈÄ≤Ë°åÁÑ°Á∏´ÈÉ®ÁΩ≤Ôºå‰æãÂ¶ÇË°åÂãïÊìç‰ΩúÂπ≥Âè∞„ÄÇÈÄèÈÅéÈÄôÂÄãÂç≥ÊèíÂç≥Áî®Ë®òÊÜ∂Á≥ªÁµ±ÔºåKARMA Â§ßÂπÖÂ¢ûÂº∑‰∫ÜÂÖ∑Ë∫´Âåñ‰ª£ÁêÜÁî¢Áîü‰∏ÄËá¥‰∏îÁ¨¶ÂêàÊÉÖÂ¢ÉÁöÑË®àÁï´ÁöÑËÉΩÂäõÔºå‰ΩøË§áÈõúÂÆ∂Â∫≠‰ªªÂãôÁöÑÂü∑Ë°åÊõ¥ÊúâÊïàÁéá„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁöÑÂØ¶È©óÂΩ±ÁâáÂèØ‰ª•Âú® https://youtu.be/4BT7fnw9ehs ÊâæÂà∞„ÄÇ

##### **End-to-End Graph Flattening Method for Large Language Models**
2409.14880v1 by Bin Hong, Jinze Wu, Jiayu Liu, Liang Ding, Jing Sha, Kai Zhang, Shijin Wang, Zhenya Huang

In recent years, the breakthrough of Large Language Models (LLMs) offers new
ideas for achieving universal methods on graph data. The common practice of
converting graphs into natural language for LLMs, which refers to graph
flattening, exhibits good generalizability and interpretability. However, the
poor organization of the textual format results in poor performance in
long-distance scenario understanding. Inspired by human cognitive reasoning
habits, we propose a novel method for graph flattening to fit LLMs, termed as
End-to-End DAG-Path prompting (EEDP). Experiments on real-world datasets show
that EEDP enhances the reasoning performance of LLMs in long-distance scenarios
while maintaining excellent performance in short-distance scenarios,
demonstrating good robustness in the face of distance variations.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÁ™ÅÁ†¥ÁÇ∫Âú®ÂúñÂΩ¢Ë≥áÊñô‰∏≠ÈÅîÊàêÈÄöÁî®ÊñπÊ≥ïÊèê‰æõ‰∫ÜÊñ∞ÊÉ≥Ê≥ï„ÄÇÂ∞áÂúñÂΩ¢ËΩâÊèõÁÇ∫Ëá™ÁÑ∂Ë™ûË®Ä‰ª•‰æõ LLM ‰ΩøÁî®ÁöÑÂ∏∏Ë¶ãÂÅöÊ≥ïÔºåÂç≥ÂúñÂΩ¢ÊâÅÂπ≥ÂåñÔºåÂ±ïÁèæÂá∫ËâØÂ•ΩÁöÑÈÄöÁî®ÊÄßÂíåÂèØËß£ÈáãÊÄß„ÄÇÁÑ∂ËÄåÔºåÊñáÊú¨Ê†ºÂºèÁµÑÁπî‰∏ç‰Ω≥Â∞éËá¥Âú®Èï∑Ë∑ùÈõ¢Â†¥ÊôØÁêÜËß£‰∏≠Ë°®Áèæ‰∏ç‰Ω≥„ÄÇÂèóÂà∞‰∫∫È°ûË™çÁü•Êé®ÁêÜÁøíÊÖ£ÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ï‰æÜÈÄ≤Ë°åÂúñÂΩ¢ÊâÅÂπ≥Âåñ‰ª•ÈÖçÂêà LLMÔºåÁ®±ÁÇ∫Á´ØÂà∞Á´Ø DAG Ë∑ØÂæëÊèêÁ§∫ (EEDP)„ÄÇÂú®ÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåEEDP Â¢ûÂº∑‰∫Ü LLM Âú®Èï∑Ë∑ùÈõ¢Â†¥ÊôØ‰∏≠ÁöÑÊé®ÁêÜÊÄßËÉΩÔºåÂêåÊôÇÂú®Áü≠Ë∑ùÈõ¢Â†¥ÊôØ‰∏≠‰øùÊåÅ‰∫ÜÂá∫Ëâ≤ÁöÑÊÄßËÉΩÔºåÂú®Èù¢Â∞çË∑ùÈõ¢ËÆäÂåñÊôÇË°®ÁèæÂá∫ËâØÂ•ΩÁöÑÈ≠ØÊ£íÊÄß„ÄÇ

##### **RACOON: An LLM-based Framework for Retrieval-Augmented Column Type Annotation with a Knowledge Graph**
2409.14556v1 by Linxi Wei, Guorui Xiao, Magdalena Balazinska

As an important component of data exploration and integration, Column Type
Annotation (CTA) aims to label columns of a table with one or more semantic
types. With the recent development of Large Language Models (LLMs), researchers
have started to explore the possibility of using LLMs for CTA, leveraging their
strong zero-shot capabilities. In this paper, we build on this promising work
and improve on LLM-based methods for CTA by showing how to use a Knowledge
Graph (KG) to augment the context information provided to the LLM. Our
approach, called RACOON, combines both pre-trained parametric and
non-parametric knowledge during generation to improve LLMs' performance on CTA.
Our experiments show that RACOON achieves up to a 0.21 micro F-1 improvement
compared against vanilla LLM inference.

ÊëòË¶ÅÔºö‰ΩúÁÇ∫Ë≥áÊñôÊé¢ÂãòËàáÊï¥ÂêàÁöÑÈáçË¶ÅÁµÑÊàêÈÉ®ÂàÜÔºåÊ¨Ñ‰ΩçÈ°ûÂûãË®ªËß£ (CTA) ÁöÑÁõÆÊ®ôÊòØ‰ΩøÁî®‰∏ÄÂÄãÊàñÂ§öÂÄãË™ûÊÑèÈ°ûÂûãÊ®ôË®òË°®Ê†ºÊ¨Ñ‰Ωç„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËøëÊúüÁôºÂ±ïÔºåÁ†îÁ©∂‰∫∫Âì°Â∑≤ÈñãÂßãÊé¢Ë®é‰ΩøÁî® LLM ‰æÜÈÄ≤Ë°å CTA ÁöÑÂèØËÉΩÊÄßÔºå‰∏¶Âà©Áî®ÂÖ∂Âº∑Â§ßÁöÑÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂª∫Á´ãÂú®ÈÄôÂÄãÊúâÂâçÊôØÁöÑÁ†îÁ©∂‰∏äÔºå‰∏¶ÈÄèÈÅéÂ±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî®Áü•Ë≠òÂúñË≠ú (KG) ‰æÜÊì¥ÂÖÖÊèê‰æõÁµ¶ LLM ÁöÑËÑàÁµ°Ë≥áË®äÔºåÈÄ≤ËÄåÊîπÂñÑÂü∫Êñº LLM ÁöÑ CTA ÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÁ®±ÁÇ∫ RACOONÔºåÂÆÉÂú®ÁîüÊàêÈÅéÁ®ã‰∏≠ÁµêÂêàÈ†êÂÖàË®ìÁ∑¥ÁöÑÂèÉÊï∏ÂºèÂíåÈùûÂèÉÊï∏ÂºèÁü•Ë≠òÔºå‰ª•ÊîπÂñÑ LLM Âú® CTA ‰∏äÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåËàáÁ¥îÁ≤πÁöÑ LLM Êé®Ë´ñÁõ∏ÊØîÔºåRACOON Âú®ÂæÆÂûã F-1 ‰∏äÁöÑÈÄ≤Ê≠•È´òÈÅî 0.21„ÄÇ

##### **Graph Neural Network Framework for Sentiment Analysis Using Syntactic Feature**
2409.14000v1 by Linxiao Wu, Yuanshuai Luo, Binrong Zhu, Guiran Liu, Rui Wang, Qian Yu

Amidst the swift evolution of social media platforms and e-commerce
ecosystems, the domain of opinion mining has surged as a pivotal area of
exploration within natural language processing. A specialized segment within
this field focuses on extracting nuanced evaluations tied to particular
elements within textual contexts. This research advances a composite framework
that amalgamates the positional cues of topical descriptors. The proposed
system converts syntactic structures into a matrix format, leveraging
convolutions and attention mechanisms within a graph to distill salient
characteristics. Incorporating the positional relevance of descriptors relative
to lexical items enhances the sequential integrity of the input. Trials have
substantiated that this integrated graph-centric scheme markedly elevates the
efficacy of evaluative categorization, showcasing preeminence.

ÊëòË¶ÅÔºöÈö®ËëóÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞ÂíåÈõªÂ≠êÂïÜÂãôÁîüÊÖãÁ≥ªÁµ±ÁöÑÂø´ÈÄüÊºîÈÄ≤ÔºåÊÑèË¶ãÊé¢ÂãòÈ†òÂüüÂ∑≤ÊàêÁÇ∫Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠‰∏ÄÈ†ÖÈóúÈçµÁöÑÊé¢Á¥¢È†òÂüü„ÄÇÊ≠§È†òÂüü‰∏≠ÁöÑÂ∞àÈñÄÂçÄÂ°äËëóÈáçÊñºÂæûÊñáÂ≠óËÑàÁµ°‰∏≠ÁöÑÁâπÂÆöÂÖÉÁ¥†‰∏≠ËêÉÂèñÂá∫Á¥∞ÂæÆÁöÑË©ïÂÉπ„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãË§áÂêàÊû∂ÊßãÔºåÂ∞á‰∏ªÈ°åÊèèËø∞Ë©ûÁöÑ‰ΩçÁΩÆÁ∑öÁ¥¢Âêà‰ΩµÂú®‰∏ÄËµ∑„ÄÇÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±Â∞áÂè•Ê≥ïÁµêÊßãËΩâÊèõÊàêÁü©Èô£Ê†ºÂºèÔºå‰∏¶Âú®ÂúñÂΩ¢‰∏≠ÈÅãÁî®Âç∑Á©çÂíåÊ≥®ÊÑèÂäõÊ©üÂà∂‰æÜËêÉÂèñÂá∫È°ØËëóÁâπÂæµ„ÄÇÁ¥çÂÖ•ÊèèËø∞Ë©ûÁõ∏Â∞çÊñºË©ûÂΩôÈ†ÖÁõÆÁöÑ‰ΩçÁΩÆÁõ∏ÈóúÊÄßÔºåÂèØÂº∑ÂåñËº∏ÂÖ•ÁöÑÈ†ÜÂ∫èÂÆåÊï¥ÊÄß„ÄÇË©¶È©óË≠âÂØ¶ÔºåÈÄôÁ®ÆÊï¥ÂêàÂúñÂΩ¢ÁÇ∫‰∏≠ÂøÉÁöÑÊû∂ÊßãÈ°ØËëóÊèêÂçá‰∫ÜË©ï‰º∞ÂàÜÈ°ûÁöÑÊïàËÉΩÔºåÂ±ïÁèæÂá∫ÂçìË∂äÊÄß„ÄÇ

##### **ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources**
2409.13537v1 by Shuting Yang, Zehui Liu, Wolfgang Mayer

Recent developments in large language models (LLMs) have led to significant
improvements in intelligent dialogue systems'ability to handle complex
inquiries. However, current LLMs still exhibit limitations in specialized
domain knowledge, particularly in technical fields such as agriculture. To
address this problem, we propose ShizishanGPT, an intelligent question
answering system for agriculture based on the Retrieval Augmented Generation
(RAG) framework and agent architecture. ShizishanGPT consists of five key
modules: including a generic GPT-4 based module for answering general
questions; a search engine module that compensates for the problem that the
large language model's own knowledge cannot be updated in a timely manner; an
agricultural knowledge graph module for providing domain facts; a retrieval
module which uses RAG to supplement domain knowledge; and an agricultural agent
module, which invokes specialized models for crop phenotype prediction, gene
expression analysis, and so on. We evaluated the ShizishanGPT using a dataset
containing 100 agricultural questions specially designed for this study. The
experimental results show that the tool significantly outperforms general LLMs
as it provides more accurate and detailed answers due to its modular design and
integration of different domain knowledge sources. Our source code, dataset,
and model weights are publicly available at https://github.com/Zaiwen/CropGPT.

ÊëòË¶ÅÔºöËøë‰æÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÁôºÂ±ïÔºåÂ§ßÂπÖÊèêÂçá‰∫ÜÊô∫ÊÖßÂ∞çË©±Á≥ªÁµ±ËôïÁêÜË§áÈõúË©¢ÂïèÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑ LLM ‰ªçÂ±ïÁèæÂá∫Âú®Â∞àÊ•≠È†òÂüüÁü•Ë≠òÁöÑÈôêÂà∂ÔºåÁâπÂà•ÊòØÂú®Ëæ≤Ê•≠Á≠âÊäÄË°ìÈ†òÂüü„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ ShizishanGPTÔºå‰∏ÄÂÄãÂü∫ÊñºÊ™¢Á¥¢Êì¥ÂÖÖÁîüÊàêÔºàRAGÔºâÊû∂ÊßãÂíå‰ª£ÁêÜÊû∂ÊßãÁöÑËæ≤Ê•≠Êô∫ÊÖßÂïèÁ≠îÁ≥ªÁµ±„ÄÇShizishanGPT ÂåÖÂê´‰∫îÂÄãÈóúÈçµÊ®°ÁµÑÔºöÂåÖÂê´‰∏ÄÂÄãÁî®ÊñºÂõûÁ≠î‰∏ÄËà¨ÂïèÈ°åÁöÑÈÄöÁî® GPT-4 Âü∫Á§éÊ®°ÁµÑÔºõ‰∏ÄÂÄãÊêúÂ∞ãÂºïÊìéÊ®°ÁµÑÔºåÁî®ÊñºÂΩåË£úÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÊú¨Ë∫´ÁöÑÁü•Ë≠òÁÑ°Ê≥ïÂèäÊôÇÊõ¥Êñ∞ÁöÑÂïèÈ°åÔºõ‰∏ÄÂÄãËæ≤Ê•≠Áü•Ë≠òÂúñË≠úÊ®°ÁµÑÔºåÁî®ÊñºÊèê‰æõÈ†òÂüü‰∫ãÂØ¶Ôºõ‰∏ÄÂÄã‰ΩøÁî® RAG ‰æÜË£úÂÖÖÈ†òÂüüÁü•Ë≠òÁöÑÊ™¢Á¥¢Ê®°ÁµÑÔºõ‰ª•Âèä‰∏ÄÂÄãËæ≤Ê•≠‰ª£ÁêÜÊ®°ÁµÑÔºåÁî®ÊñºÂëºÂè´Áî®Êñº‰ΩúÁâ©Ë°®ÂûãÈ†êÊ∏¨„ÄÅÂü∫Âõ†Ë°®ÁèæÂàÜÊûêÁ≠âÁöÑÂ∞àÊ•≠Ê®°Âûã„ÄÇÊàëÂÄë‰ΩøÁî®‰∏ÄÂÄãÁâπÂà•ÁÇ∫ÈÄôÈ†ÖÁ†îÁ©∂Ë®≠Ë®àÁöÑÔºåÂåÖÂê´ 100 ÂÄãËæ≤Ê•≠ÂïèÈ°åÁöÑË≥áÊñôÈõÜ‰æÜË©ï‰º∞ ShizishanGPT„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÁî±ÊñºÂÖ∂Ê®°ÁµÑÂåñË®≠Ë®àÂíåÊï¥Âêà‰∫Ü‰∏çÂêåÁöÑÈ†òÂüüÁü•Ë≠ò‰æÜÊ∫êÔºåÊ≠§Â∑•ÂÖ∑È°ØËëóÂÑ™Êñº‰∏ÄËà¨ÁöÑ LLMÔºåÂõ†ÁÇ∫ÂÆÉÊèê‰æõ‰∫ÜÊõ¥Ê∫ñÁ¢∫‰∏îË©≥Á¥∞ÁöÑÁ≠îÊ°à„ÄÇÊàëÂÄëÁöÑÂéüÂßãÁ¢º„ÄÅË≥áÊñôÈõÜÂíåÊ®°ÂûãÊ¨äÈáçÂ∑≤ÂÖ¨ÈñãÊñº https://github.com/Zaiwen/CropGPT„ÄÇ

##### **LM-assisted keyword biasing with Aho-Corasick algorithm for Transducer-based ASR**
2409.13514v1 by Iuliia Thorbecke, Juan Zuluaga-Gomez, Esa√∫ Villatoro-Tello, Andres Carofilis, Shashi Kumar, Petr Motlicek, Karthik Pandia, Aravind Ganapathiraju

Despite the recent success of end-to-end models for automatic speech
recognition, recognizing special rare and out-of-vocabulary words, as well as
fast domain adaptation with text, are still challenging. It often happens that
biasing to the special entities leads to a degradation in the overall
performance. We propose a light on-the-fly method to improve automatic speech
recognition performance by combining a bias list of named entities with a
word-level n-gram language model with the shallow fusion approach based on the
Aho-Corasick string matching algorithm. The Aho-Corasick algorithm has proved
to be more efficient than other methods and allows fast context adaptation. An
n-gram language model is introduced as a graph with fail and output arcs, where
the arc weights are adapted from the n-gram probabilities. The language model
is used as an additional support to keyword biasing when the language model is
combined with bias entities in a single context graph to take care of the
overall performance. We demonstrate our findings on 4 languages, 2 public and 1
private datasets including performance on named entities and out-of-vocabulary
entities. We achieve up to 21.6% relative improvement in the general word error
rate with no practical difference in the inverse real-time factor.

ÊëòË¶ÅÔºöÂÑòÁÆ°Á´ØÂ∞çÁ´ØÊ®°ÂûãÂú®Ëá™ÂãïË™ûÈü≥Ëæ®Ë≠ò‰∏äÁç≤ÂæóËøëÊúüÊàêÂäüÔºåËæ®Ë≠òÁâπÊÆäÁΩïË¶ã‰∏î‰∏çÂú®Ë©ûÂΩôË°®‰∏≠ÁöÑÂ≠óË©ûÔºå‰ª•ÂèäÈÄèÈÅéÊñáÂ≠óÈÄ≤Ë°åÂø´ÈÄüÈ†òÂüüÈÅ©ÊáâÔºå‰ªçÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂÅèÂêëÁâπÊÆäÂØ¶È´îÁ∂ìÂ∏∏Â∞éËá¥Êï¥È´îÊïàËÉΩÈôç‰Ωé„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂç≥ÊôÇËºïÈáèÊñπÊ≥ïÔºåÈÄèÈÅéÂ∞áÂëΩÂêçÂØ¶È´îÁöÑÂÅèÂ∑ÆÊ∏ÖÂñÆÔºåËàáÂü∫Êñº Aho-Corasick Â≠ó‰∏≤ÈÖçÂ∞çÊºîÁÆóÊ≥ïÁöÑÊ∑∫Â±§ËûçÂêàÊñπÊ≥ïÔºåÁµêÂêàÂ≠óÂÖÉÁ≠âÁ¥ö n-gram Ë™ûË®ÄÊ®°ÂûãÔºå‰æÜÊîπÂñÑËá™ÂãïË™ûÈü≥Ëæ®Ë≠òÊïàËÉΩ„ÄÇAho-Corasick ÊºîÁÆóÊ≥ïÂ∑≤Ë¢´Ë≠âÊòéÊØîÂÖ∂‰ªñÊñπÊ≥ïÊõ¥ÊúâÊïàÁéáÔºå‰∏¶ÂÖÅË®±Âø´ÈÄüËÑàÁµ°ÈÅ©Êáâ„ÄÇn-gram Ë™ûË®ÄÊ®°ÂûãË¢´ÂºïÂÖ•ÁÇ∫ÂÖ∑ÊúâÂ§±ÊïóËàáËº∏Âá∫ÂºßÁ∑öÁöÑÂúñÂΩ¢ÔºåÂÖ∂‰∏≠ÂºßÁ∑öÊ¨äÈáçÂæû n-gram Ê©üÁéáÊîπÁ∑®ËÄå‰æÜ„ÄÇË™ûË®ÄÊ®°ÂûãÁî®‰ΩúÈóúÈçµÂ≠óÂÅèÂ∑ÆÁöÑÈ°çÂ§ñÊîØÊè¥ÔºåÁï∂Ë™ûË®ÄÊ®°ÂûãËàáÂÅèÂ∑ÆÂØ¶È´îÁµêÂêàÂú®ÂñÆ‰∏ÄËÑàÁµ°ÂúñÂΩ¢‰∏≠ÊôÇÔºåÁî®ÊñºÁÖßÈ°ßÊï¥È´îÊïàËÉΩ„ÄÇÊàëÂÄëÂú® 4 Á®ÆË™ûË®Ä„ÄÅ2 ÂÄãÂÖ¨ÈñãÂíå 1 ÂÄãÁßÅ‰∫∫Ë≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫ÊàëÂÄëÁöÑÁôºÁèæÔºåÂåÖÊã¨ÂëΩÂêçÂØ¶È´îÂíå‰∏çÂú®Ë©ûÂΩôË°®‰∏≠ÁöÑÂØ¶È´îÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÂú®‰∏ÄËà¨Â≠óÂÖÉÈåØË™§Áéá‰∏äÁç≤ÂæóÈ´òÈÅî 21.6% ÁöÑÁõ∏Â∞çÊîπÂñÑÔºå‰∏îÂú®ÂèçÂêëÂç≥ÊôÇÂõ†Â≠ê‰∏≠Ê≤íÊúâÂØ¶ÈöõÂ∑ÆÁï∞„ÄÇ

##### **AQA: Adaptive Question Answering in a Society of LLMs via Contextual Multi-Armed Bandit**
2409.13447v2 by Mohanna Hoveyda, Arjen P. de Vries, Maarten de Rijke, Harrie Oosterhuis, Faegheh Hasibi

In question answering (QA), different questions can be effectively addressed
with different answering strategies. Some require a simple lookup, while others
need complex, multi-step reasoning to be answered adequately. This observation
motivates the development of a dynamic method that adaptively selects the most
suitable QA strategy for each question, enabling more efficient and effective
systems capable of addressing a broader range of question types. To this aim,
we build on recent advances in the orchestration of multiple large language
models (LLMs) and formulate adaptive QA as a dynamic orchestration challenge.
We define this as a contextual multi-armed bandit problem, where the context is
defined by the characteristics of the incoming question and the action space
consists of potential communication graph configurations among the LLM agents.
We then train a linear upper confidence bound model to learn an optimal mapping
between different question types and their corresponding optimal multi-LLM
communication graph representation. Our experiments show that the proposed
solution is viable for adaptive orchestration of a QA system with multiple
modules, as it combines the superior performance of more complex strategies
while avoiding their costs when simpler strategies suffice.

ÊëòË¶ÅÔºöÂú®ÂïèÁ≠î (QA) ‰∏≠Ôºå‰∏çÂêåÁöÑÂïèÈ°åÂèØ‰ª•Áî®‰∏çÂêåÁöÑÂõûÁ≠îÁ≠ñÁï•ÊúâÊïàÂú∞Ëß£Ê±∫„ÄÇÊúâ‰∫õÂïèÈ°åÂè™ÈúÄË¶ÅÁ∞°ÂñÆÁöÑÊü•Ë©¢ÔºåËÄåÂè¶‰∏Ä‰∫õÂïèÈ°åÂâáÈúÄË¶ÅË§áÈõúÁöÑÂ§öÊ≠•È©üÊé®ÁêÜÊâçËÉΩÂæóÂà∞ÂÖÖÂàÜÁöÑÂõûÁ≠î„ÄÇÈÄôÂÄãËßÄÂØü‰øÉ‰ΩøÈñãÁôº‰∏ÄÁ®ÆÂãïÊÖãÊñπÊ≥ïÔºåËÉΩÈÅ©ÊáâÊÄßÂú∞ÁÇ∫ÊØèÂÄãÂïèÈ°åÈÅ∏ÊìáÊúÄÂêàÈÅ©ÁöÑ QA Á≠ñÁï•ÔºåÂæûËÄåÂØ¶ÁèæÊõ¥È´òÊïà‰∏îÊúâÊïàÁöÑÁ≥ªÁµ±ÔºåËÉΩËß£Ê±∫Êõ¥Âª£Ê≥õÁöÑÈ°ûÂûãÂïèÈ°å„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂª∫Á´ãÂú®Â§öÂÄãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á∑®ÊéíÁöÑÊúÄÊñ∞ÈÄ≤Â±ï‰πã‰∏äÔºå‰∏¶Â∞áÈÅ©ÊáâÊÄß QA Âà∂ÂÆöÁÇ∫‰∏ÄÂÄãÂãïÊÖãÁ∑®ÊéíÊåëÊà∞„ÄÇÊàëÂÄëÂ∞áÂÖ∂ÂÆöÁæ©ÁÇ∫‰∏ÄÂÄãÊÉÖÂ¢ÉÂ§öÈáçÈÅ∏ÊìáÂïèÈ°åÔºåÂÖ∂‰∏≠ÊÉÖÂ¢ÉÁî±Ëº∏ÂÖ•ÂïèÈ°åÁöÑÁâπÂæµÂÆöÁæ©ÔºåËÄåÂãï‰ΩúÁ©∫ÈñìÁî± LLM ‰ª£ÁêÜ‰πãÈñìÁöÑÊΩõÂú®ÈÄö‰ø°ÂúñÂΩ¢ÈÖçÁΩÆÁµÑÊàê„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË®ìÁ∑¥‰∏ÄÂÄãÁ∑öÊÄß‰∏äÈôêÁΩÆ‰ø°ÁïåÊ®°ÂûãÔºå‰ª•Â≠∏Áøí‰∏çÂêåÂïèÈ°åÈ°ûÂûãÂèäÂÖ∂Â∞çÊáâÁöÑÊúÄ‰Ω≥Â§ö LLM ÈÄö‰ø°ÂúñÂΩ¢Ë°®Á§∫‰πãÈñìÁöÑÊúÄ‰Ω≥Êò†Â∞Ñ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑËß£Ê±∫ÊñπÊ°àÈÅ©Áî®ÊñºÂÖ∑ÊúâÂ§öÂÄãÊ®°ÁµÑÁöÑ QA Á≥ªÁµ±ÁöÑÈÅ©ÊáâÊÄßÁ∑®ÊéíÔºåÂõ†ÁÇ∫ÂÆÉÁµêÂêà‰∫ÜÊõ¥Ë§áÈõúÁ≠ñÁï•ÁöÑÂÑ™Ë∂äÊïàËÉΩÔºåÂêåÊôÇÂú®ËºÉÁ∞°ÂñÆÁöÑÁ≠ñÁï•Ë∂≥Â§†ÊôÇÈÅøÂÖç‰∫ÜÂÖ∂ÊàêÊú¨„ÄÇ

##### **GAProtoNet: A Multi-head Graph Attention-based Prototypical Network for Interpretable Text Classification**
2409.13312v1 by Ximing Wen, Wenjuan Tan, Rosina O. Weber

Pretrained transformer-based Language Models (LMs) are well-known for their
ability to achieve significant improvement on text classification tasks with
their powerful word embeddings, but their black-box nature, which leads to a
lack of interpretability, has been a major concern. In this work, we introduce
GAProtoNet, a novel white-box Multi-head Graph Attention-based Prototypical
Network designed to explain the decisions of text classification models built
with LM encoders. In our approach, the input vector and prototypes are regarded
as nodes within a graph, and we utilize multi-head graph attention to
selectively construct edges between the input node and prototype nodes to learn
an interpretable prototypical representation. During inference, the model makes
decisions based on a linear combination of activated prototypes weighted by the
attention score assigned for each prototype, allowing its choices to be
transparently explained by the attention weights and the prototypes projected
into the closest matching training examples. Experiments on multiple public
datasets show our approach achieves superior results without sacrificing the
accuracy of the original black-box LMs. We also compare with four alternative
prototypical network variations and our approach achieves the best accuracy and
F1 among all. Our case study and visualization of prototype clusters also
demonstrate the efficiency in explaining the decisions of black-box models
built with LMs.

ÊëòË¶ÅÔºöÈ†êË®ìÁ∑¥ÁöÑ Transformer Âü∫ÊñºË™ûË®ÄÊ®°Âûã (LM) ‰ª•ÂÖ∂Âº∑Â§ßÁöÑË©ûÂµåÂÖ•ÂäüËÉΩËÄåËÅûÂêçÔºåËÉΩÂ§†Âú®ÊñáÊú¨ÂàÜÈ°û‰ªªÂãô‰∏≠Áç≤ÂæóÈ°ØËëóÁöÑÊîπÈÄ≤Ôºå‰ΩÜÂÖ∂ÈªëÁõíÊÄßË≥™Â∞éËá¥Áº∫‰πèÂèØËß£ÈáãÊÄßÔºå‰∏ÄÁõ¥ÊòØ‰∏ÄÂÄã‰∏ªË¶ÅÂïèÈ°å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü GAProtoNetÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÁôΩÁõíÂ§öÈ†≠ÂúñÊ≥®ÊÑèÂäõÂéüÂûãÁ∂≤Ë∑ØÔºåÊó®Âú®Ëß£Èáã‰ΩøÁî® LM Á∑®Á¢ºÂô®Âª∫Á´ãÁöÑÊñáÊú¨ÂàÜÈ°ûÊ®°ÂûãÁöÑÊ±∫Á≠ñ„ÄÇÂú®ÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏≠ÔºåËº∏ÂÖ•ÂêëÈáèÂíåÂéüÂûãË¢´Ë¶ñÁÇ∫ÂúñÂΩ¢‰∏≠ÁöÑÁØÄÈªûÔºåÊàëÂÄëÂà©Áî®Â§öÈ†≠ÂúñÊ≥®ÊÑèÂäõÂú®Ëº∏ÂÖ•ÁØÄÈªûÂíåÂéüÂûãÁØÄÈªû‰πãÈñìÊúâÈÅ∏ÊìáÂú∞ÊßãÈÄ†ÈÇäÁ∑£Ôºå‰ª•Â≠∏ÁøíÂèØËß£ÈáãÁöÑÂéüÂûãË°®Á§∫„ÄÇÂú®Êé®ÁêÜÈÅéÁ®ã‰∏≠ÔºåÊ®°ÂûãÊ†πÊìöÊøÄÊ¥ªÂéüÂûãÁöÑÁ∑öÊÄßÁµÑÂêàÂÅöÂá∫Ê±∫Á≠ñÔºåË©≤ÁµÑÂêàÁî±ÂàÜÈÖçÁµ¶ÊØèÂÄãÂéüÂûãÁöÑÊ≥®ÊÑèÂäõÂàÜÊï∏Âä†Ê¨äÔºåÂæûËÄåÂÖÅË®±ÈÄöÈÅéÊ≥®ÊÑèÂäõÊ¨äÈáçÂíåÊäïÂΩ±Âà∞ÊúÄÊé•ËøëÂåπÈÖçË®ìÁ∑¥ÁØÑ‰æãÁöÑÂéüÂûã‰æÜÈÄèÊòéÂú∞Ëß£ÈáãÂÖ∂ÈÅ∏Êìá„ÄÇÂú®Â§öÂÄãÂÖ¨ÂÖ±Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®‰∏çÁäßÁâ≤ÂéüÂßãÈªëÁõí LM ÁöÑÊ∫ñÁ¢∫ÊÄßÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂèñÂæó‰∫ÜÂÑ™Áï∞ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÈÇÑËàáÂõõÁ®ÆÊõø‰ª£ÂéüÂûãÁ∂≤Ë∑ØËÆäÈ´îÈÄ≤Ë°å‰∫ÜÊØîËºÉÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®ÊâÄÊúâËÆäÈ´î‰∏≠ÂèñÂæó‰∫ÜÊúÄ‰Ω≥ÁöÑÊ∫ñÁ¢∫ÊÄßÂíå F1„ÄÇÊàëÂÄëÁöÑÊ°à‰æãÁ†îÁ©∂ÂíåÂéüÂûãÁæ§ÈõÜÁöÑÂèØË¶ñÂåñ‰πüË≠âÊòé‰∫Ü‰ΩøÁî® LM Âª∫Á´ãÁöÑÈªëÁõíÊ®°ÂûãÊ±∫Á≠ñÁöÑËß£ÈáãÊïàÁéá„ÄÇ

##### **Leveraging Knowledge Graphs and LLMs to Support and Monitor Legislative Systems**
2409.13252v1 by Andrea Colombo

Knowledge Graphs (KGs) have been used to organize large datasets into
structured, interconnected information, enhancing data analytics across various
fields. In the legislative context, one potential natural application of KGs is
modeling the intricate set of interconnections that link laws and their
articles with each other and the broader legislative context.
  At the same time, the rise of large language models (LLMs) such as GPT has
opened new opportunities in legal applications, such as text generation and
document drafting. Despite their potential, the use of LLMs in legislative
contexts is critical since it requires the absence of hallucinations and
reliance on up-to-date information, as new laws are published on a daily basis.
  This work investigates how Legislative Knowledge Graphs and LLMs can
synergize and support legislative processes. We address three key questions:
the benefits of using KGs for legislative systems, how LLM can support
legislative activities by ensuring an accurate output, and how we can allow
non-technical users to use such technologies in their activities. To this aim,
we develop Legis AI Platform, an interactive platform focused on Italian
legislation that enhances the possibility of conducting legislative analysis
and that aims to support lawmaking activities.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) Â∑≤Ë¢´Áî®ÊñºÂ∞áÂ§ßÂûãË≥áÊñôÈõÜÊï¥ÁêÜÊàêÁµêÊßãÂåñ„ÄÅÁõ∏‰∫íÈóúËÅØÁöÑË≥áË®äÔºå‰ª•Â¢ûÂº∑ÂêÑÁ®ÆÈ†òÂüüÁöÑË≥áÊñôÂàÜÊûê„ÄÇÂú®Á´ãÊ≥ïËÉåÊôØ‰∏ãÔºåKG ÁöÑ‰∏ÄÂÄãÊΩõÂú®Ëá™ÁÑ∂ÊáâÁî®ÊòØÂª∫Ê®°ÈÄ£ÁµêÊ≥ïÂæãÂèäÂÖ∂Ê¢ùÊñáÂΩºÊ≠§‰πãÈñì‰ª•ÂèäÊõ¥Âª£Ê≥õÁ´ãÊ≥ïËÉåÊôØÁöÑË§áÈõúÁõ∏‰∫íÈÄ£ÁµêÈõÜ„ÄÇ
ËàáÊ≠§ÂêåÊôÇÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºà‰æãÂ¶Ç GPTÔºâÁöÑËààËµ∑ÁÇ∫Ê≥ïÂæãÊáâÁî®ÈñãÈó¢‰∫ÜÊñ∞ÁöÑÊ©üÊúÉÔºå‰æãÂ¶ÇÊñáÂ≠óÁî¢ÁîüÂíåÊñá‰ª∂Ëµ∑Ëçâ„ÄÇÂÑòÁÆ°ÊúâÂÖ∂ÊΩõÂäõÔºåÂú®Á´ãÊ≥ïËÉåÊôØ‰∏ã‰ΩøÁî® LLM Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÈúÄË¶ÅÊ≤íÊúâÂπªË¶∫‰∏¶‰∏î‰æùË≥¥ÊñºÊúÄÊñ∞ÁöÑË≥áË®äÔºåÂõ†ÁÇ∫ÊØèÂ§©ÈÉΩÊúÉÂÖ¨‰ΩàÊñ∞ÁöÑÊ≥ïÂæã„ÄÇ
ÈÄôÈ†ÖÂ∑•‰ΩúÊé¢Ë®é‰∫ÜÁ´ãÊ≥ïÁü•Ë≠òÂúñË≠úÂíå LLM Â¶Ç‰ΩïÁî¢ÁîüÂçîÂêåÊïàÊáâ‰∏¶ÊîØÊè¥Á´ãÊ≥ïÁ®ãÂ∫è„ÄÇÊàëÂÄëÊé¢Ë®é‰∫Ü‰∏âÂÄãÈóúÈçµÂïèÈ°åÔºö‰ΩøÁî® KG Â∞çÁ´ãÊ≥ïÁ≥ªÁµ±ÁöÑÂ•ΩËôï„ÄÅLLM Â¶Ç‰ΩïÈÄèÈÅéÁ¢∫‰øùÊ∫ñÁ¢∫ÁöÑËº∏Âá∫ÊîØÊè¥Á´ãÊ≥ïÊ¥ªÂãïÔºå‰ª•ÂèäÊàëÂÄëÂ¶Ç‰ΩïËÆìÈùûÊäÄË°ì‰ΩøÁî®ËÄÖÂú®‰ªñÂÄëÁöÑÊ¥ªÂãï‰∏≠‰ΩøÁî®ÈÄô‰∫õÊäÄË°ì„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÈñãÁôº‰∫Ü Legis AI PlatformÔºåÈÄôÊòØ‰∏ÄÂÄãÂ∞àÊ≥®ÊñºÁæ©Â§ßÂà©Á´ãÊ≥ïÁöÑ‰∫íÂãïÂºèÂπ≥Âè∞ÔºåÂèØÂ¢ûÂº∑ÈÄ≤Ë°åÁ´ãÊ≥ïÂàÜÊûêÁöÑÂèØËÉΩÊÄßÔºå‰∏¶Êó®Âú®ÊîØÊè¥Á´ãÊ≥ïÊ¥ªÂãï„ÄÇ

##### **Knowledge-Based Domain-Oriented Data Augmentation for Enhancing Unsupervised Sentence Embedding**
2409.12887v1 by Peichao Lai, Zhengfeng Zhang, Bin Cui

Recently, unsupervised sentence embedding models have received significant
attention in downstream natural language processing tasks. Using large language
models (LLMs) for data augmentation has led to considerable improvements in
previous studies. Nevertheless, these strategies emphasize data augmentation
with extensive generic corpora, neglecting the consideration of few-shot domain
data. The synthesized data lacks fine-grained information and may introduce
negative sample noise. This study introduces a novel pipeline-based data
augmentation method that leverages LLM to synthesize the domain-specific
dataset. It produces both positive and negative samples through entity- and
quantity-aware augmentation, utilizing an entity knowledge graph to synthesize
samples with fine-grained semantic distinctions, increasing training sample
diversity and relevance. We then present a Gaussian-decayed gradient-assisted
Contrastive Sentence Embedding (GCSE) model to reduce synthetic data noise and
improve model discrimination to reduce negative sample noise. Experimental
results demonstrate that our approach achieves state-of-the-art semantic
textual similarity performance with fewer synthetic data samples and lesser LLM
parameters, demonstrating its efficiency and robustness in varied backbones.

ÊëòË¶ÅÔºöËøëÊù•ÔºåÊó†ÁõëÁù£Âè•Â≠êÂµåÂÖ•Ê®°ÂûãÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑ‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÂèóÂà∞ÂπøÊ≥õÂÖ≥Ê≥®„ÄÇÂú®ÂÖàÂâçÁ†îÁ©∂‰∏≠Ôºå‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËøõË°åÊï∞ÊçÆÊâ©ÂÖÖÂ∑≤Â∏¶Êù•ÊòæËëóÁöÑÊîπËøõ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÁ≠ñÁï•Âº∫Ë∞É‰ΩøÁî®ÂπøÊ≥õÁöÑÈÄöÁî®ËØ≠ÊñôÂ∫ìËøõË°åÊï∞ÊçÆÊâ©ÂÖÖÔºåÂøΩÁï•‰∫ÜÂØπÂ∞ëÈáèÈ¢ÜÂüüÊï∞ÊçÆÁöÑËÄÉÈáè„ÄÇÂêàÊàêÁöÑËµÑÊñôÁº∫‰πèÁªÜÁ≤íÂ∫¶‰ø°ÊÅØÔºåÂπ∂ÂèØËÉΩÂºïÂÖ•Ë¥üÈù¢Ê†∑Êú¨Âô™Â£∞„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÁÆ°ÈÅìÁöÑÊï∞ÊçÆÊâ©ÂÖÖÊñπÊ≥ïÔºåËØ•ÊñπÊ≥ïÂà©Áî® LLM Êù•ÂêàÊàêÁâπÂÆö‰∫éÈ¢ÜÂüüÁöÑËµÑÊñôÈõÜ„ÄÇÂÆÉÈÄöËøáÂÆû‰ΩìÂíåÊï∞ÈáèÊÑüÁü•Êâ©ÂÖÖ‰∫ßÁîüÊ≠£Èù¢ÂíåË¥üÈù¢Ê†∑Êú¨ÔºåÂà©Áî®ÂÆû‰ΩìÁü•ËØÜÂõæË∞±Êù•ÂêàÊàêÂÖ∑ÊúâÁªÜÁ≤íÂ∫¶ËØ≠‰πâÂå∫Âà´ÁöÑÊ†∑Êú¨ÔºåÂ¢ûÂä†ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÂ§öÊ†∑ÊÄßÂíåÁõ∏ÂÖ≥ÊÄß„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™È´òÊñØË°∞ÂáèÊ¢ØÂ∫¶ËæÖÂä©ÂØπÊØîÂè•Â≠êÂµåÂÖ•ÔºàGCSEÔºâÊ®°ÂûãÔºå‰ª•ÂáèÂ∞ëÂêàÊàêÊï∞ÊçÆÂô™Â£∞ÔºåÂπ∂ÊèêÈ´òÊ®°ÂûãÂà§Âà´ËÉΩÂäõÔºå‰ª•ÂáèÂ∞ëË¥üÈù¢Ê†∑Êú¨Âô™Â£∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ª•Êõ¥Â∞ëÁöÑÂêàÊàêÊï∞ÊçÆÊ†∑Êú¨ÂíåÊõ¥Â∞ëÁöÑ LLM ÂèÇÊï∞ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑËØ≠‰πâÊñáÊú¨Áõ∏‰ººÊÄßÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÂêÑÁßçÈ™®Âπ≤ÁΩë‰∏≠ÁöÑÊïàÁéáÂíåÈ≤ÅÊ£íÊÄß„ÄÇ

##### **KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning**
2409.12865v1 by Junnan Liu, Qianren Mao, Weifeng Jiang, Jianxin Li

Knowledge graph reasoning plays a vital role in various applications and has
garnered considerable attention. Recently, path-based methods have achieved
impressive performance. However, they may face limitations stemming from
constraints in message-passing neural networks, such as missing paths and
information over-squashing. In this paper, we revisit the application of
transformers for knowledge graph reasoning to address the constraints faced by
path-based methods and propose a novel method KnowFormer.KnowFormer utilizes a
transformer architecture to perform reasoning on knowledge graphs from the
message-passing perspective, rather than reasoning by textual information like
previous pretrained language model based methods. Specifically, we define the
attention computation based on the query prototype of knowledge graph
reasoning, facilitating convenient construction and efficient optimization. To
incorporate structural information into the self-attention mechanism, we
introduce structure-aware modules to calculate query, key, and value
respectively. Additionally, we present an efficient attention computation
method for better scalability. Experimental results demonstrate the superior
performance of KnowFormer compared to prominent baseline methods on both
transductive and inductive benchmarks.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠úÊé®ÁêÜÂú®ÂêÑÁ®ÆÊáâÁî®‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤Ôºå‰∏¶Â∑≤Áç≤ÂæóÁõ∏Áï∂Â§ßÁöÑÈóúÊ≥®„ÄÇÊúÄËøëÔºåÂü∫ÊñºË∑ØÂæëÁöÑÊñπÊ≥ïÂ∑≤ÂèñÂæó‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂèØËÉΩÊúÉÈù¢Ëá®Ê∫êËá™Ë®äÊÅØÂÇ≥ÈÅûÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÈôêÂà∂Ôºå‰æãÂ¶ÇË∑ØÂæëÈÅ∫Â§±ÂíåË≥áË®äÈÅéÂ∫¶Â£ìÁ∏Æ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈáçÊñ∞Êé¢Ë®éTransformerÂú®Áü•Ë≠òÂúñË≠úÊé®ÁêÜ‰∏≠ÁöÑÊáâÁî®Ôºå‰ª•Ëß£Ê±∫Âü∫ÊñºË∑ØÂæëÁöÑÊñπÊ≥ïÊâÄÈù¢Ëá®ÁöÑÈôêÂà∂Ôºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÊñπÊ≥ï KnowFormer„ÄÇKnowFormer Âà©Áî®TransformerÊû∂ÊßãÂæûË®äÊÅØÂÇ≥ÈÅûÁöÑËßíÂ∫¶Â∞çÁü•Ë≠òÂúñË≠úÂü∑Ë°åÊé®ÁêÜÔºåËÄå‰∏çÊòØÂÉè‰πãÂâçÁöÑÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÊâÄ‰æùË≥¥ÁöÑÊñáÊú¨Ë≥áË®äÈÇ£Ê®£ÈÄ≤Ë°åÊé®ÁêÜ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊ†πÊìöÁü•Ë≠òÂúñË≠úÊé®ÁêÜÁöÑÊü•Ë©¢ÂéüÂûãÂÆöÁæ©Ê≥®ÊÑèÂäõÁöÑÈÅãÁÆóÔºå‰øÉÈÄ≤‰æøÂà©ÁöÑÂª∫ÊßãÂíåÊúâÊïàÁöÑÊúÄ‰Ω≥Âåñ„ÄÇÁÇ∫‰∫ÜÂ∞áÁµêÊßãË≥áË®äÁ¥çÂÖ•Ëá™Ê≥®ÊÑèÂäõÊ©üÂà∂ÔºåÊàëÂÄëÂºïÂÖ•ÁµêÊßãÊÑüÁü•Ê®°ÁµÑ‰æÜÂàÜÂà•Ë®àÁÆóÊü•Ë©¢„ÄÅÈçµÂíåÂÄº„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊúâÊïàÁéáÁöÑÊ≥®ÊÑèÂäõÈÅãÁÆóÊñπÊ≥ï‰ª•Áç≤ÂæóÊõ¥Â•ΩÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåKnowFormer Âú®ËΩâÂ∞éÂíåÊ≠∏Á¥çÂü∫Ê∫ñ‰∏äÈÉΩÂÑ™ÊñºÂÇëÂá∫ÁöÑÂü∫Á∑öÊñπÊ≥ï„ÄÇ

##### **A New Perspective on ADHD Research: Knowledge Graph Construction with LLMs and Network Based Insights**
2409.12853v1 by Hakan T. Otal, Stephen V. Faraone, M. Abdullah Canbaz

Attention-Deficit/Hyperactivity Disorder (ADHD) is a challenging disorder to
study due to its complex symptomatology and diverse contributing factors. To
explore how we can gain deeper insights on this topic, we performed a network
analysis on a comprehensive knowledge graph (KG) of ADHD, constructed by
integrating scientific literature and clinical data with the help of
cutting-edge large language models. The analysis, including k-core techniques,
identified critical nodes and relationships that are central to understanding
the disorder. Building on these findings, we developed a context-aware chatbot
using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG),
enabling accurate and informed interactions. Our knowledge graph not only
advances the understanding of ADHD but also provides a powerful tool for
research and clinical applications.

ÊëòË¶ÅÔºöÊ≥®ÊÑèÂäõÁº∫Èô∑ÈÅéÂãïÁóá (ADHD) ÊòØ‰∏ÄÁ®ÆÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÁñæÁóÖÔºåÁî±ÊñºÂÖ∂Ë§áÈõúÁöÑÁóáÁãÄÂíåÂ§öÊ®£ÂåñÁöÑÊàêÂõ†„ÄÇÁÇ∫‰∫ÜÊé¢Ë®éÂ¶Ç‰ΩïÊ∑±ÂÖ•‰∫ÜËß£ÈÄôÂÄã‰∏ªÈ°åÔºåÊàëÂÄëÂ∞ç‰∏ÄÂÄãÁî±ÁßëÂ≠∏ÊñáÁçªÂíåËá®Â∫äÊï∏ÊìöÊï¥ÂêàËÄåÊàêÁöÑ ADHD ÂÖ®Èù¢Áü•Ë≠òÂúñË≠ú (KG) ÈÄ≤Ë°å‰∫ÜÁ∂≤Ë∑ØÂàÜÊûêÔºå‰∏¶ÂÄüÂä©Â∞ñÁ´ØÁöÑË™ûË®ÄÊ®°Âûã„ÄÇÂàÜÊûêÔºåÂåÖÊã¨ k-core ÊäÄË°ìÔºåË≠òÂà•Âá∫Â∞çÊñºÁêÜËß£Ê≠§ÁñæÁóÖËá≥ÈóúÈáçË¶ÅÁöÑÈóúÈçµÁØÄÈªûÂíåÈóú‰øÇ„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄë‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÈñãÁôº‰∫Ü‰∏ÄÂÄãÂÖ∑ÂÇôÊÉÖÂ¢ÉÊÑüÁü•ËÉΩÂäõÁöÑËÅäÂ§©Ê©üÂô®‰∫∫Ôºå‰ª•ÂØ¶ÁèæÊ∫ñÁ¢∫‰∏îÊúâÊ†πÊìöÁöÑ‰∫íÂãï„ÄÇÊàëÂÄëÁöÑÁü•Ë≠òÂúñË≠ú‰∏çÂÉÖ‰øÉÈÄ≤‰∫ÜÂ∞ç ADHD ÁöÑÁêÜËß£ÔºåÈÇÑÁÇ∫Á†îÁ©∂ÂíåËá®Â∫äÊáâÁî®Êèê‰æõ‰∫ÜÂº∑Â§ßÁöÑÂ∑•ÂÖ∑„ÄÇ

##### **Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data**
2409.12437v1 by Jiaming Zhou, Abbas Ghaddar, Ge Zhang, Liheng Ma, Yaochen Hu, Soumyasundar Pal, Mark Coates, Bin Wang, Yingxue Zhang, Jianye Hao

Despite recent advances in training and prompting strategies for Large
Language Models (LLMs), these models continue to face challenges with complex
logical reasoning tasks that involve long reasoning chains. In this work, we
explore the potential and limitations of using graph-based synthetic reasoning
data as training signals to enhance LLMs' reasoning capabilities. Our extensive
experiments, conducted on two established natural language reasoning tasks --
inductive reasoning and spatial reasoning -- demonstrate that supervised
fine-tuning (SFT) with synthetic graph-based reasoning data effectively
enhances LLMs' reasoning performance without compromising their effectiveness
on other standard evaluation benchmarks.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑË®ìÁ∑¥ÂíåÊèêÁ§∫Á≠ñÁï•ÊñπÈù¢ÊúâËøëÊúüÁöÑÈÄ≤Â±ïÔºåÈÄô‰∫õÊ®°ÂûãÂú®Ê∂âÂèäÈï∑Êé®ÁêÜÈèàÁöÑË§áÈõúÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏≠‰ªçÈù¢Ëá®ÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü‰ΩøÁî®Âü∫ÊñºÂúñÂΩ¢ÁöÑÂêàÊàêÊé®ÁêÜÊï∏Êìö‰ΩúÁÇ∫Ë®ìÁ∑¥Ë®äËôü‰ª•Â¢ûÂº∑ LLM Êé®ÁêÜËÉΩÂäõÁöÑÊΩõÂäõÂíåÈôêÂà∂„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÊó¢ÂÆöÁöÑËá™ÁÑ∂Ë™ûË®ÄÊé®ÁêÜ‰ªªÂãôÔºàÊ≠∏Á¥çÊé®ÁêÜÂíåÁ©∫ÈñìÊé®ÁêÜÔºâ‰∏äÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºåË≠âÊòé‰∫Ü‰ΩøÁî®Âü∫ÊñºÂúñÂΩ¢ÁöÑÂêàÊàêÊé®ÁêÜÊï∏ÊìöÈÄ≤Ë°åÁõ£Áù£ÂæÆË™ø (SFT) ÊúâÊïàÂú∞Â¢ûÂº∑‰∫Ü LLM ÁöÑÊé®ÁêÜÊÄßËÉΩÔºåËÄå‰∏çÊúÉÊêçÂÆ≥ÂÖ∂Âú®ÂÖ∂‰ªñÊ®ôÊ∫ñË©ï‰º∞Âü∫Ê∫ñ‰∏äÁöÑÊïàËÉΩ„ÄÇ

##### **Textualized Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation**
2409.12411v1 by Chen Liang, Zhifan Feng, Zihe Liu, Wenbin Jiang, Jinan Xu, Yufeng Chen, Yong Wang

Chain-of-thought prompting significantly boosts the reasoning ability of
large language models but still faces three issues: hallucination problem,
restricted interpretability, and uncontrollable generation. To address these
challenges, we present AgentCOT, a llm-based autonomous agent framework, which
can solve complex problems in an agent-style manner by multiple round LLM
generation. At each step, AgentCOT selects an action and executes it to yield
an intermediate result with supporting evidence. In addition, we integrate the
step's index into the reasoning process to form a graph structure for complex
inference logic. We introduce two new strategies to enhance the performance of
AgentCOT.We conduct extensive experiments to verify the effectiveness of our
method on six common benchmarks. Results exhibit that our method brings in
substantial improvements over current competitive approaches.

ÊëòË¶ÅÔºöÈèàÊ¢ùÊÄùËÄÉÊèêÁ§∫È°ØËëóÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜ‰ªçÈù¢Ëá®‰∏âÂÄãÂïèÈ°åÔºöÂπªË¶∫ÂïèÈ°å„ÄÅÂèóÈôêÁöÑÂèØËß£ÈáãÊÄßÔºå‰ª•ÂèäÁÑ°Ê≥ïÊéßÂà∂ÁöÑÁîüÊàê„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü AgentCOTÔºå‰∏ÄÂÄãÂü∫Êñº llm ÁöÑËá™‰∏ª‰ª£ÁêÜÊû∂ÊßãÔºåÂÆÉÂèØ‰ª•ÈÄöÈÅéÂ§öËº™ LLM ÁîüÊàê‰ª•‰ª£ÁêÜÊ®£ÂºèËß£Ê±∫Ë§áÈõúÂïèÈ°å„ÄÇÂú®ÊØè‰∏ÄÊ≠•‰∏≠ÔºåAgentCOT ÈÅ∏Êìá‰∏ÄÂÄãÂãï‰Ωú‰∏¶Âü∑Ë°åÂÆÉÔºå‰ª•Áî¢Áîü‰∏ÄÂÄãÂÖ∑ÊúâÊîØÊåÅË≠âÊìöÁöÑ‰∏≠ÈñìÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞áÊ≠•È©üÁ¥¢ÂºïÊï¥ÂêàÂà∞Êé®ÁêÜÈÅéÁ®ã‰∏≠Ôºå‰ª•ÂΩ¢Êàê‰∏ÄÂÄãÂúñÂΩ¢ÁµêÊßãÔºåÁî®ÊñºË§áÈõúÁöÑÊé®ÁêÜÈÇèËºØ„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÂÖ©Á®ÆÊñ∞Á≠ñÁï•‰æÜÂ¢ûÂº∑ AgentCOT ÁöÑÊÄßËÉΩ„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•È©óË≠âÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÂÖ≠ÂÄãÂ∏∏Ë¶ãÂü∫Ê∫ñ‰∏äÁöÑÊúâÊïàÊÄß„ÄÇÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÁöÑÊñπÊ≥ïÊØîÁï∂ÂâçÁöÑÁ´∂Áà≠ÊñπÊ≥ïÊúâ‰∫ÜÈ°ØËëóÁöÑÊîπÈÄ≤„ÄÇ

##### **GUNet: A Graph Convolutional Network United Diffusion Model for Stable and Diversity Pose Generation**
2409.11689v1 by Shuowen Liang, Sisi Li, Qingyun Wang, Cen Zhang, Kaiquan Zhu, Tian Yang

Pose skeleton images are an important reference in pose-controllable image
generation. In order to enrich the source of skeleton images, recent works have
investigated the generation of pose skeletons based on natural language. These
methods are based on GANs. However, it remains challenging to perform diverse,
structurally correct and aesthetically pleasing human pose skeleton generation
with various textual inputs. To address this problem, we propose a framework
with GUNet as the main model, PoseDiffusion. It is the first generative
framework based on a diffusion model and also contains a series of variants
fine-tuned based on a stable diffusion model. PoseDiffusion demonstrates
several desired properties that outperform existing methods. 1) Correct
Skeletons. GUNet, a denoising model of PoseDiffusion, is designed to
incorporate graphical convolutional neural networks. It is able to learn the
spatial relationships of the human skeleton by introducing skeletal information
during the training process. 2) Diversity. We decouple the key points of the
skeleton and characterise them separately, and use cross-attention to introduce
textual conditions. Experimental results show that PoseDiffusion outperforms
existing SoTA algorithms in terms of stability and diversity of text-driven
pose skeleton generation. Qualitative analyses further demonstrate its
superiority for controllable generation in Stable Diffusion.

ÊëòË¶ÅÔºöÂßøÂã¢È™®Êû∂ÂúñÂÉèÊòØÂßøÂã¢ÂèØÊéßÂúñÂÉèÁîüÊàê‰∏≠ÈáçË¶ÅÁöÑÂèÉËÄÉ„ÄÇÁÇ∫‰∫ÜË±êÂØåÈ™®Êû∂ÂúñÂÉèÁöÑ‰æÜÊ∫êÔºåÊúÄËøëÁöÑÁ†îÁ©∂Ë™øÊü•‰∫ÜÂü∫ÊñºËá™ÁÑ∂Ë™ûË®ÄÁöÑÂßøÂã¢È™®Êû∂ÁîüÊàê„ÄÇÈÄô‰∫õÊñπÊ≥ïÂü∫Êñº GAN„ÄÇÁÑ∂ËÄåÔºåË¶ÅÂü∑Ë°åÂ§öÊ®£Âåñ„ÄÅÁµêÊßãÊ≠£Á¢∫‰∏îÁæéËßÄÁöÑ‰∫∫È´îÂßøÂã¢È™®Êû∂ÁîüÊàêÔºå‰∏¶ÂÖ∑ÊúâÂêÑÁ®ÆÊñáÊú¨Ëº∏ÂÖ•Ôºå‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰ª• GUNet ÁÇ∫‰∏ªË¶ÅÊ®°ÂûãÁöÑÊ°ÜÊû∂ÔºåPoseDiffusion„ÄÇÂÆÉÊòØÂü∫ÊñºÊì¥Êï£Ê®°ÂûãÁöÑÁ¨¨‰∏ÄÂÄãÁîüÊàêÊ°ÜÊû∂ÔºåÈÇÑÂåÖÂê´‰∏ÄÁ≥ªÂàóÂü∫ÊñºÁ©©ÂÆöÊì¥Êï£Ê®°ÂûãÈÄ≤Ë°åÂæÆË™øÁöÑËÆäÈ´î„ÄÇPoseDiffusion Â±ïÁ§∫‰∫ÜÂ§öÈ†ÖÂÑ™ÊñºÁèæÊúâÊñπÊ≥ïÁöÑÁêÜÊÉ≥Â±¨ÊÄß„ÄÇ1) Ê≠£Á¢∫ÁöÑÈ™®Êû∂„ÄÇPoseDiffusion ÁöÑÂéªÂô™Ê®°Âûã GUNet Ë¢´Ë®≠Ë®àÁÇ∫ÁµêÂêàÂúñÂΩ¢Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇÂÆÉËÉΩÂ§†ÈÄèÈÅéÂú®Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠ÂºïÂÖ•È™®Êû∂Ë≥áË®ä‰æÜÂ≠∏Áøí‰∫∫È´îÈ™®Êû∂ÁöÑÁ©∫ÈñìÈóú‰øÇ„ÄÇ2) Â§öÊ®£ÊÄß„ÄÇÊàëÂÄëËß£ËÄ¶È™®Êû∂ÁöÑÈóúÈçµÈªû‰∏¶ÂàÜÂà•Â∞çÂÖ∂ÈÄ≤Ë°åË°®ÂæµÔºå‰∏¶‰ΩøÁî®‰∫§ÂèâÊ≥®ÊÑèÂäõ‰æÜÂºïÂÖ•ÊñáÊú¨Ê¢ù‰ª∂„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåPoseDiffusion Âú®ÊñáÊú¨È©ÖÂãïÂßøÂã¢È™®Êû∂ÁîüÊàêÁöÑÁ©©ÂÆöÊÄßÂíåÂ§öÊ®£ÊÄßÊñπÈù¢ÂÑ™ÊñºÁèæÊúâÁöÑ SoTA ÊºîÁÆóÊ≥ï„ÄÇÂÆöÊÄßÂàÜÊûêÈÄ≤‰∏ÄÊ≠•Ë≠âÊòé‰∫ÜÂÆÉÂú® Stable Diffusion ‰∏≠ÂèØÊéßÁîüÊàêÁöÑÂÑ™Ë∂äÊÄß„ÄÇ

##### **FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction**
2409.11509v1 by Ziwei Li, Xiaoqi Wang, Hong-You Chen, Han-Wei Shen, Wei-Lun Chao

Federated learning (FL) has rapidly evolved as a promising paradigm that
enables collaborative model training across distributed participants without
exchanging their local data. Despite its broad applications in fields such as
computer vision, graph learning, and natural language processing, the
development of a data projection model that can be effectively used to
visualize data in the context of FL is crucial yet remains heavily
under-explored. Neighbor embedding (NE) is an essential technique for
visualizing complex high-dimensional data, but collaboratively learning a joint
NE model is difficult. The key challenge lies in the objective function, as
effective visualization algorithms like NE require computing loss functions
among pairs of data. In this paper, we introduce \textsc{FedNE}, a novel
approach that integrates the \textsc{FedAvg} framework with the contrastive NE
technique, without any requirements of shareable data. To address the lack of
inter-client repulsion which is crucial for the alignment in the global
embedding space, we develop a surrogate loss function that each client learns
and shares with each other. Additionally, we propose a data-mixing strategy to
augment the local data, aiming to relax the problems of invisible neighbors and
false neighbors constructed by the local $k$NN graphs. We conduct comprehensive
experiments on both synthetic and real-world datasets. The results demonstrate
that our \textsc{FedNE} can effectively preserve the neighborhood data
structures and enhance the alignment in the global embedding space compared to
several baseline methods.

ÊëòË¶ÅÔºöËÅØÂêàÂºèÂ≠∏Áøí (FL) Â∑≤ËøÖÈÄüÊºîËÆäÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÁØÑ‰æãÔºåÂÆÉÂèØ‰ª•Âú®ÂàÜÂ∏ÉÂºèÂèÉËàáËÄÖ‰πãÈñìÈÄ≤Ë°åÂçî‰ΩúÊ®°ÂûãË®ìÁ∑¥ÔºåËÄåÁÑ°ÈúÄ‰∫§Êèõ‰ªñÂÄëÁöÑÊú¨Âú∞Êï∏Êìö„ÄÇÂÑòÁÆ°ÂÆÉÂú®ÈõªËÖ¶Ë¶ñË¶∫„ÄÅÂúñÂΩ¢Â≠∏ÁøíÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁ≠âÈ†òÂüüÊúâÂª£Ê≥õÁöÑÊáâÁî®Ôºå‰ΩÜÈñãÁôº‰∏ÄÂÄãÊï∏ÊìöÊäïÂΩ±Ê®°ÂûãÔºåÂèØÊúâÊïàÁî®ÊñºÂú® FL ÁöÑËÉåÊôØ‰∏ãË¶ñË¶∫ÂåñÊï∏ÊìöÔºåÈÄô‰∏ÄÈªûËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜ‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÁöÑÊé¢Á¥¢„ÄÇÈÑ∞ÂüüÂµåÂÖ• (NE) ÊòØÁî®ÊñºË¶ñË¶∫ÂåñË§áÈõúÈ´òÁ∂≠Êï∏ÊìöÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨ÊäÄË°ìÔºå‰ΩÜÂçî‰ΩúÂ≠∏Áøí‰∏ÄÂÄãËÅØÂêà NE Ê®°ÂûãÂæàÂõ∞Èõ£„ÄÇ‰∏ªË¶ÅÁöÑÊåëÊà∞Âú®ÊñºÁõÆÊ®ôÂáΩÊï∏ÔºåÂõ†ÁÇ∫ÂÉè NE ÈÄôÊ®£ÁöÑÊúâÊïàË¶ñË¶∫ÂåñÊºîÁÆóÊ≥ïÈúÄË¶ÅË®àÁÆóÊï∏ÊìöÂ∞ç‰πãÈñìÁöÑÊêçÂ§±ÂáΩÊï∏„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü \textsc{FedNE}ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÂÆÉÂ∞á \textsc{FedAvg} Ê°ÜÊû∂ËàáÂ∞çÊØî NE ÊäÄË°ìÁõ∏Êï¥ÂêàÔºåËÄåÁÑ°ÈúÄ‰ªª‰ΩïÂèØÂÖ±‰∫´Êï∏ÊìöÁöÑË¶ÅÊ±Ç„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Â∞çÊñºÂú®ÂÖ®Â±ÄÂµåÂÖ•Á©∫Èñì‰∏≠Â∞çÈΩäËá≥ÈóúÈáçË¶ÅÁöÑÂÆ¢Êà∂Á´ØÈñìÊéíÊñ•Âäõ‰∏çË∂≥ÁöÑÂïèÈ°åÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ª£ÁêÜÊêçÂ§±ÂáΩÊï∏ÔºåÊØèÂÄãÂÆ¢Êà∂Á´ØÂ≠∏ÁøíÊ≠§ÂáΩÊï∏‰∏¶ËàáÂΩºÊ≠§ÂÖ±‰∫´„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊï∏ÊìöÊ∑∑ÂêàÁ≠ñÁï•‰æÜÊì¥ÂÖÖÊú¨Âú∞Êï∏ÊìöÔºåÊó®Âú®Á∑©Ëß£Áî±Êú¨Âú∞ $k$NN ÂúñÂΩ¢ÊßãÈÄ†ÁöÑ‰∏çÂèØË¶ãÈÑ∞ÂüüÂíåÈåØË™§ÈÑ∞ÂüüÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÂú®ÂêàÊàêÂíåÁúüÂØ¶‰∏ñÁïåÊï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶È©ó„ÄÇÁµêÊûúË°®ÊòéÔºåËàáÂπæÁ®ÆÂü∫Á∑öÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑ \textsc{FedNE} ÂèØ‰ª•ÊúâÊïàÂú∞‰øùÁïôÈÑ∞ÂüüÊï∏ÊìöÁµêÊßã‰∏¶Â¢ûÂº∑Âú®ÂÖ®Â±ÄÂµåÂÖ•Á©∫Èñì‰∏≠ÁöÑÂ∞çÈΩä„ÄÇ

##### **Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning**
2409.11147v1 by Yukang Lin, Bingchen Zhong, Shuoran Jiang, Joanna Siebert, Qingcai Chen

Large language models(LLMs) have exhibited remarkable few-shot learning
capabilities and unified the paradigm of NLP tasks through the in-context
learning(ICL) technique. Despite the success of ICL, the quality of the
exemplar demonstrations can significantly influence the LLM's performance.
Existing exemplar selection methods mainly focus on the semantic similarity
between queries and candidate exemplars. On the other hand, the logical
connections between reasoning steps can be beneficial to depict the
problem-solving process as well. In this paper, we proposes a novel method
named Reasoning Graph-enhanced Exemplar Retrieval(RGER). RGER first quires LLM
to generate an initial response, then expresses intermediate problem-solving
steps to a graph structure. After that, it employs graph kernel to select
exemplars with semantic and structural similarity. Extensive experiments
demonstrate the structural relationship is helpful to the alignment of queries
and candidate exemplars. The efficacy of RGER on math and logit reasoning tasks
showcases its superiority over state-of-the-art retrieval-based approaches. Our
code is released at https://github.com/Yukang-Lin/RGER.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ÂçìË∂äÁöÑÂ∞ëÈáèÂ≠∏ÁøíËÉΩÂäõÔºå‰∏¶ÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏Áøí (ICL) ÊäÄË°ìÁµ±‰∏Ä‰∫Ü NLP ‰ªªÂãôÁöÑÁØÑ‰æã„ÄÇÂÑòÁÆ° ICL Â∑≤ÊàêÂäüÔºåÁØÑ‰æãÁ§∫ÁØÑÁöÑÂìÅË≥™ÊúÉÈ°ØËëóÂΩ±Èüø LLM ÁöÑÊïàËÉΩ„ÄÇÁèæÊúâÁöÑÁØÑ‰æãÈÅ∏ÊìáÊñπÊ≥ï‰∏ªË¶ÅËëóÈáçÊñºÊü•Ë©¢ËàáÂÄôÈÅ∏ÁØÑ‰æã‰πãÈñìÁöÑË™ûÊÑèÁõ∏‰ººÊÄß„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÊé®ÁêÜÊ≠•È©ü‰πãÈñìÁöÑÈÇèËºØÈÄ£ÁµêÊúâÂä©ÊñºÊèèÁπ™ÂïèÈ°åËß£Ê±∫ÊµÅÁ®ã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫Êé®ÁêÜÂúñÂ¢ûÂº∑ÁØÑ‰æãÊ™¢Á¥¢ (RGER) ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇRGER È¶ñÂÖàË¶ÅÊ±Ç LLM Áî¢Áîü‰∏ÄÂÄãÂàùÂßãÂõûÊáâÔºåÁÑ∂ÂæåÂ∞á‰∏≠ÈñìÂïèÈ°åËß£Ê±∫Ê≠•È©üË°®Á§∫ÁÇ∫ÂúñÂΩ¢ÁµêÊßã„ÄÇ‰πãÂæåÔºåÂÆÉÊé°Áî®ÂúñÂΩ¢Ê†∏ÈÅ∏ÂèñÂÖ∑ÊúâË™ûÊÑèÂíåÁµêÊßãÁõ∏‰ººÊÄßÁöÑÁØÑ‰æã„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÁµêÊßãÈóú‰øÇÊúâÂä©ÊñºÊü•Ë©¢ÂíåÂÄôÈÅ∏ÁØÑ‰æãÁöÑÂ∞çÈΩä„ÄÇRGER Âú®Êï∏Â≠∏ÂíåÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏äÁöÑÂäüÊïàÂ±ïÁ§∫‰∫ÜÂÆÉÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÊ™¢Á¥¢ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∑≤ÁôºÂ∏ÉÊñº https://github.com/Yukang-Lin/RGER„ÄÇ

##### **Semformer: Transformer Language Models with Semantic Planning**
2409.11143v1 by Yongjing Yin, Junran Ding, Kai Song, Yue Zhang

Next-token prediction serves as the dominant component in current neural
language models. During the training phase, the model employs teacher forcing,
which predicts tokens based on all preceding ground truth tokens. However, this
approach has been found to create shortcuts, utilizing the revealed prefix to
spuriously fit future tokens, potentially compromising the accuracy of the
next-token predictor. In this paper, we introduce Semformer, a novel method of
training a Transformer language model that explicitly models the semantic
planning of response. Specifically, we incorporate a sequence of planning
tokens into the prefix, guiding the planning token representations to predict
the latent semantic representations of the response, which are induced by an
autoencoder. In a minimal planning task (i.e., graph path-finding), our model
exhibits near-perfect performance and effectively mitigates shortcut learning,
a feat that standard training methods and baseline models have been unable to
accomplish. Furthermore, we pretrain Semformer from scratch with 125M
parameters, demonstrating its efficacy through measures of perplexity,
in-context learning, and fine-tuning on summarization tasks.

ÊëòË¶ÅÔºöÂú®Áï∂ÂâçÁöÑË™ûË®ÄÊ®°Âûã‰∏≠Ôºå‰∏ã‰∏ÄÂÄãË©ûÂΩôÈ†êÊ∏¨ÊòØ‰∏ªÂ∞éÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂú®Ë®ìÁ∑¥ÈöéÊÆµÔºåÊ®°ÂûãÊé°Áî®ÊïôÂ∏´Âº∑Âà∂Ê≥ïÔºåÊ†πÊìöÊâÄÊúâÂâç‰∏ÄÂÄãÁöÑÁúüÂØ¶Ë©ûÂΩô‰æÜÈ†êÊ∏¨Ë©ûÂΩô„ÄÇÁÑ∂ËÄåÔºåÁôºÁèæÈÄôÁ®ÆÊñπÊ≥ïÊúÉÁî¢ÁîüÊç∑ÂæëÔºåÂà©Áî®Â∑≤Êè≠Èú≤ÁöÑÂâçÁ∂¥‰æÜËôõÂÅáÂú∞Á¨¶ÂêàÂæåÁ∫åÁöÑË©ûÂΩôÔºåÊΩõÂú®ÊúÉÂç±ÂÆ≥‰∏ã‰∏ÄÂÄãË©ûÂΩôÈ†êÊ∏¨Âô®ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π SemformerÔºå‰∏ÄÁ®ÆË®ìÁ∑¥ Transformer Ë™ûË®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ïÔºåÊòéÁ¢∫Âú∞Âª∫ÊßãÂõûÊáâÁöÑË™ûÊÑèË¶èÂäÉ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞á‰∏ÄÁ≥ªÂàóË¶èÂäÉË©ûÂΩôÁ¥çÂÖ•ÂâçÁ∂¥ÔºåÂºïÂ∞éË¶èÂäÉË©ûÂΩôÁöÑË°®ÂæµÂéªÈ†êÊ∏¨ÂõûÊáâÁöÑÊΩõÂú®Ë™ûÊÑèË°®ÂæµÔºåÈÄô‰∫õË°®ÂæµÊòØÁî±Ëá™ÂãïÁ∑®Á¢ºÂô®Ë™òÂ∞éÁöÑ„ÄÇÂú®‰∏ÄÂÄãÊúÄÂ∞èÁöÑË¶èÂäÉ‰ªªÂãôÔºàÂç≥ÂúñÂΩ¢Ë∑ØÂæëÂ∞ãÊâæÔºâ‰∏≠ÔºåÊàëÂÄëÁöÑÊ®°ÂûãË°®ÁèæÂá∫Êé•ËøëÂÆåÁæéÁöÑÊïàËÉΩÔºå‰∏¶ÊúâÊïàÂú∞Ê∏õËºïÊç∑ÂæëÂ≠∏ÁøíÔºåÈÄôÊòØÊ®ôÊ∫ñË®ìÁ∑¥ÊñπÊ≥ïÂíåÂü∫Á∑öÊ®°ÂûãÁÑ°Ê≥ïÈÅîÊàêÁöÑÂ£ØËàâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæûÈ†≠ÈñãÂßã‰ΩøÁî® 1.25 ÂÑÑÂÄãÂèÉÊï∏È†êË®ìÁ∑¥ SemformerÔºåÈÄèÈÅéÂõ∞ÊÉëÂ∫¶„ÄÅË™ûÂ¢ÉÂ≠∏ÁøíÂíåÂú®ÊëòË¶Å‰ªªÂãô‰∏äÁöÑÂæÆË™ø‰æÜË≠âÊòéÂÖ∂ÂäüÊïà„ÄÇ

##### **KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**
2409.10921v1 by Yanbei Jiang, Krista A. Ehinger, Jey Han Lau

Exploring the narratives conveyed by fine-art paintings is a challenge in
image captioning, where the goal is to generate descriptions that not only
precisely represent the visual content but also offer a in-depth interpretation
of the artwork's meaning. The task is particularly complex for artwork images
due to their diverse interpretations and varied aesthetic principles across
different artistic schools and styles. In response to this, we present KALE
Knowledge-Augmented vision-Language model for artwork Elaborations), a novel
approach that enhances existing vision-language models by integrating artwork
metadata as additional knowledge. KALE incorporates the metadata in two ways:
firstly as direct textual input, and secondly through a multimodal
heterogeneous knowledge graph. To optimize the learning of graph
representations, we introduce a new cross-modal alignment loss that maximizes
the similarity between the image and its corresponding metadata. Experimental
results demonstrate that KALE achieves strong performance (when evaluated with
CIDEr, in particular) over existing state-of-the-art work across several
artwork datasets. Source code of the project is available at
https://github.com/Yanbei-Jiang/Artwork-Interpretation.

ÊëòË¶ÅÔºöÊé¢Á¥¢Áî±ÁæéÊúØÁªòÁîª‰º†ËææÁöÑÂèô‰∫ãÊòØÂõæÂÉèÂ≠óÂπï‰∏≠ÁöÑÊåëÊàòÔºåÂÖ∂ÁõÆÊ†áÊòØÁîüÊàê‰∏ç‰ªÖÂáÜÁ°ÆÂú∞Ë°®Á§∫ËßÜËßâÂÜÖÂÆπËÄå‰∏îËøòÊèê‰æõÂØπËâ∫ÊúØÂìÅÂê´‰πâÁöÑÊ∑±ÂÖ•Ëß£ÈáäÁöÑÊèèËø∞„ÄÇÁî±‰∫éÂÖ∂‰∏çÂêåÁöÑËß£ÈáäÂíåË∑®‰∏çÂêåËâ∫ÊúØÊµÅÊ¥æÂíåÈ£éÊ†ºÁöÑ‰∏çÂêåÁæéÂ≠¶ÂéüÂàôÔºåËøôÈ°π‰ªªÂä°ÂØπ‰∫éËâ∫ÊúØÂìÅÂõæÂÉèÊù•ËØ¥Â∞§ÂÖ∂Â§çÊùÇ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøôÁßçÊÉÖÂÜµÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü KALE Áü•ËØÜÂ¢ûÂº∫ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁî®‰∫éËâ∫ÊúØÂìÅÈòêÈáäÔºå‰∏ÄÁßçÈÄöËøáÂ∞ÜËâ∫ÊúØÂìÅÂÖÉÊï∞ÊçÆ‰Ωú‰∏∫ÈôÑÂä†Áü•ËØÜÊù•Â¢ûÂº∫Áé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊñπÊ≥ï„ÄÇKALE ‰ª•‰∏§ÁßçÊñπÂºèÂêàÂπ∂ÂÖÉÊï∞ÊçÆÔºöÈ¶ñÂÖà‰Ωú‰∏∫Áõ¥Êé•ÊñáÊú¨ËæìÂÖ•ÔºåÂÖ∂Ê¨°ÈÄöËøáÂ§öÊ®°ÊÄÅÂºÇÊûÑÁü•ËØÜÂõæ„ÄÇ‰∏∫‰∫Ü‰ºòÂåñÂõæË°®ÁöÑÂ≠¶‰π†Ë°®Á§∫ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑË∑®Ê®°ÊÄÅÂØπÈΩêÊçüÂ§±ÔºåÂÆÉÊúÄÂ§ßÂåñÂõæÂÉè‰∏éÂÖ∂ÂØπÂ∫îÂÖÉÊï∞ÊçÆ‰πãÈó¥ÁöÑÁõ∏‰ººÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåKALE Âú®‰ΩøÁî® CIDEr ËØÑ‰º∞Êó∂ÔºåÂú®Âá†‰∏™Ëâ∫ÊúØÂìÅÊï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫Ü‰ºòÂºÇÁöÑÊÄßËÉΩÔºàÁâπÂà´ÊòØ‰∏éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÁöÑÂ∑•‰ΩúÁõ∏ÊØîÔºâ„ÄÇËØ•È°πÁõÆÁöÑÊ∫ê‰ª£Á†ÅÂèØÂú® https://github.com/Yanbei-Jiang/Artwork-Interpretation Ëé∑Âæó„ÄÇ

##### **A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**
2409.10403v1 by Zhang Zheng

This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊèêÁ§∫Â≠¶‰π†Ê°ÜÊû∂ÁöÑÁü•ËØÜÂ¢ûÂº∫ÁñæÁóÖËØäÊñ≠ÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ï‰ªé‰∏é‰∏¥Â∫äÁóÖ‰æãÁõ∏ÂÖ≥ÁöÑÂ§ñÈÉ®Áü•ËØÜÂõæË∞±‰∏≠Ê£ÄÁ¥¢ÁªìÊûÑÂåñÁü•ËØÜÔºåÂØπÂÖ∂ËøõË°åÁºñÁ†ÅÔºåÂπ∂Â∞ÜÂÖ∂Ê≥®ÂÖ•Âà∞ÊèêÁ§∫Ê®°Êùø‰∏≠Ôºå‰ª•Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÂØπ‰ªªÂä°ÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨Âú®‰∏â‰∏™ÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºöCHIP-CTC„ÄÅIMCS-V2-NER Âíå KUAKE-QTR„ÄÇÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ËØÑ‰º∞ÊåáÊ†á‰∏äÊòéÊòæ‰ºò‰∫éÁé∞ÊúâÊ®°ÂûãÔºåÂú® CHIP-CTC Êï∞ÊçÆÈõÜ‰∏äÁöÑ F1 ÂæóÂàÜÊèêÈ´ò‰∫Ü 2.4%ÔºåÂú® IMCS-V2-NER Êï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü 3.1%ÔºåÂú® KUAKE-QTR Êï∞ÊçÆÈõÜ‰∏äÊèêÈ´ò‰∫Ü 4.2%„ÄÇÊ≠§Â§ñÔºåÊ∂àËûçÁ†îÁ©∂ËØÅÂÆû‰∫ÜÁü•ËØÜÊ≥®ÂÖ•Ê®°ÂùóÁöÑÂÖ≥ÈîÆ‰ΩúÁî®ÔºåÂõ†‰∏∫ÁßªÈô§Ê≠§Ê®°Âùó‰ºöÂØºËá¥ F1 ÂæóÂàÜÊòæÁùÄ‰∏ãÈôç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï‰∏ç‰ªÖÊúâÊïàÊèêÈ´ò‰∫ÜÁñæÁóÖËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÔºåËÄå‰∏îÂ¢ûÂº∫‰∫ÜÈ¢ÑÊµãÁöÑÂèØËß£ÈáäÊÄßÔºå‰∏∫‰∏¥Â∫äËØäÊñ≠Êèê‰æõ‰∫ÜÊõ¥ÂèØÈù†ÁöÑÊîØÊåÅÂíåËØÅÊçÆ„ÄÇ

##### **MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation**
2409.10294v2 by Shanshan Wang, Chun Zhang, Ning Zhang

The Knowledge Graph-to-Text Generation task aims to convert structured
knowledge graphs into coherent and human-readable natural language text. Recent
efforts in this field have focused on enhancing pre-trained language models
(PLMs) by incorporating graph structure information to capture the intricate
structure details of knowledge graphs. However, most of these approaches tend
to capture only single-granularity structure information, concentrating either
on the relationships between entities within the original graph or on the
relationships between words within the same entity or across different
entities. This narrow focus results in a significant limitation: models that
concentrate solely on entity-level structure fail to capture the nuanced
semantic relationships between words, while those that focus only on word-level
structure overlook the broader relationships between original entire entities.
To overcome these limitations, this paper introduces the Multi-granularity
Graph Structure Attention (MGSA), which is based on PLMs. The encoder of the
model architecture features an entity-level structure encoding module, a
word-level structure encoding module, and an aggregation module that
synthesizes information from both structure. This multi-granularity structure
encoding approach allows the model to simultaneously capture both entity-level
and word-level structure information, providing a more comprehensive
understanding of the knowledge graph's structure information, thereby
significantly improving the quality of the generated text. We conducted
extensive evaluations of the MGSA model using two widely recognized KG-to-Text
Generation benchmark datasets, WebNLG and EventNarrative, where it consistently
outperformed models that rely solely on single-granularity structure
information, demonstrating the effectiveness of our approach.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠úËΩâÊñáÂ≠óÁîüÊàê‰ªªÂãôÊó®Âú®Â∞áÁµêÊßãÂåñÁöÑÁü•Ë≠òÂúñË≠úËΩâÊèõÁÇ∫ÈÄ£Ë≤´‰∏î‰∫∫È°ûÂèØËÆÄÁöÑËá™ÁÑ∂Ë™ûË®ÄÊñáÂ≠ó„ÄÇÊúÄËøëÂú®ÈÄôÂÄãÈ†òÂüüÁöÑÂä™ÂäõÈõÜ‰∏≠Âú®ÈÄèÈÅéÁ¥çÂÖ•ÂúñÂΩ¢ÁµêÊßãË≥áË®ä‰æÜÂ¢ûÂº∑È†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°Âûã (PLM)Ôºå‰ª•Êì∑ÂèñÁü•Ë≠òÂúñË≠úÁöÑË§áÈõúÁµêÊßãÁ¥∞ÁØÄ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ï‰∏≠ÁöÑÂ§ßÂ§öÊï∏ÂÇæÂêëÊñºÂÉÖÊì∑ÂèñÂñÆ‰∏ÄÁ≤íÂ∫¶ÁöÑÁµêÊßãË≥áË®äÔºåÂ∞àÊ≥®ÊñºÂéüÂßãÂúñÂΩ¢‰∏≠ÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇÊàñÂêå‰∏ÄÂÄãÂØ¶È´îÊàñ‰∏çÂêåÂØ¶È´î‰πãÈñìÁöÑË©ûÂΩôÈóú‰øÇ„ÄÇÈÄôÁ®ÆÁãπÈöòÁöÑÁÑ¶ÈªûÂ∞éËá¥‰∫Ü‰∏ÄÂÄãÈáçÂ§ßÁöÑÈôêÂà∂ÔºöÂÉÖÂ∞àÊ≥®ÊñºÂØ¶È´îÂ±§Á¥öÁµêÊßãÁöÑÊ®°ÂûãÁÑ°Ê≥ïÊì∑ÂèñË©ûÂΩô‰πãÈñìÁöÑÁ¥∞ÂæÆË™ûÁæ©Èóú‰øÇÔºåËÄåÂÉÖÂ∞àÊ≥®ÊñºË©ûÂΩôÂ±§Á¥öÁµêÊßãÁöÑÊ®°ÂûãÂâáÂøΩÁï•‰∫ÜÂéüÂßãÊï¥ÂÄãÂØ¶È´î‰πãÈñìÁöÑÊõ¥Âª£Ê≥õÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÊú¨ÊñáÂºïÂÖ•‰∫ÜÂü∫Êñº PLM ÁöÑÂ§öÁ≤íÂ∫¶ÂúñÂΩ¢ÁµêÊßãÊ≥®ÊÑèÂäõ (MGSA)„ÄÇÊ®°ÂûãÊû∂ÊßãÁöÑÁ∑®Á¢ºÂô®ÂÖ∑Êúâ‰∏ÄÂÄãÂØ¶È´îÂ±§Á¥öÁµêÊßãÁ∑®Á¢ºÊ®°ÁµÑ„ÄÅ‰∏ÄÂÄãË©ûÂΩôÂ±§Á¥öÁµêÊßãÁ∑®Á¢ºÊ®°ÁµÑÔºå‰ª•Âèä‰∏ÄÂÄãÂæûÂÖ©ÂÄãÁµêÊßã‰∏≠Á∂úÂêàË≥áË®äÁöÑËÅöÂêàÊ®°ÁµÑ„ÄÇÈÄôÁ®ÆÂ§öÁ≤íÂ∫¶ÁµêÊßãÁ∑®Á¢ºÊñπÊ≥ïÂÖÅË®±Ê®°ÂûãÂêåÊôÇÊì∑ÂèñÂØ¶È´îÂ±§Á¥öÂíåË©ûÂΩôÂ±§Á¥öÁöÑÁµêÊßãË≥áË®äÔºåÊèê‰æõÂ∞çÁü•Ë≠òÂúñË≠úÁµêÊßãË≥áË®äÊõ¥ÂÖ®Èù¢ÁöÑÁêÜËß£ÔºåÂæûËÄåÈ°ØËëóÊèêÂçáÊâÄÁîüÊàêÊñáÂ≠óÁöÑÂìÅË≥™„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãÂª£Ê≥õË™çÂèØÁöÑ KG ËΩâÊñáÂ≠óÁîüÊàêÂü∫Ê∫ñË≥áÊñôÈõÜ WebNLG Âíå EventNarrative Â∞ç MGSA Ê®°ÂûãÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑË©ï‰º∞ÔºåÂÆÉÂßãÁµÇÂÑ™ÊñºÂÉÖ‰æùË≥¥ÂñÆ‰∏ÄÁ≤íÂ∫¶ÁµêÊßãË≥áË®äÁöÑÊ®°ÂûãÔºåË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain**
2409.10077v1 by Le Xiao, Yunfei Xu, Jing Zhao

Domain-specific Named Entity Recognition (NER), whose goal is to recognize
domain-specific entities and their categories, provides an important support
for constructing domain knowledge graphs. Currently, deep learning-based
methods are widely used and effective in NER tasks, but due to the reliance on
large-scale labeled data. As a result, the scarcity of labeled data in a
specific domain will limit its application.Therefore, many researches started
to introduce few-shot methods and achieved some results. However, the entity
structures in specific domains are often complex, and the current few-shot
methods are difficult to adapt to NER tasks with complex features.Taking the
Chinese coal chemical industry domain as an example,there exists a complex
structure of multiple entities sharing a single entity, as well as multiple
relationships for the same pair of entities, which affects the NER task under
the sample less condition.In this paper, we propose a Large Language Models
(LLMs)-based entity recognition framework LLM-DER for the domain-specific
entity recognition problem in Chinese, which enriches the entity information by
generating a list of relationships containing entity types through LLMs, and
designing a plausibility and consistency evaluation method to remove
misrecognized entities, which can effectively solve the complex structural
entity recognition problem in a specific domain.The experimental results of
this paper on the Resume dataset and the self-constructed coal chemical dataset
Coal show that LLM-DER performs outstandingly in domain-specific entity
recognition, not only outperforming the existing GPT-3.5-turbo baseline, but
also exceeding the fully-supervised baseline, verifying its effectiveness in
entity recognition.

ÊëòË¶ÅÔºö<paragraph>È†òÂüüÁâπÂÆöÂëΩÂêçÂØ¶È´îËæ®Ë≠òÔºàNERÔºâÔºåÂÖ∂ÁõÆÊ®ôÊòØËæ®Ë≠òÈ†òÂüüÁâπÂÆöÂØ¶È´îÂèäÂÖ∂È°ûÂà•ÔºåÁÇ∫Âª∫ÊßãÈ†òÂüüÁü•Ë≠òÂúñË≠úÊèê‰æõÈáçË¶ÅÁöÑÊîØÊè¥„ÄÇÁõÆÂâçÔºåÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂª£Ê≥õÁî®Êñº NER ‰ªªÂãô‰∏îÂçÅÂàÜÊúâÊïàÔºå‰ΩÜÁî±Êñº‰æùË≥¥ÊñºÂ§ßË¶èÊ®°Ê®ôË®òË≥áÊñô„ÄÇÂõ†Ê≠§ÔºåÁâπÂÆöÈ†òÂüü‰∏≠Ê®ôË®òË≥áÊñôÁöÑÁ®ÄÂ∞ëÊúÉÈôêÂà∂ÂÖ∂ÊáâÁî®„ÄÇÂõ†Ê≠§ÔºåË®±Â§öÁ†îÁ©∂ÈñãÂßãÂºïÂÖ•Â∞ëÈáèÊ®£Êú¨ÊñπÊ≥ï‰∏¶Áç≤Âæó‰∏Ä‰∫õÊàêÊûú„ÄÇÁÑ∂ËÄåÔºåÁâπÂÆöÈ†òÂüü‰∏≠ÁöÑÂØ¶È´îÁµêÊßãÈÄöÂ∏∏ÂæàË§áÈõúÔºåËÄåÁõÆÂâçÁöÑÂ∞ëÈáèÊ®£Êú¨ÊñπÊ≥ïÈõ£‰ª•ÈÅ©ÊáâÂÖ∑ÊúâË§áÈõúÁâπÂæµÁöÑ NER ‰ªªÂãô„ÄÇ‰ª•‰∏≠ÂúãÁÖ§ÂåñÂ∑•Áî¢Ê•≠È†òÂüüÁÇ∫‰æãÔºåÂ≠òÂú®Â§öÂÄãÂØ¶È´îÂÖ±Áî®ÂñÆ‰∏ÄÂØ¶È´îÁöÑË§áÈõúÁµêÊßãÔºå‰ª•ÂèäÂêå‰∏ÄÂ∞çÂØ¶È´îÊúâÂ§öÈáçÈóú‰øÇÔºåÈÄôÊúÉÂΩ±ÈüøÊ®£Êú¨ËºÉÂ∞ëÊ¢ù‰ª∂‰∏ãÁöÑ NER ‰ªªÂãô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂØ¶È´îËæ®Ë≠òÊû∂Êßã LLM-DERÔºåÁî®Êñº‰∏≠ÊñáÈ†òÂüüÁâπÂÆöÂØ¶È´îËæ®Ë≠òÂïèÈ°åÔºåÈÄöÈÅé LLM ÁîüÊàêÂåÖÂê´ÂØ¶È´îÈ°ûÂûãÁöÑÈóú‰øÇÊ∏ÖÂñÆÔºå‰∏¶Ë®≠Ë®à‰∏ÄÂÄãÂêàÁêÜÊÄßÂíå‰∏ÄËá¥ÊÄßË©ï‰º∞ÊñπÊ≥ï‰æÜÁßªÈô§Ëæ®Ë≠òÈåØË™§ÁöÑÂØ¶È´îÔºåÂæûËÄåÂèØ‰ª•ÊúâÊïàËß£Ê±∫ÁâπÂÆöÈ†òÂüü‰∏≠Ë§áÈõúÁµêÊßãÂØ¶È´îËæ®Ë≠òÂïèÈ°å„ÄÇÊú¨ÊñáÂú® Resume Ë≥áÊñôÈõÜÂíåËá™Âª∫ÁÖ§ÂåñÂ∑•Ë≥áÊñôÈõÜ Coal ‰∏äÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåLLM-DER Âú®È†òÂüüÁâπÂÆöÂØ¶È´îËæ®Ë≠ò‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰∏çÂÉÖÂÑ™ÊñºÁèæÊúâÁöÑ GPT-3.5-turbo Âü∫Ê∫ñÔºåÈÇÑË∂ÖÈÅé‰∫ÜÂÆåÂÖ®Áõ£Áù£ÁöÑÂü∫Á∑öÔºåÈ©óË≠â‰∫ÜÂÖ∂Âú®ÂØ¶È´îËæ®Ë≠ò‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ</paragraph>

##### **On the Diagram of Thought**
2409.10038v1 by Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao

We introduce Diagram of Thought (DoT), a framework that models iterative
reasoning in large language models (LLMs) as the construction of a directed
acyclic graph (DAG) within a single model. Unlike traditional approaches that
represent reasoning as linear chains or trees, DoT organizes propositions,
critiques, refinements, and verifications into a cohesive DAG structure,
allowing the model to explore complex reasoning pathways while maintaining
logical consistency. Each node in the diagram corresponds to a proposition that
has been proposed, critiqued, refined, or verified, enabling the LLM to
iteratively improve its reasoning through natural language feedback. By
leveraging auto-regressive next-token prediction with role-specific tokens, DoT
facilitates seamless transitions between proposing ideas and critically
evaluating them, providing richer feedback than binary signals. Furthermore, we
formalize the DoT framework using Topos Theory, providing a mathematical
foundation that ensures logical consistency and soundness in the reasoning
process. This approach enhances both the training and inference processes
within a single LLM, eliminating the need for multiple models or external
control mechanisms. DoT offers a conceptual framework for designing
next-generation reasoning-specialized models, emphasizing training efficiency,
robust reasoning capabilities, and theoretical grounding. The code is available
at https://github.com/diagram-of-thought/diagram-of-thought.

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π‰∫ÜÊÄùÊÉ≥ÂúñÔºàDoTÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÂÆÉÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÁöÑËø≠‰ª£Êé®ÁêÜÂª∫Ê®°ÁÇ∫Âú®ÂñÆ‰∏ÄÊ®°ÂûãÂÖßÂª∫Êßã‰∏ÄÂÄãÊúâÂêëÁÑ°Áí∞ÂúñÔºàDAGÔºâ„ÄÇËàáÂ∞áÊé®ÁêÜË°®Á§∫ÁÇ∫Á∑öÊÄßÈèàÊàñÊ®πÁöÑÂÇ≥Áµ±ÊñπÊ≥ï‰∏çÂêåÔºåDoT Â∞áÂëΩÈ°å„ÄÅÊâπÂà§„ÄÅ‰øÆÊ≠£ÂíåÈ©óË≠âÁµÑÁπîÊàê‰∏ÄÂÄãÊúâÂáùËÅöÂäõÁöÑ DAG ÁµêÊßãÔºåÂÖÅË®±Ê®°ÂûãÊé¢Á¥¢Ë§áÈõúÁöÑÊé®ÁêÜË∑ØÂæëÔºåÂêåÊôÇ‰øùÊåÅÈÇèËºØ‰∏ÄËá¥ÊÄß„ÄÇÂúñË°®‰∏≠ÁöÑÊØèÂÄãÁØÄÈªûÂ∞çÊáâÊñº‰∏ÄÂÄãÂ∑≤Ë¢´ÊèêÂá∫„ÄÅÊâπÂà§„ÄÅ‰øÆÊ≠£ÊàñÈ©óË≠âÁöÑÂëΩÈ°åÔºå‰Ωø LLM ËÉΩÂ§†ÈÄöÈÅéËá™ÁÑ∂Ë™ûË®ÄÂõûÈ•ãËø≠‰ª£Âú∞ÊîπÈÄ≤ÂÖ∂Êé®ÁêÜ„ÄÇÈÄöÈÅéÂà©Áî®ÂÖ∑ÊúâËßíËâ≤ÁâπÂÆöÊ®ôË®òÁöÑËá™ÂãïÂõûÊ≠∏‰∏ã‰∏ÄÂÄãÊ®ôË®òÈ†êÊ∏¨ÔºåDoT ‰øÉÈÄ≤‰∫ÜÊèêÂá∫ÊÉ≥Ê≥ïÂíåÊâπÂà§ÊÄßË©ï‰º∞ÂÆÉÂÄë‰πãÈñìÁöÑÁÑ°Á∏´ÈÅéÊ∏°ÔºåÊèê‰æõ‰∫ÜÊØî‰∫åÂÖÉ‰ø°ËôüÊõ¥Ë±êÂØåÁöÑÂõûÈ•ã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰ΩøÁî®ÊãìÊí≤ÁêÜË´ñÂΩ¢ÂºèÂåñ‰∫Ü DoT Ê°ÜÊû∂ÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊï∏Â≠∏Âü∫Á§éÔºå‰ª•Á¢∫‰øùÊé®ÁêÜÈÅéÁ®ã‰∏≠ÁöÑÈÇèËºØ‰∏ÄËá¥ÊÄßÂíåÂÅ•ÂÖ®ÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂ¢ûÂº∑‰∫ÜÂñÆ‰∏Ä LLM ÂÖßÁöÑË®ìÁ∑¥ÂíåÊé®ÁêÜÈÅéÁ®ãÔºåÊ∂àÈô§‰∫ÜÂ∞çÂ§öÂÄãÊ®°ÂûãÊàñÂ§ñÈÉ®ÊéßÂà∂Ê©üÂà∂ÁöÑÈúÄË¶Å„ÄÇDoT ÁÇ∫Ë®≠Ë®à‰∏ã‰∏Ä‰ª£Êé®ÁêÜÂ∞àÁî®Ê®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊ¶ÇÂøµÊ°ÜÊû∂ÔºåÂº∑Ë™øË®ìÁ∑¥ÊïàÁéá„ÄÅÂº∑Â§ßÁöÑÊé®ÁêÜËÉΩÂäõÂíåÁêÜË´ñÂü∫Á§é„ÄÇ‰ª£Á¢ºÂèØÂú® https://github.com/diagram-of-thought/diagram-of-thought Áç≤Âæó„ÄÇ

##### **Entity-Aware Self-Attention and Contextualized GCN for Enhanced Relation Extraction in Long Sentences**
2409.13755v1 by Xin Wang, Xinyi Bai

Relation extraction as an important natural Language processing (NLP) task is
to identify relations between named entities in text. Recently, graph
convolutional networks over dependency trees have been widely used to capture
syntactic features and achieved attractive performance. However, most existing
dependency-based approaches ignore the positive influence of the words outside
the dependency trees, sometimes conveying rich and useful information on
relation extraction. In this paper, we propose a novel model, Entity-aware
Self-attention Contextualized GCN (ESC-GCN), which efficiently incorporates
syntactic structure of input sentences and semantic context of sequences. To be
specific, relative position self-attention obtains the overall semantic
pairwise correlation related to word position, and contextualized graph
convolutional networks capture rich intra-sentence dependencies between words
by adequately pruning operations. Furthermore, entity-aware attention layer
dynamically selects which token is more decisive to make final relation
prediction. In this way, our proposed model not only reduces the noisy impact
from dependency trees, but also obtains easily-ignored entity-related semantic
representation. Extensive experiments on various tasks demonstrate that our
model achieves encouraging performance as compared to existing dependency-based
and sequence-based models. Specially, our model excels in extracting relations
between entities of long sentences.

ÊëòË¶ÅÔºöÈóú‰øÇËêÉÂèñ‰ΩúÁÇ∫‰∏ÄÈ†ÖÈáçË¶ÅÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰ªªÂãôÔºåÊòØÁÇ∫‰∫ÜËæ®Ë≠òÊñáÊú¨‰∏≠ÂëΩÂêçÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊúÄËøëÔºå‰æùÂ≠òÊ®π‰∏äÁöÑÂúñÂΩ¢Âç∑Á©çÁ∂≤Ë∑ØÂ∑≤Âª£Ê≥õÁî®ÊñºÊì∑ÂèñÂè•Ê≥ïÁâπÂæµÔºå‰∏¶Áç≤Âæó‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂü∫Êñº‰æùÂ≠òÁöÑÊºîÁÆóÊ≥ïÂ§ßÂ§öÂøΩÁï•‰æùÂ≠òÊ®πÂ§ñÂñÆÂ≠óÁöÑÊ≠£Èù¢ÂΩ±ÈüøÔºåÈÄô‰∫õÂñÆÂ≠óÊúâÊôÇÊúÉÂÇ≥ÈÅîË±êÂØå‰∏îÊúâÁî®ÁöÑÈóú‰øÇËêÉÂèñË≥áË®ä„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ®°ÂûãÔºåÂØ¶È´îÊÑüÁü•Ëá™ÊàëÊ≥®ÊÑèËÑàÁµ°Âåñ GCN (ESC-GCN)ÔºåÂÆÉÊúâÊïàÂú∞Êï¥ÂêàËº∏ÂÖ•Âè•Â≠êÁöÑÂè•Ê≥ïÁµêÊßãÂíåÂ∫èÂàóÁöÑË™ûÊÑèËÑàÁµ°„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÁõ∏Â∞ç‰ΩçÁΩÆËá™ÊàëÊ≥®ÊÑèÊúÉÂèñÂæóËàáÂñÆÂ≠ó‰ΩçÁΩÆÁõ∏ÈóúÁöÑÊï¥È´îË™ûÊÑèÊàêÂ∞çÈóúËÅØÊÄßÔºåËÄåËÑàÁµ°ÂåñÂúñÂΩ¢Âç∑Á©çÁ∂≤Ë∑ØÂâáÈÄèÈÅéÈÅ©Áï∂ÁöÑ‰øÆÂâ™ÈÅãÁÆóÔºåÊì∑ÂèñÂñÆÂ≠ó‰πãÈñìË±êÂØåÁöÑÂè•Â≠êÂÖß‰æùÂ≠òÈóú‰øÇ„ÄÇÊ≠§Â§ñÔºåÂØ¶È´îÊÑüÁü•Ê≥®ÊÑèÂ±§ÊúÉÂãïÊÖãÂú∞ÈÅ∏ÊìáÂì™ÂÄãË®òËôüÂ∞çÊñºÂÅöÂá∫ÊúÄÁµÇÈóú‰øÇÈ†êÊ∏¨ËºÉÁÇ∫Ê±∫ÂÆöÊÄß„ÄÇËóâÁî±ÈÄôÁ®ÆÊñπÂºèÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊ®°Âûã‰∏çÂÉÖÊ∏õÂ∞ë‰∫Ü‰æùÂ≠òÊ®πÂ∏∂‰æÜÁöÑÈõúË®äÂΩ±ÈüøÔºåÈÇÑËÉΩÂèñÂæóÂÆπÊòìË¢´ÂøΩÁï•ÁöÑÂØ¶È´îÁõ∏ÈóúË™ûÊÑèË°®Âæµ„ÄÇÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåËàáÁèæÊúâÁöÑÂü∫Êñº‰æùÂ≠òÂíåÂü∫ÊñºÂ∫èÂàóÁöÑÊ®°ÂûãÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞‰∫Ü‰ª§‰∫∫ÈºìËàûÁöÑÊïàËÉΩ„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ËêÉÂèñÈï∑Âè•‰∏≠ÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤„ÄÇ

##### **Generating Event-oriented Attribution for Movies via Two-Stage Prefix-Enhanced Multimodal LLM**
2409.09362v1 by Yuanjie Lyu, Tong Xu, Zihan Niu, Bo Peng, Jing Ke, Enhong Chen

The prosperity of social media platforms has raised the urgent demand for
semantic-rich services, e.g., event and storyline attribution. However, most
existing research focuses on clip-level event understanding, primarily through
basic captioning tasks, without analyzing the causes of events across an entire
movie. This is a significant challenge, as even advanced multimodal large
language models (MLLMs) struggle with extensive multimodal information due to
limited context length. To address this issue, we propose a Two-Stage
Prefix-Enhanced MLLM (TSPE) approach for event attribution, i.e., connecting
associated events with their causal semantics, in movie videos. In the local
stage, we introduce an interaction-aware prefix that guides the model to focus
on the relevant multimodal information within a single clip, briefly
summarizing the single event. Correspondingly, in the global stage, we
strengthen the connections between associated events using an inferential
knowledge graph, and design an event-aware prefix that directs the model to
focus on associated events rather than all preceding clips, resulting in
accurate event attribution. Comprehensive evaluations of two real-world
datasets demonstrate that our framework outperforms state-of-the-art methods.

ÊëòË¶ÅÔºöÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞ÁöÑËì¨ÂãÉÁôºÂ±ïÔºåÊèêÂçá‰∫ÜÂ∞çË™ûÊÑèË±êÂØåÊúçÂãôÔºà‰æãÂ¶Ç‰∫ã‰ª∂ÂíåÊïÖ‰∫ãÁ∑öÊ≠∏Âõ†ÔºâÁöÑËø´ÂàáÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂Â§ßÂ§öËëóÈáçÊñºÁâáÊÆµÂ±§Á¥öÁöÑ‰∫ã‰ª∂ÁêÜËß£Ôºå‰∏ªË¶ÅÊòØÈÄèÈÅéÂü∫Á§éÁöÑÂ≠óÂπï‰ªªÂãôÔºåËÄåÊú™ÂàÜÊûêÊï¥ÈÉ®ÈõªÂΩ±‰∏≠‰∫ã‰ª∂ÁôºÁîüÁöÑÂéüÂõ†„ÄÇÈÄôÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫Âç≥‰ΩøÊòØÈÄ≤ÈöéÁöÑÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ‰πüÊúÉÂõ†ÁÇ∫ÂèóÈôêÁöÑËÑàÁµ°Èï∑Â∫¶ËÄåÈõ£‰ª•ËôïÁêÜÂª£Ê≥õÁöÑÂ§öÊ®°ÊÖãË≥áË®ä„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ©ÈöéÊÆµÂâçÁΩÆË©ûÂ¢ûÂº∑ MLLM (TSPE) ÊñπÊ≥ïÔºåÁî®ÊñºÈõªÂΩ±ÂΩ±Áâá‰∏≠ÁöÑ‰∫ã‰ª∂Ê≠∏Âõ†Ôºå‰πüÂ∞±ÊòØÂ∞áÁõ∏Èóú‰∫ã‰ª∂ËàáÂÖ∂Âõ†ÊûúË™ûÊÑèÈÄ£ÁµêËµ∑‰æÜ„ÄÇÂú®Â±ÄÈÉ®ÈöéÊÆµÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄã‰∫íÂãïÊÑüÁü•ÂâçÁΩÆË©ûÔºåÂºïÂ∞éÊ®°ÂûãÂ∞àÊ≥®ÊñºÂñÆ‰∏ÄÁâáÊÆµ‰∏≠ÁöÑÁõ∏ÈóúÂ§öÊ®°ÊÖãË≥áË®äÔºåÁ∞°Ë¶ÅÂú∞Á∏ΩÁµêÂñÆ‰∏Ä‰∫ã‰ª∂„ÄÇÁõ∏ÊáâÂú∞ÔºåÂú®Êï¥È´îÈöéÊÆµÔºåÊàëÂÄë‰ΩøÁî®Êé®ÁêÜÁü•Ë≠òÂúñË≠úÂº∑ÂåñÁõ∏Èóú‰∫ã‰ª∂‰πãÈñìÁöÑÈÄ£ÁµêÔºå‰∏¶Ë®≠Ë®à‰∫Ü‰∏ÄÂÄã‰∫ã‰ª∂ÊÑüÁü•ÂâçÁΩÆË©ûÔºåÂºïÂ∞éÊ®°ÂûãÂ∞àÊ≥®ÊñºÁõ∏Èóú‰∫ã‰ª∂ÔºåËÄåÈùûÊâÄÊúâÂâçÁΩÆÁâáÊÆµÔºåÈÄ≤ËÄåÁî¢ÁîüÊ∫ñÁ¢∫ÁöÑ‰∫ã‰ª∂Ê≠∏Âõ†„ÄÇÂ∞çÂÖ©ÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÁöÑÂÖ®Èù¢Ë©ï‰º∞È°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊû∂ÊßãÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÊñπÊ≥ï„ÄÇ

##### **ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models**
2409.09318v1 by Yahan Tu, Rui Hu, Jitao Sang

Hallucination poses a significant challenge for multimodal large language
models (MLLMs). However, existing benchmarks for evaluating hallucinations are
static, which can lead to potential data contamination. This paper introduces
ODE, an open-set, dynamic protocol for evaluating object existence
hallucinations in MLLMs. Our framework employs graph structures to model
associations between real-word concepts and generates novel samples for both
general and domain-specific scenarios. The dynamic combination of concepts,
along with various combination principles, ensures a broad sample distribution.
Experimental results show that MLLMs exhibit higher hallucination rates with
ODE-generated samples, effectively avoiding data contamination. Moreover, these
samples can also be used for fine-tuning to improve MLLM performance on
existing benchmarks.

ÊëòË¶ÅÔºöÂπªË¶∫Â∞çÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑË©ï‰º∞ÂπªË¶∫Âü∫Ê∫ñÊòØÈùúÊÖãÁöÑÔºåÈÄôÂèØËÉΩÂ∞éËá¥ÊΩõÂú®ÁöÑË≥áÊñôÊ±°Êüì„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü ODEÔºå‰∏ÄÁ®ÆÈñãÊîæÂºè„ÄÅÂãïÊÖãÁöÑÂçîÂÆöÔºåÁî®ÊñºË©ï‰º∞ MLLM ‰∏≠ÁöÑÁâ©‰ª∂Â≠òÂú®ÂπªË¶∫„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊé°Áî®ÂúñÂΩ¢ÁµêÊßã‰æÜÂª∫Ê®°ÁúüÂØ¶‰∏ñÁïåÊ¶ÇÂøµ‰πãÈñìÁöÑÈóúËÅØÔºå‰∏¶ÁÇ∫‰∏ÄËà¨ÂíåÁâπÂÆöÈ†òÂüüÊÉÖÂ¢ÉÁî¢ÁîüÊñ∞ÁöÑÁØÑ‰æã„ÄÇÊ¶ÇÂøµÁöÑÂãïÊÖãÁµÑÂêàÔºå‰ª•ÂèäÂêÑÁ®ÆÁµÑÂêàÂéüÂâáÔºåÁ¢∫‰øù‰∫ÜÂª£Ê≥õÁöÑÁØÑ‰æãÂàÜ‰Ωà„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåMLLM Âú® ODE ÁîüÊàêÁöÑÁØÑ‰æã‰∏≠Ë°®ÁèæÂá∫ËºÉÈ´òÁöÑÂπªË¶∫ÁéáÔºåÊúâÊïàÈÅøÂÖç‰∫ÜË≥áÊñôÊ±°Êüì„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÁØÑ‰æã‰πüÂèØË¢´Áî®ÊñºÂæÆË™øÔºå‰ª•ÊîπÂñÑ MLLM Âú®ÁèæÊúâÂü∫Ê∫ñ‰∏äÁöÑÊïàËÉΩ„ÄÇ

##### **Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks**
2409.09026v1 by Florian Gr√∂tschla, Luca Str√§ssle, Luca A. Lanzend√∂rfer, Roger Wattenhofer

Music recommender systems frequently utilize network-based models to capture
relationships between music pieces, artists, and users. Although these
relationships provide valuable insights for predictions, new music pieces or
artists often face the cold-start problem due to insufficient initial
information. To address this, one can extract content-based information
directly from the music to enhance collaborative-filtering-based methods. While
previous approaches have relied on hand-crafted audio features for this
purpose, we explore the use of contrastively pretrained neural audio embedding
models, which offer a richer and more nuanced representation of music. Our
experiments demonstrate that neural embeddings, particularly those generated
with the Contrastive Language-Audio Pretraining (CLAP) model, present a
promising approach to enhancing music recommendation tasks within graph-based
frameworks.

ÊëòË¶ÅÔºöÈü≥Ê®ÇÊé®Ëñ¶Á≥ªÁµ±Á∂ìÂ∏∏‰ΩøÁî®Âü∫ÊñºÁ∂≤Ë∑ØÁöÑÊ®°Âûã‰æÜÊì∑ÂèñÈü≥Ê®Ç‰ΩúÂìÅ„ÄÅËóùË°ìÂÆ∂Âíå‰ΩøÁî®ËÄÖ‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÂÑòÁÆ°ÈÄô‰∫õÈóú‰øÇÁÇ∫È†êÊ∏¨Êèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰ΩÜÁî±ÊñºÂàùÂßãË≥áË®ä‰∏çË∂≥ÔºåÊñ∞ÁöÑÈü≥Ê®Ç‰ΩúÂìÅÊàñËóùË°ìÂÆ∂Á∂ìÂ∏∏Èù¢Ëá®ÂÜ∑ÂïüÂãïÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂèØ‰ª•ÂæûÈü≥Ê®Ç‰∏≠Áõ¥Êé•Êì∑ÂèñÂü∫ÊñºÂÖßÂÆπÁöÑË≥áË®äÔºå‰ª•Â¢ûÂº∑Âü∫ÊñºÂçîÂêåÈÅéÊøæÁöÑÊñπÊ≥ï„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÂÅöÊ≥ïÂ∑≤‰æùË≥¥ÊâãÂ∑•Ë£Ω‰ΩúÁöÑÈü≥Ë®äÁâπÂæµ‰æÜÈÅîÊàêÊ≠§ÁõÆÁöÑÔºå‰ΩÜÊàëÂÄëÊé¢Á¥¢‰ΩøÁî®Â∞çÊØîÈ†êË®ìÁ∑¥Á•ûÁ∂ìÈü≥Ë®äÂµåÂÖ•Ê®°ÂûãÔºåÈÄôÊèê‰æõ‰∫ÜÊõ¥Ë±êÂØå‰∏îÊõ¥Á¥∞Á∑ªÁöÑÈü≥Ê®ÇË°®Á§∫„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÁ•ûÁ∂ìÂµåÂÖ•ÔºåÁâπÂà•ÊòØ‰ΩøÁî®Â∞çÊØîË™ûË®ÄÈü≥Ë®äÈ†êË®ìÁ∑¥ (CLAP) Ê®°ÂûãÁî¢ÁîüÁöÑÂµåÂÖ•ÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Áî®ÊñºÂ¢ûÂº∑ÂúñÂΩ¢ÂåñÊ°ÜÊû∂‰∏≠ÁöÑÈü≥Ê®ÇÊé®Ëñ¶‰ªªÂãô„ÄÇ

##### **Contri(e)ve: Context + Retrieve for Scholarly Question Answering**
2409.09010v1 by Kanchan Shivashankar, Nadine Steinmetz

Scholarly communication is a rapid growing field containing a wealth of
knowledge. However, due to its unstructured and document format, it is
challenging to extract useful information from them through conventional
document retrieval methods. Scholarly knowledge graphs solve this problem, by
representing the documents in a semantic network, providing, hidden insights,
summaries and ease of accessibility through queries. Naturally, question
answering for scholarly graphs expands the accessibility to a wider audience.
But some of the knowledge in this domain is still presented as unstructured
text, thus requiring a hybrid solution for question answering systems. In this
paper, we present a two step solution using open source Large Language
Model(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the
context pertaining to the question from different structured and unstructured
data sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,
we implement prompt engineering to improve the information retrieval
performance of the LLM. Our approach achieved an F1 score of 40% and also
observed some anomalous responses from the LLM, that are discussed in the final
part of the paper.

ÊëòË¶ÅÔºöÂ≠∏Ë°ì‰∫§ÊµÅÊòØ‰∏ÄÂÄãÂø´ÈÄüÊàêÈï∑ÁöÑÈ†òÂüüÔºåÂåÖÂê´‰∫ÜË±êÂØåÁöÑÁü•Ë≠ò„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂÖ∂ÈùûÁµêÊßãÂåñÂíåÊñá‰ª∂Ê†ºÂºèÔºåÈÄèÈÅéÂÇ≥Áµ±ÁöÑÊñá‰ª∂Ê™¢Á¥¢ÊñπÊ≥ïÂæàÈõ£Âæû‰∏≠ËêÉÂèñÂá∫ÊúâÁî®ÁöÑË≥áË®ä„ÄÇÂ≠∏Ë°ìÁü•Ë≠òÂúñË≠úËß£Ê±∫‰∫ÜÈÄôÂÄãÂïèÈ°åÔºåÂÆÉ‰ª•Ë™ûÁæ©Á∂≤Ë∑ØÂëàÁèæÊñá‰ª∂ÔºåÊèê‰æõÈö±ËóèÁöÑË¶ãËß£„ÄÅÊëòË¶ÅÂíåÈÄèÈÅéÊü•Ë©¢ËºïÈ¨ÜÂ≠òÂèñ„ÄÇËá™ÁÑ∂Âú∞ÔºåÂ≠∏Ë°ìÂúñË≠úÁöÑÂïèÁ≠îÊì¥Â±ï‰∫ÜÂ∞çÊõ¥Âª£Ê≥õÂèóÁúæÁöÑÂ≠òÂèñÊÄß„ÄÇ‰ΩÜÈÄôÂÄãÈ†òÂüü‰∏≠ÁöÑ‰∏Ä‰∫õÁü•Ë≠ò‰ªçÁÑ∂‰ª•ÈùûÁµêÊßãÂåñÊñáÂ≠óÂëàÁèæÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏ÄÂÄãÊ∑∑ÂêàËß£Ê±∫ÊñπÊ°à‰æÜÈÄ≤Ë°åÂïèÁ≠îÁ≥ªÁµ±„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰ΩøÁî®ÈñãÊîæÂéüÂßãÁ¢ºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂÖ©Ê≠•È©üËß£Ê±∫ÊñπÊ°àÔºöLlama3.1 for Scholarly-QALD Ë≥áÊñôÈõÜ„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂæû‰∏çÂêåÁöÑÁµêÊßãÂåñÂíåÈùûÁµêÊßãÂåñË≥áÊñô‰æÜÊ∫ê‰∏≠ËêÉÂèñËàáÂïèÈ°åÁõ∏ÈóúÁöÑËÑàÁµ°ÔºöDBLP„ÄÅSemOpenAlex Áü•Ë≠òÂúñË≠úÂíåÁ∂≠Âü∫ÁôæÁßëÊñáÂ≠ó„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÂØ¶‰ΩúÊèêÁ§∫Â∑•Á®ã‰ª•ÊîπÂñÑ LLM ÁöÑË≥áË®äÊ™¢Á¥¢ÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈÅîÂà∞‰∫Ü 40% ÁöÑ F1 ÂàÜÊï∏Ôºå‰∏¶‰∏î‰πüËßÄÂØüÂà∞ LLM ÁöÑ‰∏Ä‰∫õÁï∞Â∏∏ÂõûÊáâÔºåÈÄô‰∫õÂõûÊáâÂú®Êú¨ÊñáÁöÑÊúÄÂæå‰∏ÄÈÉ®ÂàÜ‰∏≠ÈÄ≤Ë°å‰∫ÜË®éË´ñ„ÄÇ

##### **SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity**
2409.09007v1 by Qitian Wu, Kai Yang, Hengrui Zhang, David Wipf, Junchi Yan

Learning representations on large graphs is a long-standing challenge due to
the inter-dependence nature. Transformers recently have shown promising
performance on small graphs thanks to its global attention for capturing
all-pair interactions beyond observed structures. Existing approaches tend to
inherit the spirit of Transformers in language and vision tasks, and embrace
complicated architectures by stacking deep attention-based propagation layers.
In this paper, we attempt to evaluate the necessity of adopting multi-layer
attentions in Transformers on graphs, which considerably restricts the
efficiency. Specifically, we analyze a generic hybrid propagation layer,
comprised of all-pair attention and graph-based propagation, and show that
multi-layer propagation can be reduced to one-layer propagation, with the same
capability for representation learning. It suggests a new technical path for
building powerful and efficient Transformers on graphs, particularly through
simplifying model architectures without sacrificing expressiveness. As
exemplified by this work, we propose a Simplified Single-layer Graph
Transformers (SGFormer), whose main component is a single-layer global
attention that scales linearly w.r.t. graph sizes and requires none of any
approximation for accommodating all-pair interactions. Empirically, SGFormer
successfully scales to the web-scale graph ogbn-papers100M, yielding
orders-of-magnitude inference acceleration over peer Transformers on
medium-sized graphs, and demonstrates competitiveness with limited labeled
data.

ÊëòË¶ÅÔºöÂú®Â§ßÂûãÂúñË°®‰∏äÂ≠∏ÁøíË°®ÂæµÁî±ÊñºÁõ∏‰∫í‰æùË≥¥ÁöÑÊÄßË≥™ËÄåÊàêÁÇ∫‰∏ÄÈ†ÖÈï∑ÊúüÁöÑÊåëÊà∞„ÄÇÁî±Êñº Transfomer ËÉΩÂ§†ÈáùÂ∞çÊâÄÊúâÊàêÂ∞ç‰∫íÂãïÈÄ≤Ë°åÂÖ®Â±ÄÈóúÊ≥®ÔºåË∂ÖË∂äËßÄÊ∏¨ÁµêÊßãÔºåÂõ†Ê≠§ÊúÄËøëÂú®Â∞èÂûãÂúñË°®‰∏äÂ±ïÁèæÂá∫‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩ„ÄÇÁèæÊúâÁöÑÊñπÊ≥ïÂÇæÂêëÊñºÁπºÊâø Transformer Âú®Ë™ûË®ÄÂíåË¶ñË¶∫‰ªªÂãô‰∏≠ÁöÑÁ≤æÁ•ûÔºå‰∏¶ÈÄöÈÅéÂ†ÜÁñäÂü∫ÊñºÊ∑±Â∫¶ÈóúÊ≥®ÁöÑÂÇ≥Êí≠Â±§‰æÜÊé°Áî®Ë§áÈõúÁöÑÊû∂Êßã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂòóË©¶Ë©ï‰º∞Âú®ÂúñË°®‰∏äÊé°Áî®Â§öÂ±§Ê≥®ÊÑèÂäõ Transformer ÁöÑÂøÖË¶ÅÊÄßÔºåÈÄôÊ•µÂ§ßÂú∞ÈôêÂà∂‰∫ÜÊïàÁéá„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂàÜÊûê‰∫Ü‰∏ÄÂÄãÈÄöÁî®ÁöÑÊ∑∑ÂêàÂÇ≥Êí≠Â±§ÔºåÂÆÉÂåÖÂê´ÊâÄÊúâÊàêÂ∞çÊ≥®ÊÑèÂäõÂíåÂü∫ÊñºÂúñË°®ÁöÑÂÇ≥Êí≠Ôºå‰∏¶Ë°®ÊòéÂ§öÂ±§ÂÇ≥Êí≠ÂèØ‰ª•Á∞°ÂåñÁÇ∫ÂñÆÂ±§ÂÇ≥Êí≠ÔºåÂÖ∑ÊúâÁõ∏ÂêåÁöÑË°®ÂæµÂ≠∏ÁøíËÉΩÂäõ„ÄÇÈÄôÁÇ∫Âú®ÂúñË°®‰∏äÊßãÂª∫Âº∑Â§ßËÄåÈ´òÊïàÁöÑ Transformer Êèê‰æõ‰∫Ü‰∏ÄÊ¢ùÊñ∞ÁöÑÊäÄË°ìË∑ØÂæëÔºåÁâπÂà•ÊòØÈÄöÈÅéÁ∞°ÂåñÊ®°ÂûãÊû∂ÊßãÔºåËÄåÁÑ°ÈúÄÁäßÁâ≤Ë°®ÈÅîËÉΩÂäõ„ÄÇÊ≠£Â¶ÇÈÄôÈ†ÖÂ∑•‰ΩúÊâÄ‰æãË≠âÁöÑÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂåñÁöÑÂñÆÂ±§ÂúñÂΩ¢ Transformer (SGFormer)ÔºåÂÖ∂‰∏ªË¶ÅÁµÑÊàêÈÉ®ÂàÜÊòØ‰∏ÄÂÄãÂñÆÂ±§ÂÖ®Â±ÄÊ≥®ÊÑèÂäõÔºåÂÆÉËàáÂúñÂΩ¢Â§ßÂ∞èÊàêÁ∑öÊÄßÊØî‰æãÔºå‰∏¶‰∏î‰∏çÈúÄË¶Å‰ªª‰ΩïËøë‰ºº‰æÜÈÅ©ÊáâÊâÄÊúâÊàêÂ∞ç‰∫íÂãï„ÄÇÊ†πÊìöÁ∂ìÈ©óÔºåSGFormer ÊàêÂäüÂú∞Êì¥Â±ïÂà∞Á∂≤Ë∑ØË¶èÊ®°ÁöÑÂúñË°® ogbn-papers100MÔºåÂú®‰∏≠Á≠âÂ§ßÂ∞èÁöÑÂúñË°®‰∏äÁî¢Áîü‰∫ÜÊØîÂêåÂÑï Transformer Âø´ÂπæÂÄãÊï∏ÈáèÁ¥öÁöÑÊé®Ë´ñÂä†ÈÄüÔºå‰∏¶Ë≠âÊòé‰∫ÜÂú®Ê®ôÁ±§Ë≥áÊñôÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÂÖ∑ÊúâÁ´∂Áà≠Âäõ„ÄÇ

##### **Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies**
2409.08864v1 by Zhiqiang Zhong, Davide Mottin

Large Language Models (LLMs) have shown remarkable capabilities in processing
various data structures, including graphs. While previous research has focused
on developing textual encoding methods for graph representation, the emergence
of multimodal LLMs presents a new frontier for graph comprehension. These
advanced models, capable of processing both text and images, offer potential
improvements in graph understanding by incorporating visual representations
alongside traditional textual data. This study investigates the impact of graph
visualisations on LLM performance across a range of benchmark tasks at node,
edge, and graph levels. Our experiments compare the effectiveness of multimodal
approaches against purely textual graph representations. The results provide
valuable insights into both the potential and limitations of leveraging visual
graph modalities to enhance LLMs' graph structure comprehension abilities.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ËôïÁêÜÂêÑÁ®ÆÊï∏ÊìöÁµêÊßãÔºàÂåÖÊã¨ÂúñÂΩ¢ÔºâÊñπÈù¢Ë°®ÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÂÑòÁÆ°ÂÖàÂâçÁöÑÁ†îÁ©∂ËëóÈáçÊñºÈñãÁôºÂúñÂΩ¢Ë°®Á§∫ÁöÑÊñáÊú¨Á∑®Á¢ºÊñπÊ≥ïÔºå‰ΩÜÂ§öÊ®°ÊÖã LLM ÁöÑÂá∫ÁèæÁÇ∫ÂúñÂΩ¢ÁêÜËß£Êèê‰æõ‰∫ÜÊñ∞ÁöÑÈ†òÂüü„ÄÇÈÄô‰∫õÂÖàÈÄ≤ÁöÑÊ®°ÂûãËÉΩÂ§†ËôïÁêÜÊñáÊú¨ÂíåÂúñÂÉèÔºåÈÄèÈÅéÁµêÂêàË¶ñË¶∫Ë°®Á§∫ËàáÂÇ≥Áµ±ÊñáÊú¨Ë≥áÊñôÔºåÊèê‰æõÂúñÂΩ¢ÁêÜËß£ÁöÑÊΩõÂú®ÊîπÈÄ≤„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂúñÂΩ¢Ë¶ñË¶∫ÂåñÂ∞ç LLM Âú®ÁØÄÈªû„ÄÅÈÇäÁ∑£ÂíåÂúñÂΩ¢Â±§Á¥ö‰∏ÄÁ≥ªÂàóÂü∫Ê∫ñ‰ªªÂãôÁöÑÊïàËÉΩÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÊØîËºÉ‰∫ÜÂ§öÊ®°ÊÖãÊñπÊ≥ïËàáÁ¥îÊñáÊú¨ÂúñÂΩ¢Ë°®Á§∫ÁöÑÊúâÊïàÊÄß„ÄÇÁµêÊûúÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰∫ÜËß£Âà©Áî®Ë¶ñË¶∫ÂúñÂΩ¢Ê®°ÊÖã‰æÜÂ¢ûÂº∑ LLM ÂúñÂΩ¢ÁµêÊßãÁêÜËß£ËÉΩÂäõÁöÑÊΩõÂäõËàáÈôêÂà∂„ÄÇ

##### **A RAG Approach for Generating Competency Questions in Ontology Engineering**
2409.08820v1 by Xueli Pan, Jacco van Ossenbruggen, Victor de Boer, Zhisheng Huang

Competency question (CQ) formulation is central to several ontology
development and evaluation methodologies. Traditionally, the task of crafting
these competency questions heavily relies on the effort of domain experts and
knowledge engineers which is often time-consuming and labor-intensive. With the
emergence of Large Language Models (LLMs), there arises the possibility to
automate and enhance this process. Unlike other similar works which use
existing ontologies or knowledge graphs as input to LLMs, we present a
retrieval-augmented generation (RAG) approach that uses LLMs for the automatic
generation of CQs given a set of scientific papers considered to be a domain
knowledge base. We investigate its performance and specifically, we study the
impact of different number of papers to the RAG and different temperature
setting of the LLM. We conduct experiments using GPT-4 on two domain ontology
engineering tasks and compare results against ground-truth CQs constructed by
domain experts. Empirical assessments on the results, utilizing evaluation
metrics (precision and consistency), reveal that compared to zero-shot
prompting, adding relevant domain knowledge to the RAG improves the performance
of LLMs on generating CQs for concrete ontology engineering tasks.

ÊëòË¶ÅÔºöËÉΩÂäõÂïèÈ°å (CQ) ÁöÑÂà∂ÂÆöÊòØÂπæÂÄãÊú¨‰ΩìË´ñÁôºÂ±ïÂíåË©ï‰º∞ÊñπÊ≥ïÁöÑ‰∏≠ÂøÉ„ÄÇÂÇ≥Áµ±‰∏äÔºåÂà∂ÂÆöÈÄô‰∫õËÉΩÂäõÂïèÈ°åÁöÑ‰ªªÂãôÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÈ†òÂüüÂ∞àÂÆ∂ÂíåÁü•Ë≠òÂ∑•Á®ãÂ∏´ÁöÑÂä™ÂäõÔºåÈÄôÈÄöÂ∏∏ÊòØËÄóÊôÇ‰∏îÂãûÂäõÂØÜÈõÜÁöÑ„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÔºåËá™ÂãïÂåñÂíåÂ¢ûÂº∑Ê≠§ÈÅéÁ®ãÁöÑÂèØËÉΩÊÄßÂá∫Áèæ‰∫Ü„ÄÇËàáÂÖ∂‰ªñ‰ΩøÁî®ÁèæÊúâÊú¨‰ΩìË´ñÊàñÁü•Ë≠òÂúñË≠ú‰ΩúÁÇ∫ LLM Ëº∏ÂÖ•ÁöÑÈ°û‰ººÂ∑•‰Ωú‰∏çÂêåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî® LLM Ëá™ÂãïÁîüÊàêË¢´Ë™çÁÇ∫ÊòØÈ†òÂüüÁü•Ë≠òÂ∫´ÁöÑ‰∏ÄÁµÑÁßëÂ≠∏Ë´ñÊñáÁöÑ CQ„ÄÇÊàëÂÄëÁ†îÁ©∂ÂÖ∂ÊÄßËÉΩÔºåÁâπÂà•ÊòØÊàëÂÄëÁ†îÁ©∂‰∏çÂêåÊï∏ÈáèÁöÑË´ñÊñáÂ∞ç RAG ÁöÑÂΩ±ÈüøÂíå LLM ÁöÑ‰∏çÂêåÊ∫´Â∫¶Ë®≠ÁΩÆ„ÄÇÊàëÂÄë‰ΩøÁî® GPT-4 Â∞çÂÖ©ÂÄãÈ†òÂüüÊú¨‰ΩìË´ñÂ∑•Á®ã‰ªªÂãôÈÄ≤Ë°åÂØ¶È©óÔºå‰∏¶Â∞áÁµêÊûúËàáÁî±È†òÂüüÂ∞àÂÆ∂ÊßãÈÄ†ÁöÑÁúüÂØ¶ CQ ÈÄ≤Ë°åÊØîËºÉ„ÄÇÂà©Áî®Ë©ï‰º∞ÊåáÊ®ôÔºàÁ≤æÁ¢∫Â∫¶Âíå‰∏ÄËá¥ÊÄßÔºâÂ∞çÁµêÊûúÈÄ≤Ë°åÁöÑÂØ¶Ë≠âË©ï‰º∞Ë°®ÊòéÔºåËàáÈõ∂Ê¨°ÊèêÁ§∫Áõ∏ÊØîÔºåÂ∞áÁõ∏ÈóúÈ†òÂüüÁü•Ë≠òÊ∑ªÂä†Âà∞ RAG ÂèØ‰ª•ÊèêÈ´ò LLM Âú®ÁÇ∫ÂÖ∑È´îÊú¨‰ΩìË´ñÂ∑•Á®ã‰ªªÂãôÁîüÊàê CQ ÊñπÈù¢ÁöÑÊÄßËÉΩ„ÄÇ

##### **ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model**
2409.08543v1 by Zezheng Qin

Recommender Systems (RS) play a pivotal role in boosting user satisfaction by
providing personalized product suggestions in domains such as e-commerce and
entertainment. This study examines the integration of multimodal data text and
audio into large language models (LLMs) with the aim of enhancing
recommendation performance. Traditional text and audio recommenders encounter
limitations such as the cold-start problem, and recent advancements in LLMs,
while promising, are computationally expensive. To address these issues,
Low-Rank Adaptation (LoRA) is introduced, which enhances efficiency without
compromising performance. The ATFLRec framework is proposed to integrate audio
and text modalities into a multimodal recommendation system, utilizing various
LoRA configurations and modality fusion techniques. Results indicate that
ATFLRec outperforms baseline models, including traditional and graph neural
network-based approaches, achieving higher AUC scores. Furthermore, separate
fine-tuning of audio and text data with distinct LoRA modules yields optimal
performance, with different pooling methods and Mel filter bank numbers
significantly impacting performance. This research offers valuable insights
into optimizing multimodal recommender systems and advancing the integration of
diverse data modalities in LLMs.

ÊëòË¶ÅÔºöÊé®Ëñ¶Á≥ªÁµ± (RS) Âú®ÊèêÂçá‰ΩøÁî®ËÄÖÊªøÊÑèÂ∫¶‰∏≠ÊâÆÊºîËëóËàâË∂≥ËºïÈáçÁöÑËßíËâ≤ÔºåÂÆÉÂú®ÈõªÂ≠êÂïÜÂãôÂíåÂ®õÊ®ÇÁ≠âÈ†òÂüüÊèê‰æõÂÄã‰∫∫ÂåñÁöÑÁî¢ÂìÅÂª∫Ë≠∞„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂ∞áÂ§öÊ®°ÊÖãË≥áÊñôÊñáÂ≠óÂíåÈü≥Ë®äÊï¥ÂêàÂà∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Ôºå‰ª•Â¢ûÂº∑Êé®Ëñ¶ÊïàËÉΩ„ÄÇÂÇ≥Áµ±ÁöÑÊñáÂ≠óÂíåÈü≥Ë®äÊé®Ëñ¶Âô®ÊúÉÈÅáÂà∞ÂÜ∑ÂïüÂãïÂïèÈ°åÁ≠âÈôêÂà∂ÔºåËÄå LLM ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÈõñÁÑ∂ÂæàÊúâÂâçÊôØÔºå‰ΩÜË®àÁÆóÊàêÊú¨ÂæàÈ´ò„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÂºïÂÖ•‰∫Ü‰ΩéÁß©ÈÅ©Êáâ (LoRA)ÔºåÂÆÉÂú®‰∏çÂΩ±ÈüøÊïàËÉΩÁöÑÊÉÖÊ≥Å‰∏ãÊèêÂçá‰∫ÜÊïàÁéá„ÄÇATFLRec Ê°ÜÊû∂Ë¢´ÊèêÂá∫‰æÜÂ∞áÈü≥Ë®äÂíåÊñáÂ≠óÊ®°ÊÖãÊï¥ÂêàÂà∞Â§öÊ®°ÊÖãÊé®Ëñ¶Á≥ªÁµ±‰∏≠ÔºåÂà©Áî®ÂêÑÁ®Æ LoRA ÈÖçÁΩÆÂíåÊ®°ÊÖãËûçÂêàÊäÄË°ì„ÄÇÁµêÊûúË°®ÊòéÔºåATFLRec ÂÑ™ÊñºÂü∫Á∑öÊ®°ÂûãÔºåÂåÖÊã¨ÂÇ≥Áµ±ÂíåÂü∫ÊñºÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊñπÊ≥ïÔºåÈÅîÂà∞‰∫ÜÊõ¥È´òÁöÑ AUC ÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®‰∏çÂêåÁöÑ LoRA Ê®°ÁµÑÂ∞çÈü≥Ë®äÂíåÊñáÂ≠óË≥áÊñôÈÄ≤Ë°åÂñÆÁç®ÂæÆË™øÊúÉÁî¢ÁîüÊúÄ‰Ω≥ÊïàËÉΩÔºå‰∏çÂêåÁöÑÊ±†ÂåñÊñπÊ≥ïÂíå Mel ÊøæÊ≥¢Âô®ÁµÑÊï∏ÊúÉÂ∞çÊïàËÉΩÁî¢ÁîüÈ°ØËëóÂΩ±Èüø„ÄÇÊú¨Á†îÁ©∂Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË¶ãËß£ÔºåÁî®ÊñºÊúÄ‰Ω≥ÂåñÂ§öÊ®°ÊÖãÊé®Ëñ¶Á≥ªÁµ±Ôºå‰∏¶Êé®ÂãïÂ∞á‰∏çÂêåÁöÑË≥áÊñôÊ®°ÊÖãÊï¥ÂêàÂà∞ LLM ‰∏≠„ÄÇ

##### **What Makes a Maze Look Like a Maze?**
2409.08202v1 by Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu

A unique aspect of human visual understanding is the ability to flexibly
interpret abstract concepts: acquiring lifted rules explaining what they
symbolize, grounding them across familiar and unfamiliar contexts, and making
predictions or reasoning about them. While off-the-shelf vision-language models
excel at making literal interpretations of images (e.g., recognizing object
categories such as tree branches), they still struggle to make sense of such
visual abstractions (e.g., how an arrangement of tree branches may form the
walls of a maze). To address this challenge, we introduce Deep Schema Grounding
(DSG), a framework that leverages explicit structured representations of visual
abstractions for grounding and reasoning. At the core of DSG are
schemas--dependency graph descriptions of abstract concepts that decompose them
into more primitive-level symbols. DSG uses large language models to extract
schemas, then hierarchically grounds concrete to abstract components of the
schema onto images with vision-language models. The grounded schema is used to
augment visual abstraction understanding. We systematically evaluate DSG and
different methods in reasoning on our new Visual Abstractions Dataset, which
consists of diverse, real-world images of abstract concepts and corresponding
question-answer pairs labeled by humans. We show that DSG significantly
improves the abstract visual reasoning performance of vision-language models,
and is a step toward human-aligned understanding of visual abstractions.

ÊëòË¶ÅÔºö‰∫∫È°ûË¶ñË¶∫ÁêÜËß£ÁöÑÁç®ÁâπÈù¢ÂêëÂú®ÊñºÈùàÊ¥ªË©ÆÈáãÊäΩË±°Ê¶ÇÂøµÁöÑËÉΩÂäõÔºöÁç≤ÂèñËß£ÈáãÂÖ∂Ë±°ÂæµÊÑèÁæ©ÁöÑÊèêÂçáË¶èÂâáÔºåÂú®ÁÜüÊÇâÂíå‰∏çÁÜüÊÇâÁöÑËÉåÊôØ‰∏ãÂ•†ÂÆöÂÖ∂Âü∫Á§éÔºå‰∏¶Â∞çÂÖ∂ÈÄ≤Ë°åÈ†êÊ∏¨ÊàñÊé®ÁêÜ„ÄÇÈõñÁÑ∂ÁèæÊàêÁöÑË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÊìÖÈï∑Â∞çÂΩ±ÂÉèÈÄ≤Ë°åÂ≠óÈù¢Ë©ÆÈáãÔºà‰æãÂ¶ÇËæ®Ë≠òÊ®πÊûùÁ≠âÁâ©È´îÈ°ûÂà•ÔºâÔºå‰ΩÜÂÆÉÂÄëÂú®ÁêÜËß£Ê≠§È°ûË¶ñË¶∫ÊäΩË±°Ê¶ÇÂøµÊôÇ‰ªçÊúâÂõ∞Èõ£Ôºà‰æãÂ¶ÇÊ®πÊûùÁöÑÊéíÂàóÂ¶Ç‰ΩïÂΩ¢ÊàêËø∑ÂÆÆÁöÑÁâÜÂ£ÅÔºâ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÊ≠§ÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊ∑±Â∫¶Ê®°ÂºèÂü∫Á§éÔºàDSGÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÂà©Áî®Ë¶ñË¶∫ÊäΩË±°Ê¶ÇÂøµÁöÑÊòéÁ¢∫ÁµêÊßãÂåñË°®Á§∫‰æÜÈÄ≤Ë°åÂü∫Á§éÂíåÊé®ÁêÜ„ÄÇDSG ÁöÑÊ†∏ÂøÉÊòØÊ®°Âºè‚Äî‚ÄîÊäΩË±°Ê¶ÇÂøµÁöÑ‰æùË≥¥ÂúñÊèèËø∞ÔºåÂ∞áÂÖ∂ÂàÜËß£ÁÇ∫Êõ¥ÂéüÂßãÂ±§Á¥öÁöÑÁ¨¶Ëôü„ÄÇDSG ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜÊèêÂèñÊ®°ÂºèÔºåÁÑ∂ÂæåÂ∞áÊ®°ÂºèÁöÑÂÖ∑È´îÁµÑÊàêÈÉ®ÂàÜÂàÜÂ±§Âü∫Á§éÂà∞ÂΩ±ÂÉè‰∏äÔºå‰∏¶‰ΩøÁî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã„ÄÇÂü∫Á§éÊ®°ÂºèÁî®ÊñºÊì¥ÂÖÖË¶ñË¶∫ÊäΩË±°ÁêÜËß£„ÄÇÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫Ü DSG ÂíåÊàëÂÄëÁöÑÊñ∞Ë¶ñË¶∫ÊäΩË±°Ë≥áÊñôÈõÜ‰∏äÁöÑ‰∏çÂêåÊé®ÁêÜÊñπÊ≥ïÔºåË©≤Ë≥áÊñôÈõÜÂåÖÂê´ÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïåÁöÑÊäΩË±°Ê¶ÇÂøµÂΩ±ÂÉèÔºå‰ª•ÂèäÁî±‰∫∫È°ûÊ®ôË®òÁöÑÂ∞çÊáâÂïèÈ°åËß£Á≠îÂ∞ç„ÄÇÊàëÂÄëË≠âÊòé DSG Â§ßÂπÖÊèêÂçá‰∫ÜË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÊäΩË±°Ë¶ñË¶∫Êé®ÁêÜÊïàËÉΩÔºå‰∏¶‰∏îÊúùËëóËàá‰∫∫È°û‰∏ÄËá¥ÁöÑË¶ñË¶∫ÊäΩË±°ÁêÜËß£ÈÇÅÈÄ≤‰∏ÄÊ≠•„ÄÇ

##### **Towards a graph-based foundation model for network traffic analysis**
2409.08111v1 by Louis Van Langendonck, Ismael Castell-Uroz, Pere Barlet-Ros

Foundation models have shown great promise in various fields of study. A
potential application of such models is in computer network traffic analysis,
where these models can grasp the complexities of network traffic dynamics and
adapt to any specific task or network environment with minimal fine-tuning.
Previous approaches have used tokenized hex-level packet data and the model
architecture of large language transformer models. We propose a new, efficient
graph-based alternative at the flow-level. Our approach represents network
traffic as a dynamic spatio-temporal graph, employing a self-supervised link
prediction pretraining task to capture the spatial and temporal dynamics in
this network graph framework. To evaluate the effectiveness of our approach, we
conduct a few-shot learning experiment for three distinct downstream network
tasks: intrusion detection, traffic classification, and botnet classification.
Models finetuned from our pretrained base achieve an average performance
increase of 6.87\% over training from scratch, demonstrating their ability to
effectively learn general network traffic dynamics during pretraining. This
success suggests the potential for a large-scale version to serve as an
operational foundational model.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÂ∑≤Âú®ÂêÑÂÄãÁ†îÁ©∂È†òÂüü‰∏≠Â±ïÁèæÂá∫Ê•µÂ§ßÁöÑÂâçÊôØ„ÄÇÊ≠§È°ûÊ®°ÂûãÁöÑÊΩõÂú®ÊáâÁî®‰πã‰∏ÄÂú®ÊñºÈõªËÖ¶Á∂≤Ë∑ØÊµÅÈáèÂàÜÊûêÔºåÂÖ∂‰∏≠ÈÄô‰∫õÊ®°ÂûãÂèØ‰ª•ÊéåÊè°Á∂≤Ë∑ØÊµÅÈáèÂãïÊÖãÁöÑË§áÈõúÊÄßÔºå‰∏¶‰ª•ÊúÄÂ∞èÁöÑÂæÆË™øÈÅ©Êáâ‰ªª‰ΩïÁâπÂÆö‰ªªÂãôÊàñÁ∂≤Ë∑ØÁí∞Â¢É„ÄÇÂÖàÂâçÁöÑÂÅöÊ≥ïÂ∑≤‰ΩøÁî®Ê®ôË®òÂåñÂçÅÂÖ≠ÈÄ≤‰ΩçÂ±§Á¥öÂ∞ÅÂåÖË≥áÊñôÂíåÂ§ßÂûãË™ûË®ÄËΩâÊèõÂô®Ê®°ÂûãÁöÑÊ®°ÂûãÊû∂Êßã„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑ„ÄÅÊúâÊïàÁöÑÊµÅÁ®ãÂ±§Á¥öÂúñÂΩ¢ÂåñÊõø‰ª£ÊñπÊ°à„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∞áÁ∂≤Ë∑ØÊµÅÈáèË°®Á§∫ÁÇ∫ÂãïÊÖãÊôÇÁ©∫ÂúñÂΩ¢ÔºåÊé°Áî®Ëá™ÊàëÁõ£Áù£ÈÄ£ÁµêÈ†êÊ∏¨È†êË®ìÁ∑¥‰ªªÂãô‰æÜÊçïÊçâÊ≠§Á∂≤Ë∑ØÂúñÂΩ¢Êû∂Êßã‰∏≠ÁöÑÁ©∫ÈñìÂíåÊôÇÈñìÂãïÊÖã„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÊàëÂÄëÂÅöÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊàëÂÄëÂ∞ç‰∏âÂÄã‰∏çÂêåÁöÑ‰∏ãÊ∏∏Á∂≤Ë∑Ø‰ªªÂãôÔºàÂÖ•‰æµÂÅµÊ∏¨„ÄÅÊµÅÈáèÂàÜÈ°ûÂíåÊÆ≠Â±çÁ∂≤Ë∑ØÂàÜÈ°ûÔºâÈÄ≤Ë°åÂ∞ëÈáèÂ≠∏ÁøíÂØ¶È©ó„ÄÇÂæûÊàëÂÄëÁöÑÈ†êË®ìÁ∑¥Âü∫Á§éÂæÆË™øÁöÑÊ®°ÂûãÔºåÂÖ∂Âπ≥ÂùáÊïàËÉΩÊèêÂçá 6.87%ÔºåÈ´òÊñºÂæûÈ†≠Ë®ìÁ∑¥ÔºåÈÄôË≠âÊòé‰∫ÜÂÆÉÂÄëÂú®È†êË®ìÁ∑¥ÊúüÈñìÊúâÊïàÂ≠∏Áøí‰∏ÄËà¨Á∂≤Ë∑ØÊµÅÈáèÂãïÊÖãÁöÑËÉΩÂäõ„ÄÇÈÄôÈ†ÖÊàêÂäüÈ°ØÁ§∫Âá∫Â§ßË¶èÊ®°ÁâàÊú¨ÊúâÊΩõÂäõ‰ΩúÁÇ∫ÈÅã‰ΩúÂü∫Á§éÊ®°Âûã„ÄÇ

##### **Learning Rules from KGs Guided by Language Models**
2409.07869v1 by Zihang Peng, Daria Stepanova, Vinh Thinh Ho, Heike Adel, Alessandra Russo, Simon Ott

Advances in information extraction have enabled the automatic construction of
large knowledge graphs (e.g., Yago, Wikidata or Google KG), which are widely
used in many applications like semantic search or data analytics. However, due
to their semi-automatic construction, KGs are often incomplete. Rule learning
methods, concerned with the extraction of frequent patterns from KGs and
casting them into rules, can be applied to predict potentially missing facts. A
crucial step in this process is rule ranking. Ranking of rules is especially
challenging over highly incomplete or biased KGs (e.g., KGs predominantly
storing facts about famous people), as in this case biased rules might fit the
data best and be ranked at the top based on standard statistical metrics like
rule confidence. To address this issue, prior works proposed to rank rules not
only relying on the original KG but also facts predicted by a KG embedding
model. At the same time, with the recent rise of Language Models (LMs), several
works have claimed that LMs can be used as alternative means for KG completion.
In this work, our goal is to verify to which extent the exploitation of LMs is
helpful for improving the quality of rule learning systems.

ÊëòË¶ÅÔºöË≥áË®äËêÉÂèñÁöÑÈÄ≤Â±ïÂ∑≤ËÉΩËá™ÂãïÂª∫ÊßãÂ§ßÂûãÁü•Ë≠òÂúñË≠úÔºà‰æãÂ¶Ç Yago„ÄÅWikidata Êàñ Google KGÔºâÔºåÈÄô‰∫õÁü•Ë≠òÂúñË≠úÂª£Ê≥õÁî®ÊñºË®±Â§öÊáâÁî®Á®ãÂºèÔºå‰æãÂ¶ÇË™ûÊÑèÊêúÂ∞ãÊàñË≥áÊñôÂàÜÊûê„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÈÄô‰∫õÁü•Ë≠òÂúñË≠úÊòØÂçäËá™ÂãïÂª∫ÊßãÁöÑÔºåÂõ†Ê≠§ÈÄöÂ∏∏‰∏¶‰∏çÂÆåÊï¥„ÄÇË¶èÂâáÂ≠∏ÁøíÊñπÊ≥ïËëóÈáçÊñºÂæûÁü•Ë≠òÂúñË≠ú‰∏≠ËêÉÂèñÈ†ªÁπÅÊ®°ÂºèÔºå‰∏¶Â∞áÂÆÉÂÄëËΩâÊèõÁÇ∫Ë¶èÂâáÔºåÂèØÊáâÁî®ÊñºÈ†êÊ∏¨ÊΩõÂú®ÈÅ∫Â§±ÁöÑ‰∫ãÂØ¶„ÄÇÊ≠§ÈÅéÁ®ã‰∏≠ÁöÑ‰∏ÄÂÄãÈóúÈçµÊ≠•È©üÊòØË¶èÂâáÊéíÂ∫è„ÄÇË¶èÂâáÊéíÂ∫èÂú®È´òÂ∫¶‰∏çÂÆåÊï¥ÊàñÊúâÂÅèÂ∑ÆÁöÑÁü•Ë≠òÂúñË≠úÔºà‰æãÂ¶ÇÔºå‰∏ªË¶ÅÂÑ≤Â≠òÂêç‰∫∫‰∫ãÂØ¶ÁöÑÁü•Ë≠òÂúñË≠úÔºâ‰∏≠ÁâπÂà•ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫Âú®ÈÄôÁ®ÆÊÉÖÊ≥Å‰∏ãÔºåÊúâÂÅèÂ∑ÆÁöÑË¶èÂâáÂèØËÉΩÊúÄÁ¨¶ÂêàË≥áÊñôÔºå‰∏¶Ê†πÊìöÊ®ôÊ∫ñÁµ±Ë®àÈáèÂ∫¶Ôºà‰æãÂ¶ÇË¶èÂâá‰ø°ÂøÉÔºâÊéíÂú®ÊúÄÂâçÈù¢„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂÖàÂâçÁöÑÁ†îÁ©∂ÊèêÂá∫‰∏çÂè™‰æùË≥¥ÂéüÂßãÁü•Ë≠òÂúñË≠úÔºåÈÇÑË¶Å‰æùË≥¥Áü•Ë≠òÂúñË≠úÂµåÂÖ•Ê®°ÂûãÈ†êÊ∏¨ÁöÑ‰∫ãÂØ¶‰æÜÂ∞çË¶èÂâáÈÄ≤Ë°åÊéíÂ∫è„ÄÇÂêåÊôÇÔºåÈö®ËëóË™ûË®ÄÊ®°Âûã (LM) ÁöÑËààËµ∑Ôºå‰∏Ä‰∫õÁ†îÁ©∂ËÅ≤Á®± LM ÂèØÁî®‰ΩúÁü•Ë≠òÂúñË≠úÂÆåÊàêÁöÑÊõø‰ª£ÊñπÊ≥ï„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÈ©óË≠âÂà©Áî® LM Âú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÊúâÂä©ÊñºÊèêÂçáË¶èÂâáÂ≠∏ÁøíÁ≥ªÁµ±ÁöÑÂìÅË≥™„ÄÇ

##### **Multi-object event graph representation learning for Video Question Answering**
2409.07747v1 by Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Vizcarra, Mori Kurokawa

Video question answering (VideoQA) is a task to predict the correct answer to
questions posed about a given video. The system must comprehend spatial and
temporal relationships among objects extracted from videos to perform causal
and temporal reasoning. While prior works have focused on modeling individual
object movements using transformer-based methods, they falter when capturing
complex scenarios involving multiple objects (e.g., "a boy is throwing a ball
in a hoop"). We propose a contrastive language event graph representation
learning method called CLanG to address this limitation. Aiming to capture
event representations associated with multiple objects, our method employs a
multi-layer GNN-cluster module for adversarial graph representation learning,
enabling contrastive learning between the question text and its relevant
multi-object event graph. Our method outperforms a strong baseline, achieving
up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and
TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal
and temporal questions, highlighting its strength in reasoning multiple
object-based events.

ÊëòË¶ÅÔºöÂΩ±ÁâáÂïèÁ≠î (VideoQA) ÊòØ‰∏ÄÈ†Ö‰ªªÂãôÔºåÁî®ÊñºÈ†êÊ∏¨ÈáùÂ∞çÁµ¶ÂÆöÂΩ±ÁâáÊèêÂá∫ÁöÑÂïèÈ°åÁöÑÊ≠£Á¢∫Á≠îÊ°à„ÄÇÁ≥ªÁµ±ÂøÖÈ†à‰∫ÜËß£ÂæûÂΩ±Áâá‰∏≠ÊèêÂèñÁöÑÁâ©‰ª∂‰πãÈñìÁöÑÁ©∫ÈñìÂíåÊôÇÈñìÈóú‰øÇÔºåÊâçËÉΩÂü∑Ë°åÂõ†ÊûúÈóú‰øÇÂíåÊôÇÈñìÊé®ÁêÜ„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÁ†îÁ©∂ÈõÜ‰∏≠Êñº‰ΩøÁî®Âü∫ÊñºTransformerÁöÑÊ®°Âûã‰æÜÂª∫Ê®°ÂÄãÂà•Áâ©‰ª∂ÁöÑÂãï‰ΩúÔºå‰ΩÜÂú®ÊçïÊçâÊ∂âÂèäÂ§öÂÄãÁâ©‰ª∂ÁöÑË§áÈõúÂ†¥ÊôØÔºà‰æãÂ¶Ç„Äå‰∏ÄÂÄãÁî∑Â≠©Ê≠£Âú®Â∞áÁêÉÊäïÈÄ≤Á±ÉÊ°Ü„ÄçÔºâÊôÇÔºåÂÆÉÂÄëÊúÉÂá∫ÁèæÂïèÈ°å„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∞çÊØîÂºèË™ûË®Ä‰∫ã‰ª∂ÂúñË°®Ë°®Á§∫Â≠∏ÁøíÊñπÊ≥ïÔºåÁ®±ÁÇ∫ CLanGÔºå‰ª•Ëß£Ê±∫Ê≠§ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÊçïÊçâËàáÂ§öÂÄãÁâ©‰ª∂Áõ∏ÈóúÁöÑ‰∫ã‰ª∂Ë°®Á§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊé°Áî®Â§öÂ±§ GNN ÈõÜÁæ§Ê®°ÁµÑÈÄ≤Ë°åÂ∞çÊäóÂºèÂúñË°®Ë°®Á§∫Â≠∏ÁøíÔºå‰ΩøÂïèÈ°åÊñáÂ≠óÂèäÂÖ∂Áõ∏ÈóúÁöÑÂ§öÁâ©‰ª∂‰∫ã‰ª∂ÂúñË°®‰πãÈñìËÉΩÂ§†ÈÄ≤Ë°åÂ∞çÊØîÂºèÂ≠∏Áøí„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂÑ™ÊñºÂº∑Â§ßÁöÑÂü∫Ê∫ñÔºåÂú®ÂÖ©ÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ VideoQA Ë≥áÊñôÈõÜ NExT-QA Âíå TGIF-QA-R ‰∏äÈÅîÂà∞‰∫ÜÈ´òÈÅî 2.2% ÁöÑÊõ¥È´òÊ∫ñÁ¢∫Â∫¶„ÄÇÁâπÂà•ÊòØÔºåÂú®ËôïÁêÜÂõ†ÊûúÈóú‰øÇÂíåÊôÇÈñìÂïèÈ°åÊñπÈù¢ÊØîÂü∫Ê∫ñÈ´òÂá∫ 2.8%ÔºåÁ™ÅÈ°Ø‰∫ÜÂÆÉÂú®Êé®ÁêÜÂ§öÂÄãÂü∫ÊñºÁâ©‰ª∂ÁöÑ‰∫ã‰ª∂ÊñπÈù¢ÁöÑÂÑ™Âã¢„ÄÇ

##### **Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code**
2409.07368v3 by Khiem Ton, Nhi Nguyen, Mahmoud Nazzal, Abdallah Khreishah, Cristian Borcea, NhatHai Phan, Ruoming Jin, Issa Khalil, Yelong Shen

This paper introduces SGCode, a flexible prompt-optimizing system to generate
secure code with large language models (LLMs). SGCode integrates recent
prompt-optimization approaches with LLMs in a unified system accessible through
front-end and back-end APIs, enabling users to 1) generate secure code, which
is free of vulnerabilities, 2) review and share security analysis, and 3)
easily switch from one prompt optimization approach to another, while providing
insights on model and system performance. We populated SGCode on an AWS server
with PromSec, an approach that optimizes prompts by combining an LLM and
security tools with a lightweight generative adversarial graph neural network
to detect and fix security vulnerabilities in the generated code. Extensive
experiments show that SGCode is practical as a public tool to gain insights
into the trade-offs between model utility, secure code generation, and system
cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is
available at: https://sgcode.codes/.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁ¥π SGCodeÔºå‰∏ÄÂÄãÂΩàÊÄßÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÁ≥ªÁµ±ÔºåÁî®ÊñºÁîüÊàêÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂÆâÂÖ®Á®ãÂºèÁ¢º„ÄÇSGCode Â∞áÊúÄËøëÁöÑÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÊñπÊ≥ïËàá LLM Êï¥ÂêàÂú®‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÁ≥ªÁµ±‰∏≠ÔºåÂèØÈÄèÈÅéÂâçÁ´ØÂíåÂæåÁ´Ø API Â≠òÂèñÔºå‰ΩøÁî®Êà∂ËÉΩÂ§† 1) Áî¢ÁîüÂÆâÂÖ®ÁöÑÁ®ãÂºèÁ¢ºÔºåÊ≤íÊúâÊºèÊ¥ûÔºå2) Ê™¢Èñ±ÂíåÂàÜ‰∫´ÂÆâÂÖ®ÊÄßÂàÜÊûêÔºå‰ª•Âèä 3) Âæû‰∏ÄÁ®ÆÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÊñπÊ≥ïËºïÈ¨ÜÂàáÊèõÂà∞Âè¶‰∏ÄÁ®ÆÔºåÂêåÊôÇÊèê‰æõÊ®°ÂûãÂíåÁ≥ªÁµ±ÊïàËÉΩÁöÑË¶ãËß£„ÄÇÊàëÂÄëÂú® AWS ‰º∫ÊúçÂô®‰∏ä‰ΩøÁî® PromSec Â°´ÂÖÖ SGCodeÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈÄèÈÅéÁµêÂêà LLM ÂíåÂÆâÂÖ®ÊÄßÂ∑•ÂÖ∑Ôºå‰ª•ÂèäËºïÈáèÁ¥öÁîüÊàêÂ∞çÊäóÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø‰æÜÊúÄ‰Ω≥ÂåñÊèêÁ§∫Ôºå‰ª•ÂÅµÊ∏¨Âíå‰øÆÂæ©Áî¢ÁîüÁ®ãÂºèÁ¢º‰∏≠ÁöÑÂÆâÂÖ®ÊÄßÊºèÊ¥ûÁöÑÊñπÊ≥ï„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåSGCode ‰ΩúÁÇ∫‰∏ÄÂÄãÂÖ¨Áî®Â∑•ÂÖ∑ÔºåÂú®Ê®°ÂûãÊïàÁî®„ÄÅÂÆâÂÖ®Á®ãÂºèÁ¢ºÁî¢ÁîüÂíåÁ≥ªÁµ±ÊàêÊú¨‰πãÈñìÁöÑÊ¨äË°°‰∏≠Áç≤ÂæóË¶ãËß£ÔºåÊòØÂØ¶Áî®ÁöÑ„ÄÇËàáÊèêÁ§∫ LLM Áõ∏ÊØîÔºåSGCode Âè™ÊúâÈÇäÈöõÊàêÊú¨„ÄÇSGCode ÂèØÂú® https://sgcode.codes/ ÂèñÂæó„ÄÇ

##### **Semantic Interoperability on Blockchain by Generating Smart Contracts Based on Knowledge Graphs**
2409.12171v1 by William Van Woensel, Oshani Seneviratne

Background: Health 3.0 allows decision making to be based on longitudinal
data from multiple institutions, from across the patient's healthcare journey.
In such a distributed setting, blockchain smart contracts can act as neutral
intermediaries to implement trustworthy decision making.
  Objective: In a distributed setting, transmitted data will be structured
using standards (such as HL7 FHIR) for semantic interoperability. In turn, the
smart contract will require interoperability with this standard, implement a
complex communication setup (e.g., using oracles), and be developed using
blockchain languages (e.g., Solidity). We propose the encoding of smart
contract logic using a high-level semantic Knowledge Graph, using concepts from
the domain standard. We then deploy this semantic KG on blockchain.
  Methods: Off-chain, a code generation pipeline compiles the KG into a
concrete smart contract, which is then deployed on-chain. Our pipeline targets
an intermediary bridge representation, which can be transpiled into a specific
blockchain language. Our choice avoids on-chain rule engines, with
unpredictable and likely higher computational cost; it is thus in line with the
economic rules of blockchain.
  Results: We applied our code generation approach to generate smart contracts
for 3 health insurance cases from Medicare. We discuss the suitability of our
approach - the need for a neutral intermediary - for a number of healthcare use
cases. Our evaluation finds that the generated contracts perform well in terms
of correctness and execution cost ("gas") on blockchain.
  Conclusions: We showed that it is feasible to automatically generate smart
contract code based on a semantic KG, in a way that respects the economic rules
of blockchain. Future work includes studying the use of Large Language Models
(LLM) in our approach, and evaluations on other blockchains.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöHealth 3.0 ÂÖÅË®±Ê±∫Á≠ñÂà∂ÂÆöÂü∫Êñº‰æÜËá™Â§öÂÄãÊ©üÊßãÁöÑÁ∏±ÂêëÊï∏ÊìöÔºå‰æÜËá™ÊÇ£ËÄÖÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ê≠∑Á®ã„ÄÇÂú®ÈÄôÁ®ÆÂàÜÂ∏ÉÂºèË®≠ÁΩÆ‰∏≠ÔºåÂçÄÂ°äÈèàÊô∫ËÉΩÂêàÁ¥ÑÂèØ‰ª•‰ΩúÁÇ∫‰∏≠Á´ãÁöÑ‰ª≤‰ªã‰æÜÂØ¶ÊñΩÂÄºÂæó‰ø°Ë≥¥ÁöÑÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÁõÆÊ®ôÔºöÂú®ÂàÜÂ∏ÉÂºèË®≠ÁΩÆ‰∏≠ÔºåÂÇ≥Ëº∏ÁöÑÊï∏ÊìöÂ∞á‰ΩøÁî®Ê®ôÊ∫ñÔºà‰æãÂ¶Ç HL7 FHIRÔºâÈÄ≤Ë°åÁµêÊßãÂåñÔºå‰ª•ÂØ¶ÁèæË™ûÁæ©‰∫íÊìç‰ΩúÊÄß„ÄÇÂèçÈÅé‰æÜÔºåÊô∫ËÉΩÂêàÁ¥ÑÂ∞áÈúÄË¶ÅËàáÊ≠§Ê®ôÊ∫ñ‰∫íÊìç‰ΩúÔºåÂØ¶ÊñΩË§áÈõúÁöÑÈÄö‰ø°Ë®≠ÁΩÆÔºà‰æãÂ¶ÇÔºå‰ΩøÁî®È†êË®ÄÊ©üÔºâÔºå‰∏¶‰ΩøÁî®ÂçÄÂ°äÈèàË™ûË®ÄÔºà‰æãÂ¶ÇÔºåSolidityÔºâÈñãÁôº„ÄÇÊàëÂÄëÊèêË≠∞‰ΩøÁî®‰æÜËá™È†òÂüüÊ®ôÊ∫ñÁöÑÊ¶ÇÂøµÔºå‰ΩøÁî®È´òÁ¥öË™ûÁæ©Áü•Ë≠òÂúñÂ∞çÊô∫ËÉΩÂêàÁ¥ÑÈÇèËºØÈÄ≤Ë°åÁ∑®Á¢º„ÄÇÁÑ∂ÂæåÊàëÂÄëÂ∞áÈÄôÂÄãË™ûÁæ©Áü•Ë≠òÂúñÈÉ®ÁΩ≤Âú®ÂçÄÂ°äÈèà‰∏ä„ÄÇÊñπÊ≥ïÔºöÈèà‰∏ãÔºå‰∏ÄÂÄã‰ª£Á¢ºÁîüÊàêÁÆ°ÈÅìÂ∞áÁü•Ë≠òÂúñÁ∑®Ë≠ØÊàêÂÖ∑È´îÁöÑÊô∫ËÉΩÂêàÁ¥ÑÔºåÁÑ∂ÂæåÂ∞áÂÖ∂ÈÉ®ÁΩ≤Âà∞Èèà‰∏ä„ÄÇÊàëÂÄëÁöÑÁÆ°ÈÅìÈáùÂ∞ç‰∏≠ÈñìÊ©ãÊé•Ë°®Á§∫ÔºåÂèØ‰ª•ËΩâË≠ØÊàêÁâπÂÆöÁöÑÂçÄÂ°äÈèàË™ûË®Ä„ÄÇÊàëÂÄëÁöÑÈÅ∏ÊìáÈÅøÂÖç‰∫ÜÈèà‰∏äË¶èÂâáÂºïÊìéÔºåÂÖ∂‰∏çÂèØÈ†êÊ∏¨‰∏îÂèØËÉΩË®àÁÆóÊàêÊú¨Êõ¥È´òÔºõÂõ†Ê≠§ÔºåÂÆÉÁ¨¶ÂêàÂçÄÂ°äÈèàÁöÑÁ∂ìÊøüË¶èÂâá„ÄÇÁµêÊûúÔºöÊàëÂÄëÊáâÁî®ÊàëÂÄëÁöÑ‰ª£Á¢ºÁîüÊàêÊñπÊ≥ï‰æÜÁîüÊàê‰æÜËá™ Medicare ÁöÑ 3 ÂÄãÂÅ•Â∫∑‰øùÈö™Ê°à‰æãÁöÑÊô∫ËÉΩÂêàÁ¥Ñ„ÄÇÊàëÂÄëË®éË´ñ‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÁöÑÈÅ©Áî®ÊÄß‚Äî‚ÄîÂ∞ç‰∏≠Á´ã‰ª≤‰ªãÁöÑÈúÄÊ±Ç‚Äî‚ÄîÂ∞çÊñºË®±Â§öÈÜ´ÁôÇ‰øùÂÅ•Áî®‰æã„ÄÇÊàëÂÄëÁöÑË©ï‰º∞ÁôºÁèæÔºåÁîüÊàêÁöÑÂêàÁ¥ÑÂú®ÂçÄÂ°äÈèà‰∏äÁöÑÊ≠£Á¢∫ÊÄßÂíåÂü∑Ë°åÊàêÊú¨Ôºà‚Äúgas‚ÄùÔºâÊñπÈù¢Ë°®ÁèæËâØÂ•Ω„ÄÇÁµêË´ñÔºöÊàëÂÄëË°®ÊòéÔºå‰ª•Á¨¶ÂêàÂçÄÂ°äÈèàÁ∂ìÊøüË¶èÂâáÁöÑÊñπÂºèËá™ÂãïÁîüÊàêÂü∫ÊñºË™ûÁæ©Áü•Ë≠òÂúñÁöÑÊô∫ËÉΩÂêàÁ¥Ñ‰ª£Á¢ºÊòØÂèØË°åÁöÑ„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂ÂåÖÊã¨Á†îÁ©∂ÊàëÂÄëÁöÑÊñπÊ≥ï‰∏≠‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•ÂèäÂ∞çÂÖ∂‰ªñÂçÄÂ°äÈèàÁöÑË©ï‰º∞„ÄÇ</paragraph>

##### **Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model**
2409.07088v1 by Daehee Kim, Deokhyung Kang, Sangwon Ryu, Gary Geunbae Lee

Knowledge Graph-to-Text (G2T) generation involves verbalizing structured
knowledge graphs into natural language text. Recent advancements in Pretrained
Language Models (PLMs) have improved G2T performance, but their effectiveness
depends on datasets with precise graph-text alignment. However, the scarcity of
high-quality, general-domain G2T generation datasets restricts progress in the
general-domain G2T generation research. To address this issue, we introduce
Wikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T
dataset generated using a novel method that leverages Large Language Model
(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain
graph-text pairs, offers high graph-text consistency without relying on
external ontologies. Experimental results demonstrate that PLM fine-tuned on
WikiOFGraph outperforms those trained on other datasets across various
evaluation metrics. Our method proves to be a scalable and effective solution
for generating high-quality G2T data, significantly advancing the field of G2T
generation.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠úÂà∞ÊñáÂ≠ó (G2T) ÁîüÊàêÊ∂âÂèäÂ∞áÁµêÊßãÂåñÁü•Ë≠òÂúñË≠úË°®ÈÅîÁÇ∫Ëá™ÁÑ∂Ë™ûË®ÄÊñáÂ≠ó„ÄÇÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊîπÂñÑ‰∫Ü G2T ÁöÑÊïàËÉΩÔºå‰ΩÜÂÖ∂ÊúâÊïàÊÄßÂèñÊ±∫ÊñºÂÖ∑ÊúâÁ≤æÁ¢∫ÂúñÂΩ¢ÊñáÂ≠óÂ∞çÈΩäÁöÑË≥áÊñôÈõÜ„ÄÇÁÑ∂ËÄåÔºåÈ´òÂìÅË≥™„ÄÅ‰∏ÄËà¨È†òÂüü G2T ÁîüÊàêË≥áÊñôÈõÜÁöÑÁ®ÄÂ∞ëÊÄßÈôêÂà∂‰∫Ü‰∏ÄËà¨È†òÂüü G2T ÁîüÊàêÁ†îÁ©∂ÁöÑÈÄ≤Â±ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÁ∂≠Âü∫ÁôæÁßëÊú¨‰ΩìÂÖçË≤ªÂúñÂΩ¢ÊñáÂ≠óË≥áÊñôÈõÜ (WikiOFGraph)ÔºåÈÄôÊòØ‰∏ÄÂÄã‰ΩøÁî®Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âíå Data-QuestEval ÁöÑÊñ∞ÊñπÊ≥ïÁîüÊàêÁöÑÊñ∞Â§ßÂûã G2T Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÈÄôÂÄãÊñ∞Ë≥áÊñôÈõÜÂåÖÂê´ 585 Ëê¨ÂÄã‰∏ÄËà¨È†òÂüüÁöÑÂúñÂΩ¢ÊñáÂ≠óÂ∞çÔºåÊèê‰æõÈ´òÂúñÂΩ¢ÊñáÂ≠ó‰∏ÄËá¥ÊÄßÔºåËÄå‰∏ç‰æùË≥¥ÊñºÂ§ñÈÉ®Êú¨‰Ωì„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂú® WikiOFGraph ‰∏äÂæÆË™øÁöÑ PLM Âú®ÂêÑÁ®ÆË©ï‰º∞ÊåáÊ®ô‰∏äÂÑ™ÊñºÂú®ÂÖ∂‰ªñË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑ PLM„ÄÇÊàëÂÄëÁöÑÈÄôÂÄãÊñπÊ≥ïË¢´Ë≠âÊòéÊòØ‰∏ÄÂÄãÂèØÊì¥ÂÖÖ‰∏îÊúâÊïàÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºÁîüÊàêÈ´òÂìÅË≥™ÁöÑ G2T Ë≥áÊñôÔºåÈ°ØËëóÊé®Âãï‰∫Ü G2T ÁîüÊàêÈ†òÂüüÁöÑÁôºÂ±ï„ÄÇ

##### **Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence**
2409.07064v1 by Jiun-Ting Li, Bi-Cheng Yan, Tien-Hong Lo, Yi-Cheng Wang, Yung-Chang Hsu, Berlin Chen

Automated speaking assessment in conversation tests (ASAC) aims to evaluate
the overall speaking proficiency of an L2 (second-language) speaker in a
setting where an interlocutor interacts with one or more candidates. Although
prior ASAC approaches have shown promising performance on their respective
datasets, there is still a dearth of research specifically focused on
incorporating the coherence of the logical flow within a conversation into the
grading model. To address this critical challenge, we propose a hierarchical
graph model that aptly incorporates both broad inter-response interactions
(e.g., discourse relations) and nuanced semantic information (e.g., semantic
words and speaker intents), which is subsequently fused with contextual
information for the final prediction. Extensive experimental results on the
NICT-JLE benchmark dataset suggest that our proposed modeling approach can
yield considerable improvements in prediction accuracy with respect to various
assessment metrics, as compared to some strong baselines. This also sheds light
on the importance of investigating coherence-related facets of spoken responses
in ASAC.

ÊëòË¶ÅÔºöËá™ÂãïÂ∞çË©±Ë©ïÈáè‰∏≠ÁöÑËá™ÂãïÂåñÂè£Ë™™Ë©ïÈáèÔºàASACÔºâÊó®Âú®Ë©ï‰º∞ L2ÔºàÁ¨¨‰∫åË™ûË®ÄÔºâË©±ËÄÖÂú®Ëàá‰∏Ä‰ΩçÊàñÂ§ö‰ΩçÊáâË©¶ËÄÖ‰∫íÂãïÁöÑÁí∞Â¢É‰∏≠ÔºåÊï¥È´îÁöÑÂè£Ë™™ËÉΩÂäõ„ÄÇÂÑòÁÆ°ÂÖàÂâçÁöÑ ASAC ÊñπÊ≥ïÂú®ÂÖ∂ÂêÑËá™ÁöÑË≥áÊñôÈõÜ‰∏äÂ±ïÁèæÂá∫ÊúâÂâçÈÄîÁöÑË°®ÁèæÔºå‰ΩÜ‰ªçÁº∫‰πèÂ∞àÊ≥®ÊñºÂ∞áÂ∞çË©±‰∏≠ÈÇèËºØÊµÅÁ®ãÁöÑÈÄ£Ë≤´ÊÄßÁ¥çÂÖ•Ë©ïÂàÜÊ®°ÂûãÁöÑÁ†îÁ©∂„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÈ†ÖÈóúÈçµÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈöéÂ±§ÂºèÂúñÂΩ¢Ê®°ÂûãÔºåÂÆÉÈÅ©Áï∂Âú∞ÁµêÂêà‰∫ÜÂª£Ê≥õÁöÑÂõûÊáâÈñì‰∫íÂãïÔºà‰æãÂ¶ÇÔºöË™ûÁØáÈóú‰øÇÔºâÂíåÁ¥∞ÂæÆÁöÑË™ûÁæ©Ë≥áË®äÔºà‰æãÂ¶ÇÔºöË™ûÁæ©Â≠óË©ûÂíåË™™Ë©±ËÄÖÊÑèÂúñÔºâÔºåÈö®ÂæåËàáËÑàÁµ°Ë≥áË®äËûçÂêàÔºå‰ª•ÈÄ≤Ë°åÊúÄÁµÇÈ†êÊ∏¨„ÄÇÂú® NICT-JLE Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàá‰∏Ä‰∫õÂº∑Â§ßÁöÑÂü∫Ê∫ñÁ∑öÁõ∏ÊØîÔºåÊàëÂÄëÊèêÂá∫ÁöÑÂª∫Ê®°ÊñπÊ≥ïÂèØ‰ª•È°ØËëóÊèêÂçáÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÔºåÁâπÂà•ÊòØÂú®ÂêÑÁ®ÆË©ïÈáèÊåáÊ®ôÊñπÈù¢„ÄÇÈÄô‰πüÈó°Êòé‰∫ÜÂú® ASAC ‰∏≠Êé¢Ë®éÂè£Ë™ûÂõûÊáâÁöÑÈÄ£Ë≤´ÊÄßÁõ∏ÈóúÈù¢ÂêëÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **FreeRide: Harvesting Bubbles in Pipeline Parallelism**
2409.06941v1 by Jiashu Zhang, Zihan Pan, Molly, Xu, Khuzaima Daudjee, Sihang Liu

The occurrence of bubbles in pipeline parallelism is an inherent limitation
that can account for more than 40% of the large language model (LLM) training
time and is one of the main reasons for the underutilization of GPU resources
in LLM training. Harvesting these bubbles for GPU side tasks can increase
resource utilization and reduce training costs but comes with challenges.
First, because bubbles are discontinuous with various shapes, programming side
tasks becomes difficult while requiring excessive engineering effort. Second, a
side task can compete with pipeline training for GPU resources and incur
significant overhead. To address these challenges, we propose FreeRide, a
system designed to harvest bubbles in pipeline parallelism for side tasks.
FreeRide provides programmers with interfaces to implement side tasks easily,
manages bubbles and side tasks during pipeline training, and controls access to
GPU resources by side tasks to reduce overhead. We demonstrate that FreeRide
achieves 7.8% average cost savings with a negligible overhead of about 1% in
training LLMs while serving model training, graph analytics, and image
processing side tasks.

ÊëòË¶ÅÔºöÁÆ°Á∑öÂπ≥Ë°åËôïÁêÜ‰∏≠ÁôºÁîüÊ∞£Ê≥°ÊòØ‰∏ÄÂÄãÂõ∫ÊúâÈôêÂà∂ÔºåÂèØËÉΩ‰ΩîÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ë®ìÁ∑¥ÊôÇÈñìÁöÑ 40% ‰ª•‰∏äÔºå‰∏¶‰∏îÊòØ LLM Ë®ìÁ∑¥‰∏≠ GPU Ë≥áÊ∫êÂà©Áî®‰∏çË∂≥ÁöÑ‰∏ªË¶ÅÂéüÂõ†‰πã‰∏Ä„ÄÇÊî∂ÈõÜÈÄô‰∫õÊ∞£Ê≥°‰ª•ÈÄ≤Ë°å GPU ÂÅ¥Èù¢‰ªªÂãôÂèØ‰ª•ÊèêÈ´òË≥áÊ∫êÂà©Áî®Áéá‰∏¶Èôç‰ΩéË®ìÁ∑¥ÊàêÊú¨Ôºå‰ΩÜÊúÉÂ∏∂‰æÜÊåëÊà∞„ÄÇÈ¶ñÂÖàÔºåÁî±ÊñºÊ∞£Ê≥°ÊòØ‰∏çÈÄ£Á∫åÁöÑ‰∏îÂΩ¢ÁãÄÂêÑÁï∞ÔºåÂõ†Ê≠§Á∑®ÂØ´Á®ãÂºèÂÅ¥Èù¢‰ªªÂãôËÆäÂæóÂõ∞Èõ£ÔºåÂêåÊôÇÈúÄË¶ÅÈÅéÂ§öÁöÑÂ∑•Á®ãÂ∑•‰Ωú„ÄÇÂÖ∂Ê¨°ÔºåÂÅ¥Èù¢‰ªªÂãôÂèØËÉΩÊúÉËàáÁÆ°Á∑öË®ìÁ∑¥Á´∂Áà≠ GPU Ë≥áÊ∫êÔºå‰∏¶ÈÄ†ÊàêÈ°ØËëóÁöÑÈñãÈä∑„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü FreeRideÔºåÈÄôÊòØ‰∏ÄÂÄãÊó®Âú®Êî∂ÈõÜÁÆ°Á∑öÂπ≥Ë°åËôïÁêÜ‰∏≠ÁöÑÊ∞£Ê≥°‰ª•ÈÄ≤Ë°åÂÅ¥Èù¢‰ªªÂãôÁöÑÁ≥ªÁµ±„ÄÇFreeRide ÁÇ∫Á®ãÂºèË®≠Ë®àÂ∏´Êèê‰æõ‰∫ÜËºïÈ¨ÜÂØ¶‰ΩúÂÅ¥Èù¢‰ªªÂãôÁöÑ‰ªãÈù¢ÔºåÂú®ÁÆ°Á∑öË®ìÁ∑¥ÊúüÈñìÁÆ°ÁêÜÊ∞£Ê≥°ÂíåÂÅ¥Èù¢‰ªªÂãôÔºå‰∏¶ÊéßÂà∂ÂÅ¥Èù¢‰ªªÂãôÂ∞ç GPU Ë≥áÊ∫êÁöÑÂ≠òÂèñ‰ª•Ê∏õÂ∞ëÈñãÈä∑„ÄÇÊàëÂÄëË≠âÊòé FreeRide Âú®Ë®ìÁ∑¥ LLM ÊôÇÂèØÁØÄÁúÅ 7.8% ÁöÑÂπ≥ÂùáÊàêÊú¨ÔºåÂêåÊôÇÂú®Âü∑Ë°åÊ®°ÂûãË®ìÁ∑¥„ÄÅÂúñÂΩ¢ÂàÜÊûêÂíåÂΩ±ÂÉèËôïÁêÜÂÅ¥Èù¢‰ªªÂãôÊôÇÔºåÈñãÈä∑ÂèØÂøΩÁï•‰∏çË®àÔºåÁ¥ÑÁÇ∫ 1%„ÄÇ

##### **Generative Hierarchical Materials Search**
2409.06762v1 by Sherry Yang, Simon Batzner, Ruiqi Gao, Muratahan Aykol, Alexander L. Gaunt, Brendan McMorrow, Danilo J. Rezende, Dale Schuurmans, Igor Mordatch, Ekin D. Cubuk

Generative models trained at scale can now produce text, video, and more
recently, scientific data such as crystal structures. In applications of
generative approaches to materials science, and in particular to crystal
structures, the guidance from the domain expert in the form of high-level
instructions can be essential for an automated system to output candidate
crystals that are viable for downstream research. In this work, we formulate
end-to-end language-to-structure generation as a multi-objective optimization
problem, and propose Generative Hierarchical Materials Search (GenMS) for
controllable generation of crystal structures. GenMS consists of (1) a language
model that takes high-level natural language as input and generates
intermediate textual information about a crystal (e.g., chemical formulae), and
(2) a diffusion model that takes intermediate information as input and
generates low-level continuous value crystal structures. GenMS additionally
uses a graph neural network to predict properties (e.g., formation energy) from
the generated crystal structures. During inference, GenMS leverages all three
components to conduct a forward tree search over the space of possible
structures. Experiments show that GenMS outperforms other alternatives of
directly using language models to generate structures both in satisfying user
request and in generating low-energy structures. We confirm that GenMS is able
to generate common crystal structures such as double perovskites, or spinels,
solely from natural language input, and hence can form the foundation for more
complex structure generation in near future.

ÊëòË¶ÅÔºö<paragraph>Â§ßË¶èÊ®°Ë®ìÁ∑¥ÁöÑÁîüÊàêÊ®°ÂûãÁèæÂú®ÂèØ‰ª•Áî¢ÁîüÊñáÂ≠ó„ÄÅÂΩ±ÁâáÔºå‰ª•ÂèäÊúÄËøëÁöÑÁßëÂ≠∏Ë≥áÊñôÔºå‰æãÂ¶ÇÊô∂È´îÁµêÊßã„ÄÇÂú®ÁîüÊàêÊñπÊ≥ïÊáâÁî®ÊñºÊùêÊñôÁßëÂ≠∏ÔºåÂ∞§ÂÖ∂ÊòØÊô∂È´îÁµêÊßãÊôÇÔºåÈ†òÂüüÂ∞àÂÆ∂ÁöÑÊåáÂ∞éÔºå‰ª•È´òÈöéÊåá‰ª§ÁöÑÂΩ¢ÂºèÔºåÂ∞çÊñºËá™ÂãïÂåñÁ≥ªÁµ±Ëº∏Âá∫ÂèØË°åÊñº‰∏ãÊ∏∏Á†îÁ©∂ÁöÑÂÄôÈÅ∏Êô∂È´îËá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ∞áÁ´ØÂ∞çÁ´ØË™ûË®ÄÂà∞ÁµêÊßãÁîüÊàêÂà∂ÂÆöÁÇ∫Â§öÁõÆÊ®ôÊúÄ‰Ω≥ÂåñÂïèÈ°åÔºå‰∏¶ÊèêÂá∫ÁîüÊàêÂàÜÂ±§ÊùêÊñôÊêúÂ∞ã (GenMS) ‰ª•ÊéßÂà∂Êô∂È´îÁµêÊßãÁöÑÁîüÊàê„ÄÇGenMS ÂåÖÂê´ (1) ‰∏ÄÂÄãË™ûË®ÄÊ®°ÂûãÔºåÂÆÉÂ∞áÈ´òÈöéËá™ÁÑ∂Ë™ûË®Ä‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰∏¶ÁîüÊàêÊúâÈóúÊô∂È´îÁöÑ‰∏≠ÈñìÊñáÂ≠óË≥áË®äÔºà‰æãÂ¶ÇÂåñÂ≠∏ÂÖ¨ÂºèÔºâÔºå‰ª•Âèä (2) ‰∏ÄÂÄãÊì¥Êï£Ê®°ÂûãÔºåÂÆÉÂ∞á‰∏≠ÈñìË≥áË®ä‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰∏¶ÁîüÊàê‰ΩéÈöéÈÄ£Á∫åÂÄºÊô∂È´îÁµêÊßã„ÄÇGenMS Ê≠§Â§ñ‰ΩøÁî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂæûÁîüÊàêÁöÑÊô∂È´îÁµêÊßãÈ†êÊ∏¨Â±¨ÊÄßÔºà‰æãÂ¶ÇÂΩ¢ÊàêËÉΩÔºâ„ÄÇÂú®Êé®ÁêÜÊúüÈñìÔºåGenMS Âà©Áî®ÊâÄÊúâ‰∏âÂÄãÁµÑ‰ª∂Â∞çÂèØËÉΩÁöÑÁµêÊßãÁ©∫ÈñìÈÄ≤Ë°åÂâçÂêëÊ®πÁãÄÊêúÂ∞ã„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåGenMS ÂÑ™ÊñºÁõ¥Êé•‰ΩøÁî®Ë™ûË®ÄÊ®°Âûã‰æÜÁîüÊàêÁµêÊßãÁöÑÂÖ∂‰ªñÊõø‰ª£ÊñπÊ°àÔºåÁÑ°Ë´ñÊòØÂú®ÊªøË∂≥‰ΩøÁî®ËÄÖË¶ÅÊ±ÇÊàñÁîüÊàê‰ΩéËÉΩÁµêÊßãÊñπÈù¢„ÄÇÊàëÂÄëÁ¢∫Ë™ç GenMS ËÉΩÂ§†ÂÉÖÂæûËá™ÁÑ∂Ë™ûË®ÄËº∏ÂÖ•ÁîüÊàêÂ∏∏Ë¶ãÁöÑÊô∂È´îÁµêÊßãÔºå‰æãÂ¶ÇÈõôÈà£Èà¶Á§¶ÊàñÂ∞ñÊô∂Áü≥ÔºåÂõ†Ê≠§ÂèØ‰ª•Âú®‰∏ç‰πÖÁöÑÂ∞á‰æÜÂΩ¢ÊàêÊõ¥Ë§áÈõúÁµêÊßãÁîüÊàêÁöÑÂü∫Á§é„ÄÇ</paragraph>

##### **Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization**
2409.06433v1 by Gollam Rabby, S√∂ren Auer, Jennifer D'Souza, Allard Oelen

The increasing amount of published scholarly articles, exceeding 2.5 million
yearly, raises the challenge for researchers in following scientific progress.
Integrating the contributions from scholarly articles into a novel type of
cognitive knowledge graph (CKG) will be a crucial element for accessing and
organizing scholarly knowledge, surpassing the insights provided by titles and
abstracts. This research focuses on effectively conveying structured scholarly
knowledge by utilizing large language models (LLMs) to categorize scholarly
articles and describe their contributions in a structured and comparable
manner. While previous studies explored language models within specific
research domains, the extensive domain-independent knowledge captured by LLMs
offers a substantial opportunity for generating structured contribution
descriptions as CKGs. Additionally, LLMs offer customizable pathways through
prompt engineering or fine-tuning, thus facilitating to leveraging of smaller
LLMs known for their efficiency, cost-effectiveness, and environmental
considerations. Our methodology involves harnessing LLM knowledge, and
complementing it with domain expert-verified scholarly data sourced from a CKG.
This strategic fusion significantly enhances LLM performance, especially in
tasks like scholarly article categorization and predicate recommendation. Our
method involves fine-tuning LLMs with CKG knowledge and additionally injecting
knowledge from a CKG with a novel prompting technique significantly increasing
the accuracy of scholarly knowledge extraction. We integrated our approach in
the Open Research Knowledge Graph (ORKG), thus enabling precise access to
organized scholarly knowledge, crucially benefiting domain-independent
scholarly knowledge exchange and dissemination among policymakers, industrial
practitioners, and the general public.

ÊëòË¶ÅÔºö<paragraph>ÊØèÂπ¥Ë∂ÖÈÅé 250 Ëê¨ÁØáÁöÑÂ≠∏Ë°ìÊñáÁ´†ÁôºË°®Êï∏ÈáèÊåÅÁ∫åÂ¢ûÂä†ÔºåÂ∞çÁ†îÁ©∂‰∫∫Âì°ËøΩËπ§ÁßëÂ≠∏ÈÄ≤Â±ïÂ∏∂‰æÜÊåëÊà∞„ÄÇÂ∞áÂ≠∏Ë°ìÊñáÁ´†ÁöÑË≤¢ÁçªÊï¥ÂêàÂà∞Êñ∞ÂûãÊÖãÁöÑË™çÁü•Áü•Ë≠òÂúñË≠ú (CKG) ‰∏≠ÔºåÂ∞áÊàêÁÇ∫Â≠òÂèñÂíåÁµÑÁπîÂ≠∏Ë°ìÁü•Ë≠òÁöÑÈóúÈçµË¶ÅÁ¥†ÔºåË∂ÖË∂äÊ®ôÈ°åÂíåÊëòË¶ÅÊèê‰æõÁöÑË¶ãËß£„ÄÇÊú¨Á†îÁ©∂Â∞àÊ≥®ÊñºÊúâÊïàÂÇ≥ÈÅîÁµêÊßãÂåñÁöÑÂ≠∏Ë°ìÁü•Ë≠òÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂàÜÈ°ûÂ≠∏Ë°ìÊñáÁ´†Ôºå‰∏¶‰ª•ÁµêÊßãÂåñ‰∏îÂèØÊØîËºÉÁöÑÂΩ¢ÂºèÊèèËø∞ÂÖ∂Ë≤¢Áçª„ÄÇÈõñÁÑ∂ÂÖàÂâçÁöÑÁ†îÁ©∂Âú®ÁâπÂÆöÁ†îÁ©∂È†òÂüü‰∏≠Êé¢Á¥¢Ë™ûË®ÄÊ®°ÂûãÔºå‰ΩÜ LLM ÊçïÊçâÂà∞ÁöÑÂª£Ê≥õÈ†òÂüüÁÑ°ÈóúÁü•Ë≠òÔºåÁÇ∫Áî¢ÁîüÁµêÊßãÂåñÁöÑË≤¢ÁçªÊèèËø∞Êèê‰æõ‰∫ÜÂØ¶Ë≥™Ê©üÊúÉÔºå‰æãÂ¶Ç CKG„ÄÇÊ≠§Â§ñÔºåLLM ÈÄèÈÅéÊèêÁ§∫Â∑•Á®ãÊàñÂæÆË™øÊèê‰æõÂèØËá™Ë®ÇË∑ØÂæëÔºåÂæûËÄå‰øÉÈÄ≤Âà©Áî®‰ª•ÊïàÁéá„ÄÅÊàêÊú¨ÊïàÁõäÂíåÁí∞Â¢ÉËÄÉÈáèËÅûÂêçÁöÑËºÉÂ∞èÂûã LLM„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨Âà©Áî® LLM Áü•Ë≠òÔºå‰∏¶ÈÄèÈÅé CKG ‰æÜÊ∫êÁöÑÈ†òÂüüÂ∞àÂÆ∂È©óË≠âÂ≠∏Ë°ìË≥áÊñô‰æÜË£úÂÖÖ„ÄÇÈÄôÁ®ÆÁ≠ñÁï•ËûçÂêàÈ°ØËëóÊèêÂçá LLM ÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®Â≠∏Ë°ìÊñáÁ´†ÂàÜÈ°ûÂíåË¨ÇË©ûÊé®Ëñ¶Á≠â‰ªªÂãô‰∏≠„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÂåÖÊã¨‰ª• CKG Áü•Ë≠òÂæÆË™ø LLMÔºå‰∏¶ÈÄèÈÅéÊñ∞ÁöÑÊèêÁ§∫ÊäÄË°ìÊ≥®ÂÖ• CKG ÁöÑÁü•Ë≠òÔºåÈ°ØËëóÊèêÂçáÂ≠∏Ë°ìÁü•Ë≠òËêÉÂèñÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÂÅöÊ≥ïÊï¥ÂêàÂà∞ÈñãÊîæÁ†îÁ©∂Áü•Ë≠òÂúñË≠ú (ORKG) ‰∏≠ÔºåÂæûËÄåËÉΩÁ≤æÊ∫ñÂ≠òÂèñÂ∑≤ÁµÑÁπîÁöÑÂ≠∏Ë°ìÁü•Ë≠òÔºåÈÄôÂ∞çÊîøÁ≠ñÂà∂ÂÆöËÄÖ„ÄÅÁî¢Ê•≠ÂæûÊ•≠‰∫∫Âì°Âíå‰∏ÄËà¨Â§ßÁúæ‰πãÈñìÁöÑÈ†òÂüüÁÑ°ÈóúÂ≠∏Ë°ìÁü•Ë≠ò‰∫§ÊµÅÂíåÂÇ≥Êí≠Ëá≥ÈóúÈáçË¶Å„ÄÇ</paragraph>

##### **TopoChat: Enhancing Topological Materials Retrieval With Large Language Model and Multi-Source Knowledge**
2409.13732v1 by HuangChao Xu, Baohua Zhang, Zhong Jin, Tiannian Zhu, Quansheng Wu, Hongming Weng

Large language models (LLMs), such as ChatGPT, have demonstrated impressive
performance in the text generation task, showing the ability to understand and
respond to complex instructions. However, the performance of naive LLMs in
speciffc domains is limited due to the scarcity of domain-speciffc corpora and
specialized training. Moreover, training a specialized large-scale model
necessitates signiffcant hardware resources, which restricts researchers from
leveraging such models to drive advances. Hence, it is crucial to further
improve and optimize LLMs to meet speciffc domain demands and enhance their
scalability. Based on the condensed matter data center, we establish a material
knowledge graph (MaterialsKG) and integrate it with literature. Using large
language models and prompt learning, we develop a specialized dialogue system
for topological materials called TopoChat. Compared to naive LLMs, TopoChat
exhibits superior performance in structural and property querying, material
recommendation, and complex relational reasoning. This system enables efffcient
and precise retrieval of information and facilitates knowledge interaction,
thereby encouraging the advancement on the ffeld of condensed matter materials.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÔºå‰æãÂ¶Ç ChatGPTÔºåÂú®ÊñáÊú¨ÁîüÊàê‰ªªÂãô‰∏≠Â±ïÁèæ‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑË°®ÁèæÔºåÂ±ïÁèæ‰∫ÜÁêÜËß£ÂíåÂõûÊáâË§áÈõúÊåá‰ª§ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑË™ûÊñôÂ∫´ÂíåÂ∞àÊ•≠Ë®ìÁ∑¥ÔºåÂ§©ÁúüÁöÑ LLM Âú®ÁâπÂÆöÈ†òÂüüÁöÑË°®ÁèæÂèóÂà∞ÈôêÂà∂„ÄÇÊ≠§Â§ñÔºåË®ìÁ∑¥‰∏ÄÂÄãÂ∞àÊ•≠ÁöÑÂ§ßË¶èÊ®°Ê®°ÂûãÈúÄË¶ÅÂ§ßÈáèÁöÑÁ°¨È´îË≥áÊ∫êÔºåÈÄôÈôêÂà∂‰∫ÜÁ†îÁ©∂‰∫∫Âì°Âà©Áî®ÈÄô‰∫õÊ®°Âûã‰æÜÊé®ÂãïÈÄ≤Â±ï„ÄÇÂõ†Ê≠§ÔºåÈÄ≤‰∏ÄÊ≠•ÊîπÈÄ≤ÂíåÂÑ™Âåñ LLM ‰ª•ÊªøË∂≥ÁâπÂÆöÈ†òÂüüÁöÑÈúÄÊ±Ç‰∏¶Â¢ûÂº∑ÂÖ∂ÂèØÊì¥ÂÖÖÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÂü∫ÊñºÂáùËÅöÊÖãË≥áÊñô‰∏≠ÂøÉÔºåÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÊùêÊñôÁü•Ë≠òÂúñË≠úÔºàMaterialsKGÔºâÔºå‰∏¶Â∞áÂÖ∂ËàáÊñáÁçªÊï¥Âêà„ÄÇ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂíåÊèêÁ§∫Â≠∏ÁøíÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ TopoChat ÁöÑÊãìÊí≤ÊùêÊñôÂ∞àÁî®Â∞çË©±Á≥ªÁµ±„ÄÇËàáÂ§©ÁúüÁöÑ LLM Áõ∏ÊØîÔºåTopoChat Âú®ÁµêÊßãÂíåÂ±¨ÊÄßÊü•Ë©¢„ÄÅÊùêÊñôÊé®Ëñ¶ÂíåË§áÈõúÈóú‰øÇÊé®ÁêÜÊñπÈù¢Ë°®ÁèæÂá∫ÂÑ™Áï∞ÁöÑÊÄßËÉΩ„ÄÇË©≤Á≥ªÁµ±ËÉΩÂ§†ÊúâÊïà‰∏îÊ∫ñÁ¢∫Âú∞Ê™¢Á¥¢Ë≥áË®äÔºå‰∏¶‰øÉÈÄ≤Áü•Ë≠ò‰∫íÂãïÔºåÂæûËÄå‰øÉÈÄ≤ÂáùËÅöÊÖãÊùêÊñôÈ†òÂüüÁöÑÈÄ≤Ê≠•„ÄÇ

##### **KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation**
2409.13731v3 by Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou

The recently developed retrieval-augmented generation (RAG) technology has
enabled the efficient construction of domain-specific applications. However, it
also has limitations, including the gap between vector similarity and the
relevance of knowledge reasoning, as well as insensitivity to knowledge logic,
such as numerical values, temporal relations, expert rules, and others, which
hinder the effectiveness of professional knowledge services. In this work, we
introduce a professional domain knowledge service framework called Knowledge
Augmented Generation (KAG). KAG is designed to address the aforementioned
challenges with the motivation of making full use of the advantages of
knowledge graph(KG) and vector retrieval, and to improve generation and
reasoning performance by bidirectionally enhancing large language models (LLMs)
and KGs through five key aspects: (1) LLM-friendly knowledge representation,
(2) mutual-indexing between knowledge graphs and original chunks, (3)
logical-form-guided hybrid reasoning engine, (4) knowledge alignment with
semantic reasoning, and (5) model capability enhancement for KAG. We compared
KAG with existing RAG methods in multihop question answering and found that it
significantly outperforms state-of-theart methods, achieving a relative
improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We
have successfully applied KAG to two professional knowledge Q&A tasks of Ant
Group, including E-Government Q&A and E-Health Q&A, achieving significant
improvement in professionalism compared to RAG methods.

ÊëòË¶ÅÔºöÊúÄËøëÈñãÁôºÁöÑÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊäÄË°ìÂ∑≤ËÉΩÊúâÊïàÂª∫ÊßãÁâπÂÆöÈ†òÂüüÁöÑÊáâÁî®Á®ãÂºè„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰πüÊúâ‰∏Ä‰∫õÈôêÂà∂ÔºåÂåÖÊã¨ÂêëÈáèÁõ∏‰ººÂ∫¶ËàáÁü•Ë≠òÊé®ÁêÜÁõ∏ÈóúÊÄß‰πãÈñìÁöÑÂ∑ÆË∑ùÔºå‰ª•ÂèäÂ∞çÁü•Ë≠òÈÇèËºØÁöÑ‰∏çÊïèÊÑüÊÄßÔºå‰æãÂ¶ÇÊï∏Â≠óÂÄº„ÄÅÊôÇÈñìÈóú‰øÇ„ÄÅÂ∞àÂÆ∂Ë¶èÂâáÁ≠âÔºåÈÄô‰∫õÈôêÂà∂ÈòªÁ§ô‰∫ÜÂ∞àÊ•≠Áü•Ë≠òÊúçÂãôÁöÑÊúâÊïàÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÁ®±ÁÇ∫Áü•Ë≠òÂ¢ûÂº∑ÁîüÊàê (KAG) ÁöÑÂ∞àÊ•≠È†òÂüüÁü•Ë≠òÊúçÂãôÊû∂Êßã„ÄÇKAG Êó®Âú®Ëß£Ê±∫‰∏äËø∞ÊåëÊà∞ÔºåÁõÆÁöÑÊòØÂÖÖÂàÜÂà©Áî®Áü•Ë≠òÂúñ (KG) ÂíåÂêëÈáèÊ™¢Á¥¢ÁöÑÂÑ™Âã¢Ôºå‰∏¶ÈÄöÈÅé‰ª•‰∏ã‰∫îÂÄãÈóúÈçµÊñπÈù¢ÈõôÂêëÂ¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âíå KG ‰æÜÊîπÂñÑÁîüÊàêÂíåÊé®ÁêÜÊïàËÉΩÔºö(1) LLM ÂèãÂñÑÁöÑÁü•Ë≠òË°®Á§∫Ôºå(2) Áü•Ë≠òÂúñËàáÂéüÂßãÂçÄÂ°ä‰πãÈñìÁöÑÁõ∏‰∫íÁ¥¢ÂºïÔºå(3) ÈÇèËºØÂΩ¢ÂºèÂºïÂ∞éÁöÑÊ∑∑ÂêàÊé®ÁêÜÂºïÊìéÔºå(4) ËàáË™ûÁæ©Êé®ÁêÜÁöÑÁü•Ë≠òÂ∞çÈΩäÔºå‰ª•Âèä (5) KAG ÁöÑÊ®°ÂûãÂäüËÉΩÂ¢ûÂº∑„ÄÇÊàëÂÄëÂú®Â§öË∑≥ÂïèÈ°åËß£Á≠î‰∏≠ÊØîËºÉ‰∫Ü KAG ËàáÁèæÊúâÁöÑ RAG ÊñπÊ≥ïÔºåÁôºÁèæÂÆÉÈ°ØËëóÂÑ™ÊñºÁèæÊúâÊäÄË°ìÔºåÂú® F1 ÂàÜÊï∏ÊñπÈù¢ÔºåÂú® 2wiki ‰∏äÊèêÈ´ò‰∫Ü 19.6%ÔºåÂú® hotpotQA ‰∏äÊèêÈ´ò‰∫Ü 33.5%„ÄÇÊàëÂÄëÂ∑≤ÊàêÂäüÂ∞á KAG ÊáâÁî®ÊñºËûûËüªÈõÜÂúòÁöÑÂÖ©ÂÄãÂ∞àÊ•≠Áü•Ë≠òÂïèÁ≠î‰ªªÂãôÔºåÂåÖÊã¨ÈõªÂ≠êÊîøÂãôÂïèÁ≠îÂíåÈõªÂ≠êÂÅ•Â∫∑ÂïèÁ≠îÔºåËàá RAG ÊñπÊ≥ïÁõ∏ÊØîÔºåÂ∞àÊ•≠ÊÄßÊúâÈ°ØËëóÊèêÂçá„ÄÇ

##### **Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity**
2409.06091v1 by Dongyue Li, Aneesh Sharma, Hongyang R. Zhang

Multitask learning is a widely used paradigm for training models on diverse
tasks, with applications ranging from graph neural networks to language model
fine-tuning. Since tasks may interfere with each other, a key notion for
modeling their relationships is task affinity. This includes pairwise task
affinity, computed among pairs of tasks, and higher-order affinity, computed
among subsets of tasks. Naively computing either of them requires repeatedly
training on data from various task combinations, which is computationally
intensive. We present a new algorithm Grad-TAG that can estimate task
affinities without this repeated training.
  The key idea of Grad-TAG is to train a "base" model for all tasks and then
use a linearization technique to estimate the loss of the model for a specific
task combination. The linearization works by computing a gradient-based
approximation of the loss, using low-dimensional projections of gradients as
features in a logistic regression to predict labels for the task combination.
We show that the linearized model can provably approximate the loss when the
gradient-based approximation is accurate, and also empirically verify that on
several large models. Then, given the estimated task affinity, we design a
semi-definite program for clustering similar tasks by maximizing the average
density of clusters.
  We evaluate Grad-TAG's performance across seven datasets, including
multi-label classification on graphs, and instruction fine-tuning of language
models. Our task affinity estimates are within 2.7% distance to the true
affinities while needing only 3% of FLOPs in full training. On our largest
graph with 21M edges and 500 labeling tasks, our algorithm delivers estimates
within 5% distance to the true affinities, using only 112 GPU hours. Our
results show that Grad-TAG achieves excellent performance and runtime tradeoffs
compared to existing approaches.

ÊëòË¶ÅÔºöÂ§ö‰ªªÂãôÂ≠∏ÁøíÊòØ‰∏ÄÁ®ÆÂª£Ê≥õ‰ΩøÁî®ÁöÑÁØÑ‰æãÔºåÁî®ÊñºÂú®‰∏çÂêåÁöÑ‰ªªÂãô‰∏äË®ìÁ∑¥Ê®°ÂûãÔºåÂÖ∂ÊáâÁî®ÁØÑÂúçÂæûÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂà∞Ë™ûË®ÄÊ®°ÂûãÂæÆË™ø„ÄÇÁî±Êñº‰ªªÂãôÂèØËÉΩÊúÉÁõ∏‰∫íÂπ≤ÊìæÔºåÂõ†Ê≠§Âª∫Ê®°ÂÆÉÂÄëÈóú‰øÇÁöÑ‰∏ÄÂÄãÈóúÈçµÊ¶ÇÂøµÊòØ‰ªªÂãôË¶™ÂíåÊÄß„ÄÇÈÄôÂåÖÊã¨ÊàêÂ∞ç‰ªªÂãôË¶™ÂíåÊÄßÔºåÂú®ÊàêÂ∞ç‰ªªÂãô‰πãÈñìË®àÁÆóÔºå‰ª•ÂèäÈ´òÈöéË¶™ÂíåÊÄßÔºåÂú®‰ªªÂãôÂ≠êÈõÜ‰πãÈñìË®àÁÆó„ÄÇÂ§©ÁúüÂú∞Ë®àÁÆóÂÖ∂‰∏≠‰ªª‰Ωï‰∏ÄÂÄãÈÉΩÈúÄË¶ÅÈáçË§áË®ìÁ∑¥‰æÜËá™ÂêÑÁ®Æ‰ªªÂãôÁµÑÂêàÁöÑË≥áÊñôÔºåÈÄôÂú®Ë®àÁÆó‰∏äÂæàÂØÜÈõÜ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊºîÁÆóÊ≥ï Grad-TAGÔºåÂÆÉÂèØ‰ª•Âú®Ê≤íÊúâÈáçË§áË®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ã‰º∞Ë®à‰ªªÂãôË¶™ÂíåÊÄß„ÄÇ
Grad-TAG ÁöÑÈóúÈçµÊÄùÊÉ≥ÊòØÁÇ∫ÊâÄÊúâ‰ªªÂãôË®ìÁ∑¥‰∏ÄÂÄã„ÄåÂü∫Á§é„ÄçÊ®°ÂûãÔºåÁÑ∂Âæå‰ΩøÁî®Á∑öÊÄßÂåñÊäÄË°ì‰æÜ‰º∞Ë®àÊ®°ÂûãÂ∞çÁâπÂÆö‰ªªÂãôÁµÑÂêàÁöÑÊêçÂ§±„ÄÇÁ∑öÊÄßÂåñÈÄöÈÅéË®àÁÆóÊêçÂ§±ÁöÑÂü∫ÊñºÊ¢ØÂ∫¶ÁöÑËøë‰ººÂÄº‰æÜÂ∑•‰ΩúÔºå‰ΩøÁî®Ê¢ØÂ∫¶ÁöÑ‰ΩéÁ∂≠ÊäïÂΩ±‰ΩúÁÇ∫ÁâπÂæµÔºåÂú®ÈÇèËºØËø¥Ê≠∏‰∏≠È†êÊ∏¨‰ªªÂãôÁµÑÂêàÁöÑÊ®ôÁ±§„ÄÇÊàëÂÄëË≠âÊòé‰∫ÜÁï∂Âü∫ÊñºÊ¢ØÂ∫¶ÁöÑËøë‰ººÂÄºÊ∫ñÁ¢∫ÊôÇÔºåÁ∑öÊÄßÂåñÊ®°ÂûãÂèØ‰ª•Ë≠âÊòéÂú∞Ëøë‰ººÊêçÂ§±Ôºå‰∏¶‰∏îÂú®ÂπæÂÄãÂ§ßÂûãÊ®°Âûã‰∏äÁ∂ìÈ©óÈ©óË≠â‰∫ÜÈÄô‰∏ÄÈªû„ÄÇÁÑ∂ÂæåÔºåÁµ¶ÂÆö‰º∞Ë®àÁöÑ‰ªªÂãôË¶™ÂíåÊÄßÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂçäÂÆöÁ®ãÂºèÔºåÈÄöÈÅéÊúÄÂ§ßÂåñÂè¢ÈõÜÁöÑÂπ≥ÂùáÂØÜÂ∫¶‰æÜÂ∞çÈ°û‰ººÁöÑ‰ªªÂãôÈÄ≤Ë°åÂè¢ÈõÜ„ÄÇ
ÊàëÂÄëË©ï‰º∞‰∫Ü Grad-TAG Âú®‰∏ÉÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩÔºåÂåÖÊã¨ÂúñÂΩ¢‰∏äÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°ûÔºå‰ª•ÂèäË™ûË®ÄÊ®°ÂûãÁöÑÊåá‰ª§ÂæÆË™ø„ÄÇÊàëÂÄëÁöÑ‰ªªÂãôË¶™ÂíåÊÄß‰º∞Ë®àËàáÁúüÂØ¶Ë¶™ÂíåÊÄßË∑ùÈõ¢Âú® 2.7% ‰ª•ÂÖßÔºåÂêåÊôÇÂè™ÈúÄË¶Å 3% ÁöÑ FLOP ÈÄ≤Ë°åÂÆåÊï¥Ë®ìÁ∑¥„ÄÇÂú®ÊàëÂÄëÊúÄÂ§ßÁöÑÂúñÂΩ¢ÔºàÊúâ 2100 Ëê¨Ê¢ùÈÇäÂíå 500 ÂÄãÊ®ôÁ±§‰ªªÂãôÔºâ‰∏äÔºåÊàëÂÄëÁöÑÊºîÁÆóÊ≥ïÊèê‰æõÁöÑ‰º∞Ë®àËàáÁúüÂØ¶Ë¶™ÂíåÊÄßË∑ùÈõ¢Âú® 5% ‰ª•ÂÖßÔºåÂè™‰ΩøÁî® 112 ÂÄã GPU Â∞èÊôÇ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåGrad-TAG Âú®ÊïàËÉΩÂíåÂü∑Ë°åÊôÇÈñìÊ¨äË°°ÊñπÈù¢ÂèñÂæó‰∫ÜÂÑ™Áï∞ÁöÑË°®Áèæ„ÄÇ

##### **OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System**
2409.07497v1 by Ningyu Zhang, Zekun Xi, Yujie Luo, Peng Wang, Bozhong Tian, Yunzhi Yao, Jintian Zhang, Shumin Deng, Mengshu Sun, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen

Knowledge representation has been a central aim of AI since its inception.
Symbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can
both represent knowledge. KGs provide highly accurate and explicit knowledge
representation, but face scalability issue; while LLMs offer expansive coverage
of knowledge, but incur significant training costs and struggle with precise
and reliable knowledge manipulation. To this end, we introduce OneEdit, a
neural-symbolic prototype system for collaborative knowledge editing using
natural language, which facilitates easy-to-use knowledge management with KG
and LLM. OneEdit consists of three modules: 1) The Interpreter serves for user
interaction with natural language; 2) The Controller manages editing requests
from various users, leveraging the KG with rollbacks to handle knowledge
conflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the
knowledge from the Controller to edit KG and LLM. We conduct experiments on two
new datasets with KGs which demonstrate that OneEdit can achieve superior
performance.

ÊëòË¶ÅÔºöÁü•Ë≠òË°®ÂæµËá™‰∫∫Â∑•Êô∫ÊÖßË™ïÁîü‰ª•‰æÜ‰∏ÄÁõ¥ÊòØÂÖ∂Ê†∏ÂøÉÁõÆÊ®ô„ÄÇ
Á¨¶ËôüÁü•Ë≠òÂúñË≠ú (KG) ÂíåÁ•ûÁ∂ìË™ûË®ÄÂ§ßÊ®°Âûã (LLM) ÈÉΩÂèØ‰ª•Ë°®ÂæµÁü•Ë≠ò„ÄÇKG Êèê‰æõÈ´òÂ∫¶Ê∫ñÁ¢∫‰∏îÊòéÁ¢∫ÁöÑÁü•Ë≠òË°®ÂæµÔºå‰ΩÜÈù¢Ëá®ÂèØÊì¥ÂÖÖÊÄßÁöÑÂïèÈ°åÔºõËÄå LLM Êèê‰æõÂª£Ê≥õÁöÑÁü•Ë≠òÊ∂µËìãÁØÑÂúçÔºå‰ΩÜÊúÉÁî¢ÁîüÂ§ßÈáèÁöÑË®ìÁ∑¥ÊàêÊú¨Ôºå‰∏¶‰∏îÂú®Á≤æÁ¢∫‰∏îÂèØÈù†ÁöÑÁü•Ë≠òÊìç‰ΩúÊñπÈù¢ÈÅáÂà∞Âõ∞Èõ£„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü OneEditÔºåÈÄôÊòØ‰∏ÄÂÄã‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÈÄ≤Ë°åÂçî‰ΩúÁü•Ë≠òÁ∑®ËºØÁöÑÁ•ûÁ∂ìÁ¨¶ËôüÂéüÂûãÁ≥ªÁµ±ÔºåÂÆÉ‰øÉÈÄ≤‰∫Ü‰ΩøÁî® KG Âíå LLM ÈÄ≤Ë°åÊòìÊñº‰ΩøÁî®ÁöÑÁü•Ë≠òÁÆ°ÁêÜ„ÄÇOneEdit ÂåÖÂê´‰∏âÂÄãÊ®°ÁµÑÔºö1) Ëß£Ë≠ØÂô®Áî®Êñº‰ΩøÁî®ËÄÖÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÈÄ≤Ë°å‰∫íÂãïÔºõ2) ÊéßÂà∂Âô®ÁÆ°ÁêÜ‰æÜËá™‰∏çÂêå‰ΩøÁî®ËÄÖÁöÑÁ∑®ËºØË´ãÊ±ÇÔºåÂà©Áî® KG ÂíåÂõûÊªæ‰æÜËôïÁêÜÁü•Ë≠òË°ùÁ™Å‰∏¶Èò≤Ê≠¢ÊúâÊØíÁöÑÁü•Ë≠òÊîªÊìäÔºõ3) Á∑®ËºØÂô®Âà©Áî®‰æÜËá™ÊéßÂà∂Âô®ÁöÑÁü•Ë≠ò‰æÜÁ∑®ËºØ KG Âíå LLM„ÄÇÊàëÂÄëÂ∞çÂÖ©ÂÄãÂÖ∑Êúâ KG ÁöÑÊñ∞Ë≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂØ¶È©óÔºåË≠âÊòé OneEdit ÂèØ‰ª•ÂØ¶ÁèæÂÑ™Áï∞ÁöÑÊïàËÉΩ„ÄÇ

##### **SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning**
2409.05556v1 by Alireza Ghafarollahi, Markus J. Buehler

A key challenge in artificial intelligence is the creation of systems capable
of autonomously advancing scientific understanding by exploring novel domains,
identifying complex patterns, and uncovering previously unseen connections in
vast scientific data. In this work, we present SciAgents, an approach that
leverages three core concepts: (1) the use of large-scale ontological knowledge
graphs to organize and interconnect diverse scientific concepts, (2) a suite of
large language models (LLMs) and data retrieval tools, and (3) multi-agent
systems with in-situ learning capabilities. Applied to biologically inspired
materials, SciAgents reveals hidden interdisciplinary relationships that were
previously considered unrelated, achieving a scale, precision, and exploratory
power that surpasses traditional human-driven research methods. The framework
autonomously generates and refines research hypotheses, elucidating underlying
mechanisms, design principles, and unexpected material properties. By
integrating these capabilities in a modular fashion, the intelligent system
yields material discoveries, critique and improve existing hypotheses, retrieve
up-to-date data about existing research, and highlights their strengths and
limitations. Our case studies demonstrate scalable capabilities to combine
generative AI, ontological representations, and multi-agent modeling,
harnessing a `swarm of intelligence' similar to biological systems. This
provides new avenues for materials discovery and accelerates the development of
advanced materials by unlocking Nature's design principles.

ÊëòË¶ÅÔºöÂú®‰∫∫Â∑•Êô∫ËÉΩ‰∏≠Ôºå‰∏ÄÂÄãÈóúÈçµÁöÑÊåëÊà∞ÊòØÂâµÈÄ†Âá∫ÊúâËÉΩÂäõÈÄèÈÅéÊé¢Á¥¢Êñ∞È†òÂüü„ÄÅË≠òÂà•Ë§áÈõúÊ®°ÂºèÔºå‰ª•ÂèäÂú®Â§ßÈáèÁöÑÁßëÂ≠∏Êï∏Êìö‰∏≠ÁôºÁèæÂâçÊâÄÊú™Ë¶ãÁöÑÈóúËÅØÔºå‰æÜËá™‰∏ªÊé®ÈÄ≤ÁßëÂ≠∏ÁêÜËß£ÁöÑÁ≥ªÁµ±„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü SciAgentsÔºå‰∏ÄÁ®ÆÂà©Áî®‰∏âÂÄãÊ†∏ÂøÉÊ¶ÇÂøµÁöÑÊñπÊ≥ïÔºö(1) ‰ΩøÁî®Â§ßË¶èÊ®°ÁöÑÊú¨‰ΩìÁü•Ë≠òÂúñË≠ú‰æÜÊï¥ÁêÜÂíåÈÄ£Áµê‰∏çÂêåÁöÑÁßëÂ≠∏Ê¶ÇÂøµÔºå(2) ‰∏ÄÂ•óÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÊï∏ÊìöÊ™¢Á¥¢Â∑•ÂÖ∑Ôºå‰ª•Âèä (3) ÂÖ∑ÊúâÂéü‰ΩçÂ≠∏ÁøíËÉΩÂäõÁöÑÂ§ö‰ª£ÁêÜÁ≥ªÁµ±„ÄÇÊáâÁî®ÊñºÁîüÁâ©ÂïüÁôºÊùêÊñôÔºåSciAgents Êè≠Á§∫‰∫Ü‰ª•ÂâçË¢´Ë™çÁÇ∫ÁÑ°ÈóúÁöÑÈö±ËóèË∑®Â≠∏ÁßëÈóú‰øÇÔºåÈÅîÂà∞‰∫ÜË∂ÖË∂äÂÇ≥Áµ±‰∫∫ÁÇ∫Á†îÁ©∂ÊñπÊ≥ïÁöÑË¶èÊ®°„ÄÅÁ≤æÁ¢∫Â∫¶ÂíåÊé¢Á¥¢ËÉΩÂäõ„ÄÇË©≤Ê°ÜÊû∂Ëá™‰∏ªÁîüÊàêÂíåÂÑ™ÂåñÁ†îÁ©∂ÂÅáË®≠ÔºåÈó°ÊòéÂü∫Á§éÊ©üÂà∂„ÄÅË®≠Ë®àÂéüÁêÜÂíåÊÑèÂ§ñÁöÑÊùêÊñôÁâπÊÄß„ÄÇÈÄèÈÅé‰ª•Ê®°ÁµÑÂåñÊñπÂºèÊï¥ÂêàÈÄô‰∫õËÉΩÂäõÔºåÊô∫ËÉΩÁ≥ªÁµ±Áî¢ÁîüÊùêÊñôÁôºÁèæ„ÄÅÊâπÂà§ÂíåÊîπÈÄ≤ÁèæÊúâÂÅáË®≠„ÄÅÊ™¢Á¥¢ÈóúÊñºÁèæÊúâÁ†îÁ©∂ÁöÑÊúÄÊñ∞Êï∏ÊìöÔºå‰∏¶Âº∑Ë™øÂÆÉÂÄëÁöÑÂÑ™ÈªûÂíåÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÊ°à‰æãÁ†îÁ©∂Â±ïÁ§∫‰∫ÜÁµêÂêàÁîüÊàêÂºè AI„ÄÅÊú¨‰ΩìË°®Á§∫ÂíåÂ§ö‰ª£ÁêÜÂª∫Ê®°ÁöÑÂèØÊì¥ÂÖÖËÉΩÂäõÔºåÂà©Áî®È°û‰ººÊñºÁîüÁâ©Á≥ªÁµ±ÁöÑ„ÄåÊô∫ÊÖßÁæ§È´î„Äç„ÄÇÈÄôÁÇ∫ÊùêÊñôÁôºÁèæÊèê‰æõ‰∫ÜÊñ∞ÈÄîÂæëÔºå‰∏¶ÈÄèÈÅéËß£ÈéñÂ§ßËá™ÁÑ∂ÁöÑË®≠Ë®àÂéüÁêÜ‰æÜÂä†ÈÄüÂÖàÈÄ≤ÊùêÊñôÁöÑÈñãÁôº„ÄÇ

##### **Assessing SPARQL capabilities of Large Language Models**
2409.05925v1 by Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt

The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)
offers significant synergistic potential for knowledge-driven applications. One
possible integration is the interpretation and generation of formal languages,
such as those used in the Semantic Web, with SPARQL being a core technology for
accessing KGs. In this paper, we focus on measuring out-of-the box capabilities
of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries
applying a quantitative approach.
  We implemented various benchmarking tasks in the LLM-KG-Bench framework for
automated execution and evaluation with several LLMs. The tasks assess
capabilities along the dimensions of syntax, semantic read, semantic create,
and the role of knowledge graph prompt inclusion.
  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,
and Claude models. Our findings indicate that working with SPARQL SELECT
queries is still challenging for LLMs and heavily depends on the specific LLM
as well as the complexity of the task. While fixing basic syntax errors seems
to pose no problems for the best of the current LLMs evaluated, creating
semantically correct SPARQL SELECT queries is difficult in several cases.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÊï¥ÂêàÁÇ∫Áü•Ë≠òÈ©ÖÂãïÊáâÁî®Á®ãÂºèÊèê‰æõ‰∫ÜÈ°ØËëóÁöÑÁ∂úÊïàÊΩõÂäõ„ÄÇ‰∏ÄÁ®ÆÂèØËÉΩÁöÑÊï¥ÂêàÊòØËß£ÈáãÂíåÁî¢ÁîüÂΩ¢ÂºèÂåñË™ûË®ÄÔºå‰æãÂ¶ÇË™ûÁæ©Á∂≤Ë∑Ø‰∏≠‰ΩøÁî®ÁöÑË™ûË®ÄÔºåËÄå SPARQL ÊòØÂ≠òÂèñ KG ÁöÑÊ†∏ÂøÉÊäÄË°ì„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºË°°Èáè LLM ÈñãÁÆ±Âç≥Áî®ÁöÑËÉΩÂäõÔºå‰ª•‰ΩøÁî® SPARQLÔºåÊõ¥ÂÖ∑È´îÂú∞Ë™™Ôºå‰ΩøÁî® SPARQL SELECT Êü•Ë©¢ÊáâÁî®ÈáèÂåñÊñπÊ≥ï„ÄÇ
  ÊàëÂÄëÂú® LLM-KG-Bench Êû∂Êßã‰∏≠ÂØ¶‰Ωú‰∫ÜÂêÑÁ®ÆÂü∫Ê∫ñÊ∏¨Ë©¶‰ªªÂãôÔºå‰ª•Ëá™ÂãïÂü∑Ë°åÂíåË©ï‰º∞Â§öÂÄã LLM„ÄÇÈÄô‰∫õ‰ªªÂãôË©ï‰º∞‰∫ÜË™ûÊ≥ï„ÄÅË™ûÁæ©ËÆÄÂèñ„ÄÅË™ûÁæ©Âª∫Á´ãÂíåÁü•Ë≠òÂúñË≠úÊèêÁ§∫ÂåÖÂê´ÁöÑËßíËâ≤Á≠âÈù¢ÂêëÁöÑËÉΩÂäõ„ÄÇ
  Êúâ‰∫ÜÈÄô‰∫õÊñ∞ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶‰ªªÂãôÔºåÊàëÂÄëË©ï‰º∞‰∫Ü GPT„ÄÅGemini Âíå Claude Ê®°ÂûãÁöÑÈÅ∏È†Ö„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºå‰ΩøÁî® SPARQL SELECT Êü•Ë©¢Â∞çÊñº LLM ‰æÜË™™‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰∏¶‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÊ±∫ÊñºÂÖ∑È´îÁöÑ LLM ‰ª•Âèä‰ªªÂãôÁöÑË§áÈõúÊÄß„ÄÇÂÑòÁÆ°‰øÆÂæ©Âü∫Êú¨ÁöÑË™ûÊ≥ïÈåØË™§‰ºº‰πéÂ∞çÁõÆÂâçË©ï‰º∞ÁöÑÊúÄ‰Ω≥ LLM ‰æÜË™™‰∏çÊàêÂïèÈ°åÔºå‰ΩÜÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÂª∫Á´ãË™ûÁæ©Ê≠£Á¢∫ÁöÑ SPARQL SELECT Êü•Ë©¢ÂæàÂõ∞Èõ£„ÄÇ

##### **KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**
2409.05370v1 by Yingshu Li, Zhanyu Wang, Yunyi Liu, Lei Wang, Lingqiao Liu, Luping Zhou

Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âº∑Â§ßÁöÑÂäüËÉΩÔºåÈÄ≤Ë°åÊïò‰∫ãÁîüÊàê„ÄÅÈÇèËºØÊé®ÁêÜÂíåÂ∏∏Ë≠òÁü•Ë≠òÊï¥ÂêàÔºåÊú¨Á†îÁ©∂Ê∑±ÂÖ•Êé¢Ë®éÂà©Áî® LLM ‰æÜÂ¢ûÂº∑Ëá™ÂãïÂåñÊîæÂ∞ÑÂ†±ÂëäÁîüÊàê (R2Gen)„ÄÇÂÑòÁÆ° LLM ÊìÅÊúâË±êÂØåÁöÑÁü•Ë≠òÔºå‰ΩÜË¶ÅÊúâÊïàËß∏ÁôºÈÄô‰∫õÂ§ßÂûãÊ®°Âûã‰∏≠ËàáÁâπÂÆö‰ªªÂãôÔºàÂ¶Ç R2GenÔºâÁõ∏ÈóúÁöÑÁü•Ë≠òÔºåÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÁ†îÁ©∂ÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü KARGENÔºå‰∏ÄÂÄãÂü∫Êñº LLM ÁöÑÁü•Ë≠òÂ¢ûÂº∑Ëá™ÂãïÂåñÊîæÂ∞ÑÂ†±ÂëäÁîüÊàêÊ°ÜÊû∂„ÄÇÂà©Áî®ÂáçÁµêÁöÑ LLM ‰æÜÁîüÊàêÂ†±ÂëäÔºåË©≤Ê°ÜÊû∂Êï¥Âêà‰∫Ü‰∏ÄÂÄãÁü•Ë≠òÂúñË≠úÔºå‰ª•Ëß£Èéñ LLM ‰∏≠ËàáËÉ∏ÈÉ®ÁñæÁóÖÁõ∏ÈóúÁöÑÁü•Ë≠òÔºå‰ª•Â¢ûÂº∑ÁîüÊàêÂ†±ÂëäÁöÑËá®Â∫äÊïàÁî®„ÄÇÈÄôÊòØÈÄèÈÅéÂà©Áî®Áü•Ë≠òÂúñË≠ú‰ª•Ë®≠Ë®àÁöÑÊñπÂºèÊèêÂèñËàáÁñæÁóÖÁõ∏ÈóúÁöÑÁâπÂæµ‰æÜÂØ¶ÁèæÁöÑ„ÄÇÁî±ÊñºÊîæÂ∞ÑÂ†±ÂëäÂåÖÂê´Ê≠£Â∏∏ÂíåÁñæÁóÖÁõ∏ÈóúÁöÑÁôºÁèæÔºåÂõ†Ê≠§ÊèêÂèñÁöÑÂúñÂΩ¢Â¢ûÂº∑ÁñæÁóÖÁõ∏ÈóúÁâπÂæµËàáÂçÄÂüüÂΩ±ÂÉèÁâπÂæµÊï¥ÂêàÔºåÂÖºÈ°ßÂÖ©ÂÄãÊñπÈù¢„ÄÇÊàëÂÄëÊé¢Á¥¢‰∫ÜÂÖ©Á®ÆËûçÂêàÊñπÊ≥ïÔºå‰ª•Ëá™ÂãïÂÑ™ÂÖàÊéíÂ∫èÂíåÈÅ∏ÊìáÊúÄÁõ∏ÈóúÁöÑÁâπÂæµ„ÄÇËûçÂêàÁöÑÁâπÂæµÁî± LLM ‰ΩøÁî®Ôºå‰ª•ÁîüÊàêÂ∞çÁñæÁóÖÊõ¥ÊïèÊÑü‰∏îÂìÅË≥™Êõ¥È´òÁöÑÂ†±Âëä„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® MIMIC-CXR Âíå IU-Xray Ë≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇ</paragraph>

##### **Action is the primary key: a categorical framework for episode description and logical reasoning**
2409.04793v1 by Yoshiki Fukada

This research presents a computational framework for describing and
recognizing episodes and for logical reasoning. This framework, named
cognitive-logs, consists of a set of relational and graph databases.
Cognitive-logs record knowledge, particularly in episodes that consist of
"actions" represented by verbs in natural languages and "participants" who
perform the actions. These objects are connected by arrows (morphisms) that
link each action to its participant and link cause to effect. Operations based
on category theory enable comparisons between episodes and deductive
inferences, including abstractions of stories. One of the goals of this study
is to develop a database-driven artificial intelligence. This artificial
intelligence thinks like a human but possesses the accuracy and rigour of a
machine. The vast capacities of databases (up to petabyte scales in current
technologies) enable the artificial intelligence to store a greater volume of
knowledge than neural-network based artificial intelligences. Cognitive-logs
serve as a model of human cognition and designed with references to cognitive
linguistics. Cognitive-logs also have the potential to model various human mind
activities.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãË®àÁÆóÊ°ÜÊû∂ÔºåÁî®‰æÜÊèèËø∞ÂíåËæ®Ë≠ò‰∫ã‰ª∂‰ª•ÂèäÈÄ≤Ë°åÈÇèËºØÊé®ÁêÜ„ÄÇÈÄôÂÄãÊ°ÜÊû∂ÂêçÁÇ∫Ë™çÁü•Êó•Ë™åÔºåÂåÖÂê´‰∏ÄÁµÑÈóúËÅØÂºèÂíåÂúñÂΩ¢Ë≥áÊñôÂ∫´„ÄÇË™çÁü•Êó•Ë™åË®òÈåÑÁü•Ë≠òÔºåÁâπÂà•ÊòØÂåÖÂê´Áî±Ëá™ÁÑ∂Ë™ûË®Ä‰∏≠ÁöÑÂãïË©ûË°®Á§∫ÁöÑ„ÄåÂãï‰Ωú„ÄçÂíåÂü∑Ë°åÂãï‰ΩúÁöÑ„ÄåÂèÉËàáËÄÖ„ÄçÁöÑ‰∫ã‰ª∂„ÄÇÈÄô‰∫õÁâ©‰ª∂Áî±ÁÆ≠È†≠ÔºàÊÖãÂ∞ÑÔºâÈÄ£Êé•ÔºåÂ∞áÊØèÂÄãÂãï‰ΩúÈÄ£ÁµêÂà∞ÂÖ∂ÂèÉËàáËÄÖÔºå‰∏¶Â∞áÂéüÂõ†ÈÄ£ÁµêÂà∞ÁµêÊûú„ÄÇÂü∫ÊñºÁØÑÁñáË´ñÁöÑÈÅãÁÆóÂèØÊØîËºÉ‰∫ã‰ª∂ÂíåÊºîÁππÊé®Ë´ñÔºåÂåÖÊã¨ÊïÖ‰∫ãÁöÑÊäΩË±°Âåñ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÊ®ô‰πã‰∏ÄÊòØÈñãÁôº‰∏ÄÂÄãË≥áÊñôÂ∫´È©ÖÂãïÁöÑ‰∫∫Â∑•Êô∫ÊÖß„ÄÇÈÄôÂÄã‰∫∫Â∑•Êô∫ÊÖßÊÄùËÄÉÊñπÂºèÂÉè‰∫∫È°ûÔºå‰ΩÜÊìÅÊúâÊ©üÂô®Ëà¨ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂö¥Ë¨πÊÄß„ÄÇË≥áÊñôÂ∫´ÁöÑÈæêÂ§ßÂÆπÈáèÔºàÂú®ÁõÆÂâçÁöÑÊäÄË°ì‰∏≠ÂèØÈÅîÁöÆ‰ΩçÂÖÉÁµÑÁ≠âÁ¥öÔºâ‰Ωø‰∫∫Â∑•Êô∫ÊÖßËÉΩÂ§†ÂÑ≤Â≠òÊØîÂü∫ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊõ¥Â§ßÁöÑÁü•Ë≠òÈáè„ÄÇË™çÁü•Êó•Ë™å‰ΩúÁÇ∫‰∫∫È°ûË™çÁü•ÁöÑÊ®°ÂûãÔºå‰∏¶ÂèÉËÄÉË™çÁü•Ë™ûË®ÄÂ≠∏ÈÄ≤Ë°åË®≠Ë®à„ÄÇË™çÁü•Êó•Ë™å‰πüÊúâÊΩõÂäõÊ®°Êì¨ÂêÑÁ®Æ‰∫∫È°ûÂøÉÊô∫Ê¥ªÂãï„ÄÇ

##### **Accelerating Training with Neuron Interaction and Nowcasting Networks**
2409.04434v1 by Boris Knyazev, Abhinav Moudgil, Guillaume Lajoie, Eugene Belilovsky, Simon Lacoste-Julien

Neural network training can be accelerated when a learnable update rule is
used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable
update rules can be costly and unstable to train and use. A simpler recently
proposed approach to accelerate training is to use Adam for most of the
optimization steps and periodically, only every few steps, nowcast (predict
future) parameters. We improve this approach by Neuron interaction and
Nowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural
networks to more accurately nowcast parameters by learning in a supervised way
from a set of training trajectories over multiple tasks. We show that in some
networks, such as Transformers, neuron connectivity is non-trivial. By
accurately modeling neuron connectivity, we allow NiNo to accelerate Adam
training by up to 50\% in vision and language tasks.

ÊëòË¶ÅÔºöÁ•ûÁªèÁΩëÁªúËÆ≠ÁªÉÂèØ‰ª•Âä†ÈÄüÔºåÂΩì‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÊõ¥Êñ∞ËßÑÂàôË¢´Áî®Êù•‰ª£ÊõøÁªèÂÖ∏ÁöÑËá™ÈÄÇÂ∫î‰ºòÂåñÂô®Ôºà‰æãÂ¶Ç AdamÔºâ„ÄÇÁÑ∂ËÄåÔºåÂèØÂ≠¶‰π†ÁöÑÊõ¥Êñ∞ËßÑÂàôÂèØËÉΩÊòØÊòÇË¥µ‰∏î‰∏çÁ®≥ÂÆöÁöÑÔºåÈúÄË¶ÅËÆ≠ÁªÉÂíå‰ΩøÁî®„ÄÇ‰∏ÄÁßçÊúÄËøëÊèêÂá∫ÁöÑÊõ¥ÁÆÄÂçïÁöÑÂä†ÈÄüËÆ≠ÁªÉÁöÑÊñπÊ≥ïÊòØÔºåÂØπ‰∫éÂ§ßÂ§öÊï∞ÁöÑ‰ºòÂåñÊ≠•È™§‰ΩøÁî® AdamÔºåÂπ∂‰∏îÂÆöÊúüÂú∞Ôºå‰ªÖÊØèÈöîÂá†Ê≠•ÔºåÈ¢ÑÊµãÔºàÈ¢ÑÊµãÊú™Êù•ÔºâÂèÇÊï∞„ÄÇÊàë‰ª¨ÈÄöËøáÁ•ûÁªèÂÖÉ‰∫§‰∫íÂíåÈ¢ÑÊµãÔºàNiNoÔºâÁΩëÁªúÊù•ÊîπËøõËøôÁßçÊñπÊ≥ï„ÄÇNiNo Âà©Áî®Á•ûÁªèÂÖÉËøûÊé•ÂíåÂõæÁ•ûÁªèÁΩëÁªúÔºåÈÄöËøá‰ªéÂ§ö‰∏™‰ªªÂä°‰∏≠ÁöÑ‰∏ÄÁªÑËÆ≠ÁªÉËΩ®Ëøπ‰∏≠‰ª•ÁõëÁù£ÊñπÂºèÂ≠¶‰π†ÔºåÊõ¥ÂáÜÁ°ÆÂú∞È¢ÑÊµãÂèÇÊï∞„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÂú®‰∏Ä‰∫õÁΩëÁªú‰∏≠Ôºå‰æãÂ¶Ç TransformerÔºåÁ•ûÁªèÂÖÉËøûÊé•ÊòØÈùûÂπ≥Âá°ÁöÑ„ÄÇÈÄöËøáÂáÜÁ°ÆÂú∞Âª∫Ê®°Á•ûÁªèÂÖÉËøûÊé•ÔºåÊàë‰ª¨ÂÖÅËÆ∏ NiNo Â∞Ü Adam ËÆ≠ÁªÉÂä†ÈÄüÈ´òËææ 50%ÔºåÁî®‰∫éËßÜËßâÂíåËØ≠Ë®Ä‰ªªÂä°„ÄÇ

##### **Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets**
2409.04286v1 by Desiree Heim, Christian Jilek, Adrian Ulges, Andreas Dengel

Current publicly available knowledge work data collections lack diversity,
extensive annotations, and contextual information about the users and their
documents. These issues hinder objective and comparable data-driven evaluations
and optimizations of knowledge work assistance systems. Due to the considerable
resources needed to collect such data in real-life settings and the necessity
of data censorship, collecting such a dataset appears nearly impossible. For
this reason, we propose a configurable, multi-agent knowledge work dataset
generator. This system simulates collaborative knowledge work among agents
producing Large Language Model-generated documents and accompanying data
traces. Additionally, the generator captures all background information, given
in its configuration or created during the simulation process, in a knowledge
graph. Finally, the resulting dataset can be utilized and shared without
privacy or confidentiality concerns.
  This paper introduces our approach's design and vision and focuses on
generating authentic knowledge work documents using Large Language Models. Our
study involving human raters who assessed 53% of the generated and 74% of the
real documents as realistic demonstrates the potential of our approach.
Furthermore, we analyze the authenticity criteria mentioned in the
participants' comments and elaborate on potential improvements for identified
common issues.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÂâçÂÖ¨ÈñãÂèØÁî®ÁöÑÁü•Ë≠òÂ∑•‰ΩúË≥áÊñôËíêÈõÜÁº∫‰πèÂ§öÂÖÉÊÄß„ÄÅÂª£Ê≥õË®ªËß£Âíå‰ΩøÁî®ËÄÖÂèäÂÖ∂Êñá‰ª∂ËÉåÊôØË≥áË®ä„ÄÇÈÄô‰∫õÂïèÈ°åÈòªÁ§ô‰∫ÜÂÆ¢ËßÄ‰∏îÂèØÊØîËºÉÁöÑË≥áÊñôÈ©ÖÂãïË©ï‰º∞Ôºå‰ª•ÂèäÁü•Ë≠òÂ∑•‰ΩúÂçîÂä©Á≥ªÁµ±ÁöÑÊúÄ‰Ω≥Âåñ„ÄÇÁî±ÊñºÂú®ÁèæÂØ¶ÁîüÊ¥ª‰∏≠ËíêÈõÜÊ≠§È°ûË≥áÊñôÈúÄË¶ÅÂ§ßÈáèË≥áÊ∫êÔºåËÄå‰∏îÂøÖÈ†àÂØ©Êü•Ë≥áÊñôÔºåËíêÈõÜÊ≠§È°ûË≥áÊñôÁµÑÈ°ØÁÑ∂Âπæ‰πé‰∏çÂèØËÉΩ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂèØË®≠ÂÆöÁöÑÂ§öÈáç‰ª£ÁêÜÁü•Ë≠òÂ∑•‰ΩúË≥áÊñôÁµÑÁî¢ÁîüÂô®„ÄÇÊ≠§Á≥ªÁµ±Ê®°Êì¨‰ª£ÁêÜ‰πãÈñìÁöÑÂçî‰ΩúÁü•Ë≠òÂ∑•‰ΩúÔºåÁî¢ÁîüÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁî¢ÁîüÁöÑÊñá‰ª∂ÂíåÈö®ÈôÑÁöÑË≥áÊñôËøΩËπ§„ÄÇÊ≠§Â§ñÔºåÁî¢ÁîüÂô®ÊúÉÊì∑ÂèñÊâÄÊúâËÉåÊôØË≥áË®äÔºåÂú®ÁµÑÊÖã‰∏≠Êèê‰æõÊàñÂú®Ê®°Êì¨ÈÅéÁ®ã‰∏≠Âª∫Á´ãÔºå‰∏¶Â∞áÂÖ∂ÂÑ≤Â≠òÂú®Áü•Ë≠òÂúñË≠ú‰∏≠„ÄÇÊúÄÂæåÔºåÁî¢ÁîüÁöÑË≥áÊñôÁµÑÂèØ‰ª•‰ΩøÁî®ÂíåÂàÜ‰∫´ÔºåÁÑ°È†àÊìîÂøÉÈö±ÁßÅÊàñÊ©üÂØÜÊÄß„ÄÇ
Êú¨Êñá‰ªãÁ¥πÊàëÂÄëÊñπÊ≥ïÁöÑË®≠Ë®àÂíåÈ°òÊôØÔºå‰∏¶Â∞àÊ≥®Êñº‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁî¢ÁîüÁúüÂØ¶ÁöÑÁü•Ë≠òÂ∑•‰ΩúÊñá‰ª∂„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ê∂âÂèä‰∫∫È°ûË©ïÂàÜÂì°Ôºå‰ªñÂÄëË©ï‰º∞‰∫Ü 53% ÁöÑÁî¢ÁîüÊñá‰ª∂Âíå 74% ÁöÑÁúüÂØ¶Êñá‰ª∂ÁÇ∫ÁúüÂØ¶ÔºåÈÄôË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàÜÊûêÂèÉËàáËÄÖË©ïË´ñ‰∏≠ÊèêÂà∞ÁöÑÁúüÂØ¶ÊÄßÊ®ôÊ∫ñÔºå‰∏¶Ë©≥Á¥∞Ë™™ÊòéÂ∑≤Ë≠òÂà•Â∏∏Ë¶ãÂïèÈ°åÁöÑÊΩõÂú®ÊîπÂñÑÊñπÊ≥ï„ÄÇ</paragraph>

##### **GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding**
2409.04183v1 by Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang

Programming languages possess rich semantic information such as data flow
that is represented by graphs and not available from the surface form of source
code. Recent code language models have scaled to billions of parameters, but
model source code solely as text tokens while ignoring any other structural
information. Conversely, models that do encode structural information of code
make modifications to the Transformer architecture, limiting their scale and
compatibility with pretrained LLMs. In this work, we take the best of both
worlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph
neural networks and cross-modal alignment technologies to inject the structural
information of code into LLMs as an auxiliary task during finetuning. This
framework is both model-agnostic and task-agnostic, as it can be applied to any
code LLM for any code downstream task, and requires the structural graph data
only at training time from a corpus unrelated to the finetuning data, while
incurring no cost at inference time over the baseline LLM. Experiments on five
code tasks with four different baseline LLMs ranging in size from 350M to 8B
validate the effectiveness of GALLa, demonstrating consistent improvement over
the baseline, even for powerful models such as LLaMA3.

ÊëòË¶ÅÔºöÁ®ãÂºèË™ûË®ÄÊìÅÊúâË±êÂØåÁöÑË™ûÊÑèË≥áË®äÔºå‰æãÂ¶ÇÁî±ÂúñÂΩ¢Ë°®Á§∫‰∏îÁÑ°Ê≥ïÂæûÂéüÂßãÁ¢ºË°®Èù¢ÂΩ¢ÂºèÂèñÂæóÁöÑË≥áÊñôÊµÅÁ®ã„ÄÇÊúÄËøëÁöÑÁ®ãÂºèÁ¢ºË™ûË®ÄÊ®°ÂûãÂ∑≤Êì¥ÂÖÖËá≥Êï∏ÂçÅÂÑÑÂÄãÂèÉÊï∏Ôºå‰ΩÜÊ®°ÂûãÂéüÂßãÁ¢ºÂÉÖ‰ΩúÁÇ∫ÊñáÂ≠óÁ¨¶ËôüÔºåËÄåÂøΩÁï•‰ªª‰ΩïÂÖ∂‰ªñÁµêÊßãË≥áË®ä„ÄÇÂèç‰πãÔºåÁ∑®Á¢ºÁ®ãÂºèÁ¢ºÁµêÊßãË≥áË®äÁöÑÊ®°ÂûãÊúÉ‰øÆÊîπ Transformer Êû∂ÊßãÔºåÈôêÂà∂ÂÖ∂Ë¶èÊ®°ÂíåËàáÈ†êÂÖàË®ìÁ∑¥ÁöÑ LLM ÁöÑÁõ∏ÂÆπÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé°Áî® GALLaÔºàÂúñÂΩ¢Â∞çÈΩäÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºâÊì∑ÂèñÂÖ©ÂÖ®ÂÖ∂ÁæéÁöÑÂÑ™Èªû„ÄÇGALLa Âà©Áî®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂíåË∑®Ê®°ÊÖãÂ∞çÈΩäÊäÄË°ìÔºåÂú®ÂæÆË™øÊúüÈñìÂ∞áÁ®ãÂºèÁ¢ºÁöÑÁµêÊßãË≥áË®äÊ≥®ÂÖ• LLM ‰ΩúÁÇ∫ËºîÂä©‰ªªÂãô„ÄÇÊ≠§Êû∂ÊßãÂêåÊôÇ‰∏ç‰æùË≥¥Ê®°ÂûãÂíå‰ªªÂãôÔºåÂõ†ÁÇ∫ÂÆÉÂèØ‰ª•ÊáâÁî®Êñº‰ªª‰ΩïÁ®ãÂºèÁ¢º LLM ÁöÑ‰ªª‰ΩïÁ®ãÂºèÁ¢º‰∏ãÊ∏∏‰ªªÂãôÔºå‰∏¶‰∏îÂÉÖÂú®Ë®ìÁ∑¥ÊúüÈñìÂæûËàáÂæÆË™øË≥áÊñôÁÑ°ÈóúÁöÑË™ûÊñôÂ∫´ÂèñÂæóÁµêÊßãÂúñÂΩ¢Ë≥áÊñôÔºåÂêåÊôÇÂú®Êé®Ë´ñÊúüÈñì‰∏çÁî¢ÁîüÊØîÂü∫Ê∫ñ LLM Êõ¥È´òÁöÑÊàêÊú¨„ÄÇÂú®‰∫îÂÄãÁ®ãÂºèÁ¢º‰ªªÂãô‰∏≠ÈÄ≤Ë°åÂØ¶È©óÔºå‰ΩøÁî®ÂõõÂÄã‰∏çÂêåÁöÑÂü∫Ê∫ñ LLMÔºåË¶èÊ®°Âæû 350M Âà∞ 8BÔºåÈ©óË≠â GALLa ÁöÑÊúâÊïàÊÄßÔºåË≠âÊòéÂç≥‰ΩøÂ∞çÊñº LLaMA3 Á≠âÂº∑Â§ßÊ®°ÂûãÔºå‰πüËÉΩÊåÅÁ∫åÂÑ™ÊñºÂü∫Ê∫ñ„ÄÇ

##### **Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering**
2409.04181v1 by Larissa Pusch, Tim O. F. Conrad

Advancements in natural language processing have revolutionized the way we
can interact with digital information systems, such as databases, making them
more accessible. However, challenges persist, especially when accuracy is
critical, as in the biomedical domain. A key issue is the hallucination
problem, where models generate information unsupported by the underlying data,
potentially leading to dangerous misinformation. This paper presents a novel
approach designed to bridge this gap by combining Large Language Models (LLM)
and Knowledge Graphs (KG) to improve the accuracy and reliability of
question-answering systems, on the example of a biomedical KG. Built on the
LangChain framework, our method incorporates a query checker that ensures the
syntactical and semantic validity of LLM-generated queries, which are then used
to extract information from a Knowledge Graph, substantially reducing errors
like hallucinations. We evaluated the overall performance using a new benchmark
dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo
and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other
models in generating accurate queries, open-source models like llama3:70b show
promise with appropriate prompt engineering. To make this approach accessible,
a user-friendly web-based interface has been developed, allowing users to input
natural language queries, view generated and corrected Cypher queries, and
verify the resulting paths for accuracy. Overall, this hybrid approach
effectively addresses common issues such as data gaps and hallucinations,
offering a reliable and intuitive solution for question answering systems. The
source code for generating the results of this paper and for the user-interface
can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈÄ≤Â±ïÂæπÂ∫ïÊîπËÆä‰∫ÜÊàëÂÄëËàáÊï∏‰ΩçË≥áË®äÁ≥ªÁµ±Ôºà‰æãÂ¶ÇË≥áÊñôÂ∫´Ôºâ‰∫íÂãïÁöÑÊñπÂºèÔºåËÆìÈÄô‰∫õÁ≥ªÁµ±ËÆäÂæóÊõ¥ÊòìÊñºÂ≠òÂèñ„ÄÇÁÑ∂ËÄåÔºåÊåëÊà∞‰ªçÁÑ∂Â≠òÂú®ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∫ñÁ¢∫ÊÄßËá≥ÈóúÈáçË¶ÅÁöÑÊÉÖÊ≥Å‰∏ãÔºå‰æãÂ¶ÇÂú®ÁîüÁâ©ÈÜ´Â≠∏È†òÂüü„ÄÇ‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÊòØÂπªË¶∫ÂïèÈ°åÔºåÂÖ∂‰∏≠Ê®°ÂûãÊúÉÁî¢ÁîüÊú™Á∂ìÂü∫Á§éË≥áÊñôÈ©óË≠âÁöÑË≥áË®äÔºåÂèØËÉΩÂ∞éËá¥Âç±Èö™ÁöÑÈåØË™§Ë≥áË®ä„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄèÈÅéÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁü•Ë≠òÂúñË≠ú (KG) ‰æÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºå‰ª•ÊèêÈ´òÁîüÁâ©ÈÜ´Â≠∏ KG ‰∏≠ÂïèÁ≠îÁ≥ªÁµ±ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂª∫Á´ãÂú® LangChain Ê°ÜÊû∂‰∏äÔºåÁµêÂêà‰∫Ü‰∏ÄÂÄãÊü•Ë©¢Ê™¢Êü•Âô®ÔºåÂèØÁ¢∫‰øù LLM ÁîüÊàêÁöÑÊü•Ë©¢Âú®Ë™ûÊ≥ïÂíåË™ûÊÑè‰∏äÊúâÊïàÔºåÁÑ∂ÂæåÁî®ÊñºÂæûÁü•Ë≠òÂúñË≠ú‰∏≠ËêÉÂèñË≥áË®äÔºåÂ§ßÂπÖÊ∏õÂ∞ëÂπªË¶∫Á≠âÈåØË™§„ÄÇÊàëÂÄë‰ΩøÁî®‰∏ÄÂÄãÊñ∞ÁöÑ 50 ÂÄãÁîüÁâ©ÈÜ´Â≠∏ÂïèÈ°åÂü∫Ê∫ñË≥áÊñôÈõÜË©ï‰º∞‰∫ÜÊï¥È´îÊïàËÉΩÔºåÊ∏¨Ë©¶‰∫ÜÂåÖÊã¨ GPT-4 Turbo Âíå llama3:70b Âú®ÂÖßÁöÑÂπæÂÄã LLM„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÈõñÁÑ∂ GPT-4 Turbo Âú®Áî¢ÁîüÊ∫ñÁ¢∫Êü•Ë©¢ÊñπÈù¢ÂÑ™ÊñºÂÖ∂‰ªñÊ®°ÂûãÔºå‰ΩÜÂÉè llama3:70b ÈÄôÊ®£ÁöÑÈñãÊ∫êÊ®°ÂûãÂú®ÈÅ©Áï∂ÁöÑÊèêÁ§∫Â∑•Á®ã‰∏ãÈ°ØÁ§∫Âá∫ÂâçÊôØ„ÄÇÁÇ∫‰∫ÜËÆìÈÄôÁ®ÆÊñπÊ≥ïÊòìÊñº‰ΩøÁî®ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ΩøÁî®ËÄÖÂèãÂñÑÁöÑÁ∂≤Ë∑Ø‰ªãÈù¢ÔºåËÆì‰ΩøÁî®ËÄÖÂèØ‰ª•Ëº∏ÂÖ•Ëá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢„ÄÅÊ™¢Ë¶ñÁî¢ÁîüÂíåÊõ¥Ê≠£ÁöÑ Cypher Êü•Ë©¢Ôºå‰∏¶È©óË≠âÁµêÊûúË∑ØÂæëÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÈÄôÁ®ÆÊ∑∑ÂêàÊñπÊ≥ïÊúâÊïàÂú∞Ëß£Ê±∫‰∫ÜË≥áÊñôÂ∑ÆË∑ùÂíåÂπªË¶∫Á≠âÂ∏∏Ë¶ãÂïèÈ°åÔºåÁÇ∫ÂïèÁ≠îÁ≥ªÁµ±Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂèØÈù†‰∏îÁõ¥ËßÄÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÊú¨ÊñáÁµêÊûúÁî¢ÁîüÁöÑÂéüÂßãÁ¢ºÂíå‰ΩøÁî®ËÄÖ‰ªãÈù¢ÁöÑÂéüÂßãÁ¢ºÂèØ‰ª•Âú®ÊàëÂÄëÁöÑ Git ÂÑ≤Â≠òÂ∫´‰∏≠ÊâæÂà∞Ôºöhttps://git.zib.de/lpusch/cyphergenkg-gui

##### **Refining Wikidata Taxonomy using Large Language Models**
2409.04056v1 by Yiwen Peng, Thomas Bonald, Mehwish Alam

Due to its collaborative nature, Wikidata is known to have a complex
taxonomy, with recurrent issues like the ambiguity between instances and
classes, the inaccuracy of some taxonomic paths, the presence of cycles, and
the high level of redundancy across classes. Manual efforts to clean up this
taxonomy are time-consuming and prone to errors or subjective decisions. We
present WiKC, a new version of Wikidata taxonomy cleaned automatically using a
combination of Large Language Models (LLMs) and graph mining techniques.
Operations on the taxonomy, such as cutting links or merging classes, are
performed with the help of zero-shot prompting on an open-source LLM. The
quality of the refined taxonomy is evaluated from both intrinsic and extrinsic
perspectives, on a task of entity typing for the latter, showing the practical
interest of WiKC.

ÊëòË¶ÅÔºöÁî±ÊñºÂÖ∂Âçî‰ΩúÊÄßË≥™ÔºåWikidata Â∑≤Áü•ÂÖ∑ÊúâË§áÈõúÁöÑÂàÜÈ°ûÊ≥ïÔºå‰∏¶ÊúâÈáçË§áÁôºÁîüÁöÑÂïèÈ°åÔºå‰æãÂ¶ÇÂØ¶‰æãÂíåÈ°ûÂà•‰πãÈñìÁöÑÊ≠ßÁæ©„ÄÅÊüê‰∫õÂàÜÈ°ûË∑ØÂæëÁöÑ‰∏çÊ∫ñÁ¢∫ÊÄß„ÄÅÂæ™Áí∞ÁöÑÂ≠òÂú®Ôºå‰ª•ÂèäÈ°ûÂà•‰πãÈñìÁöÑÈ´òÂÜóÈ§ò„ÄÇÊâãÂãïÊ∏ÖÁêÜÊ≠§ÂàÜÈ°ûÊ≥ïÁöÑÂ∑•‰ΩúÊó¢ËÄóÊôÇÂèàÂÆπÊòìÂá∫ÁèæÈåØË™§Êàñ‰∏ªËßÄÂà§Êñ∑„ÄÇÊàëÂÄëÊèêÂá∫ WiKCÔºåÈÄôÊòØ Wikidata ÂàÜÈ°ûÊ≥ïÁöÑÊñ∞ÁâàÊú¨Ôºå‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÂúñÂΩ¢ÊåñÊéòÊäÄË°ìËá™ÂãïÊ∏ÖÁêÜ„ÄÇÂàÜÈ°ûÊ≥ï‰∏äÁöÑÊìç‰ΩúÔºå‰æãÂ¶ÇÂâ™ÂàáÈèàÊé•ÊàñÂêà‰ΩµÈ°ûÂà•ÔºåÊòØÂú®ÈñãÊ∫ê LLM ‰∏äÂÄüÂä©Èõ∂Ê¨°ÊèêÁ§∫ÁöÑÂπ´Âä©‰∏ãÂü∑Ë°åÁöÑ„ÄÇÁ≤æÁÖâÂàÜÈ°ûÊ≥ïÁöÑÂìÅË≥™ÂæûÂÖßÂú®ÂíåÂ§ñÂú®ÁöÑËßÄÈªûÈÄ≤Ë°åË©ï‰º∞ÔºåÂú®ÂæåËÄÖÁöÑÂØ¶È´îÂàÜÂûã‰ªªÂãô‰∏äÔºåÈ°ØÁ§∫‰∫Ü WiKC ÁöÑÂØ¶ÈöõËààË∂£„ÄÇ

##### **Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features**
2409.04009v1 by Miao Fan, Yeqi Bai, Mingming Sun, Ping Li

Relation classification (RC) plays a pivotal role in both natural language
understanding and knowledge graph completion. It is generally formulated as a
task to recognize the relationship between two entities of interest appearing
in a free-text sentence. Conventional approaches on RC, regardless of feature
engineering or deep learning based, can obtain promising performance on
categorizing common types of relation leaving a large proportion of
unrecognizable long-tail relations due to insufficient labeled instances for
training. In this paper, we consider few-shot learning is of great practical
significance to RC and thus improve a modern framework of metric learning for
few-shot RC. Specifically, we adopt the large-margin ProtoNet with fine-grained
features, expecting they can generalize well on long-tail relations. Extensive
experiments were conducted by FewRel, a large-scale supervised few-shot RC
dataset, to evaluate our framework: LM-ProtoNet (FGF). The results demonstrate
that it can achieve substantial improvements over many baseline approaches.

ÊëòË¶ÅÔºöÈóú‰øÇÂàÜÈ°û (RC) Âú®Ëá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ÂíåÁü•Ë≠òÂúñË≠úÂÆåÊàê‰∏≠ÊâÆÊºîËëóÈóúÈçµËßíËâ≤„ÄÇÂÆÉÈÄöÂ∏∏Ë¢´Ë°®Ëø∞ÁÇ∫‰∏ÄÂÄã‰ªªÂãôÔºåÁî®ÊñºËæ®Ë≠òÂá∫ÁèæÂú®Ëá™Áî±ÊñáÂ≠óÂè•Â≠ê‰∏≠ÁöÑÂÖ©ÂÄãÊÑüËààË∂£ÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÁÑ°Ë´ñÊòØÂü∫ÊñºÁâπÂæµÂ∑•Á®ãÈÇÑÊòØÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂÇ≥Áµ± RC ÊñπÊ≥ïÔºåÈÉΩÂèØ‰ª•Â∞çÂ∏∏Ë¶ãÁöÑÈóú‰øÇÈ°ûÂûãÈÄ≤Ë°åÂàÜÈ°ûÔºåÂæûËÄåÁç≤ÂæóÊúâÂ∏åÊúõÁöÑÊïàËÉΩÔºå‰ΩÜÁî±ÊñºË®ìÁ∑¥Ê®ôÁ±§ÂØ¶‰æã‰∏çË∂≥ÔºåÂõ†Ê≠§ÁÑ°Ê≥ïËæ®Ë≠òÂá∫Â§ßÈáèÁöÑÈï∑Â∞æÈóú‰øÇ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË™çÁÇ∫Â∞ëÊ®£Êú¨Â≠∏ÁøíÂ∞ç RC ÂÖ∑ÊúâÈáçË¶ÅÁöÑÂØ¶Áî®ÊÑèÁæ©ÔºåÂõ†Ê≠§ÊîπÈÄ≤‰∫ÜÂ∫¶ÈáèÂ≠∏ÁøíÁöÑÁèæ‰ª£Ê°ÜÊû∂Ôºå‰ª•ÈÄ≤Ë°åÂ∞ëÊ®£Êú¨ RC„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊé°Áî®ÂÖ∑ÊúâÁ¥∞Á≤íÂ∫¶ÁâπÂæµÁöÑÂ§ßÈÇäË∑ù ProtoNetÔºåÊúüÊúõÂÆÉÂÄëËÉΩÂú®Èï∑Â∞æÈóú‰øÇ‰∏äÂæàÂ•ΩÂú∞Ê¶ÇÊã¨„ÄÇÊàëÂÄë‰ΩøÁî®Â§ßÂûãÁõ£Áù£Â∞ëÊ®£Êú¨ RC Ë≥áÊñôÈõÜ FewRel ÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•Ë©ï‰º∞ÊàëÂÄëÁöÑÊ°ÜÊû∂ÔºöLM-ProtoNet (FGF)„ÄÇÁµêÊûúË°®ÊòéÔºåÂÆÉÂèØ‰ª•ÊØîË®±Â§öÂü∫Á∑öÊñπÊ≥ïÁç≤ÂæóÈ°ØËëóÊîπÈÄ≤„ÄÇ

##### **Rx Strategist: Prescription Verification using LLM Agents System**
2409.03440v1 by Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, Huy Phan Thanh

To protect patient safety, modern pharmaceutical complexity demands strict
prescription verification. We offer a new approach - Rx Strategist - that makes
use of knowledge graphs and different search strategies to enhance the power of
Large Language Models (LLMs) inside an agentic framework. This multifaceted
technique allows for a multi-stage LLM pipeline and reliable information
retrieval from a custom-built active ingredient database. Different facets of
prescription verification, such as indication, dose, and possible drug
interactions, are covered in each stage of the pipeline. We alleviate the
drawbacks of monolithic LLM techniques by spreading reasoning over these
stages, improving correctness and reliability while reducing memory demands.
Our findings demonstrate that Rx Strategist surpasses many current LLMs,
achieving performance comparable to that of a highly experienced clinical
pharmacist. In the complicated world of modern medications, this combination of
LLMs with organized knowledge and sophisticated search methods presents a
viable avenue for reducing prescription errors and enhancing patient outcomes.

ÊëòË¶ÅÔºöÁÇ∫‰∫Ü‰øùË≠∑ÊÇ£ËÄÖÂÆâÂÖ®ÔºåÁèæ‰ª£Ëó•ÂìÅË§áÈõúÊÄßË¶ÅÊ±ÇÂö¥Ê†ºÁöÑËôïÊñπÈ©óË≠â„ÄÇÊàëÂÄëÊèê‰æõ‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ï - Rx Strategist - ÂÆÉÂà©Áî®Áü•Ë≠òÂúñË≠úÂíå‰∏çÂêåÁöÑÊêúÂ∞ãÁ≠ñÁï•‰æÜÂ¢ûÂº∑‰ª£ÁêÜÊû∂ÊßãÂÖßÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂäüËÉΩ„ÄÇÈÄôÁ®ÆÂ§öÊñπÈù¢ÁöÑÊäÄË°ìÂÖÅË®±Â§öÈöéÊÆµÁöÑ LLM ÁÆ°Á∑öÂíåÂæûËá™Ë®Ç‰∏ªÂãïÊàêÂàÜË≥áÊñôÂ∫´‰∏≠ÂèØÈù†Âú∞Êì∑ÂèñË≥áË®ä„ÄÇËôïÊñπÈ©óË≠âÁöÑ‰∏çÂêåÈù¢ÂêëÔºå‰æãÂ¶ÇÈÅ©ÊáâÁóá„ÄÅÂäëÈáèÂíåÂèØËÉΩÁöÑËó•Áâ©‰∫§‰∫í‰ΩúÁî®ÔºåÈÉΩÂú®ÁÆ°Á∑öÁöÑÊØèÂÄãÈöéÊÆµ‰∏≠Ê∂µËìã„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÊé®ÁêÜÂàÜÊï£Âú®ÈÄô‰∫õÈöéÊÆµ‰æÜÊ∏õËºïÂñÆ‰∏Ä LLM ÊäÄË°ìÁöÑÁº∫ÈªûÔºåÂêåÊôÇÊèêÈ´òÊ≠£Á¢∫ÊÄßÂíåÂèØÈù†ÊÄßÔºå‰∏¶Ê∏õÂ∞ëË®òÊÜ∂È´îÈúÄÊ±Ç„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåRx Strategist Ë∂ÖË∂äË®±Â§öÁèæÊúâÁöÑ LLMÔºåÈÅîÂà∞ËàáÁ∂ìÈ©óË±êÂØåÁöÑËá®Â∫äËó•ÂäëÂ∏´Áõ∏Áï∂ÁöÑË°®Áèæ„ÄÇÂú®Áèæ‰ª£Ëó•Áâ©Ë§áÈõúÁöÑ‰∏ñÁïå‰∏≠ÔºåÈÄôÁ®ÆÂ∞á LLM ËàáÊúâÁµÑÁπîÁöÑÁü•Ë≠òÂíåÂÖàÈÄ≤ÊêúÂ∞ãÊñπÊ≥ïÁõ∏ÁµêÂêàÔºåÁÇ∫Ê∏õÂ∞ëËôïÊñπÈåØË™§ÂíåÊîπÂñÑÊÇ£ËÄÖÈ†êÂæåÊèê‰æõ‰∫ÜÂèØË°åÁöÑÈÄîÂæë„ÄÇ

##### **iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models**
2409.03284v1 by Yassir Lairgi, Ludovic Moncla, R√©my Cazabet, Khalid Benabdeslem, Pierre Cl√©au

Most available data is unstructured, making it challenging to access valuable
information. Automatically building Knowledge Graphs (KGs) is crucial for
structuring data and making it accessible, allowing users to search for
information effectively. KGs also facilitate insights, inference, and
reasoning. Traditional NLP methods, such as named entity recognition and
relation extraction, are key in information retrieval but face limitations,
including the use of predefined entity types and the need for supervised
learning. Current research leverages large language models' capabilities, such
as zero- or few-shot learning. However, unresolved and semantically duplicated
entities and relations still pose challenges, leading to inconsistent graphs
and requiring extensive post-processing. Additionally, most approaches are
topic-dependent. In this paper, we propose iText2KG, a method for incremental,
topic-independent KG construction without post-processing. This plug-and-play,
zero-shot method is applicable across a wide range of KG construction scenarios
and comprises four modules: Document Distiller, Incremental Entity Extractor,
Incremental Relation Extractor, and Graph Integrator and Visualization. Our
method demonstrates superior performance compared to baseline methods across
three scenarios: converting scientific papers to graphs, websites to graphs,
and CVs to graphs.

ÊëòË¶ÅÔºöÂ§ßÈÉ®ÂàÜÂèØÁî®Ë≥áÊñôÁÇ∫ÈùûÁµêÊßãÂåñÔºåÈÄô‰ΩøÂæóÂ≠òÂèñÊúâÂÉπÂÄºÁöÑË≥áË®äËÆäÂæóÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇËá™ÂãïÂª∫Á´ãÁü•Ë≠òÂúñË≠ú (KG) Â∞çÊñºÁµêÊßãÂåñË≥áÊñôÂíåËÆìË≥áÊñôÊòìÊñºÂ≠òÂèñËá≥ÈóúÈáçË¶ÅÔºåËÆì‰ΩøÁî®ËÄÖËÉΩÂ§†ÊúâÊïàÂú∞ÊêúÂ∞ãË≥áË®ä„ÄÇKG ‰πü‰øÉÈÄ≤Ë¶ãËß£„ÄÅÊé®Ë´ñÂíåÊé®ÁêÜ„ÄÇÂÇ≥Áµ±ÁöÑ NLP ÊñπÊ≥ïÔºå‰æãÂ¶ÇÂëΩÂêçÂØ¶È´îËæ®Ë≠òÂíåÈóú‰øÇËêÉÂèñÔºåÂú®Ë≥áË®äÊ™¢Á¥¢‰∏≠ÊòØÈóúÈçµÔºå‰ΩÜÈù¢Ëá®ÈôêÂà∂ÔºåÂåÖÊã¨‰ΩøÁî®È†êÂÆöÁæ©ÁöÑÂØ¶È´îÈ°ûÂûãÂíåÈúÄË¶ÅÁõ£Áù£ÂºèÂ≠∏Áøí„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂ÊâÄÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑËÉΩÂäõÔºå‰æãÂ¶ÇÈõ∂Ê¨°ÊàñÂ∞ëÊ¨°Â≠∏Áøí„ÄÇÁÑ∂ËÄåÔºåÊú™Ëß£Ê±∫ÂíåË™ûÁæ©ÈáçË§áÁöÑÂØ¶È´îÂíåÈóú‰øÇ‰ªçÁÑ∂ÊßãÊàêÊåëÊà∞ÔºåÂ∞éËá¥ÂúñÂΩ¢‰∏ç‰∏ÄËá¥ÔºåÈúÄË¶ÅÂª£Ê≥õÁöÑÂæåËôïÁêÜ„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏ÊñπÊ≥ïÈÉΩ‰æùË≥¥Êñº‰∏ªÈ°å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ iText2KGÔºå‰∏ÄÁ®ÆÁî®ÊñºÊº∏ÈÄ≤Âºè„ÄÅËàá‰∏ªÈ°åÁÑ°ÈóúÁöÑ KG Âª∫ÊßãÊñπÊ≥ïÔºåÁÑ°ÈúÄÂæåËôïÁêÜ„ÄÇÈÄôÁ®ÆÂç≥ÊèíÂç≥Áî®„ÄÅÈõ∂Ê¨°ÁöÑÊñπÊ≥ïÈÅ©Áî®ÊñºÂª£Ê≥õÁöÑ KG Âª∫ÊßãÂ†¥ÊôØÔºå‰∏¶ÂåÖÂê´ÂõõÂÄãÊ®°ÁµÑÔºöÊñá‰ª∂Á≤æÈ§æÂô®„ÄÅÊº∏ÈÄ≤ÂºèÂØ¶È´îËêÉÂèñÂô®„ÄÅÊº∏ÈÄ≤ÂºèÈóú‰øÇËêÉÂèñÂô®Ôºå‰ª•ÂèäÂúñÂΩ¢Êï¥ÂêàÂô®ÂíåË¶ñË¶∫ÂåñÂô®„ÄÇËàáÂü∫Á∑öÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®‰∏âÁ®ÆÂ†¥ÊôØ‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩÔºöÂ∞áÁßëÂ≠∏Ë´ñÊñáËΩâÊèõÁÇ∫ÂúñÂΩ¢„ÄÅÁ∂≤Á´ôËΩâÊèõÁÇ∫ÂúñÂΩ¢Ôºå‰ª•ÂèäÂ±•Ê≠∑ËΩâÊèõÁÇ∫ÂúñÂΩ¢„ÄÇ

##### **GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding**
2409.03258v1 by Yukun Cao, Shuo Han, Zengyi Gao, Zezhong Ding, Xike Xie, S. Kevin Zhou

Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ËôïÁêÜÂúñÂΩ¢ÁöÑËÉΩÂäõÔºå‰ΩÜÂÆÉÂÄëÂú®ÈÄèÈÅéÂúñÂΩ¢ÊèèËø∞Â∫èÂàóÊèêÁ§∫ÁêÜËß£ÂúñÂΩ¢ÁµêÊßãË≥áË®äÊôÇÊúÉÈÅáÂà∞Âõ∞Èõ£ÔºåÁâπÂà•ÊòØÂú®ÂúñÂΩ¢Â§ßÂ∞èÂ¢ûÂä†ÊôÇ„ÄÇÊàëÂÄëÂ∞áÊ≠§ÊåëÊà∞Ê≠∏Âõ†Êñº LLM Âú®ÂúñÂΩ¢ÊèèËø∞Â∫èÂàó‰∏≠‰∏çÂêå‰ΩçÁΩÆÁöÑË®òÊÜ∂ÂäõË°®Áèæ‰∏çÂùáÔºåÁ®±ÁÇ∫„Äå‰ΩçÁΩÆÂÅèË™§„Äç„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü GraphInsightÔºå‰∏ÄÂÄãÊó®Âú®ÊîπÂñÑ LLM Â∞çÂ∑®ËßÄÂíåÂæÆËßÄÂ±§Á¥öÂúñÂΩ¢Ë≥áË®äÁêÜËß£ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇGraphInsight ‰ª•ÂÖ©ÂÄãÈóúÈçµÁ≠ñÁï•ÁÇ∫Âü∫Á§éÔºö1) Â∞áÈóúÈçµÂúñÂΩ¢Ë≥áË®äÊîæÁΩÆÂú® LLM Â±ïÁèæËºÉÂº∑Ë®òÊÜ∂ÂäõË°®ÁèæÁöÑ‰ΩçÁΩÆÔºå‰ª•Âèä 2) Ë™øÊü•‰∏ÄÂÄãÂèóÂà∞Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÂïüÁôºÁöÑ„ÄÅÈáùÂ∞çË®òÊÜ∂ÂäõË°®ÁèæËºÉÂº±ÂçÄÂüüÁöÑËºïÈáèÁ¥öÂ§ñÈÉ®Áü•Ë≠òÂ∫´„ÄÇÊ≠§Â§ñÔºåGraphInsight Êé¢Á¥¢Â∞áÈÄôÂÖ©ÂÄãÁ≠ñÁï•Êï¥ÂêàÂà∞ LLM ‰ª£ÁêÜÁ®ãÂ∫è‰∏≠Ôºå‰ª•ËôïÁêÜÈúÄË¶ÅÂ§öÊ≠•È©üÊé®ÁêÜÁöÑË§áÂêàÂúñÂΩ¢‰ªªÂãô„ÄÇÂú®ÂÖ∑ÊúâÂª£Ê≥õË©ïÈáè‰ªªÂãôÁöÑÂü∫Ê∫ñ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶Ë≠âÁ†îÁ©∂È°ØÁ§∫ÔºåGraphInsight Âú®ÁêÜËß£ÂêÑÁ®ÆÂ§ßÂ∞èÁöÑÂúñÂΩ¢ÁµêÊßãÊñπÈù¢ÔºåÊòéÈ°ØÂÑ™ÊñºÊâÄÊúâÂÖ∂‰ªñÂúñÂΩ¢ÊèèËø∞ÊñπÊ≥ïÔºà‰æãÂ¶ÇÊèêÁ§∫ÊäÄÂ∑ßÂíåÈáçÊñ∞ÊéíÂ∫èÁ≠ñÁï•Ôºâ„ÄÇ

##### **Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models**
2409.03155v1 by Jie Ma, Zhitao Gao, Qi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei, Jing Tao, Lingyun Song, Jun Liu, Chen Zhang, Lizhen Cui

Large Language Models (LLMs) may suffer from hallucinations in real-world
applications due to the lack of relevant knowledge. In contrast, knowledge
graphs encompass extensive, multi-relational structures that store a vast array
of symbolic facts. Consequently, integrating LLMs with knowledge graphs has
been extensively explored, with Knowledge Graph Question Answering (KGQA)
serving as a critical touchstone for the integration. This task requires LLMs
to answer natural language questions by retrieving relevant triples from
knowledge graphs. However, existing methods face two significant challenges:
\textit{excessively long reasoning paths distracting from the answer
generation}, and \textit{false-positive relations hindering the path
refinement}. In this paper, we propose an iterative interactive KGQA framework
that leverages the interactive learning capabilities of LLMs to perform
reasoning and Debating over Graphs (DoG). Specifically, DoG employs a
subgraph-focusing mechanism, allowing LLMs to perform answer trying after each
reasoning step, thereby mitigating the impact of lengthy reasoning paths. On
the other hand, DoG utilizes a multi-role debate team to gradually simplify
complex questions, reducing the influence of false-positive relations. This
debate mechanism ensures the reliability of the reasoning process. Experimental
results on five public datasets demonstrate the effectiveness and superiority
of our architecture. Notably, DoG outperforms the state-of-the-art method ToG
by 23.7\% and 9.1\% in accuracy on WebQuestions and GrailQA, respectively.
Furthermore, the integration experiments with various LLMs on the mentioned
datasets highlight the flexibility of DoG. Code is available at
\url{https://github.com/reml-group/DoG}.

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî±ÊñºÁº∫‰πèÁõ∏ÈóúÁü•Ë≠òÔºåÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÂèØËÉΩÊúÉÁî¢ÁîüÂπªË¶∫„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºåÁü•Ë≠òÂúñË≠úÂåÖÂê´Âª£Ê≥õÁöÑÂ§öÈáçÈóú‰øÇÁµêÊßãÔºåÂÑ≤Â≠òÂ§ßÈáèÁ¨¶Ëôü‰∫ãÂØ¶„ÄÇÂõ†Ê≠§ÔºåÂ∞á LLM ËàáÁü•Ë≠òÂúñË≠úÊï¥ÂêàÂ∑≤Âª£Ê≥õÊé¢Ë®éÔºåÂÖ∂‰∏≠Áü•Ë≠òÂúñË≠úÂïèÈ°åËß£Á≠î (KGQA) ÊàêÁÇ∫Êï¥ÂêàÁöÑÈáçË¶ÅË©¶ÈáëÁü≥„ÄÇÊ≠§‰ªªÂãôË¶ÅÊ±Ç LLM ÈÄèÈÅéÂæûÁü•Ë≠òÂúñË≠ú‰∏≠Êì∑ÂèñÁõ∏Èóú‰∏âÂÖÉÁµÑ‰æÜÂõûÁ≠îËá™ÁÑ∂Ë™ûË®ÄÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÈù¢Ëá®ÂÖ©È†ÖÈáçÂ§ßÊåëÊà∞Ôºö\textit{ÈÅéÈï∑ÁöÑÊé®ÁêÜË∑ØÂæëÊúÉÂàÜÊï£ÂõûÁ≠îÁî¢Áîü}Ôºå‰ª•Âèä\textit{ÈåØË™§Ê≠£ÂêëÈóú‰øÇÈòªÁ§ôË∑ØÂæëÁ≤æÁÖâ}„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂèçË¶Ü‰∫íÂãïÁöÑ KGQA Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî® LLM ÁöÑ‰∫íÂãïÂ≠∏ÁøíËÉΩÂäõ‰æÜÂü∑Ë°åÊé®ÁêÜÂíåÂúñÂΩ¢ËæØË´ñ (DoG)„ÄÇÂÖ∑È´î‰æÜË™™ÔºåDoG Êé°Áî®Â≠êÂúñËÅöÁÑ¶Ê©üÂà∂ÔºåÂÖÅË®± LLM Âú®ÊØèÂÄãÊé®ÁêÜÊ≠•È©üÂæåÂü∑Ë°åÁ≠îÊ°àÂòóË©¶ÔºåÂæûËÄåÊ∏õËºïÂÜóÈï∑Êé®ÁêÜË∑ØÂæëÁöÑÂΩ±Èüø„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåDoG Âà©Áî®Â§öËßíËâ≤ËæØË´ñÂ∞èÁµÑÈÄêÊº∏Á∞°ÂåñË§áÈõúÂïèÈ°åÔºåÊ∏õÂ∞ëÈåØË™§Ê≠£ÂêëÈóú‰øÇÁöÑÂΩ±Èüø„ÄÇÈÄôÁ®ÆËæØË´ñÊ©üÂà∂Á¢∫‰øù‰∫ÜÊé®ÁêÜÈÅéÁ®ãÁöÑÂèØÈù†ÊÄß„ÄÇÂú®‰∫îÂÄãÂÖ¨ÂÖ±Êï∏ÊìöÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÊû∂ÊßãÁöÑÊúâÊïàÊÄßÂíåÂÑ™Ë∂äÊÄß„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåDoG Âú® WebQuestions Âíå GrailQA ‰∏äÁöÑÊ∫ñÁ¢∫Â∫¶ÂàÜÂà•ÊØîÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï ToG È´òÂá∫ 23.7% Âíå 9.1%„ÄÇÊ≠§Â§ñÔºåÂú®‰∏äËø∞Êï∏ÊìöÈõÜ‰∏äËàáÂêÑÁ®Æ LLM ÁöÑÊï¥ÂêàÂØ¶È©óÁ™ÅÈ°Ø‰∫Ü DoG ÁöÑÈùàÊ¥ªÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú®\url{https://github.com/reml-group/DoG}ÂèñÂæó„ÄÇ</paragraph>

##### **Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification**
2409.02481v1 by Junyoung Lee, Ninad Dixit, Kaustav Chakrabarti, S. Supraja

Effective question classification is crucial for AI-driven educational tools,
enabling adaptive learning systems to categorize questions by skill area,
difficulty level, and competence. This classification not only supports
educational diagnostics and analytics but also enhances complex tasks like
information retrieval and question answering by associating questions with
relevant categories. Traditional methods, often based on word embeddings and
conventional classifiers, struggle to capture the nuanced relationships in
natural language, leading to suboptimal performance. To address this, we
propose a novel approach leveraging graph convolutional networks (GCNs), named
Phrase Question-Graph Convolutional Network (PQ-GCN) to better model the
inherent structure of questions. By representing questions as graphs -- where
nodes signify words or phrases and edges denote syntactic or semantic
relationships -- our method allows GCNs to learn from the interconnected nature
of language more effectively. Additionally, we explore the incorporation of
phrase-based features to enhance classification accuracy, especially in
low-resource settings. Our findings demonstrate that GCNs, augmented with these
features, offer a promising solution for more accurate and context-aware
question classification, bridging the gap between graph neural network research
and practical educational applications.

ÊëòË¶ÅÔºöÊúâÊïàÁöÑÂïèÈ°åÂàÜÈ°ûÂ∞çÊñº AI È©ÖÂãïÁöÑÊïôËÇ≤Â∑•ÂÖ∑Ëá≥ÈóúÈáçË¶ÅÔºå
ËÆìÈÅ©ÊáâÊÄßÂ≠∏ÁøíÁ≥ªÁµ±ËÉΩ‰æùÊìöÊäÄËÉΩÈ†òÂüü„ÄÅ
Èõ£Â∫¶Á≠âÁ¥öÂíåËÉΩÂäõÂ∞çÂïèÈ°åÈÄ≤Ë°åÂàÜÈ°û„ÄÇÈÄôÁ®ÆÂàÜÈ°û‰∏çÂÉÖÊîØÊè¥
ÊïôËÇ≤Ë®∫Êñ∑ÂíåÂàÜÊûêÔºåÈÇÑËÉΩÈÄèÈÅéÂ∞áÂïèÈ°åËàá
Áõ∏ÈóúÈ°ûÂà•ÈóúËÅØËµ∑‰æÜÔºåÂ¢ûÂº∑Ë≥áË®äÊ™¢Á¥¢ÂíåÂïèÈ°åËß£Á≠îÁ≠âË§áÈõú‰ªªÂãô„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÈÄöÂ∏∏Âª∫Á´ãÂú®Ë©ûÂµåÂÖ•Âíå
ÂÇ≥Áµ±ÂàÜÈ°ûÂô®‰∏äÔºåÈõ£‰ª•ÊçïÊçâËá™ÁÑ∂Ë™ûË®Ä‰∏≠ÁöÑÁ¥∞ÂæÆÈóú‰øÇÔºåÂ∞éËá¥Ê¨°‰Ω≥ÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄë
ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂà©Áî®ÂúñÂΩ¢Âç∑Á©çÁ∂≤Ë∑Ø (GCN)ÔºåÁ®±ÁÇ∫
Phrase Question-Graph Convolutional Network (PQ-GCN) ‰æÜÊõ¥Â•ΩÂú∞Âª∫Ê®°ÂïèÈ°åÁöÑÂÖßÂú®ÁµêÊßã„ÄÇÈÄèÈÅéÂ∞áÂïèÈ°åË°®Á§∫ÁÇ∫ÂúñÂΩ¢‚Äî‚ÄîÂÖ∂‰∏≠
ÁØÄÈªûË°®Á§∫Ë©ûÊàñË©ûÁµÑÔºåÈÇäÁ∑£Ë°®Á§∫Ë™ûÊ≥ïÊàñË™ûÁæ©Èóú‰øÇ‚Äî‚ÄîÊàëÂÄëÁöÑÊ®°ÂûãÂÖÅË®± GCN Êõ¥ÊúâÊïàÂú∞ÂæûË™ûË®ÄÁöÑÁõ∏‰∫íÈÄ£ÁµêÊÄßË≥™‰∏≠Â≠∏Áøí„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Á¥¢‰∫ÜÊï¥Âêà
Âü∫ÊñºË©ûÁµÑÁöÑÁâπÂæµ‰ª•Â¢ûÂº∑ÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÔºåÁâπÂà•ÊòØÂú®
‰ΩéË≥áÊ∫êË®≠ÂÆö‰∏≠„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåGCN Âú®ÈÄô‰∫õ
ÁâπÂæµÁöÑÂ¢ûÂº∑‰∏ãÔºåÁÇ∫Êõ¥Ê∫ñÁ¢∫‰∏îÂÖ∑ÂÇôÊÉÖÂ¢ÉÊÑüÁü•ËÉΩÂäõÁöÑÂïèÈ°åÂàÜÈ°ûÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁ∏ÆÂ∞è‰∫ÜÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÁ†îÁ©∂
ËàáÂØ¶ÈöõÊïôËÇ≤ÊáâÁî®‰πãÈñìÁöÑÂ∑ÆË∑ù„ÄÇ

##### **Multi-modal Situated Reasoning in 3D Scenes**
2409.02389v1 by Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, Siyuan Huang

Situation awareness is essential for understanding and reasoning about 3D
scenes in embodied AI agents. However, existing datasets and benchmarks for
situated understanding are limited in data modality, diversity, scale, and task
scope. To address these limitations, we propose Multi-modal Situated Question
Answering (MSQA), a large-scale multi-modal situated reasoning dataset,
scalably collected leveraging 3D scene graphs and vision-language models (VLMs)
across a diverse range of real-world 3D scenes. MSQA includes 251K situated
question-answering pairs across 9 distinct question categories, covering
complex scenarios within 3D scenes. We introduce a novel interleaved
multi-modal input setting in our benchmark to provide text, image, and point
cloud for situation and question description, resolving ambiguity in previous
single-modality convention (e.g., text). Additionally, we devise the
Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models'
situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN
highlight the limitations of existing vision-language models and underscore the
importance of handling multi-modal interleaved inputs and situation modeling.
Experiments on data scaling and cross-domain transfer further demonstrate the
efficacy of leveraging MSQA as a pre-training dataset for developing more
powerful situated reasoning models.

ÊëòË¶ÅÔºöÊÉÖÂ¢ÉÊÑüÁü•Â∞çÊñºÁêÜËß£ÂíåÊé®ÁêÜÂÖ∑Ë∫´ AI ‰ª£ÁêÜ‰∏≠ÁöÑ 3D Â†¥ÊôØËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑË≥áÊñôÈõÜÂíåÂü∫Ê∫ñÂú®Ë≥áÊñôÊ®°ÊÖã„ÄÅÂ§öÊ®£ÊÄß„ÄÅË¶èÊ®°Âíå‰ªªÂãôÁØÑÂúçÊñπÈù¢Â∞çÊñºÊÉÖÂ¢ÉÁêÜËß£‰æÜË™™ÊòØÊúâÈôêÁöÑ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂ§öÊ®°ÊÖãÊÉÖÂ¢ÉÂïèÁ≠î (MSQA)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂ§ßÂûãÂ§öÊ®°ÊÖãÊÉÖÂ¢ÉÊé®ÁêÜË≥áÊñôÈõÜÔºåÂèØÈÄèÈÅéÂà©Áî® 3D Â†¥ÊôØÂúñÂíåË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) Âú®ÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïå 3D Â†¥ÊôØ‰∏≠ÈÄ≤Ë°åÂèØÊì¥ÂÖÖÊî∂ÈõÜ„ÄÇMSQA ÂåÖÂê´ 251K ÂÄãÊÉÖÂ¢ÉÂïèÁ≠îÂ∞çÔºåÊ∂µËìã 9 ÂÄã‰∏çÂêåÁöÑÂïèÈ°åÈ°ûÂà•ÔºåÊ∂µËìã 3D Â†¥ÊôØ‰∏≠ÁöÑË§áÈõúÂ†¥ÊôØ„ÄÇÊàëÂÄëÂú®Âü∫Ê∫ñ‰∏≠ÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ‰∫§ÈåØÂ§öÊ®°ÊÖãËº∏ÂÖ•Ë®≠ÂÆöÔºå‰ª•Êèê‰æõÊñáÂ≠ó„ÄÅÂΩ±ÂÉèÂíåÈªûÈõ≤ÔºåÁî®ÊñºÊÉÖÂ¢ÉÂíåÂïèÈ°åÊèèËø∞ÔºåËß£Ê±∫‰ª•ÂâçÂñÆ‰∏ÄÊ®°ÊÖãÊÖ£‰æãÔºà‰æãÂ¶ÇÊñáÂ≠óÔºâ‰∏≠ÁöÑÊ≠ßÁæ©„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∫ÜÂ§öÊ®°ÊÖãÊÉÖÂ¢É‰∏ã‰∏ÄÊ≠•Â∞éËà™ (MSNN) Âü∫Ê∫ñÔºå‰ª•Ë©ï‰º∞Ê®°ÂûãÁöÑÂ∞éËà™ÊÉÖÂ¢ÉÊé®ÁêÜ„ÄÇMSQA Âíå MSNN ÁöÑÁ∂úÂêàË©ï‰º∞Á™ÅÈ°Ø‰∫ÜÁèæÊúâË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÈôêÂà∂Ôºå‰∏¶Âº∑Ë™ø‰∫ÜËôïÁêÜÂ§öÊ®°ÊÖã‰∫§ÈåØËº∏ÂÖ•ÂíåÊÉÖÂ¢ÉÂª∫Ê®°ÁöÑÈáçË¶ÅÊÄß„ÄÇË≥áÊñôÊì¥ÂÖÖÂíåË∑®È†òÂüüËΩâÁßªÁöÑÂØ¶È©óÈÄ≤‰∏ÄÊ≠•Ë≠âÊòé‰∫ÜÂà©Áî® MSQA ‰ΩúÁÇ∫È†êË®ìÁ∑¥Ë≥áÊñôÈõÜ‰æÜÈñãÁôºÊõ¥Âº∑Â§ßÁöÑÊÉÖÂ¢ÉÊé®ÁêÜÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Grounding Language Models in Autonomous Loco-manipulation Tasks**
2409.01326v1 by Jin Wang, Nikos Tsagarakis

Humanoid robots with behavioral autonomy have consistently been regarded as
ideal collaborators in our daily lives and promising representations of
embodied intelligence. Compared to fixed-based robotic arms, humanoid robots
offer a larger operational space while significantly increasing the difficulty
of control and planning. Despite the rapid progress towards general-purpose
humanoid robots, most studies remain focused on locomotion ability with few
investigations into whole-body coordination and tasks planning, thus limiting
the potential to demonstrate long-horizon tasks involving both mobility and
manipulation under open-ended verbal instructions. In this work, we propose a
novel framework that learns, selects, and plans behaviors based on tasks in
different scenarios. We combine reinforcement learning (RL) with whole-body
optimization to generate robot motions and store them into a motion library. We
further leverage the planning and reasoning features of the large language
model (LLM), constructing a hierarchical task graph that comprises a series of
motion primitives to bridge lower-level execution with higher-level planning.
Experiments in simulation and real-world using the CENTAURO robot show that the
language model based planner can efficiently adapt to new loco-manipulation
tasks, demonstrating high autonomy from free-text commands in unstructured
scenes.

ÊëòË¶ÅÔºöÂÖ∑ÊúâË°åÁÇ∫Ëá™‰∏ªÊ¨äÁöÑ‰∫∫ÂΩ¢Ê©üÂô®‰∫∫‰∏ÄÁõ¥Ë¢´Ë¶ñÁÇ∫ÊàëÂÄëÊó•Â∏∏ÁîüÊ¥ª‰∏≠ÁêÜÊÉ≥ÁöÑÂêà‰ΩúËÄÖÔºå‰πüÊòØÂÖ∑È´îÊô∫ËÉΩÁöÑÊúâÂ∏åÊúõÁöÑ‰ª£Ë°®„ÄÇËàáÂõ∫ÂÆöÂºèÊ©üÂô®ÊâãËáÇÁõ∏ÊØîÔºå‰∫∫ÂΩ¢Ê©üÂô®‰∫∫Êèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÊìç‰ΩúÁ©∫ÈñìÔºåÂêåÊôÇÈ°ØËëóÂ¢ûÂä†‰∫ÜÊéßÂà∂ÂíåË¶èÂäÉÁöÑÈõ£Â∫¶„ÄÇÂÑòÁÆ°ÊúùËëóÈÄöÁî®‰∫∫ÂΩ¢Ê©üÂô®‰∫∫Âø´ÈÄüÁôºÂ±ïÔºå‰ΩÜÂ§ßÂ§öÊï∏Á†îÁ©∂‰ªçÁÑ∂ÈõÜ‰∏≠Âú®ÈÅãÂãïËÉΩÂäõ‰∏äÔºåÂæàÂ∞ëÁ†îÁ©∂ÂÖ®Ë∫´ÂçîË™øÂíå‰ªªÂãôË¶èÂäÉÔºåÂæûËÄåÈôêÂà∂‰∫ÜÂ±ïÁ§∫Ê∂âÂèäÁßªÂãïÂíåÊìç‰ΩúÁöÑÈï∑Êúü‰ªªÂãôÁöÑÊΩõÂäõÔºåÂêåÊôÇÈÇÑËÉΩÊé•ÂèóÈñãÊîæÂºèÂè£È†≠Êåá‰ª§„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊ°ÜÊû∂ÔºåË©≤Ê°ÜÊû∂ÂèØ‰ª•Ê†πÊìö‰∏çÂêåÂ†¥ÊôØ‰∏≠ÁöÑ‰ªªÂãôÂ≠∏Áøí„ÄÅÈÅ∏ÊìáÂíåË¶èÂäÉË°åÁÇ∫„ÄÇÊàëÂÄëÂ∞áÂº∑ÂåñÂ≠∏Áøí (RL) ËàáÂÖ®Ë∫´ÂÑ™ÂåñÁõ∏ÁµêÂêàÔºå‰ª•ÁîüÊàêÊ©üÂô®‰∫∫Âãï‰Ωú‰∏¶Â∞áÂÖ∂Â≠òÂÑ≤Âà∞Âãï‰ΩúÂ∫´‰∏≠„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑË¶èÂäÉÂíåÊé®ÁêÜÂäüËÉΩÔºåÊßãÂª∫‰∫Ü‰∏ÄÂÄãÂàÜÂ±§‰ªªÂãôÂúñÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏ÄÁ≥ªÂàóÈÅãÂãïÂéüË™ûÔºå‰ª•Ê©ãÊé•‰ΩéÁ¥öÂü∑Ë°åÂíåÈ´òÁ¥öË¶èÂäÉ„ÄÇÂú®Ê®°Êì¨Âíå‰ΩøÁî® CENTAURO Ê©üÂô®‰∫∫ÁöÑÁèæÂØ¶‰∏ñÁïå‰∏≠ÁöÑÂØ¶È©óË°®ÊòéÔºåÂü∫ÊñºË™ûË®ÄÊ®°ÂûãÁöÑË¶èÂäÉÂô®ÂèØ‰ª•ÊúâÊïàÈÅ©ÊáâÊñ∞ÁöÑÈÅãÂãïÊìç‰Ωú‰ªªÂãôÔºåË≠âÊòé‰∫ÜÂú®ÈùûÁµêÊßãÂåñÂ†¥ÊôØ‰∏≠ÂæûËá™Áî±ÊñáÊú¨ÂëΩ‰ª§‰∏≠Áç≤ÂæóÁöÑÈ´òÂ∫¶Ëá™‰∏ªÊÄß„ÄÇ

##### **LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning**
2409.01145v1 by Haoran Yang, Xiangyu Zhao, Sirui Huang, Qing Li, Guandong Xu

Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised
graph learning that has attracted attention across various application
scenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet
to be explored. Because conventional augmentation techniques like feature
embedding masking cannot directly process textual attributes on TAGs. A naive
strategy for applying GCL to TAGs is to encode the textual attributes into
feature embeddings via a language model and then feed the embeddings into the
following GCL module for processing. Such a strategy faces three key
challenges: I) failure to avoid information loss, II) semantic loss during the
text encoding phase, and III) implicit augmentation constraints that lead to
uncontrollable and incomprehensible results. In this paper, we propose a novel
GCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to
produce textual augmentations and LLMs' powerful natural language processing
(NLP) abilities to address the three limitations aforementioned to pave the way
for applying GCL to TAG tasks. Extensive experiments on four high-quality TAG
datasets illustrate the superiority of the proposed LATEX-GCL method. The
source codes and datasets are released to ease the reproducibility, which can
be accessed via this link: https://anonymous.4open.science/r/LATEX-GCL-0712.

ÊëòË¶ÅÔºöÂúñÂΩ¢Â∞çÊØîÂ≠∏Áøí (GCL) ÊòØËá™Áõ£Áù£ÂúñÂΩ¢Â≠∏ÁøíÁöÑÂº∑Â§ßÁØÑ‰æãÔºåÂ∑≤Âú®ÂêÑÁ®ÆÊáâÁî®Â†¥ÊôØ‰∏≠ÂºïËµ∑ÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåGCL Â∞çÊñºÂú®ÊñáÊú¨Ë®ªËß£ÂúñÂΩ¢ (TAG) ‰∏äÂ≠∏ÁøíÂ∞öÊú™Ë¢´Êé¢Ë®é„ÄÇÂõ†ÁÇ∫ÁâπÂæµÂµåÂÖ•ÈÅÆÁΩ©Á≠âÂÇ≥Áµ±Êì¥ÂÖÖÊäÄË°ìÁÑ°Ê≥ïÁõ¥Êé•ËôïÁêÜ TAG ‰∏äÁöÑÊñáÊú¨Â±¨ÊÄß„ÄÇÂ∞á GCL ÊáâÁî®Êñº TAG ÁöÑ‰∏ÄÁ®ÆÂ§©ÁúüÁ≠ñÁï•ÊòØÈÄöÈÅéË™ûË®ÄÊ®°ÂûãÂ∞áÊñáÊú¨Â±¨ÊÄßÁ∑®Á¢ºÂà∞ÁâπÂæµÂµåÂÖ•‰∏≠ÔºåÁÑ∂ÂæåÂ∞áÂµåÂÖ•Ëº∏ÂÖ•ÂæåÁ∫åÁöÑ GCL Ê®°ÁµÑÈÄ≤Ë°åËôïÁêÜ„ÄÇÈÄôÁ®ÆÁ≠ñÁï•Èù¢Ëá®‰∏âÂÄãÈóúÈçµÊåëÊà∞ÔºöI) ÁÑ°Ê≥ïÈÅøÂÖçË≥áË®äÈÅ∫Â§±ÔºåII) Âú®ÊñáÊú¨Á∑®Á¢ºÈöéÊÆµÁôºÁîüË™ûÁæ©ÈÅ∫Â§±Ôºå‰ª•Âèä III) Â∞éËá¥ÁÑ°Ê≥ïÊéßÂà∂‰∏îÈõ£‰ª•ÁêÜËß£ÁµêÊûúÁöÑÈö±ÂºèÊì¥ÂÖÖÁ¥ÑÊùü„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ LATEX-GCL ÁöÑÊñ∞Á©é GCL Ê°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁî¢ÁîüÊñáÊú¨Êì¥ÂÖÖÔºå‰ª•Âèä LLM Âº∑Â§ßÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ËÉΩÂäõ‰æÜËß£Ê±∫‰∏äËø∞‰∏âÂÄãÈôêÂà∂ÔºåÁÇ∫Â∞á GCL ÊáâÁî®Êñº TAG ‰ªªÂãôÈã™Âπ≥ÈÅìË∑Ø„ÄÇÂú®ÂõõÂÄãÈ´òÂìÅË≥™ TAG Ë≥áÊñôÈõÜ‰∏äÁöÑÂ§ßÈáèÂØ¶È©óË™™Êòé‰∫ÜÊâÄÊèêÂá∫ÁöÑ LATEX-GCL ÊñπÊ≥ïÁöÑÂÑ™Ë∂äÊÄß„ÄÇÂéüÂßãÁ¢ºÂíåË≥áÊñôÈõÜÂ∑≤ÁôºÂ∏É‰ª•Á∞°ÂåñÂèØÈáçË£ΩÊÄßÔºåÂèØÈÄèÈÅéÊ≠§ÈÄ£ÁµêÂ≠òÂèñÔºöhttps://anonymous.4open.science/r/LATEX-GCL-0712„ÄÇ

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á∂ìÂ∏∏Áº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÔºåÂç≥‰ΩøÁ∂ìÈÅéÂæÆË™øÁöÑÊ®°Âûã‰πüÂÆπÊòìÁî¢ÁîüÂπªË¶∫„ÄÇÂõ†Ê≠§ÔºåÈúÄË¶ÅÊõ¥Â§öÂèØÈù†ÁöÑÊ®°Âûã‰æÜÁ¥çÂÖ•Â§ñÈÉ®Áü•Ë≠ò„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊµÅÁ®ã 4StepFocusÔºåÁâπÂà•ÊòØÈ†êËôïÁêÜÊ≠•È©üÔºåÂèØ‰ª•Â§ßÂπÖÊîπÂñÑ LLM ÁöÑÁ≠îÊ°à„ÄÇÈÄôÊòØÈÄèÈÅéÊèê‰æõÂèóÂºïÂ∞éÁöÑÂ§ñÈÉ®Áü•Ë≠òÂ≠òÂèñÔºåÂà©Áî®Ê®°ÂûãËá™Ë°åÊì∑ÂèñÈóúËÅØÊÄßËÑàÁµ°ÂíåÈÄ≤Ë°åÂü∫Êú¨Êé®ÁêÜÁöÑËÉΩÂäõ‰æÜÂØ¶ÁèæÁöÑ„ÄÇÊ≠§ÊñπÊ≥ïÈÄèÈÅéÂú®ÂçäÁµêÊßãÂåñÁü•Ë≠òÂ∫´‰∏≠ÈÄ≤Ë°åÂü∫Êñº‰∏âÂÖÉÁµÑÁöÑÊêúÂ∞ãÔºå‰ª•Áõ¥Êé•‰∏îÂèØËøΩËπ§ÁöÑÊñπÂºèÁ∏ÆÂ∞èÊΩõÂú®Ê≠£Á¢∫Á≠îÊ°àÁöÑÁØÑÂúçÔºåÁÑ∂ÂæåÂÜçÂàáÊèõÂà∞ÊΩõÂú®Ë°®ÂæµÔºåÊ†πÊìöÈùûÁµêÊßãÂåñË≥áÊñôÂ∞çÈÄô‰∫õÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂêç„ÄÇÈÄôËàáÁ¥îÁ≤πÂü∫ÊñºÊΩõÂú®Ë°®ÂæµÁöÑÁõ∏ÈóúÊñπÊ≥ïÊúâÊâÄÂçÄÂà•„ÄÇ4StepFocus ÂåÖÂê´‰ª•‰∏ãÊ≠•È©üÔºö1) Áî± LLM ÈÄ≤Ë°å‰∏âÂÖÉÁµÑÁî¢Áîü‰ª•Êì∑ÂèñÈóúËÅØË≥áÊñôÔºå2) Âú®ÈÄô‰∫õ‰∏âÂÖÉÁµÑ‰∏≠ÊõøÊèõËÆäÊï∏Ôºå‰ª•Êé°Áî®Áü•Ë≠òÂúñË°®Á∏ÆÂ∞èÁ≠îÊ°àÂÄôÈÅ∏ÁØÑÂúçÔºå3) ‰ΩøÁî®Ê∂âÂèäÈóúËÅØÈùûÁµêÊßãÂåñË≥áÊñôÁöÑÂêëÈáèÁõ∏‰ººÊÄßÊêúÂ∞ãÂ∞çÂâ©È§òÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂ∫èÔºå4) Áî± LLM ÈáçÊñ∞Â∞çÊúÄ‰Ω≥ÂÄôÈÅ∏Á≠îÊ°àÈÄ≤Ë°åÊéíÂêçÔºå‰∏¶Êèê‰æõËÉåÊôØË≥áÊñô„ÄÇÂú®ÈÜ´ÁôÇ„ÄÅÁî¢ÂìÅÊé®Ëñ¶ÂíåÂ≠∏Ë°ìË´ñÊñáÊêúÂ∞ãÊ∏¨Ë©¶ÈõÜ‰∏≠ÈÄ≤Ë°åÁöÑÂØ¶È©óË≠âÊòéÔºåÈÄôÁ®ÆÊñπÊ≥ïÁ¢∫ÂØ¶ÊòØ‰∏ÄÁ®ÆÂº∑Â§ßÁöÑÊì¥ÂÖÖ„ÄÇÂÆÉ‰∏çÂÉÖÂ¢ûÂä†‰∫Ü‰æÜËá™Ë≥áË®äÊ™¢Á¥¢ÁöÑÁõ∏ÂÖ≥ÂèØËøΩËπ§ËÉåÊôØË≥áË®äÔºåËÄå‰∏îËàáÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºå‰πüÂ§ßÂπÖÊèêÂçá‰∫ÜÊïàËÉΩ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©é‰∏îÈÆÆÂ∞ëÊé¢Á¥¢ÁöÑÊñπÂêëÔºåÂõ†Ê≠§Êèê‰æõ‰∫ÜÂª£Ê≥õÁöÑÊú™‰æÜÂ∑•‰ΩúÊ©üÊúÉ„ÄÇ‰ΩøÁî®ÁöÑÂéüÂßãÁ¢ºÂèØÂú® https://github.com/kramerlab/4StepFocus ÂèñÂæó„ÄÇ

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁü•Ë≠òÂ∑•Á®ãÂíåÂ§öË™ûË®ÄË™ûÁæ©Êé®ÁêÜÊäÄË°ìÁöÑÊú¨‰ΩìË®≠Ë®àÔºåÁî®ÊñºÂª∫Á´ã‰∏ÄÂÄãËá™ÂãïÂåñÁ≥ªÁµ±Ôºå‰ª•Áü•Ë≠òÂúñË≠úÁöÑÂΩ¢ÂºèÂê∏Êî∂Âç∞Â∫¶ÊñôÁêÜÁöÑÁÉπÈ£™Ë≥áË®ä„ÄÇÈáçÈªûÂú®ÊñºË®≠Ë®àÊô∫ÊÖßÊñπÊ≥ïÔºå‰ª•Êé®Â∞éÊú¨‰ΩìË®≠Ë®àÔºå‰∏¶ÂÖ®Èù¢Êì∑ÂèñÈóúÊñºÈ£üÁâ©„ÄÅÈ£üË≠ú„ÄÅÈ£üÊùê„ÄÅÁÉπÈ£™ÁâπÊÄßÔºå‰ª•ÂèäÊúÄÈáçË¶ÅÁöÑÁáüÈ§äÁöÑÁü•Ë≠òÔºå‰∏¶Êì¥Â§ßË¶èÊ®°„ÄÇÊàëÂÄëÂú®ÈÄôÂÄãÁ†îË®éÊúÉË´ñÊñá‰∏≠‰ªãÁ¥π‰∫ÜÊàëÂÄëÊ≠£Âú®ÈÄ≤Ë°åÁöÑÂ∑•‰ΩúÔºåË©≥Á¥∞ÊèèËø∞‰∫ÜÊï¥ÁêÜÂç∞Â∫¶ÊñôÁêÜÁü•Ë≠òÁõ∏ÈóúÁöÑÊåëÊà∞Ôºå‰∏¶ÊèêÂá∫‰∫ÜÊàëÂÄëÁöÑÈ´òÈöéÊú¨‰ΩìË®≠Ë®à„ÄÇÊàëÂÄë‰πüÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÂÆÉ‰ΩøÁî® AI„ÄÅLLM ÂíåË™ûË®ÄÊäÄË°ìÔºåÂæûÂÖ¨ÂÖ±È†òÂüüÁöÑÈ£üË≠úÈÉ®ËêΩÊ†ºÁ∂≤Á´ô‰∏≠Êï¥ÁêÜË≥áË®äÔºå‰ª•Âª∫Á´ãÂç∞Â∫¶ÊñôÁêÜÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇÊú¨ÊñáÊèêÂá∫ÁöÑÁü•Ë≠òÊï¥ÁêÜÊñπÊ≥ïÊòØÈÄöÁî®ÁöÑÔºåÂèØ‰ª•Ë§áË£ΩÂà∞‰ªª‰ΩïÈ†òÂüü„ÄÇË®≠Ë®àËàáÊáâÁî®ÁÑ°ÈóúÔºåÂèØÁî®Êñº AI È©ÖÂãïÁöÑÊô∫ÊÖßÂàÜÊûê„ÄÅÂª∫Á´ãÂÄã‰∫∫ÂåñÊï∏‰ΩçÂÅ•Â∫∑Êé®Ëñ¶Á≥ªÁµ±Ôºå‰ª•Âèä‰ΩøÁî®‰ΩøÁî®ËÄÖË≥áË®ä„ÄÅÈ£üÁâ©ÁîüÁâ©ÂåñÂ≠∏„ÄÅÂú∞ÁêÜË≥áË®ä„ÄÅËæ≤Ê•≠Ë≥áË®äÁ≠âËÑàÁµ°Ë≥áË®äÔºå‰æÜË£úÂÖÖÂç∞Â∫¶ÊñôÁêÜÁöÑÁü•Ë≠òÂúñË≠ú„ÄÇ

##### **Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph**
2409.00727v1 by Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuanhui Yang, Yuanyuan Zhu, Chuang Hu, Bo Du, Jiawei Jiang

Text-attributed graph (TAG) is an important type of graph structured data
with text descriptions for each node. Few- and zero-shot node classification on
TAGs have many applications in fields such as academia and social networks.
However, the two tasks are challenging due to the lack of supervision signals,
and existing methods only use the contrastive loss to align graph-based node
embedding and language-based text embedding. In this paper, we propose Hound to
improve accuracy by introducing more supervision signals, and the core idea is
to go beyond the node-text pairs that come with data. Specifically, we design
three augmentation techniques, i.e., node perturbation, text matching, and
semantics negation to provide more reference nodes for each text and vice
versa. Node perturbation adds/drops edges to produce diversified node
embeddings that can be matched with a text. Text matching retrieves texts with
similar embeddings to match with a node. Semantics negation uses a negative
prompt to construct a negative text with the opposite semantics, which is
contrasted with the original node and text. We evaluate Hound on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that Hound
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.

ÊëòË¶ÅÔºöÊñáÂ≠óÂ±ûÊÄßÂúñ (TAG) ÊòØ‰∏ÄÁ®ÆÈáçË¶ÅÁöÑÂúñÂΩ¢ÁµêÊßãÂåñË≥áÊñôÈ°ûÂûãÔºåÂÖ∂‰∏≠ÊØèÂÄãÁØÄÈªûÈÉΩÊúâÊñáÂ≠óÊèèËø∞„ÄÇTAG ‰∏äÁöÑÂ∞ëÊ®£Êú¨ÂíåÈõ∂Ê®£Êú¨ÁØÄÈªûÂàÜÈ°ûÂú®Â≠∏Ë°ìÁïåÂíåÁ§æ‰∫§Á∂≤Ë∑ØÁ≠âÈ†òÂüüÊúâË®±Â§öÊáâÁî®„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁº∫‰πèÁõ£Áù£Ë®äËôüÔºåÈÄôÂÖ©ÂÄã‰ªªÂãôÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÁèæÊúâÊñπÊ≥ïÂÉÖ‰ΩøÁî®Â∞çÊØîÊêçÂ§±‰æÜÂ∞çÈΩäÂü∫ÊñºÂúñÂΩ¢ÁØÄÈªûÁöÑÂµåÂÖ•ÂíåÂü∫ÊñºË™ûË®ÄÁöÑÊñáÂ≠óÂµåÂÖ•„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ Hound ‰æÜÈÄèÈÅéÂºïÂÖ•Êõ¥Â§öÁõ£Áù£Ë®äËôü‰æÜÊîπÂñÑÊ∫ñÁ¢∫Â∫¶ÔºåÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØË∂ÖË∂äË≥áÊñô‰∏≠ÈôÑÂ∏∂ÁöÑÁØÄÈªûÊñáÂ≠óÂ∞ç„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏âÁ®ÆÊì¥ÂÖÖÊäÄË°ìÔºåÂç≥ÁØÄÈªûÊìæÂãï„ÄÅÊñáÂ≠óÈÖçÂ∞çÂíåË™ûÁæ©Âê¶ÂÆöÔºåÁÇ∫ÊØèÂÄãÊñáÂ≠óÊèê‰æõÊõ¥Â§öÂèÉËÄÉÁØÄÈªûÔºåÂèç‰πã‰∫¶ÁÑ∂„ÄÇÁØÄÈªûÊìæÂãïÊñ∞Â¢û/Âà™Èô§ÈÇäÁ∑£‰ª•Áî¢ÁîüÂèØ‰ª•ËàáÊñáÂ≠óÈÖçÂ∞çÁöÑÂ§öÊ®£ÂåñÁØÄÈªûÂµåÂÖ•„ÄÇÊñáÂ≠óÈÖçÂ∞çÊì∑ÂèñÂÖ∑ÊúâÈ°û‰ººÂµåÂÖ•ÁöÑÊñáÂ≠ó‰ª•ËàáÁØÄÈªûÈÖçÂ∞ç„ÄÇË™ûÁæ©Âê¶ÂÆö‰ΩøÁî®Ë≤†Èù¢ÊèêÁ§∫‰æÜÂª∫ÊßãÂÖ∑ÊúâÁõ∏ÂèçË™ûÁæ©ÁöÑË≤†Èù¢ÊñáÂ≠óÔºåËàáÂéüÂßãÁØÄÈªûÂíåÊñáÂ≠óÂΩ¢ÊàêÂ∞çÊØî„ÄÇÊàëÂÄëÂú® 5 ÂÄãË≥áÊñôÈõÜ‰∏äË©ï‰º∞ HoundÔºå‰∏¶Ëàá 13 ÂÄãÊúÄÂÖàÈÄ≤ÁöÑÂü∫Á∑öÈÄ≤Ë°åÊØîËºÉ„ÄÇÁµêÊûúË°®ÊòéÔºåHound Âú®ÊâÄÊúâÂü∫Á∑ö‰∏äÂßãÁµÇË°®ÁèæÂÑ™Áï∞ÔºåÂÖ∂Ê∫ñÁ¢∫Â∫¶ÈÄöÂ∏∏ÊØîÊïàËÉΩÊúÄ‰Ω≥ÁöÑÂü∫Á∑öÊèêÈ´ò‰∫Ü 5% ‰ª•‰∏ä„ÄÇ

##### **WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction**
2409.00331v1 by Oktie Hassanzadeh

Recently, there has been an increasing interest in the construction of
general-domain and domain-specific causal knowledge graphs. Such knowledge
graphs enable reasoning for causal analysis and event prediction, and so have a
range of applications across different domains. While great progress has been
made toward automated construction of causal knowledge graphs, the evaluation
of such solutions has either focused on low-level tasks (e.g., cause-effect
phrase extraction) or on ad hoc evaluation data and small manual evaluations.
In this paper, we present a corpus, task, and evaluation framework for causal
knowledge graph construction. Our corpus consists of Wikipedia articles for a
collection of event-related concepts in Wikidata. The task is to extract causal
relations between event concepts from the corpus. The evaluation is performed
in part using existing causal relations in Wikidata to measure recall, and in
part using Large Language Models to avoid the need for manual or crowd-sourced
evaluation. We evaluate a pipeline for causal knowledge graph construction that
relies on neural models for question answering and concept linking, and show
how the corpus and the evaluation framework allow us to effectively find the
right model for each task. The corpus and the evaluation framework are publicly
available.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºå‰∫∫ÂÄëÂ∞çÈÄöÁî®È†òÂüüÂíåÁâπÂÆöÈ†òÂüüÂõ†ÊûúÁü•Ë≠òÂúñË≠úÁöÑÂª∫ÊßãË∂ä‰æÜË∂äÊÑüËààË∂£„ÄÇÊ≠§È°ûÁü•Ë≠òÂúñË≠úËÉΩÂ§†Êé®ÁêÜÂõ†ÊûúÂàÜÊûêÂíå‰∫ã‰ª∂È†êÊ∏¨ÔºåÂõ†Ê≠§Âú®‰∏çÂêåÈ†òÂüü‰∏≠ÊúâÂª£Ê≥õÁöÑÊáâÁî®„ÄÇÈõñÁÑ∂Âú®Âõ†ÊûúÁü•Ë≠òÂúñË≠úÁöÑËá™ÂãïÂª∫ÊßãÊñπÈù¢ÂèñÂæó‰∫ÜÈáçÂ§ßÈÄ≤Â±ïÔºå‰ΩÜÊ≠§È°ûËß£Ê±∫ÊñπÊ°àÁöÑË©ï‰º∞Ë¶ÅÂòõËëóÈáçÊñº‰ΩéÈöé‰ªªÂãôÔºà‰æãÂ¶ÇÂõ†ÊûúÈóú‰øÇÁü≠Ë™ûÊì∑ÂèñÔºâÔºåË¶ÅÂòõËëóÈáçÊñºËá®ÊôÇË©ï‰º∞Ë≥áÊñôÂíåÂ∞èÂûãÊâãÂãïË©ï‰º∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãË™ûÊñôÂ∫´„ÄÅ‰ªªÂãôÂíåÂõ†ÊûúÁü•Ë≠òÂúñË≠úÂª∫ÊßãË©ï‰º∞Êû∂Êßã„ÄÇÊàëÂÄëÁöÑË™ûÊñôÂ∫´ÂåÖÂê´Á∂≠Âü∫ÁôæÁßëÊñáÁ´†ÔºåÂÖ∂‰∏≠ÂåÖÂê´ Wikidata ‰∏≠‰∏ÄÁ≥ªÂàó‰∫ã‰ª∂Áõ∏ÈóúÊ¶ÇÂøµ„ÄÇ‰ªªÂãôÊòØÂæûË™ûÊñôÂ∫´‰∏≠Êì∑Âèñ‰∫ã‰ª∂Ê¶ÇÂøµ‰πãÈñìÁöÑÂõ†ÊûúÈóú‰øÇ„ÄÇË©ï‰º∞ÈÉ®ÂàÜ‰ΩøÁî® Wikidata ‰∏≠ÁèæÊúâÁöÑÂõ†ÊûúÈóú‰øÇ‰æÜË°°ÈáèÂè¨ÂõûÁéáÔºåÈÉ®ÂàÜ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜÈÅøÂÖçÊâãÂãïÊàñÁæ§ÁúæÂ§ñÂåÖË©ï‰º∞ÁöÑÈúÄË¶Å„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü‰∏ÄÂÄãÂõ†ÊûúÁü•Ë≠òÂúñË≠úÂª∫ÊßãÁÆ°ÈÅìÔºåË©≤ÁÆ°ÈÅì‰æùË≥¥ÊñºÁî®ÊñºÂïèÁ≠îÂíåÊ¶ÇÂøµÈÄ£ÁµêÁöÑÁ•ûÁ∂ìÊ®°ÂûãÔºå‰∏¶Â±ïÁ§∫‰∫ÜË™ûÊñôÂ∫´ÂíåË©ï‰º∞Êû∂ÊßãÂ¶Ç‰ΩïËÆìÊàëÂÄëÊúâÊïàÂú∞ÁÇ∫ÊØèÂÄã‰ªªÂãôÊâæÂà∞ÂêàÈÅ©ÁöÑÊ®°Âûã„ÄÇË™ûÊñôÂ∫´ÂíåË©ï‰º∞Êû∂ÊßãÂÖ¨ÈñãÊèê‰æõ„ÄÇ</paragraph>

##### **HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications**
2409.09046v1 by Rishi Kalra, Zekun Wu, Ayesha Gulley, Airlie Hilliard, Xin Guan, Adriano Koshiyama, Philip Treleaven

While Large Language Models (LLMs) excel in text generation and
question-answering, their effectiveness in AI legal and policy is limited by
outdated knowledge, hallucinations, and inadequate reasoning in complex
contexts. Retrieval-Augmented Generation (RAG) systems improve response
accuracy by integrating external knowledge but struggle with retrieval errors,
poor context integration, and high costs, particularly in interpreting
qualitative and quantitative AI legal texts. This paper introduces a Hybrid
Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,
exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity
classifier for adaptive parameter tuning, a hybrid retrieval strategy combining
dense, sparse, and knowledge graph methods, and an evaluation framework with
specific question types and metrics. By dynamically adjusting parameters,
HyPA-RAG significantly improves retrieval accuracy and response fidelity.
Testing on LL144 shows enhanced correctness, faithfulness, and contextual
precision, addressing the need for adaptable NLP systems in complex,
high-stakes AI legal and policy applications.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈõñÁÑ∂Âú®ÊñáÂ≠óÁî¢ÁîüÂíåÂïèÁ≠îÊñπÈù¢Ë°®ÁèæÂÑ™Áï∞Ôºå‰ΩÜÂÖ∂Âú® AI Ê≥ïÂæãÂíåÊîøÁ≠ñ‰∏≠ÁöÑÊïàËÉΩÂçªÂèóÂà∞ÈÅéÊôÇÁü•Ë≠ò„ÄÅÂπªË¶∫‰ª•ÂèäÂú®Ë§áÈõúËÑàÁµ°‰∏≠Êé®ÁêÜ‰∏çË∂≥ÁöÑÈôêÂà∂„ÄÇÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Á≥ªÁµ±ÈÄèÈÅéÊï¥ÂêàÂ§ñÈÉ®Áü•Ë≠ò‰æÜÊîπÂñÑÂõûÊáâÊ∫ñÁ¢∫ÊÄßÔºå‰ΩÜÂçªÂú®Ê™¢Á¥¢ÈåØË™§„ÄÅËÑàÁµ°Êï¥Âêà‰∏çËâØ‰ª•ÂèäÊàêÊú¨È´òÊòÇÊñπÈù¢Èù¢Ëá®ÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®Ë©ÆÈáãÂÆöÊÄßÂíåÂÆöÈáèÁöÑ AI Ê≥ïÂæãÊñáÊú¨ÊôÇ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÂ∞àÁÇ∫ AI Ê≥ïÂæãÂíåÊîøÁ≠ñÈáèË∫´ÊâìÈÄ†ÁöÑÊ∑∑ÂêàÂèÉÊï∏Ëá™ÈÅ©Êáâ RAG (HyPA-RAG) Á≥ªÁµ±Ôºå‰ª•Á¥êÁ¥ÑÂ∏ÇÂú∞ÊñπÊ≥ïÂæã 144 (LL144) ÁÇ∫‰æã„ÄÇHyPA-RAG ‰ΩøÁî®Êü•Ë©¢Ë§áÈõúÂ∫¶ÂàÜÈ°ûÂô®ÈÄ≤Ë°åËá™ÈÅ©ÊáâÂèÉÊï∏Ë™øÊï¥ÔºåÁµêÂêàÁ®†ÂØÜ„ÄÅÁ®ÄÁñèÂíåÁü•Ë≠òÂúñË°®ÊñπÊ≥ïÁöÑÊ∑∑ÂêàÊ™¢Á¥¢Á≠ñÁï•Ôºå‰ª•ÂèäÂåÖÂê´ÁâπÂÆöÂïèÈ°åÈ°ûÂûãÂíåÊåáÊ®ôÁöÑË©ï‰º∞Êû∂Êßã„ÄÇÈÄèÈÅéÂãïÊÖãË™øÊï¥ÂèÉÊï∏ÔºåHyPA-RAG Â§ßÂπÖÊîπÂñÑ‰∫ÜÊ™¢Á¥¢Ê∫ñÁ¢∫ÊÄßÂíåÂõûÊáâ‰øùÁúüÂ∫¶„ÄÇÂú® LL144 ‰∏äÁöÑÊ∏¨Ë©¶È°ØÁ§∫Âá∫Â¢ûÂº∑ÁöÑÊ≠£Á¢∫ÊÄß„ÄÅÂø†ÂØ¶Â∫¶ÂíåËÑàÁµ°Ê∫ñÁ¢∫Â∫¶ÔºåÊªøË∂≥‰∫ÜÂú®Ë§áÈõú„ÄÅÈ´òÈ¢®Èö™ÁöÑ AI Ê≥ïÂæãÂíåÊîøÁ≠ñÊáâÁî®‰∏≠Â∞çÂèØÈÅ©Êáâ NLP Á≥ªÁµ±ÁöÑÈúÄÊ±Ç„ÄÇ

##### **LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**
2408.16224v2 by Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng

Recent advances in large vision-language models (VLMs) typically employ
vision encoders based on the Vision Transformer (ViT) architecture. The
division of the images into patches by ViT results in a fragmented perception,
thereby hindering the visual understanding capabilities of VLMs. In this paper,
we propose an innovative enhancement to address this limitation by introducing
a Scene Graph Expression (SGE) module in VLMs. This module extracts and
structurally expresses the complex semantic information within images, thereby
improving the foundational perception and understanding abilities of VLMs.
Extensive experiments demonstrate that integrating our SGE module significantly
enhances the VLM's performance in vision-language tasks, indicating its
effectiveness in preserving intricate semantic details and facilitating better
visual understanding.

ÊëòË¶ÅÔºöËøë‰æÜÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑÈÄ≤Â±ïÈÄöÂ∏∏Êé°Áî®Âü∫ÊñºË¶ñË¶∫ËΩâÊèõÂô® (ViT) Êû∂ÊßãÁöÑË¶ñË¶∫Á∑®Á¢ºÂô®„ÄÇViT Â∞áÂΩ±ÂÉèÂàÜÂâ≤ÊàêÂçÄÂ°äÊúÉÈÄ†ÊàêÁ†¥Á¢éÁöÑÊÑüÁü•ÔºåÂæûËÄåÈòªÁ§ô VLM ÁöÑË¶ñË¶∫ÁêÜËß£ËÉΩÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÈ†ÖÂâµÊñ∞ÁöÑÂ¢ûÂº∑ÂäüËÉΩÔºåÈÄèÈÅéÂú® VLM ‰∏≠ÂºïÂÖ•Â†¥ÊôØÂúñË°®ÈÅî (SGE) Ê®°ÁµÑ‰æÜËß£Ê±∫Ê≠§ÈôêÂà∂„ÄÇÊ≠§Ê®°ÁµÑÊúÉËêÉÂèñÂΩ±ÂÉè‰∏≠ÁöÑË§áÈõúË™ûÊÑèË≥áË®ä‰∏¶‰ª•ÁµêÊßãÂåñÁöÑÊñπÂºèË°®ÈÅîÔºåÂæûËÄåÊîπÂñÑ VLM ÁöÑÂü∫Á§éÊÑüÁü•ÂíåÁêÜËß£ËÉΩÂäõ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÊï¥ÂêàÊàëÂÄëÁöÑ SGE Ê®°ÁµÑËÉΩÈ°ØËëóÊèêÂçá VLM Âú®Ë¶ñË¶∫Ë™ûË®Ä‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩÔºåË°®Á§∫ÂÆÉÂú®‰øùÁïôË§áÈõúÁöÑË™ûÊÑèÁ¥∞ÁØÄÂíå‰øÉÈÄ≤Êõ¥Â•ΩÁöÑË¶ñË¶∫ÁêÜËß£ÊñπÈù¢ÂæàÊúâÊïà„ÄÇ

##### **LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments**
2408.15903v1 by Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai

The rapid obsolescence of information in Large Language Models (LLMs) has
driven the development of various techniques to incorporate new facts. However,
existing methods for knowledge editing still face difficulties with multi-hop
questions that require accurate fact identification and sequential logical
reasoning, particularly among numerous fact updates. To tackle these
challenges, this paper introduces Graph Memory-based Editing for Large Language
Models (GMeLLo), a straitforward and effective method that merges the explicit
knowledge representation of Knowledge Graphs (KGs) with the linguistic
flexibility of LLMs. Beyond merely leveraging LLMs for question answering,
GMeLLo employs these models to convert free-form language into structured
queries and fact triples, facilitating seamless interaction with KGs for rapid
updates and precise multi-hop reasoning. Our results show that GMeLLo
significantly surpasses current state-of-the-art knowledge editing methods in
the multi-hop question answering benchmark, MQuAKE, especially in scenarios
with extensive knowledge edits.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Ë≥áË®äÂø´ÈÄüÈÅéÊôÇÔºå‰øÉ‰ΩøÂêÑÁ®ÆÊäÄË°ìÁôºÂ±ï‰ª•Á¥çÂÖ•Êñ∞‰∫ãÂØ¶„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁü•Ë≠òÁ∑®ËºØÊñπÊ≥ïÂú®ÈúÄË¶ÅÊ∫ñÁ¢∫‰∫ãÂØ¶Ëæ®Ë≠òÂíåÈ†ÜÂ∫èÈÇèËºØÊé®ÁêÜÁöÑÂ§öË∑≥ÂïèÈ°å‰∏ä‰ªçÈù¢Ëá®Âõ∞Èõ£ÔºåÁâπÂà•ÊòØÂú®ÁúæÂ§ö‰∫ãÂØ¶Êõ¥Êñ∞‰∏≠„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨Êñá‰ªãÁ¥π‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂúñË®òÊÜ∂Á∑®ËºØ (GMeLLo)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁõ¥Êé•‰∏îÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁµêÂêà‰∫ÜÁü•Ë≠òÂúñË≠ú (KG) ÁöÑÊòéÁ¢∫Áü•Ë≠òË°®Á§∫Ëàá LLM ÁöÑË™ûË®ÄÈùàÊ¥ªÊÄß„ÄÇGMeLLo ‰∏çÂÉÖÂà©Áî® LLM ‰æÜÂõûÁ≠îÂïèÈ°åÔºåÈÇÑ‰ΩøÁî®ÈÄô‰∫õÊ®°ÂûãÂ∞áËá™Áî±ÂΩ¢ÂºèÁöÑË™ûË®ÄËΩâÊèõÁÇ∫ÁµêÊßãÂåñÊü•Ë©¢Âíå‰∫ãÂØ¶‰∏âÂÖÉÁµÑÔºå‰øÉÈÄ≤Ëàá KG ÁöÑÁÑ°Á∏´‰∫íÂãïÔºå‰ª•‰æøÂø´ÈÄüÊõ¥Êñ∞ÂíåÁ≤æÁ¢∫ÁöÑÂ§öË∑≥Êé®ÁêÜ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂú®Â§öË∑≥ÂïèÈ°åÂõûÁ≠îÂü∫Ê∫ñ MQuAKE ‰∏≠ÔºåGMeLLo ÊòéÈ°ØË∂ÖË∂ä‰∫ÜÁï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑÁü•Ë≠òÁ∑®ËºØÊñπÊ≥ïÔºåÁâπÂà•ÊòØÂú®Âª£Ê≥õÁü•Ë≠òÁ∑®ËºØÁöÑÂ†¥ÊôØ‰∏≠„ÄÇ

##### **VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**
2408.14895v2 by Shusaku Egami, Takahiro Ugai, Swe Nwe Nwe Htun, Ken Fukuda

Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data
(e.g., images and videos) into symbols, have attracted attention as resources
enabling knowledge processing and machine learning across modalities. However,
the construction of MMKGs for videos consisting of multiple events, such as
daily activities, is still in the early stages. In this paper, we construct an
MMKG based on synchronized multi-view simulated videos of daily activities.
Besides representing the content of daily life videos as event-centric
knowledge, our MMKG also includes frame-by-frame fine-grained changes, such as
bounding boxes within video frames. In addition, we provide support tools for
querying our MMKG. As an application example, we demonstrate that our MMKG
facilitates benchmarking vision-language models by providing the necessary
vision-language datasets for a tailored task.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÖãÁü•Ë≠òÂúñÔºàMMKGÔºâÂ∞áÂêÑÁ®ÆÈùûÁ¨¶ËôüÊï∏ÊìöÔºà‰æãÂ¶ÇÔºåÂΩ±ÂÉèÂíåÂΩ±ÁâáÔºâËΩâÊèõÁÇ∫Á¨¶ËôüÔºåÊàêÁÇ∫‰∏ÄÁ®ÆË≥áÊ∫êÔºåËÉΩËÆìË∑®Ê®°ÊÖãÁöÑÁü•Ë≠òËôïÁêÜÂíåÊ©üÂô®Â≠∏ÁøíÊàêÁÇ∫ÂèØËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÂåÖÂê´Â§öÂÄã‰∫ã‰ª∂Ôºà‰æãÂ¶ÇÊó•Â∏∏ÁîüÊ¥ªÊ¥ªÂãïÔºâÁöÑÂΩ±ÁâáÔºåÂÖ∂ MMKG ÁöÑÂª∫Êßã‰ªçËôïÊñºÊó©ÊúüÈöéÊÆµ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂü∫ÊñºÊØèÊó•Ê¥ªÂãïÁöÑÂêåÊ≠•Â§öË¶ñËßíÊ®°Êì¨ÂΩ±ÁâáÔºåÂª∫Êßã‰∫Ü‰∏ÄÂÄã MMKG„ÄÇÈô§‰∫ÜÂ∞áÊó•Â∏∏ÁîüÊ¥ªÂΩ±ÁâáÁöÑÂÖßÂÆπË°®Á§∫ÁÇ∫‰ª•‰∫ã‰ª∂ÁÇ∫‰∏≠ÂøÉÁöÑÁü•Ë≠òÂ§ñÔºåÊàëÂÄëÁöÑ MMKG ‰πüÂåÖÂê´ÈÄêÂπÄÁöÑÁ¥∞ÂæÆËÆäÂåñÔºå‰æãÂ¶ÇÂΩ±ÁâáÂπÄ‰∏≠ÁöÑÈÇäÁïåÊ°Ü„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÊèê‰æõ‰∫ÜÁî®ÊñºÊü•Ë©¢ MMKG ÁöÑÊîØÊè¥Â∑•ÂÖ∑„ÄÇ‰ΩúÁÇ∫ÊáâÁî®ÁØÑ‰æãÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑ MMKG Â¶Ç‰ΩïÈÄèÈÅéÊèê‰æõÁâπÂÆö‰ªªÂãôÊâÄÈúÄÁöÑË¶ñË¶∫Ë™ûË®ÄË≥áÊñôÈõÜÔºå‰æÜ‰øÉÈÄ≤Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇ

##### **XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model**
2408.16021v1 by Yasir Ali Farrukh, Syed Wali, Irfan Khan, Nathaniel D. Bastian

In the rapidly evolving field of cybersecurity, the integration of flow-level
and packet-level information for real-time intrusion detection remains a
largely untapped area of research. This paper introduces "XG-NID," a novel
framework that, to the best of our knowledge, is the first to fuse flow-level
and packet-level data within a heterogeneous graph structure, offering a
comprehensive analysis of network traffic. Leveraging a heterogeneous graph
neural network (GNN) with graph-level classification, XG-NID uniquely enables
real-time inference while effectively capturing the intricate relationships
between flow and packet payload data. Unlike traditional GNN-based
methodologies that predominantly analyze historical data, XG-NID is designed to
accommodate the heterogeneous nature of network traffic, providing a robust and
real-time defense mechanism. Our framework extends beyond mere classification;
it integrates Large Language Models (LLMs) to generate detailed, human-readable
explanations and suggest potential remedial actions, ensuring that the insights
produced are both actionable and comprehensible. Additionally, we introduce a
new set of flow features based on temporal information, further enhancing the
contextual and explainable inferences provided by our model. To facilitate
practical application and accessibility, we developed "GNN4ID," an open-source
tool that enables the extraction and transformation of raw network traffic into
the proposed heterogeneous graph structure, seamlessly integrating flow and
packet-level data. Our comprehensive quantitative comparative analysis
demonstrates that XG-NID achieves an F1 score of 97\% in multi-class
classification, outperforming existing baseline and state-of-the-art methods.
This sets a new standard in Network Intrusion Detection Systems by combining
innovative data fusion with enhanced interpretability and real-time
capabilities.

ÊëòË¶ÅÔºö<paragraph>Âú®Âø´ÈÄüÁôºÂ±ïÁöÑÁ∂≤Ë∑ØÂÆâÂÖ®È†òÂüü‰∏≠ÔºåÊï¥ÂêàÊµÅÂ±§Á¥öÂíåÂ∞ÅÂåÖÂ±§Á¥öË≥áË®ä‰ª•ÈÄ≤Ë°åÂç≥ÊôÇÂÖ•‰æµÂÅµÊ∏¨Ôºå‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÂ∞öÊú™ÈñãÁôºÁöÑÁ†îÁ©∂È†òÂüü„ÄÇÊú¨Êñá‰ªãÁ¥π„ÄåXG-NID„ÄçÔºå‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊû∂ÊßãÔºåÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂú®Áï∞Ë≥™ÂúñÂΩ¢ÁµêÊßã‰∏≠ËûçÂêàÊµÅÂ±§Á¥öÂíåÂ∞ÅÂåÖÂ±§Á¥öË≥áÊñôÁöÑÊû∂ÊßãÔºåÊèê‰æõÂ∞çÁ∂≤Ë∑ØÊµÅÈáèÁöÑÂÖ®Èù¢ÂàÜÊûê„ÄÇÈÄèÈÅéÂà©Áî®Áï∞Ë≥™ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂíåÂúñÂΩ¢Â±§Á¥öÂàÜÈ°ûÔºåXG-NID Áç®ÁâπÂú∞ÂØ¶ÁèæÂç≥ÊôÇÊé®Ë´ñÔºåÂêåÊôÇÊúâÊïàÊì∑ÂèñÊµÅÂíåÂ∞ÅÂåÖÈÖ¨ËºâË≥áÊñô‰πãÈñìÁöÑË§áÈõúÈóú‰øÇ„ÄÇËàáÂÇ≥Áµ±Âü∫Êñº GNN ÁöÑÊñπÊ≥ïÔºà‰∏ªË¶ÅÂàÜÊûêÊ≠∑Âè≤Ë≥áÊñôÔºâ‰∏çÂêåÔºåXG-NID Ë¢´Ë®≠Ë®àÊàêÈÅ©ÊáâÁ∂≤Ë∑ØÊµÅÈáèÁöÑÁï∞Ë≥™ÊÄßÔºåÊèê‰æõÂº∑Â§ß‰∏îÂç≥ÊôÇÁöÑÈò≤Á¶¶Ê©üÂà∂„ÄÇÊàëÂÄëÁöÑÊû∂Êßã‰∏çÂÉÖÈôêÊñºÂàÜÈ°ûÔºõÂÆÉÊï¥ÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ª•Áî¢ÁîüË©≥Á¥∞„ÄÅ‰∫∫È°ûÂèØËÆÄÁöÑËß£Èáã‰∏¶Âª∫Ë≠∞ÊΩõÂú®ÁöÑË£úÊïëÊé™ÊñΩÔºåÁ¢∫‰øùÁî¢ÁîüÁöÑË¶ãËß£Êó¢ÂèØÊìç‰ΩúÂèàÊòìÊñºÁêÜËß£„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊ†πÊìöÊôÇÈñìË≥áË®äÂºïÂÖ•‰∏ÄÁµÑÊñ∞ÁöÑÊµÅÁâπÂæµÔºåÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑Ê®°ÂûãÊèê‰æõÁöÑËÑàÁµ°ÂíåÂèØËß£ÈáãÊé®Ë´ñ„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÂØ¶ÈöõÊáâÁî®ÂíåÂèØÂèäÊÄßÔºåÊàëÂÄëÈñãÁôº‰∫Ü„ÄåGNN4ID„ÄçÔºå‰∏ÄÂÄãÈñãÊîæÂéüÂßãÁ¢ºÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Â∞áÂéüÂßãÁ∂≤Ë∑ØÊµÅÈáèÊèêÂèñ‰∏¶ËΩâÊèõÁÇ∫Âª∫Ë≠∞ÁöÑÁï∞Ë≥™ÂúñÂΩ¢ÁµêÊßãÔºåÁÑ°Á∏´Êï¥ÂêàÊµÅÂíåÂ∞ÅÂåÖÂ±§Á¥öË≥áÊñô„ÄÇÊàëÂÄëÂÖ®Èù¢ÁöÑÂÆöÈáèÊØîËºÉÂàÜÊûêË°®ÊòéÔºåXG-NID Âú®Â§öÈ°ûÂà•ÂàÜÈ°û‰∏≠ÈÅîÂà∞ 97% ÁöÑ F1 ÂàÜÊï∏ÔºåÂÑ™ÊñºÁèæÊúâÁöÑÂü∫Ê∫ñÂíåÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇÈÄôÈÄèÈÅéÁµêÂêàÂâµÊñ∞ÁöÑË≥áÊñôËûçÂêà„ÄÅÂ¢ûÂº∑ÁöÑÂèØËß£ÈáãÊÄßÂíåÂç≥ÊôÇÂäüËÉΩÔºåÂú®Á∂≤Ë∑ØÂÖ•‰æµÂÅµÊ∏¨Á≥ªÁµ±‰∏≠Ê®πÁ´ã‰∫ÜÊñ∞ÁöÑÊ®ôÊ∫ñ„ÄÇ</paragraph>

##### **Process Trace Querying using Knowledge Graphs and Notation3**
2409.04452v1 by William Van Woensel

In process mining, a log exploration step allows making sense of the event
traces; e.g., identifying event patterns and illogical traces, and gaining
insight into their variability. To support expressive log exploration, the
event log can be converted into a Knowledge Graph (KG), which can then be
queried using general-purpose languages. We explore the creation of semantic KG
using the Resource Description Framework (RDF) as a data model, combined with
the general-purpose Notation3 (N3) rule language for querying. We show how
typical trace querying constraints, inspired by the state of the art, can be
implemented in N3. We convert case- and object-centric event logs into a
trace-based semantic KG; OCEL2 logs are hereby "flattened" into traces based on
object paths through the KG. This solution offers (a) expressivity, as queries
can instantiate constraints in multiple ways and arbitrarily constrain
attributes and relations (e.g., actors, resources); (b) flexibility, as OCEL2
event logs can be serialized as traces in arbitrary ways based on the KG; and
(c) extensibility, as others can extend our library by leveraging the same
implementation patterns.

ÊëòË¶ÅÔºöÂú®ÊµÅÁ®ãÊåñÊéò‰∏≠ÔºåÊó•ÂøóÊé¢Á¥¢Ê≠•È™§ÂèØ‰ª•ÁêÜËß£‰∫ã‰ª∂ËΩ®ËøπÔºõ‰æãÂ¶ÇÔºåËØÜÂà´‰∫ã‰ª∂Ê®°ÂºèÂíåÈùûÈÄªËæëËΩ®ËøπÔºåÂπ∂Ê∑±ÂÖ•‰∫ÜËß£ÂÖ∂ÂèØÂèòÊÄß„ÄÇ‰∏∫‰∫ÜÊîØÊåÅË°®ËææÊÄßÊó•ÂøóÊé¢Á¥¢Ôºå‰∫ã‰ª∂Êó•ÂøóÂèØ‰ª•ËΩ¨Êç¢‰∏∫Áü•ËØÜÂõæ (KG)ÔºåÁÑ∂ÂêéÂèØ‰ª•‰ΩøÁî®ÈÄöÁî®ËØ≠Ë®ÄÂØπÂÖ∂ËøõË°åÊü•ËØ¢„ÄÇÊàë‰ª¨Êé¢Á¥¢‰ΩøÁî®ËµÑÊ∫êÊèèËø∞Ê°ÜÊû∂ (RDF) ‰Ωú‰∏∫Êï∞ÊçÆÊ®°ÂûãÂàõÂª∫ËØ≠‰πâ KGÔºåÂπ∂ÁªìÂêàÈÄöÁî® Notation3 (N3) ËßÑÂàôËØ≠Ë®ÄËøõË°åÊü•ËØ¢„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî® N3 ÂÆûÁé∞ÂèóÁé∞ÊúâÊäÄÊúØÂêØÂèëÁöÑÂÖ∏ÂûãËΩ®ËøπÊü•ËØ¢Á∫¶Êùü„ÄÇÊàë‰ª¨Â∞ÜÊ°à‰æãÂíåÂØπË±°‰∏≠ÂøÉ‰∫ã‰ª∂Êó•ÂøóËΩ¨Êç¢‰∏∫Âü∫‰∫éËΩ®ËøπÁöÑËØ≠‰πâ KGÔºõOCEL2 Êó•ÂøóÂú®Ê≠§Ë¢´‚ÄúÊâÅÂπ≥Âåñ‚Äù‰∏∫Âü∫‰∫éÈÄöËøá KG ÁöÑÂØπË±°Ë∑ØÂæÑÁöÑËΩ®Ëøπ„ÄÇÊ≠§Ëß£ÂÜ≥ÊñπÊ°àÊèê‰æõ (a) Ë°®ËææÂäõÔºåÂõ†‰∏∫Êü•ËØ¢ÂèØ‰ª•‰ª•Â§öÁßçÊñπÂºèÂÆû‰æãÂåñÁ∫¶ÊùüÂπ∂‰ªªÊÑèÁ∫¶ÊùüÂ±ûÊÄßÂíåÂÖ≥Á≥ªÔºà‰æãÂ¶ÇÔºåÂèÇ‰∏éËÄÖ„ÄÅËµÑÊ∫êÔºâÔºõ(b) ÁÅµÊ¥ªÔºåÂõ†‰∏∫ OCEL2 ‰∫ã‰ª∂Êó•ÂøóÂèØ‰ª•Âü∫‰∫é KG ‰ª•‰ªªÊÑèÊñπÂºèÂ∫èÂàóÂåñ‰∏∫ËΩ®ËøπÔºõ‰ª•Âèä (c) ÂèØÊâ©Â±ïÊÄßÔºåÂõ†‰∏∫ÂÖ∂‰ªñ‰∫∫ÂèØ‰ª•ÈÄöËøáÂà©Áî®Áõ∏ÂêåÁöÑÂÆûÁé∞Ê®°ÂºèÊù•Êâ©Â±ïÊàë‰ª¨ÁöÑÂ∫ì„ÄÇ

##### **PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method**
2409.00092v1 by Runtao Ren, Jian Ma

As humanity stands on the brink of a new era of technological innovation, the
ability to rapidly transform creative ideas into protected intellectual
property (IP) is more crucial than ever. However, the conventional processes
for patent drafting are fraught with challenges, demanding a nuanced
understanding of advanced field knowledge and technical concepts. Existing
large language models (LLMs), while powerful, often fall short in this IP
creation domain due to their lack of specialized knowledge and
context-awareness necessary for generating technically accurate patent
documents. To bridge this critical gap, we propose a groundbreaking framework
for Knowledge Fine-Tuning (KFT) of LLMs, designed to endow AI with the ability
to autonomously mine, understand, and apply domain-specific knowledge. Our
model, PatentGPT leverages a unique combination of knowledge graph-based
pre-training, domain-specific supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF). Through extensive evaluation, PatentGPT
has demonstrated outstanding performance, scoring up to approximately 400%
higher in patent related benchmark tests compared to state-of-the-art models.
By KFT method the model's capability to not only assist but also augment human
creativity and innovation, our approach sets a new standard for AI-driven
intellectual property generation, paving the way for more efficient and
effective invention processes.

ÊëòË¶ÅÔºö<paragraph>Èö®Ëëó‰∫∫È°ûÈÇÅÂÖ•ÁßëÊäÄÂâµÊñ∞ÁöÑÊñ∞Á¥ÄÂÖÉÔºåËøÖÈÄüÂ∞áÂâµÊÑèÈªûÂ≠êËΩâÂåñÁÇ∫Âèó‰øùË≠∑ÁöÑÊô∫ÊÖßË≤°Áî¢ÔºàIPÔºâÁöÑËÉΩÂäõÊØî‰ª•ÂæÄ‰ªª‰ΩïÊôÇÂÄôÈÉΩÊõ¥Âä†ÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑÂ∞àÂà©Ëµ∑ËçâÁ®ãÂ∫èÂÖÖÊªøÊåëÊà∞ÔºåÈúÄË¶ÅÂ∞çÂÖàÈÄ≤È†òÂüüÁü•Ë≠òÂíåÊäÄË°ìÊ¶ÇÂøµÊúâÁ¥∞Á∑ªÂÖ•ÂæÆÁöÑ‰∫ÜËß£„ÄÇÁèæÊúâÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÈõñÁÑ∂Âº∑Â§ßÔºå‰ΩÜÁî±ÊñºÁº∫‰πèÁî¢ÁîüÊäÄË°ì‰∏äÊ∫ñÁ¢∫ÁöÑÂ∞àÂà©Êñá‰ª∂ÁöÑÂ∞àÊ•≠Áü•Ë≠òÂíåÊÉÖÂ¢ÉÊÑèË≠òÔºåÂõ†Ê≠§Â∏∏Â∏∏ÁÑ°Ê≥ïÊªøË∂≥Ê≠§ IP Ââµ‰ΩúÈ†òÂüüÁöÑÈúÄÊ±Ç„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÈóúÈçµÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑ LLM Áü•Ë≠òÂæÆË™ø (KFT) Êû∂ÊßãÔºåÊó®Âú®Ë≥¶‰∫à AI Ëá™‰∏ªÊåñÊéò„ÄÅÁêÜËß£ÂíåÊáâÁî®ÁâπÂÆöÈ†òÂüüÁü•Ë≠òÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÊ®°Âûã PatentGPT ÂÖÖÂàÜÂà©Áî®‰∫ÜÂü∫ÊñºÁü•Ë≠òÂúñË°®ÁöÑÈ†êË®ìÁ∑¥„ÄÅÁâπÂÆöÈ†òÂüüÁöÑÁõ£Áù£ÂºèÂæÆË™ø (SFT) Âíå‰∫∫È°ûÂõûÈ•ãÁöÑÂº∑ÂåñÂ≠∏Áøí (RLHF) ÁöÑÁç®ÁâπÁµÑÂêà„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑË©ï‰º∞ÔºåPatentGPT Â∑≤Â±ïÁèæÂá∫ÂÇëÂá∫ÁöÑË°®ÁèæÔºåÂú®ËàáÊúÄÂÖàÈÄ≤Ê®°ÂûãÁõ∏ÊØîÁöÑÂ∞àÂà©Áõ∏ÈóúÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåÂæóÂàÜÈ´òÂá∫Á¥Ñ 400%„ÄÇÈÄèÈÅé KFT ÊñπÊ≥ïÔºåÊ≠§Ê®°Âûã‰∏çÂÉÖËÉΩÂ§†ÂçîÂä©ÔºåÈÇÑËÉΩÊì¥Â¢û‰∫∫È°ûÁöÑÂâµÈÄ†ÂäõÂíåÂâµÊñ∞ÂäõÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÁÇ∫ AI È©ÖÂãïÁöÑÊô∫ÊÖßË≤°Áî¢ÁîüÊàêÊ®πÁ´ã‰∫ÜÊñ∞Ê®ôÊ∫ñÔºåÁÇ∫Êõ¥ÊúâÊïàÁéá‰∏îÊõ¥ÊúâÊïàÁöÑÁôºÊòéÊµÅÁ®ãÈã™Ë∑Ø„ÄÇ</paragraph>

##### **DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework Based on Large Language Models**
2408.14185v1 by Ziai Zhou, Bin Zhou, Hao Liu

Real-time dynamic path planning in complex traffic environments presents
challenges, such as varying traffic volumes and signal wait times. Traditional
static routing algorithms like Dijkstra and A* compute shortest paths but often
fail under dynamic conditions. Recent Reinforcement Learning (RL) approaches
offer improvements but tend to focus on local optima, risking dead-ends or
boundary issues. This paper proposes a novel approach based on causal inference
for real-time dynamic path planning, balancing global and local optimality. We
first use the static Dijkstra algorithm to compute a globally optimal baseline
path. A distributed control strategy then guides vehicles along this path. At
intersections, DynamicRouteGPT performs real-time decision-making for local
path selection, considering real-time traffic, driving preferences, and
unexpected events. DynamicRouteGPT integrates Markov chains, Bayesian
inference, and large-scale pretrained language models like Llama3 8B to provide
an efficient path planning solution. It dynamically adjusts to traffic
scenarios and driver preferences and requires no pre-training, offering broad
applicability across road networks. A key innovation is the construction of
causal graphs for counterfactual reasoning, optimizing path decisions.
Experimental results show that our method achieves state-of-the-art performance
in real-time dynamic path planning for multiple vehicles while providing
explainable path selections, offering a novel and efficient solution for
complex traffic environments.

ÊëòË¶ÅÔºöÂú®Ë§áÈõú‰∫§ÈÄöÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÂØ¶ÊôÇÂãïÊÖãË∑ØÂæëË¶èÂäÉÊúÉÈù¢Ëá®ÊåëÊà∞Ôºå‰æãÂ¶Ç‰∫§ÈÄöÊµÅÈáèËÆäÂåñÂíå‰ø°ËôüÁ≠âÂæÖÊôÇÈñì„ÄÇÂÇ≥Áµ±ÁöÑÈùúÊÖãË∑ØÁî±ÊºîÁÆóÊ≥ïÔºå‰æãÂ¶Ç Dijkstra Âíå A*ÔºåÊúÉË®àÁÆóÊúÄÁü≠Ë∑ØÂæëÔºå‰ΩÜÈÄöÂ∏∏Âú®ÂãïÊÖãÊ¢ù‰ª∂‰∏ãÊúÉÂ§±Êïó„ÄÇÊúÄËøëÁöÑÂº∑ÂåñÂ≠∏Áøí (RL) ÊñπÊ≥ïÊèê‰æõ‰∫ÜÊîπÈÄ≤Ôºå‰ΩÜÂÇæÂêëÊñºÈóúÊ≥®Â±ÄÈÉ®ÊúÄÂÑ™ÔºåÂÜíËëóÈô∑ÂÖ•Ê≠ªËÉ°ÂêåÊàñÈÇäÁïåÂïèÈ°åÁöÑÈ¢®Èö™„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂõ†ÊûúÊé®Ë´ñÁöÑÊñ∞Á©éÊñπÊ≥ïÔºåÁî®ÊñºÂØ¶ÊôÇÂãïÊÖãË∑ØÂæëË¶èÂäÉÔºåÂπ≥Ë°°ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ÊúÄÂÑ™ÊÄß„ÄÇÊàëÂÄëÈ¶ñÂÖà‰ΩøÁî®ÈùúÊÖã Dijkstra ÊºîÁÆóÊ≥ïË®àÁÆóÂÖ®Â±ÄÊúÄÂÑ™Âü∫Á∑öË∑ØÂæë„ÄÇÁÑ∂ÂæåÔºå‰∏ÄÂÄãÂàÜÂ∏ÉÂºèÊéßÂà∂Á≠ñÁï•Ê≤øËëóÈÄôÊ¢ùË∑ØÂæëÂºïÂ∞éËªäËºõ„ÄÇÂú®‰∫§ÂèâË∑ØÂè£ÔºåDynamicRouteGPT ÈáùÂ∞çÂ±ÄÈÉ®Ë∑ØÂæëÈÅ∏ÊìáÂü∑Ë°åÂØ¶ÊôÇÊ±∫Á≠ñÔºåËÄÉÈáèÂØ¶ÊôÇ‰∫§ÈÄö„ÄÅÈßïÈßõÂÅèÂ•ΩÂíåÊÑèÂ§ñ‰∫ã‰ª∂„ÄÇDynamicRouteGPT Êï¥Âêà‰∫ÜÈ¶¨ÂèØÂ§´Èèà„ÄÅË≤ùÊ∞èÊé®Ë´ñÂíå Llama3 8B Á≠âÂ§ßË¶èÊ®°È†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÔºå‰ª•Êèê‰æõÊúâÊïàÁöÑË∑ØÂæëË¶èÂäÉËß£Ê±∫ÊñπÊ°à„ÄÇÂÆÉÊúÉÂãïÊÖãË™øÊï¥Âà∞‰∫§ÈÄöÁãÄÊ≥ÅÂíåÈßïÈßõÂÅèÂ•ΩÔºå‰∏¶‰∏î‰∏çÈúÄË¶ÅÈ†êÂÖàË®ìÁ∑¥ÔºåÂú®ÈÅìË∑ØÁ∂≤Ë∑Ø‰∏äÊèê‰æõÂª£Ê≥õÁöÑÈÅ©Áî®ÊÄß„ÄÇ‰∏ÄÂÄãÈóúÈçµÂâµÊñ∞ÊòØÂª∫Á´ãÂèç‰∫ãÂØ¶Êé®ÁêÜÁöÑÂõ†ÊûúÂúñÔºå‰ª•ÊúÄ‰Ω≥ÂåñË∑ØÂæëÊ±∫Á≠ñ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®Â§öËºõËªäËºõÁöÑÂØ¶ÊôÇÂãïÊÖãË∑ØÂæëË¶èÂäÉ‰∏≠ÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂêåÊôÇÊèê‰æõÂèØËß£ÈáãÁöÑË∑ØÂæëÈÅ∏ÊìáÔºåÁÇ∫Ë§áÈõúÁöÑ‰∫§ÈÄöÁí∞Â¢ÉÊèê‰æõ‰∏ÄÁ®ÆÊñ∞Á©é‰∏îÊúâÊïàÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Exploring the Potential of Large Language Models for Heterophilic Graphs**
2408.14134v1 by Yuxia Wu, Shujie Li, Yuan Fang, Chuan Shi

Graph Neural Networks (GNNs) are essential for various graph-based learning
tasks. Notably, classical GNN architectures operate under the assumption of
homophily, which posits that connected nodes are likely to share similar
features. However, this assumption limits the effectiveness of GNNs in handling
heterophilic graphs where connected nodes often exhibit dissimilar
characteristics. Existing approaches for homophily graphs such as non-local
neighbor extension and architectural refinement overlook the rich textual data
associated with nodes, which could unlock deeper insights into these
heterophilic contexts. With advancements in Large Language Models (LLMs), there
is significant promise to enhance GNNs by leveraging the extensive open-world
knowledge within LLMs to more effectively interpret and utilize textual data
for characterizing heterophilic graphs. In this work, we explore the potential
of LLMs for modeling heterophilic graphs and propose a novel two-stage
framework: LLM-enhanced edge discriminator and LLM-guided edge reweighting.
Specifically, in the first stage, we fine-tune the LLM to better identify
homophilic and heterophilic edges based on the textual information of their
nodes. In the second stage, we adaptively manage message propagation in GNNs
for different edge types based on node features, structures, and heterophilic
or homophilic characteristics. To cope with the computational demands when
deploying LLMs in practical scenarios, we further explore model distillation
techniques to fine-tune smaller, more efficient models that maintain
competitive performance. Extensive experiments validate the effectiveness of
our framework, demonstrating the feasibility of using LLMs to enhance GNNs for
node classification on heterophilic graphs.

ÊëòË¶ÅÔºöÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∞çÊñºÂêÑÁ®ÆÂü∫ÊñºÂúñÂΩ¢ÁöÑÂ≠∏Áøí‰ªªÂãôËá≥ÈóúÈáçË¶Å„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂÇ≥Áµ±ÁöÑ GNN Êû∂ÊßãÂú®ÂêåË≥™ÊÄßÁöÑÂÅáË®≠‰∏ãÈÅã‰ΩúÔºåË©≤ÂÅáË®≠Ë™çÁÇ∫ÈÄ£Êé•ÁöÑÁØÄÈªûÂèØËÉΩÂÖ±‰∫´È°û‰ººÁöÑÁâπÂæµ„ÄÇÁÑ∂ËÄåÔºåÊ≠§ÂÅáË®≠ÈôêÂà∂‰∫Ü GNN Âú®ËôïÁêÜÁï∞Ë≥™ÊÄßÂúñÂΩ¢‰∏≠ÁöÑÊïàËÉΩÔºåÂÖ∂‰∏≠ÈÄ£Êé•ÁöÑÁØÄÈªûÈÄöÂ∏∏Ë°®ÁèæÂá∫‰∏çÂêåÁöÑÁâπÂæµ„ÄÇÁèæÊúâÁöÑÂêåË≥™ÊÄßÂúñÂΩ¢ÊñπÊ≥ïÔºà‰æãÂ¶ÇÈùûÂ±ÄÈÉ®ÈÑ∞ÂüüÂª∂‰º∏ÂíåÊû∂ÊßãÊîπÈÄ≤ÔºâÂøΩÁï•‰∫ÜËàáÁØÄÈªûÁõ∏ÈóúÁöÑË±êÂØåÊñáÊú¨Ë≥áÊñôÔºåÈÄôÂèØ‰ª•Ê∑±ÂÖ•‰∫ÜËß£ÈÄô‰∫õÁï∞Ë≥™ÊÄßËÑàÁµ°„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈÄ≤Ê≠•ÔºåÈÄèÈÅéÂà©Áî® LLM ‰∏≠Âª£Ê≥õÁöÑÈñãÊîæ‰∏ñÁïåÁü•Ë≠ò‰æÜÂ¢ûÂº∑ GNNÔºåÂ∞çÊñºÊõ¥ÊúâÊïàÂú∞Ë©ÆÈáãÂíåÂà©Áî®ÊñáÊú¨Ë≥áÊñô‰æÜË°®ÂæµÁï∞Ë≥™ÊÄßÂúñÂΩ¢ÊúâÂæàÂ§ßÁöÑÂ∏åÊúõ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Âú®Áï∞Ë≥™ÊÄßÂúñÂΩ¢Âª∫Ê®°‰∏≠ÁöÑÊΩõÂäõÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂÖ©ÈöéÊÆµÊû∂ÊßãÔºöLLM Â¢ûÂº∑ÈÇäÁ∑£Âà§Âà•Âô®Âíå LLM ÂºïÂ∞éÈÇäÁ∑£ÈáçÊñ∞Âä†Ê¨ä„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂú®Á¨¨‰∏ÄÈöéÊÆµÔºåÊàëÂÄëÂæÆË™ø LLM ‰ª•Ê†πÊìöÂÖ∂ÁØÄÈªûÁöÑÊñáÊú¨Ë≥áË®äÔºåÊõ¥Â•ΩÂú∞Ë≠òÂà•ÂêåË≥™ÊÄßÂíåÁï∞Ë≥™ÊÄßÈÇäÁ∑£„ÄÇÂú®Á¨¨‰∫åÈöéÊÆµÔºåÊàëÂÄëÊ†πÊìöÁØÄÈªûÁâπÂæµ„ÄÅÁµêÊßãÂíåÁï∞Ë≥™ÊÄßÊàñÂêåË≥™ÊÄßÁâπÂæµÔºåËá™ÈÅ©ÊáâÂú∞ÁÆ°ÁêÜ GNN ‰∏≠‰∏çÂêåÈÇäÁ∑£È°ûÂûãÁöÑË®äÊÅØÂÇ≥ÈÅû„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÂú®ÂØ¶ÈöõÂ†¥ÊôØ‰∏≠ÈÉ®ÁΩ≤ LLM ÊôÇÁöÑË®àÁÆóÈúÄÊ±ÇÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Êé¢Ë®éÊ®°ÂûãËêÉÂèñÊäÄË°ìÔºå‰ª•ÂæÆË™øËºÉÂ∞è„ÄÅÊõ¥ÊúâÊïàÁéáÁöÑÊ®°ÂûãÔºå‰ª•Á∂≠ÊåÅÁ´∂Áà≠Âäõ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÊû∂ÊßãÁöÑÊúâÊïàÊÄßÔºåË≠âÊòé‰∫Ü‰ΩøÁî® LLM ‰æÜÂ¢ûÂº∑ GNN ‰ª•ÈÄ≤Ë°åÁï∞Ë≥™ÊÄßÂúñÂΩ¢‰∏äÁöÑÁØÄÈªûÂàÜÈ°ûÁöÑÂèØË°åÊÄß„ÄÇ

##### **Towards Graph Prompt Learning: A Survey and Beyond**
2408.14520v3 by Qingqing Long, Yuchen Yan, Peiyan Zhang, Chen Fang, Wentao Cui, Zhiyuan Ning, Meng Xiao, Ning Cao, Xiao Luo, Lingjun Xu, Shiyue Jiang, Zheng Fang, Chong Chen, Xian-Sheng Hua, Yuanchun Zhou

Large-scale "pre-train and prompt learning" paradigms have demonstrated
remarkable adaptability, enabling broad applications across diverse domains
such as question answering, image recognition, and multimodal retrieval. This
approach fully leverages the potential of large-scale pre-trained models,
reducing downstream data requirements and computational costs while enhancing
model applicability across various tasks. Graphs, as versatile data structures
that capture relationships between entities, play pivotal roles in fields such
as social network analysis, recommender systems, and biological graphs. Despite
the success of pre-train and prompt learning paradigms in Natural Language
Processing (NLP) and Computer Vision (CV), their application in graph domains
remains nascent. In graph-structured data, not only do the node and edge
features often have disparate distributions, but the topological structures
also differ significantly. This diversity in graph data can lead to
incompatible patterns or gaps between pre-training and fine-tuning on
downstream graphs. We aim to bridge this gap by summarizing methods for
alleviating these disparities. This includes exploring prompt design
methodologies, comparing related techniques, assessing application scenarios
and datasets, and identifying unresolved problems and challenges. This survey
categorizes over 100 relevant works in this field, summarizing general design
principles and the latest applications, including text-attributed graphs,
molecules, proteins, and recommendation systems. Through this extensive review,
we provide a foundational understanding of graph prompt learning, aiming to
impact not only the graph mining community but also the broader Artificial
General Intelligence (AGI) community.

ÊëòË¶ÅÔºö<paragraph>Â§ßË¶èÊ®°„ÄåÈ†êË®ìÁ∑¥ËàáÊèêÁ§∫Â≠∏Áøí„ÄçÁØÑ‰æãÂ∑≤Â±ïÁèæÂá∫ÂçìË∂äÁöÑÈÅ©ÊáâÊÄßÔºåËÉΩÂª£Ê≥õÊáâÁî®ÊñºÂêÑÁ®ÆÈ†òÂüüÔºå‰æãÂ¶ÇÂïèÁ≠î„ÄÅÂΩ±ÂÉèËæ®Ë≠òÂíåÂ§öÊ®°ÊÖãÊ™¢Á¥¢„ÄÇÊ≠§ÊñπÊ≥ïÂÖÖÂàÜÂà©Áî®‰∫ÜÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°ÂûãÁöÑÊΩõÂäõÔºåÈôç‰Ωé‰∫Ü‰∏ãÊ∏∏Ë≥áÊñôÈúÄÊ±ÇÂíåÈÅãÁÆóÊàêÊú¨ÔºåÂêåÊôÇÊèêÂçá‰∫ÜÊ®°ÂûãÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÂúñÂΩ¢‰ΩúÁÇ∫ËÉΩÊçïÊçâÂØ¶È´î‰πãÈñìÈóú‰øÇÁöÑÂ§öÂäüËÉΩË≥áÊñôÁµêÊßãÔºåÂú®Á§æÁæ§Á∂≤Ë∑ØÂàÜÊûê„ÄÅÊé®Ëñ¶Á≥ªÁµ±ÂíåÁîüÁâ©ÂúñÂΩ¢Á≠âÈ†òÂüüÊâÆÊºîËëóÈóúÈçµËßíËâ≤„ÄÇÂÑòÁÆ°È†êË®ìÁ∑¥ËàáÊèêÁ§∫Â≠∏ÁøíÁØÑ‰æãÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÂíåÈõªËÖ¶Ë¶ñË¶∫ (CV) ‰∏≠Áç≤ÂæóÊàêÂäüÔºåÂÆÉÂÄëÂú®ÂúñÂΩ¢È†òÂüüÁöÑÊáâÁî®‰ªçÂ±¨Ëµ∑Ê≠•ÈöéÊÆµ„ÄÇÂú®ÂúñÂΩ¢ÁµêÊßãË≥áÊñô‰∏≠ÔºåÁØÄÈªûÂíåÈÇäÁ∑£ÁâπÂæµ‰∏çÂÉÖÁ∂ìÂ∏∏Êúâ‰∏çÂêåÁöÑÂàÜ‰ΩàÔºåÂÖ∂ÊãìÊí≤ÁµêÊßã‰πüÂ§ß‰∏çÁõ∏Âêå„ÄÇÂúñÂΩ¢Ë≥áÊñôÁöÑÈÄôÁ®ÆÂ§öÊ®£ÊÄßÂèØËÉΩÂ∞éËá¥È†êË®ìÁ∑¥ÂíåÂæÆË™ø‰πãÈñìÂá∫Áèæ‰∏çÁõ∏ÂÆπÁöÑÊ®°ÂºèÊàñÂ∑ÆË∑ù„ÄÇÊàëÂÄëÊó®Âú®ÈÄèÈÅéÁ∏ΩÁµêÊ∏õËºïÈÄô‰∫õÂ∑ÆÁï∞ÁöÑÊñπÊ≥ï‰æÜÂΩåË£úÊ≠§Â∑ÆË∑ù„ÄÇÈÄôÂåÖÊã¨Êé¢Á¥¢ÊèêÁ§∫Ë®≠Ë®àÊñπÊ≥ï„ÄÅÊØîËºÉÁõ∏ÈóúÊäÄË°ì„ÄÅË©ï‰º∞ÊáâÁî®Â†¥ÊôØÂíåË≥áÊñôÈõÜÔºå‰ª•ÂèäÊâæÂá∫Êú™Ëß£Ê±∫ÁöÑÂïèÈ°åÂíåÊåëÊà∞„ÄÇÊú¨Ë™øÊü•ÂàÜÈ°û‰∫ÜÊ≠§È†òÂüü‰∏≠ 100 Â§öÈ†ÖÁõ∏ÈóúËëó‰ΩúÔºåÁ∏ΩÁµê‰∫Ü‰∏ÄËà¨Ë®≠Ë®àÂéüÂâáÂíåÊúÄÊñ∞ÊáâÁî®ÔºåÂåÖÊã¨ÊñáÂ≠óÂ±¨ÊÄßÂúñÂΩ¢„ÄÅÂàÜÂ≠ê„ÄÅËõãÁôΩË≥™ÂíåÊé®Ëñ¶Á≥ªÁµ±„ÄÇÈÄèÈÅéÈÄôÈ†ÖÂª£Ê≥õÁöÑÂõûÈ°ßÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂúñÂΩ¢ÊèêÁ§∫Â≠∏ÁøíÁöÑÂü∫Êú¨ÁêÜËß£ÔºåÊó®Âú®‰∏çÂÉÖÂΩ±ÈüøÂúñÂΩ¢ÊåñÊéòÁ§æÁæ§Ôºå‰πüÂΩ±ÈüøÊõ¥Âª£Ê≥õÁöÑ‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖß (AGI) Á§æÁæ§„ÄÇ</paragraph>

##### **CodeGraph: Enhancing Graph Reasoning of LLMs with Code**
2408.13863v1 by Qiaolong Cai, Zhaowei Wang, Shizhe Diao, James Kwok, Yangqiu Song

With the increasing popularity of large language models (LLMs), reasoning on
basic graph algorithm problems is an essential intermediate step in assessing
their abilities to process and infer complex graph reasoning tasks. Existing
methods usually convert graph-structured data to textual descriptions and then
use LLMs for reasoning and computation. However, LLMs often produce computation
errors on arithmetic parts in basic graph algorithm problems, such as counting
number of edges. In addition, they struggle to control or understand the output
of the reasoning process, raising concerns about whether LLMs are simply
guessing. In this paper, we introduce CodeGraph, a method that encodes graph
problem solutions as code. The methods solve new graph problems by learning
from exemplars, generating programs, and executing them via a program
interpreter. Using the few-shot setting, we evaluate CodeGraph with the base
LLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and
Mixtral-8x7B Instruct. Experimental results on six tasks with six graph
encoding methods in the GraphQA dataset demonstrate that CodeGraph can boost
performance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on
the task. Compared to the existing methods, CodeGraph demonstrates strong
performance on arithmetic problems in graph tasks and offers a more
controllable and interpretable approach to the reasoning process.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊó•Êº∏ÊôÆÂèäÔºåÂ∞çÂü∫Êú¨ÂúñÂΩ¢ÊºîÁÆóÊ≥ïÂïèÈ°åÈÄ≤Ë°åÊé®ÁêÜÊòØË©ï‰º∞ÂÆÉÂÄëËôïÁêÜÂíåÊé®Ë´ñË§áÈõúÂúñÂΩ¢Êé®ÁêÜ‰ªªÂãôÁöÑËÉΩÂäõ‰∏≠‰∏ÄÂÄãÈáçË¶ÅÁöÑ‰∏≠ÈñìÊ≠•È©ü„ÄÇÁèæÊúâÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÂ∞áÂúñÂΩ¢ÁµêÊßãÂåñÁöÑË≥áÊñôËΩâÊèõÊàêÊñáÂ≠óÊèèËø∞ÔºåÁÑ∂Âæå‰ΩøÁî® LLM ÈÄ≤Ë°åÊé®ÁêÜÂíåÈÅãÁÆó„ÄÇÁÑ∂ËÄåÔºåLLM ÈÄöÂ∏∏ÊúÉÂú®Âü∫Êú¨ÂúñÂΩ¢ÊºîÁÆóÊ≥ïÂïèÈ°å‰∏≠Ôºå‰æãÂ¶ÇË®àÁÆóÈÇäÁ∑£Êï∏ÈáèÔºåÂ∞çÁÆóË°ìÈÉ®ÂàÜÁî¢ÁîüÈÅãÁÆóÈåØË™§„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÄëÈõ£‰ª•ÊéßÂà∂ÊàñÁêÜËß£Êé®ÁêÜÈÅéÁ®ãÁöÑËº∏Âá∫ÔºåÈÄôÂºïÁôº‰∫Ü LLM ÊòØÂê¶Âè™ÊòØÂú®ÁåúÊ∏¨ÁöÑÁñëÊÖÆ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü CodeGraphÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞áÂúñÂΩ¢ÂïèÈ°åËß£Ê±∫ÊñπÊ°àÁ∑®Á¢ºÁÇ∫Á®ãÂºèÁ¢ºÁöÑÊñπÊ≥ï„ÄÇÈÄô‰∫õÊñπÊ≥ïÈÄèÈÅéÂ≠∏ÁøíÁØÑ‰æã„ÄÅÁî¢ÁîüÁ®ãÂºèÔºå‰∏¶ÈÄèÈÅéÁ®ãÂºèÁ¢ºÁõ¥Ë≠ØÂô®Âü∑Ë°åÂÆÉÂÄë‰æÜËß£Ê±∫Êñ∞ÁöÑÂúñÂΩ¢ÂïèÈ°å„ÄÇ‰ΩøÁî®Â∞ëÊ¨°ÂòóË©¶Ë®≠ÂÆöÔºåÊàëÂÄë‰ΩøÁî®Âü∫Á§é LLM ÁÇ∫ GPT-3.5 Turbo„ÄÅLlama3-70B Instruct„ÄÅMixtral-8x22B Instruct Âíå Mixtral-8x7B Instruct ‰æÜË©ï‰º∞ CodeGraph„ÄÇÂú® GraphQA Ë≥áÊñôÈõÜ‰∏≠‰ΩøÁî®ÂÖ≠Á®ÆÂúñÂΩ¢Á∑®Á¢ºÊñπÊ≥ïÂ∞çÂÖ≠È†Ö‰ªªÂãôÈÄ≤Ë°åÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåCodeGraph ÂèØ‰ª•Â∞á LLM ‰∏≠ÁöÑÂúñÂΩ¢Êé®ÁêÜ‰ªªÂãôÁöÑÊïàËÉΩÊèêÂçá 1.3% Âà∞ 58.6%ÔºåÂÖ∑È´îÂèñÊ±∫Êñº‰ªªÂãô„ÄÇËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCodeGraph Âú®ÂúñÂΩ¢‰ªªÂãô‰∏≠ÁöÑÁÆóË°ìÂïèÈ°å‰∏äË°®ÁèæÂá∫Âº∑ÂãÅÁöÑÊïàËÉΩÔºå‰∏¶ÁÇ∫Êé®ÁêÜÈÅéÁ®ãÊèê‰æõÊõ¥ÂÖ∑ÂèØÊéßÊÄßÂíåÂèØËß£ÈáãÊÄßÁöÑÊñπÊ≥ï„ÄÇ

##### **LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings**
2408.14512v1 by Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu

Zero-shot graph machine learning, especially with graph neural networks
(GNNs), has garnered significant interest due to the challenge of scarce
labeled data. While methods like self-supervised learning and graph prompt
learning have been extensively explored, they often rely on fine-tuning with
task-specific labels, limiting their effectiveness in zero-shot scenarios.
Inspired by the zero-shot capabilities of instruction-fine-tuned large language
models (LLMs), we introduce a novel framework named Token Embedding-Aligned
Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and
cross-task zero-shot learners for graph machine learning. Concretely, we
pretrain a GNN, aligning its representations with token embeddings of an LLM.
We then train a linear projector that transforms the GNN's representations into
a fixed number of graph token embeddings without tuning the LLM. A unified
instruction is designed for various graph tasks at different levels, such as
node classification (node-level) and link prediction (edge-level). These design
choices collectively enhance our method's effectiveness in zero-shot learning,
setting it apart from existing methods. Experiments show that our graph token
embeddings help the LLM predictor achieve state-of-the-art performance on
unseen datasets and tasks compared to other methods using LLMs as predictors.

ÊëòË¶ÅÔºöÈõ∂ÁØÑ‰æãÂúñÂΩ¢Ê©üÂô®Â≠∏ÁøíÔºåÁâπÂà•ÊòØÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN)ÔºåÁî±ÊñºÁ®ÄÊúâÊ®ôÁ±§Ë≥áÊñôÁöÑÊåëÊà∞ËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÈõñÁÑ∂Ëá™Áõ£Áù£ÂºèÂ≠∏ÁøíÂíåÂúñÂΩ¢ÊèêÁ§∫Â≠∏ÁøíÁ≠âÊñπÊ≥ïÂ∑≤Ë¢´Âª£Ê≥õÊé¢Á¥¢Ôºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏‰æùË≥¥Êñº‰ªªÂãôÁâπÂÆöÊ®ôÁ±§ÁöÑÂæÆË™øÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®Èõ∂ÁØÑ‰æãÂ†¥ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂèóÂà∞Êåá‰ª§ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈõ∂ÁØÑ‰æãÂäüËÉΩÁöÑÂïüÁôºÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ Token Embedding-Aligned Graph Language Model (TEA-GLM) ÁöÑÊñ∞Ê°ÜÊû∂ÔºåÂÆÉÂà©Áî® LLM ‰ΩúÁÇ∫Ë∑®Ë≥áÊñôÈõÜÂíåË∑®‰ªªÂãôÁöÑÈõ∂ÁØÑ‰æãÂ≠∏ÁøíÂô®ÔºåÁî®ÊñºÂúñÂΩ¢Ê©üÂô®Â≠∏Áøí„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ†êË®ìÁ∑¥‰∏ÄÂÄã GNNÔºåÂ∞áÂÖ∂Ë°®Á§∫Ëàá LLM ÁöÑ token embedding Â∞çÈΩä„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëË®ìÁ∑¥‰∏ÄÂÄãÁ∑öÊÄßÊäïÂΩ±Ê©üÔºåÂ∞á GNN ÁöÑË°®Á§∫ËΩâÊèõÁÇ∫Âõ∫ÂÆöÊï∏ÈáèÁöÑÂúñÂΩ¢ token embeddingÔºåËÄåÁÑ°ÈúÄË™øÊï¥ LLM„ÄÇÁµ±‰∏ÄÁöÑÊåá‰ª§ÊòØÁÇ∫‰∏çÂêåÂ±§Á¥öÁöÑÂêÑÁ®ÆÂúñÂΩ¢‰ªªÂãôË®≠Ë®àÁöÑÔºå‰æãÂ¶ÇÁØÄÈªûÂàÜÈ°ûÔºàÁØÄÈªûÂ±§Á¥öÔºâÂíåÈÄ£ÁµêÈ†êÊ∏¨ÔºàÈÇäÁ∑£Â±§Á¥öÔºâ„ÄÇÈÄô‰∫õË®≠Ë®àÈÅ∏ÊìáÂÖ±ÂêåÂ¢ûÂº∑‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Èõ∂ÁØÑ‰æãÂ≠∏Áøí‰∏≠ÁöÑÊúâÊïàÊÄßÔºå‰ΩøÂÖ∂ÊúâÂà•ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇÂØ¶È©óË°®ÊòéÔºåËàá‰ΩøÁî® LLM ‰ΩúÁÇ∫È†êÊ∏¨Âô®ÁöÑÂÖ∂‰ªñÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂúñÂΩ¢ token embedding Âπ´Âä© LLM È†êÊ∏¨Âô®Âú®Êú™Ë¶ãÈÅéÁöÑË≥áÊñôÈõÜÂíå‰ªªÂãô‰∏äÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇ

##### **Hierarchical Network Fusion for Multi-Modal Electron Micrograph Representation Learning with Foundational Large Language Models**
2408.13661v1 by Sakhinana Sagar Srinivas, Geethan Sannidhi, Venkataramana Runkana

Characterizing materials with electron micrographs is a crucial task in
fields such as semiconductors and quantum materials. The complex hierarchical
structure of micrographs often poses challenges for traditional classification
methods. In this study, we propose an innovative backbone architecture for
analyzing electron micrographs. We create multi-modal representations of the
micrographs by tokenizing them into patch sequences and, additionally,
representing them as vision graphs, commonly referred to as patch attributed
graphs. We introduce the Hierarchical Network Fusion (HNF), a multi-layered
network structure architecture that facilitates information exchange between
the multi-modal representations and knowledge integration across different
patch resolutions. Furthermore, we leverage large language models (LLMs) to
generate detailed technical descriptions of nanomaterials as auxiliary
information to assist in the downstream task. We utilize a cross-modal
attention mechanism for knowledge fusion across cross-domain
representations(both image-based and linguistic insights) to predict the
nanomaterial category. This multi-faceted approach promises a more
comprehensive and accurate representation and classification of micrographs for
nanomaterial identification. Our framework outperforms traditional methods,
overcoming challenges posed by distributional shifts, and facilitating
high-throughput screening.

ÊëòË¶ÅÔºöÂà©Áî®ÈõªÂ≠êÈ°ØÂæÆÁÖßÁâá‰æÜË°®ÂæµÊùêÊñôÔºåÂú®ÂçäÂ∞éÈ´îÂíåÈáèÂ≠êÊùêÊñôÁ≠âÈ†òÂüü‰∏≠ÊòØ‰∏ÄÈ†ÖËá≥ÈóúÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇÈ°ØÂæÆÁÖßÁâáË§áÈõúÁöÑÂàÜÂ±§ÁµêÊßãÈÄöÂ∏∏ÊúÉÂ∞çÂÇ≥Áµ±ÂàÜÈ°ûÊñπÊ≥ïÂ∏∂‰æÜÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑ‰∏ªÂππÊû∂ÊßãÔºåÁî®ÊñºÂàÜÊûêÈõªÂ≠êÈ°ØÂæÆÁÖßÁâá„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÈ°ØÂæÆÁÖßÁâá‰ª£ÊèõÊàêÂçÄÂ°äÂ∫èÂàó‰æÜÂª∫Á´ãÂÖ∂Â§öÊ®°ÊÖãË°®Á§∫ÔºåÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÂ∞áÂÖ∂Ë°®Á§∫ÁÇ∫Ë¶ñË¶∫ÂúñÂΩ¢ÔºåÈÄöÂ∏∏Á®±ÁÇ∫ÂçÄÂ°äÂ±¨ÊÄßÂúñÂΩ¢„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÂàÜÂ±§Á∂≤Ë∑ØËûçÂêà (HNF)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ§öÂ±§Á∂≤Ë∑ØÁµêÊßãÊû∂ÊßãÔºåÊúâÂä©ÊñºÂ§öÊ®°ÊÖãË°®Á§∫‰πãÈñìÁöÑË≥áË®ä‰∫§ÊèõÔºå‰ª•Âèä‰∏çÂêåÂçÄÂ°äËß£ÊûêÂ∫¶‰πãÈñìÁöÑÁü•Ë≠òÊï¥Âêà„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÁî¢ÁîüÂ•àÁ±≥ÊùêÊñôÁöÑË©≥Á¥∞ÊäÄË°ìË™™ÊòéÔºå‰ΩúÁÇ∫ËºîÂä©Ë≥áË®äÔºå‰ª•ÂçîÂä©‰∏ãÊ∏∏‰ªªÂãô„ÄÇÊàëÂÄëÂà©Áî®Ë∑®Ê®°ÊÖãÊ≥®ÊÑèÂäõÊ©üÂà∂ÔºåÂú®Ë∑®È†òÂüüË°®Á§∫ÔºàÂü∫ÊñºÂΩ±ÂÉèÂíåË™ûË®ÄÊ¥ûÂØüÂäõÔºâ‰∏≠ÈÄ≤Ë°åÁü•Ë≠òËûçÂêàÔºå‰ª•È†êÊ∏¨Â•àÁ±≥ÊùêÊñôÈ°ûÂà•„ÄÇÈÄôÁ®ÆÂ§öÊñπÈù¢ÁöÑÂÅöÊ≥ïÊúâÊúõÁÇ∫Â•àÁ±≥ÊùêÊñôË≠òÂà•Êèê‰æõÊõ¥ÂÖ®Èù¢‰∏îÊ∫ñÁ¢∫ÁöÑË°®Á§∫ÂíåÂàÜÈ°û„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂÑ™ÊñºÂÇ≥Áµ±ÊñπÊ≥ïÔºåÂÖãÊúç‰∫ÜÂàÜ‰ΩàËΩâÁßªÂ∏∂‰æÜÁöÑÊåëÊà∞Ôºå‰∏¶‰øÉÈÄ≤‰∫ÜÈ´òÈÄöÈáèÁØ©ÈÅ∏„ÄÇ

##### **GNN: Graph Neural Network and Large Language Model for Data Discovery**
2408.13609v2 by Thomas Hoang

Our algorithm GNN: Graph Neural Network and Large Language Model for Data
Discovery inherit the benefits of \cite{hoang2024plod} (PLOD: Predictive
Learning Optimal Data Discovery), \cite{Hoang2024BODBO} (BOD: Blindly Optimal
Data Discovery) in terms of overcoming the challenges of having to predefine
utility function and the human input for attribute ranking, which helps prevent
the time-consuming loop process. In addition to these previous works, our
algorithm GNN leverages the advantages of graph neural networks and large
language models to understand text type values that cannot be understood by
PLOD and MOD, thus making the task of predicting outcomes more reliable. GNN
could be seen as an extension of PLOD in terms of understanding the text type
value and the user's preferences, not only numerical values but also text
values, making the promise of data science and analytics purposes.

ÊëòË¶ÅÔºöÊàëÂÄëÁöÑÊºîÁÆóÊ≥ï GNNÔºöÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÔºåÁî®ÊñºË≥áÊñôÊé¢Á¥¢ÔºåÁπºÊâø‰∫Ü \cite{hoang2024plod}ÔºàPLODÔºöÈ†êÊ∏¨ÊÄßÊúÄ‰Ω≥Ë≥áÊñôÊé¢Á¥¢Ôºâ„ÄÅ\cite{Hoang2024BODBO}ÔºàBODÔºöÁõ≤ÁõÆÊúÄ‰Ω≥Ë≥áÊñôÊé¢Á¥¢ÔºâÁöÑÂÑ™ÈªûÔºåÂú®ÊñºÂÖãÊúçÂøÖÈ†àÈ†êÂÖàÂÆöÁæ©ÊïàÁî®ÂáΩÊï∏Âíå‰∫∫È°ûËº∏ÂÖ•Â±¨ÊÄßÊéíÂêçÁöÑÊåëÊà∞ÔºåÈÄôÊúâÂä©ÊñºÈò≤Ê≠¢ËÄóÊôÇÁöÑËø¥ÂúàËôïÁêÜ„ÄÇÈô§‰∫ÜÈÄô‰∫õÂÖàÂâçÁöÑ‰ΩúÂìÅÔºåÊàëÂÄëÁöÑÊºîÁÆóÊ≥ï GNN Âà©Áî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ§ßË™ûË®ÄÊ®°ÂûãÁöÑÂÑ™ÈªûÔºå‰æÜÁêÜËß£ PLOD Âíå MOD ÁÑ°Ê≥ïÁêÜËß£ÁöÑÊñáÂ≠óÈ°ûÂûãÂÄºÔºåÂæûËÄå‰ΩøÈ†êÊ∏¨ÁµêÊûúÁöÑ‰ªªÂãôÊõ¥ÂèØÈù†„ÄÇGNN ÂèØ‰ª•Ë¶ñÁÇ∫ PLOD Âú®ÁêÜËß£ÊñáÂ≠óÈ°ûÂûãÂÄºÂíå‰ΩøÁî®ËÄÖÂÅèÂ•ΩÊñπÈù¢ÁöÑÂª∂‰º∏Ôºå‰∏çÂÉÖÊòØÊï∏ÂÄºÔºåÈÇÑÊúâÊñáÂ≠óÂÄºÔºåÈÄôÂØ¶Áèæ‰∫ÜË≥áÊñôÁßëÂ≠∏ÂíåÂàÜÊûêÁõÆÁöÑÁöÑÊâøË´æ„ÄÇ

##### **HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation**
2408.13521v1 by Azmine Toushik Wasi

Knowledge Graphs (KGs) serving as semantic networks, prove highly effective
in managing complex interconnected data in different domains, by offering a
unified, contextualized, and structured representation with flexibility that
allows for easy adaptation to evolving knowledge. Processing complex Human
Resources (HR) data, KGs can help in different HR functions like recruitment,
job matching, identifying learning gaps, and enhancing employee retention.
Despite their potential, limited efforts have been made to implement practical
HR knowledge graphs. This study addresses this gap by presenting a framework
for effectively developing HR knowledge graphs from documents using Large
Language Models. The resulting KG can be used for a variety of downstream
tasks, including job matching, identifying employee skill gaps, and many more.
In this work, we showcase instances where HR KGs prove instrumental in precise
job matching, yielding advantages for both employers and employees. Empirical
evidence from experiments with information propagation in KGs and Graph Neural
Nets, along with case studies underscores the effectiveness of KGs in tasks
such as job and employee recommendations and job area classification. Code and
data are available at : https://github.com/azminewasi/HRGraph

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ‰ΩúÁÇ∫Ë™ûÁæ©Á∂≤Ë∑ØÔºåË≠âÊòéÂú®ÁÆ°ÁêÜ‰∏çÂêåÈ†òÂüü‰∏≠Ë§áÈõúÁöÑ‰∫íÈÄ£Ë≥áÊñôÊñπÈù¢ÈùûÂ∏∏ÊúâÊïàÔºåÈÄèÈÅéÊèê‰æõÁµ±‰∏Ä„ÄÅËÑàÁµ°Âåñ‰∏îÁµêÊßãÂåñÁöÑË°®Á§∫Ôºå‰∏¶ÂÖ∑ÂÇôÈùàÊ¥ªÊÄßÔºåÂèØËºïÈ¨ÜÈÅ©Êáâ‰∏çÊñ∑ËÆäÂåñÁöÑÁü•Ë≠ò„ÄÇKG ËôïÁêÜË§áÈõúÁöÑ‰∫∫ÂäõË≥áÊ∫ê (HR) Ë≥áÊñôÔºåÊúâÂä©Êñº‰∏çÂêåÁöÑ HR ÂäüËÉΩÔºå‰æãÂ¶ÇÊãõÂãü„ÄÅÂ∑•‰ΩúÂåπÈÖç„ÄÅÊâæÂá∫Â≠∏ÁøíÂ∑ÆË∑ùÂíåÊèêÂçáÂì°Â∑•ÁïôÂ≠òÁéá„ÄÇÂÑòÁÆ°ÊúâÂÖ∂ÊΩõÂäõÔºå‰ΩÜÂØ¶‰ΩúÂØ¶Áî®ÁöÑ HR Áü•Ë≠òÂúñË≠úÁöÑÂä™ÂäõÊúâÈôê„ÄÇÊú¨Á†îÁ©∂ÈÄèÈÅéÊèêÂá∫‰∏ÄÂÄãÊû∂ÊßãÔºåÂæûÊñá‰ª∂‰∏≠‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊúâÊïàÈñãÁôº HR Áü•Ë≠òÂúñË≠úÔºå‰æÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ù„ÄÇÁî¢ÁîüÁöÑ KG ÂèØÁî®ÊñºÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãôÔºåÂåÖÊã¨Â∑•‰ΩúÂåπÈÖç„ÄÅÊâæÂá∫Âì°Â∑•ÊäÄËÉΩÂ∑ÆË∑ùÁ≠â„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü HR KG Âú®Á≤æÁ¢∫Â∑•‰ΩúÂåπÈÖç‰∏≠Ë≠âÊòéÊúâÁî®ÁöÑÁØÑ‰æãÔºåÁÇ∫Èõá‰∏ªÂíåÂì°Â∑•Â∏∂‰æÜÂÑ™Âã¢„ÄÇÈÄèÈÅé KG ÂíåÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠Ë≥áË®äÂÇ≥Êí≠ÁöÑÂØ¶È©óÊâÄÂæóÁöÑÂØ¶Ë≠âÔºå‰ª•ÂèäÊ°à‰æãÁ†îÁ©∂ÔºåÂº∑Ë™ø‰∫Ü KG Âú®Â∑•‰ΩúÂíåÂì°Â∑•Êé®Ëñ¶‰ª•ÂèäÂ∑•‰ΩúÈ†òÂüüÂàÜÈ°ûÁ≠â‰ªªÂãô‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÔºöhttps://github.com/azminewasi/HRGraph

##### **Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation**
2408.13432v1 by Yi-Hui Chen, Eric Jui-Lin Lu, Kwan-Ho Cheng

The main task of the KGQA system (Knowledge Graph Question Answering) is to
convert user input questions into query syntax (such as SPARQL). With the rise
of modern popular encoders and decoders like Transformer and ConvS2S, many
scholars have shifted the research direction of SPARQL generation to the Neural
Machine Translation (NMT) architecture or the generative AI field of
Text-to-SPARQL. In NMT-based QA systems, the system treats knowledge base query
syntax as a language. It uses NMT-based translation models to translate natural
language questions into query syntax. Scholars use popular architectures
equipped with cross-attention, such as Transformer, ConvS2S, and BiLSTM, to
train translation models for query syntax. To achieve better query results,
this paper improved the ConvS2S encoder and added multi-head attention from the
Transformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the
n-gram language model. The principle is to use convolutional layers to capture
local hidden features in the input sequence with different receptive fields,
using multi-head attention to calculate dependencies between them. Ultimately,
we found that the translation model based on the Multi-Head Conv encoder
achieved better performance than other encoders, obtaining 76.52\% and 83.37\%
BLEU-1 (BiLingual Evaluation Understudy) on the QALD-9 and LC-QuAD-1.0
datasets, respectively. Additionally, in the end-to-end system experiments on
the QALD-9 and LC-QuAD-1.0 datasets, we achieved leading results over other
KGQA systems, with Macro F1-measures reaching 52\% and 66\%, respectively.
Moreover, the experimental results show that with limited computational
resources, if one possesses an excellent encoder-decoder architecture and
cross-attention, experts and scholars can achieve outstanding performance
equivalent to large pre-trained models using only general embeddings.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË°®ÂïèÁ≠îÁ≥ªÁµ± (KGQA) ÁöÑ‰∏ªË¶Å‰ªªÂãôÊòØÂ∞á‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁöÑÂïèÈ°åËΩâÊèõÊàêÊü•Ë©¢Ë™ûÊ≥ï (‰æãÂ¶Ç SPARQL)„ÄÇÈö®Ëëó Transformer Âíå ConvS2S Á≠âÁèæ‰ª£ÊµÅË°åÁ∑®Á¢ºÂô®ÂíåËß£Á¢ºÂô®ÁöÑÂ¥õËµ∑ÔºåË®±Â§öÂ≠∏ËÄÖÂ∑≤Â∞á SPARQL ÁîüÊàêÁöÑÁ†îÁ©∂ÊñπÂêëËΩâÁßªÂà∞Á•ûÁ∂ìÊ©üÂô®ÁøªË≠Ø (NMT) Êû∂ÊßãÊàñÊñáÂ≠óËΩâ SPARQL ÁöÑÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÈ†òÂüü„ÄÇÂú®Âü∫Êñº NMT ÁöÑÂïèÁ≠îÁ≥ªÁµ±‰∏≠ÔºåÁ≥ªÁµ±Â∞áÁü•Ë≠òÂ∫´Êü•Ë©¢Ë™ûÊ≥ïË¶ñÁÇ∫‰∏ÄÁ®ÆË™ûË®Ä„ÄÇÂÆÉ‰ΩøÁî®Âü∫Êñº NMT ÁöÑÁøªË≠ØÊ®°ÂûãÂ∞áËá™ÁÑ∂Ë™ûË®ÄÂïèÈ°åËΩâÊèõÊàêÊü•Ë©¢Ë™ûÊ≥ï„ÄÇÂ≠∏ËÄÖ‰ΩøÁî®ÈÖçÂÇôË∑®Ê≥®ÊÑèÂäõÊ©üÂà∂ÁöÑÁÜ±ÈñÄÊû∂ÊßãÔºå‰æãÂ¶Ç Transformer„ÄÅConvS2S Âíå BiLSTMÔºå‰æÜË®ìÁ∑¥Êü•Ë©¢Ë™ûÊ≥ïÁöÑÁøªË≠ØÊ®°Âûã„ÄÇÁÇ∫‰∫ÜÁç≤ÂæóÊõ¥Â•ΩÁöÑÊü•Ë©¢ÁµêÊûúÔºåÊú¨ÊñáÊîπÈÄ≤‰∫Ü ConvS2S Á∑®Á¢ºÂô®Ôºå‰∏¶Âæû Transformer ‰∏≠Âä†ÂÖ•Â§öÈ†≠Ê≥®ÊÑèÂäõÊ©üÂà∂ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº n-gram Ë™ûË®ÄÊ®°ÂûãÁöÑÂ§öÈ†≠Âç∑Á©çÁ∑®Á¢ºÂô® (MHC Á∑®Á¢ºÂô®)„ÄÇÂÖ∂ÂéüÁêÜÊòØ‰ΩøÁî®Âç∑Á©çÂ±§‰ª•‰∏çÂêåÁöÑÊÑüÂèóÈáéÊì∑ÂèñËº∏ÂÖ•Â∫èÂàó‰∏≠ÁöÑÂ±ÄÈÉ®Èö±ËóèÁâπÂæµÔºå‰∏¶‰ΩøÁî®Â§öÈ†≠Ê≥®ÊÑèÂäõÊ©üÂà∂Ë®àÁÆóÂÆÉÂÄë‰πãÈñìÁöÑ‰æùË≥¥Èóú‰øÇ„ÄÇÊúÄÁµÇÔºåÊàëÂÄëÁôºÁèæÂü∫ÊñºÂ§öÈ†≠Âç∑Á©çÁ∑®Á¢ºÂô®ÁöÑÁøªË≠ØÊ®°ÂûãÊØîÂÖ∂‰ªñÁ∑®Á¢ºÂô®Áç≤Âæó‰∫ÜÊõ¥Â•ΩÁöÑÊïàËÉΩÔºåÂàÜÂà•Âú® QALD-9 Âíå LC-QuAD-1.0 Ë≥áÊñôÈõÜ‰∏äÁç≤Âæó 76.52% Âíå 83.37% ÁöÑ BLEU-1ÔºàÈõôË™ûË©ï‰º∞Á†îÁ©∂ÔºâÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÂú® QALD-9 Âíå LC-QuAD-1.0 Ë≥áÊñôÈõÜÁöÑÁ´ØÂà∞Á´ØÁ≥ªÁµ±ÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëÂú®ÂÖ∂‰ªñ KGQA Á≥ªÁµ±‰∏≠ÂèñÂæó‰∫ÜÈ†òÂÖàÁöÑÁµêÊûúÔºåÂ∑®ËßÄ F1 Ê∏¨ÈáèÂÄºÂàÜÂà•ÈÅîÂà∞ 52% Âíå 66%„ÄÇÊ≠§Â§ñÔºåÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂ¶ÇÊûúÊìÅÊúâÂá∫Ëâ≤ÁöÑÁ∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Êû∂ÊßãÂíåË∑®Ê≥®ÊÑèÂäõÊ©üÂà∂ÔºåÂç≥‰ΩøÂú®ÈÅãÁÆóË≥áÊ∫êÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞àÂÆ∂ÂíåÂ≠∏ËÄÖ‰ªçÂèØ‰ª•‰ΩøÁî®‰∏ÄËà¨ÁöÑÂµåÂÖ•‰æÜÁç≤ÂæóÁ≠âÂêåÊñºÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°ÂûãÁöÑÂÇëÂá∫ÊïàËÉΩ„ÄÇ

##### **CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers**
2408.13366v1 by Ekaterina Trofimova, Emil Sataev, Abhijit Singh Jowhari

This paper presents CodeRefine, a novel framework for automatically
transforming research paper methodologies into functional code using Large
Language Models (LLMs). Our multi-step approach first extracts and summarizes
key text chunks from papers, analyzes their code relevance, and creates a
knowledge graph using a predefined ontology. Code is then generated from this
structured representation and enhanced through a proposed retrospective
retrieval-augmented generation approach. CodeRefine addresses the challenge of
bridging theoretical research and practical implementation, offering a more
accurate alternative to LLM zero-shot prompting. Evaluations on diverse
scientific papers demonstrate CodeRefine's ability to improve code
implementation from the paper, potentially accelerating the adoption of
cutting-edge algorithms in real-world applications.

ÊëòË¶ÅÔºöÊú¨ÁØáË´ñÊñáÊèêÂá∫ CodeRefineÔºå‰∏ÄÂÄãÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞áÁ†îÁ©∂Ë´ñÊñáÊñπÊ≥ïËá™ÂãïËΩâÊèõÁÇ∫ÂäüËÉΩÁ®ãÂºèÁ¢ºÁöÑÊñ∞Á©éÊû∂Êßã„ÄÇÊàëÂÄëÁöÑÂ§öÊ≠•È©üÊñπÊ≥ïÈ¶ñÂÖàÂæûË´ñÊñá‰∏≠ËêÉÂèñ‰∏¶ÊëòË¶ÅÂá∫ÈóúÈçµÊñáÂ≠óÂçÄÂ°äÔºåÂàÜÊûêÂÖ∂Á®ãÂºèÁ¢ºÁõ∏ÈóúÊÄßÔºå‰∏¶‰ΩøÁî®È†êÂÆöÁæ©ÁöÑÊú¨‰ΩìÂª∫Á´ãÁü•Ë≠òÂúñË≠ú„ÄÇÊé•ËëóÂæûÈÄôÂÄãÁµêÊßãÂåñË°®Á§∫Áî¢ÁîüÁ®ãÂºèÁ¢ºÔºå‰∏¶ÈÄèÈÅéÊèêÂá∫ÁöÑÂõûÊ∫ØÂºèÊ™¢Á¥¢Â¢ûÂº∑Áî¢ÁîüÊñπÊ≥ïÈÄ≤Ë°åÂº∑Âåñ„ÄÇCodeRefine Ëß£Ê±∫‰∫ÜÁêÜË´ñÁ†îÁ©∂ËàáÂØ¶ÈöõÂØ¶‰Ωú‰πãÈñìÁöÑÈ¥ªÊ∫ùÔºåÊèê‰æõÊØî LLM Èõ∂Ê¨°ÊèêÁ§∫Êõ¥Á≤æÁ¢∫ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÂú®ÂêÑÁ®ÆÁßëÂ≠∏Ë´ñÊñá‰∏äÁöÑË©ï‰º∞Ë≠âÊòé‰∫Ü CodeRefine ÂæûË´ñÊñáÊîπÂñÑÁ®ãÂºèÁ¢ºÂØ¶‰ΩúÁöÑËÉΩÂäõÔºåÈÄôÊúâÊΩõÂäõÂä†ÈÄüÂ∞ñÁ´ØÊºîÁÆóÊ≥ïÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÊé°Áî®„ÄÇ

##### **Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving**
2408.14494v1 by Sakhinana Sagar Srinivas, Vijay Sri Vaikunth, Venkataramana Runkana

We present the Process Engineering Operations Assistant (PEOA), an AI-driven
framework designed to solve complex problems in the chemical and process
industries. The framework employs a modular architecture orchestrated by a
meta-agent, which serves as the central coordinator, managing an action
generator and instruction-tuned small-scale language models (expert models).
The action generator decomposes complex problems into sub-tasks and identifies
suitable expert models to execute each, delivering precise solutions for
multi-step problem-solving. Key techniques include advanced knowledge modeling
using property graphs for improved information retrieval, facilitating more
accurate and contextually relevant solutions. Additionally, the framework
utilizes a teacher-student transfer-learning approach with GPT-4 (Omni) to
fine-tune the action generator and expert models for domain adaptation,
alongside an iterative problem-solving mechanism with sophisticated error
handling. Custom datasets were developed to evaluate the framework against
leading proprietary language models on various engineering tasks. The results
demonstrate the framework effectiveness in automating calculations,
accelerating prototyping, and providing AI-augmented decision support for
industrial processes, marking a significant advancement in process engineering
capabilities.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫ÜË£ΩÁ®ãÂ∑•Á®ã‰ΩúÊ•≠Âä©ÁêÜ (PEOA)ÔºåÈÄôÊòØ‰∏ÄÂÄãÁî± AI È©ÖÂãïÁöÑÊû∂ÊßãÔºåÊó®Âú®Ëß£Ê±∫ÂåñÂ≠∏ÂíåË£ΩÁ®ãÁî¢Ê•≠‰∏≠ÁöÑË§áÈõúÂïèÈ°å„ÄÇË©≤Êû∂ÊßãÊé°Áî®Ê®°ÁµÑÂåñÊû∂ÊßãÔºåÁî±‰∏ÄÂÄãÂÖÉ‰ª£ÁêÜÁ®ãÂºèÂçîË™øÔºåË©≤‰ª£ÁêÜÁ®ãÂºè‰ΩúÁÇ∫‰∏≠Â§ÆÂçîË™øÂô®ÔºåÁÆ°ÁêÜÂãï‰ΩúÁî¢ÁîüÂô®ÂíåÊåá‰ª§Ë™øÊï¥ÁöÑÂ∞èË¶èÊ®°Ë™ûË®ÄÊ®°Âûã (Â∞àÂÆ∂Ê®°Âûã)„ÄÇÂãï‰ΩúÁî¢ÁîüÂô®Â∞áË§áÈõúÁöÑÂïèÈ°åÂàÜËß£ÁÇ∫Â≠ê‰ªªÂãôÔºå‰∏¶Ë≠òÂà•ÂêàÈÅ©ÁöÑÂ∞àÂÆ∂Ê®°Âûã‰æÜÂü∑Ë°åÊØèÂÄã‰ªªÂãôÔºåÁÇ∫Â§öÊ≠•È©üÂïèÈ°åËß£Ê±∫Êèê‰æõÁ≤æÁ¢∫ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÈóúÈçµÊäÄË°ìÂåÖÊã¨‰ΩøÁî®Â±¨ÊÄßÂúñÈÄ≤Ë°åÈÄ≤ÈöéÁü•Ë≠òÂª∫Ê®°Ôºå‰ª•ÊîπÂñÑË≥áË®äÊ™¢Á¥¢ÔºåÊèê‰æõÊõ¥Ê∫ñÁ¢∫‰∏îËàáËÑàÁµ°Áõ∏ÈóúÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÊ≠§Â§ñÔºåË©≤Êû∂ÊßãÊé°Áî®ÊïôÂ∏´-Â≠∏ÁîüÂÇ≥Ëº∏Â≠∏ÁøíÊñπÊ≥ïÔºå‰ΩøÁî® GPT-4 (Omni) ‰æÜÂæÆË™øÂãï‰ΩúÁî¢ÁîüÂô®ÂíåÂ∞àÂÆ∂Ê®°ÂûãÔºå‰ª•ÈÄ≤Ë°åÈ†òÂüüÈÅ©ÊáâÔºå‰ª•ÂèäÂÖ∑ÂÇôÁ≤æÁ∑ªÈåØË™§ËôïÁêÜÂäüËÉΩÁöÑËø≠‰ª£ÂïèÈ°åËß£Ê±∫Ê©üÂà∂„ÄÇÈñãÁôº‰∫ÜËá™Ë®ÇË≥áÊñôÈõÜÔºå‰ª•ÈáùÂ∞çÂêÑÁ®ÆÂ∑•Á®ã‰ªªÂãôË©ï‰º∞Ë©≤Êû∂ÊßãËàáÈ†òÂÖàÁöÑÂ∞àÊúâË™ûË®ÄÊ®°Âûã„ÄÇÁµêÊûúË≠âÊòé‰∫ÜË©≤Êû∂ÊßãÂú®Ëá™ÂãïÂåñË®àÁÆó„ÄÅÂä†ÈÄüÂª∫Ê®°ÂíåÊèê‰æõ AI Â¢ûÂº∑Ê±∫Á≠ñÊîØÊè¥ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÊ®ôË™åËëóË£ΩÁ®ãÂ∑•Á®ãËÉΩÂäõÁöÑÈáçÂ§ßÈÄ≤Â±ï„ÄÇ

##### **A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language**
2408.12578v2 by Ekdeep Singh Lubana, Kyogo Kawaguchi, Robert P. Dick, Hidenori Tanaka

Increase in data, size, or compute can lead to sudden learning of specific
capabilities by a neural network -- a phenomenon often called "emergence''.
Beyond scientific understanding, establishing the causal factors underlying
such emergent capabilities is crucial to enable risk regulation frameworks for
AI. In this work, we seek inspiration from study of emergent properties in
other fields and propose a phenomenological definition for the concept in the
context of neural networks. Our definition implicates the acquisition of
general structures underlying the data-generating process as a cause of sudden
performance growth for specific, narrower tasks. We empirically investigate
this definition by proposing an experimental system grounded in a
context-sensitive formal language and find that Transformers trained to perform
tasks on top of strings from this language indeed exhibit emergent
capabilities. Specifically, we show that once the language's underlying grammar
and context-sensitivity inducing structures are learned by the model,
performance on narrower tasks suddenly begins to improve. We then analogize our
network's learning dynamics with the process of percolation on a bipartite
graph, establishing a formal phase transition model that predicts the shift in
the point of emergence observed in our experiments when changing the data
structure. Overall, our experimental and theoretical frameworks yield a step
towards better defining, characterizing, and predicting emergence in neural
networks.

ÊëòË¶ÅÔºö<paragraph>Ë≥áÊñô„ÄÅË¶èÊ®°ÊàñÈÅãÁÆóÁöÑÂ¢ûÂä†ÔºåÂèØËÉΩÊúÉÂ∞éËá¥Á•ûÁ∂ìÁ∂≤Ë∑ØÁ™ÅÁÑ∂Â≠∏ÊúÉÁâπÂÆöËÉΩÂäõ‚Äî‚ÄîÈÄôÁ®ÆÁèæË±°Â∏∏Á®±ÁÇ∫„ÄåÊπßÁèæ„Äç„ÄÇÈô§‰∫ÜÁßëÂ≠∏ÁêÜËß£‰πãÂ§ñÔºåÁ¢∫Á´ãÈÄôÁ®ÆÊπßÁèæËÉΩÂäõËÉåÂæåÁöÑÂü∫Êú¨ÂéüÂõ†ÔºåÂ∞çÊñºÁÇ∫ AI Âª∫Á´ãÈ¢®Èö™Ê≥ïË¶èÊ°ÜÊû∂Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂæûÂÖ∂‰ªñÈ†òÂüü‰∏≠Â∞çÊπßÁèæÁâπÊÄßÁöÑÁ†îÁ©∂‰∏≠Â∞ãÊ±ÇÈùàÊÑüÔºå‰∏¶ÈáùÂ∞çÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÊ¶ÇÂøµÊèêÂá∫ÁèæË±°Â≠∏ÂÆöÁæ©„ÄÇÊàëÂÄëÁöÑÂÆöÁæ©ÊöóÁ§∫ÔºåÂèñÂæóË≥áÊñôÁî¢ÁîüÁ®ãÂ∫èËÉåÂæåÁöÑÈÄöÁî®ÁµêÊßãÔºåÊòØÁâπÂÆö„ÄÅËºÉÁãπÈöò‰ªªÂãôÁ™ÅÁÑ∂ÊïàËÉΩÊèêÂçáÁöÑÂéüÂõ†„ÄÇÊàëÂÄëÈÄèÈÅéÊèêÂá∫‰∏ÄÂÄã‰ª•ÊÉÖÂ¢ÉÊïèÊÑüÂΩ¢ÂºèË™ûË®ÄÁÇ∫Âü∫Á§éÁöÑÂØ¶È©óÁ≥ªÁµ±ÔºåÂ∞çÈÄôÂÄãÂÆöÁæ©ÈÄ≤Ë°åÂØ¶Ë≠âÁ†îÁ©∂ÔºåÁôºÁèæÁ∂ìÈÅéË®ìÁ∑¥‰ª•Âü∑Ë°åÈÄôÂÄãË™ûË®Ä‰∏≠Â≠ó‰∏≤È†ÇÈÉ®‰ªªÂãôÁöÑ TransformerÔºåÁ¢∫ÂØ¶Â±ïÁèæÂá∫ÊπßÁèæËÉΩÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ±ïÁ§∫Âá∫Ê®°Âûã‰∏ÄÊó¶Â≠∏ÊúÉË™ûË®ÄÁöÑÂ∫ïÂ±§ÊñáÊ≥ïÂíåÊÉÖÂ¢ÉÊïèÊÑüË™òÂ∞éÁµêÊßãÔºåÂ∞çËºÉÁãπÈöò‰ªªÂãôÁöÑÊïàËÉΩÂ∞±ÊúÉÁ™ÅÁÑ∂ÈñãÂßãÊèêÂçá„ÄÇÊé•ËëóÊàëÂÄëÂ∞áÁ∂≤Ë∑ØÁöÑÂ≠∏ÁøíÂãïÊÖãÈ°ûÊØîÁÇ∫‰∫åÈÉ®Âúñ‰∏äÁöÑÊª≤ÊµÅÈÅéÁ®ãÔºåÂª∫Á´ã‰∏ÄÂÄãÊ≠£ÂºèÁöÑÁõ∏ËÆäÊ®°ÂûãÔºåÁî®ÊñºÈ†êÊ∏¨Âú®ÊîπËÆäË≥áÊñôÁµêÊßãÊôÇÔºåÊàëÂÄëÂú®ÂØ¶È©ó‰∏≠ËßÄÂØüÂà∞ÁöÑÊπßÁèæÈªû‰ΩçÁßª„ÄÇÊï¥È´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÂØ¶È©óÂíåÁêÜË´ñÊ°ÜÊû∂ÊúùËëóÊõ¥ÂÆåÂñÑÂú∞ÂÆöÁæ©„ÄÅÊèèËø∞ÂíåÈ†êÊ∏¨Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÊπßÁèæÈÇÅÈÄ≤‰∫Ü‰∏ÄÊ≠•„ÄÇ</paragraph>

##### **Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language**
2409.00061v1 by Arief Purnama Muharram, Ayu Purwarianti

Automated fact-checking is a key strategy to overcome the spread of COVID-19
misinformation on the internet. These systems typically leverage deep learning
approaches through Natural Language Inference (NLI) to verify the truthfulness
of information based on supporting evidence. However, one challenge that arises
in deep learning is performance stagnation due to a lack of knowledge during
training. This study proposes using a Knowledge Graph (KG) as external
knowledge to enhance NLI performance for automated COVID-19 fact-checking in
the Indonesian language. The proposed model architecture comprises three
modules: a fact module, an NLI module, and a classifier module. The fact module
processes information from the KG, while the NLI module handles semantic
relationships between the given premise and hypothesis. The representation
vectors from both modules are concatenated and fed into the classifier module
to produce the final result. The model was trained using the generated
Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia.
Our study demonstrates that incorporating KGs can significantly improve NLI
performance in fact-checking, achieving the best accuracy of 0,8616. This
suggests that KGs are a valuable component for enhancing NLI performance in
automated fact-checking.

ÊëòË¶ÅÔºöËá™Âãï‰∫ãÂØ¶Êü•Ê†∏ÊòØÂÖãÊúçÁ∂≤Ë∑Ø‰∏ä COVID-19 ÈåØË™§Ë≥áË®äÊï£Êí≠ÁöÑ‰∏ÄÈ†ÖÈóúÈçµÁ≠ñÁï•„ÄÇÈÄô‰∫õÁ≥ªÁµ±ÈÄöÂ∏∏ÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ (NLI) ‰æÜÂà©Áî®Ê∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºåÊ†πÊìöÊîØÊè¥Ë≠âÊìöÈ©óË≠âË≥áË®äÁöÑÁúüÂØ¶ÊÄß„ÄÇÁÑ∂ËÄåÔºåÂú®Ê∑±Â∫¶Â≠∏Áøí‰∏≠ÊúÉÂá∫Áèæ‰∏ÄÂÄãÊåëÊà∞ÔºåÈÇ£Â∞±ÊòØÂú®Ë®ìÁ∑¥ÊúüÈñìÂõ†Áº∫‰πèÁü•Ë≠òËÄåÂ∞éËá¥ÊïàËÉΩÂÅúÊªØ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÊèêÂá∫‰ΩøÁî®Áü•Ë≠òÂúñË≠ú (KG) ‰ΩúÁÇ∫Â§ñÈÉ®Áü•Ë≠òÔºå‰ª•Â¢ûÂº∑Ëá™ÂãïÂåñ COVID-19 ‰∫ãÂØ¶Êü•Ê†∏ÁöÑ NLI ÊïàËÉΩÔºå‰∏¶‰ª•Âç∞Â∞ºË™ûÈÄ≤Ë°å„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÊû∂ÊßãÂåÖÂê´‰∏âÂÄãÊ®°ÁµÑÔºö‰∫ãÂØ¶Ê®°ÁµÑ„ÄÅNLI Ê®°ÁµÑÂíåÂàÜÈ°ûÂô®Ê®°ÁµÑ„ÄÇ‰∫ãÂØ¶Ê®°ÁµÑËôïÁêÜ‰æÜËá™ KG ÁöÑË≥áË®äÔºåËÄå NLI Ê®°ÁµÑÂâáËôïÁêÜÁµ¶ÂÆöÂâçÊèêÂíåÂÅáË®≠‰πãÈñìÁöÑË™ûÁæ©Èóú‰øÇ„ÄÇ‰æÜËá™ÂÖ©ÂÄãÊ®°ÁµÑÁöÑË°®Á§∫ÂêëÈáèÊúÉ‰∏≤Êé•Ëµ∑‰æÜÔºå‰∏¶Ëº∏ÂÖ•ÂàÜÈ°ûÂô®Ê®°ÁµÑ‰ª•Áî¢ÁîüÊúÄÁµÇÁµêÊûú„ÄÇÊ≠§Ê®°Âûã‰ΩøÁî®Áî¢ÁîüÁöÑÂç∞Â∞ºË™û COVID-19 ‰∫ãÂØ¶Êü•Ê†∏Ë≥áÊñôÈõÜÂíå COVID-19 KG Bahasa Indonesia ÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ë≠âÊòéÔºåÁ¥çÂÖ• KG ÂèØ‰ª•È°ØËëóÊîπÂñÑ‰∫ãÂØ¶Êü•Ê†∏‰∏≠ÁöÑ NLI ÊïàËÉΩÔºåÈÅîÂà∞ 0.8616 ÁöÑÊúÄ‰Ω≥Ê∫ñÁ¢∫Â∫¶„ÄÇÈÄôË°®Á§∫ KG ÊòØÂ¢ûÂº∑Ëá™ÂãïÂåñ‰∫ãÂØ¶Êü•Ê†∏‰∏≠ NLI ÊïàËÉΩÁöÑÂØ∂Ë≤¥ÁµÑÊàêÈÉ®ÂàÜ„ÄÇ

##### **Cell-ontology guided transcriptome foundation model**
2408.12373v1 by Xinyu Yuan, Zhihao Zhan, Zuobai Zhang, Manqi Zhou, Jianan Zhao, Boyu Han, Yue Li, Jian Tang

Transcriptome foundation models TFMs hold great promises of deciphering the
transcriptomic language that dictate diverse cell functions by self-supervised
learning on large-scale single-cell gene expression data, and ultimately
unraveling the complex mechanisms of human diseases. However, current TFMs
treat cells as independent samples and ignore the taxonomic relationships
between cell types, which are available in cell ontology graphs. We argue that
effectively leveraging this ontology information during the TFM pre-training
can improve learning biologically meaningful gene co-expression patterns while
preserving TFM as a general purpose foundation model for downstream zero-shot
and fine-tuning tasks. To this end, we present \textbf{s}ingle \textbf{c}ell,
\textbf{Cell}-\textbf{o}ntology guided TFM scCello. We introduce cell-type
coherence loss and ontology alignment loss, which are minimized along with the
masked gene expression prediction loss during the pre-training. The novel loss
component guide scCello to learn the cell-type-specific representation and the
structural relation between cell types from the cell ontology graph,
respectively. We pre-trained scCello on 22 million cells from CellxGene
database leveraging their cell-type labels mapped to the cell ontology graph
from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates
competitive generalization and transferability performance over the existing
TFMs on biologically important tasks including identifying novel cell types of
unseen cells, prediction of cell-type-specific marker genes, and cancer drug
responses.

ÊëòË¶ÅÔºö<paragraph>ËΩâÈåÑÁµÑÂü∫Á§éÊ®°Âûã TFM ÊâøË´æËß£Á¢ºËΩâÈåÑÁµÑË™ûË®ÄÔºåÂÆÉÈÄèÈÅéÂú®Â§ßÂûãÂñÆÁ¥∞ËÉûÂü∫Âõ†Ë°®ÁèæË≥áÊñô‰∏äÈÄ≤Ë°åËá™ÊàëÁõ£Áù£Â≠∏ÁøíÔºå‰æÜÊ±∫ÂÆö‰∏çÂêåÁöÑÁ¥∞ËÉûÂäüËÉΩÔºå‰∏¶ÊúÄÁµÇËß£Èñã‰∫∫È°ûÁñæÁóÖÁöÑË§áÈõúÊ©üÂà∂„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑ TFM Â∞áÁ¥∞ËÉûË¶ñÁÇ∫Áç®Á´ãÊ®£Êú¨Ôºå‰∏¶ÂøΩÁï•Á¥∞ËÉûÈ°ûÂûã‰πãÈñìÁöÑÂàÜÈ°ûÈóú‰øÇÔºåËÄåÈÄôÂú®Á¥∞ËÉûÊú¨È´îË´ñÂúñË°®‰∏≠ÊòØÂèØÁî®ÁöÑ„ÄÇÊàëÂÄëË™çÁÇ∫Âú® TFM È†êË®ìÁ∑¥ÊúüÈñìÊúâÊïàÂà©Áî®Ê≠§Êú¨È´îË´ñË≥áË®äÔºåÂèØ‰ª•ÊîπÂñÑÂ≠∏ÁøíÁîüÁâ©Â≠∏‰∏äÊúâÊÑèÁæ©ÁöÑÂü∫Âõ†ÂÖ±Ë°®ÁèæÊ®°ÂºèÔºåÂêåÊôÇ‰øùÁïô TFM ‰ΩúÁÇ∫‰∏ãÊ∏∏Èõ∂Ê¨°Â≠∏ÁøíÂíåÂæÆË™ø‰ªªÂãôÁöÑ‰∏ÄËà¨Áî®ÈÄîÂü∫Á§éÊ®°Âûã„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ÂñÆÁ¥∞ËÉû„ÄÅÁ¥∞ËÉûÊú¨È´îË´ñÂºïÂ∞éÁöÑ TFM scCello„ÄÇÊàëÂÄëÂºïÂÖ•Á¥∞ËÉûÈ°ûÂûã‰∏ÄËá¥ÊÄßÊêçÂ§±ÂíåÊú¨È´îË´ñÂ∞çÈΩäÊêçÂ§±ÔºåÂú®È†êË®ìÁ∑¥ÊúüÈñìÊúÉÂ∞áÂÖ∂ËàáÈÅÆÁΩ©Âü∫Âõ†Ë°®ÁèæÈ†êÊ∏¨ÊêçÂ§±‰∏ÄËµ∑ÊúÄÂ∞èÂåñ„ÄÇÈÄôÂÄãÊñ∞Á©éÁöÑÊêçÂ§±ÁµÑ‰ª∂ÂºïÂ∞é scCello ÂàÜÂà•ÂæûÁ¥∞ËÉûÊú¨È´îË´ñÂúñË°®‰∏≠Â≠∏ÁøíÁ¥∞ËÉûÈ°ûÂûãÁâπÂÆöË°®Á§∫ÂíåÁ¥∞ËÉûÈ°ûÂûã‰πãÈñìÁöÑÁµêÊßãÈóú‰øÇ„ÄÇÊàëÂÄëÂú® CellxGene Ë≥áÊñôÂ∫´‰∏≠Â∞ç 2200 Ëê¨ÂÄãÁ¥∞ËÉûÈÄ≤Ë°å scCello È†êË®ìÁ∑¥ÔºåÂà©Áî®ÂÖ∂Á¥∞ËÉûÈ°ûÂûãÊ®ôÁ±§Â∞çÊáâÂà∞ÈñãÊîæÁîüÁâ©ÂíåÁîüÁâ©ÈÜ´Â≠∏Êú¨È´îÈëÑÈÄ†Âª†ÁöÑÁ¥∞ËÉûÊú¨È´îË´ñÂúñË°®„ÄÇÊàëÂÄëÁöÑ TFM Âú®ÁîüÁâ©Â≠∏‰∏äÈáçË¶ÅÁöÑ‰ªªÂãô‰∏äÂ±ïÁ§∫‰∫ÜÊØîÁèæÊúâ TFM Êõ¥ÂÖ∑Á´∂Áà≠ÂäõÁöÑÊ≥õÂåñÂíåÂèØËΩâÁßªÊÄßÔºåÂåÖÊã¨Ë≠òÂà•Êú™Ë¶ãÁ¥∞ËÉûÁöÑÊñ∞Á¥∞ËÉûÈ°ûÂûã„ÄÅÈ†êÊ∏¨Á¥∞ËÉûÈ°ûÂûãÁâπÂÆöÊ®ôË®òÂü∫Âõ†ÂíåÁôåÁóáËó•Áâ©ÂèçÊáâ„ÄÇ</paragraph>

##### **Graph Retrieval Augmented Trustworthiness Reasoning**
2408.12333v2 by Ying Zhu, Shengchang Li, Ziqian Kong, Peilan Xu

Trustworthiness reasoning is crucial in multiplayer games with incomplete
information, enabling agents to identify potential allies and adversaries,
thereby enhancing reasoning and decision-making processes. Traditional
approaches relying on pre-trained models necessitate extensive domain-specific
data and considerable reward feedback, with their lack of real-time
adaptability hindering their effectiveness in dynamic environments. In this
paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework,
leveraging the Retrieval-Augmented Generation (RAG) technique to bolster
trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness
graph, updating it in real-time with evidential information, and retrieves
relevant trust data to augment the reasoning capabilities of Large Language
Models (LLMs). We validate our approach through experiments on the multiplayer
game "Werewolf," comparing GRATR against baseline LLM and LLM enhanced with
Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the
baseline methods by over 30\% in winning rate, with superior reasoning
performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as
identity and objective amnesia, and crucially, it renders the reasoning process
more transparent and traceable through the use of the trustworthiness graph.

ÊëòË¶ÅÔºö<paragraph>Âú®‰ø°ÊÅØ‰∏çÂÆåÊï¥ÁöÑÂ§ö‰∫∫ÈÅäÊà≤‰∏≠ÔºåÂèØ‰ø°Â∫¶Êé®ÁêÜËá≥ÈóúÈáçË¶ÅÔºåËÆì‰ª£ÁêÜ‰∫∫ËÉΩÂ§†Ë≠òÂà•ÊΩõÂú®ÁöÑÁõüÂèãÂíåÊïµ‰∫∫ÔºåÂæûËÄåÂ¢ûÂº∑Êé®ÁêÜÂíåÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ã„ÄÇ‰æùË≥¥È†êÂÖàË®ìÁ∑¥Ê®°ÂûãÁöÑÂÇ≥Áµ±ÊñπÊ≥ïÈúÄË¶ÅÂ§ßÈáèÁöÑÁâπÂÆöÈ†òÂüüÊï∏ÊìöÂíåÂ§ßÈáèÁöÑÁçéÂãµÂõûÈ•ãÔºåËÄåÂÆÉÂÄëÁº∫‰πèÂØ¶ÊôÇÈÅ©ÊáâÊÄßÊúÉÈòªÁ§ôÂÆÉÂÄëÂú®ÂãïÊÖãÁí∞Â¢É‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂúñÂΩ¢Ê™¢Á¥¢Â¢ûÂº∑Êé®ÁêÜ (GRATR) Ê°ÜÊû∂ÔºåÂà©Áî®Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊäÄË°ì‰æÜÂä†Âº∑‰ª£ÁêÜ‰∫∫ÁöÑÂèØ‰ø°Â∫¶Êé®ÁêÜ„ÄÇGRATR ÊßãÂª∫‰∫Ü‰∏ÄÂÄãÂãïÊÖãÂèØ‰ø°Â∫¶ÂúñÂΩ¢Ôºå‰∏¶‰ΩøÁî®Ë≠âÊìö‰ø°ÊÅØÂØ¶ÊôÇÊõ¥Êñ∞ÂÆÉÔºå‰∏¶Ê™¢Á¥¢Áõ∏ÈóúÁöÑ‰ø°‰ªªÊï∏Êìö‰ª•Â¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàëÂÄëÈÄöÈÅéÂ§ö‰∫∫ÈÅäÊà≤„ÄåÁãº‰∫∫„ÄçÁöÑÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÂ∞á GRATR ËàáÂü∫Ê∫ñ LLM Âíå‰ΩøÁî® Native RAG Âíå Rerank RAG Â¢ûÂº∑ÁöÑ LLM ÈÄ≤Ë°å‰∫ÜÊØîËºÉ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåGRATR Âú®Áç≤ÂãùÁéá‰∏äÊØîÂü∫Ê∫ñÊñπÊ≥ïÈ´òÂá∫ 30%ÔºåÂÖ∑ÊúâÂçìË∂äÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåGRATR ÊúâÊïàÂú∞Ê∏õËºï‰∫Ü LLM ÁöÑÂπªË¶∫Ôºå‰æãÂ¶ÇË∫´‰ªΩÂíåÁõÆÊ®ôÂÅ•ÂøòÁóáÔºåÊúÄÈáçË¶ÅÁöÑÊòØÔºåÂÆÉÈÄöÈÅé‰ΩøÁî®ÂèØ‰ø°Â∫¶ÂúñÂΩ¢‰ΩøÊé®ÁêÜÈÅéÁ®ãÊõ¥ÈÄèÊòé‰∏îÂèØËøΩËπ§„ÄÇ</paragraph>

##### **MedDiT: A Knowledge-Controlled Diffusion Transformer Framework for Dynamic Medical Image Generation in Virtual Simulated Patient**
2408.12236v1 by Yanzeng Li, Cheng Zeng, Jinchao Zhang, Jie Zhou, Lei Zou

Medical education relies heavily on Simulated Patients (SPs) to provide a
safe environment for students to practice clinical skills, including medical
image analysis. However, the high cost of recruiting qualified SPs and the lack
of diverse medical imaging datasets have presented significant challenges. To
address these issues, this paper introduces MedDiT, a novel
knowledge-controlled conversational framework that can dynamically generate
plausible medical images aligned with simulated patient symptoms, enabling
diverse diagnostic skill training. Specifically, MedDiT integrates various
patient Knowledge Graphs (KGs), which describe the attributes and symptoms of
patients, to dynamically prompt Large Language Models' (LLMs) behavior and
control the patient characteristics, mitigating hallucination during medical
conversation. Additionally, a well-tuned Diffusion Transformer (DiT) model is
incorporated to generate medical images according to the specified patient
attributes in the KG. In this paper, we present the capabilities of MedDiT
through a practical demonstration, showcasing its ability to act in diverse
simulated patient cases and generate the corresponding medical images. This can
provide an abundant and interactive learning experience for students, advancing
medical education by offering an immersive simulation platform for future
healthcare professionals. The work sheds light on the feasibility of
incorporating advanced technologies like LLM, KG, and DiT in education
applications, highlighting their potential to address the challenges faced in
simulated patient-based medical education.

ÊëòË¶ÅÔºöÈÜ´Â≠∏ÊïôËÇ≤È´òÂ∫¶‰æùË≥¥Ê®°Êì¨ÁóÖ‰∫∫ (SP) Êèê‰æõ‰∏ÄÂÄãÂÆâÂÖ®ÁöÑÁí∞Â¢ÉÔºåËÆìÂ≠∏ÁîüÁ∑¥ÁøíËá®Â∫äÊäÄËÉΩÔºåÂåÖÊã¨ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê„ÄÇÁÑ∂ËÄåÔºåÊãõÂãüÂêàÊ†º SP ÁöÑÈ´òÊàêÊú¨ÂíåÁº∫‰πèÂ§öÊ®£ÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÂ∑≤ÈÄ†ÊàêÈ°ØËëóÁöÑÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊú¨Êñá‰ªãÁ¥π MedDiTÔºå‰∏ÄÂÄãÊñ∞Á©éÁöÑÁü•Ë≠òÊéßÂà∂Â∞çË©±Êû∂ÊßãÔºåÂÆÉÂèØ‰ª•ÂãïÊÖãÁî¢ÁîüÁ¨¶ÂêàÊ®°Êì¨ÁóÖ‰∫∫ÁóáÁãÄÁöÑÂêàÁêÜÈÜ´Â≠∏ÂΩ±ÂÉèÔºåÂØ¶ÁèæÂ§öÊ®£ÁöÑË®∫Êñ∑ÊäÄËÉΩË®ìÁ∑¥„ÄÇÂÖ∑È´î‰æÜË™™ÔºåMedDiT Êï¥Âêà‰∫ÜÂêÑÁ®ÆÁóÖ‰∫∫Áü•Ë≠òÂúñË≠ú (KG)ÔºåÊèèËø∞ÁóÖ‰∫∫ÁöÑÂ±¨ÊÄßÂíåÁóáÁãÄÔºå‰ª•ÂãïÊÖãÊèêÁ§∫Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑË°åÁÇ∫Ôºå‰∏¶ÊéßÂà∂ÁóÖ‰∫∫ÁâπÂæµÔºåÊ∏õËºïÈÜ´Â≠∏Â∞çË©±‰∏≠ÁöÑÂπªË¶∫„ÄÇÊ≠§Â§ñÔºåÈÇÑÁ¥çÂÖ•‰∏ÄÂÄãÁ∂ìÈÅéÂæÆË™øÁöÑÊì¥Êï£Transformer (DiT) Ê®°ÂûãÔºåÊ†πÊìö KG ‰∏≠ÊåáÂÆöÁöÑÁóÖ‰∫∫Â±¨ÊÄßÁî¢ÁîüÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂØ¶ÈöõÁ§∫ÁØÑÂ±ïÁ§∫ MedDiT ÁöÑÂäüËÉΩÔºåÂ±ïÁ§∫ÂÆÉÂú®‰∏çÂêåÊ®°Êì¨ÁóÖ‰∫∫Ê°à‰æã‰∏≠‰ΩúÁî®‰∏¶Áî¢ÁîüÁõ∏ÊáâÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑËÉΩÂäõ„ÄÇÈÄôÂèØ‰ª•ÁÇ∫Â≠∏ÁîüÊèê‰æõË±êÂØå‰∏î‰∫íÂãïÁöÑÂ≠∏ÁøíÈ´îÈ©óÔºåÈÄèÈÅéÊèê‰æõË∫´Ê≠∑ÂÖ∂Â¢ÉÁöÑÊ®°Êì¨Âπ≥Âè∞ÔºåÊèêÂçáÈÜ´Â≠∏ÊïôËÇ≤ÔºåÈÄ†Á¶èÊú™‰æÜÁöÑÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈó°Êòé‰∫ÜÂú®ÊïôËÇ≤ÊáâÁî®‰∏≠Êï¥Âêà LLM„ÄÅKG Âíå DiT Á≠âÂÖàÈÄ≤ÊäÄË°ìÁöÑÂèØË°åÊÄßÔºåÁ™ÅÈ°ØÂÆÉÂÄëÂú®Ëß£Ê±∫Ê®°Êì¨ÁóÖ‰∫∫ÁÇ∫Âü∫Á§éÁöÑÈÜ´Â≠∏ÊïôËÇ≤ÊâÄÈù¢Ëá®ÊåëÊà∞ÁöÑÊΩõÂäõ„ÄÇ

##### **Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning**
2408.12116v1 by Junlin He, Tong Nie, Wei Ma

In the geospatial domain, universal representation models are significantly
less prevalent than their extensive use in natural language processing and
computer vision. This discrepancy arises primarily from the high costs
associated with the input of existing representation models, which often
require street views and mobility data. To address this, we develop a novel,
training-free method that leverages large language models (LLMs) and auxiliary
map data from OpenStreetMap to derive geolocation representations (LLMGeovec).
LLMGeovec can represent the geographic semantics of city, country, and global
scales, which acts as a generic enhancer for spatio-temporal learning.
Specifically, by direct feature concatenation, we introduce a simple yet
effective paradigm for enhancing multiple spatio-temporal tasks including
geographic prediction (GP), long-term time series forecasting (LTSF), and
graph-based spatio-temporal forecasting (GSTF). LLMGeovec can seamlessly
integrate into a wide spectrum of spatio-temporal learning models, providing
immediate enhancements. Experimental results demonstrate that LLMGeovec
achieves global coverage and significantly boosts the performance of leading
GP, LTSF, and GSTF models.

ÊëòË¶ÅÔºöÂú®Á©∫ÈñìÂú∞ÁêÜÈ†òÂüüÔºåÈÄöÁî®Ë°®Á§∫Ê®°ÂûãÈ°ØËëóÂ∞ëÊñºÂÆÉÂÄëÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÈõªËÖ¶Ë¶ñË¶∫‰∏≠ÁöÑÂª£Ê≥õ‰ΩøÁî®„ÄÇÈÄôÁ®ÆÂ∑ÆÁï∞‰∏ªË¶ÅÊ∫êÊñºÁèæÊúâË°®Á§∫Ê®°ÂûãÁöÑËº∏ÂÖ•ÊàêÊú¨È´òÔºåÈÄôÈÄöÂ∏∏ÈúÄË¶ÅË°óÊôØÂíåÊµÅÂãïÊÄßË≥áÊñô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÈñãÁôº‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂÖçË®ìÁ∑¥ÊñπÊ≥ïÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âíå OpenStreetMap ÁöÑËºîÂä©Âú∞ÂúñË≥áÊñô‰æÜÊé®Â∞éÂú∞ÁêÜ‰ΩçÁΩÆË°®Á§∫ (LLMGeovec)„ÄÇLLMGeovec ÂèØ‰ª•Ë°®Á§∫ÂüéÂ∏Ç„ÄÅÂúãÂÆ∂ÂíåÂÖ®ÁêÉË¶èÊ®°ÁöÑÂú∞ÁêÜË™ûÁæ©Ôºå‰ΩúÁÇ∫ÊôÇÁ©∫Â≠∏ÁøíÁöÑÈÄöÁî®Â¢ûÂº∑Âô®„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÈÄöÈÅéÁõ¥Êé•ÁâπÂæµ‰∏≤Êé•ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÁØÑ‰æãÔºåÁî®ÊñºÂ¢ûÂº∑Â§öÂÄãÊôÇÁ©∫‰ªªÂãôÔºåÂåÖÊã¨Âú∞ÁêÜÈ†êÊ∏¨ (GP)„ÄÅÈï∑ÊúüÊôÇÈñìÂ∫èÂàóÈ†êÊ∏¨ (LTSF) ÂíåÂü∫ÊñºÂúñÂΩ¢ÁöÑÊôÇÁ©∫È†êÊ∏¨ (GSTF)„ÄÇLLMGeovec ÂèØ‰ª•ÁÑ°Á∏´Êï¥ÂêàÂà∞Âª£Ê≥õÁöÑÊôÇÁ©∫Â≠∏ÁøíÊ®°Âûã‰∏≠ÔºåÊèê‰æõÁ´ãÂç≥ÁöÑÂ¢ûÂº∑„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåLLMGeovec ÈÅîÂà∞‰∫ÜÂÖ®ÁêÉË¶ÜËìãÁéáÔºå‰∏¶È°ØËëóÊèêÂçá‰∫ÜÈ†òÂÖàÁöÑ GP„ÄÅLTSF Âíå GSTF Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇ

##### **Enabling Small Models for Zero-Shot Classification through Model Label Learning**
2408.11449v1 by Jia Zhang, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li

Vision-language models (VLMs) like CLIP have demonstrated impressive
zero-shot ability in image classification tasks by aligning text and images but
suffer inferior performance compared with task-specific expert models. On the
contrary, expert models excel in their specialized domains but lack zero-shot
ability for new tasks. How to obtain both the high performance of expert models
and zero-shot ability is an important research direction. In this paper, we
attempt to demonstrate that by constructing a model hub and aligning models
with their functionalities using model labels, new tasks can be solved in a
zero-shot manner by effectively selecting and reusing models in the hub. We
introduce a novel paradigm, Model Label Learning (MLL), which bridges the gap
between models and their functionalities through a Semantic Directed Acyclic
Graph (SDAG) and leverages an algorithm, Classification Head Combination
Optimization (CHCO), to select capable models for new tasks. Compared with the
foundation model paradigm, it is less costly and more scalable, i.e., the
zero-shot ability grows with the sizes of the model hub. Experiments on seven
real-world datasets validate the effectiveness and efficiency of MLL,
demonstrating that expert models can be effectively reused for zero-shot tasks.
Our code will be released publicly.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÔºàVLMÔºâÔºå‰æãÂ¶Ç CLIPÔºåÂ∑≤Âú®ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãô‰∏≠Â±ïÁèæ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÊñπÊ≥ïÊòØÂ∞çÈΩäÊñáÂ≠óÂíåÂΩ±ÂÉèÔºå‰ΩÜËàáÁâπÂÆö‰ªªÂãôÁöÑÂ∞àÂÆ∂Ê®°ÂûãÁõ∏ÊØîÔºåÂÖ∂ÊïàËÉΩËºÉÂ∑Æ„ÄÇÁõ∏ÂèçÂú∞ÔºåÂ∞àÂÆ∂Ê®°ÂûãÂú®ÂÖ∂Â∞àÊ•≠È†òÂüü‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂ∞çÊñºÊñ∞‰ªªÂãôÁº∫‰πèÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõ„ÄÇÂ¶Ç‰ΩïÂêåÊôÇÁç≤ÂæóÂ∞àÂÆ∂Ê®°ÂûãÁöÑÈ´òÊïàËÉΩÂíåÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂòóË©¶ÈÄèÈÅéÂª∫Á´ãÊ®°Âûã‰∏≠ÂøÉÔºå‰∏¶‰ΩøÁî®Ê®°ÂûãÊ®ôÁ±§Â∞áÊ®°ÂûãËàáÂÖ∂ÂäüËÉΩÂ∞çÈΩäÔºåË≠âÊòéÂèØ‰ª•ÈÄèÈÅéÊúâÊïàÈÅ∏ÊìáÂíåÈáçË§á‰ΩøÁî®‰∏≠ÂøÉ‰∏≠ÁöÑÊ®°ÂûãÔºå‰ª•Èõ∂Ê¨°Â≠∏ÁøíÁöÑÊñπÂºèËß£Ê±∫Êñ∞‰ªªÂãô„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÁØÑ‰æãÔºåÂç≥Ê®°ÂûãÊ®ôÁ±§Â≠∏ÁøíÔºàMLLÔºâÔºåÂÆÉÈÄèÈÅéË™ûÁæ©Â∞éÂêëÈùûÂæ™Áí∞ÂúñÔºàSDAGÔºâÂΩåÂêàÊ®°ÂûãÂèäÂÖ∂ÂäüËÉΩ‰πãÈñìÁöÑÂ∑ÆË∑ùÔºå‰∏¶Âà©Áî®‰∏ÄÁ®ÆÊºîÁÆóÊ≥ïÔºåÂç≥ÂàÜÈ°ûÈ†≠ÁµÑÂêàÊúÄ‰Ω≥ÂåñÔºàCHCOÔºâÔºåÁÇ∫Êñ∞‰ªªÂãôÈÅ∏ÊìáÊúâËÉΩÂäõÁöÑÊ®°Âûã„ÄÇËàáÂü∫Á§éÊ®°ÂûãÁØÑ‰æãÁõ∏ÊØîÔºåÂÆÉÁöÑÊàêÊú¨ËºÉ‰Ωé‰∏îÊõ¥ÂÖ∑ÂèØÊì¥ÂÖÖÊÄßÔºå‰πüÂ∞±ÊòØË™™ÔºåÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÊúÉÈö®ËëóÊ®°Âûã‰∏≠ÂøÉË¶èÊ®°ÁöÑÊì¥Â§ßËÄåÂ¢ûÈï∑„ÄÇÂú®‰∏ÉÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÈ©óË≠â‰∫Ü MLL ÁöÑÊúâÊïàÊÄßÂíåÊïàÁéáÔºåË≠âÊòé‰∫ÜÂ∞àÂÆ∂Ê®°ÂûãÂèØ‰ª•ÊúâÊïàÂú∞ÈáçË§áÁî®ÊñºÈõ∂Ê¨°Â≠∏Áøí‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∞áÂÖ¨ÈñãÁôºÂ∏É„ÄÇ

