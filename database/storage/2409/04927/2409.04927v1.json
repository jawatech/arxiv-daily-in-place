{"2409.04927": {"publish_time": "2024-09-07", "title": "Just ASR + LLM? A Study on Speech Large Language Models' Ability to Identify and Understand Speaker in Spoken Dialogue", "paper_summary": "In recent years, we have observed a rapid advancement in speech language\nmodels (SpeechLLMs), catching up with humans' listening and reasoning\nabilities. Remarkably, SpeechLLMs have demonstrated impressive spoken dialogue\nquestion-answering (SQA) performance in benchmarks like Gaokao, the English\nlistening test of the college entrance exam in China, which seemingly requires\nunderstanding both the spoken content and voice characteristics of speakers in\na conversation. However, after carefully examining Gaokao's questions, we find\nthe correct answers to many questions can be inferred from the conversation\ncontext alone without identifying the speaker asked in the question. Our\nevaluation of state-of-the-art models Qwen-Audio and WavLLM in both Gaokao and\nour proposed \"What Do You Like?\" dataset shows a significantly higher accuracy\nin these context-based questions than in identity-critical questions, which can\nonly be answered correctly with correct speaker identification. Our results and\nanalysis suggest that when solving SQA, the current SpeechLLMs exhibit limited\nspeaker awareness from the audio and behave similarly to an LLM reasoning from\nthe conversation transcription without sound. We propose that our definitions\nand automated classification of context-based and identity-critical questions\ncould offer a more accurate evaluation framework of SpeechLLMs in SQA tasks.", "paper_summary_zh": "\u8fd1\u5e74\u6765\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SpeechLLM\uff09\u5feb\u901f\u8fdb\u6b65\uff0c\u8d76\u4e0a\u4e86\u4eba\u7c7b\u7684\u542c\u529b\u548c\u63a8\u7406\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSpeechLLM \u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u53e3\u8bed\u5bf9\u8bdd\u95ee\u7b54\uff08SQA\uff09\u8868\u73b0\uff0c\u4f8b\u5982\u9ad8\u8003\uff0c\u8fd9\u662f\u4e2d\u56fd\u5927\u5b66\u5165\u5b66\u8003\u8bd5\u7684\u82f1\u8bed\u542c\u529b\u6d4b\u8bd5\uff0c\u5b83\u4f3c\u4e4e\u9700\u8981\u7406\u89e3\u5bf9\u8bdd\u4e2d\u8bf4\u8bdd\u8005\u7684\u53e3\u8bed\u5185\u5bb9\u548c\u58f0\u97f3\u7279\u5f81\u3002\u7136\u800c\uff0c\u5728\u4ed4\u7ec6\u5ba1\u9605\u9ad8\u8003\u9898\u76ee\u540e\uff0c\u6211\u4eec\u53d1\u73b0\u8bb8\u591a\u9898\u76ee\u7684\u6b63\u786e\u7b54\u6848\u53ef\u4ee5\u4ec5\u4ece\u5bf9\u8bdd\u8bed\u5883\u4e2d\u63a8\u65ad\u51fa\u6765\uff0c\u800c\u65e0\u9700\u8bc6\u522b\u95ee\u9898\u4e2d\u8be2\u95ee\u7684\u8bf4\u8bdd\u8005\u3002\u6211\u4eec\u5bf9\u6700\u5148\u8fdb\u7684\u6a21\u578b Qwen-Audio \u548c WavLLM \u5728\u9ad8\u8003\u548c\u6211\u4eec\u63d0\u51fa\u7684\u201c\u4f60\u559c\u6b22\u4ec0\u4e48\uff1f\u201d\u6570\u636e\u96c6\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8fd9\u4e9b\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u9898\u76ee\u6bd4\u5728\u8eab\u4efd\u5173\u952e\u9898\u76ee\u4e2d\u51c6\u786e\u5ea6\u9ad8\u5f97\u591a\uff0c\u800c\u540e\u8005\u53ea\u6709\u5728\u6b63\u786e\u8bc6\u522b\u8bf4\u8bdd\u8005\u540e\u624d\u80fd\u6b63\u786e\u56de\u7b54\u3002\u6211\u4eec\u7684\u7ed3\u679c\u548c\u5206\u6790\u8868\u660e\uff0c\u5728\u89e3\u51b3 SQA \u65f6\uff0c\u5f53\u524d\u7684 SpeechLLM \u4ece\u97f3\u9891\u4e2d\u8868\u73b0\u51fa\u6709\u9650\u7684\u8bf4\u8bdd\u8005\u610f\u8bc6\uff0c\u5e76\u4e14\u5728\u6ca1\u6709\u58f0\u97f3\u7684\u60c5\u51b5\u4e0b\u4ece\u5bf9\u8bdd\u8f6c\u5f55\u4e2d\u63a8\u7406\u65f6\u8868\u73b0\u5f97\u7c7b\u4f3c\u4e8e LLM\u3002\u6211\u4eec\u5efa\u8bae\uff0c\u6211\u4eec\u5bf9\u57fa\u4e8e\u4e0a\u4e0b\u6587\u548c\u8eab\u4efd\u5173\u952e\u95ee\u9898\u7684\u5b9a\u4e49\u548c\u81ea\u52a8\u5206\u7c7b\u53ef\u4ee5\u4e3a SQA \u4efb\u52a1\u4e2d\u7684 SpeechLLM \u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "author": "Junkai Wu et.al.", "authors": "Junkai Wu, Xulin Fan, Bo-Ru Lu, Xilin Jiang, Nima Mesgarani, Mark Hasegawa-Johnson, Mari Ostendorf", "id": "2409.04927v1", "paper_url": "http://arxiv.org/abs/2409.04927v1", "repo": "https://github.com/wjk0925/slt2024-speechllm-speaker-understanding"}}