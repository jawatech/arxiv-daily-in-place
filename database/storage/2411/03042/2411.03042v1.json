{"2411.03042": {"publish_time": "2024-11-05", "title": "Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning", "paper_summary": "Residual networks, as discrete approximations of Ordinary Differential\nEquations (ODEs), have inspired significant advancements in neural network\ndesign, including multistep methods, high-order methods, and multi-particle\ndynamical systems. The precision of the solution to ODEs significantly affects\nparameter optimization, thereby impacting model performance. In this work, we\npresent a series of advanced explorations of Transformer architecture design to\nminimize the error compared to the true ``solution.'' First, we introduce a\npredictor-corrector learning framework to minimize truncation errors, which\nconsists of a high-order predictor and a multistep corrector. Second, we\npropose an exponential moving average-based coefficient learning method to\nstrengthen our higher-order predictor. Extensive experiments on large-scale\nmachine translation, abstractive summarization, language modeling, and natural\nlanguage understanding benchmarks demonstrate the superiority of our approach.\nOn the WMT'14 English-German and English-French tasks, our model achieved BLEU\nscores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual\nmachine translation task, our model surpasses a robust 3.8B DeepNet by an\naverage of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats\nLLama models by 5.7 accuracy points on the LM Harness Evaluation.", "paper_summary_zh": "\u6b98\u5dee\u7db2\u8def\uff0c\u4f5c\u70ba\u5e38\u5fae\u5206\u65b9\u7a0b\u5f0f (ODE) \u7684\u96e2\u6563\u8fd1\u4f3c\uff0c\u6fc0\u767c\u4e86\u795e\u7d93\u7db2\u8def\u8a2d\u8a08\u7684\u91cd\u5927\u9032\u5c55\uff0c\u5305\u62ec\u591a\u6b65\u6cd5\u3001\u9ad8\u968e\u65b9\u6cd5\u548c\u591a\u7c92\u5b50\u52d5\u529b\u7cfb\u7d71\u3002ODE \u89e3\u7684\u7cbe\u5ea6\u6703\u986f\u8457\u5f71\u97ff\u53c3\u6578\u6700\u4f73\u5316\uff0c\u9032\u800c\u5f71\u97ff\u6a21\u578b\u6548\u80fd\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u9032\u968e\u7684 Transformer \u67b6\u69cb\u8a2d\u8a08\u63a2\u7d22\uff0c\u4ee5\u6700\u5c0f\u5316\u8207\u771f\u5be6\u300c\u89e3\u300d\u7684\u8aa4\u5dee\u3002\u9996\u5148\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u9810\u6e2c\u6821\u6b63\u5b78\u7fd2\u67b6\u69cb\u4f86\u6700\u5c0f\u5316\u622a\u65b7\u8aa4\u5dee\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u500b\u9ad8\u968e\u9810\u6e2c\u5668\u548c\u4e00\u500b\u591a\u6b65\u6821\u6b63\u5668\u3002\u5176\u6b21\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc\u6307\u6578\u79fb\u52d5\u5e73\u5747\u7684\u4fc2\u6578\u5b78\u7fd2\u65b9\u6cd5\uff0c\u4ee5\u5f37\u5316\u6211\u5011\u7684\u9ad8\u968e\u9810\u6e2c\u5668\u3002\u5728\u5927\u578b\u6a5f\u5668\u7ffb\u8b6f\u3001\u62bd\u8c61\u6458\u8981\u3001\u8a9e\u8a00\u5efa\u6a21\u548c\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u512a\u8d8a\u6027\u3002\u5728 WMT'14 \u82f1\u5fb7\u548c\u82f1\u6cd5\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u7684\u6a21\u578b\u5206\u5225\u9054\u5230\u4e86 30.95 \u548c 44.27 \u7684 BLEU \u5206\u6578\u3002\u6b64\u5916\uff0c\u5728 OPUS \u591a\u8a9e\u8a00\u6a5f\u5668\u7ffb\u8b6f\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u7684\u6a21\u578b\u50c5\u4f7f\u7528 1/3 \u7684\u53c3\u6578\uff0c\u5c31\u8d85\u8d8a\u4e86\u5f37\u5927\u7684 3.8B DeepNet\uff0c\u5e73\u5747\u9ad8\u51fa 2.9 SacreBLEU\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u9084\u5728 LM Harness Evaluation \u4e0a\u6bd4 LLama \u6a21\u578b\u9ad8\u51fa 5.7 \u500b\u6e96\u78ba\u5ea6\u9ede\u3002", "author": "Bei Li et.al.", "authors": "Bei Li, Tong Zheng, Rui Wang, Jiahao Liu, Qingyan Guo, Junliang Guo, Xu Tan, Tong Xiao, Jingbo Zhu, Jingang Wang, Xunliang Cai", "id": "2411.03042v1", "paper_url": "http://arxiv.org/abs/2411.03042v1", "repo": "null"}}