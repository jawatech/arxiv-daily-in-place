{"2409.11402": {"publish_time": "2024-09-17", "title": "NVLM: Open Frontier-Class Multimodal LLMs", "paper_summary": "We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we are releasing the model weights and will open-source the code for the\ncommunity: https://nvlm-project.github.io/.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a8\u51fa NVLM 1.0\uff0c\u9019\u662f\u4e00\u500b\u524d\u6cbf\u7d1a\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5bb6\u65cf\uff0c\u5728\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u6210\u679c\uff0c\u8207\u9818\u5148\u7684\u5c08\u6709\u6a21\u578b\uff08\u4f8b\u5982 GPT-4o\uff09\u548c\u958b\u653e\u8a2a\u554f\u6a21\u578b\uff08\u4f8b\u5982 Llama 3-V 405B \u548c InternVL 2\uff09\u76f8\u5ab2\u7f8e\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cNVLM 1.0 \u5728\u591a\u6a21\u614b\u8a13\u7df4\u5f8c\uff0c\u5176\u7d14\u6587\u5b57\u8868\u73fe\u512a\u65bc\u5176 LLM \u4e3b\u5e79\u3002\u5728\u6a21\u578b\u8a2d\u8a08\u65b9\u9762\uff0c\u6211\u5011\u5c0d\u50c5\u89e3\u78bc\u5668\u591a\u6a21\u614b LLM\uff08\u4f8b\u5982 LLaVA\uff09\u548c\u57fa\u65bc\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u6a21\u578b\uff08\u4f8b\u5982 Flamingo\uff09\u9032\u884c\u4e86\u5168\u9762\u6bd4\u8f03\u3002\u6839\u64da\u5169\u7a2e\u65b9\u6cd5\u7684\u512a\u7f3a\u9ede\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u65e2\u80fd\u63d0\u9ad8\u8a13\u7df4\u6548\u7387\uff0c\u53c8\u80fd\u589e\u5f37\u591a\u6a21\u614b\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u70ba\u57fa\u65bc\u5716\u584a\u7684\u52d5\u614b\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u5f15\u5165\u4e86 1-D \u78c1\u78da\u6a19\u7c64\u8a2d\u8a08\uff0c\u9019\u986f\u8457\u63d0\u5347\u4e86\u591a\u6a21\u614b\u63a8\u7406\u548c OCR \u76f8\u95dc\u4efb\u52d9\u7684\u6548\u80fd\u3002\u95dc\u65bc\u8a13\u7df4\u8cc7\u6599\uff0c\u6211\u5011\u7cbe\u5fc3\u7b56\u5283\u4e26\u63d0\u4f9b\u4e86\u6709\u95dc\u6211\u5011\u591a\u6a21\u614b\u9810\u8a13\u7df4\u548c\u76e3\u7763\u5fae\u8abf\u8cc7\u6599\u96c6\u7684\u8a73\u7d30\u8cc7\u8a0a\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u8cc7\u6599\u96c6\u54c1\u8cea\u548c\u4efb\u52d9\u591a\u6a23\u6027\u6bd4\u898f\u6a21\u66f4\u91cd\u8981\uff0c\u5373\u4f7f\u5728\u9810\u8a13\u7df4\u968e\u6bb5\uff0c\u4e5f\u9069\u7528\u65bc\u6240\u6709\u67b6\u69cb\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u70ba NVLM-1.0 \u6a21\u578b\u958b\u767c\u4e86\u751f\u7522\u7d1a\u591a\u6a21\u614b\uff0c\u8b93\u5b83\u5011\u80fd\u5920\u5728\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u540c\u6642\u8207\u5176 LLM \u4e3b\u5e79\u76f8\u6bd4\uff0c\u7dad\u6301\u751a\u81f3\u63d0\u5347\u7d14\u6587\u5b57\u8868\u73fe\u3002\u70ba\u4e86\u5be6\u73fe\u9019\u4e00\u9ede\uff0c\u6211\u5011\u5728\u591a\u6a21\u614b\u8a13\u7df4\u4e2d\u88fd\u4f5c\u4e26\u6574\u5408\u4e86\u4e00\u500b\u9ad8\u54c1\u8cea\u7684\u7d14\u6587\u5b57\u8cc7\u6599\u96c6\uff0c\u4ee5\u53ca\u5927\u91cf\u7684\u591a\u6a21\u614b\u6578\u5b78\u548c\u63a8\u7406\u8cc7\u6599\uff0c\u5f9e\u800c\u589e\u5f37\u4e86\u8de8\u6a21\u614b\u7684\u6578\u5b78\u548c\u7de8\u78bc\u80fd\u529b\u3002\u70ba\u4e86\u63a8\u52d5\u8a72\u9818\u57df\u7684\u7814\u7a76\uff0c\u6211\u5011\u5c07\u91cb\u51fa\u6a21\u578b\u6b0a\u91cd\uff0c\u4e26\u5c07\u70ba\u793e\u7fa4\u958b\u653e\u539f\u59cb\u78bc\uff1ahttps://nvlm-project.github.io/\u3002</paragraph>", "author": "Wenliang Dai et.al.", "authors": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping", "id": "2409.11402v1", "paper_url": "http://arxiv.org/abs/2409.11402v1", "repo": "null"}}