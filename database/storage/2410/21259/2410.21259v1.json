{"2410.21259": {"publish_time": "2024-10-28", "title": "AutoBench-V: Can Large Vision-Language Models Benchmark Themselves?", "paper_summary": "Large Vision-Language Models (LVLMs) have become essential for advancing the\nintegration of visual and linguistic information, facilitating a wide range of\ncomplex applications and tasks. However, the evaluation of LVLMs presents\nsignificant challenges as the evaluation benchmark always demands lots of human\ncost for its construction, and remains static, lacking flexibility once\nconstructed. Even though automatic evaluation has been explored in textual\nmodality, the visual modality remains under-explored. As a result, in this\nwork, we address a question: \"Can LVLMs serve as a path to automatic\nbenchmarking?\". We introduce AutoBench-V, an automated framework for serving\nevaluation on demand, i.e., benchmarking LVLMs based on specific aspects of\nmodel capability. Upon receiving an evaluation capability, AutoBench-V\nleverages text-to-image models to generate relevant image samples and then\nutilizes LVLMs to orchestrate visual question-answering (VQA) tasks, completing\nthe evaluation process efficiently and flexibly. Through an extensive\nevaluation of seven popular LVLMs across five demanded user inputs (i.e.,\nevaluation capabilities), the framework shows effectiveness and reliability. We\nobserve the following: (1) Our constructed benchmark accurately reflects\nvarying task difficulties; (2) As task difficulty rises, the performance gap\nbetween models widens; (3) While models exhibit strong performance in abstract\nlevel understanding, they underperform in details reasoning tasks; and (4)\nConstructing a dataset with varying levels of difficulties is critical for a\ncomprehensive and exhaustive evaluation. Overall, AutoBench-V not only\nsuccessfully utilizes LVLMs for automated benchmarking but also reveals that\nLVLMs as judges have significant potential in various domains.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5df2\u6210\u70ba\u63a8\u9032\u8996\u89ba\u548c\u8a9e\u8a00\u8cc7\u8a0a\u6574\u5408\u7684\u5fc5\u8981\u5143\u7d20\uff0c\u4fc3\u9032\u4e86\u5404\u7a2e\u8907\u96dc\u61c9\u7528\u7a0b\u5f0f\u548c\u4efb\u52d9\u3002\u7136\u800c\uff0cLVLMs \u7684\u8a55\u4f30\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6230\uff0c\u56e0\u70ba\u8a55\u4f30\u57fa\u6e96\u7e3d\u662f\u9700\u8981\u5927\u91cf\u4eba\u529b\u6210\u672c\u624d\u80fd\u5efa\u7f6e\uff0c\u800c\u4e14\u4e00\u65e6\u5efa\u7f6e\u5b8c\u6210\u5c31\u6703\u4fdd\u6301\u975c\u614b\uff0c\u7f3a\u4e4f\u9748\u6d3b\u6027\u3002\u5118\u7ba1\u81ea\u52d5\u8a55\u4f30\u5df2\u5728\u6587\u5b57\u6a21\u5f0f\u4e2d\u9032\u884c\u63a2\u8a0e\uff0c\u4f46\u8996\u89ba\u6a21\u5f0f\u4ecd\u672a\u5145\u5206\u63a2\u8a0e\u3002\u56e0\u6b64\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4e00\u500b\u554f\u984c\uff1a\u300cLVLMs \u80fd\u5426\u4f5c\u70ba\u81ea\u52d5\u57fa\u6e96\u6e2c\u8a66\u7684\u9014\u5f91\uff1f\u300d\u6211\u5011\u5f15\u5165\u4e86 AutoBench-V\uff0c\u4e00\u500b\u81ea\u52d5\u5316\u67b6\u69cb\uff0c\u7528\u65bc\u4f9d\u9700\u6c42\u63d0\u4f9b\u8a55\u4f30\uff0c\u4ea6\u5373\uff0c\u6839\u64da\u6a21\u578b\u80fd\u529b\u7684\u7279\u5b9a\u9762\u5411\u5c0d LVLMs \u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u3002\u5728\u6536\u5230\u8a55\u4f30\u80fd\u529b\u5f8c\uff0cAutoBench-V \u5229\u7528\u6587\u5b57\u8f49\u5f71\u50cf\u6a21\u578b\u7522\u751f\u76f8\u95dc\u7684\u5f71\u50cf\u7bc4\u4f8b\uff0c\u7136\u5f8c\u5229\u7528 LVLMs \u7de8\u6392\u8996\u89ba\u554f\u7b54 (VQA) \u4efb\u52d9\uff0c\u6709\u6548\u7387\u4e14\u9748\u6d3b\u5730\u5b8c\u6210\u8a55\u4f30\u6d41\u7a0b\u3002\u900f\u904e\u5c0d\u4e03\u500b\u5e38\u898b LVLMs \u9032\u884c\u5ee3\u6cdb\u8a55\u4f30\uff0c\u6db5\u84cb\u4e94\u9805\u4f7f\u7528\u8005\u8f38\u5165\u9700\u6c42\uff08\u4ea6\u5373\uff0c\u8a55\u4f30\u80fd\u529b\uff09\uff0c\u6b64\u67b6\u69cb\u5c55\u73fe\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002\u6211\u5011\u89c0\u5bdf\u5230\u4ee5\u4e0b\u5e7e\u9ede\uff1a(1) \u6211\u5011\u5efa\u69cb\u7684\u57fa\u6e96\u6e2c\u8a66\u6e96\u78ba\u53cd\u6620\u4e86\u4e0d\u540c\u7684\u4efb\u52d9\u96e3\u5ea6\uff1b(2) \u96a8\u8457\u4efb\u52d9\u96e3\u5ea6\u4e0a\u5347\uff0c\u6a21\u578b\u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\u64f4\u5927\uff1b(3) \u5118\u7ba1\u6a21\u578b\u5728\u62bd\u8c61\u5c64\u7d1a\u7406\u89e3\u65b9\u9762\u5c55\u73fe\u51fa\u5f37\u52c1\u6548\u80fd\uff0c\u4f46\u5728\u7d30\u7bc0\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u537b\u4e0d\u4f73\uff1b(4) \u5efa\u7acb\u4e00\u500b\u5177\u6709\u4e0d\u540c\u96e3\u5ea6\u5c64\u7d1a\u7684\u8cc7\u6599\u96c6\u5c0d\u65bc\u5168\u9762\u4e14\u8a73\u76e1\u7684\u8a55\u4f30\u81f3\u95dc\u91cd\u8981\u3002\u6574\u9ad4\u800c\u8a00\uff0cAutoBench-V \u4e0d\u50c5\u6210\u529f\u5730\u5229\u7528 LVLMs \u9032\u884c\u81ea\u52d5\u57fa\u6e96\u6e2c\u8a66\uff0c\u4e5f\u63ed\u793a\u4e86 LVLMs \u4f5c\u70ba\u8a55\u5be9\u5728\u5404\u500b\u9818\u57df\u5177\u6709\u986f\u8457\u7684\u6f5b\u529b\u3002", "author": "Han Bao et.al.", "authors": "Han Bao, Yue Huang, Yanbo Wang, Jiayi Ye, Xiangqi Wang, Xiuyin Chen, Mohamed Elhoseiny, Xiangliang Zhang", "id": "2410.21259v1", "paper_url": "http://arxiv.org/abs/2410.21259v1", "repo": "null"}}