{"2410.03062": {"publish_time": "2024-10-04", "title": "Image First or Text First? Optimising the Sequencing of Modalities in Large Language Model Prompting and Reasoning Tasks", "paper_summary": "This paper examines how the sequencing of images and text within multi-modal\nprompts influences the reasoning performance of large language models (LLMs).\nWe performed empirical evaluations using three commercial LLMs. Our results\ndemonstrate that the order in which modalities are presented can significantly\naffect performance, particularly in tasks of varying complexity. For simpler\ntasks involving a single image, modality sequencing had a clear impact on\naccuracy. However, in more complex tasks involving multiple images and\nintricate reasoning steps, the effect of sequencing diminished, likely due to\nthe increased cognitive demands of the task. Our findings also highlight the\nimportance of question/prompt structure. In nested and multi-step reasoning\ntasks, modality sequencing played a key role in shaping model performance.\nWhile LLMs excelled in the initial stages of reasoning, they struggled to\nre-incorporate earlier information, underscoring the challenges of multi-hop\nreasoning within transformer architectures. This suggests that aligning the\nsequence of modalities with the logical flow of reasoning steps is more\ncritical than modality order alone. These insights offer valuable implications\nfor improving multi-modal prompt design, with broader applications across\nfields such as education, medical imaging, and cross-modal learning.", "paper_summary_zh": "\u672c\u6587\u63a2\u8a0e\u5728\u591a\u6a21\u614b\u63d0\u793a\u4e2d\u5f71\u50cf\u548c\u6587\u5b57\u7684\u9806\u5e8f\u5982\u4f55\u5f71\u97ff\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u8868\u73fe\u3002\u6211\u5011\u4f7f\u7528\u4e09\u500b\u5546\u7528 LLM \u9032\u884c\u5be6\u8b49\u8a55\u4f30\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u6a21\u614b\u5448\u73fe\u7684\u9806\u5e8f\u6703\u986f\u8457\u5f71\u97ff\u8868\u73fe\uff0c\u7279\u5225\u662f\u5728\u4e0d\u540c\u8907\u96dc\u5ea6\u4efb\u52d9\u4e2d\u3002\u5c0d\u65bc\u6d89\u53ca\u55ae\u4e00\u5f71\u50cf\u7684\u8f03\u7c21\u55ae\u4efb\u52d9\uff0c\u6a21\u614b\u9806\u5e8f\u5c0d\u6e96\u78ba\u5ea6\u6709\u660e\u986f\u5f71\u97ff\u3002\u7136\u800c\uff0c\u5728\u6d89\u53ca\u591a\u500b\u5f71\u50cf\u548c\u8907\u96dc\u63a8\u7406\u6b65\u9a5f\u7684\u8f03\u8907\u96dc\u4efb\u52d9\u4e2d\uff0c\u9806\u5e8f\u7684\u5f71\u97ff\u6e1b\u5f31\uff0c\u9019\u53ef\u80fd\u662f\u7531\u65bc\u4efb\u52d9\u7684\u8a8d\u77e5\u9700\u6c42\u589e\u52a0\u3002\u6211\u5011\u7684\u767c\u73fe\u4e5f\u7a81\u986f\u4e86\u554f\u984c/\u63d0\u793a\u7d50\u69cb\u7684\u91cd\u8981\u6027\u3002\u5728\u5d4c\u5957\u548c\u591a\u6b65\u9a5f\u63a8\u7406\u4efb\u52d9\u4e2d\uff0c\u6a21\u614b\u9806\u5e8f\u5728\u5851\u9020\u6a21\u578b\u8868\u73fe\u4e2d\u626e\u6f14\u95dc\u9375\u89d2\u8272\u3002\u96d6\u7136 LLM \u5728\u63a8\u7406\u7684\u521d\u59cb\u968e\u6bb5\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5b83\u5011\u96e3\u4ee5\u91cd\u65b0\u6574\u5408\u65e9\u671f\u7684\u8cc7\u8a0a\uff0c\u9019\u51f8\u986f\u4e86Transformer\u67b6\u69cb\u4e2d\u591a\u8df3\u63a8\u7406\u7684\u6311\u6230\u3002\u9019\u8868\u660e\u5c07\u6a21\u614b\u9806\u5e8f\u8207\u63a8\u7406\u6b65\u9a5f\u7684\u908f\u8f2f\u6d41\u7a0b\u5c0d\u9f4a\u6bd4\u55ae\u7368\u7684\u6a21\u614b\u9806\u5e8f\u66f4\u70ba\u91cd\u8981\u3002\u9019\u4e9b\u898b\u89e3\u70ba\u6539\u5584\u591a\u6a21\u614b\u63d0\u793a\u8a2d\u8a08\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u555f\u793a\uff0c\u4e26\u5728\u6559\u80b2\u3001\u91ab\u5b78\u5f71\u50cf\u548c\u8de8\u6a21\u614b\u5b78\u7fd2\u7b49\u9818\u57df\u6709\u66f4\u5ee3\u6cdb\u7684\u61c9\u7528\u3002", "author": "Grant Wardle et.al.", "authors": "Grant Wardle, Teo Susnjak", "id": "2410.03062v1", "paper_url": "http://arxiv.org/abs/2410.03062v1", "repo": "null"}}