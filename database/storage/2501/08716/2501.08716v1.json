{"2501.08716": {"publish_time": "2025-01-15", "title": "The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of Instruction Tuning and In-Context Learning Capabilities", "paper_summary": "Large Language Models (LLMs), trained on extensive web-scale corpora, have\ndemonstrated remarkable abilities across diverse tasks, especially as they are\nscaled up. Nevertheless, even state-of-the-art models struggle in certain\ncases, sometimes failing at problems solvable by young children, indicating\nthat traditional notions of task complexity are insufficient for explaining LLM\ncapabilities. However, exploring LLM capabilities is complicated by the fact\nthat most widely-used models are also \"instruction-tuned\" to respond\nappropriately to prompts. With the goal of disentangling the factors\ninfluencing LLM performance, we investigate whether instruction-tuned models\npossess fundamentally different capabilities from base models that are prompted\nusing in-context examples. Through extensive experiments across various model\nfamilies, scales and task types, which included instruction tuning 90 different\nLLMs, we demonstrate that the performance of instruction-tuned models is\nsignificantly correlated with the in-context performance of their base\ncounterparts. By clarifying what instruction-tuning contributes, we extend\nprior research into in-context learning, which suggests that base models use\npriors from pretraining data to solve tasks. Specifically, we extend this\nunderstanding to instruction-tuned models, suggesting that their pretraining\ndata similarly sets a limiting boundary on the tasks they can solve, with the\nadded influence of the instruction-tuning dataset.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5ee3\u6cdb\u7684\u7db2\u8def\u898f\u6a21\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\uff0c\u5728\u4e0d\u540c\u7684\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u7279\u5225\u662f\u5728\u5b83\u5011\u64f4\u5927\u898f\u6a21\u6642\u3002\u7136\u800c\uff0c\u5373\u4f7f\u662f\u6700\u5148\u9032\u7684\u6a21\u578b\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\u4e5f\u6703\u9047\u5230\u56f0\u96e3\uff0c\u6709\u6642\u751a\u81f3\u7121\u6cd5\u89e3\u6c7a\u5e7c\u5152\u53ef\u4ee5\u89e3\u6c7a\u7684\u554f\u984c\uff0c\u9019\u8868\u793a\u50b3\u7d71\u4efb\u52d9\u8907\u96dc\u6027\u7684\u6982\u5ff5\u4e0d\u8db3\u4ee5\u89e3\u91cb LLM \u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u63a2\u7d22 LLM \u80fd\u529b\u6703\u56e0\u70ba\u4e00\u500b\u4e8b\u5be6\u800c\u8b8a\u5f97\u8907\u96dc\uff0c\u90a3\u5c31\u662f\u5927\u591a\u6578\u5ee3\u6cdb\u4f7f\u7528\u7684\u6a21\u578b\u4e5f\u7d93\u904e\u300c\u6307\u4ee4\u8abf\u6574\u300d\uff0c\u4ee5\u9069\u7576\u5730\u56de\u61c9\u63d0\u793a\u3002\u70ba\u4e86\u89e3\u958b\u5f71\u97ff LLM \u6548\u80fd\u7684\u56e0\u7d20\uff0c\u6211\u5011\u8abf\u67e5\u7d93\u904e\u6307\u4ee4\u8abf\u6574\u7684\u6a21\u578b\u662f\u5426\u5177\u5099\u8207\u4f7f\u7528\u60c5\u5883\u7bc4\u4f8b\u63d0\u793a\u7684\u57fa\u672c\u6a21\u578b\u622a\u7136\u4e0d\u540c\u7684\u80fd\u529b\u3002\u900f\u904e\u91dd\u5c0d\u5404\u7a2e\u6a21\u578b\u5bb6\u65cf\u3001\u898f\u6a21\u548c\u4efb\u52d9\u985e\u578b\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u5176\u4e2d\u5305\u62ec\u5c0d 90 \u500b\u4e0d\u540c\u7684 LLM \u9032\u884c\u6307\u4ee4\u8abf\u6574\uff0c\u6211\u5011\u8b49\u660e\u4e86\u7d93\u904e\u6307\u4ee4\u8abf\u6574\u7684\u6a21\u578b\u7684\u6548\u80fd\u8207\u5176\u57fa\u672c\u5c0d\u61c9\u6a21\u578b\u7684\u60c5\u5883\u6548\u80fd\u986f\u8457\u76f8\u95dc\u3002\u900f\u904e\u91d0\u6e05\u6307\u4ee4\u8abf\u6574\u7684\u8ca2\u737b\uff0c\u6211\u5011\u64f4\u5c55\u4e86\u5148\u524d\u5c0d\u60c5\u5883\u5b78\u7fd2\u7684\u7814\u7a76\uff0c\u9019\u8868\u660e\u57fa\u672c\u6a21\u578b\u4f7f\u7528\u9810\u8a13\u7df4\u8cc7\u6599\u4e2d\u7684\u5148\u9a57\u77e5\u8b58\u4f86\u89e3\u6c7a\u4efb\u52d9\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u6b64\u7406\u89e3\u64f4\u5c55\u5230\u7d93\u904e\u6307\u4ee4\u8abf\u6574\u7684\u6a21\u578b\uff0c\u9019\u8868\u660e\u4ed6\u5011\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u540c\u6a23\u8a2d\u5b9a\u4e86\u4e00\u500b\u9650\u5236\u908a\u754c\uff0c\u9650\u5236\u4e86\u4ed6\u5011\u53ef\u4ee5\u89e3\u6c7a\u7684\u4efb\u52d9\uff0c\u4e26\u589e\u52a0\u4e86\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u96c6\u7684\u5f71\u97ff\u3002", "author": "Irina Bigoulaeva et.al.", "authors": "Irina Bigoulaeva, Harish Tayyar Madabushi, Iryna Gurevych", "id": "2501.08716v1", "paper_url": "http://arxiv.org/abs/2501.08716v1", "repo": "https://github.com/ukplab/arxiv2025-inherent-limits-plms"}}