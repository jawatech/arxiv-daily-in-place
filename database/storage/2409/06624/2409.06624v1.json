{"2409.06624": {"publish_time": "2024-09-10", "title": "A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio", "paper_summary": "Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7ecf\u5e38\u9700\u8981\u6301\u7eed\u9884\u8bad\u7ec3 (CPT) \u4ee5\u83b7\u5f97\u4e0d\u719f\u6089\u7684\u8bed\u8a00\u6280\u80fd\u6216\u9002\u5e94\u65b0\u9886\u57df\u3002CPT \u7684\u5de8\u989d\u8bad\u7ec3\u6210\u672c\u901a\u5e38\u9700\u8981\u8c28\u614e\u9009\u62e9\u5173\u952e\u8d85\u53c2\u6570\uff0c\u4f8b\u5982\u989d\u5916\u8bed\u8a00\u6216\u9886\u57df\u8bed\u6599\u5e93\u7684\u6df7\u5408\u6bd4\u4f8b\u3002\u7136\u800c\uff0c\u6ca1\u6709\u7cfb\u7edf\u6027\u7684\u7814\u7a76\u6765\u5f25\u5408\u7406\u60f3\u6df7\u5408\u6bd4\u4f8b\u4e0e\u5b9e\u9645\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u53ca\u5b9e\u9a8c\u7f29\u653e\u5b9a\u5f8b\u4e0e\u5b9e\u9645\u90e8\u7f72\u5728\u5b8c\u6574\u6a21\u578b\u89c4\u6a21\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5728 Llama-3 8B \u548c 70B \u4e0a\u6267\u884c CPT \u4ee5\u589e\u5f3a\u5176\u4e2d\u6587\u80fd\u529b\u3002\u6211\u4eec\u7814\u7a76\u4e86 8B \u5927\u5c0f\u4e0a\u7684\u9644\u52a0\u8bed\u8a00\u6df7\u5408\u6bd4\u4f8b (ALMR) \u548c\u5b66\u4e60\u7387 (LR) \u4e4b\u95f4\u7684\u6700\u4f18\u76f8\u5173\u6027\uff0c\u8be5\u76f8\u5173\u6027\u76f4\u63a5\u6307\u793a\u6700\u4f18\u5b9e\u9a8c\u8bbe\u7f6e\u3002\u901a\u8fc7\u5f7b\u5e95\u9009\u62e9\u8d85\u53c2\u6570\u548c\u968f\u540e\u7684\u5fae\u8c03\uff0c\u6a21\u578b\u80fd\u529b\u4e0d\u4ec5\u5728\u4e0e\u4e2d\u6587\u76f8\u5173\u7684\u57fa\u51c6\u4e0a\u5f97\u5230\u63d0\u5347\uff0c\u800c\u4e14\u5728\u5305\u62ec\u6570\u5b66\u3001\u7f16\u7801\u548c\u60c5\u5546\u5728\u5185\u7684\u4e00\u4e9b\u7279\u5b9a\u9886\u57df\u4e5f\u5f97\u5230\u63d0\u5347\u3002\u6211\u4eec\u5728\u4e00\u4e2a\u73b0\u5b9e\u751f\u6d3b\u804a\u5929\u7cfb\u7edf\u4e0a\u90e8\u7f72\u4e86 LLM \u7684\u6700\u7ec8 70B \u7248\u672c\uff0c\u8be5\u7cfb\u7edf\u83b7\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\u3002", "author": "Ningyuan Xi et.al.", "authors": "Ningyuan Xi, Yetao Wu, Kun Fan, Teng Chen, Qingqing Gu, Peng Yu, Jinxian Qu, Chenxi Liu, Zhonglin Jiang, Yong Chen, Luo Ji", "id": "2409.06624v1", "paper_url": "http://arxiv.org/abs/2409.06624v1", "repo": "null"}}