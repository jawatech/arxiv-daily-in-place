{"2502.10266": {"publish_time": "2025-02-14", "title": "Are Large Language Models the future crowd workers of Linguistics?", "paper_summary": "Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities.", "paper_summary_zh": "<paragraph>\u5f9e\u4eba\u985e\u53c3\u8207\u8005\u5f15\u51fa\u8cc7\u6599\u662f\u5be6\u8b49\u8a9e\u8a00\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u6838\u5fc3\u8cc7\u6599\u6536\u96c6\u7b56\u7565\u4e4b\u4e00\u3002\u6b64\u985e\u7814\u7a76\u7684\u53c3\u8207\u8005\u6578\u91cf\u53ef\u80fd\u5dee\u7570\u5f88\u5927\uff0c\u5f9e\u5c11\u6578\u4eba\u5230\u7fa4\u773e\u5916\u5305\u7684\u898f\u6a21\u90fd\u6709\u3002\u5373\u4f7f\u4ed6\u5011\u63d0\u4f9b\u4e86\u8c50\u5bcc\u4e14\u5ee3\u6cdb\u7684\u8cc7\u6599\uff0c\u9019\u5169\u7a2e\u8a2d\u5b9a\u90fd\u4f34\u96a8\u8457\u8a31\u591a\u7f3a\u9ede\uff0c\u4f8b\u5982\u5728\u5b8c\u6210\u4efb\u52d9\u671f\u9593\u5c0d\u53c3\u8207\u8005\u6ce8\u610f\u529b\u63a7\u5236\u4e0d\u4f73\u3001\u7fa4\u773e\u5916\u5305\u74b0\u5883\u4e2d\u7684\u5de5\u4f5c\u689d\u4ef6\u4e0d\u7a69\u5b9a\uff0c\u4ee5\u53ca\u8017\u6642\u7684\u5be6\u9a57\u8a2d\u8a08\u3002\u57fa\u65bc\u9019\u4e9b\u539f\u56e0\uff0c\u672c\u7814\u7a76\u65e8\u5728\u56de\u7b54\u4e00\u500b\u554f\u984c\uff0c\u5373\u5982\u679c\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7d0d\u5165\u5be6\u8b49\u8a9e\u8a00\u7ba1\u9053\uff0c\u5b83\u5011\u662f\u5426\u53ef\u4ee5\u514b\u670d\u9019\u4e9b\u969c\u7919\u3002\u9032\u884c\u4e86\u5169\u500b\u8907\u88fd\u6848\u4f8b\u7814\u7a76\u4ee5\u91d0\u6e05\u6b64\u4e8b\uff1aCruz (2023) \u548c Lombard \u7b49\u4eba (2021)\u3002\u6700\u521d\u70ba\u4eba\u985e\u53c3\u8207\u8005\u8a2d\u8a08\u7684\u5169\u500b\u5f37\u5236\u5f15\u51fa\u4efb\u52d9\uff0c\u5728\u5efa\u8b70\u7684\u67b6\u69cb\u4e2d\u4f7f\u7528 OpenAI \u7684 GPT-4o-mini \u6a21\u578b\u8907\u88fd\u3002\u5b83\u8207\u6211\u5011\u7684\u96f6\u6b21\u63d0\u793a\u57fa\u6e96\u7684\u6548\u80fd\u5c55\u73fe\u4e86 LLM \u7684\u6709\u6548\u6027\u548c\u9ad8\u5ea6\u901a\u7528\u6027\uff0c\u9019\u5f80\u5f80\u5728\u8a9e\u8a00\u4efb\u52d9\u4e2d\u512a\u65bc\u4eba\u985e\u7dda\u4eba\u3002\u7b2c\u4e8c\u6b21\u8907\u88fd\u7684\u7d50\u679c\u9032\u4e00\u6b65\u5f37\u8abf\u4e86\u63a2\u7d22\u5176\u4ed6\u63d0\u793a\u6280\u8853\u7684\u5fc5\u8981\u6027\uff0c\u4f8b\u5982\u601d\u8003\u93c8 (CoT) \u63d0\u793a\uff0c\u5728\u7b2c\u4e8c\u6b21\u8ffd\u8e64\u5be6\u9a57\u4e2d\uff0c\u5b83\u8b49\u660e\u5728\u95dc\u9375\u9805\u76ee\u548c\u586b\u7a7a\u9805\u76ee\u4e0a\u8207\u4eba\u985e\u8868\u73fe\u7684\u4e00\u81f4\u6027\u66f4\u9ad8\u3002\u9451\u65bc\u672c\u7814\u7a76\u7684\u898f\u6a21\u6709\u9650\uff0c\u9032\u4e00\u6b65\u63a2\u8a0e LLM \u5728\u5be6\u8b49\u8a9e\u8a00\u5b78\u4e2d\u7684\u8868\u73fe\u4ee5\u53ca\u5728\u4eba\u6587\u5b78\u79d1\u4e2d\u5176\u4ed6\u672a\u4f86\u7684\u61c9\u7528\u662f\u503c\u5f97\u7684\u3002</paragraph>", "author": "Iris Ferrazzo et.al.", "authors": "Iris Ferrazzo", "id": "2502.10266v1", "paper_url": "http://arxiv.org/abs/2502.10266v1", "repo": "null"}}