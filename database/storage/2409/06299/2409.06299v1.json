{"2409.06299": {"publish_time": "2024-09-10", "title": "Enhancing Long Video Understanding via Hierarchical Event-Based Memory", "paper_summary": "Recently, integrating visual foundation models into large language models\n(LLMs) to form video understanding systems has attracted widespread attention.\nMost of the existing models compress diverse semantic information within the\nwhole video and feed it into LLMs for content comprehension. While this method\nexcels in short video understanding, it may result in a blend of multiple event\ninformation in long videos due to coarse compression, which causes information\nredundancy. Consequently, the semantics of key events might be obscured within\nthe vast information that hinders the model's understanding capabilities. To\naddress this issue, we propose a Hierarchical Event-based Memory-enhanced LLM\n(HEM-LLM) for better understanding of long videos. Firstly, we design a novel\nadaptive sequence segmentation scheme to divide multiple events within long\nvideos. In this way, we can perform individual memory modeling for each event\nto establish intra-event contextual connections, thereby reducing information\nredundancy. Secondly, while modeling current event, we compress and inject the\ninformation of the previous event to enhance the long-term inter-event\ndependencies in videos. Finally, we perform extensive experiments on various\nvideo understanding tasks and the results show that our model achieves\nstate-of-the-art performances.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6574\u5408\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u4ee5\u5f62\u6210\u89c6\u9891\u7406\u89e3\u7cfb\u7edf\u5df2\u5f15\u8d77\u5e7f\u6cdb\u5173\u6ce8\u3002\n\u5927\u591a\u6570\u73b0\u6709\u6a21\u578b\u90fd\u4f1a\u538b\u7f29\u6574\u4e2a\u89c6\u9891\u4e2d\u7684\u5404\u79cd\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u8f93\u5165 LLM \u4ee5\u8fdb\u884c\u5185\u5bb9\u7406\u89e3\u3002\u867d\u7136\u6b64\u65b9\u6cd5\u5728\u77ed\u89c6\u9891\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u7c97\u7565\u7684\u538b\u7f29\uff0c\u5b83\u53ef\u80fd\u4f1a\u5bfc\u81f4\u957f\u89c6\u9891\u4e2d\u591a\u4e2a\u4e8b\u4ef6\u4fe1\u606f\u7684\u6df7\u5408\uff0c\u4ece\u800c\u5bfc\u81f4\u4fe1\u606f\u5197\u4f59\u3002\u56e0\u6b64\uff0c\u5173\u952e\u4e8b\u4ef6\u7684\u8bed\u4e49\u53ef\u80fd\u4f1a\u88ab\u9690\u85cf\u5728\u5927\u91cf\u4fe1\u606f\u4e2d\uff0c\u4ece\u800c\u963b\u788d\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u4e8b\u4ef6\u8bb0\u5fc6\u589e\u5f3a LLM (HEM-LLM) \u4ee5\u4fbf\u66f4\u597d\u5730\u7406\u89e3\u957f\u89c6\u9891\u3002\u9996\u5148\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u5e8f\u5217\u5206\u5272\u65b9\u6848\uff0c\u4ee5\u5212\u5206\u957f\u89c6\u9891\u4e2d\u7684\u591a\u4e2a\u4e8b\u4ef6\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u6bcf\u4e2a\u4e8b\u4ef6\u6267\u884c\u5355\u72ec\u7684\u8bb0\u5fc6\u5efa\u6a21\uff0c\u4ee5\u5efa\u7acb\u4e8b\u4ef6\u5185\u90e8\u7684\u4e0a\u4e0b\u6587\u8fde\u63a5\uff0c\u4ece\u800c\u51cf\u5c11\u4fe1\u606f\u5197\u4f59\u3002\u5176\u6b21\uff0c\u5728\u5bf9\u5f53\u524d\u4e8b\u4ef6\u8fdb\u884c\u5efa\u6a21\u65f6\uff0c\u6211\u4eec\u538b\u7f29\u5e76\u6ce8\u5165\u524d\u4e00\u4e2a\u4e8b\u4ef6\u7684\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u89c6\u9891\u4e2d\u957f\u671f\u4e8b\u4ef6\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9\u5404\u79cd\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "author": "Dingxin Cheng et.al.", "authors": "Dingxin Cheng, Mingda Li, Jingyu Liu, Yongxin Guo, Bin Jiang, Qingbin Liu, Xi Chen, Bo Zhao", "id": "2409.06299v1", "paper_url": "http://arxiv.org/abs/2409.06299v1", "repo": "null"}}