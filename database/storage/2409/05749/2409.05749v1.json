{"2409.05749": {"publish_time": "2024-09-09", "title": "ReL-SAR: Representation Learning for Skeleton Action Recognition with Convolutional Transformers and BYOL", "paper_summary": "To extract robust and generalizable skeleton action recognition features,\nlarge amounts of well-curated data are typically required, which is a\nchallenging task hindered by annotation and computation costs. Therefore,\nunsupervised representation learning is of prime importance to leverage\nunlabeled skeleton data. In this work, we investigate unsupervised\nrepresentation learning for skeleton action recognition. For this purpose, we\ndesigned a lightweight convolutional transformer framework, named ReL-SAR,\nexploiting the complementarity of convolutional and attention layers for\njointly modeling spatial and temporal cues in skeleton sequences. We also use a\nSelection-Permutation strategy for skeleton joints to ensure more informative\ndescriptions from skeletal data. Finally, we capitalize on Bootstrap Your Own\nLatent (BYOL) to learn robust representations from unlabeled skeleton sequence\ndata. We achieved very competitive results on limited-size datasets: MCAD,\nIXMAS, JHMDB, and NW-UCLA, showing the effectiveness of our proposed method\nagainst state-of-the-art methods in terms of both performance and computational\nefficiency. To ensure reproducibility and reusability, the source code\nincluding all implementation parameters is provided at:\nhttps://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL", "paper_summary_zh": "<paragraph>\u70ba\u4e86\u63d0\u53d6\u5f37\u5065\u4e14\u53ef\u6982\u62ec\u7684\u9aa8\u67b6\u52d5\u4f5c\u8fa8\u8b58\u7279\u5fb5\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u7cbe\u5fc3\u7b56\u5283\u8cc7\u6599\uff0c\u9019\u662f\u4e00\u9805\u53d7\u5230\u8a3b\u89e3\u548c\u904b\u7b97\u6210\u672c\u963b\u7919\u7684\u8271\u96e3\u4efb\u52d9\u3002\u56e0\u6b64\uff0c\u7121\u76e3\u7763\u5f0f\u8868\u5fb5\u5b78\u7fd2\u5c0d\u65bc\u5229\u7528\u672a\u6a19\u8a18\u9aa8\u67b6\u8cc7\u6599\u81f3\u95dc\u91cd\u8981\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u9aa8\u67b6\u52d5\u4f5c\u8fa8\u8b58\u7684\u7121\u76e3\u7763\u5f0f\u8868\u5fb5\u5b78\u7fd2\u3002\u70ba\u6b64\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u8f15\u91cf\u7d1a\u7684\u5377\u7a4d\u8f49\u63db\u5668\u6846\u67b6\uff0c\u540d\u70ba ReL-SAR\uff0c\u5229\u7528\u5377\u7a4d\u548c\u6ce8\u610f\u529b\u5c64\u7684\u4e92\u88dc\u6027\u4f86\u806f\u5408\u5efa\u6a21\u9aa8\u67b6\u5e8f\u5217\u4e2d\u7684\u7a7a\u9593\u548c\u6642\u9593\u7dda\u7d22\u3002\u6211\u5011\u9084\u4f7f\u7528\u9aa8\u67b6\u95dc\u7bc0\u7684\u9078\u64c7\u6392\u5217\u7b56\u7565\uff0c\u4ee5\u78ba\u4fdd\u5f9e\u9aa8\u67b6\u8cc7\u6599\u4e2d\u7372\u5f97\u66f4\u591a\u8cc7\u8a0a\u6027\u7684\u63cf\u8ff0\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5229\u7528 Bootstrap Your Own Latent (BYOL) \u5f9e\u672a\u6a19\u8a18\u7684\u9aa8\u67b6\u5e8f\u5217\u8cc7\u6599\u4e2d\u5b78\u7fd2\u5f37\u5065\u7684\u8868\u5fb5\u3002\u6211\u5011\u5728\u6709\u9650\u5927\u5c0f\u7684\u8cc7\u6599\u96c6\u4e0a\u53d6\u5f97\u4e86\u975e\u5e38\u6709\u7af6\u722d\u529b\u7684\u7d50\u679c\uff1aMCAD\u3001IXMAS\u3001JHMDB \u548c NW-UCLA\uff0c\u986f\u793a\u4e86\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6548\u80fd\u548c\u904b\u7b97\u6548\u7387\u65b9\u9762\u512a\u65bc\u6700\u5148\u9032\u7684\u65b9\u6cd5\u3002\u70ba\u4e86\u78ba\u4fdd\u53ef\u8907\u88fd\u6027\u548c\u53ef\u91cd\u8907\u4f7f\u7528\u6027\uff0c\u539f\u59cb\u7a0b\u5f0f\u78bc\uff08\u5305\u62ec\u6240\u6709\u5be6\u4f5c\u53c3\u6578\uff09\u53ef\u5728\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1a\nhttps://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL</paragraph>", "author": "Safwen Naimi et.al.", "authors": "Safwen Naimi, Wassim Bouachir, Guillaume-Alexandre Bilodeau", "id": "2409.05749v1", "paper_url": "http://arxiv.org/abs/2409.05749v1", "repo": "null"}}