{"2409.16167": {"publish_time": "2024-09-24", "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering", "paper_summary": "Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.", "paper_summary_zh": "\u4f4e\u79e9\u9069\u61c9 (LoRA) \u5df2\u6210\u70ba\u4e00\u7a2e\u5ee3\u53d7\u6b61\u8fce\u7684\u6280\u8853\uff0c\u7528\u65bc\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u9069\u61c9\u5404\u7a2e\u9818\u57df\uff0c\u9019\u662f\u56e0\u70ba\u5b83\u5177\u6709\u6a21\u7d44\u5316\u8a2d\u8a08\uff0c\u4e14\u5728 Huggingface \u7b49\u5e73\u53f0\u4e0a\u5ee3\u6cdb\u53ef\u7528\u3002\u9019\u7a2e\u6a21\u7d44\u5316\u5f15\u8d77\u4e86\u4eba\u5011\u5c0d\u7d50\u5408\u591a\u500b LoRA \u4ee5\u589e\u5f37 LLM \u80fd\u529b\u7684\u8208\u8da3\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 LoRA \u7d44\u6210\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u65bc\u9700\u8981\u984d\u5916\u8a13\u7df4\u7684\u7279\u5b9a\u4efb\u52d9\u9069\u61c9\uff0c\u800c\u7576\u524d\u7684\u6a21\u578b\u5408\u4f75\u6280\u8853\u901a\u5e38\u7121\u6cd5\u5145\u5206\u5229\u7528 LoRA \u7684\u6a21\u7d44\u5316\u7279\u6027\uff0c\u5c0e\u81f4\u53c3\u6578\u5e72\u64fe\u548c\u6548\u80fd\u4e0b\u964d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4ee5\u66f4\u7cbe\u7d30\u7684\u7c92\u5ea6\u5206\u89e3\u548c\u91cd\u65b0\u7d44\u88dd\u591a\u500b LoRA \u7684\u53ef\u884c\u6027\uff0c\u985e\u4f3c\u65bc\u7d44\u88dd\u6a02\u9ad8\u7a4d\u6728\u3002\u6211\u5011\u5f15\u5165\u4e86\u6700\u5c0f\u8a9e\u7fa9\u55ae\u5143 (MSU) \u7684\u6982\u5ff5\uff0c\u5176\u4e2d\u5c0d\u61c9\u65bc LoRA \u4e2d\u6bcf\u500b\u79e9\u7684\u53c3\u6578\u4f5c\u70ba\u7368\u7acb\u55ae\u5143\u904b\u4f5c\u3002\u9019\u4e9b MSU \u5c55\u793a\u4e86\u6392\u5217\u4e0d\u8b8a\u6027\u548c\u4e32\u63a5\u52a0\u7e3d\u7b49\u50f9\u6027\uff0c\u5141\u8a31\u9748\u6d3b\u7d44\u5408\u4ee5\u5275\u5efa\u65b0\u7684 LoRA\u3002\u57fa\u65bc\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LoRA-LEGO \u6846\u67b6\u3002\u6b64\u6846\u67b6\u901a\u904e\u5c07\u4f86\u81ea\u4e0d\u540c LoRA \u7684 MSU \u5206\u7d44\u5230 $k$ \u500b\u53e2\u96c6\u4e2d\uff0c\u57f7\u884c\u79e9\u7d1a\u53c3\u6578\u5206\u7fa4\u3002\u6bcf\u500b\u53e2\u96c6\u7684\u8cea\u5fc3\u4f5c\u70ba\u4e00\u500b\u5177\u4ee3\u8868\u6027\u7684 MSU\uff0c\u5141\u8a31\u7d44\u88dd\u4e00\u500b\u79e9\u8abf\u6574\u70ba $k$ \u7684\u5408\u4f75 LoRA\u3002\u6b64\u5916\uff0c\u6211\u5011\u61c9\u7528\u96d9\u91cd\u91cd\u65b0\u52a0\u6b0a\u7b56\u7565\u4f86\u6700\u4f73\u5316\u5408\u4f75 LoRA \u7684\u898f\u6a21\u3002\u5728\u5404\u7a2e\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7684\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728 LoRA \u5408\u4f75\u4e2d\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002", "author": "Ziyu Zhao et.al.", "authors": "Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, Fei Wu", "id": "2409.16167v1", "paper_url": "http://arxiv.org/abs/2409.16167v1", "repo": "null"}}