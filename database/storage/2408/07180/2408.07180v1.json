{"2408.07180": {"publish_time": "2024-08-13", "title": "Unlocking Efficiency: Adaptive Masking for Gene Transformer Models", "paper_summary": "Gene transformer models such as Nucleotide Transformer, DNABert, and LOGO are\ntrained to learn optimal gene sequence representations by using the Masked\nLanguage Modeling (MLM) training objective over the complete Human Reference\nGenome. However, the typical tokenization methods employ a basic sliding window\nof tokens, such as k-mers, that fail to utilize gene-centric semantics. This\ncould result in the (trivial) masking of easily predictable sequences, leading\nto inefficient MLM training. Time-variant training strategies are known to\nimprove pretraining efficiency in both language and vision tasks. In this work,\nwe focus on using curriculum masking where we systematically increase the\ndifficulty of masked token prediction task by using a Pointwise Mutual\nInformation-based difficulty criterion, as gene sequences lack well-defined\nsemantic units similar to words or sentences of NLP domain. Our proposed\nCurriculum Masking-based Gene Masking Strategy (CM-GEMS) demonstrates superior\nrepresentation learning capabilities compared to baseline masking approaches\nwhen evaluated on downstream gene sequence classification tasks. We perform\nextensive evaluation in both few-shot (five datasets) and full dataset settings\n(Genomic Understanding Evaluation benchmark consisting of 27 tasks). Our\nfindings reveal that CM-GEMS outperforms state-of-the-art models (DNABert-2,\nNucleotide transformer, DNABert) trained at 120K steps, achieving similar\nresults in just 10K and 1K steps. We also demonstrate that Curriculum-Learned\nLOGO (a 2-layer DNABert-like model) can achieve nearly 90% of the\nstate-of-the-art model performance of 120K steps. We will make the models and\ncodes publicly available at https://github.com/roysoumya/curriculum-GeneMask.", "paper_summary_zh": "\u57fa\u56e0\u8f49\u63db\u5668\u6a21\u578b\uff0c\u4f8b\u5982 Nucleotide Transformer\u3001DNABert \u548c LOGO\uff0c\n\u900f\u904e\u4f7f\u7528\u5b8c\u6574\u7684\u300c\u4eba\u985e\u53c3\u8003\u57fa\u56e0\u7d44\u300d\u4e0a\u7684\u906e\u853d\u8a9e\u8a00\u6a21\u578b (MLM) \u8a13\u7df4\u76ee\u6a19\uff0c\u4f86\u5b78\u7fd2\u6700\u4f73\u57fa\u56e0\u5e8f\u5217\u8868\u793a\u3002\u7136\u800c\uff0c\u5178\u578b\u7684\u6a19\u8a18\u5316\u65b9\u6cd5\u63a1\u7528\u57fa\u672c\u6a19\u8a18\u6ed1\u52d5\u8996\u7a97\uff0c\u4f8b\u5982 k-mers\uff0c\u7121\u6cd5\u5229\u7528\u4ee5\u57fa\u56e0\u70ba\u4e2d\u5fc3\u7684\u8a9e\u7fa9\u3002\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\uff08\u5e73\u51e1\u7684\uff09\u5c0d\u5bb9\u6613\u9810\u6e2c\u7684\u5e8f\u5217\u9032\u884c\u906e\u853d\uff0c\u9032\u800c\u5c0e\u81f4\u4f4e\u6548\u7684 MLM \u8a13\u7df4\u3002\u5df2\u77e5\u6642\u8b8a\u8a13\u7df4\u7b56\u7565\u53ef\u4ee5\u540c\u6642\u6539\u5584\u8a9e\u8a00\u548c\u8996\u89ba\u4efb\u52d9\u4e2d\u7684\u9810\u8a13\u7df4\u6548\u7387\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u4f7f\u7528\u8ab2\u7a0b\u906e\u853d\uff0c\u5176\u4e2d\u6211\u5011\u900f\u904e\u4f7f\u7528\u57fa\u65bc\u9ede\u5c0d\u4e92\u4fe1\u606f\u96e3\u5ea6\u7684\u6a19\u6e96\uff0c\u7cfb\u7d71\u6027\u5730\u589e\u52a0\u906e\u853d\u6a19\u8a18\u9810\u6e2c\u4efb\u52d9\u7684\u96e3\u5ea6\uff0c\u56e0\u70ba\u57fa\u56e0\u5e8f\u5217\u7f3a\u4e4f\u985e\u4f3c\u65bc NLP \u9818\u57df\u4e2d\u55ae\u5b57\u6216\u53e5\u5b50\u7684\u660e\u78ba\u8a9e\u7fa9\u55ae\u4f4d\u3002\u6211\u5011\u63d0\u51fa\u7684\u57fa\u65bc\u8ab2\u7a0b\u906e\u853d\u7684\u57fa\u56e0\u906e\u853d\u7b56\u7565 (CM-GEMS) \u5728\u8a55\u4f30\u4e0b\u6e38\u57fa\u56e0\u5e8f\u5217\u5206\u985e\u4efb\u52d9\u6642\uff0c\u5c55\u73fe\u51fa\u512a\u65bc\u57fa\u7dda\u906e\u853d\u65b9\u6cd5\u7684\u5353\u8d8a\u8868\u793a\u5b78\u7fd2\u80fd\u529b\u3002\u6211\u5011\u5728\u5c11\u91cf\u6a23\u672c\uff08\u4e94\u500b\u8cc7\u6599\u96c6\uff09\u548c\u5b8c\u6574\u8cc7\u6599\u96c6\u8a2d\u5b9a\uff08\u5305\u542b 27 \u500b\u4efb\u52d9\u7684\u57fa\u56e0\u9ad4\u7406\u89e3\u8a55\u4f30\u57fa\u6e96\uff09\u4e2d\u57f7\u884c\u5ee3\u6cdb\u8a55\u4f30\u3002\u6211\u5011\u7684\u767c\u73fe\u986f\u793a\uff0cCM-GEMS \u512a\u65bc\u5728 120K \u6b65\u9a5f\u4e2d\u8a13\u7df4\u7684\u6700\u65b0\u6a21\u578b\uff08DNABert-2\u3001Nucleotide transformer\u3001DNABert\uff09\uff0c\u50c5\u5728 10K \u548c 1K \u6b65\u9a5f\u4e2d\u5c31\u9054\u6210\u985e\u4f3c\u7684\u7d50\u679c\u3002\u6211\u5011\u4e5f\u8b49\u660e\u4e86\u8ab2\u7a0b\u5b78\u7fd2 LOGO\uff08\u4e00\u500b 2 \u5c64\u7684\u985e\u4f3c DNABert \u6a21\u578b\uff09\u53ef\u4ee5\u9054\u6210\u63a5\u8fd1 120K \u6b65\u9a5f\u6700\u65b0\u6a21\u578b\u6548\u80fd\u7684 90%\u3002\u6211\u5011\u5c07\u5728 https://github.com/roysoumya/curriculum-GeneMask \u516c\u958b\u6a21\u578b\u548c\u7a0b\u5f0f\u78bc\u3002", "author": "Soumyadeep Roy et.al.", "authors": "Soumyadeep Roy, Shamik Sural, Niloy Ganguly", "id": "2408.07180v1", "paper_url": "http://arxiv.org/abs/2408.07180v1", "repo": "https://github.com/roysoumya/curriculum-genemask"}}