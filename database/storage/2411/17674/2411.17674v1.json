{"2411.17674": {"publish_time": "2024-11-26", "title": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting", "paper_summary": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%.", "paper_summary_zh": "\u7406\u89e3\u5c0d\u8a71\u4e2d\u7684\u60c5\u7dd2\u901a\u5e38\u9700\u8981\u5916\u90e8\u77e5\u8b58\u624d\u80fd\u6e96\u78ba\u7406\u89e3\u5167\u5bb9\u3002\u96a8\u8457 LLM \u8b8a\u5f97\u8d8a\u4f86\u8d8a\u5f37\u5927\uff0c\u6211\u5011\u4e0d\u5e0c\u671b\u6eff\u8db3\u65bc\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\u7684\u6709\u9650\u80fd\u529b\u3002\u7136\u800c\uff0cLLM \u8981\u561b\u53ea\u80fd\u8655\u7406\u6587\u5b57\u6a21\u5f0f\uff0c\u8981\u561b\u8655\u7406\u591a\u5a92\u9ad4\u8cc7\u8a0a\u7684\u6210\u672c\u592a\u9ad8\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\u5229\u7528 LLM \u7684\u5f37\u5927\u529f\u80fd\u548c\u591a\u5a92\u9ad4\u6a21\u5f0f\u7684\u88dc\u5145\u529f\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u6846\u67b6 Lantern\uff0c\u5b83\u53ef\u4ee5\u901a\u904e\u63d0\u793a\u5177\u6709\u611f\u53d7\u91ce\u611f\u77e5\u6ce8\u610f\u6b0a\u91cd\u7684 LLM \u4f86\u63d0\u5347\u7279\u5b9a\u9999\u8349\u6a21\u578b\u7684\u6027\u80fd\u3002\u6b64\u6846\u67b6\u8a13\u7df4\u4e86\u4e00\u500b\u591a\u4efb\u52d9\u9999\u8349\u6a21\u578b\u4f86\u7522\u751f\u60c5\u7dd2\u985e\u5225\u7684\u6a5f\u7387\u548c\u7dad\u5ea6\u5206\u6578\u3002\u9019\u4e9b\u9810\u6e2c\u88ab\u8f38\u5165\u5230 LLM \u4e2d\u4f5c\u70ba\u53c3\u8003\uff0c\u4ee5\u8abf\u6574\u6bcf\u500b\u60c5\u7dd2\u985e\u5225\u7684\u9810\u6e2c\u6a5f\u7387\u53ca\u5176\u5916\u90e8\u77e5\u8b58\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u3002\u6211\u5011\u5c07\u5c0d\u8a71\u5207\u6210\u4e0d\u540c\u7684\u611f\u53d7\u91ce\uff0c\u6bcf\u500b\u6a23\u672c\u90fd\u5305\u542b\u5728\u6070\u597d t \u500b\u611f\u53d7\u91ce\u4e2d\u3002\u6700\u5f8c\uff0c\u5c07 LLM \u7684\u9810\u6e2c\u8207\u611f\u53d7\u91ce\u611f\u77e5\u6ce8\u610f\u529b\u9a45\u52d5\u52a0\u6b0a\u6a21\u7d44\u5408\u4f75\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u9999\u8349\u6a21\u578b CORECT \u548c SDT \u8207 GPT-4 \u6216 Llama-3.1-405B \u4e00\u8d77\u90e8\u7f72\u5728 Lantern \u4e2d\u3002\u5728 IEMOCAP \u4e2d\u9032\u884c\u7684 4 \u5411\u548c 6 \u5411\u8a2d\u5b9a\u7684\u5be6\u9a57\u8868\u660e\uff0cLantern \u53ef\u4ee5\u5c07\u7576\u524d\u9999\u8349\u6a21\u578b\u7684\u6027\u80fd\u986f\u8457\u63d0\u5347\u81f3 1.23% \u548c 1.80%\u3002", "author": "Liyun Zhang et.al.", "authors": "Liyun Zhang, Dian Ding, Yu Lu, Yi-Chao Chen, Guangtao Xue", "id": "2411.17674v1", "paper_url": "http://arxiv.org/abs/2411.17674v1", "repo": "null"}}