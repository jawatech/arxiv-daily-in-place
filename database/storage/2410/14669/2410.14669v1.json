{"2410.14669": {"publish_time": "2024-10-18", "title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples", "paper_summary": "Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.", "paper_summary_zh": "<paragraph>\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u6700\u8fd1\u7684\u8996\u89ba\u554f\u7b54 (VQA) \u57fa\u6e96\u6e2c\u8a66\u4e2d\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u9019\u4e9b\u57fa\u6e96\u6e2c\u8a66\u8a55\u4f30\u4e86\u8907\u96dc\u7684\u8996\u89ba\u8a9e\u8a00\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u662f\u5426\u771f\u6b63\u6709\u6548\uff1f\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 VLM \u4ecd\u96e3\u4ee5\u8655\u7406\u4eba\u985e\u53ef\u4ee5\u8f15\u9b06\u56de\u7b54\u7684\u81ea\u7136\u5f71\u50cf\u548c\u554f\u984c\uff0c\u6211\u5011\u5c07\u5176\u7a31\u70ba\u81ea\u7136\u5c0d\u6297\u6a23\u672c\u3002\u6211\u5011\u9084\u767c\u73fe\uff0c\u4f7f\u7528\u73fe\u6210\u7684\u6a21\u578b\uff08\u4f8b\u5982 CLIP \u548c ChatGPT\uff09\u5f9e\u81ea\u7136\u5f71\u50cf\u6587\u5b57\u8a9e\u6599\u5eab\u4e2d\u751f\u6210\u9019\u4e9b VQA \u6a23\u672c\u51fa\u5947\u5730\u5bb9\u6613\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u534a\u81ea\u52d5\u5316\u65b9\u6cd5\u4f86\u6536\u96c6\u4e00\u500b\u65b0\u7684\u57fa\u6e96\u6e2c\u8a66 NaturalBench\uff0c\u4ee5\u4fbf\u4f7f\u7528 10,000 \u500b\u7d93\u4eba\u985e\u9a57\u8b49\u7684 VQA \u6a23\u672c\u53ef\u9760\u5730\u8a55\u4f30 VLM\u3002\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u63a1\u7528\u4e86**\u4ee5\u8996\u89ba\u70ba\u4e2d\u5fc3**\u7684\u8a2d\u8a08\uff0c\u5c07\u6bcf\u500b\u554f\u984c\u8207\u5169\u5f35\u7522\u751f\u4e0d\u540c\u7b54\u6848\u7684\u5f71\u50cf\u914d\u5c0d\uff0c\u9632\u6b62\u76f2\u89e3\u5728\u4e0d\u4f7f\u7528\u5f71\u50cf\u7684\u60c5\u6cc1\u4e0b\u56de\u7b54\u554f\u984c\u3002\u9019\u4f7f\u5f97 NaturalBench \u6bd4\u53ef\u4ee5\u4f7f\u7528\u5e38\u8b58\u5148\u9a57\u4f86\u89e3\u6c7a\u7684\u5148\u524d\u57fa\u6e96\u6e2c\u8a66\u66f4\u5177\u6311\u6230\u6027\u3002\u6211\u5011\u5728 NaturalBench \u4e0a\u8a55\u4f30\u4e86 53 \u500b\u6700\u5148\u9032\u7684 VLM\uff0c\u7d50\u679c\u986f\u793a LLaVA-OneVision\u3001Cambrian-1\u3001Llama3.2-Vision\u3001Molmo\u3001Qwen2-VL\uff0c\u751a\u81f3 GPT-4o \u7b49\u6a21\u578b\u843d\u5f8c\u65bc\u4eba\u985e\u8868\u73fe (\u8d85\u904e 90%) 50%-70%\u3002\u6211\u5011\u5f9e\u5169\u500b\u89d2\u5ea6\u5206\u6790\u4e86 NaturalBench \u7684\u96e3\u9ede\uff1a(1) \u7d44\u5408\u6027\uff1a\u89e3\u6c7a NaturalBench \u9700\u8981\u591a\u6a23\u5316\u7684\u8996\u89ba\u8a9e\u8a00\u6280\u80fd\uff0c\u5305\u62ec\u7406\u89e3\u5c6c\u6027\u7e6b\u7d50\u3001\u7269\u4ef6\u95dc\u4fc2\u4ee5\u53ca\u9ad8\u7d1a\u63a8\u7406\uff0c\u4f8b\u5982\u908f\u8f2f\u548c\u8a08\u6578\u3002\u70ba\u6b64\uff0c\u8207\u5148\u524d\u4f7f\u7528\u6bcf\u500b\u6a23\u672c\u4e00\u500b\u6a19\u7c64\u7684\u5de5\u4f5c\u4e0d\u540c\uff0c\u6211\u5011\u4f7f\u7528 1 \u5230 8 \u500b\u6280\u80fd\u6a19\u7c64\u6a19\u8a18\u6bcf\u500b NaturalBench \u6a23\u672c\uff0c\u4ee5\u9032\u884c\u7d30\u5fae\u7684\u8a55\u4f30\u3002(2) \u504f\u5dee\uff1aNaturalBench \u63ed\u9732\u4e86 VLM \u4e2d\u56b4\u91cd\u7684\u504f\u5dee\uff0c\u56e0\u70ba\u6a21\u578b\u901a\u5e38\u6703\u9078\u64c7\u76f8\u540c\u7684\u7b54\u6848\uff0c\u800c\u4e0d\u7ba1\u5f71\u50cf\u70ba\u4f55\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c07\u57fa\u6e96\u7b56\u5c55\u65b9\u6cd5\u61c9\u7528\u65bc\u5404\u7a2e\u6578\u64da\u4f86\u6e90\uff0c\u5305\u62ec\u9577\u6a19\u984c\uff08\u8d85\u904e 100 \u500b\u5b57\uff09\u548c\u975e\u82f1\u8a9e\u8a9e\u8a00\uff08\u4f8b\u5982\u4e2d\u6587\u548c\u5370\u5730\u8a9e\uff09\uff0c\u7a81\u986f\u4e86\u5176\u5c0d VLM \u9032\u884c\u52d5\u614b\u8a55\u4f30\u7684\u6f5b\u529b\u3002</paragraph>", "author": "Baiqi Li et.al.", "authors": "Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan", "id": "2410.14669v1", "paper_url": "http://arxiv.org/abs/2410.14669v1", "repo": "null"}}