{"2410.08431": {"publish_time": "2024-10-11", "title": "oRetrieval Augmented Generation for 10 Large Language Models and its Generalizability in Assessing Medical Fitness", "paper_summary": "Large Language Models (LLMs) show potential for medical applications but\noften lack specialized clinical knowledge. Retrieval Augmented Generation (RAG)\nallows customization with domain-specific information, making it suitable for\nhealthcare. This study evaluates the accuracy, consistency, and safety of RAG\nmodels in determining fitness for surgery and providing preoperative\ninstructions. We developed LLM-RAG models using 35 local and 23 international\npreoperative guidelines and tested them against human-generated responses. A\ntotal of 3,682 responses were evaluated. Clinical documents were processed\nusing Llamaindex, and 10 LLMs, including GPT3.5, GPT4, and Claude-3, were\nassessed. Fourteen clinical scenarios were analyzed, focusing on seven aspects\nof preoperative instructions. Established guidelines and expert judgment were\nused to determine correct responses, with human-generated answers serving as\ncomparisons. The LLM-RAG models generated responses within 20 seconds,\nsignificantly faster than clinicians (10 minutes). The GPT4 LLM-RAG model\nachieved the highest accuracy (96.4% vs. 86.6%, p=0.016), with no\nhallucinations and producing correct instructions comparable to clinicians.\nResults were consistent across both local and international guidelines. This\nstudy demonstrates the potential of LLM-RAG models for preoperative healthcare\ntasks, highlighting their efficiency, scalability, and reliability.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u986f\u793a\u51fa\u5728\u91ab\u7642\u61c9\u7528\u65b9\u9762\u7684\u6f5b\u529b\uff0c\u4f46\u901a\u5e38\u7f3a\u4e4f\u5c08\u696d\u7684\u81e8\u5e8a\u77e5\u8b58\u3002\u6aa2\u7d22\u64f4\u5145\u751f\u6210 (RAG) \u5141\u8a31\u4f7f\u7528\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u8a0a\u9032\u884c\u81ea\u8a02\uff0c\u4f7f\u5176\u9069\u7528\u65bc\u91ab\u7642\u4fdd\u5065\u3002\u672c\u7814\u7a76\u8a55\u4f30 RAG \u6a21\u578b\u5728\u78ba\u5b9a\u624b\u8853\u9069\u61c9\u75c7\u548c\u63d0\u4f9b\u8853\u524d\u8aaa\u660e\u65b9\u9762\u7684\u6e96\u78ba\u6027\u3001\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u3002\u6211\u5011\u4f7f\u7528 35 \u4efd\u7576\u5730\u548c 23 \u4efd\u570b\u969b\u8853\u524d\u6307\u5357\u958b\u767c\u4e86 LLM-RAG \u6a21\u578b\uff0c\u4e26\u5c07\u5b83\u5011\u8207\u4eba\u70ba\u7522\u751f\u7684\u56de\u61c9\u9032\u884c\u4e86\u6e2c\u8a66\u3002\u7e3d\u5171\u8a55\u4f30\u4e86 3,682 \u4efd\u56de\u61c9\u3002\u81e8\u5e8a\u6587\u4ef6\u4f7f\u7528 Llamaindex \u8655\u7406\uff0c\u4e26\u8a55\u4f30\u4e86 10 \u500b LLM\uff0c\u5305\u62ec GPT3.5\u3001GPT4 \u548c Claude-3\u3002\u5206\u6790\u4e86 14 \u500b\u81e8\u5e8a\u5834\u666f\uff0c\u91cd\u9ede\u95dc\u6ce8\u8853\u524d\u8aaa\u660e\u7684\u4e03\u500b\u65b9\u9762\u3002\u4f7f\u7528\u65e2\u5b9a\u7684\u6307\u5357\u548c\u5c08\u5bb6\u5224\u65b7\u4f86\u78ba\u5b9a\u6b63\u78ba\u7684\u56de\u61c9\uff0c\u4e26\u4ee5\u4eba\u70ba\u7522\u751f\u7684\u7b54\u6848\u4f5c\u70ba\u6bd4\u8f03\u3002LLM-RAG \u6a21\u578b\u5728 20 \u79d2\u5167\u7522\u751f\u56de\u61c9\uff0c\u986f\u8457\u5feb\u65bc\u81e8\u5e8a\u91ab\u751f (10 \u5206\u9418)\u3002GPT4 LLM-RAG \u6a21\u578b\u9054\u5230\u4e86\u6700\u9ad8\u7684\u6e96\u78ba\u5ea6 (96.4% \u5c0d\u6bd4 86.6%\uff0cp=0.016)\uff0c\u6c92\u6709\u51fa\u73fe\u5e7b\u89ba\uff0c\u4e26\u7522\u751f\u4e86\u8207\u81e8\u5e8a\u91ab\u751f\u76f8\u7576\u7684\u6b63\u78ba\u8aaa\u660e\u3002\u7d50\u679c\u5728\u7576\u5730\u548c\u570b\u969b\u6307\u5357\u4e2d\u662f\u4e00\u81f4\u7684\u3002\u672c\u7814\u7a76\u5c55\u793a\u4e86 LLM-RAG \u6a21\u578b\u5728\u8853\u524d\u91ab\u7642\u4fdd\u5065\u4efb\u52d9\u4e2d\u7684\u6f5b\u529b\uff0c\u7a81\u51fa\u4e86\u5b83\u5011\u7684\u6548\u7387\u3001\u53ef\u64f4\u5145\u6027\u548c\u53ef\u9760\u6027\u3002", "author": "Yu He Ke et.al.", "authors": "Yu He Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan Liu, Alex Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, Chang-Fu Kuo, Shao-Chun Wu, Vesela P. Kovacheva, Daniel Shu Wei Ting", "id": "2410.08431v1", "paper_url": "http://arxiv.org/abs/2410.08431v1", "repo": "null"}}