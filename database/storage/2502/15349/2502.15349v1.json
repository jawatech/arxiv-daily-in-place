{"2502.15349": {"publish_time": "2025-02-21", "title": "AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms", "paper_summary": "Transformers and large language models (LLMs) have revolutionized machine\nlearning, with attention mechanisms at the core of their success. As the\nlandscape of attention variants expands, so too do the challenges of optimizing\ntheir performance, particularly across different hardware platforms. Current\noptimization strategies are often narrowly focused, requiring extensive manual\nintervention to accommodate changes in model configurations or hardware\nenvironments. In this paper, we introduce AttentionEngine, a comprehensive\nframework designed to streamline the optimization of attention mechanisms\nacross heterogeneous hardware backends. By decomposing attention computation\ninto modular operations with customizable components, AttentionEngine enables\nflexible adaptation to diverse algorithmic requirements. The framework further\nautomates kernel optimization through a combination of programmable templates\nand a robust cross-platform scheduling strategy. Empirical results reveal\nperformance gains of up to 10x on configurations beyond the reach of existing\nmethods. AttentionEngine offers a scalable, efficient foundation for developing\nand deploying attention mechanisms with minimal manual tuning. Our code has\nbeen open-sourced and is available at\nhttps://github.com/microsoft/AttentionEngine.", "paper_summary_zh": "Transformer\u548c\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u5fb9\u5e95\u9769\u65b0\u4e86\u6a5f\u5668\u5b78\u7fd2\uff0c\u800c\u6ce8\u610f\u529b\u6a5f\u5236\u662f\u5176\u6210\u529f\u7684\u6838\u5fc3\u3002\u96a8\u8457\u6ce8\u610f\u529b\u8b8a\u9ad4\u7684\u7248\u5716\u64f4\u5c55\uff0c\u512a\u5316\u5176\u6548\u80fd\u7684\u6311\u6230\u4e5f\u8d8a\u4f86\u8d8a\u591a\uff0c\u7279\u5225\u662f\u5728\u4e0d\u540c\u7684\u786c\u9ad4\u5e73\u53f0\u4e0a\u3002\u76ee\u524d\u7684\u6700\u4f73\u5316\u7b56\u7565\u901a\u5e38\u7126\u9ede\u72f9\u9698\uff0c\u9700\u8981\u5927\u91cf\u624b\u52d5\u4ecb\u5165\u624d\u80fd\u9069\u61c9\u6a21\u578b\u7d44\u614b\u6216\u786c\u9ad4\u74b0\u5883\u7684\u8b8a\u66f4\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 AttentionEngine\uff0c\u4e00\u500b\u5168\u9762\u7684\u67b6\u69cb\uff0c\u65e8\u5728\u7c21\u5316\u8de8\u7570\u8cea\u786c\u9ad4\u5f8c\u7aef\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u6700\u4f73\u5316\u3002\u900f\u904e\u5c07\u6ce8\u610f\u529b\u904b\u7b97\u5206\u89e3\u70ba\u5177\u6709\u53ef\u81ea\u8a02\u7d44\u4ef6\u7684\u6a21\u7d44\u5316\u904b\u7b97\uff0cAttentionEngine \u80fd\u5920\u9748\u6d3b\u5730\u9069\u61c9\u4e0d\u540c\u7684\u6f14\u7b97\u6cd5\u9700\u6c42\u3002\u9019\u500b\u67b6\u69cb\u9032\u4e00\u6b65\u900f\u904e\u53ef\u7a0b\u5f0f\u5316\u7bc4\u672c\u548c\u5f37\u5927\u7684\u8de8\u5e73\u53f0\u6392\u7a0b\u7b56\u7565\uff0c\u81ea\u52d5\u5316\u6838\u5fc3\u6700\u4f73\u5316\u3002\u7d93\u9a57\u7d50\u679c\u986f\u793a\uff0c\u5728\u73fe\u6709\u65b9\u6cd5\u7121\u6cd5\u9054\u5230\u7684\u7d44\u614b\u4e0a\uff0c\u6548\u80fd\u63d0\u5347\u4e86 10 \u500d\u3002AttentionEngine \u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u3001\u9ad8\u6548\u7684\u57fa\u790e\uff0c\u7528\u65bc\u958b\u767c\u548c\u90e8\u7f72\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4e14\u53ea\u9700\u6700\u5c11\u7684\u8abf\u6574\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5df2\u958b\u653e\u539f\u59cb\u78bc\uff0c\u53ef\u5728 https://github.com/microsoft/AttentionEngine \u53d6\u5f97\u3002", "author": "Feiyang Chen et.al.", "authors": "Feiyang Chen, Yu Cheng, Lei Wang, Yuqing Xia, Ziming Miao, Lingxiao Ma, Fan Yang, Jilong Xue, Zhi Yang, Mao Yang, Haibo Chen", "id": "2502.15349v1", "paper_url": "http://arxiv.org/abs/2502.15349v1", "repo": "null"}}