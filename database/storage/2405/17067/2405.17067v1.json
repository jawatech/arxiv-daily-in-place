{"2405.17067": {"publish_time": "2024-05-27", "title": "Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization", "paper_summary": "Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. To demonstrate this flaw of LLMs, we\nconstruct an adversarial dataset, named as $\\textbf{ADT (Adversarial Dataset\nfor Tokenizer)}$, which draws upon the vocabularies of various open-source LLMs\nto challenge LLMs' tokenization. ADT consists of two subsets: the manually\nconstructed ADT-Human and the automatically generated ADT-Auto. Our empirical\nresults reveal that our ADT is highly effective on challenging the tokenization\nof leading LLMs, including GPT-4o, Llama-3, Qwen2.5-max and so on, thus\ndegrading these LLMs' capabilities. Moreover, our method of automatic data\ngeneration has been proven efficient and robust, which can be applied to any\nopen-source LLMs. To the best of our knowledge, our study is the first to\ninvestigating LLMs' vulnerability in terms of challenging their token\nsegmentation, which will shed light on the subsequent research of improving\nLLMs' capabilities through optimizing their tokenization process and\nalgorithms.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u5c55\u73fe\u51fa\u9a5a\u4eba\u7684\u80fd\u529b\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u6211\u5011\u4e5f\u89c0\u5bdf\u5230 LLM \u50be\u5411\u65bc\u5c0d\u7279\u5b9a\u67e5\u8a62\u7522\u751f\u4e0d\u6e96\u78ba\u7684\u56de\u61c9\u3002\u9019\u7a2e\u7f3a\u9677\u53ef\u4ee5\u8ffd\u6eaf\u5230 LLM \u5fc5\u9808\u7d93\u6b77\u7684\u6a19\u8a18\u5316\u6b65\u9a5f\uff0c\u9019\u662f\u6240\u6709 LLM \u56fa\u6709\u7684\u4e0d\u53ef\u907f\u514d\u7684\u9650\u5236\u3002\u4e8b\u5be6\u4e0a\uff0c\u4e0d\u6b63\u78ba\u7684\u6a19\u8a18\u5316\u662f\u963b\u7919 LLM \u7cbe\u78ba\u7406\u89e3\u8f38\u5165\u7684\u95dc\u9375\u9ede\uff0c\u5f9e\u800c\u5c0e\u81f4\u4e0d\u4ee4\u4eba\u6eff\u610f\u7684\u8f38\u51fa\u3002\u70ba\u4e86\u8b49\u660e LLM \u7684\u9019\u500b\u7f3a\u9677\uff0c\u6211\u5011\u69cb\u5efa\u4e86\u4e00\u500b\u5c0d\u6297\u6027\u8cc7\u6599\u96c6\uff0c\u7a31\u70ba $\\textbf{ADT\uff08\u6a19\u8a18\u5316\u5c0d\u6297\u6027\u8cc7\u6599\u96c6\uff09}$\uff0c\u5b83\u5229\u7528\u4e86\u5404\u7a2e\u958b\u6e90 LLM \u7684\u8a5e\u5f59\u4f86\u6311\u6230 LLM \u7684\u6a19\u8a18\u5316\u3002ADT \u5305\u542b\u5169\u500b\u5b50\u96c6\uff1a\u624b\u52d5\u69cb\u5efa\u7684 ADT-Human \u548c\u81ea\u52d5\u751f\u6210\u7684 ADT-Auto\u3002\u6211\u5011\u7684\u5be6\u8b49\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684 ADT \u5728\u6311\u6230\u5305\u62ec GPT-4o\u3001Llama-3\u3001Qwen2.5-max \u7b49\u9818\u5148 LLM \u7684\u6a19\u8a18\u5316\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u5f9e\u800c\u964d\u4f4e\u4e86\u9019\u4e9b LLM \u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u81ea\u52d5\u8cc7\u6599\u751f\u6210\u65b9\u6cd5\u5df2\u88ab\u8b49\u660e\u662f\u9ad8\u6548\u4e14\u5f37\u5927\u7684\uff0c\u53ef\u4ee5\u61c9\u7528\u65bc\u4efb\u4f55\u958b\u6e90 LLM\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u6211\u5011\u7684\u7814\u7a76\u662f\u7b2c\u4e00\u500b\u5728\u6311\u6230 LLM \u7684\u6a19\u8a18\u5206\u5272\u65b9\u9762\u7684\u6f0f\u6d1e\u9032\u884c\u8abf\u67e5\u7684\u7814\u7a76\uff0c\u9019\u5c07\u70ba\u901a\u904e\u512a\u5316\u5176\u6a19\u8a18\u5316\u904e\u7a0b\u548c\u6f14\u7b97\u6cd5\u4f86\u6539\u9032 LLM \u80fd\u529b\u7684\u5f8c\u7e8c\u7814\u7a76\u63d0\u4f9b\u6307\u5f15\u3002", "author": "Dixuan Wang et.al.", "authors": "Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Guochao Jiang, Jiaqing Liang, Deqing Yang", "id": "2405.17067v1", "paper_url": "http://arxiv.org/abs/2405.17067v1", "repo": "null"}}