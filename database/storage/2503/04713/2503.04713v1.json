{"2503.04713": {"publish_time": "2025-03-06", "title": "Scaling Rich Style-Prompted Text-to-Speech Datasets", "paper_summary": "We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale\ndataset that annotates speech utterances with rich style captions. While rich\nabstract tags (e.g. guttural, nasal, pained) have been explored in small-scale\nhuman-annotated datasets, existing large-scale datasets only cover basic tags\n(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech\nembedders, classifiers and an audio language model to automatically scale rich\ntag annotations for the first time. ParaSpeechCaps covers a total of 59 style\ntags, including both speaker-level intrinsic tags and utterance-level\nsituational tags. It consists of 342 hours of human-labelled data (PSC-Base)\nand 2427 hours of automatically annotated data (PSC-Scaled). We finetune\nParler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and\nachieve improved style consistency (+7.9% Consistency MOS) and speech quality\n(+15.5% Naturalness MOS) over the best performing baseline that combines\nexisting rich style tag datasets. We ablate several of our dataset design\nchoices to lay the foundation for future work in this space. Our dataset,\nmodels and code are released at https://github.com/ajd12342/paraspeechcaps .", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39\u4e86Paralinguistic Speech Captions (ParaSpeechCaps)\uff0c\u9019\u662f\u4e00\u500b\u5e36\u6709\u8c50\u5bcc\u98a8\u683c\u6a19\u8a18\u7684\u8a9e\u97f3\u8a9e\u6599\u7684\u5927\u578b\u8cc7\u6599\u96c6\u3002\u96d6\u7136\u5728\u5c0f\u578b\u4eba\u5de5\u6a19\u8a18\u8cc7\u6599\u96c6\u4e2d\u5df2\u7d93\u63a2\u7d22\u4e86\u8c50\u5bcc\u7684\u62bd\u8c61\u6a19\u7c64\uff08\u4f8b\u5982\uff0c\u5589\u97f3\u3001\u9f3b\u97f3\u3001\u75db\u82e6\uff09\uff0c\u4f46\u73fe\u6709\u7684\u5927\u578b\u8cc7\u6599\u96c6\u50c5\u6db5\u84cb\u57fa\u672c\u6a19\u7c64\uff08\u4f8b\u5982\uff0c\u4f4e\u6c89\u3001\u7de9\u6162\u3001\u97ff\u4eae\uff09\u3002\u6211\u5011\u7d50\u5408\u4e86\u73fe\u6210\u7684\u6587\u672c\u548c\u8a9e\u97f3\u5d4c\u5165\u5668\u3001\u5206\u985e\u5668\u548c\u97f3\u8a0a\u8a9e\u8a00\u6a21\u578b\uff0c\u9996\u6b21\u81ea\u52d5\u64f4\u5c55\u8c50\u5bcc\u7684\u6a19\u7c64\u8a3b\u91cb\u3002ParaSpeechCaps \u5171\u6db5\u84cb 59 \u7a2e\u98a8\u683c\u6a19\u7c64\uff0c\u5305\u62ec\u8aaa\u8a71\u8005\u5c64\u9762\u7684\u5167\u5728\u6a19\u7c64\u548c\u8a9e\u53e5\u5c64\u9762\u7684\u60c5\u5883\u6a19\u7c64\u3002\u5b83\u5305\u542b 342 \u5c0f\u6642\u7684\u4eba\u5de5\u6a19\u8a18\u6578\u64da (PSC-Base) \u548c 2427 \u5c0f\u6642\u7684\u81ea\u52d5\u6a19\u8a18\u6578\u64da (PSC-Scaled)\u3002\u6211\u5011\u5728 ParaSpeechCaps \u4e0a\u5fae\u8abf\u4e86 Parler-TTS\uff0c\u9019\u662f\u4e00\u500b\u958b\u6e90\u7684\u98a8\u683c\u63d0\u793a TTS \u6a21\u578b\uff0c\u8207\u7d50\u5408\u73fe\u6709\u8c50\u5bcc\u98a8\u683c\u6a19\u7c64\u8cc7\u6599\u96c6\u7684\u6700\u4f73\u6027\u80fd\u57fa\u6e96\u76f8\u6bd4\uff0c\u5be6\u73fe\u4e86\u66f4\u9ad8\u7684\u98a8\u683c\u4e00\u81f4\u6027\uff08+7.9% \u4e00\u81f4\u6027 MOS\uff09\u548c\u8a9e\u97f3\u8cea\u91cf\uff08+15.5% \u81ea\u7136\u5ea6 MOS\uff09\u3002\u6211\u5011\u522a\u9664\u4e86\u6211\u5011\u8cc7\u6599\u96c6\u8a2d\u8a08\u4e2d\u7684\u5e7e\u500b\u9078\u9805\uff0c\u70ba\u8a72\u9818\u57df\u7684\u672a\u4f86\u5de5\u4f5c\u5960\u5b9a\u4e86\u57fa\u790e\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u3001\u6a21\u578b\u548c\u7a0b\u5f0f\u78bc\u5df2\u767c\u4f48\u5728 https://github.com/ajd12342/paraspeechcaps\u3002\n", "author": "Anuj Diwan et.al.", "authors": "Anuj Diwan, Zhisheng Zheng, David Harwath, Eunsol Choi", "id": "2503.04713v1", "paper_url": "http://arxiv.org/abs/2503.04713v1", "repo": "null"}}