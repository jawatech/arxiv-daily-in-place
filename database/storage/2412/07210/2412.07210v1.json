{"2412.07210": {"publish_time": "2024-12-10", "title": "EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models", "paper_summary": "Distributed training methods are crucial for large language models (LLMs).\nHowever, existing distributed training methods often suffer from communication\nbottlenecks, stragglers, and limited elasticity. Local SGD methods have been\nproposed to address these issues, but their effectiveness remains limited to\nsmall-scale training due to additional memory overhead and lack of concerns on\nefficiency and stability. To tackle these issues, we propose EDiT, an\ninnovative Efficient Distributed Training method that combines a tailored Local\nSGD approach with model sharding techniques to enhance large-scale training\nefficiency. EDiT performs layer-wise parameter synchronization during forward\npass, reducing communication and memory overhead and enabling the overlap of\ncomputation and communication. Besides, EDiT employs a pseudo gradient penalty\nstrategy to suppress loss spikes, which ensures training stability and improve\nperformance. Additionally, we introduce A-EDiT, a fully asynchronous variant of\nEDiT that accommodates heterogeneous clusters. Building on EDiT/A-EDiT, we\nconduct a series of experiments to validate large-scale asynchronous training\nfor LLMs, accompanied by comprehensive analyses. Experimental results\ndemonstrate the superior performance of EDiT/A-EDiT, establishing them as\nrobust solutions for distributed LLM training in diverse computational\necosystems.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5206\u6563\u8a13\u7df4\u65b9\u6cd5\u81f3\u95dc\u91cd\u8981\u3002\n\u7136\u800c\uff0c\u73fe\u6709\u7684\u5206\u6563\u8a13\u7df4\u65b9\u6cd5\u5e38\u5e38\u6703\u9047\u5230\u901a\u8a0a\u74f6\u9838\u3001\u843d\u5f8c\u8005\u548c\u5f48\u6027\u53d7\u9650\u7684\u554f\u984c\u3002\u5df2\u7d93\u63d0\u51fa\u4f7f\u7528\u5c40\u90e8 SGD \u65b9\u6cd5\u4f86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u4f46\u7531\u65bc\u984d\u5916\u7684\u8a18\u61b6\u9ad4\u8ca0\u64d4\u4ee5\u53ca\u5c0d\u6548\u7387\u548c\u7a69\u5b9a\u6027\u7684\u95dc\u6ce8\u4e0d\u8db3\uff0c\u5176\u6709\u6548\u6027\u4ecd\u7136\u50c5\u9650\u65bc\u5c0f\u898f\u6a21\u8a13\u7df4\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 EDiT\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u9ad8\u6548\u5206\u6563\u8a13\u7df4\u65b9\u6cd5\uff0c\u5b83\u7d50\u5408\u4e86\u91cf\u8eab\u6253\u9020\u7684\u5c40\u90e8 SGD \u65b9\u6cd5\u548c\u6a21\u578b\u5206\u7247\u6280\u8853\u4f86\u63d0\u5347\u5927\u898f\u6a21\u8a13\u7df4\u7684\u6548\u7387\u3002EDiT \u5728\u524d\u5411\u50b3\u905e\u671f\u9593\u57f7\u884c\u9010\u5c64\u53c3\u6578\u540c\u6b65\uff0c\u6e1b\u5c11\u901a\u8a0a\u548c\u8a18\u61b6\u9ad4\u8ca0\u64d4\uff0c\u4e26\u4f7f\u904b\u7b97\u548c\u901a\u8a0a\u91cd\u758a\u3002\u6b64\u5916\uff0cEDiT \u63a1\u7528\u507d\u68af\u5ea6\u7f70\u5247\u7b56\u7565\u4f86\u6291\u5236\u640d\u5931\u5c16\u5cf0\uff0c\u9019\u78ba\u4fdd\u4e86\u8a13\u7df4\u7684\u7a69\u5b9a\u6027\u4e26\u63d0\u5347\u4e86\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86 A-EDiT\uff0c\u9019\u662f\u4e00\u7a2e EDiT \u7684\u5b8c\u5168\u975e\u540c\u6b65\u8b8a\u9ad4\uff0c\u53ef\u9069\u61c9\u7570\u8cea\u7fa4\u96c6\u3002\u5efa\u7acb\u5728 EDiT/A-EDiT \u7684\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u9032\u884c\u4e86\u4e00\u7cfb\u5217\u5be6\u9a57\uff0c\u4ee5\u9a57\u8b49\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u5927\u898f\u6a21\u975e\u540c\u6b65\u8a13\u7df4\uff0c\u4e26\u9644\u4e0a\u5168\u9762\u7684\u5206\u6790\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86 EDiT/A-EDiT \u7684\u512a\u7570\u6548\u80fd\uff0c\u78ba\u7acb\u4e86\u5b83\u5011\u4f5c\u70ba\u5206\u6563\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u8a13\u7df4\u5728\u5404\u7a2e\u904b\u7b97\u751f\u614b\u7cfb\u7d71\u4e2d\u7a69\u5065\u89e3\u6c7a\u65b9\u6848\u7684\u5730\u4f4d\u3002", "author": "Jialiang Cheng et.al.", "authors": "Jialiang Cheng, Ning Gao, Yun Yue, Zhiling Ye, Jiadi Jiang, Jian Sha", "id": "2412.07210v1", "paper_url": "http://arxiv.org/abs/2412.07210v1", "repo": "null"}}