{"2411.09688": {"publish_time": "2024-11-14", "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference", "paper_summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.", "paper_summary_zh": "\u65b0\u8208\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u61c9\u7528\u7a0b\u5f0f\u9700\u8981\u9577\u7684\u8f38\u5165\u63d0\u793a\uff0c\u624d\u80fd\u57f7\u884c\u8907\u96dc\u7684\u4e0b\u6e38\u4efb\u52d9\uff0c\u4f8b\u5982\u6587\u4ef6\u5206\u6790\u548c\u7a0b\u5f0f\u78bc\u7522\u751f\u3002\u5c0d\u65bc\u9019\u4e9b\u9577\u8108\u7d61\u9577\u5ea6\u7684\u61c9\u7528\u7a0b\u5f0f\u4f86\u8aaa\uff0c\u8f38\u5165\u63d0\u793a\u7684\u9577\u5ea6\u5728\u63a8\u8ad6\u6548\u7387\u65b9\u9762\u69cb\u6210\u91cd\u5927\u6311\u6230\uff0c\u56e0\u70ba\u63a8\u8ad6\u6210\u672c\u6703\u96a8\u8457\u5e8f\u5217\u9577\u5ea6\u7dda\u6027\u589e\u52a0\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u9019\u4e9b\u61c9\u7528\u7a0b\u5f0f\u4e2d\u7684\u8a31\u591a\u61c9\u7528\u7a0b\u5f0f\uff0c\u63d0\u793a\u4e2d\u7684\u5927\u90e8\u5206\u8108\u7d61\u5728\u4e0d\u540c\u7684\u4f7f\u7528\u8005\u8f38\u5165\u4e2d\u90fd\u662f\u56fa\u5b9a\u7684\uff0c\u56e0\u6b64\u63d0\u4f9b\u4e86\u57f7\u884c\u96e2\u7dda\u6700\u4f73\u5316\u4ee5\u5feb\u901f\u8655\u7406\u4f7f\u7528\u8005\u8f38\u5165\u7684\u6a5f\u6703\uff0c\u56e0\u70ba\u5b83\u5011\u5df2\u88ab\u63a5\u6536\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528 Squeezed Attention \u4f5c\u70ba\u4e00\u7a2e\u6a5f\u5236\uff0c\u4ee5\u52a0\u901f LLM \u61c9\u7528\u7a0b\u5f0f\uff0c\u5176\u4e2d\u8f38\u5165\u63d0\u793a\u7684\u5927\u90e8\u5206\u662f\u56fa\u5b9a\u7684\u3002\u6211\u5011\u9996\u5148\u5229\u7528 K \u5e73\u5747\u7fa4\u96c6\u5728\u96e2\u7dda\u6a21\u5f0f\u4e0b\u6839\u64da\u8a9e\u7fa9\u76f8\u4f3c\u6027\u5c0d\u56fa\u5b9a\u8108\u7d61\u7684\u9375\u9032\u884c\u5206\u7d44\uff0c\u4e26\u4f7f\u7528\u55ae\u4e00\u8cea\u5fc3\u503c\u8868\u793a\u6bcf\u500b\u7fa4\u96c6\u3002\u5728\u63a8\u8ad6\u671f\u9593\uff0c\u6211\u5011\u5c07\u4f7f\u7528\u8005\u8f38\u5165\u4e2d\u7684\u67e5\u8a62\u4ee3\u5e63\u8207\u8cea\u5fc3\u9032\u884c\u6bd4\u8f03\uff0c\u4ee5\u9810\u6e2c\u56fa\u5b9a\u8108\u7d61\u4e2d\u7684\u54ea\u4e9b\u9375\u5728\u8a9e\u7fa9\u4e0a\u76f8\u95dc\uff0c\u4e26\u4e14\u9700\u8981\u5728\u63a8\u8ad6\u671f\u9593\u8f09\u5165\u3002\u7136\u5f8c\uff0c\u6211\u5011\u50c5\u4f7f\u7528\u56fa\u5b9a\u8108\u7d61\u4e2d\u7684\u9019\u4e9b\u91cd\u8981\u9375\u8a08\u7b97\u78ba\u5207\u7684\u6ce8\u610f\u529b\uff0c\u5f9e\u800c\u6e1b\u5c11\u983b\u5bec\u548c\u904b\u7b97\u6210\u672c\u3002\u6211\u5011\u9084\u5c07\u65b9\u6cd5\u64f4\u5145\u5957\u4ef6\u70ba\u4f7f\u7528\u968e\u5c64\u8cea\u5fc3\u67e5\u8a62\u4f86\u8b58\u5225\u91cd\u8981\u9375\uff0c\u9019\u53ef\u4ee5\u5c07\u6ce8\u610f\u529b\u7684\u8907\u96dc\u5ea6\u5f9e\u7dda\u6027\u964d\u4f4e\u5230\u5c0d\u6578\uff0c\u76f8\u5c0d\u65bc\u8108\u7d61\u9577\u5ea6\u800c\u8a00\u3002\u6211\u5011\u5be6\u4f5c\u6700\u4f73\u5316\u7684 Triton \u6838\u5fc3\uff0c\u7528\u65bc\u8cea\u5fc3\u6bd4\u8f03\u548c\u5177\u6709\u91cd\u8981\u9375\u7684\u7a00\u758f FlashAttention\uff0c\u5728\u9577\u8108\u7d61\u63a8\u8ad6\u7684\u9810\u586b\u548c\u7522\u751f\u968e\u6bb5\u5be6\u73fe\u8d85\u904e 4 \u500d\u7684\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u5011\u5df2\u91dd\u5c0d\u5404\u7a2e\u9577\u8108\u7d61\u57fa\u6e96\u5ee3\u6cdb\u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\uff0c\u5305\u62ec LongBench\uff0c\u5176\u4e2d\u5728\u4e0d\u640d\u5931\u6e96\u78ba\u6027\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u4e86 KV \u5feb\u53d6\u9810\u7b97\u6e1b\u5c11 3 \u500d\uff0c\u4e26\u4e14\u5c0d\u65bc\u5404\u7a2e\u6a21\u578b\uff0c\u6e1b\u5c11\u4e86\u591a\u9054 8 \u500d\uff0c\u6e96\u78ba\u5ea6\u5dee\u8ddd\u5c0f\u65bc 0.5 \u9ede\u3002", "author": "Coleman Hooper et.al.", "authors": "Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael W. Mahoney, Kurt Keutzer, Amir Gholami", "id": "2411.09688v1", "paper_url": "http://arxiv.org/abs/2411.09688v1", "repo": "null"}}