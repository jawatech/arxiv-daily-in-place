{"2409.03662": {"publish_time": "2024-09-05", "title": "The representation landscape of few-shot learning and fine-tuning in large language models", "paper_summary": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common\nstrategies for improving the performance of modern large language models (LLMs)\non specific tasks. Despite their different natures, these strategies often lead\nto comparable performance gains. However, little is known about whether they\ninduce similar representations inside LLMs. We approach this problem by\nanalyzing the probability landscape of their hidden representations in the two\ncases. More specifically, we compare how LLMs solve the same question-answering\ntask, finding that ICL and SFT create very different internal structures, in\nboth cases undergoing a sharp transition in the middle of the network. In the\nfirst half of the network, ICL shapes interpretable representations\nhierarchically organized according to their semantic content. In contrast, the\nprobability landscape obtained with SFT is fuzzier and semantically mixed. In\nthe second half of the model, the fine-tuned representations develop\nprobability modes that better encode the identity of answers, while the\nlandscape of ICL representations is characterized by less defined peaks. Our\napproach reveals the diverse computational strategies developed inside LLMs to\nsolve the same task across different conditions, allowing us to make a step\ntowards designing optimal methods to extract information from language models.", "paper_summary_zh": "\u6587\u672c\u5185\u5b66\u4e60 (ICL) \u548c\u76d1\u7763\u5fae\u8c03 (SFT) \u662f\u4e24\u79cd\u5e38\u89c1\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u5347\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u5c3d\u7ba1\u5176\u672c\u8d28\u4e0d\u540c\uff0c\u4f46\u8fd9\u4e9b\u7b56\u7565\u901a\u5e38\u4f1a\u5bfc\u81f4\u53ef\u6bd4\u7684\u6027\u80fd\u63d0\u5347\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u5b83\u4eec\u662f\u5426\u4f1a\u5728 LLM \u5185\u8bf1\u53d1\u7c7b\u4f3c\u7684\u8868\u5f81\uff0c\u6211\u4eec\u6240\u77e5\u751a\u5c11\u3002\u6211\u4eec\u901a\u8fc7\u5206\u6790\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\u5176\u9690\u85cf\u8868\u5f81\u7684\u6982\u7387\u5206\u5e03\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86 LLM \u5982\u4f55\u89e3\u51b3\u76f8\u540c\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u53d1\u73b0 ICL \u548c SFT \u521b\u5efa\u4e86\u975e\u5e38\u4e0d\u540c\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u7f51\u7edc\u4e2d\u95f4\u90fd\u7ecf\u5386\u4e86\u6025\u5267\u7684\u8f6c\u53d8\u3002\u5728\u7f51\u7edc\u7684\u524d\u534a\u90e8\u5206\uff0cICL \u6839\u636e\u8bed\u4e49\u5185\u5bb9\u5206\u5c42\u7ec4\u7ec7\u53ef\u89e3\u91ca\u7684\u8868\u5f81\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4f7f\u7528 SFT \u83b7\u5f97\u7684\u6982\u7387\u5206\u5e03\u5219\u66f4\u52a0\u6a21\u7cca\u4e14\u8bed\u4e49\u6df7\u5408\u3002\u5728\u6a21\u578b\u7684\u540e\u534a\u90e8\u5206\uff0c\u5fae\u8c03\u540e\u7684\u8868\u5f81\u53d1\u5c55\u51fa\u6982\u7387\u6a21\u5f0f\uff0c\u66f4\u597d\u5730\u7f16\u7801\u7b54\u6848\u7684\u8eab\u4efd\uff0c\u800c ICL \u8868\u5f81\u7684\u5206\u5e03\u5219\u4ee5\u4e0d\u592a\u660e\u786e\u7684\u5cf0\u503c\u4e3a\u7279\u5f81\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u63ed\u793a\u4e86 LLM \u5185\u90e8\u5f00\u53d1\u7684\u4e0d\u540c\u8ba1\u7b97\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u76f8\u540c\u4efb\u52a1\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u671d\u7740\u8bbe\u8ba1\u4ece\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u6700\u4f18\u65b9\u6cd5\u8fc8\u51fa\u4e00\u6b65\u3002", "author": "Diego Doimo et.al.", "authors": "Diego Doimo, Alessandro Serra, Alessio Ansuini, Alberto Cazzaniga", "id": "2409.03662v1", "paper_url": "http://arxiv.org/abs/2409.03662v1", "repo": "https://github.com/diegodoimo/geometry_icl_finetuning"}}