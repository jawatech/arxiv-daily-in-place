{"2412.07012": {"publish_time": "2024-12-09", "title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models", "paper_summary": "With the rise of multimodal applications, instruction data has become\ncritical for training multimodal language models capable of understanding\ncomplex image-based queries. Existing practices rely on powerful but costly\nlarge language models (LLMs) or multimodal language models (MLMs) to produce\ninstruction data. These are often prone to hallucinations, licensing issues and\nthe generation process is often hard to scale and interpret. In this work, we\npresent a programmatic approach that employs scene graphs as symbolic\nrepresentations of images and human-written programs to systematically\nsynthesize vision-centric instruction data. Our approach ensures the\ninterpretability and controllability of the data generation process and scales\nefficiently while maintaining factual accuracy. By implementing a suite of 24\nsingle-image, 14 multi-image instruction generators, and a scene graph\ngeneration pipeline, we build a scalable, cost-effective system: ProVision\nwhich produces diverse question-answer pairs concerning objects, attributes,\nrelations, depth, etc., for any given image. Applied to Visual Genome and\nDataComp datasets, we generate over 10 million instruction data points,\nProVision-10M, and leverage them in both pretraining and instruction tuning\nstages of MLMs. When adopted in the instruction tuning stage, our single-image\ninstruction data yields up to a 7% improvement on the 2D split and 8% on the 3D\nsplit of CVBench, along with a 3% increase in performance on QBench2,\nRealWorldQA, and MMMU. Our multi-image instruction data leads to an 8%\nimprovement on Mantis-Eval. Incorporation of our data in both pre-training and\nfine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6%\nacross 11 benchmarks.", "paper_summary_zh": "\u96a8\u8457\u591a\u6a21\u614b\u61c9\u7528\u7a0b\u5f0f\u8208\u8d77\uff0c\u6307\u793a\u8cc7\u6599\u5df2\u6210\u70ba\u8a13\u7df4\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b\u7684\u95dc\u9375\uff0c\u6b64\u985e\u6a21\u578b\u80fd\u7406\u89e3\u8907\u96dc\u7684\u5716\u50cf\u5f0f\u67e5\u8a62\u3002\u73fe\u884c\u505a\u6cd5\u4ef0\u8cf4\u5f37\u5927\u4f46\u6602\u8cb4\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6216\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b (MLM) \u4f86\u7522\u751f\u6307\u793a\u8cc7\u6599\u3002\u9019\u4e9b\u6a21\u578b\u5e38\u5bb9\u6613\u51fa\u73fe\u5e7b\u89ba\u3001\u6388\u6b0a\u554f\u984c\uff0c\u4e14\u751f\u6210\u904e\u7a0b\u901a\u5e38\u96e3\u4ee5\u64f4\u5145\u548c\u8a6e\u91cb\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7a0b\u5f0f\u5316\u65b9\u6cd5\uff0c\u63a1\u7528\u5834\u666f\u5716\u8868\u4f5c\u70ba\u5716\u50cf\u7684\u7b26\u865f\u8868\u5fb5\uff0c\u4e26\u4f7f\u7528\u4eba\u5beb\u7684\u7a0b\u5f0f\u7cfb\u7d71\u6027\u5730\u7d9c\u5408\u4ee5\u8996\u89ba\u70ba\u4e2d\u5fc3\u7684\u6307\u793a\u8cc7\u6599\u3002\u6211\u5011\u7684\u505a\u6cd5\u78ba\u4fdd\u8cc7\u6599\u7522\u751f\u904e\u7a0b\u7684\u53ef\u8a6e\u91cb\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4e26\u5728\u7dad\u6301\u4e8b\u5be6\u6e96\u78ba\u6027\u7684\u540c\u6642\u6709\u6548\u7387\u5730\u64f4\u5145\u3002\u900f\u904e\u5be6\u4f5c\u4e00\u7d44 24 \u500b\u55ae\u4e00\u5716\u50cf\u300114 \u500b\u591a\u5716\u50cf\u6307\u793a\u7522\u751f\u5668\uff0c\u4ee5\u53ca\u5834\u666f\u5716\u8868\u7522\u751f\u7ba1\u9053\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u3001\u5177\u6210\u672c\u6548\u76ca\u7684\u7cfb\u7d71\uff1aProVision\uff0c\u5b83\u80fd\u91dd\u5c0d\u4efb\u4f55\u7d66\u5b9a\u7684\u5716\u50cf\u7522\u751f\u95dc\u65bc\u7269\u4ef6\u3001\u5c6c\u6027\u3001\u95dc\u4fc2\u3001\u6df1\u5ea6\u7b49\u7684\u5404\u7a2e\u554f\u7b54\u914d\u5c0d\u3002\u61c9\u7528\u65bc Visual Genome \u548c DataComp \u8cc7\u6599\u96c6\uff0c\u6211\u5011\u7522\u751f\u8d85\u904e 1000 \u842c\u500b\u6307\u793a\u8cc7\u6599\u9ede\uff0cProVision-10M\uff0c\u4e26\u5728 MLM \u7684\u9810\u8a13\u7df4\u548c\u6307\u793a\u5fae\u8abf\u968e\u6bb5\u4e2d\u52a0\u4ee5\u5229\u7528\u3002\u7576\u63a1\u7528\u65bc\u6307\u793a\u5fae\u8abf\u968e\u6bb5\u6642\uff0c\u6211\u5011\u7684\u55ae\u4e00\u5716\u50cf\u6307\u793a\u8cc7\u6599\u5728 CVBench \u7684 2D \u5206\u5272\u4e2d\u63d0\u5347\u4e86 7%\uff0c\u5728 3D \u5206\u5272\u4e2d\u63d0\u5347\u4e86 8%\uff0c\u5728 QBench2\u3001RealWorldQA \u548c MMMU \u4e2d\u7684\u6548\u80fd\u4e5f\u63d0\u5347\u4e86 3%\u3002\u6211\u5011\u7684\u591a\u5716\u50cf\u6307\u793a\u8cc7\u6599\u5728 Mantis-Eval \u4e2d\u63d0\u5347\u4e86 8%\u3002\u5c07\u6211\u5011\u7684\u8cc7\u6599\u6574\u5408\u5230 xGen-MM-4B \u7684\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u968e\u6bb5\uff0c\u5728 11 \u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u5e73\u5747\u63d0\u5347\u4e86 1.6%\u3002", "author": "Jieyu Zhang et.al.", "authors": "Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, silvio savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, Ran Xu", "id": "2412.07012v1", "paper_url": "http://arxiv.org/abs/2412.07012v1", "repo": "https://github.com/jieyuz2/provision"}}