{"2410.13344": {"publish_time": "2024-10-17", "title": "Cerberus: Efficient Inference with Adaptive Parallel Decoding and Sequential Knowledge Enhancement", "paper_summary": "Large language models (LLMs) often face a bottleneck in inference speed due\nto their reliance on auto-regressive decoding. Recently, parallel decoding has\nshown significant promise in enhancing inference efficiency. However, we have\nidentified two key issues with existing parallel decoding frameworks: (1)\ndecoding heads fail to balance prediction accuracy and the parallelism of\nexecution, and (2) parallel decoding is not a universal solution, as it can\nbring unnecessary overheads at some challenging decoding steps. To address\nthese issues, we propose Cerberus, an adaptive parallel decoding framework\nintroduces the gating mechanism to enable the LLMs to adaptively choose\nappropriate decoding approaches at each decoding step, along with introducing a\nnew paradigm of decoding heads that introduce the sequential knowledge while\nmaintaining execution parallelism. The experiment results demonstrate that the\nCerberus can achieve up to 2.12x speed up compared to auto-regressive decoding,\nand outperforms one of the leading parallel decoding frameworks, Medusa, with a\n10% - 30% increase in acceleration and superior generation quality.", "paper_summary_zh": "\u5927\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u5e38\u6703\u56e0\u4f9d\u8cf4\u81ea\u8ff4\u6b78\u89e3\u78bc\u800c\u5c0e\u81f4\u63a8\u8ad6\u901f\u5ea6\u51fa\u73fe\u74f6\u9838\u3002\u6700\u8fd1\uff0c\u5e73\u884c\u89e3\u78bc\u5df2\u5c55\u73fe\u51fa\u986f\u8457\u7684\u63d0\u5347\u63a8\u8ad6\u6548\u7387\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u6211\u5011\u5df2\u627e\u51fa\u73fe\u6709\u7684\u5e73\u884c\u89e3\u78bc\u67b6\u69cb\u4e2d\u5b58\u5728\u7684\u5169\u500b\u4e3b\u8981\u554f\u984c\uff1a(1) \u89e3\u78bc\u5668\u7121\u6cd5\u5728\u9810\u6e2c\u6e96\u78ba\u5ea6\u548c\u57f7\u884c\u4e26\u884c\u6027\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\uff0c\u4ee5\u53ca (2) \u5e73\u884c\u89e3\u78bc\u4e26\u975e\u901a\u7528\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u56e0\u70ba\u5b83\u53ef\u80fd\u6703\u5728\u67d0\u4e9b\u5177\u6709\u6311\u6230\u6027\u7684\u89e3\u78bc\u6b65\u9a5f\u4e2d\u5e36\u4f86\u4e0d\u5fc5\u8981\u7684\u958b\u92b7\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa Cerberus\uff0c\u9019\u662f\u4e00\u500b\u81ea\u9069\u61c9\u5e73\u884c\u89e3\u78bc\u67b6\u69cb\uff0c\u5f15\u5165\u4e86\u9598\u63a7\u6a5f\u5236\uff0c\u8b93 LLM \u80fd\u5920\u5728\u6bcf\u500b\u89e3\u78bc\u6b65\u9a5f\u4e2d\u81ea\u9069\u61c9\u5730\u9078\u64c7\u9069\u7576\u7684\u89e3\u78bc\u65b9\u6cd5\uff0c\u540c\u6642\u5f15\u5165\u4e00\u7a2e\u65b0\u7684\u89e3\u78bc\u5668\u7bc4\u4f8b\uff0c\u5728\u7dad\u6301\u57f7\u884c\u4e26\u884c\u6027\u7684\u540c\u6642\uff0c\u5f15\u5165\u4e86\u9806\u5e8f\u77e5\u8b58\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u8207\u81ea\u8ff4\u6b78\u89e3\u78bc\u76f8\u6bd4\uff0cCerberus \u53ef\u4ee5\u5c07\u901f\u5ea6\u63d0\u5347\u9054 2.12 \u500d\uff0c\u4e26\u512a\u65bc\u5176\u4e2d\u4e00\u500b\u9818\u5148\u7684\u5e73\u884c\u89e3\u78bc\u67b6\u69cb Medusa\uff0c\u52a0\u901f\u63d0\u5347 10% - 30%\uff0c\u4e14\u751f\u6210\u54c1\u8cea\u66f4\u4f73\u3002", "author": "Yuxuan Liu et.al.", "authors": "Yuxuan Liu, Wenyuan Li, Laizhong Cui, Hailiang Yang", "id": "2410.13344v1", "paper_url": "http://arxiv.org/abs/2410.13344v1", "repo": "null"}}