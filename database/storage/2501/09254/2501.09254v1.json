{"2501.09254": {"publish_time": "2025-01-16", "title": "Clone-Robust AI Alignment", "paper_summary": "A key challenge in training Large Language Models (LLMs) is properly aligning\nthem with human preferences. Reinforcement Learning with Human Feedback (RLHF)\nuses pairwise comparisons from human annotators to train reward functions and\nhas emerged as a popular alignment method. However, input datasets in RLHF are\nnot necessarily balanced in the types of questions and answers that are\nincluded. Therefore, we want RLHF algorithms to perform well even when the set\nof alternatives is not uniformly distributed. Drawing on insights from social\nchoice theory, we introduce robustness to approximate clones, a desirable\nproperty of RLHF algorithms which requires that adding near-duplicate\nalternatives does not significantly change the learned reward function. We\nfirst demonstrate that the standard RLHF algorithm based on regularized maximum\nlikelihood estimation (MLE) fails to satisfy this property. We then propose the\nweighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\nby weighting alternatives based on their similarity to other alternatives. This\nnew algorithm guarantees robustness to approximate clones while preserving\ndesirable theoretical properties.", "paper_summary_zh": "\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6642\u7684\u4e00\u9805\u95dc\u9375\u6311\u6230\u662f\u9069\u7576\u5730\u5c07\u5176\u8207\u4eba\u985e\u504f\u597d\u7d50\u5408\u3002\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u4f7f\u7528\u4f86\u81ea\u4eba\u985e\u8a3b\u89e3\u8005\u7684\u6210\u5c0d\u6bd4\u8f03\u4f86\u8a13\u7df4\u734e\u52f5\u51fd\u6578\uff0c\u4e26\u5df2\u6210\u70ba\u4e00\u7a2e\u6d41\u884c\u7684\u7d50\u5408\u65b9\u6cd5\u3002\u7136\u800c\uff0cRLHF \u4e2d\u7684\u8f38\u5165\u8cc7\u6599\u96c6\u5728\u6240\u5305\u542b\u7684\u554f\u7b54\u985e\u578b\u4e2d\u4e0d\u4e00\u5b9a\u5e73\u8861\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5e0c\u671b RLHF \u6f14\u7b97\u6cd5\u5373\u4f7f\u5728\u66ff\u4ee3\u65b9\u6848\u96c6\u6c92\u6709\u5747\u52fb\u5206\u4f48\u6642\u4e5f\u80fd\u8868\u73fe\u826f\u597d\u3002\u501f\u9452\u793e\u6703\u9078\u64c7\u7406\u8ad6\u7684\u898b\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5c0d\u8fd1\u4f3c\u514b\u9686\u7684\u7a69\u5065\u6027\uff0c\u9019\u662f RLHF \u6f14\u7b97\u6cd5\u7684\u4e00\u9805\u7406\u60f3\u7279\u6027\uff0c\u5b83\u8981\u6c42\u52a0\u5165\u8fd1\u4f3c\u91cd\u8907\u7684\u66ff\u4ee3\u65b9\u6848\u4e0d\u6703\u986f\u8457\u6539\u8b8a\u5df2\u5b78\u7fd2\u7684\u734e\u52f5\u51fd\u6578\u3002\u6211\u5011\u9996\u5148\u8b49\u660e\u57fa\u65bc\u6b63\u5247\u5316\u6700\u5927\u4f3c\u7136\u4f30\u8a08 (MLE) \u7684\u6a19\u6e96 RLHF \u6f14\u7b97\u6cd5\u7121\u6cd5\u6eff\u8db3\u6b64\u7279\u6027\u3002\u7136\u5f8c\u6211\u5011\u63d0\u51fa\u52a0\u6b0a MLE\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684 RLHF \u6f14\u7b97\u6cd5\uff0c\u5b83\u900f\u904e\u6839\u64da\u66ff\u4ee3\u65b9\u6848\u8207\u5176\u4ed6\u66ff\u4ee3\u65b9\u6848\u7684\u76f8\u4f3c\u6027\u5c0d\u66ff\u4ee3\u65b9\u6848\u52a0\u6b0a\u4f86\u4fee\u6539\u6a19\u6e96\u6b63\u5247\u5316 MLE\u3002\u9019\u7a2e\u65b0\u6f14\u7b97\u6cd5\u4fdd\u8b49\u4e86\u5c0d\u8fd1\u4f3c\u514b\u9686\u7684\u7a69\u5065\u6027\uff0c\u540c\u6642\u4fdd\u7559\u4e86\u7406\u60f3\u7684\u7406\u8ad6\u7279\u6027\u3002", "author": "Ariel D. Procaccia et.al.", "authors": "Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang", "id": "2501.09254v1", "paper_url": "http://arxiv.org/abs/2501.09254v1", "repo": "null"}}