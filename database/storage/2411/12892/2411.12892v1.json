{"2411.12892": {"publish_time": "2024-11-19", "title": "Selective Attention: Enhancing Transformer through Principled Context Control", "paper_summary": "The attention mechanism within the transformer architecture enables the model\nto weigh and combine tokens based on their relevance to the query. While\nself-attention has enjoyed major success, it notably treats all queries $q$ in\nthe same way by applying the mapping $V^\\top\\text{softmax}(Kq)$, where $V,K$\nare the value and key embeddings respectively. In this work, we argue that this\nuniform treatment hinders the ability to control contextual sparsity and\nrelevance. As a solution, we introduce the $\\textit{Selective Self-Attention}$\n(SSA) layer that augments the softmax nonlinearity with a principled\ntemperature scaling strategy. By controlling temperature, SSA adapts the\ncontextual sparsity of the attention map to the query embedding and its\nposition in the context window. Through theory and experiments, we demonstrate\nthat this alleviates attention dilution, aids the optimization process, and\nenhances the model's ability to control softmax spikiness of individual\nqueries. We also incorporate temperature scaling for value embeddings and show\nthat it boosts the model's ability to suppress irrelevant/noisy tokens.\nNotably, SSA is a lightweight method which introduces less than 0.5% new\nparameters through a weight-sharing strategy and can be fine-tuned on existing\nLLMs. Extensive empirical evaluations demonstrate that SSA-equipped models\nachieve a noticeable and consistent accuracy improvement on language modeling\nbenchmarks.", "paper_summary_zh": "Transformer\u67b6\u69cb\u4e2d\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u4f7f\u6a21\u578b\u80fd\u5920\u6839\u64da\u4ee3\u5e63\u8207\u67e5\u8a62\u7684\u76f8\u95dc\u6027\u4f86\u52a0\u6b0a\u4e26\u7d44\u5408\u4ee3\u5e63\u3002\u96d6\u7136\u81ea\u6211\u6ce8\u610f\u529b\u5df2\u7d93\u53d6\u5f97\u4e86\u91cd\u5927\u7684\u6210\u529f\uff0c\u4f46\u5b83\u660e\u986f\u5730\u5c0d\u6240\u6709\u67e5\u8a62 $q$ \u63a1\u7528\u76f8\u540c\u7684\u65b9\u5f0f\uff0c\u901a\u904e\u61c9\u7528\u6620\u5c04 $V^\\top\\text{softmax}(Kq)$\uff0c\u5176\u4e2d $V,K$ \u5206\u5225\u662f\u503c\u5d4c\u5165\u548c\u9375\u5d4c\u5165\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a8d\u70ba\u9019\u7a2e\u7d71\u4e00\u7684\u8655\u7406\u963b\u7919\u4e86\u63a7\u5236\u4e0a\u4e0b\u6587\u7a00\u758f\u6027\u548c\u76f8\u95dc\u6027\u7684\u80fd\u529b\u3002\u4f5c\u70ba\u89e3\u6c7a\u65b9\u6848\uff0c\u6211\u5011\u5f15\u5165\u4e86 $\\textit{\u9078\u64c7\u6027\u81ea\u6211\u6ce8\u610f\u529b}$ (SSA) \u5c64\uff0c\u8a72\u5c64\u7528\u4e00\u500b\u57fa\u65bc\u539f\u5247\u7684\u6eab\u5ea6\u7e2e\u653e\u7b56\u7565\u4f86\u64f4\u5145 softmax \u975e\u7dda\u6027\u3002\u901a\u904e\u63a7\u5236\u6eab\u5ea6\uff0cSSA \u5c07\u6ce8\u610f\u529b\u5716\u7684\u4e0a\u4e0b\u6587\u7a00\u758f\u6027\u9069\u61c9\u67e5\u8a62\u5d4c\u5165\u53ca\u5176\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u7684\u4f4d\u7f6e\u3002\u901a\u904e\u7406\u8ad6\u548c\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86\u9019\u6e1b\u8f15\u4e86\u6ce8\u610f\u529b\u7684\u7a00\u91cb\uff0c\u5e6b\u52a9\u4e86\u512a\u5316\u904e\u7a0b\uff0c\u4e26\u589e\u5f37\u4e86\u6a21\u578b\u63a7\u5236\u55ae\u500b\u67e5\u8a62\u7684 softmax \u5c16\u5cf0\u7684\u80fd\u529b\u3002\u6211\u5011\u9084\u5c07\u6eab\u5ea6\u7e2e\u653e\u6574\u5408\u5230\u503c\u5d4c\u5165\u4e2d\uff0c\u4e26\u8868\u660e\u5b83\u63d0\u5347\u4e86\u6a21\u578b\u6291\u5236\u7121\u95dc/\u566a\u8072\u4ee3\u5e63\u7684\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSSA \u662f\u4e00\u7a2e\u8f15\u91cf\u7d1a\u7684\u65b9\u6cd5\uff0c\u901a\u904e\u6b0a\u91cd\u5171\u4eab\u7b56\u7565\u5f15\u5165\u4e86\u4e0d\u5230 0.5% \u7684\u65b0\u53c3\u6578\uff0c\u4e26\u4e14\u53ef\u4ee5\u5728\u73fe\u6709\u7684 LLM \u4e0a\u9032\u884c\u5fae\u8abf\u3002\u5ee3\u6cdb\u7684\u5be6\u8b49\u8a55\u4f30\u8868\u660e\uff0c\u914d\u5099\u4e86 SSA \u7684\u6a21\u578b\u5728\u8a9e\u8a00\u5efa\u6a21\u57fa\u6e96\u4e0a\u53d6\u5f97\u4e86\u986f\u8457\u4e14\u4e00\u81f4\u7684\u6e96\u78ba\u6027\u63d0\u5347\u3002", "author": "Xuechen Zhang et.al.", "authors": "Xuechen Zhang, Xiangyu Chang, Mingchen Li, Amit Roy-Chowdhury, Jiasi Chen, Samet Oymak", "id": "2411.12892v1", "paper_url": "http://arxiv.org/abs/2411.12892v1", "repo": "null"}}