{"2411.08248": {"publish_time": "2024-11-12", "title": "Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach", "paper_summary": "Deep learning underpins most of the currently advanced natural language\nprocessing (NLP) tasks such as textual classification, neural machine\ntranslation (NMT), abstractive summarization and question-answering (QA).\nHowever, the robustness of the models, particularly QA models, against\nadversarial attacks is a critical concern that remains insufficiently explored.\nThis paper introduces QA-Attack (Question Answering Attack), a novel word-level\nadversarial strategy that fools QA models. Our attention-based attack exploits\nthe customized attention mechanism and deletion ranking strategy to identify\nand target specific words within contextual passages. It creates deceptive\ninputs by carefully choosing and substituting synonyms, preserving grammatical\nintegrity while misleading the model to produce incorrect responses. Our\napproach demonstrates versatility across various question types, particularly\nwhen dealing with extensive long textual inputs. Extensive experiments on\nmultiple benchmark datasets demonstrate that QA-Attack successfully deceives\nbaseline QA models and surpasses existing adversarial techniques regarding\nsuccess rate, semantics changes, BLEU score, fluency and grammar error rate.", "paper_summary_zh": "\u6df1\u5ea6\u5b78\u7fd2\u652f\u6490\u4e86\u76ee\u524d\u5927\u90e8\u5206\u9032\u968e\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\uff0c\u4f8b\u5982\u6587\u672c\u5206\u985e\u3001\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f (NMT)\u3001\u6458\u8981\u6458\u8981\u548c\u554f\u7b54 (QA)\u3002\u7136\u800c\uff0c\u6a21\u578b\u7684\u7a69\u5065\u6027\uff0c\u7279\u5225\u662f\u554f\u7b54\u6a21\u578b\uff0c\u5c0d\u65bc\u5c0d\u6297\u653b\u64ca\u7684\u62b5\u6297\u529b\u662f\u4e00\u500b\u91cd\u8981\u7684\u554f\u984c\uff0c\u4ecd\u7136\u6c92\u6709\u5f97\u5230\u5145\u5206\u7684\u63a2\u8a0e\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u554f\u7b54\u653b\u64ca (QA-Attack)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u8a5e\u7d1a\u5c0d\u6297\u7b56\u7565\uff0c\u53ef\u4ee5\u6b3a\u9a19\u554f\u7b54\u6a21\u578b\u3002\u6211\u5011\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u653b\u64ca\u5229\u7528\u4e86\u81ea\u8a02\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u548c\u522a\u9664\u6392\u540d\u7b56\u7565\u4f86\u8b58\u5225\u548c\u9396\u5b9a\u4e0a\u4e0b\u6587\u6bb5\u843d\u4e2d\u7684\u7279\u5b9a\u8a5e\u5f59\u3002\u5b83\u901a\u904e\u4ed4\u7d30\u9078\u64c7\u548c\u66ff\u63db\u540c\u7fa9\u8a5e\u4f86\u5275\u5efa\u5177\u6709\u6b3a\u9a19\u6027\u7684\u8f38\u5165\uff0c\u5728\u8aa4\u5c0e\u6a21\u578b\u7522\u751f\u4e0d\u6b63\u78ba\u7684\u56de\u61c9\u7684\u540c\u6642\uff0c\u4fdd\u6301\u8a9e\u6cd5\u7684\u5b8c\u6574\u6027\u3002\u6211\u5011\u7684\u505a\u6cd5\u8b49\u660e\u4e86\u5728\u5404\u7a2e\u554f\u984c\u985e\u578b\u4e2d\u5177\u6709\u591a\u6a23\u6027\uff0c\u7279\u5225\u662f\u5728\u8655\u7406\u5ee3\u6cdb\u7684\u9577\u6587\u672c\u8f38\u5165\u6642\u3002\u5728\u591a\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cQA-Attack \u6210\u529f\u5730\u6b3a\u9a19\u4e86\u57fa\u6e96\u554f\u7b54\u6a21\u578b\uff0c\u4e26\u5728\u6210\u529f\u7387\u3001\u8a9e\u7fa9\u8b8a\u5316\u3001BLEU \u5206\u6578\u3001\u6d41\u66a2\u6027\u548c\u8a9e\u6cd5\u932f\u8aa4\u7387\u65b9\u9762\u8d85\u8d8a\u4e86\u73fe\u6709\u7684\u5c0d\u6297\u6280\u8853\u3002", "author": "Jiyao Li et.al.", "authors": "Jiyao Li, Mingze Ni, Yongshun Gong, Wei Liu", "id": "2411.08248v1", "paper_url": "http://arxiv.org/abs/2411.08248v1", "repo": "https://github.com/utsjiyaoli/qa-attack"}}