{"2501.02511": {"publish_time": "2025-01-05", "title": "Can Impressions of Music be Extracted from Thumbnail Images?", "paper_summary": "In recent years, there has been a notable increase in research on machine\nlearning models for music retrieval and generation systems that are capable of\ntaking natural language sentences as inputs. However, there is a scarcity of\nlarge-scale publicly available datasets, consisting of music data and their\ncorresponding natural language descriptions known as music captions. In\nparticular, non-musical information such as suitable situations for listening\nto a track and the emotions elicited upon listening is crucial for describing\nmusic. This type of information is underrepresented in existing music caption\ndatasets due to the challenges associated with extracting it directly from\nmusic data. To address this issue, we propose a method for generating music\ncaption data that incorporates non-musical aspects inferred from music\nthumbnail images, and validated the effectiveness of our approach through human\nevaluations. Additionally, we created a dataset with approximately 360,000\ncaptions containing non-musical aspects. Leveraging this dataset, we trained a\nmusic retrieval model and demonstrated its effectiveness in music retrieval\ntasks through evaluation.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u91dd\u5c0d\u53ef\u5c07\u81ea\u7136\u8a9e\u8a00\u53e5\u5b50\u4f5c\u70ba\u8f38\u5165\u7684\u97f3\u6a02\u6aa2\u7d22\u548c\u751f\u6210\u7cfb\u7d71\u7684\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u7684\u7814\u7a76\u986f\u8457\u589e\u52a0\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u5305\u542b\u97f3\u6a02\u8cc7\u6599\u53ca\u5176\u5c0d\u61c9\u7684\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\uff08\u7a31\u70ba\u97f3\u6a02\u6a19\u984c\uff09\u7684\u5927\u898f\u6a21\u516c\u958b\u53ef\u7528\u8cc7\u6599\u96c6\u3002\u7279\u5225\u662f\uff0c\u975e\u97f3\u6a02\u8cc7\u8a0a\uff0c\u4f8b\u5982\u9069\u5408\u8046\u807d\u66f2\u76ee\u548c\u8046\u807d\u6642\u5f15\u767c\u7684\u60c5\u7dd2\uff0c\u5c0d\u65bc\u63cf\u8ff0\u97f3\u6a02\u81f3\u95dc\u91cd\u8981\u3002\u7531\u65bc\u5f9e\u97f3\u6a02\u8cc7\u6599\u4e2d\u76f4\u63a5\u64f7\u53d6\u6b64\u985e\u8cc7\u8a0a\u7684\u6311\u6230\uff0c\u6b64\u985e\u8cc7\u8a0a\u5728\u73fe\u6709\u7684\u97f3\u6a02\u6a19\u984c\u8cc7\u6599\u96c6\u4e2d\u5448\u73fe\u4e0d\u8db3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u751f\u6210\u97f3\u6a02\u6a19\u984c\u8cc7\u6599\u7684\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u7d0d\u5165\u4e86\u5f9e\u97f3\u6a02\u7e2e\u5716\u5f71\u50cf\u63a8\u65b7\u7684\u975e\u97f3\u6a02\u9762\u5411\uff0c\u4e26\u900f\u904e\u4eba\u70ba\u8a55\u4f30\u9a57\u8b49\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u4e00\u500b\u5305\u542b\u7d04 360,000 \u500b\u5305\u542b\u975e\u97f3\u6a02\u9762\u5411\u7684\u6a19\u984c\u7684\u8cc7\u6599\u96c6\u3002\u5229\u7528\u9019\u500b\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b\u97f3\u6a02\u6aa2\u7d22\u6a21\u578b\uff0c\u4e26\u900f\u904e\u8a55\u4f30\u8b49\u660e\u4e86\u5b83\u5728\u97f3\u6a02\u6aa2\u7d22\u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\u3002", "author": "Takashi Harada et.al.", "authors": "Takashi Harada, Takehiro Motomitsu, Katsuhiko Hayashi, Yusuke Sakai, Hidetaka Kamigaito", "id": "2501.02511v1", "paper_url": "http://arxiv.org/abs/2501.02511v1", "repo": "null"}}