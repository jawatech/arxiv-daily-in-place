{"2411.16003": {"publish_time": "2024-11-24", "title": "eFedLLM: Efficient LLM Inference Based on Federated Learning", "paper_summary": "Large Language Models (LLMs) herald a transformative era in artificial\nintelligence (AI). However, the expansive scale of data and parameters of LLMs\nrequires high-demand computational and memory resources, restricting their\naccessibility to a broader range of users and researchers. This paper\nintroduces an effective approach that enhances the operational efficiency and\naffordability of LLM inference. By utilizing transformer-based federated\nlearning (FL) with model-parallel distributed training, our model efficiently\ndistributes the computational loads and memory requirements across a network of\nparticipants. This strategy permits users, especially those with limited\nresources to train state-of-the-art LLMs collaboratively. We also innovate an\nincentive mechanism within the FL framework, rewarding constructive\ncontributions and filtering out malicious activities, thereby safeguarding the\nintegrity and reliability of the training process. Concurrently, we leverage\nmemory hierarchy strategies and Singular Value Decomposition (SVD) on weight\nmatrices to boost computational and memory efficiencies further. Our results,\nderived from formulaic analyses and numerical calculations, demonstrate\nsignificant optimization of resource use and democratize access to cutting-edge\nLLMs, ensuring that a wide scale of users can both contribute to and benefit\nfrom these advanced models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9810\u544a\u8457\u4eba\u5de5\u667a\u6167 (AI) \u7684\u8b8a\u9769\u6642\u4ee3\u3002\u7136\u800c\uff0cLLM \u7684\u8cc7\u6599\u548c\u53c3\u6578\u898f\u6a21\u9f90\u5927\uff0c\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u8cc7\u6e90\uff0c\u9650\u5236\u4e86\u5176\u5c0d\u66f4\u5ee3\u6cdb\u7684\u4f7f\u7528\u8005\u548c\u7814\u7a76\u4eba\u54e1\u7684\u53ef\u53ca\u6027\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8 LLM \u63a8\u8ad6\u7684\u904b\u4f5c\u6548\u7387\u548c\u8ca0\u64d4\u80fd\u529b\u3002\u900f\u904e\u5229\u7528\u57fa\u65bc Transformer \u7684\u806f\u90a6\u5b78\u7fd2 (FL) \u548c\u6a21\u578b\u4e26\u884c\u5206\u5e03\u5f0f\u8a13\u7df4\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u5c07\u904b\u7b97\u8ca0\u8f09\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\u5206\u4f48\u5728\u53c3\u8207\u8005\u7684\u7db2\u8def\u4e2d\u3002\u6b64\u7b56\u7565\u5141\u8a31\u4f7f\u7528\u8005\uff08\u7279\u5225\u662f\u90a3\u4e9b\u8cc7\u6e90\u6709\u9650\u7684\u4f7f\u7528\u8005\uff09\u5408\u4f5c\u8a13\u7df4\u6700\u5148\u9032\u7684 LLM\u3002\u6211\u5011\u9084\u5728 FL \u67b6\u69cb\u4e2d\u5275\u65b0\u4e86\u4e00\u7a2e\u6fc0\u52f5\u6a5f\u5236\uff0c\u734e\u52f5\u5efa\u8a2d\u6027\u7684\u8ca2\u737b\u4e26\u904e\u6ffe\u6389\u60e1\u610f\u6d3b\u52d5\uff0c\u5f9e\u800c\u4fdd\u8b77\u8a13\u7df4\u904e\u7a0b\u7684\u5b8c\u6574\u6027\u548c\u53ef\u9760\u6027\u3002\u540c\u6642\uff0c\u6211\u5011\u5229\u7528\u8a18\u61b6\u9ad4\u968e\u5c64\u7b56\u7565\u548c\u6b0a\u91cd\u77e9\u9663\u4e0a\u7684\u5947\u7570\u503c\u5206\u89e3 (SVD) \u4f86\u9032\u4e00\u6b65\u63d0\u5347\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u6548\u7387\u3002\u6211\u5011\u5f9e\u516c\u5f0f\u5206\u6790\u548c\u6578\u503c\u8a08\u7b97\u4e2d\u5f97\u51fa\u7684\u7d50\u679c\uff0c\u8b49\u660e\u4e86\u8cc7\u6e90\u4f7f\u7528\u7684\u986f\u8457\u6700\u4f73\u5316\uff0c\u4e26\u4f7f\u5c16\u7aef\u7684 LLM \u6c11\u4e3b\u5316\uff0c\u78ba\u4fdd\u5ee3\u6cdb\u7684\u4f7f\u7528\u8005\u65e2\u80fd\u8ca2\u737b\u9019\u4e9b\u5148\u9032\u6a21\u578b\uff0c\u4e5f\u80fd\u5f9e\u4e2d\u53d7\u76ca\u3002", "author": "Shengwen Ding et.al.", "authors": "Shengwen Ding, Chenhui Hu", "id": "2411.16003v1", "paper_url": "http://arxiv.org/abs/2411.16003v1", "repo": "null"}}