{"2405.11577": {"publish_time": "2024-05-19", "title": "A Multi-Perspective Analysis of Memorization in Large Language Models", "paper_summary": "Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6578\u5341\u5104\u500b\u53c3\u6578\u7684\u9f90\u5927\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\uff0c\u5728\u5404\u500b\u9818\u57df\u5c55\u73fe\u51fa\u524d\u6240\u672a\u6709\u7684\u6548\u80fd\u3002\u5118\u7ba1\u5c0d\u5176\u5353\u8d8a\u7684\u8868\u73fe\u611f\u5230\u9a5a\u8a1d\uff0c\u7814\u7a76\u4eba\u54e1\u4e5f\u6ce8\u610f\u5230\u9019\u4e9b LLM \u7684\u4e00\u4e9b\u7279\u6b8a\u884c\u70ba\u3002\u5176\u4e2d\u4e00\u7a2e\u884c\u70ba\u662f\u8a18\u61b6\uff0cLLM \u53ef\u4ee5\u7522\u751f\u7528\u65bc\u8a13\u7df4\u5b83\u5011\u7684\u76f8\u540c\u5167\u5bb9\u3002\u5118\u7ba1\u5148\u524d\u7684\u7814\u7a76\u8a0e\u8ad6\u904e\u8a18\u61b6\uff0c\u4f46 LLM \u7684\u8a18\u61b6\u4ecd\u7136\u7f3a\u4e4f\u89e3\u91cb\uff0c\u7279\u5225\u662f\u8a18\u61b6\u7684\u539f\u56e0\u548c\u7522\u751f\u5b83\u5011\u7684\u52d5\u529b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f9e\u5404\u7a2e\u89d2\u5ea6\u5168\u9762\u8a0e\u8ad6\u8a18\u61b6\uff0c\u4e26\u5c07\u8a0e\u8ad6\u7bc4\u570d\u64f4\u5c55\u5230\u4e0d\u50c5\u50c5\u662f\u8a18\u61b6\u7684\u5167\u5bb9\uff0c\u9084\u5305\u62ec\u66f4\u5c11\u548c\u672a\u8a18\u61b6\u7684\u5167\u5bb9\u3002\u900f\u904e\u5404\u7a2e\u7814\u7a76\uff0c\u6211\u5011\u767c\u73fe\uff1a(1) \u900f\u904e\u5be6\u9a57\uff0c\u6211\u5011\u63ed\u793a\u4e86\u6a21\u578b\u5927\u5c0f\u3001\u5ef6\u7e8c\u5927\u5c0f\u548c\u4e0a\u4e0b\u6587\u5927\u5c0f\u4e4b\u9593\u7684\u8a18\u61b6\u95dc\u4fc2\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u672a\u8a18\u61b6\u53e5\u5b50\u5982\u4f55\u8f49\u63db\u70ba\u8a18\u61b6\u53e5\u5b50\u3002(2) \u900f\u904e\u5d4c\u5165\u5206\u6790\uff0c\u6211\u5011\u5c55\u793a\u4e86\u4e0d\u540c\u8a18\u61b6\u5206\u6578\u53e5\u5b50\u7684\u5d4c\u5165\u7a7a\u9593\u4e2d\uff0c\u6a21\u578b\u5927\u5c0f\u7684\u5206\u5e03\u548c\u89e3\u78bc\u52d5\u614b\u3002n-gram \u7d71\u8a08\u5206\u6790\u5448\u73fe d (3) \u5c0d n-gram \u548c\u71b5\u89e3\u78bc\u52d5\u614b\u7684\u5206\u6790\u767c\u73fe\uff0c\u7576\u6a21\u578b\u958b\u59cb\u7522\u751f\u8a18\u61b6\u53e5\u5b50\u6216\u672a\u8a18\u61b6\u53e5\u5b50\u6642\uff0c\u6703\u7522\u751f\u908a\u754c\u6548\u61c9\u3002(4) \u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b Transformer \u6a21\u578b\u4f86\u9810\u6e2c\u4e0d\u540c\u6a21\u578b\u7684\u8a18\u61b6\uff0c\u8868\u660e\u53ef\u4ee5\u901a\u904e\u4e0a\u4e0b\u6587\u4f86\u9810\u6e2c\u8a18\u61b6\u3002", "author": "Bowen Chen et.al.", "authors": "Bowen Chen, Namgi Han, Yusuke Miyao", "id": "2405.11577v1", "paper_url": "http://arxiv.org/abs/2405.11577v1", "repo": "null"}}