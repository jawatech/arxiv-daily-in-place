{"2410.06561": {"publish_time": "2024-10-09", "title": "Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching", "paper_summary": "Knowledge Distillation (KD) has emerged as a pivotal technique for neural\nnetwork compression and performance enhancement. Most KD methods aim to\ntransfer dark knowledge from a cumbersome teacher model to a lightweight\nstudent model based on Kullback-Leibler (KL) divergence loss. However, the\nstudent performance improvements achieved through KD exhibit diminishing\nmarginal returns, where a stronger teacher model does not necessarily lead to a\nproportionally stronger student model. To address this issue, we empirically\nfind that the KL-based KD method may implicitly change the inter-class\nrelationships learned by the student model, resulting in a more complex and\nambiguous decision boundary, which in turn reduces the model's accuracy and\ngeneralization ability. Therefore, this study argues that the student model\nshould learn not only the probability values from the teacher's output but also\nthe relative ranking of classes, and proposes a novel Correlation Matching\nKnowledge Distillation (CMKD) method that combines the Pearson and Spearman\ncorrelation coefficients-based KD loss to achieve more efficient and robust\ndistillation from a stronger teacher model. Moreover, considering that samples\nvary in difficulty, CMKD dynamically adjusts the weights of the Pearson-based\nloss and Spearman-based loss. CMKD is simple yet practical, and extensive\nexperiments demonstrate that it can consistently achieve state-of-the-art\nperformance on CIRAR-100 and ImageNet, and adapts well to various teacher\narchitectures, sizes, and other KD methods.", "paper_summary_zh": "\u77e5\u8b58\u8403\u53d6 (KD) \u5df2\u6210\u70ba\u795e\u7d93\u7db2\u8def\u58d3\u7e2e\u548c\u6548\u80fd\u589e\u5f37\u7684\u95dc\u9375\u6280\u8853\u3002\u5927\u591a\u6578 KD \u65b9\u6cd5\u7684\u76ee\u6a19\u662f\u6839\u64da Kullback-Leibler (KL) \u5dee\u7570\u640d\u5931\uff0c\u5c07\u9f90\u5927\u6559\u5e2b\u6a21\u578b\u4e2d\u7684\u6df1\u5c64\u77e5\u8b58\u8f49\u79fb\u5230\u8f15\u91cf\u7d1a\u5b78\u751f\u6a21\u578b\u3002\u7136\u800c\uff0c\u900f\u904e KD \u9054\u6210\u7684\u5b78\u751f\u6548\u80fd\u63d0\u5347\u8868\u73fe\u51fa\u905e\u6e1b\u7684\u908a\u969b\u5831\u916c\uff0c\u5176\u4e2d\u8f03\u5f37\u5927\u7684\u6559\u5e2b\u6a21\u578b\u4e26\u4e0d\u4e00\u5b9a\u6703\u5c0e\u81f4\u6210\u6bd4\u4f8b\u5730\u66f4\u5f37\u5927\u7684\u5b78\u751f\u6a21\u578b\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u6839\u64da\u7d93\u9a57\u767c\u73fe\uff0c\u57fa\u65bc KL \u7684 KD \u65b9\u6cd5\u53ef\u80fd\u6703\u96b1\u542b\u5730\u6539\u8b8a\u5b78\u751f\u6a21\u578b\u6240\u5b78\u7fd2\u7684\u985e\u9593\u95dc\u4fc2\uff0c\u5c0e\u81f4\u66f4\u8907\u96dc\u4e14\u6a21\u7a1c\u5169\u53ef\u7684\u6c7a\u7b56\u908a\u754c\uff0c\u9032\u800c\u964d\u4f4e\u6a21\u578b\u7684\u6e96\u78ba\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u8a8d\u70ba\u5b78\u751f\u6a21\u578b\u4e0d\u61c9\u53ea\u5b78\u7fd2\u6559\u5e2b\u8f38\u51fa\u4e2d\u7684\u6a5f\u7387\u503c\uff0c\u9084\u61c9\u5b78\u7fd2\u985e\u5225\u7684\u76f8\u5c0d\u6392\u540d\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u76f8\u95dc\u6027\u6bd4\u5c0d\u77e5\u8b58\u8403\u53d6 (CMKD) \u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u7d50\u5408\u76ae\u723e\u68ee\u548c\u65af\u76ae\u723e\u66fc\u76f8\u95dc\u4fc2\u6578\u70ba\u57fa\u790e\u7684 KD \u640d\u5931\uff0c\u4ee5\u5f9e\u66f4\u5f37\u5927\u7684\u6559\u5e2b\u6a21\u578b\u4e2d\u9054\u6210\u66f4\u6709\u6548\u7387\u4e14\u7a69\u5065\u7684\u8403\u53d6\u3002\u6b64\u5916\uff0c\u8003\u91cf\u5230\u6a23\u672c\u96e3\u5ea6\u6709\u5225\uff0cCMKD \u6703\u52d5\u614b\u8abf\u6574\u57fa\u65bc\u76ae\u723e\u68ee\u7684\u640d\u5931\u548c\u57fa\u65bc\u65af\u76ae\u723e\u66fc\u7684\u640d\u5931\u7684\u6b0a\u91cd\u3002CMKD \u65e2\u7c21\u55ae\u53c8\u5be6\u7528\uff0c\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u5b83\u53ef\u4ee5\u5728 CIRAR-100 \u548c ImageNet \u4e0a\u6301\u7e8c\u9054\u6210\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u4e26\u4e14\u80fd\u5f88\u597d\u5730\u9069\u61c9\u5404\u7a2e\u6559\u5e2b\u67b6\u69cb\u3001\u5927\u5c0f\u548c\u5176\u4ed6 KD \u65b9\u6cd5\u3002", "author": "Wenqi Niu et.al.", "authors": "Wenqi Niu, Yingchao Wang, Guohui Cai, Hanpo Hou", "id": "2410.06561v1", "paper_url": "http://arxiv.org/abs/2410.06561v1", "repo": "null"}}