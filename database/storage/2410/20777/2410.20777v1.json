{"2410.20777": {"publish_time": "2024-10-28", "title": "KD-LoRA: A Hybrid Approach to Efficient Fine-Tuning with LoRA and Knowledge Distillation", "paper_summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious downstream tasks. However, the high computational and memory\nrequirements of LLMs are a major bottleneck. To address this,\nparameter-efficient fine-tuning (PEFT) methods such as low-rank adaptation\n(LoRA) have been proposed to reduce computational costs while ensuring minimal\nloss in performance. Additionally, knowledge distillation (KD) has been a\npopular choice for obtaining compact student models from teacher models. In\nthis work, we present KD-LoRA, a novel fine-tuning method that combines LoRA\nwith KD. Our results demonstrate that KD-LoRA achieves performance comparable\nto full fine-tuning (FFT) and LoRA while significantly reducing resource\nrequirements. Specifically, KD-LoRA retains 98% of LoRA's performance on the\nGLUE benchmark, while being 40% more compact. Additionally, KD-LoRA reduces GPU\nmemory usage by 30% compared to LoRA, while decreasing inference time by 30%\ncompared to both FFT and LoRA. We evaluate KD-LoRA across three encoder-only\nmodels: BERT, RoBERTa, and DeBERTaV3. Code is available at\nhttps://github.com/rambodazimi/KD-LoRA.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u4e2d\u5c55\u793a\u4e86\u975e\u51e1\u7684\u8868\u73fe\u3002\u7136\u800c\uff0cLLM \u7684\u9ad8\u8a08\u7b97\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\u662f\u4e00\u500b\u4e3b\u8981\u7684\u74f6\u9838\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u5df2\u7d93\u63d0\u51fa\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\uff0c\u4f8b\u5982\u4f4e\u79e9\u9069\u61c9 (LoRA)\uff0c\u4ee5\u964d\u4f4e\u8a08\u7b97\u6210\u672c\uff0c\u540c\u6642\u78ba\u4fdd\u6548\u80fd\u7684\u6700\u5c0f\u640d\u5931\u3002\u6b64\u5916\uff0c\u77e5\u8b58\u84b8\u993e (KD) \u4e00\u76f4\u662f\u5f9e\u6559\u5e2b\u6a21\u578b\u4e2d\u7372\u53d6\u7cbe\u7c21\u5b78\u751f\u6a21\u578b\u7684\u71b1\u9580\u9078\u64c7\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 KD-LoRA\uff0c\u4e00\u7a2e\u7d50\u5408 LoRA \u8207 KD \u7684\u65b0\u5fae\u8abf\u65b9\u6cd5\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0cKD-LoRA \u9054\u5230\u4e86\u8207\u5b8c\u5168\u5fae\u8abf (FFT) \u548c LoRA \u76f8\u7576\u7684\u6548\u80fd\uff0c\u540c\u6642\u986f\u8457\u964d\u4f4e\u4e86\u8cc7\u6e90\u9700\u6c42\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cKD-LoRA \u5728 GLUE \u57fa\u6e96\u6e2c\u8a66\u4e2d\u4fdd\u7559\u4e86 LoRA 98% \u7684\u6548\u80fd\uff0c\u540c\u6642\u7cbe\u7c21\u4e86 40%\u3002\u6b64\u5916\uff0c\u8207 LoRA \u76f8\u6bd4\uff0cKD-LoRA \u5c07 GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86 30%\uff0c\u8207 FFT \u548c LoRA \u76f8\u6bd4\uff0c\u5c07\u63a8\u7406\u6642\u9593\u6e1b\u5c11\u4e86 30%\u3002\u6211\u5011\u5728\u4e09\u500b\u50c5\u7de8\u78bc\u5668\u6a21\u578b\uff1aBERT\u3001RoBERTa \u548c DeBERTaV3 \u4e2d\u8a55\u4f30\u4e86 KD-LoRA\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/rambodazimi/KD-LoRA \u53d6\u5f97\u3002", "author": "Rambod Azimi et.al.", "authors": "Rambod Azimi, Rishav Rishav, Marek Teichmann, Samira Ebrahimi Kahou", "id": "2410.20777v1", "paper_url": "http://arxiv.org/abs/2410.20777v1", "repo": "https://github.com/rambodazimi/kd-lora"}}