{"2409.06666": {"publish_time": "2024-09-10", "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models", "paper_summary": "Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.", "paper_summary_zh": "LLaMA-Omni \u7b49\u6a21\u578b\u80fd\u900f\u904e\u8a9e\u97f3\u8207\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9032\u884c\u5373\u6642\u4e92\u52d5\uff0c\u8207\u50b3\u7d71\u7684\u6587\u5b57\u4e92\u52d5\u76f8\u6bd4\uff0c\u5927\u5e45\u63d0\u5347\u4f7f\u7528\u8005\u9ad4\u9a57\u3002\u7136\u800c\uff0c\u95dc\u65bc\u5982\u4f55\u6839\u64da\u958b\u653e\u539f\u59cb\u78bc LLM \u5efa\u69cb\u8a9e\u97f3\u4e92\u52d5\u6a21\u578b\uff0c\u76ee\u524d\u4ecd\u7f3a\u4e4f\u63a2\u8a0e\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa LLaMA-Omni\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u6a21\u578b\u67b6\u69cb\uff0c\u5c08\u70ba\u8207 LLM \u9032\u884c\u4f4e\u5ef6\u9072\u3001\u9ad8\u54c1\u8cea\u7684\u8a9e\u97f3\u4e92\u52d5\u800c\u8a2d\u8a08\u3002LLaMA-Omni \u6574\u5408\u4e86\u9810\u8a13\u7df4\u7684\u8a9e\u97f3\u7de8\u78bc\u5668\u3001\u8a9e\u97f3\u9069\u914d\u5668\u3001LLM \u548c\u4e32\u6d41\u8a9e\u97f3\u89e3\u78bc\u5668\u3002\u5b83\u6d88\u9664\u4e86\u8a9e\u97f3\u8f49\u9304\u7684\u9700\u8981\uff0c\u4e26\u80fd\u76f4\u63a5\u5f9e\u8a9e\u97f3\u6307\u4ee4\u7522\u751f\u6587\u5b57\u548c\u8a9e\u97f3\u56de\u61c9\uff0c\u4e14\u5ef6\u9072\u6975\u4f4e\u3002\u6211\u5011\u6839\u64da\u6700\u65b0\u7684 Llama-3.1-8B-Instruct \u6a21\u578b\u5efa\u7acb\u6211\u5011\u7684\u6a21\u578b\u3002\u70ba\u4e86\u8b93\u6a21\u578b\u8207\u8a9e\u97f3\u4e92\u52d5\u60c5\u5883\u76f8\u7b26\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u540d\u70ba InstructS2S-200K \u7684\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b 200K \u500b\u8a9e\u97f3\u6307\u4ee4\u548c\u5c0d\u61c9\u7684\u8a9e\u97f3\u56de\u61c9\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u8207\u5148\u524d\u7684\u8a9e\u97f3\u8a9e\u8a00\u6a21\u578b\u76f8\u6bd4\uff0cLLaMA-Omni \u5728\u5167\u5bb9\u548c\u98a8\u683c\u4e0a\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u56de\u61c9\uff0c\u56de\u61c9\u5ef6\u9072\u4f4e\u81f3 226 \u6beb\u79d2\u3002\u6b64\u5916\uff0c\u8a13\u7df4 LLaMA-Omni \u53ea\u9700\u8981\u4e0d\u5230 3 \u5929\u7684\u6642\u9593\uff0c\u800c\u4e14\u50c5\u4f7f\u7528 4 \u500b GPU\uff0c\u70ba\u672a\u4f86\u8a9e\u97f3\u8a9e\u8a00\u6a21\u578b\u7684\u6709\u6548\u958b\u767c\u92ea\u8def\u3002", "author": "Qingkai Fang et.al.", "authors": "Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng", "id": "2409.06666v1", "paper_url": "http://arxiv.org/abs/2409.06666v1", "repo": "https://github.com/ictnlp/llama-omni"}}