{"2503.15924": {"publish_time": "2025-03-20", "title": "Towards Automatic Continual Learning: A Self-Adaptive Framework for Continual Instruction Tuning", "paper_summary": "Continual instruction tuning enables large language models (LLMs) to learn\nincrementally while retaining past knowledge, whereas existing methods\nprimarily focus on how to retain old knowledge rather than on selecting which\nnew knowledge to learn. In domain-specific contexts, maintaining data quality\nand managing system constraints remain key challenges. To address these issues,\nwe propose an automated continual instruction tuning framework that dynamically\nfilters incoming data, which identify and reduce redundant data across\nsuccessive updates. Our approach utilizes a small proxy model for efficient\nperplexity-based filtering, and updates the proxy to ensure that the filtering\ncriteria remain aligned with the evolving state of the deployed model. Compared\nto existing static data selection methods, our framework can effectively handle\nincrementally acquired data and shifting distributions. Additionally, it\naddresses practical deployment challenges by enabling seamless model updates,\nsupporting version rollback and incorporating automatic checkpoint evaluation.\nWe evaluated the system in real-world medical scenarios. It reduced\ncomputational costs by 66.7% and improved model performance, and achieved\nautonomous updates, thus demonstrating its effectiveness for automatic\ncontinual instruction tuning.", "paper_summary_zh": "<paragraph>\u6301\u7e8c\u6559\u5b78\u8abf\u6574\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u5920\u5728\u4fdd\u7559\u904e\u53bb\u77e5\u8b58\u7684\u540c\u6642\u9010\u6b65\u5b78\u7fd2\uff0c\u800c\u73fe\u6709\u65b9\u6cd5\u4e3b\u8981\u95dc\u6ce8\u5982\u4f55\u4fdd\u7559\u820a\u77e5\u8b58\uff0c\u800c\u4e0d\u662f\u9078\u64c7\u5b78\u7fd2\u54ea\u4e9b\u65b0\u77e5\u8b58\u3002\u5728\u7279\u5b9a\u9818\u57df\u7684\u74b0\u5883\u4e2d\uff0c\u7dad\u6301\u8cc7\u6599\u54c1\u8cea\u548c\u7ba1\u7406\u7cfb\u7d71\u9650\u5236\u4ecd\u7136\u662f\u4e3b\u8981\u6311\u6230\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u81ea\u52d5\u5316\u7684\u6301\u7e8c\u6559\u5b78\u8abf\u6574\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u52d5\u614b\u904e\u6ffe\u8f38\u5165\u8cc7\u6599\uff0c\u8b58\u5225\u4e26\u6e1b\u5c11\u9023\u7e8c\u66f4\u65b0\u4e2d\u7684\u5197\u9918\u8cc7\u6599\u3002\u6211\u5011\u7684\u65b9\u6cd5\u5229\u7528\u4e00\u500b\u5c0f\u578b\u4ee3\u7406\u6a21\u578b\u9032\u884c\u6709\u6548\u7387\u7684\u57fa\u65bc\u56f0\u60d1\u5ea6\u7684\u904e\u6ffe\uff0c\u4e26\u66f4\u65b0\u4ee3\u7406\u4ee5\u78ba\u4fdd\u904e\u6ffe\u6a19\u6e96\u8207\u5df2\u90e8\u7f72\u6a21\u578b\u7684\u6f14\u8b8a\u72c0\u614b\u4fdd\u6301\u4e00\u81f4\u3002\u8207\u73fe\u6709\u7684\u975c\u614b\u8cc7\u6599\u9078\u64c7\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u8655\u7406\u9010\u6b65\u7372\u53d6\u7684\u8cc7\u6599\u548c\u8b8a\u5316\u7684\u5206\u4f48\u3002\u6b64\u5916\uff0c\u5b83\u9084\u900f\u904e\u5be6\u73fe\u7121\u7e2b\u6a21\u578b\u66f4\u65b0\u3001\u652f\u63f4\u7248\u672c\u56de\u6efe\u548c\u6574\u5408\u81ea\u52d5\u6aa2\u67e5\u9ede\u8a55\u4f30\u4f86\u89e3\u6c7a\u5be6\u969b\u90e8\u7f72\u6311\u6230\u3002\u6211\u5011\u5728\u771f\u5be6\u4e16\u754c\u7684\u91ab\u7642\u5834\u666f\u4e2d\u8a55\u4f30\u4e86\u8a72\u7cfb\u7d71\u3002\u5b83\u5c07\u8a08\u7b97\u6210\u672c\u964d\u4f4e\u4e86 66.7%\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6548\u80fd\uff0c\u4e26\u5be6\u73fe\u4e86\u81ea\u4e3b\u66f4\u65b0\uff0c\u5f9e\u800c\u8b49\u660e\u4e86\u5176\u5728\u81ea\u52d5\u6301\u7e8c\u6559\u5b78\u8abf\u6574\u65b9\u9762\u7684\u6709\u6548\u6027\u3002</paragraph>\n", "author": "Peiyi Lin et.al.", "authors": "Peiyi Lin, Fukai Zhang, Kai Niu, Hao Fu", "id": "2503.15924v1", "paper_url": "http://arxiv.org/abs/2503.15924v1", "repo": "null"}}