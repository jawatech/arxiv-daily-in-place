
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-05**|**Wings: Learning Multimodal LLMs without Text-only Forgetting**|Yi-Kai Zhang et.al.|[2406.03496v1](http://arxiv.org/abs/2406.03496v1)|null|
|**2024-06-05**|**Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends**|Sanjana Ramprasad et.al.|[2406.03487v1](http://arxiv.org/abs/2406.03487v1)|null|
|**2024-06-05**|**BIPED: Pedagogically Informed Tutoring System for ESL Education**|Soonwoo Kwon et.al.|[2406.03486v1](http://arxiv.org/abs/2406.03486v1)|null|
|**2024-06-05**|**QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead**|Amir Zandieh et.al.|[2406.03482v1](http://arxiv.org/abs/2406.03482v1)|[link](https://github.com/amirzandieh/qjl)|
|**2024-06-05**|**Does your data spark joy? Performance gains from domain upsampling at the end of training**|Cody Blakeney et.al.|[2406.03476v1](http://arxiv.org/abs/2406.03476v1)|null|
|**2024-06-05**|**Using Synchronic Definitions and Semantic Relations to Classify Semantic Change Types**|Pierluigi Cassotti et.al.|[2406.03452v1](http://arxiv.org/abs/2406.03452v1)|null|
|**2024-06-05**|**What is the Best Way for ChatGPT to Translate Poetry?**|Shanshan Wang et.al.|[2406.03450v1](http://arxiv.org/abs/2406.03450v1)|null|
|**2024-06-05**|**FILS: Self-Supervised Video Feature Prediction In Semantic Language Space**|Mona Ahmadian et.al.|[2406.03447v1](http://arxiv.org/abs/2406.03447v1)|null|
|**2024-06-05**|**Pre-trained Large Language Models Use Fourier Features to Compute Addition**|Tianyi Zhou et.al.|[2406.03445v1](http://arxiv.org/abs/2406.03445v1)|null|
|**2024-06-05**|**Are language models rational? The case of coherence norms and belief revision**|Thomas Hofweber et.al.|[2406.03442v1](http://arxiv.org/abs/2406.03442v1)|null|
|**2024-06-05**|**Cycles of Thought: Measuring LLM Confidence through Stable Explanations**|Evan Becker et.al.|[2406.03441v1](http://arxiv.org/abs/2406.03441v1)|null|
|**2024-06-05**|**Text-to-Events: Synthetic Event Camera Streams from Conditional Text Input**|Joachim Ott et.al.|[2406.03439v1](http://arxiv.org/abs/2406.03439v1)|null|
|**2024-06-05**|**Automating Turkish Educational Quiz Generation Using Large Language Models**|Kamyar Zeinalipour et.al.|[2406.03397v1](http://arxiv.org/abs/2406.03397v1)|null|
|**2024-06-05**|**IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models**|David Ifeoluwa Adelani et.al.|[2406.03368v1](http://arxiv.org/abs/2406.03368v1)|null|
|**2024-06-05**|**CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning**|Xinrui Lin et.al.|[2406.03367v1](http://arxiv.org/abs/2406.03367v1)|null|
|**2024-06-05**|**LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback**|Timon Ziegenbein et.al.|[2406.03363v1](http://arxiv.org/abs/2406.03363v1)|null|
|**2024-06-05**|**Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize**|Tianren Zhang et.al.|[2406.03345v1](http://arxiv.org/abs/2406.03345v1)|[link](https://github.com/trzhang0116/feature-contamination)|
|**2024-06-05**|**Audio Mamba: Bidirectional State Space Model for Audio Representation Learning**|Mehmet Hamza Erol et.al.|[2406.03344v1](http://arxiv.org/abs/2406.03344v1)|null|
|**2024-06-05**|**The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches**|Bhashithe Abeysinghe et.al.|[2406.03339v1](http://arxiv.org/abs/2406.03339v1)|null|
|**2024-06-05**|**Reproducibility study of FairAC**|Gijs de Jong et.al.|[2406.03314v1](http://arxiv.org/abs/2406.03314v1)|[link](https://github.com/oxkitsune/fact)|
|**2024-06-05**|**The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games**|Mikhail Mozikov et.al.|[2406.03299v1](http://arxiv.org/abs/2406.03299v1)|null|
|**2024-06-05**|**Evaluating AI fairness in credit scoring with the BRIO tool**|Greta Coraglia et.al.|[2406.03292v1](http://arxiv.org/abs/2406.03292v1)|null|
|**2024-06-05**|**SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms**|Xingrun Xing et.al.|[2406.03287v1](http://arxiv.org/abs/2406.03287v1)|[link](https://github.com/xingrun-xing/spikelm)|
|**2024-06-05**|**Enhancing Repository-Level Code Generation with Integrated Contextual Information**|Zhiyuan Pan et.al.|[2406.03283v1](http://arxiv.org/abs/2406.03283v1)|null|
|**2024-06-05**|**FusionBench: A Comprehensive Benchmark of Deep Model Fusion**|Anke Tang et.al.|[2406.03280v1](http://arxiv.org/abs/2406.03280v1)|null|
|**2024-06-05**|**Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning**|Mohamed Elsayed et.al.|[2406.03276v1](http://arxiv.org/abs/2406.03276v1)|null|
|**2024-06-05**|**Enhancing CTC-based speech recognition with diverse modeling units**|Shiyi Han et.al.|[2406.03274v1](http://arxiv.org/abs/2406.03274v1)|null|
|**2024-06-05**|**Multi-Microphone Speech Emotion Recognition using the Hierarchical Token-semantic Audio Transformer Architecture**|Ohad Cohen et.al.|[2406.03272v1](http://arxiv.org/abs/2406.03272v1)|null|
|**2024-06-05**|**Deep Generative Models for Proton Zero Degree Calorimeter Simulations in ALICE, CERN**|Patryk Będkowski et.al.|[2406.03263v1](http://arxiv.org/abs/2406.03263v1)|null|
|**2024-06-05**|**Prompt-based Visual Alignment for Zero-shot Policy Transfer**|Haihan Gao et.al.|[2406.03250v1](http://arxiv.org/abs/2406.03250v1)|null|
|**2024-06-05**|**Large Language Models as Evaluators for Recommendation Explanations**|Xiaoyu Zhang et.al.|[2406.03248v1](http://arxiv.org/abs/2406.03248v1)|[link](https://github.com/xiaoyu-sz/llmasevaluator)|
|**2024-06-05**|**Error-preserving Automatic Speech Recognition of Young English Learners' Language**|Janick Michot et.al.|[2406.03235v1](http://arxiv.org/abs/2406.03235v1)|[link](https://github.com/mict-zhaw/chall_e2e_stt)|
|**2024-06-05**|**Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning**|Inwoo Hwang et.al.|[2406.03234v1](http://arxiv.org/abs/2406.03234v1)|[link](https://github.com/iwhwang/Fine-Grained-Causal-RL)|
|**2024-06-05**|**Global Clipper: Enhancing Safety and Reliability of Transformer-based Object Detection Models**|Qutub Syed Sha et.al.|[2406.03229v1](http://arxiv.org/abs/2406.03229v1)|null|
|**2024-06-05**|**Challenges and Considerations in the Evaluation of Bayesian Causal Discovery**|Amir Mohammad Karimi Mamaghan et.al.|[2406.03209v1](http://arxiv.org/abs/2406.03209v1)|null|
|**2024-06-05**|**ChatLang-8: An LLM-Based Synthetic Data Generation Framework for Grammatical Error Correction**|Jeiyoon Park et.al.|[2406.03202v1](http://arxiv.org/abs/2406.03202v1)|null|
|**2024-06-05**|**Situation Monitor: Diversity-Driven Zero-Shot Out-of-Distribution Detection using Budding Ensemble Architecture for Object Detection**|Qutub Syed et.al.|[2406.03188v1](http://arxiv.org/abs/2406.03188v1)|null|
|**2024-06-05**|**Missci: Reconstructing Fallacies in Misrepresented Science**|Max Glockner et.al.|[2406.03181v1](http://arxiv.org/abs/2406.03181v1)|null|
|**2024-06-05**|**StatBot.Swiss: Bilingual Open Data Exploration in Natural Language**|Farhad Nooralahzadeh et.al.|[2406.03170v1](http://arxiv.org/abs/2406.03170v1)|null|
|**2024-06-05**|**CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs**|Shuang Ao et.al.|[2406.03158v1](http://arxiv.org/abs/2406.03158v1)|[link](https://github.com/aoshuang92/css_uq_llms)|
|**2024-06-05**|**Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks: An Extended Investigation**|Marvin Schmitt et.al.|[2406.03154v1](http://arxiv.org/abs/2406.03154v1)|null|
|**2024-06-05**|**Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation**|Hao Li et.al.|[2406.03151v1](http://arxiv.org/abs/2406.03151v1)|null|
|**2024-06-05**|**Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models**|Jerry Yao-Chieh Hu et.al.|[2406.03136v1](http://arxiv.org/abs/2406.03136v1)|null|
|**2024-06-05**|**Towards Real-world Scenario: Imbalanced New Intent Discovery**|Shun Zhang et.al.|[2406.03127v1](http://arxiv.org/abs/2406.03127v1)|null|
|**2024-06-05**|**Space Decomposition for Sentence Embedding**|Wuttikorn Ponwitayarat et.al.|[2406.03125v1](http://arxiv.org/abs/2406.03125v1)|[link](https://github.com/kornwtp/mixsp)|
|**2024-06-05**|**Enhancing the Resilience of Graph Neural Networks to Topological Perturbations in Sparse Graphs**|Shuqi He et.al.|[2406.03097v1](http://arxiv.org/abs/2406.03097v1)|null|
|**2024-06-05**|**FragRel: Exploiting Fragment-level Relations in the External Memory of Large Language Models**|Xihang Yue et.al.|[2406.03092v1](http://arxiv.org/abs/2406.03092v1)|null|
|**2024-06-05**|**Cryptocurrency Frauds for Dummies: How ChatGPT introduces us to fraud?**|Wail Zellagui et.al.|[2406.03079v1](http://arxiv.org/abs/2406.03079v1)|null|
|**2024-06-05**|**Towards Federated Domain Unlearning: Verification Methodologies and Challenges**|Kahou Tam et.al.|[2406.03078v1](http://arxiv.org/abs/2406.03078v1)|null|
|**2024-06-05**|**Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework**|Xiaoxi Sun et.al.|[2406.03075v1](http://arxiv.org/abs/2406.03075v1)|null|
|**2024-06-05**|**Exploiting LMM-based knowledge for image classification tasks**|Maria Tzelepi et.al.|[2406.03071v1](http://arxiv.org/abs/2406.03071v1)|null|
|**2024-06-05**|**A-Bench: Are LMMs Masters at Evaluating AI-generated Images?**|Zicheng Zhang et.al.|[2406.03070v1](http://arxiv.org/abs/2406.03070v1)|[link](https://github.com/q-future/a-bench)|
|**2024-06-05**|**How Truncating Weights Improves Reasoning in Language Models**|Lei Chen et.al.|[2406.03068v1](http://arxiv.org/abs/2406.03068v1)|null|
|**2024-06-05**|**RadBARTsum: Domain Specific Adaption of Denoising Sequence-to-Sequence Models for Abstractive Radiology Report Summarization**|Jinge Wu et.al.|[2406.03062v1](http://arxiv.org/abs/2406.03062v1)|null|
|**2024-06-05**|**StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning**|Shaolei Zhang et.al.|[2406.03049v1](http://arxiv.org/abs/2406.03049v1)|[link](https://github.com/ictnlp/streamspeech)|
|**2024-06-05**|**From Tarzan to Tolkien: Controlling the Language Proficiency Level of LLMs for Content Generation**|Ali Malik et.al.|[2406.03030v1](http://arxiv.org/abs/2406.03030v1)|null|
|**2024-06-05**|**Analyzing the Influence of Training Samples on Explanations**|André Artelt et.al.|[2406.03012v1](http://arxiv.org/abs/2406.03012v1)|null|
|**2024-06-05**|**Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models**|Sheng-Lun Wei et.al.|[2406.03009v1](http://arxiv.org/abs/2406.03009v1)|null|
|**2024-06-05**|**DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences**|Yidong Huang et.al.|[2406.03008v1](http://arxiv.org/abs/2406.03008v1)|null|
|**2024-06-05**|**BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents**|Yifei Wang et.al.|[2406.03007v1](http://arxiv.org/abs/2406.03007v1)|[link](https://github.com/dpamk/badagent)|
|**2024-06-05**|**Evaluation of data inconsistency for multi-modal sentiment analysis**|Yufei Wang et.al.|[2406.03004v1](http://arxiv.org/abs/2406.03004v1)|null|
|**2024-06-05**|**EdgeSync: Faster Edge-model Updating via Adaptive Continuous Learning for Video Data Drift**|Peng Zhao et.al.|[2406.03001v1](http://arxiv.org/abs/2406.03001v1)|null|
|**2024-06-05**|**Simplification of Risk Averse POMDPs with Performance Guarantees**|Yaacov Pariente et.al.|[2406.03000v1](http://arxiv.org/abs/2406.03000v1)|null|
|**2024-06-05**|**Learning Semantic Traversability with Egocentric Video and Automated Annotation Strategy**|Yunho Kim et.al.|[2406.02989v1](http://arxiv.org/abs/2406.02989v1)|null|
|**2024-06-05**|**Tensor Polynomial Additive Model**|Yang Chen et.al.|[2406.02980v1](http://arxiv.org/abs/2406.02980v1)|null|
|**2024-06-05**|**Efficient User Sequence Learning for Online Services via Compressed Graph Neural Networks**|Yucheng Wu et.al.|[2406.02979v1](http://arxiv.org/abs/2406.02979v1)|[link](https://github.com/wuyucheng2002/ecseq)|
|**2024-06-05**|**DA-Flow: Dual Attention Normalizing Flow for Skeleton-based Video Anomaly Detection**|Ruituo Wu et.al.|[2406.02976v1](http://arxiv.org/abs/2406.02976v1)|null|
|**2024-06-05**|**Readability-guided Idiom-aware Sentence Simplification (RISS) for Chinese**|Jingshen Zhang et.al.|[2406.02974v1](http://arxiv.org/abs/2406.02974v1)|null|
|**2024-06-05**|**Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture of Large Language Models**|Raeid Saqur et.al.|[2406.02969v1](http://arxiv.org/abs/2406.02969v1)|null|
|**2024-06-05**|**Generative AI and Digital Neocolonialism in Global Education: Towards an Equitable Framework**|Matthew Nyaaba et.al.|[2406.02966v1](http://arxiv.org/abs/2406.02966v1)|null|
|**2024-06-05**|**Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models**|Qiang Sun et.al.|[2406.02962v1](http://arxiv.org/abs/2406.02962v1)|null|
|**2024-06-05**|**Adversarial Moment-Matching Distillation of Large Language Models**|Chen Jia et.al.|[2406.02959v1](http://arxiv.org/abs/2406.02959v1)|null|
|**2024-06-05**|**PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs**|Charlie Hou et.al.|[2406.02958v1](http://arxiv.org/abs/2406.02958v1)|[link](https://github.com/houcharlie/pre-text)|
|**2024-06-05**|**4D ASR: Joint Beam Search Integrating CTC, Attention, Transducer, and Mask Predict Decoders**|Yui Sudo et.al.|[2406.02950v1](http://arxiv.org/abs/2406.02950v1)|null|
|**2024-06-05**|**The Task-oriented Queries Benchmark (ToQB)**|Keun Soo Yim et.al.|[2406.02943v1](http://arxiv.org/abs/2406.02943v1)|null|
|**2024-06-05**|**Multivariate Physics-Informed Convolutional Autoencoder for Anomaly Detection in Power Distribution Systems with High Penetration of DERs**|Mehdi Jabbari Zideh et.al.|[2406.02927v1](http://arxiv.org/abs/2406.02927v1)|null|
|**2024-06-05**|**SYN2REAL: Leveraging Task Arithmetic for Mitigating Synthetic-Real Discrepancies in ASR Domain Adaptation**|Hsuan Su et.al.|[2406.02925v1](http://arxiv.org/abs/2406.02925v1)|null|
|**2024-06-05**|**Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models**|Peijie Dong et.al.|[2406.02924v1](http://arxiv.org/abs/2406.02924v1)|null|
|**2024-06-05**|**Text Injection for Neural Contextual Biasing**|Zhong Meng et.al.|[2406.02921v1](http://arxiv.org/abs/2406.02921v1)|null|
|**2024-06-05**|**MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering Medical Knowledge**|Yuxuan Zhou et.al.|[2406.02919v1](http://arxiv.org/abs/2406.02919v1)|[link](https://github.com/thumlp/multifaceteval)|
|**2024-06-05**|**Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity**|Wentao Guo et.al.|[2406.02913v1](http://arxiv.org/abs/2406.02913v1)|null|
|**2024-06-05**|**Improving In-Context Learning with Prediction Feedback for Sentiment Analysis**|Hongling Xu et.al.|[2406.02911v1](http://arxiv.org/abs/2406.02911v1)|null|
|**2024-06-05**|**Open Grounded Planning: Challenges and Benchmark Construction**|Shiguang Guo et.al.|[2406.02903v1](http://arxiv.org/abs/2406.02903v1)|null|
|**2024-06-05**|**Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms**|Rafael Rafailov et.al.|[2406.02900v1](http://arxiv.org/abs/2406.02900v1)|null|
|**2024-06-05**|**Language Model Can Do Knowledge Tracing: Simple but Effective Method to Integrate Language Model and Knowledge Tracing Task**|Unggi Lee et.al.|[2406.02893v1](http://arxiv.org/abs/2406.02893v1)|null|
|**2024-06-05**|**HYDRA: Model Factorization Framework for Black-Box LLM Personalization**|Yuchen Zhuang et.al.|[2406.02888v1](http://arxiv.org/abs/2406.02888v1)|null|
|**2024-06-05**|**PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs**|Rongzhi Zhang et.al.|[2406.02886v1](http://arxiv.org/abs/2406.02886v1)|null|
|**2024-06-05**|**Outdated Issue Aware Decoding for Factual Knowledge Editing**|Zengkui Sun et.al.|[2406.02882v1](http://arxiv.org/abs/2406.02882v1)|null|
|**2024-06-05**|**Controllable Talking Face Generation by Implicit Facial Keypoints Editing**|Dong Zhao et.al.|[2406.02880v1](http://arxiv.org/abs/2406.02880v1)|null|
|**2024-06-05**|**LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation**|Zengkui Sun et.al.|[2406.02876v1](http://arxiv.org/abs/2406.02876v1)|null|
|**2024-06-05**|**Combinatorial Optimization with Automated Graph Neural Networks**|Yang Liu et.al.|[2406.02872v1](http://arxiv.org/abs/2406.02872v1)|[link](https://github.com/amazon-science/co-with-gnns-example)|
|**2024-06-05**|**Sound Heuristic Search Value Iteration for Undiscounted POMDPs with Reachability Objectives**|Qi Heng Ho et.al.|[2406.02871v1](http://arxiv.org/abs/2406.02871v1)|null|
|**2024-06-05**|**Oscillations enhance time-series prediction in reservoir computing with feedback**|Yuji Kawai et.al.|[2406.02867v1](http://arxiv.org/abs/2406.02867v1)|null|
|**2024-06-05**|**NUMCoT: Numerals and Units of Measurement in Chain-of-Thought Reasoning using Large Language Models**|Ancheng Xu et.al.|[2406.02864v1](http://arxiv.org/abs/2406.02864v1)|null|
|**2024-06-05**|**LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation**|Yi-Pei Chen et.al.|[2406.02863v1](http://arxiv.org/abs/2406.02863v1)|null|
|**2024-06-05**|**Xmodel-LM Technical Report**|Yichuan Wang et.al.|[2406.02856v1](http://arxiv.org/abs/2406.02856v1)|[link](https://github.com/xiaoduoailab/xmodellm)|
|**2024-06-05**|**Item-Language Model for Conversational Recommendation**|Li Yang et.al.|[2406.02844v1](http://arxiv.org/abs/2406.02844v1)|null|
|**2024-06-05**|**Too Big to Fail: Larger Language Models are Disproportionately Resilient to Induction of Dementia-Related Linguistic Anomalies**|Changye Li et.al.|[2406.02830v1](http://arxiv.org/abs/2406.02830v1)|null|
|**2024-06-05**|**Stochastic Diffusion: A Diffusion Probabilistic Model for Stochastic Time Series Forecasting**|Yuansan Liu et.al.|[2406.02827v1](http://arxiv.org/abs/2406.02827v1)|null|
|**2024-06-05**|**Exploring Robustness in Doctor-Patient Conversation Summarization: An Analysis of Out-of-Domain SOAP Notes**|Yu-Wen Chen et.al.|[2406.02826v1](http://arxiv.org/abs/2406.02826v1)|null|

#### Abstracts
##### **Wings: Learning Multimodal LLMs without Text-only Forgetting**
2406.03496v1 by Yi-Kai Zhang, Shiyin Lu, Yang Li, Yanqing Ma, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye

Multimodal large language models (MLLMs), initiated with a trained LLM, first
align images with text and then fine-tune on multimodal mixed inputs. However,
the MLLM catastrophically forgets the text-only instructions, which do not
include images and can be addressed within the initial LLM. In this paper, we
present Wings, a novel MLLM that excels in both text-only dialogues and
multimodal comprehension. Analyzing MLLM attention in multimodal instructions
reveals that text-only forgetting is related to the attention shifts from
pre-image to post-image text. From that, we construct extra modules that act as
the boosted learner to compensate for the attention shift. The complementary
visual and textual learners, like "wings" on either side, are connected in
parallel within each layer's attention block. Initially, image and text inputs
are aligned with visual learners operating alongside the main attention,
balancing focus on visual elements. Textual learners are later collaboratively
integrated with attention-based routing to blend the outputs of the visual and
textual learners. We design the Low-Rank Residual Attention (LoRRA) to
guarantee high efficiency for learners. Our experimental results demonstrate
that Wings outperforms equally-scaled MLLMs in both text-only and visual
question-answering tasks. On a newly constructed Interleaved Image-Text (IIT)
benchmark, Wings exhibits superior performance from text-only-rich to
multimodal-rich question-answering tasks.

摘要：多模态大语言模型 (MLLM) 从训练过的 LLM 开始，首先将图像与文本对齐，然后针对多模态混合输入进行微调。然而，MLLM 灾难性地忘记了仅文本的指令，其中不包括图像，并且可以在初始 LLM 中解决。在本文中，我们展示了 Wings，一种新颖的 MLLM，它在仅文本对话和多模态理解方面都非常出色。分析多模态指令中的 MLLM 注意力揭示了仅文本遗忘与注意力从图像前文本转移到图像后文本有关。由此，我们构建了充当增强学习器的额外模块来补偿注意力转移。互补的视觉和文本学习器，就像两侧的“翅膀”，在每个层的注意力块内并行连接。最初，图像和文本输入与视觉学习器对齐，与主要注意力一起操作，平衡对视觉元素的关注。文本学习器后来与基于注意力的路由协作集成，以混合视觉和文本学习器的输出。我们设计了低秩残差注意力 (LoRRA) 来保证学习器的效率。我们的实验结果表明，Wings 在仅文本和视觉问答任务中都优于规模相同的 MLLM。在新建的交错图像文本 (IIT) 基准测试中，Wings 从仅文本丰富的问答任务到多模态丰富的问答任务都表现出卓越的性能。

##### **Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends**
2406.03487v1 by Sanjana Ramprasad, Elisa Ferracane, Zachary C. Lipton

Recent advancements in large language models (LLMs) have considerably
advanced the capabilities of summarization systems. However, they continue to
face concerns about hallucinations. While prior work has evaluated LLMs
extensively in news domains, most evaluation of dialogue summarization has
focused on BART-based models, leaving a gap in our understanding of their
faithfulness. Our work benchmarks the faithfulness of LLMs for dialogue
summarization, using human annotations and focusing on identifying and
categorizing span-level inconsistencies. Specifically, we focus on two
prominent LLMs: GPT-4 and Alpaca-13B. Our evaluation reveals subtleties as to
what constitutes a hallucination: LLMs often generate plausible inferences,
supported by circumstantial evidence in the conversation, that lack direct
evidence, a pattern that is less prevalent in older models. We propose a
refined taxonomy of errors, coining the category of "Circumstantial Inference"
to bucket these LLM behaviors and release the dataset. Using our taxonomy, we
compare the behavioral differences between LLMs and older fine-tuned models.
Additionally, we systematically assess the efficacy of automatic error
detection methods on LLM summaries and find that they struggle to detect these
nuanced errors. To address this, we introduce two prompt-based approaches for
fine-grained error detection that outperform existing metrics, particularly for
identifying "Circumstantial Inference."

摘要：大型語言模型 (LLM) 的最新進展已大幅提升摘要系統的能力。然而，它們仍然面臨關於幻覺的疑慮。雖然先前的工作已廣泛評估新聞領域的 LLM，但對話摘要的大部分評估都集中在基於 BART 的模型上，對其忠實度了解不足。我們的研究基準測試了對話摘要中 LLM 的忠實度，使用人工註解並專注於識別和分類跨度級別的不一致。具體來說，我們專注於兩個著名的 LLM：GPT-4 和 Alpaca-13B。我們的評估揭示了構成幻覺的細微差別：LLM 通常會根據對話中的情境證據產生合理的推論，但缺乏直接證據，這種模式在較舊的模型中較不普遍。我們提出了一個精緻的錯誤分類法，創造了「情境推論」類別來分類這些 LLM 行為並發布資料集。使用我們的分類法，我們比較了 LLM 和較舊的微調模型之間的行為差異。此外，我們系統地評估了自動錯誤偵測方法在 LLM 摘要中的效能，並發現它們難以偵測到這些細微的錯誤。為了解決這個問題，我們引入了兩種基於提示的方法，用於精細的錯誤偵測，其優於現有的指標，特別是在識別「情境推論」方面。

##### **BIPED: Pedagogically Informed Tutoring System for ESL Education**
2406.03486v1 by Soonwoo Kwon, Sojung Kim, Minju Park, Seunghyun Lee, Kyuseok Kim

Large Language Models (LLMs) have a great potential to serve as readily
available and cost-efficient Conversational Intelligent Tutoring Systems (CITS)
for teaching L2 learners of English. Existing CITS, however, are designed to
teach only simple concepts or lack the pedagogical depth necessary to address
diverse learning strategies. To develop a more pedagogically informed CITS
capable of teaching complex concepts, we construct a BIlingual
PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human
English tutoring interactions. Through post-hoc analysis of the tutoring
interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9
student acts), which we use to further annotate the collected dataset. Based on
a two-step framework of first predicting the appropriate tutor act then
generating the corresponding response, we implemented two CITS models using
GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the
implemented models not only replicate the style of human teachers but also
employ diverse and contextually appropriate pedagogical strategies.

摘要：大型語言模型 (LLM) 具有作為現成且經濟高效的英語 L2 學習者的對話式智慧教學系統 (CITS) 的巨大潛力。然而，現有的 CITS 僅設計用於教授簡單的概念，或缺乏必要的教學深度來應對不同的學習策略。為了開發一個更具教學意義的 CITS，能夠教授複雜的概念，我們構建了一個雙語教學法教學數據集 (BIPED)，其中包含一對一的、人與人之間的英語輔導互動。通過對輔導互動的事後分析，我們提出了一個對話行為詞彙表（34 個輔導行為和 9 個學生行為），我們使用這些行為對收集的數據集進行進一步註釋。基於首先預測適當的輔導行為，然後生成相應的回應的兩步架構，我們分別使用 GPT-4 和 SOLAR-KO 實施了兩個 CITS 模型。我們通過實驗證明，所實施的模型不僅複製了人類教師的風格，而且還採用了多樣化且符合語境的教學策略。

##### **QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead**
2406.03482v1 by Amir Zandieh, Majid Daliri, Insu Han

Serving LLMs requires substantial memory due to the storage requirements of
Key-Value (KV) embeddings in the KV cache, which grows with sequence length. An
effective approach to compress KV cache is quantization. However, traditional
quantization methods face significant memory overhead due to the need to store
quantization constants (at least a zero point and a scale) in full precision
per data block. Depending on the block size, this overhead can add 1 or 2 bits
per quantized number. We introduce QJL, a new quantization approach that
consists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit
quantization. In contrast to existing methods, QJL eliminates memory overheads
by removing the need for storing quantization constants. We propose an
asymmetric estimator for the inner product of two vectors and demonstrate that
applying QJL to one vector and a standard JL transform without quantization to
the other provides an unbiased estimator with minimal distortion. We have
developed an efficient implementation of the QJL sketch and its corresponding
inner product estimator, incorporating a lightweight CUDA kernel for optimized
computation. When applied across various LLMs and NLP tasks to quantize the KV
cache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV
cache memory usage without compromising accuracy, all while achieving faster
runtime. Codes are available at \url{https://github.com/amirzandieh/QJL}.

摘要：服务 LLM 需要大量的内存，这是因为 KV 缓存中键值 (KV) 嵌入的存储需求，它会随着序列长度增长而增加。压缩 KV 缓存的有效方法是量化。但是，传统的量化方法由于需要以完整精度存储量化常数（至少一个零点和一个比例）而面临巨大的内存开销，具体取决于数据块的大小，此开销可能为每个量化数字增加 1 或 2 位。我们引入了 QJL，这是一种新的量化方法，它包括一个 Johnson-Lindenstrauss (JL) 变换，后面是符号位量化。与现有方法相比，QJL 通过消除存储量化常数的需要来消除内存开销。我们为两个向量的内积提出了一个非对称估计量，并证明将 QJL 应用于一个向量，并将标准 JL 变换应用于另一个向量（不量化）而无失真，提供了一个无偏估计量。我们开发了 QJL 草图及其对应的内积估计量的有效实现，并加入了一个轻量级的 CUDA 内核以优化计算。当应用于各种 LLM 和 NLP 任务以将 KV 缓存量化为只有 3 位时，QJL 展示了 KV 缓存内存使用量减少了五倍以上，而没有影响准确性，同时实现了更快的运行时。代码可在 \url{https://github.com/amirzandieh/QJL} 获得。

##### **Does your data spark joy? Performance gains from domain upsampling at the end of training**
2406.03476v1 by Cody Blakeney, Mansheej Paul, Brett W. Larsen, Sean Owen, Jonathan Frankle

Pretraining datasets for large language models (LLMs) have grown to trillions
of tokens composed of large amounts of CommonCrawl (CC) web scrape along with
smaller, domain-specific datasets. It is expensive to understand the impact of
these domain-specific datasets on model capabilities as training at large FLOP
scales is required to reveal significant changes to difficult and emergent
benchmarks. Given the increasing cost of experimenting with pretraining data,
how does one determine the optimal balance between the diversity in general web
scrapes and the information density of domain specific data? In this work, we
show how to leverage the smaller domain specific datasets by upsampling them
relative to CC at the end of training to drive performance improvements on
difficult benchmarks. This simple technique allows us to improve up to 6.90 pp
on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data
mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2
(7B)$\unicode{x2014}$a model trained for twice as long. We experiment with
ablating the duration of domain upsampling from 5% to 30% of training and find
that 10% to 20% percent is optimal for navigating the tradeoff between general
language modeling capabilities and targeted benchmarks. We also use domain
upsampling to characterize at scale the utility of individual datasets for
improving various benchmarks by removing them during this final phase of
training. This tool opens up the ability to experiment with the impact of
different pretraining datasets at scale, but at an order of magnitude lower
cost compared to full pretraining runs.

摘要：大型语言模型 (LLM) 的预训练数据集已增长到数万亿个标记，其中包含大量 CommonCrawl (CC) 网络抓取，以及较小的特定领域数据集。了解这些特定领域数据集对模型功能的影响代价高昂，因为需要在大规模浮点运算 (FLOP) 范围内进行训练才能揭示对困难和新兴基准的重大变化。鉴于对预训练数据进行实验的成本越来越高，如何确定通用网络抓取的多样性与特定领域数据的的信息密度之间的最佳平衡？在这项工作中，我们展示了如何通过在训练结束时相对于 CC 对较小的特定领域数据集进行上采样来提高其利用率，从而提高困难基准的性能。这种简单的技术使我们能够在针对 1 万亿 (T) 个标记训练的 7B 模型上，相对于基本数据组合，在 MMLU 上提高高达 6.90 pp，在 GSM8K 上提高 8.26 pp，在 HumanEval 上提高 6.17 pp，从而与 Llama-2 (7B) 相媲美——一个训练时间长达两倍的模型。我们尝试从训练的 5% 到 30% 减少领域上采样的持续时间，并发现 10% 到 20% 是在通用语言建模功能和目标基准之间进行权衡的最佳选择。我们还使用领域上采样来大规模表征各个数据集在训练的最后阶段通过删除它们来改善各种基准的效用。此工具开启了以较低数量级成本（与完整的预训练运行相比）大规模试验不同预训练数据集影响的能力。

##### **Using Synchronic Definitions and Semantic Relations to Classify Semantic Change Types**
2406.03452v1 by Pierluigi Cassotti, Stefano De Pascale, Nina Tahmasebi

There is abundant evidence of the fact that the way words change their
meaning can be classified in different types of change, highlighting the
relationship between the old and new meanings (among which generalization,
specialization and co-hyponymy transfer). In this paper, we present a way of
detecting these types of change by constructing a model that leverages
information both from synchronic lexical relations and definitions of word
meanings. Specifically, we use synset definitions and hierarchy information
from WordNet and test it on a digitized version of Blank's (1997) dataset of
semantic change types. Finally, we show how the sense relationships can improve
models for both approximation of human judgments of semantic relatedness as
well as binary Lexical Semantic Change Detection.

摘要：有大量证据表明，单词改变其含义的方式可以分为不同类型的变化，突出了旧含义和新含义之间的关系（其中包括概括、特化和共下位关系转移）。在本文中，我们提出了一种通过构建一个利用来自共时词汇关系和单词含义定义的信息的模型来检测这些类型变化的方法。具体来说，我们使用 WordNet 中的同义词集定义和层次结构信息，并在 Blank（1997）语义变化类型数据集的数字化版本上对其进行测试。最后，我们展示了意义关系如何改进模型，以近似人类对语义相关性的判断以及二进制词汇语义变化检测。

##### **What is the Best Way for ChatGPT to Translate Poetry?**
2406.03450v1 by Shanshan Wang, Derek F. Wong, Jingming Yao, Lidia S. Chao

Machine translation (MT) has historically faced significant challenges when
applied to literary works, particularly in the domain of poetry translation.
The advent of Large Language Models such as ChatGPT holds potential for
innovation in this field. This study examines ChatGPT's capabilities in
English-Chinese poetry translation tasks, utilizing targeted prompts and small
sample scenarios to ascertain optimal performance. Despite promising outcomes,
our analysis reveals persistent issues in the translations generated by ChatGPT
that warrant attention. To address these shortcomings, we propose an
Explanation-Assisted Poetry Machine Translation (EAPMT) method, which leverages
monolingual poetry explanation as a guiding information for the translation
process. Furthermore, we refine existing evaluation criteria to better suit the
nuances of modern poetry translation. We engaged a panel of professional poets
for assessments, complemented evaluations by using GPT-4. The results from both
human and machine evaluations demonstrate that our EAPMT method outperforms
traditional translation methods of ChatGPT and the existing online systems.
This paper validates the efficacy of our method and contributes a novel
perspective to machine-assisted literary translation.

摘要：機器翻譯 (MT) 在應用於文學作品時，特別是在詩歌翻譯領域，從歷史上來看一直面臨著重大的挑戰。大型語言模型（例如 ChatGPT）的出現，為這一領域的創新提供了潛力。本研究探討了 ChatGPT 在英漢詩歌翻譯任務中的能力，利用有針對性的提示和小樣本場景來確定最佳性能。儘管取得了有希望的成果，但我們的分析揭示了 ChatGPT 生成的翻譯中存在持續的問題，值得關注。為了解決這些缺點，我們提出了一種解釋輔助詩歌機器翻譯 (EAPMT) 方法，該方法利用單語詩歌解釋作為翻譯過程的指導信息。此外，我們改進了現有的評估標準，以更好地適應現代詩歌翻譯的細微差別。我們聘請了一組專業詩人進行評估，並使用 GPT-4 對評估進行了補充。人類和機器評估的結果表明，我們的 EAPMT 方法優於 ChatGPT 的傳統翻譯方法和現有的在線系統。本文驗證了我們方法的有效性，並為機器輔助文學翻譯貢獻了一個新的視角。

##### **FILS: Self-Supervised Video Feature Prediction In Semantic Language Space**
2406.03447v1 by Mona Ahmadian, Frank Guerin, Andrew Gilbert

This paper demonstrates a self-supervised approach for learning semantic
video representations. Recent vision studies show that a masking strategy for
vision and natural language supervision has contributed to developing
transferable visual pretraining. Our goal is to achieve a more semantic video
representation by leveraging the text related to the video content during the
pretraining in a fully self-supervised manner. To this end, we present FILS, a
novel self-supervised video Feature prediction In semantic Language Space
(FILS). The vision model can capture valuable structured information by
correctly predicting masked feature semantics in language space. It is learned
using a patch-wise video-text contrastive strategy, in which the text
representations act as prototypes for transforming vision features into a
language space, which are then used as targets for semantically meaningful
feature prediction using our masked encoder-decoder structure. FILS
demonstrates remarkable transferability on downstream action recognition tasks,
achieving state-of-the-art on challenging egocentric datasets, like
Epic-Kitchens, Something-SomethingV2, Charades-Ego, and EGTEA, using ViT-Base.
Our efficient method requires less computation and smaller batches compared to
previous works.

摘要：本文展示了用于学习语义视频表征的自监督方法。最近的视觉研究表明，视觉和自然语言监督的掩蔽策略有助于开发可转移的视觉预训练。我们的目标是通过在预训练期间充分利用与视频内容相关的文本，来实现更语义化的视频表征。为此，我们提出了 FILS，一种语义语言空间中的新颖自监督视频特征预测 (FILS)。视觉模型可以通过在语言空间中正确预测掩蔽特征语义来捕获有价值的结构化信息。它使用基于 patch 的视频文本对比策略进行学习，其中文本表征充当将视觉特征转换为语言空间的原型，然后使用我们的掩蔽编码器-解码器结构作为语义有意义的特征预测的目标。FILS 在下游动作识别任务中展示了卓越的可转移性，使用 ViT-Base 在具有挑战性的以自我为中心的数据集（如 Epic-Kitchens、Something-SomethingV2、Charades-Ego 和 EGTEA）上实现了最先进的效果。与以前的工作相比，我们的高效方法需要更少的计算和更小的批次。

##### **Pre-trained Large Language Models Use Fourier Features to Compute Addition**
2406.03445v1 by Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia

Pre-trained large language models (LLMs) exhibit impressive mathematical
reasoning capabilities, yet how they compute basic arithmetic, such as
addition, remains unclear. This paper shows that pre-trained LLMs add numbers
using Fourier features -- dimensions in the hidden state that represent numbers
via a set of features sparse in the frequency domain. Within the model, MLP and
attention layers use Fourier features in complementary ways: MLP layers
primarily approximate the magnitude of the answer using low-frequency features,
while attention layers primarily perform modular addition (e.g., computing
whether the answer is even or odd) using high-frequency features. Pre-training
is crucial for this mechanism: models trained from scratch to add numbers only
exploit low-frequency features, leading to lower accuracy. Introducing
pre-trained token embeddings to a randomly initialized model rescues its
performance. Overall, our analysis demonstrates that appropriate pre-trained
representations (e.g., Fourier features) can unlock the ability of Transformers
to learn precise mechanisms for algorithmic tasks.

摘要：預先訓練的大型語言模型 (LLM) 展現出令人印象深刻的數學推理能力，但它們如何計算基本算術，例如加法，仍然不清楚。本文顯示預先訓練的 LLM 使用傅立葉特徵來加數字——隱藏狀態中的維度，透過一組在頻域中稀疏的特徵來表示數字。在模型中，MLP 和注意力層以互補的方式使用傅立葉特徵：MLP 層主要使用低頻率特徵來近似答案的幅度，而注意力層主要使用高頻率特徵來執行模組加法（例如，計算答案是偶數還是奇數）。預訓練對於這個機制至關重要：從頭訓練的模型只能使用低頻率特徵來加數字，導致準確度較低。將預先訓練的符號嵌入引入隨機初始化的模型可以挽救其效能。總體而言，我們的分析表明適當的預先訓練表示（例如，傅立葉特徵）可以解鎖 Transformer 學習演算法任務的精確機制的可能性。

##### **Are language models rational? The case of coherence norms and belief revision**
2406.03442v1 by Thomas Hofweber, Peter Hase, Elias Stengel-Eskin, Mohit Bansal

Do norms of rationality apply to machine learning models, in particular
language models? In this paper we investigate this question by focusing on a
special subset of rational norms: coherence norms. We consider both logical
coherence norms as well as coherence norms tied to the strength of belief. To
make sense of the latter, we introduce the Minimal Assent Connection (MAC) and
propose a new account of credence, which captures the strength of belief in
language models. This proposal uniformly assigns strength of belief simply on
the basis of model internal next token probabilities. We argue that rational
norms tied to coherence do apply to some language models, but not to others.
This issue is significant since rationality is closely tied to predicting and
explaining behavior, and thus it is connected to considerations about AI safety
and alignment, as well as understanding model behavior more generally.

摘要：理性規範是否適用於機器學習模型，特別是語言模型？在本文中，我們透過關注理性規範的特殊子集：相干性規範來探討這個問題。我們考慮邏輯相干性規範以及與信念強度相關的相干性規範。為了理解後者，我們引入了最小同意連接 (MAC)，並提出了一個新的信念度說明，它捕捉了語言模型中信念的強度。這個提議僅根據模型內部下一個符號機率均勻地分配信念強度。我們認為與相干性相關的理性規範確實適用於某些語言模型，但不適用於其他語言模型。這個問題很重要，因為理性與預測和解釋行為密切相關，因此它與對 AI 安全性和一致性以及更廣泛地理解模型行為的考量有關。

##### **Cycles of Thought: Measuring LLM Confidence through Stable Explanations**
2406.03441v1 by Evan Becker, Stefano Soatto

In many high-risk machine learning applications it is essential for a model
to indicate when it is uncertain about a prediction. While large language
models (LLMs) can reach and even surpass human-level accuracy on a variety of
benchmarks, their overconfidence in incorrect responses is still a
well-documented failure mode. Traditional methods for ML uncertainty
quantification can be difficult to directly adapt to LLMs due to the
computational cost of implementation and closed-source nature of many models. A
variety of black-box methods have recently been proposed, but these often rely
on heuristics such as self-verbalized confidence. We instead propose a
framework for measuring an LLM's uncertainty with respect to the distribution
of generated explanations for an answer. While utilizing explanations is not a
new idea in and of itself, by interpreting each possible model+explanation pair
as a test-time classifier we can calculate a posterior answer distribution over
the most likely of these classifiers. We demonstrate how a specific instance of
this framework using explanation entailment as our classifier likelihood
improves confidence score metrics (in particular AURC and AUROC) over baselines
across five different datasets. We believe these results indicate that our
framework is both a well-principled and effective way of quantifying
uncertainty in LLMs.

摘要：在許多高風險機器學習應用中，模型在對預測不確定時，指示其不確定性至關重要。儘管大型語言模型 (LLM) 在各種基準測試中可以達到甚至超過人類的準確度，但它們對不正確回應的過度自信仍然是一個有據可查的失敗模式。傳統的 ML 不確定性量化方法由於實作的計算成本和許多模型的封閉原始碼特性，可能難以直接調整為 LLM。最近提出了各種黑盒方法，但這些方法通常依賴於啟發法，例如自我表達的信心。我們改為提出一個框架，用來針對答案的產生解釋分布來衡量 LLM 的不確定性。儘管利用解釋本身並不是一個新想法，但透過將每個可能的模型 + 解釋配對解釋為測試時間分類器，我們可以計算出這些分類器中最可能的分類器上的後驗答案分布。我們展示了這個框架的一個特定實例，使用解釋蘊涵作為我們的分類器可能性，如何改善五個不同資料集上基線的信心分數指標（特別是 AURC 和 AUROC）。我們相信這些結果表明，我們的框架是一種基於良好原則且有效的方法，用來量化 LLM 中的不確定性。

##### **Text-to-Events: Synthetic Event Camera Streams from Conditional Text Input**
2406.03439v1 by Joachim Ott, Zuowen Wang, Shih-Chii Liu

Event cameras are advantageous for tasks that require vision sensors with
low-latency and sparse output responses. However, the development of deep
network algorithms using event cameras has been slow because of the lack of
large labelled event camera datasets for network training. This paper reports a
method for creating new labelled event datasets by using a text-to-X model,
where X is one or multiple output modalities, in the case of this work, events.
Our proposed text-to-events model produces synthetic event frames directly from
text prompts. It uses an autoencoder which is trained to produce sparse event
frames representing event camera outputs. By combining the pretrained
autoencoder with a diffusion model architecture, the new text-to-events model
is able to generate smooth synthetic event streams of moving objects. The
autoencoder was first trained on an event camera dataset of diverse scenes. In
the combined training with the diffusion model, the DVS gesture dataset was
used. We demonstrate that the model can generate realistic event sequences of
human gestures prompted by different text statements. The classification
accuracy of the generated sequences, using a classifier trained on the real
dataset, ranges between 42% to 92%, depending on the gesture group. The results
demonstrate the capability of this method in synthesizing event datasets.

摘要：事件相機對於需要具有低延遲和稀疏輸出回應的視覺感測器的任務來說是有利的。然而，由於缺乏用於網路訓練的大型標籤事件相機資料集，使用事件相機開發深度網路演算法的速度很慢。本文報告了一種使用文本到 X 模型建立新的標籤事件資料集的方法，其中 X 是單一或多個輸出模式，在本工作中為事件。我們提出的文本到事件模型直接從文本提示產生合成事件幀。它使用一個自動編碼器，經過訓練可以產生表示事件相機輸出的稀疏事件幀。通過將預先訓練的自動編碼器與擴散模型架構相結合，新的文本到事件模型能夠生成移動物體的平滑合成事件串流。自動編碼器首先在具有不同場景的事件相機資料集上進行訓練。在與擴散模型的組合訓練中，使用了 DVS 手勢資料集。我們證明了該模型可以生成由不同文本陳述提示的人類手勢的逼真事件序列。使用在真實資料集上訓練的分類器，生成序列的分類準確度在 42% 到 92% 之間，具體取決於手勢組。結果證明了這種方法在合成事件資料集方面的能力。

##### **Automating Turkish Educational Quiz Generation Using Large Language Models**
2406.03397v1 by Kamyar Zeinalipour, Yusuf Gökberk Keptiğ, Marco Maggini, Marco Gori

Crafting quizzes from educational content is a pivotal activity that benefits
both teachers and students by reinforcing learning and evaluating
understanding. In this study, we introduce a novel approach to generate quizzes
from Turkish educational texts, marking a pioneering endeavor in educational
technology specifically tailored to the Turkish educational context. We present
a specialized dataset, named the Turkish-Quiz-Instruct, comprising an extensive
collection of Turkish educational texts accompanied by multiple-choice and
short-answer quizzes. This research leverages the capabilities of Large
Language Models (LLMs), including GPT-4-Turbo, GPT-3.5-Turbo,
Llama-2-7b-chat-hf, and Llama-2-13b-chat-hf, to automatically generate quiz
questions and answers from the Turkish educational content. Our work delineates
the methodology for employing these LLMs in the context of Turkish educational
material, thereby opening new avenues for automated Turkish quiz generation.
The study not only demonstrates the efficacy of using such models for
generating coherent and relevant quiz content but also sets a precedent for
future research in the domain of automated educational content creation for
languages other than English. The Turkish-Quiz-Instruct dataset is introduced
as a valuable resource for researchers and practitioners aiming to explore the
boundaries of educational technology and language-specific applications of LLMs
in Turkish. By addressing the challenges of quiz generation in a non-English
context specifically Turkish, this study contributes significantly to the field
of Turkish educational technology, providing insights into the potential of
leveraging LLMs for educational purposes across diverse linguistic landscapes.

摘要：<paragraph>從教育內容中製作測驗是一項關鍵的活動，它通過加強學習和評估理解力，使教師和學生受益。在本研究中，我們介紹了一種從土耳其教育文本中生成測驗的新方法，這標誌著專門針對土耳其教育背景的教育技術的開創性努力。我們提供了一個名為 Turkish-Quiz-Instruct 的專業數據集，其中包含大量土耳其教育文本，以及多項選擇和簡答測驗。本研究利用了大型語言模型 (LLM) 的能力，包括 GPT-4-Turbo、GPT-3.5-Turbo、Llama-2-7b-chat-hf 和 Llama-2-13b-chat-hf，從土耳其教育內容中自動生成測驗題目和答案。我們的研究描述了在土耳其教育材料背景下使用這些 LLM 的方法，從而為自動化土耳其測驗生成開闢了新途徑。本研究不僅展示了使用此類模型生成連貫且相關的測驗內容的功效，還為英語以外語言的自動化教育內容創建領域的未來研究樹立了先例。Turkish-Quiz-Instruct 數據集被引入作為一個寶貴的資源，供研究人員和從業者探索教育技術的界限和土耳其語中 LLM 的特定語言應用。通過解決非英語背景（特別是土耳其語）中測驗生成的挑戰，本研究為土耳其教育技術領域做出了重大貢獻，提供了利用 LLM 在不同語言環境中進行教育目的的潛力的見解。</paragraph>

##### **IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models**
2406.03368v1 by David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda, Godson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, Pontus Stenetorp

Despite the widespread adoption of Large language models (LLMs), their
remarkable capabilities remain limited to a few high-resource languages.
Additionally, many low-resource languages (e.g. African languages) are often
evaluated only on basic text classification tasks due to the lack of
appropriate or comprehensive benchmarks outside of high-resource languages. In
this paper, we introduce IrokoBench -- a human-translated benchmark dataset for
16 typologically-diverse low-resource African languages covering three tasks:
natural language inference~(AfriXNLI), mathematical reasoning~(AfriMGSM), and
multi-choice knowledge-based QA~(AfriMMLU). We use IrokoBench to evaluate
zero-shot, few-shot, and translate-test settings~(where test sets are
translated into English) across 10 open and four proprietary LLMs. Our
evaluation reveals a significant performance gap between high-resource
languages~(such as English and French) and low-resource African languages. We
observe a significant performance gap between open and proprietary models, with
the highest performing open model, Aya-101 only at 58\% of the best-performing
proprietary model GPT-4o performance. Machine translating the test set to
English before evaluation helped to close the gap for larger models that are
English-centric, like LLaMa 3 70B. These findings suggest that more efforts are
needed to develop and adapt LLMs for African languages.

摘要：儘管大型語言模型 (LLM) 被廣泛採用，它們卓越的能力仍僅限於少數資源豐富的語言。此外，許多資源貧乏的語言（例如非洲語言）通常僅針對基本的文字分類任務進行評估，這是因為在資源豐富的語言之外缺乏適當或全面的基準。在本文中，我們介紹了 IrokoBench，這是一個針對 16 種類型多樣的資源貧乏的非洲語言進行人工翻譯的基準資料集，涵蓋三項任務：自然語言推論 (AfriXNLI)、數學推理 (AfriMGSM) 和多選題知識型問答 (AfriMMLU)。我們使用 IrokoBench 來評估零次學習、少次學習和翻譯測試設定（其中測試集被翻譯成英文）在 10 個開放原始碼和四個專有 LLM 中的表現。我們的評估顯示，資源豐富的語言（例如英語和法語）和資源貧乏的非洲語言之間存在顯著的效能差距。我們觀察到開放原始碼模型和專有模型之間存在顯著的效能差距，效能最高的開放原始碼模型 Aya-101 僅達到效能最佳的專有模型 GPT-4o 效能的 58%。在評估之前將測試集機器翻譯成英文有助於縮小以英語為中心的較大型模型（例如 LLaMa 3 70B）的差距。這些發現表明，需要付出更多努力來開發和調整適用於非洲語言的 LLM。

##### **CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning**
2406.03367v1 by Xinrui Lin, Yangfan Wu, Huanyu Yang, Yu Zhang, Yanyong Zhang, Jianmin Ji

Large Language Models (LLMs) possess extensive foundational knowledge and
moderate reasoning abilities, making them suitable for general task planning in
open-world scenarios. However, it is challenging to ground a LLM-generated plan
to be executable for the specified robot with certain restrictions. This paper
introduces CLMASP, an approach that couples LLMs with Answer Set Programming
(ASP) to overcome the limitations, where ASP is a non-monotonic logic
programming formalism renowned for its capacity to represent and reason about a
robot's action knowledge. CLMASP initiates with a LLM generating a basic
skeleton plan, which is subsequently tailored to the specific scenario using a
vector database. This plan is then refined by an ASP program with a robot's
action knowledge, which integrates implementation details into the skeleton,
grounding the LLM's abstract outputs in practical robot contexts. Our
experiments conducted on the VirtualHome platform demonstrate CLMASP's
efficacy. Compared to the baseline executable rate of under 2% with LLM
approaches, CLMASP significantly improves this to over 90%.

摘要：大型語言模型 (LLM) 擁有廣泛的基本知識和適度的推理能力，使其適合在開放世界場景中進行一般任務規劃。然而，要將 LLM 生成的計畫設定為基礎，以便具備特定限制的指定機器人執行，是一項挑戰。本文介紹 CLMASP，這是一種將 LLM 與答案設定程式設計 (ASP) 結合起來的方法，以克服限制，其中 ASP 是一種非單調邏輯程式設計形式主義，以其表示和推理機器人動作知識的能力而聞名。CLMASP 以 LLM 產生基本架構計畫開始，隨後使用向量資料庫根據具體場景進行調整。然後，由具備機器人動作知識的 ASP 程式對此計畫進行優化，將實施細節整合到架構中，將 LLM 的抽象輸出設定為實際機器人場景。我們在 VirtualHome 平台上進行的實驗證明了 CLMASP 的功效。與 LLM 方法低於 2% 的基準可執行率相比，CLMASP 將其顯著提高到 90% 以上。

##### **LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback**
2406.03363v1 by Timon Ziegenbein, Gabriella Skitalinskaya, Alireza Bayat Makou, Henning Wachsmuth

Ensuring that online discussions are civil and productive is a major
challenge for social media platforms. Such platforms usually rely both on users
and on automated detection tools to flag inappropriate arguments of other
users, which moderators then review. However, this kind of post-hoc moderation
is expensive and time-consuming, and moderators are often overwhelmed by the
amount and severity of flagged content. Instead, a promising alternative is to
prevent negative behavior during content creation. This paper studies how
inappropriate language in arguments can be computationally mitigated. We
propose a reinforcement learning-based rewriting approach that balances content
preservation and appropriateness based on existing classifiers, prompting an
instruction-finetuned large language model (LLM) as our initial policy. Unlike
related style transfer tasks, rewriting inappropriate arguments allows deleting
and adding content permanently. It is therefore tackled on document level
rather than sentence level. We evaluate different weighting schemes for the
reward function in both absolute and relative human assessment studies.
Systematic experiments on non-parallel data provide evidence that our approach
can mitigate the inappropriateness of arguments while largely preserving their
content. It significantly outperforms competitive baselines, including few-shot
learning, prompting, and humans.

摘要：確保網路討論有禮貌且有成效是社群媒體平台的一大挑戰。此類平台通常仰賴使用者和自動化偵測工具來標記其他使用者的不當論點，然後由管理員審查。然而，這種事後的審核成本高且耗時，而管理員通常會因為被標記的內容數量和嚴重性而不知所措。相反地，一個有前途的替代方案是在內容建立期間預防負面行為。本文研究如何以運算方式減輕論點中的不當語言。我們提出一個基於強化學習的重寫方法，它根據現有的分類器平衡內容保留和適切性，並提示一個指令微調過的大型語言模型 (LLM) 作為我們的初始政策。與相關的風格轉移任務不同，重寫不當的論點允許永久刪除和新增內容。因此，它是在文件層級而非句子層級上處理。我們在絕對和相對人類評估研究中評估獎勵函數的不同加權方案。非平行數據的系統性實驗提供了證據，證明我們的做法可以在很大程度上保留論點內容的同時減輕論點的不當性。它顯著優於競爭基準，包括少次學習、提示和人類。

##### **Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize**
2406.03345v1 by Tianren Zhang, Chujie Zhao, Guanyu Chen, Yizhou Jiang, Feng Chen

Learning representations that generalize under distribution shifts is
critical for building robust machine learning models. However, despite
significant efforts in recent years, algorithmic advances in this direction
have been limited. In this work, we seek to understand the fundamental
difficulty of out-of-distribution generalization with deep neural networks. We
first empirically show that perhaps surprisingly, even allowing a neural
network to explicitly fit the representations obtained from a teacher network
that can generalize out-of-distribution is insufficient for the generalization
of the student network. Then, by a theoretical study of two-layer ReLU networks
optimized by stochastic gradient descent (SGD) under a structured feature
model, we identify a fundamental yet unexplored feature learning proclivity of
neural networks, feature contamination: neural networks can learn uncorrelated
features together with predictive features, resulting in generalization failure
under distribution shifts. Notably, this mechanism essentially differs from the
prevailing narrative in the literature that attributes the generalization
failure to spurious correlations. Overall, our results offer new insights into
the non-linear feature learning dynamics of neural networks and highlight the
necessity of considering inductive biases in out-of-distribution
generalization.

摘要：學習在分佈轉移下進行泛化的表示，對於建立強健的機器學習模型至關重要。然而，儘管近年來做出了重大努力，但朝此方向的演算法進展仍然有限。在這項工作中，我們試圖了解深度神經網路在分佈外泛化方面的基本難度。我們首先憑經驗表明，也許令人驚訝的是，即使允許神經網路明確地符合從能夠泛化到分佈外的教師網路獲得的表示，對於學生網路的泛化也是不夠的。然後，通過對在結構化特徵模型下由隨機梯度下降 (SGD) 最佳化的兩層 ReLU 網路的理論研究，我們確定了神經網路一個基本但尚未探索的特徵學習傾向，即特徵污染：神經網路可以將不相關的特徵與預測特徵一起學習，導致在分佈轉移下泛化失敗。值得注意的是，這種機制與文獻中將泛化失敗歸因於虛假相關性的流行說法有本質上的不同。總體而言，我們的結果為神經網路的非線性特徵學習動態提供了新的見解，並強調了在分佈外泛化中考慮歸納偏見的必要性。

##### **Audio Mamba: Bidirectional State Space Model for Audio Representation Learning**
2406.03344v1 by Mehmet Hamza Erol, Arda Senocak, Jiu Feng, Joon Son Chung

Transformers have rapidly become the preferred choice for audio
classification, surpassing methods based on CNNs. However, Audio Spectrogram
Transformers (ASTs) exhibit quadratic scaling due to self-attention. The
removal of this quadratic self-attention cost presents an appealing direction.
Recently, state space models (SSMs), such as Mamba, have demonstrated potential
in language and vision tasks in this regard. In this study, we explore whether
reliance on self-attention is necessary for audio classification tasks. By
introducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based
model for audio classification, we aim to address this question. We evaluate
AuM on various audio datasets - comprising six different benchmarks - where it
achieves comparable or better performance compared to well-established AST
model.

摘要：Transformer 已迅速成為音訊分類的首選，超越基於 CNN 的方法。然而，音訊頻譜圖 Transformer (AST) 由於自注意力而表現出二次縮放。移除此二次自注意力成本呈現一個有吸引力的方向。最近，狀態空間模型 (SSM)，例如 Mamba，已證明在這方面在語言和視覺任務中具有潛力。在本研究中，我們探討是否依賴自注意力對於音訊分類任務是必要的。透過引入音訊 Mamba (AuM)，第一個無自注意力、純粹基於 SSM 的音訊分類模型，我們旨在解決這個問題。我們在各種音訊資料集（包括六個不同的基準）上評估 AuM，在這些資料集上，它與已建立良好的 AST 模型相比，達到可比較或更好的效能。

##### **The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches**
2406.03339v1 by Bhashithe Abeysinghe, Ruhan Circi

Chatbots have been an interesting application of natural language generation
since its inception. With novel transformer based Generative AI methods,
building chatbots have become trivial. Chatbots which are targeted at specific
domains such as medicine, psychology, and general information retrieval are
implemented rapidly. This, however, should not distract from the need to
evaluate the chatbot responses. Especially because the natural language
generation community does not entirely agree upon how to effectively evaluate
such applications. With this work we discuss the issue further with the
increasingly popular LLM based evaluations and how they correlate with human
evaluations. Additionally, we introduce a comprehensive factored evaluation
mechanism that can be utilized in conjunction with both human and LLM-based
evaluations.
  We present the results of an experimental evaluation conducted using this
scheme in one of our chatbot implementations, and subsequently compare
automated, traditional human evaluation, factored human evaluation, and
factored LLM evaluation. Results show that factor based evaluation produces
better insights on which aspects need to be improved in LLM applications and
further strengthens the argument to use human evaluation in critical spaces
where main functionality is not direct retrieval.

摘要：聊天機器人是自然語言生成自問世以來的一個有趣的應用。有了基於新穎Transformer的生成式 AI 方法，構建聊天機器人已變得微不足道。針對特定領域（如醫學、心理學和一般信息檢索）的聊天機器人可以快速實施。然而，這不應分散對評估聊天機器人回應的需求。特別是因為自然語言生成社群對於如何有效評估此類應用並未完全達成共識。在這項工作中，我們進一步討論了這個問題，以及它與人類評估之間的相關性，以及越來越流行的基於 LLM 的評估。此外，我們引入了一個全面的分層評估機制，它可以與基於人類和 LLM 的評估同時使用。
我們展示了使用此方案在我們的聊天機器人實作中進行的實驗評估結果，並隨後比較自動化、傳統的人類評估、分層人類評估和分層 LLM 評估。結果表明，基於因子的評估可以產生更好的見解，說明哪些方面需要在 LLM 應用中改進，並進一步加強了在主要功能不是直接檢索的關鍵領域中使用人類評估的論點。

##### **Reproducibility study of FairAC**
2406.03314v1 by Gijs de Jong, Macha J. Meijer, Derck W. E. Prinzhorn, Harold Ruiter

This work aims to reproduce the findings of the paper "Fair Attribute
Completion on Graph with Missing Attributes" written by Guo, Chu, and Li
arXiv:2302.12977 by investigating the claims made in the paper. This paper
suggests that the results of the original paper are reproducible and thus, the
claims hold. However, the claim that FairAC is a generic framework for many
downstream tasks is very broad and could therefore only be partially tested.
Moreover, we show that FairAC is generalizable to various datasets and
sensitive attributes and show evidence that the improvement in group fairness
of the FairAC framework does not come at the expense of individual fairness.
Lastly, the codebase of FairAC has been refactored and is now easily applicable
for various datasets and models.

摘要：本研究旨在通過調查 Guo、Chu 和 Li 在 arXiv:2302.12977 中撰寫的論文「在屬性缺失的圖表上進行公平屬性完成」中提出的主張，重現該論文的發現。論文表明，原始論文的結果是可以重現的，因此這些主張是成立的。然而，聲稱 FairAC 是許多下游任務的通用框架的說法非常廣泛，因此只能進行部分測試。此外，我們證明 FairAC 可以推廣到各種資料集和敏感屬性，並顯示 FairAC 框架中群體公平性的改進並非以犧牲個人公平性為代價的證據。最後，FairAC 的程式碼庫已經過重構，現在可以輕鬆應用於各種資料集和模型。

##### **The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games**
2406.03299v1 by Mikhail Mozikov, Nikita Severin, Valeria Bodishtianu, Maria Glushanina, Mikhail Baklashkin, Andrey V. Savchenko, Ilya Makarov

Behavior study experiments are an important part of society modeling and
understanding human interactions. In practice, many behavioral experiments
encounter challenges related to internal and external validity,
reproducibility, and social bias due to the complexity of social interactions
and cooperation in human user studies. Recent advances in Large Language Models
(LLMs) have provided researchers with a new promising tool for the simulation
of human behavior. However, existing LLM-based simulations operate under the
unproven hypothesis that LLM agents behave similarly to humans as well as
ignore a crucial factor in human decision-making: emotions.
  In this paper, we introduce a novel methodology and the framework to study
both, the decision-making of LLMs and their alignment with human behavior under
emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two
different classes of behavioral game theory showed that emotions profoundly
impact the performance of LLMs, leading to the development of more optimal
strategies. While there is a strong alignment between the behavioral responses
of GPT-3.5 and human participants, particularly evident in bargaining games,
GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality
decisions. Surprisingly, emotional prompting, particularly with `anger'
emotion, can disrupt the "superhuman" alignment of GPT-4, resembling human
emotional responses.

摘要：行為研究實驗是社會建模和理解人類互動的重要部分。在實務上，許多行為實驗會遇到與內部和外部效度、可複製性以及社會偏見相關的挑戰，這是因為在人類使用者研究中，社會互動和合作很複雜。大型語言模型 (LLM) 的最新進展為研究人員提供了模擬人類行為的有前途的新工具。然而，現有的基於 LLM 的模擬在一個未經證實的假設下運作，即 LLM 代理人的行為與人類類似，而且忽略了人類決策中的一個關鍵因素：情緒。在本文中，我們介紹了一種新的方法和架構，用於研究 LLM 的決策制定及其在情緒狀態下與人類行為的一致性。在兩個不同類別的行為博弈論中，使用 GPT-3.5 和 GPT-4 對四個遊戲進行的實驗表明，情緒會對 LLM 的表現產生深遠的影響，並導致制定更佳的策略。雖然 GPT-3.5 和人類參與者的行為反應之間存在很強的一致性，特別是在討價還價遊戲中很明顯，但 GPT-4 表現出一致的行為，在理性決策中忽略誘發的情緒。令人驚訝的是，情緒提示，特別是帶著「憤怒」情緒，會破壞 GPT-4 的「超人」一致性，類似於人類的情緒反應。

##### **Evaluating AI fairness in credit scoring with the BRIO tool**
2406.03292v1 by Greta Coraglia, Francesco A. Genco, Pellegrino Piantadosi, Enrico Bagli, Pietro Giuffrida, Davide Posillipo, Giuseppe Primiero

We present a method for quantitative, in-depth analyses of fairness issues in
AI systems with an application to credit scoring. To this aim we use BRIO, a
tool for the evaluation of AI systems with respect to social unfairness and,
more in general, ethically undesirable behaviours. It features a model-agnostic
bias detection module, presented in \cite{DBLP:conf/beware/CoragliaDGGPPQ23},
to which a full-fledged unfairness risk evaluation module is added. As a case
study, we focus on the context of credit scoring, analysing the UCI German
Credit Dataset \cite{misc_statlog_(german_credit_data)_144}. We apply the BRIO
fairness metrics to several, socially sensitive attributes featured in the
German Credit Dataset, quantifying fairness across various demographic
segments, with the aim of identifying potential sources of bias and
discrimination in a credit scoring model. We conclude by combining our results
with a revenue analysis.

摘要：我們提出一個方法，用於對 AI 系統中的公平性問題進行定量、深入的分析，並將其應用於信用評分。為此，我們使用 BRIO，一個用於評估 AI 系統在社會不公平性方面的工具，更一般地說，用於評估道德上不可取的行為。它具有一個與模型無關的偏差檢測模組，如 \cite{DBLP:conf/beware/CoragliaDGGPPQ23} 中所示，並添加了一個完整的公平性風險評估模組。作為一個案例研究，我們專注於信用評分的背景，分析 UCI 德國信用資料集 \cite{misc_statlog_(german_credit_data)_144}。我們將 BRIO 公平性指標應用於德國信用資料集中出現的幾個社會敏感屬性，量化各種人口統計區段的公平性，目的是識別信用評分模型中偏差和歧視的潛在來源。我們最後將我們的結果與收入分析結合起來。

##### **SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms**
2406.03287v1 by Xingrun Xing, Zheng Zhang, Ziyi Ni, Shitao Xiao, Yiming Ju, Siqi Fan, Yequan Wang, Jiajun Zhang, Guoqi Li

Towards energy-efficient artificial intelligence similar to the human brain,
the bio-inspired spiking neural networks (SNNs) have advantages of biological
plausibility, event-driven sparsity, and binary activation. Recently,
large-scale language models exhibit promising generalization capability, making
it a valuable issue to explore more general spike-driven models. However, the
binary spikes in existing SNNs fail to encode adequate semantic information,
placing technological challenges for generalization. This work proposes the
first fully spiking mechanism for general language tasks, including both
discriminative and generative ones. Different from previous spikes with {0,1}
levels, we propose a more general spike formulation with bi-directional,
elastic amplitude, and elastic frequency encoding, while still maintaining the
addition nature of SNNs. In a single time step, the spike is enhanced by
direction and amplitude information; in spike frequency, a strategy to control
spike firing rate is well designed. We plug this elastic bi-spiking mechanism
in language modeling, named SpikeLM. It is the first time to handle general
language tasks with fully spike-driven models, which achieve much higher
accuracy than previously possible. SpikeLM also greatly bridges the performance
gap between SNNs and ANNs in language modeling. Our code is available at
https://github.com/Xingrun-Xing/SpikeLM.

摘要：朝向類似人腦的能源效率人工智慧，
受生物啟發的脈衝神經網路 (SNN) 具有生物學可信度、事件驅動的稀疏性以及二進位元活化的優點。最近，
大規模語言模型展現出令人滿意的概化能力，使其成為探索更多一般性脈衝驅動模型的寶貴議題。然而，
現有 SNN 中的二進位元脈衝無法編碼足夠的語義資訊，對概化造成技術挑戰。這項工作提出針對一般語言任務的第一個完整脈衝機制，包括判別與生成任務。不同於先前具有 {0,1} 層級的脈衝，我們提出一個更通用的脈衝公式，具有雙向、彈性振幅和彈性頻率編碼，同時仍維持 SNN 的加法性質。在單一時間步驟中，脈衝會因方向和振幅資訊而增強；在脈衝頻率中，我們精心設計了一個控制脈衝發射速率的策略。我們將這個彈性雙脈衝機制插入語言模型中，稱為 SpikeLM。這是第一次使用完全脈衝驅動模型處理一般語言任務，其達到的準確度遠高於以往。SpikeLM 也大幅縮小了 SNN 和 ANN 在語言模型中的效能差距。我們的程式碼可以在 https://github.com/Xingrun-Xing/SpikeLM 取得。

##### **Enhancing Repository-Level Code Generation with Integrated Contextual Information**
2406.03283v1 by Zhiyuan Pan, Xing Hu, Xin Xia, Xiaohu Yang

Large language models (LLMs) have demonstrated remarkable capabilities in
code generation tasks. However, repository-level code generation presents
unique challenges, particularly due to the need to utilize information spread
across multiple files within a repository. Existing retrieval-based approaches
sometimes fall short as they are limited in obtaining a broader and deeper
repository context. In this paper, we present CatCoder, a novel code generation
framework designed for statically typed programming languages. CatCoder
enhances repository-level code generation by integrating relevant code and type
context. Specifically, it leverages static analyzers to extract type
dependencies and merges this information with retrieved code to create
comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we
adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks.
The results show that CatCoder outperforms the RepoCoder baseline by up to
17.35%, in terms of pass@k score. Furthermore, the generalizability of CatCoder
is assessed using various LLMs, including both code-specialized models and
general-purpose models. Our findings indicate consistent performance
improvements across all models, which underlines the practicality of CatCoder.

摘要：大型語言模型 (LLM) 已在程式碼產生任務中展現出驚人的能力。然而，儲存庫層級的程式碼產生會帶來獨特的挑戰，特別是因為需要利用散佈在儲存庫中多個檔案中的資訊。現有的基於檢索的方法有時會不足，因為它們在取得更廣泛和深入的儲存庫內容方面受到限制。在本文中，我們提出 CatCoder，一個專為靜態類型程式語言設計的新穎程式碼產生架構。CatCoder 透過整合相關程式碼和類型內容來增強儲存庫層級的程式碼產生。具體來說，它利用靜態分析器來提取類型依賴性，並將此資訊與檢索的程式碼合併，以建立 LLM 的全面提示。為了評估 CatCoder 的有效性，我們調整並建構包含 199 個 Java 任務和 90 個 Rust 任務的基準。結果顯示，CatCoder 在 pass@k 分數方面優於 RepoCoder 基準，最多可達 17.35%。此外，CatCoder 的泛化能力使用各種 LLM 來評估，包括程式碼專用模型和通用模型。我們的研究結果表明，所有模型的效能都有持續的提升，這突顯了 CatCoder 的實用性。

##### **FusionBench: A Comprehensive Benchmark of Deep Model Fusion**
2406.03280v1 by Anke Tang, Li Shen, Yong Luo, Han Hu, Bo Do, Dacheng Tao

Deep model fusion is an emerging technique that unifies the predictions or
parameters of several deep neural networks into a single model in a
cost-effective and data-efficient manner. This enables the unified model to
take advantage of the original models' strengths, potentially exceeding their
performance. Although a variety of deep model fusion techniques have been
introduced, their evaluations tend to be inconsistent and often inadequate to
validate their effectiveness and robustness against distribution shifts. To
address this issue, we introduce FusionBench, which is the first comprehensive
benchmark dedicated to deep model fusion. FusionBench covers a wide range of
tasks, including open-vocabulary image classification, text classification, and
text-to-text generation. Each category includes up to eight tasks with
corresponding task-specific models, featuring both full fine-tuning and LoRA
fine-tuning, as well as models of different sizes, to ensure fair and balanced
comparisons of various multi-task model fusion techniques across different
tasks, model scales, and fine-tuning strategies. We implement and evaluate a
broad spectrum of deep model fusion techniques. These techniques range from
model ensemble methods, which combine the predictions to improve the overall
performance, to model merging, which integrates different models into a single
one, and model mixing methods, which upscale or recombine the components of the
original models. FusionBench now contains 26 distinct tasks, 74 fine-tuned
models, and 16 fusion techniques, and we are committed to consistently
expanding the benchmark with more tasks, models, and fusion techniques. In
addition, we offer a well-documented set of resources and guidelines to aid
researchers in understanding and replicating the benchmark results. Homepage
https://tanganke.github.io/fusion_bench/

摘要：深度模型融合是一種新興技術，它以經濟有效且資料有效率的方式，將多個深度神經網路的預測或參數統一到單一模型中。這使統一模型能夠利用原始模型的優勢，並可能超越其效能。儘管已推出各種深度模型融合技術，但其評估往往不一致，且通常不足以驗證其對抗分佈轉移的有效性和穩健性。為了解決這個問題，我們引入了 FusionBench，這是第一個專門用於深度模型融合的綜合基準。FusionBench 涵蓋廣泛的任務，包括開放式詞彙影像分類、文字分類和文字到文字生成。每個類別包含多達八個任務，並附有對應的特定任務模型，採用完整微調和 LoRA 微調，以及不同大小的模型，以確保在不同的任務、模型規模和微調策略中，對各種多任務模型融合技術進行公平且平衡的比較。我們實作並評估了廣泛的深度模型融合技術。這些技術範圍從模型整體方法（結合預測以改善整體效能）到模型合併（將不同的模型整合到單一模型中），以及模型混合方法（提升或重新組合原始模型的組成部分）。FusionBench 目前包含 26 個不同的任務、74 個微調模型和 16 種融合技術，我們致力於持續擴充基準，納入更多任務、模型和融合技術。此外，我們提供了一組文件完善的資源和指南，以協助研究人員了解和複製基準結果。首頁 https://tanganke.github.io/fusion_bench/

##### **Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning**
2406.03276v1 by Mohamed Elsayed, Homayoon Farrahi, Felix Dangel, A. Rupam Mahmood

Second-order information is valuable for many applications but challenging to
compute. Several works focus on computing or approximating Hessian diagonals,
but even this simplification introduces significant additional costs compared
to computing a gradient. In the absence of efficient exact computation schemes
for Hessian diagonals, we revisit an early approximation scheme proposed by
Becker and LeCun (1989, BL89), which has a cost similar to gradients and
appears to have been overlooked by the community. We introduce HesScale, an
improvement over BL89, which adds negligible extra computation. On small
networks, we find that this improvement is of higher quality than all
alternatives, even those with theoretical guarantees, such as unbiasedness,
while being much cheaper to compute. We use this insight in reinforcement
learning problems where small networks are used and demonstrate HesScale in
second-order optimization and scaling the step-size parameter. In our
experiments, HesScale optimizes faster than existing methods and improves
stability through step-size scaling. These findings are promising for scaling
second-order methods in larger models in the future.

摘要：二階資訊對許多應用程式來說很有價值，但計算起來卻很困難。許多研究專注於計算或近似 Hessian 對角線，但即使這種簡化與計算梯度相比，也增加了顯著的額外成本。在沒有針對 Hessian 對角線的有效精確計算方案的情況下，我們重新檢視 Becker 和 LeCun（1989 年，BL89）提出的早期近似方案，其成本類似於梯度，而且似乎已被社群所忽略。我們引入了 HesScale，這是 BL89 的改進版，增加了可忽略的額外計算。在小型網路中，我們發現這種改進比所有替代方案的品質都更高，即使是那些具有理論保證（例如無偏性）的方案也是如此，而且計算成本低得多。我們在使用小型網路的強化學習問題中使用這種見解，並在二階最佳化和縮放步長參數中展示 HesScale。在我們的實驗中，HesScale 最佳化的速度比現有方法快，並透過步長縮放改善穩定性。這些發現對於未來在較大型模型中縮放二階方法很有前景。

##### **Enhancing CTC-based speech recognition with diverse modeling units**
2406.03274v1 by Shiyi Han, Zhihong Lei, Mingbin Xu, Xingyu Na, Zhen Huang

In recent years, the evolution of end-to-end (E2E) automatic speech
recognition (ASR) models has been remarkable, largely due to advances in deep
learning architectures like transformer. On top of E2E systems, researchers
have achieved substantial accuracy improvement by rescoring E2E model's N-best
hypotheses with a phoneme-based model. This raises an interesting question
about where the improvements come from other than the system combination
effect. We examine the underlying mechanisms driving these gains and propose an
efficient joint training approach, where E2E models are trained jointly with
diverse modeling units. This methodology does not only align the strengths of
both phoneme and grapheme-based models but also reveals that using these
diverse modeling units in a synergistic way can significantly enhance model
accuracy. Our findings offer new insights into the optimal integration of
heterogeneous modeling units in the development of more robust and accurate ASR
systems.

摘要：近年來，端對端（E2E）自動語音辨識（ASR）模型的演進備受矚目，這在很大程度上要歸功於深度學習架構（例如 Transformer）的進步。在 E2E 系統之上，研究人員透過使用基於音素的模型重新評分 E2E 模型的 N 個最佳假設，進而大幅提升了準確度。這引發了一個有趣的問題，即除了系統組合效應之外，改進來自何處。我們探討了驅動這些增益的底層機制，並提出了一種高效的聯合訓練方法，其中 E2E 模型與多樣化的建模單元聯合訓練。這種方法不僅結合了基於音素和音素的模型的優點，還揭示了以協同方式使用這些多樣化的建模單元可以顯著提升模型準確度。我們的研究結果為在開發更強大、更準確的 ASR 系統時，如何最佳整合異質建模單元提供了新的見解。

##### **Multi-Microphone Speech Emotion Recognition using the Hierarchical Token-semantic Audio Transformer Architecture**
2406.03272v1 by Ohad Cohen, Gershon Hazan, Sharon Gannot

Most emotion recognition systems fail in real-life situations (in the wild
scenarios) where the audio is contaminated by reverberation. Our study explores
new methods to alleviate the performance degradation of Speech Emotion
Recognition (SER) algorithms and develop a more robust system for adverse
conditions. We propose processing multi-microphone signals to address these
challenges and improve emotion classification accuracy. We adopt a
state-of-the-art transformer model, the Hierarchical Token-semantic Audio
Transformer (HTS-AT), to handle multi-channel audio inputs. We evaluate two
strategies: averaging mel-spectrograms across channels and summing
patch-embedded representations. Our multimicrophone model achieves superior
performance compared to single-channel baselines when tested on real-world
reverberant environments.

摘要：大多數情緒辨識系統在真實生活情境（在野外情境）中，當音訊受到殘響汙染時，都會失效。我們的研究探討了減輕語音情緒辨識 (SER) 演算法效能降低的新方法，並為惡劣條件開發更強健的系統。我們提出處理多麥克風訊號以應對這些挑戰並提升情緒分類準確度。我們採用最先進的轉換器模型，分層標記語意音訊轉換器 (HTS-AT)，來處理多聲道音訊輸入。我們評估了兩種策略：在各聲道間平均梅爾頻譜圖，以及對區塊嵌入式表示進行加總。我們的多麥克風模型在真實世界殘響環境中進行測試時，與單聲道基準線相比，達到了卓越的效能。

##### **Deep Generative Models for Proton Zero Degree Calorimeter Simulations in ALICE, CERN**
2406.03263v1 by Patryk Będkowski, Jan Dubiński, Kamil Deja, Przemysław Rokita

Simulating detector responses is a crucial part of understanding the
inner-workings of particle collisions in the Large Hadron Collider at CERN. The
current reliance on statistical Monte-Carlo simulations strains CERN's
computational grid, underscoring the urgency for more efficient alternatives.
Addressing these challenges, recent proposals advocate for generative machine
learning methods. In this study, we present an innovative deep learning
simulation approach tailored for the proton Zero Degree Calorimeter in the
ALICE experiment. Leveraging a Generative Adversarial Network model with
Selective Diversity Increase loss, we directly simulate calorimeter responses.
To enhance its capabilities in modeling a broad range of calorimeter response
intensities, we expand the SDI-GAN architecture with additional regularization.
Moreover, to improve the spatial fidelity of the generated data, we introduce
an auxiliary regressor network. Our method offers a significant speedup when
comparing to the traditional Monte-Carlo based approaches.

摘要：模擬偵測器反應是了解大型強子對撞機中粒子碰撞內部運作的重要部分。目前的統計蒙特卡羅模擬依賴性會對 CERN 的計算網格造成負擔，這凸顯了對更有效率的替代方案的迫切需求。為了應對這些挑戰，最近的提案主張使用生成式機器學習方法。在本研究中，我們提出了一種創新的深度學習模擬方法，專門針對 ALICE 實驗中的質子零度量能器。我們利用具有選擇性多樣性增加損失的生成對抗網路模型，直接模擬量能器反應。為了增強其在建模各種量能器反應強度方面的能力，我們使用額外的正則化來擴展 SDI-GAN 架構。此外，為了提高生成資料的空間保真度，我們引入了輔助回歸器網路。與傳統的基於蒙特卡羅的方法相比，我們的這種方法提供了顯著的加速。

##### **Prompt-based Visual Alignment for Zero-shot Policy Transfer**
2406.03250v1 by Haihan Gao, Rui Zhang, Qi Yi, Hantao Yao, Haochen Li, Jiaming Guo, Shaohui Peng, Yunkai Gao, QiCheng Wang, Xing Hu, Yuanbo Wen, Zihao Zhang, Zidong Du, Ling Li, Qi Guo, Yunji Chen

Overfitting in RL has become one of the main obstacles to applications in
reinforcement learning(RL). Existing methods do not provide explicit semantic
constrain for the feature extractor, hindering the agent from learning a
unified cross-domain representation and resulting in performance degradation on
unseen domains. Besides, abundant data from multiple domains are needed. To
address these issues, in this work, we propose prompt-based visual alignment
(PVA), a robust framework to mitigate the detrimental domain bias in the image
for zero-shot policy transfer. Inspired that Visual-Language Model (VLM) can
serve as a bridge to connect both text space and image space, we leverage the
semantic information contained in a text sequence as an explicit constraint to
train a visual aligner. Thus, the visual aligner can map images from multiple
domains to a unified domain and achieve good generalization performance. To
better depict semantic information, prompt tuning is applied to learn a
sequence of learnable tokens. With explicit constraints of semantic
information, PVA can learn unified cross-domain representation under limited
access to cross-domain data and achieves great zero-shot generalization ability
in unseen domains. We verify PVA on a vision-based autonomous driving task with
CARLA simulator. Experiments show that the agent generalizes well on unseen
domains under limited access to multi-domain data.

摘要：RL 中的过拟合已成为强化学习 (RL) 应用的主要障碍之一。现有方法没有为特征提取器提供明确的语义约束，从而阻碍代理学习统一的跨域表示，并导致在未见域上性能下降。此外，需要来自多个域的大量数据。为了解决这些问题，在这项工作中，我们提出了基于提示的视觉对齐 (PVA)，这是一个稳健的框架，用于减轻图像中对零镜头策略转移的有害域偏差。受视觉语言模型 (VLM) 可以作为连接文本空间和图像空间的桥梁的启发，我们利用文本序列中包含的语义信息作为显式约束来训练视觉对齐器。因此，视觉对齐器可以将来自多个域的图像映射到一个统一的域，并实现良好的泛化性能。为了更好地描述语义信息，应用提示调整来学习一系列可学习的标记。通过语义信息的显式约束，PVA 可以在对跨域数据访问有限的情况下学习统一的跨域表示，并在未见域中实现出色的零镜头泛化能力。我们在基于视觉的自动驾驶任务中使用 CARLA 模拟器验证了 PVA。实验表明，在对多域数据访问有限的情况下，该代理在未见域上具有良好的泛化能力。

##### **Large Language Models as Evaluators for Recommendation Explanations**
2406.03248v1 by Xiaoyu Zhang, Yishan Li, Jiayin Wang, Bowen Sun, Weizhi Ma, Peijie Sun, Min Zhang

The explainability of recommender systems has attracted significant attention
in academia and industry. Many efforts have been made for explainable
recommendations, yet evaluating the quality of the explanations remains a
challenging and unresolved issue. In recent years, leveraging LLMs as
evaluators presents a promising avenue in Natural Language Processing tasks
(e.g., sentiment classification, information extraction), as they perform
strong capabilities in instruction following and common-sense reasoning.
However, evaluating recommendation explanatory texts is different from these
NLG tasks, as its criteria are related to human perceptions and are usually
subjective. In this paper, we investigate whether LLMs can serve as evaluators
of recommendation explanations. To answer the question, we utilize real user
feedback on explanations given from previous work and additionally collect
third-party annotations and LLM evaluations. We design and apply a 3-level meta
evaluation strategy to measure the correlation between evaluator labels and the
ground truth provided by users. Our experiments reveal that LLMs, such as GPT4,
can provide comparable evaluations with appropriate prompts and settings. We
also provide further insights into combining human labels with the LLM
evaluation process and utilizing ensembles of multiple heterogeneous LLM
evaluators to enhance the accuracy and stability of evaluations. Our study
verifies that utilizing LLMs as evaluators can be an accurate, reproducible and
cost-effective solution for evaluating recommendation explanation texts. Our
code is available at https://github.com/Xiaoyu-SZ/LLMasEvaluator.

摘要：推薦系統的可解釋性已在學術界和產業中引起廣泛關注。許多努力都致力於可解釋的推薦，然而評估解釋品質仍然是一個具有挑戰性和尚未解決的問題。近年來，利用 LLM 作為評估者在自然語言處理任務（例如情緒分類、資訊萃取）中提供了一條有前途的途徑，因為它們在遵循指令和常識推理方面表現出強大的能力。然而，評估推薦說明文字與這些 NLG 任務不同，因為它的標準與人類的感知有關，而且通常是主觀的。在本文中，我們探討 LLM 是否可以作為推薦說明的評估者。為了回答這個問題，我們利用真實使用者對先前工作提供的說明的回饋，並另外收集第三方註解和 LLM 評估。我們設計並應用了一個 3 級元評估策略來衡量評估者標籤與使用者提供的真實資料之間的關聯性。我們的實驗表明，LLM（例如 GPT4）可以提供具有適當提示和設定的比較評估。我們還進一步深入探討將人類標籤與 LLM 評估過程相結合，並利用多個異質 LLM 評估者的集合來提高評估的準確性和穩定性。我們的研究驗證了利用 LLM 作為評估者可以成為評估推薦說明文字的準確、可複製且具有成本效益的解決方案。我們的程式碼可在 https://github.com/Xiaoyu-SZ/LLMasEvaluator 取得。

##### **Error-preserving Automatic Speech Recognition of Young English Learners' Language**
2406.03235v1 by Janick Michot, Manuela Hürlimann, Jan Deriu, Luzia Sauer, Katsiaryna Mlynchyk, Mark Cieliebak

One of the central skills that language learners need to practice is speaking
the language. Currently, students in school do not get enough speaking
opportunities and lack conversational practice. Recent advances in speech
technology and natural language processing allow for the creation of novel
tools to practice their speaking skills. In this work, we tackle the first
component of such a pipeline, namely, the automated speech recognition module
(ASR), which faces a number of challenges: first, state-of-the-art ASR models
are often trained on adult read-aloud data by native speakers and do not
transfer well to young language learners' speech. Second, most ASR systems
contain a powerful language model, which smooths out errors made by the
speakers. To give corrective feedback, which is a crucial part of language
learning, the ASR systems in our setting need to preserve the errors made by
the language learners. In this work, we build an ASR system that satisfies
these requirements: it works on spontaneous speech by young language learners
and preserves their errors. For this, we collected a corpus containing around
85 hours of English audio spoken by learners in Switzerland from grades 4 to 6
on different language learning tasks, which we used to train an ASR model. Our
experiments show that our model benefits from direct fine-tuning on children's
voices and has a much higher error preservation rate than other models.

摘要：語言學習者需要練習的核心技能之一是說出
這門語言。目前，學校的學生沒有足夠的機會說出
並缺乏會話練習。語音技術和自然語言處理的最新進展允許創建新穎
工具來練習他們的口語技能。在這項工作中，我們解決了這種管道的第一個
組成部分，即自動語音識別模組
(ASR)，它面臨許多挑戰：首先，最先進的 ASR 模型
通常由母語人士訓練成人朗讀資料，且無法順利轉移至年輕語言學習者的語音。其次，大多數 ASR 系統
包含強大的語言模型，可平滑出說話者所犯的錯誤。為了提供矯正回饋，這是語言
學習的關鍵部分，我們環境中的 ASR 系統需要保留語言學習者所犯的錯誤。在這項工作中，我們建立了一個 ASR 系統，滿足
這些需求：它適用於年輕語言學習者的自發性語音，並保留他們的錯誤。為此，我們收集了一個包含約
85 小時的英語音訊語料庫，由瑞士 4 到 6 年級的學習者在不同的語言學習任務中說出，我們用它來訓練 ASR 模型。我們的
實驗顯示，我們的模型受益於對兒童聲音的直接微調，且錯誤保留率遠高於其他模型。

##### **Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning**
2406.03234v1 by Inwoo Hwang, Yunhyeok Kwak, Suhyung Choi, Byoung-Tak Zhang, Sanghack Lee

Causal dynamics learning has recently emerged as a promising approach to
enhancing robustness in reinforcement learning (RL). Typically, the goal is to
build a dynamics model that makes predictions based on the causal relationships
among the entities. Despite the fact that causal connections often manifest
only under certain contexts, existing approaches overlook such fine-grained
relationships and lack a detailed understanding of the dynamics. In this work,
we propose a novel dynamics model that infers fine-grained causal structures
and employs them for prediction, leading to improved robustness in RL. The key
idea is to jointly learn the dynamics model with a discrete latent variable
that quantizes the state-action space into subgroups. This leads to recognizing
meaningful context that displays sparse dependencies, where causal structures
are learned for each subgroup throughout the training. Experimental results
demonstrate the robustness of our method to unseen states and locally spurious
correlations in downstream tasks where fine-grained causal reasoning is
crucial. We further illustrate the effectiveness of our subgroup-based approach
with quantization in discovering fine-grained causal relationships compared to
prior methods.

摘要：因果动力学学习最近已成为一种有前景的方法，可以增强强化学习 (RL) 中的鲁棒性。通常，目标是建立一个基于实体之间因果关系进行预测的动力学模型。尽管因果联系通常仅在某些情况下才会显现，但现有方法却忽略了这种细粒度的关系，并且缺乏对动力学的详细理解。在这项工作中，我们提出了一种新的动力学模型，该模型推断出细粒度的因果结构并将其用于预测，从而提高了 RL 中的鲁棒性。关键思想是将动力学模型与离散潜在变量联合学习，该变量将状态动作空间量化为子组。这导致识别显示稀疏依赖关系的有意义的上下文，其中在整个训练过程中为每个子组学习因果结构。实验结果证明了我们的方法对下游任务中未见状态和局部虚假相关性的鲁棒性，其中细粒度的因果推理至关重要。我们进一步说明了基于子组的方法与量化相结合的有效性，与先前的方法相比，它在发现细粒度的因果关系方面具有优势。

##### **Global Clipper: Enhancing Safety and Reliability of Transformer-based Object Detection Models**
2406.03229v1 by Qutub Syed Sha, Michael Paulitsch, Karthik Pattabiraman, Korbinian Hagn, Fabian Oboril, Cornelius Buerkle, Kay-Ulrich Scholl, Gereon Hinz, Alois Knoll

As transformer-based object detection models progress, their impact in
critical sectors like autonomous vehicles and aviation is expected to grow.
Soft errors causing bit flips during inference have significantly impacted DNN
performance, altering predictions. Traditional range restriction solutions for
CNNs fall short for transformers. This study introduces the Global Clipper and
Global Hybrid Clipper, effective mitigation strategies specifically designed
for transformer-based models. It significantly enhances their resilience to
soft errors and reduces faulty inferences to ~ 0\%. We also detail extensive
testing across over 64 scenarios involving two transformer models (DINO-DETR
and Lite-DETR) and two CNN models (YOLOv3 and SSD) using three datasets,
totalling approximately 3.3 million inferences, to assess model robustness
comprehensively. Moreover, the paper explores unique aspects of attention
blocks in transformers and their operational differences from CNNs.

摘要：隨著基於Transformer的物件偵測模型的進展，預計它們在自動駕駛車輛和航空等關鍵領域的影響將會增加。在推論期間導致位元翻轉的軟錯誤已顯著影響 DNN 效能，並改變預測。傳統的 CNN 範圍限制解決方案對於Transformer來說還不夠。本研究介紹了 Global Clipper 和 Global Hybrid Clipper，這兩種專門為基於Transformer的模型設計的有效緩解策略。它顯著增強了它們對軟錯誤的復原力，並將錯誤推論減少到約 0%。我們還詳細說明了使用三個資料集的 64 個以上情境中的廣泛測試，其中涉及兩個Transformer模型（DINO-DETR 和 Lite-DETR）和兩個 CNN 模型（YOLOv3 和 SSD），總計約 330 萬個推論，以全面評估模型的穩健性。此外，本文探討了Transformer中注意力區塊的獨特面向及其與 CNN 的運作差異。

##### **Challenges and Considerations in the Evaluation of Bayesian Causal Discovery**
2406.03209v1 by Amir Mohammad Karimi Mamaghan, Panagiotis Tigas, Karl Henrik Johansson, Yarin Gal, Yashas Annadani, Stefan Bauer

Representing uncertainty in causal discovery is a crucial component for
experimental design, and more broadly, for safe and reliable causal decision
making. Bayesian Causal Discovery (BCD) offers a principled approach to
encapsulating this uncertainty. Unlike non-Bayesian causal discovery, which
relies on a single estimated causal graph and model parameters for assessment,
evaluating BCD presents challenges due to the nature of its inferred quantity -
the posterior distribution. As a result, the research community has proposed
various metrics to assess the quality of the approximate posterior. However,
there is, to date, no consensus on the most suitable metric(s) for evaluation.
In this work, we reexamine this question by dissecting various metrics and
understanding their limitations. Through extensive empirical evaluation, we
find that many existing metrics fail to exhibit a strong correlation with the
quality of approximation to the true posterior, especially in scenarios with
low sample sizes where BCD is most desirable. We highlight the suitability (or
lack thereof) of these metrics under two distinct factors: the identifiability
of the underlying causal model and the quantity of available data. Both factors
affect the entropy of the true posterior, indicating that the current metrics
are less fitting in settings of higher entropy. Our findings underline the
importance of a more nuanced evaluation of new methods by taking into account
the nature of the true posterior, as well as guide and motivate the development
of new evaluation procedures for this challenge.

摘要：在因果發現中表示不確定性是實驗設計中至關重要的一環，更廣泛地說，對於安全且可靠的因果決策制定而言也是如此。貝氏因果發現 (BCD) 提供了一個原則性的方法來概括這種不確定性。與依賴單個估計因果圖和模型參數進行評估的非貝氏因果發現不同，評估 BCD 由於其推論量的性質（後驗分佈）而面臨挑戰。因此，研究社群提出了各種指標來評估近似後驗的品質。然而，到目前為止，對於最適合評估的指標尚未達成共識。在這項工作中，我們透過剖析各種指標並瞭解其限制來重新審視這個問題。透過廣泛的經驗評估，我們發現許多現有指標無法展現與逼近真實後驗品質的強相關性，特別是在 BCD 最理想的低樣本量場景中。我們重點說明了這些指標在兩個不同因素下的適用性（或缺乏適用性）：基礎因果模型的可識別性以及可用資料的數量。這兩個因素都會影響真實後驗的熵，這表示目前的指標在較高熵的設定中較不合適。我們的發現強調了更細緻地評估新方法的重要性，方法是考量真實後驗的性質，並指導和激勵開發新的評估程序來應對此挑戰。

##### **ChatLang-8: An LLM-Based Synthetic Data Generation Framework for Grammatical Error Correction**
2406.03202v1 by Jeiyoon Park, Chanjun Park, Heuiseok Lim

We explore and improve the capabilities of LLMs to generate data for
grammatical error correction (GEC). When merely producing parallel sentences,
their patterns are too simplistic to be valuable as a corpus. To address this
issue, we propose an automated framework that includes a Subject Selector,
Grammar Selector, Prompt Manager, and Evaluator. Additionally, we introduce a
new dataset for GEC tasks, named \textbf{ChatLang-8}, which encompasses eight
types of subject nouns and 23 types of grammar. It consists of 1 million pairs
featuring human-like grammatical errors. Our experiments reveal that ChatLang-8
exhibits a more uniform pattern composition compared to existing GEC datasets.
Furthermore, we observe improved model performance when using ChatLang-8
instead of existing GEC datasets. The experimental results suggest that our
framework and ChatLang-8 are valuable resources for enhancing ChatGPT's data
generation capabilities.

摘要：我們探討並改善 LLM 生成語法錯誤修正 (GEC) 資料的能力。當僅產生平行句子時，其模式過於簡化，無法作為語料庫有價值。為了解決這個問題，我們提出一個自動化框架，其中包含主詞選擇器、語法選擇器、提示管理員和評估器。此外，我們還引入一個新的 GEC 任務資料集，稱為 \textbf{ChatLang-8}，它包含八種類型的名詞和 23 種類型的語法。它包含 100 萬對具有類似人類語法錯誤的句子。我們的實驗表明，與現有的 GEC 資料集相比，ChatLang-8 呈現出更統一的模式組成。此外，我們觀察到在使用 ChatLang-8 而不是現有的 GEC 資料集時，模型效能有所提升。實驗結果表明，我們的框架和 ChatLang-8 是增強 ChatGPT 資料產生能力的寶貴資源。

##### **Situation Monitor: Diversity-Driven Zero-Shot Out-of-Distribution Detection using Budding Ensemble Architecture for Object Detection**
2406.03188v1 by Qutub Syed, Michael Paulitsch, Korbinian Hagn, Neslihan Kose Cihangir, Kay-Ulrich Scholl, Fabian Oboril, Gereon Hinz, Alois Knoll

We introduce Situation Monitor, a novel zero-shot Out-of-Distribution (OOD)
detection approach for transformer-based object detection models to enhance
reliability in safety-critical machine learning applications such as autonomous
driving. The Situation Monitor utilizes the Diversity-based Budding Ensemble
Architecture (DBEA) and increases the OOD performance by integrating a
diversity loss into the training process on top of the budding ensemble
architecture, detecting Far-OOD samples and minimizing false positives on
Near-OOD samples. Moreover, utilizing the resulting DBEA increases the model's
OOD performance and improves the calibration of confidence scores, particularly
concerning the intersection over union of the detected objects. The DBEA model
achieves these advancements with a 14% reduction in trainable parameters
compared to the vanilla model. This signifies a substantial improvement in
efficiency without compromising the model's ability to detect OOD instances and
calibrate the confidence scores accurately.

摘要：我們引入了情境監控器，一種新穎的零次學習異質分佈 (OOD) 偵測方法，適用於基於Transformer的物件偵測模型，以提升安全關鍵機器學習應用程式的可靠性，例如自動駕駛。情境監控器利用基於多樣性的萌芽整體架構 (DBEA)，並透過將多樣性損失整合至萌芽整體架構的訓練過程中，提升 OOD 效能，偵測遠 OOD 樣本並將近 OOD 樣本的誤報降至最低。此外，利用產生的 DBEA 可提升模型的 OOD 效能，並改善信心分數的校準，特別是關於偵測物件的聯集相交。與香草模型相比，DBEA 模型在可訓練參數方面減少了 14%，進而達成這些進展。這表示在不影響模型偵測 OOD 實例和準確校準信心分數的能力下，大幅提升了效率。

##### **Missci: Reconstructing Fallacies in Misrepresented Science**
2406.03181v1 by Max Glockner, Yufang Hou, Preslav Nakov, Iryna Gurevych

Health-related misinformation on social networks can lead to poor
decision-making and real-world dangers. Such misinformation often misrepresents
scientific publications and cites them as "proof" to gain perceived
credibility. To effectively counter such claims automatically, a system must
explain how the claim was falsely derived from the cited publication. Current
methods for automated fact-checking or fallacy detection neglect to assess the
(mis)used evidence in relation to misinformation claims, which is required to
detect the mismatch between them. To address this gap, we introduce Missci, a
novel argumentation theoretical model for fallacious reasoning together with a
new dataset for real-world misinformation detection that misrepresents
biomedical publications. Unlike previous fallacy detection datasets, Missci (i)
focuses on implicit fallacies between the relevant content of the cited
publication and the inaccurate claim, and (ii) requires models to verbalize the
fallacious reasoning in addition to classifying it. We present Missci as a
dataset to test the critical reasoning abilities of large language models
(LLMs), that are required to reconstruct real-world fallacious arguments, in a
zero-shot setting. We evaluate two representative LLMs and the impact of
different levels of detail about the fallacy classes provided to the LLM via
prompts. Our experiments and human evaluation show promising results for GPT 4,
while also demonstrating the difficulty of this task.

摘要：社交網路上的健康相關錯誤資訊可能會導致錯誤的決策和現實世界的危險。這種錯誤資訊通常會曲解科學出版物，並將其引用為獲得感知可信度的「證明」。為了有效自動反駁此類說法，系統必須說明該說法是如何從引用的出版物中錯誤衍生的。目前用於自動事實查核或謬誤檢測的方法忽略了評估與錯誤資訊說法相關的（錯誤）使用的證據，這是檢測它們之間不匹配所必需的。為了解決這個差距，我們引入了 Missci，一個新的論證理論模型，用於謬誤推理，並提供了一個新的用於曲解生物醫學出版物的真實世界錯誤資訊檢測的資料集。與先前的謬誤檢測資料集不同，Missci (i) 專注於引用的出版物相關內容與不準確說法之間的隱含謬誤，以及 (ii) 要求模型除了對謬誤推理進行分類外，還要將其表達出來。我們將 Missci 呈現為一個資料集，用於測試大型語言模型 (LLM) 的批判性推理能力，這些模型需要在零次學習設置中重建真實世界的謬誤論證。我們評估了兩個具有代表性的 LLM 以及通過提示向 LLM 提供不同詳細程度的謬誤類別的影響。我們的實驗和人工評估顯示出 GPT 4 的有希望的結果，同時也證明了這項任務的難度。

##### **StatBot.Swiss: Bilingual Open Data Exploration in Natural Language**
2406.03170v1 by Farhad Nooralahzadeh, Yi Zhang, Ellery Smith, Sabine Maennel, Cyril Matthey-Doret, Raphaël de Fondville, Kurt Stockinger

The potential for improvements brought by Large Language Models (LLMs) in
Text-to-SQL systems is mostly assessed on monolingual English datasets.
However, LLMs' performance for other languages remains vastly unexplored. In
this work, we release the StatBot.Swiss dataset, the first bilingual benchmark
for evaluating Text-to-SQL systems based on real-world applications. The
StatBot.Swiss dataset contains 455 natural language/SQL-pairs over 35 big
databases with varying level of complexity for both English and German.
  We evaluate the performance of state-of-the-art LLMs such as GPT-3.5-Turbo
and mixtral-8x7b-instruct for the Text-to-SQL translation task using an
in-context learning approach. Our experimental analysis illustrates that
current LLMs struggle to generalize well in generating SQL queries on our novel
bilingual dataset.

摘要：大型語言模型（LLM）在文字轉 SQL 系統中帶來的潛在改進，大多評估於單語系的英文資料集。然而，LLM 在其他語言的表現仍然鮮少被探討。在這項工作中，我們發布 StatBot.Swiss 資料集，這是第一個基於真實世界應用來評估文字轉 SQL 系統的雙語基準。StatBot.Swiss 資料集包含 455 組自然語言/SQL 對，涵蓋 35 個大型資料庫，其複雜程度對於英文和德文來說都有所不同。我們評估了最先進的 LLM（例如 GPT-3.5-Turbo 和 mixtral-8x7b-instruct）在文字轉 SQL 翻譯任務中的表現，並使用情境學習法。我們的實驗分析說明，目前的 LLM 在我們新穎的雙語資料集上產生 SQL 查詢時，難以很好地概化。

##### **CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs**
2406.03158v1 by Shuang Ao, Stefan Rueger, Advaith Siddharthan

Despite the impressive capability of large language models (LLMs), knowing
when to trust their generations remains an open challenge. The recent
literature on uncertainty quantification of natural language generation (NLG)
utilises a conventional natural language inference (NLI) classifier to measure
the semantic dispersion of LLMs responses. These studies employ logits of NLI
classifier for semantic clustering to estimate uncertainty. However, logits
represent the probability of the predicted class and barely contain feature
information for potential clustering. Alternatively, CLIP (Contrastive
Language-Image Pre-training) performs impressively in extracting image-text
pair features and measuring their similarity. To extend its usability, we
propose Contrastive Semantic Similarity, the CLIP-based feature extraction
module to obtain similarity features for measuring uncertainty for text pairs.
We apply this method to selective NLG, which detects and rejects unreliable
generations for better trustworthiness of LLMs. We conduct extensive
experiments with three LLMs on several benchmark question-answering datasets
with comprehensive evaluation metrics. Results show that our proposed method
performs better in estimating reliable responses of LLMs than comparable
baselines. Results show that our proposed method performs better in estimating
reliable responses of LLMs than comparable baselines. The code are available at
\url{https://github.com/AoShuang92/css_uq_llms}.

摘要：儘管大型語言模型 (LLM) 具有令人印象深刻的能力，但何時相信它們的世代仍然是一個公開的挑戰。最近關於自然語言生成 (NLG) 不確定性量化的文獻利用傳統的自然語言推論 (NLI) 分類器來測量 LLM 回應的語義分散。這些研究採用 NLI 分類器的 logit 進行語義聚類以估計不確定性。然而，logit 代表預測類別的機率，幾乎不包含潛在聚類的特徵資訊。或者，CLIP（對比語言影像預訓練）在提取影像文字對特徵和測量它們的相似性方面表現出色。為了擴展其可用性，我們提出對比語義相似性，一個基於 CLIP 的特徵提取模組，用於獲取相似性特徵以測量文字對的不確定性。我們將此方法應用於選擇性 NLG，它偵測並拒絕不可靠的世代，以提高 LLM 的可信度。我們對三個 LLM 進行了廣泛的實驗，採用了幾個基准問題回答資料集和全面的評估指標。結果表明，與可比較的基準相比，我們提出的方法在估計 LLM 的可靠回應方面表現得更好。結果表明，與可比較的基準相比，我們提出的方法在估計 LLM 的可靠回應方面表現得更好。程式碼可在 \url{https://github.com/AoShuang92/css_uq_llms} 取得。

##### **Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks: An Extended Investigation**
2406.03154v1 by Marvin Schmitt, Paul-Christian Bürkner, Ullrich Köthe, Stefan T. Radev

Recent advances in probabilistic deep learning enable efficient amortized
Bayesian inference in settings where the likelihood function is only implicitly
defined by a simulation program (simulation-based inference; SBI). But how
faithful is such inference if the simulation represents reality somewhat
inaccurately, that is, if the true system behavior at test time deviates from
the one seen during training? We conceptualize the types of such model
misspecification arising in SBI and systematically investigate how the
performance of neural posterior approximators gradually deteriorates as a
consequence, making inference results less and less trustworthy. To notify
users about this problem, we propose a new misspecification measure that can be
trained in an unsupervised fashion (i.e., without training data from the true
distribution) and reliably detects model misspecification at test time. Our
experiments clearly demonstrate the utility of our new measure both on toy
examples with an analytical ground-truth and on representative scientific tasks
in cell biology, cognitive decision making, disease outbreak dynamics, and
computer vision. We show how the proposed misspecification test warns users
about suspicious outputs, raises an alarm when predictions are not trustworthy,
and guides model designers in their search for better simulators.

摘要：最近在概率深度学习中的进步让有效摊销的贝氏推理在似然函数仅由模拟程序隐式定义的设定中成为可能（基于模拟的推理；SBI）。但是，如果模拟在某种程度上不准确地反映现实，即如果测试时的真实系统行为偏离训练期间观察到的行为，那么这种推理有多么可靠？我们概念化了 SBI 中出现的此类模型错误规范的类型，并系统地研究了神经后验近似器的性能如何因此而逐渐恶化，从而使得推理结果越来越不可靠。为了通知用户此问题，我们提出了一个新的错误规范度量，该度量可以在无监督的方式下训练（即，无需来自真实分布的训练数据），并在测试时可靠地检测到模型错误规范。我们的实验清楚地证明了我们新度量的效用，既在具有分析性基本事实的玩具示例上，也在细胞生物学、认知决策制定、疾病爆发动态和计算机视觉中的代表性科学任务上。我们展示了所提出的错误规范测试如何警告用户可疑的输出，在预测不可靠时发出警报，并在寻找更好的模拟器的过程中指导模型设计者。

##### **Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation**
2406.03151v1 by Hao Li, Yuping Wu, Viktor Schlegel, Riza Batista-Navarro, Tharindu Madusanka, Iqra Zahid, Jiayan Zeng, Xiaochi Wang, Xinran He, Yizhi Li, Goran Nenadic

With the recent advances of large language models (LLMs), it is no longer
infeasible to build an automated debate system that helps people to synthesise
persuasive arguments. Previous work attempted this task by integrating multiple
components. In our work, we introduce an argument mining dataset that captures
the end-to-end process of preparing an argumentative essay for a debate, which
covers the tasks of claim and evidence identification (Task 1 ED), evidence
convincingness ranking (Task 2 ECR), argumentative essay summarisation and
human preference ranking (Task 3 ASR) and metric learning for automated
evaluation of resulting essays, based on human feedback along argument quality
dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are
fully annotated with the various properties supporting the aforementioned
tasks. We evaluate multiple generative baselines for each of these tasks,
including representative LLMs. We find, that while they show promising results
on individual tasks in our benchmark, their end-to-end performance on all four
tasks in succession deteriorates significantly, both in automated measures as
well as in human-centred evaluation. This challenge presented by our proposed
dataset motivates future research on end-to-end argument mining and
summarisation. The repository of this project is available at
https://github.com/HarrywillDr/ArgSum-Datatset

摘要：<paragraph>隨著大型語言模型 (LLM) 的最新進展，建立一個自動化辯論系統來幫助人們綜合有說服力的論點已不再不可行。先前的研究嘗試透過整合多個組成部分來達成這項任務。在我們的研究中，我們引入了一個論點挖掘資料集，它擷取了為辯論準備論證文章的端對端流程，其中涵蓋了論點和證據識別（任務 1 ED）、證據說服力排名（任務 2 ECR）、論證文章摘要和人類偏好排名（任務 3 ASR）以及基於人類回饋的論證品質維度對結果文章進行自動化評估的度量學習（任務 4 SQE）。我們的資料集包含 14k 個論點範例，並針對支援上述任務的各種屬性進行了完整的註解。我們針對這些任務中的每一個任務評估了多個生成基準，包括具代表性的 LLM。我們發現，儘管它們在基準中的個別任務上表現出有希望的結果，但它們在連續四個任務上的端對端效能卻顯著下降，無論是在自動化測量還是以人為中心的評估中都是如此。我們提出的資料集所呈現的這個挑戰，激勵了未來對端對端論點挖掘和摘要的研究。此專案的存放庫可以在 https://github.com/HarrywillDr/ArgSum-Datatset 取得</paragraph>

##### **Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models**
2406.03136v1 by Jerry Yao-Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, Han Liu

We study the computational limits of Low-Rank Adaptation (LoRA) update for
finetuning transformer-based models using fine-grained complexity theory. Our
key observation is that the existence of low-rank decompositions within the
gradient computation of LoRA adaptation leads to possible algorithmic speedup.
This allows us to (i) identify a phase transition behavior and (ii) prove the
existence of nearly linear algorithms by controlling the LoRA update
computation term by term, assuming the Strong Exponential Time Hypothesis
(SETH). For the former, we identify a sharp transition in the efficiency of all
possible rank-$r$ LoRA update algorithms for transformers, based on specific
norms resulting from the multiplications of the input sequence $\mathbf{X}$,
pretrained weights $\mathbf{W^\star}$, and adapter matrices $\alpha \mathbf{B}
\mathbf{A} / r$. Specifically, we derive a shared upper bound threshold for
such norms and show that efficient (sub-quadratic) approximation algorithms of
LoRA exist only below this threshold. For the latter, we prove the existence of
nearly linear approximation algorithms for LoRA adaptation by utilizing the
hierarchical low-rank structures of LoRA gradients and approximating the
gradients with a series of chained low-rank approximations. To showcase our
theory, we consider two practical scenarios: partial (e.g., only $\mathbf{W}_V$
and $\mathbf{W}_Q$) and full adaptations (e.g., $\mathbf{W}_Q$, $\mathbf{W}_V$,
and $\mathbf{W}_K$) of weights in attention heads.

摘要：<paragraph>我們使用精細的複雜性理論研究低秩適應 (LoRA) 更新的計算限制，以微調基於轉換器的模型。我們的關鍵觀察是，在 LoRA 適應的梯度計算中存在低秩分解，這會導致可能的演算法加速。這讓我們能夠 (i) 識別相變行為，以及 (ii) 證明近似線性演算法的存在，方法是逐項控制 LoRA 更新計算項，假設強指數時間假設 (SETH)。對於前者，我們根據輸入序列 $\mathbf{X}$、預訓練權重 $\mathbf{W^\star}$ 和適配器矩陣 $\alpha \mathbf{B} \mathbf{A} / r$ 的乘積產生的特定範數，識別出所有可能的秩-$r$ LoRA 更新演算法效率的急劇轉變。具體來說，我們為此類範數推導出一個共用上限閾值，並顯示 LoRA 的有效（次二次）近似演算法僅存在於此閾值以下。對於後者，我們證明了 LoRA 適應的近似線性演算法的存在，方法是利用 LoRA 梯度的階層式低秩結構，並使用一系列鏈式低秩近似來近似梯度。為了展示我們的理論，我們考慮了兩個實際場景：權重在注意力頭中的部分（例如，僅 $\mathbf{W}_V$ 和 $\mathbf{W}_Q$）和完全適應（例如，$\mathbf{W}_Q$、$\mathbf{W}_V$ 和 $\mathbf{W}_K$）。</paragraph>

##### **Towards Real-world Scenario: Imbalanced New Intent Discovery**
2406.03127v1 by Shun Zhang, Chaoran Yan, Jian Yang, Jiaheng Liu, Ying Mo, Jiaqi Bai, Tongliang Li, Zhoujun Li

New Intent Discovery (NID) aims at detecting known and previously undefined
categories of user intent by utilizing limited labeled and massive unlabeled
data. Most prior works often operate under the unrealistic assumption that the
distribution of both familiar and new intent classes is uniform, overlooking
the skewed and long-tailed distributions frequently encountered in real-world
scenarios. To bridge the gap, our work introduces the imbalanced new intent
discovery (i-NID) task, which seeks to identify familiar and novel intent
categories within long-tailed distributions. A new benchmark (ImbaNID-Bench)
comprised of three datasets is created to simulate the real-world long-tail
distributions. ImbaNID-Bench ranges from broad cross-domain to specific
single-domain intent categories, providing a thorough representation of
practical use cases. Besides, a robust baseline model ImbaNID is proposed to
achieve cluster-friendly intent representations. It includes three stages:
model pre-training, generation of reliable pseudo-labels, and robust
representation learning that strengthens the model performance to handle the
intricacies of real-world data distributions. Our extensive experiments on
previous benchmarks and the newly established benchmark demonstrate the
superior performance of ImbaNID in addressing the i-NID task, highlighting its
potential as a powerful baseline for uncovering and categorizing user intents
in imbalanced and long-tailed
distributions\footnote{\url{https://github.com/Zkdc/i-NID}}.

摘要：新意圖發現 (NID) 旨在透過利用有限標籤和大量未標籤資料，來偵測已知和先前未定義的使用者意圖類別。大多數先前的工作通常在不切實際的假設下運作，即熟悉和新意圖類別的分布是均勻的，而忽略了在現實世界場景中經常遇到的偏態和長尾分布。為了縮小差距，我們的研究引入了不平衡新意圖發現 (i-NID) 任務，該任務尋求在長尾分布中識別熟悉和新穎的意圖類別。一個由三個資料集組成的全新基準 (ImbaNID-Bench) 被建立起來，用於模擬現實世界的長尾分布。ImbaNID-Bench 的範圍從廣泛的跨網域到特定的單一網域意圖類別，提供了實際使用案例的全面表示。此外，還提出了一個強健的基準模型 ImbaNID，以實現友善群集的意圖表示。它包括三個階段：模型預訓練、產生可靠的偽標籤，以及強健的表示學習，這增強了模型處理現實世界資料分布的複雜性的效能。我們在先前基準和新建立的基準上進行的廣泛實驗，證明了 ImbaNID 在處理 i-NID 任務時的卓越效能，突顯了其作為在不平衡和長尾分布中揭露和分類使用者意圖的強大基準的潛力\footnote{\url{https://github.com/Zkdc/i-NID}}。

##### **Space Decomposition for Sentence Embedding**
2406.03125v1 by Wuttikorn Ponwitayarat, Peerat Limkonchotiwat, Ekapol Chuangsuwanich, Sarana Nutanong

Determining sentence pair similarity is crucial for various NLP tasks. A
common technique to address this is typically evaluated on a continuous
semantic textual similarity scale from 0 to 5. However, based on a linguistic
observation in STS annotation guidelines, we found that the score in the range
[4,5] indicates an upper-range sample, while the rest are lower-range samples.
This necessitates a new approach to treating the upper-range and lower-range
classes separately. In this paper, we introduce a novel embedding space
decomposition method called MixSP utilizing a Mixture of Specialized
Projectors, designed to distinguish and rank upper-range and lower-range
samples accurately. The experimental results demonstrate that MixSP decreased
the overlap representation between upper-range and lower-range classes
significantly while outperforming competitors on STS and zero-shot benchmarks.

摘要：確定句子對相似性對於各種 NLP 任務至關重要。常見的解決此問題的技術通常會在 0 到 5 的連續語義文字相似性量表上進行評估。然而，根據 STS 標註指南中的語言學觀察，我們發現範圍 [4,5] 中的分數表示上層樣本，而其餘為下層樣本。這需要一種新的方法來分別處理上層和下層類別。在本文中，我們介紹了一種稱為 MixSP 的新嵌入空間分解方法，它利用專業投影機混合來區分和排名上層和下層樣本。實驗結果表明，MixSP 大幅減少了上層和下層類別之間的重疊表示，同時在 STS 和零次基準測試中優於競爭對手。

##### **Enhancing the Resilience of Graph Neural Networks to Topological Perturbations in Sparse Graphs**
2406.03097v1 by Shuqi He, Jun Zhuang, Ding Wang, Luyao Peng, Jun Song

Graph neural networks (GNNs) have been extensively employed in node
classification. Nevertheless, recent studies indicate that GNNs are vulnerable
to topological perturbations, such as adversarial attacks and edge disruptions.
Considerable efforts have been devoted to mitigating these challenges. For
example, pioneering Bayesian methodologies, including GraphSS and LlnDT,
incorporate Bayesian label transitions and topology-based label sampling to
strengthen the robustness of GNNs. However, GraphSS is hindered by slow
convergence, while LlnDT faces challenges in sparse graphs. To overcome these
limitations, we propose a novel label inference framework, TraTopo, which
combines topology-driven label propagation, Bayesian label transitions, and
link analysis via random walks. TraTopo significantly surpasses its
predecessors on sparse graphs by utilizing random walk sampling, specifically
targeting isolated nodes for link prediction, thus enhancing its effectiveness
in topological sampling contexts. Additionally, TraTopo employs a shortest-path
strategy to refine link prediction, thereby reducing predictive overhead and
improving label inference accuracy. Empirical evaluations highlight TraTopo's
superiority in node classification, significantly exceeding contemporary GCN
models in accuracy.

摘要：圖神經網路 (GNN) 已廣泛用於節點分類。然而，最近的研究表明，GNN 容易受到拓撲擾動的影響，例如對抗性攻擊和邊緣中斷。已經投入大量精力來緩解這些挑戰。例如，開創性的貝氏方法，包括 GraphSS 和 LlnDT，結合貝氏標籤轉換和基於拓撲的標籤抽樣，以增強 GNN 的穩健性。然而，GraphSS 受到收斂速度慢的阻礙，而 LlnDT 在稀疏圖中面臨挑戰。為了克服這些限制，我們提出了一個新的標籤推論框架 TraTopo，它結合了拓撲驅動的標籤傳播、貝氏標籤轉換和通過隨機遊走的鏈接分析。TraTopo 通過利用隨機遊走抽樣，特別針對孤立節點進行鏈接預測，從而顯著超越了其在稀疏圖上的前身，從而增強了其在拓撲抽樣上下文中的有效性。此外，TraTopo 採用最短路徑策略來優化鏈接預測，從而減少預測開銷並提高標籤推論準確度。經驗評估突出了 TraTopo 在節點分類中的優越性，其準確度顯著超過了當代 GCN 模型。

##### **FragRel: Exploiting Fragment-level Relations in the External Memory of Large Language Models**
2406.03092v1 by Xihang Yue, Linchao Zhu, Yi Yang

To process contexts with unlimited length using Large Language Models (LLMs),
recent studies explore hierarchically managing the long text. Only several text
fragments are taken from the external memory and passed into the temporary
working memory, i.e., LLM's context window. However, existing approaches
isolatedly handle the text fragments without considering their structural
connections, thereby suffering limited capability on texts with intensive
inter-relations, e.g., coherent stories and code repositories. This work
attempts to resolve this by exploiting the fragment-level relations in external
memory. First, we formulate the fragment-level relations and present several
instantiations for different text types. Next, we introduce a relation-aware
fragment assessment criteria upon previous independent fragment assessment.
Finally, we present the fragment-connected Hierarchical Memory based LLM. We
validate the benefits of involving these relations on long story understanding,
repository-level code generation, and long-term chatting.

摘要：為了使用大型語言模型 (LLM) 處理具有無限長度的內容，最近的研究探索了以分層方式管理長文字。僅從外部記憶體中取出幾個文字片段並傳遞到臨時工作記憶體，即 LLM 的內容視窗。然而，現有的方法孤立地處理文字片段，而沒有考慮它們的結構連接，因此在具有密集交互關係的文字（例如連貫的故事和程式碼儲存庫）上的能力有限。這項工作試圖透過利用外部記憶體中的片段級關係來解決這個問題。首先，我們制定了片段級關係，並為不同的文字類型提出了幾個實例。接下來，我們在先前的獨立片段評估之上介紹了一個關係感知片段評估標準。最後，我們提出了基於片段連接的分層記憶體 LLM。我們驗證了將這些關係應用於長篇故事理解、儲存庫級別的程式碼產生和長期聊天中的好處。

##### **Cryptocurrency Frauds for Dummies: How ChatGPT introduces us to fraud?**
2406.03079v1 by Wail Zellagui, Abdessamad Imine, Yamina Tadjeddine

Recent advances in the field of large language models (LLMs), particularly
the ChatGPT family, have given rise to a powerful and versatile machine
interlocutor, packed with knowledge and challenging our understanding of
learning. This interlocutor is a double-edged sword: it can be harnessed for a
wide variety of beneficial tasks, but it can also be used to cause harm. This
study explores the complicated interaction between ChatGPT and the growing
problem of cryptocurrency fraud. Although ChatGPT is known for its adaptability
and ethical considerations when used for harmful purposes, we highlight the
deep connection that may exist between ChatGPT and fraudulent actions in the
volatile cryptocurrency ecosystem. Based on our categorization of
cryptocurrency frauds, we show how to influence outputs, bypass ethical terms,
and achieve specific fraud goals by manipulating ChatGPT prompts. Furthermore,
our findings emphasize the importance of realizing that ChatGPT could be a
valuable instructor even for novice fraudsters, as well as understanding and
safely deploying complex language models, particularly in the context of
cryptocurrency frauds. Finally, our study underlines the importance of using
LLMs responsibly and ethically in the digital currency sector, identifying
potential risks and resolving ethical issues. It should be noted that our work
is not intended to encourage and promote fraud, but rather to raise awareness
of the risks of fraud associated with the use of ChatGPT.

摘要：近期大型語言模型 (LLM) 領域的進展，特別是 ChatGPT 家族，催生了一位強大且多才多藝的機器對話者，它具備豐富的知識並挑戰我們對學習的理解。這個對話者是一把雙面刃：它可用於各種有益任務，但它也可能被用於造成傷害。本研究探討了 ChatGPT 與日益嚴重的加密貨幣詐欺之間複雜的交互作用。儘管 ChatGPT 以其適應性和在用於有害目的時的道德考量而聞名，但我們強調了 ChatGPT 與波動的加密貨幣生態系統中的欺詐行為之間可能存在的深層聯繫。根據我們的加密貨幣詐欺分類，我們展示了如何透過操縱 ChatGPT 提示來影響輸出、繞過道德條款並達成具體的詐欺目標。此外，我們的研究結果強調了意識到 ChatGPT 甚至對新手詐騙者來說都可能是一位有價值的指導者，以及理解和安全地部署複雜的語言模型，特別是在加密貨幣詐欺的背景下，這一點的重要性。最後，我們的研究強調了在數位貨幣領域負責任且合乎道德地使用 LLM 的重要性，找出潛在風險並解決道德問題。需要注意的是，我們的研究並非旨在鼓勵和宣傳詐欺，而是要提高人們對與使用 ChatGPT 相關的詐欺風險的認識。

##### **Towards Federated Domain Unlearning: Verification Methodologies and Challenges**
2406.03078v1 by Kahou Tam, Kewei Xu, Li Li, Huazhu Fu

Federated Learning (FL) has evolved as a powerful tool for collaborative
model training across multiple entities, ensuring data privacy in sensitive
sectors such as healthcare and finance. However, the introduction of the Right
to Be Forgotten (RTBF) poses new challenges, necessitating federated unlearning
to delete data without full model retraining. Traditional FL unlearning
methods, not originally designed with domain specificity in mind, inadequately
address the complexities of multi-domain scenarios, often affecting the
accuracy of models in non-targeted domains or leading to uniform forgetting
across all domains. Our work presents the first comprehensive empirical study
on Federated Domain Unlearning, analyzing the characteristics and challenges of
current techniques in multi-domain contexts. We uncover that these methods
falter, particularly because they neglect the nuanced influences of
domain-specific data, which can lead to significant performance degradation and
inaccurate model behavior. Our findings reveal that unlearning
disproportionately affects the model's deeper layers, erasing critical
representational subspaces acquired during earlier training phases. In
response, we propose novel evaluation methodologies tailored for Federated
Domain Unlearning, aiming to accurately assess and verify domain-specific data
erasure without compromising the model's overall integrity and performance.
This investigation not only highlights the urgent need for domain-centric
unlearning strategies in FL but also sets a new precedent for evaluating and
implementing these techniques effectively.

摘要：联邦学习 (FL) 已发展成为一种用于在多个实体间进行协作模型训练的强大工具，确保医疗保健和金融等敏感领域的的数据隐私。然而，遗忘权 (RTBF) 的引入带来了新的挑战，需要联邦取消学习来删除数据，而无需进行完全的模型重新训练。传统的 FL 取消学习方法最初并未考虑领域特异性，无法充分解决多领域场景的复杂性，通常会影响非目标领域的模型准确性或导致所有领域的统一遗忘。我们的工作提出了第一个关于联邦领域取消学习的综合实证研究，分析了当前技术在多领域环境中的特征和挑战。我们发现这些方法会失败，特别是由于它们忽略了特定领域数据的影响，这可能导致严重的性能下降和不准确的模型行为。我们的研究结果表明，取消学习会对模型的更深层产生不成比例的影响，抹去在早期训练阶段获得的关键表征子空间。作为回应，我们提出了针对联邦领域取消学习量身定制的新颖评估方法，旨在准确评估和验证特定领域的 data 擦除，同时不损害模型的整体完整性和性能。此调查不仅突出了 FL 中以领域为中心的取消学习策略的迫切需要，还为有效评估和实施这些技术树立了新的先例。

##### **Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework**
2406.03075v1 by Xiaoxi Sun, Jinpeng Li, Yan Zhong, Dongyan Zhao, Rui Yan

The advent of large language models (LLMs) has facilitated the development of
natural language text generation. It also poses unprecedented challenges, with
content hallucination emerging as a significant concern. Existing solutions
often involve expensive and complex interventions during the training process.
Moreover, some approaches emphasize problem disassembly while neglecting the
crucial validation process, leading to performance degradation or limited
applications. To overcome these limitations, we propose a Markov Chain-based
multi-agent debate verification framework to enhance hallucination detection
accuracy in concise claims. Our method integrates the fact-checking process,
including claim detection, evidence retrieval, and multi-agent verification. In
the verification stage, we deploy multiple agents through flexible Markov
Chain-based debates to validate individual claims, ensuring meticulous
verification outcomes. Experimental results across three generative tasks
demonstrate that our approach achieves significant improvements over baselines.

摘要：大型語言模型 (LLM) 的出現促進了自然語言文本生成的發展。它也帶來了前所未有的挑戰，其中內容虛構已成為一個重要的問題。現有的解決方案通常涉及在訓練過程中進行昂貴且複雜的干預。此外，一些方法強調問題分解，而忽視了關鍵的驗證過程，導致性能下降或應用受限。為了克服這些限制，我們提出了一個基於馬可夫鏈的多主體辯論驗證框架，以增強簡潔陳述中的虛構檢測準確性。我們的辦法整合了事實查核過程，包括陳述檢測、證據檢索和多主體驗證。在驗證階段，我們通過基於馬可夫鏈的靈活辯論部署多個主體來驗證個別陳述，確保細緻的驗證結果。跨三個生成任務的實驗結果表明，我們的辦法在基線上取得了顯著的改進。

##### **Exploiting LMM-based knowledge for image classification tasks**
2406.03071v1 by Maria Tzelepi, Vasileios Mezaris

In this paper we address image classification tasks leveraging knowledge
encoded in Large Multimodal Models (LMMs). More specifically, we use the
MiniGPT-4 model to extract semantic descriptions for the images, in a
multimodal prompting fashion. In the current literature, vision language models
such as CLIP, among other approaches, are utilized as feature extractors, using
only the image encoder, for solving image classification tasks. In this paper,
we propose to additionally use the text encoder to obtain the text embeddings
corresponding to the MiniGPT-4-generated semantic descriptions. Thus, we use
both the image and text embeddings for solving the image classification task.
The experimental evaluation on three datasets validates the improved
classification performance achieved by exploiting LMM-based knowledge.

摘要：在本文中，我们解决利用大型多模态模型 (LMM) 中编码的知识的图像分类任务。更具体地说，我们使用 MiniGPT-4 模型以多模态提示方式为图像提取语义描述。在当前的文献中，CLIP 等视觉语言模型作为特征提取器，仅使用图像编码器来解决图像分类任务。在本文中，我们建议另外使用文本编码器来获取对应于 MiniGPT-4 生成的语义描述的文本嵌入。因此，我们使用图像和文本嵌入来解决图像分类任务。在三个数据集上的实验评估验证了利用基于 LMM 的知识所实现的改进的分类性能。

##### **A-Bench: Are LMMs Masters at Evaluating AI-generated Images?**
2406.03070v1 by Zicheng Zhang, Haoning Wu, Chunyi Li, Yingjie Zhou, Wei Sun, Xiongkuo Min, Zijian Chen, Xiaohong Liu, Weisi Lin, Guangtao Zhai

How to accurately and efficiently assess AI-generated images (AIGIs) remains
a critical challenge for generative models. Given the high costs and extensive
time commitments required for user studies, many researchers have turned
towards employing large multi-modal models (LMMs) as AIGI evaluators, the
precision and validity of which are still questionable. Furthermore,
traditional benchmarks often utilize mostly natural-captured content rather
than AIGIs to test the abilities of LMMs, leading to a noticeable gap for
AIGIs. Therefore, we introduce A-Bench in this paper, a benchmark designed to
diagnose whether LMMs are masters at evaluating AIGIs. Specifically, A-Bench is
organized under two key principles: 1) Emphasizing both high-level semantic
understanding and low-level visual quality perception to address the intricate
demands of AIGIs. 2) Various generative models are utilized for AIGI creation,
and various LMMs are employed for evaluation, which ensures a comprehensive
validation scope. Ultimately, 2,864 AIGIs from 16 text-to-image models are
sampled, each paired with question-answers annotated by human experts, and
tested across 18 leading LMMs. We hope that A-Bench will significantly enhance
the evaluation process and promote the generation quality for AIGIs. The
benchmark is available at https://github.com/Q-Future/A-Bench.

摘要：如何準確有效地評估 AI 生成的圖像 (AIGI) 仍然是生成模型的一項重大挑戰。鑑於使用者研究需要高成本和大量時間承諾，許多研究人員已轉向採用大型多模態模型 (LMM) 作為 AIGI 評估器，其準確性和有效性仍有待商榷。此外，傳統基準通常主要利用自然擷取的內容，而非 AIGI 來測試 LMM 的能力，導致 AIGI 出現顯著的差距。因此，我們在本文中介紹 A-Bench，這是一個基準，旨在診斷 LMM 是否是評估 AIGI 的專家。具體來說，A-Bench 是根據兩個關鍵原則組織的：1) 強調高階語義理解和低階視覺品質感知，以滿足 AIGI 的複雜需求。2) 採用各種生成模型來建立 AIGI，並採用各種 LMM 來進行評估，這確保了全面的驗證範圍。最終，從 16 個文字轉圖像模型中抽取 2,864 個 AIGI，每個 AIGI 都配有人類專家註解的問題和答案，並在 18 個領先的 LMM 中進行測試。我們希望 A-Bench 能夠顯著增強評估過程，並促進 AIGI 的生成品質。基準可在 https://github.com/Q-Future/A-Bench 取得。

##### **How Truncating Weights Improves Reasoning in Language Models**
2406.03068v1 by Lei Chen, Joan Bruna, Alberto Bietti

In addition to the ability to generate fluent text in various languages,
large language models have been successful at tasks that involve basic forms of
logical "reasoning" over their context. Recent work found that selectively
removing certain components from weight matrices in pre-trained models can
improve such reasoning capabilities. We investigate this phenomenon further by
carefully studying how certain global associations tend to be stored in
specific weight components or Transformer blocks, in particular feed-forward
layers. Such associations may hurt predictions in reasoning tasks, and removing
the corresponding components may then improve performance. We analyze how this
arises during training, both empirically and theoretically, on a two-layer
Transformer trained on a basic reasoning task with noise, a toy associative
memory model, and on the Pythia family of pre-trained models tested on simple
reasoning tasks.

摘要：除了生成各種語言的流利文字外，大型語言模型在涉及其背景中基本形式的邏輯「推理」的任務上也取得了成功。最近的研究發現，從預訓練模型的權重矩陣中選擇性地移除某些組成部分，可以改善這種推理能力。我們通過仔細研究某些全局關聯如何傾向於儲存在特定權重組成部分或 Transformer 區塊（尤其是前饋層）中，進一步探討了這種現象。這種關聯可能會損害推理任務中的預測，而移除對應的組成部分可能會提高效能。我們分析了在訓練過程中這是如何產生的，無論是在經驗上還是理論上，在一個訓練於帶有雜訊的基本推理任務、玩具關聯記憶模型和在簡單推理任務上測試的 Pythia 預訓練模型家族上的兩層 Transformer。

##### **RadBARTsum: Domain Specific Adaption of Denoising Sequence-to-Sequence Models for Abstractive Radiology Report Summarization**
2406.03062v1 by Jinge Wu, Abul Hasan, Honghan Wu

Radiology report summarization is a crucial task that can help doctors
quickly identify clinically significant findings without the need to review
detailed sections of reports. This study proposes RadBARTsum, a domain-specific
and ontology facilitated adaptation of the BART model for abstractive radiology
report summarization. The approach involves two main steps: 1) re-training the
BART model on a large corpus of radiology reports using a novel entity masking
strategy to improving biomedical domain knowledge learning, and 2) fine-tuning
the model for the summarization task using the Findings and Background sections
to predict the Impression section. Experiments are conducted using different
masking strategies. Results show that the re-training process with domain
knowledge facilitated masking improves performances consistently across various
settings. This work contributes a domain-specific generative language model for
radiology report summarization and a method for utilising medical knowledge to
realise entity masking language model. The proposed approach demonstrates a
promising direction of enhancing the efficiency of language models by deepening
its understanding of clinical knowledge in radiology reports.

摘要：放射科報告摘要是一項至關重要的任務，它可以幫助醫生在無需檢閱報告的詳細部分的情況下快速找出臨床上重要的發現。本研究提出 RadBARTsum，這是一種針對特定領域和促進 BART 模型適應抽象放射科報告摘要的本体。該方法涉及兩個主要步驟：1）使用新穎的實體遮罩策略在大量放射科報告語料庫上重新訓練 BART 模型，以改進生物醫學領域知識學習，以及 2）使用發現和背景部分微調摘要任務的模型以預測印象部分。使用不同的遮罩策略進行實驗。結果表明，促進領域知識遮罩的重新訓練過程持續改進了各種設置下的性能。這項工作為放射科報告摘要貢獻了一個特定領域的生成式語言模型，以及一種利用醫學知識實現實體遮罩語言模型的方法。所提出的方法展示了一個通過加深其對放射科報告中臨床知識的理解來提高語言模型效率的有希望的方向。

##### **StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning**
2406.03049v1 by Shaolei Zhang, Qingkai Fang, Shoutao Guo, Zhengrui Ma, Min Zhang, Yang Feng

Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech
translation) outputs target speech while receiving streaming speech inputs,
which is critical for real-time communication. Beyond accomplishing translation
between speech, Simul-S2ST requires a policy to control the model to generate
corresponding target speech at the opportune moment within speech inputs,
thereby posing a double challenge of translation and policy. In this paper, we
propose StreamSpeech, a direct Simul-S2ST model that jointly learns translation
and simultaneous policy in a unified framework of multi-task learning. Adhering
to a multi-task learning approach, StreamSpeech can perform offline and
simultaneous speech recognition, speech translation and speech synthesis via an
"All-in-One" seamless model. Experiments on CVSS benchmark demonstrate that
StreamSpeech achieves state-of-the-art performance in both offline S2ST and
Simul-S2ST tasks. Besides, StreamSpeech is able to present high-quality
intermediate results (i.e., ASR or translation results) during simultaneous
translation process, offering a more comprehensive real-time communication
experience.

摘要：同時語音對語音翻譯（Simul-S2ST，也稱為串流語音翻譯）在接收串流語音輸入時輸出目標語音，這對於即時通訊至關重要。除了完成語音之間的翻譯外，Simul-S2ST 需要一個策略來控制模型在語音輸入中適時生成對應的目標語音，從而對翻譯和策略提出雙重挑戰。在本文中，我們提出了 StreamSpeech，這是一個直接的 Simul-S2ST 模型，它在多任務學習的統一框架中聯合學習翻譯和同時策略。遵循多任務學習方法，StreamSpeech 可以通過「一體化」無縫模型執行離線和同時語音識別、語音翻譯和語音合成。CVSS 基準上的實驗表明，StreamSpeech 在離線 S2ST 和 Simul-S2ST 任務中都取得了最先進的性能。此外，StreamSpeech 能夠在同時翻譯過程中呈現高品質的中間結果（即 ASR 或翻譯結果），提供更全面的即時通訊體驗。

##### **From Tarzan to Tolkien: Controlling the Language Proficiency Level of LLMs for Content Generation**
2406.03030v1 by Ali Malik, Stephen Mayhew, Chris Piech, Klinton Bicknell

We study the problem of controlling the difficulty level of text generated by
Large Language Models (LLMs) for contexts where end-users are not fully
proficient, such as language learners. Using a novel framework, we evaluate the
effectiveness of several key approaches for this task, including few-shot
prompting, supervised finetuning, and reinforcement learning (RL), utilising
both GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.
  Our findings reveal a large performance gap between GPT-4 and the open source
models when using prompt-based strategies. However, we show how to bridge this
gap with a careful combination of finetuning and RL alignment. Our best model,
CALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and
other strategies, at only a fraction of the cost. We further validate the
quality of our results through a small-scale human study.

摘要：我們研究控制大型語言模型 (LLM) 所產生文字難度等級的問題，適用於最終使用者能力不足的環境，例如語言學習者。我們使用一個新架構，評估幾種關鍵方法的有效性，包括少樣本提示、監督微調和強化學習 (RL)，同時使用 GPT-4 和 LLama2-7B 和 Mistral-7B 等開源替代方案。
我們的研究結果顯示，在使用基於提示的策略時，GPT-4 和開源模型之間存在很大的效能差距。然而，我們展示如何透過微調和 RL 對齊的仔細組合來縮小這個差距。我們最好的模型 CALM（CEFR 對齊語言模型）超越了 GPT-4 和其他策略的效能，而且成本僅為一小部分。我們進一步透過小規模的人類研究驗證了我們結果的品質。

##### **Analyzing the Influence of Training Samples on Explanations**
2406.03012v1 by André Artelt, Barbara Hammer

EXplainable AI (XAI) constitutes a popular method to analyze the reasoning of
AI systems by explaining their decision-making, e.g. providing a counterfactual
explanation of how to achieve recourse. However, in cases such as unexpected
explanations, the user might be interested in learning about the cause of this
explanation -- e.g. properties of the utilized training data that are
responsible for the observed explanation. Under the umbrella of data valuation,
first approaches have been proposed that estimate the influence of data samples
on a given model. In this work, we take a slightly different stance, as we are
interested in the influence of single samples on a model explanation rather
than the model itself. Hence, we propose the novel problem of identifying
training data samples that have a high influence on a given explanation (or
related quantity) and investigate the particular case of differences in the
cost of the recourse between protected groups. For this, we propose an
algorithm that identifies such influential training samples.

摘要：可解釋人工智慧 (XAI) 是一種流行的方法，用於分析人工智慧系統的推理，方法是說明它們的決策，例如提供反事實的說明來說明如何獲得追索權。然而，在意外說明等情況下，使用者可能對了解此說明的原因感興趣，例如負責觀察到的說明的已使用訓練資料的屬性。在資料評估的範疇下，已提出第一種估計資料樣本對特定模型影響的方法。在這項工作中，我們採取略微不同的立場，因為我們感興趣的是單一樣本對模型說明的影響，而不是模型本身。因此，我們提出了一個新的問題，即識別對特定說明（或相關數量）有高度影響的訓練資料樣本，並調查受保護群體之間追索權成本差異的特定情況。為此，我們提出了一種演算法，用於識別此類有影響力的訓練樣本。

##### **Unveiling Selection Biases: Exploring Order and Token Sensitivity in Large Language Models**
2406.03009v1 by Sheng-Lun Wei, Cheng-Kuang Wu, Hen-Hsen Huang, Hsin-Hsi Chen

In this paper, we investigate the phenomena of "selection biases" in Large
Language Models (LLMs), focusing on problems where models are tasked with
choosing the optimal option from an ordered sequence. We delve into biases
related to option order and token usage, which significantly impact LLMs'
decision-making processes. We also quantify the impact of these biases through
an extensive empirical analysis across multiple models and tasks. Furthermore,
we propose mitigation strategies to enhance model performance. Our key
contributions are threefold: 1) Precisely quantifying the influence of option
order and token on LLMs, 2) Developing strategies to mitigate the impact of
token and order sensitivity to enhance robustness, and 3) Offering a detailed
analysis of sensitivity across models and tasks, which informs the creation of
more stable and reliable LLM applications for selection problems.

摘要：在本文中，我們探討大型語言模型 (LLM) 中的「選擇偏差」現象，重點關注模型從有序序列中選擇最佳選項的問題。我們深入探討與選項順序和代碼使用相關的偏差，這些偏差會顯著影響 LLM 的決策過程。我們還通過跨多個模型和任務的廣泛實證分析量化了這些偏差的影響。此外，我們提出了減輕模型效能的策略。我們的關鍵貢獻有三方面：1) 精確量化選項順序和代碼對 LLM 的影響，2) 開發策略以減輕代碼和順序敏感性的影響，以增強穩健性，以及 3) 提供跨模型和任務的詳細敏感性分析，這有助於為選擇問題建立更穩定和可靠的 LLM 應用程式。

##### **DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences**
2406.03008v1 by Yidong Huang, Jacob Sansom, Ziqiao Ma, Felix Gervits, Joyce Chai

Recent advancements in foundation models (FMs) have unlocked new prospects in
autonomous driving, yet the experimental settings of these studies are
preliminary, over-simplified, and fail to capture the complexity of real-world
driving scenarios in human environments. It remains under-explored whether FM
agents can handle long-horizon navigation tasks with free-from dialogue and
deal with unexpected situations caused by environmental dynamics or task
changes. To explore the capabilities and boundaries of FMs faced with the
challenges above, we introduce DriVLMe, a video-language-model-based agent to
facilitate natural and effective communication between humans and autonomous
vehicles that perceive the environment and navigate. We develop DriVLMe from
both embodied experiences in a simulated environment and social experiences
from real human dialogue. While DriVLMe demonstrates competitive performance in
both open-loop benchmarks and closed-loop human studies, we reveal several
limitations and challenges, including unacceptable inference time, imbalanced
training data, limited visual understanding, challenges with multi-turn
interactions, simplified language generation from robotic experiences, and
difficulties in handling on-the-fly unexpected situations like environmental
dynamics and task changes.

摘要：近期在基礎模型 (FM) 的進展為自動駕駛開啟了新的前景，然而這些研究的實驗設定仍處於初步、過於簡化的階段，未能捕捉人類環境中真實駕駛場景的複雜性。FM 代理是否能處理免對話的長時程導航任務，並應對環境動態或任務變更所造成的意外情況，仍有待進一步探討。為了探索 FM 在面對上述挑戰時的能力和界限，我們引入了 DriVLMe，一個基於影片語言模型的代理，用於促進人類與感知環境並導航的自動駕駛車輛之間自然且有效的溝通。我們從模擬環境中的具身經驗和真實人類對話中的社會經驗中開發了 DriVLMe。儘管 DriVLMe 在開迴路基準和閉迴路人體研究中都展現出具有競爭力的表現，我們揭露了幾個限制和挑戰，包括不可接受的推論時間、不平衡的訓練資料、受限的視覺理解、多輪互動的挑戰、從機器人經驗中簡化的語言生成，以及難以處理環境動態和任務變更等即時意外情況。

##### **BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents**
2406.03007v1 by Yifei Wang, Dizhan Xue, Shengjie Zhang, Shengsheng Qian

With the prosperity of large language models (LLMs), powerful LLM-based
intelligent agents have been developed to provide customized services with a
set of user-defined tools. State-of-the-art methods for constructing LLM agents
adopt trained LLMs and further fine-tune them on data for the agent task.
However, we show that such methods are vulnerable to our proposed backdoor
attacks named BadAgent on various agent tasks, where a backdoor can be embedded
by fine-tuning on the backdoor data. At test time, the attacker can manipulate
the deployed LLM agents to execute harmful operations by showing the trigger in
the agent input or environment. To our surprise, our proposed attack methods
are extremely robust even after fine-tuning on trustworthy data. Though
backdoor attacks have been studied extensively in natural language processing,
to the best of our knowledge, we could be the first to study them on LLM agents
that are more dangerous due to the permission to use external tools. Our work
demonstrates the clear risk of constructing LLM agents based on untrusted LLMs
or data. Our code is public at https://github.com/DPamK/BadAgent

摘要：隨著大型語言模型 (LLM) 的蓬勃發展，功能強大的基於 LLM 的智慧代理程式已被開發出來，以提供客製化服務，並配有一組使用者定義的工具。建構 LLM 代理程式的最新方法採用已訓練的 LLM，並進一步針對代理程式任務的資料進行微調。然而，我們發現這些方法容易受到我們提出的後門攻擊，我們稱之為 BadAgent，針對各種代理程式任務，後門可以透過針對後門資料進行微調來嵌入。在測試時，攻擊者可以透過在代理程式輸入或環境中顯示觸發器，來操縱已部署的 LLM 代理程式以執行有害操作。令我們驚訝的是，我們提出的攻擊方法即使在針對可信賴資料進行微調後，仍然極為強大。儘管後門攻擊已在自然語言處理中廣泛研究，但據我們所知，我們可能是第一個研究 LLM 代理程式的後門攻擊，而 LLM 代理程式由於被允許使用外部工具，因此更加危險。我們的研究清楚地表明，基於不受信任的 LLM 或資料來建構 LLM 代理程式具有明顯風險。我們的程式碼已公開於 https://github.com/DPamK/BadAgent

##### **Evaluation of data inconsistency for multi-modal sentiment analysis**
2406.03004v1 by Yufei Wang, Mengyue Wu

Emotion semantic inconsistency is an ubiquitous challenge in multi-modal
sentiment analysis (MSA). MSA involves analyzing sentiment expressed across
various modalities like text, audio, and videos. Each modality may convey
distinct aspects of sentiment, due to subtle and nuanced expression of human
beings, leading to inconsistency, which may hinder the prediction of artificial
agents. In this work, we introduce a modality conflicting test set and assess
the performance of both traditional multi-modal sentiment analysis models and
multi-modal large language models (MLLMs). Our findings reveal significant
performance degradation across traditional models when confronted with
semantically conflicting data and point out the drawbacks of MLLMs when
handling multi-modal emotion analysis. Our research presents a new challenge
and offer valuable insights for the future development of sentiment analysis
systems.

摘要：情感語意不一致是多模態情緒分析 (MSA) 中普遍存在的挑戰。MSA 涉及分析跨越文本、音訊和影片等各種模態所表達的情緒。由於人類表達的微妙和細緻，每個模態可能會傳達情緒的不同面向，導致不一致，這可能會阻礙人工代理的預測。在這項工作中，我們引入了模態衝突測試集，並評估了傳統多模態情緒分析模型和多模態大型語言模型 (MLLM) 的效能。我們的研究結果顯示，當面對語意衝突資料時，傳統模型的效能會顯著下降，並指出 MLLM 在處理多模態情緒分析時的缺點。我們的研究提出了新的挑戰，並為情緒分析系統的未來發展提供有價值的見解。

##### **EdgeSync: Faster Edge-model Updating via Adaptive Continuous Learning for Video Data Drift**
2406.03001v1 by Peng Zhao, Runchu Dong, Guiqin Wang, Cong Zhao

Real-time video analytics systems typically place models with fewer weights
on edge devices to reduce latency. The distribution of video content features
may change over time for various reasons (i.e. light and weather change) ,
leading to accuracy degradation of existing models, to solve this problem,
recent work proposes a framework that uses a remote server to continually train
and adapt the lightweight model at edge with the help of complex model.
However, existing analytics approaches leave two challenges untouched: firstly,
retraining task is compute-intensive, resulting in large model update delays;
secondly, new model may not fit well enough with the data distribution of the
current video stream. To address these challenges, in this paper, we present
EdgeSync, EdgeSync filters the samples by considering both timeliness and
inference results to make training samples more relevant to the current video
content as well as reduce the update delay, to improve the quality of training,
EdgeSync also designs a training management module that can efficiently adjusts
the model training time and training order on the runtime. By evaluating real
datasets with complex scenes, our method improves about 3.4% compared to
existing methods and about 10% compared to traditional means.

摘要：实时视频分析系统通常在边缘设备上放置权重较少的模型以减少延迟。视频内容特征的分布可能因各种原因（即光线和天气变化）而随时间变化，从而导致现有模型的准确性下降，为了解决这个问题，最近的工作提出了一种使用远程服务器来持续训练和调整边缘设备上的轻量级模型的框架，在复杂模型的帮助下。然而，现有的分析方法留下了两个未解决的挑战：首先，重新训练任务计算密集，导致模型更新延迟大；其次，新模型可能与当前视频流的数据分布不够匹配。为了应对这些挑战，在本文中，我们提出了 EdgeSync，EdgeSync 通过考虑及时性和推理结果来过滤样本，以使训练样本与当前视频内容更相关，并减少更新延迟，以提高训练质量，EdgeSync 还设计了一个训练管理模块，可以在运行时有效地调整模型训练时间和训练顺序。通过评估具有复杂场景的真实数据集，我们的方法与现有方法相比提高了约 3.4%，与传统方法相比提高了约 10%。

##### **Simplification of Risk Averse POMDPs with Performance Guarantees**
2406.03000v1 by Yaacov Pariente, Vadim Indelman

Risk averse decision making under uncertainty in partially observable domains
is a fundamental problem in AI and essential for reliable autonomous agents. In
our case, the problem is modeled using partially observable Markov decision
processes (POMDPs), when the value function is the conditional value at risk
(CVaR) of the return. Calculating an optimal solution for POMDPs is
computationally intractable in general. In this work we develop a
simplification framework to speedup the evaluation of the value function, while
providing performance guarantees. We consider as simplification a
computationally cheaper belief-MDP transition model, that can correspond, e.g.,
to cheaper observation or transition models. Our contributions include general
bounds for CVaR that allow bounding the CVaR of a random variable X, using a
random variable Y, by assuming bounds between their cumulative distributions.
We then derive bounds for the CVaR value function in a POMDP setting, and show
how to bound the value function using the computationally cheaper belief-MDP
transition model and without accessing the computationally expensive model in
real-time. Then, we provide theoretical performance guarantees for the
estimated bounds. Our results apply for a general simplification of a
belief-MDP transition model and support simplification of both the observation
and state transition models simultaneously.

摘要：在部分可观察域中，规避风险的决策制定是人工智能中的一个基本问题，对于可靠的自主代理至关重要。在我们的案例中，该问题使用部分可观察马尔可夫决策过程 (POMDP) 建模，其中值函数是收益的条件风险价值 (CVaR)。为 POMDP 计算最优解在通常情况下在计算上是难以处理的。在这项工作中，我们开发了一个简化框架来加速对值函数的评估，同时提供性能保证。我们认为简化是一种计算成本更低的信念-MDP 转换模型，例如，它可以对应于更便宜的观察或转换模型。我们的贡献包括 CVaR 的一般边界，它允许使用随机变量 Y 来限定随机变量 X 的 CVaR，方法是假设它们的累积分布之间的边界。然后，我们推导出 POMDP 设置中的 CVaR 值函数的边界，并展示如何使用计算成本更低的信念-MDP 转换模型来限定值函数，而无需实时访问计算成本昂贵的模型。然后，我们为估计的边界提供理论性能保证。我们的结果适用于信念-MDP 转换模型的一般简化，并同时支持观察和状态转换模型的简化。

##### **Learning Semantic Traversability with Egocentric Video and Automated Annotation Strategy**
2406.02989v1 by Yunho Kim, Jeong Hyun Lee, Choongin Lee, Juhyeok Mun, Donghoon Youm, Jeongsoo Park, Jemin Hwangbo

For reliable autonomous robot navigation in urban settings, the robot must
have the ability to identify semantically traversable terrains in the image
based on the semantic understanding of the scene. This reasoning ability is
based on semantic traversability, which is frequently achieved using semantic
segmentation models fine-tuned on the testing domain. This fine-tuning process
often involves manual data collection with the target robot and annotation by
human labelers which is prohibitively expensive and unscalable. In this work,
we present an effective methodology for training a semantic traversability
estimator using egocentric videos and an automated annotation process.
Egocentric videos are collected from a camera mounted on a pedestrian's chest.
The dataset for training the semantic traversability estimator is then
automatically generated by extracting semantically traversable regions in each
video frame using a recent foundation model in image segmentation and its
prompting technique. Extensive experiments with videos taken across several
countries and cities, covering diverse urban scenarios, demonstrate the high
scalability and generalizability of the proposed annotation method.
Furthermore, performance analysis and real-world deployment for autonomous
robot navigation showcase that the trained semantic traversability estimator is
highly accurate, able to handle diverse camera viewpoints, computationally
light, and real-world applicable. The summary video is available at
https://youtu.be/EUVoH-wA-lA.

摘要：對於在城市環境中可靠的自主機器人導航，機器人必須具備根據場景的語義理解，識別圖像中語義可穿越地形的的能力。這種推理能力基於語義可穿越性，而語義可穿越性通常使用在測試領域微調的語義分割模型來實現。這個微調過程通常涉及使用目標機器人手動收集資料，以及由人工標籤員進行標註，這成本過高且無法擴展。在這項工作中，我們提出了一個使用自我中心影片和自動標註過程來訓練語義可穿越性估計器的有效方法。自我中心影片是從安裝在行人胸前的相機收集的。然後，通過使用影像分割中最近的基礎模型及其提示技術，從每個影片格中提取語義可穿越區域，自動生成用於訓練語義可穿越性估計器的資料集。使用跨越多個國家和城市拍攝的影片進行的廣泛實驗，涵蓋了不同的城市場景，證明了所提出的標註方法具有高度的可擴展性和通用性。此外，對於自主機器人導航的效能分析和實際部署展示了訓練後的語義可穿越性估計器具有高度的準確性，能夠處理不同的相機視角，計算量輕，且適用於實際世界。摘要影片可於 https://youtu.be/EUVoH-wA-lA 取得。

##### **Tensor Polynomial Additive Model**
2406.02980v1 by Yang Chen, Ce Zhu, Jiani Liu, Yipeng Liu

Additive models can be used for interpretable machine learning for their
clarity and simplicity. However, In the classical models for high-order data,
the vectorization operation disrupts the data structure, which may lead to
degenerated accuracy and increased computational complexity. To deal with these
problems, we propose the tensor polynomial addition model (TPAM). It retains
the multidimensional structure information of high-order inputs with tensor
representation. The model parameter compression is achieved using a
hierarchical and low-order symmetric tensor approximation. In this way, complex
high-order feature interactions can be captured with fewer parameters.
Moreover, The TPAM preserves the inherent interpretability of additive models,
facilitating transparent decision-making and the extraction of meaningful
feature values. Additionally, leveraging TPAM's transparency and ability to
handle higher-order features, it is used as a post-processing module for other
interpretation models by introducing two variants for class activation maps.
Experimental results on a series of datasets demonstrate that TPAM can enhance
accuracy by up to 30\%, and compression rate by up to 5 times, while
maintaining a good interpretability.

摘要：加法模型因其清晰和简单性而可用于可解释的机器学习。然而，在高阶数据的经典模型中，向量化操作会破坏数据结构，这可能导致精度下降和计算复杂度增加。为了解决这些问题，我们提出了张量多项式加法模型 (TPAM)。它使用张量表示保留了高阶输入的多维结构信息。模型参数压缩是使用分层和低阶对称张量近似来实现的。通过这种方式，可以用更少的参数捕获复杂的高阶特征交互。此外，TPAM 保留了加法模型的固有可解释性，促进了透明的决策制定和有意义的特征值的提取。此外，利用 TPAM 的透明性和处理高阶特征的能力，它被用作其他解释模型的后处理模块，方法是为类激活映射引入两个变体。一系列数据集上的实验结果表明，TPAM 可以将准确率提高高达 30%，压缩率提高高达 5 倍，同时保持良好的可解释性。

##### **Efficient User Sequence Learning for Online Services via Compressed Graph Neural Networks**
2406.02979v1 by Yucheng Wu, Liyue Chen, Yu Cheng, Shuai Chen, Jinyu Xu, Leye Wang

Learning representations of user behavior sequences is crucial for various
online services, such as online fraudulent transaction detection mechanisms.
Graph Neural Networks (GNNs) have been extensively applied to model sequence
relationships, and extract information from similar sequences. While user
behavior sequence data volume is usually huge for online applications, directly
applying GNN models may lead to substantial computational overhead during both
the training and inference stages and make it challenging to meet real-time
requirements for online services. In this paper, we leverage graph compression
techniques to alleviate the efficiency issue. Specifically, we propose a novel
unified framework called ECSeq, to introduce graph compression techniques into
relation modeling for user sequence representation learning. The key module of
ECSeq is sequence relation modeling, which explores relationships among
sequences to enhance sequence representation learning, and employs graph
compression algorithms to achieve high efficiency and scalability. ECSeq also
exhibits plug-and-play characteristics, seamlessly augmenting pre-trained
sequence representation models without modifications. Empirical experiments on
both sequence classification and regression tasks demonstrate the effectiveness
of ECSeq. Specifically, with an additional training time of tens of seconds in
total on 100,000+ sequences and inference time preserved within $10^{-4}$
seconds/sample, ECSeq improves the prediction R@P$_{0.9}$ of the widely used
LSTM by $\sim 5\%$.

摘要：學習使用者行為序列的表示對於各種線上服務至關重要，例如線上詐騙交易偵測機制。圖神經網路 (GNN) 已廣泛應用於模型序列關係，並從類似序列中提取資訊。雖然使用者行為序列資料量對於線上應用來說通常很大，但直接應用 GNN 模型可能會在訓練和推論階段導致大量的計算負擔，並使得滿足線上服務的即時需求變得具有挑戰性。在本文中，我們利用圖壓縮技術來減輕效率問題。具體來說，我們提出一個名為 ECSeq 的新統一架構，將圖壓縮技術引入關係建模中，以進行使用者序列表示學習。ECSeq 的關鍵模組是序列關係建模，它探索序列之間的關係以增強序列表示學習，並採用圖壓縮演算法來實現高效率和可擴充性。ECSeq 還展現了即插即用的特性，無需修改即可無縫地擴充預先訓練的序列表示模型。在序列分類和回歸任務上的實證實驗證明了 ECSeq 的有效性。具體來說，在 100,000 多個序列上總共額外訓練數十秒，並且推論時間保持在 $10^{-4}$ 秒/樣本內，ECSeq 將廣泛使用的 LSTM 的預測 R@P$_{0.9}$ 提升了約 5%。

##### **DA-Flow: Dual Attention Normalizing Flow for Skeleton-based Video Anomaly Detection**
2406.02976v1 by Ruituo Wu, Yang Chen, Jian Xiao, Bing Li, Jicong Fan, Frédéric Dufaux, Ce Zhu, Yipeng Liu

Cooperation between temporal convolutional networks (TCN) and graph
convolutional networks (GCN) as a processing module has shown promising results
in skeleton-based video anomaly detection (SVAD). However, to maintain a
lightweight model with low computational and storage complexity, shallow GCN
and TCN blocks are constrained by small receptive fields and a lack of
cross-dimension interaction capture. To tackle this limitation, we propose a
lightweight module called the Dual Attention Module (DAM) for capturing
cross-dimension interaction relationships in spatio-temporal skeletal data. It
employs the frame attention mechanism to identify the most significant frames
and the skeleton attention mechanism to capture broader relationships across
fixed partitions with minimal parameters and flops. Furthermore, the proposed
Dual Attention Normalizing Flow (DA-Flow) integrates the DAM as a
post-processing unit after GCN within the normalizing flow framework.
Simulations show that the proposed model is robust against noise and negative
samples. Experimental results show that DA-Flow reaches competitive or better
performance than the existing state-of-the-art (SOTA) methods in terms of the
micro AUC metric with the fewest number of parameters. Moreover, we found that
even without training, simply using random projection without dimensionality
reduction on skeleton data enables substantial anomaly detection capabilities.

摘要：時序卷積網路 (TCN) 和圖形卷積網路 (GCN) 作為處理模組的合作，在基於骨架的影片異常偵測 (SVAD) 中展現了令人振奮的成果。然而，為了維持輕量級模型，並降低運算和儲存複雜度，淺層 GCN 和 TCN 區塊受到小感受野和缺乏跨維度互動擷取的限制。為了克服這個限制，我們提出一個名為雙注意力模組 (DAM) 的輕量級模組，用於擷取時空骨架資料中的跨維度互動關係。它採用幀注意力機制來識別最重要的幀，並採用骨架注意力機制來擷取跨越固定分割的更廣泛關係，且具有最少的參數和浮點運算。此外，所提出的雙注意力正規化流 (DA-Flow) 將 DAM 整合為正規化流架構中 GCN 之後的後處理單元。模擬顯示，所提出的模型對於雜訊和負面樣本具有穩健性。實驗結果顯示，DA-Flow 在微型 AUC 指標方面，以最少的參數數量達到與現有最先進 (SOTA) 方法競爭或更好的效能。此外，我們發現即使不進行訓練，僅在骨架資料上使用隨機投影而不進行維度削減，也能實現大量的異常偵測功能。

##### **Readability-guided Idiom-aware Sentence Simplification (RISS) for Chinese**
2406.02974v1 by Jingshen Zhang, Xinglu Chen, Xinying Qiu, Zhimin Wang, Wenhe Feng

Chinese sentence simplification faces challenges due to the lack of
large-scale labeled parallel corpora and the prevalence of idioms. To address
these challenges, we propose Readability-guided Idiom-aware Sentence
Simplification (RISS), a novel framework that combines data augmentation
techniques with lexcial simplification. RISS introduces two key components: (1)
Readability-guided Paraphrase Selection (RPS), a method for mining high-quality
sentence pairs, and (2) Idiom-aware Simplification (IAS), a model that enhances
the comprehension and simplification of idiomatic expressions. By integrating
RPS and IAS using multi-stage and multi-task learning strategies, RISS
outperforms previous state-of-the-art methods on two Chinese sentence
simplification datasets. Furthermore, RISS achieves additional improvements
when fine-tuned on a small labeled dataset. Our approach demonstrates the
potential for more effective and accessible Chinese text simplification.

摘要：中文句子簡化面臨缺乏大規模標記平行語料庫和成語盛行的挑戰。為了應對這些挑戰，我們提出了可讀性引導的成語感知句子簡化 (RISS)，這是一種結合數據擴充技術與詞彙簡化的創新架構。RISS 引入了兩個關鍵組成部分：(1) 可讀性引導的同義詞選擇 (RPS)，一種用於挖掘高品質句子對的方法，以及 (2) 成語感知簡化 (IAS)，一種增強成語表達理解和簡化的模型。透過使用多階段和多任務學習策略整合 RPS 和 IAS，RISS 在兩個中文句子簡化數據集上優於先前的最新方法。此外，RISS 在一個小型標記數據集上進行微調時獲得了額外的改進。我們的做法展示了更有效和可存取的中文文本簡化的潛力。

##### **Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture of Large Language Models**
2406.02969v1 by Raeid Saqur, Anastasis Kratsios, Florian Krach, Yannick Limmer, Jacob-Junqi Tian, John Willes, Blanka Horvath, Frank Rudzicz

We propose MoE-F -- a formalised mechanism for combining $N$ pre-trained
expert Large Language Models (LLMs) in online time-series prediction tasks by
adaptively forecasting the best weighting of LLM predictions at every time
step. Our mechanism leverages the conditional information in each expert's
running performance to forecast the best combination of LLMs for predicting the
time series in its next step. Diverging from static (learned) Mixture of
Experts (MoE) methods, MoE-F employs time-adaptive stochastic filtering
techniques to combine experts. By framing the expert selection problem as a
finite state-space, continuous-time Hidden Markov model (HMM), we can leverage
the Wohman-Shiryaev filter. Our approach first constructs $N$ parallel filters
corresponding to each of the $N$ individual LLMs. Each filter proposes its best
combination of LLMs, given the information that they have access to.
Subsequently, the $N$ filter outputs are aggregated to optimize a lower bound
for the loss of the aggregated LLMs, which can be optimized in closed-form,
thus generating our ensemble predictor. Our contributions here are: (I) the
MoE-F algorithm -- deployable as a plug-and-play filtering harness, (II)
theoretical optimality guarantees of the proposed filtering-based gating
algorithm, and (III) empirical evaluation and ablative results using state of
the art foundational and MoE LLMs on a real-world Financial Market Movement
task where MoE-F attains a remarkable 17% absolute and 48.5% relative F1
measure improvement over the next best performing individual LLM expert.

摘要：<paragraph>我們提出 MoE-F，這是一種形式化的機制，用於在線上時間序列預測任務中結合 $N$ 個預先訓練的專家大型語言模型 (LLM)，方法是自適應地預測每個時間步長中 LLM 預測的最佳加權。我們的機制利用每個專家的執行效能中的條件資訊，預測用於預測其下一個步驟中時間序列的最佳 LLM 組合。與靜態（已學習）專家混合 (MoE) 方法不同，MoE-F 採用時間自適應隨機濾波技術來結合專家。透過將專家選擇問題建構為有限狀態空間、連續時間隱藏馬可夫模型 (HMM)，我們可以利用 Wohman-Shiryaev 濾波器。我們的做法首先建立 $N$ 個平行濾波器，分別對應於 $N$ 個個別 LLM。每個濾波器根據其可存取的資訊，提出其最佳的 LLM 組合。隨後，將 $N$ 個濾波器輸出聚合，以最佳化聚合 LLM 的損失下界，該損失下界可以在閉合形式中最佳化，從而產生我們的整體預測器。我們在此的貢獻包括：(I) MoE-F 演算法——可作為即插即用的濾波器套件進行部署，(II) 所提出的基於濾波的閘控演算法的理論最佳性保證，以及 (III) 使用最先進的基本 LLM 和 MoE LLM 對真實世界的金融市場變動任務進行經驗評估和消融結果，其中 MoE-F 在 F1 測量上獲得了顯著的 17% 絕對值和 48.5% 相對值改進，優於表現第二佳的個別 LLM 專家。</paragraph>

##### **Generative AI and Digital Neocolonialism in Global Education: Towards an Equitable Framework**
2406.02966v1 by Matthew Nyaaba, Alyson Leigh Wright, Gyu Lim Choi

This paper critically discusses how Generative Artificial Intelligence
(GenAI) might impose Western ideologies on non-Western societies, perpetuating
digital neocolonialism in education through its inherent biases and further
suggests strategies to mitigate these effects. Our discussions demonstrated
that GenAI can foster cultural imperialism by generating content that primarily
incorporates cultural references and examples relevant to Western students,
thereby alienating students from non-Western backgrounds. Also, the predominant
use of Western languages by GenAI can marginalize non-dominant languages,
making educational content less accessible to speakers of indigenous languages
and potentially impacting their ability to learn in their first language.
Additionally, GenAI often generates content and curricula that reflect the
perspectives of technologically dominant countries, overshadowing marginalized
indigenous knowledge and practices. Moreover, the cost of access to GenAI
intensifies educational inequality and the control of GenAI data could lead to
commercial exploitation without benefiting local students and their
communities. We propose human-centric reforms to prioritize cultural diversity
and equity in GenAI development; a liberatory design to empower educators and
students to identify and dismantle the oppressive structures within GenAI
applications; foresight by design to create an adjustable GenAI systems to meet
future educational needs, and finally, effective prompting skills to reduces
the retrieval of neocolonial outputs.

摘要：本文批判性地討論創生式人工智慧 (GenAI) 如何對非西方社會施加西方意識形態，並通過其固有的偏見在教育中延續數位新殖民主義，並進一步提出策略來減輕這些影響。我們的討論表明，GenAI 可以通過產生主要包含與西方學生相關的文化參照和範例的內容來促進文化帝國主義，從而疏遠非西方背景的學生。此外，GenAI 主要使用西方語言可能會邊緣化非主流語言，這使得教育內容對於原住民語言的使用者來說較難理解，並可能影響他們用母語學習的能力。此外，GenAI 通常會產生反映技術主導國家觀點的內容和課程，掩蓋了被邊緣化的原住民知識和實踐。此外，取得 GenAI 的費用加劇了教育不平等，而 GenAI 資料的控制可能會導致商業剝削，而不會使當地學生及其社區受益。我們提出以人為本的改革，以優先考慮 GenAI 開發中的文化多樣性和公平性；一種解放式的設計，使教育工作者和學生能夠識別和消滅 GenAI 應用程式中的壓迫性結構；通過設計的前瞻性，創造一個可調整的 GenAI 系統，以滿足未來的教育需求，最後，有效的提示技巧，以減少新殖民主義輸出的檢索。

##### **Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models**
2406.02962v1 by Qiang Sun, Yuanyi Luo, Wenxiao Zhang, Sirui Li, Jichunyang Li, Kai Niu, Xiangrui Kong, Wei Liu

Even for a conservative estimate, 80% of enterprise data reside in
unstructured files, stored in data lakes that accommodate heterogeneous
formats. Classical search engines can no longer meet information seeking needs,
especially when the task is to browse and explore for insight formulation. In
other words, there are no obvious search keywords to use. Knowledge graphs, due
to their natural visual appeals that reduce the human cognitive load, become
the winning candidate for heterogeneous data integration and knowledge
representation.
  In this paper, we introduce Docs2KG, a novel framework designed to extract
multimodal information from diverse and heterogeneous unstructured documents,
including emails, web pages, PDF files, and Excel files. Dynamically generates
a unified knowledge graph that represents the extracted key information,
Docs2KG enables efficient querying and exploration of document data lakes.
Unlike existing approaches that focus on domain-specific data sources or
pre-designed schemas, Docs2KG offers a flexible and extensible solution that
can adapt to various document structures and content types. The proposed
framework unifies data processing supporting a multitude of downstream tasks
with improved domain interpretability. Docs2KG is publicly accessible at
https://docs2kg.ai4wa.com, and a demonstration video is available at
https://docs2kg.ai4wa.com/Video.

摘要：即使採用保守估計，80% 的企業資料都存在於非結構化檔案中，儲存在容納異質格式的資料湖中。傳統搜尋引擎不再能滿足資訊搜尋需求，尤其是在任務是瀏覽和探索以形成見解時。換句話說，沒有明顯的搜尋關鍵字可以使用。知識圖譜因其自然視覺吸引力而降低了人類的認知負擔，成為異質資料整合和知識表示的獲勝候選者。
在本文中，我們介紹 Docs2KG，一個新穎的架構，旨在從多樣且異質的非結構化文件中擷取多模態資訊，包括電子郵件、網頁、PDF 檔案和 Excel 檔案。動態產生一個統一的知識圖譜，表示擷取的關鍵資訊，Docs2KG 能夠有效查詢和探索文件資料湖。與專注於特定領域資料來源或預先設計的架構的現有方法不同，Docs2KG 提供了一個靈活且可擴充的解決方案，可以適應各種文件結構和內容類型。所提出的架構統一了資料處理，支援多項下游任務，並改善了領域可解釋性。Docs2KG 可於 https://docs2kg.ai4wa.com 公開存取，示範影片可於 https://docs2kg.ai4wa.com/Video 取得。

##### **Adversarial Moment-Matching Distillation of Large Language Models**
2406.02959v1 by Chen Jia

Knowledge distillation (KD) has been shown to be highly effective in guiding
a student model with a larger teacher model and achieving practical benefits in
improving the computational and memory efficiency for large language models
(LLMs). State-of-the-art KD methods for LLMs mostly rely on minimizing explicit
distribution distance between teacher and student probability predictions.
Instead of optimizing these mandatory behaviour cloning objectives, we explore
an imitation learning strategy for KD of LLMs. In particular, we minimize the
imitation gap by matching the action-value moments of the teacher's behavior
from both on- and off-policy perspectives. To achieve this action-value
moment-matching goal, we propose an adversarial training algorithm to jointly
estimate the moment-matching distance and optimize the student policy to
minimize it. Results from both task-agnostic instruction-following experiments
and task-specific experiments demonstrate the effectiveness of our method and
achieve new state-of-the-art performance.

摘要：知識蒸餾 (KD) 已被證明在使用較大的教師模型指導學生模型時非常有效，並在改善大型語言模型 (LLM) 的計算和記憶體效率方面獲得實際效益。LLM 最先進的 KD 方法大多依賴於最小化教師和學生機率預測之間的明確分佈距離。我們探索了一種模仿學習策略，用於 LLM 的 KD，而不是最佳化這些強制行為複製目標。具體來說，我們透過比對教師行為的動作價值矩，從策略和非策略觀點來最小化模仿差距。為了達成此動作價值矩比對目標，我們提出了一種對抗訓練演算法，用於共同估計矩比對距離並最佳化學生策略以將其最小化。任務不可知指令遵循實驗和任務特定實驗的結果證明了我們方法的有效性，並達到了新的最先進效能。

##### **PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs**
2406.02958v1 by Charlie Hou, Akshat Shrivastava, Hongyuan Zhan, Rylan Conway, Trang Le, Adithya Sagar, Giulia Fanti, Daniel Lazar

On-device training is currently the most common approach for training machine
learning (ML) models on private, distributed user data. Despite this, on-device
training has several drawbacks: (1) most user devices are too small to train
large models on-device, (2) on-device training is communication- and
computation-intensive, and (3) on-device training can be difficult to debug and
deploy. To address these problems, we propose Private Evolution-Text
(PrE-Text), a method for generating differentially private (DP) synthetic
textual data. First, we show that across multiple datasets, training small
models (models that fit on user devices) with PrE-Text synthetic data
outperforms small models trained on-device under practical privacy regimes
($\epsilon=1.29$, $\epsilon=7.58$). We achieve these results while using
9$\times$ fewer rounds, 6$\times$ less client computation per round, and
100$\times$ less communication per round. Second, finetuning large models on
PrE-Text's DP synthetic data improves large language model (LLM) performance on
private data across the same range of privacy budgets. Altogether, these
results suggest that training on DP synthetic data can be a better option than
training a model on-device on private distributed data. Code is available at
https://github.com/houcharlie/PrE-Text.

摘要：目前，裝置上訓練是針對私人、分散使用者資料訓練機器學習 (ML) 模型最常見的方法。儘管如此，裝置上訓練有幾個缺點：(1) 大部分使用者裝置太小，無法在裝置上訓練大型模型，(2) 裝置上訓練需要大量通訊和運算，以及 (3) 裝置上訓練可能難以除錯和部署。為了解決這些問題，我們提出 Private Evolution-Text (PrE-Text)，這是一種產生差異化私人 (DP) 合成文字資料的方法。首先，我們展示在多個資料集上，使用 PrE-Text 合成資料訓練小型模型（適用於使用者裝置的模型）優於在實際隱私制度下訓練的裝置上小型模型（$\epsilon=1.29$、$\epsilon=7.58$）。我們在使用 9 倍較少回合、每一回合 6 倍較少用戶端運算以及每一回合 100 倍較少通訊的情況下，達到這些結果。其次，在 PrE-Text 的 DP 合成資料上微調大型模型，會改善大型語言模型 (LLM) 在相同隱私預算範圍內針對私人資料的效能。總之，這些結果顯示在 DP 合成資料上訓練會比在私人分散資料上訓練模型來得更好。程式碼可於 https://github.com/houcharlie/PrE-Text 取得。

##### **4D ASR: Joint Beam Search Integrating CTC, Attention, Transducer, and Mask Predict Decoders**
2406.02950v1 by Yui Sudo, Muhammad Shakeel, Yosuke Fukumoto, Brian Yan, Jiatong Shi, Yifan Peng, Shinji Watanabe

End-to-end automatic speech recognition (E2E-ASR) can be classified into
several network architectures, such as connectionist temporal classification
(CTC), recurrent neural network transducer (RNN-T), attention-based
encoder-decoder, and mask-predict models. Each network architecture has
advantages and disadvantages, leading practitioners to switch between these
different models depending on application requirements. Instead of building
separate models, we propose a joint modeling scheme where four decoders (CTC,
RNN-T, attention, and mask-predict) share the same encoder -- we refer to this
as 4D modeling. The 4D model is trained using multitask learning, which will
bring model regularization and maximize the model robustness thanks to their
complementary properties. To efficiently train the 4D model, we introduce a
two-stage training strategy that stabilizes multitask learning. In addition, we
propose three novel one-pass beam search algorithms by combining three decoders
(CTC, RNN-T, and attention) to further improve performance. These three beam
search algorithms differ in which decoder is used as the primary decoder. We
carefully evaluate the performance and computational tradeoffs associated with
each algorithm. Experimental results demonstrate that the jointly trained 4D
model outperforms the E2E-ASR models trained with only one individual decoder.
Furthermore, we demonstrate that the proposed one-pass beam search algorithm
outperforms the previously proposed CTC/attention decoding.

摘要：端到端自動語音辨識 (E2E-ASR) 可分類為數種網路架構，例如連接時間分類 (CTC)、遞迴神經網路轉換器 (RNN-T)、基於注意力的編碼器-解碼器和遮罩預測模型。每種網路架構都有優缺點，導致實務工作者根據應用程式需求在這些不同模型之間切換。我們提出一個聯合建模方案，其中四個解碼器 (CTC、RNN-T、注意力和遮罩預測) 共用同一個編碼器，而不是建構獨立的模型，我們稱之為 4D 建模。4D 模型使用多任務學習進行訓練，這將帶來模型正則化，並由於其互補特性而最大化模型的穩健性。為了有效訓練 4D 模型，我們引入了一個兩階段訓練策略，以穩定多任務學習。此外，我們透過結合三個解碼器 (CTC、RNN-T 和注意力) 來提出三種新穎的單次光束搜尋演算法，以進一步提升效能。這三種光束搜尋演算法的不同之處在於哪個解碼器被用作主要解碼器。我們仔細評估與每種演算法相關的效能和運算折衷。實驗結果證明，聯合訓練的 4D 模型優於僅使用一個個別解碼器訓練的 E2E-ASR 模型。此外，我們證明所提出的單次光束搜尋演算法優於先前提出的 CTC/注意力解碼。

##### **The Task-oriented Queries Benchmark (ToQB)**
2406.02943v1 by Keun Soo Yim

Task-oriented queries (e.g., one-shot queries to play videos, order food, or
call a taxi) are crucial for assessing the quality of virtual assistants,
chatbots, and other large language model (LLM)-based services. However, a
standard benchmark for task-oriented queries is not yet available, as existing
benchmarks in the relevant NLP (Natural Language Processing) fields have
primarily focused on task-oriented dialogues. Thus, we present a new
methodology for efficiently generating the Task-oriented Queries Benchmark
(ToQB) using existing task-oriented dialogue datasets and an LLM service. Our
methodology involves formulating the underlying NLP task to summarize the
original intent of a speaker in each dialogue, detailing the key steps to
perform the devised NLP task using an LLM service, and outlining a framework
for automating a major part of the benchmark generation process. Through a case
study encompassing three domains (i.e., two single-task domains and one
multi-task domain), we demonstrate how to customize the LLM prompts (e.g.,
omitting system utterances or speaker labels) for those three domains and
characterize the generated task-oriented queries. The generated ToQB dataset is
made available to the public. We further discuss new domains that can be added
to ToQB by community contributors and its practical applications.

摘要：以任務為導向的查詢（例如，播放影片、訂購食物或叫計程車的一次性查詢）對於評估虛擬助理、聊天機器人和其他大型語言模型 (LLM) 服務的品質至關重要。然而，目前還沒有以任務為導向的查詢的標準基準，因為相關自然語言處理 (NLP) 領域現有的基準主要集中在以任務為導向的對話上。因此，我們提出了一種新的方法，使用現有的以任務為導向的對話資料集和 LLM 服務，有效產生以任務為導向的查詢基準 (ToQB)。我們的做法包括制定基礎的 NLP 任務，以總結每個對話中說話者的原始意圖，詳細說明使用 LLM 服務執行所設計 NLP 任務的主要步驟，並概述自動化基準產生流程大部分的架構。透過一個涵蓋三個網域（即兩個單一任務網域和一個多任務網域）的案例研究，我們示範如何自訂 LLM 提示（例如，省略系統發言或說話者標籤）以符合這三個網域，並描述產生的以任務為導向的查詢。產生的 ToQB 資料集已公開提供。我們進一步討論社區貢獻者可以新增到 ToQB 的新網域，以及其實際應用。

##### **Multivariate Physics-Informed Convolutional Autoencoder for Anomaly Detection in Power Distribution Systems with High Penetration of DERs**
2406.02927v1 by Mehdi Jabbari Zideh, Sarika Khushalani Solanki

Despite the relentless progress of deep learning models in analyzing the
system conditions under cyber-physical events, their abilities are limited in
the power system domain due to data availability issues, cost of data
acquisition, and lack of interpretation and extrapolation for the data beyond
the training windows. In addition, the integration of distributed energy
resources (DERs) such as wind and solar generations increases the complexities
and nonlinear nature of power systems. Therefore, an interpretable and reliable
methodology is of utmost need to increase the confidence of power system
operators and their situational awareness for making reliable decisions. This
has led to the development of physics-informed neural network (PINN) models as
more interpretable, trustworthy, and robust models where the underlying
principled laws are integrated into the training process of neural network
models to achieve improved performance. This paper proposes a multivariate
physics-informed convolutional autoencoder (PIConvAE) model to detect cyber
anomalies in power distribution systems with unbalanced configurations and high
penetration of DERs. The physical laws are integrated through a customized loss
function that embeds the underlying Kirchhoff's circuit laws into the training
process of the autoencoder. The performance of the multivariate PIConvAE model
is evaluated on two unbalanced power distribution grids, IEEE 123-bus system
and a real-world feeder in Riverside, CA. The results show the exceptional
performance of the proposed method in detecting various cyber anomalies in both
systems. In addition, the model's effectiveness is evaluated in data scarcity
scenarios with different training data ratios. Finally, the model's performance
is compared with existing machine learning models where the PIConvAE model
surpasses other models with considerably higher detection metrics.

摘要：儘管深度學習模型在分析網路實體事件下的系統條件方面不斷進步，但由於資料取得問題、資料取得成本、以及訓練視窗外資料的詮釋和外推不足，其能力在電力系統領域受到限制。此外，風能和太陽能發電等分散式能源 (DER) 的整合增加了電力系統的複雜性和非線性性質。因此，迫切需要一種可解釋且可靠的方法來提升電力系統操作員的信心，並提高其情境意識以做出可靠的決策。這導致物理訊息神經網路 (PINN) 模型的發展，作為更具可解釋性、可信度和穩健性的模型，其中基本定律原則被整合到神經網路模型的訓練過程中以提升效能。本文提出一個多變量物理訊息卷積自動編碼器 (PIConvAE) 模型，以偵測具有不平衡組態和高 DER 滲透率的配電系統中的網路異常。物理定律透過一個自訂損失函數整合，將基礎的基爾霍夫電路定律嵌入自動編碼器的訓練過程中。多變量 PIConvAE 模型的效能評估於兩個不平衡配電電網，IEEE 123-bus 系統和加州河濱市的一個實際饋線。結果顯示所提出的方法在偵測兩個系統中的各種網路異常方面具有傑出的效能。此外，該模型的有效性在具有不同訓練資料比率的資料稀少情況下得到評估。最後，將該模型的效能與現有的機器學習模型進行比較，其中 PIConvAE 模型超越其他模型，具有顯著更高的偵測指標。

##### **SYN2REAL: Leveraging Task Arithmetic for Mitigating Synthetic-Real Discrepancies in ASR Domain Adaptation**
2406.02925v1 by Hsuan Su, Hua Farn, Shang-Tse Chen, Hung-yi Lee

Recent advancements in large language models (LLMs) have introduced the 'task
vector' concept, which has significantly impacted various domains but remains
underexplored in speech recognition. This paper presents a novel 'SYN2REAL'
task vector for domain adaptation in automatic speech recognition (ASR),
specifically targeting text-only domains. Traditional fine-tuning on synthetic
speech often results in performance degradation due to acoustic mismatches. To
address this issue, we propose creating a 'SYN2REAL' vector by subtracting the
parameter differences between models fine-tuned on real and synthetic speech.
This vector effectively bridges the gap between the two domains. Experiments on
the SLURP dataset demonstrate that our approach yields an average improvement
of 11.15% in word error rate for unseen target domains, highlighting the
potential of task vectors in enhancing speech domain adaptation.

摘要：大型語言模型 (LLM) 的最新進展引入了「任務向量」的概念，這對各個領域產生了重大影響，但在語音辨識方面仍未得到充分的探索。本文針對自動語音辨識 (ASR) 中的領域適應，提出了一個新的「SYN2REAL」任務向量，特別針對純文字領域。傳統上，在合成語音上進行微調通常會因聲學不匹配而導致效能下降。為了解決這個問題，我們建議透過減去針對真實語音和合成語音進行微調的模型之間的參數差異，來建立一個「SYN2REAL」向量。這個向量有效地彌合了這兩個領域之間的差距。在 SLURP 資料集上的實驗表明，我們的做法對未見目標領域的字元錯誤率產生了平均 11.15% 的改善，突顯了任務向量在增強語音領域適應方面的潛力。

##### **Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models**
2406.02924v1 by Peijie Dong, Lujun Li, Zhenheng Tang, Xiang Liu, Xinglin Pan, Qiang Wang, Xiaowen Chu

Despite the remarkable capabilities, Large Language Models (LLMs) face
deployment challenges due to their extensive size. Pruning methods drop a
subset of weights to accelerate, but many of them require retraining, which is
prohibitively expensive and computationally demanding. Recently, post-training
pruning approaches introduced novel metrics, enabling the pruning of LLMs
without retraining. However, these metrics require the involvement of human
experts and tedious trial and error. To efficiently identify superior pruning
metrics, we develop an automatic framework for searching symbolic pruning
metrics using genetic programming. In particular, we devise an elaborate search
space encompassing the existing pruning metrics to discover the potential
symbolic pruning metric. We propose an opposing operation simplification
strategy to increase the diversity of the population. In this way, Pruner-Zero
allows auto-generation of symbolic pruning metrics. Based on the searched
results, we explore the correlation between pruning metrics and performance
after pruning and summarize some principles. Extensive experiments on LLaMA and
LLaMA-2 on language modeling and zero-shot tasks demonstrate that our
Pruner-Zero obtains superior performance than SOTA post-training pruning
methods. Code at: \url{https://github.com/pprp/Pruner-Zero}.

摘要：儘管具有非凡的能力，但大型語言模型 (LLM) 因其龐大規模而面臨部署挑戰。修剪方法會捨棄一部分權重以加速，但其中許多方法需要重新訓練，這非常昂貴且在運算上要求很高。最近，訓練後修剪方法引進了新指標，能夠在不重新訓練的情況下修剪 LLM。然而，這些指標需要人類專家參與和繁瑣的試錯過程。為了有效找出優越的修剪指標，我們開發了一個自動化架構，使用遺傳程式設計來搜尋符號修剪指標。特別是，我們設計了一個精細的搜尋空間，包含現有的修剪指標，以找出潛在的符號修剪指標。我們提出一個對立運算簡化策略，以增加族群的多樣性。藉由這種方式，Pruner-Zero 允許自動產生符號修剪指標。根據搜尋結果，我們探討了修剪指標與修剪後效能之間的關聯性，並總結了一些原則。在 LLaMA 和 LLaMA-2 上針對語言建模和零次學習任務進行的廣泛實驗證明，我們的 Pruner-Zero 獲得比 SOTA 訓練後修剪方法更優異的效能。程式碼在：\url{https://github.com/pprp/Pruner-Zero}。

##### **Text Injection for Neural Contextual Biasing**
2406.02921v1 by Zhong Meng, Zelin Wu, Rohit Prabhavalkar, Cal Peyser, Weiran Wang, Nanxin Chen, Tara N. Sainath, Bhuvana Ramabhadran

Neural contextual biasing effectively improves automatic speech recognition
(ASR) for crucial phrases within a speaker's context, particularly those that
are infrequent in the training data. This work proposes contextual text
injection (CTI) to enhance contextual ASR. CTI leverages not only the paired
speech-text data, but also a much larger corpus of unpaired text to optimize
the ASR model and its biasing component. Unpaired text is converted into
speech-like representations and used to guide the model's attention towards
relevant bias phrases. Moreover, we introduce a contextual text-injected (CTI)
minimum word error rate (MWER) training, which minimizes the expected WER
caused by contextual biasing when unpaired text is injected into the model.
Experiments show that CTI with 100 billion text sentences can achieve up to
43.3% relative WER reduction from a strong neural biasing model. CTI-MWER
provides a further relative improvement of 23.5%.

摘要：神經脈絡偏置有效改善了自動語音辨識 (ASR) 中說話者脈絡內關鍵詞彙的辨識，特別是那些在訓練資料中出現頻率低的詞彙。本研究提出脈絡文本注入 (CTI) 來增強脈絡 ASR。CTI 不僅利用配對的語音文字資料，還利用一個更大的未配對文字語料庫來最佳化 ASR 模型及其偏置組成。未配對的文字轉換成類語音的表示，並用於引導模型的注意力朝向相關的偏置詞彙。此外，我們引入了脈絡文本注入 (CTI) 最小詞彙錯誤率 (MWER) 訓練，它會將未配對的文字注入模型時所造成的預期 WER 降到最低。實驗顯示，使用 1,000 億個文字句子的 CTI 可以讓強大的神經偏置模型的 WER 相對減少多達 43.3%。CTI-MWER 進一步提供了 23.5% 的相對進步。

##### **MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering Medical Knowledge**
2406.02919v1 by Yuxuan Zhou, Xien Liu, Chen Ning, Ji Wu

Large language models (LLMs) have excelled across domains, also delivering
notable performance on the medical evaluation benchmarks, such as MedQA.
However, there still exists a significant gap between the reported performance
and the practical effectiveness in real-world medical scenarios. In this paper,
we aim to explore the causes of this gap by employing a multifaceted
examination schema to systematically probe the actual mastery of medical
knowledge by current LLMs. Specifically, we develop a novel evaluation
framework MultifacetEval to examine the degree and coverage of LLMs in encoding
and mastering medical knowledge at multiple facets (comparison, rectification,
discrimination, and verification) concurrently. Based on the MultifacetEval
framework, we construct two multifaceted evaluation datasets: MultiDiseK (by
producing questions from a clinical disease knowledge base) and MultiMedQA (by
rephrasing each question from a medical benchmark MedQA into multifaceted
questions). The experimental results on these multifaceted datasets demonstrate
that the extent of current LLMs in mastering medical knowledge is far below
their performance on existing medical benchmarks, suggesting that they lack
depth, precision, and comprehensiveness in mastering medical knowledge.
Consequently, current LLMs are not yet ready for application in real-world
medical tasks. The codes and datasets are available at
https://github.com/THUMLP/MultifacetEval.

摘要：大型語言模型 (LLM) 在各個領域都表現出色，也在醫療評估基準（例如 MedQA）上取得顯著的成效。然而，在實際的醫療場景中，報告的成效與實際的效能之間仍存在顯著的差距。在本文中，我們旨在透過採用多面向的檢驗架構來系統性地探討實際醫療知識掌握程度，進而探討造成此差距的原因。具體來說，我們開發了一個新的評估架構 MultifacetEval，以檢驗 LLM 在編碼和掌握醫療知識的多個面向（比較、修正、區分和驗證）的程度和涵蓋範圍。基於 MultifacetEval 架構，我們構建了兩個多面向的評估資料集：MultiDiseK（透過從臨床疾病知識庫產生問題）和 MultiMedQA（透過將醫療基準 MedQA 中的每個問題改寫為多面向問題）。在這些多面向資料集上的實驗結果表明，當前 LLM 在掌握醫療知識方面的程度遠低於其在現有醫療基準上的表現，這表明它們在掌握醫療知識方面缺乏深度、精確度和全面性。因此，當前的 LLM 尚未準備好應用於實際的醫療任務。程式碼和資料集可在 https://github.com/THUMLP/MultifacetEval 取得。

##### **Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity**
2406.02913v1 by Wentao Guo, Jikai Long, Yimeng Zeng, Zirui Liu, Xinyu Yang, Yide Ran, Jacob R. Gardner, Osbert Bastani, Christopher De Sa, Xiaodong Yu, Beidi Chen, Zhaozhuo Xu

Zeroth-order optimization (ZO) is a memory-efficient strategy for fine-tuning
Large Language Models using only forward passes. However, the application of ZO
fine-tuning in memory-constrained settings such as mobile phones and laptops is
still challenging since full precision forward passes are infeasible. In this
study, we address this limitation by integrating sparsity and quantization into
ZO fine-tuning of LLMs. Specifically, we investigate the feasibility of
fine-tuning an extremely small subset of LLM parameters using ZO. This approach
allows the majority of un-tuned parameters to be quantized to accommodate the
constraint of limited device memory. Our findings reveal that the pre-training
process can identify a set of "sensitive parameters" that can guide the ZO
fine-tuning of LLMs on downstream tasks. Our results demonstrate that
fine-tuning 0.1% sensitive parameters in the LLM with ZO can outperform the
full ZO fine-tuning performance, while offering wall-clock time speedup.
Additionally, we show that ZO fine-tuning targeting these 0.1% sensitive
parameters, combined with 4 bit quantization, enables efficient ZO fine-tuning
of an Llama2-7B model on a GPU device with less than 8 GiB of memory and
notably reduced latency.

摘要：零階最佳化 (ZO) 是一種記憶體效率高的策略，用於僅使用前向傳遞來微調大型語言模型。然而，ZO 微調在記憶體受限的設定中（例如行動電話和筆電）的應用仍然具有挑戰性，因為全精度的前向傳遞不可行。在本研究中，我們透過將稀疏性和量化整合到 LLM 的 ZO 微調中來解決這個限制。具體來說，我們探討了使用 ZO 微調 LLM 極小參數子集的可行性。這種方法允許將大多數未調整的參數量化，以適應裝置記憶體有限的限制。我們的研究結果顯示，預訓練過程可以找出一個「敏感參數」集合，用於指導 LLM 在下游任務上的 ZO 微調。我們的結果證明，使用 ZO 微調 LLM 中的 0.1% 敏感參數可以優於完整的 ZO 微調效能，同時提供牆上時間加速。此外，我們表明 ZO 微調針對這些 0.1% 敏感參數，結合 4 位元量化，可以在記憶體小於 8 GiB 的 GPU 裝置上有效執行 Llama2-7B 模型的 ZO 微調，而且大幅降低延遲。

##### **Improving In-Context Learning with Prediction Feedback for Sentiment Analysis**
2406.02911v1 by Hongling Xu, Qianlong Wang, Yice Zhang, Min Yang, Xi Zeng, Bing Qin, Ruifeng Xu

Large language models (LLMs) have achieved promising results in sentiment
analysis through the in-context learning (ICL) paradigm. However, their ability
to distinguish subtle sentiments still remains a challenge. Inspired by the
human ability to adjust understanding via feedback, this paper enhances ICL by
incorporating prior predictions and feedback, aiming to rectify sentiment
misinterpretation of LLMs. Specifically, the proposed framework consists of
three steps: (1) acquiring prior predictions of LLMs, (2) devising predictive
feedback based on correctness, and (3) leveraging a feedback-driven prompt to
refine sentiment understanding. Experimental results across nine sentiment
analysis datasets demonstrate the superiority of our framework over
conventional ICL methods, with an average F1 improvement of 5.95%.

摘要：大型語言模型 (LLM) 已透過情境學習 (ICL) 典範在情緒分析中取得令人滿意的成果。然而，它們區分細微情緒的能力仍然是一項挑戰。受到人類透過回饋調整理解能力的啟發，本文透過整合先前的預測和回饋來增強 ICL，旨在修正 LLM 對情緒的誤解。具體來說，所提出的架構包含三個步驟：(1) 獲取 LLM 的先前預測，(2) 根據正確性設計預測回饋，以及 (3) 利用回饋驅動提示來改善情緒理解。橫跨九個情緒分析資料集的實驗結果證明了我們架構優於傳統 ICL 方法，F1 平均提升了 5.95%。

##### **Open Grounded Planning: Challenges and Benchmark Construction**
2406.02903v1 by Shiguang Guo, Ziliang Deng, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun

The emergence of large language models (LLMs) has increasingly drawn
attention to the use of LLMs for human-like planning. Existing work on
LLM-based planning either focuses on leveraging the inherent language
generation capabilities of LLMs to produce free-style plans, or employs
reinforcement learning approaches to learn decision-making for a limited set of
actions within restricted environments. However, both approaches exhibit
significant discrepancies from the open and executable requirements in
real-world planning. In this paper, we propose a new planning task--open
grounded planning. The primary objective of open grounded planning is to ask
the model to generate an executable plan based on a variable action set,
thereby ensuring the executability of the produced plan. To this end, we
establishes a benchmark for open grounded planning spanning a wide range of
domains. Then we test current state-of-the-art LLMs along with five planning
approaches, revealing that existing LLMs and methods still struggle to address
the challenges posed by grounded planning in open domains. The outcomes of this
paper define and establish a foundational dataset for open grounded planning,
and shed light on the potential challenges and future directions of LLM-based
planning.

摘要：隨著大型語言模型 (LLM) 的出現，人們越來越關注將 LLM 用於類人規劃。現有的基於 LLM 的規劃工作，要嘛著重於利用 LLM 固有的語言生成能力來產生自由形式的計畫，要嘛採用強化學習方法來學習在受限環境中針對有限的動作集進行決策。然而，這兩種方法都與現實世界的規劃中的開放和可執行需求有顯著的差異。在本文中，我們提出了一項新的規劃任務——開放式接地規劃。開放式接地規劃的主要目標是要求模型根據可變動作集產生可執行計畫，從而確保產生的計畫的可執行性。為此，我們建立了一個開放式接地規劃基準，涵蓋廣泛的領域。然後，我們測試了現有的最先進的 LLM 以及五種規劃方法，結果發現現有的 LLM 和方法仍然難以應對開放式領域中接地規劃帶來的挑戰。本文的成果定義並建立了一個開放式接地規劃基礎資料集，並闡明了基於 LLM 的規劃的潛在挑戰和未來方向。

##### **Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms**
2406.02900v1 by Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, Scott Niekum

Reinforcement Learning from Human Feedback (RLHF) has been crucial to the
recent success of Large Language Models (LLMs), however, it is often a complex
and brittle process. In the classical RLHF framework, a reward model is first
trained to represent human preferences, which is in turn used by an online
reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue
with such methods is \emph{reward over-optimization} or \emph{reward hacking},
where performance as measured by the learned proxy reward model increases, but
true quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs)
like Direct Preference Optimization have emerged as alternatives to the
classical RLHF pipeline by circumventing the reward modeling phase. However,
although DAAs do not use a separate proxy reward model, they still commonly
deteriorate from over-optimization. While the so-called reward hacking
phenomenon is not well-defined for DAAs, we still uncover similar trends: at
higher KL budgets, DAA algorithms exhibit similar degradation patterns to their
classic RLHF counterparts. In particular, we find that DAA methods deteriorate
not only across a wide range of KL budgets but also often before even a single
epoch of the dataset is completed. Through extensive empirical experimentation,
this work formulates and formalizes the reward over-optimization or hacking
problem for DAAs and explores its consequences across objectives, training
regimes, and model scales.

摘要：強化學習從人類回饋（RLHF）已經成為大型語言模型（LLMs）最近成功的關鍵，然而，這通常是一個複雜且脆弱的過程。在經典的 RLHF 框架中，首先訓練一個獎勵模型來表示人類偏好，然後由在線強化學習（RL）演算法用於最佳化 LLM。此類方法的一個顯著問題是\emph{獎勵過度最佳化}或\emph{獎勵破解}，其中根據已學習的代理獎勵模型測量的效能會增加，但真正的品質會停滯甚至惡化。直接對齊演算法（DDA），例如直接偏好最佳化，已作為迴避獎勵建模階段的經典 RLHF 管線的替代方案出現。然而，儘管 DDA 不使用單獨的代理獎勵模型，但它們仍然普遍因過度最佳化而惡化。雖然所謂的獎勵破解現象對於 DDA 來說並未定義明確，但我們仍然發現類似的趨勢：在較高的 KL 預算下，DAA 演算法會表現出與其經典 RLHF 對應項類似的衰退模式。特別是，我們發現 DAA 方法不僅在廣泛的 KL 預算中惡化，而且通常甚至在資料集的單個世代完成之前就會惡化。透過廣泛的實證實驗，這項工作制定並形式化了 DDA 的獎勵過度最佳化或破解問題，並探討其對目標、訓練機制和模型規模的影響。

##### **Language Model Can Do Knowledge Tracing: Simple but Effective Method to Integrate Language Model and Knowledge Tracing Task**
2406.02893v1 by Unggi Lee, Jiyeong Bae, Dohee Kim, Sookbun Lee, Jaekwon Park, Taekyung Ahn, Gunho Lee, Damji Stratton, Hyeoncheol Kim

Knowledge Tracing (KT) is a critical task in online learning for modeling
student knowledge over time. Despite the success of deep learning-based KT
models, which rely on sequences of numbers as data, most existing approaches
fail to leverage the rich semantic information in the text of questions and
concepts. This paper proposes Language model-based Knowledge Tracing (LKT), a
novel framework that integrates pre-trained language models (PLMs) with KT
methods. By leveraging the power of language models to capture semantic
representations, LKT effectively incorporates textual information and
significantly outperforms previous KT models on large benchmark datasets.
Moreover, we demonstrate that LKT can effectively address the cold-start
problem in KT by leveraging the semantic knowledge captured by PLMs.
Interpretability of LKT is enhanced compared to traditional KT models due to
its use of text-rich data. We conducted the local interpretable model-agnostic
explanation technique and analysis of attention scores to interpret the model
performance further. Our work highlights the potential of integrating PLMs with
KT and paves the way for future research in KT domain.

摘要：知識追蹤 (KT) 是線上學習中一項重要的任務，用於建模學生的知識。儘管基於深度學習的 KT 模型很成功，這些模型依賴於數字序列作為資料，但現有的大部分方法都無法利用問題和概念文字中的豐富語意資訊。本文提出基於語言模型的知識追蹤 (LKT)，一個將預先訓練好的語言模型 (PLM) 與 KT 方法整合起來的新穎架構。透過利用語言模型擷取語意表徵的能力，LKT 有效地納入了文字資訊，並且在大型基準資料集上明顯優於先前的 KT 模型。此外，我們證明 LKT 可以透過利用 PLM 擷取的語意知識，有效地解決 KT 中的冷啟動問題。由於 LKT 使用了豐富文字的資料，因此與傳統的 KT 模型相比，其可解釋性得到了增強。我們執行了局部可解釋模型不可知解釋技術和注意力分數分析，以進一步解釋模型效能。我們的研究重點在於整合 PLM 與 KT 的潛力，並為 KT 領域的未來研究鋪路。

##### **HYDRA: Model Factorization Framework for Black-Box LLM Personalization**
2406.02888v1 by Yuchen Zhuang, Haotian Sun, Yue Yu, Qifan Wang, Chao Zhang, Bo Dai

Personalization has emerged as a critical research area in modern intelligent
systems, focusing on mining users' behavioral history and adapting to their
preferences for delivering tailored experiences. Despite the remarkable
few-shot capabilities exhibited by black-box large language models (LLMs), the
inherent opacity of their model parameters presents significant challenges in
aligning the generated output with individual expectations. Existing solutions
have primarily focused on prompt design to incorporate user-specific profiles
and behaviors; however, such approaches often struggle to generalize
effectively due to their inability to capture shared knowledge among all users.
To address these challenges, we propose HYDRA, a model factorization framework
that captures both user-specific behavior patterns from historical data and
shared general knowledge among all users to deliver personalized generation. In
order to capture user-specific behavior patterns, we first train a reranker to
prioritize the most useful information from top-retrieved relevant historical
records. By combining the prioritized history with the corresponding query, we
train an adapter to align the output with individual user-specific preferences,
eliminating the reliance on access to inherent model parameters of black-box
LLMs. Both the reranker and the adapter can be decomposed into a base model
with multiple user-specific heads, resembling a hydra. The base model maintains
shared knowledge across users, while the multiple personal heads capture
user-specific preferences. Experimental results demonstrate that HYDRA
outperforms existing state-of-the-art prompt-based methods by an average
relative improvement of 9.01% across five diverse personalization tasks in the
LaMP benchmark. Our implementation is available at
https://github.com/night-chen/HYDRA.

摘要：<paragraph>個人化已成為現代智慧系統中一項重要的研究領域，專注於挖掘使用者的行為歷史，並根據其偏好提供客製化體驗。儘管黑盒子大型語言模型 (LLM) 展現出非凡的少量次數能力，但其模型參數固有的不透明性在將產生的輸出與個別期望相符時，帶來重大挑戰。現有的解決方案主要專注於提示設計，以納入使用者特定的個人資料和行為；然而，此類方法由於無法擷取所有使用者之間的共用知識，因此通常難以有效概化。為了應對這些挑戰，我們提出 HYDRA，這是一個模型分解架構，可從歷史資料中擷取使用者特定的行為模式，並在所有使用者之間共用一般知識，以提供個人化生成。為了擷取使用者特定的行為模式，我們首先訓練一個重新排名器，以優先處理從頂端擷取到的相關歷史記錄中最有用的資訊。透過將優先排序的歷史記錄與對應的查詢相結合，我們訓練一個適配器，以將輸出與個別使用者的特定偏好相符，無需依賴存取黑盒子 LLM 的固有模型參數。重新排名器和適配器都可以分解成一個具有多個使用者特定主體的基礎模型，類似於九頭蛇。基礎模型維護使用者之間的共用知識，而多個個人主體則擷取使用者特定的偏好。實驗結果顯示，在 LaMP 基準中的五項不同的個人化任務中，HYDRA 的表現優於現有的最先進提示式方法，平均相對改善幅度為 9.01%。我們的實作可在 https://github.com/night-chen/HYDRA 取得。</paragraph>

##### **PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs**
2406.02886v1 by Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Haorui Wang, Zhen Qin, Feng Han, Jialu Liu, Simon Baumgartner, Michael Bendersky, Chao Zhang

Large Language Models (LLMs) have exhibited impressive capabilities in
various tasks, yet their vast parameter sizes restrict their applicability in
resource-constrained settings. Knowledge distillation (KD) offers a viable
solution by transferring expertise from large teacher models to compact student
models. However, traditional KD techniques face specific challenges when
applied to LLMs, including restricted access to LLM outputs, significant
teacher-student capacity gaps, and the inherited mis-calibration issue. In this
work, we present PLaD, a novel preference-based LLM distillation framework.
PLaD exploits the teacher-student capacity discrepancy to generate
pseudo-preference pairs where teacher outputs are preferred over student
outputs. Then, PLaD leverages a ranking loss to re-calibrate student's
estimation of sequence likelihood, which steers the student's focus towards
understanding the relative quality of outputs instead of simply imitating the
teacher. PLaD bypasses the need for access to teacher LLM's internal states,
tackles the student's expressivity limitations, and mitigates the student
mis-calibration issue. Through extensive experiments on two sequence generation
tasks and with various LLMs, we demonstrate the effectiveness of our proposed
PLaD framework.

摘要：大型語言模型 (LLM) 已在各種任務中展現出令人印象深刻的能力，但其龐大的參數規模限制了它們在資源受限環境中的適用性。知識蒸餾 (KD) 提供了一個可行的解決方案，方法是將大型教師模型的專業知識轉移到精簡的學生模型。然而，當應用於 LLM 時，傳統的 KD 技術會面臨具體挑戰，包括受限訪問 LLM 輸出、顯著的教師學生容量差距以及繼承的錯誤校準問題。在這項工作中，我們提出了 PLaD，這是一個基於偏好的 LLM 蒸餾框架。PLaD 利用教師學生容量差異來生成偽偏好對，其中教師輸出優先於學生輸出。然後，PLaD 利用排名損失來重新校準學生對序列可能性的估計，這將學生的注意力引導到理解輸出的相對品質，而不是簡單地模仿教師。PLaD 避開了訪問教師 LLM 內部狀態的需要，解決了學生的表現力限制，並減輕了學生的錯誤校準問題。通過在兩個序列生成任務和各種 LLM 上進行廣泛的實驗，我們證明了我們提出的 PLaD 框架的有效性。

##### **Outdated Issue Aware Decoding for Factual Knowledge Editing**
2406.02882v1 by Zengkui Sun, Yijin Liu, Jiaan Wang, Fandong Meng, Jinan Xu, Yufeng Chen, Jie Zhou

Recently, Knowledge Editing has received increasing attention, since it could
update the specific knowledge from outdated ones in pretrained models without
re-training. However, as pointed out by recent studies, existing related
methods tend to merely memorize the superficial word composition of the edited
knowledge, rather than truly learning and absorbing it. Consequently, on the
reasoning questions, we discover that existing methods struggle to utilize the
edited knowledge to reason the new answer, and tend to retain outdated
responses, which are generated by the original models utilizing original
knowledge. Nevertheless, the outdated responses are unexpected for the correct
answers to reasoning questions, which we named as the outdated issue. To
alleviate this issue, in this paper, we propose a simple yet effective decoding
strategy, i.e., outDated ISsue aware deCOding (DISCO), to enhance the
performance of edited models on reasoning questions. Specifically, we capture
the difference in the probability distribution between the original and edited
models. Further, we amplify the difference of the token prediction in the
edited model to alleviate the outdated issue, and thus enhance the model
performance w.r.t the edited knowledge. Experimental results suggest that
applying DISCO could enhance edited models to reason, e.g., on reasoning
questions, DISCO outperforms the prior SOTA method by 12.99 F1 scores, and
reduces the ratio of the outdated issue to 5.78% on the zsRE dataset.

摘要：近年來，知識編輯受到越來越多的關注，因為它可以在不重新訓練的情況下，從預訓練模型中更新過時的特定知識。然而，正如最近的研究指出的，現有的相關方法往往只是記住編輯知識的表面字詞組成，而不是真正學習和吸收它。因此，在推理問題上，我們發現現有方法難以利用編輯知識來推理新答案，並且傾向於保留過時的回應，這些回應是由原始模型利用原始知識生成的。儘管如此，過時的回應對於推理問題的正確答案來說是意料之外的，我們稱之為過時問題。為了緩解這個問題，在本文中，我們提出了一個簡單但有效的解碼策略，即過時問題感知解碼 (DISCO)，以增強編輯模型在推理問題上的效能。具體來說，我們捕捉原始模型和編輯模型之間概率分佈的差異。此外，我們擴大了編輯模型中標記預測的差異，以緩解過時問題，從而增強模型相對於編輯知識的效能。實驗結果表明，應用 DISCO 可以增強編輯模型的推理能力，例如，在推理問題上，DISCO 在 F1 分數上優於先前的 SOTA 方法 12.99，並將 zsRE 資料集上的過時問題比率降低至 5.78%。

##### **Controllable Talking Face Generation by Implicit Facial Keypoints Editing**
2406.02880v1 by Dong Zhao, Jiaying Shi, Wenjun Li, Shudong Wang, Shenghui Xu, Zhaoming Pan

Audio-driven talking face generation has garnered significant interest within
the domain of digital human research. Existing methods are encumbered by
intricate model architectures that are intricately dependent on each other,
complicating the process of re-editing image or video inputs. In this work, we
present ControlTalk, a talking face generation method to control face
expression deformation based on driven audio, which can construct the head pose
and facial expression including lip motion for both single image or sequential
video inputs in a unified manner. By utilizing a pre-trained video synthesis
renderer and proposing the lightweight adaptation, ControlTalk achieves precise
and naturalistic lip synchronization while enabling quantitative control over
mouth opening shape. Our experiments show that our method is superior to
state-of-the-art performance on widely used benchmarks, including HDTF and
MEAD. The parameterized adaptation demonstrates remarkable generalization
capabilities, effectively handling expression deformation across same-ID and
cross-ID scenarios, and extending its utility to out-of-domain portraits,
regardless of languages.

摘要：受音訊驅動的說話人臉生成在數位人像研究領域中備受關注。現有方法受到複雜的模型架構所累，這些架構錯綜複雜地相互依賴，使得重新編輯影像或影片輸入的過程變得複雜。在這項工作中，我們提出 ControlTalk，一種基於受驅動音訊控制臉部表情變形的說話人臉生成方法，它能夠以統一的方式建構頭部姿勢和臉部表情，包括單一影像或序列影片輸入的唇部動作。透過使用預先訓練好的影片合成渲染器並提出輕量化適應，ControlTalk 能夠在精確且自然地進行唇部同步的同時，還能對嘴巴張開的形狀進行量化控制。我們的實驗顯示，我們的模型在廣泛使用的基準上優於最先進的效能，包括 HDTF 和 MEAD。參數化適應展示出非凡的泛化能力，能夠有效地處理同 ID 和跨 ID 場景中的表情變形，並將其效用擴展到領域外的肖像，而與語言無關。

##### **LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation**
2406.02876v1 by Zengkui Sun, Yijin Liu, Fandong Meng, Jinan Xu, Yufeng Chen, Jie Zhou

Multilingual neural machine translation models generally distinguish
translation directions by the language tag (LT) in front of the source or
target sentences. However, current LT strategies cannot indicate the desired
target language as expected on zero-shot translation, i.e., the off-target
issue. Our analysis reveals that the indication of the target language is
sensitive to the placement of the target LT. For example, when placing the
target LT on the decoder side, the indication would rapidly degrade along with
decoding steps, while placing the target LT on the encoder side would lead to
copying or paraphrasing the source input. To address the above issues, we
propose a simple yet effective strategy named Language Converter Strategy
(LCS). By introducing the target language embedding into the top encoder
layers, LCS mitigates confusion in the encoder and ensures stable language
indication for the decoder. Experimental results on MultiUN, TED, and OPUS-100
datasets demonstrate that LCS could significantly mitigate the off-target
issue, with language accuracy up to 95.28%, 96.21%, and 85.35% meanwhile
outperforming the vanilla LT strategy by 3.07, 3,3, and 7.93 BLEU scores on
zero-shot translation, respectively.

摘要：多語言神經機器翻譯模型通常會在原始語言或目標語言句子前面加上語言標籤 (LT) 來區分翻譯方向。然而，目前的 LT 策略無法如預期般在零次學習翻譯中指出所需的目標語言，也就是說，會出現離題問題。我們的分析顯示，目標語言的指示與目標 LT 的位置有關。例如，當將目標 LT 放置在解碼器側時，指示會隨著解碼步驟快速下降，而將目標 LT 放置在編碼器側則會導致複製或改寫原始輸入。為了解決上述問題，我們提出了一個簡單但有效的策略，稱為語言轉換策略 (LCS)。透過將目標語言嵌入引入頂層編碼器層，LCS 可減輕編碼器的混淆，並確保為解碼器提供穩定的語言指示。在 MultiUN、TED 和 OPUS-100 資料集上的實驗結果顯示，LCS 可以顯著減輕離題問題，語言準確度分別高達 95.28%、96.21% 和 85.35%，同時在零次學習翻譯中分別比傳統 LT 策略高出 3.07、3.3 和 7.93 的 BLEU 分數。

##### **Combinatorial Optimization with Automated Graph Neural Networks**
2406.02872v1 by Yang Liu, Peng Zhang, Yang Gao, Chuan Zhou, Zhao Li, Hongyang Chen

In recent years, graph neural networks (GNNs) have become increasingly
popular for solving NP-hard combinatorial optimization (CO) problems, such as
maximum cut and maximum independent set. The core idea behind these methods is
to represent a CO problem as a graph and then use GNNs to learn the node/graph
embedding with combinatorial information. Although these methods have achieved
promising results, given a specific CO problem, the design of GNN architectures
still requires heavy manual work with domain knowledge. Existing automated GNNs
are mostly focused on traditional graph learning problems, which is
inapplicable to solving NP-hard CO problems. To this end, we present a new
class of \textbf{AUTO}mated \textbf{G}NNs for solving \textbf{NP}-hard
problems, namely \textbf{AutoGNP}. We represent CO problems by GNNs and focus
on two specific problems, i.e., mixed integer linear programming and quadratic
unconstrained binary optimization. The idea of AutoGNP is to use graph neural
architecture search algorithms to automatically find the best GNNs for a given
NP-hard combinatorial optimization problem. Compared with existing graph neural
architecture search algorithms, AutoGNP utilizes two-hop operators in the
architecture search space. Moreover, AutoGNP utilizes simulated annealing and a
strict early stopping policy to avoid local optimal solutions. Empirical
results on benchmark combinatorial problems demonstrate the superiority of our
proposed model.

摘要：<paragraph>近年來，圖神經網路 (GNN) 對於解決 NP 難組合最佳化 (CO) 問題（如最大割和最大獨立集）變得越來越受歡迎。這些方法背後的主要概念是將 CO 問題表示為圖形，然後使用 GNN 學習具有組合資訊的節點/圖形嵌入。儘管這些方法已取得令人滿意的結果，但針對特定 CO 問題，GNN 架構的設計仍需要大量具備領域知識的手動工作。現有的自動化 GNN 主要集中於傳統圖形學習問題，這不適用於解決 NP 難 CO 問題。為此，我們提出了一類新的用於解決 NP 難問題的自動化 GNN，即 AutoGNP。我們使用 GNN 表示 CO 問題，並專注於兩個特定問題，即混合整數線性規劃和二次無約束二元最佳化。AutoGNP 的概念是使用圖形神經架構搜尋演算法自動為給定的 NP 難組合最佳化問題找出最佳 GNN。與現有的圖形神經架構搜尋演算法相比，AutoGNP 在架構搜尋空間中使用兩跳運算子。此外，AutoGNP 利用模擬退火和嚴格的提前停止策略來避免局部最佳解。基準組合問題的經驗結果證明了我們提出的模型的優越性。</paragraph>

##### **Sound Heuristic Search Value Iteration for Undiscounted POMDPs with Reachability Objectives**
2406.02871v1 by Qi Heng Ho, Martin S. Feather, Federico Rossi, Zachary N. Sunberg, Morteza Lahijanian

Partially Observable Markov Decision Processes (POMDPs) are powerful models
for sequential decision making under transition and observation uncertainties.
This paper studies the challenging yet important problem in POMDPs known as the
(indefinite-horizon) Maximal Reachability Probability Problem (MRPP), where the
goal is to maximize the probability of reaching some target states. This is
also a core problem in model checking with logical specifications and is
naturally undiscounted (discount factor is one). Inspired by the success of
point-based methods developed for discounted problems, we study their
extensions to MRPP. Specifically, we focus on trial-based heuristic search
value iteration techniques and present a novel algorithm that leverages the
strengths of these techniques for efficient exploration of the belief space
(informed search via value bounds) while addressing their drawbacks in handling
loops for indefinite-horizon problems. The algorithm produces policies with
two-sided bounds on optimal reachability probabilities. We prove convergence to
an optimal policy from below under certain conditions. Experimental evaluations
on a suite of benchmarks show that our algorithm outperforms existing methods
in almost all cases in both probability guarantees and computation time.

摘要：部分可觀察馬可夫決策過程 (POMDP) 是在轉移和觀察不確定性下進行順序決策的有力模型。本文研究了 POMDP 中具有挑戰性但重要的問題，稱為（無限時間軸）最大可達機率問題 (MRPP)，其目標是最大化達到某些目標狀態的機率。這也是具有邏輯規範的模型檢查中的核心問題，並且本質上沒有折扣（折扣因子為一）。受到為折扣問題開發的基於點的方法的成功啟發，我們研究了它們對 MRPP 的擴充。具體而言，我們專注於基於試驗的啟發式搜尋值迭代技術，並提出了一種新演算法，它利用了這些技術的優勢來有效探索信念空間（透過值邊界進行明智搜尋），同時解決了它們在處理無限時間軸問題的迴圈時的缺點。該演算法產生對最佳可達機率具有雙邊邊界的策略。我們證明了在特定條件下從下方收斂到最佳策略。對一系列基準的實驗評估表明，我們的演算法在幾乎所有情況下，在機率保證和運算時間方面都優於現有方法。

##### **Oscillations enhance time-series prediction in reservoir computing with feedback**
2406.02867v1 by Yuji Kawai, Takashi Morita, Jihoon Park, Minoru Asada

Reservoir computing, a machine learning framework used for modeling the
brain, can predict temporal data with little observations and minimal
computational resources. However, it is difficult to accurately reproduce the
long-term target time series because the reservoir system becomes unstable.
This predictive capability is required for a wide variety of time-series
processing, including predictions of motor timing and chaotic dynamical
systems. This study proposes oscillation-driven reservoir computing (ODRC) with
feedback, where oscillatory signals are fed into a reservoir network to
stabilize the network activity and induce complex reservoir dynamics. The ODRC
can reproduce long-term target time series more accurately than conventional
reservoir computing methods in a motor timing and chaotic time-series
prediction tasks. Furthermore, it generates a time series similar to the target
in the unexperienced period, that is, it can learn the abstract generative
rules from limited observations. Given these significant improvements made by
the simple and computationally inexpensive implementation, the ODRC would serve
as a practical model of various time series data. Moreover, we will discuss
biological implications of the ODRC, considering it as a model of neural
oscillations and their cerebellar processors.

摘要：儲存器運算是一種用於建模大腦的機器學習架構，它可以用少量的觀察和最少的運算資源預測時間資料。然而，由於儲存器系統會變得不穩定，因此難以準確地複製長期目標時間序列。這種預測能力對於各種時間序列處理是必需的，包括運動時序和混亂動力系統的預測。本研究提出帶有回饋的振盪驅動儲存器運算 (ODRC)，其中將振盪訊號輸入到儲存器網路中以穩定網路活動並誘導複雜的儲存器動態。ODRC 能比傳統的儲存器運算方法更準確地複製長期目標時間序列，用於運動時序和混亂時間序列預測任務。此外，它會在未經驗的時期產生類似於目標的時間序列，也就是說，它可以從有限的觀察中學習抽象的生成規則。由於簡單且運算成本低廉的實作帶來了這些顯著的改進，因此 ODRC 將可作為各種時間序列資料的實用模型。此外，我們將討論 ODRC 的生物學意義，並將其視為神經振盪及其小腦處理器的模型。

##### **NUMCoT: Numerals and Units of Measurement in Chain-of-Thought Reasoning using Large Language Models**
2406.02864v1 by Ancheng Xu, Minghuan Tan, Lei Wang, Min Yang, Ruifeng Xu

Numeral systems and units of measurement are two conjoined topics in
activities of human beings and have mutual effects with the languages
expressing them. Currently, the evaluation of Large Language Models (LLMs)
often involves mathematical reasoning, yet little attention is given to how
minor changes in numbers or units can drastically alter the complexity of
problems and the performance of LLMs. In this paper, we scrutinize existing
LLMs on processing of numerals and units of measurement by constructing
datasets with perturbations. We first anatomize the reasoning of math word
problems to different sub-procedures like numeral conversions from language to
numbers and measurement conversions based on units. Then we further annotate
math word problems from ancient Chinese arithmetic works which are challenging
in numerals and units of measurement. Experiments on perturbed datasets
demonstrate that LLMs still encounter difficulties in handling numeral and
measurement conversions.

摘要：數字系統和計量單位是人類活動中兩個相互結合的主題，並與表達它們的語言相互影響。目前，大型語言模型 (LLM) 的評估通常涉及數學推理，但對於數字或單位的微小變化如何大幅改變問題的複雜性和 LLM 的效能，卻鮮少受到關注。在本文中，我們透過建構具有擾動的資料集，仔細審查現有的 LLM 處理數字和計量單位的方式。我們首先剖析數學文字題的推理，區分為不同的子程序，例如從語言轉換為數字的數字轉換，以及基於單位的測量轉換。然後，我們進一步註解古代中國算術著作中的數學文字題，這些題目在數字和計量單位方面具有挑戰性。在擾動資料集上的實驗表明，LLM 在處理數字和測量轉換方面仍然遇到困難。

##### **LLM as a Scorer: The Impact of Output Order on Dialogue Evaluation**
2406.02863v1 by Yi-Pei Chen, KuanChao Chu, Hideki Nakayama

This research investigates the effect of prompt design on dialogue evaluation
using large language models (LLMs). While LLMs are increasingly used for
scoring various inputs, creating effective prompts for dialogue evaluation
remains challenging due to model sensitivity and subjectivity in dialogue
assessments. Our study experimented with different prompt structures, altering
the sequence of output instructions and including explanatory reasons. We found
that the order of presenting reasons and scores significantly influences LLMs'
scoring, with a "reason-first" approach yielding more comprehensive
evaluations. This insight is crucial for enhancing the accuracy and consistency
of LLM-based evaluations.

摘要：本研究探討提示設計對話評估的影響，並使用大型語言模型 (LLM)。儘管 LLM 愈來愈常被用於評分各項輸入，但由於模型敏感度和對話評估中的主觀性，為對話評估建立有效的提示仍是一項挑戰。我們的研究實驗了不同的提示結構，改變輸出指令的順序，並加入解釋性理由。我們發現呈現理由和分數的順序顯著影響 LLM 的評分，「先理由」的方法產生更全面的評估。此見解對於提升基於 LLM 的評估的準確性和一致性至關重要。

##### **Xmodel-LM Technical Report**
2406.02856v1 by Yichuan Wang, Yang Liu, Yu Yan, Xucheng Huang, Ling Jiang

We introduce Xmodel-LM, a compact and efficient 1.1B language model
pre-trained on over 2 trillion tokens. Trained on our self-built dataset
(Xdata), which balances Chinese and English corpora based on downstream task
optimization, Xmodel-LM exhibits remarkable performance despite its smaller
size. It notably surpasses existing open-source language models of similar
scale. Our model checkpoints and code are publicly accessible on GitHub at
https://github.com/XiaoduoAILab/XmodelLM.

摘要：我們推出 Xmodel-LM，一個精簡且高效的 1.1B 語言模型，已針對超過 2 兆個符號進行預訓練。在我們自建的資料集 (Xdata) 上進行訓練，該資料集根據下游任務最佳化平衡了中文和英文語料庫，儘管 Xmodel-LM 規模較小，但仍展現出卓越的效能。它顯著超越了規模相似的現有開源語言模型。我們的模型檢查點和程式碼已在 GitHub 上公開，網址為 https://github.com/XiaoduoAILab/XmodelLM。

##### **Item-Language Model for Conversational Recommendation**
2406.02844v1 by Li Yang, Anushya Subbiah, Hardik Patel, Judith Yue Li, Yanwei Song, Reza Mirghaderi, Vikram Aggarwal

Large-language Models (LLMs) have been extremely successful at tasks like
complex dialogue understanding, reasoning and coding due to their emergent
abilities. These emergent abilities have been extended with multi-modality to
include image, audio, and video capabilities. Recommender systems, on the other
hand, have been critical for information seeking and item discovery needs.
Recently, there have been attempts to apply LLMs for recommendations. One
difficulty of current attempts is that the underlying LLM is usually not
trained on the recommender system data, which largely contains user interaction
signals and is often not publicly available. Another difficulty is user
interaction signals often have a different pattern from natural language text,
and it is currently unclear if the LLM training setup can learn more
non-trivial knowledge from interaction signals compared with traditional
recommender system methods. Finally, it is difficult to train multiple LLMs for
different use-cases, and to retain the original language and reasoning
abilities when learning from recommender system data. To address these three
limitations, we propose an Item-Language Model (ILM), which is composed of an
item encoder to produce text-aligned item representations that encode user
interaction signals, and a frozen LLM that can understand those item
representations with preserved pretrained knowledge. We conduct extensive
experiments which demonstrate both the importance of the language-alignment and
of user interaction knowledge in the item encoder.

摘要：大型語言模型 (LLM) 在複雜對話理解、推理和編碼等任務上取得極佳的成功，這要歸功於它們新興的能力。這些新興的能力已擴展到多模態，包括影像、音訊和影片功能。另一方面，推薦系統對於資訊搜尋和商品探索需求至關重要。最近，已有嘗試將 LLM 應用於推薦。目前嘗試的一項難處在於，基礎 LLM 通常並非針對推薦系統資料進行訓練，而這些資料大多包含使用者互動訊號，且通常不會公開。另一項難處在於，使用者互動訊號通常與自然語言文字有不同的模式，目前尚不清楚 LLM 訓練設定是否能從互動訊號中學習到比傳統推薦系統方法更多的非平凡知識。最後，很難針對不同的使用案例訓練多個 LLM，並在從推薦系統資料中學習時保留原始語言和推理能力。為了解決這三個限制，我們提出了一種商品語言模型 (ILM)，它由一個商品編碼器組成，用於產生與文字對齊的商品表徵，以編碼使用者互動訊號，以及一個凍結的 LLM，它可以理解那些保留了預訓練知識的商品表徵。我們進行了廣泛的實驗，證明了商品編碼器中語言對齊和使用者互動知識的重要性。

##### **Too Big to Fail: Larger Language Models are Disproportionately Resilient to Induction of Dementia-Related Linguistic Anomalies**
2406.02830v1 by Changye Li, Zhecheng Sheng, Trevor Cohen, Serguei Pakhomov

As artificial neural networks grow in complexity, understanding their inner
workings becomes increasingly challenging, which is particularly important in
healthcare applications. The intrinsic evaluation metrics of autoregressive
neural language models (NLMs), perplexity (PPL), can reflect how "surprised" an
NLM model is at novel input. PPL has been widely used to understand the
behavior of NLMs. Previous findings show that changes in PPL when masking
attention layers in pre-trained transformer-based NLMs reflect linguistic
anomalies associated with Alzheimer's disease dementia. Building upon this, we
explore a novel bidirectional attention head ablation method that exhibits
properties attributed to the concepts of cognitive and brain reserve in human
brain studies, which postulate that people with more neurons in the brain and
more efficient processing are more resilient to neurodegeneration. Our results
show that larger GPT-2 models require a disproportionately larger share of
attention heads to be masked/ablated to display degradation of similar
magnitude to masking in smaller models. These results suggest that the
attention mechanism in transformer models may present an analogue to the
notions of cognitive and brain reserve and could potentially be used to model
certain aspects of the progression of neurodegenerative disorders and aging.

摘要：隨著人工神經網路的複雜性提升，理解其內部運作也變得越來越困難，這在醫療保健應用上特別重要。自迴歸神經語言模型 (NLM) 的內在評估指標，困惑度 (PPL)，可以反映 NLM 模型對新輸入感到有多「驚訝」。PPL 已被廣泛用於了解 NLM 的行為。先前的研究發現，在預先訓練的基於 Transformer 的 NLM 中遮蔽注意力層時，PPL 的變化反映了與阿茲海默症痴呆症相關的語言異常。在此基礎上，我們探索了一種新穎的雙向注意力頭部消融方法，該方法表現出歸因於人類大腦研究中認知和腦力儲備概念的特性，這些概念假設大腦中神經元越多、處理效率越高的人對神經退化更具抵抗力。我們的結果表明，較大的 GPT-2 模型需要不成比例地更大比例的注意力頭部被遮蔽/消融，才能表現出與較小模型中遮蔽相似的程度的退化。這些結果表明，Transformer 模型中的注意力機制可能呈現出與認知和腦力儲備概念類似的現象，並且有可能用於模擬神經退化性疾病和老化的某些方面進展。

##### **Stochastic Diffusion: A Diffusion Probabilistic Model for Stochastic Time Series Forecasting**
2406.02827v1 by Yuansan Liu, Sudanthi Wijewickrema, Dongting Hu, Christofer Bester, Stephen O'Leary, James Bailey

Recent innovations in diffusion probabilistic models have paved the way for
significant progress in image, text and audio generation, leading to their
applications in generative time series forecasting. However, leveraging such
abilities to model highly stochastic time series data remains a challenge. In
this paper, we propose a novel Stochastic Diffusion (StochDiff) model which
learns data-driven prior knowledge at each time step by utilizing the
representational power of the stochastic latent spaces to model the variability
of the multivariate time series data. The learnt prior knowledge helps the
model to capture complex temporal dynamics and the inherent uncertainty of the
data. This improves its ability to model highly stochastic time series data.
Through extensive experiments on real-world datasets, we demonstrate the
effectiveness of our proposed model on stochastic time series forecasting.
Additionally, we showcase an application of our model for real-world surgical
guidance, highlighting its potential to benefit the medical community.

摘要：最近在擴散機率模型的創新為影像、文字和音訊生成領域的顯著進展鋪路，並應用於生成式時序預測。然而，利用這些能力來建模高度隨機的時序資料仍然是一項挑戰。在本文中，我們提出一個新的隨機擴散 (StochDiff) 模型，它利用隨機潛在空間的表示能力在每個時間步長學習資料驅動的先驗知識，以建模多變量時序資料的變異性。學習到的先驗知識有助於模型捕捉複雜的時間動態和資料的內在不確定性。這改善了它建模高度隨機時序資料的能力。透過對真實世界資料集的廣泛實驗，我們展示了我們提出的模型在隨機時序預測上的有效性。此外，我們展示了我們模型在真實世界手術指導中的應用，突顯了它對醫學界潛在的益處。

##### **Exploring Robustness in Doctor-Patient Conversation Summarization: An Analysis of Out-of-Domain SOAP Notes**
2406.02826v1 by Yu-Wen Chen, Julia Hirschberg

Summarizing medical conversations poses unique challenges due to the
specialized domain and the difficulty of collecting in-domain training data. In
this study, we investigate the performance of state-of-the-art doctor-patient
conversation generative summarization models on the out-of-domain data. We
divide the summarization model of doctor-patient conversation into two
configurations: (1) a general model, without specifying subjective (S),
objective (O), and assessment (A) and plan (P) notes; (2) a SOAP-oriented model
that generates a summary with SOAP sections. We analyzed the limitations and
strengths of the fine-tuning language model-based methods and GPTs on both
configurations. We also conducted a Linguistic Inquiry and Word Count analysis
to compare the SOAP notes from different datasets. The results exhibit a strong
correlation for reference notes across different datasets, indicating that
format mismatch (i.e., discrepancies in word distribution) is not the main
cause of performance decline on out-of-domain data. Lastly, a detailed analysis
of SOAP notes is included to provide insights into missing information and
hallucinations introduced by the models.

摘要：摘要醫療對話會因為專業領域和收集領域內訓練資料的困難性而產生獨特的挑戰。在本研究中，我們探討了最先進的醫師-病患對話生成摘要模型在領域外資料的表現。我們將醫師-病患對話的摘要模型分為兩種組態：(1) 一般模型，不指定主觀 (S)、客觀 (O) 和評估 (A) 和計畫 (P) 筆記；(2) 以 SOAP 為導向的模型，生成帶有 SOAP 部分的摘要。我們分析了兩種組態中微調語言模型和 GPT 的限制和優點。我們還進行了語言探討和字數分析，以比較不同資料集的 SOAP 筆記。結果顯示，不同資料集的參考筆記有很強的相關性，這表示格式不匹配（例如，字詞分佈的差異）並非領域外資料效能下降的主要原因。最後，我們納入了 SOAP 筆記的詳細分析，以提供對模型引入的遺漏資訊和幻覺的見解。

