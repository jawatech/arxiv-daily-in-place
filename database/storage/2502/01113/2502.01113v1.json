{"2502.01113": {"publish_time": "2025-02-03", "title": "GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation", "paper_summary": "Retrieval-augmented generation (RAG) has proven effective in integrating\nknowledge into large language models (LLMs). However, conventional RAGs\nstruggle to capture complex relationships between pieces of knowledge, limiting\ntheir performance in intricate reasoning that requires integrating knowledge\nfrom multiple sources. Recently, graph-enhanced retrieval augmented generation\n(GraphRAG) builds graph structure to explicitly model these relationships,\nenabling more effective and efficient retrievers. Nevertheless, its performance\nis still hindered by the noise and incompleteness within the graph structure.\nTo address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for\nretrieval augmented generation. GFM-RAG is powered by an innovative graph\nneural network that reasons over graph structure to capture complex\nquery-knowledge relationships. The GFM with 8M parameters undergoes a two-stage\ntraining process on large-scale datasets, comprising 60 knowledge graphs with\nover 14M triples and 700k documents. This results in impressive performance and\ngeneralizability for GFM-RAG, making it the first graph foundation model\napplicable to unseen datasets for retrieval without any fine-tuning required.\nExtensive experiments on three multi-hop QA datasets and seven domain-specific\nRAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance\nwhile maintaining efficiency and alignment with neural scaling laws,\nhighlighting its potential for further improvement.", "paper_summary_zh": "\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u5df2\u8b49\u660e\u5728\u6574\u5408\u77e5\u8b58\u5230\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u6709\u6548\u3002\u7136\u800c\uff0c\u50b3\u7d71\u7684 RAG \u96e3\u4ee5\u6355\u6349\u77e5\u8b58\u7247\u6bb5\u4e4b\u9593\u7684\u8907\u96dc\u95dc\u4fc2\uff0c\u9650\u5236\u4e86\u5b83\u5011\u5728\u9700\u8981\u6574\u5408\u4f86\u81ea\u591a\u500b\u4f86\u6e90\u7684\u77e5\u8b58\u7684\u8907\u96dc\u63a8\u7406\u4e2d\u7684\u8868\u73fe\u3002\u6700\u8fd1\uff0c\u5716\u8868\u589e\u5f37\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (GraphRAG) \u5efa\u7acb\u5716\u8868\u7d50\u69cb\u4f86\u660e\u78ba\u5efa\u6a21\u9019\u4e9b\u95dc\u4fc2\uff0c\u5f9e\u800c\u5be6\u73fe\u66f4\u6709\u6548\u7387\u7684\u6aa2\u7d22\u5668\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u5176\u6548\u80fd\u4ecd\u53d7\u5230\u5716\u8868\u7d50\u69cb\u4e2d\u96dc\u8a0a\u548c\u4e0d\u5b8c\u6574\u6027\u7684\u963b\u7919\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 GFM-RAG\uff0c\u4e00\u7a2e\u7528\u65bc\u6aa2\u7d22\u589e\u5f37\u751f\u6210\u7684\u5168\u65b0\u5716\u8868\u57fa\u790e\u6a21\u578b (GFM)\u3002GFM-RAG \u7531\u4e00\u500b\u5275\u65b0\u7684\u5716\u795e\u7d93\u7db2\u8def\u9a45\u52d5\uff0c\u8a72\u7db2\u8def\u5728\u5716\u8868\u7d50\u69cb\u4e0a\u9032\u884c\u63a8\u7406\u4ee5\u6355\u6349\u8907\u96dc\u7684\u67e5\u8a62\u77e5\u8b58\u95dc\u4fc2\u3002\u5177\u6709 8M \u53c3\u6578\u7684 GFM \u5728\u5927\u578b\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5169\u968e\u6bb5\u8a13\u7df4\u6d41\u7a0b\uff0c\u5305\u62ec 60 \u500b\u5305\u542b\u8d85\u904e 14M \u500b\u4e09\u5143\u7d44\u548c 700k \u500b\u6587\u4ef6\u7684\u6587\u4ef6\u3002\u9019\u70ba GFM-RAG \u5e36\u4f86\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\u548c\u901a\u7528\u6027\uff0c\u4f7f\u5176\u6210\u70ba\u7b2c\u4e00\u500b\u9069\u7528\u65bc\u672a\u898b\u904e\u8cc7\u6599\u96c6\u7684\u5716\u8868\u57fa\u790e\u6a21\u578b\uff0c\u800c\u7121\u9700\u4efb\u4f55\u5fae\u8abf\u3002\u5728\u4e09\u500b\u591a\u8df3\u554f\u7b54\u8cc7\u6599\u96c6\u548c\u4e03\u500b\u7279\u5b9a\u9818\u57df RAG \u8cc7\u6599\u96c6\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cGFM-RAG \u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u540c\u6642\u4fdd\u6301\u4e86\u6548\u7387\u4e26\u8207\u795e\u7d93\u64f4\u5145\u5b9a\u5f8b\u4fdd\u6301\u4e00\u81f4\uff0c\u7a81\u986f\u4e86\u5176\u9032\u4e00\u6b65\u6539\u9032\u7684\u6f5b\u529b\u3002", "author": "Linhao Luo et.al.", "authors": "Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Dinh Phung, Chen Gong, Shirui Pan", "id": "2502.01113v1", "paper_url": "http://arxiv.org/abs/2502.01113v1", "repo": "null"}}