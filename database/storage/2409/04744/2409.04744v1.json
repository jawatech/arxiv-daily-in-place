{"2409.04744": {"publish_time": "2024-09-07", "title": "LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs", "paper_summary": "The uncertainty inherent in the environmental transition model of\nReinforcement Learning (RL) necessitates a careful balance between exploration\nand exploitation to optimize the use of computational resources for accurately\nestimating an agent's expected reward. Achieving balance in control systems is\nparticularly challenging in scenarios with sparse rewards. However, given the\nextensive prior knowledge available for many environments, it is redundant to\nbegin learning from scratch in such settings. To address this, we introduce\n\\textbf{L}anguage \\textbf{M}odel \\textbf{G}uided \\textbf{T}rade-offs (i.e.,\n\\textbf{LMGT}), a novel, sample-efficient framework that leverages the\ncomprehensive prior knowledge embedded in Large Language Models (LLMs) and\ntheir adeptness at processing non-standard data forms, such as wiki tutorials.\nLMGT proficiently manages the exploration-exploitation trade-off by employing\nreward shifts guided by LLMs, which direct agents' exploration endeavors,\nthereby improving sample efficiency. We have thoroughly tested LMGT across\nvarious RL tasks and deployed it in industrial-grade RL recommendation systems,\nwhere it consistently outperforms baseline methods. The results indicate that\nour framework can significantly reduce the time cost required during the\ntraining phase in RL.", "paper_summary_zh": "\u5728\u5f37\u5316\u5b78\u7fd2\uff08RL\uff09\u7684\u74b0\u5883\u8f49\u63db\u6a21\u578b\u4e2d\uff0c\u56fa\u6709\u7684\u4e0d\u78ba\u5b9a\u6027\u9700\u8981\u5728\u63a2\u7d22\u548c\u5229\u7528\u4e4b\u9593\u53d6\u5f97\u4ed4\u7d30\u7684\u5e73\u8861\uff0c\u4ee5\u6700\u4f73\u5316\u8a08\u7b97\u8cc7\u6e90\u7684\u4f7f\u7528\uff0c\u4ee5\u7cbe\u6e96\u4f30\u8a08\u4ee3\u7406\u9810\u671f\u7684\u734e\u52f5\u3002\u5728\u63a7\u5236\u7cfb\u7d71\u4e2d\u53d6\u5f97\u5e73\u8861\u5728\u734e\u52f5\u7a00\u758f\u7684\u60c5\u6cc1\u4e0b\u7279\u5225\u5177\u6709\u6311\u6230\u6027\u3002\u7136\u800c\uff0c\u7531\u65bc\u8a31\u591a\u74b0\u5883\u90fd\u6709\u5ee3\u6cdb\u7684\u5148\u9a57\u77e5\u8b58\uff0c\u56e0\u6b64\u5728\u9019\u7a2e\u8a2d\u5b9a\u4e2d\u5f9e\u982d\u958b\u59cb\u5b78\u7fd2\u662f\u591a\u9918\u7684\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86**L**anguage **M**odel **G**uided **T**rade-offs\uff08\u5373**LMGT**\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u4e14\u6a23\u672c\u6548\u7387\u9ad8\u7684\u67b6\u69cb\uff0c\u5b83\u5229\u7528\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u5d4c\u5165\u7684\u5168\u9762\u5148\u9a57\u77e5\u8b58\uff0c\u4ee5\u53ca\u5b83\u5011\u8655\u7406\u975e\u6a19\u6e96\u6578\u64da\u5f62\u5f0f\uff08\u4f8b\u5982 wiki \u6559\u7a0b\uff09\u7684\u9748\u6d3b\u6027\u3002LMGT \u900f\u904e\u63a1\u7528\u7531 LLM \u5f15\u5c0e\u7684\u734e\u52f5\u8f49\u79fb\u4f86\u719f\u7df4\u5730\u7ba1\u7406\u63a2\u7d22-\u5229\u7528\u6b0a\u8861\uff0c\u6307\u5c0e\u4ee3\u7406\u7684\u63a2\u7d22\u5de5\u4f5c\uff0c\u5f9e\u800c\u63d0\u9ad8\u6a23\u672c\u6548\u7387\u3002\u6211\u5011\u5df2\u7d93\u5fb9\u5e95\u6e2c\u8a66\u4e86 LMGT \u5728\u5404\u7a2e RL \u4efb\u52d9\u4e2d\u7684\u8868\u73fe\uff0c\u4e26\u5c07\u5176\u90e8\u7f72\u5728\u5de5\u696d\u7d1a RL \u63a8\u85a6\u7cfb\u7d71\u4e2d\uff0c\u5728\u9019\u4e9b\u7cfb\u7d71\u4e2d\uff0c\u5b83\u59cb\u7d42\u512a\u65bc\u57fa\u7dda\u65b9\u6cd5\u3002\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u67b6\u69cb\u53ef\u4ee5\u986f\u8457\u6e1b\u5c11 RL \u8a13\u7df4\u968e\u6bb5\u6240\u9700\u7684\u6642\u9593\u6210\u672c\u3002", "author": "Yongxin Deng et.al.", "authors": "Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Wei Chu, Yinghui Xu", "id": "2409.04744v1", "paper_url": "http://arxiv.org/abs/2409.04744v1", "repo": "null"}}