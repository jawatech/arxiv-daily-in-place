{"2405.03248": {"publish_time": "2024-05-06", "title": "Communication-Efficient Federated Learning with Adaptive Compression under Dynamic Bandwidth", "paper_summary": "Federated learning can train models without directly providing local data to\nthe server. However, the frequent updating of the local model brings the\nproblem of large communication overhead. Recently, scholars have achieved the\ncommunication efficiency of federated learning mainly by model compression. But\nthey ignore two problems: 1) network state of each client changes dynamically;\n2) network state among clients is not the same. The clients with poor bandwidth\nupdate local model slowly, which leads to low efficiency. To address this\nchallenge, we propose a communication-efficient federated learning algorithm\nwith adaptive compression under dynamic bandwidth (called AdapComFL).\nConcretely, each client performs bandwidth awareness and bandwidth prediction.\nThen, each client adaptively compresses its local model via the improved sketch\nmechanism based on his predicted bandwidth. Further, the server aggregates\nsketched models with different sizes received. To verify the effectiveness of\nthe proposed method, the experiments are based on real bandwidth data which are\ncollected from the network topology we build, and benchmark datasets which are\nobtained from open repositories. We show the performance of AdapComFL\nalgorithm, and compare it with existing algorithms. The experimental results\nshow that our AdapComFL achieves more efficient communication as well as\ncompetitive accuracy compared to existing algorithms.", "paper_summary_zh": "", "author": "Ying Zhuansun et.al.", "authors": "Ying Zhuansun,Dandan Li,Xiaohong Huang,Caijun Sun", "id": "2405.03248v1", "paper_url": "http://arxiv.org/abs/2405.03248v1", "repo": "null"}}