{"2411.09921": {"publish_time": "2024-11-15", "title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level", "paper_summary": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motion\nunderstanding task that requires generating visual answers (video segmentation\nmasks) according to the input question, and hence needs implicit spatiotemporal\nreasoning and grounding. This task extends existing spatiotemporal grounding\nwork focusing on explicit action/motion grounding, to a more general format by\nenabling implicit reasoning via questions. To facilitate the development of the\nnew task, we collect a large-scale dataset called GROUNDMORE, which comprises\n1,715 video clips, 249K object masks that are deliberately designed with 4\nquestion types (Causal, Sequential, Counterfactual, and Descriptive) for\nbenchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE\nuniquely requires models to generate visual answers, providing a more concrete\nand visually interpretable response than plain texts. It evaluates models on\nboth spatiotemporal grounding and reasoning, fostering to address complex\nchallenges in motion-related video reasoning, temporal perception, and\npixel-level understanding. Furthermore, we introduce a novel baseline model\nnamed Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the\nmultimodal reasoning ability from the Multimodal LLM, the pixel-level\nperception capability from the grounding model (SAM), and the temporal\nperception ability from a lightweight localization head. MORA achieves\nrespectable performance on GROUNDMORE outperforming the best existing visual\ngrounding baseline model by an average of 21.5% relatively. We hope this novel\nand challenging task will pave the way for future advancements in robust and\ngeneral motion understanding via video reasoning segmentation", "paper_summary_zh": "<paragraph>\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u904b\u52d5\u57fa\u790e\u5f71\u7247\u63a8\u7406\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u904b\u52d5\u7406\u89e3\u4efb\u52d9\uff0c\u9700\u8981\u6839\u64da\u8f38\u5165\u554f\u984c\u751f\u6210\u8996\u89ba\u7b54\u6848\uff08\u5f71\u7247\u5206\u5272\u906e\u7f69\uff09\uff0c\u56e0\u6b64\u9700\u8981\u96b1\u542b\u7684\u6642\u7a7a\u63a8\u7406\u548c\u57fa\u790e\u3002\u6b64\u4efb\u52d9\u64f4\u5c55\u4e86\u73fe\u6709\u7684\u6642\u7a7a\u57fa\u790e\u5de5\u4f5c\uff0c\u5c08\u6ce8\u65bc\u660e\u78ba\u7684\u52d5\u4f5c/\u904b\u52d5\u57fa\u790e\uff0c\u901a\u904e\u554f\u984c\u555f\u7528\u96b1\u542b\u63a8\u7406\uff0c\u8f49\u8b8a\u70ba\u66f4\u901a\u7528\u7684\u683c\u5f0f\u3002\u70ba\u4e86\u4fc3\u9032\u65b0\u4efb\u52d9\u7684\u767c\u5c55\uff0c\u6211\u5011\u6536\u96c6\u4e86\u4e00\u500b\u540d\u70ba GROUNDMORE \u7684\u5927\u578b\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b 1,715 \u500b\u5f71\u7247\u526a\u8f2f\u3001249K \u500b\u7269\u4ef6\u906e\u7f69\uff0c\u9019\u4e9b\u906e\u7f69\u7d93\u904e\u7cbe\u5fc3\u8a2d\u8a08\uff0c\u6709 4 \u7a2e\u985e\u578b\u7684\u554f\u984c\uff08\u56e0\u679c\u3001\u9806\u5e8f\u3001\u53cd\u4e8b\u5be6\u548c\u63cf\u8ff0\u6027\uff09\uff0c\u7528\u65bc\u8a55\u91cf\u6df1\u5165\u4e14\u5168\u9762\u7684\u904b\u52d5\u63a8\u7406\u80fd\u529b\u3002GROUNDMORE \u7368\u7279\u5730\u8981\u6c42\u6a21\u578b\u751f\u6210\u8996\u89ba\u7b54\u6848\uff0c\u63d0\u4f9b\u6bd4\u7d14\u6587\u5b57\u66f4\u5177\u9ad4\u4e14\u8996\u89ba\u4e0a\u53ef\u89e3\u91cb\u7684\u56de\u61c9\u3002\u5b83\u5728\u6642\u7a7a\u57fa\u790e\u548c\u63a8\u7406\u4e0a\u8a55\u4f30\u6a21\u578b\uff0c\u4fc3\u9032\u4e86\u89e3\u8207\u904b\u52d5\u76f8\u95dc\u7684\u5f71\u7247\u63a8\u7406\u3001\u6642\u9593\u611f\u77e5\u548c\u50cf\u7d20\u5c64\u7d1a\u7406\u89e3\u4e2d\u7684\u8907\u96dc\u6311\u6230\u3002\u6b64\u5916\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u540d\u70ba\u904b\u52d5\u57fa\u790e\u5f71\u7247\u63a8\u7406\u52a9\u7406\uff08MORA\uff09\u7684\u65b0\u7a4e\u57fa\u7dda\u6a21\u578b\u3002MORA \u7d50\u5408\u4e86\u591a\u6a21\u614b LLM \u7684\u591a\u6a21\u614b\u63a8\u7406\u80fd\u529b\u3001\u57fa\u790e\u6a21\u578b\uff08SAM\uff09\u7684\u50cf\u7d20\u5c64\u7d1a\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u53ca\u8f15\u91cf\u7d1a\u5b9a\u4f4d\u982d\u7684\u6642\u9593\u611f\u77e5\u80fd\u529b\u3002MORA \u5728 GROUNDMORE \u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u5c0a\u656c\u7684\u6548\u80fd\uff0c\u5e73\u5747\u512a\u65bc\u73fe\u6709\u6700\u4f73\u8996\u89ba\u57fa\u790e\u57fa\u7dda\u6a21\u578b 21.5%\u3002\u6211\u5011\u5e0c\u671b\u9019\u500b\u65b0\u7a4e\u4e14\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\u5c07\u70ba\u900f\u904e\u5f71\u7247\u63a8\u7406\u5206\u5272\uff0c\u5728\u7a69\u5065\u4e14\u901a\u7528\u7684\u904b\u52d5\u7406\u89e3\u65b9\u9762\u672a\u4f86\u9032\u5c55\u92ea\u8def\u3002</paragraph>", "author": "Andong Deng et.al.", "authors": "Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed Mian, Mohit Bansal, Chen Chen", "id": "2411.09921v1", "paper_url": "http://arxiv.org/abs/2411.09921v1", "repo": "null"}}