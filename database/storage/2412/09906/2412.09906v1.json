{"2412.09906": {"publish_time": "2024-12-13", "title": "Enhancing the Reasoning Capabilities of Small Language Models via Solution Guidance Fine-Tuning", "paper_summary": "Large language models (LLMs) have demonstrated remarkable performance across\na wide range of tasks. Advances in prompt engineering and fine-tuning\ntechniques have further enhanced their ability to address complex reasoning\nchallenges. However, these advanced capabilities are often exclusive to models\nexceeding 100 billion parameters. Although Chain-of-Thought (CoT) fine-tuning\nmethods have been explored for smaller models (under 10 billion parameters),\nthey typically depend on extensive CoT training data, which can introduce\ninconsistencies and limit effectiveness in low-data settings. To overcome these\nlimitations, this paper introduce a new reasoning strategy Solution Guidance\n(SG) and a plug-and-play training paradigm Solution-Guidance Fine-Tuning (SGFT)\nfor enhancing the reasoning capabilities of small language models. SG focuses\non problem understanding and decomposition at the semantic and logical levels,\nrather than specific computations, which can effectively improve the SLMs'\ngeneralization and reasoning abilities. With only a small amount of SG training\ndata, SGFT can fine-tune a SLM to produce accurate problem-solving guidances,\nwhich can then be flexibly fed to any SLM as prompts, enabling it to generate\ncorrect answers directly. Experimental results demonstrate that our method\nsignificantly improves the performance of SLMs on various reasoning tasks,\nenhancing both their practicality and efficiency within resource-constrained\nenvironments.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5ee3\u6cdb\u7684\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6548\u80fd\u3002\u63d0\u793a\u5de5\u7a0b\u548c\u5fae\u8abf\u6280\u8853\u7684\u9032\u5c55\u9032\u4e00\u6b65\u63d0\u5347\u4e86\u5b83\u5011\u8655\u7406\u8907\u96dc\u63a8\u7406\u6311\u6230\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u9032\u968e\u529f\u80fd\u901a\u5e38\u5c08\u5c6c\u65bc\u8d85\u904e 1000 \u5104\u500b\u53c3\u6578\u7684\u6a21\u578b\u3002\u5118\u7ba1\u5df2\u91dd\u5c0d\u8f03\u5c0f\u6a21\u578b\uff08\u5c0f\u65bc 100 \u5104\u500b\u53c3\u6578\uff09\u63a2\u7d22\u601d\u7dad\u93c8 (CoT) \u5fae\u8abf\u65b9\u6cd5\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u4f9d\u8cf4\u65bc\u5927\u91cf\u7684 CoT \u8a13\u7df4\u8cc7\u6599\uff0c\u9019\u53ef\u80fd\u6703\u5728\u8cc7\u6599\u91cf\u5c11\u7684\u60c5\u6cc1\u4e0b\u9020\u6210\u4e0d\u4e00\u81f4\u6027\u4e26\u9650\u5236\u6548\u80fd\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7684\u63a8\u7406\u7b56\u7565\u300c\u89e3\u6c7a\u65b9\u6848\u6307\u5c0e\u300d(SG) \u548c\u4e00\u7a2e\u5373\u63d2\u5373\u7528\u8a13\u7df4\u7bc4\u4f8b\u300c\u89e3\u6c7a\u65b9\u6848\u6307\u5c0e\u5fae\u8abf\u300d(SGFT)\uff0c\u7528\u65bc\u589e\u5f37\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002SG \u5c08\u6ce8\u65bc\u8a9e\u610f\u548c\u908f\u8f2f\u5c64\u9762\u7684\u554f\u984c\u7406\u89e3\u548c\u5206\u89e3\uff0c\u800c\u975e\u7279\u5b9a\u904b\u7b97\uff0c\u9019\u53ef\u4ee5\u6709\u6548\u63d0\u5347 SLM \u7684\u6982\u5316\u548c\u63a8\u7406\u80fd\u529b\u3002SGFT \u50c5\u9700\u5c11\u91cf\u7684 SG \u8a13\u7df4\u8cc7\u6599\uff0c\u5c31\u80fd\u5fae\u8abf SLM \u4ee5\u7522\u751f\u6e96\u78ba\u7684\u89e3\u6c7a\u554f\u984c\u6307\u5c0e\uff0c\u7136\u5f8c\u53ef\u4ee5\u9748\u6d3b\u5730\u5c07\u5176\u63d0\u4f9b\u7d66\u4efb\u4f55 SLM \u4f5c\u70ba\u63d0\u793a\uff0c\u4f7f\u5176\u80fd\u5920\u76f4\u63a5\u7522\u751f\u6b63\u78ba\u7684\u7b54\u6848\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u986f\u8457\u63d0\u5347\u4e86 SLM \u5728\u5404\u7a2e\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u6548\u80fd\uff0c\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u63d0\u5347\u4e86\u5176\u5be6\u7528\u6027\u548c\u6548\u7387\u3002", "author": "Jing Bi et.al.", "authors": "Jing Bi, Yuting Wu, Weiwei Xing, Zhenjie Wei", "id": "2412.09906v1", "paper_url": "http://arxiv.org/abs/2412.09906v1", "repo": "https://github.com/bijings/sgft"}}