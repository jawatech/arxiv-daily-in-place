{"2412.19684": {"publish_time": "2024-12-27", "title": "Boosting Private Domain Understanding of Efficient MLLMs: A Tuning-free, Adaptive, Universal Prompt Optimization Framework", "paper_summary": "Efficient multimodal large language models (EMLLMs), in contrast to\nmultimodal large language models (MLLMs), reduce model size and computational\ncosts and are often deployed on resource-constrained devices. However, due to\ndata privacy concerns, existing open-source EMLLMs rarely have access to\nprivate domain-specific data during the pre-training process, making them\ndifficult to directly apply in device-specific domains, such as certain\nbusiness scenarios. To address this weakness, this paper focuses on the\nefficient adaptation of EMLLMs to private domains, specifically in two areas:\n1) how to reduce data requirements, and 2) how to avoid parameter fine-tuning.\nSpecifically, we propose a tun\\textbf{\\underline{I}}ng-free,\na\\textbf{\\underline{D}}aptiv\\textbf{\\underline{E}},\nunivers\\textbf{\\underline{AL}} \\textbf{\\underline{Prompt}} Optimization\nFramework, abbreviated as \\textit{\\textbf{\\ourmethod{}}} which consists of two\nstages: 1) Predefined Prompt, based on the reinforcement searching strategy,\ngenerate a prompt optimization strategy tree to acquire optimization priors; 2)\nPrompt Reflection initializes the prompt based on optimization priors, followed\nby self-reflection to further search and refine the prompt. By doing so,\n\\ourmethod{} elegantly generates the ``ideal prompts'' for processing private\ndomain-specific data. Note that our method requires no parameter fine-tuning\nand only a small amount of data to quickly adapt to the data distribution of\nprivate data. Extensive experiments across multiple tasks demonstrate that our\nproposed \\ourmethod{} significantly improves both efficiency and performance\ncompared to baselines.", "paper_summary_zh": "\u8207\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u76f8\u6bd4\uff0c\u9ad8\u6548\u7684\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (EMLLM) \u53ef\u7e2e\u5c0f\u6a21\u578b\u898f\u6a21\u548c\u904b\u7b97\u6210\u672c\uff0c\u800c\u4e14\u901a\u5e38\u90e8\u7f72\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u88dd\u7f6e\u4e0a\u3002\u7136\u800c\uff0c\u7531\u65bc\u8cc7\u6599\u96b1\u79c1\u554f\u984c\uff0c\u73fe\u6709\u7684\u958b\u653e\u539f\u59cb\u78bc EMLLM \u5728\u9810\u8a13\u7df4\u904e\u7a0b\u4e2d\u5f88\u5c11\u80fd\u5b58\u53d6\u79c1\u4eba\u9818\u57df\u7279\u5b9a\u8cc7\u6599\uff0c\u9019\u4f7f\u5f97\u5b83\u5011\u96e3\u4ee5\u76f4\u63a5\u61c9\u7528\u5728\u7279\u5b9a\u88dd\u7f6e\u9818\u57df\uff0c\u4f8b\u5982\u67d0\u4e9b\u5546\u696d\u5834\u666f\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u7f3a\u9ede\uff0c\u672c\u6587\u5c08\u6ce8\u65bc EMLLM \u5c0d\u79c1\u4eba\u9818\u57df\u7684\u6709\u6548\u9069\u61c9\uff0c\u7279\u5225\u662f\u5728\u5169\u500b\u65b9\u9762\uff1a1) \u5982\u4f55\u6e1b\u5c11\u8cc7\u6599\u9700\u6c42\uff0c\u4ee5\u53ca 2) \u5982\u4f55\u907f\u514d\u53c3\u6578\u5fae\u8abf\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7121\u9700\u8abf\u6821\u3001\u53ef\u9069\u61c9\u3001\u901a\u7528\u7684\u63d0\u793a\u6700\u4f73\u5316\u67b6\u69cb\uff0c\u7c21\u7a31\u70ba \\textit{\\textbf{\\ourmethod{}}}\uff0c\u5b83\u5305\u542b\u5169\u500b\u968e\u6bb5\uff1a1) \u9810\u5b9a\u7fa9\u63d0\u793a\uff0c\u57fa\u65bc\u5f37\u5316\u641c\u5c0b\u7b56\u7565\uff0c\u7522\u751f\u63d0\u793a\u6700\u4f73\u5316\u7b56\u7565\u6a39\u4ee5\u53d6\u5f97\u6700\u4f73\u5316\u5148\u9a57\uff1b2) \u63d0\u793a\u53cd\u6620\u6839\u64da\u6700\u4f73\u5316\u5148\u9a57\u521d\u59cb\u5316\u63d0\u793a\uff0c\u7136\u5f8c\u9032\u884c\u81ea\u6211\u53cd\u6620\u4ee5\u9032\u4e00\u6b65\u641c\u5c0b\u548c\u6539\u5584\u63d0\u793a\u3002\u85c9\u7531\u9019\u9ebc\u505a\uff0c\\ourmethod{} \u512a\u96c5\u5730\u7522\u751f\u7528\u65bc\u8655\u7406\u79c1\u4eba\u9818\u57df\u7279\u5b9a\u8cc7\u6599\u7684\u300c\u7406\u60f3\u63d0\u793a\u300d\u3002\u8acb\u6ce8\u610f\uff0c\u6211\u5011\u7684\u6a21\u578b\u4e0d\u9700\u8981\u53c3\u6578\u5fae\u8abf\uff0c\u800c\u4e14\u53ea\u9700\u8981\u5c11\u91cf\u8cc7\u6599\u5c31\u80fd\u5feb\u901f\u9069\u61c9\u79c1\u4eba\u8cc7\u6599\u7684\u8cc7\u6599\u5206\u4f48\u3002\u8de8\u591a\u9805\u4efb\u52d9\u7684\u5927\u91cf\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u63d0\u51fa\u7684 \\ourmethod{} \u8207\u57fa\u6e96\u76f8\u6bd4\uff0c\u986f\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6548\u80fd\u3002", "author": "Jiang Liu et.al.", "authors": "Jiang Liu, Bolin Li, Haoyuan Li, Tianwei Lin, Wenqiao Zhang, Tao Zhong, Zhelun Yu, Jinghao Wei, Hao Cheng, Hao Jiang, Zheqi Lv, Juncheng Li, Siliang Tang, Yueting Zhuang", "id": "2412.19684v1", "paper_url": "http://arxiv.org/abs/2412.19684v1", "repo": "null"}}