{"2408.12981": {"publish_time": "2024-08-23", "title": "QD-VMR: Query Debiasing with Contextual Understanding Enhancement for Video Moment Retrieval", "paper_summary": "Video Moment Retrieval (VMR) aims to retrieve relevant moments of an\nuntrimmed video corresponding to the query. While cross-modal interaction\napproaches have shown progress in filtering out query-irrelevant information in\nvideos, they assume the precise alignment between the query semantics and the\ncorresponding video moments, potentially overlooking the misunderstanding of\nthe natural language semantics. To address this challenge, we propose a novel\nmodel called \\textit{QD-VMR}, a query debiasing model with enhanced contextual\nunderstanding. Firstly, we leverage a Global Partial Aligner module via video\nclip and query features alignment and video-query contrastive learning to\nenhance the cross-modal understanding capabilities of the model. Subsequently,\nwe employ a Query Debiasing Module to obtain debiased query features\nefficiently, and a Visual Enhancement module to refine the video features\nrelated to the query. Finally, we adopt the DETR structure to predict the\npossible target video moments. Through extensive evaluations of three benchmark\ndatasets, QD-VMR achieves state-of-the-art performance, proving its potential\nto improve the accuracy of VMR. Further analytical experiments demonstrate the\neffectiveness of our proposed module. Our code will be released to facilitate\nfuture research.", "paper_summary_zh": "\u5f71\u7247\u6642\u523b\u6aa2\u7d22 (VMR) \u7684\u76ee\u7684\u662f\u6aa2\u7d22\u8207\u67e5\u8a62\u76f8\u61c9\u7684\u672a\u4fee\u526a\u5f71\u7247\u7684\u76f8\u95dc\u6642\u523b\u3002\u96d6\u7136\u8de8\u6a21\u614b\u4e92\u52d5\u65b9\u6cd5\u5df2\u986f\u793a\u51fa\u5728\u5f71\u7247\u4e2d\u904e\u6ffe\u8207\u67e5\u8a62\u7121\u95dc\u8cc7\u8a0a\u7684\u9032\u5c55\uff0c\u4f46\u5b83\u5011\u5047\u8a2d\u67e5\u8a62\u8a9e\u610f\u8207\u5c0d\u61c9\u7684\u5f71\u7247\u6642\u523b\u4e4b\u9593\u7684\u7cbe\u78ba\u5c0d\u9f4a\uff0c\u53ef\u80fd\u6703\u5ffd\u7565\u5c0d\u81ea\u7136\u8a9e\u8a00\u8a9e\u610f\u7684\u8aa4\u89e3\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba\u300cQD-VMR\u300d\u7684\u65b0\u7a4e\u6a21\u578b\uff0c\u9019\u662f\u4e00\u500b\u5177\u5099\u589e\u5f37\u8108\u7d61\u7406\u89e3\u7684\u67e5\u8a62\u53bb\u504f\u5dee\u6a21\u578b\u3002\u9996\u5148\uff0c\u6211\u5011\u900f\u904e\u5f71\u7247\u7247\u6bb5\u548c\u67e5\u8a62\u7279\u5fb5\u5c0d\u9f4a\u4ee5\u53ca\u5f71\u7247-\u67e5\u8a62\u5c0d\u6bd4\u5b78\u7fd2\uff0c\u5229\u7528\u4e00\u500b\u5168\u5c40\u90e8\u5206\u5c0d\u9f4a\u5668\u6a21\u7d44\u4f86\u589e\u5f37\u6a21\u578b\u7684\u8de8\u6a21\u614b\u7406\u89e3\u80fd\u529b\u3002\u96a8\u5f8c\uff0c\u6211\u5011\u63a1\u7528\u4e00\u500b\u67e5\u8a62\u53bb\u504f\u5dee\u6a21\u7d44\u4f86\u6709\u6548\u53d6\u5f97\u53bb\u504f\u5dee\u7684\u67e5\u8a62\u7279\u5fb5\uff0c\u4e26\u63a1\u7528\u4e00\u500b\u8996\u89ba\u589e\u5f37\u6a21\u7d44\u4f86\u6539\u5584\u8207\u67e5\u8a62\u76f8\u95dc\u7684\u5f71\u7247\u7279\u5fb5\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63a1\u7528 DETR \u7d50\u69cb\u4f86\u9810\u6e2c\u53ef\u80fd\u7684\u76ee\u6a19\u5f71\u7247\u6642\u523b\u3002\u900f\u904e\u5c0d\u4e09\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u7684\u5ee3\u6cdb\u8a55\u4f30\uff0cQD-VMR \u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u8b49\u660e\u4e86\u5176\u6539\u5584 VMR \u7cbe\u78ba\u5ea6\u7684\u6f5b\u529b\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u63d0\u51fa\u7684\u6a21\u7d44\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5c07\u6703\u91cb\u51fa\uff0c\u4ee5\u5229\u672a\u4f86\u7684\u7814\u7a76\u3002", "author": "Chenghua Gao et.al.", "authors": "Chenghua Gao, Min Li, Jianshuo Liu, Junxing Ren, Lin Chen, Haoyu Liu, Bo Meng, Jitao Fu, Wenwen Su", "id": "2408.12981v1", "paper_url": "http://arxiv.org/abs/2408.12981v1", "repo": "null"}}