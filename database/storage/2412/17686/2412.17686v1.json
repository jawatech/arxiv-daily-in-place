{"2412.17686": {"publish_time": "2024-12-23", "title": "Large Language Model Safety: A Holistic Survey", "paper_summary": "The rapid development and deployment of large language models (LLMs) have\nintroduced a new frontier in artificial intelligence, marked by unprecedented\ncapabilities in natural language understanding and generation. However, the\nincreasing integration of these models into critical applications raises\nsubstantial safety concerns, necessitating a thorough examination of their\npotential risks and associated mitigation strategies.\n  This survey provides a comprehensive overview of the current landscape of LLM\nsafety, covering four major categories: value misalignment, robustness to\nadversarial attacks, misuse, and autonomous AI risks. In addition to the\ncomprehensive review of the mitigation methodologies and evaluation resources\non these four aspects, we further explore four topics related to LLM safety:\nthe safety implications of LLM agents, the role of interpretability in\nenhancing LLM safety, the technology roadmaps proposed and abided by a list of\nAI companies and institutes for LLM safety, and AI governance aimed at LLM\nsafety with discussions on international cooperation, policy proposals, and\nprospective regulatory directions.\n  Our findings underscore the necessity for a proactive, multifaceted approach\nto LLM safety, emphasizing the integration of technical solutions, ethical\nconsiderations, and robust governance frameworks. This survey is intended to\nserve as a foundational resource for academy researchers, industry\npractitioners, and policymakers, offering insights into the challenges and\nopportunities associated with the safe integration of LLMs into society.\nUltimately, it seeks to contribute to the safe and beneficial development of\nLLMs, aligning with the overarching goal of harnessing AI for societal\nadvancement and well-being. A curated list of related papers has been publicly\navailable at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u767c\u5c55\u548c\u90e8\u7f72\u5f15\u9032\u4e86\u4eba\u5de5\u667a\u6167\u7684\u65b0\u9818\u57df\uff0c\u4ee5\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u7684\u7a7a\u524d\u80fd\u529b\u70ba\u6a19\u8a8c\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u8207\u95dc\u9375\u61c9\u7528\u7a0b\u5f0f\u6574\u5408\u7684\u589e\u52a0\u5f15\u767c\u4e86\u91cd\u5927\u7684\u5b89\u5168\u554f\u984c\uff0c\u9700\u8981\u5fb9\u5e95\u6aa2\u67e5\u5176\u6f5b\u5728\u98a8\u96aa\u548c\u76f8\u95dc\u7684\u7de9\u89e3\u7b56\u7565\u3002\n\u9019\u9805\u8abf\u67e5\u63d0\u4f9b\u4e86 LLM \u5b89\u5168\u73fe\u6cc1\u7684\u5168\u9762\u6982\u8ff0\uff0c\u6db5\u84cb\u56db\u500b\u4e3b\u8981\u985e\u5225\uff1a\u50f9\u503c\u932f\u4f4d\u3001\u5c0d\u6297\u653b\u64ca\u7684\u7a69\u5065\u6027\u3001\u8aa4\u7528\u548c\u81ea\u4e3b AI \u98a8\u96aa\u3002\u9664\u4e86\u5c0d\u9019\u56db\u500b\u65b9\u9762\u7684\u7de9\u89e3\u65b9\u6cd5\u548c\u8a55\u4f30\u8cc7\u6e90\u9032\u884c\u5168\u9762\u56de\u9867\u5916\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63a2\u8a0e\u4e86\u8207 LLM \u5b89\u5168\u76f8\u95dc\u7684\u56db\u500b\u4e3b\u984c\uff1aLLM \u4ee3\u7406\u7684\u5b89\u5168\u6027\u5f71\u97ff\u3001\u53ef\u89e3\u91cb\u6027\u5728\u589e\u5f37 LLM \u5b89\u5168\u6027\u4e2d\u7684\u4f5c\u7528\u3001AI \u516c\u53f8\u548c\u6a5f\u69cb\u70ba LLM \u5b89\u5168\u6027\u63d0\u51fa\u7684\u548c\u9075\u5b88\u7684\u6280\u672f\u8def\u7dda\u5716\uff0c\u4ee5\u53ca\u91dd\u5c0d LLM \u5b89\u5168\u6027\u7684 AI \u6cbb\u7406\uff0c\u8a0e\u8ad6\u4e86\u570b\u969b\u5408\u4f5c\u3001\u653f\u7b56\u63d0\u6848\u548c\u6f5b\u5728\u7684\u6cd5\u898f\u65b9\u5411\u3002\n\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u5f37\u8abf\u4e86\u4e3b\u52d5\u3001\u591a\u65b9\u9762\u7684 LLM \u5b89\u5168\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\uff0c\u5f37\u8abf\u6280\u8853\u89e3\u6c7a\u65b9\u6848\u3001\u502b\u7406\u8003\u91cf\u548c\u7a69\u5065\u7684\u6cbb\u7406\u67b6\u69cb\u7684\u6574\u5408\u3002\u9019\u9805\u8abf\u67e5\u65e8\u5728\u4f5c\u70ba\u5b78\u8853\u7814\u7a76\u4eba\u54e1\u3001\u7522\u696d\u5f9e\u696d\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u7684\u57fa\u790e\u8cc7\u6e90\uff0c\u63d0\u4f9b\u5c0d LLM \u5b89\u5168\u6574\u5408\u5230\u793e\u6703\u4e2d\u76f8\u95dc\u6311\u6230\u548c\u6a5f\u6703\u7684\u898b\u89e3\u3002\u6700\u7d42\uff0c\u5b83\u65e8\u5728\u4fc3\u9032 LLM \u7684\u5b89\u5168\u548c\u6709\u76ca\u767c\u5c55\uff0c\u8207\u5229\u7528 AI \u4fc3\u9032\u793e\u6703\u9032\u6b65\u548c\u798f\u7949\u7684\u7e3d\u9ad4\u76ee\u6a19\u4fdd\u6301\u4e00\u81f4\u3002\u5df2\u516c\u958b\u6574\u7406\u7684\u76f8\u95dc\u8ad6\u6587\u6e05\u55ae https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers\u3002", "author": "Dan Shi et.al.", "authors": "Dan Shi, Tianhao Shen, Yufei Huang, Zhigen Li, Yongqi Leng, Renren Jin, Chuang Liu, Xinwei Wu, Zishan Guo, Linhao Yu, Ling Shi, Bojian Jiang, Deyi Xiong", "id": "2412.17686v1", "paper_url": "http://arxiv.org/abs/2412.17686v1", "repo": "null"}}