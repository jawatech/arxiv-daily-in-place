# arxiv-daily
 Automated deployment @ 2024-06-29 20:21:58 Asia/Taipei
> Welcome to contribute! Add your topics and keywords in [`topic.yml`](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/topic.yml).
> You can also view historical data through the [storage](https://github.com/jawatech/arxiv-daily-in-place/blob/main/database/storage).

## AI

### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-27**|**HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale**|Junying Chen et.al.|[2406.19280v1](http://arxiv.org/abs/2406.19280v1)|null|
|**2024-06-27**|**Segment Anything Model for automated image data annotation: empirical studies using text prompts from Grounding DINO**|Fuseini Mumuni et.al.|[2406.19057v1](http://arxiv.org/abs/2406.19057v1)|null|
|**2024-06-27**|**FedMap: Iterative Magnitude-Based Pruning for Communication-Efficient Federated Learning**|Alexander Herzog et.al.|[2406.19050v1](http://arxiv.org/abs/2406.19050v1)|null|
|**2024-06-27**|**CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting Universal Machine Learning for Accelerated Cardiac MRI**|Zi Wang et.al.|[2406.19043v1](http://arxiv.org/abs/2406.19043v1)|null|
|**2024-06-27**|**Lithium-Ion Battery System Health Monitoring and Fault Analysis from Field Data Using Gaussian Processes**|Joachim Schaeffer et.al.|[2406.19015v1](http://arxiv.org/abs/2406.19015v1)|null|
|**2024-06-27**|**FedMLP: Federated Multi-Label Medical Image Classification under Task Heterogeneity**|Zhaobin Sun et.al.|[2406.18995v1](http://arxiv.org/abs/2406.18995v1)|[link](https://github.com/szbonaldo/fedmlp)|
|**2024-06-27**|**Alignment For Performance Improvement in Conversation Bots**|Raghav Garg et.al.|[2406.18954v1](http://arxiv.org/abs/2406.18954v1)|null|
|**2024-06-27**|**Correspondence-Free Non-Rigid Point Set Registration Using Unsupervised Clustering Analysis**|Mingyang Zhao et.al.|[2406.18817v1](http://arxiv.org/abs/2406.18817v1)|[link](https://github.com/zikai1/cvpr24_pointsetreg)|
|**2024-06-26**|**WavRx: a Disease-Agnostic, Generalizable, and Privacy-Preserving Speech Health Diagnostic Model**|Yi Zhu et.al.|[2406.18731v1](http://arxiv.org/abs/2406.18731v1)|[link](https://github.com/zhu00121/wavrx)|
|**2024-06-26**|**Petal-X: Human-Centered Visual Explanations to Improve Cardiovascular Risk Communication**|Diego Rojo et.al.|[2406.18690v1](http://arxiv.org/abs/2406.18690v1)|null|
|**2024-06-26**|**Stable Diffusion Segmentation for Biomedical Images with Single-step Reverse Process**|Tianyu Lin et.al.|[2406.18361v2](http://arxiv.org/abs/2406.18361v2)|[link](https://github.com/lin-tianyu/stable-diffusion-seg)|
|**2024-06-26**|**Automatic Prediction of Amyotrophic Lateral Sclerosis Progression using Longitudinal Speech Transformer**|Liming Wang et.al.|[2406.18625v1](http://arxiv.org/abs/2406.18625v1)|null|
|**2024-06-26**|**EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction Using Large Language Multimodal Models**|Chun-Chieh Liao et.al.|[2406.18087v1](http://arxiv.org/abs/2406.18087v1)|null|
|**2024-06-26**|**Few-Shot Medical Image Segmentation with High-Fidelity Prototypes**|Song Tang et.al.|[2406.18074v1](http://arxiv.org/abs/2406.18074v1)|[link](https://github.com/tntek/dspnet)|
|**2024-06-26**|**Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources**|Yiming Li et.al.|[2406.18049v1](http://arxiv.org/abs/2406.18049v1)|null|
|**2024-06-26**|**Automated Clinical Data Extraction with Knowledge Conditioned LLMs**|Diya Li et.al.|[2406.18027v1](http://arxiv.org/abs/2406.18027v1)|null|
|**2024-06-26**|**AutoOPE: Automated Off-Policy Estimator Selection**|Nicol√≤ Felicioni et.al.|[2406.18022v1](http://arxiv.org/abs/2406.18022v1)|null|
|**2024-06-26**|**Multi-step Knowledge Retrieval and Inference over Unstructured Data**|Aditya Kalyanpur et.al.|[2406.17987v1](http://arxiv.org/abs/2406.17987v1)|null|
|**2024-06-25**|**Domain Adaptation of Echocardiography Segmentation Via Reinforcement Learning**|Arnaud Judge et.al.|[2406.17902v1](http://arxiv.org/abs/2406.17902v1)|null|
|**2024-06-25**|**CTBench: A Comprehensive Benchmark for Evaluating Language Model Capabilities in Clinical Trial Design**|Nafis Neehal et.al.|[2406.17888v1](http://arxiv.org/abs/2406.17888v1)|[link](https://github.com/nafis-neehal/CTBench_LLM)|
|**2024-06-25**|**BayTTA: Uncertainty-aware medical image classification with optimized test-time augmentation using Bayesian model averaging**|Zeinab Sherkatghanad et.al.|[2406.17640v1](http://arxiv.org/abs/2406.17640v1)|[link](https://github.com/z-sherkat/baytta)|
|**2024-06-25**|**Towards Compositional Interpretability for XAI**|Sean Tull et.al.|[2406.17583v1](http://arxiv.org/abs/2406.17583v1)|null|
|**2024-06-25**|**On the consistency of hyper-parameter selection in value-based deep reinforcement learning**|Johan Obando-Ceron et.al.|[2406.17523v1](http://arxiv.org/abs/2406.17523v1)|null|
|**2024-06-25**|**TSynD: Targeted Synthetic Data Generation for Enhanced Medical Image Classification**|Joshua Niemeijer et.al.|[2406.17473v1](http://arxiv.org/abs/2406.17473v1)|null|
|**2024-06-25**|**AI for the prediction of early stages of Alzheimer's disease from neuroimaging biomarkers -- A narrative review of a growing field**|Thorsten Rudroff et.al.|[2406.17822v1](http://arxiv.org/abs/2406.17822v1)|null|
|**2024-06-25**|**Task-Agnostic Federated Learning**|Zhengtao Yao et.al.|[2406.17235v1](http://arxiv.org/abs/2406.17235v1)|null|
|**2024-06-24**|**Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars**|Wesley Brewer et.al.|[2406.17812v1](http://arxiv.org/abs/2406.17812v1)|null|
|**2024-06-24**|**PIC2O-Sim: A Physics-Inspired Causality-Aware Dynamic Convolutional Neural Operator for Ultra-Fast Photonic Device FDTD Simulation**|Pingchuan Ma et.al.|[2406.17810v1](http://arxiv.org/abs/2406.17810v1)|[link](https://github.com/ScopeX-ASU/PIC2O-Sim)|
|**2024-06-24**|**The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources**|Shayne Longpre et.al.|[2406.16746v2](http://arxiv.org/abs/2406.16746v2)|null|
|**2024-06-24**|**Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings**|Andrea Posada et.al.|[2406.16611v1](http://arxiv.org/abs/2406.16611v1)|[link](https://github.com/anpoc/language-models-in-medicine)|
|**2024-06-24**|**Guardrails for avoiding harmful medical product recommendations and off-label promotion in generative AI models**|Daniel Lopez-Martinez et.al.|[2406.16455v1](http://arxiv.org/abs/2406.16455v1)|null|
|**2024-06-24**|**A large language model for predicting T cell receptor-antigen binding specificity**|Xing Fang et.al.|[2406.16995v1](http://arxiv.org/abs/2406.16995v1)|[link](https://github.com/hliulab/tcrlm)|
|**2024-06-24**|**Graph-Augmented LLMs for Personalized Health Insights: A Case Study in Sleep Analysis**|Ajan Subramanian et.al.|[2406.16252v2](http://arxiv.org/abs/2406.16252v2)|null|
|**2024-06-23**|**Continuous Output Personality Detection Models via Mixed Strategy Training**|Rong Wang et.al.|[2406.16223v1](http://arxiv.org/abs/2406.16223v1)|null|
|**2024-06-23**|**On Instabilities of Unsupervised Denoising Diffusion Models in Magnetic Resonance Imaging Reconstruction**|Tianyu Han et.al.|[2406.16983v1](http://arxiv.org/abs/2406.16983v1)|null|
|**2024-06-23**|**Research on Disease Prediction Model Construction Based on Computer AI deep Learning Technology**|Yang Lin et.al.|[2406.16982v1](http://arxiv.org/abs/2406.16982v1)|null|
|**2024-06-23**|**Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking**|Yuwei Zhang et.al.|[2406.16148v1](http://arxiv.org/abs/2406.16148v1)|[link](https://github.com/evelyn0414/opera)|
|**2024-06-23**|**Predicting Individual Depression Symptoms from Acoustic Features During Speech**|Sebastian Rodriguez et.al.|[2406.16000v1](http://arxiv.org/abs/2406.16000v1)|null|
|**2024-06-23**|**Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care**|Hassan Alhuzali et.al.|[2406.15966v1](http://arxiv.org/abs/2406.15966v1)|null|
|**2024-06-22**|**SEDMamba: Enhancing Selective State Space Modelling with Bottleneck Mechanism and Fine-to-Coarse Temporal Fusion for Efficient Error Detection in Robot-Assisted Surgery**|Jialang Xu et.al.|[2406.15920v1](http://arxiv.org/abs/2406.15920v1)|null|
|**2024-06-22**|**Real-time Speech Summarization for Medical Conversations**|Khai Le-Duc et.al.|[2406.15888v1](http://arxiv.org/abs/2406.15888v1)|[link](https://github.com/leduckhai/multimed)|
|**2024-06-21**|**Automated radiotherapy treatment planning guided by GPT-4Vision**|Sheng Liu et.al.|[2406.15609v1](http://arxiv.org/abs/2406.15609v1)|null|
|**2024-06-21**|**Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach**|Chengzhe Piao et.al.|[2406.15346v1](http://arxiv.org/abs/2406.15346v1)|[link](https://github.com/chengzhepiao/coldstartbglp)|
|**2024-06-21**|**Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms**|Santiago Berrezueta-Guzman et.al.|[2406.15198v1](http://arxiv.org/abs/2406.15198v1)|null|
|**2024-06-21**|**This actually looks like that: Proto-BagNets for local and global interpretability-by-design**|Kerol Djoumessi et.al.|[2406.15168v2](http://arxiv.org/abs/2406.15168v2)|[link](https://github.com/kdjoumessi/proto-bagnets)|
|**2024-06-21**|**FA-Net: A Fuzzy Attention-aided Deep Neural Network for Pneumonia Detection in Chest X-Rays**|Ayush Roy et.al.|[2406.15117v1](http://arxiv.org/abs/2406.15117v1)|[link](https://github.com/ayushroy2001/fa-net)|
|**2024-06-21**|**Tri-VQA: Triangular Reasoning Medical Visual Question Answering for Multi-Attribute Analysis**|Lin Fan et.al.|[2406.15050v1](http://arxiv.org/abs/2406.15050v1)|null|
|**2024-06-21**|**Human-AI collectives produce the most accurate differential diagnoses**|N. Z√∂ller et.al.|[2406.14981v1](http://arxiv.org/abs/2406.14981v1)|[link](https://github.com/nikozoe/human_ai_collectives)|
|**2024-06-21**|**Deep Imbalanced Regression to Estimate Vascular Age from PPG Data: a Novel Digital Biomarker for Cardiovascular Health**|Guangkun Nie et.al.|[2406.14953v1](http://arxiv.org/abs/2406.14953v1)|null|
|**2024-06-21**|**Extraction of 3D trajectories of mandibular condyles from 2D real-time MRI**|Karyna Isaieva et.al.|[2406.14925v1](http://arxiv.org/abs/2406.14925v1)|null|
|**2024-06-21**|**AI-based Anomaly Detection for Clinical-Grade Histopathological Diagnostics**|Jonas Dippel et.al.|[2406.14866v1](http://arxiv.org/abs/2406.14866v1)|null|
|**2024-06-20**|**ACR: A Benchmark for Automatic Cohort Retrieval**|Dung Ngoc Thai et.al.|[2406.14780v1](http://arxiv.org/abs/2406.14780v1)|null|
|**2024-06-20**|**A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes**|Syed I. Munzir et.al.|[2406.14757v1](http://arxiv.org/abs/2406.14757v1)|null|
|**2024-06-20**|**An updated overview of radiomics-based artificial intelligence (AI) methods in breast cancer screening and diagnosis**|Reza Elahi et.al.|[2406.14735v1](http://arxiv.org/abs/2406.14735v1)|null|
|**2024-06-20**|**This Looks Better than That: Better Interpretable Models with ProtoPNeXt**|Frank Willard et.al.|[2406.14675v1](http://arxiv.org/abs/2406.14675v1)|null|
|**2024-06-20**|**Computation-Efficient Semi-Supervised Learning for ECG-based Cardiovascular Diseases Detection**|Rushuang Zhou et.al.|[2406.14377v1](http://arxiv.org/abs/2406.14377v1)|null|
|**2024-06-20**|**Automatic Labels are as Effective as Manual Labels in Biomedical Images Classification with Deep Learning**|Niccol√≤ Marini et.al.|[2406.14351v1](http://arxiv.org/abs/2406.14351v1)|[link](https://github.com/ilmaro8/wsi_analysis)|
|**2024-06-20**|**Infusing clinical knowledge into tokenisers for language models**|Abul Hasan et.al.|[2406.14312v1](http://arxiv.org/abs/2406.14312v1)|null|
|**2024-06-20**|**Enhancing robustness of data-driven SHM models: adversarial training with circle loss**|Xiangli Yang et.al.|[2406.14232v1](http://arxiv.org/abs/2406.14232v1)|null|
|**2024-06-20**|**A Data-Driven Guided Decoding Mechanism for Diagnostic Captioning**|Panagiotis Kaliosis et.al.|[2406.14164v1](http://arxiv.org/abs/2406.14164v1)|[link](https://github.com/nlpaueb/dmmcs)|
|**2024-06-20**|**Resource-efficient Medical Image Analysis with Self-adapting Forward-Forward Networks**|Johanna P. M√ºller et.al.|[2406.14038v1](http://arxiv.org/abs/2406.14038v1)|null|
|**2024-06-20**|**Reasoning Like a Doctor: Improving Medical Dialogue Systems via Diagnostic Reasoning Process Alignment**|Kaishuai Xu et.al.|[2406.13934v1](http://arxiv.org/abs/2406.13934v1)|[link](https://github.com/kaishxu/emulation)|
|**2024-06-19**|**ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World**|Weixiang Yan et.al.|[2406.13890v1](http://arxiv.org/abs/2406.13890v1)|[link](https://github.com/weixiangyan/clinicallab)|
|**2024-06-19**|**MAMA-MIA: A Large-Scale Multi-Center Breast Cancer DCE-MRI Benchmark Dataset with Expert Segmentations**|Lidia Garrucho et.al.|[2406.13844v1](http://arxiv.org/abs/2406.13844v1)|[link](https://github.com/lidiagarrucho/mama-mia)|
|**2024-06-19**|**IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards for Better Well-Being**|Amelie Gyrard et.al.|[2406.13791v1](http://arxiv.org/abs/2406.13791v1)|null|
|**2024-06-19**|**BEACON: Balancing Convenience and Nutrition in Meals With Long-Term Group Recommendations and Reasoning on Multimodal Recipes**|Vansh Nagpal et.al.|[2406.13714v1](http://arxiv.org/abs/2406.13714v1)|null|
|**2024-06-19**|**EndoUIC: Promptable Diffusion Transformer for Unified Illumination Correction in Capsule Endoscopy**|Long Bai et.al.|[2406.13705v1](http://arxiv.org/abs/2406.13705v1)|[link](https://github.com/longbai1006/endouic)|
|**2024-06-19**|**Leveraging Large Language Models for Patient Engagement: The Power of Conversational AI in Digital Health**|Bo Wen et.al.|[2406.13659v1](http://arxiv.org/abs/2406.13659v1)|null|
|**2024-06-19**|**Enhance the Image: Super Resolution using Artificial Intelligence in MRI**|Ziyu Li et.al.|[2406.13625v1](http://arxiv.org/abs/2406.13625v1)|null|
|**2024-06-19**|**Optimizing Psychological Counseling with Instruction-Tuned Large Language Models**|Wenjie Li et.al.|[2406.13617v1](http://arxiv.org/abs/2406.13617v1)|null|
|**2024-06-19**|**Certificates of Differential Privacy and Unlearning for Gradient-Based Training**|Matthew Wicker et.al.|[2406.13433v1](http://arxiv.org/abs/2406.13433v1)|[link](https://github.com/psosnin/AbstractGradientTraining)|
|**2024-06-19**|**Explainable by-design Audio Segmentation through Non-Negative Matrix Factorization and Probing**|Martin Lebourdais et.al.|[2406.13385v1](http://arxiv.org/abs/2406.13385v1)|[link](https://github.com/Lebourdais/3MAS)|
|**2024-06-19**|**Biomedical Visual Instruction Tuning with Clinician Preference Alignment**|Hejie Cui et.al.|[2406.13173v1](http://arxiv.org/abs/2406.13173v1)|[link](https://github.com/mao1207/BioMed-VITAL)|
|**2024-06-19**|**Cardiac Copilot: Automatic Probe Guidance for Echocardiography with World Model**|Haojun Jiang et.al.|[2406.13165v1](http://arxiv.org/abs/2406.13165v1)|null|
|**2024-06-19**|**Oralytics Reinforcement Learning Algorithm**|Anna L. Trella et.al.|[2406.13127v1](http://arxiv.org/abs/2406.13127v1)|[link](https://github.com/StatisticalReinforcementLearningLab/oralytics_algorithm_design)|
|**2024-06-18**|**Accelerating Complex Disease Treatment through Network Medicine and GenAI: A Case Study on Drug Repurposing for Breast Cancer**|Ahmed Abdeen Hamed et.al.|[2406.13106v3](http://arxiv.org/abs/2406.13106v3)|null|
|**2024-06-18**|**Deriving Hematological Disease Classes Using Fuzzy Logic and Expert Knowledge: A Comprehensive Machine Learning Approach with CBC Parameters**|Salem Ameen et.al.|[2406.13015v1](http://arxiv.org/abs/2406.13015v1)|null|
|**2024-06-18**|**Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation**|Nikolas Koutsoubis et.al.|[2406.12815v1](http://arxiv.org/abs/2406.12815v1)|[link](https://github.com/niko-k98/awesome-list-federated-learning-review)|
|**2024-06-18**|**Probabilistic Temporal Prediction of Continuous Disease Trajectories and Treatment Effects Using Neural SDEs**|Joshua Durso-Finley et.al.|[2406.12807v1](http://arxiv.org/abs/2406.12807v1)|null|
|**2024-06-18**|**Large Language Model as a Universal Clinical Multi-task Decoder**|Yujiang Wu et.al.|[2406.12738v1](http://arxiv.org/abs/2406.12738v1)|null|
|**2024-06-18**|**Online-Adaptive Anomaly Detection for Defect Identification in Aircraft Assembly**|Siddhant Shete et.al.|[2406.12698v1](http://arxiv.org/abs/2406.12698v1)|null|
|**2024-06-18**|**Transforming Surgical Interventions with Embodied Intelligence for Ultrasound Robotics**|Huan Xu et.al.|[2406.12651v1](http://arxiv.org/abs/2406.12651v1)|null|
|**2024-06-18**|**An Empirical Study on the Fairness of Foundation Models for Multi-Organ Image Segmentation**|Qin Li et.al.|[2406.12646v1](http://arxiv.org/abs/2406.12646v1)|null|
|**2024-06-18**|**Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine**|Rui Yang et.al.|[2406.12449v1](http://arxiv.org/abs/2406.12449v1)|null|
|**2024-06-18**|**Adversarial Attacks on Large Language Models in Medicine**|Yifan Yang et.al.|[2406.12259v1](http://arxiv.org/abs/2406.12259v1)|null|
|**2024-06-18**|**Enhancing Diagnostic Reliability of Foundation Model with Uncertainty Estimation in OCT Images**|Yuanyuan Peng et.al.|[2406.16942v1](http://arxiv.org/abs/2406.16942v1)|null|
|**2024-06-18**|**Time Series Modeling for Heart Rate Prediction: From ARIMA to Transformers**|Haowei Ni et.al.|[2406.12199v2](http://arxiv.org/abs/2406.12199v2)|null|
|**2024-06-18**|**Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models**|Lulu Zhao et.al.|[2406.12182v1](http://arxiv.org/abs/2406.12182v1)|null|
|**2024-06-17**|**Slicing Through Bias: Explaining Performance Gaps in Medical Image Analysis using Slice Discovery Methods**|Vincent Olesen et.al.|[2406.12142v1](http://arxiv.org/abs/2406.12142v1)|null|
|**2024-06-17**|**WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions**|Seyedali Mohammadi et.al.|[2406.12058v2](http://arxiv.org/abs/2406.12058v2)|null|
|**2024-06-17**|**MedCalc-Bench: Evaluating Large Language Models for Medical Calculations**|Nikhil Khandekar et.al.|[2406.12036v3](http://arxiv.org/abs/2406.12036v3)|[link](https://github.com/ncbi-nlp/medcalc-bench)|
|**2024-06-17**|**Socially Interactive Agents for Robotic Neurorehabilitation Training: Conceptualization and Proof-of-concept Study**|Rhythm Arora et.al.|[2406.12035v1](http://arxiv.org/abs/2406.12035v1)|null|
|**2024-06-17**|**Improving Quality Control of Whole Slide Images by Explicit Artifact Augmentation**|Artur Jurgas et.al.|[2406.11538v1](http://arxiv.org/abs/2406.11538v1)|null|
|**2024-06-17**|**FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction**|Muhao Xu et.al.|[2406.11928v1](http://arxiv.org/abs/2406.11928v1)|[link](https://github.com/mhxu1998/flexcare)|
|**2024-06-17**|**Formally Certified Approximate Model Counting**|Yong Kiam Tan et.al.|[2406.11414v2](http://arxiv.org/abs/2406.11414v2)|null|
|**2024-06-17**|**Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection**|Sungwon Park et.al.|[2406.11260v1](http://arxiv.org/abs/2406.11260v1)|null|
|**2024-06-17**|**Scorecards for Synthetic Medical Data Evaluation and Reporting**|Ghada Zamzmi et.al.|[2406.11143v1](http://arxiv.org/abs/2406.11143v1)|null|
|**2024-06-17**|**Diffusion Models in Low-Level Vision: A Survey**|Chunming He et.al.|[2406.11138v1](http://arxiv.org/abs/2406.11138v1)|null|
|**2024-06-17**|**Towards Understanding Emotions for Engaged Mental Health Conversations**|Kellie Yu Hui Sim et.al.|[2406.11135v1](http://arxiv.org/abs/2406.11135v1)|null|
|**2024-06-16**|**Boosting Medical Image Classification with Segmentation Foundation Model**|Pengfei Gu et.al.|[2406.11026v1](http://arxiv.org/abs/2406.11026v1)|null|

#### Abstracts
##### **HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale**
2406.19280v1 by Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang

The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºå‰æãÂ¶Ç GPT-4VÔºåÂ∏¶Êù•‰∫ÜÈáçÂ§ßÁöÑËøõÊ≠•„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÂåªÁñóËßÜËßâÊñáÊú¨Êï∞ÊçÆÁöÑÊï∞ÈáèÂíåË¥®ÈáèÁöÑÈôêÂà∂ÔºåËøô‰∫õÊ®°ÂûãÂú®ÂåªÁñóÂ§öÊ®°ÊÄÅËÉΩÂäõÊñπÈù¢‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàòÔºåËøôÊ∫ê‰∫éÊï∞ÊçÆÈöêÁßÅÈóÆÈ¢òÂíåÈ´òÊòÇÁöÑÊ†áÊ≥®ÊàêÊú¨„ÄÇËôΩÁÑ∂ÂºÄÂàõÊÄßÁöÑÊñπÊ≥ïÂà©Áî® PubMed ÁöÑÂ§ßËßÑÊ®°„ÄÅÂéªÊ†áËØÜÂåñÁöÑÂåªÂ≠¶ÂõæÂÉèÊñáÊú¨ÂØπÊù•Ëß£ÂÜ≥Ëøô‰∫õÈôêÂà∂Ôºå‰ΩÜÁî±‰∫éÂõ∫ÊúâÁöÑÊï∞ÊçÆÂô™Â£∞ÔºåÂÆÉ‰ª¨‰ªçÁÑ∂Â≠òÂú®‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨‰ªé PubMed ‰∏≠‰ºòÂåñ‰∫ÜÂåªÂ≠¶ÂõæÂÉèÊñáÊú¨ÂØπÔºåÂπ∂‰ª•‚ÄúÈùûÁõ≤‚ÄùÁöÑÊñπÂºèÈááÁî®‰∫Ü MLLMÔºàGPT-4VÔºâÊù•ÂØπÊï∞ÊçÆËøõË°åÂéªÂô™ÂíåÈáçÊñ∞Ê†ºÂºèÂåñÔºå‰ªéËÄåÂàõÂª∫‰∫ÜÂåÖÂê´ 130 ‰∏á‰∏™ÂåªÂ≠¶ VQA Ê†∑Êú¨ÁöÑ PubMedVision Êï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨ÁöÑÈ™åËØÅË°®ÊòéÔºö(1) PubMedVision ÂèØ‰ª•ÊòæËëóÂ¢ûÂº∫ÂΩìÂâç MLLM ÁöÑÂåªÁñóÂ§öÊ®°ÊÄÅËÉΩÂäõÔºåÂú®ÂåÖÊã¨ MMMU ÂÅ•Â∫∑‰∏éÂåªÂ≠¶ËΩ®ÈÅìÂú®ÂÜÖÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊîπËøõÔºõ(2) ÂåªÂ≠¶‰∏ìÂÆ∂ÁöÑÊâãÂä®Ê£ÄÊü•ÂíåÂÆûËØÅÁªìÊûúÈ™åËØÅ‰∫ÜÊàë‰ª¨Êï∞ÊçÆÈõÜ‰∏éÂÖ∂‰ªñÊï∞ÊçÆÊûÑÂª∫ÊñπÊ≥ïÁõ∏ÊØîÁöÑÂçìË∂äÊï∞ÊçÆË¥®Èáè„ÄÇ‰ΩøÁî® PubMedVisionÔºåÊàë‰ª¨ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™ 34B ÂåªÂ≠¶ MLLM HuatuoGPT-VisionÔºåÂÆÉÂú®ÂºÄÊ∫ê MLLM ‰∏≠ÁöÑÂåªÂ≠¶Â§öÊ®°ÊÄÅÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫ÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇ

##### **Segment Anything Model for automated image data annotation: empirical studies using text prompts from Grounding DINO**
2406.19057v1 by Fuseini Mumuni, Alhassan Mumuni

Grounding DINO and the Segment Anything Model (SAM) have achieved impressive
performance in zero-shot object detection and image segmentation, respectively.
Together, they have a great potential in revolutionizing zero-shot semantic
segmentation or data annotation. Yet, in specialized domains like medical image
segmentation, objects of interest (e.g., organs, tissues, and tumors) may not
fall in existing class names. To address this problem, the referring expression
comprehension (REC) ability of Grounding DINO is leveraged to detect arbitrary
targets by their language descriptions. However, recent studies have
highlighted severe limitation of the REC framework in this application setting
owing to its tendency to make false positive predictions when the target is
absent in the given image. And, while this bottleneck is central to the
prospect of open-set semantic segmentation, it is still largely unknown how
much improvement can be achieved by studying the prediction errors. To this
end, we perform empirical studies on eight publicly available datasets and
reveal that these errors consistently follow a predictable pattern and can,
thus, be mitigated by a simple strategy. Specifically, we show that these false
positive detections with appreciable confidence scores generally occupy large
image areas and can usually be filtered by their relative sizes. More
importantly, we expect these observations to inspire future research in
improving REC-based detection and automated segmentation. Using this technique,
we evaluate the performance of SAM on multiple datasets from various
specialized domains and report significant improvement in segmentation
performance and annotation time savings over manual approaches.

ÊëòË¶ÅÔºöGrounding DINO Âíå Segment Anything Model (SAM) Âú®Èõ∂Ê¨°Áâ©È´îÂÅµÊ∏¨ÂíåÂΩ±ÂÉèÂàÜÂâ≤‰∏≠ÂàÜÂà•ÂèñÂæó‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑË°®Áèæ„ÄÇÂÆÉÂÄëÂÖ±ÂêåÊìÅÊúâÂú®Èõ∂Ê¨°Ë™ûÊÑèÂàÜÂâ≤ÊàñË≥áÊñôÊ®ôË®ª‰∏≠ÊéÄËµ∑Èù©ÂëΩÁöÑÂ∑®Â§ßÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Á≠âÂ∞àÊ•≠È†òÂüü‰∏≠ÔºåÊÑüËààË∂£ÁöÑÁâ©È´îÔºà‰æãÂ¶ÇÂô®ÂÆò„ÄÅÁµÑÁπîÂíåËÖ´Áò§ÔºâÂèØËÉΩ‰∏çÂú®ÁèæÊúâÁöÑÈ°ûÂà•ÂêçÁ®±‰∏≠„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂà©Áî® Grounding DINO ÁöÑÊåáÊ∂âË°®ÈÅîÁêÜËß£ (REC) ËÉΩÂäõÔºåÈÄèÈÅéË™ûË®ÄÊèèËø∞‰æÜÂÅµÊ∏¨‰ªªÊÑèÁõÆÊ®ô„ÄÇÁÑ∂ËÄåÔºåÊúÄËøëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫Ü REC Êû∂ÊßãÂú®ÈÄôÂÄãÊáâÁî®Ë®≠ÂÆö‰∏≠ÁöÑÂö¥ÈáçÈôêÂà∂ÔºåÂõ†ÁÇ∫Áï∂ÁõÆÊ®ô‰∏çÂ≠òÂú®ÊñºÁµ¶ÂÆöÁöÑÂΩ±ÂÉè‰∏≠ÊôÇÔºåÂÆÉÂÇæÂêëÊñºÂÅöÂá∫ÂÅáÈôΩÊÄßÈ†êÊ∏¨„ÄÇËÄå‰∏îÔºåÈõñÁÑ∂Ê≠§Áì∂È†∏Â∞çÊñºÈñãÊîæÂºèË™ûÊÑèÂàÜÂâ≤ÁöÑÂâçÊôØËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÈÄèÈÅéÁ†îÁ©∂È†êÊ∏¨Ë™§Â∑ÆËÉΩÁç≤ÂæóÂ§öÂ∞ëÊîπÂñÑ‰ªçÊòØÊú™Áü•ÁöÑ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂ∞çÂÖ´ÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÂØ¶Ë≠âÁ†îÁ©∂Ôºå‰∏¶Êè≠Á§∫ÈÄô‰∫õË™§Â∑ÆÂßãÁµÇÈÅµÂæ™ÂèØÈ†êÊ∏¨ÁöÑÊ®°ÂºèÔºåÂõ†Ê≠§ÔºåÂèØ‰ª•ÈÄèÈÅé‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÁ≠ñÁï•‰æÜÊ∏õËºï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË°®ÊòéÈÄô‰∫õÂÖ∑ÊúâÂèØËßÄÁΩÆ‰ø°Â∫¶ÁöÑÂÅáÈôΩÊÄßÂÅµÊ∏¨ÈÄöÂ∏∏‰ΩîÊìöËºÉÂ§ßÁöÑÂΩ±ÂÉèÂçÄÂüüÔºå‰∏¶‰∏îÈÄöÂ∏∏ÂèØ‰ª•Ê†πÊìöÂÆÉÂÄëÁöÑÁõ∏Â∞çÂ§ßÂ∞è‰æÜÈÅéÊøæ„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÈ†êÊúüÈÄô‰∫õËßÄÂØüÁµêÊûúÂ∞áÊøÄÂãµÊú™‰æÜÂú®ÊîπÂñÑÂü∫Êñº REC ÁöÑÂÅµÊ∏¨ÂíåËá™ÂãïÂàÜÂâ≤ÁöÑÁ†îÁ©∂„ÄÇ‰ΩøÁî®Ê≠§ÊäÄË°ìÔºåÊàëÂÄëË©ï‰º∞‰∫Ü SAM Âú®‰æÜËá™ÂêÑÁ®ÆÂ∞àÊ•≠È†òÂüüÁöÑÂ§öÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩÔºå‰∏¶Â†±ÂëäÂú®ÂàÜÂâ≤ÊïàËÉΩÂíåÊ®ôË®ªÊôÇÈñìÁØÄÁúÅ‰∏äÁõ∏ËºÉÊñºÊâãÂãïÊñπÊ≥ïÊúâÈ°ØËëóÁöÑÊîπÂñÑ„ÄÇ

##### **FedMap: Iterative Magnitude-Based Pruning for Communication-Efficient Federated Learning**
2406.19050v1 by Alexander Herzog, Robbie Southam, Ioannis Mavromatis, Aftab Khan

Federated Learning (FL) is a distributed machine learning approach that
enables training on decentralized data while preserving privacy. However, FL
systems often involve resource-constrained client devices with limited
computational power, memory, storage, and bandwidth. This paper introduces
FedMap, a novel method that aims to enhance the communication efficiency of FL
deployments by collaboratively learning an increasingly sparse global model
through iterative, unstructured pruning. Importantly, FedMap trains a global
model from scratch, unlike other methods reported in the literature, making it
ideal for privacy-critical use cases such as in the medical and finance
domains, where suitable pre-training data is often limited. FedMap adapts
iterative magnitude-based pruning to the FL setting, ensuring all clients prune
and refine the same subset of the global model parameters, therefore gradually
reducing the global model size and communication overhead. The iterative nature
of FedMap, forming subsequent models as subsets of predecessors, avoids
parameter reactivation issues seen in prior work, resulting in stable
performance. In this paper we provide an extensive evaluation of FedMap across
diverse settings, datasets, model architectures, and hyperparameters, assessing
performance in both IID and non-IID environments. Comparative analysis against
the baseline approach demonstrates FedMap's ability to achieve more stable
client model performance. For IID scenarios, FedMap achieves over $90$\%
pruning without significant performance degradation. In non-IID settings, it
achieves at least $~80$\% pruning while maintaining accuracy. FedMap offers a
promising solution to alleviate communication bottlenecks in FL systems while
retaining model accuracy.

ÊëòË¶ÅÔºöËÅîÈÇ¶Â≠¶‰π† (FL) ÊòØ‰∏ÄÁßçÂàÜÂ∏ÉÂºèÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ïÔºåÂèØÂú®‰øùÊä§ÈöêÁßÅÁöÑÂêåÊó∂ÂØπÂàÜÊï£Êï∞ÊçÆËøõË°åËÆ≠ÁªÉ„ÄÇÁÑ∂ËÄåÔºåFL Á≥ªÁªüÈÄöÂ∏∏Ê∂âÂèäËµÑÊ∫êÂèóÈôêÁöÑÂÆ¢Êà∑Á´ØËÆæÂ§áÔºåÂÖ∂ËÆ°ÁÆóËÉΩÂäõ„ÄÅÂÜÖÂ≠ò„ÄÅÂ≠òÂÇ®ÂíåÂ∏¶ÂÆΩÊúâÈôê„ÄÇÊú¨Êñá‰ªãÁªç‰∫Ü FedMapÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂçè‰ΩúÂ≠¶‰π†‰∏Ä‰∏™‰∏çÊñ≠Á®ÄÁñèÁöÑÂÖ®Â±ÄÊ®°ÂûãÔºàÈÄöËøáËø≠‰ª£ÁöÑÈùûÁªìÊûÑÂåñÂâ™ÊûùÔºâÊù•ÊèêÈ´ò FL ÈÉ®ÁΩ≤ÁöÑÈÄö‰ø°ÊïàÁéá„ÄÇÈáçË¶ÅÁöÑÊòØÔºåFedMap ‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉ‰∏Ä‰∏™ÂÖ®Â±ÄÊ®°ÂûãÔºåËøô‰∏éÊñáÁåÆ‰∏≠Êä•ÈÅìÁöÑÂÖ∂‰ªñÊñπÊ≥ï‰∏çÂêåÔºå‰ΩøÂÖ∂ÈùûÂ∏∏ÈÄÇÂêàÈöêÁßÅËá≥ÂÖ≥ÈáçË¶ÅÁöÑÁî®‰æãÔºå‰æãÂ¶ÇÂåªÁñóÂíåÈáëËûçÈ¢ÜÂüüÔºåÂÖ∂‰∏≠ÂêàÈÄÇÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈÄöÂ∏∏ÊúâÈôê„ÄÇFedMap Â∞ÜÂü∫‰∫éËø≠‰ª£ÂπÖÂ∫¶ÁöÑÂâ™ÊûùË∞ÉÊï¥Âà∞ FL ËÆæÁΩÆ‰∏≠ÔºåÁ°Æ‰øùÊâÄÊúâÂÆ¢Êà∑Á´ØÈÉΩÂâ™ÊûùÂπ∂‰ºòÂåñÂÖ®Â±ÄÊ®°ÂûãÂèÇÊï∞ÁöÑÁõ∏ÂêåÂ≠êÈõÜÔºå‰ªéËÄåÈÄêÊ∏êÂáèÂ∞ëÂÖ®Â±ÄÊ®°ÂûãÂ§ßÂ∞èÂíåÈÄö‰ø°ÂºÄÈîÄ„ÄÇFedMap ÁöÑËø≠‰ª£ÊÄßË¥®ÔºåÂ∞ÜÂêéÁª≠Ê®°ÂûãÂΩ¢Êàê‰∏∫Ââç‰ª£Ê®°ÂûãÁöÑÂ≠êÈõÜÔºåÈÅøÂÖç‰∫ÜÂÖàÂâçÂ∑•‰Ωú‰∏≠ÁúãÂà∞ÁöÑÂèÇÊï∞ÈáçÊñ∞ÊøÄÊ¥ªÈóÆÈ¢òÔºå‰ªéËÄå‰∫ßÁîü‰∫ÜÁ®≥ÂÆöÁöÑÊÄßËÉΩ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂØπ FedMap Âú®‰∏çÂêåËÆæÁΩÆ„ÄÅÊï∞ÊçÆÈõÜ„ÄÅÊ®°ÂûãÊû∂ÊûÑÂíåË∂ÖÂèÇÊï∞‰∏≠ËøõË°å‰∫ÜÂπøÊ≥õËØÑ‰º∞ÔºåËØÑ‰º∞‰∫ÜÂú® IID ÂíåÈùû IID ÁéØÂ¢É‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ‰∏éÂü∫ÂáÜÊñπÊ≥ïÁöÑÊØîËæÉÂàÜÊûêËØÅÊòé‰∫Ü FedMap ËÉΩÂ§üÂÆûÁé∞Êõ¥Á®≥ÂÆöÁöÑÂÆ¢Êà∑Á´ØÊ®°ÂûãÊÄßËÉΩ„ÄÇÂØπ‰∫é IID Âú∫ÊôØÔºåFedMap Âú®‰∏çÊòæÁùÄÈôç‰ΩéÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞‰∫ÜË∂ÖËøá 90% ÁöÑÂâ™Êûù„ÄÇÂú®Èùû IID ËÆæÁΩÆ‰∏≠ÔºåÂÆÉÂú®‰øùÊåÅÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÂÆûÁé∞‰∫ÜËá≥Â∞ë 80% ÁöÑÂâ™Êûù„ÄÇFedMap ‰∏∫ÁºìËß£ FL Á≥ªÁªü‰∏≠ÁöÑÈÄö‰ø°Áì∂È¢àÂêåÊó∂‰øùÊåÅÊ®°ÂûãÂáÜÁ°ÆÊÄßÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊúâÂ∏åÊúõÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

##### **CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting Universal Machine Learning for Accelerated Cardiac MRI**
2406.19043v1 by Zi Wang, Fanwen Wang, Chen Qin, Jun Lyu, Ouyang Cheng, Shuo Wang, Yan Li, Mengyao Yu, Haoyu Zhang, Kunyuan Guo, Zhang Shi, Qirong Li, Ziqiang Xu, Yajing Zhang, Hao Li, Sha Hua, Binghua Chen, Longyu Sun, Mengting Sun, Qin Li, Ying-Hua Chu, Wenjia Bai, Jing Qin, Xiahai Zhuang, Claudia Prieto, Alistair Young, Michael Markl, He Wang, Lianming Wu, Guang Yang, Xiaobo Qu, Chengyan Wang

Cardiac magnetic resonance imaging (MRI) has emerged as a clinically
gold-standard technique for diagnosing cardiac diseases, thanks to its ability
to provide diverse information with multiple modalities and anatomical views.
Accelerated cardiac MRI is highly expected to achieve time-efficient and
patient-friendly imaging, and then advanced image reconstruction approaches are
required to recover high-quality, clinically interpretable images from
undersampled measurements. However, the lack of publicly available cardiac MRI
k-space dataset in terms of both quantity and diversity has severely hindered
substantial technological progress, particularly for data-driven artificial
intelligence. Here, we provide a standardized, diverse, and high-quality
CMRxRecon2024 dataset to facilitate the technical development, fair evaluation,
and clinical transfer of cardiac MRI reconstruction approaches, towards
promoting the universal frameworks that enable fast and robust reconstructions
across different cardiac MRI protocols in clinical practice. To the best of our
knowledge, the CMRxRecon2024 dataset is the largest and most diverse publicly
available cardiac k-space dataset. It is acquired from 330 healthy volunteers,
covering commonly used modalities, anatomical views, and acquisition
trajectories in clinical cardiac MRI workflows. Besides, an open platform with
tutorials, benchmarks, and data processing tools is provided to facilitate data
usage, advanced method development, and fair performance evaluation.

ÊëòË¶ÅÔºöÂøÉËáüÁ£ÅÂÖ±ÊåØÈÄ†ÂΩ± (MRI) Â∑≤ÊàêÁÇ∫Ë®∫Êñ∑ÂøÉËáüÁñæÁóÖÁöÑËá®Â∫äÈáëÊ®ôÊ∫ñÊäÄË°ìÔºåÂõ†ÁÇ∫ÂÆÉËÉΩÂ§†ÈÄèÈÅéÂ§öÁ®ÆÊ®°ÂºèÂíåËß£ÂâñË¶ñÂúñÊèê‰æõÂ§öÊ®£ÂåñÁöÑË≥áË®ä„ÄÇÂä†ÈÄüÂøÉËáü MRI Ê•µÊúâÊúõÂØ¶ÁèæÁúÅÊôÇ‰∏îÂ∞çÊÇ£ËÄÖÂèãÂñÑÁöÑÂΩ±ÂÉèÔºåÁÑ∂ÂæåÈúÄË¶ÅÈÄ≤ÈöéÂΩ±ÂÉèÈáçÂª∫ÊñπÊ≥ïÂæûÊ¨†Êé°Ê®£Ê∏¨Èáè‰∏≠ÈÇÑÂéüÈ´òÂìÅË≥™„ÄÅËá®Â∫ä‰∏äÂèØËß£ËÆÄÁöÑÂΩ±ÂÉè„ÄÇÁÑ∂ËÄåÔºåÂú®Êï∏ÈáèÂíåÂ§öÊ®£ÊÄßÊñπÈù¢Áº∫‰πèÂÖ¨ÈñãÁöÑÂøÉËáü MRI k Á©∫ÈñìË≥áÊñôÈõÜÂö¥ÈáçÈòªÁ§ô‰∫ÜÊäÄË°ìÈÄ≤Â±ïÔºåÁâπÂà•ÊòØÂ∞çÊñºË≥áÊñôÈ©ÖÂãïÁöÑ‰∫∫Â∑•Êô∫ÊÖß„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèê‰æõÊ®ôÊ∫ñÂåñ„ÄÅÂ§öÊ®£Âåñ‰∏îÈ´òÂìÅË≥™ÁöÑ CMRxRecon2024 Ë≥áÊñôÈõÜÔºå‰ª•‰øÉÈÄ≤ÊäÄË°ìÁôºÂ±ï„ÄÅÂÖ¨Ê≠£Ë©ï‰º∞ÂíåÂøÉËáü MRI ÈáçÂª∫ÊñπÊ≥ïÁöÑËá®Â∫äËΩâÁßªÔºåÊúùÂêëÊé®ÂãïÈÄöÁî®Ê°ÜÊû∂Ôºå‰ΩøËá®Â∫äÂØ¶Âãô‰∏≠ÁöÑ‰∏çÂêåÂøÉËáü MRI ÂçîÂÆöËÉΩÂ§†ÈÄ≤Ë°åÂø´ÈÄü‰∏îÁ©©ÂÅ•ÁöÑÈáçÂª∫„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåCMRxRecon2024 Ë≥áÊñôÈõÜÊòØÊúÄÂ§ß‰∏îÊúÄÂ§öÊ®£ÂåñÁöÑÂÖ¨ÈñãÂøÉËáü k Á©∫ÈñìË≥áÊñôÈõÜ„ÄÇÂÆÉÊòØÂæû 330 ‰ΩçÂÅ•Â∫∑ÂøóÈ°òËÄÖÂèñÂæóÔºåÊ∂µËìãËá®Â∫äÂøÉËáü MRI Â∑•‰ΩúÊµÅÁ®ã‰∏≠Â∏∏Áî®ÁöÑÊ®°Âºè„ÄÅËß£ÂâñË¶ñÂúñÂíåÊì∑ÂèñËªåË∑°„ÄÇÊ≠§Â§ñÔºåÊèê‰æõ‰∏ÄÂÄãÂåÖÂê´ÊïôÂ≠∏Ë™≤Á®ã„ÄÅÂü∫Ê∫ñÂíåË≥áÊñôËôïÁêÜÂ∑•ÂÖ∑ÁöÑÈñãÊîæÂπ≥Âè∞Ôºå‰ª•‰øÉÈÄ≤Ë≥áÊñô‰ΩøÁî®„ÄÅÈÄ≤ÈöéÊñπÊ≥ïÈñãÁôºÂíåÂÖ¨Ê≠£ÁöÑÊïàËÉΩË©ï‰º∞„ÄÇ

##### **Lithium-Ion Battery System Health Monitoring and Fault Analysis from Field Data Using Gaussian Processes**
2406.19015v1 by Joachim Schaeffer, Eric Lenz, Duncan Gulla, Martin Z. Bazant, Richard D. Braatz, Rolf Findeisen

Health monitoring, fault analysis, and detection are critical for the safe
and sustainable operation of battery systems. We apply Gaussian process
resistance models on lithium iron phosphate battery field data to effectively
separate the time-dependent and operating point-dependent resistance. The data
set contains 29 battery systems returned to the manufacturer for warranty, each
with eight cells in series, totaling 232 cells and 131 million data rows. We
develop probabilistic fault detection rules using recursive spatiotemporal
Gaussian processes. These processes allow the quick processing of over a
million data points, enabling advanced online monitoring and furthering the
understanding of battery pack failure in the field. The analysis underlines
that often, only a single cell shows abnormal behavior or a knee point,
consistent with weakest-link failure for cells connected in series, amplified
by local resistive heating. The results further the understanding of how
batteries degrade and fail in the field and demonstrate the potential of
efficient online monitoring based on data. We open-source the code and publish
the large data set upon completion of the review of this article.

ÊëòË¶ÅÔºöÂÅ•Â∫∑Áõ£Êéß„ÄÅÊïÖÈöúÂàÜÊûêÂíåÊ™¢Ê∏¨Â∞çÊñºÈõªÊ±†Á≥ªÁµ±ÁöÑÂÆâÂÖ®ÂíåÂèØÊåÅÁ∫åÈÅã‰ΩúËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÂ∞áÈ´òÊñØÈÅéÁ®ãÈõªÈòªÊ®°ÂûãÊáâÁî®ÊñºÁ£∑ÈÖ∏ÈêµÈã∞ÈõªÊ±†ÁèæÂ†¥Êï∏ÊìöÔºå‰ª•ÊúâÊïàÂàÜÈõ¢ÊôÇÈñìÁõ∏ÈóúÂíåÈÅã‰ΩúÈªûÁõ∏ÈóúÁöÑÈõªÈòª„ÄÇË©≤Êï∏ÊìöÈõÜÂåÖÂê´ 29 ÂÄãÈÄÄÂõûÁµ¶Ë£ΩÈÄ†ÂïÜÈÄ≤Ë°å‰øùÂõ∫ÁöÑÈõªÊ±†Á≥ªÁµ±ÔºåÊØèÂÄãÁ≥ªÁµ±ÊúâÂÖ´ÂÄã‰∏≤ËÅØÈõªÊ±†ÔºåÁ∏ΩË®à 232 ÂÄãÈõªÊ±†Âíå 1.31 ÂÑÑÂàóÊï∏Êìö„ÄÇÊàëÂÄë‰ΩøÁî®ÈÅûËø¥ÊôÇÁ©∫È´òÊñØÈÅéÁ®ãÈñãÁôºÂá∫Ê©üÁéáÊÄßÊïÖÈöúÊ™¢Ê∏¨Ë¶èÂâá„ÄÇÈÄô‰∫õÈÅéÁ®ãÂÖÅË®±Âø´ÈÄüËôïÁêÜË∂ÖÈÅé‰∏ÄÁôæËê¨ÂÄãÊï∏ÊìöÈªûÔºåÂØ¶ÁèæÈÄ≤ÈöéÁ∑ö‰∏äÁõ£ÊéßÔºå‰∏¶ÈÄ≤‰∏ÄÊ≠•‰∫ÜËß£ÈõªÊ±†ÁµÑÂú®ÁèæÂ†¥ÁöÑÊïÖÈöú„ÄÇÂàÜÊûêÂº∑Ë™øÔºåÈÄöÂ∏∏Âè™Êúâ‰∏ÄÂÄãÈõªÊ±†È°ØÁ§∫Âá∫Áï∞Â∏∏Ë°åÁÇ∫ÊàñÊãêÈªûÔºåÈÄôËàá‰∏≤ËÅØÈÄ£Êé•ÈõªÊ±†ÁöÑÂº±Áí∞ÊïÖÈöú‰∏ÄËá¥Ôºå‰∏¶Âõ†Â±ÄÈÉ®ÈõªÈòªÂä†ÁÜ±ËÄåÊîæÂ§ß„ÄÇÁµêÊûúÈÄ≤‰∏ÄÊ≠•‰∫ÜËß£ÈõªÊ±†Â¶Ç‰ΩïÂú®ÁèæÂ†¥ÈÄÄÂåñÂíåÊïÖÈöúÔºå‰∏¶Â±ïÁ§∫Âü∫ÊñºÊï∏ÊìöÁöÑÊúâÊïàÁ∑ö‰∏äÁõ£ÊéßÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÂú®ÂÆåÊàêÊú¨ÊñáÂØ©Êü•ÂæåÈñãÊ∫êÁ®ãÂºèÁ¢º‰∏¶ÁôºÂ∏ÉÂ§ßÂûãÊï∏ÊìöÈõÜ„ÄÇ

##### **FedMLP: Federated Multi-Label Medical Image Classification under Task Heterogeneity**
2406.18995v1 by Zhaobin Sun, Nannan Wu, Junjie Shi, Li Yu, Xin Yang, Kwang-Ting Cheng, Zengqiang Yan

Cross-silo federated learning (FL) enables decentralized organizations to
collaboratively train models while preserving data privacy and has made
significant progress in medical image classification. One common assumption is
task homogeneity where each client has access to all classes during training.
However, in clinical practice, given a multi-label classification task,
constrained by the level of medical knowledge and the prevalence of diseases,
each institution may diagnose only partial categories, resulting in task
heterogeneity. How to pursue effective multi-label medical image classification
under task heterogeneity is under-explored. In this paper, we first formulate
such a realistic label missing setting in the multi-label FL domain and propose
a two-stage method FedMLP to combat class missing from two aspects: pseudo
label tagging and global knowledge learning. The former utilizes a warmed-up
model to generate class prototypes and select samples with high confidence to
supplement missing labels, while the latter uses a global model as a teacher
for consistency regularization to prevent forgetting missing class knowledge.
Experiments on two publicly-available medical datasets validate the superiority
of FedMLP against the state-of-the-art both federated semi-supervised and noisy
label learning approaches under task heterogeneity. Code is available at
https://github.com/szbonaldo/FedMLP.

ÊëòË¶ÅÔºöË∑®Ë≥áÊñôÂ∫´ËÅØÈÇ¶Â≠∏Áøí (FL) ËÆìÂàÜÊï£ÂºèÁµÑÁπîËÉΩÂ§†Âú®‰øùÁïôË≥áÊñôÈö±ÁßÅÁöÑÂêåÊôÇÂçî‰ΩúË®ìÁ∑¥Ê®°ÂûãÔºå‰∏¶Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûÊñπÈù¢ÂèñÂæóÈ°ØËëóÈÄ≤Â±ï„ÄÇ‰∏ÄÂÄãÂ∏∏Ë¶ãÁöÑÂÅáË®≠ÊòØ‰ªªÂãôÂêåË≥™ÊÄßÔºåÂÖ∂‰∏≠ÊØèÂÄãÂÆ¢Êà∂Á´ØÂú®Ë®ìÁ∑¥ÊúüÈñìÈÉΩÂèØ‰ª•Â≠òÂèñÊâÄÊúâÈ°ûÂà•„ÄÇÁÑ∂ËÄåÔºåÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÔºåÁµ¶ÂÆö‰∏ÄÂÄãÂ§öÊ®ôÁ±§ÂàÜÈ°û‰ªªÂãôÔºåÂèóÈôêÊñºÈÜ´Â≠∏Áü•Ë≠òÁöÑÂ±§Á¥öÂíåÁñæÁóÖÁöÑÊµÅË°åÁéáÔºåÊØèÂÄãÊ©üÊßãÂèØËÉΩÂè™Ë®∫Êñ∑ÈÉ®ÂàÜÈ°ûÂà•ÔºåÂ∞éËá¥‰ªªÂãôÁï∞Ë≥™ÊÄß„ÄÇÂ¶Ç‰ΩïÂú®‰ªªÂãôÁï∞Ë≥™ÊÄß‰∏ãËøΩÊ±ÇÊúâÊïàÁöÑÂ§öÊ®ôÁ±§ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰ªçÊúâÂæÖÊé¢Ë®é„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÂú®Â§öÊ®ôÁ±§ FL È†òÂüü‰∏≠Âà∂ÂÆöÈÄôÁ®ÆÁèæÂØ¶ÁöÑÊ®ôÁ±§ÈÅ∫Â§±Ë®≠ÂÆöÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂÖ©ÈöéÊÆµÊñπÊ≥ï FedMLP ‰æÜÂæûÂÖ©ÂÄãÊñπÈù¢Ëß£Ê±∫È°ûÂà•ÈÅ∫Â§±ÂïèÈ°åÔºöÂÅΩÊ®ôÁ±§Ê®ôË®òÂíåÂÖ®Â±ÄÁü•Ë≠òÂ≠∏Áøí„ÄÇÂâçËÄÖÂà©Áî®ÁÜ±Ë∫´Ê®°ÂûãÁî¢ÁîüÈ°ûÂà•ÂéüÂûãÔºå‰∏¶ÈÅ∏ÊìáÂÖ∑ÊúâÈ´òÂ∫¶‰ø°ÂøÉÁöÑÊ®£Êú¨‰æÜË£úÂÖÖÈÅ∫Â§±Ê®ôÁ±§ÔºåËÄåÂæåËÄÖ‰ΩøÁî®ÂÖ®Â±ÄÊ®°Âûã‰ΩúÁÇ∫ÊïôÂ∏´ÈÄ≤Ë°å‰∏ÄËá¥ÊÄßÊ≠£ÂâáÂåñÔºå‰ª•Èò≤Ê≠¢ÈÅ∫ÂøòÈÅ∫Â§±È°ûÂà•ÁöÑÁü•Ë≠ò„ÄÇÂú®ÂÖ©ÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑÈÜ´Â≠∏Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óÈ©óË≠â‰∫Ü FedMLP Âú®‰ªªÂãôÁï∞Ë≥™ÊÄß‰∏ãÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑËÅØÂêàÂçäÁõ£Áù£ÂíåÈõúË®äÊ®ôÁ±§Â≠∏ÁøíÊñπÊ≥ï„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/szbonaldo/FedMLP ÂèñÂæó„ÄÇ

##### **Alignment For Performance Improvement in Conversation Bots**
2406.18954v1 by Raghav Garg, Kapil Sharma, Shrey Singla

This paper shows that alignment methods can achieve superior adherence to
guardrails compared to instruction fine-tuning alone in conversational agents,
also known as bots, within predefined guidelines or 'guardrails'. It examines
traditional training approaches such as instruction fine-tuning and the recent
advancements in direct alignment methods like Identity Preference Optimization
(IPO), and Kahneman-Tversky Optimization (KTO). The effectiveness of alignment
techniques both pre and post-instruction tuning is highlighted, illustrating
their potential to optimize conversational bots in domains that require strict
adherence to specified rules, such as customer care.

ÊëòË¶ÅÔºöÊú¨ÊñáÈ°ØÁ§∫ÔºåÂú®Â∞çË©±‰ª£ÁêÜÔºà‰πüÁ®±ÁÇ∫Ê©üÂô®‰∫∫Ôºâ‰∏≠ÔºåËàáÂñÆÁç®Â∞çÊåá‰ª§ÈÄ≤Ë°åÂæÆË™øÁõ∏ÊØîÔºåÂ∞çÈΩäÊñπÊ≥ïÂèØ‰ª•ÂØ¶ÁèæÂ∞çÈò≤Ë≠∑Ê¨ÑÁöÑÊõ¥‰Ω≥ÈÅµÂÆàÔºåÈÄôÂú®È†êÂÆöÁæ©ÁöÑÊ∫ñÂâáÊàñ„ÄåÈò≤Ë≠∑Ê¨Ñ„Äç‰∏≠„ÄÇÂÆÉÊé¢Ë®é‰∫ÜÂÇ≥Áµ±ÁöÑË®ìÁ∑¥ÊñπÊ≥ïÔºå‰æãÂ¶ÇÊåá‰ª§ÂæÆË™øÂíåÁõ¥Êé•Â∞çÈΩäÊñπÊ≥ïÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºå‰æãÂ¶ÇË∫´ÂàÜÂÅèÂ•ΩÊúÄ‰Ω≥Âåñ (IPO) Âíå Kahneman-Tversky ÊúÄ‰Ω≥Âåñ (KTO)„ÄÇÂº∑Ë™ø‰∫ÜÂ∞çÈΩäÊäÄË°ìÂú®Êåá‰ª§Ë™øÊï¥ÂâçÂæåÁöÑÊúâÊïàÊÄßÔºåË™™Êòé‰∫ÜÂÆÉÂÄëÂú®ÈúÄË¶ÅÂö¥Ê†ºÈÅµÂÆàÁâπÂÆöË¶èÂâáÔºà‰æãÂ¶ÇÂÆ¢Êà∂ÊúçÂãôÔºâÁöÑÈ†òÂüü‰∏≠ÊúÄ‰Ω≥ÂåñÂ∞çË©±Ê©üÂô®‰∫∫ÁöÑÊΩõÂäõ„ÄÇ

##### **Correspondence-Free Non-Rigid Point Set Registration Using Unsupervised Clustering Analysis**
2406.18817v1 by Mingyang Zhao, Jingen Jiang, Lei Ma, Shiqing Xin, Gaofeng Meng, Dong-Ming Yan

This paper presents a novel non-rigid point set registration method that is
inspired by unsupervised clustering analysis. Unlike previous approaches that
treat the source and target point sets as separate entities, we develop a
holistic framework where they are formulated as clustering centroids and
clustering members, separately. We then adopt Tikhonov regularization with an
$\ell_1$-induced Laplacian kernel instead of the commonly used Gaussian kernel
to ensure smooth and more robust displacement fields. Our formulation delivers
closed-form solutions, theoretical guarantees, independence from dimensions,
and the ability to handle large deformations. Subsequently, we introduce a
clustering-improved Nystr\"om method to effectively reduce the computational
complexity and storage of the Gram matrix to linear, while providing a rigorous
bound for the low-rank approximation. Our method achieves high accuracy results
across various scenarios and surpasses competitors by a significant margin,
particularly on shapes with substantial deformations. Additionally, we
demonstrate the versatility of our method in challenging tasks such as shape
transfer and medical registration.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈùûÂâõÊÄßÈªûÈõÜÈÖçÊ∫ñÊñπÊ≥ïÔºåÂÖ∂ÈùàÊÑü‰æÜËá™ÁÑ°Áõ£Áù£ËÅöÈ°ûÂàÜÊûê„ÄÇËàáÂ∞áÊ∫êÈªûÈõÜÂíåÁõÆÊ®ôÈªûÈõÜË¶ñÁÇ∫Áç®Á´ãÂØ¶È´îÁöÑÂÖàÂâçÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÊï¥È´îÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠ÂÆÉÂÄëÂàÜÂà•Ë¢´Ë°®Ëø∞ÁÇ∫ËÅöÈ°ûË≥™ÂøÉÂíåËÅöÈ°ûÊàêÂì°„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊé°Áî® Tikhonov Ê≠£ÂâáÂåñÂíå $\ell_1$ Ë™òÂ∞éÁöÑÊãâÊôÆÊãâÊñØÊ†∏ÔºåËÄå‰∏çÊòØÂ∏∏Áî®ÁöÑÈ´òÊñØÊ†∏Ôºå‰ª•Á¢∫‰øùÂπ≥Êªë‰∏îÊõ¥Á©©ÂÅ•ÁöÑ‰ΩçÁßªÂ†¥„ÄÇÊàëÂÄëÁöÑÂÖ¨ÂºèÊèê‰æõ‰∫ÜÈñâÂºèËß£„ÄÅÁêÜË´ñ‰øùË≠â„ÄÅÁç®Á´ãÊñºÁ∂≠Â∫¶‰ª•ÂèäËôïÁêÜÂ§ßËÆäÂΩ¢ÁöÑÁöÑËÉΩÂäõ„ÄÇÈö®ÂæåÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆËÅöÈ°ûÊîπÈÄ≤ÁöÑ Nystr\"om ÊñπÊ≥ïÔºå‰ª•ÊúâÊïàÂú∞Â∞á Gram Áü©Èô£ÁöÑË®àÁÆóË§áÈõúÂ∫¶ÂíåÂ≠òÂÑ≤Á©∫ÈñìÈôç‰ΩéÂà∞Á∑öÊÄßÔºåÂêåÊôÇÁÇ∫‰ΩéÁß©Ëøë‰ººÊèê‰æõÂö¥Ê†ºÁöÑÁïåÈôê„ÄÇÊàëÂÄëÁöÑÁÆóÊ≥ïÂú®ÂêÑÁ®ÆÂ†¥ÊôØ‰∏≠ÈÉΩËÉΩÂèñÂæóÈ´òÁ≤æÂ∫¶ÁöÑÁµêÊûúÔºå‰∏¶‰∏îÂ§ßÂπÖË∂ÖË∂äÁ´∂Áà≠Â∞çÊâãÔºåÁâπÂà•ÊòØÂú®ÂΩ¢ÁãÄËÆäÂΩ¢ÂæàÂ§ßÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®ÂΩ¢ÁãÄËΩâÁßªÂíåÈÜ´Â≠∏ÈÖçÊ∫ñÁ≠âÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô‰∏≠Ë≠âÊòé‰∫ÜÊàëÂÄëÁÆóÊ≥ïÁöÑÂ§öÂäüËÉΩÊÄß„ÄÇ

##### **WavRx: a Disease-Agnostic, Generalizable, and Privacy-Preserving Speech Health Diagnostic Model**
2406.18731v1 by Yi Zhu, Tiago Falk

Speech is known to carry health-related attributes, which has emerged as a
novel venue for remote and long-term health monitoring. However, existing
models are usually tailored for a specific type of disease, and have been shown
to lack generalizability across datasets. Furthermore, concerns have been
raised recently towards the leakage of speaker identity from health embeddings.
To mitigate these limitations, we propose WavRx, a speech health diagnostics
model that captures the respiration and articulation related dynamics from a
universal speech representation. Our in-domain and cross-domain experiments on
six pathological speech datasets demonstrate WavRx as a new state-of-the-art
health diagnostic model. Furthermore, we show that the amount of speaker
identity entailed in the WavRx health embeddings is significantly reduced
without extra guidance during training. An in-depth analysis of the model was
performed, thus providing physiological interpretation of its improved
generalizability and privacy-preserving ability.

ÊëòË¶ÅÔºöË™ûÈü≥Â∑≤Áü•ÊúÉÊâøËºâËàáÂÅ•Â∫∑Áõ∏ÈóúÁöÑÂ±¨ÊÄßÔºåÈÄôÂ∑≤ÊàêÁÇ∫ÈÅ†Ë∑ùÂíåÈï∑ÊúüÂÅ•Â∫∑Áõ£ÊéßÁöÑÊñ∞ÈÄîÂæë„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊ®°ÂûãÈÄöÂ∏∏ÈáùÂ∞çÁâπÂÆöÈ°ûÂûãÁöÑÁñæÁóÖÈáèË∫´ÊâìÈÄ†Ôºå‰∏îÂ∑≤È°ØÁ§∫Âá∫Áº∫‰πèË∑®Ë≥áÊñôÈõÜÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇÊ≠§Â§ñÔºåÊúÄËøëÂ∑≤Â∞çÂæûÂÅ•Â∫∑ÂµåÂÖ•‰∏≠Ê¥©ÊºèË™™Ë©±ËÄÖË∫´ÂàÜÊèêÂá∫ÁñëÊÖÆ„ÄÇÁÇ∫‰∫ÜÊ∏õËºïÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫ WavRxÔºåÈÄôÊòØ‰∏ÄÁ®ÆË™ûÈü≥ÂÅ•Â∫∑Ë®∫Êñ∑Ê®°ÂûãÔºåÂÆÉÊúÉÊì∑Âèñ‰æÜËá™ÈÄöÁî®Ë™ûÈü≥Ë°®Á§∫ÁöÑÂëºÂê∏ÂíåÁôºÈü≥Áõ∏ÈóúÂãïÊÖã„ÄÇÊàëÂÄëÂú®ÂÖ≠ÂÄãÁóÖÁêÜÊÄßË™ûÈü≥Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÈ†òÂüüÂÖßÂíåË∑®È†òÂüüÂØ¶È©óÔºåË≠âÊòé WavRx ÊòØ‰∏ÄÁ®ÆÊñ∞ÁöÑÊúÄÂÖàÈÄ≤ÂÅ•Â∫∑Ë®∫Êñ∑Ê®°Âûã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈ°ØÁ§∫ÔºåÂú®Ë®ìÁ∑¥ÊúüÈñìÊ≤íÊúâÈ°çÂ§ñÁöÑÊåáÂ∞é‰∏ãÔºåWavRx ÂÅ•Â∫∑ÂµåÂÖ•‰∏≠ÂåÖÂê´ÁöÑË™™Ë©±ËÄÖË∫´ÂàÜÊï∏ÈáèÂ∑≤È°ØËëóÊ∏õÂ∞ë„ÄÇÂ∞çÊ®°ÂûãÈÄ≤Ë°å‰∫ÜÊ∑±ÂÖ•ÂàÜÊûêÔºåÂæûËÄåÊèê‰æõ‰∫ÜÂÖ∂ÊîπÂñÑÁöÑÊ¶ÇÊã¨ÊÄßÂíåÈö±ÁßÅ‰øùË≠∑ËÉΩÂäõÁöÑÁîüÁêÜËß£Èáã„ÄÇ

##### **Petal-X: Human-Centered Visual Explanations to Improve Cardiovascular Risk Communication**
2406.18690v1 by Diego Rojo, Houda Lamqaddam, Lucija Gosak, Katrien Verbert

Cardiovascular diseases (CVDs), the leading cause of death worldwide, can be
prevented in most cases through behavioral interventions. Therefore, effective
communication of CVD risk and projected risk reduction by risk factor
modification plays a crucial role in reducing CVD risk at the individual level.
However, despite interest in refining risk estimation with improved prediction
models such as SCORE2, the guidelines for presenting these risk estimations in
clinical practice remained essentially unchanged in the last few years, with
graphical score charts (GSCs) continuing to be one of the prevalent systems.
This work describes the design and implementation of Petal-X, a novel tool to
support clinician-patient shared decision-making by explaining the CVD risk
contributions of different factors and facilitating what-if analysis. Petal-X
relies on a novel visualization, Petal Product Plots, and a tailor-made global
surrogate model of SCORE2, whose fidelity is comparable to that of the GSCs
used in clinical practice. We evaluated Petal-X compared to GSCs in a
controlled experiment with 88 healthcare students, all but one with experience
with chronic patients. The results show that Petal-X outperforms GSC in
critical tasks, such as comparing the contribution to the patient's 10-year CVD
risk of each modifiable risk factor, without a significant loss of perceived
transparency, trust, or intent to use. Our study provides an innovative
approach to the visualization and explanation of risk in clinical practice
that, due to its model-agnostic nature, could continue to support
next-generation artificial intelligence risk assessment models.

ÊëòË¶ÅÔºöÂøÉË°ÄÁÆ°ÁñæÁóÖ (CVD) ÊòØÂÖ®ÁêÉ‰∏ªË¶ÅÁöÑÊ≠ª‰∫°ÂéüÂõ†ÔºåÂú®Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÂèØ‰ª•ÈÄöËøáË°å‰∏∫Âπ≤È¢ÑÊù•È¢ÑÈò≤„ÄÇÂõ†Ê≠§ÔºåÈÄöËøáÊîπÂèòÂç±Èô©Âõ†Â≠êÊù•ÊúâÊïàÊ≤üÈÄö CVD È£éÈô©ÂíåÈ¢ÑÊµãÁöÑÈ£éÈô©Èôç‰ΩéÂú®Èôç‰Ωé‰∏™‰∫∫Â±ÇÈù¢ÁöÑ CVD È£éÈô©‰∏≠Ëµ∑ÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇÁÑ∂ËÄåÔºåÂ∞ΩÁÆ°ÊúâÂÖ¥Ë∂£ÈÄöËøáÊîπËøõÁöÑÈ¢ÑÊµãÊ®°ÂûãÔºà‰æãÂ¶Ç SCORE2ÔºâÊù•‰ºòÂåñÈ£éÈô©‰º∞ËÆ°Ôºå‰ΩÜÂú®‰∏¥Â∫äÂÆûË∑µ‰∏≠ÂëàÁé∞Ëøô‰∫õÈ£éÈô©‰º∞ËÆ°ÁöÑÊåáÂçóÂú®ËøáÂéªÂá†Âπ¥‰∏≠Âü∫Êú¨‰∏ä‰øùÊåÅ‰∏çÂèòÔºåÂõæÂΩ¢ËØÑÂàÜÂõæË°® (GSC) ‰ªçÁÑ∂ÊòØÊµÅË°åÁöÑÁ≥ªÁªü‰πã‰∏Ä„ÄÇËøôÈ°πÂ∑•‰ΩúÊèèËø∞‰∫Ü Petal-X ÁöÑËÆæËÆ°ÂíåÂÆûÁé∞ÔºåPetal-X ÊòØ‰∏ÄÁßçÊñ∞Â∑•ÂÖ∑ÔºåÈÄöËøáËß£Èáä‰∏çÂêåÂõ†Á¥†ÂØπ CVD È£éÈô©ÁöÑË¥°ÁåÆÂπ∂‰øÉËøõÂÅáËÆæÂàÜÊûêÊù•ÊîØÊåÅ‰∏¥Â∫äÂåªÁîüÂíåÊÇ£ËÄÖÁöÑÂÖ±ÂêåÂÜ≥Á≠ñ„ÄÇPetal-X ‰æùËµñ‰∫é‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèØËßÜÂåñÂ∑•ÂÖ∑‚Äî‚ÄîËä±Áì£‰∫ßÂìÅÂõæÔºå‰ª•Âèä SCORE2 ÁöÑÂÆöÂà∂ÂÖ®Â±ÄÊõø‰ª£Ê®°ÂûãÔºåÂÖ∂‰øùÁúüÂ∫¶‰∏é‰∏¥Â∫äÂÆûË∑µ‰∏≠‰ΩøÁî®ÁöÑ GSC Áõ∏ÂΩì„ÄÇÊàë‰ª¨Âú®‰∏Ä‰∏™ÂèóÊéßÂÆûÈ™å‰∏≠ËØÑ‰º∞‰∫Ü Petal-X ‰∏é GSC ÁöÑÂØπÊØîÔºåÂÆûÈ™åÂØπË±°‰∏∫ 88 ÂêçÂåªÂ≠¶ÁîüÔºåÈô§‰∏Ä‰∫∫Â§ñÔºåÊâÄÊúâÂ≠¶ÁîüÈÉΩÊúâÊÖ¢ÊÄßÁóÖÊÇ£ËÄÖÁöÑÁªèÈ™å„ÄÇÁªìÊûúË°®ÊòéÔºåPetal-X Âú®ÂÖ≥ÈîÆ‰ªªÂä°‰∏≠‰ºò‰∫é GSCÔºå‰æãÂ¶ÇÊØîËæÉÂèØÊîπÂèòÂç±Èô©Âõ†Â≠êÂØπÊÇ£ËÄÖ 10 Âπ¥ CVD È£éÈô©ÁöÑË¥°ÁåÆÔºåËÄå‰∏ç‰ºöÊòæÁùÄÈôç‰ΩéÊÑüÁü•ÁöÑÈÄèÊòéÂ∫¶„ÄÅ‰ø°‰ªªÂ∫¶Êàñ‰ΩøÁî®ÊÑèÊÑø„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫Ü‰∏ÄÁßçÂàõÊñ∞ÁöÑÊñπÊ≥ïÊù•ÂèØËßÜÂåñÂíåËß£Èáä‰∏¥Â∫äÂÆûË∑µ‰∏≠ÁöÑÈ£éÈô©ÔºåÁî±‰∫éÂÖ∂‰∏éÊ®°ÂûãÊó†ÂÖ≥ÁöÑÊÄßË¥®ÔºåÂèØ‰ª•ÁªßÁª≠ÊîØÊåÅ‰∏ã‰∏Ä‰ª£‰∫∫Â∑•Êô∫ËÉΩÈ£éÈô©ËØÑ‰º∞Ê®°Âûã„ÄÇ

##### **Stable Diffusion Segmentation for Biomedical Images with Single-step Reverse Process**
2406.18361v2 by Tianyu Lin, Zhiguang Chen, Zhonghao Yan, Weijiang Yu, Fudan Zheng

Diffusion models have demonstrated their effectiveness across various
generative tasks. However, when applied to medical image segmentation, these
models encounter several challenges, including significant resource and time
requirements. They also necessitate a multi-step reverse process and multiple
samples to produce reliable predictions. To address these challenges, we
introduce the first latent diffusion segmentation model, named SDSeg, built
upon stable diffusion (SD). SDSeg incorporates a straightforward latent
estimation strategy to facilitate a single-step reverse process and utilizes
latent fusion concatenation to remove the necessity for multiple samples.
Extensive experiments indicate that SDSeg surpasses existing state-of-the-art
methods on five benchmark datasets featuring diverse imaging modalities.
Remarkably, SDSeg is capable of generating stable predictions with a solitary
reverse step and sample, epitomizing the model's stability as implied by its
name. The code is available at
https://github.com/lin-tianyu/Stable-Diffusion-Seg

ÊëòË¶ÅÔºöÊì¥Êï£Ê®°ÂûãÂ∑≤Ë≠âÊòéÂÖ∂Âú®ÂêÑÁ®ÆÁîüÊàê‰ªªÂãô‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÁÑ∂ËÄåÔºåÁï∂ÊáâÁî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤ÊôÇÔºåÈÄô‰∫õÊ®°ÂûãÊúÉÈÅáÂà∞‰∏Ä‰∫õÊåëÊà∞ÔºåÂåÖÊã¨È°ØËëóÁöÑË≥áÊ∫êÂíåÊôÇÈñìÈúÄÊ±Ç„ÄÇÂÆÉÂÄëÈÇÑÈúÄË¶Å‰∏ÄÂÄãÂ§öÊ≠•È©üÁöÑÂèçÂêëËôïÁêÜÂíåÂ§öÂÄãÊ®£Êú¨ÊâçËÉΩÁî¢ÁîüÂèØÈù†ÁöÑÈ†êÊ∏¨„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÁ¨¨‰∏ÄÂÄãÊΩõÂú®Êì¥Êï£ÂàÜÂâ≤Ê®°ÂûãÔºåÂêçÁÇ∫ SDSegÔºåÂª∫Á´ãÂú®Á©©ÂÆöÊì¥Êï£ (SD) ‰πã‰∏ä„ÄÇSDSeg ÁµêÂêà‰∫Ü‰∏ÄÂÄãÁõ¥Êé•ÁöÑÊΩõÂú®‰º∞Ë®àÁ≠ñÁï•Ôºå‰ª•‰øÉÈÄ≤ÂñÆÊ≠•ÂèçÂêëËôïÁêÜÔºå‰∏¶Âà©Áî®ÊΩõÂú®ËûçÂêà‰∏≤Êé•‰æÜÊ∂àÈô§Â∞çÂ§öÂÄãÊ®£Êú¨ÁöÑÈúÄË¶Å„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåSDSeg Âú®ÂÖ∑Êúâ‰∏çÂêåÂΩ±ÂÉèÊ®°ÂºèÁöÑ‰∫îÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äË∂ÖË∂ä‰∫ÜÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÊñπÊ≥ï„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåSDSeg ËÉΩÂ§†ÈÄöÈÅéÂñÆÁç®ÁöÑÂèçÂêëÊ≠•È©üÂíåÊ®£Êú¨ÁîüÊàêÁ©©ÂÆöÁöÑÈ†êÊ∏¨ÔºåÈÄôÈ´îÁèæ‰∫ÜË©≤Ê®°ÂûãÁöÑÁ©©ÂÆöÊÄßÔºåÊ≠£Â¶ÇÂÖ∂ÂêçÁ®±ÊâÄÊöóÁ§∫ÁöÑÈÇ£Ê®£„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/lin-tianyu/Stable-Diffusion-Seg Áç≤Âæó

##### **Automatic Prediction of Amyotrophic Lateral Sclerosis Progression using Longitudinal Speech Transformer**
2406.18625v1 by Liming Wang, Yuan Gong, Nauman Dawalatabad, Marco Vilela, Katerina Placek, Brian Tracey, Yishu Gong, Alan Premasiri, Fernando Vieira, James Glass

Automatic prediction of amyotrophic lateral sclerosis (ALS) disease
progression provides a more efficient and objective alternative than manual
approaches. We propose ALS longitudinal speech transformer (ALST), a neural
network-based automatic predictor of ALS disease progression from longitudinal
speech recordings of ALS patients. By taking advantage of high-quality
pretrained speech features and longitudinal information in the recordings, our
best model achieves 91.0\% AUC, improving upon the previous best model by 5.6\%
relative on the ALS TDI dataset. Careful analysis reveals that ALST is capable
of fine-grained and interpretable predictions of ALS progression, especially
for distinguishing between rarer and more severe cases. Code is publicly
available.

ÊëòË¶ÅÔºöËÇåËêéÁº©ÊÄß‰æßÁ¥¢Á°¨ÂåñÁóá (ALS) ÁñæÁóÖËøõÂ±ïÁöÑËá™Âä®È¢ÑÊµãÊèê‰æõ‰∫ÜÊØîÊâãÂä®ÊñπÊ≥ïÊõ¥ÊúâÊïà‰∏îÂÆ¢ËßÇÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü ALS Á∫µÂêëËØ≠Èü≥ËΩ¨Êç¢Âô® (ALST)ÔºåËøôÊòØ‰∏ÄÁßçÂü∫‰∫éÁ•ûÁªèÁΩëÁªúÁöÑ ALS ÁñæÁóÖËøõÂ±ïËá™Âä®È¢ÑÊµãÂô®ÔºåÂèØ‰ªé ALS ÊÇ£ËÄÖÁöÑÁ∫µÂêëËØ≠Èü≥ËÆ∞ÂΩï‰∏≠ËøõË°åÈ¢ÑÊµã„ÄÇÈÄöËøáÂà©Áî®ÂΩïÈü≥‰∏≠È´òË¥®ÈáèÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠Èü≥ÁâπÂæÅÂíåÁ∫µÂêë‰ø°ÊÅØÔºåÊàë‰ª¨ÁöÑÊúÄ‰Ω≥Ê®°ÂûãÂÆûÁé∞‰∫Ü 91.0% ÁöÑ AUCÔºåÁõ∏ÂØπ‰∫é ALS TDI Êï∞ÊçÆÈõÜ‰∏äÁöÑÂÖàÂâçÊúÄ‰Ω≥Ê®°ÂûãÊèêÈ´ò‰∫Ü 5.6%„ÄÇ‰ªîÁªÜÂàÜÊûêË°®ÊòéÔºåALST ËÉΩÂ§üÂØπ ALS ËøõÂ±ïËøõË°åÁªÜÁ≤íÂ∫¶‰∏îÂèØËß£ÈáäÁöÑÈ¢ÑÊµãÔºåÂ∞§ÂÖ∂ÊòØÂú®Âå∫ÂàÜÁΩïËßÅÁóÖ‰æãÂíå‰∏•ÈáçÁóÖ‰æãÊñπÈù¢„ÄÇ‰ª£Á†ÅÂ∑≤ÂÖ¨ÂºÄ„ÄÇ

##### **EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction Using Large Language Multimodal Models**
2406.18087v1 by Chun-Chieh Liao, Wei-Ting Kuo, I-Hsuan Hu, Yen-Chen Shih, Jun-En Ding, Feng Liu, Fang-Ming Hung

Traditional diagnosis of chronic diseases involves in-person consultations
with physicians to identify the disease. However, there is a lack of research
focused on predicting and developing application systems using clinical notes
and blood test values. We collected five years of Electronic Health Records
(EHRs) from Taiwan's hospital database between 2017 and 2021 as an AI database.
Furthermore, we developed an EHR-based chronic disease prediction platform
utilizing Large Language Multimodal Models (LLMMs), successfully integrating
with frontend web and mobile applications for prediction. This prediction
platform can also connect to the hospital's backend database, providing
physicians with real-time risk assessment diagnostics. The demonstration link
can be found at https://www.youtube.com/watch?v=oqmL9DEDFgA.

ÊëòË¶ÅÔºöÂÇ≥Áµ±ÊÖ¢ÊÄßÁóÖÁöÑË®∫Êñ∑Ê∂âÂèäË¶™Ëá™Ë´ÆË©¢ÈÜ´Â∏´‰ª•ÊâæÂá∫ÁñæÁóÖ„ÄÇÁÑ∂ËÄåÔºåÁº∫‰πèÈáùÂ∞ç‰ΩøÁî®Ëá®Â∫äÁ≠ÜË®òÂíåË°ÄÊ∂≤Ê™¢È©óÂÄº‰æÜÈ†êÊ∏¨ÂíåÈñãÁôºÊáâÁî®Á≥ªÁµ±ÁöÑÁ†îÁ©∂„ÄÇÊàëÂÄëÂæû2017Âπ¥Âà∞2021Âπ¥ÈñìÊî∂ÈõÜ‰∫ÜÂè∞ÁÅ£ÈÜ´Èô¢Ë≥áÊñôÂ∫´‰∏≠‰∫îÂπ¥ÁöÑÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑÔºàEHRÔºâ‰ΩúÁÇ∫AIË≥áÊñôÂ∫´„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÂü∫ÊñºEHRÁöÑÊÖ¢ÊÄßÁóÖÈ†êÊ∏¨Âπ≥Âè∞ÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÂ§öÊ®°ÊÖãÊ®°ÂûãÔºàLLMMÔºâÔºåÊàêÂäüÊï¥ÂêàÂâçÁ´ØÁ∂≤Ë∑ØÂíåË°åÂãïÊáâÁî®Á®ãÂºèÈÄ≤Ë°åÈ†êÊ∏¨„ÄÇÈÄôÂÄãÈ†êÊ∏¨Âπ≥Âè∞‰πüÂèØ‰ª•ÈÄ£Êé•Âà∞ÈÜ´Èô¢ÁöÑÂæåÁ´ØË≥áÊñôÂ∫´ÔºåÁÇ∫ÈÜ´Â∏´Êèê‰æõÂç≥ÊôÇÈ¢®Èö™Ë©ï‰º∞Ë®∫Êñ∑„ÄÇÁ§∫ÁØÑÈÄ£ÁµêÂèØ‰ª•Âú®https://www.youtube.com/watch?v=oqmL9DEDFgAÊâæÂà∞„ÄÇ

##### **Few-Shot Medical Image Segmentation with High-Fidelity Prototypes**
2406.18074v1 by Song Tang, Shaxu Yan, Xiaozhi Qi, Jianxin Gao, Mao Ye, Jianwei Zhang, Xiatian Zhu

Few-shot Semantic Segmentation (FSS) aims to adapt a pretrained model to new
classes with as few as a single labelled training sample per class. Despite the
prototype based approaches have achieved substantial success, existing models
are limited to the imaging scenarios with considerably distinct objects and not
highly complex background, e.g., natural images. This makes such models
suboptimal for medical imaging with both conditions invalid. To address this
problem, we propose a novel Detail Self-refined Prototype Network (DSPNet) to
constructing high-fidelity prototypes representing the object foreground and
the background more comprehensively. Specifically, to construct global
semantics while maintaining the captured detail semantics, we learn the
foreground prototypes by modelling the multi-modal structures with clustering
and then fusing each in a channel-wise manner. Considering that the background
often has no apparent semantic relation in the spatial dimensions, we integrate
channel-specific structural information under sparse channel-aware regulation.
Extensive experiments on three challenging medical image benchmarks show the
superiority of DSPNet over previous state-of-the-art methods.

ÊëòË¶ÅÔºöÂ∞ëÊ†∑Êú¨ËØ≠‰πâÂàÜÂâ≤ (FSS) Êó®Âú®‰ª•ÊØèÁ±ª‰ªÖ‰∏Ä‰∏™Ê†áËÆ∞ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊñπÂºèÂ∞ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãË∞ÉÊï¥Âà∞Êñ∞Á±ª„ÄÇÂ∞ΩÁÆ°Âü∫‰∫éÂéüÂûãÁöÑÂäûÊ≥ïÂ∑≤ÂèñÂæóÈáçÂ§ßÊàêÂäüÔºå‰ΩÜÁé∞ÊúâÊ®°Âûã‰ªÖÈôê‰∫éÂØπË±°ÊòéÊòæ‰∏çÂêå‰∏îËÉåÊôØ‰∏çÂ§™Â§çÊùÇÁöÑÊàêÂÉèÂú∫ÊôØÔºå‰æãÂ¶ÇËá™ÁÑ∂ÂõæÂÉè„ÄÇËøô‰ΩøÂæóÊ≠§Á±ªÊ®°Âûã‰∏çÈÄÇÁî®‰∫éÂêåÊó∂‰∏çÊª°Ë∂≥Ëøô‰∏§‰∏™Êù°‰ª∂ÁöÑÂåªÂ≠¶ÂΩ±ÂÉè„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁªÜËäÇËá™Á≤æÁÇºÂéüÂûãÁΩëÁªú (DSPNet)Ôºå‰ª•ÊûÑÂª∫È´ò‰øùÁúüÂéüÂûãÔºåÊõ¥ÂÖ®Èù¢Âú∞Ë°®Á§∫ÂØπË±°ÂâçÊôØÂíåËÉåÊôØ„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰∏∫‰∫ÜÂú®‰øùÊåÅÊçïËé∑ÁöÑÁªÜËäÇËØ≠‰πâÁöÑÂêåÊó∂ÊûÑÂª∫ÂÖ®Â±ÄËØ≠‰πâÔºåÊàë‰ª¨ÈÄöËøá‰ΩøÁî®ËÅöÁ±ªÂØπÂ§öÊ®°ÊÄÅÁªìÊûÑËøõË°åÂª∫Ê®°ÔºåÁÑ∂Âêé‰ª•ÈÄêÈÄöÈÅìÁöÑÊñπÂºèËûçÂêàÊØè‰∏™ÁªìÊûÑÔºå‰ªéËÄåÂ≠¶‰π†ÂâçÊôØÂéüÂûã„ÄÇËÄÉËôëÂà∞ËÉåÊôØÂú®Á©∫Èó¥Áª¥Â∫¶‰∏äÈÄöÂ∏∏Ê≤°ÊúâÊòéÊòæÁöÑËØ≠‰πâÂÖ≥Á≥ªÔºåÊàë‰ª¨Âú®Á®ÄÁñèÈÄöÈÅìÊÑüÁü•Ë∞ÉËäÇ‰∏ãÊï¥ÂêàÁâπÂÆö‰∫éÈÄöÈÅìÁöÑÁªìÊûÑ‰ø°ÊÅØ„ÄÇÂú®‰∏â‰∏™ÊûÅÂÖ∑ÊåëÊàòÊÄßÁöÑÂåªÂ≠¶ÂΩ±ÂÉèÂü∫ÂáÜ‰∏äËøõË°åÁöÑÂπøÊ≥õÂÆûÈ™åË°®ÊòéÔºåDSPNet ‰ºò‰∫é‰ª•ÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ

##### **Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources**
2406.18049v1 by Yiming Li, Deepthi Viswaroopan, William He, Jianfu Li, Xu Zuo, Hua Xu, Cui Tao

Adverse event (AE) extraction following COVID-19 vaccines from text data is
crucial for monitoring and analyzing the safety profiles of immunizations.
Traditional deep learning models are adept at learning intricate feature
representations and dependencies in sequential data, but often require
extensive labeled data. In contrast, large language models (LLMs) excel in
understanding contextual information, but exhibit unstable performance on named
entity recognition tasks, possibly due to their broad but unspecific training.
This study aims to evaluate the effectiveness of LLMs and traditional deep
learning models in AE extraction, and to assess the impact of ensembling these
models on performance. In this study, we utilized reports and posts from the
VAERS (n=621), Twitter (n=9,133), and Reddit (n=131) as our corpora. Our goal
was to extract three types of entities: "vaccine", "shot", and "ae". We
explored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5,
GPT-4, and Llama-2, as well as traditional deep learning models like RNN and
BioBERT. To enhance performance, we created ensembles of the three models with
the best performance. For evaluation, we used strict and relaxed F1 scores to
evaluate the performance for each entity type, and micro-average F1 was used to
assess the overall performance. The ensemble model achieved the highest
performance in "vaccine", "shot", and "ae" with strict F1-scores of 0.878,
0.930, and 0.925, respectively, along with a micro-average score of 0.903. In
conclusion, this study demonstrates the effectiveness and robustness of
ensembling fine-tuned traditional deep learning models and LLMs, for extracting
AE-related information. This study contributes to the advancement of biomedical
natural language processing, providing valuable insights into improving AE
extraction from text data for pharmacovigilance and public health surveillance.

ÊëòË¶ÅÔºöÂæûÊñáÊú¨Ë≥áÊñô‰∏≠Êì∑Âèñ COVID-19 Áñ´ËãóÁöÑ‰∏çËâØ‰∫ã‰ª∂ (AE) Â∞çÊñºÁõ£ÊéßÂíåÂàÜÊûêÂÖçÁñ´ÁöÑÂÆâÂÖ®ÊÄßÈùûÂ∏∏ÈáçË¶Å„ÄÇÂÇ≥Áµ±Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÊìÖÈï∑Â≠∏ÁøíÂ∫èÂàóË≥áÊñô‰∏≠ÁöÑË§áÈõúÁâπÂæµË°®Á§∫Âíå‰æùË≥¥Èóú‰øÇÔºå‰ΩÜÈÄöÂ∏∏ÈúÄË¶ÅÂ§ßÈáèÁöÑÊ®ôÁ±§Ë≥áÊñô„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊìÖÈï∑ÁêÜËß£‰∏ä‰∏ãÊñáË≥áË®äÔºå‰ΩÜÂú®ÂëΩÂêçÂØ¶È´îË≠òÂà•‰ªªÂãô‰∏äÁöÑË°®Áèæ‰∏çÁ©©ÂÆöÔºåÈÄôÂèØËÉΩÊòØÂõ†ÁÇ∫ÂÆÉÂÄëÁöÑË®ìÁ∑¥ÁØÑÂúçÂª£Ê≥õ‰ΩÜÁº∫‰πèÈáùÂ∞çÊÄß„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Ë©ï‰º∞ LLM ÂíåÂÇ≥Áµ±Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂú® AE Êì∑Âèñ‰∏≠ÁöÑÊúâÊïàÊÄßÔºå‰∏¶Ë©ï‰º∞Â∞áÈÄô‰∫õÊ®°ÂûãÁµÑÊàêÁöÑÂΩ±Èüø„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂà©Áî® VAERS (n=621)„ÄÅTwitter (n=9,133) Âíå Reddit (n=131) ÁöÑÂ†±ÂëäÂíåÊñáÁ´†‰ΩúÁÇ∫Ë™ûÊñôÂ∫´„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÊì∑Âèñ‰∏âÁ®ÆÈ°ûÂûãÁöÑÂØ¶È´îÔºö„ÄåÁñ´Ëãó„Äç„ÄÅ„ÄåÊ≥®Â∞Ñ„ÄçÂíå„Äå‰∏çËâØ‰∫ã‰ª∂„Äç„ÄÇÊàëÂÄëÊé¢Á¥¢‰∏¶ÂæÆË™ø‰∫ÜÂ§öÂÄã LLMÔºåÂåÖÊã¨ GPT-2„ÄÅGPT-3.5„ÄÅGPT-4 Âíå Llama-2Ôºå‰ª•ÂèäÂÇ≥Áµ±Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰æãÂ¶Ç RNN Âíå BioBERTÔºàGPT-4 Èô§Â§ñÔºâ„ÄÇÁÇ∫‰∫ÜÊèêÂçáÊïàËÉΩÔºåÊàëÂÄëÂª∫Á´ã‰∫ÜË°®ÁèæÊúÄ‰Ω≥ÁöÑ‰∏âÂÄãÊ®°ÂûãÁöÑÈõÜÂêà„ÄÇÂú®Ë©ï‰º∞ÊñπÈù¢ÔºåÊàëÂÄë‰ΩøÁî®Âö¥Ê†ºÂíåÊîæÂØ¨ÁöÑ F1 ÂàÜÊï∏‰æÜË©ï‰º∞ÊØèÂÄãÂØ¶È´îÈ°ûÂûãÁöÑÊïàËÉΩÔºå‰∏¶‰ΩøÁî®ÂæÆÂπ≥Âùá F1 ‰æÜË©ï‰º∞Êï¥È´îÊïàËÉΩ„ÄÇÁµÑÂêàÊ®°ÂûãÂú®„ÄåÁñ´Ëãó„Äç„ÄÅ„ÄåÊ≥®Â∞Ñ„ÄçÂíå„Äå‰∏çËâØ‰∫ã‰ª∂„Äç‰∏≠ÂàÜÂà•‰ª• 0.878„ÄÅ0.930 Âíå 0.925 ÁöÑÂö¥Ê†º F1 ÂàÜÊï∏Áç≤ÂæóÊúÄÈ´òÊïàËÉΩÔºåÂæÆÂπ≥ÂùáÂàÜÊï∏ÁÇ∫ 0.903„ÄÇÁµêË´ñÊòØÔºåÊú¨Á†îÁ©∂Ë≠âÊòé‰∫ÜÂæÆË™øÂæåÁöÑÂÇ≥Áµ±Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂíå LLM ÁöÑÈõÜÂêàÂú®Êì∑ÂèñËàá AE Áõ∏ÈóúÁöÑË≥áË®äÊñπÈù¢ÁöÑÊúâÊïàÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇÊú¨Á†îÁ©∂ÊúâÂä©Êñº‰øÉÈÄ≤ÁîüÁâ©ÈÜ´Â≠∏Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÔºå‰∏¶Êèê‰æõÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰ª•ÊîπÂñÑËó•Áâ©Ë≠¶ÊàíÂíåÂÖ¨ÂÖ±Ë°õÁîüÁõ£Ê∏¨‰∏≠ÂæûÊñáÊú¨Ë≥áÊñô‰∏≠Êì∑Âèñ AE ÁöÑÊñπÂºè„ÄÇ

##### **Automated Clinical Data Extraction with Knowledge Conditioned LLMs**
2406.18027v1 by Diya Li, Asim Kadav, Aijing Gao, Rui Li, Richard Bourgon

The extraction of lung lesion information from clinical and medical imaging
reports is crucial for research on and clinical care of lung-related diseases.
Large language models (LLMs) can be effective at interpreting unstructured text
in reports, but they often hallucinate due to a lack of domain-specific
knowledge, leading to reduced accuracy and posing challenges for use in
clinical settings. To address this, we propose a novel framework that aligns
generated internal knowledge with external knowledge through in-context
learning (ICL). Our framework employs a retriever to identify relevant units of
internal or external knowledge and a grader to evaluate the truthfulness and
helpfulness of the retrieved internal-knowledge rules, to align and update the
knowledge bases. Our knowledge-conditioned approach also improves the accuracy
and reliability of LLM outputs by addressing the extraction task in two stages:
(i) lung lesion finding detection and primary structured field parsing,
followed by (ii) further parsing of lesion description text into additional
structured fields. Experiments with expert-curated test datasets demonstrate
that this ICL approach can increase the F1 score for key fields (lesion size,
margin and solidity) by an average of 12.9% over existing ICL methods.

ÊëòË¶ÅÔºöÂæûËá®Â∫äÂíåÈÜ´Â≠∏ÂΩ±ÂÉèÂ†±Âëä‰∏≠ËêÉÂèñËÇ∫ÈÉ®ÁóÖÁÅ∂Ë≥áË®äÂ∞çÊñºËÇ∫ÈÉ®Áõ∏ÈóúÁñæÁóÖÁöÑÁ†îÁ©∂ÂíåËá®Â∫äÁÖßË≠∑Ëá≥ÈóúÈáçË¶Å„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂèØ‰ª•ÊúâÊïàËß£ËÆÄÂ†±Âëä‰∏≠ÁöÑÈùûÁµêÊßãÂåñÊñáÂ≠óÔºå‰ΩÜÁî±ÊñºÁº∫‰πèÁâπÂÆöÈ†òÂüüÁü•Ë≠òÔºåÂÆÉÂÄëÁ∂ìÂ∏∏ÊúÉÂá∫ÁèæÂπªË¶∫ÔºåÂ∞éËá¥Ê∫ñÁ¢∫Â∫¶Èôç‰ΩéÔºå‰∏¶Â∞çÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠‰ΩøÁî®Â∏∂‰æÜÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÈÄèÈÅéËÑàÁµ°‰∏≠Â≠∏Áøí (ICL) Â∞áÁî¢ÁîüÁöÑÂÖßÈÉ®Áü•Ë≠òËàáÂ§ñÈÉ®Áü•Ë≠òÂ∞çÈΩä„ÄÇÊàëÂÄëÁöÑÊ°ÜÊû∂Êé°Áî®Ê™¢Á¥¢Âô®‰æÜË≠òÂà•ÂÖßÈÉ®ÊàñÂ§ñÈÉ®Áü•Ë≠òÁöÑÁõ∏ÂÖ≥ÂñÆÂÖÉÔºå‰∏¶Êé°Áî®Ë©ïÂàÜÂô®‰æÜË©ï‰º∞Ê™¢Á¥¢Âà∞ÁöÑÂÖßÈÉ®Áü•Ë≠òË¶èÂâáÁöÑÁúüÂØ¶ÊÄßÂíåÊúâÁõäÊÄßÔºå‰ª•Â∞çÈΩäÂíåÊõ¥Êñ∞Áü•Ë≠òÂ∫´„ÄÇÊàëÂÄë‰ª•Áü•Ë≠òÁÇ∫Ê¢ù‰ª∂ÁöÑÊñπÊ≥ï‰πüÈÄèÈÅé‰ª•‰∏ãÂÖ©ÂÄãÈöéÊÆµ‰æÜËôïÁêÜËêÉÂèñ‰ªªÂãôÔºåÈÄ≤ËÄåÊèêÈ´ò LLM Ëº∏Âá∫ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄßÔºö(i) ËÇ∫ÈÉ®ÁóÖÁÅ∂ÁôºÁèæÂÅµÊ∏¨Âíå‰∏ªË¶ÅÁµêÊßãÂåñÊ¨Ñ‰ΩçÂàÜÊûêÔºåÊé•ËëóÊòØ (ii) ÈÄ≤‰∏ÄÊ≠•Â∞áÁóÖÁÅ∂ÊèèËø∞ÊñáÂ≠óÂàÜÊûêÁÇ∫ÂÖ∂‰ªñÁµêÊßãÂåñÊ¨Ñ‰Ωç„ÄÇ‰ΩøÁî®Â∞àÂÆ∂Á≠ñÂ±ïÁöÑÊ∏¨Ë©¶Ë≥áÊñôÈõÜÈÄ≤Ë°åÁöÑÂØ¶È©óË≠âÊòéÔºåÈÄôÂÄã ICL ÊñπÊ≥ïÂèØ‰ª•Â∞áÈóúÈçµÊ¨Ñ‰Ωç (ÁóÖÁÅ∂Â§ßÂ∞è„ÄÅÈÇäÁ∑£ÂíåÂØ¶ÂøÉÂ∫¶) ÁöÑ F1 ÂàÜÊï∏Âπ≥ÂùáÊèêÈ´ò 12.9%ÔºåÂÑ™ÊñºÁèæÊúâÁöÑ ICL ÊñπÊ≥ï„ÄÇ

##### **AutoOPE: Automated Off-Policy Estimator Selection**
2406.18022v1 by Nicol√≤ Felicioni, Michael Benigni, Maurizio Ferrari Dacrema

The Off-Policy Evaluation (OPE) problem consists of evaluating the
performance of counterfactual policies with data collected by another one. This
problem is of utmost importance for various application domains, e.g.,
recommendation systems, medical treatments, and many others. To solve the OPE
problem, we resort to estimators, which aim to estimate in the most accurate
way possible the performance that the counterfactual policies would have had if
they were deployed in place of the logging policy. In the literature, several
estimators have been developed, all with different characteristics and
theoretical guarantees. Therefore, there is no dominant estimator, and each
estimator may be the best one for different OPE problems, depending on the
characteristics of the dataset at hand. While the selection of the estimator is
a crucial choice for an accurate OPE, this problem has been widely overlooked
in the literature. We propose an automated data-driven OPE estimator selection
method based on machine learning. In particular, the core idea we propose in
this paper is to create several synthetic OPE tasks and use a machine learning
model trained to predict the best estimator for those synthetic tasks. We
empirically show how our method is able to generalize to unseen tasks and make
a better estimator selection compared to a baseline method on several
real-world datasets, with a computational cost significantly lower than the one
of the baseline.

ÊëòË¶ÅÔºöÈõ¢Á∑öÁ≠ñÁï•Ë©ï‰º∞ (OPE) ÂïèÈ°åÂåÖÂê´‰ΩøÁî®Áî±ÂÖ∂‰ªñÊîøÁ≠ñÊî∂ÈõÜÁöÑË≥áÊñôË©ï‰º∞Âèç‰∫ãÂØ¶ÊîøÁ≠ñÁöÑÊïàËÉΩ„ÄÇÊ≠§ÂïèÈ°åÂ∞çÊñºÂêÑÁ®ÆÊáâÁî®È†òÂüüËá≥ÈóúÈáçË¶ÅÔºå‰æãÂ¶ÇÊé®Ëñ¶Á≥ªÁµ±„ÄÅÈÜ´ÁôÇÊ≤ªÁôÇÁ≠â„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ OPE ÂïèÈ°åÔºåÊàëÂÄëÊ±ÇÂä©Êñº‰º∞Ë®àÂô®ÔºåÂÖ∂ÁõÆÊ®ôÊòØ‰ª•ÊúÄÁ≤æÁ¢∫ÁöÑÊñπÂºè‰º∞Ë®àÂèç‰∫ãÂØ¶ÊîøÁ≠ñÂú®ÈÉ®ÁΩ≤ÊñºË®òÈåÑÊîøÁ≠ñÊôÇÊâÄÂÖ∑ÂÇôÁöÑÊïàËÉΩ„ÄÇÂú®ÊñáÁçª‰∏≠ÔºåÂ∑≤Á∂ìÈñãÁôºÂá∫Â§öÂÄã‰º∞Ë®àÂô®ÔºåÊØèÂÄã‰º∞Ë®àÂô®ÈÉΩÂÖ∑Êúâ‰∏çÂêåÁöÑÁâπÊÄßÂíåÁêÜË´ñ‰øùË≠â„ÄÇÂõ†Ê≠§ÔºåÊ≤íÊúâ‰∏ªÂ∞é‰º∞Ë®àÂô®ÔºåÊØèÂÄã‰º∞Ë®àÂô®ÂèØËÉΩÊòØ‰∏çÂêå OPE ÂïèÈ°åÁöÑÊúÄ‰Ω≥‰º∞Ë®àÂô®ÔºåÂÖ∑È´îÂèñÊ±∫ÊñºÊâãÈÇäË≥áÊñôÈõÜÁöÑÁâπÂæµ„ÄÇÈõñÁÑ∂‰º∞Ë®àÂô®ÁöÑÈÅ∏ÊìáÂ∞çÊñºÊ∫ñÁ¢∫ÁöÑ OPE Ëá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÈÄôÂÄãÂïèÈ°åÂú®ÊñáÁçª‰∏≠Â∑≤Ë¢´Âª£Ê≥õÂøΩË¶ñ„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑËá™ÂãïÂåñË≥áÊñôÈ©ÖÂãï OPE ‰º∞Ë®àÂô®ÈÅ∏ÊìáÊñπÊ≥ï„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÂú®ÈÄôÁØáË´ñÊñá‰∏≠ÊèêÂá∫ÁöÑÊ†∏ÂøÉÊ¶ÇÂøµÊòØÂª∫Á´ãÂπæÂÄãÂêàÊàê OPE ‰ªªÂãôÔºå‰∏¶‰ΩøÁî®Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãË®ìÁ∑¥‰æÜÈ†êÊ∏¨ÈÄô‰∫õÂêàÊàê‰ªªÂãôÁöÑÊúÄ‰Ω≥‰º∞Ë®àÂô®„ÄÇÊàëÂÄë‰ª•ÂØ¶Ë≠âÊñπÂºèË™™ÊòéÊàëÂÄëÁöÑÊ®°ÂûãÂ¶Ç‰ΩïËÉΩÂ§†Ê¶ÇÂåñÁÇ∫Êú™Ë¶ã‰ªªÂãôÔºå‰∏¶Âú®ÂπæÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äËàáÂü∫Á∑öÊñπÊ≥ïÁõ∏ÊØîÔºåÂÅöÂá∫Êõ¥Â•ΩÁöÑ‰º∞Ë®àÂô®ÈÅ∏ÊìáÔºå‰∏îÈÅãÁÆóÊàêÊú¨ÈÅ†‰ΩéÊñºÂü∫Á∑öÊñπÊ≥ï„ÄÇ

##### **Multi-step Knowledge Retrieval and Inference over Unstructured Data**
2406.17987v1 by Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, CJ McFate, Lori Moon, Nati Seifu, Maksim Eremeev, Jose Barrera, Eric Brown, David Ferrucci

The advent of Large Language Models (LLMs) and Generative AI has
revolutionized natural language applications across various domains. However,
high-stakes decision-making tasks in fields such as medical, legal and finance
require a level of precision, comprehensiveness, and logical consistency that
pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to
deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI
platform to tackle these problems. The platform integrates fine-tuned LLMs for
knowledge extraction and alignment with a robust symbolic reasoning engine for
logical inference, planning and interactive constraint solving. We describe
Cora, a Collaborative Research Assistant built on this platform, that is
designed to perform complex research and discovery tasks in high-stakes
domains. This paper discusses the multi-step inference challenges inherent in
such domains, critiques the limitations of existing LLM-based methods, and
demonstrates how Cora's neuro-symbolic approach effectively addresses these
issues. We provide an overview of the system architecture, key algorithms for
knowledge extraction and formal reasoning, and present preliminary evaluation
results that highlight Cora's superior performance compared to well-known LLM
and RAG baselines.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁîüÊàêÂºè AI ÁöÑÂá∫ÁèæÂæπÂ∫ïÊîπËÆä‰∫ÜÂêÑÂÄãÈ†òÂüüÁöÑËá™ÁÑ∂Ë™ûË®ÄÊáâÁî®„ÄÇÁÑ∂ËÄåÔºåÈÜ´ÁôÇ„ÄÅÊ≥ïÂæãÂíåÈáëËûçÁ≠âÈ†òÂüüÁöÑÈ´òÈ¢®Èö™Ê±∫Á≠ñÂà∂ÂÆö‰ªªÂãôÈúÄË¶ÅÁ≤æÁ¢∫Â∫¶„ÄÅÂÖ®Èù¢ÊÄßÂíåÈÇèËºØ‰∏ÄËá¥ÊÄßÔºåËÄåÁ¥îÁ≤πÁöÑ LLM ÊàñÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊñπÊ≥ïÈÄöÂ∏∏ÁÑ°Ê≥ïÊèê‰æõ„ÄÇÂú® Elemental Cognition (EC)ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÁ•ûÁ∂ìÁ¨¶Ëôü AI Âπ≥Âè∞‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇË©≤Âπ≥Âè∞Êï¥Âêà‰∫ÜÁ∂ìÈÅéÂæÆË™øÁöÑ LLMÔºåÁî®ÊñºÁü•Ë≠òÊèêÂèñÂíåËàáÂº∑Â§ßÁöÑÁ¨¶ËôüÊé®ÁêÜÂºïÊìéÂ∞çÈΩäÔºåÁî®ÊñºÈÇèËºØÊé®ÁêÜ„ÄÅË¶èÂäÉÂíå‰∫íÂãïÁ¥ÑÊùüÊ±ÇËß£„ÄÇÊàëÂÄëÊèèËø∞‰∫Ü CoraÔºå‰∏ÄÂÄãÂª∫Á´ãÂú®ÈÄôÂÄãÂπ≥Âè∞‰∏äÁöÑÂçî‰ΩúÁ†îÁ©∂Âä©ÁêÜÔºåÂÆÉË¢´Ë®≠Ë®àÁî®ÊñºÂú®È´òÈ¢®Èö™È†òÂüüÂü∑Ë°åË§áÈõúÁöÑÁ†îÁ©∂ÂíåÁôºÁèæ‰ªªÂãô„ÄÇÊú¨ÊñáË®éË´ñ‰∫ÜÊ≠§È°ûÈ†òÂüü‰∏≠Âõ∫ÊúâÁöÑÂ§öÊ≠•È©üÊé®ÁêÜÊåëÊà∞ÔºåÊâπË©ï‰∫ÜÁèæÊúâÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºå‰∏¶Â±ïÁ§∫‰∫Ü Cora ÁöÑÁ•ûÁ∂ìÁ¨¶ËôüÊñπÊ≥ïÂ¶Ç‰ΩïÊúâÊïàÂú∞Ëß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÊàëÂÄëÊ¶ÇËø∞‰∫ÜÁ≥ªÁµ±Êû∂Êßã„ÄÅÁü•Ë≠òÊèêÂèñÂíåÂΩ¢ÂºèÊé®ÁêÜÁöÑÈóúÈçµÊºîÁÆóÊ≥ïÔºå‰∏¶Êèê‰æõ‰∫ÜÂàùÊ≠•Ë©ï‰º∞ÁµêÊûúÔºåÁ™ÅÂá∫‰∫Ü Cora ËàáÁúæÊâÄÂë®Áü•ÁöÑ LLM Âíå RAG Âü∫Ê∫ñÁõ∏ÊØîÁöÑÂÑ™Áï∞ÊÄßËÉΩ„ÄÇ

##### **Domain Adaptation of Echocardiography Segmentation Via Reinforcement Learning**
2406.17902v1 by Arnaud Judge, Thierry Judge, Nicolas Duchateau, Roman A. Sandler, Joseph Z. Sokol, Olivier Bernard, Pierre-Marc Jodoin

Performance of deep learning segmentation models is significantly challenged
in its transferability across different medical imaging domains, particularly
when aiming to adapt these models to a target domain with insufficient
annotated data for effective fine-tuning. While existing domain adaptation (DA)
methods propose strategies to alleviate this problem, these methods do not
explicitly incorporate human-verified segmentation priors, compromising the
potential of a model to produce anatomically plausible segmentations. We
introduce RL4Seg, an innovative reinforcement learning framework that reduces
the need to otherwise incorporate large expertly annotated datasets in the
target domain, and eliminates the need for lengthy manual human review. Using a
target dataset of 10,000 unannotated 2D echocardiographic images, RL4Seg not
only outperforms existing state-of-the-art DA methods in accuracy but also
achieves 99% anatomical validity on a subset of 220 expert-validated subjects
from the target domain. Furthermore, our framework's reward network offers
uncertainty estimates comparable with dedicated state-of-the-art uncertainty
methods, demonstrating the utility and effectiveness of RL4Seg in overcoming
domain adaptation challenges in medical image segmentation.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÂàÜÂâ≤Ê®°ÂûãÁöÑÊïàËÉΩÔºåÂú®‰∏çÂêåÈÜ´Â≠∏ÂΩ±ÂÉèÈ†òÂüüÁöÑËΩâÁßªÊÄß‰∏äÂèóÂà∞È°ØËëóÁöÑÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÁõÆÊ®ôÈ†òÂüüÁöÑÈÅ©ÊáâÔºåË≥áÊñô‰∏çË∂≥‰ª•ÈÄ≤Ë°åÊúâÊïàÂæÆË™øÊôÇ„ÄÇÁèæÊúâÁöÑÈ†òÂüüÈÅ©Êáâ (DA) ÊñπÊ≥ïÊèêÂá∫Á≠ñÁï•‰æÜÁ∑©Ëß£Ê≠§ÂïèÈ°åÔºå‰ΩÜÈÄô‰∫õÊñπÊ≥ï‰∏¶Êú™ÊòéÁ¢∫Á¥çÂÖ•‰∫∫ÁÇ∫È©óË≠âÁöÑÂàÜÂâ≤ÂÖàÈ©óÔºåÈÄôÊúÉÂΩ±ÈüøÊ®°ÂûãÁî¢ÁîüËß£ÂâñÂ≠∏‰∏äÂêàÁêÜÁöÑÂàÜÂâ≤ÁöÑÂèØËÉΩÊÄß„ÄÇÊàëÂÄëÂºïÈÄ≤ RL4SegÔºå‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂº∑ÂåñÂ≠∏ÁøíÊû∂ÊßãÔºåÂÆÉÊ∏õÂ∞ë‰∫ÜÂú®ÁõÆÊ®ôÈ†òÂüü‰∏≠Á¥çÂÖ•Â§ßÈáèÂ∞àÂÆ∂Ë®ªËß£Ë≥áÊñôÈõÜÁöÑÈúÄÊ±ÇÔºå‰∏¶Ê∂àÈô§‰∫ÜÂÜóÈï∑ÁöÑ„ÄÅÊâãÂãïÁöÑ‰∫∫Â∑•ÂØ©Êü•ÈúÄÊ±Ç„ÄÇ‰ΩøÁî®ÂåÖÂê´ 10,000 ÂÄãÊú™Ë®ªËß£ÁöÑ 2D Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÁöÑÁõÆÊ®ôË≥áÊñôÈõÜÔºåRL4Seg ‰∏çÂÉÖÂú®Ê∫ñÁ¢∫ÊÄß‰∏äÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ÁöÑ DA ÊñπÊ≥ïÔºåËÄå‰∏îÂú®ÁõÆÊ®ôÈ†òÂüü‰∏≠Ôºå220 ÂÄãÂ∞àÂÆ∂È©óË≠âÁöÑÂèóË©¶ËÄÖÁöÑÂ≠êÈõÜ‰∏≠ÔºåËß£ÂâñÂ≠∏ÊïàÂ∫¶ÈÅîÂà∞ 99%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊû∂ÊßãÁöÑÁçéÂãµÁ∂≤Ë∑ØÊèê‰æõ‰∫ÜËàáÂ∞àÈñÄÁöÑÊúÄÂÖàÈÄ≤ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÊñπÊ≥ïÁõ∏Áï∂ÁöÑ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÔºåÈÄôË≠âÊòé‰∫Ü RL4Seg Âú®ÂÖãÊúçÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤‰∏≠ÁöÑÈ†òÂüüÈÅ©ÊáâÊåëÊà∞ÊñπÈù¢ÁöÑÊïàÁî®ÂíåÊúâÊïàÊÄß„ÄÇ

##### **CTBench: A Comprehensive Benchmark for Evaluating Language Model Capabilities in Clinical Trial Design**
2406.17888v1 by Nafis Neehal, Bowen Wang, Shayom Debopadhaya, Soham Dan, Keerthiram Murugesan, Vibha Anand, Kristin P. Bennett

CTBench is introduced as a benchmark to assess language models (LMs) in
aiding clinical study design. Given study-specific metadata, CTBench evaluates
AI models' ability to determine the baseline features of a clinical trial (CT),
which include demographic and relevant features collected at the trial's start
from all participants. These baseline features, typically presented in CT
publications (often as Table 1), are crucial for characterizing study cohorts
and validating results. Baseline features, including confounders and
covariates, are also necessary for accurate treatment effect estimation in
studies involving observational data. CTBench consists of two datasets:
"CT-Repo," containing baseline features from 1,690 clinical trials sourced from
clinicaltrials.gov, and "CT-Pub," a subset of 100 trials with more
comprehensive baseline features gathered from relevant publications. Two
LM-based evaluation methods are developed to compare the actual baseline
feature lists against LM-generated responses. "ListMatch-LM" and
"ListMatch-BERT" use GPT-4o and BERT scores (at various thresholds),
respectively, for evaluation. To establish baseline results, advanced prompt
engineering techniques using LLaMa3-70B-Instruct and GPT-4o in zero-shot and
three-shot learning settings are applied to generate potential baseline
features. The performance of GPT-4o as an evaluator is validated through
human-in-the-loop evaluations on the CT-Pub dataset, where clinical experts
confirm matches between actual and LM-generated features. The results highlight
a promising direction with significant potential for improvement, positioning
CTBench as a useful tool for advancing research on AI in CT design and
potentially enhancing the efficacy and robustness of CTs.

ÊëòË¶ÅÔºöCTBench Ë¢´ÂºïÂÖ•‰ΩúÁÇ∫‰∏ÄÂÄãÂü∫Ê∫ñÔºåÁî®ÊñºË©ï‰º∞Ë™ûË®ÄÊ®°Âûã (LM) Âú®Âπ´Âä©Ëá®Â∫äÁ†îÁ©∂Ë®≠Ë®à‰∏≠ÁöÑ‰ΩúÁî®„ÄÇCTBench ÊúÉË©ï‰º∞ AI Ê®°ÂûãÂú®Áµ¶ÂÆöÁâπÂÆöÁ†îÁ©∂ÁöÑÂÖÉÊï∏ÊìöÂæåÔºåÊ±∫ÂÆöËá®Â∫äË©¶È©ó (CT) Âü∫Á∑öÁâπÂæµÁöÑËÉΩÂäõÔºåÂÖ∂‰∏≠ÂåÖÊã¨Âú®Ë©¶È©óÈñãÂßãÊôÇÂæûÊâÄÊúâÂèÉËàáËÄÖÊî∂ÈõÜÁöÑ‰∫∫Âè£Áµ±Ë®àÂíåÁõ∏ÈóúÁâπÂæµ„ÄÇÈÄô‰∫õÂü∫Á∑öÁâπÂæµÈÄöÂ∏∏ÊúÉÂú® CT Âá∫ÁâàÁâ©‰∏≠ÂëàÁèæÔºàÈÄöÂ∏∏ÊòØË°®Ê†º 1ÔºâÔºåÂ∞çÊñºË°®ÂæµÁ†îÁ©∂Áæ§ÁµÑÂíåÈ©óË≠âÁµêÊûúËá≥ÈóúÈáçË¶Å„ÄÇÂü∫Á∑öÁâπÂæµÔºàÂåÖÊã¨Ê∑∑Ê∑ÜÂõ†Â≠êÂíåÂçîËÆäÈáèÔºâÂ∞çÊñºÊ∫ñÁ¢∫‰º∞Ë®àÊ∂âÂèäËßÄÂØüÊï∏ÊìöÁöÑÁ†îÁ©∂‰∏≠ÁöÑÊ≤ªÁôÇÊïàÊûú‰πüÂæàÊúâÂøÖË¶Å„ÄÇCTBench ÂåÖÂê´ÂÖ©ÂÄãÊï∏ÊìöÈõÜÔºö„ÄåCT-Repo„ÄçÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™ clinicaltrials.gov ÁöÑ 1,690 ÂÄãËá®Â∫äË©¶È©óÁöÑÂü∫Á∑öÁâπÂæµÔºå‰ª•Âèä„ÄåCT-Pub„ÄçÔºå‰∏ÄÂÄãÂåÖÂê´ 100 ÂÄãË©¶È©óÁöÑÂ≠êÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂæûÁõ∏ÈóúÂá∫ÁâàÁâ©Êî∂ÈõÜÁöÑÊõ¥ÂÖ®Èù¢ÁöÑÂü∫Á∑öÁâπÂæµ„ÄÇÈñãÁôº‰∫ÜÂÖ©Á®ÆÂü∫Êñº LM ÁöÑË©ï‰º∞ÊñπÊ≥ïÔºåÁî®ÊñºÊØîËºÉÂØ¶ÈöõÁöÑÂü∫Á∑öÁâπÂæµÂàóË°®Âíå LM ÁîüÊàêÁöÑÂõûÊáâ„ÄÇ„ÄåListMatch-LM„ÄçÂíå„ÄåListMatch-BERT„ÄçÂàÜÂà•‰ΩøÁî® GPT-4o Âíå BERT ÂàÜÊï∏ÔºàÂú®ÂêÑÁ®ÆÈñæÂÄº‰∏ãÔºâÈÄ≤Ë°åË©ï‰º∞„ÄÇÁÇ∫‰∫ÜÂª∫Á´ãÂü∫Á∑öÁµêÊûúÔºåÂú®Èõ∂Ê¨°Â≠∏ÁøíÂíå‰∏âÊ¨°Â≠∏ÁøíË®≠ÁΩÆ‰∏≠‰ΩøÁî® LLaMa3-70B-Instruct Âíå GPT-4o ÁöÑÈÄ≤ÈöéÊèêÁ§∫Â∑•Á®ãÊäÄË°ìÔºåÁî®ÊñºÁîüÊàêÊΩõÂú®ÁöÑÂü∫Á∑öÁâπÂæµ„ÄÇGPT-4o ‰ΩúÁÇ∫Ë©ï‰º∞Âô®ÁöÑÊÄßËÉΩÈÄöÈÅéÂú® CT-Pub Êï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°å‰∫∫Ê©ü‰∫§‰∫íË©ï‰º∞ÂæóÂà∞È©óË≠âÔºåÂú®Ë©≤Ë©ï‰º∞‰∏≠ÔºåËá®Â∫äÂ∞àÂÆ∂Á¢∫Ë™çÂØ¶ÈöõÁâπÂæµÂíå LM ÁîüÊàêÁöÑÁâπÂæµ‰πãÈñìÁöÑÂåπÈÖç„ÄÇÁµêÊûúÁ™ÅÈ°Ø‰∫Ü‰∏ÄÂÄãÊúâÂ∏åÊúõÁöÑÊñπÂêëÔºåÂÖ∑ÊúâÈ°ØËëóÁöÑÊîπÈÄ≤ÊΩõÂäõÔºåÂ∞á CTBench ÂÆö‰ΩçÁÇ∫‰øÉÈÄ≤ CT Ë®≠Ë®à‰∏≠ AI Á†îÁ©∂ÁöÑÊúâÁî®Â∑•ÂÖ∑Ôºå‰∏¶ÊúâÂèØËÉΩÊèêÈ´ò CT ÁöÑÂäüÊïàÂíåÁ©©ÂÅ•ÊÄß„ÄÇ

##### **BayTTA: Uncertainty-aware medical image classification with optimized test-time augmentation using Bayesian model averaging**
2406.17640v1 by Zeinab Sherkatghanad, Moloud Abdar, Mohammadreza Bakhtyari, Vladimir Makarenkov

Test-time augmentation (TTA) is a well-known technique employed during the
testing phase of computer vision tasks. It involves aggregating multiple
augmented versions of input data. Combining predictions using a simple average
formulation is a common and straightforward approach after performing TTA. This
paper introduces a novel framework for optimizing TTA, called BayTTA
(Bayesian-based TTA), which is based on Bayesian Model Averaging (BMA). First,
we generate a model list associated with different variations of the input data
created through TTA. Then, we use BMA to combine model predictions weighted by
their respective posterior probabilities. Such an approach allows one to take
into account model uncertainty, and thus to enhance the predictive performance
of the related machine learning or deep learning model. We evaluate the
performance of BayTTA on various public data, including three medical image
datasets comprising skin cancer, breast cancer, and chest X-ray images and two
well-known gene editing datasets, CRISPOR and GUIDE-seq. Our experimental
results indicate that BayTTA can be effectively integrated into
state-of-the-art deep learning models used in medical image analysis as well as
into some popular pre-trained CNN models such as VGG-16, MobileNetV2,
DenseNet201, ResNet152V2, and InceptionRes-NetV2, leading to the enhancement in
their accuracy and robustness performance.

ÊëòË¶ÅÔºöÊ∏¨Ë©¶ÊôÇÈñìÊì¥ÂÖÖ (TTA) ÊòØ‰∏ÄÁ®ÆÂú®ÈõªËÖ¶Ë¶ñË¶∫‰ªªÂãôÁöÑÊ∏¨Ë©¶ÈöéÊÆµ‰∏≠Âª£Ê≥õ‰ΩøÁî®ÁöÑÊäÄË°ì„ÄÇÂÆÉÊ∂âÂèäËÅöÂêàËº∏ÂÖ•Ë≥áÊñôÁöÑË®±Â§öÊì¥ÂÖÖÁâàÊú¨„ÄÇÂú®Âü∑Ë°å TTA ‰πãÂæåÔºå‰ΩøÁî®Á∞°ÂñÆÂπ≥ÂùáÂÖ¨ÂºèÁµÑÂêàÈ†êÊ∏¨ÊòØ‰∏ÄÁ®ÆÂ∏∏Ë¶ã‰∏îÁõ¥Êé•ÁöÑÊñπÊ≥ï„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÁî®ÊñºÊúÄ‰Ω≥Âåñ TTA ÁöÑÊñ∞Ê°ÜÊû∂ÔºåÁ®±ÁÇ∫ BayTTAÔºàÂü∫ÊñºË≤ùÊ∞èÁöÑ TTAÔºâÔºåÂÆÉÂü∫ÊñºË≤ùÊ∞èÊ®°ÂûãÂπ≥Âùá (BMA)„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÁî¢Áîü‰∏ÄÂÄãËàáËº∏ÂÖ•Ë≥áÊñôÁöÑ‰∏çÂêåËÆäÁï∞Áõ∏ÈóúÁöÑÊ®°ÂûãÊ∏ÖÂñÆÔºåÈÄô‰∫õËÆäÁï∞ÊòØÈÄèÈÅé TTA Âª∫Á´ãÁöÑ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄë‰ΩøÁî® BMA ‰æÜÁµÑÂêàÊ®°ÂûãÈ†êÊ∏¨ÔºåÂÖ∂Ê¨äÈáçÁî±ÂÆÉÂÄëÂêÑËá™ÁöÑÂæåÈ©óÊ©üÁéáÊ±∫ÂÆö„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂÖÅË®±ËÄÉÊÖÆÊ®°ÂûãÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÔºåÂæûËÄåÂ¢ûÂº∑Áõ∏ÈóúÊ©üÂô®Â≠∏ÁøíÊàñÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÈ†êÊ∏¨ÊÄßËÉΩ„ÄÇÊàëÂÄëÂú®ÂêÑÁ®ÆÂÖ¨ÈñãË≥áÊñô‰∏äË©ï‰º∞ BayTTA ÁöÑÊÄßËÉΩÔºåÂåÖÊã¨‰∏âÂÄãÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ÁöÆËÜöÁôå„ÄÅ‰π≥ÁôåÂíåËÉ∏ÈÉ® X ÂÖâÂΩ±ÂÉèÔºå‰ª•ÂèäÂÖ©ÂÄãËëóÂêçÁöÑÂü∫Âõ†Á∑®ËºØË≥áÊñôÈõÜÔºåCRISPOR Âíå GUIDE-seq„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåBayTTA ÂèØ‰ª•ÊúâÊïàÊï¥ÂêàÂà∞Áî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÁöÑÊúÄÊñ∞Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰∏≠Ôºå‰ª•Âèä‰∏Ä‰∫õÊµÅË°åÁöÑÈ†êË®ìÁ∑¥ CNN Ê®°Âûã‰∏≠Ôºå‰æãÂ¶Ç VGG-16„ÄÅMobileNetV2„ÄÅDenseNet201„ÄÅResNet152V2 Âíå InceptionRes-NetV2ÔºåÂæûËÄåÊèêÂçáÂÆÉÂÄëÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂÅ•Â£ØÊÄßË°®Áèæ„ÄÇ

##### **Towards Compositional Interpretability for XAI**
2406.17583v1 by Sean Tull, Robin Lorenz, Stephen Clark, Ilyas Khan, Bob Coecke

Artificial intelligence (AI) is currently based largely on black-box machine
learning models which lack interpretability. The field of eXplainable AI (XAI)
strives to address this major concern, being critical in high-stakes areas such
as the finance, legal and health sectors.
  We present an approach to defining AI models and their interpretability based
on category theory. For this we employ the notion of a compositional model,
which sees a model in terms of formal string diagrams which capture its
abstract structure together with its concrete implementation. This
comprehensive view incorporates deterministic, probabilistic and quantum
models. We compare a wide range of AI models as compositional models, including
linear and rule-based models, (recurrent) neural networks, transformers, VAEs,
and causal and DisCoCirc models.
  Next we give a definition of interpretation of a model in terms of its
compositional structure, demonstrating how to analyse the interpretability of a
model, and using this to clarify common themes in XAI. We find that what makes
the standard 'intrinsically interpretable' models so transparent is brought out
most clearly diagrammatically. This leads us to the more general notion of
compositionally-interpretable (CI) models, which additionally include, for
instance, causal, conceptual space, and DisCoCirc models.
  We next demonstrate the explainability benefits of CI models. Firstly, their
compositional structure may allow the computation of other quantities of
interest, and may facilitate inference from the model to the modelled
phenomenon by matching its structure. Secondly, they allow for diagrammatic
explanations for their behaviour, based on influence constraints, diagram
surgery and rewrite explanations. Finally, we discuss many future directions
for the approach, raising the question of how to learn such meaningfully
structured models in practice.

ÊëòË¶ÅÔºö<paragraph>‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÁõÆÂâçÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÁº∫‰πèÂèØËß£ÈáãÊÄßÁöÑÈªëÁõíÊ©üÂô®Â≠∏ÁøíÊ®°Âûã„ÄÇÂèØËß£ÈáãÊÄß‰∫∫Â∑•Êô∫ÊÖßÔºàXAIÔºâÈ†òÂüüËá¥ÂäõÊñºËß£Ê±∫ÈÄôÂÄã‰∏ªË¶ÅÂïèÈ°åÔºåÈÄôÂú®ÈáëËûç„ÄÅÊ≥ïÂæãÂíåÂÅ•Â∫∑Á≠âÈ´òÈ¢®Èö™È†òÂüüËá≥ÈóúÈáçË¶Å„ÄÇ
ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÁØÑÁñáË´ñÂÆöÁæ© AI Ê®°ÂûãÂèäÂÖ∂ÂèØËß£ÈáãÊÄßÁöÑÊñπÊ≥ï„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊé°Áî®ÁµÑÂêàÊ®°ÂûãÁöÑÊ¶ÇÂøµÔºåÂÆÉ‰ª•ÂΩ¢ÂºèÂº¶ÂúñÁöÑÂΩ¢ÂºèÁúãÂæÖÊ®°ÂûãÔºåÈÄô‰∫õÂº¶ÂúñÊçïÁç≤‰∫ÜÊ®°ÂûãÁöÑÊäΩË±°ÁµêÊßãÂèäÂÖ∂ÂÖ∑È´îÂØ¶Áèæ„ÄÇÈÄôÁ®ÆÁ∂úÂêàËßÄÈªûÂåÖÂê´‰∫ÜÁ¢∫ÂÆöÊÄß„ÄÅÊ¶ÇÁéáÊÄßÂíåÈáèÂ≠êÊ®°Âûã„ÄÇÊàëÂÄëÂ∞áÂêÑÁ®Æ AI Ê®°Âûã‰ΩúÁÇ∫ÁµÑÂêàÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉÔºåÂåÖÊã¨Á∑öÊÄßÂíåÂü∫ÊñºË¶èÂâáÁöÑÊ®°Âûã„ÄÅÔºàÈÅûËø¥ÔºâÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅTransformer„ÄÅVAEÔºå‰ª•ÂèäÂõ†ÊûúÂíå DisCoCirc Ê®°Âûã„ÄÇ
Êé•‰∏ã‰æÜÔºåÊàëÂÄëÊ†πÊìöÊ®°ÂûãÁöÑÁµÑÂêàÁµêÊßãÁµ¶Âá∫Ê®°ÂûãËß£ÈáãÁöÑÂÆöÁæ©ÔºåÂ±ïÁ§∫Â¶Ç‰ΩïÂàÜÊûêÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÔºå‰∏¶‰ΩøÁî®ÂÆÉ‰æÜÊæÑÊ∏Ö XAI ‰∏≠ÁöÑÂ∏∏Ë¶ã‰∏ªÈ°å„ÄÇÊàëÂÄëÁôºÁèæÔºåËÆìÊ®ôÊ∫ñÁöÑ„ÄåÂÖßÂú®ÂèØËß£Èáã„ÄçÊ®°ÂûãÂ¶ÇÊ≠§ÈÄèÊòéÁöÑÂéüÂõ†Âú®ÂúñË°®‰∏≠Ë°®ÁèæÂæóÊúÄÁÇ∫Ê∏ÖÊ•ö„ÄÇÈÄôÂºïÂ∞éÊàëÂÄëÂæóÂá∫Êõ¥‰∏ÄËà¨ÁöÑÁµÑÂêàÂèØËß£ÈáãÔºàCIÔºâÊ®°ÂûãÊ¶ÇÂøµÔºåÂÆÉÂè¶Â§ñÈÇÑÂåÖÊã¨Âõ†Êûú„ÄÅÊ¶ÇÂøµÁ©∫ÈñìÂíå DisCoCirc Ê®°Âûã„ÄÇ
Êé•‰∏ã‰æÜÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü CI Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÂÑ™Âã¢„ÄÇÈ¶ñÂÖàÔºåÂÆÉÂÄëÁöÑÁµÑÂêàÁµêÊßãÂÖÅË®±Ë®àÁÆóÂÖ∂‰ªñÊÑüËààË∂£ÁöÑÈáèÔºå‰∏¶ÂèØËÉΩÈÄöÈÅéÂåπÈÖçÊ®°ÂûãÁöÑÁµêÊßã‰æÜ‰øÉÈÄ≤ÂæûÊ®°ÂûãÂà∞Ë¢´Âª∫Ê®°ÁèæË±°ÁöÑÊé®ÁêÜ„ÄÇÂÖ∂Ê¨°ÔºåÂÆÉÂÄëÂÖÅË®±Â∞çÂÖ∂Ë°åÁÇ∫ÈÄ≤Ë°åÂúñËß£Ë™™ÊòéÔºåÈÄô‰∫õË™™ÊòéÂü∫ÊñºÂΩ±ÈüøÁ¥ÑÊùü„ÄÅÂúñËß£ÊâãË°ìÂíåÈáçÂØ´Ë™™Êòé„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÈÄôÁ®ÆÊñπÊ≥ïÁöÑË®±Â§öÊú™‰æÜÊñπÂêëÔºåÊèêÂá∫‰∫ÜÂ¶Ç‰ΩïÂú®ÂØ¶Ë∏ê‰∏≠Â≠∏ÁøíÈÄôÁ®ÆÊúâÊÑèÁæ©ÁöÑÁµêÊßãÂåñÊ®°ÂûãÁöÑÂïèÈ°å„ÄÇ</paragraph>

##### **On the consistency of hyper-parameter selection in value-based deep reinforcement learning**
2406.17523v1 by Johan Obando-Ceron, Jo√£o G. M. Ara√∫jo, Aaron Courville, Pablo Samuel Castro

Deep reinforcement learning (deep RL) has achieved tremendous success on
various domains through a combination of algorithmic design and careful
selection of hyper-parameters. Algorithmic improvements are often the result of
iterative enhancements built upon prior approaches, while hyper-parameter
choices are typically inherited from previous methods or fine-tuned
specifically for the proposed technique. Despite their crucial impact on
performance, hyper-parameter choices are frequently overshadowed by algorithmic
advancements. This paper conducts an extensive empirical study focusing on the
reliability of hyper-parameter selection for value-based deep reinforcement
learning agents, including the introduction of a new score to quantify the
consistency and reliability of various hyper-parameters. Our findings not only
help establish which hyper-parameters are most critical to tune, but also help
clarify which tunings remain consistent across different training regimes.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Âº∑ÂåñÂ≠∏ÁøíÔºàÊ∑±Â∫¶ RLÔºâÈÄèÈÅéÊºîÁÆóÊ≥ïË®≠Ë®àËàá‰ªîÁ¥∞ÈÅ∏ÊìáË∂ÖÂèÉÊï∏ÁöÑÁµêÂêàÔºåÂú®ÂêÑÁ®ÆÈ†òÂüüÂèñÂæóÂ∑®Â§ßÁöÑÊàêÂäü„ÄÇÊºîÁÆóÊ≥ïÁöÑÊîπÈÄ≤ÈÄöÂ∏∏ÊòØÂª∫Á´ãÂú®ÂÖàÂâçÊñπÊ≥ï‰∏äÁöÑÂèçË¶ÜÂ¢ûÂº∑ÁöÑÁµêÊûúÔºåËÄåË∂ÖÂèÉÊï∏ÁöÑÈÅ∏ÊìáÈÄöÂ∏∏ÊâøË•≤Ëá™ÂÖàÂâçÁöÑÊäÄË°ìÔºåÊàñÈáùÂ∞çÂª∫Ë≠∞ÁöÑÊäÄË°ìÈÄ≤Ë°åÂæÆË™ø„ÄÇÂÑòÁÆ°Ë∂ÖÂèÉÊï∏ÁöÑÈÅ∏ÊìáÂ∞çÊïàËÉΩÊúâÊ±∫ÂÆöÊÄßÁöÑÂΩ±ÈüøÔºå‰ΩÜÂÆÉÁ∂ìÂ∏∏Ë¢´ÊºîÁÆóÊ≥ïÁöÑÈÄ≤Ê≠•ÊâÄÊé©Ëìã„ÄÇÊú¨ÊñáÈÄ≤Ë°å‰∏ÄÈ†ÖÂª£Ê≥õÁöÑÂØ¶Ë≠âÁ†îÁ©∂ÔºåÈáçÈªûÂú®ÊñºÂü∫ÊñºÂÉπÂÄºÁöÑÊ∑±Â∫¶Âº∑ÂåñÂ≠∏Áøí‰ª£ÁêÜÁ®ãÂºèË∂ÖÂèÉÊï∏ÈÅ∏ÊìáÁöÑÂèØÈù†ÊÄßÔºåÂåÖÊã¨ÂºïÂÖ•‰∏ÄÂÄãÊñ∞ÁöÑË©ïÂàÜÔºåÁî®ÊñºÈáèÂåñÂêÑÁ®ÆË∂ÖÂèÉÊï∏ÁöÑ‰∏ÄËá¥ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÁöÑÁôºÁèæ‰∏çÂÉÖÊúâÂä©ÊñºÁ¢∫ÂÆöÂì™‰∫õË∂ÖÂèÉÊï∏ÊúÄÈúÄË¶ÅË™øÊï¥ÔºåÈÇÑÊúâÂä©ÊñºÈáêÊ∏ÖÂì™‰∫õË™øÊï¥Âú®‰∏çÂêåÁöÑË®ìÁ∑¥Ê©üÂà∂‰∏≠‰øùÊåÅ‰∏ÄËá¥„ÄÇ

##### **TSynD: Targeted Synthetic Data Generation for Enhanced Medical Image Classification**
2406.17473v1 by Joshua Niemeijer, Jan Ehrhardt, Hristina Uzunova, Heinz Handels

The usage of medical image data for the training of large-scale machine
learning approaches is particularly challenging due to its scarce availability
and the costly generation of data annotations, typically requiring the
engagement of medical professionals. The rapid development of generative models
allows towards tackling this problem by leveraging large amounts of realistic
synthetically generated data for the training process. However, randomly
choosing synthetic samples, might not be an optimal strategy.
  In this work, we investigate the targeted generation of synthetic training
data, in order to improve the accuracy and robustness of image classification.
Therefore, our approach aims to guide the generative model to synthesize data
with high epistemic uncertainty, since large measures of epistemic uncertainty
indicate underrepresented data points in the training set. During the image
generation we feed images reconstructed by an auto encoder into the classifier
and compute the mutual information over the class-probability distribution as a
measure for uncertainty.We alter the feature space of the autoencoder through
an optimization process with the objective of maximizing the classifier
uncertainty on the decoded image. By training on such data we improve the
performance and robustness against test time data augmentations and adversarial
attacks on several classifications tasks.

ÊëòË¶ÅÔºöÁî±ÊñºÈÜ´ÁôÇÂΩ±ÂÉèË≥áÊñôÁöÑÂèñÂæó‰∏çÊòìÔºå‰∏îË≥áÊñôÊ®ôË®ªÁöÑÁî¢ÁîüÊàêÊú¨È´òÊòÇÔºåÈÄöÂ∏∏ÈúÄË¶ÅÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°ÂèÉËàáÔºåÂõ†Ê≠§‰ΩøÁî®ÈÜ´ÁôÇÂΩ±ÂÉèË≥áÊñô‰æÜË®ìÁ∑¥Â§ßÂûãÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁâπÂà•ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÁîüÊàêÂºèÊ®°ÂûãÁöÑÂø´ÈÄüÁôºÂ±ïÔºåÂÖÅË®±ÈÄèÈÅéÂà©Áî®Â§ßÈáèÈÄºÁúüÁöÑÂêàÊàêË≥áÊñô‰æÜË®ìÁ∑¥ÊµÅÁ®ãÔºå‰ª•Ëß£Ê±∫Ê≠§ÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåÈö®Ê©üÈÅ∏ÊìáÂêàÊàêÊ®£Êú¨ÂèØËÉΩ‰∏çÊòØÊúÄ‰Ω≥Á≠ñÁï•„ÄÇ
  Âú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÁ†îÁ©∂ÂêàÊàêË®ìÁ∑¥Ë≥áÊñôÁöÑÁõÆÊ®ôÁîüÊàêÔºå‰ª•ÊèêÈ´òÂΩ±ÂÉèÂàÜÈ°ûÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÊó®Âú®ÂºïÂ∞éÁîüÊàêÂºèÊ®°ÂûãÂêàÊàêÂÖ∑ÊúâÈ´òË™çË≠òË´ñ‰∏çÁ¢∫ÂÆöÊÄßÁöÑË≥áÊñôÔºåÂõ†ÁÇ∫Ë™çË≠òË´ñ‰∏çÁ¢∫ÂÆöÊÄßÁöÑÈ´òÊåáÊ®ôË°®Á§∫Ë®ìÁ∑¥ÈõÜ‰∏≠‰ª£Ë°®ÊÄß‰∏çË∂≥ÁöÑË≥áÊñôÈªû„ÄÇÂú®ÂΩ±ÂÉèÁîüÊàêÈÅéÁ®ã‰∏≠ÔºåÊàëÂÄëÂ∞áËá™ÂãïÁ∑®Á¢ºÂô®ÈáçÂª∫ÁöÑÂΩ±ÂÉèËº∏ÂÖ•ÂàÜÈ°ûÂô®Ôºå‰∏¶Ë®àÁÆóÈ°ûÂà•Ê©üÁéáÂàÜ‰ΩàÁöÑ‰∫íË≥áË®ä‰ΩúÁÇ∫‰∏çÁ¢∫ÂÆöÊÄßÁöÑÊåáÊ®ô„ÄÇÊàëÂÄëÈÄèÈÅéÂÑ™ÂåñÊµÅÁ®ã‰æÜÊîπËÆäËá™ÂãïÁ∑®Á¢ºÂô®ÁöÑÁâπÂæµÁ©∫ÈñìÔºåÁõÆÊ®ôÊòØÊúÄÂ§ßÂåñËß£Á¢ºÂΩ±ÂÉè‰∏äÂàÜÈ°ûÂô®ÁöÑÊú™Á¢∫ÂÆöÊÄß„ÄÇÈÄèÈÅéË®ìÁ∑¥Ê≠§È°ûË≥áÊñôÔºåÊàëÂÄëÊîπÂñÑ‰∫ÜÂú®Â§öÈ†ÖÂàÜÈ°û‰ªªÂãô‰∏≠Â∞çÊ∏¨Ë©¶ÊôÇÈñìË≥áÊñôÊì¥ÂÖÖÂíåÂ∞çÊäóÊîªÊìäÁöÑÊïàËÉΩÂíåÁ©©ÂÅ•ÊÄß„ÄÇ

##### **AI for the prediction of early stages of Alzheimer's disease from neuroimaging biomarkers -- A narrative review of a growing field**
2406.17822v1 by Thorsten Rudroff, Oona Rainio, Riku Kl√©n

Objectives: The objectives of this narrative review are to summarize the
current state of AI applications in neuroimaging for early Alzheimer's disease
(AD) prediction and to highlight the potential of AI techniques in improving
early AD diagnosis, prognosis, and management.
  Methods: We conducted a narrative review of studies using AI techniques
applied to neuroimaging data for early AD prediction. We examined
single-modality studies using structural MRI and PET imaging, as well as
multi-modality studies integrating multiple neuroimaging techniques and
biomarkers. Furthermore, they reviewed longitudinal studies that model AD
progression and identify individuals at risk of rapid decline.
  Results: Single-modality studies using structural MRI and PET imaging have
demonstrated high accuracy in classifying AD and predicting progression from
mild cognitive impairment (MCI) to AD. Multi-modality studies, integrating
multiple neuroimaging techniques and biomarkers, have shown improved
performance and robustness compared to single-modality approaches. Longitudinal
studies have highlighted the value of AI in modeling AD progression and
identifying individuals at risk of rapid decline. However, challenges remain in
data standardization, model interpretability, generalizability, clinical
integration, and ethical considerations.
  Conclusion: AI techniques applied to neuroimaging data have the potential to
improve early AD diagnosis, prognosis, and management. Addressing challenges
related to data standardization, model interpretability, generalizability,
clinical integration, and ethical considerations is crucial for realizing the
full potential of AI in AD research and clinical practice. Collaborative
efforts among researchers, clinicians, and regulatory agencies are needed to
develop reliable, robust, and ethical AI tools that can benefit AD patients and
society.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÊ®ôÔºöÊú¨ÊïòËø∞ÊÄßÂõûÈ°ßÁöÑÁõÆÊ®ôÊòØÁ∏ΩÁµê AI ÊáâÁî®ÊñºÁ•ûÁ∂ìÂΩ±ÂÉèÂ≠∏‰ª•ÈÄ≤Ë°åÈòøËå≤Êµ∑ÈªòÁóá (AD) Êó©ÊúüÈ†êÊ∏¨ÁöÑÁèæÊ≥ÅÔºå‰∏¶Âº∑Ë™ø AI ÊäÄË°ìÂú®ÊîπÂñÑ AD Êó©ÊúüË®∫Êñ∑„ÄÅÈ†êÂæåÂíåÁÆ°ÁêÜÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ
ÊñπÊ≥ïÔºöÊàëÂÄëÂ∞ç‰ΩøÁî® AI ÊäÄË°ìÊáâÁî®ÊñºÁ•ûÁ∂ìÂΩ±ÂÉèÂ≠∏Êï∏Êìö‰ª•ÈÄ≤Ë°åÊó©Êúü AD È†êÊ∏¨ÁöÑÁ†îÁ©∂ÈÄ≤Ë°å‰∫ÜÊïòËø∞ÊÄßÂõûÈ°ß„ÄÇÊàëÂÄëÊ™¢Ë¶ñ‰∫Ü‰ΩøÁî®ÁµêÊßãÊÄß MRI Âíå PET ÂΩ±ÂÉèÁöÑÂñÆ‰∏ÄÊñπÂºèÁ†îÁ©∂Ôºå‰ª•ÂèäÊï¥ÂêàÂ§öÁ®ÆÁ•ûÁ∂ìÂΩ±ÂÉèÂ≠∏ÊäÄË°ìÂíåÁîüÁâ©Ê®ôË®òÁöÑÂ§öÊñπÂºèÁ†îÁ©∂„ÄÇÊ≠§Â§ñÔºå‰ªñÂÄëÂõûÈ°ß‰∫ÜÂ∞ç AD ÈÄ≤Á®ãÂª∫Ê®°‰∏¶ÊâæÂá∫Âø´ÈÄüÊÉ°ÂåñÈ¢®Èö™ÂÄãÈ´îÁöÑÁ∏±ÂêëÁ†îÁ©∂„ÄÇ
ÁµêÊûúÔºö‰ΩøÁî®ÁµêÊßãÊÄß MRI Âíå PET ÂΩ±ÂÉèÁöÑÂñÆ‰∏ÄÊñπÂºèÁ†îÁ©∂Â∑≤Ë≠âÊòéÂú®ÂàÜÈ°û AD ÂíåÈ†êÊ∏¨ÂæûËºïÂ∫¶Ë™çÁü•ÈöúÁ§ô (MCI) Âà∞ AD ÁöÑÈÄ≤Á®ãÊñπÈù¢ÂÖ∑ÊúâÂæàÈ´òÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÊï¥ÂêàÂ§öÁ®ÆÁ•ûÁ∂ìÂΩ±ÂÉèÂ≠∏ÊäÄË°ìÂíåÁîüÁâ©Ê®ôË®òÁöÑÂ§öÊñπÂºèÁ†îÁ©∂Â∑≤È°ØÁ§∫Âá∫ËàáÂñÆ‰∏ÄÊñπÂºèÊñπÊ≥ïÁõ∏ÊØîÔºåÊîπÈÄ≤ÁöÑÊïàËÉΩÂíåÁ©©ÂÅ•ÊÄß„ÄÇÁ∏±ÂêëÁ†îÁ©∂Âº∑Ë™ø‰∫Ü AI Âú®Â∞ç AD ÈÄ≤Á®ãÂª∫Ê®°ÂíåÊâæÂá∫Âø´ÈÄüÊÉ°ÂåñÈ¢®Èö™ÂÄãÈ´îÊñπÈù¢ÁöÑÂÉπÂÄº„ÄÇÁÑ∂ËÄåÔºåÊï∏ÊìöÊ®ôÊ∫ñÂåñ„ÄÅÊ®°ÂûãÂèØËß£ÈáãÊÄß„ÄÅÂèØÊ¶ÇÂåñÊÄß„ÄÅËá®Â∫äÊï¥ÂêàÂíåÂÄ´ÁêÜËÄÉÈáè‰ªçÂ≠òÂú®ÊåëÊà∞„ÄÇ
ÁµêË´ñÔºöÊáâÁî®ÊñºÁ•ûÁ∂ìÂΩ±ÂÉèÂ≠∏Êï∏ÊìöÁöÑ AI ÊäÄË°ìÊúâÊΩõÂäõÊîπÂñÑ AD Êó©ÊúüË®∫Êñ∑„ÄÅÈ†êÂæåÂíåÁÆ°ÁêÜ„ÄÇËß£Ê±∫ËàáÊï∏ÊìöÊ®ôÊ∫ñÂåñ„ÄÅÊ®°ÂûãÂèØËß£ÈáãÊÄß„ÄÅÂèØÊ¶ÇÂåñÊÄß„ÄÅËá®Â∫äÊï¥ÂêàÂíåÂÄ´ÁêÜËÄÉÈáèÁõ∏ÈóúÁöÑÊåëÊà∞Â∞çÊñºÂØ¶Áèæ AI Âú® AD Á†îÁ©∂ÂíåËá®Â∫äÂØ¶Âãô‰∏≠ÁöÑÂÖ®ÈÉ®ÊΩõÂäõËá≥ÈóúÈáçË¶Å„ÄÇÁ†îÁ©∂‰∫∫Âì°„ÄÅËá®Â∫äÈÜ´ÁîüÂíåÊ≥ïË¶èÊ©üÊßã‰πãÈñìÁöÑÂêà‰ΩúÂä™ÂäõÂ∞çÊñºÈñãÁôºÂèØÈù†„ÄÅÁ©©ÂÅ•‰∏îÁ¨¶ÂêàÂÄ´ÁêÜÁöÑ AI Â∑•ÂÖ∑ÊòØÂøÖË¶ÅÁöÑÔºåÈÄô‰∫õÂ∑•ÂÖ∑ÂèØ‰ª•‰Ωø AD ÊÇ£ËÄÖÂíåÁ§æÊúÉÂèóÁõä„ÄÇ</paragraph>

##### **Task-Agnostic Federated Learning**
2406.17235v1 by Zhengtao Yao, Hong Nguyen, Ajitesh Srivastava, Jose Luis Ambite

In the realm of medical imaging, leveraging large-scale datasets from various
institutions is crucial for developing precise deep learning models, yet
privacy concerns frequently impede data sharing. federated learning (FL)
emerges as a prominent solution for preserving privacy while facilitating
collaborative learning. However, its application in real-world scenarios faces
several obstacles, such as task & data heterogeneity, label scarcity,
non-identically distributed (non-IID) data, computational vaiation, etc. In
real-world, medical institutions may not want to disclose their tasks to FL
server and generalization challenge of out-of-network institutions with un-seen
task want to join the on-going federated system. This study address
task-agnostic and generalization problem on un-seen tasks by adapting
self-supervised FL framework. Utilizing Vision Transformer (ViT) as consensus
feature encoder for self-supervised pre-training, no initial labels required,
the framework enabling effective representation learning across diverse
datasets and tasks. Our extensive evaluations, using various real-world non-IID
medical imaging datasets, validate our approach's efficacy, retaining 90\% of
F1 accuracy with only 5\% of the training data typically required for
centralized approaches and exhibiting superior adaptability to
out-of-distribution task. The result indicate that federated learning
architecture can be a potential approach toward multi-task foundation modeling.

ÊëòË¶ÅÔºöÂú®ÂåªÂ≠¶ÂΩ±ÂÉèÈ¢ÜÂüüÔºåÂà©Áî®Êù•Ëá™‰∏çÂêåÊú∫ÊûÑÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÂØπ‰∫éÂºÄÂèëÁ≤æÁ°ÆÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÈöêÁßÅÈóÆÈ¢òÁªèÂ∏∏ÈòªÁ¢çÊï∞ÊçÆÂÖ±‰∫´„ÄÇËÅîÈÇ¶Â≠¶‰π† (FL) ‰Ωú‰∏∫‰∏ÄÁßçÊó¢ËÉΩ‰øùÊä§ÈöêÁßÅÂèàËÉΩ‰øÉËøõÂçè‰ΩúÂ≠¶‰π†ÁöÑÁ™ÅÂá∫Ëß£ÂÜ≥ÊñπÊ°àËÄåÂá∫Áé∞„ÄÇÁÑ∂ËÄåÔºåÂÖ∂Âú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®Èù¢‰∏¥ÁùÄ‰∏Ä‰∫õÈöúÁ¢çÔºå‰æãÂ¶Ç‰ªªÂä°ÂíåÊï∞ÊçÆÂºÇÊûÑÊÄß„ÄÅÊ†áÁ≠æÁ®ÄÁº∫ÊÄß„ÄÅÈùûÂêåÂàÜÂ∏ÉÔºàÈùû IIDÔºâÊï∞ÊçÆ„ÄÅËÆ°ÁÆóÂèòÂºÇÁ≠â„ÄÇÂú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÔºåÂåªÁñóÊú∫ÊûÑÂèØËÉΩ‰∏çÊÉ≥Âêë FL ÊúçÂä°Âô®ÈÄèÈú≤ÂÖ∂‰ªªÂä°ÔºåÂπ∂‰∏îÁΩëÁªúÂ§ñÊú∫ÊûÑÂú®ÈÅáÂà∞Êú™ËßÅ‰ªªÂä°Êó∂ÊÉ≥Ë¶ÅÂä†ÂÖ•Ê≠£Âú®ËøõË°åÁöÑËÅîÈÇ¶Á≥ªÁªüÁöÑÊ≥õÂåñÊåëÊàò„ÄÇÊú¨Á†îÁ©∂ÈÄöËøáÈááÁî®Ëá™ÁõëÁù£ FL Ê°ÜÊû∂Êù•Ëß£ÂÜ≥‰∏é‰ªªÂä°Êó†ÂÖ≥ÂíåÊú™ËßÅ‰ªªÂä°ÁöÑÊ≥õÂåñÈóÆÈ¢ò„ÄÇÂà©Áî®ËßÜËßâ Transformer (ViT) ‰Ωú‰∏∫Ëá™ÁõëÁù£È¢ÑËÆ≠ÁªÉÁöÑÂÖ±ËØÜÁâπÂæÅÁºñÁ†ÅÂô®ÔºåÊó†ÈúÄÂàùÂßãÊ†áÁ≠æÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÂú®‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜÂíå‰ªªÂä°‰∏≠ËøõË°åÊúâÊïàÁöÑË°®Á§∫Â≠¶‰π†„ÄÇÊàë‰ª¨‰ΩøÁî®ÂêÑÁßçÁé∞ÂÆû‰∏ñÁïåÈùû IID ÂåªÂ≠¶ÂΩ±ÂÉèÊï∞ÊçÆÈõÜËøõË°åÁöÑÂπøÊ≥õËØÑ‰º∞È™åËØÅ‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºå‰ªÖ‰ΩøÁî®ÈõÜ‰∏≠ÂºèÊñπÊ≥ïÈÄöÂ∏∏ÊâÄÈúÄÁöÑ 5% ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÂ∞±‰øùÁïô‰∫Ü 90% ÁöÑ F1 ÂáÜÁ°ÆÂ∫¶ÔºåÂπ∂‰∏îË°®Áé∞Âá∫ÂØπÂàÜÂ∏ÉÂ§ñ‰ªªÂä°ÁöÑÂçìË∂äÈÄÇÂ∫îÊÄß„ÄÇÁªìÊûúË°®ÊòéÔºåËÅîÈÇ¶Â≠¶‰π†Êû∂ÊûÑÂèØ‰ª•Êàê‰∏∫Â§ö‰ªªÂä°Âü∫Á°ÄÂª∫Ê®°ÁöÑÊΩúÂú®ÊñπÊ≥ï„ÄÇ

##### **Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars**
2406.17812v1 by Wesley Brewer, Aditya Kashi, Sajal Dash, Aristeidis Tsaris, Junqi Yin, Mallikarjun Shankar, Feiyi Wang

In a post-ChatGPT world, this paper explores the potential of leveraging
scalable artificial intelligence for scientific discovery. We propose that
scaling up artificial intelligence on high-performance computing platforms is
essential to address such complex problems. This perspective focuses on
scientific use cases like cognitive simulations, large language models for
scientific inquiry, medical image analysis, and physics-informed approaches.
The study outlines the methodologies needed to address such challenges at scale
on supercomputers or the cloud and provides exemplars of such approaches
applied to solve a variety of scientific problems.

ÊëòË¶ÅÔºöÂú® ChatGPT ÂæåÁöÑ‰∏ñÁïå‰∏≠ÔºåÊú¨ÊñáÊé¢Ë®é‰∫ÜÂà©Áî®ÂèØÊì¥ÂÖÖÁöÑ‰∫∫Â∑•Êô∫ÊÖßÈÄ≤Ë°åÁßëÂ≠∏ÁôºÁèæÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÊèêÂá∫ÔºåÂú®È´òÊÄßËÉΩÈÅãÁÆóÂπ≥Âè∞‰∏äÊì¥ÂÖÖ‰∫∫Â∑•Êô∫ÊÖßÂ∞çÊñºËß£Ê±∫ÈÄô‰∫õË§áÈõúÂïèÈ°åËá≥ÈóúÈáçË¶Å„ÄÇÊ≠§ËßÄÈªûËëóÈáçÊñºÁßëÂ≠∏Áî®‰æãÔºå‰æãÂ¶ÇË™çÁü•Ê®°Êì¨„ÄÅÁî®ÊñºÁßëÂ≠∏Êé¢Á©∂ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÅÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÂíåÁâ©ÁêÜË≥áË®äÊñπÊ≥ï„ÄÇÊú¨Á†îÁ©∂Ê¶ÇËø∞‰∫ÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÊâÄÈúÄÁöÑÊäÄË°ìÔºå‰ª•‰æøÂú®Ë∂ÖÁ¥öÈõªËÖ¶ÊàñÈõ≤Á´ØÊì¥ÂÖÖÔºå‰∏¶Êèê‰æõÊ≠§È°ûÊñπÊ≥ïÁöÑÁØÑ‰æãÔºå‰ª•Ëß£Ê±∫ÂêÑÁ®ÆÁßëÂ≠∏ÂïèÈ°å„ÄÇ

##### **PIC2O-Sim: A Physics-Inspired Causality-Aware Dynamic Convolutional Neural Operator for Ultra-Fast Photonic Device FDTD Simulation**
2406.17810v1 by Pingchuan Ma, Haoyu Yang, Zhengqi Gao, Duane S. Boning, Jiaqi Gu

The finite-difference time-domain (FDTD) method, which is important in
photonic hardware design flow, is widely adopted to solve time-domain Maxwell
equations. However, FDTD is known for its prohibitive runtime cost, taking
minutes to hours to simulate a single device. Recently, AI has been applied to
realize orders-of-magnitude speedup in partial differential equation (PDE)
solving. However, AI-based FDTD solvers for photonic devices have not been
clearly formulated. Directly applying off-the-shelf models to predict the
optical field dynamics shows unsatisfying fidelity and efficiency since the
model primitives are agnostic to the unique physical properties of Maxwell
equations and lack algorithmic customization. In this work, we thoroughly
investigate the synergy between neural operator designs and the physical
property of Maxwell equations and introduce a physics-inspired AI-based FDTD
prediction framework PIC2O-Sim which features a causality-aware dynamic
convolutional neural operator as its backbone model that honors the space-time
causality constraints via careful receptive field configuration and explicitly
captures the permittivity-dependent light propagation behavior via an efficient
dynamic convolution operator. Meanwhile, we explore the trade-offs among
prediction scalability, fidelity, and efficiency via a multi-stage partitioned
time-bundling technique in autoregressive prediction. Multiple key techniques
have been introduced to mitigate iterative error accumulation while maintaining
efficiency advantages during autoregressive field prediction. Extensive
evaluations on three challenging photonic device simulation tasks have shown
the superiority of our PIC2O-Sim method, showing 51.2% lower roll-out
prediction error, 23.5 times fewer parameters than state-of-the-art neural
operators, providing 300-600x higher simulation speed than an open-source FDTD
numerical solver.

ÊëòË¶ÅÔºöÊôÇÂüüÊúâÈôêÂ∑ÆÂàÜÊ≥ï (FDTD) „ÅØ„ÄÅÂÖâ„Éè„Éº„Éâ„Ç¶„Çß„Ç¢Ë®≠Ë®à„Éï„É≠„Éº„Å´„Åä„ÅÑ„Å¶ÈáçË¶Å„Å™ÊâãÊ≥ï„Åß„ÄÅÊôÇÈ†òÂüü„Éû„ÇØ„Çπ„Ç¶„Çß„É´ÊñπÁ®ãÂºè„ÇíËß£„Åè„Åü„ÇÅ„Å´Â∫É„ÅèÊé°Áî®„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åü„Å†„Åó„ÄÅFDTD „ÅØ„Åù„ÅÆËÜ®Â§ß„Å™„É©„É≥„Çø„Ç§„É†„Ç≥„Çπ„Éà„ÅßÁü•„Çâ„Çå„Å¶„Åä„Çä„ÄÅ1 „Å§„ÅÆ„Éá„Éê„Ç§„Çπ„Çí„Ç∑„Éü„É•„É¨„Éº„Éà„Åô„Çã„ÅÆ„Å´Êï∞ÂàÜ„Åã„ÇâÊï∞ÊôÇÈñì„Åã„Åã„Çä„Åæ„Åô„ÄÇÊúÄËøë„ÄÅAI „ÅåÂÅèÂæÆÂàÜÊñπÁ®ãÂºè (PDE) „ÅÆËß£Ê≥ï„Å´„Åä„Åë„ÇãÊ°ÅÈÅï„ÅÑ„ÅÆÈ´òÈÄüÂåñ„ÇíÂÆüÁèæ„Åô„Çã„Åü„ÇÅ„Å´ÈÅ©Áî®„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åó„Åã„Åó„ÄÅÂÖâ„Éá„Éê„Ç§„ÇπÂêë„Åë„ÅÆ AI „Éô„Éº„Çπ„ÅÆ FDTD „ÇΩ„É´„Éê„Éº„ÅØÊòéÁ¢∫„Å´ÂÆöÂºèÂåñ„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇÂ∏ÇË≤©„ÅÆ„É¢„Éá„É´„Çí„Åù„ÅÆ„Åæ„ÅæÈÅ©Áî®„Åó„Å¶ÂÖâÂ†¥„ÅÆ„ÉÄ„Ç§„Éä„Éü„ÇØ„Çπ„Çí‰∫àÊ∏¨„Åô„Çã„Å®„ÄÅ„É¢„Éá„É´„ÅÆÂü∫Êú¨Ë¶ÅÁ¥†„Åå„Éû„ÇØ„Çπ„Ç¶„Çß„É´ÊñπÁ®ãÂºè„ÅÆÂõ∫Êúâ„ÅÆÁâ©ÁêÜÁâπÊÄß„Å´ÁÑ°Èñ¢‰øÇ„Åß„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆ„Ç´„Çπ„Çø„Éû„Ç§„Ç∫„ÅåÊ¨†Â¶Ç„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅÂø†ÂÆüÂ∫¶„Å®ÂäπÁéá„Åå‰∏çÂçÅÂàÜ„Å´„Å™„Çä„Åæ„Åô„ÄÇ„Åì„ÅÆÁ†îÁ©∂„Åß„ÅØ„ÄÅ„Éã„É•„Éº„É©„É´ÊºîÁÆóÂ≠ê„ÅÆË®≠Ë®à„Å®„Éû„ÇØ„Çπ„Ç¶„Çß„É´ÊñπÁ®ãÂºè„ÅÆÁâ©ÁêÜÁâπÊÄß„Å®„ÅÆÁõ∏‰πóÂäπÊûú„ÇíÂæπÂ∫ïÁöÑ„Å´Ë™øÊüª„Åó„ÄÅÁâ©ÁêÜÂ≠¶„Å´Âü∫„Å•„Åè AI „Éô„Éº„Çπ„ÅÆ FDTD ‰∫àÊ∏¨„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ PIC2O-Sim „ÇíÂ∞éÂÖ•„Åó„Åæ„Åô„ÄÇPIC2O-Sim „ÅØ„ÄÅÊÖéÈáç„Å™ÂèóÂÆπÈáéÊßãÊàê„Å´„Çà„Å£„Å¶ÊôÇÁ©∫ÈñìÂõ†ÊûúÂæãÂà∂Á¥Ñ„ÇíÂ∞äÈáç„Åó„ÄÅÂäπÁéáÁöÑ„Å™ÂãïÁöÑÁï≥„ÅøËæº„ÅøÊºîÁÆóÂ≠ê„Å´„Çà„Å£„Å¶Ë™òÈõªÁéá‰æùÂ≠ò„ÅÆÂÖâ‰ºùÊê¨ÊåôÂãï„ÇíÊòéÁ§∫ÁöÑ„Å´Êçâ„Åà„Çã„ÄÅÂõ†ÊûúÂæã„ÇíË™çË≠ò„Åô„ÇãÂãïÁöÑÁï≥„ÅøËæº„Åø„Éã„É•„Éº„É©„É´ÊºîÁÆóÂ≠ê„Çí„Éê„ÉÉ„ÇØ„Éú„Éº„É≥„É¢„Éá„É´„Å®„Åó„Å¶ÂÇô„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åù„ÅÆ‰∏ÄÊñπ„Åß„ÄÅËá™Â∑±ÂõûÂ∏∞‰∫àÊ∏¨„Å´„Åä„Åë„Çã„Éû„É´„ÉÅ„Çπ„ÉÜ„Éº„Ç∏„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥ÊôÇÈñì„Éê„É≥„Éâ„É™„É≥„Ç∞ÊâãÊ≥ï„Å´„Çà„Å£„Å¶„ÄÅ‰∫àÊ∏¨„ÅÆ„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£„ÄÅÂø†ÂÆüÂ∫¶„ÄÅÂäπÁéá„ÅÆÈñì„ÅÆ„Éà„É¨„Éº„Éâ„Ç™„Éï„ÇíË™øÊüª„Åó„Åæ„Åô„ÄÇËá™Â∑±ÂõûÂ∏∞Â†¥„ÅÆ‰∫àÊ∏¨‰∏≠„Å´ÂäπÁéá„ÅÆÂà©ÁÇπ„ÇíÁ∂≠ÊåÅ„Åó„Å™„Åå„ÇâÂèçÂæ©ÁöÑ„Å™Ë™§„Çä„ÅÆËìÑÁ©ç„ÇíËªΩÊ∏õ„Åô„Çã„Åü„ÇÅ„Å´„ÄÅË§áÊï∞„ÅÆ‰∏ªË¶Å„Å™ÊâãÊ≥ï„ÅåÂ∞éÂÖ•„Åï„Çå„Åæ„Åó„Åü„ÄÇ3 „Å§„ÅÆÂõ∞Èõ£„Å™ÂÖâ„Éá„Éê„Ç§„Çπ„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„Çø„Çπ„ÇØ„Å´Èñ¢„Åô„ÇãÂ∫ÉÁØÑ„Å™Ë©ï‰æ°„Å´„Çà„Çä„ÄÅPIC2O-Sim ÊâãÊ≥ï„ÅÆÂÑ™‰ΩçÊÄß„ÅåÁ§∫„Åï„Çå„ÄÅ51.2% ‰Ωé„ÅÑ„É≠„Éº„É´„Ç¢„Ç¶„Éà‰∫àÊ∏¨Ë™§Â∑Æ„ÄÅÊúÄÂÖàÁ´Ø„ÅÆ„Éã„É•„Éº„É©„É´ÊºîÁÆóÂ≠ê„Çà„Çä„ÇÇ 23.5 ÂÄçÂ∞ë„Å™„ÅÑ„Éë„É©„É°„Éº„Çø„Éº„ÄÅ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„Çπ FDTD Êï∞ÂÄ§„ÇΩ„É´„Éê„Éº„Çà„Çä„ÇÇ 300 ÔΩû 600 ÂÄçÈ´òÈÄü„Å™„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥ÈÄüÂ∫¶„ÅåÊèê‰æõ„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ

##### **The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources**
2406.16746v2 by Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, Sayash Kapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, Maribeth Rauh, Aviya Skowron, Bertie Vidgen, Laura Weidinger, Arvind Narayanan, Victor Sanh, David Adelani, Percy Liang, Rishi Bommasani, Peter Henderson, Sasha Luccioni, Yacine Jernite, Luca Soldaini

Foundation model development attracts a rapidly expanding body of
contributors, scientists, and applications. To help shape responsible
development practices, we introduce the Foundation Model Development
Cheatsheet: a growing collection of 250+ tools and resources spanning text,
vision, and speech modalities. We draw on a large body of prior work to survey
resources (e.g. software, documentation, frameworks, guides, and practical
tools) that support informed data selection, processing, and understanding,
precise and limitation-aware artifact documentation, efficient model training,
advance awareness of the environmental impact from training, careful model
evaluation of capabilities, risks, and claims, as well as responsible model
release, licensing and deployment practices. We hope this curated collection of
resources helps guide more responsible development. The process of curating
this list, enabled us to review the AI development ecosystem, revealing what
tools are critically missing, misused, or over-used in existing practices. We
find that (i) tools for data sourcing, model evaluation, and monitoring are
critically under-serving ethical and real-world needs, (ii) evaluations for
model safety, capabilities, and environmental impact all lack reproducibility
and transparency, (iii) text and particularly English-centric analyses continue
to dominate over multilingual and multi-modal analyses, and (iv) evaluation of
systems, rather than just models, is needed so that capabilities and impact are
assessed in context.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°ÂûãÈñãÁôºÂê∏Âºï‰∫ÜËøÖÈÄüÊì¥Â±ïÁöÑË≤¢ÁçªËÄÖ„ÄÅÁßëÂ≠∏ÂÆ∂ÂíåÊáâÁî®Á®ãÂºè‰∏ªÈ´î„ÄÇÁÇ∫‰∫ÜÂçîÂä©Â°ëÈÄ†Ë≤†Ë≤¨‰ªªÁöÑÈñãÁôºÂØ¶ÂãôÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü„ÄåÂü∫Á§éÊ®°ÂûãÈñãÁôºÁßòÁ¨à„ÄçÔºö‰∏ÄÂÄãÂåÖÂê´Ë∂ÖÈÅé 250 ÂÄãÂ∑•ÂÖ∑ÂíåË≥áÊ∫êÁöÑÊàêÈï∑‰∏≠ÈõÜÂêàÔºåÊ∂µËìãÊñáÂ≠ó„ÄÅË¶ñË¶∫ÂíåË™ûÈü≥Ê®°Âºè„ÄÇÊàëÂÄëÂà©Áî®Â§ßÈáèÁöÑÂÖàÂâçÂ∑•‰Ωú‰æÜË™øÊü•Ë≥áÊ∫êÔºà‰æãÂ¶ÇËªüÈ´î„ÄÅÊñá‰ª∂„ÄÅÊû∂Êßã„ÄÅÊåáÂçóÂíåÂØ¶Áî®Â∑•ÂÖ∑ÔºâÔºåÈÄô‰∫õË≥áÊ∫êÊîØÊè¥ÊòéÊô∫ÁöÑË≥áÊñôÈÅ∏Êìá„ÄÅËôïÁêÜÂíåÁêÜËß£„ÄÅÁ≤æÁ¢∫‰∏îÂÖ∑ÂÇôÈôêÂà∂ÊÑèË≠òÁöÑ‰∫∫Â∑•Áî¢Âá∫Êñá‰ª∂„ÄÅÊúâÊïàÁéáÁöÑÊ®°ÂûãË®ìÁ∑¥„ÄÅÊèêÂâç‰∫ÜËß£Ë®ìÁ∑¥Â∞çÁí∞Â¢ÉÁöÑÂΩ±Èüø„ÄÅ‰ªîÁ¥∞Ë©ï‰º∞Ê®°ÂûãÁöÑËÉΩÂäõ„ÄÅÈ¢®Èö™ÂíåËÅ≤ÊòéÔºå‰ª•ÂèäË≤†Ë≤¨‰ªªÁöÑÊ®°ÂûãÁôºÂ∏É„ÄÅÊéàÊ¨äÂíåÈÉ®ÁΩ≤ÂØ¶Âãô„ÄÇÊàëÂÄëÂ∏åÊúõÈÄôÂÄãÁ∂ìÈÅéÊï¥ÁêÜÁöÑË≥áÊ∫êÈõÜÂêàÊúâÂä©ÊñºÂºïÂ∞éÊõ¥Ë≤†Ë≤¨‰ªªÁöÑÈñãÁôº„ÄÇÊï¥ÁêÜÈÄôÂÄãÊ∏ÖÂñÆÁöÑÈÅéÁ®ãËÆìÊàëÂÄëÂæó‰ª•Ê™¢Ë¶ñ AI ÈñãÁôºÁîüÊÖãÁ≥ªÁµ±ÔºåÊè≠Èú≤ÁèæÊúâÂØ¶Âãô‰∏≠Âì™‰∫õÂ∑•ÂÖ∑Âö¥Èáç‰∏çË∂≥„ÄÅ‰ΩøÁî®‰∏çÁï∂ÊàñÈÅéÂ∫¶‰ΩøÁî®„ÄÇÊàëÂÄëÁôºÁèæÔºö(i) Ë≥áÊñô‰æÜÊ∫ê„ÄÅÊ®°ÂûãË©ï‰º∞ÂíåÁõ£ÊéßÁöÑÂ∑•ÂÖ∑Âö¥ÈáçÁÑ°Ê≥ïÊªøË∂≥ÈÅìÂæ∑ÂíåÁèæÂØ¶‰∏ñÁïåÁöÑÈúÄÊ±ÇÔºå(ii) Ê®°ÂûãÂÆâÂÖ®ÊÄß„ÄÅËÉΩÂäõÂíåÁí∞Â¢ÉÂΩ±ÈüøÁöÑË©ï‰º∞ÈÉΩÁº∫‰πèÂèØË§áË£ΩÊÄßÂíåÈÄèÊòéÂ∫¶Ôºå(iii) ÊñáÂ≠óÂíåÁâπÂà•ÊòØËã±Ë™ûÁÇ∫‰∏≠ÂøÉÁöÑÂàÜÊûêÊåÅÁ∫å‰∏ªÂ∞éÂ§öË™ûË®ÄÂíåÂ§öÊ®°ÂºèÂàÜÊûêÔºåËÄå‰∏î (iv) ÈúÄË¶ÅË©ï‰º∞Á≥ªÁµ±ÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÊ®°ÂûãÔºå‰ª•‰æøÂú®‰∏ä‰∏ãÊñá‰∏≠Ë©ï‰º∞ËÉΩÂäõÂíåÂΩ±Èüø„ÄÇ

##### **Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings**
2406.16611v1 by Andrea Posada, Daniel Rueckert, Felix Meissen, Philip M√ºller

Since the emergence of the Transformer architecture, language model
development has increased, driven by their promising potential. However,
releasing these models into production requires properly understanding their
behavior, particularly in sensitive domains such as medicine. Despite this
need, the medical literature still lacks technical assessments of pre-trained
language models, which are especially valuable in resource-constrained settings
in terms of computational power or limited budget. To address this gap, we
provide a comprehensive survey of language models in the medical domain. In
addition, we selected a subset of these models for thorough evaluation,
focusing on classification and text generation tasks. Our subset encompasses 53
models, ranging from 110 million to 13 billion parameters, spanning the three
families of Transformer-based models and from diverse knowledge domains. This
study employs a series of approaches for text classification together with
zero-shot prompting instead of model training or fine-tuning, which closely
resembles the limited resource setting in which many users of language models
find themselves. Encouragingly, our findings reveal remarkable performance
across various tasks and datasets, underscoring the latent potential of certain
models to contain medical knowledge, even without domain specialization.
Consequently, our study advocates for further exploration of model applications
in medical contexts, particularly in resource-constrained settings. The code is
available on https://github.com/anpoc/Language-models-in-medicine.

ÊëòË¶ÅÔºöËá™ Transformer Êû∂ÊßãÂïè‰∏ñ‰ª•‰æÜÔºåË™ûË®ÄÊ®°ÂûãÂú®ÊΩõÂäõÂÇôÂèóÁúãÂ•Ω‰∏ãÔºåÁôºÂ±ïÂ¶ÇÁÅ´Â¶ÇËçº„ÄÇÁÑ∂ËÄåÔºåÂ∞áÈÄô‰∫õÊ®°ÂûãÊáâÁî®ÊñºÁîüÁî¢Áí∞Â¢ÉÔºåÈúÄË¶ÅÈÅ©Áï∂Âú∞‰∫ÜËß£ÂÖ∂Ë°åÁÇ∫ÔºåÁâπÂà•ÊòØÂú®ÈÜ´Â≠∏Á≠âÊïèÊÑüÈ†òÂüü„ÄÇÂÑòÁÆ°ÊúâÊ≠§ÈúÄÊ±ÇÔºåÈÜ´Â≠∏ÊñáÁçª‰ªçÁº∫‰πèÂ∞çÈ†êÂÖàË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÁöÑÊäÄË°ìË©ï‰º∞ÔºåËÄåÈÄôÂú®ÈÅãÁÆóËÉΩÂäõÊàñÈ†êÁÆóÊúâÈôêÁöÑË≥áÊ∫êÂèóÈôêÁí∞Â¢É‰∏≠ÁâπÂà•ÊúâÂÉπÂÄº„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÁº∫Âè£ÔºåÊàëÂÄëÂ∞çÈÜ´Â≠∏È†òÂüüÁöÑË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑË™øÊü•„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊåëÈÅ∏‰∫ÜÂÖ∂‰∏≠‰∏ÄÈÉ®ÂàÜÊ®°ÂûãÈÄ≤Ë°åÂæπÂ∫ïË©ï‰º∞ÔºåÈáçÈªûÂú®ÊñºÂàÜÈ°ûÂíåÊñáÂ≠óÁîüÊàê‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÂ≠êÈõÜÊ∂µËìã 53 ÂÄãÊ®°ÂûãÔºåÂèÉÊï∏Âæû 1.1 ÂÑÑÂà∞ 130 ÂÑÑ‰∏çÁ≠âÔºåÊ©´Ë∑® Transformer ÁÇ∫Âü∫Á§éÁöÑÊ®°ÂûãÁöÑ‰∏âÂÄãÁ≥ªÂàóÔºå‰∏îÊ∂µËìãÂ§öÂÖÉÁöÑÁü•Ë≠òÈ†òÂüü„ÄÇÊú¨Á†îÁ©∂Êé°Áî®‰∏ÄÁ≥ªÂàóÊñáÂ≠óÂàÜÈ°ûÊñπÊ≥ïÔºå‰∏¶Êê≠ÈÖçÈõ∂Ê¨°ÊèêÁ§∫ÔºåËÄåÈùûÊ®°ÂûãË®ìÁ∑¥ÊàñÂæÆË™øÔºåÈÄôËàáË®±Â§öË™ûË®ÄÊ®°Âûã‰ΩøÁî®ËÄÖË∫´ËôïÁöÑË≥áÊ∫êÂèóÈôêÁí∞Â¢ÉÈùûÂ∏∏È°û‰ºº„ÄÇ‰ª§‰∫∫ÊåØÂ•ÆÁöÑÊòØÔºåÊàëÂÄëÁöÑÁôºÁèæÈ°ØÁ§∫Âú®ÂêÑÁ®Æ‰ªªÂãôÂíåË≥áÊñôÈõÜ‰∏äÈÉΩÊúâÂÇëÂá∫ÁöÑË°®ÁèæÔºåÂº∑Ë™ø‰∫ÜÊüê‰∫õÊ®°ÂûãÂú®Ê≤íÊúâÈ†òÂüüÂ∞àÊ•≠Áü•Ë≠òÁöÑÊÉÖÊ≥Å‰∏ãÔºåËòäÂê´ÈÜ´Â≠∏Áü•Ë≠òÁöÑÊΩõÂäõ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂‰∏ªÂºµÈÄ≤‰∏ÄÊ≠•Êé¢Ë®éÊ®°ÂûãÂú®ÈÜ´ÁôÇÁí∞Â¢É‰∏≠ÁöÑÊáâÁî®ÔºåÁâπÂà•ÊòØÂú®Ë≥áÊ∫êÂèóÈôêÁöÑÁí∞Â¢É‰∏≠„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/anpoc/Language-models-in-medicine ÂèñÂæó„ÄÇ

##### **Guardrails for avoiding harmful medical product recommendations and off-label promotion in generative AI models**
2406.16455v1 by Daniel Lopez-Martinez

Generative AI (GenAI) models have demonstrated remarkable capabilities in a
wide variety of medical tasks. However, as these models are trained using
generalist datasets with very limited human oversight, they can learn uses of
medical products that have not been adequately evaluated for safety and
efficacy, nor approved by regulatory agencies. Given the scale at which GenAI
may reach users, unvetted recommendations pose a public health risk. In this
work, we propose an approach to identify potentially harmful product
recommendations, and demonstrate it using a recent multimodal large language
model.

ÊëòË¶ÅÔºöÁîüÊàêÂºè AI (GenAI) Ê®°ÂûãÂú®ÂêÑÁßçÂåªÁñó‰ªªÂãô‰∏≠Â±ïÁ§∫Âá∫ÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÈÄô‰∫õÊ®°ÂûãÊòØ‰ΩøÁî®ÈùûÂ∏∏ÊúâÈôêÁöÑ‰∫∫È°ûÁõ£Áù£ÁöÑ‰∏ÄËà¨Ë≥áÊñôÈõÜÈÄ≤Ë°åË®ìÁ∑¥ÔºåÂõ†Ê≠§ÂÆÉÂÄëÂèØ‰ª•Â≠∏ÁøíÂ∞öÊú™ÂÖÖÂàÜË©ï‰º∞ÂÖ∂ÂÆâÂÖ®ÊÄßÂíåÊúâÊïàÊÄßÔºå‰πüÊú™Á∂ìÁõ£ÁÆ°Ê©üÊßãÊâπÂáÜÁöÑÈÜ´ÁôÇÁî¢ÂìÅÁî®ÈÄî„ÄÇÈëëÊñº GenAI ÂèØËÉΩÊé•Ëß∏‰ΩøÁî®ËÄÖÁöÑË¶èÊ®°ÔºåÊú™Á∂ìÂØ©Êü•ÁöÑÂª∫Ë≠∞ÊúÉÊßãÊàêÂÖ¨ÂÖ±ÂÅ•Â∫∑È¢®Èö™„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆË≠òÂà•ÊΩõÂú®ÊúâÂÆ≥Áî¢ÂìÅÂª∫Ë≠∞ÁöÑÊñπÊ≥ïÔºå‰∏¶‰ΩøÁî®ÊúÄËøëÁöÑÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°åÁ§∫ÁØÑ„ÄÇ

##### **A large language model for predicting T cell receptor-antigen binding specificity**
2406.16995v1 by Xing Fang, Chenpeng Yu, Shiye Tian, Hui Liu

The human immune response depends on the binding of T-cell receptors (TCRs)
to antigens (pTCR), which elicits the T cells to eliminate viruses, tumor
cells, and other pathogens. The ability of human immunity system responding to
unknown viruses and bacteria stems from the TCR diversity. However, this vast
diversity poses challenges on the TCR-antigen binding prediction methods. In
this study, we propose a Masked Language Model (MLM), referred to as tcrLM, to
overcome limitations in model generalization. Specifically, we randomly masked
sequence segments and train tcrLM to infer the masked segment, thereby extract
expressive feature from TCR sequences. Meanwhile, we introduced virtual
adversarial training techniques to enhance the model's robustness. We built the
largest TCR CDR3 sequence dataset to date (comprising 2,277,773,840 residuals),
and pre-trained tcrLM on this dataset. Our extensive experimental results
demonstrate that tcrLM achieved AUC values of 0.937 and 0.933 on independent
test sets and external validation sets, respectively, which remarkably
outperformed four previously published prediction methods. On a large-scale
COVID-19 pTCR binding test set, our method outperforms the current
state-of-the-art method by at least 8%, highlighting the generalizability of
our method. Furthermore, we validated that our approach effectively predicts
immunotherapy response and clinical outcomes on a clinical cohorts. These
findings clearly indicate that tcrLM exhibits significant potential in
predicting antigenic immunogenicity.

ÊëòË¶ÅÔºö‰∫∫È´îÂÖçÁñ´ÂèçÊáâÂèñÊ±∫Êñº T Á¥∞ËÉûÂèóÈ´î (TCR) ËàáÊäóÂéü (pTCR) ÁöÑÁµêÂêàÔºåÈÄôÊúÉÂºïÁôº T Á¥∞ËÉûÊ∂àÈô§ÁóÖÊØí„ÄÅËÖ´Áò§Á¥∞ËÉûÂíåÂÖ∂‰ªñÁóÖÂéüÈ´î„ÄÇ‰∫∫È´îÂÖçÁñ´Á≥ªÁµ±Â∞çÊú™Áü•ÁóÖÊØíÂíåÁ¥∞ËèåÂÅöÂá∫ÂèçÊáâÁöÑËÉΩÂäõÊ∫êÊñº TCR ÁöÑÂ§öÊ®£ÊÄß„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÂª£Ê≥õÁöÑÂ§öÊ®£ÊÄßÂ∞ç TCR-ÊäóÂéüÁµêÂêàÈ†êÊ∏¨ÊñπÊ≥ïÊèêÂá∫‰∫ÜÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ tcrLM ÁöÑÈÅÆÁΩ©Ë™ûË®ÄÊ®°Âûã (MLM)Ôºå‰ª•ÂÖãÊúçÊ®°ÂûãÊ¶ÇÊã¨‰∏≠ÁöÑÈôêÂà∂„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈö®Ê©üÈÅÆÁΩ©Â∫èÂàóÁâáÊÆµ‰∏¶Ë®ìÁ∑¥ tcrLM Êé®Êñ∑ÈÅÆÁΩ©ÁâáÊÆµÔºåÂæûËÄåÂæû TCR Â∫èÂàó‰∏≠ÊèêÂèñË°®ÈÅîÁâπÂæµ„ÄÇÂêåÊôÇÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜËôõÊì¨Â∞çÊäóË®ìÁ∑¥ÊäÄË°ì‰æÜÂ¢ûÂº∑Ê®°ÂûãÁöÑÈ≠ØÊ£íÊÄß„ÄÇÊàëÂÄëÂª∫Á´ã‰∫ÜËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÂ§ßÁöÑ TCR CDR3 Â∫èÂàóÊï∏ÊìöÈõÜÔºàÂåÖÂê´ 2,277,773,840 ÂÄãÊÆòÂü∫ÔºâÔºå‰∏¶Âú®Ë©≤Êï∏ÊìöÈõÜ‰∏äÈ†êË®ìÁ∑¥‰∫Ü tcrLM„ÄÇÊàëÂÄëÂª£Ê≥õÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåtcrLM Âú®Áç®Á´ãÊ∏¨Ë©¶ÈõÜÂíåÂ§ñÈÉ®È©óË≠âÈõÜ‰∏äÂàÜÂà•ÂØ¶Áèæ‰∫Ü 0.937 Âíå 0.933 ÁöÑ AUC ÂÄºÔºåÈÄôÈ°ØËëóÂÑ™ÊñºÂõõÁ®ÆÂÖàÂâçÁôºË°®ÁöÑÈ†êÊ∏¨ÊñπÊ≥ï„ÄÇÂú®‰∏ÄÂÄãÂ§ßË¶èÊ®°ÁöÑ COVID-19 pTCR ÁµêÂêàÊ∏¨Ë©¶ÈõÜ‰∏≠ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊØîÁï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãËá≥Â∞ëÈ´òÂá∫ 8%ÔºåÁ™ÅÂá∫‰∫ÜÊàëÂÄëÊ®°ÂûãÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÊúâÊïàÈ†êÊ∏¨‰∫ÜËá®Â∫ä‰∫∫Áæ§ÁöÑÂÖçÁñ´Ê≤ªÁôÇÂèçÊáâÂíåËá®Â∫äÁµêÊûú„ÄÇÈÄô‰∫õÁôºÁèæÊòéÁ¢∫Ë°®Êòé tcrLM Âú®È†êÊ∏¨ÊäóÂéüÂÖçÁñ´ÂéüÊÄßÊñπÈù¢ÂÖ∑ÊúâÈ°ØËëóÁöÑÊΩõÂäõ„ÄÇ

##### **Graph-Augmented LLMs for Personalized Health Insights: A Case Study in Sleep Analysis**
2406.16252v2 by Ajan Subramanian, Zhongqi Yang, Iman Azimi, Amir M. Rahmani

Health monitoring systems have revolutionized modern healthcare by enabling
the continuous capture of physiological and behavioral data, essential for
preventive measures and early health intervention. While integrating this data
with Large Language Models (LLMs) has shown promise in delivering interactive
health advice, traditional methods like Retrieval-Augmented Generation (RAG)
and fine-tuning often fail to fully utilize the complex, multi-dimensional, and
temporally relevant data from wearable devices. These conventional approaches
typically provide limited actionable and personalized health insights due to
their inadequate capacity to dynamically integrate and interpret diverse health
data streams. In response, this paper introduces a graph-augmented LLM
framework designed to significantly enhance the personalization and clarity of
health insights. Utilizing a hierarchical graph structure, the framework
captures inter and intra-patient relationships, enriching LLM prompts with
dynamic feature importance scores derived from a Random Forest Model. The
effectiveness of this approach is demonstrated through a sleep analysis case
study involving 20 college students during the COVID-19 lockdown, highlighting
the potential of our model to generate actionable and personalized health
insights efficiently. We leverage another LLM to evaluate the insights for
relevance, comprehensiveness, actionability, and personalization, addressing
the critical need for models that process and interpret complex health data
effectively. Our findings show that augmenting prompts with our framework
yields significant improvements in all 4 criteria. Through our framework, we
can elicit well-crafted, more thoughtful responses tailored to a specific
patient.

ÊëòË¶ÅÔºöÂÅ•Â∫∑Áõ£ÊéßÁ≥ªÁµ±ÈÄèÈÅéÊåÅÁ∫åÊî∂ÈõÜÁîüÁêÜÂíåË°åÁÇ∫Ë≥áÊñôÔºåÂæπÂ∫ïÊîπËÆä‰∫ÜÁèæ‰ª£ÈÜ´ÁôÇ‰øùÂÅ•ÔºåÈÄô‰∫õË≥áÊñôÂ∞çÊñºÈ†êÈò≤Êé™ÊñΩÂíåÊó©ÊúüÂÅ•Â∫∑Âπ≤È†êËá≥ÈóúÈáçË¶Å„ÄÇÈõñÁÑ∂Â∞áÈÄô‰∫õË≥áÊñôËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êï¥ÂêàÔºåÂ∑≤Â±ïÁèæÂá∫Êèê‰æõ‰∫íÂãïÂºèÂÅ•Â∫∑Âª∫Ë≠∞ÁöÑÊΩõÂäõÔºå‰ΩÜÂÇ≥Áµ±ÊñπÊ≥ïÔºà‰æãÂ¶ÇÊ™¢Á¥¢Êì¥ÂÖÖÁîüÊàê (RAG) ÂíåÂæÆË™øÔºâÈÄöÂ∏∏ÁÑ°Ê≥ïÂÖÖÂàÜÂà©Áî®Á©øÊà¥ÂºèË£ùÁΩÆ‰∏≠Ë§áÈõú„ÄÅÂ§öÈù¢Âêë‰∏îËàáÊôÇÈñìÁõ∏ÈóúÁöÑË≥áÊñô„ÄÇÈÄô‰∫õÂÇ≥Áµ±ÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÊèê‰æõÊúâÈôêÁöÑÂèØË°å‰∏îÂÄã‰∫∫ÂåñÁöÑÂÅ•Â∫∑Ë¶ãËß£ÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁÑ°Ê≥ïÂãïÊÖãÊï¥ÂêàÂíåË©ÆÈáã‰∏çÂêåÁöÑÂÅ•Â∫∑Ë≥áÊñô‰∏≤ÊµÅ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂúñÂΩ¢Êì¥ÂÖÖ LLM Êû∂ÊßãÔºåÊó®Âú®Â§ßÂπÖÊèêÂçáÂÅ•Â∫∑Ë¶ãËß£ÁöÑÂÄã‰∫∫ÂåñÂíåÊ∏ÖÊô∞Â∫¶„ÄÇÈÄôÂÄãÊû∂ÊßãÂà©Áî®ÈöéÂ±§ÂºèÂúñÂΩ¢ÁµêÊßãÔºåÊì∑ÂèñÊÇ£ËÄÖ‰πãÈñìÂíåÊÇ£ËÄÖÂÖßÈÉ®ÁöÑÈóú‰øÇÔºå‰∏¶‰ΩøÁî®Âæû Random Forest Ê®°ÂûãË°çÁîüÁöÑÂãïÊÖãÁâπÂæµÈáçË¶ÅÊÄßË©ïÂàÜÔºåË±êÂØå LLM ÊèêÁ§∫„ÄÇÈÄèÈÅé‰∏ÄÈ†ÖÁù°Áú†ÂàÜÊûêÊ°à‰æãÁ†îÁ©∂ÔºàÂú® COVID-19 Â∞ÅÈéñÊúüÈñìÈáùÂ∞ç 20 ÂêçÂ§ßÂ≠∏ÁîüÈÄ≤Ë°åÔºâË≠âÊòé‰∫ÜÈÄôÂÄãÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÁ™ÅÈ°Ø‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊúâÊïàÁî¢ÁîüÂèØË°å‰∏îÂÄã‰∫∫ÂåñÁöÑÂÅ•Â∫∑Ë¶ãËß£ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÂà©Áî®Âè¶‰∏ÄÂÄã LLM Ë©ï‰º∞Ë¶ãËß£ÁöÑÁõ∏ÈóúÊÄß„ÄÅÂÖ®Èù¢ÊÄß„ÄÅÂèØË°åÊÄßÂíåÂÄã‰∫∫ÂåñÔºåÊªøË∂≥‰∫ÜÊ®°ÂûãÊúâÊïàËôïÁêÜÂíåË©ÆÈáãË§áÈõúÂÅ•Â∫∑Ë≥áÊñôÁöÑÈóúÈçµÈúÄÊ±Ç„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫Ôºå‰ΩøÁî®ÊàëÂÄëÁöÑÊû∂ÊßãÊì¥ÂÖÖÊèêÁ§∫ÔºåÂèØ‰ª•Âú®ÊâÄÊúâ 4 ÂÄãÊ®ôÊ∫ñ‰∏≠Â§ßÂπÖÊîπÂñÑ„ÄÇÈÄèÈÅéÊàëÂÄëÁöÑÊû∂ÊßãÔºåÊàëÂÄëÂèØ‰ª•ÂºïÁôºÁ≤æÂøÉË®≠Ë®à„ÄÅÊõ¥Âë®ÂÖ®ÁöÑÂõûÊáâÔºåÈáùÂ∞çÁâπÂÆöÊÇ£ËÄÖÈáèË∫´ÊâìÈÄ†„ÄÇ

##### **Continuous Output Personality Detection Models via Mixed Strategy Training**
2406.16223v1 by Rong Wang, Kun Sun

The traditional personality models only yield binary results. This paper
presents a novel approach for training personality detection models that
produce continuous output values, using mixed strategies. By leveraging the
PANDORA dataset, which includes extensive personality labeling of Reddit
comments, we developed models that predict the Big Five personality traits with
high accuracy. Our approach involves fine-tuning a RoBERTa-base model with
various strategies such as Multi-Layer Perceptron (MLP) integration, and
hyperparameter tuning. The results demonstrate that our models significantly
outperform traditional binary classification methods, offering precise
continuous outputs for personality traits, thus enhancing applications in AI,
psychology, human resources, marketing and health care fields.

ÊëòË¶ÅÔºöÂÇ≥Áµ±ÁöÑ‰∫∫Ê†ºÊ®°ÂûãÂè™Áî¢Áîü‰∫åÈÄ≤Âà∂ÁµêÊûú„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®Ê∑∑ÂêàÁ≠ñÁï•Ë®ìÁ∑¥‰∫∫Ê†ºÊ™¢Ê∏¨Ê®°ÂûãÔºå‰ª•Áî¢ÁîüÈÄ£Á∫åÁöÑËº∏Âá∫ÂÄº„ÄÇÈÄöÈÅéÂà©Áî® PANDORA Êï∏ÊìöÈõÜÔºåÂÖ∂‰∏≠ÂåÖÊã¨Â∞ç Reddit Ë©ïË´ñÁöÑÂª£Ê≥õ‰∫∫Ê†ºÊ®ôÁ±§ÔºåÊàëÂÄëÈñãÁôº‰∫ÜÂèØ‰ª•È´òÁ≤æÂ∫¶È†êÊ∏¨‰∫îÂ§ßÊÄßÊ†ºÁâπË≥™ÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊ∂âÂèä‰ΩøÁî®Â§öÂ±§ÊÑüÁü•Âô® (MLP) ÈõÜÊàêÂíåË∂ÖÂèÉÊï∏Ë™øÊï¥Á≠âÂêÑÁ®ÆÁ≠ñÁï•‰æÜÂæÆË™ø RoBERTa Âü∫Á§éÊ®°Âûã„ÄÇÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊòéÈ°ØÂÑ™ÊñºÂÇ≥Áµ±ÁöÑ‰∫åÂÖÉÂàÜÈ°ûÊñπÊ≥ïÔºåÁÇ∫‰∫∫Ê†ºÁâπË≥™Êèê‰æõ‰∫ÜÁ≤æÁ¢∫ÁöÑÈÄ£Á∫åËº∏Âá∫ÔºåÂæûËÄåÂ¢ûÂº∑‰∫ÜÂú® AI„ÄÅÂøÉÁêÜÂ≠∏„ÄÅ‰∫∫ÂäõË≥áÊ∫ê„ÄÅÁáüÈä∑ÂíåÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑÊáâÁî®„ÄÇ

##### **On Instabilities of Unsupervised Denoising Diffusion Models in Magnetic Resonance Imaging Reconstruction**
2406.16983v1 by Tianyu Han, Sven Nebelung, Firas Khader, Jakob Nikolas Kather, Daniel Truhn

Denoising diffusion models offer a promising approach to accelerating
magnetic resonance imaging (MRI) and producing diagnostic-level images in an
unsupervised manner. However, our study demonstrates that even tiny worst-case
potential perturbations transferred from a surrogate model can cause these
models to generate fake tissue structures that may mislead clinicians. The
transferability of such worst-case perturbations indicates that the robustness
of image reconstruction may be compromised due to MR system imperfections or
other sources of noise. Moreover, at larger perturbation strengths, diffusion
models exhibit Gaussian noise-like artifacts that are distinct from those
observed in supervised models and are more challenging to detect. Our results
highlight the vulnerability of current state-of-the-art diffusion-based
reconstruction models to possible worst-case perturbations and underscore the
need for further research to improve their robustness and reliability in
clinical settings.

ÊëòË¶ÅÔºöÂéªÂô™Êì¥Êï£Ê®°ÂûãÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Âä†ÈÄüÁ£ÅÂÖ±ÊåØÊàêÂÉè (MRI) ‰∏¶‰ª•ÁÑ°Áõ£Áù£ÁöÑÊñπÂºèÁî¢ÁîüË®∫Êñ∑Á¥öÂà•ÁöÑÂΩ±ÂÉè„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÊòØÂæû‰ª£ÁêÜÊ®°ÂûãÂÇ≥Ëº∏ÁöÑÊ•µÂ∞èÊúÄÂ£ûÊÉÖÊ≥ÅÊΩõÂú®ÊìæÂãïÔºå‰πüÊúÉÂ∞éËá¥ÈÄô‰∫õÊ®°ÂûãÁî¢ÁîüÂèØËÉΩË™§Â∞éËá®Â∫äÈÜ´ÁîüÁöÑÂÅáÁµÑÁπîÁµêÊßã„ÄÇÈÄôÁ®ÆÊúÄÂ£ûÊÉÖÊ≥ÅÊìæÂãïÁöÑÂèØÂÇ≥ÈÅûÊÄßË°®ÊòéÔºåÁî±Êñº MR Á≥ªÁµ±Áº∫Èô∑ÊàñÂÖ∂‰ªñÈõúË®ä‰æÜÊ∫êÔºåÂΩ±ÂÉèÈáçÂª∫ÁöÑÁ©©ÂÅ•ÊÄßÂèØËÉΩÊúÉÂèóÂà∞ÊêçÂÆ≥„ÄÇÊ≠§Â§ñÔºåÂú®ËºÉÂ§ßÁöÑÊìæÂãïÂº∑Â∫¶‰∏ãÔºåÊì¥Êï£Ê®°ÂûãÊúÉË°®ÁèæÂá∫ËàáÁõ£Áù£Ê®°Âûã‰∏≠ËßÄÂØüÂà∞ÁöÑ‰∏çÂêåÁöÑÈ´òÊñØÈõúË®äÊ®£ÂºèÁöÑ‰∫∫Â∑•Ë£ΩÂìÅÔºå‰∏¶‰∏îÊõ¥Èõ£‰ª•Ê™¢Ê∏¨„ÄÇÊàëÂÄëÁöÑÁµêÊûúÁ™ÅÈ°Ø‰∫ÜÁï∂ÂâçÊúÄÂÖàÈÄ≤ÁöÑÂü∫ÊñºÊì¥Êï£ÁöÑÈáçÂª∫Ê®°ÂûãÂ∞çÂèØËÉΩÁöÑÊúÄÂ£ûÊÉÖÊ≥ÅÊìæÂãïÁöÑËÑÜÂº±ÊÄßÔºå‰∏¶Âº∑Ë™øÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂‰ª•ÊèêÈ´òÂÖ∂Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÁ©©ÂÅ•ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ

##### **Research on Disease Prediction Model Construction Based on Computer AI deep Learning Technology**
2406.16982v1 by Yang Lin, Muqing Li, Ziyi Zhu, Yinqiu Feng, Lingxi Xiao, Zexi Chen

The prediction of disease risk factors can screen vulnerable groups for
effective prevention and treatment, so as to reduce their morbidity and
mortality. Machine learning has a great demand for high-quality labeling
information, and labeling noise in medical big data poses a great challenge to
efficient disease risk warning methods. Therefore, this project intends to
study the robust learning algorithm and apply it to the early warning of
infectious disease risk. A dynamic truncated loss model is proposed, which
combines the traditional mutual entropy implicit weight feature with the mean
variation feature. It is robust to label noise. A lower bound on training loss
is constructed, and a method based on sampling rate is proposed to reduce the
gradient of suspected samples to reduce the influence of noise on training
results. The effectiveness of this method under different types of noise was
verified by using a stroke screening data set as an example. This method
enables robust learning of data containing label noise.

ÊëòË¶ÅÔºöÁñæÁóÖÈ¢®Èö™Âõ†Â≠êÁöÑÈ†êÊ∏¨ÂèØ‰ª•ÁØ©ÈÅ∏Âá∫ËÑÜÂº±ÊóèÁæ§ÔºåÈÄ≤Ë°åÊúâÊïàÁöÑÈ†êÈò≤ÂíåÊ≤ªÁôÇÔºå‰ª•Èôç‰ΩéÂÖ∂ÁΩπÁóÖÁéáÂíåÊ≠ª‰∫°Áéá„ÄÇÊ©üÂô®Â≠∏ÁøíÂ∞çÊñºÈ´òÂìÅË≥™ÁöÑÊ®ôÁ±§Ë≥áË®äÊúâÊ•µÂ§ßÁöÑÈúÄÊ±ÇÔºåËÄåÈÜ´ÁôÇÂ§ßÊï∏Êìö‰∏≠ÁöÑÊ®ôÁ±§ÈõúË®äÂ∞çÊñºÁñæÁóÖÈ¢®Èö™È†êË≠¶ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÈÄ†ÊàêÊ•µÂ§ßÁöÑÊåëÊà∞„ÄÇÂõ†Ê≠§ÔºåÊú¨Ë®àÁï´Êì¨Êé¢Ë®éÂ∞çÊäóÂºèÂ≠∏ÁøíÊºîÁÆóÊ≥ïÔºå‰∏¶Â∞áÂÖ∂ÊáâÁî®ÊñºÂÇ≥ÊüìÁóÖÈ¢®Èö™ÁöÑÈ†êË≠¶„ÄÇÊèêÂá∫‰∏ÄÂÄãÂãïÊÖãÊà™Êñ∑ÊêçÂ§±Ê®°ÂûãÔºåÂ∞áÂÇ≥Áµ±ÁöÑ‰∫íË≥áË®äÈö±Âê´Ê¨äÈáçÁâπÂæµËàáÂπ≥ÂùáËÆäÁï∞ÁâπÂæµÁµêÂêàÔºåÂ∞çÊ®ôÁ±§ÈõúË®äÂÖ∑ÊúâÈ≠ØÊ£íÊÄß„ÄÇÂª∫ÊßãË®ìÁ∑¥ÊêçÂ§±ÁöÑ‰∏ãÁïåÔºå‰∏¶ÊèêÂá∫Âü∫ÊñºÊäΩÊ®£ÁéáÁöÑÊñπÊ≥ïÔºåÈôç‰ΩéÁñë‰ººÊ®£Êú¨ÁöÑÊ¢ØÂ∫¶Ôºå‰ª•Èôç‰ΩéÈõúË®äÂ∞çË®ìÁ∑¥ÁµêÊûúÁöÑÂΩ±Èüø„ÄÇ‰ª•‰∏≠È¢®ÁØ©Ê™¢Ë≥áÊñôÈõÜÁÇ∫‰æãÔºåÈ©óË≠âÊ≠§ÊñπÊ≥ïÂú®‰∏çÂêåÂûãÊÖãÈõúË®ä‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§ÊñπÊ≥ïËÉΩÂ∞çÂê´ÊúâÊ®ôÁ±§ÈõúË®äÁöÑË≥áÊñôÈÄ≤Ë°åÈ≠ØÊ£íÂ≠∏Áøí„ÄÇ

##### **Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking**
2406.16148v1 by Yuwei Zhang, Tong Xia, Jing Han, Yu Wu, Georgios Rizos, Yang Liu, Mohammed Mosuily, Jagmohan Chauhan, Cecilia Mascolo

Respiratory audio, such as coughing and breathing sounds, has predictive
power for a wide range of healthcare applications, yet is currently
under-explored. The main problem for those applications arises from the
difficulty in collecting large labeled task-specific data for model
development. Generalizable respiratory acoustic foundation models pretrained
with unlabeled data would offer appealing advantages and possibly unlock this
impasse. However, given the safety-critical nature of healthcare applications,
it is pivotal to also ensure openness and replicability for any proposed
foundation model solution. To this end, we introduce OPERA, an OPEn Respiratory
Acoustic foundation model pretraining and benchmarking system, as the first
approach answering this need. We curate large-scale respiratory audio datasets
(~136K samples, 440 hours), pretrain three pioneering foundation models, and
build a benchmark consisting of 19 downstream respiratory health tasks for
evaluation. Our pretrained models demonstrate superior performance (against
existing acoustic models pretrained with general audio on 16 out of 19 tasks)
and generalizability (to unseen datasets and new respiratory audio modalities).
This highlights the great promise of respiratory acoustic foundation models and
encourages more studies using OPERA as an open resource to accelerate research
on respiratory audio for health. The system is accessible from
https://github.com/evelyn0414/OPERA.

ÊëòË¶ÅÔºöÂëºÂê∏Èü≥Ë®äÔºå‰æãÂ¶ÇÂí≥ÂóΩÂíåÂëºÂê∏ËÅ≤ÔºåÂ∞çÊñºÂª£Ê≥õÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®ÂÖ∑ÊúâÈ†êÊ∏¨ËÉΩÂäõÔºå‰ΩÜÁõÆÂâç‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÁöÑÊé¢Ë®é„ÄÇÂ∞çÊñºÈÄô‰∫õÊáâÁî®‰æÜË™™Ôºå‰∏ªË¶ÅÂïèÈ°åÂú®ÊñºÈõ£‰ª•Êî∂ÈõÜÁî®ÊñºÊ®°ÂûãÈñãÁôºÁöÑÂ§ßÈáèÊ®ôË®òÁâπÂÆö‰ªªÂãôË≥áÊñô„ÄÇ‰ΩøÁî®Êú™Ê®ôË®òË≥áÊñôÈ†êÂÖàË®ìÁ∑¥ÁöÑÂèØÊ¶ÇÂåñÂëºÂê∏ËÅ≤Â≠∏Âü∫Á§éÊ®°ÂûãÂ∞áÊèê‰æõÊúâÂê∏ÂºïÂäõÁöÑÂÑ™Âã¢Ôºå‰∏¶ÊúâÂèØËÉΩÊâìÁ†¥ÈÄôÁ®ÆÂÉµÂ±Ä„ÄÇÁÑ∂ËÄåÔºåÈëëÊñºÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®ÁöÑÂÆâÂÖ®ÊÄßËá≥ÈóúÈáçË¶ÅÔºåÂõ†Ê≠§Â∞çÊñº‰ªª‰ΩïÊèêÂá∫ÁöÑÂü∫Á§éÊ®°ÂûãËß£Ê±∫ÊñπÊ°àÔºåÁ¢∫‰øùÈñãÊîæÊÄßÂíåÂèØË§áË£ΩÊÄß‰πüËá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü OPERAÔºå‰∏ÄÂÄãÈñãÊîæÁöÑÂëºÂê∏ËÅ≤Â≠∏Âü∫Á§éÊ®°ÂûãÈ†êË®ìÁ∑¥ÂíåÂü∫Ê∫ñÁ≥ªÁµ±Ôºå‰ΩúÁÇ∫ÊªøË∂≥Ê≠§ÈúÄÊ±ÇÁöÑÁ¨¨‰∏ÄÁ®ÆÊñπÊ≥ï„ÄÇÊàëÂÄëÁ≠ñÂäÉ‰∫ÜÂ§ßË¶èÊ®°ÁöÑÂëºÂê∏Èü≥Ë®äË≥áÊñôÈõÜÔºàÁ¥Ñ 136K ÂÄãÊ®£Êú¨Ôºå440 Â∞èÊôÇÔºâÔºåÈ†êÂÖàË®ìÁ∑¥‰∫Ü‰∏âÂÄãÈñãÂâµÊÄßÁöÑÂü∫Á§éÊ®°ÂûãÔºå‰∏¶Âª∫Á´ã‰∫Ü‰∏ÄÂÄãÂü∫Ê∫ñÔºåÂÖ∂‰∏≠ÂåÖÂê´ 19 ÂÄã‰∏ãÊ∏∏ÂëºÂê∏ÂÅ•Â∫∑‰ªªÂãô‰ª•‰æõË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÈ†êË®ìÁ∑¥Ê®°ÂûãÂ±ïÁ§∫Âá∫ÂçìË∂äÁöÑÊïàËÉΩÔºàÂú® 19 ÂÄã‰ªªÂãô‰∏≠Êúâ 16 ÂÄã‰ªªÂãôÂÑ™Êñº‰ΩøÁî®‰∏ÄËà¨Èü≥Ë®äÈ†êÂÖàË®ìÁ∑¥ÁöÑÁèæÊúâËÅ≤Â≠∏Ê®°ÂûãÔºâÂíåÊ¶ÇÂåñËÉΩÂäõÔºàÂ∞çÊú™Ë¶ãÈÅéÁöÑË≥áÊñôÈõÜÂíåÊñ∞ÁöÑÂëºÂê∏Èü≥Ë®äÊñπÂºèÔºâ„ÄÇÈÄôÁ™ÅÈ°Ø‰∫ÜÂëºÂê∏ËÅ≤Â≠∏Âü∫Á§éÊ®°ÂûãÁöÑÂ∑®Â§ßÂâçÊôØÔºå‰∏¶ÈºìÂãµÊõ¥Â§öÁ†îÁ©∂‰ΩøÁî® OPERA ‰ΩúÁÇ∫ÈñãÊîæË≥áÊ∫êÔºå‰ª•Âä†ÈÄüÂëºÂê∏Èü≥Ë®äÂú®ÂÅ•Â∫∑ÊñπÈù¢ÁöÑÁ†îÁ©∂„ÄÇË©≤Á≥ªÁµ±ÂèØÂæû https://github.com/evelyn0414/OPERA ÂèñÂæó„ÄÇ

##### **Predicting Individual Depression Symptoms from Acoustic Features During Speech**
2406.16000v1 by Sebastian Rodriguez, Sri Harsha Dumpala, Katerina Dikaios, Sheri Rempel, Rudolf Uher, Sageev Oore

Current automatic depression detection systems provide predictions directly
without relying on the individual symptoms/items of depression as denoted in
the clinical depression rating scales. In contrast, clinicians assess each item
in the depression rating scale in a clinical setting, thus implicitly providing
a more detailed rationale for a depression diagnosis. In this work, we make a
first step towards using the acoustic features of speech to predict individual
items of the depression rating scale before obtaining the final depression
prediction. For this, we use convolutional (CNN) and recurrent (long short-term
memory (LSTM)) neural networks. We consider different approaches to learning
the temporal context of speech. Further, we analyze two variants of voting
schemes for individual item prediction and depression detection. We also
include an animated visualization that shows an example of item prediction over
time as the speech progresses.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑËá™ÂãïÊÜÇÈ¨±ÁóáÂÅµÊ∏¨Á≥ªÁµ±ÊúÉÁõ¥Êé•Êèê‰æõÈ†êÊ∏¨ÔºåËÄå‰∏ç‰æùË≥¥Ëá®Â∫äÊÜÇÈ¨±ÁóáË©ïÂàÜÈáèË°®‰∏≠ÊâÄË°®Á§∫ÁöÑÂÄãÂà•ÁóáÁãÄ/È†ÖÁõÆ„ÄÇÁõ∏ËºÉ‰πã‰∏ãÔºåËá®Â∫äÈÜ´ÁîüÊúÉÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠Ë©ï‰º∞ÊÜÇÈ¨±ÁóáË©ïÂàÜÈáèË°®‰∏≠ÁöÑÊØèÂÄãÈ†ÖÁõÆÔºåÂõ†Ê≠§ÊúÉÈö±Âê´Êèê‰æõÊÜÇÈ¨±ÁóáË®∫Êñ∑ÁöÑÊõ¥Ë©≥Á¥∞‰æùÊìö„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË∏èÂá∫Á¨¨‰∏ÄÊ≠•Ôºå‰ΩøÁî®Ë™ûÈü≥ÁöÑÈü≥Ë®äÁâπÂæµ‰æÜÈ†êÊ∏¨ÊÜÇÈ¨±ÁóáË©ïÂàÜÈáèË°®ÁöÑÂÄãÂà•È†ÖÁõÆÔºåÁÑ∂ÂæåÂÜçÂèñÂæóÊúÄÁµÇÁöÑÊÜÇÈ¨±ÁóáÈ†êÊ∏¨„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄë‰ΩøÁî®Âç∑Á©ç (CNN) ÂíåÈÅûËø¥ (Èï∑Áü≠ÊúüË®òÊÜ∂ (LSTM)) Á•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇÊàëÂÄëËÄÉÊÖÆ‰∏çÂêåÁöÑÊñπÊ≥ï‰æÜÂ≠∏ÁøíË™ûÈü≥ÁöÑÊôÇÈñìËÑàÁµ°„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàÜÊûêÂÖ©Á®ÆÊäïÁ•®ÊñπÊ°àÁöÑËÆäÈ´îÔºåÁî®ÊñºÂÄãÂà•È†ÖÁõÆÈ†êÊ∏¨ÂíåÊÜÇÈ¨±ÁóáÂÅµÊ∏¨„ÄÇÊàëÂÄë‰πüÂåÖÂê´‰∏ÄÂÄãÂãïÁï´Ë¶ñË¶∫ÂåñÔºåÈ°ØÁ§∫Èö®ËëóË™ûÈü≥ÈÄ≤Â±ïÔºåÈ†ÖÁõÆÈ†êÊ∏¨ÁöÑ‰∏ÄÂÄãÁØÑ‰æã„ÄÇ

##### **Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care**
2406.15966v1 by Hassan Alhuzali, Ashwag Alasmari

Pre-trained Language Models (PLMs) have the potential to transform mental
health support by providing accessible and culturally sensitive resources.
However, despite this potential, their effectiveness in mental health care and
specifically for the Arabic language has not been extensively explored. To
bridge this gap, this study evaluates the effectiveness of foundational models
for classification of Questions and Answers (Q&A) in the domain of mental
health care. We leverage the MentalQA dataset, an Arabic collection featuring
Q&A interactions related to mental health. In this study, we conducted
experiments using four different types of learning approaches: traditional
feature extraction, PLMs as feature extractors, Fine-tuning PLMs and prompting
large language models (GPT-3.5 and GPT-4) in zero-shot and few-shot learning
settings. While traditional feature extractors combined with Support Vector
Machines (SVM) showed promising performance, PLMs exhibited even better results
due to their ability to capture semantic meaning. For example, MARBERT achieved
the highest performance with a Jaccard Score of 0.80 for question
classification and a Jaccard Score of 0.86 for answer classification. We
further conducted an in-depth analysis including examining the effects of
fine-tuning versus non-fine-tuning, the impact of varying data size, and
conducting error analysis. Our analysis demonstrates that fine-tuning proved to
be beneficial for enhancing the performance of PLMs, and the size of the
training data played a crucial role in achieving high performance. We also
explored prompting, where few-shot learning with GPT-3.5 yielded promising
results. There was an improvement of 12% for question and classification and
45% for answer classification. Based on our findings, it can be concluded that
PLMs and prompt-based approaches hold promise for mental health support in
Arabic.

ÊëòË¶ÅÔºö<paragraph>È†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) ÊúâÊΩõÂäõÈÄèÈÅéÊèê‰æõÂèØÂ≠òÂèñ‰∏îÂÖ∑ÊñáÂåñÊïèÊÑüÂ∫¶ÁöÑË≥áÊ∫ê‰æÜËΩâÂåñÂøÉÁêÜÂÅ•Â∫∑ÊîØÊåÅ„ÄÇ
ÁÑ∂ËÄåÔºåÂÑòÁÆ°ÊúâÊ≠§ÊΩõÂäõÔºåÂÆÉÂÄëÂú®ÂøÉÁêÜ‰øùÂÅ•‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÁâπÂà•ÊòØÂ∞çÊñºÈòøÊãâ‰ºØË™ûÔºåÂ∞öÊú™Âª£Ê≥õÊé¢Ë®é„ÄÇÁÇ∫‰∫ÜÂΩåË£úÊ≠§Â∑ÆË∑ùÔºåÊú¨Á†îÁ©∂Ë©ï‰º∞Âü∫Á§éÊ®°ÂûãÂú®ÂøÉÁêÜ‰øùÂÅ•È†òÂüü‰∏≠Â∞çÂïèÈ°åËàáËß£Á≠î (Q&A) ÈÄ≤Ë°åÂàÜÈ°ûÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÂà©Áî® MentalQA Ë≥áÊñôÈõÜÔºåÈÄôÊòØ‰∏ÄÂÄã‰ª•ÈòøÊãâ‰ºØË™ûÁÇ∫ÁâπËâ≤ÁöÑÈõÜÂêàÔºåÂåÖÂê´ËàáÂøÉÁêÜÂÅ•Â∫∑Áõ∏ÈóúÁöÑ Q&A ‰∫íÂãï„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄë‰ΩøÁî®ÂõõÁ®Æ‰∏çÂêåÈ°ûÂûãÁöÑÂ≠∏ÁøíÊñπÊ≥ïÈÄ≤Ë°åÂØ¶È©óÔºöÂÇ≥Áµ±ÁâπÂæµËêÉÂèñ„ÄÅPLM ‰ΩúÁÇ∫ÁâπÂæµËêÉÂèñÂô®„ÄÅÂæÆË™ø PLMÔºå‰ª•ÂèäÂú®Èõ∂Ê¨°Â≠∏ÁøíÂíåÂ∞ëÊ¨°Â≠∏ÁøíË®≠ÂÆö‰∏≠ÊèêÁ§∫Â§ßÂûãË™ûË®ÄÊ®°Âûã (GPT-3.5 Âíå GPT-4)„ÄÇÈõñÁÑ∂ÂÇ≥Áµ±ÁâπÂæµËêÉÂèñÂô®ÁµêÂêàÊîØÊè¥ÂêëÈáèÊ©ü (SVM) È°ØÁ§∫Âá∫ÊúâÂ∏åÊúõÁöÑÊïàËÉΩÔºå‰ΩÜ PLM Áî±ÊñºËÉΩÂ§†Êì∑ÂèñË™ûÁæ©ÊÑèÁæ©ÔºåÂõ†Ê≠§Ë°®ÁèæÂæóÊõ¥Â•Ω„ÄÇ‰æãÂ¶ÇÔºåMARBERT Âú®ÂïèÈ°åÂàÜÈ°û‰∏≠ÈÅîÂà∞ÊúÄÈ´òÊïàËÉΩÔºåJaccard ÂæóÂàÜÁÇ∫ 0.80ÔºåÂú®Á≠îÊ°àÂàÜÈ°û‰∏≠ Jaccard ÂæóÂàÜÁÇ∫ 0.86„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄ≤Ë°åÊ∑±ÂÖ•ÂàÜÊûêÔºåÂåÖÊã¨Ê™¢Êü•ÂæÆË™øËàáÈùûÂæÆË™øÁöÑÂΩ±Èüø„ÄÅ‰∏çÂêåË≥áÊñôÂ§ßÂ∞èÁöÑÂΩ±ÈüøÔºå‰ª•ÂèäÈÄ≤Ë°åÈåØË™§ÂàÜÊûê„ÄÇÊàëÂÄëÁöÑÂàÜÊûêË°®ÊòéÔºåÂæÆË™øË¢´Ë≠âÊòéÊúâÂä©ÊñºÊèêÂçá PLM ÁöÑÊïàËÉΩÔºåËÄåË®ìÁ∑¥Ë≥áÊñôÁöÑÂ§ßÂ∞èÂú®ÈÅîÊàêÈ´òÊïàËÉΩ‰∏≠ÊâÆÊºîËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÊàëÂÄë‰πüÊé¢Ë®é‰∫ÜÊèêÁ§∫ÔºåÂÖ∂‰∏≠‰ΩøÁî® GPT-3.5 ÁöÑÂ∞ëÊ¨°Â≠∏ÁøíÁî¢ÁîüÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇÂïèÈ°åËàáÂàÜÈ°ûÈÄ≤Ê≠•‰∫Ü 12%ÔºåÁ≠îÊ°àÂàÜÈ°ûÈÄ≤Ê≠•‰∫Ü 45%„ÄÇÊ†πÊìöÊàëÂÄëÁöÑÁôºÁèæÔºåÂèØ‰ª•ÂæóÂá∫ÁµêË´ñÔºåPLM ÂíåÂü∫ÊñºÊèêÁ§∫ÁöÑÊñπÊ≥ïÊúâÊúõÁÇ∫ÈòøÊãâ‰ºØË™ûÁöÑÂøÉÁêÜÂÅ•Â∫∑Êèê‰æõÊîØÊåÅ„ÄÇ</paragraph>

##### **SEDMamba: Enhancing Selective State Space Modelling with Bottleneck Mechanism and Fine-to-Coarse Temporal Fusion for Efficient Error Detection in Robot-Assisted Surgery**
2406.15920v1 by Jialang Xu, Nazir Sirajudeen, Matthew Boal, Nader Francis, Danail Stoyanov, Evangelos Mazomenos

Automated detection of surgical errors can improve robotic-assisted surgery.
Despite promising progress, existing methods still face challenges in capturing
rich temporal context to establish long-term dependencies while maintaining
computational efficiency. In this paper, we propose a novel hierarchical model
named SEDMamba, which incorporates the selective state space model (SSM) into
surgical error detection, facilitating efficient long sequence modelling with
linear complexity. SEDMamba enhances selective SSM with bottleneck mechanism
and fine-to-coarse temporal fusion (FCTF) to detect and temporally localize
surgical errors in long videos. The bottleneck mechanism compresses and
restores features within their spatial dimension, thereby reducing
computational complexity. FCTF utilizes multiple dilated 1D convolutional
layers to merge temporal information across diverse scale ranges, accommodating
errors of varying durations. Besides, we deploy an established observational
clinical human reliability assessment tool (OCHRA) to annotate the errors of
suturing tasks in an open-source radical prostatectomy dataset (SAR-RARP50),
constructing the first frame-level in-vivo surgical error detection dataset to
support error detection in real-world scenarios. Experimental results
demonstrate that our SEDMamba outperforms state-of-the-art methods with at
least 1.82% AUC and 3.80% AP performance gain with significantly reduced
computational complexity.

ÊëòË¶ÅÔºöËá™ÂãïÂÅµÊ∏¨ÊâãË°ìÈåØË™§ÂèØ‰ª•ÊîπÂñÑÊ©üÂô®‰∫∫ËºîÂä©ÊâãË°ì„ÄÇ
ÂÑòÁÆ°Êúâ‰ª§‰∫∫ÊåØÂ•ÆÁöÑÈÄ≤Â±ïÔºåÁèæÊúâÊñπÊ≥ïÂú®ÊçïÊçâË±êÂØåÁöÑÊôÇÈñìËÉåÊôØ‰ª•Âª∫Á´ãÈï∑Êúü‰æùË≥¥Èóú‰øÇÁöÑÂêåÊôÇÔºåÂú®Á∂≠ÊåÅÈÅãÁÆóÊïàÁéáÊñπÈù¢‰ªçÈù¢Ëá®ÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ SEDMamba ÁöÑÊñ∞ÂàÜÂ±§Ê®°ÂûãÔºåÂÆÉÂ∞áÈÅ∏ÊìáÊÄßÁãÄÊÖãÁ©∫ÈñìÊ®°Âûã (SSM) Á¥çÂÖ•ÊâãË°ìÈåØË™§ÂÅµÊ∏¨‰∏≠Ôºå‰øÉÈÄ≤ÂÖ∑ÊúâÁ∑öÊÄßË§áÈõúÂ∫¶ÁöÑÊúâÊïàÈï∑Â∫èÂàóÂª∫Ê®°„ÄÇSEDMamba ‰ª•Áì∂È†∏Ê©üÂà∂ÂíåÁ≤æÁ¥∞Âà∞Á≤óÁï•ÁöÑÊôÇÈñìËûçÂêà (FCTF) Â¢ûÂº∑ÈÅ∏ÊìáÊÄß SSMÔºå‰ª•ÂÅµÊ∏¨ÂíåÊö´ÊôÇÂÆö‰ΩçÈï∑ÂΩ±Áâá‰∏≠ÁöÑÊâãË°ìÈåØË™§„ÄÇÁì∂È†∏Ê©üÂà∂Âú®ÂÆÉÂÄëÁöÑÁ©∫ÈñìÁ∂≠Â∫¶ÂÖßÂ£ìÁ∏ÆÂíåÈÇÑÂéüÁâπÂæµÔºåÂæûËÄåÈôç‰ΩéÈÅãÁÆóË§áÈõúÂ∫¶„ÄÇFCTF ‰ΩøÁî®Â§öÂÄãËÜ®ËÑπÁöÑ 1D Êç≤Á©çÂ±§‰æÜÂêà‰Ωµ‰∏çÂêåÁØÑÂúçÁöÑÊôÇÈñìË≥áË®äÔºåÂÆπÁ¥ç‰∏çÂêåÊåÅÁ∫åÊôÇÈñìÁöÑÈåØË™§„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÉ®ÁΩ≤‰∏ÄÂÄãÂ∑≤Âª∫Á´ãÁöÑËßÄÂØüÊÄßËá®Â∫ä‰∫∫È°ûÂèØÈù†ÊÄßË©ï‰º∞Â∑•ÂÖ∑ (OCHRA) ‰æÜË®ªËß£ÈñãÊ∫êÊ†πÊ≤ªÊÄßÂâçÂàóËÖ∫ÂàáÈô§Ë°ìË≥áÊñôÈõÜ (SAR-RARP50) ‰∏≠Á∏´Âêà‰ªªÂãôÁöÑÈåØË™§ÔºåÂª∫ÊßãÁ¨¨‰∏ÄÂÄãÂπÄÁ¥öÂà•ÁöÑÈ´îÂÖßÊâãË°ìÈåØË™§ÂÅµÊ∏¨Ë≥áÊñôÈõÜÔºå‰ª•ÊîØÊè¥Âú®ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÂÅµÊ∏¨ÈåØË™§„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑ SEDMamba ‰ª•Ëá≥Â∞ë 1.82% AUC Âíå 3.80% AP ÊïàËÉΩÊèêÂçáÔºåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÔºåÂêåÊôÇÂ§ßÂπÖÈôç‰ΩéÈÅãÁÆóË§áÈõúÂ∫¶„ÄÇ

##### **Real-time Speech Summarization for Medical Conversations**
2406.15888v1 by Khai Le-Duc, Khai-Nguyen Nguyen, Long Vo-Dang, Truong-Son Hy

In doctor-patient conversations, identifying medically relevant information
is crucial, posing the need for conversation summarization. In this work, we
propose the first deployable real-time speech summarization system for
real-world applications in industry, which generates a local summary after
every N speech utterances within a conversation and a global summary after the
end of a conversation. Our system could enhance user experience from a business
standpoint, while also reducing computational costs from a technical
perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the
first speech summarization dataset for medical conversations. Thirdly, we are
the first to utilize LLM and human annotators collaboratively to create gold
standard and synthetic summaries for medical conversation summarization.
Finally, we present baseline results of state-of-the-art models on VietMed-Sum.
All code, data (English-translated and Vietnamese) and models are available
online: https://github.com/leduckhai/MultiMed

ÊëòË¶ÅÔºöÂú®ÈÜ´ÁîüÂíåÁóÖ‰∫∫ÁöÑÂ∞çË©±‰∏≠ÔºåË≠òÂà•ËàáÈÜ´ÁôÇÁõ∏ÈóúÁöÑË≥áË®äËá≥ÈóúÈáçË¶ÅÔºåÈÄôÊèêÂá∫‰∫ÜÂ∞çË©±ÊëòË¶ÅÁöÑÈúÄÊ±Ç„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁ¨¨‰∏ÄÂÄãÂèØÈÉ®ÁΩ≤ÁöÑÂç≥ÊôÇË™ûÈü≥ÊëòË¶ÅÁ≥ªÁµ±ÔºåÁî®ÊñºÁî¢Ê•≠‰∏≠ÁöÑÁúüÂØ¶‰∏ñÁïåÊáâÁî®ÔºåÂÆÉÊúÉÂú®Â∞çË©±‰∏≠ÁöÑÊØè N ÂÄãË™ûÈü≥ÁôºË©±ÂæåÁî¢Áîü‰∏ÄÂÄãÂ±ÄÈÉ®ÊëòË¶ÅÔºå‰∏¶Âú®Â∞çË©±ÁµêÊùüÂæåÁî¢Áîü‰∏ÄÂÄãÂÖ®Â±ÄÊëòË¶Å„ÄÇÊàëÂÄëÁöÑÁ≥ªÁµ±ÂèØ‰ª•ÂæûÂïÜÊ•≠ËßíÂ∫¶ÊèêÂçá‰ΩøÁî®ËÄÖÈ´îÈ©óÔºåÂêåÊôÇÂæûÊäÄË°ìËßíÂ∫¶Èôç‰ΩéÈÅãÁÆóÊàêÊú¨„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü VietMed-SumÔºåÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÈáùÂ∞çÈÜ´ÁôÇÂ∞çË©±ÁöÑË™ûÈü≥ÊëòË¶ÅË≥áÊñôÈõÜ„ÄÇÁ¨¨‰∏âÔºåÊàëÂÄëÊòØÁ¨¨‰∏ÄÂÄãÂà©Áî® LLM Âíå‰∫∫Â∑•Ê®ôË®ªËÄÖÂçî‰ΩúÔºåÁÇ∫ÈÜ´ÁôÇÂ∞çË©±ÊëòË¶ÅÂª∫Á´ãÈªÉÈáëÊ®ôÊ∫ñÂíåÂêàÊàêÊëòË¶Å„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü VietMed-Sum ‰∏äÊúÄÂÖàÈÄ≤Ê®°ÂûãÁöÑÂü∫Ê∫ñÁµêÊûú„ÄÇÊâÄÊúâÁ®ãÂºèÁ¢º„ÄÅË≥áÊñôÔºàÂ∑≤ÁøªË≠ØÊàêËã±ÊñáÂíåË∂äÂçóË™ûÔºâÂíåÊ®°ÂûãÈÉΩÂèØ‰ª•Âú®Á∑ö‰∏äÂèñÂæóÔºöhttps://github.com/leduckhai/MultiMed

##### **Automated radiotherapy treatment planning guided by GPT-4Vision**
2406.15609v1 by Sheng Liu, Oscar Pastor-Serrano, Yizheng Chen, Matthew Gopaulchan, Weixing Liang, Mark Buyyounouski, Erqi Pollom, Quynh-Thu Le, Michael Gensheimer, Peng Dong, Yong Yang, James Zou, Lei Xing

Radiotherapy treatment planning is a time-consuming and potentially
subjective process that requires the iterative adjustment of model parameters
to balance multiple conflicting objectives. Recent advancements in large
foundation models offer promising avenues for addressing the challenges in
planning and clinical decision-making. This study introduces GPT-RadPlan, a
fully automated treatment planning framework that harnesses prior radiation
oncology knowledge encoded in multi-modal large language models, such as
GPT-4Vision (GPT-4V) from OpenAI. GPT-RadPlan is made aware of planning
protocols as context and acts as an expert human planner, capable of guiding a
treatment planning process. Via in-context learning, we incorporate clinical
protocols for various disease sites as prompts to enable GPT-4V to acquire
treatment planning domain knowledge. The resulting GPT-RadPlan agent is
integrated into our in-house inverse treatment planning system through an API.
The efficacy of the automated planning system is showcased using multiple
prostate and head & neck cancer cases, where we compared GPT-RadPlan results to
clinical plans. In all cases, GPT-RadPlan either outperformed or matched the
clinical plans, demonstrating superior target coverage and organ-at-risk
sparing. Consistently satisfying the dosimetric objectives in the clinical
protocol, GPT-RadPlan represents the first multimodal large language model
agent that mimics the behaviors of human planners in radiation oncology
clinics, achieving remarkable results in automating the treatment planning
process without the need for additional training.

ÊëòË¶ÅÔºöÊîæÂ∞ÑÊ≤ªÁôÇË®àÁï´ÊòØ‰∏ÄÂÄãËÄóÊôÇ‰∏îÂèØËÉΩ‰∏ªËßÄÁöÑÈÅéÁ®ãÔºåÈúÄË¶ÅÂèçË¶ÜË™øÊï¥Ê®°ÂûãÂèÉÊï∏‰ª•Âπ≥Ë°°Â§öÈáçË°ùÁ™ÅÁöÑÁõÆÊ®ô„ÄÇÂ§ßÂûãÂü∫Á§éÊ®°ÂûãÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÁÇ∫Ëß£Ê±∫Ë¶èÂäÉÂíåËá®Â∫äÊ±∫Á≠ñ‰∏≠ÁöÑÊåëÊà∞Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÈÄîÂæë„ÄÇÊú¨Á†îÁ©∂‰ªãÁ¥π‰∫Ü GPT-RadPlanÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Ëá™ÂãïÂåñÁöÑÊ≤ªÁôÇË®àÁï´Ê°ÜÊû∂ÔºåÂà©Áî®‰∫ÜÁ∑®Á¢ºÂú®Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºà‰æãÂ¶Ç OpenAI ÁöÑ GPT-4Vision (GPT-4V)Ôºâ‰∏≠ÁöÑÂÖàÂâçÁöÑÊîæÂ∞ÑËÖ´Áò§Áü•Ë≠ò„ÄÇGPT-RadPlan ÊÑèË≠òÂà∞Ë¶èÂäÉÂçîÂÆö‰ΩúÁÇ∫ËÉåÊôØÔºå‰∏¶ÊâÆÊºî‰∏Ä‰ΩçÂ∞àÂÆ∂‰∫∫È°ûË¶èÂäÉËÄÖÁöÑËßíËâ≤ÔºåËÉΩÂ§†ÊåáÂ∞éÊ≤ªÁôÇË®àÁï´ÁöÑÈÅéÁ®ã„ÄÇÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏ÁøíÔºåÊàëÂÄëÂ∞áÂêÑÁ®ÆÁñæÁóÖÈÉ®‰ΩçÁöÑËá®Â∫äÂçîÂÆöÁ¥çÂÖ•ÊèêÁ§∫‰∏≠ÔºåËÆì GPT-4V Áç≤ÂèñÊ≤ªÁôÇË®àÁï´È†òÂüüÁöÑÁü•Ë≠ò„ÄÇÁî¢ÁîüÁöÑ GPT-RadPlan ‰ª£ÁêÜÈÄèÈÅé API Êï¥ÂêàÂà∞ÊàëÂÄëÂÖßÈÉ®ÁöÑÈÄÜÂêëÊ≤ªÁôÇË®àÁï´Á≥ªÁµ±‰∏≠„ÄÇËá™ÂãïÂåñË¶èÂäÉÁ≥ªÁµ±ÁöÑÊïàÂäõÈÄèÈÅéÂ§öÂÄãÂâçÂàóËÖ∫ÂíåÈ†≠È†∏ÁôåÊ°à‰æãÂ±ïÁ§∫ÔºåÊàëÂÄëÂ∞á GPT-RadPlan ÁöÑÁµêÊûúËàáËá®Â∫äË®àÁï´ÈÄ≤Ë°åÊØîËºÉ„ÄÇÂú®ÊâÄÊúâÊ°à‰æã‰∏≠ÔºåGPT-RadPlan ÈÉΩÂÑ™ÊñºÊàñÁ¨¶ÂêàËá®Â∫äË®àÁï´ÔºåÂ±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÁõÆÊ®ôË¶ÜËìãÁéáÂíåÂô®ÂÆòÂú®È¢®Èö™‰∏≠ÁöÑ‰øùË≠∑„ÄÇGPT-RadPlan ÊåÅÁ∫åÊªøË∂≥Ëá®Â∫äÂçîÂÆö‰∏≠ÁöÑÂäëÈáèÊ∏¨ÈáèÁõÆÊ®ôÔºå‰ª£Ë°®‰∫ÜÁ¨¨‰∏ÄÂÄãÂ§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰ª£ÁêÜÔºåÂÆÉÊ®°Êì¨‰∫ÜÊîæÂ∞ÑËÖ´Áò§Ë®∫ÊâÄ‰∏≠‰∫∫È°ûË¶èÂäÉËÄÖÁöÑË°åÁÇ∫ÔºåÂú®ÁÑ°ÈúÄÈ°çÂ§ñË®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂú®Ëá™ÂãïÂåñÊ≤ªÁôÇË®àÁï´ÈÅéÁ®ã‰∏≠ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊàêÊûú„ÄÇ

##### **Privacy Preserved Blood Glucose Level Cross-Prediction: An Asynchronous Decentralized Federated Learning Approach**
2406.15346v1 by Chengzhe Piao, Taiyu Zhu, Yu Wang, Stephanie E Baldeweg, Paul Taylor, Pantelis Georgiou, Jiahao Sun, Jun Wang, Kezhi Li

Newly diagnosed Type 1 Diabetes (T1D) patients often struggle to obtain
effective Blood Glucose (BG) prediction models due to the lack of sufficient BG
data from Continuous Glucose Monitoring (CGM), presenting a significant "cold
start" problem in patient care. Utilizing population models to address this
challenge is a potential solution, but collecting patient data for training
population models in a privacy-conscious manner is challenging, especially
given that such data is often stored on personal devices. Considering the
privacy protection and addressing the "cold start" problem in diabetes care, we
propose "GluADFL", blood Glucose prediction by Asynchronous Decentralized
Federated Learning. We compared GluADFL with eight baseline methods using four
distinct T1D datasets, comprising 298 participants, which demonstrated its
superior performance in accurately predicting BG levels for cross-patient
analysis. Furthermore, patients' data might be stored and shared across various
communication networks in GluADFL, ranging from highly interconnected (e.g.,
random, performs the best among others) to more structured topologies (e.g.,
cluster and ring), suitable for various social networks. The asynchronous
training framework supports flexible participation. By adjusting the ratios of
inactive participants, we found it remains stable if less than 70% are
inactive. Our results confirm that GluADFL offers a practical,
privacy-preserving solution for BG prediction in T1D, significantly enhancing
the quality of diabetes management.

ÊëòË¶ÅÔºöÊñ∞Ë®∫Êñ∑ÁöÑ 1 ÂûãÁ≥ñÂ∞øÁóÖ (T1D) ÊÇ£ËÄÖÁî±ÊñºÁº∫‰πè‰æÜËá™ÈÄ£Á∫åË°ÄÁ≥ñÁõ£Ê∏¨ (CGM) ÁöÑË∂≥Â§†Ë°ÄÁ≥ñ (BG) Ë≥áÊñôÔºåÂõ†Ê≠§Â∏∏Â∏∏Èõ£‰ª•ÂèñÂæóÊúâÊïàÁöÑË°ÄÁ≥ñÈ†êÊ∏¨Ê®°ÂûãÔºåÈÄôÂú®ÊÇ£ËÄÖÁÖßË≠∑‰∏≠ÂëàÁèæÂá∫È°ØËëóÁöÑ„ÄåÂÜ∑ÂïüÂãï„ÄçÂïèÈ°å„ÄÇÂà©Áî®ÊóèÁæ§Ê®°Âûã‰æÜÊáâÂ∞çÊ≠§ÊåëÊà∞ÊòØ‰∏ÄÁ®ÆÊΩõÂú®ÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰ΩÜ‰ª•Ê≥®ÈáçÈö±ÁßÅÁöÑÊñπÂºèÊî∂ÈõÜÊÇ£ËÄÖË≥áÊñô‰æÜË®ìÁ∑¥ÊóèÁæ§Ê®°ÂûãÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÁâπÂà•ÊòØÂú®Ê≠§È°ûË≥áÊñôÈÄöÂ∏∏ÂÑ≤Â≠òÂú®ÂÄã‰∫∫Ë£ùÁΩÆ‰∏≠„ÄÇËÄÉÈáèÂà∞Èö±ÁßÅ‰øùË≠∑‰∏¶Ëß£Ê±∫Á≥ñÂ∞øÁóÖÁÖßË≠∑‰∏≠ÁöÑ„ÄåÂÜ∑ÂïüÂãï„ÄçÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫„ÄåGluADFL„ÄçÔºåÈÄèÈÅéÈùûÂêåÊ≠•ÂàÜÊï£ÂºèËÅØÂêàÂ≠∏Áøí‰æÜÈ†êÊ∏¨Ë°ÄÁ≥ñ„ÄÇÊàëÂÄë‰ΩøÁî®ÂõõÂÄã‰∏çÂêåÁöÑ T1D Ë≥áÊñôÈõÜÔºàÂåÖÂê´ 298 ‰ΩçÂèÉËàáËÄÖÔºâÂ∞á GluADFL ËàáÂÖ´Á®ÆÂü∫Ê∫ñÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉÔºåÈÄôË≠âÊòé‰∫ÜÂÖ∂Âú®Ê∫ñÁ¢∫È†êÊ∏¨Ë∑®ÊÇ£ËÄÖÂàÜÊûêÁöÑ BG ÂÄºÊñπÈù¢ÁöÑÂÑ™Áï∞ÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊÇ£ËÄÖÁöÑË≥áÊñôÂèØËÉΩÂÑ≤Â≠òÂú® GluADFL ‰∏≠‰∏¶ÈÄèÈÅéÂêÑÁ®ÆÈÄöË®äÁ∂≤Ë∑ØÂàÜ‰∫´ÔºåÁØÑÂúçÂæûÈ´òÂ∫¶‰∫íÈÄ£Ôºà‰æãÂ¶ÇÔºåÈö®Ê©üÔºåÂú®ÂÖ∂‰ªñÁ∂≤Ë∑Ø‰∏≠Ë°®ÁèæÊúÄ‰Ω≥ÔºâÂà∞Êõ¥ÁµêÊßãÂåñÁöÑÊãìÊí≤Ôºà‰æãÂ¶ÇÔºåÂè¢ÈõÜÂíåÁí∞ÔºâÔºåÈÅ©Áî®ÊñºÂêÑÁ®ÆÁ§æÁæ§Á∂≤Ë∑Ø„ÄÇÈùûÂêåÊ≠•Ë®ìÁ∑¥Êû∂ÊßãÊîØÊè¥ÂΩàÊÄßÂèÉËàá„ÄÇÈÄèÈÅéË™øÊï¥ÈùûÊ¥ªË∫çÂèÉËàáËÄÖÁöÑÊØîÁéáÔºåÊàëÂÄëÁôºÁèæÂ¶ÇÊûúÈùûÊ¥ªË∫çÂèÉËàáËÄÖÂ∞ëÊñº 70%ÔºåÂÆÉ‰ªçÁÑ∂‰øùÊåÅÁ©©ÂÆö„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÂØ¶ GluADFL Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂØ¶Áî®ÁöÑ„ÄÅ‰øùË≠∑Èö±ÁßÅÁöÑ T1D BG È†êÊ∏¨Ëß£Ê±∫ÊñπÊ°àÔºåÂ§ßÂπÖÊèêÂçá‰∫ÜÁ≥ñÂ∞øÁóÖÁÆ°ÁêÜÁöÑÂìÅË≥™„ÄÇ

##### **Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms**
2406.15198v1 by Santiago Berrezueta-Guzman, Mohanad Kandil, Mar√≠a-Luisa Mart√≠n-Ruiz, Iv√°n Pau-de-la-Cruz, Stephan Krusche

Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental
condition characterized by inattention, hyperactivity, and impulsivity, which
can significantly impact an individual's daily functioning and quality of life.
Occupational therapy plays a crucial role in managing ADHD by fostering the
development of skills needed for daily living and enhancing an individual's
ability to participate fully in school, home, and social situations. Recent
studies highlight the potential of integrating Large Language Models (LLMs)
like ChatGPT and Socially Assistive Robots (SAR) to improve psychological
treatments. This integration aims to overcome existing limitations in mental
health therapy by providing tailored support and adapting to the unique needs
of this sensitive group. However, there remains a significant gap in research
exploring the combined use of these advanced technologies in ADHD therapy,
suggesting an opportunity for novel therapeutic approaches.
  Thus, we integrated two advanced language models, ChatGPT-4 Turbo and
Claude-3 Opus, into a robotic assistant to explore how well each model performs
in robot-assisted interactions. Additionally, we have compared their
performance in a simulated therapy scenario to gauge their effectiveness
against a clinically validated customized model. The results of this study show
that ChatGPT-4 Turbo excelled in performance and responsiveness, making it
suitable for time-sensitive applications. Claude-3 Opus, on the other hand,
showed strengths in understanding, coherence, and ethical considerations,
prioritizing safe and engaging interactions. Both models demonstrated
innovation and adaptability, but ChatGPT-4 Turbo offered greater ease of
integration and broader language support. The selection between them hinges on
the specific demands of ADHD therapy.

ÊëòË¶ÅÔºöÊ≥®ÊÑèÂäõ‰∏çË∂≥ÈÅéÂãïÁóá (ADHD) ÊòØ‰∏ÄÁ®ÆÁ•ûÁ∂ìÁôºÂ±ïÁãÄÊ≥ÅÔºåÂÖ∂ÁâπÂæµÁÇ∫Ê≥®ÊÑèÂäõ‰∏çÈõÜ‰∏≠„ÄÅÈÅéÂãïÂíåË°ùÂãïÔºåÂèØËÉΩÊúÉÂ∞çÂÄã‰∫∫ÁöÑÊó•Â∏∏ÁîüÊ¥ªÂäüËÉΩÂíåÁîüÊ¥ªÂìÅË≥™ÈÄ†ÊàêÈáçÂ§ßÂΩ±Èüø„ÄÇËÅ∑ËÉΩÊ≤ªÁôÇÂú®ÁÆ°ÁêÜ ADHD ÊñπÈù¢ÁôºÊèÆËëóËá≥ÈóúÈáçË¶ÅÁöÑ‰ΩúÁî®ÔºåÈÄöÈÅéÂüπÈ§äÊó•Â∏∏ÁîüÊ¥ªÊâÄÈúÄÁöÑÊäÄËÉΩ‰∏¶Â¢ûÂº∑ÂÄã‰∫∫Âú®Â≠∏Ê†°„ÄÅÂÆ∂Â∫≠ÂíåÁ§æ‰∫§Â†¥ÂêàÂÖÖÂàÜÂèÉËàáÁöÑËÉΩÂäõ„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÊï¥ÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºàÂ¶Ç ChatGPTÔºâÂíåÁ§æ‰∫§ËºîÂä©Ê©üÂô®‰∫∫ (SAR) ‰ª•ÊîπÂñÑÂøÉÁêÜÊ≤ªÁôÇÁöÑÊΩõÂäõ„ÄÇÈÄôÁ®ÆÊï¥ÂêàÊó®Âú®ÈÄöÈÅéÊèê‰æõÈáèË∫´ÂÆöÂà∂ÁöÑÊîØÊåÅ‰∏¶ÈÅ©ÊáâÈÄô‰∏ÄÊïèÊÑüÁæ§È´îÁöÑÁç®ÁâπÈúÄÊ±ÇÔºå‰æÜÂÖãÊúçÂøÉÁêÜÂÅ•Â∫∑Ê≤ªÁôÇ‰∏≠ÁèæÊúâÁöÑÈôêÂà∂„ÄÇÁÑ∂ËÄåÔºåÂú®Êé¢Á¥¢ÈÄô‰∫õÂÖàÈÄ≤ÊäÄË°ìÂú® ADHD Ê≤ªÁôÇ‰∏≠ÁöÑÁ∂úÂêàÊáâÁî®ÊñπÈù¢ÔºåÁ†îÁ©∂‰ªçÂ≠òÂú®È°ØËëóÂ∑ÆË∑ùÔºåÈÄôË°®ÊòéÊúâÊ©üÊúÉÊé°Áî®Êñ∞ÁöÑÊ≤ªÁôÇÊñπÊ≥ï„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂ∞áÂÖ©ÂÄãÂÖàÈÄ≤ÁöÑË™ûË®ÄÊ®°Âûã ChatGPT-4 Turbo Âíå Claude-3 Opus Êï¥ÂêàÂà∞Ê©üÂô®‰∫∫Âä©ÊâãÔºå‰ª•Êé¢Á¥¢ÊØèÂÄãÊ®°ÂûãÂú®Ê©üÂô®‰∫∫ËºîÂä©‰∫íÂãï‰∏≠ÁöÑË°®Áèæ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊØîËºÉ‰∫ÜÂÆÉÂÄëÂú®Ê®°Êì¨Ê≤ªÁôÇÂ†¥ÊôØ‰∏≠ÁöÑË°®ÁèæÔºå‰ª•Ë©ï‰º∞ÂÆÉÂÄëÂ∞çËá®Â∫äÈ©óË≠âÁöÑËá™Ë®ÇÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇÊú¨Á†îÁ©∂ÁöÑÁµêÊûúË°®ÊòéÔºåChatGPT-4 Turbo Âú®ÊÄßËÉΩÂíåÈüøÊáâËÉΩÂäõÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩøÂÖ∂ÈÅ©Áî®ÊñºÂ∞çÊôÇÈñìÊïèÊÑüÁöÑÊáâÁî®„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåClaude-3 Opus Âú®ÁêÜËß£„ÄÅÈÄ£Ë≤´ÊÄßÂíåÈÅìÂæ∑ËÄÉÈáèÊñπÈù¢Ë°®ÁèæÂá∫ÂÑ™Âã¢ÔºåÂÑ™ÂÖàËÄÉÊÖÆÂÆâÂÖ®ÂíåÂºï‰∫∫ÂÖ•ÂãùÁöÑ‰∫íÂãï„ÄÇÈÄôÂÖ©ÂÄãÊ®°ÂûãÈÉΩÂ±ïÁ§∫‰∫ÜÂâµÊñ∞ÂíåÈÅ©ÊáâÊÄßÔºå‰ΩÜ ChatGPT-4 Turbo Êèê‰æõ‰∫ÜÊõ¥ËºïÈ¨ÜÁöÑÊï¥ÂêàÂíåÊõ¥Âª£Ê≥õÁöÑË™ûË®ÄÊîØÊåÅ„ÄÇÂÆÉÂÄë‰πãÈñìÁöÑÈÅ∏ÊìáÂèñÊ±∫Êñº ADHD Ê≤ªÁôÇÁöÑÂÖ∑È´îÈúÄÊ±Ç„ÄÇ

##### **This actually looks like that: Proto-BagNets for local and global interpretability-by-design**
2406.15168v2 by Kerol Djoumessi, Bubacarr Bah, Laura K√ºhlewein, Philipp Berens, Lisa Koch

Interpretability is a key requirement for the use of machine learning models
in high-stakes applications, including medical diagnosis. Explaining black-box
models mostly relies on post-hoc methods that do not faithfully reflect the
model's behavior. As a remedy, prototype-based networks have been proposed, but
their interpretability is limited as they have been shown to provide coarse,
unreliable, and imprecise explanations. In this work, we introduce
Proto-BagNets, an interpretable-by-design prototype-based model that combines
the advantages of bag-of-local feature models and prototype learning to provide
meaningful, coherent, and relevant prototypical parts needed for accurate and
interpretable image classification tasks. We evaluated the Proto-BagNet for
drusen detection on publicly available retinal OCT data. The Proto-BagNet
performed comparably to the state-of-the-art interpretable and
non-interpretable models while providing faithful, accurate, and clinically
meaningful local and global explanations. The code is available at
https://github.com/kdjoumessi/Proto-BagNets.

ÊëòË¶ÅÔºöÂèØËß£ÈáãÊÄßÊòØÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂú®È´òÈ¢®Èö™ÊáâÁî®ÔºàÂåÖÊã¨ÈÜ´ÁôÇË®∫Êñ∑Ôºâ‰∏≠‰ΩøÁî®ÁöÑÈóúÈçµË¶ÅÊ±Ç„ÄÇËß£ÈáãÈªëÁõíÊ®°Âûã‰∏ªË¶Å‰æùË≥¥Êñº‰∫ãÂæåÊñπÊ≥ïÔºåËÄåÈÄô‰∫õÊñπÊ≥ïÁÑ°Ê≥ïÂø†ÂØ¶Âú∞ÂèçÊò†Ê®°ÂûãÁöÑË°åÁÇ∫„ÄÇ‰ΩúÁÇ∫Ë£úÊïëÊé™ÊñΩÔºåÂ∑≤Á∂ìÊèêÂá∫‰∫ÜÂü∫ÊñºÂéüÂûãÁöÑÁ∂≤Ë∑ØÔºå‰ΩÜÁî±ÊñºÂÆÉÂÄëÂ∑≤Ë¢´Ë≠âÊòéÊúÉÊèê‰æõÁ≤óÁï•„ÄÅ‰∏çÂèØÈù†‰∏î‰∏çÁ≤æÁ¢∫ÁöÑËß£ÈáãÔºåÂõ†Ê≠§ÂÆÉÂÄëÁöÑÂèØËß£ÈáãÊÄßÂèóÂà∞ÈôêÂà∂„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü Proto-BagNetsÔºåÈÄôÊòØ‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÂü∫ÊñºË®≠Ë®àÁöÑÂéüÂûãÊ®°ÂûãÔºåÂÆÉÁµêÂêà‰∫ÜÂ±ÄÈÉ®ÁâπÂæµÊ®°ÂûãÂíåÂéüÂûãÂ≠∏ÁøíÁöÑÂÑ™ÈªûÔºå‰ª•Êèê‰æõÊ∫ñÁ¢∫‰∏îÂèØËß£ÈáãÁöÑÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãôÊâÄÈúÄÁöÑ„ÄÅÊúâÊÑèÁæ©„ÄÅÈÄ£Ë≤´‰∏îÁõ∏ÈóúÁöÑÂéüÂûãÈÉ®ÂàÜ„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü Proto-BagNet Âú®ÂÖ¨ÈñãÂèØÁî®ÁöÑË¶ñÁ∂≤ËÜú OCT Ë≥áÊñô‰∏äÁöÑÈªÉÊñëÈªûÊ™¢Ê∏¨„ÄÇProto-BagNet ÁöÑË°®ÁèæËàáÊúÄÂÖàÈÄ≤ÁöÑÂèØËß£ÈáãÂíå‰∏çÂèØËß£ÈáãÊ®°ÂûãÁõ∏Áï∂ÔºåÂêåÊôÇÊèê‰æõ‰∫ÜÂø†ÂØ¶„ÄÅÊ∫ñÁ¢∫‰∏îÂú®Ëá®Â∫ä‰∏äÊúâÊÑèÁæ©ÁöÑÂ±ÄÈÉ®ÂíåÊï¥È´îËß£Èáã„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/kdjoumessi/Proto-BagNets ÂèñÂæó„ÄÇ

##### **FA-Net: A Fuzzy Attention-aided Deep Neural Network for Pneumonia Detection in Chest X-Rays**
2406.15117v1 by Ayush Roy, Anurag Bhattacharjee, Diego Oliva, Oscar Ramos-Soto, Francisco J. Alvarez-Padilla, Ram Sarkar

Pneumonia is a respiratory infection caused by bacteria, fungi, or viruses.
It affects many people, particularly those in developing or underdeveloped
nations with high pollution levels, unhygienic living conditions, overcrowding,
and insufficient medical infrastructure. Pneumonia can cause pleural effusion,
where fluids fill the lungs, leading to respiratory difficulty. Early diagnosis
is crucial to ensure effective treatment and increase survival rates. Chest
X-ray imaging is the most commonly used method for diagnosing pneumonia.
However, visual examination of chest X-rays can be difficult and subjective. In
this study, we have developed a computer-aided diagnosis system for automatic
pneumonia detection using chest X-ray images. We have used DenseNet-121 and
ResNet50 as the backbone for the binary class (pneumonia and normal) and
multi-class (bacterial pneumonia, viral pneumonia, and normal) classification
tasks, respectively. We have also implemented a channel-specific spatial
attention mechanism, called Fuzzy Channel Selective Spatial Attention Module
(FCSSAM), to highlight the specific spatial regions of relevant channels while
removing the irrelevant channels of the extracted features by the backbone. We
evaluated the proposed approach on a publicly available chest X-ray dataset,
using binary and multi-class classification setups. Our proposed method
achieves accuracy rates of 97.15\% and 79.79\% for the binary and multi-class
classification setups, respectively. The results of our proposed method are
superior to state-of-the-art (SOTA) methods. The code of the proposed model
will be available at: https://github.com/AyushRoy2001/FA-Net.

ÊëòË¶ÅÔºöËÇ∫ÁÇéÊòØ‰∏ÄÁ®ÆÁî±Á¥∞Ëèå„ÄÅÁúüËèåÊàñÁóÖÊØíÂºïËµ∑ÁöÑÂëºÂê∏ÈÅìÊÑüÊüì„ÄÇ
ÂÆÉÂΩ±ÈüøË®±Â§ö‰∫∫ÔºåÁâπÂà•ÊòØÁôºÂ±ï‰∏≠ÂúãÂÆ∂ÊàñÊú™ÈñãÁôºÂúãÂÆ∂ÔºåÈÄô‰∫õÂúãÂÆ∂Ê±°ÊüìÁ®ãÂ∫¶È´ò„ÄÅÁîüÊ¥ªÊ¢ù‰ª∂‰∏çË°õÁîü„ÄÅ‰∫∫Âè£ÈÅéÊñºÁ®†ÂØÜÔºå‰∏îÈÜ´ÁôÇÂü∫Á§éË®≠ÊñΩ‰∏çË∂≥„ÄÇËÇ∫ÁÇéÊúÉÂ∞éËá¥ËÉ∏ËÖîÁ©çÊ∂≤ÔºåÊ∂≤È´îÊúÉÂÖÖÊªøËÇ∫ÈÉ®ÔºåÂ∞éËá¥ÂëºÂê∏Âõ∞Èõ£„ÄÇÊó©ÊúüË®∫Êñ∑Â∞çÊñºÁ¢∫‰øùÊúâÊïàÊ≤ªÁôÇÂíåÊèêÈ´òÂ≠òÊ¥ªÁéáËá≥ÈóúÈáçË¶Å„ÄÇËÉ∏ÈÉ® X ÂÖâÂΩ±ÂÉèÊ™¢Êü•ÊòØË®∫Êñ∑ËÇ∫ÁÇéÊúÄÂ∏∏Áî®ÁöÑÊñπÊ≥ï„ÄÇ
ÁÑ∂ËÄåÔºåËÉ∏ÈÉ® X ÂÖâÁöÑË¶ñË¶∫Ê™¢Êü•ÂèØËÉΩÂõ∞Èõ£‰∏î‰∏ªËßÄ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÈõªËÖ¶ËºîÂä©Ë®∫Êñ∑Á≥ªÁµ±ÔºåÁî®Êñº‰ΩøÁî®ËÉ∏ÈÉ® X ÂÖâÂΩ±ÂÉèËá™ÂãïÂÅµÊ∏¨ËÇ∫ÁÇé„ÄÇÊàëÂÄë‰ΩøÁî® DenseNet-121 Âíå ResNet50 ‰ΩúÁÇ∫‰∫åÂÖÉÈ°ûÂà•ÔºàËÇ∫ÁÇéÂíåÊ≠£Â∏∏ÔºâÂíåÂ§öÈ°ûÂà•ÔºàÁ¥∞ËèåÊÄßËÇ∫ÁÇé„ÄÅÁóÖÊØíÊÄßËÇ∫ÁÇéÂíåÊ≠£Â∏∏ÔºâÂàÜÈ°û‰ªªÂãôÁöÑ‰∏ªÂππ„ÄÇÊàëÂÄëÈÇÑÂØ¶‰Ωú‰∫Ü‰∏ÄÂÄãÈÄöÈÅìÁâπÂÆöÁöÑÁ©∫ÈñìÊ≥®ÊÑèÂäõÊ©üÂà∂ÔºåÁ®±ÁÇ∫Ê®°Á≥äÈÄöÈÅìÈÅ∏ÊìáÊÄßÁ©∫ÈñìÊ≥®ÊÑèÂäõÊ®°ÁµÑ (FCSSAM)Ôºå‰ª•Á™ÅÈ°ØÁõ∏ÈóúÈÄöÈÅìÁöÑÁâπÂÆöÁ©∫ÈñìÂçÄÂüüÔºåÂêåÊôÇÁßªÈô§‰∏ªÂππÊèêÂèñÁâπÂæµÁöÑÁÑ°ÈóúÈÄöÈÅì„ÄÇÊàëÂÄëÂú®ÂÖ¨ÈñãÁöÑËÉ∏ÈÉ® X ÂÖâË≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºå‰ΩøÁî®‰∫åÂÖÉÂíåÂ§öÈ°ûÂà•ÂàÜÈ°ûË®≠ÂÆö„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®‰∫åÂÖÉÂíåÂ§öÈ°ûÂà•ÂàÜÈ°ûË®≠ÂÆö‰∏≠ÂàÜÂà•ÈÅîÂà∞ 97.15% Âíå 79.79% ÁöÑÊ∫ñÁ¢∫Áéá„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÁµêÊûúÂÑ™ÊñºÊúÄÂÖàÈÄ≤ (SOTA) ÁöÑÊñπÊ≥ï„ÄÇÊâÄÊèêÂá∫Ê®°ÂûãÁöÑÁ®ãÂºèÁ¢ºÂ∞áÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÔºöhttps://github.com/AyushRoy2001/FA-Net„ÄÇ

##### **Tri-VQA: Triangular Reasoning Medical Visual Question Answering for Multi-Attribute Analysis**
2406.15050v1 by Lin Fan, Xun Gong, Cenyang Zheng, Yafei Ou

The intersection of medical Visual Question Answering (Med-VQA) is a
challenging research topic with advantages including patient engagement and
clinical expert involvement for second opinions. However, existing Med-VQA
methods based on joint embedding fail to explain whether their provided results
are based on correct reasoning or coincidental answers, which undermines the
credibility of VQA answers. In this paper, we investigate the construction of a
more cohesive and stable Med-VQA structure. Motivated by causal effect, we
propose a novel Triangular Reasoning VQA (Tri-VQA) framework, which constructs
reverse causal questions from the perspective of "Why this answer?" to
elucidate the source of the answer and stimulate more reasonable forward
reasoning processes. We evaluate our method on the Endoscopic Ultrasound (EUS)
multi-attribute annotated dataset from five centers, and test it on medical VQA
datasets. Experimental results demonstrate the superiority of our approach over
existing methods. Our codes and pre-trained models are available at
https://anonymous.4open.science/r/Tri_VQA.

ÊëòË¶ÅÔºöÈÜ´Â≠∏Ë¶ñË¶∫ÂïèÁ≠î (Med-VQA) ÁöÑ‰∫§ÂèâÈ†òÂüüÊòØ‰∏ÄÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÁ†îÁ©∂‰∏ªÈ°åÔºåÂÖ∂ÂÑ™ÈªûÂåÖÊã¨ÊÇ£ËÄÖÂèÉËàáÂíåËá®Â∫äÂ∞àÂÆ∂ÁöÑÂèÉËàá‰ª•Êèê‰æõÁ¨¨‰∫åÊÑèË¶ã„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂü∫ÊñºËÅØÂêàÂµåÂÖ•ÁöÑ Med-VQA ÊñπÊ≥ïÁÑ°Ê≥ïËß£ÈáãÂÖ∂Êèê‰æõÁöÑÁµêÊûúÊòØÂü∫ÊñºÊ≠£Á¢∫ÁöÑÊé®ÁêÜÈÇÑÊòØÂ∑ßÂêàÁöÑÁ≠îÊ°àÔºåÈÄôÊúÉÊêçÂÆ≥ VQA Á≠îÊ°àÁöÑÂèØ‰ø°Â∫¶„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÊßãÂª∫Êõ¥Á∑äÂØÜ‰∏îÁ©©ÂÆöÁöÑ Med-VQA ÁµêÊßã„ÄÇÂèóÂà∞Âõ†ÊûúÈóú‰øÇÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑ‰∏âËßíÊé®ÁêÜ VQA (Tri-VQA) Ê°ÜÊû∂ÔºåË©≤Ê°ÜÊû∂Âæû„ÄåÁÇ∫‰ªÄÈ∫ºÈÄôÂÄãÁ≠îÊ°àÔºü„ÄçÁöÑËßíÂ∫¶ÊßãÂª∫ÂèçÂêëÂõ†ÊûúÂïèÈ°åÔºå‰ª•Èó°ÊòéÁ≠îÊ°àÁöÑ‰æÜÊ∫ê‰∏¶ÊøÄÁôºÊõ¥ÂêàÁêÜÁöÑÊ≠£ÂêëÊé®ÁêÜÈÅéÁ®ã„ÄÇÊàëÂÄëÂú®‰æÜËá™‰∫îÂÄã‰∏≠ÂøÉÁöÑÂÖßË¶ñÈè°Ë∂ÖÈü≥Ê≥¢ (EUS) Â§öÂ±¨ÊÄßË®ªÈáãË≥áÊñôÈõÜ‰∏äË©ï‰º∞‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÔºå‰∏¶Âú®ÈÜ´Â≠∏ VQA Ë≥áÊñôÈõÜ‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åÊ∏¨Ë©¶„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂÑ™ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÂèØÂú® https://anonymous.4open.science/r/Tri_VQA ÂèñÂæó„ÄÇ

##### **Human-AI collectives produce the most accurate differential diagnoses**
2406.14981v1 by N. Z√∂ller, J. Berger, I. Lin, N. Fu, J. Komarneni, G. Barabucci, K. Laskowski, V. Shia, B. Harack, E. A. Chu, V. Trianni, R. H. J. M. Kurvers, S. M. Herzog

Artificial intelligence systems, particularly large language models (LLMs),
are increasingly being employed in high-stakes decisions that impact both
individuals and society at large, often without adequate safeguards to ensure
safety, quality, and equity. Yet LLMs hallucinate, lack common sense, and are
biased - shortcomings that may reflect LLMs' inherent limitations and thus may
not be remedied by more sophisticated architectures, more data, or more human
feedback. Relying solely on LLMs for complex, high-stakes decisions is
therefore problematic. Here we present a hybrid collective intelligence system
that mitigates these risks by leveraging the complementary strengths of human
experience and the vast information processed by LLMs. We apply our method to
open-ended medical diagnostics, combining 40,762 differential diagnoses made by
physicians with the diagnoses of five state-of-the art LLMs across 2,133
medical cases. We show that hybrid collectives of physicians and LLMs
outperform both single physicians and physician collectives, as well as single
LLMs and LLM ensembles. This result holds across a range of medical specialties
and professional experience, and can be attributed to humans' and LLMs'
complementary contributions that lead to different kinds of errors. Our
approach highlights the potential for collective human and machine intelligence
to improve accuracy in complex, open-ended domains like medical diagnostics.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÔºåÂ∞§ÂÖ∂ÊòØÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå
Ë∂ä‰æÜË∂äÂ∏∏Ë¢´Áî®ÊñºÂΩ±ÈüøÂÄã‰∫∫ÂíåÊï¥ÂÄãÁ§æÊúÉÁöÑÈ´òÈ¢®Èö™Ê±∫Á≠ñÔºå‰ΩÜÈÄöÂ∏∏Ê≤íÊúâË∂≥Â§†ÁöÑÈò≤Ë≠∑Êé™ÊñΩ‰æÜÁ¢∫‰øù
ÂÆâÂÖ®„ÄÅÂìÅË≥™ÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÁÑ∂ËÄåÔºåLLM ÊúÉÁî¢ÁîüÂπªË¶∫„ÄÅÁº∫‰πèÂ∏∏Ë≠òÔºå‰∏¶‰∏îÊúâÂÅèË¶ã - ÈÄô‰∫õÁº∫ÈªûÂèØËÉΩÂèçÊò†Âá∫ LLM Âõ∫ÊúâÁöÑÈôêÂà∂ÔºåÂõ†Ê≠§ÂèØËÉΩÁÑ°Ê≥ïÈÄèÈÅéÊõ¥Á≤æÂØÜÁöÑÊû∂Êßã„ÄÅÊõ¥Â§öË≥áÊñôÊàñÊõ¥Â§ö‰∫∫È°ûÂõûÈ•ã‰æÜË£úÊïë„ÄÇÂõ†Ê≠§ÔºåÂÉÖ‰æùË≥¥ LLM ‰æÜÂÅöÂá∫Ë§áÈõú„ÄÅÈ´òÈ¢®Èö™ÁöÑÊ±∫Á≠ñÊòØÊúâÂïèÈ°åÁöÑ„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ∑∑ÂêàÈõÜÈ´îÊô∫ÊÖßÁ≥ªÁµ±ÔºåÈÄèÈÅéÂà©Áî®‰∫∫È°ûÁ∂ìÈ©óÂíå LLM ËôïÁêÜÁöÑÈæêÂ§ßË≥áË®äÁöÑ‰∫íË£úÂÑ™Âã¢‰æÜÈôç‰ΩéÈÄô‰∫õÈ¢®Èö™„ÄÇÊàëÂÄëÂ∞áÊñπÊ≥ïÊáâÁî®ÊñºÈñãÊîæÂºèÈÜ´ÁôÇË®∫Êñ∑ÔºåÁµêÂêàÈÜ´Â∏´ÂÅöÂá∫ÁöÑ 40,762 ÂÄãÈëëÂà•Ë®∫Êñ∑Ôºå‰ª•Âèä‰∫îÂÄãÊúÄÂÖàÈÄ≤ÁöÑ LLM Âú® 2,133 ÂÄãÈÜ´ÁôÇÊ°à‰æã‰∏≠ÁöÑË®∫Êñ∑„ÄÇÊàëÂÄëË≠âÊòéÔºåÈÜ´Â∏´Âíå LLM ÁöÑÊ∑∑ÂêàÈõÜÈ´îÂÑ™ÊñºÂñÆ‰∏ÄÈÜ´Â∏´ÂíåÈÜ´Â∏´ÈõÜÈ´îÔºå‰ª•ÂèäÂñÆ‰∏Ä LLM Âíå LLM Êï¥Âêà„ÄÇÈÄôÂÄãÁµêÊûúÈÅ©Áî®ÊñºÂêÑÁ®ÆÈÜ´ÁôÇÂ∞àÁßëÂíåÂ∞àÊ•≠Á∂ìÈ©óÔºå‰∏¶‰∏îÂèØ‰ª•Ê≠∏Âõ†Êñº‰∫∫È°ûÂíå LLM ÁöÑ‰∫íË£úË≤¢ÁçªÔºåÂ∞éËá¥‰∏çÂêåÈ°ûÂûãÁöÑÈåØË™§„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÁ™ÅÈ°Ø‰∫ÜÈõÜÈ´î‰∫∫È°ûÂíåÊ©üÂô®Êô∫ÊÖßÂú®ÊîπÂñÑÈÜ´ÁôÇË®∫Êñ∑Á≠âË§áÈõú„ÄÅÈñãÊîæÂºèÈ†òÂüü‰∏≠ÁöÑÊ∫ñÁ¢∫ÊÄßÁöÑÊΩõÂäõ„ÄÇ

##### **Deep Imbalanced Regression to Estimate Vascular Age from PPG Data: a Novel Digital Biomarker for Cardiovascular Health**
2406.14953v1 by Guangkun Nie, Qinghao Zhao, Gongzheng Tang, Jun Li, Shenda Hong

Photoplethysmography (PPG) is emerging as a crucial tool for monitoring human
hemodynamics, with recent studies highlighting its potential in assessing
vascular aging through deep learning. However, real-world age distributions are
often imbalanced, posing significant challenges for deep learning models. In
this paper, we introduce a novel, simple, and effective loss function named the
Dist Loss to address deep imbalanced regression tasks. We trained a
one-dimensional convolutional neural network (Net1D) incorporating the Dist
Loss on the extensive UK Biobank dataset (n=502,389) to estimate vascular age
from PPG signals and validate its efficacy in characterizing cardiovascular
health. The model's performance was validated on a 40% held-out test set,
achieving state-of-the-art results, especially in regions with small sample
sizes. Furthermore, we divided the population into three subgroups based on the
difference between predicted vascular age and chronological age: less than -10
years, between -10 and 10 years, and greater than 10 years. We analyzed the
relationship between predicted vascular age and several cardiovascular events
over a follow-up period of up to 10 years, including death, coronary heart
disease, and heart failure. Our results indicate that the predicted vascular
age has significant potential to reflect an individual's cardiovascular health
status. Our code will be available at https://github.com/Ngk03/AI-vascular-age.

ÊëòË¶ÅÔºöÂÖâÈõªÂÆπÁ©çÊèèË®òÊ≥ï (PPG) Ê≠£ÈÄêÊº∏ÊàêÁÇ∫Áõ£Ê∏¨‰∫∫È´îË°ÄÊµÅÂãïÂäõÂ≠∏ÁöÑ‰∏ÄÈ†ÖÈáçË¶ÅÂ∑•ÂÖ∑ÔºåÊúÄËøëÁöÑÁ†îÁ©∂Âº∑Ë™øÂÖ∂ÈÄèÈÅéÊ∑±Â∫¶Â≠∏ÁøíË©ï‰º∞Ë°ÄÁÆ°ËÄÅÂåñÁöÑÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÁúüÂØ¶‰∏ñÁïåÁöÑÂπ¥ÈΩ°ÂàÜ‰ΩàÈÄöÂ∏∏‰∏çÂπ≥Ë°°ÔºåÂ∞çÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∏ÄÂÄãÊñ∞Á©é„ÄÅÁ∞°ÂñÆ‰∏îÊúâÊïàÁöÑÊêçÂ§±ÂáΩÊï∏ÔºåÁ®±ÁÇ∫ Dist LossÔºå‰ª•Ëß£Ê±∫Ê∑±Â∫¶‰∏çÂπ≥Ë°°Ëø¥Ê≠∏‰ªªÂãô„ÄÇÊàëÂÄëÂú®ÈæêÂ§ßÁöÑËã±ÂúãÁîüÁâ©ÈäÄË°åË≥áÊñôÈõÜ (n=502,389) ‰∏äË®ìÁ∑¥‰∫Ü‰∏ÄÂÄã‰∏ÄÁ∂≠Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (Net1D)ÔºåÁµêÂêà Dist Loss Âæû PPG Ë®äËôü‰º∞Ë®àË°ÄÁÆ°Âπ¥ÈΩ°Ôºå‰∏¶È©óË≠âÂÖ∂Âú®Ë°®ÂæµÂøÉË°ÄÁÆ°ÂÅ•Â∫∑ÁöÑÊïàËÉΩ„ÄÇË©≤Ê®°ÂûãÁöÑÊïàËÉΩÁ∂ìÈÅé 40% ÁöÑÁïôÂá∫Ê∏¨Ë©¶ÈõÜÈ©óË≠âÔºåÁç≤Âæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûúÔºåÁâπÂà•ÊòØÂú®Ê®£Êú¨ÈáèÂ∞èÁöÑÂçÄÂüü„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊ†πÊìöÈ†êÊ∏¨ÁöÑË°ÄÁÆ°Âπ¥ÈΩ°ÂíåÂØ¶ÈöõÂπ¥ÈΩ°‰πãÈñìÁöÑÂ∑ÆÁï∞Â∞á‰∫∫Áæ§ÂàÜÁÇ∫‰∏âÂÄãÂ≠êÁæ§ÔºöÂ∞èÊñº -10 Ê≠≤„ÄÅ‰ªãÊñº -10 Âà∞ 10 Ê≠≤‰πãÈñìÔºå‰ª•ÂèäÂ§ßÊñº 10 Ê≠≤„ÄÇÊàëÂÄëÂàÜÊûê‰∫ÜÈ†êÊ∏¨ÁöÑË°ÄÁÆ°Âπ¥ÈΩ°ËàáÂæåÁ∫åÈï∑ÈÅî 10 Âπ¥ÁöÑÊï∏ÂÄãÂøÉË°ÄÁÆ°‰∫ã‰ª∂‰πãÈñìÁöÑÈóú‰øÇÔºåÂåÖÊã¨Ê≠ª‰∫°„ÄÅÂÜ†ÂøÉÁóÖÂíåÂøÉË°∞Á´≠„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÈ†êÊ∏¨ÁöÑË°ÄÁÆ°Âπ¥ÈΩ°ÂÖ∑ÊúâÂèçÊò†ÂÄã‰∫∫ÂøÉË°ÄÁÆ°ÂÅ•Â∫∑ÁãÄÊ≥ÅÁöÑÈ°ØËëóÊΩõÂäõ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂ∞áÂú® https://github.com/Ngk03/AI-vascular-age Êèê‰æõ„ÄÇ

##### **Extraction of 3D trajectories of mandibular condyles from 2D real-time MRI**
2406.14925v1 by Karyna Isaieva, Justine Lecl√®re, Guillaume Paillart, Guillaume Drouot, Jacques Felblinger, Xavier Dubernard, Pierre-Andr√© Vuissoz

Computing the trajectories of mandibular condyles directly from MRI could
provide a comprehensive examination, allowing for the extraction of both
anatomical and kinematic details. This study aimed to investigate the
feasibility of extracting 3D condylar trajectories from 2D real-time MRI and to
assess their precision.Twenty healthy subjects underwent real-time MRI while
opening and closing their jaws. One axial and two sagittal slices were
segmented using a U-Net-based algorithm. The centers of mass of the resulting
masks were projected onto the coordinate system based on anatomical markers and
temporally adjusted using a common projection. The quality of the computed
trajectories was evaluated using metrics designed to estimate movement
reproducibility, head motion, and slice placement symmetry.The segmentation of
the axial slices demonstrated good-to-excellent quality; however, the
segmentation of the sagittal slices required some fine-tuning. The movement
reproducibility was acceptable for most cases; nevertheless, head motion
displaced the trajectories by 1 mm on average. The difference in the
superior-inferior coordinate of the condyles in the closed jaw position was 1.7
mm on average.Despite limitations in precision, real-time MRI enables the
extraction of condylar trajectories with sufficient accuracy for evaluating
clinically relevant parameters such as condyle displacement, trajectories
aspect, and symmetry.

ÊëòË¶ÅÔºöÈÄèÈÅé MRI Áõ¥Êé•Ë®àÁÆó‰∏ãÈ°éÈ´ÅÁöÑËªåË∑°ÂèØ‰ª•Êèê‰æõÂÖ®Èù¢ÁöÑÊ™¢Êü•Ôºå‰∏¶ÂÖÅË®±ÊèêÂèñËß£ÂâñÂíåÈÅãÂãïÁ¥∞ÁØÄ„ÄÇÊú¨Á†îÁ©∂Êó®Âú®Êé¢Ë®éÂæû 2D Âç≥ÊôÇ MRI ‰∏≠ÊèêÂèñ 3D È´ÅÁãÄËªåË∑°ÁöÑÂèØË°åÊÄßÔºå‰∏¶Ë©ï‰º∞ÂÖ∂Ê∫ñÁ¢∫Â∫¶„ÄÇ‰∫åÂçÅ‰ΩçÂÅ•Â∫∑ÂèóË©¶ËÄÖÂú®ÂºµÈñâ‰∏ãÈ°éÊôÇÊé•ÂèóÂç≥ÊôÇ MRI„ÄÇ‰ΩøÁî®Âü∫Êñº U-Net ÁöÑÊºîÁÆóÊ≥ïÂàÜÂâ≤‰∏ÄÂÄãËª∏ÂêëÂàáÁâáÂíåÂÖ©ÂÄãÁü¢ÁãÄÂàáÁâá„ÄÇÂ∞áÊâÄÂæóÈÅÆÁΩ©ÁöÑË≥™ÂøÉÊäïÂΩ±Âà∞Âü∫ÊñºËß£ÂâñÊ®ôË®òÁöÑÂ∫ßÊ®ôÁ≥ªÁµ±‰∏äÔºå‰∏¶‰ΩøÁî®ÂÖ±ÂêåÊäïÂΩ±ÈÄ≤Ë°åÊôÇÈñìË™øÊï¥„ÄÇË®àÁÆóËªåË∑°ÁöÑÂìÅË≥™‰ΩøÁî®ÊåáÊ®ôÈÄ≤Ë°åË©ï‰º∞ÔºåÈÄô‰∫õÊåáÊ®ôÊó®Âú®‰º∞Ë®àÈÅãÂãïÈáçÁèæÊÄß„ÄÅÈ†≠ÈÉ®ÈÅãÂãïÂíåÂàáÁâáÊîæÁΩÆÂ∞çÁ®±ÊÄß„ÄÇËª∏ÂêëÂàáÁâáÁöÑÂàÜÂâ≤È°ØÁ§∫Âá∫ËâØÂ•ΩÂà∞Ê•µ‰Ω≥ÁöÑÂìÅË≥™ÔºõÁÑ∂ËÄåÔºåÁü¢ÁãÄÂàáÁâáÁöÑÂàÜÂâ≤ÈúÄË¶Å‰∏Ä‰∫õÂæÆË™ø„ÄÇÂú®Â§öÊï∏ÊÉÖÊ≥Å‰∏ãÔºåÈÅãÂãïÈáçÁèæÊÄßÊòØÂèØ‰ª•Êé•ÂèóÁöÑÔºõÁÑ∂ËÄåÔºåÈ†≠ÈÉ®ÈÅãÂãïÂπ≥ÂùáÂ∞áËªåË∑°Áßª‰Ωç 1 mm„ÄÇÈñâÂêà‰∏ãÈ°é‰ΩçÁΩÆ‰∏≠È´ÅÁãÄÁ™ÅÁöÑ‰∏ä‰∏ãÂ∫ßÊ®ôÂ∑ÆÁï∞Âπ≥ÂùáÁÇ∫ 1.7 mm„ÄÇÂÑòÁÆ°Ê∫ñÁ¢∫Â∫¶ÊúâÂÖ∂ÈôêÂà∂ÔºåÂç≥ÊôÇ MRI ËÉΩÂ§†ÊèêÂèñÈ´ÅÁãÄËªåË∑°ÔºåÂÖ∂Ê∫ñÁ¢∫Â∫¶Ë∂≥‰ª•Ë©ï‰º∞‰∏ãÈ°éÈ´Å‰ΩçÁßª„ÄÅËªåË∑°Â§ñËßÄÂíåÂ∞çÁ®±ÊÄßÁ≠âËá®Â∫äÁõ∏ÈóúÂèÉÊï∏„ÄÇ

##### **AI-based Anomaly Detection for Clinical-Grade Histopathological Diagnostics**
2406.14866v1 by Jonas Dippel, Niklas Preni√ül, Julius Hense, Philipp Liznerski, Tobias Winterhoff, Simon Schallenberg, Marius Kloft, Oliver Buchstab, David Horst, Maximilian Alber, Lukas Ruff, Klaus-Robert M√ºller, Frederick Klauschen

While previous studies have demonstrated the potential of AI to diagnose
diseases in imaging data, clinical implementation is still lagging behind. This
is partly because AI models require training with large numbers of examples
only available for common diseases. In clinical reality, however, only few
diseases are common, whereas the majority of diseases are less frequent
(long-tail distribution). Current AI models overlook or misclassify these
diseases. We propose a deep anomaly detection approach that only requires
training data from common diseases to detect also all less frequent diseases.
We collected two large real-world datasets of gastrointestinal biopsies, which
are prototypical of the problem. Herein, the ten most common findings account
for approximately 90% of cases, whereas the remaining 10% contained 56 disease
entities, including many cancers. 17 million histological images from 5,423
cases were used for training and evaluation. Without any specific training for
the diseases, our best-performing model reliably detected a broad spectrum of
infrequent ("anomalous") pathologies with 95.0% (stomach) and 91.0% (colon)
AUROC and generalized across scanners and hospitals. By design, the proposed
anomaly detection can be expected to detect any pathological alteration in the
diagnostic tail of gastrointestinal biopsies, including rare primary or
metastatic cancers. This study establishes the first effective clinical
application of AI-based anomaly detection in histopathology that can flag
anomalous cases, facilitate case prioritization, reduce missed diagnoses and
enhance the general safety of AI models, thereby driving AI adoption and
automation in routine diagnostics and beyond.

ÊëòË¶ÅÔºöÂÑòÁÆ°ÂÖàÂâçÁöÑÁ†îÁ©∂Â∑≤Á∂ìË≠âÊòé AI Âú®ÂΩ±ÂÉèË≥áÊñô‰∏≠Ë®∫Êñ∑ÁñæÁóÖÁöÑÊΩõÂäõÔºå‰ΩÜËá®Â∫äÂØ¶Âãô‰ªçËêΩÂæåË®±Â§ö„ÄÇÈÄôÊòØÈÉ®ÂàÜÂéüÂõ†Âú®Êñº AI Ê®°ÂûãÈúÄË¶ÅÂ§ßÈáèÁØÑ‰æãÈÄ≤Ë°åË®ìÁ∑¥ÔºåËÄåÈÄô‰∫õÁØÑ‰æãÂÉÖÈÅ©Áî®ÊñºÂ∏∏Ë¶ãÁñæÁóÖ„ÄÇÁÑ∂ËÄåÔºåÂú®Ëá®Â∫äÁèæÂØ¶‰∏≠ÔºåÂè™ÊúâÂ∞ëÊï∏ÁñæÁóÖÊòØÂ∏∏Ë¶ãÁöÑÔºåËÄåÂ§ßÂ§öÊï∏ÁñæÁóÖËºÉ‰∏çÂ∏∏Ë¶ãÔºàÈï∑Â∞æÂàÜ‰ΩàÔºâ„ÄÇÁõÆÂâçÁöÑ AI Ê®°ÂûãÊúÉÂøΩÁï•ÊàñÈåØË™§ÂàÜÈ°ûÈÄô‰∫õÁñæÁóÖ„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ∑±Â∫¶Áï∞Â∏∏ÂÅµÊ∏¨ÊñπÊ≥ïÔºåÂÆÉÂè™ÈúÄË¶Å‰æÜËá™Â∏∏Ë¶ãÁñæÁóÖÁöÑË®ìÁ∑¥Ë≥áÊñôÔºåÂ∞±ËÉΩÂÅµÊ∏¨ÊâÄÊúâËºÉ‰∏çÂ∏∏Ë¶ãÁöÑÁñæÁóÖ„ÄÇÊàëÂÄëÊî∂ÈõÜ‰∫ÜÂÖ©ÂÄãÂ§ßÂûãÁöÑËÉÉËÖ∏ÈÅìÂàáÁâáÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÔºåÂÆÉÂÄëÊòØÊ≠§ÂïèÈ°åÁöÑÂÖ∏ÂûãÁØÑ‰æã„ÄÇÂú®Ê≠§ÔºåÊúÄÂ∏∏Ë¶ãÁöÑÂçÅÁ®ÆÁôºÁèæÁ¥Ñ‰ΩîÁóÖ‰æãÁöÑ 90%ÔºåËÄåÂÖ∂È§ò 10% ÂâáÂåÖÂê´ 56 Á®ÆÁñæÁóÖÂØ¶È´îÔºåÂåÖÊã¨Ë®±Â§öÁôåÁóá„ÄÇ1700 Ëê¨Âºµ‰æÜËá™ 5,423 ÂÄãÁóÖ‰æãÁöÑÁµÑÁπîÂ≠∏ÂΩ±ÂÉèÁî®ÊñºË®ìÁ∑¥ÂíåË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÊúÄ‰Ω≥ÊïàËÉΩÊ®°ÂûãÂú®Ê≤íÊúâÈáùÂ∞çÈÄô‰∫õÁñæÁóÖÈÄ≤Ë°å‰ªª‰ΩïÁâπÂÆöË®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂèØÈù†Âú∞ÂÅµÊ∏¨Âà∞Âª£Ê≥õÁöÑÁΩïË¶ãÔºà„ÄåÁï∞Â∏∏„ÄçÔºâÁóÖÁêÜÔºåÂÖ∂ AUROC ÂàÜÂà•ÁÇ∫ 95.0%ÔºàËÉÉÔºâÂíå 91.0%ÔºàÁµêËÖ∏ÔºâÔºå‰∏¶Âª£Ê≥õÊáâÁî®ÊñºÊéÉÊèèÂÑÄÂíåÈÜ´Èô¢„ÄÇ‰æùÊìöË®≠Ë®àÔºåÊâÄÊèêÂá∫ÁöÑÁï∞Â∏∏ÂÅµÊ∏¨È†êË®àÂèØ‰ª•ÂÅµÊ∏¨ËÉÉËÖ∏ÈÅìÂàáÁâáË®∫Êñ∑Â∞æÁ´ØÁöÑ‰ªª‰ΩïÁóÖÁêÜÊÄßÊîπËÆäÔºåÂåÖÊã¨ÁΩïË¶ãÁöÑÂéüÁôºÊÄßÊàñËΩâÁßªÊÄßÁôåÁóá„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Âª∫Á´ã‰∫ÜÁ¨¨‰∏ÄÂÄãÊúâÊïàÁöÑ AI Áï∞Â∏∏ÂÅµÊ∏¨Ëá®Â∫äÊáâÁî®ÔºåÂÆÉÂèØ‰ª•Âú®ÁµÑÁπîÁóÖÁêÜÂ≠∏‰∏≠Ê®ôË®òÁï∞Â∏∏ÁóÖ‰æã„ÄÅ‰øÉÈÄ≤ÁóÖ‰æãÂÑ™ÂÖàÈ†ÜÂ∫è„ÄÅÊ∏õÂ∞ëÊºèË®∫‰∏¶ÊèêÂçá AI Ê®°ÂûãÁöÑÊï¥È´îÂÆâÂÖ®ÊÄßÔºåÂæûËÄåÊé®Âãï AI Âú®Â∏∏Ë¶èË®∫Êñ∑ÂèäÂÖ∂‰ªñÈ†òÂüüÁöÑÊé°Áî®ÂíåËá™ÂãïÂåñ„ÄÇ

##### **ACR: A Benchmark for Automatic Cohort Retrieval**
2406.14780v1 by Dung Ngoc Thai, Victor Ardulov, Jose Ulises Mena, Simran Tiwari, Gleb Erofeev, Ramy Eskander, Karim Tarabishy, Ravi B Parikh, Wael Salloum

Identifying patient cohorts is fundamental to numerous healthcare tasks,
including clinical trial recruitment and retrospective studies. Current cohort
retrieval methods in healthcare organizations rely on automated queries of
structured data combined with manual curation, which are time-consuming,
labor-intensive, and often yield low-quality results. Recent advancements in
large language models (LLMs) and information retrieval (IR) offer promising
avenues to revolutionize these systems. Major challenges include managing
extensive eligibility criteria and handling the longitudinal nature of
unstructured Electronic Medical Records (EMRs) while ensuring that the solution
remains cost-effective for real-world application. This paper introduces a new
task, Automatic Cohort Retrieval (ACR), and evaluates the performance of LLMs
and commercial, domain-specific neuro-symbolic approaches. We provide a
benchmark task, a query dataset, an EMR dataset, and an evaluation framework.
Our findings underscore the necessity for efficient, high-quality ACR systems
capable of longitudinal reasoning across extensive patient databases.

ÊëòË¶ÅÔºöË≠òÂà•ÊÇ£ËÄÖÁæ§È´îÊòØË®±Â§öÈÜ´ÁôÇ‰øùÂÅ•‰ªªÂãôÁöÑÂü∫Á§éÔºåÂåÖÊã¨Ëá®Â∫äË©¶È©óÊãõÂãüÂíåÂõûÈ°ßÊÄßÁ†îÁ©∂„ÄÇÁï∂ÂâçÈÜ´ÁôÇ‰øùÂÅ•ÁµÑÁπî‰∏≠ÁöÑÁæ§È´îÊ™¢Á¥¢ÊñπÊ≥ï‰æùË≥¥ÊñºÁµêÊßãÂåñÊï∏ÊìöÁöÑËá™ÂãïÂåñÊü•Ë©¢Ôºå‰∏¶ÁµêÂêà‰∫∫Â∑•Êï¥ÁêÜÔºåÈÄôÊó¢ËÄóÊôÇÂèàË≤ªÂäõÔºåËÄå‰∏îÂ∏∏Â∏∏ÊúÉÁî¢Áîü‰ΩéÂìÅË≥™ÁöÑÁµêÊûú„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåË≥áË®äÊ™¢Á¥¢ (IR) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÁÇ∫Èù©Êñ∞ÈÄô‰∫õÁ≥ªÁµ±Êèê‰æõ‰∫ÜÊúâÂâçÊôØÁöÑÈÄîÂæë„ÄÇ‰∏ªË¶ÅÁöÑÊåëÊà∞ÂåÖÊã¨ÁÆ°ÁêÜÂª£Ê≥õÁöÑË≥áÊ†ºÊ®ôÊ∫ñÔºå‰ª•ÂèäËôïÁêÜÈùûÁµêÊßãÂåñÈõªÂ≠êÁóÖÊ≠∑ (EMR) ÁöÑÁ∏±ÂêëÊÄßË≥™ÔºåÂêåÊôÇÁ¢∫‰øùËß£Ê±∫ÊñπÊ°àÂú®ÁèæÂØ¶‰∏ñÁïåÁöÑÊáâÁî®‰∏≠‰ªçÁÑ∂ÂÖ∑ÊúâÊàêÊú¨ÊïàÁõä„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÈ†ÖÊñ∞‰ªªÂãôÔºåËá™ÂãïÁæ§È´îÊ™¢Á¥¢ (ACR)Ôºå‰∏¶Ë©ï‰º∞‰∫Ü LLM ÂíåÂïÜÊ•≠„ÄÅÁâπÂÆöÈ†òÂüüÁöÑÁ•ûÁ∂ìÁ¨¶ËôüÊñπÊ≥ïÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂü∫Ê∫ñ‰ªªÂãô„ÄÅ‰∏ÄÂÄãÊü•Ë©¢Ë≥áÊñôÈõÜ„ÄÅ‰∏ÄÂÄã EMR Ë≥áÊñôÈõÜÂíå‰∏ÄÂÄãË©ï‰º∞Êû∂Êßã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÂº∑Ë™ø‰∫ÜÂ∞çÊúâÊïà„ÄÅÈ´òÂìÅË≥™ÁöÑ ACR Á≥ªÁµ±ÁöÑÂøÖË¶ÅÊÄßÔºåÈÄô‰∫õÁ≥ªÁµ±ËÉΩÂ§†Ë∑®Âª£Ê≥õÁöÑÊÇ£ËÄÖË≥áÊñôÂ∫´ÈÄ≤Ë°åÁ∏±ÂêëÊé®ÁêÜ„ÄÇ

##### **A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes**
2406.14757v1 by Syed I. Munzir, Daniel B. Hier, Chelsea Oommen, Michael D. Carrithers

High-throughput phenotyping, the automated mapping of patient signs and
symptoms to standardized ontology concepts, is essential to gaining value from
electronic health records (EHR) in the support of precision medicine. Despite
technological advances, high-throughput phenotyping remains a challenge. This
study compares three computational approaches to high-throughput phenotyping: a
Large Language Model (LLM) incorporating generative AI, a Natural Language
Processing (NLP) approach utilizing deep learning for span categorization, and
a hybrid approach combining word vectors with machine learning. The approach
that implemented GPT-4 (a Large Language Model) demonstrated superior
performance, suggesting that Large Language Models are poised to be the
preferred method for high-throughput phenotyping of physician notes.

ÊëòË¶ÅÔºöÈ´òÈÄöÈáèË°®ÂûãÂàÜÊûêÔºåÂ∞áÊÇ£ËÄÖÁóáÁãÄÂíåÈ´îÂæµËá™ÂãïÂ∞çÊáâÂà∞Ê®ôÊ∫ñÂåñÊú¨È´îÊ¶ÇÂøµÔºåÂ∞çÊñºÂú®Á≤æÊ∫ñÈÜ´ÁôÇ‰∏≠Áç≤ÂèñÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑÔºàEHRÔºâÁöÑÂÉπÂÄºËá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°ÊúâÊäÄË°ìÈÄ≤Â±ïÔºåÈ´òÈÄöÈáèË°®ÂûãÂàÜÊûê‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÊåëÊà∞„ÄÇÊú¨Á†îÁ©∂ÊØîËºÉ‰∫Ü‰∏âÁ®ÆÈ´òÈÄöÈáèË°®ÂûãÂàÜÊûêÁöÑË®àÁÆóÊñπÊ≥ïÔºö‰∏ÄÂÄãÁµêÂêàÁîüÊàêÂºè AI ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ„ÄÅ‰∏ÄÂÄãÂà©Áî®Ê∑±Â∫¶Â≠∏ÁøíÈÄ≤Ë°åË∑®Â∫¶ÂàÜÈ°ûÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÔºàNLPÔºâÊñπÊ≥ïÔºå‰ª•Âèä‰∏ÄÂÄãÁµêÂêàË©ûÂêëÈáèËàáÊ©üÂô®Â≠∏ÁøíÁöÑÊ∑∑ÂêàÊñπÊ≥ï„ÄÇÂØ¶‰Ωú GPT-4Ôºà‰∏ÄÂÄãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºâÁöÑÊñπÊ≥ïË°®ÁèæÂá∫ÂÑ™Áï∞ÁöÑÊïàËÉΩÔºåÈÄôË°®Á§∫Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊúâÊúõÊàêÁÇ∫ÈÜ´Â∏´ÂÇôÂøòÈåÑÁöÑÈ´òÈÄöÈáèË°®ÂûãÂàÜÊûêÈ¶ñÈÅ∏ÊñπÊ≥ï„ÄÇ

##### **An updated overview of radiomics-based artificial intelligence (AI) methods in breast cancer screening and diagnosis**
2406.14735v1 by Reza Elahi, Mahdis Nazari

Current imaging methods for diagnosing BC are associated with limited
sensitivity and specificity and modest positive predictive power. The recent
progress in image analysis using artificial intelligence (AI) has created great
promise to improve breast cancer (BC) diagnosis and subtype differentiation. In
this case, novel quantitative computational methods, such as radiomics, have
been developed to improve the sensitivity and specificity of early BC diagnosis
and classification. The potential of radiomics in improving the diagnostic
efficacy of imaging studies has been shown in several studies. In this review
article, we discuss the radiomics workflow and current hand-crafted radiomics
methods in the diagnosis and classification of BC based on most recent studies
on different imaging modalities, e.g. MRI, mammography, contrast-enhanced
spectral mammography (CESM), ultrasound imaging, and digital breast
tumosynthesis (DBT). We also discuss current challenges and potential
strategies to improve the specificity and sensitivity of radiomics in breast
cancer to help achieve a higher level of BC classification and diagnosis in the
clinical setting. The growing field of AI incorporation with imaging
information has opened a great opportunity to provide a higher level of care
for BC patients.

ÊëòË¶ÅÔºöÁõÆÂâçÁî®ÊñºË®∫Êñ∑‰π≥ÁôåÁöÑÂΩ±ÂÉèÊñπÊ≥ïËàáÊúâÈôêÁöÑÊïèÊÑüÂ∫¶ÂíåÁâπÁï∞Â∫¶‰ª•ÂèäÈÅ©Â∫¶ÁöÑÈôΩÊÄßÈ†êÊ∏¨ËÉΩÂäõÊúâÈóú„ÄÇËøëÊúü‰ΩøÁî®‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÂΩ±ÂÉèÂàÜÊûêÈÄ≤Â±ïÁÇ∫ÊîπÂñÑ‰π≥Áôå (BC) Ë®∫Êñ∑Âíå‰∫ûÂûãÂçÄÂàÜÂâµÈÄ†‰∫ÜÊ•µÂ§ßÁöÑÂ∏åÊúõ„ÄÇÂú®ÈÄôÁ®ÆÊÉÖÊ≥Å‰∏ãÔºåÂ∑≤Á∂ìÈñãÁôºÂá∫Êñ∞ÁöÑÂÆöÈáèË®àÁÆóÊñπÊ≥ïÔºà‰æãÂ¶ÇÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏Ôºâ‰æÜÊèêÈ´òÊó©Êúü‰π≥ÁôåË®∫Êñ∑ÂíåÂàÜÈ°ûÁöÑÊïèÊÑüÂ∫¶ÂíåÁâπÁï∞Â∫¶„ÄÇÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏Âú®ÊèêÈ´òÂΩ±ÂÉèÁ†îÁ©∂Ë®∫Êñ∑ÊïàËÉΩÁöÑÊΩõÂäõÂ∑≤Âú®Â§öÈ†ÖÁ†îÁ©∂‰∏≠ÂæóÂà∞Ë≠âÂØ¶„ÄÇÂú®ÈÄôÁØáË©ïË´ñÊñáÁ´†‰∏≠ÔºåÊàëÂÄëÊ†πÊìö‰∏çÂêåÂΩ±ÂÉèÊ®°ÂºèÔºà‰æãÂ¶Ç MRI„ÄÅ‰π≥ÊàøÊîùÂΩ±„ÄÅÂ∞çÊØîÂ¢ûÂº∑ÂÖâË≠ú‰π≥ÊàøÊîùÂΩ± (CESM)„ÄÅË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÂíåÊï∏‰Ωç‰π≥ÊàøÊñ∑Â±§ÂêàÊàê (DBT)ÔºâÁöÑÊúÄÊñ∞Á†îÁ©∂ÔºåË®éË´ñÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏Â∑•‰ΩúÊµÅÁ®ãÂíåÁõÆÂâçÊâãÂ∑•Ë£Ω‰ΩúÁöÑÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏ÊñπÊ≥ïÂú®‰π≥ÁôåË®∫Êñ∑ÂíåÂàÜÈ°û‰∏≠ÁöÑÊáâÁî®„ÄÇÊàëÂÄë‰πüË®éË´ñ‰∫ÜÁï∂ÂâçÊåëÊà∞ÂíåÊΩõÂú®Á≠ñÁï•Ôºå‰ª•ÊèêÈ´òÊîæÂ∞ÑÁâπÂæµÁµÑÂ≠∏Âú®‰π≥Áôå‰∏≠ÁöÑÁâπÁï∞Â∫¶ÂíåÊïèÊÑüÂ∫¶Ôºå‰ª•ÂçîÂä©Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÈÅîÂà∞Êõ¥È´òÂ±§Á¥öÁöÑ‰π≥ÁôåÂàÜÈ°ûÂíåË®∫Êñ∑„ÄÇÂ∞á AI Á¥çÂÖ•ÂΩ±ÂÉèË≥áË®äÁöÑÈ†òÂüüÊ≠£Âú®ÊàêÈï∑ÔºåÈÄôÁÇ∫Êèê‰æõÊõ¥È´òÂ±§Á¥öÁöÑ‰π≥ÁôåÊÇ£ËÄÖÁÖßË≠∑ÈñãÂïü‰∫Ü‰∏ÄÂÄãÁµï‰Ω≥Ê©üÊúÉ„ÄÇ

##### **This Looks Better than That: Better Interpretable Models with ProtoPNeXt**
2406.14675v1 by Frank Willard, Luke Moffett, Emmanuel Mokel, Jon Donnelly, Stark Guo, Julia Yang, Giyoung Kim, Alina Jade Barnett, Cynthia Rudin

Prototypical-part models are a popular interpretable alternative to black-box
deep learning models for computer vision. However, they are difficult to train,
with high sensitivity to hyperparameter tuning, inhibiting their application to
new datasets and our understanding of which methods truly improve their
performance. To facilitate the careful study of prototypical-part networks
(ProtoPNets), we create a new framework for integrating components of
prototypical-part models -- ProtoPNeXt. Using ProtoPNeXt, we show that applying
Bayesian hyperparameter tuning and an angular prototype similarity metric to
the original ProtoPNet is sufficient to produce new state-of-the-art accuracy
for prototypical-part models on CUB-200 across multiple backbones. We further
deploy this framework to jointly optimize for accuracy and prototype
interpretability as measured by metrics included in ProtoPNeXt. Using the same
resources, this produces models with substantially superior semantics and
changes in accuracy between +1.3% and -1.5%. The code and trained models will
be made publicly available upon publication.

ÊëòË¶ÅÔºöÂéüÂûãÈÉ®ÂàÜÊ®°ÂûãÊòØËÆ°ÁÆóÊú∫ËßÜËßâ‰∏≠‰∏ÄÁßçÊµÅË°åÁöÑÂèØËß£ÈáäÁöÑÈªëÁõíÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÊõø‰ª£ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨ÂæàÈöæËÆ≠ÁªÉÔºåÂØπË∂ÖÂèÇÊï∞Ë∞ÉÊï¥È´òÂ∫¶ÊïèÊÑüÔºåËøôÊäëÂà∂‰∫ÜÂÆÉ‰ª¨Âú®Êñ∞Êï∞ÊçÆÈõÜ‰∏äÁöÑÂ∫îÁî®Ôºå‰ª•ÂèäÊàë‰ª¨ÂØπÁúüÊ≠£ÊèêÈ´òÂÖ∂ÊÄßËÉΩÁöÑÊñπÊ≥ïÁöÑÁêÜËß£„ÄÇ‰∏∫‰∫Ü‰øÉËøõÂØπÂéüÂûãÈÉ®ÂàÜÁΩëÁªú (ProtoPNets) ÁöÑ‰ªîÁªÜÁ†îÁ©∂ÔºåÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊ°ÜÊû∂Êù•ÈõÜÊàêÂéüÂûãÈÉ®ÂàÜÊ®°ÂûãÁöÑÁªÑ‰ª∂‚Äî‚ÄîProtoPNeXt„ÄÇ‰ΩøÁî® ProtoPNeXtÔºåÊàë‰ª¨Ë°®ÊòéÂØπÂéüÂßã ProtoPNet Â∫îÁî®Ë¥ùÂè∂ÊñØË∂ÖÂèÇÊï∞Ë∞ÉÊï¥ÂíåËßíÂ∫¶ÂéüÂûãÁõ∏‰ººÊÄßÂ∫¶ÈáèË∂≥‰ª•Âú®Â§ö‰∏™È™®Âπ≤ÁΩë‰∏ä‰∏∫ CUB-200 ‰∏äÁöÑÂéüÂûãÈÉ®ÂàÜÊ®°Âûã‰∫ßÁîüÊñ∞ÁöÑÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨Ëøõ‰∏ÄÊ≠•ÈÉ®ÁΩ≤Ê≠§Ê°ÜÊû∂‰ª•Ê†πÊçÆ ProtoPNeXt ‰∏≠ÂåÖÂê´ÁöÑÊåáÊ†áÂÖ±Âêå‰ºòÂåñÂáÜÁ°ÆÊÄßÂíåÂéüÂûãÂèØËß£ÈáäÊÄß„ÄÇ‰ΩøÁî®Áõ∏ÂêåÁöÑËµÑÊ∫êÔºåËøô‰ºö‰∫ßÁîüÂÖ∑ÊúâÊòéÊòæ‰ºòË∂äÁöÑËØ≠‰πâÂíåÂáÜÁ°ÆÊÄßÂèòÂåñÁöÑÊ®°ÂûãÔºå‰ªã‰∫é +1.3% Âíå -1.5% ‰πãÈó¥„ÄÇ‰ª£Á†ÅÂíåËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂ∞ÜÂú®ÂèëÂ∏ÉÂêéÂÖ¨ÂºÄ„ÄÇ

##### **Computation-Efficient Semi-Supervised Learning for ECG-based Cardiovascular Diseases Detection**
2406.14377v1 by Rushuang Zhou, Zijun Liu, Lei Clifton, David A. Clifton, Kannie W. Y. Chan, Yuan-Ting Zhang, Yining Dong

Label scarcity problem is the main challenge that hinders the wide
application of deep learning systems in automatic cardiovascular diseases
(CVDs) detection using electrocardiography (ECG). Tuning pre-trained models
alleviates this problem by transferring knowledge learned from large datasets
to downstream small datasets. However, bottlenecks in computational efficiency
and CVDs detection performance limit its clinical applications. It is difficult
to improve the detection performance without significantly sacrificing model
computational efficiency. Here, we propose a computation-efficient
semi-supervised learning paradigm (FastECG) for robust and
computation-efficient CVDs detection using ECG. It enables a robust adaptation
of pre-trained models on downstream datasets with limited supervision and high
computational efficiency. First, a random-deactivation technique is developed
to achieve robust and fast low-rank adaptation of pre-trained weights.
Subsequently, we propose a one-shot rank allocation module to determine the
optimal ranks for the update matrices of the pre-trained weights. Finally, a
lightweight semi-supervised learning pipeline is introduced to enhance model
performance by leveraging labeled and unlabeled data with high computational
efficiency. Extensive experiments on four downstream ECG datasets demonstrate
that FastECG not only outperforms the state-of-the-art methods in multi-label
CVDs detection but also consumes fewer GPU footprints, training time, and
parameter storage space. As such, this paradigm provides an effective solution
for achieving high computational efficiency and robust detection performance in
the clinical applications of pre-trained models under limited supervision.

ÊëòË¶ÅÔºöÊ®ôÁ±§Á®ÄÁº∫ÂïèÈ°åÊòØÈòªÁ§ôÊ∑±Â∫¶Â≠∏ÁøíÁ≥ªÁµ±Âú®Ëá™ÂãïÂøÉË°ÄÁÆ°ÁñæÁóÖ (CVD) ‰ΩøÁî®ÂøÉÈõªÂúñ (ECG) Ê™¢Ê∏¨‰∏≠Âª£Ê≥õÊáâÁî®‰πã‰∏ªË¶ÅÊåëÊà∞„ÄÇË™øÊï¥È†êË®ìÁ∑¥Ê®°ÂûãÈÄèÈÅéÂ∞áÂæûÂ§ßÂûãË≥áÊñôÈõÜÂ≠∏Âà∞ÁöÑÁü•Ë≠òËΩâÁßªÂà∞‰∏ãÊ∏∏Â∞èÂûãË≥áÊñôÈõÜÔºå‰æÜÁ∑©Ëß£Ê≠§ÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåÈÅãÁÆóÊïàÁéáÂíå CVD Ê™¢Ê∏¨ÊïàËÉΩÁöÑÁì∂È†∏ÈôêÂà∂‰∫ÜÂÖ∂Ëá®Â∫äÊáâÁî®„ÄÇÂú®‰∏çÈ°ØËëóÁäßÁâ≤Ê®°ÂûãÈÅãÁÆóÊïàÁéáÁöÑÊÉÖÊ≥Å‰∏ãÔºåÈõ£‰ª•ÊîπÂñÑÊ™¢Ê∏¨ÊïàËÉΩ„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÈÅãÁÆóÊïàÁéáÁöÑÂçäÁõ£Áù£Â≠∏ÁøíÁØÑ‰æã (FastECG)ÔºåÁî®Êñº‰ΩøÁî® ECG ÈÄ≤Ë°åÁ©©ÂÅ•‰∏îÈÅãÁÆóÊïàÁéáÈ´òÁöÑ CVD Ê™¢Ê∏¨„ÄÇÂÆÉËÉΩËÆìÈ†êË®ìÁ∑¥Ê®°ÂûãÂú®Áõ£Áù£ÊúâÈôê‰∏îÈÅãÁÆóÊïàÁéáÈ´òÁöÑ‰∏ãÊ∏∏Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁ©©ÂÅ•ÁöÑË™øÊï¥„ÄÇÈ¶ñÂÖàÔºåÈñãÁôºÂá∫‰∏ÄÁ®ÆÈö®Ê©üÂÅúÁî®ÊäÄË°ìÔºå‰ª•ÈÅîÊàêÈ†êË®ìÁ∑¥Ê¨äÈáçÁöÑÁ©©ÂÅ•‰∏îÂø´ÈÄüÁöÑ‰ΩéÁß©Ë™øÊï¥„ÄÇÊé•ËëóÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰∏ÄÊ¨°ÊÄßÁß©ÈÖçÁΩÆÊ®°ÁµÑÔºåÁî®ÊñºÁ¢∫ÂÆöÈ†êË®ìÁ∑¥Ê¨äÈáçÁöÑÊõ¥Êñ∞Áü©Èô£‰πãÊúÄ‰Ω≥Áß©„ÄÇÊúÄÂæåÔºåÂºïÂÖ•‰∏ÄÂÄãËºïÈáèÁ¥öÁöÑÂçäÁõ£Áù£Â≠∏ÁøíÁÆ°Á∑öÔºå‰ª•Âà©Áî®Ê®ôÁ±§ÂíåÊú™Ê®ôÁ±§Ë≥áÊñôÔºå‰∏¶Âú®È´òÈÅãÁÆóÊïàÁéá‰∏ãÊèêÂçáÊ®°ÂûãÊïàËÉΩ„ÄÇÂú®ÂõõÂÄã‰∏ãÊ∏∏ ECG Ë≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåFastECG ‰∏çÂÉÖÂú®Â§öÊ®ôÁ±§ CVD Ê™¢Ê∏¨‰∏≠ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÔºåËÄå‰∏îÊ∂àËÄóÊõ¥Â∞ëÁöÑ GPU Âç†Áî®Á©∫Èñì„ÄÅË®ìÁ∑¥ÊôÇÈñìÂíåÂèÉÊï∏ÂÑ≤Â≠òÁ©∫Èñì„ÄÇÂõ†Ê≠§ÔºåÊ≠§ÁØÑ‰æãÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÊïàÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºÂú®Áõ£Áù£ÊúâÈôêÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊñºÈ†êË®ìÁ∑¥Ê®°ÂûãÁöÑËá®Â∫äÊáâÁî®‰∏≠ÈÅîÊàêÈ´òÈÅãÁÆóÊïàÁéáÂíåÁ©©ÂÅ•ÁöÑÊ™¢Ê∏¨ÊïàËÉΩ„ÄÇ

##### **Automatic Labels are as Effective as Manual Labels in Biomedical Images Classification with Deep Learning**
2406.14351v1 by Niccol√≤ Marini, Stefano Marchesin, Lluis Borras Ferris, Simon P√ºttmann, Marek Wodzinski, Riccardo Fratti, Damian Podareanu, Alessandro Caputo, Svetla Boytcheva, Simona Vatrano, Filippo Fraggetta, Iris Nagtegaal, Gianmaria Silvello, Manfredo Atzori, Henning M√ºller

The increasing availability of biomedical data is helping to design more
robust deep learning (DL) algorithms to analyze biomedical samples. Currently,
one of the main limitations to train DL algorithms to perform a specific task
is the need for medical experts to label data. Automatic methods to label data
exist, however automatic labels can be noisy and it is not completely clear
when automatic labels can be adopted to train DL models. This paper aims to
investigate under which circumstances automatic labels can be adopted to train
a DL model on the classification of Whole Slide Images (WSI). The analysis
involves multiple architectures, such as Convolutional Neural Networks (CNN)
and Vision Transformer (ViT), and over 10000 WSIs, collected from three use
cases: celiac disease, lung cancer and colon cancer, which one including
respectively binary, multiclass and multilabel data. The results allow
identifying 10% as the percentage of noisy labels that lead to train
competitive models for the classification of WSIs. Therefore, an algorithm
generating automatic labels needs to fit this criterion to be adopted. The
application of the Semantic Knowledge Extractor Tool (SKET) algorithm to
generate automatic labels leads to performance comparable to the one obtained
with manual labels, since it generates a percentage of noisy labels between
2-5%. Automatic labels are as effective as manual ones, reaching solid
performance comparable to the one obtained training models with manual labels.

ÊëòË¶ÅÔºöÈö®ËëóÁîüÁâ©ÈÜ´Â≠∏Ë≥áÊñôÁöÑÊó•ÁõäÊôÆÂèäÔºåÊúâÂä©ÊñºË®≠Ë®àÊõ¥Á©©ÂÅ•ÁöÑÊ∑±Â∫¶Â≠∏Áøí (DL) ÊºîÁÆóÊ≥ï‰æÜÂàÜÊûêÁîüÁâ©ÈÜ´Â≠∏Ê®£Êú¨„ÄÇÁõÆÂâçÔºåË®ìÁ∑¥ DL ÊºîÁÆóÊ≥ïÂü∑Ë°åÁâπÂÆö‰ªªÂãôÁöÑ‰∏ªË¶ÅÈôêÂà∂‰πã‰∏ÄÂú®ÊñºÈÜ´Â≠∏Â∞àÂÆ∂Ê®ôË®òË≥áÊñôÁöÑÈúÄÊ±Ç„ÄÇÊ®ôË®òË≥áÊñôÁöÑËá™ÂãïÂåñÊñπÊ≥ïÁ¢∫ÂØ¶Â≠òÂú®ÔºåÁÑ∂ËÄåËá™ÂãïÂåñÊ®ôÁ±§ÂèØËÉΩÊúÉÁî¢ÁîüÈõúË®äÔºåËÄå‰∏îÂ∞ö‰∏çÊ∏ÖÊ•ö‰ΩïÊôÇÂèØ‰ª•Êé°Áî®Ëá™ÂãïÂåñÊ®ôÁ±§‰æÜË®ìÁ∑¥ DL Ê®°Âûã„ÄÇÊú¨ÊñáÊó®Âú®Êé¢Ë®éÂú®‰ΩïÁ®ÆÊÉÖÊ≥Å‰∏ãÂèØ‰ª•Êé°Áî®Ëá™ÂãïÂåñÊ®ôÁ±§‰æÜË®ìÁ∑¥ DL Ê®°ÂûãÂ∞çÂÖ®ÂàáÁâáÂΩ±ÂÉè (WSI) ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÂàÜÊûêÊ∂âÂèäÂ§öÁ®ÆÊû∂ÊßãÔºå‰æãÂ¶ÇÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåË¶ñË¶∫Transformer (ViT)Ôºå‰ª•ÂèäË∂ÖÈÅé 10000 ÂÄã WSIÔºåÈÄô‰∫õ WSI ‰æÜËá™‰∏âÁ®Æ‰ΩøÁî®Ê°à‰æãÔºö‰π≥Á≥úÁÄâ„ÄÅËÇ∫ÁôåÂíåÁµêËÖ∏ÁôåÔºåÂÖ∂‰∏≠‰∏ÄÂÄãÂàÜÂà•ÂåÖÊã¨‰∫åÂÖÉ„ÄÅÂ§öÈ°ûÂíåÂ§öÊ®ôÁ±§Ë≥áÊñô„ÄÇÁµêÊûúÂèØ‰ª•Â∞áÁî¢ÁîüÈõúË®äÊ®ôÁ±§ÁöÑÊØî‰æãÁ¢∫ÂÆöÁÇ∫ 10%ÔºåÈÄôÂ∞áÂ∞éËá¥Ë®ìÁ∑¥Âá∫ÂÖ∑ÊúâÁ´∂Áà≠ÂäõÁöÑ WSI ÂàÜÈ°ûÊ®°Âûã„ÄÇÂõ†Ê≠§ÔºåÁî¢ÁîüËá™ÂãïÂåñÊ®ôÁ±§ÁöÑÊºîÁÆóÊ≥ïÈúÄË¶ÅÁ¨¶ÂêàÊ≠§Ê∫ñÂâáÊâçËÉΩË¢´Êé°Áî®„ÄÇÂ∞áË™ûÁæ©Áü•Ë≠òËêÉÂèñÂ∑•ÂÖ∑ (SKET) ÊºîÁÆóÊ≥ïÊáâÁî®ÊñºÁî¢ÁîüËá™ÂãïÂåñÊ®ôÁ±§ÔºåÂÖ∂ÊïàËÉΩÂèØËàá‰ΩøÁî®‰∫∫Â∑•Ê®ôÁ±§Áç≤ÂæóÁöÑÊïàËÉΩÁõ∏Â™≤ÁæéÔºåÂõ†ÁÇ∫ÂÆÉÁî¢ÁîüÁöÑÈõúË®äÊ®ôÁ±§ÊØî‰æãÂú® 2-5% ‰πãÈñì„ÄÇËá™ÂãïÂåñÊ®ôÁ±§Ëàá‰∫∫Â∑•Ê®ôÁ±§‰∏ÄÊ®£ÊúâÊïàÔºåÂèØÈÅîÂà∞Ëàá‰ΩøÁî®‰∫∫Â∑•Ê®ôÁ±§Ë®ìÁ∑¥Ê®°ÂûãÊâÄÁç≤ÂæóÁöÑÊïàËÉΩÁõ∏Áï∂ÁöÑÁ©©ÂÅ•ÊïàËÉΩ„ÄÇ

##### **Infusing clinical knowledge into tokenisers for language models**
2406.14312v1 by Abul Hasan, Jinge Wu, Quang Ngoc Nguyen, Salom√© Andres, Imane Guellil, Huayu Zhang, Arlene Casey, Beatrice Alex, Bruce Guthrie, Honghan Wu

This study introduces a novel knowledge enhanced tokenisation mechanism,
K-Tokeniser, for clinical text processing. Technically, at initialisation
stage, K-Tokeniser populates global representations of tokens based on semantic
types of domain concepts (such as drugs or diseases) from either a domain
ontology like Unified Medical Language System or the training data of the task
related corpus. At training or inference stage, sentence level localised
context will be utilised for choosing the optimal global token representation
to realise the semantic-based tokenisation. To avoid pretraining using the new
tokeniser, an embedding initialisation approach is proposed to generate
representations for new tokens. Using three transformer-based language models,
a comprehensive set of experiments are conducted on four real-world datasets
for evaluating K-Tokeniser in a wide range of clinical text analytics tasks
including clinical concept and relation extraction, automated clinical coding,
clinical phenotype identification, and clinical research article
classification. Overall, our models demonstrate consistent improvements over
their counterparts in all tasks. In particular, substantial improvements are
observed in the automated clinical coding task with 13\% increase on Micro
$F_1$ score. Furthermore, K-Tokeniser also shows significant capacities in
facilitating quicker converge of language models. Specifically, using
K-Tokeniser, the language models would only require 50\% of the training data
to achieve the best performance of the baseline tokeniser using all training
data in the concept extraction task and less than 20\% of the data for the
automated coding task. It is worth mentioning that all these improvements
require no pre-training process, making the approach generalisable.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÂºïÂÖ•‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÁü•Ë≠òÂ¢ûÂº∑Ê®ôË®òÂåñÊ©üÂà∂ÔºåK-TokeniserÔºåÁî®ÊñºËá®Â∫äÊñáÊú¨ËôïÁêÜ„ÄÇÊäÄË°ì‰∏äÔºåÂú®ÂàùÂßãÂåñÈöéÊÆµÔºåK-Tokeniser ÊúÉÊ†πÊìö‰æÜËá™È†òÂüüÊ¶ÇÂøµÔºà‰æãÂ¶ÇËó•Áâ©ÊàñÁñæÁóÖÔºâÁöÑË™ûÁæ©È°ûÂûãÔºåÂæûÁµ±‰∏ÄÈÜ´Â≠∏Ë™ûË®ÄÁ≥ªÁµ±Êàñ‰ªªÂãôÁõ∏ÈóúË™ûÊñôÂ∫´ÁöÑË®ìÁ∑¥Ë≥áÊñô‰∏≠ÔºåÂ°´ÂÖÖÊ®ôË®òÁöÑÂÖ®Â±ÄË°®Á§∫„ÄÇÂú®Ë®ìÁ∑¥ÊàñÊé®Ë´ñÈöéÊÆµÔºåÂè•Â≠êÁ¥öÂà•ÁöÑÂ±ÄÈÉ®Âåñ‰∏ä‰∏ãÊñáÂ∞áË¢´Áî®ÊñºÈÅ∏ÊìáÊúÄ‰Ω≥ÁöÑÂÖ®Â±ÄÊ®ôË®òË°®Á§∫Ôºå‰ª•ÂØ¶ÁèæÂü∫ÊñºË™ûÁæ©ÁöÑÊ®ôË®òÂåñ„ÄÇÁÇ∫‰∫ÜÈÅøÂÖç‰ΩøÁî®Êñ∞ÁöÑÊ®ôË®òÂåñÂô®ÈÄ≤Ë°åÈ†êË®ìÁ∑¥ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂµåÂÖ•ÂàùÂßãÂåñÊñπÊ≥ïÔºå‰ª•Áî¢ÁîüÊñ∞Ê®ôË®òÁöÑË°®Á§∫„ÄÇ‰ΩøÁî®‰∏âÁ®ÆÂü∫ÊñºTransformerÁöÑË™ûË®ÄÊ®°ÂûãÔºåÂ∞çÂõõÂÄãÁúüÂØ¶‰∏ñÁïåÊï∏ÊìöÈõÜÈÄ≤Ë°å‰∫Ü‰∏ÄÁµÑÂÖ®Èù¢ÁöÑÂØ¶È©óÔºå‰ª•Ë©ï‰º∞ K-Tokeniser Âú®Âª£Ê≥õÁöÑËá®Â∫äÊñáÊú¨ÂàÜÊûê‰ªªÂãô‰∏≠ÁöÑË°®ÁèæÔºåÂåÖÊã¨Ëá®Â∫äÊ¶ÇÂøµÂíåÈóú‰øÇÊèêÂèñ„ÄÅËá™ÂãïËá®Â∫äÁ∑®Á¢º„ÄÅËá®Â∫äË°®ÂûãË≠òÂà•ÂíåËá®Â∫äÁ†îÁ©∂ÊñáÁ´†ÂàÜÈ°û„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊâÄÊúâ‰ªªÂãô‰∏≠ÈÉΩÂ±ïÁ§∫Âá∫ÊØîÂÖ∂Â∞çÊáâÊ®°ÂûãÊõ¥‰∏ÄËá¥ÁöÑÊîπÈÄ≤„ÄÇÁâπÂà•ÊòØÔºåÂú®Ëá™ÂãïËá®Â∫äÁ∑®Á¢º‰ªªÂãô‰∏≠ËßÄÂØüÂà∞‰∫ÜÈ°ØËëóÁöÑÊîπÈÄ≤ÔºåMicro $F_1$ ÂæóÂàÜÊèêÈ´ò‰∫Ü 13%„ÄÇÊ≠§Â§ñÔºåK-Tokeniser ÈÇÑÈ°ØÁ§∫Âá∫È°ØËëóÁöÑËÉΩÂäõÔºåÂèØ‰ª•‰øÉÈÄ≤Ë™ûË®ÄÊ®°ÂûãÊõ¥Âø´ÁöÑÊî∂ÊñÇ„ÄÇÂÖ∑È´î‰æÜË™™Ôºå‰ΩøÁî® K-TokeniserÔºåË™ûË®ÄÊ®°ÂûãÂè™ÈúÄË¶Å 50% ÁöÑË®ìÁ∑¥Êï∏ÊìöÂç≥ÂèØÂú®Ê¶ÇÂøµÊèêÂèñ‰ªªÂãô‰∏≠ÈÅîÂà∞Âü∫Á∑öÊ®ôË®òÂåñÂô®‰ΩøÁî®ÊâÄÊúâË®ìÁ∑¥Êï∏ÊìöÁöÑÊúÄ‰Ω≥ÊÄßËÉΩÔºåËÄåËá™ÂãïÁ∑®Á¢º‰ªªÂãôÂâá‰∏çÂà∞ 20% ÁöÑÊï∏Êìö„ÄÇÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºåÊâÄÊúâÈÄô‰∫õÊîπÈÄ≤ÈÉΩ‰∏çÈúÄË¶ÅÈ†êË®ìÁ∑¥ÈÅéÁ®ãÔºåÈÄô‰ΩøÂæóË©≤ÊñπÊ≥ïÂÖ∑ÊúâÊôÆÈÅçÊÄß„ÄÇ

##### **Enhancing robustness of data-driven SHM models: adversarial training with circle loss**
2406.14232v1 by Xiangli Yang, Xijie Deng, Hanwei Zhang, Yang Zou, Jianxi Yang

Structural health monitoring (SHM) is critical to safeguarding the safety and
reliability of aerospace, civil, and mechanical infrastructure. Machine
learning-based data-driven approaches have gained popularity in SHM due to
advancements in sensors and computational power. However, machine learning
models used in SHM are vulnerable to adversarial examples -- even small changes
in input can lead to different model outputs. This paper aims to address this
problem by discussing adversarial defenses in SHM. In this paper, we propose an
adversarial training method for defense, which uses circle loss to optimize the
distance between features in training to keep examples away from the decision
boundary. Through this simple yet effective constraint, our method demonstrates
substantial improvements in model robustness, surpassing existing defense
mechanisms.

ÊëòË¶ÅÔºöÁµêÊßãÂÅ•Â∫∑Áõ£Ê∏¨ (SHM) Â∞ç‰øùÈöúËà™Â§™„ÄÅÂúüÊú®ÂíåÊ©üÊ¢∞Âü∫Á§éË®≠ÊñΩÁöÑÂÆâÂÖ®ÂíåÂèØÈù†ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÊ©üÂô®Â≠∏ÁøíÁÇ∫Âü∫Á§éÁöÑË≥áÊñôÈ©ÖÂãïÊñπÊ≥ïÁî±ÊñºÊÑüÊ∏¨Âô®ÂíåË®àÁÆóËÉΩÂäõÁöÑÈÄ≤Ê≠•ÔºåÂú® SHM ‰∏≠Áç≤ÂæóÊôÆÂèä„ÄÇÁÑ∂ËÄåÔºåÁî®Êñº SHM ÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÁØÑ‰æãÁöÑÂΩ±Èüø‚Äî‚ÄîËº∏ÂÖ•ÁöÑÂæÆÂ∞èËÆäÊõ¥ÁîöËá≥ÂèØËÉΩÂ∞éËá¥‰∏çÂêåÁöÑÊ®°ÂûãËº∏Âá∫„ÄÇÊú¨ÊñáÊó®Âú®ÈÄèÈÅéË®éË´ñ SHM ‰∏≠ÁöÑÂ∞çÊäóÊÄßÈò≤Á¶¶‰æÜËß£Ê±∫Ê≠§ÂïèÈ°å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁî®ÊñºÈò≤Á¶¶ÁöÑÂ∞çÊäóÊÄßË®ìÁ∑¥ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®ÂúìÂΩ¢ÊêçÂ§±‰æÜÊúÄ‰Ω≥ÂåñË®ìÁ∑¥‰∏≠ÁâπÂæµ‰πãÈñìÁöÑË∑ùÈõ¢Ôºå‰ª•‰ΩøÁØÑ‰æãÈÅ†Èõ¢Ê±∫Á≠ñÈÇäÁïå„ÄÇÈÄèÈÅéÈÄôÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÁ¥ÑÊùüÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂ±ïÁ§∫‰∫ÜÊ®°ÂûãÁ©©ÂÅ•ÊÄßÁöÑÈ°ØËëóÊîπÂñÑÔºåË∂ÖË∂ä‰∫ÜÁèæÊúâÁöÑÈò≤Á¶¶Ê©üÂà∂„ÄÇ

##### **A Data-Driven Guided Decoding Mechanism for Diagnostic Captioning**
2406.14164v1 by Panagiotis Kaliosis, John Pavlopoulos, Foivos Charalampakos, Georgios Moschovis, Ion Androutsopoulos

Diagnostic Captioning (DC) automatically generates a diagnostic text from one
or more medical images (e.g., X-rays, MRIs) of a patient. Treated as a draft,
the generated text may assist clinicians, by providing an initial estimation of
the patient's condition, speeding up and helping safeguard the diagnostic
process. The accuracy of a diagnostic text, however, strongly depends on how
well the key medical conditions depicted in the images are expressed. We
propose a new data-driven guided decoding method that incorporates medical
information, in the form of existing tags capturing key conditions of the
image(s), into the beam search of the diagnostic text generation process. We
evaluate the proposed method on two medical datasets using four DC systems that
range from generic image-to-text systems with CNN encoders and RNN decoders to
pre-trained Large Language Models. The latter can also be used in few- and
zero-shot learning scenarios. In most cases, the proposed mechanism improves
performance with respect to all evaluation measures. We provide an open-source
implementation of the proposed method at https://github.com/nlpaueb/dmmcs.

ÊëòË¶ÅÔºöË®∫Êñ∑Ê®ôÈ°å (DC) ÊúÉËá™ÂãïÂæû‰∏Ä‰ΩçÊàñÂ§ö‰ΩçÁóÖÊÇ£ÁöÑÈÜ´ÁôÇÂΩ±ÂÉè (‰æãÂ¶Ç X ÂÖâ„ÄÅÁ£ÅÊåØÈÄ†ÂΩ±) ‰∏≠Áî¢Áîü‰∏ÄÂâáË®∫Êñ∑ÊñáÂ≠ó„ÄÇÁî¢ÁîüÁöÑÊñáÂ≠óË¶ñÁÇ∫ËçâÁ®øÔºåÂèØÂçîÂä©Ëá®Â∫äÈÜ´ÁîüÊèê‰æõÁóÖÊÇ£ÁãÄÊ≥ÅÁöÑÂàùÊ≠•‰º∞Ë®àÔºåÂä†ÈÄü‰∏¶ÂçîÂä©‰øùÈöúË®∫Êñ∑Á®ãÂ∫è„ÄÇÁÑ∂ËÄåÔºåË®∫Êñ∑ÊñáÂ≠óÁöÑÊ∫ñÁ¢∫ÊÄßÈ´òÂ∫¶‰ª∞Ë≥¥ÂΩ±ÂÉè‰∏≠ÊâÄÊèèÁπ™ÁöÑ‰∏ªË¶ÅÈÜ´ÁôÇÁãÄÊ≥ÅÂ¶Ç‰ΩïË°®ÈÅî„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑË≥áÊñôÈ©ÖÂãïÂºïÂ∞éËß£Á¢ºÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂ∞áÈÜ´ÁôÇË≥áË®ä‰ª•ÁèæÊúâÊ®ôÁ±§ÁöÑÂΩ¢ÂºèÁ¥çÂÖ•Ë®∫Êñ∑ÊñáÂ≠óÁî¢ÁîüÈÅéÁ®ãÁöÑÊ≥¢ÊùüÊêúÂ∞ã‰∏≠ÔºåÁî®‰ª•Êì∑ÂèñÂΩ±ÂÉèÁöÑ‰∏ªË¶ÅÁãÄÊ≥Å„ÄÇÊàëÂÄë‰ΩøÁî®ÂõõÂÄã DC Á≥ªÁµ±Âú®ÂÖ©ÂÄãÈÜ´ÁôÇË≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºåÈÄô‰∫õÁ≥ªÁµ±ÁöÑÁØÑÂúçÂæûÂÖ∑ÂÇô CNN Á∑®Á¢ºÂô®Âíå RNN Ëß£Á¢ºÂô®ÁöÑÈÄöÁî®ÂΩ±ÂÉèËΩâÊñáÂ≠óÁ≥ªÁµ±Âà∞È†êÂÖàË®ìÁ∑¥ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÇÂæåËÄÖ‰πüÂèØÂú®Â∞ëÊ®£Êú¨ÂíåÈõ∂Ê®£Êú¨Â≠∏ÁøíÊÉÖÂ¢É‰∏≠‰ΩøÁî®„ÄÇÂú®Â§ßÈÉ®ÂàÜÊÉÖÊ≥Å‰∏ãÔºåÊâÄÊèêÂá∫ÁöÑÊ©üÂà∂Âú®ÊâÄÊúâË©ïÈáèÊåáÊ®ôÊñπÈù¢ÂùáÊèêÂçá‰∫ÜÊïàËÉΩ„ÄÇÊàëÂÄëÂú® https://github.com/nlpaueb/dmmcs ‰∏≠Êèê‰æõ‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÈñãÊ∫êÂØ¶‰Ωú„ÄÇ

##### **Resource-efficient Medical Image Analysis with Self-adapting Forward-Forward Networks**
2406.14038v1 by Johanna P. M√ºller, Bernhard Kainz

We introduce a fast Self-adapting Forward-Forward Network (SaFF-Net) for
medical imaging analysis, mitigating power consumption and resource
limitations, which currently primarily stem from the prevalent reliance on
back-propagation for model training and fine-tuning. Building upon the recently
proposed Forward-Forward Algorithm (FFA), we introduce the Convolutional
Forward-Forward Algorithm (CFFA), a parameter-efficient reformulation that is
suitable for advanced image analysis and overcomes the speed and generalisation
constraints of the original FFA. To address hyper-parameter sensitivity of FFAs
we are also introducing a self-adapting framework SaFF-Net fine-tuning
parameters during warmup and training in parallel. Our approach enables more
effective model training and eliminates the previously essential requirement
for an arbitrarily chosen Goodness function in FFA. We evaluate our approach on
several benchmarking datasets in comparison with standard Back-Propagation (BP)
neural networks showing that FFA-based networks with notably fewer parameters
and function evaluations can compete with standard models, especially, in
one-shot scenarios and large batch sizes. The code will be available at the
time of the conference.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄë‰ªãÁ¥π‰∏ÄÁ®ÆÁî®ÊñºÈÜ´ÁôÇÂΩ±ÂÉèÂàÜÊûêÁöÑÂø´ÈÄüËá™ÈÅ©ÊáâÂâçÈ•ãÂâçÈ•ãÁ∂≤Ë∑Ø (SaFF-Net)Ôºå‰ª•Ê∏õËºïÁõÆÂâç‰∏ªË¶ÅÊ∫êËá™ÊñºÈÅéÂ∫¶‰æùË≥¥ÂèçÂêëÂÇ≥Êí≠ÈÄ≤Ë°åÊ®°ÂûãË®ìÁ∑¥ÂíåÂæÆË™øÁöÑÂäüËÄóÂíåË≥áÊ∫êÈôêÂà∂„ÄÇÂú®ÊúÄËøëÊèêÂá∫ÁöÑÂâçÈ•ãÂâçÈ•ãÊºîÁÆóÊ≥ï (FFA) ÁöÑÂü∫Á§é‰∏äÔºåÊàëÂÄë‰ªãÁ¥π‰∫ÜÂç∑Á©çÂâçÈ•ãÂâçÈ•ãÊºîÁÆóÊ≥ï (CFFA)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂèÉÊï∏ÊúâÊïàÁéáÁöÑÈáçÊñ∞Âà∂ÂÆöÔºåÈÅ©Áî®ÊñºÈÄ≤ÈöéÂΩ±ÂÉèÂàÜÊûêÔºå‰∏¶ÂÖãÊúç‰∫ÜÂéüÂßã FFA ÁöÑÈÄüÂ∫¶ÂíåÊ¶ÇÊã¨ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ FFA ÁöÑË∂ÖÂèÉÊï∏ÊïèÊÑüÊÄßÔºåÊàëÂÄëÈÇÑÂú®ÁÜ±Ë∫´ÂíåË®ìÁ∑¥ÊúüÈñì‰∏¶Ë°åÂºïÂÖ•‰∫ÜËá™ÈÅ©ÊáâÊû∂Êßã SaFF-Net ÂæÆË™øÂèÉÊï∏„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïËÉΩÊõ¥ÊúâÊïàÂú∞ÈÄ≤Ë°åÊ®°ÂûãË®ìÁ∑¥Ôºå‰∏¶Ê∂àÈô§‰∫Ü FFA ‰∏≠ÂÖàÂâçÂ∞ç‰ªªÊÑèÈÅ∏ÊìáÁöÑÂÑ™ËâØÂáΩÊï∏ÁöÑÂü∫Êú¨ÈúÄÊ±Ç„ÄÇÊàëÂÄëÂú®ÂπæÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÂÅöÊ≥ïÔºå‰∏¶ËàáÊ®ôÊ∫ñÂèçÂêëÂÇ≥Êí≠ (BP) Á•ûÁ∂ìÁ∂≤Ë∑ØÈÄ≤Ë°åÊØîËºÉÔºåÁµêÊûúÈ°ØÁ§∫Âü∫Êñº FFA ÁöÑÁ∂≤Ë∑ØÂÖ∑ÊúâÊòéÈ°ØËºÉÂ∞ëÁöÑÂèÉÊï∏ÂíåÂáΩÊï∏Ë©ï‰º∞ÔºåÂèØ‰ª•ËàáÊ®ôÊ∫ñÊ®°ÂûãÁ´∂Áà≠ÔºåÁâπÂà•ÊòØÂú®ÂñÆÊ¨°Â†¥ÊôØÂíåÂ§ßÊâπÊ¨°Â§ßÂ∞è‰∏≠„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂú®ÊúÉË≠∞ÊúüÈñìÊèê‰æõ„ÄÇ</paragraph>

##### **Reasoning Like a Doctor: Improving Medical Dialogue Systems via Diagnostic Reasoning Process Alignment**
2406.13934v1 by Kaishuai Xu, Yi Cheng, Wenjun Hou, Qiaoyu Tan, Wenjie Li

Medical dialogue systems have attracted significant attention for their
potential to act as medical assistants. Enabling these medical systems to
emulate clinicians' diagnostic reasoning process has been the long-standing
research focus. Previous studies rudimentarily realized the simulation of
clinicians' diagnostic process by fine-tuning language models on high-quality
dialogue datasets. Nonetheless, they overly focus on the outcomes of the
clinician's reasoning process while ignoring their internal thought processes
and alignment with clinician preferences. Our work aims to build a medical
dialogue system that aligns with clinicians' diagnostic reasoning processes. We
propose a novel framework, Emulation, designed to generate an appropriate
response that relies on abductive and deductive diagnostic reasoning analyses
and aligns with clinician preferences through thought process modeling.
Experimental results on two datasets confirm the efficacy of Emulation.
Crucially, our framework furnishes clear explanations for the generated
responses, enhancing its transparency in medical consultations.

ÊëòË¶ÅÔºöÈÜ´ÁôÇÂ∞çË©±Á≥ªÁµ±Âõ†ÂÖ∂‰ΩúÁÇ∫ÈÜ´ÁôÇÂä©ÁêÜÁöÑÊΩõÂäõËÄåÂÇôÂèóÈóúÊ≥®„ÄÇËÆìÈÄô‰∫õÈÜ´ÁôÇÁ≥ªÁµ±Ê®°Êì¨Ëá®Â∫äÈÜ´ÁîüÁöÑË®∫Êñ∑Êé®ÁêÜÈÅéÁ®ã‰∏ÄÁõ¥ÊòØÈï∑ÊúüÁöÑÁ†îÁ©∂ÈáçÈªû„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂ÈÄöÈÅéÂæÆË™øÈ´òÂìÅË≥™Â∞çË©±Ë≥áÊñôÈõÜ‰∏äÁöÑË™ûË®ÄÊ®°ÂûãÔºåÂàùÊ≠•ÂØ¶Áèæ‰∫ÜÂ∞çËá®Â∫äÈÜ´ÁîüË®∫Êñ∑ÈÅéÁ®ãÁöÑÊ®°Êì¨„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§Ôºå‰ªñÂÄëÈÅéÊñºÈóúÊ≥®Ëá®Â∫äÈÜ´ÁîüÊé®ÁêÜÈÅéÁ®ãÁöÑÁµêÊûúÔºåËÄåÂøΩË¶ñ‰∫Ü‰ªñÂÄëÁöÑÂÖßÈÉ®ÊÄùËÄÉÈÅéÁ®ã‰ª•ÂèäËàáËá®Â∫äÈÜ´ÁîüÂÅèÂ•ΩÁöÑÂ∞çÈΩä„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Êó®Âú®Âª∫Á´ã‰∏ÄÂÄãËàáËá®Â∫äÈÜ´ÁîüÁöÑË®∫Êñ∑Êé®ÁêÜÈÅéÁ®ã‰øùÊåÅ‰∏ÄËá¥ÁöÑÈÜ´ÁôÇÂ∞çË©±Á≥ªÁµ±„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂ÔºåÁ®±ÁÇ∫ EmulationÔºåÊó®Âú®Áî¢ÁîüÈÅ©Áï∂ÁöÑÂõûÊáâÔºåÈÄô‰∫õÂõûÊáâ‰æùË≥¥ÊñºÊºîÁππÂíåÊ≠∏Á¥çË®∫Êñ∑Êé®ÁêÜÂàÜÊûêÔºå‰∏¶ÈÄöÈÅéÊÄùËÄÉÈÅéÁ®ãÂª∫Ê®°ËàáËá®Â∫äÈÜ´ÁîüÁöÑÂÅèÂ•Ω‰øùÊåÅ‰∏ÄËá¥„ÄÇÂú®ÂÖ©ÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÂØ¶‰∫Ü Emulation ÁöÑÂäüÊïà„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂ÁÇ∫ÁîüÊàêÁöÑÂõûÊáâÊèê‰æõ‰∫ÜÊ∏ÖÊô∞ÁöÑËß£ÈáãÔºåÂ¢ûÂº∑‰∫ÜÂÖ∂Âú®ÈÜ´ÁôÇË´ÆË©¢‰∏≠ÁöÑÈÄèÊòéÂ∫¶„ÄÇ

##### **ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World**
2406.13890v1 by Weixiang Yan, Haitian Liu, Tengxiao Wu, Qian Chen, Wen Wang, Haoyuan Chai, Jiayi Wang, Weishan Zhao, Yixin Zhang, Renjun Zhang, Li Zhu

LLMs have achieved significant performance progress in various NLP
applications. However, LLMs still struggle to meet the strict requirements for
accuracy and reliability in the medical field and face many challenges in
clinical applications. Existing clinical diagnostic evaluation benchmarks for
evaluating medical agents powered by LLMs have severe limitations. Firstly,
most existing medical evaluation benchmarks face the risk of data leakage or
contamination. Secondly, existing benchmarks often neglect the characteristics
of multiple departments and specializations in modern medical practice.
Thirdly, existing evaluation methods are limited to multiple-choice questions,
which do not align with the real-world diagnostic scenarios. Lastly, existing
evaluation methods lack comprehensive evaluations of end-to-end real clinical
scenarios. These limitations in benchmarks in turn obstruct advancements of
LLMs and agents for medicine. To address these limitations, we introduce
ClinicalLab, a comprehensive clinical diagnosis agent alignment suite.
ClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical
diagnostic evaluation benchmark for evaluating medical agents and LLMs.
ClinicalBench is based on real cases that cover 24 departments and 150
diseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for
evaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate
17 LLMs and find that their performance varies significantly across different
departments. Based on these findings, in ClinicalLab, we propose ClinicalAgent,
an end-to-end clinical agent that aligns with real-world clinical diagnostic
practices. We systematically investigate the performance and applicable
scenarios of variants of ClinicalAgent on ClinicalBench. Our findings
demonstrate the importance of aligning with modern medical practices in
designing medical agents.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊáâÁî®‰∏≠ÂèñÂæóÈ°ØËëóÁöÑÈÄ≤Â±ï„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰ªçÈõ£‰ª•ÊªøË∂≥ÈÜ´ÁôÇÈ†òÂüüÂ∞çÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄßÁöÑÂö¥Ê†ºË¶ÅÊ±ÇÔºå‰∏¶Âú®Ëá®Â∫äÊáâÁî®‰∏≠Èù¢Ëá®Ë®±Â§öÊåëÊà∞„ÄÇÁèæÊúâÁöÑËá®Â∫äË®∫Êñ∑Ë©ï‰º∞Âü∫Ê∫ñÁî®ÊñºË©ï‰º∞Áî±Â§ßÂûãË™ûË®ÄÊ®°ÂûãÈ©ÖÂãïÁöÑÈÜ´ÁôÇ‰ª£ÁêÜÔºå‰ΩÜÂ≠òÂú®Âö¥ÈáçÁöÑÈôêÂà∂„ÄÇÈ¶ñÂÖàÔºåÁèæÊúâÁöÑÈÜ´ÁôÇË©ï‰º∞Âü∫Ê∫ñÂ§ßÂ§öÈù¢Ëá®Êï∏ÊìöÊ¥©Èú≤ÊàñÊ±°ÊüìÁöÑÈ¢®Èö™„ÄÇÂÖ∂Ê¨°ÔºåÁèæÊúâÁöÑÂü∫Ê∫ñÈÄöÂ∏∏ÂøΩÁï•Áèæ‰ª£ÈÜ´ÁôÇÂØ¶Âãô‰∏≠Â§öÂÄãÈÉ®ÈñÄÂíåÂ∞àÁßëÁöÑÁâπÂæµ„ÄÇÁ¨¨‰∏âÔºåÁèæÊúâÁöÑË©ï‰º∞ÊñπÊ≥ïÂÉÖÈôêÊñºÈÅ∏ÊìáÈ°åÔºåËàáÁèæÂØ¶‰∏ñÁïåÁöÑË®∫Êñ∑ÊÉÖÂ¢É‰∏çÁ¨¶„ÄÇÊúÄÂæåÔºåÁèæÊúâÁöÑË©ï‰º∞ÊñπÊ≥ïÁº∫‰πèÂ∞çÁ´ØÂà∞Á´ØÁúüÂØ¶Ëá®Â∫äÊÉÖÂ¢ÉÁöÑÂÖ®Èù¢Ë©ï‰º∞„ÄÇÂü∫Ê∫ñÁöÑÈÄô‰∫õÈôêÂà∂ÂèçÈÅé‰æÜÈòªÁ§ô‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂíåÈÜ´Â≠∏‰ª£ÁêÜÁöÑÈÄ≤Ê≠•„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü ClinicalLabÔºå‰∏ÄÂÄãÂÖ®Èù¢ÁöÑËá®Â∫äË®∫Êñ∑‰ª£ÁêÜÂ∞çÈΩäÂ•ó‰ª∂„ÄÇClinicalLab ÂåÖÂê´ ClinicalBenchÔºå‰∏ÄÂÄãÁ´ØÂà∞Á´ØÁöÑÂ§öÈÉ®ÈñÄËá®Â∫äË®∫Êñ∑Ë©ï‰º∞Âü∫Ê∫ñÔºåÁî®ÊñºË©ï‰º∞ÈÜ´ÁôÇ‰ª£ÁêÜÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÇClinicalBench Âü∫ÊñºÊ∂µËìã 24 ÂÄãÈÉ®ÈñÄÂíå 150 Á®ÆÁñæÁóÖÁöÑÁúüÂØ¶Ê°à‰æã„ÄÇClinicalLab ÈÇÑÂåÖÊã¨ÂõõÈ†ÖÊñ∞ÊåáÊ®ôÔºàClinicalMetricsÔºâÔºåÁî®ÊñºË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Ëá®Â∫äË®∫Êñ∑‰ªªÂãô‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü 17 ÂÄãÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåÁôºÁèæÂÆÉÂÄëÂú®‰∏çÂêåÈÉ®ÈñÄÁöÑË°®ÁèæÂ∑ÆÁï∞ÂæàÂ§ß„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÂú® ClinicalLab ‰∏≠ÊèêÂá∫‰∫Ü ClinicalAgentÔºå‰∏ÄÂÄãËàáÁèæÂØ¶‰∏ñÁïåËá®Â∫äË®∫Êñ∑ÂØ¶ÂãôÁõ∏Á¨¶ÁöÑÁ´ØÂà∞Á´ØËá®Â∫ä‰ª£ÁêÜ„ÄÇÊàëÂÄëÁ≥ªÁµ±Âú∞Á†îÁ©∂‰∫Ü ClinicalAgent ËÆäÈ´îÂú® ClinicalBench ‰∏äÁöÑÊïàËÉΩÂíåÈÅ©Áî®ÊÉÖÂ¢É„ÄÇÊàëÂÄëÁöÑÁôºÁèæË≠âÊòé‰∫ÜÂú®Ë®≠Ë®àÈÜ´ÁôÇ‰ª£ÁêÜÊôÇËàáÁèæ‰ª£ÈÜ´ÁôÇÂØ¶ÂãôÁõ∏Á¨¶ÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **MAMA-MIA: A Large-Scale Multi-Center Breast Cancer DCE-MRI Benchmark Dataset with Expert Segmentations**
2406.13844v1 by Lidia Garrucho, Claire-Anne Reidel, Kaisar Kushibar, Smriti Joshi, Richard Osuala, Apostolia Tsirikoglou, Maciej Bobowicz, Javier del Riego, Alessandro Catanese, Katarzyna Gwo≈∫dziewicz, Maria-Laura Cosaka, Pasant M. Abo-Elhoda, Sara W. Tantawy, Shorouq S. Sakrana, Norhan O. Shawky-Abdelfatah, Amr Muhammad Abdo-Salem, Androniki Kozana, Eugen Divjak, Gordana Ivanac, Katerina Nikiforaki, Michail E. Klontzas, Rosa Garc√≠a-Dosd√°, Meltem Gulsun-Akpinar, Oƒüuz Lafcƒ±, Ritse Mann, Carlos Mart√≠n-Isla, Fred Prior, Kostas Marias, Martijn P. A. Starmans, Fredrik Strand, Oliver D√≠az, Laura Igual, Karim Lekadir

Current research in breast cancer Magnetic Resonance Imaging (MRI),
especially with Artificial Intelligence (AI), faces challenges due to the lack
of expert segmentations. To address this, we introduce the MAMA-MIA dataset,
comprising 1506 multi-center dynamic contrast-enhanced MRI cases with expert
segmentations of primary tumors and non-mass enhancement areas. These cases
were sourced from four publicly available collections in The Cancer Imaging
Archive (TCIA). Initially, we trained a deep learning model to automatically
segment the cases, generating preliminary segmentations that significantly
reduced expert segmentation time. Sixteen experts, averaging 9 years of
experience in breast cancer, then corrected these segmentations, resulting in
the final expert segmentations. Additionally, two radiologists conducted a
visual inspection of the automatic segmentations to support future quality
control studies. Alongside the expert segmentations, we provide 49 harmonized
demographic and clinical variables and the pretrained weights of the well-known
nnUNet architecture trained using the DCE-MRI full-images and expert
segmentations. This dataset aims to accelerate the development and benchmarking
of deep learning models and foster innovation in breast cancer diagnostics and
treatment planning.

ÊëòË¶ÅÔºö‰π≥ËÖ∫ÁôåÁ£ÅÂÖ±ÊåØÊàêÂÉè (MRI) ÁöÑÁèæ‰ªäÁ†îÁ©∂ÔºåÁâπÂà•ÊòØËàá‰∫∫Â∑•Êô∫ÊÖß (AI) Áõ∏ÈóúÁöÑÁ†îÁ©∂ÔºåÁî±ÊñºÁº∫‰πèÂ∞àÂÆ∂ÂàÜÊÆµËÄåÈù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü MAMA-MIA Ë≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ 1506 ‰æãÂ§ö‰∏≠ÂøÉÂãïÊÖãÂ∞çÊØîÂ¢ûÂº∑ MRI Ê°à‰æãÔºå‰ª•ÂèäÂéüÁôºËÖ´Áò§ÂíåÈùûËÖ´Â°äÂ¢ûÂº∑ÂçÄÂüüÁöÑÂ∞àÂÆ∂ÂàÜÊÆµ„ÄÇÈÄô‰∫õÊ°à‰æã‰æÜËá™ÁôåÁóáÂΩ±ÂÉèÊ™îÊ°àÈ§® (TCIA) ‰∏≠ÁöÑÂõõÂÄãÂÖ¨ÈñãÂèØÂèñÂæóÁöÑÈõÜÂêà„ÄÇÊúÄÂàùÔºåÊàëÂÄëË®ìÁ∑¥‰∫Ü‰∏ÄÂÄãÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰æÜËá™ÂãïÂàÜÊÆµÊ°à‰æãÔºåÁî¢ÁîüÂàùÊ≠•ÂàÜÊÆµÔºåÂ§ßÂπÖÊ∏õÂ∞ë‰∫ÜÂ∞àÂÆ∂ÂàÜÊÆµÊôÇÈñì„ÄÇ16 ‰ΩçÂ∞àÂÆ∂ÔºåÂπ≥ÂùáÊìÅÊúâ 9 Âπ¥‰π≥ËÖ∫ÁôåÁ∂ìÈ©óÔºåÁÑ∂Âæå‰øÆÊ≠£ÈÄô‰∫õÂàÜÊÆµÔºåÁî¢ÁîüÊúÄÁµÇÁöÑÂ∞àÂÆ∂ÂàÜÊÆµ„ÄÇÊ≠§Â§ñÔºåÂÖ©‰ΩçÊîæÂ∞ÑÁßëÈÜ´Â∏´Â∞çËá™ÂãïÂàÜÊÆµÈÄ≤Ë°åË¶ñË¶∫Ê™¢Êü•Ôºå‰ª•ÊîØÊè¥Êú™‰æÜÁöÑÂìÅË≥™ÊéßÁÆ°Á†îÁ©∂„ÄÇÈô§‰∫ÜÂ∞àÂÆ∂ÂàÜÊÆµÂ§ñÔºåÊàëÂÄëÈÇÑÊèê‰æõ‰∫Ü 49 ÂÄãË™øÂíåÁöÑ‰∫∫Âè£Áµ±Ë®àÂíåËá®Â∫äËÆäÊï∏Ôºå‰ª•Âèä‰ΩøÁî® DCE-MRI ÂÖ®ÂΩ±ÂÉèÂíåÂ∞àÂÆ∂ÂàÜÊÆµË®ìÁ∑¥ÁöÑËëóÂêç nnUNet Êû∂ÊßãÁöÑÈ†êË®ìÁ∑¥Ê¨äÈáç„ÄÇÊ≠§Ë≥áÊñôÈõÜÊó®Âú®Âä†ÈÄüÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÈñãÁôºÂíåÂü∫Ê∫ñÊ∏¨Ë©¶Ôºå‰∏¶‰øÉÈÄ≤‰π≥ËÖ∫ÁôåË®∫Êñ∑ÂíåÊ≤ªÁôÇË®àÁï´ÁöÑÂâµÊñ∞„ÄÇ

##### **IoT-Based Preventive Mental Health Using Knowledge Graphs and Standards for Better Well-Being**
2406.13791v1 by Amelie Gyrard, Seyedali Mohammadi, Manas Gaur, Antonio Kung

Sustainable Development Goals (SDGs) give the UN a road map for development
with Agenda 2030 as a target. SDG3 "Good Health and Well-Being" ensures healthy
lives and promotes well-being for all ages. Digital technologies can support
SDG3. Burnout and even depression could be reduced by encouraging better
preventive health. Due to the lack of patient knowledge and focus to take care
of their health, it is necessary to help patients before it is too late. New
trends such as positive psychology and mindfulness are highly encouraged in the
USA. Digital Twin (DT) can help with the continuous monitoring of emotion using
physiological signals (e.g., collected via wearables). Digital twins facilitate
monitoring and provide constant health insight to improve quality of life and
well-being with better personalization. Healthcare DT challenges are
standardizing data formats, communication protocols, and data exchange
mechanisms. To achieve those data integration and knowledge challenges, we
designed the Mental Health Knowledge Graph (ontology and dataset) to boost
mental health. The Knowledge Graph (KG) acquires knowledge from ontology-based
mental health projects classified within the LOV4IoT ontology catalog (Emotion,
Depression, and Mental Health). Furthermore, the KG is mapped to standards
(e.g., ontologies) when possible. Standards from ETSI SmartM2M, ITU/WHO, ISO,
W3C, NIST, and IEEE are relevant to mental health.

ÊëòË¶ÅÔºöÊ∞∏Á∫åÁôºÂ±ïÁõÆÊ®ôÔºàSDGÔºâÁÇ∫ËÅØÂêàÂúãÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁôºÂ±ïË∑ØÁ∑öÂúñÔºåÁõÆÊ®ôÁÇ∫ 2030 Âπ¥Ë≠∞Á®ã„ÄÇSDG3„ÄåËâØÂ•ΩÂÅ•Â∫∑ËàáÁ¶èÁ•â„ÄçÁ¢∫‰øùÊâÄÊúâÂπ¥ÈΩ°Â±§ÁöÑÂÅ•Â∫∑ÁîüÊ¥ª‰∏¶‰øÉÈÄ≤Á¶èÁ•â„ÄÇÊï∏‰ΩçÁßëÊäÄÂèØ‰ª•ÊîØÊè¥ SDG3„ÄÇÈÄèÈÅéÈºìÂãµÊõ¥Â•ΩÁöÑÈ†êÈò≤ÊÄß‰øùÂÅ•ÔºåÂèØ‰ª•Ê∏õÂ∞ëÂÄ¶ÊÄ†ÁîöËá≥ÊÜÇÈ¨±Áóá„ÄÇÁî±ÊñºÊÇ£ËÄÖÁº∫‰πèÂÅ•Â∫∑Áü•Ë≠òÂíåÈóúÊ≥®Ëá™Ë∫´ÂÅ•Â∫∑ÁöÑÁÑ¶ÈªûÔºåÂõ†Ê≠§ÊúâÂøÖË¶ÅÂú®ÁÇ∫ÊôÇÂ∑≤Êôö‰πãÂâçÂπ´Âä©ÊÇ£ËÄÖ„ÄÇÁ©çÊ•µÂøÉÁêÜÂ≠∏ÂíåÊ≠£ÂøµÁ≠âÊñ∞Ë∂®Âã¢Âú®ÁæéÂúãÂèóÂà∞È´òÂ∫¶ÈºìÂãµ„ÄÇÊï∏‰ΩçÈõôËÉûËÉéÔºàDTÔºâÂèØ‰ª•ÈÄèÈÅéÁîüÁêÜË®äËôüÔºà‰æãÂ¶ÇÈÄèÈÅéÁ©øÊà¥ÂºèË£ùÁΩÆÊî∂ÈõÜÔºâÂçîÂä©ÊåÅÁ∫åÁõ£ÊéßÊÉÖÁ∑í„ÄÇÊï∏‰ΩçÈõôËÉûËÉé‰øÉÈÄ≤Áõ£Êéß‰∏¶Êèê‰æõÊåÅÁ∫åÁöÑÂÅ•Â∫∑Ë¶ãËß£Ôºå‰ª•ÈÄèÈÅéÊõ¥Â•ΩÁöÑÂÄã‰∫∫ÂåñÊîπÂñÑÁîüÊ¥ªÂìÅË≥™ÂíåÁ¶èÁ•â„ÄÇÈÜ´ÁôÇ‰øùÂÅ• DT ÁöÑÊåëÊà∞Âú®ÊñºÊ®ôÊ∫ñÂåñË≥áÊñôÊ†ºÂºè„ÄÅÈÄöË®äÂçîÂÆöÂíåË≥áÊñô‰∫§ÊèõÊ©üÂà∂„ÄÇÁÇ∫‰∫ÜÈÅîÊàêÈÄô‰∫õË≥áÊñôÊï¥ÂêàÂíåÁü•Ë≠òÊåëÊà∞ÔºåÊàëÂÄëË®≠Ë®à‰∫ÜÂøÉÁêÜÂÅ•Â∫∑Áü•Ë≠òÂúñË≠úÔºàÊú¨‰ΩìÂíåË≥áÊñôÈõÜÔºâ‰æÜÊèêÂçáÂøÉÁêÜÂÅ•Â∫∑„ÄÇÁü•Ë≠òÂúñË≠úÔºàKGÔºâÂæû LOV4IoT Êú¨‰ΩìÁõÆÈåÑÔºàÊÉÖÁ∑í„ÄÅÊÜÇÈ¨±ÁóáÂíåÂøÉÁêÜÂÅ•Â∫∑Ôºâ‰∏≠ÂàÜÈ°ûÁöÑÂü∫ÊñºÊú¨‰ΩìÁöÑÂøÉÁêÜÂÅ•Â∫∑Â∞àÊ°à‰∏≠Áç≤ÂèñÁü•Ë≠ò„ÄÇÊ≠§Â§ñÔºåKG Áõ°ÂèØËÉΩÂ∞çÊáâÂà∞Ê®ôÊ∫ñÔºà‰æãÂ¶ÇÊú¨‰ΩìÔºâ„ÄÇETSI SmartM2M„ÄÅITU/WHO„ÄÅISO„ÄÅW3C„ÄÅNIST Âíå IEEE ÁöÑÊ®ôÊ∫ñËàáÂøÉÁêÜÂÅ•Â∫∑Áõ∏Èóú„ÄÇ

##### **BEACON: Balancing Convenience and Nutrition in Meals With Long-Term Group Recommendations and Reasoning on Multimodal Recipes**
2406.13714v1 by Vansh Nagpal, Siva Likitha Valluru, Kausik Lakkaraju, Biplav Srivastava

A common, yet regular, decision made by people, whether healthy or with any
health condition, is to decide what to have in meals like breakfast, lunch, and
dinner, consisting of a combination of foods for appetizer, main course, side
dishes, desserts, and beverages. However, often this decision is seen as a
trade-off between nutritious choices (e.g., low salt and sugar) or convenience
(e.g., inexpensive, fast to prepare/obtain, taste better). In this preliminary
work, we present a data-driven approach for the novel meal recommendation
problem that can explore and balance choices for both considerations while also
reasoning about a food's constituents and cooking process. Beyond the problem
formulation, our contributions also include a goodness measure, a recipe
conversion method from text to the recently introduced multimodal rich recipe
representation (R3) format, and learning methods using contextual bandits that
show promising results.

ÊëòË¶ÅÔºö‰∫∫ÂÄëÂ∏∏Ë¶è‰∏îÂÆöÊúüÂÅöÂá∫ÁöÑÊ±∫ÂÆöÔºåÁÑ°Ë´ñÊòØÂÅ•Â∫∑ÁöÑ‰∫∫ÊàñÊòØÊúâ‰ªª‰ΩïÂÅ•Â∫∑ÁãÄÊ≥ÅÁöÑ‰∫∫ÔºåÂ∞±ÊòØÊ±∫ÂÆöÊó©È§ê„ÄÅÂçàÈ§êÂíåÊôöÈ§êË¶ÅÂêÉ‰ªÄÈ∫ºÔºåÂåÖÊã¨ÈñãËÉÉËèú„ÄÅ‰∏ªËèú„ÄÅÈÖçËèú„ÄÅÁîúÈªûÂíåÈ£≤ÊñôÁ≠âÈ£üÁâ©ÁöÑÁµÑÂêà„ÄÇÁÑ∂ËÄåÔºåÈÄôÂÄãÊ±∫ÂÆöÈÄöÂ∏∏Ë¢´Ë¶ñÁÇ∫ÁáüÈ§äÈÅ∏ÊìáÔºà‰æãÂ¶Ç‰ΩéÈπΩÂíå‰ΩéÁ≥ñÔºâÊàñ‰æøÂà©ÊÄßÔºà‰æãÂ¶Ç‰æøÂÆú„ÄÅÂø´ÈÄüÊ∫ñÂÇô/ÂèñÂæó„ÄÅÂë≥ÈÅìËºÉÂ•ΩÔºâ‰πãÈñìÁöÑÊ¨äË°°„ÄÇÂú®ÈÄôÈ†ÖÂàùÊ≠•Â∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆË≥áÊñôÈ©ÖÂãïÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÊñ∞Á©éÁöÑÈ§êÈªûÊé®Ëñ¶ÂïèÈ°åÔºåË©≤ÊñπÊ≥ïÂèØ‰ª•Êé¢Á¥¢ÂíåÂπ≥Ë°°ÈÄôÂÖ©ÊñπÈù¢ÁöÑÈÅ∏ÊìáÔºåÂêåÊôÇ‰πüËÉΩÊé®ÁêÜÈ£üÁâ©ÁöÑÊàêÂàÜÂíåÁÉπÈ£™ÈÅéÁ®ã„ÄÇÈô§‰∫ÜÂïèÈ°åÁöÑË°®Ëø∞Â§ñÔºåÊàëÂÄëÁöÑË≤¢ÁçªÈÇÑÂåÖÊã¨‰∏ÄÂÄãÂÑ™ËâØÂ∫¶ÈáèÂ∫¶„ÄÅ‰∏ÄÁ®ÆÂæûÊñáÂ≠óËΩâÊèõÁÇ∫ÊúÄËøëÊé®Âá∫ÁöÑÂ§öÊ®°ÊÖãË±êÂØåÈ£üË≠úË°®Á§∫Ê≥ï (R3) Ê†ºÂºèÁöÑÈ£üË≠úËΩâÊèõÊñπÊ≥ïÔºå‰ª•Âèä‰ΩøÁî®ÊÉÖÂ¢ÉÂº∑ÁõúÁöÑÂ≠∏ÁøíÊñπÊ≥ïÔºåÈÄô‰∫õÊñπÊ≥ïÈ°ØÁ§∫Âá∫ÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇ

##### **EndoUIC: Promptable Diffusion Transformer for Unified Illumination Correction in Capsule Endoscopy**
2406.13705v1 by Long Bai, Qiaozhi Tan, Tong Chen, Wan Jun Nah, Yanheng Li, Zhicheng He, Sishen Yuan, Zhen Chen, Jinlin Wu, Mobarakol Islam, Zhen Li, Hongbin Liu, Hongliang Ren

Wireless Capsule Endoscopy (WCE) is highly valued for its non-invasive and
painless approach, though its effectiveness is compromised by uneven
illumination from hardware constraints and complex internal dynamics, leading
to overexposed or underexposed images. While researchers have discussed the
challenges of low-light enhancement in WCE, the issue of correcting for
different exposure levels remains underexplored. To tackle this, we introduce
EndoUIC, a WCE unified illumination correction solution using an end-to-end
promptable diffusion transformer (DFT) model. In our work, the illumination
prompt module shall navigate the model to adapt to different exposure levels
and perform targeted image enhancement, in which the Adaptive Prompt
Integration (API) and Global Prompt Scanner (GPS) modules shall further boost
the concurrent representation learning between the prompt parameters and
features. Besides, the U-shaped restoration DFT model shall capture the
long-range dependencies and contextual information for unified illumination
restoration. Moreover, we present a novel Capsule-endoscopy Exposure Correction
(CEC) dataset, including ground-truth and corrupted image pairs annotated by
expert photographers. Extensive experiments against a variety of
state-of-the-art (SOTA) methods on four datasets showcase the effectiveness of
our proposed method and components in WCE illumination restoration, and the
additional downstream experiments further demonstrate its utility for clinical
diagnosis and surgical assistance.

ÊëòË¶ÅÔºöÁÑ°Á∑öËÜ†ÂõäÂÖßË¶ñÈè°ÔºàWCEÔºâÂõ†ÂÖ∂Èùû‰æµÂÖ•ÊÄßÂíåÁÑ°ÁóõÁöÑÊñπÊ≥ïËÄåÂÇôÂèóÈáçË¶ñÔºåÂÑòÁÆ°ÂÖ∂ÊúâÊïàÊÄßÂèóÂà∞Á°¨È´îÈôêÂà∂ÂíåË§áÈõúÂÖßÈÉ®ÂãïÂäõÂ≠∏Â∞éËá¥ÁÖßÊòé‰∏çÂùáÁöÑÂΩ±ÈüøÔºåÂ∞éËá¥ÂΩ±ÂÉèÊõùÂÖâÈÅéÂ∫¶ÊàñÊõùÂÖâ‰∏çË∂≥„ÄÇÂÑòÁÆ°Á†îÁ©∂‰∫∫Âì°Â∑≤Ë®éË´ñ WCE ‰∏≠‰ΩéÂÖâÂ¢ûÂº∑ÁöÑÊåëÊà∞Ôºå‰ΩÜÈáùÂ∞ç‰∏çÂêåÊõùÂÖâÁ≠âÁ¥öÈÄ≤Ë°åÊ†°Ê≠£ÁöÑÂïèÈ°å‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Ë®é„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü EndoUICÔºåÈÄôÊòØ‰∏ÄÂÄã‰ΩøÁî®Á´ØÂà∞Á´ØÂèØÊèêÁ§∫Êì¥Êï£ËΩâÊèõÂô® (DFT) Ê®°ÂûãÁöÑ WCE Áµ±‰∏ÄÁÖßÊòéÊ†°Ê≠£Ëß£Ê±∫ÊñπÊ°à„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÁÖßÊòéÊèêÁ§∫Ê®°ÁµÑÊáâÂºïÂ∞éÊ®°ÂûãÈÅ©Êáâ‰∏çÂêåÁöÑÊõùÂÖâÁ≠âÁ¥ö‰∏¶Âü∑Ë°åÁõÆÊ®ôÂΩ±ÂÉèÂ¢ûÂº∑ÔºåÂÖ∂‰∏≠Ëá™ÈÅ©ÊáâÊèêÁ§∫Êï¥Âêà (API) ÂíåÂÖ®Â±ÄÊèêÁ§∫ÊéÉÊèèÂô® (GPS) Ê®°ÁµÑÊáâÈÄ≤‰∏ÄÊ≠•ÊèêÂçáÊèêÁ§∫ÂèÉÊï∏ÂíåÁâπÂæµ‰πãÈñìÁöÑ‰∏¶ÁôºË°®Á§∫Â≠∏Áøí„ÄÇÊ≠§Â§ñÔºåU ÂΩ¢Âæ©Âéü DFT Ê®°ÂûãÊáâÊçïÊçâÈï∑Ë∑ùÈõ¢‰æùË≥¥Èóú‰øÇÂíåËÑàÁµ°Ë≥áË®äÔºå‰ª•ÈÄ≤Ë°åÁµ±‰∏ÄÁÖßÊòéÂæ©Âéü„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂâµÊñ∞ÁöÑËÜ†ÂõäÂÖßË¶ñÈè°ÊõùÂÖâÊ†°Ê≠£ (CEC) Ë≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÊã¨Áî±Â∞àÊ•≠ÊîùÂΩ±Â∏´Ë®ªËß£ÁöÑÁúüÂØ¶ÂíåÊêçÂ£ûÂΩ±ÂÉèÂ∞ç„ÄÇÈáùÂ∞çÂõõÂÄãË≥áÊñôÈõÜÁöÑÂêÑÁ®ÆÊúÄÊñ∞ (SOTA) ÊñπÊ≥ïÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂíåÁµÑÊàêÂú® WCE ÁÖßÊòéÂæ©Âéü‰∏≠ÁöÑÊúâÊïàÊÄßÔºåËÄåÈ°çÂ§ñÁöÑ‰∏ãÊ∏∏ÂØ¶È©óÈÄ≤‰∏ÄÊ≠•Ë≠âÊòé‰∫ÜÂÖ∂Âú®Ëá®Â∫äË®∫Êñ∑ÂíåÊâãË°ìËºîÂä©‰∏≠ÁöÑÊïàÁî®„ÄÇ

##### **Leveraging Large Language Models for Patient Engagement: The Power of Conversational AI in Digital Health**
2406.13659v1 by Bo Wen, Raquel Norel, Julia Liu, Thaddeus Stappenbeck, Farhana Zulkernine, Huamin Chen

The rapid advancements in large language models (LLMs) have opened up new
opportunities for transforming patient engagement in healthcare through
conversational AI. This paper presents an overview of the current landscape of
LLMs in healthcare, specifically focusing on their applications in analyzing
and generating conversations for improved patient engagement. We showcase the
power of LLMs in handling unstructured conversational data through four case
studies: (1) analyzing mental health discussions on Reddit, (2) developing a
personalized chatbot for cognitive engagement in seniors, (3) summarizing
medical conversation datasets, and (4) designing an AI-powered patient
engagement system. These case studies demonstrate how LLMs can effectively
extract insights and summarizations from unstructured dialogues and engage
patients in guided, goal-oriented conversations. Leveraging LLMs for
conversational analysis and generation opens new doors for many
patient-centered outcomes research opportunities. However, integrating LLMs
into healthcare raises important ethical considerations regarding data privacy,
bias, transparency, and regulatory compliance. We discuss best practices and
guidelines for the responsible development and deployment of LLMs in healthcare
settings. Realizing the full potential of LLMs in digital health will require
close collaboration between the AI and healthcare professionals communities to
address technical challenges and ensure these powerful tools' safety, efficacy,
and equity.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÈÄ≤Â±ïÁÇ∫ÈÄèÈÅéÂ∞çË©±Âºè AI ËΩâËÆäÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊÇ£ËÄÖÂèÉËàáÂ∫¶ÈñãÂïü‰∫ÜÊñ∞ÁöÑÊ©üÊúÉ„ÄÇÊú¨ÊñáÊ¶ÇËø∞‰∫ÜÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ LLM ÁöÑÁèæÊ≥ÅÔºåÁâπÂà•Â∞àÊ≥®ÊñºÂÆÉÂÄëÂú®ÂàÜÊûêÂíåÁî¢ÁîüÂ∞çË©±‰ª•ÊîπÂñÑÊÇ£ËÄÖÂèÉËàáÂ∫¶ÁöÑÊáâÁî®„ÄÇÊàëÂÄëÈÄèÈÅéÂõõÂÄãÊ°à‰æãÁ†îÁ©∂Â±ïÁ§∫‰∫Ü LLM Âú®ËôïÁêÜÈùûÁµêÊßãÂåñÂ∞çË©±Ë≥áÊñôÊñπÈù¢ÁöÑËÉΩÂäõÔºö(1) ÂàÜÊûê Reddit ‰∏äÁöÑÂøÉÁêÜÂÅ•Â∫∑Ë®éË´ñÔºå(2) ÁÇ∫ËÄÅÂπ¥‰∫∫ÁöÑË™çÁü•ÂèÉËàáÈñãÁôºÂÄã‰∫∫ÂåñËÅäÂ§©Ê©üÂô®‰∫∫Ôºå(3) Á∏ΩÁµêÈÜ´ÁôÇÂ∞çË©±Ë≥áÊñôÈõÜÔºå‰ª•Âèä (4) Ë®≠Ë®à AI È©ÖÂãïÁöÑÊÇ£ËÄÖÂèÉËàáÁ≥ªÁµ±„ÄÇÈÄô‰∫õÊ°à‰æãÁ†îÁ©∂Â±ïÁ§∫‰∫Ü LLM Â¶Ç‰ΩïÂæûÈùûÁµêÊßãÂåñÂ∞çË©±‰∏≠ÊúâÊïàÂú∞ÊèêÂèñË¶ãËß£ÂíåÊëòË¶ÅÔºå‰∏¶ËÆìÊÇ£ËÄÖÂèÉËàáÊúâÊåáÂ∞éÊÄßÁöÑ„ÄÅ‰ª•ÁõÆÊ®ôÁÇ∫Â∞éÂêëÁöÑÂ∞çË©±„ÄÇÂà©Áî® LLM ÈÄ≤Ë°åÂ∞çË©±ÂàÜÊûêÂíåÁî¢ÁîüÁÇ∫‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÊàêÊûúÁ†îÁ©∂Ê©üÊúÉÈñãÂïü‰∫ÜÊñ∞ÁöÑÈÄîÂæë„ÄÇÁÑ∂ËÄåÔºåÂ∞á LLM Êï¥ÂêàÂà∞ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÊúÉÂºïÁôºÊúâÈóúË≥áÊñôÈö±ÁßÅ„ÄÅÂÅèÂ∑Æ„ÄÅÈÄèÊòéÂ∫¶ÂíåÊ≥ïË¶èÈÅµÂæ™ÁöÑÈáçË¶ÅÂÄ´ÁêÜËÄÉÈáè„ÄÇÊàëÂÄëË®éË´ñ‰∫ÜÂú®ÈÜ´ÁôÇ‰øùÂÅ•Áí∞Â¢É‰∏≠Ë≤†Ë≤¨‰ªªÂú∞ÈñãÁôºÂíåÈÉ®ÁΩ≤ LLM ÁöÑÊúÄ‰Ω≥ÂØ¶ÂãôÂíåÊ∫ñÂâá„ÄÇË¶ÅÂØ¶Áèæ LLM Âú®Êï∏‰ΩçÂÅ•Â∫∑‰∏≠ÁöÑÂÖ®ÈÉ®ÊΩõÂäõÔºåÈúÄË¶Å AI ÂíåÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°Á§æÁæ§ÂØÜÂàáÂêà‰ΩúÔºå‰ª•ÊáâÂ∞çÊäÄË°ìÊåëÊà∞‰∏¶Á¢∫‰øùÈÄô‰∫õÂº∑Â§ßÂ∑•ÂÖ∑ÁöÑÂÆâÂÖ®„ÄÅÊïàËÉΩÂíåÂÖ¨Âπ≥ÊÄß„ÄÇ

##### **Enhance the Image: Super Resolution using Artificial Intelligence in MRI**
2406.13625v1 by Ziyu Li, Zihan Li, Haoxiang Li, Qiuyun Fan, Karla L. Miller, Wenchuan Wu, Akshay S. Chaudhari, Qiyuan Tian

This chapter provides an overview of deep learning techniques for improving
the spatial resolution of MRI, ranging from convolutional neural networks,
generative adversarial networks, to more advanced models including
transformers, diffusion models, and implicit neural representations. Our
exploration extends beyond the methodologies to scrutinize the impact of
super-resolved images on clinical and neuroscientific assessments. We also
cover various practical topics such as network architectures, image evaluation
metrics, network loss functions, and training data specifics, including
downsampling methods for simulating low-resolution images and dataset
selection. Finally, we discuss existing challenges and potential future
directions regarding the feasibility and reliability of deep learning-based MRI
super-resolution, with the aim to facilitate its wider adoption to benefit
various clinical and neuroscientific applications.

ÊëòË¶ÅÔºöÊú¨Á´†Ê¶ÇËø∞‰∫ÜÁî®ÊñºÊîπÂñÑ MRI Á©∫ÈñìËß£ÊûêÂ∫¶ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊäÄË°ìÔºåÊ∂µËìã‰∫ÜÂæûÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑ØÂà∞Êõ¥ÂÖàÈÄ≤ÁöÑÊ®°ÂûãÔºåÂåÖÊã¨Transformer„ÄÅÊì¥Êï£Ê®°ÂûãÂíåÈö±ÂºèÁ•ûÁ∂ìË°®Á§∫„ÄÇÊàëÂÄëÁöÑÊé¢Ë®é‰∏çÂÉÖÈôêÊñºÊñπÊ≥ïË´ñÔºåÈÇÑÂØ©Êü•‰∫ÜË∂ÖËß£ÊûêÂΩ±ÂÉèÂ∞çËá®Â∫äÂíåÁ•ûÁ∂ìÁßëÂ≠∏Ë©ï‰º∞ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄë‰πüÊ∂µËìã‰∫ÜÂêÑÁ®ÆÂØ¶Âãô‰∏ªÈ°åÔºå‰æãÂ¶ÇÁ∂≤Ë∑ØÊû∂Êßã„ÄÅÂΩ±ÂÉèË©ï‰º∞ÊåáÊ®ô„ÄÅÁ∂≤Ë∑ØÊêçÂ§±ÂáΩÊï∏ÂíåË®ìÁ∑¥Ë≥áÊñôË¶èÁØÑÔºåÂåÖÊã¨Áî®ÊñºÊ®°Êì¨‰ΩéËß£ÊûêÂ∫¶ÂΩ±ÂÉèÁöÑ‰∏ãÊé°Ê®£ÊñπÊ≥ïÂíåË≥áÊñôÈõÜÈÅ∏Êìá„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑ MRI Ë∂ÖËß£ÊûêÊäÄË°ìÁöÑÂèØË°åÊÄßÂíåÂèØÈù†ÊÄßÊñπÈù¢ÁöÑÁèæÊúâÊåëÊà∞ÂíåÊΩõÂú®Êú™‰æÜÊñπÂêëÔºåÁõÆÁöÑÊòØ‰øÉÈÄ≤ÂÖ∂Êõ¥Âª£Ê≥õÁöÑÊé°Áî®Ôºå‰ª•ÈÄ†Á¶èÂêÑÁ®ÆËá®Â∫äÂíåÁ•ûÁ∂ìÁßëÂ≠∏ÊáâÁî®„ÄÇ

##### **Optimizing Psychological Counseling with Instruction-Tuned Large Language Models**
2406.13617v1 by Wenjie Li, Tianyu Sun, Kun Qian, Wenhong Wang

The advent of large language models (LLMs) has significantly advanced various
fields, including natural language processing and automated dialogue systems.
This paper explores the application of LLMs in psychological counseling,
addressing the increasing demand for mental health services. We present a
method for instruction tuning LLMs with specialized prompts to enhance their
performance in providing empathetic, relevant, and supportive responses. Our
approach involves developing a comprehensive dataset of counseling-specific
prompts, refining them through feedback from professional counselors, and
conducting rigorous evaluations using both automatic metrics and human
assessments. The results demonstrate that our instruction-tuned model
outperforms several baseline LLMs, highlighting its potential as a scalable and
accessible tool for mental health support.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂá∫ÁèæÈ°ØËëóÊèêÂçá‰∫ÜÂêÑÁ®ÆÈ†òÂüüÔºåÂåÖÊã¨Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåËá™ÂãïÂ∞çË©±Á≥ªÁµ±„ÄÇÈÄôÁØáË´ñÊñáÊé¢Ë®é‰∫Ü LLM Âú®ÂøÉÁêÜË´ÆÂïÜ‰∏≠ÁöÑÊáâÁî®Ôºå‰ª•Ëß£Ê±∫Â∞çÂøÉÁêÜÂÅ•Â∫∑ÊúçÂãôÊó•ÁõäÂ¢ûÈï∑ÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Â∞àÈñÄÊèêÁ§∫‰æÜË™øÊï¥ LLM Êåá‰ª§ÁöÑÊñπÊ≥ïÔºå‰ª•Â¢ûÂº∑ÂÆÉÂÄëÂú®Êèê‰æõÂêåÁêÜ„ÄÅÁõ∏ÈóúÂíåÊîØÊåÅÊÄßÂõûÊáâÊñπÈù¢ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨ÈñãÁôº‰∏ÄÂÄãÂÖ®Èù¢ÁöÑË´ÆÂïÜÁâπÂÆöÊèêÁ§∫Ë≥áÊñôÈõÜÔºåÈÄèÈÅéÂ∞àÊ•≠Ë´ÆÂïÜÂ∏´ÁöÑÂõûÈ•ã‰æÜÊîπÂñÑÊèêÁ§∫Ôºå‰∏¶‰ΩøÁî®Ëá™ÂãïÂåñÊåáÊ®ôÂíå‰∫∫Â∑•Ë©ï‰º∞ÈÄ≤Ë°åÂö¥Ë¨πÁöÑË©ï‰º∞„ÄÇÁµêÊûúË°®ÊòéÔºåÊàëÂÄëË™øÊï¥Êåá‰ª§ÁöÑÊ®°ÂûãÂÑ™ÊñºÂπæÂÄãÂü∫Ê∫ñ LLMÔºåÁ™ÅÈ°Ø‰∫ÜÂÆÉ‰ΩúÁÇ∫ÂøÉÁêÜÂÅ•Â∫∑ÊîØÊè¥ÁöÑÂèØÊì¥ÂÖÖ‰∏îÂèØÂ≠òÂèñÂ∑•ÂÖ∑ÁöÑÊΩõÂäõ„ÄÇ

##### **Certificates of Differential Privacy and Unlearning for Gradient-Based Training**
2406.13433v1 by Matthew Wicker, Philip Sosnin, Adrianna Janik, Mark N. M√ºller, Adrian Weller, Calvin Tsay

Proper data stewardship requires that model owners protect the privacy of
individuals' data used during training. Whether through anonymization with
differential privacy or the use of unlearning in non-anonymized settings, the
gold-standard techniques for providing privacy guarantees can come with
significant performance penalties or be too weak to provide practical
assurances. In part, this is due to the fact that the guarantee provided by
differential privacy represents the worst-case privacy leakage for any
individual, while the true privacy leakage of releasing the prediction for a
given individual might be substantially smaller or even, as we show,
non-existent. This work provides a novel framework based on convex relaxations
and bounds propagation that can compute formal guarantees (certificates) that
releasing specific predictions satisfies $\epsilon=0$ privacy guarantees or do
not depend on data that is subject to an unlearning request. Our framework
offers a new verification-centric approach to privacy and unlearning
guarantees, that can be used to further engender user trust with tighter
privacy guarantees, provide formal proofs of robustness to certain membership
inference attacks, identify potentially vulnerable records, and enhance current
unlearning approaches. We validate the effectiveness of our approach on tasks
from financial services, medical imaging, and natural language processing.

ÊëòË¶ÅÔºöÈÅ©Áï∂ÁöÑË≥áÊñôÁÆ°ÁêÜË¶ÅÊ±ÇÊ®°ÂûãÊìÅÊúâËÄÖ‰øùË≠∑ÂÄã‰∫∫Âú®Ë®ìÁ∑¥ÊúüÈñìÊâÄ‰ΩøÁî®Ë≥áÊñôÁöÑÈö±ÁßÅ„ÄÇÁÑ°Ë´ñÊòØÈÄèÈÅéÂÖ∑ÊúâÂ∑ÆÂàÜÈö±ÁßÅÁöÑÂåøÂêçÂåñÊàñÊòØÂú®ÈùûÂåøÂêçÂåñË®≠ÂÆö‰∏≠‰ΩøÁî®ÂøòË®òÔºåÊèê‰æõÈö±ÁßÅ‰øùË≠âÁöÑÈªÉÈáëÊ®ôÊ∫ñÊäÄË°ìÈÉΩÂèØËÉΩ‰º¥Èö®ËëóÈ°ØËëóÁöÑÊïàËÉΩÊêçÂ§±ÔºåÊàñÈÅéÊñºËñÑÂº±ËÄåÁÑ°Ê≥ïÊèê‰æõÂØ¶Èöõ‰øùË≠â„ÄÇÈÉ®ÂàÜÂéüÂõ†Âú®ÊñºÔºåÂ∑ÆÂàÜÈö±ÁßÅÊèê‰æõÁöÑ‰øùË≠â‰ª£Ë°®‰ªª‰ΩïÂÄã‰∫∫ÁöÑÊúÄÂ∑ÆÊÉÖÊ≥ÅÈö±ÁßÅÊ¥©ÊºèÔºåËÄåÈáãÂá∫Áµ¶ÂÆöÂÄã‰∫∫È†êÊ∏¨ÁöÑÁúüÂØ¶Èö±ÁßÅÊ¥©ÊºèÂèØËÉΩÂ§ßÂπÖÊ∏õÂ∞ëÔºåÁîöËá≥Â¶ÇÊàëÂÄëÊâÄÂ±ïÁ§∫ÁöÑÔºå‰∏çÂ≠òÂú®„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊèê‰æõ‰∏ÄÂÄãÊñ∞ÁöÑÊû∂ÊßãÔºåÂü∫ÊñºÂá∏ÂºõË±´ÂíåÈÇäÁïåÂÇ≥Êí≠ÔºåÂèØ‰ª•Ë®àÁÆóÂΩ¢ÂºèÂåñ‰øùË≠âÔºàË≠âÊòéÔºâÔºåÈáãÂá∫ÁâπÂÆöÈ†êÊ∏¨ÊªøË∂≥ $\epsilon=0$ Èö±ÁßÅ‰øùË≠âÔºåÊàñ‰∏ç‰æùË≥¥ÊñºÂèóÂøòË®òË¶ÅÊ±ÇÁ¥ÑÊùüÁöÑË≥áÊñô„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊèê‰æõ‰∏ÄÂÄãÊñ∞ÁöÑ‰ª•È©óË≠âÁÇ∫‰∏≠ÂøÉÁöÑÈö±ÁßÅÂíåÂøòË®ò‰øùË≠âÊñπÊ≥ïÔºåÂèØÁî®ÊñºÈÄ≤‰∏ÄÊ≠•ÊèêÂçá‰ΩøÁî®ËÄÖÂ∞çÊõ¥Âö¥Ê†ºÈö±ÁßÅ‰øùË≠âÁöÑ‰ø°‰ªªÔºåÊèê‰æõÂ∞çÁâπÂÆöÊàêÂì°Êé®Ë´ñÊîªÊìäÁöÑÊ≠£ÂºèÁ©©ÂÅ•ÊÄßË≠âÊòéÔºåË≠òÂà•ÊΩõÂú®ÁöÑËÑÜÂº±Ë®òÈåÑÔºå‰∏¶Â¢ûÂº∑ÁõÆÂâçÁöÑÂøòË®òÊñπÊ≥ï„ÄÇÊàëÂÄëÂú®ÈáëËûçÊúçÂãô„ÄÅÈÜ´Â≠∏ÂΩ±ÂÉèÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑ‰ªªÂãô‰∏≠È©óË≠â‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Explainable by-design Audio Segmentation through Non-Negative Matrix Factorization and Probing**
2406.13385v1 by Martin Lebourdais, Th√©o Mariotte, Antonio Almud√©var, Marie Tahon, Alfonso Ortega

Audio segmentation is a key task for many speech technologies, most of which
are based on neural networks, usually considered as black boxes, with
high-level performances. However, in many domains, among which health or
forensics, there is not only a need for good performance but also for
explanations about the output decision. Explanations derived directly from
latent representations need to satisfy "good" properties, such as
informativeness, compactness, or modularity, to be interpretable. In this
article, we propose an explainable-by-design audio segmentation model based on
non-negative matrix factorization (NMF) which is a good candidate for the
design of interpretable representations. This paper shows that our model
reaches good segmentation performances, and presents deep analyses of the
latent representation extracted from the non-negative matrix. The proposed
approach opens new perspectives toward the evaluation of interpretable
representations according to "good" properties.

ÊëòË¶ÅÔºöÈü≥Ë®äÂçÄÊÆµÂåñÊòØË®±Â§öË™ûÈü≥ÊäÄË°ìÁöÑÈóúÈçµ‰ªªÂãôÔºåÂÖ∂‰∏≠Â§ßÈÉ®ÂàÜÂü∫ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÈÄöÂ∏∏Ë¢´Ë¶ñÁÇ∫ÈªëÁõíÂ≠êÔºåÂÖ∑ÊúâÈ´òÈöéÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂú®Ë®±Â§öÈ†òÂüü‰∏≠ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂÅ•Â∫∑ÊàñÊ≥ïÈÜ´Â≠∏Ôºå‰∏çÂÉÖÈúÄË¶ÅËâØÂ•ΩÁöÑÊïàËÉΩÔºå‰πüÈúÄË¶ÅÂ∞çËº∏Âá∫Ê±∫Á≠ñÈÄ≤Ë°åË™™Êòé„ÄÇÁõ¥Êé•ÂæûÊΩõÂú®Ë°®Âæµ‰∏≠Ë°çÁîüÁöÑË™™ÊòéÈúÄË¶ÅÊªøË∂≥„ÄåËâØÂ•Ω„ÄçÁöÑÁâπÊÄßÔºå‰æãÂ¶ÇË≥áË®äÊÄß„ÄÅÁ∑äÊπäÊÄßÊàñÊ®°ÁµÑÂåñÔºåÊâçËÉΩË¢´Ëß£Èáã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÈü≥Ë®äÂçÄÊÆµÂåñÊ®°ÂûãÔºåË©≤Ê®°ÂûãÂü∫ÊñºÈùûË≤†Áü©Èô£ÂàÜËß£ (NMF)ÔºåÈÄôÊòØÈùûË≤†Ë°®ÂæµË®≠Ë®àÁöÑËâØÂ•ΩÂÄôÈÅ∏ËÄÖ„ÄÇÊú¨ÊñáÈ°ØÁ§∫ÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞ËâØÂ•ΩÁöÑÂçÄÊÆµÂåñÊïàËÉΩÔºå‰∏¶Â∞çÂæûÈùûË≤†Áü©Èô£‰∏≠ÊèêÂèñÁöÑÊΩõÂú®Ë°®ÂæµÈÄ≤Ë°åÊ∑±ÂÖ•ÂàÜÊûê„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÁÇ∫Ê†πÊìö„ÄåËâØÂ•Ω„ÄçÁâπÊÄßË©ï‰º∞ÂèØËß£ÈáãÁöÑË°®ÂæµÈñãÂïüÊñ∞ÁöÑËßÄÈªû„ÄÇ

##### **Biomedical Visual Instruction Tuning with Clinician Preference Alignment**
2406.13173v1 by Hejie Cui, Lingjun Mao, Xin Liang, Jieyu Zhang, Hui Ren, Quanzheng Li, Xiang Li, Carl Yang

Recent advancements in multimodal foundation models have showcased impressive
capabilities in understanding and reasoning with visual and textual
information. Adapting these foundation models trained for general usage to
specialized domains like biomedicine requires large-scale domain-specific
instruction datasets. While existing works have explored curating such datasets
automatically, the resultant datasets are not explicitly aligned with domain
expertise. In this work, we propose a data-centric framework, Biomedical Visual
Instruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that
incorporates clinician preferences into both stages of generating and selecting
instruction data for tuning biomedical multimodal foundation models. First,
during the generation stage, we prompt the GPT-4V generator with a diverse set
of clinician-selected demonstrations for preference-aligned data candidate
generation. Then, during the selection phase, we train a separate selection
model, which explicitly distills clinician and policy-guided model preferences
into a rating function to select high-quality data for medical instruction
tuning. Results show that the model tuned with the instruction-following data
from our method demonstrates a significant improvement in open visual chat
(18.5% relatively) and medical VQA (win rate up to 81.73%). Our
instruction-following data and models are available at BioMed-VITAL.github.io.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÁöÑÊúÄÊñ∞ËøõÂ±ïÂ±ïÁ§∫‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåËÉΩÂ§üÁêÜËß£ÂíåÊé®ÁêÜËßÜËßâÂíåÊñáÊú¨‰ø°ÊÅØ„ÄÇÂ∞ÜËøô‰∫õÈíàÂØπ‰∏ÄËà¨Áî®ÈÄîËÆ≠ÁªÉÁöÑÂü∫Á°ÄÊ®°ÂûãË∞ÉÊï¥Âà∞ÁîüÁâ©ÂåªÂ≠¶Á≠â‰∏ì‰∏öÈ¢ÜÂüüÈúÄË¶ÅÂ§ßËßÑÊ®°ÁöÑÁâπÂÆöÈ¢ÜÂüüÊåáÂØºÊï∞ÊçÆÈõÜ„ÄÇËôΩÁÑ∂Áé∞ÊúâÂ∑•‰ΩúÂ∑≤ÁªèÊé¢Á¥¢‰∫ÜËá™Âä®Êï¥ÁêÜÊ≠§Á±ªÊï∞ÊçÆÈõÜÔºå‰ΩÜÁªìÊûúÊï∞ÊçÆÈõÜÂπ∂Êú™ÊòéÁ°Æ‰∏éÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜ‰øùÊåÅ‰∏ÄËá¥„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰ª•Êï∞ÊçÆ‰∏∫‰∏≠ÂøÉÊ°ÜÊû∂ÔºåÂç≥ÁîüÁâ©ÂåªÂ≠¶ËßÜËßâÊåá‰ª§Ë∞ÉÊï¥‰∏é‰∏¥Â∫äÂåªÁîüÂÅèÂ•ΩÂØπÈΩê (BioMed-VITAL)ÔºåÂÆÉÂ∞Ü‰∏¥Â∫äÂåªÁîüÂÅèÂ•ΩÁ∫≥ÂÖ•ÁîüÊàêÂíåÈÄâÊã©Áî®‰∫éË∞ÉÊï¥ÁîüÁâ©ÂåªÂ≠¶Â§öÊ®°ÊÄÅÂü∫Á°ÄÊ®°ÂûãÁöÑÊåá‰ª§Êï∞ÊçÆÁöÑ‰∏§‰∏™Èò∂ÊÆµ„ÄÇÈ¶ñÂÖàÔºåÂú®ÁîüÊàêÈò∂ÊÆµÔºåÊàë‰ª¨‰ΩøÁî®‰∏ÄÁªÑÁî±‰∏¥Â∫äÂåªÁîüÈÄâÊã©ÁöÑÊºîÁ§∫ÊèêÁ§∫ GPT-4V ÁîüÊàêÂô®Ôºå‰ª•ÁîüÊàêÂÅèÂ•ΩÂØπÈΩêÁöÑÊï∞ÊçÆÂÄôÈÄâ„ÄÇÁÑ∂ÂêéÔºåÂú®ÈÄâÊã©Èò∂ÊÆµÔºåÊàë‰ª¨ËÆ≠ÁªÉ‰∏Ä‰∏™ÂçïÁã¨ÁöÑÈÄâÊã©Ê®°ÂûãÔºåËØ•Ê®°ÂûãÊòéÁ°ÆÂú∞Â∞Ü‰∏¥Â∫äÂåªÁîüÂíåÊîøÁ≠ñÊåáÂØºÁöÑÊ®°ÂûãÂÅèÂ•ΩÊèêÁÇºÊàê‰∏Ä‰∏™ËØÑÁ∫ßÂáΩÊï∞Ôºå‰ª•ÈÄâÊã©Áî®‰∫éÂåªÂ≠¶Êåá‰ª§Ë∞ÉÊï¥ÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆ„ÄÇÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®Êàë‰ª¨ÊñπÊ≥ï‰∏≠ÁöÑÊåá‰ª§ÈÅµÂæ™Êï∞ÊçÆË∞ÉÊï¥ÁöÑÊ®°ÂûãÂú®ÂºÄÊîæÂºèËßÜËßâËÅäÂ§©ÔºàÁõ∏ÂØπÊèêÈ´ò 18.5%ÔºâÂíåÂåªÂ≠¶ VQAÔºàËé∑ËÉúÁéáÈ´òËææ 81.73%ÔºâÊñπÈù¢Ë°®Áé∞Âá∫ÊòæÁùÄÊèêÂçá„ÄÇÊàë‰ª¨ÁöÑÊåá‰ª§ÈÅµÂæ™Êï∞ÊçÆÂíåÊ®°ÂûãÂèØÂú® BioMed-VITAL.github.io Ëé∑Âæó„ÄÇ

##### **Cardiac Copilot: Automatic Probe Guidance for Echocardiography with World Model**
2406.13165v1 by Haojun Jiang, Zhenguo Sun, Ning Jia, Meng Li, Yu Sun, Shaqi Luo, Shiji Song, Gao Huang

Echocardiography is the only technique capable of real-time imaging of the
heart and is vital for diagnosing the majority of cardiac diseases. However,
there is a severe shortage of experienced cardiac sonographers, due to the
heart's complex structure and significant operational challenges. To mitigate
this situation, we present a Cardiac Copilot system capable of providing
real-time probe movement guidance to assist less experienced sonographers in
conducting freehand echocardiography. This system can enable non-experts,
especially in primary departments and medically underserved areas, to perform
cardiac ultrasound examinations, potentially improving global healthcare
delivery. The core innovation lies in proposing a data-driven world model,
named Cardiac Dreamer, for representing cardiac spatial structures. This world
model can provide structure features of any cardiac planes around the current
probe position in the latent space, serving as an precise navigation map for
autonomous plane localization. We train our model with real-world ultrasound
data and corresponding probe motion from 110 routine clinical scans with 151K
sample pairs by three certified sonographers. Evaluations on three standard
planes with 37K sample pairs demonstrate that the world model can reduce
navigation errors by up to 33\% and exhibit more stable performance.

ÊëòË¶ÅÔºöË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÊòØÂîØ‰∏ÄËÉΩÂç≥ÊôÇÂΩ±ÂÉèÂåñÂøÉËáüÁöÑÊäÄË°ìÔºåÂ∞çÊñºË®∫Êñ∑Â§ßÈÉ®ÂàÜÁöÑÂøÉËáüÁñæÁóÖËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÂøÉËáüÁµêÊßãË§áÈõú‰∏îÊìç‰Ωú‰∏äÊúâÁõ∏Áï∂ÁöÑÊåëÊà∞ÔºåÂõ†Ê≠§Á∂ìÈ©óË±êÂØåÁöÑÂøÉËáüË∂ÖÈü≥Ê≥¢Ê™¢Êü•Âì°Âö¥ÈáçÁü≠Áº∫„ÄÇÁÇ∫‰∫ÜÁ∑©Ëß£ÈÄôÁ®ÆÊÉÖÊ≥ÅÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂ•óÂøÉËáüËºîÂä©ÈßïÈßõÁ≥ªÁµ±ÔºåËÉΩÂ§†Êèê‰æõÂç≥ÊôÇÁöÑÊé¢È†≠ÁßªÂãïÂºïÂ∞éÔºåÂçîÂä©Á∂ìÈ©óËºÉÂ∞ëÁöÑË∂ÖÈü≥Ê≥¢Ê™¢Êü•Âì°ÈÄ≤Ë°åÂæíÊâãË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÊ™¢Êü•„ÄÇÊ≠§Á≥ªÁµ±ËÆìÈùûÂ∞àÂÆ∂ÔºåÁâπÂà•ÊòØÂú®Âü∫Â±§ÈÉ®ÈñÄÂíåÈÜ´ÁôÇË≥áÊ∫ê‰∏çË∂≥ÁöÑÂú∞ÂçÄÔºå‰πüËÉΩÂü∑Ë°åÂøÉËáüË∂ÖÈü≥Ê≥¢Ê™¢Êü•ÔºåÊúâÊΩõÂäõÊîπÂñÑÂÖ®ÁêÉÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãô„ÄÇÊ†∏ÂøÉÁöÑÂâµÊñ∞Âú®ÊñºÊèêÂá∫‰∏ÄÂÄãË≥áÊñôÈ©ÖÂãïÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåÁ®±ÁÇ∫ÂøÉËáüÂ§¢ÂπªÂÆ∂ÔºåÁî®ÊñºË°®Á§∫ÂøÉËáüÁöÑÁ©∫ÈñìÁµêÊßã„ÄÇÊ≠§‰∏ñÁïåÊ®°ÂûãËÉΩÊèê‰æõ‰ªª‰ΩïÂøÉËáüÂπ≥Èù¢ÁöÑÁµêÊßãÁâπÂæµÔºåÂúçÁπûËëóÊΩõÂú®Á©∫Èñì‰∏≠ÁöÑÁï∂ÂâçÊé¢È†≠‰ΩçÁΩÆÔºå‰ΩúÁÇ∫Ëá™‰∏ªÂπ≥Èù¢ÂÆö‰ΩçÁöÑÁ≤æÁ¢∫Â∞éËà™Âú∞Âúñ„ÄÇÊàëÂÄëÂà©Áî® 110 Ê¨°‰æãË°åËá®Â∫äÊéÉÊèèÁöÑÁúüÂØ¶‰∏ñÁïåË∂ÖÈü≥Ê≥¢Êï∏ÊìöÂíåÂ∞çÊáâÁöÑÊé¢È†≠ÈÅãÂãïÔºåÁî±‰∏â‰ΩçË™çË≠âÁöÑË∂ÖÈü≥Ê≥¢Ê™¢Êü•Âì°Êèê‰æõ 151K ÂÄãÊ®£Êú¨Â∞çÔºå‰æÜË®ìÁ∑¥ÊàëÂÄëÁöÑÊ®°Âûã„ÄÇÂú®‰∏âÂÄãÊ®ôÊ∫ñÂπ≥Èù¢‰∏äÔºå‰ΩøÁî® 37K ÂÄãÊ®£Êú¨Â∞çÈÄ≤Ë°åÁöÑË©ï‰º∞È°ØÁ§∫Ôºå‰∏ñÁïåÊ®°ÂûãÂèØ‰ª•Â∞áÂ∞éËà™Ë™§Â∑ÆÊ∏õÂ∞ëÂ§öÈÅî 33%Ôºå‰∏îË°®ÁèæÂá∫Êõ¥Á©©ÂÆöÁöÑÊïàËÉΩ„ÄÇ

##### **Oralytics Reinforcement Learning Algorithm**
2406.13127v1 by Anna L. Trella, Kelly W. Zhang, Stephanie M. Carpenter, David Elashoff, Zara M. Greer, Inbal Nahum-Shani, Dennis Ruenger, Vivek Shetty, Susan A. Murphy

Dental disease is still one of the most common chronic diseases in the United
States. While dental disease is preventable through healthy oral self-care
behaviors (OSCB), this basic behavior is not consistently practiced. We have
developed Oralytics, an online, reinforcement learning (RL) algorithm that
optimizes the delivery of personalized intervention prompts to improve OSCB. In
this paper, we offer a full overview of algorithm design decisions made using
prior data, domain expertise, and experiments in a simulation test bed. The
finalized RL algorithm was deployed in the Oralytics clinical trial, conducted
from fall 2023 to summer 2024.

ÊëòË¶ÅÔºöÁâôÁßëÁñæÁóÖ‰ªçÁÑ∂ÊòØÁæéÂúãÊúÄÂ∏∏Ë¶ãÁöÑÊÖ¢ÊÄßÁñæÁóÖ‰πã‰∏Ä„ÄÇÈõñÁÑ∂ÁâôÁßëÁñæÁóÖÂèØÈÄèÈÅéÂÅ•Â∫∑ÁöÑÂè£ËÖîËá™Êàë‰øùÂÅ•Ë°åÁÇ∫ÔºàOSCBÔºâÈ†êÈò≤Ôºå‰ΩÜÈÄôÁ®ÆÂü∫Êú¨Ë°åÁÇ∫‰∏¶Êú™ÊåÅÁ∫åÂØ¶Ë°å„ÄÇÊàëÂÄëÈñãÁôºÂá∫ OralyticsÔºå‰∏ÄÁ®ÆÁ∑ö‰∏äÂº∑ÂåñÂ≠∏ÁøíÔºàRLÔºâÊºîÁÆóÊ≥ïÔºåÂèØÊúÄ‰Ω≥ÂåñÂÄã‰∫∫Âåñ‰ªãÂÖ•ÊèêÁ§∫ÁöÑÂÇ≥ÈÅûÔºå‰ª•ÊîπÂñÑ OSCB„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞áÂÖ®Èù¢Ê¶ÇËø∞ÊºîÁÆóÊ≥ïË®≠Ë®àÊ±∫Á≠ñÔºåÈÄô‰∫õÊ±∫Á≠ñÊòØ‰ΩøÁî®ÂÖàÂâçÁöÑË≥áÊñô„ÄÅÈ†òÂüüÂ∞àÊ•≠Áü•Ë≠òÂíåÊ®°Êì¨Ê∏¨Ë©¶Âπ≥Âè∞‰∏≠ÁöÑÂØ¶È©óÊâÄÂÅöÂá∫ÁöÑ„ÄÇÊúÄÁµÇÁöÑ RL ÊºîÁÆóÊ≥ïÂ∑≤ÈÉ®ÁΩ≤Âú® Oralytics Ëá®Â∫äË©¶È©ó‰∏≠ÔºåË©≤Ë©¶È©óÊñº 2023 Âπ¥ÁßãÂ≠£Ëá≥ 2024 Âπ¥Â§èÂ≠£ÈÄ≤Ë°å„ÄÇ

##### **Accelerating Complex Disease Treatment through Network Medicine and GenAI: A Case Study on Drug Repurposing for Breast Cancer**
2406.13106v3 by Ahmed Abdeen Hamed, Tamer E. Fandy

The objective of this research is to introduce a network specialized in
predicting drugs that can be repurposed by investigating real-world evidence
sources, such as clinical trials and biomedical literature. Specifically, it
aims to generate drug combination therapies for complex diseases (e.g., cancer,
Alzheimer's). We present a multilayered network medicine approach, empowered by
a highly configured ChatGPT prompt engineering system, which is constructed on
the fly to extract drug mentions in clinical trials. Additionally, we introduce
a novel algorithm that connects real-world evidence with disease-specific
signaling pathways (e.g., KEGG database). This sheds light on the
repurposability of drugs if they are found to bind with one or more protein
constituents of a signaling pathway. To demonstrate, we instantiated the
framework for breast cancer and found that, out of 46 breast cancer signaling
pathways, the framework identified 38 pathways that were covered by at least
two drugs. This evidence signals the potential for combining those drugs.
Specifically, the most covered signaling pathway, ID hsa:2064, was covered by
108 drugs, some of which can be combined. Conversely, the signaling pathway ID
hsa:1499 was covered by only two drugs, indicating a significant gap for
further research. Our network medicine framework, empowered by GenAI, shows
promise in identifying drug combinations with a high degree of specificity,
knowing the exact signaling pathways and proteins that serve as targets. It is
noteworthy that ChatGPT successfully accelerated the process of identifying
drug mentions in clinical trials, though further investigations are required to
determine the relationships among the drug mentions.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÁöÑÁõÆÁöÑÊòØ‰ªãÁ¥π‰∏ÄÂÄãÂ∞àÈñÄÁî®ÊñºÈ†êÊ∏¨Ëó•Áâ©Áî®ÈÄîÁöÑÁ∂≤Ë∑ØÔºåÈÄèÈÅéË™øÊü•ÁúüÂØ¶‰∏ñÁïåË≠âÊìö‰æÜÊ∫êÔºå‰æãÂ¶ÇËá®Â∫äË©¶È©óÂíåÁîüÁâ©ÈÜ´Â≠∏ÊñáÁçª„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂÆÉÊó®Âú®ÁÇ∫Ë§áÈõúÁñæÁóÖÔºà‰æãÂ¶ÇÁôåÁóá„ÄÅÈòøËå≤Êµ∑ÈªòÁóáÔºâÁî¢ÁîüËó•Áâ©ÁµÑÂêàÁôÇÊ≥ï„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ§öÂ±§Á∂≤Ë∑ØÈÜ´Â≠∏ÊñπÊ≥ïÔºåÁî±‰∏ÄÂÄãÈ´òÂ∫¶ÈÖçÁΩÆÁöÑ ChatGPT ÊèêÁ§∫Â∑•Á®ãÁ≥ªÁµ±Êèê‰æõÊîØÊè¥ÔºåË©≤Á≥ªÁµ±ÊúÉÂãïÊÖãÂª∫ÊßãÔºå‰ª•ËêÉÂèñËá®Â∫äË©¶È©ó‰∏≠ÁöÑËó•Áâ©ÊèêÂèä„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊºîÁÆóÊ≥ïÔºåÂ∞áÁúüÂØ¶‰∏ñÁïåË≠âÊìöËàáÁâπÂÆöÁñæÁóÖÁöÑË®äËôüÂÇ≥ÈÅûË∑ØÂæëÔºà‰æãÂ¶Ç KEGG Ë≥áÊñôÂ∫´ÔºâÈÄ£ÁµêËµ∑‰æÜ„ÄÇÂ¶ÇÊûúÁôºÁèæËó•Áâ©ËàáË®äËôüÂÇ≥ÈÅûË∑ØÂæëÁöÑ‰∏ÄÂÄãÊàñÂ§öÂÄãËõãÁôΩË≥™ÊàêÂàÜÁµêÂêàÔºåÈÄôÂ∞áÊúâÂä©Êñº‰∫ÜËß£Ëó•Áâ©ÁöÑÂÜçÂà©Áî®ÊÄß„ÄÇÁÇ∫‰∫ÜÁ§∫ÁØÑÔºåÊàëÂÄëÁÇ∫‰π≥ÁôåÂª∫Á´ã‰∫ÜÊû∂ÊßãÔºåÁôºÁèæÂá∫ 46 Ê¢ù‰π≥ÁôåË®äËôüÂÇ≥ÈÅûË∑ØÂæë‰∏≠ÔºåË©≤Êû∂ÊßãËæ®Ë≠òÂá∫ 38 Ê¢ùË∑ØÂæëËá≥Â∞ëÂåÖÂê´ÂÖ©Á®ÆËó•Áâ©„ÄÇÊ≠§Ë≠âÊìöÈ°ØÁ§∫‰∫ÜÁµêÂêàÈÄô‰∫õËó•Áâ©ÁöÑÊΩõÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊ∂µËìãÁØÑÂúçÊúÄÂª£ÁöÑË®äËôüÂÇ≥ÈÅûË∑ØÂæë ID hsa:2064ÔºåÊ∂µËìã‰∫Ü 108 Á®ÆËó•Áâ©ÔºåÂÖ∂‰∏≠‰∏Ä‰∫õÂèØ‰ª•ÁµêÂêà„ÄÇÁõ∏ÂèçÂú∞ÔºåË®äËôüÂÇ≥ÈÅûË∑ØÂæë ID hsa:1499 ÂÉÖÊ∂µËìãÂÖ©Á®ÆËó•Áâ©ÔºåÈÄôË°®Á§∫ÊúâÂæàÂ§ßÁöÑÁ†îÁ©∂Á©∫ÁôΩ„ÄÇÊàëÂÄëÁöÑÁ∂≤Ë∑ØÈÜ´Â≠∏Êû∂ÊßãÁî± GenAI Êèê‰æõÊîØÊè¥ÔºåÂú®Ëæ®Ë≠òÂÖ∑ÊúâÈ´òÂ∫¶ÁâπÁï∞ÊÄßÁöÑËó•Áâ©ÁµÑÂêàÊñπÈù¢È°ØÁ§∫Âá∫ÂâçÊôØÔºå‰∫ÜËß£‰ΩúÁÇ∫ÁõÆÊ®ôÁöÑÁ≤æÁ¢∫Ë®äËôüÂÇ≥ÈÅûË∑ØÂæëÂíåËõãÁôΩË≥™„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåChatGPT ÊàêÂäüÂú∞Âä†ÈÄü‰∫ÜÂú®Ëá®Â∫äË©¶È©ó‰∏≠Ëæ®Ë≠òËó•Áâ©ÊèêÂèäÁöÑÈÅéÁ®ãÔºåÂÑòÁÆ°ÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•ÁöÑË™øÊü•‰æÜÁ¢∫ÂÆöËó•Áâ©ÊèêÂèä‰πãÈñìÁöÑÈóú‰øÇ„ÄÇ

##### **Deriving Hematological Disease Classes Using Fuzzy Logic and Expert Knowledge: A Comprehensive Machine Learning Approach with CBC Parameters**
2406.13015v1 by Salem Ameen, Ravivarman Balachandran, Theodoros Theodoridis

In the intricate field of medical diagnostics, capturing the subtle
manifestations of diseases remains a challenge. Traditional methods, often
binary in nature, may not encapsulate the nuanced variances that exist in
real-world clinical scenarios. This paper introduces a novel approach by
leveraging Fuzzy Logic Rules to derive disease classes based on expert domain
knowledge from a medical practitioner. By recognizing that diseases do not
always fit into neat categories, and that expert knowledge can guide the
fuzzification of these boundaries, our methodology offers a more sophisticated
and nuanced diagnostic tool.
  Using a dataset procured from a prominent hospital, containing detailed
patient blood count records, we harness Fuzzy Logic Rules, a computational
technique celebrated for its ability to handle ambiguity. This approach, moving
through stages of fuzzification, rule application, inference, and ultimately
defuzzification, produces refined diagnostic predictions. When combined with
the Random Forest classifier, the system adeptly predicts hematological
conditions using Complete Blood Count (CBC) parameters.
  Preliminary results showcase high accuracy levels, underscoring the
advantages of integrating fuzzy logic into the diagnostic process. When
juxtaposed with traditional diagnostic techniques, it becomes evident that
Fuzzy Logic, especially when guided by medical expertise, offers significant
advancements in the realm of hematological diagnostics. This paper not only
paves the path for enhanced patient care but also beckons a deeper dive into
the potentialities of fuzzy logic in various medical diagnostic applications.

ÊëòË¶ÅÔºöÂú®Ë§áÈõúÁöÑÈÜ´ÁôÇË®∫Êñ∑È†òÂüü‰∏≠ÔºåÊçïÊçâÁñæÁóÖÁöÑÁ¥∞ÂæÆË°®Áèæ‰ªçÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÈÄöÂ∏∏Êú¨Ë≥™‰∏äÊòØ‰∫åÂÖÉÁöÑÔºåÂèØËÉΩÁÑ°Ê≥ïÊ¶ÇÊã¨ÁèæÂØ¶‰∏ñÁïåËá®Â∫äÊÉÖÂ¢É‰∏≠Â≠òÂú®ÁöÑÁ¥∞ÂæÆÂ∑ÆÁï∞„ÄÇÊú¨ÊñáÈÄèÈÅéÂà©Áî®Ê®°Á≥äÈÇèËºØË¶èÂâáÔºåÊ†πÊìöÈÜ´ÁôÇÂæûÊ•≠‰∫∫Âì°ÁöÑÂ∞àÊ•≠È†òÂüüÁü•Ë≠òÊé®Â∞éÁñæÁóÖÈ°ûÂà•ÔºåÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊâøË™çÁñæÁóÖ‰∏¶‰∏çÁ∏ΩÊòØÁ¨¶ÂêàÊòéÁ¢∫ÁöÑÈ°ûÂà•ÔºåËÄå‰∏îÂ∞àÂÆ∂Áü•Ë≠òÂèØ‰ª•ÊåáÂ∞éÈÄô‰∫õÈÇäÁïåÁöÑÊ®°Á≥äÂåñÔºåÂæûËÄåÊèê‰æõÊõ¥Á≤æÁ∑ª‰∏îÁ¥∞Á∑ªÁöÑË®∫Êñ∑Â∑•ÂÖ∑„ÄÇ
‰ΩøÁî®Âæû‰∏ÄÂÆ∂Áü•ÂêçÈÜ´Èô¢ÂèñÂæóÁöÑË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´Ë©≥Á¥∞ÁöÑÊÇ£ËÄÖË°ÄÁêÉË®àÊï∏Ë®òÈåÑÔºåÊàëÂÄëÂà©Áî®Ê®°Á≥äÈÇèËºØË¶èÂâáÔºåÈÄôÊòØ‰∏ÄÁ®Æ‰ª•ËôïÁêÜÊ®°Á≥äÊÄßËÉΩÂäõËÄåËëóÁ®±ÁöÑË®àÁÆóÊäÄË°ì„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÁ∂ìÈÅéÊ®°Á≥äÂåñ„ÄÅË¶èÂâáÊáâÁî®„ÄÅÊé®Ë´ñÂíåÊúÄÁµÇÂéªÊ®°Á≥äÂåñÁöÑÈöéÊÆµÔºåÁî¢ÁîüÁ≤æÁÖâÁöÑË®∫Êñ∑È†êÊ∏¨„ÄÇËàáÈö®Ê©üÊ£ÆÊûóÂàÜÈ°ûÂô®ÁµêÂêà‰ΩøÁî®ÊôÇÔºåË©≤Á≥ªÁµ±‰ΩøÁî®ÂÖ®Ë°ÄÁ¥∞ËÉûË®àÊï∏ (CBC) ÂèÉÊï∏ÁÜüÁ∑¥Âú∞È†êÊ∏¨Ë°ÄÊ∂≤Â≠∏ÁãÄÊ≥Å„ÄÇ
ÂàùÊ≠•ÁµêÊûúÂ±ïÁ§∫‰∫ÜÈ´òÊ∫ñÁ¢∫Â∫¶ÔºåÂº∑Ë™ø‰∫ÜÂ∞áÊ®°Á≥äÈÇèËºØÊï¥ÂêàÂà∞Ë®∫Êñ∑ÈÅéÁ®ã‰∏≠ÁöÑÂÑ™Èªû„ÄÇËàáÂÇ≥Áµ±Ë®∫Êñ∑ÊäÄË°ì‰∏¶ÁΩÆÊôÇÔºåÈ°ØÁÑ∂Ê®°Á≥äÈÇèËºØÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈÜ´ÁôÇÂ∞àÊ•≠Áü•Ë≠òÁöÑÊåáÂ∞é‰∏ãÔºåÁÇ∫Ë°ÄÊ∂≤Â≠∏Ë®∫Êñ∑È†òÂüüÊèê‰æõ‰∫ÜÈ°ØËëóÁöÑÈÄ≤Â±ï„ÄÇÊú¨Êñá‰∏çÂÉÖÁÇ∫Â¢ûÂº∑ÁöÑÊÇ£ËÄÖÁÖßË≠∑Èã™Âπ≥‰∫ÜÈÅìË∑ØÔºå‰πüÂëºÁ±≤Êõ¥Ê∑±ÂÖ•Âú∞Êé¢Ë®éÊ®°Á≥äÈÇèËºØÂú®ÂêÑÁ®ÆÈÜ´ÁôÇË®∫Êñ∑ÊáâÁî®‰∏≠ÁöÑÊΩõÂäõ„ÄÇ

##### **Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation**
2406.12815v1 by Nikolas Koutsoubis, Yasin Yilmaz, Ravi P. Ramachandran, Matthew Schabath, Ghulam Rasool

Machine learning (ML) and Artificial Intelligence (AI) have fueled remarkable
advancements, particularly in healthcare. Within medical imaging, ML models
hold the promise of improving disease diagnoses, treatment planning, and
post-treatment monitoring. Various computer vision tasks like image
classification, object detection, and image segmentation are poised to become
routine in clinical analysis. However, privacy concerns surrounding patient
data hinder the assembly of large training datasets needed for developing and
training accurate, robust, and generalizable models. Federated Learning (FL)
emerges as a compelling solution, enabling organizations to collaborate on ML
model training by sharing model training information (gradients) rather than
data (e.g., medical images). FL's distributed learning framework facilitates
inter-institutional collaboration while preserving patient privacy. However,
FL, while robust in privacy preservation, faces several challenges. Sensitive
information can still be gleaned from shared gradients that are passed on
between organizations during model training. Additionally, in medical imaging,
quantifying model confidence\uncertainty accurately is crucial due to the noise
and artifacts present in the data. Uncertainty estimation in FL encounters
unique hurdles due to data heterogeneity across organizations. This paper
offers a comprehensive review of FL, privacy preservation, and uncertainty
estimation, with a focus on medical imaging. Alongside a survey of current
research, we identify gaps in the field and suggest future directions for FL
research to enhance privacy and address noisy medical imaging data challenges.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏Áøí (ML) Âíå‰∫∫Â∑•Êô∫ÊÖß (AI) Â∑≤Êé®ÂãïÈ°ØËëóÁöÑÈÄ≤Â±ïÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇ‰øùÂÅ•ÊñπÈù¢„ÄÇÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÔºåML Ê®°ÂûãÊúâÊúõÊîπÂñÑÁñæÁóÖË®∫Êñ∑„ÄÅÊ≤ªÁôÇË¶èÂäÉÂíåÊ≤ªÁôÇÂæåÁõ£Êéß„ÄÇÂêÑÁ®ÆÈõªËÖ¶Ë¶ñË¶∫‰ªªÂãôÔºå‰æãÂ¶ÇÂΩ±ÂÉèÂàÜÈ°û„ÄÅÁâ©‰ª∂ÂÅµÊ∏¨ÂíåÂΩ±ÂÉèÂàÜÂâ≤ÔºåÈÉΩÊ∫ñÂÇôÂ•ΩÂú®Ëá®Â∫äÂàÜÊûê‰∏≠ÊàêÁÇ∫Â∏∏Ë¶è„ÄÇÁÑ∂ËÄåÔºåÂúçÁπûÊÇ£ËÄÖË≥áÊñôÁöÑÈö±ÁßÅÂïèÈ°åÈòªÁ§ô‰∫ÜÁµÑÂª∫Â§ßÂûãË®ìÁ∑¥Ë≥áÊñôÈõÜÔºåËÄåÈÄôÂ∞çÊñºÈñãÁôºÂíåË®ìÁ∑¥Ê∫ñÁ¢∫„ÄÅÂº∑ÂÅ•‰∏îÂèØÊ¶ÇÂåñÁöÑÊ®°ÂûãÊòØÂøÖË¶ÅÁöÑ„ÄÇËÅØÈÇ¶Â≠∏Áøí (FL) ÊàêÁÇ∫‰∏ÄÂÄãÂºï‰∫∫Ê≥®ÁõÆÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰ΩøÁµÑÁπîËÉΩÂ§†ÈÄèÈÅéÂàÜ‰∫´Ê®°ÂûãË®ìÁ∑¥Ë≥áË®ä (Ê¢ØÂ∫¶) ËÄå‰∏çÊòØË≥áÊñôÔºà‰æãÂ¶ÇÈÜ´Â≠∏ÂΩ±ÂÉèÔºâ‰æÜÂçî‰ΩúÈÄ≤Ë°å ML Ê®°ÂûãË®ìÁ∑¥„ÄÇFL ÁöÑÂàÜÊï£ÂºèÂ≠∏ÁøíÊû∂Êßã‰øÉÈÄ≤‰∫ÜÊ©üÊßãÈñìÁöÑÂçî‰ΩúÔºåÂêåÊôÇ‰øùË≠∑‰∫ÜÊÇ£ËÄÖÈö±ÁßÅ„ÄÇÁÑ∂ËÄåÔºåFL ÈõñÁÑ∂Âú®Èö±ÁßÅ‰øùË≠∑ÊñπÈù¢ÂæàÂº∑Â§ßÔºå‰ΩÜ‰ªçÈù¢Ëá®Ë®±Â§öÊåëÊà∞„ÄÇÊïèÊÑüË≥áË®ä‰ªçÁÑ∂ÂèØ‰ª•ÂæûÁµÑÁπîÂú®Ê®°ÂûãË®ìÁ∑¥ÊúüÈñìÂÇ≥ÈÅûÁöÑÂÖ±‰∫´Ê¢ØÂ∫¶‰∏≠Êî∂ÈõÜ„ÄÇÊ≠§Â§ñÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÔºåÁî±ÊñºË≥áÊñô‰∏≠Â≠òÂú®ÈõúË®äÂíå‰∫∫Â∑•Ë£ΩÂìÅÔºåÂõ†Ê≠§Ê∫ñÁ¢∫ÈáèÂåñÊ®°Âûã‰ø°ÂøÉ/‰∏çÁ¢∫ÂÆöÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÁî±ÊñºÁµÑÁπîÈñìË≥áÊñôÁï∞Ë≥™ÊÄßÔºåFL ‰∏≠ÁöÑ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÊúÉÈÅáÂà∞Áç®ÁâπÈöúÁ§ô„ÄÇÊú¨ÊñáÂÖ®Èù¢ÂõûÈ°ß‰∫Ü FL„ÄÅÈö±ÁßÅ‰øùË≠∑Âíå‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÔºåÈáçÈªûÊîæÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏ä„ÄÇÈô§‰∫ÜÂ∞çÁï∂ÂâçÁ†îÁ©∂ÈÄ≤Ë°åË™øÊü•Â§ñÔºåÊàëÂÄëÈÇÑÊâæÂá∫Ë©≤È†òÂüüÁöÑÂ∑ÆË∑ùÔºå‰∏¶ÊèêÂá∫ FL Á†îÁ©∂ÁöÑÊú™‰æÜÊñπÂêëÔºå‰ª•Â¢ûÂº∑Èö±ÁßÅ‰∏¶Ëß£Ê±∫ÈõúË®äÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÁöÑÊåëÊà∞„ÄÇ

##### **Probabilistic Temporal Prediction of Continuous Disease Trajectories and Treatment Effects Using Neural SDEs**
2406.12807v1 by Joshua Durso-Finley, Berardino Barile, Jean-Pierre Falet, Douglas L. Arnold, Nick Pawlowski, Tal Arbel

Personalized medicine based on medical images, including predicting future
individualized clinical disease progression and treatment response, would have
an enormous impact on healthcare and drug development, particularly for
diseases (e.g. multiple sclerosis (MS)) with long term, complex, heterogeneous
evolutions and no cure. In this work, we present the first stochastic causal
temporal framework to model the continuous temporal evolution of disease
progression via Neural Stochastic Differential Equations (NSDE). The proposed
causal inference model takes as input the patient's high dimensional images
(MRI) and tabular data, and predicts both factual and counterfactual
progression trajectories on different treatments in latent space. The NSDE
permits the estimation of high-confidence personalized trajectories and
treatment effects. Extensive experiments were performed on a large,
multi-centre, proprietary dataset of patient 3D MRI and clinical data acquired
during several randomized clinical trials for MS treatments. Our results
present the first successful uncertainty-based causal Deep Learning (DL) model
to: (a) accurately predict future patient MS disability evolution (e.g. EDSS)
and treatment effects leveraging baseline MRI, and (b) permit the discovery of
subgroups of patients for which the model has high confidence in their response
to treatment even in clinical trials which did not reach their clinical
endpoints.

ÊëòË¶ÅÔºöÂü∫ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÂÄã‰∫∫ÂåñÈÜ´ÁôÇÔºåÂåÖÊã¨È†êÊ∏¨Êú™‰æÜÂÄã‰∫∫ÂåñËá®Â∫äÁñæÁóÖÈÄ≤Á®ãÂíåÊ≤ªÁôÇÂèçÊáâÔºåÂ∞áÂ∞çÈÜ´ÁôÇ‰øùÂÅ•ÂíåËó•Áâ©ÈñãÁôºÁî¢ÁîüÂ∑®Â§ßÂΩ±ÈüøÔºåÁâπÂà•ÊòØÂ∞çÊñºÈï∑Êúü„ÄÅË§áÈõú„ÄÅÁï∞Ë≥™ÊÄßÊºîÂåñ‰∏îÁÑ°Ê≥ïÊ≤ªÁôíÁöÑÁñæÁóÖÔºà‰æãÂ¶ÇÂ§öÁôºÊÄßÁ°¨ÂåñÁóá (MS)Ôºâ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁ¨¨‰∏ÄÂÄãÈö®Ê©üÂõ†ÊûúÊôÇÈñìÊ°ÜÊû∂ÔºåÈÄèÈÅéÁ•ûÁ∂ìÈö®Ê©üÂæÆÂàÜÊñπÁ®ãÂºè (NSDE) Â∞çÁñæÁóÖÈÄ≤Á®ãÁöÑÈÄ£Á∫åÊôÇÈñìÊºîÂåñÈÄ≤Ë°åÂª∫Ê®°„ÄÇÊâÄÊèêÂá∫ÁöÑÂõ†ÊûúÊé®Ë´ñÊ®°Âûã‰ª•ÁóÖÊÇ£ÁöÑÈ´òÁ∂≠Â∫¶ÂΩ±ÂÉèÔºàMRIÔºâÂíåË°®Ê†ºË≥áÊñô‰ΩúÁÇ∫Ëº∏ÂÖ•Ôºå‰∏¶È†êÊ∏¨ÊΩõÂú®Á©∫Èñì‰∏≠‰∏çÂêåÊ≤ªÁôÇÁöÑÂØ¶ÈöõÂíåÂèç‰∫ãÂØ¶ÈÄ≤Á®ãËªåË∑°„ÄÇNSDE ÂÖÅË®±‰º∞Ë®àÈ´òÂèØ‰ø°Â∫¶ÁöÑÂÄã‰∫∫ÂåñËªåË∑°ÂíåÊ≤ªÁôÇÊïàÊûú„ÄÇÂú®ÈáùÂ∞ç MS Ê≤ªÁôÇÈÄ≤Ë°åÁöÑÂπæÈ†ÖÈö®Ê©üËá®Â∫äË©¶È©ó‰∏≠ÔºåÂ∞ç‰∏ÄÂÄãÂ§ßÂûã„ÄÅÂ§ö‰∏≠ÂøÉ„ÄÅÂ∞àÊúâË≥áÊñôÈõÜÁöÑÁóÖÊÇ£ 3D MRI ÂíåËá®Â∫äË≥áÊñôÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©ó„ÄÇÊàëÂÄëÁöÑÁµêÊûúÂ±ïÁ§∫‰∫ÜÁ¨¨‰∏ÄÂÄãÊàêÂäüÁöÑÂü∫Êñº‰∏çÁ¢∫ÂÆöÊÄßÁöÑÂõ†ÊûúÊ∑±Â∫¶Â≠∏Áøí (DL) Ê®°ÂûãÔºö(a) Ê∫ñÁ¢∫È†êÊ∏¨Êú™‰æÜÁöÑÁóÖÊÇ£ MS ÊÆòÁñæÊºîÂåñÔºà‰æãÂ¶Ç EDSSÔºâÂíåÂà©Áî®Âü∫Á∑ö MRI ÁöÑÊ≤ªÁôÇÊïàÊûúÔºå‰ª•Âèä (b) ÂÖÅË®±ÁôºÁèæÂç≥‰ΩøÂú®Êú™ÈÅîÂà∞Ëá®Â∫äÁµÇÈªûÁöÑËá®Â∫äË©¶È©ó‰∏≠ÔºåÊ®°ÂûãÂ∞çÂÖ∂Ê≤ªÁôÇÂèçÊáâÂÖ∑ÊúâÈ´òÂ∫¶‰ø°ÂøÉÁöÑÁóÖÊÇ£Â≠êÁæ§„ÄÇ

##### **Large Language Model as a Universal Clinical Multi-task Decoder**
2406.12738v1 by Yujiang Wu, Hongjian Song, Jiawen Zhang, Xumeng Wen, Shun Zheng, Jiang Bian

The development of effective machine learning methodologies for enhancing the
efficiency and accuracy of clinical systems is crucial. Despite significant
research efforts, managing a plethora of diversified clinical tasks and
adapting to emerging new tasks remain significant challenges. This paper
presents a novel paradigm that employs a pre-trained large language model as a
universal clinical multi-task decoder. This approach leverages the flexibility
and diversity of language expressions to handle task topic variations and
associated arguments. The introduction of a new task simply requires the
addition of a new instruction template. We validate this framework across
hundreds of tasks, demonstrating its robustness in facilitating multi-task
predictions, performing on par with traditional multi-task learning and
single-task learning approaches. Moreover, it shows exceptional adaptability to
new tasks, with impressive zero-shot performance in some instances and superior
data efficiency in few-shot scenarios. This novel approach offers a unified
solution to manage a wide array of new and emerging tasks in clinical
applications.

ÊëòË¶ÅÔºö<paragraph>ÈñãÁôºÊúâÊïàÁöÑÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÔºå‰ª•ÊèêÂçáËá®Â∫äÁ≥ªÁµ±ÁöÑÊïàÁéáÂíåÊ∫ñÁ¢∫ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÂÑòÁÆ°Á†îÁ©∂‰ªòÂá∫Áõ∏Áï∂Â§ßÁöÑÂä™ÂäõÔºåÁÆ°ÁêÜÂ§ßÈáèÂ§öÊ®£ÂåñÁöÑËá®Â∫ä‰ªªÂãôÂíåÈÅ©ÊáâÊñ∞Ëàà‰ªªÂãô‰ªçÁÑ∂ÊòØÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÁØÑ‰æãÔºåÊé°Áî®È†êÂÖàË®ìÁ∑¥ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰ΩúÁÇ∫ÈÄöÁî®ÁöÑËá®Â∫äÂ§ö‰ªªÂãôËß£Á¢ºÂô®„ÄÇÊ≠§ÊñπÊ≥ïÂà©Áî®Ë™ûË®ÄË°®ÈÅîÁöÑÈùàÊ¥ªÊÄßËàáÂ§öÊ®£ÊÄß‰æÜËôïÁêÜ‰ªªÂãô‰∏ªÈ°åËÆäÂåñÂíåÁõ∏ÈóúË´ñÈªû„ÄÇÂºïÂÖ•Êñ∞‰ªªÂãôÂè™ÈúÄË¶ÅÊñ∞Â¢û‰∏ÄÂÄãÊñ∞ÁöÑÊåá‰ª§ÁØÑÊú¨„ÄÇÊàëÂÄëÈ©óË≠â‰∫ÜÈÄôÂÄãÊû∂ÊßãÂú®Êï∏ÁôæÂÄã‰ªªÂãô‰∏≠ÔºåË≠âÊòé‰∫ÜÂÆÉÂú®‰øÉÈÄ≤Â§ö‰ªªÂãôÈ†êÊ∏¨ÊñπÈù¢ÁöÑÁ©©ÂÅ•ÊÄßÔºåÂü∑Ë°åËàáÂÇ≥Áµ±Â§ö‰ªªÂãôÂ≠∏ÁøíÂíåÂñÆ‰ªªÂãôÂ≠∏ÁøíÊñπÊ≥ïÁõ∏Áï∂„ÄÇÊ≠§Â§ñÔºåÂÆÉÂ±ïÁèæÂá∫Â∞çÊñ∞‰ªªÂãôÁöÑÂçìË∂äÈÅ©ÊáâÊÄßÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÂÖ∑Êúâ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊïàËÉΩÔºåÂú®Â∞ëÈáèÂ≠∏ÁøíÂ†¥ÊôØ‰∏≠ÂÖ∑ÊúâÂÑ™Áï∞ÁöÑË≥áÊñôÊïàÁéá„ÄÇÈÄôÁ®ÆÊñ∞ÊñπÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁµ±‰∏ÄÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰æÜÁÆ°ÁêÜËá®Â∫äÊáâÁî®‰∏≠ÂêÑÁ®ÆÊñ∞ÁöÑÂíåÊñ∞Ëàà‰ªªÂãô„ÄÇ</paragraph>

##### **Online-Adaptive Anomaly Detection for Defect Identification in Aircraft Assembly**
2406.12698v1 by Siddhant Shete, Dennis Mronga, Ankita Jadhav, Frank Kirchner

Anomaly detection deals with detecting deviations from established patterns
within data. It has various applications like autonomous driving, predictive
maintenance, and medical diagnosis. To improve anomaly detection accuracy,
transfer learning can be applied to large, pre-trained models and adapt them to
the specific application context. In this paper, we propose a novel framework
for online-adaptive anomaly detection using transfer learning. The approach
adapts to different environments by selecting visually similar training images
and online fitting a normality model to EfficientNet features extracted from
the training subset. Anomaly detection is then performed by computing the
Mahalanobis distance between the normality model and the test image features.
Different similarity measures (SIFT/FLANN, Cosine) and normality models (MVG,
OCSVM) are employed and compared with each other. We evaluate the approach on
different anomaly detection benchmarks and data collected in controlled
laboratory settings. Experimental results showcase a detection accuracy
exceeding 0.975, outperforming the state-of-the-art ET-NET approach.

ÊëòË¶ÅÔºöÁï∞Â∏∏ÂÅµÊ∏¨ËôïÁêÜÂÅµÊ∏¨Ë≥áÊñô‰∏≠Êó¢ÊúâÊ®°ÂºèÁöÑÂÅèÂ∑Æ„ÄÇÂÆÉÊúâÂêÑÁ®ÆÊáâÁî®Ôºå‰æãÂ¶ÇËá™ÂãïÈßïÈßõ„ÄÅÈ†êÊ∏¨ÊÄßÁ∂≠Ë≠∑ÂíåÈÜ´ÁôÇË®∫Êñ∑„ÄÇÁÇ∫‰∫ÜÊîπÂñÑÁï∞Â∏∏ÂÅµÊ∏¨ÁöÑÊ∫ñÁ¢∫ÊÄßÔºåËΩâÁßªÂ≠∏ÁøíÂèØ‰ª•ÊáâÁî®ÊñºÂ§ßÂûãÈ†êË®ìÁ∑¥Ê®°ÂûãÔºå‰∏¶Â∞áÂÆÉÂÄëÈÅ©ÊáâÂà∞ÁâπÂÆöÁöÑÊáâÁî®ÊÉÖÂ¢É‰∏≠„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÁî®Êñº‰ΩøÁî®ËΩâÁßªÂ≠∏ÁøíÈÄ≤Ë°åÁ∑ö‰∏äËá™ÈÅ©ÊáâÁï∞Â∏∏ÂÅµÊ∏¨„ÄÇÊ≠§ÊñπÊ≥ïÈÄèÈÅéÈÅ∏ÊìáË¶ñË¶∫‰∏äÁõ∏‰ººÁöÑË®ìÁ∑¥ÂΩ±ÂÉèÔºå‰∏¶Á∑ö‰∏äÊì¨Âêà‰∏ÄÂÄãÂ∏∏ÊÖãÊ®°ÂûãÂà∞ÂæûË®ìÁ∑¥Â≠êÈõÜ‰∏≠ËêÉÂèñÁöÑ EfficientNet ÁâπÂæµÔºå‰æÜÈÅ©Êáâ‰∏çÂêåÁöÑÁí∞Â¢É„ÄÇÁÑ∂ÂæåÈÄèÈÅéË®àÁÆóÂ∏∏ÊÖãÊ®°ÂûãÂíåÊ∏¨Ë©¶ÂΩ±ÂÉèÁâπÂæµ‰πãÈñìÁöÑÈ¶¨Ê∞èË∑ùÈõ¢‰æÜÂü∑Ë°åÁï∞Â∏∏ÂÅµÊ∏¨„ÄÇÊé°Áî®‰∏çÂêåÁöÑÁõ∏‰ººÊÄßÂ∫¶ÈáèÔºàSIFT/FLANN„ÄÅÈ§òÂº¶ÔºâÂíåÂ∏∏ÊÖãÊ®°ÂûãÔºàMVG„ÄÅOCSVMÔºâÔºå‰∏¶Áõ∏‰∫íÊØîËºÉ„ÄÇÊàëÂÄëÂú®‰∏çÂêåÁöÑÁï∞Â∏∏ÂÅµÊ∏¨Âü∫Ê∫ñÂíåÂèóÊéßÂØ¶È©óÂÆ§Ë®≠ÂÆö‰∏≠Êî∂ÈõÜÁöÑË≥áÊñô‰∏äË©ï‰º∞Ê≠§ÊñπÊ≥ï„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÂÅµÊ∏¨Ê∫ñÁ¢∫Â∫¶Ë∂ÖÈÅé 0.975ÔºåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑ ET-NET ÊñπÊ≥ï„ÄÇ

##### **Transforming Surgical Interventions with Embodied Intelligence for Ultrasound Robotics**
2406.12651v1 by Huan Xu, Jinlin Wu, Guanglin Cao, Zhen Chen, Zhen Lei, Hongbin Liu

Ultrasonography has revolutionized non-invasive diagnostic methodologies,
significantly enhancing patient outcomes across various medical domains.
Despite its advancements, integrating ultrasound technology with robotic
systems for automated scans presents challenges, including limited command
understanding and dynamic execution capabilities. To address these challenges,
this paper introduces a novel Ultrasound Embodied Intelligence system that
synergistically combines ultrasound robots with large language models (LLMs)
and domain-specific knowledge augmentation, enhancing ultrasound robots'
intelligence and operational efficiency. Our approach employs a dual strategy:
firstly, integrating LLMs with ultrasound robots to interpret doctors' verbal
instructions into precise motion planning through a comprehensive understanding
of ultrasound domain knowledge, including APIs and operational manuals;
secondly, incorporating a dynamic execution mechanism, allowing for real-time
adjustments to scanning plans based on patient movements or procedural errors.
We demonstrate the effectiveness of our system through extensive experiments,
including ablation studies and comparisons across various models, showcasing
significant improvements in executing medical procedures from verbal commands.
Our findings suggest that the proposed system improves the efficiency and
quality of ultrasound scans and paves the way for further advancements in
autonomous medical scanning technologies, with the potential to transform
non-invasive diagnostics and streamline medical workflows.

ÊëòË¶ÅÔºöË∂ÖÈü≥Ê≥¢ÂæπÂ∫ïÊîπËÆä‰∫ÜÈùû‰æµÂÖ•ÊÄßË®∫Êñ∑ÊñπÊ≥ïÔºåÂ§ßÂπÖÊèêÂçáÂêÑÁ®ÆÈÜ´ÁôÇÈ†òÂüüÁöÑÊÇ£ËÄÖÊ≤ªÁôÇÊàêÊûú„ÄÇÂÑòÁÆ°ÊúâÈÄô‰∫õÈÄ≤Â±ïÔºå‰ΩÜÂ∞áË∂ÖÈü≥Ê≥¢ÊäÄË°ìËàáÊ©üÂô®‰∫∫Á≥ªÁµ±Êï¥Âêà‰ª•ÈÄ≤Ë°åËá™ÂãïÂåñÊéÉÊèèÊúÉÁî¢ÁîüÊåëÊà∞ÔºåÂåÖÊã¨ÊúâÈôêÁöÑÊåá‰ª§ÁêÜËß£ÂíåÂãïÊÖãÂü∑Ë°åËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑË∂ÖÈü≥Ê≥¢ÂÖ∑Ë∫´Êô∫ÊÖßÁ≥ªÁµ±ÔºåË©≤Á≥ªÁµ±Â∞áË∂ÖÈü≥Ê≥¢Ê©üÂô®‰∫∫ËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÊì¥ÂÖÖÁµêÂêàÂú®‰∏ÄËµ∑ÔºåÂ¢ûÂº∑Ë∂ÖÈü≥Ê≥¢Ê©üÂô®‰∫∫ÁöÑÊô∫ÊÖßÂíåÊìç‰ΩúÊïàÁéá„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊé°Áî®ÈõôÈáçÁ≠ñÁï•ÔºöÈ¶ñÂÖàÔºåÂ∞á LLM ËàáË∂ÖÈü≥Ê≥¢Ê©üÂô®‰∫∫Êï¥ÂêàÔºåÈÄèÈÅéÂÖ®Èù¢ÁêÜËß£Ë∂ÖÈü≥Ê≥¢È†òÂüüÁü•Ë≠òÔºàÂåÖÊã¨ API ÂíåÊìç‰ΩúÊâãÂÜäÔºâÔºåÂ∞áÈÜ´ÁîüÁöÑÂè£È†≠ÊåáÁ§∫ËΩâÊèõÁÇ∫Á≤æÁ¢∫ÁöÑÂãï‰ΩúË¶èÂäÉÔºõÂÖ∂Ê¨°ÔºåÂä†ÂÖ•ÂãïÊÖãÂü∑Ë°åÊ©üÂà∂ÔºåÂÖÅË®±Ê†πÊìöÊÇ£ËÄÖÁßªÂãïÊàñÁ®ãÂ∫èÈåØË™§Âç≥ÊôÇË™øÊï¥ÊéÉÊèèË®àÁï´„ÄÇÊàëÂÄëÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óÔºàÂåÖÊã¨Ê∂àËûçÁ†îÁ©∂ÂíåÂêÑÁ®ÆÊ®°ÂûãÁöÑÊØîËºÉÔºâË≠âÊòé‰∫ÜÊàëÂÄëÁ≥ªÁµ±ÁöÑÊúâÊïàÊÄßÔºåÂ±ïÁ§∫‰∫ÜÊ†πÊìöÂè£È†≠Êåá‰ª§Âü∑Ë°åÈÜ´ÁôÇÁ®ãÂ∫èÁöÑÈ°ØËëóÈÄ≤Ê≠•„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±ÊîπÂñÑ‰∫ÜË∂ÖÈü≥Ê≥¢ÊéÉÊèèÁöÑÊïàÁéáÂíåÂìÅË≥™Ôºå‰∏¶ÁÇ∫Ëá™‰∏ªÈÜ´ÁôÇÊéÉÊèèÊäÄË°ìÁöÑÈÄ≤‰∏ÄÊ≠•ÁôºÂ±ïÈã™Ë∑ØÔºåÊúâÊΩõÂäõËΩâÂûãÈùû‰æµÂÖ•ÊÄßË®∫Êñ∑‰∏¶Á∞°ÂåñÈÜ´ÁôÇÂ∑•‰ΩúÊµÅÁ®ã„ÄÇ

##### **An Empirical Study on the Fairness of Foundation Models for Multi-Organ Image Segmentation**
2406.12646v1 by Qin Li, Yizhe Zhang, Yan Li, Jun Lyu, Meng Liu, Longyu Sun, Mengting Sun, Qirong Li, Wenyue Mao, Xinran Wu, Yajing Zhang, Yinghua Chu, Shuo Wang, Chengyan Wang

The segmentation foundation model, e.g., Segment Anything Model (SAM), has
attracted increasing interest in the medical image community. Early pioneering
studies primarily concentrated on assessing and improving SAM's performance
from the perspectives of overall accuracy and efficiency, yet little attention
was given to the fairness considerations. This oversight raises questions about
the potential for performance biases that could mirror those found in
task-specific deep learning models like nnU-Net. In this paper, we explored the
fairness dilemma concerning large segmentation foundation models. We
prospectively curate a benchmark dataset of 3D MRI and CT scans of the organs
including liver, kidney, spleen, lung and aorta from a total of 1056 healthy
subjects with expert segmentations. Crucially, we document demographic details
such as gender, age, and body mass index (BMI) for each subject to facilitate a
nuanced fairness analysis. We test state-of-the-art foundation models for
medical image segmentation, including the original SAM, medical SAM and SAT
models, to evaluate segmentation efficacy across different demographic groups
and identify disparities. Our comprehensive analysis, which accounts for
various confounding factors, reveals significant fairness concerns within these
foundational models. Moreover, our findings highlight not only disparities in
overall segmentation metrics, such as the Dice Similarity Coefficient but also
significant variations in the spatial distribution of segmentation errors,
offering empirical evidence of the nuanced challenges in ensuring fairness in
medical image segmentation.

ÊëòË¶ÅÔºö‰æãÂ¶ÇÔºå‰ªª‰ΩïÂàÜÂâ≤Ê®°ÂûãÔºàSAMÔºâÁ≠âÂàÜÂâ≤Âü∫Á°ÄÊ®°ÂûãÂú®ÂåªÂ≠¶ÂΩ±ÂÉèÁ§æÁæ§‰∏≠Â∑≤ÂºïËµ∑Ë∂äÊù•Ë∂äÂ§öÁöÑÂÖ¥Ë∂£„ÄÇÊó©ÊúüÂºÄÂàõÊÄßÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠‰∫é‰ªéÊï¥‰ΩìÂáÜÁ°ÆÊÄßÂíåÊïàÁéáÁöÑËßíÂ∫¶ËØÑ‰º∞ÂíåÊîπËøõ SAM ÁöÑÊÄßËÉΩÔºå‰ΩÜÂæàÂ∞ëÂÖ≥Ê≥®ÂÖ¨Âπ≥ÊÄßËÄÉÈáè„ÄÇËøôÁßçÁñèÂøΩÂºïËµ∑‰∫Ü‰∫∫‰ª¨ÂØπÊÄßËÉΩÂÅèÂ∑ÆÁöÑË¥®ÁñëÔºåËøô‰∫õÂÅèÂ∑ÆÂèØËÉΩÂèçÊò†Âú® nnU-Net Á≠âÁâπÂÆö‰ªªÂä°Ê∑±Â∫¶Â≠¶‰π†Ê®°Âûã‰∏≠ÂèëÁé∞ÁöÑÂÅèÂ∑Æ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Êé¢ËÆ®‰∫Ü‰∏éÂ§ßÂûãÂàÜÂâ≤Âü∫Á°ÄÊ®°ÂûãÊúâÂÖ≥ÁöÑÂÖ¨Âπ≥ÊÄßÂõ∞Â¢É„ÄÇÊàë‰ª¨ÂâçÁûªÊÄßÂú∞Êï¥ÁêÜ‰∫Ü‰∏Ä‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´Êù•Ëá™ 1056 ÂêçÂÅ•Â∫∑ÂèóËØïËÄÖÁöÑÂô®ÂÆòÔºàÂåÖÊã¨ËÇùËÑè„ÄÅËÇæËÑè„ÄÅËÑæËÑè„ÄÅËÇ∫Âíå‰∏ªÂä®ËÑâÔºâÁöÑ 3D MRI Âíå CT Êâ´ÊèèÔºåÂπ∂Áî±‰∏ìÂÆ∂ËøõË°åÂàÜÂâ≤„ÄÇËá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØÔºåÊàë‰ª¨ËÆ∞ÂΩï‰∫ÜÊØè‰∏™ÂèóËØïËÄÖÁöÑÊÄßÂà´„ÄÅÂπ¥ÈæÑÂíå‰ΩìÈáçÊåáÊï∞ (BMI) Á≠â‰∫∫Âè£ÁªüËÆ°ËØ¶ÁªÜ‰ø°ÊÅØÔºå‰ª•‰øÉËøõÁªÜËá¥ÂÖ•ÂæÆÁöÑÂÖ¨Âπ≥ÊÄßÂàÜÊûê„ÄÇÊàë‰ª¨ÊµãËØï‰∫ÜÁî®‰∫éÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤ÁöÑÊúÄÊñ∞Âü∫Á°ÄÊ®°ÂûãÔºåÂåÖÊã¨ÂéüÂßã SAM„ÄÅÂåªÂ≠¶ SAM Âíå SAT Ê®°ÂûãÔºå‰ª•ËØÑ‰º∞‰∏çÂêå‰∫∫Âè£Áæ§‰ΩìÁöÑÂàÜÂâ≤ÊïàÊûúÂπ∂ÊâæÂá∫Â∑ÆÂºÇ„ÄÇÊàë‰ª¨ÁöÑÁªºÂêàÂàÜÊûêËÄÉËôë‰∫ÜÂêÑÁßçÊ∑∑ÊùÇÂõ†Á¥†ÔºåÊè≠Á§∫‰∫ÜËøô‰∫õÂü∫Á°ÄÊ®°Âûã‰∏≠Â≠òÂú®ÁöÑÈáçÂ§ßÂÖ¨Âπ≥ÊÄßÈóÆÈ¢ò„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûú‰∏ç‰ªÖÁ™ÅÂá∫‰∫ÜÊï¥‰ΩìÂàÜÂâ≤ÊåáÊ†áÔºà‰æãÂ¶ÇÈ™∞Â≠êÁõ∏‰ººÁ≥ªÊï∞ÔºâÁöÑÂ∑ÆÂºÇÔºåËøòÁ™ÅÂá∫‰∫ÜÂàÜÂâ≤ÈîôËØØÁöÑÁ©∫Èó¥ÂàÜÂ∏ÉÁöÑÊòæÁùÄÂ∑ÆÂºÇÔºå‰∏∫Á°Æ‰øùÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄßÊèê‰æõ‰∫ÜÁªÜÂæÆÊåëÊàòÁöÑÁªèÈ™åËØÅÊçÆ„ÄÇ

##### **Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine**
2406.12449v1 by Rui Yang, Yilin Ning, Emilia Keppo, Mingxuan Liu, Chuan Hong, Danielle S Bitterman, Jasmine Chiat Ling Ong, Daniel Shu Wei Ting, Nan Liu

Generative artificial intelligence (AI) has brought revolutionary innovations
in various fields, including medicine. However, it also exhibits limitations.
In response, retrieval-augmented generation (RAG) provides a potential
solution, enabling models to generate more accurate contents by leveraging the
retrieval of external knowledge. With the rapid advancement of generative AI,
RAG can pave the way for connecting this transformative technology with medical
applications and is expected to bring innovations in equity, reliability, and
personalization to health care.

ÊëòË¶ÅÔºöÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩ (AI) Â∑≤ÁÇ∫ÂåÖÊã¨ÈÜ´Â≠∏Âú®ÂÖßÁöÑÂêÑÂÄãÈ†òÂüüÂ∏∂‰æÜÈù©ÂëΩÊÄßÁöÑÂâµÊñ∞„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰πüË°®ÁèæÂá∫Â±ÄÈôêÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊΩõÂú®ÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰ΩøÊ®°ÂûãËÉΩÂ§†ÈÄèÈÅéÂà©Áî®Â§ñÈÉ®Áü•Ë≠òÁöÑÊ™¢Á¥¢‰æÜÁî¢ÁîüÊõ¥Ê∫ñÁ¢∫ÁöÑÂÖßÂÆπ„ÄÇÈö®ËëóÁîüÊàêÂºè AI ÁöÑÂø´ÈÄüÈÄ≤Â±ïÔºåRAG ÂèØ‰ª•ÁÇ∫Â∞áÈÄôÈ†ÖËΩâÂûãÊäÄË°ìËàáÈÜ´ÁôÇÊáâÁî®Áõ∏ÈÄ£Èã™Ë∑ØÔºå‰∏¶ÊúâÊúõÁÇ∫ÈÜ´ÁôÇ‰øùÂÅ•Â∏∂‰æÜÂÖ¨Âπ≥ÊÄß„ÄÅÂèØÈù†ÊÄßÂíåÂÄã‰∫∫ÂåñÁöÑÂâµÊñ∞„ÄÇ

##### **Adversarial Attacks on Large Language Models in Medicine**
2406.12259v1 by Yifan Yang, Qiao Jin, Furong Huang, Zhiyong Lu

The integration of Large Language Models (LLMs) into healthcare applications
offers promising advancements in medical diagnostics, treatment
recommendations, and patient care. However, the susceptibility of LLMs to
adversarial attacks poses a significant threat, potentially leading to harmful
outcomes in delicate medical contexts. This study investigates the
vulnerability of LLMs to two types of adversarial attacks in three medical
tasks. Utilizing real-world patient data, we demonstrate that both open-source
and proprietary LLMs are susceptible to manipulation across multiple tasks.
This research further reveals that domain-specific tasks demand more
adversarial data in model fine-tuning than general domain tasks for effective
attack execution, especially for more capable models. We discover that while
integrating adversarial data does not markedly degrade overall model
performance on medical benchmarks, it does lead to noticeable shifts in
fine-tuned model weights, suggesting a potential pathway for detecting and
countering model attacks. This research highlights the urgent need for robust
security measures and the development of defensive mechanisms to safeguard LLMs
in medical applications, to ensure their safe and effective deployment in
healthcare settings.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êï¥ÂêàÂà∞ÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®Á®ãÂºè‰∏≠ÔºåÂú®ÈÜ´ÁôÇË®∫Êñ∑„ÄÅÊ≤ªÁôÇÂª∫Ë≠∞ÂíåÁóÖ‰∫∫ÁÖßË≠∑ÊñπÈù¢Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÈÄ≤Â±ï„ÄÇÁÑ∂ËÄåÔºåLLM Â∞çÂ∞çÊäóÊÄßÊîªÊìäÁöÑÊïèÊÑüÊÄßÊßãÊàêÈáçÂ§ßÂ®ÅËÑÖÔºåÂèØËÉΩÂ∞éËá¥Âú®ÂæÆÂ¶ôÁöÑÈÜ´ÁôÇÁí∞Â¢É‰∏≠ÈÄ†ÊàêÊúâÂÆ≥ÁöÑÂæåÊûú„ÄÇÊú¨Á†îÁ©∂Ë™øÊü•‰∫Ü LLM Âú®‰∏âÈ†ÖÈÜ´ÁôÇ‰ªªÂãô‰∏≠Â∞çÂÖ©Á®ÆÂ∞çÊäóÊÄßÊîªÊìäÁöÑËÑÜÂº±ÊÄß„ÄÇÂà©Áî®ÁúüÂØ¶‰∏ñÁïåÁöÑÁóÖ‰∫∫Ë≥áÊñôÔºåÊàëÂÄëË≠âÊòéÈñãÊ∫êÂíåÂ∞àÊúâ LLM ÈÉΩÂÆπÊòìÂèóÂà∞Â§öÈ†Ö‰ªªÂãôÁöÑÊìçÁ∏±„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÈÄ≤‰∏ÄÊ≠•Êè≠Á§∫ÔºåËàá‰∏ÄËà¨È†òÂüü‰ªªÂãôÁõ∏ÊØîÔºåÁâπÂÆöÈ†òÂüü‰ªªÂãôÈúÄË¶ÅÂú®Ê®°ÂûãÂæÆË™ø‰∏≠Êõ¥Â§öÂ∞çÊäóÊÄßË≥áÊñôÊâçËÉΩÊúâÊïàÂü∑Ë°åÊîªÊìäÔºåÁâπÂà•ÊòØÂ∞çÊñºÂäüËÉΩÊõ¥Âº∑Â§ßÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÁôºÁèæÔºåÈõñÁÑ∂Êï¥ÂêàÂ∞çÊäóÊÄßË≥áÊñô‰∏¶‰∏çÊúÉÈ°ØËëóÈôç‰ΩéÈÜ´ÁôÇÂü∫Ê∫ñ‰∏äÁöÑÊï¥È´îÊ®°ÂûãÊïàËÉΩÔºå‰ΩÜÂÆÉÁ¢∫ÂØ¶ÊúÉÂ∞éËá¥ÂæÆË™øÊ®°ÂûãÊ¨äÈáçÁôºÁîüÊòéÈ°ØÁöÑËΩâËÆäÔºåÈÄôË°®ÊòéÊúâÊΩõÂú®ÈÄîÂæëÂèØ‰ª•ÂÅµÊ∏¨ÂíåÂèçÂà∂Ê®°ÂûãÊîªÊìä„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÂ∞çÂÅ•ÂÖ®ÂÆâÂÖ®Êé™ÊñΩÂíåÈò≤Á¶¶Ê©üÂà∂ÈñãÁôºÁöÑËø´ÂàáÈúÄÊ±ÇÔºå‰ª•‰øùË≠∑ÈÜ´ÁôÇÊáâÁî®Á®ãÂºè‰∏≠ÁöÑ LLMÔºåÁ¢∫‰øùÂÆÉÂÄëÂú®ÈÜ´ÁôÇ‰øùÂÅ•Áí∞Â¢É‰∏≠ÂÆâÂÖ®ÊúâÊïàÂú∞ÈÉ®ÁΩ≤„ÄÇ

##### **Enhancing Diagnostic Reliability of Foundation Model with Uncertainty Estimation in OCT Images**
2406.16942v1 by Yuanyuan Peng, Aidi Lin, Meng Wang, Tian Lin, Ke Zou, Yinglin Cheng, Tingkun Shi, Xulong Liao, Lixia Feng, Zhen Liang, Xinjian Chen, Huazhu Fu, Haoyu Chen

Inability to express the confidence level and detect unseen classes has
limited the clinical implementation of artificial intelligence in the
real-world. We developed a foundation model with uncertainty estimation (FMUE)
to detect 11 retinal conditions on optical coherence tomography (OCT). In the
internal test set, FMUE achieved a higher F1 score of 96.76% than two
state-of-the-art algorithms, RETFound and UIOS, and got further improvement
with thresholding strategy to 98.44%. In the external test sets obtained from
other OCT devices, FMUE achieved an accuracy of 88.75% and 92.73% before and
after thresholding. Our model is superior to two ophthalmologists with a higher
F1 score (95.17% vs. 61.93% &71.72%). Besides, our model correctly predicts
high uncertainty scores for samples with ambiguous features, of
non-target-category diseases, or with low-quality to prompt manual checks and
prevent misdiagnosis. FMUE provides a trustworthy method for automatic retinal
anomalies detection in the real-world clinical open set environment.

ÊëòË¶ÅÔºöÁÑ°Ê≥ïË°®ÈÅî‰ø°ÂøÉÁ≠âÁ¥öÂíåÂÅµÊ∏¨Êú™Ë¶ãÈ°ûÂà•Â∑≤ÈôêÂà∂‰∫Ü‰∫∫Â∑•Êô∫ÊÖßÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠ÁöÑËá®Â∫äÂØ¶ÊñΩ„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÂÖ∑Êúâ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®à (FMUE) ÁöÑÂü∫Á§éÊ®°ÂûãÔºå‰ª•Âú®ÂÖâÂ≠∏Áõ∏Âπ≤Êñ∑Â±§ÊéÉÊèè (OCT) ‰∏äÂÅµÊ∏¨ 11 Á®ÆË¶ñÁ∂≤ËÜúÁñæÁóÖ„ÄÇÂú®ÂÖßÈÉ®Ê∏¨Ë©¶ÈõÜ‰∏≠ÔºåFMUE ÈÅîÂà∞‰∫ÜÊØîÂÖ©Á®ÆÊúÄÂÖàÈÄ≤ÁöÑÊºîÁÆóÊ≥ï RETFound Âíå UIOS Êõ¥È´òÁöÑ F1 ÂàÜÊï∏ 96.76%Ôºå‰∏¶ÈÄöÈÅéÈñæÂÄºÁ≠ñÁï•ÈÄ≤‰∏ÄÊ≠•ÊèêÂçáËá≥ 98.44%„ÄÇÂú®ÂæûÂÖ∂‰ªñ OCT Ë®≠ÂÇôÁç≤ÂæóÁöÑÂ§ñÈÉ®Ê∏¨Ë©¶ÈõÜ‰∏≠ÔºåFMUE Âú®ÈñæÂÄºËôïÁêÜÂâçÂæåÂàÜÂà•ÈÅîÂà∞‰∫Ü 88.75% Âíå 92.73% ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂÑ™ÊñºÂÖ©‰ΩçÁúºÁßëÈÜ´ÁîüÔºåÂÖ∑ÊúâÊõ¥È´òÁöÑ F1 ÂàÜÊï∏Ôºà95.17% Â∞çÊØî 61.93% Âíå 71.72%Ôºâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•Ê≠£Á¢∫È†êÊ∏¨ÂÖ∑ÊúâÊ®°Á®úÂÖ©ÂèØÁâπÂæµ„ÄÅÈùûÁõÆÊ®ôÈ°ûÂà•ÁñæÁóÖÊàñ‰ΩéÂìÅË≥™ÁöÑÊ®£Êú¨ÁöÑÈ´ò‰∏çÁ¢∫ÂÆöÊÄßÂàÜÊï∏Ôºå‰ª•ÊèêÁ§∫ÊâãÂãïÊ™¢Êü•‰∏¶Èò≤Ê≠¢Ë™§Ë®∫„ÄÇFMUE ÁÇ∫Âú®ÁèæÂØ¶‰∏ñÁïåËá®Â∫äÈñãÊîæÈõÜÁí∞Â¢É‰∏≠Ëá™ÂãïÊ™¢Ê∏¨Ë¶ñÁ∂≤ËÜúÁï∞Â∏∏Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂÄºÂæó‰ø°Ë≥¥ÁöÑÊñπÊ≥ï„ÄÇ

##### **Time Series Modeling for Heart Rate Prediction: From ARIMA to Transformers**
2406.12199v2 by Haowei Ni, Shuchen Meng, Xieming Geng, Panfeng Li, Zhuoying Li, Xupeng Chen, Xiaotong Wang, Shiyao Zhang

Cardiovascular disease (CVD) is a leading cause of death globally,
necessitating precise forecasting models for monitoring vital signs like heart
rate, blood pressure, and ECG. Traditional models, such as ARIMA and Prophet,
are limited by their need for manual parameter tuning and challenges in
handling noisy, sparse, and highly variable medical data. This study
investigates advanced deep learning models, including LSTM, and
transformer-based architectures, for predicting heart rate time series from the
MIT-BIH Database. Results demonstrate that deep learning models, particularly
PatchTST, significantly outperform traditional models across multiple metrics,
capturing complex patterns and dependencies more effectively. This research
underscores the potential of deep learning to enhance patient monitoring and
CVD management, suggesting substantial clinical benefits. Future work should
extend these findings to larger, more diverse datasets and real-world clinical
applications to further validate and optimize model performance.

ÊëòË¶ÅÔºöÂøÉË°ÄÁÆ°ÁñæÁóÖ (CVD) ÊòØÂÖ®ÁêÉ‰∏ªË¶ÅÁöÑÊ≠ª‰∫°ÂéüÂõ†Ôºå
ÈúÄË¶ÅÁ≤æÁ¢∫ÁöÑÈ†êÊ∏¨Ê®°Âûã‰æÜÁõ£Ê∏¨ÁîüÂëΩÂæµË±°Ôºå‰æãÂ¶ÇÂøÉÁéá„ÄÅË°ÄÂ£ìÂíåÂøÉÈõªÂúñ„ÄÇÂÇ≥Áµ±Ê®°ÂûãÔºå‰æãÂ¶Ç ARIMA Âíå ProphetÔºå
ÂèóÂà∞ÊâãÂãïÂèÉÊï∏Ë™øÊï¥ÈúÄÊ±ÇÂíåËôïÁêÜÊúâÈõúË®ä„ÄÅÁ®ÄÁñè‰∏îËÆäÂåñÊ•µÂ§ßÁöÑÈÜ´ÁôÇÊï∏ÊìöÁöÑÊåëÊà∞ÊâÄÈôêÂà∂„ÄÇÈÄôÈ†ÖÁ†îÁ©∂
Êé¢Ë®éÈÄ≤ÈöéÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºåÂåÖÊã¨ LSTMÔºå‰ª•ÂèäÂü∫ÊñºËΩâÊèõÂô®ÁöÑÊû∂ÊßãÔºåÁî®ÊñºÂæû
MIT-BIH Ë≥áÊñôÂ∫´È†êÊ∏¨ÂøÉÁéáÊôÇÈñìÂ∫èÂàó„ÄÇÁµêÊûúË≠âÊòéÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºåÁâπÂà•ÊòØ
PatchTSTÔºåÂú®Â§öÈ†ÖÊåáÊ®ô‰∏äÊòéÈ°ØÂÑ™ÊñºÂÇ≥Áµ±Ê®°ÂûãÔºåÊõ¥ÊúâÊïàÂú∞Êì∑ÂèñË§áÈõúÊ®°ÂºèÂíå‰æùË≥¥Èóú‰øÇ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂
Âº∑Ë™øÊ∑±Â∫¶Â≠∏ÁøíÂú®Â¢ûÂº∑ÊÇ£ËÄÖÁõ£Ê∏¨Âíå
CVD ÁÆ°ÁêÜÁöÑÊΩõÂäõÔºå‰∏¶ÊèêÂá∫ÂØ¶Ë≥™ÊÄßÁöÑËá®Â∫äÁõäËôï„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂Êáâ
Â∞áÈÄô‰∫õÁôºÁèæÊì¥Â±ïÂà∞Êõ¥Â§ß„ÄÅÊõ¥Â§öÊ®£ÂåñÁöÑË≥áÊñôÈõÜÂíåÁúüÂØ¶‰∏ñÁïåÁöÑËá®Â∫ä
ÊáâÁî®Ôºå‰ª•ÈÄ≤‰∏ÄÊ≠•È©óË≠âÂíåÊúÄ‰Ω≥ÂåñÊ®°ÂûãÊïàËÉΩ„ÄÇ

##### **Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models**
2406.12182v1 by Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou, Donglin Hao, Yonghua Lin

Recently, both closed-source LLMs and open-source communities have made
significant strides, outperforming humans in various general domains. However,
their performance in specific professional fields such as medicine, especially
within the open-source community, remains suboptimal due to the complexity of
medical knowledge. We propose Aquila-Med, a bilingual medical LLM based on
Aquila, addressing these challenges through continue pre-training, supervised
fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). We
construct a large-scale Chinese and English medical dataset for continue
pre-training and a high-quality SFT dataset, covering extensive medical
specialties. Additionally, we develop a high-quality Direct Preference
Optimization (DPO) dataset for further alignment. Aquila-Med achieves notable
results across single-turn, multi-turn dialogues, and medical multiple-choice
questions, demonstrating the effectiveness of our approach. We open-source the
datasets and the entire training process, contributing valuable resources to
the research community. Our models and datasets will released at
https://huggingface.co/BAAI/AquilaMed-RL.

ÊëòË¶ÅÔºöËøëÊù•ÔºåÈó≠Ê∫ê LLM ÂíåÂºÄÊ∫êÁ§æÂå∫ÈÉΩÂèñÂæó‰∫ÜÈáçÂ§ßËøõÂ±ïÔºåÂú®ÂêÑÁßçÈÄöÁî®È¢ÜÂüüÁöÑË°®Áé∞ÈÉΩ‰ºò‰∫é‰∫∫Á±ª„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨Âú®ÂåªÂ≠¶Á≠âÁâπÂÆö‰∏ì‰∏öÈ¢ÜÂüüÁöÑÊÄßËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂºÄÊ∫êÁ§æÂå∫‰∏≠ÔºåÁî±‰∫éÂåªÂ≠¶Áü•ËØÜÁöÑÂ§çÊùÇÊÄßÔºå‰ªçÁÑ∂‰∏çÂ§üÁêÜÊÉ≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü Aquila-MedÔºå‰∏Ä‰∏™Âü∫‰∫é Aquila ÁöÑÂèåËØ≠ÂåªÂ≠¶ LLMÔºåÈÄöËøáÊåÅÁª≠È¢ÑËÆ≠ÁªÉ„ÄÅÁõëÁù£ÂæÆË∞É (SFT) Âíå‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π† (RLHF) Êù•Â∫îÂØπËøô‰∫õÊåëÊàò„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑ‰∏≠Ëã±ÊñáÂåªÂ≠¶Êï∞ÊçÆÈõÜÔºåÁî®‰∫éÊåÅÁª≠È¢ÑËÆ≠ÁªÉÂíåÈ´òË¥®ÈáèÁöÑ SFT Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñÂπøÊ≥õÁöÑÂåªÂ≠¶‰∏ì‰∏ö„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™È´òË¥®ÈáèÁöÑÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ (DPO) Êï∞ÊçÆÈõÜÔºå‰ª•Ëøõ‰∏ÄÊ≠•ÂØπÈΩê„ÄÇAquila-Med Âú®ÂçïËΩÆ„ÄÅÂ§öËΩÆÂØπËØùÂíåÂåªÂ≠¶Â§öÈ°πÈÄâÊã©È¢ò‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊàêÊûúÔºåËØÅÊòé‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÊàë‰ª¨ÂºÄÊ∫êÊï∞ÊçÆÈõÜÂíåÊï¥‰∏™ËÆ≠ÁªÉËøáÁ®ãÔºå‰∏∫Á†îÁ©∂Á§æÂå∫Ë¥°ÁåÆ‰∫ÜÂÆùË¥µÁöÑËµÑÊ∫ê„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜÂ∞ÜÂú® https://huggingface.co/BAAI/AquilaMed-RL ÂèëÂ∏É„ÄÇ

##### **Slicing Through Bias: Explaining Performance Gaps in Medical Image Analysis using Slice Discovery Methods**
2406.12142v1 by Vincent Olesen, Nina Weng, Aasa Feragen, Eike Petersen

Machine learning models have achieved high overall accuracy in medical image
analysis. However, performance disparities on specific patient groups pose
challenges to their clinical utility, safety, and fairness. This can affect
known patient groups - such as those based on sex, age, or disease subtype - as
well as previously unknown and unlabeled groups. Furthermore, the root cause of
such observed performance disparities is often challenging to uncover,
hindering mitigation efforts. In this paper, to address these issues, we
leverage Slice Discovery Methods (SDMs) to identify interpretable
underperforming subsets of data and formulate hypotheses regarding the cause of
observed performance disparities. We introduce a novel SDM and apply it in a
case study on the classification of pneumothorax and atelectasis from chest
x-rays. Our study demonstrates the effectiveness of SDMs in hypothesis
formulation and yields an explanation of previously observed but unexplained
performance disparities between male and female patients in widely used chest
X-ray datasets and models. Our findings indicate shortcut learning in both
classification tasks, through the presence of chest drains and ECG wires,
respectively. Sex-based differences in the prevalence of these shortcut
features appear to cause the observed classification performance gap,
representing a previously underappreciated interaction between shortcut
learning and model fairness analyses.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê‰∏≠Áç≤Âæó‰∫ÜÂæàÈ´òÁöÑÊï¥È´îÊ∫ñÁ¢∫Â∫¶„ÄÇÁÑ∂ËÄåÔºåÁâπÂÆöÊÇ£ËÄÖÁæ§È´îÁöÑÊïàËÉΩÂ∑ÆÁï∞Â∞çÂÖ∂Ëá®Â∫äÊïàÁî®„ÄÅÂÆâÂÖ®ÊÄßËàáÂÖ¨Âπ≥ÊÄßÊßãÊàêÊåëÊà∞„ÄÇÈÄôÂèØËÉΩÊúÉÂΩ±ÈüøÂ∑≤Áü•ÁöÑÊÇ£ËÄÖÁæ§È´îÔºà‰æãÂ¶ÇÂü∫ÊñºÊÄßÂà•„ÄÅÂπ¥ÈΩ°ÊàñÁñæÁóÖ‰∫ûÂûãÔºâ‰ª•ÂèäÂÖàÂâçÊú™Áü•ÂíåÊú™Ê®ôË®òÁöÑÁæ§È´î„ÄÇÊ≠§Â§ñÔºåÈÄôÁ®ÆËßÄÂØüÂà∞ÁöÑÊïàËÉΩÂ∑ÆÁï∞ÁöÑÊ†πÊú¨ÂéüÂõ†ÈÄöÂ∏∏Èõ£‰ª•ÁôºÁèæÔºåÈòªÁ§ô‰∫ÜÊîπÂñÑÊé™ÊñΩ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂà©Áî®ÂàáÁâáÁôºÁèæÊñπÊ≥ï (SDM) ‰æÜË≠òÂà•ÂèØËß£ÈáãÁöÑË≥áÊñôÊïàËÉΩ‰∏ç‰Ω≥ÁöÑÂ≠êÈõÜÔºå‰∏¶ÈáùÂ∞çËßÄÂØüÂà∞ÁöÑÊïàËÉΩÂ∑ÆÁï∞ÂéüÂõ†Âà∂ÂÆöÂÅáË®≠„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑ SDMÔºå‰∏¶Âú®ËÉ∏ÈÉ® X ÂÖâÁâá‰∏≠ËÇ∫ÁÇéÂíåËÇ∫‰∏çÂºµÁöÑÂàÜÈ°ûÊ°à‰æãÁ†îÁ©∂‰∏≠ÊáâÁî®ÂÆÉ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ë≠âÊòé‰∫Ü SDM Âú®ÂÅáË®≠Âà∂ÂÆö‰∏≠ÁöÑÊúâÊïàÊÄßÔºå‰∏¶Â∞çÂú®Âª£Ê≥õ‰ΩøÁî®ÁöÑËÉ∏ÈÉ® X ÂÖâÁâáË≥áÊñôÈõÜÂíåÊ®°Âûã‰∏≠Áî∑ÊÄßÂíåÂ•≥ÊÄßÊÇ£ËÄÖ‰πãÈñìÂÖàÂâçËßÄÂØüÂà∞‰ΩÜÁÑ°Ê≥ïËß£ÈáãÁöÑÊïàËÉΩÂ∑ÆÁï∞Êèê‰æõ‰∫ÜËß£Èáã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÂú®ÂàÜÈ°û‰ªªÂãô‰∏≠ÔºåÈÄèÈÅéËÉ∏ËÖîÂºïÊµÅÁÆ°Âíå ECG Á∑öË∑ØÁöÑÂ≠òÂú®ÔºåÂ≠òÂú®Êç∑ÂæëÂ≠∏Áøí„ÄÇÈÄô‰∫õÊç∑ÂæëÁâπÂæµÂú®ÊÇ£ÁóÖÁéá‰∏äÁöÑÂü∫ÊñºÊÄßÂà•ÁöÑÂ∑ÆÁï∞‰ºº‰πéÂ∞éËá¥‰∫ÜËßÄÂØüÂà∞ÁöÑÂàÜÈ°ûÊïàËÉΩÂ∑ÆË∑ùÔºåÈÄô‰ª£Ë°®‰∫ÜÊç∑ÂæëÂ≠∏ÁøíÂíåÊ®°ÂûãÂÖ¨Âπ≥ÊÄßÂàÜÊûê‰πãÈñìÂÖàÂâçÊú™Ë¢´ÈáçË¶ñÁöÑ‰∫§‰∫í‰ΩúÁî®„ÄÇ

##### **WellDunn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions**
2406.12058v2 by Seyedali Mohammadi, Edward Raff, Jinendra Malekar, Vedant Palit, Francis Ferraro, Manas Gaur

Language Models (LMs) are being proposed for mental health applications where
the heightened risk of adverse outcomes means predictive performance may not be
a sufficient litmus test of a model's utility in clinical practice. A model
that can be trusted for practice should have a correspondence between
explanation and clinical determination, yet no prior research has examined the
attention fidelity of these models and their effect on ground truth
explanations. We introduce an evaluation design that focuses on the robustness
and explainability of LMs in identifying Wellness Dimensions (WD). We focus on
two mental health and well-being datasets: (a) Multi-label Classification-based
MultiWD, and (b) WellXplain for evaluating attention mechanism veracity against
expert-labeled explanations. The labels are based on Halbert Dunn's theory of
wellness, which gives grounding to our evaluation. We reveal four surprising
results about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4
lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM fails to deliver any
remarkable improvements in performance or explanations. (2) Re-examining LMs'
predictions based on a confidence-oriented loss function reveals a significant
performance drop. (3) Across all LMs/LLMs, the alignment between attention and
explanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental
health-specific LMs/LLMs overlook domain-specific knowledge and undervalue
explanations, causing these discrepancies. This study highlights the need for
further research into their consistency and explanations in mental health and
well-being.

ÊëòË¶ÅÔºöË™ûË®ÄÊ®°Âûã (LM) Â∑≤Ë¢´ÊèêË≠∞Áî®ÊñºÂøÉÁêÜÂÅ•Â∫∑ÊáâÁî®ÔºåÂú®ÈÄô‰∫õÊáâÁî®‰∏≠Ôºå‰∏çËâØÁµêÊûúÁöÑÈ¢®Èö™ÂçáÈ´òÊÑèÂë≥ËëóÈ†êÊ∏¨ÊÄßË°®ÁèæÂèØËÉΩ‰∏çË∂≥‰ª•‰ΩúÁÇ∫Ëá®Â∫äÂØ¶Âãô‰∏≠Ê®°ÂûãÊïàÁî®ÁöÑË©¶ÈáëÁü≥„ÄÇÂèØ‰ø°Ë≥¥ÂØ¶ÂãôÁöÑÊ®°ÂûãÊáâÂú®Ëß£ÈáãÂíåËá®Â∫äÂà§Êñ∑‰πãÈñìÊúâÂ∞çÊáâÈóú‰øÇÔºå‰ΩÜÊ≤íÊúâÂÖàÂâçÁöÑÁ†îÁ©∂Êé¢Ë®éÈÅéÈÄô‰∫õÊ®°ÂûãÁöÑÊ≥®ÊÑèÂäõ‰øùÁúüÂ∫¶ÂèäÂÖ∂Â∞çÁúüÂØ¶Ëß£ÈáãÁöÑÂΩ±Èüø„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆË©ï‰º∞Ë®≠Ë®àÔºåÂ∞àÊ≥®Êñº LM Âú®Ë≠òÂà•ÂÅ•Â∫∑Á∂≠Â∫¶ (WD) ÊôÇÁöÑÁ©©ÂÅ•ÊÄßÂíåÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÂ∞àÊ≥®ÊñºÂÖ©ÂÄãÂøÉÁêÜÂÅ•Â∫∑ÂíåÁ¶èÁ•âÊï∏ÊìöÈõÜÔºö(a) Âü∫ÊñºÂ§öÊ®ôÁ±§ÂàÜÈ°ûÁöÑÂ§ö WDÔºå‰ª•Âèä (b) WellXplainÔºåÁî®ÊñºË©ï‰º∞Ê≥®ÊÑèÂäõÊ©üÂà∂ÁúüÂØ¶ÊÄßËàáÂ∞àÂÆ∂Ê®ôË®òÁöÑËß£Èáã„ÄÇÈÄô‰∫õÊ®ôÁ±§Âü∫Êñº Halbert Dunn ÁöÑÂÅ•Â∫∑ÁêÜË´ñÔºåÁÇ∫ÊàëÂÄëÁöÑË©ï‰º∞Êèê‰æõ‰∫Ü‰æùÊìö„ÄÇÊàëÂÄëÊè≠Á§∫‰∫ÜÊúâÈóú LM/LLM ÁöÑÂõõÂÄãÈ©ö‰∫∫ÁµêÊûúÔºö(1) ÂÑòÁÆ°ÂÖ∑ÊúâÈ°û‰∫∫ÁöÑËÉΩÂäõÔºåGPT-3.5/4 ‰ªçËêΩÂæåÊñº RoBERTa Âíå MedAlpacaÔºåËÄåÂæÆË™øÁöÑ LLM Âú®ÊïàËÉΩÊàñËß£ÈáãÊñπÈù¢‰∏¶Êú™Â∏∂‰æÜ‰ªª‰ΩïÈ°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇ(2) Ê†πÊìö‰ª•‰ø°ÂøÉÁÇ∫Â∞éÂêëÁöÑÊêçÂ§±ÂáΩÊï∏ÈáçÊñ∞Ê™¢Êü• LM ÁöÑÈ†êÊ∏¨ÔºåÈ°ØÁ§∫ÊïàËÉΩÂ§ßÂπÖ‰∏ãÈôç„ÄÇ(3) Âú®ÊâÄÊúâ LM/LLM ‰∏≠ÔºåÊ≥®ÊÑèÂäõÂíåËß£Èáã‰πãÈñìÁöÑ‰∏ÄËá¥ÊÄß‰ªçÁÑ∂Âæà‰ΩéÔºåLLM ÁöÑÂæóÂàÜÊÖò‰∏çÂøçÁùπÔºåÂÉÖÁÇ∫ 0.0„ÄÇ(4) Â§ßÂ§öÊï∏ÂøÉÁêÜÂÅ•Â∫∑Â∞àÁî® LM/LLM ÂøΩË¶ñÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÔºå‰∏¶‰Ωé‰º∞Ëß£ÈáãÔºåÂ∞éËá¥ÈÄô‰∫õÂ∑ÆÁï∞„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™øÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÂÆÉÂÄëÂú®ÂøÉÁêÜÂÅ•Â∫∑ÂíåÁ¶èÁ•âÊñπÈù¢ÁöÑ‰∏ÄËá¥ÊÄßÂíåËß£Èáã„ÄÇ

##### **MedCalc-Bench: Evaluating Large Language Models for Medical Calculations**
2406.12036v3 by Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu

As opposed to evaluating computation and logic-based reasoning, current
benchmarks for evaluating large language models (LLMs) in medicine are
primarily focused on question-answering involving domain knowledge and
descriptive reasoning. While such qualitative capabilities are vital to medical
diagnosis, in real-world scenarios, doctors frequently use clinical calculators
that follow quantitative equations and rule-based reasoning paradigms for
evidence-based decision support. To this end, we propose MedCalc-Bench, a
first-of-its-kind dataset focused on evaluating the medical calculation
capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000
manually reviewed instances from 55 different medical calculation tasks. Each
instance in MedCalc-Bench consists of a patient note, a question requesting to
compute a specific medical value, a ground truth answer, and a step-by-step
explanation showing how the answer is obtained. While our evaluation results
show the potential of LLMs in this area, none of them are effective enough for
clinical settings. Common issues include extracting the incorrect entities, not
using the correct equation or rules for a calculation task, or incorrectly
performing the arithmetic for the computation. We hope our study highlights the
quantitative knowledge and reasoning gaps in LLMs within medical settings,
encouraging future improvements of LLMs for various clinical calculation tasks.

ÊëòË¶ÅÔºöËàáË©ï‰º∞Ë®àÁÆóÂíåÂü∫ÊñºÈÇèËºØÁöÑÊé®ÁêÜ‰∏çÂêåÔºåÁõÆÂâçÁî®ÊñºË©ï‰º∞ÈÜ´Â≠∏‰∏≠Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂü∫Ê∫ñ‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ê∂âÂèäÈ†òÂüüÁü•Ë≠òÂíåÊèèËø∞ÊÄßÊé®ÁêÜÁöÑÂïèÁ≠î‰∏ä„ÄÇÈõñÁÑ∂Ê≠§È°ûÂÆöÊÄßËÉΩÂäõÂ∞çÊñºÈÜ´ÁôÇË®∫Êñ∑Ëá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÂú®ÂØ¶ÈöõÊÉÖÊ≥Å‰∏≠ÔºåÈÜ´ÁîüÁ∂ìÂ∏∏‰ΩøÁî®ÈÅµÂæ™ÂÆöÈáèÊñπÁ®ãÂºèÂíåÂü∫ÊñºË¶èÂâáÁöÑÊé®ÁêÜÁØÑ‰æãÁöÑËá®Â∫äË®àÁÆóÂô®‰æÜÈÄ≤Ë°åÂü∫ÊñºË≠âÊìöÁöÑÊ±∫Á≠ñÊîØÊåÅ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü MedCalc-BenchÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞àÊ≥®ÊñºË©ï‰º∞ LLM ÈÜ´Â≠∏Ë®àÁÆóËÉΩÂäõÁöÑÂêåÈ°ûÊï∏ÊìöÈõÜ„ÄÇMedCalc-Bench ÂåÖÂê´‰∏ÄÂÄãË©ï‰º∞ÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™ 55 ÂÄã‰∏çÂêåÈÜ´Â≠∏Ë®àÁÆó‰ªªÂãôÁöÑ 1000 Â§öÂÄãÊâãÂãïÂØ©Êü•ÁöÑÂØ¶‰æã„ÄÇMedCalc-Bench ‰∏≠ÁöÑÊØèÂÄãÂØ¶‰æãÈÉΩÂåÖÂê´‰∏ÄÂÄãÊÇ£ËÄÖÂÇôË®ª„ÄÅ‰∏ÄÂÄãË´ãÊ±ÇË®àÁÆóÁâπÂÆöÈÜ´Â≠∏ÂÄºÁöÑÊèêÂïè„ÄÅ‰∏ÄÂÄãÂü∫Êú¨‰∫ãÂØ¶Á≠îÊ°à‰ª•Âèä‰∏ÄÂÄãÈÄêÊ≠•Ë™™ÊòéÂ¶Ç‰ΩïÁç≤ÂæóÁ≠îÊ°àÁöÑË™™Êòé„ÄÇÈõñÁÑ∂ÊàëÂÄëÁöÑË©ï‰º∞ÁµêÊûúÈ°ØÁ§∫‰∫Ü LLM Âú®Ê≠§È†òÂüüÁöÑÊΩõÂäõÔºå‰ΩÜÊ≤íÊúâ‰ªª‰Ωï‰∏ÄÂÄã LLM Ë∂≥Â§†ÊúâÊïàÂú∞Áî®ÊñºËá®Â∫äÁí∞Â¢É„ÄÇÂ∏∏Ë¶ãÂïèÈ°åÂåÖÊã¨ÊèêÂèñ‰∏çÊ≠£Á¢∫ÁöÑÂØ¶È´î„ÄÅÊú™ÈáùÂ∞çË®àÁÆó‰ªªÂãô‰ΩøÁî®Ê≠£Á¢∫ÁöÑÊñπÁ®ãÂºèÊàñË¶èÂâáÔºåÊàñÈåØË™§Âú∞Âü∑Ë°åË®àÁÆóÁöÑÁÆóË°ìÈÅãÁÆó„ÄÇÊàëÂÄëÂ∏åÊúõÊàëÂÄëÁöÑÁ†îÁ©∂ËÉΩÁ™ÅÈ°Ø LLM Âú®ÈÜ´ÁôÇÁí∞Â¢É‰∏≠ÁöÑÂÆöÈáèÁü•Ë≠òÂíåÊé®ÁêÜÂ∑ÆË∑ùÔºå‰∏¶ÈºìÂãµÊú™‰æÜÊîπÈÄ≤ LLM ‰ª•Êáâ‰ªòÂêÑÁ®ÆËá®Â∫äË®àÁÆó‰ªªÂãô„ÄÇ

##### **Socially Interactive Agents for Robotic Neurorehabilitation Training: Conceptualization and Proof-of-concept Study**
2406.12035v1 by Rhythm Arora, Pooja Prajod, Matteo Lavit Nicora, Daniele Panzeri, Giovanni Tauro, Rocco Vertechy, Matteo Malosio, Elisabeth Andr√©, Patrick Gebhard

Individuals with diverse motor abilities often benefit from intensive and
specialized rehabilitation therapies aimed at enhancing their functional
recovery. Nevertheless, the challenge lies in the restricted availability of
neurorehabilitation professionals, hindering the effective delivery of the
necessary level of care. Robotic devices hold great potential in reducing the
dependence on medical personnel during therapy but, at the same time, they
generally lack the crucial human interaction and motivation that traditional
in-person sessions provide. To bridge this gap, we introduce an AI-based system
aimed at delivering personalized, out-of-hospital assistance during
neurorehabilitation training. This system includes a rehabilitation training
device, affective signal classification models, training exercises, and a
socially interactive agent as the user interface. With the assistance of a
professional, the envisioned system is designed to be tailored to accommodate
the unique rehabilitation requirements of an individual patient. Conceptually,
after a preliminary setup and instruction phase, the patient is equipped to
continue their rehabilitation regimen autonomously in the comfort of their
home, facilitated by a socially interactive agent functioning as a virtual
coaching assistant. Our approach involves the integration of an interactive
socially-aware virtual agent into a neurorehabilitation robotic framework, with
the primary objective of recreating the social aspects inherent to in-person
rehabilitation sessions. We also conducted a feasibility study to test the
framework with healthy patients. The results of our preliminary investigation
indicate that participants demonstrated a propensity to adapt to the system.
Notably, the presence of the interactive agent during the proposed exercises
did not act as a source of distraction; instead, it positively impacted users'
engagement.

ÊëòË¶ÅÔºöËÇ¢È´îËÉΩÂäõ‰∏çÂêåÁöÑÂÄã‰∫∫ÂæÄÂæÄÂèóÁõäÊñºÂØÜÈõÜ‰∏îÂ∞àÊ•≠ÁöÑÂæ©ÂÅ•ÁôÇÊ≥ïÔºåÁõÆÁöÑÊòØÂ¢ûÈÄ≤ÂÖ∂ÂäüËÉΩÊÄßÂæ©Âéü„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÊåëÊà∞Âú®ÊñºÁ•ûÁ∂ìÂæ©ÂÅ•Â∞àÊ•≠‰∫∫Âì°Êï∏ÈáèÊúâÈôêÔºåÈòªÁ§ô‰∫ÜÂøÖË¶ÅÁÖßË≠∑Á≠âÁ¥öÁöÑÊúâÊïàÊèê‰æõ„ÄÇÊ©üÂô®‰∫∫Ë£ùÁΩÆÂú®Èôç‰ΩéÊ≤ªÁôÇÊúüÈñìÂ∞çÈÜ´ÁôÇ‰∫∫Âì°ÁöÑ‰æùË≥¥ÊñπÈù¢ÂÖ∑ÊúâÊ•µÂ§ßÁöÑÊΩõÂäõÔºå‰ΩÜÂêåÊôÇÔºåÂÆÉÂÄëÈÄöÂ∏∏Áº∫‰πèÂÇ≥Áµ±Èù¢Â∞çÈù¢ÁôÇÁ®ãÊèê‰æõÁöÑÈóúÈçµ‰∫∫Èöõ‰∫íÂãïÂíåÂãïÊ©ü„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü‰∏ÄÂÄãÂü∫Êñº‰∫∫Â∑•Êô∫ÊÖßÁöÑÁ≥ªÁµ±ÔºåÊó®Âú®ÊñºÁ•ûÁ∂ìÂæ©ÂÅ•Ë®ìÁ∑¥ÊúüÈñìÊèê‰æõÂÄã‰∫∫Âåñ„ÄÅÈô¢Â§ñÁöÑÂçîÂä©„ÄÇÊ≠§Á≥ªÁµ±ÂåÖÂê´Âæ©ÂÅ•Ë®ìÁ∑¥Ë£ùÁΩÆ„ÄÅÊÉÖÊÑüË®äËôüÂàÜÈ°ûÊ®°Âûã„ÄÅË®ìÁ∑¥Á∑¥ÁøíÂíå‰∏ÄÂÄã‰ΩúÁÇ∫‰ΩøÁî®ËÄÖ‰ªãÈù¢ÁöÑÁ§æÊúÉ‰∫íÂãï‰ª£ÁêÜ‰∫∫„ÄÇÂú®Â∞àÊ•≠‰∫∫Âì°ÁöÑÂçîÂä©‰∏ãÔºåÈ†êÊÉ≥‰∏≠ÁöÑÁ≥ªÁµ±Êó®Âú®ÈáèË∫´ÊâìÈÄ†Ôºå‰ª•ÈÅ©ÊáâÂÄãÂà•ÁóÖÊÇ£Áç®ÁâπÁöÑÁ•ûÁ∂ìÂæ©ÂÅ•ÈúÄÊ±Ç„ÄÇÂú®Ê¶ÇÂøµ‰∏äÔºåÂú®ÂàùÊ≠•Ë®≠ÂÆöÂíåË™™ÊòéÈöéÊÆµÂæåÔºåÁóÖÊÇ£ÂÖ∑ÂÇôÂú®ËàíÈÅ©ÁöÑÂÆ∂‰∏≠Ëá™‰∏ªÊåÅÁ∫åÂÖ∂Âæ©ÂÅ•Ë®àÁï´ÁöÑÊ¢ù‰ª∂Ôºå‰∏¶Áî±‰∏ÄÂÄã‰ΩúÁÇ∫ËôõÊì¨ÊïôÁ∑¥Âä©ÁêÜÁöÑÁ§æÊúÉ‰∫íÂãï‰ª£ÁêÜ‰∫∫Êèê‰æõÂçîÂä©„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊ∂âÂèäÂ∞á‰∏ÄÂÄã‰∫íÂãï‰∏îÂÖ∑Á§æÊúÉÊÑèË≠òÁöÑËôõÊì¨‰ª£ÁêÜ‰∫∫Êï¥ÂêàÂà∞Á•ûÁ∂ìÂæ©ÂÅ•Ê©üÂô®‰∫∫Êû∂Êßã‰∏≠ÔºåÂÖ∂‰∏ªË¶ÅÁõÆÁöÑÊòØÈáçÂª∫Èù¢Â∞çÈù¢Âæ©ÂÅ•ÁôÇÁ®ã‰∏≠Âõ∫ÊúâÁöÑÁ§æÊúÉÈù¢Âêë„ÄÇÊàëÂÄë‰πüÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÂèØË°åÊÄßÁ†îÁ©∂Ôºå‰ª•ÂÅ•Â∫∑ÁóÖÊÇ£Ê∏¨Ë©¶Ê≠§Êû∂Êßã„ÄÇÊàëÂÄëÂàùÊ≠•Ë™øÊü•ÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÂèÉËàáËÄÖË°®ÁèæÂá∫ÈÅ©ÊáâÊ≠§Á≥ªÁµ±ÁöÑÂÇæÂêë„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂú®ÊâÄÂª∫Ë≠∞ÁöÑÁ∑¥ÁøíÊúüÈñìÔºå‰∫íÂãï‰ª£ÁêÜ‰∫∫ÁöÑÂ≠òÂú®‰∏¶Êú™ÊàêÁÇ∫ÂàÜÂøÉÁöÑ‰æÜÊ∫êÔºõÁõ∏ÂèçÂú∞ÔºåÂÆÉÂ∞ç‰ΩøÁî®ËÄÖÁöÑÂèÉËàáÁî¢Áîü‰∫ÜÊ≠£Èù¢ÁöÑÂΩ±Èüø„ÄÇ

##### **Improving Quality Control of Whole Slide Images by Explicit Artifact Augmentation**
2406.11538v1 by Artur Jurgas, Marek Wodzinski, Marina D'Amato, Jeroen van der Laak, Manfredo Atzori, Henning M√ºller

The problem of artifacts in whole slide image acquisition, prevalent in both
clinical workflows and research-oriented settings, necessitates human
intervention and re-scanning. Overcoming this challenge requires developing
quality control algorithms, that are hindered by the limited availability of
relevant annotated data in histopathology. The manual annotation of
ground-truth for artifact detection methods is expensive and time-consuming.
This work addresses the issue by proposing a method dedicated to augmenting
whole slide images with artifacts. The tool seamlessly generates and blends
artifacts from an external library to a given histopathology dataset. The
augmented datasets are then utilized to train artifact classification methods.
The evaluation shows their usefulness in classification of the artifacts, where
they show an improvement from 0.10 to 0.01 AUROC depending on the artifact
type. The framework, model, weights, and ground-truth annotations are freely
released to facilitate open science and reproducible research.

ÊëòË¶ÅÔºöÂú®Ëá®Â∫ä‰∏äÊàñÁ†îÁ©∂‰∏≠ÔºåÂÖ®ÂπªÁáàÁâáÂΩ±ÂÉèÊì∑ÂèñÊôÇÁî¢ÁîüÁöÑÂÅΩÂÉèÂïèÈ°åÔºåÈúÄË¶Å‰∫∫ÁÇ∫‰ªãÂÖ•ÂíåÈáçÊñ∞ÊéÉÊèè„ÄÇÂÖãÊúçÈÄôÂÄãÊåëÊà∞ÈúÄË¶ÅÈñãÁôºÂìÅË≥™ÊéßÁÆ°ÊºîÁÆóÊ≥ïÔºå‰ΩÜÁµÑÁπîÁóÖÁêÜÂ≠∏‰∏≠Áõ∏ÈóúË®ªËß£Ë≥áÊñôÊúâÈôêÔºåÈòªÁ§ô‰∫ÜÊºîÁÆóÊ≥ïÁöÑÁôºÂ±ï„ÄÇ‰∫∫Â∑•Ë®ªËß£ÂÅΩÂÉèÂÅµÊ∏¨ÊñπÊ≥ïÁöÑÁúüÂØ¶ÊÉÖÊ≥ÅÊó¢ÊòÇË≤¥ÂèàË≤ªÊôÇ„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊñπÊ≥ï‰æÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÈÄôÂÄãÊñπÊ≥ïÂ∞àÈñÄÁî®‰æÜÂ¢ûÂä†ÂÖ®ÂπªÁáàÁâáÂΩ±ÂÉè‰∏≠ÁöÑÂÅΩÂÉè„ÄÇÈÄôÂÄãÂ∑•ÂÖ∑ÂèØ‰ª•ÁÑ°Á∏´Âú∞ÂæûÂ§ñÈÉ®Ë≥áÊñôÂ∫´Áî¢Áîü‰∏¶Ê∑∑ÂêàÂÅΩÂÉèÂà∞Áµ¶ÂÆöÁöÑÁµÑÁπîÁóÖÁêÜÂ≠∏Ë≥áÊñôÈõÜ„ÄÇÁÑ∂Âæå‰ΩøÁî®Êì¥ÂÖÖÂæåÁöÑË≥áÊñôÈõÜ‰æÜË®ìÁ∑¥ÂÅΩÂÉèÂàÜÈ°ûÊñπÊ≥ï„ÄÇË©ï‰º∞È°ØÁ§∫Âá∫ÂÆÉÂÄëÂú®ÂÅΩÂÉèÂàÜÈ°û‰∏≠ÁöÑÊïàÁî®ÔºåÂú®‰∏çÂêåÁöÑÂÅΩÂÉèÈ°ûÂûã‰∏≠ÔºåÂÆÉÂÄëÁöÑ AUROC Âæû 0.10 ÈÄ≤Ê≠•Âà∞ 0.01„ÄÇÈÄôÂÄãÊû∂Êßã„ÄÅÊ®°Âûã„ÄÅÊ¨äÈáçÂíåÁúüÂØ¶Ë®ªËß£ÊòØÂÖçË≤ªÈáãÂá∫ÁöÑÔºå‰ª•Âà©ÊñºÈñãÊîæÁßëÂ≠∏ÂíåÂèØÈáçË£ΩÁöÑÁ†îÁ©∂„ÄÇ

##### **FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction**
2406.11928v1 by Muhao Xu, Zhenfeng Zhu, Youru Li, Shuai Zheng, Yawei Zhao, Kunlun He, Yao Zhao

Multimodal electronic health record (EHR) data can offer a holistic
assessment of a patient's health status, supporting various predictive
healthcare tasks. Recently, several studies have embraced the multitask
learning approach in the healthcare domain, exploiting the inherent
correlations among clinical tasks to predict multiple outcomes simultaneously.
However, existing methods necessitate samples to possess complete labels for
all tasks, which places heavy demands on the data and restricts the flexibility
of the model. Meanwhile, within a multitask framework with multimodal inputs,
how to comprehensively consider the information disparity among modalities and
among tasks still remains a challenging problem. To tackle these issues, a
unified healthcare prediction model, also named by \textbf{FlexCare}, is
proposed to flexibly accommodate incomplete multimodal inputs, promoting the
adaption to multiple healthcare tasks. The proposed model breaks the
conventional paradigm of parallel multitask prediction by decomposing it into a
series of asynchronous single-task prediction. Specifically, a task-agnostic
multimodal information extraction module is presented to capture decorrelated
representations of diverse intra- and inter-modality patterns. Taking full
account of the information disparities between different modalities and
different tasks, we present a task-guided hierarchical multimodal fusion module
that integrates the refined modality-level representations into an individual
patient-level representation. Experimental results on multiple tasks from
MIMIC-IV/MIMIC-CXR/MIMIC-NOTE datasets demonstrate the effectiveness of the
proposed method. Additionally, further analysis underscores the feasibility and
potential of employing such a multitask strategy in the healthcare domain. The
source code is available at https://github.com/mhxu1998/FlexCare.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÁîµÂ≠êÂÅ•Â∫∑ËÆ∞ÂΩïÔºàEHRÔºâÊï∞ÊçÆÂèØ‰ª•Êèê‰æõÊÇ£ËÄÖÂÅ•Â∫∑Áä∂ÂÜµÁöÑÂÖ®Èù¢ËØÑ‰º∞ÔºåÊîØÊåÅÂêÑÁßçÈ¢ÑÊµãÊÄßÂåªÁñó‰øùÂÅ•‰ªªÂä°„ÄÇÊúÄËøëÔºå‰∏Ä‰∫õÁ†îÁ©∂ÈááÁî®‰∫ÜÂåªÁñó‰øùÂÅ•È¢ÜÂüüÁöÑÂ§öÂàÜ‰ªªÂä°Â≠¶‰π†ÊñπÊ≥ïÔºåÂà©Áî®‰∏¥Â∫ä‰ªªÂä°‰πãÈó¥Âõ∫ÊúâÁöÑÁõ∏ÂÖ≥ÊÄßÊù•ÂêåÊó∂È¢ÑÊµãÂ§ö‰∏™ÁªìÊûú„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÊñπÊ≥ïÈúÄË¶ÅÊ†∑Êú¨‰∏∫ÊâÄÊúâ‰ªªÂä°ÈÉΩÊã•ÊúâÂÆåÊï¥ÁöÑÊ†áÁ≠æÔºåËøôÂØπÊï∞ÊçÆÊèêÂá∫‰∫ÜÂæàÈ´òÁöÑË¶ÅÊ±ÇÔºåÂπ∂ÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÁÅµÊ¥ªÊÄß„ÄÇÂêåÊó∂ÔºåÂú®ÂÖ∑ÊúâÂ§öÊ®°ÊÄÅËæìÂÖ•ÁöÑÂ§ö‰ªªÂä°Ê°ÜÊû∂‰∏≠ÔºåÂ¶Ç‰ΩïÂÖ®Èù¢ËÄÉËôëÊ®°ÊÄÅ‰πãÈó¥Âíå‰ªªÂä°‰πãÈó¥ÁöÑ‰ø°ÊÅØÂ∑ÆÂºÇ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊèêÂá∫‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂåªÁñó‰øùÂÅ•È¢ÑÊµãÊ®°ÂûãÔºå‰πüÁß∞‰∏∫\textbf{FlexCare}Ôºå‰ª•ÁÅµÊ¥ªÂú∞ÈÄÇÂ∫î‰∏çÂÆåÊï¥ÁöÑÂ§öÊ®°ÊÄÅËæìÂÖ•Ôºå‰øÉËøõÂØπÂ§ö‰∏™ÂåªÁñó‰øùÂÅ•‰ªªÂä°ÁöÑÈÄÇÂ∫î„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÊâìÁ†¥‰∫ÜÂπ∂Ë°åÂ§ö‰ªªÂä°È¢ÑÊµãÁöÑ‰º†ÁªüËåÉÂºèÔºåÂ∞ÜÂÖ∂ÂàÜËß£‰∏∫‰∏ÄÁ≥ªÂàóÂºÇÊ≠•Âçï‰ªªÂä°È¢ÑÊµã„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏é‰ªªÂä°Êó†ÂÖ≥ÁöÑÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÊèêÂèñÊ®°ÂùóÔºå‰ª•ÊçïËé∑‰∏çÂêåÊ®°ÊÄÅÂÜÖÂíåÊ®°ÊÄÅÈó¥Ê®°ÂºèÁöÑÂéªÁõ∏ÂÖ≥Ë°®Á§∫„ÄÇÂÖÖÂàÜËÄÉËôë‰∏çÂêåÊ®°ÊÄÅÂíå‰∏çÂêå‰ªªÂä°‰πãÈó¥‰ø°ÊÅØÂ∑ÆÂºÇÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰ªªÂä°ÊåáÂØºÁöÑÂàÜÂ±ÇÂ§öÊ®°ÊÄÅËûçÂêàÊ®°ÂùóÔºåÂ∞ÜÁ≤æÁÇºÁöÑÊ®°ÊÄÅÁ∫ßË°®Á§∫ÈõÜÊàêÂà∞‰∏Ä‰∏™ÂçïÁã¨ÁöÑÊÇ£ËÄÖÁ∫ßË°®Á§∫‰∏≠„ÄÇMIMIC-IV/MIMIC-CXR/MIMIC-NOTE Êï∞ÊçÆÈõÜ‰∏≠Â§ö‰∏™‰ªªÂä°ÁöÑÂÆûÈ™åÁªìÊûúËØÅÊòé‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåËøõ‰∏ÄÊ≠•ÁöÑÂàÜÊûêÂº∫Ë∞É‰∫ÜÂú®ÂåªÁñó‰øùÂÅ•È¢ÜÂüüÈááÁî®ËøôÁßçÂ§ö‰ªªÂä°Á≠ñÁï•ÁöÑÂèØË°åÊÄßÂíåÊΩúÂäõ„ÄÇÊ∫ê‰ª£Á†ÅÂèØÂú® https://github.com/mhxu1998/FlexCare Ëé∑Âæó„ÄÇ

##### **Formally Certified Approximate Model Counting**
2406.11414v2 by Yong Kiam Tan, Jiong Yang, Mate Soos, Magnus O. Myreen, Kuldeep S. Meel

Approximate model counting is the task of approximating the number of
solutions to an input Boolean formula. The state-of-the-art approximate model
counter for formulas in conjunctive normal form (CNF), ApproxMC, provides a
scalable means of obtaining model counts with probably approximately correct
(PAC)-style guarantees. Nevertheless, the validity of ApproxMC's approximation
relies on a careful theoretical analysis of its randomized algorithm and the
correctness of its highly optimized implementation, especially the latter's
stateful interactions with an incremental CNF satisfiability solver capable of
natively handling parity (XOR) constraints.
  We present the first certification framework for approximate model counting
with formally verified guarantees on the quality of its output approximation.
Our approach combines: (i) a static, once-off, formal proof of the algorithm's
PAC guarantee in the Isabelle/HOL proof assistant; and (ii) dynamic, per-run,
verification of ApproxMC's calls to an external CNF-XOR solver using proof
certificates. We detail our general approach to establish a rigorous connection
between these two parts of the verification, including our blueprint for
turning the formalized, randomized algorithm into a verified proof checker, and
our design of proof certificates for both ApproxMC and its internal CNF-XOR
solving steps. Experimentally, we show that certificate generation adds little
overhead to an approximate counter implementation, and that our certificate
checker is able to fully certify $84.7\%$ of instances with generated
certificates when given the same time and memory limits as the counter.

ÊëòË¶ÅÔºöËøë‰ººÊ®°ÂûãËÆ°Êï∞ÊòØËøë‰ººËæìÂÖ•Â∏ÉÊûóÂÖ¨ÂºèÁöÑËß£ÁöÑÊï∞ÈáèÁöÑ‰ªªÂä°„ÄÇÈíàÂØπÂêàÂèñËåÉÂºè (CNF) ‰∏≠ÁöÑÂÖ¨ÂºèÁöÑÊúÄÊñ∞Ëøë‰ººÊ®°ÂûãËÆ°Êï∞Âô® ApproxMCÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑÊñπÊ≥ïÊù•Ëé∑ÂèñÂÖ∑ÊúâËøë‰ººÊ≠£Á°Æ (PAC) È£éÊ†º‰øùËØÅÁöÑÊ®°ÂûãËÆ°Êï∞„ÄÇÁÑ∂ËÄåÔºåApproxMC ÁöÑËøë‰ººÁöÑÊúâÊïàÊÄß‰æùËµñ‰∫éÂØπÂÖ∂ÈöèÊú∫ÁÆóÊ≥ïÁöÑ‰ªîÁªÜÁêÜËÆ∫ÂàÜÊûê‰ª•ÂèäÂÖ∂È´òÂ∫¶‰ºòÂåñÁöÑÂÆûÁé∞ÁöÑÊ≠£Á°ÆÊÄßÔºåÁâπÂà´ÊòØÂêéËÄÖ‰∏éËÉΩÂ§üÂéüÁîüÂ§ÑÁêÜÂ•áÂÅ∂Ê†°È™å (XOR) Á∫¶ÊùüÁöÑÂ¢ûÈáè CNF ÂèØÊª°Ë∂≥ÊÄßÊ±ÇËß£Âô®ÁöÑÊúâÁä∂ÊÄÅ‰∫§‰∫í„ÄÇ
Êàë‰ª¨ÊèêÂá∫‰∫ÜÁ¨¨‰∏Ä‰∏™Ëøë‰ººÊ®°ÂûãËÆ°Êï∞ËÆ§ËØÅÊ°ÜÊû∂ÔºåÂØπËæìÂá∫Ëøë‰ººÁöÑË¥®ÈáèÊèê‰æõÂΩ¢ÂºèÈ™åËØÅÁöÑ‰øùËØÅ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÁªìÂêà‰∫ÜÔºö(i) ÁÆóÊ≥ïÁöÑ PAC ‰øùËØÅÂú® Isabelle/HOL ËØÅÊòéÂä©Êâã‰∏≠ÁöÑÈùôÊÄÅ„ÄÅ‰∏ÄÊ¨°ÊÄß„ÄÅÂΩ¢ÂºèËØÅÊòéÔºõ‰ª•Âèä (ii) ‰ΩøÁî®ËØÅÊòéËØÅ‰π¶ÂØπ ApproxMC ÂØπÂ§ñÈÉ® CNF-XOR Ê±ÇËß£Âô®ÁöÑË∞ÉÁî®ËøõË°åÂä®ÊÄÅ„ÄÅÊØèÊ¨°ËøêË°åÈ™åËØÅ„ÄÇÊàë‰ª¨ËØ¶ÁªÜ‰ªãÁªç‰∫ÜÊàë‰ª¨Âª∫Á´ãÈ™åËØÅËøô‰∏§ÈÉ®ÂàÜ‰πãÈó¥‰∏•Ê†ºËÅîÁ≥ªÁöÑ‰∏ÄËà¨ÊñπÊ≥ïÔºåÂåÖÊã¨Êàë‰ª¨Áî®‰∫éÂ∞ÜÂΩ¢ÂºèÂåñÁöÑÈöèÊú∫ÁÆóÊ≥ïËΩ¨Êç¢‰∏∫ÁªèËøáÈ™åËØÅÁöÑËØÅÊòéÊ£ÄÊü•Âô®ÁöÑËìùÂõæÔºå‰ª•ÂèäÊàë‰ª¨‰∏∫ ApproxMC ÂèäÂÖ∂ÂÜÖÈÉ® CNF-XOR Ê±ÇËß£Ê≠•È™§ËÆæËÆ°ÁöÑËØÅÊòéËØÅ‰π¶„ÄÇÂú®ÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨Ë°®ÊòéËØÅ‰π¶ÁîüÊàêÂá†‰πé‰∏ç‰ºöÁªôËøë‰ººËÆ°Êï∞Âô®ÂÆûÁé∞Â¢ûÂä†ÂºÄÈîÄÔºåÂπ∂‰∏îÊàë‰ª¨ÁöÑËØÅ‰π¶Ê£ÄÊü•Âô®ËÉΩÂ§üÂú®‰∏éËÆ°Êï∞Âô®Áõ∏ÂêåÁöÑÊó∂Èó¥ÂíåÂÜÖÂ≠òÈôêÂà∂‰∏ãÂÆåÂÖ®È™åËØÅ 84.7% ÁöÑÂÖ∑ÊúâÁîüÊàêËØÅ‰π¶ÁöÑÂÆû‰æã„ÄÇ

##### **Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection**
2406.11260v1 by Sungwon Park, Sungwon Han, Meeyoung Cha

The spread of fake news negatively impacts individuals and is regarded as a
significant social challenge that needs to be addressed. A number of
algorithmic and insightful features have been identified for detecting fake
news. However, with the recent LLMs and their advanced generation capabilities,
many of the detectable features (e.g., style-conversion attacks) can be
altered, making it more challenging to distinguish from real news. This study
proposes adversarial style augmentation, AdStyle, to train a fake news detector
that remains robust against various style-conversion attacks. Our model's key
mechanism is the careful use of LLMs to automatically generate a diverse yet
coherent range of style-conversion attack prompts. This improves the generation
of prompts that are particularly difficult for the detector to handle.
Experiments show that our augmentation strategy improves robustness and
detection performance when tested on fake news benchmark datasets.

ÊëòË¶ÅÔºöÂÅáÊñ∞ËÅûÁöÑÊï£Â∏ÉÂ∞çÂÄã‰∫∫ÈÄ†ÊàêË≤†Èù¢ÂΩ±ÈüøÔºå‰∏¶Ë¢´Ë¶ñÁÇ∫ÈúÄË¶ÅËß£Ê±∫ÁöÑÈáçÂ§ßÁ§æÊúÉÊåëÊà∞„ÄÇÂ∑≤Á∂ìÊâæÂá∫Ë®±Â§öÊºîÁÆóÊ≥ïÂíåÊúâË¶ãÂú∞ÁöÑÂäüËÉΩ‰æÜÂÅµÊ∏¨ÂÅáÊñ∞ËÅû„ÄÇÁÑ∂ËÄåÔºåÈö®ËëóËøëÊúüÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂèäÂÖ∂ÂÖàÈÄ≤ÁöÑÁî¢ÁîüËÉΩÂäõÔºåË®±Â§öÂèØÂÅµÊ∏¨ÁöÑÂäüËÉΩÔºà‰æãÂ¶ÇÈ¢®Ê†ºËΩâÊèõÊîªÊìäÔºâÈÉΩÂèØËÉΩË¢´ÊîπËÆäÔºå‰ΩøÂæóËàáÁúüÂØ¶Êñ∞ËÅûÁöÑÂçÄÂà•Êõ¥ÂÖ∑ÊåëÊà∞ÊÄß„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫Â∞çÊäóÂºèÈ¢®Ê†ºÊì¥ÂÖÖÔºåAdStyleÔºå‰æÜË®ìÁ∑¥‰∏ÄÂÄãÂ∞çÂêÑÁ®ÆÈ¢®Ê†ºËΩâÊèõÊîªÊìä‰øùÊåÅÁ©©ÂÅ•ÁöÑÂÅáÊñ∞ËÅûÂÅµÊ∏¨Âô®„ÄÇÊàëÂÄëÊ®°ÂûãÁöÑÈóúÈçµÊ©üÂà∂ÊòØÂ∞èÂøÉÂú∞‰ΩøÁî® LLM Ëá™ÂãïÁî¢ÁîüÂ§öÊ®£‰∏îÈÄ£Ë≤´ÁöÑÈ¢®Ê†ºËΩâÊèõÊîªÊìäÊèêÁ§∫ÁØÑÂúç„ÄÇÈÄôÊîπÂñÑ‰∫ÜÊèêÁ§∫ÁöÑÁî¢ÁîüÔºåÁâπÂà•ÊòØÂ∞çÊñºÂÅµÊ∏¨Âô®Èõ£‰ª•ËôïÁêÜÁöÑÊèêÁ§∫„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåÁï∂Âú®ÂÅáÊñ∞ËÅûÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÊôÇÔºåÊàëÂÄëÁöÑÊì¥ÂÖÖÁ≠ñÁï•ÊîπÂñÑ‰∫ÜÁ©©ÂÅ•ÊÄßÂíåÂÅµÊ∏¨ÊïàËÉΩ„ÄÇ

##### **Scorecards for Synthetic Medical Data Evaluation and Reporting**
2406.11143v1 by Ghada Zamzmi, Adarsh Subbaswamy, Elena Sizikova, Edward Margerrison, Jana Delfino, Aldo Badano

The growing utilization of synthetic medical data (SMD) in training and
testing AI-driven tools in healthcare necessitates a systematic framework for
assessing SMD quality. The current lack of a standardized methodology to
evaluate SMD, particularly in terms of its applicability in various medical
scenarios, is a significant hindrance to its broader acceptance and utilization
in healthcare applications. Here, we outline an evaluation framework designed
to meet the unique requirements of medical applications, and introduce the
concept of SMD scorecards, which can serve as comprehensive reports that
accompany artificially generated datasets. This can help standardize evaluation
and enable SMD developers to assess and further enhance the quality of SMDs by
identifying areas in need of attention and ensuring that the synthetic data
more accurately approximate patient data.

ÊëòË¶ÅÔºöÈö®ËëóÂêàÊàêÈÜ´ÁôÇË≥áÊñô (SMD) Âú®Ë®ìÁ∑¥ÂíåÊ∏¨Ë©¶ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÈ©ÖÂãïÂ∑•ÂÖ∑ÁöÑÂà©Áî®ÁéáÊó•ÁõäÊèêÈ´òÔºåÈúÄË¶Å‰∏ÄÂÄãÁ≥ªÁµ±ÊÄßÁöÑÊû∂Êßã‰æÜË©ï‰º∞ SMD ÁöÑÂìÅË≥™„ÄÇÁõÆÂâçÁº∫‰πèÊ®ôÊ∫ñÂåñÁöÑË©ï‰º∞ SMD ÁöÑÊñπÊ≥ïÔºåÁâπÂà•ÊòØÂú®ÂÖ∂ÊñºÂêÑÁ®ÆÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠ÁöÑÈÅ©Áî®ÊÄßÊñπÈù¢ÔºåÈÄôÂö¥ÈáçÈòªÁ§ô‰∫ÜÂÖ∂Âú®ÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰∏≠ÁöÑÊõ¥Âª£Ê≥õÊé•ÂèóÂíåÂà©Áî®„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊ¶ÇËø∞‰∫Ü‰∏ÄÂÄãË©ï‰º∞Êû∂ÊßãÔºåÊó®Âú®ÊªøË∂≥ÈÜ´ÁôÇÊáâÁî®ÁöÑÁç®ÁâπÈúÄÊ±ÇÔºå‰∏¶ÂºïÂÖ•‰∫Ü SMD Ë®òÂàÜÂç°ÁöÑÊ¶ÇÂøµÔºåÂÆÉÂèØ‰ª•‰ΩúÁÇ∫‰∫∫Â∑•ÁîüÊàêË≥áÊñôÈõÜÁöÑÁ∂úÂêàÂ†±Âëä„ÄÇÈÄôÊúâÂä©ÊñºÊ®ôÊ∫ñÂåñË©ï‰º∞Ôºå‰∏¶‰Ωø SMD ÈñãÁôº‰∫∫Âì°ËÉΩÂ§†Ë©ï‰º∞ÂíåÈÄ≤‰∏ÄÊ≠•ÊèêÈ´ò SMD ÁöÑÂìÅË≥™ÔºåÊñπÊ≥ïÊòØÊâæÂá∫ÈúÄË¶ÅÈóúÊ≥®ÁöÑÈ†òÂüüÔºå‰∏¶Á¢∫‰øùÂêàÊàêË≥áÊñôÊõ¥Ê∫ñÁ¢∫Âú∞Ëøë‰ººÊñºÊÇ£ËÄÖË≥áÊñô„ÄÇ

##### **Diffusion Models in Low-Level Vision: A Survey**
2406.11138v1 by Chunming He, Yuqi Shen, Chengyu Fang, Fengyang Xiao, Longxiang Tang, Yulun Zhang, Wangmeng Zuo, Zhenhua Guo, Xiu Li

Deep generative models have garnered significant attention in low-level
vision tasks due to their generative capabilities. Among them, diffusion
model-based solutions, characterized by a forward diffusion process and a
reverse denoising process, have emerged as widely acclaimed for their ability
to produce samples of superior quality and diversity. This ensures the
generation of visually compelling results with intricate texture information.
Despite their remarkable success, a noticeable gap exists in a comprehensive
survey that amalgamates these pioneering diffusion model-based works and
organizes the corresponding threads. This paper proposes the comprehensive
review of diffusion model-based techniques. We present three generic diffusion
modeling frameworks and explore their correlations with other deep generative
models, establishing the theoretical foundation. Following this, we introduce a
multi-perspective categorization of diffusion models, considering both the
underlying framework and the target task. Additionally, we summarize extended
diffusion models applied in other tasks, including medical, remote sensing, and
video scenarios. Moreover, we provide an overview of commonly used benchmarks
and evaluation metrics. We conduct a thorough evaluation, encompassing both
performance and efficiency, of diffusion model-based techniques in three
prominent tasks. Finally, we elucidate the limitations of current diffusion
models and propose seven intriguing directions for future research. This
comprehensive examination aims to facilitate a profound understanding of the
landscape surrounding denoising diffusion models in the context of low-level
vision tasks. A curated list of diffusion model-based techniques in over 20
low-level vision tasks can be found at
https://github.com/ChunmingHe/awesome-diffusion-models-in-low-level-vision.

ÊëòË¶ÅÔºöÊ∑±Â∫¶ÁîüÊàêÊ®°ÂûãÂú®‰ΩéÂ±ÇÊ¨°ËßÜËßâ‰ªªÂä°‰∏≠Ëé∑Âæó‰∫ÜÊòæËëóÁöÑÂÖ≥Ê≥®ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÂÖ∑ÊúâÁîüÊàêËÉΩÂäõ„ÄÇÂÖ∂‰∏≠Ôºå‰ª•Ê≠£ÂêëÊâ©Êï£ËøáÁ®ãÂíåÂèçÂêëÂéªÂô™ËøáÁ®ã‰∏∫ÁâπÂæÅÁöÑÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂõ†ÂÖ∂ÁîüÊàêÊõ¥È´òË¥®ÈáèÂíåÂ§öÊ†∑ÊÄßÊ†∑Êú¨ÁöÑËÉΩÂäõËÄåÂ§áÂèóËµûË™â„ÄÇËøôÁ°Æ‰øù‰∫ÜÁîüÊàêËßÜËßâ‰∏äÂºï‰∫∫Ê≥®ÁõÆÁöÑÁªìÊûúÔºåÂπ∂ÂÖ∑ÊúâÂ§çÊùÇÁ∫πÁêÜ‰ø°ÊÅØ„ÄÇÂ∞ΩÁÆ°ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊàêÂäüÔºå‰ΩÜÂú®Â∞ÜËøô‰∫õÂºÄÂàõÊÄßÁöÑÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÂ∑•‰ΩúÊ±áÈõÜËµ∑Êù•Âπ∂ÁªÑÁªáÁõ∏Â∫îÁöÑÁ∫øÁ®ãÁöÑÁªºÂêàË∞ÉÊü•‰∏≠Ôºå‰ªçÁÑ∂Â≠òÂú®ÊòéÊòæÁöÑÂ∑ÆË∑ù„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊäÄÊúØÁöÑÂÖ®Èù¢ÁªºËø∞„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏â‰∏™ÈÄöÁî®ÁöÑÊâ©Êï£Âª∫Ê®°Ê°ÜÊû∂ÔºåÂπ∂Êé¢ËÆ®‰∫ÜÂÆÉ‰ª¨‰∏éÂÖ∂‰ªñÊ∑±Â∫¶ÁîüÊàêÊ®°ÂûãÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÂª∫Á´ã‰∫ÜÁêÜËÆ∫Âü∫Á°Ä„ÄÇÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåÊàë‰ª¨‰ªãÁªç‰∫ÜÊâ©Êï£Ê®°ÂûãÁöÑÂ§öËßÜËßíÂàÜÁ±ªÔºåÂêåÊó∂ËÄÉËôë‰∫ÜÂ∫ïÂ±ÇÊ°ÜÊû∂ÂíåÁõÆÊ†á‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊÄªÁªì‰∫ÜÂ∫îÁî®‰∫éÂÖ∂‰ªñ‰ªªÂä°ÁöÑÊâ©Â±ïÊâ©Êï£Ê®°ÂûãÔºåÂåÖÊã¨ÂåªÂ≠¶„ÄÅÈÅ•ÊÑüÂíåËßÜÈ¢ëÂú∫ÊôØ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Ê¶ÇËø∞‰∫ÜÂ∏∏Áî®ÁöÑÂü∫ÂáÜÂíåËØÑ‰º∞ÊåáÊ†á„ÄÇÊàë‰ª¨ÂØπÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊäÄÊúØÂú®‰∏â‰∏™Á™ÅÂá∫ÁöÑ‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩÂíåÊïàÁéáËøõË°å‰∫ÜÂΩªÂ∫ïÁöÑËØÑ‰º∞„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÈòêÊòé‰∫ÜÂΩìÂâçÊâ©Êï£Ê®°ÂûãÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏É‰∏™Êú™Êù•Á†îÁ©∂ÁöÑÊúâË∂£ÊñπÂêë„ÄÇËøôÊ¨°ÂÖ®Èù¢Ê£ÄÊü•Êó®Âú®‰øÉËøõÂØπ‰ΩéÂ±ÇÊ¨°ËßÜËßâ‰ªªÂä°ËÉåÊôØ‰∏ãÂéªÂô™Êâ©Êï£Ê®°ÂûãÂë®Âõ¥ÁéØÂ¢ÉÁöÑÊ∑±ÂÖ•ÁêÜËß£„ÄÇÂèØ‰ª•Âú® https://github.com/ChunmingHe/awesome-diffusion-models-in-low-level-vision ÊâæÂà∞Ë∂ÖËøá 20 ‰∏™‰ΩéÂ±ÇÊ¨°ËßÜËßâ‰ªªÂä°‰∏≠Âü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÊäÄÊúØÁöÑÁ≤æÈÄâÂàóË°®„ÄÇ

##### **Towards Understanding Emotions for Engaged Mental Health Conversations**
2406.11135v1 by Kellie Yu Hui Sim, Kohleen Tijing Fortuno, Kenny Tsu Wei Choo

Providing timely support and intervention is crucial in mental health
settings. As the need to engage youth comfortable with texting increases,
mental health providers are exploring and adopting text-based media such as
chatbots, community-based forums, online therapies with licensed professionals,
and helplines operated by trained responders. To support these text-based media
for mental health--particularly for crisis care--we are developing a system to
perform passive emotion-sensing using a combination of keystroke dynamics and
sentiment analysis. Our early studies of this system posit that the analysis of
short text messages and keyboard typing patterns can provide emotion
information that may be used to support both clients and responders. We use our
preliminary findings to discuss the way forward for applying AI to support
mental health providers in providing better care.

ÊëòË¶ÅÔºöÂú®ÂøÉÁêÜÂÅ•Â∫∑Áí∞Â¢É‰∏≠Êèê‰æõÂèäÊôÇÁöÑÊîØÊè¥Âíå‰ªãÂÖ•Ëá≥ÈóúÈáçË¶Å„ÄÇÈö®ËëóËàáÈùíÂ∞ëÂπ¥‰∫íÂãïÊôÇÔºå‰ΩøÁî®ÊñáÂ≠óË®äÊÅØÁöÑÈúÄÊ±ÇÂ¢ûÂä†ÔºåÂøÉÁêÜÂÅ•Â∫∑ÊúçÂãôÊèê‰æõËÄÖÊ≠£Âú®Êé¢Á¥¢ÂíåÊé°Áî®Âü∫ÊñºÊñáÂ≠óË®äÊÅØÁöÑÂ™íÈ´îÔºå‰æãÂ¶ÇËÅäÂ§©Ê©üÂô®‰∫∫„ÄÅÁ§æÁæ§Ë´ñÂ£á„ÄÅÁî±ÊåÅÁÖßÂ∞àÊ•≠‰∫∫Âì°Êèê‰æõÁöÑÁ∑ö‰∏äÁôÇÊ≥ïÔºå‰ª•ÂèäÁî±ÂèóÈÅéË®ìÁ∑¥ÁöÑÂõûÊáâËÄÖÁáüÈÅãÁöÑÊ±ÇÂä©Â∞àÁ∑ö„ÄÇÁÇ∫‰∫ÜÊîØÊè¥ÈÄô‰∫õÁî®ÊñºÂøÉÁêÜÂÅ•Â∫∑ÁöÑÂü∫ÊñºÊñáÂ≠óË®äÊÅØÁöÑÂ™íÈ´î‚Äî‚ÄîÁâπÂà•ÊòØÂç±Ê©üÁÖßË≠∑‚Äî‚ÄîÊàëÂÄëÊ≠£Âú®ÈñãÁôº‰∏ÄÂÄãÁ≥ªÁµ±Ôºå‰ΩøÁî®ÊåâÈçµÂãïÂäõÂ≠∏ÂíåÊÉÖÁ∑íÂàÜÊûêÁöÑÁµÑÂêà‰æÜÂü∑Ë°åË¢´ÂãïÊÉÖÁ∑íÊÑüÊ∏¨„ÄÇÊàëÂÄëÂ∞çÈÄôÂÄãÁ≥ªÁµ±ÁöÑÊó©ÊúüÁ†îÁ©∂ÂÅáË®≠ÔºåÂ∞çÁ∞°Áü≠ÊñáÂ≠óË®äÊÅØÂíåÈçµÁõ§Ëº∏ÂÖ•Ê®°ÂºèÁöÑÂàÜÊûêÂèØ‰ª•Êèê‰æõÊÉÖÁ∑íË≥áË®äÔºåÂèØÁî®ÊñºÊîØÊè¥ÂÄãÊ°àÂíåÂõûÊáâËÄÖ„ÄÇÊàëÂÄë‰ΩøÁî®ÊàëÂÄëÁöÑÂàùÊ≠•ÁôºÁèæ‰æÜË®éË´ñÂ∞á AI ÊáâÁî®ÊñºÊîØÊè¥ÂøÉÁêÜÂÅ•Â∫∑ÊúçÂãôÊèê‰æõËÄÖÊèê‰æõÊõ¥Â•ΩÁÖßË≠∑ÁöÑÊú™‰æÜÊñπÂêë„ÄÇ

##### **Boosting Medical Image Classification with Segmentation Foundation Model**
2406.11026v1 by Pengfei Gu, Zihan Zhao, Hongxiao Wang, Yaopeng Peng, Yizhe Zhang, Nishchal Sapkota, Chaoli Wang, Danny Z. Chen

The Segment Anything Model (SAM) exhibits impressive capabilities in
zero-shot segmentation for natural images. Recently, SAM has gained a great
deal of attention for its applications in medical image segmentation. However,
to our best knowledge, no studies have shown how to harness the power of SAM
for medical image classification. To fill this gap and make SAM a true
``foundation model'' for medical image analysis, it is highly desirable to
customize SAM specifically for medical image classification. In this paper, we
introduce SAMAug-C, an innovative augmentation method based on SAM for
augmenting classification datasets by generating variants of the original
images. The augmented datasets can be used to train a deep learning
classification model, thereby boosting the classification performance.
Furthermore, we propose a novel framework that simultaneously processes raw and
SAMAug-C augmented image input, capitalizing on the complementary information
that is offered by both. Experiments on three public datasets validate the
effectiveness of our new approach.

ÊëòË¶ÅÔºö‰ªª‰ΩïÂçÄÊÆµÊ®°Âûã (SAM) Âú®Ëá™ÁÑ∂ÂΩ±ÂÉèÁöÑÈõ∂ÁôºÂ∞ÑÂçÄÊÆµ‰∏≠Â±ïÁèæ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõ„ÄÇÊúÄËøëÔºåSAM Âõ†ÂÖ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂçÄÊÆµ‰∏≠ÁöÑÊáâÁî®ËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåÊìöÊàëÂÄëÊâÄÁü•ÔºåÊ≤íÊúâÁ†îÁ©∂È°ØÁ§∫Â¶Ç‰ΩïÂà©Áî® SAM ÁöÑÂäõÈáèÈÄ≤Ë°åÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩÔºå‰∏¶ËÆì SAM ÊàêÁÇ∫ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÁöÑÁúüÊ≠£„ÄåÂü∫Á§éÊ®°Âûã„ÄçÔºåÈùûÂ∏∏Â∏åÊúõÈáùÂ∞çÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûÂÆ¢Ë£ΩÂåñ SAM„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π SAMAug-CÔºå‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊì¥ÂÖÖÊñπÊ≥ïÔºåÂü∫Êñº SAMÔºåÈÄèÈÅéÁî¢ÁîüÂéüÂßãÂΩ±ÂÉèÁöÑËÆäÈ´î‰æÜÊì¥ÂÖÖÂàÜÈ°ûË≥áÊñôÈõÜ„ÄÇÊì¥ÂÖÖÁöÑË≥áÊñôÈõÜÂèØÁî®ÊñºË®ìÁ∑¥Ê∑±Â∫¶Â≠∏ÁøíÂàÜÈ°ûÊ®°ÂûãÔºåÂæûËÄåÊèêÂçáÂàÜÈ°ûÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÊû∂ÊßãÔºåÂêåÊôÇËôïÁêÜÂéüÂßãÂíå SAMAug-C Êì¥ÂÖÖÂΩ±ÂÉèËº∏ÂÖ•ÔºåÂà©Áî®ÂÖ©ËÄÖÊèê‰æõÁöÑ‰∫íË£úË≥áË®ä„ÄÇÂú®‰∏âÂÄãÂÖ¨ÈñãË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÊñ∞ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ


### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-27**|**Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment**|Hao Fei et.al.|[2406.19255v1](http://arxiv.org/abs/2406.19255v1)|null|
|**2024-06-27**|**TrustUQA: A Trustful Framework for Unified Structured Data Question Answering**|Wen Zhang et.al.|[2406.18916v1](http://arxiv.org/abs/2406.18916v1)|null|
|**2024-06-26**|**Fast Optimizer Benchmark**|Simon Blauth et.al.|[2406.18701v1](http://arxiv.org/abs/2406.18701v1)|[link](https://github.com/automl/fob)|
|**2024-06-26**|**Cascading Large Language Models for Salient Event Graph Generation**|Xingwei Tan et.al.|[2406.18449v1](http://arxiv.org/abs/2406.18449v1)|null|
|**2024-06-26**|**Sanskrit Knowledge-based Systems: Annotation and Computational Tools**|Hrishikesh Terdalkar et.al.|[2406.18276v1](http://arxiv.org/abs/2406.18276v1)|null|
|**2024-06-26**|**Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints**|Ran Song et.al.|[2406.18085v1](http://arxiv.org/abs/2406.18085v1)|[link](https://github.com/Maxpa1n/gcplm-kgc)|
|**2024-06-26**|**AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning**|Yifan Yang et.al.|[2406.18060v1](http://arxiv.org/abs/2406.18060v1)|[link](https://github.com/yifanycc/adazeta)|
|**2024-06-25**|**DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph**|Zhehao Zhang et.al.|[2406.17271v1](http://arxiv.org/abs/2406.17271v1)|[link](https://github.com/salt-nlp/darg)|
|**2024-06-25**|**CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph**|Tong Zhou et.al.|[2406.17231v1](http://arxiv.org/abs/2406.17231v1)|[link](https://github.com/tongzhou21/CogMG)|
|**2024-06-24**|**Link Prediction with Untrained Message Passing Layers**|Lisi Qarkaxhija et.al.|[2406.16687v1](http://arxiv.org/abs/2406.16687v1)|null|
|**2024-06-24**|**CLEAR: Can Language Models Really Understand Causal Graphs?**|Sirui Chen et.al.|[2406.16605v1](http://arxiv.org/abs/2406.16605v1)|[link](https://github.com/opencausalab/clear)|
|**2024-06-24**|**KEHRL: Learning Knowledge-Enhanced Language Representations with Hierarchical Reinforcement Learning**|Dongyang Li et.al.|[2406.16374v1](http://arxiv.org/abs/2406.16374v1)|[link](https://github.com/MatNLP/KEHRL)|
|**2024-06-24**|**Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models**|Yichen Sun et.al.|[2406.16333v1](http://arxiv.org/abs/2406.16333v1)|null|
|**2024-06-24**|**Graph-Augmented LLMs for Personalized Health Insights: A Case Study in Sleep Analysis**|Ajan Subramanian et.al.|[2406.16252v2](http://arxiv.org/abs/2406.16252v2)|null|
|**2024-06-23**|**GraphEval2000: Benchmarking and Improving Large Language Models on Graph Datasets**|Qiming Wu et.al.|[2406.16176v1](http://arxiv.org/abs/2406.16176v1)|null|
|**2024-06-23**|**Can LLM Graph Reasoning Generalize beyond Pattern Memorization?**|Yizhuo Zhang et.al.|[2406.15992v1](http://arxiv.org/abs/2406.15992v1)|null|
|**2024-06-22**|**LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning**|Guangsi Shi et.al.|[2406.15859v1](http://arxiv.org/abs/2406.15859v1)|null|
|**2024-06-22**|**Large Language Models for Link Stealing Attacks Against Graph Neural Networks**|Faqian Guan et.al.|[2406.16963v1](http://arxiv.org/abs/2406.16963v1)|null|
|**2024-06-21**|**Inferring Pluggable Types with Machine Learning**|Kazi Amanul Islam Siddiqui et.al.|[2406.15676v1](http://arxiv.org/abs/2406.15676v1)|null|
|**2024-06-21**|**NLP-KG: A System for Exploratory Search of Scientific Literature in Natural Language Processing**|Tim Schopf et.al.|[2406.15294v1](http://arxiv.org/abs/2406.15294v1)|[link](https://github.com/nlp-knowledge-graph/nlp-kg-webapp)|
|**2024-06-21**|**Unsupervised Extraction of Dialogue Policies from Conversations**|Makesh Narsimhan Sreedhar et.al.|[2406.15214v1](http://arxiv.org/abs/2406.15214v1)|null|
|**2024-06-21**|**Uni-Mol2: Exploring Molecular Pretraining Model at Scale**|Xiaohong Ji et.al.|[2406.14969v1](http://arxiv.org/abs/2406.14969v1)|null|
|**2024-06-20**|**Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks**|Sefika Efeoglu et.al.|[2406.14745v2](http://arxiv.org/abs/2406.14745v2)|null|
|**2024-06-20**|**Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics**|Seungbeen Lee et.al.|[2406.14703v1](http://arxiv.org/abs/2406.14703v1)|null|
|**2024-06-20**|**TAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models**|Jiarui Feng et.al.|[2406.14683v1](http://arxiv.org/abs/2406.14683v1)|[link](https://github.com/jiaruifeng/taglas)|
|**2024-06-20**|**HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation**|Jin Wang et.al.|[2406.14655v1](http://arxiv.org/abs/2406.14655v1)|null|
|**2024-06-20**|**GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models**|Shilong Li et.al.|[2406.14550v1](http://arxiv.org/abs/2406.14550v1)|null|
|**2024-06-20**|**medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs**|Mingyi Jia et.al.|[2406.14326v1](http://arxiv.org/abs/2406.14326v1)|null|
|**2024-06-20**|**Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs**|Junjie Wang et.al.|[2406.14282v1](http://arxiv.org/abs/2406.14282v1)|[link](https://github.com/zjukg/lpkg)|
|**2024-06-20**|**ReaLHF: Optimized RLHF Training for Large Language Models through Parameter Reallocation**|Zhiyu Mei et.al.|[2406.14088v1](http://arxiv.org/abs/2406.14088v1)|[link](https://github.com/openpsi-project/realhf)|
|**2024-06-20**|**HIGHT: Hierarchical Graph Tokenization for Graph-Language Alignment**|Yongqiang Chen et.al.|[2406.14021v1](http://arxiv.org/abs/2406.14021v1)|null|
|**2024-06-19**|**A Pure Transformer Pretraining Framework on Text-attributed Graphs**|Yu Song et.al.|[2406.13873v1](http://arxiv.org/abs/2406.13873v1)|[link](https://github.com/songyyyy/gspt)|
|**2024-06-19**|**Knowledge Graph-Enhanced Large Language Models via Path Selection**|Haochen Liu et.al.|[2406.13862v1](http://arxiv.org/abs/2406.13862v1)|[link](https://github.com/haochenliu2000/kelp)|
|**2024-06-19**|**Few-shot Knowledge Graph Relational Reasoning via Subgraph Adaptation**|Haochen Liu et.al.|[2406.15507v1](http://arxiv.org/abs/2406.15507v1)|null|
|**2024-06-19**|**Dr.E Bridges Graphs with Large Language Models through Words**|Zipeng Liu et.al.|[2406.15504v1](http://arxiv.org/abs/2406.15504v1)|null|
|**2024-06-19**|**Enhancing Distractor Generation for Multiple-Choice Questions with Retrieval Augmented Pretraining and Knowledge Graph Integration**|Han-Cheng Yu et.al.|[2406.13578v1](http://arxiv.org/abs/2406.13578v1)|null|
|**2024-06-19**|**LangTopo: Aligning Language Descriptions of Graphs with Tokenized Topological Modeling**|Zhong Guan et.al.|[2406.13250v1](http://arxiv.org/abs/2406.13250v1)|null|
|**2024-06-19**|**Enhancing Collaborative Semantics of Language Model-Driven Recommendations via Graph-Aware Learning**|Zhong Guan et.al.|[2406.13235v1](http://arxiv.org/abs/2406.13235v1)|null|
|**2024-06-19**|**Bridging Law and Data: Augmenting Reasoning via a Semi-Structured Dataset with IRAC methodology**|Xiaoxi Kang et.al.|[2406.13217v1](http://arxiv.org/abs/2406.13217v1)|null|
|**2024-06-19**|**PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes**|He Cao et.al.|[2406.13193v1](http://arxiv.org/abs/2406.13193v1)|[link](https://github.com/idea-xl/presto)|
|**2024-06-19**|**QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism**|Bo Wang et.al.|[2406.13167v1](http://arxiv.org/abs/2406.13167v1)|null|
|**2024-06-18**|**Bridging Local Details and Global Context in Text-Attributed Graphs**|Yaoke Wang et.al.|[2406.12608v1](http://arxiv.org/abs/2406.12608v1)|null|
|**2024-06-18**|**MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction**|Yuyan Liu et.al.|[2406.12950v1](http://arxiv.org/abs/2406.12950v1)|[link](https://github.com/nyushcs/moleculargpt)|
|**2024-06-18**|**LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document Summarization**|Masafumi Enomoto et.al.|[2406.12494v1](http://arxiv.org/abs/2406.12494v1)|null|
|**2024-06-18**|**Interpretable Catastrophic Forgetting of Large Language Model Fine-tuning via Instruction Vector**|Gangwei Jiang et.al.|[2406.12227v2](http://arxiv.org/abs/2406.12227v2)|null|
|**2024-06-17**|**DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs**|Jiasheng Zhang et.al.|[2406.12072v2](http://arxiv.org/abs/2406.12072v2)|[link](https://github.com/zjs123/DTGB)|
|**2024-06-17**|**UniGLM: Training One Unified Language Model for Text-Attributed Graphs**|Yi Fang et.al.|[2406.12052v1](http://arxiv.org/abs/2406.12052v1)|[link](https://github.com/nyushcs/uniglm)|
|**2024-06-17**|**GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models**|Yi Fang et.al.|[2406.11945v1](http://arxiv.org/abs/2406.11945v1)|[link](https://github.com/nyushcs/gaugllm)|
|**2024-06-17**|**Input Conditioned Graph Generation for Language Agents**|Lukas Vierling et.al.|[2406.11555v1](http://arxiv.org/abs/2406.11555v1)|[link](https://github.com/lukasvierling/dynamicgptswarm)|
|**2024-06-17**|**Large Language Models and Knowledge Graphs for Astronomical Entity Disambiguation**|Golnaz Shapurian et.al.|[2406.11400v1](http://arxiv.org/abs/2406.11400v1)|null|
|**2024-06-17**|**How Good are LLMs at Relation Extraction under Low-Resource Scenario? Comprehensive Evaluation**|Dawulie Jinensibieke et.al.|[2406.11162v2](http://arxiv.org/abs/2406.11162v2)|[link](https://github.com/victor812-hub/entity_datasets)|
|**2024-06-17**|**Contextual Knowledge Graph**|Chengjin Xu et.al.|[2406.11160v2](http://arxiv.org/abs/2406.11160v2)|null|
|**2024-06-17**|**Are Large Language Models a Good Replacement of Taxonomies?**|Yushi Sun et.al.|[2406.11131v2](http://arxiv.org/abs/2406.11131v2)|[link](https://github.com/ysunbp/taxoglimpse)|
|**2024-06-16**|**DocNet: Semantic Structure in Inductive Bias Detection Models**|Jessica Zhu et.al.|[2406.10965v1](http://arxiv.org/abs/2406.10965v1)|[link](https://github.com/nlpresearchanon/DocNet)|
|**2024-06-16**|**Light Up the Shadows: Enhance Long-Tailed Entity Grounding with Concept-Guided Vision-Language Models**|Yikai Zhang et.al.|[2406.10902v1](http://arxiv.org/abs/2406.10902v1)|null|
|**2024-06-16**|**KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs**|Aihua Pei et.al.|[2406.10802v1](http://arxiv.org/abs/2406.10802v1)|[link](https://github.com/aika-wsd/KGPA)|
|**2024-06-15**|**A Comprehensive Survey of Foundation Models in Medicine**|Wasif Khan et.al.|[2406.10729v1](http://arxiv.org/abs/2406.10729v1)|null|
|**2024-06-15**|**SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task**|Ziije Zhong et.al.|[2406.10710v1](http://arxiv.org/abs/2406.10710v1)|null|
|**2024-06-15**|**Large Language Models as Event Forecasters**|Libo Zhang et.al.|[2406.10492v1](http://arxiv.org/abs/2406.10492v1)|null|
|**2024-06-15**|**Unlocking Large Language Model's Planning Capabilities with Maximum Diversity Fine-tuning**|Wenjun Li et.al.|[2406.10479v1](http://arxiv.org/abs/2406.10479v1)|null|
|**2024-06-14**|**Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models**|Manas Jhalani et.al.|[2406.09994v1](http://arxiv.org/abs/2406.09994v1)|null|
|**2024-06-14**|**DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning**|Zeyu Gao et.al.|[2406.09953v1](http://arxiv.org/abs/2406.09953v1)|null|
|**2024-06-14**|**TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs**|Zhuofeng Li et.al.|[2406.10310v1](http://arxiv.org/abs/2406.10310v1)|null|
|**2024-06-13**|**Automated Molecular Concept Generation and Labeling with Large Language Models**|Shichang Zhang et.al.|[2406.09612v1](http://arxiv.org/abs/2406.09612v1)|null|
|**2024-06-13**|**Cross-Modality Program Representation Learning for Electronic Design Automation with High-Level Synthesis**|Zongyue Qin et.al.|[2406.09606v1](http://arxiv.org/abs/2406.09606v1)|null|
|**2024-06-13**|**Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models**|Yushi Hu et.al.|[2406.09403v1](http://arxiv.org/abs/2406.09403v1)|null|
|**2024-06-13**|**Transformers meet Neural Algorithmic Reasoners**|Wilfried Bounsi et.al.|[2406.09308v1](http://arxiv.org/abs/2406.09308v1)|null|
|**2024-06-13**|**SememeLM: A Sememe Knowledge Enhanced Method for Long-tail Relation Representation**|Shuyi Li et.al.|[2406.10297v1](http://arxiv.org/abs/2406.10297v1)|null|
|**2024-06-13**|**Hierarchical Compression of Text-Rich Graphs via Large Language Models**|Shichang Zhang et.al.|[2406.11884v1](http://arxiv.org/abs/2406.11884v1)|null|
|**2024-06-13**|**ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions**|Xu Zhang et.al.|[2406.08842v1](http://arxiv.org/abs/2406.08842v1)|null|
|**2024-06-12**|**Research Trends for the Interplay between Large Language Models and Knowledge Graphs**|Hanieh Khorashadizadeh et.al.|[2406.08223v1](http://arxiv.org/abs/2406.08223v1)|null|
|**2024-06-12**|**SHACL2FOL: An FOL Toolkit for SHACL Decision Problems**|Paolo Pareti et.al.|[2406.08018v1](http://arxiv.org/abs/2406.08018v1)|[link](https://github.com/paolo7/shacl2fol)|
|**2024-06-11**|**Efficient Parallel Multi-Hop Reasoning: A Scalable Approach for Knowledge Graph Analysis**|Jesmin Jahan Tithi et.al.|[2406.07727v1](http://arxiv.org/abs/2406.07727v1)|null|
|**2024-06-11**|**TextGrad: Automatic "Differentiation" via Text**|Mert Yuksekgonul et.al.|[2406.07496v1](http://arxiv.org/abs/2406.07496v1)|[link](https://github.com/zou-group/textgrad)|
|**2024-06-11**|**CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization**|Frederic Kirstein et.al.|[2406.07494v2](http://arxiv.org/abs/2406.07494v2)|null|
|**2024-06-11**|**Large Language Models for Constrained-Based Causal Discovery**|Kai-Hendrik Cohrs et.al.|[2406.07378v1](http://arxiv.org/abs/2406.07378v1)|[link](https://github.com/ipl-uv/causal_gpt)|
|**2024-06-11**|**Scaling Large-Language-Model-based Multi-Agent Collaboration**|Chen Qian et.al.|[2406.07155v1](http://arxiv.org/abs/2406.07155v1)|[link](https://github.com/openbmb/chatdev)|
|**2024-06-11**|**Mining Frequent Structures in Conceptual Models**|Mattia Fumagalli et.al.|[2406.07129v1](http://arxiv.org/abs/2406.07129v1)|[link](https://github.com/unibz-core/cm-mining_experimentdata)|
|**2024-06-11**|**Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene Graph**|Sergey Linok et.al.|[2406.07113v2](http://arxiv.org/abs/2406.07113v2)|null|
|**2024-06-11**|**DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs**|Haishuo Fang et.al.|[2406.07080v1](http://arxiv.org/abs/2406.07080v1)|[link](https://github.com/UKPLab/acl2024-DARA)|
|**2024-06-11**|**Improving Multi-hop Logical Reasoning in Knowledge Graphs with Context-Aware Query Representation Learning**|Jeonghoon Kim et.al.|[2406.07034v1](http://arxiv.org/abs/2406.07034v1)|[link](https://github.com/kjh9503/caqr)|
|**2024-06-10**|**MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension**|Khiem Le et.al.|[2406.06777v2](http://arxiv.org/abs/2406.06777v2)|null|
|**2024-06-10**|**The Curse of Popularity: Popular Entities have Catastrophic Side Effects when Deleting Knowledge from Language Models**|Ryosuke Takahashi et.al.|[2406.06032v1](http://arxiv.org/abs/2406.06032v1)|null|
|**2024-06-10**|**HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs**|Pranoy Panda et.al.|[2406.06027v1](http://arxiv.org/abs/2406.06027v1)|null|
|**2024-06-08**|**Generalist Multimodal AI: A Review of Architectures, Challenges and Opportunities**|Sai Munikoti et.al.|[2406.05496v1](http://arxiv.org/abs/2406.05496v1)|null|
|**2024-06-07**|**TLEX: An Efficient Method for Extracting Exact Timelines from TimeML Temporal Graphs**|Mustafa Ocal et.al.|[2406.05265v1](http://arxiv.org/abs/2406.05265v1)|null|
|**2024-06-07**|**LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering**|Harry Li et.al.|[2406.06621v1](http://arxiv.org/abs/2406.06621v1)|[link](https://github.com/mit-ll/linkq)|
|**2024-06-07**|**Compositional Generalization with Grounded Language Models**|Sondre Wold et.al.|[2406.04989v1](http://arxiv.org/abs/2406.04989v1)|[link](https://github.com/ltgoslo/text-graph-generalization)|
|**2024-06-07**|**CRAG -- Comprehensive RAG Benchmark**|Xiao Yang et.al.|[2406.04744v1](http://arxiv.org/abs/2406.04744v1)|null|
|**2024-06-07**|**Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**|Yong-Min Shin et.al.|[2406.04612v1](http://arxiv.org/abs/2406.04612v1)|[link](https://github.com/jordan7186/gatt)|
|**2024-06-07**|**OCDB: Revisiting Causal Discovery with a Comprehensive Benchmark and Evaluation Framework**|Wei Zhou et.al.|[2406.04598v1](http://arxiv.org/abs/2406.04598v1)|null|
|**2024-06-06**|**ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions**|Sreyan Ghosh et.al.|[2406.04286v1](http://arxiv.org/abs/2406.04286v1)|[link](https://github.com/sreyan88/abex)|
|**2024-06-06**|**Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models**|Ling Yang et.al.|[2406.04271v1](http://arxiv.org/abs/2406.04271v1)|[link](https://github.com/yangling0818/buffer-of-thought-llm)|
|**2024-06-06**|**Transformers need glasses! Information over-squashing in language tasks**|Federico Barbero et.al.|[2406.04267v1](http://arxiv.org/abs/2406.04267v1)|null|
|**2024-06-06**|**The CLRS-Text Algorithmic Reasoning Language Benchmark**|Larisa Markeeva et.al.|[2406.04229v1](http://arxiv.org/abs/2406.04229v1)|[link](https://github.com/google-deepmind/clrs)|
|**2024-06-06**|**Tox-BART: Leveraging Toxicity Attributes for Explanation Generation of Implicit Hate Speech**|Neemesh Yadav et.al.|[2406.03953v1](http://arxiv.org/abs/2406.03953v1)|null|
|**2024-06-06**|**Performance of large language models in numerical vs. semantic medical knowledge: Benchmarking on evidence-based Q&As**|Eden Avnat et.al.|[2406.03855v1](http://arxiv.org/abs/2406.03855v1)|null|
|**2024-06-06**|**Are Large Language Models the New Interface for Data Pipelines?**|Sylvio Barbon Junior et.al.|[2406.06596v1](http://arxiv.org/abs/2406.06596v1)|null|
|**2024-06-06**|**Efficient Knowledge Infusion via KG-LLM Alignment**|Zhouyu Jiang et.al.|[2406.03746v1](http://arxiv.org/abs/2406.03746v1)|null|
|**2024-06-06**|**FastGAS: Fast Graph-based Annotation Selection for In-Context Learning**|Zihan Chen et.al.|[2406.03730v1](http://arxiv.org/abs/2406.03730v1)|null|

#### Abstracts
##### **Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment**
2406.19255v1 by Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan

While pre-training large-scale video-language models (VLMs) has shown
remarkable potential for various downstream video-language tasks, existing VLMs
can still suffer from certain commonly seen limitations, e.g., coarse-grained
cross-modal aligning , under-modeling of temporal dynamics, detached
video-language view. In this work, we target enhancing VLMs with a fine-grained
structural spatio-temporal alignment learning method (namely Finsta). First of
all, we represent the input texts and videos with fine-grained scene graph (SG)
structures, both of which are further unified into a holistic SG (HSG) for
bridging two modalities. Then, an SG-based framework is built, where the
textual SG (TSG) is encoded with a graph Transformer, while the video dynamic
SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for
spatial and temporal feature propagation. A spatial-temporal Gaussian
differential graph Transformer is further devised to strengthen the sense of
the changes in objects across spatial and temporal dimensions. Next, based on
the fine-grained structural features of TSG and DSG, we perform object-centered
spatial alignment and predicate-centered temporal alignment respectively,
enhancing the video-language grounding in both the spatiality and temporality.
We design our method as a plug&play system, which can be integrated into
existing well-trained VLMs for further representation augmentation, without
training from scratch or relying on SG annotations in downstream applications.
On 6 representative VL modeling tasks over 12 datasets in both standard and
long-form video scenarios, Finsta consistently improves the existing 13
strong-performing VLMs persistently, and refreshes the current state-of-the-art
end task performance significantly in both the fine-tuning and zero-shot
settings.

ÊëòË¶ÅÔºö<paragraph>ÈõñÁÑ∂È†êË®ìÁ∑¥Â§ßÂûãË¶ñË®äË™ûË®ÄÊ®°Âûã (VLM) Â∑≤Â±ïÁèæÂá∫Â∞çÂêÑÁ®Æ‰∏ãÊ∏∏Ë¶ñË®äË™ûË®Ä‰ªªÂãôÁöÑÈ°ØËëóÊΩõÂäõÔºå‰ΩÜÁèæÊúâÁöÑ VLM ‰ªçÂèØËÉΩÂèóÂà∞Êüê‰∫õÂ∏∏Ë¶ãÈôêÂà∂ÁöÑÂΩ±ÈüøÔºå‰æãÂ¶ÇÁ≤óÁ≤íÂ∫¶ÁöÑË∑®Ê®°ÊÖãÂ∞çÈΩä„ÄÅÂ∞çÊôÇÈñìÂãïÊÖãÁöÑÂª∫Ê®°‰∏çË∂≥„ÄÅÂàÜÈõ¢ÁöÑË¶ñË®äË™ûË®ÄÊ™¢Ë¶ñ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ª•ÂÖ∑ÂÇôÁ¥∞Á≤íÂ∫¶ÁµêÊßãÂåñÊôÇÁ©∫Â∞çÈΩäÂ≠∏ÁøíÊñπÊ≥ï (Âç≥ Finsta) ÁöÑÂ¢ûÂº∑ VLM ÁÇ∫ÁõÆÊ®ô„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰ª•Á¥∞Á≤íÂ∫¶ÁöÑÂ†¥ÊôØÂúñ (SG) ÁµêÊßãË°®Á§∫Ëº∏ÂÖ•ÊñáÂ≠óÂíåË¶ñË®äÔºåÂÖ©ËÄÖÈÄ≤‰∏ÄÊ≠•Áµ±‰∏ÄÂà∞‰∏ÄÂÄãÊï¥È´î SG (HSG) ‰∏≠Ôºå‰ª•Ê©ãÊé•ÂÖ©ÂÄãÊ®°ÊÖã„ÄÇÁÑ∂ÂæåÔºåÂª∫Á´ã‰∏ÄÂÄãÂü∫Êñº SG ÁöÑÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠ÊñáÂ≠ó SG (TSG) ‰ΩøÁî®ÂúñÂΩ¢ Transformer Á∑®Á¢ºÔºåËÄåË¶ñË®äÂãïÊÖã SG (DSG) Âíå HSG Ââá‰ΩøÁî®Êñ∞Á©éÁöÑÈÅûËø¥ÂúñÂΩ¢ Transformer Âª∫Ê®°Ôºå‰ª•ÈÄ≤Ë°åÁ©∫ÈñìÂíåÊôÇÈñìÁâπÂæµÂÇ≥Êí≠„ÄÇÈÄ≤‰∏ÄÊ≠•Ë®≠Ë®à‰∫Ü‰∏ÄÂÄãÊôÇÁ©∫È´òÊñØÂ∑ÆÂàÜÂúñÂΩ¢ TransformerÔºå‰ª•Â¢ûÂº∑Áâ©È´îÂú®ÊôÇÁ©∫Á∂≠Â∫¶‰∏≠ËÆäÂåñÁöÑÊÑüË¶∫„ÄÇÊé•‰∏ã‰æÜÔºåÊ†πÊìö TSG Âíå DSG ÁöÑÁ¥∞Á≤íÂ∫¶ÁµêÊßãÁâπÂæµÔºåÊàëÂÄëÂàÜÂà•Âü∑Ë°å‰ª•Áâ©‰ª∂ÁÇ∫‰∏≠ÂøÉÁöÑÁ©∫ÈñìÂ∞çÈΩäÂíå‰ª•Ë¨ÇË©ûÁÇ∫‰∏≠ÂøÉÁöÑÊôÇÂ∫èÂ∞çÈΩäÔºåÂ¢ûÂº∑Ë¶ñË®äË™ûË®ÄÂú®Á©∫ÈñìÂíåÊôÇÈñì‰∏äÁöÑÂü∫Á§é„ÄÇÊàëÂÄëÂ∞áÊñπÊ≥ïË®≠Ë®àÁÇ∫‰∏ÄÂÄãÂç≥ÊèíÂç≥Áî®ÁöÑÁ≥ªÁµ±ÔºåÂèØ‰ª•Êï¥ÂêàÂà∞ÁèæÊúâÁöÑË®ìÁ∑¥ËâØÂ•ΩÁöÑ VLM ‰∏≠Ôºå‰ª•ÈÄ≤‰∏ÄÊ≠•Êì¥ÂÖÖË°®Á§∫ÔºåËÄåÁÑ°ÈúÄÂæûÈ†≠ÈñãÂßãË®ìÁ∑¥Êàñ‰æùË≥¥‰∏ãÊ∏∏ÊáâÁî®Á®ãÂºè‰∏≠ÁöÑ SG Ê®ôË®ª„ÄÇÂú® 12 ÂÄãË≥áÊñôÈõÜ‰∏äÁöÑ 6 ÂÄã‰ª£Ë°®ÊÄß VL Âª∫Ê®°‰ªªÂãô‰∏≠ÔºåÁÑ°Ë´ñÊòØÂú®Ê®ôÊ∫ñË¶ñË®äÂ†¥ÊôØÈÇÑÊòØÈï∑Ê†ºÂºèË¶ñË®äÂ†¥ÊôØ‰∏≠ÔºåFinsta ÈÉΩÊåÅÁ∫åÊîπÂñÑÁèæÊúâÁöÑ 13 ÂÄãÊïàËÉΩÂº∑Â§ßÁöÑ VLMÔºå‰∏¶Âú®ÂæÆË™øÂíåÈõ∂Ê¨°Â≠∏ÁøíË®≠ÂÆö‰∏≠È°ØËëóÊõ¥Êñ∞ÁõÆÂâçÁöÑÊúÄÊñ∞ÊäÄË°ìÊúÄÁµÇ‰ªªÂãôÊïàËÉΩ„ÄÇ</paragraph>

##### **TrustUQA: A Trustful Framework for Unified Structured Data Question Answering**
2406.18916v1 by Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang, Huajun Chen

Natural language question answering (QA) over structured data sources such as
tables and knowledge graphs (KGs) have been widely investigated, for example
with Large Language Models (LLMs). The main solutions include question to
formal query parsing and retrieval-based answer generation. However, current
methods of the former often suffer from weak generalization, failing to dealing
with multiple sources simultaneously, while the later is limited in
trustfulness. In this paper, we propose UnifiedTQA, a trustful QA framework
that can simultaneously support multiple types of structured data in a unified
way. To this end, it adopts an LLM-friendly and unified knowledge
representation method called Condition Graph (CG), and uses an LLM and
demonstration-based two-level method for CG querying. For enhancement, it is
also equipped with dynamic demonstration retrieval. We have evaluated
UnifiedTQA with 5 benchmarks covering 3 types of structured data. It
outperforms 2 existing unified structured data QA methods and in comparison
with the baselines that are specific to a data type, it achieves
state-of-the-art on 2 of them. Further more, we demonstrates potential of our
method for more general QA tasks, QA over mixed structured data and QA across
structured data.

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄÂïèÁ≠î (QA) ÈÄèÈÅéÁµêÊßãÂåñË≥áÊñô‰æÜÊ∫êÔºà‰æãÂ¶ÇË°®Ê†ºÂíåÁü•Ë≠òÂúñË≠ú (KGs)ÔºâÂ∑≤Âª£Ê≥õÁ†îÁ©∂Ôºå‰æãÂ¶Ç‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇ‰∏ªË¶ÅËß£Ê±∫ÊñπÊ°àÂåÖÊã¨ÂïèÈ°åËΩâÊèõÊàêÂΩ¢ÂºèÂåñÊü•Ë©¢Ëß£ÊûêÂíåÂü∫ÊñºÊ™¢Á¥¢ÁöÑÁ≠îÊ°àÁî¢Áîü„ÄÇÁÑ∂ËÄåÔºåÂâçËÄÖÁöÑÁèæË°åÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÁî¢ÁîüÂº±Ê≥õÂåñÔºåÁÑ°Ê≥ïÂêåÊôÇËôïÁêÜÂ§öÂÄã‰æÜÊ∫êÔºåËÄåÂæåËÄÖÂâáÂèóÂà∞ÂèØ‰ø°Â∫¶ÁöÑÈôêÂà∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ UnifiedTQAÔºå‰∏ÄÂÄãÂèØ‰ø°Ë≥¥ÁöÑ QA Ê°ÜÊû∂ÔºåËÉΩÂ§†‰ª•Áµ±‰∏ÄÁöÑÊñπÂºèÂêåÊôÇÊîØÊè¥Â§öÁ®ÆÈ°ûÂûãÁöÑÁµêÊßãÂåñË≥áÊñô„ÄÇÁÇ∫Ê≠§ÔºåÂÆÉÊé°Áî®‰∫Ü‰∏ÄÁ®Æ LLM ÂèãÂñÑ‰∏îÁµ±‰∏ÄÁöÑÁü•Ë≠òË°®Á§∫ÊñπÊ≥ïÔºåÁ®±ÁÇ∫Ê¢ù‰ª∂Âúñ (CG)Ôºå‰∏¶‰ΩøÁî® LLM ÂíåÂü∫ÊñºÁ§∫ÁØÑÁöÑ‰∫åÈöéÊñπÊ≥ïÈÄ≤Ë°å CG Êü•Ë©¢„ÄÇÁÇ∫‰∫ÜÂä†Âº∑ÔºåÂÆÉÈÇÑÈÖçÂÇô‰∫ÜÂãïÊÖãÁ§∫ÁØÑÊ™¢Á¥¢„ÄÇÊàëÂÄëÂ∑≤Á∂ì‰ΩøÁî®Ê∂µËìã 3 Á®ÆÈ°ûÂûãÁµêÊßãÂåñË≥áÊñôÁöÑ 5 ÂÄãÂü∫Ê∫ñË©ï‰º∞ UnifiedTQA„ÄÇÂÆÉÂÑ™Êñº 2 Á®ÆÁèæÊúâÁöÑÁµ±‰∏ÄÁµêÊßãÂåñË≥áÊñô QA ÊñπÊ≥ïÔºå‰∏¶‰∏îËàáÁâπÂÆöÊñºË≥áÊñôÈ°ûÂûãÁöÑÂü∫Á∑öÁõ∏ÊØîÔºåÂÆÉÂú®ÂÖ∂‰∏≠ 2 ÂÄãÂü∫Ê∫ñ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ∞¥Âπ≥„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Êõ¥ÈÄöÁî®ÁöÑ QA ‰ªªÂãô„ÄÅÊ∑∑ÂêàÁµêÊßãÂåñË≥áÊñôÁöÑ QA ÂíåË∑®ÁµêÊßãÂåñË≥áÊñôÁöÑ QA ‰∏≠ÁöÑÊΩõÂäõ„ÄÇ

##### **Fast Optimizer Benchmark**
2406.18701v1 by Simon Blauth, Tobias B√ºrger, Zacharias H√§ringer, J√∂rg Franke, Frank Hutter

In this paper, we present the Fast Optimizer Benchmark (FOB), a tool designed
for evaluating deep learning optimizers during their development. The benchmark
supports tasks from multiple domains such as computer vision, natural language
processing, and graph learning. The focus is on convenient usage, featuring
human-readable YAML configurations, SLURM integration, and plotting utilities.
FOB can be used together with existing hyperparameter optimization (HPO) tools
as it handles training and resuming of runs. The modular design enables
integration into custom pipelines, using it simply as a collection of tasks. We
showcase an optimizer comparison as a usage example of our tool. FOB can be
found on GitHub: https://github.com/automl/FOB.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂø´ÈÄüÂÑ™ÂåñÂô®Âü∫Ê∫ñ (FOB)ÔºåÈÄôÊòØ‰∏ÄÂÄãÁî®ÊñºÂú®ÈñãÁôºÈÅéÁ®ã‰∏≠Ë©ï‰º∞Ê∑±Â∫¶Â≠∏ÁøíÂÑ™ÂåñÂô®ÁöÑÂ∑•ÂÖ∑„ÄÇÂü∫Ê∫ñÊîØÊåÅ‰æÜËá™Â§öÂÄãÈ†òÂüüÁöÑ‰ªªÂãôÔºå‰æãÂ¶ÇÈõªËÖ¶Ë¶ñË¶∫„ÄÅËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÂúñÂΩ¢Â≠∏Áøí„ÄÇÈáçÈªûÂú®ÊñºÊñπ‰æø‰ΩøÁî®ÔºåÂÖ∑Êúâ‰∫∫È°ûÂèØËÆÄÁöÑ YAML ÈÖçÁΩÆ„ÄÅSLURM Êï¥ÂêàÂíåÁπ™ÂúñÁ®ãÂºè„ÄÇFOB ÂèØ‰ª•ËàáÁèæÊúâÁöÑË∂ÖÂèÉÊï∏ÂÑ™Âåñ (HPO) Â∑•ÂÖ∑‰∏ÄËµ∑‰ΩøÁî®ÔºåÂõ†ÁÇ∫ÂÆÉÂèØ‰ª•ËôïÁêÜË®ìÁ∑¥ÂíåÊÅ¢Âæ©ÈÅãË°å„ÄÇÊ®°ÁµÑÂåñË®≠Ë®àËÉΩÂ§†Êï¥ÂêàÂà∞Ëá™Ë®ÇÁÆ°Á∑ö‰∏≠ÔºåÂè™ÈúÄÂ∞áÂÖ∂Áî®‰Ωú‰ªªÂãôÈõÜÂêàÂç≥ÂèØ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏ÄÂÄãÂÑ™ÂåñÂô®ÊØîËºÉ‰ΩúÁÇ∫ÊàëÂÄëÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®ÁØÑ‰æã„ÄÇFOB ÂèØ‰ª•Âæû GitHub ÊâæÂà∞Ôºöhttps://github.com/automl/FOB„ÄÇ

##### **Cascading Large Language Models for Salient Event Graph Generation**
2406.18449v1 by Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, Yulan He

Generating event graphs from long documents is challenging due to the
inherent complexity of multiple tasks involved such as detecting events,
identifying their relationships, and reconciling unstructured input with
structured graphs. Recent studies typically consider all events with equal
importance, failing to distinguish salient events crucial for understanding
narratives. This paper presents CALLMSAE, a CAscading Large Language Model
framework for SAlient Event graph generation, which leverages the capabilities
of LLMs and eliminates the need for costly human annotations. We first identify
salient events by prompting LLMs to generate summaries, from which salient
events are identified. Next, we develop an iterative code refinement prompting
strategy to generate event relation graphs, removing hallucinated relations and
recovering missing edges. Fine-tuning contextualised graph generation models on
the LLM-generated graphs outperforms the models trained on CAEVO-generated
data. Experimental results on a human-annotated test set show that the proposed
method generates salient and more accurate graphs, outperforming competitive
baselines.

ÊëòË¶ÅÔºöÁî±ÊñºÊ∂âÂèäÂ§öÈ†Ö‰ªªÂãôÁöÑÂÖßÂú®Ë§áÈõúÊÄßÔºå‰æãÂ¶ÇÂÅµÊ∏¨‰∫ã‰ª∂„ÄÅË≠òÂà•ÂÖ∂Èóú‰øÇÔºå‰ª•ÂèäË™øÂíåÈùûÁµêÊßãÂåñËº∏ÂÖ•ËàáÁµêÊßãÂåñÂúñË°®ÔºåÂõ†Ê≠§ÂæûÈï∑ÁØáÊñá‰ª∂Áî¢Áîü‰∫ã‰ª∂ÂúñË°®ÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂ÈÄöÂ∏∏Â∞áÊâÄÊúâ‰∫ã‰ª∂Ë¶ñÁÇ∫ÂêåÁ≠âÈáçË¶ÅÔºåÊú™ËÉΩÂçÄÂàÜÂ∞çÁêÜËß£Êïò‰∫ãËá≥ÈóúÈáçË¶ÅÁöÑÈ°ØËëó‰∫ã‰ª∂„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü CALLMSAEÔºå‰∏ÄÂÄãÁî®ÊñºÁîüÊàêÈ°ØËëó‰∫ã‰ª∂ÂúñË°®ÁöÑÂ±§ÁñäÂºèÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÊ°ÜÊû∂ÔºåÂÆÉÂà©Áî®‰∫Ü LLM ÁöÑÂäüËÉΩÔºå‰∏¶Ê∂àÈô§‰∫ÜÂ∞çÊòÇË≤¥ÁöÑ‰∫∫Â∑•Ê®ôË®ªÁöÑÈúÄÊ±Ç„ÄÇÊàëÂÄëÈ¶ñÂÖàÈÄèÈÅéÊèêÁ§∫ LLM Áî¢ÁîüÊëòË¶Å‰æÜË≠òÂà•È°ØËëó‰∫ã‰ª∂ÔºåÂæû‰∏≠Ë≠òÂà•Âá∫È°ØËëó‰∫ã‰ª∂„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÂèçË¶ÜÁöÑÁ®ãÂºèÁ¢ºÁ≤æÁÖâÊèêÁ§∫Á≠ñÁï•‰æÜÁî¢Áîü‰∫ã‰ª∂Èóú‰øÇÂúñË°®ÔºåÁßªÈô§ÂπªË¶∫Èóú‰øÇ‰∏¶ÊÅ¢Âæ©ÈÅ∫Â§±ÁöÑÈÇäÁ∑£„ÄÇÂú® LLM ÁîüÊàêÁöÑÂúñË°®‰∏äÂæÆË™øÊÉÖÂ¢ÉÂåñÂúñË°®ÁîüÊàêÊ®°ÂûãÔºåÂÖ∂Ë°®ÁèæÂÑ™ÊñºÂú® CAEVO ÁîüÊàêÁöÑË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°Âûã„ÄÇÂú®‰∫∫Â∑•Ê®ôË®ªÁöÑÊ∏¨Ë©¶ÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÁî¢Áîü‰∫ÜÈ°ØËëó‰∏îÊõ¥Ê∫ñÁ¢∫ÁöÑÂúñË°®ÔºåÂÑ™ÊñºÁ´∂Áà≠ÊÄßÁöÑÂü∫Ê∫ñ„ÄÇ

##### **Sanskrit Knowledge-based Systems: Annotation and Computational Tools**
2406.18276v1 by Hrishikesh Terdalkar

We address the challenges and opportunities in the development of knowledge
systems for Sanskrit, with a focus on question answering. By proposing a
framework for the automated construction of knowledge graphs, introducing
annotation tools for ontology-driven and general-purpose tasks, and offering a
diverse collection of web-interfaces, tools, and software libraries, we have
made significant contributions to the field of computational Sanskrit. These
contributions not only enhance the accessibility and accuracy of Sanskrit text
analysis but also pave the way for further advancements in knowledge
representation and language processing. Ultimately, this research contributes
to the preservation, understanding, and utilization of the rich linguistic
information embodied in Sanskrit texts.

ÊëòË¶ÅÔºöÊàëÂÄëËëóÊâãËß£Ê±∫Ê¢µË™ûÁü•Ë≠òÁ≥ªÁµ±ÈñãÁôº‰∏≠ÁöÑÊåëÊà∞ÂíåÊ©üÊúÉÔºåÈáçÈªûÂú®ÊñºÂïèÈ°åËß£Á≠î„ÄÇÈÄèÈÅéÊèêÂá∫‰∏ÄÂÄãÁî®ÊñºËá™ÂãïÂª∫ÊßãÁü•Ë≠òÂúñË≠úÁöÑÊû∂ÊßãÔºåÂ∞éÂÖ•Áî®ÊñºÊú¨È´îÈ©ÖÂãïÂíå‰∏ÄËà¨Áî®ÈÄî‰ªªÂãôÁöÑË®ªËß£Â∑•ÂÖ∑Ôºå‰∏¶Êèê‰æõÂ§öÊ®£ÂåñÁöÑÁ∂≤Ë∑Ø‰ªãÈù¢„ÄÅÂ∑•ÂÖ∑ÂíåËªüÈ´îÂáΩÂºèÂ∫´ÔºåÊàëÂÄëÂ∞çË®àÁÆóÊ¢µË™ûÈ†òÂüüÂÅöÂá∫‰∫ÜÈáçÂ§ßË≤¢Áçª„ÄÇÈÄô‰∫õË≤¢Áçª‰∏çÂÉÖÂ¢ûÂº∑‰∫ÜÊ¢µË™ûÊñáÊú¨ÂàÜÊûêÁöÑÂèØÂ≠òÂèñÊÄßÂíåÊ∫ñÁ¢∫ÊÄßÔºå‰πüÁÇ∫Áü•Ë≠òË°®ÂæµÂíåË™ûË®ÄËôïÁêÜÁöÑÈÄ≤‰∏ÄÊ≠•ÈÄ≤Â±ïÈã™Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇÊúÄÁµÇÔºåÈÄôÈ†ÖÁ†îÁ©∂ÊúâÂä©Êñº‰øùÂ≠ò„ÄÅÁêÜËß£ÂíåÂà©Áî®Ê¢µË™ûÊñáÊú¨‰∏≠ËòäÂê´ÁöÑË±êÂØåË™ûË®ÄË≥áË®ä„ÄÇ

##### **Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints**
2406.18085v1 by Ran Song, Shizhu He, Shengxiang Gao, Li Cai, Kang Liu, Zhengtao Yu, Jun Zhao

Multilingual Knowledge Graph Completion (mKGC) aim at solving queries like
(h, r, ?) in different languages by reasoning a tail entity t thus improving
multilingual knowledge graphs. Previous studies leverage multilingual
pretrained language models (PLMs) and the generative paradigm to achieve mKGC.
Although multilingual pretrained language models contain extensive knowledge of
different languages, its pretraining tasks cannot be directly aligned with the
mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit
a pronounced English-centric bias. This makes it difficult for mKGC to achieve
good results, particularly in the context of low-resource languages. To
overcome previous problems, this paper introduces global and local knowledge
constraints for mKGC. The former is used to constrain the reasoning of answer
entities, while the latter is used to enhance the representation of query
contexts. The proposed method makes the pretrained model better adapt to the
mKGC task. Experimental results on public datasets demonstrate that our method
outperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32% and
16.03%, which indicates that our proposed method has significant enhancement on
mKGC.

ÊëòË¶ÅÔºöÂ§öË™ûË®ÄÁü•Ë≠òÂúñË≠úÂÆåÊàê (mKGC) Êó®Âú®ÈÄèÈÅéÊé®ÁêÜÂ∞æÈÉ®ÂØ¶È´î t ‰æÜËß£Ê±∫‰∏çÂêåË™ûË®Ä‰∏≠ÁöÑÊü•Ë©¢Ôºå‰æãÂ¶Ç (h, r, ?)ÔºåÈÄ≤ËÄåÊîπÂñÑÂ§öË™ûË®ÄÁü•Ë≠òÂúñË≠ú„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Âà©Áî®Â§öË™ûË®ÄÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (PLM) ÂíåÁîüÊàêÁØÑ‰æã‰æÜÈÅîÊàê mKGC„ÄÇÂÑòÁÆ°Â§öË™ûË®ÄÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÂåÖÂê´‰∏çÂêåË™ûË®ÄÁöÑÂª£Ê≥õÁü•Ë≠òÔºå‰ΩÜÂÖ∂È†êË®ìÁ∑¥‰ªªÂãôÁÑ°Ê≥ïÁõ¥Êé•Ëàá mKGC ‰ªªÂãôÂ∞çÈΩä„ÄÇÊ≠§Â§ñÔºåÁõÆÂâçÂ§ßÂ§öÊï∏ÁöÑÁü•Ë≠òÂúñË≠úÂíå PLM ÈÉΩÂ±ïÁèæÂá∫ÊòéÈ°ØÁöÑËã±Ë™û‰∏≠ÂøÉÂÅèË™§„ÄÇÈÄô‰ΩøÂæó mKGC Èõ£‰ª•ÈÅîÊàêËâØÂ•ΩÁöÑÁµêÊûúÔºåÁâπÂà•ÊòØÂú®‰ΩéË≥áÊ∫êË™ûË®ÄÁöÑËÑàÁµ°‰∏≠„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÂÖàÂâçÁöÑÂïèÈ°åÔºåÊú¨ÊñáÈáùÂ∞ç mKGC ÂºïÂÖ•‰∫ÜÂÖ®ÂüüËàáÂ±ÄÈÉ®Áü•Ë≠òÈôêÂà∂„ÄÇÂâçËÄÖÁî®ÊñºÈôêÂà∂Á≠îÊ°àÂØ¶È´îÁöÑÊé®ÁêÜÔºåËÄåÂæåËÄÖÁî®ÊñºÂä†Âº∑Êü•Ë©¢ËÑàÁµ°ÁöÑË°®Á§∫„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï‰ΩøÂæóÈ†êË®ìÁ∑¥Ê®°ÂûãËÉΩÊõ¥Â•ΩÂú∞ÈÅ©Êáâ mKGC ‰ªªÂãô„ÄÇÂÖ¨ÈñãË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú® Hits@1 Âíå Hits@10 ‰∏äÂπ≥ÂùáÂÑ™ÊñºÂÖàÂâçÁöÑ SOTA 12.32% Âíå 16.03%ÔºåÈÄôË°®Á§∫ÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÈ°ØËëóÂú∞Â¢ûÂº∑‰∫Ü mKGC„ÄÇ

##### **AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning**
2406.18060v1 by Yifan Yang, Kai Zhen, Ershad Banijamal, Athanasios Mouchtaris, Zheng Zhang

Fine-tuning large language models (LLMs) has achieved remarkable performance
across various natural language processing tasks, yet it demands more and more
memory as model sizes keep growing. To address this issue, the recently
proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs
using only forward passes, thereby avoiding the need for a backpropagation
graph. However, significant performance drops and a high risk of divergence
have limited their widespread adoption. In this paper, we propose the Adaptive
Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed
to improve the performance and convergence of the ZO methods. To enhance
dimension-dependent ZO estimation accuracy, we introduce a fast-forward,
low-parameter tensorized adapter. To tackle the frequently observed divergence
issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number
schedule that guarantees convergence. Detailed theoretical analysis and
extensive experimental results on Roberta-Large and Llama-2-7B models
substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory
efficiency, and convergence speed.

ÊëòË¶ÅÔºöÂæÆË∞ÉÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁßçËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÔºå‰ΩÜÈöèÁùÄÊ®°ÂûãËßÑÊ®°ÁöÑ‰∏çÊñ≠Êâ©Â§ßÔºåÂÆÉÂØπÂÜÖÂ≠òÁöÑÈúÄÊ±Ç‰πüË∂äÊù•Ë∂äÂ§ß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊúÄËøëÊèêÂá∫ÁöÑÂÜÖÂ≠òÈ´òÊïàÈõ∂Èò∂ (MeZO) ÊñπÊ≥ïËØïÂõæ‰ªÖ‰ΩøÁî®ÂâçÂêë‰º†ÈÄíÊù•ÂæÆË∞É LLMÔºå‰ªéËÄåÈÅøÂÖç‰∫ÜÂØπÂèçÂêë‰º†Êí≠ÂõæÁöÑÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºå‰∏•ÈáçÁöÑÊÄßËÉΩ‰∏ãÈôçÂíåÂèëÊï£ÁöÑÈ´òÈ£éÈô©ÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÂπøÊ≥õÈááÁî®„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËá™ÈÄÇÂ∫îÈõ∂Èò∂Âº†ÈáèËÆ≠ÁªÉËá™ÈÄÇÂ∫î (AdaZeta) Ê°ÜÊû∂Ôºå‰∏ìÈó®ËÆæËÆ°Áî®‰∫éÊèêÈ´ò ZO ÊñπÊ≥ïÁöÑÊÄßËÉΩÂíåÊî∂ÊïõÊÄß„ÄÇ‰∏∫‰∫ÜÂ¢ûÂº∫Áª¥Â∫¶Áõ∏ÂÖ≥ÁöÑ ZO ‰º∞ËÆ°Á≤æÂ∫¶ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Âø´ÈÄüÂâçÂêë„ÄÅ‰ΩéÂèÇÊï∞Âº†ÈáèÂåñÈÄÇÈÖçÂô®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Âú®Â§ßËßÑÊ®° ZO ÂæÆË∞É‰ªªÂä°‰∏≠ÁªèÂ∏∏ËßÇÂØüÂà∞ÁöÑÂèëÊï£ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Ëá™ÈÄÇÂ∫îÊü•ËØ¢Êï∞ÈáèËÆ°ÂàíÔºå‰ª•‰øùËØÅÊî∂ÊïõÊÄß„ÄÇÂØπ Roberta-Large Âíå Llama-2-7B Ê®°ÂûãÁöÑËØ¶ÁªÜÁêÜËÆ∫ÂàÜÊûêÂíåÂπøÊ≥õÁöÑÂÆûÈ™åÁªìÊûúËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑ AdaZeta Ê°ÜÊû∂Âú®ÂáÜÁ°ÆÊÄß„ÄÅÂÜÖÂ≠òÊïàÁéáÂíåÊî∂ÊïõÈÄüÂ∫¶ÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph**
2406.17271v1 by Zhehao Zhang, Jiaao Chen, Diyi Yang

The current paradigm of evaluating Large Language Models (LLMs) through
static benchmarks comes with significant limitations, such as vulnerability to
data contamination and a lack of adaptability to the evolving capabilities of
LLMs. Therefore, evaluation methods that can adapt and generate evaluation data
with controlled complexity are urgently needed. In this work, we introduce
Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to
dynamically extend current benchmarks with controlled complexity and diversity.
Specifically, we first extract the reasoning graphs of data points in current
benchmarks and then perturb the reasoning graphs to generate novel testing
data. Such newly generated test samples can have different levels of complexity
while maintaining linguistic diversity similar to the original benchmarks. We
further use a code-augmented LLM to ensure the label correctness of newly
generated data. We apply our DARG framework to diverse reasoning tasks in four
domains with 15 state-of-the-art LLMs. Experimental results show that almost
all LLMs experience a performance decrease with increased complexity and
certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit
more biases when being evaluated via the data generated by DARG with higher
complexity levels. These observations provide useful insights into how to
dynamically and adaptively evaluate LLMs. The code is available at
https://github.com/SALT-NLP/DARG.

ÊëòË¶ÅÔºöÁõÆÂâçÈÄèÈÅéÈùúÊÖãÂü∫Ê∫ñË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÁØÑ‰æã‰º¥Èö®ËëóÈ°ØËëóÁöÑÈôêÂà∂Ôºå‰æãÂ¶ÇÂÆπÊòìÂèóÂà∞Ë≥áÊñôÊ±°ÊüìÔºå‰ª•ÂèäÁº∫‰πèÈÅ©Êáâ LLM ‰∏çÊñ∑ÊºîÈÄ≤ÁöÑËÉΩÂäõ„ÄÇÂõ†Ê≠§ÔºåËø´ÂàáÈúÄË¶ÅËÉΩÂ§†ÈÅ©Êáâ‰∏¶Áî¢ÁîüÂÖ∑ÊúâÂèóÊéßË§áÈõúÊÄßÁöÑË©ï‰º∞Ë≥áÊñôÁöÑË©ï‰º∞ÊñπÊ≥ï„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéËá™ÈÅ©ÊáâÊé®ÁêÜÂúñÂΩ¢ÊºîÂåñ (DARG) ÂºïÂÖ• LLM ÁöÑÂãïÊÖãË©ï‰º∞Ôºå‰ª•ÂãïÊÖãÂª∂‰º∏ÁõÆÂâçÂÖ∑ÊúâÂèóÊéßË§áÈõúÊÄßÂíåÂ§öÊ®£ÊÄßÁöÑÂü∫Ê∫ñ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÊì∑ÂèñÁõÆÂâçÂü∫Ê∫ñ‰∏≠Ë≥áÊñôÈªûÁöÑÊé®ÁêÜÂúñÂΩ¢ÔºåÁÑ∂ÂæåÊìæÂãïÊé®ÁêÜÂúñÂΩ¢‰ª•Áî¢ÁîüÊñ∞ÁöÑÊ∏¨Ë©¶Ë≥áÊñô„ÄÇÈÄô‰∫õÊñ∞Áî¢ÁîüÁöÑÊ∏¨Ë©¶Ê®£Êú¨ÂèØ‰ª•Êúâ‰∏çÂêåÁöÑË§áÈõúÊÄßÂ±§Á¥öÔºåÂêåÊôÇÁ∂≠ÊåÅËàáÂéüÂßãÂü∫Ê∫ñÈ°û‰ººÁöÑË™ûË®ÄÂ§öÊ®£ÊÄß„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®Á®ãÂºèÁ¢ºÂ¢ûÂº∑ÁöÑ LLM ‰æÜÁ¢∫‰øùÊñ∞Áî¢ÁîüË≥áÊñôÁöÑÊ®ôÁ±§Ê≠£Á¢∫ÊÄß„ÄÇÊàëÂÄëÂ∞á DARG Êû∂ÊßãÂ•óÁî®ÊñºÂõõÂÄãÈ†òÂüü‰∏≠ÁöÑÂêÑÁ®ÆÊé®ÁêÜ‰ªªÂãôÔºå‰∏¶‰ΩøÁî® 15 ÂÄãÊúÄÂÖàÈÄ≤ÁöÑ LLM„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÂπæ‰πéÊâÄÊúâ LLM Âú®Ë§áÈõúÊÄßÂ¢ûÂä†ÁöÑÊÉÖÊ≥Å‰∏ãÈÉΩÊúÉÂá∫ÁèæÊïàËÉΩ‰∏ãÈôçÔºåËÄåÊüê‰∫õ LLM ÂâáË°®ÁèæÂá∫È°ØËëóÁöÑ‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁôºÁèæ LLM Âú®ÈÄèÈÅé DARG Áî¢ÁîüÂÖ∑ÊúâËºÉÈ´òË§áÈõúÊÄßÂ±§Á¥öÁöÑË≥áÊñôÈÄ≤Ë°åË©ï‰º∞ÊôÇÔºåÊúÉË°®ÁèæÂá∫Êõ¥Â§öÂÅèÂ∑Æ„ÄÇÈÄô‰∫õËßÄÂØüÁµêÊûúÊèê‰æõ‰∫ÜÊúâÁî®ÁöÑË¶ãËß£ÔºåË™™ÊòéÂ¶Ç‰ΩïÂãïÊÖã‰∏îËá™ÈÅ©ÊáâÂú∞Ë©ï‰º∞ LLM„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/SALT-NLP/DARG ÂèñÂæó„ÄÇ

##### **CogMG: Collaborative Augmentation Between Large Language Model and Knowledge Graph**
2406.17231v1 by Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao

Large language models have become integral to question-answering applications
despite their propensity for generating hallucinations and factually inaccurate
content. Querying knowledge graphs to reduce hallucinations in LLM meets the
challenge of incomplete knowledge coverage in knowledge graphs. On the other
hand, updating knowledge graphs by information extraction and knowledge graph
completion faces the knowledge update misalignment issue. In this work, we
introduce a collaborative augmentation framework, CogMG, leveraging knowledge
graphs to address the limitations of LLMs in QA scenarios, explicitly targeting
the problems of incomplete knowledge coverage and knowledge update
misalignment. The LLMs identify and decompose required knowledge triples that
are not present in the KG, enriching them and aligning updates with real-world
demands. We demonstrate the efficacy of this approach through a supervised
fine-tuned LLM within an agent framework, showing significant improvements in
reducing hallucinations and enhancing factual accuracy in QA responses. Our
code and video are publicly available.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂ∑≤ÊàêÁÇ∫ÂïèÁ≠îÊáâÁî®Á®ãÂºè‰∏≠‰∏çÂèØÊàñÁº∫ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÂÑòÁÆ°ÂÆÉÂÄëÂÇæÂêëÊñºÁî¢ÁîüÂπªË¶∫Âíå‰∫ãÂØ¶‰∏çÊ≠£Á¢∫ÁöÑÂÖßÂÆπ„ÄÇÊü•Ë©¢Áü•Ë≠òÂúñË°®‰ª•Ê∏õÂ∞ë LLM ‰∏≠ÁöÑÂπªË¶∫ÊúÉÈÅáÂà∞Áü•Ë≠òÂúñË°®‰∏≠Áü•Ë≠òË¶ÜËìã‰∏çÂÆåÊï¥ÁöÑÊåëÊà∞„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÈÄöÈÅéË≥áË®äËêÉÂèñÂíåÁü•Ë≠òÂúñË°®ÂÆåÊàê‰æÜÊõ¥Êñ∞Áü•Ë≠òÂúñË°®ÊúÉÈù¢Ëá®Áü•Ë≠òÊõ¥Êñ∞ÈåØ‰ΩçÂïèÈ°å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂçî‰ΩúÊì¥ÂÖÖÊû∂Êßã CogMGÔºåÂà©Áî®Áü•Ë≠òÂúñË°®‰æÜËß£Ê±∫ LLM Âú® QA Â†¥ÊôØ‰∏≠ÁöÑÈôêÂà∂ÔºåÊòéÁ¢∫ÈáùÂ∞ç‰∏çÂÆåÊï¥ÁöÑÁü•Ë≠òË¶ÜËìãÂíåÁü•Ë≠òÊõ¥Êñ∞ÈåØ‰ΩçÂïèÈ°å„ÄÇLLM Ë≠òÂà•‰∏¶ÂàÜËß£ KG ‰∏≠‰∏çÂ≠òÂú®ÁöÑÊâÄÈúÄÁü•Ë≠ò‰∏âÂÖÉÁµÑÔºåË±êÂØåÂÆÉÂÄë‰∏¶Â∞áÊõ¥Êñ∞ËàáÁèæÂØ¶‰∏ñÁïåÁöÑÈúÄÊ±Ç‰øùÊåÅ‰∏ÄËá¥„ÄÇÊàëÂÄëÈÄèÈÅé‰ª£ÁêÜÊû∂Êßã‰∏≠Áõ£Áù£ÂæÆË™øÁöÑ LLM Â±ïÁ§∫‰∫ÜÈÄôÁ®ÆÊñπÊ≥ïÁöÑÂäüÊïàÔºåÈ°ØÁ§∫Âá∫Âú®Ê∏õÂ∞ëÂπªË¶∫ÂíåÂ¢ûÂº∑ QA ÂõûÊáâ‰∏≠ÁöÑ‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄßÊñπÈù¢ÊúâÈ°ØËëóÁöÑÊîπÈÄ≤„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåÂΩ±ÁâáÂÖ¨ÈñãÊèê‰æõ„ÄÇ

##### **Link Prediction with Untrained Message Passing Layers**
2406.16687v1 by Lisi Qarkaxhija, Anatol E. Wegner, Ingo Scholtes

Message passing neural networks (MPNNs) operate on graphs by exchanging
information between neigbouring nodes. MPNNs have been successfully applied to
various node-, edge-, and graph-level tasks in areas like molecular science,
computer vision, natural language processing, and combinatorial optimization.
However, most MPNNs require training on large amounts of labeled data, which
can be costly and time-consuming. In this work, we explore the use of various
untrained message passing layers in graph neural networks, i.e. variants of
popular message passing architecture where we remove all trainable parameters
that are used to transform node features in the message passing step. Focusing
on link prediction, we find that untrained message passing layers can lead to
competitive and even superior performance compared to fully trained MPNNs,
especially in the presence of high-dimensional features. We provide a
theoretical analysis of untrained message passing by relating the inner
products of features implicitly produced by untrained message passing layers to
path-based topological node similarity measures. As such, untrained message
passing architectures can be viewed as a highly efficient and interpretable
approach to link prediction.

ÊëòË¶ÅÔºöË®äÊÅØÂÇ≥ÈÅûÁ•ûÁ∂ìÁ∂≤Ë∑Ø (MPNN) ÈÄèÈÅé‰∫§ÊèõÈÑ∞ËøëÁØÄÈªû‰πãÈñìÁöÑË≥áË®ä‰æÜËôïÁêÜÂúñÂΩ¢„ÄÇMPNN Â∑≤ÊàêÂäüÊáâÁî®ÊñºÂêÑÁ®ÆÁØÄÈªû„ÄÅÈÇäÁ∑£ÂíåÂúñÂΩ¢Â±§Á¥öÁöÑ‰ªªÂãôÔºå‰æãÂ¶ÇÂàÜÂ≠êÁßëÂ≠∏„ÄÅÈõªËÖ¶Ë¶ñË¶∫„ÄÅËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÁµÑÂêàÊúÄ‰Ω≥Âåñ„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏ MPNN ÈúÄË¶ÅÂ§ßÈáèÊ®ôÁ±§Ë≥áÊñôÊâçËÉΩÈÄ≤Ë°åË®ìÁ∑¥ÔºåÈÄôÂèØËÉΩÊúÉÂæàÊòÇË≤¥‰∏îËÄóÊôÇ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂú®ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠‰ΩøÁî®ÂêÑÁ®ÆÊú™Ë®ìÁ∑¥ÁöÑË®äÊÅØÂÇ≥ÈÅûÂ±§Ôºå‰πüÂ∞±ÊòØË™™ÔºåÊàëÂÄëÁßªÈô§‰∫ÜÊâÄÊúâÁî®ÊñºÂú®Ë®äÊÅØÂÇ≥ÈÅûÊ≠•È©ü‰∏≠ËΩâÊèõÁØÄÈªûÁâπÂæµÁöÑÂèØË®ìÁ∑¥ÂèÉÊï∏ÔºåÈÄôÊòØÁÜ±ÈñÄË®äÊÅØÂÇ≥ÈÅûÊû∂ÊßãÁöÑËÆäÈ´î„ÄÇÂ∞àÊ≥®ÊñºÈÄ£ÁµêÈ†êÊ∏¨ÔºåÊàëÂÄëÁôºÁèæÊú™Ë®ìÁ∑¥ÁöÑË®äÊÅØÂÇ≥ÈÅûÂ±§ÂèØ‰ª•Áî¢ÁîüÂÖ∑ÊúâÁ´∂Áà≠ÂäõÔºåÁîöËá≥ÂÑ™ÊñºÂÆåÂÖ®Ë®ìÁ∑¥ÁöÑ MPNN ÁöÑÊïàËÉΩÔºåÂ∞§ÂÖ∂ÊòØÂú®Â≠òÂú®È´òÁ∂≠ÁâπÂæµÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÊú™Ë®ìÁ∑¥ÁöÑË®äÊÅØÂÇ≥ÈÅûÂ±§Èö±Âê´Áî¢ÁîüÁöÑÁâπÂæµÁöÑÂÖßÁ©çËàáÂü∫ÊñºË∑ØÂæëÁöÑÊãìÊí≤ÁØÄÈªûÁõ∏‰ººÂ∫¶Ê∏¨ÈáèÈóúËÅØÔºåÊèê‰æõÊú™Ë®ìÁ∑¥Ë®äÊÅØÂÇ≥ÈÅûÁöÑÁêÜË´ñÂàÜÊûê„ÄÇÂõ†Ê≠§ÔºåÊú™Ë®ìÁ∑¥ÁöÑË®äÊÅØÂÇ≥ÈÅûÊû∂ÊßãÂèØ‰ª•Ë¶ñÁÇ∫‰∏ÄÁ®ÆÈ´òÂ∫¶ÊúâÊïà‰∏îÂèØËß£ÈáãÁöÑÈÄ£ÁµêÈ†êÊ∏¨ÊñπÊ≥ï„ÄÇ

##### **CLEAR: Can Language Models Really Understand Causal Graphs?**
2406.16605v1 by Sirui Chen, Mengying Xu, Kun Wang, Xingyu Zeng, Rui Zhao, Shengjie Zhao, Chaochao Lu

Causal reasoning is a cornerstone of how humans interpret the world. To model
and reason about causality, causal graphs offer a concise yet effective
solution. Given the impressive advancements in language models, a crucial
question arises: can they really understand causal graphs? To this end, we
pioneer an investigation into language models' understanding of causal graphs.
Specifically, we develop a framework to define causal graph understanding, by
assessing language models' behaviors through four practical criteria derived
from diverse disciplines (e.g., philosophy and psychology). We then develop
CLEAR, a novel benchmark that defines three complexity levels and encompasses
20 causal graph-based tasks across these levels. Finally, based on our
framework and benchmark, we conduct extensive experiments on six leading
language models and summarize five empirical findings. Our results indicate
that while language models demonstrate a preliminary understanding of causal
graphs, significant potential for improvement remains. Our project website is
at https://github.com/OpenCausaLab/CLEAR.

ÊëòË¶ÅÔºöÂõ†ÊûúÊé®ÁêÜÊòØ‰∫∫È°ûË©ÆÈáã‰∏ñÁïåÁöÑÂü∫Áü≥„ÄÇÁÇ∫‰∫ÜÂ∞çÂõ†ÊûúÈóú‰øÇÂª∫Ê®°ÂíåÊé®ÁêÜÔºåÂõ†ÊûúÂúñÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁ∞°ÊΩîËÄåÊúâÊïàÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÈëëÊñºË™ûË®ÄÊ®°ÂûãÁöÑÈ©ö‰∫∫ÈÄ≤Ê≠•Ôºå‰∏ÄÂÄãÈóúÈçµÂïèÈ°åÂá∫Áèæ‰∫ÜÔºöÂÆÉÂÄëÁúüÁöÑËÉΩÁêÜËß£Âõ†ÊûúÂúñÂóéÔºüÁÇ∫Ê≠§ÔºåÊàëÂÄëÁéáÂÖàÂ∞çË™ûË®ÄÊ®°ÂûãÂ∞çÂõ†ÊûúÂúñÁöÑÁêÜËß£ÈÄ≤Ë°å‰∫ÜË™øÊü•„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÊ°ÜÊû∂‰æÜÂÆöÁæ©Âõ†ÊûúÂúñÁêÜËß£ÔºåÈÄöÈÅéÂæû‰∏çÂêåÂ≠∏ÁßëÔºà‰æãÂ¶ÇÂì≤Â≠∏ÂíåÂøÉÁêÜÂ≠∏ÔºâË°çÁîüÁöÑÂõõÂÄãÂØ¶Áî®Ê®ôÊ∫ñ‰æÜË©ï‰º∞Ë™ûË®ÄÊ®°ÂûãÁöÑË°åÁÇ∫„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÈñãÁôº‰∫Ü CLEARÔºå‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Ê∫ñÔºåÂÆÉÂÆöÁæ©‰∫Ü‰∏âÂÄãË§áÈõúÊÄßÁ¥öÂà•Ôºå‰∏¶Ê∂µËìã‰∫ÜÈÄô‰∫õÁ¥öÂà•‰∏≠ÁöÑ 20 ÂÄãÂü∫ÊñºÂõ†ÊûúÂúñÁöÑ‰ªªÂãô„ÄÇÊúÄÂæåÔºåÂü∫ÊñºÊàëÂÄëÁöÑÊ°ÜÊû∂ÂíåÂü∫Ê∫ñÔºåÊàëÂÄëÂ∞çÂÖ≠ÂÄãÈ†òÂÖàÁöÑË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰∏¶Á∏ΩÁµê‰∫Ü‰∫îÈ†ÖÂØ¶Ë≠âÁôºÁèæ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂÑòÁÆ°Ë™ûË®ÄÊ®°ÂûãÂ±ïÁ§∫‰∫ÜÂ∞çÂõ†ÊûúÂúñÁöÑÂàùÊ≠•ÁêÜËß£Ôºå‰ΩÜ‰ªçÊúâÂæàÂ§ßÁöÑÊîπÈÄ≤ÊΩõÂäõ„ÄÇÊàëÂÄëÁöÑÈ†ÖÁõÆÁ∂≤Á´ô‰ΩçÊñº https://github.com/OpenCausaLab/CLEAR„ÄÇ

##### **KEHRL: Learning Knowledge-Enhanced Language Representations with Hierarchical Reinforcement Learning**
2406.16374v1 by Dongyang Li, Taolin Zhang, Longtao Huang, Chengyu Wang, Xiaofeng He, Hui Xue

Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation
triples from knowledge graphs (KGs) and integrate these external data sources
into language models via self-supervised learning. Previous works treat
knowledge enhancement as two independent operations, i.e., knowledge injection
and knowledge integration. In this paper, we propose to learn
Knowledge-Enhanced language representations with Hierarchical Reinforcement
Learning (KEHRL), which jointly addresses the problems of detecting positions
for knowledge injection and integrating external knowledge into the model in
order to avoid injecting inaccurate or irrelevant knowledge. Specifically, a
high-level reinforcement learning (RL) agent utilizes both internal and prior
knowledge to iteratively detect essential positions in texts for knowledge
injection, which filters out less meaningful entities to avoid diverting the
knowledge learning direction. Once the entity positions are selected, a
relevant triple filtration module is triggered to perform low-level RL to
dynamically refine the triples associated with polysemic entities through
binary-valued actions. Experiments validate KEHRL's effectiveness in probing
factual knowledge and enhancing the model's performance on various natural
language understanding tasks.

ÊëòË¶ÅÔºöÁü•Ë≠òÂ¢ûÂº∑È†êË®ìÁ∑¥Ë™ûË®ÄÊ®°Âûã (KEPLM) Âà©Áî®Áü•Ë≠òÂúñË≠ú (KG) ‰∏≠ÁöÑÈóúËÅØ‰∏âÂÖÉÁµÑÔºå‰∏¶ÈÄèÈÅéËá™ÊàëÁõ£Áù£ÂºèÂ≠∏ÁøíÂ∞áÈÄô‰∫õÂ§ñÈÉ®Ë≥áÊñô‰æÜÊ∫êÊï¥ÂêàÂà∞Ë™ûË®ÄÊ®°Âûã‰∏≠„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∞áÁü•Ë≠òÂ¢ûÂº∑Ë¶ñÁÇ∫ÂÖ©ÂÄãÁç®Á´ãÁöÑÊìç‰ΩúÔºåÂç≥Áü•Ë≠òÊ≥®ÂÖ•ÂíåÁü•Ë≠òÊï¥Âêà„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂª∫Ë≠∞‰ΩøÁî®ÂàÜÂ±§Âº∑ÂåñÂ≠∏Áøí (KEHRL) Â≠∏ÁøíÁü•Ë≠òÂ¢ûÂº∑Ë™ûË®ÄË°®ÂæµÔºåÈÄôÂÖ±ÂêåËß£Ê±∫‰∫ÜÂÅµÊ∏¨Áü•Ë≠òÊ≥®ÂÖ•‰ΩçÁΩÆÂíåÂ∞áÂ§ñÈÉ®Áü•Ë≠òÊï¥ÂêàÂà∞Ê®°Âûã‰∏≠ÁöÑÂïèÈ°åÔºå‰ª•ÈÅøÂÖçÊ≥®ÂÖ•‰∏çÊ∫ñÁ¢∫Êàñ‰∏çÁõ∏ÈóúÁöÑÁü•Ë≠ò„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÈ´òÈöéÂº∑ÂåñÂ≠∏Áøí (RL) ‰ª£ÁêÜ‰ΩøÁî®ÂÖßÈÉ®ÂíåÂÖàÈ©óÁü•Ë≠òÔºåÂèçË¶ÜÂÅµÊ∏¨ÊñáÂ≠ó‰∏≠Áü•Ë≠òÊ≥®ÂÖ•ÁöÑÈáçË¶Å‰ΩçÁΩÆÔºåÈÄôÊúÉÈÅéÊøæÊéâËºÉ‰∏çÈáçË¶ÅÁöÑÂØ¶È´îÔºå‰ª•ÈÅøÂÖçËΩâÁßªÁü•Ë≠òÂ≠∏ÁøíÊñπÂêë„ÄÇ‰∏ÄÊó¶ÈÅ∏ÂÆöÂØ¶È´î‰ΩçÁΩÆÔºåÂ∞±ÊúÉËß∏ÁôºÁõ∏ÈóúÁöÑ‰∏âÂÖÉÁµÑÈÅéÊøæÊ®°ÁµÑÔºåÈÄèÈÅé‰∫åÈÄ≤Âà∂Âãï‰ΩúÂãïÊÖãÁ≤æÁÖâËàáÂ§öÁæ©ÂØ¶È´îÁõ∏ÈóúÁöÑ‰∏âÂÖÉÁµÑ„ÄÇÂØ¶È©óÈ©óË≠â‰∫Ü KEHRL Âú®Êé¢Êü•‰∫ãÂØ¶Áü•Ë≠òÂíåÂ¢ûÂº∑Ê®°ÂûãÂú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄÁêÜËß£‰ªªÂãô‰∏äÁöÑÊïàËÉΩ„ÄÇ

##### **Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models**
2406.16333v1 by Yichen Sun, Zhixuan Chu, Zhan Qin, Kui Ren

The rapid advancement of Text-to-Image(T2I) generative models has enabled the
synthesis of high-quality images guided by textual descriptions. Despite this
significant progress, these models are often susceptible in generating contents
that contradict the input text, which poses a challenge to their reliability
and practical deployment. To address this problem, we introduce a novel
diffusion-based framework to significantly enhance the alignment of generated
images with their corresponding descriptions, addressing the inconsistency
between visual output and textual input. Our framework is built upon a
comprehensive analysis of inconsistency phenomena, categorizing them based on
their manifestation in the image. Leveraging a state-of-the-art large language
module, we first extract objects and construct a knowledge graph to predict the
locations of these objects in potentially generated images. We then integrate a
state-of-the-art controllable image generation model with a visual text
generation module to generate an image that is consistent with the original
prompt, guided by the predicted object locations. Through extensive experiments
on an advanced multimodal hallucination benchmark, we demonstrate the efficacy
of our approach in accurately generating the images without the inconsistency
with the original prompt. The code can be accessed via
https://github.com/TruthAI-Lab/PCIG.

ÊëòË¶ÅÔºöÊñáÊú¨Âà∞ÂõæÂÉè (T2I) ÁîüÊàêÊ®°ÂûãÁöÑÂø´ÈÄüËøõÊ≠•‰ΩøÂæóÂêàÊàêÁî±ÊñáÊú¨ÊèèËø∞ÂºïÂØºÁöÑÈ´òË¥®ÈáèÂõæÂÉèÊàê‰∏∫ÂèØËÉΩ„ÄÇÂ∞ΩÁÆ°ÂèñÂæó‰∫ÜËøô‰∫õÈáçÂ§ßËøõÂ±ïÔºå‰ΩÜËøô‰∫õÊ®°ÂûãÂú®ÁîüÊàê‰∏éËæìÂÖ•ÊñáÊú¨Áõ∏ÁüõÁõæÁöÑÂÜÖÂÆπÊñπÈù¢ÈÄöÂ∏∏ÂæàÊïèÊÑüÔºåËøôÂØπÂÆÉ‰ª¨ÁöÑÂèØÈù†ÊÄßÂíåÂÆûÈôÖÈÉ®ÁΩ≤ÊèêÂá∫‰∫ÜÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂü∫‰∫éÊâ©Êï£ÁöÑÊ°ÜÊû∂Ôºå‰ª•ÊòæÁùÄÂ¢ûÂº∫ÁîüÊàêÂõæÂÉè‰∏éÂÖ∂Áõ∏Â∫îÊèèËø∞ÁöÑ‰∏ÄËá¥ÊÄßÔºåËß£ÂÜ≥ËßÜËßâËæìÂá∫ÂíåÊñáÊú¨ËæìÂÖ•‰πãÈó¥ÁöÑ‰∏ç‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âª∫Á´ãÂú®ÂØπ‰∏ç‰∏ÄËá¥Áé∞Ë±°ÁöÑÂÖ®Èù¢ÂàÜÊûê‰πã‰∏äÔºåÊ†πÊçÆÂÆÉ‰ª¨Âú®ÂõæÂÉè‰∏≠ÁöÑË°®Áé∞ÂØπÂÆÉ‰ª¨ËøõË°åÂàÜÁ±ª„ÄÇÂà©Áî®ÊúÄÂÖàËøõÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂùóÔºåÊàë‰ª¨È¶ñÂÖàÊèêÂèñÂØπË±°Âπ∂ÊûÑÂª∫Áü•ËØÜÂõæË∞±Êù•È¢ÑÊµãËøô‰∫õÂØπË±°Âú®ÊΩúÂú®ÁîüÊàêÁöÑÂõæÂÉè‰∏≠ÁöÑ‰ΩçÁΩÆ„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨Â∞ÜÊúÄÂÖàËøõÁöÑÂèØÊéßÂõæÂÉèÁîüÊàêÊ®°Âûã‰∏éËßÜËßâÊñáÊú¨ÁîüÊàêÊ®°ÂùóÈõÜÊàêÂú®‰∏ÄËµ∑Ôºå‰ª•ÁîüÊàê‰∏éÂéüÂßãÊèêÁ§∫‰∏ÄËá¥ÁöÑÂõæÂÉèÔºåÂπ∂Áî±È¢ÑÊµãÁöÑÂØπË±°‰ΩçÁΩÆÂºïÂØº„ÄÇÈÄöËøáÂú®È´òÁ∫ßÂ§öÊ®°ÊÄÅÂπªËßâÂü∫ÂáÜ‰∏äËøõË°åÂπøÊ≥õÁöÑÂÆûÈ™åÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÂáÜÁ°ÆÁîüÊàêÂõæÂÉèÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåËÄå‰∏ç‰ºö‰∏éÂéüÂßãÊèêÁ§∫‰∏ç‰∏ÄËá¥„ÄÇÂèØ‰ª•ÈÄöËøá https://github.com/TruthAI-Lab/PCIG ËÆøÈóÆ‰ª£Á†Å„ÄÇ

##### **Graph-Augmented LLMs for Personalized Health Insights: A Case Study in Sleep Analysis**
2406.16252v2 by Ajan Subramanian, Zhongqi Yang, Iman Azimi, Amir M. Rahmani

Health monitoring systems have revolutionized modern healthcare by enabling
the continuous capture of physiological and behavioral data, essential for
preventive measures and early health intervention. While integrating this data
with Large Language Models (LLMs) has shown promise in delivering interactive
health advice, traditional methods like Retrieval-Augmented Generation (RAG)
and fine-tuning often fail to fully utilize the complex, multi-dimensional, and
temporally relevant data from wearable devices. These conventional approaches
typically provide limited actionable and personalized health insights due to
their inadequate capacity to dynamically integrate and interpret diverse health
data streams. In response, this paper introduces a graph-augmented LLM
framework designed to significantly enhance the personalization and clarity of
health insights. Utilizing a hierarchical graph structure, the framework
captures inter and intra-patient relationships, enriching LLM prompts with
dynamic feature importance scores derived from a Random Forest Model. The
effectiveness of this approach is demonstrated through a sleep analysis case
study involving 20 college students during the COVID-19 lockdown, highlighting
the potential of our model to generate actionable and personalized health
insights efficiently. We leverage another LLM to evaluate the insights for
relevance, comprehensiveness, actionability, and personalization, addressing
the critical need for models that process and interpret complex health data
effectively. Our findings show that augmenting prompts with our framework
yields significant improvements in all 4 criteria. Through our framework, we
can elicit well-crafted, more thoughtful responses tailored to a specific
patient.

ÊëòË¶ÅÔºöÂÅ•Â∫∑Áõ£ÊéßÁ≥ªÁµ±ÈÄèÈÅéÊåÅÁ∫åÊî∂ÈõÜÁîüÁêÜÂíåË°åÁÇ∫Ë≥áÊñôÔºåÂæπÂ∫ïÊîπËÆä‰∫ÜÁèæ‰ª£ÈÜ´ÁôÇ‰øùÂÅ•ÔºåÈÄô‰∫õË≥áÊñôÂ∞çÊñºÈ†êÈò≤Êé™ÊñΩÂíåÊó©ÊúüÂÅ•Â∫∑Âπ≤È†êËá≥ÈóúÈáçË¶Å„ÄÇÈõñÁÑ∂Â∞áÈÄô‰∫õË≥áÊñôËàáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êï¥ÂêàÔºåÂ∑≤Â±ïÁèæÂá∫Êèê‰æõ‰∫íÂãïÂºèÂÅ•Â∫∑Âª∫Ë≠∞ÁöÑÊΩõÂäõÔºå‰ΩÜÂÇ≥Áµ±ÊñπÊ≥ïÔºà‰æãÂ¶ÇÊ™¢Á¥¢Êì¥ÂÖÖÁîüÊàê (RAG) ÂíåÂæÆË™øÔºâÈÄöÂ∏∏ÁÑ°Ê≥ïÂÖÖÂàÜÂà©Áî®Á©øÊà¥ÂºèË£ùÁΩÆ‰∏≠Ë§áÈõú„ÄÅÂ§öÈù¢Âêë‰∏îËàáÊôÇÈñìÁõ∏ÈóúÁöÑË≥áÊñô„ÄÇÈÄô‰∫õÂÇ≥Áµ±ÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÊèê‰æõÊúâÈôêÁöÑÂèØË°å‰∏îÂÄã‰∫∫ÂåñÁöÑÂÅ•Â∫∑Ë¶ãËß£ÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁÑ°Ê≥ïÂãïÊÖãÊï¥ÂêàÂíåË©ÆÈáã‰∏çÂêåÁöÑÂÅ•Â∫∑Ë≥áÊñô‰∏≤ÊµÅ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂúñÂΩ¢Êì¥ÂÖÖ LLM Êû∂ÊßãÔºåÊó®Âú®Â§ßÂπÖÊèêÂçáÂÅ•Â∫∑Ë¶ãËß£ÁöÑÂÄã‰∫∫ÂåñÂíåÊ∏ÖÊô∞Â∫¶„ÄÇÈÄôÂÄãÊû∂ÊßãÂà©Áî®ÈöéÂ±§ÂºèÂúñÂΩ¢ÁµêÊßãÔºåÊì∑ÂèñÊÇ£ËÄÖ‰πãÈñìÂíåÊÇ£ËÄÖÂÖßÈÉ®ÁöÑÈóú‰øÇÔºå‰∏¶‰ΩøÁî®Âæû Random Forest Ê®°ÂûãË°çÁîüÁöÑÂãïÊÖãÁâπÂæµÈáçË¶ÅÊÄßË©ïÂàÜÔºåË±êÂØå LLM ÊèêÁ§∫„ÄÇÈÄèÈÅé‰∏ÄÈ†ÖÁù°Áú†ÂàÜÊûêÊ°à‰æãÁ†îÁ©∂ÔºàÂú® COVID-19 Â∞ÅÈéñÊúüÈñìÈáùÂ∞ç 20 ÂêçÂ§ßÂ≠∏ÁîüÈÄ≤Ë°åÔºâË≠âÊòé‰∫ÜÈÄôÂÄãÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÁ™ÅÈ°Ø‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊúâÊïàÁî¢ÁîüÂèØË°å‰∏îÂÄã‰∫∫ÂåñÁöÑÂÅ•Â∫∑Ë¶ãËß£ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÂà©Áî®Âè¶‰∏ÄÂÄã LLM Ë©ï‰º∞Ë¶ãËß£ÁöÑÁõ∏ÈóúÊÄß„ÄÅÂÖ®Èù¢ÊÄß„ÄÅÂèØË°åÊÄßÂíåÂÄã‰∫∫ÂåñÔºåÊªøË∂≥‰∫ÜÊ®°ÂûãÊúâÊïàËôïÁêÜÂíåË©ÆÈáãË§áÈõúÂÅ•Â∫∑Ë≥áÊñôÁöÑÈóúÈçµÈúÄÊ±Ç„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫Ôºå‰ΩøÁî®ÊàëÂÄëÁöÑÊû∂ÊßãÊì¥ÂÖÖÊèêÁ§∫ÔºåÂèØ‰ª•Âú®ÊâÄÊúâ 4 ÂÄãÊ®ôÊ∫ñ‰∏≠Â§ßÂπÖÊîπÂñÑ„ÄÇÈÄèÈÅéÊàëÂÄëÁöÑÊû∂ÊßãÔºåÊàëÂÄëÂèØ‰ª•ÂºïÁôºÁ≤æÂøÉË®≠Ë®à„ÄÅÊõ¥Âë®ÂÖ®ÁöÑÂõûÊáâÔºåÈáùÂ∞çÁâπÂÆöÊÇ£ËÄÖÈáèË∫´ÊâìÈÄ†„ÄÇ

##### **GraphEval2000: Benchmarking and Improving Large Language Models on Graph Datasets**
2406.16176v1 by Qiming Wu, Zichen Chen, Will Corcoran, Misha Sra, Ambuj K. Singh

Large language models (LLMs) have achieved remarkable success in natural
language processing (NLP), demonstrating significant capabilities in processing
and understanding text data. However, recent studies have identified
limitations in LLMs' ability to reason about graph-structured data. To address
this gap, we introduce GraphEval2000, the first comprehensive graph dataset,
comprising 40 graph data structure problems along with 2000 test cases.
Additionally, we introduce an evaluation framework based on GraphEval2000,
designed to assess the graph reasoning abilities of LLMs through coding
challenges. Our dataset categorizes test cases into four primary and four
sub-categories, ensuring a comprehensive evaluation. We evaluate eight popular
LLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of
directed graphs compared to undirected ones. While private LLMs consistently
outperform open-source models, the performance gap is narrowing. Furthermore,
to improve the usability of our evaluation framework, we propose Structured
Symbolic Decomposition (SSD), an instruction-based method designed to enhance
LLM performance on GraphEval2000. Results show that SSD improves the
performance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an
increase of 11.11\%, 33.37\%, and 33.37\%, respectively.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰∏≠ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊàêÂäüÔºåÂú®ËôïÁêÜÂíåÁêÜËß£ÊñáÊú¨Êï∏ÊìöÊñπÈù¢Ë°®ÁèæÂá∫È°ØËëóÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÊúÄËøëÁöÑÁ†îÁ©∂ÁôºÁèæ LLM Âú®Êé®ÁêÜÂúñÂΩ¢ÁµêÊßãÊï∏ÊìöÁöÑËÉΩÂäõÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü GraphEval2000ÔºåÁ¨¨‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂúñÂΩ¢Êï∏ÊìöÈõÜÔºåÂåÖÂê´ 40 ÂÄãÂúñÂΩ¢Êï∏ÊìöÁµêÊßãÂïèÈ°å‰ª•Âèä 2000 ÂÄãÊ∏¨Ë©¶Áî®‰æã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÂºïÂÖ•‰∫ÜÂü∫Êñº GraphEval2000 ÁöÑË©ï‰º∞Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöÈÅéÁ∑®Á¢ºÊåëÊà∞Ë©ï‰º∞ LLM ÁöÑÂúñÂΩ¢Êé®ÁêÜËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÊï∏ÊìöÈõÜÂ∞áÊ∏¨Ë©¶Áî®‰æãÂàÜÁÇ∫ÂõõÂÄã‰∏ªË¶ÅÈ°ûÂà•ÂíåÂõõÂÄãÂ≠êÈ°ûÂà•ÔºåÁ¢∫‰øùÈÄ≤Ë°åÂÖ®Èù¢ÁöÑË©ï‰º∞„ÄÇÊàëÂÄëÂú® GraphEval2000 ‰∏äË©ï‰º∞‰∫ÜÂÖ´ÂÄãÊµÅË°åÁöÑ LLMÔºåÁµêÊûúË°®ÊòéÔºåËàáÁÑ°ÂêëÂúñÁõ∏ÊØîÔºåLLM Â∞çÊúâÂêëÂúñÁöÑÁêÜËß£Êõ¥Â•Ω„ÄÇÈõñÁÑ∂ÁßÅÊúâ LLM ÊåÅÁ∫åÂÑ™ÊñºÈñãÊ∫êÊ®°ÂûãÔºå‰ΩÜÊÄßËÉΩÂ∑ÆË∑ùÊ≠£Âú®Á∏ÆÂ∞è„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜÊèêÈ´òÊàëÂÄëË©ï‰º∞Ê°ÜÊû∂ÁöÑÂèØÁî®ÊÄßÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁµêÊßãÂåñÁ¨¶ËôüÂàÜËß£ (SSD)Ôºå‰∏ÄÁ®ÆÂü∫ÊñºÊåá‰ª§ÁöÑÊñπÊ≥ïÔºåÊó®Âú®Â¢ûÂº∑ LLM Âú® GraphEval2000 ‰∏äÁöÑÊÄßËÉΩ„ÄÇÁµêÊûúË°®ÊòéÔºåSSD ÂàÜÂà•ÊèêÈ´ò‰∫Ü GPT-3.5„ÄÅGPT-4 Âíå GPT-4o Âú®Ë§áÈõúÂúñÂΩ¢ÂïèÈ°å‰∏äÁöÑÊÄßËÉΩÔºåÂàÜÂà•Â¢ûÂä†‰∫Ü 11.11%„ÄÅ33.37% Âíå 33.37%„ÄÇ

##### **Can LLM Graph Reasoning Generalize beyond Pattern Memorization?**
2406.15992v1 by Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xiaochuang Han, Tianxing He, Yulia Tsvetkov

Large language models (LLMs) demonstrate great potential for problems with
implicit graphical structures, while recent works seek to enhance the graph
reasoning capabilities of LLMs through specialized instruction tuning. The
resulting 'graph LLMs' are evaluated with in-distribution settings only, thus
it remains underexplored whether LLMs are learning generalizable graph
reasoning skills or merely memorizing patterns in the synthetic training data.
To this end, we propose the NLGift benchmark, an evaluation suite of LLM graph
reasoning generalization: whether LLMs could go beyond semantic, numeric,
structural, reasoning patterns in the synthetic training data and improve
utility on real-world graph-based tasks. Extensive experiments with two LLMs
across four graph reasoning tasks demonstrate that while generalization on
simple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to
generalize across reasoning and real-world patterns, casting doubt on the
benefit of synthetic graph tuning for real-world tasks with underlying network
structures. We explore three strategies to improve LLM graph reasoning
generalization, and we find that while post-training alignment is most
promising for real-world tasks, empowering LLM graph reasoning to go beyond
pattern memorization remains an open research question.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞çÊñºÂÖ∑ÊúâÈö±ÂºèÂúñÂΩ¢ÁµêÊßãÁöÑÂïèÈ°åÂ±ïÁèæÂá∫Â∑®Â§ßÁöÑÊΩõÂäõÔºåËÄåËøëÊúüÁ†îÁ©∂ÂâáÈÄèÈÅéÂ∞àÊ•≠Êåá‰ª§Ë™øÊï¥‰æÜÂ¢ûÂº∑ LLM ÁöÑÂúñÂΩ¢Êé®ÁêÜËÉΩÂäõ„ÄÇÁî±Ê≠§Áî¢ÁîüÁöÑ„ÄåÂúñÂΩ¢ LLM„ÄçÂÉÖÂú®ÂàÜÂ∏ÉÂÖßË®≠ÂÆö‰∏≠ÈÄ≤Ë°åË©ï‰º∞ÔºåÂõ†Ê≠§ LLM ÊòØÂê¶Â≠∏ÁøíÂà∞ÂèØÊ¶ÇÊã¨ÁöÑÂúñÂΩ¢Êé®ÁêÜÊäÄËÉΩÔºåÊàñÂÉÖÂÉÖË®òÊÜ∂ÂêàÊàêË®ìÁ∑¥Ë≥áÊñô‰∏≠ÁöÑÊ®°ÂºèÔºå‰ªçÊú™Áç≤ÂæóÂÖÖÂàÜÊé¢Ë®é„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫ NLGift Âü∫Ê∫ñÔºåÈÄôÊòØ‰∏ÄÂÄã LLM ÂúñÂΩ¢Êé®ÁêÜÊ¶ÇÊã¨Ë©ï‰º∞Â•ó‰ª∂ÔºöLLM ÊòØÂê¶ÂèØ‰ª•Ë∂ÖË∂äÂêàÊàêË®ìÁ∑¥Ë≥áÊñô‰∏≠ÁöÑË™ûÁæ©„ÄÅÊï∏ÂÄº„ÄÅÁµêÊßãÊé®ÁêÜÊ®°ÂºèÔºå‰∏¶ÊèêÂçáÂú®ÁúüÂØ¶‰∏ñÁïåÂü∫ÊñºÂúñÂΩ¢ÁöÑ‰ªªÂãô‰∏≠ÁöÑÊïàÁî®„ÄÇÈÄèÈÅéÂÖ©ÂÄã LLM Âú®ÂõõÂÄãÂúñÂΩ¢Êé®ÁêÜ‰ªªÂãô‰∏≠ÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåÂÑòÁÆ°Âú®Á∞°ÂñÆÊ®°ÂºèÔºàË™ûÁæ©„ÄÅÊï∏ÂÄºÔºâ‰∏äÁöÑÊ¶ÇÊã¨‰ª§‰∫∫ÊªøÊÑèÔºå‰ΩÜ LLM Èõ£‰ª•Âú®Êé®ÁêÜÂíåÁúüÂØ¶‰∏ñÁïåÊ®°Âºè‰∏≠Ê¶ÇÊã¨ÔºåÂ∞çÂêàÊàêÂúñÂΩ¢Ë™øÊï¥Â∞çÊñºÂÖ∑ÊúâÂü∫Á§éÁ∂≤Ë∑ØÁµêÊßãÁöÑÁúüÂØ¶‰∏ñÁïå‰ªªÂãôÁöÑÁõäËôïÊèêÂá∫Ë≥™Áñë„ÄÇÊàëÂÄëÊé¢Ë®é‰∫Ü‰∏âÁ®ÆÁ≠ñÁï•‰æÜÊîπÂñÑ LLM ÂúñÂΩ¢Êé®ÁêÜÊ¶ÇÊã¨ÔºåÊàëÂÄëÁôºÁèæÔºåÂÑòÁÆ°Ë®ìÁ∑¥ÂæåÂ∞çÈΩäÂ∞çÁúüÂØ¶‰∏ñÁïå‰ªªÂãôÊúÄÊúâÂ∏åÊúõÔºå‰ΩÜË≥¶ËÉΩ LLM ÂúñÂΩ¢Êé®ÁêÜ‰ª•Ë∂ÖË∂äÊ®°ÂºèË®òÊÜ∂‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈñãÊîæÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇ

##### **LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning**
2406.15859v1 by Guangsi Shi, Xiaofeng Deng, Linhao Luo, Lijuan Xia, Lei Bao, Bei Ye, Fei Du, Shirui Pan, Yuxiao Li

Recommender systems are pivotal in enhancing user experiences across various
web applications by analyzing the complicated relationships between users and
items. Knowledge graphs(KGs) have been widely used to enhance the performance
of recommender systems. However, KGs are known to be noisy and incomplete,
which are hard to provide reliable explanations for recommendation results. An
explainable recommender system is crucial for the product development and
subsequent decision-making. To address these challenges, we introduce a novel
recommender that synergies Large Language Models (LLMs) and KGs to enhance the
recommendation and provide interpretable results. Specifically, we first
harness the power of LLMs to augment KG reconstruction. LLMs comprehend and
decompose user reviews into new triples that are added into KG. In this way, we
can enrich KGs with explainable paths that express user preferences. To enhance
the recommendation on augmented KGs, we introduce a novel subgraph reasoning
module that effectively measures the importance of nodes and discovers
reasoning for recommendation. Finally, these reasoning paths are fed into the
LLMs to generate interpretable explanations of the recommendation results. Our
approach significantly enhances both the effectiveness and interpretability of
recommender systems, especially in cross-selling scenarios where traditional
methods falter. The effectiveness of our approach has been rigorously tested on
four open real-world datasets, with our methods demonstrating a superior
performance over contemporary state-of-the-art techniques by an average
improvement of 12%. The application of our model in a multinational engineering
and technology company cross-selling recommendation system further underscores
its practical utility and potential to redefine recommendation practices
through improved accuracy and user trust.

ÊëòË¶ÅÔºöÊé®Ëñ¶Á≥ªÁµ±Âú®ÂàÜÊûê‰ΩøÁî®ËÄÖËàáÈ†ÖÁõÆ‰πãÈñìÁöÑË§áÈõúÈóú‰øÇÔºåÊèêÂçáÂêÑÁ®ÆÁ∂≤Ë∑ØÊáâÁî®Á®ãÂºèÁöÑ‰ΩøÁî®ËÄÖÈ´îÈ©óÊñπÈù¢Ëá≥ÈóúÈáçË¶Å„ÄÇÁü•Ë≠òÂúñË≠ú (KG) Â∑≤Âª£Ê≥õÁî®ÊñºÊèêÂçáÊé®Ëñ¶Á≥ªÁµ±ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåKG Â∑≤Áü•ÊúâÈõúË®ä‰∏î‰∏çÂÆåÊï¥ÔºåÈõ£‰ª•Êèê‰æõÂèØÈù†ÁöÑÊé®Ëñ¶ÁµêÊûúË™™Êòé„ÄÇÂèØËß£ÈáãÁöÑÊé®Ëñ¶Á≥ªÁµ±Â∞çÊñºÁî¢ÂìÅÈñãÁôºÂíåÂæåÁ∫åÊ±∫Á≠ñÂà∂ÂÆöËá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÈÄ≤‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊé®Ëñ¶Á≥ªÁµ±ÔºåÁµêÂêàÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âíå KGÔºå‰ª•Â¢ûÂº∑Êé®Ëñ¶‰∏¶Êèê‰æõÂèØËß£ÈáãÁöÑÁµêÊûú„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÂà©Áî® LLM ÁöÑÂäõÈáè‰æÜÊì¥ÂÖÖ KG ÈáçÂª∫„ÄÇLLM ÁêÜËß£‰∏¶Â∞á‰ΩøÁî®ËÄÖË©ïË´ñÂàÜËß£ÊàêÊñ∞ÁöÑ‰∏âÂÖÉÁµÑÔºå‰∏¶Â∞áÂÖ∂Êñ∞Â¢ûÂà∞ KG ‰∏≠„ÄÇÈÄèÈÅéÈÄôÁ®ÆÊñπÂºèÔºåÊàëÂÄëÂèØ‰ª•Áî®Ë°®ÈÅî‰ΩøÁî®ËÄÖÂÅèÂ•ΩÁöÑÂèØËß£ÈáãË∑ØÂæë‰æÜË±êÂØå KG„ÄÇÁÇ∫‰∫ÜÂ¢ûÂº∑Â∞çÊì¥ÂÖÖ KG ÁöÑÊé®Ëñ¶ÔºåÊàëÂÄëÂºïÈÄ≤‰∏ÄÂÄãÂâµÊñ∞ÁöÑÂ≠êÂúñÊé®ÁêÜÊ®°ÁµÑÔºåË©≤Ê®°ÁµÑÊúâÊïàÂú∞Ë°°ÈáèÁØÄÈªûÁöÑÈáçË¶ÅÊÄßÔºå‰∏¶ÊâæÂá∫Êé®Ëñ¶ÁöÑÁêÜÁî±„ÄÇÊúÄÂæåÔºåÈÄô‰∫õÊé®ÁêÜË∑ØÂæëÊúÉËº∏ÂÖ•Âà∞ LLM ‰∏≠Ôºå‰ª•Áî¢ÁîüÊé®Ëñ¶ÁµêÊûúÁöÑÂèØËß£ÈáãË™™Êòé„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈ°ØËëóÊèêÂçá‰∫ÜÊé®Ëñ¶Á≥ªÁµ±ÁöÑÊúâÊïàÊÄßÂíåÂèØËß£ÈáãÊÄßÔºåÁâπÂà•ÊòØÂú®ÂÇ≥Áµ±ÊñπÊ≥ïÂ§±ÊïàÁöÑ‰∫§ÂèâÈä∑ÂîÆÊÉÖÊ≥Å‰∏≠„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÁöÑÊúâÊïàÊÄßÂ∑≤Âú®ÂõõÂÄãÈñãÊîæÁöÑÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁ∂ìÈÅéÂö¥Ê†ºÊ∏¨Ë©¶ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ±ïÁ§∫Âá∫ÊØîÁï∂‰ª£ÊúÄÂÖàÈÄ≤ÊäÄË°ìÈ´òÂá∫Âπ≥Âùá 12% ÁöÑÂçìË∂äÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú®‰∏ÄÂÆ∂Ë∑®ÂúãÂ∑•Á®ãÂíåÊäÄË°ìÂÖ¨Âè∏‰∫§ÂèâÈä∑ÂîÆÊé®Ëñ¶Á≥ªÁµ±‰∏≠ÁöÑÊáâÁî®ÈÄ≤‰∏ÄÊ≠•Á™ÅÈ°Ø‰∫ÜÂÖ∂ÂØ¶Áî®ÊÄßÂíåÈÄèÈÅéÊèêÂçáÊ∫ñÁ¢∫Â∫¶Âíå‰ΩøÁî®ËÄÖ‰ø°‰ªª‰æÜÈáçÊñ∞ÂÆöÁæ©Êé®Ëñ¶ÂØ¶ÂãôÁöÑÊΩõÂäõ„ÄÇ

##### **Large Language Models for Link Stealing Attacks Against Graph Neural Networks**
2406.16963v1 by Faqian Guan, Tianqing Zhu, Hui Sun, Wanlei Zhou, Philip S. Yu

Graph data contains rich node features and unique edge information, which
have been applied across various domains, such as citation networks or
recommendation systems. Graph Neural Networks (GNNs) are specialized for
handling such data and have shown impressive performance in many applications.
However, GNNs may contain of sensitive information and susceptible to privacy
attacks. For example, link stealing is a type of attack in which attackers
infer whether two nodes are linked or not. Previous link stealing attacks
primarily relied on posterior probabilities from the target GNN model,
neglecting the significance of node features. Additionally, variations in node
classes across different datasets lead to different dimensions of posterior
probabilities. The handling of these varying data dimensions posed a challenge
in using a single model to effectively conduct link stealing attacks on
different datasets. To address these challenges, we introduce Large Language
Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively
integrate textual features and exhibit strong generalizability, enabling
attacks to handle diverse data dimensions across various datasets. We design
two distinct LLM prompts to effectively combine textual features and posterior
probabilities of graph nodes. Through these designed prompts, we fine-tune the
LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the
LLM using multiple datasets and enable the LLM to learn features from different
datasets simultaneously. Experimental results show that our approach
significantly enhances the performance of existing link stealing attack tasks
in both white-box and black-box scenarios. Our method can execute link stealing
attacks across different datasets using only a single model, making link
stealing attacks more applicable to real-world scenarios.

ÊëòË¶ÅÔºöÂúñÂΩ¢Êï∏ÊìöÂåÖÂê´Ë±êÂØåÁöÑÁØÄÈªûÁâπÂæµÂíåÁç®ÁâπÁöÑÈÇäÁ∑£Ë≥áË®äÔºåÂ∑≤ÊáâÁî®ÊñºÂêÑÁ®ÆÈ†òÂüüÔºå‰æãÂ¶ÇÂºïÊñáÁ∂≤Ë∑ØÊàñÊé®Ëñ¶Á≥ªÁµ±„ÄÇÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∞àÈñÄÁî®ÊñºËôïÁêÜÊ≠§È°ûÊï∏ÊìöÔºå‰∏¶Âú®Ë®±Â§öÊáâÁî®‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåGNN ÂèØËÉΩÂåÖÂê´ÊïèÊÑüË≥áË®äÔºå‰∏îÂÆπÊòìÂèóÂà∞Èö±ÁßÅÊîªÊìä„ÄÇ‰æãÂ¶ÇÔºåÈÄ£ÁµêÁ´äÂèñÊòØ‰∏ÄÁ®ÆÊîªÊìäÔºåÊîªÊìäËÄÖÊé®Êñ∑ÂÖ©ÂÄãÁØÄÈªûÊòØÂê¶ÈÄ£Áµê„ÄÇÂÖàÂâçÁöÑÈÄ£ÁµêÁ´äÂèñÊîªÊìä‰∏ªË¶Å‰æùË≥¥ÊñºÁõÆÊ®ô GNN Ê®°ÂûãÁöÑÂæåÈ©óÊ©üÁéáÔºåÂøΩÁï•ÁØÄÈªûÁâπÂæµÁöÑÈáçË¶ÅÊÄß„ÄÇÊ≠§Â§ñÔºå‰∏çÂêåË≥áÊñôÈõÜ‰∏≠ÁöÑÁØÄÈªûÈ°ûÂà•ËÆäÂåñÂ∞éËá¥ÂæåÈ©óÊ©üÁéáÁöÑ‰∏çÂêåÁ∂≠Â∫¶„ÄÇËôïÁêÜÈÄô‰∫õ‰∏çÂêåÁöÑË≥áÊñôÁ∂≠Â∫¶Âú®‰ΩøÁî®ÂñÆ‰∏ÄÊ®°ÂûãÂ∞ç‰∏çÂêåË≥áÊñôÈõÜÂü∑Ë°åÈÄ£ÁµêÁ´äÂèñÊîªÊìäÊôÇÊßãÊàê‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂ∞ç GNN Âü∑Ë°åÈÄ£ÁµêÁ´äÂèñÊîªÊìä„ÄÇLLM ÂèØ‰ª•ÊúâÊïàÊï¥ÂêàÊñáÂ≠óÁâπÂæµ‰∏¶Â±ïÁèæÂº∑Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÔºå‰ΩøÊîªÊìäËÉΩÂ§†ËôïÁêÜ‰∏çÂêåË≥áÊñôÈõÜ‰∏≠ÁöÑ‰∏çÂêåË≥áÊñôÁ∂≠Â∫¶„ÄÇÊàëÂÄëË®≠Ë®à‰∫ÜÂÖ©ÂÄã‰∏çÂêåÁöÑ LLM ÊèêÁ§∫Ôºå‰ª•ÊúâÊïàÁµêÂêàÊñáÂ≠óÁâπÂæµÂíåÂúñÂΩ¢ÁØÄÈªûÁöÑÂæåÈ©óÊ©üÁéá„ÄÇÈÄèÈÅéÈÄô‰∫õË®≠Ë®àÁöÑÊèêÁ§∫ÔºåÊàëÂÄëÂæÆË™ø LLM ‰ª•ÈÅ©ÊáâÈÄ£ÁµêÁ´äÂèñÊîªÊìä‰ªªÂãô„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰ΩøÁî®Â§öÂÄãË≥áÊñôÈõÜÂæÆË™ø LLMÔºå‰∏¶‰Ωø LLM ËÉΩÂ§†ÂêåÊôÇÂæû‰∏çÂêåÁöÑË≥áÊñôÈõÜ‰∏≠Â≠∏ÁøíÁâπÂæµ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈ°ØËëóÊèêÂçá‰∫ÜÁèæÊúâÈÄ£ÁµêÁ´äÂèñÊîªÊìä‰ªªÂãôÂú®ÁôΩÁõíÂíåÈªëÁõíÂ†¥ÊôØ‰∏≠ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂÉÖ‰ΩøÁî®ÂñÆ‰∏ÄÊ®°ÂûãÂ∞±ËÉΩË∑®‰∏çÂêåË≥áÊñôÈõÜÂü∑Ë°åÈÄ£ÁµêÁ´äÂèñÊîªÊìäÔºå‰ΩøÈÄ£ÁµêÁ´äÂèñÊîªÊìäÊõ¥ÈÅ©Áî®ÊñºÂØ¶ÈöõÂ†¥ÊôØ„ÄÇ

##### **Inferring Pluggable Types with Machine Learning**
2406.15676v1 by Kazi Amanul Islam Siddiqui, Martin Kellogg

Pluggable type systems allow programmers to extend the type system of a
programming language to enforce semantic properties defined by the programmer.
Pluggable type systems are difficult to deploy in legacy codebases because they
require programmers to write type annotations manually. This paper investigates
how to use machine learning to infer type qualifiers automatically. We propose
a novel representation, NaP-AST, that encodes minimal dataflow hints for the
effective inference of type qualifiers. We evaluate several model architectures
for inferring type qualifiers, including Graph Transformer Network, Graph
Convolutional Network and Large Language Model. We further validated these
models by applying them to 12 open-source programs from a prior evaluation of
the NullAway pluggable typechecker, lowering warnings in all but one
unannotated project. We discovered that GTN shows the best performance, with a
recall of .89 and precision of 0.6. Furthermore, we conduct a study to estimate
the number of Java classes needed for good performance of the trained model.
For our feasibility study, performance improved around 16k classes, and
deteriorated due to overfitting around 22k classes.

ÊëòË¶ÅÔºöÂèØÊèíÊãîÁ±ªÂûãÁ≥ªÁªüÂÖÅËÆ∏Á®ãÂ∫èÂëòÊâ©Â±ïÁºñÁ®ãËØ≠Ë®ÄÁöÑÁ±ªÂûãÁ≥ªÁªüÔºå‰ª•ÊâßË°åÁ®ãÂ∫èÂëòÂÆö‰πâÁöÑËØ≠‰πâÂ±ûÊÄß„ÄÇÂèØÊèíÊãîÁ±ªÂûãÁ≥ªÁªüÈöæ‰ª•ÈÉ®ÁΩ≤Âú®ÈÅóÁïô‰ª£Á†ÅÂ∫ì‰∏≠ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Ë¶ÅÊ±ÇÁ®ãÂ∫èÂëòÊâãÂä®ÁºñÂÜôÁ±ªÂûãÊ≥®Èáä„ÄÇÊú¨ÊñáÁ†îÁ©∂Â¶Ç‰Ωï‰ΩøÁî®Êú∫Âô®Â≠¶‰π†Ëá™Âä®Êé®Êñ≠Á±ªÂûãÈôêÂÆöÁ¨¶„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑË°®Á§∫ÂΩ¢Âºè NaP-ASTÔºåÂÆÉÂØπÁ±ªÂûãÈôêÂÆöÁ¨¶ÁöÑÊúâÊïàÊé®Êñ≠ÁºñÁ†Å‰∫ÜÊúÄÂ∞èÁöÑÊï∞ÊçÆÊµÅÊèêÁ§∫„ÄÇÊàë‰ª¨ËØÑ‰º∞‰∫ÜÁî®‰∫éÊé®Êñ≠Á±ªÂûãÈôêÂÆöÁ¨¶ÁöÑÂá†ÁßçÊ®°ÂûãÊû∂ÊûÑÔºåÂåÖÊã¨ÂõæËΩ¨Êç¢Âô®ÁΩëÁªú„ÄÅÂõæÂç∑ÁßØÁΩëÁªúÂíåÂ§ßËØ≠Ë®ÄÊ®°Âûã„ÄÇÊàë‰ª¨ÈÄöËøáÂ∞ÜËøô‰∫õÊ®°ÂûãÂ∫îÁî®‰∫é NullAway ÂèØÊèíÊãîÁ±ªÂûãÊ£ÄÊü•Âô®ÁöÑÂÖàÂâçËØÑ‰º∞‰∏≠ÁöÑ 12 ‰∏™ÂºÄÊ∫êÁ®ãÂ∫èÔºåËøõ‰∏ÄÊ≠•È™åËØÅ‰∫ÜËøô‰∫õÊ®°ÂûãÔºåÈô§‰∫Ü‰∏Ä‰∏™Êú™Ê≥®ÈáäÁöÑÈ°πÁõÆÂ§ñÔºåÈôç‰Ωé‰∫ÜÊâÄÊúâÈ°πÁõÆÁöÑË≠¶Âëä„ÄÇÊàë‰ª¨ÂèëÁé∞ GTN Ë°®Áé∞ÊúÄ‰Ω≥ÔºåÂè¨ÂõûÁéá‰∏∫ 0.89ÔºåÁ≤æÁ°ÆÁéá‰∏∫ 0.6„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøõË°å‰∫Ü‰∏ÄÈ°πÁ†îÁ©∂Ôºå‰ª•‰º∞ËÆ°ËÆ≠ÁªÉÊ®°ÂûãËâØÂ•ΩÊÄßËÉΩÊâÄÈúÄÁöÑ Java Á±ªÊï∞Èáè„ÄÇÂØπ‰∫éÊàë‰ª¨ÁöÑÂèØË°åÊÄßÁ†îÁ©∂ÔºåÊÄßËÉΩÊèêÈ´ò‰∫ÜÁ∫¶ 16k ‰∏™Á±ªÔºåÂπ∂‰∏îÁî±‰∫éÂú® 22k ‰∏™Á±ªÂ∑¶Âè≥ËøáÂ∫¶ÊãüÂêàËÄåÊÅ∂Âåñ„ÄÇ

##### **NLP-KG: A System for Exploratory Search of Scientific Literature in Natural Language Processing**
2406.15294v1 by Tim Schopf, Florian Matthes

Scientific literature searches are often exploratory, whereby users are not
yet familiar with a particular field or concept but are interested in learning
more about it. However, existing systems for scientific literature search are
typically tailored to keyword-based lookup searches, limiting the possibilities
for exploration. We propose NLP-KG, a feature-rich system designed to support
the exploration of research literature in unfamiliar natural language
processing (NLP) fields. In addition to a semantic search, NLP-KG allows users
to easily find survey papers that provide a quick introduction to a field of
interest. Further, a Fields of Study hierarchy graph enables users to
familiarize themselves with a field and its related areas. Finally, a chat
interface allows users to ask questions about unfamiliar concepts or specific
articles in NLP and obtain answers grounded in knowledge retrieved from
scientific publications. Our system provides users with comprehensive
exploration possibilities, supporting them in investigating the relationships
between different fields, understanding unfamiliar concepts in NLP, and finding
relevant research literature. Demo, video, and code are available at:
https://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp.

ÊëòË¶ÅÔºöÁßëÂ≠∏ÊñáÁçªÊêúÂ∞ãÈÄöÂ∏∏Â∏∂ÊúâÊé¢Á¥¢ÊÄßË≥™Ôºå‰ΩøÁî®ËÄÖÂèØËÉΩÂ∞öÊú™ÁÜüÊÇâÁâπÂÆöÈ†òÂüüÊàñÊ¶ÇÂøµÔºå‰ΩÜÊúâËààË∂£ÈÄ≤‰∏ÄÊ≠•‰∫ÜËß£„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁßëÂ≠∏ÊñáÁçªÊêúÂ∞ãÁ≥ªÁµ±ÈÄöÂ∏∏ÈáùÂ∞çÂü∫ÊñºÈóúÈçµÂ≠óÁöÑÊü•Ë©¢ÊêúÂ∞ãÈÄ≤Ë°åË™øÊï¥ÔºåÈôêÂà∂‰∫ÜÊé¢Á¥¢ÁöÑÂèØËÉΩÊÄß„ÄÇÊàëÂÄëÊèêÂá∫ NLP-KGÔºåÈÄôÊòØ‰∏ÄÂÄãÂäüËÉΩË±êÂØåÁöÑÁ≥ªÁµ±ÔºåÊó®Âú®ÊîØÊè¥Âú®‰∏çÁÜüÊÇâÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) È†òÂüüÊé¢Á¥¢Á†îÁ©∂ÊñáÁçª„ÄÇÈô§‰∫ÜË™ûÁæ©ÊêúÂ∞ãÂ§ñÔºåNLP-KG ‰ΩøÁî®ËÄÖÂèØ‰ª•ËºïÈ¨ÜÊâæÂà∞Êèê‰æõÁâπÂÆöÈ†òÂüüÂø´ÈÄüÂÖ•ÈñÄÁöÑË™øÊü•Ë´ñÊñá„ÄÇÊ≠§Â§ñÔºåÁ†îÁ©∂È†òÂüüÈöéÂ±§ÂúñË°®‰ΩøÁî®Êà∂ËÉΩÂ§†ÁÜüÊÇâ‰∏ÄÂÄãÈ†òÂüüÂèäÂÖ∂Áõ∏ÈóúÈ†òÂüü„ÄÇÊúÄÂæåÔºåËÅäÂ§©‰ªãÈù¢‰ΩøÁî®Êà∂ÂèØ‰ª•Ë©¢ÂïèÊúâÈóú NLP ‰∏≠‰∏çÁÜüÊÇâÊ¶ÇÂøµÊàñÁâπÂÆöÊñáÁ´†ÁöÑÂïèÈ°åÔºå‰∏¶ÂèñÂæóÊ§çÂü∫ÊñºÂæûÁßëÂ≠∏Âá∫ÁâàÂìÅ‰∏≠Êì∑ÂèñÁöÑÁü•Ë≠òÁöÑÁ≠îÊ°à„ÄÇÊàëÂÄëÁöÑÁ≥ªÁµ±ÁÇ∫‰ΩøÁî®ËÄÖÊèê‰æõÂÖ®Èù¢ÁöÑÊé¢Á¥¢ÂèØËÉΩÊÄßÔºåÊîØÊè¥‰ªñÂÄëË™øÊü•‰∏çÂêåÈ†òÂüü‰πãÈñìÁöÑÈóú‰øÇÔºå‰∫ÜËß£ NLP ‰∏≠‰∏çÁÜüÊÇâÊ¶ÇÂøµÔºå‰∏¶ÊâæÂà∞Áõ∏ÈóúÁöÑÁ†îÁ©∂ÊñáÁçª„ÄÇÁ§∫ÁØÑ„ÄÅÂΩ±ÁâáÂíåÁ®ãÂºèÁ¢ºÂèØÊñº‰∏ãÂàóÁ∂≤ÂùÄÂèñÂæóÔºöhttps://github.com/NLP-Knowledge-Graph/NLP-KG-WebApp„ÄÇ

##### **Unsupervised Extraction of Dialogue Policies from Conversations**
2406.15214v1 by Makesh Narsimhan Sreedhar, Traian Rebedea, Christopher Parisien

Dialogue policies play a crucial role in developing task-oriented dialogue
systems, yet their development and maintenance are challenging and typically
require substantial effort from experts in dialogue modeling. While in many
situations, large amounts of conversational data are available for the task at
hand, people lack an effective solution able to extract dialogue policies from
this data. In this paper, we address this gap by first illustrating how Large
Language Models (LLMs) can be instrumental in extracting dialogue policies from
datasets, through the conversion of conversations into a unified intermediate
representation consisting of canonical forms. We then propose a novel method
for generating dialogue policies utilizing a controllable and interpretable
graph-based methodology. By combining canonical forms across conversations into
a flow network, we find that running graph traversal algorithms helps in
extracting dialogue flows. These flows are a better representation of the
underlying interactions than flows extracted by prompting LLMs. Our technique
focuses on giving conversation designers greater control, offering a
productivity tool to improve the process of developing dialogue policies.

ÊëòË¶ÅÔºöÂ∞çË©±ÊîøÁ≠ñÂú®ÈñãÁôº‰ªªÂãôÂ∞éÂêëÂ∞çË©±Á≥ªÁµ±‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤ÔºåÁÑ∂ËÄåÂÆÉÂÄëÁöÑÈñãÁôºÂíåÁ∂≠Ë≠∑ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰∏îÈÄöÂ∏∏ÈúÄË¶ÅÂ∞çË©±Âª∫Ê®°Â∞àÂÆ∂ÁöÑÂ§ßÈáèÂ∑•‰Ωú„ÄÇÈõñÁÑ∂Âú®Ë®±Â§öÊÉÖÊ≥Å‰∏ãÔºåÂ§ßÈáèÂ∞çË©±Ë≥áÊñôÂèØÁî®ÊñºÊâãÈÇäÁöÑÂ∑•‰ΩúÔºå‰ΩÜ‰∫∫ÂÄëÁº∫‰πè‰∏ÄÁ®ÆÊúâÊïàÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁÑ°Ê≥ïÂæûÈÄô‰∫õË≥áÊñô‰∏≠ÊèêÂèñÂ∞çË©±ÊîøÁ≠ñ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÈ¶ñÂÖàË™™ÊòéÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â¶Ç‰ΩïÈÄèÈÅéÂ∞áÂ∞çË©±ËΩâÊèõÊàêÁî±Ë¶èÁØÑÂΩ¢ÂºèÁµÑÊàêÁöÑÁµ±‰∏Ä‰∏≠ÈñìË°®Á§∫ÔºåÂæûË≥áÊñôÈõÜ‰∏≠ÊèêÂèñÂ∞çË©±ÊîøÁ≠ñÔºå‰æÜË™™ÊòéÂ¶Ç‰ΩïËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ù„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂà©Áî®ÂèØÊéß‰∏îÂèØËß£ÈáãÁöÑÂü∫ÊñºÂúñÂΩ¢ÁöÑÊñπÊ≥ï‰æÜÁî¢ÁîüÂ∞çË©±ÊîøÁ≠ñÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÈÄèÈÅéÂ∞áÂ∞çË©±‰∏≠ÁöÑË¶èÁØÑÂΩ¢ÂºèÁµÑÂêàÊàêÊµÅÁ∂≤Ë∑ØÔºåÊàëÂÄëÁôºÁèæÂü∑Ë°åÂúñÂΩ¢ÈÅçÊ≠∑ÊºîÁÆóÊ≥ïÊúâÂä©ÊñºÊèêÂèñÂ∞çË©±ÊµÅ„ÄÇÈÄô‰∫õÊµÅÊØîÈÄèÈÅéÊèêÁ§∫ LLM ÊèêÂèñÁöÑÊµÅÊõ¥ËÉΩ‰ª£Ë°®Â∫ïÂ±§‰∫íÂãï„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂ∞àÊ≥®ÊñºËÆìÂ∞çË©±Ë®≠Ë®àÂ∏´ÊìÅÊúâÊõ¥Â§ßÁöÑÊéßÂà∂Ê¨äÔºåÊèê‰æõ‰∏ÄÁ®ÆÁîüÁî¢ÂäõÂ∑•ÂÖ∑‰æÜÊîπÂñÑÈñãÁôºÂ∞çË©±ÊîøÁ≠ñÁöÑÈÅéÁ®ã„ÄÇ

##### **Uni-Mol2: Exploring Molecular Pretraining Model at Scale**
2406.14969v1 by Xiaohong Ji, Wang Zhen, Zhifeng Gao, Hang Zheng, Linfeng Zhang, Guolin Ke, Weinan E

In recent years, pretraining models have made significant advancements in the
fields of natural language processing (NLP), computer vision (CV), and life
sciences. The significant advancements in NLP and CV are predominantly driven
by the expansion of model parameters and data size, a phenomenon now recognized
as the scaling laws. However, research exploring scaling law in molecular
pretraining models remains unexplored. In this work, we present Uni-Mol2 , an
innovative molecular pretraining model that leverages a two-track transformer
to effectively integrate features at the atomic level, graph level, and
geometry structure level. Along with this, we systematically investigate the
scaling law within molecular pretraining models, characterizing the power-law
correlations between validation loss and model size, dataset size, and
computational resources. Consequently, we successfully scale Uni-Mol2 to 1.1
billion parameters through pretraining on 800 million conformations, making it
the largest molecular pretraining model to date. Extensive experiments show
consistent improvement in the downstream tasks as the model size grows. The
Uni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an
average 27% improvement on the QM9 and 14% on COMPAS-1D dataset.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÈ†êË®ìÁ∑¥Ê®°ÂûãÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP)„ÄÅÈõªËÖ¶Ë¶ñË¶∫ (CV) ÂíåÁîüÂëΩÁßëÂ≠∏È†òÂüüÂèñÂæó‰∫ÜÈ°ØËëóÈÄ≤Â±ï„ÄÇNLP Âíå CV ÁöÑÈ°ØËëóÈÄ≤Â±ï‰∏ªË¶ÅÁî±Ê®°ÂûãÂèÉÊï∏ÂíåË≥áÊñôÂ§ßÂ∞èÁöÑÊì¥ÂÖÖÊâÄÊé®ÂãïÔºåÈÄôÁ®ÆÁèæË±°ÁèæÂú®Ë¢´Ë™çÁÇ∫ÊòØË¶èÊ®°ÂåñÂÆöÂæã„ÄÇÁÑ∂ËÄåÔºåÊé¢Á¥¢ÂàÜÂ≠êÈ†êË®ìÁ∑¥Ê®°Âûã‰∏≠Ë¶èÊ®°ÂåñÂÆöÂæãÁöÑÁ†îÁ©∂‰ªçÁÑ∂Êú™Ë¢´Êé¢Á¥¢„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü Uni-Mol2Ôºå‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂàÜÂ≠êÈ†êË®ìÁ∑¥Ê®°ÂûãÔºåÂÆÉÂà©Áî®ÈõôËªåTransformerÂú®ÂéüÂ≠êÂ±§Á¥ö„ÄÅÂúñÂ±§Á¥öÂíåÂπæ‰ΩïÁµêÊßãÂ±§Á¥öÊúâÊïàÂú∞Êï¥ÂêàÁâπÂæµ„ÄÇÈô§Ê≠§‰πãÂ§ñÔºåÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Á†îÁ©∂‰∫ÜÂàÜÂ≠êÈ†êË®ìÁ∑¥Ê®°Âûã‰∏≠ÁöÑË¶èÊ®°ÂåñÂÆöÂæãÔºåÊèèËø∞‰∫ÜÈ©óË≠âÊêçÂ§±ËàáÊ®°ÂûãÂ§ßÂ∞è„ÄÅË≥áÊñôÈõÜÂ§ßÂ∞èÂíåË®àÁÆóË≥áÊ∫ê‰πãÈñìÁöÑÂÜ™ÂæãÁõ∏ÈóúÊÄß„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊàêÂäüÂú∞Â∞á Uni-Mol2 Êì¥Â±ïÂà∞ 11 ÂÑÑÂÄãÂèÉÊï∏Ôºå‰∏¶ÈÄöÈÅéÂ∞ç 8 ÂÑÑÂÄãÊßãÂΩ¢ÈÄ≤Ë°åÈ†êË®ìÁ∑¥Ôºå‰ΩøÂÖ∂ÊàêÁÇ∫ËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÂ§ßÁöÑÂàÜÂ≠êÈ†êË®ìÁ∑¥Ê®°Âûã„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË°®ÊòéÔºåÈö®ËëóÊ®°ÂûãÂ§ßÂ∞èÁöÑÂ¢ûÈï∑Ôºå‰∏ãÊ∏∏‰ªªÂãôÁöÑË°®ÁèæÊåÅÁ∫åÊîπÂñÑ„ÄÇÊìÅÊúâ 11B ÂèÉÊï∏ÁöÑ Uni-Mol2 ‰πüÂÑ™ÊñºÁèæÊúâÊñπÊ≥ïÔºåÂú® QM9 ‰∏äÂπ≥ÂùáÊîπÂñÑ‰∫Ü 27%ÔºåÂú® COMPAS-1D Ë≥áÊñôÈõÜ‰∏äÊîπÂñÑ‰∫Ü 14%„ÄÇ

##### **Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks**
2406.14745v2 by Sefika Efeoglu, Adrian Paschke

Information Extraction (IE) is crucial for converting unstructured data into
structured formats like Knowledge Graphs (KGs). A key task within IE is
Relation Extraction (RE), which identifies relationships between entities in
text. Various RE methods exist, including supervised, unsupervised, weakly
supervised, and rule-based approaches. Recent studies leveraging pre-trained
language models (PLMs) have shown significant success in this area. In the
current era dominated by Large Language Models (LLMs), fine-tuning these models
can overcome limitations associated with zero-shot LLM prompting-based RE
methods, especially regarding domain adaptation challenges and identifying
implicit relations between entities in sentences. These implicit relations,
which cannot be easily extracted from a sentence's dependency tree, require
logical inference for accurate identification. This work explores the
performance of fine-tuned LLMs and their integration into the Retrieval
Augmented-based (RAG) RE approach to address the challenges of identifying
implicit relations at the sentence level, particularly when LLMs act as
generators within the RAG framework. Empirical evaluations on the TACRED,
TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant
performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B,
and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL,
where implicit relations are common, surpassing previous results on this
dataset. Additionally, our method outperforms previous works on TACRED, TACREV,
and Re-TACRED, demonstrating exceptional performance across diverse evaluation
scenarios.

ÊëòË¶ÅÔºöË≥áË®äËêÉÂèñÔºàIEÔºâÂ∞çÊñºÂ∞áÈùûÁµêÊßãÂåñË≥áÊñôËΩâÊèõÊàêÁü•Ë≠òÂúñË≠úÔºàKGÔºâÁ≠âÁµêÊßãÂåñÊ†ºÂºèËá≥ÈóúÈáçË¶Å„ÄÇIE ‰∏≠ÁöÑ‰∏ÄÈ†ÖÈóúÈçµ‰ªªÂãôÊòØÈóú‰øÇËêÉÂèñÔºàREÔºâÔºåÁî®ÊñºË≠òÂà•ÊñáÂ≠ó‰∏≠ÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇ„ÄÇRE ÊñπÊ≥ïÂ§öÁ®ÆÂ§öÊ®£ÔºåÂåÖÊã¨Áõ£Áù£Âºè„ÄÅÈùûÁõ£Áù£Âºè„ÄÅÂº±Áõ£Áù£ÂºèÂíåÂü∫ÊñºË¶èÂâáÁöÑÊñπÊ≥ï„ÄÇÊúÄËøëÂà©Áî®È†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÔºàPLMÔºâÁöÑÁ†îÁ©∂Â∑≤Âú®Ê≠§È†òÂüüÂ±ïÁèæÈ°ØËëóÊàêÊûú„ÄÇÂú®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏ªÂ∞éÁöÑÁï∂ÂâçÊôÇ‰ª£ÔºåÂæÆË™øÈÄô‰∫õÊ®°ÂûãÂèØ‰ª•ÂÖãÊúçËàáÈõ∂Ê¨°Â≠∏Áøí LLM ÊèêÁ§∫Âºè RE ÊñπÊ≥ïÁõ∏ÈóúÁöÑÈôêÂà∂ÔºåÁâπÂà•ÊòØÂú®È†òÂüüÈÅ©ÊáâÊåëÊà∞ÂíåË≠òÂà•Âè•Â≠ê‰∏≠ÂØ¶È´î‰πãÈñìÁöÑÈö±Âê´Èóú‰øÇÊñπÈù¢„ÄÇÈÄô‰∫õÈö±Âê´Èóú‰øÇÁÑ°Ê≥ïËºïÊòìÂæûÂè•Â≠êÁöÑ‰æùË≥¥Ê®π‰∏≠ËêÉÂèñÔºåÈúÄË¶ÅÈÇèËºØÊé®Ë´ñÊâçËÉΩÊ∫ñÁ¢∫Ë≠òÂà•„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊé¢Ë®é‰∫ÜÂæÆË™øÂæåÁöÑ LLM ÁöÑÊïàËÉΩÔºå‰ª•ÂèäÂÆÉÂÄëÊï¥ÂêàÂà∞Ê™¢Á¥¢Â¢ûÂº∑ÂºèÔºàRAGÔºâRE ÊñπÊ≥ï‰∏≠‰ª•Ëß£Ê±∫Âú®Âè•Â≠êÂ±§Á¥öË≠òÂà•Èö±Âê´Èóú‰øÇÁöÑÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú® LLM Âú® RAG Ê°ÜÊû∂‰∏≠ÂÖÖÁï∂ÁîüÊàêÂô®ÁöÑÊôÇÂæå„ÄÇÂú® TACRED„ÄÅTACRED-RevisitedÔºàTACREVÔºâ„ÄÅRe-TACRED Âíå SemEVAL Ë≥áÊñôÈõÜ‰∏äÁöÑÁ∂ìÈ©óË©ï‰º∞È°ØÁ§∫ÔºåÂæÆË™øÂæåÁöÑ LLMÔºåÂåÖÊã¨ Llama2-7B„ÄÅMistral-7B Âíå T5ÔºàÂ§ßÂûãÔºâÔºåÂ§ßÂπÖÊèêÂçá‰∫ÜÊïàËÉΩ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® SemEVAL ‰∏äÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÈÄ≤Â±ïÔºåÂõ†ÁÇ∫Èö±Âê´Èóú‰øÇÂæàÂ∏∏Ë¶ãÔºåË∂ÖË∂ä‰∫ÜÈÄôÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÂÖàÂâçÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú® TACRED„ÄÅTACREV Âíå Re-TACRED ‰∏äÂÑ™ÊñºÂÖàÂâçÁöÑÂ∑•‰ΩúÔºåË≠âÊòé‰∫ÜÂú®‰∏çÂêåÁöÑË©ï‰º∞Â†¥ÊôØ‰∏≠Ë°®ÁèæÂá∫Ëâ≤ÁöÑÊïàËÉΩ„ÄÇ

##### **Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics**
2406.14703v1 by Seungbeen Lee, Seungwon Lim, Seungju Han, Giyeong Oh, Hyungjoo Chae, Jiwan Chung, Minju Kim, Beong-woo Kwak, Yeonsoo Lee, Dongha Lee, Jinyoung Yeo, Youngjae Yu

The idea of personality in descriptive psychology, traditionally defined
through observable behavior, has now been extended to Large Language Models
(LLMs) to better understand their behavior. This raises a question: do LLMs
exhibit distinct and consistent personality traits, similar to humans? Existing
self-assessment personality tests, while applicable, lack the necessary
validity and reliability for precise personality measurements. To address this,
we introduce TRAIT, a new tool consisting of 8K multi-choice questions designed
to assess the personality of LLMs with validity and reliability. TRAIT is built
on the psychometrically validated human questionnaire, Big Five Inventory (BFI)
and Short Dark Triad (SD-3), enhanced with the ATOMIC10X knowledge graph for
testing personality in a variety of real scenarios. TRAIT overcomes the
reliability and validity issues when measuring personality of LLM with
self-assessment, showing the highest scores across three metrics: refusal rate,
prompt sensitivity, and option order sensitivity. It reveals notable insights
into personality of LLM: 1) LLMs exhibit distinct and consistent personality,
which is highly influenced by their training data (i.e., data used for
alignment tuning), and 2) current prompting techniques have limited
effectiveness in eliciting certain traits, such as high psychopathy or low
conscientiousness, suggesting the need for further research in this direction.

ÊëòË¶ÅÔºöÂú®ÂÇ≥Áµ±ÂøÉÁêÜÂ≠∏‰∏≠Ôºå‰∫∫Ê†ºÁöÑÊ¶ÇÂøµÊòØÈÄèÈÅéÂèØËßÄÂØüÁöÑË°åÁÇ∫‰æÜÂÆöÁæ©ÁöÑÔºåÁèæÂú®Â∑≤Êì¥Â±ïÂà∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•Êõ¥‰∫ÜËß£ÂÖ∂Ë°åÁÇ∫„ÄÇÈÄôÂºïÁôº‰∫Ü‰∏ÄÂÄãÂïèÈ°åÔºöLLM ÊòØÂê¶ÂÉè‰∫∫È°û‰∏ÄÊ®£Ë°®ÁèæÂá∫Áç®Áâπ‰∏î‰∏ÄËá¥ÁöÑ‰∫∫Ê†ºÁâπË≥™ÔºüÁèæÊúâÁöÑËá™ÊàëË©ïÈáè‰∫∫Ê†ºÊ∏¨È©óÈõñÁÑ∂ÈÅ©Áî®Ôºå‰ΩÜÁº∫‰πèÁ≤æÁ¢∫‰∫∫Ê†ºÊ∏¨ÈáèÊâÄÈúÄÁöÑÊïàÂ∫¶Âíå‰ø°Â∫¶„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü TRAITÔºåÈÄôÊòØ‰∏ÄÂÄãÁî± 8K ÂÄãÂ§öÈáçÈÅ∏ÊìáÈ°åÁµÑÊàêÁöÑÂÖ®Êñ∞Â∑•ÂÖ∑ÔºåÊó®Âú®Ë©ï‰º∞ LLM ÁöÑ‰∫∫Ê†ºÔºå‰∏¶ÂÖ∑ÂÇôÊïàÂ∫¶Âíå‰ø°Â∫¶„ÄÇTRAIT Âª∫ÊßãÊñºÁ∂ìÈÅéÂøÉÁêÜÊ∏¨ÈáèÈ©óË≠âÁöÑ‰∫∫È°ûÂïèÂç∑ÔºåÂ§ß‰∫î‰∫∫Ê†ºÈáèË°® (BFI) ÂíåÁ∞°Áü≠ÈªëÊöó‰∏âÂÖÉÁµÑ (SD-3)Ôºå‰∏¶Â¢ûÂº∑‰∫Ü ATOMIC10X Áü•Ë≠òÂúñË≠úÔºå‰ª•‰æøÂú®ÂêÑÁ®ÆÂØ¶ÈöõÂ†¥ÊôØ‰∏≠Ê∏¨Ë©¶‰∫∫Ê†º„ÄÇTRAIT ÂÖãÊúç‰∫Ü‰ΩøÁî®Ëá™ÊàëË©ïÈáèÊ∏¨Èáè LLM ‰∫∫Ê†ºÊôÇÁöÑ‰ø°Â∫¶ÂíåÊïàÂ∫¶ÂïèÈ°åÔºåÂú®‰∏âÈ†ÖÊåáÊ®ôÔºàÊãíÁµïÁéá„ÄÅÊèêÁ§∫ÊïèÊÑüÂ∫¶ÂíåÈÅ∏È†ÖÈ†ÜÂ∫èÊïèÊÑüÂ∫¶Ôºâ‰∏≠È°ØÁ§∫Âá∫ÊúÄÈ´òÂàÜ„ÄÇÂÆÉÊè≠Á§∫‰∫Ü LLM ‰∫∫Ê†ºÁöÑÈáçË¶ÅË¶ãËß£Ôºö1) LLM Ë°®ÁèæÂá∫Áç®Áâπ‰∏î‰∏ÄËá¥ÁöÑ‰∫∫Ê†ºÔºåÈÄôÊ∑±ÂèóÂÖ∂Ë®ìÁ∑¥Ë≥áÊñôÔºàÂç≥Áî®ÊñºÂ∞çÈΩäË™øÊï¥ÁöÑË≥áÊñôÔºâÂΩ±ÈüøÔºå‰ª•Âèä 2) ÁõÆÂâçÁöÑÊèêÁ§∫ÊäÄË°ìÂú®ÂºïÁôºÊüê‰∫õÁâπË≥™Ôºà‰æãÂ¶ÇÈ´òÁ≤æÁ•ûÁóÖË≥™Êàñ‰ΩéÁõ°Ë≤¨ÊÄßÔºâÊñπÈù¢ÊïàÊûúÊúâÈôêÔºåÈÄôË°®Á§∫ÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÈÄôÂÄãÊñπÂêë„ÄÇ

##### **TAGLAS: An atlas of text-attributed graph datasets in the era of large graph and language models**
2406.14683v1 by Jiarui Feng, Hao Liu, Lecheng Kong, Yixin Chen, Muhan Zhang

In this report, we present TAGLAS, an atlas of text-attributed graph (TAG)
datasets and benchmarks. TAGs are graphs with node and edge features
represented in text, which have recently gained wide applicability in training
graph-language or graph foundation models. In TAGLAS, we collect and integrate
more than 23 TAG datasets with domains ranging from citation graphs to molecule
graphs and tasks from node classification to graph question-answering. Unlike
previous graph datasets and benchmarks, all datasets in TAGLAS have a unified
node and edge text feature format, which allows a graph model to be
simultaneously trained and evaluated on multiple datasets from various domains.
Further, we provide a standardized, efficient, and simplified way to load all
datasets and tasks. We also provide useful utils like text-to-embedding
conversion, and graph-to-text conversion, which can facilitate different
evaluation scenarios. Finally, we also provide standard and easy-to-use
evaluation utils. The project is open-sourced at
https://github.com/JiaruiFeng/TAGLAS and is still under construction. Please
expect more datasets/features in the future.

ÊëòË¶ÅÔºö<paragraph>Âú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü TAGLASÔºå‰∏Ä‰∏™ÊñáÊú¨Â±ûÊÄßÂõæ (TAG)
Êï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÁöÑÂõæÈõÜ„ÄÇTAG ÊòØÂÖ∑Êúâ‰ª•ÊñáÊú¨Ë°®Á§∫ÁöÑËäÇÁÇπÂíåËæπÁâπÂæÅÁöÑÂõæÔºåÊúÄËøëÂú®ËÆ≠ÁªÉ
ÂõæËØ≠Ë®ÄÊàñÂõæÂü∫Á°ÄÊ®°Âûã‰∏≠Ëé∑Âæó‰∫ÜÂπøÊ≥õÁöÑÂ∫îÁî®„ÄÇÂú® TAGLAS ‰∏≠ÔºåÊàë‰ª¨Êî∂ÈõÜÂπ∂Êï¥Âêà
‰∫Ü 23 ‰∏™‰ª•‰∏äÁöÑ TAG Êï∞ÊçÆÈõÜÔºåÂÖ∂È¢ÜÂüü‰ªéÂºïÊñáÂõæÂà∞ÂàÜÂ≠ê
ÂõæÂíå‰ªªÂä°Ôºå‰ªéËäÇÁÇπÂàÜÁ±ªÂà∞ÂõæÈóÆÁ≠î„ÄÇ‰∏é
‰ª•ÂâçÁöÑÂõæÊï∞ÊçÆÈõÜÂíåÂü∫ÂáÜ‰∏çÂêåÔºåTAGLAS ‰∏≠ÁöÑÊâÄÊúâÊï∞ÊçÆÈõÜÈÉΩÂÖ∑ÊúâÁªü‰∏ÄÁöÑ
ËäÇÁÇπÂíåËæπÊñáÊú¨ÁâπÂæÅÊ†ºÂºèÔºåËøôÂÖÅËÆ∏ÂõæÊ®°ÂûãÂú®Êù•Ëá™‰∏çÂêåÈ¢ÜÂüüÁöÑÂ§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂêåÊó∂ËÆ≠ÁªÉÂíåËØÑ‰º∞„ÄÇ
Ê≠§Â§ñÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏ÄÁßçÊ†áÂáÜÂåñ„ÄÅÈ´òÊïà‰∏îÁÆÄÂåñÁöÑÊñπÂºèÊù•Âä†ËΩΩÊâÄÊúâ
Êï∞ÊçÆÈõÜÂíå‰ªªÂä°„ÄÇÊàë‰ª¨ËøòÊèê‰æõÊúâÁî®ÁöÑÂÆûÁî®Á®ãÂ∫èÔºåÂ¶ÇÊñáÊú¨Âà∞ÂµåÂÖ•
ËΩ¨Êç¢Ôºå‰ª•ÂèäÂõæÂà∞ÊñáÊú¨ËΩ¨Êç¢ÔºåËøôÂèØ‰ª•‰øÉËøõ‰∏çÂêåÁöÑ
ËØÑ‰º∞Âú∫ÊôØ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ËøòÊèê‰æõÊ†áÂáÜ‰∏îÊòì‰∫é‰ΩøÁî®ÁöÑ
ËØÑ‰º∞ÂÆûÁî®Á®ãÂ∫è„ÄÇËØ•È°πÁõÆÂú®
https://github.com/JiaruiFeng/TAGLAS ÂºÄÊ∫êÔºåÂπ∂‰∏î‰ªçÂú®Âª∫ËÆæ‰∏≠„ÄÇËØ∑
ÊúüÂæÖÊú™Êù•ÊúâÊõ¥Â§öÁöÑÊï∞ÊçÆÈõÜ/ÂäüËÉΩ„ÄÇ</paragraph>

##### **HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation**
2406.14655v1 by Jin Wang, Rui Dai, Weijie Wang, Luca Rossini, Francesco Ruscelli, Nikos Tsagarakis

Enabling robots to autonomously perform hybrid motions in diverse
environments can be beneficial for long-horizon tasks such as material
handling, household chores, and work assistance. This requires extensive
exploitation of intrinsic motion capabilities, extraction of affordances from
rich environmental information, and planning of physical interaction behaviors.
Despite recent progress has demonstrated impressive humanoid whole-body control
abilities, they struggle to achieve versatility and adaptability for new tasks.
In this work, we propose HYPERmotion, a framework that learns, selects and
plans behaviors based on tasks in different scenarios. We combine reinforcement
learning with whole-body optimization to generate motion for 38 actuated joints
and create a motion library to store the learned skills. We apply the planning
and reasoning features of the large language models (LLMs) to complex
loco-manipulation tasks, constructing a hierarchical task graph that comprises
a series of primitive behaviors to bridge lower-level execution with
higher-level planning. By leveraging the interaction of distilled spatial
geometry and 2D observation with a visual language model (VLM) to ground
knowledge into a robotic morphology selector to choose appropriate actions in
single- or dual-arm, legged or wheeled locomotion. Experiments in simulation
and real-world show that learned motions can efficiently adapt to new tasks,
demonstrating high autonomy from free-text commands in unstructured scenes.
Videos and website: hy-motion.github.io/

ÊëòË¶ÅÔºö<paragraph>ËÆìÊ©üÂô®‰∫∫ËÉΩÂ§†Âú®‰∏çÂêåÁí∞Â¢É‰∏≠Ëá™‰∏ªÂü∑Ë°åÊ∑∑ÂêàÂãï‰ΩúÔºåÂ∞çÊñºÊùêÊñôÊê¨ÈÅã„ÄÅÂÆ∂ÂãôÂíåÂ∑•‰ΩúÂçîÂä©Á≠âÈï∑Êúü‰ªªÂãôÂèØËÉΩÊòØÊúâÁõäÁöÑ„ÄÇÈÄôÈúÄË¶ÅÂª£Ê≥õÂà©Áî®ÂÖßÂú®ÈÅãÂãïËÉΩÂäõÔºåÂæûË±êÂØåÁöÑÁí∞Â¢ÉË≥áË®ä‰∏≠ÊèêÂèñÂèØË≤†ÊìîÊÄßÔºå‰ª•ÂèäË¶èÂäÉÁâ©ÁêÜ‰∫íÂãïË°åÁÇ∫„ÄÇÂÑòÁÆ°ÊúÄËøëÁöÑÈÄ≤Â±ïÂ∑≤Ë≠âÊòé‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑ‰∫∫ÂΩ¢ÂÖ®Ë∫´ÊéßÂà∂ËÉΩÂäõÔºå‰ΩÜÂÆÉÂÄë‰ªçÈõ£‰ª•ÂØ¶ÁèæÊñ∞‰ªªÂãôÁöÑÂ§öÂäüËÉΩÊÄßÂíåÈÅ©ÊáâÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ HYPERmotionÔºå‰∏ÄÂÄãÂü∫Êñº‰∏çÂêåÂ†¥ÊôØ‰∏≠ÁöÑ‰ªªÂãô‰æÜÂ≠∏Áøí„ÄÅÈÅ∏ÊìáÂíåË¶èÂäÉË°åÁÇ∫ÁöÑÊ°ÜÊû∂„ÄÇÊàëÂÄëÁµêÂêàÂº∑ÂåñÂ≠∏ÁøíËàáÂÖ®Ë∫´ÊúÄ‰Ω≥ÂåñÔºåÁÇ∫ 38 ÂÄãÂãï‰ΩúÈóúÁØÄÁî¢ÁîüÂãï‰ΩúÔºå‰∏¶Âª∫Á´ã‰∏ÄÂÄãÂãï‰ΩúÂ∫´‰æÜÂÑ≤Â≠òÂ≠∏ÁøíÂà∞ÁöÑÊäÄËÉΩ„ÄÇÊàëÂÄëÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑË¶èÂäÉÂíåÊé®ÁêÜÂäüËÉΩÊáâÁî®ÊñºË§áÈõúÁöÑÈÅãÂãïÊìçÁ∏±‰ªªÂãôÔºåÊßãÂª∫‰∏ÄÂÄãÈöéÂ±§Âºè‰ªªÂãôÂúñÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏ÄÁ≥ªÂàóÂü∫Êú¨Ë°åÁÇ∫Ôºå‰ª•Ê©ãÊé•‰ΩéÈöéÂü∑Ë°åËàáÈ´òÈöéË¶èÂäÉ„ÄÇÈÄèÈÅéÂà©Áî®Ëí∏È§æÁ©∫ÈñìÂπæ‰ΩïÂíå 2D ËßÄÊ∏¨ËàáË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÁöÑ‰∫íÂãïÔºåÂ∞áÁü•Ë≠òÂü∫Á§éÂåñÁÇ∫Ê©üÂô®‰∫∫ÂΩ¢ÊÖãÈÅ∏ÊìáÂô®Ôºå‰ª•Âú®ÂñÆËáÇÊàñÈõôËáÇ„ÄÅËÖøÈÉ®ÊàñËº™ÂºèÈÅãÂãï‰∏≠ÈÅ∏ÊìáÈÅ©Áï∂ÁöÑÂãï‰Ωú„ÄÇÊ®°Êì¨ÂíåÁèæÂØ¶‰∏ñÁïåÁöÑÂØ¶È©óË°®ÊòéÔºåÂ≠∏ÁøíÂà∞ÁöÑÂãï‰ΩúÂèØ‰ª•ÊúâÊïàÈÅ©ÊáâÊñ∞‰ªªÂãôÔºåË≠âÊòé‰∫ÜÂú®ÈùûÁµêÊßãÂåñÂ†¥ÊôØ‰∏≠ÂæûËá™Áî±ÊñáÂ≠óÊåá‰ª§‰∏≠Áç≤ÂæóÈ´òÂ∫¶Ëá™‰∏ªÊÄß„ÄÇÂΩ±ÁâáÂíåÁ∂≤Á´ôÔºöhy-motion.github.io/</paragraph>

##### **GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models**
2406.14550v1 by Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, Bo Zheng

Long-context capabilities are essential for large language models (LLMs) to
tackle complex and long-input tasks. Despite numerous efforts made to optimize
LLMs for long contexts, challenges persist in robustly processing long inputs.
In this paper, we introduce GraphReader, a graph-based agent system designed to
handle long texts by structuring them into a graph and employing an agent to
explore this graph autonomously. Upon receiving a question, the agent first
undertakes a step-by-step analysis and devises a rational plan. It then invokes
a set of predefined functions to read node content and neighbors, facilitating
a coarse-to-fine exploration of the graph. Throughout the exploration, the
agent continuously records new insights and reflects on current circumstances
to optimize the process until it has gathered sufficient information to
generate an answer. Experimental results on the LV-Eval dataset reveal that
GraphReader, using a 4k context window, consistently outperforms GPT-4-128k
across context lengths from 16k to 256k by a large margin. Additionally, our
approach demonstrates superior performance on four challenging single-hop and
multi-hop benchmarks.

ÊëòË¶ÅÔºöÈï∑Ë™ûÂ¢ÉËÉΩÂäõÂ∞çÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜË™™Ëá≥ÈóúÈáçË¶ÅÔºåÂèØÊáâÂ∞çË§áÈõú‰∏îËº∏ÂÖ•Èï∑Â∫¶ËºÉÈï∑ÁöÑ‰ªªÂãô„ÄÇÂÑòÁÆ°Â∑≤ÈáùÂ∞ç LLM ÈÄ≤Ë°åË®±Â§öÊúÄ‰Ω≥ÂåñÂ∑•‰Ωú‰ª•ÊáâÂ∞çÈï∑Ë™ûÂ¢ÉÔºå‰ΩÜÂº∑ÂÅ•Âú∞ËôïÁêÜÈï∑Ëº∏ÂÖ•‰ªçÂ≠òÂú®ÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π GraphReaderÔºå‰∏ÄÂÄãÂü∫ÊñºÂúñË°®ÁöÑ‰ª£ÁêÜÁ≥ªÁµ±ÔºåÊó®Âú®ÈÄèÈÅéÂ∞áÈï∑ÊñáÊú¨ÁµêÊßãÂåñÊàê‰∏ÄÂÄãÂúñË°®Ôºå‰∏¶‰ΩøÁî®‰ª£ÁêÜÁ®ãÂºèËá™‰∏ªÊé¢Á¥¢Ê≠§ÂúñË°®Ôºå‰æÜËôïÁêÜÈï∑ÊñáÊú¨„ÄÇÂú®Êî∂Âà∞ÂïèÈ°åÂæåÔºå‰ª£ÁêÜÁ®ãÂºèÈ¶ñÂÖàÈÄ≤Ë°åÈÄêÊ≠•ÂàÜÊûêÔºå‰∏¶Êì¨ÂÆö‰∏ÄÂÄãÂêàÁêÜË®àÁï´„ÄÇÁÑ∂ÂæåÔºåÂÆÉÊúÉÂëºÂè´‰∏ÄÁµÑÈ†êÂÆöÁæ©ÁöÑÂáΩÂºè‰æÜËÆÄÂèñÁØÄÈªûÂÖßÂÆπÂíåÈÑ∞ËøëÁØÄÈªûÔºå‰øÉÈÄ≤Â∞çÂúñË°®ÁöÑÁ≤óÁï•Âà∞Á≤æÁ¥∞Êé¢Á¥¢„ÄÇÂú®Êï¥ÂÄãÊé¢Á¥¢ÈÅéÁ®ã‰∏≠Ôºå‰ª£ÁêÜÁ®ãÂºèÊúÉÊåÅÁ∫åË®òÈåÑÊñ∞ÁöÑË¶ãËß£Ôºå‰∏¶ÂèçÊÄùÁï∂ÂâçÊÉÖÊ≥ÅÔºå‰ª•ÊúÄ‰Ω≥ÂåñËôïÁêÜÁ®ãÂ∫èÔºåÁõ¥Âà∞Êî∂ÈõÜÂà∞Ë∂≥Â§†ÁöÑË≥áË®ä‰æÜÁî¢ÁîüÁ≠îÊ°à„ÄÇÂú® LV-Eval Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåGraphReader ‰ΩøÁî® 4k Ë™ûÂ¢ÉË¶ñÁ™óÔºåÂú® 16k Âà∞ 256k ÁöÑË™ûÂ¢ÉÈï∑Â∫¶‰∏≠ÔºåÂßãÁµÇÂ§ßÂπÖÂÑ™Êñº GPT-4-128k„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®ÂõõÂÄãÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂñÆË∑≥ÂíåÂ§öË∑≥Âü∫Ê∫ñÊ∏¨Ë©¶‰∏≠Â±ïÁèæÂá∫ÂçìË∂äÁöÑÊïàËÉΩ„ÄÇ

##### **medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs**
2406.14326v1 by Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang

Electronic Medical Records (EMRs), while integral to modern healthcare,
present challenges for clinical reasoning and diagnosis due to their complexity
and information redundancy. To address this, we proposed medIKAL (Integrating
Knowledge Graphs as Assistants of LLMs), a framework that combines Large
Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic
capabilities. medIKAL assigns weighted importance to entities in medical
records based on their type, enabling precise localization of candidate
diseases within KGs. It innovatively employs a residual network-like approach,
allowing initial diagnosis by the LLM to be merged into KG search results.
Through a path-based reranking algorithm and a fill-in-the-blank style prompt
template, it further refined the diagnostic process. We validated medIKAL's
effectiveness through extensive experiments on a newly introduced open-sourced
Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis
in real-world settings.

ÊëòË¶ÅÔºöÈõªÂ≠êÁóÖÊ≠∑ (EMR) ÈõñÁÑ∂ÊòØÁèæ‰ª£ÈÜ´ÁôÇ‰øùÂÅ•‰∏çÂèØÊàñÁº∫ÁöÑ‰∏ÄÈÉ®ÂàÜÔºå‰ΩÜÁî±ÊñºÂÖ∂Ë§áÈõúÊÄßÂíåË≥áË®äÂÜóÈ§òÔºåÂ∞çËá®Â∫äÊé®ÁêÜÂíåË®∫Êñ∑ÊèêÂá∫‰∫ÜÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü medIKALÔºàÂ∞áÁü•Ë≠òÂúñË≠úÊï¥ÂêàÁÇ∫ LLM ÁöÑÂä©ÁêÜÔºâÔºå‰∏ÄÂÄãÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÁü•Ë≠òÂúñË≠ú (KG) ÁµêÂêàÁöÑÊ°ÜÊû∂Ôºå‰ª•Â¢ûÂº∑Ë®∫Êñ∑ËÉΩÂäõ„ÄÇmedIKAL Ê†πÊìöÈÜ´ÁôÇË®òÈåÑ‰∏≠ÂØ¶È´îÁöÑÈ°ûÂûãÁÇ∫ÂÖ∂ÂàÜÈÖçÂä†Ê¨äÈáçË¶ÅÊÄßÔºåÂæûËÄåËÉΩÂ§†Á≤æÁ¢∫ÂÆö‰Ωç KG ‰∏≠ÁöÑÂÄôÈÅ∏ÁñæÁóÖ„ÄÇÂÆÉÂâµÊñ∞Âú∞Êé°Áî®‰∫ÜÈ°û‰ººÊÆòÂ∑ÆÁ∂≤Ë∑ØÁöÑÊñπÊ≥ïÔºåÂÖÅË®± LLM ÁöÑÂàùÊ≠•Ë®∫Êñ∑Ëàá KG ÊêúÂ∞ãÁµêÊûúÂêà‰Ωµ„ÄÇÈÄèÈÅéÂü∫ÊñºË∑ØÂæëÁöÑÈáçÊñ∞ÊéíÂ∫èÊºîÁÆóÊ≥ïÂíåÂ°´Á©∫ÂºèÊèêÁ§∫ÁØÑÊú¨ÔºåÈÄ≤‰∏ÄÊ≠•ÂÑ™Âåñ‰∫ÜË®∫Êñ∑ÈÅéÁ®ã„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞çÊñ∞Êé®Âá∫ÁöÑÈñãÊ∫ê‰∏≠Êñá EMR Ë≥áÊñôÈõÜÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºåÈ©óË≠â‰∫Ü medIKAL ÁöÑÊúâÊïàÊÄßÔºåË≠âÊòé‰∫ÜÂÖ∂Âú®ÁèæÂØ¶‰∏ñÁïå‰∏≠ÊîπÂñÑËá®Â∫äË®∫Êñ∑ÁöÑÊΩõÂäõ„ÄÇ

##### **Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs**
2406.14282v1 by Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, Jeff Z. Pan, Wen Zhang, Huajun Chen

Improving the performance of large language models (LLMs) in complex
question-answering (QA) scenarios has always been a research focal point.
Recent studies have attempted to enhance LLMs' performance by combining
step-wise planning with external retrieval. While effective for advanced models
like GPT-3.5, smaller LLMs face challenges in decomposing complex questions,
necessitating supervised fine-tuning. Previous work has relied on manual
annotation and knowledge distillation from teacher LLMs, which are
time-consuming and not accurate enough. In this paper, we introduce a novel
framework for enhancing LLMs' planning capabilities by using planning data
derived from knowledge graphs (KGs). LLMs fine-tuned with this data have
improved planning capabilities, better equipping them to handle complex QA
tasks that involve retrieval. Evaluations on multiple datasets, including our
newly proposed benchmark, highlight the effectiveness of our framework and the
benefits of KG-derived planning data.

ÊëòË¶ÅÔºö<paragraph>ÊîπÂñÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ë§áÈõúÂïèÁ≠î (QA) ÊÉÖÂ¢É‰∏≠ÁöÑÊïàËÉΩ‰∏ÄÁõ¥ÊòØÁ†îÁ©∂ÈáçÈªû„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂ÂòóË©¶ÈÄèÈÅéÁµêÂêàÈÄêÊ≠•Ë¶èÂäÉËàáÂ§ñÈÉ®Êì∑Âèñ‰æÜÂ¢ûÂº∑ LLM ÁöÑÊïàËÉΩ„ÄÇÈõñÁÑ∂Â∞çÊñº GPT-3.5 Á≠âÈÄ≤ÈöéÊ®°Âûã‰æÜË™™ÂæàÊúâÊïàÔºå‰ΩÜËºÉÂ∞èÁöÑ LLM Âú®ÂàÜËß£Ë§áÈõúÂïèÈ°åÊôÇÊúÉÈù¢Ëá®ÊåëÊà∞ÔºåÂõ†Ê≠§ÈúÄË¶ÅÁõ£Áù£ÂæÆË™ø„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂‰ª∞Ë≥¥‰∫∫Â∑•Ê®ôË®ªÂíåÊïôÂ∏´ LLM ÁöÑÁü•Ë≠òËêÉÂèñÔºåÈÄôËÄóÊôÇ‰∏î‰∏çÂ§†Á≤æÁ¢∫„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊû∂ÊßãÔºåÈÄèÈÅé‰ΩøÁî®ÂæûÁü•Ë≠òÂúñË≠ú (KG) ‰∏≠Ë°çÁîüÁöÑË¶èÂäÉË≥áÊñô‰æÜÂ¢ûÂº∑ LLM ÁöÑË¶èÂäÉËÉΩÂäõ„ÄÇ‰ΩøÁî®Ê≠§Ë≥áÊñôÂæÆË™øÁöÑ LLM ÊîπÂñÑ‰∫ÜË¶èÂäÉËÉΩÂäõÔºåËÆìÂÆÉÂÄëÊõ¥ËÉΩËôïÁêÜÊ∂âÂèäÊì∑ÂèñÁöÑË§áÈõú QA ‰ªªÂãô„ÄÇÂú®Â§öÂÄãË≥áÊñôÈõÜÔºàÂåÖÊã¨ÊàëÂÄëÊñ∞ÊèêÂá∫ÁöÑÂü∫Ê∫ñÔºâ‰∏äÁöÑË©ï‰º∞Á™ÅÈ°Ø‰∫ÜÊàëÂÄëÊû∂ÊßãÁöÑÊúâÊïàÊÄßÔºå‰ª•Âèä KG Ë°çÁîüË¶èÂäÉË≥áÊñôÁöÑÂ•ΩËôï„ÄÇ</paragraph>

##### **ReaLHF: Optimized RLHF Training for Large Language Models through Parameter Reallocation**
2406.14088v1 by Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, Yi Wu

Reinforcement Learning from Human Feedback (RLHF) stands as a pivotal
technique in empowering large language model (LLM) applications. Since RLHF
involves diverse computational workloads and intricate dependencies among
multiple LLMs, directly adopting parallelization techniques from supervised
training can result in sub-optimal performance. To overcome this limitation, we
propose a novel approach named parameter ReaLlocation, which dynamically
redistributes LLM parameters in the cluster and adapts parallelization
strategies during training. Building upon this idea, we introduce ReaLHF, a
pioneering system capable of automatically discovering and running efficient
execution plans for RLHF training given the desired algorithmic and hardware
configurations. ReaLHF formulates the execution plan for RLHF as an augmented
dataflow graph. Based on this formulation, ReaLHF employs a tailored search
algorithm with a lightweight cost estimator to discover an efficient execution
plan. Subsequently, the runtime engine deploys the selected plan by effectively
parallelizing computations and redistributing parameters. We evaluate ReaLHF on
the LLaMA-2 models with up to $4\times70$ billion parameters and 128 GPUs. The
experiment results showcase ReaLHF's substantial speedups of $2.0-10.6\times$
compared to baselines. Furthermore, the execution plans generated by ReaLHF
exhibit an average of $26\%$ performance improvement over heuristic approaches
based on Megatron-LM. The source code of ReaLHF is publicly available at
https://github.com/openpsi-project/ReaLHF .

ÊëòË¶ÅÔºöÂº∑ÂåñÂ≠∏Áøí‰æÜËá™‰∫∫È°ûÂõûÈ•ã (RLHF) ÊòØ‰∏ÄÁ®ÆÈóúÈçµÊäÄË°ìÔºåÁî®ÊñºË≥¶ËÉΩÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊáâÁî®Á®ãÂºè„ÄÇÁî±Êñº RLHF Ê∂âÂèäÂ§öÁ®ÆÈÅãÁÆóÂ∑•‰ΩúË≤†ËºâÂíåÂ§öÂÄã LLM ‰πãÈñìÁöÑË§áÈõú‰æùË≥¥Èóú‰øÇÔºåÁõ¥Êé•Êé°Áî®Áõ£Áù£ÂºèË®ìÁ∑¥ÁöÑÂπ≥Ë°åÂåñÊäÄË°ìÂèØËÉΩÊúÉÂ∞éËá¥Ê¨°‰Ω≥ÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂêçÁÇ∫ÂèÉÊï∏ÈáçÊñ∞ÈÖçÁΩÆÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÆÉÊúÉÂãïÊÖãÈáçÊñ∞ÂàÜÈÖçÂè¢ÈõÜ‰∏≠ÁöÑ LLM ÂèÉÊï∏Ôºå‰∏¶Âú®Ë®ìÁ∑¥ÊúüÈñìË™øÊï¥Âπ≥Ë°åÂåñÁ≠ñÁï•„ÄÇÂú®Ê≠§Ê¶ÇÂøµÁöÑÂü∫Á§é‰∏äÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü ReaLHFÔºåÈÄôÊòØ‰∏ÄÂÄãÈñãÂâµÊÄßÁöÑÁ≥ªÁµ±ÔºåËÉΩÂ§†Ëá™ÂãïÁôºÁèæ‰∏¶Âü∑Ë°å RLHF Ë®ìÁ∑¥ÁöÑÈ´òÊïàÂü∑Ë°åË®àÁï´Ôºå‰∏¶ËÄÉÈáèÊâÄÈúÄÁöÑÊºîÁÆóÊ≥ïÂíåÁ°¨È´îÁµÑÊÖã„ÄÇReaLHF Â∞á RLHF ÁöÑÂü∑Ë°åË®àÁï´Âà∂ÂÆöÁÇ∫‰∏ÄÂÄãÊì¥Â¢ûË≥áÊñôÊµÅÂúñ„ÄÇÂü∫ÊñºÊ≠§Âà∂ÂÆöÔºåReaLHF Êé°Áî®ÈáèË∫´ÊâìÈÄ†ÁöÑÊêúÂ∞ãÊºîÁÆóÊ≥ïÔºåÊê≠ÈÖçËºïÈáèÁ¥öÊàêÊú¨‰º∞Ë®àÂô®Ôºå‰ª•ÁôºÁèæÈ´òÊïàÁöÑÂü∑Ë°åË®àÁï´„ÄÇÈö®ÂæåÔºåÂü∑Ë°åÊôÇÈñìÂºïÊìéÈÄèÈÅéÊúâÊïàÂπ≥Ë°åÂåñÈÅãÁÆóÂíåÈáçÊñ∞ÂàÜÈÖçÂèÉÊï∏Ôºå‰æÜÈÉ®ÁΩ≤ÊâÄÈÅ∏ÁöÑË®àÁï´„ÄÇÊàëÂÄëÂú® LLaMA-2 Ê®°Âûã‰∏äË©ï‰º∞ ReaLHFÔºåË©≤Ê®°ÂûãÊúÄÂ§öÊúâ $4\times70$0 ÂÑÑÂÄãÂèÉÊï∏Âíå 128 ÂÄã GPU„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåËàáÂü∫Ê∫ñÁõ∏ÊØîÔºåReaLHF ÁöÑÈÄüÂ∫¶ÊèêÂçá‰∫Ü $2.0-10.6\times$„ÄÇÊ≠§Â§ñÔºåReaLHF ÁîüÊàêÁöÑÂü∑Ë°åË®àÁï´ÊØîÂü∫Êñº Megatron-LM ÁöÑÂïüÁôºÂºèÊñπÊ≥ïÔºåÂπ≥ÂùáÊïàËÉΩÊèêÂçá‰∫Ü $26\%$„ÄÇReaLHF ÁöÑÂéüÂßãÁ®ãÂºèÁ¢ºÂÖ¨ÈñãÊñº https://github.com/openpsi-project/ReaLHF„ÄÇ

##### **HIGHT: Hierarchical Graph Tokenization for Graph-Language Alignment**
2406.14021v1 by Yongqiang Chen, Quanming Yao, Juzheng Zhang, James Cheng, Yatao Bian

Recently there has been a surge of interest in extending the success of large
language models (LLMs) to graph modality, such as social networks and
molecules. As LLMs are predominantly trained with 1D text data, most existing
approaches adopt a graph neural network to represent a graph as a series of
node tokens and feed these tokens to LLMs for graph-language alignment. Despite
achieving some successes, existing approaches have overlooked the hierarchical
structures that are inherent in graph data. Especially, in molecular graphs,
the high-order structural information contains rich semantics of molecular
functional groups, which encode crucial biochemical functionalities of the
molecules. We establish a simple benchmark showing that neglecting the
hierarchical information in graph tokenization will lead to subpar
graph-language alignment and severe hallucination in generated outputs. To
address this problem, we propose a novel strategy called HIerarchical GrapH
Tokenization (HIGHT). HIGHT employs a hierarchical graph tokenizer that
extracts and encodes the hierarchy of node, motif, and graph levels of
informative tokens to improve the graph perception of LLMs. HIGHT also adopts
an augmented graph-language supervised fine-tuning dataset, enriched with the
hierarchical graph information, to further enhance the graph-language
alignment. Extensive experiments on 7 molecule-centric benchmarks confirm the
effectiveness of HIGHT in reducing hallucination by 40%, as well as significant
improvements in various molecule-language downstream tasks.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºå‰∫∫‰ª¨ÂØπÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÊàêÂäüÊâ©Â±ïÂà∞ÂõæÊ®°ÂºèÔºà‰æãÂ¶ÇÁ§æ‰∫§ÁΩëÁªúÂíåÂàÜÂ≠êÔºâ‰∫ßÁîü‰∫ÜÊµìÂéöÁöÑÂÖ¥Ë∂£„ÄÇÁî±‰∫é LLM ‰∏ªË¶Å‰ΩøÁî®‰∏ÄÁª¥ÊñáÊú¨Êï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÂõ†Ê≠§Â§ßÂ§öÊï∞Áé∞ÊúâÊñπÊ≥ïÈááÁî®ÂõæÁ•ûÁªèÁΩëÁªúÂ∞ÜÂõæË°®Á§∫‰∏∫‰∏ÄÁ≥ªÂàóËäÇÁÇπÊ†áËÆ∞ÔºåÂπ∂Â∞ÜËøô‰∫õÊ†áËÆ∞È¶àÈÄÅËá≥ LLM ‰ª•ËøõË°åÂõæËØ≠Ë®ÄÂØπÈΩê„ÄÇÂ∞ΩÁÆ°ÂèñÂæó‰∫Ü‰∏Ä‰∫õÊàêÂäüÔºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂç¥ÂøΩËßÜ‰∫ÜÂõæÊï∞ÊçÆ‰∏≠Âõ∫ÊúâÁöÑÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇÁâπÂà´ÊòØÂú®ÂàÜÂ≠êÂõæ‰∏≠ÔºåÈ´òÈò∂ÁªìÊûÑ‰ø°ÊÅØÂåÖÂê´‰∏∞ÂØåÁöÑÂàÜÂ≠êÂÆòËÉΩÂõ¢ËØ≠‰πâÔºåÂÆÉÂØπÂàÜÂ≠êÁöÑÂÖ≥ÈîÆÁîüÂåñÂäüËÉΩËøõË°åÁºñÁ†Å„ÄÇÊàë‰ª¨Âª∫Á´ã‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂü∫ÂáÜÔºåË°®ÊòéÂú®ÂõæÊ†áËÆ∞Âåñ‰∏≠ÂøΩÁï•Â±ÇÊ¨°‰ø°ÊÅØ‰ºöÂØºËá¥Ê¨°‰ºòÁöÑÂõæËØ≠Ë®ÄÂØπÈΩêÔºåÂπ∂Âú®ÁîüÊàêÁöÑËæìÂá∫‰∏≠Âá∫Áé∞‰∏•ÈáçÁöÑÂπªËßâ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁß∞‰∏∫ÂàÜÂ±ÇÂõæÊ†áËÆ∞Âåñ (HIGHT) ÁöÑÊñ∞Á≠ñÁï•„ÄÇHIGHT ÈááÁî®ÂàÜÂ±ÇÂõæÊ†áËÆ∞Âô®ÔºåËØ•Ê†áËÆ∞Âô®ÊèêÂèñÂíåÁºñÁ†Å‰ø°ÊÅØÊ†áËÆ∞ÁöÑËäÇÁÇπ„ÄÅ‰∏ªÈ¢òÂíåÂõæÁ∫ßÂà´Â±ÇÊ¨°ÁªìÊûÑÔºå‰ª•ÊîπÂñÑ LLM ÁöÑÂõæÊÑüÁü•„ÄÇHIGHT ËøòÈááÁî®‰∫Ü‰∏Ä‰∏™ÁªèËøáÊâ©ÂÖÖÁöÑÂõæËØ≠Ë®ÄÁõëÁù£ÂæÆË∞ÉÊï∞ÊçÆÈõÜÔºåËØ•Êï∞ÊçÆÈõÜÂåÖÂê´ÂàÜÂ±ÇÂõæ‰ø°ÊÅØÔºå‰ª•Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫ÂõæËØ≠Ë®ÄÂØπÈΩê„ÄÇÂú® 7 ‰∏™‰ª•ÂàÜÂ≠ê‰∏∫‰∏≠ÂøÉÁöÑÂü∫ÂáÜ‰∏äÁöÑÂ§ßÈáèÂÆûÈ™åËØÅÂÆû‰∫Ü HIGHT Âú®Â∞ÜÂπªËßâÂáèÂ∞ë 40% ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºå‰ª•ÂèäÂú®ÂêÑÁßçÂàÜÂ≠êËØ≠Ë®Ä‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÊòæËëóÊîπËøõ„ÄÇ</paragraph>

##### **A Pure Transformer Pretraining Framework on Text-attributed Graphs**
2406.13873v1 by Yu Song, Haitao Mao, Jiachen Xiao, Jingzhe Liu, Zhikai Chen, Wei Jin, Carl Yang, Jiliang Tang, Hui Liu

Pretraining plays a pivotal role in acquiring generalized knowledge from
large-scale data, achieving remarkable successes as evidenced by large models
in CV and NLP. However, progress in the graph domain remains limited due to
fundamental challenges such as feature heterogeneity and structural
heterogeneity. Recently, increasing efforts have been made to enhance node
feature quality with Large Language Models (LLMs) on text-attributed graphs
(TAGs), demonstrating superiority to traditional bag-of-words or word2vec
techniques. These high-quality node features reduce the previously critical
role of graph structure, resulting in a modest performance gap between Graph
Neural Networks (GNNs) and structure-agnostic Multi-Layer Perceptrons (MLPs).
Motivated by this, we introduce a feature-centric pretraining perspective by
treating graph structure as a prior and leveraging the rich, unified feature
space to learn refined interaction patterns that generalizes across graphs. Our
framework, Graph Sequence Pretraining with Transformer (GSPT), samples node
contexts through random walks and employs masked feature reconstruction to
capture pairwise proximity in the LLM-unified feature space using a standard
Transformer. By utilizing unified text representations rather than varying
structures, our framework achieves significantly better transferability among
graphs within the same domain. GSPT can be easily adapted to both node
classification and link prediction, demonstrating promising empirical success
on various datasets.

ÊëòË¶ÅÔºöÈ†êË®ìÁ∑¥Âú®ÂæûÂ§ßÂûãË≥áÊñô‰∏≠Áç≤ÂèñÂª£Ê≥õÁü•Ë≠òÊñπÈù¢ÁôºÊèÆ‰∫ÜÈóúÈçµ‰ΩúÁî®ÔºåÂæû CV Âíå NLP ‰∏≠ÁöÑÂ§ßÂûãÊ®°ÂûãÊâÄË≠âÊòéÁöÑÈ°ØËëóÊàêÂäü‰∏≠Âç≥ÂèØË¶ã‰∏ÄÊñë„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁâπÂæµÁï∞Ë≥™ÊÄßÂíåÁµêÊßãÁï∞Ë≥™ÊÄßÁ≠âÂü∫Êú¨ÊåëÊà∞ÔºåÂúñÂΩ¢È†òÂüüÁöÑÈÄ≤Â±ï‰ªçÁÑ∂ÊúâÈôê„ÄÇÊúÄËøëÔºå‰∫∫ÂÄëÂú®ÊñáÊú¨Â±¨ÊÄßÂúñ (TAG) ‰∏ä‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÂ¢ûÂº∑ÁØÄÈªûÁâπÂæµÂìÅË≥™Ôºå‰∏¶Â∑≤ÂÅöÂá∫Ë∂ä‰æÜË∂äÂ§öÂä™ÂäõÔºåË≠âÊòéÂÖ∂ÂÑ™ÊñºÂÇ≥Áµ±ÁöÑË©ûË¢ãÊàñ word2vec ÊäÄË°ì„ÄÇÈÄô‰∫õÈ´òÂìÅË≥™ÁØÄÈªûÁâπÂæµÈôç‰Ωé‰∫ÜÂúñÂΩ¢ÁµêÊßãÂÖàÂâçËá≥ÈóúÈáçË¶ÅÁöÑ‰ΩúÁî®ÔºåÂ∞éËá¥ÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂíåËàáÁµêÊßãÁÑ°ÈóúÁöÑÂ§öÂ±§ÊÑüÁü•Âô® (MLP) ‰πãÈñìÁöÑÊïàËÉΩÂ∑ÆË∑ùÁ∏ÆÂ∞è„ÄÇÂèóÂà∞Ê≠§ÂïüÁôºÔºåÊàëÂÄëÈÄèÈÅéÂ∞áÂúñÂΩ¢ÁµêÊßãË¶ñÁÇ∫ÂÖàÈ©óÔºå‰∏¶Âà©Áî®Ë±êÂØåÁöÑÁµ±‰∏ÄÁâπÂæµÁ©∫Èñì‰æÜÂ≠∏ÁøíÂú®ÂúñÂΩ¢‰∏≠Ê¶ÇÊã¨ÁöÑÁ≤æÁ∑ª‰∫íÂãïÊ®°ÂºèÔºåÂºïÂÖ•‰∫Ü‰ª•ÁâπÂæµÁÇ∫‰∏≠ÂøÉÁöÑÈ†êË®ìÁ∑¥ËßÄÈªû„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂúñÂΩ¢Â∫èÂàóÈ†êË®ìÁ∑¥Ëàá Transformer (GSPT)ÔºåÈÄèÈÅéÈö®Ê©üÈÅäËµ∞ÂèñÊ®£ÁØÄÈªûËÑàÁµ°Ôºå‰∏¶Êé°Áî®ÈÅÆËîΩÁâπÂæµÈáçÂª∫Ôºå‰ª•‰ΩøÁî®Ê®ôÊ∫ñ Transformer Âú® LLM Áµ±‰∏ÄÁâπÂæµÁ©∫Èñì‰∏≠Êì∑ÂèñÊàêÂ∞çÊé•ËøëÂ∫¶„ÄÇÈÄèÈÅéÂà©Áî®Áµ±‰∏ÄÁöÑÊñáÂ≠óË°®ÂæµÔºåËÄåÈùûËÆäÂåñÁöÑÁµêÊßãÔºåÊàëÂÄëÁöÑÊû∂ÊßãÂú®Âêå‰∏ÄÂÄãÁ∂≤Âüü‰∏≠ÁöÑÂúñÂΩ¢‰πãÈñìÈÅîÂà∞‰∫ÜÈ°ØËëóÊõ¥Â•ΩÁöÑÂèØÂÇ≥ÈÅûÊÄß„ÄÇGSPT ÂèØ‰ª•ËºïÈ¨ÜÂú∞Ë™øÊï¥Âà∞ÁØÄÈªûÂàÜÈ°ûÂíåÈÄ£ÁµêÈ†êÊ∏¨ÔºåÂú®ÂêÑÁ®ÆË≥áÊñôÈõÜ‰∏äÂ±ïÁèæÂá∫ÊúâÂ∏åÊúõÁöÑÂØ¶Ë≠âÊàêÂäü„ÄÇ

##### **Knowledge Graph-Enhanced Large Language Models via Path Selection**
2406.13862v1 by Haochen Liu, Song Wang, Yaochen Zhu, Yushun Dong, Jundong Li

Large Language Models (LLMs) have shown unprecedented performance in various
real-world applications. However, they are known to generate factually
inaccurate outputs, a.k.a. the hallucination problem. In recent years,
incorporating external knowledge extracted from Knowledge Graphs (KGs) has
become a promising strategy to improve the factual accuracy of LLM-generated
outputs. Nevertheless, most existing explorations rely on LLMs themselves to
perform KG knowledge extraction, which is highly inflexible as LLMs can only
provide binary judgment on whether a certain knowledge (e.g., a knowledge path
in KG) should be used. In addition, LLMs tend to pick only knowledge with
direct semantic relationship with the input text, while potentially useful
knowledge with indirect semantics can be ignored. In this work, we propose a
principled framework KELP with three stages to handle the above problems.
Specifically, KELP is able to achieve finer granularity of flexible knowledge
extraction by generating scores for knowledge paths with input texts via latent
semantic matching. Meanwhile, knowledge paths with indirect semantic
relationships with the input text can also be considered via trained encoding
between the selected paths in KG and the input text. Experiments on real-world
datasets validate the effectiveness of KELP.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆÂØ¶ÈöõÊáâÁî®‰∏≠Â±ïÁèæ‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÊúÉÁî¢Áîü‰∫ãÂØ¶‰∏ä‰∏çÊ∫ñÁ¢∫ÁöÑËº∏Âá∫Ôºå‰πüÂ∞±ÊòØÊâÄË¨ÇÁöÑÂπªË¶∫ÂïèÈ°å„ÄÇËøëÂπ¥‰æÜÔºåÁ¥çÂÖ•ÂæûÁü•Ë≠òÂúñË≠ú (KG) ‰∏≠ËêÉÂèñÁöÑÂ§ñÈÉ®Áü•Ë≠òÂ∑≤ÊàêÁÇ∫ÊîπÂñÑ LLM ÁîüÊàêÁöÑËº∏Âá∫‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄßÁöÑÊúâÂâçÈÄîÁ≠ñÁï•„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁèæÊúâÁöÑÊé¢Á¥¢Â§ßÂ§ö‰æùË≥¥ LLM Êú¨Ë∫´‰æÜÂü∑Ë°å KG Áü•Ë≠òËêÉÂèñÔºåÈÄôÈùûÂ∏∏‰∏çÈùàÊ¥ªÔºåÂõ†ÁÇ∫ LLM Âè™ÊúÉÂ∞çÁâπÂÆöÁü•Ë≠òÔºà‰æãÂ¶ÇÔºåKG ‰∏≠ÁöÑÁü•Ë≠òË∑ØÂæëÔºâÊòØÂê¶ÊáâË©≤‰ΩøÁî®Êèê‰æõ‰∫åÂÖÉÂà§Êñ∑„ÄÇÊ≠§Â§ñÔºåLLM ÂÇæÂêëÂÉÖÊåëÈÅ∏ËàáËº∏ÂÖ•ÊñáÂ≠óÊúâÁõ¥Êé•Ë™ûÁæ©Èóú‰øÇÁöÑÁü•Ë≠òÔºåËÄåÂèØËÉΩÂ∞çË™ûÊÑèÊúâÈñìÊé•ÈóúËÅØÁöÑÊúâÁî®Áü•Ë≠òÂèØËÉΩÊúÉË¢´ÂøΩÁï•„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊúâÂéüÂâáÁöÑ KELP Êû∂ÊßãÔºåÂåÖÂê´‰∏âÂÄãÈöéÊÆµ‰æÜËôïÁêÜ‰∏äËø∞ÂïèÈ°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåKELP ËÉΩÂ§†ÈÄèÈÅéÈö±Âê´Ë™ûÁæ©ÊØîÂ∞çÁÇ∫Áü•Ë≠òË∑ØÂæëËàáËº∏ÂÖ•ÊñáÂ≠óÁî¢ÁîüÂàÜÊï∏ÔºåÈÄ≤ËÄåÈÅîÊàêÊõ¥Á¥∞Á∑ªÁöÑÂΩàÊÄßÁü•Ë≠òËêÉÂèñ„ÄÇÂêåÊôÇÔºå‰πüÂèØ‰ª•ÈÄèÈÅéÂú® KG ‰∏≠ÈÅ∏ÂÆöÁöÑË∑ØÂæëËàáËº∏ÂÖ•ÊñáÂ≠ó‰πãÈñìË®ìÁ∑¥Á∑®Á¢ºÁöÑÊñπÂºèÔºåËÄÉÈáèËàáËº∏ÂÖ•ÊñáÂ≠óÊúâÈñìÊé•Ë™ûÁæ©Èóú‰øÇÁöÑÁü•Ë≠òË∑ØÂæë„ÄÇÂú®ÂØ¶ÈöõË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÈ©óË≠â‰∫Ü KELP ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Few-shot Knowledge Graph Relational Reasoning via Subgraph Adaptation**
2406.15507v1 by Haochen Liu, Song Wang, Chen Chen, Jundong Li

Few-shot Knowledge Graph (KG) Relational Reasoning aims to predict unseen
triplets (i.e., query triplets) for rare relations in KGs, given only several
triplets of these relations as references (i.e., support triplets). This task
has gained significant traction due to the widespread use of knowledge graphs
in various natural language processing applications. Previous approaches have
utilized meta-training methods and manually constructed meta-relation sets to
tackle this task. Recent efforts have focused on edge-mask-based methods, which
exploit the structure of the contextualized graphs of target triplets (i.e., a
subgraph containing relevant triplets in the KG). However, existing
edge-mask-based methods have limitations in extracting insufficient information
from KG and are highly influenced by spurious information in KG. To overcome
these challenges, we propose SAFER (Subgraph Adaptation for Few-shot Relational
Reasoning), a novel approach that effectively adapts the information in
contextualized graphs to various subgraphs generated from support and query
triplets to perform the prediction. Specifically, SAFER enables the extraction
of more comprehensive information from support triplets while minimizing the
impact of spurious information when predicting query triplets. Experimental
results on three prevalent datasets demonstrate the superiority of our proposed
framework SAFER.

ÊëòË¶ÅÔºöÂ∞èÊ†∑Êú¨Áü•ËØÜÂõæË∞± (KG) ÂÖ≥Á≥ªÊé®ÁêÜÊó®Âú®È¢ÑÊµã KG ‰∏≠ÁΩïËßÅÂÖ≥Á≥ªÁöÑÁúã‰∏çËßÅ‰∏âÂÖÉÁªÑÔºàÂç≥Êü•ËØ¢‰∏âÂÖÉÁªÑÔºâÔºåËÄå‰ªÖÁªôÂá∫Âá†‰∏™‰∏âÂÖÉÁªÑ‰Ωú‰∏∫ÂèÇËÄÉÔºàÂç≥ÊîØÊåÅ‰∏âÂÖÉÁªÑÔºâ„ÄÇÁî±‰∫éÁü•ËØÜÂõæË∞±Âú®ÂêÑÁßçËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∫îÁî®Á®ãÂ∫è‰∏≠ÁöÑÂπøÊ≥õ‰ΩøÁî®ÔºåËøôÈ°π‰ªªÂä°Ëé∑Âæó‰∫ÜÊòæËëóÁöÑÂÖ≥Ê≥®„ÄÇ‰ª•ÂâçÁöÑÊñπÊ≥ïÂà©Áî®ÂÖÉËÆ≠ÁªÉÊñπÊ≥ïÂíåÊâãÂä®ÊûÑÂª∫ÁöÑÂÖÉÂÖ≥Á≥ªÈõÜÊù•Ëß£ÂÜ≥Ê≠§‰ªªÂä°„ÄÇÊúÄËøëÁöÑÂä™ÂäõÈõÜ‰∏≠Âú®Âü∫‰∫éËæπÁºòÊé©Á†ÅÁöÑÊñπÊ≥ï‰∏äÔºåËØ•ÊñπÊ≥ïÂà©Áî®ÁõÆÊ†á‰∏âÂÖÉÁªÑÁöÑ‰∏ä‰∏ãÊñáÂåñÂõæÁöÑÁªìÊûÑÔºàÂç≥ÂåÖÂê´ KG ‰∏≠Áõ∏ÂÖ≥‰∏âÂÖÉÁªÑÁöÑÂ≠êÂõæÔºâ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂü∫‰∫éËæπÁºòÊé©Á†ÅÁöÑÊñπÊ≥ïÂú®‰ªé KG ‰∏≠ÊèêÂèñ‰∏çË∂≥‰ø°ÊÅØÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄßÔºåÂπ∂‰∏îÂèó KG ‰∏≠ËôöÂÅá‰ø°ÊÅØÁöÑÊûÅÂ§ßÂΩ±Âìç„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü SAFERÔºàÁî®‰∫éÂ∞èÊ†∑Êú¨ÂÖ≥Á≥ªÊé®ÁêÜÁöÑÂ≠êÂõæËá™ÈÄÇÂ∫îÔºâÔºå‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÂÆÉÊúâÊïàÂú∞Â∞Ü‰∏ä‰∏ãÊñáÂåñÂõæ‰∏≠ÁöÑ‰ø°ÊÅØÈÄÇÂ∫î‰ªéÊîØÊåÅÂíåÊü•ËØ¢‰∏âÂÖÉÁªÑÁîüÊàêÁöÑ‰∏çÂêåÂ≠êÂõæ‰ª•ÊâßË°åÈ¢ÑÊµã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåSAFER ËÉΩÂ§ü‰ªéÊîØÊåÅ‰∏âÂÖÉÁªÑ‰∏≠ÊèêÂèñÊõ¥ÂÖ®Èù¢ÁöÑ‰ø°ÊÅØÔºåÂêåÊó∂Âú®È¢ÑÊµãÊü•ËØ¢‰∏âÂÖÉÁªÑÊó∂ÊúÄÂ§ßÁ®ãÂ∫¶Âú∞ÂáèÂ∞ëËôöÂÅá‰ø°ÊÅØÁöÑÂΩ±Âìç„ÄÇÂú®‰∏â‰∏™ÊµÅË°åÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúËØÅÊòé‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑ SAFER Ê°ÜÊû∂ÁöÑ‰ºòË∂äÊÄß„ÄÇ

##### **Dr.E Bridges Graphs with Large Language Models through Words**
2406.15504v1 by Zipeng Liu, Likang Wu, Ming He, Zhong Guan, Hongke Zhao, Nan Feng

Significant efforts have been directed toward integrating powerful Large
Language Models (LLMs) with diverse modalities, particularly focusing on the
fusion of vision, language, and audio data. However, the graph-structured data,
inherently rich in structural and domain-specific knowledge, have not yet been
gracefully adapted to LLMs. Existing methods either describe the graph with raw
text, suffering the loss of graph structural information, or feed Graph Neural
Network (GNN) embeddings directly into LLM at the cost of losing semantic
representation. To bridge this gap, we introduce an innovative, end-to-end
modality-aligning framework, equipped with a pretrained Dual-Residual Vector
Quantized-Variational AutoEncoder (Dr.E). This framework is specifically
designed to facilitate token-level alignment with LLMs, enabling an effective
translation of the intrinsic `language' of graphs into comprehensible natural
language. Our experimental evaluations on standard GNN node classification
tasks demonstrate competitive performance against other state-of-the-art
approaches. Additionally, our framework ensures interpretability, efficiency,
and robustness, with its effectiveness further validated under both fine-tuning
and few-shot settings. This study marks the first successful endeavor to
achieve token-level alignment between GNNs and LLMs.

ÊëòË¶ÅÔºöÂ§ßÈáèÁöÑÂä™ÂäõÂ∑≤ÊäïÂÖ•Âà∞Â∞áÂº∑Â§ßÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ëàá‰∏çÂêåÁöÑÊ®°ÊÖãÊï¥ÂêàÔºåÁâπÂà•ÊòØÂ∞àÊ≥®ÊñºË¶ñË¶∫„ÄÅË™ûË®ÄÂíåÈü≥Ë®äË≥áÊñôÁöÑËûçÂêà„ÄÇÁÑ∂ËÄåÔºåÂúñÂΩ¢ÁµêÊßãÂåñÁöÑË≥áÊñôÊú¨Ë≥™‰∏äÂØåÂê´ÁµêÊßãÂíåÈ†òÂüüÁâπÂÆöÁöÑÁü•Ë≠òÔºå‰ΩÜÂ∞öÊú™ÂÑ™ÈõÖÂú∞ÈÅ©Êáâ LLM„ÄÇÁèæÊúâÊñπÊ≥ï‰∏çÊòØÁî®ÂéüÂßãÊñáÂ≠óÊèèËø∞ÂúñÂΩ¢ÔºåÂ∞éËá¥ÂúñÂΩ¢ÁµêÊßãË≥áË®äÈÅ∫Â§±ÔºåÂ∞±ÊòØÂ∞áÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÁöÑÂµåÂÖ•Áõ¥Êé•È•ãÂÖ• LLMÔºå‰ª£ÂÉπÊòØÂ§±ÂéªË™ûÁæ©Ë°®Á§∫„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑÁ´ØÂà∞Á´ØÊ®°ÊÖãÂ∞çÈΩäÊ°ÜÊû∂ÔºåÈÖçÂÇô‰∫Ü‰∏ÄÂÄãÈ†êÂÖàË®ìÁ∑¥ÁöÑÈõôÊÆòÂ∑ÆÂêëÈáèÈáèÂåñËÆäÂàÜËá™Á∑®Á¢ºÂô® (Dr.E)„ÄÇÊ≠§Ê°ÜÊû∂ÁâπÂà•Ë®≠Ë®àÁî®Êñº‰øÉÈÄ≤Ëàá LLM ÁöÑÊ®ôË®òÂ±§Á¥öÂ∞çÈΩäÔºåËÆìÂúñÂΩ¢ÁöÑÂÖßÂú®„ÄåË™ûË®Ä„ÄçËÉΩÊúâÊïàËΩâÊèõÊàêÊòìÊñºÁêÜËß£ÁöÑËá™ÁÑ∂Ë™ûË®Ä„ÄÇÊàëÂÄëÂú®Ê®ôÊ∫ñ GNN ÁØÄÈªûÂàÜÈ°û‰ªªÂãô‰∏äÁöÑÂØ¶È©óË©ï‰º∞È°ØÁ§∫ÔºåËàáÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑË°®ÁèæÂÖ∑ÊúâÁ´∂Áà≠Âäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂Á¢∫‰øù‰∫ÜËß£ÈáãÊÄß„ÄÅÊïàÁéáÂíåÁ©©ÂÅ•ÊÄßÔºåÂú®ÂæÆË™øÂíåÂ∞ëÊ®£Êú¨Ë®≠ÂÆö‰∏ãÈÄ≤‰∏ÄÊ≠•È©óË≠âÂÖ∂ÊúâÊïàÊÄß„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Ê®ôË™åËëóÂú® GNN Âíå LLM ‰πãÈñìÂØ¶ÁèæÊ®ôË®òÂ±§Á¥öÂ∞çÈΩäÁöÑÈ¶ñÊ¨°ÊàêÂäüÂòóË©¶„ÄÇ

##### **Enhancing Distractor Generation for Multiple-Choice Questions with Retrieval Augmented Pretraining and Knowledge Graph Integration**
2406.13578v1 by Han-Cheng Yu, Yu-An Shih, Kin-Man Law, Kai-Yu Hsieh, Yu-Chen Cheng, Hsin-Chih Ho, Zih-An Lin, Wen-Chuan Hsu, Yao-Chung Fan

In this paper, we tackle the task of distractor generation (DG) for
multiple-choice questions. Our study introduces two key designs. First, we
propose \textit{retrieval augmented pretraining}, which involves refining the
language model pretraining to align it more closely with the downstream task of
DG. Second, we explore the integration of knowledge graphs to enhance the
performance of DG. Through experiments with benchmarking datasets, we show that
our models significantly outperform the state-of-the-art results. Our
best-performing model advances the F1@3 score from 14.80 to 16.47 in MCQ
dataset and from 15.92 to 16.50 in Sciq dataset.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ËôïÁêÜÂ§öÈÅ∏È°åÁöÑÂπ≤ÊìæÂô®ÁîüÊàê (DG) ‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÂºïÂÖ•‰∫ÜÂÖ©ÂÄãÈóúÈçµË®≠Ë®à„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫„ÄåÊ™¢Á¥¢Â¢ûÂº∑È†êË®ìÁ∑¥„ÄçÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂÑ™ÂåñË™ûË®ÄÊ®°ÂûãÈ†êË®ìÁ∑¥Ôºå‰ΩøÂÖ∂Ëàá DG ÁöÑ‰∏ãÊ∏∏‰ªªÂãôÊõ¥Á∑äÂØÜÂú∞Â∞çÈΩä„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊé¢Ë®éÁü•Ë≠òÂúñË°®ÁöÑÊï¥ÂêàÔºå‰ª•Â¢ûÂº∑ DG ÁöÑÊïàËÉΩ„ÄÇÈÄèÈÅéÂü∫Ê∫ñË≥áÊñôÈõÜÁöÑÂØ¶È©óÔºåÊàëÂÄëË≠âÊòéÊàëÂÄëÁöÑÊ®°ÂûãÊòéÈ°ØÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÊïàËÉΩÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÂ∞á MCQ Ë≥áÊñôÈõÜ‰∏≠ÁöÑ F1@3 ÂàÜÊï∏Âæû 14.80 ÊèêÂçáÂà∞ 16.47ÔºåÂú® Sciq Ë≥áÊñôÈõÜ‰∏≠Âæû 15.92 ÊèêÂçáÂà∞ 16.50„ÄÇ

##### **LangTopo: Aligning Language Descriptions of Graphs with Tokenized Topological Modeling**
2406.13250v1 by Zhong Guan, Hongke Zhao, Likang Wu, Ming He, Jianpin Fan

Recently, large language models (LLMs) have been widely researched in the
field of graph machine learning due to their outstanding abilities in language
comprehension and learning. However, the significant gap between natural
language tasks and topological structure modeling poses a nonnegligible
challenge. Specifically, since natural language descriptions are not sufficient
for LLMs to understand and process graph-structured data, fine-tuned LLMs
perform even worse than some traditional GNN models on graph tasks, lacking
inherent modeling capabilities for graph structures. Existing research overly
emphasizes LLMs' understanding of semantic information captured by external
models, while inadequately exploring graph topological structure modeling,
thereby overlooking the genuine capabilities that LLMs lack. Consequently, in
this paper, we introduce a new framework, LangTopo, which aligns graph
structure modeling with natural language understanding at the token level.
LangTopo quantifies the graph structure modeling capabilities of GNNs and LLMs
by constructing a codebook for the graph modality and performs consistency
maximization. This process aligns the text description of LLM with the
topological modeling of GNN, allowing LLM to learn the ability of GNN to
capture graph structures, enabling LLM to handle graph-structured data
independently. We demonstrate the effectiveness of our proposed method on
multiple datasets.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºåÂ§ßËØ≠Ë®ÄÊ®°Âûã (LLM) Áî±‰∫éÂÖ∂Âú®ËØ≠Ë®ÄÁêÜËß£ÂíåÂ≠¶‰π†ÊñπÈù¢ÁöÑÂá∫Ëâ≤ËÉΩÂäõËÄåÂú®ÂõæÊú∫Âô®Â≠¶‰π†È¢ÜÂüüÂèóÂà∞ÂπøÊ≥õÁ†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåËá™ÁÑ∂ËØ≠Ë®Ä‰ªªÂä°ÂíåÊãìÊâëÁªìÊûÑÂª∫Ê®°‰πãÈó¥ÁöÑÂ∑®Â§ßÂ∑ÆË∑ùÊûÑÊàê‰∫Ü‰∏çÂèØÂøΩËßÜÁöÑÊåëÊàò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÁî±‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞‰∏çË∂≥‰ª•ËÆ© LLM ÁêÜËß£ÂíåÂ§ÑÁêÜÂõæÁªìÊûÑÂåñÊï∞ÊçÆÔºåÂõ†Ê≠§ÁªèËøáÂæÆË∞ÉÁöÑ LLM Âú®Âõæ‰ªªÂä°‰∏äÁöÑË°®Áé∞ÁîöËá≥ÊØî‰∏Ä‰∫õ‰º†ÁªüÁöÑ GNN Ê®°ÂûãËøòË¶ÅÂ∑ÆÔºåÁº∫‰πèÂØπÂõæÁªìÊûÑÁöÑÂõ∫ÊúâÂª∫Ê®°ËÉΩÂäõ„ÄÇÁé∞ÊúâÁ†îÁ©∂ËøáÂàÜÂº∫Ë∞É LLM ÂØπÂ§ñÈÉ®Ê®°ÂûãÊçïËé∑ÁöÑËØ≠‰πâ‰ø°ÊÅØÁöÑÁêÜËß£ÔºåËÄåÂØπÂõæÊãìÊâëÁªìÊûÑÂª∫Ê®°ÁöÑÊé¢Á¥¢‰∏çË∂≥Ôºå‰ªéËÄåÂøΩËßÜ‰∫Ü LLM ÊâÄÁº∫‰πèÁöÑÁúüÊ≠£ËÉΩÂäõ„ÄÇÂõ†Ê≠§ÔºåÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÊ°ÜÊû∂ LangTopoÔºåÂÆÉÂú®Ê†áËÆ∞Á∫ßÂà´Â∞ÜÂõæÁªìÊûÑÂª∫Ê®°‰∏éËá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£Áõ∏ÁªìÂêà„ÄÇLangTopo ÈÄöËøá‰∏∫ÂõæÊ®°ÊÄÅÊûÑÂª∫Á†ÅÊú¨Âπ∂ÊâßË°å‰∏ÄËá¥ÊÄßÊúÄÂ§ßÂåñÊù•ÈáèÂåñ GNN Âíå LLM ÁöÑÂõæÁªìÊûÑÂª∫Ê®°ËÉΩÂäõ„ÄÇÊ≠§ËøáÁ®ãÂ∞Ü LLM ÁöÑÊñáÊú¨ÊèèËø∞‰∏é GNN ÁöÑÊãìÊâëÂª∫Ê®°Áõ∏ÁªìÂêàÔºå‰Ωø LLM ËÉΩÂ§üÂ≠¶‰π† GNN ÊçïËé∑ÂõæÁªìÊûÑÁöÑËÉΩÂäõÔºå‰ªéËÄå‰Ωø LLM ËÉΩÂ§üÁã¨Á´ãÂ§ÑÁêÜÂõæÁªìÊûÑÂåñÊï∞ÊçÆ„ÄÇÊàë‰ª¨Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ</paragraph>

##### **Enhancing Collaborative Semantics of Language Model-Driven Recommendations via Graph-Aware Learning**
2406.13235v1 by Zhong Guan, Likang Wu, Hongke Zhao, Ming He, Jianpin Fan

Large Language Models (LLMs) are increasingly prominent in the recommendation
systems domain. Existing studies usually utilize in-context learning or
supervised fine-tuning on task-specific data to align LLMs into
recommendations. However, the substantial bias in semantic spaces between
language processing tasks and recommendation tasks poses a nonnegligible
challenge. Specifically, without the adequate capturing ability of
collaborative information, existing modeling paradigms struggle to capture
behavior patterns within community groups, leading to LLMs' ineffectiveness in
discerning implicit interaction semantic in recommendation scenarios. To
address this, we consider enhancing the learning capability of language
model-driven recommendation models for structured data, specifically by
utilizing interaction graphs rich in collaborative semantics. We propose a
Graph-Aware Learning for Language Model-Driven Recommendations (GAL-Rec).
GAL-Rec enhances the understanding of user-item collaborative semantics by
imitating the intent of Graph Neural Networks (GNNs) to aggregate multi-hop
information, thereby fully exploiting the substantial learning capacity of LLMs
to independently address the complex graphs in the recommendation system.
Sufficient experimental results on three real-world datasets demonstrate that
GAL-Rec significantly enhances the comprehension of collaborative semantics,
and improves recommendation performance.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Êé®Ëñ¶Á≥ªÁµ±È†òÂüü‰∏≠Ë∂ä‰æÜË∂äÁ™ÅÂá∫„ÄÇÁèæÊúâÁ†îÁ©∂ÈÄöÂ∏∏Âà©Áî®ÊÉÖÂ¢ÉÂ≠∏ÁøíÊàñÂú®ÁâπÂÆö‰ªªÂãôÊï∏Êìö‰∏äÈÄ≤Ë°åÁõ£Áù£ÂæÆË™øÔºå‰ª•Â∞á LLM Ë™øÊï¥ÁÇ∫Âª∫Ë≠∞„ÄÇÁÑ∂ËÄåÔºåË™ûË®ÄËôïÁêÜ‰ªªÂãôÂíåÊé®Ëñ¶‰ªªÂãô‰πãÈñìË™ûÁæ©Á©∫ÈñìÁöÑÂØ¶Ë≥™ÊÄßÂÅèÂ∑ÆÊßãÊàê‰∫Ü‰∏çÂèØÂøΩË¶ñÁöÑÊåëÊà∞„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÁèæÊúâÁöÑÂª∫Ê®°ÁØÑ‰æãÂú®Áº∫‰πèÂçî‰Ωú‰ø°ÊÅØÁöÑÂÖÖÂàÜÊçïÁç≤ËÉΩÂäõÁöÑÊÉÖÊ≥Å‰∏ãÔºåÈõ£‰ª•ÊçïÊçâÁ§æÁæ§Áæ§ÁµÑÂÖßÁöÑË°åÁÇ∫Ê®°ÂºèÔºåÂ∞éËá¥ LLM ÁÑ°Ê≥ïÂú®Êé®Ëñ¶Â†¥ÊôØ‰∏≠Ëæ®Ë≠òÈö±Âê´ÁöÑ‰∫íÂãïË™ûÁæ©„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëËÄÉÊÖÆÂ¢ûÂº∑Ë™ûË®ÄÊ®°ÂûãÈ©ÖÂãïÊé®Ëñ¶Ê®°ÂûãÂ∞çÁµêÊßãÂåñÊï∏ÊìöÁöÑÂ≠∏ÁøíËÉΩÂäõÔºåÁâπÂà•ÊòØÈÄöÈÅéÂà©Áî®ÂØåÂê´Âçî‰ΩúË™ûÁæ©ÁöÑ‰∫§‰∫íÂúñ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂúñÊÑüÁü•Ë™ûË®ÄÊ®°ÂûãÈ©ÖÂãïÊé®Ëñ¶Â≠∏ÁøíÔºàGAL-RecÔºâ„ÄÇGAL-Rec ÈÄöÈÅéÊ®°‰ªøÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàGNNÔºâËÅöÂêàÂ§öË∑≥‰ø°ÊÅØÁöÑÊÑèÂúñ‰æÜÂ¢ûÂº∑Â∞ç‰ΩøÁî®ËÄÖÈ†ÖÁõÆÂçî‰ΩúË™ûÁæ©ÁöÑÁêÜËß£ÔºåÂæûËÄåÂÖÖÂàÜÂà©Áî® LLM ÁöÑÂØ¶Ë≥™ÊÄßÂ≠∏ÁøíËÉΩÂäõ‰æÜÁç®Á´ãËôïÁêÜÊé®Ëñ¶Á≥ªÁµ±‰∏≠ÁöÑË§áÈõúÂúñ„ÄÇÂú®‰∏âÂÄãÁúüÂØ¶‰∏ñÁïåÊï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂÖÖÂàÜÂØ¶È©óÁµêÊûúË°®ÊòéÔºåGAL-Rec Â§ßÂ§ßÂ¢ûÂº∑‰∫ÜÂ∞çÂçî‰ΩúË™ûÁæ©ÁöÑÁêÜËß£Ôºå‰∏¶ÊîπÂñÑ‰∫ÜÊé®Ëñ¶ÊÄßËÉΩ„ÄÇ

##### **Bridging Law and Data: Augmenting Reasoning via a Semi-Structured Dataset with IRAC methodology**
2406.13217v1 by Xiaoxi Kang, Lizhen Qu, Lay-Ki Soon, Zhuang Li, Adnan Trakic

The effectiveness of Large Language Models (LLMs) in legal reasoning is often
limited due to the unique legal terminologies and the necessity for highly
specialized knowledge. These limitations highlight the need for high-quality
data tailored for complex legal reasoning tasks. This paper introduces
LEGALSEMI, a benchmark specifically curated for legal scenario analysis.
LEGALSEMI comprises 54 legal scenarios, each rigorously annotated by legal
experts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion)
framework. In addition, LEGALSEMI is accompanied by a structured knowledge
graph (SKG). A series of experiments were conducted to assess the usefulness of
LEGALSEMI for IRAC analysis. The experimental results demonstrate the
effectiveness of incorporating the SKG for issue identification, rule
retrieval, application and conclusion generation using four different LLMs.
LEGALSEMI will be publicly available upon acceptance of this paper.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ê≥ïÂæãÊé®ÁêÜ‰∏≠ÁöÑÊúâÊïàÊÄßÈÄöÂ∏∏ÂèóÂà∞Áç®ÁâπÁöÑÊ≥ïÂæãË°ìË™ûÂíåÈ´òÂ∫¶Â∞àÊ•≠Áü•Ë≠òÁöÑÂøÖË¶ÅÊÄßÁöÑÈôêÂà∂„ÄÇÈÄô‰∫õÈôêÂà∂Á™ÅÈ°Ø‰∫ÜÈáùÂ∞çË§áÈõúÊ≥ïÂæãÊé®ÁêÜ‰ªªÂãôÈáèË∫´ÊâìÈÄ†ÁöÑÈ´òÂìÅË≥™Ë≥áÊñôÁöÑÈúÄÊ±Ç„ÄÇÊú¨Êñá‰ªãÁ¥π LEGALSEMIÔºåÈÄôÊòØ‰∏ÄÂÄãÂ∞àÈñÄÁÇ∫Ê≥ïÂæãÊÉÖÂ¢ÉÂàÜÊûêÁ≠ñÂäÉÁöÑÂü∫Ê∫ñ„ÄÇLEGALSEMI ÂåÖÂê´ 54 ÂÄãÊ≥ïÂæãÊÉÖÂ¢ÉÔºåÊØèÂÄãÊÉÖÂ¢ÉÈÉΩÁ∂ìÈÅéÊ≥ïÂæãÂ∞àÂÆ∂Ê†πÊìöÂÖ®Èù¢ÁöÑ IRACÔºàÂïèÈ°å„ÄÅË¶èÂâá„ÄÅÊáâÁî®„ÄÅÁµêË´ñÔºâÊû∂ÊßãÂö¥Ê†ºË®ªËß£„ÄÇÊ≠§Â§ñÔºåLEGALSEMI ÈÇÑÈôÑÂ∏∂‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÁü•Ë≠òÂúñË≠ú (SKG)„ÄÇÈÄ≤Ë°å‰∫Ü‰∏ÄÁ≥ªÂàóÂØ¶È©ó‰æÜË©ï‰º∞ LEGALSEMI Â∞ç IRAC ÂàÜÊûêÁöÑÊúâÁî®ÊÄß„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÂ∞á SKG Á¥çÂÖ•ÂïèÈ°åË≠òÂà•„ÄÅË¶èÂâáÊ™¢Á¥¢„ÄÅÊáâÁî®ÂíåÁµêË´ñÁîüÊàê‰∏≠ÁöÑÊúâÊïàÊÄßÔºå‰ΩøÁî®‰∫ÜÂõõÁ®Æ‰∏çÂêåÁöÑ LLM„ÄÇLEGALSEMI Â∞áÂú®Êú¨ÊñáÁç≤Êé•ÂèóÂæåÂÖ¨Èñã„ÄÇ

##### **PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes**
2406.13193v1 by He Cao, Yanjun Shao, Zhiyuan Liu, Zijing Liu, Xiangru Tang, Yuan Yao, Yu Li

Multimodal Large Language Models (MLLMs) have seen growing adoption across
various scientific disciplines. These advancements encourage the investigation
of molecule-text modeling within synthetic chemistry, a field dedicated to
designing and conducting chemical reactions to synthesize new compounds with
desired properties and applications. Current approaches, however, often neglect
the critical role of multiple molecule graph interaction in understanding
chemical reactions, leading to suboptimal performance in synthetic chemistry
tasks. This study introduces PRESTO(Progressive Pretraining Enhances Synthetic
Chemistry Outcomes), a new framework that bridges the molecule-text modality
gap by integrating a comprehensive benchmark of pretraining strategies and
dataset configurations. It progressively improves multimodal LLMs through
cross-modal alignment and multi-graph understanding. Our extensive experiments
demonstrate that PRESTO offers competitive results in downstream synthetic
chemistry tasks. The code can be found at https://github.com/IDEA-XL/PRESTO.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÂú®ÂêÑ‰∏™ÁßëÂ≠¶Â≠¶Áßë‰∏≠ÈÄêÊ∏êË¢´ÈááÁî®„ÄÇËøô‰∫õËøõÊ≠•‰øÉËøõ‰∫ÜÂêàÊàêÂåñÂ≠¶‰∏≠ÂàÜÂ≠êÊñáÊú¨Âª∫Ê®°ÁöÑÁ†îÁ©∂ÔºåÂêàÊàêÂåñÂ≠¶Ëá¥Âäõ‰∫éËÆæËÆ°ÂíåËøõË°åÂåñÂ≠¶ÂèçÂ∫î‰ª•ÂêàÊàêÂÖ∑ÊúâÊâÄÈúÄÊÄßË¥®ÂíåÂ∫îÁî®ÁöÑÊñ∞ÂåñÂêàÁâ©„ÄÇÁÑ∂ËÄåÔºåÂΩìÂâçÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÂøΩÁï•‰∫ÜÂ§ö‰∏™ÂàÜÂ≠êÂõæÁõ∏‰∫í‰ΩúÁî®Âú®ÁêÜËß£ÂåñÂ≠¶ÂèçÂ∫î‰∏≠ÁöÑÂÖ≥ÈîÆ‰ΩúÁî®ÔºåÂØºËá¥Âú®ÂêàÊàêÂåñÂ≠¶‰ªªÂä°‰∏≠ÁöÑÊÄßËÉΩ‰∏ç‰Ω≥„ÄÇÊú¨Á†îÁ©∂‰ªãÁªç‰∫Ü PRESTOÔºàÊ∏êËøõÂºèÈ¢ÑËÆ≠ÁªÉÂ¢ûÂº∫ÂêàÊàêÂåñÂ≠¶ÊàêÊûúÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÊï¥ÂêàÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÂíåÊï∞ÊçÆÈõÜÈÖçÁΩÆÁöÑÁªºÂêàÂü∫ÂáÜÊù•Âº•ÂêàÁêÜÂàÜÂ≠êÊñáÊú¨Ê®°ÊÄÅÂ∑ÆË∑ù„ÄÇÂÆÉÈÄöËøáË∑®Ê®°ÊÄÅÂØπÈΩêÂíåÂ§öÂõæÁêÜËß£ÈÄêÊ≠•ÊîπËøõÂ§öÊ®°ÊÄÅ LLM„ÄÇÊàë‰ª¨ÁöÑÂπøÊ≥õÂÆûÈ™åË°®ÊòéÔºåPRESTO Âú®‰∏ãÊ∏∏ÂêàÊàêÂåñÂ≠¶‰ªªÂä°‰∏≠Êèê‰æõ‰∫ÜÊúâÁ´û‰∫âÂäõÁöÑÁªìÊûú„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/IDEA-XL/PRESTO ‰∏≠ÊâæÂà∞„ÄÇ

##### **QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism**
2406.13167v1 by Bo Wang, Heyan Huang, Yixin Cao, Jiahao Ying, Wei Tang, Chong Feng

While large language models (LLMs) have made notable advancements in natural
language processing, they continue to struggle with processing extensive text.
Memory mechanism offers a flexible solution for managing long contexts,
utilizing techniques such as compression, summarization, and structuring to
facilitate nuanced and efficient handling of large volumes of text. However,
existing techniques face challenges with static knowledge integration, leading
to insufficient adaptation to task-specific needs and missing
multi-segmentation relationships, which hinders the dynamic reorganization and
logical combination of relevant segments during the response process. To
address these issues, we introduce a novel strategy, Question then Reflection
Memory Mechanism (QRMeM), incorporating a dual-structured memory pool. This
pool synergizes static textual content with structured graph guidance,
fostering a reflective trial-and-error approach for navigating and identifying
relevant segments. Our evaluation across multiple-choice questions (MCQ) and
multi-document question answering (Multi-doc QA) benchmarks showcases QRMeM
enhanced performance compared to existing approaches.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊñπÈù¢ÂèñÂæóÈ°ØËëóÈÄ≤Â±ïÔºå‰ΩÜÂÆÉÂÄëÂú®ËôïÁêÜÂ§ßÈáèÊñáÂ≠óÊôÇ‰ªçÈù¢Ëá®Âõ∞Èõ£„ÄÇË®òÊÜ∂Ê©üÂà∂Êèê‰æõ‰∫Ü‰∏ÄÂÄãÈùàÊ¥ªÁöÑËß£Ê±∫ÊñπÊ°àÔºåÁî®ÊñºÁÆ°ÁêÜÈï∑ÁØáËÑàÁµ°ÔºåÂà©Áî®Â£ìÁ∏Æ„ÄÅÊëòË¶ÅÂíåÁµêÊßãÂåñÁ≠âÊäÄË°ìÔºå‰ª•‰øÉÈÄ≤Â∞çÂ§ßÈáèÊñáÂ≠óÁöÑÁ¥∞Á∑ª‰∏îÊúâÊïàÁéáÁöÑËôïÁêÜ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊäÄË°ìÂú®ÈùúÊÖãÁü•Ë≠òÊï¥ÂêàÊñπÈù¢Èù¢Ëá®ÊåëÊà∞ÔºåÂ∞éËá¥ÁÑ°Ê≥ïÂÖÖÂàÜÈÅ©ÊáâÁâπÂÆö‰ªªÂãôÁöÑÈúÄÊ±ÇÔºå‰∏îÁº∫Â∞ëÂ§öÈáçÂàÜÊÆµÈóú‰øÇÔºåÈÄôÈòªÁ§ô‰∫ÜÂõûÊáâÈÅéÁ®ã‰∏≠Áõ∏ÈóúÂçÄÊÆµÁöÑÂãïÊÖãÈáçÁµÑÂíåÈÇèËºØÁµÑÂêà„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á≠ñÁï•ÔºåÂç≥ÂïèÁ≠îÂèçÊÄùË®òÊÜ∂Ê©üÂà∂ (QRMeM)Ôºå‰∏¶ÁµêÂêà‰∫ÜÈõôÁµêÊßãÂåñË®òÊÜ∂Ê±†„ÄÇÊ≠§Ë®òÊÜ∂Ê±†Â∞áÈùúÊÖãÊñáÊú¨ÂÖßÂÆπËàáÁµêÊßãÂåñÂúñÂΩ¢ÊåáÂ∞éÁµêÂêàËµ∑‰æÜÔºå‰øÉÈÄ≤‰∫Ü‰∏ÄÁ®ÆÂèçÊÄùÊÄßÁöÑË©¶ÈåØÊñπÊ≥ïÔºåÁî®ÊñºÂ∞éËà™ÂíåË≠òÂà•Áõ∏ÈóúÂçÄÊÆµ„ÄÇÊàëÂÄëÂú®Â§öÈÅ∏È°å (MCQ) ÂíåÂ§öÊñá‰ª∂ÂïèÁ≠î (Multi-doc QA) Âü∫Ê∫ñ‰∏äÁöÑË©ï‰º∞È°ØÁ§∫ÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåQRMeM Â¢ûÂº∑‰∫ÜÊïàËÉΩ„ÄÇ

##### **Bridging Local Details and Global Context in Text-Attributed Graphs**
2406.12608v1 by Yaoke Wang, Yun Zhu, Wenqiao Zhang, Yueting Zhuang, Yunfei Li, Siliang Tang

Representation learning on text-attributed graphs (TAGs) is vital for
real-world applications, as they combine semantic textual and contextual
structural information. Research in this field generally consist of two main
perspectives: local-level encoding and global-level aggregating, respectively
refer to textual node information unification (e.g., using Language Models) and
structure-augmented modeling (e.g., using Graph Neural Networks). Most existing
works focus on combining different information levels but overlook the
interconnections, i.e., the contextual textual information among nodes, which
provides semantic insights to bridge local and global levels. In this paper, we
propose GraphBridge, a multi-granularity integration framework that bridges
local and global perspectives by leveraging contextual textual information,
enhancing fine-grained understanding of TAGs. Besides, to tackle scalability
and efficiency challenges, we introduce a graphaware token reduction module.
Extensive experiments across various models and datasets show that our method
achieves state-of-theart performance, while our graph-aware token reduction
module significantly enhances efficiency and solves scalability issues.

ÊëòË¶ÅÔºöÊñáÊú¨Â±ûÊÄßÂõæ (TAG) ‰∏äÁöÑË°®ÂæÅÂ≠¶‰π†ÂØπ‰∫éÂÆûÈôÖÂ∫îÁî®Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÁªìÂêà‰∫ÜËØ≠‰πâÊñáÊú¨Âíå‰∏ä‰∏ãÊñáÁªìÊûÑ‰ø°ÊÅØ„ÄÇËØ•È¢ÜÂüüÁöÑÁ†îÁ©∂ÊâÄÊ∂âÂèäÁöÑ‰∏§‰∏™‰∏ªË¶ÅËßÇÁÇπÔºöÂ±ÄÈÉ®ÁºñÁ†ÅÂíåÂÖ®Â±ÄËÅöÂêàÔºåÂàÜÂà´ÊåáÊñáÊú¨ËäÇÁÇπ‰ø°ÊÅØÁªü‰∏ÄÔºà‰æãÂ¶ÇÔºå‰ΩøÁî®ËØ≠Ë®ÄÊ®°ÂûãÔºâÂíåÁªìÊûÑÂ¢ûÂº∫Âª∫Ê®°Ôºà‰æãÂ¶ÇÔºå‰ΩøÁî®ÂõæÁ•ûÁªèÁΩëÁªúÔºâ„ÄÇÂ§ßÂ§öÊï∞Áé∞ÊúâÂ∑•‰Ωú‰æßÈáç‰∫éÁªìÂêà‰∏çÂêåÁöÑ‰ø°ÊÅØÁ∫ßÂà´Ôºå‰ΩÜÂøΩÁï•‰∫ÜÁõ∏‰∫íËÅîÁ≥ªÔºåÂç≥ËäÇÁÇπ‰πãÈó¥ÁöÑ‰∏ä‰∏ãÊñáÊñáÊú¨‰ø°ÊÅØÔºåÂÆÉÊèê‰æõ‰∫ÜËØ≠‰πâËßÅËß£‰ª•Ê°•Êé•Â±ÄÈÉ®ÂíåÂÖ®Â±ÄÁ∫ßÂà´„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü GraphBridgeÔºåËøôÊòØ‰∏Ä‰∏™Â§öÁ≤íÂ∫¶ÈõÜÊàêÊ°ÜÊû∂ÔºåÂÆÉÈÄöËøáÂà©Áî®‰∏ä‰∏ãÊñáÊñáÊú¨‰ø°ÊÅØÊù•Ê°•Êé•Â±ÄÈÉ®ÂíåÂÖ®Â±ÄËßÜËßíÔºåÂ¢ûÂº∫‰∫ÜÂØπ TAG ÁöÑÁªÜÁ≤íÂ∫¶ÁêÜËß£„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜËß£ÂÜ≥ÂèØÊâ©Â±ïÊÄßÂíåÊïàÁéáÊåëÊàòÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂõæÊÑüÁü•‰ª§ÁâåÁº©ÂáèÊ®°Âùó„ÄÇË∑®ÂêÑÁßçÊ®°ÂûãÂíåÊï∞ÊçÆÈõÜÁöÑÂπøÊ≥õÂÆûÈ™åË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåËÄåÊàë‰ª¨ÁöÑÂõæÊÑüÁü•‰ª§ÁâåÁº©ÂáèÊ®°ÂùóÊòæÁùÄÊèêÈ´ò‰∫ÜÊïàÁéáÂπ∂Ëß£ÂÜ≥‰∫ÜÂèØÊâ©Â±ïÊÄßÈóÆÈ¢ò„ÄÇ

##### **MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction**
2406.12950v1 by Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, Qiaoyu Tan

Molecular property prediction (MPP) is a fundamental and crucial task in drug
discovery. However, prior methods are limited by the requirement for a large
number of labeled molecules and their restricted ability to generalize for
unseen and new tasks, both of which are essential for real-world applications.
To address these challenges, we present MolecularGPT for few-shot MPP. From a
perspective on instruction tuning, we fine-tune large language models (LLMs)
based on curated molecular instructions spanning over 1000 property prediction
tasks. This enables building a versatile and specialized LLM that can be
adapted to novel MPP tasks without any fine-tuning through zero- and few-shot
in-context learning (ICL). MolecularGPT exhibits competitive in-context
reasoning capabilities across 10 downstream evaluation datasets, setting new
benchmarks for few-shot molecular prediction tasks. More importantly, with just
two-shot examples, MolecularGPT can outperform standard supervised graph neural
network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM
baselines by up to 16.6% increase on classification accuracy and decrease of
199.17 on regression metrics (e.g., RMSE) under zero-shot. This study
demonstrates the potential of LLMs as effective few-shot molecular property
predictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.

ÊëòË¶ÅÔºöÂàÜÂ≠êÁâπÊÄßÈ†êÊ∏¨ (MPP) ÊòØËó•Áâ©ÁôºÁèæ‰∏≠ÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨‰∏îËá≥ÈóúÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÂÖàÂâçÁöÑÁ†îÁ©∂ÊñπÊ≥ïÂèóÂà∞Â§ßÈáèÊ®ôË®òÂàÜÂ≠êÈúÄÊ±ÇÂíåÊ¶ÇÂåñÂà∞Êú™Ë¶ãÂíåÊñ∞‰ªªÂãôÁöÑËÉΩÂäõÂèóÈôêÁöÑÈôêÂà∂ÔºåÈÄôÂÖ©ËÄÖÂ∞çÊñºÂØ¶ÈöõÊáâÁî®ÈÉΩÊòØÂøÖ‰∏çÂèØÂ∞ëÁöÑ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫Áî®ÊñºÂ∞ëÊ®£Êú¨ MPP ÁöÑ MolecularGPT„ÄÇÂæûÊåá‰ª§ÂæÆË™øÁöÑËßíÂ∫¶‰æÜÁúãÔºåÊàëÂÄëÊ†πÊìöÊ∂µËìã 1000 Â§öÈ†ÖÁâπÊÄßÈ†êÊ∏¨‰ªªÂãôÁöÑÁ≠ñÂäÉÂàÜÂ≠êÊåá‰ª§ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇÈÄô‰ΩøÂæóÊßãÂª∫‰∏ÄÂÄãÂ§öÂäüËÉΩ‰∏îÂ∞àÊ•≠ÁöÑ LLM ÊàêÁÇ∫ÂèØËÉΩÔºåË©≤ LLM ÂèØ‰ª•ÈÄöÈÅéÈõ∂Ê®£Êú¨ÂíåÂ∞ëÊ®£Êú¨ÊÉÖÂ¢ÉÂ≠∏Áøí (ICL) ÈÅ©ÊáâÊñ∞ÁöÑ MPP ‰ªªÂãôÔºåËÄåÁÑ°ÈúÄ‰ªª‰ΩïÂæÆË™ø„ÄÇMolecularGPT Âú® 10 ÂÄã‰∏ãÊ∏∏Ë©ï‰º∞Ë≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÂÖ∑ÊúâÁ´∂Áà≠ÂäõÁöÑÊÉÖÂ¢ÉÊé®ÁêÜËÉΩÂäõÔºåÁÇ∫Â∞ëÊ®£Êú¨ÂàÜÂ≠êÈ†êÊ∏¨‰ªªÂãôË®≠ÂÆö‰∫ÜÊñ∞ÁöÑÂü∫Ê∫ñ„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÂÉÖ‰ΩøÁî®ÂÖ©Ê®£Êú¨ÁØÑ‰æãÔºåMolecularGPT Â∞±ÂèØ‰ª•Âú® 7 ÂÄãË≥áÊñôÈõÜ‰∏≠ÁöÑ 4 ÂÄãË≥áÊñôÈõÜ‰∏äÂÑ™ÊñºÊ®ôÊ∫ñÁõ£Áù£ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÊñπÊ≥ï„ÄÇÂÆÉÈÇÑÂú®Èõ∂Ê®£Êú¨‰∏ãÔºåÈÄöÈÅéÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÊèêÈ´ò‰∫Ü 16.6% ÂíåÂõûÊ≠∏ÊåáÊ®ôÔºà‰æãÂ¶Ç RMSEÔºâÊ∏õÂ∞ë‰∫Ü 199.17ÔºåÂæûËÄåÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑ LLM Âü∫Ê∫ñ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Ë≠âÊòé‰∫Ü LLM ‰ΩúÁÇ∫ÊúâÊïàÂ∞ëÊ®£Êú¨ÂàÜÂ≠êÁâπÊÄßÈ†êÊ∏¨Âô®ÁöÑÊΩõÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/NYUSHCS/MolecularGPT Áç≤Âæó„ÄÇ

##### **LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document Summarization**
2406.12494v1 by Masafumi Enomoto, Kunihiro Takeoka, Kosuke Akimoto, Kiril Gashteovski, Masafumi Oyamada

Open-Domain Multi-Document Summarization (ODMDS) is crucial for addressing
diverse information needs, which aims to generate a summary as answer to user's
query, synthesizing relevant content from multiple documents in a large
collection. Existing approaches that first find relevant passages and then
generate a summary using a language model are inadequate for ODMDS. This is
because open-ended queries often require additional context for the retrieved
passages to cover the topic comprehensively, making it challenging to retrieve
all relevant passages initially. While iterative retrieval methods have been
explored for multi-hop question answering (MQA), they are impractical for ODMDS
due to high latency from repeated large language model (LLM) inference for
reasoning. To address this issue, we propose LightPAL, a lightweight passage
retrieval method for ODMDS that constructs a graph representing passage
relationships using an LLM during indexing and employs random walk instead of
iterative reasoning and retrieval at inference time. Experiments on ODMDS
benchmarks show that LightPAL outperforms baseline retrievers in summary
quality while being significantly more efficient than an iterative MQA
approach.

ÊëòË¶ÅÔºöÈñãÊîæÈ†òÂüüÂ§öÊñá‰ª∂ÊëòË¶Å (ODMDS) Â∞çÊñºËß£Ê±∫‰∏çÂêåÁöÑË≥áË®äÈúÄÊ±ÇËá≥ÈóúÈáçË¶ÅÔºåÂÖ∂ÁõÆÁöÑÊòØÊ†πÊìö‰ΩøÁî®ËÄÖÁöÑÊü•Ë©¢Áî¢ÁîüÊëòË¶Å‰ΩúÁÇ∫Á≠îÊ°àÔºåÁ∂úÂêà‰æÜËá™Â§ßÂûãÈõÜÂêà‰∏≠Â§öÂÄãÊñá‰ª∂ÁöÑÁõ∏ÈóúÂÖßÂÆπ„ÄÇÁèæÊúâÁöÑÊñπÊ≥ïÊòØÂÖàÊâæÂà∞Áõ∏ÈóúÊÆµËêΩÔºåÁÑ∂Âæå‰ΩøÁî®Ë™ûË®ÄÊ®°ÂûãÁî¢ÁîüÊëòË¶ÅÔºåÂ∞çÊñº ODMDS ‰æÜË™™ÊòØ‰∏çÂ§†ÁöÑ„ÄÇÈÄôÊòØÂõ†ÁÇ∫ÈñãÊîæÂºèÊü•Ë©¢ÈÄöÂ∏∏ÈúÄË¶ÅÈ°çÂ§ñÁöÑÂÖßÂÆπÔºåÊâçËÉΩËÆìÊì∑ÂèñÁöÑÊÆµËêΩÂÖ®Èù¢Ê∂µËìã‰∏ªÈ°åÔºåÈÄô‰ΩøÂæó‰∏ÄÈñãÂßãÂ∞±Êì∑ÂèñÊâÄÊúâÁõ∏ÈóúÊÆµËêΩÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÈõñÁÑ∂Â∑≤Á∂ìÊé¢Á¥¢‰∫ÜÂèçË¶ÜÊì∑ÂèñÁöÑÊñπÊ≥ïÁî®ÊñºÂ§öË∑≥ÂïèÈ°åËß£Á≠î (MQA)Ôºå‰ΩÜÁî±ÊñºÂèçË¶ÜÈÄ≤Ë°åÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Êé®Ë´ñ‰ª•ÈÄ≤Ë°åÊé®ÁêÜÔºåÂõ†Ê≠§ÂÆÉÂÄë‰∏çÈÅ©Áî®Êñº ODMDSÔºåÂõ†ÁÇ∫ÈÄôÊúÉÂ∞éËá¥È´òÂª∂ÈÅ≤„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü LightPALÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈáùÂ∞ç ODMDS ÁöÑËºïÈáèÁ¥öÊÆµËêΩÊì∑ÂèñÊñπÊ≥ïÔºåÂÆÉÂú®Á¥¢ÂºïÊôÇ‰ΩøÁî® LLM Âª∫Á´ãË°®Á§∫ÊÆµËêΩÈóú‰øÇÁöÑÂúñÔºå‰∏¶Âú®Êé®Ë´ñÊôÇ‰ΩøÁî®Èö®Ê©üÈÅäËµ∞ÔºåËÄå‰∏çÊòØÂèçË¶ÜÊé®ÁêÜÂíåÊì∑Âèñ„ÄÇÂú® ODMDS Âü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåLightPAL Âú®ÊëòË¶ÅÂìÅË≥™‰∏äÂÑ™ÊñºÂü∫Á∑öÊì∑ÂèñÂô®ÔºåÂêåÊôÇÊØîÂèçË¶Ü MQA ÊñπÊ≥ïÊúâÊïàÁéáÂæóÂ§ö„ÄÇ

##### **Interpretable Catastrophic Forgetting of Large Language Model Fine-tuning via Instruction Vector**
2406.12227v2 by Gangwei Jiang, Caigao Jiang, Zhaoyi Li, Siqiao Xue, Jun Zhou, Linqi Song, Defu Lian, Ying Wei

Fine-tuning large language models (LLMs) can cause them to lose their general
capabilities. However, the intrinsic mechanisms behind such forgetting remain
unexplored. In this paper, we begin by examining this phenomenon by focusing on
knowledge understanding and instruction following, with the latter identified
as the main contributor to forgetting during fine-tuning. Consequently, we
propose the Instruction Vector (IV) framework to capture model representations
highly related to specific instruction-following capabilities, thereby making
it possible to understand model-intrinsic forgetting. Through the analysis of
IV dynamics pre and post-training, we suggest that fine-tuning mostly adds
specialized reasoning patterns instead of erasing previous skills, which may
appear as forgetting. Building on this insight, we develop IV-guided training,
which aims to preserve original computation graph, thereby mitigating
catastrophic forgetting. Empirical tests on three benchmarks confirm the
efficacy of this new approach, supporting the relationship between IVs and
forgetting. Our code will be made available soon.

ÊëòË¶ÅÔºöÂæÆË∞ÉÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÂèØËÉΩÂØºËá¥ÂÆÉ‰ª¨‰∏ßÂ§±‰∏ÄËà¨ËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÈÅóÂøòËÉåÂêéÁöÑÂÜÖÂú®Êú∫Âà∂‰ªçÊú™ÂæóÂà∞Êé¢Á¥¢„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨È¶ñÂÖàÈÄöËøáÂÖ≥Ê≥®Áü•ËØÜÁêÜËß£ÂíåÊåá‰ª§ÈÅµÂæ™Êù•Ê£ÄÈ™åËøôÁßçÁé∞Ë±°ÔºåÂÖ∂‰∏≠ÂêéËÄÖË¢´ËÆ§‰∏∫ÊòØÂæÆË∞ÉËøáÁ®ã‰∏≠ÈÅóÂøòÁöÑ‰∏ªË¶ÅÂéüÂõ†„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊåá‰ª§ÂêëÈáè (IV) Ê°ÜÊû∂Êù•ÊçïËé∑‰∏éÁâπÂÆöÊåá‰ª§ÈÅµÂæ™ËÉΩÂäõÈ´òÂ∫¶Áõ∏ÂÖ≥ÁöÑÊ®°ÂûãË°®Á§∫Ôºå‰ªéËÄåÂèØ‰ª•ÁêÜËß£Ê®°ÂûãÂÜÖÂú®ÁöÑÈÅóÂøò„ÄÇÈÄöËøáÂØπËÆ≠ÁªÉÂâçÂêéÁöÑ IV Âä®ÊÄÅËøõË°åÂàÜÊûêÔºåÊàë‰ª¨ËÆ§‰∏∫ÂæÆË∞É‰∏ªË¶ÅÊ∑ªÂä†‰∫Ü‰∏ìÈó®ÁöÑÊé®ÁêÜÊ®°ÂºèÔºåËÄå‰∏çÊòØÊäπÈô§ÂÖàÂâçÁöÑÊäÄËÉΩÔºåËøôÂèØËÉΩË°®Áé∞‰∏∫ÈÅóÂøò„ÄÇÂü∫‰∫éËøô‰∏ÄËßÅËß£ÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü IV ÊåáÂØºËÆ≠ÁªÉÔºåÂÖ∂ÁõÆÊ†áÊòØ‰øùÁïôÂéüÂßãËÆ°ÁÆóÂõæÔºå‰ªéËÄåÂáèËΩªÁÅæÈöæÊÄßÈÅóÂøò„ÄÇÂú®‰∏â‰∏™Âü∫ÂáÜ‰∏äÁöÑÁªèÈ™åÊµãËØïËØÅÂÆû‰∫ÜËøôÁßçÊñ∞ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊîØÊåÅ‰∫Ü IV ÂíåÈÅóÂøò‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂ∞ÜÂæàÂø´Êèê‰æõ„ÄÇ

##### **DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs**
2406.12072v2 by Jiasheng Zhang, Jialin Chen, Menglin Yang, Aosong Feng, Shuang Liang, Jie Shao, Rex Ying

Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world
scenarios, where each node and edge are associated with text descriptions, and
both the graph structure and text descriptions evolve over time. Despite their
broad applicability, there is a notable scarcity of benchmark datasets tailored
to DyTAGs, which hinders the potential advancement in many research fields. To
address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB),
a collection of large-scale, time-evolving graphs from diverse domains, with
nodes and edges enriched by dynamically changing text attributes and
categories. To facilitate the use of DTGB, we design standardized evaluation
procedures based on four real-world use cases: future link prediction,
destination node retrieval, edge classification, and textual relation
generation. These tasks require models to understand both dynamic graph
structures and natural language, highlighting the unique challenges posed by
DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB,
evaluating 7 popular dynamic graph learning algorithms and their variants of
adapting to text attributes with LLM embeddings, along with 6 powerful large
language models (LLMs). Our results show the limitations of existing models in
handling DyTAGs. Our analysis also demonstrates the utility of DTGB in
investigating the incorporation of structural and textual dynamics. The
proposed DTGB fosters research on DyTAGs and their broad applications. It
offers a comprehensive benchmark for evaluating and advancing models to handle
the interplay between dynamic graph structures and natural language. The
dataset and source code are available at https://github.com/zjs123/DTGB.

ÊëòË¶ÅÔºö<paragraph>ÂãïÊÖãÊñáÂ≠óÂ±¨ÊÄßÂúñË°® (DyTAGs) ÊôÆÈÅçÂ≠òÂú®ÊñºÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØ‰∏≠ÔºåÂÖ∂‰∏≠ÊØèÂÄãÁØÄÈªûÂíåÈÇäÁ∑£ÈÉΩËàáÊñáÂ≠óÊèèËø∞Áõ∏ÈóúËÅØÔºå‰∏îÂúñË°®ÁµêÊßãÂíåÊñáÂ≠óÊèèËø∞ÊúÉÈö®ËëóÊôÇÈñìÊºîËÆä„ÄÇÂÑòÁÆ°ÂÖ∂Âª£Ê≥õÁöÑÈÅ©Áî®ÊÄßÔºå‰ΩÜÂ∞àÈñÄÈáùÂ∞ç DyTAGs ÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜÂçªÂçÅÂàÜÁ®ÄÂ∞ëÔºåÈÄôÈòªÁ§ô‰∫ÜË®±Â§öÁ†îÁ©∂È†òÂüüÁöÑÊΩõÂú®ÈÄ≤Â±ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂãïÊÖãÊñáÂ≠óÂ±¨ÊÄßÂúñË°®Âü∫Ê∫ñ (DTGB)ÔºåÂÆÉÊòØ‰∏ÄÂÄã‰æÜËá™‰∏çÂêåÈ†òÂüüÁöÑÂ§ßÂûã„ÄÅÊôÇËÆäÂúñË°®ÁöÑÈõÜÂêàÔºåÂÖ∂ÁØÄÈªûÂíåÈÇäÁ∑£Áî±ÂãïÊÖãËÆäÂåñÁöÑÊñáÂ≠óÂ±¨ÊÄßÂíåÈ°ûÂà•Ë±êÂØåÂåñ„ÄÇÁÇ∫‰∫Ü‰æøÊñº‰ΩøÁî® DTGBÔºåÊàëÂÄëÊ†πÊìöÂõõÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑÁî®‰æãË®≠Ë®à‰∫ÜÊ®ôÊ∫ñÂåñÁöÑË©ï‰º∞Á®ãÂ∫èÔºöÊú™‰æÜÈÄ£ÁµêÈ†êÊ∏¨„ÄÅÁõÆÁöÑÂú∞ÁØÄÈªûÊ™¢Á¥¢„ÄÅÈÇäÁ∑£ÂàÜÈ°ûÂíåÊñáÂ≠óÈóú‰øÇÁîüÊàê„ÄÇÈÄô‰∫õ‰ªªÂãôË¶ÅÊ±ÇÊ®°ÂûãÂêåÊôÇÁêÜËß£ÂãïÊÖãÂúñË°®ÁµêÊßãÂíåËá™ÁÑ∂Ë™ûË®ÄÔºåÁ™ÅÈ°Ø‰∫Ü DyTAGs Â∏∂‰æÜÁöÑÁç®ÁâπÊåëÊà∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞ç DTGB ÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂü∫Ê∫ñÂØ¶È©óÔºåË©ï‰º∞‰∫Ü 7 Á®ÆÊµÅË°åÁöÑÂãïÊÖãÂúñË°®Â≠∏ÁøíÊºîÁÆóÊ≥ïÂèäÂÖ∂‰ΩøÁî® LLM ÂµåÂÖ•ÈÅ©ÊáâÊñáÂ≠óÂ±¨ÊÄßÁöÑËÆäÈ´îÔºå‰ª•Âèä 6 Á®ÆÂº∑Â§ßÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫‰∫ÜÁèæÊúâÊ®°ÂûãÂú®ËôïÁêÜ DyTAGs ÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàëÂÄëÁöÑÂàÜÊûê‰πüË≠âÊòé‰∫Ü DTGB Âú®Á†îÁ©∂ÁµêÊßãÂíåÊñáÂ≠óÂãïÊÖãÁöÑÁµêÂêàÊñπÈù¢ÁöÑÊïàÁî®„ÄÇÊâÄÊèêÂá∫ÁöÑ DTGB ‰øÉÈÄ≤‰∫ÜÂ∞ç DyTAGs ÂèäÂÖ∂Âª£Ê≥õÊáâÁî®ÁöÑÁ†îÁ©∂„ÄÇÂÆÉÁÇ∫Ë©ï‰º∞ÂíåÊé®ÈÄ≤Ê®°Âûã‰ª•ËôïÁêÜÂãïÊÖãÂúñË°®ÁµêÊßãÂíåËá™ÁÑ∂Ë™ûË®Ä‰πãÈñìÁöÑ‰∫§‰∫í‰ΩúÁî®Êèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂü∫Ê∫ñ„ÄÇË≥áÊñôÈõÜÂíåÂéüÂßãÁ¢ºÂèØÂú® https://github.com/zjs123/DTGB ÂèñÂæó„ÄÇ</paragraph>

##### **UniGLM: Training One Unified Language Model for Text-Attributed Graphs**
2406.12052v1 by Yi Fang, Dongzhe Fan, Sirui Ding, Ninghao Liu, Qiaoyu Tan

Representation learning on text-attributed graphs (TAGs), where nodes are
represented by textual descriptions, is crucial for textual and relational
knowledge systems and recommendation systems. Currently, state-of-the-art
embedding methods for TAGs primarily focus on fine-tuning language models
(e.g., BERT) using structure-aware training signals. While effective, these
methods are tailored for individual TAG and cannot generalize across various
graph scenarios. Given the shared textual space, leveraging multiple TAGs for
joint fine-tuning, aligning text and graph structure from different aspects,
would be more beneficial. Motivated by this, we introduce a novel Unified Graph
Language Model (UniGLM) framework, the first graph embedding model that
generalizes well to both in-domain and cross-domain TAGs. Specifically, UniGLM
is trained over multiple TAGs with different domains and scales using
self-supervised contrastive learning. UniGLM includes an adaptive positive
sample selection technique for identifying structurally similar nodes and a
lazy contrastive module that is devised to accelerate training by minimizing
repetitive encoding calculations. Extensive empirical results across 9
benchmark TAGs demonstrate UniGLM's efficacy against leading embedding
baselines in terms of generalization (various downstream tasks and backbones)
and transfer learning (in and out of domain scenarios). The code is available
at https://github.com/NYUSHCS/UniGLM.

ÊëòË¶ÅÔºöÂú®ÊñáÂ≠óÂ±ûÊÄßÂõæ (TAG) ‰∏äÁöÑË°®ÂæÅÂ≠¶‰π†ÔºåÂÖ∂‰∏≠ËäÇÁÇπÁî±ÊñáÂ≠óÊèèËø∞Ë°®Á§∫ÔºåÂØπ‰∫éÊñáÂ≠óÂíåÂÖ≥Á≥ªÁü•ËØÜÁ≥ªÁªü‰ª•ÂèäÊé®ËçêÁ≥ªÁªüËá≥ÂÖ≥ÈáçË¶Å„ÄÇÁõÆÂâçÔºåTAG ÁöÑÊúÄÂÖàËøõÂµåÂÖ•ÊñπÊ≥ï‰∏ªË¶ÅÈõÜ‰∏≠‰∫é‰ΩøÁî®ÁªìÊûÑÊÑüÁü•ËÆ≠ÁªÉ‰ø°Âè∑ÂæÆË∞ÉËØ≠Ë®ÄÊ®°ÂûãÔºà‰æãÂ¶ÇÔºåBERTÔºâ„ÄÇËôΩÁÑ∂ÊúâÊïàÔºå‰ΩÜËøô‰∫õÊñπÊ≥ïÊòØÈíàÂØπÂçï‰∏™ TAG ÈáèË∫´ÂÆöÂà∂ÁöÑÔºåÂπ∂‰∏îÊó†Ê≥ïÊ¶ÇÊã¨Âà∞ÂêÑÁßçÂõæÂú∫ÊôØ„ÄÇÈâ¥‰∫éÂÖ±‰∫´ÁöÑÊñáÊú¨Á©∫Èó¥ÔºåÂà©Áî®Â§ö‰∏™ TAG ËøõË°åËÅîÂêàÂæÆË∞ÉÔºå‰ªé‰∏çÂêåÊñπÈù¢Ë∞ÉÊï¥ÊñáÊú¨ÂíåÂõæÁªìÊûÑÔºåÂ∞ÜÊõ¥ÊúâÁõä„ÄÇÂèóÊ≠§ÂêØÂèëÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÁªü‰∏ÄÂõæËØ≠Ë®ÄÊ®°Âûã (UniGLM) Ê°ÜÊû∂ÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Âú®ÂüüÂÜÖÂíåË∑®Âüü TAG ‰∏≠ÈÉΩËÉΩÂæàÂ•ΩÂú∞Ê¶ÇÊã¨ÁöÑÂõæÂµåÂÖ•Ê®°Âûã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåUniGLM ‰ΩøÁî®Ëá™ÁõëÁù£ÂØπÊØîÂ≠¶‰π†Âú®ÂÖ∑Êúâ‰∏çÂêåÂüüÂíåËßÑÊ®°ÁöÑÂ§ö‰∏™ TAG ‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇUniGLM ÂåÖÊã¨‰∏ÄÁßçËá™ÈÄÇÂ∫îÊ≠£Ê†∑Êú¨ÈÄâÊã©ÊäÄÊúØÔºåÁî®‰∫éËØÜÂà´ÁªìÊûÑÁõ∏‰ººÁöÑËäÇÁÇπÔºå‰ª•Âèä‰∏Ä‰∏™Âª∂ËøüÂØπÊØîÊ®°ÂùóÔºåËØ•Ê®°ÂùóÊó®Âú®ÈÄöËøáÊúÄÂ∞èÂåñÈáçÂ§çÁºñÁ†ÅËÆ°ÁÆóÊù•Âä†ÈÄüËÆ≠ÁªÉ„ÄÇË∑® 9 ‰∏™Âü∫ÂáÜ TAG ÁöÑÂπøÊ≥õÂÆûËØÅÁªìÊûúËØÅÊòé‰∫Ü UniGLM Âú®Ê≥õÂåñÔºàÂêÑÁßç‰∏ãÊ∏∏‰ªªÂä°Âíå‰∏ªÂπ≤ÔºâÂíåËøÅÁßªÂ≠¶‰π†ÔºàÂüüÂÜÖÂíåÂüüÂ§ñÂú∫ÊôØÔºâÊñπÈù¢Áõ∏ÂØπ‰∫éÈ¢ÜÂÖàÂµåÂÖ•Âü∫ÂáÜÁöÑÂäüÊïà„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/NYUSHCS/UniGLM Ëé∑Âæó„ÄÇ

##### **GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models**
2406.11945v1 by Yi Fang, Dongzhe Fan, Daochen Zha, Qiaoyu Tan

This work studies self-supervised graph learning for text-attributed graphs
(TAGs) where nodes are represented by textual attributes. Unlike traditional
graph contrastive methods that perturb the numerical feature space and alter
the graph's topological structure, we aim to improve view generation through
language supervision. This is driven by the prevalence of textual attributes in
real applications, which complement graph structures with rich semantic
information. However, this presents challenges because of two major reasons.
First, text attributes often vary in length and quality, making it difficulty
to perturb raw text descriptions without altering their original semantic
meanings. Second, although text attributes complement graph structures, they
are not inherently well-aligned. To bridge the gap, we introduce GAugLLM, a
novel framework for augmenting TAGs. It leverages advanced large language
models like Mistral to enhance self-supervised graph learning. Specifically, we
introduce a mixture-of-prompt-expert technique to generate augmented node
features. This approach adaptively maps multiple prompt experts, each of which
modifies raw text attributes using prompt engineering, into numerical feature
space. Additionally, we devise a collaborative edge modifier to leverage
structural and textual commonalities, enhancing edge augmentation by examining
or building connections between nodes. Empirical results across five benchmark
datasets spanning various domains underscore our framework's ability to enhance
the performance of leading contrastive methods as a plug-in tool. Notably, we
observe that the augmented features and graph structure can also enhance the
performance of standard generative methods, as well as popular graph neural
networks. The open-sourced implementation of our GAugLLM is available at
Github.

ÊëòË¶ÅÔºö<paragraph>Êú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÊñáÂ≠óÂ±¨ÊÄßÂúñ (TAG) ÁöÑËá™ÊàëÁõ£Áù£ÂúñÂ≠∏ÁøíÔºåÂÖ∂‰∏≠ÁØÄÈªûÁî±ÊñáÂ≠óÂ±¨ÊÄßË°®Á§∫„ÄÇËàáÊìæÂãïÊï∏ÂÄºÁâπÂæµÁ©∫ÈñìÂíåÊîπËÆäÂúñÂΩ¢ÊãìÊí≤ÁµêÊßãÁöÑÂÇ≥Áµ±ÂúñÂΩ¢Â∞çÊØîÊñπÊ≥ï‰∏çÂêåÔºåÊàëÂÄëÊó®Âú®ÈÄèÈÅéË™ûË®ÄÁõ£Áù£‰æÜÊîπÂñÑË¶ñÂúñÁîüÊàê„ÄÇÈÄôÊòØÂõ†ÁÇ∫ÊñáÂ≠óÂ±¨ÊÄßÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÂæàÊôÆÈÅçÔºåÂÆÉ‰ª•Ë±êÂØåÁöÑË™ûÁæ©Ë≥áË®äË£úÂÖÖÂúñÂΩ¢ÁµêÊßã„ÄÇÁÑ∂ËÄåÔºåÈÄôÊúÉÂ∏∂‰æÜÊåëÊà∞ÔºåÂéüÂõ†ÊúâÂÖ©ÂÄã„ÄÇÈ¶ñÂÖàÔºåÊñáÂ≠óÂ±¨ÊÄßÈÄöÂ∏∏Èï∑Â∫¶ÂíåÂìÅË≥™‰∏çÂêåÔºåÈÄô‰ΩøÂæóÂú®‰∏çÊîπËÆäÂéüÂßãË™ûÁæ©ÊÑèÁæ©ÁöÑÊÉÖÊ≥Å‰∏ãÊìæÂãïÂéüÂßãÊñáÂ≠óÊèèËø∞ËÆäÂæóÂõ∞Èõ£„ÄÇÂÖ∂Ê¨°ÔºåÂÑòÁÆ°ÊñáÂ≠óÂ±¨ÊÄßË£úÂÖÖ‰∫ÜÂúñÂΩ¢ÁµêÊßãÔºå‰ΩÜÂÆÉÂÄë‰∏¶ÈùûÂ§©ÁîüÂ∞±ÂæàÂ•ΩÂú∞Â∞çÈΩä„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü GAugLLMÔºåÈÄôÊòØ‰∏ÄÂÄãÁî®ÊñºÊì¥ÂÖÖ TAG ÁöÑÊñ∞Ê°ÜÊû∂„ÄÇÂÆÉÂà©Áî®‰∫Ü Mistral Á≠âÂÖàÈÄ≤ÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜÂ¢ûÂº∑Ëá™ÊàëÁõ£Áù£ÂúñÂΩ¢Â≠∏Áøí„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊ∑∑ÂêàÊèêÁ§∫Â∞àÂÆ∂ÁöÑÊäÄË°ì‰æÜÁîüÊàêÊì¥ÂÖÖÁöÑÁØÄÈªûÁâπÂæµ„ÄÇÈÄôÁ®ÆÊñπÊ≥ïËá™ÈÅ©ÊáâÂú∞Â∞áÂ§öÂÄãÊèêÁ§∫Â∞àÂÆ∂Êò†Â∞ÑÂà∞Êï∏ÂÄºÁâπÂæµÁ©∫ÈñìÔºåÊØèÂÄãÊèêÁ§∫Â∞àÂÆ∂ÈÉΩ‰ΩøÁî®ÊèêÁ§∫Â∑•Á®ã‰øÆÊîπÂéüÂßãÊñáÂ≠óÂ±¨ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂçî‰ΩúÈÇäÁ∑£‰øÆÊîπÂô®‰æÜÂà©Áî®ÁµêÊßãÂíåÊñáÂ≠óÁöÑÂÖ±ÊÄßÔºåÈÄöÈÅéÊ™¢Êü•ÊàñÂª∫Á´ãÁØÄÈªû‰πãÈñìÁöÑÈÄ£Êé•‰æÜÂ¢ûÂº∑ÈÇäÁ∑£Êì¥ÂÖÖ„ÄÇË∑®Ë∂äÂêÑÁ®ÆÈ†òÂüüÁöÑ‰∫îÂÄãÂü∫Ê∫ñË≥áÊñôÈõÜÁöÑÁ∂ìÈ©óÁµêÊûúÂº∑Ë™ø‰∫ÜÊàëÂÄëÁöÑÊ°ÜÊû∂‰ΩúÁÇ∫Â§ñÊéõÂ∑•ÂÖ∑Â¢ûÂº∑È†òÂÖàÂ∞çÊØîÊñπÊ≥ïÊïàËÉΩÁöÑËÉΩÂäõ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëËßÄÂØüÂà∞Êì¥ÂÖÖÁöÑÁâπÂæµÂíåÂúñÂΩ¢ÁµêÊßã‰πüÂèØ‰ª•Â¢ûÂº∑Ê®ôÊ∫ñÁîüÊàêÊñπÊ≥ï‰ª•ÂèäÊµÅË°åÁöÑÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑ GAugLLM ÁöÑÈñãÊ∫êÂØ¶ÁèæÂèØ‰ª•Âú® Github ‰∏äÊâæÂà∞„ÄÇ</paragraph>

##### **Input Conditioned Graph Generation for Language Agents**
2406.11555v1 by Lukas Vierling, Jie Fu, Kai Chen

Recent progress in Large Language Models (LLMs) and language agents has
demonstrated significant promise for various future applications across
multiple disciplines. While traditional approaches to language agents often
rely on fixed, handcrafted designs, our research aims to develop both learnable
and dynamic agents. Our method uses an existing framework that abstracts
language agents as graphs. Within this graph framework, we aim to learn a model
that can generate edges for every given input to the language agent. This
allows us to generate edges that represent the flow of communication within the
graph based on the given input, thereby adjusting the internal communication of
a language agent. We learn to generate these edges using a pretrained LLM that
is fine-tuned with reinforcement learning. This LLM can be fine-tuned on
several datasets simultaneously, and we hypothesize that the model learns to
adapt to these different domains during training, achieving good overall
performance when encountering data from different domains during deployment. We
demonstrate that our approach surpasses the previous static approach by nearly
6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when
trained with a sparsity-inducing loss. It also performs superior in additional
experiments conducted with the MMLU and Mini Crossword Puzzles datasets. The
code is available at https://github.com/lukasVierling/DynamicGPTSwarm.

ÊëòË¶ÅÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÂíåËØ≠Ë®Ä‰ª£ÁêÜÊúÄËøëÁöÑËøõÂ±ïÂ∑≤Â±ïÁ§∫Âá∫ÂØπË∑®Â§ö‰∏™Â≠¶ÁßëÁöÑÂêÑÁßçÊú™Êù•Â∫îÁî®ÁöÑÈáçÂ§ßÂâçÊôØ„ÄÇËôΩÁÑ∂‰º†ÁªüÁöÑËØ≠Ë®Ä‰ª£ÁêÜÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÂõ∫ÂÆöÁöÑÊâãÂ∑•ËÆæËÆ°Ôºå‰ΩÜÊàë‰ª¨ÁöÑÁ†îÁ©∂Êó®Âú®ÂºÄÂèëÂèØÂ≠¶‰π†ÂíåÂä®ÊÄÅÁöÑ‰ª£ÁêÜ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ΩøÁî®‰∫Ü‰∏Ä‰∏™Áé∞ÊúâÁöÑÊ°ÜÊû∂ÔºåÂ∞ÜËØ≠Ë®Ä‰ª£ÁêÜÊäΩË±°‰∏∫Âõæ„ÄÇÂú®Ëøô‰∏™ÂõæÊ°ÜÊû∂ÂÜÖÔºåÊàë‰ª¨Êó®Âú®Â≠¶‰π†‰∏Ä‰∏™Ê®°ÂûãÔºåËØ•Ê®°ÂûãÂèØ‰ª•‰∏∫ËØ≠Ë®Ä‰ª£ÁêÜÁöÑÊØè‰∏™ÁªôÂÆöËæìÂÖ•ÁîüÊàêËæπ„ÄÇËøô‰ΩøÊàë‰ª¨ËÉΩÂ§üÁîüÊàê‰ª£Ë°®Âõæ‰∏≠Âü∫‰∫éÁªôÂÆöËæìÂÖ•ÁöÑÈÄö‰ø°ÊµÅÁöÑËæπÔºå‰ªéËÄåË∞ÉÊï¥ËØ≠Ë®Ä‰ª£ÁêÜÁöÑÂÜÖÈÉ®ÈÄö‰ø°„ÄÇÊàë‰ª¨Â≠¶‰π†‰ΩøÁî®ÁªèËøáÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÁöÑÈ¢ÑËÆ≠ÁªÉ LLM Êù•ÁîüÊàêËøô‰∫õËæπ„ÄÇËØ• LLM ÂèØ‰ª•ÂêåÊó∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞ÉÔºåÊàë‰ª¨ÂÅáËÆæËØ•Ê®°ÂûãÂú®ËÆ≠ÁªÉÊúüÈó¥Â≠¶‰π†ÈÄÇÂ∫îËøô‰∫õ‰∏çÂêåÁöÑÂüüÔºåÂú®ÈÉ®ÁΩ≤ÊúüÈó¥ÈÅáÂà∞Êù•Ëá™‰∏çÂêåÂüüÁöÑÊï∞ÊçÆÊó∂ÂÆûÁé∞ËâØÂ•ΩÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇÊàë‰ª¨ËØÅÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú® MMLU Âíå CMMLU ÁöÑÁªÑÂêàÊï∞ÊçÆÈõÜ‰∏äÊØîÂÖàÂâçÁöÑÈùôÊÄÅÊñπÊ≥ïÈ´òÂá∫Ëøë 6% ÁöÑÂáÜÁ°ÆÂ∫¶ÔºåÂπ∂‰∏îÂú®‰ΩøÁî®Á®ÄÁñèÊÄßËØ±ÂØºÊçüÂ§±ËøõË°åËÆ≠ÁªÉÊó∂È´òÂá∫ 10% ‰ª•‰∏ä„ÄÇÂÆÉËøòÂú®‰ΩøÁî® MMLU ÂíåËø∑‰Ω†Â°´Â≠óÊ∏∏ÊàèÊï∞ÊçÆÈõÜËøõË°åÁöÑÂÖ∂‰ªñÂÆûÈ™å‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/lukasVierling/DynamicGPTSwarm Ëé∑Âæó„ÄÇ

##### **Large Language Models and Knowledge Graphs for Astronomical Entity Disambiguation**
2406.11400v1 by Golnaz Shapurian

This paper presents an experiment conducted during a hackathon, focusing on
using large language models (LLMs) and knowledge graph clustering to extract
entities and relationships from astronomical text. The study demonstrates an
approach to disambiguate entities that can appear in various contexts within
the astronomical domain. By collecting excerpts around specific entities and
leveraging the GPT-4 language model, relevant entities and relationships are
extracted. The extracted information is then used to construct a knowledge
graph, which is clustered using the Leiden algorithm. The resulting Leiden
communities are utilized to identify the percentage of association of unknown
excerpts to each community, thereby enabling disambiguation. The experiment
showcases the potential of combining LLMs and knowledge graph clustering
techniques for information extraction in astronomical research. The results
highlight the effectiveness of the approach in identifying and disambiguating
entities, as well as grouping them into meaningful clusters based on their
relationships.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁ¥π‰∫ÜÂú®ÈªëÂÆ¢È¶¨ÊãâÊùæ‰∏≠ÈÄ≤Ë°åÁöÑÂØ¶È©óÔºåÈáçÈªûÂú®Êñº‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁü•Ë≠òÂúñË≠úËÅöÈ°ûÔºåÂæûÂ§©ÊñáÊñáÊú¨‰∏≠ÊèêÂèñÂØ¶È´îÂíåÈóú‰øÇ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Â±ïÁ§∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂèØ‰ª•Ê∂àÈô§Âú®Â§©ÊñáÈ†òÂüü‰∏≠ÂêÑÁ®ÆË™ûÂ¢É‰∏≠Âá∫ÁèæÁöÑÂØ¶È´îÊ≠ßÁæ©ÊÄß„ÄÇÈÄèÈÅéÊî∂ÈõÜÁâπÂÆöÂØ¶È´îÂë®ÂúçÁöÑÊëòÈåÑÔºå‰∏¶Âà©Áî® GPT-4 Ë™ûË®ÄÊ®°ÂûãÔºåÂèØ‰ª•ÊèêÂèñÁõ∏ÈóúÂØ¶È´îÂíåÈóú‰øÇ„ÄÇÁÑ∂Âæå‰ΩøÁî®ÊèêÂèñÁöÑË≥áË®ä‰æÜÂª∫ÊßãÁü•Ë≠òÂúñË≠úÔºå‰∏¶‰ΩøÁî® Leiden ÊºîÁÆóÊ≥ïÈÄ≤Ë°åËÅöÈ°û„ÄÇÁµêÊûúÁî¢ÁîüÁöÑ Leiden Á§æÁæ§Áî®ÊñºË≠òÂà•Êú™Áü•ÊëòÈåÑËàáÊØèÂÄãÁ§æÁæ§ÁöÑÈóúËÅØÁôæÂàÜÊØîÔºåÂæûËÄåÊ∂àÈô§Ê≠ßÁæ©ÊÄß„ÄÇË©≤ÂØ¶È©óÂ±ïÁ§∫‰∫ÜÁµêÂêà LLM ÂíåÁü•Ë≠òÂúñË≠úËÅöÈ°ûÊäÄË°ìÂú®Â§©ÊñáÁ†îÁ©∂‰∏≠ÈÄ≤Ë°åË≥áË®äÊèêÂèñÁöÑÊΩõÂäõ„ÄÇÁµêÊûúÁ™ÅÈ°Ø‰∫ÜË©≤ÊñπÊ≥ïÂú®Ë≠òÂà•ÂíåÊ∂àÈô§ÂØ¶È´îÊ≠ßÁæ©ÊÄßÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºå‰ª•ÂèäÊ†πÊìöÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇÂ∞áÂØ¶È´îÂàÜÁµÑÂà∞ÊúâÊÑèÁæ©ÁöÑÁæ§ÈõÜ‰∏≠„ÄÇ

##### **How Good are LLMs at Relation Extraction under Low-Resource Scenario? Comprehensive Evaluation**
2406.11162v2 by Dawulie Jinensibieke, Mieradilijiang Maimaiti, Wentao Xiao, Yuanhang Zheng, Xiaobo Wang

Relation Extraction (RE) serves as a crucial technology for transforming
unstructured text into structured information, especially within the framework
of Knowledge Graph development. Its importance is emphasized by its essential
role in various downstream tasks. Besides the conventional RE methods which are
based on neural networks and pre-trained language models, large language models
(LLMs) are also utilized in the research field of RE. However, on low-resource
languages (LRLs), both conventional RE methods and LLM-based methods perform
poorly on RE due to the data scarcity issues. To this end, this paper
constructs low-resource relation extraction datasets in 10 LRLs in three
regions (Central Asia, Southeast Asia and Middle East). The corpora are
constructed by translating the original publicly available English RE datasets
(NYT10, FewRel and CrossRE) using an effective multilingual machine
translation. Then, we use the language perplexity (PPL) to filter out the
low-quality data from the translated datasets. Finally, we conduct an empirical
study and validate the performance of several open-source LLMs on these
generated LRL RE datasets.

ÊëòË¶ÅÔºöÈóú‰øÇÊäΩÂèñ (RE) ÊòØ‰∏ÄÁ®ÆÂ∞áÈùûÁµêÊßãÂåñÊñáÂ≠óËΩâÊèõÁÇ∫ÁµêÊßãÂåñË≥áË®äÁöÑÈóúÈçµÊäÄË°ìÔºåÁâπÂà•ÊòØÂú®Áü•Ë≠òÂúñË≠úÈñãÁôºÁöÑÊû∂Êßã‰∏≠„ÄÇÂÖ∂ÈáçË¶ÅÊÄßÂú®ÊñºÂÆÉÂú®ÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÊâÆÊºîËëó‰∏çÂèØÊàñÁº∫ÁöÑËßíËâ≤„ÄÇÈô§‰∫ÜÂü∫ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÁöÑÂÇ≥Áµ± RE ÊñπÊ≥ï‰πãÂ§ñÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰πüË¢´Áî®Êñº RE ÁöÑÁ†îÁ©∂È†òÂüü„ÄÇÁÑ∂ËÄåÔºåÂú®‰ΩéË≥áÊ∫êË™ûË®Ä (LRL) ‰∏≠ÔºåÁî±ÊñºË≥áÊñôÁ®ÄÂ∞ëÁöÑÂïèÈ°åÔºåÂÇ≥Áµ±ÁöÑ RE ÊñπÊ≥ïÂíåÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÂú® RE ‰∏äÁöÑË°®ÁèæÈÉΩÂæàÂ∑Æ„ÄÇÊúâÈëëÊñºÊ≠§ÔºåÊú¨ÊñáÂú®‰∏âÂÄãÂú∞ÂçÄÔºà‰∏≠‰∫û„ÄÅÊù±Âçó‰∫ûÂíå‰∏≠Êù±ÔºâÁöÑ 10 Á®Æ LRL ‰∏≠Âª∫Êßã‰∫Ü‰ΩéË≥áÊ∫êÈóú‰øÇÊäΩÂèñË≥áÊñôÈõÜ„ÄÇÈÄô‰∫õË™ûÊñôÂ∫´ÊòØÈÄèÈÅé‰ΩøÁî®‰∏ÄÁ®ÆÊúâÊïàÁöÑÂ§öË™ûË®ÄÊ©üÂô®ÁøªË≠Ø‰æÜÁøªË≠ØÂéüÂßãÂÖ¨ÈñãÁöÑËã±Êñá RE Ë≥áÊñôÈõÜÔºàNYT10„ÄÅFewRel Âíå CrossREÔºâËÄåÂª∫ÊßãÁöÑ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄë‰ΩøÁî®Ë™ûË®ÄÂõ∞ÊÉëÂ∫¶ (PPL) ÂæûÁøªË≠ØÂæåÁöÑË≥áÊñôÈõÜ‰∏≠ÁØ©ÈÅ∏Âá∫‰ΩéÂìÅË≥™ÁöÑË≥áÊñô„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈÄ≤Ë°å‰∏ÄÈ†ÖÂØ¶Ë≠âÁ†îÁ©∂Ôºå‰∏¶È©óË≠â‰∫ÜÂπæÂÄãÈñãÊ∫ê LLM Âú®ÈÄô‰∫õÁîüÊàêÁöÑ LRL RE Ë≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩ„ÄÇ

##### **Contextual Knowledge Graph**
2406.11160v2 by Chengjin Xu, Muzhi Li, Cehao Yang, Xuhui Jiang, Lumingyuan Tang, Yiyan Qi, Jian Guo

Knowledge Graphs (KGs) are foundational structures in many AI applications,
representing entities and their interrelations through triples. However,
triple-based KGs lack the contextual information of relational knowledge, like
temporal dynamics and provenance details, which are crucial for comprehensive
knowledge representation and effective reasoning. Instead, \textbf{Contextual
Knowledge Graphs} (CKGs) expand upon the conventional structure by
incorporating additional information such as time validity, geographic
location, and source provenance. This integration provides a more nuanced and
accurate understanding of knowledge, enabling KGs to offer richer insights and
support more sophisticated reasoning processes. In this work, we first discuss
the inherent limitations of triple-based KGs and introduce the concept of
contextual KGs, highlighting their advantages in knowledge representation and
reasoning. We then present \textbf{KGR$^3$, a context-enriched KG reasoning
paradigm} that leverages large language models (LLMs) to retrieve candidate
entities and related contexts, rank them based on the retrieved information,
and reason whether sufficient information has been obtained to answer a query.
Our experimental results demonstrate that KGR$^3$ significantly improves
performance on KG completion (KGC) and KG question answering (KGQA) tasks,
validating the effectiveness of incorporating contextual information on KG
representation and reasoning.

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ÊòØË®±Â§ö AI ÊáâÁî®‰∏≠ÁöÑÂü∫Á§éÁµêÊßãÔºåÈÄèÈÅé‰∏âÂÖÉÁµÑË°®Á§∫ÂØ¶È´îÂèäÂÖ∂Áõ∏‰∫íÈóú‰øÇ„ÄÇÁÑ∂ËÄåÔºåÂü∫Êñº‰∏âÂÖÉÁµÑÁöÑ KG Áº∫‰πèÈóú‰øÇÁü•Ë≠òÁöÑËÉåÊôØË≥áË®äÔºå‰æãÂ¶ÇÊôÇÈñìÂãïÊÖãÂíå‰æÜÊ∫êÁ¥∞ÁØÄÔºåÈÄô‰∫õË≥áË®äÂ∞çÊñºÂÖ®Èù¢ÁöÑÁü•Ë≠òË°®ÂæµÂíåÊúâÊïàÁöÑÊé®ÁêÜËá≥ÈóúÈáçË¶Å„ÄÇÁõ∏ÂèçÂú∞Ôºå**ËÉåÊôØÁü•Ë≠òÂúñË≠ú** (CKG) ÈÄèÈÅéÁ¥çÂÖ•ÊôÇÈñìÊúâÊïàÊÄß„ÄÅÂú∞ÁêÜ‰ΩçÁΩÆÂíå‰æÜÊ∫êÂá∫ËôïÁ≠âÈ°çÂ§ñË≥áË®äÔºåÊì¥Â±ï‰∫ÜÂÇ≥Áµ±ÁöÑÁµêÊßã„ÄÇÈÄôÁ®ÆÊï¥ÂêàÊèê‰æõ‰∫ÜÂ∞çÁü•Ë≠òÊõ¥Á¥∞Á∑ª‰∏îÊ∫ñÁ¢∫ÁöÑÁêÜËß£Ôºå‰Ωø KG ËÉΩÊèê‰æõÊõ¥Ë±êÂØåÁöÑË¶ãËß£‰∏¶ÊîØÊè¥Êõ¥Á≤æÂØÜÁöÑÊé®ÁêÜÈÅéÁ®ã„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàË®éË´ñÂü∫Êñº‰∏âÂÖÉÁµÑÁöÑ KG ÁöÑÂÖßÂú®ÈôêÂà∂Ôºå‰∏¶‰ªãÁ¥πËÉåÊôØ KG ÁöÑÊ¶ÇÂøµÔºåÂº∑Ë™øÂÆÉÂÄëÂú®Áü•Ë≠òË°®ÂæµÂíåÊé®ÁêÜÊñπÈù¢ÁöÑÂÑ™Âã¢„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫**KGR$^3$Ôºå‰∏ÄÁ®ÆËÉåÊôØË±êÂØåÁöÑ KG Êé®ÁêÜÁØÑ‰æã**ÔºåÂÆÉÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜÊì∑ÂèñÂÄôÈÅ∏ÂØ¶È´îÂíåÁõ∏ÈóúËÉåÊôØÔºåÊ†πÊìöÊì∑ÂèñÁöÑË≥áË®äÂ∞çÂÆÉÂÄëÈÄ≤Ë°åÊéíÂêçÔºå‰∏¶Êé®Ë´ñÊòØÂê¶Â∑≤Áç≤ÂæóË∂≥Â§†ÁöÑË≥áË®ä‰æÜÂõûÁ≠îÊü•Ë©¢„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåKGR$^3$ Â§ßÂπÖÊèêÂçá‰∫Ü KG ÂÆåÊàê (KGC) Âíå KG ÂïèÁ≠î (KGQA) ‰ªªÂãôÁöÑÊïàËÉΩÔºåÈ©óË≠â‰∫ÜÂú® KG Ë°®ÂæµÂíåÊé®ÁêÜ‰∏≠Á¥çÂÖ•ËÉåÊôØË≥áË®äÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Are Large Language Models a Good Replacement of Taxonomies?**
2406.11131v2 by Yushi Sun, Hao Xin, Kai Sun, Yifan Ethan Xu, Xiao Yang, Xin Luna Dong, Nan Tang, Lei Chen

Large language models (LLMs) demonstrate an impressive ability to internalize
knowledge and answer natural language questions. Although previous studies
validate that LLMs perform well on general knowledge while presenting poor
performance on long-tail nuanced knowledge, the community is still doubtful
about whether the traditional knowledge graphs should be replaced by LLMs. In
this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made
obsolete by LLMs. Intuitively, LLMs should perform well on common taxonomies
and at taxonomy levels that are common to people. Unfortunately, there lacks a
comprehensive benchmark that evaluates the LLMs over a wide range of taxonomies
from common to specialized domains and at levels from root to leaf so that we
can draw a confident conclusion. To narrow the research gap, we constructed a
novel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to
evaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten
representative taxonomies from common to specialized domains with in-depth
experiments of different levels of entities in this taxonomy from root to leaf.
Our comprehensive experiments of eighteen state-of-the-art LLMs under three
prompting settings validate that LLMs can still not well capture the knowledge
of specialized taxonomies and leaf-level entities.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â±ïÁ§∫‰∫ÜÂ∞áÁü•Ë≠òÂÖßÂåñ‰∏¶ÂõûÁ≠îËá™ÁÑ∂Ë™ûË®ÄÂïèÈ°åÁöÑÈ©ö‰∫∫ËÉΩÂäõ„ÄÇÂÑòÁÆ°ÂÖàÂâçÁöÑÁ†îÁ©∂È©óË≠â‰∫Ü LLM Âú®‰∏ÄËà¨Áü•Ë≠ò‰∏äË°®ÁèæËâØÂ•ΩÔºå‰ΩÜÂú®Èï∑Â∞æÁ¥∞ÂæÆÁü•Ë≠ò‰∏äË°®Áèæ‰∏ç‰Ω≥Ôºå‰ΩÜÁ§æÁæ§‰ªçÁÑ∂Êá∑ÁñëÂÇ≥Áµ±Áü•Ë≠òÂúñË≠úÊòØÂê¶ÊáâË¢´ LLM Âèñ‰ª£„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÁü•Ë≠òÂúñË≠úÁöÑÊû∂Êßã (Âç≥ÂàÜÈ°ûÊ≥ï) ÊòØÂê¶Ë¢´ LLM Âèñ‰ª£„ÄÇÁõ¥ËßÄ‰∏äÔºåLLM ÊáâÂú®Â∏∏Ë¶ãÂàÜÈ°ûÊ≥ïÂíåÂ∞ç‰∫∫ÂÄë‰æÜË™™Â∏∏Ë¶ãÁöÑÂàÜÈ°ûÊ≥ïÂ±§Á¥ö‰∏≠Ë°®ÁèæËâØÂ•Ω„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÁº∫‰πè‰∏ÄÂÄãÁ∂úÂêàÂü∫Ê∫ñ‰æÜË©ï‰º∞ LLM Âú®Âæû‰∏ÄËà¨Âà∞Â∞àÊ•≠È†òÂüüÁöÑÂª£Ê≥õÂàÜÈ°ûÊ≥ï‰ª•ÂèäÂæûÊ†πÂà∞ËëâÁöÑÂ±§Á¥öÔºå‰ª•‰æøÊàëÂÄëÂèØ‰ª•ÂæóÂá∫Á¢∫‰ø°ÁöÑÁµêË´ñ„ÄÇÁÇ∫‰∫ÜÁ∏ÆÂ∞èÁ†îÁ©∂Â∑ÆË∑ùÔºåÊàëÂÄëÊßãÂª∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ TaxoGlimpse ÁöÑÊñ∞ÂàÜÈ°ûÊ≥ïÂ±§Á¥öÁµêÊßãÁôºÁèæÂü∫Ê∫ñÔºå‰ª•Ë©ï‰º∞ LLM Âú®ÂàÜÈ°ûÊ≥ï‰∏äÁöÑË°®Áèæ„ÄÇTaxoGlimpse Ê∂µËìã‰∫ÜÂæû‰∏ÄËà¨Âà∞Â∞àÊ•≠È†òÂüüÁöÑÂçÅÂÄã‰ª£Ë°®ÊÄßÂàÜÈ°ûÊ≥ïÔºå‰∏¶Â∞çË©≤ÂàÜÈ°ûÊ≥ï‰∏≠ÂæûÊ†πÂà∞ËëâÁöÑ‰∏çÂêåÂ±§Á¥öÂØ¶È´îÈÄ≤Ë°åÊ∑±ÂÖ•ÂØ¶È©ó„ÄÇÊàëÂÄëÂ∞ç 18 ÂÄãÊúÄÂÖàÈÄ≤ÁöÑ LLM Âú®‰∏âÁ®ÆÊèêÁ§∫Ë®≠ÂÆö‰∏ãÁöÑÁ∂úÂêàÂØ¶È©óÈ©óË≠â‰∫Ü LLM ‰ªçÁÑ∂ÁÑ°Ê≥ïÂæàÂ•ΩÂú∞ÊéåÊè°Â∞àÊ•≠ÂàÜÈ°ûÊ≥ïÂíåËëâÁ¥öÂØ¶È´îÁöÑÁü•Ë≠ò„ÄÇ

##### **DocNet: Semantic Structure in Inductive Bias Detection Models**
2406.10965v1 by Jessica Zhu, Iain Cruickshank, Michel Cukier

News will have biases so long as people have opinions. However, as social
media becomes the primary entry point for news and partisan gaps increase, it
is increasingly important for informed citizens to be able to identify bias.
People will be able to take action to avoid polarizing echo chambers if they
know how the news they are consuming is biased. In this paper, we explore an
often overlooked aspect of bias detection in documents: the semantic structure
of news articles. We present DocNet, a novel, inductive, and low-resource
document embedding and bias detection model that outperforms large language
models. We also demonstrate that the semantic structure of news articles from
opposing partisan sides, as represented in document-level graph embeddings,
have significant similarities. These results can be used to advance bias
detection in low-resource environments. Our code and data are made available at
https://github.com/nlpresearchanon.

ÊëòË¶ÅÔºöÊñ∞ËÅûÊúÉÂ≠òÂú®ÂÅèË¶ãÔºåÂè™Ë¶Å‰∫∫ÂÄëÊúâÊÑèË¶ã„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁ§æÁæ§Â™íÈ´îÊàêÁÇ∫Êñ∞ËÅûÁöÑ‰∏ªË¶ÅÂÖ•Âè£Ôºå‰∏îÈª®Ê¥æÂ∑ÆË∑ùÊì¥Â§ßÔºåÂ∞çÊñºÊúâÁü•Ë≠òÁöÑÂÖ¨Ê∞ë‰æÜË™™ÔºåËÉΩÂ§†Ëæ®Ë≠òÂÅèË¶ãËÆäÂæóÊÑà‰æÜÊÑàÈáçË¶Å„ÄÇÂ¶ÇÊûú‰∫∫ÂÄëÁü•ÈÅì‰ªñÂÄëÊâÄÊé•Êî∂ÁöÑÊñ∞ËÅûÂ¶Ç‰ΩïÂÅèÈ†óÔºå‰ªñÂÄëÂ∞±ËÉΩÊé°ÂèñË°åÂãï‰æÜÈÅøÂÖçÂÖ©Ê•µÂåñÁöÑÂêåÊ∫´Â±§„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÊñá‰ª∂ÂÅèË¶ãÂÅµÊ∏¨‰∏≠Á∂ìÂ∏∏Ë¢´ÂøΩÁï•ÁöÑ‰∏ÄÈù¢ÔºöÊñ∞ËÅûÊñáÁ´†ÁöÑË™ûÊÑèÁµêÊßã„ÄÇÊàëÂÄëÊèêÂá∫ DocNetÔºå‰∏ÄÂÄãÊñ∞Á©é„ÄÅÊ≠∏Á¥ç‰∏î‰ΩéË≥áÊ∫êÁöÑÊñá‰ª∂ÂµåÂÖ•ÂíåÂÅèË¶ãÂÅµÊ∏¨Ê®°ÂûãÔºåÂÖ∂Ë°®ÁèæÂÑ™ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÇÊàëÂÄë‰πüË≠âÊòéÔºå‰æÜËá™Â∞çÁ´ãÈª®Ê¥æÈô£ÁáüÁöÑÊñ∞ËÅûÊñáÁ´†ÁöÑË™ûÊÑèÁµêÊßãÔºåÂ¶ÇÊñá‰ª∂Â±§Á¥öÂúñÂµåÂÖ•‰∏≠ÊâÄÂëàÁèæÁöÑÔºåÂÖ∑ÊúâÈ°ØËëóÁöÑÁõ∏‰ººÊÄß„ÄÇÈÄô‰∫õÁµêÊûúÂèØÁî®ÊñºÊé®ÈÄ≤‰ΩéË≥áÊ∫êÁí∞Â¢É‰∏≠ÁöÑÂÅèË¶ãÂÅµÊ∏¨„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú® https://github.com/nlpresearchanon ÂèñÂæó„ÄÇ

##### **Light Up the Shadows: Enhance Long-Tailed Entity Grounding with Concept-Guided Vision-Language Models**
2406.10902v1 by Yikai Zhang, Qianyu He, Xintao Wang, Siyu Yuan, Jiaqing Liang, Yanghua Xiao

Multi-Modal Knowledge Graphs (MMKGs) have proven valuable for various
downstream tasks. However, scaling them up is challenging because building
large-scale MMKGs often introduces mismatched images (i.e., noise). Most
entities in KGs belong to the long tail, meaning there are few images of them
available online. This scarcity makes it difficult to determine whether a found
image matches the entity. To address this, we draw on the Triangle of Reference
Theory and suggest enhancing vision-language models with concept guidance.
Specifically, we introduce COG, a two-stage framework with COncept-Guided
vision-language models. The framework comprises a Concept Integration module,
which effectively identifies image-text pairs of long-tailed entities, and an
Evidence Fusion module, which offers explainability and enables human
verification. To demonstrate the effectiveness of COG, we create a dataset of
25k image-text pairs of long-tailed entities. Our comprehensive experiments
show that COG not only improves the accuracy of recognizing long-tailed
image-text pairs compared to baselines but also offers flexibility and
explainability.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÁü•ËØÜÂõæË∞± (MMKG) Â∑≤Ë¢´ËØÅÊòéÂØπÂêÑÁßç‰∏ãÊ∏∏‰ªªÂä°ÂæàÊúâ‰ª∑ÂÄº„ÄÇÁÑ∂ËÄåÔºåÊâ©Â±ïÂÆÉ‰ª¨ÂÖ∑ÊúâÊåëÊàòÊÄßÔºåÂõ†‰∏∫ÊûÑÂª∫Â§ßËßÑÊ®° MMKG ÁªèÂ∏∏‰ºöÂºïÂÖ•‰∏çÂåπÈÖçÁöÑÂõæÂÉèÔºàÂç≥Âô™Â£∞Ôºâ„ÄÇÁü•ËØÜÂõæË∞±‰∏≠ÁöÑÂ§ßÂ§öÊï∞ÂÆû‰ΩìÈÉΩÂ±û‰∫éÈïøÂ∞æÔºåËøôÊÑèÂë≥ÁùÄÁΩë‰∏äÂæàÂ∞ëÊúâÂÆÉ‰ª¨ÁöÑÂõæÂÉè„ÄÇËøôÁßçÁ®ÄÁº∫ÊÄß‰ΩøÂæóÈöæ‰ª•Á°ÆÂÆöÊâæÂà∞ÁöÑÂõæÂÉèÊòØÂê¶‰∏éÂÆû‰ΩìÂåπÈÖç„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂÄüÈâ¥‰∫Ü‰∏âËßíÂèÇÁÖßÁêÜËÆ∫ÔºåÂπ∂Âª∫ËÆÆÁî®Ê¶ÇÂøµÊåáÂØºÊù•Â¢ûÂº∫ËßÜËßâËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü COGÔºåËøôÊòØ‰∏Ä‰∏™Â∏¶ÊúâÊ¶ÇÂøµÊåáÂØºËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏§Èò∂ÊÆµÊ°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂ÂåÖÂê´‰∏Ä‰∏™Ê¶ÇÂøµÈõÜÊàêÊ®°ÂùóÔºåËØ•Ê®°ÂùóÊúâÊïàÂú∞ËØÜÂà´ÈïøÂ∞æÂÆû‰ΩìÁöÑÂõæÂÉèÊñáÊú¨ÂØπÔºå‰ª•Âèä‰∏Ä‰∏™ËØÅÊçÆËûçÂêàÊ®°ÂùóÔºåËØ•Ê®°ÂùóÊèê‰æõÂèØËß£ÈáäÊÄßÂπ∂ÊîØÊåÅ‰∫∫Â∑•È™åËØÅ„ÄÇ‰∏∫‰∫ÜËØÅÊòé COG ÁöÑÊúâÊïàÊÄßÔºåÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Áî± 25k ‰∏™ÈïøÂ∞æÂÆû‰ΩìÁöÑÂõæÂÉèÊñáÊú¨ÂØπÁªÑÊàêÁöÑÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨ÁöÑÁªºÂêàÂÆûÈ™åË°®ÊòéÔºå‰∏éÂü∫Á∫øÁõ∏ÊØîÔºåCOG ‰∏ç‰ªÖÊèêÈ´ò‰∫ÜËØÜÂà´ÈïøÂ∞æÂõæÂÉèÊñáÊú¨ÂØπÁöÑÂáÜÁ°ÆÊÄßÔºåËøòÊèê‰æõ‰∫ÜÁÅµÊ¥ªÊÄßÂíåÂèØËß£ÈáäÊÄß„ÄÇ

##### **KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs**
2406.10802v1 by Aihua Pei, Zehua Yang, Shunan Zhu, Ruoxi Cheng, Ju Jia, Lina Wang

Existing frameworks for assessing robustness of large language models (LLMs)
overly depend on specific benchmarks, increasing costs and failing to evaluate
performance of LLMs in professional domains due to dataset limitations. This
paper proposes a framework that systematically evaluates the robustness of LLMs
under adversarial attack scenarios by leveraging knowledge graphs (KGs). Our
framework generates original prompts from the triplets of knowledge graphs and
creates adversarial prompts by poisoning, assessing the robustness of LLMs
through the results of these adversarial attacks. We systematically evaluate
the effectiveness of this framework and its modules. Experiments show that
adversarial robustness of the ChatGPT family ranks as GPT-4-turbo > GPT-4o >
GPT-3.5-turbo, and the robustness of large language models is influenced by the
professional domains in which they operate.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÁî®ÊñºË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á©©ÂÅ•ÊÄßÁöÑÊ°ÜÊû∂ÈÅéÊñº‰æùË≥¥ÁâπÂÆöÂü∫Ê∫ñÔºåÈÄôÊúÉÂ¢ûÂä†ÊàêÊú¨ÔºåËÄå‰∏îÁî±ÊñºË≥áÊñôÈõÜÁöÑÈôêÂà∂ÔºåÁÑ°Ê≥ïË©ï‰º∞ LLM Âú®Â∞àÊ•≠È†òÂüüÁöÑÊïàËÉΩ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÂà©Áî®Áü•Ë≠òÂúñË≠ú (KG) Á≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞ LLM Âú®Â∞çÊäóÊîªÊìäÂ†¥ÊôØ‰∏ãÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÁöÑÊ°ÜÊû∂ÂæûÁü•Ë≠òÂúñË≠úÁöÑ‰∏âÂÖÉÁµÑ‰∏≠Áî¢ÁîüÂéüÂßãÊèêÁ§∫Ôºå‰∏¶ÈÄèÈÅéÊäïÊØíÂª∫Á´ãÂ∞çÊäóÊèêÁ§∫ÔºåÈÄèÈÅéÈÄô‰∫õÂ∞çÊäóÊîªÊìäÁöÑÁµêÊûú‰æÜË©ï‰º∞ LLM ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫ÜÊ≠§Ê°ÜÊû∂ÂèäÂÖ∂Ê®°ÁµÑÁöÑÊúâÊïàÊÄß„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåChatGPT ÂÆ∂ÊóèÁöÑÂ∞çÊäóÁ©©ÂÅ•ÊÄßÊéíÂêçÁÇ∫ GPT-4-turbo > GPT-4o > GPT-3.5-turboÔºåËÄå‰∏îÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÁ©©ÂÅ•ÊÄßÊúÉÂèóÂà∞ÂÖ∂ÈÅã‰ΩúÁöÑÂ∞àÊ•≠È†òÂüüÂΩ±Èüø„ÄÇ

##### **A Comprehensive Survey of Foundation Models in Medicine**
2406.10729v1 by Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang

Foundation models (FMs) are large-scale deep-learning models trained on
extensive datasets using self-supervised techniques. These models serve as a
base for various downstream tasks, including healthcare. FMs have been adopted
with great success across various domains within healthcare, including natural
language processing (NLP), computer vision, graph learning, biology, and omics.
Existing healthcare-based surveys have not yet included all of these domains.
Therefore, this survey provides a comprehensive overview of FMs in healthcare.
We focus on the history, learning strategies, flagship models, applications,
and challenges of FMs. We explore how FMs such as the BERT and GPT families are
reshaping various healthcare domains, including clinical large language models,
medical image analysis, and omics data. Furthermore, we provide a detailed
taxonomy of healthcare applications facilitated by FMs, such as clinical NLP,
medical computer vision, graph learning, and other biology-related tasks.
Despite the promising opportunities FMs provide, they also have several
associated challenges, which are explained in detail. We also outline potential
future directions to provide researchers and practitioners with insights into
the potential and limitations of FMs in healthcare to advance their deployment
and mitigate associated risks.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°Âûã (FM) ÊòØ‰ΩøÁî®Ëá™ÊàëÁõ£Áù£ÊäÄË°ìÂú®Âª£Ê≥õÊï∏ÊìöÈõÜ‰∏äË®ìÁ∑¥ÁöÑÂ§ßË¶èÊ®°Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã„ÄÇÈÄô‰∫õÊ®°Âûã‰ΩúÁÇ∫ÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãôÁöÑÂü∫Á§éÔºåÂåÖÊã¨ÈÜ´ÁôÇ‰øùÂÅ•„ÄÇFM Â∑≤Âú®ÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÂêÑÁ®ÆÈ†òÂüü‰∏≠Ë¢´Âª£Ê≥õÊé°Áî®ÔºåÂåÖÊã¨Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP)„ÄÅÈõªËÖ¶Ë¶ñË¶∫„ÄÅÂúñÂΩ¢Â≠∏Áøí„ÄÅÁîüÁâ©Â≠∏ÂíåÁµÑÂ≠∏„ÄÇÁèæÊúâÁöÑÂü∫ÊñºÈÜ´ÁôÇ‰øùÂÅ•ÁöÑË™øÊü•Â∞öÊú™Ê∂µËìãÊâÄÊúâÈÄô‰∫õÈ†òÂüü„ÄÇÂõ†Ê≠§ÔºåÊú¨Ë™øÊü•Êèê‰æõ‰∫Ü FM Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÂÖ®Èù¢Ê¶ÇËø∞„ÄÇÊàëÂÄëÂ∞àÊ≥®Êñº FM ÁöÑÊ≠∑Âè≤„ÄÅÂ≠∏ÁøíÁ≠ñÁï•„ÄÅÊóóËâ¶Ê®°Âûã„ÄÅÊáâÁî®ÂíåÊåëÊà∞„ÄÇÊàëÂÄëÊé¢Ë®é‰∫Ü BERT Âíå GPT ÂÆ∂ÊóèÁ≠â FM Â¶Ç‰ΩïÈáçÂ°ëÂêÑÁ®ÆÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÔºåÂåÖÊã¨Ëá®Â∫äÂ§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÅÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÂíåÁµÑÂ≠∏Êï∏Êìö„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÁî± FM ‰øÉÈÄ≤ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®Ë©≥Á¥∞ÂàÜÈ°ûÊ≥ïÔºå‰æãÂ¶ÇËá®Â∫ä NLP„ÄÅÈÜ´Â≠∏ÈõªËÖ¶Ë¶ñË¶∫„ÄÅÂúñÂΩ¢Â≠∏ÁøíÂíåÂÖ∂‰ªñËàáÁîüÁâ©Áõ∏ÈóúÁöÑ‰ªªÂãô„ÄÇÂÑòÁÆ° FM Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÊ©üÊúÉÔºå‰ΩÜÂÆÉÂÄë‰πüÈù¢Ëá®Ëëó‰∏Ä‰∫õÁõ∏ÈóúÁöÑÊåëÊà∞ÔºåÈÄô‰∫õÊåëÊà∞Âú®Êñá‰∏≠ÈÉΩÊúâË©≥Á¥∞Ë™™Êòé„ÄÇÊàëÂÄëÈÇÑÊ¶ÇËø∞‰∫ÜÊΩõÂú®ÁöÑÊú™‰æÜÊñπÂêëÔºåÁÇ∫Á†îÁ©∂‰∫∫Âì°ÂíåÂæûÊ•≠ËÄÖÊèê‰æõÊúâÈóú FM Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊΩõÂäõÂíåÂ±ÄÈôêÊÄßÁöÑË¶ãËß£Ôºå‰ª•Êé®ÈÄ≤ÂÖ∂ÈÉ®ÁΩ≤‰∏¶Ê∏õËºïÁõ∏ÈóúÈ¢®Èö™„ÄÇ

##### **SyntheT2C: Generating Synthetic Data for Fine-Tuning Large Language Models on the Text2Cypher Task**
2406.10710v1 by Ziije Zhong, Linqing Zhong, Zhaoze Sun, Qingyun Jin, Zengchang Qin, Xiaofan Zhang

Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG)
databases presents a promising avenue for enhancing LLMs' efficacy and
mitigating their "hallucinations". Given that most KGs reside in graph
databases accessible solely through specialized query languages (e.g., Cypher),
there exists a critical need to bridge the divide between LLMs and KG databases
by automating the translation of natural language into Cypher queries (commonly
termed the "Text2Cypher" task). Prior efforts tried to bolster LLMs'
proficiency in Cypher generation through Supervised Fine-Tuning. However, these
explorations are hindered by the lack of annotated datasets of Query-Cypher
pairs, resulting from the labor-intensive and domain-specific nature of
annotating such datasets. In this study, we propose SyntheT2C, a methodology
for constructing a synthetic Query-Cypher pair dataset, comprising two distinct
pipelines: (1) LLM-based prompting and (2) template-filling. SyntheT2C
facilitates the generation of extensive Query-Cypher pairs with values sampled
from an underlying Neo4j graph database. Subsequently, SyntheT2C is applied to
two medical databases, culminating in the creation of a synthetic dataset,
MedT2C. Comprehensive experiments demonstrate that the MedT2C dataset
effectively enhances the performance of backbone LLMs on the Text2Cypher task.
Both the SyntheT2C codebase and the MedT2C dataset will be released soon.

ÊëòË¶ÅÔºö<paragraph>Â∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÁèæÊúâÁöÑÁü•Ë≠òÂúñË≠ú (KG) Ë≥áÊñôÂ∫´Êï¥ÂêàÔºåÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊèêÂçá LLM ÊïàËÉΩ‰∏¶Ê∏õËºïÂÖ∂„ÄåÂπªË¶∫„ÄçÁöÑÈÄîÂæë„ÄÇÁî±ÊñºÂ§ßÂ§öÊï∏ KG ÈÉΩÂ≠òÂú®ÊñºÂÉÖËÉΩÈÄèÈÅéÂ∞àÁî®Êü•Ë©¢Ë™ûË®ÄÔºà‰æãÂ¶Ç CypherÔºâÂ≠òÂèñÁöÑÂúñÂΩ¢Ë≥áÊñôÂ∫´‰∏≠ÔºåÂõ†Ê≠§Ëø´ÂàáÈúÄË¶ÅËá™ÂãïÂåñÂ∞áËá™ÁÑ∂Ë™ûË®ÄËΩâÊèõÁÇ∫ Cypher Êü•Ë©¢Ôºå‰ª•ÂΩåÂêà LLM Ëàá KG Ë≥áÊñôÂ∫´‰πãÈñìÁöÑÈ¥ªÊ∫ùÔºàÈÄöÂ∏∏Á®±ÁÇ∫„ÄåText2Cypher„Äç‰ªªÂãôÔºâ„ÄÇÂÖàÂâçÁöÑÂä™ÂäõÂòóË©¶ÈÄèÈÅéÁõ£Áù£ÂæÆË™ø‰æÜÊèêÂçá LLM Âú® Cypher ÁîüÊàêÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊé¢Á¥¢ÂèóÂà∞Áº∫‰πèÊü•Ë©¢-Cypher ÈÖçÂ∞çÁöÑË®ªËß£Ë≥áÊñôÈõÜÁöÑÈòªÁ§ôÔºåÈÄôÊòØÂõ†ÁÇ∫Ê≠§È°ûË≥áÊñôÈõÜÁöÑË®ªËß£ÈúÄË¶ÅÂ§ßÈáè‰∫∫Âäõ‰∏îÂÖ∑ÊúâÁâπÂÆöÈ†òÂüüÁöÑÊÄßË≥™„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü SyntheT2CÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®ÊñºÂª∫ÊßãÂêàÊàêÊü•Ë©¢-Cypher ÈÖçÂ∞çË≥áÊñôÈõÜÁöÑÊñπÊ≥ïÔºåÂåÖÂê´ÂÖ©ÂÄã‰∏çÂêåÁöÑÁÆ°ÈÅìÔºö(1) Âü∫Êñº LLM ÁöÑÊèêÁ§∫Âíå (2) ÁØÑÊú¨Â°´ÂØ´„ÄÇSyntheT2C ‰øÉÈÄ≤‰∫ÜÂ§ßÈáèÊü•Ë©¢-Cypher ÈÖçÂ∞çÁöÑÁî¢ÁîüÔºåÂÖ∂ÂÄºÂèñÊ®£Ëá™Âü∫Á§éÁöÑ Neo4j ÂúñÂΩ¢Ë≥áÊñôÂ∫´„ÄÇÈö®ÂæåÔºåÂ∞á SyntheT2C ÊáâÁî®ÊñºÂÖ©ÂÄãÈÜ´ÁôÇË≥áÊñôÂ∫´ÔºåÊúÄÁµÇÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÂêàÊàêË≥áÊñôÈõÜ MedT2C„ÄÇÂÖ®Èù¢ÁöÑÂØ¶È©óË≠âÊòéÔºåMedT2C Ë≥áÊñôÈõÜÊúâÊïàÊèêÂçá‰∫Ü‰∏ªÂππ LLM Âú® Text2Cypher ‰ªªÂãô‰∏äÁöÑÊïàËÉΩ„ÄÇSyntheT2C Á®ãÂºèÁ¢ºÂ∫´Âíå MedT2C Ë≥áÊñôÈõÜÈÉΩÂ∞áÂæàÂø´ÈáãÂá∫„ÄÇ</paragraph>

##### **Large Language Models as Event Forecasters**
2406.10492v1 by Libo Zhang, Yue Ning

Key elements of human events are extracted as quadruples that consist of
subject, relation, object, and timestamp. This representation can be extended
to a quintuple by adding a fifth element: a textual summary that briefly
describes the event. These quadruples or quintuples, when organized within a
specific domain, form a temporal knowledge graph (TKG). Current learning
frameworks focus on a few TKG-related tasks, such as predicting an object given
a subject and a relation or forecasting the occurrences of multiple types of
events (i.e., relation) in the next time window. They typically rely on complex
structural and sequential models like graph neural networks (GNNs) and
recurrent neural networks (RNNs) to update intermediate embeddings. However,
these methods often neglect the contextual information inherent in each
quintuple, which can be effectively captured through concise textual
descriptions. In this paper, we investigate how large language models (LLMs)
can streamline the design of TKG learning frameworks while maintaining
competitive accuracy in prediction and forecasting tasks. We develop multiple
prompt templates to frame the object prediction (OP) task as a standard
question-answering (QA) task, suitable for instruction fine-tuning with an
encoder-decoder generative LLM. For multi-event forecasting (MEF), we design
simple yet effective prompt templates for each TKG quintuple. This novel
approach removes the need for GNNs and RNNs, instead utilizing an encoder-only
LLM to generate fixed intermediate embeddings, which are subsequently processed
by a prediction head with a self-attention mechanism to forecast potential
future relations. Extensive experiments on multiple real-world datasets using
various evaluation metrics validate the effectiveness and robustness of our
approach.

ÊëòË¶ÅÔºö<paragraph>‰∫∫È°û‰∫ã‰ª∂ÁöÑ‰∏ªË¶ÅÂÖÉÁ¥†Ë¢´ËêÉÂèñÁÇ∫Áî±‰∏ªË©û„ÄÅÈóú‰øÇ„ÄÅÂèóË©ûÂíåÊôÇÈñìÊà≥ÁµÑÊàêÁöÑÂõõÂÖÉÁµÑ„ÄÇÊ≠§Ë°®Á§∫Ê≥ïÂèØÈÄèÈÅéÊñ∞Â¢ûÁ¨¨‰∫îÂÄãÂÖÉÁ¥†‰æÜÂª∂‰º∏ÁÇ∫‰∫îÂÖÉÁµÑÔºöÁ∞°Ë¶ÅÊèèËø∞‰∫ã‰ª∂ÁöÑÊñáÂ≠óÊëòË¶Å„ÄÇÈÄô‰∫õÂõõÂÖÉÁµÑÊàñ‰∫îÂÖÉÁµÑÂú®ÁâπÂÆöÈ†òÂüü‰∏≠ÁµÑÁπîÊôÇÔºåÊúÉÂΩ¢ÊàêÊôÇÂ∫èÁü•Ë≠òÂúñË≠ú (TKG)„ÄÇÁõÆÂâçÁöÑÂ≠∏ÁøíÊû∂ÊßãÂ∞àÊ≥®Êñº‰∏Ä‰∫õËàá TKG Áõ∏ÈóúÁöÑ‰ªªÂãôÔºå‰æãÂ¶ÇÂú®Áµ¶ÂÆö‰∏ªË©ûÂíåÈóú‰øÇÁöÑÊÉÖÊ≥Å‰∏ãÈ†êÊ∏¨ÂèóË©ûÔºåÊàñÈ†êÊ∏¨‰∏ã‰∏ÄÂÄãÊôÇÈñìË¶ñÁ™ó‰∏≠Â§öÁ®ÆÈ°ûÂûã‰∫ã‰ª∂ÔºàÂç≥Èóú‰øÇÔºâÁöÑÁôºÁîü„ÄÇÂÆÉÂÄëÈÄöÂ∏∏‰æùË≥¥ÊñºË§áÈõúÁöÑÁµêÊßãÂíåÂ∫èÂàóÊ®°ÂûãÔºå‰æãÂ¶ÇÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂíåÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑Ø (RNN)Ôºå‰æÜÊõ¥Êñ∞‰∏≠ÈñìÂµåÂÖ•„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÁ∂ìÂ∏∏ÂøΩÁï•ÊØèÂÄã‰∫îÂÖÉÁµÑ‰∏≠Âõ∫ÊúâÁöÑËÑàÁµ°Ë≥áË®äÔºåËÄåÈÄô‰∫õË≥áË®äÂèØÈÄèÈÅéÁ∞°ÊΩîÁöÑÊñáÂ≠óÊèèËø∞ÊúâÊïàÊì∑Âèñ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â¶Ç‰ΩïÁ∞°Âåñ TKG Â≠∏ÁøíÊû∂ÊßãÁöÑË®≠Ë®àÔºåÂêåÊôÇÂú®È†êÊ∏¨ÂíåÈ†êÊ∏¨‰ªªÂãô‰∏≠Á∂≠ÊåÅÂÖ∑Á´∂Áà≠ÂäõÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÈñãÁôºÂ§öÂÄãÊèêÁ§∫ÁØÑÊú¨ÔºåÂ∞áÁâ©‰ª∂È†êÊ∏¨ (OP) ‰ªªÂãôË®≠ÂÆöÁÇ∫Ê®ôÊ∫ñÂïèÁ≠î (QA) ‰ªªÂãôÔºåÈÅ©Áî®Êñº‰ΩøÁî®Á∑®Á¢ºÂô®-Ëß£Á¢ºÂô®ÁîüÊàêÂºè LLM ÈÄ≤Ë°åÊåá‰ª§ÂæÆË™ø„ÄÇÂ∞çÊñºÂ§ö‰∫ã‰ª∂È†êÊ∏¨ (MEF)ÔºåÊàëÂÄëÁÇ∫ÊØèÂÄã TKG ‰∫îÂÖÉÁµÑË®≠Ë®àÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊèêÁ§∫ÁØÑÊú¨„ÄÇÈÄôÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÊ∂àÈô§‰∫ÜÂ∞ç GNN Âíå RNN ÁöÑÈúÄÊ±ÇÔºåËÄåÊòØÂà©Áî®ÂÉÖÁ∑®Á¢ºÂô® LLM ‰æÜÁî¢ÁîüÂõ∫ÂÆöÁöÑ‰∏≠ÈñìÂµåÂÖ•ÔºåÁÑ∂ÂæåÁî±ÂÖ∑ÊúâËá™Ê≥®ÊÑèÂäõÊ©üÂà∂ÁöÑÈ†êÊ∏¨È†≠ËôïÁêÜÈÄô‰∫õÂµåÂÖ•Ôºå‰ª•È†êÊ∏¨ÊΩõÂú®ÁöÑÊú™‰æÜÈóú‰øÇ„ÄÇ‰ΩøÁî®ÂêÑÁ®ÆË©ï‰º∞ÊåáÊ®ôÂ∞çÂ§öÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇ</paragraph>

##### **Unlocking Large Language Model's Planning Capabilities with Maximum Diversity Fine-tuning**
2406.10479v1 by Wenjun Li, Changyu Chen, Pradeep Varakantham

Large language models (LLMs) have demonstrated impressive task-solving
capabilities, achieved through either prompting techniques or system designs.
However, concerns have arisen regarding their proficiency in planning tasks, as
they often struggle to generate valid plans. This paper investigates the impact
of fine-tuning on LLMs' planning capabilities. Our findings indicate that LLMs
can achieve good performance in planning through substantial (thousands of
specific examples) fine-tuning. However, fine-tuning is associated with
significant economic and computational costs. To address this challenge, we
propose the Maximum Diversity Fine-Tuning (MDFT) strategy to improve the sample
efficiency of fine-tuning in the planning domain. Specifically, our algorithm,
referred to as MDFT-g, encodes the planning task instances with their graph
representations and selects a subset of samples in the vector space that
maximizes data diversity. We empirically demonstrate that MDFT-g consistently
outperforms existing baselines at various scales across multiple benchmark
domains.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑ‰ªªÂãôËß£Ê±∫ËÉΩÂäõÔºåÈÄôÊòØÈÄèÈÅéÊèêÁ§∫ÊäÄË°ìÊàñÁ≥ªÁµ±Ë®≠Ë®à‰æÜÈÅîÊàê„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñº LLM Âú®Ë¶èÂäÉ‰ªªÂãô‰∏≠ÁöÑËÉΩÂäõÔºåÂ∑≤Áî¢ÁîüÁñëÊÖÆÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁ∂ìÂ∏∏Èõ£‰ª•Áî¢ÁîüÊúâÊïàÁöÑË®àÁï´„ÄÇÊú¨ÊñáÊé¢Ë®éÂæÆË™øÂ∞ç LLM Ë¶èÂäÉËÉΩÂäõÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåLLM ÂèØ‰ª•ÈÄèÈÅéÂ§ßÈáèÁöÑÂæÆË™øÔºàÊï∏ÂçÉÂÄãÂÖ∑È´îÁØÑ‰æãÔºâÂú®Ë¶èÂäÉ‰∏≠Áç≤ÂæóËâØÂ•ΩÁöÑË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÂæÆË™øËàáÈ°ØËëóÁöÑÁ∂ìÊøüÂíåÈÅãÁÆóÊàêÊú¨Áõ∏Èóú„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÊ≠§ÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫ÊúÄÂ§ßÂ§öÊ®£ÊÄßÂæÆË™ø (MDFT) Á≠ñÁï•Ôºå‰ª•ÊèêÂçáË¶èÂäÉÈ†òÂüüÂæÆË™øÁöÑÊ®£Êú¨ÊïàÁéá„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÁ®±ÁÇ∫ MDFT-g ÁöÑÊºîÁÆóÊ≥ïÔºå‰ª•ÂúñÂΩ¢Ë°®Á§∫Â∞çË¶èÂäÉ‰ªªÂãôÂØ¶‰æãÈÄ≤Ë°åÁ∑®Á¢ºÔºå‰∏¶Âú®ÂêëÈáèÁ©∫Èñì‰∏≠ÈÅ∏Êìá‰∏ÄÂÄãÊ®£Êú¨Â≠êÈõÜÔºå‰ª•ÊúÄÂ§ßÂåñË≥áÊñôÂ§öÊ®£ÊÄß„ÄÇÊàëÂÄëÂØ¶Ë≠âË≠âÊòéÔºåMDFT-g Âú®Â§öÂÄãÂü∫Ê∫ñÈ†òÂüüÁöÑ‰∏çÂêåË¶èÊ®°‰∏≠ÔºåÂßãÁµÇÂÑ™ÊñºÁèæÊúâÁöÑÂü∫Ê∫ñ„ÄÇ

##### **Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models**
2406.09994v1 by Manas Jhalani, Annervaz K M, Pushpak Bhattacharyya

In the realm of multimodal tasks, Visual Question Answering (VQA) plays a
crucial role by addressing natural language questions grounded in visual
content. Knowledge-Based Visual Question Answering (KBVQA) advances this
concept by adding external knowledge along with images to respond to questions.
We introduce an approach for KBVQA, augmenting the existing vision-language
transformer encoder-decoder (OFA) model. Our main contribution involves
enhancing questions by incorporating relevant external knowledge extracted from
knowledge graphs, using a dynamic triple extraction method. We supply a
flexible number of triples from the knowledge graph as context, tailored to
meet the requirements for answering the question. Our model, enriched with
knowledge, demonstrates an average improvement of 4.75\% in Exact Match Score
over the state-of-the-art on three different KBVQA datasets. Through
experiments and analysis, we demonstrate that furnishing variable triples for
each question improves the reasoning capabilities of the language model in
contrast to supplying a fixed number of triples. This is illustrated even for
recent large language models. Additionally, we highlight the model's
generalization capability by showcasing its SOTA-beating performance on a small
dataset, achieved through straightforward fine-tuning.

ÊëòË¶ÅÔºöÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°È¢ÜÂüüÔºåËßÜËßâÈóÆÁ≠îÔºàVQAÔºâÈÄöËøáËß£ÂÜ≥Âü∫‰∫éËßÜËßâÂÜÖÂÆπÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÈóÆÈ¢òÔºåÊâÆÊºîÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÂü∫‰∫éÁü•ËØÜÁöÑËßÜËßâÈóÆÁ≠îÔºàKBVQAÔºâÈÄöËøáÊ∑ªÂä†Â§ñÈÉ®Áü•ËØÜ‰ª•ÂèäÂõæÂÉèÊù•ÂõûÁ≠îÈóÆÈ¢òÔºå‰ªéËÄåÊé®Ëøõ‰∫ÜËøô‰∏ÄÊ¶ÇÂøµ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁî®‰∫é KBVQA ÁöÑÊñπÊ≥ïÔºåÂ¢ûÂº∫‰∫ÜÁé∞ÊúâÁöÑËßÜËßâËØ≠Ë®Ä transformer ÁºñÁ†ÅÂô®Ëß£Á†ÅÂô® (OFA) Ê®°Âûã„ÄÇÊàë‰ª¨ÁöÑ‰∏ªË¶ÅË¥°ÁåÆÊ∂âÂèäÈÄöËøá‰ΩøÁî®Âä®ÊÄÅ‰∏âÂÖÉÁªÑÊèêÂèñÊñπÊ≥ïÔºåÊï¥Âêà‰ªéÁü•ËØÜÂõæË∞±‰∏≠ÊèêÂèñÁöÑÁõ∏ÂÖ≥Â§ñÈÉ®Áü•ËØÜÊù•Â¢ûÂº∫ÈóÆÈ¢ò„ÄÇÊàë‰ª¨Êèê‰æõÊù•Ëá™Áü•ËØÜÂõæË∞±ÁöÑÁÅµÊ¥ªÊï∞ÈáèÁöÑ‰∏âÂÖÉÁªÑ‰Ωú‰∏∫‰∏ä‰∏ãÊñáÔºå‰ª•Êª°Ë∂≥ÂõûÁ≠îÈóÆÈ¢òÁöÑË¶ÅÊ±Ç„ÄÇÊàë‰ª¨ÁªèËøáÁü•ËØÜ‰∏∞ÂØåÁöÑÊ®°ÂûãÂú®‰∏â‰∏™‰∏çÂêåÁöÑ KBVQA Êï∞ÊçÆÈõÜ‰∏äÔºåÂú®Á≤æÁ°ÆÂåπÈÖçÂàÜÊï∞ÊñπÈù¢Â±ïÁ§∫‰∫ÜÊØîÊúÄÂÖàËøõÊ∞¥Âπ≥Âπ≥ÂùáÊèêÈ´ò 4.75%„ÄÇÈÄöËøáÂÆûÈ™åÂíåÂàÜÊûêÔºåÊàë‰ª¨ËØÅÊòé‰∏∫ÊØè‰∏™ÈóÆÈ¢òÊèê‰æõÂèØÂèò‰∏âÂÖÉÁªÑÊèêÈ´ò‰∫ÜËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåËøô‰∏éÊèê‰æõÂõ∫ÂÆöÊï∞ÈáèÁöÑ‰∏âÂÖÉÁªÑÂΩ¢ÊàêÂØπÊØî„ÄÇÂç≥‰ΩøÂØπ‰∫éÊúÄËøëÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåËøô‰∏ÄÁÇπ‰πüÂæóÂà∞‰∫ÜËØ¥Êòé„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÈÄöËøáÂ±ïÁ§∫Ê®°ÂûãÂú®Â∞èÂûãÊï∞ÊçÆÈõÜ‰∏äÁöÑ SOTA ÂáªË¥•ÊÄßËÉΩÔºåÁ™ÅÂá∫‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåËøôÊòØÈÄöËøáÁõ¥Êé•ÂæÆË∞ÉÂÆûÁé∞ÁöÑ„ÄÇ

##### **DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning**
2406.09953v1 by Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Lingyue Guo, Ping Luo, Yanfeng Lu

Dual-arm robots offer enhanced versatility and efficiency over single-arm
counterparts by enabling concurrent manipulation of multiple objects or
cooperative execution of tasks using both arms. However, effectively
coordinating the two arms for complex long-horizon tasks remains a significant
challenge. Existing task planning methods predominantly focus on single-arm
robots or rely on predefined bimanual operations, failing to fully leverage the
capabilities of dual-arm systems. To address this limitation, we introduce
DAG-Plan, a structured task planning framework tailored for dual-arm robots.
DAG-Plan harnesses large language models (LLMs) to decompose intricate tasks
into actionable sub-tasks represented as nodes within a directed acyclic graph
(DAG). Critically, DAG-Plan dynamically assigns these sub-tasks to the
appropriate arm based on real-time environmental observations, enabling
parallel and adaptive execution. We evaluate DAG-Plan on the novel Dual-Arm
Kitchen Benchmark, comprising 9 sequential tasks with 78 sub-tasks and 26
objects. Extensive experiments demonstrate the superiority of DAG-Plan over
directly using LLM to generate plans, achieving nearly 50% higher efficiency
compared to the single-arm task planning baseline and nearly double the success
rate of the dual-arm task planning baseline.

ÊëòË¶ÅÔºöÈõôËáÇÊ©üÂô®‰∫∫ÈÄèÈÅéÂêåÊôÇÊìçÊéßÂ§öÂÄãÁâ©‰ª∂Êàñ‰ΩøÁî®ÈõôËáÇÂçîÂêåÂü∑Ë°å‰ªªÂãôÔºåÊèê‰æõÊØîÂñÆËáÇÊ©üÂô®‰∫∫Êõ¥È´òÁöÑÈùàÊ¥ªÊÄßËàáÊïàÁéá„ÄÇÁÑ∂ËÄåÔºåË¶ÅÊúâÊïàÂçîË™øÈõôËáÇ‰ª•Âü∑Ë°åË§áÈõú‰∏îÊôÇÈñìË∑®Â∫¶Èï∑ÁöÑ‰ªªÂãôÔºå‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÁèæÊúâÁöÑ‰ªªÂãôË¶èÂäÉÊñπÊ≥ï‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂñÆËáÇÊ©üÂô®‰∫∫ÔºåÊàñ‰æùË≥¥ÊñºÈ†êÂÖàÂÆöÁæ©ÁöÑÈõôÊâãÊìç‰ΩúÔºåÁÑ°Ê≥ïÂÖÖÂàÜÂà©Áî®ÈõôËáÇÁ≥ªÁµ±ÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü DAG-PlanÔºå‰∏ÄÂÄãÂ∞àÈñÄÁÇ∫ÈõôËáÇÊ©üÂô®‰∫∫ÈáèË∫´ÊâìÈÄ†ÁöÑÁµêÊßãÂåñ‰ªªÂãôË¶èÂäÉÊû∂Êßã„ÄÇDAG-Plan Âà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞áË§áÈõúÁöÑ‰ªªÂãôÂàÜËß£ÊàêÂèØÊìç‰ΩúÁöÑÂ≠ê‰ªªÂãôÔºå‰∏¶Â∞áÂÖ∂Ë°®Á§∫ÁÇ∫ÊúâÂêëÁÑ°Áí∞Âúñ (DAG) ‰∏≠ÁöÑÁØÄÈªû„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåDAG-Plan ÊúÉÊ†πÊìöÂç≥ÊôÇÁöÑÁí∞Â¢ÉËßÄÂØüÂãïÊÖãÂú∞Â∞áÈÄô‰∫õÂ≠ê‰ªªÂãôÂàÜÈÖçÁµ¶ÈÅ©Áï∂ÁöÑÊâãËáÇÔºåÂæûËÄåÂØ¶Áèæ‰∏¶Ë°åÂíåËá™ÈÅ©ÊáâÁöÑÂü∑Ë°å„ÄÇÊàëÂÄëÂú®Êñ∞ÁöÑÈõôËáÇÂªöÊàøÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠Ë©ï‰º∞‰∫Ü DAG-PlanÔºåÂÖ∂‰∏≠ÂåÖÂê´ 9 ÂÄãÈ†ÜÂ∫è‰ªªÂãô„ÄÅ78 ÂÄãÂ≠ê‰ªªÂãôÂíå 26 ÂÄãÁâ©‰ª∂„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫Ü DAG-Plan ÂÑ™ÊñºÁõ¥Êé•‰ΩøÁî® LLM ‰æÜÁî¢ÁîüË®àÁï´ÔºåËàáÂñÆËáÇ‰ªªÂãôË¶èÂäÉÂü∫Ê∫ñÁ∑öÁõ∏ÊØîÔºåÊïàÁéáÊèêÈ´ò‰∫ÜËøë 50%ÔºåËÄåËàáÈõôËáÇ‰ªªÂãôË¶èÂäÉÂü∫Ê∫ñÁ∑öÁõ∏ÊØîÔºåÊàêÂäüÁéáÂπæ‰πéÊèêÈ´ò‰∫Ü‰∏ÄÂÄç„ÄÇ

##### **TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs**
2406.10310v1 by Zhuofeng Li, Zixing Gou, Xiangnan Zhang, Zhongyuan Liu, Sirui Li, Yuntong Hu, Chen Ling, Zheng Zhang, Liang Zhao

Text-Attributed Graphs (TAGs) augment graph structures with natural language
descriptions, facilitating detailed depictions of data and their
interconnections across various real-world settings. However, existing TAG
datasets predominantly feature textual information only at the nodes, with
edges typically represented by mere binary or categorical attributes. This lack
of rich textual edge annotations significantly limits the exploration of
contextual relationships between entities, hindering deeper insights into
graph-structured data. To address this gap, we introduce Textual-Edge Graphs
Datasets and Benchmark (TEG-DB), a comprehensive and diverse collection of
benchmark textual-edge datasets featuring rich textual descriptions on nodes
and edges. The TEG-DB datasets are large-scale and encompass a wide range of
domains, from citation networks to social networks. In addition, we conduct
extensive benchmark experiments on TEG-DB to assess the extent to which current
techniques, including pre-trained language models, graph neural networks, and
their combinations, can utilize textual node and edge information. Our goal is
to elicit advancements in textual-edge graph research, specifically in
developing methodologies that exploit rich textual node and edge descriptions
to enhance graph analysis and provide deeper insights into complex real-world
networks. The entire TEG-DB project is publicly accessible as an open-source
repository on Github, accessible at
https://github.com/Zhuofeng-Li/TEG-Benchmark.

ÊëòË¶ÅÔºöÊñáÂ≠óÊ®ôË®ªÂúñÔºàTAGÔºâ‰ª•Ëá™ÁÑ∂Ë™ûË®ÄÊèèËø∞Êì¥ÂÖÖÂúñÂΩ¢ÁµêÊßãÔºåÂçîÂä©Ë©≥Á¥∞ÊèèÁπ™Ë≥áÊñôÂèäÂÖ∂Âú®ÂêÑÁ®ÆÁúüÂØ¶‰∏ñÁïåË®≠ÂÆö‰∏≠ÁöÑÁõ∏‰∫íÈÄ£Áµê„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑ TAG Ë≥áÊñôÈõÜ‰∏ªË¶ÅÂÉÖÂú®ÁØÄÈªû‰∏≠ÂëàÁèæÊñáÂ≠óË≥áË®äÔºåÈÇäÁ∑£ÈÄöÂ∏∏ÂÉÖ‰ª•‰∫åÈÄ≤‰ΩçÊàñÂàÜÈ°ûÂ±¨ÊÄßË°®Á§∫„ÄÇÈÄôÁ®ÆÁº∫‰πèË±êÂØåÁöÑÊñáÂ≠óÈÇäÁ∑£Ë®ªËß£ÔºåÊúÉÂ§ßÂπÖÈôêÂà∂Êé¢Á¥¢ÂØ¶È´îÈñìÁöÑËÑàÁµ°Èóú‰øÇÔºåÈòªÁ§ôÊ∑±ÂÖ•‰∫ÜËß£ÂúñÂΩ¢ÁµêÊßãË≥áÊñô„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§Â∑ÆË∑ùÔºåÊàëÂÄëÂºïÈÄ≤ÊñáÂ≠óÈÇäÁ∑£ÂúñÂΩ¢Ë≥áÊñôÈõÜËàáÂü∫Ê∫ñÔºàTEG-DBÔºâÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢‰∏îÂ§öÊ®£ÂåñÁöÑÂü∫Ê∫ñÊñáÂ≠óÈÇäÁ∑£Ë≥áÊñôÈõÜÈõÜÂêàÔºåÂú®ÁØÄÈªûÂíåÈÇäÁ∑£‰∏äÂÖ∑ÊúâË±êÂØåÁöÑÊñáÂ≠óÊèèËø∞„ÄÇTEG-DB Ë≥áÊñôÈõÜË¶èÊ®°ÈæêÂ§ßÔºåÊ∂µËìãÂæûÂºïÊñáÁ∂≤Ë∑ØÂà∞Á§æ‰∫§Á∂≤Ë∑ØÁöÑÂª£Ê≥õÈ†òÂüü„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú® TEG-DB ‰∏äÈÄ≤Ë°åÂª£Ê≥õÁöÑÂü∫Ê∫ñÂØ¶È©óÔºå‰ª•Ë©ï‰º∞ÁõÆÂâçÊäÄË°ìÔºàÂåÖÊã¨È†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°Âûã„ÄÅÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÂèäÂÖ∂ÁµÑÂêàÔºâÂú®‰ΩïÁ®ÆÁ®ãÂ∫¶‰∏äËÉΩÂà©Áî®ÊñáÂ≠óÁØÄÈªûÂíåÈÇäÁ∑£Ë≥áË®ä„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÂºïÁôºÊñáÂ≠óÈÇäÁ∑£ÂúñÂΩ¢Á†îÁ©∂ÁöÑÈÄ≤Â±ïÔºåÁâπÂà•ÊòØÂú®ÈñãÁôºÂà©Áî®Ë±êÂØåÊñáÂ≠óÁØÄÈªûÂíåÈÇäÁ∑£ÊèèËø∞‰æÜÂ¢ûÂº∑ÂúñÂΩ¢ÂàÜÊûê‰∏¶Êèê‰æõÂ∞çË§áÈõúÁúüÂØ¶‰∏ñÁïåÁ∂≤Ë∑ØÊõ¥Ê∑±ÂÖ•Ë¶ãËß£ÁöÑÊñπÊ≥ïË´ñ„ÄÇÊï¥ÂÄã TEG-DB Â∞àÊ°à‰ª•ÈñãÊ∫êÂÑ≤Â≠òÂ∫´ÁöÑÂΩ¢ÂºèÂÖ¨ÈñãÊñº GithubÔºåÂèØÊñº https://github.com/Zhuofeng-Li/TEG-Benchmark ÂèñÂæó„ÄÇ

##### **Automated Molecular Concept Generation and Labeling with Large Language Models**
2406.09612v1 by Shichang Zhang, Botao Xia, Zimin Zhang, Qianli Wu, Fang Sun, Ziniu Hu, Yizhou Sun

Artificial intelligence (AI) is significantly transforming scientific
research. Explainable AI methods, such as concept-based models (CMs), are
promising for driving new scientific discoveries because they make predictions
based on meaningful concepts and offer insights into the prediction process. In
molecular science, however, explainable CMs are not as common compared to
black-box models like Graph Neural Networks (GNNs), primarily due to their
requirement for predefined concepts and manual label for each instance, which
demand domain knowledge and can be labor-intensive. This paper introduces a
novel framework for Automated Molecular Concept (AutoMolCo) generation and
labeling. AutoMolCo leverages the knowledge in Large Language Models (LLMs) to
automatically generate predictive molecular concepts and label them for each
molecule. Such procedures are repeated through iterative interactions with LLMs
to refine concepts, enabling simple linear models on the refined concepts to
outperform GNNs and LLM in-context learning on several benchmarks. The whole
AutoMolCo framework is automated without any human knowledge inputs in either
concept generation, labeling, or refinement, thereby surpassing the limitations
of extant CMs while maintaining their explainability and allowing easy
intervention. Through systematic experiments on MoleculeNet and High-Throughput
Experimentation (HTE) datasets, we demonstrate that the AutoMolCo-induced
explainable CMs are beneficial and promising for molecular science research.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÊ≠£Âú®È°ØËëóÂú∞ËΩâËÆäÁßëÂ≠∏Á†îÁ©∂„ÄÇÂèØËß£ÈáãÁöÑ AI ÊñπÊ≥ïÔºå‰æãÂ¶ÇÂü∫ÊñºÊ¶ÇÂøµÁöÑÊ®°ÂûãÔºàCMÔºâÔºåÂ∞çÊñºÊé®ÂãïÊñ∞ÁöÑÁßëÂ≠∏ÁôºÁèæÂæàÊúâÂ∏åÊúõÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÊ†πÊìöÊúâÊÑèÁæ©ÁöÑÊ¶ÇÂøµÈÄ≤Ë°åÈ†êÊ∏¨Ôºå‰∏¶Êèê‰æõÂ∞çÈ†êÊ∏¨ÈÅéÁ®ãÁöÑË¶ãËß£„ÄÇÁÑ∂ËÄåÔºåÂú®ÂàÜÂ≠êÁßëÂ≠∏‰∏≠ÔºåÂèØËß£ÈáãÁöÑ CM ‰∏¶‰∏çÂÉèÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàGNNÔºâÈÄôÊ®£ÁöÑÈªëÁÆ±Ê®°ÂûãÈÇ£È∫ºÂ∏∏Ë¶ãÔºåÈÄô‰∏ªË¶ÅÊòØÁî±ÊñºÂÆÉÂÄëÈúÄË¶ÅÈ†êÂÆöÁæ©ÁöÑÊ¶ÇÂøµÂíåÊØèÂÄãÂØ¶‰æãÁöÑÊâãÂãïÊ®ôÁ±§ÔºåÈÄôÈúÄË¶ÅÈ†òÂüüÁü•Ë≠ò‰∏¶‰∏îÂèØËÉΩÈúÄË¶ÅÂ§ßÈáè‰∫∫Â∑•„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÁî®ÊñºËá™ÂãïÂàÜÂ≠êÊ¶ÇÂøµÔºàAutoMolCoÔºâÁîüÊàêÂíåÊ®ôÁ±§ÁöÑÂÖ®Êñ∞Ê°ÜÊû∂„ÄÇAutoMolCo Âà©Áî®Â§ßË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÁöÑÁü•Ë≠òËá™ÂãïÁîüÊàêÈ†êÊ∏¨ÂàÜÂ≠êÊ¶ÇÂøµ‰∏¶ÁÇ∫ÊØèÂÄãÂàÜÂ≠êÊ®ôË®òÂÆÉÂÄë„ÄÇÊ≠§È°ûÁ®ãÂ∫èÈÄöÈÅéËàá LLM ÁöÑÂèçË¶Ü‰∫§‰∫í‰æÜÈáçË§áÔºå‰ª•ÂÑ™ÂåñÊ¶ÇÂøµÔºå‰ΩøÂü∫ÊñºÂÑ™ÂåñÊ¶ÇÂøµÁöÑÁ∞°ÂñÆÁ∑öÊÄßÊ®°ÂûãÂú®ÂπæÂÄãÂü∫Ê∫ñ‰∏äÂÑ™Êñº GNN Âíå LLM ÁöÑ‰∏ä‰∏ãÊñáÂ≠∏Áøí„ÄÇÊï¥ÂÄã AutoMolCo Ê°ÜÊû∂ÊòØËá™ÂãïÂåñÁöÑÔºåÂú®Ê¶ÇÂøµÁîüÊàê„ÄÅÊ®ôË®òÊàñÂÑ™Âåñ‰∏≠ÈÉΩÊ≤íÊúâ‰ªª‰Ωï‰∫∫Â∑•Áü•Ë≠òËº∏ÂÖ•ÔºåÂæûËÄåË∂ÖË∂ä‰∫ÜÁèæÊúâ CM ÁöÑÈôêÂà∂ÔºåÂêåÊôÇ‰øùÊåÅ‰∫ÜÂÆÉÂÄëÁöÑÂèØËß£ÈáãÊÄß‰∏¶ÂÖÅË®±ËºïÈ¨ÜÂπ≤È†ê„ÄÇÈÄöÈÅéÂ∞ç MoleculeNet ÂíåÈ´òÈÄöÈáèÂØ¶È©óÔºàHTEÔºâÊï∏ÊìöÈõÜÈÄ≤Ë°åÁ≥ªÁµ±ÁöÑÂØ¶È©óÔºåÊàëÂÄëË≠âÊòé‰∫Ü AutoMolCo Ë™òÂ∞éÁöÑÂèØËß£Èáã CM Â∞çÂàÜÂ≠êÁßëÂ≠∏Á†îÁ©∂ÊòØÊúâÁõä‰∏îÊúâÂ∏åÊúõÁöÑ„ÄÇ

##### **Cross-Modality Program Representation Learning for Electronic Design Automation with High-Level Synthesis**
2406.09606v1 by Zongyue Qin, Yunsheng Bai, Atefeh Sograbizadeh, Zijian Ding, Ziniu Hu, Yizhou Sun, Jason Cong

In recent years, domain-specific accelerators (DSAs) have gained popularity
for applications such as deep learning and autonomous driving. To facilitate
DSA designs, programmers use high-level synthesis (HLS) to compile a high-level
description written in C/C++ into a design with low-level hardware description
languages that eventually synthesize DSAs on circuits. However, creating a
high-quality HLS design still demands significant domain knowledge,
particularly in microarchitecture decisions expressed as \textit{pragmas}.
Thus, it is desirable to automate such decisions with the help of machine
learning for predicting the quality of HLS designs, requiring a deeper
understanding of the program that consists of original code and pragmas.
Naturally, these programs can be considered as sequence data. In addition,
these programs can be compiled and converted into a control data flow graph
(CDFG). But existing works either fail to leverage both modalities or combine
the two in shallow or coarse ways. We propose ProgSG, a model that allows
interaction between the source code sequence modality and the graph modality in
a deep and fine-grained way. To alleviate the scarcity of labeled designs, a
pre-training method is proposed based on a suite of compiler's data flow
analysis tasks. Experimental results show that ProgSG reduces the RMSE of
design performance predictions by up to $22\%$, and identifies designs with an
average of $1.10\times$ and $1.26\times$ (up to $8.17\times$ and $13.31\times$)
performance improvement in design space exploration (DSE) task compared to HARP
and AutoDSE, respectively.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÔºåÁâπÂÆöÈ†òÂüüÂä†ÈÄüÂô®ÔºàDSAÔºâÂú®Ê∑±Â∫¶Â≠∏ÁøíÂíåËá™ÂãïÈßïÈßõÁ≠âÊáâÁî®‰∏≠Ë∂ä‰æÜË∂äÂèóÊ≠°Ëøé„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ DSA Ë®≠Ë®àÔºåÁ®ãÂºèË®≠Ë®à‰∫∫Âì°‰ΩøÁî®È´òÈöéÁ∂úÂêàÔºàHLSÔºâÂ∞á‰ª• C/C++ Á∑®ÂØ´ÁöÑÈ´òÈöéÊèèËø∞Á∑®Ë≠ØÊàê‰ΩéÈöéÁ°¨È´îÊèèËø∞Ë™ûË®ÄÁöÑË®≠Ë®àÔºåÊúÄÁµÇÂú®ÈõªË∑Ø‰∏≠ÂêàÊàê DSA„ÄÇÁÑ∂ËÄåÔºåË¶ÅÂª∫Á´ãÈ´òÂìÅË≥™ÁöÑ HLS Ë®≠Ë®àÔºå‰ªçÁÑ∂ÈúÄË¶ÅÂ§ßÈáèÁöÑÈ†òÂüüÁü•Ë≠òÔºåÁâπÂà•ÊòØÂú®Ë°®Á§∫ÁÇ∫„ÄåÊåá‰ª§„ÄçÁöÑÂæÆÊû∂ÊßãÊ±∫Á≠ñ‰∏≠„ÄÇÂõ†Ê≠§ÔºåÂæàÂ∏åÊúõËóâÂä©Ê©üÂô®Â≠∏ÁøíËá™ÂãïÂåñÈÄô‰∫õÊ±∫Á≠ñÔºå‰ª•È†êÊ∏¨ HLS Ë®≠Ë®àÁöÑÂìÅË≥™ÔºåÈÄôÈúÄË¶ÅÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÂåÖÂê´ÂéüÂßãÁ®ãÂºèÁ¢ºÂíåÊåá‰ª§ÁöÑÁ®ãÂºè„ÄÇËá™ÁÑ∂Âú∞ÔºåÈÄô‰∫õÁ®ãÂºèÂèØ‰ª•Ë¶ñÁÇ∫Â∫èÂàóË≥áÊñô„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÁ®ãÂºèÂèØ‰ª•Á∑®Ë≠Ø‰∏¶ËΩâÊèõÊàêÊéßÂà∂Ë≥áÊñôÊµÅÁ®ãÂúñÔºàCDFGÔºâ„ÄÇ‰ΩÜÁèæÊúâÁöÑ‰ΩúÂìÅ‰∏çÊòØÁÑ°Ê≥ïÂêåÊôÇÂà©Áî®ÈÄôÂÖ©Á®ÆÊñπÂºèÔºåÂ∞±ÊòØ‰ª•Ê∑∫Â±§ÊàñÁ≤óÁï•ÁöÑÊñπÂºèÂ∞áÂÆÉÂÄëÁµêÂêàÂú®‰∏ÄËµ∑„ÄÇÊàëÂÄëÊèêÂá∫ ProgSGÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖÅË®±‰ª•Ê∑±ÂÖ•‰∏îÁ¥∞Á∑ªÁöÑÊñπÂºèÂú®ÂéüÂßãÁ¢ºÂ∫èÂàóÊñπÂºèÂíåÂúñÂΩ¢ÊñπÂºè‰πãÈñìÈÄ≤Ë°å‰∫íÂãïÁöÑÊ®°Âûã„ÄÇÁÇ∫‰∫ÜÁ∑©Ëß£Ê®ôË®òË®≠Ë®àÁöÑÁ®ÄÁº∫ÊÄßÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÁ∑®Ë≠ØÂô®Ë≥áÊñôÊµÅÂàÜÊûê‰ªªÂãôÂ•ó‰ª∂ÁöÑÈ†êË®ìÁ∑¥ÊñπÊ≥ï„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåProgSG Â∞áË®≠Ë®àÊïàËÉΩÈ†êÊ∏¨ÁöÑ RMSE Èôç‰Ωé‰∫ÜÂ§öÈÅî 22%Ôºå‰∏¶Ë≠òÂà•Âá∫Ë®≠Ë®àÂú®Ë®≠Ë®àÁ©∫ÈñìÊé¢Á¥¢ÔºàDSEÔºâ‰ªªÂãô‰∏≠Âπ≥ÂùáÊïàËÉΩÊèêÂçá 1.10 ÂÄçÂíå 1.26 ÂÄçÔºàÊúÄÈ´ò 8.17 ÂÄçÂíå 13.31 ÂÄçÔºâÔºåÂàÜÂà•Ëàá HARP Âíå AutoDSE Áõ∏ÊØî„ÄÇ</paragraph>

##### **Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models**
2406.09403v1 by Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna

Humans draw to facilitate reasoning: we draw auxiliary lines when solving
geometry problems; we mark and circle when reasoning on maps; we use sketches
to amplify our ideas and relieve our limited-capacity working memory. However,
such actions are missing in current multimodal language models (LMs). Current
chain-of-thought and tool-use paradigms only use text as intermediate reasoning
steps. In this work, we introduce Sketchpad, a framework that gives multimodal
LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts
planning and reasoning according to the visual artifacts it has drawn.
Different from prior work, which uses text-to-image models to enable LMs to
draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is
closer to human sketching and better facilitates reasoning. Sketchpad can also
use specialist vision models during the sketching process (e.g., draw bounding
boxes with object detection models, draw masks with segmentation models), to
further enhance visual perception and reasoning. We experiment with a wide
range of math tasks (including geometry, functions, graphs, and chess) and
complex visual reasoning tasks. Sketchpad substantially improves performance on
all tasks over strong base models with no sketching, yielding an average gain
of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a
new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial
reasoning (83.9%), and visual correspondence (80.8%). All codes and data are in
https://visualsketchpad.github.io/.

ÊëòË¶ÅÔºö<paragraph>‰∫∫È°ûÂà©Áî®Áπ™Áï´‰æÜ‰øÉÈÄ≤Êé®ÁêÜÔºöÊàëÂÄëÂú®Ëß£Ê±∫Âπæ‰ΩïÂïèÈ°åÊôÇÊúÉÁï´ËºîÂä©Á∑öÔºõÂú®Á†îÁ©∂Âú∞ÂúñÊôÇÊúÉÊ®ôË®òÂíåÁï´ÂúàÔºõÊàëÂÄë‰ΩøÁî®ËçâÂúñ‰æÜÊì¥Â±ïÊàëÂÄëÁöÑÊÉ≥Ê≥ï‰∏¶Ê∏õËºïÊàëÂÄëÂÆπÈáèÊúâÈôêÁöÑÂ∑•‰ΩúË®òÊÜ∂„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÊ®°ÊÖãË™ûË®ÄÊ®°Âûã (LM) ‰∏≠Áº∫Â∞ëÊ≠§È°ûÂãï‰Ωú„ÄÇÁõÆÂâçÁöÑÊÄùËÄÉÈèàÂíåÂ∑•ÂÖ∑‰ΩøÁî®ÁØÑ‰æãÂÉÖÂ∞áÊñáÂ≠óÁî®‰Ωú‰∏≠ÈñìÊé®ÁêÜÊ≠•È©ü„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü SketchpadÔºå‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÂÆÉÁÇ∫Â§öÊ®°ÊÖã LM Êèê‰æõ‰∫Ü‰∏ÄÂÄãË¶ñË¶∫ËçâÂúñÊú¨ÂíåÂèØ‰ª•Âú®ËçâÂúñÊú¨‰∏äÁπ™Áï´ÁöÑÂ∑•ÂÖ∑„ÄÇLM Ê†πÊìöÂÆÉÁπ™Ë£ΩÁöÑË¶ñË¶∫Â∑•‰ª∂ÈÄ≤Ë°åË¶èÂäÉÂíåÊé®ÁêÜ„ÄÇ‰∏çÂêåÊñº‰ª•Ââç‰ΩøÁî®ÊñáÂ≠óËΩâÂúñÂÉèÊ®°Âûã‰Ωø LM ËÉΩÂ§†Áπ™Áï´ÁöÑÂÖàÂâçÂ∑•‰ΩúÔºåSketchpad ‰Ωø LM ËÉΩÂ§†‰ΩøÁî®Á∑öÊ¢ù„ÄÅÊñπÂ°ä„ÄÅÊ®ôË®òÁ≠âÈÄ≤Ë°åÁπ™Áï´ÔºåÈÄôÊõ¥Êé•ËøëÊñº‰∫∫È°ûÁöÑÁ¥†ÊèèÔºå‰∏¶‰∏îÊõ¥Â•ΩÂú∞‰øÉÈÄ≤‰∫ÜÊé®ÁêÜ„ÄÇSketchpad ‰πüÂèØ‰ª•Âú®Á¥†ÊèèÈÅéÁ®ã‰∏≠‰ΩøÁî®Â∞àÂÆ∂Ë¶ñË¶∫Ê®°ÂûãÔºà‰æãÂ¶ÇÔºå‰ΩøÁî®Áâ©‰ª∂ÂÅµÊ∏¨Ê®°ÂûãÁπ™Ë£ΩÈÇäÁïåÊ°ÜÔºå‰ΩøÁî®ÂàÜÂâ≤Ê®°ÂûãÁπ™Ë£ΩÈÅÆÁΩ©ÔºâÔºå‰ª•ÈÄ≤‰∏ÄÊ≠•Â¢ûÂº∑Ë¶ñË¶∫ÊÑüÁü•ÂíåÊé®ÁêÜ„ÄÇÊàëÂÄë‰ΩøÁî®Âª£Ê≥õÁöÑÊï∏Â≠∏‰ªªÂãôÔºàÂåÖÊã¨Âπæ‰Ωï„ÄÅÂáΩÊï∏„ÄÅÂúñÂΩ¢ÂíåË•øÊ¥ãÊ£ãÔºâÂíåË§áÈõúÁöÑË¶ñË¶∫Êé®ÁêÜ‰ªªÂãôÈÄ≤Ë°å‰∫ÜÂØ¶È©ó„ÄÇSketchpad Â§ßÂπÖÊèêÂçá‰∫ÜÊâÄÊúâ‰ªªÂãôÂú®Ê≤íÊúâÁ¥†ÊèèÁöÑÊÉÖÊ≥Å‰∏ãÂº∑Â§ßÁöÑÂü∫Á§éÊ®°ÂûãÁöÑÊïàËÉΩÔºåÂú®Êï∏Â≠∏‰ªªÂãô‰∏äÂπ≥ÂùáÊèêÂçá 12.7%ÔºåÂú®Ë¶ñË¶∫‰ªªÂãô‰∏äÊèêÂçá 8.6%„ÄÇÈÖçÂÇô Sketchpad ÁöÑ GPT-4o Âú®ÊâÄÊúâ‰ªªÂãô‰∏äÈÉΩÂâµ‰∏ã‰∫ÜÊñ∞ÁöÑÊäÄË°ìÊ∞¥Ê∫ñÔºåÂåÖÊã¨ V*Bench (80.3%)„ÄÅBLINK Á©∫ÈñìÊé®ÁêÜ (83.9%) ÂíåË¶ñË¶∫Â∞çÊáâ (80.8%)„ÄÇÊâÄÊúâÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈÉΩÂèØ‰ª•Âú® https://visualsketchpad.github.io/ ‰∏≠ÊâæÂà∞„ÄÇ</paragraph>

##### **Transformers meet Neural Algorithmic Reasoners**
2406.09308v1 by Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, Petar Veliƒçkoviƒá

Transformers have revolutionized machine learning with their simple yet
effective architecture. Pre-training Transformers on massive text datasets from
the Internet has led to unmatched generalization for natural language
understanding (NLU) tasks. However, such language models remain fragile when
tasked with algorithmic forms of reasoning, where computations must be precise
and robust. To address this limitation, we propose a novel approach that
combines the Transformer's language understanding with the robustness of graph
neural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs
proved effective as generic solvers for algorithmic tasks, when specified in
graph form. To make their embeddings accessible to a Transformer, we propose a
hybrid architecture with a two-phase training procedure, allowing the tokens in
the language model to cross-attend to the node embeddings from the NAR. We
evaluate our resulting TransNAR model on CLRS-Text, the text-based version of
the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only
models for algorithmic reasoning, both in and out of distribution.

ÊëòË¶ÅÔºöTransformer ÊÜëËóâÂÖ∂Á∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊû∂ÊßãÔºåÂæπÂ∫ïÊîπËÆä‰∫ÜÊ©üÂô®Â≠∏Áøí„ÄÇÂú®Á∂≤ÈöõÁ∂≤Ë∑Ø‰∏äÁöÑÂ§ßÈáèÊñáÂ≠óË≥áÊñôÈõÜ‰∏äÂ∞ç Transformer ÈÄ≤Ë°åÈ†êË®ìÁ∑¥ÔºåÂ∑≤ÁÇ∫Ëá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ (NLU) ‰ªªÂãôÂ∏∂‰æÜ‰∫ÜÁÑ°ËàáÂÄ´ÊØîÁöÑÊ≥õÂåñ„ÄÇÁÑ∂ËÄåÔºåÂú®ÈúÄË¶ÅÊºîÁÆóÊ≥ïÂΩ¢ÂºèÊé®ÁêÜÁöÑ‰ªªÂãô‰∏≠ÔºåÊ≠§È°ûË™ûË®ÄÊ®°Âûã‰ªçÁÑ∂ËÑÜÂº±ÔºåÂõ†ÁÇ∫Ë®àÁÆóÂøÖÈ†àÁ≤æÁ¢∫‰∏îÁ©©ÂÅ•„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÁµêÂêà‰∫Ü Transformer ÁöÑË™ûË®ÄÁêÜËß£ËàáÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÁÇ∫Âü∫Á§éÁöÑÁ•ûÁ∂ìÊºîÁÆóÊ≥ïÊé®ÁêÜÂô® (NAR) ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊ≠§È°û NAR Ë¢´Ë≠âÊòéÂú®‰ª•ÂúñÂΩ¢ÂΩ¢ÂºèÊåáÂÆöÊôÇÔºåÂèØ‰ΩúÁÇ∫ÊºîÁÆóÊ≥ï‰ªªÂãôÁöÑÈÄöÁî®Ëß£ÁÆóÂô®„ÄÇÁÇ∫‰∫ÜËÆì Transformer ÂèØ‰ª•Â≠òÂèñÂÖ∂ÂµåÂÖ•ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂÖ∑ÊúâÂÖ©ÈöéÊÆµË®ìÁ∑¥Á®ãÂ∫èÁöÑÊ∑∑ÂêàÊû∂ÊßãÔºåËÆìË™ûË®ÄÊ®°Âûã‰∏≠ÁöÑÁ¨¶ËôüÂèØ‰ª•‰∫§ÂèâÈóúÊ≥® NAR ‰∏≠ÁöÑÁØÄÈªûÂµåÂÖ•„ÄÇÊàëÂÄëÂú® CLRS-TextÔºàCLRS-30 Âü∫Ê∫ñÁöÑÊñáÂ≠óÁâàÊú¨Ôºâ‰∏äË©ï‰º∞ÊàëÂÄëÂæóÂà∞ÁöÑ TransNAR Ê®°ÂûãÔºå‰∏¶Ë≠âÊòé‰∫ÜÂú®ÊºîÁÆóÊ≥ïÊé®ÁêÜ‰∏≠ÔºåÁÑ°Ë´ñÊòØÂú®ÂàÜ‰ΩàÂÖßÊàñÂàÜ‰ΩàÂ§ñÔºåÈÉΩÊØîÂÉÖ‰ΩøÁî® Transformer ÁöÑÊ®°ÂûãÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇ

##### **SememeLM: A Sememe Knowledge Enhanced Method for Long-tail Relation Representation**
2406.10297v1 by Shuyi Li, Shaojuan Wu, Xiaowang Zhang, Zhiyong Feng

Recognizing relations between two words is a fundamental task with the broad
applications. Different from extracting relations from text, it is difficult to
identify relations among words without their contexts. Especially for long-tail
relations, it becomes more difficult due to inadequate semantic features.
Existing approaches based on language models (LMs) utilize rich knowledge of
LMs to enhance the semantic features of relations. However, they capture
uncommon relations while overlooking less frequent but meaningful ones since
knowledge of LMs seriously relies on trained data where often represents common
relations. On the other hand, long-tail relations are often uncommon in
training data. It is interesting but not trivial to use external knowledge to
enrich LMs due to collecting corpus containing long-tail relationships is
hardly feasible. In this paper, we propose a sememe knowledge enhanced method
(SememeLM) to enhance the representation of long-tail relations, in which
sememes can break the contextual constraints between wors. Firstly, we present
a sememe relation graph and propose a graph encoding method. Moreover, since
external knowledge base possibly consisting of massive irrelevant knowledge,
the noise is introduced. We propose a consistency alignment module, which
aligns the introduced knowledge with LMs, reduces the noise and integrates the
knowledge into the language model. Finally, we conducted experiments on word
analogy datasets, which evaluates the ability to distinguish relation
representations subtle differences, including long-tail relations. Extensive
experiments show that our approach outperforms some state-of-the-art methods.

ÊëòË¶ÅÔºö<paragraph>Ëæ®Ë≠òÂÖ©ÂÄãË©ûÂΩô‰πãÈñìÁöÑÈóú‰øÇÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÂª£Ê≥õÊáâÁî®ÊÄßÁöÑÂü∫Êú¨‰ªªÂãô„ÄÇËàáÂæûÊñáÊú¨‰∏≠Êì∑ÂèñÈóú‰øÇ‰∏çÂêåÔºåÂú®Ê≤íÊúâ‰∏ä‰∏ãÊñáÁöÑÊÉÖÊ≥Å‰∏ãËæ®Ë≠òË©ûÂΩô‰πãÈñìÁöÑÈóú‰øÇÂæàÂõ∞Èõ£„ÄÇÁâπÂà•ÊòØÂ∞çÊñºÈï∑Â∞æÈóú‰øÇÔºåÁî±ÊñºË™ûÊÑèÁâπÂæµ‰∏çË∂≥ÔºåËæ®Ë≠òÈõ£Â∫¶Êõ¥È´ò„ÄÇÁèæÊúâÁöÑÂü∫ÊñºË™ûË®ÄÊ®°Âûã (LM) ÁöÑÊñπÊ≥ïÂà©Áî® LM ÁöÑË±êÂØåÁü•Ë≠ò‰æÜÂ¢ûÂº∑Èóú‰øÇÁöÑË™ûÊÑèÁâπÂæµ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÊúÉÊì∑Âèñ‰∏çÂ∏∏Ë¶ãÁöÑÈóú‰øÇÔºåÂêåÊôÇÂøΩÁï•È†ªÁéáËºÉ‰Ωé‰ΩÜÊúâÊÑèÁæ©ÁöÑÈóú‰øÇÔºåÂõ†ÁÇ∫ LM ÁöÑÁü•Ë≠òÂö¥Èáç‰æùË≥¥ÊñºË®ìÁ∑¥Ë≥áÊñôÔºåËÄåË®ìÁ∑¥Ë≥áÊñôÈÄöÂ∏∏‰ª£Ë°®Â∏∏Ë¶ãÁöÑÈóú‰øÇ„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÈï∑Â∞æÈóú‰øÇÂú®Ë®ìÁ∑¥Ë≥áÊñô‰∏≠ÈÄöÂ∏∏‰∏çÂ∏∏Ë¶ã„ÄÇÁî±ÊñºÊî∂ÈõÜÂåÖÂê´Èï∑Â∞æÈóú‰øÇÁöÑË™ûÊñôÂ∫´Âπæ‰πé‰∏çÂèØË°åÔºåÂõ†Ê≠§‰ΩøÁî®Â§ñÈÉ®Áü•Ë≠ò‰æÜË±êÂØå LM ÂæàÊúâË∂£Ôºå‰ΩÜ‰∏¶‰∏çÂÆπÊòì„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆË™ûÁæ©ÁâπÂæµÁü•Ë≠òÂ¢ûÂº∑ÊñπÊ≥ï (SememeLM) ‰æÜÂ¢ûÂº∑Èï∑Â∞æÈóú‰øÇÁöÑË°®Á§∫ÔºåÂÖ∂‰∏≠Ë™ûÁæ©ÁâπÂæµÂèØ‰ª•ÊâìÁ†¥Ë©ûÂΩô‰πãÈñìÁöÑ‰∏ä‰∏ãÊñáÈôêÂà∂„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãË™ûÁæ©ÁâπÂæµÈóú‰øÇÂúñÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂúñÂΩ¢Á∑®Á¢ºÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºÂ§ñÈÉ®Áü•Ë≠òÂ∫´ÂèØËÉΩÂåÖÂê´Â§ßÈáèÁÑ°ÈóúÁöÑÁü•Ë≠òÔºåÂõ†Ê≠§ÊúÉÂºïÂÖ•ÈõúË®ä„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰∏ÄËá¥ÊÄßÂ∞çÈΩäÊ®°ÁµÑÔºåÂÆÉÂ∞áÂºïÂÖ•ÁöÑÁü•Ë≠òËàá LM Â∞çÈΩäÔºåÊ∏õÂ∞ëÈõúË®ä‰∏¶Â∞áÁü•Ë≠òÊï¥ÂêàÂà∞Ë™ûË®ÄÊ®°Âûã‰∏≠„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ∞çË©ûÂΩôÈ°ûÊØîË≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂØ¶È©óÔºåË©ï‰º∞‰∫ÜÂçÄÂàÜÈóú‰øÇË°®Á§∫Á¥∞ÂæÆÂ∑ÆÁï∞ÁöÑËÉΩÂäõÔºåÂåÖÊã¨Èï∑Â∞æÈóú‰øÇ„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÂÑ™Êñº‰∏Ä‰∫õÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇ</paragraph>

##### **Hierarchical Compression of Text-Rich Graphs via Large Language Models**
2406.11884v1 by Shichang Zhang, Da Zheng, Jiani Zhang, Qi Zhu, Xiang song, Soji Adeshina, Christos Faloutsos, George Karypis, Yizhou Sun

Text-rich graphs, prevalent in data mining contexts like e-commerce and
academic graphs, consist of nodes with textual features linked by various
relations. Traditional graph machine learning models, such as Graph Neural
Networks (GNNs), excel in encoding the graph structural information, but have
limited capability in handling rich text on graph nodes. Large Language Models
(LLMs), noted for their superior text understanding abilities, offer a solution
for processing the text in graphs but face integration challenges due to their
limitation for encoding graph structures and their computational complexities
when dealing with extensive text in large neighborhoods of interconnected
nodes. This paper introduces ``Hierarchical Compression'' (HiCom), a novel
method to align the capabilities of LLMs with the structure of text-rich
graphs. HiCom processes text in a node's neighborhood in a structured manner by
organizing the extensive textual information into a more manageable hierarchy
and compressing node text step by step. Therefore, HiCom not only preserves the
contextual richness of the text but also addresses the computational challenges
of LLMs, which presents an advancement in integrating the text processing power
of LLMs with the structural complexities of text-rich graphs. Empirical results
show that HiCom can outperform both GNNs and LLM backbones for node
classification on e-commerce and citation graphs. HiCom is especially effective
for nodes from a dense region in a graph, where it achieves a 3.48% average
performance improvement on five datasets while being more efficient than LLM
backbones.

ÊëòË¶ÅÔºö<paragraph>Âú®Ë≥áÊñôÊé¢ÂãòÊÉÖÂ¢É‰∏≠ÊôÆÈÅçÂ≠òÂú®ÁöÑÊñáÂ≠óË±êÂØåÂúñÂΩ¢Ôºå‰æãÂ¶ÇÈõªÂ≠êÂïÜÂãôÂíåÂ≠∏Ë°ìÂúñÂΩ¢ÔºåÁî±ÂÖ∑ÊúâÂêÑÁ®ÆÈóúËÅØÁöÑÊñáÂ≠óÁâπÂæµÁØÄÈªûÁµÑÊàê„ÄÇÂÇ≥Áµ±ÁöÑÂúñÂΩ¢Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÔºå‰æãÂ¶ÇÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN)ÔºåÊìÖÈï∑Á∑®Á¢ºÂúñÂΩ¢ÁµêÊßãË≥áË®äÔºå‰ΩÜÂú®ËôïÁêÜÂúñÂΩ¢ÁØÄÈªû‰∏äÁöÑË±êÂØåÊñáÂ≠óÊôÇËÉΩÂäõÊúâÈôê„ÄÇ‰ª•ÂÖ∂ÂÑ™Áï∞ÁöÑÊñáÂ≠óÁêÜËß£ËÉΩÂäõËÄåËÅûÂêçÁöÑÂ∑®ÈáèË™ûË®ÄÊ®°Âûã (LLM) ÁÇ∫ËôïÁêÜÂúñÂΩ¢‰∏≠ÁöÑÊñáÂ≠óÊèê‰æõ‰∫ÜËß£Ê±∫ÊñπÊ°àÔºå‰ΩÜÁî±ÊñºÂÖ∂Âú®Á∑®Á¢ºÂúñÂΩ¢ÁµêÊßãÊñπÈù¢ÁöÑÈôêÂà∂‰ª•ÂèäÂú®ËôïÁêÜÁõ∏‰∫íÈÄ£Êé•ÁØÄÈªûÁöÑÂª£Ê≥õÊñáÂ≠óÊôÇÈù¢Ëá®ÁöÑË®àÁÆóË§áÈõúÊÄßÔºåËÄåÈù¢Ëá®Êï¥ÂêàÊåëÊà∞„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü„ÄåÂàÜÂ±§Â£ìÁ∏Æ„Äç(HiCom)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞á LLM ÁöÑËÉΩÂäõËàáÊñáÂ≠óË±êÂØåÂúñÂΩ¢ÁµêÊßãÁõ∏ÁµêÂêàÁöÑÊñ∞ÊñπÊ≥ï„ÄÇHiCom ‰ª•ÁµêÊßãÂåñÁöÑÊñπÂºèËôïÁêÜÁØÄÈªûÈÑ∞Âüü‰∏≠ÁöÑÊñáÂ≠óÔºåÊñπÊ≥ïÊòØÂ∞áÂª£Ê≥õÁöÑÊñáÂ≠óË≥áË®äÁµÑÁπîÊàêÊõ¥ÊòìÊñºÁÆ°ÁêÜÁöÑÂ±§Á¥ö‰∏¶ÈÄêÊ≠•Â£ìÁ∏ÆÁØÄÈªûÊñáÂ≠ó„ÄÇÂõ†Ê≠§ÔºåHiCom ‰∏çÂÉÖ‰øùÁïô‰∫ÜÊñáÂ≠óÁöÑË™ûÂ¢ÉË±êÂØåÊÄßÔºåÈÇÑÊáâÂ∞ç‰∫Ü LLM ÁöÑË®àÁÆóÊåëÊà∞ÔºåÈÄô‰ª£Ë°®‰∫ÜÂ∞á LLM ÁöÑÊñáÂ≠óËôïÁêÜËÉΩÂäõËàáÊñáÂ≠óË±êÂØåÂúñÂΩ¢ÁöÑÁµêÊßãË§áÈõúÊÄßÁõ∏Êï¥ÂêàÁöÑÈÄ≤Â±ï„ÄÇÂØ¶Ë≠âÁµêÊûúË°®ÊòéÔºåÂú®ÈõªÂ≠êÂïÜÂãôÂíåÂºïÊñáÂúñÂΩ¢‰∏äÁöÑÁØÄÈªûÂàÜÈ°û‰∏≠ÔºåHiCom ÂèØ‰ª•ÂÑ™Êñº GNN Âíå LLM ‰∏ªÂππ„ÄÇHiCom Â∞çÊñºÂúñÂΩ¢‰∏≠ÂØÜÈõÜÂçÄÂüüÁöÑÁØÄÈªûÁâπÂà•ÊúâÊïàÔºåÂú®‰∫îÂÄãË≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÂπ≥ÂùáÊïàËÉΩÊèêÂçá 3.48%ÔºåÂêåÊôÇÊØî LLM ‰∏ªÂππÊõ¥ÊúâÊïàÁéá„ÄÇ</paragraph>

##### **ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions**
2406.08842v1 by Xu Zhang, Xunjian Yin, Xiaojun Wan

While substantial advancements have been made in developing large language
models (LLMs), achieving control over their behavior can be difficult. Direct
preference optimization (DPO) assumes the existence of a latent reward function
to evaluate the responses of LLMs. This assumption indicates a strict
preference ordering of different responses to the same input. However, there
always exist contradictions of preference in LLMs according to our experimental
observations. In this paper, we construct a graph structure of the preference
relationship among different responses with self-annotation to find
contradictions in the preference order. We propose ContraSolver, an algorithm
that traverses all edges on the preference graph to identify those that might
cause contradictions. ContraSolver initializes the graph with a maximum
spanning tree and identifies contradictory edges, prioritizing the resolution
of low-confidence preferences while preserving high-confidence ones.
Experimental results on four different generation tasks show that the
performance of different LLMs can be largely improved through our completely
unsupervised self-alignment. Furthermore, by analyzing the preference graphs of
LLMs with and without self-alignment by ContraSolver, we quantify the reduction
in contradictions, suggesting that resolving preference contradictions is
crucial for achieving better alignment performance.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®ÈñãÁôºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊñπÈù¢Â∑≤Á∂ìÂèñÂæóÂØ¶Ë≥™ÊÄßÈÄ≤Â±ïÔºå‰ΩÜË¶ÅÊéßÂà∂ÂÖ∂Ë°åÁÇ∫ÂèØËÉΩÊúÉÂæàÂõ∞Èõ£„ÄÇÁõ¥Êé•ÂÅèÂ•ΩÊúÄ‰Ω≥Âåñ (DPO) ÂÅáË®≠Â≠òÂú®‰∏ÄÂÄãÊΩõÂú®ÁöÑÁçéÂãµÂáΩÊï∏‰æÜË©ï‰º∞ LLM ÁöÑÂõûÊáâ„ÄÇÊ≠§ÂÅáË®≠Ë°®Á§∫Â∞çÁõ∏ÂêåËº∏ÂÖ•ÁöÑ‰∏çÂêåÂõûÊáâÊúâÂö¥Ê†ºÁöÑÂÅèÂ•ΩÊéíÂ∫è„ÄÇÁÑ∂ËÄåÔºåÊ†πÊìöÊàëÂÄëÁöÑÂØ¶È©óËßÄÂØüÔºåÂú® LLM ‰∏≠ÂßãÁµÇÂ≠òÂú®ÂÅèÂ•ΩÁöÑÁüõÁõæ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊßãÂª∫‰∫Ü‰∏ÄÂÄãÂÅèÂ•ΩÈóú‰øÇÂúñÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∏çÂêåÂõûÊáâÁöÑËá™ÊàëË®ªËß£Ôºå‰ª•ÊâæÂá∫ÂÅèÂ•ΩÈ†ÜÂ∫è‰∏≠ÁöÑÁüõÁõæ„ÄÇÊàëÂÄëÊèêÂá∫ ContraSolverÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊºîÁÆóÊ≥ïÔºåÁî®ÊñºÈÅçÊ≠∑ÂÅèÂ•ΩÂúñ‰∏äÁöÑÊâÄÊúâÈÇäÁ∑£Ôºå‰ª•ÊâæÂá∫ÂèØËÉΩÂ∞éËá¥ÁüõÁõæÁöÑÈÇäÁ∑£„ÄÇContraSolver ‰ΩøÁî®ÊúÄÂ§ßÁîüÊàêÊ®πÂàùÂßãÂåñÂúñÂΩ¢Ôºå‰∏¶ÊâæÂá∫ÁüõÁõæÁöÑÈÇäÁ∑£ÔºåÂÑ™ÂÖàËß£Ê±∫‰Ωé‰ø°ÂøÉÁöÑÂÅèÂ•ΩÔºåÂêåÊôÇ‰øùÁïôÈ´ò‰ø°ÂøÉÁöÑÂÅèÂ•Ω„ÄÇÂú®ÂõõÈ†Ö‰∏çÂêåÁöÑÁîüÊàê‰ªªÂãô‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÈÄèÈÅéÊàëÂÄëÂÆåÂÖ®ÁÑ°Áõ£Áù£ÁöÑËá™Â∞çÈΩäÔºåÂèØ‰ª•Â§ßÂπÖÊîπÂñÑ‰∏çÂêå LLM ÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÈÄèÈÅéÂàÜÊûê‰ΩøÁî® ContraSolver ÈÄ≤Ë°åËá™Â∞çÈΩäÂíåÊú™ÈÄ≤Ë°åËá™Â∞çÈΩäÁöÑ LLM ÁöÑÂÅèÂ•ΩÂúñÔºåÊàëÂÄëÈáèÂåñ‰∫ÜÁüõÁõæÁöÑÊ∏õÂ∞ëÔºåÈÄôË°®Á§∫Ëß£Ê±∫ÂÅèÂ•ΩÁüõÁõæÂ∞çÊñºÈÅîÊàêÊõ¥Â•ΩÁöÑÂ∞çÈΩäÊïàËÉΩËá≥ÈóúÈáçË¶Å„ÄÇ

##### **Research Trends for the Interplay between Large Language Models and Knowledge Graphs**
2406.08223v1 by Hanieh Khorashadizadeh, Fatima Zahra Amara, Morteza Ezzabady, Fr√©d√©ric Ieng, Sanju Tiwari, Nandana Mihindukulasooriya, Jinghua Groppe, Soror Sahri, Farah Benamara, Sven Groppe

This survey investigates the synergistic relationship between Large Language
Models (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's
capabilities in understanding, reasoning, and language processing. It aims to
address gaps in current research by exploring areas such as KG Question
Answering, ontology generation, KG validation, and the enhancement of KG
accuracy and consistency through LLMs. The paper further examines the roles of
LLMs in generating descriptive texts and natural language queries for KGs.
Through a structured analysis that includes categorizing LLM-KG interactions,
examining methodologies, and investigating collaborative uses and potential
biases, this study seeks to provide new insights into the combined potential of
LLMs and KGs. It highlights the importance of their interaction for improving
AI applications and outlines future research directions.

ÊëòË¶ÅÔºöÈÄôÈ†ÖË™øÊü•Êé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÁü•Ë≠òÂúñË≠ú (KG) ‰πãÈñìÁöÑÂçîÂêåÈóú‰øÇÔºåÈÄôÂ∞çÊèêÂçá AI Âú®ÁêÜËß£„ÄÅÊé®ÁêÜÂíåË™ûË®ÄËôïÁêÜÊñπÈù¢ÁöÑËÉΩÂäõËá≥ÈóúÈáçË¶Å„ÄÇÂÆÉÊó®Âú®ÈÄèÈÅéÊé¢Ë®éÁü•Ë≠òÂúñË≠úÂïèÁ≠î„ÄÅÊú¨‰ΩìÁîüÊàê„ÄÅÁü•Ë≠òÂúñË≠úÈ©óË≠âÔºå‰ª•ÂèäÈÄèÈÅé LLM Â¢ûÂº∑Áü•Ë≠òÂúñË≠úÁöÑÊ∫ñÁ¢∫ÊÄßÂíå‰∏ÄËá¥ÊÄßÁ≠âÈ†òÂüüÔºå‰æÜËß£Ê±∫Áï∂ÂâçÁ†îÁ©∂‰∏≠ÁöÑÂ∑ÆË∑ù„ÄÇÊú¨ÊñáÈÄ≤‰∏ÄÊ≠•Êé¢Ë®é LLM Âú®ÁÇ∫Áü•Ë≠òÂúñË≠úÁî¢ÁîüÊèèËø∞ÊÄßÊñáÂ≠óÂíåËá™ÁÑ∂Ë™ûË®ÄÊü•Ë©¢ÊñπÈù¢ÁöÑ‰ΩúÁî®„ÄÇÈÄèÈÅé‰∏ÄÈ†ÖÁµêÊßãÂåñÂàÜÊûêÔºåÂåÖÊã¨ÂàÜÈ°û LLM-KG ‰∫íÂãï„ÄÅÊ™¢Ë¶ñÊñπÊ≥ï„ÄÅ‰ª•ÂèäÊé¢Ë®éÂçî‰ΩúÁî®ÈÄîÂíåÊΩõÂú®ÂÅèÂ∑ÆÔºåÊú¨Á†îÁ©∂Êó®Âú®Êèê‰æõ LLM ÂíåÁü•Ë≠òÂúñË≠úÁµêÂêàÊΩõÂäõÁöÑÊñ∞Ë¶ãËß£„ÄÇÂÆÉÂº∑Ë™ø‰∫ÜÂÆÉÂÄë‰∫íÂãïÂ∞çÊñºÊîπÂñÑ AI ÊáâÁî®Á®ãÂºèÁöÑÈáçË¶ÅÊÄßÔºå‰∏¶Ê¶ÇËø∞‰∫ÜÊú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇ

##### **SHACL2FOL: An FOL Toolkit for SHACL Decision Problems**
2406.08018v1 by Paolo Pareti

Recent studies on the Shapes Constraint Language (SHACL), a W3C specification
for validating RDF graphs, rely on translating the language into first-order
logic in order to provide formally-grounded solutions to the validation,
containment and satisfiability decision problems. Continuing on this line of
research, we introduce SHACL2FOL, the first automatic tool that (i) translates
SHACL documents into FOL sentences and (ii) computes the answer to the two
static analysis problems of satisfiability and containment; it also allow to
test the validity of a graph with respect to a set of constraints. By
integrating with existing theorem provers, such as E and Vampire, the tool
computes the answer to the aforementioned decision problems and outputs the
corresponding first-order logic theories in the standard TPTP format. We
believe this tool can contribute to further theoretical studies of SHACL, by
providing an automatic first-order logic interpretation of its semantics, while
also benefiting SHACL practitioners, by supplying static analysis capabilities
to help the creation and management of SHACL constraints.

ÊëòË¶ÅÔºöÊúÄËøëÂ∞çÂΩ¢ÁãÄÁ¥ÑÊùüË™ûË®Ä (SHACL) ÁöÑÁ†îÁ©∂ÔºåW3C Ë¶èÁØÑÁî®ÊñºÈ©óË≠â RDF ÂúñÂΩ¢Ôºå‰æùË≥¥ÊñºÂ∞áË™ûË®ÄËΩâË≠ØÊàê‰∏ÄÈöéÈÇèËºØÔºå‰ª•‰æøÈáùÂ∞çÈ©óË≠â„ÄÅÂåÖÂê´ÂíåÂèØÊªøË∂≥ÊÄßÊ±∫Á≠ñÂïèÈ°åÊèê‰æõÊ≠£Âºè‰æùÊìöÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂª∂Á∫åÈÄôÊ¢ùÁ†îÁ©∂Ë∑ØÁ∑öÔºåÊàëÂÄë‰ªãÁ¥π SHACL2FOLÔºåÁ¨¨‰∏ÄÂÄãËá™ÂãïÂ∑•ÂÖ∑Ôºö(i) Â∞á SHACL Êñá‰ª∂ËΩâË≠ØÊàê FOL Âè•Â≠êÔºå‰ª•Âèä (ii) Ë®àÁÆóÂèØÊªøË∂≥ÊÄßÂíåÂåÖÂê´ÈÄôÂÖ©ÂÄãÈùúÊÖãÂàÜÊûêÂïèÈ°åÁöÑÁ≠îÊ°àÔºõÂÆÉ‰πüÂÖÅË®±Ê∏¨Ë©¶ÂúñÂΩ¢Áõ∏Â∞çÊñº‰∏ÄÁµÑÁ¥ÑÊùüÁöÑÊúâÊïàÊÄß„ÄÇËóâÁî±Êï¥ÂêàÁèæÊúâÁöÑÂÆöÁêÜË≠âÊòéÂô®Ôºå‰æãÂ¶Ç E Âíå VampireÔºåÊ≠§Â∑•ÂÖ∑Ë®àÁÆóÂâçËø∞Ê±∫Á≠ñÂïèÈ°åÁöÑÁ≠îÊ°àÔºå‰∏¶Ëº∏Âá∫Ê®ôÊ∫ñ TPTP Ê†ºÂºè‰∏≠Â∞çÊáâÁöÑ‰∏ÄÈöéÈÇèËºØÁêÜË´ñ„ÄÇÊàëÂÄëÁõ∏‰ø°Ê≠§Â∑•ÂÖ∑ÊúâÂä©ÊñºÈÄ≤‰∏ÄÊ≠•ÁöÑ SHACL ÁêÜË´ñÁ†îÁ©∂ÔºåËóâÁî±Êèê‰æõÂÖ∂Ë™ûÊÑèÁöÑËá™Âãï‰∏ÄÈöéÈÇèËºØË©ÆÈáãÔºåÂêåÊôÇ‰πüËÆì SHACL ÂæûÊ•≠‰∫∫Âì°ÂèóÁõäÔºåËóâÁî±Êèê‰æõÈùúÊÖãÂàÜÊûêÂäüËÉΩ‰æÜÂçîÂä©Âª∫Á´ãÂíåÁÆ°ÁêÜ SHACL Á¥ÑÊùü„ÄÇ

##### **Efficient Parallel Multi-Hop Reasoning: A Scalable Approach for Knowledge Graph Analysis**
2406.07727v1 by Jesmin Jahan Tithi, Fabio Checconi, Fabrizio Petrini

Multi-hop reasoning (MHR) is a process in artificial intelligence and natural
language processing where a system needs to make multiple inferential steps to
arrive at a conclusion or answer. In the context of knowledge graphs or
databases, it involves traversing multiple linked entities and relationships to
understand complex queries or perform tasks requiring a deeper understanding.
Multi-hop reasoning is a critical function in various applications, including
question answering, knowledge base completion, and link prediction. It has
garnered significant interest in artificial intelligence, machine learning, and
graph analytics.
  This paper focuses on optimizing MHR for time efficiency on large-scale
graphs, diverging from the traditional emphasis on accuracy which is an
orthogonal goal. We introduce a novel parallel algorithm that harnesses
domain-specific learned embeddings to efficiently identify the top K paths
between vertices in a knowledge graph to find the best answers to a three-hop
query. Our contributions are: (1) We present a new parallel algorithm to
enhance MHR performance, scalability and efficiency. (2) We demonstrate the
algorithm's superior performance on leading-edge Intel and AMD architectures
through empirical results.
  We showcase the algorithm's practicality through a case study on identifying
academic affiliations of potential Turing Award laureates in Deep Learning,
highlighting its capability to handle intricate entity relationships. This
demonstrates the potential of our approach to enabling high-performance MHR,
useful to navigate the growing complexity of modern knowledge graphs.

ÊëòË¶ÅÔºöÂ§öË∑≥Êé®ÁêÜ (MHR) ÊòØ‰∫∫Â∑•Êô∫ÊÖßÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÁöÑ‰∏ÄÂÄãÈÅéÁ®ãÔºåÁ≥ªÁµ±ÈúÄË¶ÅÂü∑Ë°åÂ§öÂÄãÊé®ÁêÜÊ≠•È©üÊâçËÉΩÂæóÂá∫ÁµêË´ñÊàñÁ≠îÊ°à„ÄÇÂú®Áü•Ë≠òÂúñË°®ÊàñË≥áÊñôÂ∫´ÁöÑËÉåÊôØ‰∏ãÔºåÂÆÉÊ∂âÂèäÈÅçÊ≠∑Â§öÂÄãÈÄ£ÁµêÂØ¶È´îÂíåÈóú‰øÇÔºå‰ª•‰∫ÜËß£Ë§áÈõúÁöÑÊü•Ë©¢ÊàñÂü∑Ë°åÈúÄË¶ÅÊõ¥Ê∑±ÂÖ•ÁêÜËß£ÁöÑ‰ªªÂãô„ÄÇÂ§öË∑≥Êé®ÁêÜÊòØÂêÑÁ®ÆÊáâÁî®‰∏≠ÁöÑ‰∏ÄÈ†ÖÈóúÈçµÂäüËÉΩÔºåÂåÖÊã¨ÂïèÁ≠î„ÄÅÁü•Ë≠òÂ∫´ÂÆåÊàêÂíåÈÄ£ÁµêÈ†êÊ∏¨„ÄÇÂÆÉÂú®‰∫∫Â∑•Êô∫ÊÖß„ÄÅÊ©üÂô®Â≠∏ÁøíÂíåÂúñÂΩ¢ÂàÜÊûê‰∏≠ÂºïËµ∑‰∫ÜÊ•µÂ§ßÁöÑËààË∂£„ÄÇ
Êú¨ÊñáÈáçÈªûÂú®ÊñºÈáùÂ∞çÂ§ßË¶èÊ®°ÂúñË°®ÊúÄ‰Ω≥Âåñ MHR ÁöÑÊôÇÈñìÊïàÁéáÔºåËàáÂÇ≥Áµ±‰∏äÂº∑Ë™øÊ∫ñÁ¢∫ÊÄßÁöÑÊ≠£‰∫§ÁõÆÊ®ô‰∏çÂêå„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑ‰∏¶Ë°åÊºîÁÆóÊ≥ïÔºåÂà©Áî®ÁâπÂÆöÊñºÈ†òÂüüÁöÑÂ≠∏ÁøíÂµåÂÖ•‰æÜÊúâÊïàÁéáÂú∞Ë≠òÂà•Áü•Ë≠òÂúñË°®‰∏≠È†ÇÈªû‰πãÈñìÁöÑÈ†ÇÂ∞ñ K Ë∑ØÂæëÔºå‰ª•ÊâæÂá∫‰∏âË∑≥Êü•Ë©¢ÁöÑÊúÄ‰Ω≥Á≠îÊ°à„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÂ¶Ç‰∏ãÔºö(1) ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑ‰∏¶Ë°åÊºîÁÆóÊ≥ïÔºå‰ª•Â¢ûÂº∑ MHR ÁöÑÊïàËÉΩ„ÄÅÂèØÊì¥ÂÖÖÊÄßÂíåÊïàÁéá„ÄÇ(2) ÊàëÂÄëÈÄèÈÅéÂØ¶Ë≠âÁµêÊûúË≠âÊòé‰∫ÜË©≤ÊºîÁÆóÊ≥ïÂú®È†òÂÖàÁöÑ Intel Âíå AMD Êû∂Êßã‰∏äÁöÑÂçìË∂äÊïàËÉΩ„ÄÇ
ÊàëÂÄëÈÄèÈÅé‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂Â±ïÁ§∫‰∫ÜË©≤ÊºîÁÆóÊ≥ïÁöÑÂØ¶Áî®ÊÄßÔºåË©≤Á†îÁ©∂Ë≠òÂà•‰∫ÜÊ∑±Â∫¶Â≠∏Áøí‰∏≠ÊΩõÂú®ÁöÑÂúñÈùàÁçéÂæó‰∏ªÁöÑÂ≠∏Ë°ìÈóú‰øÇÔºåÁ™ÅÈ°Ø‰∫ÜÂÆÉËôïÁêÜË§áÈõúÂØ¶È´îÈóú‰øÇÁöÑËÉΩÂäõ„ÄÇÈÄôË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÂØ¶ÁèæÈ´òÊÄßËÉΩ MHR ÁöÑÊΩõÂäõÔºåÊúâÂä©ÊñºÊáâÂ∞çÁèæ‰ª£Áü•Ë≠òÂúñË°®Êó•ÁõäÂ¢ûÈï∑ÁöÑË§áÈõúÊÄß„ÄÇ

##### **TextGrad: Automatic "Differentiation" via Text**
2406.07496v1 by Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, James Zou

AI is undergoing a paradigm shift, with breakthroughs achieved by systems
orchestrating multiple large language models (LLMs) and other complex
components. As a result, developing principled and automated optimization
methods for compound AI systems is one of the most important new challenges.
Neural networks faced a similar challenge in its early days until
backpropagation and automatic differentiation transformed the field by making
optimization turn-key. Inspired by this, we introduce TextGrad, a powerful
framework performing automatic ``differentiation'' via text. TextGrad
backpropagates textual feedback provided by LLMs to improve individual
components of a compound AI system. In our framework, LLMs provide rich,
general, natural language suggestions to optimize variables in computation
graphs, ranging from code snippets to molecular structures. TextGrad follows
PyTorch's syntax and abstraction and is flexible and easy-to-use. It works
out-of-the-box for a variety of tasks, where the users only provide the
objective function without tuning components or prompts of the framework. We
showcase TextGrad's effectiveness and generality across a diverse range of
applications, from question answering and molecule optimization to radiotherapy
treatment planning. Without modifying the framework, TextGrad improves the
zero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\%$ to
$55\%$, yields $20\%$ relative performance gain in optimizing LeetCode-Hard
coding problem solutions, improves prompts for reasoning, designs new druglike
small molecules with desirable in silico binding, and designs radiation
oncology treatment plans with high specificity. TextGrad lays a foundation to
accelerate the development of the next-generation of AI systems.

ÊëòË¶ÅÔºö<paragraph>AI Ê≠£Á∂ìÊ≠∑‰∏ÄÂ†¥ÂÖ∏ÁØÑËΩâÁßªÔºåÁ™ÅÁ†¥‰æÜËá™ÊñºÁ≥ªÁµ±Á∑®ÊéíÂ§öÂÄãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÂÖ∂‰ªñË§áÈõúÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂõ†Ê≠§ÔºåÁÇ∫Ë§áÂêàÂºè AI Á≥ªÁµ±ÈñãÁôºÂéüÂâáÂåñ‰∏îËá™ÂãïÂåñÁöÑÊúÄ‰Ω≥ÂåñÊñπÊ≥ïÔºåÊòØÂÖ∂‰∏≠‰∏ÄÈ†ÖÊúÄÈáçË¶ÅÁöÑÊñ∞ÊåëÊà∞„ÄÇÁ•ûÁ∂ìÁ∂≤Ë∑ØÂú®Êó©ÊúüÈù¢Ëá®È°û‰ººÁöÑÊåëÊà∞ÔºåÁõ¥Âà∞ÂèçÂêëÂÇ≥Êí≠ÂíåËá™ÂãïÂæÆÂàÜÈÄèÈÅéËÆìÊúÄ‰Ω≥ÂåñËÆäÂæóÂÆπÊòìÔºåÈÄ≤ËÄåËΩâËÆä‰∫ÜÈÄôÂÄãÈ†òÂüü„ÄÇÂèóÂà∞Ê≠§ÂïüÁôºÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü TextGradÔºå‰∏ÄÂÄãÂº∑Â§ßÁöÑÊ°ÜÊû∂ÔºåÈÄèÈÅéÊñáÂ≠óÂü∑Ë°åËá™Âãï„ÄåÂæÆÂàÜ„Äç„ÄÇTextGrad ÂèçÂêëÂÇ≥Êí≠ LLM Êèê‰æõÁöÑÊñáÂ≠óÂõûÈ•ãÔºå‰ª•ÊîπÂñÑË§áÂêàÂºè AI Á≥ªÁµ±ÁöÑÂÄãÂà•ÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂú®ÊàëÂÄëÁöÑÊ°ÜÊû∂‰∏≠ÔºåLLM Êèê‰æõË±êÂØå„ÄÅÈÄöÁî®„ÄÅËá™ÁÑ∂ÁöÑË™ûË®ÄÂª∫Ë≠∞Ôºå‰æÜÊúÄ‰Ω≥ÂåñÈÅãÁÆóÂúñ‰∏≠ÁöÑËÆäÊï∏ÔºåÁØÑÂúçÂæûÁ®ãÂºèÁ¢ºÁâáÊÆµÂà∞ÂàÜÂ≠êÁµêÊßã„ÄÇTextGrad ÈÅµÂæ™ PyTorch ÁöÑË™ûÊ≥ïÂíåÊäΩË±°Ôºå‰∏îÈùàÊ¥ª‰∏îÊòìÊñº‰ΩøÁî®„ÄÇÂÆÉÈÅ©Áî®ÊñºÂêÑÁ®Æ‰ªªÂãôÔºå‰ΩøÁî®ËÄÖÂè™ÈúÄÊèê‰æõÁõÆÊ®ôÂáΩÊï∏ÔºåËÄåÁÑ°ÈúÄË™øÊï¥Ê°ÜÊû∂ÁöÑÁµÑÊàêÈÉ®ÂàÜÊàñÊèêÁ§∫„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü TextGrad Âú®ÂêÑÁ®ÆÊáâÁî®‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÊôÆÈÅçÊÄßÔºåÂæûÂïèÁ≠îÂíåÂàÜÂ≠êÊúÄ‰Ω≥ÂåñÂà∞ÊîæÂ∞ÑÊ≤ªÁôÇË®àÁï´„ÄÇÂú®‰∏ç‰øÆÊîπÊ°ÜÊû∂ÁöÑÊÉÖÊ≥Å‰∏ãÔºåTextGrad Â∞á Google-Proof ÂïèÁ≠î‰∏≠ GPT-4o ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊ∫ñÁ¢∫Â∫¶Âæû 51% ÊèêÂçáËá≥ 55%ÔºåÂú®ÊúÄ‰Ω≥Âåñ LeetCode-Hard Á∑®Á¢ºÂïèÈ°åËß£Á≠î‰∏≠Áî¢Áîü 20% ÁöÑÁõ∏Â∞çÊïàËÉΩÊèêÂçáÔºåÊîπÂñÑÊé®ÁêÜÊèêÁ§∫ÔºåË®≠Ë®àÂÖ∑ÊúâÁêÜÊÉ≥ÁöÑÁüΩÂü∫ÁµêÂêàÁöÑÊñ∞Ëó•Áâ©Â∞èÂàÜÂ≠êÔºå‰∏¶Ë®≠Ë®àÂá∫ÂÖ∑ÊúâÈ´òÁâπÁï∞ÊÄßÁöÑÊîæÂ∞ÑËÖ´Áò§Ê≤ªÁôÇË®àÁï´„ÄÇTextGrad ÁÇ∫Âä†ÈÄüÈñãÁôº‰∏ã‰∏Ä‰ª£ AI Á≥ªÁµ±Â•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ</paragraph>

##### **CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization**
2406.07494v2 by Frederic Kirstein, Jan Philip Wahle, Bela Gipp, Terry Ruas

Abstractive dialogue summarization is the task of distilling conversations
into informative and concise summaries. Although reviews have been conducted on
this topic, there is a lack of comprehensive work detailing the challenges of
dialogue summarization, unifying the differing understanding of the task, and
aligning proposed techniques, datasets, and evaluation metrics with the
challenges. This article summarizes the research on Transformer-based
abstractive summarization for English dialogues by systematically reviewing
1262 unique research papers published between 2019 and 2024, relying on the
Semantic Scholar and DBLP databases. We cover the main challenges present in
dialog summarization (i.e., language, structure, comprehension, speaker,
salience, and factuality) and link them to corresponding techniques such as
graph-based approaches, additional training tasks, and planning strategies,
which typically overly rely on BART-based encoder-decoder models. We find that
while some challenges, like language, have seen considerable progress, mainly
due to training methods, others, such as comprehension, factuality, and
salience, remain difficult and hold significant research opportunities. We
investigate how these approaches are typically assessed, covering the datasets
for the subdomains of dialogue (e.g., meeting, medical), the established
automatic metrics and human evaluation approaches for assessing scores and
annotator agreement. We observe that only a few datasets span across all
subdomains. The ROUGE metric is the most used, while human evaluation is
frequently reported without sufficient detail on inner-annotator agreement and
annotation guidelines. Additionally, we discuss the possible implications of
the recently explored large language models and conclude that despite a
potential shift in relevance and difficulty, our described challenge taxonomy
remains relevant.

ÊëòË¶ÅÔºö<paragraph>ÊäΩË±°ÂºèÂ∞çË©±ÊëòË¶ÅÊòØÂ∞áÂ∞çË©±ÊøÉÁ∏ÆÊàêÂÖ∑ÊúâË≥áË®äÊÄß‰∏îÁ∞°ÊΩîÁöÑÊëòË¶Å„ÄÇÂÑòÁÆ°Â∑≤ÈáùÂ∞çÊ≠§‰∏ªÈ°åÈÄ≤Ë°åÂØ©Êü•Ôºå‰ΩÜ‰ªçÁº∫‰πèË©≥Á¥∞Ë™™ÊòéÂ∞çË©±ÊëòË¶ÅÊåëÊà∞„ÄÅÁµ±‰∏ÄÂ∞ç‰ªªÂãôÁöÑ‰∏çÂêåÁêÜËß£Ôºå‰ª•ÂèäÂ∞áÂª∫Ë≠∞ÁöÑÊäÄË°ì„ÄÅË≥áÊñôÈõÜÂíåË©ï‰º∞ÊåáÊ®ôËàáÊåëÊà∞Áõ∏Á¨¶ÁöÑÂÖ®Èù¢ÊÄßÁ†îÁ©∂„ÄÇÊú¨ÊñáÈÄèÈÅéÁ≥ªÁµ±ÊÄßÂú∞Ê™¢Èñ± 2019 Âπ¥Ëá≥ 2024 Âπ¥ÈñìÁôºË°®ÁöÑ 1262 ÁØáÁç®ÁâπÁ†îÁ©∂Ë´ñÊñáÔºå‰æùË≥¥Ë™ûÁæ©Â≠∏ËÄÖÂíå DBLP Ë≥áÊñôÂ∫´ÔºåÁ∏ΩÁµê‰∫ÜÂü∫Êñº Transformer ÁöÑËã±Ë™ûÂ∞çË©±ÊäΩË±°ÂºèÊëòË¶ÅÁöÑÁ†îÁ©∂„ÄÇÊàëÂÄëÊ∂µËìãÂ∞çË©±ÊëòË¶Å‰∏≠Âá∫ÁèæÁöÑ‰∏ªË¶ÅÊåëÊà∞ÔºàÂç≥Ë™ûË®Ä„ÄÅÁµêÊßã„ÄÅÁêÜËß£„ÄÅË™™Ë©±ËÄÖ„ÄÅÈ°ØËëóÊÄßÂíåÁúüÂØ¶ÊÄßÔºâÔºå‰∏¶Â∞áÂÆÉÂÄëÈÄ£ÁµêÂà∞Â∞çÊáâÁöÑÊäÄË°ìÔºå‰æãÂ¶ÇÂü∫ÊñºÂúñÂΩ¢ÁöÑÊñπÊ≥ï„ÄÅÈ°çÂ§ñÁöÑË®ìÁ∑¥‰ªªÂãôÂíåË¶èÂäÉÁ≠ñÁï•ÔºåÈÄô‰∫õÁ≠ñÁï•ÈÄöÂ∏∏ÈÅéÂ∫¶‰æùË≥¥ÊñºÂü∫Êñº BART ÁöÑÁ∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Ê®°Âûã„ÄÇÊàëÂÄëÁôºÁèæÔºåÈõñÁÑ∂Ë™ûË®ÄÁ≠â‰∏Ä‰∫õÊåëÊà∞Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÂæó‰∫ÜÈÄ≤Â±ïÔºåÈÄô‰∏ªË¶ÅÊòØÁî±ÊñºË®ìÁ∑¥ÊñπÊ≥ïÔºå‰ΩÜÂÖ∂‰ªñÊåëÊà∞Ôºå‰æãÂ¶ÇÁêÜËß£„ÄÅÁúüÂØ¶ÊÄßÂíåÈ°ØËëóÊÄßÔºå‰ªçÁÑ∂ÂæàÂõ∞Èõ£Ôºå‰∏¶ÂÖ∑ÊúâÈáçÂ§ßÁöÑÁ†îÁ©∂Ê©üÊúÉ„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜÈÄöÂ∏∏Â¶Ç‰ΩïË©ï‰º∞ÈÄô‰∫õÊñπÊ≥ïÔºåÊ∂µËìã‰∫ÜÂ∞çË©±Â≠êÈ†òÂüüÔºà‰æãÂ¶ÇÊúÉË≠∞„ÄÅÈÜ´ÁôÇÔºâÁöÑË≥áÊñôÈõÜÔºåÊó¢ÂÆöÁöÑËá™ÂãïÂåñÊåáÊ®ôÂíåÁî®ÊñºË©ï‰º∞ÂàÜÊï∏ÂíåË®ªËß£ËÄÖ‰∏ÄËá¥ÊÄßÁöÑË©ï‰º∞ÊñπÊ≥ï„ÄÇÊàëÂÄëËßÄÂØüÂà∞ÔºåÂè™ÊúâÂ∞ëÊï∏Ë≥áÊñôÈõÜË∑®Ë∂äÊâÄÊúâÂ≠êÈ†òÂüü„ÄÇROUGE ÊåáÊ®ô‰ΩøÁî®ÊúÄÈ†ªÁπÅÔºåËÄå‰∫∫È°ûË©ï‰º∞ÈÄöÂ∏∏Âú®Ê≤íÊúâË∂≥Â§†ÁöÑË®ªËß£ËÄÖÂÖßÈÉ®‰∏ÄËá¥ÊÄßÂíåË®ªËß£ÊåáÂçóÁöÑË©≥Á¥∞Ë≥áË®ä‰∏ãÈÄ≤Ë°åÂ†±Âëä„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®éË´ñ‰∫ÜËøëÊúüÊé¢Á¥¢ÁöÑÂ§ßË™ûË®ÄÊ®°ÂûãÁöÑÂèØËÉΩÂΩ±ÈüøÔºå‰∏¶ÂæóÂá∫ÁµêË´ñÔºåÂÑòÁÆ°Áõ∏ÈóúÊÄßÂíåÈõ£Â∫¶ÂèØËÉΩÁôºÁîüËΩâËÆäÔºå‰ΩÜÊàëÂÄëÊâÄÊèèËø∞ÁöÑÊåëÊà∞ÂàÜÈ°ûÊ≥ï‰ªçÁÑ∂Áõ∏Èóú„ÄÇ</paragraph>

##### **Large Language Models for Constrained-Based Causal Discovery**
2406.07378v1 by Kai-Hendrik Cohrs, Gherardo Varando, Emiliano Diaz, Vasileios Sitokonstantinou, Gustau Camps-Valls

Causality is essential for understanding complex systems, such as the
economy, the brain, and the climate. Constructing causal graphs often relies on
either data-driven or expert-driven approaches, both fraught with challenges.
The former methods, like the celebrated PC algorithm, face issues with data
requirements and assumptions of causal sufficiency, while the latter demand
substantial time and domain knowledge. This work explores the capabilities of
Large Language Models (LLMs) as an alternative to domain experts for causal
graph generation. We frame conditional independence queries as prompts to LLMs
and employ the PC algorithm with the answers. The performance of the LLM-based
conditional independence oracle on systems with known causal graphs shows a
high degree of variability. We improve the performance through a proposed
statistical-inspired voting schema that allows some control over false-positive
and false-negative rates. Inspecting the chain-of-thought argumentation, we
find causal reasoning to justify its answer to a probabilistic query. We show
evidence that knowledge-based CIT could eventually become a complementary tool
for data-driven causal discovery.

ÊëòË¶ÅÔºöÂõ†ÊûúÈóú‰øÇÂ∞çÊñºÁêÜËß£Ë§áÈõúÁöÑÁ≥ªÁµ±Ëá≥ÈóúÈáçË¶ÅÔºå‰æãÂ¶ÇÁ∂ìÊøü„ÄÅÂ§ßËÖ¶ÂíåÊ∞£ÂÄô„ÄÇÂª∫ÊßãÂõ†ÊûúÂúñË°®ÈÄöÂ∏∏‰æùË≥¥ÊñºË≥áÊñôÈ©ÖÂãïÊàñÂ∞àÂÆ∂È©ÖÂãïÁöÑÊñπÊ≥ïÔºåÈÄôÂÖ©Á®ÆÊñπÊ≥ïÈÉΩÂÖÖÊªø‰∫ÜÊåëÊà∞„ÄÇÂâçËÄÖÊñπÊ≥ïÔºå‰æãÂ¶ÇËëóÂêçÁöÑ PC ÊºîÁÆóÊ≥ïÔºåÈù¢Ëá®Ë≥áÊñôÈúÄÊ±ÇÂíåÂõ†ÊûúÂÖÖË∂≥ÊÄßÁöÑÂÅáË®≠ÂïèÈ°åÔºåËÄåÂæåËÄÖÂâáÈúÄË¶ÅÂ§ßÈáèÁöÑÊôÇÈñìÂíåÈ†òÂüüÁü•Ë≠ò„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊé¢Ë®é‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫Âõ†ÊûúÂúñÁîüÊàêÈ†òÂüüÂ∞àÂÆ∂ÁöÑÊõø‰ª£ÊñπÊ°àÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂ∞áÊ¢ù‰ª∂Áç®Á´ãÊü•Ë©¢Ë®≠ÂÆöÁÇ∫ LLM ÁöÑÊèêÁ§∫Ôºå‰∏¶‰ΩøÁî® PC ÊºîÁÆóÊ≥ïÊê≠ÈÖçÁ≠îÊ°à„ÄÇÂü∫Êñº LLM ÁöÑÊ¢ù‰ª∂Áç®Á´ãÁ•ûË´≠Âú®ÂÖ∑ÊúâÂ∑≤Áü•Âõ†ÊûúÂúñË°®ÁöÑÁ≥ªÁµ±‰∏äÁöÑÊïàËÉΩÈ°ØÁ§∫Âá∫È´òÂ∫¶ÁöÑËÆäÁï∞ÊÄß„ÄÇÊàëÂÄëÈÄèÈÅéÊèêË≠∞ÁöÑÁµ±Ë®àÂïüÁôºÂºèÊäïÁ•®Êû∂Êßã‰æÜÊîπÂñÑÊïàËÉΩÔºåË©≤Êû∂ÊßãÂÖÅË®±Â∞çÂÅáÈôΩÊÄßÂíåÂÅáÈô∞ÊÄßÁéáÈÄ≤Ë°å‰∏Ä‰∫õÊéßÂà∂„ÄÇÊ™¢Êü•ÊÄùËÄÉÈèàË´ñË≠âÔºåÊàëÂÄëÁôºÁèæÂõ†ÊûúÊé®ÁêÜÂèØ‰ª•Ë≠âÊòéÂÖ∂Â∞çÊ©üÁéáÊü•Ë©¢ÁöÑÂõûÁ≠î„ÄÇÊàëÂÄëÈ°ØÁ§∫Âü∫ÊñºÁü•Ë≠òÁöÑ CIT ÊúÄÁµÇÂèØËÉΩÊàêÁÇ∫Ë≥áÊñôÈ©ÖÂãïÂõ†ÊûúÁôºÁèæÁöÑË£úÂÖÖÂ∑•ÂÖ∑ÁöÑË≠âÊìö„ÄÇ

##### **Scaling Large-Language-Model-based Multi-Agent Collaboration**
2406.07155v1 by Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun

Pioneering advancements in large language model-powered agents have
underscored the design pattern of multi-agent collaboration, demonstrating that
collective intelligence can surpass the capabilities of each individual.
Inspired by the neural scaling law, which posits that increasing neurons leads
to emergent abilities, this study investigates whether a similar principle
applies to increasing agents in multi-agent collaboration. Technically, we
propose multi-agent collaboration networks (MacNet), which utilize directed
acyclic graphs to organize agents and streamline their interactive reasoning
via topological ordering, with solutions derived from their dialogues.
Extensive experiments show that MacNet consistently outperforms baseline
models, enabling effective agent collaboration across various network
topologies and supporting cooperation among more than a thousand agents.
Notably, we observed a small-world collaboration phenomenon, where topologies
resembling small-world properties achieved superior performance. Additionally,
we identified a collaborative scaling law, indicating that normalized solution
quality follows a logistic growth pattern as scaling agents, with collaborative
emergence occurring much earlier than previously observed instances of neural
emergence. The code and data will be available at
https://github.com/OpenBMB/ChatDev.

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÈ©±Âä®ÁöÑ‰ª£ÁêÜÁöÑÂºÄÂàõÊÄßËøõÊ≠•Âº∫Ë∞É‰∫ÜÂ§ö‰ª£ÁêÜÂçè‰ΩúÁöÑËÆæËÆ°Ê®°ÂºèÔºåËØÅÊòé‰∫ÜÈõÜ‰ΩìÊô∫ËÉΩÂèØ‰ª•Ë∂ÖË∂äÊØè‰∏™‰∏™‰ΩìÁöÑËÉΩÂäõ„ÄÇÂèóÁ•ûÁªèÁΩëÁªúÊâ©Â±ïÂÆöÂæãÁöÑÂêØÂèëÔºåËØ•ÂÆöÂæãËÆ§‰∏∫Â¢ûÂä†Á•ûÁªèÂÖÉ‰ºöÂØºËá¥ËÉΩÂäõÁöÑÊ∂åÁé∞ÔºåÊú¨Á†îÁ©∂Ë∞ÉÊü•‰∫ÜÁ±ª‰ººÁöÑÂéüÁêÜÊòØÂê¶ÈÄÇÁî®‰∫éÂ¢ûÂä†Â§ö‰ª£ÁêÜÂçè‰Ωú‰∏≠ÁöÑ‰ª£ÁêÜ„ÄÇÂú®ÊäÄÊúØ‰∏äÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂ§ö‰ª£ÁêÜÂçè‰ΩúÁΩëÁªúÔºàMacNetÔºâÔºåÂÆÉÂà©Áî®ÊúâÂêëÊó†ÁéØÂõæÊù•ÁªÑÁªá‰ª£ÁêÜÂπ∂ÈÄöËøáÊãìÊâëÊéíÂ∫èÁÆÄÂåñÂÆÉ‰ª¨ÁöÑ‰∫§‰∫íÊé®ÁêÜÔºåËß£ÂÜ≥ÊñπÊ°àÊù•Ëá™ÂÆÉ‰ª¨ÁöÑÂØπËØù„ÄÇÂπøÊ≥õÁöÑÂÆûÈ™åË°®ÊòéÔºåMacNet ÂßãÁªà‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãÔºåËÉΩÂ§üÂú®ÂêÑÁßçÁΩëÁªúÊãìÊâë‰∏≠ÂÆûÁé∞ÊúâÊïàÁöÑ‰ª£ÁêÜÂçè‰ΩúÔºåÂπ∂ÊîØÊåÅ‰∏ÄÂçÉÂ§ö‰∏™‰ª£ÁêÜ‰πãÈó¥ÁöÑÂêà‰Ωú„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàë‰ª¨ËßÇÂØüÂà∞‰∫Ü‰∏ÄÁßçÂ∞è‰∏ñÁïåÂçè‰ΩúÁé∞Ë±°ÔºåÂÖ∂‰∏≠Á±ª‰ºº‰∫éÂ∞è‰∏ñÁïåÂ±ûÊÄßÁöÑÊãìÊâëÁªìÊûÑÂèñÂæó‰∫ÜÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Á°ÆÂÆö‰∫Ü‰∏Ä‰∏™Âçè‰ΩúÊâ©Â±ïÂÆöÂæãÔºåË°®ÊòéÂΩí‰∏ÄÂåñËß£ÂÜ≥ÊñπÊ°àË¥®ÈáèÈÅµÂæ™ÈÄªËæëÂ¢ûÈïøÊ®°Âºè‰Ωú‰∏∫Êâ©Â±ï‰ª£ÁêÜÔºåÂçè‰ΩúÊ∂åÁé∞ÊØîÂÖàÂâçËßÇÂØüÂà∞ÁöÑÁ•ûÁªèÊ∂åÁé∞ÂÆû‰æãÂèëÁîüÂæóÊõ¥Êó©„ÄÇ‰ª£Á†ÅÂíåÊï∞ÊçÆÂ∞ÜÂèØÂú® https://github.com/OpenBMB/ChatDev Ëé∑Âæó„ÄÇ</paragraph>

##### **Mining Frequent Structures in Conceptual Models**
2406.07129v1 by Mattia Fumagalli, Tiago Prince Sales, Pedro Paulo F. Barcelos, Giovanni Micale, Vadim Zaytsev, Diego Calvanese, Giancarlo Guizzardi

The problem of using structured methods to represent knowledge is well-known
in conceptual modeling and has been studied for many years. It has been proven
that adopting modeling patterns represents an effective structural method.
Patterns are, indeed, generalizable recurrent structures that can be exploited
as solutions to design problems. They aid in understanding and improving the
process of creating models. The undeniable value of using patterns in
conceptual modeling was demonstrated in several experimental studies. However,
discovering patterns in conceptual models is widely recognized as a highly
complex task and a systematic solution to pattern identification is currently
lacking. In this paper, we propose a general approach to the problem of
discovering frequent structures, as they occur in conceptual modeling
languages. As proof of concept for our scientific contribution, we provide an
implementation of the approach, by focusing on UML class diagrams, in
particular OntoUML models. This implementation comprises an exploratory tool,
which, through the combination of a frequent subgraph mining algorithm and
graph manipulation techniques, can process multiple conceptual models and
discover recurrent structures according to multiple criteria. The primary
objective is to offer a support facility for language engineers. This can be
employed to leverage both good and bad modeling practices, to evolve and
maintain the conceptual modeling language, and to promote the reuse of encoded
experience in designing better models with the given language.

ÊëòË¶ÅÔºöÁµêÊßãÂåñÊñπÊ≥ïÁî®ÊñºË°®Á§∫Áü•Ë≠òÁöÑÂïèÈ°åÂú®Ê¶ÇÂøµÂª∫Ê®°‰∏≠ÊòØÁúæÊâÄÂë®Áü•ÁöÑÔºå‰∏¶‰∏îÂ∑≤Á∂ìÁ†îÁ©∂Â§öÂπ¥„ÄÇÂ∑≤Á∂ìË≠âÊòéÊé°Áî®Âª∫Ê®°Ê®°Âºè‰ª£Ë°®‰∏ÄÁ®ÆÊúâÊïàÁöÑÁµêÊßãÂåñÊñπÊ≥ï„ÄÇÊ®°ÂºèÁ¢∫ÂØ¶ÊòØÂèØÊ¶ÇÊã¨ÁöÑÈÅûËø¥ÁµêÊßãÔºåÂèØ‰ª•‰ΩúÁÇ∫Ë®≠Ë®àÂïèÈ°åÁöÑËß£Ê±∫ÊñπÊ°àÂä†‰ª•Âà©Áî®„ÄÇÂÆÉÂÄëÊúâÂä©ÊñºÁêÜËß£ÂíåÊîπÈÄ≤Âª∫Á´ãÊ®°ÂûãÁöÑÈÅéÁ®ã„ÄÇÂú®Â§öÈ†ÖÂØ¶È©óÁ†îÁ©∂‰∏≠Ë≠âÊòé‰∫ÜÂú®Ê¶ÇÂøµÂª∫Ê®°‰∏≠‰ΩøÁî®Ê®°ÂºèÁöÑ‰∏çÂèØÂê¶Ë™çÂÉπÂÄº„ÄÇÁÑ∂ËÄåÔºåÁôºÁèæÊ¶ÇÂøµÊ®°Âûã‰∏≠ÁöÑÊ®°ÂºèË¢´Âª£Ê≥õË™çÁÇ∫ÊòØ‰∏ÄÈ†ÖÈ´òÂ∫¶Ë§áÈõúÁöÑ‰ªªÂãôÔºåËÄå‰∏îÁõÆÂâçÁº∫‰πèÊ®°ÂºèË≠òÂà•ÁöÑÁ≥ªÁµ±ÊÄßËß£Ê±∫ÊñπÊ°à„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁôºÁèæÈ†ªÁπÅÁµêÊßãÂïèÈ°åÁöÑ‰∏ÄËà¨ÊñπÊ≥ïÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÂá∫ÁèæÂú®Ê¶ÇÂøµÂª∫Ê®°Ë™ûË®Ä‰∏≠„ÄÇ‰ΩúÁÇ∫ÊàëÂÄëÁßëÂ≠∏Ë≤¢ÁçªÁöÑÊ¶ÇÂøµÈ©óË≠âÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÁöÑÂØ¶ÁèæÔºåÈáçÈªûÈóúÊ≥® UML È°ûÂà•ÂúñÔºåÁâπÂà•ÊòØ OntoUML Ê®°Âûã„ÄÇÊ≠§ÂØ¶ÁèæÂåÖÂê´‰∏ÄÂÄãÊé¢Á¥¢Â∑•ÂÖ∑ÔºåË©≤Â∑•ÂÖ∑ÈÄöÈÅéÁµêÂêàÈ†ªÁπÅÂ≠êÂúñÊåñÊéòÊºîÁÆóÊ≥ïÂíåÂúñÂΩ¢ËôïÁêÜÊäÄË°ìÔºåÂèØ‰ª•ËôïÁêÜÂ§öÂÄãÊ¶ÇÂøµÊ®°ÂûãÔºå‰∏¶Ê†πÊìöÂ§öÂÄãÊ®ôÊ∫ñÁôºÁèæÈÅûËø¥ÁµêÊßã„ÄÇ‰∏ªË¶ÅÁõÆÊ®ôÊòØÁÇ∫Ë™ûË®ÄÂ∑•Á®ãÂ∏´Êèê‰æõÊîØÊè¥Â∑•ÂÖ∑„ÄÇÈÄôÂèØ‰ª•Áî®‰æÜÂà©Áî®Â•ΩÁöÑÂíåÂ£ûÁöÑÂª∫Ê®°ÂØ¶ÂãôÔºå‰æÜÊºîÈÄ≤ÂíåÁ∂≠Ë≠∑Ê¶ÇÂøµÂª∫Ê®°Ë™ûË®ÄÔºå‰∏¶‰øÉÈÄ≤Âú®Ë®≠Ë®àÊõ¥Â•ΩÁöÑÊ®°ÂûãÊôÇÂ∞çÁ∑®Á¢ºÁ∂ìÈ©óÁöÑÂÜçÂà©Áî®„ÄÇ

##### **Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene Graph**
2406.07113v2 by Sergey Linok, Tatiana Zemskova, Svetlana Ladanova, Roman Titkov, Dmitry Yudin

Locating objects referred to in natural language poses a significant
challenge for autonomous agents. Existing CLIP-based open-vocabulary methods
successfully perform 3D object retrieval with simple (bare) queries but cannot
cope with ambiguous descriptions that demand an understanding of object
relations. To tackle this problem, we propose a modular approach called BBQ
(Beyond Bare Queries), which constructs 3D scene spatial graph representation
with metric edges and utilizes a large language model as a human-to-agent
interface through our deductive scene reasoning algorithm. BBQ employs robust
DINO-powered associations to form 3D objects, an advanced raycasting algorithm
to project them to 2D, and a vision-language model to describe them as graph
nodes. On Replica and ScanNet datasets, we show that the designed method
accurately constructs 3D object-centric maps. We have demonstrated that their
quality takes a leading place for open-vocabulary 3D semantic segmentation
against other zero-shot methods. Also, we show that leveraging spatial
relations is especially effective for scenes containing multiple entities of
the same semantic class. On Sr3D and Nr3D benchmarks, our deductive approach
demonstrates a significant improvement, enabling retrieving objects by complex
queries compared to other state-of-the-art methods. Considering our design
solutions, we achieved a processing speed approximately x3 times faster than
the closest analog. This promising performance enables our approach for usage
in applied intelligent robotics projects. We make the code publicly available
at linukc.github.io/bbq/.

ÊëòË¶ÅÔºö<paragraph>Â∞çÊñºËá™‰∏ª‰ª£ÁêÜËÄåË®ÄÔºåÂÆö‰ΩçËá™ÁÑ∂Ë™ûË®Ä‰∏≠ÊâÄÊåáÊ∂âÁöÑÁâ©‰ª∂ÊúÉÂ∏∂‰æÜÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÁèæÊúâÁöÑÂü∫Êñº CLIP ÁöÑÈñãÊîæÂºèË©ûÂΩôÊñπÊ≥ïÂèØ‰ª•ÊàêÂäüÂú∞‰ΩøÁî®Á∞°ÂñÆÔºàË£∏ÔºâÊü•Ë©¢‰æÜÂü∑Ë°å 3D Áâ©‰ª∂Êì∑ÂèñÔºå‰ΩÜÁÑ°Ê≥ïÊáâÂ∞çÈúÄË¶ÅÁêÜËß£Áâ©‰ª∂Èóú‰øÇÁöÑÊ®°Á®úÂÖ©ÂèØÁöÑÊèèËø∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ BBQÔºàË∂ÖË∂äË£∏Êü•Ë©¢ÔºâÁöÑÊ®°ÁµÑÂåñÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÊßãÂª∫‰∫ÜÂÖ∑ÊúâÂ∫¶ÈáèÈÇäÁ∑£ÁöÑ 3D Â†¥ÊôØÁ©∫ÈñìÂúñÂΩ¢Ë°®Á§∫Ôºå‰∏¶ÈÄèÈÅéÊàëÂÄëÊºîÁππÂ†¥ÊôØÊé®ÁêÜÊºîÁÆóÊ≥ïÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁî®‰Ωú‰∫∫Âà∞‰ª£ÁêÜ‰ªãÈù¢„ÄÇBBQ ‰ΩøÁî®Âº∑Â§ßÁöÑ DINO È©ÖÂãïÈóúËÅØ‰æÜÂΩ¢Êàê 3D Áâ©‰ª∂Ôºå‰∏ÄÁ®ÆÈÄ≤ÈöéÁöÑÂÖâÁ∑öÊäïÂ∞ÑÊºîÁÆóÊ≥ïÂ∞áÂÆÉÂÄëÊäïÂΩ±Âà∞ 2DÔºå‰ª•Âèä‰∏ÄÂÄãË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÂ∞áÂÆÉÂÄëÊèèËø∞ÁÇ∫ÂúñÂΩ¢ÁØÄÈªû„ÄÇÂú® Replica Âíå ScanNet Ë≥áÊñôÈõÜ‰∏äÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊâÄË®≠Ë®àÁöÑÊñπÊ≥ïÂèØ‰ª•Ê∫ñÁ¢∫Âú∞Âª∫Êßã 3D Áâ©‰ª∂ÁÇ∫‰∏≠ÂøÉÁöÑÂ∞çÊáâ„ÄÇÊàëÂÄëÂ∑≤Á∂ìË≠âÊòéÔºåÂÆÉÂÄëÁöÑÂìÅË≥™Âú®ÈñãÊîæÂºèË©ûÂΩô 3D Ë™ûÊÑèÂàÜÂâ≤‰∏≠È†òÂÖàÊñºÂÖ∂‰ªñÈõ∂Ê¨°Â≠∏ÁøíÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂà©Áî®Á©∫ÈñìÈóú‰øÇÂ∞çÊñºÂåÖÂê´Â§öÂÄãÂÖ∑ÊúâÁõ∏ÂêåË™ûÊÑèÈ°ûÂà•ÁöÑÂØ¶È´îÁöÑÂ†¥ÊôØÁâπÂà•ÊúâÊïà„ÄÇÂú® Sr3D Âíå Nr3D Âü∫Ê∫ñ‰∏äÔºåÊàëÂÄëÁöÑÊºîÁππÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÈ°ØËëóÁöÑÊîπÈÄ≤ÔºåËàáÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåËÉΩÂ§†ÈÄèÈÅéË§áÈõúÁöÑÊü•Ë©¢‰æÜÊì∑ÂèñÁâ©‰ª∂„ÄÇËÄÉÈáèÊàëÂÄëÁöÑË®≠Ë®àËß£Ê±∫ÊñπÊ°àÔºåÊàëÂÄëÈÅîÂà∞‰∫ÜÊØîÊúÄÊé•ËøëÁöÑÈ°ûÊØîÂø´Á¥Ñ 3 ÂÄçÁöÑËôïÁêÜÈÄüÂ∫¶„ÄÇÈÄôÁ®ÆÊúâÂâçÈÄîÁöÑÊïàËÉΩ‰ΩøÊàëÂÄëÁöÑÂÅöÊ≥ïËÉΩÂ§†Áî®ÊñºÊáâÁî®Êô∫ÊÖßÊ©üÂô®‰∫∫Â∞àÊ°à‰∏≠„ÄÇÊàëÂÄëÂú® linukc.github.io/bbq/ ÂÖ¨ÈñãÁ®ãÂºèÁ¢º„ÄÇ</paragraph>

##### **DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs**
2406.07080v1 by Haishuo Fang, Xiaodan Zhu, Iryna Gurevych

Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning
autonomous language agents in various real-life applications. To improve the
neural-symbolic reasoning capabilities of language agents powered by Large
Language Models (LLMs) in KGQA, we propose the DecompositionAlignment-Reasoning
Agent (DARA) framework. DARA effectively parses questions into formal queries
through a dual mechanism: high-level iterative task decomposition and low-level
task grounding. Importantly, DARA can be efficiently trained with a small
number of high-quality reasoning trajectories. Our experimental results
demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms
both in-context learning-based agents with GPT-4 and alternative fine-tuned
agents, across different benchmarks in zero-shot evaluation, making such models
more accessible for real-life applications. We also show that DARA attains
performance comparable to state-of-the-art enumerating-and-ranking-based
methods for KGQA.

ÊëòË¶ÅÔºöÂõûÁ≠îÁü•Ë≠òÂúñË°®ÔºàKGQAÔºâ‰∏äÁöÑÂïèÈ°åÊòØÂêÑÁ®ÆÁèæÂØ¶ÁîüÊ¥ªÊáâÁî®‰∏≠ÈÅã‰ΩúËâØÂ•ΩÁöÑËá™‰∏ªË™ûË®Ä‰ª£ÁêÜÁöÑÈóúÈçµ„ÄÇÁÇ∫‰∫ÜÊîπÂñÑÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂú® KGQA ‰∏≠È©ÖÂãïÁöÑË™ûË®Ä‰ª£ÁêÜÁöÑÁ•ûÁ∂ìÁ¨¶ËôüÊé®ÁêÜËÉΩÂäõÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂàÜËß£Â∞çÈΩäÊé®ÁêÜ‰ª£ÁêÜÔºàDARAÔºâÊ°ÜÊû∂„ÄÇDARA ÈÄöÈÅéÈõôÈáçÊ©üÂà∂ÊúâÊïàÂú∞Â∞áÂïèÈ°åËß£ÊûêÁÇ∫Ê≠£ÂºèÊü•Ë©¢ÔºöÈ´òÁ¥öÂà•Ëø≠‰ª£‰ªªÂãôÂàÜËß£Âíå‰ΩéÁ¥öÂà•‰ªªÂãôÂü∫Á§é„ÄÇÈáçË¶ÅÁöÑÊòØÔºåDARA ÂèØ‰ª•‰ΩøÁî®Â∞ëÈáèÁöÑÈ´òÂìÅË≥™Êé®ÁêÜËªåË∑°ÈÄ≤Ë°åÊúâÊïàË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂú® LLMÔºà‰æãÂ¶Ç Llama-2-7B„ÄÅMistralÔºâ‰∏äÂæÆË™øÁöÑ DARA Âú®Èõ∂Ê¨°Ë©ï‰º∞ÁöÑ‰∏çÂêåÂü∫Ê∫ñ‰∏äÂÑ™ÊñºÂü∫Êñº‰∏ä‰∏ãÊñá‰∏≠Â≠∏ÁøíÁöÑ‰ª£ÁêÜÔºà‰ΩøÁî® GPT-4ÔºâÂíåÊõø‰ª£ÂæÆË™ø‰ª£ÁêÜÔºåÈÄô‰ΩøÂæóÊ≠§È°ûÊ®°ÂûãÊõ¥ÊòìÊñºÊáâÁî®ÊñºÁèæÂØ¶ÁîüÊ¥ª‰∏≠„ÄÇÊàëÂÄëÈÇÑË°®ÊòéÔºåDARA Áç≤Âæó‰∫ÜËàá KGQA ÁöÑÊúÄÂÖàÈÄ≤ÂàóËàâÂíåÊéíÂêçÊñπÊ≥ïÁõ∏Áï∂ÁöÑÊÄßËÉΩ„ÄÇ

##### **Improving Multi-hop Logical Reasoning in Knowledge Graphs with Context-Aware Query Representation Learning**
2406.07034v1 by Jeonghoon Kim, Heesoo Jung, Hyeju Jang, Hogun Park

Multi-hop logical reasoning on knowledge graphs is a pivotal task in natural
language processing, with numerous approaches aiming to answer First-Order
Logic (FOL) queries. Recent geometry (e.g., box, cone) and probability (e.g.,
beta distribution)-based methodologies have effectively addressed complex FOL
queries. However, a common challenge across these methods lies in determining
accurate geometric bounds or probability parameters for these queries. The
challenge arises because existing methods rely on linear sequential operations
within their computation graphs, overlooking the logical structure of the query
and the relation-induced information that can be gleaned from the relations of
the query, which we call the context of the query. To address the problem, we
propose a model-agnostic methodology that enhances the effectiveness of
existing multi-hop logical reasoning approaches by fully integrating the
context of the FOL query graph. Our approach distinctively discerns (1) the
structural context inherent to the query structure and (2) the relation-induced
context unique to each node in the query graph as delineated in the
corresponding knowledge graph. This dual-context paradigm helps nodes within a
query graph attain refined internal representations throughout the multi-hop
reasoning steps. Through experiments on two datasets, our method consistently
enhances the three multi-hop reasoning foundation models, achieving performance
improvements of up to 19.5%. Our code is available at
https://github.com/kjh9503/caqr.

ÊëòË¶ÅÔºöÂ§öË∑≥ÈÇèËºØÊé®ÁêÜÊòØËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÁöÑÈóúÈçµ‰ªªÂãôÔºåË®±Â§öÊñπÊ≥ïÊó®Âú®ÂõûÁ≠î‰∏ÄÈöéÈÇèËºØ (FOL) Êü•Ë©¢„ÄÇÊúÄËøëÁöÑÂπæ‰ΩïÂΩ¢ÁãÄÔºà‰æãÂ¶ÇÔºåÁõíÂ≠ê„ÄÅÂúìÈåêÔºâÂíåÊ©üÁéáÔºà‰æãÂ¶ÇÔºåË≤ùÂ°îÂàÜ‰ΩàÔºâÊñπÊ≥ïÊúâÊïàÂú∞Ëß£Ê±∫‰∫ÜË§áÈõúÁöÑ FOL Êü•Ë©¢„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÁöÑÂÖ±ÂêåÊåëÊà∞Âú®ÊñºÔºåÁÇ∫ÈÄô‰∫õÊü•Ë©¢Á¢∫ÂÆöÊ∫ñÁ¢∫ÁöÑÂπæ‰ΩïÁïåÈôêÊàñÊ©üÁéáÂèÉÊï∏„ÄÇÊåëÊà∞Âá∫ÁèæÁöÑÂéüÂõ†Âú®ÊñºÔºåÁèæÊúâÊñπÊ≥ï‰æùË≥¥ÊñºÂÖ∂ÈÅãÁÆóÂúñÂΩ¢‰∏≠ÁöÑÁ∑öÊÄßÈ†ÜÂ∫èÈÅãÁÆóÔºåÂøΩÁï•‰∫ÜÊü•Ë©¢ÁöÑÈÇèËºØÁµêÊßã‰ª•ÂèäÂèØ‰ª•ÂæûÊü•Ë©¢Èóú‰øÇ‰∏≠Êî∂ÈõÜÂà∞ÁöÑÈóú‰øÇË™òÂ∞éË≥áË®äÔºåÊàëÂÄëÁ®±‰πãÁÇ∫Êü•Ë©¢ÁöÑËÉåÊôØ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆËàáÊ®°ÂûãÁÑ°ÈóúÁöÑÊñπÊ≥ïÔºåÈÄèÈÅéÂÆåÂÖ®Êï¥Âêà FOL Êü•Ë©¢ÂúñÂΩ¢ÁöÑËÉåÊôØÔºå‰æÜÊèêÂçáÁèæÊúâÂ§öË∑≥ÈÇèËºØÊé®ÁêÜÊñπÊ≥ïÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÁç®ÁâπÂú∞Ëæ®Âà•‰∫Ü (1) Êü•Ë©¢ÁµêÊßãÂõ∫ÊúâÁöÑÁµêÊßãËÉåÊôØÔºå‰ª•Âèä (2) Êü•Ë©¢ÂúñÂΩ¢‰∏≠ÊØèÂÄãÁØÄÈªûÁç®ÊúâÁöÑÈóú‰øÇË™òÂ∞éËÉåÊôØÔºåÂ¶ÇÂ∞çÊáâÁöÑÁü•Ë≠òÂúñÂΩ¢‰∏≠ÊâÄÊèèÁπ™ÁöÑ„ÄÇÈÄôÁ®ÆÈõôÈáçËÉåÊôØÁØÑ‰æãÊúâÂä©ÊñºÊü•Ë©¢ÂúñÂΩ¢‰∏≠ÁöÑÁØÄÈªûÂú®Â§öË∑≥Êé®ÁêÜÊ≠•È©ü‰∏≠Áç≤ÂæóÁ≤æÁ∑ªÁöÑÂÖßÈÉ®Ë°®Á§∫„ÄÇÈÄèÈÅéÂú®ÂÖ©ÂÄãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂØ¶È©óÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊåÅÁ∫åÂ¢ûÂº∑‰∏âÁ®ÆÂ§öË∑≥Êé®ÁêÜÂü∫Á§éÊ®°ÂûãÔºåÊïàËÉΩÊèêÂçáÊúÄÈ´òÈÅî 19.5%„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/kjh9503/caqr ÂèñÂæó„ÄÇ

##### **MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension**
2406.06777v2 by Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V. Chawla

Recently, Large Language Models (LLMs) with their strong task-handling
capabilities have shown remarkable advancements across a spectrum of fields,
moving beyond natural language understanding. However, their proficiency within
the chemistry domain remains restricted, especially in solving professional
molecule-related tasks. This challenge is attributed to their inherent
limitations in comprehending molecules using only common textual
representations, i.e., SMILES strings. In this study, we seek to enhance the
ability of LLMs to comprehend molecules by designing and equipping them with a
multi-modal external module, namely MolX. In particular, instead of directly
using a SMILES string to represent a molecule, we utilize specific encoders to
extract fine-grained features from both SMILES string and 2D molecular graph
representations for feeding into an LLM. Moreover, a human-defined molecular
fingerprint is incorporated to leverage its embedded domain knowledge. Then, to
establish an alignment between MolX and the LLM's textual input space, the
whole model in which the LLM is frozen, is pre-trained with a versatile
strategy including a diverse set of tasks. Extensive experimental evaluations
demonstrate that our proposed method only introduces a small number of
trainable parameters while outperforming baselines on various downstream
molecule-related tasks ranging from molecule-to-text translation to
retrosynthesis, with and without fine-tuning the LLM.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÔºåÂÖ∑ÊúâÂº∫Â§ß‰ªªÂä°Â§ÑÁêÜËÉΩÂäõÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®ÂêÑ‰∏™È¢ÜÂüüÈÉΩÂèñÂæó‰∫ÜÊòæÁùÄËøõÊ≠•ÔºåË∂ÖË∂ä‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨Âú®ÂåñÂ≠¶È¢ÜÂüüÁöÑÁÜüÁªÉÁ®ãÂ∫¶‰ªçÁÑ∂ÂèóÂà∞ÈôêÂà∂ÔºåÂ∞§ÂÖ∂ÊòØÂú®Ëß£ÂÜ≥‰∏ì‰∏öÁöÑÂàÜÂ≠êÁõ∏ÂÖ≥‰ªªÂä°ÊñπÈù¢„ÄÇËøô‰∏ÄÊåëÊàòÂΩíÂõ†‰∫éÂÆÉ‰ª¨Âú®‰ªÖ‰ΩøÁî®Â∏∏ËßÅÊñáÊú¨Ë°®Á§∫ÔºàÂç≥ SMILES Â≠óÁ¨¶‰∏≤ÔºâÁêÜËß£ÂàÜÂ≠êÊñπÈù¢ÁöÑÂõ∫ÊúâÂ±ÄÈôêÊÄß„ÄÇÂú®ËøôÈ°πÁ†îÁ©∂‰∏≠ÔºåÊàë‰ª¨ÂØªÊ±ÇÈÄöËøáËÆæËÆ°Âíå‰∏∫ÂÖ∂ÈÖçÂ§á‰∏Ä‰∏™Â§öÊ®°ÊÄÅÂ§ñÈÉ®Ê®°ÂùóÔºàÂç≥ MolXÔºâÊù•Â¢ûÂº∫ LLM ÁêÜËß£ÂàÜÂ≠êÁöÑËÉΩÂäõ„ÄÇÁâπÂà´ÊòØÔºåÊàë‰ª¨Âà©Áî®ÁâπÂÆöÁºñÁ†ÅÂô®‰ªé SMILES Â≠óÁ¨¶‰∏≤Âíå 2D ÂàÜÂ≠êÂõæË°®Á§∫‰∏≠ÊèêÂèñÁªÜÁ≤íÂ∫¶ÁâπÂæÅ‰ª•ËæìÂÖ• LLMÔºåËÄå‰∏çÊòØÁõ¥Êé•‰ΩøÁî® SMILES Â≠óÁ¨¶‰∏≤Êù•Ë°®Á§∫ÂàÜÂ≠ê„ÄÇÊ≠§Â§ñÔºåËøòÁ∫≥ÂÖ•‰∫Ü‰∫∫Á±ªÂÆö‰πâÁöÑÂàÜÂ≠êÊåáÁ∫π‰ª•Âà©Áî®ÂÖ∂ÂµåÂÖ•ÁöÑÈ¢ÜÂüüÁü•ËØÜ„ÄÇÁÑ∂ÂêéÔºå‰∏∫‰∫ÜÂú® MolX Âíå LLM ÁöÑÊñáÊú¨ËæìÂÖ•Á©∫Èó¥‰πãÈó¥Âª∫Á´ã‰∏Ä‰∏™ÂØπÈΩêÔºåÊï¥‰∏™Ê®°ÂûãÔºàÂÖ∂‰∏≠ LLM Ë¢´ÂÜªÁªìÔºâ‰ΩøÁî®ÂåÖÊã¨ÂêÑÁßç‰ªªÂä°Âú®ÂÜÖÁöÑ‰∏Ä‰∏™ÈÄöÁî®Á≠ñÁï•ËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇÂπøÊ≥õÁöÑÂÆûÈ™åËØÑ‰º∞Ë°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ï‰ªÖÂºïÂÖ•‰∫ÜÂ∞ëÈáèÂèØËÆ≠ÁªÉÂèÇÊï∞ÔºåÂêåÊó∂Âú®ÂêÑÁßç‰∏ãÊ∏∏ÂàÜÂ≠êÁõ∏ÂÖ≥‰ªªÂä°ÔºàÂåÖÊã¨ÂàÜÂ≠êÂà∞ÊñáÊú¨ÁøªËØëÂà∞ÈÄÜÂêàÊàêÔºâ‰∏≠‰ºò‰∫éÂü∫Á∫øÔºåÊó†ËÆ∫ÊòØÂê¶ÂØπ LLM ËøõË°åÂæÆË∞É„ÄÇ</paragraph>

##### **The Curse of Popularity: Popular Entities have Catastrophic Side Effects when Deleting Knowledge from Language Models**
2406.06032v1 by Ryosuke Takahashi, Go Kamoda, Benjamin Heinzerling, Keisuke Sakaguchi, Kentaro Inui

Language models (LMs) encode world knowledge in their internal parameters
through training. However, LMs may learn personal and confidential information
from the training data, leading to privacy concerns such as data leakage.
Therefore, research on knowledge deletion from LMs is essential. This study
focuses on the knowledge stored in LMs and analyzes the relationship between
the side effects of knowledge deletion and the entities related to the
knowledge. Our findings reveal that deleting knowledge related to popular
entities can have catastrophic side effects. Furthermore, this research is the
first to analyze knowledge deletion in models trained on synthetic knowledge
graphs, indicating a new direction for controlled experiments.

ÊëòË¶ÅÔºöË™ûË®ÄÊ®°Âûã (LM) ÈÄèÈÅéË®ìÁ∑¥Â∞á‰∏ñÁïåÁü•Ë≠òÁ∑®Á¢ºÂú®ÂÖ∂ÂÖßÈÉ®ÂèÉÊï∏‰∏≠„ÄÇÁÑ∂ËÄåÔºåLM ÂèØËÉΩÊúÉÂæûË®ìÁ∑¥Ë≥áÊñô‰∏≠Â≠∏ÁøíÂà∞ÂÄã‰∫∫ÂíåÊ©üÂØÜË≥áË®äÔºåÂ∞éËá¥Ë≥áÊñôÂ§ñÊ¥©Á≠âÈö±ÁßÅÂïèÈ°å„ÄÇÂõ†Ê≠§ÔºåÁ†îÁ©∂Âæû LM ‰∏≠Âà™Èô§Áü•Ë≠òËá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ËëóÈáçÊñºÂÑ≤Â≠òÂú® LM ‰∏≠ÁöÑÁü•Ë≠òÔºå‰∏¶ÂàÜÊûêÁü•Ë≠òÂà™Èô§ÁöÑÂâØ‰ΩúÁî®ËàáËàáÁü•Ë≠òÁõ∏ÈóúÁöÑÂØ¶È´î‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫ÔºåÂà™Èô§ËàáÁÜ±ÈñÄÂØ¶È´îÁõ∏ÈóúÁöÑÁü•Ë≠òÂèØËÉΩÊúÉÈÄ†ÊàêÁÅΩÈõ£ÊÄßÁöÑÂâØ‰ΩúÁî®„ÄÇÊ≠§Â§ñÔºåÊú¨Á†îÁ©∂È¶ñÊ¨°ÂàÜÊûêÂú®ÂêàÊàêÁü•Ë≠òÂúñË≠ú‰∏äË®ìÁ∑¥ÁöÑÊ®°Âûã‰∏≠ÁöÑÁü•Ë≠òÂà™Èô§ÔºåÊåáÂá∫ÂèóÊéßÂØ¶È©óÁöÑÊñ∞ÊñπÂêë„ÄÇ

##### **HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs**
2406.06027v1 by Pranoy Panda, Ankush Agarwal, Chaitanya Devaguptapu, Manohar Kaul, Prathosh A P

Given unstructured text, Large Language Models (LLMs) are adept at answering
simple (single-hop) questions. However, as the complexity of the questions
increase, the performance of LLMs degrade. We believe this is due to the
overhead associated with understanding the complex question followed by
filtering and aggregating unstructured information in the raw text. Recent
methods try to reduce this burden by integrating structured knowledge triples
into the raw text, aiming to provide a structured overview that simplifies
information processing. However, this simplistic approach is query-agnostic and
the extracted facts are ambiguous as they lack context. To address these
drawbacks and to enable LLMs to answer complex (multi-hop) questions with ease,
we propose to use a knowledge graph (KG) that is context-aware and is distilled
to contain query-relevant information. The use of our compressed distilled KG
as input to the LLM results in our method utilizing up to $67\%$ fewer tokens
to represent the query relevant information present in the supporting
documents, compared to the state-of-the-art (SoTA) method. Our experiments show
consistent improvements over the SoTA across several metrics (EM, F1,
BERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and
MuSiQue).

ÊëòË¶ÅÔºöÁµ¶ÂÆöÈùûÁµêÊßãÂåñÊñáÊú¨ÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊìÖÈï∑ÂõûÁ≠îÁ∞°ÂñÆÔºàÂñÆË∑≥ÔºâÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåÈö®ËëóÂïèÈ°åÁöÑË§áÈõúÊÄßÂ¢ûÂä†ÔºåLLM ÁöÑÊïàËÉΩÊúÉ‰∏ãÈôç„ÄÇÊàëÂÄëÁõ∏‰ø°ÈÄôÊòØÂõ†ÁÇ∫ÁêÜËß£Ë§áÈõúÂïèÈ°åÊâÄ‰º¥Èö®ÁöÑÈñãÈä∑ÔºåÊé•ËëóÂú®ÂéüÂßãÊñáÊú¨‰∏≠ÈÅéÊøæÂíåÂΩôÁ∏ΩÈùûÁµêÊßãÂåñË≥áË®ä„ÄÇÊúÄËøëÁöÑÊñπÊ≥ïÂòóË©¶ÈÄèÈÅéÂ∞áÁµêÊßãÂåñÁü•Ë≠ò‰∏âÂÖÉÁµÑÊï¥ÂêàÂà∞ÂéüÂßãÊñáÊú¨‰∏≠‰æÜÊ∏õËºïÈÄôÂÄãË≤†ÊìîÔºåÁõÆÁöÑÊòØÊèê‰æõ‰∏ÄÂÄãÁ∞°ÂåñË≥áË®äËôïÁêÜÁöÑÁµêÊßãÂåñÊ¶ÇËßÄ„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÁ∞°ÂåñÁöÑÊñπÂºèËàáÊü•Ë©¢ÁÑ°ÈóúÔºåËÄå‰∏îÊèêÂèñÁöÑ‰∫ãÂØ¶Ê®°Á®úÂÖ©ÂèØÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁº∫‰πèËÉåÊôØ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÁº∫ÈªûÔºå‰∏¶‰Ωø LLM ËÉΩÂ§†ËºïÈ¨ÜÂõûÁ≠îË§áÈõúÔºàÂ§öË∑≥ÔºâÂïèÈ°åÔºåÊàëÂÄëÂª∫Ë≠∞‰ΩøÁî®‰∏ÄÂÄãËàáËÉåÊôØÁõ∏Èóú‰∏îÁ∂ìÈÅéÊèêÁÖâ‰ª•ÂåÖÂê´ËàáÊü•Ë©¢Áõ∏ÈóúË≥áË®äÁöÑÁü•Ë≠òÂúñË≠ú (KG)„ÄÇÂ∞áÊàëÂÄëÂ£ìÁ∏ÆÊèêÁÖâÁöÑ KG Áî®‰Ωú LLM ÁöÑËº∏ÂÖ•Ôºå‰ΩøÂæóÊàëÂÄëÁöÑÊ®°Âûã‰ΩøÁî®ÊØîÊúÄÂÖàÈÄ≤ (SoTA) ÊñπÊ≥ïÊ∏õÂ∞ëÂ§öÈÅî $67\%$ ÁöÑÊ®ôË®ò‰æÜË°®Á§∫ÊîØÊè¥Êñá‰ª∂‰∏≠ÁöÑËàáÊü•Ë©¢Áõ∏ÈóúÁöÑË≥áË®ä„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÂú®ÂÖ©ÂÄãÊµÅË°åÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜÔºàHotpotQA Âíå MuSiQueÔºâ‰∏äÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®Â§öÈ†ÖÊåáÊ®ôÔºàEM„ÄÅF1„ÄÅBERTScore Âíå‰∫∫Â∑•Ë©ï‰º∞Ôºâ‰∏≠ÈÉΩÊØî SoTA ÊúâÈ°ØËëóÁöÑÊîπÂñÑ„ÄÇ

##### **Generalist Multimodal AI: A Review of Architectures, Challenges and Opportunities**
2406.05496v1 by Sai Munikoti, Ian Stewart, Sameera Horawalavithana, Henry Kvinge, Tegan Emerson, Sandra E Thompson, Karl Pazdernik

Multimodal models are expected to be a critical component to future advances
in artificial intelligence. This field is starting to grow rapidly with a surge
of new design elements motivated by the success of foundation models in natural
language processing (NLP) and vision. It is widely hoped that further extending
the foundation models to multiple modalities (e.g., text, image, video, sensor,
time series, graph, etc.) will ultimately lead to generalist multimodal models,
i.e. one model across different data modalities and tasks. However, there is
little research that systematically analyzes recent multimodal models
(particularly the ones that work beyond text and vision) with respect to the
underling architecture proposed. Therefore, this work provides a fresh
perspective on generalist multimodal models (GMMs) via a novel architecture and
training configuration specific taxonomy. This includes factors such as
Unifiability, Modularity, and Adaptability that are pertinent and essential to
the wide adoption and application of GMMs. The review further highlights key
challenges and prospects for the field and guide the researchers into the new
advancements.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÊ®°ÂûãÈ¢ÑËÆ°Â∞ÜÊàê‰∏∫‰∫∫Â∑•Êô∫ËÉΩÊú™Êù•ÂèëÂ±ïÁöÑÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜ„ÄÇÈöèÁùÄËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ (NLP) ÂíåËßÜËßâÈ¢ÜÂüüÁöÑÂ∫ïÂ±ÇÊ®°ÂûãÂèñÂæóÊàêÂäüÔºåËøô‰∏ÄÈ¢ÜÂüüÂºÄÂßãËøÖÈÄüÂèëÂ±ïÔºåÂπ∂Ê∂åÁé∞Âá∫Â§ßÈáèÂèóÂÖ∂ÂêØÂèëÁöÑÊñ∞ËÆæËÆ°ÂÖÉÁ¥†„ÄÇ‰∫∫‰ª¨ÊôÆÈÅçÂ∏åÊúõÔºåÂ∞ÜÂ∫ïÂ±ÇÊ®°ÂûãËøõ‰∏ÄÊ≠•Êâ©Â±ïÂà∞Â§öÁßçÊ®°ÊÄÅÔºà‰æãÂ¶ÇÔºåÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅËßÜÈ¢ë„ÄÅ‰º†ÊÑüÂô®„ÄÅÊó∂Èó¥Â∫èÂàó„ÄÅÂõæÂΩ¢Á≠âÔºâÊúÄÁªàÂ∞Ü‰∫ßÁîüÈÄöÁî®Â§öÊ®°ÊÄÅÊ®°ÂûãÔºåÂç≥‰∏Ä‰∏™Ë∑®‰∏çÂêåÊï∞ÊçÆÊ®°ÊÄÅÂíå‰ªªÂä°ÁöÑÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåÂæàÂ∞ëÊúâÁ†îÁ©∂Á≥ªÁªüÂú∞ÂàÜÊûêÊúÄËøëÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàÁâπÂà´ÊòØÈÇ£‰∫õÂú®ÊñáÊú¨ÂíåËßÜËßâ‰πãÂ§ñÂ∑•‰ΩúÁöÑÊ®°ÂûãÔºâ‰∏éÂÖ∂ÊèêÂá∫ÁöÑÂ∫ïÂ±ÇÊû∂ÊûÑ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂõ†Ê≠§ÔºåËøôÈ°πÂ∑•‰ΩúÈÄöËøá‰∏ÄÁßçÊñ∞È¢ñÁöÑÊû∂ÊûÑÂíåËÆ≠ÁªÉÈÖçÁΩÆÁâπÂÆöÂàÜÁ±ªÊ≥ïÔºå‰∏∫ÈÄöÁî®Â§öÊ®°ÊÄÅÊ®°Âûã (GMM) Êèê‰æõ‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑËßÜËßí„ÄÇËøôÂåÖÊã¨Áªü‰∏ÄÊÄß„ÄÅÊ®°ÂùóÂåñÂíåÈÄÇÂ∫îÊÄßÁ≠âÂõ†Á¥†ÔºåËøô‰∫õÂõ†Á¥†‰∏é GMM ÁöÑÂπøÊ≥õÈááÁî®ÂíåÂ∫îÁî®ÂØÜÂàáÁõ∏ÂÖ≥‰∏îËá≥ÂÖ≥ÈáçË¶Å„ÄÇËØ•ÁªºËø∞Ëøõ‰∏ÄÊ≠•Âº∫Ë∞É‰∫ÜËØ•È¢ÜÂüüÁöÑÂÖ≥ÈîÆÊåëÊàòÂíåÂâçÊôØÔºåÂπ∂ÊåáÂØºÁ†îÁ©∂‰∫∫Âëò‰∫ÜËß£Êñ∞ÁöÑËøõÂ±ï„ÄÇ

##### **TLEX: An Efficient Method for Extracting Exact Timelines from TimeML Temporal Graphs**
2406.05265v1 by Mustafa Ocal, Ning Xie, Mark Finlayson

A timeline provides a total ordering of events and times, and is useful for a
number of natural language understanding tasks. However, qualitative temporal
graphs that can be derived directly from text -- such as TimeML annotations --
usually explicitly reveal only partial orderings of events and times. In this
work, we apply prior work on solving point algebra problems to the task of
extracting timelines from TimeML annotated texts, and develop an exact,
end-to-end solution which we call TLEX (TimeLine EXtraction). TLEX transforms
TimeML annotations into a collection of timelines arranged in a
trunk-and-branch structure. Like what has been done in prior work, TLEX checks
the consistency of the temporal graph and solves it; however, it adds two novel
functionalities. First, it identifies specific relations involved in an
inconsistency (which could then be manually corrected) and, second, TLEX
performs a novel identification of sections of the timelines that have
indeterminate order, information critical for downstream tasks such as aligning
events from different timelines. We provide detailed descriptions and analysis
of the algorithmic components in TLEX, and conduct experimental evaluations by
applying TLEX to 385 TimeML annotated texts from four corpora. We show that 123
of the texts are inconsistent, 181 of them have more than one ``real world'' or
main timeline, and there are 2,541 indeterminate sections across all four
corpora. A sampling evaluation showed that TLEX is 98--100% accurate with 95%
confidence along five dimensions: the ordering of time-points, the number of
main timelines, the placement of time-points on main versus subordinate
timelines, the connecting point of branch timelines, and the location of the
indeterminate sections. We provide a reference implementation of TLEX, the
extracted timelines for all texts, and the manual corrections of the
inconsistent texts.

ÊëòË¶ÅÔºöÊôÇÈñìËª∏Êèê‰æõ‰∫ã‰ª∂ÂíåÊôÇÈñìÁöÑÁ∏ΩÈ´îÈ†ÜÂ∫èÔºå‰∏¶ÂèØÁî®ÊñºË®±Â§öËá™ÁÑ∂Ë™ûË®ÄÁêÜËß£‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÂèØÁõ¥Êé•ÂæûÊñáÊú¨‰∏≠Ë°çÁîüÁöÑÂÆöÊÄßÊôÇÈñìÂúñË°®Ôºà‰æãÂ¶Ç TimeML Ê®ôË®ªÔºâÈÄöÂ∏∏Âè™ÊòéÁ¢∫Êè≠Á§∫‰∫ã‰ª∂ÂíåÊôÇÈñìÁöÑÈÉ®ÂàÜÈ†ÜÂ∫è„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂ∞áÂÖàÂâçËß£Ê±∫Èªû‰ª£Êï∏ÂïèÈ°åÁöÑÂ∑•‰ΩúÊáâÁî®ÊñºÂæû TimeML Ê®ôË®ªÊñáÊú¨‰∏≠ÊèêÂèñÊôÇÈñìËª∏ÁöÑ‰ªªÂãôÔºå‰∏¶ÈñãÁôº‰∫Ü‰∏ÄÂÄãÁ≤æÁ¢∫ÁöÑÁ´ØÂà∞Á´ØËß£Ê±∫ÊñπÊ°àÔºåÊàëÂÄëÁ®±‰πãÁÇ∫ TLEXÔºàÊôÇÈñìÁ∑öÊèêÂèñÔºâ„ÄÇTLEX Â∞á TimeML Ê®ôË®ªËΩâÊèõÁÇ∫‰∏ÄÁ≥ªÂàóÊôÇÈñìËª∏Ôºå‰∏¶‰ª•‰∏ªÂππÁµêÊßãÊéíÂàó„ÄÇËàáÂÖàÂâçÁöÑÂ∑•‰Ωú‰∏ÄÊ®£ÔºåTLEX Ê™¢Êü•ÊôÇÈñìÂúñË°®ÁöÑËá™Ê¥ΩÊÄß‰∏¶Ëß£Ê±∫ÂÆÉÔºõÁÑ∂ËÄåÔºåÂÆÉÂ¢ûÂä†‰∫ÜÂÖ©ÂÄãÊñ∞Á©éÁöÑÂäüËÉΩ„ÄÇÈ¶ñÂÖàÔºåÂÆÉË≠òÂà•Âá∫‰∏ç‰∏ÄËá¥‰∏≠Ê∂âÂèäÁöÑÂÖ∑È´îÈóú‰øÇÔºàÁÑ∂ÂæåÂèØ‰ª•ÊâãÂãïÊõ¥Ê≠£ÔºâÔºåÂÖ∂Ê¨°ÔºåTLEX Â∞çÂÖ∑Êúâ‰∏çÁ¢∫ÂÆöÈ†ÜÂ∫èÁöÑÊôÇÈñìËª∏ÈÉ®ÂàÜÈÄ≤Ë°å‰∫ÜÊñ∞ÁöÑË≠òÂà•ÔºåÈÄô‰∫õ‰ø°ÊÅØÂ∞çÊñº‰∏ãÊ∏∏‰ªªÂãôÔºà‰æãÂ¶ÇÂ∞çÈΩä‰æÜËá™‰∏çÂêåÊôÇÈñìËª∏ÁöÑ‰∫ã‰ª∂ÔºâËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÊèê‰æõ‰∫Ü TLEX ‰∏≠ÊºîÁÆóÊ≥ïÁµÑÊàêÁöÑË©≥Á¥∞ÊèèËø∞ÂíåÂàÜÊûêÔºå‰∏¶ÈÄöÈÅéÂ∞á TLEX ÊáâÁî®Êñº‰æÜËá™ÂõõÂÄãË™ûÊñôÂ∫´ÁöÑ 385 ÂÄã TimeML Ê®ôË®ªÊñáÊú¨ÈÄ≤Ë°åÂØ¶È©óË©ï‰º∞„ÄÇÊàëÂÄëË°®Êòé 123 ÂÄãÊñáÊú¨ÊòØ‰∏ç‰∏ÄËá¥ÁöÑÔºåÂÖ∂‰∏≠ 181 ÂÄãÊúâÂ§öÂÄã„ÄåÁúüÂØ¶‰∏ñÁïå„ÄçÊàñ‰∏ªÊôÇÈñìËª∏Ôºå‰∏¶‰∏îÂú®ÊâÄÊúâÂõõÂÄãË™ûÊñôÂ∫´‰∏≠ÂÖ±Êúâ 2,541 ÂÄã‰∏çÁ¢∫ÂÆöÈÉ®ÂàÜ„ÄÇÊäΩÊ®£Ë©ï‰º∞Ë°®ÊòéÔºåTLEX Âú®‰∫îÂÄãÁ∂≠Â∫¶‰∏äÂÖ∑Êúâ 98-100% ÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÁΩÆ‰ø°Â∫¶ÁÇ∫ 95%ÔºöÊôÇÈñìÈªûÁöÑÈ†ÜÂ∫è„ÄÅ‰∏ªÊôÇÈñìËª∏ÁöÑÊï∏Èáè„ÄÅÊôÇÈñìÈªûÂú®‰∏ªÊôÇÈñìËª∏ËàáÂæûÂ±¨ÊôÇÈñìËª∏‰∏äÁöÑ‰ΩçÁΩÆ„ÄÅÂàÜÊîØÊôÇÈñìËª∏ÁöÑÈÄ£Êé•Èªû‰ª•Âèä‰∏çÁ¢∫ÂÆöÈÉ®ÂàÜÁöÑ‰ΩçÁΩÆ„ÄÇÊàëÂÄëÊèê‰æõ‰∫Ü TLEX ÁöÑÂèÉËÄÉÂØ¶Áèæ„ÄÅÊâÄÊúâÊñáÊú¨ÁöÑÊèêÂèñÊôÇÈñìËª∏‰ª•Âèä‰∏ç‰∏ÄËá¥ÊñáÊú¨ÁöÑÊâãÂãïÊõ¥Ê≠£„ÄÇ

##### **LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph Question-Answering**
2406.06621v1 by Harry Li, Gabriel Appleby, Ashley Suh

We present LinkQ, a system that leverages a large language model (LLM) to
facilitate knowledge graph (KG) query construction through natural language
question-answering. Traditional approaches often require detailed knowledge of
complex graph querying languages, limiting the ability for users -- even
experts -- to acquire valuable insights from KG data. LinkQ simplifies this
process by first interpreting a user's question, then converting it into a
well-formed KG query. By using the LLM to construct a query instead of directly
answering the user's question, LinkQ guards against the LLM hallucinating or
generating false, erroneous information. By integrating an LLM into LinkQ,
users are able to conduct both exploratory and confirmatory data analysis, with
the LLM helping to iteratively refine open-ended questions into precise ones.
To demonstrate the efficacy of LinkQ, we conducted a qualitative study with
five KG practitioners and distill their feedback. Our results indicate that
practitioners find LinkQ effective for KG question-answering, and desire future
LLM-assisted systems for the exploratory analysis of graph databases.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÊèêÂá∫ LinkQÔºåÈÄôÊòØ‰∏ÄÂÄãÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄÂïèÁ≠î‰æÜ‰øÉÈÄ≤Áü•Ë≠òÂúñË≠ú (KG) Êü•Ë©¢Âª∫ÊßãÁöÑÁ≥ªÁµ±„ÄÇÂÇ≥Áµ±ÊñπÊ≥ïÈÄöÂ∏∏ÈúÄË¶ÅÂ∞çË§áÈõúÁöÑÂúñÂΩ¢Êü•Ë©¢Ë™ûË®ÄÊúâË©≥Á¥∞ÁöÑ‰∫ÜËß£ÔºåÈÄôÊúÉÈôêÂà∂‰ΩøÁî®ËÄÖÔºàÁîöËá≥ÊòØÂ∞àÂÆ∂ÔºâÂæû KG Ë≥áÊñô‰∏≠Áç≤ÂèñÊúâÂÉπÂÄºË¶ãËß£ÁöÑËÉΩÂäõ„ÄÇLinkQ ÈÄèÈÅéÂÖàË©ÆÈáã‰ΩøÁî®ËÄÖÁöÑÂïèÈ°åÔºåÁÑ∂ÂæåÂ∞áÂÖ∂ËΩâÊèõÊàêÊ†ºÂºèËâØÂ•ΩÁöÑ KG Êü•Ë©¢Ôºå‰æÜÁ∞°ÂåñÈÄôÂÄãÈÅéÁ®ã„ÄÇÈÄèÈÅé‰ΩøÁî® LLM ‰æÜÂª∫ÊßãÊü•Ë©¢ÔºåËÄå‰∏çÊòØÁõ¥Êé•ÂõûÁ≠î‰ΩøÁî®ËÄÖÁöÑÂïèÈ°åÔºåLinkQ ÂèØ‰ª•Èò≤Ê≠¢ LLM Âá∫ÁèæÂπªË¶∫ÊàñÁî¢ÁîüÈåØË™§„ÄÅ‰∏çÂØ¶ÁöÑË≥áË®ä„ÄÇÈÄèÈÅéÂ∞á LLM Êï¥ÂêàÂà∞ LinkQ ‰∏≠Ôºå‰ΩøÁî®ËÄÖÂèØ‰ª•ÈÄ≤Ë°åÊé¢Á¥¢ÊÄßÂíåÈ©óË≠âÊÄßË≥áÊñôÂàÜÊûêÔºåËÄå LLM ÂâáÊúâÂä©ÊñºÂèçË¶ÜÂ∞áÈñãÊîæÂºèÂïèÈ°åÁ≤æÁÖâÊàêÁ≤æÁ¢∫ÁöÑÂïèÈ°å„ÄÇÁÇ∫‰∫ÜË≠âÊòé LinkQ ÁöÑÊïàËÉΩÔºåÊàëÂÄëËàá‰∫î‰Ωç KG ÂØ¶ÂãôÂ∑•‰ΩúËÄÖÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÂÆöÊÄßÁ†îÁ©∂Ôºå‰∏¶Êï¥ÁêÜ‰ªñÂÄëÁöÑÂõûÈ•ã„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂØ¶ÂãôÂ∑•‰ΩúËÄÖË™çÁÇ∫ LinkQ Â∞çÊñº KG ÂïèÁ≠îÂæàÊúâÁî®Ôºå‰∏¶Â∏åÊúõÊú™‰æÜÊúâ LLM ËºîÂä©ÁöÑÁ≥ªÁµ±ÔºåÁî®ÊñºÂúñÂΩ¢Ë≥áÊñôÂ∫´ÁöÑÊé¢Á¥¢ÊÄßÂàÜÊûê„ÄÇ</paragraph>

##### **Compositional Generalization with Grounded Language Models**
2406.04989v1 by Sondre Wold, √âtienne Simon, Lucas Georges Gabriel Charpentier, Egor V. Kostylev, Erik Velldal, Lilja √òvrelid

Grounded language models use external sources of information, such as
knowledge graphs, to meet some of the general challenges associated with
pre-training. By extending previous work on compositional generalization in
semantic parsing, we allow for a controlled evaluation of the degree to which
these models learn and generalize from patterns in knowledge graphs. We develop
a procedure for generating natural language questions paired with knowledge
graphs that targets different aspects of compositionality and further avoids
grounding the language models in information already encoded implicitly in
their weights. We evaluate existing methods for combining language models with
knowledge graphs and find them to struggle with generalization to sequences of
unseen lengths and to novel combinations of seen base components. While our
experimental results provide some insight into the expressive power of these
models, we hope our work and released datasets motivate future research on how
to better combine language models with structured knowledge representations.

ÊëòË¶ÅÔºöÊé•Âú∞Ë™ûË®ÄÊ®°Âûã‰ΩøÁî®Â§ñÈÉ®Ë≥áË®ä‰æÜÊ∫êÔºå‰æãÂ¶ÇÁü•Ë≠òÂúñË≠úÔºå‰ª•Âõ†ÊáâËàáÈ†êË®ìÁ∑¥Áõ∏ÈóúÁöÑ‰∏Ä‰∫õ‰∏ÄËà¨ÊåëÊà∞„ÄÇËóâÁî±Êì¥Â±ïË™ûÊÑèÂâñÊûê‰∏≠ÁµÑÂêàÊ¶ÇÊã¨ÁöÑÂÖàÂâçÂ∑•‰ΩúÔºåÊàëÂÄëÂÖÅË®±Â∞çÈÄô‰∫õÊ®°ÂûãÂæûÁü•Ë≠òÂúñË≠ú‰∏≠ÁöÑÊ®°ÂºèÂ≠∏ÁøíÂíåÊ¶ÇÊã¨ÁöÑÁ®ãÂ∫¶ÈÄ≤Ë°åÂèóÊéßË©ï‰º∞„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÁ®ãÂ∫èÔºåÁî®ÊñºÁî¢ÁîüËàáÁü•Ë≠òÂúñË≠úÈÖçÂ∞çÁöÑËá™ÁÑ∂Ë™ûË®ÄÂïèÈ°åÔºåÈáùÂ∞çÁµÑÂêàÊÄßÁöÑ‰∏çÂêåÈù¢ÂêëÔºå‰∏¶ÈÄ≤‰∏ÄÊ≠•ÈÅøÂÖçÂ∞áË™ûË®ÄÊ®°ÂûãÊé•Âú∞Âà∞ÂÖ∂Ê¨äÈáç‰∏≠Â∑≤Èö±Âê´Á∑®Á¢ºÁöÑË≥áË®ä„ÄÇÊàëÂÄëË©ï‰º∞‰∫ÜÂ∞áË™ûË®ÄÊ®°ÂûãËàáÁü•Ë≠òÂúñË≠úÁµêÂêàÁöÑÁèæÊúâÊñπÊ≥ïÔºåÁôºÁèæÂÆÉÂÄëÈõ£‰ª•Ê¶ÇÊã¨Âà∞Èï∑Â∫¶Êú™Ë¶ãÁöÑÂ∫èÂàóÂíåÂ∑≤Ë¶ãÂü∫Á§éÁµÑ‰ª∂ÁöÑÊñ∞ÁµÑÂêà„ÄÇÈõñÁÑ∂ÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÊèê‰æõ‰∫Ü‰∏Ä‰∫õË¶ãËß£ÔºåË™™Êòé‰∫ÜÈÄô‰∫õÊ®°ÂûãÁöÑË°®ÈÅîËÉΩÂäõÔºå‰ΩÜÊàëÂÄëÂ∏åÊúõÊàëÂÄëÁöÑÂ∑•‰ΩúÂíåÁôºÂ∏ÉÁöÑË≥áÊñôÈõÜËÉΩÊøÄÂãµÊú™‰æÜÁ†îÁ©∂Â¶Ç‰ΩïÂ∞áË™ûË®ÄÊ®°ÂûãËàáÁµêÊßãÂåñÁü•Ë≠òË°®ÂæµÊõ¥Â•ΩÂú∞ÁµêÂêà„ÄÇ

##### **CRAG -- Comprehensive RAG Benchmark**
2406.04744v1 by Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, Xin Luna Dong

Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution to alleviate Large Language Model (LLM)'s deficiency in lack of
knowledge. Existing RAG datasets, however, do not adequately represent the
diverse and dynamic nature of real-world Question Answering (QA) tasks. To
bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual
question answering benchmark of 4,409 question-answer pairs and mock APIs to
simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a
diverse array of questions across five domains and eight question categories,
reflecting varied entity popularity from popular to long-tail, and temporal
dynamisms ranging from years to seconds. Our evaluation on this benchmark
highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve
<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the
accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%
questions without any hallucination. CRAG also reveals much lower accuracy in
answering questions regarding facts with higher dynamism, lower popularity, or
higher complexity, suggesting future research directions. The CRAG benchmark
laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of
participants and submissions within the first 50 days of the competition. We
commit to maintaining CRAG to serve research communities in advancing RAG
solutions and general QA solutions.

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊúÄËøë‰ΩúÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑËß£Ê±∫ÊñπÊ°àÂá∫ÁèæÔºå‰ª•Á∑©Ëß£Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Áü•Ë≠òÁº∫‰πèÊñπÈù¢ÁöÑÁº∫Èô∑„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑ RAG Ë≥áÊñôÈõÜ‰∏¶‰∏çËÉΩÂÖÖÂàÜ‰ª£Ë°®ÁèæÂØ¶‰∏ñÁïåÂïèÁ≠î (QA) ‰ªªÂãôÁöÑÂ§öÊ®£ÊÄßÂíåÂãïÊÖãÊÄß„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄô‰∏ÄÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÁ∂úÂêà RAG Âü∫Ê∫ñ (CRAG)ÔºåÈÄôÊòØ‰∏ÄÂÄãÁî± 4,409 ÂÄãÂïèÁ≠îÂ∞çÂíåÊ®°Êì¨Á∂≤Ë∑ØÂíåÁü•Ë≠òÂúñË≠ú (KG) ÊêúÂ∞ãÁöÑÊ®°Êì¨ API ÁµÑÊàêÁöÑÂü∫Êñº‰∫ãÂØ¶ÁöÑÂïèÁ≠îÂü∫Ê∫ñ„ÄÇCRAG Ë¢´Ë®≠Ë®àÊàêÂõäÊã¨Ë∑®Ë∂ä‰∫îÂÄãÈ†òÂüüÂíåÂÖ´ÂÄãÂïèÈ°åÈ°ûÂà•ÁöÑÂêÑÁ®ÆÂïèÈ°åÔºåÂèçÊò†‰∫ÜÂæûÊµÅË°åÂà∞Èï∑Â∞æÁöÑÂêÑÁ®ÆÂØ¶È´îÊµÅË°åÂ∫¶Ôºå‰ª•ÂèäÂæûÂπ¥Âà∞ÁßíÁöÑÊôÇÈñìÂãïÊÖã„ÄÇÊàëÂÄëÂ∞çÊ≠§Âü∫Ê∫ñÁöÑË©ï‰º∞Á™ÅÂá∫‰∫ÜÂÆåÂÖ®ÂÄºÂæó‰ø°Ë≥¥ÁöÑ QA ÁöÑÂ∑ÆË∑ù„ÄÇÂÑòÁÆ°Â§ßÂ§öÊï∏ÂÖàÈÄ≤ÁöÑ LLM Âú® CRAG ‰∏äÁöÑÊ∫ñÁ¢∫Áéá‰ΩéÊñºÁ≠âÊñº 34%Ôºå‰ΩÜ‰ª•‰∏ÄÁ®ÆÁõ¥Êé•ÁöÑÊñπÂºèÊ∑ªÂä† RAG ÂÉÖÂ∞áÊ∫ñÁ¢∫ÁéáÊèêÈ´òÂà∞ 44%„ÄÇÊúÄÂÖàÈÄ≤ÁöÑÁî¢Ê•≠ RAG Ëß£Ê±∫ÊñπÊ°àÂÉÖÂõûÁ≠î 63% ÁöÑÂïèÈ°åÔºå‰∏îÊ≤íÊúâ‰ªª‰ΩïÂπªË¶∫„ÄÇCRAG ÈÇÑÈ°ØÁ§∫Âú®ÂõûÁ≠îÂÖ∑ÊúâÊõ¥È´òÂãïÊÖãÊÄß„ÄÅËºÉ‰ΩéÊµÅË°åÂ∫¶ÊàñÊõ¥È´òË§áÈõúÊÄßÁöÑ‰∫ãÂØ¶Áõ∏ÈóúÂïèÈ°åÊôÇÊ∫ñÁ¢∫ÁéáË¶Å‰ΩéÂæóÂ§öÔºåÈÄôË°®Êòé‰∫ÜÊú™‰æÜÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇCRAG Âü∫Ê∫ñÁÇ∫ 2024 Âπ¥ KDD ÊùØÊåëÊà∞Ë≥ΩÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºåÂú®ÊØîË≥ΩÈñãÂßãÂæåÁöÑÂâç 50 Â§©ÂÖßÂê∏Âºï‰∫ÜÊï∏ÂçÉÂêçÂèÉËàáËÄÖÂíåÊèê‰∫§„ÄÇÊàëÂÄëÊâøË´æÁ∂≠Ë≠∑ CRAGÔºå‰ª•ÊúçÂãôÊñºÁ†îÁ©∂Á§æÁæ§ÔºåÊé®ÈÄ≤ RAG Ëß£Ê±∫ÊñπÊ°àÂíå‰∏ÄËà¨ QA Ëß£Ê±∫ÊñπÊ°à„ÄÇ

##### **Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**
2406.04612v1 by Yong-Min Shin, Siqing Li, Xin Cao, Won-Yong Shin

The self-attention mechanism has been adopted in several widely-used
message-passing neural networks (MPNNs) (e.g., GATs), which adaptively controls
the amount of information that flows along the edges of the underlying graph.
This usage of attention has made such models a baseline for studies on
explainable AI (XAI) since interpretations via attention have been popularized
in various domains (e.g., natural language processing and computer vision).
However, existing studies often use naive calculations to derive attribution
scores from attention, and do not take the precise and careful calculation of
edge attribution into consideration. In our study, we aim to fill the gap
between the widespread usage of attention-enabled MPNNs and their potential in
largely under-explored explainability, a topic that has been actively
investigated in other areas. To this end, as the first attempt, we formalize
the problem of edge attribution from attention weights in GNNs. Then, we
propose GATT, an edge attribution calculation method built upon the computation
tree. Through comprehensive experiments, we demonstrate the effectiveness of
our proposed method when evaluating attributions from GATs. Conversely, we
empirically validate that simply averaging attention weights over graph
attention layers is insufficient to interpret the GAT model's behavior. Code is
publicly available at https://github.com/jordan7186/GAtt/tree/main.

ÊëòË¶ÅÔºöËá™Ê≥®ÊÑèÂäõÊ©üÂà∂Â∑≤Ë¢´Êé°Áî®ÊñºÂ§öÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑË®äÊÅØÂÇ≥ÈÅûÁ•ûÁ∂ìÁ∂≤Ë∑Ø (MPNN)Ôºà‰æãÂ¶Ç GATÔºâÔºåÂÆÉÂèØ‰ª•Ëá™ÈÅ©ÊáâÂú∞ÊéßÂà∂Ê≤øËëóÂ∫ïÂ±§ÂúñÂΩ¢ÈÇäÁ∑£ÊµÅÂãïÁöÑË≥áË®äÈáè„ÄÇÈÄôÁ®ÆÊ≥®ÊÑèÂäõÁöÑ‰ΩøÁî®‰ΩøÂæóÊ≠§È°ûÊ®°ÂûãÊàêÁÇ∫ÂèØËß£Èáã AI (XAI) Á†îÁ©∂ÁöÑÂü∫Á∑öÔºåÂõ†ÁÇ∫ÈÄèÈÅéÊ≥®ÊÑèÂäõÁöÑË©ÆÈáãÂ∑≤Âú®ÂêÑÁ®ÆÈ†òÂüüÔºà‰æãÂ¶ÇËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÈõªËÖ¶Ë¶ñË¶∫Ôºâ‰∏≠ÊôÆÂèä„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂ÈÄöÂ∏∏‰ΩøÁî®Â§©ÁúüÁöÑË®àÁÆóÊñπÊ≥ïÂæûÊ≥®ÊÑèÂäõ‰∏≠Êé®Â∞éÂá∫Ê≠∏Âõ†ÂàÜÊï∏Ôºå‰∏¶‰∏îÊ≤íÊúâËÄÉÊÖÆÂà∞ÈÇäÁ∑£Ê≠∏Âõ†ÁöÑÁ≤æÁ¢∫‰∏î‰ªîÁ¥∞ÁöÑË®àÁÆó„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊó®Âú®Â°´Ë£úÊ≥®ÊÑèÂäõÂïüÁî® MPNN ÁöÑÂª£Ê≥õ‰ΩøÁî®ËàáÂÆÉÂÄëÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÂèØËß£ÈáãÊÄß‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÈÄôÂÄã‰∏ªÈ°åÂ∑≤Âú®ÂÖ∂‰ªñÈ†òÂüüÁ©çÊ•µÁ†îÁ©∂„ÄÇÁÇ∫Ê≠§Ôºå‰ΩúÁÇ∫Á¨¨‰∏ÄÊ¨°ÂòóË©¶ÔºåÊàëÂÄëÂ∞á GNN ‰∏≠Ê≥®ÊÑèÂäõÊ¨äÈáçÁöÑÈÇäÁ∑£Ê≠∏Âõ†ÂïèÈ°åÂΩ¢ÂºèÂåñ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫ GATTÔºå‰∏ÄÁ®ÆÂª∫Á´ãÂú®Ë®àÁÆóÊ®π‰∏äÁöÑÈÇäÁ∑£Ê≠∏Âõ†Ë®àÁÆóÊñπÊ≥ï„ÄÇÈÄèÈÅéÂÖ®Èù¢ÁöÑÂØ¶È©óÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Ë©ï‰º∞ GAT ÁöÑÊ≠∏Âõ†ÊôÇÊâÄÂÖ∑ÊúâÁöÑÊïàÊûú„ÄÇÁõ∏ÂèçÂú∞ÔºåÊàëÂÄëÊÜëÁ∂ìÈ©óÈ©óË≠â‰∫ÜÂÉÖÂ∞çÂúñÊ≥®ÊÑèÂäõÂ±§‰∏äÁöÑÊ≥®ÊÑèÂäõÊ¨äÈáçÂèñÂπ≥ÂùáÂÄº‰∏çË∂≥‰ª•Ë©ÆÈáã GAT Ê®°ÂûãÁöÑË°åÁÇ∫„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº https://github.com/jordan7186/GAtt/tree/main„ÄÇ

##### **OCDB: Revisiting Causal Discovery with a Comprehensive Benchmark and Evaluation Framework**
2406.04598v1 by Wei Zhou, Hong Huang, Guowen Zhang, Ruize Shi, Kehan Yin, Yuanyuan Lin, Bang Liu

Large language models (LLMs) have excelled in various natural language
processing tasks, but challenges in interpretability and trustworthiness
persist, limiting their use in high-stakes fields. Causal discovery offers a
promising approach to improve transparency and reliability. However, current
evaluations are often one-sided and lack assessments focused on
interpretability performance. Additionally, these evaluations rely on synthetic
data and lack comprehensive assessments of real-world datasets. These lead to
promising methods potentially being overlooked. To address these issues, we
propose a flexible evaluation framework with metrics for evaluating differences
in causal structures and causal effects, which are crucial attributes that help
improve the interpretability of LLMs. We introduce the Open Causal Discovery
Benchmark (OCDB), based on real data, to promote fair comparisons and drive
optimization of algorithms. Additionally, our new metrics account for
undirected edges, enabling fair comparisons between Directed Acyclic Graphs
(DAGs) and Completed Partially Directed Acyclic Graphs (CPDAGs). Experimental
results show significant shortcomings in existing algorithms' generalization
capabilities on real data, highlighting the potential for performance
improvement and the importance of our framework in advancing causal discovery
techniques.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Â∫¶ÊñπÈù¢ÁöÑÊåëÊà∞‰ªçÁÑ∂Â≠òÂú®ÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®È´òÈ¢®Èö™È†òÂüü‰∏≠ÁöÑ‰ΩøÁî®„ÄÇÂõ†ÊûúÁôºÁèæÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÊôØÁöÑÊñπÊ≥ï‰æÜÊèêÈ´òÈÄèÊòéÂ∫¶ÂíåÂèØÈù†ÊÄß„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑË©ï‰º∞ÈÄöÂ∏∏ÊòØ‰∏ÄÊñπÈù¢ÁöÑÔºåÁº∫‰πèÈáùÂ∞çÂèØËß£ÈáãÊÄßË°®ÁèæÁöÑË©ï‰º∞„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õË©ï‰º∞‰æùË≥¥ÊñºÂêàÊàêÊï∏ÊìöÔºå‰∏¶‰∏îÁº∫‰πèÂ∞çÁèæÂØ¶‰∏ñÁïåÊï∏ÊìöÈõÜÁöÑÂÖ®Èù¢Ë©ï‰º∞„ÄÇÈÄôÂ∞éËá¥ÊúâÂâçÊôØÁöÑÊñπÊ≥ïÂèØËÉΩË¢´ÂøΩË¶ñ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈùàÊ¥ªÁöÑË©ï‰º∞Ê°ÜÊû∂ÔºåÂÖ∂‰∏≠ÂåÖÂê´Áî®ÊñºË©ï‰º∞Âõ†ÊûúÁµêÊßãÂíåÂõ†ÊûúÊïàÊáâÂ∑ÆÁï∞ÁöÑÊåáÊ®ôÔºåÈÄô‰∫õÈÉΩÊòØÊúâÂä©ÊñºÊèêÈ´ò LLM ÂèØËß£ÈáãÊÄßÁöÑÈóúÈçµÂ±¨ÊÄß„ÄÇÊàëÂÄëÂü∫ÊñºÁúüÂØ¶Êï∏ÊìöÂºïÂÖ•‰∫ÜÈñãÊîæÂõ†ÊûúÁôºÁèæÂü∫Ê∫ñ (OCDB)Ôºå‰ª•‰øÉÈÄ≤ÂÖ¨Âπ≥ÁöÑÊØîËºÉ‰∏¶Êé®ÂãïÁÆóÊ≥ïÁöÑÂÑ™Âåñ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊñ∞ÊåáÊ®ôËÄÉÊÖÆ‰∫ÜÁÑ°ÂêëÈÇäÔºåÂæûËÄåÂèØ‰ª•Âú®ÊúâÂêëÁÑ°Áí∞Âúñ (DAG) ÂíåÂ∑≤ÂÆåÊàêÁöÑÈÉ®ÂàÜÊúâÂêëÁÑ°Áí∞Âúñ (CPDAG) ‰πãÈñìÈÄ≤Ë°åÂÖ¨Âπ≥ÁöÑÊØîËºÉ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÁèæÊúâÁÆóÊ≥ïÂú®ÁúüÂØ¶Êï∏Êìö‰∏äÁöÑÊ≥õÂåñËÉΩÂäõÂ≠òÂú®È°ØËëóÁº∫Èô∑ÔºåÁ™ÅÂá∫‰∫ÜÊÄßËÉΩÊîπÈÄ≤ÁöÑÊΩõÂäõ‰ª•ÂèäÊàëÂÄëÊ°ÜÊû∂Âú®Êé®ÈÄ≤Âõ†ÊûúÁôºÁèæÊäÄË°ì‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ

##### **ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions**
2406.04286v1 by Sreyan Ghosh, Utkarsh Tyagi, Sonal Kumar, C. K. Evuru, S Ramaneswaran, S Sakshi, Dinesh Manocha

We present ABEX, a novel and effective generative data augmentation
methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX
is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms
of an input document -- we first convert a document into its concise, abstract
description and then generate new documents based on expanding the resultant
abstraction. To learn the task of expanding abstract descriptions, we first
train BART on a large-scale synthetic dataset with abstract-document pairs.
Next, to generate abstract descriptions for a document, we propose a simple,
controllable, and training-free method based on editing AMR graphs. ABEX brings
the best of both worlds: by expanding from abstract representations, it
preserves the original semantic properties of the documents, like style and
meaning, thereby maintaining alignment with the original label and data
distribution. At the same time, the fundamental process of elaborating on
abstract descriptions facilitates diverse generations. We demonstrate the
effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource
settings. ABEX outperforms all our baselines qualitatively with improvements of
0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods from
literature in terms of context and length diversity.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫ ABEXÔºå‰∏ÄÁ®ÆÊñ∞Á©é‰∏îÊúâÊïàÁöÑÁîüÊàêÂºèË≥áÊñôÊì¥Â¢ûÊñπÊ≥ïÔºåÈÅ©Áî®Êñº‰ΩéË≥áÊ∫êËá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ (NLU) ‰ªªÂãô„ÄÇABEX Âª∫Á´ãÂú® ABstract-and-EXpand ‰πã‰∏äÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®ÊñºÁî¢ÁîüËº∏ÂÖ•Êñá‰ª∂‰∏çÂêåÂΩ¢ÂºèÁöÑÊñ∞Á©éÁØÑ‰æã - ÊàëÂÄëÈ¶ñÂÖàÂ∞áÊñá‰ª∂ËΩâÊèõÊàêÁ∞°ÊΩîÁöÑÊäΩË±°ÊèèËø∞ÔºåÁÑ∂ÂæåÊ†πÊìöÊì¥Â±ïÊâÄÂæóÊäΩË±°Áî¢ÁîüÊñ∞Êñá‰ª∂„ÄÇÁÇ∫‰∫ÜÂ≠∏ÁøíÊì¥Â±ïÊäΩË±°ÊèèËø∞ÁöÑ‰ªªÂãôÔºåÊàëÂÄëÈ¶ñÂÖàÂú®ÂÖ∑ÊúâÊäΩË±°Êñá‰ª∂ÈÖçÂ∞çÁöÑÂ§ßË¶èÊ®°ÂêàÊàêË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ BART„ÄÇÊé•‰∏ã‰æÜÔºåÁÇ∫‰∫ÜÁî¢ÁîüÊñá‰ª∂ÁöÑÊäΩË±°ÊèèËø∞ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ∞°ÂñÆ„ÄÅÂèØÊéß‰∏îÁÑ°ÈúÄË®ìÁ∑¥ÁöÑÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂü∫ÊñºÁ∑®ËºØ AMR ÂúñË°®„ÄÇABEX Â∏∂‰æÜ‰∫ÜÂÖ©ÂÖ®ÂÖ∂ÁæéÁöÑÂÑ™ÈªûÔºöÈÄöÈÅéÂæûÊäΩË±°Ë°®Á§∫‰∏≠Êì¥Â±ïÔºåÂÆÉ‰øùÁïô‰∫ÜÊñá‰ª∂ÁöÑÂéüÂßãË™ûÁæ©Â±¨ÊÄßÔºå‰æãÂ¶ÇÈ¢®Ê†ºÂíåÊÑèÁæ©ÔºåÂæûËÄå‰øùÊåÅËàáÂéüÂßãÊ®ôÁ±§ÂíåË≥áÊñôÂàÜ‰ΩàÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂêåÊôÇÔºåÈó°Ëø∞ÊäΩË±°ÊèèËø∞ÁöÑÂü∫Êú¨ÈÅéÁ®ã‰øÉÈÄ≤‰∫ÜÂ§öÊ®£ÂåñÁöÑÁîüÊàê„ÄÇÊàëÂÄëÂú®Ê©´Ë∑® 12 ÂÄãË≥áÊñôÈõÜÂíå 4 ÂÄã‰ΩéË≥áÊ∫êË®≠ÂÆöÁöÑ 4 ÂÄã NLU ‰ªªÂãô‰∏äÂ±ïÁ§∫‰∫Ü ABEX ÁöÑÊúâÊïàÊÄß„ÄÇABEX Âú®Ë≥™Èáè‰∏äÂÑ™ÊñºÊàëÂÄëÊâÄÊúâÁöÑÂü∫Ê∫ñÔºåÊîπÈÄ≤‰∫Ü 0.04% - 38.8%„ÄÇÂú®Ë≥™Èáè‰∏äÔºåABEX Âú®ËÉåÊôØÂíåÈï∑Â∫¶Â§öÊ®£ÊÄßÊñπÈù¢ÂÑ™ÊñºÊñáÁçª‰∏≠ÁöÑÊâÄÊúâÂÖàÂâçÊñπÊ≥ï„ÄÇ

##### **Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models**
2406.04271v1 by Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E. Gonzalez, Bin Cui

We introduce Buffer of Thoughts (BoT), a novel and versatile
thought-augmented reasoning approach for enhancing accuracy, efficiency and
robustness of large language models (LLMs). Specifically, we propose
meta-buffer to store a series of informative high-level thoughts, namely
thought-template, distilled from the problem-solving processes across various
tasks. Then for each problem, we retrieve a relevant thought-template and
adaptively instantiate it with specific reasoning structures to conduct
efficient reasoning. To guarantee the scalability and stability, we further
propose buffer-manager to dynamically update the meta-buffer, thus enhancing
the capacity of meta-buffer as more tasks are solved. We conduct extensive
experiments on 10 challenging reasoning-intensive tasks, and achieve
significant performance improvements over previous SOTA methods: 11% on Game of
24, 20% on Geometric Shapes and 51% on Checkmate-in-One. Further analysis
demonstrate the superior generalization ability and model robustness of our
BoT, while requiring only 12% of the cost of multi-query prompting methods
(e.g., tree/graph of thoughts) on average. Notably, we find that our
Llama3-8B+BoT has the potential to surpass Llama3-70B model. Our project is
available at: https://github.com/YangLing0818/buffer-of-thought-llm

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄë‰ªãÁ¥π‰∫ÜÊÄùÊÉ≥Á∑©Ë°ùÂçÄ (BoT)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©é‰∏îÈÄöÁî®ÁöÑÊÄùÊÉ≥Â¢ûÂº∑Êé®ÁêÜÊñπÊ≥ïÔºåÁî®ÊñºÊèêÂçáÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÅÊïàÁéáÂíåÁ©©ÂÅ•ÊÄß„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫ÂÖÉÁ∑©Ë°ùÂçÄÔºåÁî®ÊñºÂÑ≤Â≠ò‰∏ÄÁ≥ªÂàóÊúâÊÑèÁæ©ÁöÑÈ´òÈöéÊÄùÊÉ≥ÔºåÂç≥ÊÄùÊÉ≥ÁØÑÊú¨ÔºåÂæûÂêÑÁ®Æ‰ªªÂãôÁöÑËß£Ê±∫ÂïèÈ°åÈÅéÁ®ã‰∏≠ÊèêÁÖâÂá∫‰æÜ„ÄÇÁÑ∂ÂæåÔºåÂ∞çÊñºÊØèÂÄãÂïèÈ°åÔºåÊàëÂÄëÊúÉÊì∑Âèñ‰∏ÄÂÄãÁõ∏ÈóúÁöÑÊÄùÊÉ≥ÁØÑÊú¨Ôºå‰∏¶‰ª•ÁâπÂÆöÁöÑÊé®ÁêÜÁµêÊßãËá™ÈÅ©ÊáâÂú∞ÂØ¶‰æãÂåñÂÆÉÔºå‰ª•ÈÄ≤Ë°åÊúâÊïàÁéáÁöÑÊé®ÁêÜ„ÄÇÁÇ∫‰∫Ü‰øùË≠âÂèØÊì¥ÂÖÖÊÄßÂíåÁ©©ÂÆöÊÄßÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÊèêÂá∫‰∫ÜÁ∑©Ë°ùÂçÄÁÆ°ÁêÜÂì°ÔºåÁî®ÊñºÂãïÊÖãÊõ¥Êñ∞ÂÖÉÁ∑©Ë°ùÂçÄÔºåÂæûËÄåÈö®ËëóÊõ¥Â§ö‰ªªÂãôÁöÑËß£Ê±∫ËÄåÊèêÂçáÂÖÉÁ∑©Ë°ùÂçÄÁöÑÂÆπÈáè„ÄÇÊàëÂÄëÂú® 10 È†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÊé®ÁêÜÂØÜÈõÜÂûã‰ªªÂãô‰∏äÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰∏¶‰∏îÁõ∏ËºÉÊñºÂÖàÂâçÁöÑ SOTA ÊñπÊ≥ïÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçáÔºö24 ÁöÑÈÅäÊà≤ÊèêÂçá‰∫Ü 11%ÔºåÂπæ‰ΩïÂΩ¢ÁãÄÊèêÂçá‰∫Ü 20%Ôºå‰∏ÄÊãõÂ∞áÊ≠ªÊèêÂçá‰∫Ü 51%„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑÂàÜÊûêË≠âÊòé‰∫ÜÊàëÂÄë BoT ÁöÑÂÑ™Áï∞Ê≥õÂåñËÉΩÂäõÂíåÊ®°ÂûãÁ©©ÂÅ•ÊÄßÔºåÂêåÊôÇÂπ≥ÂùáÂè™ÈúÄË¶ÅÂ§öÊü•Ë©¢ÊèêÁ§∫ÊñπÊ≥ïÔºà‰æãÂ¶ÇÊÄùÊÉ≥Ê®π/ÂúñÔºâÊàêÊú¨ÁöÑ 12%„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÁôºÁèæÊàëÂÄëÁöÑ Llama3-8B+BoT ÊúâÂèØËÉΩË∂ÖË∂ä Llama3-70B Ê®°Âûã„ÄÇÊàëÂÄëÁöÑÂ∞àÊ°àÂèØÊñº‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÔºöhttps://github.com/YangLing0818/buffer-of-thought-llm</paragraph>

##### **Transformers need glasses! Information over-squashing in language tasks**
2406.04267v1 by Federico Barbero, Andrea Banino, Steven Kapturowski, Dharshan Kumaran, Jo√£o G. M. Ara√∫jo, Alex Vitvitskyi, Razvan Pascanu, Petar Veliƒçkoviƒá

We study how information propagates in decoder-only Transformers, which are
the architectural backbone of most existing frontier large language models
(LLMs). We rely on a theoretical signal propagation analysis -- specifically,
we analyse the representations of the last token in the final layer of the
Transformer, as this is the representation used for next-token prediction. Our
analysis reveals a representational collapse phenomenon: we prove that certain
distinct sequences of inputs to the Transformer can yield arbitrarily close
representations in the final token. This effect is exacerbated by the
low-precision floating-point formats frequently used in modern LLMs. As a
result, the model is provably unable to respond to these sequences in different
ways -- leading to errors in, e.g., tasks involving counting or copying.
Further, we show that decoder-only Transformer language models can lose
sensitivity to specific tokens in the input, which relates to the well-known
phenomenon of over-squashing in graph neural networks. We provide empirical
evidence supporting our claims on contemporary LLMs. Our theory also points to
simple solutions towards ameliorating these issues.

ÊëòË¶ÅÔºöÊàëÂÄëÁ†îÁ©∂Ë≥áË®äÂú®ÂÉÖËß£Á¢ºÂô® Transformer ‰∏≠Â¶Ç‰ΩïÂÇ≥Êí≠ÔºåËÄåÂÉÖËß£Á¢ºÂô® Transformer ÊòØÁèæÊúâÊúÄÂâçÊ≤øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊû∂Êßã‰∏ªÂππ„ÄÇÊàëÂÄë‰æùË≥¥ÊñºÁêÜË´ñË®äËôüÂÇ≥Êí≠ÂàÜÊûêÔºåÁâπÂà•ÊòØÔºåÊàëÂÄëÂàÜÊûê Transformer ÊúÄÂæå‰∏ÄÂ±§‰∏≠ÊúÄÂæå‰∏ÄÂÄã‰ª£Âπ£ÁöÑË°®ÂæµÔºåÂõ†ÁÇ∫ÈÄôÊòØÁî®Êñº‰∏ã‰∏ÄÂÄã‰ª£Âπ£È†êÊ∏¨ÁöÑË°®Âæµ„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÊè≠Á§∫‰∫Ü‰∏ÄÂÄãË°®ÂæµÂ¥©ÊΩ∞ÁèæË±°ÔºöÊàëÂÄëË≠âÊòéËº∏ÂÖ•Âà∞ Transformer ÁöÑÊüê‰∫õ‰∏çÂêåÈ†ÜÂ∫èÂ∫èÂàóÂèØ‰ª•Âú®ÊúÄÂæå‰∏ÄÂÄã‰ª£Âπ£‰∏≠Áî¢Áîü‰ªªÊÑèÊé•ËøëÁöÑË°®Âæµ„ÄÇÈÄôÁ®ÆÊïàÊáâÊúÉÂõ†Áèæ‰ª£ LLM ‰∏≠Á∂ìÂ∏∏‰ΩøÁî®ÁöÑ‰ΩéÁ≤æÂ∫¶ÊµÆÈªûÊ†ºÂºèËÄåÂä†Âäá„ÄÇÁµêÊûúÔºåË©≤Ê®°ÂûãÁÑ°Ê≥ïÂ∞çÈÄô‰∫õÂ∫èÂàó‰ª•‰∏çÂêåÁöÑÊñπÂºèÂÅöÂá∫ÂõûÊáâÔºåÂ∞éËá¥ÈåØË™§Ôºå‰æãÂ¶ÇÊ∂âÂèäË®àÊï∏ÊàñË§áË£ΩÁöÑ‰ªªÂãô„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË°®ÊòéÂÉÖËß£Á¢ºÂô® Transformer Ë™ûË®ÄÊ®°ÂûãÂèØËÉΩÊúÉÂ∞çËº∏ÂÖ•‰∏≠ÁöÑÁâπÂÆö‰ª£Âπ£Â§±ÂéªÊïèÊÑüÊÄßÔºåÈÄôËàáÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁúæÊâÄÂë®Áü•ÁöÑÈÅéÂ∫¶Â£ìÁ∏ÆÁèæË±°ÊúâÈóú„ÄÇÊàëÂÄëÊèê‰æõÁ∂ìÈ©óË≠âÊìöÊîØÊåÅÊàëÂÄëÂ∞çÁï∂‰ª£ LLM ÁöÑË™™Ê≥ï„ÄÇÊàëÂÄëÁöÑÁêÜË´ñ‰πüÊåáÂá∫‰∫ÜÊîπÂñÑÈÄô‰∫õÂïèÈ°åÁöÑÁ∞°ÂñÆËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **The CLRS-Text Algorithmic Reasoning Language Benchmark**
2406.04229v1 by Larisa Markeeva, Sean McLeish, Borja Ibarz, Wilfried Bounsi, Olga Kozlova, Alex Vitvitskyi, Charles Blundell, Tom Goldstein, Avi Schwarzschild, Petar Veliƒçkoviƒá

Eliciting reasoning capabilities from language models (LMs) is a critical
direction on the path towards building intelligent systems. Most recent studies
dedicated to reasoning focus on out-of-distribution performance on
procedurally-generated synthetic benchmarks, bespoke-built to evaluate specific
skills only. This trend makes results hard to transfer across publications,
slowing down progress. Three years ago, a similar issue was identified and
rectified in the field of neural algorithmic reasoning, with the advent of the
CLRS benchmark. CLRS is a dataset generator comprising graph execution traces
of classical algorithms from the Introduction to Algorithms textbook. Inspired
by this, we propose CLRS-Text -- a textual version of these algorithmic traces.
Out of the box, CLRS-Text is capable of procedurally generating trace data for
thirty diverse, challenging algorithmic tasks across any desirable input
distribution, while offering a standard pipeline in which any additional
algorithmic tasks may be created in the benchmark. We fine-tune and evaluate
various LMs as generalist executors on this benchmark, validating prior work
and revealing a novel, interesting challenge for the LM reasoning community.
Our code is available at
https://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text.

ÊëòË¶ÅÔºöÂºïÂá∫ËØ≠Ë®ÄÊ®°Âûã (LM) ÁöÑÊé®ÁêÜËÉΩÂäõÊòØÂª∫ÊûÑÊô∫ËÉΩÁ≥ªÁªüË∑ØÂæÑ‰∏äÁöÑÂÖ≥ÈîÆÊñπÂêë„ÄÇÂ§ßÂ§öÊï∞ÊúÄËøë‰∏ìÊ≥®‰∫éÊé®ÁêÜÁöÑÁ†îÁ©∂ÈÉΩÂÖ≥Ê≥®Á®ãÂ∫èÁîüÊàêÂêàÊàêÂü∫ÂáÜÁöÑÂàÜÂ∏ÉÂ§ñÊÄßËÉΩÔºå‰ªÖ‰∏∫ËØÑ‰º∞ÁâπÂÆöÊäÄËÉΩËÄåÂÆöÂà∂ÊûÑÂª∫„ÄÇËøôÁßçË∂ãÂäø‰ΩøÂæóÁªìÊûúÈöæ‰ª•Âú®Âá∫ÁâàÁâ©‰πãÈó¥ËΩ¨ÁßªÔºå‰ªéËÄåÂáèÁºì‰∫ÜËøõÂ∫¶„ÄÇ‰∏âÂπ¥ÂâçÔºåÂú®Á•ûÁªèÁÆóÊ≥ïÊé®ÁêÜÈ¢ÜÂüüÂèëÁé∞‰∫ÜÁ±ª‰ººÁöÑÈóÆÈ¢òÂπ∂Âä†‰ª•Á∫†Ê≠£ÔºåÈöèÁùÄ CLRS Âü∫ÂáÜÁöÑÂá∫Áé∞„ÄÇCLRS ÊòØ‰∏Ä‰∏™Êï∞ÊçÆÈõÜÁîüÊàêÂô®ÔºåÂåÖÂê´Êù•Ëá™ÁÆóÊ≥ïÂØºËÆ∫ÊïôÁßë‰π¶ÁöÑÁªèÂÖ∏ÁÆóÊ≥ïÁöÑÂõæÂΩ¢ÊâßË°åËΩ®Ëøπ„ÄÇÂèóÊ≠§ÂêØÂèëÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü CLRS-Text‚Äî‚ÄîËøô‰∫õÁÆóÊ≥ïËΩ®ËøπÁöÑÊñáÊú¨ÁâàÊú¨„ÄÇÂºÄÁÆ±Âç≥Áî®ÔºåCLRS-Text ËÉΩÂ§ü‰∏∫‰∏âÂçÅ‰∏™‰∏çÂêåÁöÑ„ÄÅÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÁÆóÊ≥ï‰ªªÂä°Á®ãÂ∫èÁîüÊàêËΩ®ËøπÊï∞ÊçÆÔºåË∑®Ë∂ä‰ªª‰ΩïÁêÜÊÉ≥ÁöÑËæìÂÖ•ÂàÜÂ∏ÉÔºåÂêåÊó∂Êèê‰æõ‰∏Ä‰∏™Ê†áÂáÜÁÆ°ÈÅìÔºåÂèØ‰ª•Âú®Âü∫ÂáÜ‰∏≠ÂàõÂª∫‰ªª‰ΩïÂÖ∂‰ªñÁÆóÊ≥ï‰ªªÂä°„ÄÇÊàë‰ª¨ÂØπÂêÑÁßç LM ËøõË°åÂæÆË∞ÉÂíåËØÑ‰º∞Ôºå‰Ωú‰∏∫Ê≠§Âü∫ÂáÜ‰∏äÁöÑÈÄöÊâçÊâßË°åÂô®ÔºåÈ™åËØÅ‰∫ÜÂÖàÂâçÁöÑÂ∑•‰ΩúÂπ∂Êè≠Á§∫‰∫Ü LM Êé®ÁêÜÁ§æÂå∫‰∏Ä‰∏™Êñ∞È¢ñ„ÄÅÊúâË∂£ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂèØÂú® https://github.com/google-deepmind/clrs/tree/master/clrs/_src/clrs_text Ëé∑Âæó„ÄÇ

##### **Tox-BART: Leveraging Toxicity Attributes for Explanation Generation of Implicit Hate Speech**
2406.03953v1 by Neemesh Yadav, Sarah Masud, Vikram Goyal, Vikram Goyal, Md Shad Akhtar, Tanmoy Chakraborty

Employing language models to generate explanations for an incoming implicit
hate post is an active area of research. The explanation is intended to make
explicit the underlying stereotype and aid content moderators. The training
often combines top-k relevant knowledge graph (KG) tuples to provide world
knowledge and improve performance on standard metrics. Interestingly, our study
presents conflicting evidence for the role of the quality of KG tuples in
generating implicit explanations. Consequently, simpler models incorporating
external toxicity signals outperform KG-infused models. Compared to the
KG-based setup, we observe a comparable performance for SBIC (LatentHatred)
datasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and
-4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore. Further human evaluation and
error analysis reveal that our proposed setup produces more precise
explanations than zero-shot GPT-3.5, highlighting the intricate nature of the
task.

ÊëòË¶ÅÔºöÂà©Áî®Ë™ûË®ÄÊ®°ÂûãÁÇ∫‰∏ÄÂÄãÊöóÁ§∫ÊÄßÁöÑ‰ªáÊÅ®ÊñáÁ´†Áî¢ÁîüËß£ÈáãÊòØ‰∏ÄÂÄãÊ¥ªË∫çÁöÑÁ†îÁ©∂È†òÂüü„ÄÇÈÄôÂÄãËß£ÈáãÁöÑÁõÆÁöÑÊòØË¶ÅÊòéÁ¢∫ÊΩõÂú®ÁöÑÂàªÊùøÂç∞Ë±°‰∏¶ÂçîÂä©ÂÖßÂÆπÁÆ°ÁêÜÂì°„ÄÇË®ìÁ∑¥ÈÄöÂ∏∏ÁµêÂêàÂâç k ÂÄãÁõ∏ÈóúÁü•Ë≠òÂúñË≠ú (KG) ÂÖÉÁµÑ‰ª•Êèê‰æõ‰∏ñÁïåÁü•Ë≠ò‰∏¶ÊîπÂñÑÊ®ôÊ∫ñÊåáÊ®ôÁöÑÊïàËÉΩ„ÄÇÊúâË∂£ÁöÑÊòØÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÊèêÂá∫ÁüõÁõæÁöÑË≠âÊìöÔºåË™™Êòé KG ÂÖÉÁµÑÁöÑÂìÅË≥™Âú®Áî¢ÁîüÊöóÁ§∫ÊÄßËß£Èáã‰∏≠ÊâÄÊâÆÊºîÁöÑËßíËâ≤„ÄÇÂõ†Ê≠§ÔºåÁµêÂêàÂ§ñÈÉ®ÊØíÊÄßË®äËôüÁöÑËºÉÁ∞°ÂñÆÊ®°ÂûãÂÑ™ÊñºËûçÂÖ• KG ÁöÑÊ®°Âûã„ÄÇËàáÂü∫Êñº KG ÁöÑË®≠ÂÆöÁõ∏ÊØîÔºåÊàëÂÄëËßÄÂØüÂà∞ SBIC (LatentHatred) Ë≥áÊñôÈõÜÊúâÁõ∏ËøëÁöÑÊïàËÉΩÔºåÂú® BLEU„ÄÅROUGE-L Âíå BERTScore ‰∏≠ÊïàËÉΩËÆäÂåñÁÇ∫ +0.44 (+0.49)„ÄÅ+1.83 (-1.56) Âíå -4.59 (+0.77)„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑ‰∫∫È°ûË©ï‰º∞ÂíåÈåØË™§ÂàÜÊûêÈ°ØÁ§∫ÔºåÊàëÂÄëÊèêÂá∫ÁöÑË®≠ÂÆöÁî¢ÁîüÊØîÈõ∂Ê¨°Â≠∏Áøí GPT-3.5 Êõ¥Á≤æÁ¢∫ÁöÑËß£ÈáãÔºåÁ™ÅÈ°ØÂá∫Ê≠§‰ªªÂãôÁöÑË§áÈõúÊú¨Ë≥™„ÄÇ

##### **Performance of large language models in numerical vs. semantic medical knowledge: Benchmarking on evidence-based Q&As**
2406.03855v1 by Eden Avnat, Michal Levy, Daniel Herstain, Elia Yanko, Daniel Ben Joya, Michal Tzuchman Katz, Dafna Eshel, Sahar Laros, Yael Dagan, Shahar Barami, Joseph Mermelstein, Shahar Ovadia, Noam Shomron, Varda Shalev, Raja-Elie E. Abdulnour

Clinical problem-solving requires processing of semantic medical knowledge
such as illness scripts and numerical medical knowledge of diagnostic tests for
evidence-based decision-making. As large language models (LLMs) show promising
results in many aspects of language-based clinical practice, their ability to
generate non-language evidence-based answers to clinical questions is
inherently limited by tokenization. Therefore, we evaluated LLMs' performance
on two question types: numeric (correlating findings) and semantic
(differentiating entities) while examining differences within and between LLMs
in medical aspects and comparing their performance to humans. To generate
straightforward multi-choice questions and answers (QAs) based on
evidence-based medicine (EBM), we used a comprehensive medical knowledge graph
(encompassed data from more than 50,00 peer-reviewed articles) and created the
"EBMQA". EBMQA contains 105,000 QAs labeled with medical and non-medical topics
and classified into numerical or semantic questions. We benchmarked this
dataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and
Claude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question
types and according to sub-labeled topics. For validation, six medical experts
were tested on 100 numerical EBMQA questions. We found that both LLMs excelled
more in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical
QAs. However, both LLMs showed inter and intra gaps in different medical
aspects and remained inferior to humans. Thus, their medical advice should be
addressed carefully.

ÊëòË¶ÅÔºö<paragraph>Ëá®Â∫äÂïèÈ°åËß£Ê±∫ÈúÄË¶ÅËôïÁêÜË™ûÁæ©ÈÜ´Â≠∏Áü•Ë≠òÔºå‰æãÂ¶ÇÁñæÁóÖËÖ≥Êú¨ÂíåÁî®ÊñºÂæ™Ë≠âÊ±∫Á≠ñÁöÑË®∫Êñ∑Ê∏¨Ë©¶ÁöÑÊï∏ÂÄºÈÜ´Â≠∏Áü•Ë≠ò„ÄÇÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Ë™ûË®ÄÂü∫Á§éËá®Â∫äÂØ¶ÂãôÁöÑË®±Â§öÊñπÈù¢È°ØÁ§∫Âá∫‰ª§‰∫∫ÊªøÊÑèÁöÑÁµêÊûúÔºåÂÆÉÂÄëÁî¢ÁîüÈùûË™ûË®ÄÂæ™Ë≠âÁ≠îÊ°àÁöÑËÉΩÂäõÂú®ÊñºËá®Â∫äÂïèÈ°åÊú¨Ë≥™‰∏äÂèóÂà∞Ê®ôË®òÂåñÁöÑÈôêÂà∂„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëË©ï‰º∞‰∫Ü LLM Âú®ÂÖ©Á®ÆÂïèÈ°åÈ°ûÂûã‰∏äÁöÑË°®ÁèæÔºöÊï∏ÂÄºÔºàÁõ∏ÈóúÁôºÁèæÔºâÂíåË™ûÁæ©ÔºàÂçÄÂàÜÂØ¶È´îÔºâÔºåÂêåÊôÇÊ™¢Êü• LLM Âú®ÈÜ´Â≠∏ÊñπÈù¢ÁöÑÂ∑ÆÁï∞Ôºå‰∏¶Â∞áÂÖ∂Ë°®ÁèæËàá‰∫∫È°ûÈÄ≤Ë°åÊØîËºÉ„ÄÇÁÇ∫‰∫ÜÊ†πÊìöÂæ™Ë≠âÈÜ´Â≠∏ (EBM) Áî¢ÁîüÁõ¥Êé•ÁöÑÂ§öÈÅ∏È°åÂíåÁ≠îÊ°à (QA)ÔºåÊàëÂÄë‰ΩøÁî®‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÈÜ´Â≠∏Áü•Ë≠òÂúñË≠úÔºàÂåÖÂê´‰æÜËá™ 50,000 Â§öÁØáÂêåË°åË©ïÂØ©ÊñáÁ´†ÁöÑË≥áÊñôÔºâÔºå‰∏¶Âª∫Á´ã‰∫Ü„ÄåEBMQA„Äç„ÄÇEBMQA ÂåÖÂê´ 105,000 ÂÄãÊ®ôË®òÊúâÈÜ´Â≠∏ÂíåÈùûÈÜ´Â≠∏‰∏ªÈ°åÁöÑ QAÔºå‰∏¶ÂàÜÈ°ûÁÇ∫Êï∏ÂÄºÊàñË™ûÁæ©ÂïèÈ°å„ÄÇÊàëÂÄë‰ΩøÁî®Ë∂ÖÈÅé 24,500 ÂÄã QA Âú®ÂÖ©ÂÄãÊúÄÂÖàÈÄ≤ÁöÑ LLMÔºöChat-GPT4 Âíå Claude3-Opus ‰∏äÂ∞çÊ≠§Ë≥áÊñôÈõÜÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü LLM Âú®Ë™ûÁæ©ÂíåÊï∏ÂÄºÂïèÈ°åÈ°ûÂûã‰ª•ÂèäÊ†πÊìöÊ¨°Ê®ôÁ±§‰∏ªÈ°åÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÔºåÂÖ≠‰ΩçÈÜ´Â≠∏Â∞àÂÆ∂Êé•Âèó‰∫Ü 100 ÂÄãÊï∏ÂÄº EBMQA ÂïèÈ°åÁöÑÊ∏¨Ë©¶„ÄÇÊàëÂÄëÁôºÁèæÈÄôÂÖ©ÂÄã LLM Âú®Ë™ûÁæ© QA ‰∏äÈÉΩÊØîÂú®Êï∏ÂÄº QA ‰∏äË°®ÁèæÂæóÊõ¥Âá∫Ëâ≤ÔºåËÄå Claude3 Âú®Êï∏ÂÄº QA ‰∏äË∂ÖË∂ä‰∫Ü GPT4„ÄÇÁÑ∂ËÄåÔºåÈÄôÂÖ©ÂÄã LLM Âú®‰∏çÂêåÁöÑÈÜ´Â≠∏ÊñπÈù¢ÈÉΩË°®ÁèæÂá∫ÂÖßÈÉ®ÂíåÂ§ñÈÉ®ÁöÑÂ∑ÆË∑ùÔºå‰∏¶‰∏î‰ªçÁÑ∂ÈÅúÊñº‰∫∫È°û„ÄÇÂõ†Ê≠§ÔºåÂÆÉÂÄëÁöÑÈÜ´ÁôÇÂª∫Ë≠∞ÊáâË¨πÊÖéÂ∞çÂæÖ„ÄÇ</paragraph>

##### **Are Large Language Models the New Interface for Data Pipelines?**
2406.06596v1 by Sylvio Barbon Junior, Paolo Ceravolo, Sven Groppe, Mustafa Jarrar, Samira Maghool, Florence S√®des, Soror Sahri, Maurice Van Keulen

A Language Model is a term that encompasses various types of models designed
to understand and generate human communication. Large Language Models (LLMs)
have gained significant attention due to their ability to process text with
human-like fluency and coherence, making them valuable for a wide range of
data-related tasks fashioned as pipelines. The capabilities of LLMs in natural
language understanding and generation, combined with their scalability,
versatility, and state-of-the-art performance, enable innovative applications
across various AI-related fields, including eXplainable Artificial Intelligence
(XAI), Automated Machine Learning (AutoML), and Knowledge Graphs (KG).
Furthermore, we believe these models can extract valuable insights and make
data-driven decisions at scale, a practice commonly referred to as Big Data
Analytics (BDA). In this position paper, we provide some discussions in the
direction of unlocking synergies among these technologies, which can lead to
more powerful and intelligent AI solutions, driving improvements in data
pipelines across a wide range of applications and domains integrating humans,
computers, and knowledge.

ÊëòË¶ÅÔºöË™ûË®ÄÊ®°ÂûãÊòØ‰∏ÄÂÄãË°ìË™ûÔºåÊ∂µËìãÂêÑÁ®ÆÊó®Âú®ÁêÜËß£ÂíåÁî¢Áîü‰∫∫È°ûÊ∫ùÈÄöÁöÑÊ®°Âûã„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âõ†ÂÖ∂‰ª•È°û‰ºº‰∫∫È°ûÁöÑÊµÅÂà©Â∫¶ÂíåÈÄ£Ë≤´ÊÄßËôïÁêÜÊñáÂ≠óÁöÑËÉΩÂäõËÄåÂÇôÂèóÈóúÊ≥®ÔºåÈÄô‰ΩøÂæóÂÆÉÂÄëÂ∞çÊñºÂª£Ê≥õÁöÑË≥áÊñôÁõ∏Èóú‰ªªÂãôÔºà‰ª•ÁÆ°ÈÅìÂΩ¢ÂºèË®≠Ë®àÔºâÊ•µÂÖ∑ÂÉπÂÄº„ÄÇLLM Âú®Ëá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢ÁöÑËÉΩÂäõÔºåÂä†‰∏äÂÖ∂ÂèØÊì¥ÂÖÖÊÄß„ÄÅÂ§öÂäüËÉΩÊÄßÂíåÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂèØ‰ª•Âú®ÂêÑÁ®ÆËàá AI Áõ∏ÈóúÁöÑÈ†òÂüü‰∏≠ÂïüÁî®ÂâµÊñ∞ÊáâÁî®ÔºåÂåÖÊã¨ÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI)„ÄÅËá™ÂãïÊ©üÂô®Â≠∏Áøí (AutoML) ÂíåÁü•Ë≠òÂúñË≠ú (KG)„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁõ∏‰ø°ÈÄô‰∫õÊ®°ÂûãÂèØ‰ª•ÊèêÂèñÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰∏¶Â§ßË¶èÊ®°ÂÅöÂá∫Ë≥áÊñôÈ©ÖÂãïÁöÑÊ±∫Á≠ñÔºåÈÄôÁ®ÆÂÅöÊ≥ïÈÄöÂ∏∏Á®±ÁÇ∫Â§ßÊï∏ÊìöÂàÜÊûê (BDA)„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏Ä‰∫õÈóúÊñºËß£ÈéñÈÄô‰∫õÊäÄË°ì‰πãÈñìÁöÑÂçîÂêå‰ΩúÁî®ÁöÑË®éË´ñÔºåÈÄôÂèØ‰ª•Â∞éËá¥Êõ¥Âº∑Â§ß‰∏îÊõ¥Êô∫ÊÖßÁöÑ AI Ëß£Ê±∫ÊñπÊ°àÔºåÊé®ÂãïÂêÑÁ®ÆÊáâÁî®ÂíåÈ†òÂüüÁöÑË≥áÊñôÁÆ°ÈÅìÊîπÈÄ≤ÔºåÊï¥Âêà‰∫∫È°û„ÄÅÈõªËÖ¶ÂíåÁü•Ë≠ò„ÄÇ

##### **Efficient Knowledge Infusion via KG-LLM Alignment**
2406.03746v1 by Zhouyu Jiang, Ling Zhong, Mengshu Sun, Jun Xu, Rui Sun, Hui Cai, Shuhan Luo, Zhiqiang Zhang

To tackle the problem of domain-specific knowledge scarcity within large
language models (LLMs), knowledge graph-retrievalaugmented method has been
proven to be an effective and efficient technique for knowledge infusion.
However, existing approaches face two primary challenges: knowledge mismatch
between public available knowledge graphs and the specific domain of the task
at hand, and poor information compliance of LLMs with knowledge graphs. In this
paper, we leverage a small set of labeled samples and a large-scale corpus to
efficiently construct domain-specific knowledge graphs by an LLM, addressing
the issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM
alignment strategyto enhance the LLM's capability to utilize information from
knowledge graphs. We conduct experiments with a limited-sample setting on two
biomedical question-answering datasets, and the results demonstrate that our
approach outperforms existing baselines.

ÊëòË¶ÅÔºöÈáùÂ∞çÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁâπÂÆöÈ†òÂüüÁü•Ë≠òÁ®ÄÁº∫ÁöÑÂïèÈ°åÔºåÁü•Ë≠òÂúñË≠úÊì∑ÂèñÂ¢ûÂº∑ÊñπÊ≥ïÂ∑≤Ë¢´Ë≠âÊòéÊòØ‰∏ÄÁ®ÆÊúâÊïà‰∏îÈ´òÊïàÁöÑÁü•Ë≠òÊ≥®ÂÖ•ÊäÄË°ì„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÈù¢Ëá®ÂÖ©ÂÄã‰∏ªË¶ÅÊåëÊà∞ÔºöÂÖ¨ÈñãÂèØÁî®ÁöÑÁü•Ë≠òÂúñË≠úËàá‰ªªÂãôÁâπÂÆöÈ†òÂüü‰πãÈñìÁöÑÁü•Ë≠ò‰∏çÂåπÈÖçÔºå‰ª•Âèä LLM ËàáÁü•Ë≠òÂúñË≠úÁöÑË≥áË®äÁõ∏ÂÆπÊÄß‰∏ç‰Ω≥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂà©Áî®‰∏ÄÁµÑÊ®ôË®òÊ®£Êú¨Âíå‰∏ÄÂÄãÂ§ßË¶èÊ®°Ë™ûÊñôÂ∫´ÔºåÁî± LLM ÊúâÊïàÂú∞Âª∫ÊßãÁâπÂÆöÈ†òÂüüÁöÑÁü•Ë≠òÂúñË≠úÔºåËß£Ê±∫Áü•Ë≠ò‰∏çÂåπÈÖçÁöÑÂïèÈ°å„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰∏âÈöéÊÆµÁöÑ KG-LLM Â∞çÈΩäÁ≠ñÁï•Ôºå‰ª•Â¢ûÂº∑ LLM Âà©Áî®Áü•Ë≠òÂúñË≠ú‰∏≠Ë≥áË®äÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÁîüÁâ©ÈÜ´Â≠∏ÂïèÁ≠îË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊúâÈôêÊ®£Êú¨Ë®≠ÂÆöÁöÑÂØ¶È©óÔºåÁµêÊûúË°®ÊòéÊàëÂÄëÁöÑÂÅöÊ≥ïÂÑ™ÊñºÁèæÊúâÁöÑÂü∫Á∑ö„ÄÇ

##### **FastGAS: Fast Graph-based Annotation Selection for In-Context Learning**
2406.03730v1 by Zihan Chen, Song Wang, Cong Shen, Jundong Li

In-context learning (ICL) empowers large language models (LLMs) to tackle new
tasks by using a series of training instances as prompts. Since generating the
prompts needs to sample from a vast pool of instances and annotate them (e.g.,
add labels in classification task), existing methods have proposed to select a
subset of unlabeled examples for annotation, thus enhancing the quality of
prompts and concurrently mitigating annotation costs. However, these methods
often require a long time to select instances due to their complexity,
hindering their practical viability. To address this limitation, we propose a
graph-based selection method, FastGAS, designed to efficiently identify
high-quality instances while minimizing computational overhead. Initially, we
construct a data similarity graph based on instance similarities. Subsequently,
employing a graph partitioning algorithm, we partition the graph into pieces.
Within each piece (i.e., subgraph), we adopt a greedy approach to pick the most
representative nodes. By aggregating nodes from diverse pieces and annotating
the corresponding instances, we identify a set of diverse and representative
instances for ICL. Compared to prior approaches, our method not only exhibits
superior performance on different tasks but also significantly reduces
selection time. In addition, we demonstrate the efficacy of our approach in
LLMs of larger sizes.

ÊëòË¶ÅÔºöÊÉÖÂ¢ÉÂ≠∏Áøí (ICL) Ë≥¶ËÉΩÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩøÁî®‰∏ÄÁ≥ªÂàóË®ìÁ∑¥ÂØ¶‰æã‰ΩúÁÇ∫ÊèêÁ§∫‰æÜËôïÁêÜÊñ∞‰ªªÂãô„ÄÇÁî±ÊñºÁî¢ÁîüÊèêÁ§∫ÈúÄË¶ÅÂæûÂ§ßÈáèÁöÑÂØ¶‰æã‰∏≠ÊäΩÊ®£‰∏¶Ë®ªËß£ÂÆÉÂÄëÔºà‰æãÂ¶ÇÔºåÂú®ÂàÜÈ°û‰ªªÂãô‰∏≠Êñ∞Â¢ûÊ®ôÁ±§ÔºâÔºåÁèæÊúâÊñπÊ≥ïÂ∑≤ÊèêÂá∫ÈÅ∏Êìá‰∏ÄÂÄãÊú™Ê®ôË®òÁØÑ‰æãÁöÑÂ≠êÈõÜÈÄ≤Ë°åË®ªËß£ÔºåÂæûËÄåÊèêÂçáÊèêÁ§∫ÁöÑÂìÅË≥™‰∏¶ÂêåÊôÇÈôç‰ΩéË®ªËß£ÊàêÊú¨„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÁî±ÊñºÂÖ∂Ë§áÈõúÊÄßÔºåÈÄöÂ∏∏ÈúÄË¶ÅÂæàÈï∑ÁöÑÊôÇÈñì‰æÜÈÅ∏ÊìáÂØ¶‰æãÔºåÈòªÁ§ô‰∫ÜÂÆÉÂÄëÁöÑÂØ¶Áî®ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÂúñÂΩ¢ÁöÑÈÅ∏ÊìáÊñπÊ≥ï FastGASÔºåÊó®Âú®ÊúâÊïàÂú∞Ë≠òÂà•È´òÂìÅË≥™ÂØ¶‰æãÔºåÂêåÊôÇÂ∞áÈÅãÁÆóÈñãÈä∑ÈôçÂà∞ÊúÄ‰Ωé„ÄÇÊúÄÂàùÔºåÊàëÂÄëÊ†πÊìöÂØ¶‰æãÁõ∏‰ººÊÄßÂª∫Êßã‰∏ÄÂÄãË≥áÊñôÁõ∏‰ººÊÄßÂúñÂΩ¢„ÄÇÊé•ËëóÔºåÊé°Áî®ÂúñÂΩ¢ÂàÜÂâ≤ÊºîÁÆóÊ≥ïÔºåÂ∞áÂúñÂΩ¢ÂàÜÂâ≤ÊàêÂ§öÂÄãÈÉ®ÂàÜ„ÄÇÂú®ÊØèÂÄãÈÉ®ÂàÜÔºàÂç≥Â≠êÂúñÔºâ‰∏≠ÔºåÊàëÂÄëÊé°Áî®Ë≤™Â©™Ê≥ï‰æÜÊåëÈÅ∏ÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑÁØÄÈªû„ÄÇÈÄèÈÅéÂΩôÁ∏Ω‰æÜËá™‰∏çÂêåÈÉ®ÂàÜÁöÑÁØÄÈªû‰∏¶Ë®ªËß£Â∞çÊáâÁöÑÂØ¶‰æãÔºåÊàëÂÄëË≠òÂà•Âá∫‰∏ÄÁµÑÂ§öÂÖÉ‰∏îÂÖ∑‰ª£Ë°®ÊÄßÁöÑÂØ¶‰æãÔºåÁî®Êñº ICL„ÄÇËàáÂÖàÂâçÁöÑÂÅöÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏çÂÉÖÂú®‰∏çÂêåÁöÑ‰ªªÂãô‰∏äÂ±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÊïàËÉΩÔºåËÄå‰∏îÈÇÑÂ§ßÂπÖÊ∏õÂ∞ë‰∫ÜÈÅ∏ÊìáÊôÇÈñì„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Êõ¥Â§ßË¶èÊ®°ÁöÑ LLM ‰∏≠ÁöÑÊïàËÉΩ„ÄÇ


### Medical explainable AI
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-25**|**Towards Compositional Interpretability for XAI**|Sean Tull et.al.|[2406.17583v1](http://arxiv.org/abs/2406.17583v1)|null|
|**2024-06-10**|**AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI**|K M Tawsik Jawad et.al.|[2406.06728v1](http://arxiv.org/abs/2406.06728v1)|null|
|**2024-06-10**|**Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook**|Yusif Ibrahimov et.al.|[2406.05984v1](http://arxiv.org/abs/2406.05984v1)|null|
|**2024-06-09**|**Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance**|Zhan Zhang et.al.|[2406.05746v1](http://arxiv.org/abs/2406.05746v1)|null|
|**2024-06-07**|**Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability**|Faseela Abdullakutty et.al.|[2406.12897v1](http://arxiv.org/abs/2406.12897v1)|null|
|**2024-06-07**|**Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**|Yong-Min Shin et.al.|[2406.04612v1](http://arxiv.org/abs/2406.04612v1)|[link](https://github.com/jordan7186/gatt)|
|**2024-06-04**|**Using Explainable AI for EEG-based Reduced Montage Neonatal Seizure Detection**|Dinuka Sandun Udayantha et.al.|[2406.16908v1](http://arxiv.org/abs/2406.16908v1)|null|
|**2024-06-01**|**Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques**|Samita Bai et.al.|[2406.00532v1](http://arxiv.org/abs/2406.00532v1)|null|
|**2024-06-01**|**Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech Emotion Recognition**|Alaa Nfissi et.al.|[2406.01624v2](http://arxiv.org/abs/2406.01624v2)|[link](https://github.com/alaanfissi/unveiling-hidden-factors-explainable-ai-for-feature-boosting-in-speech-emotion-recognition)|
|**2024-05-31**|**The Explanation Necessity for Healthcare AI**|Michail Mamalakis et.al.|[2406.00216v1](http://arxiv.org/abs/2406.00216v1)|null|
|**2024-05-29**|**Interdisciplinary Expertise to Advance Equitable Explainable AI**|Chloe R. Bennett et.al.|[2406.18563v1](http://arxiv.org/abs/2406.18563v1)|null|
|**2024-05-26**|**Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making**|Min Hun Lee et.al.|[2405.16424v1](http://arxiv.org/abs/2405.16424v1)|null|
|**2024-05-26**|**Exploring Nutritional Impact on Alzheimer's Mortality: An Explainable AI Approach**|Ziming Liu et.al.|[2405.17502v1](http://arxiv.org/abs/2405.17502v1)|null|
|**2024-05-23**|**Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery**|Yingying Fang et.al.|[2406.18552v1](http://arxiv.org/abs/2406.18552v1)|null|
|**2024-05-21**|**The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach**|Mohsen Jozani et.al.|[2405.13099v1](http://arxiv.org/abs/2405.13099v1)|null|
|**2024-05-17**|**ChatGPT in Classrooms: Transforming Challenges into Opportunities in Education**|Harris Bin Munawar et.al.|[2405.10645v1](http://arxiv.org/abs/2405.10645v1)|null|
|**2024-05-13**|**Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data**|Camelia Oprea et.al.|[2405.07590v1](http://arxiv.org/abs/2405.07590v1)|null|
|**2024-05-10**|**XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**|Fatemeh Nazary et.al.|[2405.06270v3](http://arxiv.org/abs/2405.06270v3)|null|
|**2024-05-09**|**To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**|Miquel Mir√≥-Nicolau et.al.|[2405.05766v1](http://arxiv.org/abs/2405.05766v1)|null|
|**2024-05-05**|**Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**|Zhusi Zhong et.al.|[2405.02815v1](http://arxiv.org/abs/2405.02815v1)|[link](https://github.com/zzs95/RSP_COVID)|
|**2024-04-27**|**Advancing Healthcare Automation: Multi-Agent Systems for Medical Necessity Justification**|Himanshu Pandey et.al.|[2404.17977v1](http://arxiv.org/abs/2404.17977v1)|null|
|**2024-04-26**|**Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**|Francesco Prinzi et.al.|[2405.02334v1](http://arxiv.org/abs/2405.02334v1)|null|
|**2024-04-25**|**Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**|Yunfei Ge et.al.|[2404.16957v1](http://arxiv.org/abs/2404.16957v1)|null|
|**2024-04-19**|**Explainable AI for Fair Sepsis Mortality Predictive Model**|Chia-Hsuan Chang et.al.|[2404.13139v1](http://arxiv.org/abs/2404.13139v1)|null|
|**2024-04-19**|**Multi Class Depression Detection Through Tweets using Artificial Intelligence**|Muhammad Osama Nusrat et.al.|[2404.13104v1](http://arxiv.org/abs/2404.13104v1)|[link](https://github.com/mnusrat786/masters-thesis)|
|**2024-04-19**|**COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**|Dmytro Shvetsov et.al.|[2404.12832v1](http://arxiv.org/abs/2404.12832v1)|[link](https://github.com/dmytro-shvetsov/counterfactual-search)|
|**2024-04-15**|**Hybrid Intelligence for Digital Humanities**|Victor de Boer et.al.|[2406.15374v1](http://arxiv.org/abs/2406.15374v1)|null|
|**2024-04-14**|**Ethical Framework for Responsible Foundational Models in Medical Imaging**|Abhijit Das et.al.|[2406.11868v1](http://arxiv.org/abs/2406.11868v1)|null|
|**2024-04-09**|**Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**|Milad Yousefi et.al.|[2404.07239v1](http://arxiv.org/abs/2404.07239v1)|null|
|**2024-04-06**|**Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**|Taminul Islam et.al.|[2404.04686v1](http://arxiv.org/abs/2404.04686v1)|null|
|**2024-04-05**|**Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**|Maryam Ahmed et.al.|[2404.03892v3](http://arxiv.org/abs/2404.03892v3)|null|
|**2024-03-26**|**Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**|Andrea Ferrario et.al.|[2403.17873v1](http://arxiv.org/abs/2403.17873v1)|null|
|**2024-03-26**|**Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**|Han Yuan et.al.|[2403.18871v1](http://arxiv.org/abs/2403.18871v1)|[link](https://github.com/han-yuan-med/template-explanation)|
|**2024-03-03**|**Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**|S√©amus Lankford et.al.|[2403.01580v1](http://arxiv.org/abs/2403.01580v1)|null|
|**2024-02-28**|**Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**|Yasin Sadeghi Bazargani et.al.|[2402.18600v1](http://arxiv.org/abs/2402.18600v1)|null|
|**2024-02-22**|**Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**|A. J. Karran et.al.|[2402.15027v2](http://arxiv.org/abs/2402.15027v2)|null|
|**2024-02-12**|**Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**|Aruna Mohan et.al.|[2402.09474v2](http://arxiv.org/abs/2402.09474v2)|null|
|**2024-02-05**|**Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**|Aryan Agrawal et.al.|[2402.05127v1](http://arxiv.org/abs/2402.05127v1)|null|
|**2024-01-24**|**Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**|Timoth√©e Schmude et.al.|[2401.13324v4](http://arxiv.org/abs/2401.13324v4)|null|
|**2024-01-02**|**Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**|Vahid Ashrafimoghari et.al.|[2401.02985v1](http://arxiv.org/abs/2401.02985v1)|null|
|**2023-12-29**|**XAI for In-hospital Mortality Prediction via Multimodal ICU Data**|Xingqiao Li et.al.|[2312.17624v1](http://arxiv.org/abs/2312.17624v1)|[link](https://github.com/lixingqiao/xai-icu)|
|**2023-12-22**|**Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**|Norman Zerbe et.al.|[2401.09450v2](http://arxiv.org/abs/2401.09450v2)|null|
|**2023-12-18**|**Robust Stochastic Graph Generator for Counterfactual Explanations**|Mario Alfonso Prado-Romero et.al.|[2312.11747v2](http://arxiv.org/abs/2312.11747v2)|null|
|**2023-12-10**|**Evaluating the Utility of Model Explanations for Model Development**|Shawn Im et.al.|[2312.06032v1](http://arxiv.org/abs/2312.06032v1)|null|
|**2023-12-05**|**Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**|Manas Gaur et.al.|[2312.06798v1](http://arxiv.org/abs/2312.06798v1)|null|
|**2023-11-28**|**Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**|Jacob R. Epifano et.al.|[2311.17133v1](http://arxiv.org/abs/2311.17133v1)|null|
|**2023-11-27**|**Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**|Benjamin Keel et.al.|[2311.15719v1](http://arxiv.org/abs/2311.15719v1)|[link](https://github.com/benkeel/vae_lung_lesion_bmvc)|
|**2023-11-24**|**MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**|Nathan Blake et.al.|[2311.14471v1](http://arxiv.org/abs/2311.14471v1)|null|
|**2023-11-21**|**Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**|Robert Gorwa et.al.|[2311.12573v2](http://arxiv.org/abs/2311.12573v2)|null|
|**2023-11-20**|**Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**|Muta Tah Hira et.al.|[2311.11932v1](http://arxiv.org/abs/2311.11932v1)|null|
|**2023-11-18**|**Representing visual classification as a linear combination of words**|Shobhit Agarwal et.al.|[2311.10933v1](http://arxiv.org/abs/2311.10933v1)|[link](https://github.com/lotterlab/task_word_explainability)|
|**2023-11-03**|**Towards objective and systematic evaluation of bias in medical imaging AI**|Emma A. M. Stanley et.al.|[2311.02115v1](http://arxiv.org/abs/2311.02115v1)|[link](https://github.com/estanley16/simba)|
|**2023-10-29**|**Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**|Adam White et.al.|[2310.19174v1](http://arxiv.org/abs/2310.19174v1)|null|
|**2023-10-03**|**Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**|Hossein Shreim et.al.|[2310.01828v2](http://arxiv.org/abs/2310.01828v2)|[link](https://github.com/geoaigroup/geoai-ecrs2023)|
|**2023-09-26**|**Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**|Muhammad Aurangzeb Ahmad et.al.|[2311.01463v1](http://arxiv.org/abs/2311.01463v1)|null|
|**2023-09-20**|**When to Trust AI: Advances and Challenges for Certification of Neural Networks**|Marta Kwiatkowska et.al.|[2309.11196v1](http://arxiv.org/abs/2309.11196v1)|null|
|**2023-09-19**|**Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**|Juan M. Garc√≠a-G√≥mez et.al.|[2309.10424v1](http://arxiv.org/abs/2309.10424v1)|null|
|**2023-09-19**|**QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**|Thanveer Shaik et.al.|[2309.10293v3](http://arxiv.org/abs/2309.10293v3)|null|
|**2023-09-18**|**Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**|Adarsa Sivaprasad et.al.|[2309.09917v1](http://arxiv.org/abs/2309.09917v1)|null|
|**2023-08-28**|**Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**|Yanjun Gao et.al.|[2308.14321v1](http://arxiv.org/abs/2308.14321v1)|null|
|**2023-08-18**|**Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**|Yun Xin Teoh et.al.|[2308.09380v1](http://arxiv.org/abs/2308.09380v1)|null|
|**2023-08-16**|**Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**|Munib Mesinovic et.al.|[2308.08407v1](http://arxiv.org/abs/2308.08407v1)|null|
|**2023-08-11**|**FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**|Karim Lekadir et.al.|[2309.12325v1](http://arxiv.org/abs/2309.12325v1)|null|
|**2023-08-10**|**Explainable AI applications in the Medical Domain: a systematic review**|Nicoletta Prentzas et.al.|[2308.05411v1](http://arxiv.org/abs/2308.05411v1)|null|
|**2023-08-01**|**Exploring the Role of Explainability in AI-Assisted Embryo Selection**|Lucia Urcelay et.al.|[2308.02534v1](http://arxiv.org/abs/2308.02534v1)|null|
|**2023-07-26**|**A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**|Timo Speith et.al.|[2307.14246v1](http://arxiv.org/abs/2307.14246v1)|null|
|**2023-07-26**|**Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**|Barnaby Crook et.al.|[2307.14239v1](http://arxiv.org/abs/2307.14239v1)|null|
|**2023-07-26**|**Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**|Henry Fraser et.al.|[2308.02047v1](http://arxiv.org/abs/2308.02047v1)|null|
|**2023-07-21**|**eXplainable Artificial Intelligence (XAI) in aging clock models**|Alena Kalyakulina et.al.|[2307.13704v3](http://arxiv.org/abs/2307.13704v3)|null|
|**2023-07-19**|**Interpreting and Correcting Medical Image Classification with PIP-Net**|Meike Nauta et.al.|[2307.10404v2](http://arxiv.org/abs/2307.10404v2)|[link](https://github.com/m-nauta/pipnet)|
|**2023-07-15**|**Explaining and visualizing black-box models through counterfactual paths**|Bastian Pfeifer et.al.|[2307.07764v3](http://arxiv.org/abs/2307.07764v3)|[link](https://github.com/pievos101/cpath)|
|**2023-07-05**|**Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**|Toygar Tanyel et.al.|[2307.02131v5](http://arxiv.org/abs/2307.02131v5)|[link](https://github.com/toygarr/counterfactual-explanations-for-medical-research)|
|**2023-06-30**|**AI and Non AI Assessments for Dementia**|Mahboobeh Parsapoor et.al.|[2307.01210v1](http://arxiv.org/abs/2307.01210v1)|null|
|**2023-06-12**|**Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**|Ruitao Xie et.al.|[2306.07306v1](http://arxiv.org/abs/2306.07306v1)|null|
|**2023-06-09**|**HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**|Rodrigo Agerri et.al.|[2306.06029v1](http://arxiv.org/abs/2306.06029v1)|null|
|**2023-06-07**|**XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**|Eli Laird et.al.|[2306.04791v1](http://arxiv.org/abs/2306.04791v1)|null|
|**2023-06-06**|**Explainable AI using expressive Boolean formulas**|Gili Rosenberg et.al.|[2306.03976v1](http://arxiv.org/abs/2306.03976v1)|null|
|**2023-06-06**|**Utterance Classification with Logical Neural Network: Explainable AI for Mental Disorder Diagnosis**|Yeldar Toleubay et.al.|[2306.03902v1](http://arxiv.org/abs/2306.03902v1)|null|
|**2023-06-02**|**XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models**|Sujith K Mandala et.al.|[2306.01668v1](http://arxiv.org/abs/2306.01668v1)|null|
|**2023-05-26**|**A Novel real-time arrhythmia detection model using YOLOv8**|Guang Jun Nicholas Ang et.al.|[2305.16727v3](http://arxiv.org/abs/2305.16727v3)|null|
|**2023-05-22**|**Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI**|Jai Vardhan et.al.|[2305.14389v2](http://arxiv.org/abs/2305.14389v2)|null|
|**2023-05-18**|**What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media**|Junwei Kuang et.al.|[2305.13127v2](http://arxiv.org/abs/2305.13127v2)|null|
|**2023-05-17**|**Echoes of Biases: How Stigmatizing Language Affects AI Performance**|Yizhi Liu et.al.|[2305.10201v4](http://arxiv.org/abs/2305.10201v4)|null|
|**2023-05-05**|**Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour**|Goda Klumbyte et.al.|[2305.03376v1](http://arxiv.org/abs/2305.03376v1)|null|
|**2023-04-25**|**Towards Explainable and Safe Conversational Agents for Mental Health: A Survey**|Surjodeep Sarkar et.al.|[2304.13191v1](http://arxiv.org/abs/2304.13191v1)|null|
|**2023-04-04**|**A Brief Review of Explainable Artificial Intelligence in Healthcare**|Zahra Sadeghi et.al.|[2304.01543v1](http://arxiv.org/abs/2304.01543v1)|null|
|**2023-03-22**|**Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models**|Frederik Pahde et.al.|[2303.12641v2](http://arxiv.org/abs/2303.12641v2)|[link](https://github.com/maxdreyer/reveal2revise)|
|**2023-03-11**|**Explainable AI for Time Series via Virtual Inspection Layers**|Johanna Vielhaben et.al.|[2303.06365v1](http://arxiv.org/abs/2303.06365v1)|null|
|**2023-03-08**|**Towards Trust of Explainable AI in Thyroid Nodule Diagnosis**|Truong Thanh Hung Nguyen et.al.|[2303.04731v1](http://arxiv.org/abs/2303.04731v1)|[link](https://github.com/hungntt/xai_thyroid)|
|**2023-03-06**|**Cybersecurity of AI medical devices: risks, legislation, and challenges**|Elisabetta Biasin et.al.|[2303.03140v1](http://arxiv.org/abs/2303.03140v1)|null|
|**2023-02-06**|**LAVA: Granular Neuron-Level Explainable AI for Alzheimer's Disease Assessment from Fundus Images**|Nooshin Yousefzadeh et.al.|[2302.03008v2](http://arxiv.org/abs/2302.03008v2)|null|
|**2023-02-02**|**Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses**|Brian Y. Lim et.al.|[2302.01241v2](http://arxiv.org/abs/2302.01241v2)|null|
|**2023-02-02**|**LesionAid: Vision Transformers-based Skin Lesion Generation and Classification**|Ghanta Sai Krishna et.al.|[2302.01104v1](http://arxiv.org/abs/2302.01104v1)|null|
|**2023-02-01**|**SkinCon: A skin disease dataset densely annotated by domain experts for fine-grained model debugging and analysis**|Roxana Daneshjou et.al.|[2302.00785v1](http://arxiv.org/abs/2302.00785v1)|null|
|**2023-01-19**|**Decision-Focused Evaluation: Analyzing Performance of Deployed Restless Multi-Arm Bandits**|Paritosh Verma et.al.|[2301.07835v1](http://arxiv.org/abs/2301.07835v1)|null|
|**2023-01-18**|**Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling**|Carlo Metta et.al.|[2302.03033v1](http://arxiv.org/abs/2302.03033v1)|null|
|**2023-01-17**|**Monotonicity for AI ethics and society: An empirical study of the monotonic neural additive model in criminology, education, health care, and finance**|Dangxing Chen et.al.|[2301.07060v1](http://arxiv.org/abs/2301.07060v1)|null|
|**2023-01-15**|**Rationalizing Predictions by Adversarial Information Calibration**|Lei Sha et.al.|[2301.06009v1](http://arxiv.org/abs/2301.06009v1)|null|
|**2023-01-05**|**Semantic match: Debugging feature attribution methods in XAI for healthcare**|Giovanni Cin√† et.al.|[2301.02080v3](http://arxiv.org/abs/2301.02080v3)|null|
|**2022-12-17**|**Context-dependent Explainability and Contestability for Trustworthy Medical Artificial Intelligence: Misclassification Identification of Morbidity Recognition Models in Preterm Infants**|Isil Guzey et.al.|[2212.08821v1](http://arxiv.org/abs/2212.08821v1)|null|

#### Abstracts
##### **Towards Compositional Interpretability for XAI**
2406.17583v1 by Sean Tull, Robin Lorenz, Stephen Clark, Ilyas Khan, Bob Coecke

Artificial intelligence (AI) is currently based largely on black-box machine
learning models which lack interpretability. The field of eXplainable AI (XAI)
strives to address this major concern, being critical in high-stakes areas such
as the finance, legal and health sectors.
  We present an approach to defining AI models and their interpretability based
on category theory. For this we employ the notion of a compositional model,
which sees a model in terms of formal string diagrams which capture its
abstract structure together with its concrete implementation. This
comprehensive view incorporates deterministic, probabilistic and quantum
models. We compare a wide range of AI models as compositional models, including
linear and rule-based models, (recurrent) neural networks, transformers, VAEs,
and causal and DisCoCirc models.
  Next we give a definition of interpretation of a model in terms of its
compositional structure, demonstrating how to analyse the interpretability of a
model, and using this to clarify common themes in XAI. We find that what makes
the standard 'intrinsically interpretable' models so transparent is brought out
most clearly diagrammatically. This leads us to the more general notion of
compositionally-interpretable (CI) models, which additionally include, for
instance, causal, conceptual space, and DisCoCirc models.
  We next demonstrate the explainability benefits of CI models. Firstly, their
compositional structure may allow the computation of other quantities of
interest, and may facilitate inference from the model to the modelled
phenomenon by matching its structure. Secondly, they allow for diagrammatic
explanations for their behaviour, based on influence constraints, diagram
surgery and rewrite explanations. Finally, we discuss many future directions
for the approach, raising the question of how to learn such meaningfully
structured models in practice.

ÊëòË¶ÅÔºö<paragraph>‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÁõÆÂâçÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥ÊñºÁº∫‰πèÂèØËß£ÈáãÊÄßÁöÑÈªëÁõíÊ©üÂô®Â≠∏ÁøíÊ®°Âûã„ÄÇÂèØËß£ÈáãÊÄß‰∫∫Â∑•Êô∫ÊÖßÔºàXAIÔºâÈ†òÂüüËá¥ÂäõÊñºËß£Ê±∫ÈÄôÂÄã‰∏ªË¶ÅÂïèÈ°åÔºåÈÄôÂú®ÈáëËûç„ÄÅÊ≥ïÂæãÂíåÂÅ•Â∫∑Á≠âÈ´òÈ¢®Èö™È†òÂüüËá≥ÈóúÈáçË¶Å„ÄÇ
ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÁØÑÁñáË´ñÂÆöÁæ© AI Ê®°ÂûãÂèäÂÖ∂ÂèØËß£ÈáãÊÄßÁöÑÊñπÊ≥ï„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊé°Áî®ÁµÑÂêàÊ®°ÂûãÁöÑÊ¶ÇÂøµÔºåÂÆÉ‰ª•ÂΩ¢ÂºèÂº¶ÂúñÁöÑÂΩ¢ÂºèÁúãÂæÖÊ®°ÂûãÔºåÈÄô‰∫õÂº¶ÂúñÊçïÁç≤‰∫ÜÊ®°ÂûãÁöÑÊäΩË±°ÁµêÊßãÂèäÂÖ∂ÂÖ∑È´îÂØ¶Áèæ„ÄÇÈÄôÁ®ÆÁ∂úÂêàËßÄÈªûÂåÖÂê´‰∫ÜÁ¢∫ÂÆöÊÄß„ÄÅÊ¶ÇÁéáÊÄßÂíåÈáèÂ≠êÊ®°Âûã„ÄÇÊàëÂÄëÂ∞áÂêÑÁ®Æ AI Ê®°Âûã‰ΩúÁÇ∫ÁµÑÂêàÊ®°ÂûãÈÄ≤Ë°åÊØîËºÉÔºåÂåÖÊã¨Á∑öÊÄßÂíåÂü∫ÊñºË¶èÂâáÁöÑÊ®°Âûã„ÄÅÔºàÈÅûËø¥ÔºâÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅTransformer„ÄÅVAEÔºå‰ª•ÂèäÂõ†ÊûúÂíå DisCoCirc Ê®°Âûã„ÄÇ
Êé•‰∏ã‰æÜÔºåÊàëÂÄëÊ†πÊìöÊ®°ÂûãÁöÑÁµÑÂêàÁµêÊßãÁµ¶Âá∫Ê®°ÂûãËß£ÈáãÁöÑÂÆöÁæ©ÔºåÂ±ïÁ§∫Â¶Ç‰ΩïÂàÜÊûêÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÔºå‰∏¶‰ΩøÁî®ÂÆÉ‰æÜÊæÑÊ∏Ö XAI ‰∏≠ÁöÑÂ∏∏Ë¶ã‰∏ªÈ°å„ÄÇÊàëÂÄëÁôºÁèæÔºåËÆìÊ®ôÊ∫ñÁöÑ„ÄåÂÖßÂú®ÂèØËß£Èáã„ÄçÊ®°ÂûãÂ¶ÇÊ≠§ÈÄèÊòéÁöÑÂéüÂõ†Âú®ÂúñË°®‰∏≠Ë°®ÁèæÂæóÊúÄÁÇ∫Ê∏ÖÊ•ö„ÄÇÈÄôÂºïÂ∞éÊàëÂÄëÂæóÂá∫Êõ¥‰∏ÄËà¨ÁöÑÁµÑÂêàÂèØËß£ÈáãÔºàCIÔºâÊ®°ÂûãÊ¶ÇÂøµÔºåÂÆÉÂè¶Â§ñÈÇÑÂåÖÊã¨Âõ†Êûú„ÄÅÊ¶ÇÂøµÁ©∫ÈñìÂíå DisCoCirc Ê®°Âûã„ÄÇ
Êé•‰∏ã‰æÜÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü CI Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÂÑ™Âã¢„ÄÇÈ¶ñÂÖàÔºåÂÆÉÂÄëÁöÑÁµÑÂêàÁµêÊßãÂÖÅË®±Ë®àÁÆóÂÖ∂‰ªñÊÑüËààË∂£ÁöÑÈáèÔºå‰∏¶ÂèØËÉΩÈÄöÈÅéÂåπÈÖçÊ®°ÂûãÁöÑÁµêÊßã‰æÜ‰øÉÈÄ≤ÂæûÊ®°ÂûãÂà∞Ë¢´Âª∫Ê®°ÁèæË±°ÁöÑÊé®ÁêÜ„ÄÇÂÖ∂Ê¨°ÔºåÂÆÉÂÄëÂÖÅË®±Â∞çÂÖ∂Ë°åÁÇ∫ÈÄ≤Ë°åÂúñËß£Ë™™ÊòéÔºåÈÄô‰∫õË™™ÊòéÂü∫ÊñºÂΩ±ÈüøÁ¥ÑÊùü„ÄÅÂúñËß£ÊâãË°ìÂíåÈáçÂØ´Ë™™Êòé„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÈÄôÁ®ÆÊñπÊ≥ïÁöÑË®±Â§öÊú™‰æÜÊñπÂêëÔºåÊèêÂá∫‰∫ÜÂ¶Ç‰ΩïÂú®ÂØ¶Ë∏ê‰∏≠Â≠∏ÁøíÈÄôÁ®ÆÊúâÊÑèÁæ©ÁöÑÁµêÊßãÂåñÊ®°ÂûãÁöÑÂïèÈ°å„ÄÇ</paragraph>

##### **AI-Driven Predictive Analytics Approach for Early Prognosis of Chronic Kidney Disease Using Ensemble Learning and Explainable AI**
2406.06728v1 by K M Tawsik Jawad, Anusha Verma, Fathi Amsaad

Chronic Kidney Disease (CKD) is one of the widespread Chronic diseases with
no known ultimo cure and high morbidity. Research demonstrates that progressive
Chronic Kidney Disease (CKD) is a heterogeneous disorder that significantly
impacts kidney structure and functions, eventually leading to kidney failure.
With the progression of time, chronic kidney disease has moved from a
life-threatening disease affecting few people to a common disorder of varying
severity. The goal of this research is to visualize dominating features,
feature scores, and values exhibited for early prognosis and detection of CKD
using ensemble learning and explainable AI. For that, an AI-driven predictive
analytics approach is proposed to aid clinical practitioners in prescribing
lifestyle modifications for individual patients to reduce the rate of
progression of this disease. Our dataset is collected on body vitals from
individuals with CKD and healthy subjects to develop our proposed AI-driven
solution accurately. In this regard, blood and urine test results are provided,
and ensemble tree-based machine-learning models are applied to predict unseen
cases of CKD. Our research findings are validated after lengthy consultations
with nephrologists. Our experiments and interpretation results are compared
with existing explainable AI applications in various healthcare domains,
including CKD. The comparison shows that our developed AI models, particularly
the Random Forest model, have identified more features as significant
contributors than XgBoost. Interpretability (I), which measures the ratio of
important to masked features, indicates that our XgBoost model achieved a
higher score, specifically a Fidelity of 98\%, in this metric and naturally in
the FII index compared to competing models.

ÊëòË¶ÅÔºöÊÖ¢ÊÄßËÖéËáüÁóÖÔºàCKDÔºâÊòØ‰∏ÄÁ®ÆÂª£Ê≥õÁöÑÊÖ¢ÊÄßÁñæÁóÖÔºåÊ≤íÊúâÂ∑≤Áü•ÁöÑÊúÄÁµÇÁôÇÊ≥ï‰∏îÁôºÁóÖÁéáÂæàÈ´ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄ≤Ë°åÊÄßÊÖ¢ÊÄßËÖéËáüÁóÖÔºàCKDÔºâÊòØ‰∏ÄÁ®ÆÁï∞Ë≥™ÊÄßÁñæÁóÖÔºåÊúÉÈ°ØËëóÂΩ±ÈüøËÖéËáüÁµêÊßãÂíåÂäüËÉΩÔºåÊúÄÁµÇÂ∞éËá¥ËÖéË°∞Á´≠„ÄÇÈö®ËëóÊôÇÈñìÁöÑÊé®ÁßªÔºåÊÖ¢ÊÄßËÖéËáüÁóÖÂ∑≤ÂæûÂΩ±ÈüøÂ∞ëÊï∏‰∫∫ÁöÑËá¥ÂëΩÁñæÁóÖËΩâËÆäÁÇ∫‰∏ÄÁ®ÆÂö¥ÈáçÁ®ãÂ∫¶‰∏çÂêåÁöÑÂ∏∏Ë¶ãÁñæÁóÖ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÊ®ôÊòØ‰ΩøÁî®ÈõÜÊàêÂ≠∏ÁøíÂíåÂèØËß£ÈáãÁöÑ AI ÈÄ≤Ë°åÊó©ÊúüÈ†êÂæåÂíå CKD Ê™¢Ê∏¨Ôºå‰∏¶Ë¶ñË¶∫Âåñ‰∏ªÂ∞éÁâπÂæµ„ÄÅÁâπÂæµÂàÜÊï∏ÂíåË°®ÁèæÂá∫ÁöÑÂÄº„ÄÇÁÇ∫Ê≠§ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ AI È©ÖÂãïÁöÑÈ†êÊ∏¨ÂàÜÊûêÊñπÊ≥ïÔºå‰ª•Âπ´Âä©Ëá®Â∫äÈÜ´ÁîüÁÇ∫ÂÄãÂà•ÊÇ£ËÄÖÈñãÂÖ∑ÁîüÊ¥ªÊñπÂºè‰øÆÊîπÂª∫Ë≠∞Ôºå‰ª•Èôç‰ΩéÈÄôÁ®ÆÁñæÁóÖÁöÑÈÄ≤Â±ïÈÄüÂ∫¶„ÄÇÊàëÂÄëÁöÑÊï∏ÊìöÈõÜÊòØÂæû CKD ÊÇ£ËÄÖÂíåÂÅ•Â∫∑ÂèóË©¶ËÄÖÁöÑË∫´È´îÁîüÂëΩÈ´îÂæµ‰∏≠Êî∂ÈõÜÁöÑÔºå‰ª•Ê∫ñÁ¢∫ÈñãÁôºÊàëÂÄëÊèêÂá∫ÁöÑ AI È©ÖÂãïÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂú®ÈÄôÊñπÈù¢ÔºåÊèê‰æõ‰∫ÜË°ÄÊ∂≤ÂíåÂ∞øÊ∂≤Ê™¢Ê∏¨ÁµêÊûúÔºå‰∏¶ÊáâÁî®Âü∫ÊñºÈõÜÊàêÊ®πÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜÈ†êÊ∏¨Êú™ÁôºÁèæÁöÑ CKD ÁóÖ‰æã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÁ∂ìÈÅéËàáËÖéËáüÁßëÈÜ´ÁîüÁöÑÈï∑ÊúüË´ÆË©¢ÂæåÂæóÂà∞È©óË≠â„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÂíåËß£ÈáãÁµêÊûúËàáÂêÑÁ®ÆÈÜ´ÁôÇ‰øùÂÅ•È†òÂüü‰∏≠ÁèæÊúâÁöÑÂèØËß£Èáã AI ÊáâÁî®ÈÄ≤Ë°å‰∫ÜÊØîËºÉÔºåÂåÖÊã¨ CKD„ÄÇÊØîËºÉË°®ÊòéÔºåÊàëÂÄëÈñãÁôºÁöÑ AI Ê®°ÂûãÔºåÁâπÂà•ÊòØÈö®Ê©üÊ£ÆÊûóÊ®°ÂûãÔºåÂ∑≤Á∂ìÁ¢∫ÂÆö‰∫ÜÊØî XgBoost Êõ¥Â§ö‰ΩúÁÇ∫ÈáçË¶ÅË≤¢ÁçªËÄÖÁöÑÁâπÂæµ„ÄÇÂèØËß£ÈáãÊÄß (I) Ë°°ÈáèÈáçË¶ÅÁâπÂæµËàáÊé©ËìãÁâπÂæµÁöÑÊØîÁéáÔºåË°®ÊòéÊàëÂÄëÁöÑ XgBoost Ê®°ÂûãÂú®ÈÄôÂÄãÊåáÊ®ô‰∏≠Áç≤Âæó‰∫ÜÊõ¥È´òÁöÑÂàÜÊï∏ÔºåÁâπÂà•ÊòØ 98% ÁöÑ‰øùÁúüÂ∫¶Ôºå‰∏¶‰∏îÂú® FII ÊåáÊï∏‰∏≠Ëá™ÁÑ∂È´òÊñºÁ´∂Áà≠Ê®°Âûã„ÄÇ

##### **Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook**
2406.05984v1 by Yusif Ibrahimov, Tarique Anwar, Tommy Yuan

Mental health constitutes a complex and pervasive global challenge, affecting
millions of lives and often leading to severe consequences. In this paper, we
conduct a thorough survey to explore the intersection of data science,
artificial intelligence, and mental healthcare, focusing on the recent
developments of mental disorder detection through online social media (OSM). A
significant portion of the population actively engages in OSM platforms,
creating a vast repository of personal data that holds immense potential for
mental health analytics. The paper navigates through traditional diagnostic
methods, state-of-the-art data- and AI-driven research studies, and the
emergence of explainable AI (XAI) models for mental healthcare. We review
state-of-the-art machine learning methods, particularly those based on modern
deep learning, while emphasising the need for explainability in healthcare AI
models. The experimental design section provides insights into prevalent
practices, including available datasets and evaluation approaches. We also
identify key issues and challenges in the field and propose promising future
research directions. As mental health decisions demand transparency,
interpretability, and ethical considerations, this paper contributes to the
ongoing discourse on advancing XAI in mental healthcare through social media.
The comprehensive overview presented here aims to guide researchers,
practitioners, and policymakers in developing the area of mental disorder
detection.

ÊëòË¶ÅÔºöÂøÉÁêÜÂÅ•Â∫∑ÊßãÊàê‰∫Ü‰∏ÄÈ†ÖË§áÈõú‰∏îÊôÆÈÅçÁöÑÂÖ®ÁêÉÊåëÊà∞ÔºåÂΩ±Èüø‰∫ÜÊï∏ÁôæËê¨‰∫∫ÁöÑÁîüÊ¥ªÔºå‰∏¶Á∂ìÂ∏∏Â∞éËá¥Âö¥ÈáçÁöÑÂæåÊûú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÂæπÂ∫ïÁöÑË™øÊü•Ôºå‰ª•Êé¢Á¥¢Êï∏ÊìöÁßëÂ≠∏„ÄÅ‰∫∫Â∑•Êô∫ÊÖßÂíåÂøÉÁêÜ‰øùÂÅ•ÁöÑ‰∫§ÈõÜÔºåÈáçÈªûÈóúÊ≥®ÈÄöÈÅéÁ∑ö‰∏äÁ§æ‰∫§Â™íÈ´î (OSM) ÈÄ≤Ë°åÂøÉÁêÜÁñæÁóÖÊ™¢Ê∏¨ÁöÑÊúÄÊñ∞ÁôºÂ±ï„ÄÇÂæàÂ§ß‰∏ÄÈÉ®ÂàÜ‰∫∫Âè£Á©çÊ•µÂèÉËàá OSM Âπ≥Âè∞ÔºåÂâµÈÄ†‰∫Ü‰∏ÄÂÄãÈæêÂ§ßÁöÑ‰∫∫Âì°Ë≥áÊñôÂ∫´ÔºåÂ∞çÂøÉÁêÜÂÅ•Â∫∑ÂàÜÊûêÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõ„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÂÇ≥Áµ±ÁöÑË®∫Êñ∑ÊñπÊ≥ï„ÄÅÊúÄÂÖàÈÄ≤ÁöÑË≥áÊñôÂíå AI È©ÖÂãïÁöÑÁ†îÁ©∂Ôºå‰ª•ÂèäÂøÉÁêÜ‰øùÂÅ•‰∏≠ÂèØËß£Èáã AI (XAI) Ê®°ÂûãÁöÑÂá∫Áèæ„ÄÇÊàëÂÄëÂõûÈ°ß‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÔºåÁâπÂà•ÊòØÈÇ£‰∫õÂü∫ÊñºÁèæ‰ª£Ê∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÔºåÂêåÊôÇÂº∑Ë™ø‰∫ÜÈÜ´ÁôÇ‰øùÂÅ• AI Ê®°Âûã‰∏≠ÂèØËß£ÈáãÊÄßÁöÑÂøÖË¶ÅÊÄß„ÄÇÂØ¶È©óË®≠Ë®àÈÉ®ÂàÜÊèê‰æõ‰∫ÜÂ∞çÊôÆÈÅçÂÅöÊ≥ïÁöÑË¶ãËß£ÔºåÂåÖÊã¨ÂèØÁî®ÁöÑË≥áÊñôÈõÜÂíåË©ï‰º∞ÊñπÊ≥ï„ÄÇÊàëÂÄëÈÇÑÊâæÂá∫Ë©≤È†òÂüüÁöÑ‰∏ªË¶ÅÂïèÈ°åÂíåÊåëÊà∞Ôºå‰∏¶ÊèêÂá∫‰∫ÜÊúâÂ∏åÊúõÁöÑÊú™‰æÜÁ†îÁ©∂ÊñπÂêë„ÄÇÁî±ÊñºÂøÉÁêÜÂÅ•Â∫∑Ê±∫Á≠ñÈúÄË¶ÅÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíåÈÅìÂæ∑ËÄÉÈáèÔºåÊú¨ÊñáÊúâÂä©ÊñºÊé®ÈÄ≤ÂøÉÁêÜ‰øùÂÅ•‰∏≠ÈÄèÈÅéÁ§æ‰∫§Â™íÈ´îÊé®ÈÄ≤ XAI ÁöÑÊåÅÁ∫åË®éË´ñ„ÄÇÈÄôË£°ÊèêÂá∫ÁöÑÂÖ®Èù¢Ê¶ÇËø∞Êó®Âú®ÂºïÂ∞éÁ†îÁ©∂‰∫∫Âì°„ÄÅÂæûÊ•≠‰∫∫Âì°ÂíåÊîøÁ≠ñÂà∂ÂÆöËÄÖÁôºÂ±ïÂøÉÁêÜÁñæÁóÖÊ™¢Ê∏¨È†òÂüü„ÄÇ

##### **Methodology and Real-World Applications of Dynamic Uncertain Causality Graph for Clinical Diagnosis with Explainability and Invariance**
2406.05746v1 by Zhan Zhang, Qin Zhang, Yang Jiao, Lin Lu, Lin Ma, Aihua Liu, Xiao Liu, Juan Zhao, Yajun Xue, Bing Wei, Mingxia Zhang, Ru Gao, Hong Zhao, Jie Lu, Fan Li, Yang Zhang, Yiming Wang, Lei Zhang, Fengwei Tian, Jie Hu, Xin Gou

AI-aided clinical diagnosis is desired in medical care. Existing deep
learning models lack explainability and mainly focus on image analysis. The
recently developed Dynamic Uncertain Causality Graph (DUCG) approach is
causality-driven, explainable, and invariant across different application
scenarios, without problems of data collection, labeling, fitting, privacy,
bias, generalization, high cost and high energy consumption. Through close
collaboration between clinical experts and DUCG technicians, 46 DUCG models
covering 54 chief complaints were constructed. Over 1,000 diseases can be
diagnosed without triage. Before being applied in real-world, the 46 DUCG
models were retrospectively verified by third-party hospitals. The verified
diagnostic precisions were no less than 95%, in which the diagnostic precision
for every disease including uncommon ones was no less than 80%. After
verifications, the 46 DUCG models were applied in the real-world in China. Over
one million real diagnosis cases have been performed, with only 17 incorrect
diagnoses identified. Due to DUCG's transparency, the mistakes causing the
incorrect diagnoses were found and corrected. The diagnostic abilities of the
clinicians who applied DUCG frequently were improved significantly. Following
the introduction to the earlier presented DUCG methodology, the recommendation
algorithm for potential medical checks is presented and the key idea of DUCG is
extracted.

ÊëòË¶ÅÔºö<paragraph>ÈÜ´ÁôÇÁÖßË≠∑‰∏≠ÈúÄË¶Å AI ËºîÂä©ÁöÑËá®Â∫äË®∫Êñ∑„ÄÇÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁº∫‰πèÂèØËß£ÈáãÊÄßÔºå‰∏¶‰∏î‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂΩ±ÂÉèÂàÜÊûê„ÄÇÊúÄËøëÈñãÁôºÁöÑÂãïÊÖã‰∏çÁ¢∫ÂÆöÂõ†ÊûúÈóú‰øÇÂúñ (DUCG) ÊñπÊ≥ïÊòØÂõ†ÊûúÈ©ÖÂãïÁöÑ„ÄÅÂèØËß£ÈáãÁöÑÔºå‰∏¶‰∏îÂú®‰∏çÂêåÁöÑÊáâÁî®Â†¥ÊôØ‰∏≠ÊòØ‰∏çËÆäÁöÑÔºåÊ≤íÊúâË≥áÊñôÊî∂ÈõÜ„ÄÅÊ®ôË®ò„ÄÅÊì¨Âêà„ÄÅÈö±ÁßÅ„ÄÅÂÅèË¶ã„ÄÅÊ¶ÇÂåñ„ÄÅÈ´òÊàêÊú¨ÂíåÈ´òËÉΩËÄóÁöÑÂïèÈ°å„ÄÇÈÄöÈÅéËá®Â∫äÂ∞àÂÆ∂Âíå DUCG ÊäÄË°ì‰∫∫Âì°‰πãÈñìÁöÑÂØÜÂàáÂêà‰ΩúÔºåÊßãÂª∫‰∫ÜÊ∂µËìã 54 ÂÄã‰∏ªË®¥ÁöÑ 46 ÂÄã DUCG Ê®°Âûã„ÄÇÂèØ‰ª•Âú®Ê≤íÊúâÂàÜÊµÅÁöÑÊÉÖÊ≥Å‰∏ãË®∫Êñ∑Âá∫ 1,000 Â§öÁ®ÆÁñæÁóÖ„ÄÇÂú®ÊáâÁî®ÊñºÂØ¶Èöõ‰∏ñÁïå‰πãÂâçÔºå46 ÂÄã DUCG Ê®°ÂûãÂ∑≤Áî±Á¨¨‰∏âÊñπÈÜ´Èô¢ÂõûÊ∫ØÊÄßÈ©óË≠â„ÄÇÈ©óË≠âÁöÑË®∫Êñ∑Á≤æÂ∫¶‰∏ç‰ΩéÊñº 95%ÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÁΩïË¶ãÁñæÁóÖÂú®ÂÖßÁöÑÊØèÁ®ÆÁñæÁóÖÁöÑË®∫Êñ∑Á≤æÂ∫¶‰∏ç‰ΩéÊñº 80%„ÄÇÈ©óË≠âÂæåÔºå46 ÂÄã DUCG Ê®°ÂûãÂ∑≤Âú®‰∏≠ÂúãÂØ¶ÈöõÊáâÁî®„ÄÇÂ∑≤Á∂ìÂü∑Ë°å‰∫ÜË∂ÖÈÅé‰∏ÄÁôæËê¨ÂÄãÁúüÂØ¶Ë®∫Êñ∑Ê°à‰æãÔºåÂÉÖÁôºÁèæ 17 ÂÄã‰∏çÊ≠£Á¢∫ÁöÑË®∫Êñ∑„ÄÇÁî±Êñº DUCG ÁöÑÈÄèÊòéÊÄßÔºåÁôºÁèæ‰∏¶Á≥æÊ≠£‰∫ÜÂ∞éËá¥‰∏çÊ≠£Á¢∫Ë®∫Êñ∑ÁöÑÈåØË™§„ÄÇÈ†ªÁπÅÊáâÁî® DUCG ÁöÑËá®Â∫äÈÜ´ÁîüÁöÑË®∫Êñ∑ËÉΩÂäõÂæóÂà∞‰∫ÜÈ°ØËëóÊèêÈ´ò„ÄÇÂú®‰ªãÁ¥π‰∫ÜÂâçÈù¢ÊèêÂá∫ÁöÑ DUCG ÊñπÊ≥ïË´ñ‰πãÂæåÔºåÊèêÂá∫‰∫ÜÊΩõÂú®ÂÅ•Â∫∑Ê™¢Êü•ÁöÑÊé®Ëñ¶ÊºîÁÆóÊ≥ïÔºå‰∏¶ÊèêÂèñ‰∫Ü DUCG ÁöÑÈóúÈçµÊÄùÊÉ≥„ÄÇ</paragraph>

##### **Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability**
2406.12897v1 by Faseela Abdullakutty, Younes Akbari, Somaya Al-Maadeed, Ahmed Bouridane, Rifat Hamoudi

It is imperative that breast cancer is detected precisely and timely to
improve patient outcomes. Diagnostic methodologies have traditionally relied on
unimodal approaches; however, medical data analytics is integrating diverse
data sources beyond conventional imaging. Using multi-modal techniques,
integrating both image and non-image data, marks a transformative advancement
in breast cancer diagnosis. The purpose of this review is to explore the
burgeoning field of multimodal techniques, particularly the fusion of
histopathology images with non-image data. Further, Explainable AI (XAI) will
be used to elucidate the decision-making processes of complex algorithms,
emphasizing the necessity of explainability in diagnostic processes. This
review utilizes multi-modal data and emphasizes explainability to enhance
diagnostic accuracy, clinician confidence, and patient engagement, ultimately
fostering more personalized treatment strategies for breast cancer, while also
identifying research gaps in multi-modality and explainability, guiding future
studies, and contributing to the strategic direction of the field.

ÊëòË¶ÅÔºöÁ≤æÁ¢∫‰∏îÂèäÊôÇÂú∞ÂÅµÊ∏¨‰π≥ÁôåÂ∞çÊñºÊîπÂñÑÊÇ£ËÄÖÈ†êÂæåËá≥ÈóúÈáçË¶Å„ÄÇË®∫Êñ∑ÊñπÊ≥ïÂÇ≥Áµ±‰∏ä‰æùË≥¥ÊñºÂñÆ‰∏ÄÊ®°ÂºèÊñπÊ≥ïÔºõÁÑ∂ËÄåÔºåÈÜ´ÁôÇË≥áÊñôÂàÜÊûêÊ≠£Âú®Êï¥ÂêàË∂ÖË∂äÂÇ≥Áµ±ÂΩ±ÂÉèÁöÑÂêÑÁ®ÆË≥áÊñô‰æÜÊ∫ê„ÄÇ‰ΩøÁî®Êï¥ÂêàÂΩ±ÂÉèÂíåÈùûÂΩ±ÂÉèË≥áÊñôÁöÑÂ§öÊ®°ÂºèÊäÄË°ìÔºåÊ®ôË™åËëó‰π≥ÁôåË®∫Êñ∑ÁöÑËÆäÈù©ÊÄßÈÄ≤Â±ï„ÄÇÊú¨ÁØáÁ∂úËø∞ÁöÑÁõÆÁöÑÊòØÊé¢Ë®éÂ§öÊ®°ÂºèÊäÄË°ìÁöÑÊñ∞ËààÈ†òÂüüÔºåÁâπÂà•ÊòØÂ∞áÁµÑÁπîÁóÖÁêÜÂ≠∏ÂΩ±ÂÉèËàáÈùûÂΩ±ÂÉèË≥áÊñôËûçÂêà„ÄÇÊ≠§Â§ñÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∞áÁî®ÊñºÈó°ÊòéË§áÈõúÊºîÁÆóÊ≥ïÁöÑÊ±∫Á≠ñÈÅéÁ®ãÔºåÂº∑Ë™øË®∫Êñ∑ÈÅéÁ®ã‰∏≠ÂèØËß£ÈáãÊÄßÁöÑÂøÖË¶ÅÊÄß„ÄÇÊú¨Á∂úËø∞Âà©Áî®Â§öÊ®°ÂºèË≥áÊñô‰∏¶Âº∑Ë™øÂèØËß£ÈáãÊÄßÔºå‰ª•ÊèêÈ´òË®∫Êñ∑Ê∫ñÁ¢∫ÊÄß„ÄÅËá®Â∫äÈÜ´Â∏´ÁöÑ‰ø°ÂøÉÂíåÊÇ£ËÄÖÂèÉËàáÂ∫¶ÔºåÊúÄÁµÇ‰øÉÈÄ≤‰π≥ÁôåÊõ¥ÂÄã‰∫∫ÂåñÁöÑÊ≤ªÁôÇÁ≠ñÁï•ÔºåÂêåÊôÇ‰πüÊâæÂá∫Â§öÊ®°ÂºèÂíåÂèØËß£ÈáãÊÄßÁöÑÁ†îÁ©∂Â∑ÆË∑ùÔºåÂºïÂ∞éÊú™‰æÜÁöÑÁ†îÁ©∂Ôºå‰∏¶ÁÇ∫Ë©≤È†òÂüüÁöÑÁ≠ñÁï•ÊñπÂêëÂÅöÂá∫Ë≤¢Áçª„ÄÇ

##### **Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks**
2406.04612v1 by Yong-Min Shin, Siqing Li, Xin Cao, Won-Yong Shin

The self-attention mechanism has been adopted in several widely-used
message-passing neural networks (MPNNs) (e.g., GATs), which adaptively controls
the amount of information that flows along the edges of the underlying graph.
This usage of attention has made such models a baseline for studies on
explainable AI (XAI) since interpretations via attention have been popularized
in various domains (e.g., natural language processing and computer vision).
However, existing studies often use naive calculations to derive attribution
scores from attention, and do not take the precise and careful calculation of
edge attribution into consideration. In our study, we aim to fill the gap
between the widespread usage of attention-enabled MPNNs and their potential in
largely under-explored explainability, a topic that has been actively
investigated in other areas. To this end, as the first attempt, we formalize
the problem of edge attribution from attention weights in GNNs. Then, we
propose GATT, an edge attribution calculation method built upon the computation
tree. Through comprehensive experiments, we demonstrate the effectiveness of
our proposed method when evaluating attributions from GATs. Conversely, we
empirically validate that simply averaging attention weights over graph
attention layers is insufficient to interpret the GAT model's behavior. Code is
publicly available at https://github.com/jordan7186/GAtt/tree/main.

ÊëòË¶ÅÔºöËá™Ê≥®ÊÑèÂäõÊ©üÂà∂Â∑≤Ë¢´Êé°Áî®ÊñºÂ§öÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑË®äÊÅØÂÇ≥ÈÅûÁ•ûÁ∂ìÁ∂≤Ë∑Ø (MPNN)Ôºà‰æãÂ¶Ç GATÔºâÔºåÂÆÉÂèØ‰ª•Ëá™ÈÅ©ÊáâÂú∞ÊéßÂà∂Ê≤øËëóÂ∫ïÂ±§ÂúñÂΩ¢ÈÇäÁ∑£ÊµÅÂãïÁöÑË≥áË®äÈáè„ÄÇÈÄôÁ®ÆÊ≥®ÊÑèÂäõÁöÑ‰ΩøÁî®‰ΩøÂæóÊ≠§È°ûÊ®°ÂûãÊàêÁÇ∫ÂèØËß£Èáã AI (XAI) Á†îÁ©∂ÁöÑÂü∫Á∑öÔºåÂõ†ÁÇ∫ÈÄèÈÅéÊ≥®ÊÑèÂäõÁöÑË©ÆÈáãÂ∑≤Âú®ÂêÑÁ®ÆÈ†òÂüüÔºà‰æãÂ¶ÇËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÈõªËÖ¶Ë¶ñË¶∫Ôºâ‰∏≠ÊôÆÂèä„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÁ†îÁ©∂ÈÄöÂ∏∏‰ΩøÁî®Â§©ÁúüÁöÑË®àÁÆóÊñπÊ≥ïÂæûÊ≥®ÊÑèÂäõ‰∏≠Êé®Â∞éÂá∫Ê≠∏Âõ†ÂàÜÊï∏Ôºå‰∏¶‰∏îÊ≤íÊúâËÄÉÊÖÆÂà∞ÈÇäÁ∑£Ê≠∏Âõ†ÁöÑÁ≤æÁ¢∫‰∏î‰ªîÁ¥∞ÁöÑË®àÁÆó„ÄÇÂú®ÊàëÂÄëÁöÑÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊó®Âú®Â°´Ë£úÊ≥®ÊÑèÂäõÂïüÁî® MPNN ÁöÑÂª£Ê≥õ‰ΩøÁî®ËàáÂÆÉÂÄëÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢ÁöÑÂèØËß£ÈáãÊÄß‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÈÄôÂÄã‰∏ªÈ°åÂ∑≤Âú®ÂÖ∂‰ªñÈ†òÂüüÁ©çÊ•µÁ†îÁ©∂„ÄÇÁÇ∫Ê≠§Ôºå‰ΩúÁÇ∫Á¨¨‰∏ÄÊ¨°ÂòóË©¶ÔºåÊàëÂÄëÂ∞á GNN ‰∏≠Ê≥®ÊÑèÂäõÊ¨äÈáçÁöÑÈÇäÁ∑£Ê≠∏Âõ†ÂïèÈ°åÂΩ¢ÂºèÂåñ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫ GATTÔºå‰∏ÄÁ®ÆÂª∫Á´ãÂú®Ë®àÁÆóÊ®π‰∏äÁöÑÈÇäÁ∑£Ê≠∏Âõ†Ë®àÁÆóÊñπÊ≥ï„ÄÇÈÄèÈÅéÂÖ®Èù¢ÁöÑÂØ¶È©óÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Ë©ï‰º∞ GAT ÁöÑÊ≠∏Âõ†ÊôÇÊâÄÂÖ∑ÊúâÁöÑÊïàÊûú„ÄÇÁõ∏ÂèçÂú∞ÔºåÊàëÂÄëÊÜëÁ∂ìÈ©óÈ©óË≠â‰∫ÜÂÉÖÂ∞çÂúñÊ≥®ÊÑèÂäõÂ±§‰∏äÁöÑÊ≥®ÊÑèÂäõÊ¨äÈáçÂèñÂπ≥ÂùáÂÄº‰∏çË∂≥‰ª•Ë©ÆÈáã GAT Ê®°ÂûãÁöÑË°åÁÇ∫„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº https://github.com/jordan7186/GAtt/tree/main„ÄÇ

##### **Using Explainable AI for EEG-based Reduced Montage Neonatal Seizure Detection**
2406.16908v1 by Dinuka Sandun Udayantha, Kavindu Weerasinghe, Nima Wickramasinghe, Akila Abeyratne, Kithmin Wickremasinghe, Jithangi Wanigasinghe, Anjula De Silva, Chamira Edussooriya

The neonatal period is the most vulnerable time for the development of
seizures. Seizures in the immature brain lead to detrimental consequences,
therefore require early diagnosis. The gold-standard for neonatal seizure
detection currently relies on continuous video-EEG monitoring; which involves
recording multi-channel electroencephalogram (EEG) alongside real-time video
monitoring within a neonatal intensive care unit (NICU). However, video-EEG
monitoring technology requires clinical expertise and is often limited to
technologically advanced and resourceful settings. Cost-effective new
techniques could help the medical fraternity make an accurate diagnosis and
advocate treatment without delay. In this work, a novel explainable deep
learning model to automate the neonatal seizure detection process with a
reduced EEG montage is proposed, which employs convolutional nets, graph
attention layers, and fully connected layers. Beyond its ability to detect
seizures in real-time with a reduced montage, this model offers the unique
advantage of real-time interpretability. By evaluating the performance on the
Zenodo dataset with 10-fold cross-validation, the presented model achieves an
absolute improvement of 8.31% and 42.86% in area under curve (AUC) and recall,
respectively.

ÊëòË¶ÅÔºöÊñ∞ÁîüÂÖíÊúüÊòØÂ§ßËÖ¶ÁôºËÇ≤ÊúÄÂÆπÊòìÂá∫ÁèæÁô≤ÁôáÁöÑÊôÇÊúü„ÄÇÂ§ßËÖ¶Â∞öÊú™ÊàêÁÜüÊôÇÁôºÁîüÁöÑÁô≤ÁôáÊúÉÈÄ†Êàê‰∏çËâØÂæåÊûúÔºåÂõ†Ê≠§ÈúÄË¶ÅÊèêÊó©Ë®∫Êñ∑„ÄÇÁõÆÂâçÊñ∞ÁîüÂÖíÁô≤ÁôáÊ™¢Ê∏¨ÁöÑÈªÉÈáëÊ®ôÊ∫ñ‰æùË≥¥ÊñºÊåÅÁ∫åÁöÑË¶ñË®äËÖ¶ÈõªÂúñ (EEG) Áõ£ÊéßÔºõÂÖ∂‰∏≠ÂåÖÊã¨Âú®Êñ∞ÁîüÂÖíÂä†Ë≠∑ÁóÖÊàø (NICU) ÂÖßÈåÑË£ΩÂ§öÈÄöÈÅìËÖ¶ÈõªÂúñ (EEG) ÂíåÈÄ≤Ë°åÂç≥ÊôÇË¶ñË®äÁõ£Êéß„ÄÇÁÑ∂ËÄåÔºåË¶ñË®äËÖ¶ÈõªÂúñÁõ£ÊéßÊäÄË°ìÈúÄË¶ÅËá®Â∫äÂ∞àÊ•≠Áü•Ë≠òÔºåËÄå‰∏îÈÄöÂ∏∏ÂÉÖÈôêÊñºÊäÄË°ìÂÖàÈÄ≤‰∏îË≥áÊ∫êË±êÂØåÁöÑÁí∞Â¢É„ÄÇÂÖ∑ÊàêÊú¨ÊïàÁõäÁöÑÊñ∞ÊäÄË°ìÂèØ‰ª•Âπ´Âä©ÈÜ´ÁôÇÁïåÊ∫ñÁ¢∫Ë®∫Êñ∑‰∏¶Á´ãÂç≥ÊèêÂÄ°Ê≤ªÁôÇ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÂèØËß£ÈáãÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰ª•Ëá™ÂãïÂåñÊñ∞ÁîüÂÖíÁô≤ÁôáÊ™¢Ê∏¨ÊµÅÁ®ãÔºå‰∏¶Ê∏õÂ∞ëËÖ¶ÈõªÂúñË£ùÁΩÆÔºåÊé°Áî®Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÅÂúñÊ≥®ÊÑèÂäõÂ±§ÂíåÂÖ®ÈÄ£Êé•Â±§„ÄÇÈô§‰∫ÜËÉΩÂ§†Âç≥ÊôÇÂÅµÊ∏¨Áô≤ÁôáÁôº‰Ωú‰∏¶Ê∏õÂ∞ëË£ùÁΩÆÂ§ñÔºåÊ≠§Ê®°ÂûãÈÇÑÊèê‰æõ‰∫ÜÂç≥ÊôÇÂèØËß£ÈáãÊÄßÁöÑÁç®ÁâπÂÑ™Âã¢„ÄÇÈÄèÈÅéË©ï‰º∞ Zenodo Ë≥áÊñôÈõÜÁöÑ 10 ÂÄç‰∫§ÂèâÈ©óË≠âÊïàËÉΩÔºåÊèêÂá∫ÁöÑÊ®°ÂûãÂú®Êõ≤Á∑ö‰∏ãÈù¢Á©ç (AUC) ÂíåÂè¨ÂõûÁéáÂàÜÂà•ÈÅîÂà∞ 8.31% Âíå 42.86% ÁöÑÁµïÂ∞çÊîπÂñÑ„ÄÇ

##### **Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques**
2406.00532v1 by Samita Bai, Sidra Nasir, Rizwan Ahmed Khan, Sheeraz Arif, Alexandre Meyer, Hubert Konik

Breast cancer (BC) stands as one of the most common malignancies affecting
women worldwide, necessitating advancements in diagnostic methodologies for
better clinical outcomes. This article provides a comprehensive exploration of
the application of Explainable Artificial Intelligence (XAI) techniques in the
detection and diagnosis of breast cancer. As Artificial Intelligence (AI)
technologies continue to permeate the healthcare sector, particularly in
oncology, the need for transparent and interpretable models becomes imperative
to enhance clinical decision-making and patient care. This review discusses the
integration of various XAI approaches, such as SHAP, LIME, Grad-CAM, and
others, with machine learning and deep learning models utilized in breast
cancer detection and classification. By investigating the modalities of breast
cancer datasets, including mammograms, ultrasounds and their processing with
AI, the paper highlights how XAI can lead to more accurate diagnoses and
personalized treatment plans. It also examines the challenges in implementing
these techniques and the importance of developing standardized metrics for
evaluating XAI's effectiveness in clinical settings. Through detailed analysis
and discussion, this article aims to highlight the potential of XAI in bridging
the gap between complex AI models and practical healthcare applications,
thereby fostering trust and understanding among medical professionals and
improving patient outcomes.

ÊëòË¶ÅÔºö‰π≥Áôå (BC) ÊòØÂΩ±ÈüøÂÖ®ÁêÉÂ•≥ÊÄßÊúÄÂ∏∏Ë¶ãÁöÑÊÉ°ÊÄßËÖ´Áò§‰πã‰∏ÄÔºåÂõ†Ê≠§ÈúÄË¶ÅÈÄ≤Ê≠•ÁöÑË®∫Êñ∑ÊñπÊ≥ïÔºå‰ª•ÊîπÂñÑËá®Â∫äÁµêÊûú„ÄÇÊú¨ÊñáÂÖ®Èù¢Êé¢Ë®é‰∫ÜÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊäÄË°ìÂú®‰π≥ÁôåÂÅµÊ∏¨ÂíåË®∫Êñ∑‰∏≠ÁöÑÊáâÁî®„ÄÇÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖß (AI) ÊäÄË°ìÊåÅÁ∫åÊª≤ÈÄèÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÔºåÁâπÂà•ÊòØÂú®ËÖ´Áò§Â≠∏‰∏≠ÔºåÈÄèÊòé‰∏îÂèØËß£ÈáãÁöÑÊ®°ÂûãÈúÄÊ±ÇËÆäÂæóÂã¢Âú®ÂøÖË°åÔºå‰ª•Â¢ûÂº∑Ëá®Â∫äÊ±∫Á≠ñÂà∂ÂÆöÂíåÊÇ£ËÄÖÁÖßË≠∑„ÄÇÊ≠§ÁØáË©ïË´ñÊé¢Ë®é‰∫ÜÂêÑÁ®Æ XAI ÊñπÊ≥ïÁöÑÊï¥ÂêàÔºå‰æãÂ¶Ç SHAP„ÄÅLIME„ÄÅGrad-CAM Á≠âÔºå‰ª•ÂèäÁî®Êñº‰π≥ÁôåÂÅµÊ∏¨ÂíåÂàÜÈ°ûÁöÑÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã„ÄÇÈÄèÈÅéÊé¢Ë®é‰π≥ÁôåË≥áÊñôÈõÜÁöÑÊ®°ÂºèÔºåÂåÖÊã¨‰π≥ÊàøÊîùÂΩ±„ÄÅË∂ÖÈü≥Ê≥¢ÂèäÂÖ∂Âú® AI ‰∏≠ÁöÑËôïÁêÜÔºåÊú¨ÊñáÈáçÈªûË™™Êòé XAI Â¶Ç‰ΩïËÉΩÂ∞éËá¥Êõ¥Ê∫ñÁ¢∫ÁöÑË®∫Êñ∑ÂíåÂÄã‰∫∫ÂåñÊ≤ªÁôÇË®àÁï´„ÄÇÂÆÉ‰πüÊé¢Ë®é‰∫ÜÂØ¶ÊñΩÈÄô‰∫õÊäÄË°ìÁöÑÊåëÊà∞Ôºå‰ª•ÂèäÂà∂ÂÆöÊ®ôÊ∫ñÂåñË©ïÈáèÊåáÊ®ô‰ª•Ë©ï‰º∞ XAI Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊúâÊïàÊÄßÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄèÈÅéË©≥Á¥∞ÁöÑÂàÜÊûêÂíåË®éË´ñÔºåÊú¨ÊñáÊó®Âú®Âº∑Ë™ø XAI Âú®Á∏ÆÂ∞èË§áÈõú AI Ê®°ÂûãËàáÂØ¶ÂãôÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰πãÈñìÂ∑ÆË∑ùÁöÑÊΩõÂäõÔºåÈÄ≤ËÄå‰øÉÈÄ≤ÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°‰πãÈñìÁöÑ‰ø°‰ªªËàáÁêÜËß£Ôºå‰∏¶ÊîπÂñÑÊÇ£ËÄÖÁöÑÁµêÊûú„ÄÇ

##### **Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech Emotion Recognition**
2406.01624v2 by Alaa Nfissi, Wassim Bouachir, Nizar Bouguila, Brian Mishara

Speech emotion recognition (SER) has gained significant attention due to its
several application fields, such as mental health, education, and
human-computer interaction. However, the accuracy of SER systems is hindered by
high-dimensional feature sets that may contain irrelevant and redundant
information. To overcome this challenge, this study proposes an iterative
feature boosting approach for SER that emphasizes feature relevance and
explainability to enhance machine learning model performance. Our approach
involves meticulous feature selection and analysis to build efficient SER
systems. In addressing our main problem through model explainability, we employ
a feature evaluation loop with Shapley values to iteratively refine feature
sets. This process strikes a balance between model performance and
transparency, which enables a comprehensive understanding of the model's
predictions. The proposed approach offers several advantages, including the
identification and removal of irrelevant and redundant features, leading to a
more effective model. Additionally, it promotes explainability, facilitating
comprehension of the model's predictions and the identification of crucial
features for emotion determination. The effectiveness of the proposed method is
validated on the SER benchmarks of the Toronto emotional speech set (TESS),
Berlin Database of Emotional Speech (EMO-DB), Ryerson Audio-Visual Database of
Emotional Speech and Song (RAVDESS), and Surrey Audio-Visual Expressed Emotion
(SAVEE) datasets, outperforming state-of-the-art methods. To the best of our
knowledge, this is the first work to incorporate model explainability into an
SER framework. The source code of this paper is publicly available via this
https://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognition.

ÊëòË¶ÅÔºöË™ûÈü≥ÊÉÖÁ∑íËæ®Ë≠ò (SER) Áî±ÊñºÂÖ∂Âú®ÂøÉÁêÜÂÅ•Â∫∑„ÄÅÊïôËÇ≤Âíå‰∫∫Ê©ü‰∫íÂãïÁ≠âÂ§öÂÄãÊáâÁî®È†òÂüüËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåSER Á≥ªÁµ±ÁöÑÊ∫ñÁ¢∫ÊÄßÂèóÂà∞È´òÁ∂≠ÁâπÂæµÈõÜÁöÑÈòªÁ§ôÔºåÈÄô‰∫õÁâπÂæµÈõÜÂèØËÉΩÂåÖÂê´‰∏çÁõ∏ÈóúÂíåÂÜóÈ§òÁöÑË≥áË®ä„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄôÂÄãÊåëÊà∞ÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁî®Êñº SER ÁöÑËø≠‰ª£ÁâπÂæµÊèêÂçáÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÂº∑Ë™øÁâπÂæµÁõ∏ÈóúÊÄßÂíåÂèØËß£ÈáãÊÄßÔºå‰ª•Â¢ûÂº∑Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊ∂âÂèä‰ªîÁ¥∞ÁöÑÁâπÂæµÈÅ∏ÊìáÂíåÂàÜÊûêÔºå‰ª•Âª∫Á´ãÈ´òÊïàÁöÑ SER Á≥ªÁµ±„ÄÇÁÇ∫‰∫ÜÈÄèÈÅéÊ®°ÂûãÂèØËß£ÈáãÊÄßËß£Ê±∫ÊàëÂÄëÁöÑÊ†∏ÂøÉÂïèÈ°åÔºåÊàëÂÄëÊé°Áî®‰∫ÜÂÖ∑Êúâ Shapley ÂÄºÁöÑÁâπÂæµË©ï‰º∞Ëø¥ÂúàÔºå‰ª•ÂèçË¶ÜÊîπÂñÑÁâπÂæµÈõÜ„ÄÇÈÄôÂÄãÈÅéÁ®ãÂú®Ê®°ÂûãÊïàËÉΩÂíåÈÄèÊòéÂ∫¶‰πãÈñìÂèñÂæóÂπ≥Ë°°ÔºåÈÄô‰ΩøÂæóÊàëÂÄëËÉΩÂ§†ÂÖ®Èù¢‰∫ÜËß£Ê®°ÂûãÁöÑÈ†êÊ∏¨„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜÂ§öÈ†ÖÂÑ™ÈªûÔºåÂåÖÊã¨Ë≠òÂà•ÂíåÁßªÈô§‰∏çÁõ∏ÈóúÂíåÂÜóÈ§òÁöÑÁâπÂæµÔºåÂæûËÄåÂª∫Á´ãÊõ¥ÊúâÊïàÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÂÆÉ‰øÉÈÄ≤‰∫ÜÂèØËß£ÈáãÊÄßÔºåÊúâÂä©ÊñºÁêÜËß£Ê®°ÂûãÁöÑÈ†êÊ∏¨‰ª•ÂèäË≠òÂà•ÊÉÖÁ∑íÊ±∫ÂÆöÁöÑÈóúÈçµÁâπÂæµ„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂ∑≤Âú®Â§öÂÄ´Â§öÊÉÖÁ∑íË™ûÈü≥ÈõÜ (TESS)„ÄÅÊüèÊûóÊÉÖÁ∑íË™ûÈü≥Ë≥áÊñôÂ∫´ (EMO-DB)„ÄÅË≥¥ÁàæÊ£ÆÈü≥Ë®äË¶ñË¶∫ÊÉÖÁ∑íË™ûÈü≥ÂíåÊ≠åÊõ≤Ë≥áÊñôÂ∫´ (RAVDESS) ÂíåËñ©ÈáåÈü≥Ë®äË¶ñË¶∫Ë°®ÈÅîÊÉÖÁ∑í (SAVEE) Ë≥áÊñôÈõÜÁöÑ SER Âü∫Ê∫ñ‰∏äÂæóÂà∞È©óË≠âÔºåÂÖ∂ÊïàËÉΩÂÑ™ÊñºÁèæÊúâÊñπÊ≥ï„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂ∞áÊ®°ÂûãÂèØËß£ÈáãÊÄßÁ¥çÂÖ• SER Êû∂ÊßãÁöÑÁ†îÁ©∂„ÄÇÊú¨ÊñáÁöÑÂéüÂßãÁ¢ºÂèØÈÄèÈÅéÊ≠§ÈÄ£ÁµêÂÖ¨ÈñãÂèñÂæóÔºöhttps://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognition„ÄÇ

##### **The Explanation Necessity for Healthcare AI**
2406.00216v1 by Michail Mamalakis, H√©lo√Øse de Vareilles, Graham Murray, Pietro Lio, John Suckling

Explainability is often critical to the acceptable implementation of
artificial intelligence (AI). Nowhere is this more important than healthcare
where decision-making directly impacts patients and trust in AI systems is
essential. This trust is often built on the explanations and interpretations
the AI provides. Despite significant advancements in AI interpretability, there
remains the need for clear guidelines on when and to what extent explanations
are necessary in the medical context. We propose a novel categorization system
with four distinct classes of explanation necessity, guiding the level of
explanation required: patient or sample (local) level, cohort or dataset
(global) level, or both levels. We introduce a mathematical formulation that
distinguishes these categories and offers a practical framework for researchers
to determine the necessity and depth of explanations required in medical AI
applications. Three key factors are considered: the robustness of the
evaluation protocol, the variability of expert observations, and the
representation dimensionality of the application. In this perspective, we
address the question: When does an AI medical application need to be explained,
and at what level of detail?

ÊëòË¶ÅÔºöÂèØËß£ÈáäÊÄßÈÄöÂ∏∏ÂØπ‰∫é‰∫∫Â∑•Êô∫ËÉΩ (AI) ÁöÑÂèØÊé•ÂèóÂÆûÊñΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®ÂåªÁñó‰øùÂÅ•È¢ÜÂüüÔºåËøô‰∏ÄÁÇπÂ∞§‰∏∫ÈáçË¶ÅÔºåÂõ†‰∏∫ÂÜ≥Á≠ñÁõ¥Êé•ÂΩ±ÂìçÊÇ£ËÄÖÔºåÂπ∂‰∏îÂØπ AI Á≥ªÁªüÁöÑ‰ø°‰ªªËá≥ÂÖ≥ÈáçË¶Å„ÄÇËøôÁßç‰ø°‰ªªÈÄöÂ∏∏Âª∫Á´ãÂú® AI Êèê‰æõÁöÑËß£ÈáäÂíåËØ†Èáä‰πã‰∏ä„ÄÇÂ∞ΩÁÆ° AI ÂèØËß£ÈáäÊÄßÂèñÂæó‰∫ÜÈáçÂ§ßËøõÂ±ïÔºå‰ΩÜ‰ªçÁÑ∂ÈúÄË¶ÅÊòéÁ°ÆÁöÑÊåáÂØºÊñπÈíàÔºåËØ¥ÊòéÂú®ÂåªÁñóÁéØÂ¢É‰∏≠‰ΩïÊó∂‰ª•ÂèäÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÈúÄË¶ÅËß£Èáä„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂàÜÁ±ªÁ≥ªÁªüÔºåËØ•Á≥ªÁªüÂÖ∑ÊúâÂõõÁßç‰∏çÂêåÁöÑËß£ÈáäÂøÖË¶ÅÊÄßÁ±ªÂà´ÔºåÊåáÂØºÊâÄÈúÄÁöÑËß£ÈáäÁ∫ßÂà´ÔºöÊÇ£ËÄÖÊàñÊ†∑Êú¨ÔºàÂ±ÄÈÉ®ÔºâÁ∫ßÂà´„ÄÅÈòüÂàóÊàñÊï∞ÊçÆÈõÜÔºàÂÖ®Â±ÄÔºâÁ∫ßÂà´ÔºåÊàñ‰∏§‰∏™Á∫ßÂà´„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Êï∞Â≠¶ÂÖ¨ÂºèÔºåËØ•ÂÖ¨ÂºèÂå∫ÂàÜ‰∫ÜËøô‰∫õÁ±ªÂà´ÔºåÂπ∂‰∏∫Á†îÁ©∂‰∫∫ÂëòÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÆûÁî®Ê°ÜÊû∂Ôºå‰ª•Á°ÆÂÆöÂåªÁñó AI Â∫îÁî®‰∏≠ÊâÄÈúÄÁöÑËß£ÈáäÁöÑÂøÖË¶ÅÊÄßÂíåÊ∑±Â∫¶„ÄÇËÄÉËôë‰∫Ü‰∏â‰∏™ÂÖ≥ÈîÆÂõ†Á¥†ÔºöËØÑ‰º∞ÂçèËÆÆÁöÑÁ®≥ÂÅ•ÊÄß„ÄÅ‰∏ìÂÆ∂ËßÇÂØüÁöÑÂèØÂèòÊÄß‰ª•ÂèäÂ∫îÁî®Á®ãÂ∫èÁöÑË°®Á§∫Áª¥Êï∞„ÄÇ‰ªéËøô‰∏™ËßíÂ∫¶Êù•ÁúãÔºåÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜËøô‰∏™ÈóÆÈ¢òÔºöAI ÂåªÁñóÂ∫îÁî®‰ΩïÊó∂ÈúÄË¶ÅËß£ÈáäÔºå‰ª•ÂèäÈúÄË¶ÅËß£ÈáäÂà∞‰ΩïÁßçÁ®ãÂ∫¶Ôºü

##### **Interdisciplinary Expertise to Advance Equitable Explainable AI**
2406.18563v1 by Chloe R. Bennett, Heather Cole-Lewis, Stephanie Farquhar, Naama Haamel, Boris Babenko, Oran Lang, Mat Fleck, Ilana Traynis, Charles Lau, Ivor Horn, Courtney Lyles

The field of artificial intelligence (AI) is rapidly influencing health and
healthcare, but bias and poor performance persists for populations who face
widespread structural oppression. Previous work has clearly outlined the need
for more rigorous attention to data representativeness and model performance to
advance equity and reduce bias. However, there is an opportunity to also
improve the explainability of AI by leveraging best practices of social
epidemiology and health equity to help us develop hypotheses for associations
found. In this paper, we focus on explainable AI (XAI) and describe a framework
for interdisciplinary expert panel review to discuss and critically assess AI
model explanations from multiple perspectives and identify areas of bias and
directions for future research. We emphasize the importance of the
interdisciplinary expert panel to produce more accurate, equitable
interpretations which are historically and contextually informed.
Interdisciplinary panel discussions can help reduce bias, identify potential
confounders, and identify opportunities for additional research where there are
gaps in the literature. In turn, these insights can suggest opportunities for
AI model improvement.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) È†òÂüüÊ≠£Âø´ÈÄüÂΩ±ÈüøËëóÂÅ•Â∫∑ËàáÈÜ´ÁôÇ‰øùÂÅ•Ôºå‰ΩÜÂ∞çÊñºÈù¢Ëá®Âª£Ê≥õÁµêÊßãÊÄßÂ£ìËø´ÁöÑ‰∫∫Áæ§‰æÜË™™ÔºåÂÅèË¶ãÂíå‰∏çËâØË°®Áèæ‰æùÁÑ∂Â≠òÂú®„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∑≤Ê∏ÖÊ•öË™™ÊòéÔºåÈúÄË¶ÅÊõ¥Âö¥Ê†ºÂú∞Ê≥®ÊÑèË≥áÊñô‰ª£Ë°®ÊÄßÂíåÊ®°ÂûãÊïàËÉΩÔºå‰ª•‰øÉÈÄ≤ÂÖ¨Âπ≥ÊÄß‰∏¶Ê∏õÂ∞ëÂÅèË¶ã„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÊúâÊ©üÊúÉÈÄèÈÅéÈÅãÁî®Á§æÊúÉÊµÅË°åÁóÖÂ≠∏ÂíåÂÅ•Â∫∑ÂÖ¨Âπ≥ÁöÑÊúÄ‰Ω≥ÂØ¶ÂãôÔºå‰æÜÊîπÂñÑ AI ÁöÑÂèØËß£ÈáãÊÄßÔºå‰ª•Âπ´Âä©ÊàëÂÄëÈáùÂ∞çÁôºÁèæÁöÑÈóúËÅØÊÄßÔºåÁôºÂ±ïÂÅáË®≠„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂèØËß£Èáã AI (XAI)Ôºå‰∏¶ÊèèËø∞‰∏ÄÂÄãË∑®È†òÂüüÂ∞àÂÆ∂Â∞èÁµÑÂØ©Êü•Êû∂ÊßãÔºå‰ª•ÂæûÂ§öÈáçËßÄÈªûË®éË´ñÂíåÊâπÂà§ÊÄßË©ï‰º∞ AI Ê®°ÂûãÁöÑËß£ÈáãÔºå‰∏¶ÊâæÂá∫ÂÅèË¶ãÈ†òÂüüÂíåÊú™‰æÜÁ†îÁ©∂ÁöÑÊñπÂêë„ÄÇÊàëÂÄëÂº∑Ë™øË∑®È†òÂüüÂ∞àÂÆ∂Â∞èÁµÑÂ∞çÊñºÁî¢ÁîüÊõ¥Ê∫ñÁ¢∫„ÄÅÂÖ¨Âπ≥ÁöÑË©ÆÈáãËá≥ÈóúÈáçË¶ÅÔºåËÄåÈÄô‰∫õË©ÆÈáãÊòØÊ†πÊìöÊ≠∑Âè≤ÂíåËÑàÁµ°ËÄå‰æÜÁöÑ„ÄÇË∑®È†òÂüüÂ∞èÁµÑË®éË´ñÊúâÂä©ÊñºÊ∏õÂ∞ëÂÅèË¶ã„ÄÅÊâæÂá∫ÊΩõÂú®ÁöÑÊ∑∑Ê∑ÜÂõ†Á¥†Ôºå‰∏¶Âú®ÊñáÁçª‰∏≠ÊúâÁº∫Âè£ÊôÇÊâæÂá∫È°çÂ§ñÁ†îÁ©∂ÁöÑÊ©üÊúÉ„ÄÇÂèçÈÅé‰æÜÔºåÈÄô‰∫õË¶ãËß£ÂèØ‰ª•Âª∫Ë≠∞ AI Ê®°ÂûãÊîπÈÄ≤ÁöÑÊ©üÊúÉ„ÄÇ

##### **Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making**
2405.16424v1 by Min Hun Lee, Silvana Xin Yi Choo, Shamala D/O Thilarajah

With advanced AI/ML, there has been growing research on explainable AI (XAI)
and studies on how humans interact with AI and XAI for effective human-AI
collaborative decision-making. However, we still have a lack of understanding
of how AI systems and XAI should be first presented to users without technical
backgrounds. In this paper, we present the findings of semi-structured
interviews with health professionals (n=12) and students (n=4) majoring in
medicine and health to study how to improve onboarding with AI and XAI. For the
interviews, we built upon human-AI interaction guidelines to create onboarding
materials of an AI system for stroke rehabilitation assessment and AI
explanations and introduce them to the participants. Our findings reveal that
beyond presenting traditional performance metrics on AI, participants desired
benchmark information, the practical benefits of AI, and interaction trials to
better contextualize AI performance, and refine the objectives and performance
of AI. Based on these findings, we highlight directions for improving
onboarding with AI and XAI and human-AI collaborative decision-making.

ÊëòË¶ÅÔºöÈö®ËëóÂÖàÈÄ≤ÁöÑ AI/MLÔºåÂ∞çÂèØËß£Èáã AI (XAI) ÁöÑÁ†îÁ©∂‰∏çÊñ∑Â¢ûÂä†Ôºå‰ª•ÂèäÈóúÊñº‰∫∫È°ûÂ¶Ç‰ΩïËàá AI Âíå XAI ‰∫íÂãï‰ª•ÈÄ≤Ë°åÊúâÊïàÁöÑ‰∫∫Â∑•Êô∫ÊÖßÂçî‰ΩúÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄë‰ªçÁÑ∂Áº∫‰πèÂ∞ç AI Á≥ªÁµ±Âíå XAI ÊáâÂ¶Ç‰ΩïÈ¶ñÂÖàÂëàÁèæÁµ¶Ê≤íÊúâÊäÄË°ìËÉåÊôØÁöÑÁî®Êà∂ÁöÑ‰∫ÜËß£„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜËàáÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì° (n=12) Âíå‰∏ª‰øÆÈÜ´Â≠∏ÂíåÂÅ•Â∫∑ÁöÑÂ≠∏Áîü (n=4) ÈÄ≤Ë°åÂçäÁµêÊßãÂåñË®™Ë´áÁöÑÁµêÊûúÔºå‰ª•Á†îÁ©∂Â¶Ç‰ΩïÊîπÂñÑ AI Âíå XAI ÁöÑÂÖ•ÈñÄ„ÄÇÂ∞çÊñºË®™Ë´áÔºåÊàëÂÄëÂª∫Á´ãÂú®‰∫∫Ê©ü‰∫íÂãïÊ∫ñÂâá‰πã‰∏äÔºåÁÇ∫‰∏≠È¢®Â∫∑Âæ©Ë©ï‰º∞Âíå AI Ëß£ÈáãÁöÑ AI Á≥ªÁµ±ÂâµÂª∫ÂÖ•ÈñÄÊùêÊñôÔºå‰∏¶Â∞áÂÆÉÂÄë‰ªãÁ¥πÁµ¶ÂèÉËàáËÄÖ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÈô§‰∫ÜÂëàÁèæÂÇ≥Áµ±ÁöÑ AI ÊÄßËÉΩÊåáÊ®ôÂ§ñÔºåÂèÉËàáËÄÖÈÇÑÂ∏åÊúõÂü∫ÂáÜ‰ø°ÊÅØ„ÄÅAI ÁöÑÂØ¶ÈöõÂ•ΩËôï‰ª•Âèä‰∫§‰∫íË©¶È©óÔºå‰ª•Êõ¥Â•ΩÂú∞Â∞á AI ÊÄßËÉΩÊÉÖÂ¢ÉÂåñÔºå‰∏¶ÂÆåÂñÑ AI ÁöÑÁõÆÊ®ôÂíåÊÄßËÉΩ„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÂº∑Ë™ø‰∫ÜÊîπÈÄ≤ AI Âíå XAI ‰ª•Âèä‰∫∫Ê©üÂçî‰ΩúÊ±∫Á≠ñÂà∂ÂÆöÁöÑÂÖ•ÈñÄÊñπÂêë„ÄÇ

##### **Exploring Nutritional Impact on Alzheimer's Mortality: An Explainable AI Approach**
2405.17502v1 by Ziming Liu, Longjian Liu, Robert E. Heidel, Xiaopeng Zhao

This article uses machine learning (ML) and explainable artificial
intelligence (XAI) techniques to investigate the relationship between
nutritional status and mortality rates associated with Alzheimers disease (AD).
The Third National Health and Nutrition Examination Survey (NHANES III)
database is employed for analysis. The random forest model is selected as the
base model for XAI analysis, and the Shapley Additive Explanations (SHAP)
method is used to assess feature importance. The results highlight significant
nutritional factors such as serum vitamin B12 and glycated hemoglobin. The
study demonstrates the effectiveness of random forests in predicting AD
mortality compared to other diseases. This research provides insights into the
impact of nutrition on AD and contributes to a deeper understanding of disease
progression.

ÊëòË¶ÅÔºöÊú¨Êñá‰ΩøÁî®Ê©üÂô®Â≠∏Áøí (ML) ÂíåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊäÄË°ì‰æÜÊé¢Ë®éÁáüÈ§äÁãÄÊ≥ÅËàáÈòøËå≤Êµ∑ÈªòÁóá (AD) Áõ∏ÈóúÁöÑÊ≠ª‰∫°Áéá‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊé°Áî®Á¨¨‰∏âÊ¨°ÂÖ®ÂúãÂÅ•Â∫∑ËàáÁáüÈ§äÊ™¢Êü•Ë™øÊü• (NHANES III) Ë≥áÊñôÂ∫´ÈÄ≤Ë°åÂàÜÊûê„ÄÇÈÅ∏ÊìáÈö®Ê©üÊ£ÆÊûóÊ®°Âûã‰ΩúÁÇ∫ XAI ÂàÜÊûêÁöÑÂü∫Á§éÊ®°ÂûãÔºå‰∏¶‰ΩøÁî® Shapley Additive Explanations (SHAP) ÊñπÊ≥ï‰æÜË©ï‰º∞ÁâπÂæµÈáçË¶ÅÊÄß„ÄÇÁµêÊûúÁ™ÅÈ°Ø‰∫ÜÈáçË¶ÅÁöÑÁáüÈ§äÂõ†Á¥†Ôºå‰æãÂ¶ÇË°ÄÊ∏ÖÁ∂≠ÁîüÁ¥† B12 ÂíåÁ≥ñÂåñË°ÄÁ¥ÖËõãÁôΩ„ÄÇË©≤Á†îÁ©∂Ë≠âÊòé‰∫ÜÈö®Ê©üÊ£ÆÊûóÂú®È†êÊ∏¨ AD Ê≠ª‰∫°ÁéáÊñπÈù¢Áõ∏ËºÉÊñºÂÖ∂‰ªñÁñæÁóÖÁöÑÊúâÊïàÊÄß„ÄÇÊú¨Á†îÁ©∂Êèê‰æõ‰∫ÜÁáüÈ§äÂ∞ç AD ÁöÑÂΩ±ÈüøÁöÑË¶ãËß£Ôºå‰∏¶ÊúâÂä©ÊñºÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÁñæÁóÖÁöÑÈÄ≤Â±ï„ÄÇ

##### **Decoding Decision Reasoning: A Counterfactual-Powered Model for Knowledge Discovery**
2406.18552v1 by Yingying Fang, Zihao Jin, Xiaodan Xing, Simon Walsh, Guang Yang

In medical imaging, particularly in early disease detection and prognosis
tasks, discerning the rationale behind an AI model's predictions is crucial for
evaluating the reliability of its decisions. Conventional explanation methods
face challenges in identifying discernible decisive features in medical image
classifications, where discriminative features are subtle or not immediately
apparent. To bridge this gap, we propose an explainable model that is equipped
with both decision reasoning and feature identification capabilities. Our
approach not only detects influential image patterns but also uncovers the
decisive features that drive the model's final predictions. By implementing our
method, we can efficiently identify and visualise class-specific features
leveraged by the data-driven model, providing insights into the decision-making
processes of deep learning models. We validated our model in the demanding
realm of medical prognosis task, demonstrating its efficacy and potential in
enhancing the reliability of AI in healthcare and in discovering new knowledge
in diseases where prognostic understanding is limited.

ÊëòË¶ÅÔºöÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÔºåÁâπÂà•ÊòØÂú®Êó©ÊúüÁñæÁóÖÊ™¢Ê∏¨ÂíåÈ†êÂæå‰ªªÂãô‰∏≠ÔºåËæ®Âà• AI Ê®°ÂûãÈ†êÊ∏¨ËÉåÂæåÁöÑÂéüÁêÜÂ∞çÊñºË©ï‰º∞ÂÖ∂Ê±∫Á≠ñÁöÑÂèØÈù†ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÂÇ≥Áµ±ÁöÑËß£ÈáãÊñπÊ≥ïÂú®Ë≠òÂà•ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰∏≠ÂèØË≠òÂà•ÁöÑÊ±∫ÂÆöÊÄßÁâπÂæµÊôÇÈù¢Ëá®ÊåëÊà∞ÔºåÂÖ∂‰∏≠ÂçÄÂà•ÊÄßÁâπÂæµÂæàÂæÆÂ¶ôÊàñ‰∏¶‰∏çÊòéÈ°Ø„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÈÄô‰∏ÄÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊ®°ÂûãÔºåË©≤Ê®°ÂûãÂÖ∑ÂÇôÊ±∫Á≠ñÊé®ÁêÜÂíåÁâπÂæµË≠òÂà•ËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏çÂÉÖÊ™¢Ê∏¨ÊúâÂΩ±ÈüøÂäõÁöÑÂΩ±ÂÉèÊ®°ÂºèÔºåÈÇÑÊè≠Á§∫‰∫ÜÊé®ÂãïÊ®°ÂûãÊúÄÁµÇÈ†êÊ∏¨ÁöÑÊ±∫ÂÆöÊÄßÁâπÂæµ„ÄÇÈÄöÈÅéÂØ¶ÊñΩÊàëÂÄëÁöÑÊ®°ÂûãÔºåÊàëÂÄëÂèØ‰ª•ÊúâÊïàË≠òÂà•ÂíåË¶ñË¶∫ÂåñÁî±Êï∏ÊìöÈ©ÖÂãïÊ®°ÂûãÂà©Áî®ÁöÑÈ°ûÁâπÂÆöÁâπÂæµÔºåÂæûËÄåÊ∑±ÂÖ•‰∫ÜËß£Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÊ±∫Á≠ñÈÅéÁ®ã„ÄÇÊàëÂÄëÂú®Ë¶ÅÊ±ÇÂö¥Ê†ºÁöÑÈÜ´Â≠∏È†êÂæå‰ªªÂãôÈ†òÂüüÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®ÊèêÈ´ò AI Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÂèØÈù†ÊÄßÂíåÁôºÁèæÈ†êÂæåÁêÜËß£ÂèóÈôêÁñæÁóÖÁöÑÊñ∞Áü•Ë≠òÊñπÈù¢ÁöÑÂäüÊïàÂíåÊΩõÂäõ„ÄÇ

##### **The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach**
2405.13099v1 by Mohsen Jozani, Jason A. Williams, Ahmed Aleroud, Sarbottam Bhagat

This study explores the relationship between informational support seeking
questions, responses, and helpfulness ratings in online health communities. We
created a labeled data set of question-response pairs and developed multimodal
machine learning and deep learning models to reliably predict informational
support questions and responses. We employed explainable AI to reveal the
emotions embedded in informational support exchanges, demonstrating the
importance of emotion in providing informational support. This complex
interplay between emotional and informational support has not been previously
researched. The study refines social support theory and lays the groundwork for
the development of user decision aids. Further implications are discussed.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®éÁ∑ö‰∏äÂÅ•Â∫∑Á§æÁæ§‰∏≠Â∞ãÊ±ÇË≥áË®äÊîØÊåÅÁöÑÂïèÈ°å„ÄÅÂõûÊáâÔºå‰ª•ÂèäÊúâÂπ´Âä©ÁöÑË©ïÂàÜ‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊàëÂÄëÂª∫Á´ã‰∫Ü‰∏ÄÁµÑÊ®ôË®òÁöÑÂïèÁ≠îÈÖçÂ∞çË≥áÊñôÈõÜÔºå‰∏¶ÈñãÁôº‰∫ÜÂ§öÊ®°ÊÖãÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÔºå‰ª•ÂèØÈù†Âú∞È†êÊ∏¨Ë≥áË®äÊîØÊåÅÂïèÈ°åÂíåÂõûÊáâ„ÄÇÊàëÂÄëÊé°Áî®ÂèØËß£ÈáãÁöÑ AI ‰æÜÊè≠Á§∫Ë≥áË®äÊîØÊåÅ‰∫§ÊµÅ‰∏≠ËòäÂê´ÁöÑÊÉÖÁ∑íÔºåË≠âÊòéÊÉÖÁ∑íÂú®Êèê‰æõË≥áË®äÊîØÊåÅ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄôÁ®ÆÊÉÖÁ∑íÊîØÊåÅÂíåË≥áË®äÊîØÊåÅ‰πãÈñìÁöÑË§áÈõú‰∫§‰∫í‰ΩúÁî®‰ª•Ââç‰∏¶Êú™Ë¢´Á†îÁ©∂ÈÅé„ÄÇÊú¨Á†îÁ©∂ÊîπÈÄ≤‰∫ÜÁ§æÊúÉÊîØÊåÅÁêÜË´ñÔºå‰∏¶ÁÇ∫‰ΩøÁî®ËÄÖÊ±∫Á≠ñËºîÂä©Â∑•ÂÖ∑ÁöÑÈñãÁôºÂ•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇË®éË´ñ‰∫ÜÈÄ≤‰∏ÄÊ≠•ÁöÑÂΩ±Èüø„ÄÇ

##### **ChatGPT in Classrooms: Transforming Challenges into Opportunities in Education**
2405.10645v1 by Harris Bin Munawar, Nikolaos Misirlis

In the era of exponential technology growth, one unexpected guest has claimed
a seat in classrooms worldwide, Artificial Intelligence. Generative AI, such as
ChatGPT, promises a revolution in education, yet it arrives with a double-edged
sword. Its potential for personalized learning is offset by issues of cheating,
inaccuracies, and educators struggling to incorporate it effectively into their
lesson design. We are standing on the brink of this educational frontier, and
it is clear that we need to navigate this terrain with a lot of care. This is a
major challenge that could undermine the integrity and value of our educational
process. So, how can we turn these challenges into opportunities? When used
inappropriately, AI tools can become the perfect tool for the cut copy paste
mentality, and quickly begin to corrode critical thinking, creativity, and deep
understanding, the most important skills in our rapidly changing world.
Teachers feel that they are not equipped to leverage this technology, widening
the digital divide among educators and institutions. Addressing these concerns
calls for an in depth research approach. We will employ empirical research,
drawing on the Technology Acceptance Model, to assess the attitudes toward
generative AI among educators and students. Understanding their perceptions,
usage patterns, and hurdles is the first crucial step in creating an effective
solution. The present study will be used as a process manual for future
researchers to apply, running their own data, based on the steps explained here

ÊëòË¶ÅÔºöÂú®ÁßëÊäÄÈ£õÈÄüÁôºÂ±ïÁöÑÊôÇ‰ª£Ôºå‰∏Ä‰ΩçÊÑèÂ§ñÁöÑË®™ÂÆ¢Â∑≤Âú®ÂÖ®ÁêÉÊïôÂÆ§‰∏≠‰ΩîÊúâ‰∏ÄÂ∏≠‰πãÂú∞ÔºåÈÇ£Â∞±ÊòØ‰∫∫Â∑•Êô∫ÊÖß„ÄÇÁîüÊàêÂºè AIÔºå‰æãÂ¶Ç ChatGPTÔºåÊâøË´æÂú®ÊïôËÇ≤È†òÂüüÊéÄËµ∑‰∏ÄÂ†¥Èù©ÂëΩÔºå‰ΩÜÂÆÉÂçªÊòØ‰∏ÄÊääÈõôÈù¢ÂàÉ„ÄÇÂÆÉÂú®ÂÄã‰∫∫ÂåñÂ≠∏ÁøíÊñπÈù¢ÁöÑÊΩõÂäõÔºåÂçªÂõ†‰ΩúÂºä„ÄÅ‰∏çÊ∫ñÁ¢∫‰ª•ÂèäÊïôËÇ≤Â∑•‰ΩúËÄÖÈõ£‰ª•Â∞áÂÖ∂ÊúâÊïàËûçÂÖ•ÊïôÂ≠∏Ë®≠Ë®àÁ≠âÂïèÈ°åËÄåÊäµÈä∑„ÄÇÊàëÂÄëÊ≠£Á´ôÂú®ÈÄôÊïôËÇ≤ÂâçÊ≤øÁöÑÈÇäÁ∑£ÔºåÈ°ØÁÑ∂ÊàëÂÄëÈúÄË¶ÅÈùûÂ∏∏Â∞èÂøÉÂú∞Êé¢Á¥¢ÈÄôÁâáÈ†òÂüü„ÄÇÈÄôÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂèØËÉΩÊúÉÊêçÂÆ≥ÊàëÂÄëÊïôËÇ≤ÈÅéÁ®ãÁöÑÂÆåÊï¥ÊÄßÂíåÂÉπÂÄº„ÄÇÈÇ£È∫ºÔºåÊàëÂÄëÂ¶Ç‰ΩïÂ∞áÈÄô‰∫õÊåëÊà∞ËΩâÂåñÁÇ∫Ê©üÈÅáÔºüÁï∂‰∏çÈÅ©Áï∂Âú∞‰ΩøÁî®ÊôÇÔºåAI Â∑•ÂÖ∑ÂèØËÉΩÊúÉÊàêÁÇ∫Ë§áË£ΩË≤º‰∏äÂøÉÊÖãÁöÑÂÆåÁæéÂ∑•ÂÖ∑Ôºå‰∏¶ËøÖÈÄüËÖêËùïÊâπÂà§ÊÄßÊÄùÁ∂≠„ÄÅÂâµÈÄ†ÂäõÂíåÊ∑±ÂÖ•ÁêÜËß£ÔºåÈÄô‰∫õÈÉΩÊòØÊàëÂÄëÂø´ÈÄüËÆäÂåñÁöÑ‰∏ñÁïå‰∏≠ÊúÄÈáçË¶ÅÁöÑÊäÄËÉΩ„ÄÇÊïôÂ∏´ÂÄëË¶∫Âæó‰ªñÂÄëÊ≤íÊúâËÉΩÂäõÂà©Áî®ÈÄôÈ†ÖÊäÄË°ìÔºåÈÄôÊì¥Â§ß‰∫ÜÊïôËÇ≤Â∑•‰ΩúËÄÖÂíåÊ©üÊßã‰πãÈñìÁöÑÊï∏‰ΩçÈ¥ªÊ∫ù„ÄÇËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÈúÄË¶ÅÊ∑±ÂÖ•ÁöÑÁ†îÁ©∂ÊñπÊ≥ï„ÄÇÊàëÂÄëÂ∞áÊé°Áî®ÂØ¶Ë≠âÁ†îÁ©∂ÔºåÂÄüÈëëÊäÄË°ìÊé•ÂèóÊ®°ÂûãÔºå‰æÜË©ï‰º∞ÊïôËÇ≤Â∑•‰ΩúËÄÖÂíåÂ≠∏ÁîüÂ∞çÁîüÊàêÂºè AI ÁöÑÊÖãÂ∫¶„ÄÇ‰∫ÜËß£‰ªñÂÄëÁöÑÁúãÊ≥ï„ÄÅ‰ΩøÁî®Ê®°ÂºèÂíåÈöúÁ§ôÊòØÂâµÈÄ†ÊúâÊïàËß£Ê±∫ÊñπÊ°àÁöÑÁ¨¨‰∏ÄÂÄãÈóúÈçµÊ≠•È©ü„ÄÇÊú¨Á†îÁ©∂Â∞á‰ΩúÁÇ∫Êú™‰æÜÁ†îÁ©∂‰∫∫Âì°ÊáâÁî®ÁöÑÊµÅÁ®ãÊâãÂÜäÔºåÊ†πÊìöÊ≠§ËôïË™™ÊòéÁöÑÊ≠•È©üÈÅãË°å‰ªñÂÄëËá™Â∑±ÁöÑÊï∏Êìö

##### **Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data**
2405.07590v1 by Camelia Oprea, Mike Gr√ºne, Mateusz Buglowski, Lena Olivier, Thorsten Orlikowsky, Stefan Kowalewski, Mark Schoberer, Andr√© Stollenwerk

With the digitalization of health care systems, artificial intelligence
becomes more present in medicine. Especially machine learning shows great
potential for complex tasks such as time series classification, usually at the
cost of transparency and comprehensibility. This leads to a lack of trust by
humans and thus hinders its active usage. Explainable artificial intelligence
tries to close this gap by providing insight into the decision-making process,
the actual usefulness of its different methods is however unclear. This paper
proposes a user study based evaluation of the explanation method Grad-CAM with
application to a neural network for the classification of breaths in time
series neonatal ventilation data. We present the perceived usefulness of the
explainability method by different stakeholders, exposing the difficulty to
achieve actual transparency and the wish for more in-depth explanations by many
of the participants.

ÊëòË¶ÅÔºöÈö®ËëóÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±ÁöÑÊï∏‰ΩçÂåñÔºå‰∫∫Â∑•Êô∫ÊÖßÂú®ÈÜ´Â≠∏È†òÂüü‰∏≠ËÆäÂæóÊõ¥Âä†ÊôÆÂèä„ÄÇÁâπÂà•ÊòØÊ©üÂô®Â≠∏ÁøíÂú®ÊôÇÈñìÂ∫èÂàóÂàÜÈ°ûÁ≠âË§áÈõú‰ªªÂãô‰∏≠Â±ïÁèæÂá∫Ê•µÂ§ßÁöÑÊΩõÂäõÔºå‰ΩÜÈÄöÂ∏∏ÊòØ‰ª•ÈÄèÊòéÂ∫¶ÂíåÂèØÁêÜËß£ÊÄßÁÇ∫‰ª£ÂÉπ„ÄÇÈÄôÂ∞éËá¥‰∫∫È°ûÁº∫‰πè‰ø°‰ªªÔºåÂæûËÄåÈòªÁ§ô‰∫ÜÂÖ∂Á©çÊ•µ‰ΩøÁî®„ÄÇÂèØËß£ÈáãÁöÑ‰∫∫Â∑•Êô∫ÊÖßË©¶ÂúñÈÄöÈÅéÊèê‰æõÂ∞çÊ±∫Á≠ñÈÅéÁ®ãÁöÑÊ¥ûÂØü‰æÜÂΩåË£úÈÄô‰∏ÄÂ∑ÆË∑ùÔºå‰ΩÜÂÖ∂‰∏çÂêåÊñπÊ≥ïÁöÑÂØ¶ÈöõÊïàÁî®Â∞ö‰∏çÊ∏ÖÊ•ö„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº‰ΩøÁî®ËÄÖÁ†îÁ©∂ÁöÑË©ï‰º∞ÔºåÂÖ∂‰∏≠ÂåÖÂê´‰∫Ü Grad-CAM Ëß£ÈáãÊñπÊ≥ïÔºå‰∏¶Â∞áÂÖ∂ÊáâÁî®ÊñºÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰ª•ÂàÜÈ°ûÊôÇÈñìÂ∫èÂàóÊñ∞ÁîüÂÖíÂëºÂê∏Êï∏Êìö‰∏≠ÁöÑÂëºÂê∏„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏çÂêåÂà©ÁõäÁõ∏ÈóúËÄÖÂ∞çÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑÊÑüÁü•ÊïàÁî®ÔºåÊè≠Á§∫‰∫ÜÂØ¶ÁèæÂØ¶ÈöõÈÄèÊòéÂ∫¶ÁöÑÈõ£Â∫¶Ôºå‰ª•ÂèäË®±Â§öÂèÉËàáËÄÖÂ∏åÊúõÁç≤ÂæóÊõ¥Ê∑±ÂÖ•ÁöÑËß£Èáã„ÄÇ

##### **XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare**
2405.06270v3 by Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio

The integration of Large Language Models (LLMs) into healthcare diagnostics
offers a promising avenue for clinical decision-making. This study outlines the
development of a novel method for zero-shot/few-shot in-context learning (ICL)
by integrating medical domain knowledge using a multi-layered structured
prompt. We also explore the efficacy of two communication styles between the
user and LLMs: the Numerical Conversational (NC) style, which processes data
incrementally, and the Natural Language Single-Turn (NL-ST) style, which
employs long narrative prompts.
  Our study systematically evaluates the diagnostic accuracy and risk factors,
including gender bias and false negative rates, using a dataset of 920 patient
records in various few-shot scenarios. Results indicate that traditional
clinical machine learning (ML) models generally outperform LLMs in zero-shot
and few-shot settings. However, the performance gap narrows significantly when
employing few-shot examples alongside effective explainable AI (XAI) methods as
sources of domain knowledge. Moreover, with sufficient time and an increased
number of examples, the conversational style (NC) nearly matches the
performance of ML models. Most notably, LLMs demonstrate comparable or superior
cost-sensitive accuracy relative to ML models.
  This research confirms that, with appropriate domain knowledge and tailored
communication strategies, LLMs can significantly enhance diagnostic processes.
The findings highlight the importance of optimizing the number of training
examples and communication styles to improve accuracy and reduce biases in LLM
applications.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÈÜ´ÁôÇË®∫Êñ∑Êï¥Âêà
ÁÇ∫Ëá®Â∫äÊ±∫Á≠ñÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊúâÂâçÊôØÁöÑÈÄîÂæë„ÄÇÊú¨Á†îÁ©∂Ê¶ÇËø∞‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÊñπÊ≥ïÁöÑÈñãÁôºÔºåÁî®ÊñºÈõ∂Ê¨°Â≠∏Áøí/Â∞ëÈáèÂ≠∏ÁøíÊÉÖÂ¢ÉÂ≠∏Áøí (ICL)ÔºåÊñπÊ≥ïÊòØ‰ΩøÁî®Â§öÂ±§ÁµêÊßãÂåñÊèêÁ§∫Êï¥ÂêàÈÜ´ÁôÇÈ†òÂüüÁü•Ë≠ò„ÄÇÊàëÂÄëÈÇÑÊé¢Ë®é‰∫Ü‰ΩøÁî®ËÄÖËàá LLM ‰πãÈñìÂÖ©Á®ÆÊ∫ùÈÄöÊñπÂºèÁöÑÂäüÊïàÔºöÊï∏ÂÄºÂ∞çË©± (NC) ÊñπÂºèÔºåÂÆÉÊúÉÈÄêÊ≠•ËôïÁêÜË≥áÊñôÔºå‰ª•ÂèäËá™ÁÑ∂Ë™ûË®ÄÂñÆÂõûÂêà (NL-ST) ÊñπÂºèÔºåÂÆÉÊúÉ‰ΩøÁî®Èï∑ÁØáÊïò‰∫ãÊèêÁ§∫„ÄÇ
ÊàëÂÄëÁöÑÁ†îÁ©∂Á≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫ÜË®∫Êñ∑Ê∫ñÁ¢∫ÊÄßÂíåÈ¢®Èö™Âõ†Â≠êÔºåÂåÖÊã¨ÊÄßÂà•ÂÅèË¶ãÂíåÂÅáÈô∞ÊÄßÁéáÔºå‰ΩøÁî®‰∫Ü‰∏ÄÂÄãÂåÖÂê´ 920 ÂÄãÊÇ£ËÄÖË®òÈåÑÁöÑË≥áÊñôÈõÜÔºåÊé°Áî®ÂêÑÁ®ÆÂ∞ëÈáèÂ≠∏ÁøíÊÉÖÂ¢É„ÄÇÁµêÊûúË°®ÊòéÔºåÂÇ≥Áµ±ÁöÑËá®Â∫äÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÈÄöÂ∏∏Âú®Èõ∂Ê¨°Â≠∏ÁøíÂíåÂ∞ëÈáèÂ≠∏ÁøíË®≠ÂÆö‰∏≠Ë°®ÁèæÂÑ™Êñº LLM„ÄÇÁÑ∂ËÄåÔºåÁï∂‰ΩøÁî®Â∞ëÈáèÂ≠∏ÁøíÁØÑ‰æã‰ª•ÂèäÊúâÊïàÁöÑÂèØËß£Èáã AI (XAI) ÊñπÊ≥ï‰ΩúÁÇ∫È†òÂüüÁü•Ë≠ò‰æÜÊ∫êÊôÇÔºåÊïàËÉΩÂ∑ÆË∑ùÊúÉÈ°ØËëóÁ∏ÆÂ∞è„ÄÇÊ≠§Â§ñÔºåÈö®ËëóÊôÇÈñìÂÖÖË∂≥ÂíåÁØÑ‰æãÊï∏ÈáèÂ¢ûÂä†ÔºåÂ∞çË©±ÊñπÂºè (NC) Âπæ‰πéÂèØ‰ª•Â™≤Áæé ML Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊúÄÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåLLM Áõ∏Â∞çÊñº ML Ê®°ÂûãÂ±ïÁèæÂá∫Áõ∏Áï∂ÊàñÊõ¥‰Ω≥ÁöÑÊàêÊú¨ÊïèÊÑüÊ∫ñÁ¢∫Â∫¶„ÄÇ
Êú¨Á†îÁ©∂Ë≠âÂØ¶ÔºåÈÄèÈÅéÈÅ©Áï∂ÁöÑÈ†òÂüüÁü•Ë≠òÂíåÈáèË∫´ÊâìÈÄ†ÁöÑÊ∫ùÈÄöÁ≠ñÁï•ÔºåLLM ÂèØ‰ª•È°ØËëóÂ¢ûÂº∑Ë®∫Êñ∑Á®ãÂ∫è„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÊúÄ‰Ω≥ÂåñË®ìÁ∑¥ÁØÑ‰æãÊï∏ÈáèÂíåÊ∫ùÈÄöÊñπÂºèÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÊèêÈ´òÊ∫ñÁ¢∫Â∫¶‰∏¶Ê∏õÂ∞ë LLM ÊáâÁî®‰∏≠ÁöÑÂÅèÂ∑Æ„ÄÇ

##### **To Trust or Not to Trust: Towards a novel approach to measure trust for XAI systems**
2405.05766v1 by Miquel Mir√≥-Nicolau, Gabriel Moy√†-Alcover, Antoni Jaume-i-Cap√≥, Manuel Gonz√°lez-Hidalgo, Maria Gemma Sempere Campello, Juan Antonio Palmer Sancho

The increasing reliance on Deep Learning models, combined with their inherent
lack of transparency, has spurred the development of a novel field of study
known as eXplainable AI (XAI) methods. These methods seek to enhance the trust
of end-users in automated systems by providing insights into the rationale
behind their decisions. This paper presents a novel approach for measuring user
trust in XAI systems, allowing their refinement. Our proposed metric combines
both performance metrics and trust indicators from an objective perspective. To
validate this novel methodology, we conducted a case study in a realistic
medical scenario: the usage of XAI system for the detection of pneumonia from
x-ray images.

ÊëòË¶ÅÔºöÈö®ËëóÂ∞çÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰æùË≥¥ÊÄßÁöÑÂ¢ûÂä†ÔºåÂä†‰∏äÂÖ∂Âõ∫ÊúâÁöÑÈÄèÊòéÂ∫¶‰∏çË∂≥Ôºå‰øÉ‰Ωø‰∏ÄÂÄãÊñ∞ÁöÑÁ†îÁ©∂È†òÂüüÁôºÂ±ïÔºåÁ®±ÁÇ∫ÂèØËß£Èáã AI (XAI) ÊñπÊ≥ï„ÄÇÈÄô‰∫õÊñπÊ≥ïÊó®Âú®ÈÄèÈÅéÊ∑±ÂÖ•‰∫ÜËß£Ê±∫Á≠ñËÉåÂæåÁöÑÂéüÁêÜÔºå‰æÜÊèêÂçáÊúÄÁµÇ‰ΩøÁî®ËÄÖÂ∞çËá™ÂãïÂåñÁ≥ªÁµ±ÁöÑ‰ø°Ë≥¥„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆË°°Èáè‰ΩøÁî®ËÄÖÂ∞ç XAI Á≥ªÁµ±‰ø°Ë≥¥Â∫¶ÁöÑÊñ∞Á©éÊñπÊ≥ïÔºåÂÖÅË®±Â∞çÂÖ∂ÈÄ≤Ë°åÊîπÈÄ≤„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊåáÊ®ôÁµêÂêà‰∫ÜÂÆ¢ËßÄËßÄÈªû‰∏ãÁöÑÊïàËÉΩÊåáÊ®ôÂíå‰ø°Ë≥¥ÊåáÊ®ô„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÈÄôÂÄãÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊàëÂÄëÂú®‰∏ÄÂÄãÁúüÂØ¶ÁöÑÈÜ´ÁôÇÂ†¥ÊôØ‰∏≠ÈÄ≤Ë°å‰∫Ü‰∏ÄÂÄãÊ°à‰æãÁ†îÁ©∂Ôºö‰ΩøÁî® XAI Á≥ªÁµ±Âæû X ÂÖâÂΩ±ÂÉè‰∏≠ÂÅµÊ∏¨ËÇ∫ÁÇé„ÄÇ

##### **Region-specific Risk Quantification for Interpretable Prognosis of COVID-19**
2405.02815v1 by Zhusi Zhong, Jie Li, Zhuoqi Ma, Scott Collins, Harrison Bai, Paul Zhang, Terrance Healey, Xinbo Gao, Michael K. Atalay, Zhicheng Jiao

The COVID-19 pandemic has strained global public health, necessitating
accurate diagnosis and intervention to control disease spread and reduce
mortality rates. This paper introduces an interpretable deep survival
prediction model designed specifically for improved understanding and trust in
COVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale
pretrained image encoder, Risk-specific Grad-CAM, and anatomical region
detection techniques, our approach produces regional interpretable outcomes
that effectively capture essential disease features while focusing on rare but
critical abnormal regions. Our model's predictive results provide enhanced
clarity and transparency through risk area localization, enabling clinicians to
make informed decisions regarding COVID-19 diagnosis with better understanding
of prognostic insights. We evaluate the proposed method on a multi-center
survival dataset and demonstrate its effectiveness via quantitative and
qualitative assessments, achieving superior C-indexes (0.764 and 0.727) and
time-dependent AUCs (0.799 and 0.691). These results suggest that our
explainable deep survival prediction model surpasses traditional survival
analysis methods in risk prediction, improving interpretability for clinical
decision making and enhancing AI system trustworthiness.

ÊëòË¶ÅÔºöCOVID-19 Áñ´ÊÉÖÂ∞çÂÖ®ÁêÉÂÖ¨ÂÖ±Ë°õÁîüÈÄ†ÊàêÂ£ìÂäõÔºåÂøÖÈ†àÈÄ≤Ë°åÊ∫ñÁ¢∫ÁöÑË®∫Êñ∑ÂíåÂπ≤È†êÔºå‰ª•ÊéßÂà∂ÁñæÁóÖÂÇ≥Êí≠‰∏¶Èôç‰ΩéÊ≠ª‰∫°Áéá„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊ∑±Â∫¶ÁîüÂ≠òÈ†êÊ∏¨Ê®°ÂûãÔºåÂ∞àÈñÄË®≠Ë®àÁî®ÊñºÈÄèÈÅéËÉ∏ÈÉ® X ÂÖâ (CXR) ÂΩ±ÂÉèÊîπÂñÑÂ∞ç COVID-19 È†êÂæåÁöÑÁêÜËß£Âíå‰ø°Ë≥¥„ÄÇÈÄèÈÅéÊï¥ÂêàÂ§ßË¶èÊ®°È†êË®ìÁ∑¥ÂΩ±ÂÉèÁ∑®Á¢ºÂô®„ÄÅÈ¢®Èö™ÁâπÂÆö Grad-CAM ÂíåËß£ÂâñÂçÄÂüüÂÅµÊ∏¨ÊäÄË°ìÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÁî¢ÁîüÂçÄÂüüÂèØËß£ÈáãÁöÑÁµêÊûúÔºåÊúâÊïàÊçïÊçâÂøÖË¶ÅÁöÑÁñæÁóÖÁâπÂæµÔºåÂêåÊôÇÂ∞àÊ≥®ÊñºÁΩïË¶ã‰ΩÜÈóúÈçµÁöÑÁï∞Â∏∏ÂçÄÂüü„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÈ†êÊ∏¨ÁµêÊûúÈÄèÈÅéÈ¢®Èö™ÂçÄÂüüÂÆö‰ΩçÊèê‰æõÂ¢ûÂº∑ÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÈÄèÊòéÂ∫¶ÔºåËÆìËá®Â∫äÈÜ´ÁîüËÉΩÂ§†Âú®Êõ¥‰∫ÜËß£È†êÂæåË¶ãËß£ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞± COVID-19 Ë®∫Êñ∑ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇÊàëÂÄëÂú®Â§ö‰∏≠ÂøÉÁîüÂ≠òË≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÔºå‰∏¶ÈÄèÈÅéÈáèÂåñÂíåË≥™ÂåñË©ï‰º∞Ë≠âÊòéÂÖ∂ÊúâÊïàÊÄßÔºåÈÅîÂà∞ÂÑ™Áï∞ÁöÑ C ÊåáÊï∏Ôºà0.764 Âíå 0.727ÔºâÂíåÊôÇÈñìÁõ∏Èóú AUCÔºà0.799 Âíå 0.691Ôºâ„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÂèØËß£ÈáãÁöÑÊ∑±Â∫¶ÁîüÂ≠òÈ†êÊ∏¨Ê®°ÂûãÂú®È¢®Èö™È†êÊ∏¨ÊñπÈù¢Ë∂ÖË∂äÂÇ≥Áµ±ÁöÑÁîüÂ≠òÂàÜÊûêÊñπÊ≥ïÔºåÊèêÂçáËá®Â∫äÊ±∫Á≠ñÁöÑËß£ÈáãÊÄßÔºå‰∏¶Â¢ûÂº∑ AI Á≥ªÁµ±ÁöÑ‰ø°Ë≥¥Â∫¶„ÄÇ

##### **Advancing Healthcare Automation: Multi-Agent Systems for Medical Necessity Justification**
2404.17977v1 by Himanshu Pandey, Akhil Amod, Shivang

This paper explores the application of Swarm-Structured Multi-Agent Systems
(MAS) to establish medical necessity, a process that involves a systematic
review of patient-specific medical structured and unstructured data against
clinical guidelines. We addressed this complex task by decomposing it into
smaller, more manageable sub-tasks. Each sub-task is handled by a specialized
AI agent. We conduct a systematic study of the impact of various prompting
strategies on these agents and benchmark different Large Language Models (LLMs)
to determine their accuracy in completing these tasks. Additionally, we
investigate how these agents can provide explainability, thereby enhancing
trust and transparency within the system.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫ÜÊáâÁî®Áæ§È´îÁµêÊßãÂ§ö‰∏ªÈ´îÁ≥ªÁµ± (MAS) ‰æÜÂª∫Á´ãÈÜ´ÁôÇÂøÖË¶ÅÊÄßÔºåÈÄôÊòØ‰∏ÄÂÄãÊ∂âÂèäÁ≥ªÁµ±ÊÄßÂØ©Êü•ÊÇ£ËÄÖÁâπÂÆöÈÜ´ÁôÇÁµêÊßãÂåñÂíåÈùûÁµêÊßãÂåñË≥áÊñôÂ∞çÁÖßËá®Â∫äÊåáÂºïÁöÑÈÅéÁ®ã„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áÊ≠§Ë§áÈõú‰ªªÂãôÂàÜËß£ÊàêËºÉÂ∞è‰∏îÊõ¥ÊòìÊñºÁÆ°ÁêÜÁöÑÂ≠ê‰ªªÂãô‰æÜËôïÁêÜ„ÄÇÊØèÂÄãÂ≠ê‰ªªÂãôÈÉΩÁî±‰∏ÄÂÄãÂ∞àÈñÄÁöÑ AI ‰∏ªÈ´îËôïÁêÜ„ÄÇÊàëÂÄëÂ∞çÂêÑÁ®ÆÊèêÁ§∫Á≠ñÁï•Â∞çÈÄô‰∫õ‰∏ªÈ´îÁöÑÂΩ±ÈüøÈÄ≤Ë°åÁ≥ªÁµ±ÊÄßÁ†îÁ©∂Ôºå‰∏¶Â∞ç‰∏çÂêåÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶Ôºå‰ª•Á¢∫ÂÆöÂÆÉÂÄëÂú®ÂÆåÊàêÈÄô‰∫õ‰ªªÂãôÊôÇÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁ†îÁ©∂ÈÄô‰∫õ‰∏ªÈ´îÂ¶Ç‰ΩïÊèê‰æõÂèØËß£ÈáãÊÄßÔºåÂæûËÄåÂ¢ûÂº∑Á≥ªÁµ±ÂÖßÁöÑ‰ø°‰ªªÂíåÈÄèÊòéÂ∫¶„ÄÇ

##### **Rad4XCNN: a new agnostic method for post-hoc global explanation of CNN-derived features by means of radiomics**
2405.02334v1 by Francesco Prinzi, Carmelo Militello, Calogero Zarcaro, Tommaso Vincenzo Bartolotta, Salvatore Gaglio, Salvatore Vitabile

In the last years, artificial intelligence (AI) in clinical decision support
systems (CDSS) played a key role in harnessing machine learning and deep
learning architectures. Despite their promising capabilities, the lack of
transparency and explainability of AI models poses significant challenges,
particularly in medical contexts where reliability is a mandatory aspect.
Achieving transparency without compromising predictive accuracy remains a key
challenge. This paper presents a novel method, namely Rad4XCNN, to enhance the
predictive power of CNN-derived features with the interpretability inherent in
radiomic features. Rad4XCNN diverges from conventional methods based on
saliency map, by associating intelligible meaning to CNN-derived features by
means of Radiomics, offering new perspectives on explanation methods beyond
visualization maps. Using a breast cancer classification task as a case study,
we evaluated Rad4XCNN on ultrasound imaging datasets, including an online
dataset and two in-house datasets for internal and external validation. Some
key results are: i) CNN-derived features guarantee more robust accuracy when
compared against ViT-derived and radiomic features; ii) conventional
visualization map methods for explanation present several pitfalls; iii)
Rad4XCNN does not sacrifice model accuracy for their explainability; iv)
Rad4XCNN provides global explanation insights enabling the physician to analyze
the model outputs and findings. In addition, we highlight the importance of
integrating interpretability into AI models for enhanced trust and adoption in
clinical practice, emphasizing how our method can mitigate some concerns
related to explainable AI methods.

ÊëòË¶ÅÔºö<paragraph>Âú®ÈÅéÂéªÂπæÂπ¥ÔºåËá®Â∫äÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ± (CDSS) ‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) Âú®Âà©Áî®Ê©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÊñπÈù¢ÁôºÊèÆ‰∫ÜÈóúÈçµ‰ΩúÁî®„ÄÇÂÑòÁÆ° AI Ê®°ÂûãÂÖ∑Êúâ‰ª§‰∫∫ÊªøÊÑèÁöÑËÉΩÂäõÔºå‰ΩÜÁº∫‰πèÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßÔºåÁâπÂà•ÊòØÂú®ÂèØÈù†ÊÄßÁÇ∫ÂøÖË¶ÅËÄÉÈáèÁöÑÈÜ´ÁôÇËÉåÊôØ‰∏ãÔºåÈÄôÂ∏∂‰æÜ‰∫ÜÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÂú®‰∏çÂΩ±ÈüøÈ†êÊ∏¨Á≤æÊ∫ñÂ∫¶ÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶ÁèæÈÄèÊòéÂ∫¶‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈóúÈçµÊåëÊà∞„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ïÔºåÂç≥ Rad4XCNNÔºå‰ª•Â¢ûÂº∑ CNN Ë°çÁîüÁâπÂæµÁöÑÈ†êÊ∏¨ËÉΩÂäõÔºåÂêåÊôÇÂÖ∑ÂÇôÊîæÂ∞ÑÁâπÂæµÂõ∫ÊúâÁöÑÂèØËß£ÈáãÊÄß„ÄÇRad4XCNN ‰∏çÂêåÊñºÂü∫ÊñºÈ°ØËëóÊÄßÂúñÁöÑÂÇ≥Áµ±ÊñπÊ≥ïÔºåÂÆÉÈÄöÈÅéÊîæÂ∞ÑÁµÑÂ≠∏Â∞áÂèØÁêÜËß£ÁöÑÂê´Áæ©Ëàá CNN Ë°çÁîüÁâπÂæµÈóúËÅØËµ∑‰æÜÔºåÁÇ∫Ë∂ÖË∂äË¶ñË¶∫ÂåñÂúñË°®ÁöÑËß£ÈáãÊñπÊ≥ïÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÄÈªû„ÄÇÊàëÂÄë‰ª•‰π≥ÁôåÂàÜÈ°û‰ªªÂãô‰ΩúÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåÂú®Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏äË©ï‰º∞ Rad4XCNNÔºåÂåÖÊã¨‰∏ÄÂÄãÁ∑ö‰∏äË≥áÊñôÈõÜÂíåÂÖ©ÂÄãÁî®ÊñºÂÖßÈÉ®ÂíåÂ§ñÈÉ®È©óË≠âÁöÑÂÖßÈÉ®Ë≥áÊñôÈõÜ„ÄÇ‰∏Ä‰∫õÈóúÈçµÁµêÊûúÂ¶Ç‰∏ãÔºöi) Ëàá ViT Ë°çÁîüÁâπÂæµÂíåÊîæÂ∞ÑÁâπÂæµÁõ∏ÊØîÔºåCNN Ë°çÁîüÁâπÂæµ‰øùË≠â‰∫ÜÊõ¥Á©©ÂÅ•ÁöÑÊ∫ñÁ¢∫Â∫¶Ôºõii) ÂÇ≥Áµ±ÁöÑË¶ñË¶∫ÂåñÂúñËß£ÈáãÊñπÊ≥ïÂ≠òÂú®‰∏Ä‰∫õÁº∫Èô∑Ôºõiii) Rad4XCNN Ê≤íÊúâÁäßÁâ≤Ê®°ÂûãÊ∫ñÁ¢∫Â∫¶‰æÜÊèõÂèñÂÖ∂ÂèØËß£ÈáãÊÄßÔºõiv) Rad4XCNN Êèê‰æõ‰∫ÜÂÖ®Â±ÄËß£ÈáãË¶ãËß£Ôºå‰ΩøÈÜ´Â∏´ËÉΩÂ§†ÂàÜÊûêÊ®°ÂûãËº∏Âá∫ÂíåÁôºÁèæ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂº∑Ë™øÂ∞áÂèØËß£ÈáãÊÄßÊï¥ÂêàÂà∞ AI Ê®°Âûã‰∏≠Â∞çÊñºÂ¢ûÂº∑Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑ‰ø°‰ªªÂíåÊé°Áî®Ëá≥ÈóúÈáçË¶ÅÔºå‰∏¶Âº∑Ë™ø‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂ¶Ç‰ΩïËÉΩÁ∑©Ëß£ËàáÂèØËß£Èáã AI ÊñπÊ≥ïÁõ∏ÈóúÁöÑ‰∏Ä‰∫õÁñëÊÖÆ„ÄÇ</paragraph>

##### **Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability**
2404.16957v1 by Yunfei Ge, Quanyan Zhu

The pervasive integration of Artificial Intelligence (AI) has introduced
complex challenges in the responsibility and accountability in the event of
incidents involving AI-enabled systems. The interconnectivity of these systems,
ethical concerns of AI-induced incidents, coupled with uncertainties in AI
technology and the absence of corresponding regulations, have made traditional
responsibility attribution challenging. To this end, this work proposes a
Computational Reflective Equilibrium (CRE) approach to establish a coherent and
ethically acceptable responsibility attribution framework for all stakeholders.
The computational approach provides a structured analysis that overcomes the
limitations of conceptual approaches in dealing with dynamic and multifaceted
scenarios, showcasing the framework's explainability, coherence, and adaptivity
properties in the responsibility attribution process. We examine the pivotal
role of the initial activation level associated with claims in equilibrium
computation. Using an AI-assisted medical decision-support system as a case
study, we illustrate how different initializations lead to diverse
responsibility distributions. The framework offers valuable insights into
accountability in AI-induced incidents, facilitating the development of a
sustainable and resilient system through continuous monitoring, revision, and
reflection.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÊôÆÂèäÊï¥ÂêàÔºåÂú®Ê∂âÂèä AI È©ÖÂãïÁ≥ªÁµ±ÁöÑ‰∫ãÊïÖ‰∏≠ÔºåË≤¨‰ªªÂíåÁæ©ÂãôÊ≠∏Â±¨Áî¢Áîü‰∫ÜË§áÈõúÁöÑÊåëÊà∞„ÄÇÈÄô‰∫õÁ≥ªÁµ±ÁöÑ‰∫íÈÄ£ÊÄß„ÄÅAI ÂºïÁôº‰∫ãÊïÖÁöÑÂÄ´ÁêÜÂïèÈ°åÔºåÂä†‰∏ä AI ÊäÄË°ìÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÂíåÁº∫‰πèÁõ∏ÊáâÊ≥ïË¶èÔºå‰ΩøÂæóÂÇ≥Áµ±Ë≤¨‰ªªÊ≠∏Â±¨Èù¢Ëá®ÊåëÊà∞„ÄÇÁÇ∫Ê≠§ÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆË®àÁÆóÂèçÊÄùÂùáË°° (CRE) ÊñπÊ≥ïÔºå‰ª•Âª∫Á´ã‰∏ÄÂÄãÈÄ£Ë≤´‰∏îÂú®ÂÄ´ÁêÜ‰∏äÂèØÊé•ÂèóÁöÑË≤¨‰ªªÊ≠∏Â±¨Êû∂ÊßãÔºåÈÅ©Áî®ÊñºÊâÄÊúâÂà©ÂÆ≥Èóú‰øÇ‰∫∫„ÄÇË®àÁÆóÊñπÊ≥ïÊèê‰æõ‰∫ÜÁµêÊßãÂåñÁöÑÂàÜÊûêÔºåÂÖãÊúç‰∫ÜÊ¶ÇÂøµÊñπÊ≥ïÂú®ËôïÁêÜÂãïÊÖã‰∏îÂ§öÈù¢ÂêëÊÉÖÂ¢ÉÊôÇÁöÑÈôêÂà∂ÔºåÂ±ïÁ§∫‰∫ÜË©≤Êû∂ÊßãÂú®Ë≤¨‰ªªÊ≠∏Â±¨ÈÅéÁ®ã‰∏≠ÂÖ∑ÂÇôÁöÑÂèØËß£ÈáãÊÄß„ÄÅÈÄ£Ë≤´ÊÄßÂíåÈÅ©ÊáâÊÄß„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜËàáÂùáË°°Ë®àÁÆó‰∏≠Á¥¢Ë≥†Áõ∏ÈóúÁöÑÂàùÂßãÂïüÂãïÂ±§Á¥öÁöÑÈóúÈçµ‰ΩúÁî®„ÄÇÊàëÂÄë‰ª• AI ËºîÂä©ÈÜ´ÁôÇÊ±∫Á≠ñÊîØÊè¥Á≥ªÁµ±ÁÇ∫Ê°à‰æãÁ†îÁ©∂ÔºåË™™Êòé‰∏çÂêåÁöÑÂàùÂßãÂåñÂ¶Ç‰ΩïÂ∞éËá¥‰∏çÂêåÁöÑË≤¨‰ªªÂàÜÈÖç„ÄÇË©≤Êû∂ÊßãÊèê‰æõ‰∫ÜÂ∞ç AI ÂºïÁôº‰∫ãÊïÖ‰∏≠ÂïèË≤¨Âà∂ÁöÑÂØ∂Ë≤¥Ë¶ãËß£ÔºåÈÄèÈÅéÊåÅÁ∫åÁõ£Êéß„ÄÅ‰øÆË®ÇÂíåÂèçÊÄùÔºå‰øÉÈÄ≤‰∫ÜÊ∞∏Á∫å‰∏îÊúâÈüåÊÄßÁöÑÁ≥ªÁµ±ÁôºÂ±ï„ÄÇ

##### **Explainable AI for Fair Sepsis Mortality Predictive Model**
2404.13139v1 by Chia-Hsuan Chang, Xiaoyang Wang, Christopher C. Yang

Artificial intelligence supports healthcare professionals with predictive
modeling, greatly transforming clinical decision-making. This study addresses
the crucial need for fairness and explainability in AI applications within
healthcare to ensure equitable outcomes across diverse patient demographics. By
focusing on the predictive modeling of sepsis-related mortality, we propose a
method that learns a performance-optimized predictive model and then employs
the transfer learning process to produce a model with better fairness. Our
method also introduces a novel permutation-based feature importance algorithm
aiming at elucidating the contribution of each feature in enhancing fairness on
predictions. Unlike existing explainability methods concentrating on explaining
feature contribution to predictive performance, our proposed method uniquely
bridges the gap in understanding how each feature contributes to fairness. This
advancement is pivotal, given sepsis's significant mortality rate and its role
in one-third of hospital deaths. Our method not only aids in identifying and
mitigating biases within the predictive model but also fosters trust among
healthcare stakeholders by improving the transparency and fairness of model
predictions, thereby contributing to more equitable and trustworthy healthcare
delivery.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÈÄèÈÅéÈ†êÊ∏¨Ê®°ÂûãÂçîÂä©ÈÜ´ÁôÇÂ∞àÊ•≠‰∫∫Âì°ÔºåÂ§ßÂπÖËΩâËÆä‰∫ÜËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠‰ΩøÁî®‰∫∫Â∑•Êô∫ÊÖßÊáâÁî®Á®ãÂºèÊôÇÂÖ¨Âπ≥ÊÄßÂíåÂèØËß£ÈáãÊÄßÁöÑÈóúÈçµÈúÄÊ±ÇÔºå‰ª•Á¢∫‰øùÂú®‰∏çÂêåÁöÑÊÇ£ËÄÖ‰∫∫Âè£Áµ±Ë®àË≥áÊñô‰∏≠Áç≤ÂæóÂÖ¨Âπ≥ÁöÑÁµêÊûú„ÄÇÈÄèÈÅéÂ∞àÊ≥®ÊñºÊïóË°ÄÁóáÁõ∏ÈóúÊ≠ª‰∫°ÁéáÁöÑÈ†êÊ∏¨Ê®°ÂûãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÊúÉÂ≠∏Áøí‰∏ÄÂÄãÊïàËÉΩÊúÄ‰Ω≥ÂåñÁöÑÈ†êÊ∏¨Ê®°ÂûãÔºåÁÑ∂ÂæåÊé°Áî®ËΩâÁßªÂ≠∏ÁøíÈÅéÁ®ã‰æÜÁî¢Áîü‰∏ÄÂÄãÂÖ∑ÊúâÊõ¥Â•ΩÂÖ¨Âπ≥ÊÄßÁöÑÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÈÇÑÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂü∫ÊñºÊéíÂàóÁöÑÁâπÂæµÈáçË¶ÅÊÄßÊºîÁÆóÊ≥ïÔºåÊó®Âú®Èó°ÊòéÊØèÂÄãÁâπÂæµÂú®Â¢ûÂº∑È†êÊ∏¨ÂÖ¨Âπ≥ÊÄßÊñπÈù¢ÁöÑË≤¢Áçª„ÄÇËàáÁèæÊúâÁöÑÂèØËß£ÈáãÊÄßÊñπÊ≥ïÂ∞àÊ≥®ÊñºËß£ÈáãÁâπÂæµÂ∞çÈ†êÊ∏¨ÊïàËÉΩÁöÑË≤¢Áçª‰∏çÂêåÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁç®ÁâπÂú∞ÂΩåË£ú‰∫ÜÁêÜËß£ÊØèÂÄãÁâπÂæµÂ¶Ç‰ΩïÊúâÂä©ÊñºÂÖ¨Âπ≥ÊÄßÁöÑÂ∑ÆË∑ù„ÄÇÈÄôÈ†ÖÈÄ≤Â±ïËá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÊïóË°ÄÁóáÁöÑÊ≠ª‰∫°ÁéáÂæàÈ´òÔºå‰∏îÂú®‰∏âÂàÜ‰πã‰∏ÄÁöÑÈÜ´Èô¢Ê≠ª‰∫°‰∏≠ÊâÆÊºîËëóËßíËâ≤„ÄÇÊàëÂÄëÁöÑÊ®°Âûã‰∏çÂÉÖÊúâÂä©ÊñºË≠òÂà•ÂíåÊ∏õËºïÈ†êÊ∏¨Ê®°Âûã‰∏≠ÁöÑÂÅèÂ∑ÆÔºåÈÇÑËÉΩÈÄèÈÅéÊèêÈ´òÊ®°ÂûãÈ†êÊ∏¨ÁöÑÈÄèÊòéÂ∫¶ÂíåÂÖ¨Âπ≥ÊÄß‰æÜÂüπÈ§äÈÜ´ÁôÇ‰øùÂÅ•Âà©ÁõäÁõ∏ÈóúËÄÖ‰πãÈñìÁöÑ‰ø°‰ªªÔºåÈÄ≤ËÄåÊúâÂä©ÊñºÊèê‰æõÊõ¥ÂÖ¨Âπ≥‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊúçÂãô„ÄÇ

##### **Multi Class Depression Detection Through Tweets using Artificial Intelligence**
2404.13104v1 by Muhammad Osama Nusrat, Waseem Shahzad, Saad Ahmed Jamal

Depression is a significant issue nowadays. As per the World Health
Organization (WHO), in 2023, over 280 million individuals are grappling with
depression. This is a huge number; if not taken seriously, these numbers will
increase rapidly. About 4.89 billion individuals are social media users. People
express their feelings and emotions on platforms like Twitter, Facebook,
Reddit, Instagram, etc. These platforms contain valuable information which can
be used for research purposes. Considerable research has been conducted across
various social media platforms. However, certain limitations persist in these
endeavors. Particularly, previous studies were only focused on detecting
depression and the intensity of depression in tweets. Also, there existed
inaccuracies in dataset labeling. In this research work, five types of
depression (Bipolar, major, psychotic, atypical, and postpartum) were predicted
using tweets from the Twitter database based on lexicon labeling. Explainable
AI was used to provide reasoning by highlighting the parts of tweets that
represent type of depression. Bidirectional Encoder Representations from
Transformers (BERT) was used for feature extraction and training. Machine
learning and deep learning methodologies were used to train the model. The BERT
model presented the most promising results, achieving an overall accuracy of
0.96.

ÊëòË¶ÅÔºöÁèæ‰ªäÔºåÊÜÇÈ¨±ÁóáÊòØ‰∏ÄÂÄãÈáçË¶ÅÁöÑË≠∞È°å„ÄÇÊ†πÊìö‰∏ñÁïåË°õÁîüÁµÑÁπî (WHO) ÁöÑË≥áÊñôÔºåÂú® 2023 Âπ¥ÔºåË∂ÖÈÅé 2.8 ÂÑÑ‰∫∫Ê≠£Âú®ËàáÊÜÇÈ¨±ÁóáÊêèÈ¨•„ÄÇÈÄôÊòØ‰∏ÄÂÄãÈæêÂ§ßÁöÑÊï∏Â≠óÔºõÂ¶ÇÊûú‰∏çË™çÁúüÁúãÂæÖÔºåÈÄô‰∫õÊï∏Â≠óÂ∞áÊúÉÂø´ÈÄüÂ¢ûÂä†„ÄÇÂ§ßÁ¥ÑÊúâ 48.9 ÂÑÑ‰∫∫ÊòØÁ§æÁæ§Â™íÈ´î‰ΩøÁî®ËÄÖ„ÄÇ‰∫∫ÂÄëÂú® Twitter„ÄÅFacebook„ÄÅReddit„ÄÅInstagram Á≠âÂπ≥Âè∞‰∏äË°®ÈÅîËá™Â∑±ÁöÑÊÑüÂèóÂíåÊÉÖÁ∑í„ÄÇÈÄô‰∫õÂπ≥Âè∞ÂåÖÂê´ÊúâÂÉπÂÄºÁöÑË≥áË®äÔºåÂèØÁî®ÊñºÁ†îÁ©∂ÁõÆÁöÑ„ÄÇÂ∑≤Á∂ìÂú®ÂêÑÁ®ÆÁ§æÁæ§Â™íÈ´îÂπ≥Âè∞‰∏äÈÄ≤Ë°å‰∫ÜÂ§ßÈáèÁöÑÁ†îÁ©∂„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÂä™Âäõ‰ªçÂ≠òÂú®Êüê‰∫õÈôêÂà∂„ÄÇÁâπÂà•ÊòØÔºåÂÖàÂâçÁöÑÁ†îÁ©∂ÂÉÖÂ∞àÊ≥®ÊñºÂÅµÊ∏¨Êé®Êñá‰∏≠ÁöÑÊÜÇÈ¨±ÁóáÂíåÊÜÇÈ¨±ÁóáÁöÑÂº∑Â∫¶„ÄÇÊ≠§Â§ñÔºåË≥áÊñôÈõÜÊ®ôÁ±§‰∏≠Â≠òÂú®‰∏çÊ∫ñÁ¢∫ÁöÑÊÉÖÊ≥Å„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂Â∑•‰Ωú‰∏≠Ôºå‰ΩøÁî®Âü∫ÊñºË©ûÂΩôÊ®ôÁ±§ÁöÑ Twitter Ë≥áÊñôÂ∫´‰∏≠ÁöÑÊé®ÊñáÈ†êÊ∏¨‰∫Ü‰∫îÁ®ÆÈ°ûÂûãÁöÑÊÜÇÈ¨±ÁóáÔºàÈõôÊ•µÂûã„ÄÅÈáçÂ∫¶„ÄÅÁ≤æÁ•ûÁóÖÂûã„ÄÅÈùûÂÖ∏ÂûãÂíåÁî¢ÂæåÔºâ„ÄÇÂèØËß£ÈáãÁöÑ AI Áî®ÊñºÈÄèÈÅéÂº∑Ë™ø‰ª£Ë°®ÊÜÇÈ¨±ÁóáÈ°ûÂûãÁöÑÊé®ÊñáÈÉ®ÂàÜ‰æÜÊèê‰æõÊé®ÁêÜ„ÄÇÂæû TransformersÔºàBERTÔºâ‰∏≠ÊèêÂèñÁöÑÈõôÂêëÁ∑®Á¢ºÂô®Ë°®Á§∫Áî®ÊñºÁâπÂæµÊèêÂèñÂíåË®ìÁ∑¥„ÄÇÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÁî®ÊñºË®ìÁ∑¥Ê®°Âûã„ÄÇBERT Ê®°ÂûãÂëàÁèæÂá∫ÊúÄÊúâÂ∏åÊúõÁöÑÁµêÊûúÔºåÈÅîÂà∞ 0.96 ÁöÑÊï¥È´îÊ∫ñÁ¢∫Â∫¶„ÄÇ

##### **COIN: Counterfactual inpainting for weakly supervised semantic segmentation for medical images**
2404.12832v1 by Dmytro Shvetsov, Joonas Ariva, Marharyta Domnich, Raul Vicente, Dmytro Fishman

Deep learning is dramatically transforming the field of medical imaging and
radiology, enabling the identification of pathologies in medical images,
including computed tomography (CT) and X-ray scans. However, the performance of
deep learning models, particularly in segmentation tasks, is often limited by
the need for extensive annotated datasets. To address this challenge, the
capabilities of weakly supervised semantic segmentation are explored through
the lens of Explainable AI and the generation of counterfactual explanations.
The scope of this research is development of a novel counterfactual inpainting
approach (COIN) that flips the predicted classification label from abnormal to
normal by using a generative model. For instance, if the classifier deems an
input medical image X as abnormal, indicating the presence of a pathology, the
generative model aims to inpaint the abnormal region, thus reversing the
classifier's original prediction label. The approach enables us to produce
precise segmentations for pathologies without depending on pre-existing
segmentation masks. Crucially, image-level labels are utilized, which are
substantially easier to acquire than creating detailed segmentation masks. The
effectiveness of the method is demonstrated by segmenting synthetic targets and
actual kidney tumors from CT images acquired from Tartu University Hospital in
Estonia. The findings indicate that COIN greatly surpasses established
attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an
alternative counterfactual explanation method introduced by Singla et al. This
evidence suggests that COIN is a promising approach for semantic segmentation
of tumors in CT images, and presents a step forward in making deep learning
applications more accessible and effective in healthcare, where annotated data
is scarce.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÊ≠£ÂäáÁÉàÂú∞ËΩâËÆäÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÊîæÂ∞ÑÂ≠∏È†òÂüüÔºåËÉΩË≠òÂà•ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÁóÖÁêÜÔºåÂåÖÊã¨ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèè (CT) Âíå X ÂÖâÊéÉÊèè„ÄÇÁÑ∂ËÄåÔºåÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®ÂàÜÂâ≤‰ªªÂãô‰∏≠ÔºåÂ∏∏Â∏∏ÂèóÂà∞Âª£Ê≥õË®ªËß£Ë≥áÊñôÈõÜÈúÄÊ±ÇÁöÑÈôêÂà∂„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÊ≠§ÊåëÊà∞ÔºåÈÄèÈÅéÂèØËß£Èáã AI ÂíåÂèç‰∫ãÂØ¶Ëß£ÈáãÁöÑÁî¢ÁîüÔºåÊé¢Ë®é‰∫ÜÂº±Áõ£Áù£Ë™ûÊÑèÂàÜÂâ≤ÁöÑËÉΩÂäõ„ÄÇÊú¨Á†îÁ©∂ÁöÑÁØÑÂúçÊòØÈñãÁôº‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂèç‰∫ãÂØ¶ÂÖßÁπ™ÊñπÊ≥ï (COIN)ÔºåÂÆÉÈÄèÈÅé‰ΩøÁî®ÁîüÊàêÊ®°ÂûãÔºåÂ∞áÈ†êÊ∏¨ÂàÜÈ°ûÊ®ôÁ±§ÂæûÁï∞Â∏∏ÁøªËΩâÁÇ∫Ê≠£Â∏∏„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÂàÜÈ°ûÂô®Â∞áËº∏ÂÖ•ÈÜ´Â≠∏ÂΩ±ÂÉè X Ë¶ñÁÇ∫Áï∞Â∏∏ÔºåË°®Á§∫Â≠òÂú®ÁóÖÁêÜÔºåÁîüÊàêÊ®°ÂûãÊó®Âú®ÂÖßÁπ™Áï∞Â∏∏ÂçÄÂüüÔºåÂæûËÄåÈÄÜËΩâÂàÜÈ°ûÂô®ÁöÑÂéüÂßãÈ†êÊ∏¨Ê®ôÁ±§„ÄÇÊ≠§ÊñπÊ≥ï‰ΩøÊàëÂÄëËÉΩÂ§†Áî¢ÁîüÁóÖÁêÜÁöÑÁ≤æÁ¢∫ÂàÜÂâ≤ÔºåËÄå‰∏ç‰æùË≥¥ÊñºÈ†êÂÖàÂ≠òÂú®ÁöÑÂàÜÂâ≤ÈÅÆÁΩ©„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåÂà©Áî®ÂΩ±ÂÉèÂ±§Á¥öÊ®ôÁ±§ÔºåÈÄôÊØîÂª∫Á´ãË©≥Á¥∞ÁöÑÂàÜÂâ≤ÈÅÆÁΩ©ÂÆπÊòìÂèñÂæóÂæóÂ§ö„ÄÇË©≤ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÈÄèÈÅéÂàÜÂâ≤ÂêàÊàêÁõÆÊ®ôÂíåÂæûÊÑõÊ≤ôÂ∞º‰∫ûÂ°îÁàæÂúñÂ§ßÂ≠∏ÈÜ´Èô¢ÂèñÂæóÁöÑ CT ÂΩ±ÂÉè‰∏≠ÁöÑÂØ¶ÈöõËÖéËáüËÖ´Áò§‰æÜË≠âÊòé„ÄÇÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåCOIN ÈÅ†ÈÅ†Ë∂ÖË∂ä‰∫ÜÊó¢ÂÆöÁöÑÊ≠∏Âõ†ÊñπÊ≥ïÔºå‰æãÂ¶Ç RISE„ÄÅScoreCAM Âíå LayerCAMÔºå‰ª•Âèä Singla Á≠â‰∫∫ÊèêÂá∫ÁöÑÂè¶‰∏ÄÁ®ÆÂèç‰∫ãÂØ¶Ëß£ÈáãÊñπÊ≥ï„ÄÇÊ≠§Ë≠âÊìöË°®ÊòéÔºåCOIN ÊòØ‰∏ÄÁ®ÆÂæàÊúâÂâçÊôØÁöÑ CT ÂΩ±ÂÉè‰∏≠ËÖ´Áò§Ë™ûÊÑèÂàÜÂâ≤ÊñπÊ≥ïÔºå‰∏¶Âú®‰ΩøÊ∑±Â∫¶Â≠∏ÁøíÊáâÁî®Âú®Ë®ªËß£Ë≥áÊñôÁ®ÄÁº∫ÁöÑÈÜ´ÁôÇ‰øùÂÅ•‰∏≠Êõ¥ÊòìÊñºÂèñÂæóÂíåÊõ¥ÊúâÊïàÊñπÈù¢ÈÇÅÈÄ≤‰∫Ü‰∏ÄÊ≠•„ÄÇ

##### **Hybrid Intelligence for Digital Humanities**
2406.15374v1 by Victor de Boer, Lise Stork

In this paper, we explore the synergies between Digital Humanities (DH) as a
discipline and Hybrid Intelligence (HI) as a research paradigm. In DH research,
the use of digital methods and specifically that of Artificial Intelligence is
subject to a set of requirements and constraints. We argue that these are
well-supported by the capabilities and goals of HI. Our contribution includes
the identification of five such DH requirements: Successful AI systems need to
be able to 1) collaborate with the (human) scholar; 2) support data criticism;
3) support tool criticism; 4) be aware of and cater to various perspectives and
5) support distant and close reading. We take the CARE principles of Hybrid
Intelligence (collaborative, adaptive, responsible and explainable) as
theoretical framework and map these to the DH requirements. In this mapping, we
include example research projects. We finally address how insights from DH can
be applied to HI and discuss open challenges for the combination of the two
disciplines.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÊï∏‰Ωç‰∫∫ÊñáÂ≠∏Áßë (DH) ‰ΩúÁÇ∫‰∏ÄÈñÄÂ≠∏ÁßëËàáÊ∑∑ÂêàÊô∫ËÉΩ (HI) ‰ΩúÁÇ∫‰∏ÄÂÄãÁ†îÁ©∂ÂÖ∏ÁØÑ‰πãÈñìÁöÑÂçîÂêå‰ΩúÁî®„ÄÇÂú® DH Á†îÁ©∂‰∏≠ÔºåÊï∏‰ΩçÊñπÊ≥ïÁöÑ‰ΩøÁî®ÔºåÁâπÂà•ÊòØ‰∫∫Â∑•Êô∫ÊÖßÁöÑ‰ΩøÁî®ÔºåÂèóÂà∞‰∏ÄÁ≥ªÂàóË¶ÅÊ±ÇÂíåÈôêÂà∂„ÄÇÊàëÂÄëË™çÁÇ∫ÈÄô‰∫õË¶ÅÊ±ÇÂíåÈôêÂà∂Áç≤Âæó HI ÁöÑËÉΩÂäõÂíåÁõÆÊ®ôÁöÑÂÖÖÂàÜÊîØÊåÅ„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÂåÖÊã¨ÊâæÂá∫‰∫îÂÄãÈÄôÊ®£ÁöÑ DH Ë¶ÅÊ±ÇÔºöÊàêÂäüÁöÑ AI Á≥ªÁµ±ÈúÄË¶ÅËÉΩÂ§† 1) ËàáÔºà‰∫∫È°ûÔºâÂ≠∏ËÄÖÂêà‰ΩúÔºõ2) ÊîØÊè¥Ë≥áÊñôÊâπË©ïÔºõ3) ÊîØÊè¥Â∑•ÂÖ∑ÊâπË©ïÔºõ4) ÂØüË¶∫‰∏¶ËøéÂêàÂêÑÁ®ÆËßÄÈªûÔºõ5) ÊîØÊè¥ÈÅ†Ë∑ùÂíåËøëË∑ùÈõ¢Èñ±ËÆÄ„ÄÇÊàëÂÄëÂ∞áÊ∑∑ÂêàÊô∫ËÉΩÁöÑ CARE ÂéüÂâáÔºàÂçî‰Ωú„ÄÅÈÅ©Êáâ„ÄÅË≤†Ë≤¨ÂíåÂèØËß£ÈáãÔºâ‰ΩúÁÇ∫ÁêÜË´ñÊû∂ÊßãÔºå‰∏¶Â∞áÈÄô‰∫õÂéüÂâáÂ∞çÊáâÂà∞ DH Ë¶ÅÊ±Ç„ÄÇÂú®Ê≠§Â∞çÊáâ‰∏≠ÔºåÊàëÂÄëÁ¥çÂÖ•ÁØÑ‰æãÁ†îÁ©∂Â∞àÊ°à„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Ë®éÂ¶Ç‰ΩïÂ∞á DH ÁöÑË¶ãËß£ÊáâÁî®Êñº HIÔºå‰∏¶Ë®éË´ñÁµêÂêàÈÄôÂÖ©ÂÄãÂ≠∏ÁßëÁöÑÈñãÊîæÊåëÊà∞„ÄÇ

##### **Ethical Framework for Responsible Foundational Models in Medical Imaging**
2406.11868v1 by Abhijit Das, Debesh Jha, Jasmer Sanjotra, Onkar Susladkar, Suramyaa Sarkar, Ashish Rauniyar, Nikhil Tomar, Vanshali Sharma, Ulas Bagci

Foundational models (FMs) have tremendous potential to revolutionize medical
imaging. However, their deployment in real-world clinical settings demands
extensive ethical considerations. This paper aims to highlight the ethical
concerns related to FMs and propose a framework to guide their responsible
development and implementation within medicine. We meticulously examine ethical
issues such as privacy of patient data, bias mitigation, algorithmic
transparency, explainability and accountability. The proposed framework is
designed to prioritize patient welfare, mitigate potential risks, and foster
trust in AI-assisted healthcare.

ÊëòË¶ÅÔºöÂü∫Á§éÊ®°Âûã (FM) ÂÖ∑ÊúâÂæπÂ∫ïÊîπËÆäÈÜ´Â≠∏ÂΩ±ÂÉèÁöÑÂ∑®Â§ßÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®ÁèæÂØ¶‰∏ñÁïåËá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÈÉ®ÁΩ≤ÈúÄË¶ÅÂª£Ê≥õÁöÑÂÄ´ÁêÜËÄÉÈáè„ÄÇÊú¨ÊñáÊó®Âú®Âº∑Ë™øËàá FM Áõ∏ÈóúÁöÑÂÄ´ÁêÜÂïèÈ°åÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊ°ÜÊû∂‰æÜÊåáÂ∞éÂÆÉÂÄëÂú®ÈÜ´Â≠∏‰∏≠ÁöÑË≤†Ë≤¨‰ªªÈñãÁôºÂíåÂØ¶ÊñΩ„ÄÇÊàëÂÄë‰ªîÁ¥∞ÂØ©Êü•‰∫ÜÂÄ´ÁêÜÂïèÈ°åÔºå‰æãÂ¶ÇÊÇ£ËÄÖÊï∏ÊìöÈö±ÁßÅ„ÄÅÂÅèÂ∑ÆÁ∑©Ëß£„ÄÅÊºîÁÆóÊ≥ïÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíåÂïèË≤¨Âà∂„ÄÇÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂Êó®Âú®ÂÑ™ÂÖàËÄÉÊÖÆÊÇ£ËÄÖÁ¶èÂà©„ÄÅÊ∏õËºïÊΩõÂú®È¢®Èö™Ôºå‰∏¶ÂüπÈ§äÂ∞ç AI ËºîÂä©ÈÜ´ÁôÇ‰øùÂÅ•ÁöÑ‰ø°‰ªª„ÄÇ

##### **Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis**
2404.07239v1 by Milad Yousefi, Shadi Farabi Maleki, Ali Jafarizadeh, Mahya Ahmadpour Youshanlui, Aida Jafari, Siamak Pedrammehr, Roohallah Alizadehsani, Ryszard Tadeusiewicz, Pawel Plawiak

Thyroid cancer is an increasing global health concern that requires advanced
diagnostic methods. The application of AI and radiomics to thyroid cancer
diagnosis is examined in this review. A review of multiple databases was
conducted in compliance with PRISMA guidelines until October 2023. A
combination of keywords led to the discovery of an English academic publication
on thyroid cancer and related subjects. 267 papers were returned from the
original search after 109 duplicates were removed. Relevant studies were
selected according to predetermined criteria after 124 articles were eliminated
based on an examination of their abstract and title. After the comprehensive
analysis, an additional six studies were excluded. Among the 28 included
studies, radiomics analysis, which incorporates ultrasound (US) images,
demonstrated its effectiveness in diagnosing thyroid cancer. Various results
were noted, some of the studies presenting new strategies that outperformed the
status quo. The literature has emphasized various challenges faced by AI
models, including interpretability issues, dataset constraints, and operator
dependence. The synthesized findings of the 28 included studies mentioned the
need for standardization efforts and prospective multicenter studies to address
these concerns. Furthermore, approaches to overcome these obstacles were
identified, such as advances in explainable AI technology and personalized
medicine techniques. The review focuses on how AI and radiomics could transform
the diagnosis and treatment of thyroid cancer. Despite challenges, future
research on multidisciplinary cooperation, clinical applicability validation,
and algorithm improvement holds the potential to improve patient outcomes and
diagnostic precision in the treatment of thyroid cancer.

ÊëòË¶ÅÔºöÁî≤ÁãÄËÖ∫ÁôåÊòØ‰∏ÄÁ®ÆÊó•ÁõäÂö¥ÈáçÁöÑÂÖ®ÁêÉÂÅ•Â∫∑ÂïèÈ°åÔºåÈúÄË¶ÅÂÖàÈÄ≤ÁöÑË®∫Êñ∑ÊñπÊ≥ï„ÄÇÊú¨ÁØáË©ïË´ñÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ËÉΩËàáÊîæÂ∞ÑÁâπÂæµÂàÜÊûêÂú®Áî≤ÁãÄËÖ∫ÁôåË®∫Êñ∑‰∏≠ÁöÑÊáâÁî®„ÄÇÂú®Á¨¶Âêà PRISMA ÊåáÂçóÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂ∞çÂ§öÂÄãË≥áÊñôÂ∫´ÈÄ≤Ë°å‰∫ÜÂõûÈ°ßÔºåÁõ¥Âà∞ 2023 Âπ¥ 10 Êúà„ÄÇÈÄöÈÅéÁµêÂêàÈóúÈçµÂ≠óÔºåÁôºÁèæ‰∫Ü‰∏ÄÁØáÈóúÊñºÁî≤ÁãÄËÖ∫ÁôåÂíåÁõ∏Èóú‰∏ªÈ°åÁöÑËã±ÊñáÂ≠∏Ë°ìÂá∫ÁâàÁâ©„ÄÇÂú®ÁßªÈô§ 109 ÁØáÈáçË§áÊñáÁçªÂæåÔºåÂéüÂßãÊêúÂ∞ãÂÖ±ÂõûÂÇ≥ 267 ÁØáË´ñÊñá„ÄÇÂú®Ê†πÊìöÈ†êÂÖàÁ¢∫ÂÆöÁöÑÊ®ôÊ∫ñÔºåÊ∑òÊ±∞‰∫Ü 124 ÁØáÊñáÁ´†ÁöÑÊëòË¶ÅÂíåÊ®ôÈ°åÂæåÔºåÈÅ∏Âá∫‰∫ÜÁõ∏ÈóúÁ†îÁ©∂„ÄÇÂú®ÈÄ≤Ë°åÂÖ®Èù¢ÂàÜÊûêÂæåÔºåÈ°çÂ§ñÊéíÈô§‰∫ÜÂÖ≠È†ÖÁ†îÁ©∂„ÄÇÂú®Á¥çÂÖ•ÁöÑ 28 È†ÖÁ†îÁ©∂‰∏≠ÔºåÁµêÂêàË∂ÖÈü≥Ê≥¢ (US) ÂΩ±ÂÉèÁöÑÊîæÂ∞ÑÁâπÂæµÂàÜÊûêÔºåË≠âÊòé‰∫ÜÂÖ∂Âú®Ë®∫Êñ∑Áî≤ÁãÄËÖ∫ÁôåÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÁ†îÁ©∂ÁµêÊûú‰∏ç‰∏ÄÔºåÊúâ‰∫õÁ†îÁ©∂ÊèêÂá∫‰∫ÜÂÑ™ÊñºÁèæÁãÄÁöÑÊñ∞Á≠ñÁï•„ÄÇÊñáÁçªÂº∑Ë™ø‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÈù¢Ëá®ÁöÑÂêÑÁ®ÆÊåëÊà∞ÔºåÂåÖÊã¨ÂèØËß£ÈáãÊÄßÂïèÈ°å„ÄÅË≥áÊñôÈõÜÈôêÂà∂ÂíåÊìç‰ΩúÂì°‰æùË≥¥ÊÄß„ÄÇ28 È†ÖÁ¥çÂÖ•Á†îÁ©∂ÁöÑÁ∂úÂêàÁôºÁèæÊèêÂà∞ÔºåÈúÄË¶ÅÊ®ôÊ∫ñÂåñÂ∑•‰ΩúÂíåÂâçÁûªÊÄßÂ§ö‰∏≠ÂøÉÁ†îÁ©∂‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÊ≠§Â§ñÔºåÈÇÑÁ¢∫ÂÆö‰∫ÜÂÖãÊúçÈÄô‰∫õÈöúÁ§ôÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇÂèØËß£Èáã‰∫∫Â∑•Êô∫ËÉΩÊäÄË°ìÂíåÂÄã‰∫∫ÂåñÈÜ´ÁôÇÊäÄË°ìÁöÑÈÄ≤Ê≠•„ÄÇÊú¨ÁØáË©ïË´ñÈáçÈªûÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÂíåÊîæÂ∞ÑÁâπÂæµÂàÜÊûêÂ¶Ç‰ΩïËΩâËÆäÁî≤ÁãÄËÖ∫ÁôåÁöÑË®∫Êñ∑ÂíåÊ≤ªÁôÇ„ÄÇÂÑòÁÆ°Â≠òÂú®ÊåëÊà∞Ôºå‰ΩÜÊú™‰æÜÂ∞çÂ§öÂ≠∏ÁßëÂêà‰Ωú„ÄÅËá®Â∫äÈÅ©Áî®ÊÄßÈ©óË≠âÂíåÊºîÁÆóÊ≥ïÊîπÈÄ≤ÁöÑÁ†îÁ©∂Ôºå‰ªçÊúâÊΩõÂäõÊîπÂñÑÁî≤ÁãÄËÖ∫ÁôåÊ≤ªÁôÇ‰∏≠ÁöÑÊÇ£ËÄÖÈ†êÂæåÂíåË®∫Êñ∑Á≤æÊ∫ñÂ∫¶„ÄÇ

##### **Predictive Modeling for Breast Cancer Classification in the Context of Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable AI**
2404.04686v1 by Taminul Islam, Md. Alif Sheakh, Mst. Sazia Tahosin, Most. Hasna Hena, Shopnil Akash, Yousef A. Bin Jardan, Gezahign Fentahun Wondmie, Hiba-Allah Nafidi, Mohammed Bourhia

Breast cancer has rapidly increased in prevalence in recent years, making it
one of the leading causes of mortality worldwide. Among all cancers, it is by
far the most common. Diagnosing this illness manually requires significant time
and expertise. Since detecting breast cancer is a time-consuming process,
preventing its further spread can be aided by creating machine-based forecasts.
Machine learning and Explainable AI are crucial in classification as they not
only provide accurate predictions but also offer insights into how the model
arrives at its decisions, aiding in the understanding and trustworthiness of
the classification results. In this study, we evaluate and compare the
classification accuracy, precision, recall, and F-1 scores of five different
machine learning methods using a primary dataset (500 patients from Dhaka
Medical College Hospital). Five different supervised machine learning
techniques, including decision tree, random forest, logistic regression, naive
bayes, and XGBoost, have been used to achieve optimal results on our dataset.
Additionally, this study applied SHAP analysis to the XGBoost model to
interpret the model's predictions and understand the impact of each feature on
the model's output. We compared the accuracy with which several algorithms
classified the data, as well as contrasted with other literature in this field.
After final evaluation, this study found that XGBoost achieved the best model
accuracy, which is 97%.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÔºå‰π≥ÁôåÁöÑÁõõË°åÁéáËøÖÈÄüÂ¢ûÂä†Ôºå‰ΩøÂÖ∂ÊàêÁÇ∫ÂÖ®ÁêÉ‰∏ªË¶ÅÁöÑÊ≠ª‰∫°ÂéüÂõ†‰πã‰∏Ä„ÄÇÂú®ÊâÄÊúâÁôåÁóá‰∏≠Ôºå‰π≥ÁôåËøÑ‰ªäÁÇ∫Ê≠¢ÊòØÊúÄÂ∏∏Ë¶ãÁöÑ„ÄÇÊâãÂãïË®∫Êñ∑Ê≠§ÁñæÁóÖÈúÄË¶ÅÂ§ßÈáèÁöÑÊôÇÈñìÂíåÂ∞àÊ•≠Áü•Ë≠ò„ÄÇÁî±Êñº‰π≥ÁôåÁöÑÊ™¢Ê∏¨ÈÅéÁ®ãËÄóÊôÇÔºåÂõ†Ê≠§ÈÄèÈÅéÂª∫Á´ãÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜÈ†êÊ∏¨ÔºåÊúâÂä©ÊñºÈò≤Ê≠¢ÂÖ∂ÈÄ≤‰∏ÄÊ≠•Êì¥Êï£„ÄÇÊ©üÂô®Â≠∏ÁøíÂíåÂèØËß£Èáã AI Âú®ÂàÜÈ°û‰∏≠Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂÄë‰∏çÂÉÖÂèØ‰ª•Êèê‰æõÊ∫ñÁ¢∫ÁöÑÈ†êÊ∏¨ÔºåÈÇÑÂèØ‰ª•Ê∑±ÂÖ•‰∫ÜËß£Ê®°ÂûãÂ¶Ç‰ΩïÂÅöÂá∫Ê±∫Á≠ñÔºåÊúâÂä©ÊñºÁêÜËß£Âíå‰ø°Ë≥¥ÂàÜÈ°ûÁµêÊûú„ÄÇÂú®Ê≠§Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëË©ï‰º∞‰∏¶ÊØîËºÉ‰∫Ü‰∫îÁ®Æ‰∏çÂêåÁöÑÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶„ÄÅÁ≤æÁ¢∫Â∫¶„ÄÅÂè¨ÂõûÁéáÂíå F1 ÂàÜÊï∏Ôºå‰ΩøÁî®‰∫Ü‰∏ÄÂÄã‰∏ªË¶ÅÁöÑË≥áÊñôÈõÜÔºàÈÅîÂç°ÈÜ´Â≠∏Èô¢ÈÜ´Èô¢ÁöÑ 500 ÂêçÊÇ£ËÄÖÔºâ„ÄÇ‰∫îÁ®Æ‰∏çÂêåÁöÑÁõ£Áù£ÂºèÊ©üÂô®Â≠∏ÁøíÊäÄË°ìÔºåÂåÖÊã¨Ê±∫Á≠ñÊ®π„ÄÅÈö®Ê©üÊ£ÆÊûó„ÄÅÈÇèËºØËø¥Ê≠∏„ÄÅÊú¥Á¥†Ë≤ùÊ∞èÂíå XGBoostÔºåÂ∑≤Áî®ÊñºÂú®ÊàëÂÄëÁöÑË≥áÊñôÈõÜ‰∏äÂèñÂæóÊúÄ‰Ω≥ÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊú¨Á†îÁ©∂Â∞á SHAP ÂàÜÊûêÊáâÁî®Êñº XGBoost Ê®°ÂûãÔºå‰ª•Ëß£ÈáãÊ®°ÂûãÁöÑÈ†êÊ∏¨‰∏¶‰∫ÜËß£ÊØèÂÄãÁâπÂæµÂ∞çÊ®°ÂûãËº∏Âá∫ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÊØîËºÉ‰∫ÜÂπæÁ®ÆÊºîÁÆóÊ≥ïÂ∞çË≥áÊñôÈÄ≤Ë°åÂàÜÈ°ûÁöÑÊ∫ñÁ¢∫Â∫¶Ôºå‰∏¶ËàáË©≤È†òÂüüÁöÑÂÖ∂‰ªñÊñáÁçªÈÄ≤Ë°åÂ∞çÊØî„ÄÇÂú®ÊúÄÂæåË©ï‰º∞ÂæåÔºåÊú¨Á†îÁ©∂ÁôºÁèæ XGBoost ÈÅîÂà∞‰∫ÜÊúÄ‰Ω≥ÁöÑÊ®°ÂûãÊ∫ñÁ¢∫Â∫¶ÔºåÁÇ∫ 97%„ÄÇ</paragraph>

##### **Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI**
2404.03892v3 by Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir

The Deep learning (DL) models for diagnosing breast cancer from mammographic
images often operate as "black boxes", making it difficult for healthcare
professionals to trust and understand their decision-making processes. The
study presents an integrated framework combining Convolutional Neural Networks
(CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis
of breast cancer using the CBIS-DDSM dataset. The methodology encompasses an
elaborate data preprocessing pipeline and advanced data augmentation techniques
to counteract dataset limitations and transfer learning using pre-trained
networks such as VGG-16, Inception-V3 and ResNet was employed. A focal point of
our study is the evaluation of XAI's effectiveness in interpreting model
predictions, highlighted by utilizing the Hausdorff measure to assess the
alignment between AI-generated explanations and expert annotations
quantitatively. This approach is critical for XAI in promoting trustworthiness
and ethical fairness in AI-assisted diagnostics. The findings from our research
illustrate the effective collaboration between CNNs and XAI in advancing
diagnostic methods for breast cancer, thereby facilitating a more seamless
integration of advanced AI technologies within clinical settings. By enhancing
the interpretability of AI driven decisions, this work lays the groundwork for
improved collaboration between AI systems and medical practitioners, ultimately
enriching patient care. Furthermore, the implications of our research extended
well beyond the current methodologies. It encourages further research into how
to combine multimodal data and improve AI explanations to meet the needs of
clinical practice.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏Áøí (DL) Áî®ÊñºÂæû‰π≥ÊàøÊîùÂΩ±Ë°ìÂΩ±ÂÉèË®∫Êñ∑‰π≥ÁôåÁöÑÊ®°ÂûãÈÄöÂ∏∏‰ª•„ÄåÈªëÁõíÂ≠ê„ÄçÊñπÂºèÈÅã‰ΩúÔºåÈÄô‰ΩøÂæóÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°Èõ£‰ª•‰ø°‰ªªÂíåÁêÜËß£ÂÖ∂Ê±∫Á≠ñÈÅéÁ®ã„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊï¥ÂêàÊû∂ÊßãÔºåÁµêÂêàÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂíåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI)Ôºå‰ª•‰ΩøÁî® CBIS-DDSM Ë≥áÊñôÈõÜÂ¢ûÂº∑‰π≥ÁôåÁöÑË®∫Êñ∑„ÄÇÊñπÊ≥ïÂåÖÂê´‰∏ÄÂÄãÁ≤æÁ¥∞ÁöÑË≥áÊñôÂâçËôïÁêÜÁÆ°Á∑öÂíåÈÄ≤ÈöéË≥áÊñôÊì¥ÂÖÖÊäÄË°ìÔºå‰ª•Â∞çÊäóË≥áÊñôÈõÜÈôêÂà∂Ôºå‰∏¶Êé°Áî®È†êÂÖàË®ìÁ∑¥ÁöÑÁ∂≤Ë∑ØÔºà‰æãÂ¶Ç VGG-16„ÄÅInception-V3 Âíå ResNetÔºâÈÄ≤Ë°åÈÅ∑ÁßªÂ≠∏Áøí„ÄÇÊàëÂÄëÁ†îÁ©∂ÁöÑÈáçÈªûÊòØË©ï‰º∞ XAI Âú®Ëß£ÈáãÊ®°ÂûãÈ†êÊ∏¨‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÈáçÈªûÂà©Áî®Ë±™ÊñØÂ§öÂ§´Ê∏¨Â∫¶ÈáèÂåñË©ï‰º∞ AI ÁîüÊàêÁöÑËß£ÈáãÂíåÂ∞àÂÆ∂Ë®ªËß£‰πãÈñìÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÂ∞çÊñº XAI Âú®‰øÉÈÄ≤ AI ËºîÂä©Ë®∫Êñ∑‰∏≠ÁöÑÂèØ‰ø°Â∫¶ÂíåÂÄ´ÁêÜÂÖ¨Âπ≥ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÁ†îÁ©∂ÁöÑÁôºÁèæË™™Êòé‰∫Ü CNN Âíå XAI Âú®Êé®ÈÄ≤‰π≥ÁôåË®∫Êñ∑ÊñπÊ≥ï‰∏≠ÁöÑÊúâÊïàÂçî‰ΩúÔºåÂæûËÄå‰øÉÈÄ≤‰∫ÜÂÖàÈÄ≤ AI ÊäÄË°ìÂú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊõ¥È†ÜÊö¢Êï¥Âêà„ÄÇÈÄèÈÅéÂ¢ûÂº∑ AI È©ÖÂãïÊ±∫Á≠ñÁöÑÂèØËß£ÈáãÊÄßÔºåÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫ AI Á≥ªÁµ±ÂíåÈÜ´ÁôÇÂæûÊ•≠‰∫∫Âì°‰πãÈñìÁöÑÊîπÂñÑÂçî‰ΩúÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºåÊúÄÁµÇË±êÂØå‰∫ÜÊÇ£ËÄÖÁÖßË≠∑„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁ†îÁ©∂ÁöÑÂΩ±ÈüøÈÅ†ÈÅ†Ë∂ÖÂá∫‰∫ÜÁõÆÂâçÁöÑÊäÄË°ì„ÄÇÂÆÉÈºìÂãµÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂Â¶Ç‰ΩïÁµêÂêàÂ§öÊ®°ÂºèË≥áÊñô‰∏¶ÊîπÂñÑ AI Ëß£ÈáãÔºå‰ª•ÊªøË∂≥Ëá®Â∫äÂØ¶ÂãôÁöÑÈúÄÊ±Ç„ÄÇ

##### **Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach**
2403.17873v1 by Andrea Ferrario, Alberto Termine, Alessandro Facchini

Human-centered explainable AI (HCXAI) advocates for the integration of social
aspects into AI explanations. Central to the HCXAI discourse is the Social
Transparency (ST) framework, which aims to make the socio-organizational
context of AI systems accessible to their users. In this work, we suggest
extending the ST framework to address the risks of social misattributions in
Large Language Models (LLMs), particularly in sensitive areas like mental
health. In fact LLMs, which are remarkably capable of simulating roles and
personas, may lead to mismatches between designers' intentions and users'
perceptions of social attributes, risking to promote emotional manipulation and
dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To
address these issues, we propose enhancing the ST framework with a fifth
'W-question' to clarify the specific social attributions assigned to LLMs by
its designers and users. This addition aims to bridge the gap between LLM
capabilities and user perceptions, promoting the ethically responsible
development and use of LLM-based technology.

ÊëòË¶ÅÔºö‰ª•‰∫∫‰∏∫Êú¨ÁöÑÂèØËß£Èáä AI (HCXAI) ÂÄ°ÂØºÂ∞ÜÁ§æ‰ºöÂ±ÇÈù¢Êï¥ÂêàÂà∞ AI Ëß£Èáä‰∏≠„ÄÇHCXAI ËØùËØ≠ÁöÑÊ†∏ÂøÉÊòØÁ§æ‰ºöÈÄèÊòéÂ∫¶ (ST) Ê°ÜÊû∂ÔºåÂÖ∂ÁõÆÊ†áÊòØËÆ© AI Á≥ªÁªüÁöÑÁ§æ‰ºöÁªÑÁªáËÉåÊôØÂØπÁî®Êà∑Êù•ËØ¥ÊòØÂèØÁêÜËß£ÁöÑ„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨Âª∫ËÆÆÊâ©Â±ï ST Ê°ÜÊû∂‰ª•Ëß£ÂÜ≥Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ‰∏≠Á§æ‰ºöÈîôËØØÂΩíÂõ†ÁöÑÈ£éÈô©ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂøÉÁêÜÂÅ•Â∫∑Á≠âÊïèÊÑüÈ¢ÜÂüü„ÄÇ‰∫ãÂÆû‰∏äÔºåLLM ËÉΩÂ§üÂá∫Ëâ≤Âú∞Ê®°ÊãüËßíËâ≤Âíå‰∫∫Ê†ºÔºåËøôÂèØËÉΩÂØºËá¥ËÆæËÆ°ËÄÖÁöÑÊÑèÂõæÂíåÁî®Êà∑ÂØπÁ§æ‰ºöÂ±ûÊÄßÁöÑËÆ§Áü•‰πãÈó¥Âá∫Áé∞ÈîôÈÖçÔºå‰ªéËÄåÊúâÈ£éÈô©‰øÉËøõÊÉÖÁª™ÊìçÁ∫µÂíåÂç±Èô©Ë°å‰∏∫„ÄÅËÆ§Áü•‰∏çÂÖ¨Ê≠£Âíå‰∏çÂêàÁêÜÁöÑ‰ø°‰ªª„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨Âª∫ËÆÆÁî®Á¨¨‰∫î‰∏™‚ÄúW ÈóÆÈ¢ò‚ÄùÊù•Â¢ûÂº∫ ST Ê°ÜÊû∂Ôºå‰ª•ÊòéÁ°ÆËÆæËÆ°ËÄÖÂíåÁî®Êà∑Ëµã‰∫à LLM ÁöÑÂÖ∑‰ΩìÁ§æ‰ºöÂ±ûÊÄß„ÄÇÊ≠§Ë°•ÂÖÖÊó®Âú®Âº•Âêà LLM ËÉΩÂäõÂíåÁî®Êà∑ËÆ§Áü•‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºå‰øÉËøõÂü∫‰∫é LLM ÁöÑÊäÄÊúØÂú®ÈÅìÂæ∑‰∏äË¥üË¥£‰ªªÂú∞ÂºÄÂèëÂíå‰ΩøÁî®„ÄÇ

##### **Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification**
2403.18871v1 by Han Yuan, Chuan Hong, Pengtao Jiang, Gangming Zhao, Nguyen Tuan Anh Tran, Xinxing Xu, Yet Yen Yan, Nan Liu

Background: Pneumothorax is an acute thoracic disease caused by abnormal air
collection between the lungs and chest wall. To address the opaqueness often
associated with deep learning (DL) models, explainable artificial intelligence
(XAI) methods have been introduced to outline regions related to pneumothorax
diagnoses made by DL models. However, these explanations sometimes diverge from
actual lesion areas, highlighting the need for further improvement. Method: We
propose a template-guided approach to incorporate the clinical knowledge of
pneumothorax into model explanations generated by XAI methods, thereby
enhancing the quality of these explanations. Utilizing one lesion delineation
created by radiologists, our approach first generates a template that
represents potential areas of pneumothorax occurrence. This template is then
superimposed on model explanations to filter out extraneous explanations that
fall outside the template's boundaries. To validate its efficacy, we carried
out a comparative analysis of three XAI methods with and without our template
guidance when explaining two DL models in two real-world datasets. Results: The
proposed approach consistently improved baseline XAI methods across twelve
benchmark scenarios built on three XAI methods, two DL models, and two
datasets. The average incremental percentages, calculated by the performance
improvements over the baseline performance, were 97.8% in Intersection over
Union (IoU) and 94.1% in Dice Similarity Coefficient (DSC) when comparing model
explanations and ground-truth lesion areas. Conclusions: In the context of
pneumothorax diagnoses, we proposed a template-guided approach for improving AI
explanations. We anticipate that our template guidance will forge a fresh
approach to elucidating AI models by integrating clinical domain expertise.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÔºöÊ∞£ËÉ∏ÊòØ‰∏ÄÁ®ÆÂõ†ËÇ∫ÈÉ®ËàáËÉ∏Â£Å‰πãÈñìÁï∞Â∏∏ÈõÜÊ∞£ÊâÄÂºïËµ∑ÁöÑÊÄ•ÊÄßËÉ∏ËÖîÁñæÁóÖ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê∑±Â∫¶Â≠∏ÁøíÔºàDLÔºâÊ®°ÂûãÁ∂ìÂ∏∏‰º¥Èö®ÁöÑ‰∏çÈÄèÊòéÊÄßÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖßÔºàXAIÔºâÊñπÊ≥ïÂ∑≤Ë¢´ÂºïÂÖ•ÔºåÁî®ÊñºÊ¶ÇËø∞Ëàá DL Ê®°ÂûãÂÅöÂá∫ÁöÑÊ∞£ËÉ∏Ë®∫Êñ∑Áõ∏ÈóúÁöÑÂçÄÂüü„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õËß£ÈáãÊúâÊôÇÊúÉËàáÂØ¶ÈöõÁóÖÁÅ∂ÂçÄÂüüÊúâÊâÄÂá∫ÂÖ•ÔºåÁ™ÅÈ°ØÂá∫ÈÄ≤‰∏ÄÊ≠•ÊîπÈÄ≤ÁöÑÂøÖË¶ÅÊÄß„ÄÇÊñπÊ≥ïÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°ÊùøÂºïÂ∞éÂºèÊñπÊ≥ïÔºåÂ∞áÊ∞£ËÉ∏ÁöÑËá®Â∫äÁü•Ë≠òÁ¥çÂÖ• XAI ÊñπÊ≥ïÁî¢ÁîüÁöÑÊ®°ÂûãËß£Èáã‰∏≠ÔºåÂæûËÄåÊèêÂçáÈÄô‰∫õËß£ÈáãÁöÑÂìÅË≥™„ÄÇÂà©Áî®ÊîæÂ∞ÑÁßëÈÜ´Â∏´Âª∫Á´ãÁöÑÁóÖÁÅ∂ÊèèÁπ™ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈ¶ñÂÖàÁî¢Áîü‰∏ÄÂÄãÊ®°ÊùøÔºåÁî®ÊñºË°®Á§∫Ê∞£ËÉ∏ÂèØËÉΩÁôºÁîüÁöÑÂçÄÂüü„ÄÇÁÑ∂ÂæåÂ∞áÊ≠§Ê®°ÊùøÁñäÂä†Âú®Ê®°ÂûãËß£Èáã‰∏äÔºå‰ª•ÁØ©ÈÅ∏Âá∫Ë∂ÖÂá∫Ê®°ÊùøÈÇäÁïåÁöÑÁÑ°ÈóúËß£Èáã„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÂÖ∂ÊïàÂäõÔºåÊàëÂÄëÂ∞ç‰∏âÁ®Æ XAI ÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÊØîËºÉÂàÜÊûêÔºåÂú®ÂÖ©ÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏≠Ëß£ÈáãÂÖ©ÂÄã DL Ê®°ÂûãÊôÇÔºåÂàÜÂà•Êé°Áî®Âíå‰∏çÊé°Áî®ÊàëÂÄëÁöÑÊ®°ÊùøÂºïÂ∞é„ÄÇÁµêÊûúÔºöÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Âª∫Á´ãÊñº‰∏âÁ®Æ XAI ÊñπÊ≥ï„ÄÅÂÖ©ÂÄã DL Ê®°ÂûãÂíåÂÖ©ÂÄãË≥áÊñôÈõÜÁöÑÂçÅ‰∫åÁ®ÆÂü∫Ê∫ñÊÉÖÂ¢É‰∏≠ÔºåÂßãÁµÇÊîπÂñÑ‰∫ÜÂü∫Ê∫ñ XAI ÊñπÊ≥ï„ÄÇÂú®ÊØîËºÉÊ®°ÂûãËß£ÈáãÂíåÁúüÂØ¶ÁóÖÁÅ∂ÂçÄÂüüÊôÇÔºåÈÄèÈÅéÂü∫Ê∫ñÊïàËÉΩÁöÑÊïàËÉΩÊîπÈÄ≤Ë®àÁÆóÂá∫ÁöÑÂπ≥ÂùáÂ¢ûÈáèÁôæÂàÜÊØîÁÇ∫‰∫§ÈõÜÊØîÔºàIoUÔºâÁöÑ 97.8% ÂíåÈ™∞Â≠êÁõ∏‰ººÊÄß‰øÇÊï∏ÔºàDSCÔºâÁöÑ 94.1%„ÄÇÁµêË´ñÔºöÂú®Ê∞£ËÉ∏Ë®∫Êñ∑ÁöÑËÉåÊôØ‰∏ãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°ÊùøÂºïÂ∞éÂºèÊñπÊ≥ïÔºåÁî®ÊñºÊîπÂñÑ AI Ëß£Èáã„ÄÇÊàëÂÄëÈ†êÊúüÊàëÂÄëÁöÑÊ®°ÊùøÂºïÂ∞éÂ∞áÈÄèÈÅéÊï¥ÂêàËá®Â∫äÈ†òÂüüÂ∞àÊ•≠Áü•Ë≠òÔºåÁÇ∫Èó°Êòé AI Ê®°ÂûãÂª∫Á´ã‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ï„ÄÇ</paragraph>

##### **Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures**
2403.01580v1 by S√©amus Lankford

In the current machine translation (MT) landscape, the Transformer
architecture stands out as the gold standard, especially for high-resource
language pairs. This research delves into its efficacy for low-resource
language pairs including both the English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi language pairs. Notably, the study identifies
the optimal hyperparameters and subword model type to significantly improve the
translation quality of Transformer models for low-resource language pairs.
  The scarcity of parallel datasets for low-resource languages can hinder MT
development. To address this, gaHealth was developed, the first bilingual
corpus of health data for the Irish language. Focusing on the health domain,
models developed using this in-domain dataset exhibited very significant
improvements in BLEU score when compared with models from the LoResMT2021
Shared Task. A subsequent human evaluation using the multidimensional quality
metrics error taxonomy showcased the superior performance of the Transformer
system in reducing both accuracy and fluency errors compared to an RNN-based
counterpart.
  Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source
applications streamlined for the development, fine-tuning, and deployment of
neural machine translation models. These tools considerably simplify the setup
and evaluation process, making MT more accessible to both developers and
translators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes
eco-friendly natural language processing research by highlighting the
environmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM
demonstrated advancements in translation performance for two low-resource
language pairs: English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi, compared to baselines from the LoResMT2021
Shared Task.

ÊëòË¶ÅÔºö<paragraph>Âú®Áï∂ÂâçÊ©üÂô®ÁøªË≠Ø (MT) È†òÂüü‰∏≠ÔºåTransformer Êû∂ÊßãËÑ´Á©éËÄåÂá∫ÔºåÊàêÁÇ∫ÈªÉÈáëÊ®ôÊ∫ñÔºåÁâπÂà•ÊòØÂ∞çÊñºÈ´òË≥áÊ∫êË™ûË®ÄÂ∞ç„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂÖ∂Â∞ç‰ΩéË≥áÊ∫êË™ûË®ÄÂ∞çÁöÑÊïàËÉΩÔºåÂåÖÊã¨Ëã±Ë™û‚ÜîÊÑõÁàæËò≠Ë™ûÂíåËã±Ë™û‚ÜîÈ¶¨ÊãâÂú∞Ë™ûË™ûË®ÄÂ∞ç„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊú¨Á†îÁ©∂Ë≠òÂà•Âá∫ÊúÄ‰Ω≥Ë∂ÖÂèÉÊï∏ÂíåÂ≠êË©ûÊ®°ÂûãÈ°ûÂûãÔºå‰ª•È°ØËëóÊèêÈ´ò Transformer Ê®°ÂûãÂ∞ç‰ΩéË≥áÊ∫êË™ûË®ÄÂ∞çÁöÑÁøªË≠ØÂìÅË≥™„ÄÇ
‰ΩéË≥áÊ∫êË™ûË®ÄÁöÑÂπ≥Ë°åË≥áÊñôÈõÜÁöÑÁ®ÄÁº∫ÊúÉÈòªÁ§ô MT ÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÈñãÁôº‰∫Ü gaHealthÔºåÈÄôÊòØÊÑõÁàæËò≠Ë™ûÁöÑÁ¨¨‰∏ÄÂÄãÈõôË™ûÂÅ•Â∫∑Ë≥áÊñôË™ûÊñôÂ∫´„ÄÇÂ∞àÊ≥®ÊñºÂÅ•Â∫∑È†òÂüüÔºå‰ΩøÁî®Ê≠§ÂüüÂÖßË≥áÊñôÈõÜÈñãÁôºÁöÑÊ®°ÂûãÂú® BLEU ÂæóÂàÜÊñπÈù¢Ë°®ÁèæÂá∫ÈùûÂ∏∏È°ØËëóÁöÑÈÄ≤Ê≠•ÔºåËàá LoResMT2021 ÂÖ±‰∫´‰ªªÂãô‰∏≠ÁöÑÊ®°ÂûãÁõ∏ÊØî„ÄÇÈö®Âæå‰ΩøÁî®Â§öÁ∂≠ÂìÅË≥™ÊåáÊ®ôÈåØË™§ÂàÜÈ°ûÊ≥ïÈÄ≤Ë°åÁöÑ‰∫∫Â∑•Ë©ï‰º∞È°ØÁ§∫ÔºåËàáÂü∫Êñº RNN ÁöÑÂ∞çÊáâÊ®°ÂûãÁõ∏ÊØîÔºåTransformer Á≥ªÁµ±Âú®Ê∏õÂ∞ëÊ∫ñÁ¢∫ÊÄßÂíåÊµÅÊö¢ÊÄßÈåØË™§ÊñπÈù¢Ë°®ÁèæÂá∫ÂÑ™Áï∞ÁöÑÊÄßËÉΩ„ÄÇ
Ê≠§Â§ñÔºåÊú¨Ë´ñÊñá‰ªãÁ¥π‰∫Ü adaptNMT Âíå adaptMLLMÔºåÈÄôÂÖ©ÂÄãÈñãÊ∫êÊáâÁî®Á®ãÂºèÁ∞°Âåñ‰∫ÜÁ•ûÁ∂ìÊ©üÂô®ÁøªË≠ØÊ®°ÂûãÁöÑÈñãÁôº„ÄÅÂæÆË™øÂíåÈÉ®ÁΩ≤„ÄÇÈÄô‰∫õÂ∑•ÂÖ∑Â§ßÂπÖÁ∞°Âåñ‰∫ÜË®≠ÂÆöÂíåË©ï‰º∞ÊµÅÁ®ãÔºåËÆì MT Êõ¥ÂÆπÊòìËÆìÈñãÁôº‰∫∫Âì°ÂíåÁøªË≠Ø‰∫∫Âì°‰ΩøÁî®„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåadaptNMT ‰ª• OpenNMT ÁîüÊÖãÁ≥ªÁµ±ÁÇ∫Âü∫Á§éÔºåÈÄöÈÅéÂº∑Ë™øÊ®°ÂûãÈñãÁôºÁöÑÁí∞Â¢ÉË∂≥Ë∑°‰æÜ‰øÉÈÄ≤ÁîüÊÖãÂèãÂ•ΩÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁ†îÁ©∂„ÄÇËàá LoResMT2021 ÂÖ±‰∫´‰ªªÂãô‰∏≠ÁöÑÂü∫Ê∫ñÁõ∏ÊØîÔºåadaptMLLM Â∞ç MLLM ÁöÑÂæÆË™øË≠âÊòé‰∫ÜËã±Ë™û‚ÜîÊÑõÁàæËò≠Ë™ûÂíåËã±Ë™û‚ÜîÈ¶¨ÊãâÂú∞Ë™ûÈÄôÂÖ©ÂÄã‰ΩéË≥áÊ∫êË™ûË®ÄÂ∞çÁöÑÁøªË≠ØÊÄßËÉΩÈÄ≤Ê≠•„ÄÇ</paragraph>

##### **Artificial Intelligence and Diabetes Mellitus: An Inside Look Through the Retina**
2402.18600v1 by Yasin Sadeghi Bazargani, Majid Mirzaei, Navid Sobhi, Mirsaeed Abdollahi, Ali Jafarizadeh, Siamak Pedrammehr, Roohallah Alizadehsani, Ru San Tan, Sheikh Mohammed Shariful Islam, U. Rajendra Acharya

Diabetes mellitus (DM) predisposes patients to vascular complications.
Retinal images and vasculature reflect the body's micro- and macrovascular
health. They can be used to diagnose DM complications, including diabetic
retinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular
disease, as well as forecast the risk of cardiovascular events. Artificial
intelligence (AI)-enabled systems developed for high-throughput detection of DR
using digitized retinal images have become clinically adopted. Beyond DR
screening, AI integration also holds immense potential to address challenges
associated with the holistic care of the patient with DM. In this work, we aim
to comprehensively review the literature for studies on AI applications based
on retinal images related to DM diagnosis, prognostication, and management. We
will describe the findings of holistic AI-assisted diabetes care, including but
not limited to DR screening, and discuss barriers to implementing such systems,
including issues concerning ethics, data privacy, equitable access, and
explainability. With the ability to evaluate the patient's health status vis a
vis DM complication as well as risk prognostication of future cardiovascular
complications, AI-assisted retinal image analysis has the potential to become a
central tool for modern personalized medicine in patients with DM.

ÊëòË¶ÅÔºöÁ≥ñÂ∞øÁóÖÔºàDMÔºâ‰ΩøÊÇ£ËÄÖÂÆπÊòìÂá∫ÁèæË°ÄÁÆ°‰ΩµÁôºÁóá„ÄÇ
Ë¶ñÁ∂≤ËÜúÂΩ±ÂÉèÂíåË°ÄÁÆ°ÂèçÊò†Ë∫´È´îÁöÑÂæÆË°ÄÁÆ°ÂíåÂ∑®Ë°ÄÁÆ°ÂÅ•Â∫∑ÁãÄÊ≥Å„ÄÇÂÆÉÂÄëÂèØÁî®ÊñºË®∫Êñ∑Á≥ñÂ∞øÁóÖ‰ΩµÁôºÁóáÔºåÂåÖÊã¨Á≥ñÂ∞øÁóÖË¶ñÁ∂≤ËÜúÁóÖËÆäÔºàDRÔºâ„ÄÅÁ•ûÁ∂ìÁóÖËÆä„ÄÅËÖéÁóÖÂíåÂãïËÑàÁ≤•Ê®£Á°¨ÂåñÊÄßÂøÉË°ÄÁÆ°ÁñæÁóÖÔºå‰ª•ÂèäÈ†êÊ∏¨ÂøÉË°ÄÁÆ°‰∫ã‰ª∂ÁöÑÈ¢®Èö™„ÄÇÁÇ∫‰ΩøÁî®Êï∏‰ΩçÂåñË¶ñÁ∂≤ËÜúÂΩ±ÂÉèÈÄ≤Ë°åÈ´òÈÄöÈáè DR Ê™¢Ê∏¨ËÄåÈñãÁôºÁöÑ‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂïüÁî®Á≥ªÁµ±Â∑≤Âú®Ëá®Â∫äÊé°Áî®„ÄÇÈô§‰∫Ü DR ÁØ©Ê™¢Â§ñÔºåAI Êï¥Âêà‰πüÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõ‰æÜÊáâÂ∞çËàáÁ≥ñÂ∞øÁóÖÊÇ£ËÄÖÊï¥È´îÁÖßË≠∑Áõ∏ÈóúÁöÑÊåëÊà∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊó®Âú®ÂÖ®Èù¢ÂõûÈ°ßÂü∫ÊñºË¶ñÁ∂≤ËÜúÂΩ±ÂÉèÁöÑ AI ÊáâÁî®Áõ∏ÈóúÁ†îÁ©∂ÁöÑÊñáÁçªÔºåÈÄô‰∫õÁ†îÁ©∂ËàáÁ≥ñÂ∞øÁóÖÁöÑË®∫Êñ∑„ÄÅÈ†êÂæåÂíåÁÆ°ÁêÜÊúâÈóú„ÄÇÊàëÂÄëÂ∞áÊèèËø∞Êï¥È´î AI ËºîÂä©Á≥ñÂ∞øÁóÖÁÖßË≠∑ÁöÑÁôºÁèæÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôêÊñº DR ÁØ©Ê™¢Ôºå‰∏¶Ë®éË´ñÂØ¶ÊñΩÊ≠§È°ûÁ≥ªÁµ±ÁöÑÈöúÁ§ôÔºåÂåÖÊã¨ËàáÂÄ´ÁêÜ„ÄÅË≥áÊñôÈö±ÁßÅ„ÄÅÂÖ¨Âπ≥Â≠òÂèñÂíåÂèØËß£ÈáãÊÄßÊúâÈóúÁöÑÂïèÈ°å„ÄÇÈÄèÈÅéË©ï‰º∞ÊÇ£ËÄÖÁöÑÂÅ•Â∫∑ÁãÄÊ≥ÅÔºåÂêåÊôÇËÄÉÈáèÁ≥ñÂ∞øÁóÖ‰ΩµÁôºÁóá‰ª•ÂèäÊú™‰æÜÂøÉË°ÄÁÆ°‰ΩµÁôºÁóáÁöÑÈ¢®Èö™È†êÂæåÔºåAI ËºîÂä©Ë¶ñÁ∂≤ËÜúÂΩ±ÂÉèÂàÜÊûêÊúâÊΩõÂäõÊàêÁÇ∫Á≥ñÂ∞øÁóÖÊÇ£ËÄÖÁèæ‰ª£ÂåñÂÄã‰∫∫ÂåñÈÜ´ÁôÇÁöÑ‰∏≠ÂøÉÂ∑•ÂÖ∑„ÄÇ

##### **Multi-stakeholder Perspective on Responsible Artificial Intelligence and Acceptability in Education**
2402.15027v2 by A. J. Karran, P. Charland, J-T. Martineau, A. Ortiz de Guinea Lopez de Arana, AM. Lesage, S. Senecal, P-M. Leger

This study investigates the acceptability of different artificial
intelligence (AI) applications in education from a multi-stakeholder
perspective, including students, teachers, and parents. Acknowledging the
transformative potential of AI in education, it addresses concerns related to
data privacy, AI agency, transparency, explainability and the ethical
deployment of AI. Through a vignette methodology, participants were presented
with four scenarios where AI's agency, transparency, explainability, and
privacy were manipulated. After each scenario, participants completed a survey
that captured their perceptions of AI's global utility, individual usefulness,
justice, confidence, risk, and intention to use each scenario's AI if
available. The data collection comprising a final sample of 1198
multi-stakeholder participants was distributed through a partner institution
and social media campaigns and focused on individual responses to four AI use
cases. A mediation analysis of the data indicated that acceptance and trust in
AI varies significantly across stakeholder groups. We found that the key
mediators between high and low levels of AI's agency, transparency, and
explainability, as well as the intention to use the different educational AI,
included perceived global utility, justice, and confidence. The study
highlights that the acceptance of AI in education is a nuanced and multifaceted
issue that requires careful consideration of specific AI applications and their
characteristics, in addition to the diverse stakeholders' perceptions.

ÊëòË¶ÅÔºöÈÄôÈ†ÖÁ†îÁ©∂ÂæûÂ§öÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑËßíÂ∫¶Êé¢Ë®é‰∏çÂêåÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊáâÁî®Âú®ÊïôËÇ≤‰∏äÁöÑÂèØÊé•ÂèóÊÄßÔºåÂåÖÊã¨Â≠∏Áîü„ÄÅËÄÅÂ∏´ÂíåÂÆ∂Èï∑„ÄÇÊâøË™ç AI Âú®ÊïôËÇ≤‰∏äÁöÑËΩâÂûãÊΩõÂäõÔºåÂÆÉËß£Ê±∫‰∫ÜËàáË≥áÊñôÈö±ÁßÅ„ÄÅAI ‰ª£ÁêÜ„ÄÅÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíå AI ÁöÑÈÅìÂæ∑ÈÉ®ÁΩ≤Áõ∏ÈóúÁöÑÁñëÊÖÆ„ÄÇÈÄèÈÅéÂ∞èÊèíÊõ≤ÊñπÊ≥ïÔºåÂèÉËàáËÄÖË¢´ÂëàÁèæ‰∫ÜÂõõÁ®ÆÊÉÖÂ¢ÉÔºåÂÖ∂‰∏≠ AI ÁöÑ‰ª£ÁêÜ„ÄÅÈÄèÊòéÂ∫¶„ÄÅÂèØËß£ÈáãÊÄßÂíåÈö±ÁßÅÂèóÂà∞ÊìçÁ∏±„ÄÇÂú®ÊØèÂÄãÊÉÖÂ¢ÉÂæåÔºåÂèÉËàáËÄÖÂÆåÊàê‰∫Ü‰∏ÄÈ†ÖË™øÊü•ÔºåË©≤Ë™øÊü•ÊçïÊçâ‰∫Ü‰ªñÂÄëÂ∞ç AI ÁöÑÊï¥È´îÊïàÁî®„ÄÅÂÄã‰∫∫ÊïàÁî®„ÄÅÊ≠£Áæ©„ÄÅ‰ø°ÂøÉ„ÄÅÈ¢®Èö™ÂíåÂ¶ÇÊûúÂèØÁî®Ôºå‰ΩøÁî®ÊØèÂÄãÊÉÖÂ¢ÉÁöÑ AI ÁöÑÊÑèÂúñÁöÑÁúãÊ≥ï„ÄÇË≥áÊñôËíêÈõÜÂåÖÂê´‰æÜËá™Âêà‰ΩúÊ©üÊßãÂíåÁ§æÁæ§Â™íÈ´îÊ¥ªÂãïÁöÑ 1198 ‰ΩçÂ§öÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÂèÉËàáËÄÖÁöÑÊúÄÁµÇÊ®£Êú¨Ôºå‰∏¶Â∞àÊ≥®ÊñºÂ∞çÂõõÂÄã AI ‰ΩøÁî®Ê°à‰æãÁöÑÂÄãÂà•ÂõûÊáâ„ÄÇÂ∞çË≥áÊñôÁöÑË™øËß£ÂàÜÊûêË°®ÊòéÔºåÂ∞ç AI ÁöÑÊé•ÂèóÂ∫¶Âíå‰ø°‰ªªÂú®Âà©ÂÆ≥Èóú‰øÇ‰∫∫ÂúòÈ´î‰πãÈñìÊúâÈ°ØËëóÂ∑ÆÁï∞„ÄÇÊàëÂÄëÁôºÁèæÔºåAI ÁöÑ‰ª£ÁêÜ„ÄÅÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßÈ´ò‰ΩéÁ®ãÂ∫¶‰πãÈñìÁöÑÈóúÈçµË™øËß£ËÄÖÔºå‰ª•Âèä‰ΩøÁî®‰∏çÂêåÊïôËÇ≤ AI ÁöÑÊÑèÂúñÔºåÂåÖÊã¨ÊÑüÁü•Âà∞ÁöÑÊï¥È´îÊïàÁî®„ÄÅÊ≠£Áæ©Âíå‰ø°ÂøÉ„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Âº∑Ë™øÔºåÊé•Âèó AI Âú®ÊïôËÇ≤‰∏äÁöÑÊáâÁî®ÊòØ‰∏ÄÂÄãÂæÆÂ¶ô‰∏îÂ§öÈù¢ÂêëÁöÑÂïèÈ°åÔºåÈô§‰∫Ü‰∏çÂêåÁöÑÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑÁúãÊ≥ïÂ§ñÔºåÈÇÑÈúÄË¶Å‰ªîÁ¥∞ËÄÉÊÖÆÂÖ∑È´îÁöÑ AI ÊáâÁî®ÂèäÂÖ∂ÁâπÂæµ„ÄÇ

##### **Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals**
2402.09474v2 by Aruna Mohan, Danne Elbers, Or Zilbershot, Fatemeh Afghah, David Vorchheimer

Remote patient monitoring based on wearable single-lead electrocardiogram
(ECG) devices has significant potential for enabling the early detection of
heart disease, especially in combination with artificial intelligence (AI)
approaches for automated heart disease detection. There have been prior studies
applying AI approaches based on deep learning for heart disease detection.
However, these models are yet to be widely accepted as a reliable aid for
clinical diagnostics, in part due to the current black-box perception
surrounding many AI algorithms. In particular, there is a need to identify the
key features of the ECG signal that contribute toward making an accurate
diagnosis, thereby enhancing the interpretability of the model. In the present
study, we develop a vision transformer approach to identify atrial fibrillation
based on single-lead ECG data. A residual network (ResNet) approach is also
developed for comparison with the vision transformer approach. These models are
applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as
well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm
heartbeats. The models enable the identification of the key regions of the
heartbeat that determine the resulting classification, and highlight the
importance of P-waves and T-waves, as well as heartbeat duration and signal
amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and
sinus bradycardia.

ÊëòË¶ÅÔºö<paragraph>Âü∫ÊñºÂèØÁ©øÊà¥ÂºèÂñÆÂ∞éÁ®ãÂøÉÈõªÂúñ (ECG) Ë£ùÁΩÆÁöÑÈÅ†Á´ØÁóÖÊÇ£Áõ£Ê∏¨Âú®Êó©ÊúüÂÅµÊ∏¨ÂøÉËáüÁñæÁóÖÊñπÈù¢ÂÖ∑ÊúâÈ°ØËëóÁöÑÊΩõÂäõÔºåÁâπÂà•ÊòØËàáÁî®ÊñºËá™ÂãïÂåñÂøÉËáüÁñæÁóÖÂÅµÊ∏¨ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÁµêÂêà‰ΩøÁî®ÊôÇ„ÄÇÂÖàÂâçÂ∑≤ÊúâÁ†îÁ©∂ÊáâÁî®Âü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑ AI ÊñπÊ≥ïÈÄ≤Ë°åÂøÉËáüÁñæÁóÖÂÅµÊ∏¨„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®°ÂûãÂ∞öÊú™Ë¢´Âª£Ê≥õÊé•ÂèóÁÇ∫Ëá®Â∫äË®∫Êñ∑ÁöÑÂèØÈù†ËºîÂä©Â∑•ÂÖ∑ÔºåÈÉ®ÂàÜÂéüÂõ†Âú®ÊñºÂúçÁπûË®±Â§ö AI ÊºîÁÆóÊ≥ïÁöÑÁï∂ÂâçÈªëÁÆ±ÊÑüÁü•„ÄÇÁâπÂà•ÊòØÔºåÊúâÂøÖË¶ÅÊâæÂá∫ÊúâÂä©ÊñºÂÅöÂá∫Ê∫ñÁ¢∫Ë®∫Êñ∑ÁöÑ ECG Ë®äËôüÈóúÈçµÁâπÂæµÔºåÂæûËÄåÂ¢ûÂº∑Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄß„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆË¶ñË¶∫ËΩâÊèõÂô®ÊñπÊ≥ïÔºå‰ª•Ê†πÊìöÂñÆÂ∞éÁ®ã ECG Ë≥áÊñôÊâæÂá∫ÂøÉÊàøÈ°´Âãï„ÄÇÊÆòÂ∑ÆÁ∂≤Ë∑Ø (ResNet) ÊñπÊ≥ï‰πüÂ∑≤ÈñãÁôºÂá∫‰æÜÔºå‰ª•‰æøËàáË¶ñË¶∫ËΩâÊèõÂô®ÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉ„ÄÇÈÄô‰∫õÊ®°ÂûãÊáâÁî®Êñº Chapman-Shaoxing Ë≥áÊñôÈõÜÔºå‰ª•ÂàÜÈ°ûÂøÉÊàøÈ°´ÂãïÔºå‰ª•ÂèäÂè¶‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÂøÉÂæã‰∏çÊï¥ÔºåÁ´áÊÄßÂøÉÂãïÈÅéÁ∑©ÔºåÂíåÊ≠£Â∏∏Á´áÊÄßÂøÉÂæãÁöÑÂøÉË∑≥„ÄÇÈÄô‰∫õÊ®°ÂûãËÉΩÂ§†ÊâæÂá∫Ê±∫ÂÆöÊúÄÁµÇÂàÜÈ°ûÁöÑÂøÉË∑≥ÈóúÈçµÂçÄÂüüÔºå‰∏¶Âº∑Ë™ø P Ê≥¢Âíå T Ê≥¢Ôºå‰ª•ÂèäÂøÉË∑≥ÊåÅÁ∫åÊôÇÈñìÂíåË®äËôüÊåØÂπÖÂú®ÂçÄÂàÜÊ≠£Â∏∏Á´áÊÄßÂøÉÂæãËàáÂøÉÊàøÈ°´ÂãïÂíåÁ´áÊÄßÂøÉÂãïÈÅéÁ∑©ÊñπÈù¢ÁöÑÈáçË¶ÅÊÄß„ÄÇ</paragraph>

##### **Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering**
2402.05127v1 by Aryan Agrawal

This paper introduces a novel paradigm for depression detection and treatment
using advanced Large Language Models (LLMs): Generative Pre-trained Transformer
4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized
prompts to diagnose, explain, and suggest therapeutic interventions for
depression. A unique few-shot prompting method enhances the models' ability to
analyze and explain depressive symptoms based on the DSM-5 criteria. In the
interaction phase, the models engage in empathetic dialogue management, drawing
from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide,
fostering supportive interactions with individuals experiencing major
depressive disorders. Additionally, the research introduces the Illuminate
Database, enriched with various CBT modules, aiding in personalized therapy
recommendations. The study evaluates LLM performance using metrics such as F1
scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) across different test sets, demonstrating their
effectiveness. This comprehensive approach blends cutting-edge AI with
established psychological methods, offering new possibilities in mental health
care and showcasing the potential of LLMs in revolutionizing depression
diagnosis and treatment strategies.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®ÂÖàÈÄ≤Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åÊÜÇÈ¨±ÁóáÂÅµÊ∏¨ÂíåÊ≤ªÁôÇÁöÑÊñ∞Ê®°ÂºèÔºöÁîüÊàêÂºèÈ†êË®ìÁ∑¥Transformer 4 (GPT-4)„ÄÅLlama 2 ËÅäÂ§©Ê©üÂô®‰∫∫Âíå Gemini„ÄÇÈÄô‰∫õ LLM Á∂ìÈÅéÂæÆË™øÔºåÂÖ∑ÂÇôÂ∞àÊ•≠ÊèêÁ§∫ÔºåÂèØË®∫Êñ∑„ÄÅËß£Èáã‰∏¶Âª∫Ë≠∞ÊÜÇÈ¨±ÁóáÁöÑÊ≤ªÁôÇ‰ªãÂÖ•ÊñπÊ≥ï„ÄÇ‰∏ÄÁ®ÆÁç®ÁâπÁöÑÂ∞ëÊ¨°ÊèêÁ§∫ÊñπÊ≥ïÂ¢ûÂº∑‰∫ÜÊ®°ÂûãÊ†πÊìö DSM-5 Ê®ôÊ∫ñÂàÜÊûêÂíåËß£ÈáãÊÜÇÈ¨±ÁóáÁãÄÁöÑËÉΩÂäõ„ÄÇÂú®‰∫íÂãïÈöéÊÆµÔºåÈÄô‰∫õÊ®°ÂûãÊúÉÂèÉËàáÂêåÁêÜÂøÉÂ∞çË©±ÁÆ°ÁêÜÔºåÂæû PsychDB ÂíåË™çÁü•Ë°åÁÇ∫ÁôÇÊ≥ï (CBT) ÊåáÂçóÁ≠âË≥áÊ∫ê‰∏≠Ê±≤ÂèñÔºå‰øÉÈÄ≤ËàáÁ∂ìÊ≠∑ÈáçÂ∫¶ÊÜÇÈ¨±ÁóáÁöÑ‰∫∫ÂÄëÁöÑÊîØÊåÅÊÄß‰∫íÂãï„ÄÇÊ≠§Â§ñÔºåÈÄôÈ†ÖÁ†îÁ©∂ÈÇÑ‰ªãÁ¥π‰∫Ü Illuminate Ë≥áÊñôÂ∫´ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂêÑÁ®Æ CBT Ê®°ÁµÑÔºåÊúâÂä©ÊñºÂÄãÊÄßÂåñÊ≤ªÁôÇÂª∫Ë≠∞„ÄÇÈÄôÈ†ÖÁ†îÁ©∂‰ΩøÁî® F1 ÂàÜÊï∏„ÄÅÊ∫ñÁ¢∫Áéá„ÄÅÂè¨ÂõûÁéá„ÄÅÈ§òÂº¶Áõ∏‰ººÂ∫¶ÂíåÈù¢ÂêëÂè¨ÂõûÁéáÁöÑ Gisting Ë©ï‰º∞ÊõøË∫´ (ROUGE) Á≠âÊåáÊ®ôÔºåÂú®‰∏çÂêåÁöÑÊ∏¨Ë©¶ÈõÜ‰∏≠Ë©ï‰º∞ LLM ÁöÑË°®ÁèæÔºåË≠âÊòé‰∫ÜÂÆÉÂÄëÁöÑÊúâÊïàÊÄß„ÄÇÈÄôÁ®ÆÁ∂úÂêàÊñπÊ≥ïÁµêÂêà‰∫ÜÂ∞ñÁ´ØÁöÑ AI ËàáÊó¢ÂÆöÁöÑÂøÉÁêÜÊñπÊ≥ïÔºåÁÇ∫ÂøÉÁêÜ‰øùÂÅ•Êèê‰æõ‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄßÔºå‰∏¶Â±ïÁ§∫‰∫Ü LLM Âú®Èù©Êñ∞ÊÜÇÈ¨±ÁóáË®∫Êñ∑ÂíåÊ≤ªÁôÇÁ≠ñÁï•ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions**
2401.13324v4 by Timoth√©e Schmude, Laura Koesten, Torsten M√∂ller, Sebastian Tschiatschek

Explanations of AI systems rarely address the information needs of people
affected by algorithmic decision-making (ADM). This gap between conveyed
information and information that matters to affected stakeholders can impede
understanding and adherence to regulatory frameworks such as the AI Act. To
address this gap, we present the "XAI Novice Question Bank": A catalog of
affected stakeholders' information needs in two ADM use cases (employment
prediction and health monitoring), covering the categories data, system
context, system usage, and system specifications. Information needs were
gathered in an interview study where participants received explanations in
response to their inquiries. Participants further reported their understanding
and decision confidence, showing that while confidence tended to increase after
receiving explanations, participants also met understanding challenges, such as
being unable to tell why their understanding felt incomplete. Explanations
further influenced participants' perceptions of the systems' risks and
benefits, which they confirmed or changed depending on the use case. When risks
were perceived as high, participants expressed particular interest in
explanations about intention, such as why and to what end a system was put in
place. With this work, we aim to support the inclusion of affected stakeholders
into explainability by contributing an overview of information and challenges
relevant to them when deciding on the adoption of ADM systems. We close by
summarizing our findings in a list of six key implications that inform the
design of future explanations for affected stakeholder audiences.

ÊëòË¶ÅÔºö<paragraph>‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÁöÑË™™ÊòéÂæàÂ∞ëËÉΩÊªøË∂≥ÂèóÊºîÁÆóÊ≥ïÊ±∫Á≠ñ (ADM) ÂΩ±ÈüøÁöÑ‰∫∫ÂÄëÁöÑË≥áË®äÈúÄÊ±Ç„ÄÇÂÇ≥ÈÅîÁöÑË≥áË®äËàáÂΩ±ÈüøÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÈáçË¶ÅÁöÑË≥áË®ä‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÂèØËÉΩÊúÉÈòªÁ§ô‰∫ÜËß£ÂíåÈÅµÂÆàÊ≥ïË¶èÊû∂ÊßãÔºå‰æãÂ¶Ç‰∫∫Â∑•Êô∫ÊÖßÊ≥ïÊ°à„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü„ÄåXAI ÂàùÂ≠∏ËÄÖÂïèÈ°åÂ∫´„ÄçÔºöÂèóÂΩ±ÈüøÂà©ÂÆ≥Èóú‰øÇ‰∫∫Ë≥áË®äÈúÄÊ±ÇÁöÑÁõÆÈåÑÔºåÊ∂µËìãÂÖ©ÂÄã ADM ‰ΩøÁî®Ê°à‰æãÔºàÂ∞±Ê•≠È†êÊ∏¨ÂíåÂÅ•Â∫∑Áõ£Ê∏¨ÔºâÔºåÊ∂µËìãË≥áÊñô„ÄÅÁ≥ªÁµ±ËÑàÁµ°„ÄÅÁ≥ªÁµ±‰ΩøÁî®ÂíåÁ≥ªÁµ±Ë¶èÊ†ºÈ°ûÂà•„ÄÇË≥áË®äÈúÄÊ±ÇÊòØÈÄèÈÅéË®™Ë´áÁ†îÁ©∂Êî∂ÈõÜÁöÑÔºåÂèÉËàáËÄÖÂú®Ë©¢ÂïèÂæåÊî∂Âà∞Ë™™Êòé„ÄÇÂèÉËàáËÄÖÈÄ≤‰∏ÄÊ≠•ÂõûÂ†±‰ªñÂÄëÁöÑÁêÜËß£ÂíåÊ±∫Á≠ñ‰ø°ÂøÉÔºåÈ°ØÁ§∫ÈõñÁÑ∂Âú®Êî∂Âà∞Ë™™ÊòéÂæå‰ø°ÂøÉÂÇæÂêëÊñºÂ¢ûÂä†Ôºå‰ΩÜÂèÉËàáËÄÖ‰πüÈÅáÂà∞‰∫ÜÁêÜËß£ÊåëÊà∞Ôºå‰æãÂ¶ÇÁÑ°Ê≥ïË™™ÊòéÁÇ∫‰ªÄÈ∫º‰ªñÂÄëÁöÑÁêÜËß£ÊÑüË¶∫‰∏çÂÆåÊï¥„ÄÇË™™ÊòéÈÄ≤‰∏ÄÊ≠•ÂΩ±ÈüøÂèÉËàáËÄÖÂ∞çÁ≥ªÁµ±È¢®Èö™ÂíåÂ•ΩËôïÁöÑÁúãÊ≥ïÔºå‰ªñÂÄëÊúÉÊ†πÊìö‰ΩøÁî®Ê°à‰æãÁ¢∫Ë™çÊàñÊîπËÆäÈÄô‰∫õÁúãÊ≥ï„ÄÇÁï∂È¢®Èö™Ë¢´Ë™çÁÇ∫ÂæàÈ´òÊôÇÔºåÂèÉËàáËÄÖË°®Á§∫ÁâπÂà•ÊúâËààË∂£‰∫ÜËß£ÊÑèÂúñÁöÑË™™ÊòéÔºå‰æãÂ¶ÇÁÇ∫‰ªÄÈ∫º‰ª•ÂèäÁÇ∫‰∫Ü‰ªÄÈ∫ºÁõÆÁöÑËÄåÂª∫Á´ãÁ≥ªÁµ±„ÄÇÈÄèÈÅéÈÄôÈ†ÖÂ∑•‰ΩúÔºåÊàëÂÄëÊó®Âú®ÈÄèÈÅéÂú®Ê±∫Á≠ñÊé°Áî® ADM Á≥ªÁµ±ÊôÇÊèê‰æõÁõ∏ÈóúË≥áË®äÂíåÊåëÊà∞ÁöÑÊ¶ÇË¶ΩÔºå‰æÜÊîØÊè¥Â∞áÂèóÂΩ±ÈüøÁöÑÂà©ÂÆ≥Èóú‰øÇ‰∫∫Á¥çÂÖ•ÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÊúÄÂæåÁ∏ΩÁµêÊàëÂÄëÁöÑÁôºÁèæÔºåÂàóÂá∫ÂÖ≠È†ÖÈóúÈçµÂΩ±ÈüøÔºåÈÄô‰∫õÂΩ±ÈüøÊúÉÂëäÁü•Êú™‰æÜÈáùÂ∞çÂèóÂΩ±ÈüøÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÂèóÁúæË™™ÊòéÁöÑË®≠Ë®à„ÄÇ</paragraph>

##### **Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education**
2401.02985v1 by Vahid Ashrafimoghari, Necdet G√ºrkan, Jordan W. Suchow

The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÂø´ÈÄüÊºîÈÄ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÁîüÊàêÂºè AI ÁöÑÈ†òÂüüÔºåÁÇ∫ÂêÑÂÄãÈ†òÂüüÁöÑÊáâÁî®ÈñãÂïü‰∫ÜÊñ∞ÈÄîÂæëÔºå‰ΩÜÂÖ∂Âú®ÂïÜÊ•≠ÊïôËÇ≤‰∏≠ÁöÑËßíËâ≤‰ªçÊú™Ë¢´ÂÖÖÂàÜÊé¢Ë®é„ÄÇÊú¨Á†îÁ©∂È¶ñÊ¨°ÂºïÂÖ•‰∫ÜÂü∫Ê∫ñÔºåÁî®‰ª•Ë©ï‰º∞‰∏ÉÂÄã‰∏ªË¶Å LLM ÁöÑÊïàËÉΩÔºåÂåÖÊã¨ OpenAI ÁöÑÊ®°Âûã (GPT-3.5 Turbo„ÄÅGPT-4 Âíå GPT-4 Turbo)„ÄÅGoogle ÁöÑÊ®°Âûã (PaLM 2„ÄÅGemini 1.0 Pro) Âíå Anthropic ÁöÑÊ®°Âûã (Claude 2 Âíå Claude 2.1)ÔºåÈÄô‰∫õÊ®°ÂûãÂ∞áÁî®ÊñºÁ†îÁ©∂ÁîüÂïÜÊ•≠Ë™≤Á®ãÂÖ•Â≠∏Á®ãÂ∫è‰∏≠ÁöÑÈóúÈçµËÄÉË©¶ GMAT„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÈ°ØÁ§∫ÔºåÂ§ßÂ§öÊï∏ LLM ÁöÑË°®ÁèæÈÉΩÂÑ™Êñº‰∫∫È°ûËÄÉÁîüÔºåÂÖ∂‰∏≠ GPT-4 Turbo ‰∏çÂÉÖÂÑ™ÊñºÂÖ∂‰ªñÊ®°ÂûãÔºåÊõ¥Ë∂ÖË∂ä‰∫ÜÈ†ÇÂ∞ñÂïÜÂ≠∏Èô¢ÁöÑÁ†îÁ©∂ÁîüÂπ≥ÂùáÂàÜÊï∏„ÄÇÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÔºåÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü GPT-4 Turbo Âú®Ëß£ÈáãÁ≠îÊ°à„ÄÅË©ï‰º∞ÂõûÊáâ„ÄÅËæ®Ë≠òÈåØË™§„ÄÅË™øÊï¥Ë™™ÊòéÂíåÁî¢ÁîüÊõø‰ª£ÊÉÖÂ¢ÉÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇËàáÂâç‰∏Ä‰ª£ÁâàÊú¨Áõ∏ÊØîÔºåÊúÄÊñ∞ÁöÑ LLM ÁâàÊú¨ GPT-4 Turbo„ÄÅClaude 2.1 Âíå Gemini 1.0 Pro Âú®Êé®ÁêÜ‰ªªÂãôÊñπÈù¢ÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•ÔºåÂá∏È°Ø‰∫ÜÂÖ∂Âú®Ëß£Ê±∫Ë§áÈõúÂïèÈ°åÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÂÑòÁÆ° AI Âú®ÊïôËÇ≤„ÄÅË©ïÈáèÂíåËºîÂ∞éÊñπÈù¢ÁöÑÊâøË´æÂæàÊòéÁ¢∫Ôºå‰ΩÜ‰ªçÊúâÊåëÊà∞Â≠òÂú®„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂‰∏çÂÉÖÈó°Êòé‰∫Ü LLM ÁöÑÂ≠∏Ë°ìÊΩõÂäõÔºå‰πüÂº∑Ë™ø‰∫ÜÂú®ÊïôËÇ≤‰∏≠ÂØ©ÊÖéÈñãÁôºÂíåÊáâÁî® AI ÁöÑÂøÖË¶ÅÊÄß„ÄÇÈö®Ëëó AI ÊäÄË°ìÁöÑÈÄ≤Ê≠•ÔºåÂª∫Á´ã AI ‰∫íÂãïÁöÑÊû∂ÊßãÂíåÂçîÂÆö„ÄÅÈ©óË≠â AI ÁîüÊàêÁöÑÂÖßÂÆπÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÅÁ¢∫‰øùÂÖ®ÁêÉÂêÑÂú∞Â§öÂÖÉÂ≠∏ÁøíËÄÖÁöÑÂ≠òÂèñÊ¨äÔºå‰ª•ÂèäÂâµÈÄ†‰∏ÄÂÄã AI ÊîØÊåÅ‰∫∫È°ûÂ∞àÊ•≠Áü•Ë≠òÁöÑÊïôËÇ≤Áí∞Â¢ÉËá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂ÁÇ∫ÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢Ë≤†Ë≤¨‰ªªÂú∞‰ΩøÁî® AI ‰æÜË±êÂØåÊïôËÇ≤È´îÈ©ó‰∏¶ÊîπÂñÑËÄÉË©¶Ê∫ñÂÇôÂíåË©ïÈáèÊñπÊ≥ïÂ•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ

##### **XAI for In-hospital Mortality Prediction via Multimodal ICU Data**
2312.17624v1 by Xingqiao Li, Jindong Gu, Zhiyong Wang, Yancheng Yuan, Bo Du, Fengxiang He

Predicting in-hospital mortality for intensive care unit (ICU) patients is
key to final clinical outcomes. AI has shown advantaged accuracy but suffers
from the lack of explainability. To address this issue, this paper proposes an
eXplainable Multimodal Mortality Predictor (X-MMP) approaching an efficient,
explainable AI solution for predicting in-hospital mortality via multimodal ICU
data. We employ multimodal learning in our framework, which can receive
heterogeneous inputs from clinical data and make decisions. Furthermore, we
introduce an explainable method, namely Layer-Wise Propagation to Transformer,
as a proper extension of the LRP method to Transformers, producing explanations
over multimodal inputs and revealing the salient features attributed to
prediction. Moreover, the contribution of each modality to clinical outcomes
can be visualized, assisting clinicians in understanding the reasoning behind
decision-making. We construct a multimodal dataset based on MIMIC-III and
MIMIC-III Waveform Database Matched Subset. Comprehensive experiments on
benchmark datasets demonstrate that our proposed framework can achieve
reasonable interpretation with competitive prediction accuracy. In particular,
our framework can be easily transferred to other clinical tasks, which
facilitates the discovery of crucial factors in healthcare research.

ÊëòË¶ÅÔºöÈ†êÊ∏¨Âä†Ë≠∑ÁóÖÊàø (ICU) ÁóÖÊÇ£ÁöÑÈô¢ÂÖßÊ≠ª‰∫°ÁéáÊòØÊúÄÁµÇËá®Â∫äÁµêÊûúÁöÑÈóúÈçµ„ÄÇAI Â∑≤Â±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÊ∫ñÁ¢∫Â∫¶Ôºå‰ΩÜÂçªÁº∫‰πèÂèØËß£ÈáãÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÂ§öÊ®°ÂºèÊ≠ª‰∫°ÁéáÈ†êÊ∏¨Âô® (X-MMP)ÔºåÊé°Áî®ÊúâÊïà‰∏îÂèØËß£ÈáãÁöÑ AI ÊñπÂºèÔºåËóâÁî±Â§öÊ®°Âºè ICU Ë≥áÊñô‰æÜÈ†êÊ∏¨Èô¢ÂÖßÊ≠ª‰∫°Áéá„ÄÇÊàëÂÄëÂú®Êû∂Êßã‰∏≠Êé°Áî®Â§öÊ®°ÂºèÂ≠∏ÁøíÔºåÂèØ‰ª•Êé•Êî∂‰æÜËá™Ëá®Â∫äË≥áÊñôÁöÑÁï∞Ë≥™Ëº∏ÂÖ•‰∏¶ÂÅöÂá∫Ê±∫Á≠ñ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂàÜÂ±§ÂÇ≥Êí≠Ëá≥ TransformerÔºå‰ΩúÁÇ∫ LRP ÊñπÊ≥ïÈÅ©Áï∂Âú∞Âª∂‰º∏Ëá≥ TransformerÔºåÂ∞çÂ§öÊ®°ÂºèËº∏ÂÖ•Áî¢ÁîüËß£ÈáãÔºå‰∏¶Êè≠Èú≤Ê≠∏Âõ†ÊñºÈ†êÊ∏¨ÁöÑÈ°ØËëóÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊØèÂÄãÊ®°ÂºèÂ∞çËá®Â∫äÁµêÊûúÁöÑË≤¢ÁçªÂèØ‰ª•Ë¶ñË¶∫ÂåñÔºåÂçîÂä©Ëá®Â∫äÈÜ´Â∏´‰∫ÜËß£Ê±∫Á≠ñËÉåÂæåÁöÑÁêÜÁî±„ÄÇÊàëÂÄëÊ†πÊìö MIMIC-III Âíå MIMIC-III Ê≥¢ÂΩ¢Ë≥áÊñôÂ∫´ÊØîÂ∞çÂ≠êÈõÜÂª∫Êßã‰∫Ü‰∏ÄÂÄãÂ§öÊ®°ÂºèË≥áÊñôÈõÜ„ÄÇÂú®Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÁöÑÂÖ®Èù¢ÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÂèØ‰ª•ÈÅîÊàêÂêàÁêÜÁöÑË©ÆÈáãÔºå‰∏¶ÂÖ∑ÂÇôÁ´∂Áà≠ÂäõÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁöÑÊû∂ÊßãÂèØ‰ª•ËºïÈ¨ÜÂú∞ËΩâÁßªÂà∞ÂÖ∂‰ªñËá®Â∫ä‰ªªÂãôÔºåÈÄôÊúâÂä©ÊñºÂú®ÈÜ´ÁôÇ‰øùÂÅ•Á†îÁ©∂‰∏≠ÁôºÁèæÈóúÈçµÂõ†Á¥†„ÄÇ

##### **Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative**
2401.09450v2 by Norman Zerbe, Lars Ole Schwen, Christian Gei√üler, Katja Wiesemann, Tom Bisson, Peter Boor, Rita Carvalho, Michael Franz, Christoph Jansen, Tim-Rasmus Kiehl, Bj√∂rn Lindequist, Nora Charlotte Pohlan, Sarah Schmell, Klaus Strohmenger, Falk Zakrzewski, Markus Plass, Michael Takla, Tobias K√ºster, Andr√© Homeyer, Peter Hufnagl

Over the past decade, artificial intelligence (AI) methods in pathology have
advanced substantially. However, integration into routine clinical practice has
been slow due to numerous challenges, including technical and regulatory
hurdles in translating research results into clinical diagnostic products and
the lack of standardized interfaces. The open and vendor-neutral EMPAIA
initiative addresses these challenges. Here, we provide an overview of EMPAIA's
achievements and lessons learned. EMPAIA integrates various stakeholders of the
pathology AI ecosystem, i.e., pathologists, computer scientists, and industry.
In close collaboration, we developed technical interoperability standards,
recommendations for AI testing and product development, and explainability
methods. We implemented the modular and open-source EMPAIA platform and
successfully integrated 14 AI-based image analysis apps from 8 different
vendors, demonstrating how different apps can use a single standardized
interface. We prioritized requirements and evaluated the use of AI in real
clinical settings with 14 different pathology laboratories in Europe and Asia.
In addition to technical developments, we created a forum for all stakeholders
to share information and experiences on digital pathology and AI. Commercial,
clinical, and academic stakeholders can now adopt EMPAIA's common open-source
interfaces, providing a unique opportunity for large-scale standardization and
streamlining of processes. Further efforts are needed to effectively and
broadly establish AI assistance in routine laboratory use. To this end, a
sustainable infrastructure, the non-profit association EMPAIA International,
has been established to continue standardization and support broad
implementation and advocacy for an AI-assisted digital pathology future.

ÊëòË¶ÅÔºöÂú®ÈÅéÂéªÁöÑÂçÅÂπ¥‰∏≠ÔºåÁóÖÁêÜÂ≠∏‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÂ∑≤Â§ßÂπÖÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºË®±Â§öÊåëÊà∞ÔºåÂåÖÊã¨Â∞áÁ†îÁ©∂ÁµêÊûúËΩâÂåñÁÇ∫Ëá®Â∫äË®∫Êñ∑Áî¢ÂìÅÂú®ÊäÄË°ìÂíåÊ≥ïË¶èÊñπÈù¢ÁöÑÈöúÁ§ôÔºå‰ª•ÂèäÁº∫‰πèÊ®ôÊ∫ñÂåñ‰ªãÈù¢ÔºåÂ∞éËá¥Êï¥ÂêàÂà∞Â∏∏Ë¶èËá®Â∫äÂØ¶Âãô‰∏≠ÈÄ≤Â±ïÁ∑©ÊÖ¢„ÄÇÈñãÊîæ‰∏îËàá‰æõÊáâÂïÜÁÑ°ÈóúÁöÑ EMPAIA Ë®àÁï´ÊáâÂ∞ç‰∫ÜÈÄô‰∫õÊåëÊà∞„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèê‰æõ EMPAIA ÁöÑÊàêÂ∞±ÂíåÁ∂ìÈ©óÊïôË®ìÁöÑÊ¶ÇËø∞„ÄÇEMPAIA Êï¥Âêà‰∫ÜÁóÖÁêÜÂ≠∏ AI ÁîüÊÖãÁ≥ªÁµ±ÁöÑÂêÑÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÔºåÂç≥ÁóÖÁêÜÂ≠∏ÂÆ∂„ÄÅÈõªËÖ¶ÁßëÂ≠∏ÂÆ∂ÂíåÁî¢Ê•≠„ÄÇÂú®ÂØÜÂàáÂêà‰Ωú‰∏ãÔºåÊàëÂÄëÂà∂ÂÆö‰∫ÜÊäÄË°ì‰∫íÈÄöÊÄßÊ®ôÊ∫ñ„ÄÅAI Ê∏¨Ë©¶ÂíåÁî¢ÂìÅÈñãÁôºÂª∫Ë≠∞Ôºå‰ª•ÂèäÂèØËß£ÈáãÊÄßÊñπÊ≥ï„ÄÇÊàëÂÄëÂØ¶‰Ωú‰∫ÜÊ®°ÁµÑÂåñ‰∏îÈñãÊîæÂéüÂßãÁ¢ºÁöÑ EMPAIA Âπ≥Ëá∫Ôºå‰∏¶ÊàêÂäüÊï¥Âêà‰∫Ü‰æÜËá™ 8 ÂÄã‰∏çÂêå‰æõÊáâÂïÜÁöÑ 14 ÂÄãÂü∫Êñº AI ÁöÑÂΩ±ÂÉèÂàÜÊûêÊáâÁî®Á®ãÂºèÔºåÂ±ïÁ§∫‰∫Ü‰∏çÂêåÁöÑÊáâÁî®Á®ãÂºèÂ¶Ç‰Ωï‰ΩøÁî®ÂñÆ‰∏ÄÁöÑÊ®ôÊ∫ñÂåñ‰ªãÈù¢„ÄÇÊàëÂÄëÂÑ™ÂÖàËÄÉÊÖÆÈúÄÊ±ÇÔºå‰∏¶Ë©ï‰º∞‰∫Ü AI Âú®Ê≠êÊ¥≤Âíå‰∫ûÊ¥≤ÁöÑ 14 ÂÄã‰∏çÂêåÁóÖÁêÜÂØ¶È©óÂÆ§‰∏≠ÁöÑÂØ¶ÈöõËá®Â∫äÊáâÁî®„ÄÇÈô§‰∫ÜÊäÄË°ìÈñãÁôºÂ§ñÔºåÊàëÂÄëÈÇÑÁÇ∫ÊâÄÊúâÂà©ÂÆ≥Èóú‰øÇ‰∫∫Âª∫Á´ã‰∫Ü‰∏ÄÂÄãË´ñÂ£áÔºå‰ª•ÂàÜ‰∫´Êï∏‰ΩçÁóÖÁêÜÂ≠∏Âíå AI ÁöÑË≥áË®äÂíåÁ∂ìÈ©ó„ÄÇÂïÜÊ•≠„ÄÅËá®Â∫äÂíåÂ≠∏Ë°ìÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁèæÂú®ÂèØ‰ª•Êé°Áî® EMPAIA ÁöÑÂ∏∏Ë¶ãÈñãÊîæÂéüÂßãÁ¢º‰ªãÈù¢ÔºåÈÄôÁÇ∫Â§ßË¶èÊ®°Ê®ôÊ∫ñÂåñÂíåÁ∞°ÂåñÊµÅÁ®ãÊèê‰æõ‰∫ÜÁç®ÁâπÁöÑÊ©üÊúÉ„ÄÇÈúÄË¶ÅÈÄ≤‰∏ÄÊ≠•ÁöÑÂä™ÂäõÊâçËÉΩÊúâÊïà‰∏îÂª£Ê≥õÂú∞Âª∫Á´ã‰æãË°åÂØ¶È©óÂÆ§‰ΩøÁî®‰∏≠ÁöÑ AI ËºîÂä©„ÄÇÁÇ∫Ê≠§ÔºåÂ∑≤ÊàêÁ´ãÈùûÁáüÂà©ÂçîÊúÉ EMPAIA InternationalÔºå‰ª•‰ΩúÁÇ∫Ê∞∏Á∫åÂü∫Á§éÊû∂ÊßãÔºåÁπºÁ∫åÈÄ≤Ë°åÊ®ôÊ∫ñÂåñÔºå‰∏¶ÊîØÊè¥Âª£Ê≥õÂØ¶‰ΩúÂíåÂÄ°Â∞é AI ËºîÂä©Êï∏‰ΩçÁóÖÁêÜÂ≠∏ÁöÑÊú™‰æÜ„ÄÇ

##### **Robust Stochastic Graph Generator for Counterfactual Explanations**
2312.11747v2 by Mario Alfonso Prado-Romero, Bardh Prenkaj, Giovanni Stilo

Counterfactual Explanation (CE) techniques have garnered attention as a means
to provide insights to the users engaging with AI systems. While extensively
researched in domains such as medical imaging and autonomous vehicles, Graph
Counterfactual Explanation (GCE) methods have been comparatively
under-explored. GCEs generate a new graph similar to the original one, with a
different outcome grounded on the underlying predictive model. Among these GCE
techniques, those rooted in generative mechanisms have received relatively
limited investigation despite demonstrating impressive accomplishments in other
domains, such as artistic styles and natural language modelling. The preference
for generative explainers stems from their capacity to generate counterfactual
instances during inference, leveraging autonomously acquired perturbations of
the input graph. Motivated by the rationales above, our study introduces
RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual
Explanations able to produce counterfactual examples from the learned latent
space considering a partially ordered generation sequence. Furthermore, we
undertake quantitative and qualitative analyses to compare RSGG-CE's
performance against SoA generative explainers, highlighting its increased
ability to engendering plausible counterfactual candidates.

ÊëòË¶ÅÔºöÂèç‰∫ãÂØ¶Ëß£Èáã (CE) ÊäÄË°ìÂ∑≤ÂºïËµ∑ÈóúÊ≥®Ôºå‰ΩúÁÇ∫‰∏ÄÁ®ÆÁÇ∫Ëàá AI Á≥ªÁµ±‰∫íÂãïÁöÑ‰ΩøÁî®ËÄÖÊèê‰æõË¶ãËß£ÁöÑÊñπÊ≥ï„ÄÇÈõñÁÑ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂíåËá™ÂãïÈßïÈßõÊ±ΩËªäÁ≠âÈ†òÂüüÂª£Ê≥õÁ†îÁ©∂ÔºåÂúñÂΩ¢Âèç‰∫ãÂØ¶Ëß£Èáã (GCE) ÊñπÊ≥ïÁõ∏Â∞çËºÉÂ∞ëË¢´Êé¢Á¥¢„ÄÇGCE ÊúÉÁî¢Áîü‰∏ÄÂÄãÈ°û‰ººÊñºÂéüÂßãÂúñÂΩ¢ÁöÑÊñ∞ÂúñÂΩ¢Ôºå‰∏¶Ê†πÊìöÂü∫Á§éÈ†êÊ∏¨Ê®°ÂûãÁî¢Áîü‰∏çÂêåÁöÑÁµêÊûú„ÄÇÂú®ÈÄô‰∫õ GCE ÊäÄË°ì‰∏≠ÔºåÂÑòÁÆ°Âú®ÂÖ∂‰ªñÈ†òÂüüÔºà‰æãÂ¶ÇËóùË°ìÈ¢®Ê†ºÂíåËá™ÁÑ∂Ë™ûË®ÄÂª∫Ê®°Ôºâ‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊàêÂ∞±Ôºå‰ΩÜÊ§çÂü∫ÊñºÁîüÊàêÊ©üÂà∂ÁöÑÊäÄË°ìÁç≤ÂæóÁöÑÈóúÊ≥®Áõ∏Â∞çÊúâÈôê„ÄÇÂ∞çÁîüÊàêÂºèËß£ÈáãÂô®ÁöÑÂÅèÂ•ΩÊ∫êÊñºÂÆÉÂÄëÂú®Êé®ÁêÜÊúüÈñìÁî¢ÁîüÂèç‰∫ãÂØ¶ÂØ¶‰æãÁöÑËÉΩÂäõÔºåÂà©Áî®Ëº∏ÂÖ•ÂúñÂΩ¢ÁöÑËá™‰∏ªÁç≤ÂèñÊìæÂãï„ÄÇÂü∫Êñº‰∏äËø∞ÁêÜÁî±ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÂºïÂÖ•‰∫Ü RSGG-CEÔºå‰∏ÄÁ®ÆÁî®ÊñºÂèç‰∫ãÂØ¶Ëß£ÈáãÁöÑÊñ∞ÂûãÁ©©ÂÅ•Èö®Ê©üÂúñÂΩ¢ÁîüÊàêÂô®ÔºåËÉΩÂ§†ÂæûÂ≠∏ÁøíÂà∞ÁöÑÊΩõÂú®Á©∫Èñì‰∏≠Áî¢ÁîüÂèç‰∫ãÂØ¶ÁØÑ‰æãÔºåËÄÉÊÖÆÈÉ®ÂàÜÊúâÂ∫èÁöÑÁîüÊàêÂ∫èÂàó„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄ≤Ë°åÂÆöÈáèÂíåÂÆöÊÄßÂàÜÊûêÔºå‰ª•ÊØîËºÉ RSGG-CE ÁöÑÊïàËÉΩËàá SoA ÁîüÊàêÂºèËß£ÈáãÂô®ÔºåÂº∑Ë™øÂÖ∂Â¢ûÂº∑‰∫ÜÁî¢ÁîüÂêàÁêÜËß£ÈáãÂÄôÈÅ∏ÁöÑËÉΩÂäõ„ÄÇ

##### **Evaluating the Utility of Model Explanations for Model Development**
2312.06032v1 by Shawn Im, Jacob Andreas, Yilun Zhou

One of the motivations for explainable AI is to allow humans to make better
and more informed decisions regarding the use and deployment of AI models. But
careful evaluations are needed to assess whether this expectation has been
fulfilled. Current evaluations mainly focus on algorithmic properties of
explanations, and those that involve human subjects often employ subjective
questions to test human's perception of explanation usefulness, without being
grounded in objective metrics and measurements. In this work, we evaluate
whether explanations can improve human decision-making in practical scenarios
of machine learning model development. We conduct a mixed-methods user study
involving image data to evaluate saliency maps generated by SmoothGrad,
GradCAM, and an oracle explanation on two tasks: model selection and
counterfactual simulation. To our surprise, we did not find evidence of
significant improvement on these tasks when users were provided with any of the
saliency maps, even the synthetic oracle explanation designed to be simple to
understand and highly indicative of the answer. Nonetheless, explanations did
help users more accurately describe the models. These findings suggest caution
regarding the usefulness and potential for misunderstanding in saliency-based
explanations.

ÊëòË¶ÅÔºöÂèØËß£Èáã AI ÁöÑÂãïÊ©ü‰πã‰∏ÄÊòØËÆì‰∫∫ÂÄëÂú®‰ΩøÁî®ÂíåÈÉ®ÁΩ≤ AI Ê®°ÂûãÊôÇÂÅöÂá∫Êõ¥Â•Ω„ÄÅÊõ¥ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇ‰ΩÜÈúÄË¶Å‰ªîÁ¥∞Ë©ï‰º∞‰ª•Ë©ï‰º∞ÊòØÂê¶Â∑≤ÈÅîÂà∞Ê≠§È†êÊúü„ÄÇÁõÆÂâçÁöÑË©ï‰º∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ëß£ÈáãÁöÑÊºîÁÆóÊ≥ïÁâπÊÄßÔºåËÄåÊ∂âÂèä‰∫∫È°ûÂèóË©¶ËÄÖÁöÑË©ï‰º∞ÈÄöÂ∏∏Êé°Áî®‰∏ªËßÄÂïèÈ°å‰æÜÊ∏¨Ë©¶‰∫∫È°ûÂ∞çËß£ÈáãÊúâÁî®ÊÄßÁöÑÁúãÊ≥ïÔºåËÄåÊ≤íÊúâÂü∫ÊñºÂÆ¢ËßÄÊåáÊ®ôÂíåÊ∏¨Èáè„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË©ï‰º∞Ëß£ÈáãÊòØÂê¶ÂèØ‰ª•Âú®Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÈñãÁôºÁöÑÂØ¶ÈöõÂ†¥ÊôØ‰∏≠ÊîπÂñÑ‰∫∫È°ûÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÊ∂âÂèäÂΩ±ÂÉèË≥áÊñôÁöÑÊ∑∑ÂêàÊñπÊ≥ï‰ΩøÁî®ËÄÖÁ†îÁ©∂Ôºå‰ª•Ë©ï‰º∞ SmoothGrad„ÄÅGradCAM ÂíåÈ†êË®ÄËß£ÈáãÂú®ÂÖ©ÂÄã‰ªªÂãô‰∏≠Áî¢ÁîüÁöÑÈ°ØËëóÊÄßÂúñÔºöÊ®°ÂûãÈÅ∏ÊìáÂíåÂèç‰∫ãÂØ¶Ê®°Êì¨„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÊ≤íÊúâÁôºÁèæ‰ªª‰ΩïÈ°ØËëóÊÄßÂúñÔºàÂç≥‰ΩøÊòØË®≠Ë®àÁÇ∫ÊòìÊñºÁêÜËß£‰∏îÈ´òÂ∫¶ÊåáÁ§∫Á≠îÊ°àÁöÑÂêàÊàêÈ†êË®ÄËß£ÈáãÔºâËÉΩËÆì‰ΩøÁî®ËÄÖÂú®ÈÄô‰∫õ‰ªªÂãô‰∏äÈ°ØËëóÊîπÂñÑÁöÑË≠âÊìö„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåËß£ÈáãÁ¢∫ÂØ¶ÊúâÂä©Êñº‰ΩøÁî®ËÄÖÊõ¥Ê∫ñÁ¢∫Âú∞ÊèèËø∞Ê®°Âûã„ÄÇÈÄô‰∫õÁôºÁèæÊèêÁ§∫ÊàëÂÄëË¶ÅÂ∞çÂü∫ÊñºÈ°ØËëóÊÄßÁöÑËß£Èáã‰∏≠ÂèØËÉΩÂ≠òÂú®Ë™§Ëß£ÁöÑÊúâÁî®ÊÄß‰øùÊåÅË¨πÊÖé„ÄÇ

##### **Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety**
2312.06798v1 by Manas Gaur, Amit Sheth

Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.

ÊëòË¶ÅÔºöÂèØËß£ÈáãÊÄßÂíåÂÆâÂÖ®ÊÄßÂª∫Á´ã‰ø°‰ªª„ÄÇÈÄô‰∫õÈúÄË¶Å‰∏ÄÂÄãÊ®°Âûã‰æÜÂ±ïÁ§∫‰∏ÄËá¥ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÁÇ∫‰∫ÜÂØ¶ÁèæÈÄô‰∫õÔºåÊúâÂøÖË¶Å‰ΩøÁî®ÂíåÂàÜÊûêÊï∏ÊìöÂíåÁü•Ë≠òÔºå‰∏¶‰ΩøÁî®Ëàá AI ÊáâÁî®Áõ∏ÈóúÁöÑÁµ±Ë®àÂíåÁ¨¶Ëôü AI ÊñπÊ≥ï - ÂñÆÁç®‰ΩøÁî®‰ªª‰Ωï‰∏ÄÁ®ÆÊñπÊ≥ïÈÉΩ‰∏çÊúÉÂ•èÊïà„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄë‰∏ªÂºµ‰∏¶Ë©¶ÂúñË≠âÊòé NeuroSymbolic AI ÊñπÊ≥ïÊõ¥ÈÅ©ÂêàÊñº‰Ωø AI ÊàêÁÇ∫Âèó‰ø°‰ªªÁöÑ AI Á≥ªÁµ±„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü CREST Ê°ÜÊû∂ÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄËá¥ÊÄß„ÄÅÂèØÈù†ÊÄß„ÄÅ‰ΩøÁî®ËÄÖÂ±§Á¥öÁöÑÂèØËß£ÈáãÊÄßÂíåÂÆâÂÖ®ÊÄßÊòØÂ¶Ç‰ΩïÂª∫Á´ãÂú® NeuroSymbolic ÊñπÊ≥ï‰∏äÁöÑÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®Êï∏ÊìöÂíåÁü•Ë≠ò‰æÜÊîØÊåÅÈóúÈçµÊáâÁî®Ôºà‰æãÂ¶ÇÂÅ•Â∫∑ÂíåÁ¶èÁ•âÔºâÁöÑË¶ÅÊ±Ç„ÄÇÊú¨ÊñáÈáçÈªûÈóúÊ≥®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÂõ†ÁÇ∫ÂÆÉÊòØ CREST Ê°ÜÊû∂‰∏≠ÈÅ∏ÊìáÁöÑ AI Á≥ªÁµ±„ÄÇLLM Âõ†ÂÖ∂Âú®ËôïÁêÜÂª£Ê≥õÁöÑËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) Â†¥ÊôØÊñπÈù¢ÁöÑÂ§öÂäüËÉΩÊÄßËÄåÂÇôÂèóÁ†îÁ©∂‰∫∫Âì°ÁöÑÈóúÊ≥®„ÄÇ‰æãÂ¶ÇÔºåChatGPT Âíå Google ÁöÑ MedPaLM Â∑≤ÊàêÁÇ∫Êèê‰æõ‰∏ÄËà¨ÂíåÂÅ•Â∫∑Áõ∏ÈóúÊü•Ë©¢‰ø°ÊÅØÁöÑÊ•µÊúâÂ∏åÊúõÁöÑÂπ≥Âè∞„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÈÄô‰∫õÊ®°Âûã‰ªçÁÑ∂ÊòØÈªëÁõíÂ≠êÔºåÂÑòÁÆ°Á¥çÂÖ•‰∫Ü‰∫∫È°ûÂèçÈ•ãÂíåÊåá‰ª§ÂºïÂ∞éÁöÑË™øÊï¥„ÄÇ‰æãÂ¶ÇÔºåÂÑòÁÆ°Âà∂ÂÆö‰∫ÜÂÆâÂÖ®Èò≤Ë≠∑Êé™ÊñΩÔºåChatGPT ‰ªçÂèØËÉΩÁî¢Áîü‰∏çÂÆâÂÖ®ÁöÑÂõûÊáâ„ÄÇCREST ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂêàÁêÜÁöÑÊñπÊ≥ïÔºåÂú® NeuroSymbolic Ê°ÜÊû∂‰∏≠Âà©Áî®Á®ãÂ∫èÂíåÂü∫ÊñºÂúñË°®ÁöÑÁü•Ë≠òÔºå‰ª•Èó°ÊòéËàá LLM Áõ∏ÈóúÁöÑÊåëÊà∞„ÄÇ

##### **Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond**
2311.17133v1 by Jacob R. Epifano, Stephen Glass, Ravi P. Ramachandran, Sharad Patel, Aaron J. Masino, Ghulam Rasool

This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Ë∞ÉÊü•‰∫ÜÂú® COVID-19 Áñ´ÊÉÖÊúüÈó¥Âèä‰ª•ÂêéÈ¢ÑÊµãÊ≠ª‰∫°ÁéáÊó∂ÔºåÂ∑≤ÈÉ®ÁΩ≤‰∫∫Â∑•Êô∫ËÉΩ (AI) Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÅÂèØËß£ÈáäÊÄßÂíåÁ®≥ÂÅ•ÊÄß„ÄÇ‰Ωú‰∏∫ÂêåÁ±ªÁ†îÁ©∂‰∏≠ÁöÑÈ¶ñ‰æãÔºåÊàë‰ª¨ÂèëÁé∞Ë¥ùÂè∂ÊñØÁ•ûÁªèÁΩëÁªú (BNN) ÂíåÊô∫ËÉΩËÆ≠ÁªÉÊäÄÊúØËÆ©Êàë‰ª¨ÁöÑÊ®°ÂûãÂú®Êï∞ÊçÆÂèëÁîüÈáçÂ§ßÂèòÂåñÊó∂‰ªçËÉΩ‰øùÊåÅÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÂº∫Ë∞É‰∫ÜÂºÄÂèëÁ®≥ÂÅ•ÁöÑ AI Ê®°ÂûãÁöÑÈáçË¶ÅÊÄßÔºåÂç≥‰ΩøÂú®ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊù°‰ª∂‰∏ãÔºåËøô‰∫õÊ®°Âûã‰πüËÉΩÂåπÈÖçÊàñË∂ÖË∂ä‰∏¥Â∫äÂåªÁîüÁöÑÈ¢ÑÊµã„ÄÇÊàë‰ª¨ÂØπÊ®°ÂûãÂèØËß£ÈáäÊÄßÁöÑÊé¢Á¥¢Ë°®ÊòéÔºåÈöèÊú∫Ê®°Âûã‰ºö‰∫ßÁîüÊõ¥Â§öÊ†∑Âåñ‰∏î‰∏™ÊÄßÂåñÁöÑËß£ÈáäÔºå‰ªéËÄåÁ™ÅÂá∫‰∫ÜÂú®Áé∞ÂÆû‰∏ñÁïåÁöÑ‰∏¥Â∫äÁéØÂ¢É‰∏≠Êèê‰æõËØ¶ÁªÜ‰∏î‰∏™ÊÄßÂåñËßÅËß£ÁöÑ AI Ê®°ÂûãÁöÑÂøÖË¶ÅÊÄß„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Âº∫Ë∞É‰∫ÜÈáèÂåñ AI Ê®°Âûã‰∏≠‰∏çÁ°ÆÂÆöÊÄßÁöÑÈáçË¶ÅÊÄßÔºåËøô‰Ωø‰∏¥Â∫äÂåªÁîüËÉΩÂ§üÊ†πÊçÆÂèØÈù†ÁöÑÈ¢ÑÊµãÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÂÜ≥Á≠ñ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÊèêÂÄ°Âú®ÂåªÁñó‰øùÂÅ•ÁöÑ AI Á†îÁ©∂‰∏≠‰ºòÂÖàËÄÉËôëÂÆûÊñΩÁßëÂ≠¶ÔºåÂπ∂Á°Æ‰øù AI Ëß£ÂÜ≥ÊñπÊ°àÂú®Áé∞ÂÆû‰∏ñÁïåÁöÑ‰∏¥Â∫äÁéØÂ¢É‰∏≠ÂÆûÁî®„ÄÅÊúâÁõä‰∏îÂèØÊåÅÁª≠„ÄÇÈÄöËøáËß£ÂÜ≥ÂåªÁñó‰øùÂÅ•ÁéØÂ¢É‰∏≠ÁöÑÁã¨ÁâπÊåëÊàòÂíåÂ§çÊùÇÊÄßÔºåÁ†îÁ©∂‰∫∫ÂëòÂèØ‰ª•ÂºÄÂèëÂá∫ÊúâÊïàÊîπÂñÑ‰∏¥Â∫äÂÆûË∑µÂíåÊÇ£ËÄÖÈ¢ÑÂêéÁöÑ AI Ê®°Âûã„ÄÇ

##### **Variational Autoencoders for Feature Exploration and Malignancy Prediction of Lung Lesions**
2311.15719v1 by Benjamin Keel, Aaron Quyn, David Jayne, Samuel D. Relton

Lung cancer is responsible for 21% of cancer deaths in the UK and five-year
survival rates are heavily influenced by the stage the cancer was identified
at. Recent studies have demonstrated the capability of AI methods for accurate
and early diagnosis of lung cancer from routine scans. However, this evidence
has not translated into clinical practice with one barrier being a lack of
interpretable models. This study investigates the application Variational
Autoencoders (VAEs), a type of generative AI model, to lung cancer lesions.
Proposed models were trained on lesions extracted from 3D CT scans in the
LIDC-IDRI public dataset. Latent vector representations of 2D slices produced
by the VAEs were explored through clustering to justify their quality and used
in an MLP classifier model for lung cancer diagnosis, the best model achieved
state-of-the-art metrics of AUC 0.98 and 93.1% accuracy. Cluster analysis shows
the VAE latent space separates the dataset of malignant and benign lesions
based on meaningful feature components including tumour size, shape, patient
and malignancy class. We also include a comparative analysis of the standard
Gaussian VAE (GVAE) and the more recent Dirichlet VAE (DirVAE), which replaces
the prior with a Dirichlet distribution to encourage a more explainable latent
space with disentangled feature representation. Finally, we demonstrate the
potential for latent space traversals corresponding to clinically meaningful
feature changes.

ÊëòË¶ÅÔºöËÇ∫ÁôåÂç†Ëã±ÂúãÁôåÁóáÊ≠ª‰∫°‰∫∫Êï∏ÁöÑ 21%Ôºå‰∫îÂπ¥Â≠òÊ¥ªÁéáÂæàÂ§ßÁ®ãÂ∫¶ÂèñÊ±∫ÊñºÁôåÁóáË¢´ÁôºÁèæÁöÑÈöéÊÆµ„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Â∑≤Ë≠âÊòé‰∫∫Â∑•Êô∫ËÉΩÊñπÊ≥ïÂÖ∑ÊúâÂæû‰æãË°åÊéÉÊèè‰∏≠Ê∫ñÁ¢∫ÂèäÊó©Ë®∫Êñ∑ËÇ∫ÁôåÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÊ≠§Ë≠âÊìöÂ∞öÊú™ËΩâÂåñÁÇ∫Ëá®Â∫äÂØ¶ÂãôÔºåÂÖ∂‰∏≠‰∏ÄÂÄãÈöúÁ§ôÊòØÁº∫‰πèÂèØËß£ÈáãÁöÑÊ®°Âûã„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÊáâÁî®ËÆäÂàÜËá™ÂãïÁ∑®Á¢ºÂô® (VAE)Ôºå‰∏ÄÁ®ÆÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÔºåÊñºËÇ∫ÁôåÁóÖÁÅ∂„ÄÇÂ∞áÊèêÂá∫ÁöÑÊ®°ÂûãË®ìÁ∑¥ÊñºÂæû LIDC-IDRI ÂÖ¨ÂÖ±Êï∏ÊìöÈõÜ‰∏≠ÊèêÂèñÁöÑ 3D ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÁóÖÁÅ∂„ÄÇÈÄöÈÅéËÅöÈ°ûÊé¢Á¥¢‰∫Ü VAE ÁîüÊàêÁöÑ 2D ÂàáÁâáÁöÑÊΩõÂú®ÂêëÈáèË°®Á§∫Ôºå‰ª•Ë≠âÊòéÂÖ∂ÂìÅË≥™Ôºå‰∏¶Áî®ÊñºËÇ∫ÁôåË®∫Êñ∑ÁöÑ MLP ÂàÜÈ°ûÂô®Ê®°ÂûãÔºåÊúÄ‰Ω≥Ê®°ÂûãÈÅîÂà∞‰∫Ü AUC 0.98 Âíå 93.1% Ê∫ñÁ¢∫Â∫¶ÁöÑÊúÄÂÖàÈÄ≤ÊåáÊ®ô„ÄÇËÅöÈ°ûÂàÜÊûêÈ°ØÁ§∫ÔºåVAE ÊΩõÂú®Á©∫ÈñìÊ†πÊìöÊúâÊÑèÁæ©ÁöÑÁâπÂæµÁµÑÊàêÔºàÂåÖÊã¨ËÖ´Áò§Â§ßÂ∞è„ÄÅÂΩ¢ÁãÄ„ÄÅÊÇ£ËÄÖÂíåÊÉ°ÊÄßÈ°ûÂà•ÔºâÂ∞áÊÉ°ÊÄßÂíåËâØÊÄßÁóÖÁÅ∂ÁöÑÊï∏ÊìöÈõÜÂàÜÈñã„ÄÇÊàëÂÄëÈÇÑÂåÖÊã¨Ê®ôÊ∫ñÈ´òÊñØ VAE (GVAE) ÂíåÊõ¥Êñ∞ÁöÑÁãÑÂà©ÂÖãÈõ∑ VAE (DirVAE) ÁöÑÊØîËºÉÂàÜÊûêÔºåÂæåËÄÖÁî®ÁãÑÂà©ÂÖãÈõ∑ÂàÜ‰ΩàÂèñ‰ª£ÂÖàÈ©óÔºå‰ª•‰øÉÈÄ≤ÂÖ∑ÊúâËß£ÈñãÁâπÂæµË°®Á§∫ÁöÑÊõ¥ÂÖ∑ÂèØËß£ÈáãÊÄßÁöÑÊΩõÂú®Á©∫Èñì„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜËàáËá®Â∫äÊúâÊÑèÁæ©ÁöÑÁâπÂæµËÆäÂåñÁõ∏ÊáâÁöÑÊΩõÂú®Á©∫ÈñìÊ©´Ë∂äÁöÑÊΩõÂäõ„ÄÇ

##### **MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting**
2311.14471v1 by Nathan Blake, Hana Chockler, David A. Kelly, Santiago Calderon Pena, Akchunya Chanchal

Existing tools for explaining the output of image classifiers can be divided
into white-box, which rely on access to the model internals, and black-box,
agnostic to the model. As the usage of AI in the medical domain grows, so too
does the usage of explainability tools. Existing work on medical image
explanations focuses on white-box tools, such as gradcam. However, there are
clear advantages to switching to a black-box tool, including the ability to use
it with any classifier and the wide selection of black-box tools available. On
standard images, black-box tools are as precise as white-box. In this paper we
compare the performance of several black-box methods against gradcam on a brain
cancer MRI dataset. We demonstrate that most black-box tools are not suitable
for explaining medical image classifications and present a detailed analysis of
the reasons for their shortcomings. We also show that one black-box tool, a
causal explainability-based rex, performs as well as \gradcam.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÂúñÂÉèÂàÜÈ°ûÂô®Ëº∏Âá∫Ëß£ÈáãÂ∑•ÂÖ∑ÂèØÂàÜÁÇ∫‰æùË≥¥ÊñºÊ®°ÂûãÂÖßÈÉ®Â≠òÂèñÊ¨äÈôêÁöÑÁôΩÁõíÔºå‰ª•ÂèäËàáÊ®°ÂûãÁÑ°ÈóúÁöÑÈªëÁõí„ÄÇÈö®Ëëó AI Âú®ÈÜ´ÁôÇÈ†òÂüüÁöÑ‰ΩøÁî®Â¢ûÂä†ÔºåÂèØËß£ÈáãÊÄßÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®‰πüÈö®‰πãÂ¢ûÂä†„ÄÇÁèæÊúâÈÜ´Â≠∏ÂΩ±ÂÉèËß£ÈáãÁöÑÂ∑•‰ΩúÈáçÈªûÂú®ÊñºÁôΩÁõíÂ∑•ÂÖ∑Ôºå‰æãÂ¶Ç gradcam„ÄÇÁÑ∂ËÄåÔºåÂàáÊèõÂà∞ÈªëÁõíÂ∑•ÂÖ∑ÊúâÊòéÈ°ØÁöÑÂÑ™ÈªûÔºåÂåÖÊã¨ËÉΩÂ§†Ëàá‰ªª‰ΩïÂàÜÈ°ûÂô®‰∏ÄËµ∑‰ΩøÁî®Ôºå‰ª•ÂèäÂª£Ê≥õÁöÑÈªëÁõíÂ∑•ÂÖ∑ÂèØ‰æõÈÅ∏Êìá„ÄÇÂú®Ê®ôÊ∫ñÂΩ±ÂÉè‰∏äÔºåÈªëÁõíÂ∑•ÂÖ∑ËàáÁôΩÁõí‰∏ÄÊ®£Á≤æÁ¢∫„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊØîËºÉ‰∫ÜÂ§öÁ®ÆÈªëÁõíÊñπÊ≥ïÂú®ËÖ¶Áôå MRI Ë≥áÊñôÈõÜ‰∏äËàá gradcam ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëË≠âÊòéÂ§ßÂ§öÊï∏ÈªëÁõíÂ∑•ÂÖ∑‰∏çÈÅ©ÂêàËß£ÈáãÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûÔºå‰∏¶Ë©≥Á¥∞ÂàÜÊûêÂÖ∂Áº∫ÈªûÁöÑÂéüÂõ†„ÄÇÊàëÂÄëÈÇÑË°®Êòé‰∏ÄÁ®ÆÈªëÁõíÂ∑•ÂÖ∑ÔºåÂü∫ÊñºÂõ†ÊûúÂèØËß£ÈáãÊÄßÁöÑ rexÔºåË°®ÁèæËàá \gradcam ‰∏ÄÊ®£Â•Ω„ÄÇ

##### **Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries**
2311.12573v2 by Robert Gorwa, Michael Veale

The AI development community is increasingly making use of hosting
intermediaries such as Hugging Face provide easy access to user-uploaded models
and training data. These model marketplaces lower technical deployment barriers
for hundreds of thousands of users, yet can be used in numerous potentially
harmful and illegal ways. In this article, we explain ways in which AI systems,
which can both `contain' content and be open-ended tools, present one of the
trickiest platform governance challenges seen to date. We provide case studies
of several incidents across three illustrative platforms -- Hugging Face,
GitHub and Civitai -- to examine how model marketplaces moderate models.
Building on this analysis, we outline important (and yet nevertheless limited)
practices that industry has been developing to respond to moderation demands:
licensing, access and use restrictions, automated content moderation, and open
policy development. While the policy challenge at hand is a considerable one,
we conclude with some ideas as to how platforms could better mobilize resources
to act as a careful, fair, and proportionate regulatory access point.

ÊëòË¶ÅÔºöAI ÁôºÂ±ïÁ§æÁæ§Êó•ÁõäÂà©Áî® Hugging Face Á≠âË®óÁÆ°‰∏≠‰ªãÔºåÊèê‰æõ‰ΩøÁî®ËÄÖ‰∏äÂÇ≥‰πãÊ®°ÂûãËàáË®ìÁ∑¥Ë≥áÊñôÁöÑÁ∞°ÊòìÂèñÂæóÁÆ°ÈÅì„ÄÇÈÄô‰∫õÊ®°ÂûãÂ∏ÇÈõÜÈôç‰Ωé‰∫ÜÊï∏ÂçÅËê¨Âêç‰ΩøÁî®ËÄÖÁöÑÊäÄË°ìÈÉ®ÁΩ≤ÈñÄÊ™ªÔºå‰ΩÜÂçªÂèØËÉΩË¢´Áî®ÊñºË®±Â§öÊΩõÂú®ÊúâÂÆ≥‰∏îÈùûÊ≥ïÁöÑÁî®ÈÄî„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË™™Êòé‰∫Ü AI Á≥ªÁµ±Êó¢ÂèØ‰ª•„ÄåÂåÖÂê´„ÄçÂÖßÂÆπÔºå‰πüÂèØ‰ª•‰ΩúÁÇ∫ÈñãÊîæÂºèÂ∑•ÂÖ∑ÔºåÈÄôÊèêÂá∫‰∫ÜËøÑ‰ªäÁÇ∫Ê≠¢ÊúÄÊ£òÊâãÁöÑÂπ≥Âè∞Ê≤ªÁêÜÊåëÊà∞‰πã‰∏Ä„ÄÇÊàëÂÄëÊèê‰æõ Hugging Face„ÄÅGitHub Âíå Civitai Á≠â‰∏âÂÄãË™™ÊòéÊÄßÂπ≥Âè∞‰∏äÊï∏Ëµ∑‰∫ã‰ª∂ÁöÑÊ°à‰æãÁ†îÁ©∂Ôºå‰ª•Êé¢Ë®éÊ®°ÂûãÂ∏ÇÈõÜÂ¶Ç‰ΩïÊéßÁÆ°Ê®°Âûã„ÄÇÊ†πÊìöÊ≠§ÂàÜÊûêÔºåÊàëÂÄëÊ¶ÇËø∞‰∫ÜÁî¢Ê•≠ÁÇ∫ÂõûÊáâÊéßÁÆ°ÈúÄÊ±ÇËÄåÁôºÂ±ïÁöÑÈáçË¶ÅÔºà‰ΩÜ‰ªçÊúâÈôêÔºâÂØ¶ÂãôÔºöÊéàÊ¨ä„ÄÅÂ≠òÂèñÂíå‰ΩøÁî®ÈôêÂà∂„ÄÅËá™ÂãïÂÖßÂÆπÊéßÁÆ°ÂíåÈñãÊîæÂºèÊîøÁ≠ñÁôºÂ±ï„ÄÇÂÑòÁÆ°ÁõÆÂâçÈù¢Ëá®ÁöÑÊîøÁ≠ñÊåëÊà∞Áõ∏Áï∂Âö¥Â≥ªÔºåÊàëÂÄë‰ªçÊèêÂá∫‰∫Ü‰∏Ä‰∫õÊÉ≥Ê≥ïÔºåË™™ÊòéÂπ≥Âè∞Â¶Ç‰ΩïËÉΩÊõ¥Â•ΩÂú∞ÂãïÂì°Ë≥áÊ∫êÔºå‰ΩúÁÇ∫Ë¨πÊÖé„ÄÅÂÖ¨Âπ≥ÂíåÈÅ©Â∫¶ÁöÑÊ≥ïË¶èÂ≠òÂèñÈªû„ÄÇ

##### **Ovarian Cancer Data Analysis using Deep Learning: A Systematic Review from the Perspectives of Key Features of Data Analysis and AI Assurance**
2311.11932v1 by Muta Tah Hira, Mohammad A. Razzaque, Mosharraf Sarker

Background and objectives: By extracting this information, Machine or Deep
Learning (ML/DL)-based autonomous data analysis tools can assist clinicians and
cancer researchers in discovering patterns and relationships from complex data
sets. Many DL-based analyses on ovarian cancer (OC) data have recently been
published. These analyses are highly diverse in various aspects of cancer
(e.g., subdomain(s) and cancer type they address) and data analysis features.
However, a comprehensive understanding of these analyses in terms of these
features and AI assurance (AIA) is currently lacking. This systematic review
aims to fill this gap by examining the existing literature and identifying
important aspects of OC data analysis using DL, explicitly focusing on the key
features and AI assurance perspectives. Methods: The PRISMA framework was used
to conduct comprehensive searches in three journal databases. Only studies
published between 2015 and 2023 in peer-reviewed journals were included in the
analysis. Results: In the review, a total of 96 DL-driven analyses were
examined. The findings reveal several important insights regarding DL-driven
ovarian cancer data analysis: - Most studies 71% (68 out of 96) focused on
detection and diagnosis, while no study addressed the prediction and prevention
of OC. - The analyses were predominantly based on samples from a non-diverse
population (75% (72/96 studies)), limited to a geographic location or country.
- Only a small proportion of studies (only 33% (32/96)) performed integrated
analyses, most of which used homogeneous data (clinical or omics). - Notably, a
mere 8.3% (8/96) of the studies validated their models using external and
diverse data sets, highlighting the need for enhanced model validation, and -
The inclusion of AIA in cancer data analysis is in a very early stage; only
2.1% (2/96) explicitly addressed AIA through explainability.

ÊëòË¶ÅÔºö<paragraph>ËÉåÊôØÂíåÁõÆÊ®ôÔºöÈÄöÈÅéÊèêÂèñÈÄô‰∫õË≥áË®äÔºåÊ©üÂô®ÊàñÊ∑±Â∫¶Â≠∏Áøí (ML/DL) Âü∫ÊñºËá™‰∏ªÊï∏ÊìöÂàÜÊûêÂ∑•ÂÖ∑ÂèØ‰ª•ÂçîÂä©Ëá®Â∫äÈÜ´ÁîüÂíåÁôåÁóáÁ†îÁ©∂‰∫∫Âì°ÂæûË§áÈõúÁöÑÊï∏ÊìöÈõÜ‰∏≠ÁôºÁèæÊ®°ÂºèÂíåÈóú‰øÇ„ÄÇÊúÄËøëÂ∑≤ÁôºË°®Ë®±Â§öÂü∫Êñº DL ÁöÑÂçµÂ∑¢Áôå (OC) Êï∏ÊìöÂàÜÊûê„ÄÇÈÄô‰∫õÂàÜÊûêÂú®ÁôåÁóáÁöÑÂêÑÂÄãÊñπÈù¢Ôºà‰æãÂ¶ÇÔºåÂÆÉÂÄëÊ∂âÂèäÁöÑÂ≠êÈ†òÂüüÂíåÁôåÁóáÈ°ûÂûãÔºâÂíåÊï∏ÊìöÂàÜÊûêÂäüËÉΩÊñπÈù¢È´òÂ∫¶Â§öÊ®£Âåñ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁº∫‰πèÂ∞çÈÄô‰∫õÂàÜÊûêÂú®ÈÄô‰∫õÁâπÂæµÂíå AI ‰øùË≠â (AIA) ÊñπÈù¢ÁöÑÂÖ®Èù¢ÁêÜËß£„ÄÇÈÄôÁØáÁ≥ªÁµ±ÊÄßÂõûÈ°ßÊó®Âú®ÈÄöÈÅéÊ™¢Ë¶ñÁèæÊúâÊñáÁçª‰∏¶ÊòéÁ¢∫ÈóúÊ≥®ÈóúÈçµÁâπÂæµÂíå AI ‰øùË≠âËßÄÈªûÔºå‰æÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩ„ÄÇÊñπÊ≥ïÔºö‰ΩøÁî® PRISMA Êû∂ÊßãÂú®‰∏âÂÄãÊúüÂàäË≥áÊñôÂ∫´‰∏≠ÈÄ≤Ë°åÂÖ®Èù¢ÊêúÂ∞ã„ÄÇÂàÜÊûêÂÉÖÂåÖÊã¨ 2015 Âπ¥Ëá≥ 2023 Âπ¥ÈñìÁôºË°®ÊñºÂêåË°åË©ïÂØ©ÊúüÂàäÁöÑÁ†îÁ©∂„ÄÇÁµêÊûúÔºöÂú®ÂõûÈ°ß‰∏≠ÔºåÁ∏ΩÂÖ±Ê™¢Ë¶ñ‰∫Ü 96 È†ÖÁî± DL È©ÖÂãïÁöÑÂàÜÊûê„ÄÇÁ†îÁ©∂ÁµêÊûúÊè≠Á§∫‰∫ÜÂπæÂÄãÈóúÊñºÁî± DL È©ÖÂãïÁöÑÂçµÂ∑¢ÁôåÊï∏ÊìöÂàÜÊûêÁöÑÈáçË¶ÅË¶ãËß£Ôºö- Â§ßÂ§öÊï∏Á†îÁ©∂ 71%Ôºà96 È†Ö‰∏≠Êúâ 68 È†ÖÔºâÂ∞àÊ≥®ÊñºÊ™¢Ê∏¨ÂíåË®∫Êñ∑ÔºåËÄåÊ≤íÊúâÁ†îÁ©∂Êé¢Ë®é OC ÁöÑÈ†êÊ∏¨ÂíåÈ†êÈò≤„ÄÇ- ÈÄô‰∫õÂàÜÊûê‰∏ªË¶ÅÂü∫Êñº‰æÜËá™ÈùûÂ§öÂÖÉÊóèÁæ§ÁöÑÊ®£Êú¨Ôºà75%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 72 È†ÖÔºâÔºâÔºåÂÉÖÈôêÊñºÊüêÂÄãÂú∞ÁêÜ‰ΩçÁΩÆÊàñÂúãÂÆ∂„ÄÇ- Âè™ÊúâÂ∞ëÈÉ®ÂàÜÁ†îÁ©∂ÔºàÂÉÖ 33%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 32 È†ÖÔºâÂü∑Ë°åÊï¥ÂêàÂàÜÊûêÔºåÂÖ∂‰∏≠Â§ßÂ§öÊï∏‰ΩøÁî®ÂêåË≥™Êï∏ÊìöÔºàËá®Â∫äÊàñÁµÑÂ≠∏Ôºâ„ÄÇ- ÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂè™Êúâ 8.3%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 8 È†ÖÔºâ‰ΩøÁî®Â§ñÈÉ®ÂíåÂ§öÂÖÉÊï∏ÊìöÈõÜÈ©óË≠â‰∫ÜÂÖ∂Ê®°ÂûãÔºåÂº∑Ë™ø‰∫ÜÂä†Âº∑Ê®°ÂûãÈ©óË≠âÁöÑÂøÖË¶ÅÊÄßÔºå‰ª•Âèä- Â∞á AIA Á¥çÂÖ•ÁôåÁóáÊï∏ÊìöÂàÜÊûê‰ªçËôïÊñºÈùûÂ∏∏Êó©ÊúüÁöÑÈöéÊÆµÔºõÂè™Êúâ 2.1%Ôºà96 È†ÖÁ†îÁ©∂‰∏≠ÁöÑ 2 È†ÖÔºâÈÄèÈÅéÂèØËß£ÈáãÊÄßÊòéÁ¢∫Êé¢Ë®é‰∫Ü AIA„ÄÇ</paragraph>

##### **Representing visual classification as a linear combination of words**
2311.10933v1 by Shobhit Agarwal, Yevgeniy R. Semenov, William Lotter

Explainability is a longstanding challenge in deep learning, especially in
high-stakes domains like healthcare. Common explainability methods highlight
image regions that drive an AI model's decision. Humans, however, heavily rely
on language to convey explanations of not only "where" but "what".
Additionally, most explainability approaches focus on explaining individual AI
predictions, rather than describing the features used by an AI model in
general. The latter would be especially useful for model and dataset auditing,
and potentially even knowledge generation as AI is increasingly being used in
novel tasks. Here, we present an explainability strategy that uses a
vision-language model to identify language-based descriptors of a visual
classification task. By leveraging a pre-trained joint embedding space between
images and text, our approach estimates a new classification task as a linear
combination of words, resulting in a weight for each word that indicates its
alignment with the vision-based classifier. We assess our approach using two
medical imaging classification tasks, where we find that the resulting
descriptors largely align with clinical knowledge despite a lack of
domain-specific language training. However, our approach also identifies the
potential for 'shortcut connections' in the public datasets used. Towards a
functional measure of explainability, we perform a pilot reader study where we
find that the AI-identified words can enable non-expert humans to perform a
specialized medical task at a non-trivial level. Altogether, our results
emphasize the potential of using multimodal foundational models to deliver
intuitive, language-based explanations of visual tasks.

ÊëòË¶ÅÔºö<paragraph>Ëß£ÈáãÊÄßÊòØÊ∑±Â∫¶Â≠∏Áøí‰∏≠Èï∑ÊúüÁöÑÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇ‰øùÂÅ•Á≠âÈ´òÈ¢®Èö™È†òÂüü„ÄÇÂ∏∏Ë¶ãÁöÑËß£ÈáãÊÄßÊñπÊ≥ïÊúÉÂº∑Ë™øÈ©ÖÂãï AI Ê®°ÂûãÊ±∫Á≠ñÁöÑÂΩ±ÂÉèÂçÄÂüü„ÄÇÁÑ∂ËÄåÔºå‰∫∫È°ûÂæàÂ§ßÁ®ãÂ∫¶‰æùË≥¥Ë™ûË®Ä‰æÜÂÇ≥ÈÅî‰∏çÂÉÖÊòØ„ÄåÂú®Âì™Ë£°„ÄçÔºåÈÇÑÊúâ„ÄåÊòØ‰ªÄÈ∫º„ÄçÁöÑËß£Èáã„ÄÇÊ≠§Â§ñÔºåÂ§ßÂ§öÊï∏Ëß£ÈáãÊÄßÊñπÊ≥ïÈÉΩÂ∞àÊ≥®ÊñºËß£ÈáãÂÄãÂà• AI È†êÊ∏¨ÔºåËÄå‰∏çÊòØÊèèËø∞ AI Ê®°Âûã‰∏ÄËà¨‰ΩøÁî®ÁöÑÁâπÂæµ„ÄÇÂæåËÄÖÂ∞çÊñºÊ®°ÂûãÂíåË≥áÊñôÈõÜÁ®ΩÊ†∏ÁâπÂà•ÊúâÁî®ÔºåÁîöËá≥ÂèØËÉΩÂú® AI ÊÑà‰æÜÊÑàÁî®ÊñºÊñ∞Á©é‰ªªÂãôÊôÇÁî¢ÁîüÁü•Ë≠ò„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰ΩøÁî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã‰æÜËæ®Ë≠òË¶ñË¶∫ÂàÜÈ°û‰ªªÂãôÁöÑË™ûË®ÄÊèèËø∞Á¨¶ÁöÑËß£ÈáãÊÄßÁ≠ñÁï•„ÄÇÈÄèÈÅéÂà©Áî®ÂΩ±ÂÉèÂíåÊñáÂ≠ó‰πãÈñìÈ†êÂÖàË®ìÁ∑¥ÁöÑËÅØÂêàÂµåÂÖ•Á©∫ÈñìÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ∞áÊñ∞ÁöÑÂàÜÈ°û‰ªªÂãô‰º∞Ë®àÁÇ∫‰∏ÄÂÄãÁ∑öÊÄßÊñáÂ≠óÁµÑÂêàÔºåÂ∞éËá¥ÊØèÂÄãÊñáÂ≠óÈÉΩÊúâÊ¨äÈáçÔºåË°®Á§∫ÂÆÉËàáÂü∫ÊñºË¶ñË¶∫ÁöÑÂàÜÈ°ûÂô®Â∞çÈΩä„ÄÇÊàëÂÄë‰ΩøÁî®ÂÖ©ÂÄãÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãô‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÂÅöÊ≥ïÔºåÊàëÂÄëÁôºÁèæÁî¢ÁîüÁöÑÊèèËø∞Á¨¶Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äËàáËá®Â∫äÁü•Ë≠ò‰∏ÄËá¥ÔºåÂÑòÁÆ°Áº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑË™ûË®ÄË®ìÁ∑¥„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÁöÑÂÅöÊ≥ï‰πüÁôºÁèæ‰∫ÜÊâÄÁî®ÂÖ¨ÈñãË≥áÊñôÈõÜ‰∏≠ÁöÑ„ÄåÊç∑ÂæëÈÄ£Á∑ö„ÄçÁöÑÂèØËÉΩÊÄß„ÄÇÁÇ∫‰∫ÜÈÅîÂà∞Ëß£ÈáãÊÄßÁöÑÂäüËÉΩÊÄßË°°ÈáèÔºåÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖË©¶È©óËÆÄËÄÖÁ†îÁ©∂ÔºåÁôºÁèæ AI Ë≠òÂà•ÁöÑÊñáÂ≠óËÉΩËÆìÈùûÂ∞àÂÆ∂‰∫∫È°ûÂú®ÈùûÂπ≥Âá°ÁöÑÂ±§Á¥öÂü∑Ë°åÂ∞àÊ•≠ÁöÑÈÜ´ÁôÇ‰ªªÂãô„ÄÇÁ∏Ω‰πãÔºåÊàëÂÄëÁöÑÁµêÊûúÂº∑Ë™ø‰∫Ü‰ΩøÁî®Â§öÊ®°ÂºèÂü∫Á§éÊ®°Âûã‰æÜÊèê‰æõÁõ¥ËßÄÁöÑ„ÄÅÂü∫ÊñºË™ûË®ÄÁöÑË¶ñË¶∫‰ªªÂãôËß£ÈáãÁöÑÊΩõÂäõ„ÄÇ</paragraph>

##### **Towards objective and systematic evaluation of bias in medical imaging AI**
2311.02115v1 by Emma A. M. Stanley, Raissa Souza, Anthony Winder, Vedant Gulve, Kimberly Amador, Matthias Wilms, Nils D. Forkert

Artificial intelligence (AI) models trained using medical images for clinical
tasks often exhibit bias in the form of disparities in performance between
subgroups. Since not all sources of biases in real-world medical imaging data
are easily identifiable, it is challenging to comprehensively assess how those
biases are encoded in models, and how capable bias mitigation methods are at
ameliorating performance disparities. In this article, we introduce a novel
analysis framework for systematically and objectively investigating the impact
of biases in medical images on AI models. We developed and tested this
framework for conducting controlled in silico trials to assess bias in medical
imaging AI using a tool for generating synthetic magnetic resonance images with
known disease effects and sources of bias. The feasibility is showcased by
using three counterfactual bias scenarios to measure the impact of simulated
bias effects on a convolutional neural network (CNN) classifier and the
efficacy of three bias mitigation strategies. The analysis revealed that the
simulated biases resulted in expected subgroup performance disparities when the
CNN was trained on the synthetic datasets. Moreover, reweighing was identified
as the most successful bias mitigation strategy for this setup, and we
demonstrated how explainable AI methods can aid in investigating the
manifestation of bias in the model using this framework. Developing fair AI
models is a considerable challenge given that many and often unknown sources of
biases can be present in medical imaging datasets. In this work, we present a
novel methodology to objectively study the impact of biases and mitigation
strategies on deep learning pipelines, which can support the development of
clinical AI that is robust and responsible.

ÊëòË¶ÅÔºö<paragraph>‰ΩøÁî®ÈÜ´Â≠∏ÂΩ±ÂÉèË®ìÁ∑¥Áî®ÊñºËá®Â∫ä‰ªªÂãôÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÔºåÂ∏∏ÊúÉÂ±ïÁèæÂá∫ÊïàËÉΩÂ∑ÆÁï∞ÁöÑÂΩ¢ÂºèÂÅèË™§ÔºåÈÄô‰∫õÂ∑ÆÁï∞Â≠òÂú®ÊñºÊ¨°Áæ§ÁµÑ‰πãÈñì„ÄÇÁî±Êñº‰∏¶ÈùûÊâÄÊúâÁúüÂØ¶‰∏ñÁïåÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñô‰∏≠ÁöÑÂÅèË™§‰æÜÊ∫êÈÉΩÂÆπÊòìËæ®Ë≠òÔºåÂõ†Ê≠§ÂÖ®Èù¢Ë©ï‰º∞ÈÄô‰∫õÂÅèË™§Â¶Ç‰ΩïÁ∑®Á¢ºÂú®Ê®°Âûã‰∏≠Ôºå‰ª•ÂèäÂÅèË™§Á∑©Ëß£ÊñπÊ≥ïÂú®ÊîπÂñÑÊïàËÉΩÂ∑ÆÁï∞ÊñπÈù¢ÁöÑËÉΩÂäõÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∏ÄÂÄãÊñ∞Á©éÁöÑÂàÜÊûêÊû∂ÊßãÔºåÁî®ÊñºÁ≥ªÁµ±ÊÄß‰∏îÂÆ¢ËßÄÂú∞Ë™øÊü•ÈÜ´Â≠∏ÂΩ±ÂÉè‰∏≠ÁöÑÂÅèË™§Â∞ç AI Ê®°ÂûãÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÈñãÁôº‰∏¶Ê∏¨Ë©¶‰∫ÜÈÄôÂÄãÊû∂ÊßãÔºåÁî®ÊñºÂü∑Ë°åÂèóÊéßÁöÑÈõªËÖ¶Ê®°Êì¨Ë©¶È©óÔºå‰ª•Ë©ï‰º∞ÈÜ´Â≠∏ÂΩ±ÂÉè AI ‰∏≠ÁöÑÂÅèË™§Ôºå‰∏¶‰ΩøÁî®‰∏ÄÂÄãÂ∑•ÂÖ∑‰æÜÁî¢ÁîüÂ∑≤Áü•ÁñæÁóÖÂΩ±ÈüøÂíåÂÅèË™§‰æÜÊ∫êÁöÑÂêàÊàêÁ£ÅÂÖ±ÊåØÂΩ±ÂÉè„ÄÇÂèØË°åÊÄßÈÄèÈÅé‰ΩøÁî®‰∏âÂÄãÂèç‰∫ãÂØ¶ÂÅèË™§ÊÉÖÂ¢É‰æÜË°°ÈáèÊ®°Êì¨ÂÅèË™§ÂΩ±ÈüøÂ∞çÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂàÜÈ°ûÂô®Âíå‰∏âÂÄãÂÅèË™§Á∑©Ëß£Á≠ñÁï•ÁöÑÊïàËÉΩ„ÄÇÂàÜÊûêÈ°ØÁ§∫ÔºåÁï∂ CNN Âú®ÂêàÊàêË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÊôÇÔºåÊ®°Êì¨ÂÅèË™§ÊúÉÂ∞éËá¥È†êÊúüÁöÑÊ¨°Áæ§ÁµÑÊïàËÉΩÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÈáçÊñ∞Âä†Ê¨äË¢´Ë¶ñÁÇ∫Ê≠§Ë®≠ÂÆö‰∏≠ÊúÄÊàêÂäüÁöÑÂÅèË™§Á∑©Ëß£Á≠ñÁï•ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜËß£ÈáãÊÄß AI ÊñπÊ≥ïÂ¶Ç‰ΩïÂçîÂä©Ë™øÊü•Ê®°Âûã‰∏≠ÂÅèË™§ÁöÑË°®ÁèæÔºåÊñπÊ≥ïÊòØ‰ΩøÁî®ÈÄôÂÄãÊû∂Êßã„ÄÇÈñãÁôºÂÖ¨Âπ≥ÁöÑ AI Ê®°ÂûãÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏≠ÊúâË®±Â§ö‰∏îÈÄöÂ∏∏Êú™Áü•ÁöÑÂÅèË™§‰æÜÊ∫ê„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂÆ¢ËßÄÂú∞Á†îÁ©∂ÂÅèË™§ÂíåÁ∑©Ëß£Á≠ñÁï•Â∞çÊ∑±Â∫¶Â≠∏ÁøíÁÆ°Á∑öÁöÑÂΩ±ÈüøÔºåÈÄôÂèØ‰ª•ÊîØÊè¥ÂÅ•ÂÖ®‰∏îË≤†Ë≤¨‰ªªÁöÑËá®Â∫ä AI ÁöÑÈñãÁôº„ÄÇ</paragraph>

##### **Predicting recovery following stroke: deep learning, multimodal data and feature selection using explainable AI**
2310.19174v1 by Adam White, Margarita Saranti, Artur d'Avila Garcez, Thomas M. H. Hope, Cathy J. Price, Howard Bowman

Machine learning offers great potential for automated prediction of
post-stroke symptoms and their response to rehabilitation. Major challenges for
this endeavour include the very high dimensionality of neuroimaging data, the
relatively small size of the datasets available for learning, and how to
effectively combine neuroimaging and tabular data (e.g. demographic information
and clinical characteristics). This paper evaluates several solutions based on
two strategies. The first is to use 2D images that summarise MRI scans. The
second is to select key features that improve classification accuracy.
Additionally, we introduce the novel approach of training a convolutional
neural network (CNN) on images that combine regions-of-interest extracted from
MRIs, with symbolic representations of tabular data. We evaluate a series of
CNN architectures (both 2D and a 3D) that are trained on different
representations of MRI and tabular data, to predict whether a composite measure
of post-stroke spoken picture description ability is in the aphasic or
non-aphasic range. MRI and tabular data were acquired from 758 English speaking
stroke survivors who participated in the PLORAS study. The classification
accuracy for a baseline logistic regression was 0.678 for lesion size alone,
rising to 0.757 and 0.813 when initial symptom severity and recovery time were
successively added. The highest classification accuracy 0.854 was observed when
8 regions-of-interest was extracted from each MRI scan and combined with lesion
size, initial severity and recovery time in a 2D Residual Neural Network.Our
findings demonstrate how imaging and tabular data can be combined for high
post-stroke classification accuracy, even when the dataset is small in machine
learning terms. We conclude by proposing how the current models could be
improved to achieve even higher levels of accuracy using images from hospital
scanners.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏ÁøíÁÇ∫Ëá™ÂãïÈ†êÊ∏¨‰∏≠È¢®ÂæåÁóáÁãÄÂèäÂÖ∂Â∞çÂæ©ÂÅ•ÁöÑÂèçÊáâÊèê‰æõ‰∫ÜÊ•µÂ§ßÁöÑÊΩõÂäõ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁöÑÈáçÂ§ßÊåëÊà∞ÂåÖÊã¨Á•ûÁ∂ìÂΩ±ÂÉèË≥áÊñôÁöÑÁ∂≠Â∫¶ÈùûÂ∏∏È´ò„ÄÅÂèØÁî®ÊñºÂ≠∏ÁøíÁöÑË≥áÊñôÈõÜË¶èÊ®°Áõ∏Â∞çËºÉÂ∞èÔºå‰ª•ÂèäÂ¶Ç‰ΩïÊúâÊïàÁµêÂêàÁ•ûÁ∂ìÂΩ±ÂÉèÂíåË°®Ê†ºË≥áÊñôÔºà‰æãÂ¶Ç‰∫∫Âè£Áµ±Ë®àË≥áË®äÂíåËá®Â∫äÁâπÂæµÔºâ„ÄÇÊú¨ÊñáÊ†πÊìöÂÖ©Á®ÆÁ≠ñÁï•Ë©ï‰º∞‰∫ÜÂ§öÁ®ÆËß£Ê±∫ÊñπÊ°à„ÄÇÁ¨¨‰∏ÄÁ®ÆÊòØ‰ΩøÁî®Á∏ΩÁµê MRI ÊéÉÊèèÁöÑ 2D ÂΩ±ÂÉè„ÄÇÁ¨¨‰∫åÁ®ÆÊòØÈÅ∏ÊìáÊúâÂä©ÊñºÊèêÈ´òÂàÜÈ°ûÁ≤æÁ¢∫Â∫¶ÁöÑÈóúÈçµÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂú®ÁµêÂêàÂæû MRI ‰∏≠ÊèêÂèñÁöÑÊÑüËààË∂£ÂçÄÂüüËàáË°®Ê†ºË≥áÊñôÁöÑÁ¨¶ËôüË°®Á§∫ÁöÑÂΩ±ÂÉè‰∏äË®ìÁ∑¥Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÁöÑÊñ∞Á©éÊñπÊ≥ï„ÄÇÊàëÂÄëË©ï‰º∞‰∫Ü‰∏ÄÁ≥ªÂàó CNN Êû∂ÊßãÔºà2D Âíå 3DÔºâÔºåÈÄô‰∫õÊû∂ÊßãÂú® MRI ÂíåË°®Ê†ºË≥áÊñôÁöÑ‰∏çÂêåË°®Á§∫‰∏äÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰ª•È†êÊ∏¨‰∏≠È¢®ÂæåÂè£Ëø∞ÂúñÁâáÊèèËø∞ËÉΩÂäõÁöÑÁ∂úÂêàÊ∏¨ÈáèÊòØÂê¶Âú®Â§±Ë™ûÁóáÊàñÈùûÂ§±Ë™ûÁóáÁØÑÂúçÂÖß„ÄÇMRI ÂíåË°®Ê†ºË≥áÊñô‰æÜËá™ 758 ÂêçÂèÉËàá PLORAS Á†îÁ©∂ÁöÑËã±Ë™û‰∏≠È¢®ÂÄñÂ≠òËÄÖ„ÄÇÂÉÖÈáùÂ∞çÁóÖÁÅ∂Â§ßÂ∞èÁöÑÂü∫Á∑öÈÇèËºØËø¥Ê≠∏ÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÁÇ∫ 0.678ÔºåÁï∂‰æùÂ∫èÂä†ÂÖ•ÂàùÂßãÁóáÁãÄÂö¥ÈáçÁ®ãÂ∫¶ÂíåÊÅ¢Âæ©ÊôÇÈñìÊôÇÔºå‰∏äÂçáËá≥ 0.757 Âíå 0.813„ÄÇÂú®ÂæûÊØèÂÄã MRI ÊéÉÊèè‰∏≠ÊèêÂèñ 8 ÂÄãÊÑüËààË∂£ÂçÄÂüü‰∏¶Âú® 2D ÊÆòÂ∑ÆÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ËàáÁóÖÁÅ∂Â§ßÂ∞è„ÄÅÂàùÂßãÂö¥ÈáçÁ®ãÂ∫¶ÂíåÊÅ¢Âæ©ÊôÇÈñìÁµêÂêàÊôÇÔºåËßÄÂØüÂà∞ÊúÄÈ´òÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ 0.854„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞áÂΩ±ÂÉèÂíåË°®Ê†ºË≥áÊñôÁµêÂêàËµ∑‰æÜ‰ª•Áç≤ÂæóÈ´òÊñº‰∏≠È¢®ÂæåÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÔºåÂç≥‰ΩøÂú®Ê©üÂô®Â≠∏ÁøíË°ìË™û‰∏≠Ë≥áÊñôÈõÜÂæàÂ∞èÁöÑÊÉÖÊ≥Å‰∏ã‰πüÊòØÂ¶ÇÊ≠§„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫Â¶Ç‰ΩïÊîπÈÄ≤ÁõÆÂâçÁöÑÊ®°ÂûãÔºå‰ª•‰ΩøÁî®‰æÜËá™ÈÜ´Èô¢ÊéÉÊèèÂÑÄÁöÑÂΩ±ÂÉè‰æÜÂØ¶ÁèæÊõ¥È´òÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇ

##### **Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation**
2310.01828v2 by Hossein Shreim, Abdul Karim Gizzini, Ali J. Ghandour

eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∑≤ÊàêÁÇ∫ËôïÁêÜ‰ªªÂãôÈóúÈçµÊáâÁî®Á®ãÂºèÊôÇÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨ÈúÄÊ±ÇÔºåÁ¢∫‰øùÊé°Áî®ÈªëÁõí AI Ê®°ÂûãÁöÑÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄß„ÄÇXAI ÁöÑÈáçË¶ÅÊÄßÊ∂µËìãÂæûÈÜ´ÁôÇ‰øùÂÅ•Âà∞ÈáëËûçÁöÑÂêÑÁ®ÆÈ†òÂüüÔºåÂú®ÈÄô‰∫õÈ†òÂüü‰∏≠Ôºå‰∫ÜËß£Ê∑±Â∫¶Â≠∏ÁøíÊºîÁÆóÊ≥ïÁöÑÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ãËá≥ÈóúÈáçË¶Å„ÄÇÂ§ßÂ§öÊï∏Âü∫Êñº AI ÁöÑÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÈÄöÂ∏∏ÊòØÈªëÁõíÂ≠êÔºõÂõ†Ê≠§ÔºåÂú®ÂΩ±ÂÉèËôïÁêÜ‰∏≠Êèê‰æõÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂèØËß£ÈáãÊÄßÂ∞çÊñºÂÖ∂Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûê„ÄÅËá™ÂãïÈßïÈßõÂíåÈÅôÊ∏¨ÊáâÁî®‰∏≠ÁöÑÂª£Ê≥õÊé°Áî®ÂíåÈÉ®ÁΩ≤Ëá≥ÈóúÈáçË¶Å„ÄÇÊúÄËøëÔºåÂ∑≤ÈáùÂ∞çÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãôÂºïÂÖ•‰∫ÜÂ§öÁ®Æ XAI ÊñπÊ≥ï„ÄÇÁõ∏ÂèçÂú∞ÔºåÂΩ±ÂÉèÂàÜÂâ≤Âú®ÂèØËß£ÈáãÊÄßÁöÑËÉåÊôØ‰∏ãÂèóÂà∞ÁöÑÈóúÊ≥®Áõ∏Â∞çËºÉÂ∞ëÔºåÂÑòÁÆ°ÂÆÉÊòØÈõªËÖ¶Ë¶ñË¶∫ÊáâÁî®‰∏≠ÁöÑ‰∏ÄÈ†ÖÂü∫Êú¨‰ªªÂãôÔºåÁâπÂà•ÊòØÂú®ÈÅôÊ∏¨‰∏≠„ÄÇÂè™ÊúâÈÉ®ÂàÜÁ†îÁ©∂ÊèêÂá∫Áî®ÊñºÂΩ±ÂÉèÂàÜÂâ≤ÁöÑÂü∫ÊñºÊ¢ØÂ∫¶ÁöÑ XAI ÊºîÁÆóÊ≥ï„ÄÇÊú¨ÊñáÊîπÁ∑®‰∫ÜÊúÄËøëÁöÑÁÑ°Ê¢ØÂ∫¶ Sobol XAI ÊñπÊ≥ï‰ª•ÈÄ≤Ë°åË™ûÊÑèÂàÜÂâ≤„ÄÇÁÇ∫‰∫ÜË°°Èáè Sobol ÊñπÊ≥ïÂú®ÂàÜÂâ≤‰∏≠ÁöÑÊïàËÉΩÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÂèØÂ≠∏ÁøíÈõúË®äÊ®°ÂûãÁöÑÂÆöÈáè XAI Ë©ï‰º∞ÊñπÊ≥ï„ÄÇÊ≠§Ê®°ÂûãÁöÑ‰∏ªË¶ÅÁõÆÁöÑÊòØÂú®Ëß£ÈáãÂúñ‰∏äË™òÁôºÈõúË®äÔºåÂÖ∂‰∏≠ËºÉÈ´òÁöÑË™òÁôºÈõúË®äË°®Á§∫ËºÉ‰ΩéÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÂèç‰πã‰∫¶ÁÑ∂„ÄÇÈÄ≤Ë°åÂü∫Ê∫ñÂàÜÊûê‰ª•Ë©ï‰º∞ÂíåÊØîËºÉ‰∏âÁ®Æ XAI ÊñπÊ≥ïÁöÑÊïàËÉΩÔºåÂåÖÊã¨ Seg-Grad-CAM„ÄÅSeg-Grad-CAM++ Âíå Seg-SobolÔºå‰∏¶‰ΩøÁî®ÊâÄÊèêÂá∫ÁöÑÂü∫ÊñºÈõúË®äÁöÑË©ï‰º∞ÊäÄË°ì„ÄÇÈÄôÊßãÊàê‰∫Ü‰ΩøÁî®È´òËß£ÊûêÂ∫¶Ë°õÊòüÂΩ±ÂÉèÂü∑Ë°åÂíåË©ï‰º∞ XAI ÊñπÊ≥ïÁöÑÈ¶ñÊ¨°ÂòóË©¶„ÄÇ

##### **Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI**
2311.01463v1 by Muhammad Aurangzeb Ahmad, Ilker Yaramis, Taposh Dutta Roy

Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Áü≠ÊôÇÈñìÂÖßÂ∑≤Âú®Â§öÂÄãÈ†òÂüü‰∏≠Â§ßÈáèÊøÄÂ¢û„ÄÇÁÑ∂ËÄåÔºåÁî±Êñº‰∫ãÂØ¶ÊÄß„ÄÅÈÄ£Ë≤´ÊÄßÂíåÂπªË¶∫Á≠âÂïèÈ°åÔºåÈÜ´ÁôÇÂíå‰øùÂÅ•È†òÂüüÂ∞çÂÖ∂Êé°Áî®Áå∂Ë±´‰∏çÊ±∫„ÄÇÈëëÊñºÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÈ´òÈ¢®Èö™ÊÄßË≥™ÔºåË®±Â§öÁ†îÁ©∂‰∫∫Âì°ÁîöËá≥Ë≠¶Âëä‰∏çË¶Å‰ΩøÁî®ÂÆÉÔºåÁõ¥Âà∞ÈÄô‰∫õÂïèÈ°åÂæóÂà∞Ëß£Ê±∫„ÄÇÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂØ¶ÊñΩÂíåÈÉ®ÁΩ≤ LLM ÁöÑÈóúÈçµÊòØ‰ΩøÈÄô‰∫õÊ®°ÂûãÂÄºÂæó‰ø°Ë≥¥„ÄÅÈÄèÊòéÔºàÁõ°ÂèØËÉΩÂ§öÔºâ‰∏îÂèØËß£Èáã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèèËø∞‰∫ÜÂª∫Á´ãÂèØÈù†„ÄÅÂÄºÂæó‰ø°Ë≥¥ÂíåÁÑ°ÂÅèË¶ãÊ®°ÂûãÁöÑÈóúÈçµË¶ÅÁ¥†Ôºå‰ΩúÁÇ∫ÂÆÉÂÄëÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂæóÂà∞Êé°Áî®ÁöÑÂøÖË¶ÅÊ¢ù‰ª∂„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂú®ÈÜ´ÁôÇ‰øùÂÅ•ËÉåÊôØ‰∏ãÂ∞çÂπªË¶∫ÈÄ≤Ë°åÈáèÂåñ„ÄÅÈ©óË≠âÂíåÁ∑©Ëß£„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫Ü LLM Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑÊú™‰æÜÂèØËÉΩÊòØ‰ªÄÈ∫ºÊ®£Â≠ê„ÄÇ

##### **When to Trust AI: Advances and Challenges for Certification of Neural Networks**
2309.11196v1 by Marta Kwiatkowska, Xiyue Zhang

Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂ∑≤Âø´ÈÄüÈÄ≤Ê≠•ÔºåÁèæÂ∑≤Ê∫ñÂÇôÈÉ®ÁΩ≤ÊñºÂª£Ê≥õÁöÑÊáâÁî®Á®ãÂºè‰∏≠Ôºå‰æãÂ¶ÇËá™‰∏ªÁ≥ªÁµ±„ÄÅÈÜ´ÁôÇË®∫Êñ∑ÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ„ÄÇÂèäÊó©Êé°Áî® AI ÊäÄË°ìÊñºÂØ¶ÈöõÊáâÁî®Á®ãÂºè‰∏¶ÈùûÊ≤íÊúâÂïèÈ°åÔºåÁâπÂà•ÊòØÂ∞çÊñºÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂÆÉÂèØËÉΩ‰∏çÁ©©ÂÆö‰∏îÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÁØÑ‰æãÁöÑÂΩ±Èüø„ÄÇÂæûÈï∑ÈÅ†‰æÜÁúãÔºåÈúÄË¶ÅÈñãÁôºÈÅ©Áï∂ÁöÑÂÆâÂÖ®‰øùË≠âÊäÄË°ìÔºå‰ª•Ê∏õÂ∞ëÂõ†ÂèØÈÅøÂÖçÁöÑÁ≥ªÁµ±ÊïÖÈöúËÄåÈÄ†ÊàêÁöÑÊΩõÂú®ÂÇ∑ÂÆ≥Ôºå‰∏¶Á¢∫‰øùÂèØ‰ø°Ë≥¥ÊÄß„ÄÇÊú¨ÊñáËëóÈáçÊñºË™çË≠âÂíåÂèØËß£ÈáãÊÄßÔºåÊ¶ÇËø∞‰∫ÜÂ∑≤ÈñãÁôºÁî®ÊñºÁ¢∫‰øù AI Ê±∫Á≠ñÂÆâÂÖ®ÁöÑÊäÄË°ìÔºå‰∏¶Ë®éË´ñÊú™‰æÜÁöÑÊåëÊà∞„ÄÇ

##### **Functional requirements to mitigate the Risk of Harm to Patients from Artificial Intelligence in Healthcare**
2309.10424v1 by Juan M. Garc√≠a-G√≥mez, Vicent Blanes-Selva, Jos√© Carlos de Bartolom√© Cenzano, Jaime Cebolla-Cornejo, Ascensi√≥n Do√±ate-Mart√≠nez

The Directorate General for Parliamentary Research Services of the European
Parliament has prepared a report to the Members of the European Parliament
where they enumerate seven main risks of Artificial Intelligence (AI) in
medicine and healthcare: patient harm due to AI errors, misuse of medical AI
tools, bias in AI and the perpetuation of existing inequities, lack of
transparency, privacy and security issues, gaps in accountability, and
obstacles in implementation.
  In this study, we propose fourteen functional requirements that AI systems
may implement to reduce the risks associated with their medical purpose: AI
passport, User management, Regulation check, Academic use only disclaimer, data
quality assessment, Clinicians double check, Continuous performance evaluation,
Audit trail, Continuous usability test, Review of retrospective/simulated
cases, Bias check, eXplainable AI, Encryption and use of field-tested
libraries, and Semantic interoperability.
  Our intention here is to provide specific high-level specifications of
technical solutions to ensure continuous good performance and use of AI systems
to benefit patients in compliance with the future EU regulatory framework.

ÊëòË¶ÅÔºöÊ≠êÊ¥≤Ë≠∞ÊúÉË≠∞ÊúÉÁ†îÁ©∂ÊúçÂãôÁ∏ΩÂ±ÄÂ∑≤ÁÇ∫Ê≠êÊ¥≤Ë≠∞ÊúÉË≠∞Âì°Ê∫ñÂÇô‰∫Ü‰∏Ä‰ªΩÂ†±ÂëäÔºåÂÖ∂‰∏≠ÂàóËàâ‰∫Ü‰∫∫Â∑•Êô∫ËÉΩ (AI) Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑ‰∏ÉÈ†Ö‰∏ªË¶ÅÈ¢®Èö™ÔºöAI ÈåØË™§Â∞éËá¥ÊÇ£ËÄÖÂèóÂà∞ÂÇ∑ÂÆ≥„ÄÅÈÜ´ÁôÇ AI Â∑•ÂÖ∑Ë¢´Êø´Áî®„ÄÅAI Â≠òÂú®ÂÅèË¶ã‰∏¶Â∞éËá¥ÁèæÊúâ inequities ÊåÅÁ∫åÂ≠òÂú®„ÄÅÁº∫‰πèÈÄèÊòéÂ∫¶„ÄÅÈö±ÁßÅÂíåÂÆâÂÖ®ÂïèÈ°å„ÄÅÂïèË≤¨Â∑ÆË∑ù‰ª•ÂèäÂØ¶ÊñΩÈöúÁ§ô„ÄÇ
  Âú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂçÅÂõõÈ†ÖÂäüËÉΩÊÄßË¶ÅÊ±ÇÔºåAI Á≥ªÁµ±ÂèØ‰ª•ÂØ¶ÊñΩÈÄô‰∫õË¶ÅÊ±Ç‰æÜÈôç‰ΩéËàáÂÖ∂ÈÜ´ÁôÇÁõÆÁöÑÁõ∏ÈóúÁöÑÈ¢®Èö™ÔºöAI Ë≠∑ÁÖß„ÄÅ‰ΩøÁî®ËÄÖÁÆ°ÁêÜ„ÄÅÊ≥ïË¶èÊ™¢Êü•„ÄÅÂÉÖÈôêÂ≠∏Ë°ìÁî®ÈÄîÂÖçË≤¨ËÅ≤Êòé„ÄÅË≥áÊñôÂìÅË≥™Ë©ï‰º∞„ÄÅËá®Â∫äÈÜ´ÁîüÈõôÈáçÊ™¢Êü•„ÄÅÊåÅÁ∫åÊïàËÉΩË©ï‰º∞„ÄÅÁ®ΩÊ†∏ËøΩËπ§„ÄÅÊåÅÁ∫åÂèØÁî®ÊÄßÊ∏¨Ë©¶„ÄÅÂõûÈ°ßÂõûÊ∫Ø/Ê®°Êì¨Ê°à‰æã„ÄÅÂÅèË¶ãÊ™¢Êü•„ÄÅÂèØËß£Èáã AI„ÄÅÂä†ÂØÜÂíå‰ΩøÁî®Á∂ìÈÅéÂØ¶Âú∞Ê∏¨Ë©¶ÁöÑÁ®ãÂºèÂ∫´Ôºå‰ª•ÂèäË™ûÊÑè‰∫íÈÄöÊÄß„ÄÇ
  ÊàëÂÄëÂú®Ê≠§ÁöÑÁõÆÁöÑÊòØÊèê‰æõÊäÄË°ìËß£Ê±∫ÊñπÊ°àÁöÑÁâπÂÆöÈ´òÈöéË¶èÊ†ºÔºå‰ª•Á¢∫‰øùÊåÅÁ∫åËâØÂ•ΩÁöÑÊïàËÉΩÔºå‰∏¶‰ΩøÁî® AI Á≥ªÁµ±Ôºå‰ª•Á¨¶ÂêàÊú™‰æÜÁöÑÊ≠êÁõüÊ≥ïË¶èÊû∂ÊßãÔºåÂæûËÄå‰ΩøÊÇ£ËÄÖÂèóÁõä„ÄÇ

##### **QXAI: Explainable AI Framework for Quantitative Analysis in Patient Monitoring Systems**
2309.10293v3 by Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Juan D. Velasquez, Niall Higgins

Artificial Intelligence techniques can be used to classify a patient's
physical activities and predict vital signs for remote patient monitoring.
Regression analysis based on non-linear models like deep learning models has
limited explainability due to its black-box nature. This can require
decision-makers to make blind leaps of faith based on non-linear model results,
especially in healthcare applications. In non-invasive monitoring, patient data
from tracking sensors and their predisposing clinical attributes act as input
features for predicting future vital signs. Explaining the contributions of
various features to the overall output of the monitoring application is
critical for a clinician's decision-making. In this study, an Explainable AI
for Quantitative analysis (QXAI) framework is proposed with post-hoc model
explainability and intrinsic explainability for regression and classification
tasks in a supervised learning approach. This was achieved by utilizing the
Shapley values concept and incorporating attention mechanisms in deep learning
models. We adopted the artificial neural networks (ANN) and attention-based
Bidirectional LSTM (BiLSTM) models for the prediction of heart rate and
classification of physical activities based on sensor data. The deep learning
models achieved state-of-the-art results in both prediction and classification
tasks. Global explanation and local explanation were conducted on input data to
understand the feature contribution of various patient data. The proposed QXAI
framework was evaluated using PPG-DaLiA data to predict heart rate and mobile
health (MHEALTH) data to classify physical activities based on sensor data.
Monte Carlo approximation was applied to the framework to overcome the time
complexity and high computation power requirements required for Shapley value
calculations.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÊäÄË°ìÂèØÁî®ÊñºÂàÜÈ°ûÁóÖÊÇ£ÁöÑË∫´È´îÊ¥ªÂãï‰∏¶È†êÊ∏¨ÈÅ†Ë∑ùÁóÖÊÇ£Áõ£ÊéßÁöÑÈáçË¶ÅÁîüÂëΩÂæµË±°„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁ≠âÈùûÁ∑öÊÄßÊ®°ÂûãÁöÑÂõûÊ≠∏ÂàÜÊûêÁî±ÊñºÂÖ∂ÈªëÁõíÂ≠êÁöÑÊÄßË≥™ËÄåÂÖ∑ÊúâÊúâÈôêÁöÑÂèØËß£ÈáãÊÄß„ÄÇÈÄôÂèØËÉΩÈúÄË¶ÅÊ±∫Á≠ñËÄÖÊ†πÊìöÈùûÁ∑öÊÄßÊ®°ÂûãÁµêÊûúÂÅöÂá∫Áõ≤ÁõÆÁöÑ‰ø°‰ª∞È£õË∫çÔºåÁâπÂà•ÊòØÂú®ÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰∏≠„ÄÇÂú®Èùû‰æµÂÖ•ÊÄßÁõ£Êéß‰∏≠Ôºå‰æÜËá™ËøΩËπ§ÊÑüÊ∏¨Âô®ÂíåÂÖ∂ÊòìÊÑüËá®Â∫äÂ±¨ÊÄßÁöÑÁóÖÊÇ£Ë≥áÊñôÂÖÖÁï∂È†êÊ∏¨Êú™‰æÜÁîüÂëΩÂæµË±°ÁöÑËº∏ÂÖ•ÁâπÂæµ„ÄÇËß£ÈáãÂêÑÁ®ÆÁâπÂæµÂ∞çÁõ£ÊéßÊáâÁî®Á®ãÂºèÊï¥È´îËº∏Âá∫ÁöÑË≤¢ÁçªÂ∞çÊñºËá®Â∫äÈÜ´ÁîüÁöÑÊ±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî®ÊñºÈáèÂåñÂàÜÊûêÁöÑÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (QXAI) Êû∂ÊßãÔºåË©≤Êû∂ÊßãÂÖ∑ÊúâÁõ£Áù£ÂºèÂ≠∏ÁøíÊñπÊ≥ï‰∏≠ÂõûÊ≠∏ÂíåÂàÜÈ°û‰ªªÂãôÁöÑ‰∫ãÂæåÊ®°ÂûãÂèØËß£ÈáãÊÄßÂíåÂÖßÂú®ÂèØËß£ÈáãÊÄß„ÄÇÈÄôÈÄèÈÅéÂà©Áî® Shapley ÂÄºÊ¶ÇÂøµ‰∏¶Â∞áÊ≥®ÊÑèÂäõÊ©üÂà∂Á¥çÂÖ•Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰æÜÂØ¶Áèæ„ÄÇÊàëÂÄëÊé°Áî®‰∫∫Â∑•Á•ûÁ∂ìÁ∂≤Ë∑Ø (ANN) ÂíåÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÈõôÂêë LSTM (BiLSTM) Ê®°ÂûãÔºåÊ†πÊìöÊÑüÊ∏¨Âô®Ë≥áÊñôÈ†êÊ∏¨ÂøÉÁéáÂíåÂàÜÈ°ûË∫´È´îÊ¥ªÂãï„ÄÇÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂú®È†êÊ∏¨ÂíåÂàÜÈ°û‰ªªÂãô‰∏≠ÈÉΩÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊàêÊûú„ÄÇÂ∞çËº∏ÂÖ•Ë≥áÊñôÈÄ≤Ë°åÂÖ®Â±ÄËß£ÈáãÂíåÂ±ÄÈÉ®Ëß£ÈáãÔºå‰ª•‰∫ÜËß£ÂêÑÁ®ÆÁóÖÊÇ£Ë≥áÊñôÁöÑÁâπÂæµË≤¢Áçª„ÄÇÊâÄÊèêÂá∫ÁöÑ QXAI Êû∂Êßã‰ΩøÁî® PPG-DaLiA Ë≥áÊñôË©ï‰º∞Ôºå‰ª•È†êÊ∏¨ÂøÉÁéáÔºå‰∏¶‰ΩøÁî®Ë°åÂãïÂÅ•Â∫∑ (MHEALTH) Ë≥áÊñôÊ†πÊìöÊÑüÊ∏¨Âô®Ë≥áÊñôÂ∞çË∫´È´îÊ¥ªÂãïÈÄ≤Ë°åÂàÜÈ°û„ÄÇËíôÂú∞Âç°ÁæÖËøë‰ººÊ≥ïÊáâÁî®ÊñºË©≤Êû∂ÊßãÔºå‰ª•ÂÖãÊúç Shapley ÂÄºË®àÁÆóÊâÄÈúÄÁöÑÊôÇÈñìË§áÈõúÂ∫¶ÂíåÈ´òÈÅãÁÆóËÉΩÂäõÈúÄÊ±Ç„ÄÇ

##### **Evaluation of Human-Understandability of Global Model Explanations using Decision Tree**
2309.09917v1 by Adarsa Sivaprasad, Ehud Reiter, Nava Tintarev, Nir Oren

In explainable artificial intelligence (XAI) research, the predominant focus
has been on interpreting models for experts and practitioners. Model agnostic
and local explanation approaches are deemed interpretable and sufficient in
many applications. However, in domains like healthcare, where end users are
patients without AI or domain expertise, there is an urgent need for model
explanations that are more comprehensible and instil trust in the model's
operations. We hypothesise that generating model explanations that are
narrative, patient-specific and global(holistic of the model) would enable
better understandability and enable decision-making. We test this using a
decision tree model to generate both local and global explanations for patients
identified as having a high risk of coronary heart disease. These explanations
are presented to non-expert users. We find a strong individual preference for a
specific type of explanation. The majority of participants prefer global
explanations, while a smaller group prefers local explanations. A task based
evaluation of mental models of these participants provide valuable feedback to
enhance narrative global explanations. This, in turn, guides the design of
health informatics systems that are both trustworthy and actionable.

ÊëòË¶ÅÔºöÂú®ÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩ (XAI) Á†îÁ©∂‰∏≠Ôºå‰∏ªË¶ÅÈáçÁÇπÂú®‰∫é‰∏∫‰∏ìÂÆ∂Âíå‰ªé‰∏öËÄÖËß£ÈáäÊ®°Âûã„ÄÇÊ®°Âûã‰∏çÂèØÁü•ÂíåÂ±ÄÈÉ®Ëß£ÈáäÊñπÊ≥ïÂú®ËÆ∏Â§öÂ∫îÁî®‰∏≠Ë¢´ËÆ§‰∏∫ÊòØÂèØËß£Èáä‰∏îË∂≥Â§üÁöÑ„ÄÇÁÑ∂ËÄåÔºåÂú®ÂåªÁñó‰øùÂÅ•Á≠âÈ¢ÜÂüüÔºåÊúÄÁªàÁî®Êà∑ÊòØÁº∫‰πè‰∫∫Â∑•Êô∫ËÉΩÊàñÈ¢ÜÂüü‰∏ì‰∏öÁü•ËØÜÁöÑÊÇ£ËÄÖÔºåÂõ†Ê≠§Ëø´ÂàáÈúÄË¶ÅÊõ¥Êòì‰∫éÁêÜËß£‰∏îËÉΩÊøÄÂèëÂØπÊ®°ÂûãÊìç‰ΩúÁöÑ‰ø°‰ªªÁöÑÊ®°ÂûãËß£Èáä„ÄÇÊàë‰ª¨ÂÅáËÆæÁîüÊàêÂèôËø∞ÊÄß„ÄÅÊÇ£ËÄÖÁâπÂÆö‰∏îÂÖ®Â±ÄÔºàÊ®°ÂûãÊï¥‰ΩìÔºâÁöÑÊ®°ÂûãËß£ÈáäÂ∞ÜËÉΩÂ§üÊèêÈ´òÂèØÁêÜËß£ÊÄßÂπ∂ÊîØÊåÅÂÜ≥Á≠ñÂà∂ÂÆö„ÄÇÊàë‰ª¨‰ΩøÁî®ÂÜ≥Á≠ñÊ†ëÊ®°ÂûãÂØπÊ≠§ËøõË°åÊµãËØïÔºå‰∏∫Ë¢´ËØÜÂà´‰∏∫ÊÇ£ÊúâÂÜ†ÂøÉÁóÖÈ´òÈ£éÈô©ÁöÑÊÇ£ËÄÖÁîüÊàêÂ±ÄÈÉ®ÂíåÂÖ®Â±ÄËß£Èáä„ÄÇËøô‰∫õËß£Èáä‰ºöÂëàÁé∞ÁªôÈùû‰∏ìÂÆ∂Áî®Êà∑„ÄÇÊàë‰ª¨ÂèëÁé∞Áî®Êà∑Âº∫ÁÉàÂÅèÂ•ΩÁâπÂÆöÁ±ªÂûãÁöÑËß£Èáä„ÄÇÂ§ßÂ§öÊï∞ÂèÇ‰∏éËÄÖÂÅèÂ•ΩÂÖ®Â±ÄËß£ÈáäÔºåËÄåËæÉÂ∞èÁöÑ‰∏ÄÁªÑÂèÇ‰∏éËÄÖÂÅèÂ•ΩÂ±ÄÈÉ®Ëß£Èáä„ÄÇÂü∫‰∫é‰ªªÂä°ÁöÑÂøÉÁêÜÊ®°ÂûãËØÑ‰º∞‰∏∫Ëøô‰∫õÂèÇ‰∏éËÄÖÊèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑÂèçÈ¶àÔºå‰ª•Â¢ûÂº∫ÂèôËø∞ÊÄßÂÖ®Â±ÄËß£Èáä„ÄÇËøôÂèçËøáÊù•ÂèàÊåáÂØº‰∫ÜÊó¢ÂÄºÂæó‰ø°ËµñÂèàÂèØÊìç‰ΩúÁöÑÂÅ•Â∫∑‰ø°ÊÅØÂ≠¶Á≥ªÁªüÁöÑËÆæËÆ°„ÄÇ

##### **Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction**
2308.14321v1 by Yanjun Gao, Ruizhe Li, John Caskey, Dmitriy Dligach, Timothy Miller, Matthew M. Churpek, Majid Afshar

Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.

ÊëòË¶ÅÔºöÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) Âíå‰æãË°åÊñá‰ª∂Ë®òÈåÑÂØ¶ÂãôÂú®ÁóÖÊÇ£ÁöÑÊó•Â∏∏ÁÖßË≠∑‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤ÔºåÊèê‰æõÂÅ•Â∫∑„ÄÅË®∫Êñ∑ÂíåÊ≤ªÁôÇÁöÑÊï¥È´îÁ¥ÄÈåÑ„ÄÇÁÑ∂ËÄåÔºåË§áÈõú‰∏îÂÜóÈï∑ÁöÑ EHR ÊïòËø∞ÊúÉËÆìÈÜ´ÁôÇ‰øùÂÅ•Êèê‰æõËÄÖË∂ÖËºâÔºåÊúâË®∫Êñ∑‰∏çÊ∫ñÁ¢∫ÁöÑÈ¢®Èö™„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂÖ∂Âú®ÂêÑÁ®ÆË™ûË®Ä‰ªªÂãô‰∏äÁöÑÊΩõÂäõÔºå‰ΩÜÂÖ∂Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑÊáâÁî®ÈúÄË¶ÅÁ¢∫‰øùÂ∞áË®∫Êñ∑ÈåØË™§ÈôçÂà∞ÊúÄ‰ΩéÔºå‰∏¶Èò≤Ê≠¢ÁóÖÊÇ£ÂèóÂà∞ÂÇ∑ÂÆ≥„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊ¶ÇËø∞‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÈÄèÈÅéÊï¥ÂêàÈÜ´Â≠∏Áü•Ë≠òÂúñË≠ú (KG) Âíå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂúñË≠úÊ®°ÂûãÔºöDr.KnowsÔºàÈùàÊÑü‰æÜËá™Ëá®Â∫äË®∫Êñ∑Êé®ÁêÜÈÅéÁ®ãÔºâÔºå‰æÜÂ¢ûÂº∑ LLM Âú®Ëá™ÂãïÂåñË®∫Êñ∑Áî¢ÁîüÈ†òÂüüÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÂæûÁæéÂúãÂúãÂÆ∂ÈÜ´Â≠∏ÂúñÊõ∏È§®ÁöÑÁµ±‰∏ÄÈÜ´Â≠∏Ë™ûË®ÄÁ≥ªÁµ± (UMLS) ‰∏≠Ë°çÁîüÂá∫ KGÔºåÈÄôÊòØ‰∏ÄÂÄãÂº∑Â§ßÁöÑÁîüÁâ©ÈÜ´Â≠∏Áü•Ë≠òÂÑ≤Â≠òÂ∫´„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂê¶ÂÆö‰∫ÜÈ†êÂÖàË®ìÁ∑¥ÁöÑÈúÄË¶ÅÔºåËÄåÊòØÂ∞á KG ‰ΩúÁÇ∫ËºîÂä©Â∑•ÂÖ∑ÔºåÂçîÂä©Ëß£ÈáãÂíåÁ∏ΩÁµêË§áÈõúÁöÑÈÜ´Â≠∏Ê¶ÇÂøµ„ÄÇ‰ΩøÁî®ÁúüÂØ¶‰∏ñÁïåÁöÑÈÜ´Èô¢Ë≥áÊñôÈõÜÔºåÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÂ∞á LLM Ëàá KG ÁµêÂêàÁöÑÂª∫Ë≠∞ÊñπÊ≥ïÊúâÊΩõÂäõÊèêÈ´òËá™ÂãïÂåñË®∫Êñ∑Áî¢ÁîüÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÊ¢ùÂèØËß£ÈáãÁöÑË®∫Êñ∑ÈÄîÂæëÔºåËÆìÊàëÂÄëÊõ¥Êé•ËøëÂØ¶Áèæ AI Â¢ûÂº∑ÁöÑË®∫Êñ∑Ê±∫Á≠ñÊîØÊè¥Á≥ªÁµ±„ÄÇ

##### **Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review**
2308.09380v1 by Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai

Existing artificial intelligence (AI) models for diagnosing knee
osteoarthritis (OA) have faced criticism for their lack of transparency and
interpretability, despite achieving medical-expert-like performance. This
opacity makes them challenging to trust in clinical practice. Recently,
explainable artificial intelligence (XAI) has emerged as a specialized
technique that can provide confidence in the model's prediction by revealing
how the prediction is derived, thus promoting the use of AI systems in
healthcare. This paper presents the first survey of XAI techniques used for
knee OA diagnosis. The XAI techniques are discussed from two perspectives: data
interpretability and model interpretability. The aim of this paper is to
provide valuable insights into XAI's potential towards a more reliable knee OA
diagnosis approach and encourage its adoption in clinical practice.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÁî®ÊñºË®∫Êñ∑ËÜùÈ™®ÈóúÁØÄÁÇé (OA) ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) Ê®°ÂûãÂõ†ÂÖ∂Áº∫‰πèÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßËÄåÂèóÂà∞ÊâπË©ïÔºåÂÑòÁÆ°ÂÆÉÂÄëÈÅîÂà∞‰∫ÜÈ°û‰ººÈÜ´Â≠∏Â∞àÂÆ∂ÁöÑË°®Áèæ„ÄÇÈÄôÁ®Æ‰∏çÈÄèÊòéÊÄß‰ΩøÂæóÂÆÉÂÄëÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠Èõ£‰ª•Ë¢´‰ø°‰ªª„ÄÇÊúÄËøëÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÂ∞àÈñÄÊäÄË°ìÔºåÂÆÉËÉΩÈÄèÈÅéÊè≠Á§∫È†êÊ∏¨ÁöÑÊé®Â∞éÊñπÂºè‰æÜÊèê‰æõÂ∞çÊ®°ÂûãÈ†êÊ∏¨ÁöÑ‰ø°ÂøÉÔºåÂæûËÄå‰øÉÈÄ≤Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠‰ΩøÁî® AI Á≥ªÁµ±„ÄÇÊú¨ÊñáÊèê‰æõ‰∫ÜÈáùÂ∞çËÜùÈ™®ÈóúÁØÄÁÇéË®∫Êñ∑ÊâÄ‰ΩøÁî®ÁöÑ XAI ÊäÄË°ìÁöÑÁ¨¨‰∏Ä‰ªΩË™øÊü•„ÄÇXAI ÊäÄË°ìÂæûÂÖ©ÂÄãËßíÂ∫¶ÈÄ≤Ë°åË®éË´ñÔºöË≥áÊñôÂèØËß£ÈáãÊÄßÂíåÊ®°ÂûãÂèØËß£ÈáãÊÄß„ÄÇÊú¨ÊñáÁöÑÁõÆÁöÑÊòØÊèê‰æõÂ∞ç XAI Âú®Êõ¥ÂèØÈù†ÁöÑËÜùÈ™®ÈóúÁØÄÁÇéË®∫Êñ∑ÊñπÊ≥ï‰∏≠ÁöÑÊΩõÂäõÁöÑÂØ∂Ë≤¥Ë¶ãËß£Ôºå‰∏¶ÈºìÂãµÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠Êé°Áî®ÂÆÉ„ÄÇ

##### **Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities**
2308.08407v1 by Munib Mesinovic, Peter Watkinson, Tingting Zhu

Recent advancements in AI applications to healthcare have shown incredible
promise in surpassing human performance in diagnosis and disease prognosis.
With the increasing complexity of AI models, however, concerns regarding their
opacity, potential biases, and the need for interpretability. To ensure trust
and reliability in AI systems, especially in clinical risk prediction models,
explainability becomes crucial. Explainability is usually referred to as an AI
system's ability to provide a robust interpretation of its decision-making
logic or the decisions themselves to human stakeholders. In clinical risk
prediction, other aspects of explainability like fairness, bias, trust, and
transparency also represent important concepts beyond just interpretability. In
this review, we address the relationship between these concepts as they are
often used together or interchangeably. This review also discusses recent
progress in developing explainable models for clinical risk prediction,
highlighting the importance of quantitative and clinical evaluation and
validation across multiple common modalities in clinical practice. It
emphasizes the need for external validation and the combination of diverse
interpretability methods to enhance trust and fairness. Adopting rigorous
testing, such as using synthetic datasets with known generative factors, can
further improve the reliability of explainability methods. Open access and
code-sharing resources are essential for transparency and reproducibility,
enabling the growth and trustworthiness of explainable research. While
challenges exist, an end-to-end approach to explainability in clinical risk
prediction, incorporating stakeholders from clinicians to developers, is
essential for success.

ÊëòË¶ÅÔºöÊúÄËøëÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊáâÁî®ÈÄ≤Â±ïÈ°ØÁ§∫Âá∫‰ª§‰∫∫Èõ£‰ª•ÁΩÆ‰ø°ÁöÑÊâøË´æÔºåÂú®Ë®∫Êñ∑ÂíåÁñæÁóÖÈ†êÂæåÊñπÈù¢Ë∂ÖË∂ä‰∫∫È°ûË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÈö®Ëëó‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÁöÑÊó•ÁõäË§áÈõúÔºå‰∫∫ÂÄëÂ∞çÂÖ∂‰∏çÈÄèÊòéÊÄß„ÄÅÊΩõÂú®ÂÅèÂ∑ÆÂíåÂ∞çÂèØËß£ÈáãÊÄßÁöÑÈúÄÊ±ÇÊÑüÂà∞ÊìîÊÜÇ„ÄÇÁÇ∫‰∫ÜÁ¢∫‰øù‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁµ±ÁöÑ‰ø°‰ªªÂíåÂèØÈù†ÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Ëá®Â∫äÈ¢®Èö™È†êÊ∏¨Ê®°Âûã‰∏≠ÔºåÂèØËß£ÈáãÊÄßËÆäÂæóËá≥ÈóúÈáçË¶Å„ÄÇÂèØËß£ÈáãÊÄßÈÄöÂ∏∏Ë¢´Á®±ÁÇ∫‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁµ±Êèê‰æõÂÖ∂Ê±∫Á≠ñÈÇèËºØÊàñÊ±∫Á≠ñÊú¨Ë∫´Â∞ç‰∫∫È°ûÂà©ÁõäÁõ∏ÈóúËÄÖÁöÑÂº∑ÊúâÂäõËß£ÈáãÁöÑËÉΩÂäõ„ÄÇÂú®Ëá®Â∫äÈ¢®Èö™È†êÊ∏¨‰∏≠ÔºåÂèØËß£ÈáãÊÄßÁöÑÂÖ∂‰ªñÊñπÈù¢ÔºåÂ¶ÇÂÖ¨Âπ≥ÊÄß„ÄÅÂÅèË¶ã„ÄÅ‰ø°‰ªªÂíåÈÄèÊòéÂ∫¶Ôºå‰πü‰ª£Ë°®‰∫ÜË∂ÖË∂äÂèØËß£ÈáãÊÄßÁöÑÈáçË¶ÅÊ¶ÇÂøµ„ÄÇÂú®Êú¨Ê¨°ÂØ©Êü•‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÈÄô‰∫õÊ¶ÇÂøµ‰πãÈñìÁöÑÈóú‰øÇÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁ∂ìÂ∏∏‰∏ÄËµ∑Êàñ‰∫íÊèõ‰ΩøÁî®„ÄÇÊú¨ÂØ©Êü•ÈÇÑË®éË´ñ‰∫ÜÁÇ∫Ëá®Â∫äÈ¢®Èö™È†êÊ∏¨ÈñãÁôºÂèØËß£ÈáãÊ®°ÂûãÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÔºåÂº∑Ë™ø‰∫ÜÂú®Ëá®Â∫äÂØ¶Ë∏ê‰∏≠Â∞çÂ§öÁ®ÆÂ∏∏Ë¶ãÊ®°ÂºèÈÄ≤Ë°åÂÆöÈáèÂíåËá®Â∫äË©ï‰º∞ÂíåÈ©óË≠âÁöÑÈáçË¶ÅÊÄß„ÄÇÂÆÉÂº∑Ë™ø‰∫ÜÂ§ñÈÉ®È©óË≠âÂíåÂ§öÊ®£ÂåñÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁõ∏ÁµêÂêàÁöÑÂøÖË¶ÅÊÄßÔºå‰ª•Â¢ûÂº∑‰ø°‰ªªÂíåÂÖ¨Âπ≥ÊÄß„ÄÇÊé°Áî®Âö¥Ê†ºÁöÑÊ∏¨Ë©¶Ôºå‰æãÂ¶Ç‰ΩøÁî®ÂÖ∑ÊúâÂ∑≤Áü•ÁîüÊàêÂõ†Á¥†ÁöÑÂêàÊàêÊï∏ÊìöÈõÜÔºåÂèØ‰ª•ÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑÂèØÈù†ÊÄß„ÄÇÈñãÊîæÁç≤ÂèñÂíå‰ª£Á¢ºÂÖ±‰∫´Ë≥áÊ∫êÂ∞çÊñºÈÄèÊòéÂ∫¶ÂíåÂèØÈáçË§áÊÄßËá≥ÈóúÈáçË¶ÅÔºåÂæûËÄå‰øÉÈÄ≤ÂèØËß£ÈáãÁ†îÁ©∂ÁöÑÂ¢ûÈï∑ÂíåÂèØ‰ø°Â∫¶„ÄÇÂÑòÁÆ°Â≠òÂú®ÊåëÊà∞Ôºå‰ΩÜÂæûËá®Â∫äÈÜ´ÁîüÂà∞ÈñãÁôº‰∫∫Âì°ÔºåÊé°Áî®Á´ØÂà∞Á´ØÁöÑÂèØËß£ÈáãÊÄßÊñπÊ≥ïÂ∞çÊñºËá®Â∫äÈ¢®Èö™È†êÊ∏¨ÁöÑÊàêÂäüËá≥ÈóúÈáçË¶Å„ÄÇ

##### **FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare**
2309.12325v1 by Karim Lekadir, Aasa Feragen, Abdul Joseph Fofanah, Alejandro F Frangi, Alena Buyx, Anais Emelie, Andrea Lara, Antonio R Porras, An-Wen Chan, Arcadi Navarro, Ben Glocker, Benard O Botwe, Bishesh Khanal, Brigit Beger, Carol C Wu, Celia Cintas, Curtis P Langlotz, Daniel Rueckert, Deogratias Mzurikwao, Dimitrios I Fotiadis, Doszhan Zhussupov, Enzo Ferrante, Erik Meijering, Eva Weicken, Fabio A Gonz√°lez, Folkert W Asselbergs, Fred Prior, Gabriel P Krestin, Gary Collins, Geletaw S Tegenaw, Georgios Kaissis, Gianluca Misuraca, Gianna Tsakou, Girish Dwivedi, Haridimos Kondylakis, Harsha Jayakody, Henry C Woodruf, Hugo JWL Aerts, Ian Walsh, Ioanna Chouvarda, Ir√®ne Buvat, Islem Rekik, James Duncan, Jayashree Kalpathy-Cramer, Jihad Zahir, Jinah Park, John Mongan, Judy W Gichoya, Julia A Schnabel, Kaisar Kushibar, Katrine Riklund, Kensaku Mori, Kostas Marias, Lameck M Amugongo, Lauren A Fromont, Lena Maier-Hein, Leonor Cerd√° Alberich, Leticia Rittner, Lighton Phiri, Linda Marrakchi-Kacem, Llu√≠s Donoso-Bach, Luis Mart√≠-Bonmat√≠, M Jorge Cardoso, Maciej Bobowicz, Mahsa Shabani, Manolis Tsiknakis, Maria A Zuluaga, Maria Bielikova, Marie-Christine Fritzsche, Marius George Linguraru, Markus Wenzel, Marleen De Bruijne, Martin G Tolsgaard, Marzyeh Ghassemi, Md Ashrafuzzaman, Melanie Goisauf, Mohammad Yaqub, Mohammed Ammar, M√≥nica Cano Abad√≠a, Mukhtar M E Mahmoud, Mustafa Elattar, Nicola Rieke, Nikolaos Papanikolaou, Noussair Lazrak, Oliver D√≠az, Olivier Salvado, Oriol Pujol, Ousmane Sall, Pamela Guevara, Peter Gordebeke, Philippe Lambin, Pieta Brown, Purang Abolmaesumi, Qi Dou, Qinghua Lu, Richard Osuala, Rose Nakasi, S Kevin Zhou, Sandy Napel, Sara Colantonio, Shadi Albarqouni, Smriti Joshi, Stacy Carter, Stefan Klein, Steffen E Petersen, Susanna Auss√≥, Suyash Awate, Tammy Riklin Raviv, Tessa Cook, Tinashe E M Mutsvangwa, Wendy A Rogers, Wiro J Niessen, X√®nia Puig-Bosch, Yi Zeng, Yunusa G Mohammed, Yves Saint James Aquino, Zohaib Salahuddin, Martijn P A Starmans

Despite major advances in artificial intelligence (AI) for medicine and
healthcare, the deployment and adoption of AI technologies remain limited in
real-world clinical practice. In recent years, concerns have been raised about
the technical, clinical, ethical and legal risks associated with medical AI. To
increase real world adoption, it is essential that medical AI tools are trusted
and accepted by patients, clinicians, health organisations and authorities.
This work describes the FUTURE-AI guideline as the first international
consensus framework for guiding the development and deployment of trustworthy
AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and
currently comprises 118 inter-disciplinary experts from 51 countries
representing all continents, including AI scientists, clinicians, ethicists,
and social scientists. Over a two-year period, the consortium defined guiding
principles and best practices for trustworthy AI through an iterative process
comprising an in-depth literature review, a modified Delphi survey, and online
consensus meetings. The FUTURE-AI framework was established based on 6 guiding
principles for trustworthy AI in healthcare, i.e. Fairness, Universality,
Traceability, Usability, Robustness and Explainability. Through consensus, a
set of 28 best practices were defined, addressing technical, clinical, legal
and socio-ethical dimensions. The recommendations cover the entire lifecycle of
medical AI, from design, development and validation to regulation, deployment,
and monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which
provides a structured approach for constructing medical AI tools that will be
trusted, deployed and adopted in real-world practice. Researchers are
encouraged to take the recommendations into account in proof-of-concept stages
to facilitate future translation towards clinical practice of medical AI.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®ÈÜ´Â≠∏ÂíåÈÜ´ÁôÇ‰øùÂÅ•ÊñπÈù¢ÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÊúâÈáçÂ§ßÈÄ≤Â±ïÔºå‰ΩÜ AI ÊäÄË°ìÁöÑÈÉ®ÁΩ≤ÂíåÊé°Áî®Âú®ÁèæÂØ¶‰∏ñÁïåÁöÑËá®Â∫äÂØ¶Âãô‰∏≠‰ªçÁÑ∂ÊúâÈôê„ÄÇËøëÂπ¥‰æÜÔºå‰∫∫ÂÄëÂ∞çËàáÈÜ´ÁôÇ AI Áõ∏ÈóúÁöÑÊäÄË°ì„ÄÅËá®Â∫ä„ÄÅÂÄ´ÁêÜÂíåÊ≥ïÂæãÈ¢®Èö™ÊèêÂá∫‰∫ÜÁñëÊÖÆ„ÄÇÁÇ∫‰∫ÜÂ¢ûÂä†ÂØ¶Èöõ‰∏ñÁïåÁöÑÊé°Áî®ÁéáÔºåÈÜ´ÁôÇ AI Â∑•ÂÖ∑ÂøÖÈ†àÁç≤ÂæóÊÇ£ËÄÖ„ÄÅËá®Â∫äÈÜ´Áîü„ÄÅÈÜ´ÁôÇÊ©üÊßãÂíåÁï∂Â±ÄÁöÑ‰ø°‰ªªÂíåÊé•Âèó„ÄÇÊú¨Á†îÁ©∂Â∞á FUTURE-AI ÊåáÂçóÊèèËø∞ÁÇ∫ÊåáÂ∞éÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂèØ‰ø°Ë≥¥ AI Â∑•ÂÖ∑ÈñãÁôºÂíåÈÉ®ÁΩ≤ÁöÑÁ¨¨‰∏ÄÂÄãÂúãÈöõÂÖ±Ë≠òÊ°ÜÊû∂„ÄÇFUTURE-AI ËÅØÁõüÊàêÁ´ãÊñº 2021 Âπ¥ÔºåÁõÆÂâçÁî±‰æÜËá™ 51 ÂÄãÂúãÂÆ∂/Âú∞ÂçÄÁöÑ 118 ‰ΩçË∑®È†òÂüüÂ∞àÂÆ∂ÁµÑÊàêÔºå‰ª£Ë°®ÊâÄÊúâÊ¥≤ÔºåÂåÖÊã¨ AI ÁßëÂ≠∏ÂÆ∂„ÄÅËá®Â∫äÈÜ´Áîü„ÄÅÂÄ´ÁêÜÂ≠∏ÂÆ∂ÂíåÁ§æÊúÉÁßëÂ≠∏ÂÆ∂„ÄÇÂú®ÂÖ©Âπ¥ÁöÑÊôÇÈñìË£°ÔºåË©≤ËÅØÁõüÈÄöÈÅé‰∏ÄÂÄãÂèçË¶ÜÈÅãÁÆóÁöÑÈÅéÁ®ãÂÆöÁæ©‰∫ÜÂèØ‰ø°Ë≥¥ AI ÁöÑÊåáÂ∞éÂéüÂâáÂíåÊúÄ‰Ω≥ÂØ¶ÂãôÔºåÂåÖÊã¨Ê∑±ÂÖ•ÁöÑÊñáÁçªÂõûÈ°ß„ÄÅ‰øÆÊîπÂæåÁöÑÂæ∑ÁàæËè≤Ë™øÊü•ÂíåÁ∑ö‰∏äÂÖ±Ë≠òÊúÉË≠∞„ÄÇFUTURE-AI Ê°ÜÊû∂ÊòØÂü∫ÊñºÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÂèØ‰ø°Ë≥¥ AI ÁöÑ 6 È†ÖÊåáÂ∞éÂéüÂâáÂª∫Á´ãÁöÑÔºåÂç≥ÂÖ¨Âπ≥ÊÄß„ÄÅÊôÆÈÅçÊÄß„ÄÅÂèØËøΩÊ∫ØÊÄß„ÄÅÂèØÁî®ÊÄß„ÄÅÂÅ•Â£ØÊÄßÂíåÂèØËß£ÈáãÊÄß„ÄÇÈÄöÈÅéÂÖ±Ë≠òÔºåÂÆöÁæ©‰∫Ü‰∏ÄÁµÑ 28 È†ÖÊúÄ‰Ω≥ÂØ¶ÂãôÔºåÊ∂µËìãÊäÄË°ì„ÄÅËá®Â∫ä„ÄÅÊ≥ïÂæãÂíåÁ§æÊúÉÂÄ´ÁêÜÂ±§Èù¢„ÄÇÂª∫Ë≠∞Ê∂µËìã‰∫ÜÈÜ´ÁôÇ AI ÁöÑÊï¥ÂÄãÁîüÂëΩÈÄ±ÊúüÔºåÂæûË®≠Ë®à„ÄÅÈñãÁôºÂíåÈ©óË≠âÂà∞Ê≥ïË¶è„ÄÅÈÉ®ÁΩ≤ÂíåÁõ£Êéß„ÄÇFUTURE-AI ÊòØ‰∏ÄÂÄãÂü∫ÊñºÈ¢®Èö™„ÄÅÁÑ°ÂÅáË®≠ÁöÑÊåáÂçóÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÊñπÊ≥ï‰æÜÊßãÂª∫ÂèØ‰ø°Ë≥¥„ÄÅÈÉ®ÁΩ≤ÂíåÊé°Áî®ÊñºÁèæÂØ¶‰∏ñÁïåÂØ¶Âãô‰∏≠ÁöÑÈÜ´ÁôÇ AI Â∑•ÂÖ∑„ÄÇÈºìÂãµÁ†îÁ©∂‰∫∫Âì°Âú®Ê¶ÇÂøµÈ©óË≠âÈöéÊÆµËÄÉÊÖÆÂª∫Ë≠∞Ôºå‰ª•‰øÉÈÄ≤Êú™‰æÜÂ∞áÈÜ´ÁôÇ AI ËΩâÂåñÁÇ∫Ëá®Â∫äÂØ¶Âãô„ÄÇ

##### **Explainable AI applications in the Medical Domain: a systematic review**
2308.05411v1 by Nicoletta Prentzas, Antonis Kakas, Constantinos S. Pattichis

Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÂú®ÈÜ´ÁôÇÈ†òÂüü‰∏≠Â∑≤ÂèñÂæóÈ°ØËëóÈÄ≤Â±ïÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉè„ÄÅÁóÖ‰∫∫ÁÖßË≠∑ÂíåÂÖ∂‰ªñÈ†òÂüü‰∏≠Âá∫Áèæ‰∫ÜÊñ∞ËààÊáâÁî®„ÄÇÈõñÁÑ∂ÈÄô‰∫õÊáâÁî®Â∑≤Âú®ÂõûÈ°ßÊÄßÁ†îÁ©∂‰∏≠Ë¢´Ë≠âÂØ¶ÊòØÊàêÂäüÁöÑÔºå‰ΩÜÂØ¶Èöõ‰∏äÂè™ÊúâÊ•µÂ∞ëÊï∏ÊáâÁî®ÊñºÂØ¶Âãô„ÄÇÈÜ´ÁôÇ AI È†òÂüüÈù¢Ëá®ËëóÂêÑÁ®ÆÊåëÊà∞ÔºåÂåÖÊã¨Âª∫Á´ã‰ΩøÁî®ËÄÖ‰ø°‰ªª„ÄÅÈÅµÂÆàÊ≥ïË¶è„ÄÅ‰ΩøÁî®Ë≥áÊñôÁ¨¶ÂêàÂÄ´ÁêÜ„ÄÇÂèØËß£Èáã AI (XAI) ÁöÑÁõÆÊ®ôÊòØËÆì‰∫∫È°û‰∫ÜËß£ AI ‰∏¶Áõ∏‰ø°ÂÖ∂ÁµêÊûú„ÄÇÊú¨ÊñáÈáùÂ∞çÊúÄËøëÂπæÂπ¥ÁôºË°®ÁöÑ 198 ÁØáÊñáÁ´†ÁöÑÂÖ∑‰ª£Ë°®ÊÄßÊ®£Êú¨ÔºåÊèêÂá∫ÊúâÈóúÈÜ´ÁôÇÊ±∫Á≠ñÊîØÊè¥ÁöÑ XAI Ëß£Ê±∫ÊñπÊ°àÁöÑÊúÄÊñ∞ÁôºÂ±ïÁöÑÊñáÁçªÂõûÈ°ß„ÄÇÁõ∏ÈóúÊñáÁ´†ÁöÑÁ≥ªÁµ±ÊÄßÁ∂úÂêàÊï¥ÁêÜÁî¢Áîü‰∫ÜÂ§öÈ†ÖÁôºÁèæÔºö(1) ÈÄô‰∫õËß£Ê±∫ÊñπÊ°àÂ§ßÂ§öÊé°Áî®ËàáÊ®°ÂûãÁÑ°ÈóúÁöÑ XAI ÊäÄË°ìÔºå(2) Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁöÑ‰ΩøÁî®ÁéáÈ´òÊñºÂÖ∂‰ªñÈ°ûÂûãÁöÑÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÔºå(3) ÂèØËß£ÈáãÊÄßË¢´Áî®Êñº‰øÉÈÄ≤‰ø°‰ªªÔºå‰ΩÜÂæàÂ∞ëÊúâÁ†îÁ©∂Â†±ÂëäÈÜ´Â∏´ÂèÉËàáËø¥ÂúàÔºå(4) Ë¶ñË¶∫Âíå‰∫íÂãïÂºè‰ΩøÁî®ËÄÖ‰ªãÈù¢Â∞çÊñºÁêÜËß£Á≥ªÁµ±ÁöÑËß£ÈáãÂíåÂª∫Ë≠∞Êõ¥ÊúâÁî®„ÄÇÈúÄË¶ÅÊõ¥Â§öÈÜ´ÁôÇÂíå AI Â∞àÂÆ∂Âêà‰ΩúÈÄ≤Ë°åÁ†îÁ©∂ÔºåÈÄôÊúâÂä©ÊñºÁÇ∫ÈÜ´ÁôÇÈ†òÂüüÁöÑ XAI Ëß£Ê±∫ÊñπÊ°àÁöÑË®≠Ë®à„ÄÅÂØ¶‰ΩúÂíåË©ï‰º∞Êèê‰æõÈÅ©Áï∂Êû∂Êßã„ÄÇ

##### **Exploring the Role of Explainability in AI-Assisted Embryo Selection**
2308.02534v1 by Lucia Urcelay, Daniel Hinjos, Pablo A. Martin-Torres, Marta Gonzalez, Marta Mendez, Salva C√≠vico, Sergio √Ålvarez-Napagao, Dario Garcia-Gasulla

In Vitro Fertilization is among the most widespread treatments for
infertility. One of its main challenges is the evaluation and selection of
embryo for implantation, a process with large inter- and intra-clinician
variability. Deep learning based methods are gaining attention, but their
opaque nature compromises their acceptance in the clinical context, where
transparency in the decision making is key. In this paper we analyze the
current work in the explainability of AI-assisted embryo analysis models,
identifying the limitations. We also discuss how these models could be
integrated in the clinical context as decision support systems, considering the
needs of clinicians and patients. Finally, we propose guidelines for the sake
of increasing interpretability and trustworthiness, pushing this technology
forward towards established clinical practice.

ÊëòË¶ÅÔºöÈ´îÂ§ñÂèóÁ≤æÊòØÊ≤ªÁôÇ‰∏çÂ≠ïÁóáÊúÄÂª£Ê≥õÁöÑÊñπÊ≥ï‰πã‰∏Ä„ÄÇÂÖ∂‰∏ªË¶ÅÊåëÊà∞‰πã‰∏ÄÊòØË©ï‰º∞ÂíåÈÅ∏ÊìáËÉöËÉéÈÄ≤Ë°åÊ§çÂÖ•ÔºåÊ≠§ÈÅéÁ®ãÂÖ∑ÊúâÂæàÂ§ßÁöÑËá®Â∫äÈñìÂíåËá®Â∫äÂÖßËÆäÁï∞ÊÄß„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÊ≠£ÂèóÂà∞ÈóúÊ≥®Ôºå‰ΩÜÂÖ∂‰∏çÈÄèÊòéÁöÑÊÄßË≥™ÊúÉÂΩ±ÈüøÂÖ∂Âú®Ëá®Â∫äÁí∞Â¢É‰∏≠ÁöÑÊé•ÂèóÂ∫¶ÔºåËÄåÈÄèÊòéÂ∫¶Âú®Ê±∫Á≠ñÂà∂ÂÆö‰∏≠Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂàÜÊûê‰∫Ü AI ËºîÂä©ËÉöËÉéÂàÜÊûêÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÊñπÈù¢ÁöÑÁèæÊúâÂ∑•‰ΩúÔºå‰∏¶ÊâæÂá∫ÂÖ∂Â±ÄÈôêÊÄß„ÄÇÊàëÂÄëÈÇÑË®éË´ñ‰∫ÜÂ¶Ç‰ΩïÂ∞áÈÄô‰∫õÊ®°Âûã‰ΩúÁÇ∫Ê±∫Á≠ñÊîØÊåÅÁ≥ªÁµ±Êï¥ÂêàÂà∞Ëá®Â∫äÁí∞Â¢É‰∏≠ÔºåÂêåÊôÇËÄÉÊÖÆËá®Â∫äÈÜ´ÁîüÂíåÊÇ£ËÄÖÁöÑÈúÄÊ±Ç„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÊèêÈ´òÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Â∫¶ÁöÑÊ∫ñÂâáÔºåÊé®ÈÄ≤ÈÄôÈ†ÖÊäÄË°ìÊúùËëóÊó¢ÂÆöÁöÑËá®Â∫äÂØ¶ÂãôÈÇÅÈÄ≤„ÄÇ

##### **A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)**
2307.14246v1 by Timo Speith, Markus Langer

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

ÊëòË¶ÅÔºöÂú®ÈúÄÊ±ÇÂ∑•Á®ã (RE) È†òÂüü‰∏≠ÔºåÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) Âú®Â∞á AI ÊîØÊåÅÁöÑÁ≥ªÁµ±Ëàá‰ΩøÁî®ËÄÖÈúÄÊ±Ç„ÄÅÁ§æÊúÉÊúüÊúõÂíåÊ≥ïË¶èÊ®ôÊ∫ñÁõ∏Á¨¶ÊñπÈù¢ÁöÑÈáçË¶ÅÊÄßÊó•ÁõäÈ°ØËëóÔºåÂ∑≤Áç≤ÂæóË™çÂèØ„ÄÇ‰∏ÄËà¨‰æÜË™™ÔºåÂèØËß£ÈáãÊÄßÂ∑≤ÊàêÁÇ∫ÂΩ±ÈüøÁ≥ªÁµ±ÂìÅË≥™ÁöÑÈáçË¶ÅÈùûÂäüËÉΩÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÂèØËß£ÈáãÊÄßËàáÊïàËÉΩ‰πãÈñìÁöÑÂÅáÂÆöÊ¨äË°°ÊåëÊà∞‰∫ÜÂèØËß£ÈáãÊÄßÁöÑÂÅáÂÆöÊ≠£Èù¢ÂΩ±Èüø„ÄÇÂ¶ÇÊûúÊªøË∂≥ÂèØËß£ÈáãÊÄßÁöÑÈúÄÊ±ÇÈúÄË¶ÅÈôç‰ΩéÁ≥ªÁµ±ÊïàËÉΩÔºåÈÇ£È∫ºÂøÖÈ†à‰ªîÁ¥∞ËÄÉÊÖÆÈÄô‰∫õÂìÅË≥™Èù¢Âêë‰∏≠Âì™‰∏ÄÂÄãÂÑ™ÂÖàÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂú®ÂÆÉÂÄë‰πãÈñìÈÄ≤Ë°åÊäòË°∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊâπÂà§ÊÄßÂú∞Êé¢Ë®é‰∫ÜÈÄôÁ®ÆÂÅáÂÆöÁöÑÊ¨äË°°„ÄÇÊàëÂÄëË™çÁÇ∫ÔºåÊúÄÂ•ΩÁöÑÊñπÊ≥ïÊòØ‰ª•‰∏ÄÁ®ÆÁ¥∞Á∑ªÁöÑÊñπÂºè‰æÜËôïÁêÜÔºåÈÄôÁ®ÆÊñπÂºèÂåÖÂê´Ë≥áÊ∫êÂèØÁî®ÊÄß„ÄÅÈ†òÂüüÁâπÊÄßÂíåÈ¢®Èö™ËÄÉÈáè„ÄÇÈÄèÈÅéÊèê‰æõÊú™‰æÜÁ†îÁ©∂ÂíåÊúÄ‰Ω≥ÂØ¶ÂãôÁöÑÂü∫Á§éÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊó®Âú®ÊèêÂçá AI ÁöÑ RE È†òÂüü„ÄÇ

##### **Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)**
2307.14239v1 by Barnaby Crook, Maximilian Schl√ºter, Timo Speith

Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.

ÊëòË¶ÅÔºöÂú®ÈúÄÊ±ÇÂ∑•Á®ãÔºàREÔºâÈ¢ÜÂüüÔºåÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩÔºàXAIÔºâÂú®Â∞Ü‰∫∫Â∑•Êô∫ËÉΩÊîØÊåÅÁöÑÁ≥ªÁªü‰∏éÁî®Êà∑ÈúÄÊ±Ç„ÄÅÁ§æ‰ºöÊúüÊúõÂíåÁõëÁÆ°Ê†áÂáÜÁõ∏‰∏ÄËá¥ÊñπÈù¢ÁöÑÈáçË¶ÅÊÄßÊó•ÁõäÂá∏ÊòæÔºåÂπ∂Ëé∑Âæó‰∫ÜËÆ§ÂèØ„ÄÇ‰∏ÄËà¨Êù•ËØ¥ÔºåÂèØËß£ÈáäÊÄßÂ∑≤Êàê‰∏∫ÂΩ±ÂìçÁ≥ªÁªüË¥®ÈáèÁöÑÈáçË¶ÅÈùûÂäüËÉΩÊÄßÈúÄÊ±Ç„ÄÇÁÑ∂ËÄåÔºåÂèØËß£ÈáäÊÄßÂíåÊÄßËÉΩ‰πãÈó¥ÁöÑÊùÉË°°ÊåëÊàò‰∫ÜÂèØËß£ÈáäÊÄßÁöÑÊ≠£Èù¢ÂΩ±Âìç„ÄÇÂ¶ÇÊûúÊª°Ë∂≥ÂèØËß£ÈáäÊÄßÁöÑË¶ÅÊ±ÇÈúÄË¶ÅÈôç‰ΩéÁ≥ªÁªüÊÄßËÉΩÔºåÈÇ£‰πàÂøÖÈ°ª‰ªîÁªÜËÄÉËôëËøô‰∫õË¥®ÈáèÊñπÈù¢‰∏≠ÁöÑÂì™‰∏Ä‰∏™‰ºòÂÖàÔºå‰ª•ÂèäÂ¶Ç‰ΩïÂú®ÂÆÉ‰ª¨‰πãÈó¥ËøõË°åÊùÉË°°„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊâπÂà§ÊÄßÂú∞ËÄÉÂØü‰∫ÜÊâÄË∞ìÁöÑÊùÉË°°„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåÊúÄÂ•Ω‰ª•‰∏ÄÁßçÁªÜËá¥ÂÖ•ÂæÆÁöÑÊñπÂºèÊù•Â§ÑÁêÜÂÆÉÔºåËøôÁßçÊñπÂºèÁªìÂêà‰∫ÜËµÑÊ∫êÂèØÁî®ÊÄß„ÄÅÈ¢ÜÂüüÁâπÂæÅÂíåÈ£éÈô©ËÄÉËôë„ÄÇÈÄöËøá‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂ÂíåÊúÄ‰Ω≥ÂÆûË∑µÊèê‰æõÂü∫Á°ÄÔºåËøôÈ°πÂ∑•‰ΩúÊó®Âú®Êé®Ëøõ‰∫∫Â∑•Êô∫ËÉΩÁöÑ RE È¢ÜÂüü„ÄÇ

##### **Acceptable risks in Europe's proposed AI Act: Reasonableness and other principles for deciding how much risk management is enough**
2308.02047v1 by Henry Fraser, Jose-Miguel Bello y Villarino

This paper critically evaluates the European Commission's proposed AI Act's
approach to risk management and risk acceptability for high-risk AI systems
that pose risks to fundamental rights and safety. The Act aims to promote
"trustworthy" AI with a proportionate regulatory burden. Its provisions on risk
acceptability require residual risks from high-risk systems to be reduced or
eliminated "as far as possible", having regard to the "state of the art". This
criterion, especially if interpreted narrowly, is unworkable and promotes
neither proportionate regulatory burden, nor trustworthiness. By contrast the
Parliament's most recent draft amendments to the risk management provisions
introduce "reasonableness", cost-benefit analysis, and are more transparent
about the value-laden and contextual nature of risk acceptability judgements.
This paper argues that the Parliament's approach is more workable, and better
balances the goals of proportionality and trustworthiness. It explains what
reasonableness in risk acceptability judgments would entail, drawing on
principles from negligence law and European medical devices regulation. And it
contends that the approach to risk acceptability judgments need a firm
foundation of civic legitimacy: including detailed guidance or involvement from
regulators, and meaningful input from affected stakeholders.

ÊëòË¶ÅÔºöÊú¨ÊñáÂö¥Ê†ºË©ï‰º∞Ê≠êÊ¥≤ÂßîÂì°ÊúÉÊèêÂá∫ÁöÑ AI Ê≥ïÊ°àÂ∞çÈ¢®Èö™ÁÆ°ÁêÜÂíåÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞çÂü∫Êú¨Ê¨äÂà©ÂíåÂÆâÂÖ®ÊßãÊàêÈ¢®Èö™ÁöÑÈ´òÈ¢®Èö™ AI Á≥ªÁµ±„ÄÇË©≤Ê≥ïÊ°àÊó®Âú®‰ª•Áõ∏Á®±ÁöÑÁõ£ÁÆ°Ë≤†Êìî‰øÉÈÄ≤„ÄåÂÄºÂæó‰ø°Ë≥¥„ÄçÁöÑ AI„ÄÇÂÖ∂ÈóúÊñºÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÁöÑÊ¢ùÊ¨æË¶ÅÊ±ÇÂ∞áÈ´òÈ¢®Èö™Á≥ªÁµ±ÁöÑÊÆòÈ§òÈ¢®Èö™Ê∏õ‰ΩéÊàñÊ∂àÈô§„ÄåÁõ°ÂèØËÉΩ„ÄçÔºå‰∏¶ËÄÉÊÖÆ„ÄåÊäÄË°ìÁãÄÊÖã„Äç„ÄÇÊ≠§Ê∫ñÂâáÔºåÁâπÂà•ÊòØÂ¶ÇÊûúÁãπÁæ©Ëß£ÈáãÔºåÁÑ°Ê≥ïÂü∑Ë°åÔºåÊó¢‰∏ç‰øÉÈÄ≤Áõ∏Á®±ÁöÑÁõ£ÁÆ°Ë≤†ÊìîÔºå‰πü‰∏ç‰øÉÈÄ≤ÂèØ‰ø°Ë≥¥ÊÄß„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåË≠∞ÊúÉÂ∞çÈ¢®Èö™ÁÆ°ÁêÜÊ¢ùÊ¨æÁöÑÊúÄÊñ∞‰øÆÊ≠£ËçâÊ°àÂºïÂÖ•‰∫Ü„ÄåÂêàÁêÜÊÄß„Äç„ÄÅÊàêÊú¨ÊïàÁõäÂàÜÊûêÔºå‰∏¶‰∏îÊõ¥ÈÄèÊòéÂú∞Ë™™Êòé‰∫ÜÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÂà§Êñ∑ÁöÑÂÉπÂÄºËßÄÂíåËÉåÊôØÊÄßË≥™„ÄÇÊú¨ÊñáË´ñË≠âË≠∞ÊúÉÁöÑÊñπÊ≥ïÊõ¥ÂèØË°åÔºå‰∏îËÉΩÊõ¥Â•ΩÂú∞Âπ≥Ë°°Áõ∏Á®±ÊÄßÂíåÂèØ‰ø°Ë≥¥ÊÄßÁöÑÁõÆÊ®ô„ÄÇÊú¨ÊñáË™™ÊòéÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÂà§Êñ∑‰∏≠ÁöÑÂêàÁêÜÊÄßÊúÉÂ∏∂‰æÜ‰ªÄÈ∫ºÔºå‰∏¶Ê†πÊìöÈÅéÂ§±Ê≥ïÂíåÊ≠êÊ¥≤ÈÜ´ÁôÇÂô®ÊùêÊ≥ïË¶è‰∏≠ÁöÑÂéüÂâáÈÄ≤Ë°åË™™Êòé„ÄÇÊú¨Êñá‰∏ªÂºµÈ¢®Èö™ÂèØÊé•ÂèóÊÄßÂà§Êñ∑ÁöÑÊñπÊ≥ïÈúÄË¶ÅÁ©©Âõ∫ÁöÑÂÖ¨Ê∞ëÂêàÊ≥ïÊÄßÂü∫Á§éÔºöÂåÖÊã¨Áõ£ÁÆ°Ê©üÊßãÁöÑË©≥Á¥∞ÊåáÂ∞éÊàñÂèÉËàáÔºå‰ª•ÂèäÂèóÂΩ±ÈüøÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑÊúâÊÑèÁæ©ÊäïÂÖ•„ÄÇ

##### **eXplainable Artificial Intelligence (XAI) in aging clock models**
2307.13704v3 by Alena Kalyakulina, Igor Yusipov, Alexey Moskalev, Claudio Franceschi, Mikhail Ivanchenko

eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the "aging clocks" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊòØÊ©üÂô®Â≠∏Áøí‰∏≠Âø´ÈÄüÈÄ≤Â±ïÁöÑÈ†òÂüüÔºåÊó®Âú®Ëß£ÈñãË§áÈõúÊ®°ÂûãÁöÑÈ†êÊ∏¨„ÄÇXAI Âú®ÊïèÊÑüÊáâÁî®‰∏≠ÁâπÂà•ÈúÄË¶ÅÔºå‰æãÂ¶ÇÂú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÔºåÁï∂Ë®∫Êñ∑„ÄÅÂª∫Ë≠∞ÂíåÊ≤ªÁôÇÈÅ∏ÊìáÂèØËÉΩ‰æùË≥¥Êñº‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ÂÅöÂá∫ÁöÑÊ±∫Á≠ñÊôÇ„ÄÇ‰∫∫Â∑•Êô∫ÊÖßÊñπÊ≥ï‰πüÂ∑≤Âª£Ê≥õÁî®ÊñºËÄÅÂåñÁ†îÁ©∂ÔºåÁâπÂà•ÊòØÂú®ÈñãÁôºÁîüÁâ©ÊôÇÈêòÊ®°ÂûãÂíåË≠òÂà•ËÄÅÂåñÂíåËàáÂπ¥ÈΩ°Áõ∏ÈóúÁñæÁóÖÁöÑÁîüÁâ©Ê®ôË™åÁâ©ÊñπÈù¢„ÄÇÁÑ∂ËÄåÔºåÈÄôË£° XAI ÁöÑÊΩõÂäõÊúâÂæÖÂÖÖÂàÜË™çË≠ò„ÄÇÊàëÂÄëË®éË´ñ‰∫Ü XAI Âú®ÈñãÁôº„ÄåËÄÅÂåñÊôÇÈêò„ÄçÊñπÈù¢ÁöÑÊáâÁî®Ôºå‰∏¶Â∞çÊåâÁâπÂÆöÁîüÁêÜÁ≥ªÁµ±ÁöÑÈáçÈªûÂàÜÈ°ûÁöÑÊñáÁçªÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂàÜÊûê„ÄÇ

##### **Interpreting and Correcting Medical Image Classification with PIP-Net**
2307.10404v2 by Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, J√∂rg Schl√∂tterer, Maurice van Keulen, Christin Seifert

Part-prototype models are explainable-by-design image classifiers, and a
promising alternative to black box AI. This paper explores the applicability
and potential of interpretable machine learning, in particular PIP-Net, for
automated diagnosis support on real-world medical imaging data. PIP-Net learns
human-understandable prototypical image parts and we evaluate its accuracy and
interpretability for fracture detection and skin cancer diagnosis. We find that
PIP-Net's decision making process is in line with medical classification
standards, while only provided with image-level class labels. Because of
PIP-Net's unsupervised pretraining of prototypes, data quality problems such as
undesired text in an X-ray or labelling errors can be easily identified.
Additionally, we are the first to show that humans can manually correct the
reasoning of PIP-Net by directly disabling undesired prototypes. We conclude
that part-prototype models are promising for medical applications due to their
interpretability and potential for advanced model debugging.

ÊëòË¶ÅÔºöÈÉ®ÂàÜÂéüÂûãÊ®°ÂûãÊòØÂèØËß£ÈáãË®≠Ë®àÁöÑÂΩ±ÂÉèÂàÜÈ°ûÂô®Ôºå‰πüÊòØÈªëÁÆ± AI ÁöÑ‰∏ÄÂÄãÊúâÂâçÈÄîÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇÈÄôÁØáË´ñÊñáÊé¢Ë®é‰∫ÜËß£ÈáãÊÄßÊ©üÂô®Â≠∏ÁøíÔºåÁâπÂà•ÊòØ PIP-NetÔºåÂú®ÁúüÂØ¶‰∏ñÁïåÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñô‰∏äËá™ÂãïÂåñË®∫Êñ∑ÊîØÊè¥ÁöÑÈÅ©Áî®ÊÄßÂíåÊΩõÂäõ„ÄÇPIP-Net Â≠∏Áøí‰∫∫È°ûÂèØÁêÜËß£ÁöÑÂéüÂûãÂΩ±ÂÉèÈÉ®ÂàÜÔºåÊàëÂÄëË©ï‰º∞ÂÖ∂Âú®È™®ÊäòÊ™¢Ê∏¨ÂíåÁöÆËÜöÁôåË®∫Êñ∑ÊñπÈù¢ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØËß£ÈáãÊÄß„ÄÇÊàëÂÄëÁôºÁèæ PIP-Net ÁöÑÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ãÁ¨¶ÂêàÈÜ´Â≠∏ÂàÜÈ°ûÊ®ôÊ∫ñÔºåÂêåÊôÇÂÉÖÊèê‰æõÂΩ±ÂÉèÂ±§Á¥öÈ°ûÂà•Ê®ôÁ±§„ÄÇÁî±Êñº PIP-Net Â∞çÂéüÂûãÁöÑÁÑ°Áõ£Áù£È†êË®ìÁ∑¥ÔºåÂõ†Ê≠§ÂèØ‰ª•ËºïÈ¨ÜË≠òÂà•Ë≥áÊñôÂìÅË≥™ÂïèÈ°åÔºå‰æãÂ¶Ç X ÂÖâ‰∏≠ÁöÑ‰∏çÈúÄË¶ÅÊñáÂ≠óÊàñÊ®ôÁ±§ÈåØË™§„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈ¶ñÊ¨°Â±ïÁ§∫‰∫∫È°ûÂèØ‰ª•ÈÄèÈÅéÁõ¥Êé•ÂÅúÁî®‰∏çÈúÄË¶ÅÁöÑÂéüÂûã‰æÜÊâãÂãï‰øÆÊ≠£ PIP-Net ÁöÑÊé®ÁêÜ„ÄÇÊàëÂÄëÂæóÂá∫ÁµêË´ñÔºåÈÉ®ÂàÜÂéüÂûãÊ®°ÂûãÁî±ÊñºÂÖ∂ÂèØËß£ÈáãÊÄßÂíåÈÄ≤ÈöéÊ®°ÂûãÈô§ÈåØÁöÑÊΩõÂäõÔºåÂõ†Ê≠§ÊúâÊúõÊáâÁî®ÊñºÈÜ´ÁôÇ„ÄÇ

##### **Explaining and visualizing black-box models through counterfactual paths**
2307.07764v3 by Bastian Pfeifer, Mateusz Krzyzinski, Hubert Baniecki, Anna Saranti, Andreas Holzinger, Przemyslaw Biecek

Explainable AI (XAI) is an increasingly important area of machine learning
research, which aims to make black-box models transparent and interpretable. In
this paper, we propose a novel approach to XAI that uses the so-called
counterfactual paths generated by conditional permutations of features. The
algorithm measures feature importance by identifying sequential permutations of
features that most influence changes in model predictions. It is particularly
suitable for generating explanations based on counterfactual paths in knowledge
graphs incorporating domain knowledge. Counterfactual paths introduce an
additional graph dimension to current XAI methods in both explaining and
visualizing black-box models. Experiments with synthetic and medical data
demonstrate the practical applicability of our approach.

ÊëòË¶ÅÔºöÂèØËß£Èáã AI (XAI) ÊòØÊ©üÂô®Â≠∏ÁøíÁ†îÁ©∂‰∏≠Êó•ÁõäÈáçË¶ÅÁöÑÈ†òÂüüÔºåÂÖ∂ÁõÆÊ®ôÊòØËÆìÈªëÁÆ±Ê®°ÂûãÈÄèÊòé‰∏îÂèØËß£Èáã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑ XAI ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®Áî±ÁâπÂæµÊ¢ù‰ª∂ÁΩÆÊèõÁî¢ÁîüÁöÑÊâÄË¨ÇÂèç‰∫ãÂØ¶Ë∑ØÂæë„ÄÇË©≤ÊºîÁÆóÊ≥ïÈÄèÈÅéË≠òÂà•ÁâπÂæµÁöÑÈ†ÜÂ∫èÁΩÆÊèõ‰æÜË°°ÈáèÁâπÂæµÈáçË¶ÅÊÄßÔºåÈÄô‰∫õÁΩÆÊèõÊúÄËÉΩÂΩ±ÈüøÊ®°ÂûãÈ†êÊ∏¨ÁöÑËÆäÂåñ„ÄÇÂÆÉÁâπÂà•ÈÅ©ÂêàÊ†πÊìöÂåÖÂê´È†òÂüüÁü•Ë≠òÁöÑÁü•Ë≠òÂúñË≠ú‰∏≠ÁöÑÂèç‰∫ãÂØ¶Ë∑ØÂæë‰æÜÁî¢ÁîüËß£Èáã„ÄÇÂèç‰∫ãÂØ¶Ë∑ØÂæëÂú®Ëß£ÈáãÂíåË¶ñË¶∫ÂåñÈªëÁÆ±Ê®°ÂûãÊôÇÔºåÁÇ∫ÁõÆÂâçÁöÑ XAI ÊñπÊ≥ïÂºïÂÖ•‰∫ÜÈ°çÂ§ñÁöÑÂúñÂΩ¢Á∂≠Â∫¶„ÄÇ‰ΩøÁî®ÂêàÊàêÂíåÈÜ´ÁôÇË≥áÊñôÈÄ≤Ë°åÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÂØ¶Áî®ÈÅ©Áî®ÊÄß„ÄÇ

##### **Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research**
2307.02131v5 by Toygar Tanyel, Serkan Ayvaz, Bilgin Keserci

The field of explainability in artificial intelligence (AI) has witnessed a
growing number of studies and increasing scholarly interest. However, the lack
of human-friendly and individual interpretations in explaining the outcomes of
machine learning algorithms has significantly hindered the acceptance of these
methods by clinicians in their research and clinical practice. To address this
issue, our study uses counterfactual explanations to explore the applicability
of "what if?" scenarios in medical research. Our aim is to expand our
understanding of magnetic resonance imaging (MRI) features used for diagnosing
pediatric posterior fossa brain tumors beyond existing boundaries. In our case
study, the proposed concept provides a novel way to examine alternative
decision-making scenarios that offer personalized and context-specific
insights, enabling the validation of predictions and clarification of
variations under diverse circumstances. Additionally, we explore the potential
use of counterfactuals for data augmentation and evaluate their feasibility as
an alternative approach in our medical research case. The results demonstrate
the promising potential of using counterfactual explanations to enhance
acceptance of AI-driven methods in clinical research.

ÊëòË¶ÅÔºöÂú®‰∫∫Â∑•Êô∫ËÉΩ (AI) ÁöÑÂèØËß£ÈáãÊÄßÈ†òÂüü‰∏≠ÔºåÂ∑≤Á∂ìÁúãÂà∞Ë∂ä‰æÜË∂äÂ§öÁöÑÁ†îÁ©∂ÂíåÂ≠∏Ë°ìËààË∂£„ÄÇÁÑ∂ËÄåÔºåÂú®Ëß£ÈáãÊ©üÂô®Â≠∏ÁøíÊºîÁÆóÊ≥ïÁöÑÁµêÊûúÊôÇÁº∫‰πè‰∫∫ÊÄßÂåñÂíåÂÄã‰∫∫ÂåñÁöÑË©ÆÈáãÔºåÈÄôÈ°ØËëóÈòªÁ§ô‰∫ÜËá®Â∫äÈÜ´ÁîüÂú®Á†îÁ©∂ÂíåËá®Â∫äÂØ¶Âãô‰∏≠Êé•ÂèóÈÄô‰∫õÊñπÊ≥ï„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÁöÑÁ†îÁ©∂‰ΩøÁî®Âèç‰∫ãÂØ¶Ëß£Èáã‰æÜÊé¢Ë®é„ÄåÂ¶ÇÊûúÔºü„ÄçÊÉÖÂ¢ÉÂú®ÈÜ´Â≠∏Á†îÁ©∂‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÊì¥Â±ïÊàëÂÄëÂ∞çÁî®ÊñºË®∫Êñ∑Â∞èÂÖíÂæåÈ°±Á™©ËÖ¶ËÖ´Áò§ÁöÑÁ£ÅÂÖ±ÊåØÊàêÂÉè (MRI) ÁâπÂæµÁöÑÁêÜËß£ÔºåË∂ÖË∂äÁèæÊúâÁöÑÁïåÁ∑ö„ÄÇÂú®ÊàëÂÄëÁöÑÊ°à‰æãÁ†îÁ©∂‰∏≠ÔºåÊâÄÊèêÂá∫ÁöÑÊ¶ÇÂøµÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ï‰æÜÊ™¢Ë¶ñÊõø‰ª£Ê±∫Á≠ñÊÉÖÂ¢ÉÔºåÊèê‰æõÂÄã‰∫∫ÂåñÂíåÁâπÂÆöÊñºÊÉÖÂ¢ÉÁöÑË¶ãËß£ÔºåÂæûËÄåËÉΩÂ§†È©óË≠âÈ†êÊ∏¨‰∏¶ÈáêÊ∏ÖÂú®‰∏çÂêåÊÉÖÊ≥Å‰∏ãÁöÑÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂèç‰∫ãÂØ¶Áî®ÊñºË≥áÊñôÊì¥ÂÖÖÁöÑÊΩõÂú®Áî®ÈÄîÔºå‰∏¶Ë©ï‰º∞ÂÖ∂‰ΩúÁÇ∫ÊàëÂÄëÈÜ´Â≠∏Á†îÁ©∂Ê°à‰æã‰∏≠Êõø‰ª£ÊñπÊ≥ïÁöÑÂèØË°åÊÄß„ÄÇÁµêÊûúË≠âÊòé‰∫Ü‰ΩøÁî®Âèç‰∫ãÂØ¶Ëß£Èáã‰æÜÂ¢ûÂº∑Ëá®Â∫äÁ†îÁ©∂‰∏≠ AI È©ÖÂãïÊñπÊ≥ïÁöÑÊé•ÂèóÂ∫¶ÁöÑÊΩõÂäõ„ÄÇ

##### **AI and Non AI Assessments for Dementia**
2307.01210v1 by Mahboobeh Parsapoor, Hamed Ghodrati, Vincenzo Dentamaro, Christopher R. Madan, Ioulietta Lazarou, Spiros Nikolopoulos, Ioannis Kompatsiaris

Current progress in the artificial intelligence domain has led to the
development of various types of AI-powered dementia assessments, which can be
employed to identify patients at the early stage of dementia. It can
revolutionize the dementia care settings. It is essential that the medical
community be aware of various AI assessments and choose them considering their
degrees of validity, efficiency, practicality, reliability, and accuracy
concerning the early identification of patients with dementia (PwD). On the
other hand, AI developers should be informed about various non-AI assessments
as well as recently developed AI assessments. Thus, this paper, which can be
readable by both clinicians and AI engineers, fills the gap in the literature
in explaining the existing solutions for the recognition of dementia to
clinicians, as well as the techniques used and the most widespread dementia
datasets to AI engineers. It follows a review of papers on AI and non-AI
assessments for dementia to provide valuable information about various dementia
assessments for both the AI and medical communities. The discussion and
conclusion highlight the most prominent research directions and the maturity of
existing solutions.

ÊëòË¶ÅÔºöÁõÆÂâç‰∫∫Â∑•Êô∫ËÉΩÈ†òÂüüÁöÑÈÄ≤Â±ïÂ∞éËá¥‰∫ÜÂêÑÁ®ÆÈ°ûÂûãÁöÑ‰∫∫Â∑•Êô∫ÊÖßÈ©ÖÂãïÁöÑÂ§±Êô∫ÁóáË©ï‰º∞ÁöÑÁôºÂ±ïÔºåÂèØÁî®ÊñºË≠òÂà•ËôïÊñºÂ§±Êô∫ÁóáÊó©ÊúüÈöéÊÆµÁöÑÊÇ£ËÄÖ„ÄÇÂÆÉÂèØ‰ª•ÂæπÂ∫ïÊîπËÆäÂ§±Êô∫ÁóáË≠∑ÁêÜË®≠ÁΩÆ„ÄÇÈáçË¶ÅÁöÑÊòØÔºåÈÜ´ÁôÇÁïåË¶Å‰∫ÜËß£ÂêÑÁ®Æ‰∫∫Â∑•Êô∫ËÉΩË©ï‰º∞Ôºå‰∏¶Ê†πÊìöÂÖ∂ÊúâÊïàÊÄß„ÄÅÊïàÁéá„ÄÅÂØ¶Áî®ÊÄß„ÄÅÂèØÈù†ÊÄßÂíåÊ∫ñÁ¢∫ÊÄßÁ®ãÂ∫¶ÔºåËÄÉÊÖÆÈÅ∏ÊìáÂÆÉÂÄë‰æÜÊó©ÊúüË≠òÂà•Â§±Êô∫ÁóáÊÇ£ËÄÖ (PwD)„ÄÇÂè¶‰∏ÄÊñπÈù¢Ôºå‰∫∫Â∑•Êô∫ËÉΩÈñãÁôº‰∫∫Âì°‰πüÊáâË©≤‰∫ÜËß£ÂêÑÁ®ÆÈùû‰∫∫Â∑•Êô∫ËÉΩË©ï‰º∞‰ª•ÂèäÊúÄËøëÈñãÁôºÁöÑ‰∫∫Â∑•Êô∫ËÉΩË©ï‰º∞„ÄÇÂõ†Ê≠§ÔºåÈÄôÁØáËá®Â∫äÈÜ´ÁîüÂíå‰∫∫Â∑•Êô∫ËÉΩÂ∑•Á®ãÂ∏´ÈÉΩÂèØ‰ª•Èñ±ËÆÄÁöÑË´ñÊñáÂ°´Ë£ú‰∫ÜÊñáÁçª‰∏≠ÈóúÊñºÂêëËá®Â∫äÈÜ´ÁîüËß£ÈáãÁèæÊúâÂ§±Êô∫ÁóáË≠òÂà•Ëß£Ê±∫ÊñπÊ°à‰ª•ÂèäÂêë‰∫∫Â∑•Êô∫ËÉΩÂ∑•Á®ãÂ∏´Ëß£ÈáãÊâÄÁî®ÊäÄË°ìÂíåÊúÄÂª£Ê≥õÁöÑÂ§±Êô∫ÁóáÊï∏ÊìöÈõÜÁöÑÁ©∫ÁôΩ„ÄÇÂÆÉÈÅµÂæ™Â∞ç‰∫∫Â∑•Êô∫ËÉΩÂíåÈùû‰∫∫Â∑•Êô∫ËÉΩÂ§±Êô∫ÁóáË©ï‰º∞Ë´ñÊñáÁöÑÂõûÈ°ßÔºåÁÇ∫‰∫∫Â∑•Êô∫ËÉΩÂíåÈÜ´ÁôÇÁïåÊèê‰æõÊúâÈóúÂêÑÁ®ÆÂ§±Êô∫ÁóáË©ï‰º∞ÁöÑÂØ∂Ë≤¥‰ø°ÊÅØ„ÄÇË®éË´ñÂíåÁµêË´ñÈáçÈªû‰ªãÁ¥π‰∫ÜÊúÄÁ™ÅÂá∫ÁöÑÁ†îÁ©∂ÊñπÂêëÂíåÁèæÊúâËß£Ê±∫ÊñπÊ°àÁöÑÊàêÁÜüÂ∫¶„ÄÇ

##### **Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation**
2306.07306v1 by Ruitao Xie, Jingbang Chen, Limai Jiang, Rui Xiao, Yi Pan, Yunpeng Cai

Explainability poses a major challenge to artificial intelligence (AI)
techniques. Current studies on explainable AI (XAI) lack the efficiency of
extracting global knowledge about the learning task, thus suffer deficiencies
such as imprecise saliency, context-aware absence and vague meaning. In this
paper, we propose the class association embedding (CAE) approach to address
these issues. We employ an encoder-decoder architecture to embed sample
features and separate them into class-related and individual-related style
vectors simultaneously. Recombining the individual-style code of a given sample
with the class-style code of another leads to a synthetic sample with preserved
individual characters but changed class assignment, following a cyclic
adversarial learning strategy. Class association embedding distills the global
class-related features of all instances into a unified domain with well
separation between classes. The transition rules between different classes can
be then extracted and further employed to individual instances. We then propose
an active XAI framework which manipulates the class-style vector of a certain
sample along guided paths towards the counter-classes, resulting in a series of
counter-example synthetic samples with identical individual characters.
Comparing these counterfactual samples with the original ones provides a
global, intuitive illustration to the nature of the classification tasks. We
adopt the framework on medical image classification tasks, which show that more
precise saliency maps with powerful context-aware representation can be
achieved compared with existing methods. Moreover, the disease pathology can be
directly visualized via traversing the paths in the class-style space.

ÊëòË¶ÅÔºö<paragraph>ÂèØËß£ÈáãÊÄßÂ∞ç‰∫∫Â∑•Êô∫ÊÖß (AI) ÊäÄË°ìÊßãÊàê‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇÁï∂ÂâçÂ∞çÂèØËß£Èáã AI (XAI) ÁöÑÁ†îÁ©∂Áº∫‰πèÊèêÂèñÂ≠∏Áøí‰ªªÂãôÊï¥È´îÁü•Ë≠òÁöÑÊïàÁéáÔºåÂõ†Ê≠§Â≠òÂú®‰∏çÁ≤æÁ¢∫ÁöÑÈ°ØËëóÊÄß„ÄÅËàáÊÉÖÂ¢ÉÁÑ°ÈóúÁöÑÁº∫Â§±ÂíåÂê´Á≥äÊÑèÁæ©Á≠âÁº∫Èô∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫È°ûÂà•ÈóúËÅØÂµåÂÖ• (CAE) ÊñπÊ≥ï‰æÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°å„ÄÇÊàëÂÄëÊé°Áî®Á∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Êû∂Êßã‰æÜÂµåÂÖ•Ê®£Êú¨ÁâπÂæµÔºå‰∏¶ÂêåÊôÇÂ∞áÂÆÉÂÄëÂàÜÁÇ∫È°ûÂà•Áõ∏ÈóúÂíåÂÄãÈ´îÁõ∏ÈóúÁöÑÊ®£ÂºèÂêëÈáè„ÄÇÂ∞áÁµ¶ÂÆöÊ®£Êú¨ÁöÑÂÄãÈ´îÊ®£Âºè‰ª£Á¢ºËàáÂè¶‰∏ÄÂÄãÊ®£Êú¨ÁöÑÈ°ûÂà•Ê®£Âºè‰ª£Á¢ºÈáçÊñ∞ÁµÑÂêàÔºåÊúÉÁî¢Áîü‰∏ÄÂÄãÂÖ∑Êúâ‰øùÁïôÂÄãÈ´îÁâπÂæµ‰ΩÜÊîπËÆäÈ°ûÂà•ÂàÜÈÖçÁöÑÂêàÊàêÊ®£Êú¨ÔºåÈÅµÂæ™Âæ™Áí∞Â∞çÊäóÂ≠∏ÁøíÁ≠ñÁï•„ÄÇÈ°ûÂà•ÈóúËÅØÂµåÂÖ•Â∞áÊâÄÊúâÂØ¶‰æãÁöÑÂÖ®Â±ÄÈ°ûÂà•Áõ∏ÈóúÁâπÂæµÊèêÁÖâÂà∞‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÈ†òÂüü‰∏≠Ôºå‰∏¶Âú®È°ûÂà•‰πãÈñìÊúâËâØÂ•ΩÁöÑÂçÄÂàÜ„ÄÇÁÑ∂ÂæåÂèØ‰ª•ÊèêÂèñ‰∏çÂêåÈ°ûÂà•‰πãÈñìÁöÑËΩâÊèõË¶èÂâáÔºå‰∏¶ÈÄ≤‰∏ÄÊ≠•ÊáâÁî®ÊñºÂÄãÂà•ÂØ¶‰æã„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰∏ªÂãï XAI Ê°ÜÊû∂ÔºåÂÆÉÊ≤øËëóÂºïÂ∞éË∑ØÂæëÊìç‰ΩúÁâπÂÆöÊ®£Êú¨ÁöÑÈ°ûÂà•Ê®£ÂºèÂêëÈáèÔºåÊúùËëóÂèçÈ°ûÂà•ÁßªÂãïÔºåÂæûËÄåÁî¢Áîü‰∏ÄÁ≥ªÂàóÂÖ∑ÊúâÁõ∏ÂêåÂÄãÈ´îÁâπÂæµÁöÑÂèç‰æãÂêàÊàêÊ®£Êú¨„ÄÇÂ∞áÈÄô‰∫õÂèç‰∫ãÂØ¶Ê®£Êú¨ËàáÂéüÂßãÊ®£Êú¨ÈÄ≤Ë°åÊØîËºÉÔºåÂèØ‰ª•Â∞çÂàÜÈ°û‰ªªÂãôÁöÑÊÄßË≥™Êèê‰æõÂÖ®Â±Ä„ÄÅÁõ¥ËßÄÁöÑË™™Êòé„ÄÇÊàëÂÄëÊé°Áî®Ë©≤Ê°ÜÊû∂ÈÄ≤Ë°åÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰ªªÂãôÔºåÁµêÊûúË°®ÊòéÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÂèØ‰ª•Áç≤ÂæóÊõ¥Á≤æÁ¢∫ÁöÑÈ°ØËëóÊÄßÂúñÔºå‰∏¶ÂÖ∑ÊúâÂº∑Â§ßÁöÑËàáÊÉÖÂ¢ÉÁÑ°ÈóúÁöÑË°®Á§∫„ÄÇÊ≠§Â§ñÔºåÁñæÁóÖÁóÖÁêÜÂ≠∏ÂèØ‰ª•Áõ¥Êé•ÈÄöÈÅéÂú®È°ûÂà•Ê®£ÂºèÁ©∫Èñì‰∏≠ÈÅçÊ≠∑Ë∑ØÂæë‰æÜÈÄ≤Ë°åÂèØË¶ñÂåñ„ÄÇ</paragraph>

##### **HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence for Digital Medicine**
2306.06029v1 by Rodrigo Agerri, I√±igo Alonso, Aitziber Atutxa, Ander Berrondo, Ainara Estarrona, Iker Garcia-Ferrero, Iakes Goenaga, Koldo Gojenola, Maite Oronoz, Igor Perez-Tejedor, German Rigau, Anar Yeginbergenova

Providing high quality explanations for AI predictions based on machine
learning is a challenging and complex task. To work well it requires, among
other factors: selecting a proper level of generality/specificity of the
explanation; considering assumptions about the familiarity of the explanation
beneficiary with the AI task under consideration; referring to specific
elements that have contributed to the decision; making use of additional
knowledge (e.g. expert evidence) which might not be part of the prediction
process; and providing evidence supporting negative hypothesis. Finally, the
system needs to formulate the explanation in a clearly interpretable, and
possibly convincing, way. Given these considerations, ANTIDOTE fosters an
integrated vision of explainable AI, where low-level characteristics of the
deep learning process are combined with higher level schemes proper of the
human argumentation capacity. ANTIDOTE will exploit cross-disciplinary
competences in deep learning and argumentation to support a broader and
innovative view of explainable AI, where the need for high-quality explanations
for clinical cases deliberation is critical. As a first result of the project,
we publish the Antidote CasiMedicos dataset to facilitate research on
explainable AI in general, and argumentation in the medical domain in
particular.

ÊëòË¶ÅÔºöÊèê‰æõÂü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑ AI È†êÊ∏¨ÁöÑÈ´òÂìÅË≥™Ë™™ÊòéÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÂíåË§áÈõúÊÄßÁöÑ‰ªªÂãô„ÄÇË¶ÅÈ†ÜÂà©ÈÄ≤Ë°åÔºåÂÆÉÈúÄË¶ÅÂÖ∑ÂÇô‰∏ãÂàóÂõ†Á¥†ÔºöÈÅ∏ÊìáÈÅ©Áï∂ÁöÑË™™ÊòéÊôÆÈÅçÊÄß/ÁâπÊÆäÊÄßÂ±§Á¥öÔºõËÄÉÈáèË™™ÊòéÂèóÁõä‰∫∫Â∞çÊâÄËÄÉÊÖÆÁöÑ AI ‰ªªÂãôÁöÑÁÜüÊÇâÁ®ãÂ∫¶ÂÅáË®≠ÔºõÂèÉÁÖß‰øÉÊàêÊ±∫Á≠ñÁöÑÁâπÂÆöÂÖÉÁ¥†ÔºõÂà©Áî®ÂèØËÉΩ‰∏çÂ±¨ÊñºÈ†êÊ∏¨Á®ãÂ∫èÁöÑ‰∏ÄÈÉ®ÂàÜÁöÑÈ°çÂ§ñÁü•Ë≠òÔºà‰æãÂ¶ÇÂ∞àÂÆ∂Ë≠âÊìöÔºâÔºõ‰∏¶Êèê‰æõÊîØÊåÅÂê¶ÂÆöÂÅáË®≠ÁöÑË≠âÊìö„ÄÇÊúÄÂæåÔºåÁ≥ªÁµ±ÈúÄË¶Å‰ª•Ê∏ÖÊô∞ÂèØËß£Èáã‰∏îÂèØËÉΩ‰ª§‰∫∫‰ø°ÊúçÁöÑÊñπÂºèÂà∂ÂÆöË™™Êòé„ÄÇÂü∫ÊñºÈÄô‰∫õËÄÉÈáèÔºåANTIDOTE ‰øÉÊàê‰∫ÜÂèØËß£Èáã AI ÁöÑÊï¥ÂêàÈ°òÊôØÔºåÂÖ∂‰∏≠Ê∑±Â∫¶Â≠∏ÁøíÁ®ãÂ∫èÁöÑ‰ΩéÈöéÁâπÂæµËàá‰∫∫È°ûË´ñË≠âËÉΩÂäõÁöÑÈ´òÈöéÊû∂ÊßãÁõ∏ÁµêÂêà„ÄÇANTIDOTE Â∞áÂà©Áî®Ê∑±Â∫¶Â≠∏ÁøíËàáË´ñË≠âÁöÑË∑®È†òÂüüËÉΩÂäõÔºå‰æÜÊîØÊåÅÂèØËß£Èáã AI Êõ¥Âª£Ê≥õ‰∏îÂâµÊñ∞ÁöÑËßÄÈªûÔºåÂÖ∂‰∏≠Â∞çËá®Â∫äÊ°à‰æãÂØ©Ë≠∞ÁöÑÈ´òÂìÅË≥™Ë™™ÊòéÈúÄÊ±ÇËá≥ÈóúÈáçË¶Å„ÄÇ‰ΩúÁÇ∫Ë©≤Â∞àÊ°àÁöÑÁ¨¨‰∏ÄÂÄãÊàêÊûúÔºåÊàëÂÄëÁôºÂ∏É‰∫Ü Antidote CasiMedicos Ë≥áÊñôÈõÜÔºå‰ª•Âà©Êñº‰∏ÄËà¨ÂèØËß£Èáã AI ÁöÑÁ†îÁ©∂ÔºåÁâπÂà•ÊòØÈÜ´ÁôÇÈ†òÂüüÁöÑË´ñË≠â„ÄÇ

##### **XInsight: Revealing Model Insights for GNNs with Flow-based Explanations**
2306.04791v1 by Eli Laird, Ayesh Madushanka, Elfi Kraka, Corey Clark

Progress in graph neural networks has grown rapidly in recent years, with
many new developments in drug discovery, medical diagnosis, and recommender
systems. While this progress is significant, many networks are `black boxes'
with little understanding of the `what' exactly the network is learning. Many
high-stakes applications, such as drug discovery, require human-intelligible
explanations from the models so that users can recognize errors and discover
new knowledge. Therefore, the development of explainable AI algorithms is
essential for us to reap the benefits of AI.
  We propose an explainability algorithm for GNNs called eXplainable Insight
(XInsight) that generates a distribution of model explanations using GFlowNets.
Since GFlowNets generate objects with probabilities proportional to a reward,
XInsight can generate a diverse set of explanations, compared to previous
methods that only learn the maximum reward sample. We demonstrate XInsight by
generating explanations for GNNs trained on two graph classification tasks:
classifying mutagenic compounds with the MUTAG dataset and classifying acyclic
graphs with a synthetic dataset that we have open-sourced. We show the utility
of XInsight's explanations by analyzing the generated compounds using QSAR
modeling, and we find that XInsight generates compounds that cluster by
lipophilicity, a known correlate of mutagenicity. Our results show that
XInsight generates a distribution of explanations that uncovers the underlying
relationships demonstrated by the model. They also highlight the importance of
generating a diverse set of explanations, as it enables us to discover hidden
relationships in the model and provides valuable guidance for further analysis.

ÊëòË¶ÅÔºö<paragraph>ËøëÂπ¥‰æÜÔºåÂúñÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÈÄ≤Â±ïËøÖÈÄüÔºåÂú®Ëó•Áâ©ÁôºÁèæ„ÄÅÈÜ´ÁôÇË®∫Êñ∑ÂíåÊé®Ëñ¶Á≥ªÁµ±ÊñπÈù¢ÈÉΩÊúâË®±Â§öÊñ∞ÁôºÂ±ï„ÄÇÈõñÁÑ∂ÈÄô‰∫õÈÄ≤Â±ïÂæàÈáçË¶ÅÔºå‰ΩÜË®±Â§öÁ∂≤Ë∑ØÈÉΩÊòØ„ÄåÈªëÁõíÂ≠ê„ÄçÔºåÂ∞çÊñºÁ∂≤Ë∑ØÂà∞Â∫ïÂú®Â≠∏Áøí„Äå‰ªÄÈ∫º„Äç‰∫ÜËß£ÁîöÂ∞ë„ÄÇË®±Â§öÈ´òÈ¢®Èö™ÊáâÁî®Ôºå‰æãÂ¶ÇËó•Áâ©ÁôºÁèæÔºåÈúÄË¶ÅÊ®°ÂûãÊèê‰æõ‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑËß£ÈáãÔºå‰ª•‰æø‰ΩøÁî®ËÄÖÂèØ‰ª•Ëæ®Ë≠òÈåØË™§‰∏¶ÁôºÁèæÊñ∞Áü•Ë≠ò„ÄÇÂõ†Ê≠§ÔºåÂèØËß£Èáã AI ÊºîÁÆóÊ≥ïÁöÑÈñãÁôºÂ∞çÊñºÊàëÂÄëÁç≤Âèñ AI ÁöÑÂ•ΩËôïËá≥ÈóúÈáçË¶Å„ÄÇ
ÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫ eXplainable Insight (XInsight) ÁöÑ GNN ÂèØËß£ÈáãÊÄßÊºîÁÆóÊ≥ïÔºåÂÆÉ‰ΩøÁî® GFlowNets Áî¢ÁîüÊ®°ÂûãËß£ÈáãÂàÜ‰Ωà„ÄÇÁî±Êñº GFlowNets ÊúÉÁî¢ÁîüÊ©üÁéáËàáÁçéÂãµÊàêÊ≠£ÊØîÁöÑÁâ©‰ª∂ÔºåÂõ†Ê≠§ËàáÂÖàÂâçÂÉÖÂ≠∏ÁøíÊúÄÂ§ßÁçéÂãµÁØÑ‰æãÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåXInsight ÂèØ‰ª•Áî¢ÁîüÂ§öÊ®£ÂåñÁöÑËß£ÈáãÈõÜÂêà„ÄÇÊàëÂÄëÈÄèÈÅéÁÇ∫Âú®ÂÖ©ÂÄãÂúñÂΩ¢ÂàÜÈ°û‰ªªÂãô‰∏≠Ë®ìÁ∑¥ÁöÑ GNN Áî¢ÁîüËß£Èáã‰æÜÂ±ïÁ§∫ XInsightÔºö‰ΩøÁî® MUTAG Ë≥áÊñôÈõÜÂ∞çËá¥Á™ÅËÆäÂåñÂêàÁâ©ÈÄ≤Ë°åÂàÜÈ°ûÔºå‰∏¶‰ΩøÁî®ÊàëÂÄëÂ∑≤ÈñãÊîæÂéüÂßãÁ¢ºÁöÑÂêàÊàêË≥áÊñôÈõÜÂ∞çÈùûÁí∞ÁãÄÂúñÂΩ¢ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÊàëÂÄëÈÄèÈÅé‰ΩøÁî® QSAR Âª∫Ê®°ÂàÜÊûêÁî¢ÁîüÁöÑÂåñÂêàÁâ©‰æÜÂ±ïÁ§∫ XInsight Ëß£ÈáãÁöÑÊïàÁî®ÔºåÊàëÂÄëÁôºÁèæ XInsight ÊúÉÁî¢ÁîüÊåâË¶™ËÑÇÊÄßÔºàÂ∑≤Áü•ÁöÑËá¥Á™ÅËÆäÁõ∏ÈóúÊÄßÔºâÂàÜÁæ§ÁöÑÂåñÂêàÁâ©„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ XInsight ÊúÉÁî¢Áîü‰∏ÄÂÄãËß£ÈáãÂàÜ‰ΩàÔºåÊè≠Á§∫Ê®°ÂûãÊâÄÂ±ïÁ§∫ÁöÑÂ∫ïÂ±§Èóú‰øÇ„ÄÇÂÆÉÂÄë‰πüÂº∑Ë™øÁî¢ÁîüÂ§öÊ®£ÂåñËß£ÈáãÈõÜÂêàÁöÑÈáçË¶ÅÊÄßÔºåÂõ†ÁÇ∫ÂÆÉ‰ΩøÊàëÂÄëËÉΩÂ§†ÁôºÁèæÊ®°Âûã‰∏≠ÁöÑÈö±ËóèÈóú‰øÇÔºå‰∏¶ÁÇ∫ÈÄ≤‰∏ÄÊ≠•ÂàÜÊûêÊèê‰æõÊúâÂÉπÂÄºÁöÑÊåáÂ∞é„ÄÇ</paragraph>

##### **Explainable AI using expressive Boolean formulas**
2306.03976v1 by Gili Rosenberg, J. Kyle Brubaker, Martin J. A. Schuetz, Grant Salton, Zhihuai Zhu, Elton Yechao Zhu, Serdar Kadƒ±oƒülu, Sima E. Borujeni, Helmut G. Katzgraber

We propose and implement an interpretable machine learning classification
model for Explainable AI (XAI) based on expressive Boolean formulas. Potential
applications include credit scoring and diagnosis of medical conditions. The
Boolean formula defines a rule with tunable complexity (or interpretability),
according to which input data are classified. Such a formula can include any
operator that can be applied to one or more Boolean variables, thus providing
higher expressivity compared to more rigid rule-based and tree-based
approaches. The classifier is trained using native local optimization
techniques, efficiently searching the space of feasible formulas. Shallow rules
can be determined by fast Integer Linear Programming (ILP) or Quadratic
Unconstrained Binary Optimization (QUBO) solvers, potentially powered by
special purpose hardware or quantum devices. We combine the expressivity and
efficiency of the native local optimizer with the fast operation of these
devices by executing non-local moves that optimize over subtrees of the full
Boolean formula. We provide extensive numerical benchmarking results featuring
several baselines on well-known public datasets. Based on the results, we find
that the native local rule classifier is generally competitive with the other
classifiers. The addition of non-local moves achieves similar results with
fewer iterations, and therefore using specialized or quantum hardware could
lead to a speedup by fast proposal of non-local moves.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏¶ÂØ¶‰Ωú‰∏ÄÂÄãÂèØËß£ÈáãÊ©üÂô®Â≠∏ÁøíÂàÜÈ°ûÊ®°ÂûãÔºåÁî®ÊñºÂü∫ÊñºË°®ÈÅîÂºèÂ∏ÉÊûóÂÖ¨ÂºèÁöÑÂèØËß£Èáã AI (XAI)„ÄÇÊΩõÂú®ÊáâÁî®ÂåÖÊã¨‰ø°Áî®Ë©ïÂàÜÂíåÈÜ´ÁôÇÁãÄÊ≥ÅË®∫Êñ∑„ÄÇÂ∏ÉÊûóÂÖ¨ÂºèÂÆöÁæ©‰∫Ü‰∏ÄÂÄãÂÖ∑ÊúâÂèØË™øÊï¥Ë§áÈõúÊÄßÔºàÊàñÂèØËß£ÈáãÊÄßÔºâÁöÑË¶èÂâáÔºåÊ†πÊìöË©≤Ë¶èÂâáÂ∞çËº∏ÂÖ•Êï∏ÊìöÈÄ≤Ë°åÂàÜÈ°û„ÄÇÈÄôÊ®£ÁöÑÂÖ¨ÂºèÂèØ‰ª•ÂåÖÂê´‰ªª‰ΩïÂèØÊáâÁî®Êñº‰∏ÄÂÄãÊàñÂ§öÂÄãÂ∏ÉÊûóËÆäÊï∏ÁöÑÈÅãÁÆóÂ≠êÔºåÂæûËÄåËàáÊõ¥Âö¥Ê†ºÁöÑÂü∫ÊñºË¶èÂâáÂíåÂü∫ÊñºÊ®πÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÊèê‰æõÊõ¥È´òÁöÑË°®ÈÅîËÉΩÂäõ„ÄÇÂàÜÈ°ûÂô®‰ΩøÁî®ÂéüÁîüÂ±ÄÈÉ®ÊúÄ‰Ω≥ÂåñÊäÄË°ìÈÄ≤Ë°åË®ìÁ∑¥ÔºåÊúâÊïàÂú∞ÊêúÁ¥¢ÂèØË°åÂÖ¨ÂºèÁöÑÁ©∫Èñì„ÄÇÊ∑∫Â±§Ë¶èÂâáÂèØ‰ª•Áî®Âø´ÈÄüÁöÑÊï¥Êï∏Á∑öÊÄßË¶èÂäÉ (ILP) Êàñ‰∫åÊ¨°ÁÑ°Á¥ÑÊùü‰∫åÂÖÉÊúÄ‰Ω≥Âåñ (QUBO) Ê±ÇËß£Âô®‰æÜÁ¢∫ÂÆöÔºåÈÄô‰∫õÊ±ÇËß£Âô®ÂèØËÉΩÁî±ÁâπÊÆäÁî®ÈÄîÁöÑÁ°¨È´îÊàñÈáèÂ≠êË£ùÁΩÆÊèê‰æõÊîØÊè¥„ÄÇÊàëÂÄëÂ∞áÂéüÁîüÂ±ÄÈÉ®ÊúÄ‰Ω≥ÂåñÂô®ÁöÑË°®ÈÅîËÉΩÂäõÂíåÊïàÁéáËàáÈÄô‰∫õË£ùÁΩÆÁöÑÂø´ÈÄüÈÅãÁÆóÁõ∏ÁµêÂêàÔºåÈÄèÈÅéÂü∑Ë°åÈùûÂ±ÄÈÉ®ÁßªÂãï‰æÜÊúÄ‰Ω≥ÂåñÂÆåÊï¥Â∏ÉÊûóÂÖ¨ÂºèÁöÑÂ≠êÊ®π„ÄÇÊàëÂÄëÊèê‰æõÂª£Ê≥õÁöÑÊï∏ÂÄºÂü∫Ê∫ñÊ∏¨Ë©¶ÁµêÊûúÔºåÂÖ∂‰∏≠ÂåÖÂê´Âú®ÁúæÊâÄÂë®Áü•ÁöÑÂÖ¨ÂÖ±Ë≥áÊñôÈõÜ‰∏ä‰ΩøÁî®Â§öÂÄãÂü∫Á∑ö„ÄÇÊ†πÊìöÁµêÊûúÔºåÊàëÂÄëÁôºÁèæÂéüÁîüÂ±ÄÈÉ®Ë¶èÂâáÂàÜÈ°ûÂô®ÈÄöÂ∏∏ËàáÂÖ∂‰ªñÂàÜÈ°ûÂô®ÂÖ∑ÊúâÁ´∂Áà≠Âäõ„ÄÇÂä†ÂÖ•ÈùûÂ±ÄÈÉ®ÁßªÂãï‰ª•ËºÉÂ∞ëÁöÑÂèçË¶ÜÈÅãÁÆóÊ¨°Êï∏ÈÅîÊàêÈ°û‰ººÁöÑÁµêÊûúÔºåÂõ†Ê≠§‰ΩøÁî®Â∞àÁî®ÊàñÈáèÂ≠êÁ°¨È´îÂèØËÉΩÊúÉÈÄèÈÅéÂø´ÈÄüÊèêÂá∫ÈùûÂ±ÄÈÉ®ÁßªÂãï‰æÜÂä†ÈÄü„ÄÇ

##### **Utterance Classification with Logical Neural Network: Explainable AI for Mental Disorder Diagnosis**
2306.03902v1 by Yeldar Toleubay, Don Joven Agravante, Daiki Kimura, Baihan Lin, Djallel Bouneffouf, Michiaki Tatsubori

In response to the global challenge of mental health problems, we proposes a
Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis
of mental disorders. Due to the lack of effective therapy coverage for mental
disorders, there is a need for an AI solution that can assist therapists with
the diagnosis. However, current Neural Network models lack explainability and
may not be trusted by therapists. The LNN is a Recurrent Neural Network
architecture that combines the learning capabilities of neural networks with
the reasoning capabilities of classical logic-based AI. The proposed system
uses input predicates from clinical interviews to output a mental disorder
class, and different predicate pruning techniques are used to achieve
scalability and higher scores. In addition, we provide an insight extraction
method to aid therapists with their diagnosis. The proposed system addresses
the lack of explainability of current Neural Network models and provides a more
trustworthy solution for mental disorder diagnosis.

ÊëòË¶ÅÔºöÁÇ∫‰∫ÜËß£Ê±∫ÂøÉÁêÜÂÅ•Â∫∑ÂïèÈ°åÁöÑÂÖ®ÁêÉÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÈÇèËºØÁ•ûÁ∂ìÁ∂≤Ë∑Ø (LNN) ÁöÑÁ•ûÁ∂ìÁ¨¶Ëôü AI ÊñπÊ≥ï‰æÜË®∫Êñ∑ÂøÉÁêÜÁñæÁóÖ„ÄÇÁî±ÊñºÁº∫‰πèÊúâÊïàÁöÑÂøÉÁêÜÁñæÁóÖÊ≤ªÁôÇÊ∂µËìãÁØÑÂúçÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏ÄÁ®Æ AI Ëß£Ê±∫ÊñπÊ°à‰æÜÂçîÂä©Ê≤ªÁôÇÂ∏´ÈÄ≤Ë°åË®∫Êñ∑„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁº∫‰πèÂèØËß£ÈáãÊÄßÔºåÊ≤ªÁôÇÂ∏´ÂèØËÉΩÁÑ°Ê≥ï‰ø°‰ªªÂÆÉÂÄë„ÄÇLNN ÊòØ‰∏ÄÁ®ÆÈÅûËø¥Á•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÔºåÂÆÉÁµêÂêà‰∫ÜÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂ≠∏ÁøíËÉΩÂäõÂíåÂü∫ÊñºÁ∂ìÂÖ∏ÈÇèËºØÁöÑ AI ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±‰ΩøÁî®‰æÜËá™Ëá®Â∫äË®™Ë´áÁöÑËº∏ÂÖ•Ë¨ÇË©û‰æÜËº∏Âá∫ÂøÉÁêÜÁñæÁóÖÈ°ûÂà•Ôºå‰∏¶‰ΩøÁî®‰∏çÂêåÁöÑË¨ÇË©ûÂâ™ÊûùÊäÄË°ì‰æÜÂØ¶ÁèæÂèØÊì¥ÂÖÖÊÄßÂíåÊõ¥È´òÁöÑÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãË¶ãËß£ÊèêÂèñÊñπÊ≥ï‰æÜÂçîÂä©Ê≤ªÁôÇÂ∏´ÈÄ≤Ë°åË®∫Êñ∑„ÄÇÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±Ëß£Ê±∫‰∫ÜÁï∂ÂâçÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁº∫‰πèÂèØËß£ÈáãÊÄßÁöÑÂïèÈ°åÔºå‰∏¶ÁÇ∫ÂøÉÁêÜÁñæÁóÖË®∫Êñ∑Êèê‰æõ‰∫ÜÊõ¥ÂÄºÂæó‰ø°Ë≥¥ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models**
2306.01668v1 by Sujith K Mandala

As machine learning models become increasingly prevalent in medical
diagnostics, the need for interpretability and transparency becomes paramount.
The XAI Renaissance signifies a significant shift in the field, aiming to
redefine the interpretability of medical diagnostic models. This paper explores
the innovative approaches and methodologies within the realm of Explainable AI
(XAI) that are revolutionizing the interpretability of medical diagnostic
models. By shedding light on the underlying decision-making process, XAI
techniques empower healthcare professionals to understand, trust, and
effectively utilize these models for accurate and reliable medical diagnoses.
This review highlights the key advancements in XAI for medical diagnostics and
their potential to transform the healthcare landscape, ultimately improving
patient outcomes and fostering trust in AI-driven diagnostic systems.

ÊëòË¶ÅÔºöÈö®ËëóÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂú®ÈÜ´ÁôÇË®∫Êñ∑‰∏≠Ë∂ä‰æÜË∂äÊôÆÈÅçÔºåÂèØËß£ÈáãÊÄßÂíåÈÄèÊòéÂ∫¶ÁöÑÈúÄÊ±ÇËÆäÂæóËá≥ÈóúÈáçË¶Å„ÄÇXAI Âæ©ËààÊ®ôË™åËëóË©≤È†òÂüüÁöÑÈáçÂ§ßËΩâËÆäÔºåÊó®Âú®ÈáçÊñ∞ÂÆöÁæ©ÈÜ´ÁôÇË®∫Êñ∑Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄß„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÂèØËß£Èáã AI (XAI) È†òÂüüÂÖßÁöÑÂâµÊñ∞ÊñπÊ≥ïÂíåÊñπÊ≥ïË´ñÔºåÈÄô‰∫õÊñπÊ≥ïÂíåÊñπÊ≥ïË´ñÊ≠£Âú®Èù©Êñ∞ÈÜ´ÁôÇË®∫Êñ∑Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄß„ÄÇÈÄöÈÅéÈó°ÊòéÂü∫Á§éÊ±∫Á≠ñÂà∂ÂÆöÈÅéÁ®ãÔºåXAI ÊäÄË°ì‰ΩøÈÜ´ÁôÇ‰øùÂÅ•Â∞àÊ•≠‰∫∫Âì°ËÉΩÂ§†ÁêÜËß£„ÄÅ‰ø°‰ªª‰∏¶ÊúâÊïàÂú∞Âà©Áî®ÈÄô‰∫õÊ®°ÂûãÈÄ≤Ë°åÊ∫ñÁ¢∫‰∏îÂèØÈù†ÁöÑÈÜ´ÁôÇË®∫Êñ∑„ÄÇÊú¨Á∂úËø∞ÈáçÈªû‰ªãÁ¥π‰∫Ü XAI Âú®ÈÜ´ÁôÇË®∫Êñ∑ÊñπÈù¢ÁöÑÈóúÈçµÈÄ≤Â±ïÂèäÂÖ∂ËΩâËÆäÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÁöÑÊΩõÂäõÔºåÊúÄÁµÇÊîπÂñÑÊÇ£ËÄÖÁöÑÊ≤ªÁôÇÊïàÊûú‰∏¶ÂüπÈ§äÂ∞ç AI È©ÖÂãïÁöÑË®∫Êñ∑Á≥ªÁµ±ÁöÑ‰ø°‰ªª„ÄÇ

##### **A Novel real-time arrhythmia detection model using YOLOv8**
2305.16727v3 by Guang Jun Nicholas Ang, Aritejh Kr Goil, Henryk Chan, Jieyi Jeric Lew, Xin Chun Lee, Raihan Bin Ahmad Mustaffa, Timotius Jason, Ze Ting Woon, Bingquan Shen

In a landscape characterized by heightened connectivity and mobility, coupled
with a surge in cardiovascular ailments, the imperative to curtail healthcare
expenses through remote monitoring of cardiovascular health has become more
pronounced. The accurate detection and classification of cardiac arrhythmias
are pivotal for diagnosing individuals with heart irregularities. This study
underscores the feasibility of employing electrocardiograms (ECG) measurements
in the home environment for real-time arrhythmia detection. Presenting a fresh
application for arrhythmia detection, this paper leverages the cutting-edge
You-Only-Look-Once (YOLO)v8 algorithm to categorize single-lead ECG signals. We
introduce a novel loss-modified YOLOv8 model, fine-tuned on the MIT-BIH
arrhythmia dataset, enabling real-time continuous monitoring. The obtained
results substantiate the efficacy of our approach, with the model attaining an
average accuracy of 99.5% and 0.992 mAP@50, and a rapid detection time of 0.002
seconds on an NVIDIA Tesla V100. Our investigation exemplifies the potential of
real-time arrhythmia detection, enabling users to visually interpret the model
output within the comfort of their homes. Furthermore, this study lays the
groundwork for an extension into a real-time explainable AI (XAI) model capable
of deployment in the healthcare sector, thereby significantly advancing the
realm of healthcare solutions.

ÊëòË¶ÅÔºö<paragraph>Âú®‰ª•È´òÂ∫¶ÈÄ£Êé•ÊÄßÂíåÊµÅÂãïÊÄßÁÇ∫ÁâπÂæµÁöÑÁí∞Â¢É‰∏≠ÔºåÂä†‰∏äÂøÉË°ÄÁÆ°ÁñæÁóÖÁöÑÊøÄÂ¢ûÔºåÈÄöÈÅéÈÅ†Á®ãÁõ£ÊéßÂøÉË°ÄÁÆ°ÂÅ•Â∫∑‰æÜÂâäÊ∏õÈÜ´ÁôÇ‰øùÂÅ•ÊîØÂá∫ÁöÑÂøÖË¶ÅÊÄßËÆäÂæóÊõ¥Âä†ÊòéÈ°Ø„ÄÇÊ∫ñÁ¢∫Ê™¢Ê∏¨ÂíåÂàÜÈ°ûÂøÉÂæã‰∏çÊï¥Â∞çÊñºË®∫Êñ∑ÊÇ£ÊúâÂøÉËáü‰∏çË¶èÂâáÁöÑ‰∫∫Ëá≥ÈóúÈáçË¶Å„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫ÜÂú®ÂÆ∂‰∏≠‰ΩøÁî®ÂøÉÈõªÂúñ (ECG) Ê∏¨ÈáèÈÄ≤Ë°åÂØ¶ÊôÇÂøÉÂæã‰∏çÊï¥Ê™¢Ê∏¨ÁöÑÂèØË°åÊÄß„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂøÉÂæã‰∏çÊï¥Ê™¢Ê∏¨ÊáâÁî®ÔºåÂà©Áî®Â∞ñÁ´ØÁöÑ You-Only-Look-Once (YOLO)v8 ÊºîÁÆóÊ≥ïÂ∞çÂñÆÂ∞éËÅØ ECG Ë®äËôüÈÄ≤Ë°åÂàÜÈ°û„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊêçÂ§±‰øÆÊîπ YOLOv8 Ê®°ÂûãÔºå‰∏¶ÈáùÂ∞ç MIT-BIH ÂøÉÂæã‰∏çÊï¥Ë≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂæÆË™øÔºåÂæûËÄåÂØ¶Áèæ‰∫ÜÂØ¶ÊôÇÁöÑÊåÅÁ∫åÁõ£Êéß„ÄÇÁç≤ÂæóÁöÑÁµêÊûúË≠âÂØ¶‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåË©≤Ê®°ÂûãÂú® NVIDIA Tesla V100 ‰∏äÈÅîÂà∞‰∫Ü 99.5% ÁöÑÂπ≥ÂùáÊ∫ñÁ¢∫Â∫¶Âíå 0.992 mAP@50Ôºå‰ª•Âèä 0.002 ÁßíÁöÑÂø´ÈÄüÊ™¢Ê∏¨ÊôÇÈñì„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Ë™™Êòé‰∫ÜÂØ¶ÊôÇÂøÉÂæã‰∏çÊï¥Ê™¢Ê∏¨ÁöÑÊΩõÂäõÔºå‰ΩøÁî®Êà∂ËÉΩÂ§†Âú®ÂÆ∂‰∏≠ËàíÈÅ©Âú∞Ë¶ñË¶∫ÂåñËß£ËÆÄÊ®°ÂûãËº∏Âá∫„ÄÇÊ≠§Â§ñÔºåÊú¨Á†îÁ©∂ÁÇ∫Êì¥Â±ïÂà∞ÂØ¶ÊôÇÂèØËß£Èáã AI (XAI) Ê®°ÂûãÂ•†ÂÆö‰∫ÜÂü∫Á§éÔºåË©≤Ê®°ÂûãËÉΩÂ§†ÈÉ®ÁΩ≤Âú®ÈÜ´ÁôÇ‰øùÂÅ•È†òÂüüÔºåÂæûËÄåÈ°ØËëóÊé®ÈÄ≤ÈÜ´ÁôÇ‰øùÂÅ•Ëß£Ê±∫ÊñπÊ°àÁöÑÈ†òÂüü„ÄÇ</paragraph>

##### **Breast Cancer Segmentation using Attention-based Convolutional Network and Explainable AI**
2305.14389v2 by Jai Vardhan, Taraka Satya Krishna Teja Malisetti

Breast cancer (BC) remains a significant health threat, with no long-term
cure currently available. Early detection is crucial, yet mammography
interpretation is hindered by high false positives and negatives. With BC
incidence projected to surpass lung cancer, improving early detection methods
is vital. Thermography, using high-resolution infrared cameras, offers promise,
especially when combined with artificial intelligence (AI). This work presents
an attention-based convolutional neural network for segmentation, providing
increased speed and precision in BC detection and classification. The system
enhances images and performs cancer segmentation with explainable AI. We
propose a transformer-attention-based convolutional architecture (UNet) for
fault identification and employ Gradient-weighted Class Activation Mapping
(Grad-CAM) to analyze areas of bias and weakness in the UNet architecture with
IRT images. The superiority of our proposed framework is confirmed when
compared with existing deep learning frameworks.

ÊëòË¶ÅÔºö‰π≥ÁôåÔºàBCÔºâ‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÂÅ•Â∫∑Â®ÅËÑÖÔºåÁõÆÂâçÂ∞öÁÑ°Èï∑ÊúüÊ≤ªÁôíÁöÑÊñπÊ≥ï„ÄÇÊó©ÊúüÁôºÁèæËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜ‰π≥ÊàøÊîùÂΩ±ÁöÑÂà§ËÆÄÂçªÂèóÂà∞È´òÂÅáÈôΩÊÄßÂíåÂÅáÈô∞ÊÄßÁöÑÈòªÁ§ô„ÄÇÁî±Êñº‰π≥ÁôåÁöÑÁôºÁîüÁéáÈ†êË®àÂ∞áË∂ÖÈÅéËÇ∫ÁôåÔºåÂõ†Ê≠§ÊîπÂñÑÊó©ÊúüÊ™¢Ê∏¨ÊñπÊ≥ïËá≥ÈóúÈáçË¶Å„ÄÇÁÜ±ÂÉèÊîùÂΩ±‰ΩøÁî®È´òËß£ÊûêÂ∫¶Á¥ÖÂ§ñÁ∑öÁõ∏Ê©üÔºåÁâπÂà•ÊòØÂú®Ëàá‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÁµêÂêà‰ΩøÁî®ÊôÇÔºåÊèê‰æõ‰∫ÜÂ∏åÊúõ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØÁî®ÊñºÂàÜÂâ≤ÔºåÂú®‰π≥ÁôåÊ™¢Ê∏¨ÂíåÂàÜÈ°û‰∏≠Êèê‰æõ‰∫ÜÊõ¥È´òÁöÑÈÄüÂ∫¶ÂíåÁ≤æÂ∫¶„ÄÇË©≤Á≥ªÁµ±Â¢ûÂº∑ÂΩ±ÂÉè‰∏¶Âü∑Ë°åÂèØËß£ÈáãÁöÑ AI ÁôåÁóáÂàÜÂâ≤„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºTransformerÊ≥®ÊÑèÂäõÁöÑÂç∑Á©çÊû∂ÊßãÔºàUNetÔºâÁî®ÊñºÊïÖÈöúË≠òÂà•Ôºå‰∏¶‰ΩøÁî®Ê¢ØÂ∫¶Âä†Ê¨äÈ°ûÊøÄÊ¥ªÊò†Â∞ÑÔºàGrad-CAMÔºâ‰æÜÂàÜÊûê UNet Êû∂Êßã‰∏≠ÂÅèË¶ãÂíåÂº±ÈªûÁöÑÂçÄÂüüÔºå‰ΩøÁî® IRT ÂΩ±ÂÉè„ÄÇËàáÁèæÊúâÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ°ÜÊû∂Áõ∏ÊØîÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊ°ÜÊû∂ÁöÑÂÑ™Ë∂äÊÄßÂæóÂà∞Ë≠âÂØ¶„ÄÇ

##### **What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media**
2305.13127v2 by Junwei Kuang, Jiaheng Xie, Zhijun Yan

Depression is the most prevalent and serious mental illness, which induces
grave financial and societal ramifications. Depression detection is key for
early intervention to mitigate those consequences. Such a high-stake decision
inherently necessitates interpretability. Although a few depression detection
studies attempt to explain the decision based on the importance score or
attention weights, these explanations misalign with the clinical depression
diagnosis criterion that is based on depressive symptoms. To fill this gap, we
follow the computational design science paradigm to develop a novel Multi-Scale
Temporal Prototype Network (MSTPNet). MSTPNet innovatively detects and
interprets depressive symptoms as well as how long they last. Extensive
empirical analyses using a large-scale dataset show that MSTPNet outperforms
state-of-the-art depression detection methods with an F1-score of 0.851. This
result also reveals new symptoms that are unnoted in the survey approach, such
as sharing admiration for a different life. We further conduct a user study to
demonstrate its superiority over the benchmarks in interpretability. This study
contributes to IS literature with a novel interpretable deep learning model for
depression detection in social media. In practice, our proposed method can be
implemented in social media platforms to provide personalized online resources
for detected depressed patients.

ÊëòË¶ÅÔºöÊÜÇÈ¨±ÁóáÊòØÊúÄÊôÆÈÅç‰∏îÂö¥ÈáçÁöÑÁ≤æÁ•ûÁñæÁóÖÔºåÊúÉÈÄ†ÊàêÂö¥ÈáçÁöÑË≤°ÂãôÂíåÁ§æÊúÉÂæåÊûú„ÄÇÊÜÇÈ¨±ÁóáÁöÑÂÅµÊ∏¨Â∞çÊñºÊó©Êúü‰ªãÂÖ•‰ª•Ê∏õËºïÈÄô‰∫õÂæåÊûúËá≥ÈóúÈáçË¶Å„ÄÇÂ¶ÇÊ≠§ÈáçÂ§ßÁöÑÊ±∫ÂÆöÊú¨Ë≥™‰∏äÈúÄË¶ÅÂèØËß£ÈáãÊÄß„ÄÇÂÑòÁÆ°‰∏Ä‰∫õÊÜÇÈ¨±ÁóáÂÅµÊ∏¨Á†îÁ©∂ÂòóË©¶Ê†πÊìöÈáçË¶ÅÊÄßÂàÜÊï∏ÊàñÊ≥®ÊÑèÂäõÊ¨äÈáç‰æÜËß£ÈáãÈÄôÂÄãÊ±∫ÂÆöÔºå‰ΩÜÈÄô‰∫õËß£ÈáãËàáÂü∫ÊñºÊÜÇÈ¨±ÁóáÁãÄÁöÑËá®Â∫äÊÜÇÈ¨±ÁóáË®∫Êñ∑Ê®ôÊ∫ñ‰∏ç‰∏ÄËá¥„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁº∫Âè£ÔºåÊàëÂÄëÈÅµÂæ™Ë®àÁÆóË®≠Ë®àÁßëÂ≠∏ÁØÑ‰æã‰æÜÈñãÁôº‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ§öÂ∞∫Â∫¶ÊôÇÈñìÂéüÂûãÁ∂≤Ë∑Ø (MSTPNet)„ÄÇMSTPNet ÂâµÊñ∞Âú∞ÂÅµÊ∏¨‰∏¶Ëß£ÈáãÊÜÇÈ¨±ÁóáÁãÄ‰ª•ÂèäÂÆÉÂÄëÊåÅÁ∫åÂ§ö‰πÖ„ÄÇ‰ΩøÁî®Â§ßË¶èÊ®°Ë≥áÊñôÈõÜÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶Ë≠âÂàÜÊûêÈ°ØÁ§∫ÔºåMSTPNet ‰ª• 0.851 ÁöÑ F1 ÂàÜÊï∏ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊÜÇÈ¨±ÁóáÂÅµÊ∏¨ÊñπÊ≥ï„ÄÇÊ≠§ÁµêÊûúÈÇÑÊè≠Á§∫‰∫ÜË™øÊü•ÊñπÊ≥ï‰∏≠Êú™Ê≥®ÊÑèÂà∞ÁöÑÊñ∞ÁóáÁãÄÔºå‰æãÂ¶ÇÂàÜ‰∫´Â∞ç‰∏çÂêåÁîüÊ¥ªÁöÑÊ¨Ω‰Ω©„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄ≤Ë°å‰ΩøÁî®ËÄÖÁ†îÁ©∂Ôºå‰ª•Ë≠âÊòéÂÖ∂Âú®ÂèØËß£ÈáãÊÄßÊñπÈù¢ÂÑ™ÊñºÂü∫Ê∫ñ„ÄÇÊú¨Á†îÁ©∂‰ª•‰∏ÄÂÄãÊñ∞Á©éÁöÑÂèØËß£ÈáãÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁÇ∫ÊÜÇÈ¨±ÁóáÂÅµÊ∏¨Âú®Á§æÁæ§Â™íÈ´î‰∏≠ÁöÑ IS ÊñáÁçªÂÅöÂá∫Ë≤¢Áçª„ÄÇÂú®ÂØ¶Âãô‰∏äÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂèØ‰ª•ÂØ¶‰ΩúÂú®Á§æÁæ§Â™íÈ´îÂπ≥Âè∞‰∏≠Ôºå‰ª•Êèê‰æõÂÄã‰∫∫ÂåñÁöÑÁ∑ö‰∏äË≥áÊ∫êÁµ¶Ë¢´ÂÅµÊ∏¨Âá∫ÊÜÇÈ¨±ÁóáÁöÑÊÇ£ËÄÖ„ÄÇ

##### **Echoes of Biases: How Stigmatizing Language Affects AI Performance**
2305.10201v4 by Yizhi Liu, Weiguang Wang, Guodong Gordon Gao, Ritu Agarwal

Electronic health records (EHRs) serve as an essential data source for the
envisioned artificial intelligence (AI)-driven transformation in healthcare.
However, clinician biases reflected in EHR notes can lead to AI models
inheriting and amplifying these biases, perpetuating health disparities. This
study investigates the impact of stigmatizing language (SL) in EHR notes on
mortality prediction using a Transformer-based deep learning model and
explainable AI (XAI) techniques. Our findings demonstrate that SL written by
clinicians adversely affects AI performance, particularly so for black
patients, highlighting SL as a source of racial disparity in AI model
development. To explore an operationally efficient way to mitigate SL's impact,
we investigate patterns in the generation of SL through a clinicians'
collaborative network, identifying central clinicians as having a stronger
impact on racial disparity in the AI model. We find that removing SL written by
central clinicians is a more efficient bias reduction strategy than eliminating
all SL in the entire corpus of data. This study provides actionable insights
for responsible AI development and contributes to understanding clinician
behavior and EHR note writing in healthcare.

ÊëòË¶ÅÔºöÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑ (EHR) ‰ΩúÁÇ∫È†êÊÉ≥‰∏≠Áî±‰∫∫Â∑•Êô∫ÊÖß (AI) Êé®ÂãïÁöÑÈÜ´ÁôÇ‰øùÂÅ•ËΩâÂûãÁöÑÈáçË¶ÅË≥áÊñô‰æÜÊ∫ê„ÄÇÁÑ∂ËÄåÔºåÂèçÊò†Âú® EHR ÂÇôË®ª‰∏≠ÁöÑËá®Â∫äÂÅèË¶ãÂèØËÉΩÂ∞éËá¥ AI Ê®°ÂûãÁπºÊâø‰∏¶Êì¥Â§ßÈÄô‰∫õÂÅèË¶ãÔºåÈÄ≤ËÄåÈÄ†ÊàêÂÅ•Â∫∑Â∑ÆÁï∞„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é EHR ÂÇôË®ª‰∏≠Ê±ôÂêçÂåñË™ûË®Ä (SL) Â∞ç‰ΩøÁî®Âü∫Êñº Transformer ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂíåÂèØËß£Èáã AI (XAI) ÊäÄË°ìÈ†êÊ∏¨Ê≠ª‰∫°ÁéáÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÁî±Ëá®Â∫äÈÜ´ÁîüÊí∞ÂØ´ÁöÑ SL ÊúÉÂ∞ç AI ÊïàËÉΩÁî¢Áîü‰∏çÂà©ÂΩ±ÈüøÔºåÁâπÂà•ÊòØÂ∞çÈªë‰∫∫ÊÇ£ËÄÖËÄåË®ÄÔºåÁ™ÅÈ°Ø SL ÊòØ AI Ê®°ÂûãÈñãÁôº‰∏≠Á®ÆÊóèÂ∑ÆÁï∞ÁöÑ‰æÜÊ∫ê„ÄÇÁÇ∫‰∫ÜÊé¢Á¥¢‰∏ÄÁ®ÆÈÅã‰Ωú‰∏äÊúâÊïàÁéáÁöÑÊñπÊ≥ï‰æÜÊ∏õËºï SL ÁöÑÂΩ±ÈüøÔºåÊàëÂÄëÈÄèÈÅéËá®Â∫äÈÜ´ÁîüÁöÑÂçî‰ΩúÁ∂≤Ë∑ØÊé¢Ë®é SL Áî¢ÁîüÁöÑÊ®°ÂºèÔºå‰∏¶ÊâæÂá∫Ê†∏ÂøÉËá®Â∫äÈÜ´ÁîüÂ∞ç AI Ê®°Âûã‰∏≠ÁöÑÁ®ÆÊóèÂ∑ÆÁï∞ÊúâËºÉÂ§ßÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÁôºÁèæÔºåÁßªÈô§Áî±Ê†∏ÂøÉËá®Â∫äÈÜ´ÁîüÊí∞ÂØ´ÁöÑ SL ÊòØÊØîÊ∂àÈô§Ë≥áÊñôÈõÜ‰∏≠ÊâÄÊúâ SL Êõ¥ÊúâÊïàÁéáÁöÑÂÅèË¶ãÊ∏õÂ∞ëÁ≠ñÁï•„ÄÇÊú¨Á†îÁ©∂Êèê‰æõÂèØË°åÁöÑË¶ãËß£ÔºåÁî®ÊñºË≤†Ë≤¨‰ªªÁöÑ AI ÈñãÁôºÔºå‰∏¶ÊúâÂä©Êñº‰∫ÜËß£Ëá®Â∫äÈÜ´ÁîüË°åÁÇ∫ÂíåÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑ EHR ÂÇôË®ªÊí∞ÂØ´„ÄÇ

##### **Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour**
2305.03376v1 by Goda Klumbyte, Hannah Piehl, Claude Draude

Contemporary automation through AI entails a substantial amount of
behind-the-scenes human labour, which is often both invisibilised and
underpaid. Since invisible labour, including labelling and maintenance work, is
an integral part of contemporary AI systems, it remains important to sensitise
users to its role. We suggest that this could be done through explainable AI
(XAI) design, particularly feminist intersectional XAI. We propose the method
of cartography, which stems from feminist intersectional research, to draw out
a systemic perspective of AI and include dimensions of AI that pertain to
invisible labour.

ÊëòË¶ÅÔºöÁï∂‰ª£ÈÄöÈÅé AI ÁöÑËá™ÂãïÂåñÈúÄË¶ÅÂ§ßÈáèÁöÑÂπïÂæå‰∫∫ÂäõÔºåÈÄôÈÄöÂ∏∏Êó¢‰∏çÂèØË¶ã‰∏îËñ™Ë≥áÈÅé‰Ωé„ÄÇÁî±Êñº‰∏çÂèØË¶ãÁöÑÂãûÂãïÔºåÂåÖÊã¨Ê®ôÁ±§ÂíåÁ∂≠Ë≠∑Â∑•‰ΩúÔºåÊòØÁï∂‰ª£ AI Á≥ªÁµ±ÁöÑÁµÑÊàêÈÉ®ÂàÜÔºåÂõ†Ê≠§ËÆì‰ΩøÁî®ËÄÖ‰∫ÜËß£ÂÖ∂ËßíËâ≤‰ªçÁÑ∂ÂæàÈáçË¶Å„ÄÇÊàëÂÄëÂª∫Ë≠∞ÈÄôÂèØ‰ª•ÈÄèÈÅéÂèØËß£ÈáãÁöÑ AIÔºàXAIÔºâË®≠Ë®à‰æÜÂÆåÊàêÔºåÁâπÂà•ÊòØÂ•≥ÊÄß‰∏ªÁæ©‰∫§ÂèâÁöÑ XAI„ÄÇÊàëÂÄëÊèêÂá∫Ê∫êËá™Â•≥ÊÄß‰∏ªÁæ©‰∫§ÂèâÁ†îÁ©∂ÁöÑË£ΩÂúñÊñπÊ≥ïÔºå‰ª•ÊèêÂá∫ AI ÁöÑÁ≥ªÁµ±ËßÄÈªûÔºå‰∏¶Á¥çÂÖ•Ëàá‰∏çÂèØË¶ãÂãûÂãïÁõ∏ÈóúÁöÑ AI Á∂≠Â∫¶„ÄÇ

##### **Towards Explainable and Safe Conversational Agents for Mental Health: A Survey**
2304.13191v1 by Surjodeep Sarkar, Manas Gaur, L. Chen, Muskan Garg, Biplav Srivastava, Bhaktee Dongaonkar

Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to
support the overburdened global healthcare system that gets 60 million primary
care visits, and 6 million Emergency Room (ER) visits annually. These systems
are built by clinical psychologists, psychiatrists, and Artificial Intelligence
(AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role
of VMHAs is to provide emotional support through information, focusing less on
developing a reflective conversation with the patient. A more comprehensive,
safe and explainable approach is required to build responsible VMHAs to ask
follow-up questions or provide a well-informed response. This survey offers a
systematic critical review of the existing conversational agents in mental
health, followed by new insights into the improvements of VMHAs with contextual
knowledge, datasets, and their emerging role in clinical decision support. We
also provide new directions toward enriching the user experience of VMHAs with
explainability, safety, and wholesome trustworthiness. Finally, we provide
evaluation metrics and practical considerations for VMHAs beyond the current
literature to build trust between VMHAs and patients in active communications.

ÊëòË¶ÅÔºöËôõÊì¨ÂøÉÁêÜÂÅ•Â∫∑Âä©ÁêÜ (VMHA) ÊåÅÁ∫åÈÄ≤Ê≠•Ôºå‰ª•ÊîØÊè¥ÊØèÂπ¥Êúâ 6000 Ëê¨‰∫∫Ê¨°ÂàùÁ¥ö‰øùÂÅ•Â∞±Ë®∫Âíå 600 Ëê¨‰∫∫Ê¨°ÊÄ•Ë®∫ÂÆ§ (ER) Â∞±Ë®∫ÁöÑË∂ÖË≤†Ëç∑ÂÖ®ÁêÉÈÜ´ÁôÇ‰øùÂÅ•Á≥ªÁµ±„ÄÇÈÄô‰∫õÁ≥ªÁµ±ÊòØÁî±Ëá®Â∫äÂøÉÁêÜÂ≠∏ÂÆ∂„ÄÅÁ≤æÁ•ûÁßëÈÜ´Â∏´Âíå‰∫∫Â∑•Êô∫ÊÖß (AI) Á†îÁ©∂‰∫∫Âì°ÁÇ∫Ë™çÁü•Ë°åÁÇ∫ÁôÇÊ≥ï (CBT) ÊâÄÂª∫Êßã„ÄÇÁõÆÂâçÔºåVMHA ÁöÑËßíËâ≤ÊòØÈÄèÈÅéË≥áË®äÊèê‰æõÊÉÖÁ∑íÊîØÊåÅÔºåËºÉÂ∞ëËëóÈáçÊñºËàáÊÇ£ËÄÖÁôºÂ±ïÂèçÊÄùÊÄßÁöÑÂ∞çË©±„ÄÇÈúÄË¶ÅÊõ¥ÂÖ®Èù¢„ÄÅÂÆâÂÖ®‰∏îÂèØËß£ÈáãÁöÑÊñπÊ≥ï‰æÜÂª∫ÊßãË≤†Ë≤¨‰ªªÁöÑ VMHAÔºå‰ª•ÊèêÂá∫ÂæåÁ∫åÂïèÈ°åÊàñÊèê‰æõÂÖÖÂàÜÁöÑÂõûÊáâ„ÄÇÈÄôÈ†ÖË™øÊü•Êèê‰æõ‰∫ÜÂ∞çÂøÉÁêÜÂÅ•Â∫∑‰∏≠ÁèæÊúâÂ∞çË©±‰ª£ÁêÜÁöÑÁ≥ªÁµ±ÊÄßÊâπÂà§ÊÄßÂõûÈ°ßÔºåÊé•ËëóÊ∑±ÂÖ•Êé¢Ë®é‰∫Ü VMHA Âú®ËÑàÁµ°Áü•Ë≠ò„ÄÅË≥áÊñôÈõÜÂíåÂÖ∂Âú®Ëá®Â∫äÊ±∫Á≠ñÊîØÊè¥‰∏≠Êñ∞ËààËßíËâ≤ÁöÑÊîπÈÄ≤„ÄÇÊàëÂÄë‰πüÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊñπÂêëÔºå‰ª•ÈÄèÈÅéÂèØËß£ÈáãÊÄß„ÄÅÂÆâÂÖ®ÊÄßËàáÊï¥È´îÂèØ‰ø°Â∫¶‰æÜË±êÂØå VMHA ÁöÑ‰ΩøÁî®ËÄÖÈ´îÈ©ó„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèê‰æõ‰∫ÜË©ïÈáèÊåáÊ®ôÂíå VMHA ÁöÑÂØ¶ÂãôËÄÉÈáèÔºåË∂ÖË∂äÁõÆÂâçÁöÑÊñáÁçªÔºåÂú® VMHA ËàáÊÇ£ËÄÖÁöÑÁ©çÊ•µÊ∫ùÈÄö‰∏≠Âª∫Á´ã‰ø°‰ªª„ÄÇ

##### **A Brief Review of Explainable Artificial Intelligence in Healthcare**
2304.01543v1 by Zahra Sadeghi, Roohallah Alizadehsani, Mehmet Akif Cifci, Samina Kausar, Rizwan Rehman, Priyakshi Mahanta, Pranjal Kumar Bora, Ammar Almasri, Rami S. Alkhawaldeh, Sadiq Hussain, Bilal Alatas, Afshin Shoeibi, Hossein Moosaei, Milan Hladik, Saeid Nahavandi, Panos M. Pardalos

XAI refers to the techniques and methods for building AI applications which
assist end users to interpret output and predictions of AI models. Black box AI
applications in high-stakes decision-making situations, such as medical domain
have increased the demand for transparency and explainability since wrong
predictions may have severe consequences. Model explainability and
interpretability are vital successful deployment of AI models in healthcare
practices. AI applications' underlying reasoning needs to be transparent to
clinicians in order to gain their trust. This paper presents a systematic
review of XAI aspects and challenges in the healthcare domain. The primary
goals of this study are to review various XAI methods, their challenges, and
related machine learning models in healthcare. The methods are discussed under
six categories: Features-oriented methods, global methods, concept models,
surrogate models, local pixel-based methods, and human-centric methods. Most
importantly, the paper explores XAI role in healthcare problems to clarify its
necessity in safety-critical applications. The paper intends to establish a
comprehensive understanding of XAI-related applications in the healthcare field
by reviewing the related experimental results. To facilitate future research
for filling research gaps, the importance of XAI models from different
viewpoints and their limitations are investigated.

ÊëòË¶ÅÔºöXAI ÊåáÁöÑÊòØÁî®ÊñºÂª∫Êßã AI ÊáâÁî®Á®ãÂºèÁöÑÊäÄË°ìÂíåÊñπÊ≥ïÔºåÈÄô‰∫õÊáâÁî®Á®ãÂºèÂèØÂçîÂä©ÊúÄÁµÇ‰ΩøÁî®ËÄÖË©ÆÈáã AI Ê®°ÂûãÁöÑËº∏Âá∫ÂíåÈ†êÊ∏¨„ÄÇÂú®È´òÈ¢®Èö™Ê±∫Á≠ñÊÉÖÂ¢É‰∏≠Ôºå‰æãÂ¶ÇÈÜ´ÁôÇÈ†òÂüüÔºåÈªëÁÆ± AI ÊáâÁî®Á®ãÂºèÂ¢ûÂä†‰∫ÜÈÄèÊòéÂ∫¶ÂíåÂèØËß£ÈáãÊÄßÁöÑÈúÄÊ±ÇÔºåÂõ†ÁÇ∫ÈåØË™§ÁöÑÈ†êÊ∏¨ÂèØËÉΩÊúÉÈÄ†ÊàêÂö¥ÈáçÁöÑÂæåÊûú„ÄÇÊ®°ÂûãÂèØËß£ÈáãÊÄßÂíåÂèØË©ÆÈáãÊÄßÂ∞çÊñºÂú®ÈÜ´ÁôÇÂØ¶Âãô‰∏≠ÊàêÂäüÈÉ®ÁΩ≤ AI Ê®°ÂûãËá≥ÈóúÈáçË¶Å„ÄÇAI ÊáâÁî®Á®ãÂºèÁöÑÂü∫Êú¨Êé®ÁêÜÈúÄË¶ÅÂ∞çËá®Â∫äÈÜ´ÁîüÈÄèÊòéÔºåÊâçËÉΩÁç≤Âæó‰ªñÂÄëÁöÑ‰ø°‰ªª„ÄÇÊú¨ÊñáÊèê‰æõ‰∫ÜÈÜ´ÁôÇÈ†òÂüü‰∏≠ XAI Èù¢ÂêëÂíåÊåëÊà∞ÁöÑÁ≥ªÁµ±ÊÄßÂõûÈ°ß„ÄÇÊú¨Á†îÁ©∂ÁöÑ‰∏ªË¶ÅÁõÆÊ®ôÊòØÂõûÈ°ßÂêÑÁ®Æ XAI ÊñπÊ≥ï„ÄÅÂÖ∂ÊåëÊà∞Ôºå‰ª•ÂèäÁõ∏ÈóúÁöÑÈÜ´ÁôÇ‰øùÂÅ•Ê©üÂô®Â≠∏ÁøíÊ®°Âûã„ÄÇÈÄô‰∫õÊñπÊ≥ïÂàÜÁÇ∫ÂÖ≠È°ûË®éË´ñÔºöÈù¢ÂêëÁâπÂæµÁöÑÊñπÊ≥ï„ÄÅÊï¥È´îÊñπÊ≥ï„ÄÅÊ¶ÇÂøµÊ®°Âûã„ÄÅ‰ª£ÁêÜÊ®°Âûã„ÄÅÂ±ÄÈÉ®Âü∫ÊñºÂÉèÁ¥†ÁöÑÊñπÊ≥ïÔºå‰ª•Âèä‰ª•‰∫∫ÁÇ∫‰∏≠ÂøÉÁöÑÊñπÊ≥ï„ÄÇÊúÄÈáçË¶ÅÁöÑÊòØÔºåÊú¨ÊñáÊé¢Ë®é‰∫Ü XAI Âú®ÈÜ´ÁôÇ‰øùÂÅ•ÂïèÈ°å‰∏≠ÁöÑËßíËâ≤Ôºå‰ª•ÈáêÊ∏ÖÂÖ∂Âú®ÂÆâÂÖ®ÈóúÈçµÊáâÁî®‰∏≠ÁöÑÂøÖË¶ÅÊÄß„ÄÇÊú¨ÊñáÊó®Âú®ÈÄèÈÅéÂõûÈ°ßÁõ∏ÈóúÁöÑÂØ¶È©óÁµêÊûúÔºåÂª∫Á´ãÂ∞çÈÜ´ÁôÇ‰øùÂÅ•È†òÂüü‰∏≠ XAI Áõ∏ÈóúÊáâÁî®Á®ãÂºèÁöÑÂÖ®Èù¢‰∫ÜËß£„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Êú™‰æÜÁ†îÁ©∂Â°´Ë£úÁ†îÁ©∂Â∑ÆË∑ùÔºåÊú¨ÊñáÊé¢Ë®é‰∫Ü XAI Ê®°ÂûãÂæû‰∏çÂêåËßÄÈªû‰æÜÁúãÁöÑÈáçË¶ÅÊÄßÂèäÂÖ∂ÈôêÂà∂„ÄÇ

##### **Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models**
2303.12641v2 by Frederik Pahde, Maximilian Dreyer, Wojciech Samek, Sebastian Lapuschkin

State-of-the-art machine learning models often learn spurious correlations
embedded in the training data. This poses risks when deploying these models for
high-stake decision-making, such as in medical applications like skin cancer
detection. To tackle this problem, we propose Reveal to Revise (R2R), a
framework entailing the entire eXplainable Artificial Intelligence (XAI) life
cycle, enabling practitioners to iteratively identify, mitigate, and
(re-)evaluate spurious model behavior with a minimal amount of human
interaction. In the first step (1), R2R reveals model weaknesses by finding
outliers in attributions or through inspection of latent concepts learned by
the model. Secondly (2), the responsible artifacts are detected and spatially
localized in the input data, which is then leveraged to (3) revise the model
behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model
correction, and (4) (re-)evaluate the model's performance and remaining
sensitivity towards the artifact. Using two medical benchmark datasets for
Melanoma detection and bone age estimation, we apply our R2R framework to VGG,
ResNet and EfficientNet architectures and thereby reveal and correct real
dataset-intrinsic artifacts, as well as synthetic variants in a controlled
setting. Completing the XAI life cycle, we demonstrate multiple R2R iterations
to mitigate different biases. Code is available on
https://github.com/maxdreyer/Reveal2Revise.

ÊëòË¶ÅÔºöÊúÄÂÖàËøõÁöÑÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÈÄöÂ∏∏‰ºöÂ≠¶‰π†ËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠ÂµåÂÖ•ÁöÑËôöÂÅáÂÖ≥ËÅî„ÄÇËøôÂú®Â∞ÜËøô‰∫õÊ®°ÂûãÈÉ®ÁΩ≤‰∫éÈ´òÈ£éÈô©ÂÜ≥Á≠ñÊó∂‰ºöÂ∏¶Êù•È£éÈô©Ôºå‰æãÂ¶ÇÂú®ÁöÆËÇ§ÁôåÊ£ÄÊµãÁ≠âÂåªÂ≠¶Â∫îÁî®‰∏≠„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü Reveal to Revise (R2R)Ôºå‰∏Ä‰∏™Ê∂µÁõñÊï¥‰∏™ÂèØËß£Èáä‰∫∫Â∑•Êô∫ËÉΩ (XAI) ÁîüÂëΩÂë®ÊúüÁöÑÊ°ÜÊû∂Ôºå‰Ωø‰ªé‰∏öËÄÖËÉΩÂ§ü‰ª•ÊúÄÂ∞ëÁöÑ‰∫∫Â∑•‰∫§‰∫íËø≠‰ª£ËØÜÂà´„ÄÅÁºìËß£ÂíåÔºàÈáçÊñ∞ÔºâËØÑ‰º∞ËôöÂÅáÊ®°ÂûãË°å‰∏∫„ÄÇÂú®Á¨¨‰∏ÄÊ≠• (1) ‰∏≠ÔºåR2R ÈÄöËøáÊâæÂá∫ÂΩíÂõ†‰∏≠ÁöÑÂºÇÂ∏∏ÂÄºÊàñÈÄöËøáÊ£ÄÊü•Ê®°ÂûãÂ≠¶‰π†ÁöÑÊΩúÂú®Ê¶ÇÂøµÊù•Êè≠Á§∫Ê®°ÂûãÁöÑÂº±ÁÇπ„ÄÇÂÖ∂Ê¨° (2)ÔºåÊ£ÄÊµãË¥üË¥£ÁöÑ‰º™ÂÉèÂπ∂Âú®ËæìÂÖ•Êï∞ÊçÆ‰∏≠ËøõË°åÁ©∫Èó¥ÂÆö‰ΩçÔºåÁÑ∂ÂêéÂà©Áî®ÂÆÉÊù• (3) ‰øÆÊîπÊ®°ÂûãË°å‰∏∫„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Â∫îÁî® RRR„ÄÅCDEP Âíå ClArC ÁöÑÊñπÊ≥ïÊù•ËøõË°åÊ®°ÂûãÊ†°Ê≠£ÔºåÂπ∂ (4)ÔºàÈáçÊñ∞ÔºâËØÑ‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩÂíåÂØπ‰º™ÂÉèÁöÑÂâ©‰ΩôÊïèÊÑüÊÄß„ÄÇ‰ΩøÁî®‰∏§‰∏™Áî®‰∫éÈªëËâ≤Á¥†Áò§Ê£ÄÊµãÂíåÈ™®ÈæÑ‰º∞ËÆ°ÁöÑÂåªÂ≠¶Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨Â∞ÜÊàë‰ª¨ÁöÑ R2R Ê°ÜÊû∂Â∫îÁî®‰∫é VGG„ÄÅResNet Âíå EfficientNet Êû∂ÊûÑÔºå‰ªéËÄåÊè≠Á§∫ÂíåÁ∫†Ê≠£‰∫ÜÁúüÂÆûÊï∞ÊçÆÈõÜÂõ∫ÊúâÁöÑ‰º™ÂÉèÔºå‰ª•ÂèäÂèóÊéßËÆæÁΩÆ‰∏≠ÁöÑÂêàÊàêÂèò‰Ωì„ÄÇÂÆåÊàê XAI ÁîüÂëΩÂë®ÊúüÔºåÊàë‰ª¨ÊºîÁ§∫‰∫ÜÂ§ö‰∏™ R2R Ëø≠‰ª£‰ª•ÂáèËΩª‰∏çÂêåÁöÑÂÅèÂ∑Æ„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/maxdreyer/Reveal2Revise ‰∏äÊâæÂà∞„ÄÇ

##### **Explainable AI for Time Series via Virtual Inspection Layers**
2303.06365v1 by Johanna Vielhaben, Sebastian Lapuschkin, Gr√©goire Montavon, Wojciech Samek

The field of eXplainable Artificial Intelligence (XAI) has greatly advanced
in recent years, but progress has mainly been made in computer vision and
natural language processing. For time series, where the input is often not
interpretable, only limited research on XAI is available. In this work, we put
forward a virtual inspection layer, that transforms the time series to an
interpretable representation and allows to propagate relevance attributions to
this representation via local XAI methods like layer-wise relevance propagation
(LRP). In this way, we extend the applicability of a family of XAI methods to
domains (e.g. speech) where the input is only interpretable after a
transformation. Here, we focus on the Fourier transformation which is
prominently applied in the interpretation of time series and LRP and refer to
our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time
series classification settings like audio and electronic health records. We
showcase how DFT-LRP reveals differences in the classification strategies of
models trained in different domains (e.g., time vs. frequency domain) or helps
to discover how models act on spurious correlations in the data.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) È†òÂüüÂú®ËøëÂπ¥‰æÜÂèñÂæóÈï∑Ë∂≥ÈÄ≤Ê≠•Ôºå‰ΩÜÈÄ≤Â±ï‰∏ªË¶ÅÊòØÂú®ÈõªËÖ¶Ë¶ñË¶∫ÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊñπÈù¢„ÄÇÂ∞çÊñºËº∏ÂÖ•ÈÄöÂ∏∏ÁÑ°Ê≥ïËß£ÈáãÁöÑÊôÇÈñìÂ∫èÂàóÔºåÂè™ÊúâÊúâÈôêÁöÑÁ†îÁ©∂ÂèØ‰æõ‰ΩøÁî® XAI„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËôõÊì¨Ê™¢Êü•Â±§ÔºåÂÆÉÂ∞áÊôÇÈñìÂ∫èÂàóËΩâÊèõÁÇ∫ÂèØËß£ÈáãÁöÑË°®Á§∫Ôºå‰∏¶ÂÖÅË®±ÈÄöÈÅéÂ±§Á¥öÁõ∏ÈóúÊÄßÂÇ≥Êí≠ (LRP) Á≠âÂ±ÄÈÉ® XAI ÊñπÊ≥ïÂ∞áÁõ∏ÈóúÊÄßÊ≠∏Âõ†ÂÇ≥Êí≠Âà∞Ê≠§Ë°®Á§∫„ÄÇËóâÊ≠§ÔºåÊàëÂÄëÂ∞á‰∏ÄÁ≥ªÂàó XAI ÊñπÊ≥ïÁöÑÈÅ©Áî®ÊÄßÊì¥Â±ïÂà∞Ëº∏ÂÖ•ÂÉÖÂú®ËΩâÊèõÂæåÊâçËÉΩËß£ÈáãÁöÑÈ†òÂüüÔºà‰æãÂ¶ÇË™ûÈü≥Ôºâ„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂÇÖÁ´ãËëâËΩâÊèõÔºåÂÆÉ‰∏ªË¶ÅÊáâÁî®ÊñºÊôÇÈñìÂ∫èÂàóÂíå LRP ÁöÑËß£ÈáãÔºå‰∏¶Â∞áÊàëÂÄëÁöÑÁ®±‰πãÁÇ∫ DFT-LRP„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü DFT-LRP Âú®ÂêÑÁ®ÆÊôÇÈñìÂ∫èÂàóÂàÜÈ°ûË®≠ÂÆöÔºà‰æãÂ¶ÇÈü≥Ë®äÂíåÈõªÂ≠êÂÅ•Â∫∑Á¥ÄÈåÑÔºâ‰∏≠ÁöÑÊïàÁî®„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü DFT-LRP Â¶Ç‰ΩïÊè≠Á§∫Âú®‰∏çÂêåÈ†òÂüüÔºà‰æãÂ¶ÇÊôÇÈñìËàáÈ†ªÁéáÂüüÔºâË®ìÁ∑¥ÁöÑÊ®°ÂûãÁöÑÂàÜÈ°ûÁ≠ñÁï•Â∑ÆÁï∞ÔºåÊàñÊúâÂä©ÊñºÁôºÁèæÊ®°ÂûãÂ¶Ç‰ΩïËôïÁêÜË≥áÊñô‰∏≠ÁöÑËôõÂÅáÈóúËÅØ„ÄÇ

##### **Towards Trust of Explainable AI in Thyroid Nodule Diagnosis**
2303.04731v1 by Truong Thanh Hung Nguyen, Van Binh Truong, Vo Thanh Khang Nguyen, Quoc Hung Cao, Quoc Khanh Nguyen

The ability to explain the prediction of deep learning models to end-users is
an important feature to leverage the power of artificial intelligence (AI) for
the medical decision-making process, which is usually considered
non-transparent and challenging to comprehend. In this paper, we apply
state-of-the-art eXplainable artificial intelligence (XAI) methods to explain
the prediction of the black-box AI models in the thyroid nodule diagnosis
application. We propose new statistic-based XAI methods, namely Kernel Density
Estimation and Density map, to explain the case of no nodule detected. XAI
methods' performances are considered under a qualitative and quantitative
comparison as feedback to improve the data quality and the model performance.
Finally, we survey to assess doctors' and patients' trust in XAI explanations
of the model's decisions on thyroid nodule images.

ÊëòË¶ÅÔºöËß£ÈáãÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÈ†êÊ∏¨ÁµêÊûúÁöÑËÉΩÂäõÂ∞çÊúÄÁµÇ‰ΩøÁî®ËÄÖËÄåË®ÄÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÂäüËÉΩÔºåÂèØÂà©Áî®‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÂäõÈáèÈÄ≤Ë°åÈÜ´ÁôÇÊ±∫Á≠ñÊµÅÁ®ãÔºåÈÄôÈÄöÂ∏∏Ë¢´Ë™çÁÇ∫ÊòØ‰∏çÈÄèÊòé‰∏îÈõ£‰ª•ÁêÜËß£ÁöÑ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÅãÁî®ÊúÄÂÖàÈÄ≤ÁöÑÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ÊñπÊ≥ï‰æÜËß£ÈáãÈªëÁõí AI Ê®°ÂûãÂú®Áî≤ÁãÄËÖ∫ÁµêÁØÄË®∫Êñ∑ÊáâÁî®‰∏≠ÁöÑÈ†êÊ∏¨ÁµêÊûú„ÄÇÊàëÂÄëÊèêÂá∫Êñ∞ÁöÑÂü∫ÊñºÁµ±Ë®àÁöÑ XAI ÊñπÊ≥ïÔºåÂç≥Ê†∏ÂØÜÂ∫¶‰º∞Ë®àÂíåÂØÜÂ∫¶ÂúñÔºå‰æÜËß£ÈáãÊú™Ê™¢Ê∏¨Âà∞ÁµêÁØÄÁöÑÊÉÖÊ≥Å„ÄÇXAI ÊñπÊ≥ïÁöÑÊïàËÉΩÊúÉÂú®ÂÆöÊÄßÂíåÂÆöÈáèÊØîËºÉ‰∏ãË¢´Ë¶ñÁÇ∫ÊîπÂñÑË≥áÊñôÂìÅË≥™ÂíåÊ®°ÂûãÊïàËÉΩÁöÑÂõûÈ•ã„ÄÇÊúÄÂæåÔºåÊàëÂÄëÈÄ≤Ë°åË™øÊü•‰ª•Ë©ï‰º∞ÈÜ´Â∏´ÂíåÊÇ£ËÄÖÂ∞ç XAI Â∞çÊ®°ÂûãÂú®Áî≤ÁãÄËÖ∫ÁµêÁØÄÂΩ±ÂÉè‰∏≠Ê±∫Á≠ñÁöÑËß£ÈáãÁöÑ‰ø°‰ªªÂ∫¶„ÄÇ

##### **Cybersecurity of AI medical devices: risks, legislation, and challenges**
2303.03140v1 by Elisabetta Biasin, Erik Kamenjasevic, Kaspar Rosager Ludvigsen

Medical devices and artificial intelligence systems rapidly transform
healthcare provisions. At the same time, due to their nature, AI in or as
medical devices might get exposed to cyberattacks, leading to patient safety
and security risks. This book chapter is divided into three parts. The first
part starts by setting the scene where we explain the role of cybersecurity in
healthcare. Then, we briefly define what we refer to when we talk about AI that
is considered a medical device by itself or supports one. To illustrate the
risks such medical devices pose, we provide three examples: the poisoning of
datasets, social engineering, and data or source code extraction. In the second
part, the paper provides an overview of the European Union's regulatory
framework relevant for ensuring the cybersecurity of AI as or in medical
devices (MDR, NIS Directive, Cybersecurity Act, GDPR, the AI Act proposal and
the NIS 2 Directive proposal). Finally, the third part of the paper examines
possible challenges stemming from the EU regulatory framework. In particular,
we look toward the challenges deriving from the two legislative proposals and
their interaction with the existing legislation concerning AI medical devices'
cybersecurity. They are structured as answers to the following questions: (1)
how will the AI Act interact with the MDR regarding the cybersecurity and
safety requirements?; (2) how should we interpret incident notification
requirements from the NIS 2 Directive proposal and MDR?; and (3) what are the
consequences of the evolving term of critical infrastructures?
  [This is a draft chapter. The final version will be available in Research
Handbook on Health, AI and the Law edited by Barry Solaiman & I. Glenn Cohen,
forthcoming 2023, Edward Elgar Publishing Ltd]

ÊëòË¶ÅÔºöÈÜ´ÁôÇË®≠ÂÇôÂíå‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±Âø´ÈÄüËΩâÂåñÈÜ´ÁôÇ‰øùÂÅ•ÁöÑÊèê‰æõÊñπÂºè„ÄÇÂêåÊôÇÔºåÁî±ÊñºÂÖ∂Êú¨Ë≥™ÔºåÈÜ´ÁôÇË®≠ÂÇô‰∏≠Êàñ‰ΩúÁÇ∫ÈÜ´ÁôÇË®≠ÂÇôÁöÑ‰∫∫Â∑•Êô∫ÊÖßÂèØËÉΩÊúÉÈÅ≠ÂèóÁ∂≤Ë∑ØÊîªÊìäÔºåÈÄ≤ËÄåÂ∞éËá¥ÊÇ£ËÄÖÂÆâÂÖ®ÂíåÂÆâÂÖ®È¢®Èö™„ÄÇÊú¨Á´†ÁØÄÂàÜÁÇ∫‰∏âÈÉ®ÂàÜ„ÄÇÁ¨¨‰∏ÄÈÉ®ÂàÜÂæûË®≠ÂÆöÂ†¥ÊôØÈñãÂßãÔºåË™™ÊòéÁ∂≤Ë∑ØÂÆâÂÖ®Âú®ÈÜ´ÁôÇ‰øùÂÅ•‰∏≠ÁöÑËßíËâ≤„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÁ∞°Ë¶ÅÂÆöÁæ©ÊàëÂÄëÂú®Ë´áË´ñË¢´Ë¶ñÁÇ∫ÈÜ´ÁôÇË®≠ÂÇôÊú¨Ë∫´ÊàñÊîØÊè¥ÈÜ´ÁôÇË®≠ÂÇôÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊôÇÊâÄÊåáÊ∂âÁöÑÂÖßÂÆπ„ÄÇÁÇ∫‰∫ÜË™™ÊòéÊ≠§È°ûÈÜ´ÁôÇË®≠ÂÇôÂ∏∂‰æÜÁöÑÈ¢®Èö™ÔºåÊàëÂÄëÊèê‰æõ‰∫Ü‰∏âÂÄãÁØÑ‰æãÔºöË≥áÊñôÈõÜ‰∏≠ÊØí„ÄÅÁ§æÊúÉÂ∑•Á®ãÂíåË≥áÊñôÊàñÂéüÂßãÁ¢ºËêÉÂèñ„ÄÇÂú®Á¨¨‰∫åÈÉ®ÂàÜÔºåÊú¨ÊñáÊ¶ÇËø∞‰∫ÜÊ≠êÁõüÁöÑÁõ£ÁÆ°Êû∂ÊßãÔºåËàáÁ¢∫‰øùÈÜ´ÁôÇË®≠ÂÇô‰∏≠Êàñ‰ΩúÁÇ∫ÈÜ´ÁôÇË®≠ÂÇôÁöÑ‰∫∫Â∑•Êô∫ÊÖßÁöÑÁ∂≤Ë∑ØÂÆâÂÖ®Áõ∏ÈóúÔºàÈÜ´ÁôÇÂô®ÊùêÊ≥ïË¶è„ÄÅÁ∂≤Ë∑ØËàáË≥áË®äÂÆâÂÖ®Êåá‰ª§„ÄÅÁ∂≤Ë∑ØÂÆâÂÖ®Ê≥ï„ÄÅ‰∏ÄËà¨Ë≥áÊñô‰øùË≠∑Ë¶èÁØÑ„ÄÅ‰∫∫Â∑•Êô∫ÊÖßÊ≥ïÊèêÊ°àÂíåÁ∂≤Ë∑ØËàáË≥áË®äÂÆâÂÖ® 2 Êåá‰ª§ÊèêÊ°àÔºâ„ÄÇÊúÄÂæåÔºåÊú¨ÊñáÁöÑÁ¨¨‰∏âÈÉ®ÂàÜÊé¢Ë®éÊ∫êËá™Ê≠êÁõüÁõ£ÁÆ°Êû∂ÊßãÁöÑÊΩõÂú®ÊåëÊà∞„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÂ±ïÊúõÊ∫êËá™ÈÄôÂÖ©È†ÖÁ´ãÊ≥ïÊèêÊ°àÁöÑÊåëÊà∞Ôºå‰ª•ÂèäÂÆÉÂÄëËàáÁèæÊúâÈóúÊñº‰∫∫Â∑•Êô∫ÊÖßÈÜ´ÁôÇË®≠ÂÇôÁ∂≤Ë∑ØÂÆâÂÖ®ÁöÑÁ´ãÊ≥ï‰πãÈñìÁöÑ‰∫íÂãï„ÄÇÂÆÉÂÄëË¢´Êû∂ÊßãÁÇ∫‰ª•‰∏ãÂïèÈ°åÁöÑËß£Á≠îÔºö(1) ‰∫∫Â∑•Êô∫ÊÖßÊ≥ïÂ∞áÂ¶Ç‰ΩïËàáÈÜ´ÁôÇÂô®ÊùêÊ≥ïË¶è‰∫íÂãïÔºåÂ∞±Á∂≤Ë∑ØÂÆâÂÖ®ÂíåÂÆâÂÖ®Ë¶ÅÊ±ÇËÄåË®ÄÔºü(2) ÊàëÂÄëÊáâÂ¶Ç‰ΩïËß£ËÆÄÁ∂≤Ë∑ØËàáË≥áË®äÂÆâÂÖ® 2 Êåá‰ª§ÊèêÊ°àÂíåÈÜ´ÁôÇÂô®ÊùêÊ≥ïË¶èÁöÑ‰∫ã‰ª∂ÈÄöÁü•Ë¶ÅÊ±ÇÔºü(3) ÈóúÈçµÂü∫Á§éË®≠ÊñΩÊºîÈÄ≤ÁöÑË°ìË™ûÊúÉÂ∏∂‰æÜ‰ªÄÈ∫ºÂæåÊûúÔºü
[ÈÄôÊòØËçâÁ®øÁ´†ÁØÄ„ÄÇÊúÄÁµÇÁâàÊú¨Â∞áÂàäËºâÊñº Barry Solaiman Âíå I. Glenn Cohen Á∑®ËºØÁöÑ„ÄäÂÅ•Â∫∑„ÄÅ‰∫∫Â∑•Êô∫ÊÖßËàáÊ≥ïÂæãÁ†îÁ©∂ÊâãÂÜä„Äã‰∏≠Ôºå2023 Âπ¥Âá∫ÁâàÔºåEdward Elgar Publishing Ltd]

##### **LAVA: Granular Neuron-Level Explainable AI for Alzheimer's Disease Assessment from Fundus Images**
2302.03008v2 by Nooshin Yousefzadeh, Charlie Tran, Adolfo Ramirez-Zamora, Jinghua Chen, Ruogu Fang, My T. Thai

Alzheimer's Disease (AD) is a progressive neurodegenerative disease and the
leading cause of dementia. Early diagnosis is critical for patients to benefit
from potential intervention and treatment. The retina has been hypothesized as
a diagnostic site for AD detection owing to its anatomical connection with the
brain. Developed AI models for this purpose have yet to provide a rational
explanation about the decision and neither infer the stage of disease's
progression. Along this direction, we propose a novel model-agnostic
explainable-AI framework, called Granular Neuron-level Explainer (LAVA), an
interpretation prototype that probes into intermediate layers of the
Convolutional Neural Network (CNN) models to assess the AD continuum directly
from the retinal imaging without longitudinal or clinical evaluation. This
method is applied to validate the retinal vasculature as a biomarker and
diagnostic modality for Alzheimer's Disease (AD) evaluation. UK Biobank
cognitive tests and vascular morphological features suggest LAVA shows strong
promise and effectiveness in identifying AD stages across the progression
continuum.

ÊëòË¶ÅÔºöÈòøËå≤Êµ∑ÈªòÁóá (AD) ÊòØ‰∏ÄÁ®ÆÈÄ≤Ë°åÊÄßÁöÑÁ•ûÁ∂ìÈÄÄÂåñÊÄßÁñæÁóÖÔºå‰πüÊòØÂ∞éËá¥Â§±Êô∫ÁóáÁöÑ‰∏ªÂõ†„ÄÇÊó©ÊúüË®∫Êñ∑Â∞çÊñºÊÇ£ËÄÖÊé•ÂèóÊΩõÂú®Âπ≤È†êÂíåÊ≤ªÁôÇËá≥ÈóúÈáçË¶Å„ÄÇÁî±ÊñºË¶ñÁ∂≤ËÜúËàáÂ§ßËÖ¶ÊúâËß£ÂâñÂ≠∏‰∏äÁöÑÈÄ£ÁµêÔºåÂõ†Ê≠§ÂÅáË®≠Ë¶ñÁ∂≤ËÜúÂèØ‰ª•‰ΩúÁÇ∫ AD Ê™¢Ê∏¨ÁöÑË®∫Êñ∑ÈÉ®‰Ωç„ÄÇÁÇ∫Ê≠§ÁõÆÁöÑËÄåÈñãÁôºÁöÑ AI Ê®°ÂûãÔºåÂ∞öÊú™Â∞çÊ±∫Á≠ñÊèê‰æõÂêàÁêÜÁöÑËß£ÈáãÔºå‰πüÁÑ°Ê≥ïÊé®Ë´ñÁñæÁóÖÈÄ≤Â±ïÁöÑÈöéÊÆµ„ÄÇÊ≤øËëóÈÄôÂÄãÊñπÂêëÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ®°Âûã‰∏çÂèØÁü•Ë´ñÂèØËß£Èáã AI Êû∂ÊßãÔºåÁ®±ÁÇ∫È°ÜÁ≤íÁ•ûÁ∂ìÂÖÉÁ¥öÂà•Ëß£ÈáãÂô® (LAVA)ÔºåÈÄôÊòØ‰∏ÄÂÄãËß£ÈáãÂéüÂûãÔºåÂèØ‰ª•Êé¢Ê∏¨Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) Ê®°ÂûãÁöÑ‰∏≠ÈñìÂ±§Ôºå‰ª•Áõ¥Êé•ÂæûË¶ñÁ∂≤ËÜúÂΩ±ÂÉèË©ï‰º∞ AD ÈÄ£Á∫åÈ´îÔºåËÄåÁÑ°ÈúÄÁ∏±ÂêëÊàñËá®Â∫äË©ï‰º∞„ÄÇÊ≠§ÊñπÊ≥ïÁî®ÊñºÈ©óË≠âË¶ñÁ∂≤ËÜúË°ÄÁÆ°‰ΩúÁÇ∫ÁîüÁâ©Ê®ôË®òÂíåÈòøËå≤Êµ∑ÈªòÁóá (AD) Ë©ï‰º∞ÁöÑË®∫Êñ∑ÊñπÂºè„ÄÇËã±ÂúãÁîüÁâ©Ë≥áÊñôÂ∫´ÁöÑË™çÁü•Ê∏¨Ë©¶ÂíåË°ÄÁÆ°ÂΩ¢ÊÖãÁâπÂæµË°®ÊòéÔºåLAVA Âú®Ë≠òÂà•ÈÄ≤Â±ïÈÄ£Á∫åÈ´î‰∏≠ÁöÑ AD ÈöéÊÆµÊñπÈù¢È°ØÁ§∫Âá∫Âº∑Â§ßÁöÑÂâçÊôØÂíåÊúâÊïàÊÄß„ÄÇ

##### **Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses**
2302.01241v2 by Brian Y. Lim, Joseph P. Cahaly, Chester Y. F. Sng, Adam Chew

Many visualizations have been developed for explainable AI (XAI), but they
often require further reasoning by users to interpret. We argue that XAI should
support diagrammatic and abductive reasoning for the AI to perform hypothesis
generation and evaluation to reduce the interpretability gap. We propose
Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii)
follow domain conventions, and iii) explain with diagrams visually or verbally.
We implemented DiagramNet for a clinical application to predict cardiac
diagnoses from heart auscultation, and explain with shape-based murmur
diagrams. In modeling studies, we found that DiagramNet not only provides
faithful murmur shape explanations, but also has better prediction performance
than baseline models. We further demonstrate the interpretability and
trustworthiness of diagrammatic explanations in a qualitative user study with
medical students, showing that clinically-relevant, diagrammatic explanations
are preferred over technical saliency map explanations. This work contributes
insights into providing domain-conventional abductive explanations for
user-centric XAI.

ÊëòË¶ÅÔºöË®±Â§öË¶ñË¶∫ÂåñÂ∑≤Ë¢´ÈñãÁôºÁî®ÊñºÂèØËß£ÈáãÁöÑ AI (XAI)Ôºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏ÈúÄË¶Å‰ΩøÁî®ËÄÖÈÄ≤‰∏ÄÊ≠•Êé®ÁêÜÊâçËÉΩËß£ËÆÄ„ÄÇÊàëÂÄë‰∏ªÂºµ XAI ÊáâÊîØÊè¥ÂúñËß£ÂíåÊºîÁππÊé®ÁêÜÔºåËÆì AI Âü∑Ë°åÂÅáË®≠Áî¢ÁîüÂíåË©ï‰º∞‰ª•Á∏ÆÂ∞èÂèØËß£ÈáãÊÄßÂ∑ÆË∑ù„ÄÇÊàëÂÄëÊèêÂá∫ÂúñËß£Âåñ‰ª• i) Âü∑Ë°åÁöÆÁàæÂ£´ÊºîÁππ-ÊºîÁππÊé®ÁêÜÔºåii) ÈÅµÂæ™È†òÂüüÊÖ£‰æãÔºå‰ª•Âèä iii) ‰ª•Ë¶ñË¶∫ÊàñÂè£Ë™ûÊñπÂºèË™™ÊòéÂúñË°®„ÄÇÊàëÂÄëÂØ¶‰Ωú‰∫Ü DiagramNet ÈÄ≤Ë°åËá®Â∫äÊáâÁî®Ôºå‰ª•ÂæûÂøÉËáüËÅΩË®∫È†êÊ∏¨ÂøÉËáüË®∫Êñ∑Ôºå‰∏¶‰ª•ÂΩ¢ÁãÄÁÇ∫Âü∫Á§éÁöÑÈõúÈü≥ÂúñË™™Êòé„ÄÇÂú®Âª∫Ê®°Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÁôºÁèæ DiagramNet ‰∏çÂÉÖÊèê‰æõ‰∫ÜÂø†ÂØ¶ÁöÑÈõúÈü≥ÂΩ¢ÁãÄË™™ÊòéÔºåËÄå‰∏îÊØîÂü∫Ê∫ñÊ®°ÂûãÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÈ†êÊ∏¨ÊïàËÉΩ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Âú®ËàáÈÜ´Â≠∏ÁîüÁöÑË≥™ÊÄß‰ΩøÁî®ËÄÖÁ†îÁ©∂‰∏≠Â±ïÁ§∫‰∫ÜÂúñËß£Ë™™ÊòéÁöÑÂèØËß£ÈáãÊÄßÂíåÂèØ‰ø°Â∫¶ÔºåÈ°ØÁ§∫Âá∫‰ª•Ëá®Â∫äÁõ∏ÈóúÁöÑÂúñËß£Ë™™ÊòéÂÑ™ÊñºÊäÄË°ìÈ°ØËëóÊÄßÂúñË™™Êòé„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊúâÂä©ÊñºÊèê‰æõ‰ª•‰ΩøÁî®ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑ XAI ÁöÑÈ†òÂüüÊÖ£‰æãÊºîÁππË™™Êòé„ÄÇ

##### **LesionAid: Vision Transformers-based Skin Lesion Generation and Classification**
2302.01104v1 by Ghanta Sai Krishna, Kundrapu Supriya, Mallikharjuna Rao K, Meetiksha Sorgile

Skin cancer is one of the most prevalent forms of human cancer. It is
recognized mainly visually, beginning with clinical screening and continuing
with the dermoscopic examination, histological assessment, and specimen
collection. Deep convolutional neural networks (CNNs) perform highly segregated
and potentially universal tasks against a classified finegrained object. This
research proposes a novel multi-class prediction framework that classifies skin
lesions based on ViT and ViTGAN. Vision transformers-based GANs (Generative
Adversarial Networks) are utilized to tackle the class imbalance. The framework
consists of four main phases: ViTGANs, Image processing, and explainable AI.
Phase 1 consists of generating synthetic images to balance all the classes in
the dataset. Phase 2 consists of applying different data augmentation
techniques and morphological operations to increase the size of the data.
Phases 3 & 4 involve developing a ViT model for edge computing systems that can
identify patterns and categorize skin lesions from the user's skin visible in
the image. In phase 3, after classifying the lesions into the desired class
with ViT, we will use explainable AI (XAI) that leads to more explainable
results (using activation maps, etc.) while ensuring high predictive accuracy.
Real-time images of skin diseases can capture by a doctor or a patient using
the camera of a mobile application to perform an early examination and
determine the cause of the skin lesion. The whole framework is compared with
the existing frameworks for skin lesion detection.

ÊëòË¶ÅÔºöÁöÆËÜöÁôåÊòØ‰∫∫È°ûÊúÄÊôÆÈÅçÁöÑÁôåÁóáÈ°ûÂûã‰πã‰∏Ä„ÄÇÂÆÉÁöÑË≠òÂà•‰∏ªË¶Å‰æùË≥¥Ë¶ñË¶∫ÔºåÂæûËá®Â∫äÁØ©Ê™¢ÈñãÂßãÔºåÊé•ËëóÊòØÁöÆËÜöÈè°Ê™¢Êü•„ÄÅÁµÑÁπîÂ≠∏Ë©ï‰º∞Ôºå‰ª•ÂèäÊ™¢È´îÊî∂ÈõÜ„ÄÇÊ∑±Â∫¶Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) ÂèØÈáùÂ∞çÂàÜÈ°ûÁöÑÁ¥∞Á≤íÂ∫¶Áâ©‰ª∂Âü∑Ë°åÈ´òÂ∫¶ÂçÄÈöî‰∏îÊΩõÂú®ÈÄöÁî®ÁöÑ‰ªªÂãô„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ§öÈ°ûÂà•È†êÊ∏¨Êû∂ÊßãÔºåÂÆÉ‰ª• ViT Âíå ViTGAN ÁÇ∫Âü∫Á§éÂ∞çÁöÆËÜöÁóÖÁÅ∂ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÂü∫ÊñºË¶ñË¶∫ËΩâÊèõÂô®ÁöÑ GANÔºàÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑ØÔºâÁî®ÊñºËß£Ê±∫È°ûÂà•‰∏çÂπ≥Ë°°ÂïèÈ°å„ÄÇÊ≠§Êû∂ÊßãÂåÖÂê´ÂõõÂÄã‰∏ªË¶ÅÈöéÊÆµÔºöViTGAN„ÄÅÂΩ±ÂÉèËôïÁêÜÂíåÂèØËß£Èáã AI„ÄÇÁ¨¨‰∏ÄÈöéÊÆµÂåÖÊã¨Áî¢ÁîüÂêàÊàêÂΩ±ÂÉèÔºå‰ª•Âπ≥Ë°°Ë≥áÊñôÈõÜ‰∏≠ÁöÑÊâÄÊúâÈ°ûÂà•„ÄÇÁ¨¨‰∫åÈöéÊÆµÂåÖÊã¨ÊáâÁî®‰∏çÂêåÁöÑË≥áÊñôÊì¥ÂÖÖÊäÄË°ìÂíåÂΩ¢ÊÖãÈÅãÁÆóÔºå‰ª•Â¢ûÂä†Ë≥áÊñôÂ§ßÂ∞è„ÄÇÁ¨¨‰∏âÂíåÁ¨¨ÂõõÈöéÊÆµÊ∂âÂèäÈñãÁôºÈÅ©Áî®ÊñºÈÇäÁ∑£ÈÅãÁÆóÁ≥ªÁµ±ÁöÑ ViT Ê®°ÂûãÔºåË©≤Ê®°ÂûãÂèØ‰ª•Ë≠òÂà•ÂúñÊ°àÔºå‰∏¶Â∞çÂΩ±ÂÉè‰∏≠Áî®Êà∂ÁöÆËÜöÂèØË¶ãÁöÑÁöÆËÜöÁóÖÁÅ∂ÈÄ≤Ë°åÂàÜÈ°û„ÄÇÂú®Á¨¨‰∏âÈöéÊÆµÔºåÂú®‰ΩøÁî® ViT Â∞áÁóÖÁÅ∂ÂàÜÈ°ûÂà∞ÊâÄÈúÄÁöÑÈ°ûÂà•ÂæåÔºåÊàëÂÄëÂ∞á‰ΩøÁî®ÂèØËß£Èáã AI (XAI)ÔºåÂÆÉÊúÉÁî¢ÁîüÊõ¥ÂÖ∑ÂèØËß£ÈáãÊÄßÁöÑÁµêÊûúÔºà‰ΩøÁî®ÂïüÁî®ÂúñÁ≠âÔºâÔºåÂêåÊôÇÁ¢∫‰øùÈ´òÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇÁöÆËÜöÁñæÁóÖÁöÑÂç≥ÊôÇÂΩ±ÂÉèÂèØ‰ª•Áî®Ë°åÂãïÊáâÁî®Á®ãÂºèÁöÑÁõ∏Ê©üÁî±ÈÜ´ÁîüÊàñÊÇ£ËÄÖÊì∑ÂèñÔºå‰ª•Âü∑Ë°åÊó©ÊúüÊ™¢Êü•‰∏¶Á¢∫ÂÆöÁöÆËÜöÁóÖÁÅ∂ÁöÑÂéüÂõ†„ÄÇÊï¥ÂÄãÊû∂ÊßãËàáÁèæÊúâÁöÑÁöÆËÜöÁóÖÁÅ∂ÂÅµÊ∏¨Êû∂ÊßãÈÄ≤Ë°åÊØîËºÉ„ÄÇ

##### **SkinCon: A skin disease dataset densely annotated by domain experts for fine-grained model debugging and analysis**
2302.00785v1 by Roxana Daneshjou, Mert Yuksekgonul, Zhuo Ran Cai, Roberto Novoa, James Zou

For the deployment of artificial intelligence (AI) in high-risk settings,
such as healthcare, methods that provide interpretability/explainability or
allow fine-grained error analysis are critical. Many recent methods for
interpretability/explainability and fine-grained error analysis use concepts,
which are meta-labels that are semantically meaningful to humans. However,
there are only a few datasets that include concept-level meta-labels and most
of these meta-labels are relevant for natural images that do not require domain
expertise. Densely annotated datasets in medicine focused on meta-labels that
are relevant to a single disease such as melanoma. In dermatology, skin disease
is described using an established clinical lexicon that allows clinicians to
describe physical exam findings to one another. To provide a medical dataset
densely annotated by domain experts with annotations useful across multiple
disease processes, we developed SkinCon: a skin disease dataset densely
annotated by dermatologists. SkinCon includes 3230 images from the Fitzpatrick
17k dataset densely annotated with 48 clinical concepts, 22 of which have at
least 50 images representing the concept. The concepts used were chosen by two
dermatologists considering the clinical descriptor terms used to describe skin
lesions. Examples include "plaque", "scale", and "erosion". The same concepts
were also used to label 656 skin disease images from the Diverse Dermatology
Images dataset, providing an additional external dataset with diverse skin tone
representations. We review the potential applications for the SkinCon dataset,
such as probing models, concept-based explanations, and concept bottlenecks.
Furthermore, we use SkinCon to demonstrate two of these use cases: debugging
mistakes of an existing dermatology AI model with concepts and developing
interpretable models with post-hoc concept bottleneck models.

ÊëòË¶ÅÔºö<paragraph>Âú®È´òÈ¢®Èö™Áí∞Â¢É‰∏≠ÈÉ®ÁΩ≤‰∫∫Â∑•Êô∫ÊÖß (AI)Ôºà‰æãÂ¶ÇÈÜ´ÁôÇ‰øùÂÅ•ÔºâÔºåÊèê‰æõÂèØËß£ÈáãÊÄß/ÂèØË™™ÊòéÊÄßÁöÑÊñπÊ≥ïÊàñÂÖÅË®±Á≤æÁ¥∞ÈåØË™§ÂàÜÊûêÈùûÂ∏∏ÈáçË¶Å„ÄÇË®±Â§öËøëÊúüÁî®ÊñºÂèØËß£ÈáãÊÄß/ÂèØË™™ÊòéÊÄßÂíåÁ≤æÁ¥∞ÈåØË™§ÂàÜÊûêÁöÑÊñπÊ≥ïÈÉΩ‰ΩøÁî®Ê¶ÇÂøµÔºåÈÄô‰∫õÊ¶ÇÂøµÊòØÂ∞ç‰∫∫È°ûÂÖ∑ÊúâË™ûÁæ©ÊÑèÁæ©ÁöÑÂÖÉÊ®ôÁ±§„ÄÇÁÑ∂ËÄåÔºåÂè™ÊúâÂ∞ëÊï∏Ë≥áÊñôÈõÜÂåÖÂê´Ê¶ÇÂøµÂ±§Á¥öÁöÑÂÖÉÊ®ôÁ±§ÔºåËÄå‰∏îÈÄô‰∫õÂÖÉÊ®ôÁ±§Â§ßÂ§öËàá‰∏çÈúÄË¶ÅÈ†òÂüüÂ∞àÊ•≠Áü•Ë≠òÁöÑËá™ÁÑ∂ÂΩ±ÂÉèÁõ∏Èóú„ÄÇÂ∞àÊ≥®ÊñºÂñÆ‰∏ÄÁñæÁóÖÔºà‰æãÂ¶ÇÈªëËâ≤Á¥†Áò§ÔºâÁöÑÂÖÉÊ®ôÁ±§ÁöÑÈÜ´Â≠∏ÂØÜÈõÜÊ®ôË®òË≥áÊñôÈõÜ„ÄÇÂú®ÁöÆËÜöÁßë‰∏≠ÔºåÁöÆËÜöÁñæÁóÖÁöÑÊèèËø∞‰ΩøÁî®Êó¢ÂÆöÁöÑËá®Â∫äË©ûÂΩôÔºåËÆìËá®Â∫äÈÜ´ÁîüÂèØ‰ª•ÂΩºÊ≠§ÊèèËø∞Ë∫´È´îÊ™¢Êü•ÁµêÊûú„ÄÇÁÇ∫‰∫ÜÊèê‰æõÁî±È†òÂüüÂ∞àÂÆ∂ÂØÜÈõÜÊ®ôË®òÁöÑÈÜ´Â≠∏Ë≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂèØË∑®Â§öÁ®ÆÁñæÁóÖÈÅéÁ®ã‰ΩøÁî®ÁöÑÊ®ôË®òÔºåÊàëÂÄëÈñãÁôº‰∫Ü SkinConÔºöÁî±ÁöÆËÜöÁßëÈÜ´Â∏´ÂØÜÈõÜÊ®ôË®òÁöÑÁöÆËÜöÁñæÁóÖË≥áÊñôÈõÜ„ÄÇSkinCon ÂåÖÂê´‰æÜËá™ Fitzpatrick 17k Ë≥áÊñôÈõÜÁöÑ 3230 ÂºµÂΩ±ÂÉèÔºåÂØÜÈõÜÊ®ôË®ò‰∫Ü 48 ÂÄãËá®Â∫äÊ¶ÇÂøµÔºåÂÖ∂‰∏≠ 22 ÂÄãÊ¶ÇÂøµËá≥Â∞ëÊúâ 50 ÂºµÂΩ±ÂÉè‰ª£Ë°®Ë©≤Ê¶ÇÂøµ„ÄÇÊâÄ‰ΩøÁî®ÁöÑÊ¶ÇÂøµÊòØÁî±ÂÖ©‰ΩçÁöÆËÜöÁßëÈÜ´Â∏´Âú®ËÄÉÈáèÁî®ÊñºÊèèËø∞ÁöÆËÜöÁóÖËÆäÁöÑËá®Â∫äÊèèËø∞Ë©ûÂΩôÂæåÈÅ∏Âá∫ÁöÑ„ÄÇÁØÑ‰æãÂåÖÊã¨„ÄåÊñëÂ°ä„Äç„ÄÅ„ÄåÈ±óÂ±ë„ÄçÂíå„ÄåÁ≥úÁàõ„Äç„ÄÇÁõ∏ÂêåÁöÑÊ¶ÇÂøµ‰πüÁî®ÊñºÊ®ôË®ò‰æÜËá™ Diverse Dermatology Images Ë≥áÊñôÈõÜÁöÑ 656 ÂºµÁöÆËÜöÁñæÁóÖÂΩ±ÂÉèÔºåÊèê‰æõÂÖ∑ÊúâÂ§öÊ®£ËÜöËâ≤Ë°®Á§∫ÁöÑÈ°çÂ§ñÂ§ñÈÉ®Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÊ™¢Ë¶ñ SkinCon Ë≥áÊñôÈõÜÁöÑÊΩõÂú®ÊáâÁî®Ôºå‰æãÂ¶ÇÊé¢Ê∏¨Ê®°Âûã„ÄÅÂü∫ÊñºÊ¶ÇÂøµÁöÑË™™ÊòéÂíåÊ¶ÇÂøµÁì∂È†∏„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰ΩøÁî® SkinCon ‰æÜÂ±ïÁ§∫ÈÄôÂÖ©ÂÄã‰ΩøÁî®Ê°à‰æãÔºö‰ΩøÁî®Ê¶ÇÂøµÈô§ÈåØÁèæÊúâÁöÆËÜöÁßë AI Ê®°ÂûãÁöÑÈåØË™§Ôºå‰ª•Âèä‰ΩøÁî®‰∫ãÂæåÊ¶ÇÂøµÁì∂È†∏Ê®°ÂûãÈñãÁôºÂèØËß£ÈáãÁöÑÊ®°Âûã„ÄÇ</paragraph>

##### **Decision-Focused Evaluation: Analyzing Performance of Deployed Restless Multi-Arm Bandits**
2301.07835v1 by Paritosh Verma, Shresth Verma, Aditya Mate, Aparna Taneja, Milind Tambe

Restless multi-arm bandits (RMABs) is a popular decision-theoretic framework
that has been used to model real-world sequential decision making problems in
public health, wildlife conservation, communication systems, and beyond.
Deployed RMAB systems typically operate in two stages: the first predicts the
unknown parameters defining the RMAB instance, and the second employs an
optimization algorithm to solve the constructed RMAB instance.
  In this work we provide and analyze the results from a first-of-its-kind
deployment of an RMAB system in public health domain, aimed at improving
maternal and child health. Our analysis is focused towards understanding the
relationship between prediction accuracy and overall performance of deployed
RMAB systems. This is crucial for determining the value of investing in
improving predictive accuracy towards improving the final system performance,
and is useful for diagnosing, monitoring deployed RMAB systems.
  Using real-world data from our deployed RMAB system, we demonstrate that an
improvement in overall prediction accuracy may even be accompanied by a
degradation in the performance of RMAB system -- a broad investment of
resources to improve overall prediction accuracy may not yield expected
results. Following this, we develop decision-focused evaluation metrics to
evaluate the predictive component and show that it is better at explaining
(both empirically and theoretically) the overall performance of a deployed RMAB
system.

ÊëòË¶ÅÔºö‰∏çÂÆâÂàÜÁöÑÂ§öËáÇÂº∑Áõú (RMAB) ÊòØ‰∏ÄÂÄãÊµÅË°åÁöÑÊ±∫Á≠ñÁêÜË´ñÊû∂ÊßãÔºåÂ∑≤Ë¢´Áî®ÊñºÊ®°Êì¨ÂÖ¨ÂÖ±Ë°õÁîü„ÄÅÈáéÁîüÂãïÁâ©‰øùËÇ≤„ÄÅÈÄöË®äÁ≥ªÁµ±Á≠âÈ†òÂüüÁöÑÁúüÂØ¶‰∏ñÁïåÈ†ÜÂ∫èÊ±∫Á≠ñÂïèÈ°å„ÄÇÂ∑≤ÈÉ®ÁΩ≤ÁöÑ RMAB Á≥ªÁµ±ÈÄöÂ∏∏ÂàÜÂÖ©ÂÄãÈöéÊÆµÈÅã‰ΩúÔºöÁ¨¨‰∏ÄÂÄãÈöéÊÆµÈ†êÊ∏¨ÂÆöÁæ© RMAB Âü∑Ë°åÂÄãÈ´îÁöÑÊú™Áü•ÂèÉÊï∏ÔºåÁ¨¨‰∫åÂÄãÈöéÊÆµÊé°Áî®ÊúÄ‰Ω≥ÂåñÊºîÁÆóÊ≥ï‰æÜËß£Ê±∫Â∑≤Âª∫ÊßãÁöÑ RMAB Âü∑Ë°åÂÄãÈ´î„ÄÇ
Âú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèê‰æõ‰∏¶ÂàÜÊûê‰∫ÜÂú®ÂÖ¨ÂÖ±Ë°õÁîüÈ†òÂüü‰∏≠È¶ñÊ¨°ÈÉ®ÁΩ≤ RMAB Á≥ªÁµ±ÁöÑÁµêÊûúÔºåÁõÆÊ®ôÊòØÊîπÂñÑÂ≠ïÁî¢Â©¶ÂíåÂÖíÁ´•ÂÅ•Â∫∑„ÄÇÊàëÂÄëÁöÑÂàÜÊûêËëóÈáçÊñº‰∫ÜËß£È†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ËàáÂ∑≤ÈÉ®ÁΩ≤ RMAB Á≥ªÁµ±ÁöÑÊï¥È´îÊïàËÉΩ‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÈÄôÂ∞çÊñºÊ±∫ÂÆöÊäïË≥áÊñºÊîπÂñÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶‰ª•ÊèêÂçáÊúÄÁµÇÁ≥ªÁµ±ÊïàËÉΩÁöÑÂÉπÂÄºËá≥ÈóúÈáçË¶ÅÔºå‰∏¶‰∏îÊúâÂä©ÊñºË®∫Êñ∑„ÄÅÁõ£ÊéßÂ∑≤ÈÉ®ÁΩ≤ÁöÑ RMAB Á≥ªÁµ±„ÄÇ
‰ΩøÁî®ÊàëÂÄëÂ∑≤ÈÉ®ÁΩ≤ RMAB Á≥ªÁµ±‰∏≠ÁöÑÁúüÂØ¶‰∏ñÁïåË≥áÊñôÔºåÊàëÂÄëË≠âÊòé‰∫ÜÊï¥È´îÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÁöÑÊèêÂçáÁîöËá≥ÂèØËÉΩ‰º¥Èö®Ëëó RMAB Á≥ªÁµ±ÊïàËÉΩÁöÑ‰∏ãÈôç‚Äî‚ÄîÂª£Ê≥õÊäïÂÖ•Ë≥áÊ∫ê‰ª•ÊîπÂñÑÊï¥È´îÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÂèØËÉΩÁÑ°Ê≥ïÁî¢ÁîüÈ†êÊúüÁöÑÁµêÊûú„ÄÇÂú®Ê≠§‰πãÂæåÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰ª•Ê±∫Á≠ñÁÇ∫ÈáçÈªûÁöÑË©ï‰º∞ÊåáÊ®ô‰æÜË©ï‰º∞È†êÊ∏¨ÂÖÉ‰ª∂Ôºå‰∏¶Ë≠âÊòéÂÆÉÊõ¥ËÉΩËß£ÈáãÂ∑≤ÈÉ®ÁΩ≤ RMAB Á≥ªÁµ±ÁöÑÊï¥È´îÊïàËÉΩÔºàÁÑ°Ë´ñÊòØÁ∂ìÈ©ó‰∏äÊàñÁêÜË´ñ‰∏äÔºâ„ÄÇ

##### **Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling**
2302.03033v1 by Carlo Metta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, Salvatore Rinzivillo

Explainable AI consists in developing mechanisms allowing for an interaction
between decision systems and humans by making the decisions of the formers
understandable. This is particularly important in sensitive contexts like in
the medical domain. We propose a use case study, for skin lesion diagnosis,
illustrating how it is possible to provide the practitioner with explanations
on the decisions of a state of the art deep neural network classifier trained
to characterize skin lesions from examples. Our framework consists of a trained
classifier onto which an explanation module operates. The latter is able to
offer the practitioner exemplars and counterexemplars for the classification
diagnosis thus allowing the physician to interact with the automatic diagnosis
system. The exemplars are generated via an adversarial autoencoder. We
illustrate the behavior of the system on representative examples.

ÊëòË¶ÅÔºöÂèØËß£Èáã AI ÊòØÂú®ÈñãÁôºÊ©üÂà∂ÔºåËÆìÊ±∫Á≠ñÁ≥ªÁµ±Ëàá‰∫∫È°û‰πãÈñìËÉΩ‰∫íÂãïÔºå‰∏¶ËÆìÂâçËÄÖÁöÑÊ±∫Á≠ñËÆäÂæóÂèØ‰ª•ÁêÜËß£„ÄÇÈÄôÂú®ÊïèÊÑüÁöÑËÑàÁµ°‰∏≠ÁâπÂà•ÈáçË¶ÅÔºå‰æãÂ¶ÇÂú®ÈÜ´ÁôÇÈ†òÂüü„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄã‰ΩøÁî®Ê°à‰æãÁ†îÁ©∂ÔºåÁî®ÊñºÁöÆËÜöÁóÖËÆäË®∫Êñ∑ÔºåË™™ÊòéÂ¶Ç‰ΩïËÆìÂü∑Ê•≠ÈÜ´Â∏´‰∫ÜËß£ÊúÄÂÖàÈÄ≤ÁöÑÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÂàÜÈ°ûÂô®Âú®Ê±∫Á≠ñ‰∏äÁöÑËß£ÈáãÔºåË©≤ÂàÜÈ°ûÂô®Á∂ìÈÅéË®ìÁ∑¥ÔºåÂèØ‰ª•ÂæûÁØÑ‰æã‰∏≠ÊèèËø∞ÁöÆËÜöÁóÖËÆä„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂåÖÂê´‰∏ÄÂÄãË®ìÁ∑¥ÈÅéÁöÑÂàÜÈ°ûÂô®ÔºåËß£ÈáãÊ®°ÁµÑÊúÉÂú®Ë©≤ÂàÜÈ°ûÂô®‰∏äÈÅã‰Ωú„ÄÇÂæåËÄÖËÉΩÂ§†ÁÇ∫ÂàÜÈ°ûË®∫Êñ∑Êèê‰æõÂü∑Ê•≠ÈÜ´Â∏´ÁØÑ‰æãÂíåÂèç‰æãÔºåÂõ†Ê≠§ËÆìÈÜ´Â∏´ÂèØ‰ª•ËàáËá™ÂãïË®∫Êñ∑Á≥ªÁµ±‰∫íÂãï„ÄÇÁØÑ‰æãÊòØÈÄèÈÅéÂ∞çÊäóÂºèËá™ÂãïÁ∑®Á¢ºÂô®Áî¢ÁîüÁöÑ„ÄÇÊàëÂÄëË™™ÊòéÁ≥ªÁµ±Âú®‰ª£Ë°®ÊÄßÁØÑ‰æã‰∏äÁöÑË°åÁÇ∫„ÄÇ

##### **Monotonicity for AI ethics and society: An empirical study of the monotonic neural additive model in criminology, education, health care, and finance**
2301.07060v1 by Dangxing Chen, Luyao Zhang

Algorithm fairness in the application of artificial intelligence (AI) is
essential for a better society. As the foundational axiom of social mechanisms,
fairness consists of multiple facets. Although the machine learning (ML)
community has focused on intersectionality as a matter of statistical parity,
especially in discrimination issues, an emerging body of literature addresses
another facet -- monotonicity. Based on domain expertise, monotonicity plays a
vital role in numerous fairness-related areas, where violations could misguide
human decisions and lead to disastrous consequences. In this paper, we first
systematically evaluate the significance of applying monotonic neural additive
models (MNAMs), which use a fairness-aware ML algorithm to enforce both
individual and pairwise monotonicity principles, for the fairness of AI ethics
and society. We have found, through a hybrid method of theoretical reasoning,
simulation, and extensive empirical analysis, that considering monotonicity
axioms is essential in all areas of fairness, including criminology, education,
health care, and finance. Our research contributes to the interdisciplinary
research at the interface of AI ethics, explainable AI (XAI), and
human-computer interactions (HCIs). By evidencing the catastrophic consequences
if monotonicity is not met, we address the significance of monotonicity
requirements in AI applications. Furthermore, we demonstrate that MNAMs are an
effective fairness-aware ML approach by imposing monotonicity restrictions
integrating human intelligence.

ÊëòË¶ÅÔºöÂú®‰∫∫Â∑•Êô∫ËÉΩ (AI) ÁöÑÊáâÁî®‰∏≠ÔºåÊºîÁÆóÊ≥ïÂÖ¨Âπ≥ÊÄßÂ∞çÊñºÂª∫Á´ã‰∏ÄÂÄãÊõ¥ÁæéÂ•ΩÁöÑÁ§æÊúÉËá≥ÈóúÈáçË¶Å„ÄÇÂÖ¨Âπ≥ÊÄß‰ΩúÁÇ∫Á§æÊúÉÊ©üÂà∂ÁöÑÂü∫Á§éÂÖ¨ÁêÜÔºåÂåÖÂê´Â§öÂÄãÈù¢Âêë„ÄÇÂÑòÁÆ°Ê©üÂô®Â≠∏Áøí (ML) Á§æÁæ§Â∑≤Â∞áÁÑ¶ÈªûÊîæÂú®‰∫§ÂèâÊÄß‰ΩúÁÇ∫Áµ±Ë®àÂêåË≥™ÊÄßÁöÑÂïèÈ°å‰∏äÔºåÁâπÂà•ÊòØÂú®Ê≠ßË¶ñÂïèÈ°å‰∏≠Ôºå‰ΩÜÊñ∞ËààÁöÑÊñáÁçªÊé¢Ë®é‰∫ÜÂè¶‰∏ÄÂÄãÈù¢Âêë‚Äî‚ÄîÂñÆË™øÊÄß„ÄÇÊ†πÊìöÈ†òÂüüÂ∞àÂÆ∂ÔºåÂñÆË™øÊÄßÂú®Ë®±Â§öËàáÂÖ¨Âπ≥ÊÄßÁõ∏ÈóúÁöÑÈ†òÂüü‰∏≠ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤ÔºåÈÅïÂèçÂñÆË™øÊÄßÂèØËÉΩÊúÉË™§Â∞é‰∫∫È°ûÊ±∫Á≠ñÔºå‰∏¶Â∞éËá¥ÁÅΩÈõ£ÊÄßÁöÑÂæåÊûú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞ÊáâÁî®ÂñÆË™øÁ•ûÁ∂ìÂä†Ê≥ïÊ®°Âûã (MNAM) ÁöÑÈáçË¶ÅÊÄßÔºåÈÄô‰∫õÊ®°Âûã‰ΩøÁî®ÂÖ¨Âπ≥ÊÑüÁü• ML ÊºîÁÆóÊ≥ï‰æÜÂº∑Âà∂Âü∑Ë°åÂÄãÂà•ÂíåÊàêÂ∞çÂñÆË™øÊÄßÂéüÂâáÔºå‰ª•Á¢∫‰øù AI ÂÄ´ÁêÜÂíåÁ§æÊúÉÁöÑÂÖ¨Âπ≥ÊÄß„ÄÇÊàëÂÄëÈÄèÈÅéÁêÜË´ñÊé®ÁêÜ„ÄÅÊ®°Êì¨ÂíåÂª£Ê≥õÁöÑÂØ¶Ë≠âÂàÜÊûêÁöÑÊ∑∑ÂêàÊñπÊ≥ïÁôºÁèæÔºåÂú®ÊâÄÊúâÂÖ¨Âπ≥È†òÂüüÔºàÂåÖÊã¨ÁäØÁΩ™Â≠∏„ÄÅÊïôËÇ≤„ÄÅÈÜ´ÁôÇ‰øùÂÅ•ÂíåÈáëËûçÔºâ‰∏≠ÔºåËÄÉÈáèÂñÆË™øÊÄßÂÖ¨ÁêÜËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÊúâÂä©Êñº AI ÂÄ´ÁêÜ„ÄÅÂèØËß£Èáã AI (XAI) Âíå‰∫∫Ê©ü‰∫íÂãï (HCI) ‰ªãÈù¢‰∏≠ÁöÑË∑®È†òÂüüÁ†îÁ©∂„ÄÇÈÄèÈÅéË≠âÊòé‰∏çÁ¨¶ÂêàÂñÆË™øÊÄßÁöÑÁÅΩÈõ£ÊÄßÂæåÊûúÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂñÆË™øÊÄßÈúÄÊ±ÇÂú® AI ÊáâÁî®‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË≠âÊòé MNAM ÊòØ‰∏ÄÁ®ÆÊúâÊïàÁöÑÂÖ¨Âπ≥ÊÑüÁü• ML ÊñπÊ≥ïÔºåÈÄèÈÅéÊñΩÂä†ÂñÆË™øÊÄßÈôêÂà∂‰æÜÊï¥Âêà‰∫∫È°ûÊô∫ÊÖß„ÄÇ

##### **Rationalizing Predictions by Adversarial Information Calibration**
2301.06009v1 by Lei Sha, Oana-Maria Camburu, Thomas Lukasiewicz

Explaining the predictions of AI models is paramount in safety-critical
applications, such as in legal or medical domains. One form of explanation for
a prediction is an extractive rationale, i.e., a subset of features of an
instance that lead the model to give its prediction on that instance. For
example, the subphrase ``he stole the mobile phone'' can be an extractive
rationale for the prediction of ``Theft''. Previous works on generating
extractive rationales usually employ a two-phase model: a selector that selects
the most important features (i.e., the rationale) followed by a predictor that
makes the prediction based exclusively on the selected features. One
disadvantage of these works is that the main signal for learning to select
features comes from the comparison of the answers given by the predictor to the
ground-truth answers. In this work, we propose to squeeze more information from
the predictor via an information calibration method. More precisely, we train
two models jointly: one is a typical neural model that solves the task at hand
in an accurate but black-box manner, and the other is a selector-predictor
model that additionally produces a rationale for its prediction. The first
model is used as a guide for the second model. We use an adversarial technique
to calibrate the information extracted by the two models such that the
difference between them is an indicator of the missed or over-selected
features. In addition, for natural language tasks, we propose a
language-model-based regularizer to encourage the extraction of fluent
rationales. Experimental results on a sentiment analysis task, a hate speech
recognition task as well as on three tasks from the legal domain show the
effectiveness of our approach to rationale extraction.

ÊëòË¶ÅÔºö<paragraph>Âú®ÂÆâÂÖ®ÈóúÈçµÊáâÁî®Á®ãÂºè‰∏≠Ôºå‰æãÂ¶ÇÊ≥ïÂæãÊàñÈÜ´ÁôÇÈ†òÂüüÔºåËß£Èáã AI Ê®°ÂûãÁöÑÈ†êÊ∏¨Ëá≥ÈóúÈáçË¶Å„ÄÇ‰∏ÄÁ®ÆÈ†êÊ∏¨ÁöÑËß£ÈáãÂΩ¢ÂºèÊòØËêÉÂèñ‰æùÊìöÔºå‰∫¶Âç≥ÊüêÂÄãÂØ¶‰æã‰∏≠Â∞éËá¥Ê®°ÂûãÂ∞çË©≤ÂØ¶‰æãÂÅöÂá∫È†êÊ∏¨ÁöÑÂ≠êÈõÜÂêàÁâπÂæµ„ÄÇ‰æãÂ¶ÇÔºåÂ≠êË©ûÁµÑ„Äå‰ªñÂÅ∑‰∫ÜÊâãÊ©ü„ÄçÂèØËÉΩÊòØ„ÄåÂÅ∑Á´ä„ÄçÈ†êÊ∏¨ÁöÑËêÉÂèñ‰æùÊìö„ÄÇÂÖàÂâçÈóúÊñºÁî¢ÁîüËêÉÂèñ‰æùÊìöÁöÑÁ†îÁ©∂ÈÄöÂ∏∏Êé°Áî®‰∫åÈöéÊÆµÊ®°ÂûãÔºö‰∏ÄÂÄãÈÅ∏ÊìáÂô®ÈÅ∏ÊìáÊúÄÈáçË¶ÅÁöÑÁâπÂæµÔºàÂç≥‰æùÊìöÔºâÔºåÊé•ËëóÊòØ‰∏ÄÂÄãÈ†êÊ∏¨Âô®ÔºåÂÆÉÊ†πÊìöÊâÄÈÅ∏ÁöÑÁâπÂæµÁç®ÂÆ∂ÂÅöÂá∫È†êÊ∏¨„ÄÇÈÄô‰∫õÁ†îÁ©∂ÁöÑ‰∏ÄÂÄãÁº∫ÈªûÊòØÔºåÂ≠∏ÁøíÈÅ∏ÊìáÁâπÂæµÁöÑ‰∏ªË¶ÅË®äËôü‰æÜËá™Â∞áÈ†êÊ∏¨Âô®Áµ¶Âá∫ÁöÑÁ≠îÊ°àËàáÁúüÂØ¶Á≠îÊ°àÈÄ≤Ë°åÊØîËºÉ„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂª∫Ë≠∞ÈÄèÈÅéË≥áË®äÊ†°Ê≠£ÊñπÊ≥ïÂæûÈ†êÊ∏¨Âô®‰∏≠Êì∑ÂèñÊõ¥Â§öË≥áË®ä„ÄÇÊõ¥Á≤æÁ¢∫Âú∞Ë™™ÔºåÊàëÂÄëËÅØÂêàË®ìÁ∑¥ÂÖ©ÂÄãÊ®°ÂûãÔºö‰∏ÄÂÄãÊòØÂÖ∏ÂûãÁöÑÈ°ûÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÔºåÂÆÉ‰ª•Ê∫ñÁ¢∫‰ΩÜÈªëÁÆ±ÁöÑÊñπÂºèËß£Ê±∫ÊâãÈÇäÁöÑ‰ªªÂãôÔºåÂè¶‰∏ÄÂÄãÊòØÈÅ∏ÊìáÂô®È†êÊ∏¨Âô®Ê®°ÂûãÔºåÂÆÉÂè¶Â§ñÁÇ∫ÂÖ∂È†êÊ∏¨Áî¢Áîü‰æùÊìö„ÄÇÁ¨¨‰∏ÄÂÄãÊ®°ÂûãÁî®‰ΩúÁ¨¨‰∫åÂÄãÊ®°ÂûãÁöÑÊåáÂçó„ÄÇÊàëÂÄë‰ΩøÁî®Â∞çÊäóÊäÄË°ìÊ†°Ê≠£ÂÖ©ÂÄãÊ®°ÂûãËêÉÂèñÁöÑË≥áË®äÔºå‰ΩøÂÆÉÂÄë‰πãÈñìÁöÑÂ∑ÆÁï∞ÊàêÁÇ∫ÈÅ∫ÊºèÊàñÈÅéÂ∫¶ÈÅ∏ÊìáÁöÑÁâπÂæµÁöÑÊåáÊ®ô„ÄÇÊ≠§Â§ñÔºåÂ∞çÊñºËá™ÁÑ∂Ë™ûË®Ä‰ªªÂãôÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºË™ûË®ÄÊ®°ÂûãÁöÑÊ≠£Ë¶èÂåñÂô®Ôºå‰ª•ÈºìÂãµËêÉÂèñÊµÅÊö¢ÁöÑ‰æùÊìö„ÄÇÊÉÖÁ∑íÂàÜÊûê‰ªªÂãô„ÄÅ‰ªáÊÅ®Ë®ÄË´ñËæ®Ë≠ò‰ªªÂãô‰ª•ÂèäÊ≥ïÂæãÈ†òÂüü‰∏âÂÄã‰ªªÂãôÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÂú®‰æùÊìöËêÉÂèñÊñπÈù¢ÁöÑÂÅöÊ≥ïÂçÅÂàÜÊúâÊïà„ÄÇ</paragraph>

##### **Semantic match: Debugging feature attribution methods in XAI for healthcare**
2301.02080v3 by Giovanni Cin√†, Tabea E. R√∂ber, Rob Goedhart, ≈û. ƒ∞lker Birbil

The recent spike in certified Artificial Intelligence (AI) tools for
healthcare has renewed the debate around adoption of this technology. One
thread of such debate concerns Explainable AI (XAI) and its promise to render
AI devices more transparent and trustworthy. A few voices active in the medical
AI space have expressed concerns on the reliability of Explainable AI
techniques and especially feature attribution methods, questioning their use
and inclusion in guidelines and standards. Despite valid concerns, we argue
that existing criticism on the viability of post-hoc local explainability
methods throws away the baby with the bathwater by generalizing a problem that
is specific to image data. We begin by characterizing the problem as a lack of
semantic match between explanations and human understanding. To understand when
feature importance can be used reliably, we introduce a distinction between
feature importance of low- and high-level features. We argue that for data
types where low-level features come endowed with a clear semantics, such as
tabular data like Electronic Health Records (EHRs), semantic match can be
obtained, and thus feature attribution methods can still be employed in a
meaningful and useful way. Finally, we sketch a procedure to test whether
semantic match has been achieved.

ÊëòË¶ÅÔºöÊúÄËøëÈÄöÈÅéË™çË≠âÁöÑ‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÈÜ´ÁôÇ‰øùÂÅ•Â∑•ÂÖ∑Êï∏ÈáèÊøÄÂ¢ûÔºåËÆìÊé°Áî®Ê≠§ÊäÄË°ìÁöÑËæØË´ñÂÜçÂ∫¶ÊµÆ‰∏äÊ™ØÈù¢„ÄÇÂÖ∂‰∏≠‰∏ÄÂÄãËæØË´ñ‰∏ªÈ°åÊòØÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖßÔºàXAIÔºâÂèäÂÖ∂ËÆì AI Ë£ùÁΩÆÊõ¥ÈÄèÊòé‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑÊâøË´æ„ÄÇÈÜ´ÁôÇ AI È†òÂüü‰∏≠ÁöÑ‰∏Ä‰∫õÁ©çÊ•µÁôºË®ÄËÄÖË°®ÈÅî‰∫ÜÂ∞çÂèØËß£Èáã AI ÊäÄË°ìÔºåÂ∞§ÂÖ∂ÊòØÁâπÂæµÊ≠∏Âõ†ÊñπÊ≥ïÁöÑÂèØÈù†ÊÄßÁñëÊÖÆÔºåË≥™ÁñëÂÖ∂Âú®Ê∫ñÂâáÂíåÊ®ôÊ∫ñ‰∏≠ÁöÑ‰ΩøÁî®ÂíåÁ¥çÂÖ•„ÄÇÂÑòÁÆ°ÊúâÂêàÁêÜÁöÑÁñëÊÖÆÔºåÊàëÂÄë‰∏ªÂºµÂ∞ç‰∫ãÂæåÂ±ÄÈÉ®ÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑÂèØË°åÊÄßÊèêÂá∫ÊâπË©ïÔºåÁ≠âÊñºÈÄ£ÂêåÊ¥óÊæ°Ê∞¥‰∏ÄËµ∑ÊääÂ¨∞ÂÖíÂÄíÊéâÔºåÂõ†ÁÇ∫ÈÄôÊòØÂú®Â∞çÂΩ±ÂÉèË≥áÊñôÁâπÊúâÁöÑÂïèÈ°åÈÄ≤Ë°åÊ¶ÇÂåñ„ÄÇÊàëÂÄëÂæûÂ∞áÂïèÈ°åÊèèËø∞ÁÇ∫Ëß£ÈáãËàá‰∫∫È°ûÁêÜËß£‰πãÈñìÁº∫‰πèË™ûÊÑèÂåπÈÖçÈñãÂßã„ÄÇÁÇ∫‰∫ÜÁû≠Ëß£‰ΩïÊôÇÂèØ‰ª•ÂèØÈù†Âú∞‰ΩøÁî®ÁâπÂæµÈáçË¶ÅÊÄßÔºåÊàëÂÄëÂçÄÂàÜ‰∫Ü‰ΩéÈöéÂíåÈ´òÈöéÁâπÂæµÁöÑÁâπÂæµÈáçË¶ÅÊÄß„ÄÇÊàëÂÄë‰∏ªÂºµÔºåÂ∞çÊñº‰ΩéÈöéÁâπÂæµÂÖ∑ÊúâÊòéÁ¢∫Ë™ûÊÑèÁöÑË≥áÊñôÈ°ûÂûãÔºå‰æãÂ¶ÇÈõªÂ≠êÂÅ•Â∫∑Ë®òÈåÑÔºàEHRÔºâÁ≠âË°®Ê†ºË≥áÊñôÔºåÂèØ‰ª•Áç≤ÂæóË™ûÊÑèÂåπÈÖçÔºåÂõ†Ê≠§‰ªçÁÑ∂ÂèØ‰ª•Âú®ÊúâÊÑèÁæ©‰∏îÊúâÁî®ÁöÑÊñπÂºè‰∏≠Êé°Áî®ÁâπÂæµÊ≠∏Âõ†ÊñπÊ≥ï„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊ¶ÇËø∞‰∫Ü‰∏ÄÂÄãÁ®ãÂ∫èÔºå‰ª•Ê∏¨Ë©¶ÊòØÂê¶Â∑≤ÈÅîÊàêË™ûÊÑèÂåπÈÖç„ÄÇ

##### **Context-dependent Explainability and Contestability for Trustworthy Medical Artificial Intelligence: Misclassification Identification of Morbidity Recognition Models in Preterm Infants**
2212.08821v1 by Isil Guzey, Ozlem Ucar, Nukhet Aladag Ciftdemir, Betul Acunas

Although machine learning (ML) models of AI achieve high performances in
medicine, they are not free of errors. Empowering clinicians to identify
incorrect model recommendations is crucial for engendering trust in medical AI.
Explainable AI (XAI) aims to address this requirement by clarifying AI
reasoning to support the end users. Several studies on biomedical imaging
achieved promising results recently. Nevertheless, solutions for models using
tabular data are not sufficient to meet the requirements of clinicians yet.
This paper proposes a methodology to support clinicians in identifying failures
of ML models trained with tabular data. We built our methodology on three main
pillars: decomposing the feature set by leveraging clinical context latent
space, assessing the clinical association of global explanations, and Latent
Space Similarity (LSS) based local explanations. We demonstrated our
methodology on ML-based recognition of preterm infant morbidities caused by
infection. The risk of mortality, lifelong disability, and antibiotic
resistance due to model failures was an open research question in this domain.
We achieved to identify misclassification cases of two models with our
approach. By contextualizing local explanations, our solution provides
clinicians with actionable insights to support their autonomy for informed
final decisions.

ÊëòË¶ÅÔºöÂÑòÁÆ°‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÂú®ÈÜ´Â≠∏È†òÂüü‰∏≠Ë°®ÁèæÂÑ™Áï∞Ôºå‰ΩÜÂÆÉÂÄë‰∏¶ÈùûÊ≤íÊúâÈåØË™§„ÄÇËÆìËá®Â∫äÈÜ´ÁîüËÉΩÂ§†Ëæ®Ë≠ò‰∏çÊ≠£Á¢∫ÁöÑÊ®°ÂûãÂª∫Ë≠∞ÔºåÂ∞çÊñºÂª∫Á´ãÂ∞çÈÜ´ÁôÇ AI ÁöÑ‰ø°‰ªªËá≥ÈóúÈáçË¶Å„ÄÇÂèØËß£Èáã AI (XAI) Êó®Âú®ÈÄèÈÅéÈáêÊ∏Ö AI Êé®ÁêÜ‰æÜÊªøË∂≥Ê≠§È†ÖÈúÄÊ±ÇÔºå‰ª•ÊîØÊè¥ÊúÄÁµÇ‰ΩøÁî®ËÄÖ„ÄÇÊúÄËøëÈáùÂ∞çÁîüÁâ©ÈÜ´Â≠∏ÂΩ±ÂÉèÈÄ≤Ë°åÁöÑÂπæÈ†ÖÁ†îÁ©∂Áç≤Âæó‰∫Ü‰ª§‰∫∫ÊªøÊÑèÁöÑÁµêÊûú„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî®Ë°®Ê†ºË≥áÊñôÁöÑÊ®°ÂûãËß£Ê±∫ÊñπÊ°àÈÇÑ‰∏çË∂≥‰ª•ÊªøË∂≥Ëá®Â∫äÈÜ´ÁîüÁöÑÈúÄÊ±Ç„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂçîÂä©Ëá®Â∫äÈÜ´ÁîüËæ®Ë≠ò‰ΩøÁî®Ë°®Ê†ºË≥áÊñôË®ìÁ∑¥ÁöÑ ML Ê®°ÂûãÁöÑÂ§±Êïó„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÂª∫Á´ãÂú®‰∏âÂÄã‰∏ªË¶ÅÊîØÊü±‰∏äÔºöÂà©Áî®Ëá®Â∫äËÉåÊôØÊΩõÂú®Á©∫ÈñìÂàÜËß£ÁâπÂæµÈõÜ„ÄÅË©ï‰º∞Êï¥È´îËß£ÈáãÁöÑËá®Â∫äÈóúËÅØÊÄßÔºå‰ª•ÂèäÂü∫ÊñºÊΩõÂú®Á©∫ÈñìÁõ∏‰ººÊÄß (LSS) ÁöÑÂ±ÄÈÉ®Ëß£Èáã„ÄÇÊàëÂÄëÂú® ML Âü∫ÊñºÊÑüÊüìÊâÄÂ∞éËá¥ÁöÑÊó©Áî¢ÂÖíÁôºÁóÖÁéáË≠òÂà•‰∏äÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÈÄôÈ†ÖÊñπÊ≥ï„ÄÇÁî±ÊñºÊ®°ÂûãÂ§±ÊïóËÄåÁî¢ÁîüÁöÑÊ≠ª‰∫°È¢®Èö™„ÄÅÁµÇË∫´ÊÆòÁñæÂíåÊäóÁîüÁ¥†ÊäóËó•ÊÄßÔºåÊòØÊ≠§È†òÂüü‰∏≠‰∏ÄÂÄãÂÖ¨ÈñãÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÊàêÂäüËæ®Ë≠òÂá∫ÂÖ©ÂÄãÊ®°ÂûãÁöÑÈåØË™§ÂàÜÈ°ûÊ°à‰æã„ÄÇÊàëÂÄëÁöÑËß£Ê±∫ÊñπÊ°àÈÄèÈÅéÂ∞áÂ±ÄÈÉ®Ëß£ÈáãËÑàÁµ°ÂåñÔºåÁÇ∫Ëá®Â∫äÈÜ´ÁîüÊèê‰æõÂèØË°åÁöÑË¶ãËß£Ôºå‰ª•ÊîØÊè¥‰ªñÂÄëËá™‰∏ªÂÅöÂá∫ÊòéÊô∫ÁöÑÊúÄÁµÇÊ±∫ÂÆö„ÄÇ


### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-27**|**Taming Data and Transformers for Audio Generation**|Moayed Haji-Ali et.al.|[2406.19388v1](http://arxiv.org/abs/2406.19388v1)|null|
|**2024-06-27**|**The Remarkable Robustness of LLMs: Stages of Inference?**|Vedang Lad et.al.|[2406.19384v1](http://arxiv.org/abs/2406.19384v1)|[link](https://github.com/vdlad/remarkable-robustness-of-llms)|
|**2024-06-27**|**Suri: Multi-constraint Instruction Following for Long-form Text Generation**|Chau Minh Pham et.al.|[2406.19371v1](http://arxiv.org/abs/2406.19371v1)|[link](https://github.com/chtmp223/suri)|
|**2024-06-27**|**Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space**|Core Francisco Park et.al.|[2406.19370v1](http://arxiv.org/abs/2406.19370v1)|null|
|**2024-06-27**|**The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models**|Xiliang Zhu et.al.|[2406.19358v1](http://arxiv.org/abs/2406.19358v1)|null|
|**2024-06-27**|**DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions**|Nigel Fernandez et.al.|[2406.19356v1](http://arxiv.org/abs/2406.19356v1)|null|
|**2024-06-27**|**Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?**|Peter Hase et.al.|[2406.19354v1](http://arxiv.org/abs/2406.19354v1)|null|
|**2024-06-27**|**IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language**|Lucky Susanto et.al.|[2406.19349v1](http://arxiv.org/abs/2406.19349v1)|null|
|**2024-06-27**|**Efficient World Models with Context-Aware Tokenization**|Vincent Micheli et.al.|[2406.19320v1](http://arxiv.org/abs/2406.19320v1)|[link](https://github.com/vmicheli/delta-iris)|
|**2024-06-27**|**Jump Starting Bandits with LLM-Generated Prior Knowledge**|Parand A. Alamdari et.al.|[2406.19317v1](http://arxiv.org/abs/2406.19317v1)|null|
|**2024-06-27**|**LiveBench: A Challenging, Contamination-Free LLM Benchmark**|Colin White et.al.|[2406.19314v1](http://arxiv.org/abs/2406.19314v1)|[link](https://github.com/livebench/livebench)|
|**2024-06-27**|**From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data**|Zheyang Xiong et.al.|[2406.19292v1](http://arxiv.org/abs/2406.19292v1)|null|
|**2024-06-27**|**HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale**|Junying Chen et.al.|[2406.19280v1](http://arxiv.org/abs/2406.19280v1)|null|
|**2024-06-27**|**VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation**|Yixiao Song et.al.|[2406.19276v1](http://arxiv.org/abs/2406.19276v1)|null|
|**2024-06-27**|**AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning**|Praneeth Vadlapati et.al.|[2406.19271v1](http://arxiv.org/abs/2406.19271v1)|[link](https://github.com/Pro-GenAI/AutoPureData)|
|**2024-06-27**|**Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding**|Yue Fan et.al.|[2406.19263v1](http://arxiv.org/abs/2406.19263v1)|null|
|**2024-06-27**|**AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI**|Kaveen Hiniduma et.al.|[2406.19256v1](http://arxiv.org/abs/2406.19256v1)|null|
|**2024-06-27**|**Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment**|Hao Fei et.al.|[2406.19255v1](http://arxiv.org/abs/2406.19255v1)|null|
|**2024-06-27**|**AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation**|Jia Fu et.al.|[2406.19251v1](http://arxiv.org/abs/2406.19251v1)|null|
|**2024-06-27**|**Application of ASV for Voice Identification after VC and Duration Predictor Improvement in TTS Models**|Borodin Kirill Nikolayevich et.al.|[2406.19243v1](http://arxiv.org/abs/2406.19243v1)|null|
|**2024-06-27**|**Revealing Fine-Grained Values and Opinions in Large Language Models**|Dustin Wright et.al.|[2406.19238v1](http://arxiv.org/abs/2406.19238v1)|null|
|**2024-06-27**|**FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts**|Shubhankar Singh et.al.|[2406.19237v1](http://arxiv.org/abs/2406.19237v1)|null|
|**2024-06-27**|**Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions**|Minghan Li et.al.|[2406.19236v1](http://arxiv.org/abs/2406.19236v1)|[link](https://github.com/lpercc/ha3d_simulator)|
|**2024-06-27**|**Seeing Is Believing: Black-Box Membership Inference Attacks Against Retrieval Augmented Generation**|Yuying Li et.al.|[2406.19234v1](http://arxiv.org/abs/2406.19234v1)|null|
|**2024-06-27**|**RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs**|Ekaterina Taktasheva et.al.|[2406.19232v1](http://arxiv.org/abs/2406.19232v1)|null|
|**2024-06-27**|**Spiking Convolutional Neural Networks for Text Classification**|Changze Lv et.al.|[2406.19230v1](http://arxiv.org/abs/2406.19230v1)|null|
|**2024-06-27**|**Tools Fail: Detecting Silent Errors in Faulty Tools**|Jimin Sun et.al.|[2406.19228v1](http://arxiv.org/abs/2406.19228v1)|null|
|**2024-06-27**|**Aligning Teacher with Student Preferences for Tailored Training Data Generation**|Yantao Liu et.al.|[2406.19227v1](http://arxiv.org/abs/2406.19227v1)|null|
|**2024-06-27**|**Simulating Classroom Education with LLM-Empowered Agents**|Zheyuan Zhang et.al.|[2406.19226v1](http://arxiv.org/abs/2406.19226v1)|null|
|**2024-06-27**|**T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings**|Bj√∂rn Deiseroth et.al.|[2406.19223v1](http://arxiv.org/abs/2406.19223v1)|null|
|**2024-06-27**|**Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos**|Zhimin Shao et.al.|[2406.19217v1](http://arxiv.org/abs/2406.19217v1)|null|
|**2024-06-27**|**SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation**|Zijun Yao et.al.|[2406.19215v1](http://arxiv.org/abs/2406.19215v1)|[link](https://github.com/thu-keg/seakr)|
|**2024-06-27**|**BISeizuRe: BERT-Inspired Seizure Data Representation to Improve Epilepsy Monitoring**|Luca Benfenati et.al.|[2406.19189v1](http://arxiv.org/abs/2406.19189v1)|null|
|**2024-06-27**|**Annotation Errors and NER: A Study with OntoNotes 5.0**|Gabriel Bernier-Colborne et.al.|[2406.19172v1](http://arxiv.org/abs/2406.19172v1)|null|
|**2024-06-27**|**The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems**|Judith Sieker et.al.|[2406.19170v1](http://arxiv.org/abs/2406.19170v1)|null|
|**2024-06-27**|**RAVEN: Multitask Retrieval Augmented Vision-Language Learning**|Varun Nagaraj Rao et.al.|[2406.19150v1](http://arxiv.org/abs/2406.19150v1)|null|
|**2024-06-27**|**BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal Supervision**|Kit Mills Bransby et.al.|[2406.19148v1](http://arxiv.org/abs/2406.19148v1)|[link](https://github.com/kitbransby/backmix)|
|**2024-06-27**|**Resolving Discrepancies in Compute-Optimal Scaling of Language Models**|Tomer Porian et.al.|[2406.19146v1](http://arxiv.org/abs/2406.19146v1)|[link](https://github.com/formll/resolving-scaling-law-discrepencies)|
|**2024-06-27**|**YZS-model: A Predictive Model for Organic Drug Solubility Based on Graph Convolutional Networks and Transformer-Attention**|Chenxu Wang et.al.|[2406.19136v1](http://arxiv.org/abs/2406.19136v1)|null|
|**2024-06-27**|**DEX-TTS: Diffusion-based EXpressive Text-to-Speech with Style Modeling on Time Variability**|Hyun Joon Park et.al.|[2406.19135v1](http://arxiv.org/abs/2406.19135v1)|[link](https://github.com/winddori2002/dex-tts)|
|**2024-06-27**|**Towards Learning Abductive Reasoning using VSA Distributed Representations**|Giacomo Camposampiero et.al.|[2406.19121v1](http://arxiv.org/abs/2406.19121v1)|[link](https://github.com/ibm/abductive-rule-learner-with-context-awareness)|
|**2024-06-27**|**CHEW: A Dataset of CHanging Events in Wikipedia**|Hsuvas Borkakoty et.al.|[2406.19116v1](http://arxiv.org/abs/2406.19116v1)|null|
|**2024-06-27**|**Computational Life: How Well-formed, Self-replicating Programs Emerge from Simple Interaction**|Blaise Ag√ºera y Arcas et.al.|[2406.19108v1](http://arxiv.org/abs/2406.19108v1)|null|
|**2024-06-27**|**Statements: Universal Information Extraction from Tables with Large Language Models for ESG KPIs**|Lokesh Mishra et.al.|[2406.19102v1](http://arxiv.org/abs/2406.19102v1)|null|
|**2024-06-27**|**Fairness and Bias in Multimodal AI: A Survey**|Tosin Adewumi et.al.|[2406.19097v1](http://arxiv.org/abs/2406.19097v1)|null|
|**2024-06-27**|**Dimensions underlying the representational alignment of deep neural networks with humans**|Florian P. Mahner et.al.|[2406.19087v1](http://arxiv.org/abs/2406.19087v1)|[link](https://github.com/florianmahner/object-dimensions)|
|**2024-06-27**|**AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries**|Irina Saparina et.al.|[2406.19073v1](http://arxiv.org/abs/2406.19073v1)|null|
|**2024-06-27**|**EmPO: Theory-Driven Dataset Construction for Empathetic Response Generation through Preference Optimization**|Ondrej Sotolar et.al.|[2406.19071v1](http://arxiv.org/abs/2406.19071v1)|[link](https://github.com/ondrejsotolar/empo)|
|**2024-06-27**|**STBench: Assessing the Ability of Large Language Models in Spatio-Temporal Analysis**|Wenbin Li et.al.|[2406.19065v1](http://arxiv.org/abs/2406.19065v1)|[link](https://github.com/lwbxc/stbench)|
|**2024-06-27**|**Segment Anything Model for automated image data annotation: empirical studies using text prompts from Grounding DINO**|Fuseini Mumuni et.al.|[2406.19057v1](http://arxiv.org/abs/2406.19057v1)|null|
|**2024-06-27**|**A look under the hood of the Interactive Deep Learning Enterprise (No-IDLE)**|Daniel Sonntag et.al.|[2406.19054v1](http://arxiv.org/abs/2406.19054v1)|null|
|**2024-06-27**|**FedMap: Iterative Magnitude-Based Pruning for Communication-Efficient Federated Learning**|Alexander Herzog et.al.|[2406.19050v1](http://arxiv.org/abs/2406.19050v1)|null|
|**2024-06-27**|**Accuracy on the wrong line: On the pitfalls of noisy data for out-of-distribution generalisation**|Amartya Sanyal et.al.|[2406.19049v1](http://arxiv.org/abs/2406.19049v1)|null|
|**2024-06-27**|**Improving Weak-to-Strong Generalization with Reliability-Aware Alignment**|Yue Guo et.al.|[2406.19032v1](http://arxiv.org/abs/2406.19032v1)|[link](https://github.com/irenehere/reliablealignment)|
|**2024-06-27**|**Lithium-Ion Battery System Health Monitoring and Fault Analysis from Field Data Using Gaussian Processes**|Joachim Schaeffer et.al.|[2406.19015v1](http://arxiv.org/abs/2406.19015v1)|null|
|**2024-06-27**|**FedMLP: Federated Multi-Label Medical Image Classification under Task Heterogeneity**|Zhaobin Sun et.al.|[2406.18995v1](http://arxiv.org/abs/2406.18995v1)|[link](https://github.com/szbonaldo/fedmlp)|
|**2024-06-27**|**Semi-supervised Concept Bottleneck Models**|Lijie Hu et.al.|[2406.18992v1](http://arxiv.org/abs/2406.18992v1)|null|
|**2024-06-27**|**RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulaiton**|Fanfan Liu et.al.|[2406.18977v1](http://arxiv.org/abs/2406.18977v1)|null|
|**2024-06-27**|**Applying LLMs for Rescoring N-best ASR Hypotheses of Casual Conversations: Effects of Domain Adaptation and Context Carry-over**|Atsunori Ogawa et.al.|[2406.18972v1](http://arxiv.org/abs/2406.18972v1)|null|
|**2024-06-27**|**UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models**|Siyuan Wu et.al.|[2406.18966v1](http://arxiv.org/abs/2406.18966v1)|null|
|**2024-06-27**|**Investigating and Defending Shortcut Learning in Personalized Diffusion Models**|Yixin Liu et.al.|[2406.18944v1](http://arxiv.org/abs/2406.18944v1)|null|
|**2024-06-27**|**Federated Graph Semantic and Structural Learning**|Wenke Huang et.al.|[2406.18937v1](http://arxiv.org/abs/2406.18937v1)|[link](https://github.com/guanchengwan/fgssl)|
|**2024-06-27**|**The single-use restriction for register automata and transducers over infinite alphabets**|Rafa≈Ç Stefa≈Ñski et.al.|[2406.18934v1](http://arxiv.org/abs/2406.18934v1)|null|
|**2024-06-27**|**Enhanced ASR Robustness to Packet Loss with a Front-End Adaptation Network**|Yehoshua Dissen et.al.|[2406.18928v1](http://arxiv.org/abs/2406.18928v1)|null|
|**2024-06-27**|**Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding**|Jiwan Chung et.al.|[2406.18925v1](http://arxiv.org/abs/2406.18925v1)|null|
|**2024-06-27**|**Time Matters: Scaling Laws for Any Budget**|Itay Inbar et.al.|[2406.18922v1](http://arxiv.org/abs/2406.18922v1)|null|
|**2024-06-27**|**Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data**|Yiting Ran et.al.|[2406.18921v1](http://arxiv.org/abs/2406.18921v1)|null|
|**2024-06-27**|**TrustUQA: A Trustful Framework for Unified Structured Data Question Answering**|Wen Zhang et.al.|[2406.18916v1](http://arxiv.org/abs/2406.18916v1)|null|
|**2024-06-27**|**Factor-Conditioned Speaking-Style Captioning**|Atsushi Ando et.al.|[2406.18910v1](http://arxiv.org/abs/2406.18910v1)|null|
|**2024-06-27**|**Historia Magistra Vitae: Dynamic Topic Modeling of Roman Literature using Neural Embeddings**|Michael Ginn et.al.|[2406.18907v1](http://arxiv.org/abs/2406.18907v1)|null|
|**2024-06-27**|**Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets**|Melanie Walsh et.al.|[2406.18906v1](http://arxiv.org/abs/2406.18906v1)|null|
|**2024-06-27**|**The Rise of Artificial Intelligence in Educational Measurement: Opportunities and Ethical Challenges**|Okan Bulut et.al.|[2406.18900v1](http://arxiv.org/abs/2406.18900v1)|null|
|**2024-06-27**|**Autonomous Control of a Novel Closed Chain Five Bar Active Suspension via Deep Reinforcement Learning**|Nishesh Singh et.al.|[2406.18899v1](http://arxiv.org/abs/2406.18899v1)|null|
|**2024-06-27**|**Can we teach language models to gloss endangered languages?**|Michael Ginn et.al.|[2406.18895v1](http://arxiv.org/abs/2406.18895v1)|null|
|**2024-06-27**|**SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models**|Vipul Rathore et.al.|[2406.18880v1](http://arxiv.org/abs/2406.18880v1)|[link](https://github.com/dair-iitd/SSP)|
|**2024-06-27**|**DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment**|Ke-Han Lu et.al.|[2406.18871v1](http://arxiv.org/abs/2406.18871v1)|null|
|**2024-06-27**|**Efficacy of Language Model Self-Play in Non-Zero-Sum Games**|Austen Liao et.al.|[2406.18872v1](http://arxiv.org/abs/2406.18872v1)|null|
|**2024-06-27**|**Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology Report Simplification**|Ziyu Yang et.al.|[2406.18859v1](http://arxiv.org/abs/2406.18859v1)|[link](https://github.com/ziyu-yang/human-evaluation)|
|**2024-06-27**|**FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus**|Yuxin Fu et.al.|[2406.18856v1](http://arxiv.org/abs/2406.18856v1)|null|
|**2024-06-27**|**LICO: Large Language Models for In-Context Molecular Optimization**|Tung Nguyen et.al.|[2406.18851v1](http://arxiv.org/abs/2406.18851v1)|null|
|**2024-06-27**|**Learning Retrieval Augmentation for Personalized Dialogue Generation**|Qiushi Huang et.al.|[2406.18847v1](http://arxiv.org/abs/2406.18847v1)|[link](https://github.com/hqsiswiliam/lapdog)|
|**2024-06-27**|**Retain, Blend, and Exchange: A Quality-aware Spatial-Stereo Fusion Approach for Event Stream Recognition**|Lan Chen et.al.|[2406.18845v1](http://arxiv.org/abs/2406.18845v1)|[link](https://github.com/event-ahu/efv_event_classification)|
|**2024-06-27**|**Disentangling Knowledge-based and Visual Reasoning by Question Decomposition in KB-VQA**|Elham J. Barezi et.al.|[2406.18839v1](http://arxiv.org/abs/2406.18839v1)|null|
|**2024-06-27**|**OutlierTune: Efficient Channel-Wise Quantization for Large Language Models**|Jinguang Wang et.al.|[2406.18832v1](http://arxiv.org/abs/2406.18832v1)|null|
|**2024-06-27**|**A Survey on Privacy Attacks Against Digital Twin Systems in AI-Robotics**|Ivan A. Fernandez et.al.|[2406.18812v1](http://arxiv.org/abs/2406.18812v1)|null|
|**2024-06-27**|**Infinite Width Models That Work: Why Feature Learning Doesn't Matter as Much as You Think**|Luke Sernau et.al.|[2406.18800v1](http://arxiv.org/abs/2406.18800v1)|null|
|**2024-06-26**|**MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data**|William Berman et.al.|[2406.18790v1](http://arxiv.org/abs/2406.18790v1)|null|
|**2024-06-26**|**Psychological Profiling in Cybersecurity: A Look at LLMs and Psycholinguistic Features**|Jean Marie Tshimula et.al.|[2406.18783v1](http://arxiv.org/abs/2406.18783v1)|null|
|**2024-06-26**|**Implicit Discourse Relation Classification For Nigerian Pidgin**|Muhammed Saeed et.al.|[2406.18776v1](http://arxiv.org/abs/2406.18776v1)|null|
|**2024-06-26**|**WV-Net: A foundation model for SAR WV-mode satellite imagery trained using contrastive self-supervised learning on 10 million images**|Yannik Glaser et.al.|[2406.18765v1](http://arxiv.org/abs/2406.18765v1)|null|
|**2024-06-26**|**Conformalized Link Prediction on Graph Neural Networks**|Tianyi Zhao et.al.|[2406.18763v1](http://arxiv.org/abs/2406.18763v1)|null|
|**2024-06-26**|**Categorical Syllogisms Revisited: A Review of the Logical Reasoning Abilities of LLMs for Analyzing Categorical Syllogism**|Shi Zong et.al.|[2406.18762v1](http://arxiv.org/abs/2406.18762v1)|null|
|**2024-06-26**|**A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems**|Karn N. Watcharasupat et.al.|[2406.18747v1](http://arxiv.org/abs/2406.18747v1)|[link](https://github.com/kwatcharasupat/query-bandit)|
|**2024-06-26**|**Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models**|Baharan Nouriinanloo et.al.|[2406.18740v1](http://arxiv.org/abs/2406.18740v1)|null|
|**2024-06-26**|**WavRx: a Disease-Agnostic, Generalizable, and Privacy-Preserving Speech Health Diagnostic Model**|Yi Zhu et.al.|[2406.18731v1](http://arxiv.org/abs/2406.18731v1)|[link](https://github.com/zhu00121/wavrx)|
|**2024-06-26**|**Jailbreaking LLMs with Arabic Transliteration and Arabizi**|Mansour Al Ghanim et.al.|[2406.18725v1](http://arxiv.org/abs/2406.18725v1)|null|
|**2024-06-26**|**Learn it or Leave it: Module Composition and Pruning for Continual Learning**|Mingyang Wang et.al.|[2406.18708v1](http://arxiv.org/abs/2406.18708v1)|null|
|**2024-06-26**|**Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling Legislative Behavior and Bipartisanship**|Zachary R. Baker et.al.|[2406.18702v1](http://arxiv.org/abs/2406.18702v1)|null|
|**2024-06-26**|**Fast Optimizer Benchmark**|Simon Blauth et.al.|[2406.18701v1](http://arxiv.org/abs/2406.18701v1)|[link](https://github.com/automl/fob)|
|**2024-06-26**|**Sequence Graph Network for Online Debate Analysis**|Quan Mai et.al.|[2406.18696v1](http://arxiv.org/abs/2406.18696v1)|null|

#### Abstracts
##### **Taming Data and Transformers for Audio Generation**
2406.19388v1 by Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, Vicente Ordonez

Generating ambient sounds and effects is a challenging problem due to data
scarcity and often insufficient caption quality, making it difficult to employ
large-scale generative models for the task. In this work, we tackle the problem
by introducing two new models. First, we propose AutoCap, a high-quality and
efficient automatic audio captioning model. We show that by leveraging metadata
available with the audio modality, we can substantially improve the quality of
captions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from
the best available captioning model at four times faster inference speed. We
then use AutoCap to caption clips from existing datasets, obtaining 761,000
audio clips with high-quality captions, forming the largest available
audio-text dataset. Second, we propose GenAu, a scalable transformer-based
audio generation architecture that we scale up to 1.25B parameters and train
with our new dataset. When compared to state-of-the-art audio generators, GenAu
obtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%
in CLAP score, indicating significantly improved quality of generated audio
compared to previous works. This shows that the quality of data is often as
important as its quantity. Besides, since AutoCap is fully automatic, new audio
samples can be added to the training dataset, unlocking the training of even
larger generative models for audio synthesis.

ÊëòË¶ÅÔºö<paragraph>Áî±ÊñºË≥áÊñôÁ®ÄÂ∞ë‰∏îÂ≠óÂπïÂìÅË≥™ÈÄöÂ∏∏‰∏çË∂≥ÔºåÁî¢ÁîüÁí∞Â¢ÉÈü≥ÊïàÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂïèÈ°åÔºåÈÄô‰ΩøÂæóÈõ£‰ª•‰ΩøÁî®Â§ßË¶èÊ®°ÁîüÊàêÊ®°Âûã‰æÜÂü∑Ë°åÊ≠§‰ªªÂãô„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂºïÂÖ•ÂÖ©ÂÄãÊñ∞Ê®°Âûã‰æÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°å„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫ AutoCapÔºåÈÄôÊòØ‰∏ÄÂÄãÈ´òÂìÅË≥™‰∏îÈ´òÊïàÁöÑËá™ÂãïÈü≥Ë®äÂ≠óÂπïÊ®°Âûã„ÄÇÊàëÂÄëÂ±ïÁ§∫ÈÄèÈÅéÂà©Áî®Èü≥Ë®äÊ®°Âºè‰∏≠ÂèØÁî®ÁöÑÂÖÉË≥áÊñôÔºåÊàëÂÄëÂèØ‰ª•Â§ßÂπÖÊèêÂçáÂ≠óÂπïÂìÅË≥™„ÄÇAutoCap ÁöÑ CIDEr ÂàÜÊï∏ÈÅîÂà∞ 83.2ÔºåÊØîÁèæÊúâÊúÄ‰Ω≥Â≠óÂπïÊ®°ÂûãÈÄ≤Ê≠•‰∫Ü 3.2%ÔºåËÄå‰∏îÊé®Ë´ñÈÄüÂ∫¶Âø´‰∫ÜÂõõÂÄç„ÄÇÁÑ∂ÂæåÊàëÂÄë‰ΩøÁî® AutoCap ÁÇ∫ÁèæÊúâË≥áÊñôÈõÜ‰∏≠ÁöÑÁâáÊÆµÂä†‰∏äÂ≠óÂπïÔºåÂèñÂæó 761,000 ÂÄãÂÖ∑ÊúâÈ´òÂìÅË≥™Â≠óÂπïÁöÑÈü≥Ë®äÁâáÊÆµÔºåÂΩ¢ÊàêÊúÄÂ§ßÁöÑÂèØÁî®Èü≥Ë®äÊñáÂ≠óË≥áÊñôÈõÜ„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫ GenAuÔºåÈÄôÊòØ‰∏ÄÂÄãÂèØÊì¥ÂÖÖÁöÑÂü∫Êñº Transformer ÁöÑÈü≥Ë®äÁîüÊàêÊû∂ÊßãÔºåÊàëÂÄëÂ∞áÂÖ∂Êì¥ÂÖÖÂà∞ 1.25B ÂÄãÂèÉÊï∏Ôºå‰∏¶‰ΩøÁî®ÊàëÂÄëÁöÑÊñ∞Ë≥áÊñôÈõÜÈÄ≤Ë°åË®ìÁ∑¥„ÄÇËàáÊúÄÂÖàÈÄ≤ÁöÑÈü≥Ë®äÁîüÊàêÂô®Áõ∏ÊØîÔºåGenAu Âú® FAD ÂàÜÊï∏ÊñπÈù¢Áç≤Âæó‰∫Ü 15.7% ÁöÑÈ°ØËëóÊèêÂçáÔºåÂú® IS ÊñπÈù¢ÊèêÂçá‰∫Ü 22.7%ÔºåÂú® CLAP ÂàÜÊï∏ÊñπÈù¢ÊèêÂçá‰∫Ü 13.5%ÔºåÈÄôË°®Á§∫ËàáÂÖàÂâçÁöÑ‰ΩúÂìÅÁõ∏ÊØîÔºåÁîüÊàêÁöÑÈü≥Ë®äÂìÅË≥™ÊúâÈ°ØËëóÊèêÂçá„ÄÇÈÄôÈ°ØÁ§∫Ë≥áÊñôÁöÑÂìÅË≥™ÈÄöÂ∏∏ËàáÂÖ∂Êï∏Èáè‰∏ÄÊ®£ÈáçË¶Å„ÄÇÊ≠§Â§ñÔºåÁî±Êñº AutoCap ÊòØÂÖ®Ëá™ÂãïÁöÑÔºåÂõ†Ê≠§ÂèØ‰ª•Â∞áÊñ∞ÁöÑÈü≥Ë®äÁØÑ‰æãÊñ∞Â¢ûÂà∞Ë®ìÁ∑¥Ë≥áÊñôÈõÜÔºåÈÄôÂ∞áÈñãÂïüË®ìÁ∑¥Êõ¥Â§ßÈü≥Ë®äÂêàÊàêÁîüÊàêÊ®°ÂûãÁöÑÂèØËÉΩÊÄß„ÄÇ</paragraph>

##### **The Remarkable Robustness of LLMs: Stages of Inference?**
2406.19384v1 by Vedang Lad, Wes Gurnee, Max Tegmark

We demonstrate and investigate the remarkable robustness of Large Language
Models by deleting and swapping adjacent layers. We find that deleting and
swapping interventions retain 72-95\% of the original model's prediction
accuracy without fine-tuning, whereas models with more layers exhibit more
robustness. Based on the results of the layer-wise intervention and further
experiments, we hypothesize the existence of four universal stages of inference
across eight different models: detokenization, feature engineering, prediction
ensembling, and residual sharpening. The first stage integrates local
information, lifting raw token representations into higher-level contextual
representations. Next is the iterative refinement of task and entity-specific
features. Then, the second half of the model begins with a phase transition,
where hidden representations align more with the vocabulary space due to
specialized model components. Finally, the last layer sharpens the following
token distribution by eliminating obsolete features that add noise to the
prediction.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄëÈÄèÈÅéÂà™Èô§Âíå‰∫§ÊèõÁõ∏ÈÑ∞Â±§ÔºåÂ±ïÁ§∫‰∏¶Êé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÈùûÂá°Á©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÁôºÁèæÂà™Èô§Âíå‰∫§Êèõ‰ªãÂÖ•Á∂≠ÊåÅ‰∫Ü 72-95% ÁöÑÂéüÂßãÊ®°ÂûãÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÔºåËÄåÊ≤íÊúâÂæÆË™øÔºåËÄåÂÖ∑ÊúâÊõ¥Â§öÂ±§ÁöÑÊ®°ÂûãÂâáË°®ÁèæÂá∫Êõ¥È´òÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊ†πÊìöÂ±§Á¥ö‰ªãÂÖ•ÁöÑÁµêÊûúÂíåÈÄ≤‰∏ÄÊ≠•ÁöÑÂØ¶È©óÔºåÊàëÂÄëÂÅáË®≠Âú®ÂÖ´Á®Æ‰∏çÂêåÊ®°Âûã‰∏≠Â≠òÂú®ÂõõÂÄãÊôÆÈÅçÁöÑÊé®Ë´ñÈöéÊÆµÔºöÂéªÁ¨¶ËôüÂåñ„ÄÅÁâπÂæµÂ∑•Á®ã„ÄÅÈ†êÊ∏¨ÈõÜÊàêÂíåÊÆòÂ∑ÆÈä≥Âåñ„ÄÇÁ¨¨‰∏ÄÈöéÊÆµÊï¥ÂêàÂ±ÄÈÉ®Ë≥áË®äÔºåÂ∞áÂéüÂßãÁ¨¶ËôüË°®Á§∫ÊèêÂçáÂà∞Êõ¥È´òÁ¥öÂà•ÁöÑ‰∏ä‰∏ãÊñáË°®Á§∫„ÄÇÊé•‰∏ã‰æÜÊòØ‰ªªÂãôÂíåÁâπÂÆöÂØ¶È´îÁâπÂæµÁöÑËø≠‰ª£ÊîπÈÄ≤„ÄÇÁÑ∂ÂæåÔºåÊ®°ÂûãÁöÑÂæåÂçäÈÉ®ÂàÜÂæûÁõ∏‰ΩçËΩâËÆäÈñãÂßãÔºåÂÖ∂‰∏≠Èö±ËóèË°®Á§∫Áî±ÊñºÂ∞àÊ•≠Ê®°ÂûãÁµÑ‰ª∂ËÄåÊõ¥Á¨¶ÂêàË©ûÂΩôÁ©∫Èñì„ÄÇÊúÄÂæåÔºåÊúÄÂæå‰∏ÄÂ±§ÈÄèÈÅéÊ∂àÈô§Â¢ûÂä†È†êÊ∏¨ÈõúË®äÁöÑÈÅéÊôÇÁâπÂæµ‰æÜÈä≥ÂåñÂæåÁ∫åÁ¨¶ËôüÂàÜ‰Ωà„ÄÇ</paragraph>

##### **Suri: Multi-constraint Instruction Following for Long-form Text Generation**
2406.19371v1 by Chau Minh Pham, Simeng Sun, Mohit Iyyer

Existing research on instruction following largely focuses on tasks with
simple instructions and short responses. In this work, we explore
multi-constraint instruction following for generating long-form text. We create
Suri, a dataset with 20K human-written long-form texts paired with
LLM-generated backtranslated instructions that contain multiple complex
constraints. Because of prohibitive challenges associated with collecting human
preference judgments on long-form texts, preference-tuning algorithms such as
DPO are infeasible in our setting; thus, we propose Instructional ORPO
(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving
negative feedback from dispreferred responses, I-ORPO obtains negative feedback
from synthetically corrupted instructions generated by an LLM. Using Suri, we
perform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The
resulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts
(~5K tokens) than base models without significant quality deterioration. Our
human evaluation shows that while both SFT and I-ORPO models satisfy most
constraints, Suri-I-ORPO generations are generally preferred for their coherent
and informative incorporation of the constraints. We release our code at
https://github.com/chtmp223/suri.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÊåá‰ª§ÈÅµÂæ™Á†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÂÖ∑ÊúâÁ∞°ÂñÆÊåá‰ª§ÂíåÁ∞°Áü≠ÂõûÊáâÁöÑ‰ªªÂãô‰∏ä„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Á¥¢Â§öÈáçÁ¥ÑÊùüÊåá‰ª§Ôºå‰ª•Áî¢ÁîüÈï∑ÁØáÊñáÂ≠ó„ÄÇÊàëÂÄëÂâµÂª∫‰∫Ü SuriÔºå‰∏ÄÂÄãÂåÖÂê´ 20K ‰∫∫Â∑•Êí∞ÂØ´Èï∑ÁØáÊñáÂ≠óÁöÑË≥áÊñôÈõÜÔºå‰∏¶ÈÖçÂ∞çÂåÖÂê´Â§öÈáçË§áÈõúÁ¥ÑÊùüÁöÑ LLM ÁîüÊàêÁöÑÂèçÂêëÁøªË≠ØÊåá‰ª§„ÄÇÁî±ÊñºËàáÊî∂ÈõÜÈï∑ÁØáÊñáÂ≠óÁöÑ‰∫∫È°ûÂÅèÂ•ΩÂà§Êñ∑Áõ∏ÈóúÁöÑÊåëÊà∞ÊÄßÔºåÂõ†Ê≠§Âú®ÊàëÂÄëÁöÑË®≠ÂÆö‰∏≠ÔºåÂÅèÂ•ΩË™øÊï¥ÊºîÁÆóÊ≥ïÔºà‰æãÂ¶Ç DPOÔºâ‰∏çÂèØË°åÔºõÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂü∫Êñº ORPO ÊºîÁÆóÊ≥ïÁöÑÂ∞çÈΩäÊñπÊ≥ïÔºåÂç≥Êåá‰ª§ÊÄß ORPO (I-ORPO)„ÄÇI-ORPO Ê≤íÊúâÂæû‰∏çÂèóÊ≠°ËøéÁöÑÂõûÊáâ‰∏≠Êé•Êî∂Ë≤†Èù¢ÂõûÈ•ãÔºåËÄåÊòØÂæûÁî± LLM ÁîüÊàêÁöÑÂêàÊàêÊêçÂ£ûÊåá‰ª§‰∏≠Áç≤ÂæóË≤†Èù¢ÂõûÈ•ã„ÄÇ‰ΩøÁî® SuriÔºåÊàëÂÄëÂ∞ç Mistral-7b-Instruct-v0.2 Âü∑Ë°åÁõ£Áù£Âíå I-ORPO ÂæÆË™ø„ÄÇÊúÄÁµÇÊ®°Âûã Suri-SFT Âíå Suri-I-ORPO ÁîüÊàêÁöÑÊñáÂ≠óÊØîÂü∫Êú¨Ê®°ÂûãÈ°ØËëóÊõ¥Èï∑Ôºà~5K ÂÄãÁ¨¶ËôüÔºâÔºåËÄåÂìÅË≥™Ê≤íÊúâÈ°ØËëó‰∏ãÈôç„ÄÇÊàëÂÄëÁöÑ‰∫∫È°ûË©ï‰º∞È°ØÁ§∫ÔºåÈõñÁÑ∂ SFT Âíå I-ORPO Ê®°ÂûãÈÉΩÊªøË∂≥Â§ßÂ§öÊï∏Á¥ÑÊùüÔºå‰ΩÜ Suri-I-ORPO ÁîüÊàêÁöÑÊñáÂ≠óÈÄöÂ∏∏Âõ†ÂÖ∂Áõ∏Âπ≤‰∏îÂÖ∑Ë≥áË®äÊÄßÁöÑÁ¥ÑÊùüÁ¥çÂÖ•ËÄåÂèóÂà∞ÂÅèÂ•Ω„ÄÇÊàëÂÄëÂú® https://github.com/chtmp223/suri/ ÁôºÂ∏ÉÊàëÂÄëÁöÑÁ®ãÂºèÁ¢º„ÄÇ

##### **Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space**
2406.19370v1 by Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, Hidenori Tanaka

Modern generative models demonstrate impressive capabilities, likely stemming
from an ability to identify and manipulate abstract concepts underlying their
training data. However, fundamental questions remain: what determines the
concepts a model learns, the order in which it learns them, and its ability to
manipulate those concepts? To address these questions, we propose analyzing a
model's learning dynamics via a framework we call the concept space, where each
axis represents an independent concept underlying the data generating process.
By characterizing learning dynamics in this space, we identify how the speed at
which a concept is learned, and hence the order of concept learning, is
controlled by properties of the data we term concept signal. Further, we
observe moments of sudden turns in the direction of a model's learning dynamics
in concept space. Surprisingly, these points precisely correspond to the
emergence of hidden capabilities, i.e., where latent interventions show the
model possesses the capability to manipulate a concept, but these capabilities
cannot yet be elicited via naive input prompting. While our results focus on
synthetically defined toy datasets, we hypothesize a general claim on emergence
of hidden capabilities may hold: generative models possess latent capabilities
that emerge suddenly and consistently during training, though a model might not
exhibit these capabilities under naive input prompting.

ÊëòË¶ÅÔºöÁèæ‰ª£ÁîüÊàêÊ®°ÂûãÂ±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåÈÄôÂæàÂèØËÉΩÊòØÂõ†ÁÇ∫ÂÆÉÂÄëËÉΩÂ§†Ë≠òÂà•‰∏¶Êìç‰ΩúÂÖ∂Ë®ìÁ∑¥Ë≥áÊñô‰∏≠ÊΩõÂú®Ê¶ÇÂøµÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºå‰ªçÁÑ∂Â≠òÂú®‰∏Ä‰∫õÂü∫Êú¨ÂïèÈ°åÔºöÊòØ‰ªÄÈ∫ºÊ±∫ÂÆö‰∫ÜÊ®°ÂûãÂ≠∏ÁøíÁöÑÊ¶ÇÂøµ„ÄÅÂ≠∏ÁøíÁöÑÈ†ÜÂ∫èÔºå‰ª•ÂèäÊìçÁ∏±ÈÄô‰∫õÊ¶ÇÂøµÁöÑËÉΩÂäõÔºüÁÇ∫‰∫ÜÂõûÁ≠îÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂª∫Ë≠∞ÈÄèÈÅé‰∏ÄÂÄãÊàëÂÄëÁ®±‰πãÁÇ∫Ê¶ÇÂøµÁ©∫ÈñìÁöÑÊ°ÜÊû∂‰æÜÂàÜÊûêÊ®°ÂûãÁöÑÂ≠∏ÁøíÂãïÊÖãÔºåÂÖ∂‰∏≠ÊØèÂÄãËª∏Á∑ö‰ª£Ë°®Ë≥áÊñôÁî¢ÁîüÈÅéÁ®ã‰∏≠‰∏ÄÂÄãÁç®Á´ãÁöÑÊ¶ÇÂøµ„ÄÇÈÄèÈÅéÊèèËø∞ÈÄôÂÄãÁ©∫Èñì‰∏≠ÁöÑÂ≠∏ÁøíÂãïÊÖãÔºåÊàëÂÄëÂèØ‰ª•ÊâæÂá∫Â≠∏Áøí‰∏ÄÂÄãÊ¶ÇÂøµÁöÑÈÄüÂ∫¶Ôºå‰ª•ÂèäÊ¶ÇÂøµÂ≠∏ÁøíÁöÑÈ†ÜÂ∫èÔºåÊòØÂ¶Ç‰ΩïÂèóÂà∞ÊàëÂÄëÁ®±‰πãÁÇ∫Ê¶ÇÂøµË®äËôüÁöÑË≥áÊñôÂ±¨ÊÄßÊéßÂà∂ÁöÑ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëËßÄÂØüÂà∞Ê®°ÂûãÂú®Ê¶ÇÂøµÁ©∫Èñì‰∏≠Â≠∏ÁøíÂãïÊÖãÊñπÂêëÁ™ÅÁÑ∂ËΩâËÆäÁöÑÊôÇÂàª„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÈÄô‰∫õÈªûÊÅ∞Â•ΩÂ∞çÊáâÊñºÈö±ËóèËÉΩÂäõÁöÑÂá∫ÁèæÔºå‰πüÂ∞±ÊòØÊΩõÂú®‰ªãÂÖ•È°ØÁ§∫Ê®°ÂûãÂÖ∑ÂÇôÊìçÁ∏±Ê¶ÇÂøµÁöÑËÉΩÂäõÔºå‰ΩÜÈÄô‰∫õËÉΩÂäõÈÇÑÁÑ°Ê≥ïÈÄèÈÅéÂñÆÁ¥îÁöÑËº∏ÂÖ•ÊèêÁ§∫ÂºïÁôº„ÄÇÈõñÁÑ∂ÊàëÂÄëÁöÑÁµêÊûúËëóÈáçÊñºÂêàÊàêÂÆöÁæ©ÁöÑÁé©ÂÖ∑Ë≥áÊñôÈõÜÔºå‰ΩÜÊàëÂÄëÂÅáË®≠‰∫Ü‰∏ÄÂÄãÈóúÊñºÈö±ËóèËÉΩÂäõÂá∫ÁèæÁöÑ‰∏ÄËà¨ÊÄß‰∏ªÂºµÂèØËÉΩÊàêÁ´ãÔºöÁîüÊàêÊ®°ÂûãÂÖ∑ÂÇôÊΩõÂú®ËÉΩÂäõÔºåÈÄô‰∫õËÉΩÂäõÂú®Ë®ìÁ∑¥ÊúüÈñìÁ™ÅÁÑ∂‰∏î‰∏ÄËá¥Âú∞Âá∫ÁèæÔºåÂÑòÁÆ°Ê®°ÂûãÂú®ÂñÆÁ¥îÁöÑËº∏ÂÖ•ÊèêÁ§∫‰∏ãÂèØËÉΩ‰∏çÊúÉË°®ÁèæÂá∫ÈÄô‰∫õËÉΩÂäõ„ÄÇ

##### **The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models**
2406.19358v1 by Xiliang Zhu, Shayna Gardiner, Tere Rold√°n, David Rossouw

Sentiment analysis serves as a pivotal component in Natural Language
Processing (NLP). Advancements in multilingual pre-trained models such as XLM-R
and mT5 have contributed to the increasing interest in cross-lingual sentiment
analysis. The recent emergence in Large Language Models (LLM) has significantly
advanced general NLP tasks, however, the capability of such LLMs in
cross-lingual sentiment analysis has not been fully studied. This work
undertakes an empirical analysis to compare the cross-lingual transfer
capability of public Small Multilingual Language Models (SMLM) like XLM-R,
against English-centric LLMs such as Llama-3, in the context of sentiment
analysis across English, Spanish, French and Chinese. Our findings reveal that
among public models, SMLMs exhibit superior zero-shot cross-lingual performance
relative to LLMs. However, in few-shot cross-lingual settings, public LLMs
demonstrate an enhanced adaptive potential. In addition, we observe that
proprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but
are outpaced by public models in few-shot scenarios.

ÊëòË¶ÅÔºöÊÉÖÁ∑íÂàÜÊûêÊòØËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰∏≠ÁöÑÈóúÈçµÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂ§öË™ûË®ÄÈ†êË®ìÁ∑¥Ê®°ÂûãÔºà‰æãÂ¶Ç XLM-R Âíå mT5ÔºâÁöÑÈÄ≤Ê≠•Ôºå‰øÉ‰Ωø‰∫∫ÂÄëÂ∞çË∑®Ë™ûË®ÄÊÉÖÁ∑íÂàÜÊûêÁî¢ÁîüË∂ä‰æÜË∂äÂ§ßÁöÑËààË∂£„ÄÇËøëÊúüÂá∫ÁèæÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â§ßÂπÖÊèêÂçá‰∏ÄËà¨ NLP ‰ªªÂãôÔºåÁÑ∂ËÄåÔºåÊ≠§È°û LLM Âú®Ë∑®Ë™ûË®ÄÊÉÖÁ∑íÂàÜÊûê‰∏≠ÁöÑËÉΩÂäõÂ∞öÊú™Áç≤ÂæóÂÖÖÂàÜÁ†îÁ©∂„ÄÇÊú¨Á†îÁ©∂ÈÄ≤Ë°åÂØ¶Ë≠âÂàÜÊûêÔºå‰ª•ÊØîËºÉÂÖ¨ÈñãÁöÑÂ∞èÂûãÂ§öË™ûË®ÄË™ûË®ÄÊ®°Âûã (SMLM)Ôºà‰æãÂ¶Ç XLM-RÔºâËàá‰ª•Ëã±Ë™ûÁÇ∫‰∏≠ÂøÉÁöÑ LLMÔºà‰æãÂ¶Ç Llama-3ÔºâÂú®Ëã±Ë™û„ÄÅË•øÁè≠ÁâôË™û„ÄÅÊ≥ïË™ûÂíå‰∏≠ÊñáÁöÑÊÉÖÁ∑íÂàÜÊûê‰∏≠ÁöÑË∑®Ë™ûË®ÄËΩâÁßªËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÂú®ÂÖ¨ÈñãÊ®°Âûã‰∏≠ÔºåSMLM Áõ∏ËºÉÊñº LLMÔºåÂ±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÈõ∂Ê¨°Â≠∏ÁøíË∑®Ë™ûË®ÄÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂú®Â∞ëÊ¨°Â≠∏ÁøíÁöÑË∑®Ë™ûË®ÄË®≠ÂÆö‰∏≠ÔºåÂÖ¨Èñã LLM Â±ïÁèæÂá∫Â¢ûÂº∑ÁöÑÈÅ©ÊáâÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëËßÄÂØüÂà∞Â∞àÊúâÁöÑ GPT-3.5 Âíå GPT-4 Âú®Èõ∂Ê¨°Â≠∏ÁøíË∑®Ë™ûË®ÄËÉΩÂäõ‰∏≠È†òÂÖàÔºå‰ΩÜÂú®Â∞ëÊ¨°Â≠∏ÁøíÂ†¥ÊôØ‰∏≠ÂçªËêΩÂæåÊñºÂÖ¨ÈñãÊ®°Âûã„ÄÇ

##### **DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions**
2406.19356v1 by Nigel Fernandez, Alexander Scarlatos, Simon Woodhead, Andrew Lan

High-quality distractors are crucial to both the assessment and pedagogical
value of multiple-choice questions (MCQs), where manually crafting ones that
anticipate knowledge deficiencies or misconceptions among real students is
difficult. Meanwhile, automated distractor generation, even with the help of
large language models (LLMs), remains challenging for subjects like math. It is
crucial to not only identify plausible distractors but also understand the
error behind them. In this paper, we introduce DiVERT (Distractor Generation
with Variational Errors Represented as Text), a novel variational approach that
learns an interpretable representation of errors behind distractors in math
MCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions
used by hundreds of thousands of students, we show that DiVERT, despite using a
base open-source LLM with 7B parameters, outperforms state-of-the-art
approaches using GPT-4o on downstream distractor generation. We also conduct a
human evaluation with math educators and find that DiVERT leads to error labels
that are of comparable quality to human-authored ones.

ÊëòË¶ÅÔºöÂÑ™Ë≥™ÁöÑÂπ≤ÊìæÈÅ∏È†ÖÂ∞çÊñºÂ§öÈÅ∏È°å (MCQ) ÁöÑË©ïÈáèÂíåÊïôÂ≠∏ÂÉπÂÄºËá≥ÈóúÈáçË¶ÅÔºåËÄåÊâãÂãïË£Ω‰ΩúËÉΩÈ†êÊúüÂà∞ÁúüÂØ¶Â≠∏ÁîüÁü•Ë≠ò‰∏çË∂≥ÊàñËßÄÂøµÈåØË™§ÁöÑÈÅ∏È†ÖÂæàÂõ∞Èõ£„ÄÇÂêåÊôÇÔºåËá™ÂãïÂåñÂπ≤ÊìæÈÅ∏È†ÖÁîüÊàêÔºåÂç≥‰ΩøÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂçîÂä©‰∏ãÔºåÂ∞çÊñºÊï∏Â≠∏Á≠âÁßëÁõÆËÄåË®Ä‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇ‰∏çÂÉÖË¶ÅÊâæÂá∫ÂêàÁêÜÁöÑÂπ≤ÊìæÈÅ∏È†ÖÔºåÈÇÑË¶Å‰∫ÜËß£ÂÖ∂ËÉåÂæåÁöÑÈåØË™§Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π DiVERTÔºà‰ª•ÊñáÂ≠óË°®Á§∫ÁöÑËÆäÁï∞ÈåØË™§Âπ≤ÊìæÈÅ∏È†ÖÁîüÊàêÔºâÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑËÆäÁï∞ÊñπÊ≥ïÔºåÂèØ‰ª•Â≠∏ÁøíÊï∏Â≠∏Â§öÈÅ∏È°å‰∏≠Âπ≤ÊìæÈÅ∏È†ÖËÉåÂæåÈåØË™§ÁöÑÂèØË©ÆÈáãË°®Á§∫„ÄÇÈÄèÈÅéÂ∞ç 1,434 ÂÄãÂïèÈ°åÁöÑÁúüÂØ¶‰∏ñÁïåÊï∏Â≠∏Â§öÈÅ∏È°åË≥áÊñôÈõÜÈÄ≤Ë°åÂØ¶È©óÔºåÊï∏ÂçÅËê¨ÂêçÂ≠∏Áîü‰ΩøÁî®ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü DiVERTÔºåÂÑòÁÆ°‰ΩøÁî®Â∏∂Êúâ 7B ÂèÉÊï∏ÁöÑÂü∫Êú¨ÈñãÊ∫ê LLMÔºå‰ΩÜÂú®‰∏ãÊ∏∏Âπ≤ÊìæÈÅ∏È†ÖÁîüÊàê‰∏≠ÂÑ™Êñº‰ΩøÁî® GPT-4o ÁöÑÊúÄÂÖàÈÄ≤ÊñπÊ≥ï„ÄÇÊàëÂÄëÈÇÑËàáÊï∏Â≠∏ÊïôËÇ≤Â∑•‰ΩúËÄÖÈÄ≤Ë°å‰∫Ü‰∫∫Â∑•Ë©ïÈáèÔºåÁôºÁèæ DiVERT Áî¢ÁîüÁöÑÈåØË™§Ê®ôÁ±§Ëàá‰∫∫Â∑•Á∑®ÂØ´ÁöÑÊ®ôÁ±§ÂìÅË≥™Áõ∏Áï∂„ÄÇ

##### **Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?**
2406.19354v1 by Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal

The model editing problem concerns how language models should learn new facts
about the world over time. While empirical research on model editing has drawn
widespread attention, the conceptual foundations of model editing remain shaky
-- perhaps unsurprisingly, since model editing is essentially belief revision,
a storied problem in philosophy that has eluded succinct solutions for decades.
Model editing nonetheless demands a solution, since we need to be able to
control the knowledge within language models. With this goal in mind, this
paper critiques the standard formulation of the model editing problem and
proposes a formal testbed for model editing research. We first describe 12 open
problems with model editing, based on challenges with (1) defining the problem,
(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the
first place. Many of these challenges are extremely difficult to address, e.g.
determining far-reaching consequences of edits, labeling probabilistic
entailments between facts, and updating beliefs of agent simulators. Next, we
introduce a semi-synthetic dataset for model editing based on Wikidata, where
we can evaluate edits against labels given by an idealized Bayesian agent. This
enables us to say exactly how belief revision in language models falls short of
a desirable epistemic standard. We encourage further research exploring
settings where such a gold standard can be compared against. Our code is
publicly available at: https://github.com/peterbhase/LLM-belief-revision

ÊëòË¶ÅÔºöÊ®°ÂûãÁ∑®ËºØÂïèÈ°åÊé¢Ë®éË™ûË®ÄÊ®°ÂûãÊáâÂ¶Ç‰ΩïÈö®ËëóÊôÇÈñìÊé®ÁßªÂ≠∏ÁøíÊúâÈóú‰∏ñÁïåÁöÑÊñ∞‰∫ãÂØ¶„ÄÇÂÑòÁÆ°Ê®°ÂûãÁ∑®ËºØÁöÑÂØ¶Ë≠âÁ†îÁ©∂Â∑≤ÂºïËµ∑Âª£Ê≥õÈóúÊ≥®Ôºå‰ΩÜÊ®°ÂûãÁ∑®ËºØÁöÑÊ¶ÇÂøµÂü∫Á§é‰ªçÁÑ∂‰∏çÁ©©Âõ∫‚Äî‚ÄîÈÄôÊàñË®±‰∏çË∂≥ÁÇ∫Â•áÔºåÂõ†ÁÇ∫Ê®°ÂûãÁ∑®ËºØÊú¨Ë≥™‰∏äÊòØ‰ø°Âøµ‰øÆÊ≠£ÔºåÈÄôÊòØ‰∏ÄÂÄãÂì≤Â≠∏‰∏≠ÁöÑËÄÅÁîüÂ∏∏Ë´áÁöÑÂïèÈ°åÔºåÊï∏ÂçÅÂπ¥‰æÜ‰∏ÄÁõ¥Ê≤íÊúâÁ∞°ÊΩîÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÊ®°ÂûãÁ∑®ËºØÈÇÑÊòØÈúÄË¶ÅËß£Ê±∫ÊñπÊ°àÔºåÂõ†ÁÇ∫ÊàëÂÄëÈúÄË¶ÅËÉΩÂ§†ÊéßÂà∂Ë™ûË®ÄÊ®°Âûã‰∏≠ÁöÑÁü•Ë≠ò„ÄÇÊú¨ËëóÈÄô‰∏ÄÁõÆÊ®ôÔºåÊú¨ÊñáÊâπÂà§‰∫ÜÊ®°ÂûãÁ∑®ËºØÂïèÈ°åÁöÑÊ®ôÊ∫ñË°®Ëø∞Ôºå‰∏¶ÊèêÂá∫‰∫ÜÊ®°ÂûãÁ∑®ËºØÁ†îÁ©∂ÁöÑÊ≠£ÂºèÊ∏¨Ë©¶Âπ≥Âè∞„ÄÇÊàëÂÄëÈ¶ñÂÖàÊèèËø∞‰∫ÜÊ®°ÂûãÁ∑®ËºØÁöÑ 12 ÂÄãÈñãÊîæÂïèÈ°åÔºåÈÄô‰∫õÂïèÈ°åÂü∫Êñº‰ª•‰∏ãÊåëÊà∞Ôºö(1) ÂÆöÁæ©ÂïèÈ°åÔºå(2) ÈñãÁôºÂü∫Ê∫ñÔºå‰ª•Âèä (3) ÂÅáË®≠ LLM È¶ñÂÖàÂÖ∑ÊúâÂèØÁ∑®ËºØÁöÑ‰ø°Âøµ„ÄÇÂÖ∂‰∏≠Ë®±Â§öÊåëÊà∞Ê•µÈõ£Ëß£Ê±∫Ôºå‰æãÂ¶ÇÁ¢∫ÂÆöÁ∑®ËºØÁöÑÊ∑±ÈÅ†ÂæåÊûú„ÄÅÊ®ôË®ò‰∫ãÂØ¶‰πãÈñìÁöÑÊ¶ÇÁéáËòäÊ∂µÔºå‰ª•ÂèäÊõ¥Êñ∞‰ª£ÁêÜÊ®°Êì¨Âô®ÁöÑ‰ø°Âøµ„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÂü∫Êñº Wikidata ‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÁî®ÊñºÊ®°ÂûãÁ∑®ËºØÁöÑÂçäÂêàÊàêÊï∏ÊìöÈõÜÔºåÂú®ÂÖ∂‰∏≠ÊàëÂÄëÂèØ‰ª•Ê†πÊìöÁêÜÊÉ≥Ë≤ùËëâÊñØ‰ª£ÁêÜÁµ¶Âá∫ÁöÑÊ®ôÁ±§‰æÜË©ï‰º∞Á∑®ËºØ„ÄÇÈÄô‰ΩøÊàëÂÄëËÉΩÂ§†Á¢∫ÂàáÂú∞Ë™™ÊòéË™ûË®ÄÊ®°Âûã‰∏≠ÁöÑ‰ø°Âøµ‰øÆÊ≠£Â¶Ç‰ΩïÈÅî‰∏çÂà∞ÁêÜÊÉ≥ÁöÑË™çË≠òË´ñÊ®ôÊ∫ñ„ÄÇÊàëÂÄëÈºìÂãµÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂Êé¢Á¥¢ÂèØ‰ª•Â∞áÈÄôÁ®ÆÈªÉÈáëÊ®ôÊ∫ñËàá‰πãÈÄ≤Ë°åÊØîËºÉÁöÑÊÉÖÊ≥Å„ÄÇÊàëÂÄëÁöÑ‰ª£Á¢ºÂ∑≤ÂÖ¨ÈñãÁôºÂ∏ÉÂú®Ôºöhttps://github.com/peterbhase/LLM-belief-revision

##### **IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language**
2406.19349v1 by Lucky Susanto, Musa Izzanardi Wijanarko, Prasetia Anugrah Pratama, Traci Hong, Ika Idris, Alham Fikri Aji, Derry Wijaya

Hate speech poses a significant threat to social harmony. Over the past two
years, Indonesia has seen a ten-fold increase in the online hate speech ratio,
underscoring the urgent need for effective detection mechanisms. However,
progress is hindered by the limited availability of labeled data for Indonesian
texts. The condition is even worse for marginalized minorities, such as Shia,
LGBTQ, and other ethnic minorities because hate speech is underreported and
less understood by detection tools. Furthermore, the lack of accommodation for
subjectivity in current datasets compounds this issue. To address this, we
introduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity
classification dataset. Comprising 43,692 entries annotated by 19 diverse
individuals, the dataset focuses on texts targeting vulnerable groups in
Indonesia, specifically during the hottest political event in the country: the
presidential election. We establish baselines for seven binary classification
tasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)
fine-tuned for hate speech classification. Furthermore, we demonstrate how
incorporating demographic information can enhance the zero-shot performance of
the large language model, gpt-3.5-turbo. However, we also caution that an
overemphasis on demographic information can negatively impact the fine-tuned
model performance due to data fragmentation.

ÊëòË¶ÅÔºö‰ªáÊÅ®Ë®ÄË´ñÂ∞çÁ§æÊúÉÂíåË´ßÊßãÊàêÈáçÂ§ßÂ®ÅËÑÖ„ÄÇÂú®ÈÅéÂéªÂÖ©Âπ¥ÔºåÂç∞Â∞ºÁöÑÁ∑ö‰∏ä‰ªáÊÅ®Ë®ÄË´ñÊØî‰æãÂ¢ûÂä†‰∫ÜÂçÅÂÄçÔºåÈÄôÂá∏È°Ø‰∫ÜÂ∞çÊúâÊïàÂÅµÊ∏¨Ê©üÂà∂ÁöÑËø´ÂàáÈúÄË¶Å„ÄÇÁÑ∂ËÄåÔºåÂç∞Â∞ºË™ûÊñáÊú¨Ê®ôË®òË≥áÊñôÁöÑÊúâÈôêÂèñÂæóÊÄßÈòªÁ§ô‰∫ÜÈÄ≤Â∫¶„ÄÇÂ∞çÊñºÂ∞ëÊï∏ÊóèÁæ§Ôºå‰æãÂ¶Ç‰ªÄËëâÊ¥æ„ÄÅLGBTQ ÂíåÂÖ∂‰ªñÂ∞ëÊï∏Ê∞ëÊóèÔºåÊÉÖÊ≥ÅÊõ¥Á≥üÔºåÂõ†ÁÇ∫‰ªáÊÅ®Ë®ÄË´ñÁöÑÈÄöÂ†±‰∏çË∂≥Ôºå‰∏îÂÅµÊ∏¨Â∑•ÂÖ∑Â∞çÂÖ∂‰∫ÜËß£ËºÉÂ∞ë„ÄÇÊ≠§Â§ñÔºåÁèæÊúâË≥áÊñôÈõÜÁº∫‰πèÂ∞ç‰∏ªËßÄÊÄßÁöÑËÄÉÈáèÔºå‰ΩøÈÄôÂÄãÂïèÈ°åÊõ¥Âä†Ë§áÈõú„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü IndoToxic2024ÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂç∞Â∞ºË™û‰ªáÊÅ®Ë®ÄË´ñÂíåÊØíÊÄßÂàÜÈ°ûË≥áÊñôÈõÜ„ÄÇË≥áÊñôÈõÜÂåÖÂê´Áî± 19 ‰Ωç‰∏çÂêåÂÄã‰∫∫Ê®ôË®òÁöÑ 43,692 ÂÄãÊ¢ùÁõÆÔºåÈáçÈªûÂú®ÊñºÈáùÂ∞çÂç∞Â∞ºÂº±Âã¢Áæ§È´îÁöÑÊñáÊú¨ÔºåÁâπÂà•ÊòØÂú®Ë©≤ÂúãÊúÄÁÜ±ÈñÄÁöÑÊîøÊ≤ªÊ¥ªÂãïÔºöÁ∏ΩÁµ±ÈÅ∏ËàâÊúüÈñì„ÄÇÊàëÂÄëÁÇ∫‰∏ÉÈ†Ö‰∫åÂÖÉÂàÜÈ°û‰ªªÂãôÂª∫Á´ãÂü∫Ê∫ñÔºå‰ΩøÁî®ÈáùÂ∞ç‰ªáÊÅ®Ë®ÄË´ñÂàÜÈ°ûÂæÆË™øÁöÑ BERT Ê®°ÂûãÔºàIndoBERTweetÔºâÔºåÈÅîÂà∞‰∫Ü 0.78 ÁöÑÂ∑®ËßÄ F1 ÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÄèÈÅéÁ¥çÂÖ•‰∫∫Âè£Áµ±Ë®àË≥áË®ä‰æÜÂ¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°Âûã gpt-3.5-turbo ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄë‰πüÊèêÈÜíÔºåÈÅéÂ∫¶Âº∑Ë™ø‰∫∫Âè£Áµ±Ë®àË≥áË®äÂèØËÉΩÊúÉÂõ†ÁÇ∫Ë≥áÊñôÂàÜÊï£ËÄåÂ∞çÂæÆË™øÊ®°ÂûãÊïàËÉΩÈÄ†ÊàêË≤†Èù¢ÂΩ±Èüø„ÄÇ

##### **Efficient World Models with Context-Aware Tokenization**
2406.19320v1 by Vincent Micheli, Eloi Alonso, Fran√ßois Fleuret

Scaling up deep Reinforcement Learning (RL) methods presents a significant
challenge. Following developments in generative modelling, model-based RL
positions itself as a strong contender. Recent advances in sequence modelling
have led to effective transformer-based world models, albeit at the price of
heavy computations due to the long sequences of tokens required to accurately
simulate environments. In this work, we propose $\Delta$-IRIS, a new agent with
a world model architecture composed of a discrete autoencoder that encodes
stochastic deltas between time steps and an autoregressive transformer that
predicts future deltas by summarizing the current state of the world with
continuous tokens. In the Crafter benchmark, $\Delta$-IRIS sets a new state of
the art at multiple frame budgets, while being an order of magnitude faster to
train than previous attention-based approaches. We release our code and models
at https://github.com/vmicheli/delta-iris.

ÊëòË¶ÅÔºöÊì¥Â±ïÊ∑±Â∫¶Âº∑ÂåñÂ≠∏Áøí (RL) ÊñπÊ≥ïÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÂú®ÁîüÊàêÂºèÂª∫Ê®°ÁöÑÁôºÂ±ï‰πãÂæåÔºåÂü∫ÊñºÊ®°ÂûãÁöÑ RL Â∞áËá™Ë∫´ÂÆö‰ΩçÁÇ∫Âº∑ÊúâÂäõÁöÑÁ´∂Áà≠ËÄÖ„ÄÇÂ∫èÂàóÂª∫Ê®°ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ∞éËá¥‰∫ÜÊúâÊïàÁöÑÂü∫ÊñºËΩâÊèõÂô®ÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåÂÑòÁÆ°Áî±ÊñºÊ∫ñÁ¢∫Ê®°Êì¨Áí∞Â¢ÉÊâÄÈúÄÁöÑÈï∑Â∫èÂàó‰ª£Âπ£ÔºåËÄå‰ªòÂá∫‰∫ÜÂ§ßÈáèË®àÁÆóÁöÑ‰ª£ÂÉπ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü $\Delta$-IRISÔºå‰∏ÄÁ®ÆÊñ∞ÁöÑ‰ª£ÁêÜÔºåÂÖ∂‰∏ñÁïåÊ®°ÂûãÊû∂ÊßãÁî±Èõ¢Êï£Ëá™ÂãïÁ∑®Á¢ºÂô®ÁµÑÊàêÔºåË©≤Á∑®Á¢ºÂô®Á∑®Á¢ºÊôÇÈñìÊ≠•Èï∑‰πãÈñìÁöÑÈö®Ê©üÂ¢ûÈáèÔºå‰ª•Âèä‰∏ÄÂÄãËá™Ëø¥Ê≠∏ËΩâÊèõÂô®ÔºåË©≤ËΩâÊèõÂô®ÈÄöÈÅé‰ΩøÁî®ÈÄ£Á∫å‰ª£Âπ£Á∏ΩÁµê‰∏ñÁïåÁöÑÁï∂ÂâçÁãÄÊÖã‰æÜÈ†êÊ∏¨Êú™‰æÜÁöÑÂ¢ûÈáè„ÄÇÂú® Crafter Âü∫Ê∫ñÊ∏¨Ë©¶‰∏≠Ôºå$\Delta$-IRIS Âú®Â§öÂÄãÂπÄÈ†êÁÆó‰∏≠Ë®≠ÂÆö‰∫ÜÊñ∞ÁöÑÊäÄË°ìÊ∞¥Ê∫ñÔºåÂêåÊôÇÊØî‰πãÂâçÁöÑÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÊñπÊ≥ïË®ìÁ∑¥ÈÄüÂ∫¶Âø´‰∏ÄÂÄãÊï∏ÈáèÁ¥ö„ÄÇÊàëÂÄëÂú® https://github.com/vmicheli/delta-iris ‰∏äÁôºÂ∏ÉÊàëÂÄëÁöÑ‰ª£Á¢ºÂíåÊ®°Âûã„ÄÇ

##### **Jump Starting Bandits with LLM-Generated Prior Knowledge**
2406.19317v1 by Parand A. Alamdari, Yanshuai Cao, Kevin H. Wilson

We present substantial evidence demonstrating the benefits of integrating
Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.
Contextual bandits have been widely used in recommendation systems to generate
personalized suggestions based on user-specific contexts. We show that LLMs,
pre-trained on extensive corpora rich in human knowledge and preferences, can
simulate human behaviours well enough to jump-start contextual multi-armed
bandits to reduce online learning regret. We propose an initialization
algorithm for contextual bandits by prompting LLMs to produce a pre-training
dataset of approximate human preferences for the bandit. This significantly
reduces online learning regret and data-gathering costs for training such
models. Our approach is validated empirically through two sets of experiments
with different bandit setups: one which utilizes LLMs to serve as an oracle and
a real-world experiment utilizing data from a conjoint survey experiment.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫Â§ßÈáèÁöÑË≠âÊìöÔºåË≠âÊòéÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÊÉÖÂ¢ÉÂ§öËáÇËÄÅËôéÊ©üÊû∂ÊßãÊï¥ÂêàËµ∑‰æÜÁöÑÂ•ΩËôï„ÄÇÊÉÖÂ¢ÉÂ§öËáÇËÄÅËôéÊ©üÂ∑≤Âª£Ê≥õÁî®ÊñºÊé®Ëñ¶Á≥ªÁµ±‰∏≠Ôºå‰ª•Ê†πÊìö‰ΩøÁî®ËÄÖÁâπÂÆöÁöÑÊÉÖÂ¢ÉÁî¢ÁîüÂÄã‰∫∫ÂåñÁöÑÂª∫Ë≠∞„ÄÇÊàëÂÄëË≠âÊòéÔºåÂú®Ë±êÂØåÁöÑ‰∫∫È°ûÁü•Ë≠òÂíåÂÅèÂ•ΩÁöÑÂª£Ê≥õË™ûÊñôÂ∫´‰∏äÈ†êÂÖàË®ìÁ∑¥ÁöÑ LLMÔºåËÉΩÂ§†ÂæàÂ•ΩÂú∞Ê®°Êì¨‰∫∫È°ûË°åÁÇ∫Ôºå‰ª•ÂïüÂãïÊÉÖÂ¢ÉÂ§öËáÇËÄÅËôéÊ©üÔºåÊ∏õÂ∞ëÁ∑ö‰∏äÂ≠∏ÁøíÈÅ∫ÊÜæ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊÉÖÂ¢ÉÂ§öËáÇËÄÅËôéÊ©üÁöÑÂàùÂßãÂåñÊºîÁÆóÊ≥ïÔºåÈÄèÈÅéÊèêÁ§∫ LLM Áî¢ÁîüËøë‰ºº‰∫∫È°ûÂÅèÂ•ΩÁöÑÈ†êË®ìÁ∑¥Ë≥áÊñôÈõÜÔºå‰æõËÄÅËôéÊ©ü‰ΩøÁî®„ÄÇÈÄôÈ°ØËëóÊ∏õÂ∞ë‰∫ÜÊ≠§È°ûÊ®°ÂûãÁöÑÁ∑ö‰∏äÂ≠∏ÁøíÈÅ∫ÊÜæÂíåË≥áÊñôÊî∂ÈõÜÊàêÊú¨„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈÄèÈÅéÂÖ©ÁµÑÂÖ∑Êúâ‰∏çÂêåËÄÅËôéÊ©üË®≠ÂÆöÁöÑÂØ¶È©óÈÄ≤Ë°åÂØ¶Ë≠âÈ©óË≠âÔºö‰∏ÄÁµÑÂà©Áî® LLM ‰ΩúÁÇ∫Á•ûË´≠ÔºåÂè¶‰∏ÄÁµÑÂØ¶ÈöõÂØ¶È©óÂà©Áî®ËÅØÂêàË™øÊü•ÂØ¶È©óÁöÑË≥áÊñô„ÄÇ

##### **LiveBench: A Challenging, Contamination-Free LLM Benchmark**
2406.19314v1 by Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, Micah Goldblum

Test set contamination, wherein test data from a benchmark ends up in a newer
model's training set, is a well-documented obstacle for fair LLM evaluation and
can quickly render benchmarks obsolete. To mitigate this, many recent
benchmarks crowdsource new prompts and evaluations from human or LLM judges;
however, these can introduce significant biases, and break down when scoring
hard questions. In this work, we introduce a new benchmark for LLMs designed to
be immune to both test set contamination and the pitfalls of LLM judging and
human crowdsourcing. We release LiveBench, the first benchmark that (1)
contains frequently-updated questions from recent information sources, (2)
scores answers automatically according to objective ground-truth values, and
(3) contains a wide variety of challenging tasks, spanning math, coding,
reasoning, language, instruction following, and data analysis. To achieve this,
LiveBench contains questions that are based on recently-released math
competitions, arXiv papers, news articles, and datasets, and it contains
harder, contamination-free versions of tasks from previous benchmarks such as
Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source
models, as well as dozens of open-source models ranging from 0.5B to 110B in
size. LiveBench is difficult, with top models achieving below 65% accuracy. We
release all questions, code, and model answers. Questions will be added and
updated on a monthly basis, and we will release new tasks and harder versions
of tasks over time so that LiveBench can distinguish between the capabilities
of LLMs as they improve in the future. We welcome community engagement and
collaboration for expanding the benchmark tasks and models.

ÊëòË¶ÅÔºöÊ∏¨Ë©¶ÈõÜÊ±°ÊüìÔºåÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÁöÑÊ∏¨Ë©¶Ë≥áÊñôÊúÄÁµÇÂá∫ÁèæÂú®ËºÉÊñ∞ÁöÑÊ®°ÂûãË®ìÁ∑¥ÈõÜ‰∏≠ÔºåÊòØÂÖ¨Âπ≥ LLM Ë©ï‰º∞ÁöÑÁúæÊâÄÂë®Áü•ÈöúÁ§ôÔºå‰∏îËÉΩÂø´ÈÄü‰ΩøÂü∫Ê∫ñÊ∏¨Ë©¶ÈÅéÊôÇ„ÄÇÁÇ∫‰∫ÜÊ∏õËºïÈÄô‰∏ÄÈªûÔºåË®±Â§öÊúÄËøëÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶Âæû‰∫∫È°ûÊàñ LLM Ë©ïÂØ©‰∏≠Áæ§ÁúæÂ§ñÂåÖÊñ∞ÁöÑÊèêÁ§∫ÂíåË©ï‰º∞ÔºõÁÑ∂ËÄåÔºåÈÄô‰∫õÂèØËÉΩÊúÉÂºïÂÖ•È°ØËëóÁöÑÂÅèË¶ãÔºå‰∏îÂú®Ë©ïÂàÜÂõ∞Èõ£ÁöÑÂïèÈ°åÊôÇÊúÉÂ¥©ÊΩ∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÁÇ∫ LLM ‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÊó®Âú®Â∞çÊ∏¨Ë©¶ÈõÜÊ±°ÊüìÂíå LLM Ë©ïÂØ©Âíå‰∫∫È°ûÁæ§ÁúæÂ§ñÂåÖÁöÑÈô∑Èò±ÂÖçÁñ´„ÄÇÊàëÂÄëÁôºÂ∏É LiveBenchÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄã (1) ÂåÖÂê´‰æÜËá™ËøëÊúüË≥áË®ä‰æÜÊ∫êÁöÑÈ†ªÁπÅÊõ¥Êñ∞ÂïèÈ°å„ÄÅ(2) Ê†πÊìöÂÆ¢ËßÄÂü∫Êú¨‰∫ãÂØ¶ÂÄºËá™ÂãïË©ïÂàÜÁ≠îÊ°àÔºå‰ª•Âèä (3) ÂåÖÂê´ÂêÑÁ®ÆÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãôÔºåÊ∂µËìãÊï∏Â≠∏„ÄÅÁ∑®Á¢º„ÄÅÊé®ÁêÜ„ÄÅË™ûË®Ä„ÄÅÈÅµÂæ™ÊåáÁ§∫ÂíåË≥áÊñôÂàÜÊûêÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶„ÄÇÁÇ∫‰∫ÜÈÅîÊàêÈÄô‰∏ÄÈªûÔºåLiveBench ÂåÖÂê´Âü∫ÊñºÊúÄËøëÁôºÂ∏ÉÁöÑÊï∏Â≠∏Á´∂Ë≥Ω„ÄÅarXiv Ë´ñÊñá„ÄÅÊñ∞ËÅûÊñáÁ´†ÂíåË≥áÊñôÈõÜÁöÑÂïèÈ°åÔºå‰∏¶‰∏îÂÆÉÂåÖÂê´‰æÜËá™ÂÖàÂâçÂü∫Ê∫ñÊ∏¨Ë©¶Ôºà‰æãÂ¶Ç Big-Bench Hard„ÄÅAMPS Âíå IFEvalÔºâÁöÑÊõ¥Èõ£„ÄÅÁÑ°Ê±°ÊüìÁöÑ‰ªªÂãôÁâàÊú¨„ÄÇÊàëÂÄëË©ï‰º∞‰∫ÜË®±Â§öËëóÂêçÁöÑÈñâÊ∫êÊ®°ÂûãÔºå‰ª•ÂèäÊï∏ÂçÅÂÄãÂ§ßÂ∞èÂæû 0.5B Âà∞ 110B ÁöÑÈñãÊ∫êÊ®°Âûã„ÄÇLiveBench ÂæàÂõ∞Èõ£ÔºåÈ†ÇÂ∞ñÊ®°ÂûãÁöÑÊ∫ñÁ¢∫Áéá‰ΩéÊñº 65%„ÄÇÊàëÂÄëÁôºÂ∏ÉÊâÄÊúâÂïèÈ°å„ÄÅÁ®ãÂºèÁ¢ºÂíåÊ®°ÂûãÁ≠îÊ°à„ÄÇÂïèÈ°åÂ∞áÊåâÊúàÊñ∞Â¢ûÂíåÊõ¥Êñ∞ÔºåÊàëÂÄëÂ∞áÈö®ËëóÊôÇÈñìÊé®ÁßªÁôºÂ∏ÉÊñ∞ÁöÑ‰ªªÂãôÂíåÊõ¥Âõ∞Èõ£ÁöÑ‰ªªÂãôÁâàÊú¨Ôºå‰ª•‰æø LiveBench ËÉΩÂçÄÂàÜ LLM Âú®Êú™‰æÜÊîπÈÄ≤ÊôÇÁöÑÂêÑÁ®ÆÂäüËÉΩ„ÄÇÊàëÂÄëÊ≠°ËøéÁ§æÁæ§ÂèÉËàáÂíåÂêà‰ΩúÔºå‰ª•Êì¥Â±ïÂü∫Ê∫ñÊ∏¨Ë©¶‰ªªÂãôÂíåÊ®°Âûã„ÄÇ

##### **From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data**
2406.19292v1 by Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, Dimitris Papailiopoulos

Recent studies have shown that Large Language Models (LLMs) struggle to
accurately retrieve information and maintain reasoning capabilities when
processing long-context inputs. To address these limitations, we propose a
finetuning approach utilizing a carefully designed synthetic dataset comprising
numerical key-value retrieval tasks. Our experiments on models like GPT-3.5
Turbo and Mistral 7B demonstrate that finetuning LLMs on this dataset
significantly improves LLMs' information retrieval and reasoning capabilities
in longer-context settings. We present an analysis of the finetuned models,
illustrating the transfer of skills from synthetic to real task evaluations
(e.g., $10.5\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5
Turbo). We also find that finetuned LLMs' performance on general benchmarks
remains almost constant while LLMs finetuned on other baseline long-context
augmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B
finetuned on our synthetic data cause no performance drop while other baseline
data can cause a drop that ranges from $2.33\%$ to $6.19\%$). Our study
highlights the potential of finetuning on synthetic data for improving the
performance of LLMs on longer-context tasks.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®Â§ÑÁêÜÈïøÊñáÊú¨ËæìÂÖ•Êó∂Èöæ‰ª•ÂáÜÁ°ÆÂú∞Ê£ÄÁ¥¢‰ø°ÊÅØÂπ∂Áª¥ÊåÅÊé®ÁêÜËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈôêÂà∂ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂæÆË∞ÉÊñπÊ≥ïÔºåÂà©Áî®Á≤æÂøÉËÆæËÆ°ÁöÑÂêàÊàêÊï∞ÊçÆÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´Êï∞Â≠óÈîÆÂÄºÊ£ÄÁ¥¢‰ªªÂä°„ÄÇÊàë‰ª¨ÂØπ GPT-3.5 Turbo Âíå Mistral 7B Á≠âÊ®°ÂûãËøõË°åÁöÑÂÆûÈ™åË°®ÊòéÔºåÂú®ËØ•Êï∞ÊçÆÈõÜ‰∏äÂØπ LLM ËøõË°åÂæÆË∞ÉÂèØ‰ª•ÊòæÁùÄÊèêÈ´ò LLM Âú®ËæÉÈïøÊñáÊú¨ÁéØÂ¢É‰∏≠ÁöÑ‰ø°ÊÅØÊ£ÄÁ¥¢ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÂØπÂæÆË∞ÉÂêéÁöÑÊ®°ÂûãËøõË°å‰∫ÜÂàÜÊûêÔºåËØ¥Êòé‰∫Ü‰ªéÂêàÊàê‰ªªÂä°Âà∞ÁúüÂÆû‰ªªÂä°ËØÑ‰º∞ÁöÑÊäÄËÉΩËΩ¨ÁßªÔºà‰æãÂ¶ÇÔºåÂú® GPT-3.5 Turbo ÁöÑ 20 ‰∏™ÊñáÊ°£ MDQA ‰∏≠ÔºåÂú®‰ΩçÁΩÆ 10 ‰∏äÊèêÈ´ò‰∫Ü 10.5%Ôºâ„ÄÇÊàë‰ª¨ËøòÂèëÁé∞ÔºåÂæÆË∞ÉÂêéÁöÑ LLM Âú®‰∏ÄËà¨Âü∫ÂáÜ‰∏äÁöÑÊÄßËÉΩÂá†‰πé‰øùÊåÅ‰∏çÂèòÔºåËÄå‰ΩøÁî®ÂÖ∂‰ªñÂü∫Á∫øÈïøÊñáÊú¨Â¢ûÂº∫Êï∞ÊçÆÂØπ LLM ËøõË°åÂæÆË∞ÉÂèØËÉΩ‰ºöÂØºËá¥Âá∫Áé∞ÂπªËßâÔºà‰æãÂ¶ÇÔºåÂú® TriviaQA ‰∏äÔºåÂú®Êàë‰ª¨ÁöÑÂêàÊàêÊï∞ÊçÆ‰∏äÂæÆË∞ÉÁöÑ Mistral 7B ‰∏ç‰ºöÂØºËá¥ÊÄßËÉΩ‰∏ãÈôçÔºåËÄåÂÖ∂‰ªñÂü∫Á∫øÊï∞ÊçÆ‰ºöÂØºËá¥‰∏ãÈôç 2.33% Ëá≥ 6.19%Ôºâ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Âº∫Ë∞É‰∫ÜÂú®ÂêàÊàêÊï∞ÊçÆ‰∏äËøõË°åÂæÆË∞É‰ª•ÊèêÈ´ò LLM Âú®ËæÉÈïøÊñáÊú¨‰ªªÂä°‰∏äÁöÑÊÄßËÉΩÁöÑÊΩúÂäõ„ÄÇ</paragraph>

##### **HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale**
2406.19280v1 by Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang

The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÂø´ÈÄüÂèëÂ±ïÔºå‰æãÂ¶Ç GPT-4VÔºåÂ∏¶Êù•‰∫ÜÈáçÂ§ßÁöÑËøõÊ≠•„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÂåªÁñóËßÜËßâÊñáÊú¨Êï∞ÊçÆÁöÑÊï∞ÈáèÂíåË¥®ÈáèÁöÑÈôêÂà∂ÔºåËøô‰∫õÊ®°ÂûãÂú®ÂåªÁñóÂ§öÊ®°ÊÄÅËÉΩÂäõÊñπÈù¢‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàòÔºåËøôÊ∫ê‰∫éÊï∞ÊçÆÈöêÁßÅÈóÆÈ¢òÂíåÈ´òÊòÇÁöÑÊ†áÊ≥®ÊàêÊú¨„ÄÇËôΩÁÑ∂ÂºÄÂàõÊÄßÁöÑÊñπÊ≥ïÂà©Áî® PubMed ÁöÑÂ§ßËßÑÊ®°„ÄÅÂéªÊ†áËØÜÂåñÁöÑÂåªÂ≠¶ÂõæÂÉèÊñáÊú¨ÂØπÊù•Ëß£ÂÜ≥Ëøô‰∫õÈôêÂà∂Ôºå‰ΩÜÁî±‰∫éÂõ∫ÊúâÁöÑÊï∞ÊçÆÂô™Â£∞ÔºåÂÆÉ‰ª¨‰ªçÁÑ∂Â≠òÂú®‰∏çË∂≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨‰ªé PubMed ‰∏≠‰ºòÂåñ‰∫ÜÂåªÂ≠¶ÂõæÂÉèÊñáÊú¨ÂØπÔºåÂπ∂‰ª•‚ÄúÈùûÁõ≤‚ÄùÁöÑÊñπÂºèÈááÁî®‰∫Ü MLLMÔºàGPT-4VÔºâÊù•ÂØπÊï∞ÊçÆËøõË°åÂéªÂô™ÂíåÈáçÊñ∞Ê†ºÂºèÂåñÔºå‰ªéËÄåÂàõÂª∫‰∫ÜÂåÖÂê´ 130 ‰∏á‰∏™ÂåªÂ≠¶ VQA Ê†∑Êú¨ÁöÑ PubMedVision Êï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨ÁöÑÈ™åËØÅË°®ÊòéÔºö(1) PubMedVision ÂèØ‰ª•ÊòæËëóÂ¢ûÂº∫ÂΩìÂâç MLLM ÁöÑÂåªÁñóÂ§öÊ®°ÊÄÅËÉΩÂäõÔºåÂú®ÂåÖÊã¨ MMMU ÂÅ•Â∫∑‰∏éÂåªÂ≠¶ËΩ®ÈÅìÂú®ÂÜÖÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊîπËøõÔºõ(2) ÂåªÂ≠¶‰∏ìÂÆ∂ÁöÑÊâãÂä®Ê£ÄÊü•ÂíåÂÆûËØÅÁªìÊûúÈ™åËØÅ‰∫ÜÊàë‰ª¨Êï∞ÊçÆÈõÜ‰∏éÂÖ∂‰ªñÊï∞ÊçÆÊûÑÂª∫ÊñπÊ≥ïÁõ∏ÊØîÁöÑÂçìË∂äÊï∞ÊçÆË¥®Èáè„ÄÇ‰ΩøÁî® PubMedVisionÔºåÊàë‰ª¨ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™ 34B ÂåªÂ≠¶ MLLM HuatuoGPT-VisionÔºåÂÆÉÂú®ÂºÄÊ∫ê MLLM ‰∏≠ÁöÑÂåªÂ≠¶Â§öÊ®°ÊÄÅÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫ÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇ

##### **VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation**
2406.19276v1 by Yixiao Song, Yekyung Kim, Mohit Iyyer

Existing metrics for evaluating the factuality of long-form text, such as
FACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input
text into "atomic claims" and verify each against a knowledge base like
Wikipedia. These metrics are not suitable for most generation tasks because
they assume that every claim is verifiable (i.e., can plausibly be proven true
or false). We address this issue with VERISCORE, a metric for diverse long-form
generation tasks that contain both verifiable and unverifiable content.
VERISCORE can be effectively implemented with either closed or fine-tuned
open-weight language models, and human evaluation confirms that VERISCORE's
extracted claims are more sensible than those from competing methods across
eight different long-form tasks. We use VERISCORE to evaluate generations from
16 different models across multiple long-form tasks and find that while GPT-4o
is the best-performing model overall, open-weight models such as Mixtral-8x22
are closing the gap. We show that an LM's VERISCORE on one task (e.g.,
biography generation) does not necessarily correlate to its VERISCORE on a
different task (e.g., long-form QA), highlighting the need for expanding
factuality evaluation across tasks with varying fact density.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÈï∑ÁØáÊñáÂ≠ó‰∫ãÂØ¶ÊÄßË©ï‰º∞ÊåáÊ®ôÔºå‰æãÂ¶Ç FACTSCORE (Min Á≠â‰∫∫Ôºå2023) Âíå SAFE (Wei Á≠â‰∫∫Ôºå2024)ÔºåÂ∞áËº∏ÂÖ•ÊñáÂ≠óÂàÜËß£Êàê„ÄåÂéüÂ≠êÊÄßÂÆ£Á®±„ÄçÔºå‰∏¶Ê†πÊìöÁü•Ë≠òÂ∫´Ôºà‰æãÂ¶ÇÁ∂≠Âü∫ÁôæÁßëÔºâÈ©óË≠âÊØèÂÄãÂÆ£Á®±„ÄÇÈÄô‰∫õÊåáÊ®ô‰∏¶‰∏çÈÅ©ÂêàÂ§ßÂ§öÊï∏ÁîüÊàê‰ªªÂãôÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÂÅáË®≠ÊØèÂÄãÂÆ£Á®±ÈÉΩÊòØÂèØÈ©óË≠âÁöÑÔºàÂç≥ÂêàÁêÜÂú∞Ë¢´Ë≠âÊòéÁÇ∫ÁúüÊàñÂÅáÔºâ„ÄÇÊàëÂÄë‰ΩøÁî® VERISCORE Ëß£Ê±∫Ê≠§ÂïèÈ°åÔºåVERISCORE ÊòØÈÅ©Áî®ÊñºÂåÖÂê´ÂèØÈ©óË≠âÂíå‰∏çÂèØÈ©óË≠âÂÖßÂÆπÁöÑÂ§öÂÖÉÈï∑ÁØáÁîüÊàê‰ªªÂãôÁöÑÊåáÊ®ô„ÄÇVERISCORE ÂèØ‰ª•ÊúâÊïàÂú∞‰ΩøÁî®Â∞ÅÈñâÊàñÂæÆË™øÁöÑÈñãÊîæÊ¨äÈáçË™ûË®ÄÊ®°ÂûãÂØ¶‰ΩúÔºåËÄå‰∫∫Â∑•Ë©ï‰º∞Á¢∫Ë™ç VERISCORE ÊèêÂèñÁöÑÂÆ£Á®±ÊØîÂÖ´Á®Æ‰∏çÂêåÈï∑ÁØá‰ªªÂãô‰∏≠Á´∂Áà≠ÊñπÊ≥ïÊèêÂèñÁöÑÂÆ£Á®±Êõ¥ÂêàÁêÜ„ÄÇÊàëÂÄë‰ΩøÁî® VERISCORE Ë©ï‰º∞‰æÜËá™ 16 ÂÄã‰∏çÂêåÊ®°ÂûãÂú®Â§öÂÄãÈï∑ÁØá‰ªªÂãô‰∏≠ÁöÑÁîüÊàêÔºå‰∏¶ÁôºÁèæÂÑòÁÆ° GPT-4o Êï¥È´îË°®ÁèæÊúÄ‰Ω≥Ôºå‰ΩÜ Mixtral-8x22 Á≠âÈñãÊîæÊ¨äÈáçÊ®°ÂûãÊ≠£Âú®Á∏ÆÂ∞èÂ∑ÆË∑ù„ÄÇÊàëÂÄëË°®ÊòéÔºåË™ûË®ÄÊ®°ÂûãÂú®‰∏ÄÂÄã‰ªªÂãôÔºà‰æãÂ¶ÇÂÇ≥Ë®òÁîüÊàêÔºâ‰∏äÁöÑ VERISCORE ‰∏¶‰∏ç‰∏ÄÂÆöËàáÂÖ∂Âú®‰∏çÂêå‰ªªÂãôÔºà‰æãÂ¶ÇÈï∑ÁØáÂïèÁ≠îÔºâ‰∏äÁöÑ VERISCORE Áõ∏ÈóúÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜÂú®‰∫ãÂØ¶ÂØÜÂ∫¶‰∏çÂêåÁöÑ‰ªªÂãô‰∏≠Êì¥Â±ï‰∫ãÂØ¶ÊÄßË©ï‰º∞ÁöÑÂøÖË¶ÅÊÄß„ÄÇ

##### **AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning**
2406.19271v1 by Praneeth Vadlapati

Up-to-date and reliable Large Language Models (LLMs) are consistently sought
after. Typically, LLMs are trained on a fixed dataset and then deployed.
However, the training data continually becomes outdated. Enable automatic
training of AI using web data involves significant concerns regarding data
quality and safety due to bias, spam, and other unsafe or unwanted text. Pure
data is essential for producing reliable models. Training a model on impure
data may result in undesirable outcomes. This research proposes a system that
collects web data and automatically filters out unwanted text with the
assistance of existing trusted AI models. In the experiment, a small sample of
web data was collected and filtered, demonstrating the system's effectiveness
in purifying the data.

ÊëòË¶ÅÔºöÊåÅÁ∫åÂ∞ãÊâæÊúÄÊñ∞‰∏îÂèØÈù†ÁöÑÂ§ßË™ûË®ÄÊ®°Âûã (LLM)„ÄÇÈÄöÂ∏∏ÔºåLLM ÊúÉÂú®Âõ∫ÂÆöË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÔºåÁÑ∂ÂæåÈÉ®ÁΩ≤„ÄÇÁÑ∂ËÄåÔºåË®ìÁ∑¥Ë≥áÊñôÊúÉÊåÅÁ∫åÈÅéÊôÇ„ÄÇ‰ΩøÁî®Á∂≤Ë∑ØË≥áÊñôËá™ÂãïË®ìÁ∑¥ AI ÊúÉÊ∂âÂèäÂà∞Ë≥áÊñôÂìÅË≥™ÂíåÂÆâÂÖ®ÊÄßÁöÑÈáçÂ§ßÁñëÊÖÆÔºåÂõ†ÁÇ∫Ë≥áÊñôÊúÉÊúâÂÅèË¶ã„ÄÅÂûÉÂúæÈÉµ‰ª∂ÂíåÂÖ∂‰ªñ‰∏çÂÆâÂÖ®Êàñ‰∏çÈúÄË¶ÅÁöÑÊñáÂ≠ó„ÄÇÁ¥îÊ∑®ÁöÑË≥áÊñôÂ∞çÊñºÂª∫Á´ãÂèØÈù†ÁöÑÊ®°ÂûãËá≥ÈóúÈáçË¶Å„ÄÇ‰ΩøÁî®‰∏çÁ¥îÊ∑®ÁöÑË≥áÊñôË®ìÁ∑¥Ê®°ÂûãÂèØËÉΩÊúÉÂ∞éËá¥‰∏çËâØÁöÑÁµêÊûú„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ≥ªÁµ±ÔºåÂèØ‰ª•Êî∂ÈõÜÁ∂≤Ë∑ØË≥áÊñôÔºå‰∏¶Âú®ÁèæÊúâÂèØ‰ø°Ë≥¥ AI Ê®°ÂûãÁöÑÂçîÂä©‰∏ãËá™ÂãïÈÅéÊøæÊéâ‰∏çÈúÄË¶ÅÁöÑÊñáÂ≠ó„ÄÇÂú®ÂØ¶È©ó‰∏≠ÔºåÊî∂ÈõÜ‰∏¶ÈÅéÊøæ‰∫ÜÁ∂≤Ë∑ØË≥áÊñôÁöÑÂ∞èÊ®£Êú¨ÔºåË≠âÊòé‰∫ÜÁ≥ªÁµ±Âú®Ê∑®ÂåñË≥áÊñôÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens Grounding**
2406.19263v1 by Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang, Xin Eric Wang

Graphical User Interfaces (GUIs) are central to our interaction with digital
devices. Recently, growing efforts have been made to build models for various
GUI understanding tasks. However, these efforts largely overlook an important
GUI-referring task: screen reading based on user-indicated points, which we
name the Screen Point-and-Read (SPR) task. This task is predominantly handled
by rigid accessible screen reading tools, in great need of new models driven by
advancements in Multimodal Large Language Models (MLLMs). In this paper, we
propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,
to address the SPR task. Based on the input point coordinate and the
corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout
Tree. Based on the tree, our ToL agent not only comprehends the content of the
indicated area but also articulates the layout and spatial relationships
between elements. Such layout information is crucial for accurately
interpreting information on the screen, distinguishing our ToL agent from other
screen reading tools. We also thoroughly evaluate the ToL agent against other
baselines on a newly proposed SPR benchmark, which includes GUIs from mobile,
web, and operating systems. Last but not least, we test the ToL agent on mobile
GUI navigation tasks, demonstrating its utility in identifying incorrect
actions along the path of agent execution trajectories. Code and data:
screen-point-and-read.github.io

ÊëòË¶ÅÔºöÂúñÂΩ¢‰ΩøÁî®ËÄÖ‰ªãÈù¢ (GUI) ÊòØÊàëÂÄëËàáÊï∏‰ΩçË£ùÁΩÆ‰∫íÂãïÁöÑÊ†∏ÂøÉ„ÄÇÊúÄËøëÔºåÂª∫ÁΩÆÂêÑÁ®Æ GUI ÁêÜËß£‰ªªÂãôÁöÑÊ®°ÂûãÂ∑≤ÊàêÁÇ∫‰∏ÄËÇ°ÊàêÈï∑‰∏≠ÁöÑË∂®Âã¢„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÂä™ÂäõÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂøΩÁï•‰∫Ü‰∏ÄÂÄãÈáçË¶ÅÁöÑ GUI ÂèÉËÄÉ‰ªªÂãôÔºöÊ†πÊìö‰ΩøÁî®ËÄÖÊåáÁ§∫ÁöÑÈªûÈÄ≤Ë°åËû¢ÂπïÊúóËÆÄÔºåÊàëÂÄëÂ∞áÂÖ∂ÂëΩÂêçÁÇ∫Ëû¢ÂπïÈªûÈÅ∏ÊúóËÆÄ (SPR) ‰ªªÂãô„ÄÇÊ≠§‰ªªÂãô‰∏ªË¶ÅÁî±ÂÉµÂåñÁöÑÁÑ°ÈöúÁ§ôËû¢ÂπïÊúóËÆÄÂ∑•ÂÖ∑ËôïÁêÜÔºåËø´ÂàáÈúÄË¶ÅÁî±Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM) ÁöÑÈÄ≤Â±ïÈ©ÖÂãïÁöÑÊñ∞Ê®°Âûã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ®πÁãÄÈÄèÈè° (ToL) ‰ª£ÁêÜÔºåÂà©Áî®‰∏ÄÁ®ÆÊñ∞ÁöÑ ToL Êé•Âú∞Ê©üÂà∂‰æÜËôïÁêÜ SPR ‰ªªÂãô„ÄÇÊ†πÊìöËº∏ÂÖ•ÈªûÂ∫ßÊ®ôÂíåÂ∞çÊáâÁöÑ GUI Ëû¢ÂπïÊà™ÂúñÔºåÊàëÂÄëÁöÑ ToL ‰ª£ÁêÜÂª∫Êßã‰∏ÄÂÄãÈöéÂ±§ÂºèÈÖçÁΩÆÊ®π„ÄÇÊ†πÊìöÊ≠§Ê®πÔºåÊàëÂÄëÁöÑ ToL ‰ª£ÁêÜ‰∏çÂÉÖÁêÜËß£ÊåáÁ§∫ÂçÄÂüüÁöÑÂÖßÂÆπÔºåÈÇÑËÉΩÊ∏ÖÊô∞Ë°®ÈÅîÂÖÉÁ¥†‰πãÈñìÁöÑÈÖçÁΩÆÂíåÁ©∫ÈñìÈóú‰øÇ„ÄÇÊ≠§È°ûÈÖçÁΩÆË≥áË®äÂ∞çÊñºÁ≤æÊ∫ñË©ÆÈáãËû¢Âπï‰∏äÁöÑË≥áË®äËá≥ÈóúÈáçË¶ÅÔºåÈÄô‰ΩøÊàëÂÄëÁöÑ ToL ‰ª£ÁêÜÂçÄÂà•ÊñºÂÖ∂‰ªñËû¢ÂπïÊúóËÆÄÂ∑•ÂÖ∑„ÄÇÊàëÂÄë‰πüÂæπÂ∫ïË©ï‰º∞‰∫Ü ToL ‰ª£ÁêÜËàáÂÖ∂‰ªñÂü∫Ê∫ñÂú®‰∏ÄÂÄãÊñ∞ÊèêÂá∫ÁöÑ SPR Ë©ïÈáèÂü∫Ê∫ñ‰∏äÁöÑË°®ÁèæÔºåÂÖ∂‰∏≠ÂåÖÂê´‰æÜËá™Ë°åÂãïË£ùÁΩÆ„ÄÅÁ∂≤È†ÅÂíå‰ΩúÊ•≠Á≥ªÁµ±ÁöÑ GUI„ÄÇÊúÄÂæå‰ΩÜ‰∏¶ÈùûÊúÄ‰∏çÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÂú®Ë°åÂãïË£ùÁΩÆ GUI Â∞éËà™‰ªªÂãô‰∏≠Ê∏¨Ë©¶ ToL ‰ª£ÁêÜÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Ë≠òÂà•‰ª£ÁêÜÂü∑Ë°åËªåË∑°Ë∑ØÂæë‰∏≠‰∏çÊ≠£Á¢∫Âãï‰ΩúÁöÑÊïàÁî®„ÄÇÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÔºöscreen-point-and-read.github.io

##### **AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI**
2406.19256v1 by Kaveen Hiniduma, Suren Byna, Jean Luca Bez, Ravi Madduri

"Garbage In Garbage Out" is a universally agreed quote by computer scientists
from various domains, including Artificial Intelligence (AI). As data is the
fuel for AI, models trained on low-quality, biased data are often ineffective.
Computer scientists who use AI invest a considerable amount of time and effort
in preparing the data for AI. However, there are no standard methods or
frameworks for assessing the "readiness" of data for AI. To provide a
quantifiable assessment of the readiness of data for AI processes, we define
parameters of AI data readiness and introduce AIDRIN (AI Data Readiness
Inspector). AIDRIN is a framework covering a broad range of readiness
dimensions available in the literature that aid in evaluating the readiness of
data quantitatively and qualitatively. AIDRIN uses metrics in traditional data
quality assessment such as completeness, outliers, and duplicates for data
evaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,
such as feature importance, feature correlations, class imbalance, fairness,
privacy, and FAIR (Findability, Accessibility, Interoperability, and
Reusability) principle compliance. AIDRIN provides visualizations and reports
to assist data scientists in further investigating the readiness of data. The
AIDRIN framework enhances the efficiency of the machine learning pipeline to
make informed decisions on data readiness for AI applications.

ÊëòË¶ÅÔºö„ÄåÂûÉÂúæÈÄ≤ÔºåÂûÉÂúæÂá∫„ÄçÊòØÂêÑÂÄãÈ†òÂüüÁöÑÈõªËÖ¶ÁßëÂ≠∏ÂÆ∂ÔºåÂåÖÊã¨‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÊôÆÈÅçÂÖ±Ë≠ò„ÄÇÁî±ÊñºË≥áÊñôÊòØ AI ÁöÑÁáÉÊñôÔºåÂõ†Ê≠§‰ΩøÁî®ÂìÅË≥™‰ΩéËêΩ„ÄÅÊúâÂÅèÂ∑ÆÁöÑË≥áÊñôË®ìÁ∑¥ÁöÑÊ®°ÂûãÈÄöÂ∏∏Ê≤íÊúâÊïàÁéá„ÄÇ‰ΩøÁî® AI ÁöÑÈõªËÖ¶ÁßëÂ≠∏ÂÆ∂ÊúÉËä±Ë≤ªÂ§ßÈáèÊôÇÈñìÂíåÁ≤æÂäõ‰æÜÊ∫ñÂÇô AI ÁöÑË≥áÊñô„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÊ≤íÊúâÊ®ôÊ∫ñÁöÑÊñπÊ≥ïÊàñÊû∂Êßã‰æÜË©ï‰º∞Ë≥áÊñôÂ∞ç AI ÁöÑ„ÄåÊ∫ñÂÇôÂ∫¶„Äç„ÄÇÁÇ∫‰∫ÜÊèê‰æõÂèØÈáèÂåñÁöÑË©ï‰º∞ÔºåË™™ÊòéË≥áÊñôÂ∞ç AI Á®ãÂ∫èÁöÑÊ∫ñÂÇôÂ∫¶ÔºåÊàëÂÄëÂÆöÁæ© AI Ë≥áÊñôÊ∫ñÂÇôÂ∫¶ÁöÑÂèÉÊï∏Ôºå‰∏¶Â∞éÂÖ• AIDRINÔºàAI Ë≥áÊñôÊ∫ñÂÇôÂ∫¶Ê™¢Êü•Âô®Ôºâ„ÄÇAIDRIN ÊòØÊ∂µËìãÂª£Ê≥õÊ∫ñÂÇôÂ∫¶Èù¢ÂêëÁöÑÊû∂ÊßãÔºåÂèØÁî®ÊñºË©ï‰º∞Ë≥áÊñôÁöÑÊ∫ñÂÇôÂ∫¶ÔºåÁÑ°Ë´ñÊòØÈáèÂåñÊàñË≥™Âåñ„ÄÇAIDRIN ‰ΩøÁî®ÂÇ≥Áµ±Ë≥áÊñôÂìÅË≥™Ë©ï‰º∞‰∏≠ÁöÑÊåáÊ®ôÔºå‰æãÂ¶ÇÂÆåÊï¥ÊÄß„ÄÅÁï∞Â∏∏ÂÄºÂíåÈáçË§áÂÄº‰æÜË©ï‰º∞Ë≥áÊñô„ÄÇÊ≠§Â§ñÔºåAIDRIN ‰ΩøÁî®ÁâπÂÆöÊñºË©ï‰º∞ AI Ë≥áÊñôÁöÑÊåáÊ®ôÔºå‰æãÂ¶ÇÁâπÂæµÈáçË¶ÅÊÄß„ÄÅÁâπÂæµÁõ∏ÈóúÊÄß„ÄÅÈ°ûÂà•Â§±Ë°°„ÄÅÂÖ¨Âπ≥ÊÄß„ÄÅÈö±ÁßÅÂíå FAIRÔºàÂèØÂ∞ãÊâæÊÄß„ÄÅÂèØÂ≠òÂèñÊÄß„ÄÅ‰∫íÊìç‰ΩúÊÄßÂíåÂèØÈáçË§á‰ΩøÁî®ÊÄßÔºâÂéüÂâáÂêàË¶èÊÄß„ÄÇAIDRIN Êèê‰æõË¶ñË¶∫ÂåñÂíåÂ†±ÂëäÔºå‰ª•ÂçîÂä©Ë≥áÊñôÁßëÂ≠∏ÂÆ∂ÈÄ≤‰∏ÄÊ≠•Ë™øÊü•Ë≥áÊñôÁöÑÊ∫ñÂÇôÂ∫¶„ÄÇAIDRIN Êû∂ÊßãÊèêÂçáÊ©üÂô®Â≠∏ÁøíÁÆ°Á∑öÁöÑÊïàÁéáÔºå‰ª•‰æøÈáùÂ∞ç AI ÊáâÁî®Á®ãÂºèÁöÑË≥áÊñôÊ∫ñÂÇôÂ∫¶ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñ„ÄÇ

##### **Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment**
2406.19255v1 by Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan

While pre-training large-scale video-language models (VLMs) has shown
remarkable potential for various downstream video-language tasks, existing VLMs
can still suffer from certain commonly seen limitations, e.g., coarse-grained
cross-modal aligning , under-modeling of temporal dynamics, detached
video-language view. In this work, we target enhancing VLMs with a fine-grained
structural spatio-temporal alignment learning method (namely Finsta). First of
all, we represent the input texts and videos with fine-grained scene graph (SG)
structures, both of which are further unified into a holistic SG (HSG) for
bridging two modalities. Then, an SG-based framework is built, where the
textual SG (TSG) is encoded with a graph Transformer, while the video dynamic
SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for
spatial and temporal feature propagation. A spatial-temporal Gaussian
differential graph Transformer is further devised to strengthen the sense of
the changes in objects across spatial and temporal dimensions. Next, based on
the fine-grained structural features of TSG and DSG, we perform object-centered
spatial alignment and predicate-centered temporal alignment respectively,
enhancing the video-language grounding in both the spatiality and temporality.
We design our method as a plug&play system, which can be integrated into
existing well-trained VLMs for further representation augmentation, without
training from scratch or relying on SG annotations in downstream applications.
On 6 representative VL modeling tasks over 12 datasets in both standard and
long-form video scenarios, Finsta consistently improves the existing 13
strong-performing VLMs persistently, and refreshes the current state-of-the-art
end task performance significantly in both the fine-tuning and zero-shot
settings.

ÊëòË¶ÅÔºö<paragraph>ÈõñÁÑ∂È†êË®ìÁ∑¥Â§ßÂûãË¶ñË®äË™ûË®ÄÊ®°Âûã (VLM) Â∑≤Â±ïÁèæÂá∫Â∞çÂêÑÁ®Æ‰∏ãÊ∏∏Ë¶ñË®äË™ûË®Ä‰ªªÂãôÁöÑÈ°ØËëóÊΩõÂäõÔºå‰ΩÜÁèæÊúâÁöÑ VLM ‰ªçÂèØËÉΩÂèóÂà∞Êüê‰∫õÂ∏∏Ë¶ãÈôêÂà∂ÁöÑÂΩ±ÈüøÔºå‰æãÂ¶ÇÁ≤óÁ≤íÂ∫¶ÁöÑË∑®Ê®°ÊÖãÂ∞çÈΩä„ÄÅÂ∞çÊôÇÈñìÂãïÊÖãÁöÑÂª∫Ê®°‰∏çË∂≥„ÄÅÂàÜÈõ¢ÁöÑË¶ñË®äË™ûË®ÄÊ™¢Ë¶ñ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ª•ÂÖ∑ÂÇôÁ¥∞Á≤íÂ∫¶ÁµêÊßãÂåñÊôÇÁ©∫Â∞çÈΩäÂ≠∏ÁøíÊñπÊ≥ï (Âç≥ Finsta) ÁöÑÂ¢ûÂº∑ VLM ÁÇ∫ÁõÆÊ®ô„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰ª•Á¥∞Á≤íÂ∫¶ÁöÑÂ†¥ÊôØÂúñ (SG) ÁµêÊßãË°®Á§∫Ëº∏ÂÖ•ÊñáÂ≠óÂíåË¶ñË®äÔºåÂÖ©ËÄÖÈÄ≤‰∏ÄÊ≠•Áµ±‰∏ÄÂà∞‰∏ÄÂÄãÊï¥È´î SG (HSG) ‰∏≠Ôºå‰ª•Ê©ãÊé•ÂÖ©ÂÄãÊ®°ÊÖã„ÄÇÁÑ∂ÂæåÔºåÂª∫Á´ã‰∏ÄÂÄãÂü∫Êñº SG ÁöÑÊ°ÜÊû∂ÔºåÂÖ∂‰∏≠ÊñáÂ≠ó SG (TSG) ‰ΩøÁî®ÂúñÂΩ¢ Transformer Á∑®Á¢ºÔºåËÄåË¶ñË®äÂãïÊÖã SG (DSG) Âíå HSG Ââá‰ΩøÁî®Êñ∞Á©éÁöÑÈÅûËø¥ÂúñÂΩ¢ Transformer Âª∫Ê®°Ôºå‰ª•ÈÄ≤Ë°åÁ©∫ÈñìÂíåÊôÇÈñìÁâπÂæµÂÇ≥Êí≠„ÄÇÈÄ≤‰∏ÄÊ≠•Ë®≠Ë®à‰∫Ü‰∏ÄÂÄãÊôÇÁ©∫È´òÊñØÂ∑ÆÂàÜÂúñÂΩ¢ TransformerÔºå‰ª•Â¢ûÂº∑Áâ©È´îÂú®ÊôÇÁ©∫Á∂≠Â∫¶‰∏≠ËÆäÂåñÁöÑÊÑüË¶∫„ÄÇÊé•‰∏ã‰æÜÔºåÊ†πÊìö TSG Âíå DSG ÁöÑÁ¥∞Á≤íÂ∫¶ÁµêÊßãÁâπÂæµÔºåÊàëÂÄëÂàÜÂà•Âü∑Ë°å‰ª•Áâ©‰ª∂ÁÇ∫‰∏≠ÂøÉÁöÑÁ©∫ÈñìÂ∞çÈΩäÂíå‰ª•Ë¨ÇË©ûÁÇ∫‰∏≠ÂøÉÁöÑÊôÇÂ∫èÂ∞çÈΩäÔºåÂ¢ûÂº∑Ë¶ñË®äË™ûË®ÄÂú®Á©∫ÈñìÂíåÊôÇÈñì‰∏äÁöÑÂü∫Á§é„ÄÇÊàëÂÄëÂ∞áÊñπÊ≥ïË®≠Ë®àÁÇ∫‰∏ÄÂÄãÂç≥ÊèíÂç≥Áî®ÁöÑÁ≥ªÁµ±ÔºåÂèØ‰ª•Êï¥ÂêàÂà∞ÁèæÊúâÁöÑË®ìÁ∑¥ËâØÂ•ΩÁöÑ VLM ‰∏≠Ôºå‰ª•ÈÄ≤‰∏ÄÊ≠•Êì¥ÂÖÖË°®Á§∫ÔºåËÄåÁÑ°ÈúÄÂæûÈ†≠ÈñãÂßãË®ìÁ∑¥Êàñ‰æùË≥¥‰∏ãÊ∏∏ÊáâÁî®Á®ãÂºè‰∏≠ÁöÑ SG Ê®ôË®ª„ÄÇÂú® 12 ÂÄãË≥áÊñôÈõÜ‰∏äÁöÑ 6 ÂÄã‰ª£Ë°®ÊÄß VL Âª∫Ê®°‰ªªÂãô‰∏≠ÔºåÁÑ°Ë´ñÊòØÂú®Ê®ôÊ∫ñË¶ñË®äÂ†¥ÊôØÈÇÑÊòØÈï∑Ê†ºÂºèË¶ñË®äÂ†¥ÊôØ‰∏≠ÔºåFinsta ÈÉΩÊåÅÁ∫åÊîπÂñÑÁèæÊúâÁöÑ 13 ÂÄãÊïàËÉΩÂº∑Â§ßÁöÑ VLMÔºå‰∏¶Âú®ÂæÆË™øÂíåÈõ∂Ê¨°Â≠∏ÁøíË®≠ÂÆö‰∏≠È°ØËëóÊõ¥Êñ∞ÁõÆÂâçÁöÑÊúÄÊñ∞ÊäÄË°ìÊúÄÁµÇ‰ªªÂãôÊïàËÉΩ„ÄÇ</paragraph>

##### **AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation**
2406.19251v1 by Jia Fu, Xiaoting Qin, Fangkai Yang, Lu Wang, Jue Zhang, Qingwei Lin, Yubo Chen, Dongmei Zhang, Saravan Rajmohan, Qi Zhang

Recent advancements in Large Language Models have transformed ML/AI
development, necessitating a reevaluation of AutoML principles for the
Retrieval-Augmented Generation (RAG) systems. To address the challenges of
hyper-parameter optimization and online adaptation in RAG, we propose the
AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online
multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical
MAB (Hier-MAB) method for efficient exploration of large search spaces. We
conduct extensive experiments on tuning hyper-parameters, such as top-k
retrieved documents, prompt compression ratio, and embedding methods, using the
ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly
optimization all three hyper-parameters demonstrate that MAB-based online
learning methods can achieve Recall@5 $\approx 0.8$ for scenarios with
prominent gradients in search space, using only $\sim20\%$ of the LLM API calls
required by the Grid Search approach. Additionally, the proposed Hier-MAB
approach outperforms other baselines in more challenging optimization
scenarios. The code will be made available at https://aka.ms/autorag.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ∑≤ËΩâËÆä ML/AI ÁöÑÈñãÁôºÔºåÈÄô‰ΩøÂæóÂøÖÈ†àÈáçÊñ∞Ë©ï‰º∞Áî®ÊñºÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Á≥ªÁµ±ÁöÑ AutoML ÂéüÂâá„ÄÇÁÇ∫‰∫ÜÊáâÂ∞ç RAG ‰∏≠ÁöÑË∂ÖÂèÉÊï∏ÊúÄ‰Ω≥ÂåñÂíåÁ∑ö‰∏äË™øÊï¥ÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü AutoRAG-HP Êû∂ÊßãÔºåÂÆÉÂ∞áË∂ÖÂèÉÊï∏Ë™øÊï¥Âà∂ÂÆöÁÇ∫‰∏ÄÂÄãÁ∑ö‰∏äÂ§öÈáçÈÅ∏ÊìáË≥≠ÂçöÊ©ü (MAB) ÂïèÈ°åÔºå‰∏¶ÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂÖ©Â±§ÈöéÂ±§Âºè MAB (Hier-MAB) ÊñπÊ≥ïÔºåÁî®ÊñºÊúâÊïàÂú∞Êé¢Á¥¢Â§ßÂûãÊêúÂ∞ãÁ©∫Èñì„ÄÇÊàëÂÄëÂ∞çË∂ÖÂèÉÊï∏Ë™øÊï¥ÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰æãÂ¶ÇÂâç k ÂêçÊ™¢Á¥¢ÁöÑÊñá‰ª∂„ÄÅÊèêÁ§∫Â£ìÁ∏ÆÁéáÂíåÂµåÂÖ•ÊñπÊ≥ïÔºå‰ΩøÁî® ALCE-ASQA ÂíåËá™ÁÑ∂ÂïèÈ°åË≥áÊñôÈõÜ„ÄÇÊàëÂÄëÂæûËÅØÂêàÊúÄ‰Ω≥ÂåñÊâÄÊúâ‰∏âÂÄãË∂ÖÂèÉÊï∏ÁöÑË©ï‰º∞‰∏≠Ë≠âÊòéÔºåÂü∫Êñº MAB ÁöÑÁ∑ö‰∏äÂ≠∏ÁøíÊñπÊ≥ïÂèØ‰ª•ÈáùÂ∞çÊêúÂ∞ãÁ©∫Èñì‰∏≠ÂÖ∑ÊúâÈ°ØËëóÊ¢ØÂ∫¶ÁöÑÂ†¥ÊôØÂØ¶Áèæ Recall@5 $\approx 0.8$ÔºåÂÉÖ‰ΩøÁî®Á∂≤Ê†ºÊêúÂ∞ãÊñπÊ≥ïÊâÄÈúÄÁöÑ LLM API ÂëºÂè´ÁöÑ $\sim20\%$„ÄÇÊ≠§Â§ñÔºåÊèêÂá∫ÁöÑ Hier-MAB ÊñπÊ≥ïÂú®Êõ¥ÂÖ∑ÊåëÊà∞ÊÄßÁöÑÊúÄ‰Ω≥ÂåñÂ†¥ÊôØ‰∏≠ÂÑ™ÊñºÂÖ∂‰ªñÂü∫Ê∫ñ„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÊñº https://aka.ms/autorag ‰∏äÊèê‰æõ„ÄÇ

##### **Application of ASV for Voice Identification after VC and Duration Predictor Improvement in TTS Models**
2406.19243v1 by Borodin Kirill Nikolayevich, Kudryavtsev Vasiliy Dmitrievich, Mkrtchian Grach Maratovich, Gorodnichev Mikhail Genadievich, Korzh Dmitrii Sergeevich

One of the most crucial components in the field of biometric security is the
automatic speaker verification system, which is based on the speaker's voice.
It is possible to utilise ASVs in isolation or in conjunction with other AI
models. In the contemporary era, the quality and quantity of neural networks
are increasing exponentially. Concurrently, there is a growing number of
systems that aim to manipulate data through the use of voice conversion and
text-to-speech models. The field of voice biometrics forgery is aided by a
number of challenges, including SSTC, ASVSpoof, and SingFake.
  This paper presents a system for automatic speaker verification. The primary
objective of our model is the extraction of embeddings from the target
speaker's audio in order to obtain information about important characteristics
of his voice, such as pitch, energy, and the duration of phonemes. This
information is used in our multivoice TTS pipeline, which is currently under
development. However, this model was employed within the SSTC challenge to
verify users whose voice had undergone voice conversion, where it demonstrated
an EER of 20.669.

ÊëòË¶ÅÔºöÁîüÁâ©ÁâπÂæµÂÆâÂÖ®È†òÂüü‰∏≠ÊúÄÈáçË¶ÅÁöÑÁµÑÊàêÈÉ®ÂàÜ‰πã‰∏ÄÊòØËá™ÂãïË™ûÈü≥È©óË≠âÁ≥ªÁµ±ÔºåÂÖ∂Âü∫Á§éÊòØË™™Ë©±ËÄÖÁöÑËÅ≤Èü≥„ÄÇ
ÂèØ‰ª•ÂñÆÁç®‰ΩøÁî® ASVÔºå‰πüÂèØ‰ª•ËàáÂÖ∂‰ªñ AI Ê®°ÂûãÁµêÂêà‰ΩøÁî®„ÄÇÂú®Áï∂‰ª£ÔºåÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂìÅË≥™ÂíåÊï∏ÈáèÊ≠£ÂëàÊåáÊï∏Á¥öÂ¢ûÈï∑„ÄÇÂêåÊôÇÔºåË∂ä‰æÜË∂äÂ§öÁ≥ªÁµ±Êó®Âú®ÈÄèÈÅé‰ΩøÁî®Ë™ûÈü≥ËΩâÊèõÂíåÊñáÂ≠óËΩâË™ûÈü≥Ê®°Âûã‰æÜÊìçÁ∏±Ë≥áÊñô„ÄÇË™ûÈü≥ÁîüÁâ©ÁâπÂæµÂÅΩÈÄ†È†òÂüüÂèóÂà∞Ë®±Â§öÊåëÊà∞ÁöÑÂπ´Âä©ÔºåÂåÖÊã¨ SSTC„ÄÅASVSpoof Âíå SingFake„ÄÇ
Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãËá™ÂãïË™ûÈü≥È©óË≠âÁ≥ªÁµ±„ÄÇÊàëÂÄëÊ®°ÂûãÁöÑ‰∏ªË¶ÅÁõÆÊ®ôÊòØÂæûÁõÆÊ®ôË™™Ë©±ËÄÖÁöÑÈü≥Ë®ä‰∏≠ËêÉÂèñÂµåÂÖ•Ôºå‰ª•ÂèñÂæóÂÖ∂ËÅ≤Èü≥‰∏≠ÈáçË¶ÅÁâπÂæµÁöÑË≥áË®äÔºå‰æãÂ¶ÇÈü≥È´ò„ÄÅËÉΩÈáèÂíåÈü≥Á¥†ÁöÑÊåÅÁ∫åÊôÇÈñì„ÄÇÊ≠§Ë≥áË®äÁî®ÊñºÊàëÂÄëÁõÆÂâçÊ≠£Âú®ÈñãÁôºÁöÑÂ§öË™ûÈü≥ TTS ÁÆ°Á∑ö‰∏≠„ÄÇÁÑ∂ËÄåÔºåÊ≠§Ê®°ÂûãÂú® SSTC ÊåëÊà∞‰∏≠Áî®ÊñºÈ©óË≠âËÅ≤Èü≥Á∂ìÈÅéË™ûÈü≥ËΩâÊèõÁöÑ‰ΩøÁî®ËÄÖÔºåÂÖ∂ EER ÁÇ∫ 20.669„ÄÇ

##### **Revealing Fine-Grained Values and Opinions in Large Language Models**
2406.19238v1 by Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein

Uncovering latent values and opinions in large language models (LLMs) can
help identify biases and mitigate potential harm. Recently, this has been
approached by presenting LLMs with survey questions and quantifying their
stances towards morally and politically charged statements. However, the
stances generated by LLMs can vary greatly depending on how they are prompted,
and there are many ways to argue for or against a given position. In this work,
we propose to address this by analysing a large and robust dataset of 156k LLM
responses to the 62 propositions of the Political Compass Test (PCT) generated
by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of
their generated stances and fine-grained analysis of the plain text
justifications for those stances. For fine-grained analysis, we propose to
identify tropes in the responses: semantically similar phrases that are
recurrent and consistent across different prompts, revealing patterns in the
text that a given LLM is prone to produce. We find that demographic features
added to prompts significantly affect outcomes on the PCT, reflecting bias, as
well as disparities between the results of tests when eliciting closed-form vs.
open domain responses. Additionally, patterns in the plain text rationales via
tropes show that similar justifications are repeatedly generated across models
and prompts even with disparate stances.

ÊëòË¶ÅÔºöÊè≠Èú≤Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑÊΩõÂú®ÂÉπÂÄºËßÄÂíåËßÄÈªûÊúâÂä©ÊñºË≠òÂà•ÂÅèË¶ã‰∏¶Ê∏õËºïÊΩõÂú®Âç±ÂÆ≥„ÄÇÊúÄËøëÔºåÈÄôÂ∑≤ÈÄèÈÅéÂêë LLM ÊèêÂá∫Ë™øÊü•ÂïèÈ°å‰∏¶ÈáèÂåñÂÆÉÂÄëÂ∞çÈÅìÂæ∑ÂíåÊîøÊ≤ªÊïèÊÑüÈô≥Ëø∞ÁöÑÁ´ãÂ†¥‰æÜÂØ¶Áèæ„ÄÇÁÑ∂ËÄåÔºåLLM Áî¢ÁîüÁöÑÁ´ãÂ†¥ÂèØËÉΩÊúÉÊ†πÊìöÊèêÁ§∫ÊñπÂºèËÄåÊúâÂæàÂ§ß‰∏çÂêåÔºåËÄå‰∏îÊúâÂæàÂ§öÊñπÊ≥ïÂèØ‰ª•ÁÇ∫ÁâπÂÆöÁ´ãÂ†¥ËæØË≠∑ÊàñÂèçÂ∞ç„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂª∫Ë≠∞ÈÄèÈÅéÂàÜÊûê 6 ÂÄã LLM ‰ΩøÁî® 420 ÂÄãÊèêÁ§∫ËÆäÈ´îÁî¢ÁîüÁöÑÊîøÊ≤ªÁæÖÁõ§Ê∏¨Ë©¶ (PCT) ÁöÑ 62 ÂÄãÂëΩÈ°åÁöÑ 156k ÂÄã LLM ÂõûÊáâÁöÑÂ§ßÂûãËÄåÁ©©ÂÅ•ÁöÑË≥áÊñôÈõÜ‰æÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°å„ÄÇÊàëÂÄëÂ∞çÂÆÉÂÄëÁî¢ÁîüÁöÑÁ´ãÂ†¥ÈÄ≤Ë°åÁ≤óÁï•ÂàÜÊûêÔºå‰∏¶Â∞çÈÄô‰∫õÁ´ãÂ†¥ÁöÑÁ¥îÊñáÂ≠óË´ñË≠âÈÄ≤Ë°åÁ¥∞Á∑ªÂàÜÊûê„ÄÇÂ∞çÊñºÁ¥∞Á∑ªÂàÜÊûêÔºåÊàëÂÄëÂª∫Ë≠∞Âú®ÂõûÊáâ‰∏≠ÊâæÂá∫ÊØîÂñªÔºöÂú®‰∏çÂêåÁöÑÊèêÁ§∫‰∏≠ÈáçË§áÂá∫Áèæ‰∏î‰∏ÄËá¥ÁöÑË™ûÁæ©Áõ∏‰ººÁâáË™ûÔºåÊè≠Á§∫Áµ¶ÂÆö LLM ÂÆπÊòìÁî¢ÁîüÁöÑÊñáÂ≠óÊ®°Âºè„ÄÇÊàëÂÄëÁôºÁèæÂä†ÂÖ•ÊèêÁ§∫ÁöÑ‰∫∫Âè£ÁâπÂæµÊúÉÈ°ØËëóÂΩ±Èüø PCT ÁöÑÁµêÊûúÔºåÂèçÊò†Âá∫ÂÅèË¶ãÔºå‰ª•ÂèäÂú®ÂºïÁôºÂ∞ÅÈñâÂºèËàáÈñãÊîæÂºèÈ†òÂüüÂõûÊáâÊôÇÊ∏¨Ë©¶ÁµêÊûú‰πãÈñìÁöÑÂ∑ÆÁï∞„ÄÇÊ≠§Â§ñÔºåÈÄèÈÅéÊØîÂñªÂú®Á¥îÊñáÂ≠óË´ñË≠â‰∏≠ÁöÑÊ®°ÂºèÈ°ØÁ§∫ÔºåÂç≥‰ΩøÁ´ãÂ†¥‰∏çÂêåÔºåÈ°û‰ººÁöÑË´ñË≠â‰πüÊúÉÂú®Ê®°ÂûãÂíåÊèêÁ§∫‰∏≠ÈáçË§áÁî¢Áîü„ÄÇ

##### **FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts**
2406.19237v1 by Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth

Existing benchmarks for visual question answering lack in visual grounding
and complexity, particularly in evaluating spatial reasoning skills. We
introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of
visual question-answering multimodal language models in reasoning with
flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and
human-verified flowchart images from three distinct content sources, along with
22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,
including information localization, decision-making, and logical progression.
We conduct a thorough baseline evaluation on a suite of both open-source and
proprietary multimodal language models using various strategies, followed by an
analysis of directional bias. The results underscore the benchmark's potential
as a vital tool for advancing the field of multimodal modeling, providing a
focused and challenging environment for enhancing model performance in visual
and logical reasoning tasks.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑË¶ñË¶∫ÂïèÁ≠îÂü∫Ê∫ñÁº∫‰πèË¶ñË¶∫Âü∫Á§éÂíåË§áÈõúÊÄßÔºåÁâπÂà•ÊòØÂú®Ë©ï‰º∞Á©∫ÈñìÊé®ÁêÜÊäÄËÉΩÊñπÈù¢„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü FlowVQAÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Âü∫Ê∫ñÔºåÊó®Âú®Ë©ï‰º∞Ë¶ñË¶∫ÂïèÁ≠îÂ§öÊ®°ÊÖãË™ûË®ÄÊ®°ÂûãÂú®‰ΩøÁî®ÊµÅÁ®ãÂúñ‰ΩúÁÇ∫Ë¶ñË¶∫ËÉåÊôØÈÄ≤Ë°åÊé®ÁêÜÁöÑËÉΩÂäõ„ÄÇFlowVQA ÂåÖÂê´ 2,272 ÂÄã‰æÜËá™‰∏âÂÄã‰∏çÂêåÂÖßÂÆπ‰æÜÊ∫êÁöÑÁ∂ìÈÅé‰ªîÁ¥∞ÁîüÊàê‰∏¶Áî±‰∫∫Â∑•È©óË≠âÁöÑÊµÅÁ®ãÂúñÂΩ±ÂÉèÔºå‰ª•Âèä 22,413 ÂÄã‰∏çÂêåÁöÑÂïèÁ≠îÂ∞çÔºåÁî®ÊñºÊ∏¨Ë©¶‰∏ÄÁ≥ªÂàóÊé®ÁêÜ‰ªªÂãôÔºåÂåÖÊã¨Ë≥áË®äÂÆö‰Ωç„ÄÅÊ±∫Á≠ñÂà∂ÂÆöÂíåÈÇèËºØÊé®ÈÄ≤„ÄÇÊàëÂÄëÂ∞ç‰∏ÄÁ≥ªÂàóÈñãÊ∫êÂíåÂ∞àÊúâÂ§öÊ®°ÊÖãË™ûË®ÄÊ®°ÂûãÈÄ≤Ë°å‰∫ÜÂæπÂ∫ïÁöÑÂü∫Ê∫ñË©ï‰º∞Ôºå‰ΩøÁî®ÂêÑÁ®ÆÁ≠ñÁï•ÔºåÁÑ∂ÂæåÂàÜÊûêÊñπÂêëÂÅèÂ∑Æ„ÄÇÁµêÊûúÂº∑Ë™ø‰∫ÜÂü∫Ê∫ñ‰ΩúÁÇ∫Êé®ÈÄ≤Â§öÊ®°ÊÖãÂª∫Ê®°È†òÂüüÁöÑÈáçË¶ÅÂ∑•ÂÖ∑ÁöÑÊΩõÂäõÔºåÁÇ∫Âú®Ë¶ñË¶∫ÂíåÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏≠Â¢ûÂº∑Ê®°ÂûãÊïàËÉΩÊèê‰æõ‰∫ÜÂ∞àÊ≥®‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÁí∞Â¢É„ÄÇ

##### **Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions**
2406.19236v1 by Minghan Li, Heng Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, Alexander G. Hauptmann

Vision-and-Language Navigation (VLN) aims to develop embodied agents that
navigate based on human instructions. However, current VLN frameworks often
rely on static environments and optimal expert supervision, limiting their
real-world applicability. To address this, we introduce Human-Aware
Vision-and-Language Navigation (HA-VLN), extending traditional VLN by
incorporating dynamic human activities and relaxing key assumptions. We propose
the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities
with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)
dataset, extending R2R with human activity descriptions. To tackle HA-VLN
challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and
Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing
cross-modal fusion and diverse training strategies for effective navigation in
dynamic human environments. A comprehensive evaluation, including metrics
considering human activities, and systematic analysis of HA-VLN's unique
challenges, underscores the need for further research to enhance HA-VLN agents'
real-world robustness and adaptability. Ultimately, this work provides
benchmarks and insights for future research on embodied AI and Sim2Real
transfer, paving the way for more realistic and applicable VLN systems in
human-populated environments.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë™ûË®ÄÂ∞éËà™ (VLN) ÁöÑÁõÆÊ®ôÊòØÈñãÁôºÂÖ∑ÂÇôÊ†πÊìö‰∫∫È°ûÊåáÁ§∫Â∞éËà™ÂäüËÉΩÁöÑÂÖ∑Ë∫´‰ª£ÁêÜ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑ VLN Êû∂ÊßãÈÄöÂ∏∏‰æùË≥¥ÊñºÈùúÊÖãÁí∞Â¢ÉÂíåÊúÄ‰Ω≥Â∞àÂÆ∂Áõ£Áù£ÔºåÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠ÁöÑÈÅ©Áî®ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂÖ∑Êúâ‰∫∫È°ûÊÑèË≠òÁöÑË¶ñË¶∫Ë™ûË®ÄÂ∞éËà™ (HA-VLN)ÔºåÈÄèÈÅéÁ¥çÂÖ•ÂãïÊÖã‰∫∫È°ûÊ¥ªÂãïÂíåÊîæÂØ¨ÈóúÈçµÂÅáË®≠‰æÜÊì¥Â±ïÂÇ≥Áµ± VLN„ÄÇÊàëÂÄëÊèêÂá∫‰∫ÜÂÖ∑Êúâ‰∫∫È°ûÊÑèË≠òÁöÑ 3D (HA3D) Ê®°Êì¨Âô®ÔºåÂÆÉÂ∞áÂãïÊÖã‰∫∫È°ûÊ¥ªÂãïËàá Matterport3D Ë≥áÊñôÈõÜÁµêÂêàÂú®‰∏ÄËµ∑Ôºå‰ª•ÂèäÂÖ∑Êúâ‰∫∫È°ûÊÑèË≠òÁöÑÊàøÈñìÂà∞ÊàøÈñì (HA-R2R) Ë≥áÊñôÈõÜÔºåÂ∞á R2R Êì¥Â±ïÂà∞‰∫∫È°ûÊ¥ªÂãïÊèèËø∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞ç HA-VLN ÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂ∞àÂÆ∂Áõ£Áù£Ë∑®Ê®°ÊÖã (VLN-CM) ÂíåÈùûÂ∞àÂÆ∂Áõ£Áù£Ê±∫Á≠ñËΩâÊèõÂô® (VLN-DT) ‰ª£ÁêÜÔºåÂà©Áî®Ë∑®Ê®°ÊÖãËûçÂêàÂíåÂ§öÊ®£ÂåñÁöÑË®ìÁ∑¥Á≠ñÁï•Âú®ÂãïÊÖã‰∫∫È°ûÁí∞Â¢É‰∏≠ÈÄ≤Ë°åÊúâÊïàÁöÑÂ∞éËà™„ÄÇÂÖ®Èù¢ÁöÑË©ï‰º∞ÔºåÂåÖÊã¨ËÄÉÊÖÆ‰∫∫È°ûÊ¥ªÂãïÁöÑÊåáÊ®ôÔºå‰ª•ÂèäÂ∞ç HA-VLN Áç®ÁâπÊåëÊà∞ÁöÑÁ≥ªÁµ±ÂàÜÊûêÔºåÂº∑Ë™ø‰∫ÜÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂‰ª•Â¢ûÂº∑ HA-VLN ‰ª£ÁêÜÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠ÁöÑÁ©©ÂÅ•ÊÄßÂíåÈÅ©ÊáâÊÄßÁöÑÂøÖË¶ÅÊÄß„ÄÇÊúÄÁµÇÔºåÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫ÂÖ∑Ë∫´ AI Âíå Sim2Real ÂÇ≥Ëº∏ÁöÑÊú™‰æÜÁ†îÁ©∂Êèê‰æõ‰∫ÜÂü∫Ê∫ñÂíåË¶ãËß£ÔºåÁÇ∫Âú®Êúâ‰∫∫È°ûÂ±Ö‰ΩèÁöÑÁí∞Â¢É‰∏≠Êõ¥ÈÄºÁúü‰∏îÈÅ©Áî®ÁöÑ VLN Á≥ªÁµ±Èã™Âπ≥‰∫ÜÈÅìË∑Ø„ÄÇ

##### **Seeing Is Believing: Black-Box Membership Inference Attacks Against Retrieval Augmented Generation**
2406.19234v1 by Yuying Li, Gaoyang Liu, Yang Yang, Chen Wang

Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that
enhances Large Language Models (LLMs) by retrieving relevant knowledge from an
external, non-parametric database. This approach aims to mitigate common LLM
issues such as hallucinations and outdated knowledge. Although existing
research has demonstrated security and privacy vulnerabilities within RAG
systems, making them susceptible to attacks like jailbreaks and prompt
injections, the security of the RAG system's external databases remains largely
underexplored. In this paper, we employ Membership Inference Attacks (MIA) to
determine whether a sample is part of the knowledge database of a RAG system,
using only black-box API access. Our core hypothesis posits that if a sample is
a member, it will exhibit significant similarity to the text generated by the
RAG system. To test this, we compute the cosine similarity and the model's
perplexity to establish a membership score, thereby building robust features.
We then introduce two novel attack strategies: a Threshold-based Attack and a
Machine Learning-based Attack, designed to accurately identify membership.
Experimental validation of our methods has achieved a ROC AUC of 82%.

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºàRAGÔºâÊòØ‰∏ÄÁ®ÆÂÖàÈÄ≤ÊäÄË°ìÔºåÂÆÉÈÄèÈÅéÂæûÂ§ñÈÉ®ÁöÑÈùûÂèÉÊï∏Ë≥áÊñôÂ∫´‰∏≠Ê™¢Á¥¢Áõ∏ÈóúÁü•Ë≠òÔºå‰æÜÂ¢ûÂº∑Â§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâ„ÄÇÊ≠§ÊñπÊ≥ïÊó®Âú®Ê∏õËºïÂ∏∏Ë¶ãÁöÑ LLM ÂïèÈ°åÔºå‰æãÂ¶ÇÂπªË¶∫ÂíåÈÅéÊôÇÁöÑÁü•Ë≠ò„ÄÇÂÑòÁÆ°ÁèæÊúâÁöÑÁ†îÁ©∂Â∑≤Ë≠âÊòé RAG Á≥ªÁµ±Â≠òÂú®ÂÆâÂÖ®ÊÄßÂíåÈö±ÁßÅÊºèÊ¥ûÔºå‰ΩøÂÖ∂ÂÆπÊòìÂèóÂà∞Ë∂äÁçÑÂíåÊèêÁ§∫Ê≥®ÂÖ•Á≠âÊîªÊìäÔºå‰ΩÜ RAG Á≥ªÁµ±Â§ñÈÉ®Ë≥áÊñôÂ∫´ÁöÑÂÆâÂÖ®ÊÄßÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªçÊú™ÂæóÂà∞Êé¢Á¥¢„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé°Áî®ÊàêÂì°Êé®Ë´ñÊîªÊìäÔºàMIAÔºâ‰æÜÁ¢∫ÂÆöÁØÑ‰æãÊòØÂê¶ÁÇ∫ RAG Á≥ªÁµ±Áü•Ë≠òË≥áÊñôÂ∫´ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÂÉÖ‰ΩøÁî®ÈªëÁõí API Â≠òÂèñ„ÄÇÊàëÂÄëÁöÑÊ†∏ÂøÉÂÅáË®≠ÊòØÔºåÂ¶ÇÊûúÁØÑ‰æãÊòØÊàêÂì°ÔºåÂÆÉÂ∞áËàá RAG Á≥ªÁµ±Áî¢ÁîüÁöÑÊñáÂ≠óÊúâÈ°ØËëóÁöÑÁõ∏‰ººÊÄß„ÄÇÁÇ∫‰∫ÜÊ∏¨Ë©¶ÈÄô‰∏ÄÈªûÔºåÊàëÂÄëË®àÁÆóÈ§òÂº¶Áõ∏‰ººÊÄßÂíåÊ®°ÂûãÁöÑÂõ∞ÊÉëÂ∫¶Ôºå‰ª•Âª∫Á´ãÊàêÂì°ÂàÜÊï∏ÔºåÂæûËÄåÂª∫Á´ãÂÅ•ÂÖ®ÁöÑÁâπÂæµ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄë‰ªãÁ¥πÂÖ©Á®ÆÊñ∞Á©éÁöÑÊîªÊìäÁ≠ñÁï•ÔºöÂü∫ÊñºÈñæÂÄºÁöÑÊîªÊìäÂíåÂü∫ÊñºÊ©üÂô®Â≠∏ÁøíÁöÑÊîªÊìäÔºåÊó®Âú®Ê∫ñÁ¢∫Ë≠òÂà•ÊàêÂì°„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÁöÑÂØ¶È©óÈ©óË≠âÂ∑≤ÈÅîÂà∞ 82% ÁöÑ ROC AUC„ÄÇ

##### **RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs**
2406.19232v1 by Ekaterina Taktasheva, Maxim Bazhukov, Kirill Koncha, Alena Fenogenova, Ekaterina Artemova

Minimal pairs are a well-established approach to evaluating the grammatical
knowledge of language models. However, existing resources for minimal pairs
address a limited number of languages and lack diversity of language-specific
grammatical phenomena. This paper introduces the Russian Benchmark of
Linguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that
differ in grammaticality and isolate a morphological, syntactic, or semantic
phenomenon. In contrast to existing benchmarks of linguistic minimal pairs,
RuBLiMP is created by applying linguistic perturbations to automatically
annotated sentences from open text corpora and carefully curating test data. We
describe the data collection protocol and present the results of evaluating 25
language models in various scenarios. We find that the widely used language
models for Russian are sensitive to morphological and agreement-oriented
contrasts but fall behind humans on phenomena requiring understanding of
structural relations, negation, transitivity, and tense. RuBLiMP, the codebase,
and other materials are publicly available.

ÊëòË¶ÅÔºöÊúÄÂ∞èÂ∞çÊòØË©ï‰º∞Ë™ûË®ÄÊ®°ÂûãË™ûÊ≥ïÁü•Ë≠òÁöÑÊó¢ÂÆöÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊúÄÂ∞èÂ∞çÁöÑË≥áÊ∫êÂÉÖÈáùÂ∞çÊúâÈôêÊï∏ÈáèÁöÑË™ûË®ÄÔºå‰∏îÁº∫‰πèË™ûË®ÄÁâπÂÆöË™ûÊ≥ïÁèæË±°ÁöÑÂ§öÊ®£ÊÄß„ÄÇÊú¨Êñá‰ªãÁ¥π‰øÑË™ûË™ûË®ÄÊúÄÂ∞èÂ∞çÂü∫Ê∫ñ (RuBLiMP)ÔºåÂÖ∂‰∏≠ÂåÖÂê´ 45k Â∞çÂú®Ë™ûÊ≥ïÊÄß‰∏äÊúâÊâÄ‰∏çÂêå‰∏îÂ≠§Á´ã‰∫ÜÂΩ¢ÊÖã„ÄÅÂè•Ê≥ïÊàñË™ûÁæ©ÁèæË±°ÁöÑÂè•Â≠ê„ÄÇËàáÁèæÊúâÁöÑË™ûË®ÄÊúÄÂ∞èÂ∞çÂü∫Ê∫ñÁõ∏ÂèçÔºåRuBLiMP ÊòØÈÄèÈÅéÂ∞ç‰æÜËá™ÈñãÊîæÊñáÂ≠óË™ûÊñôÂ∫´ÁöÑËá™ÂãïÊ®ôË®ªÂè•Â≠êÂ•óÁî®Ë™ûË®ÄÊìæÂãï‰∏¶‰ªîÁ¥∞Á≠ñÂ±ïÊ∏¨Ë©¶Ë≥áÊñôËÄåÂª∫Á´ãÁöÑ„ÄÇÊàëÂÄëÊèèËø∞Ë≥áÊñôÊî∂ÈõÜÂçîÂÆöÔºå‰∏¶Â±ïÁ§∫Âú®ÂêÑÁ®ÆÊÉÖÂ¢É‰∏ãË©ï‰º∞ 25 ÂÄãË™ûË®ÄÊ®°ÂûãÁöÑÁµêÊûú„ÄÇÊàëÂÄëÁôºÁèæÂª£Ê≥õ‰ΩøÁî®ÁöÑ‰øÑË™ûË™ûË®ÄÊ®°ÂûãÂ∞çÂΩ¢ÊÖãÂíå‰∏ÄËá¥ÊÄßÂ∞éÂêëÁöÑÂ∞çÊØîÂæàÊïèÊÑüÔºå‰ΩÜÂú®ÈúÄË¶ÅÁêÜËß£ÁµêÊßãÈóú‰øÇ„ÄÅÂê¶ÂÆö„ÄÅÈÅûÁßªÊÄßÂíåÊôÇÊÖãÁöÑÁèæË±°‰∏äÂçªËêΩÂæåÊñº‰∫∫È°û„ÄÇRuBLiMP„ÄÅÁ®ãÂºèÁ¢ºÂ∫´ÂíåÂÖ∂‰ªñÊùêÊñôÂùáÂÖ¨ÈñãÊèê‰æõ„ÄÇ

##### **Spiking Convolutional Neural Networks for Text Classification**
2406.19230v1 by Changze Lv, Jianhan Xu, Xiaoqing Zheng

Spiking neural networks (SNNs) offer a promising pathway to implement deep
neural networks (DNNs) in a more energy-efficient manner since their neurons
are sparsely activated and inferences are event-driven. However, there have
been very few works that have demonstrated the efficacy of SNNs in language
tasks partially because it is non-trivial to represent words in the forms of
spikes and to deal with variable-length texts by SNNs. This work presents a
"conversion + fine-tuning" two-step method for training SNNs for text
classification and proposes a simple but effective way to encode pre-trained
word embeddings as spike trains. We show empirically that after fine-tuning
with surrogate gradients, the converted SNNs achieve comparable results to
their DNN counterparts with much less energy consumption across multiple
datasets for both English and Chinese. We also show that such SNNs are more
robust to adversarial attacks than DNNs.

ÊëòË¶ÅÔºöËÑàË°ùÁ•ûÁ∂ìÁ∂≤Ë∑Ø (SNN) Êèê‰æõ‰∫Ü‰∏ÄÁ®Æ‰ª•Êõ¥ÁØÄËÉΩÁöÑÊñπÂºèÂØ¶‰ΩúÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø (DNN) ÁöÑÊúâÂâçÈÄîÈÄîÂæëÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁöÑÁ•ûÁ∂ìÂÖÉÊòØÁ®ÄÁñèÂïüÂãïÁöÑÔºåËÄå‰∏îÊé®Ë´ñÊòØ‰∫ã‰ª∂È©ÖÂãïÁöÑ„ÄÇÁÑ∂ËÄåÔºåÂæàÂ∞ëÊúâÁ†îÁ©∂Ë≠âÊòé SNN Âú®Ë™ûË®Ä‰ªªÂãô‰∏≠ÁöÑÊïàÂäõÔºåÈÉ®ÂàÜÂéüÂõ†ÊòØÂ∞áÂ≠óË©ûË°®Á§∫ÊàêËÑàË°ùÂΩ¢Âºè‰∏¶ËôïÁêÜ SNN ÁöÑÂèØËÆäÈï∑Â∫¶ÊñáÂ≠ó‰∏¶ÈùûÊòì‰∫ã„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊèêÂá∫‰∫Ü‰∏ÄÂÄã„ÄåËΩâÊèõ + ÂæÆË™ø„ÄçÂÖ©Ê≠•È©üÊñπÊ≥ïÔºåÁî®ÊñºË®ìÁ∑¥ SNN ‰ª•ÈÄ≤Ë°åÊñáÂ≠óÂàÜÈ°ûÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂ∞áÈ†êÂÖàË®ìÁ∑¥Â•ΩÁöÑÂ≠óË©ûÂµåÂÖ•Á∑®Á¢ºÁÇ∫ËÑàË°ùÂ∫èÂàó„ÄÇÊàëÂÄëÈÄèÈÅéÂØ¶Ë≠âÈ°ØÁ§∫ÔºåÂú®‰ΩøÁî®‰ª£ÁêÜÊ¢ØÂ∫¶ÂæÆË™øÂæåÔºåËΩâÊèõÂæåÁöÑ SNN Âú®Â§öÂÄãËã±‰∏≠ÊñáË≥áÊñôÈõÜ‰∏äÈÉΩËÉΩÈÅîÂà∞ËàáÂÖ∂ DNN Â∞çÊáâÈ†ÖÁõ∏Áï∂ÁöÑÁµêÊûúÔºåËÄå‰∏îËÉΩËÄóÊõ¥‰Ωé„ÄÇÊàëÂÄë‰πüÈ°ØÁ§∫Âá∫ÔºåÊ≠§È°û SNN ÊØî DNN Êõ¥ËÉΩÊäµÊäóÂ∞çÊäóÊÄßÊîªÊìä„ÄÇ

##### **Tools Fail: Detecting Silent Errors in Faulty Tools**
2406.19228v1 by Jimin Sun, So Yeon Min, Yingshan Chang, Yonatan Bisk

Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not
in their weights, to perform tasks on the web, and even to control robots.
However, most ontologies and surveys of tool-use have assumed the core
challenge for LLMs is choosing the tool. Instead, we introduce a framework for
tools more broadly which guides us to explore a model's ability to detect
"silent" tool errors, and reflect on how to plan. This more directly aligns
with the increasingly popular use of models as tools. We provide an initial
approach to failure recovery with promising results both on a controlled
calculator setting and embodied agent planning.

ÊëòË¶ÅÔºöÂ∑•ÂÖ∑Â∑≤ÊàêÁÇ∫Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊîØÊü±ÔºåËÆìÂÆÉÂÄëËÉΩÂ§†Êì∑ÂèñÊ¨äÈáç‰∏≠Ê≤íÊúâÁöÑÁü•Ë≠òÔºåÂú®Á∂≤Ë∑Ø‰∏≠Âü∑Ë°å‰ªªÂãôÔºåÁîöËá≥ÊéßÂà∂Ê©üÂô®‰∫∫„ÄÇ
ÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏ÈóúÊñºÂ∑•ÂÖ∑‰ΩøÁî®ÁöÑÊú¨È´îË´ñÂíåË™øÊü•ÈÉΩÂÅáË®≠Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊ†∏ÂøÉÊåëÊà∞Âú®ÊñºÈÅ∏ÊìáÂ∑•ÂÖ∑„ÄÇÁõ∏ÂèçÔºåÊàëÂÄëÁÇ∫Êõ¥Âª£Ê≥õÁöÑÂ∑•ÂÖ∑ÂºïÂÖ•‰∏ÄÂÄãÊû∂ÊßãÔºåÂºïÂ∞éÊàëÂÄëÊé¢Á¥¢Ê®°ÂûãÂÅµÊ∏¨„ÄåÈùúÈªò„ÄçÂ∑•ÂÖ∑ÈåØË™§ÁöÑËÉΩÂäõÔºå‰∏¶ÊÄùËÄÉÂ¶Ç‰ΩïË¶èÂäÉ„ÄÇÈÄôÊõ¥Áõ¥Êé•Âú∞Á¨¶ÂêàÊ®°Âûã‰ΩúÁÇ∫Â∑•ÂÖ∑Êó•ÁõäÊôÆÂèäÁöÑ‰ΩøÁî®ÊñπÂºè„ÄÇÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊïÖÈöúÂæ©ÂéüÁöÑÂàùÊ≠•ÊñπÊ≥ïÔºåÂú®ÂèóÊéßË®àÁÆóÂô®Ë®≠ÂÆöÂíåÂÖ∑È´î‰ª£ÁêÜË¶èÂäÉ‰∏≠ÈÉΩÁç≤Âæó‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇ

##### **Aligning Teacher with Student Preferences for Tailored Training Data Generation**
2406.19227v1 by Yantao Liu, Zhao Zhang, Zijun Yao, Shulin Cao, Lei Hou, Juanzi Li

Large Language Models (LLMs) have shown significant promise as copilots in
various tasks. Local deployment of LLMs on edge devices is necessary when
handling privacy-sensitive data or latency-sensitive tasks. The computational
constraints of such devices make direct deployment of powerful large-scale LLMs
impractical, necessitating the Knowledge Distillation from large-scale models
to lightweight models. Lots of work has been done to elicit diversity and
quality training examples from LLMs, but little attention has been paid to
aligning teacher instructional content based on student preferences, akin to
"responsive teaching" in pedagogy. Thus, we propose ARTE, dubbed Aligning
TeacheR with StudenT PreferencEs, a framework that aligns the teacher model
with student preferences to generate tailored training examples for Knowledge
Distillation. Specifically, we elicit draft questions and rationales from the
teacher model, then collect student preferences on these questions and
rationales using students' performance with in-context learning as a proxy, and
finally align the teacher model with student preferences. In the end, we repeat
the first step with the aligned teacher model to elicit tailored training
examples for the student model on the target task. Extensive experiments on
academic benchmarks demonstrate the superiority of ARTE over existing
instruction-tuning datasets distilled from powerful LLMs. Moreover, we
thoroughly investigate the generalization of ARTE, including the generalization
of fine-tuned student models in reasoning ability and the generalization of
aligned teacher models to generate tailored training data across tasks and
students. In summary, our contributions lie in proposing a novel framework for
tailored training example generation, demonstrating its efficacy in
experiments, and investigating the generalization of both student & aligned
teacher models in ARTE.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®Êìî‰ªªÂêÑÁ®Æ‰ªªÂãôÁöÑÂâØÈßïÈßõÊñπÈù¢Â±ïÁèæÂá∫È°ØËëóÁöÑÊΩõÂäõ„ÄÇÂú®ËôïÁêÜÈö±ÁßÅÊïèÊÑüË≥áÊñôÊàñÂª∂ÈÅ≤ÊïèÊÑü‰ªªÂãôÊôÇÔºåÂøÖÈ†àÂú®ÈÇäÁ∑£Ë®≠ÂÇô‰∏äÈÉ®ÁΩ≤ LLM„ÄÇÊ≠§È°ûË®≠ÂÇôÁöÑÈÅãÁÆóÈôêÂà∂‰ΩøÂæóÁõ¥Êé•ÈÉ®ÁΩ≤Âº∑Â§ßÁöÑÂ§ßÂûã LLM ËÆäÂæó‰∏çÂàáÂØ¶ÈöõÔºåÂõ†Ê≠§ÂøÖÈ†àÂ∞áÂ§ßË¶èÊ®°Ê®°ÂûãÁöÑÁü•Ë≠òËí∏È§æÂà∞ËºïÈáèÁ¥öÊ®°Âûã„ÄÇÂ∑≤Á∂ìÂÅö‰∫ÜÂ§ßÈáèÂ∑•‰Ωú‰æÜÂºïÁôº LLM ÁöÑÂ§öÊ®£ÊÄßÂíåÈ´òÂìÅË≥™Ë®ìÁ∑¥ÁØÑ‰æãÔºå‰ΩÜÂæàÂ∞ëÊúâ‰∫∫ÈóúÊ≥®Ê†πÊìöÂ≠∏ÁîüÁöÑÂÅèÂ•Ω‰æÜË™øÊï¥ËÄÅÂ∏´ÁöÑÊïôÂ≠∏ÂÖßÂÆπÔºåÈÄôÈ°û‰ººÊñºÊïôÂ≠∏Ê≥ï‰∏≠ÁöÑ„ÄåÂèçÊáâÂºèÊïôÂ≠∏„Äç„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü ARTEÔºåÁ®±ÁÇ∫ Aligning TeacheR with StudenT PreferencEsÔºåÈÄôÊòØ‰∏ÄÂÄãÊ°ÜÊû∂ÔºåÂÆÉÂ∞áÊïôÂ∏´Ê®°ÂûãËàáÂ≠∏ÁîüÁöÑÂÅèÂ•ΩÁõ∏ÁµêÂêàÔºåÁÇ∫Áü•Ë≠òËí∏È§æÁî¢ÁîüÈáèË∫´ÊâìÈÄ†ÁöÑË®ìÁ∑¥ÁØÑ‰æã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂæûÊïôÂ∏´Ê®°Âûã‰∏≠ÂºïÁôºÂá∫ËçâÁ®øÂïèÈ°åÂíå‰æùÊìöÔºåÁÑ∂Âæå‰ΩøÁî®Â≠∏ÁîüÁöÑË°®Áèæ‰ΩúÁÇ∫ÊÉÖÂ¢ÉÂ≠∏ÁøíÁöÑ‰ª£ÁêÜÔºåÊî∂ÈõÜÂ≠∏ÁîüÂ∞çÈÄô‰∫õÂïèÈ°åÂíå‰æùÊìöÁöÑÂÅèÂ•ΩÔºåÊúÄÂæåÂ∞áÊïôÂ∏´Ê®°ÂûãËàáÂ≠∏ÁîüÁöÑÂÅèÂ•ΩÁõ∏ÁµêÂêà„ÄÇÊúÄÂæåÔºåÊàëÂÄë‰ΩøÁî®ÁµêÂêàÁöÑÊïôÂ∏´Ê®°ÂûãÈáçË§áÁ¨¨‰∏ÄÊ≠•ÔºåÁÇ∫ÁõÆÊ®ô‰ªªÂãô‰∏äÁöÑÂ≠∏ÁîüÊ®°ÂûãÂºïÁôºÈáèË∫´ÊâìÈÄ†ÁöÑË®ìÁ∑¥ÁØÑ‰æã„ÄÇÂú®Â≠∏Ë°ìÂü∫Ê∫ñ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫Ü ARTE ÂÑ™ÊñºÂæûÂº∑Â§ßÁöÑ LLM ‰∏≠ÊèêÂèñÁöÑÁèæÊúâÊïôÂ≠∏Ë™øÊï¥Ë≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæπÂ∫ïÁ†îÁ©∂‰∫Ü ARTE ÁöÑÊ¶ÇÊã¨ÂåñÔºåÂåÖÊã¨ÂæÆË™øÂ≠∏ÁîüÊ®°ÂûãÂú®Êé®ÁêÜËÉΩÂäõ‰∏≠ÁöÑÊ¶ÇÊã¨ÂåñÔºå‰ª•ÂèäÁµêÂêàÁöÑÊïôÂ∏´Ê®°ÂûãÂú®Ë∑®‰ªªÂãôÂíåÂ≠∏Áîü‰∏≠Áî¢ÁîüÈáèË∫´ÊâìÈÄ†ÁöÑË®ìÁ∑¥Ë≥áÊñôÁöÑÊ¶ÇÊã¨Âåñ„ÄÇÁ∏Ω‰πãÔºåÊàëÂÄëÁöÑË≤¢ÁçªÂú®ÊñºÊèêÂá∫‰∏ÄÂÄãÁî®ÊñºÈáèË∫´ÊâìÈÄ†Ë®ìÁ∑¥ÁØÑ‰æãÁîüÊàêÁöÑÊñ∞Á©éÊ°ÜÊû∂ÔºåÂú®ÂØ¶È©ó‰∏≠Â±ïÁ§∫ÂÖ∂ÂäüÊïàÔºå‰∏¶Á†îÁ©∂ ARTE ‰∏≠Â≠∏ÁîüÂíåÁµêÂêàÊïôÂ∏´Ê®°ÂûãÁöÑÊ¶ÇÊã¨Âåñ„ÄÇ

##### **Simulating Classroom Education with LLM-Empowered Agents**
2406.19226v1 by Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhiyuan Liu, Lei Hou, Juanzi Li

Large language models (LLMs) have been employed in various intelligent
educational tasks to assist teaching. While preliminary explorations have
focused on independent LLM-empowered agents for specific educational tasks, the
potential for LLMs within a multi-agent collaborative framework to simulate a
classroom with real user participation remains unexplored. In this work, we
propose SimClass, a multi-agent classroom simulation framework involving user
participation. We recognize representative class roles and introduce a novel
class control mechanism for automatic classroom teaching, and conduct user
experiments in two real-world courses. Utilizing the Flanders Interactive
Analysis System and Community of Inquiry theoretical frame works from
educational analysis, we demonstrate that LLMs can simulate traditional
classroom interaction patterns effectively while enhancing user's experience.
We also observe emergent group behaviors among agents in SimClass, where agents
collaborate to create enlivening interactions in classrooms to improve user
learning process. We hope this work pioneers the application of LLM-empowered
multi-agent systems in virtual classroom teaching.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Ë¢´Áî®ÊñºÂêÑÁ®ÆÊô∫ËÉΩÊïôËÇ≤‰ªªÂãôÔºå‰ª•ÂçîÂä©ÊïôÂ≠∏„ÄÇÈõñÁÑ∂ÂàùÊ≠•Êé¢Á¥¢Â∑≤Â∞àÊ≥®ÊñºÈáùÂ∞çÁâπÂÆöÊïôËÇ≤‰ªªÂãôÁöÑÁç®Á´ã LLM Ë≥¶ËÉΩ‰ª£ÁêÜÔºå‰ΩÜ LLM Âú®Â§ö‰ª£ÁêÜÂçî‰ΩúÊû∂Êßã‰∏≠Ê®°Êì¨ÂÖ∑ÊúâÁúüÂØ¶‰ΩøÁî®ËÄÖÂèÉËàáÁöÑÊïôÂÆ§ÁöÑÊΩõÂäõ‰ªçÊú™Ë¢´Êé¢Á¥¢„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ SimClassÔºå‰∏ÄÂÄãÊ∂âÂèä‰ΩøÁî®ËÄÖÂèÉËàáÁöÑÂ§ö‰ª£ÁêÜÊïôÂÆ§Ê®°Êì¨Ê°ÜÊû∂„ÄÇÊàëÂÄëË≠òÂà•Âá∫ÂÖ∑‰ª£Ë°®ÊÄßÁöÑË™≤Á®ãËßíËâ≤Ôºå‰∏¶ÂºïÂÖ•‰∏ÄÁ®ÆÊñ∞Á©éÁöÑË™≤Á®ãÊéßÂà∂Ê©üÂà∂Áî®ÊñºËá™ÂãïÂåñÊïôÂÆ§ÊïôÂ≠∏Ôºå‰∏¶Âú®ÂÖ©ÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑË™≤Á®ã‰∏≠ÈÄ≤Ë°å‰ΩøÁî®ËÄÖÂØ¶È©ó„ÄÇÂà©Áî®ÊïôËÇ≤ÂàÜÊûê‰∏≠ÁöÑ Flanders ‰∫íÂãïÂàÜÊûêÁ≥ªÁµ±ÂíåÊé¢Á©∂Á§æÁæ§ÁêÜË´ñÊû∂ÊßãÔºåÊàëÂÄëË≠âÊòé LLM ÂèØ‰ª•ÊúâÊïàÊ®°Êì¨ÂÇ≥Áµ±ÁöÑÊïôÂÆ§‰∫íÂãïÊ®°ÂºèÔºåÂêåÊôÇÂ¢ûÂº∑‰ΩøÁî®ËÄÖÁöÑÈ´îÈ©ó„ÄÇÊàëÂÄëÈÇÑËßÄÂØüÂà∞ SimClass ‰∏≠‰ª£ÁêÜ‰πãÈñìÂá∫ÁèæÁæ§È´îË°åÁÇ∫Ôºå‰ª£ÁêÜÊúÉÂçî‰ΩúÂú®ÊïôÂÆ§‰∏≠ÂâµÈÄ†ÁÜ±Áµ°ÁöÑ‰∫íÂãïÔºå‰ª•ÊîπÂñÑ‰ΩøÁî®ËÄÖÁöÑÂ≠∏ÁøíÈÅéÁ®ã„ÄÇÊàëÂÄëÂ∏åÊúõÈÄôÈ†ÖÂ∑•‰ΩúÈñãÂâµ‰∫Ü LLM Ë≥¶ËÉΩÁöÑÂ§ö‰ª£ÁêÜÁ≥ªÁµ±Âú®ËôõÊì¨ÊïôÂÆ§ÊïôÂ≠∏‰∏≠ÁöÑÊáâÁî®„ÄÇ

##### **T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings**
2406.19223v1 by Bj√∂rn Deiseroth, Manuel Brack, Patrick Schramowski, Kristian Kersting, Samuel Weinbach

Tokenizers are crucial for encoding information in Large Language Models, but
their development has recently stagnated, and they contain inherent weaknesses.
Major limitations include computational overhead, ineffective vocabulary use,
and unnecessarily large embedding and head layers. Additionally, their
performance is biased towards a reference corpus, leading to reduced
effectiveness for underrepresented languages.
  To remedy these issues, we propose T-FREE, which directly embeds words
through sparse activation patterns over character triplets, and does not
require a reference corpus. T-FREE inherently exploits morphological
similarities and allows for strong compression of embedding layers. In our
exhaustive experimental evaluation, we achieve competitive downstream
performance with a parameter reduction of more than 85% on these layers.
Further, T-FREE shows significant improvements in cross-lingual transfer
learning.

ÊëòË¶ÅÔºöÂàÜË©ûÂô®Â∞çÊñºÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰∏≠Á∑®Á¢ºË≥áË®äËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÂÆÉÂÄëÁöÑÁôºÂ±ïÊúÄËøëÂÅúÊªØ‰∏çÂâçÔºå‰∏îÂ≠òÂú®Âõ∫ÊúâÁöÑÂº±Èªû„ÄÇ‰∏ªË¶ÅÈôêÂà∂ÂåÖÊã¨Ë®àÁÆóÈñãÈä∑„ÄÅÁÑ°ÊïàÁöÑË©ûÂΩô‰ΩøÁî®Ôºå‰ª•Âèä‰∏çÂøÖË¶ÅÁöÑÈæêÂ§ßÂµåÂÖ•ÂíåÈ†≠ÈÉ®Â±§„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÄëÁöÑÊïàËÉΩÂÅèÂêëÂèÉËÄÉË™ûÊñôÂ∫´ÔºåÂ∞éËá¥Â∞çÊú™ÂÖÖÂàÜË°®Á§∫ÁöÑË™ûË®ÄÁöÑÊïàËÉΩÈôç‰Ωé„ÄÇÁÇ∫‰∫ÜË£úÊïëÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü T-FREEÔºåÂÆÉÈÄèÈÅéÂ≠óÂÖÉ‰∏âÂÖÉÁµÑ‰∏äÁöÑÁ®ÄÁñèÊøÄÊ¥ªÊ®°ÂºèÁõ¥Êé•ÂµåÂÖ•Â≠óË©ûÔºå‰∏î‰∏çÈúÄË¶ÅÂèÉËÄÉË™ûÊñôÂ∫´„ÄÇT-FREE Âõ∫ÊúâÂú∞Âà©Áî®‰∫ÜÂΩ¢ÊÖãÁõ∏‰ººÊÄßÔºå‰∏¶ÂÖÅË®±Â∞çÂµåÂÖ•Â±§ÈÄ≤Ë°åÂº∑Â§ßÁöÑÂ£ìÁ∏Æ„ÄÇÂú®ÊàëÂÄëË©≥Áõ°ÁöÑÂØ¶È©óË©ï‰º∞‰∏≠ÔºåÊàëÂÄëÂú®ÈÄô‰∫õÂ±§‰∏äÂØ¶Áèæ‰∫ÜÂÖ∑ÊúâÁ´∂Áà≠ÂäõÁöÑ‰∏ãÊ∏∏ÊïàËÉΩÔºåÂêåÊôÇÂèÉÊï∏Ê∏õÂ∞ë‰∫Ü 85% ‰ª•‰∏ä„ÄÇÊ≠§Â§ñÔºåT-FREE Âú®Ë∑®Ë™ûË®ÄÂÇ≥Ëº∏Â≠∏Áøí‰∏≠È°ØÁ§∫Âá∫È°ØËëóÁöÑÊîπÈÄ≤„ÄÇ

##### **Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos**
2406.19217v1 by Zhimin Shao, Jialang Xu, Danail Stoyanov, Evangelos B. Mazomenos, Yueming Jin

Despite significant advancements in robotic systems and surgical data
science, ensuring safe and optimal execution in robot-assisted minimally
invasive surgery (RMIS) remains a complex challenge. Current surgical error
detection methods involve two parts: identifying surgical gestures and then
detecting errors within each gesture clip. These methods seldom consider the
rich contextual and semantic information inherent in surgical videos, limiting
their performance due to reliance on accurate gesture identification. Motivated
by the chain-of-thought prompting in natural language processing, this letter
presents a novel and real-time end-to-end error detection framework,
Chain-of-Thought (COG) prompting, leveraging contextual information from
surgical videos. This encompasses two reasoning modules designed to mimic the
decision-making processes of expert surgeons. Concretely, we first design a
Gestural-Visual Reasoning module, which utilizes transformer and attention
architectures for gesture prompting, while the second, a Multi-Scale Temporal
Reasoning module, employs a multi-stage temporal convolutional network with
both slow and fast paths for temporal information extraction. We extensively
validate our method on the public benchmark RMIS dataset JIGSAWS. Our method
encapsulates the reasoning processes inherent to surgical activities enabling
it to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,
and 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on
average, demonstrating the great potential of our approach in enhancing the
safety and efficacy of RMIS procedures and surgical education. The code will be
available.

ÊëòË¶ÅÔºöÂÑòÁÆ°Ê©üÂô®‰∫∫Á≥ªÁµ±ÂíåÊâãË°ìÊï∏ÊìöÁßëÂ≠∏ÊúâÈ°ØËëóÁöÑÈÄ≤Â±ïÔºåÁ¢∫‰øùÊ©üÂô®‰∫∫ËºîÂä©ÂæÆÂâµÊâãË°ì (RMIS) ÁöÑÂÆâÂÖ®ÂíåÊúÄ‰Ω≥Âü∑Ë°å‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖË§áÈõúÁöÑÊåëÊà∞„ÄÇÁõÆÂâçÁöÑË°ì‰∏≠ÈåØË™§ÂÅµÊ∏¨ÊñπÊ≥ïÂåÖÂê´ÂÖ©ÂÄãÈÉ®ÂàÜÔºöË≠òÂà•ÊâãË°ìÊâãÂã¢ÔºåÁÑ∂ÂæåÂú®ÊØèÂÄãÊâãÂã¢ÁâáÊÆµ‰∏≠ÂÅµÊ∏¨ÈåØË™§„ÄÇÈÄô‰∫õÊñπÊ≥ïÂæàÂ∞ëËÄÉÊÖÆÊâãË°ìÂΩ±Áâá‰∏≠Âõ∫ÊúâÁöÑË±êÂØåÊÉÖÂ¢ÉÂíåË™ûÊÑèË≥áË®äÔºåÁî±Êñº‰æùË≥¥ÊñºÊ∫ñÁ¢∫ÁöÑÊâãÂã¢Ë≠òÂà•ÔºåÂõ†Ê≠§ÈôêÂà∂‰∫ÜÂÆÉÂÄëÁöÑÊïàËÉΩ„ÄÇÂèóËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÊÄùÊÉ≥ÈèàÊèêÁ§∫ÁöÑÂïüÁôºÔºåÊú¨‰ø°‰ª∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©é‰∏îÂç≥ÊôÇÁöÑÁ´ØÂà∞Á´ØÈåØË™§ÂÅµÊ∏¨Êû∂ÊßãÔºåÊÄùÊÉ≥Èèà (COG) ÊèêÁ§∫ÔºåÂà©Áî®ÊâãË°ìÂΩ±Áâá‰∏≠ÁöÑÊÉÖÂ¢ÉË≥áË®ä„ÄÇÈÄôÂåÖÂê´ÂÖ©ÂÄãÊé®ÁêÜÊ®°ÁµÑÔºåÊó®Âú®Ê®°Êì¨Â∞àÂÆ∂Â§ñÁßëÈÜ´ÁîüÁöÑÊ±∫Á≠ñÈÅéÁ®ã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊâãÂã¢Ë¶ñË¶∫Êé®ÁêÜÊ®°ÁµÑÔºåÂÆÉÂà©Áî®TransformerÂíåÊ≥®ÊÑèÂäõÊû∂ÊßãÈÄ≤Ë°åÊâãÂã¢ÊèêÁ§∫ÔºåËÄåÁ¨¨‰∫åÂÄãÂ§öÂ∞∫Â∫¶ÊôÇÈñìÊé®ÁêÜÊ®°ÁµÑÊé°Áî®ÂÖ∑ÊúâÊÖ¢ÈÄüÂíåÂø´ÈÄüË∑ØÂæëÁöÑÂ§öÈöéÊÆµÊôÇÈñìÂç∑Á©çÁ∂≤Ë∑ØÈÄ≤Ë°åÊôÇÈñìË≥áË®äËêÉÂèñ„ÄÇÊàëÂÄëÂª£Ê≥õÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÂÖ¨ÈñãÂü∫Ê∫ñ RMIS Ë≥áÊñôÈõÜ JIGSAWS ‰∏äÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÊ¶ÇÊã¨‰∫ÜÊâãË°ìÊ¥ªÂãï‰∏≠Âõ∫ÊúâÁöÑÊé®ÁêÜÈÅéÁ®ãÔºå‰ΩøÂÖ∂Âú® F1 ÂàÜÊï∏‰∏äÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÊäÄË°ì 4.6%ÔºåÂú®Ê∫ñÁ¢∫Â∫¶‰∏äÂÑ™Êñº 4.6%ÔºåÂú® Jaccard ÊåáÊï∏‰∏äÂÑ™Êñº 5.9%ÔºåÂêåÊôÇÂπ≥ÂùáÂú® 6.69 ÊØ´ÁßíÂÖßËôïÁêÜÊØèÂÄãÂΩ±Ê†ºÔºåË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÊèêÂçá RMIS Á®ãÂ∫èÂíåÊâãË°ìÊïôËÇ≤ÁöÑÂÆâÂÖ®ÊÄßÂíåÊúâÊïàÊÄßÊñπÈù¢ÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩõÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÊúÉÊèê‰æõ„ÄÇ

##### **SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation**
2406.19215v1 by Zijun Yao, Weijian Qi, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, Juanzi Li

This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel
adaptive RAG model that extracts self-aware uncertainty of LLMs from their
internal states. SeaKR activates retrieval when the LLMs present high
self-aware uncertainty for generation. To effectively integrate retrieved
knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty
to preserve the snippet that reduces their uncertainty to the utmost. To
facilitate solving complex tasks that require multiple retrievals, SeaKR
utilizes their self-aware uncertainty to choose among different reasoning
strategies. Our experiments on both complex and simple Question Answering
datasets show that SeaKR outperforms existing adaptive RAG methods. We release
our code at https://github.com/THU-KEG/SeaKR.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁ¥π‰∫ÜËá™ÊàëÊÑüÁü•Áü•Ë≠òÊ™¢Á¥¢ (SeaKR)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑËá™ÈÅ©Êáâ RAG Ê®°ÂûãÔºåÂÆÉÂæûÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂÖßÈÉ®ÁãÄÊÖã‰∏≠ÊèêÂèñËá™ÊàëÊÑüÁü•ÁöÑ‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÁï∂ LLM ÂëàÁèæÂá∫È´òËá™ÊàëÊÑüÁü•ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÊôÇÔºåSeaKR ÊúÉÂïüÂãïÊ™¢Á¥¢‰ª•ÈÄ≤Ë°åÁîüÊàê„ÄÇÁÇ∫‰∫ÜÊúâÊïàÊï¥ÂêàÊ™¢Á¥¢Âà∞ÁöÑÁü•Ë≠òÁâáÊÆµÔºåSeaKR ÊúÉÊ†πÊìö LLM ÁöÑËá™ÊàëÊÑüÁü•‰∏çÁ¢∫ÂÆöÊÄßÂ∞çÂÆÉÂÄëÈÄ≤Ë°åÈáçÊñ∞ÊéíÂ∫èÔºå‰ª•‰øùÁïôÊúÄÂ§ßÁ®ãÂ∫¶Èôç‰ΩéÂÖ∂‰∏çÁ¢∫ÂÆöÊÄßÁöÑÁâáÊÆµ„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Ëß£Ê±∫ÈúÄË¶ÅÂ§öÈáçÊ™¢Á¥¢ÁöÑË§áÈõú‰ªªÂãôÔºåSeaKR ÊúÉÂà©Áî®ÂÖ∂Ëá™ÊàëÊÑüÁü•ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÂú®‰∏çÂêåÁöÑÊé®ÁêÜÁ≠ñÁï•‰∏≠ÈÄ≤Ë°åÈÅ∏Êìá„ÄÇÊàëÂÄëÂú®Ë§áÈõúÂíåÁ∞°ÂñÆÁöÑÂïèÁ≠îË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåSeaKR ÂÑ™ÊñºÁèæÊúâÁöÑËá™ÈÅ©Êáâ RAG ÊñπÊ≥ï„ÄÇÊàëÂÄëÂú® https://github.com/THU-KEG/SeaKR/ ‰∏äÁôºÂ∏É‰∫ÜÊàëÂÄëÁöÑÁ®ãÂºèÁ¢º„ÄÇ

##### **BISeizuRe: BERT-Inspired Seizure Data Representation to Improve Epilepsy Monitoring**
2406.19189v1 by Luca Benfenati, Thorir Mar Ingolfsson, Andrea Cossettini, Daniele Jahier Pagliari, Alessio Burrello, Luca Benini

This study presents a novel approach for EEG-based seizure detection
leveraging a BERT-based model. The model, BENDR, undergoes a two-phase training
process. Initially, it is pre-trained on the extensive Temple University
Hospital EEG Corpus (TUEG), a 1.5 TB dataset comprising over 10,000 subjects,
to extract common EEG data patterns. Subsequently, the model is fine-tuned on
the CHB-MIT Scalp EEG Database, consisting of 664 EEG recordings from 24
pediatric patients, of which 198 contain seizure events. Key contributions
include optimizing fine-tuning on the CHB-MIT dataset, where the impact of
model architecture, pre-processing, and post-processing techniques are
thoroughly examined to enhance sensitivity and reduce false positives per hour
(FP/h). We also explored custom training strategies to ascertain the most
effective setup. The model undergoes a novel second pre-training phase before
subject-specific fine-tuning, enhancing its generalization capabilities. The
optimized model demonstrates substantial performance enhancements, achieving as
low as 0.23 FP/h, 2.5$\times$ lower than the baseline model, with a lower but
still acceptable sensitivity rate, showcasing the effectiveness of applying a
BERT-based approach on EEG-based seizure detection.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÂü∫Êñº EEG ÁöÑÁô≤ÁôáÊ™¢Ê∏¨ÊñπÊ≥ïÔºåÂà©Áî®Âü∫Êñº BERT ÁöÑÊ®°Âûã„ÄÇË©≤Ê®°Âûã BENDR Á∂ìÊ≠∑‰∫ÜÂÖ©ÂÄãÈöéÊÆµÁöÑË®ìÁ∑¥ÈÅéÁ®ã„ÄÇÊúÄÂàùÔºåÂÆÉÂú® Temple University Hospital EEG Corpus (TUEG) ‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥ÔºåTUEG ÊòØ‰∏ÄÂÄãÂåÖÂê´ 10,000 Â§öÂÄãÂèóË©¶ËÄÖÁöÑ 1.5 TB Êï∏ÊìöÈõÜÔºåÁî®ÊñºÊèêÂèñÂ∏∏Ë¶ãÁöÑ EEG Êï∏ÊìöÊ®°Âºè„ÄÇÈö®ÂæåÔºåË©≤Ê®°ÂûãÂú® CHB-MIT È†≠ÁöÆ EEG Êï∏ÊìöÂ∫´‰∏äÈÄ≤Ë°åÂæÆË™øÔºåË©≤Êï∏ÊìöÂ∫´ÂåÖÂê´‰æÜËá™ 24 ÂêçÂÖíÁ´•ÊÇ£ËÄÖÁöÑ 664 ÂÄã EEG Ë®òÈåÑÔºåÂÖ∂‰∏≠ 198 ÂÄãÂåÖÂê´Áô≤ÁôáÁôº‰Ωú‰∫ã‰ª∂„ÄÇ‰∏ªË¶ÅË≤¢ÁçªÂåÖÊã¨ÂÑ™Âåñ CHB-MIT Êï∏ÊìöÈõÜ‰∏äÁöÑÂæÆË™øÔºåÂÖ∂‰∏≠ÂæπÂ∫ïÊ™¢Êü•‰∫ÜÊ®°ÂûãÊû∂Êßã„ÄÅÈ†êËôïÁêÜÂíåÂæåËôïÁêÜÊäÄË°ìÁöÑÂΩ±ÈüøÔºå‰ª•ÊèêÈ´òÈùàÊïèÂ∫¶‰∏¶Èôç‰ΩéÊØèÂ∞èÊôÇË™§Â†± (FP/h)„ÄÇÊàëÂÄëÈÇÑÊé¢Á¥¢‰∫ÜËá™ÂÆöÁæ©Ë®ìÁ∑¥Á≠ñÁï•Ôºå‰ª•Á¢∫ÂÆöÊúÄÊúâÊïàÁöÑË®≠ÁΩÆ„ÄÇË©≤Ê®°ÂûãÂú®ÁâπÂÆöÊñºÂèóË©¶ËÄÖÁöÑÂæÆË™ø‰πãÂâçÁ∂ìÊ≠∑‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÁ¨¨‰∫åÈ†êË®ìÁ∑¥ÈöéÊÆµÔºåÂ¢ûÂº∑‰∫ÜÂÖ∂Ê≥õÂåñËÉΩÂäõ„ÄÇÂÑ™ÂåñÁöÑÊ®°ÂûãÂ±ïÁ§∫‰∫ÜÈ°ØËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂØ¶Áèæ‰∫Ü‰ΩéËá≥ 0.23 FP/hÔºåÊØîÂü∫Á∑öÊ®°Âûã‰Ωé 2.5 ÂÄçÔºåÈùàÊïèÂ∫¶ÁéáËºÉ‰Ωé‰ΩÜ‰ªçÂèØÊé•ÂèóÔºåÂ±ïÁ§∫‰∫ÜÂú®Âü∫Êñº EEG ÁöÑÁô≤ÁôáÊ™¢Ê∏¨‰∏≠ÊáâÁî®Âü∫Êñº BERT ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Annotation Errors and NER: A Study with OntoNotes 5.0**
2406.19172v1 by Gabriel Bernier-Colborne, Sowmya Vajjala

Named Entity Recognition (NER) is a well-studied problem in NLP. However,
there is much less focus on studying NER datasets, compared to developing new
NER models. In this paper, we employed three simple techniques to detect
annotation errors in the OntoNotes 5.0 corpus for English NER, which is the
largest available NER corpus for English. Our techniques corrected ~10% of the
sentences in train/dev/test data. In terms of entity mentions, we corrected the
span and/or type of ~8% of mentions in the dataset, while
adding/deleting/splitting/merging a few more. These are large numbers of
changes, considering the size of OntoNotes. We used three NER libraries to
train, evaluate and compare the models trained with the original and the
re-annotated datasets, which showed an average improvement of 1.23% in overall
F-scores, with large (>10%) improvements for some of the entity types. While
our annotation error detection methods are not exhaustive and there is some
manual annotation effort involved, they are largely language agnostic and can
be employed with other NER datasets, and other sequence labelling tasks.

ÊëòË¶ÅÔºöÂëΩÂêçÂØ¶È´îËæ®Ë≠ò (NER) ÊòØËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰∏≠‰∏ÄÂÄãÁ†îÁ©∂ÈÄèÂæπÁöÑÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåËàáÈñãÁôºÊñ∞ÁöÑ NER Ê®°ÂûãÁõ∏ÊØîÔºåÂ∞ç NER Ë≥áÊñôÈõÜÁöÑÁ†îÁ©∂ËºÉÂ∞ë„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé°Áî®‰∫Ü‰∏âÁ®ÆÁ∞°ÂñÆÁöÑÊäÄË°ì‰æÜÂÅµÊ∏¨Ëã±Êñá NER ÁöÑ OntoNotes 5.0 Ë™ûÊñôÂ∫´‰∏≠ÁöÑÊ®ôË®ªÈåØË™§ÔºåËÄåÈÄôÊòØÁõÆÂâçÊúÄÂ§ßÁöÑËã±Êñá NER Ë™ûÊñôÂ∫´„ÄÇÊàëÂÄëÁöÑÊäÄË°ì‰øÆÊ≠£‰∫ÜË®ìÁ∑¥/ÈñãÁôº/Ê∏¨Ë©¶Ë≥áÊñô‰∏≠Á¥Ñ 10% ÁöÑÂè•Â≠ê„ÄÇÂú®ÂØ¶È´îÊ®ôË®ªÊñπÈù¢ÔºåÊàëÂÄë‰øÆÊ≠£‰∫ÜË≥áÊñôÈõÜ‰∏≠Á¥Ñ 8% Ê®ôË®ªÁöÑÁØÑÂúçÂíå/ÊàñÈ°ûÂûãÔºåÂêåÊôÇÊñ∞Â¢û/Âà™Èô§/ÊãÜÂàÜ/Âêà‰Ωµ‰∫Ü‰∏Ä‰∫õÂÖ∂‰ªñÊ®ôË®ª„ÄÇËÄÉÊÖÆÂà∞ OntoNotes ÁöÑË¶èÊ®°ÔºåÈÄô‰∫õÈÉΩÊòØÂ§ßÈáèÁöÑËÆäÊõ¥„ÄÇÊàëÂÄë‰ΩøÁî®‰∏âÂÄã NER ÂáΩÂºèÂ∫´‰æÜË®ìÁ∑¥„ÄÅË©ï‰º∞ÂíåÊØîËºÉ‰ΩøÁî®ÂéüÂßãÂíåÈáçÊñ∞Ê®ôË®ªÁöÑË≥áÊñôÈõÜÊâÄË®ìÁ∑¥ÁöÑÊ®°ÂûãÔºåÁµêÊûúÈ°ØÁ§∫Êï¥È´î F ÂàÜÊï∏Âπ≥ÂùáÊèêÂçá‰∫Ü 1.23%ÔºåÊüê‰∫õÂØ¶È´îÈ°ûÂûãÁöÑÊèêÂçáÂπÖÂ∫¶ÂæàÂ§ß (>10%)„ÄÇÈõñÁÑ∂ÊàëÂÄëÁöÑÊ®ôË®ªÈåØË™§ÂÅµÊ∏¨ÊñπÊ≥ï‰∏¶ÈùûË©≥Áõ°ÁÑ°ÈÅ∫ÔºåËÄå‰∏îÈúÄË¶Å‰∏Ä‰∫õÊâãÂãïÊ®ôË®ªÂ∑•‰ΩúÔºå‰ΩÜÂÆÉÂÄëÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äËàáË™ûË®ÄÁÑ°ÈóúÔºåÂèØÁî®ÊñºÂÖ∂‰ªñ NER Ë≥áÊñôÈõÜÂíåÂÖ∂‰ªñÂ∫èÂàóÊ®ôÁ±§‰ªªÂãô„ÄÇ

##### **The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems**
2406.19170v1 by Judith Sieker, Simeon Junker, Ronja Utescher, Nazia Attari, Heiko Wersing, Hendrik Buschmeier, Sina Zarrie√ü

We examine how users perceive the limitations of an AI system when it
encounters a task that it cannot perform perfectly and whether providing
explanations alongside its answers aids users in constructing an appropriate
mental model of the system's capabilities and limitations. We employ a visual
question answer and explanation task where we control the AI system's
limitations by manipulating the visual inputs: during inference, the system
either processes full-color or grayscale images. Our goal is to determine
whether participants can perceive the limitations of the system. We hypothesize
that explanations will make limited AI capabilities more transparent to users.
However, our results show that explanations do not have this effect. Instead of
allowing users to more accurately assess the limitations of the AI system,
explanations generally increase users' perceptions of the system's competence -
regardless of its actual performance.

ÊëòË¶ÅÔºöÊàëÂÄëÊé¢Ë®é‰ΩøÁî®ËÄÖÂ¶Ç‰ΩïÊÑüÁü• AI Á≥ªÁµ±ÁöÑÈôêÂà∂ÔºåÁï∂ÂÆÉÈÅáÂà∞ÁÑ°Ê≥ïÂÆåÁæéÂü∑Ë°åÁöÑ‰ªªÂãôÊôÇÔºå‰ª•ÂèäÂú®Á≠îÊ°àÊóÅÊèê‰æõËß£ÈáãÊòØÂê¶ËÉΩÂπ´Âä©‰ΩøÁî®ËÄÖÂª∫ÊßãÁ≥ªÁµ±ËÉΩÂäõÂíåÈôêÂà∂ÁöÑÈÅ©Áï∂ÂøÉÊô∫Ê®°Âûã„ÄÇÊàëÂÄëÊé°Áî®Ë¶ñË¶∫ÂïèÁ≠îÂíåËß£Èáã‰ªªÂãôÔºåÂÖ∂‰∏≠ÊàëÂÄëÈÄèÈÅéÊìç‰ΩúË¶ñË¶∫Ëº∏ÂÖ•‰æÜÊéßÂà∂ AI Á≥ªÁµ±ÁöÑÈôêÂà∂ÔºöÂú®Êé®Ë´ñÊúüÈñìÔºåÁ≥ªÁµ±ÊúÉËôïÁêÜÂÖ®ÂΩ©ÊàñÁÅ∞ÈöéÂΩ±ÂÉè„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÁ¢∫ÂÆöÂèÉËàáËÄÖÊòØÂê¶ËÉΩÊÑüÁü•Á≥ªÁµ±ÁöÑÈôêÂà∂„ÄÇÊàëÂÄëÂÅáË®≠Ëß£ÈáãÊúÉËÆìÊúâÈôêÁöÑ AI ËÉΩÂäõÂ∞ç‰ΩøÁî®ËÄÖÊõ¥ÈÄèÊòé„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫Ëß£ÈáãÊ≤íÊúâÈÄôÁ®ÆÊïàÊûú„ÄÇËß£Èáã‰∏¶Êú™ËÆì‰ΩøÁî®ËÄÖÊõ¥Ê∫ñÁ¢∫Âú∞Ë©ï‰º∞ AI Á≥ªÁµ±ÁöÑÈôêÂà∂ÔºåÂèçÂÄíËÆì‰ΩøÁî®ËÄÖÂ∞çÁ≥ªÁµ±ÁöÑËÉΩÂäõÊúâÊõ¥È´òÁöÑÊÑüÁü•ÔºåËÄåËàáÂÖ∂ÂØ¶ÈöõÊïàËÉΩÁÑ°Èóú„ÄÇ

##### **RAVEN: Multitask Retrieval Augmented Vision-Language Learning**
2406.19150v1 by Varun Nagaraj Rao, Siddharth Choudhary, Aditya Deshpande, Ravi Kumar Satzoda, Srikar Appalaraju

The scaling of large language models to encode all the world's knowledge in
model parameters is unsustainable and has exacerbated resource barriers.
Retrieval-Augmented Generation (RAG) presents a potential solution, yet its
application to vision-language models (VLMs) is under explored. Existing
methods focus on models designed for single tasks. Furthermore, they're limited
by the need for resource intensive pre training, additional parameter
requirements, unaddressed modality prioritization and lack of clear benefit
over non-retrieval baselines. This paper introduces RAVEN, a multitask
retrieval augmented VLM framework that enhances base VLMs through efficient,
task specific fine-tuning. By integrating retrieval augmented samples without
the need for additional retrieval-specific parameters, we show that the model
acquires retrieval properties that are effective across multiple tasks. Our
results and extensive ablations across retrieved modalities for the image
captioning and VQA tasks indicate significant performance improvements compared
to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a
+3\% accuracy on specific VQA question types. This underscores the efficacy of
applying RAG approaches to VLMs, marking a stride toward more efficient and
accessible multimodal learning.

ÊëòË¶ÅÔºöÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÊì¥Â±ïÁÇ∫Á∑®Á¢ºÊâÄÊúâ‰∏ñÁïåÁü•Ë≠òÁöÑÊ®°ÂûãÂèÉÊï∏ÊòØ‰∏çÂèØÊåÅÁ∫åÁöÑÔºå‰∏¶‰∏îÂä†Âäá‰∫ÜË≥áÊ∫êÈöúÁ§ô„ÄÇÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºàRAGÔºâÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊΩõÂú®ÁöÑËß£Ê±∫ÊñπÊ°àÔºå‰ΩÜÂÖ∂Âú®Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÔºàVLMÔºâ‰∏≠ÁöÑÊáâÁî®Â∞öÊú™Ë¢´Êé¢Á¥¢„ÄÇÁèæÊúâÊñπÊ≥ïÂÅ¥ÈáçÊñºÁÇ∫ÂñÆ‰∏Ä‰ªªÂãôË®≠Ë®àÁöÑÊ®°Âûã„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÄëÂèóÂà∞Ë≥áÊ∫êÂØÜÈõÜÂûãÈ†êË®ìÁ∑¥„ÄÅÈ°çÂ§ñÂèÉÊï∏ÈúÄÊ±Ç„ÄÅÊú™Ëß£Ê±∫ÁöÑÊ®°ÊÖãÂÑ™ÂÖàÁ¥öÊéíÂ∫è‰ª•ÂèäÁº∫‰πèÂÑ™ÊñºÈùûÊ™¢Á¥¢Âü∫Ê∫ñÁöÑÊòéÈ°ØÂÑ™Âã¢ÁöÑÈôêÂà∂„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü RAVENÔºå‰∏ÄÂÄãÂ§ö‰ªªÂãôÊ™¢Á¥¢Â¢ûÂº∑ VLM Ê°ÜÊû∂ÔºåÂÆÉÈÄöÈÅéÈ´òÊïàÁöÑÁâπÂÆö‰ªªÂãôÂæÆË™ø‰æÜÂ¢ûÂº∑Âü∫Á§é VLM„ÄÇÈÄöÈÅéÊï¥ÂêàÊ™¢Á¥¢Â¢ûÂº∑Ê®£Êú¨ÔºåËÄåÁÑ°ÈúÄÈ°çÂ§ñÁöÑÊ™¢Á¥¢ÁâπÂÆöÂèÉÊï∏ÔºåÊàëÂÄëË°®ÊòéË©≤Ê®°ÂûãÁç≤Âèñ‰∫ÜË∑®Â§öÂÄã‰ªªÂãôÊúâÊïàÁöÑÊ™¢Á¥¢Â±¨ÊÄß„ÄÇÊàëÂÄëÂú®ÂúñÂÉèÊ®ôÈ°åÂíå VQA ‰ªªÂãôÁöÑÊ™¢Á¥¢Ê®°ÊÖã‰∏≠ÈÄ≤Ë°åÁöÑÁµêÊûúÂíåÂª£Ê≥õÊ∂àËûçË°®ÊòéÔºåËàáÊú™Ê™¢Á¥¢Âü∫Ê∫ñÁõ∏ÊØîÔºåÊÄßËÉΩÈ°ØËëóÊèêÂçáÔºåÂú® MSCOCO ‰∏ä +1 CIDErÔºåÂú® NoCaps ‰∏ä +4 CIDErÔºåÂú®ÁâπÂÆö VQA ÂïèÈ°åÈ°ûÂûã‰∏äÊ∫ñÁ¢∫ÁéáÊé•Ëøë +3%„ÄÇÈÄôÂº∑Ë™ø‰∫ÜÂ∞á RAG ÊñπÊ≥ïÊáâÁî®Êñº VLM ÁöÑÊúâÊïàÊÄßÔºåÊ®ôË™åËëóÊúùËëóÊõ¥È´òÊïàÂíåÊõ¥ÊòìÊñºË®™ÂïèÁöÑÂ§öÊ®°ÊÖãÂ≠∏ÁøíÈÇÅÂá∫‰∫Ü‰∏ÄÊ≠•„ÄÇ

##### **BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal Supervision**
2406.19148v1 by Kit Mills Bransby, Arian Beqiri, Woo-Jin Cho Kim, Jorge Oliveira, Agisilaos Chartsias, Alberto Gomez

Neural networks can learn spurious correlations that lead to the correct
prediction in a validation set, but generalise poorly because the predictions
are right for the wrong reason. This undesired learning of naive shortcuts
(Clever Hans effect) can happen for example in echocardiogram view
classification when background cues (e.g. metadata) are biased towards a class
and the model learns to focus on those background features instead of on the
image content. We propose a simple, yet effective random background
augmentation method called BackMix, which samples random backgrounds from other
examples in the training set. By enforcing the background to be uncorrelated
with the outcome, the model learns to focus on the data within the ultrasound
sector and becomes invariant to the regions outside this. We extend our method
in a semi-supervised setting, finding that the positive effects of BackMix are
maintained with as few as 5% of segmentation labels. A loss weighting
mechanism, wBackMix, is also proposed to increase the contribution of the
augmented examples. We validate our method on both in-distribution and
out-of-distribution datasets, demonstrating significant improvements in
classification accuracy, region focus and generalisability. Our source code is
available at: https://github.com/kitbransby/BackMix

ÊëòË¶ÅÔºöÁ•ûÁªèÁΩëÁªúÂèØ‰ª•Â≠¶‰π†ÈîôËØØÁöÑÁõ∏ÂÖ≥ÊÄßÔºåËøô‰ºöÂØºËá¥È™åËØÅÈõÜ‰∏≠È¢ÑÊµãÊ≠£Á°ÆÔºå‰ΩÜÊ≥õÂåñÊÄßËæÉÂ∑ÆÔºåÂõ†‰∏∫È¢ÑÊµãÊ≠£Á°ÆÁöÑÂéüÂõ†ÊòØÈîôËØØÁöÑ„ÄÇËøôÁßçÂ§©ÁúüÊç∑ÂæÑÁöÑ‰∏çËâØÂ≠¶‰π†ÔºàÂÖãËé±ÂºóÊ±âÊñØÊïàÂ∫îÔºâÂèØËÉΩ‰ºöÂèëÁîüÂú®Ë∂ÖÂ£∞ÂøÉÂä®ÂõæËßÜÂõæÂàÜÁ±ª‰∏≠ÔºåÂΩìËÉåÊôØÁ∫øÁ¥¢Ôºà‰æãÂ¶ÇÂÖÉÊï∞ÊçÆÔºâÂÅèÂêë‰∫éÊüê‰∏™Á±ªÂà´Êó∂ÔºåÊ®°Âûã‰ºöÂ≠¶‰π†‰∏ìÊ≥®‰∫éÈÇ£‰∫õËÉåÊôØÁâπÂæÅÔºåËÄå‰∏çÊòØÂõæÂÉèÂÜÖÂÆπ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçï‰ΩÜÊúâÊïàÁöÑÈöèÊú∫ËÉåÊôØÂ¢ûÂº∫ÊñπÊ≥ïÔºåÁß∞‰∏∫ BackMixÔºåÂÆÉ‰ªéËÆ≠ÁªÉÈõÜ‰∏≠ÂÖ∂‰ªñÁ§∫‰æã‰∏≠ÈááÊ†∑ÈöèÊú∫ËÉåÊôØ„ÄÇÈÄöËøáÂº∫Âà∂ËÉåÊôØ‰∏éÁªìÊûúÊó†ÂÖ≥ÔºåÊ®°ÂûãÂ≠¶‰ºö‰∏ìÊ≥®‰∫éË∂ÖÂ£∞ÊâáÂå∫ÂÜÖÁöÑÊï∞ÊçÆÔºåÂπ∂‰∏îÂØπËØ•Âå∫Âüü‰πãÂ§ñÁöÑÂå∫Âüü‰øùÊåÅ‰∏çÂèò„ÄÇÊàë‰ª¨Âú®ÂçäÁõëÁù£ËÆæÁΩÆ‰∏≠Êâ©Â±ï‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÔºåÂèëÁé∞ BackMix ÁöÑÁßØÊûÅÊïàÊûúÂú®ÂàÜÂâ≤Ê†áÁ≠æÂ∞ëËá≥ 5% ÁöÑÊÉÖÂÜµ‰∏ãÂæó‰ª•‰øùÊåÅ„ÄÇËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÊçüÂ§±Âä†ÊùÉÊú∫Âà∂ wBackMixÔºå‰ª•Â¢ûÂä†Â¢ûÂº∫Á§∫‰æãÁöÑË¥°ÁåÆ„ÄÇÊàë‰ª¨Âú®ÂàÜÂ∏ÉÂÜÖÂíåÂàÜÂ∏ÉÂ§ñÊï∞ÊçÆÈõÜ‰∏äÈ™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑÊñπÊ≥ïÔºåËØÅÊòé‰∫ÜÂàÜÁ±ªÂáÜÁ°ÆÊÄß„ÄÅÂå∫ÂüüÂÖ≥Ê≥®ÂíåÊ≥õÂåñÊÄßÁöÑÊòæÁùÄÊèêÈ´ò„ÄÇÊàë‰ª¨ÁöÑÊ∫ê‰ª£Á†ÅÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆËé∑ÂæóÔºöhttps://github.com/kitbransby/BackMix

##### **Resolving Discrepancies in Compute-Optimal Scaling of Language Models**
2406.19146v1 by Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, Yair Carmon

Kaplan et al. and Hoffmann et al. developed influential scaling laws for the
optimal model size as a function of the compute budget, but these laws yield
substantially different predictions. We explain the discrepancy by reproducing
the Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and
identifying three factors causing the difference: last layer computational
cost, warmup duration, and scale-dependent optimizer tuning. With these factors
corrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,
"Chinchilla") scaling law. Counter to a hypothesis of Hoffmann et al., we find
that careful learning rate decay is not essential for the validity of their
scaling law. As a secondary result, we derive scaling laws for the optimal
learning rate and batch size, finding that tuning the AdamW $\beta_2$ parameter
is essential at lower batch sizes.

ÊëòË¶ÅÔºöKaplan Á≠â‰∫∫Âíå Hoffmann Á≠â‰∫∫ÈáùÂ∞çÊúÄ‰Ω≥Ê®°ÂûãÂ§ßÂ∞èÂà∂ÂÆö‰∫ÜÊúâÂΩ±ÈüøÂäõÁöÑÁ∏ÆÊîæÂÆöÂæãÔºå‰ΩúÁÇ∫Ë®àÁÆóÈ†êÁÆóÁöÑÂáΩÊï∏Ôºå‰ΩÜÈÄô‰∫õÂÆöÂæãÁî¢Áîü‰∫ÜÈ°ØËëó‰∏çÂêåÁöÑÈ†êÊ∏¨„ÄÇÊàëÂÄëÈÄèÈÅéÂú®ÂÖ©ÂÄãË≥áÊñôÈõÜÔºàOpenWebText2 Âíå RefinedWebÔºâ‰∏äÈáçÁèæ Kaplan Á∏ÆÊîæÂÆöÂæãÔºå‰∏¶ÊâæÂá∫Â∞éËá¥Â∑ÆÁï∞ÁöÑ‰∏âÂÄãÂõ†Á¥†ÔºöÊúÄÂæå‰∏ÄÂ±§ÁöÑË®àÁÆóÊàêÊú¨„ÄÅÁÜ±Ë∫´ÊåÅÁ∫åÊôÇÈñìÂíå‰æùÊìöË¶èÊ®°Ë™øÊï¥ÊúÄ‰Ω≥ÂåñÂô®„ÄÇÂú®‰øÆÊ≠£ÈÄô‰∫õÂõ†Á¥†ÂæåÔºåÊàëÂÄëÁç≤ÂæóËàá Hoffmann Á≠â‰∫∫ÔºàÂç≥„ÄåChinchilla„ÄçÔºâÁ∏ÆÊîæÂÆöÂæãÊ•µÁÇ∫Áõ∏Á¨¶ÁöÑÁµêÊûú„ÄÇËàá Hoffmann Á≠â‰∫∫ÁöÑÂÅáË®≠Áõ∏ÂèçÔºåÊàëÂÄëÁôºÁèæ‰ªîÁ¥∞ÁöÑÂ≠∏ÁøíÁéáË°∞Ê∏õÂ∞çÊñºÂÖ∂Á∏ÆÊîæÂÆöÂæãÁöÑÊúâÊïàÊÄß‰∏¶ÈùûÂøÖË¶Å„ÄÇ‰ΩúÁÇ∫Ê¨°Ë¶ÅÁµêÊûúÔºåÊàëÂÄëÊé®Â∞éÂá∫ÊúÄ‰Ω≥Â≠∏ÁøíÁéáÂíåÊâπÊ¨°Â§ßÂ∞èÁöÑÁ∏ÆÊîæÂÆöÂæãÔºåÁôºÁèæË™øÊï¥ AdamW $\beta_2$ ÂèÉÊï∏Â∞çÊñºËºÉÂ∞èÁöÑÊâπÊ¨°Â§ßÂ∞èËá≥ÈóúÈáçË¶Å„ÄÇ

##### **YZS-model: A Predictive Model for Organic Drug Solubility Based on Graph Convolutional Networks and Transformer-Attention**
2406.19136v1 by Chenxu Wang, Haowei Ming, Jian He, Yao Lu

The accurate prediction of drug molecule solubility is essential for
determining their therapeutic effectiveness and safety, influencing the drug's
ADME processes. Traditional solubility prediction techniques often fail to
capture the complex nature of molecular tructures, leading to notable
deviations between predictions and actual results. For example, the Discussion
on Advanced Drug-Like Compound Structures. Lusci highlighted issues in
capturing crucial cyclic structural information in molecules with ring
structures. To overcome this issue, our research introduces a novel deep
learning framework combining attention-based transformers, Long Short-Term
Memory (LSTM) networks, and Graph Convolutional Networks (GCN), aimed at
enhancing the precision of solubility predictions. Utilizing a training set of
9,943 compounds and testing on an anticancer compound dataset, our method
achieved a correlation coefficient ($R^2$) of 0.55 and a Root Mean Square Error
(RMSE) of 0.59, which outperforms the benchmark models' scores of 0.52 ($R^2$)
and 0.61 (RMSE). Importantly, in an additional independent test, our model
significantly outperformed the baseline with an RMSE of 1.05 compared to 1.28,
a relative accuracy improvement of 45.9%. This research not only demonstrates
the vast potential of deep learning for improving solubility prediction
accuracy but also offers novel insights for drug design and selection in the
future. Continued efforts will be directed towards optimizing the model
architecture and extending its application to better support the drug
development process, underscoring the pivotal role of deep learning in drug
discovery.

ÊëòË¶ÅÔºöÊ∫ñÁ¢∫È†êÊ∏¨Ëó•Áâ©ÂàÜÂ≠êÁöÑÊ∫∂Ëß£Â∫¶Â∞çÊñºÊ±∫ÂÆöÂÖ∂Ê≤ªÁôÇÊïàÊûúÂíåÂÆâÂÖ®ÊÄßËá≥ÈóúÈáçË¶ÅÔºå‰∏¶ÂΩ±ÈüøËó•Áâ©ÁöÑ ADME Á®ãÂ∫è„ÄÇÂÇ≥Áµ±ÁöÑÊ∫∂Ëß£Â∫¶È†êÊ∏¨ÊäÄË°ìÈÄöÂ∏∏ÁÑ°Ê≥ïÊçïÊçâÂà∞ÂàÜÂ≠êÁµêÊßãÁöÑË§áÈõúÊÄßÔºåÂ∞éËá¥È†êÊ∏¨ËàáÂØ¶ÈöõÁµêÊûú‰πãÈñìÂá∫ÁèæÈ°ØËëóÂÅèÂ∑Æ„ÄÇ‰æãÂ¶ÇÔºåÈóúÊñºÂÖàÈÄ≤È°ûËó•Áâ©ÂåñÂêàÁâ©ÁµêÊßãÁöÑË®éË´ñ„ÄÇLusci Âº∑Ë™ø‰∫ÜÊçïÊçâÂÖ∑ÊúâÁí∞ÁãÄÁµêÊßãÁöÑÂàÜÂ≠ê‰∏≠ÈóúÈçµÁí∞ÁãÄÁµêÊßã‰ø°ÊÅØÁöÑÂïèÈ°å„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ°ÜÊû∂ÔºåÁµêÂêà‰∫ÜÂü∫ÊñºÊ≥®ÊÑèÂäõÁöÑTransformer„ÄÅÈï∑Áü≠ÊúüË®òÊÜ∂ (LSTM) Á∂≤Ë∑ØÂíåÂúñÂΩ¢Âç∑Á©çÁ∂≤Ë∑Ø (GCN)ÔºåÊó®Âú®ÊèêÈ´òÊ∫∂Ëß£Â∫¶È†êÊ∏¨ÁöÑÁ≤æÂ∫¶„ÄÇÂà©Áî® 9,943 ÂÄãÂåñÂêàÁâ©ÁöÑË®ìÁ∑¥ÈõÜ‰∏¶Âú®ÊäóÁôåÂåñÂêàÁâ©Êï∏ÊìöÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞‰∫Ü 0.55 ÁöÑÁõ∏Èóú‰øÇÊï∏ ($R^2$) Âíå 0.59 ÁöÑÂùáÊñπÊ†πË™§Â∑Æ (RMSE)ÔºåÂÑ™ÊñºÂü∫Ê∫ñÊ®°ÂûãÁöÑ 0.52 ($R^2$) Âíå 0.61 (RMSE) ÂàÜÊï∏„ÄÇÈáçË¶ÅÁöÑÊòØÔºåÂú®È°çÂ§ñÁöÑÁç®Á´ãÊ∏¨Ë©¶‰∏≠ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈ°ØËëóÂÑ™ÊñºÂü∫Ê∫ñÔºåRMSE ÁÇ∫ 1.05ÔºåËÄåÂü∫Ê∫ñÁÇ∫ 1.28ÔºåÁõ∏Â∞çÊ∫ñÁ¢∫Â∫¶ÊèêÈ´ò‰∫Ü 45.9%„ÄÇÈÄôÈ†ÖÁ†îÁ©∂‰∏çÂÉÖÂ±ïÁ§∫‰∫ÜÊ∑±Â∫¶Â≠∏ÁøíÂú®ÊèêÈ´òÊ∫∂Ëß£Â∫¶È†êÊ∏¨Ê∫ñÁ¢∫ÊÄßÊñπÈù¢ÁöÑÂ∑®Â§ßÊΩõÂäõÔºåÈÇÑÁÇ∫Êú™‰æÜÁöÑËó•Áâ©Ë®≠Ë®àÂíåÈÅ∏ÊìáÊèê‰æõ‰∫ÜÊñ∞Ë¶ãËß£„ÄÇÊåÅÁ∫åÁöÑÂä™ÂäõÂ∞áËá¥ÂäõÊñºÂÑ™ÂåñÊ®°ÂûãÊû∂Êßã‰∏¶Êì¥Â±ïÂÖ∂ÊáâÁî®Ôºå‰ª•Êõ¥Â•ΩÂú∞ÊîØÊåÅËó•Áâ©ÈñãÁôºÈÅéÁ®ãÔºåÂº∑Ë™øÊ∑±Â∫¶Â≠∏ÁøíÂú®Ëó•Áâ©ÁôºÁèæ‰∏≠ÁöÑÈóúÈçµ‰ΩúÁî®„ÄÇ

##### **DEX-TTS: Diffusion-based EXpressive Text-to-Speech with Style Modeling on Time Variability**
2406.19135v1 by Hyun Joon Park, Jin Sob Kim, Wooseok Shin, Sung Won Han

Expressive Text-to-Speech (TTS) using reference speech has been studied
extensively to synthesize natural speech, but there are limitations to
obtaining well-represented styles and improving model generalization ability.
In this study, we present Diffusion-based EXpressive TTS (DEX-TTS), an acoustic
model designed for reference-based speech synthesis with enhanced style
representations. Based on a general diffusion TTS framework, DEX-TTS includes
encoders and adapters to handle styles extracted from reference speech. Key
innovations contain the differentiation of styles into time-invariant and
time-variant categories for effective style extraction, as well as the design
of encoders and adapters with high generalization ability. In addition, we
introduce overlapping patchify and convolution-frequency patch embedding
strategies to improve DiT-based diffusion networks for TTS. DEX-TTS yields
outstanding performance in terms of objective and subjective evaluation in
English multi-speaker and emotional multi-speaker datasets, without relying on
pre-training strategies. Lastly, the comparison results for the general TTS on
a single-speaker dataset verify the effectiveness of our enhanced diffusion
backbone. Demos are available here.

ÊëòË¶ÅÔºö‰ΩøÁî®ÂèÉËÄÉË™ûÈü≥ÁöÑË°®ÈÅîÂºèÊñáÂ≠óËΩâË™ûÈü≥ (TTS) Â∑≤Ë¢´Âª£Ê≥õÁ†îÁ©∂Áî®ÊñºÂêàÊàêËá™ÁÑ∂Ë™ûÈü≥Ôºå‰ΩÜÂ∞çÊñºÂèñÂæóËâØÂ•ΩÁöÑË°®ÁèæÈ¢®Ê†ºÂíåÊèêÂçáÊ®°ÂûãÊ¶ÇÊã¨ËÉΩÂäõ‰ªçÂ≠òÂú®ÈôêÂà∂„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫Âü∫ÊñºÊì¥Êï£ÁöÑË°®ÈÅîÂºè TTS (DEX-TTS)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈü≥Ë®äÊ®°ÂûãÔºåË®≠Ë®àÁî®ÊñºÂü∫ÊñºÂèÉËÄÉÁöÑË™ûÈü≥ÂêàÊàêÔºå‰∏¶Â¢ûÂº∑È¢®Ê†ºË°®Áèæ„ÄÇÂü∫Êñº‰∏ÄËà¨Êì¥Êï£ TTS Êû∂ÊßãÔºåDEX-TTS ÂåÖÂê´Á∑®Á¢ºÂô®ÂíåÈÅ©ÈÖçÂô®ÔºåÁî®ÊñºËôïÁêÜÂæûÂèÉËÄÉË™ûÈü≥‰∏≠ËêÉÂèñÁöÑÈ¢®Ê†º„ÄÇÈóúÈçµÂâµÊñ∞ÂåÖÂê´Â∞áÈ¢®Ê†ºÂçÄÂàÜÁÇ∫ÊôÇÈñì‰∏çËÆäÂíåÊôÇÈñìËÆäÁï∞È°ûÂà•Ôºå‰ª•ÈÄ≤Ë°åÊúâÊïàÁöÑÈ¢®Ê†ºËêÉÂèñÔºå‰ª•ÂèäË®≠Ë®àÂÖ∑ÂÇôÈ´òÊ¶ÇÊã¨ËÉΩÂäõÁöÑÁ∑®Á¢ºÂô®ÂíåÈÅ©ÈÖçÂô®„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•ÈáçÁñäË≤ºÁâáÂåñÂíåÂç∑Á©çÈ†ªÁéáË≤ºÁâáÂµåÂÖ•Á≠ñÁï•Ôºå‰ª•ÊîπÂñÑÁî®Êñº TTS ÁöÑÂü∫Êñº DiT ÁöÑÊì¥Êï£Á∂≤Ë∑Ø„ÄÇDEX-TTS Âú®ÂÆ¢ËßÄÂíå‰∏ªËßÄË©ï‰º∞ÊñπÈù¢Áî¢ÁîüÂÇëÂá∫ÁöÑË°®ÁèæÔºåÂú®Ëã±Ë™ûÂ§öÈáçÊèöËÅ≤Âô®ÂíåÊÉÖÁ∑íÂ§öÈáçÊèöËÅ≤Âô®Ë≥áÊñôÈõÜÔºåËÄå‰∏ç‰æùË≥¥ÊñºÈ†êË®ìÁ∑¥Á≠ñÁï•„ÄÇÊúÄÂæåÔºåÂú®ÂñÆ‰∏ÄÊèöËÅ≤Âô®Ë≥áÊñôÈõÜ‰∏äÁöÑ‰∏ÄËà¨ TTS ÊØîËºÉÁµêÊûúÈ©óË≠â‰∫ÜÊàëÂÄëÂ¢ûÂº∑ÁöÑÊì¥Êï£‰∏ªÂππÁöÑÊúâÊïàÊÄß„ÄÇÂ±ïÁ§∫ÁØÑ‰æãÂú®Ê≠§ËôïÊèê‰æõ„ÄÇ

##### **Towards Learning Abductive Reasoning using VSA Distributed Representations**
2406.19121v1 by Giacomo Camposampiero, Michael Hersche, Aleksandar Terziƒá, Roger Wattenhofer, Abu Sebastian, Abbas Rahimi

We introduce the Abductive Rule Learner with Context-awareness (ARLC), a
model that solves abstract reasoning tasks based on Learn-VRF. ARLC features a
novel and more broadly applicable training objective for abductive reasoning,
resulting in better interpretability and higher accuracy when solving Raven's
progressive matrices (RPM). ARLC allows both programming domain knowledge and
learning the rules underlying a data distribution. We evaluate ARLC on the
I-RAVEN dataset, showcasing state-of-the-art accuracy across both
in-distribution and out-of-distribution (unseen attribute-rule pairs) tests.
ARLC surpasses neuro-symbolic and connectionist baselines, including large
language models, despite having orders of magnitude fewer parameters. We show
ARLC's robustness to post-programming training by incrementally learning from
examples on top of programmed knowledge, which only improves its performance
and does not result in catastrophic forgetting of the programmed solution. We
validate ARLC's seamless transfer learning from a 2x2 RPM constellation to
unseen constellations. Our code is available at
https://github.com/IBM/abductive-rule-learner-with-context-awareness.

ÊëòË¶ÅÔºöÊàëÂÄëÂºïÂÖ•‰∫ÜÂÖ∑ÊúâËÉåÊôØÊÑüÁü•ÁöÑÊºîÁππË¶èÂâáÂ≠∏ÁøíÂô® (ARLC)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫Êñº Learn-VRF Ëß£Ê±∫ÊäΩË±°Êé®ÁêÜ‰ªªÂãôÁöÑÊ®°Âûã„ÄÇARLC ÂÖ∑ÊúâÊºîÁππÊé®ÁêÜÁöÑÊñ∞Á©é‰∏îÊõ¥Âª£Ê≥õÈÅ©Áî®ÁöÑË®ìÁ∑¥ÁõÆÊ®ôÔºåÂú®Ëß£Ê±∫Èõ∑ÊñáÁöÑÊº∏ÈÄ≤Áü©Èô£ (RPM) ÊôÇÂèØÊèêÈ´òÂèØËß£ÈáãÊÄßÂíåÊ∫ñÁ¢∫ÊÄß„ÄÇARLC ÂÖÅË®±Á®ãÂºèË®≠Ë®àÈ†òÂüüÁü•Ë≠òÂíåÂ≠∏ÁøíË≥áÊñôÂàÜ‰ΩàÁöÑÂü∫Á§éË¶èÂâá„ÄÇÊàëÂÄëÂú® I-RAVEN Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞ ARLCÔºåÂ±ïÁ§∫‰∫ÜÂú®ÂàÜ‰ΩàÂÖßÂíåÂàÜ‰ΩàÂ§ñÔºàÊú™Ë¶ãÂ±¨ÊÄßË¶èÂâáÂ∞çÔºâÊ∏¨Ë©¶‰∏≠ÁöÑÊúÄÂÖàÈÄ≤Ê∫ñÁ¢∫ÊÄß„ÄÇÂÑòÁÆ°ÂèÉÊï∏Êï∏ÈáèÂ∞ëÂπæÂÄãÊï∏ÈáèÁ¥öÔºå‰ΩÜ ARLC Ë∂ÖË∂ä‰∫ÜÁ•ûÁ∂ìÁ¨¶ËôüÂíåÈÄ£Êé•‰∏ªÁæ©Âü∫Ê∫ñÔºåÂåÖÊã¨Â§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÇÊàëÂÄëÈÄöÈÅéÂú®Á®ãÂºèË®≠Ë®àÁü•Ë≠ò‰πã‰∏äÈÄêÊ≠•ÂæûÁØÑ‰æã‰∏≠Â≠∏Áøí‰æÜÂ±ïÁ§∫ ARLC Â∞çÁ®ãÂºèË®≠Ë®àÂæåË®ìÁ∑¥ÁöÑÁ©©ÂÅ•ÊÄßÔºåÈÄôÂè™ÊúÉÊîπÂñÑÂÖ∂ÊïàËÉΩÔºåËÄå‰∏çÊúÉÂ∞éËá¥Á®ãÂºèË®≠Ë®àËß£Ê±∫ÊñπÊ°àÁöÑÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇÊàëÂÄëÈ©óË≠â‰∫Ü ARLC Âæû 2x2 RPM ÊòüÁæ§Âà∞Êú™Ë¶ãÊòüÁæ§ÁöÑÁÑ°Á∏´ÈÅ∑ÁßªÂ≠∏Áøí„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/IBM/abductive-rule-learner-with-context-awareness ÂèñÂæó„ÄÇ

##### **CHEW: A Dataset of CHanging Events in Wikipedia**
2406.19116v1 by Hsuvas Borkakoty, Luis Espinosa-Anke

We introduce CHEW, a novel dataset of changing events in Wikipedia expressed
in naturally occurring text. We use CHEW for probing LLMs for their timeline
understanding of Wikipedia entities and events in generative and classification
experiments. Our results suggest that LLMs, despite having temporal information
available, struggle to construct accurate timelines. We further show the
usefulness of CHEW-derived embeddings for identifying meaning shift.

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π CHEWÔºåÈÄôÊòØ‰∏ÄÂÄãÁ∂≠Âü∫ÁôæÁßë‰∏≠ÈóúÊñºËÆäÂãï‰∫ã‰ª∂ÁöÑÊñ∞Á©éË≥áÊñôÈõÜÔºå‰ª•Ëá™ÁÑ∂ÁôºÁîüÁöÑÊñáÂ≠óË°®ÈÅî„ÄÇÊàëÂÄë‰ΩøÁî® CHEW ‰æÜÊé¢Ê∏¨ LLM Â∞çÁ∂≠Âü∫ÁôæÁßëÂØ¶È´îÂíå‰∫ã‰ª∂ÁöÑÊôÇÈñìÂ∫èÁêÜËß£Ôºå‰∏¶Âú®ÁîüÊàêÂíåÂàÜÈ°ûÂØ¶È©ó‰∏≠ÈÄ≤Ë°åÊé¢Ê∏¨„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂÑòÁÆ° LLM ÂÖ∑ÊúâÊôÇÈñìË≥áË®äÔºå‰ΩÜ‰ªçÈõ£‰ª•Âª∫ÊßãÊ∫ñÁ¢∫ÁöÑÊôÇÈñìÂ∫è„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Â±ïÁ§∫‰∫Ü CHEW Ë°çÁîüÂµåÂÖ•Áî®ÊñºË≠òÂà•ÊÑèÁæ©ËΩâÁßªÁöÑÊïàÁî®„ÄÇ

##### **Computational Life: How Well-formed, Self-replicating Programs Emerge from Simple Interaction**
2406.19108v1 by Blaise Ag√ºera y Arcas, Jyrki Alakuijala, James Evans, Ben Laurie, Alexander Mordvintsev, Eyvind Niklasson, Ettore Randazzo, Luca Versari

The fields of Origin of Life and Artificial Life both question what life is
and how it emerges from a distinct set of "pre-life" dynamics. One common
feature of most substrates where life emerges is a marked shift in dynamics
when self-replication appears. While there are some hypotheses regarding how
self-replicators arose in nature, we know very little about the general
dynamics, computational principles, and necessary conditions for
self-replicators to emerge. This is especially true on "computational
substrates" where interactions involve logical, mathematical, or programming
rules. In this paper we take a step towards understanding how self-replicators
arise by studying several computational substrates based on various simple
programming languages and machine instruction sets. We show that when random,
non self-replicating programs are placed in an environment lacking any explicit
fitness landscape, self-replicators tend to arise. We demonstrate how this
occurs due to random interactions and self-modification, and can happen with
and without background random mutations. We also show how increasingly complex
dynamics continue to emerge following the rise of self-replicators. Finally, we
show a counterexample of a minimalistic programming language where
self-replicators are possible, but so far have not been observed to arise.

ÊëòË¶ÅÔºöÁîüÂëΩËµ∑Ê∫êÂíå‰∫∫Â∑•ÁîüÂëΩÈ†òÂüüÈÉΩË≥™ÁñëÁîüÂëΩÊòØ‰ªÄÈ∫ºÔºå‰ª•ÂèäÂÆÉÊòØÂ¶Ç‰ΩïÂæû‰∏ÄÁµÑ‰∏çÂêåÁöÑ„ÄåÁîüÂëΩÂâç„ÄçÂãïÂäõÂ≠∏‰∏≠Âá∫ÁèæÁöÑ„ÄÇÁîüÂëΩÂá∫ÁèæÁöÑÂ§ßÂ§öÊï∏Âü∫Ë≥™ÁöÑ‰∏ÄÂÄãÂÖ±ÂêåÁâπÂæµÔºåÊòØÂú®Ëá™Ë§áË£ΩÂá∫ÁèæÊôÇÂãïÂäõÂ≠∏ÁöÑÈ°ØËëóËΩâËÆä„ÄÇÈõñÁÑ∂Êúâ‰∏Ä‰∫õÈóúÊñºËá™Ë§áË£ΩÂô®Â¶Ç‰ΩïÂú®Ëá™ÁÑ∂Áïå‰∏≠Áî¢ÁîüÁöÑÂÅáË®≠Ôºå‰ΩÜÊàëÂÄëÂ∞çËá™Ë§áË£ΩÂô®Âá∫ÁèæÁöÑÊôÆÈÅçÂãïÂäõÂ≠∏„ÄÅË®àÁÆóÂéüÂâáÂíåÂøÖË¶ÅÊ¢ù‰ª∂Áü•‰πãÁîöÂ∞ë„ÄÇÂú®‰∫§‰∫íÊ∂âÂèäÈÇèËºØ„ÄÅÊï∏Â≠∏ÊàñÁ®ãÂºèË®≠Ë®àË¶èÂâáÁöÑ„ÄåË®àÁÆóÂü∫Ë≥™„Äç‰∏äÂ∞§ÂÖ∂Â¶ÇÊ≠§„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÁ†îÁ©∂Âü∫ÊñºÂêÑÁ®ÆÁ∞°ÂñÆÁ®ãÂºèË™ûË®ÄÂíåÊ©üÂô®Êåá‰ª§ÈõÜÁöÑÂπæÂÄãË®àÁÆóÂü∫Ë≥™ÔºåÊúùËëó‰∫ÜËß£Ëá™Ë§áË£ΩÂô®Â¶Ç‰ΩïÁî¢ÁîüÁöÑÊñπÂêëÈÇÅÂá∫‰∫Ü‰∏ÄÊ≠•„ÄÇÊàëÂÄëË°®ÊòéÔºåÁï∂Èö®Ê©ü„ÄÅÈùûËá™Ë§áË£ΩÁ®ãÂºèË¢´ÊîæÁΩÆÂú®Áº∫‰πè‰ªª‰ΩïÊòéÁ¢∫ÈÅ©ÊáâÂ∫¶Áí∞Â¢É‰∏≠ÊôÇÔºåËá™Ë§áË£ΩÂô®ÂæÄÂæÄÊúÉÁî¢Áîü„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÈÄôÊòØÂ¶Ç‰ΩïÁî±ÊñºÈö®Ê©ü‰∫§‰∫íÂíåËá™Êàë‰øÆÊîπËÄåÁôºÁîüÁöÑÔºå‰∏¶‰∏îÂèØ‰ª•Âú®ÊúâÊàñÊ≤íÊúâËÉåÊôØÈö®Ê©üÁ™ÅËÆäÁöÑÊÉÖÊ≥Å‰∏ãÁôºÁîü„ÄÇÊàëÂÄëÈÇÑÂ±ïÁ§∫‰∫ÜÈö®ËëóËá™Ë§áË£ΩÂô®ÁöÑËààËµ∑ÔºåÂ¶Ç‰ΩïÊåÅÁ∫åÂá∫ÁèæË∂ä‰æÜË∂äË§áÈõúÁöÑÂãïÂäõÂ≠∏„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏ÄÂÄãÊ•µÁ∞°Á®ãÂºèË™ûË®ÄÁöÑÂèç‰æãÔºåÂÖ∂‰∏≠Ëá™Ë§áË£ΩÂô®ÊòØÂèØËÉΩÁöÑÔºå‰ΩÜÂà∞ÁõÆÂâçÁÇ∫Ê≠¢Â∞öÊú™ËßÄÂØüÂà∞ÂÆÉÂÄëÁöÑÂá∫Áèæ„ÄÇ

##### **Statements: Universal Information Extraction from Tables with Large Language Models for ESG KPIs**
2406.19102v1 by Lokesh Mishra, Sohayl Dhibi, Yusik Kim, Cesar Berrospi Ramis, Shubham Gupta, Michele Dolfi, Peter Staar

Environment, Social, and Governance (ESG) KPIs assess an organization's
performance on issues such as climate change, greenhouse gas emissions, water
consumption, waste management, human rights, diversity, and policies. ESG
reports convey this valuable quantitative information through tables.
Unfortunately, extracting this information is difficult due to high variability
in the table structure as well as content. We propose Statements, a novel
domain agnostic data structure for extracting quantitative facts and related
information. We propose translating tables to statements as a new supervised
deep-learning universal information extraction task. We introduce SemTabNet - a
dataset of over 100K annotated tables. Investigating a family of T5-based
Statement Extraction Models, our best model generates statements which are 82%
similar to the ground-truth (compared to baseline of 21%). We demonstrate the
advantages of statements by applying our model to over 2700 tables from ESG
reports. The homogeneous nature of statements permits exploratory data analysis
on expansive information found in large collections of ESG reports.

ÊëòË¶ÅÔºöÁí∞Â¢É„ÄÅÁ§æÊúÉÂíåÊ≤ªÁêÜ (ESG) KPI Ë©ï‰º∞ÁµÑÁπîÂú®Ê∞£ÂÄôËÆäÈÅ∑„ÄÅÊ∫´ÂÆ§Ê∞£È´îÊéíÊîæ„ÄÅÁî®Ê∞¥Èáè„ÄÅÂª¢Ê£ÑÁâ©ÁÆ°ÁêÜ„ÄÅ‰∫∫Ê¨ä„ÄÅÂ§öÂÖÉÊÄßÂíåÊîøÁ≠ñÁ≠âË≠∞È°å‰∏äÁöÑÁ∏æÊïà„ÄÇESG Â†±ÂëäÈÄèÈÅéË°®Ê†ºÂÇ≥ÈÅîÈÄô‰∫õÊúâÂÉπÂÄºÁöÑÈáèÂåñË≥áË®ä„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÁî±ÊñºË°®Ê†ºÁµêÊßãÂíåÂÖßÂÆπÁöÑÈ´òÂ∫¶ËÆäÁï∞ÊÄßÔºåÊèêÂèñÈÄô‰∫õË≥áË®äÂæàÂõ∞Èõ£„ÄÇÊàëÂÄëÊèêÂá∫Èô≥Ëø∞Ôºå‰∏ÄÁ®ÆÁî®ÊñºÊèêÂèñÈáèÂåñ‰∫ãÂØ¶ÂíåÁõ∏ÈóúË≥áË®äÁöÑÊñ∞Á©éÈ†òÂüüÈùûÁâπÂÆöË≥áÊñôÁµêÊßã„ÄÇÊàëÂÄëÂª∫Ë≠∞Â∞áË°®Ê†ºÁøªË≠ØÊàêÈô≥Ëø∞Ôºå‰ΩúÁÇ∫‰∏ÄÈ†ÖÊñ∞ÁöÑÁõ£Áù£ÂºèÊ∑±Â∫¶Â≠∏ÁøíÈÄöÁî®Ë≥áË®äÊèêÂèñ‰ªªÂãô„ÄÇÊàëÂÄë‰ªãÁ¥π SemTabNet - ‰∏ÄÂÄãË∂ÖÈÅé 10 Ëê¨ÂÄãË®ªËß£Ë°®Ê†ºÁöÑË≥áÊñôÈõÜ„ÄÇÁ†îÁ©∂‰∏ÄÁ≥ªÂàóÂü∫Êñº T5 ÁöÑÈô≥Ëø∞ÊèêÂèñÊ®°ÂûãÔºåÊàëÂÄëÊúÄÂ•ΩÁöÑÊ®°ÂûãÁîüÊàêÁöÑÈô≥Ëø∞ËàáÁúüÂØ¶ÊÉÖÊ≥ÅÊúâ 82% ÁöÑÁõ∏‰ººÂ∫¶ÔºàËàá 21% ÁöÑÂü∫Ê∫ñÁõ∏ÊØîÔºâ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÈô≥Ëø∞ÁöÑÂÑ™ÈªûÔºåÂ∞áÊàëÂÄëÁöÑÊ®°ÂûãÊáâÁî®Êñº ESG Â†±Âëä‰∏≠ÁöÑ 2700 Â§öÂÄãË°®Ê†º„ÄÇÈô≥Ëø∞ÁöÑÂêåË≥™ÊÄßÂÖÅË®±Â∞ç ESG Â†±ÂëäÁöÑÂ§ßÈáèË≥áË®äÈõÜÂêà‰∏≠ÁôºÁèæÁöÑÂª£Ê≥õË≥áÊñôÈÄ≤Ë°åÊé¢Á¥¢ÊÄßË≥áÊñôÂàÜÊûê„ÄÇ

##### **Fairness and Bias in Multimodal AI: A Survey**
2406.19097v1 by Tosin Adewumi, Lama Alkhaled, Namrata Gurung, Goya van Boven, Irene Pagliai

The importance of addressing fairness and bias in artificial intelligence
(AI) systems cannot be over-emphasized. Mainstream media has been awashed with
news of incidents around stereotypes and bias in many of these systems in
recent years. In this survey, we fill a gap with regards to the minimal study
of fairness and bias in Large Multimodal Models (LMMs) compared to Large
Language Models (LLMs), providing 50 examples of datasets and models along with
the challenges affecting them; we identify a new category of quantifying bias
(preuse), in addition to the two well-known ones in the literature: intrinsic
and extrinsic; we critically discuss the various ways researchers are
addressing these challenges. Our method involved two slightly different search
queries on Google Scholar, which revealed that 33,400 and 538,000 links are the
results for the terms "Fairness and bias in Large Multimodal Models" and
"Fairness and bias in Large Language Models", respectively. We believe this
work contributes to filling this gap and providing insight to researchers and
other stakeholders on ways to address the challenge of fairness and bias in
multimodal A!.

ÊëòË¶ÅÔºöÂú®‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÁ≥ªÁªü‰∏≠Ëß£ÂÜ≥ÂÖ¨Âπ≥ÊÄßÂíåÂÅèÂ∑ÆÁöÑÈáçË¶ÅÊÄßÊÄé‰πàÂº∫Ë∞ÉÈÉΩ‰∏ç‰∏∫Ëøá„ÄÇËøëÂπ¥Êù•Ôºå‰∏ªÊµÅÂ™í‰ΩìÂÖÖÊñ•ÁùÄËÆ∏Â§öÊ≠§Á±ªÁ≥ªÁªü‰∏≠ÊúâÂÖ≥ÂàªÊùøÂç∞Ë±°ÂíåÂÅèÂ∑ÆÁöÑ‰∫ã‰ª∂Êñ∞Èóª„ÄÇÂú®Ê≠§Ë∞ÉÊü•‰∏≠ÔºåÊàë‰ª¨Â°´Ë°•‰∫Ü‰∏éÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMÔºâÁõ∏ÊØîÔºåÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÂØπÂÖ¨Âπ≥ÊÄßÂíåÂÅèÂ∑ÆÁöÑÁ†îÁ©∂ÊúÄÂ∞ëËøô‰∏ÄÂ∑ÆË∑ùÔºåÊèê‰æõ‰∫Ü 50 ‰∏™Êï∞ÊçÆÈõÜÂíåÊ®°ÂûãÁöÑÁ§∫‰æã‰ª•ÂèäÂΩ±ÂìçÂÆÉ‰ª¨ÁöÑÊåëÊàòÔºõÈô§‰∫ÜÊñáÁåÆ‰∏≠‰ºóÊâÄÂë®Áü•ÁöÑÂÜÖÂú®ÂíåÂ§ñÂú®‰∏§ÁßçÁ±ªÂà´Â§ñÔºåÊàë‰ª¨ËøòÁ°ÆÂÆö‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈáèÂåñÂÅèÂ∑ÆÁöÑÁ±ªÂà´ÔºàÈ¢ÑÁî®ÔºâÔºõÊàë‰ª¨ÊâπÂà§ÊÄßÂú∞ËÆ®ËÆ∫‰∫ÜÁ†îÁ©∂‰∫∫ÂëòËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÁöÑÂêÑÁßçÊñπÂºè„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÊ∂âÂèäÂú® Google Scholar ‰∏äËøõË°å‰∏§Ê¨°Áï•Êúâ‰∏çÂêåÁöÑÊêúÁ¥¢Êü•ËØ¢ÔºåÁªìÊûúÊòæÁ§∫‚ÄúÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄßÂíåÂÅèÂ∑Æ‚ÄùÂíå‚ÄúÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄßÂíåÂÅèÂ∑Æ‚ÄùËøô‰∏§‰∏™ÊúØËØ≠ÁöÑÊêúÁ¥¢ÁªìÊûúÂàÜÂà´‰∏∫ 33,400 Âíå 538,000 ‰∏™ÈìæÊé•„ÄÇÊàë‰ª¨Áõ∏‰ø°ËøôÈ°πÂ∑•‰ΩúÊúâÂä©‰∫éÂ°´Ë°•Ëøô‰∏ÄÁ©∫ÁôΩÔºåÂπ∂‰∏∫Á†îÁ©∂‰∫∫ÂëòÂíåÂÖ∂‰ªñÂà©ÁõäÁõ∏ÂÖ≥ËÄÖÊèê‰æõÊúâÂÖ≥Ëß£ÂÜ≥Â§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩ‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄßÂíåÂÅèÂ∑ÆÊåëÊàòÁöÑÊñπÊ≥ïÁöÑËßÅËß£„ÄÇ

##### **Dimensions underlying the representational alignment of deep neural networks with humans**
2406.19087v1 by Florian P. Mahner, Lukas Muttenthaler, Umut G√º√ßl√º, Martin N. Hebart

Determining the similarities and differences between humans and artificial
intelligence is an important goal both in machine learning and cognitive
neuroscience. However, similarities in representations only inform us about the
degree of alignment, not the factors that determine it. Drawing upon recent
developments in cognitive science, we propose a generic framework for yielding
comparable representations in humans and deep neural networks (DNN). Applying
this framework to humans and a DNN model of natural images revealed a
low-dimensional DNN embedding of both visual and semantic dimensions. In
contrast to humans, DNNs exhibited a clear dominance of visual over semantic
features, indicating divergent strategies for representing images. While
in-silico experiments showed seemingly-consistent interpretability of DNN
dimensions, a direct comparison between human and DNN representations revealed
substantial differences in how they process images. By making representations
directly comparable, our results reveal important challenges for
representational alignment, offering a means for improving their comparability.

ÊëòË¶ÅÔºöÁ¢∫ÂÆö‰∫∫È°ûÂíå‰∫∫Â∑•Êô∫ÊÖß‰πãÈñìÁöÑÁõ∏‰ººÊÄßÂíåÂ∑ÆÁï∞ÔºåÊòØÊ©üÂô®Â≠∏ÁøíÂíåË™çÁü•Á•ûÁ∂ìÁßëÂ≠∏‰∏≠‰∏ÄÂÄãÈáçË¶ÅÁöÑÁõÆÊ®ô„ÄÇÁÑ∂ËÄåÔºåË°®Âæµ‰∏≠ÁöÑÁõ∏‰ººÊÄßÂÉÖËÉΩÂëäË®¥ÊàëÂÄëÂ∞çÈΩäÁöÑÁ®ãÂ∫¶ÔºåËÄå‰∏çÊòØÊ±∫ÂÆöÂÆÉÁöÑÂõ†Á¥†„ÄÇÂÄüÈëíË™çÁü•ÁßëÂ≠∏ÁöÑÊúÄÊñ∞ÁôºÂ±ïÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂú®‰∫∫È°ûÂíåÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÔºàDNNÔºâ‰∏≠Áî¢ÁîüÂèØÊØîËºÉË°®ÂæµÁöÑÈÄöÁî®Ê°ÜÊû∂„ÄÇÂ∞áÈÄôÂÄãÊ°ÜÊû∂ÊáâÁî®Êñº‰∫∫È°ûÂíåËá™ÁÑ∂ÂΩ±ÂÉèÁöÑ DNN Ê®°ÂûãÔºåÊè≠Á§∫‰∫ÜË¶ñË¶∫ÂíåË™ûÁæ©Á∂≠Â∫¶ÁöÑ‰ΩéÁ∂≠ DNN ÂµåÂÖ•„ÄÇËàá‰∫∫È°ûÁõ∏ÂèçÔºåDNN Ë°®ÁèæÂá∫Ë¶ñË¶∫ÊòéÈ°ØÂÑ™ÊñºË™ûÁæ©ÁâπÂæµÔºåË°®ÊòéË°®ÂæµÂΩ±ÂÉèÁöÑ‰∏çÂêåÁ≠ñÁï•„ÄÇÈõñÁÑ∂ÈõªËÖ¶Ê®°Êì¨ÂØ¶È©óÈ°ØÁ§∫ DNN Á∂≠Â∫¶ÁöÑÂèØËß£ÈáãÊÄßÁúã‰ºº‰∏ÄËá¥Ôºå‰ΩÜ‰∫∫È°ûÂíå DNN Ë°®Âæµ‰πãÈñìÁöÑÁõ¥Êé•ÊØîËºÉÊè≠Á§∫‰∫Ü‰ªñÂÄëÂú®ËôïÁêÜÂΩ±ÂÉèÊñπÂºè‰∏äÁöÑÈáçÂ§ßÂ∑ÆÁï∞„ÄÇÈÄèÈÅéËÆìË°®ÂæµÁõ¥Êé•ÂèØÊØîËºÉÔºåÊàëÂÄëÁöÑÁµêÊûúÊè≠Á§∫‰∫ÜË°®ÂæµÂ∞çÈΩäÁöÑÈáçË¶ÅÊåëÊà∞ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊîπÂñÑÂÖ∂ÂèØÊØîËºÉÊÄßÁöÑÊñπÊ≥ï„ÄÇ

##### **AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries**
2406.19073v1 by Irina Saparina, Mirella Lapata

Practical semantic parsers are expected to understand user utterances and map
them to executable programs, even when these are ambiguous. We introduce a new
benchmark, AMBROSIA, which we hope will inform and inspire the development of
text-to-SQL parsers capable of recognizing and interpreting ambiguous requests.
Our dataset contains questions showcasing three different types of ambiguity
(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,
and corresponding SQL queries. In each case, the ambiguity persists even when
the database context is provided. This is achieved through a novel approach
that involves controlled generation of databases from scratch. We benchmark
various LLMs on AMBROSIA, revealing that even the most advanced models struggle
to identify and interpret ambiguity in questions.

ÊëòË¶ÅÔºöÂØ¶Áî®ÁöÑË™ûÁæ©Ëß£ÊûêÂô®È†êÊúüËÉΩÁêÜËß£‰ΩøÁî®ËÄÖÁöÑË©±Ë™ûÔºå‰∏¶Â∞áÂÖ∂ËΩâÊèõÊàêÂèØÂü∑Ë°åÁöÑÁ®ãÂºèÔºåÂç≥‰ΩøÈÄô‰∫õË©±Ë™ûÊúâÊ≠ßÁæ©„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Ê∫ñ AMBROSIAÔºåÊàëÂÄëÂ∏åÊúõÂÆÉËÉΩÊèê‰æõË≥áË®ä‰∏¶ÊøÄÂãµËÉΩËæ®Ë≠òÂíåË©ÆÈáãÊ≠ßÁæ©Ë´ãÊ±ÇÁöÑÊñáÂ≠óËΩâ SQL Ëß£ÊûêÂô®ÁöÑÈñãÁôº„ÄÇÊàëÂÄëÁöÑË≥áÊñôÈõÜÂåÖÂê´Â±ïÁ§∫‰∏âÁ®Æ‰∏çÂêåÈ°ûÂûãÊ≠ßÁæ©ÔºàÁØÑÂúçÊ≠ßÁæ©„ÄÅ‰æùÈôÑÊ≠ßÁæ©ÂíåÊ®°Á≥äÊÄßÔºâÁöÑÂïèÈ°å„ÄÅÂÖ∂Ë©ÆÈáãÂíåÂ∞çÊáâÁöÑ SQL Êü•Ë©¢„ÄÇÂú®ÊØèÁ®ÆÊÉÖÊ≥Å‰∏ãÔºåÂç≥‰ΩøÊèê‰æõ‰∫ÜË≥áÊñôÂ∫´ËÉåÊôØÔºåÊ≠ßÁæ©‰ªçÁÑ∂Â≠òÂú®„ÄÇÈÄôÊòØÈÄèÈÅé‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÂØ¶ÁèæÁöÑÔºåË©≤ÊñπÊ≥ïÂåÖÂê´ÂæûÈ†≠ÈñãÂßãÂèóÊéßÁîüÊàêË≥áÊñôÂ∫´„ÄÇÊàëÂÄëÂú® AMBROSIA ‰∏äÂ∞çÂêÑÁ®Æ LLM ÈÄ≤Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÁµêÊûúÈ°ØÁ§∫Âç≥‰ΩøÊòØÊúÄÂÖàÈÄ≤ÁöÑÊ®°Âûã‰πüÂæàÈõ£Ëæ®Ë≠òÂíåË©ÆÈáãÂïèÈ°å‰∏≠ÁöÑÊ≠ßÁæ©„ÄÇ

##### **EmPO: Theory-Driven Dataset Construction for Empathetic Response Generation through Preference Optimization**
2406.19071v1 by Ondrej Sotolar

Empathetic response generation is a desirable aspect of conversational
agents, crucial for facilitating engaging and emotionally intelligent
multi-turn conversations between humans and machines. Leveraging large language
models for this task has shown promising results, yet challenges persist in
ensuring both the empathetic quality of the responses and retention of the
generalization performance of the models. In this paper, we propose a novel
approach where we construct theory-driven preference datasets and use them to
align LLMs with preference optimization algorithms to address these challenges.
To measure empathetic response generation, we employ the EmpatheticDialogues
dataset, assessing empathy with the diff-EPITOME and BERTscore metrics, and
evaluate the generalization performance on the MMLU benchmark. We make all
datasets, source code, and models publicly available.

ÊëòË¶ÅÔºöÂêåÁêÜÂõûÊáâÁîüÊàêÊòØÂ∞çË©±‰ª£ÁêÜ‰∫∫ÁêÜÊÉ≥ÁöÑÁâπÈªûÔºåÂ∞çÊñº‰øÉÈÄ≤‰∫∫È°ûËàáÊ©üÂô®‰πãÈñìÂºï‰∫∫ÂÖ•Âãù‰∏îÂØåÊúâÊÉÖÊÑüÊô∫ÊÖßÁöÑÂ§öËº™Â∞çË©±Ëá≥ÈóúÈáçË¶Å„ÄÇÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜÂü∑Ë°åÊ≠§‰ªªÂãôÂ∑≤Â±ïÁèæÂá∫ÊúâÂâçÊôØÁöÑÊàêÊûúÔºåÁÑ∂ËÄåÔºåÂú®Á¢∫‰øùÂõûÊáâÁöÑÂêåÁêÜÂìÅË≥™Âíå‰øùÁïôÊ®°ÂûãÁöÑÊ¶ÇÂåñÊïàËÉΩÊñπÈù¢‰ªçÂ≠òÂú®ÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊàëÂÄëÂª∫Êßã‰∫ÜÁêÜË´ñÈ©ÖÂãïÁöÑÂÅèÂ•ΩË≥áÊñôÈõÜÔºå‰∏¶‰ΩøÁî®ÂÆÉÂÄëÂ∞á LLM ËàáÂÅèÂ•ΩÊúÄ‰Ω≥ÂåñÊºîÁÆóÊ≥ïÂ∞çÈΩäÔºå‰ª•ÊáâÂ∞çÈÄô‰∫õÊåëÊà∞„ÄÇÁÇ∫‰∫ÜË°°ÈáèÂêåÁêÜÂõûÊáâÁîüÊàêÔºåÊàëÂÄëÊé°Áî®‰∫Ü EmpatheticDialogues Ë≥áÊñôÈõÜÔºå‰ΩøÁî® diff-EPITOME Âíå BERTscore ÊåáÊ®ôË©ï‰º∞ÂêåÁêÜÂøÉÔºå‰∏¶Âú® MMLU Âü∫Ê∫ñ‰∏äË©ï‰º∞Ê¶ÇÂåñÊïàËÉΩ„ÄÇÊàëÂÄëÂÖ¨ÈñãÊèê‰æõÊâÄÊúâË≥áÊñôÈõÜ„ÄÅÂéüÂßãÁ¢ºÂíåÊ®°Âûã„ÄÇ

##### **STBench: Assessing the Ability of Large Language Models in Spatio-Temporal Analysis**
2406.19065v1 by Wenbin Li, Di Yao, Ruibo Zhao, Wenjie Chen, Zijie Xu, Chengxue Luo, Chang Gong, Quanliang Jing, Haining Tan, Jingping Bi

The rapid evolution of large language models (LLMs) holds promise for
reforming the methodology of spatio-temporal data mining. However, current
works for evaluating the spatio-temporal understanding capability of LLMs are
somewhat limited and biased. These works either fail to incorporate the latest
language models or only focus on assessing the memorized spatio-temporal
knowledge. To address this gap, this paper dissects LLMs' capability of
spatio-temporal data into four distinct dimensions: knowledge comprehension,
spatio-temporal reasoning, accurate computation, and downstream applications.
We curate several natural language question-answer tasks for each category and
build the benchmark dataset, namely STBench, containing 13 distinct tasks and
over 60,000 QA pairs. Moreover, we have assessed the capabilities of 13 LLMs,
such as GPT-4o, Gemma and Mistral. Experimental results reveal that existing
LLMs show remarkable performance on knowledge comprehension and spatio-temporal
reasoning tasks, with potential for further enhancement on other tasks through
in-context learning, chain-of-though prompting, and fine-tuning. The code and
datasets of STBench are released on https://github.com/LwbXc/STBench.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÁôºÂ±ïÔºåÊúâÊúõÊîπÈù©ÊôÇÁ©∫Ë≥áÊñôÊé¢ÂãòÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁî®ÊñºË©ï‰º∞ LLM ÊôÇÁ©∫ÁêÜËß£ËÉΩÂäõÁöÑÁ†îÁ©∂ÔºåÂ§öÂ∞ëÊúâ‰∫õÈôêÂà∂ÂíåÂÅèË¶ã„ÄÇÈÄô‰∫õÁ†îÁ©∂Ë¶Å‰∏çÊòØÊ≤íÊúâÁ¥çÂÖ•ÊúÄÊñ∞ÁöÑË™ûË®ÄÊ®°ÂûãÔºåÂ∞±ÊòØÂè™ËëóÈáçÊñºË©ï‰º∞Ë®òÊÜ∂ÁöÑÊôÇÁ©∫Áü•Ë≠ò„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊú¨ÊñáÂ∞á LLM Â∞çÊôÇÁ©∫Ë≥áÊñôÁöÑËÉΩÂäõÂâñÊûêÊàêÂõõÂÄã‰∏çÂêåÁöÑÈù¢ÂêëÔºöÁü•Ë≠òÁêÜËß£„ÄÅÊôÇÁ©∫Êé®ÁêÜ„ÄÅÊ∫ñÁ¢∫ÈÅãÁÆóÂíå‰∏ãÊ∏∏ÊáâÁî®„ÄÇÊàëÂÄëÁÇ∫ÊØèÂÄãÈ°ûÂà•Á≠ñÂäÉ‰∫ÜÂπæÂÄãËá™ÁÑ∂Ë™ûË®ÄÂïèÁ≠î‰ªªÂãôÔºå‰∏¶Âª∫Á´ãÂü∫Ê∫ñË≥áÊñôÈõÜÔºå‰πüÂ∞±ÊòØ STBenchÔºåÂÖ∂‰∏≠ÂåÖÂê´ 13 ÂÄã‰∏çÂêåÁöÑ‰ªªÂãôÂíåË∂ÖÈÅé 60,000 ÂÄãÂïèÁ≠îÈÖçÂ∞ç„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË©ï‰º∞‰∫Ü 13 ÂÄã LLM ÁöÑËÉΩÂäõÔºå‰æãÂ¶Ç GPT-4o„ÄÅGemma Âíå Mistral„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÁèæÊúâÁöÑ LLM Âú®Áü•Ë≠òÁêÜËß£ÂíåÊôÇÁ©∫Êé®ÁêÜ‰ªªÂãô‰∏äË°®ÁèæÂÇëÂá∫Ôºå‰∏¶ÊúâÊΩõÂäõÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏Áøí„ÄÅÊÄùËÄÉÈèàÊèêÁ§∫ÂíåÂæÆË™øÔºåÂú®ÂÖ∂‰ªñ‰ªªÂãô‰∏äÈÄ≤‰∏ÄÊ≠•ÊèêÂçá„ÄÇSTBench ÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂ∑≤Êñº https://github.com/LwbXc/STBench ‰∏äÁôºÂ∏É„ÄÇ

##### **Segment Anything Model for automated image data annotation: empirical studies using text prompts from Grounding DINO**
2406.19057v1 by Fuseini Mumuni, Alhassan Mumuni

Grounding DINO and the Segment Anything Model (SAM) have achieved impressive
performance in zero-shot object detection and image segmentation, respectively.
Together, they have a great potential in revolutionizing zero-shot semantic
segmentation or data annotation. Yet, in specialized domains like medical image
segmentation, objects of interest (e.g., organs, tissues, and tumors) may not
fall in existing class names. To address this problem, the referring expression
comprehension (REC) ability of Grounding DINO is leveraged to detect arbitrary
targets by their language descriptions. However, recent studies have
highlighted severe limitation of the REC framework in this application setting
owing to its tendency to make false positive predictions when the target is
absent in the given image. And, while this bottleneck is central to the
prospect of open-set semantic segmentation, it is still largely unknown how
much improvement can be achieved by studying the prediction errors. To this
end, we perform empirical studies on eight publicly available datasets and
reveal that these errors consistently follow a predictable pattern and can,
thus, be mitigated by a simple strategy. Specifically, we show that these false
positive detections with appreciable confidence scores generally occupy large
image areas and can usually be filtered by their relative sizes. More
importantly, we expect these observations to inspire future research in
improving REC-based detection and automated segmentation. Using this technique,
we evaluate the performance of SAM on multiple datasets from various
specialized domains and report significant improvement in segmentation
performance and annotation time savings over manual approaches.

ÊëòË¶ÅÔºöGrounding DINO Âíå Segment Anything Model (SAM) Âú®Èõ∂Ê¨°Áâ©È´îÂÅµÊ∏¨ÂíåÂΩ±ÂÉèÂàÜÂâ≤‰∏≠ÂàÜÂà•ÂèñÂæó‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑË°®Áèæ„ÄÇÂÆÉÂÄëÂÖ±ÂêåÊìÅÊúâÂú®Èõ∂Ê¨°Ë™ûÊÑèÂàÜÂâ≤ÊàñË≥áÊñôÊ®ôË®ª‰∏≠ÊéÄËµ∑Èù©ÂëΩÁöÑÂ∑®Â§ßÊΩõÂäõ„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤Á≠âÂ∞àÊ•≠È†òÂüü‰∏≠ÔºåÊÑüËààË∂£ÁöÑÁâ©È´îÔºà‰æãÂ¶ÇÂô®ÂÆò„ÄÅÁµÑÁπîÂíåËÖ´Áò§ÔºâÂèØËÉΩ‰∏çÂú®ÁèæÊúâÁöÑÈ°ûÂà•ÂêçÁ®±‰∏≠„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂà©Áî® Grounding DINO ÁöÑÊåáÊ∂âË°®ÈÅîÁêÜËß£ (REC) ËÉΩÂäõÔºåÈÄèÈÅéË™ûË®ÄÊèèËø∞‰æÜÂÅµÊ∏¨‰ªªÊÑèÁõÆÊ®ô„ÄÇÁÑ∂ËÄåÔºåÊúÄËøëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫Ü REC Êû∂ÊßãÂú®ÈÄôÂÄãÊáâÁî®Ë®≠ÂÆö‰∏≠ÁöÑÂö¥ÈáçÈôêÂà∂ÔºåÂõ†ÁÇ∫Áï∂ÁõÆÊ®ô‰∏çÂ≠òÂú®ÊñºÁµ¶ÂÆöÁöÑÂΩ±ÂÉè‰∏≠ÊôÇÔºåÂÆÉÂÇæÂêëÊñºÂÅöÂá∫ÂÅáÈôΩÊÄßÈ†êÊ∏¨„ÄÇËÄå‰∏îÔºåÈõñÁÑ∂Ê≠§Áì∂È†∏Â∞çÊñºÈñãÊîæÂºèË™ûÊÑèÂàÜÂâ≤ÁöÑÂâçÊôØËá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÈÄèÈÅéÁ†îÁ©∂È†êÊ∏¨Ë™§Â∑ÆËÉΩÁç≤ÂæóÂ§öÂ∞ëÊîπÂñÑ‰ªçÊòØÊú™Áü•ÁöÑ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂ∞çÂÖ´ÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÂØ¶Ë≠âÁ†îÁ©∂Ôºå‰∏¶Êè≠Á§∫ÈÄô‰∫õË™§Â∑ÆÂßãÁµÇÈÅµÂæ™ÂèØÈ†êÊ∏¨ÁöÑÊ®°ÂºèÔºåÂõ†Ê≠§ÔºåÂèØ‰ª•ÈÄèÈÅé‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÁ≠ñÁï•‰æÜÊ∏õËºï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëË°®ÊòéÈÄô‰∫õÂÖ∑ÊúâÂèØËßÄÁΩÆ‰ø°Â∫¶ÁöÑÂÅáÈôΩÊÄßÂÅµÊ∏¨ÈÄöÂ∏∏‰ΩîÊìöËºÉÂ§ßÁöÑÂΩ±ÂÉèÂçÄÂüüÔºå‰∏¶‰∏îÈÄöÂ∏∏ÂèØ‰ª•Ê†πÊìöÂÆÉÂÄëÁöÑÁõ∏Â∞çÂ§ßÂ∞è‰æÜÈÅéÊøæ„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÈ†êÊúüÈÄô‰∫õËßÄÂØüÁµêÊûúÂ∞áÊøÄÂãµÊú™‰æÜÂú®ÊîπÂñÑÂü∫Êñº REC ÁöÑÂÅµÊ∏¨ÂíåËá™ÂãïÂàÜÂâ≤ÁöÑÁ†îÁ©∂„ÄÇ‰ΩøÁî®Ê≠§ÊäÄË°ìÔºåÊàëÂÄëË©ï‰º∞‰∫Ü SAM Âú®‰æÜËá™ÂêÑÁ®ÆÂ∞àÊ•≠È†òÂüüÁöÑÂ§öÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩÔºå‰∏¶Â†±ÂëäÂú®ÂàÜÂâ≤ÊïàËÉΩÂíåÊ®ôË®ªÊôÇÈñìÁØÄÁúÅ‰∏äÁõ∏ËºÉÊñºÊâãÂãïÊñπÊ≥ïÊúâÈ°ØËëóÁöÑÊîπÂñÑ„ÄÇ

##### **A look under the hood of the Interactive Deep Learning Enterprise (No-IDLE)**
2406.19054v1 by Daniel Sonntag, Michael Barz, Thiago Gouv√™a

This DFKI technical report presents the anatomy of the No-IDLE prototype
system (funded by the German Federal Ministry of Education and Research) that
provides not only basic and fundamental research in interactive machine
learning, but also reveals deeper insights into users' behaviours, needs, and
goals. Machine learning and deep learning should become accessible to millions
of end users. No-IDLE's goals and scienfific challenges centre around the
desire to increase the reach of interactive deep learning solutions for
non-experts in machine learning. One of the key innovations described in this
technical report is a methodology for interactive machine learning combined
with multimodal interaction which will become central when we start interacting
with semi-intelligent machines in the upcoming area of neural networks and
large language models.

ÊëòË¶ÅÔºöÈÄô‰ªΩÂæ∑Âúã‰∫∫Â∑•Êô∫ÊÖßÁ†îÁ©∂‰∏≠ÂøÉÊäÄË°ìÂ†±Âëä‰ªãÁ¥π No-IDLE ÂéüÂûãÁ≥ªÁµ±ÁöÑËß£ÂâñÔºàÁî±Âæ∑ÂúãËÅØÈÇ¶ÊïôËÇ≤Êö®Á†îÁ©∂ÈÉ®Ë≥áÂä©ÔºâÔºåË©≤Á≥ªÁµ±‰∏çÂÉÖÊèê‰æõ‰∫íÂãïÂºèÊ©üÂô®Â≠∏ÁøíÁöÑÂü∫Êú¨Á†îÁ©∂Ôºå‰πüÊ∑±ÂÖ•Êé¢Ë®é‰ΩøÁî®ËÄÖÁöÑË°åÁÇ∫„ÄÅÈúÄÊ±ÇÂíåÁõÆÊ®ô„ÄÇÊ©üÂô®Â≠∏ÁøíÂíåÊ∑±Â∫¶Â≠∏ÁøíÊáâËÆìÊï∏ÁôæËê¨ÁöÑÊúÄÁµÇ‰ΩøÁî®ËÄÖÈÉΩËÉΩ‰ΩøÁî®„ÄÇNo-IDLE ÁöÑÁõÆÊ®ôÂíåÁßëÂ≠∏ÊåëÊà∞ÔºåÈáçÈªûÂú®ÊñºÊì¥Â§ß‰∫íÂãïÂºèÊ∑±Â∫¶Â≠∏ÁøíËß£Ê±∫ÊñπÊ°àÁöÑÊáâÁî®ÁØÑÂúçÔºåËÆìÈùûÊ©üÂô®Â≠∏ÁøíÂ∞àÂÆ∂‰πüËÉΩ‰ΩøÁî®„ÄÇÈÄô‰ªΩÊäÄË°ìÂ†±Âëä‰∏≠ÊèèËø∞ÁöÑÂÖ∂‰∏≠‰∏ÄÈ†ÖÈóúÈçµÂâµÊñ∞ÔºåÊòØ‰∫íÂãïÂºèÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÔºåÁµêÂêàÂ§öÊ®°Âºè‰∫íÂãïÔºåÈÄôÂ∞áÂú®ÊàëÂÄëÈñãÂßãËàáÁ•ûÁ∂ìÁ∂≤Ë∑ØÂíåÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÈÄôÈ°ûÂçäÊô∫ÊÖßÊ©üÂô®‰∫íÂãïÊôÇÔºåÊâÆÊºîÊ†∏ÂøÉËßíËâ≤„ÄÇ

##### **FedMap: Iterative Magnitude-Based Pruning for Communication-Efficient Federated Learning**
2406.19050v1 by Alexander Herzog, Robbie Southam, Ioannis Mavromatis, Aftab Khan

Federated Learning (FL) is a distributed machine learning approach that
enables training on decentralized data while preserving privacy. However, FL
systems often involve resource-constrained client devices with limited
computational power, memory, storage, and bandwidth. This paper introduces
FedMap, a novel method that aims to enhance the communication efficiency of FL
deployments by collaboratively learning an increasingly sparse global model
through iterative, unstructured pruning. Importantly, FedMap trains a global
model from scratch, unlike other methods reported in the literature, making it
ideal for privacy-critical use cases such as in the medical and finance
domains, where suitable pre-training data is often limited. FedMap adapts
iterative magnitude-based pruning to the FL setting, ensuring all clients prune
and refine the same subset of the global model parameters, therefore gradually
reducing the global model size and communication overhead. The iterative nature
of FedMap, forming subsequent models as subsets of predecessors, avoids
parameter reactivation issues seen in prior work, resulting in stable
performance. In this paper we provide an extensive evaluation of FedMap across
diverse settings, datasets, model architectures, and hyperparameters, assessing
performance in both IID and non-IID environments. Comparative analysis against
the baseline approach demonstrates FedMap's ability to achieve more stable
client model performance. For IID scenarios, FedMap achieves over $90$\%
pruning without significant performance degradation. In non-IID settings, it
achieves at least $~80$\% pruning while maintaining accuracy. FedMap offers a
promising solution to alleviate communication bottlenecks in FL systems while
retaining model accuracy.

ÊëòË¶ÅÔºöËÅîÈÇ¶Â≠¶‰π† (FL) ÊòØ‰∏ÄÁßçÂàÜÂ∏ÉÂºèÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ïÔºåÂèØÂú®‰øùÊä§ÈöêÁßÅÁöÑÂêåÊó∂ÂØπÂàÜÊï£Êï∞ÊçÆËøõË°åËÆ≠ÁªÉ„ÄÇÁÑ∂ËÄåÔºåFL Á≥ªÁªüÈÄöÂ∏∏Ê∂âÂèäËµÑÊ∫êÂèóÈôêÁöÑÂÆ¢Êà∑Á´ØËÆæÂ§áÔºåÂÖ∂ËÆ°ÁÆóËÉΩÂäõ„ÄÅÂÜÖÂ≠ò„ÄÅÂ≠òÂÇ®ÂíåÂ∏¶ÂÆΩÊúâÈôê„ÄÇÊú¨Êñá‰ªãÁªç‰∫Ü FedMapÔºåËøôÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂçè‰ΩúÂ≠¶‰π†‰∏Ä‰∏™‰∏çÊñ≠Á®ÄÁñèÁöÑÂÖ®Â±ÄÊ®°ÂûãÔºàÈÄöËøáËø≠‰ª£ÁöÑÈùûÁªìÊûÑÂåñÂâ™ÊûùÔºâÊù•ÊèêÈ´ò FL ÈÉ®ÁΩ≤ÁöÑÈÄö‰ø°ÊïàÁéá„ÄÇÈáçË¶ÅÁöÑÊòØÔºåFedMap ‰ªéÂ§¥ÂºÄÂßãËÆ≠ÁªÉ‰∏Ä‰∏™ÂÖ®Â±ÄÊ®°ÂûãÔºåËøô‰∏éÊñáÁåÆ‰∏≠Êä•ÈÅìÁöÑÂÖ∂‰ªñÊñπÊ≥ï‰∏çÂêåÔºå‰ΩøÂÖ∂ÈùûÂ∏∏ÈÄÇÂêàÈöêÁßÅËá≥ÂÖ≥ÈáçË¶ÅÁöÑÁî®‰æãÔºå‰æãÂ¶ÇÂåªÁñóÂíåÈáëËûçÈ¢ÜÂüüÔºåÂÖ∂‰∏≠ÂêàÈÄÇÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈÄöÂ∏∏ÊúâÈôê„ÄÇFedMap Â∞ÜÂü∫‰∫éËø≠‰ª£ÂπÖÂ∫¶ÁöÑÂâ™ÊûùË∞ÉÊï¥Âà∞ FL ËÆæÁΩÆ‰∏≠ÔºåÁ°Æ‰øùÊâÄÊúâÂÆ¢Êà∑Á´ØÈÉΩÂâ™ÊûùÂπ∂‰ºòÂåñÂÖ®Â±ÄÊ®°ÂûãÂèÇÊï∞ÁöÑÁõ∏ÂêåÂ≠êÈõÜÔºå‰ªéËÄåÈÄêÊ∏êÂáèÂ∞ëÂÖ®Â±ÄÊ®°ÂûãÂ§ßÂ∞èÂíåÈÄö‰ø°ÂºÄÈîÄ„ÄÇFedMap ÁöÑËø≠‰ª£ÊÄßË¥®ÔºåÂ∞ÜÂêéÁª≠Ê®°ÂûãÂΩ¢Êàê‰∏∫Ââç‰ª£Ê®°ÂûãÁöÑÂ≠êÈõÜÔºåÈÅøÂÖç‰∫ÜÂÖàÂâçÂ∑•‰Ωú‰∏≠ÁúãÂà∞ÁöÑÂèÇÊï∞ÈáçÊñ∞ÊøÄÊ¥ªÈóÆÈ¢òÔºå‰ªéËÄå‰∫ßÁîü‰∫ÜÁ®≥ÂÆöÁöÑÊÄßËÉΩ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂØπ FedMap Âú®‰∏çÂêåËÆæÁΩÆ„ÄÅÊï∞ÊçÆÈõÜ„ÄÅÊ®°ÂûãÊû∂ÊûÑÂíåË∂ÖÂèÇÊï∞‰∏≠ËøõË°å‰∫ÜÂπøÊ≥õËØÑ‰º∞ÔºåËØÑ‰º∞‰∫ÜÂú® IID ÂíåÈùû IID ÁéØÂ¢É‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ‰∏éÂü∫ÂáÜÊñπÊ≥ïÁöÑÊØîËæÉÂàÜÊûêËØÅÊòé‰∫Ü FedMap ËÉΩÂ§üÂÆûÁé∞Êõ¥Á®≥ÂÆöÁöÑÂÆ¢Êà∑Á´ØÊ®°ÂûãÊÄßËÉΩ„ÄÇÂØπ‰∫é IID Âú∫ÊôØÔºåFedMap Âú®‰∏çÊòæÁùÄÈôç‰ΩéÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞‰∫ÜË∂ÖËøá 90% ÁöÑÂâ™Êûù„ÄÇÂú®Èùû IID ËÆæÁΩÆ‰∏≠ÔºåÂÆÉÂú®‰øùÊåÅÂáÜÁ°ÆÊÄßÁöÑÂêåÊó∂ÂÆûÁé∞‰∫ÜËá≥Â∞ë 80% ÁöÑÂâ™Êûù„ÄÇFedMap ‰∏∫ÁºìËß£ FL Á≥ªÁªü‰∏≠ÁöÑÈÄö‰ø°Áì∂È¢àÂêåÊó∂‰øùÊåÅÊ®°ÂûãÂáÜÁ°ÆÊÄßÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÊúâÂ∏åÊúõÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ

##### **Accuracy on the wrong line: On the pitfalls of noisy data for out-of-distribution generalisation**
2406.19049v1 by Amartya Sanyal, Yaxi Hu, Yaodong Yu, Yian Ma, Yixin Wang, Bernhard Sch√∂lkopf

"Accuracy-on-the-line" is a widely observed phenomenon in machine learning,
where a model's accuracy on in-distribution (ID) and out-of-distribution (OOD)
data is positively correlated across different hyperparameters and data
configurations. But when does this useful relationship break down? In this
work, we explore its robustness. The key observation is that noisy data and the
presence of nuisance features can be sufficient to shatter the
Accuracy-on-the-line phenomenon. In these cases, ID and OOD accuracy can become
negatively correlated, leading to "Accuracy-on-the-wrong-line". This phenomenon
can also occur in the presence of spurious (shortcut) features, which tend to
overshadow the more complex signal (core, non-spurious) features, resulting in
a large nuisance feature space. Moreover, scaling to larger datasets does not
mitigate this undesirable behavior and may even exacerbate it. We formally
prove a lower bound on Out-of-distribution (OOD) error in a linear
classification model, characterizing the conditions on the noise and nuisance
features for a large OOD error. We finally demonstrate this phenomenon across
both synthetic and real datasets with noisy data and nuisance features.

ÊëòË¶ÅÔºö„ÄåÊ∫ñÁ¢∫ÊÄßÂú®Á∑ö‰∏ä„ÄçÊòØÊ©üÂô®Â≠∏Áøí‰∏≠Âª£Ê≥õËßÄÂØüÂà∞ÁöÑÁèæË±°ÔºåÂÖ∂‰∏≠Ê®°ÂûãÂú®ÂàÜ‰ΩàÂÖß (ID) ÂíåÂàÜ‰ΩàÂ§ñ (OOD) Ë≥áÊñô‰∏äÁöÑÊ∫ñÁ¢∫ÊÄßÂú®‰∏çÂêåÁöÑË∂ÖÂèÉÊï∏ÂíåË≥áÊñôÈÖçÁΩÆ‰∏≠ÂëàÊ≠£Áõ∏Èóú„ÄÇ‰ΩÜÊòØÈÄôÁ®ÆÊúâÁî®ÁöÑÈóú‰øÇ‰ªÄÈ∫ºÊôÇÂÄôÊúÉ‰∏≠Êñ∑ÔºüÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÂÖ∂Á©©ÂÅ•ÊÄß„ÄÇÈóúÈçµËßÄÂØüÊòØÔºåÈõúË®äË≥áÊñôÂíåÊªãÊìæÁâπÂæµÁöÑÂ≠òÂú®Ë∂≥‰ª•ÊâìÁ†¥„ÄåÊ∫ñÁ¢∫ÊÄßÂú®Á∑ö‰∏ä„ÄçÁèæË±°„ÄÇÂú®ÈÄô‰∫õÊÉÖÊ≥Å‰∏ãÔºåID Âíå OOD Ê∫ñÁ¢∫ÊÄßÂèØËÉΩÊúÉËÆäÊàêË≤†Áõ∏ÈóúÔºåÂ∞éËá¥„ÄåÊ∫ñÁ¢∫ÊÄßÂú®ÈåØË™§Á∑ö‰∏ä„Äç„ÄÇÈÄôÁ®ÆÁèæË±°‰πüÂèØËÉΩÁôºÁîüÂú®Â≠òÂú®ËôõÂÅáÔºàÊç∑ÂæëÔºâÁâπÂæµÁöÑÊÉÖÊ≥Å‰∏ãÔºåËôõÂÅáÁâπÂæµÂæÄÂæÄÊúÉÊé©ËìãÊõ¥Ë§áÈõúÁöÑË®äËôüÔºàÊ†∏ÂøÉ„ÄÅÈùûËôõÂÅáÔºâÁâπÂæµÔºåÂ∞éËá¥Â§ßÈáèÁöÑÊªãÊìæÁâπÂæµÁ©∫Èñì„ÄÇÊ≠§Â§ñÔºåÊì¥ÂÖÖÂà∞Êõ¥Â§ßÁöÑË≥áÊñôÈõÜ‰∏¶‰∏çÊúÉÊ∏õËºïÈÄôÁ®Æ‰∏çËâØË°åÁÇ∫ÔºåÁîöËá≥ÂèØËÉΩ‰ΩøÊÉÖÊ≥ÅÊÉ°Âåñ„ÄÇÊàëÂÄëÊ≠£ÂºèË≠âÊòé‰∫ÜÁ∑öÊÄßÂàÜÈ°ûÊ®°Âûã‰∏≠ Out-of-distribution (OOD) Ë™§Â∑ÆÁöÑ‰∏ãÈôêÔºåÊèèËø∞‰∫ÜÂ§ßÈáè OOD Ë™§Â∑ÆÁöÑÈõúË®äÂíåÊªãÊìæÁâπÂæµÊ¢ù‰ª∂„ÄÇÊàëÂÄëÊúÄÂæåÂú®ÂêàÊàêÂíåÁúüÂØ¶Ë≥áÊñôÈõÜ‰∏äÂ±ïÁ§∫‰∫ÜÈÄôÁ®ÆÁèæË±°ÔºåÈÄô‰∫õË≥áÊñôÈõÜÂåÖÂê´ÈõúË®äË≥áÊñôÂíåÊªãÊìæÁâπÂæµ„ÄÇ

##### **Improving Weak-to-Strong Generalization with Reliability-Aware Alignment**
2406.19032v1 by Yue Guo, Yi Yang

Large language models (LLMs) are now rapidly advancing and surpassing human
abilities on many natural language tasks. However, aligning these super-human
LLMs with human knowledge remains challenging because the supervision signals
from human annotators may be wrong. This issue, known as the "super-alignment"
problem, requires enhancing weak-to-strong generalization, where a strong LLM
must generalize from imperfect supervision provided by a weaker source. To
address this issue, we propose an approach to improve weak-to-strong
generalization by involving the reliability of weak supervision signals in the
alignment process. In our method, we query the weak supervisor for multiple
answers, estimate the answer reliability, and enhance the alignment process by
filtering out uncertain data or re-weighting reliable data. Experiments on four
datasets demonstrate that our methods effectively identify the quality of weak
labels and significantly enhance weak-to-strong generalization. Our work
presents effective techniques for error-robust model alignment, reducing error
propagation from noisy supervision and enhancing the accuracy and reliability
of LLMs. Codes are publicly available at
http://github.com/Irenehere/ReliableAlignment.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁèæÂú®Ê≠£Âø´ÈÄüÈÄ≤Ê≠•ÔºåÂú®Ë®±Â§öËá™ÁÑ∂Ë™ûË®Ä‰ªªÂãô‰∏äË∂ÖË∂ä‰∫∫È°ûËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåËÆìÈÄô‰∫õË∂Ö‰∫∫È°û LLM Ëàá‰∫∫È°ûÁü•Ë≠ò‰øùÊåÅ‰∏ÄËá¥‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫‰æÜËá™‰∫∫È°ûË®ªÈáãËÄÖÁöÑÁõ£Áù£‰ø°ËôüÂèØËÉΩÈåØË™§„ÄÇÈÄôÂÄãÂïèÈ°åÁ®±ÁÇ∫„ÄåË∂ÖÁ¥öÂ∞çÈΩä„ÄçÂïèÈ°åÔºåÈúÄË¶ÅÂ¢ûÂº∑ÂæûÂº±Âà∞Âº∑ÁöÑÊ¶ÇÊã¨ËÉΩÂäõÔºåÂÖ∂‰∏≠Âº∑Â§ßÁöÑ LLM ÂøÖÈ†àÂæûËºÉÂº±‰æÜÊ∫êÊèê‰æõÁöÑ‰∏¶‰∏çÂÆåÁæéÁöÑÁõ£Áù£‰∏≠ÈÄ≤Ë°åÊ¶ÇÊã¨„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÈÄèÈÅéÂú®Â∞çÈΩäÈÅéÁ®ã‰∏≠Á¥çÂÖ•Âº±Áõ£Áù£‰ø°ËôüÁöÑÂèØÈù†ÊÄß‰æÜÊîπÂñÑÂæûÂº±Âà∞Âº∑ÁöÑÊ¶ÇÊã¨ËÉΩÂäõ„ÄÇÂú®ÊàëÂÄëÁöÑÊñπÊ≥ï‰∏≠ÔºåÊàëÂÄëÊü•Ë©¢Âº±Áõ£Áù£ËÄÖ‰ª•ÂèñÂæóÂ§öÂÄãÁ≠îÊ°àÔºå‰º∞Ë®àÁ≠îÊ°àÁöÑÂèØÈù†ÊÄßÔºå‰∏¶ÈÄèÈÅéÁØ©ÈÅ∏‰∏çÁ¢∫ÂÆöÁöÑË≥áÊñôÊàñÈáçÊñ∞Âä†Ê¨äÂèØÈù†ÁöÑË≥áÊñô‰æÜÂ¢ûÂº∑Â∞çÈΩäÈÅéÁ®ã„ÄÇÂú®ÂõõÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊúâÊïàÂú∞Ë≠òÂà•Âº±Ê®ôÁ±§ÁöÑÂìÅË≥™Ôºå‰∏¶È°ØËëóÂ¢ûÂº∑ÂæûÂº±Âà∞Âº∑ÁöÑÊ¶ÇÊã¨ËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÊèêÂá∫‰∫ÜÈáùÂ∞çÈåØË™§Á©©ÂÅ•Ê®°ÂûãÂ∞çÈΩäÁöÑÊúâÊïàÊäÄË°ìÔºåÊ∏õÂ∞ë‰æÜËá™ÊúâÈõúË®äÁõ£Áù£ÁöÑÈåØË™§ÂÇ≥Êí≠Ôºå‰∏¶Â¢ûÂº∑ LLM ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñº http://github.com/Irenehere/ReliableAlignment„ÄÇ

##### **Lithium-Ion Battery System Health Monitoring and Fault Analysis from Field Data Using Gaussian Processes**
2406.19015v1 by Joachim Schaeffer, Eric Lenz, Duncan Gulla, Martin Z. Bazant, Richard D. Braatz, Rolf Findeisen

Health monitoring, fault analysis, and detection are critical for the safe
and sustainable operation of battery systems. We apply Gaussian process
resistance models on lithium iron phosphate battery field data to effectively
separate the time-dependent and operating point-dependent resistance. The data
set contains 29 battery systems returned to the manufacturer for warranty, each
with eight cells in series, totaling 232 cells and 131 million data rows. We
develop probabilistic fault detection rules using recursive spatiotemporal
Gaussian processes. These processes allow the quick processing of over a
million data points, enabling advanced online monitoring and furthering the
understanding of battery pack failure in the field. The analysis underlines
that often, only a single cell shows abnormal behavior or a knee point,
consistent with weakest-link failure for cells connected in series, amplified
by local resistive heating. The results further the understanding of how
batteries degrade and fail in the field and demonstrate the potential of
efficient online monitoring based on data. We open-source the code and publish
the large data set upon completion of the review of this article.

ÊëòË¶ÅÔºöÂÅ•Â∫∑Áõ£Êéß„ÄÅÊïÖÈöúÂàÜÊûêÂíåÊ™¢Ê∏¨Â∞çÊñºÈõªÊ±†Á≥ªÁµ±ÁöÑÂÆâÂÖ®ÂíåÂèØÊåÅÁ∫åÈÅã‰ΩúËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÂ∞áÈ´òÊñØÈÅéÁ®ãÈõªÈòªÊ®°ÂûãÊáâÁî®ÊñºÁ£∑ÈÖ∏ÈêµÈã∞ÈõªÊ±†ÁèæÂ†¥Êï∏ÊìöÔºå‰ª•ÊúâÊïàÂàÜÈõ¢ÊôÇÈñìÁõ∏ÈóúÂíåÈÅã‰ΩúÈªûÁõ∏ÈóúÁöÑÈõªÈòª„ÄÇË©≤Êï∏ÊìöÈõÜÂåÖÂê´ 29 ÂÄãÈÄÄÂõûÁµ¶Ë£ΩÈÄ†ÂïÜÈÄ≤Ë°å‰øùÂõ∫ÁöÑÈõªÊ±†Á≥ªÁµ±ÔºåÊØèÂÄãÁ≥ªÁµ±ÊúâÂÖ´ÂÄã‰∏≤ËÅØÈõªÊ±†ÔºåÁ∏ΩË®à 232 ÂÄãÈõªÊ±†Âíå 1.31 ÂÑÑÂàóÊï∏Êìö„ÄÇÊàëÂÄë‰ΩøÁî®ÈÅûËø¥ÊôÇÁ©∫È´òÊñØÈÅéÁ®ãÈñãÁôºÂá∫Ê©üÁéáÊÄßÊïÖÈöúÊ™¢Ê∏¨Ë¶èÂâá„ÄÇÈÄô‰∫õÈÅéÁ®ãÂÖÅË®±Âø´ÈÄüËôïÁêÜË∂ÖÈÅé‰∏ÄÁôæËê¨ÂÄãÊï∏ÊìöÈªûÔºåÂØ¶ÁèæÈÄ≤ÈöéÁ∑ö‰∏äÁõ£ÊéßÔºå‰∏¶ÈÄ≤‰∏ÄÊ≠•‰∫ÜËß£ÈõªÊ±†ÁµÑÂú®ÁèæÂ†¥ÁöÑÊïÖÈöú„ÄÇÂàÜÊûêÂº∑Ë™øÔºåÈÄöÂ∏∏Âè™Êúâ‰∏ÄÂÄãÈõªÊ±†È°ØÁ§∫Âá∫Áï∞Â∏∏Ë°åÁÇ∫ÊàñÊãêÈªûÔºåÈÄôËàá‰∏≤ËÅØÈÄ£Êé•ÈõªÊ±†ÁöÑÂº±Áí∞ÊïÖÈöú‰∏ÄËá¥Ôºå‰∏¶Âõ†Â±ÄÈÉ®ÈõªÈòªÂä†ÁÜ±ËÄåÊîæÂ§ß„ÄÇÁµêÊûúÈÄ≤‰∏ÄÊ≠•‰∫ÜËß£ÈõªÊ±†Â¶Ç‰ΩïÂú®ÁèæÂ†¥ÈÄÄÂåñÂíåÊïÖÈöúÔºå‰∏¶Â±ïÁ§∫Âü∫ÊñºÊï∏ÊìöÁöÑÊúâÊïàÁ∑ö‰∏äÁõ£ÊéßÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÂú®ÂÆåÊàêÊú¨ÊñáÂØ©Êü•ÂæåÈñãÊ∫êÁ®ãÂºèÁ¢º‰∏¶ÁôºÂ∏ÉÂ§ßÂûãÊï∏ÊìöÈõÜ„ÄÇ

##### **FedMLP: Federated Multi-Label Medical Image Classification under Task Heterogeneity**
2406.18995v1 by Zhaobin Sun, Nannan Wu, Junjie Shi, Li Yu, Xin Yang, Kwang-Ting Cheng, Zengqiang Yan

Cross-silo federated learning (FL) enables decentralized organizations to
collaboratively train models while preserving data privacy and has made
significant progress in medical image classification. One common assumption is
task homogeneity where each client has access to all classes during training.
However, in clinical practice, given a multi-label classification task,
constrained by the level of medical knowledge and the prevalence of diseases,
each institution may diagnose only partial categories, resulting in task
heterogeneity. How to pursue effective multi-label medical image classification
under task heterogeneity is under-explored. In this paper, we first formulate
such a realistic label missing setting in the multi-label FL domain and propose
a two-stage method FedMLP to combat class missing from two aspects: pseudo
label tagging and global knowledge learning. The former utilizes a warmed-up
model to generate class prototypes and select samples with high confidence to
supplement missing labels, while the latter uses a global model as a teacher
for consistency regularization to prevent forgetting missing class knowledge.
Experiments on two publicly-available medical datasets validate the superiority
of FedMLP against the state-of-the-art both federated semi-supervised and noisy
label learning approaches under task heterogeneity. Code is available at
https://github.com/szbonaldo/FedMLP.

ÊëòË¶ÅÔºöË∑®Ë≥áÊñôÂ∫´ËÅØÈÇ¶Â≠∏Áøí (FL) ËÆìÂàÜÊï£ÂºèÁµÑÁπîËÉΩÂ§†Âú®‰øùÁïôË≥áÊñôÈö±ÁßÅÁöÑÂêåÊôÇÂçî‰ΩúË®ìÁ∑¥Ê®°ÂûãÔºå‰∏¶Âú®ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°ûÊñπÈù¢ÂèñÂæóÈ°ØËëóÈÄ≤Â±ï„ÄÇ‰∏ÄÂÄãÂ∏∏Ë¶ãÁöÑÂÅáË®≠ÊòØ‰ªªÂãôÂêåË≥™ÊÄßÔºåÂÖ∂‰∏≠ÊØèÂÄãÂÆ¢Êà∂Á´ØÂú®Ë®ìÁ∑¥ÊúüÈñìÈÉΩÂèØ‰ª•Â≠òÂèñÊâÄÊúâÈ°ûÂà•„ÄÇÁÑ∂ËÄåÔºåÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÔºåÁµ¶ÂÆö‰∏ÄÂÄãÂ§öÊ®ôÁ±§ÂàÜÈ°û‰ªªÂãôÔºåÂèóÈôêÊñºÈÜ´Â≠∏Áü•Ë≠òÁöÑÂ±§Á¥öÂíåÁñæÁóÖÁöÑÊµÅË°åÁéáÔºåÊØèÂÄãÊ©üÊßãÂèØËÉΩÂè™Ë®∫Êñ∑ÈÉ®ÂàÜÈ°ûÂà•ÔºåÂ∞éËá¥‰ªªÂãôÁï∞Ë≥™ÊÄß„ÄÇÂ¶Ç‰ΩïÂú®‰ªªÂãôÁï∞Ë≥™ÊÄß‰∏ãËøΩÊ±ÇÊúâÊïàÁöÑÂ§öÊ®ôÁ±§ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÈ°û‰ªçÊúâÂæÖÊé¢Ë®é„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÂú®Â§öÊ®ôÁ±§ FL È†òÂüü‰∏≠Âà∂ÂÆöÈÄôÁ®ÆÁèæÂØ¶ÁöÑÊ®ôÁ±§ÈÅ∫Â§±Ë®≠ÂÆöÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂÖ©ÈöéÊÆµÊñπÊ≥ï FedMLP ‰æÜÂæûÂÖ©ÂÄãÊñπÈù¢Ëß£Ê±∫È°ûÂà•ÈÅ∫Â§±ÂïèÈ°åÔºöÂÅΩÊ®ôÁ±§Ê®ôË®òÂíåÂÖ®Â±ÄÁü•Ë≠òÂ≠∏Áøí„ÄÇÂâçËÄÖÂà©Áî®ÁÜ±Ë∫´Ê®°ÂûãÁî¢ÁîüÈ°ûÂà•ÂéüÂûãÔºå‰∏¶ÈÅ∏ÊìáÂÖ∑ÊúâÈ´òÂ∫¶‰ø°ÂøÉÁöÑÊ®£Êú¨‰æÜË£úÂÖÖÈÅ∫Â§±Ê®ôÁ±§ÔºåËÄåÂæåËÄÖ‰ΩøÁî®ÂÖ®Â±ÄÊ®°Âûã‰ΩúÁÇ∫ÊïôÂ∏´ÈÄ≤Ë°å‰∏ÄËá¥ÊÄßÊ≠£ÂâáÂåñÔºå‰ª•Èò≤Ê≠¢ÈÅ∫ÂøòÈÅ∫Â§±È°ûÂà•ÁöÑÁü•Ë≠ò„ÄÇÂú®ÂÖ©ÂÄãÂÖ¨ÈñãÂèØÁî®ÁöÑÈÜ´Â≠∏Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óÈ©óË≠â‰∫Ü FedMLP Âú®‰ªªÂãôÁï∞Ë≥™ÊÄß‰∏ãÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑËÅØÂêàÂçäÁõ£Áù£ÂíåÈõúË®äÊ®ôÁ±§Â≠∏ÁøíÊñπÊ≥ï„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/szbonaldo/FedMLP ÂèñÂæó„ÄÇ

##### **Semi-supervised Concept Bottleneck Models**
2406.18992v1 by Lijie Hu, Tianhao Huang, Huanyi Xie, Chenyang Ren, Zhengyu Hu, Lu Yu, Di Wang

Concept Bottleneck Models (CBMs) have garnered increasing attention due to
their ability to provide concept-based explanations for black-box deep learning
models while achieving high final prediction accuracy using human-like
concepts. However, the training of current CBMs heavily relies on the accuracy
and richness of annotated concepts in the dataset. These concept labels are
typically provided by experts, which can be costly and require significant
resources and effort. Additionally, concept saliency maps frequently misalign
with input saliency maps, causing concept predictions to correspond to
irrelevant input features - an issue related to annotation alignment. To
address these limitations, we propose a new framework called SSCBM
(Semi-supervised Concept Bottleneck Model). Our SSCBM is suitable for practical
situations where annotated data is scarce. By leveraging joint training on both
labeled and unlabeled data and aligning the unlabeled data at the concept
level, we effectively solve these issues. We proposed a strategy to generate
pseudo labels and an alignment loss. Experiments demonstrate that our SSCBM is
both effective and efficient. With only 20% labeled data, we achieved 93.19%
(96.39% in a fully supervised setting) concept accuracy and 75.51% (79.82% in a
fully supervised setting) prediction accuracy.

ÊëòË¶ÅÔºöÊ¶ÇÂøµÁì∂È†∏Ê®°Âûã (CBM) Âõ†ÂÖ∂ËÉΩÁÇ∫ÈªëÁÆ±Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÊèê‰æõÂü∫ÊñºÊ¶ÇÂøµÁöÑËß£ÈáãÔºåÂêåÊôÇ‰ΩøÁî®È°û‰∫∫Ê¶ÇÂøµÂØ¶ÁèæÈ´òÊúÄÁµÇÈ†êÊ∏¨Ê∫ñÁ¢∫ÊÄßËÄåÂÇôÂèóÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâç CBM ÁöÑË®ìÁ∑¥È´òÂ∫¶‰æùË≥¥ÊñºË≥áÊñôÈõÜ‰∏≠Ë®ªÈáãÊ¶ÇÂøµÁöÑÊ∫ñÁ¢∫ÊÄßÂíåË±êÂØåÊÄß„ÄÇÈÄô‰∫õÊ¶ÇÂøµÊ®ôÁ±§ÈÄöÂ∏∏Áî±Â∞àÂÆ∂Êèê‰æõÔºåÈÄôÂèØËÉΩÂæàÊòÇË≤¥Ôºå‰∏¶‰∏îÈúÄË¶ÅÂ§ßÈáèÁöÑË≥áÊ∫êÂíåÁ≤æÂäõ„ÄÇÊ≠§Â§ñÔºåÊ¶ÇÂøµÈ°ØËëóÊÄßÂúñÁ∂ìÂ∏∏ËàáËº∏ÂÖ•È°ØËëóÊÄßÂúñ‰∏ç‰∏ÄËá¥ÔºåÂ∞éËá¥Ê¶ÇÂøµÈ†êÊ∏¨Â∞çÊáâÊñºÁÑ°ÈóúÁöÑËº∏ÂÖ•ÁâπÂæµÔºåËÄåÈÄôÊòØ‰∏ÄÂÄãËàáË®ªÈáãÂ∞çÈΩäÁõ∏ÈóúÁöÑÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ SSCBMÔºàÂçäÁõ£Áù£Ê¶ÇÂøµÁì∂È†∏Ê®°ÂûãÔºâÁöÑÊñ∞Ê°ÜÊû∂„ÄÇÊàëÂÄëÁöÑ SSCBM ÈÅ©Áî®ÊñºË®ªÈáãÊï∏ÊìöÁ®ÄÁº∫ÁöÑÂØ¶ÈöõÊÉÖÊ≥Å„ÄÇÈÄöÈÅéÂú®Ê®ôÁ±§Êï∏ÊìöÂíåÊú™Ê®ôÁ±§Êï∏Êìö‰∏äÈÄ≤Ë°åËÅØÂêàË®ìÁ∑¥Ôºå‰∏¶Âú®Ê¶ÇÂøµÂ±§Á¥öÂ∞çÈΩäÊú™Ê®ôÁ±§Êï∏ÊìöÔºåÊàëÂÄëÊúâÊïàÂú∞Ëß£Ê±∫‰∫ÜÈÄô‰∫õÂïèÈ°å„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁîüÊàêÂÅΩÊ®ôÁ±§ÂíåÂ∞çÈΩäÊêçÂ§±ÁöÑÁ≠ñÁï•„ÄÇÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑ SSCBM Êó¢ÊúâÊïàÂèàÈ´òÊïà„ÄÇÂÉÖ‰ΩøÁî® 20% ÁöÑÊ®ôÁ±§Êï∏ÊìöÔºåÊàëÂÄëÂ∞±ÈÅîÂà∞‰∫Ü 93.19%ÔºàÂú®ÂÆåÂÖ®Áõ£Áù£ÁöÑË®≠ÁΩÆ‰∏≠ÁÇ∫ 96.39%ÔºâÁöÑÊ¶ÇÂøµÊ∫ñÁ¢∫Â∫¶Âíå 75.51%ÔºàÂú®ÂÆåÂÖ®Áõ£Áù£ÁöÑË®≠ÁΩÆ‰∏≠ÁÇ∫ 79.82%ÔºâÁöÑÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶„ÄÇ

##### **RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulaiton**
2406.18977v1 by Fanfan Liu, Feng Yan, Liming Zheng, Chengjian Feng, Yiyang Huang, Lin Ma

Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a
novel paradigm, aiming to enhance the model's ability to generalize to new
objects and instructions. However, due to variations in camera specifications
and mounting positions, existing methods exhibit significant performance
disparities across different robotic platforms. To address this challenge, we
propose RoboUniView in this paper, an innovative approach that decouples visual
feature extraction from action learning. We first learn a unified view
representation from multi-perspective views by pre-training on readily
accessible data, and then derive actions from this unified view representation
to control robotic manipulation. This unified view representation more
accurately mirrors the physical world and is not constrained by the robotic
platform's camera parameters. Thanks to this methodology, we achieve
state-of-the-art performance on the demanding CALVIN benchmark, enhancing the
success rate in the $D \to D$ setting from 88.7% to 96.2%, and in the $ABC \to
D$ setting from 82.4% to 94.2%. Moreover, our model exhibits outstanding
adaptability and flexibility: it maintains high performance under unseen camera
parameters, can utilize multiple datasets with varying camera parameters, and
is capable of joint cross-task learning across datasets. Code is provided for
re-implementation. https://github.com/liufanfanlff/RoboUniview

ÊëòË¶ÅÔºöÂà©Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) ÈÄ≤Ë°åÊ©üÂô®‰∫∫Êìç‰Ωú‰ª£Ë°®‰∏ÄÁ®ÆÊñ∞ÂÖ∏ÁØÑÔºåÊó®Âú®Â¢ûÂº∑Ê®°ÂûãÂ∞çÊñ∞Áâ©‰ª∂ÂíåÊåá‰ª§ÁöÑÊ¶ÇÂåñËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁõ∏Ê©üË¶èÊ†ºÂíåÂÆâË£ù‰ΩçÁΩÆÁöÑÂ∑ÆÁï∞ÔºåÁèæÊúâÊñπÊ≥ïÂú®‰∏çÂêåÁöÑÊ©üÂô®‰∫∫Âπ≥Âè∞‰∏äË°®ÁèæÂá∫È°ØËëóÁöÑÊïàËÉΩÂ∑ÆÁï∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄôÈ†ÖÊåëÊà∞ÔºåÊàëÂÄëÂú®Êú¨Êñá‰∏≠ÊèêÂá∫ RoboUniViewÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂ∞áË¶ñË¶∫ÁâπÂæµÊèêÂèñËàáÂãï‰ΩúÂ≠∏ÁøíÂàÜÈñã„ÄÇÊàëÂÄëÈ¶ñÂÖàÈÄèÈÅéÈ†êÂÖàË®ìÁ∑¥ÊñºÂÆπÊòìÂèñÂæóÁöÑË≥áÊñô‰∏äÔºåÂæûÂ§öË¶ñËßíË¶ñÂúñ‰∏≠Â≠∏ÁøíÁµ±‰∏ÄÁöÑË¶ñÂúñË°®Á§∫ÔºåÁÑ∂ÂæåÂæûÈÄôÂÄãÁµ±‰∏ÄÁöÑË¶ñÂúñË°®Á§∫‰∏≠Êé®Â∞éÂãï‰ΩúÔºå‰ª•ÊéßÂà∂Ê©üÂô®‰∫∫Êìç‰Ωú„ÄÇÈÄôÂÄãÁµ±‰∏ÄÁöÑË¶ñÂúñË°®Á§∫Êõ¥Ê∫ñÁ¢∫Âú∞ÂèçÊò†Áâ©ÁêÜ‰∏ñÁïåÔºå‰∏çÂèóÊ©üÂô®‰∫∫Âπ≥Âè∞Áõ∏Ê©üÂèÉÊï∏ÁöÑÁ¥ÑÊùü„ÄÇÁî±ÊñºÈÄôÂÄãÊñπÊ≥ïÔºåÊàëÂÄëÂú®Ë¶ÅÊ±ÇÂö¥Ê†ºÁöÑ CALVIN Âü∫Ê∫ñ‰∏äÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂ∞á $D \to D$ Ë®≠ÂÆö‰∏≠ÁöÑÊàêÂäüÁéáÂæû 88.7% ÊèêÂçáÂà∞ 96.2%Ôºå‰ª•ÂèäÂ∞á $ABC \to D$ Ë®≠ÂÆö‰∏≠ÁöÑÊàêÂäüÁéáÂæû 82.4% ÊèêÂçáÂà∞ 94.2%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂ±ïÁèæÂá∫ÂÇëÂá∫ÁöÑÈÅ©ÊáâÊÄßÂíåÈùàÊ¥ªÊÄßÔºöÂÆÉÂú®Êú™Ë¶ãÁöÑÁõ∏Ê©üÂèÉÊï∏‰∏ãÁ∂≠ÊåÅÈ´òÊïàËÉΩÔºåÂèØ‰ª•‰ΩøÁî®ÂÖ∑Êúâ‰∏çÂêåÁõ∏Ê©üÂèÉÊï∏ÁöÑÂ§öÂÄãË≥áÊñôÈõÜÔºå‰∏¶‰∏îËÉΩÂ§†Ë∑®Ë≥áÊñôÈõÜÈÄ≤Ë°åËÅØÂêàË∑®‰ªªÂãôÂ≠∏Áøí„ÄÇÂ∑≤Êèê‰æõÁ®ãÂºèÁ¢º‰æõÈáçÊñ∞ÂØ¶‰Ωú„ÄÇhttps://github.com/liufanfanlff/RoboUniview

##### **Applying LLMs for Rescoring N-best ASR Hypotheses of Casual Conversations: Effects of Domain Adaptation and Context Carry-over**
2406.18972v1 by Atsunori Ogawa, Naoyuki Kamo, Kohei Matsuura, Takanori Ashihara, Takafumi Moriya, Takatomo Kano, Naohiro Tawara, Marc Delcroix

Large language models (LLMs) have been successfully applied for rescoring
automatic speech recognition (ASR) hypotheses. However, their ability to
rescore ASR hypotheses of casual conversations has not been sufficiently
explored. In this study, we reveal it by performing N-best ASR hypotheses
rescoring using Llama2 on the CHiME-7 distant ASR (DASR) task. Llama2 is one of
the most representative LLMs, and the CHiME-7 DASR task provides datasets of
casual conversations between multiple participants. We investigate the effects
of domain adaptation of the LLM and context carry-over when performing N-best
rescoring. Experimental results show that, even without domain adaptation,
Llama2 outperforms a standard-size domain-adapted Transformer-LM, especially
when using a long context. Domain adaptation shortens the context length needed
with Llama2 to achieve its best performance, i.e., it reduces the computational
cost of Llama2.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊàêÂäüÊáâÁî®ÊñºÈáçÊñ∞Ë©ïÂàÜËá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) ÂÅáË®≠„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÈáçÊñ∞Ë©ïÂàÜÈùûÊ≠£ÂºèÂ∞çË©±ÁöÑ ASR ÂÅáË®≠ÁöÑËÉΩÂäõÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÈÄöÈÅéÂú® CHiME-7 ÈÅ†Á®ã ASR (DASR) ‰ªªÂãô‰∏ä‰ΩøÁî® Llama2 Âü∑Ë°å N ÂÄãÊúÄ‰Ω≥ ASR ÂÅáË®≠ÈáçÊñ∞Ë©ïÂàÜ‰æÜÊè≠Á§∫ÂÆÉ„ÄÇLlama2 ÊòØÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑ LLM ‰πã‰∏ÄÔºåËÄå CHiME-7 DASR ‰ªªÂãôÊèê‰æõ‰∫ÜÂ§öÂÄãÂèÉËàáËÄÖ‰πãÈñìÁöÑÈùûÊ≠£ÂºèÂ∞çË©±ÁöÑË≥áÊñôÈõÜ„ÄÇÊàëÂÄëÂú®Âü∑Ë°å N ÂÄãÊúÄ‰Ω≥ÈáçÊñ∞Ë©ïÂàÜÊôÇÁ†îÁ©∂‰∫Ü LLM ÁöÑÈ†òÂüüÈÅ©ÊáâÂíå‰∏ä‰∏ãÊñáÂª∂Á∫åÁöÑÂΩ±Èüø„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂç≥‰ΩøÊ≤íÊúâÈ†òÂüüÈÅ©ÊáâÔºåLlama2 ‰πüÂÑ™ÊñºÊ®ôÊ∫ñÂ§ßÂ∞èÁöÑÈ†òÂüüÈÅ©Êáâ Transformer-LMÔºåÁâπÂà•ÊòØÂú®‰ΩøÁî®Èï∑‰∏ä‰∏ãÊñáÊôÇ„ÄÇÈ†òÂüüÈÅ©ÊáâÁ∏ÆÁü≠‰∫Ü Llama2 ÈÅîÂà∞ÂÖ∂ÊúÄ‰Ω≥ÊÄßËÉΩÊâÄÈúÄÁöÑ‰∏ä‰∏ãÊñáÈï∑Â∫¶ÔºåÂç≥ÂÆÉÈôç‰Ωé‰∫Ü Llama2 ÁöÑÈÅãÁÆóÊàêÊú¨„ÄÇ

##### **UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models**
2406.18966v1 by Siyuan Wu, Yue Huang, Chujie Gao, Dongping Chen, Qihui Zhang, Yao Wan, Tianyi Zhou, Xiangliang Zhang, Jianfeng Gao, Chaowei Xiao, Lichao Sun

Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly
impacted various fields by enabling high-quality synthetic data generation and
reducing dependence on expensive human-generated datasets. Despite this,
challenges remain in the areas of generalization, controllability, diversity,
and truthfulness within the existing generative frameworks. To address these
challenges, this paper presents UniGen, a comprehensive LLM-powered framework
designed to produce diverse, accurate, and highly controllable datasets. UniGen
is adaptable, supporting all types of text datasets and enhancing the
generative process through innovative mechanisms. To augment data diversity,
UniGen incorporates an attribute-guided generation module and a group checking
feature. For accuracy, it employs a code-based mathematical assessment for
label verification alongside a retrieval-augmented generation technique for
factual validation. The framework also allows for user-specified constraints,
enabling customization of the data generation process to suit particular
requirements. Extensive experiments demonstrate the superior quality of data
generated by UniGen, and each module within UniGen plays a critical role in
this enhancement. Additionally, UniGen is applied in two practical scenarios:
benchmarking LLMs and data augmentation. The results indicate that UniGen
effectively supports dynamic and evolving benchmarking, and that data
augmentation improves LLM capabilities in various domains, including
agent-oriented abilities and reasoning skills.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰æãÂ¶Ç GPT-4 Âíå Llama3ÔºåÈÄèÈÅéËÆìÈ´òÂìÅË≥™ÁöÑÂêàÊàêË≥áÊñôÁîüÊàêÊàêÁÇ∫ÂèØËÉΩÔºå‰∏¶Èôç‰ΩéÂ∞çÊòÇË≤¥ÁöÑ‰∫∫Â∑•ÁîüÊàêË≥áÊñôÈõÜÁöÑ‰æùË≥¥ÔºåÂ∞çÂêÑÂÄãÈ†òÂüüÁî¢Áîü‰∫ÜÈáçÂ§ßÂΩ±Èüø„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÂú®ÁèæÊúâÁöÑÁîüÊàêÂºèÊû∂Êßã‰∏≠ÔºåÂú®Ê≥õÂåñ„ÄÅÂèØÊéßÊÄß„ÄÅÂ§öÊ®£ÊÄßÂíåÁúüÂØ¶ÊÄßÊñπÈù¢‰ªçÂ≠òÂú®ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü UniGenÔºå‰∏ÄÂÄãÁî± LLM È©ÖÂãïÁöÑÁ∂úÂêàÊ°ÜÊû∂ÔºåÊó®Âú®Áî¢ÁîüÂ§öÊ®£Âåñ„ÄÅÊ∫ñÁ¢∫‰∏îÈ´òÂ∫¶ÂèØÊéßÁöÑË≥áÊñôÈõÜ„ÄÇUniGen ÂÖ∑ÊúâÈÅ©ÊáâÊÄßÔºåÊîØÊè¥ÊâÄÊúâÈ°ûÂûãÁöÑÊñáÂ≠óË≥áÊñôÈõÜÔºå‰∏¶ÈÄèÈÅéÂâµÊñ∞ÁöÑÊ©üÂà∂Â¢ûÂº∑ÁîüÊàêÂºèÁ®ãÂ∫è„ÄÇÁÇ∫‰∫ÜÂ¢ûÂä†Ë≥áÊñôÂ§öÊ®£ÊÄßÔºåUniGen ÁµêÂêà‰∫ÜÂ±¨ÊÄßÂºïÂ∞éÁîüÊàêÊ®°ÁµÑÂíåÁæ§ÁµÑÊ™¢Êü•ÂäüËÉΩ„ÄÇÁÇ∫‰∫ÜÊ∫ñÁ¢∫ÊÄßÔºåÂÆÉÊé°Áî®Âü∫ÊñºÁ®ãÂºèÁ¢ºÁöÑÊï∏Â≠∏Ë©ï‰º∞ÈÄ≤Ë°åÊ®ôÁ±§È©óË≠âÔºå‰∏¶ÁµêÂêàÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÊäÄË°ìÈÄ≤Ë°å‰∫ãÂØ¶È©óË≠â„ÄÇË©≤Ê°ÜÊû∂ÈÇÑÂÖÅË®±‰ΩøÁî®ËÄÖÊåáÂÆöÁ¥ÑÊùüÔºåËÆìË≥áÊñôÁîüÊàêÁ®ãÂ∫èËÉΩÂ§†Ê†πÊìöÁâπÂÆöÈúÄÊ±ÇÈÄ≤Ë°åËá™Ë®Ç„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòé‰∫Ü UniGen ÊâÄÁîüÊàêË≥áÊñôÁöÑÂÑ™Áï∞ÂìÅË≥™ÔºåËÄå UniGen ‰∏≠ÁöÑÊØèÂÄãÊ®°ÁµÑÈÉΩÂú®ÈÄôÁ®ÆÂ¢ûÂº∑‰∏≠ÊâÆÊºîÈóúÈçµËßíËâ≤„ÄÇÊ≠§Â§ñÔºåUniGen ÊáâÁî®ÊñºÂÖ©ÂÄãÂØ¶ÈöõÂ†¥ÊôØÔºöLLM Âü∫Ê∫ñÊ∏¨Ë©¶ÂíåË≥áÊñôÊì¥ÂÖÖ„ÄÇÁµêÊûúË°®ÊòéÔºåUniGen ÊúâÊïàÂú∞ÊîØÊè¥ÂãïÊÖã‰∏î‰∏çÊñ∑ÊºîÈÄ≤ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåËÄåË≥áÊñôÊì¥ÂÖÖÂâáÊèêÂçá‰∫Ü LLM Âú®ÂêÑÁ®ÆÈ†òÂüüÁöÑËÉΩÂäõÔºåÂåÖÊã¨‰ª•‰ª£ÁêÜÁÇ∫Â∞éÂêëÁöÑËÉΩÂäõÂíåÊé®ÁêÜÊäÄËÉΩ„ÄÇ

##### **Investigating and Defending Shortcut Learning in Personalized Diffusion Models**
2406.18944v1 by Yixin Liu, Ruoxi Chen, Lichao Sun

Personalized diffusion models have gained popularity for adapting pre-trained
text-to-image models to generate images of specific topics with only a few
images. However, recent studies find that these models are vulnerable to minor
adversarial perturbation, and the fine-tuning performance is largely degraded
on corrupted datasets. Such characteristics are further exploited to craft
protective perturbation on sensitive images like portraits that prevent
unauthorized generation. In response, diffusion-based purification methods have
been proposed to remove these perturbations and retain generation performance.
However, existing works lack detailed analysis of the fundamental shortcut
learning vulnerability of personalized diffusion models and also turn to
over-purifying the images cause information loss. In this paper, we take a
closer look at the fine-tuning process of personalized diffusion models through
the lens of shortcut learning and propose a hypothesis that could explain the
underlying manipulation mechanisms of existing perturbation methods.
Specifically, we find that the perturbed images are greatly shifted from their
original paired prompt in the CLIP-based latent space. As a result, training
with this mismatched image-prompt pair creates a construction that causes the
models to dump their out-of-distribution noisy patterns to the identifier, thus
causing serious performance degradation. Based on this observation, we propose
a systematic approach to retain the training performance with purification that
realigns the latent image and its semantic meaning and also introduces
contrastive learning with a negative token to decouple the learning of wanted
clean identity and the unwanted noisy pattern, that shows strong potential
capacity against further adaptive perturbation.

ÊëòË¶ÅÔºö<paragraph>ÂÄã‰∫∫ÂåñÊì¥Êï£Ê®°ÂûãÂõ†ËÉΩ‰ª•Â∞ëÊï∏ÂΩ±ÂÉèÂ∞áÈ†êÂÖàË®ìÁ∑¥Â•ΩÁöÑÊñáÂ≠óËΩâÂΩ±ÂÉèÊ®°ÂûãË™øÊï¥ÁÇ∫Áî¢ÁîüÁâπÂÆö‰∏ªÈ°åÁöÑÂΩ±ÂÉèËÄåÂª£ÂèóÊ≠°Ëøé„ÄÇÁÑ∂ËÄåÔºåÊúÄËøëÁöÑÁ†îÁ©∂ÁôºÁèæÈÄô‰∫õÊ®°ÂûãÂÆπÊòìÂèóÂà∞ËºïÂæÆÁöÑÂ∞çÊäóÊÄßÊìæÂãïÔºå‰∏¶‰∏îÂæÆË™øÊïàËÉΩÊúÉÂú®ÊêçÂ£ûÁöÑË≥áÊñôÈõÜ‰∏äÂ§ßÂπÖ‰∏ãÈôç„ÄÇÈÄô‰∫õÁâπÊÄßÈÄ≤‰∏ÄÊ≠•Ë¢´Âà©Áî®‰æÜÂú®ÊïèÊÑüÁöÑÂΩ±ÂÉèÔºà‰æãÂ¶Ç‰∫∫ÂÉèÔºâ‰∏äË£Ω‰ΩúÈò≤Ë≠∑ÊÄßÊìæÂãïÔºå‰ª•Èò≤Ê≠¢Êú™Á∂ìÊéàÊ¨äÁöÑÁî¢Áîü„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂ∑≤Á∂ìÊèêÂá∫Âü∫ÊñºÊì¥Êï£ÁöÑÊ∑®ÂåñÊñπÊ≥ï‰æÜÁßªÈô§ÈÄô‰∫õÊìæÂãï‰∏¶‰øùÁïôÁî¢ÁîüÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊñπÊ≥ïÁº∫‰πèÂ∞çÂÄã‰∫∫ÂåñÊì¥Êï£Ê®°ÂûãÁöÑÊ†πÊú¨Êç∑ÂæëÂ≠∏ÁøíËÑÜÂº±ÊÄßÁöÑË©≥Á¥∞ÂàÜÊûêÔºåËÄå‰∏î‰πüÊúÉÈÅéÂ∫¶Ê∑®ÂåñÂΩ±ÂÉèËÄåÂ∞éËá¥Ë≥áË®äÈÅ∫Â§±„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÊç∑ÂæëÂ≠∏ÁøíÁöÑËßÄÈªû‰ªîÁ¥∞Ê™¢Ë¶ñÂÄã‰∫∫ÂåñÊì¥Êï£Ê®°ÂûãÁöÑÂæÆË™øÈÅéÁ®ãÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂÅáË™™ÔºåÂèØ‰ª•Ëß£ÈáãÁèæÊúâÊìæÂãïÊñπÊ≥ïÁöÑÊΩõÂú®ÊìçÁ∏±Ê©üÂà∂„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÁôºÁèæÊìæÂãïÁöÑÂΩ±ÂÉèÂú®Âü∫Êñº CLIP ÁöÑÊΩõÂú®Á©∫Èñì‰∏≠Â§ßÂπÖÂÅèÈõ¢ÂÖ∂ÂéüÂßãÈÖçÂ∞çÊèêÁ§∫„ÄÇÂõ†Ê≠§Ôºå‰ΩøÁî®ÈÄôÂÄã‰∏çÂåπÈÖçÁöÑÂΩ±ÂÉèÊèêÁ§∫Â∞çË®ìÁ∑¥Ë≥áÊñôÈÄ≤Ë°åË®ìÁ∑¥ÊúÉÁî¢Áîü‰∏ÄÂÄãÁµêÊßãÔºåÂ∞éËá¥Ê®°ÂûãÂ∞áÂÖ∂ÂàÜ‰ΩàÂ§ñÁöÑÈõúË®äÊ®°ÂºèÂÇæÂÄíÂà∞Ë≠òÂà•Á¨¶Ëôü‰∏≠ÔºåÂæûËÄåÂ∞éËá¥Âö¥ÈáçÁöÑÊïàËÉΩ‰∏ãÈôç„ÄÇÂü∫ÊñºÈÄôÂÄãËßÄÂØüÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ≥ªÁµ±ÂåñÁöÑÊñπÊ≥ïÔºå‰ª•Ê∑®Âåñ‰æÜ‰øùÁïôË®ìÁ∑¥ÊïàËÉΩÔºåÈáçÊñ∞Ë™øÊï¥ÊΩõÂú®ÂΩ±ÂÉèÂèäÂÖ∂Ë™ûÁæ©ÊÑèÁæ©Ôºå‰∏¶ÂºïÂÖ•ÂÖ∑ÊúâË≤†Èù¢Ê®ôË®òÁöÑÂ∞çÊØîÂ≠∏ÁøíÔºå‰ª•Ëß£ËÄ¶ÊâÄÈúÄÁöÑ‰πæÊ∑®Ë∫´ÂàÜÂíå‰∏çÈúÄË¶ÅÁöÑÈõúË®äÊ®°ÂºèÁöÑÂ≠∏ÁøíÔºåÈÄôÈ°ØÁ§∫Âá∫Â∞çÈÄ≤‰∏ÄÊ≠•Ëá™ÈÅ©ÊáâÊìæÂãïÁöÑÂº∑Â§ßÊΩõÂú®ËÉΩÂäõ„ÄÇ</paragraph>

##### **Federated Graph Semantic and Structural Learning**
2406.18937v1 by Wenke Huang, Guancheng Wan, Mang Ye, Bo Du

Federated graph learning collaboratively learns a global graph neural network
with distributed graphs, where the non-independent and identically distributed
property is one of the major challenges. Most relative arts focus on
traditional distributed tasks like images and voices, incapable of graph
structures. This paper firstly reveals that local client distortion is brought
by both node-level semantics and graph-level structure. First, for node-level
semantics, we find that contrasting nodes from distinct classes is beneficial
to provide a well-performing discrimination. We pull the local node towards the
global node of the same class and push it away from the global node of
different classes. Second, we postulate that a well-structural graph neural
network possesses similarity for neighbors due to the inherent adjacency
relationships. However, aligning each node with adjacent nodes hinders
discrimination due to the potential class inconsistency. We transform the
adjacency relationships into the similarity distribution and leverage the
global model to distill the relation knowledge into the local model, which
preserves the structural information and discriminability of the local model.
Empirical results on three graph datasets manifest the superiority of the
proposed method over its counterparts.

ÊëòË¶ÅÔºö<paragraph>ËÅØÈÇ¶ÂúñÂΩ¢Â≠∏ÁøíÈÄèÈÅéÂàÜÊï£ÂºèÂúñÂΩ¢Âçî‰ΩúÂ≠∏ÁøíÂÖ®ÁêÉÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÔºåÂÖ∂‰∏≠ÈùûÁç®Á´ã‰∏îÁõ∏ÂêåÂàÜ‰ΩàÁöÑÂ±¨ÊÄßÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞„ÄÇÂ§ßÂ§öÊï∏Áõ∏ÈóúÊäÄË°ìÂ∞àÊ≥®ÊñºÂÇ≥Áµ±ÁöÑÂàÜÂ∏ÉÂºè‰ªªÂãôÔºå‰æãÂ¶ÇÂΩ±ÂÉèÂíåË™ûÈü≥ÔºåÁÑ°Ê≥ïËôïÁêÜÂúñÂΩ¢ÁµêÊßã„ÄÇÊú¨ÊñáÈ¶ñÂÖàÊè≠Á§∫ÔºåÁØÄÈªûÂ±§Á¥öË™ûÊÑèÂíåÂúñÂΩ¢Â±§Á¥öÁµêÊßãÈÉΩÊúÉÈÄ†ÊàêÂ±ÄÈÉ®Áî®Êà∂Á´ØÂ§±Áúü„ÄÇÈ¶ñÂÖàÔºåÂ∞çÊñºÁØÄÈªûÂ±§Á¥öË™ûÊÑèÔºåÊàëÂÄëÁôºÁèæÂ∞çÊØî‰∏çÂêåÈ°ûÂà•ÁöÑÁØÄÈªûÊúâÂä©ÊñºÊèê‰æõÊïàËÉΩËâØÂ•ΩÁöÑÂçÄÂàÜ„ÄÇÊàëÂÄëÂ∞áÂ±ÄÈÉ®ÁØÄÈªûÊãâÂêëÂêåÈ°ûÂà•ÁöÑÂÖ®ÁêÉÁØÄÈªûÔºå‰∏¶Â∞áÂÖ∂Êé®Èõ¢‰∏çÂêåÈ°ûÂà•ÁöÑÂÖ®ÁêÉÁØÄÈªû„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÂÅáË®≠ÁµêÊßãËâØÂ•ΩÁöÑÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑ØÁî±ÊñºÂõ∫ÊúâÁöÑÈÑ∞Êé•Èóú‰øÇÔºåÂõ†Ê≠§ÂÖ∑ÊúâÈÑ∞ËøëÁØÄÈªûÁöÑÁõ∏‰ººÊÄß„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÊΩõÂú®ÁöÑÈ°ûÂà•‰∏ç‰∏ÄËá¥ÔºåÂ∞áÊØèÂÄãÁØÄÈªûËàáÈÑ∞Êé•ÁØÄÈªûÂ∞çÈΩäÊúÉÈòªÁ§ôÂçÄÂàÜ„ÄÇÊàëÂÄëÂ∞áÈÑ∞Êé•Èóú‰øÇËΩâÊèõÁÇ∫Áõ∏‰ººÊÄßÂàÜ‰ΩàÔºå‰∏¶Âà©Áî®ÂÖ®ÁêÉÊ®°ÂûãÂ∞áÈóú‰øÇÁü•Ë≠òËêÉÂèñÂà∞Â±ÄÈÉ®Ê®°Âûã‰∏≠ÔºåÈÄô‰øùÁïô‰∫ÜÂ±ÄÈÉ®Ê®°ÂûãÁöÑÁµêÊßãË≥áË®äÂíåÂèØÂçÄÂàÜÊÄß„ÄÇÂú®‰∏âÂÄãÂúñÂΩ¢Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶Ë≠âÁµêÊûúË≠âÊòé‰∫ÜÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂÑ™ÊñºÂÖ∂Â∞çÊáâÊñπÊ≥ï„ÄÇ</paragraph>

##### **The single-use restriction for register automata and transducers over infinite alphabets**
2406.18934v1 by Rafa≈Ç Stefa≈Ñski

This thesis studies the single-use restriction for register automata and
transducers over infinite alphabets. The restriction requires that a
read-access to a register should have the side effect of destroying its
contents. This constraint results in robust classes of languages and
transductions. For automata models, we show that one-way register automata,
two-way register automata, and orbit-finite monoids have the same expressive
power. For transducer models, we show that single-use Mealy machines and
single-use two-way transducers admit versions of the Krohn-Rhodes decomposition
theorem. Moreover, single-use Mealy machines are equivalent to an algebraic
model called local algebraic semigroup transductions. Additionally, we show
that single-use two-way transducers are equivalent to single-use streaming
string transducers (SSTs) over infinite alphabets and to regular list functions
with atoms.
  Compared with the previous work arXiv:1907.10504, this thesis offers a
coherent narrative on the single-use restriction. We introduce an abstract
notion of single-use functions and use them to define all the discussed
single-use models. We also introduce and study the algebraic models of local
semigroup transduction and local rational semigroup transduction.

ÊëòË¶ÅÔºöÊú¨Ë´ñÊñáÊé¢Ë®é‰∫ÜÁÑ°ÈôêÂ≠óÊØçË°®‰∏äË®ªÂÜäËá™ÂãïÊ©üÂíåËΩâÊèõÂô®ÁöÑÂñÆÊ¨°‰ΩøÁî®ÈôêÂà∂„ÄÇÊ≠§ÈôêÂà∂Ë¶ÅÊ±ÇÂ∞çË®ªÂÜäÂô®ÁöÑËÆÄÂèñÂ≠òÂèñÊáâÂÖ∑ÊúâÈä∑ÊØÄÂÖ∂ÂÖßÂÆπÁöÑÂâØ‰ΩúÁî®„ÄÇÊ≠§ÈôêÂà∂ÊúÉÁî¢ÁîüÂº∑ÂÅ•ÁöÑË™ûË®ÄÂíåËΩâÊèõÈ°ûÂà•„ÄÇÂ∞çÊñºËá™ÂãïÊ©üÊ®°ÂûãÔºåÊàëÂÄëË≠âÊòé‰∫ÜÂñÆÂêëË®ªÂÜäËá™ÂãïÊ©ü„ÄÅÈõôÂêëË®ªÂÜäËá™ÂãïÊ©üÂíåËªåÈÅìÊúâÈôêÂñÆÂÖÉÂÖ∑ÊúâÁõ∏ÂêåÁöÑË°®ÈÅîËÉΩÂäõ„ÄÇÂ∞çÊñºËΩâÊèõÂô®Ê®°ÂûãÔºåÊàëÂÄëË≠âÊòé‰∫ÜÂñÆÊ¨°‰ΩøÁî® Mealy Ê©üÂô®ÂíåÂñÆÊ¨°‰ΩøÁî®ÈõôÂêëËΩâÊèõÂô®ÂÖÅË®±‰ΩøÁî® Krohn-Rhodes ÂàÜËß£ÂÆöÁêÜ„ÄÇÊ≠§Â§ñÔºåÂñÆÊ¨°‰ΩøÁî® Mealy Ê©üÂô®Á≠âÊñºÁ®±ÁÇ∫Â±ÄÈÉ®‰ª£Êï∏ÂçäÁæ§ËΩâÊèõÁöÑ‰ª£Êï∏Ê®°Âûã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË≠âÊòé‰∫ÜÂñÆÊ¨°‰ΩøÁî®ÈõôÂêëËΩâÊèõÂô®Á≠âÊñºÁÑ°ÈôêÂ≠óÊØçË°®‰∏äÁöÑÂñÆÊ¨°‰ΩøÁî®‰∏≤ÊµÅÂ≠ó‰∏≤ËΩâÊèõÂô® (SST) ÂíåÂÖ∑ÊúâÂéüÂ≠êÁöÑÊ≠£ÂâáÊ∏ÖÂñÆÂáΩÊï∏„ÄÇ
ËàáÂâç‰∏ÄÁØáË´ñÊñá arXiv:1907.10504 Áõ∏ÊØîÔºåÊú¨Ë´ñÊñáÊèê‰æõ‰∫ÜÈóúÊñºÂñÆÊ¨°‰ΩøÁî®ÈôêÂà∂ÁöÑÈÄ£Ë≤´ÊïòËø∞„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÂñÆÊ¨°‰ΩøÁî®ÂáΩÊï∏ÁöÑÊäΩË±°Ê¶ÇÂøµÔºå‰∏¶‰ΩøÁî®ÂÆÉÂÄë‰æÜÂÆöÁæ©ÊâÄÊúâË®éË´ñÈÅéÁöÑÂñÆÊ¨°‰ΩøÁî®Ê®°Âûã„ÄÇÊàëÂÄëÈÇÑ‰ªãÁ¥π‰∏¶Á†îÁ©∂‰∫ÜÂ±ÄÈÉ®ÂçäÁæ§ËΩâÊèõÂíåÂ±ÄÈÉ®ÊúâÁêÜÂçäÁæ§ËΩâÊèõÁöÑ‰ª£Êï∏Ê®°Âûã„ÄÇ

##### **Enhanced ASR Robustness to Packet Loss with a Front-End Adaptation Network**
2406.18928v1 by Yehoshua Dissen, Shiry Yonash, Israel Cohen, Joseph Keshet

In the realm of automatic speech recognition (ASR), robustness in noisy
environments remains a significant challenge. Recent ASR models, such as
Whisper, have shown promise, but their efficacy in noisy conditions can be
further enhanced. This study is focused on recovering from packet loss to
improve the word error rate (WER) of ASR models. We propose using a front-end
adaptation network connected to a frozen ASR model. The adaptation network is
trained to modify the corrupted input spectrum by minimizing the criteria of
the ASR model in addition to an enhancement loss function. Our experiments
demonstrate that the adaptation network, trained on Whisper's criteria, notably
reduces word error rates across domains and languages in packet-loss scenarios.
This improvement is achieved with minimal affect to Whisper model's
foundational performance, underscoring our method's practicality and potential
in enhancing ASR models in challenging acoustic environments.

ÊëòË¶ÅÔºöÂú®Ëá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) È†òÂüü‰∏≠ÔºåÂú®ÊúâÂô™Èü≥ÁöÑÁí∞Â¢É‰∏≠‰øùÊåÅÁ©©ÂÅ•ÊÄß‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÊúÄËøëÁöÑ ASR Ê®°ÂûãÔºå‰æãÂ¶Ç WhisperÔºåÂ∑≤Â±ïÁèæÂâçÊôØÔºå‰ΩÜÂÆÉÂÄëÂú®ÊúâÂô™Èü≥Ê¢ù‰ª∂‰∏ãÁöÑÊïàËÉΩÂèØ‰ª•ÈÄ≤‰∏ÄÊ≠•ÊèêÂçá„ÄÇÊú¨Á†îÁ©∂Â∞àÊ≥®ÊñºÂæûÂ∞ÅÂåÖÈÅ∫Â§±‰∏≠Âæ©ÂéüÔºå‰ª•ÊèêÂçá ASR Ê®°ÂûãÁöÑÂ≠óÂÖÉÈåØË™§Áéá (WER)„ÄÇÊàëÂÄëÂª∫Ë≠∞‰ΩøÁî®ÈÄ£Êé•Âà∞ÂáçÁµê ASR Ê®°ÂûãÁöÑÂâçÁ´ØÈÅ©ÊáâÁ∂≤Ë∑Ø„ÄÇÈÅ©ÊáâÁ∂≤Ë∑ØÁ∂ìÈÅéË®ìÁ∑¥ÔºåÂèØ‰ª•ÈÄèÈÅéÊúÄÂ∞èÂåñ ASR Ê®°ÂûãÁöÑÊ∫ñÂâá‰ª•ÂèäÂ¢ûÂº∑ÊêçÂ§±ÂáΩÊï∏‰æÜ‰øÆÊîπÊêçÂ£ûÁöÑËº∏ÂÖ•È†ªË≠ú„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåÈáùÂ∞ç Whisper ÁöÑÊ∫ñÂâáË®ìÁ∑¥ÁöÑÈÅ©ÊáâÁ∂≤Ë∑ØÔºåÂèØ‰ª•Âú®Â∞ÅÂåÖÈÅ∫Â§±ÁöÑÊÉÖÊ≥Å‰∏ãÈ°ØËëóÈôç‰ΩéË∑®È†òÂüüÂíåË∑®Ë™ûË®ÄÁöÑÂ≠óÂÖÉÈåØË™§Áéá„ÄÇÊ≠§È†ÖÈÄ≤Ê≠•ÊòØÂú®Â∞ç Whisper Ê®°ÂûãÁöÑÂü∫Êú¨ÊïàËÉΩÂΩ±ÈüøÊúÄÂ∞èÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶ÁèæÁöÑÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Â¢ûÂº∑ÂÖ∑ÊåëÊà∞ÊÄßËÅ≤Â≠∏Áí∞Â¢É‰∏≠ÁöÑ ASR Ê®°ÂûãÊñπÈù¢ÁöÑÂØ¶Áî®ÊÄßÂíåÊΩõÂäõ„ÄÇ

##### **Selective Vision is the Challenge for Visual Reasoning: A Benchmark for Visual Argument Understanding**
2406.18925v1 by Jiwan Chung, Sungjae Lee, Minseo Kim, Seungju Han, Ashkan Yousefpour, Jack Hessel, Youngjae Yu

Visual arguments, often used in advertising or social causes, rely on images
to persuade viewers to do or believe something. Understanding these arguments
requires selective vision: only specific visual stimuli within an image are
relevant to the argument, and relevance can only be understood within the
context of a broader argumentative structure. While visual arguments are
readily appreciated by human audiences, we ask: are today's AI capable of
similar understanding?
  We collect and release VisArgs, an annotated corpus designed to make explicit
the (usually implicit) structures underlying visual arguments. VisArgs includes
1,611 images accompanied by three types of textual annotations: 5,112 visual
premises (with region annotations), 5,574 commonsense premises, and reasoning
trees connecting them to a broader argument. We propose three tasks over
VisArgs to probe machine capacity for visual argument understanding:
localization of premises, identification of premises, and deduction of
conclusions. Experiments demonstrate that 1) machines cannot fully identify the
relevant visual cues. The top-performing model, GPT-4-O, achieved an accuracy
of only 78.5%, whereas humans reached 98.0%. All models showed a performance
drop, with an average decrease in accuracy of 19.5%, when the comparison set
was changed from objects outside the image to irrelevant objects within the
image. Furthermore, 2) this limitation is the greatest factor impacting their
performance in understanding visual arguments. Most models improved the most
when given relevant visual premises as additional inputs, compared to other
inputs, for deducing the conclusion of the visual argument.

ÊëòË¶ÅÔºöË¶ñË¶∫Ë´ñË≠âÁ∂ìÂ∏∏‰ΩøÁî®Âú®Âª£ÂëäÊàñÁ§æÊúÉË≠∞È°å‰∏≠ÔºåÂà©Áî®ÂúñÁâáË™™ÊúçËßÄÁúæÊé°ÂèñË°åÂãïÊàñÁõ∏‰ø°Êüê‰ª∂‰∫ã„ÄÇ‰∫ÜËß£ÈÄô‰∫õË´ñË≠âÈúÄË¶ÅÈÅ∏ÊìáÊÄßË¶ñË¶∫ÔºöÂúñÂÉè‰∏≠Âè™ÊúâÁâπÂÆöÁöÑË¶ñË¶∫Âà∫ÊøÄËàáË´ñË≠âÁõ∏ÈóúÔºåËÄåÁõ∏ÈóúÊÄßÂè™ËÉΩÂú®Êõ¥Âª£Ê≥õÁöÑË´ñË≠âÁµêÊßã‰∏≠ÁêÜËß£„ÄÇÈõñÁÑ∂Ë¶ñË¶∫Ë´ñË≠âÂæàÂÆπÊòìË¢´‰∫∫È°ûÂèóÁúæÁêÜËß£Ôºå‰ΩÜÊàëÂÄëÊÉ≥Áü•ÈÅìÔºöÁï∂‰ªäÁöÑ‰∫∫Â∑•Êô∫ÊÖßÊòØÂê¶ËÉΩÊúâÈ°û‰ººÁöÑÁêÜËß£Ôºü
ÊàëÂÄëÊî∂ÈõÜ‰∏¶ÁôºÂ∏É VisArgsÔºåÈÄôÊòØ‰∏ÄÂÄãÂ∏∂ÊúâË®ªÈáãÁöÑË™ûÊñôÂ∫´ÔºåÊó®Âú®ÊòéÁ¢∫Ë¶ñË¶∫Ë´ñË≠âËÉåÂæåÔºàÈÄöÂ∏∏ÊòØÈö±Âê´ÁöÑÔºâÁµêÊßã„ÄÇVisArgs ÂåÖÂê´ 1,611 ÂºµÂúñÁâáÔºå‰∏¶ÈôÑÊúâ‰∏âÁ®ÆÈ°ûÂûãÁöÑÊñáÊú¨Ë®ªÈáãÔºö5,112 ÂÄãË¶ñË¶∫ÂâçÊèêÔºàÂ∏∂ÊúâÂçÄÂüüË®ªÈáãÔºâ„ÄÅ5,574 ÂÄãÂ∏∏Ë≠òÂâçÊèêÔºå‰ª•ÂèäÂ∞áÂÆÉÂÄëËàáÊõ¥Âª£Ê≥õÁöÑË´ñË≠âËÅØÁπ´Ëµ∑‰æÜÁöÑÊé®ÁêÜÊ®π„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏âÈ†ÖÈóúÊñº VisArgs ÁöÑ‰ªªÂãôÔºå‰ª•Êé¢Ë®éÊ©üÂô®Âú®Ë¶ñË¶∫Ë´ñË≠âÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõÔºöÂâçÊèêÂÆö‰Ωç„ÄÅÂâçÊèêË≠òÂà•ÂíåÁµêË´ñÊé®Ë´ñ„ÄÇÂØ¶È©óË°®Êòé 1) Ê©üÂô®ÁÑ°Ê≥ïÂÆåÂÖ®Ë≠òÂà•Áõ∏ÈóúÁöÑË¶ñË¶∫Á∑öÁ¥¢„ÄÇË°®ÁèæÊúÄÂ•ΩÁöÑÊ®°Âûã GPT-4-O ÂÉÖÈÅîÂà∞ 78.5% ÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåËÄå‰∫∫È°ûÈÅîÂà∞ 98.0%„ÄÇÁï∂ÊØîËºÉÈõÜÂæûÂúñÂÉèÂ§ñÈÉ®Â∞çË±°ËÆäÁÇ∫ÂúñÂÉèÂÖßÈÉ®ÁÑ°ÈóúÂ∞çË±°ÊôÇÔºåÊâÄÊúâÊ®°ÂûãÈÉΩË°®ÁèæÂá∫ÊÄßËÉΩ‰∏ãÈôçÔºåÊ∫ñÁ¢∫Â∫¶Âπ≥Âùá‰∏ãÈôç 19.5%„ÄÇÊ≠§Â§ñÔºå2) ÈÄôÁ®ÆÈôêÂà∂ÊòØÂΩ±ÈüøÂÆÉÂÄëÁêÜËß£Ë¶ñË¶∫Ë´ñË≠âË°®ÁèæÁöÑÊúÄÂ§ßÂõ†Á¥†„ÄÇËàáÂÖ∂‰ªñËº∏ÂÖ•Áõ∏ÊØîÔºåÂ§ßÂ§öÊï∏Ê®°ÂûãÂú®Áç≤ÂæóÁõ∏ÈóúË¶ñË¶∫ÂâçÊèê‰ΩúÁÇ∫ÈôÑÂä†Ëº∏ÂÖ•ÊôÇÔºåÂú®Êé®Ë´ñË¶ñË¶∫Ë´ñË≠âÁµêË´ñÊñπÈù¢ÁöÑÈÄ≤Ê≠•ÊúÄÂ§ß„ÄÇ

##### **Time Matters: Scaling Laws for Any Budget**
2406.18922v1 by Itay Inbar, Luke Sernau

A primary cost driver for training large models is wall-clock training time.
We show that popular time estimates based on FLOPs are poor estimates, and
construct a more accurate proxy based on memory copies. We show that with some
simple accounting, we can estimate the training speed of a transformer model
from its hyperparameters. Combined with a scaling law curve like Chinchilla,
this lets us estimate the final loss of the model. We fit our estimate to real
data with a linear regression, and apply the result to rewrite Chinchilla in
terms of a model's estimated training time as opposed to the amount of training
data. This gives an expression for the loss in terms of the model's
hyperparameters alone. We show that this expression is accurate across a wide
range of model hyperparameter values, enabling us to analytically make
architectural decisions and train models more efficiently.

ÊëòË¶ÅÔºöÂ§ßÂûãÊ®°ÂûãË®ìÁ∑¥ÁöÑ‰∏ªË¶ÅÊàêÊú¨È©ÖÂãïÂõ†Á¥†ÊòØÂØ¶ÈöõË®ìÁ∑¥ÊôÇÈñì„ÄÇ
ÊàëÂÄëÈ°ØÁ§∫Âü∫Êñº FLOP ÁöÑÁÜ±ÈñÄÊôÇÈñì‰º∞Ë®àÂÄºÊòØËºÉÂ∑ÆÁöÑ‰º∞Ë®àÂÄºÔºå
‰∏¶Ê†πÊìöË®òÊÜ∂È´îË§áË£ΩÂª∫ÊßãÊõ¥Ê∫ñÁ¢∫ÁöÑ‰ª£ÁêÜ„ÄÇÊàëÂÄëÈ°ØÁ§∫ÔºåÈÄèÈÅé‰∏Ä‰∫õ
Á∞°ÂñÆÁöÑË®àÁÆóÔºåÊàëÂÄëÂèØ‰ª•ÂæûÂÖ∂Ë∂ÖÂèÉÊï∏‰º∞Ë®àTransformerÊ®°ÂûãÁöÑË®ìÁ∑¥ÈÄüÂ∫¶„ÄÇÁµêÂêàÈ°û‰ºº Chinchilla ÁöÑÊØî‰æãÂÆöÂæãÊõ≤Á∑öÔºå
ÈÄôËÆìÊàëÂÄëÂèØ‰ª•‰º∞Ë®àÊ®°ÂûãÁöÑÊúÄÁµÇÊêçÂ§±„ÄÇÊàëÂÄë‰ΩøÁî®Á∑öÊÄßÂõûÊ≠∏Â∞áÊàëÂÄëÁöÑ‰º∞Ë®àÂÄºÂ•óÁî®Âà∞ÁúüÂØ¶
Ë≥áÊñôÔºå‰∏¶Â∞áÁµêÊûúÂ•óÁî®Âú® Chinchilla ‰∏≠Ôºå‰ª•Ê®°ÂûãÁöÑÈ†ê‰º∞Ë®ìÁ∑¥ÊôÇÈñìÈáçÂØ´ÔºåËÄå‰∏çÊòØË®ìÁ∑¥
Ë≥áÊñôÈáè„ÄÇÈÄôÁµ¶Âá∫‰∫ÜÂÉÖÊ†πÊìöÊ®°ÂûãË∂ÖÂèÉÊï∏ÁöÑÊêçÂ§±Ë°®ÈÅîÂºè„ÄÇÊàëÂÄëÈ°ØÁ§∫Ê≠§Ë°®ÈÅîÂºèÂú®Âª£Ê≥õ
ÁöÑÊ®°ÂûãË∂ÖÂèÉÊï∏ÂÄºÁØÑÂúçÂÖßÊòØÊ∫ñÁ¢∫ÁöÑÔºå‰ΩøÊàëÂÄëËÉΩÂ§†ÂàÜÊûêÊÄßÂú∞ÂÅöÂá∫
Êû∂ÊßãÊ±∫Á≠ñ‰∏¶Êõ¥ÊúâÊïàÁéáÂú∞Ë®ìÁ∑¥Ê®°Âûã„ÄÇ

##### **Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data**
2406.18921v1 by Yiting Ran, Xintao Wang, Rui Xu, Xinfeng Yuan, Jiaqing Liang, Yanghua Xiao, Deqing Yang

Role-playing agents (RPA) have been a popular application area for large
language models (LLMs), attracting significant interest from both industry and
academia.While existing RPAs well portray the characters' knowledge and tones,
they face challenges in capturing their minds, especially for small
role-playing language models (RPLMs). In this paper, we propose to enhance
RPLMs via personality-indicative data. Specifically, we leverage questions from
psychological scales and distill advanced RPAs to generate dialogues that grasp
the minds of characters. Experimental results validate that RPLMs trained with
our dataset exhibit advanced role-playing capabilities for both general and
personality-related evaluations. Code and data are available at
\href{https://github.com/alienet1109/RolePersonality}{this URL}.

ÊëòË¶ÅÔºöËßíËâ≤ÊâÆÊºî‰ª£ÁêÜÔºàRPAÔºâ‰∏ÄÁõ¥ÊòØÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÁÉ≠Èó®Â∫îÁî®È¢ÜÂüüÔºåÂê∏Âºï‰∫Ü‰∏öÁïåÂíåÂ≠¶ÊúØÁïåÁöÑÊûÅÂ§ßÂÖ¥Ë∂£„ÄÇËôΩÁÑ∂Áé∞ÊúâÁöÑ RPA ÂæàÂ•ΩÁöÑÊèèÁªò‰∫ÜËßíËâ≤ÁöÑÁü•ËØÜÂíåËØ≠Ë∞ÉÔºå‰ΩÜÂÆÉ‰ª¨Âú®ÊçïÊçâËßíËâ≤ÁöÑÂøÉÊô∫ÊñπÈù¢Èù¢‰∏¥ÊåëÊàòÔºåÁâπÂà´ÊòØÂØπ‰∫éÂ∞èÂûãËßíËâ≤ÊâÆÊºîËØ≠Ë®ÄÊ®°ÂûãÔºàRPLMÔºâ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫ÈÄöËøá‰∫∫Ê†ºÊåáÁ§∫Êï∞ÊçÆÊù•Â¢ûÂº∫ RPLM„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨Âà©Áî®ÂøÉÁêÜÈáèË°®ÁöÑÈ¢òÁõÆÂπ∂ÊèêÁÇºÈ´òÁ∫ß RPA Êù•ÁîüÊàêÂØπËØùÔºå‰ª•ÊéåÊè°ËßíËâ≤ÁöÑÂøÉÊô∫„ÄÇÂÆûÈ™åÁªìÊûúÈ™åËØÅ‰∫Ü‰ΩøÁî®Êàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜËÆ≠ÁªÉÁöÑ RPLM Âú®‰∏ÄËà¨Âíå‰∏™ÊÄßÁõ∏ÂÖ≥ËØÑ‰º∞‰∏≠ÈÉΩË°®Áé∞Âá∫È´òÁ∫ßÁöÑËßíËâ≤ÊâÆÊºîËÉΩÂäõ„ÄÇ‰ª£Á†ÅÂíåÊï∞ÊçÆÂèØÂú®\href{https://github.com/alienet1109/RolePersonality}{Ê≠§ÁΩëÂùÄ}Ëé∑Âæó„ÄÇ

##### **TrustUQA: A Trustful Framework for Unified Structured Data Question Answering**
2406.18916v1 by Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang, Huajun Chen

Natural language question answering (QA) over structured data sources such as
tables and knowledge graphs (KGs) have been widely investigated, for example
with Large Language Models (LLMs). The main solutions include question to
formal query parsing and retrieval-based answer generation. However, current
methods of the former often suffer from weak generalization, failing to dealing
with multiple sources simultaneously, while the later is limited in
trustfulness. In this paper, we propose UnifiedTQA, a trustful QA framework
that can simultaneously support multiple types of structured data in a unified
way. To this end, it adopts an LLM-friendly and unified knowledge
representation method called Condition Graph (CG), and uses an LLM and
demonstration-based two-level method for CG querying. For enhancement, it is
also equipped with dynamic demonstration retrieval. We have evaluated
UnifiedTQA with 5 benchmarks covering 3 types of structured data. It
outperforms 2 existing unified structured data QA methods and in comparison
with the baselines that are specific to a data type, it achieves
state-of-the-art on 2 of them. Further more, we demonstrates potential of our
method for more general QA tasks, QA over mixed structured data and QA across
structured data.

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄÂïèÁ≠î (QA) ÈÄèÈÅéÁµêÊßãÂåñË≥áÊñô‰æÜÊ∫êÔºà‰æãÂ¶ÇË°®Ê†ºÂíåÁü•Ë≠òÂúñË≠ú (KGs)ÔºâÂ∑≤Âª£Ê≥õÁ†îÁ©∂Ôºå‰æãÂ¶Ç‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇ‰∏ªË¶ÅËß£Ê±∫ÊñπÊ°àÂåÖÊã¨ÂïèÈ°åËΩâÊèõÊàêÂΩ¢ÂºèÂåñÊü•Ë©¢Ëß£ÊûêÂíåÂü∫ÊñºÊ™¢Á¥¢ÁöÑÁ≠îÊ°àÁî¢Áîü„ÄÇÁÑ∂ËÄåÔºåÂâçËÄÖÁöÑÁèæË°åÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÁî¢ÁîüÂº±Ê≥õÂåñÔºåÁÑ°Ê≥ïÂêåÊôÇËôïÁêÜÂ§öÂÄã‰æÜÊ∫êÔºåËÄåÂæåËÄÖÂâáÂèóÂà∞ÂèØ‰ø°Â∫¶ÁöÑÈôêÂà∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ UnifiedTQAÔºå‰∏ÄÂÄãÂèØ‰ø°Ë≥¥ÁöÑ QA Ê°ÜÊû∂ÔºåËÉΩÂ§†‰ª•Áµ±‰∏ÄÁöÑÊñπÂºèÂêåÊôÇÊîØÊè¥Â§öÁ®ÆÈ°ûÂûãÁöÑÁµêÊßãÂåñË≥áÊñô„ÄÇÁÇ∫Ê≠§ÔºåÂÆÉÊé°Áî®‰∫Ü‰∏ÄÁ®Æ LLM ÂèãÂñÑ‰∏îÁµ±‰∏ÄÁöÑÁü•Ë≠òË°®Á§∫ÊñπÊ≥ïÔºåÁ®±ÁÇ∫Ê¢ù‰ª∂Âúñ (CG)Ôºå‰∏¶‰ΩøÁî® LLM ÂíåÂü∫ÊñºÁ§∫ÁØÑÁöÑ‰∫åÈöéÊñπÊ≥ïÈÄ≤Ë°å CG Êü•Ë©¢„ÄÇÁÇ∫‰∫ÜÂä†Âº∑ÔºåÂÆÉÈÇÑÈÖçÂÇô‰∫ÜÂãïÊÖãÁ§∫ÁØÑÊ™¢Á¥¢„ÄÇÊàëÂÄëÂ∑≤Á∂ì‰ΩøÁî®Ê∂µËìã 3 Á®ÆÈ°ûÂûãÁµêÊßãÂåñË≥áÊñôÁöÑ 5 ÂÄãÂü∫Ê∫ñË©ï‰º∞ UnifiedTQA„ÄÇÂÆÉÂÑ™Êñº 2 Á®ÆÁèæÊúâÁöÑÁµ±‰∏ÄÁµêÊßãÂåñË≥áÊñô QA ÊñπÊ≥ïÔºå‰∏¶‰∏îËàáÁâπÂÆöÊñºË≥áÊñôÈ°ûÂûãÁöÑÂü∫Á∑öÁõ∏ÊØîÔºåÂÆÉÂú®ÂÖ∂‰∏≠ 2 ÂÄãÂü∫Ê∫ñ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ∞¥Âπ≥„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Êõ¥ÈÄöÁî®ÁöÑ QA ‰ªªÂãô„ÄÅÊ∑∑ÂêàÁµêÊßãÂåñË≥áÊñôÁöÑ QA ÂíåË∑®ÁµêÊßãÂåñË≥áÊñôÁöÑ QA ‰∏≠ÁöÑÊΩõÂäõ„ÄÇ

##### **Factor-Conditioned Speaking-Style Captioning**
2406.18910v1 by Atsushi Ando, Takafumi Moriya, Shota Horiguchi, Ryo Masumura

This paper presents a novel speaking-style captioning method that generates
diverse descriptions while accurately predicting speaking-style information.
Conventional learning criteria directly use original captions that contain not
only speaking-style factor terms but also syntax words, which disturbs learning
speaking-style information. To solve this problem, we introduce
factor-conditioned captioning (FCC), which first outputs a phrase representing
speaking-style factors (e.g., gender, pitch, etc.), and then generates a
caption to ensure the model explicitly learns speaking-style factors. We also
propose greedy-then-sampling (GtS) decoding, which first predicts
speaking-style factors deterministically to guarantee semantic accuracy, and
then generates a caption based on factor-conditioned sampling to ensure
diversity. Experiments show that FCC outperforms the original caption-based
training, and with GtS, it generates more diverse captions while keeping style
prediction performance.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑË™™Ë©±È¢®Ê†ºÂ≠óÂπïÊñπÊ≥ïÔºåÂÆÉËÉΩÁî¢ÁîüÂ§öÊ®£ÂåñÁöÑÊèèËø∞ÔºåÂêåÊôÇÊ∫ñÁ¢∫È†êÊ∏¨Ë™™Ë©±È¢®Ê†º‰ø°ÊÅØ„ÄÇ
ÂÇ≥Áµ±ÁöÑÂ≠∏ÁøíÊ®ôÊ∫ñÁõ¥Êé•‰ΩøÁî®ÂéüÂßãÂ≠óÂπïÔºåÂÖ∂‰∏≠‰∏çÂÉÖÂåÖÂê´Ë™™Ë©±È¢®Ê†ºÂõ†Â≠êË°ìË™ûÔºåÈÇÑÂåÖÂê´Ë™ûÊ≥ïË©ûÔºåÈÄôÊúÉÂπ≤ÊìæÂ≠∏ÁøíË™™Ë©±È¢®Ê†º‰ø°ÊÅØ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÂõ†Â≠êÊ¢ù‰ª∂Â≠óÂπï (FCC)ÔºåÂÆÉÈ¶ñÂÖàËº∏Âá∫‰∏ÄÂÄã‰ª£Ë°®Ë™™Ë©±È¢®Ê†ºÂõ†Â≠êÁöÑÁü≠Ë™ûÔºà‰æãÂ¶ÇÔºåÊÄßÂà•„ÄÅÈü≥È´òÁ≠âÔºâÔºåÁÑ∂ÂæåÁîüÊàê‰∏ÄÂÄãÂ≠óÂπï‰ª•Á¢∫‰øùÊ®°ÂûãÈ°ØÂºèÂú∞Â≠∏ÁøíË™™Ë©±È¢®Ê†ºÂõ†Â≠ê„ÄÇÊàëÂÄëÈÇÑÊèêÂá∫‰∫ÜË≤™Â©™ÁÑ∂ÂæåÊé°Ê®£ÁöÑÔºàGtSÔºâËß£Á¢ºÔºåÂÆÉÈ¶ñÂÖàÁ¢∫ÂÆöÊÄßÂú∞È†êÊ∏¨Ë™™Ë©±È¢®Ê†ºÂõ†Â≠ê‰ª•‰øùË≠âË™ûÁæ©Ê∫ñÁ¢∫ÊÄßÔºåÁÑ∂ÂæåÂü∫ÊñºÂõ†Â≠êÊ¢ù‰ª∂Êé°Ê®£ÁîüÊàê‰∏ÄÂÄãÂ≠óÂπï‰ª•‰øùË≠âÂ§öÊ®£ÊÄß„ÄÇÂØ¶È©óË°®ÊòéÔºåFCC ÂÑ™ÊñºÂü∫ÊñºÂéüÂßãÂ≠óÂπïÁöÑË®ìÁ∑¥Ôºå‰∏¶‰∏î‰ΩøÁî® GtSÔºåÂÆÉÊúÉÁîüÊàêÊõ¥Â§öÊ®£ÂåñÁöÑÂ≠óÂπïÔºåÂêåÊôÇ‰øùÊåÅÈ¢®Ê†ºÈ†êÊ∏¨ÊÄßËÉΩ„ÄÇ

##### **Historia Magistra Vitae: Dynamic Topic Modeling of Roman Literature using Neural Embeddings**
2406.18907v1 by Michael Ginn, Mans Hulden

Dynamic topic models have been proposed as a tool for historical analysis,
but traditional approaches have had limited usefulness, being difficult to
configure, interpret, and evaluate. In this work, we experiment with a recent
approach for dynamic topic modeling using BERT embeddings. We compare topic
models built using traditional statistical models (LDA and NMF) and the
BERT-based model, modeling topics over the entire surviving corpus of Roman
literature. We find that while quantitative metrics prefer statistical models,
qualitative evaluation finds better insights from the neural model.
Furthermore, the neural topic model is less sensitive to hyperparameter
configuration and thus may make dynamic topic modeling more viable for
historical researchers.

ÊëòË¶ÅÔºöÂãïÊÖã‰∏ªÈ°åÊ®°ÂûãÂ∑≤Ë¢´ÊèêË≠∞‰ΩúÁÇ∫Ê≠∑Âè≤ÂàÜÊûêÁöÑÂ∑•ÂÖ∑Ôºå‰ΩÜÂÇ≥Áµ±ÊñπÊ≥ïÁöÑÁî®ÈÄîÊúâÈôêÔºåÂõ†ÁÇ∫Èõ£‰ª•ÈÖçÁΩÆ„ÄÅË©ÆÈáãÂíåË©ï‰º∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄë‰ΩøÁî® BERT ÂÖßÂµåÂ≠óÂÖÉÂêëÈáèÂ∞çÂãïÊÖã‰∏ªÈ°åÂª∫Ê®°ÁöÑÊúÄÊñ∞ÊñπÊ≥ïÈÄ≤Ë°åÂØ¶È©ó„ÄÇÊàëÂÄëÊØîËºÉ‰ΩøÁî®ÂÇ≥Áµ±Áµ±Ë®àÊ®°ÂûãÔºàLDA Âíå NMFÔºâÂíå BERT ÁÇ∫Âü∫Á§éÁöÑÊ®°ÂûãÊâÄÂª∫Á´ãÁöÑ‰∏ªÈ°åÊ®°ÂûãÔºåÂ∞çÁæÖÈ¶¨ÊñáÂ≠∏‰∏≠ÊâÄÊúâÁèæÂ≠òË™ûÊñôÂ∫´ÁöÑ‰∏ªÈ°åÈÄ≤Ë°åÂª∫Ê®°„ÄÇÊàëÂÄëÁôºÁèæÔºåÈõñÁÑ∂ÈáèÂåñÊåáÊ®ôËºÉÂÅèÂ•ΩÁµ±Ë®àÊ®°ÂûãÔºå‰ΩÜÂÆöÊÄßË©ï‰º∞ÁôºÁèæÁ•ûÁ∂ìÊ®°ÂûãÊúâÊõ¥Â•ΩÁöÑË¶ãËß£„ÄÇÊ≠§Â§ñÔºåÁ•ûÁ∂ì‰∏ªÈ°åÊ®°ÂûãÂ∞çË∂ÖÂèÉÊï∏ÈÖçÁΩÆ‰∏çÂ§™ÊïèÊÑüÔºåÂõ†Ê≠§ÂèØËÉΩ‰ΩøÂãïÊÖã‰∏ªÈ°åÂª∫Ê®°Â∞çÊ≠∑Âè≤Á†îÁ©∂‰∫∫Âì°‰æÜË™™Êõ¥ÂèØË°å„ÄÇ

##### **Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets**
2406.18906v1 by Melanie Walsh, Anna Preus, Maria Antoniak

Large language models (LLMs) can now generate and recognize text in a wide
range of styles and genres, including highly specialized, creative genres like
poetry. But what do LLMs really know about poetry? What can they know about
poetry? We develop a task to evaluate how well LLMs recognize a specific aspect
of poetry, poetic form, for more than 20 forms and formal elements in the
English language. Poetic form captures many different poetic features,
including rhyme scheme, meter, and word or line repetition. We use this task to
reflect on LLMs' current poetic capabilities, as well as the challenges and
pitfalls of creating NLP benchmarks for poetry and for other creative tasks. In
particular, we use this task to audit and reflect on the poems included in
popular pretraining datasets. Our findings have implications for NLP
researchers interested in model evaluation, digital humanities and cultural
analytics scholars, and cultural heritage professionals.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁèæÂú®ÂèØ‰ª•ÁîüÊàêÂíåËæ®Ë≠òÂêÑÁ®ÆÈ¢®Ê†ºÂíåÈ°ûÂûãÁöÑÊñáÂ≠óÔºåÂåÖÊã¨È´òÂ∫¶Â∞àÊ•≠Âåñ„ÄÅË©©Ê≠åÁ≠âÂâµÊÑèÈ°ûÂûã„ÄÇ‰ΩÜ LLM Â∞çÊñºË©©Ê≠åÁúüÊ≠£‰∫ÜËß£Â§öÂ∞ëÔºüÂÆÉÂÄëËÉΩ‰∫ÜËß£Ë©©Ê≠åÂì™‰∫õÊñπÈù¢ÔºüÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÈ†Ö‰ªªÂãôÔºåÁî®ÊñºË©ï‰º∞ LLM Ëæ®Ë≠òÁâπÂÆöË©©Ê≠åÈù¢ÂêëÔºàË©©Ê≠åÂΩ¢ÂºèÔºâÁöÑËÉΩÂäõÔºåÊ∂µËìãËã±Ë™û‰∏≠ 20 Â§öÁ®ÆÂΩ¢ÂºèÂíåÂΩ¢ÂºèÂÖÉÁ¥†„ÄÇË©©Ê≠åÂΩ¢ÂºèÊçïÊçâ‰∫ÜË®±Â§ö‰∏çÂêåÁöÑË©©Ê≠åÁâπÂæµÔºåÂåÖÊã¨ÊäºÈüªÊ†ºÂæã„ÄÅÈüªÂæãÂíåÂ≠óË©ûÊàñË°åÈáçË§á„ÄÇÊàëÂÄë‰ΩøÁî®ÈÄôÈ†Ö‰ªªÂãô‰æÜÊé¢Ë®é LLM ÁõÆÂâçÁöÑË©©Ê≠åËÉΩÂäõÔºå‰ª•ÂèäÁÇ∫Ë©©Ê≠åÂíåÂÖ∂‰ªñÂâµÊÑè‰ªªÂãôÂª∫Á´ã NLP Âü∫Ê∫ñÊôÇÊâÄÈù¢Ëá®ÁöÑÊåëÊà∞ÂíåÈô∑Èò±„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄë‰ΩøÁî®ÈÄôÈ†Ö‰ªªÂãô‰æÜÂØ©Êü•ÂíåÊé¢Ë®éÁÜ±ÈñÄÈ†êË®ìÁ∑¥Ë≥áÊñôÈõÜ‰∏≠ÁöÑË©©Ê≠å„ÄÇÊàëÂÄëÁöÑÁôºÁèæÂ∞çÊúâËààË∂£ÈÄ≤Ë°åÊ®°ÂûãË©ï‰º∞ÁöÑ NLP Á†îÁ©∂‰∫∫Âì°„ÄÅÊï∏‰Ωç‰∫∫ÊñáÂíåÊñáÂåñÂàÜÊûêÂ≠∏ËÄÖ‰ª•ÂèäÊñáÂåñÈÅ∫Áî¢Â∞àÊ•≠‰∫∫Â£´ÂÖ∑ÊúâÂïüÁ§∫ÊÑèÁæ©„ÄÇ

##### **The Rise of Artificial Intelligence in Educational Measurement: Opportunities and Ethical Challenges**
2406.18900v1 by Okan Bulut, Maggie Beiting-Parrish, Jodi M. Casabianca, Sharon C. Slater, Hong Jiao, Dan Song, Christopher M. Ormerod, Deborah Gbemisola Fabiyi, Rodica Ivan, Cole Walsh, Oscar Rios, Joshua Wilson, Seyma N. Yildirim-Erbasli, Tarid Wongvorachan, Joyce Xinle Liu, Bin Tan, Polina Morilova

The integration of artificial intelligence (AI) in educational measurement
has revolutionized assessment methods, enabling automated scoring, rapid
content analysis, and personalized feedback through machine learning and
natural language processing. These advancements provide timely, consistent
feedback and valuable insights into student performance, thereby enhancing the
assessment experience. However, the deployment of AI in education also raises
significant ethical concerns regarding validity, reliability, transparency,
fairness, and equity. Issues such as algorithmic bias and the opacity of AI
decision-making processes pose risks of perpetuating inequalities and affecting
assessment outcomes. Responding to these concerns, various stakeholders,
including educators, policymakers, and organizations, have developed guidelines
to ensure ethical AI use in education. The National Council of Measurement in
Education's Special Interest Group on AI in Measurement and Education (AIME)
also focuses on establishing ethical standards and advancing research in this
area. In this paper, a diverse group of AIME members examines the ethical
implications of AI-powered tools in educational measurement, explores
significant challenges such as automation bias and environmental impact, and
proposes solutions to ensure AI's responsible and effective use in education.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÊï¥ÂêàÂú®ÊïôËÇ≤Ê∏¨Èáè‰∏≠Â∑≤ÂæπÂ∫ïÊîπËÆäË©ïÈáèÊñπÊ≥ïÔºåÈÄèÈÅéÊ©üÂô®Â≠∏ÁøíÂíåËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÔºåËÉΩËá™ÂãïË®àÂàÜ„ÄÅÂø´ÈÄüÂÖßÂÆπÂàÜÊûêÂíåÂÄã‰∫∫ÂåñÂõûÈ•ã„ÄÇÈÄô‰∫õÈÄ≤Â±ïÊèê‰æõÂèäÊôÇ„ÄÅ‰∏ÄËá¥ÁöÑÂõûÈ•ãÂíåÂ∞çÂ≠∏ÁîüË°®ÁèæÁöÑÂØ∂Ë≤¥Ë¶ãËß£ÔºåÂæûËÄåÂ¢ûÂº∑Ë©ïÈáèÈ´îÈ©ó„ÄÇÁÑ∂ËÄåÔºåAI Âú®ÊïôËÇ≤‰∏≠ÁöÑÈÉ®ÁΩ≤‰πüÂºïÁôº‰∫ÜÈóúÊñºÊïàÂ∫¶„ÄÅ‰ø°Â∫¶„ÄÅÈÄèÊòéÂ∫¶„ÄÅÂÖ¨Âπ≥ÊÄßÂíåÂÖ¨Ê≠£ÊÄßÁöÑÈáçÂ§ßÂÄ´ÁêÜÂïèÈ°å„ÄÇÊºîÁÆóÊ≥ïÂÅèË™§Âíå AI Ê±∫Á≠ñÈÅéÁ®ãÁöÑ‰∏çÈÄèÊòéÊÄßÁ≠âÂïèÈ°åÔºåÊúÉÈÄ†ÊàêÂª∂Á∫å‰∏çÂπ≥Á≠âÂíåÂΩ±ÈüøË©ïÈáèÁµêÊûúÁöÑÈ¢®Èö™„ÄÇÁÇ∫‰∫ÜÂõûÊáâÈÄô‰∫õÂïèÈ°åÔºåÂåÖÊã¨ÊïôËÇ≤Â∑•‰ΩúËÄÖ„ÄÅÊîøÁ≠ñÂà∂ÂÆöËÄÖÂíåÁµÑÁπîÂú®ÂÖßÁöÑÂêÑÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫Âà∂ÂÆö‰∫ÜÊ∫ñÂâáÔºå‰ª•Á¢∫‰øùÂú®ÊïôËÇ≤‰∏≠‰ΩøÁî®Âêà‰πéÂÄ´ÁêÜÁöÑ AI„ÄÇÂúãÂÆ∂Ê∏¨ÈáèÂßîÂì°ÊúÉÁöÑÊ∏¨ÈáèÂíåÊïôËÇ≤‰∏≠ÁöÑ AI Â∞àÊ•≠ËààË∂£Â∞èÁµÑ (AIME) ‰πüËëóÈáçÊñºÂà∂ÂÆöÂÄ´ÁêÜÊ®ôÊ∫ñÂíåÊé®ÈÄ≤ÈÄôÊñπÈù¢ÁöÑÁ†îÁ©∂„ÄÇÂú®Êú¨Êñá‰∏≠Ôºå‰∏ÄÁæ§Â§öÂÖÉÁöÑ AIME ÊàêÂì°Êé¢Ë®é‰∫Ü AI È©ÖÂãïÂ∑•ÂÖ∑Âú®ÊïôËÇ≤Ê∏¨Èáè‰∏≠ÁöÑÂÄ´ÁêÜÂΩ±ÈüøÔºåÊé¢Ë®é‰∫ÜËá™ÂãïÂåñÂÅèË™§ÂíåÁí∞Â¢ÉÂΩ±ÈüøÁ≠âÈáçÂ§ßÊåëÊà∞Ôºå‰∏¶ÊèêÂá∫‰∫ÜËß£Ê±∫ÊñπÊ°àÔºå‰ª•Á¢∫‰øù AI Âú®ÊïôËÇ≤‰∏≠ÁöÑË≤†Ë≤¨‰ªªÂíåÊúâÊïà‰ΩøÁî®„ÄÇ

##### **Autonomous Control of a Novel Closed Chain Five Bar Active Suspension via Deep Reinforcement Learning**
2406.18899v1 by Nishesh Singh, Sidharth Ramesh, Abhishek Shankar, Jyotishka Duttagupta, Leander Stephen D'Souza, Sanjay Singh

Planetary exploration requires traversal in environments with rugged
terrains. In addition, Mars rovers and other planetary exploration robots often
carry sensitive scientific experiments and components onboard, which must be
protected from mechanical harm. This paper deals with an active suspension
system focused on chassis stabilisation and an efficient traversal method while
encountering unavoidable obstacles. Soft Actor-Critic (SAC) was applied along
with Proportional Integral Derivative (PID) control to stabilise the chassis
and traverse large obstacles at low speeds. The model uses the rover's distance
from surrounding obstacles, the height of the obstacle, and the chassis'
orientation to actuate the control links of the suspension accurately.
Simulations carried out in the Gazebo environment are used to validate the
proposed active system.

ÊëòË¶ÅÔºöË°åÊòüÊé¢Á¥¢ÈúÄË¶ÅÂú®Â¥éÂ∂áÁöÑÂú∞ÂΩ¢‰∏≠Á©øË∂ä„ÄÇÊ≠§Â§ñÔºåÁÅ´ÊòüÊé¢Ê∏¨ËªäÂíåÂÖ∂‰ªñË°åÊòüÊé¢Ê∏¨Ê©üÂô®‰∫∫ÈÄöÂ∏∏ÊúÉÊîúÂ∏∂ÊïèÊÑüÁöÑÁßëÂ≠∏ÂØ¶È©óÂíåÁµÑ‰ª∂ÔºåÂøÖÈ†à‰øùË≠∑ÈÄô‰∫õÁµÑ‰ª∂ÂÖçÊñºÊ©üÊ¢∞ÊêçÂ£û„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫Ü‰∏ÄÂÄã‰∏ªÂãïÊá∏ÂêäÁ≥ªÁµ±ÔºåÂ∞àÊ≥®ÊñºÂ∫ïÁõ§Á©©ÂÆöÂíåÂú®ÈÅáÂà∞‰∏çÂèØÈÅøÂÖçÁöÑÈöúÁ§ôÁâ©ÊôÇÁöÑÊúâÊïàÁ©øË∂äÊñπÊ≥ï„ÄÇËªüÊÄßÂãï‰Ωú-Ë©ïË´ñÂÆ∂ (SAC) ËàáÊØî‰æãÁ©çÂàÜÂæÆÂàÜ (PID) ÊéßÂà∂‰∏ÄËµ∑ÊáâÁî®Ôºå‰ª•Á©©ÂÆöÂ∫ïÁõ§‰∏¶Âú®‰ΩéÈÄü‰∏ãÁ©øË∂äÂ§ßÂûãÈöúÁ§ôÁâ©„ÄÇË©≤Ê®°Âûã‰ΩøÁî®Êé¢Ê∏¨ËªäËàáÂë®ÂúçÈöúÁ§ôÁâ©ÁöÑË∑ùÈõ¢„ÄÅÈöúÁ§ôÁâ©ÁöÑÈ´òÂ∫¶‰ª•ÂèäÂ∫ïÁõ§ÁöÑÊñπÂêë‰æÜÊ∫ñÁ¢∫Âú∞È©ÖÂãïÊá∏ÂêäÁöÑÊéßÂà∂ÈÄ£Ê°ø„ÄÇÂú® Gazebo Áí∞Â¢É‰∏≠ÈÄ≤Ë°åÁöÑÊ®°Êì¨Áî®ÊñºÈ©óË≠âÊâÄÊèêÂá∫ÁöÑ‰∏ªÂãïÁ≥ªÁµ±„ÄÇ

##### **Can we teach language models to gloss endangered languages?**
2406.18895v1 by Michael Ginn, Mans Hulden, Alexis Palmer

Interlinear glossed text (IGT) is a popular format in language documentation
projects, where each morpheme is labeled with a descriptive annotation.
Automating the creation of interlinear glossed text can be desirable to reduce
annotator effort and maintain consistency across annotated corpora. Prior
research has explored a number of statistical and neural methods for
automatically producing IGT.
  As large language models (LLMs) have showed promising results across
multilingual tasks, even for rare, endangered languages, it is natural to
wonder whether they can be utilized for the task of generating IGT. We explore
whether LLMs can be effective at the task of interlinear glossing with
in-context learning, without any traditional training. We propose new
approaches for selecting examples to provide in-context, observing that
targeted selection can significantly improve performance. We find that
LLM-based methods beat standard transformer baselines, despite requiring no
training at all. These approaches still underperform state-of-the-art
supervised systems for the task, but are highly practical for researchers
outside of the NLP community, requiring minimal effort to use.

ÊëòË¶ÅÔºöË∑®Ë™ûË®Ä‰ªªÂãô‰∏≠ÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫‰ª§‰∫∫ÊªøÊÑèÁöÑÊàêÊûúÔºåÂç≥‰ΩøÊòØÂ∞çÊñºÁΩïË¶ãÊàñÁÄïÂç±Ë™ûË®ÄÔºåÂõ†Ê≠§Ëá™ÁÑ∂ÊúÉÂ•ΩÂ•áÂÆÉÂÄëÊòØÂê¶ËÉΩÁî®ÊñºÁî¢Áîü IGT ÁöÑ‰ªªÂãô„ÄÇÊàëÂÄëÊé¢Ë®é LLM ÊòØÂê¶ËÉΩÊúâÊïàÂü∑Ë°åË∑®Ë°åË®ªÈáã‰ªªÂãôÔºåÈÄèÈÅéÊÉÖÂ¢ÉÂ≠∏ÁøíÔºåËÄåÁÑ°ÈúÄ‰ªª‰ΩïÂÇ≥Áµ±Ë®ìÁ∑¥„ÄÇÊàëÂÄëÊèêÂá∫Êñ∞ÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÈÅ∏ÊìáÁØÑ‰æã‰ª•Êèê‰æõÊÉÖÂ¢ÉÔºå‰∏¶ËßÄÂØüÂà∞ÁõÆÊ®ôÈÅ∏ÊìáËÉΩÈ°ØËëóÊèêÂçáÊïàËÉΩ„ÄÇÊàëÂÄëÁôºÁèæÔºåÂÑòÁÆ° LLM-based ÊñπÊ≥ï‰∏çÈúÄË¶Å‰ªª‰ΩïË®ìÁ∑¥Ôºå‰ΩÜ‰ªçÂãùÈÅéÊ®ôÊ∫ñÁöÑ transformer Âü∫Ê∫ñ„ÄÇÈÄô‰∫õÊñπÊ≥ïÂú®‰ªªÂãô‰∏äÁöÑË°®Áèæ‰ªç‰∏çÂ¶ÇÊúÄÂÖàÈÄ≤ÁöÑÁõ£Áù£ÂºèÁ≥ªÁµ±Ôºå‰ΩÜÂ∞çÊñº NLP Á§æÁæ§‰ª•Â§ñÁöÑÁ†îÁ©∂‰∫∫Âì°‰æÜË™™ÈùûÂ∏∏ÂØ¶Áî®Ôºå‰ΩøÁî®ÊôÇÊâÄÈúÄÁöÑÂä™ÂäõÊ•µÂ∞ë„ÄÇ

##### **SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models**
2406.18880v1 by Vipul Rathore, Aniruddha Deb, Ankish Chandresh, Parag Singla, Mausam

Recently, very large language models (LLMs) have shown exceptional
performance on several English NLP tasks with just in-context learning (ICL),
but their utility in other languages is still underexplored. We investigate
their effectiveness for NLP tasks in low-resource languages (LRLs), especially
in the setting of zero-labelled cross-lingual transfer (0-CLT), where no
labelled training data for the target language is available -- however training
data from one or more related medium-resource languages (MRLs) is utilized,
alongside the available unlabeled test data for a target language. We introduce
Self-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT
setting.
  SSP is based on the key observation that LLMs output more accurate labels if
in-context exemplars are from the target language (even if their labels are
slightly noisy). To operationalize this, since target language training data is
not available in 0-CLT, SSP operates in two stages. In Stage I, using source
MRL training data, target language's test data is noisily labeled. In Stage II,
these noisy test data points are used as exemplars in ICL for further improved
labelling. Additionally, our implementation of SSP uses a novel Integer Linear
Programming (ILP)-based exemplar selection that balances similarity, prediction
confidence (when available) and label coverage. Experiments on three tasks and
eleven LRLs (from three regions) demonstrate that SSP strongly outperforms
existing SOTA fine-tuned and prompting-based baselines in 0-CLT setup.

ÊëòË¶ÅÔºöÊúÄËøëÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®‰ªÖ‰ΩøÁî®ËØ≠Â¢ÉÂ≠¶‰π† (ICL) ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂú®Â§öÈ°πËã±ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨Âú®ÂÖ∂‰ªñËØ≠Ë®Ä‰∏≠ÁöÑÊïàÁî®‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊàë‰ª¨Ë∞ÉÊü•‰∫ÜÂÆÉ‰ª¨Âú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä (LRL) ‰∏≠ÊâßË°åËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°ÁöÑÊúâÊïàÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Èõ∂Ê†áËÆ∞Ë∑®ËØ≠Ë®ÄËøÅÁßª (0-CLT) ÁöÑËÆæÁΩÆ‰∏≠ÔºåÂÖ∂‰∏≠Ê≤°ÊúâÁõÆÊ†áËØ≠Ë®ÄÁöÑÊ†áËÆ∞ËÆ≠ÁªÉÊï∞ÊçÆÂèØÁî®‚Äî‚Äî‰ΩÜÊù•Ëá™‰∏Ä‰∏™ÊàñÂ§ö‰∏™Áõ∏ÂÖ≥ÁöÑ‰∏≠Á≠âËµÑÊ∫êËØ≠Ë®Ä (MRL) ÁöÑËÆ≠ÁªÉÊï∞ÊçÆË¢´Âà©Áî®Ôºå‰ª•ÂèäÁõÆÊ†áËØ≠Ë®ÄÁöÑÂèØÁî®Êú™Ê†áËÆ∞ÊµãËØïÊï∞ÊçÆ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜËá™ÁõëÁù£ÊèêÁ§∫ (SSP)ÔºåËøôÊòØ‰∏ÄÁßçÈíàÂØπ 0-CLT ËÆæÁΩÆÈáèË∫´ÂÆöÂà∂ÁöÑÊñ∞Âûã ICL ÊñπÊ≥ï„ÄÇ
SSP Âü∫‰∫é‰∏Ä‰∏™ÂÖ≥ÈîÆËßÇÂØüÔºåÂç≥Â¶ÇÊûúËØ≠Â¢ÉÁ§∫‰æãÊù•Ëá™ÁõÆÊ†áËØ≠Ë®ÄÔºàÂç≥‰ΩøÂÆÉ‰ª¨ÁöÑÊ†áÁ≠æÊúâÁÇπÂòàÊùÇÔºâÔºåLLM ËæìÂá∫ÁöÑÊ†áÁ≠æ‰ºöÊõ¥ÂáÜÁ°Æ„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞Ëøô‰∏ÄÁÇπÔºåÁî±‰∫éÂú® 0-CLT ‰∏≠Ê≤°ÊúâÁõÆÊ†áËØ≠Ë®ÄËÆ≠ÁªÉÊï∞ÊçÆÔºåSSP ÂàÜ‰∏§‰∏™Èò∂ÊÆµÊìç‰Ωú„ÄÇÂú®Á¨¨‰∏ÄÈò∂ÊÆµÔºå‰ΩøÁî®Ê∫ê MRL ËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØπÁõÆÊ†áËØ≠Ë®ÄÁöÑÊµãËØïÊï∞ÊçÆËøõË°åÂô™Â£∞Ê†áËÆ∞„ÄÇÂú®Á¨¨‰∫åÈò∂ÊÆµÔºåËøô‰∫õÂòàÊùÇÁöÑÊµãËØïÊï∞ÊçÆÁÇπË¢´Áî®‰Ωú ICL ‰∏≠ÁöÑÁ§∫‰æãÔºå‰ª•Ëøõ‰∏ÄÊ≠•ÊîπËøõÊ†áËÆ∞„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂØπ SSP ÁöÑÂÆûÁé∞‰ΩøÁî®‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂü∫‰∫éÊï¥Êï∞Á∫øÊÄßËßÑÂàí (ILP) ÁöÑÁ§∫‰æãÈÄâÊã©ÔºåÂÆÉÂπ≥Ë°°‰∫ÜÁõ∏‰ººÊÄß„ÄÅÈ¢ÑÊµãÁΩÆ‰ø°Â∫¶ÔºàÂ¶ÇÊûúÂèØÁî®ÔºâÂíåÊ†áÁ≠æË¶ÜÁõñÁéá„ÄÇÂØπ‰∏â‰∏™‰ªªÂä°ÂíåÂçÅ‰∏Ä‰∏™ LRLÔºàÊù•Ëá™‰∏â‰∏™Âå∫ÂüüÔºâÁöÑÂÆûÈ™åË°®ÊòéÔºåSSP Âú® 0-CLT ËÆæÁΩÆ‰∏≠ÊòéÊòæ‰ºò‰∫éÁé∞ÊúâÁöÑ SOTA ÂæÆË∞ÉÂíåÂü∫‰∫éÊèêÁ§∫ÁöÑÂü∫ÂáÜ„ÄÇ

##### **DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment**
2406.18871v1 by Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, He Huang, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee

Recent speech language models (SLMs) typically incorporate pre-trained speech
models to extend the capabilities from large language models (LLMs). In this
paper, we propose a Descriptive Speech-Text Alignment approach that leverages
speech captioning to bridge the gap between speech and text modalities,
enabling SLMs to interpret and generate comprehensive natural language
descriptions, thereby facilitating the capability to understand both linguistic
and non-linguistic features in speech. Enhanced with the proposed approach, our
model demonstrates superior performance on the Dynamic-SUPERB benchmark,
particularly in generalizing to unseen tasks. Moreover, we discover that the
aligned model exhibits a zero-shot instruction-following capability without
explicit speech instruction tuning. These findings highlight the potential to
reshape instruction-following SLMs by incorporating rich, descriptive speech
captions.

ÊëòË¶ÅÔºöÊúÄËøëÁöÑËØ≠Èü≥Ë™ûË®ÄÊ®°Âûã (SLM) ÈÄöÂ∏∏ÊúÉÊï¥ÂêàÈ†êÂÖàË®ìÁ∑¥Â•ΩÁöÑË™ûÈü≥Ê®°ÂûãÔºå‰ª•Êì¥ÂÖÖÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËÉΩÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊèèËø∞ÊÄßË™ûÈü≥ÊñáÂ≠óÂ∞çÈΩäÊñπÊ≥ïÔºåÂà©Áî®Ë™ûÈü≥Â≠óÂπï‰æÜÂΩåÂêàË™ûÈü≥ÂíåÊñáÂ≠óÊ®°Âºè‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåËÆì SLM ËÉΩÂ§†Ë©ÆÈáã‰∏¶Áî¢ÁîüÂÖ®Èù¢ÁöÑËá™ÁÑ∂Ë™ûË®ÄÊèèËø∞ÔºåÈÄ≤ËÄå‰øÉÈÄ≤ÁêÜËß£Ë™ûÈü≥‰∏≠Ë™ûË®ÄÂíåÈùûË™ûË®ÄÁâπÂæµÁöÑËÉΩÂäõ„ÄÇÈÄèÈÅéÂª∫Ë≠∞ÁöÑÊñπÊ≥ïÂº∑ÂåñÂæåÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú® Dynamic-SUPERB Âü∫Ê∫ñ‰∏äÂ±ïÁèæÂá∫ÂÑ™Áï∞ÁöÑÊïàËÉΩÔºåÁâπÂà•ÊòØÂú®Êé®Âª£Âà∞Êú™Ë¶ã‰ªªÂãôÊôÇ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁôºÁèæÂ∞çÈΩäÊ®°ÂûãÂ±ïÁèæÂá∫Èõ∂Ê¨°Â≠∏ÁøíÊåá‰ª§ÈÅµÂæ™ËÉΩÂäõÔºåÁÑ°ÈúÄÊòéÁ¢∫ÁöÑË™ûÈü≥Êåá‰ª§Ë™øÊï¥„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°ØÂá∫ÈÄèÈÅéÊï¥ÂêàË±êÂØåÁöÑÊèèËø∞ÊÄßË™ûÈü≥Â≠óÂπïÔºåÊîπÈÄ†Êåá‰ª§ÈÅµÂæ™ SLM ÁöÑÊΩõÂäõ„ÄÇ

##### **Efficacy of Language Model Self-Play in Non-Zero-Sum Games**
2406.18872v1 by Austen Liao, Nicholas Tomlin, Dan Klein

Game-playing agents like AlphaGo have achieved superhuman performance through
self-play, which is theoretically guaranteed to yield optimal policies in
competitive games. However, most language tasks are partially or fully
cooperative, so it is an open question whether techniques like self-play can
effectively be used to improve language models. We empirically investigate this
question in a negotiation game setting known as Deal or No Deal (DoND).
Crucially, the objective in DoND can be modified to produce a fully cooperative
game, a strictly competitive one, or anything in between. We finetune language
models in self-play over multiple rounds of filtered behavior cloning in DoND
for each of these objectives. Contrary to expectations, we find that language
model self-play leads to significant performance gains in both cooperation and
competition with humans, suggesting that self-play and related techniques have
promise despite a lack of theoretical guarantees.

ÊëòË¶ÅÔºöÂÉè AlphaGo Á≠âÈÅäÊà≤‰ª£ÁêÜÂ∑≤ÈÄèÈÅéËá™Áé©ÈÅäÊà≤ÂèñÂæóË∂Ö‰∫∫È°ûÁöÑË°®ÁèæÔºåÁêÜË´ñ‰∏ä‰øùË≠âÂú®Á´∂Áà≠ÈÅäÊà≤‰∏≠Áî¢ÁîüÊúÄ‰Ω≥Á≠ñÁï•„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏Ë™ûË®Ä‰ªªÂãôÊòØÈÉ®ÂàÜÊàñÂÆåÂÖ®Âêà‰ΩúÁöÑÔºåÂõ†Ê≠§Ëá™Áé©ÈÅäÊà≤Á≠âÊäÄË°ìÊòØÂê¶ËÉΩÊúâÊïàÁî®ÊñºÊîπÂñÑË™ûË®ÄÊ®°ÂûãÔºå‰ªçÊòØ‰∏ÄÂÄãÈñãÊîæÊÄßÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÂú®Á®±ÁÇ∫„ÄåDeal or No Deal„Äç(DoND) ÁöÑÂçîÂïÜÈÅäÊà≤Ë®≠ÂÆö‰∏≠ÔºåÂ∞çÊ≠§ÂïèÈ°åÈÄ≤Ë°åÂØ¶Ë≠âË™øÊü•„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåDoND ‰∏≠ÁöÑÁõÆÊ®ôÂèØ‰ª•‰øÆÊîπÁÇ∫Áî¢ÁîüÂÆåÂÖ®Âêà‰ΩúÁöÑÈÅäÊà≤„ÄÅÂö¥Ê†ºÁöÑÁ´∂Áà≠ÈÅäÊà≤ÔºåÊàñ‰ªãÊñºÂÖ©ËÄÖ‰πãÈñìÁöÑ‰ªª‰ΩïÈÅäÊà≤„ÄÇÊàëÂÄëÂú® DoND ‰∏≠Â∞çÊØèÂÄãÁõÆÊ®ôÈÄ≤Ë°åÂ§öËº™ÈÅéÊøæË°åÁÇ∫Ë§áË£ΩÁöÑËá™Áé©ÈÅäÊà≤ÔºåÂæÆË™øË™ûË®ÄÊ®°Âûã„ÄÇËàáÈ†êÊúüÁõ∏ÂèçÔºåÊàëÂÄëÁôºÁèæË™ûË®ÄÊ®°ÂûãËá™Áé©ÈÅäÊà≤ÊúÉÂú®Ëàá‰∫∫È°ûÁöÑÂêà‰ΩúÂíåÁ´∂Áà≠‰∏≠Â∏∂‰æÜÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçáÔºåÈÄôË°®Á§∫ÂÑòÁÆ°Áº∫‰πèÁêÜË´ñ‰øùË≠âÔºåËá™Áé©ÈÅäÊà≤ÂíåÁõ∏ÈóúÊäÄË°ì‰ªçÊúâÂâçÊôØ„ÄÇ

##### **Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology Report Simplification**
2406.18859v1 by Ziyu Yang, Santhosh Cherian, Slobodan Vucetic

Radiology reports are highly technical documents aimed primarily at
doctor-doctor communication. There has been an increasing interest in sharing
those reports with patients, necessitating providing them patient-friendly
simplifications of the original reports. This study explores the suitability of
large language models in automatically generating those simplifications. We
examine the usefulness of chain-of-thought and self-correction prompting
mechanisms in this domain. We also propose a new evaluation protocol that
employs radiologists and laypeople, where radiologists verify the factual
correctness of simplifications, and laypeople assess simplicity and
comprehension. Our experimental results demonstrate the effectiveness of
self-correction prompting in producing high-quality simplifications. Our
findings illuminate the preferences of radiologists and laypeople regarding
text simplification, informing future research on this topic.

ÊëòË¶ÅÔºöÊîæÂ∞ÑÁßëÂ†±ÂëäÊòØÈ´òÂ∫¶ÊäÄË°ìÊÄßÁöÑÊñá‰ª∂Ôºå‰∏ªË¶ÅÁî®ÊñºÈÜ´ÁîüËàáÈÜ´Áîü‰πãÈñìÁöÑÊ∫ùÈÄö„ÄÇ‰∫∫ÂÄëË∂ä‰æÜË∂äÊúâËààË∂£ËàáÊÇ£ËÄÖÂàÜ‰∫´ÈÄô‰∫õÂ†±ÂëäÔºåÂõ†Ê≠§ÂøÖÈ†àÁÇ∫ÊÇ£ËÄÖÊèê‰æõÂéüÂßãÂ†±ÂëäÁöÑÊÇ£ËÄÖÂèãÂñÑÁ∞°ÂåñÁâà„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®éÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®Ëá™ÂãïÁî¢ÁîüÈÄô‰∫õÁ∞°ÂåñÁâàÁöÑÈÅ©Áî®ÊÄß„ÄÇÊàëÂÄëÊ™¢Ë¶ñ‰∫ÜÈèàÂºèÊÄùËÄÉÂíåËá™Ë®ÇÊ≠£ÊèêÁ§∫Ê©üÂà∂Âú®ÈÄôÂÄãÈ†òÂüü‰∏≠ÁöÑÊïàÁî®„ÄÇÊàëÂÄë‰πüÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑË©ï‰º∞ÂçîÂÆöÔºåÊé°Áî®ÊîæÂ∞ÑÁßëÈÜ´Â∏´ÂíåÂ§ñË°å‰∫∫ÔºåÂÖ∂‰∏≠ÊîæÂ∞ÑÁßëÈÜ´Â∏´È©óË≠âÁ∞°ÂåñÁâàÁöÑÊ≠£Á¢∫ÊÄßÔºåËÄåÂ§ñË°å‰∫∫ÂâáË©ï‰º∞Á∞°ÊΩîÊÄßÂíåÁêÜËß£Â∫¶„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜËá™Ë®ÇÊ≠£ÊèêÁ§∫Âú®Áî¢ÁîüÈ´òÂìÅË≥™Á∞°ÂåñÁâà‰∏äÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÁöÑÁôºÁèæÈó°Êòé‰∫ÜÊîæÂ∞ÑÁßëÈÜ´Â∏´ÂíåÂ§ñË°å‰∫∫Â∞çÊñáÂ≠óÁ∞°ÂåñÁöÑÂÅèÂ•ΩÔºåÁÇ∫Êú™‰æÜÈáùÂ∞çÊ≠§‰∏ªÈ°åÁöÑÁ†îÁ©∂Êèê‰æõË≥áË®ä„ÄÇ

##### **FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus**
2406.18856v1 by Yuxin Fu, Shijing Si, Leyi Mai, Xi-ang Li

Large Language Models (LLMs) have stunningly advanced the field of machine
translation, though their effectiveness within the financial domain remains
largely underexplored. To probe this issue, we constructed a fine-grained
Chinese-English parallel corpus of financial news called FFN. We acquired
financial news articles spanning between January 1st, 2014, to December 31,
2023, from mainstream media websites such as CNN, FOX, and China Daily. The
dataset consists of 1,013 main text and 809 titles, all of which have been
manually corrected. We measured the translation quality of two LLMs -- ChatGPT
and ERNIE-bot, utilizing BLEU, TER and chrF scores as the evaluation metrics.
For comparison, we also trained an OpenNMT model based on our dataset. We
detail problems of LLMs and provide in-depth analysis, intending to stimulate
further research and solutions in this largely uncharted territory. Our
research underlines the need to optimize LLMs within the specific field of
financial translation to ensure accuracy and quality.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) È©ö‰∫∫Âú∞Êé®ÈÄ≤‰∫ÜÊ©üÂô®ÁøªË≠ØÈ†òÂüüÔºåÂÑòÁÆ°ÂÆÉÂÄëÂú®ÈáëËûçÈ†òÂüüÁöÑÊúâÊïàÊÄßÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªçÊú™ÂæóÂà∞Êé¢Á¥¢„ÄÇÁÇ∫‰∫ÜÊé¢Ë®éÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊßãÂª∫‰∫Ü‰∏ÄÂÄãÁ¥∞Á≤íÂ∫¶ÁöÑ‰∏≠Ëã±Âπ≥Ë°åË™ûÊñôÂ∫´ÔºåÁ®±ÁÇ∫ FFN„ÄÇÊàëÂÄëÂæû CNN„ÄÅFOX Âíå‰∏≠ÂúãÊó•Â†±Á≠â‰∏ªÊµÅÂ™íÈ´îÁ∂≤Á´ôÁç≤Âèñ‰∫ÜÂæû 2014 Âπ¥ 1 Êúà 1 Êó•Âà∞ 2023 Âπ¥ 12 Êúà 31 Êó•ÁöÑË≤°Á∂ìÊñ∞ËÅûÊñáÁ´†„ÄÇË©≤Êï∏ÊìöÈõÜÂåÖÂê´ 1,013 ÁØáÊ≠£ÊñáÂíå 809 ÂÄãÊ®ôÈ°åÔºåÊâÄÊúâÈÄô‰∫õÈÉΩÂ∑≤ÊâãÂãïÊõ¥Ê≠£„ÄÇÊàëÂÄë‰ΩøÁî® BLEU„ÄÅTER Âíå chrF ÂàÜÊï∏‰ΩúÁÇ∫Ë©ï‰º∞ÊåáÊ®ôÔºåÊ∏¨Èáè‰∫ÜÂÖ©ÂÄã LLMÔºàChatGPT Âíå ERNIE-botÔºâÁöÑÁøªË≠ØË≥™Èáè„ÄÇÁÇ∫‰∫ÜÈÄ≤Ë°åÊØîËºÉÔºåÊàëÂÄëÈÇÑÊ†πÊìöÊàëÂÄëÁöÑÊï∏ÊìöÈõÜË®ìÁ∑¥‰∫Ü‰∏ÄÂÄã OpenNMT Ê®°Âûã„ÄÇÊàëÂÄëË©≥Á¥∞Ë™™Êòé‰∫Ü LLM ÁöÑÂïèÈ°å‰∏¶Êèê‰æõÊ∑±ÂÖ•ÂàÜÊûêÔºåÊó®Âú®ÊøÄÂãµÂú®ÈÄôÂÄãÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊú™Áü•ÁöÑÈ†òÂüüÈÄ≤Ë°åÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂ÂíåËß£Ê±∫ÊñπÊ°à„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™ø‰∫ÜÂú®ÈáëËûçÁøªË≠ØÁöÑÁâπÂÆöÈ†òÂüüÂÑ™Âåñ LLM ‰ª•Á¢∫‰øùÊ∫ñÁ¢∫ÊÄßÂíåË≥™ÈáèÁöÑÂøÖË¶ÅÊÄß„ÄÇ

##### **LICO: Large Language Models for In-Context Molecular Optimization**
2406.18851v1 by Tung Nguyen, Aditya Grover

Optimizing black-box functions is a fundamental problem in science and
engineering. To solve this problem, many approaches learn a surrogate function
that estimates the underlying objective from limited historical evaluations.
Large Language Models (LLMs), with their strong pattern-matching capabilities
via pretraining on vast amounts of data, stand out as a potential candidate for
surrogate modeling. However, directly prompting a pretrained language model to
produce predictions is not feasible in many scientific domains due to the
scarcity of domain-specific data in the pretraining corpora and the challenges
of articulating complex problems in natural language. In this work, we
introduce LICO, a general-purpose model that extends arbitrary base LLMs for
black-box optimization, with a particular application to the molecular domain.
To achieve this, we equip the language model with a separate embedding layer
and prediction layer, and train the model to perform in-context predictions on
a diverse set of functions defined over the domain. Once trained, LICO can
generalize to unseen molecule properties simply via in-context prompting. LICO
achieves state-of-the-art performance on PMO, a challenging molecular
optimization benchmark comprising over 20 objective functions.

ÊëòË¶ÅÔºöÂÑ™ÂåñÈªëÁÆ±ÂáΩÊï∏ÊòØÁßëÂ≠∏ÂíåÂ∑•Á®ã‰∏≠ÁöÑÂü∫Êú¨ÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåË®±Â§öÊñπÊ≥ïÊúÉÂ≠∏Áøí‰ª£ÁêÜÂáΩÊï∏ÔºåÂæûÊúâÈôêÁöÑÊ≠∑Âè≤Ë©ï‰º∞‰∏≠‰º∞Ë®àÂ∫ïÂ±§ÁõÆÊ®ô„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄèÈÅéÂú®Â§ßÈáèË≥áÊñô‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥ÔºåÂÖ∑ÂÇôÂº∑Â§ßÁöÑÊ®°ÂºèÂåπÈÖçËÉΩÂäõÔºåÊàêÁÇ∫‰ª£ÁêÜÂª∫Ê®°ÁöÑÊΩõÂú®ÂÄôÈÅ∏ËÄÖ„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÈ†êË®ìÁ∑¥Ë™ûÊñôÂ∫´‰∏≠Áº∫‰πèÁâπÂÆöÈ†òÂüüÁöÑË≥áÊñôÔºå‰ª•ÂèäÁî®Ëá™ÁÑ∂Ë™ûË®ÄË°®ÈÅîË§áÈõúÂïèÈ°åÁöÑÊåëÊà∞ÔºåÁõ¥Êé•ÊèêÁ§∫È†êË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°ÂûãÁî¢ÁîüÈ†êÊ∏¨Âú®Ë®±Â§öÁßëÂ≠∏È†òÂüü‰∏≠‰∏çÂèØË°å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü LICOÔºåÈÄôÊòØ‰∏ÄÂÄãÈÄöÁî®Ê®°ÂûãÔºåÂÆÉÊì¥ÂÖÖ‰∫Ü‰ªªÊÑèÂü∫Á§é LLM ‰ª•ÈÄ≤Ë°åÈªëÁÆ±ÂÑ™ÂåñÔºåÁâπÂà•ÊáâÁî®ÊñºÂàÜÂ≠êÈ†òÂüü„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÁÇ∫Ë™ûË®ÄÊ®°ÂûãÈÖçÂÇô‰∫Ü‰∏ÄÂÄãÂñÆÁç®ÁöÑÂµåÂÖ•Â±§ÂíåÈ†êÊ∏¨Â±§Ôºå‰∏¶Ë®ìÁ∑¥Ê®°ÂûãÂ∞çÂÆöÁæ©Âú®Ë©≤È†òÂüü‰∏äÁöÑÂêÑÁ®ÆÂáΩÊï∏Âü∑Ë°åÊÉÖÂ¢ÉÈ†êÊ∏¨„ÄÇË®ìÁ∑¥ÂæåÔºåLICO ÂèØ‰ª•ÈÄèÈÅéÊÉÖÂ¢ÉÊèêÁ§∫Â∞çÊú™Ë¶ãÁöÑÂàÜÂ≠êÂ±¨ÊÄßÈÄ≤Ë°åÊ¶ÇÂåñ„ÄÇLICO Âú® PMO ‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåPMO ÊòØÂåÖÂê´Ë∂ÖÈÅé 20 ÂÄãÁõÆÊ®ôÂáΩÊï∏ÁöÑÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂàÜÂ≠êÊúÄ‰Ω≥ÂåñÂü∫Ê∫ñ„ÄÇ

##### **Learning Retrieval Augmentation for Personalized Dialogue Generation**
2406.18847v1 by Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, Lilian Tang

Personalized dialogue generation, focusing on generating highly tailored
responses by leveraging persona profiles and dialogue context, has gained
significant attention in conversational AI applications. However, persona
profiles, a prevalent setting in current personalized dialogue datasets,
typically composed of merely four to five sentences, may not offer
comprehensive descriptions of the persona about the agent, posing a challenge
to generate truly personalized dialogues. To handle this problem, we propose
$\textbf{L}$earning Retrieval $\textbf{A}$ugmentation for
$\textbf{P}$ersonalized $\textbf{D}$ial$\textbf{O}$gue $\textbf{G}$eneration
($\textbf{LAPDOG}$), which studies the potential of leveraging external
knowledge for persona dialogue generation. Specifically, the proposed LAPDOG
model consists of a story retriever and a dialogue generator. The story
retriever uses a given persona profile as queries to retrieve relevant
information from the story document, which serves as a supplementary context to
augment the persona profile. The dialogue generator utilizes both the dialogue
history and the augmented persona profile to generate personalized responses.
For optimization, we adopt a joint training framework that collaboratively
learns the story retriever and dialogue generator, where the story retriever is
optimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for
the dialogue generator to generate personalized responses. Experiments
conducted on the CONVAI2 dataset with ROCStory as a supplementary data source
show that the proposed LAPDOG method substantially outperforms the baselines,
indicating the effectiveness of the proposed method. The LAPDOG model code is
publicly available for further exploration.
https://github.com/hqsiswiliam/LAPDOG

ÊëòË¶ÅÔºö<paragraph>ÂÄãÊÄßÂåñÂ∞çË©±ÁîüÊàêÔºåÂ∞àÊ≥®ÊñºÈÄèÈÅéÈÅãÁî®ËßíËâ≤Ê™îÊ°àÂíåÂ∞çË©±ËÑàÁµ°‰æÜÁî¢ÁîüÈ´òÂ∫¶ÂÆ¢Ë£ΩÂåñÁöÑÂõûÊáâÔºåÂ∑≤Âú®Â∞çË©±Âºè AI ÊáâÁî®Á®ãÂºè‰∏≠Áç≤ÂæóÈ°ØËëóÁöÑÈóúÊ≥®„ÄÇÁÑ∂ËÄåÔºåËßíËâ≤Ê™îÊ°àÊòØÁõÆÂâçÂÄãÊÄßÂåñÂ∞çË©±Ë≥áÊñôÈõÜ‰∏≠ÊôÆÈÅçÁöÑË®≠ÂÆöÔºåÈÄöÂ∏∏ÂÉÖÁî±ÂõõÂà∞‰∫îÂÄãÂè•Â≠êÁµÑÊàêÔºåÂèØËÉΩÁÑ°Ê≥ïÊèê‰æõÈóúÊñº‰ª£ÁêÜ‰∫∫ÁöÑËßíËâ≤‰πãÂÖ®Èù¢ÊèèËø∞ÔºåÂ∞çÁî¢ÁîüÁúüÊ≠£ÂÄãÊÄßÂåñÁöÑÂ∞çË©±ÊßãÊàêÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËôïÁêÜÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ $\textbf{L}$earning Retrieval $\textbf{A}$ugmentation for $\textbf{P}$ersonalized $\textbf{D}$ial$\textbf{O}$gue $\textbf{G}$eneration ($\textbf{LAPDOG}$)ÔºåÊé¢Ë®éÈÅãÁî®Â§ñÈÉ®Áü•Ë≠òÈÄ≤Ë°åËßíËâ≤Â∞çË©±ÁîüÊàêÁöÑÊΩõÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊâÄÊèêÂá∫ÁöÑ LAPDOG Ê®°ÂûãÂåÖÂê´‰∏ÄÂÄãÊïÖ‰∫ãÊ™¢Á¥¢Âô®Âíå‰∏ÄÂÄãÂ∞çË©±Áî¢ÁîüÂô®„ÄÇÊïÖ‰∫ãÊ™¢Á¥¢Âô®‰ΩøÁî®Áµ¶ÂÆöÁöÑËßíËâ≤Ê™îÊ°à‰ΩúÁÇ∫Êü•Ë©¢ÔºåÂæûÊïÖ‰∫ãÊñá‰ª∂‰∏≠Ê™¢Á¥¢Áõ∏ÈóúË≥áË®äÔºå‰ΩúÁÇ∫Ë£úÂÖÖËÑàÁµ°‰æÜÊì¥ÂÖÖËßíËâ≤Ê™îÊ°à„ÄÇÂ∞çË©±Áî¢ÁîüÂô®Âà©Áî®Â∞çË©±Ê≠∑Á®ãÂíåÊì¥ÂÖÖÁöÑËßíËâ≤Ê™îÊ°à‰æÜÁî¢ÁîüÂÄãÊÄßÂåñÁöÑÂõûÊáâ„ÄÇÁÇ∫‰∫ÜÊúÄ‰Ω≥ÂåñÔºåÊàëÂÄëÊé°Áî®‰∏ÄÂÄãËÅØÂêàË®ìÁ∑¥Êû∂ÊßãÔºåÂçîÂêåÂ≠∏ÁøíÊïÖ‰∫ãÊ™¢Á¥¢Âô®ÂíåÂ∞çË©±Áî¢ÁîüÂô®ÔºåÂÖ∂‰∏≠ÊïÖ‰∫ãÊ™¢Á¥¢Âô®ÈáùÂ∞çÊâÄÈúÄÁöÑÊúÄÁµÇÊåáÊ®ôÔºà‰æãÂ¶Ç BLEUÔºâÈÄ≤Ë°åÊúÄ‰Ω≥ÂåñÔºå‰ª•Ê™¢Á¥¢ÂÖßÂÆπ‰æõÂ∞çË©±Áî¢ÁîüÂô®Áî¢ÁîüÂÄãÊÄßÂåñÁöÑÂõûÊáâ„ÄÇÂú® CONVAI2 Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óÔºå‰ª• ROCStory ‰ΩúÁÇ∫Ë£úÂÖÖË≥áÊñô‰æÜÊ∫êÔºåÈ°ØÁ§∫ÊâÄÊèêÂá∫ÁöÑ LAPDOG ÊñπÊ≥ïÂ§ßÂπÖÂÑ™ÊñºÂü∫Á∑öÔºåË°®Á§∫ÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊúâÊïà„ÄÇLAPDOG Ê®°ÂûãÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÔºåÂèØ‰æõÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢„ÄÇ
https://github.com/hqsiswiliam/LAPDOG</paragraph>

##### **Retain, Blend, and Exchange: A Quality-aware Spatial-Stereo Fusion Approach for Event Stream Recognition**
2406.18845v1 by Lan Chen, Dong Li, Xiao Wang, Pengpeng Shao, Wei Zhang, Yaowei Wang, Yonghong Tian, Jin Tang

Existing event stream-based pattern recognition models usually represent the
event stream as the point cloud, voxel, image, etc., and design various deep
neural networks to learn their features. Although considerable results can be
achieved in simple cases, however, the model performance may be limited by
monotonous modality expressions, sub-optimal fusion, and readout mechanisms. In
this paper, we propose a novel dual-stream framework for event stream-based
pattern recognition via differentiated fusion, termed EFV++. It models two
common event representations simultaneously, i.e., event images and event
voxels. The spatial and three-dimensional stereo information can be learned
separately by utilizing Transformer and Graph Neural Network (GNN). We believe
the features of each representation still contain both efficient and redundant
features and a sub-optimal solution may be obtained if we directly fuse them
without differentiation. Thus, we divide each feature into three levels and
retain high-quality features, blend medium-quality features, and exchange
low-quality features. The enhanced dual features will be fed into the fusion
Transformer together with bottleneck features. In addition, we introduce a
novel hybrid interaction readout mechanism to enhance the diversity of features
as final representations. Extensive experiments demonstrate that our proposed
framework achieves state-of-the-art performance on multiple widely used event
stream-based classification datasets. Specifically, we achieve new
state-of-the-art performance on the Bullying10k dataset, i.e., $90.51\%$, which
exceeds the second place by $+2.21\%$. The source code of this paper has been
released on
\url{https://github.com/Event-AHU/EFV_event_classification/tree/EFVpp}.

ÊëòË¶ÅÔºöÁèæÊúâÁöÑÂü∫Êñº‰∫ã‰ª∂‰∏≤ÊµÅÁöÑÊ®°ÂºèËæ®Ë≠òÊ®°ÂûãÈÄöÂ∏∏Â∞á‰∫ã‰ª∂‰∏≤ÊµÅË°®Á§∫ÁÇ∫ÈªûÈõ≤„ÄÅÈ´îÁ¥†„ÄÅÂΩ±ÂÉèÁ≠âÔºå‰∏¶Ë®≠Ë®àÂêÑÁ®ÆÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø‰æÜÂ≠∏ÁøíÂÖ∂ÁâπÂæµ„ÄÇÂÑòÁÆ°Âú®Á∞°ÂñÆÊ°à‰æã‰∏≠ÂèØ‰ª•Áç≤ÂæóÁõ∏Áï∂ÁöÑÁµêÊûúÔºå‰ΩÜÊ®°ÂûãÊïàËÉΩÂèØËÉΩÊúÉÂèóÂà∞ÂñÆË™øÁöÑÊ®°ÊÖãË°®ÈÅî„ÄÅÊ¨°‰Ω≥ËûçÂêàÂíåËÆÄÂá∫Ê©üÂà∂ÁöÑÈôêÂà∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÈõô‰∏≤ÊµÅÊû∂ÊßãÔºåÁî®ÊñºÈÄèÈÅéÂ∑ÆÁï∞ÂåñËûçÂêàÈÄ≤Ë°åÂü∫Êñº‰∫ã‰ª∂‰∏≤ÊµÅÁöÑÊ®°ÂºèËæ®Ë≠òÔºåÁ®±ÁÇ∫ EFV++„ÄÇÂÆÉÂêåÊôÇÂª∫Ê®°‰∫ÜÂÖ©Á®ÆÂ∏∏Ë¶ãÁöÑ‰∫ã‰ª∂Ë°®Á§∫ÔºåÂç≥‰∫ã‰ª∂ÂΩ±ÂÉèÂíå‰∫ã‰ª∂È´îÁ¥†„ÄÇÁ©∫ÈñìÂíå‰∏âÁ∂≠Á´ãÈ´îË≥áË®äÂèØ‰ª•ÈÄèÈÅé‰ΩøÁî® Transformer ÂíåÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂàÜÂà•Â≠∏Áøí„ÄÇÊàëÂÄëÁõ∏‰ø°ÊØèÂÄãË°®Á§∫ÁöÑÁâπÂæµ‰ªçÁÑ∂ÂåÖÂê´ÊúâÊïàÂíåÂÜóÈ§òÁöÑÁâπÂæµÔºåÂ¶ÇÊûúÊàëÂÄëÂú®Ê≤íÊúâÂçÄÂàÜÁöÑÊÉÖÊ≥Å‰∏ãÁõ¥Êé•ËûçÂêàÂÆÉÂÄëÔºåÂèØËÉΩÊúÉÁç≤ÂæóÊ¨°‰Ω≥ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂ∞áÊØèÂÄãÁâπÂæµÂàÜÊàê‰∏âÂÄãÂ±§Á¥öÔºå‰∏¶‰øùÁïôÈ´òÂìÅË≥™ÁâπÂæµ„ÄÅÊ∑∑Âêà‰∏≠ÂìÅË≥™ÁâπÂæµÔºå‰∏¶‰∫§Êèõ‰ΩéÂìÅË≥™ÁâπÂæµ„ÄÇÂ¢ûÂº∑ÁöÑÈõôÈáçÁâπÂæµÂ∞áËàáÁì∂È†∏ÁâπÂæµ‰∏ÄËµ∑Ëº∏ÂÖ•ËûçÂêà Transformer„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊ∑∑Âêà‰∫íÂãïËÆÄÂá∫Ê©üÂà∂Ôºå‰ª•Â¢ûÂº∑ÁâπÂæµÁöÑÂ§öÊ®£ÊÄß‰ΩúÁÇ∫ÊúÄÁµÇË°®Á§∫„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊû∂ÊßãÂú®Â§öÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫Êñº‰∫ã‰ª∂‰∏≤ÊµÅÁöÑÂàÜÈ°ûË≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÂú® Bullying10k Ë≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫ÜÊñ∞ÁöÑÊúÄÂÖàÈÄ≤ÊïàËÉΩÔºåÂç≥ 90.51%ÔºåÊØîÁ¨¨‰∫åÂêçÈ´òÂá∫ +2.21%„ÄÇÊú¨ÊñáÁöÑÂéüÂßãÁ®ãÂºèÁ¢ºÂ∑≤Âú® https://github.com/Event-AHU/EFV_event_classification/tree/EFVpp ‰∏äÁôºÂ∏É„ÄÇ

##### **Disentangling Knowledge-based and Visual Reasoning by Question Decomposition in KB-VQA**
2406.18839v1 by Elham J. Barezi, Parisa Kordjamshidi

We study the Knowledge-Based visual question-answering problem, for which
given a question, the models need to ground it into the visual modality to find
the answer. Although many recent works use question-dependent captioners to
verbalize the given image and use Large Language Models to solve the VQA
problem, the research results show they are not reasonably performing for
multi-hop questions. Our study shows that replacing a complex question with
several simpler questions helps to extract more relevant information from the
image and provide a stronger comprehension of it. Moreover, we analyze the
decomposed questions to find out the modality of the information that is
required to answer them and use a captioner for the visual questions and LLMs
as a general knowledge source for the non-visual KB-based questions. Our
results demonstrate the positive impact of using simple questions before
retrieving visual or non-visual information. We have provided results and
analysis on three well-known VQA datasets including OKVQA, A-OKVQA, and KRVQA,
and achieved up to 2% improvement in accuracy.

ÊëòË¶ÅÔºöÊàëÂÄëÁ†îÁ©∂‰∫ÜÂü∫ÊñºÁü•Ë≠òÁöÑË¶ñË¶∫ÂïèÁ≠îÂïèÈ°åÔºåÂ∞çÊñºÊ≠§ÂïèÈ°åÔºåÁµ¶ÂÆö‰∏ÄÂÄãÂïèÈ°åÔºåÊ®°ÂûãÈúÄË¶ÅÂ∞áÂÖ∂Âü∫Á§éÂåñÁÇ∫Ë¶ñË¶∫Ê®°Âºè‰ª•ÊâæÂá∫Á≠îÊ°à„ÄÇÂÑòÁÆ°Ë®±Â§öËøëÊúüÁ†îÁ©∂‰ΩøÁî®‰æùË≥¥ÂïèÈ°åÁöÑÊ®ôÈ°åË™™ÊòéÂô®Â∞áÁµ¶ÂÆöÁöÑÂΩ±ÂÉèÂè£Ë™ûÂåñÔºå‰∏¶‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰æÜËß£Ê±∫ VQA ÂïèÈ°åÔºå‰ΩÜÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÂÆÉÂÄëÂ∞çÊñºÂ§öË∑≥ÂïèÈ°åÁöÑÂü∑Ë°å‰∏¶ÈùûÂêàÁêÜ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂È°ØÁ§∫ÔºåÂ∞áË§áÈõúÂïèÈ°åÊõøÊèõÁÇ∫ÂπæÂÄãËºÉÁ∞°ÂñÆÁöÑÂïèÈ°åÊúâÂä©ÊñºÂæûÂΩ±ÂÉè‰∏≠ËêÉÂèñÂá∫Êõ¥Áõ∏ÈóúÁöÑË≥áË®äÔºå‰∏¶Êèê‰æõÂ∞çÂÖ∂Êõ¥Âº∑ÁöÑÁêÜËß£„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂàÜÊûêÂàÜËß£ÁöÑÂïèÈ°åÔºåÊâæÂá∫ÂõûÁ≠îÂÆÉÂÄëÊâÄÈúÄÁöÑË≥áË®äÊ®°ÂºèÔºå‰∏¶‰ΩøÁî®Ê®ôÈ°åË™™ÊòéÂô®ÈÄ≤Ë°åË¶ñË¶∫ÂïèÈ°åÔºå‰∏¶Â∞á LLM ‰ΩúÁÇ∫ÈùûË¶ñË¶∫ KB-based ÂïèÈ°åÁöÑ‰∏ÄËà¨Áü•Ë≠ò‰æÜÊ∫ê„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÊòé‰∫ÜÂú®Êì∑ÂèñË¶ñË¶∫ÊàñÈùûË¶ñË¶∫Ë≥áË®ä‰πãÂâç‰ΩøÁî®Á∞°ÂñÆÂïèÈ°åÁöÑÊ≠£Èù¢ÂΩ±Èüø„ÄÇÊàëÂÄëÂú®‰∏âÂÄãËëóÂêçÁöÑ VQA Ë≥áÊñôÈõÜÔºåÂåÖÊã¨ OKVQA„ÄÅA-OKVQA Âíå KRVQAÔºå‰∏äÊèê‰æõ‰∫ÜÁµêÊûúÂíåÂàÜÊûêÔºå‰∏¶Âú®Ê∫ñÁ¢∫Â∫¶‰∏äÁç≤Âæó‰∫ÜÈ´òÈÅî 2% ÁöÑÊèêÂçá„ÄÇ

##### **OutlierTune: Efficient Channel-Wise Quantization for Large Language Models**
2406.18832v1 by Jinguang Wang, Yuexi Yin, Haifeng Sun, Qi Qi, Jingyu Wang, Zirui Zhuang, Tingting Yang, Jianxin Liao

Quantizing the activations of large language models (LLMs) has been a
significant challenge due to the presence of structured outliers. Most existing
methods focus on the per-token or per-tensor quantization of activations,
making it difficult to achieve both accuracy and hardware efficiency. To
address this problem, we propose OutlierTune, an efficient per-channel
post-training quantization (PTQ) method for the activations of LLMs.
OutlierTune consists of two components: pre-execution of dequantization and
symmetrization. The pre-execution of dequantization updates the model weights
by the activation scaling factors, avoiding the internal scaling and costly
additional computational overheads brought by the per-channel activation
quantization. The symmetrization further reduces the quantization differences
arising from the weight updates by ensuring the balanced numerical ranges
across different activation channels. OutlierTune is easy to implement and
hardware-efficient, introducing almost no additional computational overheads
during the inference. Extensive experiments show that the proposed framework
outperforms existing methods across multiple different tasks. Demonstrating
better generalization, this framework improves the Int6 quantization of the
instruction-tuning LLMs, such as OPT-IML, to the same level as half-precision
(FP16). Moreover, we have shown that the proposed framework is 1.48x faster
than the FP16 implementation while reducing approximately 2x memory usage.

ÊëòË¶ÅÔºöÈáèÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÊøÄÊ¥ª‰∏ÄÁõ¥ÊòØ‰∏ÄÈ°πÈáçÂ§ßÊåëÊàòÔºåÂéüÂõ†Âú®‰∫éÂ≠òÂú®ÁªìÊûÑÂåñÂºÇÂ∏∏ÂÄº„ÄÇÂ§ßÂ§öÊï∞Áé∞ÊúâÊñπÊ≥ï‰∏ìÊ≥®‰∫éÊøÄÊ¥ªÁöÑÈÄê‰ª§ÁâåÊàñÈÄêÂº†ÈáèÈáèÂåñÔºåËøô‰ΩøÂæóÈöæ‰ª•ÂêåÊó∂ÂÆûÁé∞ÂáÜÁ°ÆÊÄßÂíåÁ°¨‰ª∂ÊïàÁéá„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü OutlierTuneÔºåËøôÊòØ‰∏ÄÁßçÈíàÂØπ LLM ÊøÄÊ¥ªÁöÑÈ´òÊïàÈÄêÈÄöÈÅìÂêéËÆ≠ÁªÉÈáèÂåñ (PTQ) ÊñπÊ≥ï„ÄÇOutlierTune ÂåÖÂê´‰∏§‰∏™ÁªÑ‰ª∂ÔºöÂèçÈáèÂåñÁöÑÈ¢ÑÊâßË°åÂíåÂØπÁß∞Âåñ„ÄÇÂèçÈáèÂåñÁöÑÈ¢ÑÊâßË°åÈÄöËøáÊøÄÊ¥ªÊØî‰æãÂõ†Â≠êÊõ¥Êñ∞Ê®°ÂûãÊùÉÈáçÔºåÈÅøÂÖç‰∫ÜÈÄêÈÄöÈÅìÊøÄÊ¥ªÈáèÂåñÂ∏¶Êù•ÁöÑÂÜÖÈÉ®ÊØî‰æãÂíåÊòÇË¥µÁöÑÈ¢ùÂ§ñËÆ°ÁÆóÂºÄÈîÄ„ÄÇÂØπÁß∞ÂåñÈÄöËøáÁ°Æ‰øù‰∏çÂêåÊøÄÊ¥ªÈÄöÈÅì‰πãÈó¥ÁöÑÊï∞ÂÄºËåÉÂõ¥Âπ≥Ë°°ÔºåËøõ‰∏ÄÊ≠•ÂáèÂ∞ë‰∫ÜÁî±ÊùÉÈáçÊõ¥Êñ∞ÂºïËµ∑ÁöÑÈáèÂåñÂ∑ÆÂºÇ„ÄÇOutlierTune Êòì‰∫éÂÆûÁé∞‰∏îÁ°¨‰ª∂È´òÊïàÔºåÂú®Êé®ÁêÜÊúüÈó¥Âá†‰πéÊ≤°ÊúâÂºïÂÖ•È¢ùÂ§ñÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂Âú®Â§ö‰∏™‰∏çÂêå‰ªªÂä°‰∏≠‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇËØ•Ê°ÜÊû∂Â±ïÁ§∫‰∫ÜÊõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂ∞ÜÊåá‰ª§ÂæÆË∞É LLMÔºà‰æãÂ¶Ç OPT-IMLÔºâÁöÑ Int6 ÈáèÂåñÊèêÂçáÂà∞‰∏éÂçäÁ≤æÂ∫¶ (FP16) Áõ∏ÂêåÁöÑÊ∞¥Âπ≥„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Â∑≤ÁªèËØÅÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂ÊØî FP16 ÂÆûÁé∞Âø´ 1.48 ÂÄçÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜÂ§ßÁ∫¶ 2 ÂÄçÁöÑÂÜÖÂ≠ò‰ΩøÁî®Èáè„ÄÇ

##### **A Survey on Privacy Attacks Against Digital Twin Systems in AI-Robotics**
2406.18812v1 by Ivan A. Fernandez, Subash Neupane, Trisha Chakraborty, Shaswata Mitra, Sudip Mittal, Nisha Pillai, Jingdao Chen, Shahram Rahimi

Industry 4.0 has witnessed the rise of complex robots fueled by the
integration of Artificial Intelligence/Machine Learning (AI/ML) and Digital
Twin (DT) technologies. While these technologies offer numerous benefits, they
also introduce potential privacy and security risks. This paper surveys privacy
attacks targeting robots enabled by AI and DT models. Exfiltration and data
leakage of ML models are discussed in addition to the potential extraction of
models derived from first-principles (e.g., physics-based). We also discuss
design considerations with DT-integrated robotics touching on the impact of ML
model training, responsible AI and DT safeguards, data governance and ethical
considerations on the effectiveness of these attacks. We advocate for a trusted
autonomy approach, emphasizing the need to combine robotics, AI, and DT
technologies with robust ethical frameworks and trustworthiness principles for
secure and reliable AI robotic systems.

ÊëòË¶ÅÔºöÂ∑•Ê•≠ 4.0 Ë¶ãË≠â‰∫ÜË§áÈõúÊ©üÂô®‰∫∫ÁöÑÂ¥õËµ∑ÔºåÂÖ∂Êé®ÂãïÂäõÊòØ‰∫∫Â∑•Êô∫ÊÖß/Ê©üÂô®Â≠∏Áøí (AI/ML) ÂíåÊï∏‰ΩçÂàÜË∫´ (DT) ÊäÄË°ìÁöÑÊï¥Âêà„ÄÇÈõñÁÑ∂ÈÄô‰∫õÊäÄË°ìÊèê‰æõ‰∫ÜË®±Â§öÂ•ΩËôïÔºå‰ΩÜÂÆÉÂÄë‰πüÂºïÂÖ•‰∫ÜÊΩõÂú®ÁöÑÈö±ÁßÅÂíåÂÆâÂÖ®È¢®Èö™„ÄÇÊú¨ÊñáÊé¢Ë®é‰∫ÜÈáùÂ∞çÁî± AI Âíå DT Ê®°ÂûãÂïüÁî®Ê©üÂô®‰∫∫ÁöÑÈö±ÁßÅÊîªÊìä„ÄÇÈô§‰∫ÜË®éË´ñÂæûÁ¨¨‰∏ÄÂéüÁêÜ (‰æãÂ¶ÇÔºåÂü∫ÊñºÁâ©ÁêÜ) Ë°çÁîüÁöÑÊ®°ÂûãÁöÑÊΩõÂú®ÊèêÂèñ‰πãÂ§ñÔºåÈÇÑË®éË´ñ‰∫Ü ML Ê®°ÂûãÁöÑÊª≤ÈÄèÂíåË≥áÊñôÂ§ñÊ¥©„ÄÇÊàëÂÄëÈÇÑË®éË´ñ‰∫ÜËàá DT Êï¥ÂêàÊ©üÂô®‰∫∫Áõ∏ÈóúÁöÑË®≠Ë®àËÄÉÈáèÔºåËß∏Âèä ML Ê®°ÂûãË®ìÁ∑¥„ÄÅË≤†Ë≤¨‰ªªÁöÑ AI Âíå DT ‰øùÈöúÊé™ÊñΩ„ÄÅË≥áÊñôÊ≤ªÁêÜÂíåÈÅìÂæ∑ËÄÉÈáèÂ∞çÈÄô‰∫õÊîªÊìäÁöÑÊúâÊïàÊÄßÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÊèêÂÄ°‰∏ÄÂÄãÂèØ‰ø°Ë≥¥ÁöÑËá™‰∏ªÊñπÊ≥ïÔºåÂº∑Ë™øÈúÄË¶ÅÁµêÂêàÊ©üÂô®‰∫∫„ÄÅAI Âíå DT ÊäÄË°ìÔºå‰∏¶Êé°Áî®Âº∑Â§ßÁöÑÈÅìÂæ∑Êû∂ÊßãÂíåÂèØ‰ø°Ë≥¥ÂéüÂâáÔºå‰ª•Âª∫Á´ãÂÆâÂÖ®ÂíåÂèØÈù†ÁöÑ AI Ê©üÂô®‰∫∫Á≥ªÁµ±„ÄÇ

##### **Infinite Width Models That Work: Why Feature Learning Doesn't Matter as Much as You Think**
2406.18800v1 by Luke Sernau

Common infinite-width architectures such as Neural Tangent Kernels (NTKs)
have historically shown weak performance compared to finite models. This has
been attributed to the absence of feature learning. We show that this is not
the case. In fact, we show that infinite width NTK models are able to access
richer features than finite models by selecting relevant subfeatures from their
(infinite) feature vector. In fact, we show experimentally that NTKs
under-perform traditional finite models even when feature learning is
artificially disabled. Instead, weak performance is due to the fact that
existing constructions depend on weak optimizers like SGD. We provide an
infinite width limit based on ADAM-like learning dynamics and demonstrate
empirically that the resulting models erase this performance gap.

ÊëòË¶ÅÔºöÂ∏∏Ë¶ãÁöÑÁÑ°ÈôêÂØ¨Â∫¶Êû∂ÊßãÔºå‰æãÂ¶ÇÁ•ûÁ∂ìÂàáÁ∑öÊ†∏ (NTK)Ôºå
Âú®Ê≠∑Âè≤‰∏äÈ°ØÁ§∫Âá∫ËàáÊúâÈôêÊ®°ÂûãÁõ∏ÊØîÊïàËÉΩËºÉÂ∑Æ„ÄÇÈÄô
Ê≠∏Âõ†ÊñºÁº∫‰πèÁâπÂæµÂ≠∏Áøí„ÄÇÊàëÂÄëË°®ÊòéÊÉÖÊ≥Å‰∏¶ÈùûÂ¶ÇÊ≠§„ÄÇ‰∫ãÂØ¶‰∏äÔºåÊàëÂÄëË°®ÊòéÁÑ°ÈôêÂØ¨Â∫¶ NTK Ê®°ÂûãËÉΩÂ§†ÈÄèÈÅéÂæûÂÖ∂
ÔºàÁÑ°ÈôêÔºâÁâπÂæµÂêëÈáè‰∏≠ÈÅ∏ÊìáÁõ∏ÈóúÂ≠êÁâπÂæµ‰æÜÂ≠òÂèñÊØîÊúâÈôêÊ®°ÂûãÊõ¥Ë±êÂØåÁöÑÁâπÂæµ„ÄÇ‰∫ãÂØ¶‰∏äÔºåÊàëÂÄëÈÄèÈÅéÂØ¶È©óË°®ÊòéÔºåÂç≥‰Ωø‰∫∫Â∑•ÂÅúÁî®ÁâπÂæµÂ≠∏ÁøíÔºåNTK
ÁöÑÊïàËÉΩ‰ªç‰ΩéÊñºÂÇ≥Áµ±ÊúâÈôêÊ®°Âûã„ÄÇÂèñËÄå‰ª£‰πãÁöÑÊòØÔºåÊïàËÉΩ‰∏ç‰Ω≥ÊòØÂõ†ÁÇ∫
ÁèæÊúâÁöÑÁµêÊßã‰æùË≥¥Êñº SGD Á≠âÂº±ÂÑ™ÂåñÂô®„ÄÇÊàëÂÄëÊèê‰æõ‰∏ÄÂÄãÂü∫ÊñºÈ°û‰ºº ADAM ÁöÑÂ≠∏ÁøíÂãïÊÖãÁöÑÁÑ°ÈôêÂØ¨Â∫¶ÈôêÂà∂Ôºå‰∏¶ÂØ¶Ë≠âË≠âÊòéÁî±Ê≠§Áî¢ÁîüÁöÑÊ®°ÂûãÊ∂àÈô§‰∫ÜÈÄôÂÄãÊïàËÉΩÂ∑ÆË∑ù„ÄÇ

##### **MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data**
2406.18790v1 by William Berman, Alexander Peysakhovich

We train a model to generate images from multimodal prompts of interleaved
text and images such as "a <picture of a man> man and his <picture of a dog>
dog in an <picture of a cartoon> animated style." We bootstrap a multimodal
dataset by extracting semantically meaningful image crops corresponding to
words in the image captions of synthetically generated and publicly available
text-image data. Our model, MUMU, is composed of a vision-language model
encoder with a diffusion decoder and is trained on a single 8xH100 GPU node.
Despite being only trained on crops from the same image, MUMU learns to compose
inputs from different images into a coherent output. For example, an input of a
realistic person and a cartoon will output the same person in the cartoon
style, and an input of a standing subject and a scooter will output the subject
riding the scooter. As a result, our model generalizes to tasks such as style
transfer and character consistency. Our results show the promise of using
multimodal models as general purpose controllers for image generation.

ÊëòË¶ÅÔºöÊàëÂÄëË®ìÁ∑¥‰∏ÄÂÄãÊ®°ÂûãÔºåÂæûÁ©øÊèíÊñáÂ≠óÂíåÂúñÁâáÁöÑÂ§öÊ®°ÊÖãÊèêÁ§∫‰∏≠Áî¢ÁîüÂúñÁâáÔºå‰æãÂ¶Ç„Äå<picture of a man> ‰∏ÄÂÄãÁî∑‰∫∫Âíå‰ªñÁöÑ <picture of a dog> Áãó‰ª• <picture of a cartoon> ÂãïÁï´È¢®Ê†ºÂëàÁèæ„ÄÇ„ÄçÊàëÂÄëÈÄèÈÅéËêÉÂèñË™ûÁæ©‰∏äÊúâÊÑèÁæ©ÁöÑÂúñÁâáË£ÅÂàáÔºåÂ∞çÊáâÂà∞ÂêàÊàêÁî¢ÁîüÂíåÂÖ¨ÈñãÊèê‰æõÁöÑÊñáÂ≠óÂúñÁâáË≥áÊñô‰∏≠ÁöÑÂúñÁâáÊ®ôÈ°å‰∏≠ÁöÑÊñáÂ≠óÔºå‰æÜÂª∫Á´ã‰∏ÄÂÄãÂ§öÊ®°ÊÖãË≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÊ®°Âûã MUMU Áî±‰∏ÄÂÄãÂÖ∑ÊúâÊì¥Êï£Ëß£Á¢ºÂô®ÁöÑË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁ∑®Á¢ºÂô®ÁµÑÊàêÔºå‰∏¶Âú®‰∏ÄÂÄã 8xH100 GPU ÁØÄÈªû‰∏äË®ìÁ∑¥„ÄÇÂÑòÁÆ°Âè™Âú®Âêå‰∏ÄÂºµÂúñÁâáÁöÑË£ÅÂàá‰∏≠Ë®ìÁ∑¥ÔºåMUMU Â≠∏ÊúÉÂ∞á‰æÜËá™‰∏çÂêåÂúñÁâáÁöÑËº∏ÂÖ•ÁµÑÊàê‰∏ÄÂÄãÈÄ£Ë≤´ÁöÑËº∏Âá∫„ÄÇ‰æãÂ¶ÇÔºå‰∏ÄÂÄãÁúüÂØ¶‰∫∫Áâ©Âíå‰∏ÄÂÄãÂç°ÈÄöÁöÑËº∏ÂÖ•ÔºåÂ∞áËº∏Âá∫‰∏ÄÂÄã‰ª•Âç°ÈÄöÈ¢®Ê†ºÂëàÁèæÁöÑ‰∫∫Áâ©ÔºåËÄå‰∏ÄÂÄãÁ´ôÁ´ãÁöÑ‰∏ªÈ´îÂíå‰∏ÄÂÄãÊªëÊùøËªäÁöÑËº∏ÂÖ•ÔºåÂ∞áËº∏Âá∫‰∏ÄÂÄãÈ®éËëóÊªëÊùøËªäÁöÑ‰∏ªÈ´î„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•Âª£Ê≥õÊáâÁî®ÊñºÊ®£ÂºèËΩâÁßªÂíåËßíËâ≤‰∏ÄËá¥ÊÄßÁ≠â‰ªªÂãô„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫Âá∫‰ΩøÁî®Â§öÊ®°ÊÖãÊ®°Âûã‰ΩúÁÇ∫ÂúñÁâáÁî¢ÁîüÁöÑ‰∏ÄËà¨Áî®ÈÄîÊéßÂà∂Âô®ÁöÑÂâçÊôØ„ÄÇ

##### **Psychological Profiling in Cybersecurity: A Look at LLMs and Psycholinguistic Features**
2406.18783v1 by Jean Marie Tshimula, D'Jeff K. Nkashama, Jean Tshibangu Muabila, Ren√© Manass√© Galekwa, Hugues Kanda, Maximilien V. Dialufuma, Mbuyi Mukendi Didier, Kalala Kalonji, Serge Mundele, Patience Kinshie Lenye, Tighana Wenge Basele, Aristarque Ilunga, Christian N. Mayemba, Nathana√´l M. Kasoro, Selain K. Kasereka, Hardy Mikese, Pierre-Martin Tardif, Marc Frappier, Froduald Kabanza, Belkacem Chikhaoui, Shengrui Wang, Ali Mulenda Sumbu, Xavier Ndona, Raoul Kienge-Kienge Intudi

The increasing sophistication of cyber threats necessitates innovative
approaches to cybersecurity. In this paper, we explore the potential of
psychological profiling techniques, particularly focusing on the utilization of
Large Language Models (LLMs) and psycholinguistic features. We investigate the
intersection of psychology and cybersecurity, discussing how LLMs can be
employed to analyze textual data for identifying psychological traits of threat
actors. We explore the incorporation of psycholinguistic features, such as
linguistic patterns and emotional cues, into cybersecurity frameworks. \iffalse
Through case studies and experiments, we discuss the effectiveness of these
methods in enhancing threat detection and mitigation strategies.\fi Our
research underscores the importance of integrating psychological perspectives
into cybersecurity practices to bolster defense mechanisms against evolving
threats.

ÊëòË¶ÅÔºöÁ∂≤Ë∑ØÂ®ÅËÑÖÊó•ÁõäË§áÈõúÔºåÂõ†Ê≠§ÈúÄË¶ÅÂâµÊñ∞ÁöÑÁ∂≤Ë∑ØÂÆâÂÖ®ÊñπÊ≥ï„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÂøÉÁêÜÂÅ¥ÂØ´ÊäÄË°ìÁöÑÊΩõÂäõÔºåÁâπÂà•Â∞àÊ≥®ÊñºÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÂøÉÁêÜË™ûË®ÄÂ≠∏ÁâπÂæµ„ÄÇÊàëÂÄëÊé¢Ë®éÂøÉÁêÜÂ≠∏ÂíåÁ∂≤Ë∑ØÂÆâÂÖ®ÁöÑ‰∫§ÈõÜÔºåË®éË´ñÂ¶Ç‰ΩïÈÅãÁî® LLM ÂàÜÊûêÊñáÂ≠óË≥áÊñôÔºå‰ª•Ëæ®Ë≠òÂ®ÅËÑÖËÄÖÁöÑÂøÉÁêÜÁâπË≥™„ÄÇÊàëÂÄëÊé¢Ë®éÂ∞áÂøÉÁêÜË™ûË®ÄÂ≠∏ÁâπÂæµÔºà‰æãÂ¶ÇË™ûË®ÄÊ®°ÂºèÂíåÊÉÖÁ∑íÁ∑öÁ¥¢ÔºâÁ¥çÂÖ•Á∂≤Ë∑ØÂÆâÂÖ®Êû∂Êßã‰∏≠„ÄÇ\iffalse
ÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÂíåÂØ¶È©óÔºåÊàëÂÄëË®éË´ñÈÄô‰∫õÊñπÊ≥ïÂú®Â¢ûÂº∑Â®ÅËÑÖÂÅµÊ∏¨ÂíåÁ∑©Ëß£Á≠ñÁï•ÊñπÈù¢ÁöÑÊàêÊïà„ÄÇ\fi ÊàëÂÄëÁöÑÁ†îÁ©∂Âº∑Ë™øÂ∞áÂøÉÁêÜËßÄÈªûÊï¥ÂêàÂà∞Á∂≤Ë∑ØÂÆâÂÖ®ÂØ¶Âãô‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºå‰ª•Âä†Âº∑Â∞çÊäó‰∏çÊñ∑ÊºîËÆäÁöÑÂ®ÅËÑÖÁöÑÈò≤Á¶¶Ê©üÂà∂„ÄÇ

##### **Implicit Discourse Relation Classification For Nigerian Pidgin**
2406.18776v1 by Muhammed Saeed, Peter Bourgonje, Vera Demberg

Despite attempts to make Large Language Models multi-lingual, many of the
world's languages are still severely under-resourced. This widens the
performance gap between NLP and AI applications aimed at well-financed, and
those aimed at less-resourced languages. In this paper, we focus on Nigerian
Pidgin (NP), which is spoken by nearly 100 million people, but has
comparatively very few NLP resources and corpora. We address the task of
Implicit Discourse Relation Classification (IDRC) and systematically compare an
approach translating NP data to English and then using a well-resourced IDRC
tool and back-projecting the labels versus creating a synthetic discourse
corpus for NP, in which we translate PDTB and project PDTB labels, and then
train an NP IDR classifier. The latter approach of learning a "native" NP
classifier outperforms our baseline by 13.27\% and 33.98\% in f$_{1}$ score for
4-way and 11-way classification, respectively.

ÊëòË¶ÅÔºöÂÑòÁÆ°ÂòóË©¶ËÆìÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂ§öË™ûË®ÄÂåñÔºå‰ΩÜ‰∏ñÁïå‰∏äË®±Â§öË™ûË®Ä‰ªçÁÑ∂Âö¥ÈáçÁº∫‰πèË≥áÊ∫ê„ÄÇÈÄôÊì¥Â§ß‰∫Ü NLP Âíå AI ÊáâÁî®‰πãÈñìÁöÑÊïàËÉΩÂ∑ÆË∑ùÔºåÈÄô‰∫õÊáâÁî®Á®ãÂºèÈáùÂ∞çË≥áÈáëÂÖÖË∂≥ÁöÑË™ûË®ÄÔºåËÄåÈÇ£‰∫õÈáùÂ∞çË≥áÊ∫êËºÉÂ∞ëÁöÑË™ûË®Ä„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂ∞ºÊó•Âà©‰∫ûÁöÆÊ¨ΩË™û (NP)ÔºåÂÆÉÊúâÂ∞áËøë 1 ÂÑÑ‰∫∫‰ΩøÁî®Ôºå‰ΩÜÊØîËºÉËµ∑‰æÜÔºåNLP Ë≥áÊ∫êÂíåË™ûÊñôÂ∫´ÈùûÂ∏∏Â∞ë„ÄÇÊàëÂÄëËß£Ê±∫Èö±ÂºèË©±Ë™ûÈóú‰øÇÂàÜÈ°û (IDRC) ÁöÑ‰ªªÂãôÔºå‰∏¶Á≥ªÁµ±ÊÄßÂú∞ÊØîËºÉ‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂ∞á NP Ë≥áÊñôÁøªË≠ØÊàêËã±ÊñáÔºåÁÑ∂Âæå‰ΩøÁî®Ë≥áÊ∫êË±êÂØåÁöÑ IDRC Â∑•ÂÖ∑Ôºå‰∏¶Â∞áÊ®ôÁ±§ÂèçÂêëÊäïÂΩ±ÔºåËàáÁÇ∫ NP Âª∫Á´ãÂêàÊàêË©±Ë™ûË™ûÊñôÂ∫´ÔºåÂÖ∂‰∏≠ÊàëÂÄëÁøªË≠Ø PDTB ‰∏¶ÊäïÂΩ± PDTB Ê®ôÁ±§ÔºåÁÑ∂ÂæåË®ìÁ∑¥ NP IDR ÂàÜÈ°ûÂô®„ÄÇÂæåËÄÖÂ≠∏Áøí„ÄåÂéüÁîü„ÄçNP ÂàÜÈ°ûÂô®ÁöÑÂÅöÊ≥ïÔºåÂú® 4 ÂêëÂíå 11 ÂêëÂàÜÈ°ûÁöÑ f$_{1}$ ÂàÜÊï∏‰∏≠ÔºåÂàÜÂà•ÊØîÊàëÂÄëÁöÑÂü∫Ê∫ñÈ´òÂá∫ 13.27% Âíå 33.98%„ÄÇ

##### **WV-Net: A foundation model for SAR WV-mode satellite imagery trained using contrastive self-supervised learning on 10 million images**
2406.18765v1 by Yannik Glaser, Justin E. Stopa, Linnea M. Wolniewicz, Ralph Foster, Doug Vandemark, Alexis Mouche, Bertrand Chapron, Peter Sadowski

The European Space Agency's Copernicus Sentinel-1 (S-1) mission is a
constellation of C-band synthetic aperture radar (SAR) satellites that provide
unprecedented monitoring of the world's oceans. S-1's wave mode (WV) captures
20x20 km image patches at 5 m pixel resolution and is unaffected by cloud cover
or time-of-day. The mission's open data policy has made SAR data easily
accessible for a range of applications, but the need for manual image
annotations is a bottleneck that hinders the use of machine learning methods.
This study uses nearly 10 million WV-mode images and contrastive
self-supervised learning to train a semantic embedding model called WV-Net. In
multiple downstream tasks, WV-Net outperforms a comparable model that was
pre-trained on natural images (ImageNet) with supervised learning. Experiments
show improvements for estimating wave height (0.50 vs 0.60 RMSE using linear
probing), estimating near-surface air temperature (0.90 vs 0.97 RMSE), and
performing multilabel-classification of geophysical and atmospheric phenomena
(0.96 vs 0.95 micro-averaged AUROC). WV-Net embeddings are also superior in an
unsupervised image-retrieval task and scale better in data-sparse settings.
Together, these results demonstrate that WV-Net embeddings can support
geophysical research by providing a convenient foundation model for a variety
of data analysis and exploration tasks.

ÊëòË¶ÅÔºö<paragraph>Ê≠êÊ¥≤Â§™Á©∫Á∏ΩÁΩ≤ÁöÑÂì•ÁôΩÂ∞º Sentinel-1 (S-1) ‰ªªÂãôÊòØ‰∏ÄÁµÑ C Ê≥¢ÊÆµÂêàÊàêÂ≠îÂæëÈõ∑ÈÅî (SAR) Ë°õÊòüÔºåÂèØÊèê‰æõÂâçÊâÄÊú™ÊúâÁöÑÂÖ®ÁêÉÊµ∑Ê¥ãÁõ£Ê∏¨„ÄÇS-1 ÁöÑÊ≥¢Êµ™Ê®°Âºè (WV) ‰ª• 5 ÂÖ¨Â∞∫ÂÉèÁ¥†Ëß£ÊûêÂ∫¶Êì∑Âèñ 20x20 ÂÖ¨ÈáåÂΩ±ÂÉèÂçÄÂ°äÔºå‰∏çÂèóÈõ≤Â±§Ë¶ÜËìãÊàñÊôÇÈñìÂΩ±Èüø„ÄÇÈÄôÈ†Ö‰ªªÂãôÁöÑÈñãÊîæË≥áÊñôÊîøÁ≠ñËÆì SAR Ë≥áÊñôÂÆπÊòìÂèñÂæóÔºåÂèØÊáâÁî®ÊñºÂêÑÁ®ÆÊáâÁî®Á®ãÂºèÔºå‰ΩÜÊâãÂãïÂΩ±ÂÉèË®ªËß£ÁöÑÈúÄÊ±ÇÊòØÈòªÁ§ôÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ï‰ΩøÁî®ÁöÑÁì∂È†∏„ÄÇÊú¨Á†îÁ©∂‰ΩøÁî®Ëøë 1 ÂçÉËê¨Âºµ WV Ê®°ÂºèÂΩ±ÂÉèÂíåÂ∞çÊØîËá™ÊàëÁõ£Áù£Â≠∏ÁøíÔºåË®ìÁ∑¥‰∏ÄÂÄãÂêçÁÇ∫ WV-Net ÁöÑË™ûÊÑèÂµåÂÖ•Ê®°Âûã„ÄÇÂú®Â§öÂÄã‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÔºåWV-Net ÂÑ™Êñº‰ΩøÁî®Áõ£Áù£Â≠∏ÁøíÂú®Ëá™ÁÑ∂ÂΩ±ÂÉè (ImageNet) ‰∏äÈ†êÂÖàË®ìÁ∑¥ÁöÑÈ°û‰ººÊ®°Âûã„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåÂú®‰º∞Ë®àÊ≥¢Êµ™È´òÂ∫¶ (‰ΩøÁî®Á∑öÊÄßÊé¢Ê∏¨ÊôÇÔºå0.50 Â∞ç‰∏ä 0.60 RMSE)„ÄÅ‰º∞Ë®àËøëÂú∞Ë°®Á©∫Ê∞£Ê∫´Â∫¶ (0.90 Â∞ç‰∏ä 0.97 RMSE) ÂíåÂü∑Ë°åÂú∞ÁêÉÁâ©ÁêÜÂíåÊ∞£Ë±°ÁèæË±°ÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°û (0.96 Â∞ç‰∏ä 0.95 ÂæÆÂπ≥Âùá AUROC) ÊñπÈù¢ÈÉΩÊúâÊâÄÈÄ≤Ê≠•„ÄÇWV-Net ÂµåÂÖ•Âú®ÁÑ°Áõ£Áù£ÂΩ±ÂÉèÊ™¢Á¥¢‰ªªÂãô‰∏≠‰πüËºÉÁÇ∫Âá∫Ëâ≤Ôºå‰∏îÂú®Ë≥áÊñôÁ®ÄÁñèÁöÑË®≠ÂÆö‰∏≠ÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÊì¥ÂÖÖÊÄß„ÄÇÁ∏ΩËÄåË®Ä‰πãÔºåÈÄô‰∫õÁµêÊûúË≠âÊòé WV-Net ÂµåÂÖ•ÂèØÈÄèÈÅéÁÇ∫ÂêÑÁ®ÆË≥áÊñôÂàÜÊûêÂíåÊé¢Á¥¢‰ªªÂãôÊèê‰æõ‰æøÂà©Âü∫Á§éÊ®°ÂûãÔºå‰æÜÊîØÊè¥Âú∞ÁêÉÁâ©ÁêÜÁ†îÁ©∂„ÄÇ</paragraph>

##### **Conformalized Link Prediction on Graph Neural Networks**
2406.18763v1 by Tianyi Zhao, Jian Kang, Lu Cheng

Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications
in high-stakes domains are often hampered by unreliable predictions. Although
numerous uncertainty quantification methods have been proposed to address this
limitation, they often lack \textit{rigorous} uncertainty estimates. This work
makes the first attempt to introduce a distribution-free and model-agnostic
uncertainty quantification approach to construct a predictive interval with a
statistical guarantee for GNN-based link prediction. We term it as
\textit{conformalized link prediction.} Our approach builds upon conformal
prediction (CP), a framework that promises to construct statistically robust
prediction sets or intervals. We first theoretically and empirically establish
a permutation invariance condition for the application of CP in link prediction
tasks, along with an exact test-time coverage. Leveraging the important
structural information in graphs, we then identify a novel and crucial
connection between a graph's adherence to the power law distribution and the
efficiency of CP. This insight leads to the development of a simple yet
effective sampling-based method to align the graph structure with a power law
distribution prior to the standard CP procedure. Extensive experiments
demonstrate that for conformalized link prediction, our approach achieves the
desired marginal coverage while significantly improving the efficiency of CP
compared to baseline methods.

ÊëòË¶ÅÔºöÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Âú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂÆÉÂÄëÂú®È´òÈ¢®Èö™È†òÂüüÁöÑÊáâÁî®Â∏∏Â∏∏ÂèóÂà∞‰∏çÂèØÈù†È†êÊ∏¨ÁöÑÈòªÁ§ô„ÄÇÂÑòÁÆ°Â∑≤ÊèêÂá∫Ë®±Â§ö‰∏çÁ¢∫ÂÆöÊÄßÈáèÂåñÊñπÊ≥ï‰æÜËß£Ê±∫Ê≠§ÈôêÂà∂Ôºå‰ΩÜÂÆÉÂÄëÂ∏∏Â∏∏Áº∫‰πè„ÄåÂö¥Ë¨π„ÄçÁöÑ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®à„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈ¶ñÊ¨°ÂòóË©¶ÂºïÂÖ•ÁÑ°ÂàÜ‰Ωà‰∏îËàáÊ®°ÂûãÁÑ°ÈóúÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÈáèÂåñÊñπÊ≥ïÔºå‰ª•ÊßãÂª∫ÂÖ∑ÂÇôÁµ±Ë®à‰øùË≠âÁöÑÈ†êÊ∏¨ÂçÄÈñìÔºåÁî®ÊñºÂü∫Êñº GNN ÁöÑÈÄ£ÁµêÈ†êÊ∏¨„ÄÇÊàëÂÄëÁ®±‰πãÁÇ∫„ÄåÂÖ±ÂΩ¢ÈÄ£ÁµêÈ†êÊ∏¨„Äç„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂª∫Á´ãÂú®ÂÖ±ÂΩ¢È†êÊ∏¨ (CP) ‰πã‰∏äÔºåÈÄôÊòØ‰∏ÄÂÄãÊâøË´æÊßãÂª∫Áµ±Ë®àÁ©©ÂÅ•ÁöÑÈ†êÊ∏¨ÈõÜÊàñÂçÄÈñìÁöÑÊ°ÜÊû∂„ÄÇÊàëÂÄëÈ¶ñÂÖàÂæûÁêÜË´ñ‰∏äÂíåÁ∂ìÈ©ó‰∏äÁÇ∫ CP Âú®ÈÄ£ÁµêÈ†êÊ∏¨‰ªªÂãô‰∏≠ÁöÑÊáâÁî®Âª∫Á´ãÁΩÆÊèõ‰∏çËÆäÊ¢ù‰ª∂Ôºå‰∏¶ÈÄ≤Ë°åÁ≤æÁ¢∫ÁöÑÊ∏¨Ë©¶ÊôÇÈñìË¶ÜËìãÁéá„ÄÇÂà©Áî®ÂúñË°®‰∏≠ÁöÑÈáçË¶ÅÁµêÊßãË≥áË®äÔºåÊàëÂÄëÊé•ËëóÊâæÂá∫ÂúñË°®Â∞çÂÜ™ÂæãÂàÜ‰ΩàÁöÑÈÅµÂæ™Ëàá CP ÊïàÁéá‰πãÈñìÁöÑÂÖ®Êñ∞‰∏îÈóúÈçµÈóúËÅØ„ÄÇÊ≠§Ë¶ãËß£Â∞éËá¥ÈñãÁôºÂá∫‰∏ÄÁ®ÆÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÂü∫ÊñºÊäΩÊ®£ÁöÑÔºåÂú®Ê®ôÊ∫ñ CP Á®ãÂ∫è‰πãÂâçÂ∞áÂúñÂΩ¢ÁµêÊßãËàáÂÜ™ÂæãÂàÜ‰ΩàÂ∞çÈΩäÁöÑÊñπÊ≥ï„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåÂ∞çÊñºÂÖ±ÂΩ¢ÈÄ£ÁµêÈ†êÊ∏¨ÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈÅîÂà∞‰∫ÜÊâÄÈúÄÁöÑÈÇäÈöõË¶ÜËìãÁéáÔºåÂêåÊôÇËàáÂü∫Á∑öÊñπÊ≥ïÁõ∏ÊØîÔºåÈ°ØËëóÊèêÈ´ò‰∫Ü CP ÁöÑÊïàÁéá„ÄÇ

##### **Categorical Syllogisms Revisited: A Review of the Logical Reasoning Abilities of LLMs for Analyzing Categorical Syllogism**
2406.18762v1 by Shi Zong, Jimmy Lin

There have been a huge number of benchmarks proposed to evaluate how large
language models (LLMs) behave for logic inference tasks. However, it remains an
open question how to properly evaluate this ability. In this paper, we provide
a systematic overview of prior works on the logical reasoning ability of LLMs
for analyzing categorical syllogisms. We first investigate all the possible
variations for the categorical syllogisms from a purely logical perspective and
then examine the underlying configurations (i.e., mood and figure) tested by
the existing datasets. Our results indicate that compared to template-based
synthetic datasets, crowdsourcing approaches normally sacrifice the coverage of
configurations (i.e., mood and figure) of categorical syllogisms for more
language variations, thus bringing challenges to fully testing LLMs under
different situations. We then proceed to summarize the findings and
observations for the performances of LLMs to infer the validity of syllogisms
from the current literature. The error rate breakdown analyses suggest that the
interpretation of the quantifiers seems to be the current bottleneck that
limits the performances of the LLMs and is thus worth more attention. Finally,
we discuss several points that might be worth considering when researchers plan
on the future release of categorical syllogism datasets. We hope our work will
not only provide a timely review of the current literature regarding
categorical syllogisms, but also motivate more interdisciplinary research
between communities, specifically computational linguists and logicians.

ÊëòË¶ÅÔºö<paragraph>Â∑≤Á∂ìÊèêÂá∫Â§ßÈáèÂü∫Ê∫ñ‰æÜË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÇèËºØÊé®ÁêÜ‰ªªÂãô‰∏≠ÁöÑË°®Áèæ„ÄÇÁÑ∂ËÄåÔºåÂ¶Ç‰ΩïÊ≠£Á¢∫Ë©ï‰º∞ÈÄôÁ®ÆËÉΩÂäõ‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÈñãÊîæÊÄßÁöÑÂïèÈ°å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂ∞ç LLM ÈÇèËºØÊé®ÁêÜËÉΩÂäõÁöÑÂÖàÂâçÁ†îÁ©∂ÁöÑÁ≥ªÁµ±ÊÄßÊ¶ÇËø∞ÔºåÁî®ÊñºÂàÜÊûêÁØÑÁñá‰∏âÊÆµË´ñ„ÄÇÊàëÂÄëÈ¶ñÂÖàÂæûÁ¥îÁ≤πÈÇèËºØÁöÑËßíÂ∫¶Ë™øÊü•ÁØÑÁñá‰∏âÊÆµË´ñÁöÑÊâÄÊúâÂèØËÉΩËÆäÈ´îÔºåÁÑ∂ÂæåÊ™¢Êü•ÁèæÊúâÊï∏ÊìöÈõÜÊ∏¨Ë©¶ÁöÑÂ∫ïÂ±§ÈÖçÁΩÆÔºàÂç≥ÊÉÖÊÖãÂíåÂúñÂΩ¢Ôºâ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåËàáÂü∫ÊñºÊ®°ÊùøÁöÑÂêàÊàêÊï∏ÊìöÈõÜÁõ∏ÊØîÔºåÁæ§ÁúæÂ§ñÂåÖÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÁäßÁâ≤ÁØÑÁñá‰∏âÊÆµË´ñÁöÑÈÖçÁΩÆÔºàÂç≥ÊÉÖÊÖãÂíåÂúñÂΩ¢ÔºâË¶ÜËìãÁØÑÂúç‰ª•Áç≤ÂæóÊõ¥Â§öË™ûË®ÄËÆäÈ´îÔºåÂæûËÄåÁµ¶Âú®‰∏çÂêåÊÉÖÊ≥Å‰∏ãÂÖÖÂàÜÊ∏¨Ë©¶ LLM Â∏∂‰æÜÊåëÊà∞„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÁπºÁ∫åÁ∏ΩÁµêÂæûÁï∂ÂâçÊñáÁçª‰∏≠Êé®Ë´ñ‰∏âÊÆµË´ñÊúâÊïàÊÄßÁöÑ LLM ÊÄßËÉΩÁöÑÁôºÁèæÂíåËßÄÂØü„ÄÇÈåØË™§ÁéáÂàÜËß£ÂàÜÊûêË°®ÊòéÔºåÈáèË©ûÁöÑËß£Èáã‰ºº‰πéÊòØÁï∂ÂâçÈôêÂà∂ LLM ÊÄßËÉΩÁöÑÁì∂È†∏ÔºåÂõ†Ê≠§ÂÄºÂæóÊõ¥Â§öÈóúÊ≥®„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÁ†îÁ©∂‰∫∫Âì°Âú®Ë¶èÂäÉÁØÑÁñá‰∏âÊÆµË´ñÊï∏ÊìöÈõÜÁöÑÊú™‰æÜÁôºÂ∏ÉÊôÇÂèØËÉΩÂÄºÂæóËÄÉÊÖÆÁöÑÂπæÂÄãË¶ÅÈªû„ÄÇÊàëÂÄëÂ∏åÊúõÊàëÂÄëÁöÑÁ†îÁ©∂‰∏çÂÉÖËÉΩÂèäÊôÇÂõûÈ°ßÊúâÈóúÁØÑÁñá‰∏âÊÆµË´ñÁöÑÁï∂ÂâçÊñáÁçªÔºåÈÇÑËÉΩÊøÄÂãµÁ§æÂçÄ‰πãÈñìÈÄ≤Ë°åÊõ¥Â§öË∑®Â≠∏ÁßëÁ†îÁ©∂ÔºåÁâπÂà•ÊòØË®àÁÆóË™ûË®ÄÂ≠∏ÂÆ∂ÂíåÈÇèËºØÂ≠∏ÂÆ∂„ÄÇ</paragraph>

##### **A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems**
2406.18747v1 by Karn N. Watcharasupat, Alexander Lerch

Despite significant recent progress across multiple subtasks of audio source
separation, few music source separation systems support separation beyond the
four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current
systems that support source separation beyond this setup, most continue to rely
on an inflexible decoder setup that can only support a fixed pre-defined set of
stems. Increasing stem support in these inflexible systems correspondingly
requires increasing computational complexity, rendering extensions of these
systems computationally infeasible for long-tail instruments. In this work, we
propose Banquet, a system that allows source separation of multiple stems using
just one decoder. A bandsplit source separation model is extended to work in a
query-based setup in tandem with a music instrument recognition PaSST model. On
the MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached
the performance level of the significantly more complex 6-stem Hybrid
Transformer Demucs on VDBO stems and outperformed it on guitar and piano. The
query-based setup allows for the separation of narrow instrument classes such
as clean acoustic guitars, and can be successfully applied to the extraction of
less common stems such as reeds and organs. Implementation is available at
https://github.com/kwatcharasupat/query-bandit.

ÊëòË¶ÅÔºöÂÑòÁÆ°Âú®Èü≥Ë®ä‰æÜÊ∫êÂàÜÈõ¢ÁöÑË®±Â§öÂ≠ê‰ªªÂãô‰∏≠ÂèñÂæó‰∫ÜÈáçÂ§ßÁöÑÈÄ≤Â±ïÔºå‰ΩÜÂè™ÊúâÂ∞ëÊï∏Èü≥Ê®Ç‰æÜÊ∫êÂàÜÈõ¢Á≥ªÁµ±ÊîØÊè¥Ë∂ÖÈÅéÂõõÂÄã‰∏ªÂππÁöÑ vocal„ÄÅÈºì„ÄÅË≤ùÊñØÂíåÂÖ∂‰ªñ (VDBO) Ë®≠ÂÆöÁöÑÂàÜÈõ¢„ÄÇÂú®Ê•µÂ∞ëÊï∏ÊîØÊè¥Ë∂ÖÈÅéÊ≠§Ë®≠ÂÆöÁöÑ‰æÜÊ∫êÂàÜÈõ¢Á≥ªÁµ±‰∏≠ÔºåÂ§ßÂ§öÊï∏Á≥ªÁµ±‰ªç‰æùË≥¥Êñº‰∏çÈùàÊ¥ªÁöÑËß£Á¢ºÂô®Ë®≠ÂÆöÔºåÂè™ËÉΩÊîØÊè¥‰∏ÄÁµÑÈ†êÂÖàÂÆöÁæ©ÁöÑ‰∏ªÂππ„ÄÇÂú®ÈÄô‰∫õ‰∏çÈùàÊ¥ªÁöÑÁ≥ªÁµ±‰∏≠Â¢ûÂä†‰∏ªÂππÊîØÊè¥Â∫¶ÔºåÁõ∏Â∞çÊáâÂú∞ÈúÄË¶ÅÂ¢ûÂä†ÈÅãÁÆóË§áÈõúÂ∫¶Ôºå‰ΩøÂæóÈÄô‰∫õÁ≥ªÁµ±ÁöÑÊì¥ÂÖÖÂ∞çÈï∑Â∞æÊ®ÇÂô®ËÄåË®ÄÂú®ÈÅãÁÆó‰∏ä‰∏çÂèØË°å„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ BanquetÔºå‰∏ÄÂÄã‰ΩøÁî®‰∏ÄÂÄãËß£Á¢ºÂô®Â∞±ËÉΩÂàÜÈõ¢Â§öÂÄã‰∏ªÂππÁöÑÁ≥ªÁµ±„ÄÇ‰∏ÄÂÄãÈ†ªÊÆµÂàÜÈõ¢‰æÜÊ∫êÂàÜÈõ¢Ê®°ÂûãË¢´Êì¥ÂÖÖÔºå‰ª•ËàáÈü≥Ê®ÇÊ®ÇÂô®Ëæ®Ë≠ò PaSST Ê®°Âûã‰∏≤ËÅØÂú®‰∏ÄÂÄãÂü∫ÊñºÊü•Ë©¢ÁöÑË®≠ÂÆö‰∏≠ÈÅã‰Ωú„ÄÇÂú® MoisesDB Ë≥áÊñôÈõÜ‰∏äÔºåBanquet Âè™Êúâ 24.9 M ÂÄãÂèØË®ìÁ∑¥ÂèÉÊï∏ÔºåÊé•ËøëË§áÈõúÂ∫¶È´òÂá∫Ë®±Â§ö„ÄÅÂú® VDBO ‰∏ªÂππ‰∏äÊúâ 6 ÂÄã‰∏ªÂππÁöÑ Hybrid Transformer Demucs ÁöÑÊïàËÉΩÁ≠âÁ¥öÔºå‰∏îÂú®Âêâ‰ªñËàáÈãºÁê¥‰∏äË°®ÁèæÂæóÊØîÂÆÉÂ•Ω„ÄÇÂü∫ÊñºÊü•Ë©¢ÁöÑË®≠ÂÆöÂÖÅË®±ÂàÜÈõ¢Âá∫ÁãπÁ™ÑÁöÑÊ®ÇÂô®È°ûÂà•Ôºå‰æãÂ¶Ç‰πæÊ∑®ÁöÑÂéüËÅ≤Âêâ‰ªñÔºå‰∏îËÉΩÊàêÂäüÊáâÁî®ÊñºËºÉ‰∏çÂ∏∏Ë¶ãÁöÑ‰∏ªÂππÔºà‰æãÂ¶ÇÁ∞ßÁâáËàáÁÆ°È¢®Áê¥ÔºâÁöÑËêÉÂèñ„ÄÇÂØ¶‰ΩúÂèØÊñº https://github.com/kwatcharasupat/query-bandit ÂèñÂæó„ÄÇ

##### **Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models**
2406.18740v1 by Baharan Nouriinanloo, Maxime Lamothe

Large Language Models (LLMs) have been revolutionizing a myriad of natural
language processing tasks with their diverse zero-shot capabilities. Indeed,
existing work has shown that LLMs can be used to great effect for many tasks,
such as information retrieval (IR), and passage ranking. However, current
state-of-the-art results heavily lean on the capabilities of the LLM being
used. Currently, proprietary, and very large LLMs such as GPT-4 are the highest
performing passage re-rankers. Hence, users without the resources to leverage
top of the line LLMs, or ones that are closed source, are at a disadvantage. In
this paper, we investigate the use of a pre-filtering step before passage
re-ranking in IR. Our experiments show that by using a small number of human
generated relevance scores, coupled with LLM relevance scoring, it is
effectively possible to filter out irrelevant passages before re-ranking. Our
experiments also show that this pre-filtering then allows the LLM to perform
significantly better at the re-ranking task. Indeed, our results show that
smaller models such as Mixtral can become competitive with much larger
proprietary models (e.g., ChatGPT and GPT-4).

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÊÜëËóâÂÖ∂Â§öÊ®£ÂåñÁöÑÈõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÂæπÂ∫ïÊîπËÆä‰∫ÜÁÑ°Êï∏Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô„ÄÇ‰∫ãÂØ¶‰∏äÔºåÁèæÊúâÁ†îÁ©∂Â∑≤È°ØÁ§∫ÔºåLLM ÂèØÁî®ÊñºË®±Â§ö‰ªªÂãôÔºå‰æãÂ¶ÇË≥áË®äÊ™¢Á¥¢ (IR) ÂíåÊÆµËêΩÊéíÂ∫èÔºå‰∏¶ËÉΩÁôºÊèÆÊ•µ‰Ω≥ÁöÑÊïàÊûú„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÊúÄÊñ∞ÊäÄË°ìÊàêÊûúÊ•µÂ∫¶‰ª∞Ë≥¥ÊâÄ‰ΩøÁî®ÁöÑ LLM ÁöÑËÉΩÂäõ„ÄÇÁõÆÂâçÔºåÂ∞ÅÈñâÂéüÂßãÁ¢º‰∏îË¶èÊ®°Ê•µÂ§ßÁöÑ LLMÔºå‰æãÂ¶Ç GPT-4ÔºåÊòØÊïàËÉΩÊúÄÈ´òÁöÑÊÆµËêΩÈáçÊñ∞ÊéíÂ∫èÂô®„ÄÇÂõ†Ê≠§ÔºåÊ≤íÊúâË≥áÊ∫ê‰ΩøÁî®È†ÇÁ¥ö LLM ÊàñÂ∞ÅÈñâÂéüÂßãÁ¢º LLM ÁöÑ‰ΩøÁî®ËÄÖËôïÊñºÂä£Âã¢„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÂú® IR ‰∏≠‰ΩøÁî®ÊÆµËêΩÈáçÊñ∞ÊéíÂ∫èÂâçÁöÑÈ†êÂÖàÁØ©ÈÅ∏Ê≠•È©ü„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÈÄèÈÅé‰ΩøÁî®Â∞ëÈáè‰∫∫Â∑•Áî¢ÁîüÁöÑÁõ∏ÈóúÊÄßË©ïÂàÜÔºå‰∏¶ÁµêÂêà LLM Áõ∏ÈóúÊÄßË©ïÂàÜÔºåÂú®ÈáçÊñ∞ÊéíÂ∫èÂâçÊúâÊïàÈÅéÊøæÊéâ‰∏çÁõ∏ÈóúÁöÑÊÆµËêΩ„ÄÇÊàëÂÄëÁöÑÂØ¶È©ó‰πüÈ°ØÁ§∫ÔºåÊ≠§È†êÂÖàÁØ©ÈÅ∏ËÆì LLM Âú®ÈáçÊñ∞ÊéíÂ∫è‰ªªÂãô‰∏≠Ë°®ÁèæÂ§ßÂπÖÊèêÂçá„ÄÇ‰∫ãÂØ¶‰∏äÔºåÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåËºÉÂ∞èÁöÑÊ®°ÂûãÔºå‰æãÂ¶Ç MixtralÔºåÂèØËàáË¶èÊ®°Êõ¥Â§ßÁöÑÂ∞ÅÈñâÂéüÂßãÁ¢ºÊ®°ÂûãÔºà‰æãÂ¶Ç ChatGPT Âíå GPT-4ÔºâÁ´∂Áà≠„ÄÇ

##### **WavRx: a Disease-Agnostic, Generalizable, and Privacy-Preserving Speech Health Diagnostic Model**
2406.18731v1 by Yi Zhu, Tiago Falk

Speech is known to carry health-related attributes, which has emerged as a
novel venue for remote and long-term health monitoring. However, existing
models are usually tailored for a specific type of disease, and have been shown
to lack generalizability across datasets. Furthermore, concerns have been
raised recently towards the leakage of speaker identity from health embeddings.
To mitigate these limitations, we propose WavRx, a speech health diagnostics
model that captures the respiration and articulation related dynamics from a
universal speech representation. Our in-domain and cross-domain experiments on
six pathological speech datasets demonstrate WavRx as a new state-of-the-art
health diagnostic model. Furthermore, we show that the amount of speaker
identity entailed in the WavRx health embeddings is significantly reduced
without extra guidance during training. An in-depth analysis of the model was
performed, thus providing physiological interpretation of its improved
generalizability and privacy-preserving ability.

ÊëòË¶ÅÔºöË™ûÈü≥Â∑≤Áü•ÊúÉÊâøËºâËàáÂÅ•Â∫∑Áõ∏ÈóúÁöÑÂ±¨ÊÄßÔºåÈÄôÂ∑≤ÊàêÁÇ∫ÈÅ†Ë∑ùÂíåÈï∑ÊúüÂÅ•Â∫∑Áõ£ÊéßÁöÑÊñ∞ÈÄîÂæë„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÊ®°ÂûãÈÄöÂ∏∏ÈáùÂ∞çÁâπÂÆöÈ°ûÂûãÁöÑÁñæÁóÖÈáèË∫´ÊâìÈÄ†Ôºå‰∏îÂ∑≤È°ØÁ§∫Âá∫Áº∫‰πèË∑®Ë≥áÊñôÈõÜÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇÊ≠§Â§ñÔºåÊúÄËøëÂ∑≤Â∞çÂæûÂÅ•Â∫∑ÂµåÂÖ•‰∏≠Ê¥©ÊºèË™™Ë©±ËÄÖË∫´ÂàÜÊèêÂá∫ÁñëÊÖÆ„ÄÇÁÇ∫‰∫ÜÊ∏õËºïÈÄô‰∫õÈôêÂà∂ÔºåÊàëÂÄëÊèêÂá∫ WavRxÔºåÈÄôÊòØ‰∏ÄÁ®ÆË™ûÈü≥ÂÅ•Â∫∑Ë®∫Êñ∑Ê®°ÂûãÔºåÂÆÉÊúÉÊì∑Âèñ‰æÜËá™ÈÄöÁî®Ë™ûÈü≥Ë°®Á§∫ÁöÑÂëºÂê∏ÂíåÁôºÈü≥Áõ∏ÈóúÂãïÊÖã„ÄÇÊàëÂÄëÂú®ÂÖ≠ÂÄãÁóÖÁêÜÊÄßË™ûÈü≥Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÈ†òÂüüÂÖßÂíåË∑®È†òÂüüÂØ¶È©óÔºåË≠âÊòé WavRx ÊòØ‰∏ÄÁ®ÆÊñ∞ÁöÑÊúÄÂÖàÈÄ≤ÂÅ•Â∫∑Ë®∫Êñ∑Ê®°Âûã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈ°ØÁ§∫ÔºåÂú®Ë®ìÁ∑¥ÊúüÈñìÊ≤íÊúâÈ°çÂ§ñÁöÑÊåáÂ∞é‰∏ãÔºåWavRx ÂÅ•Â∫∑ÂµåÂÖ•‰∏≠ÂåÖÂê´ÁöÑË™™Ë©±ËÄÖË∫´ÂàÜÊï∏ÈáèÂ∑≤È°ØËëóÊ∏õÂ∞ë„ÄÇÂ∞çÊ®°ÂûãÈÄ≤Ë°å‰∫ÜÊ∑±ÂÖ•ÂàÜÊûêÔºåÂæûËÄåÊèê‰æõ‰∫ÜÂÖ∂ÊîπÂñÑÁöÑÊ¶ÇÊã¨ÊÄßÂíåÈö±ÁßÅ‰øùË≠∑ËÉΩÂäõÁöÑÁîüÁêÜËß£Èáã„ÄÇ

##### **Jailbreaking LLMs with Arabic Transliteration and Arabizi**
2406.18725v1 by Mansour Al Ghanim, Saleh Almohaimeed, Mengxin Zheng, Yan Solihin, Qian Lou

This study identifies the potential vulnerabilities of Large Language Models
(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and
its various forms. While most research has concentrated on English-based prompt
manipulation, our investigation broadens the scope to investigate the Arabic
language. We initially tested the AdvBench benchmark in Standardized Arabic,
finding that even with prompt manipulation techniques like prefix injection, it
was insufficient to provoke LLMs into generating unsafe content. However, when
using Arabic transliteration and chatspeak (or arabizi), we found that unsafe
content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3
Sonnet. Our findings suggest that using Arabic and its various forms could
expose information that might remain hidden, potentially increasing the risk of
jailbreak attacks. We hypothesize that this exposure could be due to the
model's learned connection to specific words, highlighting the need for more
comprehensive safety training across all language forms.

ÊëòË¶ÅÔºöÈÄôÈ†ÖÁ†îÁ©∂ÊâæÂá∫Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞ç„ÄåË∂äÁçÑ„ÄçÊîªÊìäÁöÑÊΩõÂú®ÊºèÊ¥ûÔºåÁâπÂà•ËëóÈáçÊñºÈòøÊãâ‰ºØË™ûÂèäÂÖ∂ÂêÑÁ®ÆÂΩ¢Âºè„ÄÇÈõñÁÑ∂Â§ßÂ§öÊï∏Á†îÁ©∂ÈÉΩÈõÜ‰∏≠Âú®‰ª•Ëã±ÊñáÁÇ∫Âü∫Á§éÁöÑÊèêÁ§∫Êìç‰ΩúÔºå‰ΩÜÊàëÂÄëÁöÑË™øÊü•Êì¥Â§ß‰∫ÜÁØÑÂúç‰ª•Á†îÁ©∂ÈòøÊãâ‰ºØË™û„ÄÇÊàëÂÄëÊúÄÂàùÂú®Ê®ôÊ∫ñÈòøÊãâ‰ºØË™û‰∏≠Ê∏¨Ë©¶‰∫Ü AdvBench Âü∫Ê∫ñÔºåÁôºÁèæÂç≥‰Ωø‰ΩøÁî®ÊèêÁ§∫Êìç‰ΩúÊäÄË°ìÔºà‰æãÂ¶ÇÂâçÁ∂¥Ê≥®ÂÖ•ÔºâÔºå‰πü‰∏çË∂≥‰ª•ÊøÄÁôº LLM Áî¢Áîü‰∏çÂÆâÂÖ®ÁöÑÂÖßÂÆπ„ÄÇÁÑ∂ËÄåÔºåÁï∂‰ΩøÁî®ÈòøÊãâ‰ºØË™ûÈü≥Ë≠ØÂíåÁ∂≤Ë∑ØË™ûË®ÄÔºàÊàñÈòøÊãâ‰ºØË™ûÔºâÊôÇÔºåÊàëÂÄëÁôºÁèæÂèØ‰ª•Âú® OpenAI GPT-4 Âíå Anthropic Claude 3 Sonnet Á≠âÂπ≥Âè∞‰∏äÁî¢Áîü‰∏çÂÆâÂÖ®ÁöÑÂÖßÂÆπ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºå‰ΩøÁî®ÈòøÊãâ‰ºØË™ûÂèäÂÖ∂ÂêÑÁ®ÆÂΩ¢ÂºèÂèØËÉΩÊúÉÊè≠Èú≤ÂèØËÉΩ‰øùÊåÅÈö±ËóèÁöÑË≥áË®äÔºåÈÄ≤ËÄåÂèØËÉΩÂ¢ûÂä†Ë∂äÁçÑÊîªÊìäÁöÑÈ¢®Èö™„ÄÇÊàëÂÄëÂÅáË®≠ÈÄôÁ®ÆÊè≠Èú≤ÂèØËÉΩÊòØÁî±ÊñºÊ®°ÂûãÂ≠∏ÁøíËàáÁâπÂÆöÂ≠óË©ûÁöÑÈóúËÅØÔºåÈÄôÂá∏È°Ø‰∫ÜÂú®ÊâÄÊúâË™ûË®ÄÂΩ¢Âºè‰∏≠ÈÄ≤Ë°åÊõ¥ÂÖ®Èù¢ÁöÑÂÆâÂÖ®Ë®ìÁ∑¥ÁöÑÂøÖË¶ÅÊÄß„ÄÇ

##### **Learn it or Leave it: Module Composition and Pruning for Continual Learning**
2406.18708v1 by Mingyang Wang, Heike Adel, Lukas Lange, Jannik Str√∂tgen, Hinrich Sch√ºtze

In real-world environments, continual learning is essential for machine
learning models, as they need to acquire new knowledge incrementally without
forgetting what they have already learned. While pretrained language models
have shown impressive capabilities on various static tasks, applying them to
continual learning poses significant challenges, including avoiding
catastrophic forgetting, facilitating knowledge transfer, and maintaining
parameter efficiency. In this paper, we introduce MoCL-P, a novel lightweight
continual learning method that addresses these challenges simultaneously.
Unlike traditional approaches that continuously expand parameters for newly
arriving tasks, MoCL-P integrates task representation-guided module composition
with adaptive pruning, effectively balancing knowledge integration and
computational overhead. Our evaluation across three continual learning
benchmarks with up to 176 tasks shows that MoCL-P achieves state-of-the-art
performance and improves parameter efficiency by up to three times,
demonstrating its potential for practical applications where resource
requirements are constrained.

ÊëòË¶ÅÔºöÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑÁí∞Â¢É‰∏≠ÔºåÊåÅÁ∫åÂ≠∏ÁøíÂ∞çÊñºÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜË™™Ëá≥ÈóúÈáçË¶ÅÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÈúÄË¶ÅÈÄêÊ≠•Áç≤ÂèñÊñ∞Áü•Ë≠òÔºåÂêåÊôÇ‰∏çÊúÉÂøòË®òÂ∑≤Á∂ìÂ≠∏Âà∞ÁöÑÊù±Ë•ø„ÄÇÂÑòÁÆ°È†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÂú®ÂêÑÁ®ÆÈùúÊÖã‰ªªÂãô‰∏äË°®ÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºå‰ΩÜÂ∞áÂÆÉÂÄëÊáâÁî®ÊñºÊåÅÁ∫åÂ≠∏ÁøíÊúÉÂ∏∂‰æÜÈáçÂ§ßÊåëÊà∞ÔºåÂåÖÊã¨ÈÅøÂÖçÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÅ‰øÉÈÄ≤Áü•Ë≠òËΩâÁßªÂíåÁ∂≠ÊåÅÂèÉÊï∏ÊïàÁéá„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü MoCL-PÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑËºïÈáèÁ¥öÊåÅÁ∫åÂ≠∏ÁøíÊñπÊ≥ïÔºåÂèØ‰ª•ÂêåÊôÇËß£Ê±∫ÈÄô‰∫õÊåëÊà∞„ÄÇËàáÁÇ∫Êñ∞‰ªªÂãôÊåÅÁ∫åÊì¥Â±ïÂèÉÊï∏ÁöÑÂÇ≥Áµ±ÊñπÊ≥ï‰∏çÂêåÔºåMoCL-P Â∞á‰ªªÂãôË°®Á§∫ÂºïÂ∞éÁöÑÊ®°ÁµÑÁµÑÊàêËàáËá™ÈÅ©ÊáâÂâ™ÊûùÁõ∏Êï¥ÂêàÔºåÊúâÊïàÂπ≥Ë°°Áü•Ë≠òÊï¥ÂêàÂíåÈÅãÁÆóË≤†Êìî„ÄÇÊàëÂÄëÂú®ÂåÖÂê´Â§öÈÅî 176 ÂÄã‰ªªÂãôÁöÑ‰∏âÂÄãÊåÅÁ∫åÂ≠∏ÁøíÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÈÄ≤Ë°åÁöÑË©ï‰º∞Ë°®ÊòéÔºåMoCL-P ÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºå‰∏¶Â∞áÂèÉÊï∏ÊïàÁéáÊèêÈ´ò‰∫Ü‰∏âÂÄçÔºåË≠âÊòé‰∫ÜÂÖ∂Âú®Ë≥áÊ∫êÈúÄÊ±ÇÂèóÈôêÁöÑÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÊΩõÂäõ„ÄÇ

##### **Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling Legislative Behavior and Bipartisanship**
2406.18702v1 by Zachary R. Baker, Zarif L. Azher

This study introduces a novel approach to simulating legislative processes
using LLM-driven virtual agents, focusing on the U.S. Senate Intelligence
Committee. We developed agents representing individual senators and placed them
in simulated committee discussions. The agents demonstrated the ability to
engage in realistic debate, provide thoughtful reflections, and find bipartisan
solutions under certain conditions. Notably, the simulation also showed promise
in modeling shifts towards bipartisanship in response to external
perturbations. Our results indicate that this LLM-driven approach could become
a valuable tool for understanding and potentially improving legislative
processes, supporting a broader pattern of findings highlighting how LLM-based
agents can usefully model real-world phenomena. Future works will focus on
enhancing agent complexity, expanding the simulation scope, and exploring
applications in policy testing and negotiation.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂‰ªãÁ¥π‰∏ÄÁ®Æ‰ΩøÁî® LLM È©ÖÂãïËôõÊì¨‰ª£ÁêÜÊ®°Êì¨Á´ãÊ≥ïÁ®ãÂ∫èÁöÑÊñ∞ÊñπÊ≥ïÔºåÈáçÈªûÂú®ÁæéÂúãÂèÉË≠∞Èô¢ÊÉÖÂ†±ÂßîÂì°ÊúÉ„ÄÇÊàëÂÄëÈñãÁôº‰∫Ü‰ª£Ë°®ÂÄãÂà•ÂèÉË≠∞Âì°ÁöÑ‰ª£ÁêÜÔºå‰∏¶Â∞á‰ªñÂÄëÁΩÆÊñºÊ®°Êì¨ÂßîÂì°ÊúÉË®éË´ñ‰∏≠„ÄÇÈÄô‰∫õ‰ª£ÁêÜÂ±ïÁ§∫‰∫ÜÂú®ÁâπÂÆöÊ¢ù‰ª∂‰∏ãÂèÉËàáÁèæÂØ¶ËæØË´ñ„ÄÅÊèê‰æõÊ∑±ÊÄùÁÜüÊÖÆÁöÑÂèçÊÄùÂíåÊâæÂà∞ÂÖ©Èª®Ëß£Ê±∫ÊñπÊ°àÁöÑËÉΩÂäõ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊ®°Êì¨‰πüÈ°ØÁ§∫Âá∫ÊúâÊúõÂú®ÊáâÂ∞çÂ§ñÈÉ®ÊìæÂãïÊôÇÊúùËëóÂÖ©Èª®Âêà‰ΩúÁöÑÊñπÂêëËΩâËÆä„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÈÄôÁ®Æ LLM È©ÖÂãïÁöÑÊñπÊ≥ïÂèØËÉΩÊàêÁÇ∫ÁêÜËß£ÂíåÊΩõÂú®ÊîπÂñÑÁ´ãÊ≥ïÁ®ãÂ∫èÁöÑÂØ∂Ë≤¥Â∑•ÂÖ∑ÔºåÊîØÊåÅÊõ¥Âª£Ê≥õÁöÑÁôºÁèæÊ®°ÂºèÔºåÂº∑Ë™øÂü∫Êñº LLM ÁöÑ‰ª£ÁêÜÂ¶Ç‰ΩïËÉΩÂ§†Â∞çÁèæÂØ¶‰∏ñÁïåÁèæË±°ÈÄ≤Ë°åÂª∫Ê®°„ÄÇÊú™‰æÜÁöÑÁ†îÁ©∂ÈáçÈªûÂ∞áÊîæÂú®ÊèêÈ´ò‰ª£ÁêÜÁöÑË§áÈõúÊÄß„ÄÅÊì¥Â§ßÊ®°Êì¨ÁØÑÂúç‰ª•ÂèäÊé¢Á¥¢Âú®ÊîøÁ≠ñÊ∏¨Ë©¶ÂíåÂçîÂïÜ‰∏≠ÁöÑÊáâÁî®„ÄÇ

##### **Fast Optimizer Benchmark**
2406.18701v1 by Simon Blauth, Tobias B√ºrger, Zacharias H√§ringer, J√∂rg Franke, Frank Hutter

In this paper, we present the Fast Optimizer Benchmark (FOB), a tool designed
for evaluating deep learning optimizers during their development. The benchmark
supports tasks from multiple domains such as computer vision, natural language
processing, and graph learning. The focus is on convenient usage, featuring
human-readable YAML configurations, SLURM integration, and plotting utilities.
FOB can be used together with existing hyperparameter optimization (HPO) tools
as it handles training and resuming of runs. The modular design enables
integration into custom pipelines, using it simply as a collection of tasks. We
showcase an optimizer comparison as a usage example of our tool. FOB can be
found on GitHub: https://github.com/automl/FOB.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂø´ÈÄüÂÑ™ÂåñÂô®Âü∫Ê∫ñ (FOB)ÔºåÈÄôÊòØ‰∏ÄÂÄãÁî®ÊñºÂú®ÈñãÁôºÈÅéÁ®ã‰∏≠Ë©ï‰º∞Ê∑±Â∫¶Â≠∏ÁøíÂÑ™ÂåñÂô®ÁöÑÂ∑•ÂÖ∑„ÄÇÂü∫Ê∫ñÊîØÊåÅ‰æÜËá™Â§öÂÄãÈ†òÂüüÁöÑ‰ªªÂãôÔºå‰æãÂ¶ÇÈõªËÖ¶Ë¶ñË¶∫„ÄÅËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÂíåÂúñÂΩ¢Â≠∏Áøí„ÄÇÈáçÈªûÂú®ÊñºÊñπ‰æø‰ΩøÁî®ÔºåÂÖ∑Êúâ‰∫∫È°ûÂèØËÆÄÁöÑ YAML ÈÖçÁΩÆ„ÄÅSLURM Êï¥ÂêàÂíåÁπ™ÂúñÁ®ãÂºè„ÄÇFOB ÂèØ‰ª•ËàáÁèæÊúâÁöÑË∂ÖÂèÉÊï∏ÂÑ™Âåñ (HPO) Â∑•ÂÖ∑‰∏ÄËµ∑‰ΩøÁî®ÔºåÂõ†ÁÇ∫ÂÆÉÂèØ‰ª•ËôïÁêÜË®ìÁ∑¥ÂíåÊÅ¢Âæ©ÈÅãË°å„ÄÇÊ®°ÁµÑÂåñË®≠Ë®àËÉΩÂ§†Êï¥ÂêàÂà∞Ëá™Ë®ÇÁÆ°Á∑ö‰∏≠ÔºåÂè™ÈúÄÂ∞áÂÖ∂Áî®‰Ωú‰ªªÂãôÈõÜÂêàÂç≥ÂèØ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∏ÄÂÄãÂÑ™ÂåñÂô®ÊØîËºÉ‰ΩúÁÇ∫ÊàëÂÄëÂ∑•ÂÖ∑ÁöÑ‰ΩøÁî®ÁØÑ‰æã„ÄÇFOB ÂèØ‰ª•Âæû GitHub ÊâæÂà∞Ôºöhttps://github.com/automl/FOB„ÄÇ

##### **Sequence Graph Network for Online Debate Analysis**
2406.18696v1 by Quan Mai, Susan Gauch, Douglas Adams, Miaoqing Huang

Online debates involve a dynamic exchange of ideas over time, where
participants need to actively consider their opponents' arguments, respond with
counterarguments, reinforce their own points, and introduce more compelling
arguments as the discussion unfolds. Modeling such a complex process is not a
simple task, as it necessitates the incorporation of both sequential
characteristics and the capability to capture interactions effectively. To
address this challenge, we employ a sequence-graph approach. Building the
conversation as a graph allows us to effectively model interactions between
participants through directed edges. Simultaneously, the propagation of
information along these edges in a sequential manner enables us to capture a
more comprehensive representation of context. We also introduce a Sequence
Graph Attention layer to illustrate the proposed information update scheme. The
experimental results show that sequence graph networks achieve superior results
to existing methods in online debates.

ÊëòË¶ÅÔºöÁ∂≤Ë∑ØËæØË´ñÊ∂âÂèäÈö®ËëóÊôÇÈñìÊé®ÁßªÈÄ≤Ë°åÁöÑÂãïÊÖãËßÄÂøµ‰∫§ÊµÅÔºåÂÖ∂‰∏≠ÂèÉËàáËÄÖÈúÄË¶ÅÁ©çÊ•µËÄÉÊÖÆÂ∞çÊâãÁöÑË´ñÈªûÔºå‰ª•ÂèçË´ñÂõûÊáâ„ÄÅÂº∑ÂåñËá™Â∑±ÁöÑËßÄÈªûÔºå‰∏¶Èö®ËëóË®éË´ñÁöÑÂ±ïÈñãÊèêÂá∫Êõ¥ÊúâË™™ÊúçÂäõÁöÑË´ñÈªû„ÄÇÂª∫Ê®°Â¶ÇÊ≠§Ë§áÈõúÁöÑÈÅéÁ®ã‰∏¶ÈùûÁ∞°ÂñÆÁöÑ‰ªªÂãôÔºåÂõ†ÁÇ∫ÂÆÉÈúÄË¶ÅÂêåÊôÇÁ¥çÂÖ•È†ÜÂ∫èÁâπÊÄßÂíåÊúâÊïàÊçïÊçâ‰∫íÂãïÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊàëÂÄëÊé°Áî®Â∫èÂàóÂúñÊñπÊ≥ï„ÄÇÂ∞áÂ∞çË©±Âª∫ÊßãÁÇ∫ÂúñË°®ËÆìÊàëÂÄëËÉΩÂ§†ÈÄèÈÅéÊúâÂêëÈÇäÊúâÊïàÂú∞Ê®°Êì¨ÂèÉËàáËÄÖ‰πãÈñìÁöÑ‰∫íÂãï„ÄÇÂêåÊôÇÔºåÊ≤øËëóÈÄô‰∫õÈÇä‰ª•È†ÜÂ∫èÊñπÂºèÂÇ≥Êí≠Ë≥áË®ä‰ΩøÊàëÂÄëËÉΩÂ§†ÊçïÊçâÊõ¥ÂÖ®Èù¢ÁöÑËÑàÁµ°Ë°®Âæµ„ÄÇÊàëÂÄëÈÇÑÂºïÂÖ•Â∫èÂàóÂúñÊ≥®ÊÑèÂäõÂ±§‰æÜË™™ÊòéÊâÄÊèêÂá∫ÁöÑË≥áË®äÊõ¥Êñ∞ÊñπÊ°à„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÂ∫èÂàóÂúñÁ∂≤Ë∑ØÂú®Á∂≤Ë∑ØËæØË´ñ‰∏≠Áç≤ÂæóÂÑ™ÊñºÁèæÊúâÊñπÊ≥ïÁöÑÂçìË∂äÊàêÊûú„ÄÇ

