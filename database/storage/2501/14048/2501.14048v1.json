{"2501.14048": {"publish_time": "2025-01-23", "title": "SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks", "paper_summary": "Modern neural networks (NNs) often do not generalize well in the presence of\na \"covariate shift\"; that is, in situations where the training and test data\ndistributions differ, but the conditional distribution of classification labels\nremains unchanged. In such cases, NN generalization can be reduced to a problem\nof learning more domain-invariant features. Domain adaptation (DA) methods\ninclude a range of techniques aimed at achieving this; however, these methods\nhave struggled with the need for extensive hyperparameter tuning, which then\nincurs significant computational costs. In this work, we introduce SIDDA, an\nout-of-the-box DA training algorithm built upon the Sinkhorn divergence, that\ncan achieve effective domain alignment with minimal hyperparameter tuning and\ncomputational overhead. We demonstrate the efficacy of our method on multiple\nsimulated and real datasets of varying complexity, including simple shapes,\nhandwritten digits, and real astronomical observations. SIDDA is compatible\nwith a variety of NN architectures, and it works particularly well in improving\nclassification accuracy and model calibration when paired with equivariant\nneural networks (ENNs). We find that SIDDA enhances the generalization\ncapabilities of NNs, achieving up to a $\\approx40\\%$ improvement in\nclassification accuracy on unlabeled target data. We also study the efficacy of\nDA on ENNs with respect to the varying group orders of the dihedral group\n$D_N$, and find that the model performance improves as the degree of\nequivariance increases. Finally, we find that SIDDA enhances model calibration\non both source and target data--achieving over an order of magnitude\nimprovement in the ECE and Brier score. SIDDA's versatility, combined with its\nautomated approach to domain alignment, has the potential to advance\nmulti-dataset studies by enabling the development of highly generalizable\nmodels.", "paper_summary_zh": "<paragraph>\u73fe\u4ee3\u795e\u7d93\u7db2\u8def (NN) \u5728\u51fa\u73fe\u300c\u5354\u8b8a\u4f4d\u79fb\u300d\u6642\u901a\u5e38\u7121\u6cd5\u5f88\u597d\u5730\u6982\u5316\uff1b\u4e5f\u5c31\u662f\u8aaa\uff0c\u5728\u8a13\u7df4\u548c\u6e2c\u8a66\u8cc7\u6599\u5206\u4f48\u4e0d\u540c\uff0c\u4f46\u5206\u985e\u6a19\u7c64\u7684\u689d\u4ef6\u5206\u4f48\u4fdd\u6301\u4e0d\u8b8a\u7684\u60c5\u6cc1\u4e0b\u3002\u5728\u9019\u7a2e\u60c5\u6cc1\u4e0b\uff0cNN \u6982\u5316\u53ef\u4ee5\u7c21\u5316\u70ba\u5b78\u7fd2\u66f4\u591a\u9818\u57df\u4e0d\u8b8a\u7279\u5fb5\u7684\u554f\u984c\u3002\u9818\u57df\u9069\u61c9 (DA) \u65b9\u6cd5\u5305\u62ec\u4e00\u7cfb\u5217\u65e8\u5728\u5be6\u73fe\u6b64\u76ee\u7684\u7684\u6280\u8853\uff1b\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u4e00\u76f4\u96e3\u4ee5\u6eff\u8db3\u5ee3\u6cdb\u7684\u8d85\u53c3\u6578\u8abf\u6574\u9700\u6c42\uff0c\u9019\u6703\u7522\u751f\u5927\u91cf\u7684\u904b\u7b97\u6210\u672c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 SIDDA\uff0c\u4e00\u7a2e\u5efa\u7acb\u5728\u8f9b\u970d\u6069\u6563\u5ea6\u4e0a\u7684\u958b\u7bb1\u5373\u7528 DA \u8a13\u7df4\u6f14\u7b97\u6cd5\uff0c\u5b83\u53ef\u4ee5\u5728\u6700\u5c0f\u7684\u8d85\u53c3\u6578\u8abf\u6574\u548c\u904b\u7b97\u958b\u92b7\u4e0b\u5be6\u73fe\u6709\u6548\u7684\u9818\u57df\u5c0d\u9f4a\u3002\u6211\u5011\u5728\u591a\u500b\u4e0d\u540c\u8907\u96dc\u7a0b\u5ea6\u7684\u6a21\u64ec\u548c\u771f\u5be6\u8cc7\u6599\u96c6\u4e0a\u5c55\u793a\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u529f\u6548\uff0c\u5305\u62ec\u7c21\u55ae\u5f62\u72c0\u3001\u624b\u5beb\u6578\u5b57\u548c\u771f\u5be6\u7684\u5929\u6587\u89c0\u6e2c\u3002SIDDA \u8207\u5404\u7a2e NN \u67b6\u69cb\u76f8\u5bb9\uff0c\u4e26\u4e14\u5728\u8207\u7b49\u8b8a\u795e\u7d93\u7db2\u8def (ENN) \u914d\u5c0d\u6642\uff0c\u7279\u5225\u80fd\u6539\u5584\u5206\u985e\u6e96\u78ba\u5ea6\u548c\u6a21\u578b\u6821\u6e96\u3002\u6211\u5011\u767c\u73fe SIDDA \u589e\u5f37\u4e86 NN \u7684\u6982\u5316\u80fd\u529b\uff0c\u5728\u672a\u6a19\u8a18\u76ee\u6a19\u8cc7\u6599\u4e0a\u7684\u5206\u985e\u6e96\u78ba\u5ea6\u63d0\u5347\u4e86\u7d04 40%\u3002\u6211\u5011\u9084\u7814\u7a76\u4e86 DA \u5c0d ENN \u7684\u529f\u6548\uff0c\u76f8\u5c0d\u65bc\u4e8c\u9762\u9ad4\u7fa4 $D_N$ \u7684\u4e0d\u540c\u7fa4\u968e\uff0c\u6211\u5011\u767c\u73fe\u96a8\u8457\u7b49\u8b8a\u7a0b\u5ea6\u7684\u589e\u52a0\uff0c\u6a21\u578b\u6548\u80fd\u4e5f\u6703\u63d0\u5347\u3002\u6700\u5f8c\uff0c\u6211\u5011\u767c\u73fe SIDDA \u589e\u5f37\u4e86\u4f86\u6e90\u548c\u76ee\u6a19\u8cc7\u6599\u7684\u6a21\u578b\u6821\u6e96\uff0c\u5728 ECE \u548c\u5e03\u8cf4\u723e\u5206\u6578\u4e0a\u7372\u5f97\u4e86\u6578\u91cf\u7d1a\u7684\u6539\u9032\u3002SIDDA \u7684\u591a\u529f\u80fd\u6027\uff0c\u52a0\u4e0a\u5176\u81ea\u52d5\u5316\u7684\u9818\u57df\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u6709\u6f5b\u529b\u900f\u904e\u4fc3\u9032\u9ad8\u5ea6\u53ef\u6982\u5316\u7684\u6a21\u578b\u958b\u767c\uff0c\u4f86\u63a8\u52d5\u591a\u8cc7\u6599\u96c6\u7814\u7a76\u3002</paragraph>", "author": "Sneh Pandya et.al.", "authors": "Sneh Pandya, Purvik Patel, Brian D. Nord, Mike Walmsley, Aleksandra \u0106iprijanovi\u0107", "id": "2501.14048v1", "paper_url": "http://arxiv.org/abs/2501.14048v1", "repo": "https://github.com/deepskies/gcnn_da"}}