{"2411.03312": {"publish_time": "2024-11-05", "title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models", "paper_summary": "Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks. However, their real-world\ndeployment is often constrained by high latency during inference due to\nsubstantial compute required to process the large number of input tokens\n(predominantly from the image) by the LLM. To reduce inference costs, one can\neither downsize the LLM or reduce the number of input image-tokens, the latter\nof which has been the focus of many recent works around token compression.\nHowever, it is unclear what the optimal trade-off is, as both the factors\ndirectly affect the VLM performance. We first characterize this optimal\ntrade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs, i.e., minimum downstream error at any given\nfixed inference compute, is achieved when using the largest LLM that fits\nwithin the inference budget while minimizing visual token count - often to a\nsingle token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take some initial steps towards building approaches tailored for\nhigh token compression settings. Code is available at\nhttps://github.com/locuslab/llava-token-compression.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5df2\u5728\u5404\u7a2e\u8996\u89ba\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u5f37\u5927\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u65bc LLM \u8655\u7406\u5927\u91cf\u8f38\u5165\u4ee3\u78bc (\u4e3b\u8981\u4f86\u81ea\u5f71\u50cf) \u6240\u9700\u7684\u5927\u91cf\u904b\u7b97\uff0c\u5176\u5be6\u969b\u90e8\u7f72\u901a\u5e38\u6703\u53d7\u5230\u63a8\u8ad6\u671f\u9593\u7684\u9ad8\u5ef6\u9072\u6240\u9650\u5236\u3002\u70ba\u4e86\u964d\u4f4e\u63a8\u8ad6\u6210\u672c\uff0c\u53ef\u4ee5\u7e2e\u5c0f LLM \u6216\u6e1b\u5c11\u8f38\u5165\u5f71\u50cf\u4ee3\u78bc\u7684\u6578\u91cf\uff0c\u5f8c\u8005\u4e00\u76f4\u662f\u8a31\u591a\u8fd1\u671f\u4ee3\u78bc\u58d3\u7e2e\u76f8\u95dc\u5de5\u4f5c\u7684\u91cd\u9ede\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u6700\u4f73\u7684\u6298\u8877\u65b9\u6848\u70ba\u4f55\uff0c\u56e0\u70ba\u9019\u5169\u500b\u56e0\u7d20\u90fd\u6703\u76f4\u63a5\u5f71\u97ff VLM \u6548\u80fd\u3002\u6211\u5011\u9996\u5148\u900f\u904e\u5efa\u7acb\u7e2e\u653e\u5b9a\u5f8b\u4f86\u63cf\u8ff0\u8996\u89ba\u4ee3\u78bc\u6578\u91cf\u548c LLM \u53c3\u6578\u4e4b\u9593\u7684\u6700\u4f73\u6298\u8877\uff0c\u4ee5\u6355\u6349\u9019\u5169\u500b\u56e0\u7d20\u6548\u80fd\u7684\u8b8a\u5316\u3002\u6211\u5011\u7684\u7d50\u679c\u63ed\u793a\u4e86\u4e00\u500b\u4ee4\u4eba\u9a5a\u8a1d\u7684\u8da8\u52e2\uff1a\u5c0d\u65bc\u8996\u89ba\u63a8\u7406\u4efb\u52d9\uff0cVLM \u4e2d\u7684\u63a8\u8ad6\u6700\u4f73\u884c\u70ba\uff0c\u5373\u5728\u4efb\u4f55\u7d66\u5b9a\u7684\u56fa\u5b9a\u63a8\u8ad6\u904b\u7b97\u4e2d\u9054\u5230\u6700\u5c0f\u4e0b\u6e38\u8aa4\u5dee\uff0c\u662f\u5728\u4f7f\u7528\u7b26\u5408\u63a8\u8ad6\u9810\u7b97\u4e14\u540c\u6642\u5c07\u8996\u89ba\u4ee3\u78bc\u6578\u91cf\u964d\u81f3\u6700\u4f4e (\u901a\u5e38\u70ba\u55ae\u4e00\u4ee3\u78bc) \u7684\u6700\u5927 LLM \u6642\u5be6\u73fe\u7684\u3002\u96d6\u7136\u4ee3\u78bc\u6e1b\u5c11\u6587\u737b\u4e3b\u8981\u96c6\u4e2d\u5728\u900f\u904e\u9069\u5ea6\u6e1b\u5c11\u4ee3\u78bc\u6578\u91cf (\u4f8b\u5982\uff0c5-10 \u500d) \u4f86\u7dad\u6301\u57fa\u790e\u6a21\u578b\u6548\u80fd\uff0c\u4f46\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u904b\u7b97\u6700\u4f73\u7684\u63a8\u8ad6\u6a5f\u5236\u9700\u8981\u5728\u66f4\u9ad8\u7684\u4ee3\u78bc\u58d3\u7e2e\u6bd4\u4e0b\u64cd\u4f5c\u3002\u6839\u64da\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u63a1\u53d6\u4e86\u4e00\u4e9b\u521d\u6b65\u6b65\u9a5f\u4f86\u5efa\u69cb\u91dd\u5c0d\u9ad8\u4ee3\u78bc\u58d3\u7e2e\u8a2d\u5b9a\u91cf\u8eab\u6253\u9020\u7684\u65b9\u6cd5\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/locuslab/llava-token-compression \u53d6\u5f97\u3002", "author": "Kevin Y. Li et.al.", "authors": "Kevin Y. Li, Sachin Goyal, Joao D. Semedo, J. Zico Kolter", "id": "2411.03312v1", "paper_url": "http://arxiv.org/abs/2411.03312v1", "repo": "https://github.com/locuslab/llava-token-compression"}}