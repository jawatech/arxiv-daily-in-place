{"2502.09992": {"publish_time": "2025-02-14", "title": "Large Language Diffusion Models", "paper_summary": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs.", "paper_summary_zh": "\u81ea\u56de\u5f52\u6a21\u578b (ARM) \u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u57fa\u77f3\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165 LLaDA \u6765\u6311\u6218\u8fd9\u4e00\u6982\u5ff5\uff0cLLaDA \u662f\u4e00\u79cd\u5728\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03 (SFT) \u8303\u4f8b\u4e0b\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u3002LLaDA \u901a\u8fc7\u6b63\u5411\u6570\u636e\u63a9\u853d\u8fc7\u7a0b\u548c\u53cd\u5411\u8fc7\u7a0b\u5bf9\u5206\u5e03\u5efa\u6a21\uff0c\u7531\u4e00\u4e2a\u9999\u8349 Transformer \u53c2\u6570\u5316\u4ee5\u9884\u6d4b\u88ab\u63a9\u853d\u7684\u6807\u8bb0\u3002\u901a\u8fc7\u4f18\u5316\u4f3c\u7136\u754c\uff0c\u5b83\u4e3a\u6982\u7387\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u751f\u6210\u65b9\u6cd5\u3002\u5728\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLaDA \u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4f18\u4e8e\u6211\u4eec\u81ea\u5df1\u6784\u5efa\u7684 ARM \u57fa\u7ebf\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cLLaDA 8B \u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u4e0e LLaMA3 8B \u7b49\u5f3a\u5927\u7684 LLM \u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u4e14\u5728 SFT \u4e4b\u540e\uff0c\u5728\u591a\u8f6e\u5bf9\u8bdd\u7b49\u6848\u4f8b\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002\u6b64\u5916\uff0cLLaDA \u89e3\u51b3\u4e86\u89e3\u6790\u8bc5\u5492\uff0c\u5728\u53cd\u5411\u8bd7\u6b4c\u5b8c\u6210\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86 GPT-4o\u3002\u6211\u4eec\u7684\u53d1\u73b0\u5c06\u6269\u6563\u6a21\u578b\u786e\u7acb\u4e3a ARM \u7684\u53ef\u884c\u4e14\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6311\u6218\u4e86\u4e0a\u8ff0\u5173\u952e LLM \u80fd\u529b\u672c\u8d28\u4e0a\u4e0e ARM \u76f8\u5173\u8054\u7684\u5047\u8bbe\u3002", "author": "Shen Nie et.al.", "authors": "Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li", "id": "2502.09992v1", "paper_url": "http://arxiv.org/abs/2502.09992v1", "repo": "null"}}