{"2410.12342": {"publish_time": "2024-10-16", "title": "TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant", "paper_summary": "Most knowledge distillation (KD) methodologies predominantly focus on\nteacher-student pairs with similar architectures, such as both being\nconvolutional neural networks (CNNs). However, the potential and flexibility of\nKD can be greatly improved by expanding it to novel Cross-Architecture KD\n(CAKD), where the knowledge of homogeneous and heterogeneous teachers can be\ntransferred flexibly to a given student. The primary challenge in CAKD lies in\nthe substantial feature gaps between heterogeneous models, originating from the\ndistinction of their inherent inductive biases and module functions. To this\nend, we introduce an assistant model as a bridge to facilitate smooth feature\nknowledge transfer between heterogeneous teachers and students. More\nimportantly, within our proposed design principle, the assistant model combines\nthe advantages of cross-architecture inductive biases and module functions by\nmerging convolution and attention modules derived from both student and teacher\nmodule functions. Furthermore, we observe that heterogeneous features exhibit\ndiverse spatial distributions in CAKD, hindering the effectiveness of\nconventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a\nspatial-agnostic InfoNCE loss to align features after spatial smoothing,\nthereby improving the feature alignments in CAKD. Our proposed method is\nevaluated across some homogeneous model pairs and arbitrary heterogeneous\ncombinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance\nfor distilled models with a maximum gain of 11.47% on CIFAR-100 and 3.67% on\nImageNet-1K. Our code and models will be released.", "paper_summary_zh": "<paragraph>\u5927\u591a\u6570\u77e5\u8bc6\u84b8\u998f (KD) \u65b9\u6cd5\u8bba\u4e3b\u8981\u5173\u6ce8\u5177\u6709\u76f8\u4f3c\u67b6\u6784\u7684\u5e08\u751f\u5bf9\uff0c\u4f8b\u5982\u90fd\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\u3002\u7136\u800c\uff0cKD \u7684\u6f5c\u529b\u548c\u7075\u6d3b\u6027\u53ef\u4ee5\u901a\u8fc7\u5c06\u5176\u6269\u5c55\u5230\u65b0\u9896\u7684\u8de8\u67b6\u6784 KD (CAKD) \u6765\u6781\u5927\u63d0\u9ad8\uff0c\u5176\u4e2d\u540c\u8d28\u548c\u5f02\u8d28\u6559\u5e08\u7684\u77e5\u8bc6\u53ef\u4ee5\u7075\u6d3b\u5730\u8f6c\u79fb\u5230\u7ed9\u5b9a\u7684\u5b66\u751f\u3002CAKD \u4e2d\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5f02\u6784\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u5b9e\u8d28\u6027\u7684\u7279\u5f81\u5dee\u8ddd\uff0c\u8fd9\u6e90\u4e8e\u5176\u56fa\u6709\u7684\u5f52\u7eb3\u504f\u5dee\u548c\u6a21\u5757\u529f\u80fd\u7684\u5dee\u5f02\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u8f85\u52a9\u6a21\u578b\u4f5c\u4e3a\u6865\u6881\uff0c\u4ee5\u4fc3\u8fdb\u5f02\u6784\u6559\u5e08\u548c\u5b66\u751f\u4e4b\u95f4\u5e73\u6ed1\u7684\u7279\u5f81\u77e5\u8bc6\u8f6c\u79fb\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5728\u6211\u4eec\u63d0\u51fa\u7684\u8bbe\u8ba1\u539f\u5219\u4e2d\uff0c\u8f85\u52a9\u6a21\u578b\u901a\u8fc7\u5408\u5e76\u6e90\u81ea\u5b66\u751f\u548c\u6559\u5e08\u6a21\u5757\u529f\u80fd\u7684\u5377\u79ef\u548c\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7ed3\u5408\u4e86\u8de8\u67b6\u6784\u5f52\u7eb3\u504f\u5dee\u548c\u6a21\u5757\u529f\u80fd\u7684\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5f02\u6784\u7279\u5f81\u5728 CAKD \u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u7a7a\u95f4\u5206\u5e03\uff0c\u963b\u788d\u4e86\u4f20\u7edf\u7684\u9010\u50cf\u7d20\u5747\u65b9\u8bef\u5dee (MSE) \u635f\u5931\u7684\u6709\u6548\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5229\u7528\u7a7a\u95f4\u65e0\u5173\u7684 InfoNCE \u635f\u5931\u5728\u7a7a\u95f4\u5e73\u6ed1\u540e\u5bf9\u9f50\u7279\u5f81\uff0c\u4ece\u800c\u6539\u5584 CAKD \u4e2d\u7684\u7279\u5f81\u5bf9\u9f50\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e00\u4e9b\u540c\u6784\u6a21\u578b\u5bf9\u548c CNN\u3001ViT \u548c MLP \u7684\u4efb\u610f\u5f02\u6784\u7ec4\u5408\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728 CIFAR-100 \u4e0a\u4ee5 11.47% \u7684\u6700\u5927\u589e\u76ca\u548c ImageNet-1K \u4e0a\u4ee5 3.67% \u7684\u6700\u5927\u589e\u76ca\u5b9e\u73b0\u4e86\u84b8\u998f\u6a21\u578b\u7684\u6700\u65b0\u6027\u80fd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u88ab\u53d1\u5e03\u3002</paragraph>", "author": "Guopeng Li et.al.", "authors": "Guopeng Li, Qiang Wang, Ke Yan, Shouhong Ding, Yuan Gao, Gui-Song Xia", "id": "2410.12342v1", "paper_url": "http://arxiv.org/abs/2410.12342v1", "repo": "null"}}