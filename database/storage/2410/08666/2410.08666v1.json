{"2410.08666": {"publish_time": "2024-10-11", "title": "DeltaDQ: Ultra-High Delta Compression for Fine-Tuned LLMs via Group-wise Dropout and Separate Quantization", "paper_summary": "Large language models achieve exceptional performance on various downstream\ntasks through supervised fine-tuning. However, the diversity of downstream\ntasks and practical requirements makes deploying multiple full-parameter\nfine-tuned models challenging. Current methods that compress the delta weight\nstruggle to achieve ultra-high compression, failing to minimize the deployment\noverhead. To address the above issue, we propose a novel distribution-driven\ndelta compression framework DeltaDQ, which utilizes Group-wise Dropout and\nSeparate Quantization to achieve ultra-high compression for the delta weight.\nWe have observed that the matrix-computed intermediate results for the delta\nweight exhibit extremely small variance and min-max range characteristics,\nreferred to as Balanced Intermediate Results. Exploiting this phenomenon, we\nintroduce Group-wise Dropout to perform dropout on the delta weight using an\noptimal group size. Furthermore, using Separate Quantization, sparse weights\nare quantized and decomposed to achieve a lower bit. Experimental results show\nthat DeltaDQ achieves 16x compression with improved accuracy compared to\nbaselines for WizardMath and WizardCoder models across different parameter\nscales. Moreover, DeltaDQ demonstrates the ability for ultra-high compression\nratio, achieving 128x compression for the WizardMath-7B model and 512x\ncompression for the WizardMath-70B model.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u4e0b\u6e38\u4efb\u52a1\u548c\u5b9e\u9645\u9700\u6c42\u7684\u591a\u6837\u6027\u4f7f\u5f97\u90e8\u7f72\u591a\u4e2a\u5168\u53c2\u6570\u5fae\u8c03\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u3002\u5f53\u524d\u538b\u7f29\u589e\u91cf\u6743\u91cd\u7684\u6a21\u578b\u96be\u4ee5\u5b9e\u73b0\u8d85\u9ad8\u538b\u7f29\uff0c\u65e0\u6cd5\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u90e8\u7f72\u5f00\u9500\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u9a71\u52a8\u7684\u589e\u91cf\u538b\u7f29\u6846\u67b6 DeltaDQ\uff0c\u5b83\u5229\u7528\u7ec4\u5185 Dropout \u548c\u5355\u72ec\u91cf\u5316\u6765\u5b9e\u73b0\u589e\u91cf\u6743\u91cd\u7684\u8d85\u9ad8\u538b\u7f29\u3002\u6211\u4eec\u5df2\u7ecf\u89c2\u5bdf\u5230\uff0c\u589e\u91cf\u6743\u91cd\u7684\u77e9\u9635\u8ba1\u7b97\u4e2d\u95f4\u7ed3\u679c\u8868\u73b0\u51fa\u6781\u5c0f\u7684\u65b9\u5dee\u548c\u6700\u5c0f-\u6700\u5927\u8303\u56f4\u7279\u5f81\uff0c\u79f0\u4e3a\u5e73\u8861\u4e2d\u95f4\u7ed3\u679c\u3002\u5229\u7528\u8fd9\u4e00\u73b0\u8c61\uff0c\u6211\u4eec\u5f15\u5165\u7ec4\u5185 Dropout\uff0c\u4f7f\u7528\u6700\u4f73\u7ec4\u5927\u5c0f\u5bf9\u589e\u91cf\u6743\u91cd\u6267\u884c Dropout\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u5355\u72ec\u91cf\u5316\uff0c\u7a00\u758f\u6743\u91cd\u88ab\u91cf\u5316\u4e3a\u5e76\u5206\u89e3\u4ee5\u5b9e\u73b0\u66f4\u4f4e\u7684\u4f4d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684 WizardMath \u548c WizardCoder \u6a21\u578b\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cDeltaDQ \u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86 16 \u500d\u538b\u7f29\u3002\u6b64\u5916\uff0cDeltaDQ \u5c55\u793a\u4e86\u8d85\u9ad8\u538b\u7f29\u7387\u7684\u80fd\u529b\uff0c\u4e3a WizardMath-7B \u6a21\u578b\u5b9e\u73b0\u4e86 128 \u500d\u538b\u7f29\uff0c\u4e3a WizardMath-70B \u6a21\u578b\u5b9e\u73b0\u4e86 512 \u500d\u538b\u7f29\u3002", "author": "Yanfeng Jiang et.al.", "authors": "Yanfeng Jiang, Zelan Yang, Bohua Chen, Shen Li, Yong Li, Tao Li", "id": "2410.08666v1", "paper_url": "http://arxiv.org/abs/2410.08666v1", "repo": "null"}}