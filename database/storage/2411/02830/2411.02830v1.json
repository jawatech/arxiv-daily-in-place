{"2411.02830": {"publish_time": "2024-11-05", "title": "Mixtures of In-Context Learners", "paper_summary": "In-context learning (ICL) adapts LLMs by providing demonstrations without\nfine-tuning the model parameters; however, it does not differentiate between\ndemonstrations and quadratically increases the complexity of Transformer LLMs,\nexhausting the memory. As a solution, we propose Mixtures of In-Context\nLearners (MoICL), a novel approach to treat subsets of demonstrations as\nexperts and learn a weighting function to merge their output distributions\nbased on a training set. In our experiments, we show performance improvements\non 5 out of 7 classification datasets compared to a set of strong baselines (up\nto +13\\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of\nICL by reducing the inference time needed to achieve the same performance with\nfewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to\n+11\\%), imbalanced (up to +49\\%), or noisy demonstrations (up to +38\\%) or can\nfilter these out from datasets. Overall, MoICL is a more expressive approach to\nlearning from demonstrations without exhausting the context window or memory.", "paper_summary_zh": "\u8a9e\u5883\u5b78\u7fd2 (ICL) \u900f\u904e\u63d0\u4f9b\u793a\u7bc4\u4f86\u8abf\u6574 LLM\uff0c\u800c\u7121\u9700\u5fae\u8abf\u6a21\u578b\u53c3\u6578\uff1b\u7136\u800c\uff0c\u5b83\u4e26\u672a\u5340\u5206\u793a\u7bc4\uff0c\u4e26\u4e8c\u6b21\u589e\u52a0 Transformer LLM \u7684\u8907\u96dc\u6027\uff0c\u8017\u76e1\u8a18\u61b6\u9ad4\u3002\u4f5c\u70ba\u89e3\u6c7a\u65b9\u6848\uff0c\u6211\u5011\u63d0\u51fa\u8a9e\u5883\u5b78\u7fd2\u5668\u6df7\u5408 (MoICL)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u5c07\u793a\u7bc4\u5b50\u96c6\u8996\u70ba\u5c08\u5bb6\uff0c\u4e26\u5b78\u7fd2\u52a0\u6b0a\u51fd\u6578\uff0c\u4ee5\u6839\u64da\u8a13\u7df4\u96c6\u5408\u4f75\u5176\u8f38\u51fa\u5206\u4f48\u3002\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u8207\u4e00\u7d44\u5f37\u5927\u7684\u57fa\u6e96\u76f8\u6bd4\uff0c\u5728 7 \u500b\u5206\u985e\u8cc7\u6599\u96c6\u4e2d\u7684 5 \u500b\u8cc7\u6599\u96c6\u4e0a\u6539\u9032\u4e86\u6548\u80fd\uff08\u8207 ICL \u548c LENS \u76f8\u6bd4\uff0c\u63d0\u5347\u5e45\u5ea6\u9ad8\u9054 +13%\uff09\u3002\u6b64\u5916\uff0c\u6211\u5011\u900f\u904e\u6e1b\u5c11\u9054\u5230\u76f8\u540c\u6548\u80fd\u6240\u9700\u7684\u63a8\u8ad6\u6642\u9593\uff0c\u4f86\u589e\u5f37 ICL \u7684\u5e15\u7d2f\u6258\u524d\u7de3\uff0c\u540c\u6642\u6e1b\u5c11\u793a\u7bc4\u6578\u91cf\u3002\u6700\u5f8c\uff0cMoICL \u5c0d\u9818\u57df\u5916\uff08\u63d0\u5347\u5e45\u5ea6\u9ad8\u9054 +11%\uff09\u3001\u4e0d\u5e73\u8861\uff08\u63d0\u5347\u5e45\u5ea6\u9ad8\u9054 +49%\uff09\u6216\u6709\u96dc\u8a0a\u7684\u793a\u7bc4\uff08\u63d0\u5347\u5e45\u5ea6\u9ad8\u9054 +38%\uff09\u66f4\u5177\u6709\u7a69\u5065\u6027\uff0c\u6216\u8005\u53ef\u4ee5\u5f9e\u8cc7\u6599\u96c6\u4e2d\u7be9\u9078\u51fa\u9019\u4e9b\u793a\u7bc4\u3002\u7e3d\u9ad4\u800c\u8a00\uff0cMoICL \u662f\u4e00\u7a2e\u66f4\u5177\u8868\u73fe\u529b\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5f9e\u793a\u7bc4\u4e2d\u5b78\u7fd2\uff0c\u800c\u4e0d\u6703\u8017\u76e1\u8a9e\u5883\u8996\u7a97\u6216\u8a18\u61b6\u9ad4\u3002", "author": "Giwon Hong et.al.", "authors": "Giwon Hong, Emile van Krieken, Edoardo Ponti, Nikolay Malkin, Pasquale Minervini", "id": "2411.02830v1", "paper_url": "http://arxiv.org/abs/2411.02830v1", "repo": "null"}}