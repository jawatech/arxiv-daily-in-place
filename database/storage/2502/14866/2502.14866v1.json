{"2502.14866": {"publish_time": "2025-02-20", "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention", "paper_summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8655\u7406\u9577\u5e8f\u5217\u65b9\u9762\u5c55\u73fe\u51fa\u9a5a\u4eba\u7684\u6f5b\u529b\uff0c\u4f46\u7531\u65bc\u9810\u586b\u5145\u968e\u6bb5\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8a08\u7b97\u8907\u96dc\u5ea6\u548c\u89e3\u78bc\u968e\u6bb5 KV \u5feb\u53d6\u7684\u5927\u91cf\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\uff0c\u6709\u6548\u63d0\u4f9b\u9019\u4e9b\u9577\u8a9e\u5883\u6a21\u578b\u670d\u52d9\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 LServe\uff0c\u4e00\u500b\u900f\u904e\u6df7\u5408\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\u9577\u5e8f\u5217 LLM \u670d\u52d9\u7684\u9ad8\u6548\u7cfb\u7d71\u3002\u6b64\u65b9\u6cd5\u5c07\u4e0d\u540c\u7684\u786c\u9ad4\u53cb\u5584\u7684\u7d50\u69cb\u5316\u7a00\u758f\u6a21\u5f0f\u7d71\u4e00\u5230\u4e00\u500b\u55ae\u4e00\u7684\u67b6\u69cb\u4e2d\uff0c\u7528\u65bc\u9810\u586b\u5145\u548c\u89e3\u78bc\u6ce8\u610f\u529b\uff0c\u5176\u4e2d\u5c0d\u8f03\u4e0d\u91cd\u8981\u7684\u7b26\u865f\u7684\u904b\u7b97\u6703\u4ee5\u5340\u584a\u65b9\u5f0f\u7565\u904e\u3002LServe \u8b49\u660e\u4e86\u975c\u614b\u548c\u52d5\u614b\u7a00\u758f\u6027\u5728\u9577\u8a9e\u5883 LLM \u6ce8\u610f\u529b\u4e2d\u7684\u76f8\u5bb9\u6027\u3002\u6b64\u8a2d\u8a08\u900f\u904e\u7d50\u5408\u9019\u4e9b\u6700\u4f73\u5316\u4f86\u5be6\u73fe\u500d\u589e\u52a0\u901f\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u4e00\u534a\u7684\u6ce8\u610f\u529b\u982d\u8f49\u63db\u70ba\u9810\u586b\u5145\u548c\u89e3\u78bc\u968e\u6bb5\u4e2d\u5e7e\u4e4e\u514d\u8cbb\u7684\u4e32\u6d41\u982d\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u50c5\u9700\u8981\u6046\u5b9a\u7684 KV \u9801\u6578\u4f86\u4fdd\u7559\u9577\u8a9e\u5883\u529f\u80fd\uff0c\u800c\u8207\u8a9e\u5883\u9577\u5ea6\u7121\u95dc\u3002\u7136\u5f8c\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u5206\u5c64\u5f0f KV \u9801\u9762\u9078\u64c7\u7b56\u7565\uff0c\u6839\u64da\u4ee5\u67e5\u8a62\u70ba\u4e2d\u5fc3\u7684\u76f8\u4f3c\u6027\u52d5\u614b\u522a\u9664 KV \u9801\u9762\u3002\u5e73\u5747\u800c\u8a00\uff0cLServe \u5c07 LLM \u9810\u586b\u5145\u52a0\u901f\u4e86 2.9 \u500d\uff0c\u5c07\u89e3\u78bc\u52a0\u901f\u4e86 1.3-2.1 \u500d\uff0c\u540c\u6642\u7dad\u6301\u9577\u8a9e\u5883\u7684\u6e96\u78ba\u6027\u3002\u7a0b\u5f0f\u78bc\u5df2\u767c\u5e03\u5728 https://github.com/mit-han-lab/omniserve\u3002", "author": "Shang Yang et.al.", "authors": "Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han", "id": "2502.14866v1", "paper_url": "http://arxiv.org/abs/2502.14866v1", "repo": "null"}}