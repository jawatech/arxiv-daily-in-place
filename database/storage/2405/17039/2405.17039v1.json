{"2405.17039": {"publish_time": "2024-05-27", "title": "BWArea Model: Learning World Model, Inverse Dynamics, and Policy for Controllable Language Generation", "paper_summary": "Large language models (LLMs) have catalyzed a paradigm shift in natural\nlanguage processing, yet their limited controllability poses a significant\nchallenge for downstream applications. We aim to address this by drawing\ninspiration from the neural mechanisms of the human brain, specifically Broca's\nand Wernicke's areas, which are crucial for language generation and\ncomprehension, respectively. In particular, Broca's area receives cognitive\ndecision signals from Wernicke's area, treating the language generation as an\nintricate decision-making process, which differs from the fully auto-regressive\nlanguage generation of existing LLMs. In a similar vein, our proposed system,\nthe BWArea model, conceptualizes language generation as a decision-making task.\nThis model has three components: a language world model, an inverse dynamics\nmodel, and a cognitive policy. Like Wernicke's area, the inverse dynamics model\nis designed to deduce the underlying cognitive intentions, or latent actions,\nbehind each token. The BWArea model is amenable to both pre-training and\nfine-tuning like existing LLMs. With 30B clean pre-training tokens, we have\ntrained a BWArea model, which achieves competitive performance with LLMs of\nequal size (1B parameters). Unlike fully auto-regressive LLMs, its pre-training\nperformance does not degenerate if dirty data unintentionally appears. This\nshows the advantage of a decomposed structure of BWArea model in reducing\nefforts in laborious data selection and labeling. Finally, we reveal that the\nBWArea model offers enhanced controllability via fine-tuning the cognitive\npolicy with downstream reward metrics, thereby facilitating alignment with\ngreater simplicity. On 9 out of 10 tasks from two suites, TextWorld and\nBigBench Hard, our method shows superior performance to auto-regressive LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u50ac\u5316\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u5178\u7bc4\u8f49\u79fb\uff0c\u4f46\u5176\u6709\u9650\u7684\u53ef\u63a7\u6027\u5c0d\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u6211\u5011\u65e8\u5728\u900f\u904e\u6c72\u53d6\u4eba\u985e\u5927\u8166\u7684\u795e\u7d93\u6a5f\u5236\uff0c\u7279\u5225\u662f\u5e03\u6d1b\u5361\u5340\u548c\u97cb\u5c3c\u514b\u5340\u7684\u9748\u611f\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u9019\u4e9b\u5340\u57df\u5206\u5225\u5c0d\u8a9e\u8a00\u751f\u6210\u548c\u7406\u89e3\u81f3\u95dc\u91cd\u8981\u3002\u7279\u5225\u662f\uff0c\u5e03\u6d1b\u5361\u5340\u5f9e\u97cb\u5c3c\u514b\u5340\u63a5\u6536\u8a8d\u77e5\u6c7a\u7b56\u8a0a\u865f\uff0c\u5c07\u8a9e\u8a00\u751f\u6210\u8996\u70ba\u4e00\u500b\u8907\u96dc\u7684\u6c7a\u7b56\u5236\u5b9a\u904e\u7a0b\uff0c\u9019\u4e0d\u540c\u65bc\u73fe\u6709 LLM \u7684\u5b8c\u5168\u81ea\u8ff4\u6b78\u8a9e\u8a00\u751f\u6210\u3002\u540c\u6a23\u5730\uff0c\u6211\u5011\u63d0\u51fa\u7684\u7cfb\u7d71\uff0c\u5373 BWArea \u6a21\u578b\uff0c\u5c07\u8a9e\u8a00\u751f\u6210\u6982\u5ff5\u5316\u70ba\u4e00\u500b\u6c7a\u7b56\u5236\u5b9a\u4efb\u52d9\u3002\u6b64\u6a21\u578b\u6709\u4e09\u500b\u7d44\u6210\u90e8\u5206\uff1a\u8a9e\u8a00\u4e16\u754c\u6a21\u578b\u3001\u9006\u52d5\u529b\u6a21\u578b\u548c\u8a8d\u77e5\u7b56\u7565\u3002\u8207\u97cb\u5c3c\u514b\u5340\u4e00\u6a23\uff0c\u9006\u52d5\u529b\u6a21\u578b\u88ab\u8a2d\u8a08\u70ba\u63a8\u8ad6\u6bcf\u500b\u6a19\u8a18\u80cc\u5f8c\u7684\u6f5b\u5728\u8a8d\u77e5\u610f\u5716\u6216\u6f5b\u5728\u52d5\u4f5c\u3002BWArea \u6a21\u578b\u9069\u7528\u65bc\u9810\u8a13\u7df4\u548c\u5fae\u8abf\uff0c\u5c31\u50cf\u73fe\u6709\u7684 LLM \u4e00\u6a23\u3002\u4f7f\u7528 30B \u500b\u4e7e\u6de8\u7684\u9810\u8a13\u7df4\u4ee3\u5e63\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b BWArea \u6a21\u578b\uff0c\u5176\u8207\u5927\u5c0f\u76f8\u7b49 (1B \u500b\u53c3\u6578) \u7684 LLM \u9054\u5230\u4e86\u7af6\u722d\u6027\u7684\u6548\u80fd\u3002\u8207\u5b8c\u5168\u81ea\u8ff4\u6b78\u7684 LLM \u4e0d\u540c\uff0c\u5982\u679c\u610f\u5916\u51fa\u73fe\u9ad2\u6578\u64da\uff0c\u5176\u9810\u8a13\u7df4\u6548\u80fd\u4e0d\u6703\u9000\u5316\u3002\u9019\u986f\u793a\u4e86 BWArea \u6a21\u578b\u5206\u89e3\u7d50\u69cb\u5728\u6e1b\u5c11\u7e41\u7463\u6578\u64da\u9078\u64c7\u548c\u6a19\u8a18\u5de5\u4f5c\u65b9\u9762\u7684\u512a\u52e2\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63ed\u793a\u4e86 BWArea \u6a21\u578b\u900f\u904e\u4f7f\u7528\u4e0b\u6e38\u734e\u52f5\u6307\u6a19\u5fae\u8abf\u8a8d\u77e5\u7b56\u7565\u4f86\u63d0\u4f9b\u589e\u5f37\u7684\u53ef\u63a7\u6027\uff0c\u5f9e\u800c\u4fc3\u9032\u8207\u66f4\u5927\u7c21\u6f54\u6027\u7684\u5c0d\u9f4a\u3002\u5728\u5169\u500b\u5957\u4ef6\uff08TextWorld \u548c BigBench Hard\uff09\u7684 10 \u500b\u4efb\u52d9\u4e2d\u6709 9 \u500b\uff0c\u6211\u5011\u7684\u6a21\u578b\u986f\u793a\u51fa\u512a\u65bc\u81ea\u8ff4\u6b78 LLM \u7684\u6548\u80fd\u3002", "author": "Chengxing Jia et.al.", "authors": "Chengxing Jia, Pengyuan Wang, Ziniu Li, Yi-Chen Li, Zhilong Zhang, Nan Tang, Yang Yu", "id": "2405.17039v1", "paper_url": "http://arxiv.org/abs/2405.17039v1", "repo": "null"}}