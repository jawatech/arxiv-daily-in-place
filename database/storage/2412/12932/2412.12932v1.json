{"2412.12932": {"publish_time": "2024-12-17", "title": "CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models", "paper_summary": "Large Vision-Language Models (LVLMs) have recently demonstrated amazing\nsuccess in multi-modal tasks, including advancements in Multi-modal\nChain-of-Thought (MCoT) reasoning. Despite these successes, current benchmarks\nstill follow a traditional paradigm with multi-modal input and text-modal\noutput, which leads to significant drawbacks such as missing visual operations\nand vague expressions. Motivated by this, we introduce a novel Chain of\nMulti-modal Thought (CoMT) benchmark to address these limitations. Different\nfrom the traditional MCoT benchmark, CoMT requires both multi-modal input and\nmulti-modal reasoning output, aiming to mimic human-like reasoning that\ninherently integrates visual operation. Specifically, CoMT consists of four\ncategories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and\n(4) Visual Selection to comprehensively explore complex visual operations and\nconcise expression in real scenarios. We evaluate various LVLMs and strategies\non CoMT, revealing some key insights into the capabilities and limitations of\nthe current approaches. We hope that CoMT can inspire more research on\nintroducing multi-modal generation into the reasoning process.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u8fd1\u671f\u5df2\u5728\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\u5c55\u73fe\u9a5a\u4eba\u7684\u6210\u529f\uff0c\u5305\u62ec\u591a\u6a21\u614b\u601d\u8003\u93c8 (MCoT) \u63a8\u7406\u7684\u9032\u5c55\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u6210\u529f\uff0c\u76ee\u524d\u7684\u57fa\u6e96\u4ecd\u9075\u5faa\u50b3\u7d71\u7bc4\u4f8b\uff0c\u5177\u6709\u591a\u6a21\u614b\u8f38\u5165\u548c\u6587\u5b57\u6a21\u614b\u8f38\u51fa\uff0c\u9019\u5c0e\u81f4\u4e86\u986f\u8457\u7684\u7f3a\u9ede\uff0c\u4f8b\u5982\u7f3a\u5c11\u8996\u89ba\u904b\u7b97\u548c\u542b\u7cca\u7684\u8868\u9054\u3002\u57fa\u65bc\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u591a\u6a21\u614b\u601d\u8003\u93c8 (CoMT) \u57fa\u6e96\uff0c\u4ee5\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\u3002\u4e0d\u540c\u65bc\u50b3\u7d71\u7684 MCoT \u57fa\u6e96\uff0cCoMT \u9700\u8981\u591a\u6a21\u614b\u8f38\u5165\u548c\u591a\u6a21\u614b\u63a8\u7406\u8f38\u51fa\uff0c\u76ee\u7684\u662f\u6a21\u64ec\u4eba\u985e\u822c\u7684\u63a8\u7406\uff0c\u5176\u672c\u8cea\u4e0a\u6574\u5408\u4e86\u8996\u89ba\u904b\u7b97\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cCoMT \u5305\u542b\u56db\u500b\u985e\u5225\uff1a(1) \u8996\u89ba\u5efa\u7acb\u3001(2) \u8996\u89ba\u522a\u9664\u3001(3) \u8996\u89ba\u66f4\u65b0\u548c (4) \u8996\u89ba\u9078\u64c7\uff0c\u4ee5\u5168\u9762\u63a2\u7d22\u8907\u96dc\u7684\u8996\u89ba\u904b\u7b97\u548c\u73fe\u5be6\u5834\u666f\u4e2d\u7684\u7c21\u6f54\u8868\u9054\u3002\u6211\u5011\u5728 CoMT \u4e0a\u8a55\u4f30\u4e86\u5404\u7a2e LVLMs \u548c\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u5c0d\u7576\u524d\u65b9\u6cd5\u7684\u80fd\u529b\u548c\u9650\u5236\u7684\u4e00\u4e9b\u95dc\u9375\u898b\u89e3\u3002\u6211\u5011\u5e0c\u671b CoMT \u80fd\u6fc0\u52f5\u66f4\u591a\u95dc\u65bc\u5c07\u591a\u6a21\u614b\u751f\u6210\u5f15\u5165\u63a8\u7406\u904e\u7a0b\u7684\u7814\u7a76\u3002", "author": "Zihui Cheng et.al.", "authors": "Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei, Xiaocheng Feng, Wanxiang Che, Min Li, Libo Qin", "id": "2412.12932v1", "paper_url": "http://arxiv.org/abs/2412.12932v1", "repo": "null"}}