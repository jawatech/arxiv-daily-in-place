{"2410.19692": {"publish_time": "2024-10-25", "title": "AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions for Conversational Search with LLMs", "paper_summary": "Generating diverse and effective clarifying questions is crucial for\nimproving query understanding and retrieval performance in open-domain\nconversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration,\nand evaluaTion of Clarifying Questions), an end-to-end LLM-based framework\naddressing the challenges of scalability and adaptability faced by existing\nmethods that rely on manual curation or template-based approaches. AGENT-CQ\nconsists of two stages: a generation stage employing LLM prompting strategies\nto generate clarifying questions, and an evaluation stage (CrowdLLM) that\nsimulates human crowdsourcing judgments using multiple LLM instances to assess\ngenerated questions and answers based on comprehensive quality metrics.\nExtensive experiments on the ClariQ dataset demonstrate CrowdLLM's\neffectiveness in evaluating question and answer quality. Human evaluation and\nCrowdLLM show that the AGENT-CQ - generation stage, consistently outperforms\nbaselines in various aspects of question and answer quality. In retrieval-based\nevaluation, LLM-generated questions significantly enhance retrieval\neffectiveness for both BM25 and cross-encoder models compared to\nhuman-generated questions.", "paper_summary_zh": "\u751f\u6210\u591a\u6a23\u4e14\u6709\u6548\u7684\u6f84\u6e05\u554f\u984c\u5c0d\u65bc\u6539\u5584\u958b\u653e\u9818\u57df\u5c0d\u8a71\u5f0f\u641c\u5c0b (CS) \u7cfb\u7d71\u4e2d\u7684\u67e5\u8a62\u7406\u89e3\u548c\u6aa2\u7d22\u6210\u6548\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u63d0\u51fa AGENT-CQ\uff08\u81ea\u52d5\u751f\u6210\u548c\u8a55\u4f30\u6f84\u6e05\u554f\u984c\uff09\uff0c\u4e00\u7a2e\u7aef\u5230\u7aef\u7684\u57fa\u65bc LLM \u7684\u67b6\u69cb\uff0c\u89e3\u6c7a\u73fe\u6709\u65b9\u6cd5\u6240\u9762\u81e8\u7684\u53ef\u64f4\u5145\u6027\u548c\u9069\u61c9\u6027\u6311\u6230\uff0c\u9019\u4e9b\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u624b\u52d5\u7b56\u5283\u6216\u57fa\u65bc\u7bc4\u672c\u7684\u65b9\u6cd5\u3002AGENT-CQ \u5305\u542b\u5169\u500b\u968e\u6bb5\uff1a\u4e00\u500b\u751f\u6210\u968e\u6bb5\uff0c\u4f7f\u7528 LLM \u63d0\u793a\u7b56\u7565\u4f86\u751f\u6210\u6f84\u6e05\u554f\u984c\uff0c\u4ee5\u53ca\u4e00\u500b\u8a55\u4f30\u968e\u6bb5 (CrowdLLM)\uff0c\u4f7f\u7528\u591a\u500b LLM \u5be6\u4f8b\u6a21\u64ec\u4eba\u5de5\u773e\u5305\u5224\u65b7\uff0c\u4ee5\u6839\u64da\u5168\u9762\u7684\u54c1\u8cea\u6307\u6a19\u8a55\u4f30\u7522\u751f\u7684\u554f\u984c\u548c\u7b54\u6848\u3002\u5728 ClariQ \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 CrowdLLM \u5728\u8a55\u4f30\u554f\u984c\u548c\u7b54\u6848\u54c1\u8cea\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u4eba\u5de5\u8a55\u4f30\u548c CrowdLLM \u986f\u793a\uff0cAGENT-CQ - \u751f\u6210\u968e\u6bb5\u5728\u554f\u984c\u548c\u7b54\u6848\u54c1\u8cea\u7684\u5404\u500b\u65b9\u9762\u59cb\u7d42\u512a\u65bc\u57fa\u6e96\u3002\u5728\u57fa\u65bc\u6aa2\u7d22\u7684\u8a55\u4f30\u4e2d\uff0c\u8207\u4eba\u5de5\u7522\u751f\u7684\u554f\u984c\u76f8\u6bd4\uff0cLLM \u751f\u6210\u7684\u554f\u984c\u986f\u8457\u63d0\u5347\u4e86 BM25 \u548c\u4ea4\u53c9\u7de8\u78bc\u6a21\u578b\u7684\u6aa2\u7d22\u6210\u6548\u3002", "author": "Clemencia Siro et.al.", "authors": "Clemencia Siro, Yifei Yuan, Mohammad Aliannejadi, Maarten de Rijke", "id": "2410.19692v1", "paper_url": "http://arxiv.org/abs/2410.19692v1", "repo": "null"}}