{"2409.20303": {"publish_time": "2024-09-30", "title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions", "paper_summary": "In an era where large language models (LLMs) are increasingly integrated into\na wide range of everyday applications, research into these models' behavior has\nsurged. However, due to the novelty of the field, clear methodological\nguidelines are lacking. This raises concerns about the replicability and\ngeneralizability of insights gained from research on LLM behavior. In this\nstudy, we discuss the potential risk of a replication crisis and support our\nconcerns with a series of replication experiments focused on prompt engineering\ntechniques purported to influence reasoning abilities in LLMs. We tested\nGPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on\nthe chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well\nas Re-Reading prompt engineering techniques, using manually double-checked\nsubsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE,\nScienceQA, and StrategyQA. Our findings reveal a general lack of statistically\nsignificant differences across nearly all techniques tested, highlighting,\namong others, several methodological weaknesses in previous research. We\npropose a forward-looking approach that includes developing robust\nmethodologies for evaluating LLMs, establishing sound benchmarks, and designing\nrigorous experimental frameworks to ensure accurate and reliable assessments of\nmodel outputs.", "paper_summary_zh": "\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u65e5\u76ca\u6574\u5408\u5230\u5404\u7a2e\u65e5\u5e38\u61c9\u7528\u7a0b\u5f0f\u7684\u6642\u4ee3\uff0c\u5c0d\u9019\u4e9b\u6a21\u578b\u884c\u70ba\u7684\u7814\u7a76\u6fc0\u589e\u3002\u7136\u800c\uff0c\u7531\u65bc\u8a72\u9818\u57df\u7684\u65b0\u7a4e\u6027\uff0c\u7f3a\u4e4f\u660e\u78ba\u7684\u65b9\u6cd5\u8ad6\u6307\u5357\u3002\u9019\u5f15\u8d77\u4e86\u5c0d\u5f9e LLM \u884c\u70ba\u7814\u7a76\u4e2d\u7372\u5f97\u7684\u898b\u89e3\u7684\u53ef\u8907\u88fd\u6027\u548c\u6982\u62ec\u6027\u7684\u64d4\u6182\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86\u8907\u88fd\u5371\u6a5f\u7684\u6f5b\u5728\u98a8\u96aa\uff0c\u4e26\u901a\u904e\u4e00\u7cfb\u5217\u8907\u88fd\u5be6\u9a57\u652f\u6301\u6211\u5011\u7684\u64d4\u6182\uff0c\u9019\u4e9b\u5be6\u9a57\u5c08\u6ce8\u65bc\u64da\u7a31\u5f71\u97ff LLM \u63a8\u7406\u80fd\u529b\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u8853\u3002\u6211\u5011\u5728\u601d\u60f3\u93c8\u3001EmotionPrompting\u3001ExpertPrompting\u3001Sandbagging \u4ee5\u53ca Re-Reading \u63d0\u793a\u5de5\u7a0b\u6280\u8853\u4e0a\u6e2c\u8a66\u4e86 GPT-3.5\u3001GPT-4o\u3001Gemini 1.5 Pro\u3001Claude 3 Opus\u3001Llama 3-8B \u548c Llama 3-70B\uff0c\u4f7f\u7528\u624b\u52d5\u96d9\u91cd\u6aa2\u67e5\u7684\u63a8\u7406\u57fa\u6e96\u5b50\u96c6\uff0c\u5305\u62ec CommonsenseQA\u3001CRT\u3001NumGLUE\u3001ScienceQA \u548c StrategyQA\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86\u5728\u5e7e\u4e4e\u6240\u6709\u6e2c\u8a66\u6280\u8853\u4e2d\u666e\u904d\u7f3a\u4e4f\u7d71\u8a08\u5b78\u4e0a\u7684\u986f\u8457\u5dee\u7570\uff0c\u5176\u4e2d\u5305\u62ec\u5f37\u8abf\u4ee5\u524d\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u82e5\u5e72\u65b9\u6cd5\u8ad6\u5f31\u9ede\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u524d\u77bb\u6027\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u958b\u767c\u7528\u65bc\u8a55\u4f30 LLM \u7684\u5065\u58ef\u65b9\u6cd5\u8ad6\u3001\u5efa\u7acb\u5065\u5168\u7684\u57fa\u6e96\u4ee5\u53ca\u8a2d\u8a08\u56b4\u8b39\u7684\u5be6\u9a57\u6846\u67b6\uff0c\u4ee5\u78ba\u4fdd\u5c0d\u6a21\u578b\u8f38\u51fa\u7684\u6e96\u78ba\u4e14\u53ef\u9760\u7684\u8a55\u4f30\u3002", "author": "Laur\u00e8ne Vaugrante et.al.", "authors": "Laur\u00e8ne Vaugrante, Mathias Niepert, Thilo Hagendorff", "id": "2409.20303v1", "paper_url": "http://arxiv.org/abs/2409.20303v1", "repo": "null"}}