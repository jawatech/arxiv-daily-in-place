{"2501.01144": {"publish_time": "2025-01-02", "title": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient LLM Inference", "paper_summary": "Large Language Models (LLMs) have achieved remarkable success, but their\nincreasing size poses significant challenges in memory usage and computational\ncosts. Quantizing both weights and activations can address these issues, with\nfine-grained block-wise quantization emerging as a promising hardware-supported\nsolution to mitigate outliers. However, existing methods struggle to capture\nnuanced block data distributions. To address this, we propose BlockDialect, a\nblock-wise fine-grained mixed format technique that assigns a per-block optimal\nnumber format from formatbook for better data representation. Additionally, we\nintroduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that\nadapt to diverse data distributions. To leverage this efficiently, we propose a\ntwo-stage approach for online DialectFP4 activation quantization. Importantly,\nDialectFP4 ensures hardware efficiency by selecting representable values as\nscaled integers compatible with low-precision integer arithmetic. BlockDialect\nachieves 11.83% (7.56%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model\ncompared to MXFP4 format with lower bit usage per data, while being only 5.46%\n(2.65%) below full precision even when quantizing full-path matrix\nmultiplication. Focusing on how to represent over how to scale, our work\npresents a promising path for energy-efficient LLM inference.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u53d6\u5f97\u986f\u8457\u6210\u529f\uff0c\u4f46\u5176\u898f\u6a21\u4e0d\u65b7\u64f4\u5927\uff0c\u5c0d\u8a18\u61b6\u9ad4\u4f7f\u7528\u548c\u904b\u7b97\u6210\u672c\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u91cf\u5316\u6b0a\u91cd\u548c\u6fc0\u6d3b\u53ef\u4ee5\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u5176\u4e2d\u7d30\u7c92\u5ea6\u5340\u584a\u91cf\u5316\u4f5c\u70ba\u4e00\u7a2e\u6709\u524d\u666f\u7684\u786c\u9ad4\u652f\u63f4\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u6e1b\u8f15\u7570\u5e38\u503c\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u96e3\u4ee5\u6355\u6349\u7d30\u5fae\u7684\u5340\u584a\u8cc7\u6599\u5206\u4f48\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 BlockDialect\uff0c\u9019\u662f\u4e00\u7a2e\u5340\u584a\u5f0f\u7d30\u7c92\u5ea6\u6df7\u5408\u683c\u5f0f\u6280\u8853\uff0c\u5b83\u70ba\u6bcf\u500b\u5340\u584a\u5206\u914d\u4e00\u500b\u4f86\u81ea\u683c\u5f0f\u7c3f\u7684\u6700\u4f73\u6578\u5b57\u683c\u5f0f\uff0c\u4ee5\u7372\u5f97\u66f4\u597d\u7684\u8cc7\u6599\u8868\u793a\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86 DialectFP4\uff0c\u9019\u662f\u4e00\u500b FP4 \u8b8a\u9ad4\uff08\u985e\u4f3c\u65bc\u65b9\u8a00\uff09\u7684\u683c\u5f0f\u7c3f\uff0c\u53ef\u4ee5\u9069\u61c9\u4e0d\u540c\u7684\u8cc7\u6599\u5206\u4f48\u3002\u70ba\u4e86\u6709\u6548\u5229\u7528\u9019\u4e00\u9ede\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7528\u65bc\u7dda\u4e0a DialectFP4 \u6fc0\u6d3b\u91cf\u5316\u7684\u5169\u968e\u6bb5\u65b9\u6cd5\u3002\u91cd\u8981\u7684\u662f\uff0cDialectFP4 \u901a\u904e\u9078\u64c7\u53ef\u8868\u793a\u7684\u503c\u4f5c\u70ba\u8207\u4f4e\u7cbe\u5ea6\u6574\u6578\u904b\u7b97\u76f8\u5bb9\u7684\u7e2e\u653e\u6574\u6578\uff0c\u4f86\u78ba\u4fdd\u786c\u9ad4\u6548\u7387\u3002\u8207\u6bcf\u500b\u8cc7\u6599\u4f7f\u7528\u8f03\u4f4e\u4f4d\u5143\u6578\u7684 MXFP4 \u683c\u5f0f\u76f8\u6bd4\uff0cBlockDialect \u5728 LLaMA3-8B (LLaMA2-7B) \u6a21\u578b\u4e0a\u5be6\u73fe\u4e86 11.83% (7.56%) \u7684\u6e96\u78ba\u5ea6\u63d0\u5347\uff0c\u5373\u4f7f\u5728\u91cf\u5316\u5168\u8def\u5f91\u77e9\u9663\u4e58\u6cd5\u6642\uff0c\u4e5f\u50c5\u6bd4\u5168\u7cbe\u5ea6\u4f4e 5.46% (2.65%)\u3002\u6211\u5011\u7684\u7814\u7a76\u91cd\u9ede\u95dc\u6ce8\u5982\u4f55\u8868\u793a\u800c\u4e0d\u662f\u5982\u4f55\u7e2e\u653e\uff0c\u70ba\u7bc0\u80fd LLM \u63a8\u8ad6\u63d0\u4f9b\u4e86\u4e00\u689d\u6709\u524d\u666f\u7684\u9014\u5f91\u3002", "author": "Wonsuk Jang et.al.", "authors": "Wonsuk Jang, Thierry Tambe", "id": "2501.01144v2", "paper_url": "http://arxiv.org/abs/2501.01144v2", "repo": "null"}}