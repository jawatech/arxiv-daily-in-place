{"2403.17873": {"publish_time": "2024-03-26", "title": "Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach", "paper_summary": "Human-centered explainable AI (HCXAI) advocates for the integration of social\naspects into AI explanations. Central to the HCXAI discourse is the Social\nTransparency (ST) framework, which aims to make the socio-organizational\ncontext of AI systems accessible to their users. In this work, we suggest\nextending the ST framework to address the risks of social misattributions in\nLarge Language Models (LLMs), particularly in sensitive areas like mental\nhealth. In fact LLMs, which are remarkably capable of simulating roles and\npersonas, may lead to mismatches between designers' intentions and users'\nperceptions of social attributes, risking to promote emotional manipulation and\ndangerous behaviors, cases of epistemic injustice, and unwarranted trust. To\naddress these issues, we propose enhancing the ST framework with a fifth\n'W-question' to clarify the specific social attributions assigned to LLMs by\nits designers and users. This addition aims to bridge the gap between LLM\ncapabilities and user perceptions, promoting the ethically responsible\ndevelopment and use of LLM-based technology.", "paper_summary_zh": "", "author": "Andrea Ferrario et.al.", "authors": "Andrea Ferrario,Alberto Termine,Alessandro Facchini", "id": "2403.17873v1", "paper_url": "http://arxiv.org/abs/2403.17873v1", "repo": "null"}}