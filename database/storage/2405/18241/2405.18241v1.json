{"2405.18241": {"publish_time": "2024-05-28", "title": "Active Use of Latent Constituency Representation in both Humans and Large Language Models", "paper_summary": "Understanding how sentences are internally represented in the human brain, as\nwell as in large language models (LLMs) such as ChatGPT, is a major challenge\nfor cognitive science. Classic linguistic theories propose that the brain\nrepresents a sentence by parsing it into hierarchically organized constituents.\nIn contrast, LLMs do not explicitly parse linguistic constituents and their\nlatent representations remains poorly explained. Here, we demonstrate that\nhumans and LLMs construct similar latent representations of hierarchical\nlinguistic constituents by analyzing their behaviors during a novel one-shot\nlearning task, in which they infer which words should be deleted from a\nsentence. Both humans and LLMs tend to delete a constituent, instead of a\nnonconstituent word string. In contrast, a naive sequence processing model that\nhas access to word properties and ordinal positions does not show this\nproperty. Based on the word deletion behaviors, we can reconstruct the latent\nconstituency tree representation of a sentence for both humans and LLMs. These\nresults demonstrate that a latent tree-structured constituency representation\ncan emerge in both the human brain and LLMs.", "paper_summary_zh": "\u4e86\u89e3\u4eba\u985e\u5927\u8166\u5167\u90e8\u5982\u4f55\u8868\u793a\u53e5\u5b50\uff0c\u4ee5\u53ca\u50cf ChatGPT \u9019\u6a23\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u662f\u8a8d\u77e5\u79d1\u5b78\u7684\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u7d93\u5178\u8a9e\u8a00\u5b78\u7406\u8ad6\u63d0\u51fa\uff0c\u5927\u8166\u901a\u904e\u5c07\u53e5\u5b50\u5206\u6790\u70ba\u5c64\u7d1a\u7d44\u7e54\u7684\u6210\u5206\u4f86\u8868\u793a\u53e5\u5b50\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cLLM \u6c92\u6709\u660e\u78ba\u5206\u6790\u8a9e\u8a00\u6210\u5206\uff0c\u5176\u6f5b\u5728\u8868\u793a\u4ecd\u7136\u89e3\u91cb\u4e0d\u6e05\u3002\u5728\u9019\u88e1\uff0c\u6211\u5011\u8b49\u660e\u4e86\u4eba\u985e\u548c LLM \u901a\u904e\u5206\u6790\u4ed6\u5011\u5728\u4e00\u500b\u65b0\u7684\u4e00\u6b21\u6027\u5b78\u7fd2\u4efb\u52d9\u4e2d\u7684\u884c\u70ba\uff0c\u69cb\u5efa\u4e86\u985e\u4f3c\u7684\u5c64\u7d1a\u8a9e\u8a00\u6210\u5206\u7684\u6f5b\u5728\u8868\u793a\uff0c\u5728\u9019\u500b\u4efb\u52d9\u4e2d\uff0c\u4ed6\u5011\u63a8\u65b7\u51fa\u61c9\u8a72\u5f9e\u53e5\u5b50\u4e2d\u522a\u9664\u54ea\u4e9b\u8a5e\u3002\u4eba\u985e\u548c LLM \u90fd\u50be\u5411\u65bc\u522a\u9664\u4e00\u500b\u6210\u5206\uff0c\u800c\u4e0d\u662f\u4e00\u500b\u975e\u6210\u5206\u8a5e\u4e32\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e00\u500b\u53ef\u4ee5\u8a2a\u554f\u8a5e\u5f59\u5c6c\u6027\u548c\u5e8f\u6578\u4f4d\u7f6e\u7684\u5e7c\u7a1a\u5e8f\u5217\u8655\u7406\u6a21\u578b\u6c92\u6709\u986f\u793a\u9019\u500b\u5c6c\u6027\u3002\u57fa\u65bc\u8a5e\u5f59\u522a\u9664\u884c\u70ba\uff0c\u6211\u5011\u53ef\u4ee5\u91cd\u5efa\u4eba\u985e\u548c LLM \u7684\u53e5\u5b50\u7684\u6f5b\u5728\u6210\u5206\u6a39\u8868\u793a\u3002\u9019\u4e9b\u7d50\u679c\u8868\u660e\uff0c\u6f5b\u5728\u6a39\u72c0\u6210\u5206\u8868\u793a\u53ef\u4ee5\u5728\u4eba\u985e\u5927\u8166\u548c LLM \u4e2d\u51fa\u73fe\u3002", "author": "Wei Liu et.al.", "authors": "Wei Liu, Ming Xiang, Nai Ding", "id": "2405.18241v1", "paper_url": "http://arxiv.org/abs/2405.18241v1", "repo": "https://github.com/y1ny/WordDeletion"}}